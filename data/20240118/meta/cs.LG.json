[
    {
        "title": "Robotic Imitation of Human Actions",
        "authors": "Josua SpisakMatthias KerzelStefan Wermter",
        "links": "http://arxiv.org/abs/2401.08381v1",
        "entry_id": "http://arxiv.org/abs/2401.08381v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08381v1",
        "summary": "Imitation can allow us to quickly gain an understanding of a new task.\nThrough a demonstration, we can gain direct knowledge about which actions need\nto be performed and which goals they have. In this paper, we introduce a new\napproach to imitation learning that tackles the challenges of a robot imitating\na human, such as the change in perspective and body schema. Our approach can\nuse a single human demonstration to abstract information about the demonstrated\ntask, and use that information to generalise and replicate it. We facilitate\nthis ability by a new integration of two state-of-the-art methods: a diffusion\naction segmentation model to abstract temporal information from the\ndemonstration and an open vocabulary object detector for spatial information.\nFurthermore, we refine the abstracted information and use symbolic reasoning to\ncreate an action plan utilising inverse kinematics, to allow the robot to\nimitate the demonstrated action.",
        "updated": "2024-01-16 14:11:54 UTC",
        "id": 1
    },
    {
        "title": "Sparse PCA with False Discovery Rate Controlled Variable Selection",
        "authors": "Jasin MachkourArnaud BreloyMichael MumaDaniel P. PalomarFrédéric Pascal",
        "links": "http://arxiv.org/abs/2401.08375v1",
        "entry_id": "http://arxiv.org/abs/2401.08375v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08375v1",
        "summary": "Sparse principal component analysis (PCA) aims at mapping large dimensional\ndata to a linear subspace of lower dimension. By imposing loading vectors to be\nsparse, it performs the double duty of dimension reduction and variable\nselection. Sparse PCA algorithms are usually expressed as a trade-off between\nexplained variance and sparsity of the loading vectors (i.e., number of\nselected variables). As a high explained variance is not necessarily synonymous\nwith relevant information, these methods are prone to select irrelevant\nvariables. To overcome this issue, we propose an alternative formulation of\nsparse PCA driven by the false discovery rate (FDR). We then leverage the\nTerminating-Random Experiments (T-Rex) selector to automatically determine an\nFDR-controlled support of the loading vectors. A major advantage of the\nresulting T-Rex PCA is that no sparsity parameter tuning is required. Numerical\nexperiments and a stock market data example demonstrate a significant\nperformance improvement.",
        "updated": "2024-01-16 14:07:36 UTC",
        "id": 2
    },
    {
        "title": "Weighted Spectral Filters for Kernel Interpolation on Spheres: Estimates of Prediction Accuracy for Noisy Data",
        "authors": "Xiaotong LiuJinxin WangDi WangShao-Bo Lin",
        "links": "http://arxiv.org/abs/2401.08364v1",
        "entry_id": "http://arxiv.org/abs/2401.08364v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08364v1",
        "summary": "Spherical radial-basis-based kernel interpolation abounds in image sciences\nincluding geophysical image reconstruction, climate trends description and\nimage rendering due to its excellent spatial localization property and perfect\napproximation performance. However, in dealing with noisy data, kernel\ninterpolation frequently behaves not so well due to the large condition number\nof the kernel matrix and instability of the interpolation process. In this\npaper, we introduce a weighted spectral filter approach to reduce the condition\nnumber of the kernel matrix and then stabilize kernel interpolation. The main\nbuilding blocks of the proposed method are the well developed spherical\npositive quadrature rules and high-pass spectral filters. Using a recently\ndeveloped integral operator approach for spherical data analysis, we\ntheoretically demonstrate that the proposed weighted spectral filter approach\nsucceeds in breaking through the bottleneck of kernel interpolation, especially\nin fitting noisy data. We provide optimal approximation rates of the new method\nto show that our approach does not compromise the predicting accuracy.\nFurthermore, we conduct both toy simulations and two real-world data\nexperiments with synthetically added noise in geophysical image reconstruction\nand climate image processing to verify our theoretical assertions and show the\nfeasibility of the weighted spectral filter approach.",
        "updated": "2024-01-16 13:46:10 UTC",
        "id": 3
    },
    {
        "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach",
        "authors": "Mahrokh Ghoddousi BoroujeniAndreas KrauseGiancarlo Ferrari Trecate",
        "links": "http://arxiv.org/abs/2401.08351v1",
        "entry_id": "http://arxiv.org/abs/2401.08351v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08351v1",
        "summary": "Federated learning aims to infer a shared model from private and\ndecentralized data stored locally by multiple clients. Personalized federated\nlearning (PFL) goes one step further by adapting the global model to each\nclient, enhancing the model's fit for different clients. A significant level of\npersonalization is required for highly heterogeneous clients, but can be\nchallenging to achieve especially when they have small datasets. To address\nthis problem, we propose a PFL algorithm named PAC-PFL for learning\nprobabilistic models within a PAC-Bayesian framework that utilizes differential\nprivacy to handle data-dependent priors. Our algorithm collaboratively learns a\nshared hyper-posterior and regards each client's posterior inference as the\npersonalization step. By establishing and minimizing a generalization bound on\nthe average true risk of clients, PAC-PFL effectively combats over-fitting.\nPACPFL achieves accurate and well-calibrated predictions, supported by\nexperiments on a dataset of photovoltaic panel power generation, FEMNIST\ndataset (Caldas et al., 2019), and Dirichlet-partitioned EMNIST dataset (Cohen\net al., 2017).",
        "updated": "2024-01-16 13:30:37 UTC",
        "id": 4
    },
    {
        "title": "We don't need no labels: Estimating post-deployment model performance under covariate shift without ground truth",
        "authors": "Jakub BiałekWojtek KuberskiNikolaos Perrakis",
        "links": "http://arxiv.org/abs/2401.08348v1",
        "entry_id": "http://arxiv.org/abs/2401.08348v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08348v1",
        "summary": "The performance of machine learning models often degrades after deployment\ndue to data distribution shifts. In many use cases, it is impossible to\ncalculate the post-deployment performance because labels are unavailable or\nsignificantly delayed. Proxy methods for evaluating model performance\nstability, like drift detection techniques, do not properly quantify data\ndistribution shift impact. As a solution, we propose a robust and accurate\nperformance estimation method for evaluating ML classification models on\nunlabeled data that accurately quantifies the impact of covariate shift on\nmodel performance. We call it multi-calibrated confidence-based performance\nestimation (M-CBPE). It is model and data-type agnostic and works for any\nperformance metric. It does not require access to the monitored model - it uses\nthe model predictions and probability estimates. M-CBPE does not need user\ninput on the nature of the covariate shift as it fully learns from the data. We\nevaluate it with over 600 dataset-model pairs from US census data and compare\nit with multiple benchmarks using several evaluation metrics. Results show that\nM-CBPE is the best method to estimate the performance of classification models\nin any evaluation context.",
        "updated": "2024-01-16 13:29:30 UTC",
        "id": 5
    }
]