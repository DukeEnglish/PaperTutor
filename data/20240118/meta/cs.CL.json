[
    {
        "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
        "authors": "Aman GuptaAnup ShirgaonkarAngels de Luis BalaguerBruno SilvaDaniel HolsteinDawei LiJennifer MarsmanLeonardo O. NunesMahsa RouzbahmanMorris SharpNick MecklenburgRafael PadilhaRanveer ChandraRenato Luiz de Freitas CunhaRoberto de M. Estevão FilhoRyan TsangSara MalvarSwati SharmaTodd HendryVijay AskiVijetha VijayendranVinamra Benara",
        "links": "http://arxiv.org/abs/2401.08406v1",
        "entry_id": "http://arxiv.org/abs/2401.08406v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08406v1",
        "summary": "There are two common ways in which developers are incorporating proprietary\nand domain-specific data when building applications of Large Language Models\n(LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the\nprompt with the external data, while fine-Tuning incorporates the additional\nknowledge into the model itself. However, the pros and cons of both approaches\nare not well understood. In this paper, we propose a pipeline for fine-tuning\nand RAG, and present the tradeoffs of both for multiple popular LLMs, including\nLlama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages,\nincluding extracting information from PDFs, generating questions and answers,\nusing them for fine-tuning, and leveraging GPT-4 for evaluating the results. We\npropose metrics to assess the performance of different stages of the RAG and\nfine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset.\nAgriculture as an industry has not seen much penetration of AI, and we study a\npotentially disruptive application - what if we could provide location-specific\ninsights to a farmer? Our results show the effectiveness of our dataset\ngeneration pipeline in capturing geographic-specific knowledge, and the\nquantitative and qualitative benefits of RAG and fine-tuning. We see an\naccuracy increase of over 6 p.p. when fine-tuning the model and this is\ncumulative with RAG, which increases accuracy by 5 p.p. further. In one\nparticular experiment, we also demonstrate that the fine-tuned model leverages\ninformation from across geographies to answer specific questions, increasing\nanswer similarity from 47% to 72%. Overall, the results point to how systems\nbuilt using LLMs can be adapted to respond and incorporate knowledge across a\ndimension that is critical for a specific industry, paving the way for further\napplications of LLMs in other industrial domains.",
        "updated": "2024-01-16 14:44:47 UTC",
        "id": 1
    },
    {
        "title": "Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine",
        "authors": "Qiao JinFangyuan ChenYiliang ZhouZiyang XuJustin M. CheungRobert ChenRonald M. SummersJustin F. RousseauPeiyun NiMarc J LandsmanSally L. BaxterSubhi J. Al'ArefYijia LiMichael F. ChiangYifan PengZhiyong Lu",
        "links": "http://arxiv.org/abs/2401.08396v1",
        "entry_id": "http://arxiv.org/abs/2401.08396v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08396v1",
        "summary": "Recent studies indicate that Generative Pre-trained Transformer 4 with Vision\n(GPT-4V) outperforms human physicians in medical challenge tasks. However,\nthese evaluations primarily focused on the accuracy of multi-choice questions\nalone. Our study extends the current scope by conducting a comprehensive\nanalysis of GPT-4V's rationales of image comprehension, recall of medical\nknowledge, and step-by-step multimodal reasoning when solving New England\nJournal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test\nthe knowledge and diagnostic capabilities of medical professionals. Evaluation\nresults confirmed that GPT-4V outperforms human physicians regarding\nmulti-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in\ncases where physicians incorrectly answer, with over 80% accuracy. However, we\ndiscovered that GPT-4V frequently presents flawed rationales in cases where it\nmakes the correct final choices (27.3%), most prominent in image comprehension\n(21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our\nfindings emphasize the necessity for further in-depth evaluations of its\nrationales before integrating such models into clinical workflows.",
        "updated": "2024-01-16 14:41:20 UTC",
        "id": 2
    },
    {
        "title": "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models",
        "authors": "Zongxin YangGuikun ChenXiaodi LiWenguan WangYi Yang",
        "links": "http://arxiv.org/abs/2401.08392v1",
        "entry_id": "http://arxiv.org/abs/2401.08392v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08392v1",
        "summary": "The field of AI agents is advancing at an unprecedented rate due to the\ncapabilities of large language models (LLMs). However, LLM-driven visual agents\nmainly focus on solving tasks for the image modality, which limits their\nability to understand the dynamic nature of the real world, making it still far\nfrom real-life applications, e.g., guiding students in laboratory experiments\nand identifying their mistakes. Considering the video modality better reflects\nthe ever-changing and perceptually intensive nature of real-world scenarios, we\ndevise DoraemonGPT, a comprehensive and conceptually elegant system driven by\nLLMs to handle dynamic video tasks. Given a video with a question/task,\nDoraemonGPT begins by converting the input video with massive content into a\nsymbolic memory that stores \\textit{task-related} attributes. This structured\nrepresentation allows for spatial-temporal querying and reasoning by sub-task\ntools, resulting in concise and relevant intermediate results. Recognizing that\nLLMs have limited internal knowledge when it comes to specialized domains\n(e.g., analyzing the scientific principles underlying experiments), we\nincorporate plug-and-play tools to assess external knowledge and address tasks\nacross different domains. Moreover, we introduce a novel LLM-driven planner\nbased on Monte Carlo Tree Search to efficiently explore the large planning\nspace for scheduling various tools. The planner iteratively finds feasible\nsolutions by backpropagating the result's reward, and multiple solutions can be\nsummarized into an improved final answer. We extensively evaluate DoraemonGPT\nin dynamic scenes and provide in-the-wild showcases demonstrating its ability\nto handle more complex questions than previous studies.",
        "updated": "2024-01-16 14:33:09 UTC",
        "id": 3
    },
    {
        "title": "Cross-lingual neural fuzzy matching for exploiting target-language monolingual corpora in computer-aided translation",
        "authors": "Miquel Esplà-GomisVíctor M. Sánchez-CartagenaJuan Antonio Pérez-OrtizFelipe Sánchez-Martínez",
        "links": "http://dx.doi.org/10.18653/v1/2022.emnlp-main.511",
        "entry_id": "http://arxiv.org/abs/2401.08374v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08374v1",
        "summary": "Computer-aided translation (CAT) tools based on translation memories (MT)\nplay a prominent role in the translation workflow of professional translators.\nHowever, the reduced availability of in-domain TMs, as compared to in-domain\nmonolingual corpora, limits its adoption for a number of translation tasks. In\nthis paper, we introduce a novel neural approach aimed at overcoming this\nlimitation by exploiting not only TMs, but also in-domain target-language (TL)\nmonolingual corpora, and still enabling a similar functionality to that offered\nby conventional TM-based CAT tools. Our approach relies on cross-lingual\nsentence embeddings to retrieve translation proposals from TL monolingual\ncorpora, and on a neural model to estimate their post-editing effort. The paper\npresents an automatic evaluation of these techniques on four language pairs\nthat shows that our approach can successfully exploit monolingual texts in a\nTM-based CAT environment, increasing the amount of useful translation\nproposals, and that our neural model for estimating the post-editing effort\nenables the combination of translation proposals obtained from monolingual\ncorpora and from TMs in the usual way. A human evaluation performed on a single\nlanguage pair confirms the results of the automatic evaluation and seems to\nindicate that the translation proposals retrieved with our approach are more\nuseful than what the automatic evaluation shows.",
        "updated": "2024-01-16 14:00:28 UTC",
        "id": 4
    },
    {
        "title": "Morphology and Syntax of the Tamil Language",
        "authors": "Kengatharaiyer Sarveswaran",
        "links": "http://arxiv.org/abs/2401.08367v1",
        "entry_id": "http://arxiv.org/abs/2401.08367v1",
        "pdf_url": "http://arxiv.org/pdf/2401.08367v1",
        "summary": "This paper provides an overview of the morphology and syntax of the Tamil\nlanguage, focusing on its contemporary usage. The paper also highlights the\ncomplexity and richness of Tamil in terms of its morphological and syntactic\nfeatures, which will be useful for linguists analysing the language and\nconducting comparative studies. In addition, the paper will be useful for those\ndeveloping computational resources for the Tamil language. It is proven as a\nrule-based morphological analyser cum generator and a computational grammar for\nTamil have already been developed based on this paper. To enhance accessibility\nfor a broader audience, the analysis is conducted without relying on any\nspecific grammatical formalism.",
        "updated": "2024-01-16 13:52:25 UTC",
        "id": 5
    }
]