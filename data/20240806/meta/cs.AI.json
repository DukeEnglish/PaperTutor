[
    {
        "title": "Self-Taught Evaluators",
        "authors": "Tianlu WangIlia KulikovOlga GolovnevaPing YuWeizhe YuanJane Dwivedi-YuRichard Yuanzhe PangMaryam Fazel-ZarandiJason WestonXian Li",
        "links": "http://arxiv.org/abs/2408.02666v1",
        "entry_id": "http://arxiv.org/abs/2408.02666v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02666v1",
        "summary": "Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.",
        "updated": "2024-08-05 17:57:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02666v1"
    },
    {
        "title": "Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?",
        "authors": "Mohammad Bahrami KarkevandiNishant VishwamitraPeyman Najafirad",
        "links": "http://arxiv.org/abs/2408.02651v1",
        "entry_id": "http://arxiv.org/abs/2408.02651v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02651v1",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nnatural language tasks, but their safety and morality remain contentious due to\ntheir training on internet text corpora. To address these concerns, alignment\ntechniques have been developed to improve the public usability and safety of\nLLMs. Yet, the potential for generating harmful content through these models\nseems to persist. This paper explores the concept of jailbreaking\nLLMs-reversing their alignment through adversarial triggers. Previous methods,\nsuch as soft embedding prompts, manually crafted prompts, and gradient-based\nautomatic prompts, have had limited success on black-box models due to their\nrequirements for model access and for producing a low variety of manually\ncrafted prompts, making them susceptible to being blocked. This paper\nintroduces a novel approach using reinforcement learning to optimize\nadversarial triggers, requiring only inference API access to the target model\nand a small surrogate model. Our method, which leverages a BERTScore-based\nreward function, enhances the transferability and effectiveness of adversarial\ntriggers on new black-box models. We demonstrate that this approach improves\nthe performance of adversarial triggers on a previously untested language\nmodel.",
        "updated": "2024-08-05 17:27:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02651v1"
    },
    {
        "title": "SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models",
        "authors": "Muxi DiaoRumei LiShiyang LiuGuogang LiaoJingang WangXunliang CaiWeiran Xu",
        "links": "http://arxiv.org/abs/2408.02632v1",
        "entry_id": "http://arxiv.org/abs/2408.02632v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02632v1",
        "summary": "As large language models (LLMs) continue to advance in capability and\ninfluence, ensuring their security and preventing harmful outputs has become\ncrucial. A promising approach to address these concerns involves training\nmodels to automatically generate adversarial prompts for red teaming. However,\nthe evolving subtlety of vulnerabilities in LLMs challenges the effectiveness\nof current adversarial methods, which struggle to specifically target and\nexplore the weaknesses of these models. To tackle these challenges, we\nintroduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving\n}\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$\noptimization framework, which enhances security by leveraging data generated by\nthe model itself. SEAS operates through three iterative stages: Initialization,\nAttack, and Adversarial Optimization, refining both the Red Team and Target\nmodels to improve robustness and safety. This framework reduces reliance on\nmanual testing and significantly enhances the security capabilities of LLMs.\nOur contributions include a novel adversarial framework, a comprehensive safety\ndataset, and after three iterations, the Target model achieves a security level\ncomparable to GPT-4, while the Red Team model shows a marked increase in attack\nsuccess rate (ASR) against advanced models.",
        "updated": "2024-08-05 16:55:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02632v1"
    },
    {
        "title": "Language Model Can Listen While Speaking",
        "authors": "Ziyang MaYakun SongChenpeng DuJian CongZhuo ChenYuping WangYuxuan WangXie Chen",
        "links": "http://arxiv.org/abs/2408.02622v1",
        "entry_id": "http://arxiv.org/abs/2408.02622v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02622v1",
        "summary": "Dialogue serves as the most natural manner of human-computer interaction\n(HCI). Recent advancements in speech language models (SLM) have significantly\nenhanced speech-based conversational AI. However, these models are limited to\nturn-based conversation, lacking the ability to interact with humans in\nreal-time spoken scenarios, for example, being interrupted when the generated\ncontent is not satisfactory. To address these limitations, we explore full\nduplex modeling (FDM) in interactive speech language models (iSLM), focusing on\nenhancing real-time interaction and, more explicitly, exploring the\nquintessential ability of interruption. We introduce a novel model design,\nnamely listening-while-speaking language model (LSLM), an end-to-end system\nequipped with both listening and speaking channels. Our LSLM employs a\ntoken-based decoder-only TTS for speech generation and a streaming\nself-supervised learning (SSL) encoder for real-time audio input. LSLM fuses\nboth channels for autoregressive generation and detects turn-taking in real\ntime. Three fusion strategies -- early fusion, middle fusion, and late fusion\n-- are explored, with middle fusion achieving an optimal balance between speech\ngeneration and real-time interaction. Two experimental settings, command-based\nFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity\nto diverse instructions. Our results highlight LSLM's capability to achieve\nduplex communication with minimal impact on existing systems. This study aims\nto advance the development of interactive speech dialogue systems, enhancing\ntheir applicability in real-world contexts.",
        "updated": "2024-08-05 16:47:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02622v1"
    },
    {
        "title": "Backward explanations via redefinition of predicates",
        "authors": "Léo SaulièresMartin C. CooperFlorence Dupin de Saint Cyr",
        "links": "http://arxiv.org/abs/2408.02606v1",
        "entry_id": "http://arxiv.org/abs/2408.02606v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02606v1",
        "summary": "History eXplanation based on Predicates (HXP), studies the behavior of a\nReinforcement Learning (RL) agent in a sequence of agent's interactions with\nthe environment (a history), through the prism of an arbitrary predicate. To\nthis end, an action importance score is computed for each action in the\nhistory. The explanation consists in displaying the most important actions to\nthe user. As the calculation of an action's importance is #W[1]-hard, it is\nnecessary for long histories to approximate the scores, at the expense of their\nquality. We therefore propose a new HXP method, called Backward-HXP, to provide\nexplanations for these histories without having to approximate scores.\nExperiments show the ability of B-HXP to summarise long histories.",
        "updated": "2024-08-05 16:31:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02606v1"
    }
]