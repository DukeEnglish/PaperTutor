[
    {
        "title": "Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics",
        "authors": "Shishira R MaiyaAnubhav GuptaMatthew GwilliamMax EhrlichAbhinav Shrivastava",
        "links": "http://arxiv.org/abs/2408.02672v1",
        "entry_id": "http://arxiv.org/abs/2408.02672v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02672v1",
        "summary": "Implicit Neural Networks (INRs) have emerged as powerful representations to\nencode all forms of data, including images, videos, audios, and scenes. With\nvideo, many INRs for video have been proposed for the compression task, and\nrecent methods feature significant improvements with respect to encoding time,\nstorage, and reconstruction quality. However, these encoded representations\nlack semantic meaning, so they cannot be used for any downstream tasks that\nrequire such properties, such as retrieval. This can act as a barrier for\nadoption of video INRs over traditional codecs as they do not offer any\nsignificant edge apart from compression. To alleviate this, we propose a\nflexible framework that decouples the spatial and temporal aspects of the video\nINR. We accomplish this with a dictionary of per-frame latents that are learned\njointly with a set of video specific hypernetworks, such that given a latent,\nthese hypernetworks can predict the INR weights to reconstruct the given frame.\nThis framework not only retains the compression efficiency, but the learned\nlatents can be aligned with features from large vision models, which grants\nthem discriminative properties. We align these latents with CLIP and show good\nperformance for both compression and video retrieval tasks. By aligning with\nVideoLlama, we are able to perform open-ended chat with our learned latents as\nthe visual inputs. Additionally, the learned latents serve as a proxy for the\nunderlying weights, allowing us perform tasks like video interpolation. These\nsemantic properties and applications, existing simultaneously with ability to\nperform compression, interpolation, and superresolution properties, are a first\nin this field of work.",
        "updated": "2024-08-05 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02672v1"
    },
    {
        "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
        "authors": "Dongyang LiuShitian ZhaoLe ZhuoWeifeng LinYu QiaoHongsheng LiPeng Gao",
        "links": "http://arxiv.org/abs/2408.02657v1",
        "entry_id": "http://arxiv.org/abs/2408.02657v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02657v1",
        "summary": "We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. Unlike existing\nautoregressive image generation approaches, Lumina-mGPT employs a pretrained\ndecoder-only transformer as a unified framework for modeling multimodal token\nsequences. Our key insight is that a simple decoder-only transformer with\nmultimodal Generative PreTraining (mGPT), utilizing the next-token prediction\nobjective on massive interleaved text-image sequences, can learn broad and\ngeneral multimodal capabilities, thereby illuminating photorealistic\ntext-to-image generation. Building on these pretrained models, we propose\nFlexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text\npairs to fully unlock their potential for high-aesthetic image synthesis at any\nresolution while maintaining their general multimodal capabilities.\nFurthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),\ntransforming Lumina-mGPT into a foundation model that seamlessly achieves\nomnipotent task unification. The resulting model demonstrates versatile\nmultimodal capabilities, including visual generation tasks like flexible\ntext-to-image generation and controllable generation, visual recognition tasks\nlike segmentation and depth estimation, and vision-language tasks like\nmultiturn visual question answering. Additionally, we analyze the differences\nand similarities between diffusion-based and autoregressive methods in a direct\ncomparison.",
        "updated": "2024-08-05 17:46:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02657v1"
    },
    {
        "title": "On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization",
        "authors": "Andriy MiranskyyAdam SorrentiViral Thakar",
        "links": "http://arxiv.org/abs/2408.02654v1",
        "entry_id": "http://arxiv.org/abs/2408.02654v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02654v1",
        "summary": "The effectiveness of training neural networks directly impacts computational\ncosts, resource allocation, and model development timelines in machine learning\napplications. An optimizer's ability to train the model adequately (in terms of\ntrained model performance) depends on the model's initial weights. Model weight\ninitialization schemes use pseudorandom number generators (PRNGs) as a source\nof randomness.\n  We investigate whether substituting PRNGs for low-discrepancy quasirandom\nnumber generators (QRNGs) -- namely Sobol' sequences -- as a source of\nrandomness for initializers can improve model performance. We examine\nMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long\nShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,\nCIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses\nten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);\nOrthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with\nweights set using PRNG- and QRNG-based initializers are compared pairwise for\neach combination of dataset, architecture, optimizer, and initialization\nscheme.\n  Our findings indicate that QRNG-based neural network initializers either\nreach a higher accuracy or achieve the same accuracy more quickly than\nPRNG-based initializers in 60% of the 120 experiments conducted. Thus, using\nQRNG-based initializers instead of PRNG-based initializers can speed up and\nimprove model training.",
        "updated": "2024-08-05 17:33:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02654v1"
    },
    {
        "title": "Interactive 3D Medical Image Segmentation with SAM 2",
        "authors": "Chuyun ShenWenhao LiYuhang ShiXiangfeng Wang",
        "links": "http://arxiv.org/abs/2408.02635v1",
        "entry_id": "http://arxiv.org/abs/2408.02635v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02635v1",
        "summary": "Interactive medical image segmentation (IMIS) has shown significant potential\nin enhancing segmentation accuracy by integrating iterative feedback from\nmedical professionals. However, the limited availability of enough 3D medical\ndata restricts the generalization and robustness of most IMIS methods. The\nSegment Anything Model (SAM), though effective for 2D images, requires\nexpensive semi-auto slice-by-slice annotations for 3D medical images. In this\npaper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta\nSAM model trained on videos, for 3D medical image segmentation. By treating\nsequential 2D slices of 3D images as video frames, SAM 2 can fully\nautomatically propagate annotations from a single frame to the entire 3D\nvolume. We propose a practical pipeline for using SAM 2 in 3D medical image\nsegmentation and present key findings highlighting its efficiency and potential\nfor further optimization. Concretely, numerical experiments on the BraTS2020\nand the medical segmentation decathlon datasets demonstrate that SAM 2 still\nhas a gap with supervised methods but can narrow the gap in specific settings\nand organ types, significantly reducing the annotation burden on medical\nprofessionals. Our code will be open-sourced and available at\nhttps://github.com/Chuyun-Shen/SAM_2_Medical_3D.",
        "updated": "2024-08-05 16:58:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02635v1"
    },
    {
        "title": "VidGen-1M: A Large-Scale Dataset for Text-to-video Generation",
        "authors": "Zhiyu TanXiaomeng YangLuozheng QinHao Li",
        "links": "http://arxiv.org/abs/2408.02629v1",
        "entry_id": "http://arxiv.org/abs/2408.02629v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02629v1",
        "summary": "The quality of video-text pairs fundamentally determines the upper bound of\ntext-to-video models. Currently, the datasets used for training these models\nsuffer from significant shortcomings, including low temporal consistency,\npoor-quality captions, substandard video quality, and imbalanced data\ndistribution. The prevailing video curation process, which depends on image\nmodels for tagging and manual rule-based curation, leads to a high\ncomputational load and leaves behind unclean data. As a result, there is a lack\nof appropriate training datasets for text-to-video models. To address this\nproblem, we present VidGen-1M, a superior training dataset for text-to-video\nmodels. Produced through a coarse-to-fine curation strategy, this dataset\nguarantees high-quality videos and detailed captions with excellent temporal\nconsistency. When used to train the video generation model, this dataset has\nled to experimental results that surpass those obtained with other models.",
        "updated": "2024-08-05 16:53:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02629v1"
    }
]