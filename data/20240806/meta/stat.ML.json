[
    {
        "title": "Peer-induced Fairness: A Causal Approach to Reveal Algorithmic Unfairness in Credit Approval",
        "authors": "Shiqi FangZexun ChenJake Ansell",
        "links": "http://arxiv.org/abs/2408.02558v1",
        "entry_id": "http://arxiv.org/abs/2408.02558v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02558v1",
        "summary": "This paper introduces a novel framework, \"peer-induced fairness\", to\nscientifically audit algorithmic fairness. It addresses a critical but often\noverlooked issue: distinguishing between adverse outcomes due to algorithmic\ndiscrimination and those resulting from individuals' insufficient capabilities.\nBy utilizing counterfactual fairness and advanced causal inference techniques,\nsuch as the Single World Intervention Graph, this model-agnostic approach\nevaluates fairness at the individual level through peer comparisons and\nhypothesis testing. It also tackles challenges like data scarcity and\nimbalance, offering a flexible, plug-and-play self-audit tool for stakeholders\nand an external audit tool for regulators, while providing explainable feedback\nfor those affected by unfavorable decisions.",
        "updated": "2024-08-05 15:35:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02558v1"
    },
    {
        "title": "Full error analysis of policy gradient learning algorithms for exploratory linear quadratic mean-field control problem in continuous time with common noise",
        "authors": "Noufel FrikhaHuyên PhamXuanye Song",
        "links": "http://arxiv.org/abs/2408.02489v1",
        "entry_id": "http://arxiv.org/abs/2408.02489v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02489v1",
        "summary": "We consider reinforcement learning (RL) methods for finding optimal policies\nin linear quadratic (LQ) mean field control (MFC) problems over an infinite\nhorizon in continuous time, with common noise and entropy regularization. We\nstudy policy gradient (PG) learning and first demonstrate convergence in a\nmodel-based setting by establishing a suitable gradient domination\ncondition.Next, our main contribution is a comprehensive error analysis, where\nwe prove the global linear convergence and sample complexity of the PG\nalgorithm with two-point gradient estimates in a model-free setting with\nunknown parameters. In this setting, the parameterized optimal policies are\nlearned from samples of the states and population distribution.Finally, we\nprovide numerical evidence supporting the convergence of our implemented\nalgorithms.",
        "updated": "2024-08-05 14:11:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02489v1"
    },
    {
        "title": "On the influence of dependent features in classification problems: a game-theoretic perspective",
        "authors": "Laura Davila-PenaAlejandro Saavedra-NievesBalbina Casas-Méndez",
        "links": "http://arxiv.org/abs/2408.02481v1",
        "entry_id": "http://arxiv.org/abs/2408.02481v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02481v1",
        "summary": "This paper deals with a new measure of the influence of each feature on the\nresponse variable in classification problems, accounting for potential\ndependencies among certain feature subsets. Within this framework, we consider\na sample of individuals characterized by specific features, each feature\nencompassing a finite range of values, and classified based on a binary\nresponse variable. This measure turns out to be an influence measure explored\nin existing literature and related to cooperative game theory. We provide an\naxiomatic characterization of our proposed influence measure by tailoring\nproperties from the cooperative game theory to our specific context.\nFurthermore, we demonstrate that our influence measure becomes a general\ncharacterization of the well-known Banzhaf-Owen value for games with a priori\nunions, from the perspective of classification problems. The definitions and\nresults presented herein are illustrated through numerical examples and various\napplications, offering practical insights into our methodologies.",
        "updated": "2024-08-05 14:02:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02481v1"
    },
    {
        "title": "On Probabilistic Embeddings in Optimal Dimension Reduction",
        "authors": "Ryan MurrayAdam Pickarski",
        "links": "http://arxiv.org/abs/2408.02433v1",
        "entry_id": "http://arxiv.org/abs/2408.02433v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02433v1",
        "summary": "Dimension reduction algorithms are a crucial part of many data science\npipelines, including data exploration, feature creation and selection, and\ndenoising. Despite their wide utilization, many non-linear dimension reduction\nalgorithms are poorly understood from a theoretical perspective. In this work\nwe consider a generalized version of multidimensional scaling, which is posed\nas an optimization problem in which a mapping from a high-dimensional feature\nspace to a lower-dimensional embedding space seeks to preserve either inner\nproducts or norms of the distribution in feature space, and which encompasses\nmany commonly used dimension reduction algorithms. We analytically investigate\nthe variational properties of this problem, leading to the following insights:\n1) Solutions found using standard particle descent methods may lead to\nnon-deterministic embeddings, 2) A relaxed or probabilistic formulation of the\nproblem admits solutions with easily interpretable necessary conditions, 3) The\nglobally optimal solutions to the relaxed problem actually must give a\ndeterministic embedding. This progression of results mirrors the classical\ndevelopment of optimal transportation, and in a case relating to the\nGromov-Wasserstein distance actually gives explicit insight into the structure\nof the optimal embeddings, which are parametrically determined and\ndiscontinuous. Finally, we illustrate that a standard computational\nimplementation of this task does not learn deterministic embeddings, which\nmeans that it learns sub-optimal mappings, and that the embeddings learned in\nthat context have highly misleading clustering structure, underscoring the\ndelicate nature of solving this problem computationally.",
        "updated": "2024-08-05 12:46:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02433v1"
    },
    {
        "title": "Graphical Modelling without Independence Assumptions for Uncentered Data",
        "authors": "Bailey AndrewDavid R. WestheadLuisa Cutillo",
        "links": "http://arxiv.org/abs/2408.02393v1",
        "entry_id": "http://arxiv.org/abs/2408.02393v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02393v1",
        "summary": "The independence assumption is a useful tool to increase the tractability of\none's modelling framework. However, this assumption does not match reality;\nfailing to take dependencies into account can cause models to fail\ndramatically. The field of multi-axis graphical modelling (also called\nmulti-way modelling, Kronecker-separable modelling) has seen growth over the\npast decade, but these models require that the data have zero mean. In the\nmulti-axis case, inference is typically done in the single sample scenario,\nmaking mean inference impossible.\n  In this paper, we demonstrate how the zero-mean assumption can cause\negregious modelling errors, as well as propose a relaxation to the zero-mean\nassumption that allows the avoidance of such errors. Specifically, we propose\nthe \"Kronecker-sum-structured mean\" assumption, which leads to models with\nnonconvex-but-unimodal log-likelihoods that can be solved efficiently with\ncoordinate descent.",
        "updated": "2024-08-05 11:40:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02393v1"
    }
]