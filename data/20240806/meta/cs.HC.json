[
    {
        "title": "Language Model Can Listen While Speaking",
        "authors": "Ziyang MaYakun SongChenpeng DuJian CongZhuo ChenYuping WangYuxuan WangXie Chen",
        "links": "http://arxiv.org/abs/2408.02622v1",
        "entry_id": "http://arxiv.org/abs/2408.02622v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02622v1",
        "summary": "Dialogue serves as the most natural manner of human-computer interaction\n(HCI). Recent advancements in speech language models (SLM) have significantly\nenhanced speech-based conversational AI. However, these models are limited to\nturn-based conversation, lacking the ability to interact with humans in\nreal-time spoken scenarios, for example, being interrupted when the generated\ncontent is not satisfactory. To address these limitations, we explore full\nduplex modeling (FDM) in interactive speech language models (iSLM), focusing on\nenhancing real-time interaction and, more explicitly, exploring the\nquintessential ability of interruption. We introduce a novel model design,\nnamely listening-while-speaking language model (LSLM), an end-to-end system\nequipped with both listening and speaking channels. Our LSLM employs a\ntoken-based decoder-only TTS for speech generation and a streaming\nself-supervised learning (SSL) encoder for real-time audio input. LSLM fuses\nboth channels for autoregressive generation and detects turn-taking in real\ntime. Three fusion strategies -- early fusion, middle fusion, and late fusion\n-- are explored, with middle fusion achieving an optimal balance between speech\ngeneration and real-time interaction. Two experimental settings, command-based\nFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity\nto diverse instructions. Our results highlight LSLM's capability to achieve\nduplex communication with minimal impact on existing systems. This study aims\nto advance the development of interactive speech dialogue systems, enhancing\ntheir applicability in real-world contexts.",
        "updated": "2024-08-05 16:47:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02622v1"
    },
    {
        "title": "DanModCap: Designing a Danmaku Moderation Tool for Video-Sharing Platforms that Leverages Impact Captions",
        "authors": "Siying HuHuanchen WangYu ZhangPiaohong WangZhicong Lu",
        "links": "http://arxiv.org/abs/2408.02574v1",
        "entry_id": "http://arxiv.org/abs/2408.02574v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02574v1",
        "summary": "Online video platforms have gained increased popularity due to their ability\nto support information consumption and sharing and the diverse social\ninteractions they afford. Danmaku, a real-time commentary feature that overlays\nuser comments on a video, has been found to improve user engagement, however,\nthe use of Danmaku can lead to toxic behaviors and inappropriate comments. To\naddress these issues, we propose a proactive moderation approach inspired by\nImpact Captions, a visual technique used in East Asian variety shows. Impact\nCaptions combine textual content and visual elements to construct emotional and\ncognitive resonance. Within the context of this work, Impact Captions were used\nto guide viewers towards positive Danmaku-related activities and elicit more\npro-social behaviors. Leveraging Impact Captions, we developed DanModCap, an\nmoderation tool that collected and analyzed Danmaku and used it as input to\nlarge generative language models to produce Impact Captions. Our evaluation of\nDanModCap demonstrated that Impact Captions reduced negative antagonistic\nemotions, increased users' desire to share positive content, and elicited\nself-control in Danmaku social action to fostering proactive community\nmaintenance behaviors. Our approach highlights the benefits of using\nLLM-supported content moderation methods for proactive moderation in a\nlarge-scale live content contexts.",
        "updated": "2024-08-05 15:48:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02574v1"
    },
    {
        "title": "Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph",
        "authors": "Zhao KaichenSong YaoxianZhao HaiquanLiu HaoyuLi TiefengLi Zhixu",
        "links": "http://arxiv.org/abs/2408.02535v1",
        "entry_id": "http://arxiv.org/abs/2408.02535v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02535v1",
        "summary": "Visual language navigation (VLN) is one of the important research in embodied\nAI. It aims to enable an agent to understand the surrounding environment and\ncomplete navigation tasks. VLN instructions could be categorized into\ncoarse-grained and fine-grained commands. Fine-grained command describes a\nwhole task with subtasks step-by-step. In contrast, coarse-grained command\ngives an abstract task description, which more suites human habits. Most\nexisting work focuses on the former kind of instruction in VLN tasks, ignoring\nthe latter abstract instructions belonging to daily life scenarios. To overcome\nthe above challenge in abstract instruction, we attempt to consider\ncoarse-grained instruction in VLN by event knowledge enhancement. Specifically,\nwe first propose a prompt-based framework to extract an event knowledge graph\n(named VLN-EventKG) for VLN integrally over multiple mainstream benchmark\ndatasets. Through small and large language model collaboration, we realize\nknowledge-enhanced navigation planning (named EventNav) for VLN tasks with\ncoarse-grained instruction input. Additionally, we design a novel dynamic\nhistory backtracking module to correct potential error action planning in real\ntime. Experimental results in various public benchmarks show our\nknowledge-enhanced method has superiority in coarse-grained-instruction VLN\nusing our proposed VLN-EventKG with over $5\\%$ improvement in success rate. Our\nproject is available at https://sites.google.com/view/vln-eventkg",
        "updated": "2024-08-05 15:08:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02535v1"
    },
    {
        "title": "Single-tap Latency Reduction with Single- or Double- tap Prediction",
        "authors": "Naoto NishidaKaori IkematsuJunichi SatoShota YamanakaKota Tsubouchi",
        "links": "http://dx.doi.org/10.1145/3604271",
        "entry_id": "http://arxiv.org/abs/2408.02525v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02525v1",
        "summary": "Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops\n(touchpad), and single and double taps are the most basic and common operations\non them. The detection of single or double taps causes the single-tap latency\nproblem, which creates a bottleneck in terms of the sensitivity of touch\ninputs. To reduce the single-tap latency, we propose a novel\nmachine-learning-based tap prediction method called PredicTaps. Our method\npredicts whether a detected tap is a single tap or the first contact of a\ndouble tap without having to wait for the hundreds of milliseconds\nconventionally required. We present three evaluations and one user evaluation\nthat demonstrate its broad applicability and usability for various tap\nsituations on two form factors (touchpad and smartphone). The results showed\nPredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops\nand to 17.6 ms on smartphones without reducing usability.",
        "updated": "2024-08-05 14:46:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02525v1"
    },
    {
        "title": "PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings",
        "authors": "Fabian Beck",
        "links": "http://arxiv.org/abs/2408.02508v1",
        "entry_id": "http://arxiv.org/abs/2408.02508v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02508v1",
        "summary": "Citations allow quickly identifying related research. If multiple\npublications are selected as seeds, specific suggestions for related literature\ncan be made based on the number of incoming and outgoing citation links to this\nselection. Interactively adding recommended publications to the selection\nrefines the next suggestion and incrementally builds a relevant collection of\npublications. Following this approach, the paper presents a search and foraging\napproach, PUREsuggest, which combines citation-based suggestions with augmented\nvisualizations of the citation network. The focus and novelty of the approach\nis, first, the transparency of how the rankings are explained visually and,\nsecond, that the process can be steered through user-defined keywords, which\nreflect topics of interests. The system can be used to build new literature\ncollections, to update and assess existing ones, as well as to use the\ncollected literature for identifying relevant experts in the field. We\nevaluated the recommendation approach through simulated sessions and performed\na user study investigating search strategies and usage patterns supported by\nthe interface.",
        "updated": "2024-08-05 14:31:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02508v1"
    }
]