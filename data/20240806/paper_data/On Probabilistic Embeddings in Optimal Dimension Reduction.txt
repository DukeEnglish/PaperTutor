On Probabilistic Embeddings in Optimal Dimension Reduction
Ryan Murray & Adam Pickarski∗
August 6, 2024
Abstract
Dimensionreductionalgorithmsareacrucialpartofmanydatasciencepipelines,includingdataex-
ploration, feature creation and selection, and denoising. Despite their wide utilization, many non-linear
dimension reduction algorithms are poorly understood from a theoretical perspective. In this work we
considerageneralizedversionofmultidimensionalscaling,whichisposedasanoptimizationproblemin
whichamappingfromahigh-dimensionalfeaturespacetoalower-dimensionalembeddingspaceseeksto
preserveeitherinnerproductsornormsofthedistributioninfeaturespace,andwhichencompassesmany
commonlyuseddimensionreductionalgorithms. Weanalyticallyinvestigatethevariationalpropertiesof
thisproblem,leadingtothefollowinginsights: 1)Solutionsfoundusingstandardparticledescentmeth-
odsmayleadtonon-deterministicembeddings,2)Arelaxedorprobabilisticformulationoftheproblem
admits solutions with easily interpretable necessary conditions, 3) The globally optimal solutions to the
relaxed problem actually must give a deterministic embedding. This progression of results mirrors the
classical development of optimal transportation, and in a case relating to the Gromov-Wasserstein dis-
tanceactuallygivesexplicitinsightintothestructureoftheoptimalembeddings,whichareparametrically
determined and discontinuous. Finally, we illustrate that a standard computational implementation of
thistaskdoesnotlearndeterministicembeddings,whichmeansthatitlearnssub-optimalmappings,and
that the embeddings learned in that context have highly misleading clustering structure, underscoring
the delicate nature of solving this problem computationally.
1 Introduction
A central task in data science is to find efficient representations of high-dimensional data. One form of this
task is known as dimension reduction, in which one seeks to construct a mapping from a high-dimensional
spacetoalow-dimensionalspacewhichapproximatelypreservesfeaturesofaninputdistribution. Dimension
reduction serves many purposes: it aids in data visualization and exploration, feature construction, and
denoising. Dimension reduction is often stated in terms of some optimization problem, and naturally the
properties and computational tractability are dependent upon the particular dimension reduction objective.
In this work, we consider dimension reduction problems corresponding to optimization problems of the
form
(cid:88)
min c(X ,X ,T(X ),T(X )),
i j i j
T
ij
whereweareconsideringtheX ∈Rdtobedatapointsinahigh-dimensionalfeaturespace,andT :Rd →Rm
i
represents a mapping, or embedding, into a lower dimensional space. A simple mnemonic here is that ‘d’ is
for “data” and ‘m’ is for “embedding”. In order to accommodate both finite data sets and large sample or
population limits, we consider a generalized problem of the form
(cid:90)(cid:90)
J(T):= c(x,x′,T(x),T(x′))µ(dx)µ(dx′), (1.1)
where we will assume that µ ∈ P(Rd), the space of probability measures on Rd. Throughout this work
we make very few assumptions upon µ: it could be supported on a discrete point cloud, a low dimensional
manifold,oracontinuousprobabilitydistribution. Wecallthisproblemthesecond-orderdimensionreduction
∗DepartmentofMathematics,NorthCarolinaStateUniversity
1
4202
guA
5
]LM.tats[
1v33420.8042:viXraproblem, where by second-order we mean that the objective function considers pairwise, or second-order,
interactions between points. This problem encompasses many common dimension reduction problems, see
Section4forexamples. Variantsofthisgeneralproblemhavealsobeenconsideredundertheheadingofmulti-
dimensional scaling and quadratic assignment problems. While not all dimension reduction algorithms can
be written in this second-order form, such algorithms generally serve as building blocks for many commonly
used methods, see Section 1.1 for more discussion.
PerhapsthesimplestversionofthisformofproblemisClassicalMultidimensionalScaling (cMDS),which,
(cid:80) (cid:80)
in the discrete setting and with X = T(X )=0, seeks to minimize the objective function
i i i i
(cid:88)
min (⟨X ,X ⟩−⟨Y −E[Y],Y −E[Y]⟩)2. (1.2)
i j i j
{Yℓ}n
ℓ=1 ij
Alternatively, this can be written, again assuming that E(X)=E[T(X)]=0,
(cid:90)(cid:90)
min (⟨x,x′⟩−⟨T(x),T(x′)⟩)2µ(dx)µ(dx′). (1.3)
T:Rd→Rm
Inbothversionsofthisproblemtheminimizerisknowntobealinearmapping,implyingthattheminimizer
is parametrically determined and smooth. Furthermore, this minimizer can be described as the projection
onto the m-dimensions of greatest variance of µ, and is equivalent to PCA. This approach to dimension
reduction is prevalent in many contexts.
However, in some settings linear embeddings are too restrictive to capture important structures in data.
For this reason a host of different cost functions have been proposed for dimension reduction, each empha-
sizing distinct priorities. In many contexts these algorithms are able to flexibly capture important features
of high-dimensional distributions inaccessible to linear embeddings, but this flexibility comes at a price:
non-linear dimension reduction problems generally can only be resolved via optimization routines, and their
solutions do not admit transparent parametric representation formulas. As such, in many cases theoretical
properties of the solutions to these problems are poorly understood. In particular, in the setting where µ is
a continuum distribution, i.e. the large data or population limit, and when c is non-convex, it is not clear
whether the problem (1.1) even admits a minimizer. We will discuss negative results in the mathematical
literature along these lines in Section 2, but in simplified terms for non-convex energies it is possible for
approximate minimizers to converge towards a limit which is not a function. While the issue of existence is
often straightforward in the finite data setting, the lack of a meaningful population limit raises significant
issues for optimization and interpretability of minimizers: we highlight this issue with a simple numerical
experiment in Example 1.1.
Similarissueswerelong-standinginthetheoryofoptimaltransportation,andourapproachinthispaper
mirrors that literature. In that context, the Monge formulation of optimal transportation seeks to minimize
(cid:90)
min c(x,T(x))µ(dx). (1.4)
T:Rd→Rd,T♯µ=ν
Here ν is an output distribution and T denotes the pushforward measure. Demonstrating that Monge’s
♯
problem has a solution was a major open problem for many years, and while the dimension reduction
problemnotablylackstheoutputdistributionconstraint, theoveralllackofconvexitywithrespecttoT still
engenders a similar type of issue.
The technical solution to this issue in optimal transportation is to instead consider a relaxed version of
the problem, namely
(cid:90)
min c(x,y)π(dxdy), (1.5)
π∈Π(µ,ν)
whereΠ(µ,ν)isthesetofprobabilitydistributionsonRd×Rdwithmarginalsµ,ν: suchprobabilitymeasures
in Π(µ,ν) are called transportation plans and are multi-valued generalizations of the transportation map
T sought for in the Monge problem. In short, this formulation relaxes the requirement that x is mapped
“deterministically”toasingleT(x),andinsteadpermitsasinglextobemappedprobabilisticallytomultiple
outputs. Demonstratingthatthisproblemhasasolutionusing“soft”analyticalmethodsisstraightforward.
Subsequently, one can establish structural properties of such relaxed solutions. Using tools such as cyclical
2monotonicity and convex analysis, one can demonstrate that under mild assumptions minimizers of (1.5)
are actually induced by a mapping, which means that the original Monge problem possesses a solution. We
can similarly pose a relaxed version of the MDS problem by seeking to minimize
(cid:26)(cid:90)(cid:90) (cid:27)
J(π):= c(x,x′,y,y′)π(dxdy)π(dx′dy′), π ∈Π(µ) (1.6)
whereweletΠ(µ)denotethesetofdistributionsonRd×Rm whichhavemarginalµinthefirstdcoordinates
and refer to this as the set of embedding plans. Throughout this article, we often use the notation X = Rd
and Y =Rm to avoid confusion about which space we are embedding to.
The optimal transportation problem is inherently one of linear programming, whereas the dimension
reductionproblemismoreaptlyseenasanon-convexquadraticprogram(seeExample2.2). Wementionthat
there is a quadratic programming variant of optimal transportation. In particular the Gromov-Wasserstein
metric1 between distributions µ,ν, supported respectively on Rd and Rm, is defined by the minimization
problem [29]
(cid:90)(cid:90) (cid:12) (cid:12)p
d (µ,ν)p = min (cid:12)|x−x′|q−|y−y′|q(cid:12) π(dxdy)π(dx′dy′). (1.7)
GWp,q
π∈Π(µ,ν)
(cid:12) (cid:12)
In the Gromov-Wasserstein problem one generally has two marginal constraints, whereas in the dimension
reduction problem there is only a single marginal constraint. As such, we can cast the dimension reduction
(cid:12)
problem as a projection problem in the Gromov-Wasserstein space: namely if we let c(x,x′,y,y′) = (cid:12)|x−
(cid:12)
(cid:12)p
x′|q−|y−y′|q(cid:12) then we have that min J(π)=min d (µ,ν)p.
(cid:12) π∈Π(µ) ν GWp,q
The question of whether minimizers of the Gromov-Wasserstein problem are always induced by trans-
portation maps has recently be studied in [8, 33], and their conclusion is that a deterministic minimizer
(i.e. a minimizer of (1.7) which is supported on the graph of a function) is guaranteed to exist. Their work
however does not establish whether minimizers are necessarily deterministic, a question which is relevant to
solutions found via optimization routines.
A natural question in the context of dimension reduction is whether optimal plans are necessarily maps,
orinotherwordswhethersolutionstotherelaxedproblem(1.6)arealwayssolutionsoftheoriginalproblem
(1.1). The following example demonstrates that for numerically constructed local minimizers, this is not
always the case.
Example 1.1. 2 We consider the problem of embedding a particular point cloud in R2 into R. The point
cloud that we choose has 1,000 points placed at (0,±.2), as well as 250 points placed randomly upon the
unit circle. When we utilize the built-in algorithm for metric multidimensional scaling in Scikit-learn, the
embedding which is found is very discontinuous: this is illustrated in Figure 1a. Indeed, changes around the
boundaryoftheunitcircledonothaveadiscerniblepattern, andappearstobenon-deterministic. Thereason
for this behavior is that due to the larger clusters near the origin, points on the unit circle are energetically
favorableateither±1,inthesensethatbotharelocalminimizerswhenotherpointsareheldfixed. Theselocal
minimizers are both nearly global minimzers as well, as the relative costs of being at either plus or minus one
are comparable: this is due to the fact that the two larger clusters are relatively close together. The behavior
of the solutions found indeed suggests that non-deterministic embedding plans can be local minimizers of the
energy, at least if perturbations are only considered in the sense of small changes to particle positions.
However, working by hand we would expect that the optimal embedding should be much more principled,
andshouldmaphalvesofthecircledeterministicallytodifferentsidesoftherealline, accordingtothecluster
they are closer to. Figure 1b, uses this ansatz to construct an initial guess for the same optimization routine
in Scikit-learn. The learned embedding, while still having a jump discontinuity, is more interpretable and
also obtains a significantly lower cost.
The previous example is, in the authors’ opinion, rather arresting from the practical point of view. The
embedding constructed by standard libraries has found four well-separated clusters, but two of the clusters
1TheGromov-Wassersteinmetricgenerallyisdefinedbetweentwometricmeasurespaces,butwerestrictourattentionhere
todistributionsontwodifferentEuclideanspacesduetotheconnectionwithdimensionreduction.
2The computation in this example was discovered in collaboration with Brian Swenson, and work about computational
aspectsofthisproblemisongoing.
3(a) Scikit-Learn Embedding (b) Globally optimal embedding?
Figure 1: An example where standard algorithms find locally optimal solutions which are not maps. Here
thepositionofthepointsrepresentstheoriginalfeaturesinX =R2,whereasthecolorrepresentsthelearned
embeddinginY =R. ThefirstgraphshowstheembeddinglearnedbytheimplementationofmetricMDSin
Scikit-learn, and the second graph shows the embedding Scikit learn finds if given an analytically-motivated
initial guess. The stress values, normalized by the the number of points squared, is also displayed, with a
clear improvement in the second image.
were constructed by breaking up the unit sphere in a completely arbitrary fashion. Considering those two
clusters as useful features or groups is clearly misleading at best.
Thiscomputationalexamplealsohighlightspotentialmathematicalchallengestoprovingtheequivalence
betweentheoriginalandtherelaxeddimensionreductionproblems,thatisbetweenproblems(1.1)and(1.6).
Indeed,theembeddinglearnedbythestandardimplementationoughttobealocalminimizerinsomesense,
suggesting that it may be possible to find local minimizers of (1.6) which are not mappings.
This work aims to address these questions, in certain contexts, through the following contributions:
1. (Proposition 2.6) We show that the dimension reduction energy (1.1) is not weakly lower semi-
continuousinanyLp spaceformanynaturalchoicesofc, meaningthatexistenceofminimizerscannot
be established using the direct method of the calculus of variations. In practice, this can lead to
highly oscillatory (i.e. non-deterministic) solutions and poor local minima during gradient descent, as
demonstrated in Example 1.1.
2. (Theorems2.8&2.9)Underappropriateconditions,wefirstshowthatforcostsoftheformc(x,x′,⟨y,y′⟩)
andc(x,x′,|y−y′|2), therelaxedproblem(1.6)hasaminimizer. Thisismostlyaconsequenceofstan-
dard arguments from the calculus of variations.
3. (Theorem3.2)Forthesameclassofcosts,wedemonstratethatanyminimizer,π,of (1.6)isessentially
supported on the set
(cid:90)
{y :J (y|x) is minimized in y}, J (y|x):= c(x,x′,y,y′)π(dx′dy′). (1.8)
π π
WecalltheproblemofminimizingJ (y|x)theMarginalProblem. Thisprobleminmanycasesprovides
π
a significant constraint upon the form of π. The argument here relies upon a construction of localized
perturbations inspired by needle perturbations from control theory.
4. (Corollary 3.6) We show that certain costs, namely those for which ⟨y,y′⟩(cid:55)→c(x,x′,⟨y,y′⟩) is convex,
willonlyadmitdeterministicminimizersof (1.6): inthejargonofoptimaltransportationsuchsolutions
are maps. These solutions will furthermore have smoothness controlled by the differentiability of c.
45. (Theorem 4.8) We show that for costs for which |y−y′|2 (cid:55)→c(x,x,|y−y′|2) has a unique minimum at
y =y′ then minimizers of (1.6) will necessarily be deterministic.
6. (Examples3.7, 3.8, &3.9)Wediscussindepththeexampleofaquarticcostin|y−y′|stemmingfrom
Gromov-Wasserstein spaces, which is known to give non-linear embeddings. In that context we can
additionally show that minimizers admit a parametric representation and have discontinuities along
specific hyperplanes.
These results have direct consequences for computational dimension reduction and their applications for
practitioners, which we further discuss in Section 5.
Theremainderoftheworkisorganizedasfollows: inSection1.1wediscussliteraturefromrelatedfields,
including various methods for dimension reduction and optimal transportation. In Sections 2, 3, and 4,
we prove the main results for generic costs, namely in Section 2 we prove the existence of solutions to the
relaxed problem (1.6), in Section 3 we demonstrate that the support of optimal plans is determined by the
Marginal Problem and that similarity costs which are convex in the inner product necessitate deterministic
minimizers, andinSection4wedescribehowtoobtainasimilarresultfornormedsquaredcosts. InSection
3 we investigate some finer properties of the Gromov-Wasserstein projection problem which also serves to
motivate the theoretical considerations in Section 4. In Section 5 we discuss ramifications of these results,
as well as some further questions.
1.1 Related Work
Dimension reduction, and specifically Multidimensional Scaling (MDS), has a long history: we refer the
reader to the books [7, 6] for an in-depth classical statistical treatment of MDS. We mention here that MDS
hasextensionstoavarietyofsettings,suchasthesettingwheretheoriginalpointsbelongtoametricspace,
or even where we only have access to a matrix of similarities or dissimilarities between our x’s. In certain
applied fields, such as psychology [14], MDS has been utilized extensively for group identification, and is
cited in [6] as an important tool for data exploration. In the case of classical Multidimensional Scaling,
which is equivalent to PCA, the explicit representation of solutions has facilitated many theoretical works,
see for example [16] and the references therein. Several computational approaches have also been developed
for speeding up the computation of MDS embeddings. Some references on the topic include [15, 35].
On the other hand, in the last twenty years there has been extensive development of new dimension
reductiontechniqueswithinthecontextofdatascience. Astandardintroductoryreferenceformanyofthose
types of algorithms is citejames2013introduction Chapter 14, and an in-depth comparison of various non-
lineardimensionreductiontechniquescanbefoundin[32]. Thesealgorithmstakeavarietyofapproachesfor
preserving either global or local structure. Some notable examples include local linear embeddings, isomap,
spectralembeddings,Sammonmapping,MultidimensionalScaling,andstochasticneighborhoodembeddings
[26, 30, 4, 27, 14, 12].
While the examples in this paper are fairly general, there are dimension reduction methods which go
beyond our framework as they utilize locally adaptive kernels, for example tSNE, UMAP, or LLE [18, 19,
26]. There has been recent interest in the mathematical community for identifying simplified models and
techniques for understanding tSNE; see for example [2] which uses stochastic processes and random matrix
theorytechniques. Therearealsosomemathematicalworkswhichseektodescribespecificaspectsoffinding
“good” solutions to SNE [17], in particular by studying early exaggeration techniques commonly used for
training. We also remark that variants of the quartic example that we focus on in this work has previously
been identified in the statistical learning literature as a particular scaling limit of tSNE [12].
Thisworkhasbeensignificantlyinfluencedbythedevelopmentofthetheoryofoptimaltransportation,a
good introduction to which can be found in [34]. Recent works in the OT literature, such as multi-marginal
transport[22]andtransportbetweenspacesofunequaldimension[21],havealsodealtwithscenariossimilar
to ours, but in situations with linear dependence on π.
There has also been a lot of interest recently in the Gromov-Wasserstein distance [20], which provides
a transportation-based metric between probability measures on two different metric spaces. Very recently
multiple authors [33, 8] have studied the question of whether optimal plans in the Gromov Wasserstein
problemareinfactrealizedbymappings. TheseworksattempttoconverttheGromov-Wassersteinproblem
intoaninhomogeneouslinear(inπ)problem,whichthentheytacklebyusinggeneraloptimaltransportation
5theory: these works are able to show the existence of an optimal mapping. In particular, in [8] a Monge
mapping is constructed as a solution to the GW problem, but the necessity of a deterministic solution is
still an open question. Furthermore, earlier works such as [33] show that in the quartic setting, if a certain
correlation matrix is non-degenerate then any optimal plan must be induced by a mapping. However, it is
unclear how to directly prove that those correlations are in fact non-degenerate. Similarly, in [1], a Monge
mapping was constructed in the special case between two spheres. Lastly, in [31] it was claimed that when
d=m=1, that optimal solutions admit simple representations (as a monotonic map); however more recent
work [3] refuted this claim and provided a counterexample.
It is important to note the connection between the Gromov-Wasserstein problem and quadratic assign-
mentproblems(QAP).Initsoriginalformulation[13],thequadraticassignmentproblemdescribesavariant
oftheoptimaltransportproblem,whereinthefunctionweminimizeisofseconddegreeintheunknownper-
mutation matrix. A notable example of the QAP is the graph matching problem which matches the edges
of two graphs in a meaningful way. This can rightly be viewed as a type of Gromov-Wasserstein problem.
We also mention that there has also been a lot of recent work trying to find fast algorithms for GW
problems, see for example [24, 31, 28]. The parametric form we derive for quartic MDS suggests that faster
algorithms may also be available for the GW projection problem as well.
Finally,therehasbeenaveinofmathematicalliterature[23,9,5,11]treatingtheminimizationofenergies
of the form
(cid:90)(cid:90)
min I(u), I(u):= Φ(x ,x ,u(x ),u(x ))dx dx .
1 2 1 2 1 2
u
The main focus of these works has been to establish conditions which guarantee the existence of minimizers
forenergiesofthistype,byprovingweaklower-semicontinuityinanappropriatetopology. Toourknowledge
eachoftheseresultsrequiressomeformofconvexitywithrespecttoΦ. Ourworkstronglycontraststhatline
ofwork,inthat1)westudyformsofΦwithspecificsymmetries,2)wedemonstratethatourenergiesarenot
weakly lower semicontinuous, and 3) we demonstrate that in spite of this lack of weak lower semicontinuity
that there still exists minimizers of our original dimension reduction problem.
2 Existence of Relaxed Solutions
In this section we consider the problem of existence of minimizers of (1.1) and (1.6). Along the way, we
demonstratethatmanyofthestandardtechniquesfromthecalculusofvariationsdonotapplytotheoriginal
problem of finding an embedding map as in (1.1), namely the lack of weak lower semi-continuity. These
theoreticalobservationsdirectlycomplimentthephenomenonobservedinExample1.1,anddemonstratethe
difficulty of proving properties of minimizers of the original problem (1.1).
Convexityplaysacrucialroleinprovingexistenceofminimizersformanyvariationalproblems. Webegin
by demonstrating, through a simple example, why convexity can fail in second-order dimension reduction
problems.
Example 2.1. We consider, as a running example throughout the paper, the quartic cost c(x,x′,y,y′) =
√
(cid:0) |x−x′|2−|y−y′|2(cid:1)2 . Fixε>0andletT ∈C1(Rd;Rm)beaLipschitzfunctionsuchthat∥DT∥ ≤ 2−ε.
∞
We consider the effect of interpolating between T(x) and −T(x). Clearly the midpoint between these two
maps is identically zero (we call this map the “zero map” through the paper), namly 1(T(x)−T(x)) ≡ 0,
2
and furthermore from the norm structure of the cost we immediately have that J(T)=J(−T). Hence if J
were midpoint convex, one would require J(T)≥J(0). However, we have
(cid:90)(cid:90)
J(T)−J(0)= |T(x)−T(x′)|4−2|x−x′|2|T(x)−T(x′)|2µ(dx)µ(dx′)
(cid:90)(cid:90)
≤ (2−ε)|x−x′|2|T(x)−T(x′)|2−2|x−x′|2|T(x)−T(x′)|2µ(dx)µ(dx′)
(cid:18)(cid:90)(cid:90) (cid:19)
=−ε |x−x′|2|T(x)−T(x′)|2µ(dx)µ(dx′) ≤0.
If µ has a direction of non-zero variance, and T is chosen to also vary in that direction, then this inequality
is strict: one can find a linear mapping which achieves this goal. Hence J is not convex with respect to T.
6It turns out that the previous observation, which primarily stems from the reflection symmetry of the
quartic cost, extends to many second-order costs that have been previously considered for dimension re-
duction. In order to make the problem more concrete, we focus on two basic examples which encompass
a broad family of practical situations. The first models the interactions of the embedded variables by an
inner product, the second by a squared norm: these costs are known in the literature for multidimensional
scaling as similarity and dissimilarity costs respectively. For concreteness we state these assumptions upon
the structure more explicitly in Section 2.2. However, with this distinction in place, we can now restate the
non-convexity result above in more generality. Later we will provide suitable assumptions to identify the
domain of definition for the dimension reduction problem.
Proposition 2.2. If the functionals
(cid:90)(cid:90) (cid:90)(cid:90)
J (T)= c(x,x′,⟨T(x),T(x′)⟩)µ(dx)µ(dx′), J (T)= c(x,x′,|T(x)−T(x′)|2)µ(dx)µ(dx′)
IP N2
are finite for functions in Lp(Rd;Rm|µ), and T ≡0 is not the global minimizer, then J &J are neither
IP N2
convex nor concave on Lp(Rd;Rm|µ).
Proof. The proof follows exactly as in Example 2.1: If J(T) = J(−T) < J(0) for some T then J cannot
be midpoint convex. Furthermore, J cannot be concave if it is non-constant and positive.
As mentioned above, this lack of functional convexity will become a significant theoretical obstacle: this
type of obstacle is well-known in the literature for the theory of the Calculus of Variations. In order to
clarify this obstruction to a broader audience, we use the next section to provide a brief overview of this
theory. AreaderwhoiscomfortablewithalloftheseconceptscansafelyskiptoSection2.2; themainresult
being Proposition 2.6 which demonstrates that the dimension reduction energy cannot be a weakly lower
semi-continuuous functional on Lp(Rd;Rm|µ).
2.1 A detour into theory of variational problems
Wenowdescribethereasonwhyexistenceofminimizersto(1.1)isachallengingproblem. Toputitconcisely,
the standard “direct method” from the calculus of variations does not apply due to the non-convexity of
c. This occurs both due to the generic the lack of (strong) compactness in Lp(Rd;Rm|µ) and the failure of
weak lower semi-continuity of J in the same space. We provide a number of standard examples to clarify
these phenomena to a broader audience.
The direct method of the calculus of variations seeks to generalize the extreme value theorem in finite
dimensionstoinfinitedimensionaloptimizationproblems. Itprovestheexistenceofminimizersofafunctional
I :U →R, where U is an infinite-dimensional space, by combining the following assumptions:
1. Coercivity: Given some set B ⊂U we have that I(Bc)>inf I.
U
2. Compactness: Under some topology τ we have that B is sequentially compact.
3. Continuity: Under that same topology, the functional I is sequentially lower semi-continuous.
One then directly shows the existence of minimizers by taking the following steps: i) Construct a sequence
of functions u ∈B so that limI(u )=inf I, ii) After taking a subsequence, u → u∗, and iii) Using the
n n U n τ
lower semi-continuity we have that I(u∗)≤liminfI(u ), implying that u∗ is a minimizer.
n
The main challenge in carrying out this approach is that if U is an infinite-dimensional normed space
and B is some ball in that norm, then B can never be compact under the same norm. As such, one needs to
select a weaker topology that allows compactness. The price to pay is that in weaker topologies continuity
of I is a stronger condition to verify.
In this section, we will primarily focus on Lp type spaces, because for many notable examples we expect
minimizersofourvariationalproblemtofailtobecontinuous. Toillustratewhythisisthecase,webeginwith
a toy problem demonstrating how non-convex functional optimization can have discontinuous minimizers.
7Example 2.3 (Double-well Potential). Let f(x,u)= 1(u2−1)2−ux and define the functional
4
(cid:90) 1
I(u)= f(x,u(x))dx. (2.1)
−1
Inthissimplecase,onecandirectlyshowthattheminimizerofthisfunctionalisgivenbyu∗(x)∈argminf(x,·)
for every x ∈ [−1,1]. We display this function in Figure 2b, and the discontinuity at x = 0 is apparent.
This occurs because there are two distinct, well-separated, global minima at x = 0. We notice that for
x∈(cid:2) − 1 − 1 , 1 + 1 ,(cid:3) , the function f(x,·) has 2 local minima, and that the global minima switches
33/2 31/2 33/2 31/2
from one side to the other at x=0: this is illustrated in Figure 2a.
u
f(x,u)
x=−0.1
x=0
x=0.1 1
x
−1 −0.5 0.5 1
−1
u
(a) Notice how as x passes through 0, the global mini- (b) As a consequence, the minimizer of I(u) is discon-
mizer of f(x,·) jumps between two values. tinuous.
In the previous example we could immediately verify that u∗ is a minimizer, by directly comparing its
energy to that of any other function. However, if we did not know the form of u∗ we would need to utilize
the direct method to prove that a minimizer exists. For the sake of illustration, we will discuss this first in
the context of the functional I. As evidenced by the previous example, we need to minimize over a function
space that permits discontinuities; we select L∞([−1,1];R) for simplicity.
Whenminimizing (2.1)overthespaceofboundedfunctions,wenoticethatcontinuityoftheenergywith
respect to the strong topology (i.e. the topology induced by the L∞ norm) is nearly immediate, because
|I(u )−I(u )|≤C sup |u (x)−u (x)|.
1 2 1 2
x∈[−1,1]
However, the bounded sequences in L∞ are far from being compact: take for example sign(sin(nx)) which
has no convergent subsequence in L∞. The standard approach is to weaken the notion of convergence on
L∞ to convergence in duality with L1, i.e. weak-* convergence, which we denote by ⇀∗. More explicitly, we
say that u ⇀∗ u∈Lp if for every v ∈Lp∗ we have that
n p
(cid:90) (cid:90) 1 1
u (x)v(x)dx→ u(x)v(x)dx, + =1.
n p p∗
We can directly check that sign(sin(nx))⇀∗ 0, and indeed we can show that any bounded sequence in L∞
∞
is weak-* compact. However, the following example shows that upon moving to this topology the functional
I is no longer lower semi-continuous.
Example 2.4. Define the sequence u (x)=sign(sin(nπx)). As we have said, the sequence has no (strongly)
n
convergent subsequence in L∞([−1,1];R), but u ⇀∗ 0. However, it can be checked directly that I(u ) =
n ∞ n
(−1)n/n → 0, and that I(0) = 1/2. Therefore I is not weakly-* lower semi-continuous. Another way of
interpreting this example is to notice that f(x,u (x)) does not converge in the weak-* topology to f(x,0).
n
8We see in the previous example that the continuity of I with respect to L∞ does not imply that it is
weak-*lowersemi-continuous: thefollowingclassicalresultlinksthisphenomenonwithconvexityforgeneral
integral functionals. For a reference, see Theorems 6.54 & 6.56 in [10].
Proposition 2.5. Let f : Rd×Rm → R be a continuous function that is bounded below. For 1 ≤ p ≤ ∞
define I :Lp(Rd;Rm)→R by
(cid:90)
I(u)= f(x,u(x))dx,
then I is weakly lower semi-continuous (weak-* if p=∞) if and only if u(cid:55)→f(x,u) is convex.
For a simple integral energy of the form I, it is possible to show existence of minimizers using direct
pointwiseoptimizationarguments. However,eveninthatcasethelimitsofapproximateminimizersmayfail
tobefunctions,highlightingpotentialissuesforcomputationalalgorithms. Furthermore,theintroductionof
othertermsinthefunctional,suchasmarginalconstraintsinoptimaltransportation,canmaketheexistence
of minimizers a very challenging problem. In our second-order case, the form of the energy is different, and
we are aware of no direct construction of minimizers. In particular, we notice that the dimension reduction
problem can be restated as
(cid:90) (cid:90)
min J (x,T(x))µ(dx), with J (x,y):= c(x,x′,y,T(x′))µ(dx′).
T T
T:Rd→Rm
As stated in the introduction, the cost function c(x,x′,y,y′) is often not convex in practice, and in many
cases we will not generally have that y (cid:55)→ J (x,y) is convex. Thus, by Proposition 2.5, we suspect that
T
the dimension reduction problem (1.1) will not be weakly lower semi-continuous. The following result
demonstrates that this indeed is the case.
Proposition2.6. Considerthedimensionreductionproblem (1.1)inthecasewherec(x,x′,y,y′)=c˜(x,x′,|y−
y′|2) for some C1 function c˜ which is symmetric in x,x′. Assume that µ has a continuous density on an
open and bounded set, and suppose that for all x̸=x′ we have that dc˜(x,x′,t)| <0. Then the dimension
dt t=0
reduction problem is not weakly lower semi-continuous.
Proof. Let us choose
(cid:18) d (cid:19)
(cid:89)
T (x)=v sign(sin(nπx ))
n i
i=1
for some v ∈Rm which will later be specified. First note that clearly T ⇀0. Furthermore, by denoting the
n
sets
(cid:40) d (cid:41) (cid:40) d (cid:41)
(cid:89) (cid:89)
E = x: sign(sin(nπx ))=1 , O = x: sign(sin(nπx ))=−1 ,
n i n i
i=1 i=1
the cost of T will be computed as
n
(cid:90)(cid:90)
J(T )= c(x,x′,0)µ(dx)µ(dx′)
n
En×En
(cid:90)(cid:90)
+ c˜(x,x′,0)µ(dx)µ(dx′)
On×On
(cid:90)(cid:90)
+2 c˜(x,x′,2|v|)µ(dx)µ(dx′)
En×On
(cid:90)(cid:90)
=J(0)+2 [c˜(x,x′,2|v|)−c˜(x,x′,0)]µ(dx)µ(dx′).
En×On
Notice that we have, by the Riemann-Lebesgue Lemma,
(cid:90)(cid:90) 1(cid:90)(cid:90)
2 [c˜(x,x′,2|v|)−c˜(x,x′,0)]µ(dx)µ(dx′)−n −→ −−∞ → [c˜(x,x′,2|v|)−c˜(x,x′,0)]µ(dx)µ(dx′).
2
En×On
9where we have used the fact that
(1+(cid:81)d sign(sin(nπx )))(1+(cid:81)d sign(sin(nπx′)))
1 (x)1 (x′)= i=1 i i=1 i
En On 4
along with the continuity of c and the density µ. Thus, given ε>0 for sufficiently large n, we have that
1(cid:90)(cid:90)
J(T )−J(0)< [c˜(x,x′,2|v|)−c˜(x,x′,0)]µ(dx)µ(dx′)+ε
n 2
1(cid:90)(cid:90)
≤ −ϕ(x,x′)|v|+o(|v|)µ(dx)µ(dx′)+ε,
2
where ϕ(x,x′) ≥ 0 with equality only possibly when x = x′ by our assumption upon the derivative of c˜.
Making v sufficiently small so that we can neglect the o(|v|) term, and taking ε → 0 then implies that
liminf J(T )<J(0), proving the result.
n n
The previous proposition demonstrates that the dimension reduction energy J is not weakly lower semi-
continuous: this implies that information about minimization is lost in limit obtained with that topology.
The standard approach to handling this situation is to instead permit limits that are multi-valued: meaning
that one x is mapped probabilistically to multiple y values. For example, in the proof of the previous
proposition we may write
π (dxdy)=µ(dx)(1 (x)δ (dy)+1 (x)δ (dy)),
n En v On −v
and then compute
(cid:90)(cid:90)
J(T )= c(x,x′,y,y′)π (dxdy)π (dx′dy′).
n n n
Using the computation with the Riemann-Lebesgue lemma in the proof of the previous proposition, it is
straightforward to show that π converges (in the sense of weak convergence of measures) to π(dxdy) =
n
µ(dx)(1/2δ (dy)+1/2δ (dy)). Hence we have that
v −v
(cid:90)(cid:90)
J(T )→ c(x,x′,y,y′)π(dxdy)π(dx′dy′).
n
Slightly abusing notation, we can then define a relaxed energy
(cid:90)(cid:90)
J(π):= c(x,x′,y,y′)π(dxdy)π(dx′dy′).
Hereπ representsaprobabilisticcouplingbetweenx’sandy’swhichgeneralizesadeterministiccoupling(or
function)mappingeachxtoasingley. Inthecontextofoptimaltransportation,thecouplingπ issometimes
called a transportation plan, whereas a deterministic coupling in that context is called a transportation map.
In the continuum mechanics literature such a probabilistic relaxation is called a Young measure. In many
contextstheexistenceofminimizersoftherelaxedenergyismorestraightforwardtoproveusingcompactness
and continuity arguments: we carry out these standard arguments in the next section.
2.2 Existence of relaxed solutions
In light of the discussion in the previous section, we turn our attention to the problem of existence of
minimizerstotherelaxedproblem(1.6). Webeginbygivingsomedefinitions. Givenµ∈P(Rd)andfamily
{ν(·|x)} ⊂ P(Rm) for which x (cid:55)→ ν(Q|x) is a measurable function for all Q ∈ B(Rm), there exists a
x∈Rd
unique (in measure) probability distribution π ∈P(Rd×Rm) such that for all P ∈B(Rd) and Q∈B(Rm),
(cid:90)
π(P ×Q)= ν(Q|x)µ(dx). (2.2)
P
Let the space of all joint probability measures which can be written in the form above be called Π(µ), more
preciselyΠ(µ):={π ∈P(Rd×Rm)|proj ♯π =µ}whicharealltheprobabilitymeasuresonRd×Rm with
Rd
X-marginal µ. In analogy to optimal transportation, we call Π(µ) the set of embedding plans for µ.
10(cid:82)(cid:82)
As soon as c is itself lower semi-continuous, the function π (cid:55)→ cdπdπ is automatically lower semi
continuous with respect to weak convergence of probability measures, by Portmanteau’s theorem. We recall
that a sequence of probability measures π ∈ P(Rd ×Rm) is said to converge weakly to π if for every
n
(cid:82) (cid:82)
bounded, continuous function f we have that fdπ → fdπ. In order to recover sequential compactness
n
for sequences of probability measures π ∈Π(µ), we must introduce the notion of tightness of measure and
n
its application on the subspace Π(µ).
Definition 2.7 (TightnessofEmbeddingPlans). Asequenceofprobabilitydistributions{π }∞ ⊂P(Rd×
n n=1
Rm)issaidtobetightifforeveryε>0,thereexistsacompactsetK ⊂Rd×Rm forwhichsup π (Kc)<ε.
ε n n ε
In the case that π ∈ Π(µ), we can find a compact set K in Rd so that µ(K ) > 1− ε. In turn if we
n d d 2
can find a compact set K so that π (Rd×K )>1− ε we can use K =K ×K and obtain the estimate
m n m 2 d m
π (Kc)<ε: this implies that when π ∈Π(µ) we only need to verify tightness in the marginal over the last
n n
m coordinates. In symbols, we write this as
{π }∞ is tight in Π(µ) ⇐⇒ ν :=proj ♯π , {ν }∞ is tight in P(Rm).
n n=1 n Y n n n=1
(cid:82)
Here we, in a slight abuse of notation, are letting ν(Q) = dπ(x,y): meaning that if we suppress
Rd×Q
the x-dependence in ν(dy|x) then we are indicating the marginal distribution in y.
ByProkhorov’stheorem,tightnessofasequenceofprobabilitymeasuresimpliesweakcompactness. Thus
the problem of existence of minimizers to the relaxed problem reduces to establishing tightness of sequences
of embedding plans with bounded energy J.
Assumptions
Wearenowreadytolistourassumptions. Asstatedbefore,wewillconsiderthefollowingtwotypesofcosts:
(IP)
c(x,x′,y,y′)=c˜(cid:0) x,x′,⟨y,y′⟩(cid:1)
(N2) c(x,x′,y,y′)=c˜(cid:0) x,x′,|y−y′|2(cid:1)
where we make the following assumptions on the function c˜:X ×X ×R→R.
(A1) For every compact set K ⊂Rd, there is an unbounded increasing function
f :R+ →R such that x,x′ ∈K =⇒ c˜(x,x′,t)≥f (t)≥0
K K
(A2) For µ⊗µ-a.e. (x,x′), c˜(x,x′,t)=c˜(x′,x,t), ∀t∈R
(A3) For µ-a.e. x, t(cid:55)→c˜(x,x,t) has a unique minimizer at t=0
Furthermore, we make the following assumptions on the growth of the derivatives which are most clearly
stated in terms of c rather than c˜:
(A4) c is a C2 function in all its variables with derivative values satisfying |D2c|≤C(1+c)
(A5) For any M >0 there exists a δ >0 and non-negative, strictly increasing continuous functions
ψ ,ψ :R+ →R+ satisfying ψ (0)=ψ (0)=0 so that for any |x|<M, |x−x′|<δ
1 2 1 2
and for any y,y′ we have D2 c(x,x′,y,y′)<−ψ (|y−y′|2)+ψ (|x−x′|2), where here
yy′ 1 2
the inequality is meant in the sense of positive definite matrices.
Assumption (A1) ensures that c is nonnegative as well as provides coercivity. Assumptions (A3)-(A5) are
listed here for completeness, but are not used in the proofs of relaxed existence. The growth condition
(A4) will allow us to integrate derivatives in a meaningful way. This assumption on growth conditions of
derivatives of c naturally holds for polynomial costs. Assumptions (A3)&(A5) are intended for costs of
the form (N2) and are also widely applicable. One last assumption we list separately as it is stronger than
necessary but encompasses many relevant costs is
(A0) For µ⊗µ-a.e. (x,x′), t(cid:55)→c˜(x,x′,t) is strictly convex
11Method
c(cid:0) x,x′,y,y′(cid:1)
PCA
(cid:0) ⟨x,x′⟩−⟨y,y′⟩(cid:1)2
Kernel PCA
(cid:0) κ(x,x′)−⟨y,y′⟩(cid:1)2
q-MDS
(cid:0) |x−x′|2−|y−y′|2(cid:1)2
(cid:0) |x−x′|2−|y−y′|2(cid:1)2
q-Sammon
|x−x′|2
|x−x′|2
Elastic Embeddings
|y−y′|2e−
2σ2
+β|x−x′|2e−|y−y′|2
Table1: AlistofseveralcostswhichfitintoourframeworkandsatisfyAssumptions(A1)-(A5); noticethat
the Elastic Embedding cost is one which does not satisfy assumption (A0) yet does satisfy (A3). The “q”
refers to quartic variants of standard costs used in dimension reduction.
As a final note, we mention that unless otherwise specified we will drop the tilde on the cost in the above
assumptions. For example, we will write c(x,x′,|y−y′|2) rather than c˜(x,x′,|y−y′|2). Provided below is a
table of several cost functions which can fit into our framework.
We now explicitly derive an upper bound which quantifies tightness under the assumption (A1). We
begin with the inner product case.
Theorem 2.8 (Inner Product Costs). Assume (IP) and (A1)&(A2) and that c is lower semi-continuous.
Let µ ∈ P(Rd) and suppose that inf J < +∞, where J is given by (1.6). Then there exists π ∈ Π(µ)
Π(µ)
such that J(π)=inf J.
Π(µ)
Proof. We consider a sequence π so that J(π )→inf J and
n n Π(µ)
J(π )≤2 inf J.
n
Π(µ)
Notice that if ⟨y,y′⟩ = 0 for ν ⊗ν-a.e. (y,y′), it must be that the support of ν is concentrated on the
singleton{0}, whichwouldtriviallygivetightnessofπ ; thuswithoutlossofgeneralitywemayassumethat
n
elements of the minimizing sequence have nontrivial support in y.
We now claim that the sequence π must be tight: the argument will essentially show that mass far
n
from the origin must be small in order for the previously displayed inequality to hold. As described in the
definition of tightness, it suffices to show that ν is tight. To begin, we let ε > 0 and partition Rm into a
n
finitenumberofdisjointconesC ,...,C whereintheanglebetweenanytwopointsisatmostπ/6anddenote
1 ℓ
C =C ∩Bc(0)fori=1,...,ℓ. LetK ⊂Rd beacompactsetsuchthatµ(Kc)< ε. Bythenon-negativity
i,r i r ε ε 2
of c which follows from (A1), we have
ℓ (cid:90)(cid:90)
(cid:88)
J(π )≥ cdπ dπ ,
n n n
i=1
(Kε×Ci,r)2
which, by assumption (A1), yields
ℓ (cid:90)(cid:90)
(cid:88)
J(π )≥ f ◦|⟨·,·⟩|dπ dπ .
n Kε n n
i=1
(Kε×Ci,r)2
Finally, by the construction of our cones, we have that y,y′ ∈C =⇒ |⟨y,y′⟩|≥|y||y′|/2, and hence
i
J(π )≥f
(r2/2)(cid:88)ℓ
(cid:0)
π (K ×C
)(cid:1)2
≥f
(r2/2)(cid:0) π n(K ε×B rc(0))(cid:1)2
.
n Kε n ε i,r Kε ℓ
i=1
12The second inequality follows by Jensen’s inequality and by virtue of C ,...,C forming a partition. The
1 ℓ
above considerations hence imply for every element of the minimizing sequence, one has
(cid:115)
2ℓinf ε
π (Rd×Bc(0))=ν (Bc(0))≤ Π(µ)J + .
n r n r f (r2/2) 2
Kε
By then making r sufficiently large we can make the right hand side smaller than ε, which shows that the
ν , and subsequently the π , are tight. Prokhorov’s Theorem gives a subsequence with a weak limit π, and
n n
π is a relaxed minimizer by the weak lower semi-continuity of J, as argued above.
The same argument, with only slight modifications to the geometry, provides the same result for the
norm-based costs.
Theorem 2.9 (Normed Costs). Assume (N2) and (A1)&(A2) and that c is lower semi-continuous. Let
µ∈P(Rd) and suppose that inf J <+∞, where J is given by (1.6). Then there exists π ∈Π(µ) such
Π(µ)
that J(π)=inf J.
Π(µ)
Proof. The main difference in the proof is that one should replace cones, which have aligned inner products,
with pairs of halfspaces which are well-separated, and hence have lower bounds on pairwise distances.
Specifically,let{π }∞ satisfyJ(π )→inf J andJ(π )≤2inf J. Sincethecostistranslation
n n=1 n Π(µ) n Π(µ)
invariantiny,withoutlossofgenerality,wemayassumethateachelementinthissequencehastheproperty
that for any k ∈1...m we have π (Rd×H+)=π (Rd×H−)=1/2 where H+ :={y ∈Rm :y >0} and
n k n k k k
H− := {y ∈ Rm : y ≤ 0}. We also write H+ = {y ∈ Rm : y > r}. As before, take K ⊂ Rd to be a
k k k,r k ε
compact set for which µ(Kc)< ε , and let ε<1/2. By the non-negativity of c, one has for any k ∈1...m
ε 4m
(cid:90)(cid:90)
J(π )≥ cdπ dπ .
n n n
(Kε×H k+ ,r)×(Kε×H k−)
By again using the bound (A1), the monotonicity and unboundedness of f , and the fact that (y,y′) ∈
Kε
H+ ×H− =⇒ |y−y′|2 >r2, then gives, for r sufficiently large,
k,r k
J(π ) (cid:16) ε (cid:17)(cid:18) 1 ε (cid:19)
n ≥π (K ×H+ )π (K ×H−)≥ ν (H+ )− −
f (r2) n ε k,r n ε k n k,r 2m 2 2m
Kε
and in turn, rearranging, summing over k, and using the fact that ε<1/2, we obtain
8inf J ε
ν (∪m H+ )≤m Π(µ) + .
n k=1 k,r f (r2) 4
Kε
By repeating the argument for the halfspaces where y <−r, we then obtain
k
16minf J ε
Π(µ)
ν ({|y| >r)≤ + ,
n ∞ f (r2) 2
Kε
and by taking r sufficiently large we can then bound ν ({|y| > r) ≤ ε. This proves tightness of the ν ,
n ∞ n
which in turn proves, up to a subsequence, existence of a weak limit π which must be a minimizer.
3 The Marginal Problem
Asdiscussedintheintroduction, manyofthestandardtoolsforexistenceoftransportationmapsinoptimal
transportationfailinthepresentcontextduetoalackofconvexityinπoftherelaxedproblem. Inparticular,
the effects of replacing an embedding plan π with π+ γγ (such that π+ γγ ∈Π(µ)) are realized as first and
second-order terms in γγ. More precisely, if γγ is a signed measure on X ×Y such that for all d-dimensional
Borel sets A, γγ(A×Rm)=0, one has
µ(A)=π(A×Rm)=[π+ γγ](A×Rm),
13so that adding γγ leaves the X-marginal invariant. With this notation along with the symmetry assumption
in (A2), one can succinctly express the change in energy due to the perturbation γγ:
(cid:90)(cid:90) (cid:90)(cid:90)
J(π+ γγ)−J(π)=2 cdπdγγ+ cdγγdγγ,. (3.1)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:J(γγ|π) =:J(γγ)
Here, γγ (cid:55)→ J(γγ|π) encapsulates the linear contribution while γγ (cid:55)→ J(γγ) represents the quadratic contri-
bution. Further developing this notation, we remark that the J(γγ|π) encodes the fact that the first-order
affect should be thought of as a linear programming problem over (x,y) (cid:55)→ (cid:82) c(x,x′,y,y′)π(dx′dy′) for a
fixed embedding plan π. Denoting this map as J (y|x), we see that the first-order problem can be formally
π
stated: for any fixed π˜ ∈P(Rd×Rm), find π such that
(cid:90)
π ∈argmin J (y|x)π(dxdy)
π˜
Π(µ)
As we are free to vary the Y-marginal of π, the above formulation strongly suggests that if π(dxdy) =
ν(dy|x)µ(dx) is optimal, then the support of ν(·|x) is concentrated on the minimizers of J (·|x). This turns
π
out to indeed be the case, but before validating the claim, we give a definition to streamline the proceeding
discussion.
Definition 3.1. Given a continuous cost c of type (IP) or (N2) which satisfies assumptions (A1)&(A2)
and a embedding plan π ∈Π(µ), we define the marginal problem of J(π) by the function
(cid:90)
J (y|x):= c(x,x′,y,y′)π(dx′dy′). (3.2)
π
Furthermore, for the set valued map λ:x(cid:55)→argminJ (·|x), we call the set of all pairs (x,λ(x)) the minimal
π
graph of J and denote it with the symbol Λ .
π π
Notice that the chosen convention is that calligraphic letters are reserved to functional problems while
standard capital letters denote functions on finite dimensional spaces. We also remark that when c is
continuous, With this definition in place, we now present the following theorem.
Theorem 3.2 (Marginal Minimization). Suppose that c is a continuous cost of type (IP) or (N2) and
satisfies assumptions (A1)&(A2). If π ∈Π(µ) is a minimizer of (1.6), then the support of π is concentrated
on the minimal graph of J . In other words, π must satisfy the implicit relation
π
π(Λ )=1. (3.3)
π
From a high level, the theorem tells us that the variational problem (1.6) may be transformed into a
finite dimensional one; that of minimizing J (·|x) for every given x (which implicitly depends on π). This
π
is analogous to the situation in optimal control wherein a value function is found by solving a PDE which
implicitly depends on the control u. Once this value function is found, one may pointwise minimize a (finite
dimensional) Hamiltonian to find the optimal control.
Continuingtheanalogywithcontrol,noticethatintheabsenceofaconvexityassumptiononc,smoothly
varying π is likely prone to get ‘stuck’ in local minima. To this end, the proof of the theorem uses local-
ized perturbations in X which transport probability mass in Y across potentially large distances. These
perturbations are analogous to needle variations used in the proof of the Pontryagin Maximum Principle.
(cid:80)
We now illustrate the proof idea in the discrete case. To this end, suppose µ = (1/n) δ and
π = (1/n)(cid:80) π δ where y ,y ,...,y ∈ Rm constitute the optimal solution to (1.6);
eachi πxi
tells
ij ij (xi,yj) 1 2 n ij
what proportion of the 1/n mass at point x will go to location y . Suppose that y ̸∈ λ(x ) for some pair
i j j i
(x i,y j) with π
ij
>0. Define a perturbation γγ which sends y
j
to y˜
j
∈λ(x i), that is
π (cid:16) (cid:17)
γγ = nij δ (xi,y˜j)−δ
(xi,yj)
14and let π˜ =π+ γγ. Computing first the effect on the linear term, J(γγ|π) we have
1(cid:104) (cid:105)
J(γγ|π)=
n
J π(y˜ j|x i)−J π(y j|x i) <0,
by marginal minimality of y˜ . Further, we have
j
(cid:16) (cid:17)
γγ⊗ γγ =(1/n2) δ (xi,y˜j)⊗δ (xi,y˜j)−δ (xi,y˜j)⊗δ (xi,yj)−δ (xi,yj)⊗δ (xi,y˜j)+δ (xi,yj)⊗δ
(xi,yj)
,
andhenceJ(γγ)=1/n2(cid:2)
c(x i,x i,y j,y j)−2c(x i,x i,y j,y˜ j)+c(x i,x i,y˜ j,y˜
j)(cid:3)
whichisclearlydominatedbythe
lineartermwhennislargeenough. ThusbyEquation(3.1),J(π+γγ)<J(π)andweobtainacontradiction
to the optimality of π. Extending this idea to the continuum case only requires a direct, measure-theoretic
argument.
Proof. Let π be an optimal solution of (1.6) and suppose for sake of contradiction that π(Λc) > 0. By
π
defining
A =(cid:8) (x,y):k−1 <J (y|x)−minJ (·|x)(cid:9) ∩{(x,y):|x|,|y|<r},
k,r π π
it follows that Λc =(cid:83)∞ A and consequentially, π(A )>0 for some (k,r)∈N2. Define the measure
π k,r=1 k,r k,r
π = π|Ak,r andtakeλ˜asameasurableselectionofλ. Thisselectionexistsbythecontinuityofthemarginal
k,r π(Ak,r)
problem, J , which follows by the continuity of c3. Choose ε < min{2π(A ),(k∥c∥ )−1} to
π k,r L∞(Ak,r×Ak,r)
construct the perturbation
(cid:18) (cid:19)
ε ν(A |x)
γγ =
2
π(Ak,r
)
·δ λ˜(x)⊗µ−π
k,r
.
k,r
(cid:82)
where we have used the representation π(A ) = ν(A |x)µ(dx). By the first restriction on ε, it follows
k,r k,r
that π+ γγ is a positive measure. Furthermore we can see that this perturbation does not affect the input
marginal, that is γγ(P ×Rm)=0 for all P ∈B(Rd).
Tracking the effects of this perturbation, the linear term becomes:
ε (cid:90) ε (cid:90)
J(γγ|π)=
2π(A )
J π(y|x)δ λ˜(x)(dy)ν(A k,r|x)µ(dx)−
2
J π(y|x)π k,r(dxdy)
k,r
<
ε (cid:90)
minJ (·|x)ν(A |x)µ(dx)−
ε (cid:90) (cid:0)
minJ
(·|x)+k−1(cid:1)ν(A k,r|x)µ(dx)
2π(A ) π k,r 2 π π(A )
k,r k,r
ε
=−
2k
where on the second to last line we make use of the lack of dependence on y in the latter integrand. As c is
nonnegative, we have the following estimate for the quadratic term:
ε2 (cid:90)(cid:90)
J(γγ)≤
4
c(x,x′,λ(x),λ(x′))ν(A k,r|x)µ(dx)ν(A k,r|x′)µ(dx′)
ε2 (cid:90)(cid:90)
+ c(x,x′,y,y′)π (dxdy)π (dx′dy′)
4 k,r k,r
≤ε2·∥c∥ .
L∞(Ak,r×Ak,r)
Putting the estimates together with (3.1), one has
ε
J(π+ γγ)−J(π)<−
k
+ε2·∥c∥
L∞(Ak,r×Ak,r)
which is negative by our choice of ε. This is a contradiction to optimality.
3TheexistenceofaminimizingmeasurableselectionofJπ followsfromatheoremofRockafeller(see14.37in[25])assoon
asJπ isaCarath´eodoryfunction.
15Remark3.3. Intheproofpresentedabovewenoticethattransportingε-massto(global)marginalminimizers
incurs a gain on the embedding cost regardless of whether or not π is optimal. This is quite different in
philosophy from the standard computational approaches which conduct particle-wise gradient descent in Y.
As evidenced by Example 1.1, particle-wise decent potentially gets caught in local minima of the marginal
problem. These local minima can lead to highly oscillatory embeddings: in the language of this work this
corresponds to probabilistic couplings.
A different way of casting this observation is that if we are only allowed to perturb a coupling π smoothly
in y then there may be local minimizers of J which are probabilistic in Y. However we shall see in Section
4 that probabilistic couplings are never optimal in our dimension reduction problems. This suggests the need
for improved computational algorithms which are capable of executing perturbations which are not smooth in
Y.
3.1 Critical point equation
In light of Theorem 3.2, it is natural to consider the necessary conditions for optimality in y of the marginal
problem, and the constraints that they impose upon the optimal solution π. To begin, we consider assump-
tions under which the marginal problem, which depends implicitly upon the measure π, is differentiable.
Lemma 3.4. Let the cost function c be of type (IP) or (N2) and satisfy assumptions (A1),(A2), and (A4).
Let π be a minimizer of (1.6). Then the funtion J is C2 in x,y.
π
Proof. Formally differentiating we should have the formula
(cid:90)
D2J (y|x)= D2c(x,x′,y,y′)π(dx′dy′).
π
However, by (A4), we can write
(cid:90)(cid:90)
|D2c|dπdπ ≤C(1+J(π))<∞.
This in turn implies that (cid:82) D2c(x,x′,y,y′)π(dx′dy′) is integrable (with respect to π), in x,y. A dominated
convergence argument, along with continuity of the derivatives, then gives that J is C2 in x,y.
π
We notice that a necessary condition for optimality is that Sptπ must be concentrated on solutions to
the nonlinear integral equation in X ×Y
(cid:90)
D J (y|x)= D c(x,x′,y,y′)π(dx′dy′)=0. (3.4)
y π y
As the goal is to establish that y is deterministically given by x, if y (cid:55)→ D J (y|x) were injective then for
y π
every given x the unique solution to D J (y|x)=0 would specify y. However, we do not expect this to be
y π
the case in general (see Example 3.7). This stands in contrast to the situation in optimal transportation
whereinD c(x,·)isassumedinjective(sometimescalledthetwistcondition)renderingtheequationDψ(x)+
x
D c(x,y) = 0 to be a prescription of y given x. Notice how the presence of the Kantorivich potential ψ
x
somehowencodestheadditionalmarginalconstraintwhichispresentinOT;intheabsenceofthisconstraint
in the dimension reduction problem, it is unsurprising there is no analogous term in (3.4).
In special cases, it can happen that the marginal problem is strictly convex as a function of y. We begin
with a simple example in the context of classical dimension reduction algorithms.
Example 3.5. Let
c(x,x′,y,y′)=(cid:0) ⟨x,x′⟩−⟨y,y′⟩(cid:1)2
. Then the marginal problem takes the form
(cid:20)(cid:90) (cid:21) (cid:20)(cid:90) (cid:21) (cid:20)(cid:90) (cid:21)
J (y|x)=xT x′x′T µ(dx′) x−2xT x′y′T π(dx′dy′) y+yT y′y′T ν(dy′) y.
π
Clearly, y (cid:55)→J (y|x) is convex and thus D J (y|x)=0 will determine y given x. Writing the critical point
π y π
equation, we see
(cid:20)(cid:90) (cid:21) (cid:20)(cid:90) (cid:21)
y′x′T π(dx′dy′) x= y′y′T ν(dy′) y, (3.5)
16indicating that the optimal map is linear, meaning y = Ax. If we utilize the singular value decomposition
A=UΣVT, we can rewrite the original optimization problem as
(cid:90)(cid:90) (cid:90)(cid:90)
J(π)=J(A)= (xTx′−xTVΣTΣVTx′)2µ(dx)µ(dx′)= (xTVT(I−ΣTΣ)Vx′)2µ(dx)µ(dx′).
This is equivalent, for centered µ, to principal component analysis.
Building upon this example, we can give the following simple corollary to Theorem 3.2.
Corollary 3.6. Suppose that c is of type (IP) and satisfies Assumptions (A0)-(A2) and (A4). Then any
optimal solution of (1.6) is supported on the graph of a function, whose smoothness is controlled by the
differentiability of t(cid:55)→c˜(x,x′,t).
Proof. We notice that the marginal problem (3.2) is represented as an integral of c integrated against a
measure on (x′,y′). Since y (cid:55)→⟨y,y′⟩ is a linear function, and t(cid:55)→c(x,x′,t) is strictly convex, we have that
the marginal problem is strictly convex on the smallest subspace supported by ν. If that subspace is Rm
then we immediately have that the marginal problem is strictly convex for almost every x, and hence has a
unique minimizer for almost every x. In turn, the map λ is actually a function, and π is supported on the
graph of that function. If the smallest subspace containing the support of ν is of dimension k < m, then
by using rotational invariance we could instead consider the problem of embedding to Rk, and the same
argumentasabovegivesthatthesolutionmustbeinducedbyamap. Finally,thesmoothnessoftheoptimal
map may be recovered by noting that solutions must solve the critical point equation D J = 0 and using
y π
the implicit function theorem.
This corollary resolves the necessity of optimal solutions to be mappings in many natural contexts,
specifically costs which are convex in ⟨y,y′⟩. Such costs include classical multi-dimensional scaling and
kernel principle component analysis. However, many of the standard costs used in dimension reduction are
non-convex in y, and have marginal problems with more complicated structure in their minimizers. We
returntoourrunningexamplewhichdemonstratesthatthemarginalproblemcanhavemultipleminimizers.
Example 3.7. In the case of c(x,x′,y,y′) = (cid:0) |x−x′|2−|y−y′|2(cid:1)2 , one has a rather explicit formula for
the marginal problem:
J (y|x)=|y|4−2yTψ (x)y−4φ (x)Ty+ζ (x). (3.6)
π π π π
The coefficients of this polynomial equation are implicitly defined by moments of the joint distribution, in
particular:
(cid:90) (cid:104) (cid:105)
ψ (x)=Id |x|2− 2y′y′T +(|y′|2−|x′|2)Id π(dx′dy′)
π m m
(cid:18)(cid:90) (cid:19) (cid:90)
φ (x)=2 y′x′T π(dx′dy′) x+ y′(|y′|2−|x′|2)π(dx′dy′)
π
(cid:124) (cid:123)(cid:122) (cid:125)
=:Φπ
(cid:18)(cid:90) (cid:19) (cid:18)(cid:90) (cid:19)
ζ (x)=|x|4+4xT x′x′T µ(dx′) x−2|x|2 (cid:0) |y′|2−|x′|2(cid:1) π(dx′dy′)
π
(cid:18)(cid:90) (cid:19) (cid:90)
+4 (cid:0) |y′|2−|x′|2(cid:1) x′T π(dx′dy′) x+ (cid:0) |y′|2−|x′|2(cid:1)2 π(dx′dy′)
where we have assumed the distribution in Rm has mean zero by using translation invariance. We notice
that the matrices ψ ,φ , and ζ , which are completely determined by moments of π, give a parametric
π π π
representation for the marginal problem, just as A did in the inner product case from the previous example.
We believe that this parametric representation should be useful for many unsupervised learning tasks, as it
will directly give properties such as statistical consistency and direct extrapolation. Furthermore, it should
facilitate more efficient computational algorithms that work in parameter space: this is the subject of current
work.
17Let η ,...,η be an orthogonal basis for which
1 m
(cid:88)m η ηT =(cid:90) (cid:104) 2yyT +(cid:0) |y|2−|x|2(cid:1) Id (cid:105) π(dxdy)
j j m
j=1
so that
m
(cid:88)
ψ (x)=|x|2Id − η ηT (3.7)
π m j j
j=1
. For simplicity, assume that |η | < |η | < ··· < |η |. Evaluating the marginal problem along the lines
1 2 m
r (t)=t ηi one finds
i |ηi|
d J (r (t)|x)=t3−(cid:0) |x|2−|η |2(cid:1) t− φT π(x)η i,
dt π i i |η |
i
which can have multiple solutions along r (t) provided |x|>|η |.
i i
This alone is not necessarily a problem under Theorem 3.2 in that the marginal problem may have several
critical points, but as long as there is a unique global minimizer we may still guarantee existence of non-
probabilistic solutions for dimension reduction problem. This said, consider the set {x|φ (x) = 0} where
π
the critical point equation can be expressed as
m
(cid:16) (cid:17) (cid:88)
|y|2−|x|2 y+ η ηTy =0.
j j
j=1
One may readily check that the solutions to the above equation are exhausted by y = ± ηj (cid:112) |x|2−|η |2 for
|ηj| j
j =1,...,m and y =0. The previous observations imply that the former case is only possible when |x|>|η |
j
which makes the square root well defined. Plugging in each of these critical points into the marginal problem,
we find that
(cid:18) (cid:113) (cid:12) (cid:19)
J ± ηj |x|2−|η |2(cid:12)x =−(cid:0) |x|2−|η |2(cid:1)2 +ζ (x)≥−(cid:0) |x|2−|η |2(cid:1)2 +ζ (x).
π |ηj| j (cid:12) j π i π
where i is the largest index for which |x| ≤ |η |. Hence for {x|φ (x) = 0, |η | < |x| ≤ |η |}, there are
i+1 π i i+1
two minimizers to the marginal problem: ± ηi (cid:112) |x|2−|η |2. The case devolves further if |η | is repeated
|ηi| i j
(|η | < ··· < |η | = ··· = |η | < ··· < |η |) and |x| ≤ |η | where any y on the k-sphere spanned by
1 j j+k−1 m j+1
η ,...,η is a minimizer of J (·|x).
j j+k−1 π
Thepreviousexampleismeanttodemonstratehowpathologicalthenatureofthemarginalminimization
problem can be: for simple costs, the marginal minimizers may be comprised of entire sub-manifolds in Rm
for a single x! Inthepursuitofdeterministicminimizers(i.e. Monge-typemaps), oneapproachmightbeto
show that these multiple minimizers can only happen on a thin set (in the above example this corresponds
to showing that φ (x) is full rank µ(dx)-a.e.), but due to the implicit dependence of the marginal problem
π
on the embedding plan π, taking this route directly has proven particularly difficult.
Another notable consequence which can be observed from the marginal problem framework is that for
normedcosts,itwillbelikelythattherewillbejumpdiscontinuitiesarisingfromananalogousphenomenonto
thatofExample2.3. Thefollowingexampleshowsthatinthecaseofq-MDS,wecanguaranteediscontinuities
in the optimal solution. We can expect the argument below to persist for any dimension reduction problem
for which argminJ (·|x) has multiple values for some x, but this property is implicitly dependent on π as
π
well and thus challenging to verify in practice.
Example 3.8. Putting technicalities of the rank of D J (y|x) aside for the moment, Example 3.7 in the
y π
previous section argues that when none of the lengths of |η | are repeated, there are m+1 distinct regions
j
for which the marginal problem is defined by a different solution. More precisely, for A := {x|φ (x) =
i π
0,|η | < |x| ≤ |η |}, we have a semi-explicit (governed by moments of the optimal solution) formula for
i i+1
the reduction map: T(x)=± ηi (cid:112) |x|2−|η |2; when x passes from A to A the optimal solution abruptly
jumps from ± ηi (cid:112) |x|2−|η |2|η ti o| ± ηi+1 (cid:112) |i x|2−|η |2. i i+1
|ηi| i |ηi+1| i+1
18Beyond this, one also can observe that for any path x+εv with φ (x)=0 (and v not in the nullspace of
π
Φ ) the marginal minimizer has a jump discontinuity at ε=0. The intuition here will come from Example
π
2.3. Indeed, the previous considerations have implied that there will be multiple minimizers when φ(x)=0,
namely ± ηi (cid:112) |x|2−|η |2 where i is the smallest index such that |x| > |η |. By plugging ± ηi (cid:112) |x|2−|η |2
|ηi| i i |ηi| i
into the marginal problem (3.6) at x+εv. We see by (3.7),
J π(cid:0) ± |ηη ii |(cid:112) |x|2−|η i|2(cid:12) (cid:12)x+εv(cid:1) =(cid:0) |x|2−|η i|2(cid:1)2
m
−2 |η ηiT i|(cid:112) |x|2−|η i|2(cid:16) |x+εv|2Id m−(cid:88) η jη jT(cid:17) |η ηi i|(cid:112) |x|2−|η i|2
j=1
±4φT(x+εv) η i (cid:112) |x|2−|η |2+ζ (x+εv)
π |η | i π
i
=(cid:0) |x|2−|η |2(cid:1)2 −2(cid:16) |x|2−|η |2(cid:17)(cid:16) |x+εv|2−|η |2(cid:17)
i i i
±8εvTΦT η i (cid:112) |x|2−|η |2+ζ (x+εv)
π|η | i π
i
where on the last line we have used the fact that φ (x+εv) = 2εΦ v. Crucially, we see that in order for
π π
the above expression to be minimal, one needs to choose the sign of the order ε term to be opposite that of
vTΦTη . In particular, this shows that near a point x for which φ (x)=0, the optimal map is
π i π
T(x+εv)=−sign(εvTΦTη ) ηi (cid:112) |x|2−|η |2+O(ε)
π i |ηi| i
whose limit does not exist at ε=0.
Now having seen the possibility of multiple minimizers to the marginal problem and how it can cause
discontinuities, we illustrate one more useful perspective in the context of dimension reduction. Being that
dimension reductionschemesinherently discard informationwhile representing datain the embeddedspace,
there must be some partition of X such that each element of the partition may be represented by a single
value in the embedding. More precisely, for the map outlined in Definition 3.1, the set {x : λ(x) = y}
represents all of the points in X which are optimally embedded to the vector y. While these sets can be
arbitrary, we expect them to form d−m dimensional manifolds. To illustrate this, we present one more
example.
Example 3.9. Let us consider a simple example where 1000 datapoints in R2 are such that 500 points are
stacked at (0,1) and the other 500 are stacked at (0,−1). The optimal embedding for the q-MDS cost into
R is clearly realized by projecting the 2 dimensional dataset onto the y-axis. This allows us to explicitly
compute
ψ (x ,x )=x2+x2−2, φ (x ,x )=2x
π 1 2 1 2 π 1 2 2
thus the critical point equation can be written y3−(x2+x2−2)y =2x . Imitating the previous computations,
√ 1 2 2 √
we first notice that when |x| < 2, ψ(x ,x ) < 0. This implies that on disk of radius 2, the marginal
1 2 √
problem (3.6) has a unique solution. Indeed for x in the set {x:|x|< 2},
d2
J (y|x)=12y2−4ψ (x)>0.
dy2 π π
√
Furthermore, if |x|≥ 2, there are be multiple minimizers along the set {x:φ (x)=0} which will lead to a
π
jump discontinuity as predicted in Example 3.8. The figure below illustrates the level sets of the minimizers,
λ :R2 →R.
π
4 Normed costs: maps via second-order conditions
In this section, we show that for a wide range of normed costs the solution of the dimension reduction
problem 1.6 is induced by a map. The main difficulty is that the dimension reduction problem of type (N2)
19Figure 3: Each band represents an equivalence class of points in R2 which all have the same minimizer in R
√
for the embedding outlined in Example 3.9. Notice that once |x| > 2, the line x = 0 has a discontinuity
2
surface.
is not marginally convex in y (i.e. y (cid:55)→J (y|x) is not convex) and thus we expect multiple minimizers to a
π
givenmarginalproblem(takeforinstanceExample1.1). Thisfollowssincey (cid:55)→c(x,x′,|y−y′|2)neednotbe
convex even when the function t (cid:55)→ c(x,x′,t) is convex. To surmount this, we track the second-order effect
of perturbations, on the level of the dimension reduction plans. We shall see that certain natural structural
conditions upon c (namely assumptions (A3)-(A5)) will then be sufficient to guarantee that optimal plans
are induced by maps.
As in Section 3, we motivate our proofs by first formally considering the case where the input distri-
bution is realized as a sum of Dirac masses, µ =
(1/n)(cid:80)n
δ , for some collection of distinct points
i=1 xi
x ,x ,...,x ∈ Rd. As in Section 3, we assume that the optimal embedding may be represented discretely
1 2 n
by π = (1/n)(cid:80) π δ for some distinct collection of vectors y ,y ,...,y ∈ Rm. Suppose that in the
ij ij (xi,yj) 1 2 n
ith row of π there are at least two nonzero entries and reorder the y’s so that π ,π > 0; this essentially
ii ij
encodes the situation where an optimal embedding maps a single x to multiple y’s.
By Theorem 3.2, both y ,y ∈ argminJ (·|x ) and thus we can transport the mass stored at (x ,y ) to
i j π i i j
(x ,y ) without violating our first-order condition. More precisely, if π >0 and π >0, the perturbation
i i ii ij
(cid:2) (cid:3)
γγ =min{π ii,π ij} δ (xi,yi)−δ
(xi,yj)
,
iswell-definedandwillhaveJ(γγ|π)=0,meaningthatitwillleavetheenergyunchangeduptosecond-order
variations. When we compute the quadratic term, we have
γγ⊗ γγ =δ (xi,yi)⊗δ (xi,yi)−δ (xi,yi)⊗δ (xi,yj)−δ (xi,yj)⊗δ (xi,yi)+δ (xi,yj)δ
(xi,yj)
and thus
J(γγ)=c(x i,x i,|y i−y i|2)−2c(x i,x i,|y i−y j|2)+c(x i,x i,|y
j
−y j|2)
=2(cid:2)
c(x ,x ,0)−c(x ,x ,|y −y
|2)(cid:3)
.
i i i i i j
Crucially, if t(cid:55)→c˜(x,x,t) has a strict global minimum for t=0 for all x, then J(γγ)<0 ⇐⇒ y
i
̸=y j. This
impliesthatforeachxtheoptimalplanmustbesupportedonlyonasingley. Weremarkthatthisargument
worksforanytypeofdissimilaritykernel. Inspiteofthetechnicaldifficultiesengenderedbythelossoflower
semicontinuity, this discrete argument suggests a very strong result: that solutions to the relaxed problem
(1.6) with normed cost must be deterministic, and hence must solve the original problem (1.1). This is
quite surprising in light of the examples presented in the introduction suggesting that Young measures can
be encountered in practice. This is because the perturbations used in particle-based optimization methods
cannot carry out perturbations of the form γγ, and can get stuck in local mins with respect to particle-wise
descent.
20Toextendthisargumenttothecontinuumsetting,onemustbeabletorepresentsolutionstothemarginal
problemlocallyinaconsistentmanner. Inparticular,itwouldbeidealtoobtainmorestructureonthenature
of the marginally minimizing set-valued map λ as outlined in Definition 3.1. Leaving technical justification
aside for the moment, suppose that locally λ admits a countable representation, i.e. λ(x) =
(cid:83)∞
λ (x)
i=1 i
for a sequence of smooth functions λ . We then can leverage the discrete argument between pairs of these
i
functions through the following proposition.
Proposition 4.1. Let λ ,λ : B (x ) → Y be continuous functions with λ (x) ̸= λ (x) for all x ∈ B (x ).
1 2 δ 0 1 2 δ 0
Assume that c is a continuous cost of type (N2) and satisfies assumptions (A1)-(A3) and assume π be a
minimizerof (1.6). Letµ ,µ betheX-marginalmeasuresofπ restrictedtothesetsy =λ (x)andy =λ (x)
1 2 1 2
and x∈B (x ). Then µ is mutually singular to µ , or in symbols µ ⊥µ , meaning that they have disjoint
δ 0 1 2 1 2
supports.
Proof. Suppose, for the sake of contradiction, that µ and µ are not mutually singular. Then the measure
1 2
µ ∧µ =µ −(µ −µ )+isnotazeromeasure,andwemayselectapointx¯∈B (x )sothatµ ∧µ (B (x¯))>
1 2 1 1 2 δ 0 1 2 ε
0 for all ε>0 sufficiently small.
We then construct the perturbation, restricted to x∈B (x¯), via
ε
(cid:104) (cid:105)
γγ(dxdy)=µ 1∧µ 2(dx) δ λ1(x)(dy)−δ λ2(x)(dy) .
Byconstructionwehavethatπ+γγ isaprobabilitymeasureandretainsthesameX marginalasπ. Wealso
note that γγ is not the zero measure by choice of x¯.
As long as µ ,µ are non-trivial, then by Theorem 3.2 we know that λ ,λ must be minimizers of the
1 2 1 2
marginal problem on the support of µ ,µ . Using the notation from the proof of Theorem 3.2 we have that
1 2
(cid:90) (cid:90)
J(γγ|π)= J π(y|x)γγ(dxdy)= [minJ π(·|x)−minJ π(·|x)]µ 1∧µ 2(dx)=0.
Bε(x¯)
The overall change in the quadratic term is given by, after using (A2),
(cid:90)(cid:90)
J(γγ)= c(x,x′,|λ i(x)−λ i(x′)|2)µ 1∧µ 2(dx)µ 1∧µ 2(dx′)
Bε(x¯)×Bε(x¯)
(cid:90)(cid:90)
+ c(x,x′,|λ (x)−λ (x′)|2)µ ∧µ (dx)µ ∧µ (dx′)
j j 1 2 1 2
Bε(x¯)×Bε(x¯)
(cid:90)(cid:90)
−2 c(x,x′,|λ (x)−λ (x′)|2)µ ∧µ (dx)µ ∧µ (dx′).
i j 1 2 1 2
Bε(x¯)×Bε(x¯)
By using the continuity of c,λ ,λ , we then estimate
1 2
J(γγ)≤2µ 1∧µ 2(B ε(x¯))2(c(x¯,x¯,0)−c(x¯,x¯,|λ 1(x¯)−λ 2(x¯)|2)+η(ε)),
where η represents a local modulus of continuity and satisfies η(ε) → 0 as ε → 0. As λ (x¯) ̸= λ (x¯), and
1 2
c(x,x,t) is strictly minimized at t=0 by (A3), we obtain that J(γγ)<0, which contradicts the minimality
of π.
An induction argument then gives the following immediate corollary.
Corollary 4.2. Let λ : O → Y be continuous functions, where O are open sets, and i ∈ {1,...,∞}. Let
i i i
π be a minimizer of 1.6 for continuous cost satisfying (N2) and (A1)-(A3), and let π˜ be the restriction of
π to the union of the sets {(x,λ (x)):x∈O }. Then π˜ has support on the graph of a function.
i i
The previous proposition offers a direct application to global minimizers of the marginal problem which
have non-degenerate Hessian in y; namely those minimizers which are also strict local minimizers. We begin
by proving two brief lemmas based upon the implicit function theorem.
Lemma 4.3. Let π be a minimizer of 1.6 for cost satisfying (N2) and (A1)-(A4). Suppose that y ̸= y
1 2
are global minimizers of the marginal problem at x¯, which both satisfy D2 J (y |x¯)>0. Then there exists a
yy π i
δ > 0 and C1 functions λ : B (x¯) → B (y ), i = 1,2 so that λ (x) is the only strict local minimizer of the
i δ δ i i
marginal problem in B (y ).
δ i
21Proof. The minimality of y and y indicate that both D J (y |x¯) = 0 and D J (y |x¯) = 0. From the
1 2 y π 1 y π 2
strict non-degeneracy assumption on D2J , the implicit function theorem allows us to construct C1 maps
y π
λ : B (x¯) → B (y ) i = 1,2, which uniquely solve D J (λ (x)|x) = 0 on the respective neighborhoods in
i δ δ i y π i
the product space. We note that without loss of generality δ can be taken small enough to guarantee the
strict local minimality of λ and λ since J was C2.
1 2 π
Lemma 4.4. Assume that c is a cost of type (N2) and satisfies assumptions (A1)-(A4) and let π be a
minimizer of (1.6). Then for every x there exists at most a countable number of global minimizers of the
marginal problem which satisfy D2 J (y|x)>0.
yy π
Proof. First note that since c is C2 by (A4), it follows from Lemma 3.4 that the marginal problem is a
C2 function in y. Furthermore, by (A1) the minimizers of the marginal point at a point x must live in a
compactsetK ⊂Y. ConsiderthesetM ⊂K ofglobalminimizersofthemarginalproblematxsatisfying
x η x
|D2J (y|x)| ≥ η. We notice that M will also be compact. As J (y|x) is C2, each element of M can be
y π η π η
surrounded by a ball of some radius r >0 which contain no other point in M : this essentially says that a
η η
global minimizer with a lower bound on the Hessian is an isolated minimizer with a quantifiable distance of
isolation. AsM iscompact,wethenhavethatitactuallymustbefinite. Bytakingη tozero,thisargument
η
shows that the number of minimizers with non-degenerate Hessian must be at most countable.
WenowchoosetodecomposetheoptimalplanintopointswheretheHessianisnon-degenerate(i.e. rank
strictly less than m) and its complement via
π =π +π , π =π , π =π . (4.1)
S I S |det(D y2 yJπ)=0 I |det(D y2 yJπ)̸=0
In terms of this decomposition, we can use Corollary 4.2 along with Lemma 4.4 to immediately give the
following.
Proposition 4.5. Let π be a minimizer of (1.6), for c of type (N2) and satisfies assumptions (A1)-(A4).
Using the decomposition (4.1), then π is supported on the graph of a function.
I
The only remaining point is to rule out multivaluedness at points where the Hessian of the marginal
problem is degenerate. We address this issue completely in the following proposition.
Proposition 4.6. Assume c is a cost of type (N2) and satisfies assumptions (A1), (A2), and (A5). If π
is an optimal plan, then π is concentrated on the graph of a function.
S
Remark 4.7. In this theorem we notice that there are no requirements on the measure µ, nor on m,d.
Furthermore, we notice that in the statement we can say that π is induced by a map on the set where ρ >0,
s
and not just π . Hence any part of the support not covered by Proposition 4.6 will be covered by Proposition
S
4.5.
Proof. The main idea of the proof lies in tracking second variations along smooth perturbations of y. A
portion of these perturbations are chosen to be in directions where the marginal problem is, up to second-
order, degenerate, so that the y,y′ terms in the second-order Taylor expansion dominate.
Tobegin,letE :={(x,y):det(D2 J (y|x))=0}. Wethenchooseameasurablefunctionϕ :E →Sm−1
yy π 0
such that
D2 J (y|x)·ϕ (x,y)=0.
yy π 0
The existence of such a function can be justified using measurable selections of the multifunction encoding
the nullspace of D2 J (y|x), see for example [25].
yy π
We will consider a point x at which ρ > 0 in the sense of Lebesgue points. Select a unit vector v so
0 s
that
π (E∩B (x )×Y ∩{ϕ (x,y)·v ≥1/2})
liminf S δ 0 0 =:ρ>0.
δ→0+ π S(E∩B δ(x 0)×Y)
Such a vector must exist because we can cover the unit sphere with a finite number of cones with opening
angle 2π/3, and we have assumed that ρ (x )>0. For any choice of δ >0, we write
s 0
E :=E∩B (x )×Y ∩{ϕ (x,y)·v ≥cos(π/8)}
v δ 0 0
22√
we notice that for (x ,y ),(x ,y )∈E we have that ϕ(x ,y )·ϕ(x ,y )≥ 2.
1 1 2 2 v 1 1 2 2 2
We note that there exists K so that |D2 J (y|x)| < K for x ∈ B (x ) and y ∈ argminJ (·|x). Such
yy π δ 0 π
a K exists because of the C2 bounds on the marginal problem and the locally uniform compactness of the
minimizers of the marginal problem.
Now we define a function ϕ:X ×Y →Y by

ϕ (x,y) if (x,y)∈E
 0 v
ϕ(x,y):= βv if x∈B (x ) and (x,y)∈/ E ,
δ 0 v
0
otherwise,
where β > 0 is a parameter that we will select later. We now utilize this function ϕ to construct a one
parameter family of functions φε(y):=y+εϕ(x,y) and an associated family of plans π by writing
x ε
π (dxdy)=φε♯ν(dy|x)µ(dx)
ε x
where π(dxdy)=ν(dy|x)µ(dx) by disintegration. We then compute
(cid:90)(cid:90)
J(π )−J(π)= c(x,x′,y+φε(y),y′+φε (y′))−c(x,x′,y,y′)π(dxdy)π(dx′dy′).
ε x x′
Taylor expanding c we then obtain
(cid:90)(cid:90)
J(π )−J(π)= D c(x,x′,y,y′)εϕ(x,y)+D c(x,x′,y,y′)εϕ(x′,y′)+1/2ε2ϕT(x,y)D2 c(x,x′,y,y′)ϕ(x,y)
ε y y′ yy
+1/2ε2ϕT(x′,y′)D2 c(x,x′,y,y′)ϕ(x′,y′)+ε2ϕT(x,y)D2 c(x,x′,y,y′)ϕ(x′,y′)π(dxdy)π(dx′dy′)
y′y′ yy′
+O(ε3).
By using Fubini’s theorem along with Theorem 3.2 and Equation (3.4), we immediately have that the order
ε terms vanish.
The order ε2 terms take the form, after removing sets where ϕ = 0, using the fact that ϕ is in the
nullspace of D2 c on E , and assuming that β is sufficiently small
yy v
(cid:90)(cid:90)
ϕT(x,y)D2 c(x,x′,y,y′)ϕ(x′,y′)π(dxdy)π(dx′dy′)
yy′
Ev×Ev
(cid:90)(cid:90)
+ βvTD2 c(x,x′,y,y′)ϕ(x′,y′)π(dxdy)π(dx′dy′)
yy′
Ev×Bδ(x0)×Y\Ev
(cid:90)(cid:90)
+ β2vTD2 c(x,x′,y,y′)vπ(dxdy)π(dx′dy′)
yy
√Bδ(x0)×Y\Ev×Bδ(x0)×Y\Ev
2(cid:90)(cid:90)
≤ −ψ (|y−y′|2)+ψ (|x−x′|2)π(dxdy)π(dx′dy′)
2 1 2
Ev×Ev
(cid:90)(cid:90)
+βcos(π/8) −ψ (|y−y′|2)+ψ (|x−x′|2)π(dxdy)π(dx′dy′)+β2Kπ(B (x )×Y \E )2.
1 2 δ 0 v
Ev×Bδ(x0)×Y\Ev
By taking β sufficiently small, we can neglect the last term. Minimality implies that this entire quantity
mustby≥0,andhencebytakingδ →0andusingthefactthattheψ ,ψ arestrictlyincreasingandzeroat
1 2
zero immediately implies that ν must be given by a Dirac mass. This then implies that on the set where
x0
ρ >0 we have that π is supported on the graph of a function.
s
Togetherwehavenowprovenourmaintheorem, whichforsimplicityispresentedwithassumption(A0)
(which implies assumption (A3)) which is the requirement for normed costs) so to simultaneously include
normed and inner product costs.
Theorem 4.8 (Deterministic solutions a.k.a. Monge Maps). Let µ ∈ P(Rd) with cost structure either
(IP) or (N2) and satisfies assumptions (A0)-(A5). Then solutions to min J(π) are concentrated on
π∈Π(µ)
the graph of a function; i.e. there is a measurable T : Rd → Rm such that π(dy|x) = δ (dy), µ(dx)-
T(x)
almost everywhere. More succinctly, solutions to the dimension reduction problem exist and are necessarily
deterministic.
23It is the necessity of a deterministic solution which is surprising in view of Example 1.1 and compliments
the work of [8].
5 Conclusion
In this work we have examined theoretical properties of some fundamental dimension reduction algorithms.
In doing so, we have focused on the optimization problem and necessary conditions associated with popu-
lation level problems. We have shown that, for natural costs based upon similarities (i.e. inner products),
and dissimilarities (i.e. norm differences), that the dimension reduction problem must be minimized by a
deterministic mapping, and that any probabilistic behavior is necessarily sub-optimal.
On the other hand, the behavior that we observe in Example 1.1 raises many difficult questions. Clearly
local minimizers found using naive particle descent methods may exhibit probabilistic behavior, which is
consistent with the failure of lower-semicontinuity we proved in Proposition 2.6. On the level of practical
applicability, we find such probabilistic behavior highly problematic. For example, it could lead to very
misleadingclusteringindatavisualization,wheresimilarpointsinfeaturespaceareprobabilisticallyassigned
to distinct clusters.
These issues raise many natural follow up questions, a few of which we list here:
• Are the issues with probabilistic minimizers found via particle descent methods still present in real-
worlddatasets? Wehavenotpursuedthisissueherebecausecomprehensivelyaddressingthisquestion
calls for a detailed study across numerous benchmark data sets.
• What computational methods can be developed to avoid spurious probabilistic behavior in dimension
reduction, and how can the necessary conditions identified in this work be used to do so?
• If non-linear dimension reduction algorithms often induce discontinuous embeddings, how greatly can
they modify the topology of the data in feature space?
• Is similar behavior relevant in other unsupervised learning methods?
We hope that these questions help to stimulate a more detailed study of dimension reduction methods.
Acknowledgements
TheauthorsgratefullyacknowledgethesupportofNSFDMS2307971andtheSimonsFoundationMP-TSM.
AP also warmly thanks Peter McGrath and Erik Bates for many helpful discussions about early versions of
the work.
References
[1] Shreya Arya, Arnab Auddy, Ranthony Edmonds, Sunhyuk Lim, Facundo Memoli, and Daniel Packer.
“The Gromov-Wasserstein distance between spheres”. In: arXiv preprint (2024).
[2] Antonio Auffinger and Daniel Fletcher. “Equilibrium Distributions for t-distributed Stochastic Neigh-
bour Embedding”. In: arXiv preprint (2023).
[3] RobertBeinert,CosmasHeiss,andGabrieleSteidl.“OnAssignmentProblemsRelatedtoGromov–Wasserstein
Distances on the Real Line”. In: SIAM Journal on Imaging Sciences 16.2 (2023), pp. 1028–1032.
[4] Mikhail Belkin and Partha Niyogi. “Laplacian Eigenmaps for Dimensionality Reduction and Data
Representation”. In: Neural Computation 15.6 (2003), pp. 1373–1396.
[5] Jos´eCBellidoandCarlosMora-Corral.“Existencefornonlocalvariationalproblemsinperidynamics”.
In: SIAM Journal on Mathematical Analysis 46.1 (2014), pp. 890–916.
[6] Ingwer Borg and Patrick JF Groenen. Modern multidimensional scaling: Theory and applications.
Springer Science & Business Media, 2005.
24[7] Trevor Cox and Michael Cox. Multidimensional scaling. Chapman and Hall, 2001.
[8] Th´eo Dumont, Th´eo Lacombe, and Franc¸ois-Xavier Vialard. “On the Existence of Monge Maps for
the Gromov–Wasserstein Problem”. In: Foundations of Computational Mathematics (Feb. 2024).
[9] Peter Elbau. “Sequential lower semi-continuity of non-local functionals”. In: arXiv preprint (2011).
[10] IreneFonsecaandGiovanniLeoni.ModernMethodsintheCalculusofVariations:Lp Spaces.Springer,
Jan. 2007.
[11] Mikil D Foss, Petronela Radu, and Cory Wright. “Existing and Regularity of Minimizers for Nonlocal
Energy Functionals”. In: Differential and Integral Equations 31.11-12 (2018), pp. 807–832.
[12] Geoffrey E Hinton and Sam Roweis. “Stochastic neighbor embedding”. In: Advances in neural infor-
mation processing systems 15 (2002).
[13] Tjalling C. Koopmans and Martin Beckmann. “Assignment Problems and the Location of Economic
Activities”. In: Econometrica 25.1 (1957), pp. 53–76.
[14] J.B. Kruskal. “Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”. In:
Psychometrika 29.1 (1964), pp. 1–27.
[15] Jan de Leeuw and Patrick Mair. “Multidimensional Scaling Using Majorization: SMACOF in R”. In:
Journal of Statistical Software, Articles 31.3 (2009), pp. 1–30.
[16] Gongkai Li, Minh Tang, Nicolas Charon, and Carey Priebe. “Central limit theorems for classical
multidimensional scaling”. In: Electronic Journal of Statistics 14.1 (2020), pp. 2362 –2394.
[17] GeorgeC.Linderman,ManasRachh,JeremyG.Hoskins,StefanSteinerberger,andYuvalKluger.“Effi-
cientAlgorithmsfort-distributedStochasticNeighborhoodEmbedding”.In:arXivpreprint abs/1712.09005
(2017).
[18] LaurensVanderMaatenandGeoffreyHinton.“Visualizingdatausingt-SNE.”In:Journal of Machine
Learning Research 9.11 (2008).
[19] Leland McInnes, John Healy, and James Melville. “Umap: Uniform manifold approximation and pro-
jection for dimension reduction”. In: arXiv preprint (2018).
[20] Facundo M´emoli. “Gromov–Wasserstein distances and the metric approach to object matching”. In:
Foundations of computational mathematics 11 (2011), pp. 417–487.
[21] Luca Nenna and Brendan Pass. “Variational problems involving unequal dimensional optimal trans-
port”. In: Journal de Math´ematiques Pures et Appliqu´ees 139 (2020), pp. 83–108.
[22] Brendan Pass. “Multi-marginal optimal transport: theory and applications”. In: ESAIM: Mathemati-
cal Modelling and Numerical Analysis-Mod´elisation Math´ematique et Analyse Num´erique 49.6 (2015),
pp. 1771–1790.
[23] Pablo Pedregal. “Nonlocal variational principles”. In: Nonlinear Analysis: Theory, Methods & Appli-
cations 29.12 (1997), pp. 1379–1392.
[24] Gabriel Peyr´e, Marco Cuturi, and Justin Solomon. “Gromov-Wasserstein averaging of kernel and dis-
tance matrices”. In: International conference on machine learning. PMLR. 2016, pp. 2664–2672.
[25] R.T. Rockafellar, M. Wets, and R.J.B. Wets. Variational Analysis. Grundlehren der mathematischen
Wissenschaften. Springer Berlin Heidelberg, 2009.
[26] Sam T. Roweis and Lawrence K. Saul. “Nonlinear Dimensionality Reduction by Locally Linear Em-
bedding”. In: Science 290.5500 (2000), pp. 2323–2326.
[27] J.W. Sammon. “A Nonlinear Mapping for Data Structure Analysis”. In: IEEE Transactions on Com-
puters C-18.5 (1969), pp. 401–409.
[28] Meyer Scetbon, Gabriel Peyr´e, and Marco Cuturi. “Linear-Time Gromov Wasserstein Distances using
Low Rank Couplings and Costs”. In: Proceedings of Machine Learning Research. PMLR. 2023.
[29] Karl-Theodor Sturm. “The Space of Spaces: Curvature Bounds and Gradient Flows on the Space of
Metric Measure Spaces”. In: Memoirs of the American Mathematical Society 290 (Aug. 2012).
25[30] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. “A Global Geometric Framework for
Nonlinear Dimensionality Reduction”. In: Science 290.5500 (2000), pp. 2319–2323.
[31] VayerTitouan,R´emiFlamary,NicolasCourty,RomainTavenard,andLaetitiaChapel.“SlicedGromov-
Wasserstein”. In: Proceedings of Machine Learning Research 32 (2019).
[32] L.P.J. Van Der Maaten, E.O. Postma, and Jaap Van den Herik. “Dimensionality Reduction: A Com-
parative Review”. In: Journal of Machine Learning Research 10.66-71 (2009), p. 13.
[33] Titouan Vayer. “A contribution to Optimal Transport on incomparable spaces”. PhD thesis. Lorient,
2020.
[34] C.Villani.OptimalTransport:OldandNew.GrundlehrendermathematischenWissenschaften.Springer
Berlin Heidelberg, 2008.
[35] Zhirong Yang, Jaakko Peltonen, and Samuel Kaski. “Majorization-minimization for manifold embed-
ding”. In: Artificial Intelligence and Statistics. PMLR. 2015, pp. 1088–1097.
26