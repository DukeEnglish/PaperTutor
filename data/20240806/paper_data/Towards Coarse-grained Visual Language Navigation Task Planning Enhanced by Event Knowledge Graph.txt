Towards Coarse-grained Visual Language Navigation Task
Planning Enhanced by Event Knowledge Graph
KaichenZhao∗ YaoxianSong∗ HaiquanZhao
22210240394@m.fudan.edu.cn songyaoxian@zju.edu.cn 22210240393@m.fudan.edu.cn
ShanghaiKeyLaboratoryofData ZhejiangUniversity ShanghaiKeyLaboratoryofData
Science,SchoolofComputerScience, Hangzhou,China Science,SchoolofComputerScience,
FudanUniversity FudanUniversity
Shanghai,China Shanghai,China
HaoyuLiu TiefengLi ZhixuLi†
hyuliu20@gmail.com litiefeng@zju.edu.cn zhixuli@fudan.edu.cn
ResearchCenterforIntelligent ZhejiangUniversity ShanghaiKeyLaboratoryofData
Robotics,ZhejiangLab Hangzhou,China Science,SchoolofComputerScience,
Hangzhou,China FudanUniversity
Shanghai,China
ABSTRACT CCSCONCEPTS
Visuallanguagenavigation(VLN)isoneoftheimportantresearch •Computingmethodologies→Informationextraction.
inembodiedAI.Itaimstoenableanagenttounderstandthesur-
roundingenvironmentandcompletenavigationtasks.VLNinstruc- KEYWORDS
tionscouldbecategorizedintocoarse-grainedandfine-grained EventKnowledgeGraph,KnowledgeRetrieval,VisualLanguage
commands.Fine-grainedcommanddescribesawholetaskwith Navigation,Taskplanning,DynamicBacktracking
subtasksstep-by-step.Incontrast,coarse-grainedcommandgives
ACMReferenceFormat:
anabstracttaskdescription,whichmoresuiteshumanhabits.Most
KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixu
existingworkfocusesontheformerkindofinstructioninVLN
Li.2024.TowardsCoarse-grainedVisualLanguageNavigationTask
tasks,ignoringthelatterabstractinstructionsbelongingtodaily
PlanningEnhancedbyEventKnowledgeGraph.InProceedingsofthe
lifescenarios.Toovercometheabovechallengeinabstractinstruc- 33rdACMInternationalConferenceonInformationandKnowledgeManage-
tion,weattempttoconsidercoarse-grainedinstructioninVLN ment(CIKM’24),October21–25,2024,Boise,ID,USA.ACM,NewYork,NY,
byeventknowledgeenhancement.Specifically,wefirstpropose USA,11pages.https://doi.org/10.1145/3627673.3679711
aprompt-basedframeworktoextractaneventknowledgegraph
(namedVLN-EventKG)forVLNintegrallyovermultiplemain-
stream benchmark datasets. Through small and large language
modelcollaboration,werealizeknowledge-enhancednavigation
planning(namedEventNav)forVLNtaskswithcoarse-grained
instructioninput.Additionally,wedesignanoveldynamichistory
backtracking module to correct potential error action planning
inrealtime.Experimentalresultsinvariouspublicbenchmarks
showourknowledge-enhancedmethodhassuperiorityincoarse-
grained-instructionVLNusingourproposedVLN-EventKGwith
over5%improvementinsuccessrate.Ourprojectisavailableat
https://sites.google.com/view/vln-eventkg
∗EqualContribution.
†Correspondingauthor.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or Figure1:ForConventionalVLNtasks,fine-grainedtaskde-
republish,topostonserversortoredistributetolists,requirespriorspecificpermission scriptionsareprovidedtotheagent.However,inthereal
and/orafee.Requestpermissionsfrompermissions@acm.org.
world,humansoftenonlyprovidecoarse-grainedtaskde-
CIKM’24,October21–25,2024,Boise,ID,USA
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. scriptions
ACMISBN979-8-4007-0436-9/24/10
https://doi.org/10.1145/3627673.3679711
4202
guA
5
]RI.sc[
1v53520.8042:viXraCIKM’24,October21–25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
1 INTRODUCTION aVLN-specificeventknowledgegraph,usedtoenhanceLLM’s
Visuallanguagenavigation(VLN)task[1,50]aimstoenableagents planningcapabilitiesinVLNtasks.Inaddition,wealsoproposea
tounderstandthesurroundingenvironmentandaccomplishtasks dynamicbacktrackingmechanismtoalleviatetheaccumulationof
basedonhumaninstructions.Itisoneofthecrucialresearchareas errorsinthemodelduringtaskexecution.
forenablingAItointeractwiththerealphysicalworld. ToverifytheeffectivenessofourproposedmethodVLN-EventKG,
ExistingVLNmethods[14,31,33,38,40]facethechallengesin weevaluateourproposedevent-knowledge-enhancedmodelEvent-
coarse-grainedinstructionsunderstanding,inwhichoraclefine- NavonpublicR2R[2],REVERIE[37],andALFRED[40]datasets.
grainedsubtaskdescriptionsaresupplementedtoassistnaviga- Ablationstudiesaboutthescaleofeventknowledgeanddynamic
tionplanning.However,forreal-worldscenarios,instructionsin backtrackingmechanismareconductedtofurtherverifythesupe-
human-robotinteractionandnavigationareusuallyabstractand rioritiesofourproposedmethod.
coarse-grained.Forinstance,aseriesoffine-graineddetailedin- Insummary,thecontributionsofthispaperareasfollows:
structionsmakenavigationplanningeasyforanagent("First,go
upstairs,thenpassthesofa,enterthekitchen,andfinallystopin (1) Wefirstintroduceeventknowledgeintothesequentialdecision-
frontoftherefrigerator"),however,inrealworld,humansoften makingprocess(i.e.VLN),considerthecorrelationofevents
givecoarse-grainedinstructions:("gototherefrigerator"),asshown intheVLNscenario,andguidethemodeltomakecorrect
inFigure.1.Themainstreamapproaches[15,31,33]useatrans- taskplanning;
formerarchitecturemodeltosolvetheentireVLNtaskend-to-end. (2) WeproposeaneweventknowledgegraphforVLNtasks,
Thesemodelsarepre-trainedonlargeVLNdatasetsusingvarious whichextractsricheventknowledgeoverpublicVLNbench-
dataaugmentationstrategies.Extrafine-grainedoraclesubtaskde- marks,usedtohelpLLMperformmorereasonabletaskplan-
scriptionsareusedduringintrainingprocess,whichareproven ning;
toimproveperformancesignificantlycomparedtomodelstrained (3) Weproposeanoveldynamicbacktrackingmechanism,al-
withoutusingthat[40]. lowingthemodeltoassesswhetherthecurrenttaskcan
Recently,withtherapiddevelopmentoflargelanguagemodels, besuccessfullyexecutedinrealtimeandbacktrackwhen
severalworks[18,30]haveexploredtouseLLM(LargeLanguage necessary;
Models)fortaskplanning,aimingtoexploitcoarse-grainedinstruc- (4) Experimental results on R2R [2], REVERIE [37], and AL-
tionstopredictfine-grainedinstructions.However,commonsense FRED [40] show that under task settings that only pro-
knowledgeinsidethegeneralLLMisnotaccuratelyapplicableto videcoarse-grainedtaskdescriptions,ourEventNavoutper-
thespecificVLNdomain,whichalsocouldbringunnecessarynoise formsthebestexistingmodelsbymorethan5%averagely
knowledgeleadingtobadperformance[7,19]ontaskplanningin insuccessrate.
VLNdatasets.Furthermore,mostofthemfocusonthewholeper-
formanceachievementmissingcoarse-grainedtaskunderstanding
andtaskdecompositionmodelingatthesymboliclevel.
2 RELATEDWORK
Tomakeupforthedeficiencyofabstractandcoarse-grainedun-
derstandinginVLN,weattempttoinvestigateVLNplanningusing VisualLanguageNavigationInrecentyears,thevisuallanguage
onlycoarse-grainedtaskinstructions.ComparedtoexistingVLNre- navigationtaskhasattractedwidespreadattentionfromresearchers.
search,twokeypointsareconsidered:1)Coarse-grainedinstruc- Thistask[1,50]aimstobuildanintelligentagentthatcanactin
tionunderstandingNooraclefine-grainedinstructionsareused athree-dimensionalenvironmentandfollowhumaninstructions.
inthemodelingstageofVLNplanner,whichrequiresthemodel Buildingasystemthatunderstandsandexecuteshumaninstruc-
tounderstandandreasontasksfromabstractandcoarse-grained tionshasbeenthesubjectofmuchpreviouswork.Theseinstruction
instructions.2)LLM-KGjointVLNmodelLLMandstructured typesinclude,butarenotlimitedto,structuredcommandsorlogic
externalknowledgegraphareutilizedcomplementarily,whichis programs[34,35],naturallanguage[4,46],images[24],oramix-
desiredtoobtainthereasoningabilityofLLMandVLN-domain tureofmodalities[25].Theseeffortsfocusonmappingthecontext
knowledgefromtheexternalknowledgegrapheliminatingout-of- ofinstructionsandstructuralwordsontofinalactions.However,
domainnoisyinformationfromgeneralLLM. intherealworld,agentsneedtobeabletoprocessrawsensory
Forcoarse-grainedinstructionunderstanding,weproposean input.Therefore,thevisuallanguagenavigationtaskintroduces
eventknowledgegraphforVLNtasksnamedVLN-EventKG.Most richunstructuredvisualcontexttoinformtheagent’sexploration,
existingVLNbenchmarksareusedasdatasourcestoextractevent- perceptionandexecution[1,5,20].
levelknowledgeofsequentialdecisionsforVLNplanning.Specifi- ConventionalMethodforVLNAlgorithmsinvisuallanguage
cally,weperformknowledgeextractiononmultipleVLNdatasetsto navigationarebasedonreinforcementlearning[21]orimitation
obtainfine-grainedtasksequencesforeachcoarse-grainedtask,in learning[8].Inaddition,auxiliarytasks,suchaspre-trainedon
whichconsequentrelationsareconceptualized.OurVLN-EventKG subtasks[54],speaker-drivenrouteselection[8],crossmodalmatch-
isusedasanexternalknowledgegraphtoguidefine-grainedsub- ing[17,49],text-basedpre-trained[6,41],progressestimation[26,
tasksgenerationfromtheinputofcoarse-grainedinstructions.For 27],areproposedtoimprovetheperformanceandgeneralization
LLM-KGjointVLNmodel,wedesignalarge-small-modelcollab- ofneuralagentsinseenandunseenenvironments.Fordata-centric
orativeframeworkusingeventknowledge,namedEventNav,in learning,researchersexplorehowtoutilizedatamoreeffectively
whichweextracttheeventknowledgeofVLNscenariostobuild andsynthesizemorediversedata.Speaker-follower[8]introducesa
speakermodeltoenhanceinstructiontrajectorypairs.Envdrop[45]TowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKM’24,October21–25,2024,Boise,ID,USA
Figure2:Theconstructionpipelineoftheeventknowledgegraph.
breaksthelimitofvisibleenvironmentvariabilitythroughenviron- 3 PROBLEMSTATEMENT
mentdropout.CCC[48]aimstolearninstruction-following(fol- Thevisuallanguagenavigationtaskisdesignedtoallowintelligent
lower)andinstructiongeneration(speaker)simultaneously.Most agentstointeractintheenvironmentaccordingtohumaninstruc-
ofthesemethodsuserecurrentneuralnetworks,andthecoreidea tionstocompletecorrespondingtasks.Foreachtask,itcontainsa
istoencodepreviousvisualobservationsandbehaviorsintoahid- triple(𝑥 1:𝐿,𝑣 1:𝑡,𝑎 1:𝑡),consistingofinstructions,visualsequences,
denstate.However,recurrentneuralnetworkshavelong-range andactionsequences.Amongthem,theinstruction𝑥 1:𝐿 isanat-
dependencyproblemsandtheirabilitytomodellongsequences urallanguagewithlength𝐿.Thevisualsequence𝑣
1:𝑡
contains𝑡
ispoor.Inrecentyears,withtheintroductionoftransformerar- images,representingtherealtrajectoryduringtheexecutionofthe
chitecture[47],itcanprovideglobal-levelattentioninteractionfor task.Theactionsequences𝑎 1:𝑡 isasequenceof𝑡 actiontypelabels
longsequencetasks.VLN-Bert[29]trainsatransformermodelto 𝑎 𝑡 ∈ {𝐴}where𝐴isactionspace.Thetaskgoalistofitafunc-
modelcompatibilitybetweeninstructionsandasetofgenerated tion𝑓,whoseinputisinstructioninformation,visualinformation,
trajectories.RecurrentVLBERT[14]usesexplicitrecursivestate historicalactions,andtheoutputisthepredictednextaction.
andpre-trainedVLBERTtoprocessobservationsatarbitrarytime
steps,butthissolutionisdifficulttosolvelongsequenceVLNtasks. 𝑎
𝑡
=𝑓(𝑥 1:𝐿,𝑣 1:𝑡,𝑎 1:𝑡−1).
(1)
Transformershavesuccessfullyimplementedawiderangeofclassi-
Theaboveprocessiscarriedoutiteratively.Theagentcontinuously
ficationandgenerationtasks,fromlanguagetoimagesandvideos.
predictsnewactionstoobtainnewvisualenvironments.Itstops
In[32],theauthorsshowthatlong-distancetaskplanningusing
untiltheagentthinksthetaskiscompletedorreachesthemaximum
reinforcementlearningtransformersischallengingandpropose
steplimit.
asolution.Thetransformerarchitecturecanalsoeffectivelyfuse
Since𝑥usuallycontainsfine-grainedtaskinstructions,andthere
informationfromdifferentmodalities,whereaunifiedtransformer
isalargegapwiththerealworld,weaimtostudytheVLNalgorithm
modelisresponsibleforsolvingproblemsthatrequiremulti-modal
thatonlycontainscoarse-grainedtaskinstructions.Inthistask
information,suchasvisualquestionanswering[23],videosubtitles
setting,duetothelackoffine-grainedinstructions,weexplore
andtemporalprediction[44]orretrieval[9].E.T.[33]proposedan
utilizingexternaleventknowledgetoenhancenavigationplanning.
algorithmtomodeltheentireVLNtaskusingaunifiedtransformer
Specifically,weconstructanovelVLN-specificeventknowledge
architecture.
graphandproposeanevent-enhancedVLNplannertoimproveVLN
LLM-basedVLNInrecentyears,thebirthofLLMhasgreatly
performanceundertheconditionofcoarse-grainedinstructions.
promotedthedevelopmentoftheentirefieldofnaturallanguage
processing.BecauseLLMhasrichcommonsenseknowledgeand
4 EVENTKNOWLEDGEGRAPHFORVLN
instruction-followingcapabilities,someworkisalsoactivelyex-
4.1 EventsinVLNScenery
ploringtheapplicationofLLMtovisuallanguagenavigation.On
task.Shahetal.[39]adoptedGPT-3[3]toidentify“landmarks” Aneventrepresentsaspecificactivityorsituationthatoccursata
orsub-objectives,whileHuangetal[16]focusedtheireffortson specificeventandlocation.Structurallyrepresentingeventsina
applying LLM to code generation code. Zhou et al [53] used a knowledgegraph,canhelpthesystemunderstandthecorrelation
LLMtoextractcommon-senseknowledgeabouttherelationship betweeneventsandhelpmodelretrieval,reasoning,andanalysis.
betweentargetobjectsandobjectsinobservationsandperform IntheVLNscenario,tocompleteacoarse-grainedinstruction,the
zero-examplereasoningobjectnavigation(zson)[28].Thesetasks modelneedstoseriallyexecuteaseriesoffine-grainedinstructions.
requireLLM’sinternalknowledgefortaskplanning.Thediffer- Wedefinethesefine-grainedinstructionsaseventknowledgein
encefromtheaboveworkisthatwefocusoneventknowledge VLNtasks.Foramissionplanningmodel,onlybyfullyunderstand-
specifictoVLNscenariosandbuilditintoaknowledgegraphina ingtheinternaleventknowledgeoftheVLNscenariocanbetter
structuredform,therebyassistingLLMinmakingmoreaccurate decisionsbemade.AlthoughthegeneralLLMhasrichexperimental
missionplanning. knowledge,theVLNtaskhasalargeamountofpriorknowledge
thatfitsthescenery,whichisspecificallyreflectedinthecorrela-
tionbetweeneachsubtask.Thisknowledgeisnotyetavailablein
thegeneralLLM.Forexample,fortheevent"pickupanapple",in
therealworld,subsequenteventsmayinclude"washtheapple",CIKM’24,October21–25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
Figure3:TheframeworkofourproposedEventNavwithlarge-small-modelcollaborationusingeventknowledge.
"eattheapple","cuttheapple","peel"andsoon,butinaspecific processoftheeventknowledgegraphandthepromptdesignduring
VLNscenery,thesubsequenteventsmayonlyinclude"cleaning knowledgeextractionrespectively.Intheend,weget8.4w,2.9w,
theapple"and"cuttingtheapple".Thismismatchineventdistribu- and3.9wsequentialsubtasksintheALFRED[40],R2R[2],and
tionmaycauseLLMtogeneratesubtasksthatdonotfittheVLN REVERIE[37]datasetsrespectively.Bymergingtheknowledgein
sceneryduringprediction.Therefore,eventknowledgeespecially thethreedatasets,weobtainedatotalof150k+nodesand120k+
withsequentialrelationshipsthatfitstheVLNscenerycanbeof relationshipseventknowledgegraph.Weusetheembeddingmodel
greathelptoLLMintaskplanning.Itcanconstrainthesubtasks toembedallnodesintoavectordatabaseforsubsequentretrieval.
predictedbyLLMwithinasmallandreasonablerange,thereby
promptingtheLLMtopredictmoreaccuratesubtasks. 5 EVENT-KNOWLEDGE-ENHANCEDVLN
5.1 FrameworkOverview
4.2 DataCollectionandConstruction Unlikemostalgorithmsthatprovidefine-grainedtaskinstructions,
Inourwork,wefirstintegrateeventknowledgewithincurrent thisworkfocusesonVLNtasksthatonlyprovidecoarse-grained
mainstreamVLNbenchmarkstobuildanovelVLN-specificevent instructions.However,simplyeliminatingallfine-grainedsubtasks
knowledge.Specifically,wetrytoextractallcoarse-grainedtasks will greatly affect the performance of the entire model. So, we
andcorrespondingsubtasksequencesinthedataset.Thusbuilding usethepowerfultaskplanningcapabilitiesoftheLLMtopredict
aneventknowledgegraph.Theeventknowledgegraphdescribes subtasks.Besides,weextractedknowledgefromtheVLNdataset
theexecutionsequenceofsubtaskscorrespondingtoeachcoarse- andbuiltaneventknowledgegraphcalledVLN-EventKG.Wede-
grainedtask.Thesesubtasksserveasexternalknowledgetoassist signacollaborativemodelarchitectureoflargeandsmallmodels.
thetaskplanningoftheLLM.WestudiedthreetypicalVLNdatasets: Thearchitectureconsistsoftwo-levelloops,theouteris"Subtask
ALFRED[40],R2R[2],andREVERIE[37].InALFRED[40],the PlanningLoop"andtheinneris"ActionPlanningLoop".
corresponding subtask sequence of each coarse-grained task is ForSubtaskPlanningLoop,LLMactsontheouterloop,which
given in the form of key-value pairs. This is already the result obtainsthreeinputs,includingthecoarse-grainedtask(Input1)
wewant,sothereisnoneedforadditionalextractionwork.For theimage-densecaptionafterBLIP2conversion(Input2),andthe
theR2R[2]dataset,thecoarse-grainedtasksandsubtasksarea similarsubtasksequenceretrievedfromtheeventknowledgegraph
unifiedtextparagraph.FortheREVERIE[37]dataset,thecoarse- VLN-EventKGbasedonthelastsubtask.Thesetwoinputswith
grainedtaskandsubtaskareseparated,butthesubtaskisatext retrievedknowledgeareconvertedintopromptsforLLMtopredict
paragraph.Therefore,fortheR2R[2]andREVERIE[37]datasets, thenextsubtask𝑥 1𝑐𝑢 :𝐿𝑟 2𝑟𝑒𝑛𝑡−𝑠𝑢𝑏𝑡𝑎𝑠𝑘 .
weuseLLMtoextractcoarse-grainedtasksandsubtasksinthem ForActionPlanningLoop,accordingtoEq.(1),thesmallmodel
tobuildaneventknowledgegraph.Figure2showtheconstruction actsontheinnerlooptopredicttheaction(Output1)𝐴 𝑡atthetimeTowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKM’24,October21–25,2024,Boise,ID,USA
Figure4:Subtaskgenerationbasedoneventknowledge(VLN-EventKG)retrievalandLLMusingpromotinglearning.
𝑡 followingtheinputtextualinstructions𝑥 1:𝐿,visualobservation above-mentionedprocessasthedynamicbacktrackingmechanism
𝑣 1:𝑡 andhistoricalactions𝑎 1:𝑡−1.Specifically,textualinstructions inSec.5.3.TheoverallmodelarchitectureisshowninFigure.3.
𝑥 1:𝐿canberewritteninto:
𝑥 1:𝐿 = [𝑥 1𝑐 :𝑜 𝐿𝑎 1𝑟𝑠𝑒−𝑔𝑟𝑎𝑖𝑛𝑒𝑑 ,𝑥 1𝑐𝑢 :𝐿𝑟 2𝑟𝑒𝑛𝑡−𝑠𝑢𝑏𝑡𝑎𝑠𝑘 ], (2) 5.2 Knowledge-enhancedPlanner
where𝑥𝑐𝑜𝑎𝑟𝑠𝑒𝑑−𝑔𝑟𝑎𝑖𝑛𝑒𝑑 and𝑥𝑐𝑢𝑟𝑟𝑒𝑛𝑡−𝑠𝑢𝑏𝑡𝑎𝑠𝑘
representtextualin-
Theeventknowledgegraphservesasplug-inknowledgeforthe
1:𝐿1 1:𝐿2 LLM.InthetaskplanningprocessoftheLLM,itistoconstrain
structions.𝐿1and𝐿2indicatethetextlengthofcoarse-grained
themodeltopredictsubtasksthataremoresuitablefortheVLN
instructionandthecurrentsubtaskinstructiongeneratedbythe scenery.Assumingthatthecurrentsubtaskis𝑇 𝑖,thepredictionof
SubtaskPlanningLoop.Visualobservation𝑣 1:𝑡includeshistorical thenextsubtask𝑇 𝑖+1canbeobtained:
visualinformationandcurrentvisualinformationattime𝑡.
𝑣 1:𝑡 = [𝑣 1:𝑡−1,𝑣 𝑡], (3) 𝑇 𝑖+1=𝐿𝐿𝑀(𝐷,𝑉,𝐻,𝐿),
(4)
Besides,ineachstep,thesmallmodelalsooutputstwoadditional 𝐿=𝑅𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙(VLN-EventKG,𝑇 𝑖),
signals(Output2)𝑆 ={0,1}and𝑅 ∈ [0,1].𝑆representswhether
thecurrentsubtaskiscompleted,and𝑅representstheprobability where𝐷 representsthedefinitionofthecoarse-grainedtask,𝑉
ofthecurrentsubtaskcompletion.𝑆hashigherpriorityifthereis representstheimage-densecaptionafterBLIP2[22]conversion,
aconflictbetween𝑆and𝑅. and𝐻 represents the subtask that has been executed. Figure. 4
Whenthesmallmodelpredictsthatthesubtaskhasbeencom- showsthepromptoftheLLMusedinsubtaskprediction.Since
pleted,𝑆is1,itpromotestheLLMtoplanthenextsubtask.When thesubtaskspredictedbytheLLMmaynotbecorrect,thesmall
thesmallmodelpredictsalowprobability𝑅ofthecurrentsubtask modelwillalsoasktheLLMtore-planthesubtaskbytriggering
completion,theLLMwillre-planthecurrentsubtask.Wedesignthe thedynamicbacktrackingmechanism,shownintheblueframe.CIKM’24,October21–25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
Dataset 𝐷 𝑎𝑣𝑔
R2R[2] 2.35
REVERIE[37] 3.57
ALFRED[40] 8.26
Table1:Theaveragenumberofactionsrequiredforeach
subtaskineachdatasetbenchmarks
Inaddition,𝑅𝑒𝑡𝑟𝑖𝑒𝑣𝑎𝑙 representsthevectorretrieval.Weuse
the embedding model to retrieve the current subtask𝑇 𝑖 as the
queryintheEventKnowledgeGraphVLN-EventKG.Weretrieve
𝑡𝑜𝑝𝑘subtasks{𝑠 1,𝑠 2,...,𝑠 𝑘}similartothecurrentsubtask𝑇 𝑖,that
is{𝑇 𝑖𝑠 1,𝑇 𝑖𝑠 2,...,𝑇 𝑖𝑠𝑘},andobtaintheirsubsequentsubtasks Figure5:Thetrainingdetailsofthedynamicbacktracking
{𝑇𝑠 1 ,𝑇𝑠 2 ,...,𝑇𝑠𝑘 }.Ultimately,𝐿isasetofsimilarsubtasks mechanism.
𝑖→𝑖+1 𝑖→𝑖+1 𝑖→𝑖+1
andtheirsubsequentsubtasks:
𝐿={(𝑇 𝑖𝑠 1,𝑇 𝑖𝑠 →1 𝑖+1),(𝑇 𝑖𝑠 2,𝑇 𝑖𝑠 →2 𝑖+1),...,(𝑇 𝑖𝑠𝑘,𝑇 𝑖𝑠 →𝑘 𝑖+1)}. (5)
5.3 DynamicHistoryBacktrackingMechanism basedonthecurrentcompletedsubtaskcontext.Weuseatransistor
Existingvisuallanguagenavigationmethodspredictthenextaction switchflagtovisualizethisprocess,shownontheleftofFigure.3.
entirelybasedonlanguageinstructionsandvisualinformation.It
Priority2:𝑅showsacontinuousdownwardtrendorthevalue
isterminatedwhenthemodelpredictsthetaskhasbeencompleted
of𝑅 islessthan0.25.Itmeansthecurrentsubtaskcouldnotbe
orexecutionreachesthemaximumstep.However,anyerrorduring completed,inwhichtheagentneedstogobacktothestartposi-
taskexecutionmaybecontinuouslyamplifiedandeventuallylead tionofthecurrentsubtask,andtheLLMispromptedtore-plan
to task failure. Under the framework of large and small model thesubtaskbasedonthelasteffectivesubtaskcontext.Aspecific
collaboration,theLLMisusedforsubtaskplanningandpredicts promptingcaseisshowninFigure.4.
thenextsubtask,whilethesmallmodelpredictsthenextaction
basedoncoarse-grainedtasks,visualinformation,andsubtasks 6 EXPERIMENTANDANALYSIS
predictedbytheLLM.SincethesubtaskspredictedbyLLMmaynot
6.1 DatasetsandSimulationEnvironments
becompletelycorrect,incorrectsubtasksmayresultinincorrect
actiongenerationwhicheventuallyleadstofailureofthenavigation AllVLNdatasetsneedtobebuiltbasedonacertainsimulationenvi-
task.However,thesubtaskspredictedbyLLMarenotcompletely ronment.TheR2R[2]andREVERIE[37]datasetsarebuiltonMat-
fixed.IfLLMcanadjustthepredictionresultsofthesubtasksbased terPort3Dsimulation.Inthesedatasets,theagentcannavigatethe
on certain signals and guide the small model to perform extra houseaccordingtothenavigationgraph.Thenavigationgraphcon-
actions,thefailureofthetaskcouldbeavoided.Specifically,during sistsofviewpointsandedges.Eachviewpointcontainsapanoramic
theexecutionofthegeneratedaction,inadditiontopredictingthe view,andthereisabidirectionaledgebetweenanytwonavigable
nextaction,thesmallmodelalsopredictstwoadditionalvalues viewpoints,thatis,theagentcanmovebidirectionallybetween
(signals),namely𝑆and𝑅.𝑆representswhetherthecurrentsubtask twoadjacentviewpoints.Initially,theagentisplacedatarandom
iscompleted(completed,set1,andviceversa),and𝑅represents viewpointandinputsanaturallanguageinstruction.Ateachstep,
theprobabilityofthecurrentsubtaskcompletion. 𝑡,theagentobtainsapanoramicgraph𝑂 𝑡 = {𝑜 𝑡,𝑖},1 ≤ 𝑖 ≤ 36,
Thepredictionwithprobability𝑅 isthetriggeringcondition whichincludesseverallocalviews.Eachlocalview𝑜 𝑡 represents
forthebacktrackingmechanism.Duringthemodeltrainingpro- anavigableviewpoint,andtheagentneedstochooseoneofthe
cess,weobtainnegativesamplesbyrandomsamplingfromaction viewpointstogo.
space.Specifically,wesetthatthesubtaskrequires𝑊 actionsto TheALFRED[40]datasetisbuiltonAI2THORsimulation.Inthis
completeandthestartpointvalueof𝑅 1 = 0.5.Forthepositive dataset,theagentisplacedatarandomposition,andineachstep,a
action trajectory, after the𝑖 −𝑡ℎ action of the subtask is com- definedactionisselectedforexecution.Theseactionsincludetwo
pleted,𝑅 𝑖 = 𝑅 1+ 2𝑊𝑖 ,𝑖 ≤ 𝑊.Fornegativeactiontrajectory,we types:movementandnavigation.Thereare6typesofmovement
useweaknegativesamplestoapproximaterealnegativesamples, actions(i.e.forward,backward,leftturn,rightturn,headup,head
byrandomsamplingwithinactionspace,thenthecorresponding down),and7typesofinteractiveactions(i.e.pickingup,putting
𝑅 𝑖 =𝑅 0− 2𝑊𝑖 ,𝑖 ≤𝑊.ThewholeprocessisshowninFigure.5. down,opening,closing,washing,cooking,andchopping).The7
Duringmodelinference,twopriorityconditionsaresettopro- typesofinteractiveactionsinvolveinteractingwithtargetobjects
moteanewsubtask. intheenvironment.Thetargetobjectoftheactionisrepresentedby
Priority1:𝑆predictedtobe1.Thisconditionhashigherpriority amask.Therefore,forinteractiveactions,theagentneedstooutput
comparedtoanotherone,whichmeansthecurrentsubtaskhas notonlytheactioncategorybutalsothemaskoftheinteractive
beencompletedandtheLLMcandirectlypredictthenextsubtask object.TowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKM’24,October21–25,2024,Boise,ID,USA
ValidationSeen ValidationUnseen TestUnseen
Methods
SR↑ NE↓ SPL↑ TL SR↑ NE↓ SPL↑ TL SR↑ NE↓ SPL↑ TL
Random 16 9.45 - 9.58 16 9.23 - 9.77 13 9.79 12 9.89
Human - - - - - - - - 86 1.61 76 11.85
Seq2Seq-SF[1] 39 6.01 40 11.33 22 7.81 19 11.67 20 7.85 18 8.13
Speaker-Follower[8] 54 4.29 49 12.58 29 7.92 27 13.02 28 7.84 24 15.19
EnvDrop[45] 55 3.82 56 9.69 39 5.92 36 10.24 41 6.02 43 11.24
RecBERT[15] 67 3.49 63 11.45 58 4.54 54 12.94 58 4.61 53 12.46
HOP[38] 68 3.23 65 11.37 57 4.52 50 12.79 56 4.39 56 12.93
AirBERT[11] 69 3.19 64 11.56 55 4.29 51 12.56 55 4.33 52 12.57
EventNav(ours) 72 2.77 63 12.58 60 4.23 51 14.56 59 4.25 49 14.55
Table2:ComparativeresultsbetweenourmethodandothermainstreammethodsontheR2R[2]datasetthatonlyprovides
coarse-grainedinstructions.
ValidationSeen ValidationUnseen TestUnseen
Methods
SR↑ OSR↑ SPL↑ TL SR↑ OSR↑ SPL↑ TL SR↑ OSR↑ SPL↑ TL
Random 3 8.92 2 11.99 2 11.93 1 10.76 2 8.88 1 10.34
Human - - - - - - - - 81 86.83 54 21.18
RCM[49] 23 29.44 22 10.70 9 14.23 7 11.98 8 11.68 7 10.60
FAST-MATTN[37] 50 55.17 26 16.35 14 28.20 6 29.70 14 23.36 9 30.69
AirBert[11] 47 48.98 42 15.16 28 34.51 22 18.71 30 34.20 23 17.91
RecBERT[15] 51 53.90 48 13.44 31 35.02 25 16.78 30 32.91 24 15.86
HOP[38] 53 54.88 37 13.80 32 36.24 26 16.46 30 33.06 24 16.38
ORIST[36] 45 49.12 42 10.73 17 25.02 15 10.90 22 29.20 19 11.38
CKR[10] 57 61.91 53 12.16 19 31.44 11 26.26 22 30.40 14 22.46
EventNav(ours) 61 63.29 45 15.25 34 39.28 23 16.25 34 39.36 23 16.10
Table3:ComparativeresultsbetweenourmethodandothermainstreammethodsontheREVERIE[37]datasetthatonly
providescoarse-grainedinstructions.
TestSeen TestUnseen
Methods
SR↑ GC↑ PLWSR↑ PLWGC↑ SR↑ GC↑ PLWSR↑ PLWGC↑
Human - - - - 91 94.5 - -
SEQ2SEQ[40] 3 8.00 7.29 12.56 1 7.30 2.42 9.09
E.T.[33] 22 29.31 12.39 17.64 4 8.60 5.77 10.24
MOCA[42] 22 28.37 12.36 26.78 5 14.30 9.24 19.19
FILM[31] 26 36.15 10.39 19.17 - - - -
EmBert[43] 25 37.69 11.29 25.60 7 12.49 9.79 18.22
EventNav(ours) 31 42.20 11.97 24.62 10 18.75 9.26 19.48
Table4:ComparativeresultsbetweenourmethodandothermainstreammethodsontheALFRED[40]datasetthatonly
providescoarse-grainedinstructions.
6.2 ImplementationDetails In"ActionPlanningLoop",wefollowtheworkof[33]andadopt
Theframeworkofourproposedmethodincludes"SubtaskPlanning aunifiedtransformer-basedmodel.Forallvisualimages,wefirst
Loop"and"ActionPlanningLoop".LLM(i.e.ChatGPT[3])isused feedthemintoResNet50[13]toobtainthevisualrepresentation
in"SubtaskPlanningLoop".Wefollowtheworkof[52]anduse vector,andthenthevectorisusedasatokenforthetransformer-
theBLIP2[22]toprovideLLMwithanimage-densecaptionofthe basedmodel.Inaddition,fortheALFRED[40]datasetbenchmark,
currentvisualenvironment.FortheeventknowledgegraphVLN- itstasksnotonlyrequiretheagenttomovebutalsorequireitto
EventKG,weusethebge-large-en[51]semanticmodeltoperform interactwiththeenvironment.Therefore,themodelalsoneedsto
vectorsimilarityretrievalandretrievesimilarsubtaskstoprompt generateamasktorepresentwhichparttointeractwith.Weuse
theLLMfortaskplanning. ResNet-50MaskR-CNN[12]togenerateatargetmask.Overall,CIKM’24,October21–25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
onlythetransformer-basedmodelandMaskR-CNN[12]arein-
volvedinthetrainingprocess,andtherestoftheparametersof
themodelsarefrozen.Wetrainourproposedmodelfor20epochs
usingPytorch1.13onfourV100GPUplatforms,withbatchsize64,
Adamoptimizer,andlearningrate1𝑒−3.
Duringtheinferenceprocess,theLLMisusedtosubtaskplan-
ning,andthesmallmodelisusedtogenerateactionstobeexecuted.
Forknowledgeenhancement,weretrieve𝑡𝑜𝑝𝑘similarsubtasksin
VLN-EventKGeachtime.Here,wechoose𝑡𝑜𝑝𝑘 =5.Inthedynamic
backtrackingmechanism,whentheprobability𝑅predictedbythe
smallmodelmeetsoneofthefollowingtwoconditions,theLLM
will be required to re-plan the subtask: 1)𝑅 < 0.25; 2)𝑅 shows
adownwardtrend𝑊 consecutivetimes.For𝑊 inFigure1and
Sec.5.3,Theaverageexecutionlength𝐷 𝑎𝑣𝑔ofeachsubtaskover Figure6:Taskplanningcasestudyunderknowledgeenhance-
differentdatasetsisshowninTable1.Weset𝑊 withweighted ment
𝐷 𝑎𝑣𝑔fordifferentdatasets(i.e.R2R[2]:2×𝐷 𝑎𝑣𝑔,REVERIE[37]and
ALFRED[40]:𝐷 𝑎𝑣𝑔).
6.3 EvaluationMetrics tablesondifferentdatasetbenchmarks,wecanfindthatourVLN-
EventKGprovidesVLNplannerswithusefuleventknowledge,and
ForR2R[2]:
coupledwiththedynamicbacktrackingmechanism,ourproposed
• TrajectoryLength(TL):theaveragepathlengthinmeters;
EventNavmodelforVLNtaskachievecompetitiveperformance
• NavigationError(NE):theaveragedistancebetweenthe
withcoarse-grainedinstructionsinput.Figure6alsoshowsacase
agent’sfinalpositionandthetargetinmeters;
studyonthecoarse-grainedVLNtask,withtheenhancementof
• SuccessRate(SR):theratioofstoppingwithin3metersto
theeventknowledgegraphofVLN-EventKG,LLMcanobtainthe
thetarget;
eventknowledgeoftheVLNscenario,therebyrealizingamore
• SuccessPathLength(SPL):thesuccessrateweightedbythe
reasonabletaskplanning.
normalizedinverseofthePathLength.
ForREVERIE[37]alsoemploysOracleSuccessRate(OSR):theratio 6.5 AblationStudy
ofhavingaviewpointalongthetrajectorywherethetargetobject
VLN-EventKGFortheutilizationofeventknowledge,weconsider
isvisible.
theimpactoftheeventknowledgegraphVLN-EventKGonthe
ForALFRED[40]:
performanceofthemodel.Weuseasmallmodelthatonlyprovides
• SuccessRate(SR):thesuccessrateofthetotaltask;
coarse-grainedinstructionsasabaseline(shortasbase)toconsider
• ConditionalSuccess(GC):theratioofsubtaskscompleted
thefollowingsituations:
overthewholetask;
• PathlengthweightedSR(PLWSR):theratioof(lengthofthe (1) base+planD:Withoutusingtheeventknowledgegraph,let
expertpath)and(lengthtakenbytheagent); LLMplansubtasksDirectly;
• PathlengthweightedGC(PLWGC):theratioof(lengthof (2) base+planS:KnowledgeisextractedSeparatelyforeach
thegroundtruthPath)and(lengthtakenbytheagent). datasetbenchmark,andtheeventknowledgegraphcorre-
spondingtoeachdatasetbenchmarkisusedduringinfer-
6.4 ComparisonsonDifferentVLNbenchmarks ence;
(3) base+planF:FusingtheeventknowledgefromVLNdataset
Basedonthesettingofcoarse-grainedtaskdescriptions,Weevalu-
benchmarkdomainsandusingtheentireeventknowledge
atetheperformanceofEventNavandmainstreamVLNmodelson
graphduringinference.
threedatasetbenchmarks:R2R[2],REVERIE[37]andALFRED[40].
Table2presentsthecomparativeresultsbetweenourmethodand AsshownresultsofSuccessRate(SR)inTable5,wefindthatin
other VLN models in R2R dataset [2] and our proposed model R2R[2],REVERIE[37],andALFRED[40]benchmark,themodels
achieves strong performance inmost metrics. The scores of SR fusedeventknowledgegraph(base+planF)performbetterthan
showanaverageimprovementof 2%overtheexistingresults.In thesingleeventknowledgegraph(base+planS).Inparticular,for
Table3,themostsignificantimprovementisobservedintheun- R2R[2]andREVERIE’s[37],themodelfusedeventknowledge
seentestdatasetofREVERIE[37],witha4%increaseinSRmetric. graphhassignificantlyimprovedthemodeleffect,withtheaverage
ComparedtotheexistingmethodsonSRandOSRmetrics,our SRimprovementover2%.Weattributeittothesimilardistributions
modelachievesanaverageimprovementof3.3%and3.2%improve- oftaskdescriptionsinabovementionedtwodatasets.Therefore,the
ments,respectively.TheresultofALFRED[40]isshowninTable4. modelsfusedeventknowledgegraphcanobtainbroaderanddiverse
Weevaluateperformanceintwosettingswithfourmetricsand externalknowledgepromotingthenextsubtaskfromtheLLM.In
achieveSOTAinfivemetrics.Comparedwiththeexistingmodels, contrast,subtasksintheALFRED[40]datasetbenchmarkmainly
theSRofourmodelisimprovedby5%and3%onTestSeenand involveinteractingwiththeenvironment,whichexistsrelative
Testunseensettings.Fromtheresultsshownintheabovethree differencesfromthefirsttwobenchmarkdomains.ThisreducesTowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKM’24,October21–25,2024,Boise,ID,USA
theeffectivenessofeventknowledgetoimproveperformancein Method/Dataset R2R[2] REVERIE[37] ALFRED[40]
base 55 22 14
theALFRED[40]benchmark.
base+planD 49 20 13
WhennotusingtheeventknowledgegraphandlettingLLM
base+planS 61 45 26
plansubtasksdirectly(base+planD),wefoundthatduetothelack base+planF 63 48 26
ofeventknowledgefromtheknowledgegraph,thenextsubtask base+planF+backtrace(Ours) 72 61 31
predictedbythemodelisusuallyverydifferentfromtheoriginal Table5:Theimpactofeventknowledgegraphonmodeltask
subtaskdistributionofthedatasetbenchmark.Thisapproachcould planningeffect.
damage the performance of the action prediction for the small
model.Therefore,ourVLN-EventKGplaysaconstraintroleinthe
subtask planning process based on coarse-grained instructions, 𝑥 R2R[2] REVERIE[37] ALFRED[40]
whichguaranteestheLLMplansubtasksclosetothedistribution 𝑊 0.1 0.25 0.5 0.1 0.25 0.5 0.1 0.25 0.5
oftheoriginaldatasetbenchmark,therebypromotingtheabstract 0.5×𝐷 𝑎𝑣𝑔 63 66 60 52 58 55 27 29 22
taskunderstandingandactiongenerationofthesmallmodel. 𝐷 𝑎𝑣𝑔 69 69 67 59 61 51 30 31 29
DynamicBacktrackingMechanismDynamicbacktracking 2×𝐷 𝑎𝑣𝑔 71 72 70 58 60 51 26 30 25
mechanismaimstodeterminewhetherthecurrentsubtaskcanbe 4×𝐷 𝑎𝑣𝑔 70 70 69 58 55 50 24 27 25
executedsuccessfullybasedontheprobabilitylevel𝑅 predicted
Table6:Inthedynamicbacktrackingmechanism,theimpact
by the small model, and guide the LLM to re-plan the subtask
of𝑥 and𝑊 ontheexperimentalresults.
attheappropriatetime.Specifically,whenoneofthefollowing
twoconditionsoccurs,thebacktrackingmechanismistriggered,
denotedinSec.6.2:
(1) 𝑅 <𝑥; thefirsteventknowledgegraphVLN-EventKGforVLNtasksby
(2) 𝑅showsadownwardtrend𝑊 consecutivetimes. extractingandconceptualizingtheactivitysequencesacrossvari-
where𝑥 and𝑊 aretwohyperparametersofthemodelinference ouspublicVLNbenchmarks.Anevent-knowledge-enhancedVLN
planningmodelisdesignedunderthelarge-small-modelcollabo-
process.Thetriggeredtimingofbacktrackinghasagreatimpact
rativeframeworkEventNavtorealizecoarse-grainedinstruction
ontheoverallperformanceofthemodel.Backtrackingtooearly
understandinganddecomposition.Adynamicbacktrackingmech-
couldcausethemodeltoexitearlyonthecorrecttrajectorywhile
anismisconsideredtofurtherimprovethesuccessrateofVLNby
backtrackingtoolatecouldfailbecausethemaximumstepsize
in-timecorrectionofintermediatedecisions.Experimentalresults
specifiedbythetaskisreached.
indicatetheimportanceofeventknowledgeinsequentialdecisions,
Fordifferentdatasetbenchmarks,wechoosedifferentvaluesof
𝑥 and𝑊 toconductablationstudies.Weselectthevaluesof𝑥 as (i.e.VLNplanning).OurproposedVLN-EventKGcombinedwith
(0.1,0.25,0.5) respectively.Wechoosethevaluesof𝑊 as (0.5× EventNaveffectivelyimproveabstracthumaninstructionsunder-
𝐷 𝑎𝑣𝑔,𝐷 𝑎𝑣𝑔,2×𝐷 𝑎𝑣𝑔,4×𝐷 𝑎𝑣𝑔),where𝐷 𝑎𝑣𝑔 denotestheaverage standingandhierarchicaltaskplanningbyover5%averagesuccess
rateinvariouspublicbenchmarks.
lengthofitssubtasksforaspecificdatasetinTable1.
Table6showstheSuccessRate(SR)correspondingtodifferent𝑥
8 ACKNOWLEDGMENTS
and𝑊 underdifferentdatasetbenchmarks.Thevalueof0.25for𝑥
gavethebestmodelresults.Inaddition,thebestresultsareobtained Wethanktheanonymousreviewersfortheirvaluablecomments.
when𝑊 is2×𝐷 𝑎𝑣𝑔 forR2R[2]and𝐷 𝑎𝑣𝑔 forREVERIE[37]and ThisworkissupportedbyPostdoctoralFellowshipProgramofCPSF
ALFRED[40].Twice𝐷 𝑎𝑣𝑔usedinR2R[2],wethinkitisbecause underGrantNumberGZC20232292,NationalNaturalScienceFoun-
thelengthsofsubtaskstepsinthisbenchmarkareshorterthan dationofChina(No.62072323,U21A20488,No.62102276),Shanghai
theothertwo,andselecting𝑊 =𝐷 𝑎𝑣𝑔 couldcausethemodelto ScienceandTechnologyInnovationActionPlan(No.22511104700),
earlybacktracktoterminatethecorrectactionplanningloop.In ChinaPostdoctoralScienceFoundation(GrantNo.2023M732563),
contrast,𝑊 shouldbeappropriatelyselectedforalargervaluefor andZhejiangLabOpenResearchProject(No.K2022NB0AB04).
REVERIE [37] and ALFRED [40] benchmarks due to the longer
averagesubtasksteps.When𝑅decreases𝐷 𝑎𝑣𝑔timescontinuously REFERENCES
canroughlydeterminethatasubtaskwilleventuallyfail,so𝑊 [1] PeterAnderson,QiWu,DamienTeney,JakeBruce,MarkJohnson,NikoSünder-
shouldbe𝐷 𝑎𝑣𝑔.Inaddition,Table5alsoshowstheimpactofthe h na au vf ig,I aa tn ioR ne :i Id n, tS et re pp rh ete in ngG vo iu sl ud a, la lyn -d gA ron ut no dn eV da nn aD vie gn atH ioe nng inel s. trV ui csi to ion n-a sn ind- rla en alg eu na vg ie
-
dynamicbacktrackingmechanismontheoverallsuccessrateof ronments.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
themodel.Ourmethod(𝑏𝑎𝑠𝑒+𝑝𝑙𝑎𝑛𝐹 +𝑏𝑎𝑐𝑘𝑡𝑟𝑎𝑐𝑒)integratesthe recognition,pages3674–3683,2018.
[2] PeterAnderson,QiWu,DamienTeney,JakeBruce,MarkJohnson,NikoSünder-
dynamicbacktracemechanismwiththewholeeventknowledge hauf,IanReid,StephenGould,andAntonVanDenHengel.Vision-and-language
graphshowingsignificantimprovementinthesuccessrateofover navigation:Interpretingvisually-groundednavigationinstructionsinrealenvi-
ronments.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
allVLNbenchmarks.
recognition,pages3674–3683,2018.
[3] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,
7 CONCLUSION PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda
Askell,etal.Languagemodelsarefew-shotlearners.Advancesinneuralinfor-
Inthispaper,weinvestigatecoarse-grainedVLNplanningguided mationprocessingsystems,33:1877–1901,2020.
[4] DavidChenandRaymondMooney. Learningtointerpretnaturallanguage
byeventknowledge.Eventknowledgeespeciallyconsequentrela-
navigationinstructionsfromobservations.InProceedingsoftheAAAIConference
tionsisconsideredforsequentialdecisions.Specifically,wepropose onArtificialIntelligence,volume25,pages859–865,2011.CIKM’24,October21–25,2024,Boise,ID,USA KaichenZhao,YaoxianSong,HaiquanZhao,HaoyuLiu,TiefengLi,andZhixuLi
[5] HowardChen,AlaneSuhr,DipendraMisra,NoahSnavely,andYoavArtzi.Touch- estimation.arXivpreprintarXiv:1901.03035,2019.
down:Naturallanguagenavigationandspatialreasoninginvisualstreetenvi- [27] Chih-YaoMa,ZuxuanWu,GhassanAlRegib,CaimingXiong,andZsoltKira.
ronments. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand Theregretfulagent:Heuristic-aidednavigationthroughprogressestimation.In
PatternRecognition,pages12538–12547,2019. ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
[6] Marc-AlexandreCôté,AkosKádár,XingdiYuan,BenKybartas,TavianBarnes, pages6732–6740,2019.
EmeryFine,JamesMoore,MatthewHausknecht,LaylaElAsri,MahmoudAdada, [28] ArjunMajumdar,GunjanAggarwal,BhavikaDevnani,JudyHoffman,andDhruv
etal. Textworld:Alearningenvironmentfortext-basedgames. InComputer Batra.Zson:Zero-shotobject-goalnavigationusingmultimodalgoalembeddings.
Games:7thWorkshop,CGW2018,HeldinConjunctionwiththe27thInternational AdvancesinNeuralInformationProcessingSystems,35:32340–32352,2022.
ConferenceonArtificialIntelligence,IJCAI2018,Stockholm,Sweden,July13,2018, [29] ArjunMajumdar,AyushShrivastava,StefanLee,PeterAnderson,DeviParikh,
RevisedSelectedPapers7,pages41–75.Springer,2019. andDhruvBatra.Improvingvision-and-languagenavigationwithimage-text
[7] VishnuSashankDorbala,JamesFMullenJr,andDineshManocha.Cananem- pairsfromtheweb.InComputerVision–ECCV2020:16thEuropeanConference,
bodiedagentfindyour“cat-shapedmug”?llm-basedzero-shotobjectnavigation. Glasgow,UK,August23–28,2020,Proceedings,PartVI16,pages259–274.Springer,
IEEERoboticsandAutomationLetters,2023. 2020.
[8] DanielFried,RonghangHu,VolkanCirik,AnnaRohrbach,JacobAndreas,Louis- [30] VincentMicheliandFrançoisFleuret. Languagemodelsarefew-shotbutlers.
PhilippeMorency,TaylorBerg-Kirkpatrick,KateSaenko,DanKlein,andTrevor arXivpreprintarXiv:2104.07972,2021.
Darrell.Speaker-followermodelsforvision-and-languagenavigation.Advances [31] SoYeonMin,DevendraSinghChaplot,PradeepRavikumar,YonatanBisk,and
inneuralinformationprocessingsystems,31,2018. RuslanSalakhutdinov.Film:Followinginstructionsinlanguagewithmodular
[9] ValentinGabeur,ChenSun,KarteekAlahari,andCordeliaSchmid.Multi-modal methods.arXivpreprintarXiv:2110.07342,2021.
transformerforvideoretrieval.InComputerVision–ECCV2020:16thEuropean [32] EmilioParisotto,FrancisSong,JackRae,RazvanPascanu,CaglarGulcehre,Sid-
Conference,Glasgow,UK,August23–28,2020,Proceedings,PartIV16,pages214– dhantJayakumar,MaxJaderberg,RaphaelLopezKaufman,AidanClark,Seb
229.Springer,2020. Noury,etal.Stabilizingtransformersforreinforcementlearning.InInternational
[10] ChenGao,SiLiu,JinyuChen,LutingWang,QiWu,BoLi,andQiTian.Room- conferenceonmachinelearning,pages7487–7498.PMLR,2020.
objectentitypromptingandreasoningforembodiedreferringexpression.IEEE [33] AlexanderPashevich,CordeliaSchmid,andChenSun.Episodictransformerfor
TransactionsonPatternAnalysisandMachineIntelligence,2023. vision-and-languagenavigation.in2021ieee.InCVFInternationalConferenceon
[11] Pierre-LouisGuhur,MakarandTapaswi,ShizheChen,IvanLaptev,andCordelia ComputerVision(ICCV),pages15922–15932,2021.
Schmid.Airbert:In-domainpretrainingforvision-and-languagenavigation.In [34] AmirPnueliandZoharManna.Thetemporallogicofreactiveandconcurrent
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages systems.Springer,16:12,1992.
1634–1643,2021. [35] XavierPuig,KevinRa,MarkoBoben,JiamanLi,TingwuWang,SanjaFidler,and
[12] KaimingHe,GeorgiaGkioxari,PiotrDollár,andRossGirshick. Maskr-cnn. AntonioTorralba.Virtualhome:Simulatinghouseholdactivitiesviaprograms.
InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
2961–2969,2017. pages8494–8502,2018.
[13] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearning [36] YuankaiQi,ZizhengPan,YicongHong,Ming-HsuanYang,AntonVanDenHen-
forimagerecognition.InProceedingsoftheIEEEconferenceoncomputervision gel,andQiWu.Theroadtoknow-where:Anobject-and-roominformedsequen-
andpatternrecognition,pages770–778,2016. tialbertforindoorvision-languagenavigation.InProceedingsoftheIEEE/CVF
[14] YHong,QWu,YQi,CRodriguez-Opazo,andSGould. Arecurrentvision- InternationalConferenceonComputerVision,pages1655–1664,2021.
and-languagebertfornavigation.arxiv2021.arXivpreprintarXiv:2011.13922,1, [37] YuankaiQi,QiWu,PeterAnderson,XinWang,WilliamYangWang,Chunhua
2021. Shen,andAntonvandenHengel.Reverie:Remoteembodiedvisualreferring
[15] YicongHong,QiWu,YuankaiQi,CristianRodriguez-Opazo,andStephenGould. expressioninrealindoorenvironments.InProceedingsoftheIEEE/CVFConference
Vlnbert:Arecurrentvision-and-languagebertfornavigation. InProceedings onComputerVisionandPatternRecognition,pages9982–9991,2020.
oftheIEEE/CVFconferenceonComputerVisionandPatternRecognition,pages [38] YanyuanQiao,YuankaiQi,YicongHong,ZhengYu,PengWang,andQiWu.
1643–1653,2021. Hop:History-and-orderawarepre-trainingforvision-and-languagenavigation,
[16] ChenguangHuang,OierMees,AndyZeng,andWolframBurgard.Visuallan- 2022.
guagemapsforrobotnavigation.In2023IEEEInternationalConferenceonRobotics [39] DhruvShah,BłażejOsiński,SergeyLevine,etal.Lm-nav:Roboticnavigation
andAutomation(ICRA),pages10608–10615.IEEE,2023. withlargepre-trainedmodelsoflanguage,vision,andaction.InConferenceon
[17] HaoshuoHuang,VihanJain,HarshMehta,AlexanderKu,GabrielMagalhaes, robotlearning,pages492–504.PMLR,2023.
JasonBaldridge,andEugeneIe.Transferablerepresentationlearninginvision- [40] MohitShridhar,JesseThomason,DanielGordon,YonatanBisk,WinsonHan,
and-languagenavigation.InProceedingsoftheIEEE/CVFinternationalconference RoozbehMottaghi,LukeZettlemoyer,andDieterFox. Alfred:Abenchmark
oncomputervision,pages7404–7413,2019. forinterpretinggroundedinstructionsforeverydaytasks.InProceedingsofthe
[18] WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch.Language IEEE/CVFconferenceoncomputervisionandpatternrecognition,pages10740–
modelsaszero-shotplanners:Extractingactionableknowledgeforembodied 10749,2020.
agents.InInternationalConferenceonMachineLearning,pages9118–9147.PMLR, [41] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam
2022. Trischler,andMatthewHausknecht. Alfworld:Aligningtextandembodied
[19] WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch.Language environmentsforinteractivelearning.arXivpreprintarXiv:2010.03768,2020.
modelsaszero-shotplanners:Extractingactionableknowledgeforembodied [42] KunalPratapSingh,SuvaanshBhambri,ByeonghwiKim,RoozbehMottaghi,and
agents.InInternationalConferenceonMachineLearning,pages9118–9147.PMLR, JonghyunChoi. Factorizingperceptionandpolicyforinteractiveinstruction
2022. following.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
[20] JacobKrantz,ErikWijmans,ArjunMajumdar,DhruvBatra,andStefanLee. Vision,pages1888–1897,2021.
Beyondthenav-graph:Vision-and-languagenavigationincontinuousenviron- [43] AlessandroSuglia,QiaoziGao,JesseThomason,GovindThattai,andGaurav
ments.InComputerVision–ECCV2020:16thEuropeanConference,Glasgow,UK, Sukhatme.Embodiedbert:Atransformermodelforembodied,language-guided
August23–28,2020,Proceedings,PartXXVIII16,pages104–120.Springer,2020. visualtaskcompletion.arXivpreprintarXiv:2108.04927,2021.
[21] JunchengLi,XinWang,SiliangTang,HaizhouShi,FeiWu,YuetingZhuang, [44] ChenSun,AustinMyers,CarlVondrick,KevinMurphy,andCordeliaSchmid.
andWilliamYangWang.Unsupervisedreinforcementlearningoftransferable Videobert:Ajointmodelforvideoandlanguagerepresentationlearning. In
meta-skillsforembodiednavigation.InProceedingsoftheIEEE/CVFConference ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages
onComputerVisionandPatternRecognition,pages12123–12132,2020. 7464–7473,2019.
[22] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2:Bootstrapping [45] HaoTan,LichengYu,andMohitBansal. Learningtonavigateunseenen-
language-imagepre-trainingwithfrozenimageencodersandlargelanguage vironments: Back translation with environmental dropout. arXiv preprint
models. InInternationalconferenceonmachinelearning,pages19730–19742. arXiv:1904.04195,2019.
PMLR,2023. [46] StefanieTellex,ThomasKollar,StevenDickerson,MatthewWalter,AshisBaner-
[23] JiasenLu,DhruvBatra,DeviParikh,andStefanLee.Vilbert:Pretrainingtask- jee,SethTeller,andNicholasRoy.Understandingnaturallanguagecommands
agnosticvisiolinguisticrepresentationsforvision-and-languagetasks.Advances forroboticnavigationandmobilemanipulation. InProceedingsoftheAAAI
inneuralinformationprocessingsystems,32,2019. ConferenceonArtificialIntelligence,volume25,pages1507–1514,2011.
[24] CoreyLynch,MohiKhansari,TedXiao,VikashKumar,JonathanTompson,Sergey [47] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
Levine,andPierreSermanet.Learninglatentplansfromplay.InConferenceon AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.
robotlearning,pages1113–1132.PMLR,2020. Advancesinneuralinformationprocessingsystems,30,2017.
[25] CoreyLynchandPierreSermanet.Groundinglanguageinplay.arXivpreprint [48] HanqingWang,WeiLiang,JianbingShen,LucVanGool,andWenguanWang.
arXiv:2005.07648,40(396):105,2020. Counterfactualcycle-consistentlearningforinstructionfollowingandgeneration
[26] Chih-YaoMa,JiasenLu,ZuxuanWu,GhassanAlRegib,ZsoltKira,RichardSocher, invision-languagenavigation. InProceedingsoftheIEEE/CVFconferenceon
andCaimingXiong. Self-monitoringnavigationagentviaauxiliaryprogress computervisionandpatternrecognition,pages15471–15481,2022.TowardsCoarse-grainedVisualLanguageNavigationTaskPlanningEnhancedbyEventKnowledgeGraph CIKM’24,October21–25,2024,Boise,ID,USA
[49] XinWang,QiuyuanHuang,AsliCelikyilmaz,JianfengGao,DinghanShen,Yuan- 2023.
FangWang,WilliamYangWang,andLeiZhang.Reinforcedcross-modalmatch- [52] GengzeZhou,YicongHong,andQiWu.Navgpt:Explicitreasoninginvision-
ingandself-supervisedimitationlearningforvision-languagenavigation. In and-languagenavigationwithlargelanguagemodels.InProceedingsoftheAAAI
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition, ConferenceonArtificialIntelligence,volume38,pages7641–7649,2024.
pages6629–6638,2019. [53] KaiwenZhou,KaizhiZheng,ConnorPryor,YilinShen,HongxiaJin,LiseGetoor,
[50] XinWang,QiuyuanHuang,AsliCelikyilmaz,JianfengGao,DinghanShen,Yuan- andXinEricWang. Esc:Explorationwithsoftcommonsenseconstraintsfor
FangWang,WilliamYangWang,andLeiZhang.Vision-languagenavigation zero-shotobjectnavigation. InInternationalConferenceonMachineLearning,
policylearningandadaptation.IEEEtransactionsonpatternanalysisandmachine pages42829–42842.PMLR,2023.
intelligence,43(12):4205–4216,2020. [54] WangZhu,HexiangHu,JiachengChen,ZhiweiDeng,VihanJain,EugeneIe,and
[51] ShitaoXiao,ZhengLiu,PeitianZhang,andNiklasMuennighof.C-pack:Packaged FeiSha.Babywalk:Goingfartherinvision-and-languagenavigationbytaking
resourcestoadvancegeneralchineseembedding.arXivpreprintarXiv:2309.07597, babysteps.arXivpreprintarXiv:2005.04625,2020.