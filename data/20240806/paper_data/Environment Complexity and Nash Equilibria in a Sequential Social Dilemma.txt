Environment Complexity and Nash Equilibria in a
Sequential Social Dilemma
MustafaYasir AndrewHowes VasiliosMavroudis
TheAlanTuringInstitute UniversityofExeter TheAlanTuringInstitute
myasir@turing.ac.uk andrew.howes@exeter.ac.uk vmavroudis@turing.ac.uk
ChrisHicks
TheAlanTuringInstitute
c.hicks@turing.ac.uk
Abstract
Multi-agentreinforcementlearning(MARL)methods,whileeffectiveinzero-sum
orpositive-sumgames,oftenyieldsuboptimaloutcomesingeneral-sumgames
wherecooperationisessentialforachievinggloballyoptimaloutcomes. Matrix
game social dilemmas, which abstract key aspects of general-sum interactions,
suchascooperation,risk,andtrust,failtomodelthetemporalandspatialdynamics
characteristicofreal-worldscenarios. Inresponse,ourstudyextendsmatrixgame
social dilemmas into more complex, higher-dimensional MARL environments.
WeadaptagridworldimplementationoftheStagHuntdilemmatomoreclosely
matchthedecision-spaceofaone-shotmatrixgamewhilealsointroducingvariable
environmentcomplexity.Ourfindingsindicatethatascomplexityincreases,MARL
agentstrainedintheseenvironmentsconvergetosuboptimalstrategies,consistent
withtherisk-dominantNashequilibriastrategiesfoundinmatrixgames. Ourwork
highlightstheimpactofenvironmentcomplexityonachievingoptimaloutcomes
inhigher-dimensionalgame-theoreticMARLenvironments.
1 Introduction
Multi-agentreinforcementlearning(MARL)isconcernedwithtrainingagentstomaximiseindividual
orsharedrewardsinenvironmentswithmultipleconcurrentlearners.AcriticalaspectofMARListhe
agentmotive,whichcanbecategorisedascompetitive(zero-sum),purelycooperative(positive-sum),
or mixed (general-sum). MARL methods have shown considerable success both in competitive
environments[9,32]andinpurelycooperativeenvironments[27,4]. Inthesesettings,agentgoals
areeitherdirectlyopposedoraligned,andlearningparadigmslikeself-play,whichtrainagentsto
computebestresponsestorationalopponents,empiricallyconvergetostablestrategies,i.e.,Nash
equilibria,thatcorrespondtooptimaloutcomes. However,Nashequilibriaingeneral-sumgames
oftencoincidewithsuboptimaloutcomes,renderingthedirectapplicationofsuchmethodsineffective
forfindingdesirablesolutions[8,6,14]. Instead,inmanygeneral-sumgames,optimaloutcomes
areoftenonlyachievablethroughcooperation—byexplicitlyfavouringcollectivegainsoverstable
individualrewards.
Matrixgamesocialdilemmas(MGSDs),aclassofgeneral-sumgames,representsituationswhere
individualrationalitymayleadtocollectiveirrationality. Thesedilemmasarewidelyusedtoexplore
cooperationinvariousdisciplines[23,26],includingMARL,wherenumerousmethodshavebeen
proposedtoinducecooperationamonglearningagents[7].WhileMGSDsarevaluableforabstracting
keyaspectsofreal-worldmulti-agentinteractions—suchascooperation,risk,andtrust—theylack
thecomplexityofreal-worldtemporalandspatialdynamics. Unlikereal-worldactions,matrixgame
17thEuropeanWorkshoponReinforcementLearning(ewrl2024).
4202
guA
4
]TG.sc[
1v84120.8042:viXraactionsarebinary,i.e.,limitedtocooperateordefectchoices,anddonotmodelextendedtimeframes
[12]. Consequently,recentworkshavelookedatextendingMGSDstohigher-dimensionalMARL
environmentsinordertomoreaccuratelymodelreal-worldscenarios(seeSection2).
Inthisstudy,wefocusonanextensionoftheStagHuntMGSDtoagridworldMARLenvironment,
and explore how unique environment dynamics—absent in the matrix game—affect the learnt
strategies of independent MARL agents. Specifically, we introduce eight novel variants of the
gridworldStagHunt,eachincoporatingadifferentdegreeofrandomisationacrossvariousdynamics,
whichwemodelasenvironmentcomplexity. Thisresearchdirectionismotivatedbytheinherent
limitationsofMGSDs,wheremodificationstoagamearetypicallyconfinedtochangesinpayoff
structure. Extendingthesegamesintohigher-dimensionalenvironmentsoffersanovelopportunityto
examinehowadditionalfactors,suchasenvironmentdynamics,influencecooperativebehaviours.
Furthermore,ourinquiryissupportedbystudiesonrandomnessinevolutionarysocialdilemmas[28],
alongwithempiricalresearchhighlightingitsimpactoncooperation[1,34]. Akeyfindingisthat
multiplevariantsofthegridworldStagHuntcanmaptothesameStagHuntMGSD;however,even
minorvariationsintheircomplexitycanpromoteconvergencetovastlydifferentstrategies. More-
over,ourworkadvancesthedevelopmentofgame-theoreticMARLenvironments,moreaccurately
reflectingreal-worldconditions,whereenvironmentdynamicsmayplayasignificantroleinshaping
general-summulti-agentinteraction. Insummary,wepresentthefollowingkeycontributions:
• Weadaptamulti-agentgridworldimplementationoftheStagHuntMGSDtomatchthe
decisionspaceofaone-shotmatrixgameandcreateeightuniqueenvironmentvariants,each
withaspecifiedlevelofcomplexity.
• We train independent MARL agents in each of these environment variants and observe
distinctempiricalconvergencepatternstoeitherthesuboptimalNashequilibriumstrategy
oramixedstrategy.
• We demonstrate that environment complexity promotes systematic convergence to the
suboptimal Nash equilibrium strategy, despite the established viability of more optimal
strategies.
• We conduct an empirical game-theoretic analysis on our trained policies, showing that
certaingridworldStagHuntvariantscanbemappedtoMGSDswithStagHuntpayoffs,and
canbemodelledasSequentialSocialDilemmas.
2 RelatedWork
AddressingtheviabilityofMGSDsinmodellingkeyaspectsofmulti-agentinteraction,numerous
studieshaveproposedhigher-dimensionalgame-theoreticMARLenvironmentstoexplorecoopera-
tionundermorerealisticsettings. TheCoinGame,awidelyusedgridworldenvironmentbasedon
repeatedMGSDs,hasbeenusedtoexaminecooperativeMARLmethodsbyFoersteretal.[8],andthe
MeltingPotsuitecombinesvariousMGSDsingridworldformatstobenchmarkMARLalgorithms
[13]. Recently, Khan et al. [11] highlight the challenges in adapting matrix games to gridworld
environmentsbydemonstratingtheshortcomingsoftheCoinGame. Relevanttoourfocusonthe
StagHunt,istheLevel-BasedForagingenvironment[5],agridworldforaginggamewhichtargets
multi-agentcooperationandcoordination,andastudybyNicaetal.[20],whichsuccessfullydemon-
stratesenhancedlearninginaMinecraft-basedstaghuntthroughgridworldabstraction. Although
existingworksprovideusefulgame-theoreticenvironments, primarilyforproposingnewMARL
methodsforrepeatedandpartiallyobservablesettings,wesimplifythelearningproblemtoone-shot
andfullyobservablegames;tofocusonmodifyingenvironmentdynamics,whilstmaintainingthe
relation between our environment and its matrix game formulation. To this end, we leverage an
existinggame-theoreticMARLenvironment[19,22],basedontheStagHuntMGSD,chosenforits
overlapwithexistingenvironmentsandaccessibilityofmanipulatinggamedynamics,whichwas
brieflyexploredbyPeysakhovichandLerer[22].
23 Background
3.1 MatrixGameSocialDilemmas
Amatrixgameisaformalrepresentationofstrategicinteractionsbetweentwoplayers,whereeach
player’sdecisionaffectstheother’soutcome. Thegameisrepresentedbyamatrixinwhicheachcell
detailstheoutcomes(orpayoffs)fortheplayersbasedontheircombinedchoices. Playerschoosea
strategywithoutknowledgeoftheother’ssimultaneousdecision,andthecombinationoftheirchoices
leadstoaspecificpayoffforeach. Inmatrixgamesocialdilemmas(MGSDs),thetwoactions,or
strategies,availabletoeachplayerare‘cooperate‘and‘defect‘,andtheexactpayoffsforeachplayer
aregivenbythevalues{R,P,S,T}∈ R,detailedinTable1. Formally,amatrixgameisasocial
dilemmawhenitsfourpayoffssatisfythefollowinginequalities:
R>P,R>S,2R>T +S, andeither: T >R,or,P >S. (1)
Table1: Payofftableforamatrixgamesocialdilemma,where(X,Y)denotesthepayoffforthe
givenrowandcolumnplayer,respectively.
Player2
Cooperate Defect
Cooperate (R,R) (S,T)
Player1
Defect (T,S) (P,P)
Central to our work, is the MGSD, Stag Hunt [31]; where the ‘cooperate‘ and ‘defect‘ actions
correspondto‘hunt‘and‘forage‘,respectively. Conceptually,thisrepresentsascenariowhereplayers
aretaskedwitheitherhuntingastagorforagingaplant. Abilateraldecisiontohuntoffersthehighest
payoff,h,tobothplayers,butcarriestheriskofbeingmauledbythestag,yieldingthelowestpossible
reward,m,toaplayerhuntingunilaterally. Inmaulingscenarios,theopposingplayermayreceivea
uniqueforagingrewardf∗ ≥f. Formally,theStagHuntischaracterisedbyTable2andcorresponds
toanMGSDwhereR=h,P =f,S =mandT =f∗.
Table2: Payofftableforthegeneralised2-playerstaghunt,whereh>f∗ ≥f >m.
Player2
Hunt Forage
Hunt (h,h) (m,f∗)
Player1
Forage (f∗,m) (f,f)
3.2 NashEquilibria
NashEquilibria,afundamentalconceptingametheoryintroducedbyNash[18],referstoasituation
inwhicheachplayerinagamemakesanoptimalchoiceconsideringthechoicesoftheotherplayers,
andnoplayerhasanythingtogainbychangingonlytheirownstrategyunilaterally. Formally,aNash
Equilibriuminagamewithnplayersisdefinedasastrategyprofile(s∗,s∗,...,s∗)suchthatfor
1 2 n
eachplayeri,thestrategys∗isabestresponsetothestrategiess∗ chosenbytheotherplayers. This
i −i
canbeexpressedas:
s∗ ∈argmaxu (s ,s∗ ), ∀i∈{1,2,...,n} (2)
i i i −i
si
wheres representsthestrategychosenbyplayeri,s∗ representsthestrategieschosenbyallother
i −i
playersexcepti,andu (s ,s )isthepayofffunctionforplayeri.IntheStagHunt,playersarefaced
i i −i
withthedecisionbetweenhuntingastagorforagingforplants,i.e.,s ∈{Hunt,Forage},andthe
i
gameisparticularlyvaluableforcontainingtwoNashEquilibriawhichreflectdifferentresolutionsof
itsinherentsocialdilemma:
3Payoff-DominantEquilibrium(Hunt,Hunt): Ifbothplayerschoosetohuntstag,neitherhasan
incentivetodeviateunilaterallysincedoingsowouldresultinalowerpayoff. Ifoneplayerswitches
toforagingwhiletheothercontinuestohunt,thedeviatingplayer’spayoffdecreasesfromh(under
successfulcooperation)tof orf∗,bothofwhicharelessthanhunderthegame’sassumptions. Thus,
(Hunt,Hunt)isaNashEquilibrium. Thisoutcomeispayoffdominantbecauseitresultsinthe
highestrewardsforbothplayers,i.e.,bothreceiveh,whichisgreaterthananyotherpayoffinthe
game.
Risk-Dominant Equilibrium (Forage,Forage): Similarly, if both players choose to forage,
deviatingtohuntingisdisadvantageous. Aunilateralswitchwouldresultinapayoffofm,asthe
lonehunterwouldfacetheriskofmauling,whichistheworstoutcome. Hence,(Forage,Forage)
alsoconstitutesaNashEquilibrium. Althoughthisequilibriumyieldsalowerpayoff,f,thanmutual
staghunting,itisconsideredriskdominantbecauseitminimisestheriskassociatedwiththepotential
non-cooperationoftheotherplayer.
3.3 MarkovGames
We model the MARL adaptation of Stag Hunt as a two-player Markov Game, also known as a
StochasticGame[25];anaturalextensionofMarkovDecisionProcessestomulti-agentsettings[17].
AMarkovgamefortwoplayersisatuple⟨S,A,R,P,γ⟩,whereS isthesetofstatesdescribingthe
environment,Aisthecombinedactionspaceacrossallplayers,givenbyA:=A ×A ,wherethe
1 2
A isisthesetofactionsavailabletoplayeri,Ristherewardfunction,givenbyR := R ×R ,
i 1 2
whereR :S×A×S →Ristherewardfunctionforplayeri,whichmapsastateandactiontuple
i
(includingtheresultingstate)toarealnumberanddefinestherewardthatplayerireceivesafterall
playerschoosetheiractions. P :S×A×S →∆(S)isthestatetransitionprobabilityfunction,and
γ isthediscountfactor,whichistypicallyconstrainedto0≤γ ≤1.
Inatwo-playerMarkovGame,eachplayerchoosesapolicythatdeterminestheiractionbasedonthe
currentstateofthegame. Thejointpolicyvector⃗πcombinestheindividualpoliciesofeachplayer,
where⃗π =(π ,π ). Forplayeri,thevaluefunctionunderthisjointpolicy,V⃗π ,isdefinedasthe
1 2 i
expectedsumofdiscountedrewardswhenbothplayersadheretotheirrespectivepartsof⃗π. The
formaldefinitionis:
(cid:34) ∞ (cid:12) (cid:35)
V i⃗π(s)=E ⃗at∼⃗π,st+1∼P(st,⃗at) (cid:88) γtR i(s t,⃗a t,s t+1)(cid:12) (cid:12) (cid:12)s 0 =s (3)
t=0
where s is the state at time t, ⃗a is the joint actions taken by Player 1 and Player 2 at time t,
t t
determinedbytheirpoliciesπ andπ ,respectively,s isthestateresultingfromtheactions⃗a ,
1 2 t+1 t
R (s ,⃗a ,s )istherewardreceivedbyplayeriaftertheactionsaretakenandthestatetransitions,
i t t t+1
andγ isthediscountfactor.
3.4 SequentialSocialDilemmas
WealsousetheSequentialSocialDilemma(SSD)model,introducedbyLeiboetal.[12],which
characteriseswhenaMarkovGamecontainsembeddedMGSDs,thusextendingtheMGSDmodel
to temporally and spatially extended settings. Formally, an SSD is a tuple ⟨M,ΠC,ΠD⟩, where
M is a Markov Game with state space S, and, ΠC and ΠD, are disjoint sets of policies that
implement cooperative and defective strategies, respectively. These policy sets are defined by
selectingametric,α : Π → R,andthresholdvaluesα andα suchthatα(π) < α ⇐⇒ π ∈
c d c
Π and α(π) > α ⇐⇒ π ∈ Π . For a given Markov Game, M, and state, s ∈ M, we
C d D
can construct an empirical payoff matrix, (R(s),P(s),S(s),T(s)), induced by the outcomes of
policies,(πC ∈ ΠC,πD ∈ ΠD),viathefollowingequations: R(s) := VπC,πC (s) = VπC,πC (s),
1 2
P(s):=VπD,πD (s)=VπD,πD (s),S(s):=VπC,πD (s)=VπD,πC (s),andT(s):=VπD,πC
(s)=
1 2 1 2 1
VπC,πD (s). Then,MisanSSDwhenthereexiststatess∈S andpolicies(πC ∈ΠC,πD ∈ΠD)
2
suchthattheinducedempiricalpayoffmatrixsatisfiesthefiveMGSDinequalitiesinEquation1.
Hence,aMarkovGameisanSSDwhenitcanbemappedbyempiricalgame-theoreticanalysis[33]
toanMGSD.
43.5 ProximalPolicyOptimisation
Proximal Policy Optimisation (PPO) is a widely used policy gradient method for single-agent
reinforcementlearning,introducedbySchulmanetal.[24]. ThecoreideabehindPPOistheuseof
aclippedsurrogateobjectivefunctiontopreventthepolicyfromdeviatingtoofarfromthecurrent
policyduringupdates. Throughitsextensiontothemulti-agentparadigm,PPOhasproveneffective
innumerousMARLenvironments,includingthosethattargetcooperation[35].
4 Methods
4.1 GridworldStagHunt
WeadaptagridworldimplementationoftheStagHuntmatrixgame,initiallydevelopedbyNesterov-
Rappoport[19]andPeysakhovichandLerer[22]. Theselectedenvironment,builtonthePettingZoo
[30] and Gym [3] frameworks, simulates a 5x5 grid game featuring two agents—red and blue,
visualised in Appendix A. Each episode begins with placing both agents, along with non-agent
entities: onestagandtwoplants,ondistinctcellsofthegrid. Theagentsandthestagcanmoveinany
ofthefourcardinaldirectionsorremainstationaryateverytimestep. Thestateoftheenvironmentis
fullyobservableforeachagent,withintegerarraysrepresentingthecoordinatesofallagentsand
entitiesonthegrid. Similarly,theactionspaceisrepresentedbyfiveuniqueintegers,corresponding
tothefourcardinaldirectionsandtheoptiontoremainstationary. Plantsremainstationarywithinan
episodeandarerandomlyre-spawnedateveryepisodestart. Individualagentrewardsaredependent
ontheactionsofbothagentsandaredeterminedattheendofeverytimestepasfollows:
• Agentsreceiveanindividualhuntingreward,h,byjointlyoccupyingthestag’sgridcell.
• Anagentreceivesaforagingreward,f,byindependently,orjointly,occupyingaplant’s
gridcell.
• Anagentreceivesamaulingreward,m,byindependentlyoccupyingthestag’sgridcell.
Therewardsvaluesassignedsatisfythegeneralisedtwo-playerstaghuntinequalityinTable2,with
valuessetath = 25,f = f∗ = 2andm = −1. Thisconfigurationsignificantlyfavourshunting
strategies over foraging strategies, whilst inducing an element of risk to hunting with a negative
maulingreward. Furthermore, toalignthegridworldstaghuntwithaone-shotmatrixgame, we
implementedaninfinite-horizonsetup. Inthisformat,episodesconcludeonlywhenbothagentshave
receivedareward:throughhunting,foraging,orbeingmauled. Inasynchronousepisodeterminations,
anagentthathasreceivedarewardisfrozenonthegrid,whiletheotheragentcontinuestoact.
4.1.1 VariableComplexity
InTable3weintroducerandomisedanddeterministicconfigurationsforthreeenvironmentparameters
in the gridworld Stag Hunt: the spawn locations of agents, the spawn locations of the stag, and
movementofthestag,whereagentandstagspawnparametersapplytothestartofeveryepisode,
andthestagmovementappliestoeverytimestep. Enumeratingallcombinationsofvaluesforthese
parametersprovideseightuniqueenvironmentvariants,eachwithaspecifieddegreeofrandomisation.
TheseenvironmentvariantsareoutlinedinTable4andweusethecorrespondingenvironmentlabelto
denotethedegreeofrandomisationinagivenenvironment,i.e.,thelabelFFRdenotesanenvironment
with:deterministicagentspawn(F),deterministicstagspawn(F)andrandomisedstagmovement(R).
Wecharacterisethespectrumofrandomisationacrosstheseeightvariantsasenvironmentcomplexity
inourstudy,whereenvironmentswithrandomisationonmultipleparameters,suchasenvironment
RRR,areconsideredmorecomplexthanthosewithdeterministicdynamics,suchasenvironment
FFF.
4.2 ExperimentalSetup
OurexperimentsuseRLLib’s[16]ProximalPolicyOptimisation(PPO)[24]implementationtotrain
pairsofagentsinthegridworldStagHuntenvironment. Weconsiderthesimplestmulti-agentvariant
of PPO, Independent PPO (IPPO), in which each agent uses an independent instance of PPO to
optimiseitsownpolicy,throughmaximisingindividualrewards[29]. Inparticular,noparametersare
sharedbetweenthemultiplepoliciesandeachagenttreatsotheragentsaspartoftheirenvironment
5Table3: Additionalconfigurationsforthreeenvironmentparametersintroducedtocontrolcomplexity
inthegridworldStagHunt.
EnvironmentParameter Value Description
Fixed Blueandredagentsspawnintheupperleftandright
AgentSpawn
cornersofthegrid,respectively.
Random Blueandredagentsspawnatrandom,distinctcellsonthe
grid.
Fixed Stagspawnsatthecentercellofthegrid.
StagSpawn
Random Stagspawnsatarandomcelllocation,distinctfromeither
agent.
Follows Stagmovestoaneighbouringcellclosesttothenearest
StagMovement
agent,measuredbyEuclideandistance.
Random Stagmovestoarandomneighbouringcell.
Table4: TheeightenvironmentvariantswecreatewithourmodificationstothegridworldStagHunt.
Eachrowrepresentsanenvironmentwithaspecifieddegreeofcomplexity.
EnvironmentLabel Agentspawn Stagspawn Stagmovement
FFF Fixed Fixed Follows
RFF Random Fixed Follows
FRF Fixed Random Follows
FFR Fixed Fixed Random
RRF Random Random Follows
FRR Fixed Random Random
RFR Random Fixed Random
RRR Random Random Random
observation. Notethat,giventheglobalobservationspaceprovidedtoagentsinthegridworldStag
Hunt,ourIPPOapproachalignscloselywiththewidelyusedmulti-agentshared-criticvariantof
PPO,MAPPO[35]. Wealsoconsiderasingle-agentapproachtothemulti-agentproblem, using
fully-centralisedPPOtoserveasbaselinetocomparetheperformanceofIPPO.Inthisapproach,
theobservationsofbothagentsarecombinedintoasingleobservation,totrainafullycentralised
joint-actionpolicythatcontrolsbothagentsandmaximisescombinedrewards.
5 Results
5.1 EnvironmentComplexityandStrategyConvergence
WeconductfivetrainingrunsofIPPOandPPOineachoftheeightenvironmentslistedinTable4,
trainingfor1,000iterations,eachconsistingof4,000environmenttimesteps. Ineachtrialweusea
distinctrandomseedfordeterminingenvironmentrandomisationdynamicswhereapplicable. We
defertheresultsofourbaseline,PPO,andthehyperparametersused,toAppendicesBandC.1.
InFigure1,whichshowsthetrainingperformanceofIPPOagents,measuredbymeancombined
episoderewardperiteration,averagedacross5trials; wegroupenvironmentsbasedonobserved
rewardpatterns. Environmentsyieldinghighrewards(i.e.,FFF,RFF,FRF,RRF,FRR)aredenoted
asGroupA,whilethosewithlowerrewards(FFR,RFR,RRR)constituteGroupB.Thesegroupings
revealtwoinsightsintothelearntstrategiesofIPPOagents:
Observation1:AgentstrainedinGroupBconvergetogloballysuboptimalstrategies,consisting
ofbilateralforaging. Byanalysingthemeanepisoderewardsattheendof1,000trainingiterations,
weinferagentstrategiesbasedonthepreviouslyestablishedpayoffstructure. Specifically,exclusive
bilateralhuntingyieldsacombinedrewardof50,significantlyhigherthanthe4frombilateralforaging.
Consequently,stableconvergencetoameanepisoderewardover4indicatesthatagentshaveadopted
strategiesthatincludebilateralhunting. Thus,inGroupB,wherethemeancombinedrewardsrarely
6IPPO-EnvironmentGroupA IPPO-EnvironmentGroupB
40 4
20 2
0 0
0 200 400 600 800 0 200 400 600 800
TrainingIterations TrainingIterations
FFF RFF FRF RRF FRR FFR RFR RRR
Figure1: TrainingperformanceofIPPOineightenvironmentswithvaryingdegreesofcomplexity,
detailedinTable4. Environmentsaregroupedbyrewardpatterns: GroupA(left)includesFFF,RFF,
FRF,RRF,FRR,andGroupB(right)includesFFR,RFR,RRR.Boldlinesrepresenttheaverage
performanceoverfivetrials,andtheshadingrepresents±1standarddeviationfromeachpoint.
exceed4(secondcolumnofFigure1),wecanconcludethatagentshavealmost-exclusivelyconverged
tobilateralforagingstrategies.
Observation2: AgentstrainedinGroupAconvergetomixed-strategies, consistingofboth
bilateralforagingandhunting. InGroupA,wheremeancombinedrewardsconsistentlyexceed
theforagingthreshold(1stcolumnofFigure1),thereisevidentnear-convergencetobilateralhunting
strategies. However,weobservethatagentsintheseenvironmentsdonotachieveaconsistentreward
of50,whichisrequiredtoindicateconvergencetoexclusivebilateralhuntingstrategies. Specifically,
inthe1,000thtrainingiteration
5.2 EscapingSuboptimalStrategies
WeconsideronlyGroupBenvironmentsfromtheprevioussubsection,inwhichweobservedIPPO
agentsconvergingtosuboptimalstrategies(Observation1). Toassessthefeasibilityofhigher-reward
strategiesintheseenvironments,webiasagentstowardbilateralhuntingstrategiesthroughasimple
curriculum.Weintroduceanewenvironmentvariant,cXXX,designedtoenforceapurelycooperative
Nash equilibrium by eliminating the reward for foraging, thus exclusively incentivising bilateral
hunting. ThiscorrespondstoaStagHuntgame,asformalisedinTable2,whereh=25,f =f∗ =0
andm=−1.
ForeachenvironmentinGroupB,acorrespondingcXXXvariantiscreated:cFFR,cRFRandcRRR.
These variants maintain their respective randomisation dynamics described in Table 4. Initially,
agentsaretrainedintheirrespectivecXXXenvironmentsfor500iterationstolearnhuntingstrategies
(Stage1). Todiscouragepassivestrategiesandfurtherpromotehunting,atimesteppenaltyof-0.5is
introducedintheseenvironments. Followingthisstep,thepre-trainedpoliciesarethentrainedin
theiroriginalenvironments(FFR,RFR,RRR)foranother500iterations,duringwhichthetimestep
penaltyisremoved(Stage2). WeuseonlyIPPOfortheseexperiments.
Observation3: CurriculumlearningallowsagentstoescapesuboptimalstrategiesinGroup
B. Our results in Figure 2 demonstrate that our curriculum enables agents to find strategies in
GroupBenvironmentsthatyieldsignificantlyhigherrewardsthanthoseobservedpreviously. In
particular,duringthesecondtrainingstage,performancebeginstoconvergetowardaveragerewards
thatsubstantiallyexceedthoseattainablethroughexclusiveforaging. Consistentlearningofhigher-
rewardstrategiesdemonstratesthatagentshavenotonlydiscovered,butalsoconvergedtostrategies
thatincludebilateralhunting—effectivelyescapingthesuboptimalstrategiesthatwerepreviously
observed.
7
draweRedosipEdenibmoCnaeMStage1 Stage2
40
20
0
0 200 400 0 200 400
TrainingIterations TrainingIterations
cFFR cRFR cRRR FFR RFR RRR
Figure 2: Training performance of IPPO in a 2-stage curriculum in Group B environments from
Section5.1. Agentsareinitiallytrainedinacooperation-inducingenvironment(cXXX),beforebeing
trainedintheirtargetenvironments.
5.3 EmpiricalGame-TheoreticAnalysis
HerewefurthercharacterisetheGroupBenvironmentswithanempiricalgame-theoreticanalysis
followingthemethodologyofLeiboetal.[12]. RecallfromSection5.1thatagentstrainedinthese
environments converged to suboptimal foraging strategies (Observation 1). In the context of the
StagHunt,foragingcorrespondstothedefectactionwithinamatrixgamesocialdilemma(MGSD).
Consequently,wecategorisethestrategiesthatthesepoliciesimplementasdefective. Conversely,
inSection5.2,wepresentalearningcurriculumthatshiftsagentsfromthesesuboptimalstrategies
towardscooperativehunting-focusedstrategies. Thuswecantraintwodistincttypesofpolicyfor
eachenvironmentinGroupB:cooperativepolicies,ΠC,anddefectivepolicies,ΠD. Thisallowsus
toconstructameta-gamewithcooperateanddefectactionsthatinvolvesamplingapolicyfromeither
categoryanddeterminingpayoffsbasedontheresultingpairwiseaveragerewards. Thismeta-game
represents an MGSD, induced by the policy sets, ΠC and ΠD, embedded within the underlying
MarkovGame,ifsuchamappingexists. Wecomputetheempiricalpayoffsofthismeta-gameas
follows:
Foreachenvironment,twopairsofpolicies,(πC,πD)and(πC,πD),aresampled,whereπC ∈ΠC
1 1 2 2 i
andπD ∈ ΠD. Thesepolicypairsrepresenttheactionsofarowplayerandacolumnplayerina
j
two-playermatrixgamewithcooperateordefectactions(i.e.,anMGSD),respectively. Specifically,
πC correspondstothecooperateactionforplayeri, andπD correspondstothedefectactionfor
i j
playerj. Theoutcomes(payoffs)foreachcombinationofactionsaredeterminedbyplayingthe
respectivepoliciesagainstoneanotherforanepisode,intherespectiveStagHuntenvironment. The
rewardsobtainedineachepisodearethenaveragedinthecorrespondingcellinthepayoffmatrix.
Thisprocedureisrepeatedforeachenvironmentuntilconvergenceofallcellvalues,yieldingtheset
ofinducedempiricalpayoffmatricesshowninTable5.
Table5: Empiricalpayoffmatricesforthemeta-gameinGroupBenvironments. Eachcellinthe
tablecontainsarewardtuple: (x,y),wherexistherewardforplayer/policy1,andyisthereward
forplayer/policy2,averagedover5,000episodes(2d.p.)
RFR RRR FFR
πC πD πC πD πC πD
2 2 2 2 2 2
πC (10.76,10.81) (0.45,1.89) (8.38,8.37) (0.42,1.87) (15.29,15.26) (0.49,1.88)
1
πD (1.95,0.59) (1.93,1.89) (1.96,0.33) (1.95,1.87) (2.02,0.24) (1.98,1.99)
1
8
draweRedosipEdenibmoCnaeMObservation 4: Group B environments map to MGSDs with Stag Hunt payoffs, and conse-
quently, can be modelled as SSDs. Section 5.3 maps a Markov Game to an MGSD, induced
byasetofpolicies. Toverifyifthismappingisvalid, hereweexaminetheempiricalpayoffsof
theconstructedMGSD.Itisimportanttonotethatitisnotnecessaryforthisprocesstogenerate
empiricalpayoffsthatconstituteavalidMGSD,asthespaceofMarkovGamesismuchlargerthan
thatofMGSDs. ThisisparticularlyrelevantinthegridworldStagHuntweconsiderinthiswork,
alongsideourmodifications,whichhasnotbeenformallycharacterisedinrelationtotheMGSDStag
Hunt. Nonetheless,theempiricalpayoffmatricesinTable5.3satisfythefivetheMGSDinequalities
inEquation1,aswellastheStagHuntinequalityinTable2. TheMarkovGamesthatmodelGroup
B environments can be mapped to MGSD Stag Hunt games using the method described above.
Consequently,theycanindeedbemodelledasSequentialSocialDilemmas(SSDs)(Section3.3).
Observation 5: Agents trained in Group B empirically converge to the suboptimal, risk-
dominantNashequilibriumstrategyfromtheMGSDStagHunt. Althoughformallyverifying
Nashequilibriaingeneral-sumMarkovGamepoliciescanoftenbeintractable[21,10],ouranalysis
provides empirical evidence that agents in Section 5.1 have converged not only to suboptimal
strategies(Insight1),butalsotoNashequilibriastrategies. Considertheoutcomesoftwodefecting
policies,(πD,πD),inTable5. IfagentshadnotconvergedtothesuboptimalNashequilibrium,we
1 2
wouldseeempiricalpayoffsfortheseoutcomesexceedingarewardof2foreachdefectingpolicy.
Instead,weobserveconvergencetoarewardlessthan2,indicatingthatwhenthesepoliciesattempt
todeviatefromthesuboptimalstrategy,theyonlyreducetheirownpayoffs. Thisbehaviouraligns
preciselywiththedefinitionofNashequilibria. Moreover,thespecificequilibriumtheyconvergeto
isreflectiveoftherisk-dominantNashequilibriumintheMGSDStagHunt(Section3.1).
6 Discussion
OurresultsindicatethatagentstrainedinGroupBenvironmentsconvergetosuboptimalstrategies
(Observation1)andthatthesepoliciesempiricallyrepresentNashequilibriastrategies(Observation
5). However,wealsodiscoverthatagentsarein-factcapableoflearningmore-optimalstrategies
within these environments (Observation 3). Despite the viability of higher-reward strategies, we
observethatagentssystematicallyconvergetothesuboptimalNashequilibriumstrategy(i.e.,mutual
foraging). Wesuggestthatthisbehaviourisaresultoftheincreasedcomplexityintroducedbyour
modificationstothegridworldStagHunt. Tosupportthis,weexplorethreealternativeexplanations
forourempiricalfindings:
First,althoughtheobservationspaceforeachagentincludesthepositionsofallentitieswithinthe
grid,convergingonhigher-rewardstrategiesinmorecomplexenvironments,suchasthoseinGroup
B,mayrequirepreviousobservations,actions,orrewardstobeincludedintheagent’sobservation.
However, this information is not available in a one-shot version of the game and thus cannot be
consideredaviablesolution. Second,itcouldbethatperforminghigher-rewardstrategiesinGroup
Benvironmentsissignificantlyimprobableduetohighlyrandomisedenvironmentdynamics,and
henceagentsarebehavingasexpected,convergingtothemostoptimalstrategy(i.e.,maximising
long-termreward). Wemitigatethisproposition(andalsothefirstexplanation)bydemonstrating
empiricallythatthroughcurriculumlearningthesameagentsareabletolearnsignificantlymore
cooperativestrategies(Observation3). Third,thedynamicsofGroupBenvironmentsmightsimply
requiremoresuitablehyperparametervaluesoradditionaltrainingtime,e.g.,toincreaseexploration
behaviourorallowmoretimetodiscoverbetterstrategies. Thus,inAppendixC.2,werunfurther
trainingexperimentsinGroupB,includingmultiplehyperparametersearches,for12,000iterations.
Theresultsshowthatnofurtherimprovementsaremadeunderanycircumstances,indicatingthatthe
observedbehaviourislikelynotanartifactofhyperparameterselectionorinsufficienttrainingtime,
butasystematicdefectiontowardlower-rewardstrategiesduetoincreasedenvironmentcomplexity.
Thus,ourfindingssuggestthattheobservedsuboptimalconvergencebehaviourisadirectresultof
theincreasedcomplexityfoundintheGroupBenvironments. Examiningtheenvironmentdynamics
withinGroupBindicatesthatthecomplexityintroducedbyarandomlymovingstag—environments
labelledbyxxR—inducessignificantuncertaintywhichconstrainslearnedstrategiestosuboptimal
equilibria. Indeed,inthreeofthefourenvironmentsfeaturingarandomlymovingstag,ouragents
9areconsistentlyunabletolearncooperativestrategies. Furthermore,environmentswithintermediate
levels of complexity— i.e., those in-between FFF and RRR—maintain average rewards above
thesuboptimalstrategythreshold(Observation2)andshowrewardsdecreasingproportionallyto
increasedenvironmentcomplexity.
Finally,ourresultsshowthatevenwithintheframeworkofSSDs,multiplevariantsofthegridworld
Stag Hunt can map to the same MGSD (Observation 4) and yet slight variations in environment
dynamics,i.e.,increasedcomplexity,canpromoteconvergencetovastlydifferentstrategies. Future
work will seek to formalise Nash equilibria strategies in SSDs, and the notion of environment
complexity,inthecontextoftheseempiricalfindings.
References
[1] Forest Baker and H. Rachlin. Teaching and learning in a probabilistic prisoner’s dilemma.
BehaviouralProcesses,2002.
[2] JamesBergstra,DanielYamins,andDavidCox. Makingascienceofmodelsearch: Hyper-
parameteroptimizationinhundredsofdimensionsforvisionarchitectures. InInternational
ConferenceonMachineLearning,pages115–123.PMLR,2013.
[3] GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,
andWojciechZaremba. Openaigym,2016.
[4] MicahCarroll,RohinShah,MarkK.Ho,ThomasL.Griffiths,SanjitA.Seshia,PieterAbbeel,
andAncaDragan. Ontheutilityoflearningabouthumansforhuman-AIcoordination. Curran
AssociatesInc.,RedHook,NY,USA,2019.
[5] FilipposChristianos,LukasSchäfer,andStefanoAlbrecht. Sharedexperienceactor-criticfor
multi-agentreinforcementlearning. Advancesinneuralinformationprocessingsystems,33:
10707–10717,2020.
[6] Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z.
Leibo,KateLarson,andThoreGraepel. Openproblemsincooperativeai,2020.
[7] YaliDu,JoelZ.Leibo,UsmanIslam,RichardWillis,andPeterSunehag. Areviewofcoopera-
tioninmulti-agentlearning. (arXiv:2312.05162),December2023. doi: 10.48550/arXiv.2312.
05162. arXiv:2312.05162[cs].
[8] JakobFoerster,RichardY.Chen,MaruanAl-Shedivat,ShimonWhiteson,PieterAbbeel,and
Igor Mordatch. Learning with opponent-learning awareness. In Proceedings of the 17th
InternationalConferenceonAutonomousAgentsandMultiAgentSystems,AAMAS’18,page
122–130,Richland,SC,2018.InternationalFoundationforAutonomousAgentsandMultiagent
Systems.
[9] MaxJaderberg,WojciechM.Czarnecki,IainDunning,LukeMarris,GuyLever,AntonioGarcia
Castañeda,CharlesBeattie,NeilC.Rabinowitz,AriS.Morcos,AvrahamRuderman,Nicolas
Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray
Kavukcuoglu,andThoreGraepel. Human-levelperformancein3dmultiplayergameswith
population-based reinforcement learning. Science, 364(6443):859–865, May 2019. ISSN
1095-9203. doi: 10.1126/science.aau6249.
[10] FivosKalogiannis,IoannisAnagnostides,IoannisPanageas,Emmanouil-VasileiosVlatakis-
Gkaragkounis, VaggosChatziafratis, andSteliosStavroulakis. EfficientlyComputingNash
EquilibriainAdversarialTeamMarkovGames. arXive-prints,art.arXiv:2208.02204,August
2022. doi: 10.48550/arXiv.2208.02204.
[11] AkbirKhan,TimonWilli,NewtonKwan,AndreaTacchetti,ChrisLu,EdwardGrefenstette,
TimRocktäschel,andJakobFoerster. Scalingopponentshapingtohighdimensionalgames.
InProceedingsofthe23rdInternationalConferenceonAutonomousAgentsandMultiagent
Systems, AAMAS ’24, page 1001–1010, Richland, SC, 2024. International Foundation for
AutonomousAgentsandMultiagentSystems. ISBN9798400704864.
10[12] JoelZ.Leibo,ViníciusFloresZambaldi,MarcLanctot,JanuszMarecki,andThoreGraepel.
Multi-agent reinforcement learning in sequential social dilemmas. In Adaptive Agents and
Multi-AgentSystems,2017.
[13] JoelZLeibo,EdgarADueñez-Guzman,AlexanderVezhnevets,JohnPAgapiou,PeterSunehag,
RaphaelKoster,JaydMatyas,CharlieBeattie,IgorMordatch,andThoreGraepel. Scalable
evaluationofmulti-agentreinforcementlearningwithmeltingpot. InInternationalconference
onmachinelearning,pages6187–6199.PMLR,2021.
[14] AdamLererandAlexanderPeysakhovich.Maintainingcooperationincomplexsocialdilemmas
usingdeepreinforcementlearning,2018.
[15] LiamLi,KevinJamieson,AfshinRostamizadeh,EkaterinaGonina,JonathanBen-Tzur,Moritz
Hardt,BenjaminRecht,andAmeetTalwalkar. Asystemformassivelyparallelhyperparameter
tuning. ProceedingsofMachineLearningandSystems,2:230–246,2020.
[16] EricLiang,RichardLiaw,RobertNishihara,PhilippMoritz,RoyFox,KenGoldberg,Joseph
Gonzalez,MichaelJordan,andIonStoica. Rllib: Abstractionsfordistributedreinforcement
learning. InInternationalconferenceonmachinelearning,pages3053–3062.PMLR,2018.
[17] MichaelL.Littman. Markovgamesasaframeworkformulti-agentreinforcementlearning.
In Proceedings of the Eleventh International Conference on International Conference on
MachineLearning,ICML’94,page157–163,SanFrancisco,CA,USA,1994.MorganKaufmann
PublishersInc. ISBN1558603352.
[18] JohnF.Nash. Equilibriumpointsinn-persongames. ProceedingsoftheNationalAcademyof
Sciences,36(1):48–49,1950. doi: 10.1073/pnas.36.1.48.
[19] DavidLvovichNesterov-Rappoport. Theevolutionoftrust: Understandingprosocialbehavior
inmulti-agentreinforcementlearningsystems. Technicalreport,2022.
[20] AndreiCristianNica,TudorBerariu,FlorinGogianu,andAdinaMagdaFlorea. Learningto
maximize return in a stag hunt collaborative scenario through deep reinforcement learning.
In 2017 19th International Symposium on Symbolic and Numeric Algorithms for Scientific
Computing(SYNASC),pages188–195,2017. doi: 10.1109/SYNASC.2017.00039.
[21] Julien Perolat, Florian Strub, Bilal Piot, and Olivier Pietquin. Learning Nash Equilibrium
for General-Sum Markov Games from Batch Data. In Aarti Singh and Jerry Zhu, editors,
Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,
volume54ofProceedingsofMachineLearningResearch,pages232–241.PMLR,20–22Apr
2017.
[22] AlexanderPeysakhovichandAdamLerer. Prosociallearningagentssolvegeneralizedstag
huntsbetterthanselfishones. InAdaptiveAgentsandMulti-AgentSystems,2017.
[23] T.C.Schelling. TheStrategyofConflict. HarvardUniversityPress,1960.
[24] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal
policyoptimizationalgorithms,2017.
[25] L.S.Shapley. Stochasticgames*. ProceedingsoftheNationalAcademyofSciences,39(10):
1095–1100,1953. doi: 10.1073/pnas.39.10.1095.
[26] JohnMaynardSmith. Evolutionandthetheoryofgames. 1976.
[27] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,
MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,KarlTuyls,andThoreGraepel.
Value-decomposition networks for cooperative multi-agent learning based on team reward.
InProceedingsofthe17thInternationalConferenceonAutonomousAgentsandMultiAgent
Systems,AAMAS’18,page2085–2087.InternationalFoundationforAutonomousAgentsand
MultiagentSystems,2018.
[28] AttilaSzolnokiandMatjažPerc. Resolvingsocialdilemmasonevolvingrandomnetworks.
EPL(EurophysicsLetters),2009.
11[29] Ming Tan. Multi-agent reinforcement learning: independent vs. cooperative agents, page
487–494. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1997. ISBN
1558604952.
[30] Jordan Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan
Sullivan,LuisSSantos,ClemensDieffendahl,CarolineHorsch,RodrigoPerez-Vicente,etal.
Pettingzoo: Gym for multi-agent reinforcement learning. Advances in Neural Information
ProcessingSystems,34:15032–15043,2021.
[31] Robert van Rooij. The stag hunt and the evolution of social structure. Studia Logica, 85:
133–138,2007.
[32] OriolVinyals,IgorBabuschkin,WojciechM.Czarnecki,MichaëlMathieu,AndrewDudzik,
JunyoungChung,DavidChoi,RichardPowell,TimoEwalds,PetkoGeorgiev,JunhyukOh,
DanHorgan,ManuelKroiss,IvoDanihelka,AjaHuang,L.Sifre,TrevorCai,JohnP.Agapiou,
MaxJaderberg,AlexanderSashaVezhnevets,RémiLeblond,TobiasPohlen,ValentinDalibard,
David Budden, Yury Sulsky, James Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang,
TobiasPfaff,YuhuaiWu,RomanRing,DaniYogatama,DarioWünsch,KatrinaMcKinney,
OliverSmith,TomSchaul,TimothyP.Lillicrap,KorayKavukcuoglu,DemisHassabis,Chris
Apps, and David Silver. Grandmaster level in starcraft ii using multi-agent reinforcement
learning. Nature,575:350–354,2019.
[33] MichaelP.Wellman. Methodsforempiricalgame-theoreticanalysis. InAAAIConferenceon
ArtificialIntelligence,2006.
[34] Chun-Lei Yang, Ching-Syang Jack Yue, and I. Yu. The rise of cooperation in correlated
matchingprisonersdilemma: Anexperiment. ExperimentalEconomics,2007.
[35] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre M. Bayen, and Yi Wu. The
surprising effectiveness of ppo in cooperative multi-agent games. In Neural Information
ProcessingSystems,2021.
A GridworldStagHunt
Figure3: RenderingofthegridworldStagHuntenvironmentusedinthisstudy.
B Hyperparameters
InTable6,welisttherelevanthyperparametersforRLlib’sPPOimplementationwhichwasemployed
throughoutthisstudy,usingthestatedhyperparametervalues. Allparametersnotstatedweresetto
None.
12Table6: HyperparametervaluesusedfortrainingPPOpoliciesthroughoutthisstudy. Thesecond
columnincludestherangeofvaluesusedforhyperparametertuninginSectionC.2,wherevalues
giveninsquarebrackets[x,y,z]representadiscretechoiceovervaluesx,yandz,andvaluesinround
brackets(x,y)representacontinuousrange,sampledaccordingtothestateddistribution.
Hyperparameter IPPO/PPO(Section5) IPPOExtended(SectionC.2)
clip 0.3 [0.2,0.3,0.4,0.45]
discount 0.99 [0.98,0.99,0.995,0.999]
trainingbatchsize 4000 4000
sgdbatchsize 128 [128,256,512]
num_sgd_iter 30 30
klcoeffient 0.2 0.2
kltarget 0.2 0.2
entropycoefficient 0 log_uniform(5×10−2,3×10−1)
vfclip 10.0 10.0
vflosscoefficient 1.0 uniform(0.5,1.0)
λ(GAE) 1.0 [0.92,0.95,0.98]
optimizer Adam Adam
learningrate 5×10−5 log_uniform(1×10−6,5×10−2)
C EnvironmentComplexityandStrategyConvergence
Inthissection,weprovideresultsofthesupplementaryexperimentsweconductedtosubstantiateour
claimsinSection5.1.
C.1 CentralisedPPO
ToestablishabaselineforourearlierresultsinSection5.1,whichusedtheIPPOtrainingmethodology,
wetrainedasinglejoint-actionpolicythatcontrolsbothagents,labelledPPO.Thedistinctionbetween
IPPOandPPOisthatwhileIPPOagentstreatotherlearnersaspartoftheirownenvironment,PPO
controlsbothagentsunderasinglepolicy. Thisdistinctionallowsustoassesstheextenttowhich
independentmulti-agentinteractionisnecessarytoachievegloballyoptimaloutcomesinthegridworld
StagHunt.
Ourresults,presentedinFigure4,furthersuggestthatenvironmentcomplexitypromotesconvergence
to suboptimal strategies. We find that the same grouping of environments from IPPO (Figure 1)
appliestoPPOagents. AlthoughtheperformanceofPPOinGroupBresemblesthatofIPPOin
GroupB,weobservethatperformanceinGroupAexhibitssignificantlylargervariancebetween
trialscomparedtoIPPO.Specifically,multipletrialsinthesameenvironment,includingthefully
deterministicenvironment,FFF,showconvergencetovastlydifferentstrategies. Thisobservation
highlights the necessity of independent multi-agent interaction in navigating the social dilemma
aspectsofourenvironment,wherePPOmaybeoverlyinfluencedbyearlybiasesintrainingruns.
Despitethis,inFigure4wecontinuetoobservethatanincreaseinenvironmentcomplexitycorrelates
withdecreasedpay-offs.
C.2 ExtendedTraininginGroupBEnvironments
InSection5.1,wefoundthatIPPOagentsconvergedtosuboptimalstrategiesinGroupB,whilst
convergingtomoreoptimalstrategiesinGroupA.Toinvestigatewhethertheobservedsuboptimal
convergencebehaviourcouldbeexplainedthrougheither: inappropriatehyperparametersforthe
given environment variants, or insufficient training time to discover more optimal strategies, we
conductedanextendedtrainingexperimentwithhyperparametertuning.
Initially,wetrainedIPPOagentsineachofthegroupBenvironments(FFR,RFR,RRR)forfive
trials,eachconsistingof1,000iterations. EachofthesetrialsusedHyperOpt’sTreeParzenEstimator
(TSE)searchalgorithm[2]andtheAsynchronousHyperband(ASHA)scheduler[15]toestimatethe
optimalhyperparametervaluesforeachtrial,wherethehyperparameterrangesusedarelistedinthe
secondcolumnofTable6. Subsequently,weselectedthebestperformingtrialineachenvironment,
13PPO-EnvironmentGroupA PPO-EnvironmentGroupB
40 4
20 2
0 0
0 200 400 600 800 0 200 400 600 800
TrainingIterations TrainingIterations
FFF RFF FRF RRF FRR FFR RFR RRR
Figure4: TrainingperformanceofPPOacrosseightenvironmentsofvaryingcomplexity,withlabels
detailedTable4. Environmentsaregroupedbyrewardpatterns: GroupA(left)includesFFF,RFF,
FRF,RRF,FRR,andGroupB(right)includesFFR,RFR,RRR.Boldlinesrepresenttheaverage
performanceacrossfivetrialsandtheshadingrepresents±1standarddeviationfromeachpoint.
IPPOExtended-EnvironmentGroupB
5
4
3
2
1
0
0 2,000 4,000 6,000 8,000 10,00012,000
TFrFaRiningIRtFeRrationsRRR
Figure5: TrainingperformanceofIPPOinGroupBenvironmentsfromSection5.1,usingthebest
hyperparametersfoundfromfivetrialsconsistingof1,000iterationseach.
measuredbymaximummeancombinedepisodereward,toextracttheoptimalsetofhyperparameter
valuesforeachenvironment. Finally,weusedthesetofoptimalhyperparametervaluestorunan
additionaltrainingexperimentforeachenvironment,consistingof12,000iterations,i.e.,4.8×107
sampledtimesteps,representingatenfoldincreaseoverthetrainingdurationusedinSection5.1.
OurresultsinFigure5,showthatIPPOagentscontinuetoconvergetothesuboptimalstrategyprevi-
ouslyobserved,despitethehyperparametertuningandextendedtrainingduration. Thisindicatesthat
thecauseofthisbehaviourislikelyneitherinsufficienttrainingtimenorunsuitablehyperparameter
values. Rather,ourresultssuggestthattheconvergencebehaviourdisplayedisadirectresultofthe
increasedcomplexityofenvironmentsinGroupB.
14
draweRedosipEdenibmoCnaeM
draweRedosipEdenibmoCnaeM