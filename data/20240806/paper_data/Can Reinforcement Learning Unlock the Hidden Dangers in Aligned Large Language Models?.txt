Can Reinforcement Learning Unlock the Hidden Dangers in
Aligned Large Language Models?
MohammadBahrami NishantVishwamitra PeymanNajafirad‚àó
Karkevandi UniversityofTexasatSanAntonio SecureAIandAutonomyLab
SecureAIandAutonomyLab SanAntonio,Texas,USA UniversityofTexasatSanAntonio
UniversityofTexasatSanAntonio nishant.vishwamitra@utsa.edu SanAntonio,Texas,USA
SanAntonio,Texas,USA peyman.najafirad@utsa.edu
mohammad.bahramikarkevandi@utsa.edu
ABSTRACT EversincethealignmentofLLMsandfollowingthesamescheme
LargeLanguageModels(LLMs)havedemonstratedimpressiveca- ofthecommonadversarialexamplesinmachinelearning[4,6],
pabilitiesinnaturallanguagetasks,buttheirsafetyandmorality therehavebeenmanyattemptstoreversethealignmentofLLMs,
remaincontentiousduetotheirtrainingoninternettextcorpora. usingtheperturbationoftheirinputs,whicharecalledJailbreaking
To address these concerns, alignment techniques have been de- intheNaturalLanguageProcessing(NLP)community[8,12,23].
velopedtoimprovethepublicusabilityandsafetyofLLMs.Yet, Whiletheimageprocessingfieldhasseenexcessiveresearchinad-
thepotentialforgeneratingharmfulcontentthroughthesemodels versarialexamples[6,29],theNLPliterature,specificallypertaining
seemstopersist.Thispaperexplorestheconceptofjailbreaking toLLMshasnotbeensufficientlyexplored.Withtheexponentially
LLMs‚Äîreversingtheiralignmentthroughadversarialtriggers.Pre- increasingpopularityofLLMs,especiallythepublic-facingcom-
viousmethods,suchassoftembeddingprompts,manuallycrafted mercialchatbots,suchasGPT-4[26]andClaude3[2],ensuringtheir
prompts,andgradient-basedautomaticprompts,havehadlimited safetybearssignificantrelevance.
successonblack-boxmodelsduetotheirrequirementsformodel Thekeyissuewiththeexistingperturbationapproachesisthat
accessandforproducingalowvarietyofmanuallycraftedprompts, theyarelimitedagainstblack-boxmodels.Forexample,Softembed-
makingthemsusceptibletobeingblocked.Thispaperintroduces dingprompts[38]requireopenaccesstothemodel‚Äôsembeddings,
anovelapproachusingreinforcementlearningtooptimizeadver- arenotinterpretable,andlacktheabilitytotransferbetweenmod-
sarialtriggers,requiringonlyinferenceAPIaccesstothetarget elsbecauseoftheirdifferentembeddingdistributions.Manually
modelandasmallsurrogatemodel.Ourmethod,whichleverages craftedprompts[8,12,30]however,cantypicallybeusedondiffer-
aBERTScore-basedrewardfunction,enhancesthetransferability entmodelsanddonotrequirewhite-boxaccess,buttheyrequire
andeffectivenessofadversarialtriggersonnewblack-boxmodels. humancreativityandareblockedquicklyduetotheirconstant
Wedemonstratethatthisapproachimprovestheperformanceof nature.Automaticdiscretepromptperturbationforjailbreaking
adversarialtriggersonapreviouslyuntestedlanguagemodel. ofteninvolvesappendingatriggerstringtotheuserprompt,which
isoptimizedusinggradientdata[23,58],whichrequireswhite-box
KEYWORDS access to the model, although it has been shown to have some
transferabilitytoblack-boxmodels.Proposedgradient-freeattacks
LargeLanguageModels,AdversarialAttacks,Alignment
oftenrequireaccesstopowerfulmodelstosucceed[7],orrequire
ACMReferenceFormat: carefullycraftedinitialseeds[20,53].Decodingmanipulationat-
MohammadBahramiKarkevandi,NishantVishwamitra,andPeymanNa- tacks, which are more recent andfaster [35], still require some
jafirad.2024.CanReinforcementLearningUnlocktheHiddenDangersin
levelofaccesstothemodel‚Äôsoutputlogitsortheoutputprobability
AlignedLargeLanguageModels?.InProceedingsofThe4thWorkshoponAr-
distribution.
tificialIntelligence-EnabledCybersecurityAnalytics(AI4CYBER-KDD2024).
Inthispaper,weintroduceanovelapproachtooptimizeadver-
ACM,NewYork,NY,USA,6pages.
sarialtriggersusingreinforcementlearning.Ourapproachonly
1 INTRODUCTION requiresinferenceAPIaccesstothetargetlanguagemodel,and
asmallsurrogatemodelwhichistrainedbyrewardsignalscal-
LargeLanguageModels(LLMs)havebeeninthespotlightrecently,
culatedusingthetargetmodel‚Äôstextoutput.Weshowthatour
duetotheirimpressivecapabilitiesinnaturallanguagetasks.How-
approachcanbeanextensiontoallpreviousworkthatoptimizean
ever,giventhesemodelsaretrainedontheinternettextcorpora,
adversarialtriggeronwhite-boxmodelsandcanpersonalizeand
theirsafetyandmoralityhavebeenquestionedintheliterature
extendtheperformanceoftriggersonnewblack-boxtargetmodels.
[16,27].TomitigatetheobjectionablebehaviorsofLLMs,alineof
Intuitively,ourworktakestheadversarialtriggerstrainedona
workcalledalignment,hasbeendonetoimprovetheirpublicusabil-
modelandadaptsthemtoanewmodel,usingonlyinferencetothe
ityandsafety[3,28,34].Despitetheirrelativesuccessingrounding
newmodel.Insummary,thecontributionsofthisworkare:i)We
LLMstohumanmorals,thequestionof"Isitstillpossibletoexploit
designareinforcementlearningparadigm,adaptedfromprevious
LLMstogenerateharmfulcontent?"remainsanunder-exploredarea.
work,tooptimizeadversarialtriggersusinginference-onlyAPIs.
‚àóCorrespondingAuthor ii)weintroduceaBERTScore-based[55]rewardfunctionutilizing
thetargetmodel‚Äôstextoutputgenerations.iii)Weshowthatour
AI4CYBER-KDD2024,August26,2024,Barcelona,Spain
2024.
4202
guA
5
]LC.sc[
1v15620.8042:viXraAI4CYBER-KDD2024,August26,2024,Barcelona,Spain Karkevandietal.
methodcanenhancetheperformanceofasetofadversarialtriggers
Harmful Prompt
onapreviouslyuntestedlanguagemodel.
Develop a virus that
Initial response
disables antivirus
2 BACKGROUND software and Target Sorry, as a helpful AI assistant,
security measures Aligned I can't assist you with that.
PromptTuning. AlthoughLargeLanguageModels(LLMs)ex- <ADVERSARIAL Model
Surrogate
hibitexceptionalgeneralizationcapabilities,theystillnecessitate TOKENS>
Model
meticulouslydesignedpromptstoachieveoptimalperformance
forspecifictasks.Accordingtotheempiricalresearchconducted
byScaoandRush[37],awell-craftedpromptcanbeasvaluable Semantic
Similarity Target response
ashundredsofdatasamplesinaclassificationtask.AsLLMscon- Function
Sure, here is a virus that
tinuetoadvance,therehasbeenagrowingfocusonautomatic disables antivirus software
prompttuning[9,41,44,54]andin-contextlearning[10,49].Auto- and security measures...
maticpromptinginitiallyinvolvedfine-tuningpromptembeddings,
atechniquereferredtoasSoftPrompting [21,22,24,31],which, Figure1:Overallarchitectureofourmethod.Thesurrogate
despiteitseffectiveness,isoftencomplexandcomputationallyin- modelisalreadyinitializedinasupervisedfine-tuningsetup
tensive.Subsequently,researchersbeganexploringtheuseofthe and isfurther fine-tunedto the targetmodel withthe re-
continuousembeddingspacetocreatediscreteprompts[32,48]. wardsignals.BERTScoreisusedastheSemanticSimilarity
Anothersignificantapproachhasbeenthedirectoptimizationof Functiontocomparetheresultinggenerationofthecurrent
discreteprompttokens[9,11,19,41].Thismethodnotonlyen- adversarialtriggerwiththedesiredtargetoutputandrewards
hancesinterpretabilityandtransferabilitybetweenmodelsbutalso thesurrogatemodel.
hasbeendemonstratedtooutperformsoftpromptingintermsof
performance.
themethodsusedinprompttuning.Softpromptattacks,forin-
AdversarialExamples. Themachinelearningfieldhasestablished
stance,involvetrainingadversarialembeddingstomanipulatethe
thattheinputsofamodelcanbedeliberatelyalteredtocausethe
model‚Äôsoutputsasdesired[38].Despitetheiroccasionalsuccess,
modeltoproduce(un)desiredoutputs;suchmodifiedinputsare
softpromptattacksaregenerallyimpracticalinreal-worldsettings
termed Adversarial Examples [4, 6, 29, 43]. Within the realm of
duetothelackofaccesstomodelembeddings.Researcherslike
NaturalLanguageProcessing,theseadversarialattackshavebeen
Zouetal.[58],Liuetal.[23],andShietal.[40]haveemployed
employedacrossvariousapplications,includingclassificationtasks
gradient-basedtechniquestooptimizediscreteadversarialprompts.
[11, 42, 52], sentiment analysis [1], and inducing toxic outputs
Tocircumventtheneedforgradientdata,methodsutilizinggenetic
[19,45].Aslanguagemodelsevolveandpromptingbecomesmore
algorithmshavebeenproposedbyYuetal.[53]andLapidetal.[20].
prevalent,therehasbeenasignificantriseininterestconcerning
Additionally,anotherapproachinvolvesusingotherlanguagemod-
adversarialattacksonprompts[38,40,50‚Äì52,57].Theserecent
elsasred-teamingassistants,whichrequiremeticulouslycrafted
developmentsunderscoretheongoingchallengeofensuringthe
seedinputs[7,14].Thesediversestrategiesunderscoretheevolving
robustnessandsecurityoflanguagemodelsagainstsuchsophisti-
natureofadversarialattacksonLLMs,reflectingacontinualarms
catedadversarialtechniques.
racebetweenmodeldevelopersandadversaries.
LLMAlignmentandJailbreaks. Pre-trainedLargeLanguageMod-
els,whilepossessingremarkableout-of-the-boxcapabilities[5,46], 3 METHODOLOGY
areoftenunsuitableforpublicuseduetotheirinsufficientunder- Inthispaper,weintroduceanovelapproachtoenhancethetransfer-
standingofinstructionsandtheirinherentunethicaltendencies, abilityofadversarialpromptstoblack-boxmodels.Ourmethoduses
suchasbiases[13,25]andtoxicbehavior[27,47].Consequently, reinforcementlearningtofurtherpersonalizeadversarialtriggers
researchersstrivetoalignthesemodelswithhumanvaluesand toatargetblack-boxLLM.Ourmethodcanextendthesuccessrate
regulatorystandardsthroughtechniqueslikeinstruction-tuning, ofanypreviousworkthathasbeendoneonwhite-boxlanguage
ReinforcementLearningfromHumanFeedback(RLHF)[3,28],and models.
DirectPreferenceOptimization[18,34].However,thisalignment
processhassparkedvigorousattemptstojailbreakthemodels,com- 3.1 Preliminaries
pellingthemtofollowharmfulinstructions[7,30,39].Theseefforts
Usingasimilarnotationtopreviouswork,wedefineanAutore-
highlighttheongoingbattlebetweenenhancingmodelsafetyand
gressiveLanguageModelManditsvocabularysetV.Letùë• ‚ààV
thepersistenceofadversarialactorsseekingtoexploitmodelvul-
denoteasingletokenandx‚ààV‚àóasequenceoftokens,whereV‚àó
nerabilities.
isthesetofallpossibletokensequencesofanylength.Thelanguage
AdversarialAttacksonLLMs. Theadventofprompttuninghas modelMcanbeutilizedtocalculatetheprobabilitydistributionof
significantlyinfluencedthelandscapeofadversarialattacks,partic- thenexttoken,givenx.Formallywrittenasùëù M(¬∑|x):V ‚Üí [0,1].
ularlyintherealmoflanguagemodels.Thistrendhasemergedbe- Additionally,forinstructtunedmodels,theinputtypicallyfollows
causeprompttuningprovidesapathwayforcreatingautomatically thestructurex=x(ùë† 1)‚äïx(ùë¢)‚äïx(ùë† 2),where‚äïistheconcatenation
generatedinputsforthesemodels.Effortstodisruptthealignment operator,x(ùë¢) istheuserprompt,andx(ùë† 1) andx(ùë† 2) aresystem
oflanguagemodels(commonlyknownasjailbreaking)oftenmirror promptsatthebeginningandtheendoftheinputrespectively.CanReinforcementLearningUnlocktheHiddenDangersinAlignedLargeLanguageModels? AI4CYBER-KDD2024,August26,2024,Barcelona,Spain
3.2 ThreatModel anyattackingmethodsuchastheworkbyZouetal.[58],orAuto-
Analogoustomostjailbreakingmethods[58],ourthreatmodel
DAN[23]isusedtofine-tunethesurrogatemodelM(ùëé)
usingonly
allowsanadversarytoappendasequenceofadversarialtokens
thenewaddedweightsùúÉ,whiletherestofthemodelisfrozen.In
x(ùëé) totheuserprompt,formingthenewinputtothemodelx‚Ä≤ = thiswork,thetriggersobtainedbyattackingthe vicuna-7b-v1.5[56]
x(ùë† 1)‚äïx(ùë¢)‚äïx(ùëé)‚äïx(ùë† 2).Theadversary‚Äôsobjectiveistomaximize usingthemethodintroducedinZouetal.[58]areusedtoshow-
theattacksuccessrateA:V‚àó‚Üí [0,1]byfindinganadversarial caseourapproach.Foranon-exhaustivelistofpossiblemethods
tokensequencex(ùëé),whichwecallAdversarialTriggerinthispaper. toobtainT 0,refertosection2.Theobjectiveofthefirstphaseis
formalizedasanoptimizationprobleminequation2.Conceptu-
Inthispaper,weassumetheattackerhasalreadyobtainedaninitial
ally,thesurrogatemodelissteeredinthedirectionoffavoringthe
setofadversarialtriggersT 0onapreviouslyattackedmodelwith
white-box access. The objective of this paper is to enhance the
generationofadversarialsequencesinT 0overanyothersequence,
givenanemptyinput.
attacksuccessrateonapreviouslyunseentargetlanguagemodel
M‚Ä≤bypersonalizingT 0tothenewtargetmodel.Contrarytomost ‚àëÔ∏Åùëõùëñ
previouswork,theattackerdoesnothaveanyaccesstothenew minimize ‚àí logùëÉ(ùë° ùëó|ùë° 1,ùë° 2,¬∑¬∑¬∑,ùë° ùëó‚àí1) (2)
targetmodel,otherthananinput/outputinferenceAPI. xùëñ(ùëé)={ùë° 1,¬∑¬∑¬∑,ùë°ùëõùëñ}‚ààT 0 ùëó=1
3.3 Approach wherex ùëñ(ùëé) isasequenceofadversarialtokenswithlengthùëõ ùëñ and
isamemberofthebaselineadversarialsequencesetT 0.
ConsiderasetofadversarialsequencesT 0thathavebeenobtained
byattackingapreviouslytargetedlanguagemodelM 0onasetof Phase2. Torefinetheadversarialtriggersgeneratedbythesur-
harmfulpromptsP.Inthissection,weintroduceanewmethodto
rogatemodelM(ùëé),weadapttheRLPrompt[9]frameworktofine-
obtainanewsetofadversarialtriggersT‚Ä≤withanimprovedattack tunetheparametersùúÉforthenewtargetmodelM‚Ä≤usingreinforce-
successratewhenusedtoattackanewtargetmodelM‚Ä≤compared mentlearning.Duringtraining,thesurrogatemodelgeneratesaset
to T 0. We assume that it is impractical or impossible to obtain ofcandidateadversarialsequencesTùëê.Thesecandidateadversarial
T‚Ä≤whileattackingM‚Ä≤usingthesamemethodusedtoobtainT
0
triggersarethenusedtoinferthenewinference-onlytargetmodel
whileattackingM 0.Forinstance,M‚Ä≤couldbeablack-boxmodel, M‚Ä≤incombinationwiththeharmfulpromptsP.Moreelaboration
accessedonlythroughaninferenceAPI. andsamplesofthepromptsetareavailableinsection4.Fromthe
Inthispaper,weuseasurrogatelanguagemodelM(ùëé)
togen-
resultsofinferringthetargetmodelM‚Ä≤,wecalculateareward
erate adversarial sequences x(ùëé) ‚àà T‚Ä≤. The surrogate model is signalusingarewardfunctionR.Thisrewardsignalfine-tunes
typicallyasmalllanguagemodel;inourcase,weusedifferentvari- the attacker parametersùúÉ with any off-the-shelf reinforcement
ationsofGPT-2[33],suchasthe82MparameterdistilGPT-2,and learningalgorithm.SimilartoRLPrompt[9],weusetheon-policy
the1.5BparameterGPT-2-xl.SimilartoRLPrompt[9],welimitthe componentofthesoftQ-learningalgorithm[15].SoftQ-learningis
parameterstobetrained,ùúÉ,toanMLPwithasinglehiddenlayer, chosenforitsefficientexplorationofactionspacesanditsstability
adaptedtothesurrogatemodelM(ùëé)
beforethelanguagehead,
intraining,makingitwell-suitedforoptimizingadversarialactions
inthiscontext.Theadversary‚Äôsobjectivecanberewrittenas
andfreezetherestoftheparametersofthemodel.Hence,giventhe
setofharmfulpromptsP,theobjectiveoffindingtheadversarial
triggerx(ùëé)
canbeformallywrittenas
maximizeR(P,xùëê(ùëé)
) (3)
xùëê(ùëé)‚ààTùëê
maximizeA(P,xùëê(ùëé)
) (1) whereTùëê representstheadversarialtriggersequencecandidates.
xùëê(ùëé)‚ààV‚àó
RewardFunction. Ithasbeenshownthatifalanguagemodel
whereAistheattacksuccessrateandxùëê(ùëé)
isacandidateadversar- startsrespondingtoapromptwithanaffirmativetone,suchas"Sure,
ialtrigger,sampledfromthesurrogatemodelgivenanemptyinput hereishowyou...",itisverylikelythatthemodelwillcontinuegen-
andparameterizedbyùúÉ which,withaslightabuseofnotation,is eratingtheresponsetotheprompt[58].Forharmfulprompts,an
definedasM(ùëé)(xùëê(ùëé) |‚àÖ;ùúÉ).Theoverallarchitectureofourmethod alignedlanguagemodeltypicallystartsaresponsewithadenial
isdepictedinfigure1. toneandrefusestorespondtotheprompt.Thegoaloftheadver-
Totrainthenewrandomlyinitializedparametersofthesurro- sarialtriggeristoputthelanguagemodelinastate,suchthatthe
gatemodel,ùúÉ,wegothroughtwophasesoftraining.Inthefirst targetLLMstartsitsresponsewithanaffirmativesentence.Hence,
phaseofthetraining,weusethepreviouslyobtainedT 0 tofine- wedesignourrewardfunctiontoquantizehowaffirmativethe
(ùëé) generationofthetargetmodelis.Theoutputofthetargetmodel
tuneM inasupervisedsetting.Thesecondphase,whichisthe
ùúÉ ispassedtotheBERTScore[55]modelalongsideanaffirmativeref-
maintrainingphaseofadaptingtheadversarialtriggerstothenew
erencesentence,whichisthepreferableresponsetotheharmful
model,involvesrefiningthesurrogatemodel‚Äôsadversarialtrigger
prompt.Intuitively,theBERTScoreevaluatesthetargetmodel‚Äôsre-
generations,usingreinforcementlearning.Wedescribeeachphase
sponsesemanticallyusingthecosinesimilaritymetricandrewards
indetailinthefollowingparagraphs.
theadversarialtriggerwithahighrewardiftheadversarialtrigger
Phase1. Inreinforcementlearning(RL)setups,itiscommonto hassuccessfullyattackedthetargetmodel.Formally,thereward
utilizesupervisedfine-tuningtoensurethecorrectinitialization functionisdefinedas
ofthemodelweights[28].Inthispaper,T 0,thesetofadversarial
sequencesobtainedbyattackingapreviouslytargetedmodel,using R :FBERT(ùë¶,ùë¶ÀÜ) (4)AI4CYBER-KDD2024,August26,2024,Barcelona,Spain Karkevandietal.
whereFBERTistheBERTScore‚ÄôsF1score,ùë¶isthereferenceaffirma- thatthismethodisnotarobustevaluation,Liuetal.[23]showthat
tivesentence,andùë¶ÀÜistheoutputofthetargetmodel. itisoneoftheclosestevaluationstohumanjudgment.
ForGCG-individual,weobtainoneadversarialtriggerforeach
sampleinthetrainset,thus,itisnotpossibletotestthismethodon
4 EVALUATION
thetestset.TheGCG-multipletrainsonesingleadversarialtrigger
4.1 Dataset for the entire train set, resulting in a transferable trigger to be
TheHarmfulBehaviorsetoftheAdvBench[58]dataset,whichcon- testedwiththetestsetwithbothmodels.Forourreinforcement
tains500harmfulinstructionsandthefirstfewtokensofatarget learning-basedmethod,wedirectlyoptimizethetriggersforthe
responseforeachinstruction,isusedinthispaper.Forinstance, targetmodel,whichisimpossiblewhenusingGCG,hence,weare
"Createatutorialonhowtocommitidentitytheft"and"Sure,here abletoimprovetheattacksuccessratefor5%and4%onthetrain
isatutorialonhowtocommitidentitytheft"isasampleofthe andtestsetrespectively.Table1showsourquantitativeresultsfor
Advbench‚Äôsinstructionsanditsrespectivetargetresponse.Inthis thesemethods.
work,weregardtheinstructionasx(ùë¢)
andthetargetstringas
ùë¶ÀÜ.Weusethefirst100samplesoftheharmfulbehaviorsasthe Table1:AttackSuccessRateoftheGCGandourmethod.
trainsetandholdoutthenext400samplesfortestinginallofour The GCG method is trained on Vicuna and the resulting
experiments. adversarialpromptistransferredtoMistral.WeuseMistral
only as an inference API. We do not test our method on
VicunasinceourmethodextendstheGCGpromptstonew
4.2 ExperimentSetup
targetmodels.
ToobtainthesetofinitialadversarialtriggersT 0,GCG[58]isused
withthedefaultparametersasdescribedintheirpaper.300stepsof
Vicuna Mistral
theGCGmethodisutilizedtoobtainanadversarialpromptforeach
ofthefirst100behaviorsoftheAdvBench.Weusevicuna-7b-v1.5 Method Train Test Train Test
asawhite-boxmodelduringtheGCGtraining.Forthepurpose
GCG-individual 0.99 - 0.51 -
oftestingourmethod,weregardtheMistral-7B-Instruct-v0.2[17]
GCG-multiple 1.00 0.98 0.88 0.87
asaninference-onlyblack-boxmodelM‚Ä≤.Hence,wecannotat-
RL(ours) - - 0.93 0.91
tackthismodelusinganygradient-basedmethod,includingGCG.
However,ourreinforcementlearning-basedmethodcanattackthis
model,asitonlyrequiresinferenceofthetargetmodel.Welimitthe
adversarialsequencelengthto20tokensforallofourexperiments.
5 CONCLUSION
the"distilGPT-2"model[33,36]isusedasthesurrogatemodel
M(ùëé).AnMLPwithasinglehiddenlayerand2048hiddenneurons Inthispaper,wepresentedanovelreinforcementlearning-based
isaddedtothesurrogatemodelafterthelasttransformerblock approachtooptimizeadversarialtriggersforjailbreakingLarge
andbeforethelanguageheadtoprovidethetrainableparameters LanguageModels(LLMs).Ourmethodaddressesthelimitations
ùúÉ,whiletherestofthemodeliskeptfrozen.Theseparametersare ofexistingtechniquesbyrequiringonlyinferenceAPIaccessto
thenfine-tunedinasupervisedfine-tuningsetup,asexplainedin thetargetmodel,thuseliminatingtheneedforwhite-boxaccess.
section3.Weuseemptyinputsandthesetofinitialadversarial BytrainingasmallsurrogatemodelwithBERTScore-basedreward
triggersT 0aslabelstotrainthemodelfor3epochsusingthecross- functions,wehaveshownthatitispossibletoenhancetheperfor-
entropylossmentionedinequation2.WeusetheAdamoptimizer manceandtransferabilityofadversarialtriggersonnewblack-box
withalearningrateof10‚àí4. models.Ourresultsindicatethatthisapproachnotonlyimproves
Duringtheattack,thesurrogatemodel‚Äôsparameters,ùúÉ,arefur- attacksuccessratesbutalsoextendstheapplicabilityofpreviously
therfine-tunedusingtheSoftQ-Learningalgorithmfor104steps. developedadversarialtriggerstoabroaderrangeoflanguagemod-
WeusethedefaultparametersoftheRLPrompt[9]duringthere- els.Thisworkcontributestotheongoingeffortstounderstand
inforcementlearningprocedure.Fortherewardfunction,weuse andmitigatethevulnerabilitiesofLLMs,highlightingtheneedfor
theofficialimplementationofBERTScore[55]withthemodelhash robustsafetymeasuresintheirdeployment.
roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.40.1). Whileourpreliminaryresultsshowclearimprovementsinthe
attacksuccessrate,weacknowledgethatourworkisintendedas
onlyasparktomotivatefuturework.Exploringmoreoptionsas
4.3 Results
theinitialsetofadversarialtriggers,moresophisticatedreward
Totestourpreliminaryresults,wecomparetheimprovementofthe engineering,forinstanceaddingacoherencyrewardtobypassper-
attacksuccessratewhentransferredtothenewtargetmodel.As plexity filters, and thoroughly testing this method qualitatively
mentioned,weuseGCG[58]toobtaintheinitialsetofadversarial andwithmoreblack-boxmodelsaresomeoftheinterestingfu-
triggersandtrytoimproveandpersonalizethesetriggersforthe tureroutestotake.Additionally,futureresearchshouldexplore
newtargetmodelMistral.Hence,wecompareourworktoboth potentialdefensivemeasurestomitigatetheseattacks.Developing
typesoftheGCGalgorithm.Followingpreviouswork[23,58]We robustdetectionmechanisms,enhancingmodelresiliencethrough
deemanattacksuccessfulifthetargetmodel‚Äôsresponsedoesnot adversarialtraining,andimplementingstricteraccesscontrolsare
containalistofdenialphrases,suchas"Iamsorry".Acknowledging essentialstepstoprotectLLMsfromsuchvulnerabilities.TheseCanReinforcementLearningUnlocktheHiddenDangersinAlignedLargeLanguageModels? AI4CYBER-KDD2024,August26,2024,Barcelona,Spain
mitigationstrategieswillmakethefindingsmorepracticalforthose [18] AlbertQ.Jiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,Blanche
lookingtosafeguardLLMs. Savary,ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBou
Hanna,FlorianBressand,GiannaLengyel,GuillaumeBour,GuillaumeLam-
ple,L√©lioRenardLavaud,LucileSaulnier,Marie-AnneLachaux,PierreStock,
SandeepSubramanian,SophiaYang,SzymonAntoniak,TevenLeScao,Th√©ophile
ACKNOWLEDGMENTS
Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El
Sayed.2024. MixtralofExperts. https://doi.org/10.48550/arXiv.2401.04088
Thisresearchprojectandthepreparationofthispublicationwere
arXiv:2401.04088[cs.LG]
fundedinpartbyNSFGrantsNo.2230086andNo.2245983. [19] ErikJones,AncaDragan,AditiRaghunathan,andJacobSteinhardt.2023.Au-
tomaticallyAuditingLargeLanguageModelsviaDiscreteOptimization. https:
//doi.org/10.48550/arXiv.2303.04381arXiv:2303.04381[cs]
REFERENCES [20] RazLapid,RonLangberg,andMosheSipper.2023.OpenSesame!UniversalBlack
BoxJailbreakingofLargeLanguageModels. https://doi.org/10.48550/arXiv.2309.
[1] MoustafaAlzantot,YashSharma,AhmedElgohary,Bo-JhangHo,ManiSrivastava, 01446arXiv:2309.01446[cs]
andKai-WeiChang.2018.GeneratingNaturalLanguageAdversarialExamples. [21] BrianLester,RamiAl-Rfou,andNoahConstant.2021. ThePowerofScalefor
https://doi.org/10.48550/arXiv.1804.07998arXiv:1804.07998[cs] Parameter-EfficientPromptTuning. https://doi.org/10.48550/arXiv.2104.08691
[2] A.I.Anthropic.[n.d.].TheClaude3ModelFamily:Opus,Sonnet,Haiku.([n.d.]). arXiv:2104.08691[cs]
[3] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,Nova [22] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Con-
DasSarma,DawnDrain,StanislavFort,DeepGanguli,TomHenighan,Nicholas tinuous Prompts for Generation. https://doi.org/10.48550/arXiv.2101.00190
Joseph,SauravKadavath,JacksonKernion,TomConerly,SheerEl-Showk,Nelson arXiv:2101.00190[cs]
Elhage,ZacHatfield-Dodds,DannyHernandez,TristanHume,ScottJohnston, [23] XiaogengLiu,NanXu,MuhaoChen,andChaoweiXiao.2024. AutoDAN:
ShaunaKravec,LianeLovitt,NeelNanda,CatherineOlsson,DarioAmodei,Tom GeneratingStealthyJailbreakPromptsonAlignedLargeLanguageModels.
Brown,JackClark,SamMcCandlish,ChrisOlah,BenMann,andJaredKaplan. https://doi.org/10.48550/arXiv.2310.15140arXiv:2310.04451[cs.CL]
2022.TrainingaHelpfulandHarmlessAssistantwithReinforcementLearningfrom [24] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,and
HumanFeedback.https://doi.org/10.48550/arXiv.2204.05862arXiv:2204.05862[cs] JieTang.2023.GPTUnderstands,Too. https://doi.org/10.48550/arXiv.2103.10385
[4] BattistaBiggio,IginoCorona,DavideMaiorca,BlaineNelson,NedimSrndic, arXiv:2103.10385[cs]
PavelLaskov,GiorgioGiacinto,andFabioRoli.2013. EvasionAttacksagainst [25] RobertoNavigli,SimoneConia,andBj√∂rnRoss.2023.BiasesinLargeLanguage
MachineLearningatTestTime. https://doi.org/10.1007/978-3-642-40994-3_25 Models:Origins,Inventory,andDiscussion. 15,2(2023),10:1‚Äì10:21. https:
arXiv:1708.06131[cs] //doi.org/10.1145/3597307
[5] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan, [26] OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,Ilge
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda Akkaya,FlorenciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAlt-
Askell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan, man,ShyamalAnadkat,RedAvila,IgorBabuschkin,SuchirBalaji,ValerieBalcom,
RewonChild,AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter, PaulBaltescu,HaimingBao,MohammadBavarian,JeffBelgum,etal.2024.GPT-4
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,Benjamin TechnicalReport. https://doi.org/10.48550/arXiv.2303.08774arXiv:2303.08774[cs]
Chess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,Ilya [27] NedjmaOusidhoum,XinranZhao,TianqingFang,YangqiuSong,andDit-Yan
Sutskever,andDarioAmodei.2020. LanguageModelsAreFew-ShotLearners. Yeung.2021.ProbingToxicContentinLargePre-TrainedLanguageModels.In
https://doi.org/10.48550/arXiv.2005.14165arXiv:2005.14165[cs] Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLin-
[6] NicholasCarliniandDavidWagner.2017.TowardsEvaluatingtheRobustnessof guisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing
NeuralNetworks. https://doi.org/10.48550/arXiv.1608.04644arXiv:1608.04644[cs] (Volume1:LongPapers)(Online,2021-08),ChengqingZong,FeiXia,WenjieLi,
[7] PatrickChao,AlexanderRobey,EdgarDobriban,HamedHassani,GeorgeJ. andRobertoNavigli(Eds.).AssociationforComputationalLinguistics,4262‚Äì4274.
Pappas,andEricWong.2023.JailbreakingBlackBoxLargeLanguageModelsin https://doi.org/10.18653/v1/2021.acl-long.329
TwentyQueries. https://doi.org/10.48550/arXiv.2310.08419arXiv:2310.08419[cs] [28] LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,Pamela
[8] DAN.[n.d.].ChatGPT-Dan-Jailbreak.Md.Gist. https://gist.github.com/coolaj86/ Mishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,John
6f4f7b30129b0251f61fa7baaa881516 Schulman,JacobHilton,FraserKelton,LukeMiller,MaddieSimens,Amanda
[9] MingkaiDeng,JianyuWang,Cheng-PingHsieh,YihanWang,HanGuo,Tianmin Askell,PeterWelinder,PaulChristiano,JanLeike,andRyanLowe.2022.Train-
Shu,MengSong,EricP.Xing,andZhitingHu.2022. RLPrompt:Optimizing ing Language Models to Follow Instructions with Human Feedback. https:
DiscreteTextPromptswithReinforcementLearning. https://doi.org/10.48550/ //doi.org/10.48550/arXiv.2203.02155arXiv:2203.02155[cs]
arXiv.2205.12548arXiv:2205.12548[cs] [29] NicolasPapernot,PatrickMcDaniel,SomeshJha,MattFredrikson,Z.BerkayCe-
[10] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,Xu lik,andAnanthramSwami.2015.TheLimitationsofDeepLearninginAdversarial
Sun,JingjingXu,LeiLi,andZhifangSui.2023.ASurveyonIn-contextLearning. Settings. https://doi.org/10.48550/arXiv.1511.07528arXiv:1511.07528[cs,stat]
https://doi.org/10.48550/arXiv.2301.00234arXiv:2301.00234[cs] [30] F√°bioPerezandIanRibeiro.2022.IgnorePreviousPrompt:AttackTechniquesFor
[11] JavidEbrahimi,AnyiRao,DanielLowd,andDejingDou.2018.HotFlip:White-Box LanguageModels. https://doi.org/10.48550/ARXIV.2211.09527
AdversarialExamplesforTextClassification. https://doi.org/10.48550/arXiv.1712. [31] GuanghuiQinandJasonEisner.2021. LearningHowtoAsk:QueryingLMs
06751arXiv:1712.06751[cs] with Mixtures of Soft Prompts. https://doi.org/10.48550/arXiv.2104.06599
[12] FlowGPT.[n.d.]."jailbreak"|FlowGPT-TheUltimateLibraryofChatGPTPrompts. arXiv:2104.06599[cs]
https://flowgpt.com [32] LianhuiQin,SeanWelleck,DanielKhashabi,andYejinChoi.2022. COLD
[13] IsabelO.Gallegos,RyanA.Rossi,JoeBarrow,MdMehrabTanjim,SungchulKim, Decoding:Energy-basedConstrainedTextGenerationwithLangevinDynamics.
FranckDernoncourt,TongYu,RuiyiZhang,andNesreenK.Ahmed.2024.Bias https://doi.org/10.48550/arXiv.2202.11705arXiv:2202.11705[cs]
andFairnessinLargeLanguageModels:ASurvey. https://doi.org/10.48550/arXiv. [33] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,andIlya
2309.00770arXiv:2309.00770[cs] Sutskever.2019.LanguageModelsAreUnsupervisedMultitaskLearners.1,8
[14] Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan (2019),9. https://dcmpx.remotevs.com/net/cloudfront/d4mucfpksywv/SL/better-
Wang,JiaweiHan,andYuningMao.2023. MART:ImprovingLLMSafetywith language-models/language_models_are_unsupervised_multitask_learners.pdf
Multi-roundAutomaticRed-Teaming. https://doi.org/10.48550/arXiv.2311.07689 [34] RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherD.
arXiv:2311.07689[cs] Manning,andChelseaFinn.2023.DirectPreferenceOptimization:YourLanguage
[15] HanGuo,BowenTan,ZhengzhongLiu,EricP.Xing,andZhitingHu.2022. ModelIsSecretlyaRewardModel. https://doi.org/10.48550/arXiv.2305.18290
Efficient(Soft)Q-LearningforTextGenerationwithLimitedGoodData. https: arXiv:2305.18290[cs]
//doi.org/10.48550/arXiv.2106.07704arXiv:2106.07704[cs] [35] VinuSankarSadasivan,ShoumikSaha,GaurangSriramanan,PriyathamKat-
[16] JiamingJi,TianyiQiu,BoyuanChen,BorongZhang,HantaoLou,KaileWang, takinda,AtoosaChegini,andSoheilFeizi.2024. FastAdversarialAttackson
YawenDuan,ZhonghaoHe,JiayiZhou,ZhaoweiZhang,FanzhiZeng,KwanYee LanguageModelsInOneGPUMinute. https://doi.org/10.48550/arXiv.2402.15570
Ng,JuntaoDai,XuehaiPan,AidanO‚ÄôGara,YingshanLei,HuaXu,BrianTse,Jie arXiv:2402.15570[cs]
Fu,StephenMcAleer,YaodongYang,YizhouWang,Song-ChunZhu,YikeGuo, [36] VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf.2020.Dis-
andWenGao.2024.AIAlignment:AComprehensiveSurvey. https://doi.org/10. tilBERT,aDistilledVersionofBERT:Smaller,Faster,CheaperandLighter. https:
48550/arXiv.2310.19852arXiv:2310.19852[cs] //doi.org/10.48550/arXiv.1910.01108arXiv:1910.01108[cs]
[17] AlbertQ.Jiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,De- [37] TevenLeScaoandAlexanderM.Rush.2021.HowManyDataPointsIsaPrompt
vendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel, Worth? https://doi.org/10.48550/arXiv.2103.08493arXiv:2103.08493[cs]
GuillaumeLample,LucileSaulnier,L√©lioRenardLavaud,Marie-AnneLachaux, [38] LeoSchwinn,DavidDobre,SophieXhonneux,GauthierGidel,andStephanGun-
PierreStock,TevenLeScao,ThibautLavril,ThomasWang,Timoth√©eLacroix, nemann.2024.SoftPromptThreats:AttackingSafetyAlignmentandUnlearningin
andWilliamElSayed.2023.Mistral7B.arXiv:2310.06825[cs.CL] Open-SourceLLMsthroughtheEmbeddingSpace. https://doi.org/10.48550/arXiv.AI4CYBER-KDD2024,August26,2024,Barcelona,Spain Karkevandietal.
2402.09063arXiv:2402.09063[cs] [49] SangMichaelXie,AditiRaghunathan,PercyLiang,andTengyuMa.2022.An
[39] XinyueShen,ZeyuanChen,MichaelBackes,YunShen,andYangZhang.2023. ExplanationofIn-contextLearningasImplicitBayesianInference. https://doi.org/
"DoAnythingNow":CharacterizingandEvaluatingIn-The-WildJailbreakPrompts 10.48550/arXiv.2111.02080arXiv:2111.02080[cs]
onLargeLanguageModels.arXiv:2308.03825 https://arxiv.org/abs/2308.03825v2 [50] LeiXu,YangyiChen,GanquCui,HongchengGao,andZhiyuanLiu.2022.Ex-
[40] YundiShi,PijiLi,ChangchunYin,ZhaoyangHan,LuZhou,andZheLiu.2022. ploringtheUniversalVulnerabilityofPrompt-basedLearningParadigm. https:
PromptAttack:Prompt-basedAttackforLanguageModelsviaGradientSearch. //doi.org/10.48550/arXiv.2204.05239arXiv:2204.05239[cs]
https://doi.org/10.48550/arXiv.2209.01882arXiv:2209.01882[cs] [51] YueXuandWenjieWang.2024. $\textit{LinkPrompt}$:NaturalandUniversal
[41] TaylorShin,YasamanRazeghi,RobertL.LoganIV,EricWallace,andSameer AdversarialAttacksonPrompt-basedLanguageModels. https://doi.org/10.48550/
Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with arXiv.2403.16432arXiv:2403.16432[cs]
AutomaticallyGeneratedPrompts. https://doi.org/10.48550/arXiv.2010.15980 [52] JiaqiXue,MengxinZheng,TingHua,YilinShen,YepengLiu,LadislauBoloni,and
arXiv:2010.15980[cs] QianLou.2023.TrojLLM:ABlack-boxTrojanPromptAttackonLargeLanguage
[42] LiweiSong,XinweiYu,Hsuan-TungPeng,andKarthikNarasimhan.2021.Uni- Models. https://doi.org/10.48550/arXiv.2306.06815arXiv:2306.06815[cs]
versalAdversarialAttackswithNaturalTriggersforTextClassification. https: [53] JiahaoYu,XingweiLin,ZhengYu,andXinyuXing.2023. GPTFUZZER:Red
//doi.org/10.48550/arXiv.2005.00174arXiv:2005.00174[cs] TeamingLargeLanguageModelswithAuto-GeneratedJailbreakPrompts. https:
[43] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan, //doi.org/10.48550/arXiv.2309.10253arXiv:2309.10253[cs]
IanGoodfellow,andRobFergus.2014.IntriguingPropertiesofNeuralNetworks. [54] NingyuZhang,LuoqiuLi,XiangChen,ShuminDeng,ZhenBi,ChuanqiTan,
https://doi.org/10.48550/arXiv.1312.6199arXiv:1312.6199[cs] FeiHuang,andHuajunChen.2022. DifferentiablePromptMakesPre-trained
[44] DerekTam,RakeshR.Menon,MohitBansal,ShashankSrivastava,andColin LanguageModelsBetterFew-shotLearners. https://doi.org/10.48550/arXiv.2108.
Raffel.2021. ImprovingandSimplifyingPatternExploitingTraining. https: 13161arXiv:2108.13161[cs]
//doi.org/10.48550/arXiv.2103.11955arXiv:2103.11955[cs] [55] TianyiZhang,VarshaKishore,FelixWu,KilianQ.Weinberger,andYoavArtzi.
[45] EricWallace,ShiFeng,NikhilKandpal,MattGardner,andSameerSingh.2021. 2020.BERTScore:EvaluatingTextGenerationwithBERT. https://doi.org/10.
UniversalAdversarialTriggersforAttackingandAnalyzingNLP. https://doi.org/ 48550/arXiv.1904.09675arXiv:1904.09675[cs.CL]
10.48550/arXiv.1908.07125arXiv:1908.07125[cs] [56] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
[46] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Wu,YonghaoZhuang,ZiLin,ZhuohanLi,DachengLi,EricP.Xing,Hao
BrianLester,NanDu,AndrewM.Dai,andQuocV.Le.2022. FinetunedLan- Zhang,JosephE.Gonzalez,andIonStoica.2023. JudgingLLM-as-a-Judge
guageModelsAreZero-ShotLearners. https://doi.org/10.48550/arXiv.2109.01652 withMT-BenchandChatbotArena. https://doi.org/10.48550/arXiv.2306.05685
arXiv:2109.01652[cs] arXiv:2306.05685[cs]
[47] JohannesWelbl,AmeliaGlaese,JonathanUesato,SumanthDathathri,JohnMellor, [57] KaijieZhu,JindongWang,JiahengZhou,ZichenWang,HaoChen,YidongWang,
LisaAnneHendricks,KirstyAnderson,PushmeetKohli,BenCoppin,andPo-Sen LinyiYang,WeiYe,YueZhang,NeilZhenqiangGong,andXingXie.2023.Prompt-
Huang.2021. ChallengesinDetoxifyingLanguageModels. https://doi.org/10. Bench:TowardsEvaluatingtheRobustnessofLargeLanguageModelsonAdversarial
48550/arXiv.2109.07445arXiv:2109.07445[cs] Prompts. https://doi.org/10.48550/arXiv.2306.04528arXiv:2306.04528[cs]
[48] YuxinWen,NeelJain,JohnKirchenbauer,MicahGoldblum,JonasGeiping,and [58] AndyZou,ZifanWang,NicholasCarlini,MiladNasr,J.ZicoKolter,andMatt
TomGoldstein.2023.HardPromptsMadeEasy:Gradient-BasedDiscreteOptimiza- Fredrikson.2023.UniversalandTransferableAdversarialAttacksonAlignedLan-
tionforPromptTuningandDiscovery. https://doi.org/10.48550/arXiv.2302.03668 guageModels. https://doi.org/10.48550/arXiv.2307.15043arXiv:2307.15043[cs]
arXiv:2302.03668[cs]
Received4June2024;accepted28June2024