BioMamba: A Pre-trained Biomedical Language Representation
Model Leveraging Mamba
Ling Yue1, Sixue Xing1, Yingzhou Lu2, and Tianfan Fu∗1
1
Computer Science Department, Rensselaer Polytechnic Institute
2School of Medicine, Stanford University
August 6, 2024
Abstract
Theadvancementofnaturallanguageprocessing(NLP)inbiologyhingesonmodels’abilitytointerpret
intricate biomedical literature. Traditional models often struggle with the complex and domain-specific
language in this field. In this paper, we present BioMamba, a pre-trained model specifically designed for
biomedical text mining. BioMamba builds upon the Mamba architecture and is pre-trained on an ex-
tensive corpus of biomedical literature. Our empirical studies demonstrate that BioMamba significantly
outperforms models like BioBERT and general-domain Mamba across various biomedical tasks. For in-
stance, BioMamba achieves a 100× reduction in perplexity and a 4× reduction in cross-entropy loss on
the BioASQ[29] test set. We provide an overview of the model architecture, pre-training process, and
fine-tuningtechniques. Additionally,wereleasethecodeandtrainedmodel1 tofacilitatefurtherresearch.
1 Introduction
Recent advancements in natural language processing (NLP) have led to the creation of pre-trained models
like BERT (Bidirectional Encoder Representations from Transformers) [11] and GPT (Generative Pre-trained
Transformer)[4], whichhavesignificantlyenhancedperformanceacrossavarietyofNLPtasks. BERTutilizes
a Transformer [32] encoder architecture to consider the bidirectional context of words in a sentence, leading
to more accurate and nuanced language representations. On the other hand, GPT employs a unidirectional,
Transformer-decoder architecture [32] to generate text by predicting the next word in a sentence based on the
preceding words.
Despite their success, these Transformer-based models face computational inefficiencies, especially with
longsequencesduetotheirquadraticcomplexityinsequencelength. Toaddresstheselimitations,theMamba
model [16] leverages structured state space models (SSMs) with parameters that are functions of the input,
offering linear complexity in sequence length and making them more efficient for handling long sequences.
This innovation makes Mamba a compelling choice for applications requiring efficient and scalable sequence
modeling.
The exponential increase in biomedical literature necessitates the development of efficient and accurate
text-mining tools to extract valuable insights [13, 7]. Traditional models often struggle to comprehend the
complexanddomain-specificlanguageprevalentinbiomedicaltexts[20]. Consequently,thereisacriticalneed
for advanced models capable of effectively handling biomedical text mining tasks, especially learning from an
unlabeled text corpus [28].
In response to these challenges, we propose a biomedical variant of the Mamba model [16], a cutting-edge
pretrained language model. BioMamba is pre-trained on PubMed (a database of biomedical literature) [5]
abstracts and fine-tuned for specific biomedical tasks such as biomedical question answering. Our main con-
tributions are listed as follows:
• Development of BioMamba: We introduce BioMamba, a domain-specific adaptation of the Mamba
model that is initialized with Mamba and then fine-tuned for biomedical text mining.
∗Correspondingauthor. fut2@rpi.edu,yuel2@rpi.edu
1https://github.com/LeoYML/BioMamba
1
4202
guA
5
]LC.sc[
1v00620.8042:viXra• EmpiricalEvaluation: WedemonstrateBioMamba’ssuperiorityoverexistingmethods(e.g.,BioBERT,
BioGPT,andgeneral-purposeMamba)throughextensiveexperimentsonvariousbiomedicalNLPtasks.
Specifically, compared with Mamba, BioMamba achieves more than 100× reduction in perplexity and
over 4× reduction in cross-entropy loss in the test set of PubMed.
• Public Release: We publicly release the well-trained BioMamba model2 on Hugging Face to facilitate
biomedical research.
BioMamba represents a significant step forward in the application of NLP to biomedical text mining,
offering enhanced performance and a deeper understanding of complex natural language in the biomedical
domain.
2 Related Work
The evolution of neural network architectures for handling sequence data has seen significant advancements
fromRecurrentNeuralNetworks(RNNs)toTransformers[32],andsubsequentlytoBidirectionalEncoderRep-
resentations from Transformers (BERT) [11], GPT (Generative Pre-trained Transformer) [4], and Structured
State Space Sequence (S4) (including Mamba) [16] recently. In the biomedical domain, these advancements
have been further specialized to address the unique challenges posed by biomedical texts, leading to the
development of models such as BioBERT [22], PubMedBERT [17], BioGPT [27], and ClinicalMamba [34].
Recurrent neural network (RNN). To handle sequence data, the recurrent neural network (RNN) was
designed, originally for natural language [19, 10]. Token is the basic unit of a sequence. The set of all the
tokens is called the vocabulary. We suppose the sequence (e.g., sentence) of interest has T tokens, i.e., the
length of the sequence is T. The sequence data can be formulated as x(1),x(2),··· ,x(T), where x(t) is the
input feature vector at time t (i.e., the t-th element in the sequence). We use h(1),h(2),··· ,h(T) to denote
the hidden state (latent variable) at different times. Generally, the RNN can be formulated as
h(t) =f (x(t),h(t−1)), o(t) =f (h(t)), (1)
1 2
where the current hidden state h(t) relies on both previous hidden state h(t−1) and the current input x(t),
wheref (·)andf (·)arebothneuralnetworks. However, whentraditionalRNNsareunrolled, theycanresult
1 2
inaverydeepneuralnetwork,withadepthequaltothelengthofthesequence. Thiscanleadtothevanishing
gradient problem, particularly for long sequences.
(a) RNN (b) Transformer
Figure 1: RNN versus Transformer.
2https://huggingface.co/LeoYML/biomamba-130m
2Transformer. The inefficiency in training RNNs is primarily due to their sequential nature. To tackle
this challenge, a revolutionary deep-learning architecture called the Transformer was introduced [32]. The
Transformer model leverages a self-attention mechanism to identify the crucial features in the input data and
performsparallelprocessingofthecompletesequence,thuseliminatingthesequentialconstraintandimproving
training efficiency. Its popularity have made it a widely adopted solution for various sequence data, including
natural language processing [32], computer vision [2], speech recognition [12, 23].
Next, we present the feedforward mechanism of the Transformer block. The transformer block does not
change the length of the sequence; thus, multiple transformers can be stacked to compose a deep model.
A transformer block is a parameterized function denoted Z = f (X) : RT×d →− RT×d, where input and
θ
output of transformers are denoted X ∈ RT×d and Z ∈ RT×d, respectively. X can be decomposed as X =
(cid:16) (cid:17) (cid:16) (cid:17)
x⊤ ;x⊤ ;··· ;x⊤ ∈RT×d,wherex(t)denotesthetokenembeddingatthet-thstep. Z= z⊤ ;z⊤ ;··· ;z⊤ ∈
(1) (2) (T) (1) (2) (T)
RT×d are the sequence of learned representations, which have the same size as the input sequence X. (1).
Positional embedding. The position of a token in the sequence can be important. To incorporate po-
sitional information of a sequence, it designs an embedding vector to differentiate the relative positions.
Each index is assigned an embedding vector and is added to the input feature. It is formally defined as
X(cid:101) =position embedding(X)=(cid:16) x⊤ ;x⊤ ;··· ;x⊤ (cid:17) +(cid:0) p(1)⊤;p(2)⊤;··· ;p(T)⊤(cid:1) ∈RT×d, where p(t)∈Rd
(1) (2) (T)
denotes the positional embedding for the t-th token in the sequence, which is learnable. The positional em-
bedding p can be the index in the sequence to reflect the absolute position in the sequence or some patterns
to indicate the relative position in the sequence, such as Sine or Cosine functions. (2). Multi-head self-
attention. Self-attention is used to capture the correlated features in the sequence data and multi-head
attention enhances the attention model by repetition. It is formally defined as
(cid:18)
(XW )(XW
)⊤(cid:19)
U=softmax Q√ K X(cid:101)W
V
∈RT×d, (2)
d
(cid:124) (cid:123)(cid:122) (cid:125)
attentionweight, RT×T
where W ,W ,W ∈ RT×d are trainable weight matrices. (3). Layer normalization is used to enhance
Q K V
training stability and efficiency. It evaluates the mean value and standard deviation on the whole batch on
each feature dimension. Then, we subtract the mean and divide by the standard deviation on all the feature
dimensions and all the data. After the layer normalization, the data distribution on each feature dimension
approximately follows the unit normal distribution, whose mean is 0 and variance is 1.
Self-supervised learning and pretraining. Self-supervised learning (SSL) is a learning paradigm that
derives supervision from unlabeled data. For instance, it might mask a subset of the input features and
then predict the masked subset based on the surrounding context. SSL is effective for learning meaningful
representations from large amounts of unlabeled data and is often used as a pretraining strategy. The SSL-
pretrained model provides a warm start and can then be fine-tuned for downstream tasks, such as supervised
learning with limited labeled data. BERT is a well-known example of pretraining model [11].
BERT BidirectionalEncoderRepresentationsfromTransformers(BERT)isapowerfulpretrainingtechnique
thathasitsrootsintheTransformerarchitectureandwasspecificallydesignedfornaturallanguageprocessing
(NLP) tasks [11]. BERT is constructed by stacking multiple layers of Transformer blocks. The output of each
layer is used as the input to the subsequent layer, thus allowing the model to learn increasingly complex
representations of the input data. This technique results in a deep, bidirectional architecture that is capable
of capturing contextual information from both the past and future tokens in a sequence. BioBERT achieves
state-of-the-art performance in biomedical text mining [22].
GPT Unlike BERT, which uses bidirectional Transformer encoder architecture, GPT (Generative Pre-
trained Transformer) [4] employs a unidirectional Transformer decoder architecture, which is good at gen-
erating coherent and contextually relevant text, making it ideal for text generation, language translation, and
conversationalagents. GPTispretrainedonextensivetextcorporausingalanguagemodelingobjective,which
involves predicting the next word in a sequence.
Structured State Space for Sequences model. Structured State Space for Sequences (S4) addresses
the limitations of Recurrent Neural Networks (RNNs) and Transformers in handling long-range dependencies
3and computational efficiency. S4 leverages state space models (SSMs) for capturing long-term dependencies
with linear complexity, unlike RNNs with vanishing gradients and Transformers with quadratic complexity.
S4 allows parallel processing similar to Transformers but with linear complexity. The core of S4 is the state
space representation:
x =Ax +Bu , (3)
t+1 t t
y =Cx +Du , (4)
t t t
where x is the hidden state, u the input, y the output, and A, B, C, D are learned parameters. This for-
t t t
mulationenablesefficientsequencemodelingthroughthelinearstructureofSSMs. Mamba[16]isaspecialized
implementation within the S4 framework. It uses SSMs with dynamic parameters as functions of the input,
allowing selective propagation or forgetting of information. The dynamic parameters are represented as:
A =f (u ), (5)
t A t
B =f (u ), (6)
t B t
C =f (u ), (7)
t C t
D =f (u ), (8)
t D t
wheref ,f ,f ,andf dynamicallyadjustparametersbasedonu . BothS4andTransformersallowparallel
A B C D t
processing but differ in handling long-range dependencies and computational complexity. Transformers use
self-attention with quadratic complexity, whereas S4 uses state space representations with linear complexity,
making it more scalable. S4 inherently captures long-term dependencies through state transitions, unlike
Transformersthatrequirepositionalencodings. MambaenhancesS4byintroducingdynamicparameterization,
adapting state space parameters based on input at each time step. This enables Mamba to handle varying
contextsandselectivelypropagateorforgetinformation,improvingcontent-basedreasoning. WhileS4provides
a robust foundation, Mamba’s dynamic nature makes it more versatile for adaptive processing. Mamba’s
success in various applications, such as computer vision [35, 21], natural language processing [33, 18], and
speech processing [30], demonstrates its versatility and effectiveness.
S4 versus Transformer. The S4 model and Transformers both support efficient parallel processing, but
they differ in their handling of dependencies. Transformers use self-attention mechanisms to capture depen-
dencies within sequences. In contrast, S4 models leverage state space models to explicitly capture long-range
dependencies.
Biomedical NLP Models. In the biomedical domain, specialized models have been developed to handle
the unique challenges posed by biomedical texts. BioBERT [22] is a domain-specific variant of BERT, pre-
trained on large-scale biomedical corpora like PubMed abstracts and PMC full-text articles. It has shown
significantimprovementsinvariousbiomedicalNLPtasks,suchasnamedentityrecognition(NER)andrelation
extraction. Similarly, PubMedBERT [17] is another BERT-based model pre-trained exclusively on PubMed
abstracts, achieving state-of-the-art performance in several biomedical benchmarks.
BioGPT[27]adaptstheGPTarchitectureforbiomedicaltextgenerationandunderstanding,leveragingthe
unidirectionalnatureofGPTtogeneratecoherentbiomedicaltextandanswerbiomedicalquestionseffectively.
ClinicalMamba [34] is a recent adaptation of the Mamba model, fine-tuned for clinical text mining tasks,
demonstrating the versatility and efficiency of the S4 framework in the biomedical domain.
Thesemodelshighlighttheimportanceofdomain-specificpretrainingandfine-tuninginachievingsuperior
performance in biomedical NLP tasks, addressing the unique challenges of biomedical language and providing
valuable tools for researchers and practitioners in the field.
3 Method
In this section, we briefly discuss the recently proposed Mamba model and then describe in detail the pre-
training and fine-tuning processes of BioMamba. BioMamba retains the fundamental structure of Mamba.
The whole framework is demonstrated in Figure 2.
4Figure 2: (left) Comparison of BERT and Mamba. BERT is pretrained on large corpora using masked
language modeling [11], its bidirectional nature provides a deep understanding of the context within the text.
Differently,Mambaisanautoregressivemodel(unidirectional)andispretrainedonunlabeledlanguagecorpus
by predicting the next token [16]. (right) BioMamba: First, BioMamba uses the parameters of the general-
purpose Mamba for initialization. Then, BioMamba is further pretrained on a biomedical text corpus (e.g.,
PubMed [5]). After that, BioMamba can be fine-tuned on downstream tasks (e.g., the question-answering
BioASQ dataset).
3.1 Mamba Model
The Mamba model is a state-of-the-art sequence model designed to address the computational inefficiencies of
Transformers on long sequences [16]. Traditional Transformers [32] rely heavily on the attention mechanism,
which, while powerful, suffers from quadratic complexity in sequence length. This makes Transformers com-
putationally expensive and memory-intensive, particularly for long sequences. To overcome these limitations,
Mamba leverages structured state space models (SSMs) with parameters that are functions of the input.
SSMs offer a compelling alternative to attention mechanisms by providing a framework for modeling se-
quenceswithlinearcomplexityinsequencelength. Thismakesthemmoreefficientforhandlinglongsequences.
Additionally, SSMs arewell-suitedfor content-based reasoning, whichis crucialfor discretemodalities suchas
language. The core innovation of Mamba lies in its ability to selectively propagate or forget information along
the sequence length dimension based on the current token. This is achieved by making the SSM parameters
dynamic and dependent on the input sequence. Mathematically, this can be represented as:
h =SSM(h ,x ;θ(x )), (9)
t t−1 t t
where h is the hidden state at time step t, x is the input token at time step t, and θ(x ) represents the
t t t
dynamic parameters of the SSM that are functions of the input token x .
t
DespitethedynamicnatureoftheSSMparameters,whichpreventstheuseofefficientconvolutions,Mamba
employs a hardware-aware parallel algorithm in recurrent mode to maintain computational efficiency. This
design choice ensures that Mamba can achieve fast inference with linear scaling in sequence length.
Furthermore, Mamba integrates these selective SSMs into a simplified end-to-end neural network architec-
turethatdoesnotrelyonattentionmechanismsorevenmulti-layerperceptron(MLP)blocks. Thisstreamlined
architecture contributes to Mamba’s high throughput and scalability.
In summary, the Mamba model offers a novel approach to sequence modeling by combining the strengths
of SSMs with dynamic parameterization and hardware-aware parallelism. By addressing the inefficiencies of
Transformers and leveraging the advantages of SSMs, Mamba achieves state-of-the-art performance across
variousmodalities, includinglanguage, audio, andgenomics, whilemaintainingcomputationalefficiency. This
makes Mamba a compelling choice for applications requiring efficient and scalable sequence modeling.
3.2 Pre-training BioMamba
Pretraining is a crucial technique for learning meaningful representations from large amounts of unlabeled
data. This initial training phase enables the model to capture general patterns, structures, and features from
5the data without the need for labeled examples. The pretrained model provides a warm start, which can be
fine-tuned for specific downstream tasks.
Inthispaper,BioMambaisinitializedwithweightsfromtheMamba-130mmodel3. Thepretrainingprocess
involves further training on a large corpus of biomedical texts, including PubMed abstracts. PubMed [5] is
a free and comprehensive database primarily used for accessing references and abstracts on life sciences and
biomedical topics. It is an essential resource for researchers, healthcare professionals, and students, providing
access to a vast collection of scientific literature.
WhilethegeneralMambamodelmayhaveencounteredsomebiomedicaldataduringitsinitialtraining,the
proportionofsuchdataistypicallyverysmall. Therefore, furtherpretrainingonatargetedbiomedicalcorpus
is necessary to enhance the model’s ability to capture domain-specific patterns and terminologies. Studies
have shown that increasing the proportion of domain-specific data during pretraining significantly improves
performance on related tasks [3, 22, 17].
ThepretrainingobjectiveforBioMamba,aswithstandardautoregressivemodels,isnext-tokenprediction.
This objective is formulated as:
T
(cid:88)
L=− logP(x |x ;θ), (10)
t <t
t=1
where x is the t-th token in the sequence, x represents all tokens preceding x , and θ denotes the model
t <t t
parameters. This objective allows the model to learn the likelihood of each token given its preceding context,
thereby capturing the sequential dependencies within the biomedical text corpus.
ByleveragingthepretrainedMamba-130mmodelandfurthertrainingondomain-specificbiomedicaltexts,
BioMambaisabletoeffectivelycapturetheintricatepatternsandterminologiesuniquetothebiomedicalfield.
This enhances its performance on downstream biomedical tasks, making it a powerful tool for applications in
life sciences and healthcare.
3.3 Fine-tuning BioMamba
Fine-tuning is the subsequent phase where the pretrained model is adapted to a specific downstream task
usingasmaller,labeleddataset. Thisinvolvessupervisedlearning,wherethemodel’sparametersareadjusted
to optimize performance for the task at hand. After pretraining, BioMamba can be fine-tuned on specific
downstream tasks in a supervised manner. In this paper, we focus on the question-answering (QA) task.
The Stanford Question Answering Dataset (SQuAD) [31] is a widely-used benchmark for training and
evaluating natural language processing models, particularly for QA systems. This dataset is designed to test
a model’s ability to understand and extract relevant information from a given text passage. To align with
the methodology presented in the BioBERT paper [22], we utilized the BioASQ factoid datasets, which were
reformatted to match the structure of the SQuAD dataset.
Thefine-tuningprocessinvolvestrainingBioMambaontheBioASQdataset(describedlaterinSection4.1),
which contains biomedical QA pairs. The QA task can be formulated as:
N
(cid:88)
L =− logP(a |H), (11)
QA i
i=1
where a represents the answer tokens, N is the number of tokens in the answer sentence, and H denotes
i
the contextualized embeddings from the transformer encoder. This learning objective allows the model to
optimize the probability of the correct answer given the contextualized embeddings, thereby enhancing its
ability to accurately extract relevant information from biomedical texts.
Byfine-tuningBioMambaontheBioASQdataset,weaimtoleveragethemodel’spretrainedknowledgeand
adaptittothespecificrequirementsofbiomedicalquestionanswering. ThisapproachensuresthatBioMamba
can effectively address the nuances of biomedical language and provide accurate answers to domain-specific
questions.
3https://huggingface.co/state-spaces/mamba-130m-hf
64 Experiments
Inthissection,wedemonstrateexperimentalresults. Specifically,webrieflydescribethedatasetsinSection4.1.
Then, we elaborate on the experimental setup in Section 4.2. The experimental results are presented in
Section 4.3. Additionally, we provide a case study in Section 4.4.
4.1 Dataset
Table 1: Statistics of BioASQ factoid datasets (biomedical question answering datasets).
Dataset # train # test
BioASQ 4b-factoid 327 161
BioASQ 5b-factoid 486 150
BioASQ 6b-factoid 618 161
TheStanfordQuestionAnsweringDataset(SQuAD)[31]servesasabenchmarkfortrainingandevaluating
naturallanguageprocessing(NLP)models,particularlyintherealmofquestionanswering(QA)systems. This
dataset is created to assess a model’s proficiency in comprehending and extracting pertinent information from
agiventextpassage. InalignmentwiththemethodologyoutlinedintheBioBERTpaper[22],weemployedthe
BioASQfactoiddatasets, whichwerereformattedtomatchthestructureoftheSQuADdataset[31]. Detailed
information is presented in Table 1. We utilized complete abstracts (PMIDs) along with the corresponding
questions and answers as provided by the BioASQ organizers. The pre-processed BioASQ datasets have been
made publicly accessible.
Foralldatasets,weadheredtothesamedatasetsplitsasutilizedinpriorstudiestoensureafairevaluation.
We present a comparative analysis of Mamba [16] and BioMamba against the current state-of-the-art
models, reporting their respective performance metrics. Notably, Mamba and BioMamba share an identical
architecture and exclusively utilize the gold standard datasets without incorporating any additional data
sources.
4.2 Experimental Setup
Table 2: Different models’ sizes.
Model Name Model Size (MB)
Mamba-130m [16] 123
BioMamba (ours) 123
BioBert [22] 103
BioGPT [27] 346
Hyperparameter setup. We used the Mamba-130m model pre-trained on the Pile dataset [15] with the
GPT2 tokenizer. The model sizes are shown in Table 2, with sizes comparable to BioBERT and smaller than
BioGPT. Hyperparameters such as batch size and learning rate scheduling for pre-training BioMamba are the
same as those for pre-training Mamba.
The BioMamba model was pre-trained with a parameter count of 124 million. The architecture consists of
12 layers, each with a model dimension of 768, same as BERT [11] and Mamba [16]. The model employs 12
attention heads, with each head having a dimension of 64. The training process involved 4800 steps, with a
learning rate set at 6e-4 and a batch size of 0.5 million tokens, totaling 2.5 billion tokens.
The training utilized the AdamW optimizer [24], configured with a gradient clip value of 1.0 and a weight
decay of 0.1. The training regimen did not include dropout. A linear learning rate warmup with cosine decay
was applied, starting from a peak learning rate following the GPT-3 specification and decaying to 1e-5. The
peakvaluewassetatfivetimestheGPT-3value. Additionally,nolinearbiastermswereused,andRMSNorm
7was employed instead of LayerNorm. The AdamW optimizer’s [24] hyperparameters were set to B = (0.9,
0.95), following the GPT-3 configuration, as opposed to the PyTorch default of B = (0.9, 0.999).
BioMamba supports a context length of 2048 and was pre-trained autoregressively. The standard autore-
gressive task aimed to predict the next token, using cross-entropy loss as the objective function.
Evaluation Metrics. For evaluating model performance on the BioASQ datasets, we utilize two impor-
tant metrics: Accuracy (ACC) and Mean Reciprocal Rank (MRR). These metrics provide a comprehensive
assessment of the models’ capabilities in different aspects:
• Accuracy (ACC): This metric measures the proportion of exact matches between the predicted and
the correct answers. It is a stringent measure that requires the predicted answer to be exactly the same
as the ground truth.
• Mean Reciprocal Rank (MRR): This metric evaluates the rank of the correct answer among the
predicted answers. It is the average of the reciprocal ranks of the correct answers, providing insight into
how high the correct answers are ranked by the model.
Forallbaselines,wereporttheAccuracy(ACC)andMeanReciprocalRank(MRR)scoresoneachdataset
to facilitate a detailed comparison of their performance. For both scores, higher values indicate better perfor-
mance.
In addition to these metrics, for autoregressive models such as BioGPT and Mamba, we utilize Perplexity
(PPL)andcross-entropy(CE)asfurtherevaluationmetrics. Perplexityisameasureofhowwellaprobability
model predicts a sample and is specifically defined to assess the likelihood of words in a sequence. Mathe-
matically, perplexity is the exponential of the average negative log-likelihood, which quantifies the model’s
uncertainty in predicting the next word [1]. Formally, it is defined as:
N
(cid:110) 1 (cid:88) (cid:111)
Perplexity(S)=exp − logQ(s |S ) , (12)
N i <i
i=1
whereSrepresentsasentence,s denotesthei-thwordinS,andS referstothesequenceofwordspreceding
i <i
thei-thwordinS. ThetermQ(s |S )istheconditionalprobabilityofthei-thwordgiventheprecedingwords,
i <i
as predicted by a well-trained language model. Lower perplexity values indicate better model performance, as
they reflect higher likelihoods of the observed sequences.
The cross-entropy (CE) function measures the distance between two categorical probability distributions
and is a popular loss function for classification. That is,
K
(cid:88)
cross-entropy(y,y˜)= y logy˜, (13)
k k
k=1
where y = [y ,··· ,y ]⊤ ∈ {0,1}K is a K-dimensional vector, where only one element is 1 while others are
1 K
0. This is known as a one-hot vector. If the k-th element is 1, then it indicates the data belongs to the k-th
category. y =[y ,··· ,y ]⊤ ∈(0,1)K is K-dimensional vector, where the k-th element denotes the predicted
(cid:101) (cid:101)1 (cid:102)K
probability between 0 and 1 that the data point belongs to the k-th category. The sum of all the elements is
equal to 1. Lower cross-entropy loss indicates better performance.
Hardware. Duringthepre-trainingphase,weutilizedaclusterofeightNVIDIAV100GPUs,eachequipped
with 32GB of memory. Due to the high computational demands, our experiments were focused solely on the
Mamba-130m model.
For the fine-tuning stage, we transitioned to a single NVIDIA A5000 GPU with 24GB of memory to
fine-tune BioMamba specifically for the BioASQ task. It’s important to note that the fine-tuning process is
significantly less computationally intensive compared to the pre-training phase.
4.3 Results & Discussion
FromtheresultsreportedinTable3,BioMambaachievesthehighestperformance,significantlyoutperforming
Mambatrainedonageneraldataset. Thishighlightstheimportanceofdomain-specifictraining. Additionally,
8Table 3: Biomedical question answering test results.
Datasets Metrics BioBert BioGPT Mamba BioMamba
ACC (↑) 0.154 0.154 0.128 0.359
BioASQ 4b
MRR (↑) 0.184 0.205 0.141 0.362
ACC (↑) 0.160 0.120 0.120 0.360
BioASQ 5b
MRR (↑) 0.257 0.127 0.140 0.403
ACC (↑) 0.129 0.032 0.129 0.194
BioASQ 6b
MRR (↑) 0.185 0.032 0.129 0.290
BioMamba surpasses both BioBERT and BioGPT, showcasing the potential of the Mamba architecture over
BERT and GPT in the biomedical domain.
The superior performance of BioMamba can be attributed to its foundation on the S4 (Structured State
SpaceSequence)model. UnlikeBERTandGPT,whichrelyontransformerarchitectures, theS4modelexcels
in handling long-range dependencies and structured sequences more efficiently. This is particularly advanta-
geous in the biomedical domain, where understanding complex relationships and terminologies is crucial.
TheS4model’sabilitytomaintainandprocesslong-termdependenciesallowsBioMambatobettercapture
the intricate details and context within biomedical texts. Compared to the transformer-based architectures of
BERT and GPT, this results in more accurate and relevant information retrieval and generation.
Table 4: Performance of Autoregressive Models on the PubMed Dataset.
Dataset Model Perplexity (↓) Cross-Entropy Loss (↓)
BioGPT 4535.04 8.42
PubMed Mamba 505.62 6.23
BioMamba 2.93 1.07
Furthermore, weconductedacomprehensiveevaluationofthreeautoregressivemodels—BioGPT,Mamba,
and BioMamba—on the PubMed abstract dataset. The evaluation metrics used were perplexity and cross-
entropy loss, both of which are critical indicators of a model’s performance in natural language processing
tasks. Theexperimentalresults,aspresentedinTable4,revealthatBioMambasignificantlyoutperformsboth
BioGPT and Mamba across the PubMed dataset. Specifically, BioMamba achieves a perplexity of 2.93 and a
cross-entropylossof1.07, whicharemarkedlylowerthanthoseofBioGPT(perplexity: 4535.04, cross-entropy
loss: 8.42) and Mamba (perplexity: 505.62, cross-entropy loss: 6.23). These results underscore BioMamba’s
superior ability to understand and predict text within the biomedical domain.
4.4 Case Study
Table 5: Example of generated text to a BioASQ-factoid Question.
Model Sample
Q: What is the association of spermidine with α-synuclein neurotoxicity?
Ideal Answer Spermidine protects against α-synuclein neurotoxicity
Mamba A: Spermissible Spermidine is a neurotoxin vitro study.
BioMamba A: Speridin is aconitine is a natural product spermidine that protects
against α-synuclein neurotoxicity.
BioBERT A: Caenorhabdit
BioGPT A: Spermidine, a naturally occurring polyamine, alleviated α-synuclein-
induced loss of climbing activity ...
Table5providesanexamplefromtheBioASQ-factoiddataset. Thesequestionsrequireextractinganswers
9directly from the given context. We sampled predictions from BioBERT, BioGPT, Mamba, and BioMamba
to observe the effect of pre-training on downstream tasks.
The results show that Mamba often provides incorrect or nonsensical answers to simple biomedical ques-
tions, as seen in its response, which incorrectly identifies spermidine as a neurotoxin. This suggests that
Mamba’s pre-training may not have been sufficiently specialized for biomedical text.
BioMamba, on the other hand, delivers more accurate answers, correctly identifying that spermidine pro-
tects against α-synuclein neurotoxicity. This indicates that BioMamba’s pre-training on biomedical data has
significantly improved its performance on domain-specific tasks. Additionally, BioMamba is capable of pro-
viding more detailed responses, including longer-named entities, which is crucial for the precision required in
biomedical contexts.
BioBERT’sresponse,“Caenorhabdit”,isincompleteandirrelevanttothequestion,highlightingapotential
issue with its ability to extract specific information from the context.
BioGPTprovidesapartiallycorrectanswer,recognizingspermidineasanaturallyoccurringpolyamineand
mentioning its role in alleviating α-synuclein-induced loss of climbing activity. However, it does not explicitly
state the protective effect against neurotoxicity, which is the key information required.
Overall, the analysis demonstrates that pre-training on domain-specific data, as seen with BioMamba,
significantly enhances the model’s ability to understand and respond to biomedical questions accurately. This
underscores the importance of specialized pre-training for improving performance in specific domains.
5 Broader Impact
BioMambahasthepotentialtosignificantlyadvancebiomedicalresearchandhealthcarebyprovidingefficient
and accurate text mining tools [37]. It can accelerate the discovery of new insights from biomedical literature,
aidingingenepathwayidentification[26,6],drugtargetidentification[39,14],diseasemechanismunderstand-
ing [40], and therapeutic strategy development [8]. Additionally, BioMamba can enhance clinical decision
support systems [25, 9], improving patient outcomes by providing healthcare professionals with precise and
contextually relevant information [36, 38].
6 Conclusion
Inthispaper,wehaveintroducedBioMamba,astate-of-the-artpre-trainedlanguagemodelforbiomedicaltext
mining. Utilizing the advanced Mamba architecture with structured state space models (SSMs) and dynamic
parameterization, BioMamba efficiently handles long sequences. Pre-trained on a vast biomedical corpus and
fine-tuned for specific tasks like question answering, BioMamba significantly outperforms existing models,
including BioBERT. Our results demonstrate its superior understanding of complex biomedical terminologies
andcontexts, highlightingitspotentialasavaluabletoolforthebiomedicalresearchcommunity. Futurework
will extend BioMamba to additional biomedical tasks and further optimize its performance, contributing to
advancements in NLP applications for biomedical research and healthcare.
References
[1] Jose Juan Almagro Armenteros et al. Language modelling for biological sequences–curated datasets and
baselines. BioRxiv, 2020.
[2] AnuragArnab,MostafaDehghani,GeorgHeigold,ChenSun,MarioLuˇci´c,andCordeliaSchmid. Vivit: A
video vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision,
pages 6836–6846, 2021.
[3] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv
preprint arXiv:1903.10676, 2019.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, et al. Language models
are few-shot learners. In Advances in Neural Information Processing Systems, 2020.
[5] Kathi Canese and Sarah Weis. Pubmed: the bibliographic database. The NCBI handbook, 2(1), 2013.
10[6] Yi-Tan Chang, Eric P Hoffman, Guoqiang Yu, David M Herrington, Robert Clarke, Chiung-Ting Wu,
Lulu Chen, and Yue Wang. Integrated identification of disease specific pathways using multi-omics data.
bioRxiv, page 666065, 2019.
[7] Sudha Cheerkoot-Jalim and Kavi Kumar Khedo. A systematic review of text mining approaches applied
tovariousapplicationareasinthebiomedicaldomain. JournalofKnowledgeManagement,25(3):642–668,
2021.
[8] Jintai Chen, Yaojun Hu, Yue Wang, Yingzhou Lu, Xu Cao, Miao Lin, Hongxia Xu, Jian Wu, Cao Xiao,
Jimeng Sun, et al. Trialbench: Multi-modal artificial intelligence-ready clinical trial datasets. arXiv
preprint arXiv:2407.00631, 2024.
[9] Tianyi Chen, Nan Hao, Yingzhou Lu, and Capucine Van Rechem. Uncertainty quantification on clinical
trial outcome prediction. arXiv preprint arXiv:2401.03482, 2024.
[10] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk,andYoshuaBengio. LearningphraserepresentationsusingRNNEncoder–Decoderforstatistical
machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1724–1734, Stroudsburg, PA, USA, 2014. Association for Computational
Linguistics.
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidi-
rectional transformers for language understanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies,
NAACL-HLT 2019., pages 4171–4186. Association for Computational Linguistics, 2019.
[12] Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a no-recurrence sequence-to-sequence model
for speech recognition. In 2018 IEEE international conference on acoustics, speech and signal processing
(ICASSP), pages 5884–5888. IEEE, 2018.
[13] WilcoWMFleurenandWynandAlkema. Applicationoftextmininginthebiomedicaldomain. Methods,
74:97–106, 2015.
[14] Yi Fu, Yingzhou Lu, Yizhi Wang, Bai Zhang, Zhen Zhang, Guoqiang Yu, Chunyu Liu, Robert Clarke,
David M Herrington, and Yue Wang. Ddn3. 0: Determining significant rewiring of biological network
structure with differential dependency networks. Bioinformatics, page btae376, 2024.
[15] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language
modeling. arXiv preprint arXiv:2101.00027, 2020.
[16] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv
preprint arXiv:2312.00752, 2023.
[17] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann,
Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural
language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1–23, 2021.
[18] Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, and Yunhe Wang. Dense-
mamba: State space models with dense hidden connection for efficient large language models. arXiv
preprint arXiv:2403.00818, 2024.
[19] S Hochreiter and J Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, November
1997.
[20] EssamHHoussein,RehabEMohamed,andAbdelmgeidAAli.Machinelearningtechniquesforbiomedical
natural language processing: a comprehensive review. IEEE Access, 9:140628–140653, 2021.
[21] Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, and Chang Xu. Localmamba: Visual state
space model with windowed selective scan. arXiv preprint arXiv:2403.09338, 2024.
11[22] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo
Kang. BioBERT: a pre-trained biomedical language representation model for biomedical text mining.
Bioinformatics, 36(4):1234–1240, 2020.
[23] Jiaju Lin and Haoxuan Hu. Audio mamba: Pretrained audio state space model for audio tagging. arXiv
preprint arXiv:2405.13636, 2024.
[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
[25] Yingzhou Lu, Tianyi Chen, Nan Hao, Capucine Van Rechem, Jintai Chen, and Tianfan Fu. Uncertainty
quantificationandinterpretabilityforclinicaltrialapprovalprediction. HealthDataScience,4:0126,2024.
[26] Yingzhou Lu, Chiung-Ting Wu, Sarah J Parker, Zuolin Cheng, Georgia Saylor, Jennifer E Van Eyk,
Guoqiang Yu, Robert Clarke, David M Herrington, and Yue Wang. COT: an efficient and accurate
method for detecting marker genes among many subtypes. Bioinformatics Advances, 2(1):vbac037, 2022.
[27] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt:
generativepre-trainedtransformerforbiomedicaltextgenerationandmining. Briefingsinbioinformatics,
23(6):bbac409, 2022.
[28] Giacomo Miolo, Giulio Mantoan, and Carlotta Orsenigo. Electramed: a new pre-trained language repre-
sentation model for biomedical nlp. arXiv preprint arXiv:2104.09585, 2021.
[29] Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima L´opez, Eul´alia Farr´e-
Maduell, Luis Gasco, Martin Krallinger, and Georgios Paliouras. Overview of bioasq 2023: The eleventh
bioasq challenge on large-scale biomedical semantic indexing and question answering. arXiv:2307.05131,
2023.
[30] ChangshengQuanandXiaofeiLi. Multichannellong-termstreamingneuralspeechenhancementforstatic
and moving speakers. arXiv preprint arXiv:2403.07675, 2024.
[31] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for
machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L(cid:32) ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
systems, pages 5998–6008, 2017.
[33] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu,
AliHatamizadeh,SudhakarSingh,DeepakNarayanan,etal.Anempiricalstudyofmamba-basedlanguage
models. arXiv preprint arXiv:2406.07887, 2024.
[34] Zhichao Yang, Avijit Mitra, Sunjae Kwon, and Hong Yu. Clinicalmamba: A generative clinical language
model on longitudinal clinical notes. arXiv preprint arXiv:2403.05795, 2024.
[35] Weihao Yu and Xinchao Wang. Mambaout: Do we really need mamba for vision? arXiv preprint
arXiv:2405.07992, 2024.
[36] LingYue,JonathanLi,SixueXing,MdZabirulIslam,BolunXia,TianfanFu,andJintaiChen. Trialdura:
Hierarchical attention transformer for interpretable clinical trial duration prediction, 2024.
[37] Ling Yue, Sixue Xing, Jintai Chen, and Tianfan Fu. Clinicalagent: Clinical trial multi-agent system with
large language model-based reasoning, 2024.
[38] LingYue,SixueXing,JintaiChen,andTianfanFu. Trialenroll: Predictingclinicaltrialenrollmentsuccess
with deep & cross network and large language models, 2024.
[39] Bai Zhang, Yi Fu, Yingzhou Lu, Zhen Zhang, Robert Clarke, Jennifer E Van Eyk, David M Herrington,
andYueWang. DDN2.0: Randpythonpackagesfordifferentialdependencynetworkanalysisofbiological
systems. bioRxiv, pages 2021–04, 2021.
[40] YongqiZhang,QuanmingYao,LingYue,XianWu,ZihengZhang,ZhenxiLin,andYefengZheng. Emerg-
ing drug interaction prediction enabled by a flow-based graph neural network with biomedical network.
Nature Computational Science, 3(12):1023–1033, 2023.
12