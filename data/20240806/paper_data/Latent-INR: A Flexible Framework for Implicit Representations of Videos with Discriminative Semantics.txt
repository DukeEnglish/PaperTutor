Latent-INR: A Flexible Framework for Implicit
Representations of Videos with Discriminative
Semantics
Shishira R Maiya∗, Anubhav Gupta∗, Matthew Gwilliam, Max Ehrlich, and
Abhinav Shrivastava
University of Maryland, College Park, USA
Abstract. Implicit Neural Networks (INRs) have emerged as powerful
representationstoencodeallformsofdata,includingimages,videos,au-
dios, and scenes. With video, many INRs for video have been proposed
forthecompressiontask,andrecentmethodsfeaturesignificantimprove-
mentswithrespecttoencodingtime,storage,andreconstructionquality.
However, these encoded representations lack semantic meaning, so they
cannot be used for any downstream tasks that require such properties,
such as retrieval. This can act as a barrier for adoption of video INRs
over traditional codecs as they do not offer any significant edge apart
from compression. To alleviate this, we propose a flexible framework
that decouples the spatial and temporal aspects of the video INR. We
accomplish this with a dictionary of per-frame latents that are learned
jointly with a set of video specific hypernetworks, such that given a la-
tent,thesehypernetworkscanpredicttheINRweightstoreconstructthe
givenframe.Thisframeworknotonlyretainsthecompressionefficiency,
but the learned latents can be aligned with features from large vision
models, which grants them discriminative properties. We align these la-
tents with CLIP and show good performance for both compression and
video retrieval tasks. By aligning with VideoLlama, we are able to per-
formopen-endedchatwithourlearnedlatentsasthevisualinputs.Ad-
ditionally,thelearnedlatentsserveasaproxyfortheunderlyingweights,
allowingusperformtaskslikevideointerpolation.Thesesemanticprop-
erties and applications, existing simultaneously with ability to perform
compression,interpolation,andsuperresolutionproperties,areafirstin
this field of work.
1 Introduction
In today’s age of content explosion, large quantities of data are created every
second, and storing them reliably and efficiently is of utmost importance for
many applications. A scalable compression technique enables companies to pro-
vide better services at reduced cost and helps the end consumer by improving
their access to high-fidelity data in addition to decongesting the network. Since
*Equal contribution
4202
guA
5
]VC.sc[
1v27620.8042:viXra2 Maiya et al.
Standard INRs Semantic-aware Latent-INR
MSE Loss Foundation Model Grid
(CLIP, VideoLlama)
Align Image Feature
x, y, t MLP Linear
r, g, b
Layer MLP
z Linear
Frame Predictors
Linear
Frame Frame MSE Loss Frame
t CNN
Applications
MSE Loss Interpolation Chat Retrieval Compression
Fig.1: ExistingINRsforvideo(left)typicallytakesometime-coordinate,ortimeand
positionalcoordinatesandtrainasinglenetworktoreconstructavideo.Incontrastto
these,weproposeanINRsystemwhereadictionaryofimplicitlatentcodesislearned
for a video, one latent per frame. The latents are aligned to the image features of a
large vision model, while simultaneously an INR system is learned which, given these
latent codes, generates a positional INR which can reconstruct the frame. With this
framework, we successfully develop an INR which performs both reconstructive tasks
like compression, and semantic downstream tasks like retrieval and interactive chat.
the early 90s, several compression techniques have been created and widely de-
ployed for this exact purpose. Out of these, JPEG [55] for images, HEVC [49],
AV1[12], and H.264 [56] for videos have emerged as the most popular choices,
owing to their simple design and scalable performance.
Inthepastdecade,theriseofdeeplearningledtoarenaissanceincomputer
vision, eventually impacting the visual data compression landscape [15,4,30].
Despite their success, these ML-based codecs have not seen widespread adop-
tionliketraditionalcodecs.Thisisinpartduetofailuretogeneralize,sinceML
codecstrainedonlargedatasetscangivesub-optimalcompressionfordatapoints
that differ significantly from their training set [59,7]. Implicit Neural Represen-
tations (INR) attempt to avoid the generalization issue by operating internally.
Instead of training large models that learn to identify general patterns in train-
ingdataandapplythemtospecificout-of-distributiondata,implicittechniques
involve training a small model to exploit the specific patterns for the given data
point.Thatis,forvideocompression,thisapproachwouldtrainonenetworkper
video, and for image compression, it would train one network per image. The
resulting model is essentially a function that represents the underlying signal in
spatial/temporal space.
Despite these advances, neural video compression remains unsolved. Various
methods address issues of compression quality [9,22], but two crucial questions
remainunanswered–(i)howtoscaleforlongervideosgivenarchitecturalrigidityLatent-INR 3
and (ii) how to reduce long encoding time due to training a network for every
video. Although recent works make some progress for these [28], the training
time is still quite long, and INR behavior for lossy compression is not well-
understood [33], limiting potential for practical adoption.
Furthermore, these approaches for INR tackle only one axis of the problem,
i.e., how to formulate video INRs with the primary goal of compression. These
aim to solve problems like long encoding time directly, by reducing it. In con-
trast to these works, we instead aim to justify the compute and time needed
to train implicit representations. So, as a step towards ML-based codecs with
compelling real-world potential, we present Latent-INR – a new flexible frame-
work for formulating video INRs, where in addition to compression, the INR
enables downstream tasks like retrieval and video question answering, without
the need to decode the video. Our framework consists of two parts: (i) a dic-
tionary of learnable latents, one for each frame, and (ii) a set of hypernetworks
learned on the entire video which, given a latent as input, predict frame-specific
weightmodulationsonthesharedbasenetwork.Thissharedbasetakesaspatial
coordinate grid as input and outputs the specific frame
This design allows us to separate the spatial and temporal aspects of the
video by modeling them separately. We can view the set of hypernetworks as a
base model that learns the general structure and style of the video, while each
learned latent conditions it to output a specific frame. The latent here acts as a
proxy for the weights of the frame-specific INR. This property is apparent from
the video interpolation ability of our model - a task that other video INR repre-
sentationsstruggletoperform.LikeothervideoINRs,ourmethodiscompetitive
forcompression,butuniquelyretainsthepropertiesoforiginalcoordinate-based
INR.Thatis,ourcontinuousrepresentationsofframesallowsforspatialinterpo-
lation, which can be leveraged for superresolution and a decoding paradigm we
refer to as “any-resolution inference.” That is, at inference/decoding time, our
same model, with no changes to latents or architecture, can decode a video at
anyresolution-akeyfeaturemissingfromtraditionalcodecs.Thislatentisalso
quite flexible, and according to the procedure shown in Figure 1, we can align
it with the features from a large vision model, such as CLIP [35] to encode the
visual semantics of the frame while retaining nice properties such as alignment
with CLIP text embeddings. This allows for a whole spectrum of applications,
including frame, concept, and whole video retrieval with text queries.
In summary, our framework gives that extra edge apart from compression to
ML-based codecs, paving the way for their widespread adoption. Concretely,
– We propose an auto-decoder latent-based framework with spatio-temporal
decoupling for implicit video representations. Compared to other video INR
methods, this is a new way of formulating the problem.
– Our system has good compression performance, competing well with other
ML-based codecs for PSNR, BPP, and decoding speed while also enabling
any-resolution inference.4 Maiya et al.
– Thelearntlatentembeddingsfromourframeworkdemonstrateinternalgen-
eralization from the encoded dataset, achieving video interpolation, a task
that other INR based methods struggle to achieve.
– We align our latents with large foundational models like CLIP [35], thus
making our representations useful for retrieval tasks.
– We align our entire dictionary with video features for VideoLlama [58] to
enable chat-style applications, including video question answering and cap-
tioning.
2 Related Work
Implicit Neural Representations (INR’s) are a class of neural networks
designedwiththeintentionofrepresentingagivendatapointordatasetperfectly
ratherthanexploitinggeneralpatternsandgeneralizingforunseendata.SIREN
[44]pioneeredtheuseofperiodicactivationstotrainsimpleMLP’sthatworked
well across images, SDF and audio. This was followed by a host of works that
improved the training process of INR’s by making them faster [37,52,32] work
acrossmultiplescales[38]andencodemultipledatapoints[14].Modelsthatused
meta learning [48,51] started gaining ground as they offered the advantages of
compressionalongwithgeneralization.[40,50]furthermadeimprovementstothis
line of work by directly learning sparse-INR’s leading to improved compression
and improved optimization by dataset selection respectively.
Hypernetworksareaclassofnetworksoptimizedforpredictingparameters
of another network, with the aim of generalizing across unseen tasks[16]. Some
utilizedtheseforscenes[46,45,13].Trans-INR[11]introducedtheparadigmofus-
ingatransformerbasedhypernetworktoconvertdatadirectlyfromimage-space
toINR’s.[21]improveduponthisideaandmadetheimportantobservationthat
it is sufficient to modulate only the first hidden layer of an INR to represent a
dataset of points. Unfortunately, these hypernetworks act on input data points
which require test-time optimizations, making them unsuitable for compression
tasks.[41]trytoovercomethiswithan“auto-decoder”framework,wherelearn-
able latents represent a dataset of videos, with each latent corresponding to
a single video, such that no encoder is needed. Others have investigated this
paradigm for a variety of modalities[42,39,5]. Still, the lack of decoupling space
from time prohibits the method from scaling to real-world videos.
Video INRs have recently gained popularity for compression. [9] was the
first implicit representation which modelled a video as a function mapping the
temporal coordinates to the corresponding frames. Later works [25,2,8,18] it-
erated on this method, providing improvements in performance. [22] enhanced
thisconceptbyincorporatinghash-grid[32]representationstospeedupencoding
times. NIRVANA [28] represented a video using a series of smaller INR models
trained in an autoregressive manner to scale for longer videos.
Video Interpolation has been a fundamental task in computer vision,
helping in creating smoother visual experiences. Over the past few years, deep
learning based methods have vastly improved the quality of these interpolationsLatent-INR 5
Interpolated
Layer-specific Frame 1 𝑍𝑍1 Latents
HyperNetwork
ℎθ 𝑍𝑍𝑗𝑗 cosine similarity
Low Rank retrieves
Factorization
"horse"
Frame N
𝜎𝜎( 𝑃𝑃𝑙𝑙
Multi Resolution ×𝑄𝑄𝑙𝑙) Dicti𝑍𝑍⋮o𝑁𝑁nary of
Hash-Grids Learnable Latents
Modulate "What's in the
video?"
𝜃𝜃𝑙𝑙
R Video
" raJ co ec hk oe ry
s
eri .d "es a
Concatenate LLM "What is the
Features jockey wearing?"
+ G
"Red and yellow
shirt."
Frame
B
𝐼𝐼𝑗𝑗
Fig.2: We propose a new framework for video INR models by decoupling the spa-
tial and temporal aspects of modeling. Our framework consists of auto-decoder based
learnable latents that modulate the base network using a hypernetwork, via low-rank
modulation. Once encoded, the resulting latents act as a proxy for the underlying
weights of the representation. On the right, we show the use of these latents for addi-
tional tasks like video interpolation. By aligning these latents to the embedding space
offoundationalmodelslikeCLIPandVideoLlama,wealsoperformretrievalandchat.
[43,19].However, current INR-based video encoders lack this feature (see discus-
sion in [10,8], for example), hindering their widespread usage.
Video Retrieval is an essential process in the digital media landscape,
wheretheobjectiveistoefficientlysearchandextractspecificvideocontentfrom
expansivedatasets.Thecomplexityofunderstandingandindexingdiversevideo
content has traditionally posed significant challenges. However, with the advent
of machine learning-based methods, there has been a remarkable improvement
in both the accuracy and efficiency of video retrieval systems [1,3,26]. These
advances are limited to systems requiring an additional model, which can act as
a burden on the system as they do not compress the data.
3 Approach
3.1 Background
Implicit Neural Representations parameterize a function,
f :X →Y where X ={(x ,y )|0≤x ≤W,0≤y ≤H}
θ i i i i
which represents a mapping between the coordinate space, with height H and
width W,andtheunderlyingsignalY.Thisformulationisusuallytrainedwitha
standardMSE-loss:||f (X)−Y|| .ForagivenvideoV ∈RN×H×W×3containing
θ 26 Maiya et al.
N frames, [44] represents them as pixels moving across time, i.e.,
f (x,y,t)=Y
θ t
Otherformulationsexistwhichlearnframe-based[9]orpatch-based[28]rep-
resentation, yet in each of these formulations, the focus is on representing the
underlying data, with the added motivation of compressing it. However, none
of these systems are designed with the goal of making these representations,
f , useful for downstream tasks [33,34]. Instead, we utilize a learnable latent, z,
θ
as a part of an auto-decoder framework, along with a hypernet h to not only
compress but to create useful representations.
f ((x,y)|θ )=Y θ =h(z ) (1)
θ t t t t
Theresultinglatentzcanbeusedforvariousdownstreamtaskslikeinterpolation
and retrieval, as we show in our work.
3.2 Latent-INR
Directly predicting the weights θ of the base network f, using the hypernet h,
isexpensive,parameter-heavy,andunsuitableforcompression.Hence,wefollow
[47] [39] and instead predict low-rank matrices, which are then applied to the
base network weights. This type of modulation acts as a form of subnetwork
selection, analogous to systems proposed in [17] [36]. For a base network f with
L layers, our formulation now looks like
f ((x,y)|θl1,θl2...θlL)=Y
θ t t t t
(2)
θl =σ(Pl×Ql)·θl h (z )=[Pl,Ql]
t l t
where θl represents the weights of the l-th layer and θl denotes the modulated
t
weightsforframet.Here,σsignifiesanactivationfunctiononthematrix-product
of low rank matrices Pl , Ql, which are of dimensions RN×r and RM×r, where
N ×M is the width of the base network f and rank r ≪(N,M). These matri-
θ
ces are responsible for adjusting the weights θ as dictated by the corresponding
l
hypernetwork h . Note that all hypernetworks use the same latent z ∈ RD as
l t
input. The rank r and the number of modulated layers essentially act a hyper-
parameters that control the compression-performance trade-off.
3.3 Model architecture
In our experiments, both the base network f and hypernetworks h are feedfor-
θ l
ward MLP’s that take in a coordinate input. Following [28], we also propose a
variationtothebasenetworkwithanadditionalconvolutionalup-sampleblock,
which accepts coordinates of centroids as input and gives frame patches as out-
put.WeusethestandardReLUforbasenetworkandtanhforthehypernetworkLatent-INR 7
as the respective non-linearities. The latents Z are initialized to be a standard
normal with small variance, as we found empirically that this made the conver-
gencefaster.ThecompletemodelarchitectureispresentedinFigure2.Formore
details, see Appendix.
3.4 Model Compression
We train this entire system end-to-end with MSE-loss as the objective function.
Once trained, we apply a standard quantization to all network parameters, fur-
ther reducing the required storage. Given ϕ, a flattened parameter tensor, we
transform it according to the following equations
(cid:24) (cid:23)
ϕ −ϕ ϕ −ϕ
ϕ = i min scale= max min (3)
i 2b 2b
where the ⌈·⌋ (round) operation converts its argument to the nearest integer as
dictatedbybitwidthbofthequantizationprocess.Wealsostorethescale,ϕ ,
max
ϕ and the parameter shapes. These quantized values for all parameters are
min
concatenated and further compressed using Huffman encoding.
3.5 Interpolation
Given a video of N frames and a scale α, the task of interpolation involves
creating α·N coherent frames. Once we encode a video using our framework,
we perform linear interpolation on the frame latents {z } and pass the resulting
t
latent through the hypernetwork. This gives us the weight modulation required
in the INR, and the updated base network is used to obtain the interpolated
frames:
z =β ·z +(1−β )·z Y =f (X;h(z )) (4)
inter i t i t−1 inter θ inter
where,
(cid:20) (cid:21)
1 2 α−1
β ∈ , ,...,
i α α α
essentiallygeneratingα−1framesbetweenanytwogivenframes.Wetrainwith
held out frames and show results for α∈{2,4,8}.
3.6 Downstream Tasks
Retrieval.Videoretrievalinvolvessearchingandretrievingvideosorclipsfrom
alargedatabasebasedonsimilaritytogivenusersearchqueriesthatareusually
in the form of text. This can be viewed as a function R mapping query q to a
set of corresponding videos V.
R:q →V (5)8 Maiya et al.
BPP vs PSNR BPP vs SSIM
1.0
30 0.8
20 0.6
NeRV-S
NeRV-L
Ours - L
Ours - M
10 Ours - S 0.4
NVP
0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
Bits Per Pixel (BPP) Bits Per Pixel (BPP)
Fig.3:WeplottheratedistortioncurvesonPSNRandSSIMtocomparecompression
with other methods. We observe that our large model achieves comparable PSNR to
thecurrentSOTA[22].Notethat,whilenotplottedhere,ourdecodingFPSissuperior.
Additional per-video results are available in the Supplementary.
The function R can use any similarity measure like cosine, euclidean, or
nearest neighbors to retrieve matches. We encode a dataset of videos using our
Latent-INR framework and use the resulting trained latents as our frame level
representations.Toensuretheselatentssharethesamespaceasthetextqueries,
we add a cosine similarity loss between the latents and the CLIP image embed-
dings of the corresponding frames. Our encoding loss function is modified to be:
L=L +λ·L (Z ,Zclip) (6)
MSE clip t t
where Zclip is the CLIP Image embedding of the input frame and λ controls the
t
strength of this loss. In all our experiments, λ is set to 0.01.
Chat. We modify the formulation from retrieval slightly, aligning our dictio-
nary of features to VideoLlama [58] instead of CLIP. Since the shapes are not
compatible, we treat our latents as tokens and project the dimension to match
the VideoLlama space. With this, we are able to integrate our latents with a
powerful LLM, substituting our latents for the raw video input tokens. We can
then perform any task that VideoLlama can, in particular question answering
andcaptioning.Wewishtoemphasizethatourlatentsareflexible–wecanalign
well with any large off the shelf model, for any downstream task.
4 Experiments
4.1 Video Compression
We perform comparative analysis for video compression on the standard Ul-
tra Video Group (UVG) dataset [31]. This dataset comprises seven high-quality
)RNSP(
oitaR
esioN-ot-langiS
kaeP
)MISS(
xednI
ytiralimiS
larutcurtSLatent-INR 9
TG
noitcurtsnoceR
Fig.4: With the same model, we can Fig.5:Weachievehighqualityreconstruc-
perform inference at any resolution, with tion and are able to reproduce even the
speeds competitive or beating HEVC. We finer details like water fountains and the
show sample frames for each resolution. hair on the horse.
videos, each featuring diverse scenes shot at 120fps over a duration of five sec-
onds. While most videos contain 600 frames, the ‘shakendry’ video is an ex-
ception with 300 frames, all at a resolution of 1080x1920. To assess the visual
quality,weusestandardmetricssuchasPeakSignal-to-NoiseRatio(PSNR)and
Structural Similarity index (SSIM). We measure the storage efficiency of these
methods using bits per pixel (BPP). As mentioned earlier, we use feedforward
MLPs for both the base network f and hypernetworks h . The base network
θ l
consistsof6layerswithlayersizeof512andeachhypernetworkthatmodulates
aselectedlayerhasonehiddenlayerofsize128withtanhnon-linearity,followed
by the output layer. In the case where we use patch centroids as inputs, we add
a convolutional layer followed by a pixel-shuffle [27] for upsampling.
Weusehash-grids[32]forpositionalencodingduetotheirhighqualityrecon-
struction,althoughitshouldbenotedwecanuseotherschemes,suchasFourier
features [52] to exchange some quality for faster training (see Appendix). We
compare our method against NeRV [9] and NVP [22], with each of them encod-
ing a video per model, and the results are presented in Figure 3. We observe
that compression from our framework is comparable to baselines at similar bpp
ranges, in addition to the other downstream benefits it offers.
Duetoourarchitecture,wearealsoabletooperateinanovelparadigm,“any-
resolution inference.” Without changing the network architecture at all, we
can decode the video at arbitrary smaller resolutions, as well as at higher res-
olutions (super-resolution) by leveraging the continuous resolution property of
our hash grids and MLPs. We show our FPS decoding at various resolutions
in Figure 4, although it should be noted that HEVC, the standard codec we
compareto,mustencodeseparatelyforeveryresolutionwhilewecanstoreallin
the same model. Figure 5 provides samples that showcase our method’s fidelity.
4.2 Video Interpolation
In our framework, we can interpolate in the latent space to generate valid inter-
polated frame outputs. We conduct experiments on two datasets: the “big buck
bunny sequence” and a selection of ten videos from the Taichi test set. Frames
are held out at a scale stride α during encoding. During testing, we interpolate
the resulting latents on the held out frames and evaluate their performance.10 Maiya et al.
Table 1: Interpolation Performance Table 2: Reconstruction and retrieval
(PSNR),fordifferentscalestrides(α). ablations of CLIP on MSR-VTT.
Dataset α NeRV NIRVANA NVP Ours Reconstruction Retrieval(T2V)
2 15.92 19.14 20.10 33.17 CLIPλ PSNR R@1 R@5 R@10
Bunny 4 15.43 18.90 19.11 28.08
0.0 30.03 0.1 0.3 0.8
8 13.68 18.67 18.08 25.88
1e-3 29.83 28.4 50.8 60.6
2 16.91 18.19 19.33 35.13 1e-2 29.46 30.2 52.4 61.0
TaiChi 4 17.14 17.71 18.52 31.84 1e-1 28.93 29.7 51.5 61.8
8 15.72 16.21 17.7 27.72 1.0 28.61 30.2 51.4 61.3
Seen Frame Interpolated Frames Seen Frame Interpolated Frames Seen Frame
v
re
N
a
n
a
v
riN
sru
O
Fig.6: We compare interpolation with Latent-INR to NVP and NIRVANA. We find
that our method has less artifacts and smoother motion in the interpolated frames.
We use the same INR models utilized for compression as our baselines, with
a reduction in network layer size and modulating mask rank. While NeRV [9]
and NVP [22] interpolate time positions used as input, NIRVANA interpolates
theweights.InTable1,weobservethatwhileotherINRmethodsfailtoproduce
perceptualframesatscaleof2,ourmodelcangivereasonableinterpolationseven
at a scale of 8. We confirm this qualitatively also, by inspecting interpolated
frames such as those shown in Figure 6. Our outputs have noticeably fewer
artifacts, and while imperfect, handle the motion better. Compared to other
videoINRmethods,ourapproachofusinglearntlatentsfacilitatesthemodelto
have an internal representation of the video content.
4.3 Downstream Tasks
Retrieval
Toshowcasetheflexibilityofourlatents,wealignthemwithCLIPandevalu-
atetheirperformanceonstandardretrievaltasks.Weutilizethevalidationsetof
COINdataset[53]andasubsetofHowto100mdatasettoevaluateperformance.
We first encode each video in our split using our Latent-INR framework withLatent-INR 11
Table3:Classandsegmentretrieval.Our Table 4: Whole video retrieval. Our
method often exceeds CLIP performance. method matches CLIP performance.
ClassLevel SegmentLevel TexttoVideo VideotoText
Dataset Method R@1 R@5 R@10 R@1 R@5 R@10 Dataset Method R@1 R@5 R@10 R@1 R@5 R@10
CLIP 31.60 44.70 50.70 6.60 13.10 16.50 CLIP 30.10 51.50 61.50 24.70 49.30 61.90
COIN MSR-VTT
Ours 34.40 45.10 50.50 6.40 13.30 17.00 Ours 30.20 52.40 61.10 25.40 49.90 61.70
CLIP 31.58 36.84 47.37 21.13 37.32 40.85 CLIP 38.4 74.8 86.6 36.2 73.6 84.8
HowTo100m* ActivityNet*
Ours 31.58 42.11 47.36 23.24 43.67 48.60 Ours 38.5 73.9 86.4 36.1 73.5 84.7
Fry Salmon
.350 .345 .343 .342 .341 .341 .341 .341
Cross the rope to wrap the bolt
.314 .313 .311 .311 .311 .311 .310 .310
Spray towards the fire
.322 .319 .317 .317 .316 .315 .313 .312
Put the clothes neatly on an ironing table
.346 .338 .335 .334 .333 .332 .330 .329
Fig.7: Nearest Neighbours for segment-level matching of sample queries from COIN
validationset.Thegreenboxesdenotethetruepositivesandtheredonesarefalsepos-
itives.Weshowtheinnerproductsimilaritybetweentheimageandthecorresponding
query inside the green boxes at the bottom of each image.
a loss that encourages the latents to be closer to the CLIP-Image embeddings
of the frames, in addition to the standard reconstruction loss. We consider two
distinct problems – retrieval of the correct class across all videos and retrieval
of the correct segment within a video. These two use cases cover both ends of
the spectrum, from localizing an event in a given video to searching for similar
eventsacrossvideos.WeutilizethestandardrecallatK,wherewehaveselected
k ∈ [1,5,10] to evaluate the efficacy of our method. The results are presented
in Table 3. We can see that our method matches CLIP in its retrieval perfor-
mance and even exceeds it in some cases. The qualitative results are presented
in Figure 7, where we visualize the top 5 nearest neighbours of the text query
that map to trained latents across all videos. Further results can be found in
the supplementary. We even find that our method can perform whole-video re-
trievalonMSR-VTT[57]andacustom1,000videosamplefromtheActivityNet
Captions[23]‘val-1’split.Weaverage-poolbothourfeaturesandCLIPfeatures
(similar to [6]) and use CLIP features computed on video captions. In Table 412 Maiya et al.
Fig.8: Latent-INR LLM. We show results for aligning our learned latents to a
VideoLlama model, which allows for interactive chat. We show successes (left)
and failures (right) for summarization (top) and question answering (bottom).
we find that our retrieval is quite competitive to retrieval using the CLIP fea-
tures themselves, showing that the learnt latents have similarly good averaging
and summarizing properties even over longer (180 seconds) videos, as well as
alignment even to the paragraph-length captions used in ActivityNet.
Video-based Chat
We evaluate the performance of our trained latents, when aligned to inter-
mediate VideoLlama features. This alignment enables access to the full scope of
text chat with video understanding. We show a sample of such results, in the
form of text and video prompts with text response, in Figure 8. These results
show the LLM is able to understand video inputs when provided in the form of
INR latents rather than raw video tokens. While not perfect, we infer the ma-
jority of the shortcomings of this system are primarily the fault of the LLM we
alignto.Furthermore,onthebasisofoursuccessinaligningwithCLIPandnow
VideoLlama,webelieveourlatentscanbealignedtoanyrepresentation.So,for
more powerful chat, one simply needs to align to a more powerful chatbot. We
thus provide these results two purposes. First, we show our model’s capability
to power efficient open-ended captioning and question answering, while still re-
tainingreconstructioncapabilities.Second,wepointtotheimmensepotentialof
our model (or a similar paradigm) to continue to be leveraged with such models
as they expand in their size and performance.
4.4 Visualizing Trained Latents
Thetrainedlatents,representingthemodulatedframes,offerintriguinginsights
when visualized in a reduced dimensional space. Utilizing Uniform Manifold
Approximation and Projection (UMAP)[29] we project the embeddings Z into
t
a 2D space, allowing for an intuitive interpretation of their relationships. In
Figure 9, we plot the UMAP for three distinct videos from the UVG dataset:Latent-INR 13
Fig.9:WevisualizethetrainedlatentsZ projectedto2DusingUMAP.Weshowthat
t
thetrainedlatentsfromourframeworkcapturemeaningfulsemanticsoftheunderlying
data. Latents for Bosphore (left), Honeybee (middle) and Jockey (right) from UVG
dataset. Dark to Light color indicates frame numbers ranging from 0 to 600.
‘Bosphore,’ ‘Honeybee,’ and ‘Jockey,’ each offering unique characteristics for
examination.
‘Bosphore’,characterizedbyitsslow-movingobjectandrelativelystaticfore-
ground, exhibits a smooth latent trajectory in the 2D space. This smoothness
reflects the minimal variance in frame content, suggesting that our method ef-
fectively captures the subtle dynamics of the scene. In contrast, the ‘Honeybee’
video, with its repetitive frames, results in latents that cluster tightly together,
signifying our model’s ability to recognize and encode repetitive patterns ef-
ficiently. The most dynamic of the three, ‘Jockey’, presents a more complex
scenario with rapid changes in both the foreground and background. Here, the
latentsformclustersaroundsimilarscenes,yetmaintainadiscernibletrajectory
through the 2D space. These visualizations illustrate the semantic richness em-
bedded within the latents obtained from our framework even when trained only
for compression.
5 Ablation Studies
CLIP λ. We investigate the impact of the large model alignment weighting
term on both reconstruction and retrieval for MSR-VTT. In Table 2, we find
that PSNR decreases slightly as λ increases. However, the retrieval performance
seems to saturate at λ=0.01. So, we suggest not tuning the λ too high for any
application, given the diminishing returns.
Layer Modulations. In our approach, we have separate hypernetworks that
modulated the selected layers. To evalute the importance of each, we design an
experiment where they are modulated in isolation. We use the same setup as
the compression experiments with the modulating mask rank fixed at 20 for all
models.InFigure10,wecanclearlyseethatthefirstfewlayershaveasignificant
impact on the encoding performance. This matches the observations from [21]
about the out sized impact of first few layers while modulating INRs.14 Maiya et al.
BPP vs PSNR for Various Layers
36.0
35
35.5
35.0
30
34.5
25 Layer 0 34.0
Layer 1 33.5 Patch Sizes
Layer 2 4
20 Layer 3 33.0 8 16 Layer 4 32.5 32
Layer 5 64
15 32.0
0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 500 1000 1500 2000 2500 3000 3500 4000
Bits Per Pixel (BPP) Epoch
Fig.10: Ablations to study the effect of layer modulations in the hypernetwork
and the effect of patch size on reconstruction quality (PSNR).
Patch Size. Scaling to higher-resolution videos can be memory-intensive. This
is particularly true when employing memory-demanding positional encoding
schemes such as hash-grids [32]. To investigate this aspect further, we experi-
mentwithmodelsthatprocesscentroidsoffixed-sizepatches,directlypredicting
thecorrespondingframepatches,tosavememory.FromFigure10,performance
is consistent for smaller patch sizes, but drops off sharply for higher patch sizes.
6 Conclusion
Limitations. Our latents are somewhat restricted by the quality of the em-
beddings they are aligned to. Additionally, more work is still required to match
standard codecs in terms of storage and encoding time, in spite of impressive
gains in terms of quality and decoding speed. Future work could both improve
the compression, and leverage more powerful vision models.
Broader Impacts. Our method for simultaneously compressing and learning
useful features for recognition could reduce the need to decode videos for these
tasksandthussavecomputationalresources,cuttingcostsandhelpingtheenvi-
ronment.However,workthatadvancesperformanceforcompressionandrecogn-
tion also has applications in surveillance and warfare.
In this work, we propose a new framework, Latent-INR, where we decou-
ple the temporal aspect from the spatial into a dictionary of learnable latents.
Theseauto-decoderbasedlearnablelatentsmodulatethelayersofthebaseINR
network via low-rank modulation using hypernetworks. Latent-INR is not only
well-suited to video compression, but the resulting latents learn an internal rep-
resentation of the data they encode that lends itself to SOTA interpolation for
video INRs. Additionally, we also augment these latents by training them to
be aligned with CLIP and VideoLlama, which allows us to bring the power of
foundational models to compressed representations, and perform retrieval and
chat-based applications like captioning and question answering. Our work thus
opens up new possibilities of research in the implicit neural space where down-
stream tasks can be performed by these model without the need for decoding.
)RNSP(
oitaR
esioN-ot-langiS
kaeP
RNSPLatent-INR 15
Acknowledgements This work was partially supported by NSF CAREER
Award (#2238769) to AS. The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclusions contained herein are those
of the authors and should not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or implied, of NSF or the U.S.
Government.
References
1. A scheme for shot detection and video retreival using spatio temporal features.
InternationalJournalofRecentTechnologyandEngineering(2019),https://api.
semanticscholar.org/CorpusID:241499090
2. Bai, Y., Dong, C., Wang, C., Yuan, C.: Ps-nerv: Patch-wise stylized neural repre-
sentationsforvideos.In:2023IEEEInternationalConferenceonImageProcessing
(ICIP). pp. 41–45. IEEE (2023)
3. Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint
video and image encoder for end-to-end retrieval. 2021 IEEE/CVF International
Conference on Computer Vision (ICCV) pp. 1708–1718 (2021), https://api.
semanticscholar.org/CorpusID:232478955
4. Ball´e, J., Minnen, D.C., Singh, S., Hwang, S.J., Johnston, N.: Variational image
compression with a scale hyperprior. ArXiv abs/1802.01436 (2018), https://
api.semanticscholar.org/CorpusID:3611540
5. Bauer, M., Dupont, E., Brock, A., Rosenbaum, D., Schwarz, J., Kim, H.: Spatial
functa: Scaling functa to imagenet classification and generation. arXiv preprint
arXiv:2302.03130 (2023)
6. Buch, S., Eyzaguirre, C., Gaidon, A., Wu, J., Fei-Fei, L., Niebles, J.C.: Revisiting
the ”video” in video-language understanding (2022)
7. Cao, L., Jiang, A., Li, W., Wu, H., Ye, N.: Oodhdr-codec: Out-of-distribution
generalization for HDR image compression. In: Thirty-Sixth AAAI Conference on
ArtificialIntelligence,AAAI2022,Thirty-FourthConferenceonInnovativeAppli-
cations of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Edu-
cational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February
22-March1,2022.pp.158–166.AAAIPress(2022).https://doi.org/10.1609/
AAAI.V36I1.19890, https://doi.org/10.1609/aaai.v36i1.19890
8. Chen,H.,Gwilliam,M.,Lim,S.N.,Shrivastava,A.:Hnerv:Ahybridneuralrepre-
sentation for videos. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 10270–10279 (2023)
9. Chen, H., He, B., Wang, H., Ren, Y., Lim, S.N., Shrivastava, A.: Nerv: Neural
representations for videos. Advances in Neural Information Processing Systems
34, 21557–21568 (2021)
10. Chen, H., Matthew, G., He, B., Lim, S.N., Shrivastava, A.: Cnerv: Content-
adaptive neural representation for visual data. In: BMVC (2022)
11. Chen,Y.,Wang,X.:Transformersasmeta-learnersforimplicitneuralrepresenta-
tions. In: European Conference on Computer Vision (2022)
12. Chen,Y.,Murherjee,D.,Han,J.,Grange,A.,Xu,Y.,Liu,Z.,Parker,S.,Chen,C.,
Su, H., Joshi, U., et al.: An overview of core coding tools in the av1 video codec.
In: 2018 picture coding symposium (PCS). pp. 41–45. IEEE (2018)16 Maiya et al.
13. Chiang, P.Z., Tsai, M.S., Tseng, H.Y., Lai, W.S., Chiu, W.C.: Stylizing 3d scene
via implicit representation and hypernetwork. In: Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision. pp. 1475–1484 (2022)
14. Dupont, E., Kim, H., Eslami, S.M.A., Rezende, D.J., Rosenbaum, D.: From data
to functa: Your data point is a function and you can treat it like one. In: Interna-
tional Conference on Machine Learning (2022), https://api.semanticscholar.
org/CorpusID:249395684
15. Ehrlich, M., Davis, L.S.: Deep residual learning in the jpeg transform domain.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) (October 2019)
16. Finn,C.,Abbeel,P.,Levine,S.:Model-agnosticmeta-learningforfastadaptation
ofdeepnetworks.In:InternationalConferenceonMachineLearning(2017),https:
//api.semanticscholar.org/CorpusID:6719686
17. Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable
neural networks. arXiv preprint arXiv:1803.03635 (2018)
18. He,B.,Yang,X.,Wang,H.,Wu,Z.,Chen,H.,Huang,S.,Ren,Y.,Lim,S.N.,Shri-
vastava,A.:Towardsscalableneuralrepresentationfordiversevideos.In:Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 6132–6142 (2023)
19. Jiang, H., Sun, D., Jampani, V., Yang, M.H., Learned-Miller, E.G., Kautz,
J.: Super slomo: High quality estimation of multiple intermediate frames for
video interpolation. 2018 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition pp. 9000–9008 (2017), https://api.semanticscholar.org/
CorpusID:10817557
20. Johnson,J.,Douze,M.,J´egou,H.:Billion-scalesimilaritysearchwithGPUs.IEEE
Transactions on Big Data 7(3), 535–547 (2019)
21. Kim, C., Lee, D., Kim, S., Cho, M., Han, W.S.: Generalizable implicit neural
representationsviainstancepatterncomposers.In:ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.11808–11817(2023)
22. Kim,S.,Yu,S.,Lee,J.,Shin,J.:Scalableneuralvideorepresentationswithlearn-
able positional features. Advances in Neural Information Processing Systems 35,
12718–12731 (2022)
23. Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.: Dense-captioning events
in videos (2017)
24. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: bootstrapping language-image pre-
training with frozen image encoders and large language models. In: Proceedings
of the 40th International Conference on Machine Learning. ICML’23, JMLR.org
(2023)
25. Li, Z., Wang, M., Pi, H., Xu, K., Mei, J., Liu, Y.: E-nerv: Expedite neural video
representation with disentangled spatial-temporal context. In: European Confer-
ence on Computer Vision. pp. 267–284. Springer (2022)
26. Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An
empirical study of clip for end to end video clip retrieval. Neurocomputing 508,
293–304 (2021), https://api.semanticscholar.org/CorpusID:233296206
27. Luo,H.,Chen,Y.,Zhou,Y.:Anextremelyeffectivespatialpyramidandpixelshuf-
fleupsamplingdecoderformultiscalemonoculardepthestimation.Computational
Intelligence and Neuroscience 2022 (2022), https://api.semanticscholar.org/
CorpusID:251272212
28. Maiya, S.R., Girish, S., Ehrlich, M., Wang, H., Lee, K.S., Poirson, P., Wu, P.,
Wang,C.,Shrivastava,A.:Nirvana:NeuralimplicitrepresentationsofvideoswithLatent-INR 17
adaptive networks and autoregressive patch-wise modeling.In: Proceedingsof the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.14378–
14387 (2023)
29. McInnes, L., Healy, J., Melville, J.: Umap: Uniform manifold approximation and
projection for dimension reduction (2020)
30. Mentzer, F., Toderici, G., Tschannen, M., Agustsson, E.: High-fidelity gen-
erative image compression. ArXiv abs/2006.09965 (2020), https://api.
semanticscholar.org/CorpusID:219721015
31. Mercat,A.,Viitanen,M.,Vanne,J.:Uvgdataset:50/120fps4ksequencesforvideo
codec analysis and development. In: Proceedings of the 11th ACM Multimedia
Systems Conference. p. 297–302. MMSys ’20, Association for Computing Machin-
ery, New York, NY, USA (2020). https://doi.org/10.1145/3339825.3394937,
https://doi.org/10.1145/3339825.3394937
32. Mu¨ller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. ACM Trans. Graph. 41(4), 102:1–102:15
(Jul 2022). https://doi.org/10.1145/3528223.3530127, https://doi.org/10.
1145/3528223.3530127
33. Padmanabhan,N.,Gwilliam,M.,Kumar,P.,Maiya,S.R.,Ehrlich,M.,Shrivastava,
A.:Explainingtheimplicitneuralcanvas:Connectingpixelstoneuronsbytracing
their contributions. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). pp. 10957–10967 (June 2024)
34. Papa,S.,Valperga,R.,Knigge,D.,Kofinas,M.,Lippe,P.,Sonke,J.J.,Gavves,E.:
Howtotrainneuralfieldrepresentations:Acomprehensivestudyandbenchmark.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 22616–22625 (June 2024)
35. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning trans-
ferablevisualmodelsfromnaturallanguagesupervision.In:InternationalConfer-
enceonMachineLearning(2021),https://api.semanticscholar.org/CorpusID:
231591445
36. Ramanujan,V.,Wortsman,M.,Kembhavi,A.,Farhadi,A.,Rastegari,M.:What’s
hiddeninarandomlyweightedneuralnetwork?In:ProceedingsoftheIEEE/CVF
conference on computer vision and pattern recognition. pp. 11893–11902 (2020)
37. Saragadam, V., LeJeune, D., Tan, J., Balakrishnan, G., Veeraraghavan, A., Bara-
niuk, R.: Wire: Wavelet implicit neural representations. 2023 IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR) pp. 18507–18516
(2023), https://api.semanticscholar.org/CorpusID:255749557
38. Saragadam, V., Tan, J., Balakrishnan, G., Baraniuk, R.G., Veeraraghavan,
A.: MINER: multiscale implicit neural representations. CoRR abs/2202.03532
(2022), https://arxiv.org/abs/2202.03532
39. Schwarz,J.R.,Tack,J.,Teh,Y.W.,Lee,J.,Shin,J.:Modality-agnosticvariational
compression of implicit neural representations. arXiv preprint arXiv:2301.09479
(2023)
40. Schwarz, J.R., Teh, Y.W.: Meta-learning sparse compression networks (2022)
41. Sen, B., Agarwal, A., Namboodiri, V.P., Jawahar, C.: Inr-v: A continuous repre-
sentation space for video-based generative tasks. arXiv preprint arXiv:2210.16579
(2022)
42. Sen, B., Singh, G., Agarwal, A., Agaram, R., Krishna, K.M., Sridhar, S.:
Hyp-nerf: Learning improved nerf priors using a hypernetwork. arXiv preprint
arXiv:2306.06093 (2023)18 Maiya et al.
43. Shi, W., Caballero, J., Husza´r, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert,
D., Wang, Z.: Real-time single image and video super-resolution using an effi-
cient sub-pixel convolutional neural network. 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) pp. 1874–1883 (2016), https:
//api.semanticscholar.org/CorpusID:7037846
44. Sitzmann, V., Martel, J.N.P., Bergman, A.W., Lindell, D.B., Wetzstein, G.: Im-
plicit neural representations with periodic activation functions (2020)
45. Sitzmann, V., Rezchikov, S., Freeman, B., Tenenbaum, J., Durand, F.: Light field
networks:Neuralscenerepresentationswithsingle-evaluationrendering.Advances
in Neural Information Processing Systems 34, 19313–19325 (2021)
46. Sitzmann, V., Zollho¨fer, M., Wetzstein, G.: Scene representation networks: Con-
tinuous 3d-structure-aware neural scene representations. Advances in Neural In-
formation Processing Systems 32 (2019)
47. Skorokhodov,I.,Ignatyev,S.,Elhoseiny,M.:Adversarialgenerationofcontinuous
images. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 10753–10764 (2021)
48. Stru¨mpler, Y., Postels, J., Yang, R., Gool, L.V., Tombari, F.: Implicit neural rep-
resentationsforimagecompression.In:EuropeanConferenceonComputerVision
(2021), https://api.semanticscholar.org/CorpusID:244954443
49. Sullivan,G.J.,Ohm,J.R.,Han,W.J.,Wiegand,T.:Overviewofthehighefficiency
videocoding(hevc)standard.IEEETransactionsoncircuitsandsystemsforvideo
technology 22(12), 1649–1668 (2012)
50. Tack, J., Kim, S., Yu, S., Lee, J., Shin, J., Schwarz, J.R.: Learning large-scale
neural fields via context pruned meta-learning (2023)
51. Tancik,M.,Mildenhall,B.,Wang,T.,Schmidt,D.,Srinivasan,P.P.,Barron,J.T.,
Ng, R.: Learned initializations for optimizing coordinate-based neural representa-
tions. In: CVPR (2021)
52. Tancik,M.,Srinivasan,P.P.,Mildenhall,B.,Fridovich-Keil,S.,Raghavan,N.,Sing-
hal,U.,Ramamoorthi,R.,Barron,J.T.,Ng,R.:Fourierfeaturesletnetworkslearn
high frequency functions in low dimensional domains. NeurIPS (2020)
53. Tang,Y.,Ding,D.,Rao,Y.,Zheng,Y.,Zhang,D.,Zhao,L.,Lu,J.,Zhou,J.:Coin:
A large-scale dataset for comprehensive instructional video analysis (2019)
54. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
55. Wallace, G.K.: The jpeg still picture compression standard. Communications of
the ACM 34(4), 30–44 (1991)
56. Wiegand,T.,Sullivan,G.,Bjontegaard,G.,Luthra,A.:Overviewoftheh.264/avc
videocodingstandard.IEEETransactionsonCircuitsandSystemsforVideoTech-
nology (2003)
57. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for
bridgingvideoandlanguage.In:ProceedingsoftheIEEEConferenceonComputer
Vision and Pattern Recognition (CVPR) (June 2016)
58. Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual lan-
guage model for video understanding. arXiv preprint arXiv:2306.02858 (2023),
https://arxiv.org/abs/2306.02858
59. Zhang, M., Zhang, A., McDonagh, S.G.: On the out-of-distribution generaliza-
tion of probabilistic image modelling. In: Neural Information Processing Systems
(2021), https://api.semanticscholar.org/CorpusID:237431305Latent-INR 1
A Network Architecture
Base Network: We use an MLP with 10 layers, width of 512 and ReLU non-
linearity as our base network f .
θ
Hypernetwork: All hypernetworks hl used to modulate a layer l of the base
network have 3 layers with a hidden dimension of 512 and tanh as non-linearity.
Unless specified, we only modulate the first hidden layer of the base network.
Latents: Each latent Z corresponding to a frame has a dimension of 512 and
t
is initialized to be standard Gaussian before training. We set our learning rate
as 5e-4 and used the standard Adam optimizer without any weight decay.
B Compression
B.1 Fourier Features
We use the multiresolution hash grid for positional encoding in all our models.
Intable5weshowresultsforfullcoordinateresolutionusingfourierfeaturesfor
positional encoding. Due to lack of a hash grid, the resulting models train upto
30% faster, but at the cost of inferior reconstruction.
B.2 Quantization
Insteadofquantizingallcomponentsequally,wenoticethatretainingthelatents
andthebasenetworkatfullprecisionprovidesbetterreconstructionatnegligible
additional storage.
B.3 Effect of latent dimension
To study the effect of latent dimension on compression, we train models by
varying it and encode the “bosphore” video from UVG dataset. The results are
presented in Figure 11. We notice that there is positive gains till dimension 512
and diminishing returns thereafter. Hence we choose that as our default latent
size in all our experiments.
C Video Retrieval
WeperformtworetrievaltasksontheCOINdataset[53]-class-level,andsegment-
level.Inbothsettings,weusethestandardvalsetasthedatabase.Forclass-level,
weusethedistinctvideo-leveltasknamesinCOINasourqueryset.Forsegment-
level, we use the set of distinct clip-level captions in COIN as our query set. We
get the CLIP ViT-B/32 text embeddings of each of these captions, and these
becomeourqueryvectors.Fordatabasevectors,weusetheper-framelearnedla-
tentsforeachvideointhedatabase.ForcomparisonwithCLIP,wereplacethese
databasevectorswiththeCLIPViT-B/32imageembeddingsforeachframe.For
class-level retrieval, we consider a result frame a positive match if it belongs to2 Maiya et al.
BPP vs PSNR for Different Latent Dimensions BPP vs SSIM for Different Latent Dimensions
35 0.9
0.8
30
0.7
Latent Dim = 64 Latent Dim = 64
25 Latent Dim = 128 0.6 Latent Dim = 128
Latent Dim = 256 Latent Dim = 256
Latent Dim = 512 Latent Dim = 512
20 Latent Dim = 1024 0.5 Latent Dim = 1024
Latent Dim = 4096 Latent Dim = 4096
0.4
0.2 0.3 0.4 0.5 0.6 0.2 0.3 0.4 0.5 0.6
Bits per Pixel (BPP) Bits per Pixel (BPP)
Fig.11: Effect of varying latent dimension across different bitrates.
Put nicely and align the quilt and the cover
.324 .323 .322 .322 .321 .320 .320 .319
Soak them in water
.307 .307 .306 .306 .305 .304 .304 .304
Pour the cooked noodles
.359 .356 .355 .355 .353 .350 .350 .349
Begin to practice Karate
.327 .326 .324 .324 .324 .324 .323 .323
Fig.12: Nearest Neighbours for segment-level matching of sample queries from
COINvalidationset.Thegreenboxesdenotethetruepositivesandtheredones
are false pos- itives. We show the inner product similarity between the image
andthecorrespondingqueryinsidethegreenboxesatthebottomofeachimage
a video with the same class label as the queried caption. On the other hand,
for segment-level retrieval, we consider a result frame a positive match only if it
belongs to a segment with the same caption as the query. Further, this search is
done over all videos. We use FAISS[20] as our retrieval implementation and use
cosine similarity as the distance metric.
We perform whole-level video retrieval as described in the main paper. For
text,weuseCLIPtocomputeafeaturefortheparagraphcaption.Forthevideo,
we compute a per-frame feature for CLIP, or use the learnt latents from Latent-
INR. For a single video feature, we then average these per-frame features. We
normalize all features, and perform retrieval by finding the closest embeddings
usingdotproductsimilarity.Bothtext-to-videoandvideo-to-textareperformed
in the same manner, the only difference being which features are used as query
and key.
)RNSP(
oitaR
esioN-ot-langiS
kaeP
)MISS(
xednI
ytiralimiS
larutcurtSLatent-INR 3
Fig.13: Additional results for Latent-INR interface with Video-LLM.
Fig.12, shows the retrieval results on the COIN data in the segment-level
setting. It can be seen that a majority of failure cases could be attributed to
visual similarity across different tasks when seen at an individual frame level.
D Video Chat
We interface our latents with learned features from Video-Llama [58] to enable
interactive chat with the compressed videos. In [58], the N video frames are
passed through a ViT based visual encoder to extract features of size k×d per
frame. These are then passed through a Query Former [24] to obtain a unified
video representation of size k ×d . This tensor is then passed to a trainable
v v
MLP layer before aligning with an LLM of our choice (LLama-2 [54] in our
models).
We align our latents Z with these per-video features of size k × d using a
v v
linear projection layer which is trained end to end. The loss function is slightly
modified to incorporate a cosine similarity loss between the terms.
L=L +λ·L (F ,FV-LLM) (7)
MSE cos t t
whereF isthepredictedfeatureandFV-LLM isthecorrespondingVideo-LLama
t t
extracted features. We show additional results of the interactive chat in Figure
13.
E Video-wise results
We plot the results for each video from UVG dataset [31] in Figures 14, 15, 16,
17, 18, 19, and 20. We show three versions of our model based on the dimension
ofthelow-rank modulatingmatrix.TheOurs-s, Ours-m,andOurs-l correspond
respectively to size = 50,100,200 The Ours-m model achieves reasonable per-
formance when compared to other methods, and at the same time can do the4 Maiya et al.
Table 5: Fourier Feature Models
Method PSNRBPP
Ours- Fourier - S 31.99 0.31
Ours- Fourier - M 33.69 0.62
Ours- Fourier - L 33.19 0.84
BPP vs PSNR for Video: beauty BPP vs SSIM for Video: beauty
35
30 0.8
25 Ours - L 0.6 Ours - S Ours - L
20 Ours - M Ours - S
NeRV - S 0.4 Ours - M
15 NeRV - L NeRV - S
NVP-L NeRV - L
10 N NV IRP V- AS NA 0.2 N NV VP P- -L S
0.2 0.4 0.6 0.8 1.0 1.2 0.2 0.4 0.6 0.8 1.0 1.2
Bits Per Pixel (BPP) Bits Per Pixel (BPP)
Fig.14: BPP vs. PSNR, SSIM for beauty.
downstream tasks of interpolation and retrieval which none of the compared
methods can.
)RNSP(
oitaR
esioN-ot-langiS
kaeP
)MISS(
xednI
ytiralimiS
larutcurtSLatent-INR 5
BPP vs PSNR for Video: bosphore BPP vs SSIM for Video: bosphore
1.0 40
0.9
35
0.8
30 Ours - L 0.7 25 Ours - S 0.6 Ours - L
Ours - M Ours - S
20 NeRV - S 0.5 Ours - M
15 N Ne VR PV -L - L 0.4 N Ne eR RV V - - S L
10 NVP-S 0.3 NVP-L
5 NIRVANA 0.2 NVP-S
0.2 0.4 0.6 0.8 1.0 1.2 0.2 0.4 0.6 0.8 1.0 1.2
Bits Per Pixel (BPP) Bits Per Pixel (BPP)
Fig.15: BPP vs. PSNR, SSIM for bosphore.
BPP vs PSNR for Video: honeybee BPP vs SSIM for Video: honeybee
40 1.0
35 0.9
30 0.8
25 O Ou ur rs s - - L S 0.7 Ours - L
20 Ours - M 0.6 Ours - S
15 N Ne eR RV V - - S L 0.5 O Neu Rrs V - -M S
NVP-L 0.4 NeRV - L
10 NVP-S NVP-L
5 NIRVANA 0.3 NVP-S
0.2 0.4 0.6 0.8 1.0 1.2 0.2 0.4 0.6 0.8 1.0 1.2
Bits Per Pixel (BPP) Bits Per Pixel (BPP)
Fig.16: BPP vs. PSNR, SSIM for honeybee.
BPP vs PSNR for Video: jockey BPP vs SSIM for Video: jockey
40 1.0
35 0.9
0.8 30 Ours - L 0.7 25 Ours - S 0.6 Ours - L
Ours - M Ours - S
20 NeRV - S 0.5 Ours - M
NeRV - L 0.4 NeRV - S
15 N NV VP P- -L S 0.3 N Ne VR PV -L - L
10 NIRVANA 0.2 NVP-S
0.2 0.4 0.6 0.8 1.0 1.2 0.2 0.4 0.6 0.8 1.0 1.2
Bits Per Pixel (BPP) Bits Per Pixel (BPP)
Fig.17: BPP vs. PSNR, SSIM for jockey.
BPP vs PSNR for Video: readysteadygo BPP vs SSIM for Video: readysteadygo
35 1.0
0.9
30 0.8
25 Ours - L 0.7 Ours - S 0.6 Ours - L
20 Ours - M Ours - S
NeRV - S 0.5 Ours - M
15 NeRV - L 0.4 NeRV - S
NVP-L NeRV - L
10 NVP-S 0.3 NVP-L
NIRVANA 0.2 NVP-S
0.2 0.4 0.6 0.8 1.0 1.2 0.2 0.4 0.6 0.8 1.0 1.2
Bits Per Pixel (BPP) Bits Per Pixel (BPP)
Fig.18: BPP vs. PSNR, SSIM for readysteadygo.
)RNSP(
oitaR
esioN-ot-langiS
kaeP
)RNSP(
oitaR
esioN-ot-langiS
kaeP
)RNSP(
oitaR esioN-ot-langiS
kaeP
)RNSP(
oitaR
esioN-ot-langiS
kaeP
)MISS(
xednI
ytiralimiS
larutcurtS
)MISS(
xednI
ytiralimiS
larutcurtS
)MISS(
xednI ytiralimiS
larutcurtS
)MISS(
xednI
ytiralimiS
larutcurtS6 Maiya et al.
BPP vs PSNR for Video: shakendry BPP vs SSIM for Video: shakendry
40 1.0
35 0.9
30 0.8
25 Ours - L 0.7 Ours - S Ours - L
20 Ours - M 0.6 Ours - S
15 N Ne eR RV V - - S L 0.5 O Neu Rrs V - -M S
10 NVP-L 0.4 NeRV - L
NVP-S NVP-L
5 NIRVANA 0.3 NVP-S
0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5
Bits Per Pixel (BPP) Bits Per Pixel (BPP)
Fig.19: BPP vs. PSNR, SSIM for shakendry.
BPP vs PSNR for Video: yachtride BPP vs SSIM for Video: yachtride
1.0 35
30 0.8
25 Ours - L 0.6 Ours - S Ours - L
20 Ours - M Ours - S
15 NeRV - S 0.4 Ours - M
NeRV - L NeRV - S
10 NVP-L 0.2 NeRV - L
NVP-S NVP-L
5 NIRVANA NVP-S
0.0
0.2 0.4 0.6 0.8 1.0 1.2 0.2 0.4 0.6 0.8 1.0 1.2
Bits Per Pixel (BPP) Bits Per Pixel (BPP)
Fig.20: BPP vs. PSNR, SSIM for yachtride.
)RNSP(
oitaR
esioN-ot-langiS
kaeP
)RNSP(
oitaR
esioN-ot-langiS
kaeP
)MISS(
xednI
ytiralimiS
larutcurtS
)MISS(
xednI
ytiralimiS
larutcurtS