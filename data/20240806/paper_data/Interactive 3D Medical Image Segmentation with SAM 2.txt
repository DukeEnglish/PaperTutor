Interactive 3D Medical Image Segmentation with SAM 2
Interactive 3D Medical Image Segmentation with SAM 2
Chuyun Shen ∗ cyshen@stu.ecnu.edu.cn
School of Computer Science and Technology
East China Normal University
Shanghai 200062, China
Wenhao Li whli@tongji.edu.cn
School of Software Engineering
Shanghai Research Institute for Intelligent Autonomous Systems
Tongji University
Shanghai 200092, China
Yuhang Shi yuhang.shi@cri-united-imaging.com
Shanghai United Imaging Healthcare Advanced Technology
Research Institute Co., Ltd.
Shanghai 201807, China
Xiangfeng Wang xfwang@cs.ecnu.edu.cn
School of Computer Science and Technology
East China Normal University
Shanghai AI Laboratory
Shanghai 200062, China
Abstract
Interactive medical image segmentation (IMIS) has shown significant potential in enhancing
segmentationaccuracybyintegratingiterativefeedbackfrommedicalprofessionals. However,
thelimitedavailabilityofenough3Dmedicaldatarestrictsthegeneralizationandrobustness
of most IMIS methods. The Segment Anything Model (SAM), though effective for 2D
images, requires expensive semi-auto slice-by-slice annotations for 3D medical images. In
this paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta SAM
model trained on videos, for 3D medical image segmentation. By treating sequential 2D
slices of 3D images as video frames, SAM 2 can fully automatically propagate annotations
from a single frame to the entire 3D volume. We propose a practical pipeline for using SAM
2 in 3D medical image segmentation and present key findings highlighting its efficiency and
potentialforfurtheroptimization. Concretely,numericalexperimentsontheBraTS2020and
the medical segmentation decathlon datasets demonstrate that SAM 2 still has a gap with
supervisedmethodsbutcannarrowthegapinspecificsettingsandorgantypes,significantly
reducing the annotation burden on medical professionals. Our code will be open-sourced
and available at https://github.com/Chuyun-Shen/SAM_2_Medical_3D.
1. Introduction
Medical image segmentation (MIS) (Ronneberger et al., 2015; Isensee et al., 2021; Zhou
et al., 2021; Cao et al., 2023) poses distinct challenges compared to natural images due to the
diverse modalities, intricate anatomical structures, unclear and complex object boundaries,
∗. ThisworkwasfinishedduringaninternshipatShanghaiUnitedImagingHealthcareAdvancedTechnology.
1
4202
guA
5
]VC.sc[
1v53620.8042:viXraChuyun Shen et al.
Figure 1: Pipeline Diagram: Utilizing Sam 2 for Propagating Slice Annotations for 3D
Interactive Medical Image Segmentation. The central slice first needs to be segmented by a
2D segmentation algorithm or annotated by a human expert either through manual labeling
or using an interactive semi-automatic algorithm. SAM 2 inputs the mask prompt and then
predicts all other slices sequentially in both directions, ultimately obtaining annotations for
all slices.
and varying object scales involved (Sharma and Aggarwal, 2010; Hesamian et al., 2019;
Huang et al., 2023). Thus, the interactive medical image segmentation (IMIS) paradigm
has garnered significant attention for substantially improving performance over conventional
methods (Xu et al., 2016; Rajchl et al., 2016; Lin et al., 2016; Castrejon et al., 2017; Wang
et al., 2018; Song et al., 2018; Liao et al., 2020a; Ma et al., 2021; Li et al., 2021).
IMIS reimagines MIS as a multi-stage, human-in-the-loop process, where medical profes-
sionals provide iterative feedback—such as marking critical points, delineating boundaries,
or defining bounding boxes—to refine model outputs. This iterative feedback loop allows
the model to integrate expert knowledge and progressively enhance segmentation accuracy.
However, the limited availability of medical data restricts most IMIS methods to a few
datasets and segmentation tasks, resulting in poor generalization and robustness.
TheSegment Anything Model (SAM)(Kirillovetal.,2023)hasshownexceptionaleffective-
nessininteractivesegmentationfornaturalimagesand,morerecently,medicalimages,thanks
to its prompt-based, zero-shot generalization capabilities (Ji et al., 2023a,b; Mohapatra
et al., 2023; Deng et al., 2023; Zhou et al., 2023; He et al., 2023a; Mazurowski et al., 2023;
Ma and Wang, 2023; Cheng et al., 2023; Zhang and Jiao, 2023; Roy et al., 2023; Huang et al.,
2Interactive 3D Medical Image Segmentation with SAM 2
Figure 2: Caparision with 3D interactive methods and supervised methods. the orange bars
represent 3D interactive algorithms, which typically handle 3D images by resizing. The blue
bars denote supervised learning algorithms, which usually process 3D images using patches.
The green bars signify algorithms based on SAM 2 segmentation. In this context, "5 clicks"
refers to interactively clicking on five points on the central 2D image using SAM, one point
per round, to generate 2D slice annotations, which are then propagated to the 3D image. "1
mask" indicates providing SAM 2 with the ground truth mask of the central 2D image, which
is then propagated to the 3D image. "Salient area" refers to results tested only on slices with
more than 256 foreground points. The bidirectional arrows indicate the difference in dice
score between SAM 2-based algorithms and the optimal algorithms. Chart 1 compares the
dice scores of 3D interactive algorithms and SAM 2 on the BraTS2020, Spleen, and Liver
datasets, while Chart 2 compares the dice scores of supervised algorithms and SAM 2 on the
Spleen, Liver, Lung, and Pancreas datasets.
2023; Mattjie et al., 2023). Despite this, SAM’s training on 2D natural images presents a
significant mismatch with the 3D nature of medical imaging modalities like CT, MRI, and
PET. Current SAM-based tools require laborious slice-by-slice annotations, even for similar
slices, which is impractical in clinical settings.
Fortunately, SAM 2 (Ravi et al., 2024), the next generation of Meta SAM trained on
videos, offers a promising solution. SAM 2 can segment entire videos based on annotations
from a single frame, utilizing interactions (clicks, boxes, or masks) on any frame to predict
spatiotemporal masks, or ‘masklets.’ Different slices of 3D medical images are sequentially
scanned and stacked over time, allowing 3D medical images to be naturally regarded as
videos. This naturally raises the following question:
Can SAM 2 segments 3D medical image based solely on 2D interactive feedbacks in a
zero-shot manner?
If we can get an affirmative answer, this paradigm shift could enable researchers to focus
on automatic segmentation for single 2D images, thus significantly reducing the amount
of expert annotation required for 3D interactive segmentation. This paper attempts to
preliminarily answer this question from an experimental perspective and has obtained some
dialectical observations. Concretely, we propose a simple and practical pipeline (Shown
3Chuyun Shen et al.
Figure 3: Dice Score Growth per Added Point of Each Round: On the BraTS2020 bench-
mark, we evaluated how much the average dice score improves per additional point in
each round for different interactive algorithms. The interactive methods used by the four
algorithms—DeepIGeoS, InterCNN, IteR-MRL, and MECCA—select 25 points in the first
round on the 3D medical image, followed by 5 additional points per round. In contrast, our
pipeline with SAM 2 adds one point per round.
in Fig.1) to enable the use of SAM 2 for 3D medical images, evaluate SAM 2’s zero-shot
performance on the Brats and some MSD datasets, and get the following key observations:
(1) The experimental results suggest that SAM 2, in a zero-shot manner, still has a gap with
supervised methods but can narrow the gap in specific settings and organ types (shown in
Fig.2). Further optimization and refinement of the medical 3D images is necessary.
(2) SAM 2’s efficiency in utilizing interactive feedback significantly surpasses that of other
3D interactive medical image segmentation algorithms. (shown in Fig.3)
Remark. Since the release of SAM 2, two works have explored its application in medical
image segmentation. Dong et al. (2024) introduce SAM 2 for 3D Medical Imaging by treating
eachsliceasa frameandleveragingamemorybank forpredictionpropagation. Theyconduct
an extensive evaluation of SAM 2 using 18 diverse medical imaging datasets, demonstrating
its performance in both single-frame 2D segmentation and multi-frame 3D segmentation.
They also identify key strategies for enhancing SAM 2’s segmentation accuracy, including
selecting the center slice of the object of interest, utilizing bidirectional propagation, and
preferring the first predicted mask over the most confident one. Another notable work,
the MedSAM-2 Framework (Zhu et al., 2024), represents the first SAM-2-based model for
medical image segmentation, addressing both 2D and 3D tasks. MedSAM-2 incorporates
the Confidence Memory Bank and Weighted Pick-up strategy, surpassing state-of-the-art
models across 15 benchmarks and 26 tasks, thereby demonstrating superior generalization
and performance. In contrast to these studies, our work does not explore different modes;
rather, it adopts settings specifically tailored for medical imaging, akin to the optimal
strategy mentioned in Dong et al. (2024). We also discussed and investigated the feasibility
of interactive annotation based on SAM 2 on 2D slices, subsequently propagating these
annotations to 3D images. Additionally, we compared the accuracy of this method with
4Interactive 3D Medical Image Segmentation with SAM 2
traditional 3D interactive medical image segmentation algorithms and supervised learning
algorithms, highlighting the gap in performance with them.
2. Related Work and Preliminaries
2.1 3D Interactive Medical Image Segmentation
In recent years, deep learning-based interactive medical image segmentation (IMIS) methods
have garnered significant interest. Xu et al. (2016) proposed using convolutional neural
networks (CNNs) for interactive image segmentation. Techniques like DeepCut (Rajchl et al.,
2016) and ScribbleSup (Lin et al., 2016) leverage weak supervision to develop interactive
segmentation approaches. Additionally, DeepIGeoS (Wang et al., 2018) incorporates a
geodesic distance metric to create a hint map for improved segmentation accuracy.
The sequential nature of the interactive segmentation process makes it well-suited for
reinforcement learning (RL). Polygon-RNN (Castrejon et al., 2017) addresses this by treating
segmentation targets as polygons and iteratively selecting polygon vertices via a recurrent
neuralnetwork(RNN).Similarly,Polygon-RNN+(Acunaetal.,2018)employsRLtoenhance
vertexselectionfurther. SeedNet(Songetal.,2018)takesadistinctapproachbydevelopingan
RL model for expert interaction generation, enabling the acquisition of simulated interaction
data at each stage of the segmentation process. IteR-MRL (Liao et al., 2020a) and BS-
IRIS (Ma et al., 2021) frame the dynamic interaction process as a Markov Decision Process
(MDP), utilizing multi-agent RL models for image segmentation. Building on IteR-MRL,
MECCA (Li et al., 2021) introduces a confidence network to address the common issue of
"interactive misunderstanding" inRL-based IMIStechniquesand toenhancethe utilization of
human feedback. Additionally, Marinov et al. (2023) provides a thorough review of the IMIS
domain. These advancements underscore the potential of deep learning and reinforcement
learning in revolutionizing interactive medical image segmentation, leading to more accurate
and efficient segmentation techniques.
2.2 Segment Anything Model and Segment Anything Model 2
The Segment Anything Model (SAM) (Kirillov et al., 2023) and its successor, the Segment
Anything Model 2 (SAM 2) (Ravi et al., 2024), introduced by Meta, are significant advance-
ments in image and video segmentation. These models aim to provide a unified framework
for segmentation tasks, drawing inspiration from foundational models in NLP and CV. SAM
focuses on image segmentation using promptable tasks to generate valid masks based on
user-defined prompts. SAM 2 extends these capabilities to video segmentation, addressing
challenges such as object motion and deformation.
Model. SAM’sarchitectureincludesanimageencoderforembeddings,apromptencoder,and
a mask decoder to integrate inputs and predict masks. SAM 2 enhances SAM’s architecture
with video processing capabilities. It introduces a temporal component for handling video
frames, generating spatio-temporal masks (masklets) to track objects across frames.
Data. SAM is trained on the SA-1B dataset, containing over 1 billion masks from 11 million
images, ensuring robust generalization. SAM 2 extends the dataset to include annotated
video sequences, allowing it to learn from dynamic scenes and temporal changes.
5Chuyun Shen et al.
Task. SAM’s promptable segmentation task generates masks based on prompts that define
target objects within an image, producing plausible masks even for ambiguous prompts.
SAM 2 expands this task to video data, generating masklets that track objects across frames,
maintaining accuracy despite object motion and varying conditions.
In summary, SAM addresses image segmentation, while SAM 2 extends capabilities to
video segmentation. For comprehensive details, refer to the primary publications (Kirillov
et al., 2023; Ravi et al., 2024) and relevant surveys (Zhang et al., 2023).
2.3 Segment Anything in Medical Images
Leveraging the foundational pre-trained models of SAM, various studies have investigated
its effectiveness in diverse zero-shot medical imaging segmentation (MIS) scenarios. For
instance, Ji et al. (2023a) performed an extensive evaluation of SAM in the everything mode
for segmenting lesion regions in different anatomical structures (e.g., brain, lung, and liver)
and imaging modalities (CT and MRI).
Further, Ji et al. (2023b) analyzed SAM’s performance in specific medical fields, such as
optical disc and cup, polyp, and skin lesion segmentation. They used both the automatic
everything mode and the manual prompt mode, employing points and bounding boxes as
prompts.
In the context of MRI brain extraction, Mohapatra et al. (2023) compared SAM’s
performance to the well-known Brain Extraction Tool (BET) from the FMRIB Software
Library. Additionally, Deng et al. (2023) evaluated SAM’s capabilities in digital pathology
segmentation tasks, including the segmentation of tumor, non-tumor tissue, and cell nuclei
in high-resolution whole-slide images. Zhou et al. (2023) applied SAM to polyp segmentation
tasks using five benchmark datasets under the everything setting.
Recently, multiple studies have rigorously assessed SAM on over ten publicly available
MIS datasets or tasks (He et al., 2023a; Mazurowski et al., 2023; Ma and Wang, 2023; Wu
et al., 2023; Huang et al., 2023; Zhang and Liu, 2023). Moreover, Liu et al. (2023) integrated
SAM with the 3D Slicer software to facilitate the design, evaluation, and application of SAM
in medical imaging segmentation.
Quantitative experimental results from these studies suggest that SAM’s zero-shot
performance is generally moderate and varies across different datasets and tasks. Specifically:
1. Using the prompt mode instead of the everything mode, SAM can exceed state-of-the-art
(SOTA) performance in tasks involving large objects, smaller quantities, and well-defined
boundaries, especially with dense human feedback. 2. However, a significant performance
gap exists between SAM and SOTA methods in tasks involving dense and amorphous object
segmentation. 3. It is also important to note that most deep learning-based MIS methods
require retraining from scratch for specific subtasks, and SAM-based methods are primarily
limited to 2D images.
3. Experiments and Results
In this study, we primarily aim to explore whether annotations made on 2D medical slices
using SAM 2 can be extended to entire 3D slices. If feasible, this could significantly reduce
the annotation cost for physicians. To ensure the generalizability of our experimental results,
we have selected two datasets: Brats2020 (Menze et al., 2014) and the medical segmentation
6Interactive 3D Medical Image Segmentation with SAM 2
decathlon (MSD) (Antonelli et al., 2022). These datasets include MRI and CT images and
encompass various commonly used medical organs and lesions.
3.1 Datasets
In this work, we primarily experiment with SAM 2 on two datasets: BraTS2020 and MSD.
The BraTS2020 dataset is part of the Brain Tumor Segmentation Challenge, focusing
on the segmentation of gliomas in pre-operative MRI scans. It includes multimodal scans
available as NIfTI files, covering native (T1), post-contrast T1-weighted (T1Gd), T2-weighted
(T2), and T2 Fluid Attenuated Inversion Recovery (T2-FLAIR) volumes. We chose T2-
FLAIR as our input 3D image modality because it is particularly effective in highlighting
differences between normal and abnormal brain tissue, making it ideal for identifying and
segmenting brain tumors. Our target is to segment the entire tumor area, including the
enhancing tumor, the peritumoral edema, and the necrotic core.
The Medical Segmentation Decathlon (MSD) dataset is another significant resource
designed to evaluate generalizable algorithms across various medical image segmentation
tasks. It includes diverse imaging modalities and anatomical structures, such as MRI and
CT scans of different organs. We utilized several tasks from MSD to segment specific organs:
Task03_Liver: Focuses on segmenting liver structures in CT images, identifying the liver.
Task06_Lung: Aims to segment lung regions in CT scans. Task07_Pancreas: Involves
segmenting the pancreas in CT images. Task09_Spleen: Targets the segmentation of the
spleen in CT scans.
These tasks help develop and benchmark robust segmentation algorithms across different
medical imaging modalities.
3.2 Evaluation Metrics
In our experiments, we utilize the Dice coefficient and the 95% Hausdorff distance (HD) as
evaluation metrics:
• Dice Coefficient (Dice, 1945): The Dice coefficient is a measure of similarity between
two sets, often used to gauge the accuracy of segmentation. It is calculated as follows:
2·∥X∩Y∥
Dice(X,Y) = 1 . (1)
∥X∥ +∥Y∥
1 1
A higher Dice coefficient indicates a greater overlap between the predicted segmentation
and the ground truth, reflecting a more accurate segmentation result.
• Normalized Surface Dice (NSD) (DeepMind, 2018): The Normalized Surface Dice
(NSD) is a metric that quantifies the similarity between two sets of points, typically
surfaces in a three-dimensional space. The NSD is defined as:
|{x ∈ X | d(x,Y) ≤ δ}∩{y ∈ Y | d(y,X) ≤ δ}|
NSD(X,Y) = , (2)
|X|+|Y|
where d(a,B) represents the minimum Euclidean distance from point a to set B, and
δ is a predefined distance threshold. This metric effectively measures the proportion
7Chuyun Shen et al.
of surface points from one set within a specified distance δ of the other set’s surface
points, normalized by the total number of surface points in both sets. The NSD score
ranges from 0 to 1, where a score closer to 1 indicates higher similarity between the
two surfaces.
3.3 Main Results
Figure 4: Interactive segmentation on a slice with SAM 2.
Inthissection, wepresenttheperformanceofSAM2underdifferentdatasetsanddifferent
settings. Our experiment loads the ‘sam2_hiera_large’ checkpoint and mainly focuses on two
settings. The first involves multiple rounds of interaction on a single slice before propagating
to the entire 3D image (shown in Fig.4). The second setting involves annotating a single
slice and then propagating it to the entire 3D image.
3.3.1 Compared with state-of-the-art methods
We compare the performance of SAM 2 with several state-of-the-art 3D interactive segmenta-
tion methods, including DeepIGeoS, InterCNN, IteR-MRL, and MECCA, on the BraTS2020,
Spleen, and Liver datasets. As shown in Table 1 and 2, SAM 2 was tested under different
configurations: with five interactive clicks (5 clicks) and a single ground truth mask (1 mask),
both with and without focusing on the salient area.
To be noticed, these state-of-the-art 3D interactive segmentation methods are trained in
resized image schema as 3D images in their original size are always too large to be loaded for
training. Also, the resized schema needs no extra process for human feedback, as the whole
image can be input into the networks. The results indicate that while SAM 2 generally lags
behindthebest-performingmethodsforBraTSandSpleen,itshowsasignificantimprovement
in the Liver dataset. Notably, using the "1 mask" setting in the salient area for the Liver
dataset, SAM 2 surpasses the best results by a considerable margin.
SAM 2 is also evaluated against several supervised methods, including nnUNet, DiNTS,
Swin UNETR, and Universal Model, across different organ segmentation tasks: Spleen, Liver,
Lung, and Pancreas, as presented in Table 2. Different from the resize schema, which is
commonly used in 3D interactive medical image segmentation, these methods are trained
8Interactive 3D Medical Image Segmentation with SAM 2
Table1: Comparisonwith3DinteractivemedicalimagesegmentationmethodsonBraTS2020,
Spleen, and Liver segmentation tasks. “SAM 2 (5 clicks)” refers to interactively clicking
on five points on the central 2D image using SAM, one point per round, to generate 2D
slice annotations, which are then propagated to the 3D image. “SAM 2 (1 mask)” indicates
providing SAM 2 with the ground truth mask of the central 2D image, which is then
propagated to the 3D image. “Salient area” refers to results tested only on slices with more
than 256 foreground points. The symbols in the following table represent the same meaning.
We use bold to indicate the best result.
Method BraTS Spleen Liver
DeepIGeoS (Wang et al., 2018) 88.54 91.97 48.57
InterCNN (Bredell et al., 2018) 88.39 93.52 59.92
IteR-MRL (Liao et al., 2020b) 89.22 91.50 62.29
MECCA (Shen et al., 2023) 91.02 94.96 71.46
SAM 2 (5 clicks) 75.52 79.59 81.32
Compared with the best results -17.03% -16.19% 13.80%
SAM 2 (1 mask) 81.29 82.77 90.18
Compared with the best results -10.69% -12.84% 26.20%
SAM 2 (5 clicks) (salient area) 81.12 92.98 84.85
Compared with the best results -10.88% -2.09% 18.74%
SAM 2 (1 mask) (salient area) 87.17 94.41 92.33
Compared with the best results -4.23% -0.58% 29.21%
with patches. Patch-based training can keep the origin resolution without losing any details,
which ensures high segmentation accuracy.
To be noticed the results of the supervised method are obtained from the MSD public
leaderboard. The zero-shot method based on SAM 2 was tested on the training set. Since it
wastrainedonanaturalimagedataset, thereisnoriskofdataleakage. Wemustacknowledge
that there may be slight differences in the distribution of the test dataset. However, for
the SAM 2-based algorithm, all datasets used are unseen, and we believe this difference is
negligible.
The results demonstrate that SAM 2, in its various configurations, achieves competitive
performance. we can see that there is a difference of 10.5% to 64.81% between SAM 2 (5
clicks) and SOTA. The difference for SAM 2 (1 mask) is relatively smaller, ranging from
3.29% to 57.84%. Particularly, when tested on salient areas, SAM 2 performs comparably to
the best results for Spleen and Liver segmentation. However, its performance varies more
significantly for Lung and Pancreas segmentation tasks.
Overall, as shown in Fig.2, we have averaged these results for a clear comparison. the
experimental results suggest that SAM 2 still has a gap with supervised methods and can
narrow the gap in specific settings and organ types. Further optimization and refinement of
the medical 3D images is necessary.
3.3.2 Statistics of improvement brought about by interaction
SAM2,benefitingfromtheSA-Vdataset,whichcomprises50.9Kvideosand642.6Kmasklets,
and its carefully designed architecture, demonstrates robust zero-shot inference capabilities
9Chuyun Shen et al.
Table 2: Comparison with supervised methods for various organs.
Spleen Liver Lung Pancreas
Method
Dice NSD Dice NSD Dice NSD Dice NSD
nnUNet (Isenseeetal.,2021) 97.43 99.89 95.75 98.55 73.97 76.02 81.64 96.14
DiNTS(Heetal.,2023b) 96.98 99.83 95.35 98.69 74.75 77.53 81.02 96.26
SwinUNETR (Tangetal.,2024) 96.99 99.84 95.35 98.34 76.60 77.40 81.85 96.57
UniversalModel(Liuetal.,2024) 97.27 99.87 95.42 98.18 80.01 81.25 82.84 96.65
SAM2(5clicks) 79.59 75.63 81.32 50.47 71.61 68.99 44.73 34.01
Comparedwiththebestresults -18.31% -24.29% -15.07% -48.86% -10.50% -15.09% -46.00% -64.81%
SAM2(1mask) 82.77 79.35 90.18 61.29 77.38 74.97 51.48 40.75
Comparedwiththebestresults -15.05% -20.56% -5.82% -37.90% -3.29% -7.73% -37.86% -57.84%
SAM2(5clicks)(salientarea) 92.98 89.71 84.85 52.69 83.93 78.68 51.45 40.82
Comparedwiththebestresults -4.57% -10.19% -11.38% -46.61% 4.90% -3.16% -37.89% -57.77%
SAM2(1mask)(salientarea) 94.41 92.46 92.33 63.13 87.48 82.39 61.04 50.65
Comparedwiththebestresults -3.10% -7.44% -3.57% -36.03% 9.34% 1.40% -26.32% -47.59%
on natural images. We evaluated the performance of SAM 2 on the BraTS2020 benchmark
and compared it with 3D interactive medical segmentation algorithms.
The algorithms DeepIGeoS, InterCNN, IteR-MRL, and MECCA adopt direct clicks on
3D medical images over five rounds, with 25 interaction points provided in the first round
and 5 additional points in each subsequent round. For SAM 2, we employed a similar setup;
however, interactions were conducted on 2D slices with only one point per round, which
SAM 2 then propagates to the entire 3D image.
To fairly compare the algorithms’ utilization of interactive feedback, we selected the Dice
Score Growth per Added Point, which is the increase in dice score during a round divided
by the number of new points added. As shown in Figure 3, SAM 2’s efficiency in utilizing
interactive feedback significantly surpasses that of other algorithms. This demonstrates that
SAM 2 possesses strong refinement capabilities based on interactions in the medical imaging
domain.
Additionally, we assessed the discrepancy between slice annotations obtained through
multiple rounds of interactive clicks and the slice ground truth. As shown in Figure 5,
performance gradually improves and approaches the ground truth with an increasing number
of clicks. This further validates the feasibility of interactive 2D slice segmentation followed
by propagation using SAM 2.
4. Conclusion
In this paper, we investigated the application of the Segment Anything Model 2 (SAM
2) for zero-shot 3D medical image segmentation. By leveraging its ability to propagate
annotations from a single 2D slice to an entire 3D volume, SAM 2 addresses the limitation
of traditional 2D trained models that they struggle with 3D medical images because they
can’t efficiently use the features and annotations from one slice across other slices. Our
experiments on the BraTS2020 and MSD datasets reveal that SAM 2, while not yet matching
theperformanceofspecializedsupervisedmethods,showspromisingresultsinspecificsettings
and organ types. The efficiency of SAM 2 in utilizing interactive feedback surpasses that of
10Interactive 3D Medical Image Segmentation with SAM 2
Figure 5: SAM 2 with different iterative steps on Brats2020 benchmark.
other 3D interactive segmentation algorithms, demonstrating its potential to significantly
reduce the annotation workload for medical professionals. However, further optimization
and refinement are necessary to enhance its performance and generalizability. This empirical
study lays the groundwork for future research into leveraging advanced models like SAM 2
to revolutionize 3D medical image segmentation, ultimately improving clinical workflows and
patient outcomes.
11Chuyun Shen et al.
References
David Acuna, Huan Ling, Amlan Kar, and Sanja Fidler. Efficient interactive annotation of
segmentation datasets with Polygon-RNN++. In CVPR, 2018.
Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-
Schneider, BennettALandman, GeertLitjens, BjoernMenze, OlafRonneberger, RonaldM
Summers, et al. The medical segmentation decathlon. Nature communications, 13(1):4128,
2022.
Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative interaction training for
segmentation editing networks. In Machine Learning in Medical Imaging: 9th Interna-
tional Workshop, MLMI 2018, Held in Conjunction with MICCAI 2018, Granada, Spain,
September 16, 2018, Proceedings 9, pages 363–370. Springer, 2018.
Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning
Wang. Swin-Unet: Unet-like pure transformer for medical image segmentation. In ECCV
Workshops Computer Vision, 2023.
Lluis Castrejon, Kaustav Kundu, Raquel Urtasun, and Sanja Fidler. Annotating object
instances with a Polygon-RNN. In CVPR, 2017.
Dongjie Cheng, Ziyuan Qin, Zekun Jiang, Shaoting Zhang, Qicheng Lao, and Kang Li.
Sam on medical images: A comprehensive study on three prompt modes. arXiv preprint
arXiv:2305.00035, 2023.
DeepMind. surface-distance, 2018. URL https://github.com/googledeepmind/
surface-distance. GitHub repository.
Ruining Deng, Can Cui, Quan Liu, Tianyuan Yao, Lucas W Remedios, Shunxing Bao,
Bennett A Landman, Lee E Wheless, Lori A Coburn, Keith T Wilson, et al. Segment
anything model (SAM) for digital pathology: Assess zero-shot segmentation on whole slide
imaging. arXiv preprint arXiv:2304.04155, 2023.
Lee R Dice. Measures of the amount of ecologic association between species. Ecology, 26(3):
297–302, 1945.
Haoyu Dong, Hanxue Gu, Yaqian Chen, Jichen Yang, and Maciej A Mazurowski. Seg-
ment anything model 2: an application to 2d and 3d medical images. arXiv preprint
arXiv:2408.00756, 2024.
Sheng He, Rina Bao, Jingpeng Li, P Ellen Grant, and Yangming Ou. Accuracy of
segment-anything model (SAM) in medical image segmentation tasks. arXiv preprint
arXiv:2304.09324, 2023a.
Yufan He, Vishwesh Nath, Dong Yang, Yucheng Tang, Andriy Myronenko, and Daguang
Xu. Swinunetr-v2: Stronger swin transformers with stagewise convolutions for 3d medical
image segmentation. In International Conference on Medical Image Computing and
Computer-Assisted Intervention, pages 416–426. Springer, 2023b.
12Interactive 3D Medical Image Segmentation with SAM 2
Mohammad Hesam Hesamian, Wenjing Jia, Xiangjian He, and Paul Kennedy. Deep learning
techniques for medical image segmentation: achievements and challenges. Journal of
Digital Imaging, 32:582–596, 2019.
Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan
Yu, Jiongquan Chen, Chaoyu Chen, et al. Segment anything model for medical images?
arXiv preprint arXiv:2304.14660, 2023.
Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein.
nnU-Net: aself-configuringmethodfordeeplearning-basedbiomedicalimagesegmentation.
Nature Methods, 18(2):203–211, 2021.
Ge-Peng Ji, Deng-Ping Fan, Peng Xu, Ming-Ming Cheng, Bowen Zhou, and Luc Van Gool.
SAM struggles in concealed scenes – empirical study on “segment anything". arXiv preprint
arXiv:2304.06022, 2023a.
Wei Ji, Jingjing Li, Qi Bi, Wenbo Li, and Li Cheng. Segment anything is not always
perfect: An investigation of SAM on different real-world applications. arXiv preprint
arXiv:2304.05750, 2023b.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,
Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross
Girshick. Segment anything. arXiv:2304.02643, 2023.
Wenhao Li, Qisen Xu, Chuyun Shen, Bin Hu, Fengping Zhu, Yuxin Li, Bo Jin, and Xiangfeng
Wang. Interactive medical image segmentation with self-adaptive confidence calibration.
arXiv preprint arXiv:2111.07716, 2021.
Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng Wang,
and Ya Zhang. Iteratively-refined interactive 3d medical image segmentawenhaotion with
multi-agent reinforcement learning. In CVPR, 2020a.
Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng Wang,
and Ya Zhang. Iteratively-refined interactive 3d medical image segmentation with multi-
agent reinforcement learning. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 9394–9402, 2020b.
Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. ScribbleSup: Scribble-supervised
convolutional networks for semantic segmentation. In CVPR, 2016.
JieLiu,YixiaoZhang,KangWang,MehmetCanYavuz,XiaoxiChen,YixuanYuan,Haoliang
Li, Yang Yang, Alan Yuille, Yucheng Tang, et al. Universal and extensible language-vision
modelsfororgansegmentationandtumordetectionfromabdominalcomputedtomography.
Medical Image Analysis, page 103226, 2024.
Yihao Liu, Jiaming Zhang, Zhangcong She, Amir Kheradmand, and Mehran Armand.
SAMM (segment any medical model): A 3D slicer integration to SAM. arXiv preprint
arXiv:2304.05622, 2023.
13Chuyun Shen et al.
Chaofan Ma, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng Wang, and
Ya Zhang. Boundary-aware supervoxel-level iteratively refined interactive 3d image seg-
mentationwithmulti-agentreinforcementlearning. IEEE Transactions on Medical Imaging,
40(10):2563–2574, 2021.
JunMaandBoWang. Segmentanythinginmedicalimages. arXiv preprint arXiv:2304.12306,
2023.
Zdravko Marinov, Paul F Jäger, Jan Egger, Jens Kleesiek, and Rainer Stiefelhagen. Deep
interactive segmentation of medical images: A systematic review and taxonomy. arXiv
preprint arXiv:2311.13964, 2023.
Christian Mattjie, Luis Vinicius de Moura, Rafaela Cappelari Ravazio, Lucas Silveira
Kupssinskü,OtávioParraga,MarceloMussiDelucis,andRodrigoCoelhoBarros. Exploring
the zero-shot capabilities of the segment anything model (sam) in 2d medical imaging: A
comprehensive evaluation and practical guideline. arXiv preprint arXiv:2305.00109, 2023.
Maciej A Mazurowski, Haoyu Dong, Hanxue Gu, Jichen Yang, Nicholas Konz, and Yixin
Zhang. Segment anything model for medical image analysis: an experimental study. arXiv
preprint arXiv:2304.10517, 2023.
Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani,
Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The
multimodal brain tumor image segmentation benchmark (brats). IEEE transactions on
medical imaging, 34(10):1993–2024, 2014.
SoveshMohapatra, AdvaitGosai, andGottfriedSchlaug. Brainextractioncomparingsegment
anything model (SAM) and fsl brain extraction tool. arXiv preprint arXiv:2304.04738,
2023.
Martin Rajchl, Matthew CH Lee, Ozan Oktay, Konstantinos Kamnitsas, Jonathan Passerat-
Palmbach, Wenjia Bai, Mellisa Damodaram, Mary A Rutherford, Joseph V Hajnal,
Bernhard Kainz, et al. DeepCut: Object segmentation from bounding box annotations
using convolutional neural networks. IEEE Transactions on Medical Imaging, 36(2):
674–683, 2016.
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma,
Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting
Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár,
and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv
preprint, 2024.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for
biomedical image segmentation. In MICCAI, 2015.
SaikatRoy,TassiloWald,GregorKoehler,MaximilianRRokuss,NicoDisch,JuliusHolzschuh,
David Zimmerer, and Klaus H Maier-Hein. SAM.MD: Zero-shot medical image segmen-
tation capabilities of the segment anything model. arXiv preprint arXiv:2304.05396,
2023.
14Interactive 3D Medical Image Segmentation with SAM 2
Neeraj Sharma and Lalit M Aggarwal. Automated medical image segmentation techniques.
Journal of Medical Physics, 35(1):3, 2010.
Chunlin Shen, Wen Li, Qiang Xu, et al. Interactive medical image segmentation with
self-adaptive confidence calibration. Frontiers in Information Technology and Electronic
Engineering, 24(9):1332–1348, 2023. doi: 10.1631/FITEE.2200299.
Gwangmo Song, Heesoo Myeong, and Kyoung Mu Lee. SeedNet: Automatic seed generation
with deep reinforcement learning for robust interactive segmentation. In CVPR, 2018.
Yucheng Tang, Jie Liu, Zongwei Zhou, Xin Yu, and Yuankai Huo. Efficient 3d representation
learningformedicalimageanalysis.WorldScientificAnnualReviewofArtificialIntelligence,
2:2450002, 2024.
Guotai Wang, Maria A Zuluaga, Wenqi Li, Rosalind Pratt, Premal A Patel, Michael Aertsen,
Tom Doel, Anna L David, Jan Deprest, Sébastien Ourselin, et al. DeepIGeoS: A deep
interactive geodesic framework for medical image segmentation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 41(7):1559–1572, 2018.
Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei Wang, Yanwu Xu, Yueming Jin, and
Tal Arbel. Medical SAM adapter: Adapting segment anything model for medical image
segmentation. arXiv preprint arXiv:2304.12620, 2023.
Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S Huang. Deep Interactive
Object Selection. In CVPR, 2016.
Chunhui Zhang, Li Liu, Yawen Cui, Guanjie Huang, Weilin Lin, Yiqian Yang, and Yuehong
Hu. A comprehensive survey on segment anything model for vision and beyond. arXiv
preprint arXiv:2305.08196, 2023.
Kaidong Zhang and Dong Liu. Customized segment anything model for medical image
segmentation. arXiv preprint arXiv:2304.13785, 2023.
Yichi Zhang and Rushi Jiao. How segment anything model (SAM) boost medical image
segmentation? arXiv preprint arXiv:2305.03678, 2023.
Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou
Yu. nnFormer: Interleaved transformer for volumetric segmentation. arXiv preprint
arXiv:2109.03201, 2021.
Tao Zhou, Yizhe Zhang, Yi Zhou, Ye Wu, and Chen Gong. Can SAM segment polyps? arXiv
preprint arXiv:2304.07583, 2023.
Jiayuan Zhu, Yunli Qi, and Junde Wu. Medical sam 2: Segment medical images as video via
segment anything model 2. arXiv preprint arXiv:2408.00874, 2024.
15Chuyun Shen et al.
A. Visualization Results
Figure A.1: SAM 2 segmentation 2D slices with 5 interactive clicks feedback. We show the
ground truth mask as orange and the predicted mask as blue. We show foreground clicks in
blue and background clicks in orange.
To further qualitatively study the accuracy of SAM 2 on medical images, we visualized
the multi-round 2D slice interactive segmentation for brain tumors and different organs in
Fig.A.1. Each row represents a different dataset, with the first column showing the Ground
Truth, followed by the results of each subsequent round. It can be observed that SAM 2
effectively refines the results gradually, producing masks that closely resemble the ground
truth.
Furthermore, using the interactive masks obtained from the fifth round, we applied SAM
2 to propagate the segmentation across the entire 3D image. The results on different datasets
are shown in Fig.A.2. Each row represents a different dataset, with odd-numbered columns
showingthegroundtruthofcorrespondingslices, followedbythepredictedmasks. Significant
differences can be seen between slices of the 3D image, indicating substantial morphological
16Interactive 3D Medical Image Segmentation with SAM 2
Figure A.2: Propagation: We show the ground truth mask as orange and the predicted mask
as blue.
variations. In most cases, SAM 2 demonstrates good performance, although there are some
failures, such as the last column in the spleen segmentation task, where the target regions in
earlier slices were not identified by SAM 2.
These visual results demonstrate the zero-shot capability of SAM 2, which can achieve
relatively accurate segmentation on medical images despite the significant differences from
natural images. However, the precision of the segmentation still requires further improvement.
17