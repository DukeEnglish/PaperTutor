On Using Quasirandom Sequences in Machine Learning for Model
Weight Initialization
Andriy Miranskyy, Adam Sorrenti, and Viral Thakar∗
Department of Computer Science, Toronto Metropolitan University, Toronto, Canada
{avm, adam.sorrenti, vthakar}@torontomu.ca
Abstract
The effectiveness of training neural networks directly impacts computational costs, resource
allocation, and model development timelines in machine learning applications. An optimizer’s
ability to train the model adequately (in terms of trained model performance) depends on the
model’sinitialweights. Modelweightinitializationschemesusepseudorandomnumbergenerators
(PRNGs) as a source of randomness.
We investigate whether substituting PRNGs for low-discrepancy quasirandom number gen-
erators (QRNGs) — namely Sobol’ sequences — as a source of randomness for initializers can
improve model performance. We examine Multi-Layer Perceptrons (MLP), Convolutional Neural
Networks (CNN), Long Short-Term Memory (LSTM), and Transformer architectures trained on
MNIST, CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses
ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal); Orthogonal, Random
Normal, Truncated Normal, and Random Uniform. Models with weights set using PRNG- and
QRNG-based initializers are compared pairwise for each combination of dataset, architecture,
optimizer, and initialization scheme.
Our findings indicate that QRNG-based neural network initializers either reach a higher
accuracy or achieve the same accuracy more quickly than PRNG-based initializers in 60% of
the 120 experiments conducted. Thus, using QRNG-based initializers instead of PRNG-based
initializers can speed up and improve model training.
1 Introduction
The effectiveness of training deep neural networks has a direct impact on computational costs,
resource allocation, and model development timelines in machine learning applications [20], [49]. The
initialization of the neural network’s weights plays a critical role in its training efficiency: random
initialization methods can introduce variations that slow the training or hinder convergence [20],
[51], [57]. Pseudorandom number generators (PRNGs) are traditionally used to initialize neural
networks[18], [24], [34]. However, thereareotherwaystogeneratesequencesofrandomnumbers. For
example, low-discrepancy quasirandom number generators (QRNGs), which provide high uniformity
in filling high-dimensional spaces, have proved effective in numerical integration in high-dimensional
spaces via Monte Carlo simulations [25].
QRNGs are also used for optimization [33]. Compared with PRNGs, low-discrepancy QRNGs
ensure a more uniform exploration of the search space. Uniformity allows for a thorough examination
∗Authors are listed alphabetically.
1
4202
guA
5
]GL.sc[
1v45620.8042:viXraof the parameter space, potentially resulting in more reliable results for functions with multiple local
optima. This leads to the following question.
Can QRNG uniformity properties be applied to neural network weight initialization? Our
hypothesis is that QRNG-based neural network weight initializers can accelerate neural network
training. The following research questions will help us explore various aspects of this hypothesis.
RQ1. What effect does the selection of optimization algorithms have on deep neural networks’
performance when initialized using the QRNG-based weight initialization scheme?
RQ2. HowdoesaQRNG-basedweightinitializationschemeaffectdeepneuralnetworks’performance?
RQ3. How does the choice of deep neural network architecture affect the model’s performance when
it is initialized using the QRNG-based weight initialization scheme?
RQ4. How does the change in the dataset affect the performance of deep neural networks initialized
with the QRNG-based weight initialization scheme?
We empirically investigate how a specific type of QRNG, namely Sobol’ sequences [58], affects the
training of four neural network architectures: Multi-Layer Perceptrons (MLP) [18], Convolutional
Neural Networks (CNN) [18], Long Short-Term Memory (LSTM) [23], and Transformer [64]. Models’
kernel weights are initialized using ten different initializers: Glorot Uniform [17], Glorot Normal [17],
He Uniform [21], He Normal [21], Lecun Uniform [30], Lecun Normal [30], Orthogonal [55], Random
Uniform [34], Random Normal [34], and Truncated Normal [34]. Models are trained on two types
of data: (a) images, represented by Modified National Institute of Standards and Technology
(MNIST) [36] and Canadian Institute For Advanced Research, 10 classes (CIFAR-10) [32] datasets,
and (b) natural language, represented by Internet Movie Database (IMDB) [38] dataset. The
stochastic gradient descent (SGD) [4], [53] and Adam [29] methods optimize the models’ weights.
Summary of contributions We investigate whether QRNG-based initializers can accelerate train-
ing in four neural network architectures. Among our contributions are: (a) developing QRNG-based
sampling mechanisms to implement ten popular initializers, (b) providing a Keras-based implemen-
tation of these initializers1, and (c) evaluating QRNG’s impact on training speed across different
datasets and optimization methods. Our research confirms the hypothesis by addressing the research
questions. We showed that QRNG-based initializers improve accuracy or accelerate neural network
training in 60% of the 120 experiments conducted. The top 25% of the improved maximum median
accuracies fall between ≈ 0.0775 and 0.3550. The negative side effects of using QRNG are minimal,
with the top 25% of observed losses leading to a decrease in accuracy between ≈ 0.0031 and 0.0402.
Data complexity may contribute to variability of results. QRNG significantly improved nine of the
ten initializers (except Random Uniform). A generalizability assessment requires further research.
The remainder of the paper is organized as follows. Section 2 discusses the mechanisms for
implementing initializers. Section 3 evaluates the initializers empirically, presents results, discusses
limitations, and poses open questions. Section 4 reviews related work. Finally, Section 5 concludes
the paper.
2 Methods
Below we provide details of the setup needed to compare the performance of PRNGs and QRNGs
for weight initialization of neural networks.
1Shared via https://github.com/miranska/qrng-init [43].
2Random number generators Pseudorandom numbers can be generated using various algorithms.
As a baseline, we use the Philox PRNG [54] (as it is the default PRNG in Keras with Tensorflow
backend [40], a popular machine learning library, see Appendix B.1 for details); see Appendix B.1.1
for additional details. Furthermore, we will use the ubiquitous Mersenne Twister PRNG [41] to
compare the performance of TensorFlow’s Philox implementation.
VariousQRNGsexist,e.g.,thosebasedonFaure[14],Halton[19],Niederreiter[46],andSobol’[58]
sequences. They provide a deterministic and evenly distributed set of points in multidimensional
space, thereby overcoming some limitations of traditional pseudorandom sequences [25]. Empirical
evidence suggests that Sobol’ sequences are generally more homogeneous [25]. Therefore, we chose to
use Sobol’ sequences in our study. Further details about Sobol’ sequences are given in Appendix A.
Distributions Detailed information about the implementation of the distributions using PRNGs
and QRNGs is provided in Appendix B. We provide a brief summary below.
Three distributions are used in the initializers under study: random uniform, random normal,
and truncated normal. Keras implementations of uniform distribution with TensorFlow backend use
the Philox PRNG; Box-Muller transforms and sampling with rejection are utilized for drawing from
random normal and truncated normal distributions. Further details are provided in Appendix B.1.
In order to implement QRNG-based distributions, we use inverse transform sampling, taking
values drawn from the Sobol’ sequences as input (see Appendix B.2 for details). For Mersenne
Twister PRNG-based distributions, we follow a similar approach: doing inverse sampling of random
sequences generated by the Mersenne Twister algorithm (see Appendix B.4 for additional details).
Initializers Theteninitializersunderstudycanbeclassifiedintothreegroups: thoseusingrandom
uniform, random normal, or truncated normal distributions. (a) Random uniform distribution is
used by Glorot Uniform, He Uniform, Lecun Uniform, and Random Uniform initializers, (b) Normal
distribution — by Orthogonal and Random Normal initializers, and (c) Truncated normal — by
Glorot Normal, He Normal, Lecun Normal, and Truncated Normal initializers. The initializer details
are provided in the Appendix C.
Initializers can also be categorized on the basis of how they handle random numbers. This
categorization will be helpful during the analysis of the results. We create three groups: shape-
agnostic, shape-dependent, and orthogonal. A shape-agnostic initializer, such as Random Normal,
Random Uniform, and Truncated Normal, uses random numbers independent of the object’s shape
whose weights we are initializing. A shape-dependent initializer2, such as Glorot Uniform, Glorot
Normal, He Uniform, He Normal, Lecun Uniform, and Lecun Normal, transforms random values
accordingtotheparametersthatgoverntheunderlyingdistributionsaswellastheshapesoftheinput
tensors. Finally, an orthogonal initializer deserves its own category: while primarily dependent on
the parameters governing the underlying distributions, it also performs a significant transformation
of the data (namely QR decomposition), which has a dramatic impact on the underlying random
sequences.
3 Results
3.1 Experimental setup
Datasets Our models are trained on two image classification datasets, MNIST and CIFAR-10,
and one text classification dataset, IMDB. The details of the datasets and data preparations are
2All shape-dependent initializers under study use variance scaling.
3given in Appendix F.
Deep Neural Network Architectures Our models utilize four fundamental deep neural network
architectures: MLP, CNN, LSTM, and Transformer, with detailed configurations provided in
Appendix D. The architectures we use are intentionally simplified to see how different random
number generators affect kernel weights (and avoid the effects of more sophisticated techniques).
Consequently, we have omitted layers (such as extra regularization and dense layers) to minimize
their influence on performance. We keep the number of kernel weights small, isolating the effects of
random number generators (a larger number of weights gives any model better predictive power,
as demonstrated in Appendix H (Figure 12) and discussed in [71]. This simplification will reduce
overall model efficacy, but our primary objective is to assess the effects of random generators, not to
optimize the performance for any particular architecture.
Random number generators TheQRNGunderstudyisbasedonSobol’sequences(seeSection2)
implemented in SciPy v.1.13.1 software [56], [65], which utilizes directional integers computed by [26].
More details are available in Appendix A.
Keras v.3.3.3 [11] with TensorFlow v.2.16.1 [40] backend provides the baseline Philox PRNG [54]
(see Appendix B.1), while NumPy library v.1.24 [42] gives us Mersenne Twister PRNG [41].
Distributions and Initializers We intialize models’ kernel weights using ten different initializers
listed in Section 2. Appendices C and B contain implementation details of initializers and their
underlying distributions, respectively. Essentially, we modify the Keras [11] classes responsible for
sampling random numbers and creating initializers. Inverse transform sampling is performed using
functions from the SciPy v.1.13.1.
Optimizers Our models are trained using Adam and SGD algorithms. It allows us to examine how
initialization methods and architectural choices are affected by the choice of optimizer. Appendix E
lists the model hyperparameters.
Combinations and repetitions of experiments For the main experiments, we explore the
following combination of models, optimizers, and datasets. With two image classification datasets,
we explore two architectures (MLP and CNN) and two optimizers (Adam and SGD), resulting
in eight experiments. Four experiments are conducted on the text classification dataset (IMDB)
using two architectures (LSTM and Transformer) and two optimizers (Adam and SGD). The 12
(= 8+4) combinations are tested against ten initializers and two random generators (PRNG and
QRNG), bringing the number of combinations to 240. Since optimization is stochastic, experiments
are repeated 100 times to assess robustness.
Training For each individual experiment run, we adhered to the following settings. Every
experiment should have the same epoch count and batch size for consistent comparisons. The models
are trained for 30 epochs, providing a substantial training duration for model convergence and
performance assessment. A fixed batch size of 64 was used, promoting efficient gradient updates and
facilitating fair comparisons.
Testbed In order to eliminate the sources of randomness from GPU software stacks, we chose a
CPU-only testbed. The experiments were performed on high-performance computing clusters with
4Intel Xeon Gold 6148 Skylake CPUs. All experiments were allocated 4 GB of memory and 2 CPU
cores.
3.2 Measure performance and analyze the results
3.2.1 Compute the central tendency and variability
To assess model performance, we evaluate prediction accuracy on a dataset’s test segment after i
epochs of training. That is, we use an epoch-to-accuracy metric to compare models across different
experimental settings (described in Section 3.1).
Given the stochastic nature of optimization, 100 experimental repetitions can result in varying
accuracy values, detailed in Appendix J. Thus, we cannot use the epoch-to-accuracy metric directly.
Instead, we will measure its central tendency and variability. To mitigate the influence of outliers
and skewness in the data, we use the median rather than the mean to measure central tendency and
the interquartile range3 (IQR) rather than the standard deviation to measure variability.
Our analysis compares the performance of two models, each employing a PRNG- or QRNG-based
source of randomness, across the same dataset, architecture, optimizer, and initializer algorithm. As
discussed in Section 3.1, we have 240 experiments, resulting in 120 pairs of models.
Specifically, we calculate the percentage of relative change in median epoch-to-accuracy (i.e., the
number of epochs required to reach a specific median accuracy threshold):
E (A)+∆ −E (A)
Q Q P
E(A) = ×100, (1)
E (A)
P
where E(A) is an efficiency metric, E (A) and E (A) are the epoch numbers at which the QRNG-
Q P
and PRNG-based versions of an initializer first reach or exceed the median accuracy A, respectively,
and ∆ = 4 represents additional epochs required so select a good starting seed for QRNG initializer
Q
(see Appendix C.2.2 for details). Using this formula, we can determine how much faster one model
is compared to another in terms of the median epoch-to-accuracy.
Additionally, we assess the variation in accuracy between the two models by the difference of
their IQRs:
D(A) = D (A)−D (A), (2)
Q P
where D (A) and D (A) are the IQR of accuracy measurements for the QRNG-based and PRNG-
Q P
based versions of the initializer at epochs E (A) and E (A), respectively.
Q P
Further details on calculating our performance metrics, E(A) and D(A), are provided with
numeric examples in Appendix G.
3.2.2 Application of performance metrics
In order to compare the performance of models, let us construct four simple classification rules
denoted by S . Note that the maximum accuracy values Amax may differ with experimental
(·) (·)
configuration. We are not comparing the efficacy of different initializers across all configurations but
rather examining a specific pair of models individually, as discussed in Section 3.2.1.
Compare maximum accuracy values To assess the correctness of prediction, we will use the
values Amax and Amax, which represent the maximum median accuracy attained by the PRNG-based
P Q
and QRNG-based models, respectively, for each combination of dataset, architecture, optimizer, and
initializer algorithm.
3The IQR of a vector x is computed as the difference between the third and first quartile of the data in x.
5Our first step is to determine which of the two models achieved the highest accuracy. To do this,
we perform a non-parametric, non-paired Mann-Whitney U test [39] implemented in R v.4.4.0 [50].
The null hypothesis is that the distributions of ζ and η differ by a location shift of 0. The one-sided
alternative “greater” hypothesis is that ζ is shifted to the right of η, and the one-sided alternative
“less” hypothesis is that ζ is shifted to the left of η.
In our study, ζ represents a distribution of 100 accuracy values achieved by the PRNG-based
model at the earliest epoch when Amax was reached, denoted E(Amax). Similarly, η represents a
P P
distribution of 100 accuracy values achieved by the QRNG-based model at the earliest epoch when
Amax was reached, denoted E(Amax). We set the p-value for the U test at 0.05, resulting in the
Q Q
following classification rule:

win, if p-value for the U test with “less” alternative hypothesis < 0.05


S
(cid:2) Amax,Amax(cid:3)
= loss, if p-value for the U test with “greater” alternative hypothesis < 0.05 .
A P Q

tie, otherwise
(3)
Compare efficiency and relative variability of achieving higher accuracy We also need a
simple but informative mechanism to aggregate the large amount of values returned by E(A) and
D(A). Let us examine the maximum median epoch-to-accuracy values reached by various setups.
To compare the “speed” with which one model achieves higher accuracy than another, we define:
A =
min(cid:0) Amax,Amax(cid:1)
, (4)
m P Q
where Amax and Amax represent the maximum accuracies attained by the PRNG-based and QRNG-
P Q
based models, respectively, for each combination of dataset, architecture, optimizer, and initializer
algorithm. The value A reflects the highest accuracy achieved by the less effective model in each
m
pair4. E(A ) indicates the relative speed at which each model reaches A , while D(A ) measures
m m m
the difference in variability at the epochs where each model reaches A .
m
The following are classification rules we use to summarize and compare the performance (in
terms of the best accuracy achieved) where QRNG might outperform PRNG. For E(A ), we simply
m
assess the count of epochs with the following conditions:

win, if E(A ) < 0
 m

S [E(A )] = tie, if E(A ) = 0 . (5)
E m m

loss, if E(A ) > 0
m
For D(A ), we use the non-parametric Fligner-Killeen median test5 [15] of homogeneity of variances
m
(implemented in R v.4.4.0) to assess whether variability is similar. The null hypothesis is that the
variances are the same in each group. We use the following classification rules since the test does
not have a one-tailed version:

 win, if the Fligner-Killeen test’s p-value < 0.05 and D(A m) < −0.01


loss, if the Fligner-Killeen test’s p-value < 0.05 and D(A ) > 0.01
m
S [D(A )] = .
D m
 tie, if the Fligner-Killeen test’s p-value < 0.05 and −0.01 ≤ D(A m) ≤ 0.01


tie, if the Fligner-Killeen test’s p-value ≥ 0.05
(6)
4The formulation of A ensures that we do not observe the special case described in Appendix G.
m
5The test is considered robust against deviations from normality [12].
6From a practical perspective, a negligible difference is defined as an IQR difference of ±0.01, and
these outcomes are classified as ties. An example of computing S [E(A )] and S [D(A )] is given
E m D m
in Appendix G.
Joining the classification rules To conclude whether a PRNG- or QRNG-based model achieved
better results in a given pair, we need to aggregate various permutations of the outputs returned by
the three S rules. Table 1 presents the aggregation of permutations observed in our experiments.
(·)
Werefertothisaggregationasthe“finaloutcome”. Forbrevity,weonlyshowasubsetofpermutations
of our experiments’ outcomes.
Table 1: The rules for aggregating the results of the three S classification rules. The symbol *
(·)
denotes any outcome.
Final outcome S S S Mask Description
A E D
loss * * l:l,*,* QRNG-based model achieved a lower level of accuracy.
Loss
tie loss * l:t,l,* QRNG-based model achieved comparable accuracy slower.
Tie tie tie tie t:t,t,t QRNG-based model achieved comparable accuracy simulta-
neously (with similar variation).
tie win win or tie w:t,w,wt QRNG-based model achieved comparable accuracy faster
(with less or similar variation).
Win win loss * w:w,l,* QRNG-basedmodelachievedahigherlevelofaccuracyfaster
and reached A values slower.
m
win tie win or tie w:w,t,wt QRNG-basedmodelachievedahigherlevelofaccuracyfaster
and reached A values simultaneously (with less or similar
m
variation).
win win loss w:w,w,l QRNG-basedmodelachievedahigherlevelofaccuracyfaster
and reached A values faster (with higher variation).
m
win win win or tie w:w,w,wt QRNG-basedmodelachievedahigherlevelofaccuracyfaster
andreachedA valuesfaster(withlessorsimilarvariation).
m
3.3 Answers to the Research Questions
This section addresses the research questions and evaluates the hypotheses presented in Section 1.
As mentioned in Section 3.1, we deliberately simplify the models to isolate the effect of random
number generators, so the absolute accuracy of the models is far from best-in-class. However, we
are interested in the differences in performance between a pair of models rather than their absolute
accuracy.
The raw data that were used to compute the E(A) and D(A) values for the 120 model pairs are
given in Appendix J. Appendix I provides a summary of the values of Amax, E (A ), and D (A ),
(·) (·) m (·) m
where (·) denotes either P or Q. The average values of Amax−Amax, denoted by α¯, and the average
Q P
values of E(A ), denoted by E¯(A ), are also given in Appendix I. We will use these data to answer
m m
the questions below.
3.3.1 Overview of the results
Before addressing specific research questions, let us review the overall results. Table 2 shows that
the QRNG-based models win 60% of the 120 experiments, tie 1%, and lose 39%.
Drillingdownintospecificreasonsforwinning, wecanseethat54%ofthe72winningexperiments
are all-around wins (w:w,w,wt), where QRNG-based models achieve a higher level of accuracy
7faster. In 22% of cases, QRNG-based models achieve higher accuracy, but A values were reached
m
more slowly. Another 13% result in the outcome w:t,w,wt, where QRNG-based models achieve
comparable accuracy faster. Similarly, 10% result in the outcome w:w,w,l, where QRNG-based
models achieve a higher level of accuracy faster but with higher variation. Finally, 1% result in the
outcome w:w,t,wt, where QRNG-based models achieve a higher level of accuracy and reach the
A value simultaneously.
m
We have a single tie, where QRNG- and PRNG-based models achieve a comparable level of
accuracy simultaneously.
Losses are divided into two groups: 53% of the 47 losing experiments are those in which QRNG-
based models achieve a similar level of accuracy more slowly (outcome l:t,l,*). The remaining 47%
of the experiments result in outcome l:l,*,*, where the QRNG-based model achieved a lower level
of accuracy than the PRNG-based model.
Furthermore, Figure 1 indicates that when QRNG-based models lose, the decrease in the
maximum median accuracy (Amax−Amax) is minimal. Conversely, when QRNG-based models win,
Q P
the improvement in maximum median accuracy is substantial. This is further illustrated in the
empirical cumulative distribution function in Figure 2. The top 25% of losses range between ≈ 0.0031
and 0.0402, while the top 25% of wins range between ≈ 0.0775 and 0.3550. Therefore, it may be
beneficial to try QRNG-based models.
Now, let us explore the answers to the specific research questions.
Table 2: Count of the final outcomes grouped by optimizer, dataset, and model.
Adam SGD Grand
Outcome CIFAR-10 IMDB MNIST CIFAR-10 IMDB MNIST Total
CNN MLP LSTM Transf. CNN MLP CNN MLP LSTM Transf. CNN MLP
l:l,*,* 4 1 1 2 5 5 1 2 1 22
l:t,l,* 2 6 1 3 1 2 5 4 1 25
LossTotal 6 1 7 3 8 6 1 2 5 6 1 1 47
t:t,t,t 1 1
TieTotal 1 1
w:t,w,wt 2 2 5 9
w:w,l,* 5 1 2 2 3 3 16
w:w,t,wt 1 1
w:w,w,l 2 1 3 1 7
w:w,w,wt 4 2 4 4 6 2 8 9 39
WinTotal 4 9 2 7 2 4 9 8 5 4 9 9 72
GrandTotal 10 10 10 10 10 10 10 10 10 10 10 10 120
3.3.2 RQ1. What effect does the selection of optimization algorithms have on deep
neural networks’ performance when initialized using the QRNG-based weight
initialization scheme?
Table2showssignificantperformancedifferencesinmodelsbasedontheoptimizerused. Modelsusing
QRNG-based initializers perform better with the SGD optimizer compared to Adam. Specifically,
with SGD, models that use QRNG-based initializers outperform PRNG-based ones in 73% of
experiments without ties (α¯ ≈ 0.06). With Adam, they win in 47% of 60 cases (α¯ ≈ 0.05) and tie in
2%. This superior performance with SGD is likely due to SGD’s inability to dynamically adjust the
840
30
Final outcome
loss
20 tie
win
10
0
0.0 0.1 0.2 0.3
max max
A −A
Q P
Figure 1: Histogram of the Amax−Amax values for three final outcomes.
Q P
learning rate, while Adam’s dynamic learning rate can compensate for the poorer starting conditions
of the PRNG initializer. For ties and losses, α¯ ≈ 0.00, reinforcing that QRNG-based initialization
may be beneficial.
3.3.3 RQ2. How does a QRNG-based weight initialization scheme affect deep neural
networks’ performance?
Table 3 indicates that QRNG initializers generally outperform PRNG initializers. Shape-agnostic
initializers win in 47% of 36 experiments without ties; shape-dependent initializers win in 67% of 72
cases and tie in 1%; orthogonal initializers win in 58% of 12 experiments without ties. The average
improvement in the maximum median accuracy for shape-agnostic, shape-dependent, and orthogonal
initializers is α¯ ≈ 0.02%, 0.07%, and 0.06%, respectively.
For individual initializers, Random Uniform performs poorly, winning only in 25% of the cases,
while Random Normal and Truncated Normal win in 58%. For shape-dependent initializers, He
Normal wins in 50% of the cases, while others win or tie in 67% to 75%. Thus, QRNG benefits a
range of initializers, perhaps with the exception of Random Uniform.
3.3.4 RQ3. How does the choice of deep neural network architecture affect the model’s
performance when it is initialized using the QRNG-based weight initialization
scheme?
For CNN architecture, QRNG-based initializers win in 60% of 40 cases (α¯ ≈ 0.08) without tie,
for MLP — win in 75% of 40 cases (α¯ ≈ 0.07) without tie, for LSTM — win in 35% of 20 cases
(α¯ ≈ 0.00) and tie in 5%, for Transfomer — win in 55% of 20 cases (α¯ ≈ 0.01) without ties.
Thus, the strongest improvements come from MLP and CNN architectures trained on computer
9
tnuoc
stnemirepxE1.00
0.75
Final outcome
loss
0.50
tie
win
0.25
0.00
1e-05 1e-04 1e-03 1e-02 1e-01
|Amax −Amax|+10−5
Q P
Figure2: Empiricalcumulativedistributionfunctionof|Amax−Amax|+10−5 forthreefinaloutcomes.
Q P
The 10−5 term is added to enable rendering of the x-axis values on a log scale.
vision datasets. However, E¯(A ) ≈ −36% for LSTM and E¯(A ) ≈ −7% for Transformer, suggesting
m m
that QRNG-based initialization may not always yield higher accuracy but can achieve comparable
accuracy faster, reducing training time and saving computational resources.
Moreover, in 60% of the 10 LSTM cases using the Adam optimizer, the result is l:t,l,* (i.e.,
the same accuracy is achieved slower). The comparison of raw data in Appendix J suggests that
LSTM reach the value A more quickly. However, the seed selection penalty (∆ = 4) results in a
m Q
performance loss. Improving the seed selection heuristic could potentially mitigate this issue in the
future (see Section 3.4.3 for more details).
Consequently, all deep neural network architectures studied show benefits in either maximum
accuracy or training speed.
3.3.5 RQ4. How does the change in the dataset affect the performance of deep neural
networks initialized with the QRNG-based weight initialization scheme?
Table 2 indicates that the impact of different datasets varies. For the MNIST dataset, QRNG-based
initializers win in 60% of 40 cases (α¯ ≈ 0.03) without ties. For CIFAR-10, they win in 75% of 40
cases (α¯ ≈ 0.11) without tie. For IMDB, they win in 45% of 40 cases (α¯ ≈ 0.00) and tie in 3%.
Partitioning data by optimizer reveals more details. For MNIST with SGD, models with QRNG-
based initializers win in 90% of 20 cases (α¯ ≈ 0.04), while with Adam, they win in 30% of cases
(α¯ ≈ 0.01).
Our analysis of the accuracy distributions given in the Appendix J suggests that the improvement
oftheaccuracyofPRNGandQRNGwithepochsissimilar. TheaccuracydistributionsinAppendixJ
suggest similar accuracy improvements with epochs for PRNG- and QRNG-based models, but the
QRNG seed selection heuristic incurs a computational penalty ∆ . This issue may be resolved in
Q
10
noitcnuf
noitubirtsid
evitalumuCTable 3: Count of the final outcomes grouped by initializer.
Shapedependent Shapeagnostic Orthogonal Grand
Outcome Glorot Glorot He He Lecun Lecun Random Random Truncated Orthogonal Total
normal uniform normal uniform normal uniform normal uniform normal
l:l,*,* 3 1 2 1 5 6 4 22
l:t,l,* 1 2 4 3 3 3 3 1 5 25
LossTotal 4 3 6 4 3 3 5 9 5 5 47
t:t,t,t 1 1
TieTotal 1 1
w:t,w,wt 2 1 1 1 1 1 1 1 9
w:w,l,* 2 2 2 3 2 2 3 16
w:w,t,wt 1 1
w:w,w,l 1 1 1 1 1 1 1 7
w:w,w,wt 3 5 3 4 5 5 6 1 6 1 39
WinTotal 8 9 6 7 9 9 7 3 7 7 72
GrandTotal 12 12 12 12 12 12 12 12 12 12 120
the future by improving seed selection heuristics (we will discuss this in Section 3.4.3).
For CIFAR-10, the results are more balanced: QRNG-based initializers win in 75% of 20 cases
withSGD(α¯ ≈ 0.11)andin65%of20caseswithAdam(α¯ ≈ 0.10). ComparedtoMNIST,CIFAR-10
isamorechallengingdataset, makingmodellearningmoredifficult. Thus, QRNG-basedinitialization
may help learn more complex data representations.
For IMDB, the results are fairly balanced. Models with QRNG-based initializers win in 45% of
the 20 cases with Adam (α¯ ≈ 0.00) and tie in 5% of the experiments, while with SGD they win in
45% of the cases (α¯ ≈ 0.00).
Based on these data, is QRNG-based initialization more helpful for computer vision datasets than
natural language datasets? The short answer is “not necessarily.” While, on average, we were unable
to improve the accuracy of the LSTM and Transformer models using QRNG-based initialization
(trained on the natural language dataset), we saw that the same accuracy can be achieved faster (as
shown in Section 3.3.4), which is still advantageous.
3.3.6 Hypothesis
The answers to the research questions support the hypothesis that QRNG-based weight initialization
can significantly accelerate neural network training. Improvements are evident across data sets,
architectures, initializers, and optimization algorithms.
3.4 Discussion
3.4.1 Practical considerations
We provide a reference implementation of QRNG-based initializers for Keras v.3.3.3, which can
readily use three different backends: JAX [7], PyTorch, or TensorFlow. It can also be easily ported
to other popular machine learning frameworks. The changes to the initializer’s underlying random
number generator do not require any changes downstream: the code for compiling the model and
training it remains unchanged. By passing a QRNG-based initializer instance to a neural network
layer, practitioners can treat these implementations as “black boxes”. This process conceals the
complexities of the underlying implementation from the practitioner. Those who wish to engage
11more deeply may draw random values with specific seeds that can be manually applied to the model
weights.
In Appendix H, we observe that a simple neural network performs better with a number of
weights following the pattern 4,8,12,.... Thus, when designing neural network layers, it is advisable
to select the number of weights that adhere to this pattern (which is often the default behaviour6).
3.4.2 Limitations
This section discusses limitations and threats to the validity of our study classified as per [70].
Inaccuraciesinourimplementationorinthecodeoftheunderlyingpackagescanaffecttheresults,
threatening internal validity. To mitigate the first threat, our QRNG-based implementations have
undergone code peer review and were tested against Keras/TensorFlow PRNG-based distributions.
ThesecondthreatmayberelatedtoissueswiththeimplementationofPhiloxPRNGinTensorFlow
backend. By training a simple MLP for a single epoch using Philox and the Mersenne Twister
algorithm, we found that the distributions of the results were similar (see Appendix H.1 for details).
Consequently, the observed behaviour is not unique to a particular PRNG or implementation.
To focus on the effect of random generators, we need to isolate other sources that may affect the
model’s performance. We identified five main areas affecting model performance that may threaten
conclusion validity: (a) random selection of training, validation, and testing datasets, (b) stochastic
nature of the optimizer, (c) sophisticated constructs in models’ archtitecture, (d) speed of the
QRNG-based initializers, and (e) limited number of epochs.
To address the first two issues, we ran each experiment 100 times. Although data shuffling (done
after each epoch) may yield different outcomes, we expect the relative performance differences to
remainconsistent. WerantheformalexperimentsonlyonCPUstoeliminatetheeffectofrandomness
from the GPU software stack (although we ran the code informally on GPUs and observed similar
results).
To address the third issue, we simplified model architectures to avoid the effects of more
sophisticated techniques. Although this reduces the overall model efficacy, our primary goal is to
assess the effects of random generators, not optimize performance.
QRNG-based initializers are generally slower than PRNG-based initializers. The computational
complexity is O(cid:0) d2N (cid:1) , where N is the maximum number of elements for any of the d
max max
dimensions; see Appendix B.3 for details and a way to reduce the complexity to O(dN ). However,
max
the initialization time for setting the weights before training is negligible compared to the duration of
modeltraining. Wetimedourinitializationcodetoassesstheeffectofthefourthissue. AppendixB.3
shows that drawing the values from the distributions takes only a fraction of a second, while training
models can take minutes or longer. As a result, QRNG-based initializer can reduce the overall
training time and achieve an accuracy comparable to PRNG with fewer epochs.
Finally, we capped training at 30 epochs in our experiments to ensure consistency across all
setups. Eventually, if we train our models for a larger number of epochs, PRNG-based models may
become more accurate than QRNG-based models, or vice versa (i.e., a different model may reach
a higher value of A than we have seen in our experiments). The data in Appendix J show that
m
this is unlikely for most cases based on the analysis of the raw accuracy plots and the changes in
the median accuracy. However, it is possible, though not likely, that extended training could yield
different results. Thus, our findings should be interpreted within the context of a 30-epoch training
limit.
The limited generalizability of our findings challenges its external validity. Although we examined
three datasets (from two modalities), four model architectures, two optimizers, and ten initializers,
6As 2n is divisible by 4 for n≥2.
12we cannot guarantee that these results will apply to other datasets, architectures, optimizers, or
initializers. However, the promising results warrant further investigation. The same empirical
methodology can be applied to other modeling scenarios through rigorously designed experiments.
The following are some open questions for future research.
3.4.3 Open questions
In our study, we demonstrated that QRNG-based initializers can outperform PRNG-based ones.
Many questions remain unanswered; below, we sketch some potential future research avenues.
WhyisQRNG-basedinitializationeffective? ItispossiblethatinsightsfromMonteCarlomethods
research,whereQRNGinitializationiscommonlyused,canbeappliedhere. Thedimensionalintegers
used to seed QRNG (see Appendix A for details) are optimized pairwise (by minimizing the number
of bad correlations between pairs of variables) [26]. The creators of these dimensional integers suggest
that the success of simple pairwise optimization may be because many practical problems can be
reduced to “low effective dimension” [9], which “means either that the integrand depends mostly
on the initial handful of variables, or that the integrand can be well approximated by a sum of
functions with each depending on only a small number of variables at a time” [26], see [66]–[68] for
further analysis. These relations capture enough information necessary for decision making. If we
apply similar logic to neural networks initialization, initializing the weights uniformly at the level of
individual layers and adding additional uniformity for each pair of layers may give an optimizer a
better initial condition than RPNG-based initialization. In order to test whether these conjectures
remain valid empirically, more complex models could be tested on larger, more intricate datasets
and tasks.
Why Random Uniform initializer benefits the least from QRNG? In Section 3.3.3, we observe
that most initializers benefit from QRNG with the exception of the Random Uniform. This initializer
mutates starting random numbers the least, simply by rescaling them to the [−0.05,0.05] range
(see Appendix C for details). Can this behaviour help us better understand why QRNG-based
initialization yields better results in other cases?
How does the performance of QRNG-based initializers vary with the number of weights? As
mentioned in Section 3.4.1, a simple neural network performs better with a number of weights
following the pattern 4,8,12,.... This raises questions about the underlying factors of this behaviour,
perhaps related to balance and QRNGs’ low-discrepancy characteristics.
When a sequence length is not a power of two, Sobol’ sequences lose balance [48]. Although our
preliminary tests with a simple model (in Appendix H.2) did not show significant practical effects of
this factor, we speculate that these effects may be more pronounced in larger and more complex
models. Therefore, it is an open question whether incorporating this factor into the design of neural
network layers can further improve QRNG initialization.
Sobol’ sequence performance may be degraded [48] if a first element is removed (as in our
implementation). According to our initial analysis, excluding the zero element did not significantly
affectneuralnetworktrainingandinitialization. However,furtherinvestigationisneededtodetermine
whether the QRNG performance can be improved.
The seeds for our QRNG-based initializers are selected sequentially using a heuristic (Ap-
pendix C.2), which is not optimal (as shown in Appendix H.2). An alternative approach to seed
selection7 or refining the heuristic could improve our results.
We have only explored one version of the Sobol’ sequences (with a specific set of direction
numbers). Other versions exist (e.g., scrambled sequences [47] or those with optimization after
7For example, selecting a seed at random for each layer, which would set ∆ =0, and potentially leading to a
Q
reduction of l:t,lt,* and w:w,lt,* outcomes, where lt would be replaced with w.
13sampling [56]) raising the question of which version is most effective for QRNG applications in
initializers. Our initial analysis shows scrambled sequences produce similar results to unscrambled
sequences but introduce numerical instability. However, quantitative assessments need more research.
Otherquasirandomsequencesexist, suchastheFaure, Halton, andNiederreitersequences. Would
those sequences perform better than Sobol’s sequences if we used them in QRNG-based initializers?
QRNG has a greater impact on shape-dependent and orthogonal initializers than on shape-
agnostic ones (as shown in Section 3.3.3). More research is required to determine the reasons for
these discrepancies.
Furthermore, while our initial experiments isolated QRNG’s effects using simple models and
datasets, further research would require scaling up complexity, varying data modalities and volumes,
model sizes, and optimizers to fully grasp QRNG’s implications.
4 Related Work
The following is a brief summary of related literature; all these works are complementary to our own.
Low-discrepancy sequences were tried for various tasks in machine learning, such as selecting a
subsetofdatafortraining[44], generatingneuralnetworks[28]andtheirsurrogates[37], replacingthe
backpropagation algorithm [27], improving optimizers [2], updating belief in Bayesian networks [10],
and tuning hyperparameters and improving regularization [3]. To the best of our knowledge,
researchers have not yet used QRNGs to initialize neural network weights.
Other random number generators have been explored to initialize neural networks. For example,
node dropout regularization was performed using true random number generators [31]. Finally,
machine learning models (including dense neural networks) may perform better when initialized
with random numbers generated by quantum computers [5] (although other researchers were unable
to replicate these results [22]).
5 Summary
We demonstrated that QRNG-based initializers can achieve higher accuracy or speed up neural
network training in 60% of the 120 experiments conducted. The top 25% of the improved maximum
median accuracies range between ≈ 0.0775 and 0.3550. The negative side effects of trying QRNG are
minimal, with the top 25% of observed losses resulting in a drop in accuracy between ≈ 0.0031 and
0.0402. This trend is observed across various data types, architectures, optimizers, and initializers.
The extent to which this effect generalizes remains to be seen. We encourage the community to
explore and validate our findings.
Acknowledgements
This work was partially supported by the Natural Sciences and Engineering Research Council
of Canada (grant # RGPIN-2022-03886). The authors thank the Digital Research Alliance of
Canada and the Department of Computer Science at Toronto Metropolitan University for providing
computational resources. The authors also express their gratitude to the members of our research
group — Montgomery Gole, Mohammad Saiful Islam, and Mohamed Sami Rakha — for their
valuable feedback on the manuscript and insightful discussions.
14References
[1] I.AntonovandV.Saleev,“Aneconomicmethodofcomputinglp -sequences,” USSRComputationalMathematics
τ
andMathematicalPhysics,vol.19,no.1,pp.252–256,1979,issn:0041-5553.doi:10.1016/0041-5553(79)90085-
5.
[2] W. H. Bangyal, K. Nisar, T. R. Soomro, A. A. Ag Ibrahim, G. A. Mallah, N. U. Hassan, and N. U. Rehman,
“An improved particle swarm optimization algorithm for data classification,” Applied Sciences, vol. 13, no. 1,
p. 283, 2022. doi: 10.3390/app13010283.
[3] J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,” J. Mach. Learn. Res., vol. 13,
pp. 281–305, 2012. doi: 10.5555/2503308.2188395.
[4] J. Bilmes, K. Asanovic, C.-W. Chin, and J. Demmel, “Using phipac to speed error back-propagation learning,”
in1997 IEEE International Conference on Acoustics, Speech, and Signal Processing,vol.5,1997,pp.4153–4156.
doi: 10.1109/ICASSP.1997.604861.
[5] J.J.Bird,A.Ekárt,andD.R.Faria,“Ontheeffectsofpseudorandomandquantum-randomnumbergenerators
in soft computing,” Soft computing, vol. 24, no. 12, pp. 9243–9256, 2020. doi: 10.1007/s00500-019-04450-0.
[6] G. E. P. Box and M. E. Muller, “A Note on the Generation of Random Normal Deviates,” The Annals of
Mathematical Statistics, vol. 29, no. 2, pp. 610–611, 1958. doi: 10.1214/aoms/1177706645.
[7] J.Bradburyetal.,JAX:ComposabletransformationsofPython+NumPyprograms,version0.3.13,2018.[Online].
Available: http://github.com/google/jax.
[8] P. Bratley and B. L. Fox, “Algorithm 659: Implementing sobol’s quasirandom sequence generator,” ACM
Transactions on Mathematical Software (TOMS), vol. 14, no. 1, pp. 88–100, 1988. doi: 10.1145/42288.214372.
[9] R. Caflisch, W. Morokoff, and A. Owen, “Valuation of morgage backed securities using brownian bridges to
reduceeffectivedimension,” The Journal of Computational Finance,vol.1,pp.27–46,1997.doi:10.21314/JCF.
1997.005.
[10] J.ChengandM.J.Druzdzel,“Computationalinvestigationoflow-discrepancysequencesinsimulationalgorithms
for bayesian networks,” in UAI ’00: Proceedings of the 16th Conference in Uncertainty in Artificial Intelligence,
Morgan Kaufmann, 2000, pp. 72–81. [Online]. Available: https://dl.acm.org/doi/pdf/10.5555/2073946.
2073956.
[11] F. Chollet et al., Keras, https://keras.io, 2015.
[12] W.J.Conover,M.E.Johnson,andM.M.Johnson,“Acomparativestudyoftestsforhomogeneityofvariances,
with applications to the outer continental shelf bidding data,” Technometrics, vol. 23, no. 4, pp. 351–361, 1981.
doi: 10.1080/00401706.1981.10487680.
[13] L. Devroye, Non-Uniform Random Variate Generation. Springer, 1986. doi: 10.1007/978-1-4613-8643-8.
[14] H.Faure,“Discrépancedesuitesassociéesàunsystèmedenumération(endimensions),” fre,Acta Arithmetica,
vol. 41, no. 4, pp. 337–351, 1982. [Online]. Available: http://eudml.org/doc/205851.
[15] M.A.FlignerandT.J.Killeen,“Distribution-freetwo-sampletestsforscale,” JournaloftheAmericanStatistical
Association, vol. 71, no. 353, pp. 210–213, 1976. doi: 10.1080/01621459.1976.10481517.
[16] K. Fukushima, “Visual feature extraction by a multilayered network of analog threshold elements,” IEEE Trans.
Syst. Sci. Cybern., vol. 5, no. 4, pp. 322–333, 1969. doi: 10.1109/TSSC.1969.300225.
[17] X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural networks,” in
Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS,
ser. JMLR Proceedings, vol. 9, JMLR.org, 2010, pp. 249–256. [Online]. Available: http://proceedings.mlr.
press/v9/glorot10a.html.
[18] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016. [Online]. Available: http:
//www.deeplearningbook.org.
[19] J. H. Halton, “On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional
integrals,” Numerische Mathematik, vol. 2, pp. 84–90, 1960. doi: 10.1007/BF01386213.
[20] B. Hanin and D. Rolnick, “How to start training: The effect of initialization and architecture,” in Advances
in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS, 2018, pp. 569–579. [Online]. Available: https://proceedings.neurips.cc/paper/2018/hash/
d81f9c1be2e08964bf9f24b15f0e4900-Abstract.html.
15[21] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification,” in 2015 IEEE International Conference on Computer Vision, ICCV, IEEE Computer
Society, 2015, pp. 1026–1034. doi: 10.1109/ICCV.2015.123.
[22] R. Heese, M. Wolter, S. Mücke, L. Franken, and N. Piatkowski, “On the effects of biased quantum random
numbers on the initialization of artificial neural networks,” Mach. Learn., vol. 113, no. 3, pp. 1189–1217, 2024.
doi: 10.1007/S10994-023-06490-Y.
[23] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780,
1997. doi: 10.1162/NECO.1997.9.8.1735.
[24] J. J. Hopfield, “Neural networks and physical systems with emergent collective computational abilities.,”
Proceedings of the national academy of sciences, vol. 79, no. 8, pp. 2554–2558, 1982. doi: 10.1073/pnas.79.8.
2554.
[25] P. Jäckel, Monte Carlo methods in finance. John Wiley & Sons, 2002.
[26] S. Joe and F. Y. Kuo, “Constructing sobol sequences with better two-dimensional projections,” SIAM Journal
on Scientific Computing, vol. 30, no. 5, pp. 2635–2654, 2008. doi: 10.1137/070709359.
[27] I. Jordanov and R. Brown, “Neural network learning using low-discrepancy sequence,” in Advanced Topics in
Artificial Intelligence, 12th Australian Joint Conference on Artificial Intelligence, AI ’99, Sydney, Australia,
December 6-10, 1999, Proceedings, N. Y. Foo, Ed., ser. Lecture Notes in Computer Science, vol. 1747, Springer,
1999, pp. 255–267. doi: 10.1007/3-540-46695-9\_22.
[28] A. Keller and M. V. Keirsbilck, “Artificial neural networks generated by low discrepancy sequences,” in Monte
Carlo and Quasi-Monte Carlo Methods - MCQMC, ser. Springer Proceedings in Mathematics and Statistics,
vol. 387, Springer, 2020, pp. 291–311. doi: 10.1007/978-3-030-98319-2\_15.
[29] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in 3rd International Conference on
Learning Representations, ICLR 2015, 2015. [Online]. Available: http://arxiv.org/abs/1412.6980.
[30] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-normalizing neural networks,” in Advances
in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Sys-
tems, 2017, pp. 971–980. [Online]. Available: https://proceedings.neurips.cc/paper/2017/hash/
5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html.
[31] A.Koivu,J.-P.Kakko,S.Mäntyniemi,andM.Sairanen,“Qualityofrandomnessandnodedropoutregularization
for fitting neural networks,” Expert Systems with Applications, vol. 207, p. 117938, 2022, issn: 0957-4174. doi:
10.1016/j.eswa.2022.117938.
[32] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” Tech. Rep., 2009.
[Online]. Available: https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf.
[33] S.S.KucherenkoandY.Sytsko,“Applicationofdeterministiclow-discrepancysequencesinglobaloptimization,”
Comput. Optim. Appl., vol. 30, no. 3, pp. 297–318, 2005. doi: 10.1007/S10589-005-4615-1.
[34] “Layer weight initializers — Keras v3 manual.” (2024), [Online]. Available: https://keras.io/api/layers/
initializers/.
[35] “Layer weight initializers — Orthogonal — Keras v3 manual.” (2024), [Online]. Available: https://keras.io/
api/layers/initializers/#orthogonalinitializer-class.
[36] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,”
Proc. IEEE, vol. 86, no. 11, pp. 2278–2324, 1998. doi: 10.1109/5.726791.
[37] M. Longo, S. Mishra, T. K. Rusch, and C. Schwab, “Higher-order quasi-monte carlo training of deep neural
networks,” SIAM J. Sci. Comput., vol. 43, no. 6, A3938–A3966, 2021. doi: 10.1137/20M1369373.
[38] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, “Learning word vectors for sentiment
analysis,” in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies, Portland, Oregon, USA: Association for Computational Linguistics, Jun. 2011, pp. 142–
150. [Online]. Available: http://www.aclweb.org/anthology/P11-1015.
[39] H. B. Mann and D. R. Whitney, “On a Test of Whether one of Two Random Variables is Stochastically Larger
than the Other,” The Annals of Mathematical Statistics, vol. 18, no. 1, pp. 50–60, 1947. doi: 10.1214/aoms/
1177730491.
[40] Martín Abadi et al., TensorFlow: Large-scale machine learning on heterogeneous systems, Software available
from tensorflow.org, 2015. [Online]. Available: https://www.tensorflow.org/.
16[41] M. Matsumoto and T. Nishimura, “Mersenne twister: A 623-dimensionally equidistributed uniform pseudo-
random number generator,” ACM Trans. Model. Comput. Simul., vol. 8, no. 1, pp. 3–30, Jan. 1998. doi:
10.1145/272991.272995.
[42] “Mersenne twister (mt19937) — numpy v1.24 manual.” (2023), [Online]. Available: https://numpy.org/doc/1.
24/reference/random/bit_generators/mt19937.html.
[43] A. Miranskyy, A. Sorrenti, and V. Thakar. “Source code for this paper.” (2024), [Online]. Available: https:
//github.com/miranska/qrng-init.
[44] S. Mishra and T. K. Rusch, “Enhancing accuracy of deep learning algorithms by training with low-discrepancy
sequences,” SIAM J. Numer. Anal., vol. 59, no. 3, pp. 1811–1834, 2021. doi: 10.1137/20M1344883.
[45] A. Nandan. “Text classification with transformer — keras manual.” (2020), [Online]. Available: https://keras.
io/examples/nlp/text_classification_with_transformer/.
[46] H. Niederreiter, “Low-discrepancy and low-dispersion sequences,” Journal of number theory, vol. 30, no. 1,
pp. 51–70, 1988. doi: 10.1016/0022-314X(88)90025-X.
[47] A. B. Owen, “Scrambling sobol’ and niederreiter-xing points,” J. Complex., vol. 14, no. 4, pp. 466–489, 1998.
doi: 10.1006/JCOM.1998.0487.
[48] A. B. Owen, “On dropping the first sobol’ point,” in Monte Carlo and Quasi-Monte Carlo Methods, A. Keller,
Ed., Cham: Springer International Publishing, 2022, pp. 71–86. doi: 10.1007/978-3-030-98319-2_4.
[49] S. J. Prince, Understanding Deep Learning. MIT Press, 2023. [Online]. Available: http://udlbook.com.
[50] RCoreTeam,R:Alanguageandenvironmentforstatisticalcomputing,RFoundationforStatisticalComputing,
Vienna, Austria, 2024. [Online]. Available: https://www.R-project.org/.
[51] S.Ramasinghe,L.E.MacDonald,M.R.Farazi,H.Saratchandran,andS.Lucey,“Howmuchdoesinitialization
affect generalization?” In International Conference on Machine Learning, ICML, ser. Proceedings of Machine
Learning Research, vol. 202, PMLR, 2023, pp. 28637–28655. [Online]. Available: https://proceedings.mlr.
press/v202/ramasinghe23a.html.
[52] “Random number generation — tensorflow core.” (2024), [Online]. Available: https://www.tensorflow.org/
guide/random_numbers#algorithms.
[53] F. Rosenblatt, “The perceptron: A probabilistic model for information storage and organization in the brain.,”
Psychological review, vol. 65, no. 6, pp. 386–408, 1958. doi: 10.1037/h0042519.
[54] J. K. Salmon, M. A. Moraes, R. O. Dror, and D. E. Shaw, “Parallel random numbers: As easy as 1, 2, 3,” in
Conference on High Performance Computing Networking, Storage and Analysis, S. A. Lathrop, J. Costa, and
W. Kramer, Eds., ACM, 2011, 16:1–16:12. doi: 10.1145/2063384.2063405.
[55] A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks,” in 2nd International Conference on Learning Representations, ICLR, 2014. [Online].
Available: http://arxiv.org/abs/1312.6120.
[56] “Scipy.stats.qmc.sobol — scipy v1.11 manual.” (2023), [Online]. Available: https://docs.scipy.org/doc/
scipy/reference/generated/scipy.stats.qmc.Sobol.html.
[57] M.Skorski,A.Temperoni,andM.Theobald,“Revisitingweightinitializationofdeepneuralnetworks,” inAsian
Conference on Machine Learning, ACML, ser. Proceedings of Machine Learning Research, vol. 157, PMLR,
2021, pp. 1192–1207. [Online]. Available: https://proceedings.mlr.press/v157/skorski21a.html.
[58] I. M. Sobol’, “On the distribution of points in a cube and the approximate evaluation of integrals,” USSR
Computational Mathematics and Mathematical Physics, vol. 7, no. 4, pp. 86–112, 1967, issn: 0041-5553. doi:
https://doi.org/10.1016/0041-5553(67)90144-9.
[59] “Tf.math.sobol_sample — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.
org/api_docs/python/tf/math/sobol_sample.
[60] “Tf.random.algorithm — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.org/
api_docs/python/tf/random/Algorithm.
[61] “Tf.random.normal — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.org/
api_docs/python/tf/random/normal.
[62] “Tf.random.truncated_normal—tensorflowv2.16manual.”(2024),[Online].Available:https://www.tensorflow.
org/api_docs/python/tf/random/truncated_normal.
17[63] “Tf.random.uniform — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.org/
api_docs/python/tf/random/uniform.
[64] A. Vaswani et al., “Attention is all you need,” in Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, 2017, pp. 5998–6008. [Online]. Available:
https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
[65] P. Virtanen et al., “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python,” Nature Methods,
vol. 17, pp. 261–272, 2020. doi: 10.1038/s41592-019-0686-2.
[66] X. Wang and K.-T. Fang, “The effective dimension and quasi-monte carlo integration,” Journal of Complexity,
vol. 19, no. 2, pp. 101–124, 2003, issn: 0885-064X. doi: 10.1016/S0885-064X(03)00003-7.
[67] X. Wang and I. H. Sloan, “Low discrepancy sequences in high dimensions: How well are their projections
distributed?” Journal of Computational and Applied Mathematics, vol. 213, no. 2, pp. 366–386, 2008. doi:
10.1016/j.cam.2007.01.005.
[68] X. Wang and I. H. Sloan, “Why are high-dimensional finance problems often of low effective dimension?” SIAM
Journal on Scientific Computing, vol. 27, no. 1, pp. 159–183, 2005. doi: 10.1137/S1064827503429429.
[69] M. Watson, C. Qian, J. Bischof, F. Chollet, et al., KerasNLP, 2022. [Online]. Available: https://github.com/
keras-team/keras-nlp.
[70] R. K. Yin, Case Study Research: Design and Methods (Applied Social Research Methods). SAGE Publications,
2009, isbn: 9781412960991.
[71] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding deep learning requires rethinking
generalization,” in 5th International Conference on Learning Representations, ICLR, OpenReview.net, 2017.
[Online]. Available: https://openreview.net/forum?id=Sy8gdB9xx.
18A Low Discrepancy Sequences
Sobol’ sequences, introduced by Sobol’ in 1967 [58], are quasirandom sequences widely used in
numerical analysis and Monte Carlo methods [25]. They provide a deterministic and evenly dis-
tributed set of points in multidimensional space, thereby overcoming some limitations of traditional
pseudorandom sequences [25], [58].
Sobol’ sequences can be constructed efficiently [1] using the following recursive formula:
x = x ⊕ v , (7)
n,k n−1,k 2 j,k
where x is the n-th draw of Sobol’ integer in dimension k, v is a direction integer (needed to
n,k j,k
initialize the recursion), ⊕ is a single XOR operation for each dimension. The recursion starts with
2
x = 0.
0,k
Various methods exist to compute v . In our work, we use Sobol’ sequences [58] implemented8
j,k
in SciPy v.1.13.1 software [56], [65], which is configured to produce Sobol’ sequences for up to
21200 dimensions using direction integers v computed by [26]. As we use individual dimensions to
j,k
initialize the networks’ layers, this number of dimensions is more than adequate. A user who needs
more than 21200 dimensions may, e.g., reuse the dimensions or try scrambled versions [47] of Sobol’s
sequences.
In general, Sobol’ sequences have d dimensions and are in the range [0,1)d. We discard the first
elementofasequence(whichisalways0),similartotheTensorFlowv.2.16.1implementation[40],[59].
Due to this, our implementation of Sobol’ sequences reduces the range to (0,1)d. It is necessary to
discard this element in order to implement QRNG-based normal and truncated normal distributions,
which we will discuss in Appendices B.2.3 and B.2.4, respectively.
B Sampling from distributions
In this work, we will need to sample values from univariate uniform, normal, and truncated normal
distributions, denoted by U , N , and T , respectively. The subscript (·) indicates the random
(·) (·) (·)
number generator to use for sampling and takes the value P for PRNG and Q for QRNG.
Figure 3 shows two-dimensional projections for PRNGs and QRNGs under study. Note that
QRNG aims to cover a domain as homogeneously and uniformly as possible.
The distributions obtained using PRNG and QRNG are compared in Figure 4. The QRNG
version is “smoother” than the PRNG version. This is because QRNGs (that use Sobol’ sequences
discussed in Appendix A) have better uniformity properties than PRNGs. Below are details on how
distributions are implemented.
B.1 Using Keras/TensorFlow PRNG
Kerasv.3.3.3[11]canusethreedifferentbackends: JAX,PyTorch,or‘TensorFlow. WeuseTensorFlow
v.2.16.1 [40]. This version of Keras is designed to use backend implementations of the distributions.
TensorFlowv.2.16.1usescontinuousuniform, normal, ortruncatednormaldistributionssampledwith
PRNGs. These implementations are widely used and well tested. Thus, we treat them as baselines
and leverage as-is. Below are some notes on TensorFlow’s implementation of the distributions.
8The implementation seems to be based on the Algorithm 659 [8].
19PRNG - Seed 1 vs. 2 PRNG - Seed 14 vs. 15 PRNG - Seed 10001 vs. 10002
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Seed = 1 Seed = 14 Seed = 10001
QRNG - Seed 1 vs. 2 QRNG - Seed 14 vs. 15 QRNG - Seed 10001 vs. 10002
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Seed = 1 Seed = 14 Seed = 10001
Figure3: Atwo-dimensionalprojectionofthefirst1024drawsoftheKeras/TensorFlowpseudorandom
number generator (top pane) and the Sobol’ sequences (bottom pane). Axis labels denote seed values
of the random generator.
B.1.1 Uniform distribution
TensorFlow draws the values from U in the range [0,1) [63]. The PRNG algorithm used by
P
TensorFlow depends on the hardware [60]. In most cases, it will be either Philox or ThreeFry [54],
but it can also select another algorithm from the underlying libraries [60]. We may not know which
specific algorithm is used in advance (although it is typically Philox [52]). However, the selection of
the PRNG algorithm is deterministic (i.e., a testbed should select PRNG consistently).
We empirically validated that on our testbed the sequences produced by automatic selection
matched those produced by the Philox algorithm. Thus, our experiments initialize neural network
weights using Philox-based PRNGs. Tensorflow may use different PRNG algorithms for other sources
of randomness, such as those used by the SGD and Adam optimizers. Nevertheless, they will be
consistent for a given testbed.
B.1.2 Normal distribution
TensorFlow draws the values from N in the range (−∞,+∞) [61]. The mean µ ∈ R and standard
P
deviation σ ∈ R (governing the distribution) vary based on application. TensorFlow’s manual and
>0
associated Python code listing do not describe the technical details of sampling from the distribution.
However, code analysis indicates that the Box-Muller transform [6] is used.
20
2
=
deeS
2
=
deeS
51
=
deeS
51
=
deeS
20001
=
deeS
20001
=
deeSdistribution_type = Uniform distribution_type = Normal distribution_type = Truncated Normal
500
400
300
200
100
0
0.0 0.2 0.4 0.6 0.8 1.0 −4 −2 0 2 4 −2 −1 0 1 2
values values values
generator
PRNG QRNG
Figure 4: Sample draws from univariate uniform, normal, and truncated normal distributions.
B.1.3 Truncated normal distribution
TensorFlow implements a specific case of a T distribution: the values are drawing from N in
P P
the range [−2σ,+2σ] [62]. Technically, the values are drawn from N with a specified µ and σ,
P
discarding and re-drawing samples with more than two standard deviations from the mean [62].
B.2 Using QRNG
We use the inverse transform sampling approach9, followed by scaling and shifting. Let us look at
the details.
B.2.1 Implementation notes
Our QRNG-based distributions are tested against the baseline Keras v.3.3.3 and TensorFlow v.2.16.1
implementations distributions (described in Appendix B.1) to assure correctness of implementation.
Files src/distributions_qr.py and tests/test_distributions_qr.py contain the code and
associated test cases, respectively [43]. Essentially, we modify Keras [11] classes that are used to
sample random numbers.
Appendix A describes how we implement d-dimensional Sobol’ sequences in the range of (0,1)d.
To maintain low discrepancy properties of Sobol’ sequences, we should sample from the beginning of
the sequence for a given dimension k. Informally, we can refer to k as the seed for our QRNG.
9Inverse transformation sampling begins with uniform samples in the range between 0 and 1. These samples can
be viewed as probabilities, which can be fed into an inverse cumulative distribution function (of a distribution of
interest). Details of the method can be found in [13, Section 2.2].
21
tnuoCB.2.2 Uniform distribution
Letussampleavectorofnrandomnumbersufromauniformdistribution(basedonSobol’sequences).
It can be achieved by scaling and shifting a Sobol’ sequence of length n for k-th dimension (which
gives us an inverse of the cumulative distribution function for the uniform distribution):
u ∼ U (a,b;k) = (b−a)s +a, (8)
Q k
with U (a,k) denoting a uniform distribution with the range (a,b) and s denoting a Sobol’ sequence
Q k
of length n for dimension k.
We can now use U (0,1,k) to get samples for normal and truncated normal distributions via the
Q
inverse transform sampling approach, as discussed below.
B.2.3 Normal distribution
To sample n random numbers n from a normal distribution that uses Sobol’-sequences based QRNG,
we use:
n ∼ N (µ,σ;k) = Φ−1[U (0,1;k)]σ+µ = Φ−1(s )σ+µ, (9)
Q Q k
where N (µ,σ;k) denotes a normal distribution with the mean µ and standard deviation σ and Φ−1
Q
is the inverse cumulative distribution function of the standard normal distribution10. We use an
implementation of Φ−1 provided by SciPy v.1.13.1.
As U (0,1;k) returns values in the range (0,1), theoretically, the output of Φ−1 should always
Q
be finite. Potentially, when working with floating points numbers, lim Φ−1(x) = −∞ and
x→0
lim Φ−1(x) = ∞. We have not experienced this issue in practice since we skip the first element
x→1
of the Sobol’ sequence. However, this issue will manifest itself if the first element is not skipped
(which can be mitigated using scrambled Sobol’ sequences [47]).
B.2.4 Truncated normal distribution
To sample n random numbers t from a truncated normal distribution that uses Sobol’-sequences
based QRNG, so that we match the [−2σ,+2σ] range discussed in Appendix B.1.3 we do
t ∼ T (µ,σ;k) = Φ−1{Φ(α)+U (0,1;k)[Φ(β)−Φ(α)]}σ+µ
Q Q
= Φ−1{Φ(α)+s [Φ(β)−Φ(α)]}σ+µ
k
(10)
(cid:124) (cid:123)(cid:122) (cid:125)
γ
≈ Φ−1{0.02+0.95s }σ+µ,
k
where Φ is the cumulative distribution function of the standard normal distribution, α = −2, and
β = 2. We use an implementation of Φ and Φ−1 provided by SciPy v.1.13.1. The values of α and β
are chosen to satisfy the range [−2σ,+2σ] requirement.
Equation 10 is slow (since Φ and Φ−1 are expensive to compute) but numerically stable. The
stability analysis is detailed below.
As above, lim Φ−1(γ) = −∞. Thus, Equation 10 may be problematic when an element in s ,
γ→0 k
denoted s, yields γ ≈ 0:
γ = Φ(α)+s[Φ(β)−Φ(α)] = 0 ⇒
Φ(α)
s = ≈ −0.02.
Φ(α)−Φ(β)
Therefore, Φ−1(γ) ̸= −∞ for all s, as s > 0. Similarly, Φ−1(γ) ̸= ∞ because lim γ ≈ 0.98.
s→1
10A standard normal distribution has a mean of zero and a standard deviation of one.
22B.3 Performance analysis
Suppose that we want to create d-dimensional sequence of random values with N values for each
k
dimension. Further, suppose that the values for the k-th dimension should be drawn independently
of the previous dimensions (which is typical for our use case, since we usually want to draw the
sequence for a particular value of k).
PRNG generators implement all three distributions (random uniform, random normal, and
truncated normal) independently of their seed values.
QRNGs based on Sobol’ sequences are different. Because Equation 7 is recursive, we must draw
N values for dimensions 1 through k and then discard the values for dimensions 1 to k−1. In this
k
scenario, the total computation cost would be:
d d
(cid:88) kN ≥ (cid:88) kN = d(d+1) N = O(cid:0) d2N (cid:1) , (11)
k max max max
2
k=1 k=1
where N is the maximum number of elements for any of the d dimensions11.
max
Figure 5 presents the amount of time needed to draw the N values from the random uniform
max
U (0,1;k),randomnormalN (0,1;k),andtruncatednormalT (0,1;k)distributions. Asexpected,
(·) (·) (·)
in the case of PRNG the amount of time required to generate the random numbers is independent
of the seed (the lines stay flat). The amount of time required to generate values from the random
uniform distribution is the fastest; the time to generate draws from random normal and truncated
normal distributions is similar but slower than for the random uniform distribution, since they use a
version of the Box-Muller transform [6].
random-normal random-uniform truncated-normal
1e-02
1e-03
1e-04
1 10 100 1000 10000 1 10 100 1000 10000 1 10 100 1000 10000
Seed k
Nmax 10 100 1000 10000 PRNG QRNG
Figure 5: Time needed to draw the N = 10,100,1000,10000 values from the random normal
max
N (0,1;k), random uniform U (0,1;k), and truncated normal T (0,1;k) distributions. x-axis
(·) (·) (·)
shows seed value k, y-axis shows execution time measured in seconds. The lines represent median
accuracy based on 1000 repetitions, while the ribbons represent the range between lower and upper
quartiles.
QRNG requires more time to draw from random uniform, random normal, and truncated normal
distributions than their PRNG counterparts. Within the QRNG group, random uniform is the
fastest, followed by random normal, and truncated normal. This is expected based on the number of
11Algorithm 659 [8] explicitly adds the cost of XOR bit manipulation O[log(d)]. The computational cost of this
calculation is negligible on modern computers, so we eliminate this term.
23
)s(
emiT
noitucexEoperations in Equations 8, 9, and 10. For all three distributions, execution time increases with k (as
expected based on Equation 11).
Asmentionedabove,ourQRNG-basedformulasareslowerthanKeras/TensorFlow’sPRNG-based
formulas. However, the time spent initializing the weights before training is immaterial compared to
the time spent training the model. As shown in Figure 5, drawing the values from the distributions
for a specific k takes only a fraction of a second (even for N = 10000). Comparatively, training a
max
model takes seconds to minutes. Thus, we can still save time training the model (since fewer epochs
are required to achieve a certain level of accuracy).
CachingSobol’sequencescanimproveimplementationperformanceifweneedtoinitializeweights
quickly (e.g., because we use very large k values or because every second matters). The sequences
can be cached in a N ×d table if we know in advance N and d. In this case, the computations
max max
will cost only
d
(cid:88)
N = dN = O(dN ).
max max max
k=1
Using this method, the order of computational complexity of QRNG would be reduced to that of
PRNG.
B.4 Using Mersenne Twister PRNG
We use the inverse transform sampling approach discussed in Appendix B.2 to compute random
uniform, random normal, and truncated normal distributions based on the Mersenne Twister PRNG.
There is only one difference: to replace s with m in Equations 8, 9, and 10, where m denotes a
k k k
vector of draws from the Mersenne Twister PRNG with seed k.
C Weight Initializers
C.1 Parameters and Implementations
Keras v.3.3.3 implements ten popular kernel initializers that use random number generators [34].
Table 4 list the initializers under study along with the default parameters (which we use in our
study).
Glorot, He and Lecun (Uniform and Normal) adapt the shape of distribution by selecting
parameters based on the number of input units, represented as N in the weight tensor, and/or the
in
number of output units, represented as N , in the weight tensor.
out
The creation of the Orthogonal initializer is more involved. The following is a quote from the
Keras manual [35].
“If the shape of the tensor to initialize is two-dimensional, it is initialized with an
orthogonal matrix obtained from the QR decomposition of a matrix of random numbers
drawn from a normal distribution. If the matrix has fewer rows than columns then the
output will have orthogonal rows. Otherwise, the output will have orthogonal columns.
If the shape of the tensor to initialize is more than two-dimensional, a matrix of
shape (shape[0] * ... * shape[n - 2], shape[n - 1]) is initialized, where n is
the length of the shape vector. The matrix is subsequently reshaped to give a tensor of
the desired shape.”
A multiplicative factor denoted by g is applied to the orthogonal matrix.
24We use Keras/TensorFlow implementations of the algorithms as-is, but we modify the code
to use QRNG rather than PRNG to sample the values from the respective distributions. Files
src/custom_initializers.py and tests/test_custom_initializers.py contain the code and
associated test cases, respectively [43].
Table 4: Parameters of the initializers under study.
Initializer Distribution Parameters
(cid:113) (cid:113)
Glorot uniform U (a,b;k) a = − 6 , b = 6
(·) Nin+Nout Nin+Nout
(cid:113)
Glorot normal T (µ,σ;k) µ = 0, σ = 2
(·) Nin+Nout
(cid:113) (cid:113)
He uniform U (a,b;k) a = − 6 , b = 6
(·) Nin Nin
(cid:113)
He normal T (µ,σ;k) µ = 0, σ = 2
(·) Nin
(cid:113) (cid:113)
Lecun uniform U (a,b;k) a = − 3 , b = 3
(·) Nin Nin
(cid:113)
Lecun normal T (µ,σ;k) µ = 0, σ = 1
(·) Nin
Orthogonal N (µ,σ;k) µ = 0, σ = 1, g = 1
(·)
Random normal N (µ,σ;k) µ = 0,σ = 0.05
(·)
Random uniform U (a,b;k) a = −0.05,b = 0.05
(·)
Truncated normal T (µ,σ;k) µ = 0,σ = 0.05
(·)
C.2 Auto-selection of seed
Inourexperiments,theseedkisautomaticallyselectedusingthefunction get_starting_dim_id_auto
inthefilesrc/train_and_eval.py. Thefilessrc/global_config.pyandtests/test_global_config.py
contain the code and associated test cases, respectively [43].
C.2.1 PRNG seed selection
Forallmodels, PRNG-basedinitializerschoosek arbitraryforeachlayerofthemodelandexperiment
(which is a default Keras/TensorFlow behaviour). The seed selection for QRNG-based initializers is
described below.
C.2.2 QRNG seed selection
In QRNG-based initializers, k is chosen sequentially for every layer. As discussed in Appendix B.2.1,
the seed k is mapped to the k-th dimension of the Sobol’ sequences. We have observed that the
25sequence’s starting value of k may significantly affect the speed with which the optimizer reaches
high accuracy values.
No universal pattern could be applied to all the models under study. However, we have seen that
models that reach high levels of accuracy in early epochs usually retain this competitive advantage
into the future. In addition, the number of seeds that lead to sub-par results is small.
As a result, we introduce a heuristic for selecting a seed for model training, as shown in
Algorithm 1. In essence, we randomly select X seeds from the range [W,Z] and train the model for
Y epochs, repeating12 the training R times. In order to match the range of dimensions used in the
implementation of Sobol’ sequences under study, the minimum value for W,X, and Z is 1 and the
maximum value is 21200 (see Appendix A for details).
Thisseedselectionprocesscanbecomeexpensive,sincewemusttrainthemodelsforanadditional
∆ = XYR−Y = Y(XR−1) epoch. The term −Y refers to the fact that we can continue to
Q
train the best model, saving Y epochs. Due to possible defects (associated with managing global
states) in the underlying libraries, we do not do this in our code to minimize the risk of giving the
QRNG-based model an unfair advantage. It is certainly reasonable to do this in a practical setting.
Our paper uses accuracy to measure performance, so we seek seeds that maximize accuracy. The
following values yield adequate results for our use cases: R = 1,X = 5,Y = 1,W = 1, and Z = 10.
Based on these parameters, QRNG-based initializers have a penalty of ∆ = Y(XR−1) = 4 epochs.
Q
Readers may adjust these values according to their use cases through empirical evaluation.
Here are some examples of values of k that we use in our models.
MLP and CNN MLP and CNN models select a new value of k for every layer. In QRNG-based
initializers, k is chosen sequentially for every layer starting at ν (the starting index is provided by
the Algorithm 1). As discussed in Appendix B.2.1, the seed k is mapped to k-th dimension of the
Sobol’ sequences.
For example, our MLP under study (described in Appendix D.1) has three dense layers that
require weight initalization. For the first layer, we will seed the initializer with k = ν, the second
layer — with k = ν +1, and the third layer — with k = ν +2. The weights are assigned to specific
units of the layer without reshaping.
Our CNN under study (described in Appendix D.2) has three convolutional and one dense layer.
Three convolutional layers will be initialized with k = ν,ν +1,ν +2, respectively; dense layer will
be initialized with k = ν +3. The tensor of the obtained weights will be reshaped to match the
dimensionality of a layer (in terms of channels and kernels). Keras/TensorFlow standard code
handles this reshaping for us.
LSTM and Transfomers LSTM and Transformer models consist of multiple submodules, each
submodule is initialized with an incrementing value of k. In LSTM models, the four gates (input,
forget, cell, and output) are treated as a single layer, and we experiment with different initializers
while seeding QRNG-based methods with k = ν; the same initializer is used for the dense layer (used
for classification) with k = ν +1. As described in Appendix D.3, the recurrent states in the LSTM
are initialized using the QRNG-based orthogonal initializer with k = ν.
In Transformer models, the multi-head attention layer consists of four fully connected layers
representing queries, keys, and values (QKV), followed by an output layer. The same initializer type
is used for all layers. The QKV multi-head attention and associated output weights are initialized
withk = ν,ν+1,ν+2,ν+3, respectively. Twoadditionalfullyconnectedlayersfollowthemulti-head
12Because optimization is stochastic, different instances of the optimizer may converge to different solutions when
starting from the same weights.
26Algorithm 1: A heuristic to automatically select a starting seed value for a given model.
Input :W ∈ {1,2,...,21200} ; /* The minimum value of the seed to try */
Input :Z ∈ {1,2,...,21200} ; /* The maximum value of the seed to try */
Input :X ∈ {1,2,...,21200} ; /* The number of seeds to try */
Input :Y ∈ Z ; /* The number of epochs to train the model */
≥1
Input :R ∈ Z ; /* The number of times to train a model with a particular
≥1
seed */
Input :model_cfg ; /* Model configuration */
Input :train_data ; /* Dataset to train the model */
Input :test_data ; /* Dataset to test the model */
Output:Suggested seed / dimension id denoted by ν
1 ν ← ∅ ;
2 best_metric_value ← ∅ ;
3 seeds ← Sample without replacement X integers in the range [W,Z] ;
4 seeds ← Sort seeds ; /* Increase reproducibility when metric values are tied */
5 foreach seed in seeds do
6 for 1 to R do
7 Set starting seed value to seed ;
8 model ← Initialize the model using model_cfg ; /* The seed value is assigned
sequentially beginning at the starting seed value, i.e., seed */
9 trained_model ← Train the model for Y epochs on train_data ;
10 current_metric_value ← Evaluate trained_model on test_data and compute model’s
performance on test_data ;
/* Assume that we want to maximize the metric */
11 if best_metric_value < current_metric_value or best_metric_value = ∅ then
12 best_metric_value ← current_metric_value ;
13 ν ← seed ; /* New best seed */
14 return ν ;
attention layer and are initialized with k = ν +4,ν +5, respectively. The dense layer used for
classification is initialized with k = ν +6.
D Models
The architecture of the models is shown below. As discussed in Section 3.1, we keep the models
simple to isolate the effect of random number generators on model performance.
D.1 MLP
D.1.1 MLP for the performance experiments in Appendix H
Figure 6 shows an MLP architecture designed for image processing (input images are flattened). It
has one fully connected layer. For a given experiment, the number of neurons in the layer varies
from 1 to 70. ReLU (Rectified Linear Unit) activation functions [16] are used after the layer. We
experiment with different weight initialization techniques for kernels, and biases are initialized to
27zero. Details of the experiments are given in Appendix H. The source code for the model can be
found in src/model/baseline_ann_one_layer.py [43].
Thefinaloutputlayerconsistsofadenselayerwithasoftmaxactivationfunctionandtenneurons
(one per class). The kernel initializer is Glorot Uniform (the default for Keras v .3.3.3), and the
biases are zero.
Layer Type Input Layer Dense Dense
MNIST MNIST: MNIST: MNIST:
Output Shape (None, 784) (None, u) (None, 10)
Figure 6: MLP architecture (used in the experiments in Appendix H). The number of units (neurons)
u varies from 1 to 70. The leftmost node represents a legend.
D.1.2 MLP for core experiments
Figure 7 shows an MLP architecture designed for image processing (input images are flattened).
It has two fully connected layers consisting of 32 neurons each. ReLU (Rectified Linear Unit)
activation functions [16] are employed after each layer. For kernels, we experiment with different
weight initialization techniques (listed in Section 2), and biases are initialized to zero. The source
code for the model can be found in src/model/baseline_ann.py [43].
Thefinaloutputlayerconsistsofadenselayerwithasoftmaxactivationfunctionandtenneurons
(one per class). In each experiment, the kernel initializer is the same as the fully connected layers
initializer, and biases are initialized to zero.
Layer Type InputLayer Dense Dense Dense
Dataset MNIST: CIFAR: MNIST: CIFAR: MNIST: CIFAR: MNIST: CIFAR:
Output Shape (None, 784) (None, 3072) (None, 32) (None, 32) (None, 32) (None, 32) (None, 10) (None, 10)
Figure 7: MLP architecture used in the core experiments. The leftmost node represents a legend.
D.2 CNN
Figure 8 pictures a CNN architecture designed for image processing. It consists of three convolutional
layers, each with 32, 64, and 64 filters. The feature maps are downsampled using Max pooling layers,
and ReLU activations are applied after each convolution layer. Similarly to MLP architectures,
kernel initializers (shwon in Section 2) differ based on experiment, while biases are initialized to zero.
The source code for the model can be found in src/model/baseline_cnn.py [43].
The final output layer consists of a dense layer with a softmax activation function and ten
neurons (one per class). In each experiment, the kernel initializer is the same as the convolutional
layer initializer, and biases are initialized to zero.
D.3 LSTM
Figure9depictsanLSTMarchitecture(atypeofrecurrentneuralnetwork)designedforclassification.
LSTM cells maintain four gates that constrain recurrent states. A gate consists of 8 neurons whose
weights are initialized using an initializer (listed in Section 2) specific to each experiment. Biases are
28Layer Type Input Layer Convolution 2D Max Pooling 2D Convolution 2D
MNIST CIFAR MNIST: CIFAR: MNIST: CIFAR: MNIST: CIFAR: MNIST: CIFAR:
Output ShapeOutput Shape (None, 28, 28, 1)(None, 32, 32, 3) (None, 26, 26, 32)(None, 30, 30, 32) (None, 13, 13, 32)(None, 15, 15, 32) (None, 11, 11, 64)(None, 13, 13, 64)
Dense Flatten Convolution 2D Max Pooling 2D
MNIST: CIFAR: MNIST: CIFAR: MNIST: CIFAR: MNIST: CIFAR:
(None, 10)(None, 10) (None, 576)(None, 1024) (None, 3, 3, 64)(None, 4, 4, 64) (None, 5, 5, 64)(None, 6, 6, 64)
Figure 8: CNN architecture used in the core experiments. Top-left node represents a legend.
initialized to zero. A recurrent weights matrix is populated using an orthogonal initializer which is
initialized with the help of QRNG for QRNG experiments and with PRNG for PRNG experiments.
Sigmoid activation function is use for the recurrent step.
The input data are passed through a 32-dimensional embedding layer. Default Keras v .3.3.3
initialization scheme (PRNG-based) is used to initialize the layer.
The final output layer consists of a dense layer with a softmax activation function and two
neurons (one per class). In each experiment, the kernel initializer is the same as the LSTM gates
initializer, and biases are initialized to zero.
The source code for the model can be found in src/model/baseline_lstm.py [43].
Layer Type InputLayer Embedding LSTM Flatten Dense
Dataset IMDB: IMDB: IMDB: IMDB: IMDB:
Output Shape (None, 512) (None, 512, 32) (None, 8) (None, 8) (None, 2)
Figure 9: LSTM architecture used in the core experiments. The leftmost node represents a legend.
D.4 Transformer
Figure10displaysanencoder-onlyTransformerarchitecturedesignedforclassification. Toimplement
it, we use KerasNLP v.0.12.1 package [69].
It is a simplified version of the architecture given in the Keras manual [45]. It has two attention
heads that take positional embedding vectors of size 32 for each token (the default Keras v .3.3.3
PRNG-based scheme is used for initialization). Two fully connected layers are used, each consisting
of 32 neurons and a dropout rate of 0.1. We experiment with different weight initialization techniques
(depicted in Section 2) for attention and fully connected layers while uniformly initializing positional
embedding weights. Biases are initialized to zero. The source code for the model can be found in
src/model/baseline_transformer.py [43].
The final output layer consists of a dense layer with a softmax activation function and two
neurons (one per class). In each experiment, the kernel initializer is the same as the attention heads’
initializer, and biases are initialized to zero.
The source code for the model can be found in src/model/baseline_transformer.py [43].
E Optimizers
Both Adam and SGD optimizers use the same learning rate of 0.0001. The only exception was
SGD-trained LSTM, where we saw no improvement in 30 epochs. For that case, we increased the
learning rate to 0.01.
29Layer Type Input Layer Token And Position Embedding Transformer Block Flatten Dense
Dataset IMDB: IMDB: IMDB: IMDB: IMDB:
Output Shape (None, 512) (None, 512, 32) (None, 512, 32) (None, 16384) (None, 2)
Figure 10: Transformer architecture used in the core experiments. The leftmost node represents a
legend.
In all experiments, the remaining hyperparameters of the optimizers are left at their default
values, as set by Keras v.3.3.3. In particular, SGD momentum is set to 0; Adam hyperparameters
are β = 0.9, β = 0.999, and ϵˆ= 10−7.
1 2
F Datasets
The source code for the data pipeline can be found in src/datapipeline.py [43]. For all the
datasets, labels are converted using one-hot encoding. To eliminate the risk of randomness from
the data split process, we use complete training and test datasets provided by Keras unchanged
(without splitting the train dataset into train and validation). The following are the details for each
of the three datasets.
F.1 MNIST
TheMNISThandwrittendigitdataset[36]iswidelyusedincomputervision. Itcomprisesacollection
of 28×28 grayscale images of handwritten digits (0–9); the pixel values are in the range of 0 to 255.
In total, there are 60000 training images and 10000 testing images. The images are labelled with
their corresponding digits, making them suitable for classification tasks.
The MNIST dataset is divided into two subsets: 60000 observations for training and 10000 for
testing. This distribution follows a ratio of 6 : 1, respectively.
F.2 CIFAR-10
Another popular dataset for image classification tasks is the CIFAR-10 [32]. It consists of 60000
32×32 colour images (for each of the three channels, the pixel values range from 0 to 255) spread
across ten different classes, with 6000 images per class. The dataset includes a variety of objects
and animals, making it a more challenging dataset than MNIST. The detailed class list includes
airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.
TheCIFARdatasetissplitinto50000observationsfortrainingand10000fortesting,maintaining
a ratio of 5 : 1, respectively.
F.3 IMDB
The Large Movie Review Dataset [38] also known as the IMDB dataset is a binary sentiment analysis
dataset using IMDB reviews. Sequence modelling architectures, such as Recurrent Neural Networks
and Transformers, are commonly benchmarked on this dataset. There are 50000 English movie
reviews tagged with positive or negative sentiments.
The reviews are tokenized by word (as per [38]) using a vocabulary size of 20000 and a maximum
review length of 512 tokens for all experiments. IMDB reviews with less than 512 tokens are
zero-padded.
30For the IMDB dataset, the division includes 25000 observations for training and 25000 for
testing, resulting in a ratio of 1 : 1, respectively.
G Examples of computing measures of performance
Special case: level of accuracy not reached When a model in a pair of models fails to reach a
predetermined accuracy level A within 30 epochs, we assign an epoch value of 31 to E (A) of this
(·)
model in Equation 1. This assignment offers a conservative approximation of E(A). For example,
if E (A) = 20 and E (A) > 30, using E (A) = 31 yields E(A) = (20−31)/31 ≈ −35%, which
Q P P
should be interpreted as E(A) ≤ −35%. In this scenario, D(A) remains indeterminate.
Quantitative example Consider an example given in Figure 11. Using box-and-whisker plots,
we show the distribution of test accuracy for the 100 experiments. x-axis represents epoch number,
y-axis represents model accuracy on test dataset.
Note that E(A) and D(A) for A = 0.05,0.1,0.15, and 0.2 are identical since we reach A ≈ 0.2
during the first epoch. Since neither model reaches A ≥ 0.55, E and D for A ≥ 0.55 are not
computed.
Let us examine three values of A = 0.2,0.4,0.5.
Both models reach A = 0.2 at the first epoch. Based on Equation 1,
E (0.2)+∆ −E (0.2) 1+4−1
Q Q P
E(0.2) = ×100 = ×100 = 500%
E (0.2) 1
P
and, based on Equation 2,
D(0.2) = D (0.2)−D (0.2) ≈ 0.021−0.040 = −0.019.
Q P
When A = 0.4, the QRNG version of the initializer reaches this median accuracy at epoch 7,
while PRNG version at epoch 14. Thus,
E (0.4)+∆ −E (0.4) 7+4−13
Q Q P
E(0.4) = ×100 = ×100 ≈ −15%
E (0.4) 13
P
and
D(0.4) = D (0.4)−D (0.4) ≈ 0.019−0.029 = −0.010.
Q P
Finally, the QRNG version of the initializer reaches A = 0.5 at epoch 23, while PRNG version
does not reach this accuracy in 30 epochs. In this case, we set E (0.5) = 31 and the computation
P
becomes:
E (0.5)+∆ −E (0.5) 26+4−31
Q Q P
E(0.5) ≤ ×100 = ×100 ≈ −3%
E (0.5) 31
P
and
D(0.5) = D (0.5)−D (0.5) ≈ 0.023−indeterminate = indeterminate.
Q P
Figure 11 shows us that the maximum values of A achieved by the model with the PRNG
version of the initializer is ≈ 0.47 and with the QRNG version of the initializer is ≈ 0.51. Based on
Equation 4,
A =
min(cid:0) Amax,Amax(cid:1)
≈ min(0.47,0.51) = 0.47.
m P Q
31This median accuracy is achieved by the PRNG version in epoch 30 and by the QRNG version in
epoch 18. Thus,
E (0.47)+∆ −E (0.47) 18+4−30
Q Q P
E(A ) = E(0.47) = ×100 = ×100 ≈ −26%
M
E (0.47) 30
P
and
D(A ) = D(0.47) = D (0.47)−D (0.47) ≈ 0.023−0.037 = −0.014.
M Q P
Based on Equation 5,
S [E(A )] = S [E(A )] = S [E(0.47)] = S [−26%] = win,
E m E m E E
The p-value of the Fligner-Killeen test is ≈ 3.5×10−4. Based on Equation 6,
S [D(A )] = S [D(A )] = S [D(0.47)] = S [−0.014] = win.
D m D m D D
Thus, we can conclude that for the setup under study (CIFAR-10 test dataset, SGD optimizer,
Glorot Uniform initializer, and CNN model), the model with QRNG-based initializer reaches the best
accuracy than the model with PRNG-based initializer faster by ≈ 26%. Moreover, the QRNG-based
model has less variation (in terms of IQRs) by 0.014.
glorot-uniform
0.5
0.4
0.3
0.2
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Epoch
intializer PRNG QRNG
Figure 11: Accuracy distributions for CIFAR-10 test dataset, SGD optimizer, Glorot Uniform
optimizer, and CNN model. Horizontal dotted lines indicate accuracy thresholds of A = 0.2,0.3, and
0.4. Horizontal dashed line indicated A ≈ 0.39.
m
32
ycaruccAH The impact of QRNG and PRNG on a perceptron performance
We have demonstrated the effectiveness of QRNG by answering RQs 1–4. However, the underlying
reasons for this effectiveness deserve further investigation. Let us examine a two-layer MLP model
(showninFigure6)togetanideaofhowtherandomnumberinitializeraffectsthemodelperformance.
In order to isolate the impact of a random number generator, the output layer initializer will remain
the same (namely, the default choice for Keras and TensorFlow: PRNG-based Glorot Uniform
initializer).
Five random number generators are examined for the hidden layer: two PRNGs (Philox and
Mersenne Twister13, discussed in Section 3.1) and three QRNGs using Sobol’ sequences. The number
of units in this layer ranges from 1 to 70. The weights are initialized using one of the ten initializers
under study (listed in Section 2).
The seeds for the Philox and Mersenne Twister PRNGs are chosen at random for each experiment
(see Appendix C.2.1). In the three QRNG experiments, the starting seed is set at a constant value
of 1 or 5, or it is automatically selected using the algorithm described in the Appendix C.2.2.
Models are trained on the MNIST dataset for a single epoch using Adam optimizer. We repeat
each experiment 100 times to measure the variance in test accuracy. We use the median as a measure
of central tendency and the IQR to assess dispersion, as these are less sensitive to outliers and
skewness than the mean and standard deviation.
H.1 PRNGs
Figure 12 shows the results. For PRNG-based models, increasing the number of hidden layer units
increases the test accuracy mainly monotonically. Both PRNG initialization methods produce
similar median and IQR accuracy values. The effects of two PRNGs on the optimizer appear similar.
Consequently, the difference between the PRNG and QRNG results in RQs 1–4 cannot
be attributed to Philox algorithm implementation problems.
H.2 QRNGs
Figure 12 suggests that models initialized with QRNGs show a different dynamics of the accuracy
values that change with u than models initialized with PRNGs. Specifically, QRNG-based models
exhibits a sawtooth pattern, that (based on eyeballing of the figure) can be grouped depending on
the odd- and even-numbered count of units u, where the even-numbered count of units is further
partitioned into two sequences as follows:

odd, if (u−1) mod 2 = 0


u-sequence = even: 2, 6, 10, ..., if (u−2) mod 4 = 0 . (12)

even: 4, 8, 12, ..., if u mod 4 = 0
More research is needed to better understand why these three sequences of unit counts perform
differently. Our empirical findings are as follows.
The three sequences yield similar accuracy at high u values (probably because the effect of the
QRNG is overpowered by the flexibility provided by the large number of weights at that stage).
Amongthesmallervaluesofu, theoddvaluesyieldthelowestlevelsofaccuracy, whileu = 2,6,10,...
gives a higher level of accuracy, and u = 4,8,12,... — the highest level of accuracy. For example,
13See Appendix B.4 for details on how Mersenne Twister is used to draw values from different distributions.
33Table 5: Median accuracy plus-minus IQR values for Random Uniform initializer and u =
31,32,33,34.
Random number generator u = 31 u = 32 u = 33 u = 34
PRNG (Mersenne Twister) 0.54±0.11 0.55±0.10 0.55±0.09 0.58±0.11
PRNG (Philox) 0.53±0.08 0.55±0.09 0.56±0.10 0.57±0.09
QRNG (Sobol’ — starting seed = 1) 0.40±0.12 0.87±0.03 0.47±0.13 0.70±0.07
QRNG (Sobol’ — starting seed = 5) 0.54±0.10 0.67±0.18 0.57±0.10 0.63±0.07
QRNG (Sobol’ — starting seed = auto) 0.69±0.18 0.84±0.05 0.69±0.17 0.76±0.19
Table 5 illustrates that for u = 33, QRNG with automatic seed selection scheme for Random Uniform
initializer generates a median accuracy of 0.69, for u = 34 — 0.76, and for u = 32 — 0.84.
Figures 13 and 14 show the median and IQR values of accuracy obtained by the models on a
test dataset. We plot the data for the three u-sequences separately for better readability. For all
sequences of u, the median accuracy increases (although non-monotonically), but the pattern of
behavior for u = 4,8,12,... is the least volatile (but IQR values may sometimes be higher).
H.2.1 PRNG vs. QRNG
Thedifferenceinperformancebetweenmodelsusingdifferentrandomnumbergeneratorsissometimes
striking. For instance, Table 5 shows that the median accuracy using the Random Uniform initializer
with32unitsis0.55(IQRof0.09)forPRNGPhiloxand0.84(IQRof0.05)forQRNGwithautomatic
seed selection. In this case, QRNG offers 1.5 times higher median accuracy and significantly less
variability (by 0.04).
H.2.2 QRNG seed selection methods
When comparing three QRNG initialization methods, the automatic starting seed selection method
results in the least variability in precision with u (compare the accuracy values in Table 5).
However, for u = 4,8,12,..., QRNG-based models with a starting seed value of 1 slightly
outperform the automatic method. For instance, Table 5 shows that a starting seed of 1 yields an
accuracy of 0.87±0.03, compared to 0.84±0.05 for the automatic method. Conversely, a starting
seed of 5 yields an accuracy of only 0.68±0.18.
When u is odd or u = 2,6,10,..., automatic seed selection outperforms the constant seeds 1 and
5. For instance, for u = 33, QRNG-based models using seed 1 and 5 achieve accuracies of 0.47±0.13
and 0.57±0.10, respectively. These values are comparable to or lower than those produced of
PRNG (Philox), which yields an accuracy of 0.56±0.10. In contrast, the QRNG-based model with
automatic seed selection achieves an accuracy of 0.69±0.17, which is 1.2 times better than that of
PRNG (Philox).
This suggests that seed selection is crucial and that our heuristic for seed selection
may need further refinement.
H.2.3 QRNG sequence length
Finally, if the sequence length is not a power of two, Sobol’ sequences lose their balance properties,
which is detrimental to Monte Carlo-based methods [48]. However, Figures 12 and 13 show no
dramatic deviations in accuracy near u = 1,2,4,8,...,64 (compared to the closest value u in the
corresponding sequence). Although some effect may be present (e.g., at u = 32), from a practical
34perspective, this implies that we can use the sequences for weight initialization even if the
number of samples is not a power of two.
35glorot-normal glorot-uniform
0.75
0.50
0.25
he-normal he-uniform
0.75
0.50
0.25
lecun-normal lecun-uniform
0.75
0.50
0.25
orthogonal random-normal
0.75
0.50
0.25
random-uniform truncated-normal
0.75
0.50
0.25
0 20 40 60 0 20 40 60
Units Count
PRNG (Mersenne Twister) PRNG (Philox) QRNG (Sobol' - starting_seed = 1) QRNG (Sobol' - starting_seed = 5) QRNG (Sobol' - starting_seed = auto)
Figure 12: The median test accuracy for the perceptron-based model (given in Figure 6). The x-axis
shows the number of units u in the model. The y-axis represents the model’s accuracy on the MNIST
testdataset. Thelinesrepresentmedianaccuracybasedon100repetitions, whiletheribbonsindicate
the range between the lower and upper quartiles. Vertical dotted lines show u = 2,4,8,16,32,64.
36
ycaruccAeven: 2, 6, 10, ... even: 4, 8, 12, ... odd
0.75
0.50
0.25
initializer
glorot-normal
0.75
glorot-uniform
he-normal
he-uniform
0.50 lecun-normal
lecun-uniform
orthogonal
random-normal
0.25
random-uniform
truncated-normal
0.75
0.50
0.25
0 20 40 60 0 20 40 60 0 20 40 60
Units Count
Figure 13: The median test accuracy for the perceptron-based model (given in Figure 6) using
three different QRNG-based initialization schemes for three sequences of u (shown in Equation 12).
Vertical dotted lines show u = 2,4,8,16,32,64.
37
ycaruccA
naideM
QRNG
(Sobol'
- starting_seed
=
1)
QRNG
(Sobol'
- starting_seed
=
5)
QRNG
(Sobol'
- starting_seed
=
auto)even: 2, 6, 10, ... even: 4, 8, 12, ... odd
0.5
0.4
0.3
0.2
0.1
0.0
0.5 initializer
glorot-normal
0.4 glorot-uniform
he-normal
0.3 he-uniform
lecun-normal
0.2 lecun-uniform
orthogonal
random-normal
0.1
random-uniform
truncated-normal
0.0
0.5
0.4
0.3
0.2
0.1
0.0
0 20 40 60 0 20 40 60 0 20 40 60
Units Count
Figure 14: The median test accuracy for the perceptron-based model (given in Figure 6) using
three different QRNG-based initialization schemes for three sequences of u (shown in Equation 12).
Vertical dotted lines show u = 2,4,8,16,32,64.
I Summary statistics
The values of Amax and Amax required to compute S for all cases are shown graphically in Figure 15.
P Q A
The values of E (A ) and E (A ), necessary for calculating E(A ), are presented in Figure 16.
P m Q m m
The values of D (A ) and D (A ), used to determine D(A ), are illustrated in Figure 17. The
P m Q m m
average values of Amax−Amax, denoted by α¯, grouped by optimizer, dataset, and model, are shown
Q P
in Table 6; grouped by initializer — in Table 7; grouped by model and optimizer — in Table 8; and
grouped by dataset and optimizer — in Table 9 . Finally, the average E(A ) values, denoted by
m
E¯(A ), grouped by model and optimizer are given in Table 10.
m
38
ycaruccA
fo RQI
QRNG
(Sobol'
- starting_seed
=
1)
QRNG
(Sobol'
- starting_seed
=
5)
QRNG
(Sobol'
- starting_seed
=
auto)CIFAR-10, CNN CIFAR-10, MLP IMDB, LSTM
1.00
0.75
initializer
glorot-normal
0.50 glorot-uniform
he-normal
he-uniform
0.25
lecun-normal
lecun-uniform
IMDB, Transformer MNIST, CNN MNIST, MLP orthogonal
1.00 random-normal
random-uniform
truncated-normal
0.75
optimizer
0.50 Adam
SGD
0.25
0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
Max Accuracy (PRNG): Amax
P
Figure 15: Amax and Amax values needed to compute S for all 120 cases. Black line is governed by
P Q A
equation Amax = Amax.
Q P
CIFAR-10, CNN CIFAR-10, MLP IMDB, LSTM
30
initializer
20
glorot-normal
glorot-uniform
10 he-normal
he-uniform
lecun-normal
0 lecun-uniform
IMDB, Transformer MNIST, CNN MNIST, MLP orthogonal
random-normal
30
random-uniform
truncated-normal
20
optimizer
Adam
10 SGD
0
0 10 20 30 0 10 20 30 0 10 20 30
Epoch Count (PRNG): E(A )
P m
Figure 16: E (A ) and E (A ) values needed to compute E(A ) for all 120 cases. Black line is
P m Q m m
governed by equation E (A) = E (A). To improve plot readability, E (A ) and E (A ) values
Q P P m Q m
are jittered. Horizontal and vertical dotted lines represent 30 epochs.
39
xamA
:)GNRQ(
ycaruccA
xaM
)
A(
E
:)GNRQ(
tnuoC
hcopE
Q
m
QCIFAR-10, CNN CIFAR-10, MLP IMDB, LSTM
10-1
initializer
10-2
glorot-normal
glorot-uniform
10-3 he-normal
he-uniform
lecun-normal
10-4 lecun-uniform
IMDB, Transformer MNIST, CNN MNIST, MLP orthogonal
random-normal
10-1 random-uniform
truncated-normal
10-2 optimizer
Adam
10-3 SGD
10-4
10-4 10-3 10-2 10-1 10-4 10-3 10-2 10-1 10-4 10-3 10-2 10-1
IQR (PRNG): D(A )
P m
Figure 17: D (A ) and D (A ) values needed to compute D(A ) for all 120 cases. Black
P m Q m m
line is governed by equation D(A) = D (A) − D (A) = 0; blue line is governed by D(A) =
Q P
D (A)−D (A) = 0.01; green line is governed by D(A) = D (A)−D (A) = −0.01.
Q P Q P
40
)
A(
D
:)GNRQ(
RQI
m
Q41
.ledom
dna
,tesatad
,rezimitpo
yb
depuorg
,α¯
yb
detoned
,xamA−xamA
egarevA
:6
elbaT
P
Q
dnarG
DGS
DGS
madA
madA
latoT
latoT
TSINM
TSINM
BDMI
BDMI
01-RAFIC
01-RAFIC
latoT
TSINM
TSINM
BDMI
BDMI
01-RAFIC
01-RAFIC
emoctuO
latoT
PLM
NNC
latoT
.fsnarT
MTSL
latoT
PLM
NNC
latoT
PLM
NNC
latoT
.fsnarT
MTSL
latoT
PLM
NNC
10.0-
10.0-
00.0
00.0
10.0-
10.0-
40.0-
40.0-
00.0
10.0-
10.0-
00.0
00.0
00.0
00.0
00.0
00.0
00.0
*,*,l:l
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
*,l,t:l
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
10.0-
00.0
40.0-
00.0
00.0
10.0-
00.0
00.0
00.0
00.0
00.0
00.0
00.0
latoTssoL
00.0
00.0
00.0
00.0
t,t,t:t
00.0
00.0
00.0
00.0
latoTeiT
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
tw,w,t:w
90.0
01.0
10.0
10.0
51.0
20.0
43.0
90.0
00.0
00.0
00.0
00.0
51.0
51.0
*,l,w:w
00.0
00.0
00.0
00.0
tw,t,w:w
80.0
70.0
20.0
20.0
80.0
60.0
41.0
21.0
21.0
21.0
l,w,w:w
50.0
60.0
50.0
70.0
10.0
01.0
70.0
21.0
30.0
10.0
10.0
10.0
10.0
50.0
20.0
60.0
tw,w,w:w
60.0
60.0
40.0
70.0
10.0
00.0
10.0
00.0
11.0
50.0
71.0
50.0
10.0
10.0
00.0
00.0
10.0
00.0
01.0
11.0
60.0
latoTniW
30.0
40.0
40.0
70.0
10.0
00.0
00.0
00.0
90.0
40.0
51.0
20.0
00.0
00.0
00.0
00.0
00.0
00.0
60.0
01.0
20.0
latoTdnarG
.rezilaitini
yb
depuorg
,α¯
yb
detoned
,xamA−xamA
egarevA
:7 elbaT
P
Q
dnarG
lanogohtrO
epahS
citsongaepahS
epahS
tnednepedepahS
latoT
latoT
citsonga
detacnurT
modnaR
modnaR
tnedneped
nuceL
nuceL
eH
eH
torolG
torolG
emoctuO
latoT
lamron
mrofinu
lamron
latoT
mrofinu
lamron
mrofinu
lamron
mrofinu
lamron
10.0-
10.0-
00.0
10.0-
00.0
10.0-
00.0
10.0-
00.0
10.0-
*,*,l:l
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
*,l,t:l
00.0
00.0
00.0
00.0
10.0-
00.0
00.0
00.0
00.0
00.0
00.0
00.0
10.0-
latoTssoL
00.0
00.0
00.0
t,t,t:t
00.0
00.0
00.0
latoTeiT
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
00.0
tw,w,t:w
90.0
80.0
01.0
90.0
80.0
51.0
71.0
00.0
70.0
*,l,w:w
00.0
00.0
tw,t,w:w
80.0
41.0
30.0
30.0
80.0
80.0
80.0
11.0
41.0
20.0
l,w,w:w
50.0
30.0
20.0
20.0
10.0
30.0
60.0
70.0
70.0
90.0
01.0
30.0
40.0
tw,w,w:w
60.0
60.0
20.0
20.0
10.0
30.0
70.0
60.0
60.0
21.0
21.0
30.0
40.0
latoTniW
30.0
40.0
10.0
10.0
00.0
10.0
40.0
50.0
50.0
70.0
60.0
20.0
20.0
latoTdnarGTable 8: Average Amax−Amax, denoted by α¯, grouped by model and optimizer.
Q P
Outcome CNN CNN LSTM LSTM MLP MLP Transformer Transformer Grand
Adam SGD Total Adam SGD Total Adam SGD Total Adam SGD Total Total
l:l,*,* 0.00 -0.02 -0.01 0.00 0.00 -0.01 -0.01 0.00 -0.01 -0.01 -0.01
l:t,l,* 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
LossTotal 0.00 -0.02 0.00 0.00 0.00 0.00 -0.01 0.00 -0.01 0.00 0.00 0.00 0.00
t:t,t,t 0.00 0.00 0.00
TieTotal 0.00 0.00 0.00
w:t,w,wt 0.00 0.00 0.00 0.00 0.00 0.00
w:w,l,* 0.00 0.34 0.17 0.15 0.02 0.10 0.00 0.01 0.01 0.09
w:w,t,wt 0.00 0.00 0.00
w:w,w,l 0.14 0.14 0.12 0.06 0.09 0.02 0.02 0.08
w:w,w,wt 0.06 0.06 0.06 0.01 0.07 0.05 0.01 0.01 0.05
WinTotal 0.04 0.09 0.08 0.00 0.00 0.00 0.08 0.06 0.07 0.01 0.01 0.01 0.06
GrandTotal 0.01 0.08 0.04 0.00 0.00 0.00 0.05 0.05 0.05 0.00 0.00 0.00 0.03
Table 9: Average Amax−Amax, denoted by α¯, grouped by dataset and optimizer.
Q P
Outcome CIFAR-10 CIFAR-10 IMDB IMDB MNIST MNIST Grand
Adam SGD Total Adam SGD Total Adam SGD Total Total
l:l,*,* 0.00 -0.04 -0.01 0.00 -0.01 0.00 -0.01 0.00 -0.01 -0.01
l:t,l,* 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Loss Total 0.00 -0.01 -0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00
t:t,t,t 0.00 0.00 0.00
Tie Total 0.00 0.00 0.00
w:t,w,wt 0.00 0.00 0.00 0.00
w:w,l,* 0.15 0.15 0.15 0.00 0.01 0.01 0.00 0.00 0.09
w:w,t,wt 0.00 0.00 0.00
w:w,w,l 0.12 0.08 0.10 0.02 0.02 0.08
w:w,w,wt 0.05 0.10 0.08 0.01 0.01 0.01 0.05 0.04 0.05
Win Total 0.10 0.11 0.11 0.00 0.00 0.00 0.01 0.04 0.03 0.06
Grand Total 0.06 0.09 0.08 0.00 0.00 0.00 0.00 0.04 0.02 0.03
42Table 10: Average E(A ) values, denoted by E¯(A ), grouped by model and optimizer.
m m
Outcome CNN CNN LSTM LSTM MLP MLP Transformer Transformer Grand
Adam SGD Total Adam SGD Total Adam SGD Total Adam SGD Total Total
l:l,*,* 41 72 47 47 47 100 100 233 104 169 83
l:t,l,* 13 13 31 70 49 31 806 612 29 15 18 125
LossTotal 31 72 37 33 70 48 90 806 305 165 44 85 106
t:t,t,t 0 0 0
TieTotal 0 0 0
w:t,w,wt -8 -47 -36 -18 -18 -32
w:w,l,* 9 400 204 333 400 358 36 11 17 235
w:w,t,wt 0 0 0
w:w,w,l -63 -63 -27 -59 -46 -3 -3 -43
w:w,w,wt -47 -43 -44 -37 -61 -52 -25 -25 -46
WinTotal -28 7 -1 -8 -47 -36 107 21 58 -14 7 -7 19
GrandTotal 14 14 14 22 11 17 101 139 120 39 30 34 53
43J Details of Models’ Performance
Using box-and-whisker plots, we show the distribution of test accuracy for the 100 experiments.
Each subplot in a figure shows the accuracy for a given PRNG- and QRNG-based initializer type.
x-axis represents epoch number, y-axis represents model accuracy on test dataset. In order to make
it easier for the reader (since there are 12 figures), we created a table of figures as shown in Table 11.
Additionally, we create a heatmap summary plot, shown in Figure 18, which summarizes the
dynamics depicted in the box-and-whisker plots on one page. Table 12 lists 17 values of E(A) from
Figure 18 that are capped at 100%.
Table 11: A table of figures showing the distributions of test accuracy based on 100 experiments.
Model Optimizer MNIST CIFAR-10 IMDB
SGD Fig. 19 Fig. 23 –
MLP
Adam Fig. 20 Fig. 24 –
SGD Fig. 21 Fig. 25 –
CNN
Adam Fig. 22 Fig. 26 –
SGD – – Fig. 27
LSTM
Adam – – Fig. 28
SGD – – Fig. 29
Transformer
Adam – – Fig. 30
Table 12: The values of E(A) > 100% (see Figure 18 for details).
Dataset Architecture Initializer Optimizer E(A)
CIFAR-10 CNN He Normal SGD 400
CIFAR-10 CNN He Uniform SGD 400
CIFAR-10 MLP Glorot normal Adam 400
CIFAR-10 MLP Glorot normal SGD 400
CIFAR-10 MLP Glorot uniform SGD 400
CIFAR-10 MLP He normal SGD 2000
CIFAR-10 MLP He uniform Adam 400
CIFAR-10 MLP He uniform SGD 400
CIFAR-10 MLP Lecun normal Adam 400
CIFAR-10 MLP Orthogonal Adam 400
CIFAR-10 MLP Orthogonal SGD 400
IMDB LSTM Lecun normal SGD 209
IMDB Transformer He normal Adam 238
IMDB Transformer He uniform Adam 229
IMDB Transformer Random normal SGD 138
MNIST MLP Glorot normal Adam 154
MNIST MLP Random uniform Adam 146
44glorot-normal glorot-uniform
0.75
0.50
0.25
he-normal he-uniform
0.75
0.50
0.25
lecun-normal lecun-uniform
0.75
0.50
0.25
orthogonal random-normal
0.75
0.50
0.25
random-uniform truncated-normal
0.75
0.50
0.25
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 19: Accuracy distributions for MNIST test dataset, SGD optimizer and MLP model.
45
ycaruccAglorot-normal glorot-uniform
1.00
0.75
0.50
0.25
he-normal he-uniform
1.00
0.75
0.50
0.25
lecun-normal lecun-uniform
1.00
0.75
0.50
0.25
orthogonal random-normal
1.00
0.75
0.50
0.25
random-uniform truncated-normal
1.00
0.75
0.50
0.25
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 20: Accuracy distributions for MNIST test dataset, Adam optimizer and MLP model.
46
ycaruccAglorot-normal glorot-uniform
1.0
0.8
0.6
0.4
he-normal he-uniform
1.0
0.8
0.6
0.4
lecun-normal lecun-uniform
1.0
0.8
0.6
0.4
orthogonal random-normal
1.0
0.8
0.6
0.4
random-uniform truncated-normal
1.0
0.8
0.6
0.4
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 21: Accuracy distributions for MNIST test dataset, SGD optimizer and CNN model.
47
ycaruccAglorot-normal glorot-uniform
0.975
0.950
0.925
0.900
0.875
he-normal he-uniform
0.975
0.950
0.925
0.900
0.875
lecun-normal lecun-uniform
0.975
0.950
0.925
0.900
0.875
orthogonal random-normal
0.975
0.950
0.925
0.900
0.875
random-uniform truncated-normal
0.975
0.950
0.925
0.900
0.875
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 22: Accuracy distributions for MNIST test dataset, Adam optimizer and CNN model.
48
ycaruccAglorot-normal glorot-uniform
0.4
0.3
0.2
0.1
he-normal he-uniform
0.4
0.3
0.2
0.1
lecun-normal lecun-uniform
0.4
0.3
0.2
0.1
orthogonal random-normal
0.4
0.3
0.2
0.1
random-uniform truncated-normal
0.4
0.3
0.2
0.1
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 23: Accuracy distributions for CIFAR-10 test dataset, SGD optimizer and MLP model.
49
ycaruccAglorot-normal glorot-uniform
0.5
0.4
0.3
0.2
0.1
he-normal he-uniform
0.5
0.4
0.3
0.2
0.1
lecun-normal lecun-uniform
0.5
0.4
0.3
0.2
0.1
orthogonal random-normal
0.5
0.4
0.3
0.2
0.1
random-uniform truncated-normal
0.5
0.4
0.3
0.2
0.1
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 24: Accuracy distributions for CIFAR-10 test dataset, Adam optimizer and MLP model.
50
ycaruccAglorot-normal glorot-uniform
0.5
0.4
0.3
0.2
0.1
he-normal he-uniform
0.5
0.4
0.3
0.2
0.1
lecun-normal lecun-uniform
0.5
0.4
0.3
0.2
0.1
orthogonal random-normal
0.5
0.4
0.3
0.2
0.1
random-uniform truncated-normal
0.5
0.4
0.3
0.2
0.1
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 25: Accuracy distributions for CIFAR-10 test dataset, SGD optimizer and CNN model.
51
ycaruccAglorot-normal glorot-uniform
0.6
0.4
0.2
he-normal he-uniform
0.6
0.4
0.2
lecun-normal lecun-uniform
0.6
0.4
0.2
orthogonal random-normal
0.6
0.4
0.2
random-uniform truncated-normal
0.6
0.4
0.2
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 26: Accuracy distributions for CIFAR-10 test dataset, Adam optimizer and CNN model.
52
ycaruccAglorot-normal glorot-uniform
0.54
0.52
0.50
0.48
he-normal he-uniform
0.54
0.52
0.50
0.48
lecun-normal lecun-uniform
0.54
0.52
0.50
0.48
orthogonal random-normal
0.54
0.52
0.50
0.48
random-uniform truncated-normal
0.54
0.52
0.50
0.48
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 27: Accuracy distributions for IMDB test dataset, SGD optimizer and LSTM model.
53
ycaruccAglorot-normal glorot-uniform
0.9
0.8
0.7
0.6
0.5
he-normal he-uniform
0.9
0.8
0.7
0.6
0.5
lecun-normal lecun-uniform
0.9
0.8
0.7
0.6
0.5
orthogonal random-normal
0.9
0.8
0.7
0.6
0.5
random-uniform truncated-normal
0.9
0.8
0.7
0.6
0.5
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 28: Accuracy distributions for IMDB test dataset, Adam optimizer and LSTM model.
54
ycaruccAglorot-normal glorot-uniform
0.65
0.60
0.55
0.50
he-normal he-uniform
0.65
0.60
0.55
0.50
lecun-normal lecun-uniform
0.65
0.60
0.55
0.50
orthogonal random-normal
0.65
0.60
0.55
0.50
random-uniform truncated-normal
0.65
0.60
0.55
0.50
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 29: Accuracy distributions for IMDB test dataset, SGD optimizer and Transformer model.
55
ycaruccAglorot-normal glorot-uniform
0.9
0.8
0.7
0.6
0.5
he-normal he-uniform
0.9
0.8
0.7
0.6
0.5
lecun-normal lecun-uniform
0.9
0.8
0.7
0.6
0.5
orthogonal random-normal
0.9
0.8
0.7
0.6
0.5
random-uniform truncated-normal
0.9
0.8
0.7
0.6
0.5
1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930 1 2 3 4 5 6 7 8 9 101112131415161718192021222324252627282930
Epoch
intializer PRNG QRNG
Figure 30: Accuracy distributions for IMDB test dataset, Adam optimizer and Transformer model.
56
ycaruccAcifar10, baseline-ann, cifar10, baseline-ann, cifar10, baseline-cnn, cifar10, baseline-cnn,
adam sgd adam sgd
truncated-normal
random-uniform
random-normal
orthogonal
lecun-uniform
lecun-normal
he-uniform
he-normal
glorot-uniform
glorot-normal
D(A)
basim eld inb e_ -r lse tv mie , w ads, am baim sed lib n_ er -e lsv ti mew , ss g, d basi em lid nb e a_ - dtr r ae a mv ni se fw ors m, er, basi em lid nb e_ - str r ge a dv ni se fw ors m, er,
0.0
truncated-normal
random-uniform
random-normal -0.1
orthogonal
lecun-uniform
lecun-normal
he-uniform E(A)
100
he-normal
glorot-uniform 50
glorot-normal
0
mnist, b aa ds ae mline-ann, mnist, baseline-ann, sgd mnist, b aa ds ae mline-cnn, mnist, baseline-cnn, sgd -50
-100
truncated-normal
random-uniform
random-normal
orthogonal
lecun-uniform
lecun-normal
he-uniform
he-normal
glorot-uniform
glorot-normal
0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00 0.25 0.50 0.75 1.00
Accuracy Threshold (A)
Figure 18: A summary plot showing the values of E(A) in the main colour of a tile and D(A) in
the circle in the middle of a given tile. A yellow circle indicates that D(A) is indeterminate (i.e.,
pseudorandom or quasirandom experiments did not achieve a given level of accuracy in 30 epochs).
ThevaluesofthemedianaccuracythresholdsAshownare0.10,0.15,0.20,...,0.95. A0.05increment
is chosen for A to ensure the plots are readable.
Among all the tiles, 17 have E(A) > 100%. To improve the readability of the figures’ colours, we
cap these cases at 100% and list them in Table 12.
A negative (blue) value of E(A) indicates that the QRNG-based version of initializer achieves A
faster than the PRNG-based version. A negative (blue) value of D(A) indicates that QRNG-based
version of the initializers exhibit less variation in A than PRNG-based version. The values of A
m
are given by the rightmost tile in each row of the heatmaps.
57
rezilaitinI