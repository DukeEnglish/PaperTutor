Graphical Modelling without Independence Assumptions for Uncentered Data
BaileyAndrew1,DavidRWesthead1,LuisaCutillo1
1UniversityofLeeds
sceba@leeds.ac.uk,D.R.Westhead@leeds.ac.uk,L.Cutillo@leeds.ac.uk
Abstract Conditional dependency graphs have several convenient
properties.Theyareinterpretable,andintuitivelycapturethe
Theindependenceassumptionisausefultooltoincreasethe ‘directeffect’oftwovariablesoneachother(ratherthan‘in-
tractabilityofone’smodellingframework.However,thisas-
directeffects’passingthroughconfoundersandmediators).
sumption does not match reality; failing to take dependen-
Theyaretypicallysparse,andarerelatedtocausality1.They
ciesintoaccountcancausemodelstofaildramatically.The
also have a very useful property in multivariate Gaussian
fieldofmulti-axisgraphicalmodelling(alsocalledmulti-way
datasets,namelythattwoGaussianvariablesarecondition-
modelling,Kronecker-separablemodelling)hasseengrowth
over the past decade, but these models require that the data allyindependentifandonlyifthecorrespondingterminthe
havezeromean.Inthemulti-axiscase,inferenceistypically inversecovariancematrixiszero.
done in the single sample scenario, making mean inference
impossible.
x⊥⊥y |D G ⇐aus ⇒sian Σ−1 =0
In this paper, we demonstrate how the zero-mean assump- \x\y xy
tion can cause egregious modelling errors, as well as pro- The inverse covariance matrix is also called the preci-
pose a relaxation to the zero-mean assumption that allows sion matrix, and we will denote it Ψ. It can be interpreted
the avoidance of such errors. Specifically, we propose the as an adjacency matrix for the graph of conditional depen-
“Kronecker-sum-structured mean” assumption, which leads dencies.Duetothisconvenientcorrespondence,anddueto
tomodelswithnonconvex-but-unimodallog-likelihoodsthat
the fact that practically all multi-axis work has so far been
canbesolvedefficientlywithcoordinatedescent.
limited to the Gaussian case, we will assume in this paper
that our data is Gaussian. Through tools such as the Non-
Introduction paranormalSkeptic(Liuetal.2012),thisassumptioncanbe
weakendtotheGaussiancopulaassumption(intuitively,the
Weoftenwishtofindnetworks(‘graphs’)thatdescribeour variableshavearbitrarycontinuousmarginalsbutstillinter-
data.Forexample,wemaybeinterestedingeneregulatory act‘Gaussian-ly’).
networks in biology, or social interaction networks in epi- Whenmakingindependenceassumptions,thisproblemis
demiology. In these cases, the graph itself is an object of solvedbywell-knownmethodssuchastheGraphicalLasso
interest. When the goal of an analysis does not involve a (Friedman,Hastie,andTibshirani2008).However,indepen-
graph, the creation of one can still be useful as a prepro- denceassumptionsareoftenfalseinpractice.Theybecome
cessingsteptofurtherinsights,suchasforclustering.Their particularlyegregiousinthecaseofomicsdata,suchassin-
use is not limited to clustering; the first step of the popu- glecellRNA-sequencing(scRNA-seq).Thesedatasetstake
lar dimensionality reduction method UMAP, for example, theformofacells-by-genesmatrix.Wemightbeinterested
istocreatea(typicallynearestneighbors)graph(McInnes, inageneregulatorynetwork,inwhichcaseweassumethe
Healy, and Melville 2020), but one could also experiment cellsareindependent.Alternatively,wemightbeinterested
withothergraphs. inthecellularmicroenvironment,inwhichcaseweassume
Whiletherearemanytypesofgraphs,thispaperwillfo- thegenesareindependent.Whicheveronewewanttolearn,
cusonconditionaldependencygraphs.Intuitively,twover- we are put in the awkward position of assuming the other
tices are connected in such a graph if they are still statis- doesnotexist.
ticallydependentafterconditioningouttheothervariables. To avoid independence assumptions altogether, we can
Wedenotethepropertythatxandy areconditionallyinde- vectorize our dataset. Instead of n samples of m features,
pendent, given z, as x ⊥⊥ y | z. We will focus on the case wehave1sampleofnmfeatures.ForscRNA-seq,thepre-
whereweconditionoverallothervariablesinthedatasetD, cisionmatrixisofsizeO(n2m2)andrepresentsthedepen-
def
i.e.wherez =D = D−{x,y}.
\x\y 1Undermodestassumptions,thatcanbeviolatedinsomereal-
world scenarios, conditional dependency graphs form the ‘skele-
x⊥⊥y |z ⇐d ⇒ef P[x,y |z]=P[x|z]P[y |z] ton’ofacausaldirectedacyclicgraph,i.e.thecausalgraphwith
directionalityremoved.
4202
guA
5
]EM.tats[
1v39320.8042:viXradenciesbetweendifferent(cell,gene)pairs.Theproblembe- butisphilosophicallytroubling.Supposewehaveadataset
comesbothcomputationallyandstatisticallyintractable;we of only two points, both distinct and independent. The true
quicklyrunoutofspacetostoresuchmatrices,andhaveno mean value is just their average. Now, suppose we dupli-
way of being confident in our results to any degree of sta- catethefirstpointseveraltimes,perhapswithsomemodest
tisticalcertainty.Weneedtofindamiddlegroundbetween noiseadded-themeanestimatewillliemuchclosertothe
independenceandfulldependence. firstpoint.
Todothis,wecantakeadvantageofmatrixstructure.We In fact, we can choose how many times to duplicate the
are not truly interested in a graph of (cell, gene) pairs, but first or second points in order to place the estimated mean
rather a graph of cells and a graph of genes. Thus, we can anywhere between the original points. However, this esti-
makea‘graphdecompositionassumption’;our(cell,gene) mated mean is an artifact of the dependencies in our data!
pairgraphshouldbeabletobefactored(forsomedefinition Thetruemean,ultimately,stillliesatitsoriginallocation.
offactoring)intothecellgraphandthegenegraph. As an example, suppose we were studying a (non-
Several choices of decomposition have appeared in the Hodgkin’s)lymphomadatasetthatwasrepresentativeofthe
literature, such as the Kronecker product (Dutilleul 1999) general population. Only 2% of lymphoma cases are lym-
and squared Kronecker sum decompositions (Wang, Jang, phoblastic lymphoma2, but specialized CAR-T cell treat-
and Hero 2020); in this paper, we focus on the Kronecker ments have been approved for it in the UK, such as
sum decomposition (Kalaitzis et al. 2013) due to its popu- Brexucabtagene Autoleucel (NICE, 2013). In models that
larity,convexity,relationshiptoconditionaldependence,in- don’t take into account dependencies, this signal could be
terpretabilityasaCartesianproduct,andcorrespondenceto drownedoutbytheother,morecommon,subtypes;the‘av-
amaximumentropydistribution.TheKroneckersumisde- eragelymphomacase’wouldlookmuchmoreliketheaver-
noted⊕andisdefinedasA ⊕B =A ⊗I + agecaseofitslargestsubtype,ratherthanbeingrepresenta-
a×a b×b a×a b×b
I ⊗B ,where⊗istheKroneckerproduct.Theopera- tiveofallsubtypes.Thefactthatraresubtypesmaybenefit
a×a b×b
tionisassociative,allowingastraightforwardgeneralization fromspecializedtreatmentssimplygetsdrownedoutbythe
tomorethantwoaxes(i.e.tensor-variatedatasets);thiswas propertiesoftheothers.
firstexploredbyGreenewald,Zhou,andHero(2019).Under
thezero-meanandKroneckersumdecomposabilityassump- OurContributions
tions,themodelforadatasetDisasfollows:
PR Curves for Barabasi-Albert graph (100 nodes)
[+1 offset all entries]
1.0
D ∼N KS(0,{Ψ ℓ} ℓ) GmGM
 (cid:32) (cid:33)−1 N Teo rn ac Le an sstr oal GmGM
(cid:77) 0.8 Noncentral TeraLasso
⇐⇒ vec[D]∼N 0, Ψ ℓ 
ℓ
0.6
Thezero-meanassumptionisconspicuous,butnecessary;
recall that we transformed our dataset from n samples and
m features to a single sample of nm features. Estimating 0.4
the mean of a single sample is a recipe for disaster; when
onecentersthedataset,theywillbeleftwithaconstantzero
0.2
vector.
Wecouldconsiderdecomposingthemeanofourmodel,
just like for our precision matrices. A natural and analo- 0.0
0.0 0.2 0.4 0.6 0.8 1.0
gousdecompositionwouldbethatourmeantakestheform Recall
µ cells⊕(cid:101)µ
genes
=µ cells⊗1+1⊗µ genes;thisoperationin-
Figure 1: Precision and recall for on a synthetic dataset in
termixesthemeansinthesamewaytheKroneckersumin-
which data was generated from a Kronecker-sum normal
termixes the variances. For Kronecker product models (not
distribution,andthenhadaconstantvalueof1addedtoev-
Kronecker sum models), this has been considered before
eryentryofthematrix.Errorbarsarethebest/worstperfor-
(Allen and Tibshirani 2010). It was shown that the maxi-
manceover10trials;thecenterlineisaverageperformance.
mum likelihood for µ has the following form (in the two-
ℓ
axis case, where d represents the number of elements in
ℓ
axisℓ): Wearenaturallyledtothefollowingquestion:
Problem1. ForthenoncentralKroneckersummodel,does
1T (cid:0) X−1µT(cid:1) 1T (cid:0) X−1µT(cid:1) using the mean decomposition (cid:76) (cid:102) µ allow its parameters
µ = 2 µ = 1 tobeeffectivelyestimated? ℓ ℓ
1 d 2 d
2 1
In this paper, we show that the answer to this question
We will see later that such formulas are not as simple in
isyes.FortheKronecker-sum-structuredmodel,theestima-
theKroneckersumcase.First,though,letusnotethatthees-
torsforµ dependonΨ .However,duetothisdependency,
timateofµ doesnotdependonthedependenciesinthedata ℓ ℓ
ℓ
(Ψ ℓ).Thisisamathematicallyconvenientpropertytohave, 2AccordingtotheLeukemiaFoundationcharity.
noisicerPourproblemisnolongerconvex;therecouldbesomenon- Kronecker-sum-decomposablevectors.Finally,letK be
Mv
globalminima! thespaceofvectorsoftheformXy,whereX ∈ K ,y ∈
M
Problem2. Isourestimatorfortheparametersofthenon-
K v. All of these spaces are linear subspaces of Rd∀ or
centralKronecker-sum-structurednormaldistributionguar-
Rd∀×d∀. We may use K as a shorthand when discussing
propertiesthatapplytobothK andK .
anteedtobetheglobalmaximumlikelihoodestimator? M v
It is important to note that the parameterization of K we
Wealsoshowthattheanswertothisquestionisalsoaffir- have used so far is not identifiable. If (cid:80) c = 0, then
mative;infact,therearenonon-globalminima,despitethe (cid:76) (Ψ +c I) = (cid:76) Ψ . Greenewald, Zℓ hoℓ u, and Hero
nonconvexity. ℓ ℓ ℓ ℓ ℓ (cid:76)
Onemayalsowonderifthezero-meanassumptionistruly
(2019) use the identifiable representation τI
∀
+ ℓΨ(cid:101)ℓ,
(cid:104) (cid:105)
problematic.Infact,thezero-meanassumptioncanhavean wheretr Ψ(cid:101)ℓ =0.Likewise,wecanidentifiablyrepresent
extremely dramatic negative effect on performance. In the ourmeandecompositionasm1+(cid:76) (cid:102) µ ,where1Tµ =0.
extremelysimplescenarioinwhichweaddaconstantoffset ℓ(cid:101)ℓ (cid:101)ℓ
Thelastimportantpropertywewillintroduceisthematrix
toeveryelement,wecanseethatmethodsthatdonottryto
representation of Kronecker products of vectors: note that
estimate the mean are practically worthless, whereas those
(cid:16) (cid:17)
that do (which we shall call ‘noncentral’) maintain reason- µ ⊗1 = I ⊗1 µ .
ℓ d\ℓ dℓ d\ℓ ℓ
ableperformance(Figure1).
EstimationMethod
Notation
Asashorthand,letx=vec[D].Ournegativeloglikelihood
In this paper, lowercase letters a will refer to scalars, low- looksjustlikethatofournormaldistribution,exceptthatthe
ercase bold v will refer to vectors, uppercase bold M will parametersarerestrictedtolieinK.Toensureexistenceof
refertomatrices,anduppercasecalligraphicT willreferto Ω,itisnecessarytoeitheraddaregularizationpenalty,asin
tensors.WewillconceptualizeourdatasettobeatensorD TeraLasso,orrestrictΩtoalow-ranksubspace,asproposed
withaxeslengthsd ℓ foreachaxisℓ.d <ℓ andd >ℓ areshort- byAndrew,Westhead,andCutillo(2024b).Thesearedetails
hands for the product of lengths of axes before and after ℓ, oftheestimatorforΩ,whichdonotaffecttheestimatorof
respectively.d \ℓistheproductofallaxesotherthanℓ;from ω;wedonotfocusontheseaspectsoftheproblem.
the perspective of ℓ, this is the number of samples for that
a tax ti is o. nd o∀ fi Ks oth lde ap aro nd du Bct ado ef ra (l 2l 0d 0ℓ. 9W )fe orty tep nic sa ol rly opfo erll ao tiw onth s.eno-
p(D)=
(cid:112) |Ω| e− 21(x−ω)TΩ(x−ω)
d∀
Let I and 1 be the a × a identity matrix and length (2π) 2
a a
a vector of ones, respectively. We formally define the Kro- −1 1
NLL(D)∝ log|Ω|+ (x−ω)T Ω(x−ω)
neckersumofmatricesandanalogousoperationonvectors
2 2
asfollows: Despite its ubiquity, the normal distribution is actually
fairly poorly behaved from an optimization perspective; its
(cid:77) (cid:88) NLLisnotconvex,norisitevengeodesicallyconvex(Hos-
Ψ = I ⊗Ψ ⊗I
ℓ d<ℓ ℓ d>ℓ
seini and Sra 2015). Thankfully, the MLE of the mean
ℓ ℓ
(for the unrestricted normal) has a closed form solution,
(cid:77) (cid:103) ℓµ
ℓ
=(cid:88) 1
d<ℓ
⊗µ ℓ⊗1
d>ℓ
ω inp= racn1 ti(cid:80) cen
i
.Ux ni, fos ro tut nh ais ten lyo ,n dc uo env toex oi uty rKdoes ren sto rt ica tf if oe nc ,t thre issu sl ots
-
ℓ v
lutionnolongerholds.
(cid:76)
We will let Ψ \ℓ be a shorthand for ℓ′̸=ℓΨ ℓ′, with an While it is not convex in (Ω,ω) jointly, it is convex in
analogous definition for µ \ℓ. Kronecker products, and by each argument individually - i.e, it is biconvex. This sug-
extension Kroneckersums, havea convenient permutation- gests a flip-flop optimization scheme. For fixed ω, the es-
invariance property; we can swap the order of summands timation of Ω reduces to the noncentral case; several algo-
without affecting the results, as long as we perform this rithms already exist to solve this. Thus, we focus on opti-
swap consistently across an equation and permute matri- mizationwhenΩisfixed.
ces accordingly; we can typically rewrite equations involv- Optimization w.r.t. ω can be framed as a convex con-
(cid:76)
ing Ψ into those involving Ψ ⊕Ψ . We will define strainedquadraticprogramming(QP)problem.However,it
ℓ ℓ ℓ \ℓ
(cid:76) (cid:76) willturnouttobeconvenienttofurtherbreaktheproblemup
Ω = Ψ and ω = (cid:102) µ . We define noncentral Kro-
ℓ ℓ ℓ ℓ intoitsconstituentparts(m,{µ }),andminimizethemeach
neckersumnormaldistributionas: (cid:101)ℓ
individually. Let θ ℓ = 1T d ℓ′Ψ ℓ′1 d ℓ′, θ \ℓ = (cid:80) ℓ′̸=ℓ dℓd d∀ ℓ′θ ℓ,
vec[D]∼N (cid:0) ω,Ω−1(cid:1) andx ℓ bethevectorizationofDdoneintheorderasifaxis
ℓwerethefirstaxis.Topreservespace,wewilldefertheal-
WeletK bethespaceofKronecker-sum-decomposable gebraicmanipulationstotheappendix,andsimplynotehere
M
matrices3,i.e.Ω ∈ K .Likewise,ω ∈ K isthespaceof thattheoptimalvalueofm,withallothervaluesfixed,is:
M v
3Technically, we need to specify the dimensionality of each (cid:16) (cid:76) (cid:17)(cid:16) (cid:76) (cid:17)
x−(cid:102) µ (cid:102)Ψ 1
terminthedecompositiontodefinethisspace,butthiswouldbe ℓ(cid:101)ℓ ℓ dℓ
m= (1)
notationallycumbersomeandisalwaysclearfromcontext. (cid:80) d θ
ℓ \ℓ ℓSomemethods,suchasGmGM(Andrew,Westhead,and
Cutillo 2024a), require only a single eigendecomposition,
and then find the optimum while staying in ‘eigen-space’.
Theyusetheeigenvectorsofcovariancematrices;whenthe
Exact Block
Coordinate Descent mean is updated, the new covariance matrices can be ex-
pressedintermsofrank-one-updatesoftheoriginalcovari-
ancematrix.
Drop-in Solution Exact Block Rank-one updates of eigendecompositions are cheap
(TeraLasso, GmGM, etc) Coordinate Descent
(comparatively); thus, for each flip-flop of our algorithm,
if GmGM is used as the solver for Ψ , then it can
ℓ
stay entirely in eigenspace - we preserve the ‘only-one-
Exact Block Unconstrained eigendecomposition’ property of GmGM. Other methods,
Coordinate Descent Quadratic Programming such as TeraLasso, will still require multiple eigendecom-
positions,butthesewillremainusefulforourcalculationof
A−1.Itisbecauseoftheseconvenientcomputationalprop-
ℓ
ertiesthatwehavechosenaflip-floprouteforestimationof
Equality-Constrained Equality-Constrained
Quadratic Programming Quadratic Programming µ ,despitethefactthattheoptimizationproblemisjointly
(cid:101)ℓ
convexin{µ } .
(cid:101)ℓ ℓ
Figure2:Agraphicalsummaryofthemannerinwhichwe
divideourproblemintosub-problems.Whilethemainprob- Algorithm1:NoncentralKS-structuredGaussianMLE
lemisnonconvex,allsub-problemsareconvex,andbyThe-
Require: DatasetDofsized ,...,d
orem1themainproblemhasauniquesolution. (cid:76) 1 K
Require: Sub-procedure GRAPHthatestimatesΨ
ℓ
suchasTeraLasso,GmGM
(cid:16) (cid:17)
Observe that, when the elements of x are independent, Output: IdentifiableMLE m,µ (cid:101)ℓ,τ,Ψ(cid:101)ℓ
this formula expresses the average distance of the dataset #InitializeParameterswithReasonableGuesses
fro Fm or0 µ (cid:101)(a ℓf (t wer ita hcc µ (cid:101)o \u ℓn fiti xn eg df ,o ar nt dh ae ga ax ii ns- dw efis ee rrm ine ga tn hs eµ (cid:101) dℓ e) r. ivation fm or(0 ℓ) ∈← {d 11 ∀ ,.(cid:80) ..,d i K=∀ 1 }v de oc[D] i
totheappendix)wehavethefollowingQPproblem: Letx bevectorizedDwithℓasfirstaxis.
ℓ
Initialguessisthemeanalongaxisℓ:
argmin µTA µ +bTµ µ(0) ← 1 (cid:80)d\ℓ x −m(0)
(cid:101)ℓ ℓ(cid:101)ℓ ℓ (cid:101)ℓ (cid:101)ℓ d\ℓ i=1 ℓ,i
µ(cid:101)ℓ endfor
whereA ℓ =d \ℓΨ ℓ+θ \ℓI dℓ #IterateUntilConvergence
(cid:104) (cid:105) whilenotconvergeddo
b =md 1T Ψ +mθ 1 + µT Ψ 1 1 (cid:110) (cid:111) (cid:16) (cid:17)
ℓ \ℓ dℓ ℓ \ℓ dℓ (cid:101)\ℓ \ℓ d\ℓ dℓ Ψ(i+1) ←(cid:76) GRAPH D−m(i)1−(cid:76) (cid:102) µ(i)
−xT (cid:0) Ψ ⊗1 (cid:1) −xT (cid:0) 1 ⊗Ψ 1 (cid:1) ℓ ℓ ℓ′ ℓ
ℓ ℓ d\ℓ ℓ ℓ \ℓ d\ℓ whilenotconvergeddo
µT1 =0 #FindMeanParameters
(cid:101)ℓ dℓ m(i+1) ←(Equation1)
Partsofthedefinitionofb ℓcanbesimplifiedfurther,lead- forℓ∈{1,...,K}do
ing to more efficient computations, but such manipulations µ(i+1) ←(Equation2)
are notationally complicated to express; we defer them to
(cid:101)ℓ
endfor
theappendix,wherewealsoshowthatA isguaranteedto
ℓ endwhile
be invertible. QP problems with linear equality constraints
endwhile
haveclosed-formsolutions.Inparticular,wehavethat: (cid:16)(cid:110) (cid:111) (cid:17)
Map{Ψ ℓ}toidentifiableparameterization Ψ(cid:101)ℓ ,τ
1T A−1b
µ = dℓ ℓ ℓ A−11 −A−1b (2)
(cid:101)ℓ 1T A−11 ℓ dℓ ℓ ℓ
dℓ ℓ dℓ SeeAlgorithm1forapseudocodepresentationofoural-
As we have closed-form solutions to all our coordinate- gorithm. Note that the pseudocode does not take into ac-
wiseminima,blockcoordinatedescentisanaturalsolution. count potential speedups that come from sharing informa-
While typically it is advisable to avoid directly calculating tionbetweentheprecisionmatrixandmeanestimationtasks,
inversesA−1inthesolutiontoQPproblems,inversionhere such as sharing eigenvectors. We wanted our method to
ℓ
islikelytobecheap;mostsolversforΨ requirecalculating be able to be used as a ‘drop-in wrapper’ for pre-existing
ℓ
eigendecompositons of Ψ . Thus, the eigendecomposition, methods.Accordingly,wehavepresentedandimplemented
ℓ
and hence inverse, of A is readily available. This answers our methodology in its most general form. For a graphical
ℓ
Problem1. overviewofhowweoptimize,seeFigure2.GlobalOptimality positive semidefinite, we cannot guarantee the global min-
imum is unique (but all local minima remain global); typi-
The optimization procedure we consider is not convex,
cally, solvers for Kronecker-sum-structured Ω require it to
merelybiconvex.Howdoweknowifthesolutionwefindis
bepositivedefinite.
globallyoptimal?Inthissection,wewillshowthat,despite
Thestrategyusedtoprovethechainofimplicationsisto
thenon-convexity,theproblemstillhasthepropertythatall
show that, when s = 1, A and A′ correspond to the same
local minima are global minima. In fact, when identifiable
optimizationproblem.Since(Ω,ω,s) → Γisaone-to-one
representations are used, we will see that there is exactly
mapping,aswellasthefactthatthismappingpreservesthe
oneminimum.Asinthelastsection,wewilldeferthemore
objectivefunction’svalues,andfinallythefactthatK is
mechanicaldetailsoftheproofthetheappendix,andrather Mv
alinearsubspaceandhencepreservesstrictconvexityofB,
giveasketchofthemainideashere.
we have that there is exactly one minimum to B which by
Theorem 1. The maximum likelihood of the noncentral thechainofimplicationsisalsotheuniqueminimumofA.
Kronecker-sum-structurednormaldistributionhasaunique
All such claims are proven in the appendix. In particu-
maximum, which is global. The estimator defined by Algo-
lar,uniquenessoftheglobalminimumholdswhenoneuses
rithm1convergestothis.
convexregularizationpenaltiesonΩ,andfurthermoreholds
(subject to a reasonable constrant on ω) when one restricts
Proof. Seeappendix.Sketchgivenbelow.
Ωtobealow-rankmatrix,suchasthatconsideredbyavari-
antofGmGM(Andrew,Westhead,andCutillo2024b).The
The general idea is to perform a one-to-one transforma-
proofinthelattercaseisconsiderablymorecomplicated,re-
tionontheproblemtoturnitintoastrictlyconvexproblem
quiring mathematical machinery particularly from work by
inadifferentsetofparameters-specifically,aninstanceof
Hartwig(1976)andHolbrook(2018);itholdswhenwere-
conicprogrammingwithlinearconstraints.LettingAbeour
strictωtobeorthogonaltothenullspaceofΩ(areasonable
originalproblem,A′beaslightmodificationofit,andBbe
assumptionsincedatageneratedfromthedistributionshould
theconicproblem,weshowthefollowingchainofimplica-
alsobeorthogonaltothenullspace).
tions:
We claimed that our block coordinate descent algorithm
convergestothisuniqueoptimalvalue.Blockcoordinatede-
(ω,Ω)∈localminima(A) scent methods are not guaranteed to converge for smooth
nonconvexproblems.However,whenthereisauniquemin-
⇐⇒ (ω,Ω,s=1)∈localminima(A′)
imum for each block, as is the case here, block coordinate
⇐⇒ (Γ)∈localminima(B) descentisknowntoconverge(Tseng2001;Luenbergerand
⇐⇒ (Γ)∈globalminima(B) (convexity) Ye2008).
Γ is a one-to-one function of ω,Ω, and s, where s is Results
somewhatlikeaslackvariable.Weshowthatlocalminima
alwaysoccurats=1,andfurthermorethatatlocalminima,
thevalueoftheobjectivefunctionofAequalsthevalueof Figure MeanDistribution
the objective function of B for the corresponding parame-
Figure1 Constantmeanof1
ters.Thus,alllocalminimaofAmustobtainthesamevalue,
(cid:76)
i.e. they are global minima. In fact, because the transfor-
Figure3 m+(cid:102) ℓµ
ℓ
µ ∼N(0,I), m∼N(0,1)
mationisone-to-one,andBhasauniqueglobalminimum, ℓ
thereisexactlyonelocalminimumofA. Figure4(left) N(0, 21 0I)
Thetransformationweuseis: Figure4(right) 1ω 4
ω ∼Poisson(10)
ij
(cid:20) (cid:21)
x∼N (cid:0) ω,Ω−1(cid:1) → x ∼N (cid:0) 0,Γ−1(cid:1) Table 1: The distributionsof the means in each of our syn-
1
theticdataexperiments,andthecorrespondingfigures.Both
(cid:20) Ω −Ωω (cid:21) meandistributionsforFigure4haveavarianceof0.05.This
Γ= −ωTΩ 1 +ωTΩω variancehadtobesmallerfortheunstructureddistributions,
s otherwisethenoisewouldoverwhelmthesignalinallalgo-
s>0
rithms.WemakenostructuralassumptionsforFigure4;the
meanisfreetotakeanyform,i.e.itisnotKS-decomposable.
We first came across a similar transformation in a paper
by Hosseini and Sra (2015), although their transformation
was for the covariance matrix; this transform here can be Allexperimentswererunona2020MacBookProwithan
derived from theirs using block matrix inversion. The vari- M1chipand8GBofRAM.Ourmethodwasimplemented
able 1 isusedtoallowΓtobeanypositivedefinitematrix using NumPy 1.25.2 (Harris et al. 2020) and SciPy 1.12.0
s
(subjecttoconstraintsonΩ,ω).Otherwise,thevalueofthe (Virtanenetal.2020);forprecisionmatrixroutines,weused
bottomrightentrywouldbedeterminedbytheotherentries. Greenewald,Zhou,andHero’sreferenceimplementationof
Itshouldbeclearthat,aslongasΩispositivedefinite,this TeraLasso(2019)aswellasGmGM0.5.3.GmGMandTer-
transformation is one-to-one given a fixed s. If it is merely aLasso are fairly similar algorithms; GmGM outputs posi-PR Curves for Barabasi-Albert graph (100 nodes) PR Curves for Barabasi-Albert graph (100 nodes) PR Curves for Barabasi-Albert graph (100 nodes)
1.0 [+KS-structured G Gmau Gs Msian] 1.0 [+Unstructured G Ga mu Gs Msian] 1.0 [+Unstructured GP moi Gs Mson]
0.8 N T Neo orn nac cLe ean nsst tr roa al l G Tem raG LM asso 0.8 N T Neo orn nac cLe ean nsst tr roa al l G Tem raG LM asso 0.8 N T Neo orn nac cLe ean nsst tr roa al l G Tem raG LM asso
0.6 0.6 0.6
0.4 0.4
0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Recall Recall
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Recall
Figure 4: Precision and recall for on a synthetic dataset in
Figure 3: Precision and recall for on a synthetic dataset in whichdatawasgeneratedfromazero-meanKronecker-sum
whichdatawasgeneratedfromanoncentralKronecker-sum normal distribution, with independent Gaussian (left) and
normal distribution. Error bars are the best/worst perfor- Poisson(right)noiseaddedtoeveryelement.Errorbarsare
manceover10trials;thecenterlineisaverageperformance. thebest/worstperformanceover10trials;thecenterlineis
NoncentralTeraLassohadlargevarianceinitsperformance averageperformance.
duetoTeraLasso’shighsusceptibilitytochangesinthereg-
ularization parameter (changes as small as 10−8 can have
large effects on the number of edges kept). This led to our bothGaussianandPoisson-distributedrandommeans(Fig-
gridsearchbeingtoocoarse-grainedduringsometrials. ure 4) and found that there was still a drop-off in perfor-
manceforPoissonmeans,butnotnearlyasdramaticasthat
seenintheothercasesconsidered.Therewasnodrop-offin
tivedefinitematrices4anddoesnotuseregularization,while theGaussiancase.
TeraLassousesregularizationanddoesnotrequiretheout- Whenweincreasedthevarianceofourmeansinourun-
putstobepositivedefinite(aslongastheKroneckersumof structuredtests,boththestandardalgorithmsandourmean-
the outputs is). By presenting results with each algorithm, corrected algorithms dropped in performance equally. We
we hope to show that our proposed mean estimation algo- suspectthisisbecause,forlargevariances,thesignalisover-
rithmisnotspecifictoanyonemethodofprecisionestima- whelmedbythenoise.Forsmallvariances,sinceeachrow/-
tion.Allcodeisprovidedinthesupplementarymaterial. columnofthemeanisuncorrelated,theireffectsaverageto
In this section, we compare the performance of preci- zero, and thus our mean-corrected algorithms will also es-
sion matrix algorithms with and without our mean estima- timate means of zero (making them idendical to the uncor-
tionwrapperacrossavarietyoftestsuites.Wefirstcompare rected case). The Poisson distribution behaves slightly dif-
performancesonsyntheticdata,inwhichthegraphsaregen- ferently,asthemeanisinsteadaconstantnonzeronumber,
eratedfromaBarabasi-Albert(power-law)distribution.We andhenceourmean-correctedalgorithmisabletocorrectly
generatemeansforoursyntheticdatafromavarietyofdis- accountforthis,whereasthestandardalgorithmsexperience
tributions; see Table 1. We also compare them on Erdos- degradation in performance similar to that of the constant-
Renyi graphs, achieving similar results, but for concise- mean offset test (Figure 1), albeit to a lesser degree as the
nesswedeferthoseresultstotheappendix.Next,wecom- offsetissmaller.
pareperformancesonthereal-worldCOIL-20videodataset The unstructured mean experiment has important conse-
(Nene, Nayar, and Murase, 1996), demonstrating clear im- quences: if real-world data has an unstructured mean, then
provements.Finally,weshowthat,withoutproperlytaking it is less important to correct for the means! However, it is
intoaccountthemean,precisionmatrixestimationwillyield likely that the mean does have a structured component, as
clearlywrongresultsontranscriptomicsdatasetssuchasthe each axis will have its own latent factors affecting the out-
mouseembryostemcell‘E-MTAB-2805’dataset(Buettner comeinadditiontoanyunstructurednoisethatmayormay
etal.2015). notexist.Todemonstratethis,werunouralgorithmontwo
Forsyntheticdata,wealreadysawthedramaticdeteriora- real-worlddatasets,showingthatourmeancorrectiontruly
tioninperformanceofalgorithmsthatmakethezero-mean doesimproveperformanceinpractice.
assumption in Figure 1, while mean-corrected algorithms The COIL-20 dataset consists of 20 videos (each with
still perform well. That alone should produce cause for 72 frames) of objects rotating 360 degrees; it has become
worry.Zero-meanalgorithmsalsoperformpoorlywhenwe somewhatcustomaryformulti-axismethodstopresenttheir
generatedatafromthenoncentralKronecker-sum-structured performance on the rubber duck object of this dataset. The
Gaussiandistribution;understandably,ourcorrectionisim- datasetischosenbecauseofitseasetovalidate:framesthat
munetothisproblem(Figure3). are close in time (i.e. adjacent) should be connected in the
Finally, we may wonder what happens if our mean has graph.WepresentourresultsgraphicallyinFigure5,from
absolutelynostructure,witheveryelementofourgenerated whichwecaneasilyseetheimprovement.Wepre-processed
matrixhavingadifferent,independentmean.Wecompared the data using the nonparanormal skeptic (Liu et al. 2012)
to relax the Gaussian assumption of the models, and we
4Semi-definiteinAndrew,Westhead,andCutillo(2024b). chosethresholding/regularizationparameterssuchthatthere
noisicerP
noisicerP noisicerPGmGM Noncentral GmGM TeraLasso Noncentral TeraLasso Assortativity of Assortativity of
GmGM cell graph noncentral GmGM cell graph
1.00 1.00
GmGM Noncentral GmGM
0.75 Random 0.75 Random
0.50 Min assortativity 0.50 Max assortativity
0.25 0.25 Max assortativity: 0.16
0.00 0.00
−0.25 −0.25
−0.50 Min assortativity: -0.47 −0.50
−0.75 −0.75
Figure 5: A comparison of the results on the COIL-20 −1.000% 1% 10% 100% −1.000% 1% 10% 100%
Percentage of edges kept (log scale) Percentage of edges kept (log scale)
dataset’s frames. Each circle represents the performance of
a different algorithm; the prefix ‘noncentral’ denotes that Figure 6: Assortativity of standard GmGM and our mean-
thealgorithmusesourmeanestimationmethod.Theimages correctedGmGMaswevarythenumberofedgeskept,on
correspond to frames of the video (a duck rotating 360 de- theE-MTAB-2805cell-cycledataset.Thecellswerelabeled
grees),arrangedinorder.Dashedbluelinesindicateacon- bytheirstageinthecellcycle(G1/S/G2M).
nectionbetweennon-adjacentframes;solidblacklinesindi-
cateadjacentconnections.
runscontrarytointuition:cellswithsomefactorincommon
should not be caused by that same factor to look different.
wouldbeapproximately144edges.
Whencorrectingforthemeans,weinsteadseeamodestpos-
Weconsidered‘correct’edgestobethoseconnectingad-
itivetendency(assortativity=0.16),whichismorereason-
jacent frames or frames with a single frame in between.
able.Figure6showsthechangeinassortativityforGmGM,
GmGMhad59/147(40%)correctandTeraLassohad55/142
withandwithoutourcorrection,aswevarythethresholdof
(39%) correct. Once we wrap the methods in our mean es-
how many edges to keep. This shows that our findings are
timationprocedure,thisroseto127/147(86%)and126/141
notdependentonourchoiceofpenalty,butratherareacon-
(89%), respectively. Furthermore, all ‘wrong’ connections
sistentfeatureoftheproblem.Wedidnotruntheexperiment
of our noncentral algorithms were nearly correct: with one
withTeraLassoduetoitsslowruntime,buttheresultsforit
exception, every connection was between frames with at
wouldlikelybebroadlysimilar(astheyhavebeenintherest
mosttwoframesbetweenthem.Forthestandardalgorithms,
ofthispaper).
more than a third of all edges failed this criteria. The non-
centralalgorithmsalsohadhighrecall,at88%forbothnon-
centralGmGMandTeraLasso.Theuncorrectedalgorithms
Conclusion
had a recall of 41% and 38%. Overall, the results for this
experimentarefairlyconclusivelyinfavorofusingourcor-
Current Kronecker-structured models do not take into ac-
rection.
count the mean of the data. As one might expect, this can
OurfinalexperimentisontheE-MTAB-2805scRNA-seq
lead to wildly incorrect inferences, as best exemplified in
dataset. This dataset consists of 288 mouse embryo stem
Figures1and6.Asshownintheseexamples,failuretotake
cellsand34,573uniquegenes,witheachcellbeinglabeled
into account the mean not only results in incorrect infer-
by its stage in the cell cycle (G1, S, or G2M). It has pre-
ences, but in fact it can result in inferences directly oppo-
viously been considered by the creators of scBiGLasso (Li
sitetoreality:asthresholding/regularizationincreased,algo-
et al. 2022), in which they limited it to a hand-selected set
rithmsthatdidnottakeintoaccountthemeandecreasedin
of 167 mitosis-related genes - we limited it to the same set
bothprecisionandrecall,andidentifiedanextremelystrong
of genes. We would expect that cells at the same stage in
(and incorrect) repulsive force between cells in the same
the same cycle should have some similarities, and hence
stageofthecellcycle.
shouldhavesometendencytoclustertogetherinourlearned
graphs. This tendency may be weak - scRNA-seq data is Inthispaper,wehavedemonstratedthatthiscanbefixed
very noisy, and many other biological signals are compet- by adding a mean-estimating wrapper around standard al-
ingforinfluenceonacell’sexpressionprofile-butitshould gorithms. While the means and precisions are nonsepera-
exist. ble in the KS-structured case, and hence coincide with a
We can use ‘assortativity’ as a measure for the tendency nonconvex optimization problem, we proved that there is a
forcellswithinastagetoconnect:assortativityrangesfrom unique global optimum, to which our algorithm converges.
-1to1,andcorrespondstoaPearsoncorrelationcoefficient We implemented our wrapper as a ‘drop-in’ wrapper (so
of‘within-stage’degreesofadjacentnodes.Whenitisposi- that it can easily be used with any ‘black box’ standard
tive,itindicatesthestrengthofthetendencyofnodesinthe precision-estimatingalgorithm),butwealsopointtoaspects
samecellcyclestagetoconnecttoeachother.Ifitisnega- ofourmethodthatmakeiteasiertomoretightlyintegrateit
tive,cellsindifferingcellcyclestagestendtobeconnected intoprecision-estimatingframeworks.Inparticular,itcanbe
toeachother.Ifzero,thereisnotendency;theconnections made to preserve the ‘only-one-eigendecomposition’ prop-
areeffectivelyrandom. ertyofGmGM,whichishighlybeneficialforscalability.
When using standard algorithms, without correcting for As future work, we aim to use this model to better un-
the mean, we find that, not only is there not a tendency for derstandgeneregulatorynetworksandthecellularmicroen-
cellstoconnecttosame-stagecells,butthereisaverystrong vironment in single-cell omics data. We would also like to
tendencyforthemnottoconnect(assortativity=-0.47).This generalizethemethodtothemulti-modalcase,asinGmGM.
ytivitatrossA )M2G
sv
S
sv
1G(
ytivitatrossA )M2G
sv
S
sv
1G(References Kolda, T. G.; and Bader, B. W. 2009. Tensor Decomposi-
Allen,G.I.;andTibshirani,R.2010. Transposableregular- tionsandApplications. SIAMReview,51(3):455–500. Pub-
izedcovariancemodelswithanapplicationtomissingdata lisher:SocietyforIndustrialandAppliedMathematics.
imputation. Theannalsofappliedstatistics,4(2):764–790. Li, S.; Lo´pez-Garc´ıa, M.; Lawrence, N. D.; and Cutillo, L.
Andrew, B.; Westhead, D. R.; and Cutillo, L. 2024a. 2022. Scalable Bigraphical Lasso: Two-way Sparse Net-
GmGM:afastmulti-axisGaussiangraphicalmodel.Confer- workInferenceforCountData.ArXiv:2203.07912[cs,stat].
enceName:The27thInternationalConferenceonArtificial Liu,H.;Han,F.;Yuan,M.;Lafferty,J.;andWasserman,L.
IntelligenceandStatisticsISSN:2640-3498MeetingName: 2012. ThenonparanormalSKEPTIC. InProceedingsofthe
The27thInternationalConferenceonArtificialIntelligence 29thInternationalConferenceonMachineLearning,ICML
andStatisticsPlace:Vale`ncia,SpainPublisher:Proceedings 2012,1415–1422.
ofMachineLearningResearch. Luenberger,D.G.;andYe,Y.2008. LinearandNonlinear
Andrew,B.;Westhead,D.R.;andCutillo,L.2024b. Mak- Programming,volume116ofInternationalSeriesinOper-
ingMulti-AxisGaussianGraphicalModelsScalabletoMil- ations Research & Management Science. New York, NY:
lions of Samples and Features. ArXiv:2407.19892 [cs, q- Springer US. ISBN 978-0-387-74502-2 978-0-387-74503-
bio,stat]. 9.
Buettner, F.; Natarajan, K. N.; Casale, F. P.; Proserpio, V.; McInnes,L.;Healy,J.;andMelville,J.2020. UMAP:Uni-
Scialdone,A.;Theis,F.J.;Teichmann,S.A.;Marioni,J.C.; form Manifold Approximation and Projection for Dimen-
and Stegle, O. 2015. Computational analysis of cell-to- sionReduction. ArXiv:1802.03426[cs,stat].
cell heterogeneity in single-cell RNA-sequencing data re- Nene, S. A.; Nayar, S. K.; and Murase, H. ???? Columbia
vealshiddensubpopulationsofcells. NatureBiotechnology, ObjectImageLibrary(COIL-20).
33(2): 155–160. Number: 2 Publisher: Nature Publishing
Tseng,P.2001.ConvergenceofaBlockCoordinateDescent
Group.
MethodforNondifferentiableMinimization. JournalofOp-
Dutilleul,P.1999. Themlealgorithmforthematrixnormal timizationTheoryandApplications,109(3):475–494.
distribution. Journal of Statistical Computation and Simu-
Virtanen, P.; Gommers, R.; Oliphant, T. E.; Haberland,
lation,64(2):105–123. Publisher:Taylor&Francis eprint:
M.; Reddy, T.; Cournapeau, D.; Burovski, E.; Peterson, P.;
https://doi.org/10.1080/00949659908811970.
Weckesser, W.; Bright, J.; van der Walt, S. J.; Brett, M.;
Friedman, J.; Hastie, T.; and Tibshirani, R. 2008. Sparse
Wilson, J.; Millman, K. J.; Mayorov, N.; Nelson, A. R. J.;
inversecovarianceestimationwiththegraphicallasso. Bio-
Jones,E.;Kern,R.;Larson,E.;Carey,C.J.;Polat,I.;Feng,
statistics,9(3):432–441.
Y.; Moore, E. W.; VanderPlas, J.; Laxalde, D.; Perktold, J.;
Greenewald, K.; Zhou, S.; and Hero, A. 2019. Tensor Cimrman, R.; Henriksen, I.; Quintero, E. A.; Harris, C. R.;
graphical lasso (TeraLasso). Journal of the Royal Statisti- Archibald, A. M.; Ribeiro, A. H.; Pedregosa, F.; and van
calSociety.SeriesB(StatisticalMethodology),81(5):901– Mulbregt, P. 2020. SciPy 1.0: fundamental algorithms for
931.Publisher:[RoyalStatisticalSociety,OxfordUniversity scientific computing in Python. Nature Methods, 17(3):
Press]. 261–272. Number:3Publisher:NaturePublishingGroup.
Harris,C.R.;Millman,K.J.;vanderWalt,S.J.;Gommers, Wang, Y.; Jang, B.; and Hero, A. 2020. The Sylvester
R.;Virtanen,P.;Cournapeau,D.;Wieser,E.;Taylor,J.;Berg, Graphical Lasso (SyGlasso). In Proceedings of the Twenty
S.; Smith, N. J.; Kern, R.; Picus, M.; Hoyer, S.; van Kerk- Third International Conference on Artificial Intelligence
wijk, M. H.; Brett, M.; Haldane, A.; del R´ıo, J. F.; Wiebe, andStatistics,1943–1953.PMLR. ISSN:2640-3498.
M.;Peterson,P.;Ge´rard-Marchant,P.;Sheppard,K.;Reddy,
T.; Weckesser, W.; Abbasi, H.; Gohlke, C.; and Oliphant,
DeferredProofs
T. E. 2020. Array programming with NumPy. Nature,
585(7825):357–362. Number:7825Publisher:NaturePub- In this portion of the appendix, we will formally prove the
lishingGroup. claimspresentedinthepaper,namelywewillprovethatthe
locations of coordinate-wise minima reported in the paper
Hartwig,R.E.1976. SingularValueDecompositionandthe
areindeedaccurate,andfurthermorethatthefulloptimiza-
Moore–PenroseInverseofBorderedMatrices. SIAMJour-
tionproblemhasauniquesolution.
nalonAppliedMathematics,31(1):31–41. Publisher:Soci-
etyforIndustrialandAppliedMathematics.
DerivationofGradients
Holbrook,A.2018. Differentiatingthepseudodeterminant.
First,wewillderivegradientswithrespecttoallparameters.
LinearAlgebraanditsApplications,548:293–304.
Recall that the negative log-likelihood is, up to an additive
Hosseini,R.;andSra,S.2015. MatrixManifoldOptimiza-
constant:
tion for Gaussian Mixtures. In Advances in Neural Infor-
mation Processing Systems, volume 28. Curran Associates,
−1 1
Inc. NLL(D)∝ log|Ω|+ (x−ω)T Ω(x−ω)
Kalaitzis, A.; Lafferty, J.; Lawrence, N. D.; and Zhou, S. 2 2
2013.TheBigraphicalLasso.InProceedingsofthe30thIn- Wedonotneedtoworkoutthegradientw.r.t.Ψ ,aswe
ℓ
ternational Conference on Machine Learning, 1229–1237. deferthatoptimizationtasktoalgorithmssuchasTeraLasso.
PMLR. ISSN:1938-7228. Thus,weareleftwiththefollowingquadraticfunction:Proof.
1 (cid:32) (cid:33) (cid:32) (cid:33)
NLL(D)∝ 2(x−ω)T Ω(x−ω) 1T
d∀
(cid:77) M
ℓ
1
d∀
=1T
d∀
(cid:88) I
d<ℓ
⊗M ℓ⊗I
d>ℓ
1
d∀
ℓ ℓ
We will differentiate with respect to the identifiable pa-
rametersµ (cid:101)ℓ,m.Generically,foranyparameterp,thederiva- =(cid:88)
1T 1 ⊗1T M 1 ⊗1T 1
tivewilllooklike: d<ℓ d<ℓ dℓ ℓ dℓ d>ℓ d>ℓ
ℓ
(cid:88)
∂
NLL(D)=−(x−ω)T
Ω(cid:18) ∂ ω(cid:19) = d <ℓd >ℓ1T dℓM ℓ1 dℓ
ℓ
∂p ∂p
(cid:88)
= d 1T M 1
Obervethat:
\ℓ dℓ ℓ dℓ
ℓ
(cid:18) (cid:19)
∂ ∂ (cid:77)
∂mω = ∂m m1 d∀ +(cid:103) ℓµ (cid:101)ℓ Recallthat1T dℓΨ ℓ1 dℓ d =ef θ ℓ.Atzero,theequalitycanbe
reshapedintothefollowing:
=1
d∀
∂∂ µ (cid:101)ℓω = ∂∂ µ (cid:101)ℓ(cid:77) (cid:103) ℓµ (cid:101)ℓ m(cid:32) (cid:79) 1T dℓ(cid:33) (cid:77) (cid:103) ℓΨ ℓ1
ℓ
=(cid:18) x−(cid:77) (cid:103) ℓµ (cid:101)ℓ(cid:19)T (cid:77) (cid:103) ℓΨ ℓ1
ℓ
∂ (cid:16) (cid:17) ℓ
=
∂µ (cid:101)ℓ
µ (cid:101)ℓ⊕(cid:101)µ
(cid:101)\ℓ
(uptopermutation)
=⇒ m(cid:88) d θ =(cid:18) x−(cid:77) (cid:103) µ (cid:19)T (cid:77) (cid:103) Ψ 1
∂ (cid:16) (cid:17) \ℓ ℓ ℓ(cid:101)ℓ ℓ ℓ ℓ
= µ ⊗1 +1 ⊗µ ℓ
∂µ (cid:101)ℓ (cid:101)ℓ d\ℓ dℓ (cid:101)\ℓ (Lemma1)
=I dℓ ⊗1 d\ℓ (cid:16) x−(cid:76)
(cid:102) µ
(cid:17)T (cid:76)
(cid:102) Ψ 1
ℓ(cid:101)ℓ ℓ ℓ ℓ
The latter derivation is also obvious from our ability to =⇒ m= (cid:80)
(cid:0) (cid:1) d θ
rewrite µ ⊗1 as I ⊗1 µ . We gave the deriva- ℓ \ℓ ℓ
(cid:101)ℓ d\ℓ dℓ d\ℓ (cid:101)ℓ
tive up to permutation, for notational simplicity (since we
DerivationoftheCoordinate-wiseMinimaofµ
areonlyinterestedinwhenitequalszero).Withouttheper- (cid:101)ℓ
mutation, it is easy to see that it is 1 ⊗I ⊗1 . Fi- Rather than considering the gradients, we will frame this
nally, note that the latter derivative wd< ouℓ ld bedℓ analod g> oℓ us if problemasanequality-constrainedQPproblem,whichhas
donew.r.t.theunconstrainedparametersµ . closed-formminima.Here,wewillletx ℓ beourvectorized
ℓ
dataset, vectorized in an order such that axis ℓ was consid-
DerivationoftheCoordinate-wiseMinimumofm ered the first axis. We’ll further write x (cid:101)ℓ = x ℓ − m1 d∀
(cid:76)
to keep our equations compact, and represent (cid:102) µ as
To derive the coordinate-wise optimum of m, we will find ℓ′(cid:101)ℓ′
whenthegradientiszero. µ (cid:101)ℓ⊕(cid:101)µ (cid:101)\ℓ.
∂
1(cid:18)
(cid:77)
(cid:19)T (cid:18)
(cid:77)
(cid:19)
∂mNLL(D)=−(x−ω)T (cid:32)Ω1 d∀
(cid:33)(cid:32) (cid:33)
arg µ(cid:101)m ℓin 2 x (cid:101)ℓ−(cid:103) ℓ′µ (cid:101)ℓ′ Ω x (cid:101)ℓ−(cid:103) ℓ′µ (cid:101)ℓ′
=−(x−ω)T (cid:77) Ψ
ℓ
(cid:79) 1
dℓ
where µ (cid:101)T ℓ1 dℓ =0
ℓ ℓ Wethenexpandtheequation,removingtermsthatdonot
=−(x−ω)T (cid:18) (cid:77) (cid:103) Ψ 1 (cid:19) involveµ (cid:101)ℓandrecallingthatuTMv=vTMu.
ℓ ℓ
ℓ
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
(cid:18) (cid:77) (cid:19)T (cid:18) (cid:77) (cid:19) argmin 1 (cid:77) (cid:103) µT Ω (cid:77) (cid:103) µ −xTΩ (cid:77) (cid:103) µ
= m1 d∀ −x+(cid:103) ℓµ (cid:101)ℓ (cid:103) ℓΨ ℓ1 ℓ µ(cid:101)ℓ 2 ℓ′(cid:101)ℓ′ ℓ′(cid:101)ℓ′ (cid:101)ℓ ℓ′(cid:101)ℓ′
where µT1 =0
(cid:101)ℓ dℓ
(cid:76)
Beforeproceeding,let’sproveahelpfullemma: Again, we split (cid:102) ℓ′µ
(cid:101)ℓ′
into µ (cid:101)ℓ⊕(cid:101)µ
(cid:101)\ℓ
and remove terms
Lemma1. Foranysetofd ℓ×d ℓmatricesM ℓ: notcontainingµ (cid:101)ℓ.

1T d∀(cid:32) (cid:77)
ℓ
M ℓ(cid:33) 1 d∀ =(cid:88)
ℓ
d \ℓ1T dℓM ℓ1 dℓ arg µ(cid:101)m ℓin  +(cid:16)1 2 1 d(cid:0) ℓµ (cid:101) ⊗ℓ⊗ µ (cid:101)\1 ℓd −\ℓ(cid:1) x (cid:101)T ℓ(cid:17)Ω T(cid:0) Ωµ (cid:101)ℓ (cid:0)⊗ µ (cid:101)ℓ1 ⊗d\ 1ℓ(cid:1) d\ℓ(cid:1)where µT1 =0
(cid:101)ℓ dℓ
Finally,recallthatµ (cid:101)ℓ⊗1 d\ℓ =(cid:0) I dℓ ⊗1 \ℓ(cid:1) µ (cid:101)ℓ.Let’sde- A ℓ=(cid:0) I dℓ ⊗1 d\ℓ(cid:1)T Ω(cid:0) I dℓ ⊗1 d\ℓ(cid:1)
finethefollowingconstants: (cid:0) (cid:1)T(cid:0) (cid:1)(cid:0) (cid:1)
= I ⊗1 Ψ ⊗I +I ⊗Ψ I ⊗1
dℓ d\ℓ ℓ d\ℓ dℓ \ℓ dℓ d\ℓ
(cid:16) (cid:17)
(cid:0) (cid:1)T (cid:0) (cid:1) = Ψ ⊗1T 1 +I ⊗1T Ψ 1
A ℓ = I dℓ ⊗1 d\ℓ Ω I dℓ ⊗1 d\ℓ (3) ℓ d\ℓ d\ℓ dℓ d\ℓ \ℓ d\ℓ
b ℓ =(cid:16) 1 dℓ ⊗µ (cid:101)\ℓ−x (cid:101)ℓ(cid:17)T Ω(cid:0) I dℓ ⊗1 d\ℓ(cid:1) (4) =d \ℓΨ ℓ+(cid:88) d ℓd d∀ ℓ′θ ℓI dℓ (Lemma1)
ℓ′̸=ℓ
Note that this definition is different from the one in the
=d Ψ +θ I
main paper; in the next section we will show that they are
\ℓ ℓ \ℓ dℓ
equivalent. With these constants, we can rewrite our mini-
mizationproblemas:
(cid:16) (cid:17)T (cid:0) (cid:1)
b = 1 ⊗µ −x Ω I ⊗1
ℓ dℓ (cid:101)\ℓ (cid:101)ℓ dℓ d\ℓ
arg µ(cid:101)m ℓin 1 2µ (cid:101)T ℓA ℓµ (cid:101)ℓ+bT ℓµ (cid:101)ℓ =(cid:16) 1 dℓ ⊗µ (cid:101)\ℓ−x (cid:101)ℓ(cid:17)T (cid:0) Ψ ℓ⊗1 d\ℓ +I dℓ ⊗Ψ \ℓ1 d\ℓ(cid:1)
where µ (cid:101)T ℓ1 dℓ =0 =1T dℓΨ ℓ⊗µ (cid:101)T \ℓ1 d\ℓ +1 dℓ ⊗µ (cid:101)T d\ℓΨ \ℓ1 d\ℓ
This is an equality-constrained QP problem; these have −x (cid:101)T ℓ (cid:0) Ψ ℓ⊗1 d\ℓ +I dℓ ⊗Ψ \ℓ1 d\ℓ(cid:1)
closed-formsolutions.LettingλbetheLagrangemultiplier
thatappearswhenconsideringtheconstraint,weknowthat Here, we can observe that µ (cid:101)T \ℓ1 d\ℓ = 0 due to our con-
thesolutionisthesolutiontoalinearequation: straints.Thus,weget:
(cid:20) 1A Tℓ 1 0dℓ(cid:21)(cid:20) µ (cid:101) λℓ(cid:21) =(cid:20) − 0b ℓ(cid:21) b ℓ =(cid:104) 1T d\ℓΨ ℓ1 d\ℓ(cid:105) 1 dℓ−x (cid:101)T ℓ (cid:0) Ψ ℓ⊗1 d\ℓ+I dℓ ⊗Ψ \ℓ1 d\ℓ(cid:1)
dℓ
Wethenarriveatthesolutionwithsomesimplealgebra.
Expandingoutx intox −m1 yieldsthedefinitionof
(cid:101)ℓ ℓ d∀
b giveninthemainpaper.
ℓ
A µ +1 λ=−b
ℓ(cid:101)ℓ dℓ ℓ
=⇒ 1T dℓµ (cid:101)ℓ+1T dℓA− ℓ11 dℓλ=−1T dℓA− ℓ1b ℓ m1T d∀(cid:0) Ψ ℓ⊗1
d\ℓ
+I
dℓ
⊗Ψ \ℓ1 d\ℓ(cid:1)
=⇒ λ= − 11 TT d AℓA −− ℓ 111b ℓ =md \ℓ1T dℓΨ ℓ+mθ \ℓ1 d\ℓ
dℓ ℓ dℓ Unfortunately, this still involves a large multiplication;
=⇒ A µ =
1T dℓA− ℓ1b ℓ
1 −b
canweexpressitinasimplerway?
ℓ(cid:101)ℓ 1T A−11 dℓ ℓ We can, but it involves introducing the concept of a ma-
dℓ ℓ dℓ tricization of a tensor: for a d ×...×d tensor T, the ℓ-
1 K
Whichleadsustothefollowingclosed-formsolution: matricizationofit,mat [T]isad ×d matrixconstructed
ℓ ℓ \ℓ
bystackingallaxestogetherexceptℓ.Thisallowsusamore
def
µ = 1T dℓA− ℓ1b ℓ A−11 −A−1b for Wm eal cd ae nfi tn hi et nio un to ilf izx eℓ a:x wℓ el= l-kv ne oc wm nca ot ℓ nv[D en] i. entrelationbe-
(cid:101)ℓ 1T dℓA− ℓ11
dℓ
ℓ dℓ ℓ ℓ
tweenKroneckerproductsandvectorization5:
DerivationofA andb
ℓ ℓ
WhilethederivationofA andb givenintheprevioussec-
vec[M]T (U⊗V)=vec(cid:2) UTMV(cid:3)T
ℓ ℓ
tionyieldsaformulaforthecoordinate-wiseminimaofµ ,
(cid:101)ℓ NotethatΨ issymmetric,sothetransposeisirrelevant.
calculatingA andb usingEquations3and4requiresmul- ℓ
ℓ ℓ Thisallowsustosimplify.
tiplyingtogetherverylargematrices(Ωisd ×d ).Inthis
∀ ∀
section, we will produce a simpler formula involving only
d ℓ W× ed wℓm illat mri ax km ea un si ep ou fla tt hio en fs a.
ct that Ω has a lower dimen-
vecmat[D]T (cid:0) Ψ ℓ⊗1 d\ℓ +I dℓ ⊗Ψ \ℓ1 d\ℓ(cid:1)
(cid:2) (cid:3)T (cid:2) (cid:3)T
sional parameterization in terms of Ψ . We could instead =vec Ψ mat [D]1 +vec mat [D]Ψ 1
ℓ ℓ ℓ d\ℓ ℓ \ℓ d\ℓ
have chosen to parameterize it in terms of the identifiable
(cid:16) (cid:17) 5Therearetwoconventionsforvec,rows-firstorcolumns-first;
parameterization Ψ(cid:101)ℓ,τ ,butthereisnotmuchgainindo-
thatconventionaffectstheorderofmultiplicationpresentedhere.
ing so; it would merely make the formulas presented here Weusetherows-firstconventionasthatalignswiththeconvention
morecomplicated. usedformat.As a final step for simplification, we recall another rela- =wTΩw w̸=0
tion from Kolda and Bader (2009). First, let D × ℓ M rep- >0 (positive-definitenessofΩ)
resent multiplying a tensor D along its ℓth axis by a ma-
Infact,thisargumentshowsthatA isnotmerelyinvert-
trix M. Note that for matrices, N × M = MTN and ℓ
0 ible,butpositivedefinite!
N× M = NM. Intuitively, it corresponds to batch ma-
1 WhatifΩismerelypositivesemidefinite?Notethatour
trixmultiplicationwhenthetensorhasmorethantwoaxes.
NLLcontainsa−log|Ω|term,whichdivergestoinfinityas
Ωapproachessemidefiniteness.Sinceitisknownthatopti-
(cid:34) (cid:35)
(cid:79) × mizersforΩconverge(seeforexampleTheorem6inthepa-
mat [T] M =mat T M
ℓ ℓ′ ℓ ℓ′ perthatintroducedTeraLasso(Greenewald,Zhou,andHero
ℓ′̸=ℓ ℓ′̸=ℓ 2019)),thiscannothappen:Ωmustbepositivedefinite.
Someworkhasbeendoneinvestigatingthecasewherewe
Thisallowsustorewriteourexpressionas:
forceΩtoberestrictedtothesetofrank-kmatrices,suchas
in the preprint by Andrew, Westhead, and Cutillo (2024b).
(cid:2) (cid:3)T (cid:2) (cid:3)T
vec Ψ ℓmat ℓ[D]1 d\ℓ +vec mat ℓ[D]Ψ \ℓ1 d\ℓ The log-determinant becomes a log-pseudodeterminant. In
=vec(cid:2) Ψ mat [D]1 (cid:3)T this case, A ℓ is indeed not invertible; the QP problem has
ℓ ℓ d\ℓ multiple solutions. However, in such work the nullspace
(cid:88) (cid:2) (cid:3)T of Ω is known6: we can add linear constraints to our
+ vec mat [D× Ψ ]1
ℓ ℓ′ ℓ′ d\ℓ
QP problem to force our domain to be orthogonal to
ℓ′̸=ℓ
the nullspace, recovering uniqueness-of-coordinate-wise-
These always result in vectors, so we can drop the vec solutionandhenceguaranteeingthatthecoordinatedescent
prefix. ofAlgorithm1converges.
(cid:88) ProofofGlobalOptimality
Ψ mat [D]1 + mat [D× Ψ ]1
ℓ ℓ d\ℓ ℓ ℓ′ ℓ′ d\ℓ Inthissection,wewillproveTheorem1inthemainpaper.
ℓ′̸=ℓ
Theorem 1. The maximum likelihood of the noncentral
This looks complicated, but has a fairly straightforward Kronecker-sum-structurednormaldistributionhasaunique
interpretation. The Ψ ℓmat ℓ[D]1 d\ℓ term corresponds to maximum, which is global. The estimator defined by Algo-
summing over all axes of D except ℓ, and then transform- rithm1convergestothis.
ingthesumbyΨ ℓ.Eachmat ℓ[D× ℓ′ Ψ ℓ′]1 d\ℓ termcanbe ThefactthatAlgorithm1convergesfollowsfromthefact
interpretedasabatchmatrixmultiplicationalongaxisℓ′,be- thatcoordinatedescentconvergestostationarypointsifthe
foresummingoverallaxesbutℓ.Ofcourse,inpracticeitis coordinate-wiseminimaareunique,whichweknowistrue
moreefficienttoperformsomesumsfirstbeforethemulti- giventhatourestimatorsforµ ,m,andΩareunique(esti-
ℓ
plication, as given below, but it causes rather cumbersome mating Ω is not considered here, but it is a strictly convex
notation. optimizationproblemsowhateverdrop-in-algorithmweuse
for it should give a unique answer, potentially differing in
(cid:34)(cid:32) (cid:33) (cid:35) thepreciseparameterizationofΩbutnotΩitself).
(cid:88) mat D × 1 × Ψ 1 To prove that the negative log-likelihood has a unique
ℓ \ℓ′′ ℓ′ ℓ′ \ℓ′
ℓ′̸=ℓ ℓ̸=ℓ′′̸=ℓ′ minimum,weintendtoprovethefollowingchainofimpli-
cations:
Tosummarize,wehavethat:
(ω,Ω)∈localminima(A)
(cid:104) (cid:105)
b ℓ = 1T d\ℓΨ ℓ1 d\ℓ 1 dℓ +md \ℓ1T dℓΨ ℓ+mθ \ℓ1 d\ℓ ⇐ 1⇒ (ω,Ω,s=1)∈localminima(A′)
(cid:88)
−Ψ mat [D]1 − mat [D× Ψ ]1 ⇐⇒ (Γ)∈localminima(B)
ℓ ℓ d\ℓ ℓ ℓ′ ℓ′ d\ℓ
2
ℓ′̸=ℓ
⇐⇒ (Γ)∈globalminima(B)
3
InvertibilityofA
ℓ WewillfirstfocusonthecasewhereΩisestimatedwith
Recallthat,fromtheprevioussection,A ℓcanbewrittenas: regularizers, rather than the low-rank subspace restriction.
Letting ρ be some convex regularizer for the precision ma-
(cid:80)
(cid:0) (cid:1)T (cid:0) (cid:1) trix(typicallythegraphicallasso,ρ(Ω)= ρ ∥Ψ ∥ ),
A ℓ = I dℓ ⊗1 d\ℓ Ω I dℓ ⊗1 d\ℓ wedefineourproblemsas: ℓ ℓ ℓ 1,od
Ω is a precision matrix, i.e. positive definite. Further-
ℓ
m veo cr te o, r(cid:0) (I itd iℓ s⊗ a1 md a\ tℓ r(cid:1) ixn te hv ae tr ‘m coa pp ys -an no dn -z pe ar so tev se ’c tt ho ers into put th ve ez ce tr oo
r
 ar ωg ,m Ωin −log|Ω|+(x−ω)TΩ(x−ω)+ρ(Ω)
d \ℓtimes). A= where Ω∈ ωK ∈M K⪰0
v
vTA ℓv=vT (cid:0) I dℓ ⊗1 d\ℓ(cid:1)T Ω(cid:0) I dℓ ⊗1 d\ℓ(cid:1) v v̸=0 6TheworkfindsanexactsolutionfortheeigenvectorsofΨ ℓ. a wωrg h,Ωm
e,
rsi en −log|Γ|+ Ω(cid:2) x ∈T
K
M1(cid:3) Γ ⪰(cid:20) 0x 1(cid:21) +ρ(Ω) wlf oo enrm
g re
eu
r
cl oa
o
vnn eeo
r-t
ωolo
-
.n
o
Ug ne
e
nr
, ds
eh
i
ro nl tcd hes eita lon
i
wsd
th
rth
ar
noe kuA
g
ah′ ssi→
un mve
pB
r tt ii
obm
nil
,a itp wyp eoin
f
cg
aΩ
nis
t
ahn dao
dt
A′ = ω ∈K thefollowingconstrainttoAandA′torecoveruniqueness.
v

Γ=(cid:20)
−ωΩ
TΩs>0
1
+− ωΩ Tω
Ωω(cid:21)
ω ⊥nullspace[Ω]
s Theconstraintsworkbyenforcingω tobeorthogonalto
 (cid:20) (cid:21)
arg Γmin −log|Γ|+(cid:2) xT 1(cid:3) Γ x
1
+ρ(Γ 1:d∀,1:d∀) t ah ne yn cu ol mls pp oac ne eno tf oΩ f. ωN to ht ae tt ih sa nt oth ti os rh tha os gn oo nae lff te oct tho en nB ul, lss pin ac ce
e
B= where ΓΓ 1:1 d: ∀d∀ ,d, ∀1: +d∀
1
∈∈ KK MM
v
g ne et es dm eda fp op re id mt po lz ice ar to i. oT nh (i 2s ),p are ns de tr hv ees ct oh ne vo exn ie t- yto n- eo en de edm fa op rp ii mng
-
Γ⪰0 plication(3).
Recall that K is a linear subspace, so B corresponds Whatneedstobere-provenforimplication(1)toholdis
Mv
thefollowing:
toaconicprogramwithlinearconstraints.Theobjectiveis
strictlyconvex.Thus,Bhasauniquelocalminimum,which 1. LocalminimaofA′occuronlyats=1
isalsotheglobalminimum.Thissatisfiesimplication(3). 2. Ats=1,AandA′havethesamesolutions.
Next, note that there is a one-to-one correspondence be-
Unfortunately, pseudodeterminants are much harder to
tween (ω,Ω,s) and Γ. Ω = Γ , and since Ω is
1:d∀,1:d∀
work with than typical determinants. We rely on two re-
guaranteed-to-be-invertible we can get ω from Γ .
1:d∀,d∀+1
sultsfromtheliterature,fromHolbrook(2018)andHartwig
Finally, 1 can be derived with knowledge of the previous
s (1976):
twoparametersfromΓ .Sincethecorrespondence
d∀+1,d∀+1
isone-to-one,theminimaofA′ andBareidentical-since Theorem2.20(Holbrook2018).
Bhasauniqueminimum,sodoesA′.Thus,toproveimpli- ddet†A=det†[A]tr(cid:2) A†dA(cid:3)
cation(2)weneedonlyshowthattheminimumofA′occurs
whens=1. Where det† is the pseudodeterminant and A† is the pseu-
doinverse.
(cid:18) (cid:12) (cid:12)(cid:19)
log|Γ|=log
|Ω|(cid:12) (cid:12)1 +ωTΩω−ωTΩΩ−1Ωω(cid:12)
(cid:12)
Corollary1.
(cid:12)s (cid:12)
d logdet†Γ (cid:20) dΓ(cid:21)
(Blockmatrixdeterminant) =tr Γ†
ds ds
=log|Ω|−logs
(cid:20) (cid:21) (cid:20) (cid:21) Thesecondtheorem,byHartwig,givesanexpressionfor
(cid:2) xT 1(cid:3) Γ x =(cid:2) xTΩ−ωTΩ 1 +ωTΩω−xTΩω(cid:3)x the pseudoinverse of a ‘bordered matrix’, i.e. a matrix of
1 s 1 (cid:20) (cid:21)
A c
=xTΩx−ωTΩx+ 1 +ωTΩω−xTΩω theform b d .However,theysplittheproblemintosev-
s eralsub-cases,whicharenotimmediatelyapparentfromthe
=(x−ω)T Ω(x−ω)+ 1 form of A,b,c, and d. Thus, we will first manipulate our
s matrix into one of these sub-cases before stating the theo-
Firstly,notethatalltermsinvolvingsareseparablefrom rem.
the rest. Differentiating with respect to s, we see that 0 = Recall that Ω is positive semi-definite, and thus it has
(cid:20) (cid:21)(cid:20) (cid:21)
1 − 1 at the minimum, i.e. that s = s2; since s > 0, Λ 0 VT
ts his les a2 ves s = 1 must hold at the minimum; thus, impli- an eigendecomposition [V 1 V 2] 0 0 V1 T . Letting
2
cation(2)holds.Secondly,notethattheseequationsleadto V=[V V ],observethefollowing:
1 2
thesameNLLasA,exceptwithanadditive 1 +logsterm
s
thrownin.Sinceweknows=1attheminimum,A′reduces (cid:20) (cid:21)(cid:20) (cid:21)(cid:20) (cid:21)
VT 0 Ω −Ωω V 0
toA;implication(1)holdsaswell.
0 1 −ωTΩ 1 +ωTΩω 0 1
Thiscompletestheproof;allthreeimplicationshold.The s
log-likelihoodofA′andBats=1arethesame,andhence (cid:20) VTΩV −VTΩω (cid:21)
=
alllocalminimaareglobalminima.Thisholdswhenusing ωTΩV 1 +ωTΩω
s
regularization,butwhataboutrestrictionstoalow-ranksub-  
Λ 0 −ΛVTω
space? 1
= 0 0 0 
Global Optimality under Low-Rank Restrictions As −ωTV Λ 0 1 +ωTΩω
1 s
mentioned in previous sections, most methods of estimat-
ing Ω use regularization to guarantee existence of a solu- Wewillnowfindthepseudoinverseofthisnewmatrix,as
tion.SomerestrictΩtoalow-ranksubspace,whichrequires UM†UT =(cid:0) UMUT(cid:1)† forunitarymatricesU.Infact,the
changing determinants to pseudodeterminants. All the im- bottom-rightentryisentirelyunchangedbythistransforma-
plications still hold in this case, but the block-determinant tion,anditwillturnouttobeallweneedforourproof.Theorem (Hartwig(1976)). Supposewehaveamatrixof isequivalentto:
 
Σ 0 q
theform 0 0 0fordiagonalΣoffullrank,andz =  argmin−logdet†Ω+(x−ω)T Ω(x−ω)+ρ(Ω)
pT 0 d 
ω,Ω
d−pTΣ−1q ̸= 0.Then,thelowerrighthandentryofthe A= where Ω∈K M ⪰0
p rase nu kd Σo .inverse is z1. Furthermore, the matrix has rank 1+ 
ω
⊥ω nu∈ llsK pav
ceΩ
Thisis,ofcourse,aspecialcaseoftheirfullresultgiving Thiscompletestheproof.
aformulafortheentirepseudoinverse-butitisallweneed.
Corollary2. ThelowerrighthandentryofΓ†iss. Erdos-RenyiSyntheticData
We mentioned in the results section that we repeated our
Proof. Wealreadysawthatourmatrixcanbeturnedintothe
experiments with Erdos-Renyi graphs (p=0.05) instead of
formrequiredbyHartwig’stheoremwithoutchangingwhat
Barabasi-Albert graphs. We found identical results, except
thelowerrighthandentrywillbe.Itthensufficestoobserve
thatz = 1 +ωTΩω−ωTV ΛΛ−1ΛVTω = 1. thatzero-meanalgorithmsperformmuchworseunderPois-
s 1 1 s son noise. See Figure 7 for the results, which use the same
From Corollary 1, the derivative of the log pseudodeter- meandistributionsasfromthemainpaper.
(cid:20) (cid:20) 0 0 (cid:21)(cid:21) (cid:34) 0 −1Γ† (cid:35)
minant is tr Γ†
0 − 1
= tr
0
−s 12 Γ†1:d∀,d∀+1 .
PR Curves for Er d [+os 1- R oe ffn sy ei t g ar la l p eh n t( r1 ie0 s0 ] nodes, p=0.05) PR Curves for [E +rd Ko Ss -s-R tre un cy tui g rer dap Gh a ( u1 s0 s0 ia n no ]des, p=0.05)
s2 s2 d∀+1,d∀+1 1.0 GmGM 1.0 GmGM
Thus,itonlydependsonthebottomrightentryofthepseu- Noncentral GmGM Noncentral GmGM
TeraLasso TeraLasso
doinverse;inparticular,itis 1.Fromthis,theargumentsin 0.8 Noncentral TeraLasso 0.8 Noncentral TeraLasso
s
theprevioussectionhold,andwecanshowthatanyminima 0.6 0.6
occuronlyats=1.
All that remains is to show that at s = 1, A and 0.4 0.4
A′ have the same solutions. We already know from ear-
0.2 0.2
lier that the pseudodeterminant of Γ is the same as the
 Λ 0 −ΛVTω  0.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.0 0.2 0.4 0.6 0.8 1.0
1 Recall Recall
pseudodeterminant of  0 0 0 . Re- PR Curves for Erdos-Renyi graph (100 nodes, p=0.05) PR Curves for Erdos-Renyi graph (100 nodes, p=0.05)
[+Unstructured Gaussian] [+Unstructured Poisson]
−ωTV 1Λ 0 1+ωTΩω 1.0 G Nom nG cM
entral GmGM
1.0 G Nom nG cM
entral GmGM
moving out the zero rows and columns, we are left with 0.8 T Ne or na cL ea nss tro al TeraLasso 0.8 T Ne or na cL ea nss tro al TeraLasso
(cid:20) (cid:21)
Λ −ΛVTω
−ωTV Λ 1+ωT1 Ωω ,whichbyHartwigisafullrank 0.6 0.6
1
matrix(recallthetheoremstateditsrankas1+rankΛand 0.4 0.4
thatΛcorrespondstothenon-zeroeigenvalues).Hence,we
0.2 0.2
canusetheordinaryblockdeterminantrule.
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(cid:20) (cid:21) Recall Recall
Λ −ΛVTω
det†Γ=det 1
−ωTV Λ 1+ωTΩω Figure7:Precisionandrecallforonasyntheticdataset.Er-
1
ror bars are the best/worst performance over 10 trials; the
=det[Λ]det(cid:2) 1+ωTΩω−ωTV 1ΛV 1Tω(cid:3) centerlineisaverageperformance.
Recall that we required ω to be orthogonal to the
nullspaceofΩ,resultingin:
det†Γ=det[Λ]det(cid:2) 1+ωTV ΛVTω−ωTV ΛVTω(cid:3)
1 1 1 1
=detΛ
=det†Ω
Thus,wehavethat:
A′
= a wωrg h,Ωm
e,
rsi en −logdet†Γ+ Ω(cid:2) ∈xT
K
M1 ⪰(cid:3) Γ 0(cid:20) x 1(cid:21) +ρ(Ω)
ω ∈K

ω
⊥nullspav
ceΩ
s>0
noisicerP
noisicerP
noisicerP
noisicerP