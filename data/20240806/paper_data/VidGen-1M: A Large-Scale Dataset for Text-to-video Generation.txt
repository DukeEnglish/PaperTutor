VIDGEN-1M: A LARGE-SCALE DATASET FOR TEXT-
TO-VIDEO GENERATION
ZhiyuTan1 XiaomengYang2 LuozhengQin2 HaoLi∗1
1FudanUniversity 2ShanghaiAcademyofAIforScience
https://sais-fuxi.github.io/projects/vidgen-1m
The video shows a highway winding through a lush green landscape. The road is surrounded by dense trees and vegetation on
both sides. The sky is overcast, and the mountains in the distance are partially obscured by clouds. The highway appears to be in
good condition, with clear lane markings. There are several vehicles traveling on the road, including cars and trucks. The colors in
the video are predominantly green from the trees and grey from the road and sky.
The video shows a cartoon monkey standing on a wooden dock in a pond. The monkey is holding a red bucket and appears to be
feeding the swans. There are several swans in the water, and they are all white with orange beaks. The background consists of
green trees and a small building with a red roof. The water is blue, and the sky is clear.
The video shows a person riding a scooter at a skate park. The person is wearing a helmet and a black t-shirt. They ride up a ramp
and perform a trick in the air before landing back on the ramp. The person then rides away from the ramp. The skate park has
various ramps and obstacles, and there are trees in the background.
Figure1: Asnapshotofthevideo-textpairsinVidGen-1M.
ABSTRACT
Thequalityofvideo-textpairsfundamentallydeterminestheupperboundoftext-
to-video models. Currently, the datasets used for training these models suffer
from significant shortcomings, including low temporal consistency, poor-quality
captions, substandard video quality, and imbalanced data distribution. The pre-
vailing video curation process, which depends on image models for tagging and
manualrule-basedcuration,leadstoahighcomputationalloadandleavesbehind
uncleandata. Asaresult,thereisalackofappropriatetrainingdatasetsfortext-
to-video models. To address this problem, we present VidGen-1M, a superior
trainingdatasetfortext-to-videomodels. Producedthroughacoarse-to-finecura-
tionstrategy,thisdatasetguaranteeshigh-qualityvideosanddetailedcaptionswith
excellent temporal consistency. When used to train the video generation model,
thisdatasethasledtoexperimentalresultsthatsurpassthoseobtainedwithother
models.
∗CorrespondingAuthor.
1
4202
guA
5
]VC.sc[
1v92620.8042:viXra1 INTRODUCTION
Recently, there have been significant advancements in text-to-video models, such as Latte (Ma
et al., 2024), SORA, OpenSora (Zheng et al., 2024), and Window Attention Latent Transformer
(W.A.L.T)(Guptaetal.,2023). However, trainingoncurrentvideo-textdatasetsfaceschallenges,
includingunstabletrainingandpoorperformance. Theseissuesstemfromseveralproblemsinher-
entinexistingvideo-textdatasets: 1)LowQualityCaptions: Thecaptionsexhibitpoorconsistency
with the videos and lack detailed captions. As demonstrated in DALLE-3 (Betker et al., 2023),
training text-to-image models on descriptive synthetic captions (DSC) significantly improves the
performance on text-to-image alignment, which is also called ”prompt following”. While exist-
ingtext-to-videodatasets,suchasHD-VILA-100M(Xueetal.,2022)andPandas-70M(Chenetal.,
2024),particularlylackpreciseanddetailedcaptions,theaveragelengthofcaptionsinthesedatasets
islessthan15words. Thesecaptionsoftenfailtocaptureobjectmotion,actions,andcameramove-
ments,hinderingthemodel’sabilitytoeffectivelylearnthesemanticandtemporalinformationinthe
videos. 2)LowQualityVideos: Theexistingdatasetsuffersfrompoorvideoqualityandaesthetics,
resultinginmodelstrainedonthisdatasetbeingunabletogeneratehigh-qualityvideos. 3)Tempo-
ral Inconsistency: The scene splitting algorithm fails to accurately detect scene transitions within
videos, leading to instability in training models based on such data. 4)Data Imbalance: Current
datasets, primarily composed of videos sourced from the internet, are often dominated by indoor
humanscenes,leadingtosignificantdataimbalanceissues.
Therearetwomainchallengesincuratingadatasetsuitablefortext-to-videogenerationtasks. First,
theexistingcurationprocessreliesoneitherimagemodelsoropticalflowmodels. Imagemodels,
such as the CLIP (Radford et al., 2021), lack the capability to capture temporal relationships in
videos. Conversely, using optical flow scores to curate videos with fast camera movements and
staticscenesisinaccurate. Second,comparedtothedatacurationprocessforimage-textpairs,the
process for video-text pairs is significantly more complex and computationally intensive, posing
majorchallengesfortheacademiccommunity.
Totackletheaforementionedchallenges,weintroduceamulti-stagedatacurationprocesscomposed
ofthreestages: coarsecuration,captioning,andfinecuration. Duringthecoarsecurationstage,we
utilizeexistingmodelstoperformscenesplittingandtaggingonthevideos.Basedonthesetags,we
filterandsamplethevideostocreateacurateddataset. Thisprocessensuresabalanceddistribution
acrossdifferentcategoriesandstyleswhilereducingthenumberofvideosthatneedtobeprocessed
inthesubsequentcomputationallyintensivecuratingphase.Duringthecaptioningstage,weemploy
avideocaptioningmodeltogeneratedescriptivesyntheticcaptions(DSC)forthevideos. Inthefine
curation stage, we employ a large language model (LLM) to refine video captions. This process
addresseserrorsfromthecoarsecurationstage,suchasincorrectlyfilteredvideos(e.g.,thosewith
scenetransitions)anderrorsincaptiongeneration(e.g.,captionsmissingtheEOStoken).
In this work, we introduce a large-scale dataset comprising 1 million video clips with descriptive
synthetic captions (DSC). The dataset features high-quality, open-domain videos accompanied by
richcaptionsaveraging89.2wordseach. Thesecaptionsnotonlyensurestrongertext-videoalign-
ment but also accurately capture the dynamic elements of the videos. Furthermore, the improved
temporalconsistencyofthevideosmitigatesinstabilityduringmodeltraining.Additionally,thedis-
tributionofvideosacrossdifferentcategoriesandstylesisbalanced. Trainingtext-to-videomodels
onourproposeddatasetachievessuperiorperformancecomparedtoexistingmethods.
Insummary,ourmaincontributionscanbesummarizedasfollows:
• We introduce a high-quality video dataset specifically designed for training text-to-video
models.
• Weproposeamulti-stagecurationmethodthatachievesprecise,high-qualitycurateddata
withlimitedcomputationalresources.
• We release our text-to-video model, which generates high-quality videos that surpass the
performanceofexistingstate-of-the-artmethods.
2Dataset Year Text Domain #Videos AVL ATL Res
MSVD(Chen&Dolan,2011) 2011 Human Open 2K 9.7s 8.7words -
LSMDC(Rohrbachetal.,2015) 2015 Human Movie 118K 4.8s 7.0words 1080p
UCF101(Soomroetal.,2012) 2015 Human Action 13K 7.2s 4.3words 240p
MSR-VTT(Xuetal.,2016) 2016 Human Open 10K 15.0s 9.3words 240p
DiDeMo(AnneHendricksetal.,2017) 2017 Human Flickr 27K 6.9s 8.0words -
ActivityNet(CabaHeilbronetal.,2015) 2017 Human Action 100K 36.0s 13.5words -
YouCook2(Zhouetal.,2018) 2018 Human Cooking 14K 19.6s 8.8words -
VATEX(Wangetal.,2019) 2019 Human Open 41K ∼10s 15.2words -
HowTo100M(Miechetal.,2019) 2019 ASR Open 136M 3.6s 4.0words 240p
YT-Temporal-180M(Zellersetal.,2021) 2021 ASR Open 180M - - -
HD-VILA-100M(Xueetal.,2022) 2022 ASR Open 103M 13.4s 32.5words 720p
Panda-70M(Chenetal.,2024) 2024 Auto Open 70.8M 8.5s 13.2words 720p
VIDGEN-1M(Ours) 2024 Auto Open 1M 10.6s 89.3words 720p
Table 1: Comparison of our dataset and other video-text datasets. “AVL” and “ATL” are abbreviations for
“AverageVideoLength”and“AverageTextLength”,respectively.
2 RELATED WORK
2.1 VIDEO-TEXTDATASET
Tofacilitatethedevelopmentofvideounderstandingandgeneration,researchersbuildalargevol-
ume of video-text datasets that vary in video length, resolution, domain, and scale. For instance,
UCF101(Soomroetal.,2012)isoriginallyanactionrecognitiondatasetconsistingof13,320videos,
whichcanbeclassifiedinto101categories. Formulateaunifiedtextconditionsforeachcategory,
UCF101iswidelyusedforbenchmarkingtext-to-videogeneration. MSVD(Chen&Dolan,2011)
andMSRVTT(Xuetal.,2016)aretwoopen-domainvideo-textdatasetspopularinvideoretrieval.
Thesedatasetscollectvideosfirstandthenannotatethesevideoswithhumanannotators. However,
duetotheheavycostofhumanannotation, theyareusuallylimitedbyscale, usuallyatthousands
scale. To alleviate this and expand video-text datasets to million scale, How2100M, HD-VILA-
100M (Xue et al., 2022) and YT-Temporal-180M (Zellers et al., 2021) propose to automatically
annotate videos with subtitles generated by ASR models. Meanwhile, Webvid scrapes 10.7 mil-
lion videos along with text annotation. While Panda-70M Chen et al. (2024) collects 70 million
high-resolutionandsemanticallycoherentvideosamples.
Theselargescalevideo-textdatasetssurlylaythecornerstonefortheadavancementoftext-to-video
generation. However, theyarelimitedbylowqualitycaptions, lowvideoquality, temporalincon-
sistency and data imbalance. To alleviate these challenges, we meticulously curate Panda-70M in
a coarse-to-fine way. Owing to our comprehensive and effective data curation, VidGen-1M fea-
tureshighvideoquality,highvideo-textconsistencyandbalancedanddiversevideocontent,which
significantlydifferitwithpreviousworks.
2.2 TEXT-TO-VIDEOGENERATIONMODEL
Toinvestigatethebestpracticefordesigningvideogenerationmodels, researchershasmadease-
ries of progress. SVD (Blattmann et al., 2023) first utilize SDXL (Podell et al., 2023) to generate
images conditioning on the input text, and then generate videos based on the generated images.
MAGVIT2(Yuetal.,2023)isaVQGAN(Esseretal.,2021)modelthataddressestheproblemof
codebook size and utilization by employing the lookup-free technique and training a large code-
book. Specifically,itmapsvideosintoquantizedvideotokensequencesandgeneratesvideosinan
autoregressive manner. WALT (Gupta et al., 2023) proposes to patchify input videos to lower the
trainingcosts.Meanwhile,Latte(Maetal.,2024)initializesitsparametersfromPixart(Chenetal.),
andinvestigatethetrainingeffciencyof4DiTvariants.SORAsparkstherevolutionoftext-to-video
generation,emergingaseriesofDiT(Peebles&Xie,2023)-basedvideogenerationmodels,suchas
OpenSora(Zhengetal.,2024)andMira(Juetal.,2024).Exceptfortheseopensourcemodels,there
arealsosomecommercialvideogenerationmodelsthatexhibitstronggenerationperformance,such
askling,dreamachine,andvidu.
33 METHOD
IntheconstructionofVidGen, weharnessed3.8millionhigh-resolution, long-durationvideosde-
rivedfromtheHD-VILAdataset.Thesevideosweresubsequentlysplitinto108millionvideoclips.
Following this, we tagged and sampled these video clips. The VILA model was then utilized for
videocaptioning. Lastly,torectifyanydatacuratingerrorsfromtheprecedingsteps,wedeployed
theLLMforfurthercaptioncurating.
3.1 COARSECURATION
To achieve efficient data curation with limited computational resources, we first employ a coarse
curationapproach. Thisinvolvesscenesplitting,videotagging,filtering,andsamplingtoreducethe
computationalloadinsubsequentstagesofcaptioningandfinecuration.
3.1.1 SCENESPLITTING
Motion inconsistencies, such as scene changes and fades, are frequently observed in raw videos.
However,sincemotioninconsistenciesdirectlycutoffthevideosemantics,text-to-videomodelsare
significantlysensitivetoandconfusedbythem,leadingtoheavyimpairmentontrainingefficiency.
Toalleviatetheirimpairment,wefollowthepriorresearch(Blattmannetal.,2023;Chenetal.,2024;
Zhengetal.,2024)toutilizePySceneDetect(Castellano,2014)inacascadingmannertodetectand
removescenetransitionsinourrawvideos.
3.1.2 TAGGING
In order to construct a dataset suitable for training text-to-video models, the data must meet the
followingcriteria: high-qualityvideos,balancedcategories,andstrongtemporalconsistencywithin
the videos. To achieve this goal, we first need to tag each splitted video clip. Subsequently, these
tagsserveasthebasisforcuratingandsampling.
VideoQuality. Thevisualqualityofvideosisofparamountimportancefortheefficienttrainingof
text-to-videomodels.Inordertoenhancethequalityofgeneratedvideosintext-to-videogeneration,
weadoptastrategyoffilteringoutvideoswithlowaestheticappealandhighOCRscores. Inthis
context, weemploytheLAIONAestheticsmodeltopredictandevaluateaestheticscores, thereby
ensuring a superior quality of training data. Particularly, the aesthetics models can also filter out
visuallyabnormalvideos,suchasvideoswithirregularcolordistributionorweirdvisualelements.
TemporalConsistency.Incorrectscenesplittinginvideoscansignificantlyimpairtheeffectiveness
of model training. High temporal consistency is a crucial characteristic required for the training
data in text-to-video models. To ensure this, we utilize the CLIP model to extract visual features
and assess temporal consistency. This assessment is achieved by calculating the cosine similarity
betweenthestartingandendingframesofvideoclips,therebyprovidingaquantitativemeasureof
continuityandcoherence.
CategoryTheHD-VILA-100Mvideodatasetdisplayssignificantimbalancesacrossitscategories,
resultinginlessthanoptimalperformanceofvideogenerationmodelsforthesecategories.Totackle
thisissue,wedeploypredefinedcategorytagstolabeleachvideo,withtheassistanceoftheCLIP
model. Specifically,weextracttheCLIPimagefeaturesfromtheinitial,middle,andfinalframesof
eachvideo,computetheiraverage,andthendeterminethesimilaritybetweentheseaveragedimage
features and the textual features associated with each tag. This methodology enables us to assign
themostfittingtagstoeachvideo.
Motion. We employ the RAFT (Teed & Deng, 2020) model to predict the optical flow score of
videos. As both static videos and those with excessively fast motion are detrimental for training
text-to-videomodels,wefilteroutthesevideosbasedontheiropticalflowscores.
3.1.3 SAMPLING
By employing tags associated with visual quality, temporal consistency, category, and motion, we
undertookthetask offilteringandsampling. Thecurateddatadistributionacross multiple dimen-
sions in our dataset is depicted in Figure 2. This figure clearly indicates that videos characterized
4(a) Category distribution (b) Aesthetics distribution (c) Text-Video alignment distribution
(d) Temproal consistency distribution (e) OCR distribution (f) Motion distribution
Figure2: Distributionofcurateddata.
bylowquality,staticscene,excessivemotionspeed,andthosedemonstratinginadequatealignment
between text and video along with poor temporal consistency were systematically removed. Con-
currently,wehaveensuredarelativelyevendistributionofsamplesacrossdiversecategories.
3.2 CAPTIONING
Dataset VN/DN VV/DV AvgN AvgV
Pandas-70M 16.1% 19.2% 4.3 1.9
Ours 20.3% 41.1% 22.5 15.9
Table 2: Statistics of noun and verb concepts for different datasets. VN: valid distinct nouns (ap-
pearingmorethan10times); DN:totaldistinctnouns; AvgN:averagenouncountpervideo. VV:
validdistinctverbs(appearingmorethan10times);DV:totaldistinctverbs;AvgN:averageverbs
countpervideo.
Thequalityofvideocaptionsexertsacriticalinfluenceontext-to-videomodel, whilethecaptions
intheHD-VILA-100Mdatasetdemonstrateseveralproblems,includingmisalignmentbetweentext
andvideo,inadequatedescriptions,andlimitedvocabularyuse. Toenhancetheinformationdensity
ofthecaptions,weemploythecutting-edgevision-languagemodel,VILA(Linetal.,2024). Owing
totheremarkablevideocaptioningabilityofVILA,wehavesignificantlyenhancedcaptionquality.
Aftercaptioning,weapplyclipscoretofilteroutthetext-videopairswithlowsimilarity.
WepresentavocabularyanalysisinTable2,whereweidentifyvaliddistinctnounsandvaliddistinct
verbsasthosethatappearmorethan10timesinthedataset. UtilizingtheVILAmodelontheHD-
VILA-100Mdataset,wehavegeneratedtheenhancedHD-VILA-100Mdataset. InthePanda-70M
dataset, there are 270K distinct nouns and 76K distinct verbs; however, only 16.1% and 19.2% of
thesemeetthevaliditycriteria,respectively. CaptionsgeneratedusingVILAsubstantiallyenhance
the valid ratio as well as the average count of nouns and verbs per video, thereby increasing the
conceptualdensity.
5Video Caption Pairs ST FLG Redup
The video shows a white boat moving through the water at a high speed, creating a large wake behind it. The boat is moving from the left to
the right of the frame, and the wake it leaves behind is white and frothy. The water is a deep blue color, and the sky is clear and blue. In the
background, there is a hilly coastline with green vegetation.
This video is an animated cartoon featuring a group of cats. The setting is a green field with a small pond. The first frame shows a small cat
running towards the pond. The second frame shows a larger cat and a smaller cat standing by the pond. The third frame shows the small cat
jumping into the pond. The fourth frame shows the small cat swimming in the pond. The fifth frame shows the small cat jumping out of the
pond. The sixth frame shows the small cat running towards the larger cat. The seventh frame shows the small cat jumping into the larger cat's
arms. The eighth frame shows the larger cat holding the small cat. The ninth frame shows the larger cat and the smaller cat smiling. The tenth
frame shows the larger cat and the smaller cat hugging.
The video opens with a close-up shot of a small green frog sitting on a white surface. The frog has a smooth texture and is looking directly at
the camera. The next shot shows a brown frog being held in a person's hand. The frog has a rough texture and is looking forward. The video
includes text overlays that read "THIS RARE FROG IS THE UNLIKELY SYMBOL OF THE BATTLE OVER ENDANGERED SPECIES.
The video shows a woman in a purple blazer and a striped shirt sitting in front of a blue background. She is speaking to the camera and
appears to be in a news studio. There is a caption at the bottom of the screen that reads \\"BREAKING BREAKING BREAKING BREAKING
BREAKING BREAKING BREAKING BREAKING BREAKING BREAKING BREAKING
Figure3: CaptionCurationResult. EmployingLlama3.1tocuratevideocaptions, asproducedby
VILA,resultsinasignificantimprovementinthequalityofthetrainingdataset. Thisisspecifically
evidenced through improvements in video temporal consistency and alignment between text and
video. Consequently, thismethodfacilitatesnotableimprovementsintheperformanceofthetext-
to-videomodel.
3.3 FINECURATION
In the stages of coarse curation and captioning, filtering for text-image alignment and temporal
consistencyusingtheCLIPscorecanremovesomeinconsistentdata,butitisnotentirelyeffective.
Consequently, issues such as scene transitions in video, and two typical description errors occur
in video captions: 1) Failed generating eos token, where the model fails to properly terminate the
generationprocess,leadingtoloopingorrepetitivetokengeneration,and2)Frame-levelgeneration,
where the model lacks understanding of the dynamic relationships between frames and generates
isolateddescriptionsforeachframe,resultingincaptionsthatlackcoherenceandfailtoaccurately
reflectthevideo’soverallstorylineandactionsequence.
To address the mentioned data curating issues, one potential solution is manual annotation, but
this approach is prohibitively expensive. Fortunately, with recent advancements in large language
models,thisproblemcanberesolved. ErrorsincaptionsgeneratedbyMulti-ModalLanguageMod-
els (MLLMs) can be identified by analyzing specific patterns, such as scene transitions, repetitive
content,andframe-leveldescriptions,usingaLanguageModel(LLM).ModelslikeLLAMA3have
shownexceptionalproficiencyinthesetasks,makingthemaviablealternativetomanualannotation.
In our endeavor to isolate and remove video-text pairings that exhibit discrepancies in both text-
videoalignmentandtemporalconsistency,weleveragedthecutting-edgeLanguageModel(LLM),
LLAMA3.1,toscrutinizetherespectivecaptions. Theapplicationofthefinecurationhasfacilitated
amarkedimprovementinthequalityofthetext-videopairs,asevidencedinFigure3.Ourstudypri-
marilycentersaroundthreecriticalfactors: SceneTransition(ST),Frame-levelGeneration(FLG),
andReduplication(Redup).
64 EXPERIMENTS
4.1 IMPLEMENTATIONDETAILS.
Experimentsetup.Toevaluatetheeffectivenessofourtext-to-videotrainingdataset,weperformed
acomprehensiveevaluationusingthebasemodel,composedofbothspatialandtemporalattention
blocks. Toacceleratethetrainingprocess, weinitiallyconductedextensivepre-trainingonalarge
collection of low-resolution 256 × 256 images and videos. Following this, we carried out joint
trainingonourVidGen-1Mdatasetusing512×512pxresolutionimagesand4-secondvideos.
4.2 EXPERIMENTRESULTS
4.2.1 QUALITATIVEEVALUATION.
As displayed in Figure4, our model’s ability to generate superior-quality videos is a testament to
therobustnessofthehigh-resolutionVidGen-1Mdataset. Thisdataset’shighqualityisreflectedin
therealismanddetailofthegeneratedvideos,reinforcingitseffectivenessintrainingourmodel. A
noteworthycharacteristicofourgeneratedvideosistheirstrong”promptfollowing”ability,which
isadirectoutcomeofthehighconsistencybetweenvideo-textpairsinthetrainingdata.Thisconsis-
tencyensuresthatthemodelcanaccuratelyinterpretthetextualpromptsandgeneratecorresponding
videocontentwithhighfidelity.ThefirstexamplefurtherunderlinesthehighqualityoftheVidGen-
1M dataset. The generated video demonstrates remarkable realism - from the diver’s hair floating
underwatertothemotionofthebubbles.Thesedetails,whichshowcasesignificanttemporalconsis-
tencyandadheretoreal-worldphysics,highlightthemodel’scapabilitytogeneratebelievableand
visuallyaccuratevideocontent.
TheVidGen-1Mdataset’squalityhasfar-reachingimplicationsforthefieldofcomputervision,par-
ticularlyfortext-to-videogeneration. Byprovidinghigh-resolutionandtemporalconsistencytrain-
ingdata, VidGen-1M enablesmodelsto generatemorerealistic andhigh-qualityvideos. This can
lead to advancements in video generation techniques, pushing the boundaries of what is currently
possible. Furthermore, thehigh-qualitydataprovidedbyVidGen-1Mcouldpotentiallystreamline
themodeltrainingprocess. Withmoreaccurateanddetailedtrainingdata, modelscanlearnmore
effectively,potentiallyreducingtheneedforextensivecomputationalresourcesandtime-consuming
trainingperiods. Inthisway,VidGen-1Mnotonlyimprovestheoutcomesoftext-to-videogenera-
tionbutalsocontributestomoreefficientandsustainablemodeltrainingpractices.
5 CONCLUSION
Inthispaper,weintroduceahigh-qualityvideo-textdatasetfeatureshighvideoquality,highcaption
quality,hightemporalconsistencyandhighvideo-textalignment,specificallydesignedforthetrain-
ingoftext-to-videogenerationmodels. Theaforementionedvarioushighqualityfeaturesarisefrom
ourmeticulouslydatacurationprocedure,whichefficientlyensureshighdataqualityinacoarse-to-
finemanner. ToverifytheeffectivenessofVidGen-1M,wetrainatext-to-videogenerationmodel
onit. Theresultsarepromising,themodeltrainedonVidGen-1MachievesremarkablybetterFVD
scoresonzero-shotUCF101,comparedwithstate-of-the-arttext-to-videomodels. Tobootstrapthe
developmentofhighperformancevideogenerationmodels,wewillreleaseVidGen-1M,alongwith
therelatedcodesandthemodelstrainedonit,tothepublic.
REFERENCES
LisaAnneHendricks,OliverWang,EliShechtman,JosefSivic,TrevorDarrell,andBryanRussell.
Localizing moments in video with natural language. In Proceedings of the IEEE international
conferenceoncomputervision,pp.5803–5812,2017.
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. Computer
Science.https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023.
7Prompt: In clear blue water, a diver in a black wetsuit and yellow mask,
holding a camera and surrounded by bubbles, is filming the vast
underwater world.
Prompt: A brightly lit rocket trails smoke as it ascends into the stark
night sky.
Prompt: In a dimly lit, green and murky aquarium, variously sized yellow
and orange fish swim and hide among rocks and plants.
Prompt: A man in a red cap and grey shirt stands on a beach, speaking
to the camera, with the calm sea showing small waves and rocks visible
in the water, and a setting sun in a clear sky with a few clouds.
Figure 4: Qualitative Evaluation: The model we’ve developed can generate videos from natural
language prompts at a resolution of 512 × 512. These videos are 4 seconds long and play at 8
frames per second. Notably, our model can generate photorealistic videos that maintain temporal
consistencyandalignaccuratelywiththetextualprompt.
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal.Stablevideodiffusion:Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023.
Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet:
A large-scale video benchmark for human activity understanding. In Proceedings of the ieee
conferenceoncomputervisionandpatternrecognition,pp.961–970,2015.
8Brandon Castellano. Pyscenedetect. https://github.com/Breakthrough/
PySceneDetect,2014.
David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In
Proceedingsofthe49thannualmeetingoftheassociationforcomputationallinguistics: human
languagetechnologies,pp.190–200,2011.
JunsongChen,YUJincheng,GEChongjian,LeweiYao,EnzeXie,ZhongdaoWang,JamesKwok,
Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-alpha: Fast training of diffusion transformer
forphotorealistictext-to-imagesynthesis. InTheTwelfthInternationalConferenceonLearning
Representations.
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao,
Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m:
Captioning70mvideoswithmultiplecross-modalityteachers. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.13320–13331,2024.
PatrickEsser,RobinRombach,andBjornOmmer. Tamingtransformersforhigh-resolutionimage
synthesis. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecogni-
tion,pp.12873–12883,2021.
Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang,
and Jose´ Lezama. Photorealistic video generation with diffusion models. arXiv preprint
arXiv:2312.06662,2023.
XuanJu,YimingGao,ZhaoyangZhang,ZiyangYuan,XintaoWang,AilingZeng,YuXiong,Qiang
Xu, and Ying Shan. Miradata: A large-scale video dataset with long durations and structured
captions,2024. URLhttps://arxiv.org/abs/2407.06358.
JiLin,HongxuYin,WeiPing,PavloMolchanov,MohammadShoeybi,andSongHan.Vila:Onpre-
trainingforvisuallanguagemodels. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.26689–26699,2024.
Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen,
and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint
arXiv:2401.03048,2024.
AntoineMiech,DimitriZhukov,Jean-BaptisteAlayrac,MakarandTapaswi,IvanLaptev,andJosef
Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated
video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pp.
2630–2640,2019.
WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pp.4195–4205,2023.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
AnnaRohrbach,MarcusRohrbach,NiketTandon,andBerntSchiele. Adatasetformoviedescrip-
tion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
3202–3212,2015.
KhurramSoomro,AmirRoshanZamir,andMubarakShah.Ucf101:Adatasetof101humanactions
classesfromvideosinthewild. arXivpreprintarXiv:1212.0402,2012.
ZacharyTeedandJiaDeng. Raft: Recurrentall-pairsfieldtransformsforopticalflow. InComputer
Vision–ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,
PartII16,pp.402–419.Springer,2020.
9XinWang,JiaweiWu,JunkunChen,LeiLi,Yuan-FangWang,andWilliamYangWang. Vatex: A
large-scale,high-qualitymultilingualdatasetforvideo-and-languageresearch. InProceedingsof
theIEEE/CVFinternationalconferenceoncomputervision,pp.4581–4591,2019.
JunXu,TaoMei,TingYao,andYongRui. Msr-vtt: Alargevideodescriptiondatasetforbridging
video and language. In Proceedings of the IEEE conference on computer vision and pattern
recognition,pp.5288–5296,2016.
HongweiXue,TiankaiHang,YanhongZeng,YuchongSun,BeiLiu,HuanYang,JianlongFu,and
Baining Guo. Advancing high-resolution video-language representation with large-scale video
transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.5036–5045,2022.
LijunYu, Jose´ Lezama, NiteshBGundavarapu, LucaVersari, KihyukSohn, DavidMinnen, Yong
Cheng,AgrimGupta,XiuyeGu,AlexanderGHauptmann,etal.Languagemodelbeatsdiffusion–
tokenizeriskeytovisualgeneration. arXivpreprintarXiv:2310.05737,2023.
Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and
YejinChoi.Merlot:Multimodalneuralscriptknowledgemodels.Advancesinneuralinformation
processingsystems,34:23634–23651,2021.
Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun
Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all,
March2024. URLhttps://github.com/hpcaitech/Open-Sora.
LuoweiZhou,ChenliangXu,andJasonCorso. Towardsautomaticlearningofproceduresfromweb
instructionalvideos.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32,
2018.
10