TechnicalReport
LUMINA-MGPT: ILLUMINATE FLEXIBLE PHOTORE-
ALISTIC TEXT-TO-IMAGE GENERATION WITH MULTI-
MODAL GENERATIVE PRETRAINING
DongyangLiu1∗ ShitianZhao1∗ LeZhuo1∗ WeifengLin1∗ YuQiao1 HongshengLi1,2
PengGao1†‡∗
1ShanghaiAILaboratory 2TheChineseUniversityofHongKong
ABSTRACT
WepresentLumina-mGPT,afamilyofmultimodalautoregressivemodelscapable
ofvariousvisionandlanguagetasks,particularlyexcellingingeneratingflexible
photorealisticimagesfromtextdescriptions. Unlikeexistingautoregressiveimage
generationapproaches,Lumina-mGPTemploysapretraineddecoder-onlytrans-
former as a unified framework for modeling multimodal token sequences. Our
keyinsightisthatasimpledecoder-onlytransformerwithmultimodalGenerative
PreTraining (mGPT), utilizing the next-token prediction objective on massive
interleavedtext-imagesequences,canlearnbroadandgeneralmultimodalcapa-
bilities, thereby illuminating photorealistic text-to-image generation. Building
on these pretrained models, we propose Flexible Progressive Supervised Fine-
tuning(FP-SFT)onhigh-qualityimage-textpairstofullyunlocktheirpotential
forhigh-aestheticimagesynthesisatanyresolutionwhilemaintainingtheirgen-
eralmultimodalcapabilities. Furthermore,weintroduceOminiponentSupervised
Finetuning (Omni-SFT), transforming Lumina-mGPT into a foundation model
thatseamlesslyachievesomnipotenttaskunification. Theresultingmodeldemon-
strates versatile multimodal capabilities, including visual generation tasks like
flexibletext-to-imagegenerationandcontrollablegeneration,visualrecognition
taskslikesegmentationanddepthestimation,andvision-languagetaskslikemulti-
turn visual question answering. Additionally, we analyze the differences and
similaritiesbetweendiffusion-basedandautoregressivemethodsinadirectcom-
parison. All code and checkpoints will be available on our GitHub repository:
https://github.com/Alpha-VLLM/Lumina-mGPT.
1 INTRODUCTION
Seminalmodels,includingDALL-E3(Betkeretal.,2023),StableDiffusion3(Esseretal.,2024),
andSoRA(Brooksetal.,2024),havedemonstratedsuperiorperformanceinphotorealisticimageand
videogenerationusingdiffusion-basedgenerativemodelingovercontinuouslatentimagefeatures.
In contrast, autoregressive (AR) generative models, which rely on “next-token prediction,” have
revolutionizedtextgenerationwithgroundbreakingreasoningabilities,asexemplifiedbymodels
likeGPT-4(Achiametal.,2023)andGemini(Teametal.,2023),overdiscretetokenrepresentation.
However,AR-basedgenerativemodelingovervector-quantizedimagefeaturesstilllagsfarbehind
diffusion-basedcounterpartsintermsofphotorealisticandcontrollableimagegeneration.
Althoughpreviousautoregressiveefforts,suchasDALL-E(Rameshetal.,2021),CogView(Ding
etal.,2021),Parti(Yuetal.,2022),OFA(Wangetal.,2022),Unified-IO(Luetal.,2022;2024a),
LlamaGen(Sunetal.,2024),andChameleon(Team,2024a),haveextensivelyexploredgenerative
modelingovervector-quantizedimagefeaturesfollowingtheparadigmoflargelanguagemodels
(LLMs),theirresultsontext-to-imagegenerationhaveeitherbeenunsatisfactoryorproducedhigh-
∗EqualContribution
†CorrespondingAuthors
‡ProjectLead
1
4202
guA
5
]VC.sc[
1v75620.8042:viXraTechnicalReport
1024x1024 1024x1024 640x1536
640x1536 768x768 768x768 1024x1024
1280x800 1024x1024
1024x1024 1024x1024 1024x1024
Dense Labeling
1024x1024 1024x1024
Controllable Generation
Figure1: SelectedphotorealisticimagesofvariousresolutionsgeneratedbyLumina-mGPT.Atthe
bottom,weincludetwoexamplestodemonstratetheomnipotenttaskunificationinLumina-mGPT.
2TechnicalReport
Table1: Overviewofthedesignchoicesandcapabilitiesofmultimodalautoregressiveapproaches.
Lumina-mGPT is the only model capable of both flexible photorealistic image generation and
multimodaltaskunification,duetoitsdecoder-onlytransformerdesignandmultimodalgenerative
pretraining.
Model Multimodal FlexiblePhotorealistic Task
Method
Architecture Pretraining ImageGeneration Extensibility
DALL-E(Rameshetal.,2021) Decoder-only × × ×
Cogview(Dingetal.,2021) Decoder-only × × ×
Parti(Yuetal.,2022) Encoder-Decoder × × ×
LlamaGen(Sunetal.,2024) Encoder-Decoder × × ×
OFA(Wangetal.,2022) Encoder-Decoder ✓ × ✓
Unified-IO(Luetal.,2022) Encoder-Decoder × × ✓
Unified-IO2(Luetal.,2024a) Encoder-Decoder ✓ × ✓
Chameleon(Team,2024a) Decoder-only ✓ × ×
Lumina-mGPT Decoder-only ✓ ✓ ✓
qualitysampleslimitedtoacademicbenchmarkslikeImageNet(Dengetal.,2009). Wesummarize
thelimitationsofpreviousapproachesbelow:
Randomly-InitializedTransformerWhiletransferlearninghasrevolutionizedkeyfieldssuchasvi-
sualrecognition(Heetal.,2016;Radfordetal.,2021;Luetal.,2019)andlanguagegeneration(Raffel
etal.,2020;Radfordetal.,2018;2019;Brownetal.,2020),popularautoregressiveimagegeneration
approachessuchasDALL-E,Parti,andLlamaGenalladoptarandomly-initializedcausaltransformer,
which fails to utilize pretrained transferable representation and large-scale datasets. As a result,
AR-basedapproachesoftenleadtopoorimagegenerationqualityandslowconvergencewithout
leveragingproperlarge-scalepretraining.
VerboseEncoder-DecoderArchiteture DALL-E and CogView initially propose using a decoder-
only transformer for image generation with discrete representation, where a single transformer
acts as both a text encoder and an image token decoder. However, subsequent approaches, such
as Parti and LlamaGen, adopt a verbose encoder-decoder architecture that injects frozen T5 text
features (Raffel et al., 2020) using cross-attention or prefix-filling approaches, motivated by the
findingsofImagen(Sahariaetal.,2022). ComparedtothetrendinscalingLLMs(Touvronetal.,
2023;Baietal.,2023;Bietal.,2024), suchencoder-decoderarchitectureiscumbersomedueto
thedecouplingoftextencodingandimagetokenmodeling. Thisdesignsignificantlycomplicates
theautoregressive-decodingframework,limitsthescalabilityofimagegeneration,andhindersthe
generalizationtoadditionalmodalitiesandtasks.
LimitedDecodingResolutionandFlexibilityNaturalimagesexsitinvariousresolutionsandaspect
ratios. Advanceddiffusionmodels(Chenetal.,2024a;b;Esseretal.,2024;Team,2024b;Lietal.,
2024b;Zhuoetal.,2024;Luetal.,2024b)cansuccessfullygeneratediversephotorealisticimagesat
arbitraryresolutionwithskewedratios. Incontrast,currentAR-basedapproaches(Yuetal.,2022;
Team, 2024a; Sun et al., 2024) rely on central-cropping a low-resolution 512 × 512 image and
transformingthecroppedlow-resolutionimageintoafixed-lengthsequenceofdiscretetokensusinga
pretrainedVector-QuantizedVariationalAutoencoder(VQ-VAE)(VanDenOordetal.,2017;Razavi
etal.,2019;Esseretal.,2021). Thisapproachsimplifiesautoregressivetrainingbutatthecostof
deterioratedimagequalityandgenerationflexibility.
PoorTaskExtensabilityAutoregressivemodelingexcelsatunifiedgenerativemodelingforvarious
tasksandmodalitiesusingdiscretetokens(Wangetal.,2022;Luetal.,2022;2024a). However,
previousAR-basedimagegenerationapproaches(Rameshetal.,2021;Dingetal.,2021;Yuetal.,
2022;Sunetal.,2024)havebeenlimitedtotext-to-imagegenerationwithoutexploringtheunification
with other tasks, such as dense labeling and controllable image generation. This lack of task
extensibilitysignificantlyconstrainstheapplicabilityofAR-basedimagegenerationmodelstoa
broaderrangeofscenarios.
Toaddresstheaforementionedchallenges,wepresentLumina-mGPT,adecoder-onlytransformer
initiatedwitheffectivemultimodalGenerativePreTraining(mGPT)andthensupervised-finetuned
over flexible, high-quality, high-resolution discrete image tokens in a progressive manner. This
3TechnicalReport
frameworkilluminatesflexiblehigh-resolutionphotorealisticimagegenerationandcanbeeasily
extendedtosolvevariouscomponenttasksinaunifiedmanner. Weprovideadetailedcomparison
of the architecture design choices and model capabilities of existing multimodal autoregressive
approachesinTable1. ThekeyfeaturesofLumina-mGPTareoutlinedbelow:
➀EffectiveMultimodalGenerativePretrainingUnlikecommonlyadoptedapproachesthatutilizea
randomly-initializedcausaltransformertogeneratediscreteimagetokensinanautoregressivemanner,
Lumina-mGPT is initialized from an effective multimodal generative pretraining representation.
mGPTisamultimodalautoregressivetransformertrainedonalarge-scaleimage-textinterleaved
corpus using a simple “next-token prediction” loss. Our key finding is that leveraging the rich
knowledgefromlarge-scalemultimodalpretraining,mGPTcanlearnbroadandgeneralmultimodal
representation. Thisservesasanexcellentstartingpoint,remarkablysimplifyingthetaskoftext-
to-imagegeneration,acceleratingtrainingconvergenceondownstreamtasks,andunifyingvarious
vision-languagetasks. Duetothelackoftrainingresources,thismGPTrepresentationisdirectly
adoptedfromthepretrainedChameleon7Band30BmodelsreleasedbyMeta(Team,2024a).
➁SimpleDecoder-OnlyArchitecture As mGPT is pretrained on multimodal tokens, it naturally
learns vision and language representation within a single transformer. Consequently, with one
decoder-onlycausaltransformer,Lumina-mGPTcanhandlebothtextencodingandimagetoken
decodingsimultaneously. Bysharingthisdecoder-onlyframeworkwithtext-onlyLLMs,Lumina-
mGPTcanbenefitfromwell-establishedtheoriesandinfrastructuresintheLLMcommunity,such
asscalingproperties(Chowdheryetal.,2022;Brownetal.,2020)andtechniquestooptimizethe
trainingandinferenceprocesses(Daoetal.,2022;Dao,2024;Kwonetal.,2023). Wetrainafamily
ofLumina-mGPTswithupto30billionparameterstoprovideaflexibletrade-offbetweenefficiency
andperformanceandtoexplorethelimitofitsmultimodalcapabilitiesasmodelparametersscale.
➂FlexibleHigh-ResolutionDecodingBenefitingfromlarge-scalepretrainingonmulti-modalin-
terleaved datasets and advanced architectural designs including query-key normalization (QK-
Norm)(Henryetal.,2020;Luetal.,2024a;Zhuoetal.,2024;Team,2024a)androtarypositional
embedding (RoPE) (Su et al., 2024), the mGPTs are ideal initializations for flexible photorealis-
tic image generation. We thus propose Flexible Progressive Supervised Finetuning (FP-SFT) to
fullyexploitthepotentialofmGPTforflexible, photorealistichigh-resolutiontext-to-imagegen-
eration. Thisapproachstartswithlow-resolutiondiscretetokensandprogressivelytransitionsto
high-resolutiondiscretetokens. Combinedwithresolution-awareprompts,thisweak-to-strongSFT
strategyeffectivelyandefficientlyenhancestheimagegenerationcapabilitiesofLumina-mGPT.To
equip Lumina-GPT with the ability to understand and generate images at flexible resolution and
aspectratio,wealsoproposeUnambiguousimageRepresentation(Uni-Rep),whicheliminatesthe
ambiguityintheoriginal2Dshapeoftheflattened1Dimagetokensbyintroducingheight/width
indicatorsand<end-of-line>tokens.
➃OmnipotentTaskUnificationOneadvantageofAR-basedapproachesoverdiffusion-basedgener-
ativemodelingistheirpromisingpotentialforunifiedsequencemodelingacrossvariousmodalities
andtaskswithinasingleframework(Chenetal.,2022;Wangetal.,2022;Luetal.,2022;2024a).
In this paper, we explore this potential through preliminary experiments. Specifically, we intro-
duceOmnipotentSupervisedFinetuning(Omni-SFT),whichtreatsvarioustasks–suchasmulti-turn
dialog(Achiametal.,2023;Meta,2024;Baietal.,2023;Bietal.,2024),visualmulti-turnunder-
standing(Liuetal.,2023;Chenetal.,2024c),denselabeling(Carionetal.,2020;Kirillovetal.,
2023;Yangetal.,2024),flexibleimagegeneration(Zhuoetal.,2024;Renetal.,2024;Duetal.,
2024),andspatial-conditionalimagegeneration(Zhangetal.,2023;Yeetal.,2023)–asaunified
discretemodelingtask. WethenperformOmni-SFTovermGPTtoseamlesslyachieveomnipotent
taskunificationviaanaturallanguageinterface.
Wesummarizeourcontributionsasfollows: (1)Wediscoverthatautoregressivetransformerswith
multimodalgenerativepertaining,ratherthanbeingtrainedfromscratch,canserveasaneffective
initializationforphotoelastictext-to-imagegenerationandtheunificationofvisionandlanguagetasks.
(2)BuildingonmGPTs,weintroducetwonovelfinetuningstrategies,FP-SFTandOmni-SFT,tofully
unleashtheirpotentialinaweak-to-strongparadigmusingonly10Mhigh-qualitytext-imagedata. (3)
Theresultingmodel,Lumina-mGPT,notonlydemonstratesimpressivephotorealistictext-to-image
generationatanyresolution,bridgingthegapbetweenautoregressiveanddiffusion-basedmethods,
butalsoseamlesslysupportsvariousdownstreamtasksusingalanguageinterface.
4TechnicalReport
2 RELATED WORK
MultimodalLargeLanguageModels Recentlargelanguagemodels(LLMs)(Chowdheryetal.,
2022;Touvronetal.,2023;Brownetal.,2020;Achiametal.,2023;Teametal.,2023)demonstrate
stronginstruction-followingandreasoningcapabilities,coupledwithextensiveworldknowledge. To
extendLLMs’expertisefromthetextdomaintomultimodaldomainssuchasimagesandvideos,
priorworks(Liuetal.,2023;Linetal.,2023a;Maazetal.,2023;Linetal.,2023b)havealigned
pretrainedencodersfromvariousmodalitieswithLLMsbycuratingmultimodalinstructiontuning
datasets. Althoughthesemultimodallargelanguagemodels(MLLMs)haveexhibitedpowerfulvisual
understandingcapabilities,theirmultimodalfunctionalityisprimarilylimitedtoperceivingthevisual
world, exemplifiedbytaskssuchasvisualquestionansweringandimagecaptioning, ratherthan
generatingvisualoutputs. AnotherlineofresearchKohetal.(2024);Dongetal.(2024);Sunetal.
(2023);Wuetal. hasproposedaugmentingMLLMswiththeabilitytogenerateimages,videos,and
audiofromtextinstructions. Theseapproachesintroduceadditionalvisualtokensforgenerationand
alignthesegenerativetokensasconditionalinformationwithapretrainedgenerator,suchasStable
Diffusion(Esseretal.,2024;Podelletal.,2023)fortext-to-imagegeneration. Consequently,the
generationcapabilitiesheavilyrelyontheexternalexpertgeneratorratherthanMLLMsthemselves,
resultingininconsistentandinferiorgenerationresults. Tocombinethestrengthofbothapproaches,
ourmodelaimstolearnbothunderstandingandgenerationofimagesusinganMLLMwithnative
multimodalcapabilities,drawinginspirationfromChameleon(Team,2024a),amixed-modalearly-
fusionfoundationmodel.
Text-to-ImageGeneration Thetaskoftext-to-imagegenerationseekstosynthesizephotorealistic
anddiverseimagesbasedontextualdescriptions. Nowadays, diffusionmodels, whetherinpixel
space (Saharia et al., 2022) or in latent space (Podell et al., 2023; Esser et al., 2024; Pernias
etal.,2024),havebecomethede-factoapproachesduetotheirsuperiorperformance,particularlyin
producingextremelyhigh-aestheticimages.Amongthesemodels,therecenttrendofscalingdiffusion
transformers(DiTs)(Chenetal.,2024b;a;Gaoetal.,2024;Esseretal.,2024;Lietal.,2024b;Team,
2024b)suggestsaunifiedarchitectureforbothtextandimagemodeling. However,existingDiTsstill
leverageseparatelanguagemodels,suchasCLIP(Radfordetal.,2021)orT5(Raffeletal.,2020),as
textencoders. Thismodalitygapbetweentextandimagerepresentationsnotonlyleadstoinaccurate
generationbutalsohindersthedevelopmentofaunifiedmultimodalfoundationalgenerativemodel.
Comparedtothedominanceofdiffusionmodels,theprogressofautoregressiveimagegenerationhas
receivedlessattentioninrecentyears. Earlyworks(Rameshetal.,2021;Dingetal.,2021)proposed
atwo-stagegenerationapproach: first,trainingaVQ-VAEVanDenOordetal.(2017);Esseretal.
(2021)forimagetokenizationandde-tokenization,andthenusinganautoregressivetransformerto
modeldiscreteimagetokensequences,akintolanguagemodeling. Parti(Yuetal.,2022)scaledup
theautoregressivetransformerto20billionparameters,demonstratingpromisinghigh-fidelityimage
generationresults. LlamaGen(Sunetal.,2024)furtherimprovedtheimagetokenizerandintegrated
advancedtechniquesinLLMs,bridgingtheperformancegapwithdiffusioncounterparts. Unlike
PartiandLlamaGen,Lumina-mGPTproposesmultimodalgenerativepertainingonunifiedtext-image
sequences,followedbysupervisedfinetuningonhigh-qualitytext-to-imagepairs,achievingflexible
high-aestheticimagegenerationwithautoregressivemodels.
3 METHODOLGY
Lumina-mGPTisadecoder-onlytransformerinitializedwithmultimodalGenerativePreTraining
(mGPT)andfinetunedoverhigh-qualitymultimodaltokensderivedfromvarioustasks. Basedonthe
robustmGPTrepresentationandourproposedsupervisedfinetuningstrategieswithunambiguous
imagerepresentation,Lumina-mGPTachievessuperiorperformanceinphotorealisticimagegenera-
tionandomnipotenttaskunificationwithhighflexibilityinimageresolutionandaspectratio. Inthis
section,wefirstintroducemGPT,followedbythetrainingandinferencedetailsofLumina-mGPT.
3.1 REVISITINGMGPTWITHCHAMELEON
mGPTrepresentsthefamilyofmodelsutilizingadecoder-onlytransformerarchitecture,pretrained
on extensive multimodal token sequences. These models exhibit exceptional native multimodal
understanding,generation,andreasoningcapabilities,offeringthepotentialforuniversalmodeling
5TechnicalReport
acrossvariousmodalitiesandtasks. Weusetherecentopen-sourcemodel,Chameleon(Team,2024a),
asanexampletoillustratethedesignchoicesandimplementingdetailsofmGPT.
MultimodalTokenization Tounifytextandimagesintoamultimodaltokensequence,itisessential
firsttotokenizebothtextandimageintodiscretespace. Especiallyforimages,thechoiceoftokenizer
iscrucialasitdeterminestheupperlimitofgenerationquality. Specifically, Chameleontrainsa
newbytepairencodingtokenizerfortext. Forimages,itadoptsthequantization-basedtokenization
methodfollowingpriorworks(VanDenOordetal.,2017;Razavietal.,2019;Esseretal.,2021;Yu
etal.,2022;Rameshetal.,2021),convertingcontinuousimagepatchesintodiscretetokensfroma
fixedcodebookwhilereducingspatialdimensions. Thequantizedimagetokensarethenflattened
intoa1Dsequenceandconcatenatedwithtexttokensinvariouswaystoformamultimodaltoken
sequenceforunifiedmodeling.
Decoder-OnlyTransformer UnlikeUnified-IOandPartiusinganencoder-decoderarchitecture
withpretrainedencoders,mGPTtrainsadecoder-onlyautoregressivetransformerfromscratchafter
transformingtextandimageinputsintoaunifiedsequenceofdiscretetokensx=(x ,x ,...,x ),
1 2 T
leadingtoasimplerandmoreunifiedapproachformultimodalgenerativemodeling. mGPTadoptsa
standarddensetransformerarchitectureforscalabilityandgeneralizability,withminoradaptations
likeRoPE(Suetal.,2024)andSwiGLU(Shazeer,2020)activationfunction,followingtheLLaMA
series(Touvronetal.,2023).
However, this standard transformer architecture exhibits training instability when scaling up the
modelsizeandcontextlengthofmultimodaltokensequences,asobservedinbothChameleon(Team,
2024a) and Lumina-Next (Zhuo et al., 2024). It has been found that this instability is caused by
theuncontrollablegrowthofnetworkactivationsintransformerblocks. Therefore,Pre-Norm,Post-
Norm,andQK-Normareaddedtoeachtransformerblocktopreservethemagnitudeofintermediate
activationandfurtherstabilizethetrainingprocess.
TrainingObjective Duringtraining,mGPTmodelstheconditionalprobabilityp(x |x ,...,x )
t 1 t−1
ofmultimodalsequencesusingthestandardnext-tokenpredictionobjective.Additionally,Chameleon
appliesz-loss(Chowdheryetal.,2022)tostabilizethetrainingof7Band30Bmodels. Initially,we
underestimatedtheimportanceofz-lossinourattemptsbecauseitisabsentinmostworksrelevant
tothetrainingof(multimodal)LLMs. However,wefoundthatwithoutthisterm,themagnitudeof
logitssurges,causingthelosstodiverge. Ontheotherhand,withz-loss,weobservethatforthe7B
and30Bmodels,theoptimaltemperatureforinference-timeimagegenerationismuchlowerthan
thatforthe1Bmodel,asthemagnitudeoflogitsissignificantlyreducedinthelargermodels.
LimitationsofChameleon AlthoughmGPTslikeChameleondemonstratepotentialforjointimage
andtextunderstandingwithinonedecoder-onlytransformer,theirimagegenerationabilityremains
inferiortostate-of-the-artdiffusion-basedframeworks(Esseretal.,2024;Chenetal.,2024a;Lietal.,
2024b;Team,2024b;Zhuoetal.,2024)inbothqualityandresolutionflexibility. Moreover,itis
worthnotingthattheimagegenerationabilityisevenabsentintheopen-sourceversionofChameleon.
Additionally, the capabilities of Chameleon are confined to vision-language and text-only tasks,
excludingabroaderrangeofvision-centrictasks. Theseincludeclassicvisualrecognitiontaskssuch
assegmentationanddepthprediction,aswellascreativevisualgenerationtaskslikecontrollable
generationandimageediting. Lumina-mGPTisbuiltuponChameleontounlockitsfullpotentialfor
flexiblephotorealisticimagegenerationandtobecomeaversatilevisiongeneralist.
3.2 LUMINA-MGPT
BuildingonpretrainedMultimodalGenerativePretraining(mGPT)representation,Lumina-mGPT
enablesflexiblephotoelasticimagegenerationaswellasvisionandlanguagetasksunification. To
fully unlock the potential of mGPT, we propose Unambiguous Image Representation (Uni-Rep),
FlexibleProgressiveSupervisedFinetuning(FP-SFT)andOmnipotentSupervisedFinetuning(Omni-
SFT).Uni-Repeliminatestheambiguityintheoriginal2Dshapeoftheflattened1Dimagetokens,
building the basis for image understanding and generation at flexible resolution and aspect ratio.
FP-SFTprogressivelyfinetunesmGPTtogeneratehigher-resolutiondiscreteimagetokensinasimple-
to-hardmanner,whileincorporatingmulti-taskfinetuningtopreventthecatastrophicforgettingoftext
knowledge. AftertheFP-SFTprocess,Lumina-mGPTbecomescapableofgeneratingphotorealistic
6TechnicalReport
VQ-VAE
Codebook
0
1 …
N 19 25 4 37
0 13 32 8
13 34 7 31
<InputImage> <OutputImage>
(Resolution: 512x384)
Resolution-Aware Prompt Unambiguous Image Representation
Generate an image of 512x384
<start-of-image> <end-of-image>
according to the following prompt:
Two dogsare wearing matching
height/width
brown cowboy hats against a blue <end-of-line>
indicator
background.
SOI 384 512 19 25 4 37 EOL 0 13 32 8 EOL 13 34 7 31 EOL EOI
Figure2: IllustrationofResolution-AwarePrompt(bottomleft)andUnambiguousImageRepre-
sentation(bottomright). Thesedesignsareusedinallsupervisedfinetuningstagestoeliminatethe
ambiguityinimagerepresentation,enablingflexibleresolutionimagemodeling.
images at flexible resolutions. Building on the image generation ability from the FP-SFT stage,
we continue to finetune Lumina-mGPT over omnipotent task datasets by tokenizing images and
annotationsfromdenselabeling,spatial-conditionalimagegeneration,andmulti-turneditingdatasets
intodiscretetokens. Allfinetuningstagesadoptthenext-tokenpredictionobjectivetojointlymodel
multimodaltokensandintegratez-loss,similartothepretrainingstage.
3.2.1 EFFECTIVEINITLIZATION
Large-scalepre-trainingandscalablemodelarchitecturehavebeenwidelyverifiedasthegoldenpath
toadvancedintelligence. AsmGPTs,likeChameleon(Team,2024a),arepretrainedonlarge-scale
image-textinterleaveddatasetsandhavedevelopedeffectiveandgeneralizablerepresentationsfor
both image and text, they can better serve as the starting pointing for flexible photorealistic im-
agegenerationandbeyondthanrandominitializationorlanguage-onlymodels. Furthermore,the
LLaMA(Touvronetal.,2023)architecture,whichincorporatesfeatureslikequery-keynormaliza-
tion(Henryetal.,2020)androtarypositionencoding(Suetal.,2024),hasdemonstratedstrength
andscalabilitythroughextensivevalidation(Touvronetal.,2023;Meta,2024;Gaoetal.,2024). By
initializingfromtheChameleonmGPT,whichadherestotheLLaMAarchitecture,wecanleverage
thesearchitecturaladvantages. Consequently,initializingfrommGPTsallowedustoefficientlytrain
high-performingLumina-mGPTmodels,withparametersrangingfrom7Bto30B,usingjust10M
high-qualityimage-textdatapoints.
Weemphasizethatpre-trainingadecoder-onlymultimodalGenerativePreTraining(mGPT)model
hasbeenextensivelyexploredandappliedacrossvariousapplications(Team,2024a;Wangetal.,
2022;Luetal.,2024a). Unliketheseexistingapproaches,whichaimtounifydifferentmodalities
andtasks,theprimarycontributionofLumina-mGPTliesindemonstratingthebenefitsofstarting
withanappropriatemGPTrepresentationratherthanusinglargelanguagemodels(LLMs)orrandom
initialization, especially in text-to-image generation. We believe this finding can inspire future
advancementsinimagegenerationandvisiongeneralistmodels.
3.2.2 SUPERVISEDFINETUNINGFORLUMINA-MGPT
UnambiguousImageRepresentation Existingmethods,suchasChameleon(Team,2024a)and
LlamaGen(Sunetal.,2024),representimagesas1Dflattenedsequencesof2Ddiscreteimagecodes.
Whilethisapproachisadequateforfixedimageresolution,itbecomesambiguouswhentheimage
resolutionissupportedtobevariable,aswithLumina-mGPT.Forinstance,imageswithresolutions
of512×512, 256×1024, and1024×256canallbeencodedintothesamenumberoftokens,
7
QV
redocnE ezitnauQ
QV
redoceDTechnicalReport
makingitimpossibletoinfertheoriginalshapewithoutexaminingthetokencontents. Thisambiguity
posessignificantchallengesforbothimageperceptionandgeneration.
Toaddressthisproblem,weproposeUnambiguousimageRepresentation(Uni-Rep),whichaugments
the image representations by adding extra height/width indicator tokens immediately after the
<start-of-image>tokenandinserting<end-of-line>tokensafterimagetokensbelonging
tothesamerow. AsshowninFigure2,thismodificationensuresthattheoriginalshapeoftheimages
canbeaccuratelyparsedfromthe1Drepresentationwithoutadditionalcontextordelvingintothe
contentsoftheimagetokens. ThisenhancementprovidesthefoundationforLumina-mGPT’sability
toperformimage-relatedtasksatanyresolutionandaspectratio.
Notethatwhileeithertheheight/widthindicatorsorthe<end-of-line>tokenscanindependently
achievedisambiguation,westillusebothsimultaneouslybecausetheyhavedistinctbenefits. When
generatingimages,theheight/widthindicators,generatedbeforeanyimagetokens,pre-determine
theshapeoftheimage,aidingLumina-mGPTincomposingtheimagecontents. Ontheotherhand,
the<end-of-line>tokenscanserveasanchors,offeringthe1Dtokenssequencewithadditional
explicitspatialinformation.
FlexibleProgressiveSupervisedFinetuning(FP-SFT) TheFP-SFTprocessequipsthepretrained
mGPTwiththecapabilitytogeneratehigh-resolutionimageswithflexibleaspectratiosinapro-
gressivemanner. Theprocessisdividedintothreestages,wheretheproductofwidthandheight
approximates5122,7682,and10242,respectively. Ineachstage,asetofcandidateresolutionswith
similarareasbutdifferentheight-widthratiosareprepared,andeachimageismatchedtothemost
suitable resolution. In the low-resolution stage, shorter sequence lengths and the resulting high
trainingthroughputallowthemodeltoquicklytraversealargeamountofdata,learningthegeneral
compositionofimagesandabroadspectrumofvisualconcepts. Conversely,inthehigh-resolution
stage, the model is expected to focus on learning high-frequency fine-grained details unique to
high-resolution images. Benefiting from the strong foundation built during the high-throughput
pretrainingandlow-resolutionfinetuningstages,thelow-throughputhigh-resolutionfinetuningstage
isdata-efficient,therebyenhancingtheoverallefficiencyoftheFP-SFTprocess.
Ameticulouslycurateddatasetofhigh-resolutionphotorealisticimage-textpairsisusedforFP-SFT.
Moreover,thepure-textdatafromOpenHermess(Teknium,2023)andtheimage-to-textdatafrom
Mini-Gemini(Lietal.,2024a)arealsoincorporatedduringtrainingtopreventcatastrophicforgetting.
To provide users with a natural way to specify the desired resolution of generated images, we
developedtheresolution-awareprompt(Figure2). Foreachimageanditscorrespondingdescription,
thepromptisstructuredasfollows:
Generate an image of {width}x{height} according to the following
prompt: \n {description}
OmnipotentSupervisedFinetuning(Omni-SFT) Whileflexiblephotorealisticimagegeneration
istheprimarytargetofLumina-mGPT,wefindthattheresultingmodelafterFP-SFTcanbeefficiently
transferredtoawidespectrumofimageunderstandingandgenerationtasks. WethuspresentOmni-
SFT,apreliminaryexplorationtowardboostingLumina-mGPTtoavisualgeneralist. Trainingdata
forOmni-SFTconsistsofthefollowing:
1. Single-andmulti-turnlanguage-guidedimage-editingfromMagicBrush(Zhangetal.,2024)and
SEED(Geetal.,2024)(onlyinvolvingthereal-worldandmulti-turnsubsets).
2. Densepredictiontasks,includingsurfacenormestimationfromNYUv2Silbermanetal.(2012)
andScanNetDaietal.(2017),depthestimationfromKittiv2Cabonetal.(2020)andSintelButler
etal.(2012),poseestimationfromMSCOCOLinetal.(2014),semanticsegmentationdataannotated
withOneFormer(Jainetal.,2023)onimagefromLaion(Schuhmannetal.,2022),andgrounding
datafromRefCOCO(Kazemzadehetal.,2014).
3. In-housespatial-conditionalimagegenerationdata,withconditionsincludingsurfacenorm,depth,
pose,andsegmentation.
4. AsmallfractionofdatasampledfromthoseusedinthepreviousFP-SFTprocess.
8TechnicalReport
AsdemonstratedinSection4.2,afterOmni-SFT,Lumina-mGPTexhibitsageneralcapabilityfor
completingawiderangeoftasksotherthantext-to-imagegeneration,indicatingthepotentialfor
buildingamultimodalgeneralistalongthisdirection.
Training Setup Though multiple tasks are involved in the SFT process, a unified next-token-
predictionlossisusedforallofthetasks. AsLumina-mGPTisdesignedasachatmodel,alldataare
organizedintosingleormulti-turndialogs,withthelossappliedonlytotheresponseparts. Forall
experiments,theAdamW(Loshchilov&Hutter,2017)optimizerwithweightdecay=0.1andbetas=
(0.9,0.95)isused,andthelearningrateissetto2e-5. Forstabilizingtraining,z-lossisappliedwith
weight1e-5forboththe7Bandthe30Bmodel,andforthe7Bmodeldropoutwithprobability0.05
isadditionallyapplied. Toaccommodatethelargemodelvolume,PyTorchFSDP(Zhaoetal.,2023)
isemployedwithgradientcheckpointing. Toincreasetrainingthroughput,alldataarepre-tokenized
beforetrainingandareclusteredaccordingtothenumberoftokens,ensuringthateachglobalbatch
iscomposedofdatawithsimilarlengths.
3.2.3 INFERENCEFORLUMINA-MGPT
Classfier-FreeGuidance Classifier-FreeGuidance(CFG)(Ho&Salimans,2022;Sanchezetal.,
2023)isoriginallyproposedtoenhancethequalityandtextalignmentofgeneratedsamplesintext-to-
imagediffusionmodels. Weincorporatethistechniqueintoautoregressivemodelsduringinference.
Whengeneratinganimagetoken,theCFG-processedlogitsl areformulatedasl =l+s(l−l′),
cfg cfg
wherelrepresentstheoriginallogitsconditionedonthecompletecontext;l′representsthecontext-
independentlogits,whichareconditionedsolelyonthetokensfollowingthe<start-of-image>
token of thecurrently generating image, and are independentof any prior context; s denotes the
guidancescaleofClassifier-FreeGuidance. TomakeCFGwork,duringtraining,thecontextbefore
<start-of-image>israndomlydroppedbyaprobabilityof10%. Inpractice,KVcachecanbeusedfor
acceleratingthecomputationofbothlandl′. AsshowninSection4.4,CFGhasasignificantimpact
onthegenerationperformanceofLumina-mGPT.
Different Decoding Hyperparameters for Image and Text During inference, the sampling
strategyofautoregressivemodelsinvolvesnumeroushyperparametersthatsignificantlyinfluence
the sampling results. We find that the optimal decoding hyperparameters differ greatly between
textdecodinganddiscreteimagecodedecoding. Forexample,thetop-k=5settingperformswellin
generatingtext. However,whengeneratingimages,thevalueoftop-kshouldbemuchlarger(e.g.
2000)toavoidrepetitiveandmeaninglesspatterns. Therefore,weimplementastatus-awarecontrol
mechanismforinference. Specifically,asetofdefaulthyperparametersisusedfortextdecoding;
oncea<start-of-image>tokenisgenerated,thehyperparametersswitchtothoseoptimized
forimagegeneration. Afterthe<end-of-image>tokenisgenerated,theparametersreverttothe
initialsettings.
4 EXPERIMENTS
4.1 FUNDAMENTALPHOTOREALISTICTEXT-TO-IMAGEGENERATION
Wefirstdemonstratethefundamentaltext-to-imagegenerationcapabilitiesofLumina-mGPTwith
FP-SFT.AsshowninFigure1,Lumina-mGPTcangeneratephotorealisticimagesinavarietyof
resolutions, achieving the first native 1K autoregressive generation without model cascaden (Yu
etal.,2022;Sahariaetal.,2022;Changetal.,2023;Perniasetal.,2024),acommontechniquein
text-to-magegeneration. Thesegeneratedimagesexhibitstrongsemanticcoherencewithintricate
visualdetails,despitebeingfinetunedonlimitedcomputationalresourcesandtext-imagepairs.
CompaisonwithSoTAAR-basedApproaches Wecomparethetext-to-imagesynthesisability
ofLumina-mGPTwithLlamaGen(Sunetal.,2024)andParti(Yuetal.,2022). LlamaGenbeats
state-of-the-art diffusion models on ImageNet FID score. Compared with LlamaGen, Lumina-
mGPTcanachievebettervisualqualityasshowninFigure3intext-to-imagegeneration. Notethat
Lumina-mGPTonlyrequires10Mimage-textpairswhileLlamaGenistrainedover50Mlow-quality
image-textpairsaccompaniedby10Min-houseaestheticimage-textpairs. ComparedtoParti,an
ARtext-to-imagemodelwith20billionparameters,Lumina-mGPTalsodemonstratesbettervisual
9TechnicalReport
n
e
G
a
m
a
lL
T
P
G
m
-
a
n
im
u
L
ti
r
a
P
T
P
G
m
-
a
n
im
u
L
Figure 3: Qualitative comparison with LlamaGen and Parti. Lumina-mGPT can generate more
photorealisticimageswithhighaestheticsandfine-graineddetails.
qualityandaesthetics. However,duetosignificantdifferencesincomputationalcostsandtraining
datasets,Lumina-mGPTdemonstratesinferiortextinstructionfollowingabilitycomparedtoParti. In
addition,neitherLlamaGennorPartisupportstheend-to-endgenerationof1Kresolutionimageswith
arbitraryaspectratios,asachievedbyLumina-mGPT.LlamaGenonlysupportsafixedresolutionof
512×512,whilePartigenerates1024×1024imagesusinganadditionalsuper-resolutionupsampler.
OntheEffectivenessofFP-SFT TofurthervalidatetheeffectivenessofFP-SFT,wevisualize
theimagesgeneratedatdifferentfinetuningstagesinFigure4. Withincreasingimageresolution,
weobserveaprogressivedecreaseinvisualartifactsintroducedbyVQ-VAEandtheemergenceof
diversefine-grainedvisualdetails. Fromtheseillustrations,wecanconcludethatourFP-SFTcan
unleashthepotentialofgeneratinghigh-qualityimagesfrommGPTinaprogressivemanner.
4.2 OMNIPOTENTTASKUNIFICATIONWITHLUMINA-MGPT
ByperformingOmni-SFToverFP-SFT,Lumina-mGPTcanperformamultitudeofcapabilities,which
canbecategorizedintotext-onlymulti-turndialog,visualmulti-turndialog,multi-turnimageediting,
denselabeling,andspatial-conditionalimagesynthesis. Tointuitivelyillustratethesecapabilities,
10TechnicalReport
512x512 768x768 1024x1024
Figure 4: Samples with zoom-ins generated by Lumina-mGPT in different resolution finetuning
stages. Thevisualdetailscontinuouslyimprovealongwiththeprogressivelyincreasingresolution.
wequalitativelyvisualizehowvarioustypesofdownstreamtaskscanbeseamlesslyintegratedinto
Lumina-mGPTinFigure12to16.
First, Lumina-mGPT supports general text-only tasks in LLMs, such as solving math problems,
coding,andcommonsensereasoning,thankstotheextensivepertaininginChameleonandourmulti-
taskfinetuningtomitigatecatastropheforgetting. Forinstance,Lumina-mGPTcorrectlyanswers
“Which is bigger? 9.9 or 9.11”,asshowninFigure12,aquestionthathasconfused
almostallexistingLLMsincludingGPT-4(Achiametal.,2023)andGemini(Teametal.,2023).
AsdemonstratedinFigure13,Lumina-mGPTisalsocapableofhandlingvariousvision-language
tasksincludingimagecaptioning,visualquestionanswering,andgeneralmulti-turnmulti-image
dialog. Asavisualgeneralist,itisimportanttoincorporateclassicvisualrecognitiontasks. Using
naturallanguageasaunifiedinterface,Lumina-mGPTcanperformmultiplecomputervisiontasks
includingimagesegmentation,poseestimation,depthestimation,surfacenormalestimation,and
referringobjectdetection. SeeFigure14and15forexamples.
Beyondphotorealisticimagegeneration,itisalsovitaltoaddadditionalcontroltothetext-to-image
generation model. Demonstrated in Figure 15 and 16, Lumina-mGPT supports versatile spatial
control, includingdepthmaps, segmentationmaps, normalmaps, andhumanposes, toguidethe
generation of target images. Given the above examples, though preliminary, they showcase that
Lumina-mGPTcaneffectivelyfollowdiverseinstructions,indicatingitspromisingpotentialasa
unificationofvariouschallengingtasksinoneframework.
4.3 COMPARISONWITHDIFFUSION-BASEDAPPRAOCHES
Foralongperiodoftime,diffusionmodelshavedominatedthefieldoftext-to-imagegeneration
compared to autoregressive models. Although LlamaGen claims to beat diffusion models, their
resultsarelimitedtotheImageNetbenchmarkandtherehasbeennodirectcomparisonbetweenthese
twoarchitecturessofar. Inthissection,weaimtoprovideadetailedcomparisonofautoregressive
anddiffusion-basedmethodstrainedonthesesametext-imagedatasets,focusingonimagequality,
diversity,text-rendering,andmultilingualcapabilities. Specifically,weadoptLumina-mGPTand
Lumina-Next-SFT (Zhuo et al., 2024) as representatives of autoregressive and diffusion-based
methods,respectively. AdirectvisualcomparisonbetweenLumina-Next-SFTandLumina-mGPT
revealsboththesimilaritiesanddifferencesbetweenautoregressiveanddiffusion-basedgenerative
modelingapproaches.
On the Similarity between Diffusion- and AR-based Generation Given the same set of text
prompts, both diffusion- and AR-based approaches generate photorealistic images with similar
aesthetic style and fine-grained details, illustrated in Figure 5. This reveals the fact that both
11TechnicalReport
Seed: 1 Seed: 2 Seed: 3 Seed: 4
T
F
S
-
t
x
e
N
-
a
n
im
u
L
T
P
G
m
-
a
n
im
u
L
T
F
S
-
t
x
e
N
-
a
n
im
u
L
T
P
G
m
-
a
n
im
u
L
Figure5: ComparisonbetweenLumina-Next-SFTandLumina-mGPTusingdifferentrandomseeds.
ImagesgeneratedbyLumina-mGPTexhibitcomparableaestheticswithgreaterdiversity.
architecturescanachievesatisfactorytext-to-imagegenerationperformancewhenprovidedwiththe
sametrainingdata,trainingbudget,andcomparablemodelsizes. TheAR-basedmethodsdisplay
remarkable visual aesthetics on par with their diffusion counterparts, challenging the notion that
diffusionmodelsaremoreeffectiveandpromisingarchitectureingenerativemodeling. Thisfinding
alsoalignswiththeplatonicrepresentationhypothesis(Huhetal.,2024)thatneuralnetworksare
convergingtolearnasharedrepresentationspacedespitebeingtrainedwithdifferentarchitectures
andobjectives. Therefore,thishypothesishighlightstheimportanceofcollectingmorehigh-quality
data and optimizing training infrastructure as directions for data and model scaling, to boost the
overallmodelperformancethatisagnostictoanyspecificarchitecture.
OntheDifferencesbetweenDiffusion-andAR-basedGenerationDespitethevisualsimilarities,
there are notable differences between diffusion- and AR-based methods. As shown in Figure 5,
Lumina-mGPT exhibits more diversity using different random seeds, while Lumina-Next-SFT
generatessimilarimageswithidenticallayoutsandtextures. Thiscanbepartlyattributedtotheuse
ofhightemperatureandtop-kvaluesinLumina-mGPT.However,excessivediversityalsocausesour
modeltobelessstableandmorepronetoproducingvisualartifacts,whichisdiscussedinSection4.7.
12TechnicalReport
T
F
S
-
t
x
e
N
-
a
n
im
u
L
T
P
G
m
-
a
n
im
u
L
Figure6: TextrenderingcomparisonbetweenLumina-Next-SFTandLumina-mGPT.Fromleftto
right,thecorrecttextstoberenderedontheimageare: “Bloom”,“Hope”,“Brew”,and“Joy”.
T
F
S
-
t
x
e
N
-
a
n
im
u2
L-
R
iA
-
a
n
im
u
TL
P
G
m
-
a
n
im
u
L
3
-
R
iA
a- prompt1 prompt2 prompt3 prompt4
n
im
u prompt1: is wearing .
L
Prompt2: is sleeping on .
prompt3: 日落时分的海滩，阳光透过云层在海面上撒下一片金色的光芒，海浪轻轻拍打着沙滩，远处
的帆船在余晖中若隐若现。
prompt4: 森林中的清晨，阳光穿过茂密的树冠，形成斑驳的光影，地上覆盖着厚厚的落叶，几只小鹿
在林间悠闲地觅食。
Figure7:EmojiandmultilingualinstructionunderstandingabilitycomparisonbetweenLumina-Next-
SFTandLumina-mGPT.Lumina-mGPTstruggletounderstandemojisandmultilingualprompts.
is wearing .
WealsocomparethetextrenderingandmultilingualunderstandingcapabilitiesbetweenLumina-
mGPTandLumina-Next-SFT.AsillustratedinFigure6,Lumina-mGPTexhibitssignificantlybetter
text synthesizing results, while Lumina-Next-SFT struggles to generate any complete character.
WearguethatthisunderscorestheimportanceofmGPT,wherethemodellearnsaseamlessmulti-
modalrepresentationbetweentextandimagesusingmassiveinterleaveddataduringthepertaining
13TechnicalReport
T=0.7 T=0.8 T=0.9 T=1.0
0
.
4
=
G
F
C
,
0
0
0
2
=
k
-
p
o
T
Top-k=50 Top-k=500 Top-k=2000 Top-k=8192
0
.
4
=
G
F
C
,
0
.
1
=
T
CFG=1.0 CFG=2.0 CFG=4.0 CFG=8.0
0
0
0
2
=
k
-
p
o
T
,
0
.
1
=
T
Figure8: SamplesgeneratedbyLumina-mGPTusingdifferentdecodingconfigs. Increasingtempera-
ture,top-k,andguidancescaleallleadtoimprovedvisualdetails.
stage. However,whenitcomestomultilingualunderstanding,Lumina-mGPTperformsworsethan
Lumina-Next-SFTintermsofemojiandChinesepromptsshowninFigure7. Thereasonisthat
although Lumina-mGPT learns better text-image alignment, the lack of multilingual text corpus
usedinpertaininglimitsitsperformance. Incontrast,thetextencoderusedinLumina-Next-SFT
showcases significantly stronger multilingual capabilities than Chameleon. Hence, we hope that
by comprehensively enhancing the capabilities of the base mGPT model, such as adding more
multilingualdata,Lumina-mGPTcanbenefitinalldownstreamtasks.
Inadditiontotext-to-imagegeneration,Lumina-mGPTsupportsvariousvisionandlanguagetasks
withinaunifiedframework. However,thedesignofdiffusionmodelslimitstheircompatibilityand
performanceacrossmultiplemodalitiesandtasks.Theyoftenrequirespecificarchitecturedesignsand
additionaltrainingforeachunseentask(Keetal.,2024;Xuetal.,2023). Incontrast,Lumina-mGPT
treatsinputfromallmodalitiesasmultimodaltokensequencesandleveragesnaturallanguageasthe
interfacetounifydiversetaskswithnext-tokenprediction.
4.4 DECODINGCONFIGURATIONMATTERS
Inautoregressivemodels,variousconfigurationparametersduringLumina-mGPT’sdecodingstage
significantlyaffectsamplequality(Holtzmanetal.,2020;Radfordetal.,2018;2019). Hyperparame-
terssuchastemperature(T),top-k,andclassifier-freeguidancescale(CFG)havenotbeenextensively
investigatedinthevisualdomain. Inthissection,weexplorehowthesehyperparametersinfluence
thegeneratedimageintermsofquality,texture,andstyle.
InfluenceofTemperatureToevaluatetheeffectofthesedecodingparameters,wefirstsetastandard
decodingconfiguration: T=1.0,Top-k=2000,CFG=4.0,whichservesasagood-to-usesetting. From
14TechnicalReport
e
g
am
I
lanig
irO
E
A
V
Q-
V
T
PG
m
anim-
uL
E
A
V
L
X
D-
S
Figure 9: Reconstruction quality comparison between different methods. Lumina-mGPT can be
instructedtorefinetheencodedVQcodes,leadingtobetterreconstructedimages.
this baseline, we gradually shift T from 0.7 to 1.0 to generate corresponding images at different
temperatures. The results are shown in Figure8. It is evident that when setting the temperature
low, visual details diminish and objects tend to be over-smoothed. Conversely, when setting the
temperaturehigh,thegeneratedimagescontainrichvisualcontentbutarepronetocontainmore
artifacts.
InfluenceofTop-kBasedonthestandarddecodingsetting,wevarythetop-kvalue,from50to8192,
where8192isequaltothesizeoftheVQ-VAEcodebookusedusedinLumina-mGPT.Theresults,
visualizedinFigure8,indicateasimilartrendwithincreasingtemperature. Whentop-kislow,the
imagecontentandtexturearerelativelysimple,exhibitingtheover-smoothedproblemaswell. When
top-kissethigh,theimagedetailandtexturearediverse,makingitmoreaestheticallyappealing,
whileincreasingthepotentialofartifacts.
Influence of Classifier-Free Guidance CFG is extremely important in text-to-image diffusion
models(Ho&Salimans,2022;Sanchezetal.,2023). Toverifyitseffectivenessinautoregressive
generation,wechangetheCFGvaluefrom1.0to8.0. AsshowninFigure8,withtheCFGincreasing,
thequalityofgeneratedimagesimproves,provingtheeffectivenessoftheclassifier-freeguidancein
thiscontext.
15TechnicalReport
prompt tokens
image tokens
n
o
it
n
e
t
t
A
SOI EOL
Postion of 1D tokens
Figure10: Visualizationofaveragedattentionlogitsfromthelastimagetoken. Weobserve“attention
sinks”(Xiaoetal.,2024),whereaddedindicatortokensallocatealargeproportionofattentionscore.
4.5 LUMINA-MGPTASVQCODESREFINER
Itiswell-knownthatbothVQ-VAE(VanDenOordetal.,2017;Razavietal.,2019;Esseretal.,
2021)andVAE(Kingma&Welling,2013)compressimagesintolatentrepresentationsatthecost
of information loss, which introduces quality degradation at reconstruction, especially for high-
frequencydetailssuchasedges,hair,andtext. Thisevitablelossupperboundsthequalityofimage
generationforalllatentspacegenerativemodels(Esseretal.,2024;Yuetal.,2022;Sunetal.,2024).
However,wesurprisinglyfoundthatLumina-mGPTcanimprovethediscreteimagetokensencoded
by VQ-VAE using a simple “No edit. <|image|>” instruction, then Lumina-mGPT can
generateasequenceofrefinedimagetokensthatcanbedecodedintoabetterimage.
WecomparethereconstructionqualityofVQ-VAEusedinChameleon,Chameleon’sVQ-VAEusing
VQ codes refined by Lumina-mGPT, and the SDXL VAE (Podell et al., 2023) in Figure 9. The
SDXLVAEshowcasessignificantlybetterreconstructionperformancecomparedtoChameleon’s
VQ-VAE.Thisadvantagealsoexplainswhydiffusion-basedmethodscanbeatautoregressivemodels
intext-to-imagegeneration. However,afterapplyingtheaforementionedsimpletricktorefinethe
VQcodesusingLumina-mGPT,thequalityofreconstructedimagesexhibitsremarkablegainsin
bothfine-grainedvisualdetailsandtextrendering,onparwiththeSDXLVAE.Wehypothesizethat
Lumina-mGPTlearnstheunderlyingdistributionofrealimagetokensduringlarge-scalepertaining
andhigh-qualityFP-SFT.TheOmni-SFTfurthertriggerstheemergenceofsuchzero-shotcapability
aftertrainingonvarioustasksandinstructions. Webelievethiszero-shotcapabilitytobridgethe
representationgapbetweentheencoderanddecoderinVQ-VAEsuggestspromisingdirectionsto
improvethecurrentparadigmoftwo-stagelatentgenerativemodeling. Forexample,wecanleverage
Lumina-mGPTasateachertorefinetheVQcodesforstudenttrainingortodesigninference-time
techniquesbymanipulatingthetokenlogitssimilarlytoclassifier-freeguidance.
4.6 ATTENTIONVISUALIZATION
TobetterunderstandthesamplingbehaviorofLumina-mGPT,wevisualizetheaverageattention
logitsofthelastimagetokenduringtext-to-imagegeneration,asshowninFigure10. Theresults
reveal that the attention score decreases for distant tokens, indicatin that the model pays more
attentiontolocaltokenscomparedtodistantones. Thisbehavioralignswiththelong-termdecay
property designed in RoPE. Besides, we observe a similar pattern known as “attention sinks” in
LLMs(Xiaoetal.,2024),wherealargeproportionofattentionscoreisallocatedtoasmallnumber
oftokens. Afterlookingintodetails,thesesinktokensareidentifiedtobetheindicators,includingthe
<start-of-image>and<end-of-line>tokens. Notably,theattentionscoresoftexttokens
aresignificantlysmallerthanthoseofthe<start-of-image>token. Thissuggeststhatmostof
thesemanticinformationintexttokensmayhavebeenencapsulatedintothe<start-of-image>
token. TheseintriguingfindingsindicatethatLumina-mGPTaggregatemoreinformationfromthese
sinktokens,demonstratingtheeffectivenessoftheseindicatorsinourproposedUni-Repandopening
upthepotentialforacceleratingthesamplingprocesswiththesesinktokens.
16TechnicalReport
n
o
it
a
r
e
n
e
G
e
g
a
m
I
Depth Detection
g
n
ile
b
a
L
e
s
n
e
D
Surface Normal Human Pose
n
o
it
a
r
e
n
e
G
e
lb
a
llo
r
t
n
o
C
Figure11: FailurecasesofcurrentLumina-mGPT.Duetoinadequatetrainingandlimiteddatasize,
Lumina-mGPTsometimesstrugglestounderstandinputconditionsandproducevisualartifacts.
4.7 LIMITATIONS
FailureCases DespiteLumina-mGPTcangeneratephotorealisticimages,itsometimesproduces
images with noticeable visual artifacts. For example, Lumina-mGPT may generate people and
animalswithunreasonablelimbs,asshowninthefirstrowofFigure11. Besides,comparedtoSoTA
text-to-imagegenerationapproachesincludingSD3(Esseretal.,2024),Kolors(Team,2024b),and
HunyuanDiT(Lietal.,2024b),allpretrainedover1Bimage-textpairs,Lumina-mGPT’sprompt-
followingabilityisinferiorduetothelimitedtrainingresourcesanddatasize,whicharemanytimes
smallerthantheseSoTAmethods. Regardingdenselabelingandcontrollablegeneration,Lumina-
mGPTcurrentlyshowcasespreliminaryresultswithalimitedtrainingbudget. Hence,thesecond
rowinFigure11providessuchanexamplewhereLumina-mGPTproducesinaccuratepredictions
orsemanticallyinconsistentimages,failingtounderstandthegivenimageconditions. Therefore,
weexpectbyscalingdatasizewithmorecomputationalresources,Lumina-mGPTcaneffectively
addresstheabovefailurecasessuchasinadequateinstruction-followingabilityandvisualartifacts.
GenerationSpeed Autoregressivemodelsrequirenumerousnetworkevaluationsduringinference
due to the nature of next-token prediction, similar to the iterative denoising process in diffusion
models. Thisbecomesworsewhengeneratinghigh-resolutionimages,whichoftenrequireminutes
togenerateafullsequenceofimagetokens,significantlyslowerthancurrentdiffusionmodelswith
advancedsamplers. However,therehavebeenplentyoftechniquestooptimizetheinferencespeed
designedforautoregressivemodels,suchasvLLM(Kwonetal.,2023)andFlashAttention(Dao
etal.,2022;Dao,2024). Webelievethatbyintegratingtheseapproachesinthefuture,Lumina-mGPT
canachievearemarkablespeedupduringinference.
VQ-VAEReconstructionQuality VQ-VAEisemployedastheimagetokenizertoconvertcontinu-
ousimagesintodiscretetokenrepresentations. Meanwhile,italsointroducesinformationbottlenecks
17TechnicalReport
bycompressingthespatialdimensionsofimages. Asaresult,thereconstructionqualityofVQ-VAE
largelydeterminestheupperlimitofgenerationquality. WediscoverthattheVQ-VAEproposed
inChameleonsometimesstrugglestoreconstructhigh-frequencydetails,especiallywhentextand
human faces are present in images. Incorporating further improvements on VQ-VAE, such as
FSQ(Mentzeretal.,2023),mayalsoenhancethegenerationqualityofLumina-mGPT.
5 CONCOLUSION
Inthiswork,weintroduceLumina-mGPT,adecoder-onlytransformerthatcanproducediverse,photo-
realisticimagesatanyresolutionfromtextprompts. Insteadofrandominitialization,Lumina-mGPT
features initializing from an autoregressive transformer with multimodal Generative PreTraining
(mGPT).Leveragingthegeneralmultimodalrepresentationlearnedfrommassiveinterleaveddata,
wedesigntwofinetuningstrategiesnamedFP-SFTandOmni-SFTtounleashthepotentialofmGPT
ontext-to-imagegenerationandomnipotenttaskunification,respectively. WedemonstrateLumina-
mGPT’sbroadmultimodalcapabilitiesacrossawiderangeoftasks,highlightingitshigh-quality
text-to-imagegenerationinside-by-sidecomparisonwithdiffusionmodels.
18TechnicalReport
1. Multi-Turn Dialog
I'm planning a trip to Paris.
That sounds exciting! Paris is a beautiful city with a rich history
and culture. Have you considered checking the weather, flight
deals, and any specific activities you're interested in while
planning your trip?
No, I have not. Can you do some planning for me?
Of course! Here's a general outline of some things you can include in your itinerary when planning your trip to
Paris:
1. Visit iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and Louvre Museum. These are all
must-see attractions and offer a glimpse into Paris's architectural prowess and artistic heritage.
2. Take a romantic walk along the Seine River, which is a picturesque and romantic way to explore the city's
central area. Stroll along the riverside, enjoying the scenery and the gentle sound of the water flowing nearby.
3. Explore Montmartre, one of Paris' most famous neighborhoods. With its narrow streets, colorful houses, and
numerous art galleries, Montmartre is a must-visit destination for art and culture enthusiasts.
4. Take a food tour, as Paris is known for its delectable cuisine. Many of the city's restaurants offer a mix of
traditional and modern dishes, and you can try traditional Parisian snacks and drinks at cafes and bakeries.
5. Visit the Musée de l'Orangerie, which is home to the famous Monet painting "Water Lilies." This museum is
located in a beautiful, park-like setting with plenty of outdoor seating options, allowing you to enjoy the art in a
more relaxed and enjoyable atmosphere.
6. Experience a traditional Parisian night out, with its lively bars, vibrant nightclubs, and numerous music and
entertainment venues. Paris is known for its late-night scene and its bustling club culture.
7. Take a guided tour of the Palace of Versailles, which is located just outside the city and about 20 miles east
of Paris. This majestic estate is an excellent day trip from the city and offers a chance to explore the opulent
and extravagant lifestyles of the French aristocracy.
Remember to research flight deals, local transportation options, and activities that interest you before
embarking on your journey to Paris. Good luck and have a great trip!
2. Math
Which is bigger? 9.9 or 9.11
The bigger value between 9.9 and 9.11 is 9.9.
A bat and a ball together cost $1.10. The bat costs $1.00 more than
the ball. How much does the ball cost? Let's think step by step.
Step 1: Identify the given information
We know that the total cost of the bat and the ball combined is $1.10. We also know that the bat costs $1.00
more than the ball.
Step 2: Create an equation
Let's denote the cost of the ball as 'x' (since it's the unknown variable). Then, the cost of the bat is 'x + $1.00'
(since it's $1.00 more than the ball).
Step 3: Use the given information to form an equation
Using the given information, we can create an equation:
(x + $1.00) + x = $1.10
Step 4: Simplify and solve for the unknown variable 'x'
Combining the terms with 'x', we get:
2x + $1.00 = $1.10
2x + $1.00 - $1.00 = $1.10 - $1.00
2x = $0.10
Step 5: Solve for the unknown variable 'x'
To find the cost of the ball (x), we need to divide both sides of the equation by 2:
x = $0.10 / 2
x = $0.05
Step 6: State the conclusion
The ball costs $0.05, which is 5 cents.
3. Coding
Write a python script, implementing a sorting algorithm.
Here is a simple sorting algorithm implemented in Python:
def sort_list(list):
sorted_list = list
for i in range(len(sorted_list) - 1):
min_index = i
for j in range(i + 1, len(sorted_list)):
if sorted_list[min_index] > sorted_list[j]:
min_index = j
sorted_list[i], sorted_list[min_index] = sorted_list[min_index], sorted_list[i]
return sorted_list
list = [3,6,8,1,7]
print(sort_list(list))
The sort_list function implements the selection sort algorithm. This is one of the simplest sorting algorithms,
but it can be quite slow and inefficient for larger datasets. It works by finding the minimum element in an
unsorted part of the list and swapping it with the first unsorted element. This process continues until the entire
list is sorted.
Figure12: Lumina-mGPTasageneralistforvariousmultimodaltasks.
19TechnicalReport
4. Flexible Photorealistic Image Generation
“Generate an image of 1280x800 according to the following prompt: A serene alpine meadow in
spring: The meadow is a carpet of vibrant green, dotted with colorful wildflowers in full bloom.
Snow-capped mountains provide a stunning backdrop, their peaks glittering in the bright sunlight. “<|image|>”
A clear, babbling brook winds through the meadow, adding to the scene's tranquility”.
“Generate an image of 640x1536 according to the following prompt: A serene alpine meadow in
s pp or win eg r: fA u lr ,o ic sk se ht o l wau nn ac gh ai in ng s tin at o b as cp ka dce ro, pc a op ft u ar te wd i la igt ht th e sk p ye . a Fk l ao mf ei sts a na dsc se mnt o: k T e h te r ar io l c bk ee ht in, dsl e ite k a sa n itd “<|image|>”
pierces through the atmosphere. The intense light from the engines illuminates the scene,
creating a breathtaking contrast with the darkening sky.”
5. Image Caption
“Describe this image in detail.<|image|>”
“The image captures a moment on a city street that appears to be in a Southeast Asian country, likely
Vietnam, based on the script on the signs, which is in Devanagari script used for several languages in
India, but also used in Southeast Asia, particularly in Vietnam. The presence of English text on the signs
indicates that this is a region frequented by tourists or by locals familiar with Western languages. The
scene is bustling with a diverse array of individuals and goods, indicating a market or marketplace. People
are seen engaged in various activities, such as walking and handling items for sale, which contributes to
the impression of a busy urban environment. The image's composition shows a snapshot of daily life,
illustrating the interaction between people and their surroundings in this urban setting.”
6. Visual Question Answering
“What might happen in this image in the next second?<|image|>”
“In the next second, the photographer is likely to continue taking photos of this moment. The
crocodile and its open mouth are positioned to create an appealing composition for a shot. Given
the open mouth and visible teeth, the photographer might capture a more dramatic and intense
image by positioning themselves and the crocodile closer to the camera, aiming to include more
of the crocodile's head in the frame. The next photograph could potentially feature the
crocodile with its mouth open wide to mimic the one in the first image, creating a sense of
continuity and action in the scene.”
7. Multi-Turn Multi-Image Dialog
“Generate an image of 768x768 according to the following prompt: “<image|>”
A piece of porcelain is placed on the table.”
“What if I use the tool shown in this image to hit the porcelain?<|image|>”
“Based on the image, the porcelain teapot with its intricate floral designs and
golden trim is shown in a stationary position. However, the tool depicted—a red,
metallic hammer with a black rubber grip—is designed for striking purposes. If
used to hit the teapot, the hammer's weighty metal head would likely cause the
teapot to shatter or chip due to its fragile nature, especially if struck with any
force applied from above as a hammer would. “
8. Image Editing
“<image|>”
“Generate an image of 512x512 according to the following prompt:
A cup of coffee is placed on the table.”
“<image|>”
“Remove the cup in the background.”
“<image|>”
“Add a spoon.”
Figure13: Lumina-mGPTasageneralistforvariousmultimodaltasks.
20TechnicalReport
9. Image Segmentation
“Semantic segmentation. <|image|>”
“<|image|>”
10. Pose Estimation
“Human pose estimation. <|image|>”
“<|image|>”
Human pose estimation.
11. Depth Estimation
“Depth estimation. <|image|>”
“<|image|>”
12. Surface Normal Estimation
“Surface normal estimation. <|image|>”
“<|image|>”
Figure14: Lumina-mGPTasageneralistforvariousmultimodaltasks.
21TechnicalReport
13. Referring Object Detection
“Detect: The red apple. <|image|>”
“<|image|>”
“Detect: The green apple. <|image|>”
“<|image|>”
14. Depth Conditioned Image Generation
“Generate an image according to the provided image, and
according to the following caption: “<|image|>”
An ice cream with pink color. <|image|>”
Human pose estimation.
“Generate an image according to the provided image, and
according to the following caption: “<|image|>”
An ice cream with vanilla flavor. <|image|>”
“Generate an image according to the provided image, and
according to the following caption: “<|image|>”
An ice cream with golden textured background. <|image|>”
Figure15: Lumina-mGPTasageneralistforvariousmultimodaltasks.
22TechnicalReport
15. Segmentation Conditioned Image Generation
“Generate an image according to the provided image, and
according to the following caption:
A butterfly is flying in the forest. <|image|>”
“<|image|>”
16. Surface Normal Conditioned Image Generation
“Generate an image according to the provided image, and
according to the following caption:
A birthday cake. <|image|>”
“<|image|>”
17. Pose conditioned Image Generation
“Generate an image according to the provided image, and
according to the following caption:
A woman is holding a bag. <|image|>”
“<|image|>”
“Generate an image according to the provided image, and
according to the following caption:
A woman is holding a bag, wearing a hat. <|image|>”
“<|image|>”
Figure16: Lumina-mGPTasageneralistforvariousmultimodaltasks.
23TechnicalReport
REFERENCES
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,
YuHan,FeiHuang,etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023.
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. Computer
Science.https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023.
XiaoBi,DeliChen,GuantingChen,ShanhuangChen,DamaiDai,ChengqiDeng,HonghuiDing,
KaiDong,QiushiDu,ZheFu,etal. Deepseekllm: Scalingopen-sourcelanguagemodelswith
longtermism. arXivpreprintarXiv:2401.02954,2024.
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe
Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video
generation models as world simulators. 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. AdvancesinNeuralInformationProcessingSystems,33:1877–1901,2020.
DanielJButler,JonasWulff,GarrettBStanley,andMichaelJBlack. Anaturalisticopensource
movieforopticalflowevaluation. InComputerVision–ECCV2012: 12thEuropeanConference
onComputerVision,Florence,Italy,October7-13,2012,Proceedings,PartVI12,pp.611–625.
Springer,2012.
Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint
arXiv:2001.10773,2020.
NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKirillov,andSergey
Zagoruyko. End-to-endobjectdetectionwithtransformers. InEuropeanconferenceoncomputer
vision,pp.213–229.Springer,2020.
HuiwenChang,HanZhang,JarredBarber,AaronMaschinot,JoseLezama,LuJiang,Ming-Hsuan
Yang,KevinPatrickMurphy,WilliamTFreeman,MichaelRubinstein,etal. Muse: Text-to-image
generationviamaskedgenerativetransformers. InInternationalConferenceonMachineLearning,
pp.4055–4075.PMLR,2023.
Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang,
PingLuo, HuchuanLu, andZhenguoLi. Pixart-\sigma: Weak-to-strongtrainingofdiffusion
transformerfor4ktext-to-imagegeneration. arXivpreprintarXiv:2403.04692,2024a.
JunsongChen,JinchengYU,ChongjianGE,LeweiYao,EnzeXie,ZhongdaoWang,JamesKwok,
PingLuo,HuchuanLu,andZhenguoLi. Pixart-$\alpha$: Fasttrainingofdiffusiontransformer
forphotorealistictext-to-imagesynthesis. InTheTwelfthInternationalConferenceonLearning
Representations,2024b. URLhttps://openreview.net/forum?id=eAKmQPe3m1.
TingChen,SaurabhSaxena,LalaLi,Tsung-YiLin,DavidJFleet,andGeoffreyEHinton. Aunified
sequence interface for vision tasks. Advances in Neural Information Processing Systems, 35:
31333–31346,2022.
ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligning
for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pp.24185–24198,2024c.
24TechnicalReport
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,ParkerSchuh,
KensenShi,SashaTsvyashchenko,JoshuaMaynez,AbhishekRao,ParkerBarnes,YiTay,Noam
Shazeer,VinodkumarPrabhakaran,EmilyReif,NanDu,BenHutchinson,ReinerPope,James
Bradbury,JacobAustin,MichaelIsard,GuyGur-Ari,PengchengYin,TojuDuke,AnselmLev-
skaya,SanjayGhemawat,SunipaDev,HenrykMichalewski,XavierGarcia,VedantMisra,Kevin
Robinson,LiamFedus,DennyZhou,DaphneIppolito,DavidLuan,HyeontaekLim,BarretZoph,
AlexanderSpiridonov,RyanSepassi,DavidDohan,ShivaniAgrawal,MarkOmernick,AndrewM.
Dai,ThanumalayanSankaranarayanaPillai,MariePellat,AitorLewkowycz,EricaMoreira,Rewon
Child,OleksandrPolozov,KatherineLee,ZongweiZhou,XuezhiWang,BrennanSaeta,Mark
Diaz,OrhanFirat,MicheleCatasta,JasonWei,KathyMeier-Hellstern,DouglasEck,JeffDean,
SlavPetrov,andNoahFiedel. PaLM:Scalinglanguagemodelingwithpathways. arXivpreprint
arXiv:2204.02311,2022.
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nießner. Scannet: Richly-annotated3dreconstructionsofindoorscenes. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pp.5828–5839,2017.
Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In
InternationalConferenceonLearningRepresentations(ICLR),2024.
TriDao,DanielY.Fu,StefanoErmon,AtriRudra,andChristopherRé. FlashAttention: Fastand
memory-efficientexactattentionwithIO-awareness.InAdvancesinNeuralInformationProcessing
Systems(NeurIPS),2022.
JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-scale
hierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pp.248–255.Ieee,2009.
MingDing,ZhuoyiYang,WenyiHong,WendiZheng,ChangZhou,DaYin,JunyangLin,XuZou,
ZhouShao,HongxiaYang,etal. Cogview: Masteringtext-to-imagegenerationviatransformers.
Advancesinneuralinformationprocessingsystems,34:19822–19835,2021.
RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,ZhengGe,JinrongYang,LiangZhao,Jianjian
Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi.
DreamLLM:Synergisticmultimodalcomprehensionandcreation. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024. URLhttps://openreview.net/forum?
id=y01KGvd9Bw.
RuoyiDu, DongliangChang, TimothyHospedales, Yi-ZheSong, andZhanyuMa. Demofusion:
Democratisinghigh-resolutionimagegenerationwithno$$$. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.6159–6168,2024.
PatrickEsser,RobinRombach,andBjornOmmer. Tamingtransformersforhigh-resolutionimage
synthesis. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pp.12873–12883,2021.
PatrickEsser,SumithKulal,AndreasBlattmann,RahimEntezari,JonasMüller,HarrySaini,Yam
Levi,DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersfor
high-resolutionimagesynthesis. InForty-firstInternationalConferenceonMachineLearning,
2024.
PengGao,LeZhuo,ZiyiLin,ChrisLiu,JunsongChen,RuoyiDu,EnzeXie,XuLuo,LongtianQiu,
YuhangZhang,etal. Lumina-t2x: Transformingtextintoanymodality,resolution,andduration
viaflow-basedlargediffusiontransformers. arXivpreprintarXiv:2405.05945,2024.
YuyingGe,SijieZhao,ChenLi,YixiaoGe,andYingShan. Seed-data-edittechnicalreport: Ahybrid
datasetforinstructionalimageediting. arXivpreprintarXiv:2405.04007,2024.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Identitymappingsindeepresidualnet-
works. InComputerVision–ECCV2016:14thEuropeanConference,Amsterdam,TheNetherlands,
October11–14,2016,Proceedings,PartIV14,pp.630–645.Springer,2016.
25TechnicalReport
AlexHenry,PrudhviRajDachapally,ShubhamPawar,andYuxuanChen. Query-keynormalization
fortransformers. arXivpreprintarXiv:2010.04245,2020.
JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecuriouscaseofneuraltext
degeneration. InInternationalConferenceonLearningRepresentations, 2020. URLhttps:
//openreview.net/forum?id=rygGQyrFvH.
Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation
hypothesis. InInternationalConferenceonMachineLearning,2024.
JiteshJain,JiachenLi,MangTikChiu,AliHassani,NikitaOrlov,andHumphreyShi. Oneformer:
Onetransformertoruleuniversalimagesegmentation.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.2989–2998,2023.
SaharKazemzadeh,VicenteOrdonez,MarkMatten,andTamaraBerg. Referitgame: Referringto
objectsinphotographsofnaturalscenes. InProceedingsofthe2014conferenceonempirical
methodsinnaturallanguageprocessing(EMNLP),pp.787–798,2014.
BingxinKe,AntonObukhov,ShengyuHuang,NandoMetzger,RodrigoCayeDaudt,andKonrad
Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
9492–9502,2024.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
Xiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal.Segmentanything.InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pp.4015–4026,2023.
JingYuKoh,DanielFried,andRussRSalakhutdinov. Generatingimageswithmultimodallanguage
models. AdvancesinNeuralInformationProcessingSystems,36,2024.
WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,LianminZheng,CodyHaoYu,JosephE.
Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelanguagemodel
servingwithpagedattention. InProceedingsoftheACMSIGOPS29thSymposiumonOperating
SystemsPrinciples,2023.
YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini: Miningthepotentialofmulti-modalityvisionlanguagemodels.
arXivpreprintarXiv:2403.18814,2024a.
ZhiminLi,JianweiZhang,QinLin,JiangfengXiong,YanxinLong,XinchiDeng,YingfangZhang,
Xingchao Liu, MinbinHuang, Zedong Xiao, etal. Hunyuan-dit: A powerful multi-resolution
diffusiontransformerwithfine-grainedchineseunderstanding. arXivpreprintarXiv:2405.08748,
2024b.
BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023a.
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dollár,andCLawrenceZitnick. Microsoftcoco:Commonobjectsincontext. InComputerVision–
ECCV2014: 13thEuropeanConference,Zurich,Switzerland,September6-12,2014,Proceedings,
PartV13,pp.740–755.Springer,2014.
ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,Wenqi
Shao,KeqinChen,etal. Sphinx: Thejointmixingofweights,tasks,andvisualembeddingsfor
multi-modallargelanguagemodels. arXivpreprintarXiv:2311.07575,2023b.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprintarXiv:2304.08485,2023.
26TechnicalReport
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
JiasenLu,DhruvBatra,DeviParikh,andStefanLee.Vilbert:Pretrainingtask-agnosticvisiolinguistic
representationsforvision-and-languagetasks. Advancesinneuralinformationprocessingsystems,
32,2019.
JiasenLu,ChristopherClark,RowanZellers,RoozbehMottaghi,andAniruddhaKembhavi. Unified-
io: Aunifiedmodelforvision,language,andmulti-modaltasks. InTheEleventhInternational
ConferenceonLearningRepresentations,2022.
JiasenLu,ChristopherClark,SanghoLee,ZichenZhang,SavyaKhosla,RyanMarten,DerekHoiem,
andAniruddhaKembhavi. Unified-io2: Scalingautoregressivemultimodalmodelswithvision
languageaudioandaction. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pp.26439–26455,2024a.
ZeyuLu,ZiDongWang,DiHuang,ChengyueWu,XihuiLiu,WanliOuyang,andLEIBAI. Fit:
Flexiblevisiontransformerfordiffusionmodel.InForty-firstInternationalConferenceonMachine
Learning,2024b. URLhttps://openreview.net/forum?id=jZVen2JguY.
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt:
Towards detailed video understanding via large vision and language models. arXiv preprint
arXiv:2306.05424,2023.
FabianMentzer,DavidMinnen,EirikurAgustsson,andMichaelTschannen.Finitescalarquantization:
Vq-vaemadesimple. arXivpreprintarXiv:2309.15505,2023.
AIMeta. Introducingmetallama3: Themostcapableopenlyavailablellmtodate. MetaAI,2024.
Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville.
Würstchen: Anefficientarchitectureforlarge-scaletext-to-imagediffusionmodels. InTheTwelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.
net/forum?id=gU58d5QeGv.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understandingbygenerativepre-training. 2018.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. Journalofmachinelearningresearch,21(140):1–67,2020.
AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,
andIlyaSutskever. Zero-shottext-to-imagegeneration. InInternationalconferenceonmachine
learning,pp.8821–8831.Pmlr,2021.
AliRazavi,AaronVandenOord,andOriolVinyals. Generatingdiversehigh-fidelityimageswith
vq-vae-2. Advancesinneuralinformationprocessingsystems,32,2019.
JingjingRen,WenboLi,HaoyuChen,RenjingPei,BinShao,YongGuo,LongPeng,FenglongSong,
andLeiZhu. Ultrapixel: Advancingultra-high-resolutionimagesynthesistonewpeaks. arXiv
preprintarXiv:2407.02158,2024.
27TechnicalReport
ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyLDenton,Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding. Advancesinneuralinformation
processingsystems,35:36479–36494,2022.
GuillaumeSanchez,HongluFan,AlexanderSpangher,EladLevi,PawanSasankaAmmanamanchi,
andStellaBiderman. Stayontopicwithclassifier-freeguidance. arXivpreprintarXiv:2306.17806,
2023.
ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b: An
open large-scale dataset for training next generation image-text models. Advances in Neural
InformationProcessingSystems,35:25278–25294,2022.
NoamShazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,2020.
NathanSilberman,DerekHoiem,PushmeetKohli,andRobFergus. Indoorsegmentationandsupport
inferencefromrgbdimages. InComputerVision–ECCV2012: 12thEuropeanConferenceon
Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V 12, pp. 746–760.
Springer,2012.
JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,andYunfengLiu.Roformer:Enhanced
transformerwithrotarypositionembedding. Neurocomputing,568:127063,2024.
Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.
Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint
arXiv:2406.06525,2024.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
JingjingLiu,TiejunHuang,andXinlongWang. Emu: Generativepretraininginmultimodality. In
TheTwelfthInternationalConferenceonLearningRepresentations,2023.
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
arXiv:2405.09818,2024a.
GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
KolorsTeam. Kolors: Effectivetrainingofdiffusionmodelforphotorealistictext-to-imagesynthesis.
arXivpreprint,2024b.
Teknium. Openhermes2.5: Anopendatasetofsyntheticdataforgeneralistllmassistants, 2023.
URLhttps://huggingface.co/datasets/teknium/OpenHermes-2.5.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
AaronVanDenOord, OriolVinyals, etal. Neuraldiscreterepresentationlearning. Advancesin
neuralinformationprocessingsystems,30,2017.
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,
Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through
a simple sequence-to-sequence learning framework. In International conference on machine
learning,pp.23318–23340.PMLR,2022.
ShengqiongWu,HaoFei,LeigangQu,WeiJi,andTat-SengChua. Next-gpt: Any-to-anymultimodal
llm. InForty-firstInternationalConferenceonMachineLearning.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming
language models with attention sinks. In The Twelfth International Conference on Learning
Representations,2024. URLhttps://openreview.net/forum?id=NG7sS51zVF.
28TechnicalReport
JiaruiXu,SifeiLiu,ArashVahdat,WonminByeon,XiaolongWang,andShaliniDeMello. Open-
vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.2955–2966,2023.
LiheYang,BingyiKang,ZilongHuang,XiaogangXu,JiashiFeng,andHengshuangZhao. Depth
anything: Unleashingthepoweroflarge-scaleunlabeleddata. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.10371–10381,2024.
HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-adapter: Textcompatibleimageprompt
adapterfortext-to-imagediffusionmodels. arXivpreprintarXiv:2308.06721,2023.
JiahuiYu,YuanzhongXu,JingYuKoh,ThangLuong,GunjanBaid,ZiruiWang,VijayVasudevan,
AlexanderKu,YinfeiYang,BurcuKaragolAyan,etal. Scalingautoregressivemodelsforcontent-
richtext-to-imagegeneration. TransactionsonMachineLearningResearch,2022.
KaiZhang,LingboMo,WenhuChen,HuanSun,andYuSu. Magicbrush: Amanuallyannotated
datasetforinstruction-guidedimageediting. AdvancesinNeuralInformationProcessingSystems,
36,2024.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pp.3836–3847,2023.
YanliZhao,AndrewGu,RohanVarma,LiangLuo,Chien-ChinHuang,MinXu,LessWright,Hamid
Shojanazeri,MyleOtt,SamShleifer,etal. Pytorchfsdp: experiencesonscalingfullyshardeddata
parallel. arXivpreprintarXiv:2304.11277,2023.
LeZhuo,RuoyiDu,HanXiao,YangguangLi,DongyangLiu,RongjieHuang,WenzeLiu,Lirui
Zhao,Fu-YunWang,ZhanyuMa,etal. Lumina-next: Makinglumina-t2xstrongerandfasterwith
next-dit. arXivpreprintarXiv:2406.18583,2024.
29