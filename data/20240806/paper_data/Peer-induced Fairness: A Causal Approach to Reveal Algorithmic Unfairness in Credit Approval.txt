Peer-induced Fairness: A Causal Approach for Algorithmic
Fairness Auditing
Shiqi Fang∗1, Zexun Chen†1, and Jake Ansell‡1
1Business School, University of Edinburgh, Edinburgh, EH8 9JS, United Kingdom.
Abstract
Asartificialintelligenceandautomationincreasinglypermeatedecision-makingsystems,
ensuringalgorithmicfairnesshasbecomecritical. Thispaperaddressesafundamentalques-
tionoftenoverlooked: howtoauditalgorithmicfairnessscientifically. Itiscrucialtodiscern
whetheradversedecisionsstemfromalgorithmicdiscriminationormerelyfromthesubjects’
insufficient capabilities. To tackle this, we develop an algorithmic fairness auditing frame-
work, “peer-induced fairness”, that leverages counterfactual fairness and advanced causal
inferencetechniques,suchastheSingleWorldInterventionGraph. Ourapproachtranscends
the typical trade-off between quantitative fairness measures and accuracy by aiming to as-
sess algorithmic fairness at the individual level through peer comparisons and hypothesis
testing, particularly in contexts like credit approval. This framework effectively addresses
data scarcity and imbalance—frequent data quality challenges in traditional models, and is
a uniquely model-agnostic and flexible self-audit tool for stakeholders and an external audit
tool for regulators, in a plug-and-play fashion. Additionally, it offers explainable feedback
forthosewhoreceiveunfavourabledecisionsduetoinsufficientcapabilities. Wevalidateour
frameworkinapracticalcontext,highlightingthedegreeofalgorithmicbiasthatarises,with
41.51%and56.40%ofsubjectsbeingeitherdiscriminatedagainstorprivilegedrespectively.
It could also serve as a transparent, and adaptable tool suitable for diverse applications.
Key Words: Ethics in OR, Algorithmic Fairness, Causality, Counterfactual Fairness
1 Introduction
Algorithmic data-driven methods are extensively employed across a broad spectrum of fields,
including healthcare, advertising, employment, supply chain, credit scoring, criminal justice
(Kozodoi et al., 2022; Guan et al., 2020; Chen and Hooker, 2022; Berk et al., 2017; Dwork et al.,
∗S.Fang-6@sms.ed.ac.uk
†Zexun.Chen@ed.ac.uk
‡J.Ansell@ed.ac.uk
1
4202
guA
5
]PA.tats[
1v85520.8042:viXra2012; Lodi et al., 2023, 2024). These approaches are being adopted to replace human decision-
making with the aim of reducing biases and ostensibly moving society towards greater equality.
This is because algorithms and robots, as non-human entities, do not inherently possess the
biases that can influence human judgement, thereby reducing the potential for discrimination.
Admittedly, algorithms do not discriminate against groups of individuals; however, algorithmic
data-driven models often rely on historical datasets that may contain significant biases. There
are a number of potential areas of use of these datasets where algorithmic bias might arise.
Therefore, to truly achieve the objective of reducing discrimination, it is crucial to develop
algorithmic models that are specifically designed to handle and correct for biases within these
datasets. Such models must effectively make decisions that are free from discrimination based
on protected characteristics, such as gender or marital status (Lessmann et al., 2015; Kozodoi
et al., 2022).
Such algorithmic fairness is not only applicable to people but exists in any context where
an organisation may be treated unfairly in a decision process. As regulatory policies and docu-
ments, such as the General Data Protection Regulation (GDPR) (Voigt and Von Dem Bussche,
2017) and the Markets in Financial Instruments Directive II (MiFID II) (Yeoh, 2019), continue
to evolve, algorithmic fairness auditing and monitoring becomes increasingly significant. The
Europe Artificial Intelligence Act (Madiega, 2021) (EU AI Act), the first comprehensive AI law
in the world, requires that high-risk applications, such as credit scoring, to identify discrimi-
nation themselves. It also mandates that AI systems in these applications should be assessed
before being put on the market and continuously throughout their life cycle. British Standards
Institution (BSI) subsequently requires that AI providers ensure full compliance with the EU
AI Act (British Standards Institution, 2023). To support clients who will be regulated by this
legislation, BSI emphasises the significance of algorithm auditing services, which are designed
to help AI providers meet the necessary standards and regulatory requirements set by the EU
AI Act (British Standards Institution, 2023). Therefore, offering a reasonable tool for self-audit
for stakeholders, as well as for external audits by regulators, is crucial.
However, a key challenge is obtaining precise and stable auditing results, especially given
poor data quality, such as data scarcity and imbalance, which are common issues in many types
of research. Additionally, it is crucial to distinguish whether the rejection of an individual or
organisation is due to discrimination or inherent incapability.
In response, our paper employs a straightforward yet effective causal framework to audit the
existence of algorithmic bias—that is, to determine whether certain individuals or organisations
are being unfairly treated. The core concept involves comparing treatment in model outcomes
among similarly situated individuals or organisations within the dataset. By analysing these
comparisons,weaimtoascertainwhetheranyindividualororganisationisbeingunfairlytreated
by the algorithm.
Althoughtheconceptofcomparingsimilarindividualsisneithernewnorcomplex,ourframe-
2work, termed “peer-induced fairness” makes significant contributions to the field. First, this is
the first framework to formalise a practical concept of “peer-induced fairness” specifically de-
signed to audit algorithmic biases. Unlike traditional static measures of fairness, “peer-induced
fairness” is an advanced framework that leverages counterfactual fairness (Wu et al., 2019)
and causal inference techniques, such as Single World Intervention Graphs (SWIGs) (Richard-
son and Robins, 2013) and peer observation theory (Li and Jain, 2016; Ho and Su, 2009).
Stakeholders and regulators could use it as a bias audit tool for self-assessment and external as-
sessment. Second, the core counterfactual comparison approach makes “peer-induced fairness”
robust against data scarcity issues—a common challenge where protected groups are often un-
derrepresented in datasets. This methodology circumvents the need for traditional statistical
estimates within the protected group by utilising robust counterfactual statistics derived from
well-represented peer groups. This approach allows us to use data from a single group, thereby
making our method robust to population imbalance. Third, “peer-induced fairness” offers a
transparent framework that could distinguish the subjects who are unfairly treated and merely
not capable. It enables comparisons between an individual’s and their peers’ data, providing
watch-out insights regarding the key features of why they are fairly treated but still rejected.
Fourth, we validate our framework on access to finance for small and medium-sized enterprises
(SMEs). Our application highlights in detail many advantages of using the framework which
details unfairness and other aspects such as explainability. Given it relates to organisations,
it expands the literature from predominantly individual-focused studies to include corporate
entities, using firm size as a protected characteristic, given that smaller businesses are more
likely to be denied loans and unfairly treated in accessing finance. Fifth, our criteria and frame-
work are adaptable, offering potential for broader application across various fields and datasets.
It is not only applicable to credit scoring for individuals but also extends to firms and can
be adapted to any domain requiring fairness analysis. Its versatility stems from its ability to
handle multifaceted scenarios independently of the specific fairness measures or the underlying
problem because it assesses peers in a counterfactual manner.
The rest of our paper is structured as follows. Section 2 reviews the relevant literature. Sec-
tion 3 starts from the counterfactual world and introduces the casual framework. Section 4 pro-
poses the peer observation theory, corresponding peer identification process and “peer-induced
fairness” framework. Section 5 and Section 6 demonstrate our experiment procedure and em-
pirical results. Section 7 concludes.
2 Literature review
Thetheoryandpracticesurroundingfairnesshavegarneredincreasingattentionfrombothschol-
arsandregulators(FederalTradeCommission,2023;Rohner,1979;VoigtandVonDemBussche,
2017; Kehrenberg et al., 2020). The concept of algorithmic fairness in automated decision-
3making systems is notably complex and lacks a universally accepted definition. Several frame-
works have been proposed to address this challenge (Dwork et al., 2012; Hardt et al., 2016).
Despite advancements in developing measures to uphold fairness criteria, there remains a
considerablegapinthepracticalapplicationoftheseframeworks. Traditionally,academicefforts
have focused on transforming the concept of fairness into quantifiable definitions that address
discrimination within specific datasets. However, those responsible for implementing these
algorithms—suchaspractitioners,policymakers,andjudicialfigures—facesignificantchallenges
in choosing the most appropriate fairness definition to suit their unique circumstances (Kusner
et al., 2017; Huang et al., 2020; Dixon et al., 2018; Foulds et al., 2020; Hickey et al., 2020).
For example, the criteria for fairness required to address gender disparities may vary markedly
from those needed for racial issues, and similarly, from the broader, non-demographic contexts,
such as ensuring equitable treatment between large corporations and SMEs in credit approval
processes (Lu and Calabrese, 2023). It is impractical to adopt a single quantitative fairness
definition as a universal solution for all sectors. Furthermore, it is essential to recognise that,
despite their positive intentions, some fairness models can inadvertently increase discrimination
(Kozodoi et al., 2022). This highlights the urgent need for a detailed and context-specific
evaluation of fairness definitions, to ensure that the deployment of algorithmic decision-making
systems genuinely contributes to reducing bias (Kusner et al., 2017).
Inresponsetopersistentissuesinalgorithmicdecision-making, acausally-orientedapproach
to fairness has been advocated (Kusner et al., 2017), focusing on the relationships between pro-
tected features and data. Subsequent studies (Pfohl et al., 2019; Kim et al., 2021; Kusner
et al., 2017; Chiappa, 2019) have shown the efficacy of causal inference techniques in devel-
oping fair algorithms. However, counterfactual fairness encounters significant limitations, such
as unidentifiability from observational data under certain conditions, which complicates the
measurement of counterfactual outcomes (Wu et al., 2019). Additionally, the challenge of data
scarcity often hinders decision-making processes intended to implement fairness constraints.
Historical biases typically result in datasets where protected groups are underrepresented (Iosi-
fidis and Ntoutsi, 2018), thereby skewing the accuracy of fairness criteria and utility metrics.
This imbalance, particularly the under-representation of minority groups in training data (i.e.,
representational disparity), leads to their diminished influence on model objectives (Hashimoto
et al., 2018). As a result, biased measures of discrimination may emerge (Sha et al., 2023;
Dablain et al., 2022). For example, in the finance sector, the availability of credit approval
data for minority groups is substantially lower than for majority groups, complicating the fair
assessment of creditworthiness—a critical aspect of many established fairness frameworks. Fur-
thermore, the implementation of complex causal frameworks often depends on intricate causal
graph assumptions and elaborate causal inferences, such as those detailed in (Chiappa, 2019).
These calculations are not only complex but also typically require Monte Carlo approximations.
While these frameworks are adequate for group-level fairness analyses, they are less suited to
4addressing the needs of specific individuals or firms. Those seeking to enhance their chances
of approval for future financing applications require more tangible explanations and actionable
feedback than that which Monte Carlo approximations can provide. Regulatory authorities
have consistently emphasised the necessity for transparent and explainable models to provide
cleardecision-makinggrounds(Chenetal.,2024;VoigtandVonDemBussche,2017). However,
current explanation-related fairness criteria usually incorporate explainability into the fairness
framework (Zhao et al., 2023; Hickey et al., 2020). There is still a gap in providing explanations
ofthefairnessframework, whichiscrucialasitaidspeopleinunderstandingthespecificreasons
behind the rejections. Therefore, this paper contributes to filling the gaps by designing a novel
fairness framework using a causal lens, to stably audit algorithmic bias with group imbalance
and data scarcity, and provide explanations to promote the transparency of our framework.
3 Counterfactual fairness and SWIGs
The attractiveness of counterfactual reasoning stems from its capacity to rigorously analyse
causal relationships, unearth potential biases, and furnish methodologies for elucidating de-
cisions made by models. Counterfactual reasoning critically examines and establishes causal
connections by contemplating hypothetical scenarios under altered conditions (e.g., “If the in-
dividual were not a woman, would her application be approved for a loan?”). Counterfactual
fairness is a concept that has been explored and represented in diverse forms within the aca-
demic literature (Pfohl et al., 2019; Kim et al., 2021; Kusner et al., 2017; Wu et al., 2019). In
this paper, we adopt the general framework as described by Wu et al. (2019).
LetS representthesetofprotectedfeaturesofanindividual,whichbydefinition,mustnotbe
subject to bias under any fairness doctrine. Additionally, let Z represent the set of unprotected
features, with X ⊆ Z specifying the subset of observable features for any given individual. The
outcomeofthedecision-makingprocess,potentiallyinfluencedbyhistoricalbiases,isdenotedby
Y. We utilise a historical dataset D, sampled from a distribution P(Z,S,Y), to train a classifier
f : (Z,S) (cid:55)→ Yˆ, where Yˆ is the prediction generated by a machine learning algorithm aiming
to estimate Y. The causal structure underlying the distribution P(Z,S,Yˆ) is represented by a
graph causal model G.
Definition 1 (Counterfactualfairness). GivenasetoffeaturesX ⊆ Z,aclassifierf : (X,S) (cid:55)→
Yˆ is counterfactually fair with respect to X if under any observable context X = x and S = s,
P(Yˆ = y|X = x,S = s) = P(Yˆ = y|X = x,S = s), (1)
S←s S←s′
for all y and for any value s′ attainable by S.
For a binary protected feature and a dichotomous decision outcome, a simplified version can
be formulated.
5Definition 2. Given a set of features X ⊆ Z, a binary classifier f : (X,S) (cid:55)→ Yˆ is counterfac-
tually fair with respect to X if under any observable context X = x and S = s ,
−
P(Yˆ = 1|X = x,S = s ) = P(Yˆ = 1|X = x,S = s ), (2)
S←s− − S←s+ −
for all y and S = {s ,s }.
+ −
For illustrative purposes, imagine a scenario where individuals/organisations are evaluated
for accessing finance using a predictive model, which determines the decision outcome, repre-
sented as Yˆ. Let us focus on a firm from the smallest SME (i.e., micro group), denoted by s
−
withaspecificprofilex. Thelikelihoodthatthisfirmreceivesafavourableoutcomeisexpressed
as P(Yˆ|s ,x), which is equivalent to P(Yˆ = 1|S = s ,X = x) by maintaining the firm’s
− S←s− −
protectedfeature(i.e., firms’size)unaltered. Suppose, hypothetically, thatthisfirm’sprotected
feature is changed from s to s . The probability of a favourable outcome after such a counter-
− +
factual modification is denoted by P(Yˆ |s ,x). Counterfactual fairness is achieved when the
s+ −
probabilities P(Yˆ |s ,x) and P(Yˆ |s ,x) are equal, suggesting that the treatment of the firm
s− − s+ −
wouldremainconsistentirrespectiveoftheirgroupmembership. Thisconditionunderscoresthe
essence of counterfactual fairness, where the decision-making process is indifferent to changes
in the protected features of the firms.
X(s ) x X(s ) x
− −
S s − Y(s −,x) S s + Y(s +,x)
(a) Actual Scenario: G(s ,x) (b) Counterfactual Scenario: G˜(s ,x)
− +
X(s ) x′
+
S s Y(s ,x′)
+ +
(c) Actual Scenario: G(s ,x′)
+
Figure 1: SWIGs for Graphical Causal Models (GCM). (a): The SWIG G(s ,x) rep-
−
resents the actual scenario for an individual with features (s ,x). (b): The SWIG G˜(s ,x)
− +
illustratesthecounterfactualscenario, assumingtheindividual’sprotectedfeaturechangesfrom
s tos , whiletheirotherfeaturesxremainthesame. (c): TheSWIGG(s ,x′)representsthe
− + +
actual scenario for an individual with features (s ,x′). The actual SWIG G(s ,x) corresponds
+ −
to the conditional distribution Yˆ |s ,x. Conversely, in the counterfactual SWIG G˜(s ,x)
s− − +
refers to Yˆ |s ,x, denoting the outcome distribution had the individual been featured with
s+ −
s , given that the actual features are (s ,x). Thus the directed link from s to X(s ) is not
+ − + −
the fact (shown in green colour). Note: G˜(s ,x) ̸= G(s ,x′) because G˜(s ,x) is counterfactual
+ + +
scenario with actual features (s ,x) and G(s ,x′) is the fact with features (s ,x′).
− + +
Amorenuancedcomprehensionofcounterfactualfairnessmaybefacilitatedthroughthelens
6ofSWIGs1 (RichardsonandRobins,2013). Consideranindividualbelongingtoadisadvantaged
groups ,characterisedbyfeaturesx. Thelabels couldexertadirectinfluenceontheoutcome
− −
Y, or it may indirectly impact Y through its effect on other observable features X. If we
postulate a counterfactual scenario in which the individual’s group designation changes from s
−
to s , the corresponding Graphical Causal Models (GCMs) for both actual and hypothetical
+
situations can be depicted using SWIGs, as illustrated in Fig. 1. Counterfactual fairness is
attained if the predictor, consistent with the actual GCM and the counterfactual GCM, yields
identical probabilities for the outcome given the specific features (s ,x).
−
Next, let us review some pivotal conclusions derived from the SWIGs as depicted in Panel
(a) of Fig. 1 and propose some notations. A key aspect we will discuss is the factorisation
properties of the joint distribution of all variables within a SWIG, applicable to any protected
feature s ,s and other features x, which can be mathematically represented as follows:
− +
G(s,x) : P(S,X(s),Y(s,x)) = P(S)·P(X(s))·P(Y(s,x)),s ∈ {s ,s }. (3)
− +
Furthermore, the modularity property is observed where:
P(X(s) = x) = P(X = x|S = s),s ∈ {s ,s }, (4)
− +
P(Y(s,x) = y) = P(Y = y|X = x,S = s),s ∈ {s ,s }, (5)
− +
highlighting the left-hand side is the potential outcome while the right-hand side is the observa-
tional conditional probability. In the context of the counterfactual scenario with actual features
(s ,x) shown in Panel (b) of Fig. 1, a similar joint distribution is applicable:
−
G˜(s ,x) : P(S,X(s ),Y(s ,x)) = P(S)·P(X(s ))·P(Y(s ,x)). (6)
+ − + − +
4 Peer-induced fairness with causal framework
While the concept of counterfactual fairness is theoretically straightforward and can be easily
described, its application in practice is hampered by the challenges in identifying counterfactual
outcomesfromobservationaldataincertainscenarios,ashighlightedbyWuetal.(2019). Specif-
ically, the probability P(Yˆ |s ,x) as a potential outcome remains elusive for direct calculation
s+ −
due to its unidentifiability. To navigate this impediment and facilitate a feasible implementa-
tion of counterfactual fairness, we propose a practical approximation method that utilises peer
comparison as an effective strategy.
1In SWIGs, black nodes represent random variables, while red nodes indicate fixed values, representing ex-
perimental interventions. Arrows depict causal relationships between variables.
74.1 Discrimination from peer comparisons
The phenomenon of discrimination, a ubiquitous aspect of daily life, is extensively explored
within cognitive science literature. Research indicates that perceptions of discrimination are
shaped not only by personal experiences but also through comparisons with peers who, despite
possessing similar capabilities, skills, or knowledge, experience differential treatment, leading
to missed opportunities. These perceptions are cultivated both through individual encounters
and the lens of peer experiences. When an individual’s treatment aligns with that of their
peer group, perceptions of being biased tend to diminish. Studies have shown that social and
financial ties are more likely to form among individuals who share similarities in revenue lev-
els, consumption behaviours, educational background, class, gender, race, or creditworthiness,
illustrating a preference for homogeneity (Li et al., 2020; Haenlein, 2011; Goel and Goldstein,
2014; Wei et al., 2016).
4.2 Fairness through peer observations
Building on the concept of bias through peer comparisons discussed previously, we propose a
more rigorous mathematical representation to demonstrate this idea effectively.
Consider an individual A from a protected group with a protected status S = s and other
−
unprotected features X = x, denoted as A = (s ,x). Assuming the protected and unprotected
−
groups are comparable, if there exists a group of peers C = {C ,C ,···} from the unprotected
1 2
group S = s , represented as {(s ,x ),(s ,x ),···}, forming an A-oriented network. We
+ + 1 + 2
use the expectation of the probability P(Yˆ |s ,x ) across these peers C to approximate the
s+ + j
counterfactual P(Yˆ |s ,x), mathematically expressed as
s+ −
P(Yˆ |s ,x) ≈ E [P(Yˆ |s ,x )], (7)
s+ − (s+,xj)∈C s+ + j
where E[·] is the expectation (or average) notation. This peer-based counterfactual approx-
imation is intuitive, adhering to the non-discrimination principle where, ideally, the unob-
served counterfactual probability aligns consistently with the average observed among peers.
The method avoids the necessity for conventional statistical estimations within the protected
groupbyemployingresilientcounterfactualstatisticsobtainedfromadequatelyrepresentedpeer
groups. It adeptly addresses data scarcity within the protected group.
4.3 Peer definition and identification
Before initiating peer comparisons, we need to formulate the definition of peers.
Definition 3 (δ-peer). Let us consider an individual A belonging to a protected group, charac-
terised by a protected feature S = s and a set of unprotected features X = x , represented as
− 0
A = (s ,x ). AssumingthereexistsasetofindividualsB = {B ,B ,···}fromtheunprotected
− 0 1 2
8group, where B = (s ,x ) for i = 1,2,.... An individual C ∈ B is defined as δ-peer of A if the
i + i
difference in joint distributions between C’s actual SWIG, G(s ,x ), and A’s counterfactual
+ j
SWIG, G˜(s ,x ), is less than a threshold δ,
+ 0
(cid:12) (cid:12)
(cid:12)P(G(s ,x ))−P(G˜(s ,x ))(cid:12) < δ, (8)
(cid:12) + j + 0 (cid:12)
where P(G(s ,x )) = P(S,X(s ),Y(s ,x )) and P(G˜(s ,x )) = P(S,X(s ),Y(s ,x )).
+ j + + j + 0 − + 0
The concept of a peer in the graphical causal model is defined through the interrelations
among three random variables: S, X, and Y. For rigorous and unbiased comparisons, it is
essential that a peer exhibits a joint distribution similar to the counterfactual scenario.
Despite the appealing theoretical foundation of the δ-peer concept, its practical implemen-
tation encounters significant challenges. A primary obstacle is the difficulty in calculating
P(Y(s ,x)) from Eq. (6) for the counterfactual case (s ,x), which is crucial for assessing peer
+ +
similarity in such contexts. This complication stems from the representation of x as the un-
protected features for the protected group, where direct calculation of this probability is often
unfeasible due to the absence of observational data. To address this and develop a more feasible
approach for peer selection, we re-examine Eq. (3) and Eq. (6).
Since it is not feasible to directly derive G˜(s ,x) from observational data, we have no choice
+
but use the information from G(s ,x) as a proxy for approximation, which has been discussed
+
in Section 4.2. Upon comparing Eq. (3) and Eq. (6), the difference lies in the terms X and Y.
Referring to Panel (a) of Fig. 1 and considering x as the observable unprotected features of
0
an individual from the protected group S = s , we can compute P(X(s ) = x ) using Bayes’
− − 0
formula:
P(X(s ) = x ) = P(X = x |S = s )
− 0 0 −
P(X = x )P(S = s |X = x )
0 − 0
= . (9)
P(S = s )
−
Similarly, we can determine P(X(s ) = x ):
+ 0
P(X(s ) = x ) = P(X = x |S = s )
+ 0 0 +
P(X = x )P(S = s |X = x )
0 + 0
= . (10)
P(S = s )
+
However, because x are the observable unprotected features for an individual from the pro-
0
tected group S = s , estimating P(S = s |X = x ) directly is not feasible. Given that S
− + 0
represents a binary set, we can infer:
P(S = s |X = x ) = 1−P(S = s |X = x ), (11)
+ 0 − 0
P(S = s ) = 1−P(S = s ). (12)
+ −
9We propose a unified notation for both P(X(s )) and P(X(s )) to streamline calculations
− +
and ensure consistency across analyses:
P(X(s) = x) = P(X = x)ξ(s,x), (13)
where ξ(s,x) is defined as the identification coefficient (IC). This coefficient adjusts the prob-
ability values to reflect the conditions of being either a factual or counterfactual group, and is
given by:

 1 ·P(S = s |X = x), if s = s ,
ξ(s,x) =
P(S=s−) − −
(14)
 1 ·(1−P(S = s |X = x)), if s = s .
1−P(S=s−) − +
Although direct evaluation of the joint distribution G˜(s ,x) is not feasible, we can facilitate
+
the comparison by utilising the computable ξ(s,x). This approach hinges on quantitative com-
parisonandaddressesthecriticalquestion: “How can peers be identified?”. Traditionalmethods
often employ multi-dimensional matching to identify similar individuals within datasets, typ-
ically focusing on unprotected features X. However, the causal impact of protected features
S on X, coupled with the high dimensionality of X, poses significant challenges to the effi-
cacy of these conventional matching techniques. The complexity introduced by the curse of
dimensionality makes the straightforward application of these methods problematic.
We propose a practical approach to implement a δ-peer identification algorithm. The ap-
proach utilises information from the counterpart group, effectively addressing the issues of data
scarcity and imbalance theoretically.
Theorem 1 (δ-peer identification). Consider an individual A = (s ,x ) and assuming there
− 0
are a group of individuals B = {B ,B ,··· } from unprotected group, where B = (s ,x ). An
1 2 j + j
individual C ∈ B is identified as a δ-peer of A if:
|ξ(s ,x )−ξ(s ,x )| < δ. (15)
− 0 + j
10Proof. According to Definition 3, we have
(cid:12) (cid:12)
(cid:12)P(G(s ,x ))−P(G˜(s ,x ))(cid:12)
(cid:12) + j + 0 (cid:12)
(cid:12) (cid:12)
= (cid:12)P(S,X(s ),Y(s ,x ))−P(S,X(s ),Y(s ,x ))(cid:12)
(cid:12) + + j − + 0 (cid:12)
(cid:12) (cid:12)
= P(s )·(cid:12)P(X(s ))·P(Y(s ,x ))−P(X(s ))P(Y(s ,x ))(cid:12)
+ (cid:12) + + j − + 0 (cid:12)
(cid:12) (cid:12)
= P(s )·(cid:12)P(x )·ξ(s ,x )·P(Y|s ,x )−P(x )·ξ(s ,x )·P(Y|s ,x )(cid:12)
+ (cid:12) j + j + j 0 − 0 + 0 (cid:12)
(cid:12) (cid:12)
= P(s )·(cid:12)ξ(s ,x )·P(Y,x |s )−ξ(s ,x )·P(Y,x |s )(cid:12)
+ (cid:12) + j j + − 0 0 + (cid:12)
= P(s )·P(Y,x |s )·|ξ(s ,x )−ξ(s ,x )|
+ j + − 0 + j
≤ P(s )·P(Y,x |s )·δ
+ j +
< δ.
The derivation of the second equation is underpinned by the factorisation property, as detailed
in Eq. (3) and Eq. (6). The transition to the third equation leverages the modularity property,
whichisarticulatedinEq.(5). ThetransitionfromP(X(s ))andP(X(s ))intoP(x )·ξ(s ,x)
+ − j +
and P(x )·ξ(s ,x ) refer to Eq. (13). Regarding the fifth equation, it addresses the practical
0 + 0
consideration of dealing with high-dimensional continuous variables in X. Given the high-
dimensional nature of X, the probability of X equating to a specific value within this space
is nominally small. Thus, for practical purposes, the distinction between P(Y,X = x |s ) and
j +
P(Y,X = x |s ) is considered negligible (i.e., P(Y,x |s ) = P(Y,x |s )). Therefore, C is
0 + j + 0 +
considered as a peer of A according to Definition 3.
Consequently, we can generate an algorithm shown in Algorithm 1 to identify all peers in
the dataset.
Algorithm 1: Identification of δ-Peers for Protected Group Individuals
Input: A set of individuals {A} = {(s ,x )} from the protected group, a set of individuals
− 0
{B }N ={(s ,x )}fromtheunprotectedgroup,athresholdδ,andaminimumnumber
i i=1 + i
of peers U.
Output: A subset of {B } designated as δ-peers of A, with each protected individual having at
i
least U peers.
foreach A=(s ,x ) do
1 − 0
Initialise an empty list of peers for A, denoted as Peers
2 A
Compute ξ(s ,x ) for A
3 − 0
foreach B =(s ,x ) in {B }N do
4 i + i i i=1
Compute ξ(s ,x ) for B
5 + i i
Calculate the difference ∆=|ξ(s ,x )−ξ(s ,x )|
6 − 0 + i
if ∆<δ then
7
Add B to Peers
8 i A
end
9
end
10
end
11
114.4 Peer-induced fairness
Following the idea of peer comparison, definition, and identification, we can now introduce the
concept of peer-induced fairness.
Definition 4 ((δ,f)-peer-inducedfairness2). ConsideranindividualA = (s ,x )andassuming
− 0
A has a number of δ-peers C = {C ,C ,···} where C = (s ,x ). A is said to be fairly treated
1 2 j + j
by the peers subject to (δ,f) if and only if
P(Yˆ |s ,x ) = E [P(Yˆ |C )], (16)
s− − 0 Cj∈C s+ j
where Yˆ is the predictive outcome provided with the classifier f.
Asdiscussedinprevioussections,whilewecandirectlyestimateP(Yˆ |s ,x)fromindividual
s− −
observations, estimating the expected value E [P(Yˆ |C )] presents challenges due to the
Cj∈C s+ j
limitednumberofobservationsavailableforδ-peers. Consequently,wehavetorelyonobservable
peerstoapproximatethepopulationmean. Toformalisethis, weintroducetherandomvariable
T = P(Yˆ |C ). (17)
j s+ j
Upon examining the distribution of T , we find that it does not follow a normal distribution,
j
with details presented in Supplementary Materials. Therefore, we randomly select a subset of
peers and use the sample mean to estimate the population mean,
K
1 (cid:88)
T¯ = P(Yˆ |C ), (18)
K
s+ j
j=1
where K is a large enough number of peers in the subset.
According to the Central Limit Theorem, the sample mean T¯ follows a normal distribution,
and thus E[T¯], can be employed to estimate the overall predictive probabilities of favourable
outcomes among peers, denoted as E[T] = µ. Based on this, we propose a proposition that a
synthetic individual, defined using IC 3, can also be considered as a δ-peer.
Proposition 1. Let A be an individual and C = {C ,C ,...} denote all of A’s δ-peers.
1 2
Define a synthetic individual T¯ using the average IC of any subset C of K peers, where
i i
C = {Ci,...,Ci } ⊆ C, i ∈ {1,2,...,N} and Ci represents the j-th peer in the i-th selec-
i 1 K j
tion with the unprotected feature xi. This synthetic individual T¯ can also be considered as a
j i
δ-peer of A.
2Although the term “peer-induced fairness” has been used in other contexts as noted by (Ho and Su, 2009;
Li and Jain, 2016), our concept is novel in its reliance on a structured causal reasoning framework specifically
tailored for classification tasks.
3AlthoughthesyntheticindividualisdefinedbyIC,thecorrespondingpredictivefavourableoutcomeproba-
bilities calculation should follow Eq. (18).
12Proof. To demonstrate that the synthetic individual T¯ qualifies as a δ-peer of A, we compare
i
A’s IC, ξ(s ,x ), against the average IC of any K peers of A, denoted as
(cid:80)K
ξ(s ,x )/K.
− 0 j=1 + j
The difference is calculated as follows:
(cid:12) (cid:12)
(cid:12) K (cid:12)
(cid:12) 1 (cid:88) (cid:12)
(cid:12)ξ(s −,x 0)− ξ(s +,x j)(cid:12)
(cid:12) K (cid:12)
(cid:12) j=1 (cid:12)
(cid:12) (cid:12)
(cid:12) K (cid:12)
1 (cid:12) (cid:88) (cid:12)
= (cid:12)Kξ(s −,x 0)− ξ(s +,x j)(cid:12)
K (cid:12) (cid:12)
(cid:12) j=1 (cid:12)
1
= |(ξ(s ,x )−ξ(s ,x ))+···+(ξ(s ,x )−ξ(s ,x ))|
− 0 + 1 − 0 + K
K
K
1 (cid:88)
≤ |ξ(s ,x )−ξ(s ,x )|
− 0 + j
K
j=1
≤ δ.
This inequality shows that the average discrepancy between A’s IC and that of T¯ is within
i
δ. Hence, according to Theorem 1, T¯ indeed qualifies as a δ-peer of A.
i
Consequently, by randomly selecting K peers from the set of all observed δ-peers N times,
we calculate the predictive favourable outcome probabilities T¯ for each i-th selection using
i
Eq. (18). Here
K
1 (cid:88)
T¯ = P(Yˆ |Ci). (19)
i K s+ j
j=1
We then utilise the mean of the observed sample mean distribution, {T¯}N , which are all
i i=1
confirmed δ-peers as per Proposition 1, to estimate the overall mean µ of favourable outcome
probabilities among all peers.
4.5 Hypothesis testing for peer-induced fairness
Finally, to formalise the process of auditing whether an individual in a protected group is
subjected to algorithmic bias, we propose a hypothesis-testing framework. This framework is
predicatedonanappropriatethresholdforpeeridentificationδ andaspecificclassifierf. Itaims
to test whether the sample mean distribution {T¯}N is statistically equivalent to P(Yˆ |s ,x).
i i=1 s− −
Since T¯ follows a normal distribution and N is a large enough number, our hypothesis is
i
consistent with the standard z-test, which is designed to evaluate the presence of algorithmic
bias statistically.
• H (Null Hypothesis): The individual A = (s ,x ) is equally treated according to
0 − 0
13(δ,f)-“peer-induced fairness” criterion,
H : E[T¯] = P(Yˆ |s ,x ). (20)
0 i s− − 0
• H (Alternative Hypothesis): The individual A is subject to algorithmic bias under
1
(δ,f)-“peer-induced fairness” criterion, which is evidenced by a significant disparity in
treatment compared to their unprotected peers,
H : E[T¯] ̸= P(Yˆ |s ,x ). (21)
1 i s− − 0
Furthermore, it is also feasible to consider two additional scenarios: checking whether the
individual is algorithmically discriminated against, where H : E[T¯] < P(Yˆ |s ,x ), or algo-
2 i s− − 0
rithmically benefited, where H : E[T¯] > P(Yˆ |s ,x ).
3 i s− − 0
5 Experiment setup
Fairness concepts apply not only to decision-making concerning groups of people but also to
broader domains, such as companies within the economic system. The banking loan approval
algorithmthatuseshistoricaldataoftenplacesmicro-firmsatadisadvantageduetotheirsmaller
size and limited historical records compared to larger firms (Cenni et al., 2015). Consequently,
these smaller entities may face higher interest rates or be more likely to be denied loans, despite
their growth potential.
Therefore, to illustrate the usefulness of our novel “peer-induced fairness” framework, we
consider a real example using SMEs data. The collected data is from the UK Archive Small and
Medium-Sized Enterprise Finance Monitor (BDRC Continental, 2023). The dataset compiles
surveyinformationonSMEs4 spanningfrom2011Q1to2023Q4, withapproximately4,500tele-
phone interviews conducted per quarter across the UK. Each interview provides insights into
the experiences of SMEs with external financing over the past 12 months, including their antici-
patedfuturefinancialneedsandperceivedobstaclestogrowth. Italsodetailsthecharacteristics
of the SMEs and their owners or managers. To avoid redundancy, we selected survey results
from 2012Q4 to 2020Q2. We focused on 15 important features identified from the literature
(Sun et al., 2021; Calabrese et al., 2022; Cowling et al., 2016, 2022, 2012) (refer to Table 1 for
details). These features demonstrate various dimensions of the loan application process. After
filteringoutdatapointswithmorethan20%missingfeatures, weobtainedadatasetcomprising
4,159 data points for our analysis. The details of data cleaning are presented in Supplementary
Materials.
4SMEs included in this survey meet the four criteria: 1) employ no more than 250 individuals, 2) have an
annual turnover not exceeding £25 million, 3) do not operate as social enterprises or non-profit organisations,
and 4) are not owned by another company by more than 50%.
14Table 1
Description of features.
Feature Category Value Percentage
risk multivariate&ordinal minimal 19.59%
low 43.11%
average 25.98%
aboveaverage 11.31%
principal multivariate&nominal construction 6.64%
agriculture,huntingandforestry 10.82%
fishing 12.01%
healthandsocialwork 12.62%
hotelsandrestaurants 11.68%
manufacturing 8.69%
realestate,rentingandbusinessactivities 16.68%
transport,storageandcommunication 9.63%
wholesale/retail 11.23%
othercommunity,socialandpersonalservice 9.63%
legalstatus multivariate&nominal soleproprietorship 4.88%
partnership 10.57%
limitedliabilitypartnership 7.50%
limitedliabilitycompany 77.05%
lossorprofit multivariate&ordinal loss 86.07%
brokeneven 8.69%
profit 5.25%
turnovergrowthrate multivariate&ordinal grownmorethan20% 13.69%
grownbutbylessthan20% 40.33%
stayedthesame 33.69%
declined 12.30%
fundsinjection binary no 67.17%
yes 32.83%
creditpurchase binary no 18.48%
yes 81.52%
startups binary no 97.5%
yes 2.5%
previousturn-down binary no 90.94%
yes 9.06%
London&SouthEast binary yes 76.39%
no 23.61%
businessinnovation binary no 40.16%
yes 59.84%
product/servicedevelopment binary no 70.25%
yes 29.75%
regularmanagementaccount binary no 19.06%
yes 80.94%
writtenplan binary no 37.58%
yes 62.42%
financequalification binary no 45.66%
yes 54.34%
For this analysis, we designate the observed features listed in Table 1 as X. We treat the
firm size as the protected attribute S, defined by a combination of the number of employees
and annual turnover. This categorisation results in 1,719 micro-firms (s = s ) classified as
−
protected and 2,440 non-micro firms (s = s ) as unprotected5. We consider the outcome of
+
5Micro-firmsaredefinedasthosewithfewerthan10employeesandanannualturnoveroflessthan£2million
(Sun et al., 2021).
15bank loan applications, denoted as Y, due to the significant role of bank loans in SME financing
(Sun et al., 2021). The dataset records 3,391 approvals (y = 1) and 768 rejections (y = 0),
highlighting the decisions faced by SMEs in securing financial support.
In subsequent analyses, we focus on the 1,719 micro-firms to determine if they have expe-
rienced algorithmic bias using our “peer-induced fairness” framework. Initially, it is essential
to identify each firm’s peers by computing the IC as specified in Algorithm 1. Without loss
of generality, we set the default δ to 0.3 times the standard deviation of the micro-firms ICs.
This flexible threshold can be adjusted based on the dataset of the specific research field. The
specific robustness tests are presented in Supplementary Materials. However, direct estimation
of IC from observed data is challenging, necessitating the use of a fitted model. For simplicity,
we employ a logistic classifier to estimate the probability of an individual being labelled as part
of the protected group, with performance and robustness test in Supplementary Materials.
Following Proposition 1, we then utilise the expectation of the observed sample mean dis-
tribution to estimate the overall mean of all peers. As a standard approach, we randomly
sample N = 100 times, and each sample consists of K = 30 data points. We consider only
those micro-firms with more than U = 35 peers; firms with fewer than 35 peers are labelled as
“Unknown”. Again, direct estimation of the mean from Eq. (18) is not feasible, requiring the
use of a predictive classification model. By default, we use a logistic classifier with performance
in Supplementary Materials, although any classification model could be substituted. The ro-
bustness of the classification model is demonstrated in Supplementary Materials. The data are
typically split into training (80%) and testing (20%) sets, with hyper-parameters optimised via
grid search and 5-fold cross-validation. The model yielding the highest AUC value is selected
for predictions on the target Y.
Ultimately, we conduct hypothesis tests (i.e., H , H , H ) to compare the mean approval
0 2 3
likelihood of the peers against that of the micro-firms, thereby identifying potential algorithmic
bias,discriminationandprivilege. Thesignificancelevelforthehypothesistestissetatα = 5%.
Due to the presence of “Unknown” micro-firms, hypothesis testing is performed on a subset of
the data, consisting of 1007 data points.
6 Experiment results
In this section, we present the experimental results derived from the SMEs data to demon-
strate the efficacy of our “peer-induced fairness” framework. These results illustrate how the
framework operates in practice and underscore its potential for auditing bias in algorithmic
decision-making.
166.1 Algorithmic fairness auditing
With the evolution of algorithmic fairness methods and the increasing regulatory demands for
data protection and transparency in decision-making processes, there is a growing emphasis on
applying advanced methodologies to ensure fairness in decision systems. For instance, in the
banking sector, it is becoming common to self-scrutinise or externally audit decision-making
processes to determine whether they meet established fairness criteria or if they continue to
exhibit significant algorithmic bias.
Following the steps outlined in Section 5, we have identified algorithmic bias within this
SMEs dataset. The scatter plot (refer to Fig. 2) comparing micro-firms to their peers regard-
ing approval likelihood reveals that only 2.48% of them are treated fairly, indicating signifi-
cant disparities in the credit approval system. 97.52% of them experience algorithmic bias,
with 41.51% of micro-firms experiencing discrimination. An intriguing observation is that the
remaining 56.40% of micro-firms, despite being under-represented, benefit from the decision
system, receiving approval likelihoods higher than the average for their peers.
Higher than Lower than Equal
0.95
0.90
0.85
0.80
0.75
0.70
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Pr(Y=1|X=x,S=s )
Figure 2: Comparative analysis of loan approval likelihood for micro-firms against peers. The
black dashed 45-degree line, denoting Y = X, symbolises perfect fairness. Red and orange data
points represent micro-firms with approval likelihoods significantly lower or higher, respectively
than the average of their peers. Blue points denote no significant difference.
To identify the specific extent of discrimination and privilege faced by each micro-firm, we
compare the approval likelihood difference between a given micro-firm A = (s−,x ) and its
0
peers. For micro-firms with a higher likelihood of approval, we allow for greater tolerance when
assessing extreme algorithmic bias, adjusting the standard based on each firm’s approval likeli-
hood. Specifically, we consider a micro-firm to experience extreme algorithmic bias if the abso-
lute difference exceeds 0.1 times its own approval likelihood. Mathematically, this is expressed
as |P(Yˆ |s ,x )−E[T¯]| > 0.1×P(Yˆ |s ,x ). A negative difference indicates discrimination,
s− − 0 i s− − 0
while a positive difference signifies privilege. This approach ensures the flexibility of the stan-
dard, making it suitable for firms in different situations. Specifically, 26.71% of micro-firms
17
]T[Eexperience substantial discrimination, with their approval likelihood markedly lower than that
of their peers, as shown in Panel (a)-(c) in Fig. 3, at both group and individual levels. 32.17%
of micro-firms are extremely privileged, as shown in Panel (g)-(i). Even though algorithmic
privilege might seem beneficial for micro-firms, neither scenario is desirable. We advocate for
transparency and fairness in decision-making processes. Arbitrary or opaque factors influencing
decisions are contrary to the principles of fairness and should be rigorously addressed to ensure
equitable treatment across all applicants.
ED Others Selected micro-firm Micro-firms Peers group Peers Selected micro-firm
0.95 50
8
0.90 40
0.85 6 30
0.80 4 20
0.75 2 10
0.70 0 0
0.2 0.4 0.6 0.8 1.0 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0
Pr(Y=1|X=x,S=s ) Pr(Y=1|X=x) Pr(Y=1|X=x)
(a) (b) (c)
FT Others Selected micro-firm Micro-firms Peers group Peers Selected micro-firm
0.95 12.5
6
0.90 10.0
0.85 4 7.5
0.80 5.0
2
0.75 2.5
0.70 0 0.0
0.2 0.4 0.6 0.8 1.0 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0
Pr(Y=1|X=x,S=s ) Pr(Y=1|X=x) Pr(Y=1|X=x)
(d) (e) (f)
EP Others Selected micro-firm Micro-firms Peers group Peers Selected micro-firm
0.95
15 10
0.90
8
0.85 10 6
0.80 4
5
0.75 2
0.70 0 0
0.2 0.4 0.6 0.8 1.0 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0
Pr(Y=1|X=x,S=s ) Pr(Y=1|X=x) Pr(Y=1|X=x)
(g) (h) (i)
Figure 3: Comparative analysis of loan approval likelihood for micro-firms under each algo-
rithmic treatment category against peers. (a)-(c): Extremely discriminated (ED) micro-firms.
(d)-(f): Fairly treated (FT) micro-firms. (g)-(i): Extremely privileged (EP) micro-firms. The
secondcolumn providestheapprovallikelihoodcomparisonbetween thesemicro-firmsand their
peers at the group level. The third column provides the comparison, at the individual level,
between the selected micro-firm in the first column and its peers.
It is important to emphasise that our framework is a tool for audits by regulators and
18
]T[E
]T[E
]T[E
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneDstakeholders, aiming to detect algorithmic bias. In the credit loan application, rejected cus-
tomers are particularly concerned about whether they were treated fairly, while regulators and
banks require detailed results to audit the fairness of their models. Therefore, our framework
also includes detailed information on accepted applicants. Additionally, without compromising
generalisability to other research areas, it is crucial to focus on all applicants.
We also validate our framework by investigating the connection between accessing finance
outcomes and disparities in algorithmic bias. Among these markedly discriminated micro-firms,
52.42% were denied loans, whereas only 9.97% of their peers faced rejection, highlighting a sig-
nificant disparity in rejection rates. The rejection rate of micro firms decreases and that of their
peers increases with the diminished discrimination. The difference in rejection rates between
micro-firms and their peers also decreases. The rejection rates of peers fluctuate around the
rejection rate of fairly treated micro-firms. This fluctuation indicates that within the cate-
gory, some micro-firms experience higher rejection rates compared to their peers, while others
experience lower rejection rates, illustrating a gradual convergence in rejection rates across
categories with less pronounced discrimination. Notably, even the lowest peer rejection rate
surpasses that of micro-firms in the extremely privileged category, where micro-firms experi-
ence the lowest rejection rates, as in Fig. 4. These findings, derived from our bias audit based
on financing outcomes prediction, align with the observed financing results. This congruence
further validates the utility of our framework in accurately reflecting disparities and biases in
the loan approval process. Further details of the degree of algorithmic bias are provided in
Supplementary Materials.
Rejected micro-firms (ED) Rejected micro-firms (EP)
Rejected micro-firms (FT) Rejected peers
100%
80%
60%
40%
20%
0%
ED FT EP
Algorithmic treatment categories
Figure4: Rejectionratesofmicro-firmsacrossalgorithmictreatmentcategoriesandtheirpeers.
Thealgorithmictreatmentcategoriesincludeextremelydiscriminated(ED),fairlytreated(FT),
and extremely privileged (EP). Each category includes multiple micro-firms with a single re-
jection rate, shown as histograms, while the rejection rate of peers of each micro-firm in this
category is represented in the black line with error bars to indicate variability.
19
egatnecrePFrom the analysis presented, it is evident that our “peer-induced fairness” framework not
only identifies disparities in algorithmic fairness but also facilitates the visual representation
of individual-level discrepancies across all users in the dataset. This capability allows for clear
visualisation of algorithmic fairness, where discrimination or benefit is readily distinguishable.
Such insights are invaluable not only for regulatory purposes but also for verifying the effec-
tiveness of algorithmic fairness models. Furthermore, we subjected all results to robustness
tests, varying the level of peer identification threshold, model fitting selection, and prediction
algorithms to ensure the integrity of our findings.
6.2 Data scarcity and imbalance
Data scarcity and imbalance significantly influence the performance of advanced machine learn-
ing models due to the potential for inaccurate parameter estimation (Chen et al., 2024; Less-
mann et al., 2015). This issue is particularly pronounced in the field of algorithmic fairness,
where the representation of minority groups is often limited compared to majority groups. This
discrepancy caused by the poor data quality, subsequently affects the assessment of algorithmic
fairness.
Our “peer-induced fairness” framework addresses these challenges uniquely. Unlike tradi-
tional models that rely heavily on the data from the protected group, our framework bases all
parameter estimations on peers identified within the unprotected group. This group typically
possesses ample data points, effectively mitigating issues related to data scarcity and group
imbalance, making our framework robust theoretically.
We investigate the robustness of our peer-induced fairness framework by evaluating the
percentages of unfairly treated (PUT) protected individuals or organisations and the invariant
outcome ratio (IOR) under varying levels of imbalance. The imbalance ratio, ω, is defined as
the proportion of samples in the protected class:
#(S = s )
−
ω = ,
#(S = s )+#(S = s )
+ −
where #(·) denotes the cardinality of a set. A perfectly balanced dataset corresponds to ω =
50%. The PUT is calculated as the number of unfairly treated individuals or organisations
divided by the total number of selected subjects in the experiments with different ω. The IOR
is computed as the number of selected individuals or organisations in the experiment with ω
that have unchanged predictive outcomes compared to the original experiment (ω = 41.33%)
divided by the number of commonly selected subjects in both the experiment with ω and the
original experiment.
In SMEs experiment, building upon the default settings outlined in Section 5, we explore
the influence of varying imbalance ratios by randomly selecting subsets of the original dataset
with controlled imbalance ratios. Specifically, we evaluate performance at imbalance ratios
20of ω = {36.33%,31.33%,26.33%,21.33%,16.33%,11.33%}. By decreasing the percentage of
micro-firms in these subsets, we assess the framework’s performance across different levels of
imbalance. To mitigate the effects of randomness inherent in subset selection, the process is
repeated five times. The detailed procedure is presented in Supplementary Materials.
The results are visualised in Fig. 5 and demonstrate the robustness of our framework. From
the view of PUT, the small error bars across all the imbalance levels suggest the results across
the five repetitions are highly consistent. This observation underscores the robustness of our
“peer-induced framework” to imbalanced datasets. From the view of IOR, it is approximately
95% and remains stable across different imbalance levels. This aligns with our expectations, as
the framework does not rely on data from the minority group but rather leverages information
from the unprotected group, leading to inherent robustness. The small error bars suggest that
for imbalance ratios greater than or equal to 16.33%, the results regarding IOR are also highly
consistent.
PUT IOR
100% 100%
90% 90%
80% 80%
70% 70%
11.33% 16.33% 21.33% 26.33% 31.33% 36.33%
Figure 5: Percentage of unfairly treated micro-firms and invariant outcome ratio at different
groupimbalancelevels. Theimbalancelevelisrepresentedonthex-axisasapercentage,ranging
from 11.33% to 36.33%. The left y-axis shows the percentage of unfairly treated micro-firms
(blueline),whiletherighty-axisdisplaystheinvariantoutcomeratio(redline)astheimbalance
level changes from the initial level to other levels.
These findings underscore the stability of our “peer-induced fairness” framework, distin-
guishing itself from others by effectively addressing the data scarcity and imbalance issues.
Given the widespread nature of these issues, our framework holds considerable significance for
researchers investigating algorithmic fairness and data imbalance. An alternative computation
method is also provided in Supplementary Materials to ensure robustness.
6.3 Explainable fairness discovery
In our previous experiments, we were able to distinctly classify individuals from the protected
group into two categories: fairly-treated and unfairly-treated groups. Our analysis now turns
21
TUP ROIto those who were rejected while still fairly treated, to understand the reasons behind their
rejectionsbycomparingtheirfeatureswiththoseoftheirpeers. Additionally, the“peer-induced
fairness” framework allows us to provide a clear watch-out list of a series of features.
Given the existence of accepted peers as the counterfactual instances with positive access-
ing finance outcomes, the micro-firm which is fairly treated should originally have the same
outcomes. Our framework identifies the feature differences between each rejected while fairly
treated micro-firm and its accepted peers by hypothesis testing, with details presented in Sup-
plementary Materials. For each feature, we summarise the percentage of these micro-firms that
perform significantly worse than their accepted peers. We consider some actionable and key
features to identify and understand these discrepancies, as in Fig. 6. The descriptions for each
feature value are shown in Supplementary Materials. Results show that even though none of
them have been rejected previously and only 25% of them perform worse on financial qualifi-
cations and written plans, banks generally prioritise the financial and business health of firms.
75% of these micro-firms invest excessively in business innovation and have lower risk ratings.
Besides, half of them invest in product/service development and have lower profits. The un-
certain returns and high risks associated with innovation lead to the failure or commercial
non-viability of most innovative products (Coad and Rao, 2008; Hall, 2002; Freel, 2007), exac-
erbating already poor-performing risk indicators. The worse performance on these key features
makes banks cautious about the long-term financial sustainability of these firms. It also reflects
the capability of these micro-firms, negatively affecting their loan approvals.
100%
80%
60%
40%
20%
0%
PT FQ WP LP PS BI RI
Features
Figure 6: Comparative analysis of key features for rejected while fairly treated micro-firms vs.
accepted peers. The x-axis represents the selected key attributes being analysed, including
finance qualification (FQ), written plan (WP), previous turn-down (PT), loss or profit (LP),
risk (RI), product/service development (PS), business innovation (BI). The y-axis represents
the percentage of those micro-firms with significantly worse performance than their accepted
peers on each feature.
This exploration identifies the differences between micro-firms and their peers for each fea-
ture and summarises the percentage of micro-firms that perform worse on each feature. This
22
egatnecrep
tnacifingiSexplainable analysis not only enhances the transparency of our framework but also supports
regulators and stakeholders in understanding the specific challenges most incapable micro-firms
face and highlights the features that they need to watch out for and pay extra attention to.
7 Conclusion
In this paper, we introduce a novel fairness framework within a causal framework, termed
“peer-induced fairness”, as a bias auditing tool for internal and external assessment, in a plug-
and-playfashion. Itappliestheprinciplesofcounterfactualfairness,stipulatingthattheaverage
treatment of an individual should align with that of their peers. We identify peers based on
similar joint distributions but resort to IC due to the unidentifiability of the counterfactual
distribution. The framework requires equal treatment with peers. This approach effectively
tackles data scarcity and group imbalance by utilising robust counterfactual statistics derived
fromwell-representedpeergroups,therebyensuringmorestablebiasauditing. Besides,basedon
theessenceofpeercomparison,wecouldalsoprovideanexplainablewatch-outlistforthosewho
receive unfavourable decisions due to insufficient capabilities, promoting transparency of our
method. We have applied this framework to SMEs, but it has the potential for a generalisation
to other research domains.
By experimenting on SMEs data, our research findings reveal that by comparing micro-
firms with their peers, banks and regulators can effectively audit algorithmic bias. Specifically,
only 2.48% of micro-firms are treated fairly. 41.51% and 56.40% of micro-firms are either
discriminated against or privileged respectively. Even though some micro-firms benefit from
algorithmicfavouritism,itisessentialtoensureequitabletreatmentacrossallapplicants. Nearly
half of the micro-firms experiencing extreme discrimination are rejected, with a rejection rate,
compared to only 9.97% among their peers. This difference diminishes and becomes negative
as discrimination lessens and shifts towards privilege. Up to 95% of micro-firms maintained
consistent auditing results despite changing imbalance levels, demonstrating the stability of our
framework with data scarcity and imbalance issues. Additionally, the approach highlights the
key features that financial institutions need to pay more attention to and rejected micro-firms
mayneedtoaddress,whilstclearlyfairlytreatingmicro-firms. Thecomparisoncoulddistinguish
the bias and incapability faced by micro-firms, helping banks and regulators understand the
specific issues these firms encounter.
Our research is significant for researchers who aim to scientifically audit algorithmic bias.
This tool could perform with common data quality issues, like data scarcity and imbalance,
ensuring accuracy and stability in measuring unfairness. Besides, our framework could also
distinguish those incapable protected individuals from being biased individuals, preventing im-
proving the treatment of less capable individuals at the expense of the treatment of capable,
unprotected individuals. Besides, our empirical analysis is based on SMEs. Unlike previous
23studies that mainly focused on individual loans, our research extends the focus to the firm
level. The protected feature is the firms’ size, differing from the traditional focus on person-
level characteristics. This approach broadens the perspective of fairness. Despite the focus on
SMEs loan approval, the fairness audit framework proposed can be applicable to other domains
where algorithmic unfairness may occur, especially those suffering from group imbalance and
data scarcity.
In the domain of fairness research, class imbalance is also a crucial issue involving poor data
quality. It refers to the imbalance on the target label, leading the model to favour the majority
class, thereby affecting the overall performance of the model and the fairness measure. Previous
studies discussing the impact of class imbalance on fairness focus on the education domain (Sha
et al., 2022, 2023), with the issue remaining unexplored in the credit scoring domain. This
is particularly important because different datasets exhibit significant variations in features,
labels, missing values, and sample sizes. Besides, Iosifidis et al. (2021, 2019) have proposed
fair models for addressing class imbalance, but there is still a lack of a fairness framework
that explicitly considers class imbalance naturally. Exploring the impact of class imbalance on
algorithmic fairness measures, and developing fairness criteria related to class imbalance, are
crucial for all domains reliant on precise data-driven decision-making.
Acknowledgments
TheauthorsofthismanuscriptwouldliketothankProf.RaffaellaCalabreseandDr.YizheDong
for their assistance and support in the discussion and research direction.
References
BDRC Continental (2023). SME Finance MonitorSmall- and Medium-Sized Enterprise Finance
Monitor, 2011-2023.
Berk,R.,Heidari,H.,Jabbari,S.,Kearns,M.,andRoth,A.(2017). FairnessinCriminalJustice
Risk Assessments: The State of the Art. Sociological Methods & Research, 50(1):3–44.
British Standards Institution (2023). British standards institution: EU AI act readiness assess-
ment and algorithmic auditing.
Calabrese, R., Degl’Innocenti, M., and Zhou, S. (2022). Expectations of access to debt finance
for SMEs in times of uncertainty. Journal of Small Business Management, 60(6):1351–1378.
Cenni, S., Monferr`a, S., Salotti, V., Sangiorgi, M., and Torluccio, G. (2015). Credit rationing
and relationship lending. Does firm size matter? Journal of Banking & Finance, 53:249–265.
24Chen,V.X.andHooker,J.(2022). Combiningleximaxfairnessandefficiencyinamathematical
programming model. European Journal of Operational Research, 299(1):235–248.
Chen, Y., Calabrese, R., and Martin-Barragan, B. (2024). Interpretable machine learning for
imbalanced credit scoring datasets. European Journal of Operational Research, 312(1):357–
372.
Chiappa,S.(2019).Path-specificcounterfactualfairness.InProceedingsoftheAAAIConference
on Artificial Intelligence, volume 33, pages (pp. 7801–7808).
Coad, A. and Rao, R. (2008). Innovation and firm growth in high-tech sectors: A quantile
regression approach. Research policy, 37(4):633–648.
Cowling, M., Liu, W., and Calabrese, R. (2022). Has previous loan rejection scarred firms from
applying for loans during Covid-19? Small Business Economics, 59(4):1327–1350.
Cowling, M., Liu, W., and Ledger, A. (2012). Small business financing in the UK before
and during the current financial crisis. International Small Business Journal: Researching
Entrepreneurship, 30(7):778–800.
Cowling, M., Liu, W., and Zhang, N. (2016). Access to bank finance for UK SMEs in the wake
of the recent financial crisis. International Journal of Entrepreneurial Behavior & Research,
22(6):903–932.
Dablain, D., Krawczyk, B., andChawla, N.(2022). TowardsAHolisticViewofBiasinMachine
Learning: Bridging Algorithmic Fairness and Imbalanced Learning. arXiv:2207.06084 [cs].
Dixon, L., Li, J., Sorensen, J., Thain, N., and Vasserman, L. (2018). Measuring and mitigating
unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on
AI, Ethics, and Society, pages (pp. 67–73), New Orleans LA USA. ACM.
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. (2012). Fairness through
awareness. InProceedings of the 3rd Innovations in Theoretical Computer Science Conference
on - ITCS ’12, pages (pp. 214–226), Cambridge, Massachusetts. ACM Press.
Federal Trade Commission, U. S. (2023). Fair credit reporting act.
Foulds, J. R., Islam, R., Keya, K. N., and Pan, S. (2020). An intersectional definition of
fairness. In 2020 IEEE 36th International Conference on Data Engineering (ICDE), pages
(pp. 1918–1921), Dallas, TX, USA. IEEE.
Freel, M.S.(2007). Aresmallinnovatorscreditrationed? Small Business Economics, 28(1):23–
35.
25Goel, S. and Goldstein, D. G. (2014). Predicting Individual Behavior with Social Networks.
Marketing Science, 33(1):82–93.
Guan, Z., Ye, T., and Yin, R. (2020). Channel coordination under Nash bargaining fairness
concerns in differential games of goodwill accumulation. European Journal of Operational
Research, 285(3):916–930.
Haenlein,M.(2011). Asocialnetworkanalysisofcustomer-levelrevenuedistribution. Marketing
Letters, 22(1):15–29.
Hall, B. H. (2002). The financing of research and development. Oxford review of economic
policy, 18(1):35–51.
Hardt, M., Price, E., Price, E., and Srebro, N. (2016). Equality of opportunity in supervised
learning. In Advances in Neural Information Processing Systems, page 29.
Hashimoto, T. B., Srivastava, M., Namkoong, H., and Liang, P. (2018). Fairness Without De-
mographics in Repeated Loss Minimization. Proceedings of the 35th International Conference
on Machine Learning, 80:1929–1938.
Hickey,J.M.,DiStefano,P.G.,andVasileiou,V.(2020). Fairnessbyexplicabilityandadversar-
ial SHAP learning. In Machine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part
III, pages (pp. 174–190). Springer International Publishing.
Ho, T.-H. and Su, X. (2009). Peer-induced fairness in games. American Economic Review,
99(5):2022–2049.
Huang, W., Wu, Y., Zhang, L., and Wu, X. (2020). Fairness through equality of effort. In
Companion Proceedings of the Web Conference 2020, pages (pp. 743–751).
Iosifidis, V., Fetahu, B., and Ntoutsi, E. (2019). FAE: A Fairness-Aware Ensemble Frame-
work. In 2019 IEEE International Conference on Big Data (Big Data), pages 1375–1380, Los
Angeles, CA, USA. IEEE.
Iosifidis, V. and Ntoutsi, E. (2018). Dealing with bias via data augmentation in supervised
learning scenarios. Jo Bates Paul D. Clough Robert J¨aschke, 24:11.
Iosifidis, V., Zhang, W., and Ntoutsi, E. (2021). Online Fairness-Aware Learning with Imbal-
anced Data Streams. arXiv:2108.06231 [cs].
Kehrenberg, T., Chen, Z., and Quadrianto, N. (2020). Tuning Fairness by Balancing Target
Labels. Frontiers in Artificial Intelligence, 3:33.
26Kim,H.,Shin,S.,Jang,J.,Song,K.,Joo,W.,Kang,W.,andMoon,I.-C.(2021).Counterfactual
fairness with disentangled causal effect variational autoencoder. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 35, pages (pp. 8128–8136).
Kozodoi, N., Jacob, J., and Lessmann, S. (2022). Fairness in credit scoring: Assessment, imple-
mentation and profit implications. European Journal of Operational Research, 297(3):1083–
1094.
Kusner,M.J.,Loftus,J.R.,Russell,C.,andSilva,R.(2017). Counterfactualfairness. Advances
in Neural Information Processing Systems, 30.
Lessmann, S., Baesens, B., Seow, H.-V., and Thomas, L. C. (2015). Benchmarking state-of-the-
art classification algorithms for credit scoring: An update of research. European Journal of
Operational Research, 247(1):124–136.
Li, K. J. and Jain, S. (2016). Behavior-based pricing: An analysis of the impact of peer-induced
fairness. Management Science, 62(9):2705–2721.
Li, Y., Wang, X., Djehiche, B., and Hu, X. (2020). Credit scoring by incorporating dynamic
networked information. European Journal of Operational Research, 286(3):1103–1112.
Lodi, A., Olivier, P., Pesant, G., and Sankaranarayanan, S. (2024). Fairness over time in
dynamic resource allocation with an application in healthcare. Mathematical Programming,
203(1-2):285–318.
Lodi, A., Sankaranarayanan, S., and Wang, G. (2023). A framework for fair decision-making
over time with time-invariant utilities. European Journal of Operational Research, page
S0377221723008718.
Lu, X. and Calabrese, R. (2023). The Cohort Shapley value to measure fairness in financing
small and medium enterprises in the UK. Finance Research Letters, 58:104542.
Madiega, T. (2021). Artificial intelligence act. European Parliament: European Parliamentary
Research Service.
Pfohl, S., Duan, T., Ding, D. Y., and Shah, N. H. (2019). Counterfactual reasoning for fair
clinicalriskprediction. InProceedings of the 4th Machine Learning for Healthcare Conference,
volume 106, pages (pp. 325–358).
Richardson, T. S. and Robins, J. M. (2013). Single world intervention graphs (swigs): A unifi-
cation of the counterfactual and graphical approaches to causality. Center for the Statistics
and the Social Sciences, University of Washington Series. Working Paper, 128(30):2013.
Rohner, R. J. (1979). Equal credit opportunity act. Bus. Law., 34:1423.
27Sha, L., Gaˇsevi´c, D., and Chen, G. (2023). Lessons from debiasing data for fair and accurate
predictive modeling in education. Expert Systems with Applications, 228:120323.
Sha, L., Rakovic, M., Das, A., Gasevic, D., and Chen, G. (2022). Leveraging Class Balancing
Techniques to Alleviate Algorithmic Bias for Predictive Tasks in Education. IEEE Transac-
tions on Learning Technologies, 15(4):481–492.
Sun, M., Calabrese, R., and Girardone, C. (2021). What affects bank debt rejections? Bank
lending conditions for UK SMEs. European Journal of Finance, 27(6):537–563.
Voigt, P.andVonDemBussche, A.(2017). TheEUgeneraldataprotectionregulation(GDPR)
(1st ed.). Cham: Springer International Publishing, 10(3152676):10–5555.
Wei, Y., Yildirim, P., Van Den Bulte, C., and Dellarocas, C. (2016). Credit Scoring with Social
Network Data. Marketing Science, 35(2):234–258.
Wu, Y., Zhang, L., and Wu, X. (2019). Counterfactual Fairness: Unidentification, Bound and
Algorithm. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial
Intelligence, pages 1438–1444, Macao, China. International Joint Conferences on Artificial
Intelligence Organization.
Yeoh, P. (2019). Mifid ii key concerns. Journal of Financial Regulation and Compliance,
27(1):110–123.
Zhao, Y., Wang, Y., and Derr, T. (2023). Fairness and explainability: Bridging the gap towards
fair model explanations. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 37, pages 11363–11371.
28