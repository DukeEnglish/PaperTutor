Command-line Obfuscation Detection using Small
Language Models
1st Vojtech Outrata 2nd Michael Adam Polak 3rd Martin Kopp
Cisco systems Cisco systems Cisco systems
Prague, Czechia Prague, Czechia Prague, Czechia
voutrata@cisco.com mipolak@cisco.com markopp@cisco.com
Abstract—Toavoiddetection,adversariesoftenusecommand- detection. As the model is developed from scratch, the paper
line obfuscation. There are numerous techniques of the describes each step of the training process and a thorough
command-line obfuscation, all designed to alter the command-
evaluation of the model on real-world telemetry with an
line syntax without affecting its original functionality. This
emphasis on malicious activity.
variability forces most security solutions to create an exhaus-
tive enumeration of signatures for even a single pattern. In Todemonstratethecapabilitiesofourdetectionmethod,we
contrast to using signatures, we have implemented a scalable presentacasestudywithcommandsforexistingmalwarethat
NLP-based detection method that leverages a custom-trained, avoidedtraditionaldetectiontechniquesandpreviouslyunseen
small transformer language model that can be applied to any
samplesofobfuscatedmaliciouscommandsalldetectedbyour
source of execution logs. The evaluation on top of real-world
model.
telemetry demonstrates that our approach yields high-precision
detections even on high-volume telemetry from a diverse set
II. PRIORART
of environments spanning from universities and businesses to
healthcare or finance. The practical value is demonstrated in a To address the limitations of signature-based detectors, ex-
casestudyofreal-worldsamplesdetectedbyourmodel.Weshow
tensiveeffortshavebeendirectedtowarddevelopingmachine-
the model’s superiority to signatures on established malware
learning solutions for specific obfuscation and malware detec-
known to employ obfuscation and showcase previously unseen
obfuscated samples detected by our model. tion scenarios. One such effort resulted in the creation of a
specialized detector for obfuscation in Java malware [1]. As
I. INTRODUCTION
the model is tailored specifically for Java malware, it utilizes
Traditional malware detection approaches include matching Java bytecode itself for feature extraction. A similar approach
hashes of executed binaries or writing signatures for the mali- is described in [2], where authors apply a machine-learning
cious command-lines. To avoid detection by hash matching, approach to detect obfuscation techniques in Android appli-
attackers leverage common binaries already preinstalled on cations. The authors use the application opcodes (instruction
the targeted machine, so-called living off the land binaries machine code) to detect obfuscated applications in two ways.
(LoLBins). These binaries are used for everyday activities, so The first approach performs feature extraction followed by a
matching their hash is not an option. binary classifier, while the second approach utilizes LSTM-
When LOLBins are utilized, signature matching on top of RNNneuralnetworksontheopcodesdirectly.Amoregeneral
the command-line is one of the few available options for approach is studied in [3], where the authors apply similar
detecting stealthy malware. Similarly, as with hash matching, methods on top of system logs produced by running the
attackerstrytoavoidsignaturesaswell.Theychangethecase Microsoftanti-malwareengineontopoftheWindowsportable
of the characters, add unnecessary white-space characters, or executable (PE) file format.
add characters ignored by the command-line interpreter such All previously described methods differ significantly from
as “ ˆ ” or “ ` ”, etc. Collectively, these modifications that our approach as they specialize in in-depth analysis of the
onlychangethesyntaxofthecommandandnottheexecution bytecode of applications or even directly running the exe-
are called obfuscation techniques. Writing signatures for ob- cutableandrecordingitsactivity.Ourworktargetsobfuscation
fuscationtechniquesisaprohibitivelydifficulttasksincethere detection at a higher level of the command-line interface
are endless possibilities when combining the techniques. This and text analysis. Such an approach is studied in [4], where
complexity makes it impossible for signatures to capture the the authors target the classification of malicious PowerShell
genericobfuscationpatternsandrequiresamoresophisticated commands using various NLP techniques, including convolu-
detection approach. tional/recurrent neural networks. However, obfuscation tech-
In our work, we focus on obfuscation detection on top niquesareonlymentionedasonepossiblefeatureofmalicious
of general command-line execution logs, specifically on top samples,andtheworkfocusesonPowerShellonly.Adetection
of LOLBin executions. Since the command-line resembles approach targeting specifically obfuscation for PowerShell
a language where each command represents a sentence, we has been developed by Daniel Bohanon’s project, Revoke-
propose a transformer-based language model for obfuscation Obfuscation [5]. Their method relies on extracting features
4202
guA
5
]RC.sc[
1v73620.8042:viXraFig. 1. Tokenization of raw command-line compared to tokenization of preprocessed command-line by the same tokenizer. The GUID pattern in the raw
command-lineistokenizedintomanynon-meaningfultokens.Incontrast,asinglegeneraltokenrepresentsthepatterninthetokenizedpreprocessedcommand-
line.
from PowerShell’s inbuilt Abstract Syntax Tree followed by TABLEI
a shallow classifier. While a powerful model, it can only PATTERNSREPLACEDINTHECOMMAND-LINE
detectPowerShellobfuscationandisnotscalabletolarge-scale
Replacedpattern Token
deployment due to the expensive construction of the syntax
GUID(globallyuniqueidentifier) [GUID]
tree followed by feature extraction.
IP(bothv4andv6) [IP]
We build upon previous works in two main aspects. Firstly
Date(numerousformats) [DATE]
by leveraging the transformer-based architecture proven supe-
Number [NUM]
rior to other natural language processing methods in recent
URL [URL]
years. As obfuscation techniques greatly impact the structure
of the command-line, the transformer model easily recognizes
obfuscated samples. Consequently, smaller models scalable to
is replaced with a generic token [GUID], resulting in:
high-volume telemetries can be utilized without loss of clas-
sification performance. Secondly, we were able to extend the /C C:\WINDOWS\TEMP\{[GUID]}.bat
model’sdetectioncapabilitiesbeyondPowerShelltonumerous
The preprocessed command-lines are more straightforward to
other LOLBins.
tokenize, as demonstrated in Figure 1. In the raw command,
III. METHOD theGUIDpatternissplitintonumerousshorttokens,resulting
in a significantly longer token sequence. In comparison, the
Developing a transformer-based language model from
preprocessed command-line contains only a single [GUID]
scratch requires multiple data preprocessing and model train-
token representing the entire pattern.
ing steps. The process can be separated into the following
Incaseswherethesepatternsareobfuscated(e.g.,Raspberry
phases:
Robin malware), pattern matching will fail; consequently, the
1) Data preprocessing
obfuscated parts will not be replaced.
2) Tokenizer training
3) Model pre-training
B. Tokenizer training
4) Model fine-tuning
Beforethepreprocessed datacanbefed into themodel,the
Weprovideadetaileddescriptionofeachstepinitsrespective
text has to be split into smaller tokens by a trained tokenizer.
section.
Onefeasibleapproachistoadaptanalreadyexistingtokenizer,
A. Data preprocessing such as the tokenizer from BERT [6]. However, most pre-
trainedtokenizers(alongwiththemodel)aretrainedonnatural
Transformer-basedlanguagemodelscomprehendtextbydi-
language.Thecommand-linessignificantlydifferfromnatural
vidingitintosmallerunits(tokens)andassessingthemeaning
language in both syntax and semantics. Since the tokenizer
of each token in relation to its context, such as the tokens
quality heavily influences downstream task performance and
in the rest of the sentence. For this mechanism to yield
the speed of the resulting model, we train a custom tokenizer
effective results, the tokens should convey relevant meaning.
for command-line logs (presented in Section IV-B).
In command-line data, however, many artifacts exist that are
The negative impact on downstream task performance was
notrelevanttootherpartsofthecommandorthetaskathand.
studied on multilingual language models and their tokeniz-
We have identified that concrete values of specific patterns,
ers[7].Theauthorscompareddedicatedmonolingualtokeniz-
such as IP addresses, dates, or GUIDs in the command-lines,
ers with the multilingual tokenizer from the mBERT model.
areirrelevantforobfuscationdetection.Wereplacethesenon-
They concluded that custom tokenizers positively impact the
meaningful command-line parts with a generic token in the
model’s downstream performance in nearly all cases.
preprocessingphasetoavoidfeedingthemintothemodel.All
The second disadvantage to consider is the speed of the
such patterns and their substitute tokens are listed in Table I.
models. As pointed out in the recent study [8], the out-
For example, the GUID in the following cmd command-
of-domain tokenizers result in less compressed data (statis-
line:
tically splitting the same text into more tokens) even with
/C C:\WINDOWS\TEMP\{EFA6314F- significantlylargervocabularysizes.Theinferencespeedwith
05CB-4D5D-98C5-8FBC574E96A6}.bat custom tokenizers then benefits from smaller vocabulary size(smaller input embedding dimension) and more compressed
data (fewer tokens produced for the same amount of text).
Oncethetokenizeristrained,itcanprocesstheinputstring,
representingthecommand-line,intoatokensequencethatcan
be analyzed by the language model.
C. Model pre-training
Generally, the performance of language models on down-
stream tasks benefits from the model learning the language
itself first. This phase of model training, called pre-training,
utilizes large volumes of data without labels to achieve that
goal. There are numerous established pre-training methods
in natural language processing, such as masked language
modeling introduced in BERT [6] or the standard objective Fig.2. Theγ parametercontrolsfocusonalreadywell-classifiedexamples.
for large language models, causal language modeling [9]. Thecross-entropylossisidenticaltoFLwithγ setto0.Imagesource:[11].
For our use case, we have selected the discriminative pre-
training objective introduced by ELECTRA [10]. The reasons
Wherep isthemodel’spredictedprobabilityoftheground
for selecting ELECTRA are twofold: t
truth label, and γ is the focusing hyperparameter of the loss
• It is computationally efficient (as demonstrated in the function. The impact of the γ parameter and comparison of
original paper).
FL to cross-entropy is displayed in Figure 2.
• The training objective is semantically similar to obfusca- The machine learning community already widely adopted
tion detection.
FL for class imbalanced problems. For example, in cyber-
Itutilizestwotransformer-basedlanguagemodels,agenerator,
security, FL was successfully utilized in intrusion detection
and a discriminator. The generator model is trained with the
systems [12], [13].
standard masked language modeling objective, where in each
sentence, a portion of tokens (e.g., 15%) is replaced with a IV. TRAINING
[MASK]token,andthemodelistrainedtopredicttheoriginal
In this section, we provide a detailed description of the
identities of the masked-out tokens. The discriminator model
training process from Section III using our data. We begin by
istrainedtorecognizewhichtokenswerereplacedbythegen-
describingthespecificdataused,followedbyacomprehensive
erator model. As the generator gets better at predicting tokens
account of each training phase.
contextuallysimilar tothe originalones,the discriminatorhas
to learn more nuanced differences in the sentence structure.
A. Data
After the training, the generator model is discarded, and
the discriminator model is fine-tuned for downstream tasks. While the presented methods are designed to work on top
For more detailed information about the pre-training, refer to of any source of execution logs, the particular data used in
the original paper [10]. this work were gathered from the Cisco Secure Endpoint
(CSE) telemetry. In total, we have gathered approximately
D. Model Fine-tuning 350 million unique execution logs for training and evaluation.
In this section, the discriminator model is fine-tuned to This volume corresponds to 20 days of telemetry for ∼1000
detect obfuscated commands. A common issue with detection private networks of various sizes, ranging from universities to
tasks in the cybersecurity domain is the inherent class imbal- industry, finance, or healthcare.
ance, as malware activities hide in large volumes of benign The gathered data contains execution logs for the following
traffic. Obfuscation detection is no exception. From gathered LOLBins: PowerShell, cmd, msiexec, rundll, and explorer.
domain knowledge, we estimate the ratio could be as high as An example of an obfuscated command for each selected
oneobfuscatedsampleperhundredsofthousandsofexecution executable is displayed in Table II.
logs. Such class imbalance makes obfuscation detection an The real-world obfuscated samples for this paper were
exceptionally hard classification task, as the model must be obtained in cooperation with Cisco Talos. However, because
extremely precise to avoid overwhelming cybersecurity ana- real-world samples are scarce and expensive to acquire, we
lysts with false positive samples. were able to gather only around 1500 unique real-world
To overcome the class imbalance, we adapt both the fine- samples.Suchalowamountofpositivesamplescouldpresent
tuningdataset,describedinSectionIV-D,andthemodeltrain- two severe drawbacks. First, a low variety of obfuscation
ing itself. Specifically, we utilized focal loss (FL) introduced techniques is represented in data for the model to learn and
in [11] for class imbalanced problems. The focal loss for one generalize. Second, as mentioned in a study on preprocessing
sample is defined as: methods for class imbalanced datasets [14] and validated in
our experiments, with increasing class imbalance, the model
FL(p )=−(1−p )γlog(p ) training becomes unstable, and models diverge to a majority
t t tTABLEII
EXAMPLESOFOBFUSCATEDSAMPLESFOREACHEXECUTABLE.THEOBFUSCATIONTECHNIQUESDIFFERBASEDONEACHBINARY,THEOBFUSCATION
FORRUNDLLISMAINLYINOBFUSCATINGTHENAMEOFTHEDLLLIBRARY,WHILECMDANDPOWERSHELLALLOWFORMORECOMPLEX
OBFUSCATIONTECHNIQUES.
Binary ObfuscatedCommand
rundll rundll32.exe \ˆ&&&ˆ&&ˆˆˆ&&ˆˆˆˆˆ&ˆ&&&&ˆˆ&&ˆˆˆˆ&ˆ.{[GUID]},zVh5Hfr3Vd5DLrFl
msiexec mSIexeC -Q -IhTtP://NT3[.]XyZ:8080/5mGgMqZvXTg/DESKTOP_NAME=USER_NAME
explorer exPLOrER RemoVAbLe Disk
PowerShell powershell.exe -exec bypass -noni -nop -w 1 -C IEx( $( set-iTem ’vaRiABLE:OfS’ ’’)+
[STrInG]( ’91>78!101g11...j116;99’.SpLit(’{;gX:<g!j>’ ) | forEACH{([int] $_-As[cHAR]) }) +
$(sET-ItEm ’VaRIaBLe:oFS’ ’ ’))
cmd C:\WINDOWS\system32\cmd.exe /cPˆoˆwˆeˆrˆSˆhˆeˆlˆlˆ.eˆxˆeˆ -NoˆExit
-Exˆec Byˆpass -ˆEC YwBhAGˆwAYwA=
class classifier, not detecting any obfuscated samples. There-
fore,onlyalimitednumberofbenignsamplescanbeincluded
in the training set.
Toavoidthesedisadvantages,weleveragedtwoopen-source
obfuscation tools, Invoke-Obfuscation [15] for PowerShell
obfuscation, and Invoke-DOSfuscation [16] for cmd obfusca-
tion to generate additional ”artificially obfuscated” samples.
For further details about the artificial samples and utilized
obfuscation techniques, please refer to Appendix A.
As each step of model training utilizes a different dataset,
thedetailsoftheirconstructionaredescribedineachrespective
section.
B. Tokenizer training
Training a proper tokenizer is very data intensive. We used
300 million execution logs to train a custom tokenizer with Fig. 3. Comparison of the produced number of tokens for out-of-domain
theWordPiecealgorithm[17]andtheHuggingfaceTokenizers tokenizerandcustomtokenizerswithvariousvocabularysizesontheevalu-
ationdataset.
library[18].Sincethevocabularysizeisakeyhyper-parameter
fortokenizertraining,wehavetrainedtokenizerswithvarious
vocabulary sizes: 1k, 5k, 10k, 20k, and 30k to asses the vocabulary sizes offer far better performance from the per-
optimal size. spective of inference speed.
In the evaluation of tokenizer quality, we focus on two To further validate that the custom tokenizers are more
aspects: the data compression and manual inspection of suitable for execution logs data, we have manually inspected
tokenized command-lines. The data compression measures the produced token sequences on random samples from the
how many tokens each tokenizer produces for the evaluation evaluation dataset. To illustrate the difference in tokeniza-
dataset. In a study on tokenizers [8], authors utilize a similar tion, we present an example execution log tokenized by
metric, data compression ratio to a baseline tokenizer, to three tokenizers, bert-base-cased, custom tokenizer 1k, and
study the impact of various tokenizers on the inference speed. custom tokenizer 20k in Figure 4.
Specifically,theinferencespeedisinfluencedbythenumberof The figure clearly illustrates the tokenizers’ ability to pro-
produced tokens and the tokens’ embedding dimension equal duce meaningful tokens representing specific patterns in the
to the vocabulary size of the utilized tokenizer. execution logs, such as “cmd.exe”, “System32”, or “WIN-
The data compression metric was evaluated on a dataset of DOWS”. As these patterns are prevalent and have an estab-
never-seen5millionuniquecommand-lines.Intheevaluation, lished meaning in the execution logs, we expect the tokenizer
wealsoincludedapre-trainedtokenizerwithavocabularysize to produce one token for each of such patterns.
of 32k trained primarily on natural language (tokenizer from The analysis led to the following conclusions. The 1k
Huggingfacehub-bert-base-cased)asabaselinemodel.The vocabulary size is insufficient for an accurate representation
results are displayed in Figure 3. of execution logs, leading to the tokenizer splitting common
The results show that custom-trained tokenizers achieve patterns into arbitrary tokens. Also, the large out-of-domain
significantlybetterdatacompressionatfarsmallervocabulary tokenizer fails to produce meaningful tokens from the preva-
sizes. For instance, the custom tokenizer with a vocabulary lentexecutionlog-specificpatterns.Thecustomtokenizerwith
size of 10k produces more than 30 million fewer tokens than a sufficient vocabulary size (20k) produces meaningful tokens
theout-of-domaintokenizerwhilebeingmorethanthreetimes representing specific patterns in the execution logs. From our
smaller. Consequently, the custom tokenizers with sufficient inspection, the custom tokenizer with a 20k vocabulary sizeFig. 4. Oneexecution logtokenized by out-of-domain tokenizer (BERTTOKENIZER) andtwo customtokenizers forcommand-line data withvocabulary
sizes1kand20k,respectively.
TABLEIII
FINE-TUNEDMODEL’SPERFORMANCEONTHETESTDATASET.
Precision Recall support
Benign 0.9987098 0.9999615 130045
Obfuscated 0.9996846 0.9991448 12846
as discussed in Appendix C. As the non-pre-trained model
also performed worse in evaluation, all further experiments
are done with the pre-trained model.
D. Fine-tuning
While the pre-training phase is responsible for getting the
model used to the general data, the fine-tuning phase is, in
our case, a supervised classification task during which the
Fig. 5. Training loss curves for both pre-trained and non-pre-trained small
model learns to classify obfuscated commands. As with most
modelsonthefine-tuningdataset.Thenon-pre-trainedmodelshowsproblem-
aticinstabilityduringtrainingascomparedtothepre-trainedmodel. supervised classification tasks, the underlying dataset is a
crucialaspectoftheproblem.Thefine-tuningdatasetweused
has the following structure:
offersthebestperformancewhenconsideringthetokenquality
together with the inference speed and is therefore used for all • 540k randomly sampled benign data
further experiments. • 52.5k artificially obfuscated samples
• 1.5k real-world obfuscated samples
C. Pre-training
The imbalance ratio was set to 10 benign commands per one
The ELECTRA pre-training objective is a self-supervised obfuscatedcommand(realorartificial)asincreasingitbeyond
task that does not require labels but benefits from a large 30:1 led to training instability and model degradation to the
amount of data. We used approximately 40 million randomly majority classifier. To not further increase the class imbalance
sampled execution logs to pre-train models with discriminator and include a wide variety of benign commands simulta-
modelsizes(inthenumberofparameters)9M,6M,4.3M,and neously, the positive class was supplemented by artificially
750K, arbitrarily named large, medium, small, and miniature. obfuscated samples. The disadvantage of oversampling the
The paired generator models are scaled down by a factor positiveclasswithartificialsamplesisthatspecificobfuscation
of 4. Since a larger than small model did not improve the techniques present only in real-world samples become sparse
downstream task performance, the small model is used for all in the training data, and the model might not learn to detect
further training and experiments. them. We validate the model’s performance on individual
The details on model parameters and the pre-training pro- techniques as well as provide details on the training process
cess are available in Appendix B. For our use case, it is vital and caveats in Appendix C.
to analyze whether the pre-training of such a model limited Theperformanceofthefine-tunedmodelonthetestdataset,
in size has a positive impact on downstream task learning and composedof1/4ofthetotalsamples,ispresentedinTableIII.
performance. The test dataset metrics show that the model learned the
The comparison of pre-trained versus non-pre-trained mod- classificationtaskalmostperfectly.Asmanyoftheobfuscation
els learning obfuscation detection is displayed in Figure 5. techniques significantly impact the structure of the command-
The figure shows that the pre-trained model is more stable line, it was the expected outcome. The different structure is
during learning than the non-pre-trained model. The training theneasilyrecognizablebyananalystevenwithoutanin-depth
instability is caused by the nature of the fine-tuning dataset, analysis of the executed commands.TABLEIV TABLEV
FINE-TUNEDMODEL’SEVALUATIONONTHREEDAYS’WORTHOF FINALMODEL’SEVALUATIONONFOURDAYS’WORTHOFNEVER-SEEN
NEVER-SEENDATA.THEDETECTIONSOFTHEMODELWEREMANUALLY DATA.THETABLEPRESENTSDETECTIONSOFTHEMODELMANUALLY
LABELEDINTOTHREEDISTINCTCATEGORIES.THEDECISIONTHRESHOLD LABELEDINTOTHREEDISTINCTCATEGORIES.
OF0.5WASAPPLIED.
Samplecategory Count
Samplecategory Count
obfuscatedmalicious 64
obfuscatedmalicious 38
obfuscatedbenign 6
obfuscatedbenign 148
non-obfuscated(falsepositive) 21
non-obfuscated(falsepositive) 583
The supplemented samples from the train portion of the fine-
E. Real-world telemetry
tuning dataset contain all seen obfuscation techniques and
While the model’s performance metrics on the test dataset wereaddedfortworeasons:first,tobalanceoutthehighclass
are impressive, it is vital to analyze them with respect to imbalance to a 1:10 ratio, and second, to prevent the model
the real-world telemetry, which has class imbalance orders of from overfitting on the 38 positive samples of low variety
magnitude higher compared to the test dataset. found during manual labeling.
Higher class imbalance naturally leads to a significantly In this last phase, the model was trained to correct its
lowercountofpositivesampleswhilethenumberofproduced predictions on approximately 1700 gathered samples, mostly
falsepositivesremainsapproximatelythesame.Asaresult,the consisting of manually labeled examples. The resulting model
precision of the model on real-world telemetry is significantly is used in the evaluation section.
lower compared to the evaluation on the test dataset.
We can observe the effect when the model is evaluated V. EVALUATION
on three days of never-seen telemetry with a significantly
In this section, we focus on the two most important aspects
higher class imbalance (our estimate is approximately 1:300
of the final model: the classification performance and the
000). Out of 24 million commands, the model assigned a
inference speed.
probability of obfuscation greater than 0.1 to approximately
1600 samples. To assess the model’s performance, we have
A. Classification performance
manually categorized these samples into the following three
categories: To assess the final model’s performance, we evaluated the
• obfuscated malicious - Obfuscated commands produced model on 4 more days of new never-seen telemetry and again
byeitherrecognizedmaliciousactivityorcommandswith manually labeled the commands with an assigned obfuscation
ahighprobabilityofbeingproducedbymaliciousactivity probability of 0.5 or higher.
worth investigating further. The results, presented in Table V, show that the last fine-
• obfuscated benign-Obfuscatedcommandsproducedby tuningstep,wherethefine-tunedmodeladjusteditspredictions
non-malicious activity (e.g., excessive use of cmd escape onasmallamountofincorrectlypredictedsamples,drastically
character, using environment variables in the commands, improved the model’s precision. Even without raising the
...). decision threshold above 0.5, the model achieved precision
• non-obfuscated above 70%.
Furthermore, Figure 6 indicates that we can achieve even
The results for samples with assigned probability higher
higher precision by raising the decision threshold. However,
than the decision threshold of 0.5 are presented in Table IV.
the higher precision naturally comes at the cost of losing true
Whilethemodelproducedapproximatelytheexpectedamount
positive detections with lower obfuscation probability scores.
of false positives, it achieved only around 5% precision
(countingonlyobfuscatedmalicioushitsastruepositives)due
B. Inference speed
to the low number of true positive samples.
To improve the model’s precision on obfuscated malicious The last key parameter of the developed model is the
commands, we further trained the fine-tuned model to correct inferencespeed.Weevaluatedthemodelinferencespeedwith
its predictions on all the manually labeled samples with the full weights (fp32) and the model weights converted to
an assigned probability of 0.1 or higher. Since analysts are half-precision (fp16).
generally also not interested in obfuscated benign samples, Figure 7 shows that reducing the model weight’s precision
the following dataset was constructed for the final training: resultsinaninferencespeedgainof∼50%whilenosignificant
• negative class consists of non-obfuscated and obfus- classification performance loss was observed compared to the
cated benign samples full model.
• positive class consists of the obfuscated malicious sam- Overall, the small model size results in extremely fast
plessupplementedwithobfuscatedsamplesfromthetrain inference speed, as the model is capable of analyzing million
portion of the fine-tuning dataset execution logs in 2.5 minutes even on a single low-end GPU.Fig.6. Finalmodel’sprecisionandnumberofdetectionsbasedondecisionthreshold.Themetricswereevaluatedonfourdaysofnever-seentelemetry.
A. Raspberry Robin
Raspberry Robin is a worm that leverages multiple pre-
installed binaries on the operating system, such as msiexec,
rundll32, odbcconf, etc. Due to the malware’s prevalence, it
has a well defined kill-chain [19], [20]:
1) initial access through an infected external drive with a
malicious lnk file
2) cmd executes the malicious lnk file
3) execution of explorer with the infected external drive as
the argument
4) msiexecinstallsamaliciousDLLfromaremotelocation
5) rundll32launchestheDLLusingodbcconf orfodhelper,
etc.
6) outboundconnectiontoaCommandandControl(C&C)
server through TOR
Since the kill-chain is well defined, there are multiple
reports[19],[20],[21]thatprovidesignature-likepseudo-code
Fig.7. Modelinferencespeedmeasuredonarandomsampleofonemillion to detect Raspberry Robin. In the context of this case study,
executionlogs.Thespeedwasmeasuredforboththemodelwithfullweights we focus only on the signatures that correspond to commands
(fp32)andwithhalf-precision(fp16)weights.Themeasurementwasdoneon
of the kill-chain containing obfuscation techniques.
NVIDIA®T416GBGPU.
For instance, none of the reports contain a signature for
Step 2 of the kill-chain utilizing the white-space insertion
technique:
VI. CASESTUDY
C:\Windows\System32\cmd.exe
\r\r\r\t\r\t\r\r\r\t\r\t\r
So far, we presented development steps for a custom NLP-
/RCmD<qjM.chK
baseddetectionmodelandcompleteditsevaluation.Tofurther
demonstrate the model’s value, we present specific examples In this step cmd executes a malicious script “qjM.chK”
of obfuscated commands detected by our model that would to which the lnk file on the external drive from Step 1
be missed by common detection approaches for the malware points to. As the executions from this step usually contain
theywereattributedto.Specifically,thiscasestudyshowcases a plethora of combinations, types, and locations of white-
variants of Raspberry Robin and Gamarue, which do not space characters, writing a reliable signature for this step
follow previously known patterns. Lastly, we present two new is exceptionally difficult. Since the only other reoccurring
obfuscated samples whose attribution to a malware family or feature of execution is the commonly used “/R” switch, the
campaign is currently unknown to us. options for a reliable signature-based detector are limited.However, for our model, the various white-space insertions theexpected10-70characterstoalowercountenoughtoevade
into the command are obvious signals that the command was detection,ascanbeseeninthesampledetectedbyourmodel:
obfuscated.
C:\Windows\System32\rundll32.exe \BrP
The next phase of the kill-chain, Step 3, is also reliably
5lzjT.P5d.RvTxRB.zfLtZJ.BjT.1lV.vbHl.l
detected by our model while no signatures are given by the
VFzl,cUySwoImK8cAeW4Y
reports. The execution usually contains an obfuscated execu-
tion of the binary explorer along with the infected external This again demonstrates that our obfuscation detection
drive: model can detect novel samples even for established malware
with well documented kill-chain.
ExplOˆRER USB Dˆrive
C. Unknown malware obfuscation-based detections
The use of obfuscation techniques, along with changing
names of the external drives, results in virtually no overlap Our model’s biggest strength, however, lies in detecting
among the execution logs of this step in the kill chain. unknown/unseenobfuscatedcommands.Wepresenttwonovel
Therefore, the only consistent feature of the executions is the examples it has detected to demonstrate the model’s capabili-
use of obfuscation techniques, which our model detects. ties.
Thesubsequentofthekill-chain,Step4,hasasignature-like The first example contains two hard-to-detect techniques:
pseudo-code [20] defined for it: variable names obfuscated by using “-”, “#”, etc. in their
names and constructing the command from these variables
• process == (‘msiexec’)
during execution as can be seen in the following command:
• process command line includes (‘http:’, ‘https’)
• process command line includes (‘-q’, ‘-Q’) cmd.exe /c set --$#$--=net&&
The signature, however, can be evaded with obfuscation set ’’’=at&&set ;;;;=st&&
techniques, as can be seen in one of our detections [19]: cmd.exe /c %--$#$--%%;;;;%%’’’
% -s -p UDP
mSIexeC -Q-IhtTP://NT3[.]XyZ:8080/
5qGgVaPvXFg/desktop-name=username The goal of this command is to check statistics of UDP
traffic using netstat, which could be used as a part of the
This command is obfuscated using mixed-character case. It initial discovery process in a compromised environment.
may not seem like a significant change, but many signatures
Anotherexampleofanobfuscatedcommandflaggedbyour
expect the protocol to be in its standard lower-case form. model is the following PowerShell execution:
Furthermore, the flags “-Q-I” are concatenated with the URL,
C:\Windows\System32\WindowsPowerShell\
which many signatures would not account for. Even such tiny
v1.0\powershell.exe -w hidden $test=
changes are sufficient to avoid detection in many cases.
’htt’+’p://’+’XX[.]XX[.]XX[.]XX’+
As demonstrated by the detections observed in our teleme-
’/login.php’;$response =
try, we were able to catch numerous steps of the kill-chain,
$(New-Objectsystem.Net.WebClient)
including parts that would be missed by signature detection.
.DownloadString($test);
B. Gamarue $log=$response;
$command =[scriptblock]::Create($log);
Gamarue, also known as Andromeda or Wauchos, is a
&$command;
worm/botnet consistently ranking among the most prevalent
malware families. Due to the malware’s prevalence and age, The PowerShell sample is obfuscated by splitting the origi-
it has a well documented kill-chain [22]: nalcommandintomultiplestringsandconcatenatingtheircon-
1) Initial access through an infected external drive with a tent during execution. Specifically, this command downloads
malicious lnk file and executes the content from a URL with an embedded IP
2) Malicious code installation via rundll32 referencing the address:“http://XX[.]XX[.]XX[.]XX/login.php”.Thecontents
lnk file oftheexecutedscriptarecurrentlyunknownsincethewebsite
was not available at the time of writing this paper. As this is
Similarly, as for Raspberry Robin, there is a signature-like
a common technique used by malware such as Metamorfo (or
pseudo-code defined for the execution of Step 2 of the kill-
Casbaniero)[23],thecommandishighlysuspiciousandlikely
chain:
malicious.
• process == (‘rundll32’)
• process command line includes (‘/\S{10, 70}.\S{10, VII. CONCLUSION
70},\w{16}/’)
In this paper, we presented a novel method for detect-
The signature already accounts for obfuscation since the ing obfuscated command-lines using custom-trained small
arguments of rundll32 use uncommon character combinations transformer neural networks. We provided a detailed step-
changing in each execution. However, even slight modifica- by-step description of training such a model from scratch.
tions of the command by changing the counts of letters from Notably, the NLP-based approach allowed us to extend themodel’s detection capabilities beyond PowerShell to multiple [14] R.Halusˇka,J.Brabec,andT.Koma´rek,“Benchmarkofdatapreprocess-
other LOLBins, which is a significant advancement compared ingmethodsforimbalancedclassification,”in2022IEEEInternational
ConferenceonBigData(BigData),pp.2970–2979,IEEE,2022.
to previous works on obfuscation detection. By evaluating
[15] D. Bohannon, “Invoke-obfuscation.” [Online]. Available: https://
numerousdaysofreal-worldtelemetry,wehavedemonstrated github.com/danielbohannon/Invoke-Obfuscation, 2016. Accessed June
that the model produces high-precision detections even for 6,2024.
[16] D. Bohannon, “Invoke-dosfuscation.” [Online]. Available: https://
largetelemetries.Thepracticalvalueofourmodelwasfurther
github.com/danielbohannon/Invoke-DOSfuscation,2018.AccessedJune
highlighted in a case study of two prevalent malware families 6,2024.
that heavily utilize obfuscation in various parts of their life- [17] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,
M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al., “Google’s neural
cycles. The model detected novel samples that did not follow
machine translation system: Bridging the gap between human and
previously known patterns. As future work, we are extending machinetranslation,”arXivpreprintarXiv:1609.08144,2016.
ourdetectioncapabilitiesfromWindows-basedexecutionlogs [18] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,
P.Cistac,T.Rault,R.Louf,M.Funtowicz,etal.,“Huggingface’strans-
to Unix-based systems.
formers: State-of-the-art natural language processing,” arXiv preprint
arXiv:1910.03771,2019.
Acknowledgments [19] O. M. Erdogan, “Raspberry robin: Highly evasive worm spreads over
external disks.” [Online]. Available: https://blogs.cisco.com/security/
We would like to thank all cybersecurity analysts (Cisco raspberry-robin-highly-evasive-worm-spreads-over-external-disks,
TalosteamandTreoteam)whoprovideduswithmuch-needed 2022. AccessedJune26,2024.
[20] Red Canary, “Raspberry robin.” [Online]. Available: https:
real-world obfuscation samples for our research and were
//redcanary.com/threat-detection-report/threats/raspberry-robin/, 2023.
instrumental in analyzing obfuscated samples with a special AccessedJune14,2024.
thankstoMeghanCorreafromCiscoTalosforherexceptional [21] L. Podber and S. Rand, “Raspberry robin gets the worm early.”
[Online]. Available: https://redcanary.com/blog/threat-intelligence/
contributions and cooperation. We would also like to express
raspberry-robin/,2022. AccessedJune14,2024.
our gratitude to Jaroslav Hlavac, Benjamin Paterek, and Petr [22] Red Canary, “Gamarue.” [Online]. Available: https://redcanary.com/
Pulc for their insightful discussions and comments. threat-detection-report/threats/gamarue/,2020. AccessedJune3,2024.
[23] D.Saunders,O.Bavly,N.Lifshitz,andI.Shohat,“Breakingdownthe
casbaneiroinfectionchain.”[Online].Available:https://www.sygnia.co/
REFERENCES blog/breaking-down-casbaneiro-infection-chain/, 2022. Accessed July
24,2024.
[1] R. Kumar and A. R. E. Vaishakh, “Detection of obfuscation in java [24] J.Kaplan,S.McCandlish,T.Henighan,T.B.Brown,B.Chess,R.Child,
malware,”ProcediaComputerScience,vol.78,pp.521–529,2016. S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural
[2] A.Bacci,A.Bartoli,F.Martinelli,E.Medvet,andF.Mercaldo,“Detec- languagemodels,”arXivpreprintarXiv:2001.08361,2020.
tionofobfuscationtechniquesinandroidapplications,”inProceedings
of the 13th International Conference on Availability, Reliability and APPENDIXA
Security,pp.1–9,2018. ARTIFICIALLYOBFUSCATEDSAMPLES
[3] B. Athiwaratkun and J. W. Stokes, “Malware classification with lstm
and gru language models and a character-level cnn,” in 2017 IEEE Thissectiondescribestheprocessofcreatingtheartificially
international conference on acoustics, speech and signal processing obfuscated samples for the fine-tuning dataset from the gath-
(ICASSP),pp.2482–2486,IEEE,2017.
ered execution logs. Generally, the execution log consists of
[4] D. Hendler, S. Kels, and A. Rubin, “Detecting malicious powershell
commandsusingdeepneuralnetworks,”inProceedingsofthe2018on two parts. The first part specifies the executed LOLBin, while
Asia conference on computer and communications security, pp. 187– thesecondrepresentstheexecutable’sarguments.Forexample,
197,2018.
in the following execution log:
[5] D.BohannonandL.Holmes,“Revoke-obfuscation.”[Online].Available:
https://github.com/danielbohannon/Revoke-Obfuscation, 2017. Ac-
C:\WINDOWS\system32\cmd.exe /c
cessedJune4,2024.
[6] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert:Pre-training tasklist.exe /fi imagename eq logonui*
of deep bidirectional transformers for language understanding,” arXiv /fi session eq 11,684
preprintarXiv:1810.04805,2018.
[7] P. Rust, J. Pfeiffer, I. Vulic´, S. Ruder, and I. Gurevych, “How good the executed binary is
is your tokenizer? on the monolingual performance of multilingual
languagemodels,”arXivpreprintarXiv:2012.15613,2020. C:\WINDOWS\system32\cmd.exe
[8] G. Dagan, G. Synnaeve, and B. Rozie`re, “Getting the most out of
your tokenizer for pre-training and domain adaptation,” arXiv preprint andtheactualexecutedcmdcommandgivenbythearguments
arXiv:2402.01035,2024.
follows:
[9] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskever,etal.,
“Language models are unsupervised multitask learners,” OpenAI blog, /c tasklist.exe /fi imagename eq
vol.1,no.8,p.9,2019.
[10] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-
logonui* /fi session eq 11,684
training text encoders as discriminators rather than generators,” arXiv
As the executed binary is supplied by the operating system,
preprintarXiv:2003.10555,2020.
[11] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dolla´r, “Focal loss we apply obfuscation techniques to the actual command only.
for dense object detection,” in Proceedings of the IEEE international The command is passed down to the respective obfuscation
conferenceoncomputervision,pp.2980–2988,2017.
tool, either Invoke-Obfuscation for PowerShell or Invoke-
[12] M. Mulyanto, M. Faisal, S. W. Prakosa, and J.-S. Leu, “Effectiveness
of focal loss for minority classification in network intrusion detection DOSfuscation for cmd.
systems,”Symmetry,vol.13,no.1,p.4,2020. We have selected a set of obfuscation techniques to be
[13] A.S.Dina,A.Siddique,andD.Manivannan,“Adeeplearningapproach
appliedtoarandomsampleofextractedcommands.However,
for intrusion detection in internet of things using focal loss function,”
InternetofThings,vol.22,p.100699,2023. some techniques apply only to commands containing specificFig. 8. Pre-training loss curves for the small model size. The generator stops learning early in the training, limited by its smaller size. The discriminator
stopslearningshortlyafter.
Fig.9. Largemodelpre-trainingmetrics.Boththegeneratoranddiscriminatormodelscontinuelearningthroughouttheentirelearningprocess.
elements, such as strings, and there is no guarantee that a in:
randomly sampled command would contain these elements.
Invoke-Obfuscation -ScriptBlock
In such cases, the commands would not be obfuscated and
{command-to-obfuscate} -Command
would introduce wrongly labeled samples in our dataset. To
’token,all,1’ -Quiet
minimize such occurrences, we have tried to select/combine
techniques that apply to a more general set of commands. Similarly, for Invoke-DOSfuscation and the technique “en-
Utilized obfuscation techniques are listed in Table VI. vironment variable light”:
Each listed technique is applied to a sample of random
Invoke-DOSfuscation -Command
commands from the telemetry with the respective obfuscation
command-to-obfuscate -CliCommand
tool so that all listed techniques are equally represented in
"encoding,1" -Quiet;
the fine-tuning dataset. When calling each obfuscation tool to
obfuscatecommandsfromPowerShellconsole,the“toolcom-
APPENDIXB
mand”columncontainsastringthatspecifieswhichtechnique
PRE-TRAININGDETAILS
the tool should apply1. In the case of Invoke-Obfuscation, the
command for applying the “token obfuscation” technique to The parameter configurations for each model size are listed
an extracted command “command-to-obfuscate” would result in Table VII. The configurations were inspired by the paper
introducing the ELECTRA style of pre-training. Training
hyper-parameters were copied from the study for all training
1Whenspecifyingthetechniqueintheinteractiveconsoleoftheobfuscation
tool,thecommashavetobereplacedbyforwardslashes. runs and are listed in Table VIIITABLEVI
UTILIZEDOBFUSCATIONTECHNIQUES.
executable obfuscationtechnique toolcommand
token obfuscation token,all,1
command compressing compress,1
command encoding ASCII encoding,1
command encoding hex encoding,2
command encoding octal encoding,3
command encoding binary encoding,4
powershell
command encoding SecureString encoding,5
command encoding BXOR encoding,6
command encoding special chars encoding,7
command encoding whitespace encoding,8
command string concatenate string,1
command string concatenate reorder string,2
environment variable light encoding,1
environment variable medium encoding,2
payload concat light payload,concat,1 Fig.10. Fine-tuninglosscurveofthepre-trainedmodel.
cmd payload concat medium payload,concat,2
payload concat reverse light payload,reverse,1
APPENDIXC
payload concat reverse medium payload,reverse,2
MODELFINE-TUNING
payload forcode payload,forcode,1
The fine-tuning dataset contains samples that inherently
cause the model training to be unstable. The fine-tuning loss
curve for the pre-trained model is displayed in Figure 10. The
Model pre-training was performed for every model size model starts learning the task quickly, as expected. However,
and custom tokenizer with every vocabulary size. The im- the loss function peaks occurring during training indicate
pact of vocabulary size on pre-training was consistent with inconsistenciesinthedataset.Closerinspectionofthesamples
the tokenization evaluation in Section IV-B, where the best causinglossfunctionpeaksrevealsthatthreetypesofsamples
performance provided custom tokenizers with sizes 10k and are responsible for confusing the model:
higher. To be consistent with the rest of the paper, all further • incorrectly obfuscated (by the obfuscation tool) com-
described experiments utilize the custom tokenizer with a 20k mands labeled as obfuscated
vocabulary size. • obfuscated commands in the sampled telemetry automat-
ically labeled as benign
However, tokenizer quality is only one of the key factors in
• obfuscated samples with rare obfuscation technique
model pre-training. A recent large-scale study on pre-training
Since it is not feasible to manually label and inspect every
language models shows that pre-training dominantly depends
artificially obfuscated command at the scale of the dataset,
on the model size [24]. We observed this effect in our model
there are rare samples where the obfuscation tool fails at
pre-training as well.
obfuscating the command. A common case is when a certain
Figure 8 shows that the small model could achieve only obfuscationtechniquedoesnotapplytotheparticularsampled
limited performance during pre-training. Both the generator command (e.g., no string token in a PowerShell command to
and discriminator learning are halted early in the training obfuscate), as discussed in Appendix A. Such samples were
process. However, the medium and large model sizes were easily identified by the loss peak analysis and removed from
far less limited in this regard. The pre-training metrics for the dataset.
the large model are depicted in Figure 9. The figure shows As for the obfuscated samples in telemetry, they very
thatthelargermodelsizeallowedthegeneratortocontinually rarely come from malware activity. However, some benign
learn throughout the entire learning process. Furthermore, commands excessively use techniques heavily utilized in ob-
the discriminator learning was also not halted and continued fuscation, such as escaping characters, command encoding,
throughout the entire pre-training duration, achieving signifi- etc. These benign commands cause a large portion of the
cantly lower loss function values. training peaks, as they are not that uncommon.
While the medium and large models offer better perfor-
mance with regards to pre-training metrics and could be used
for advanced tasks such as semantic similarity search, we
observed little to no advantage over the pre-trained small
model in the fine-tuning phase.TABLEVII
PARAMETERSPECIFICATIONSFOREACHMODELSIZE.
size model embedding size hidden size num hidden layers intermediate size num attention heads
discriminator 128 256 8 1024 4
large
generator 128 64 8 256 1
discriminator 128 256 4 1024 4
medium
generator 128 64 4 256 1
discriminator 128 256 2 1024 4
small
generator 128 64 2 256 1
discriminator 32 64 2 256 2
miniature
generator 16 32 2 64 1
Fig.11. Testdatasetrecallforindividualobfuscationtechniques.
TABLEVIII characterinsertion,andasaresult,themodelexperienceslarge
PRE-TRAININGHYPER-PARAMETERS lossfunctionpeaksbyconfidentlypredictingthesesamplesas
obfuscated. While the pattern is similar to some obfuscation
Hyper-parameter value
techniques, the model learned to distinguish between these
learningrate 5e-4
weightdecay 0.01 samples and actual obfuscation techniques during the last
batchsize 32 phase of model training described in Section IV-D.
The last category consists of samples from real telemetry
utilizingtechniquesunavailableintheobfuscationtools.Since
An example of such a command excessively using such samples are exceedingly rare in the fine-tuning dataset,
command-line escaping characters is: the classification model tends to focus on the more prevalent
techniques.
C:\Windows\system32\cmd.exe /d /s
The model’s recall for each obfuscation technique in the
/c mocha ˆˆˆ--recursiveˆˆˆ
test dataset is depicted in Figure 11. The figure shows
ˆˆˆ--colorsˆˆˆ ˆˆˆ./test/config.jsˆˆˆ
that the model’s performance on some obfuscation tech-
ˆˆˆ./testˆˆˆ ˆˆˆ--exitˆˆˆ
niques present only in real-world samples, such as “ex-
Removing the commands from the telemetry is not an option, plorer name obfuscation” or “cmd whitespace insertion” is
since the model should not trigger on these samples to keep slightly lower as there are only a few samples for these
high precision of interesting detections. These samples are techniques in the dataset.
structurallyveryclosetosomeobfuscationtechniques,suchas