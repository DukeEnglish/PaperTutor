Language Model Can Listen While Speaking
ZiyangMa1,2 YakunSong1,2 ChenpengDu2 JianCong2 ZhuoChen2
YupingWang2 YuxuanWang2 XieChen1∗
1MoEKeyLabofArtificialIntelligence,X-LANCELab,ShanghaiJiaoTongUniversity
2ByteDanceInc.
Abstract
Dialogueservesasthemostnaturalmannerofhuman-computerinteraction(HCI).
Recentadvancementsinspeechlanguagemodels(SLM),havesignificantlyen-
hanced speech-based conversational AI. However, these models are limited to
turn-basedconversation,lackingtheabilitytointeractwithhumansinreal-time
spokenscenarios,forexample,beinginterruptedwhenthegeneratedcontentisnot
satisfactory. Toaddresstheselimitations,weexplorefullduplexmodeling(FDM)
ininteractivespeechlanguagemodels(iSLM),focusingonenhancingreal-time
interactionand,moreexplicitly,exploringthequintessentialabilityofinterruption.
We introduce a novel model design, namely listening-while-speaking language
model (LSLM), an end-to-end system equipped with both listening and speak-
ingchannels. OurLSLMemploysatoken-baseddecoder-onlyTTSforspeech
generationandastreamingself-supervisedlearning(SSL)encoderforreal-time
audioinput. LSLMfusesbothchannelsforautoregressivegenerationanddetects
turn-taking in real time. Three fusion strategies—early fusion, middle fusion,
andlatefusion—areexplored,withmiddlefusionachievinganoptimalbalance
betweenspeechgenerationandreal-timeinteraction. Twoexperimentalsettings,
command-basedFDMandvoice-basedFDM,demonstrateLSLM’srobustnessto
noiseandsensitivitytodiverseinstructions. OurresultshighlightLSLM’scapa-
bilitytoachieveduplexcommunicationwithminimalimpactonexistingsystems.
Thisstudyaimstoadvancethedevelopmentofinteractivespeechdialoguesystems,
enhancingtheirapplicabilityinreal-worldcontexts2.
IndexTermsFullDuplexModeling,InteractiveSpeechLanguageModel
1 Introduction
Dialogueisthemostnaturalwayofhuman-computerinteraction(HCI).Withtherapiddevelopmentof
GPT-style[29]largelanguagemodels(LLM)andthescalingofTransformer-style[39]architectures,
textualconversationalAI,suchasChatGPT[27,1]andLLaMA[36,37],havebecomeasignificant
partofdailylife. However, thesemodelsarelimitedtotextinputandoutputandcannotinteract
directlywithhumansinarbitraryscenarios.
Incorporating spoken and auditory interfaces into conversational AI enhances HCI convenience.
LeveragingtechniquesfromtextLLMs,thespeechlanguagemodel(SLM)processesspeechsimilarly
to text. This paradigm involves encoding the speech signal into discrete tokens or continuous
embeddings,modelingthemwithalanguagemodel,anddecodingthespeechtokensorembeddings
backtothespeechsignal. Somestudies[19,17,26]utilizesthisparadigmforspeechcontinuation,
generatingexpressivespeechandnaturalmulti-rounddialogue.Otherresearchemploysthisparadigm
∗Correspondingauthor.
2Democanbefoundathttps://ddlbojack.github.io/LSLM
Preprint.
4202
guA
5
]LC.sc[
1v22620.8042:viXratotask-specificapplications,suchasdecoder-onlyhigh-fidelityTTS[40,3,31,13]anddecoder-only
streamingASR[33,38,4,8]Moreover,SpeechGPT[48]andLauraGPT[5]initializeSLMsusing
LLMs,expandingspeechtokenstotheLLMvocabularyandcontinuingtrainingonspeech. This
empowers SLM to comprehend semantic information and equips SLM with dialogue capability.
Despitetheseadvances,allthesemodelsarelimitedtoturn-basedconversationsandcannothandle
real-timesoundorinterruptions,limitingtheirapplicabilityinreal-lifescenarios.
Interactionandturn-takingareessentialabilitiesfornaturalcommunicationamonghumans. Atthe
dawnoftheend-to-endspeechdialoguesystemexplosion,wefocusoninvestigatingFullDuplex
Modeling(FDM)ininteractiveSpeechLanguageModels(iSLM),acrucialtopicaffectinguser
experience. Linet. al[22]proposestoprocessreal-timeaudioinputwithaseparatecomprehension
module. Otherworks[49,41]suggestmodifyingtheorderinwhichtexttokensareorganizedinthe
LLMtotackletheduplexmodelingproblem. Allthesemodelsarebasedontext-centricLLMsthat
requireexternalASRandTTSmodulesforspokendialogue. Asaresult,latencyremainsperceivable
andtheparalinguisticabilityisstilllacking. WebelievetheFDMcapabilityshouldbeanintrinsic
capabilityofSLMs,enablingsimultaneouslisteningandspeaking.
To engage FDM capability for iSLM, we propose Listening-while-Speaking Language Model
(LSLM),anend-to-endmodelwithbothlisteningandspeakingchannels. TheproposedLSLMusesa
token-baseddecoder-onlyTTStomodeltheabilitytospeakandastreamingself-supervisedlearning
(SSL)encodertomodeltheabilitytolisten. LSLMfusesthesetwochannelsanddetectsturn-taking
inrealtime. Weexplorethreestrategiesforfusingduplexsignals: EarlyFusion,MiddleFusion,
andLateFusion. Experimentsdemonstratethatmiddlefusionachievesagoodbalancebetween
speechgenerationandreal-timeinteractioncapabilities.
In addition, interactive dialogue systems for realistic scenarios have two important features: 1)
Listeningchannelsarenotalwaysclean. UsersmayinteractwithiSLMsindifferentscenarios,
containinghigh-frequencynoise(e.g.,telephoneringing)andlow-frequencynoise(e.g.,whitenoise).
2)ItispossiblethattheiSLMinteractswithanunseenspeaker. iSLMsshouldrecognizeand
respondtonewvoicesandinstructions,notdismissthemasnoise. Therefore,iSLMshouldhaveboth
robustnesstonoiseandsensitivitytounseenspeakers. TotestLSLM,wedesignedtwoscenarios:
Command-basedFDM,whereLSLMisinterruptedbyaspecificcommand,andVoice-basedFDM,
whereLSLMcanbeinterruptedbyvariouswordsfromunseenspeakers. Experimentalresultsshow
thatLSLMwithalisteningchannelisrobusttonoisyinputandsensitivetoturning-taking.
Ourcontributionsaresummarizedasfollows:
1. Weformulateanimportanttask,FullDuplexModeling(FDM),appliedintheinteractive
speechlanguagemodel(iSLM).
2. We propose Listening-while-Speaking Language Model (LSLM), an end-to-end single
modelwiththefocusofmodelingtheturn-takingproblem. LSLMcanlistentotheoutside
signalandprovidefeedbackinrealtimewhilespeaking.
3. Weintroducethreemethodsforfusingduplexsignals: EarlyFusion,MiddleFusion,and
LateFusion,withMiddleFusionprovidingtheoptimaltradeoffbetweenspeechgeneration
andreal-timeinteraction.
4. We tested the FDM ability of the proposed LSLM in two scenarios: Command-based
FDMandVoice-basedFDM.ExperimentsindicatethatourproposedLSLMcanachieve
duplexingcapabilitywithlittleimpactontheprevioussystem.
2 RelatedWork
Figure1illustratesthedistinctionsbetweensimplex,halfduplex,andfullduplexspeechlanguage
modelsfromatelecommunicationperspective. AnSLMwithfullduplexmodeling(FDM)capability
canbereferredtoasaninteractivespeechlanguagemodel(iSLM).
2.1 SimplexandHalfDuplexSpeechLanguageModel
SimplexSLMs,depictedinFigure1(A)and1(B),arelimitedtoasinglechannel,eitherforlistening
orspeaking. WiththeassistanceofLLM,simplexSLMsexhibitstrongunderstandingcapabilities.
2Figure1: Illustrationofsimplex,halfduplex,andfullduplexspeechlanguagemodels. (A):Simplex
speechlanguagemodelwithlisteningability. (B):Simplexspeechlanguagemodelwithspeaking
ability. (C):Halfduplexspeechlanguagemodelwithbothlisteningandspeakingabilities. (D):Full
duplexspeechlanguagemodelcanlistenwhilespeaking.
RepresentativeworksincludeLLM-basedASR[46,24,45,32],LLM-basedspeechtranslation[28,
7, 16, 6], and LLM-based speech emotion understanding [44, 21, 20]. Similarly, simplex SLMs
havedemonstratedrobustgenerationcapabilities,asseeninLLM-basedTTS[15,25,18,31]. Some
research leverages the powerful in-context learning capabilities of LLMs to extend task-specific
abilitiestomoreuniversalapplications,suchasspeechunderstanding[11],audiounderstanding[14],
orboth[35,9,10]. Despitetheirgrowingpowerandversatility,simplexSLMsarelimitedtoone-way
communication(eitherhuman→machineormachine→human). LLMshavefacilitatedaparadigm
shiftfromsimplexmodelstohalf-duplexmodels,alsoknownasturn-basedmodels,asshownin
Figure1(C).ProminentmodelsincludeSpeechGPT[48],LauraGPT[5],andVioLA[42]. While
thesehalfduplexmodelscanbothlistenandspeak, theyareconstrainedtoperformingonlyone
actionatthesameinstant,thusfailingtoaddresstheturn-takingproblem.
2.2 FullDuplexSpeechLanguageModel
FullduplexSLMs,asshowninFigure1(D),havethecapabilitytolistenandspeaksimultaneously,
allowing for turn-taking whenever a human interrupts the machine. Recent efforts [49, 41] have
attemptedtobuildfullduplexcapabilitiesontext-centricLLMswithcascadeASRandTTSmodules.
Cutting-edge products like GPT-4o 3 and Moshi 4 exhibit full duplex capability in their spoken
dialoguesystems. Despitetheseadvancements,therearenopubliclyavailableopen-sourcemodels
or detailed analyses of full duplex SLMs. This gap highlights the need for further research and
developmenttofullyunderstandandoptimizefullduplexcapabilityinspeechlanguagemodels.
3 FullDuplexModeling(FDM)
Asimplexorhalfduplexspokendialoguesystemcanbemodeledbyfindingtheparametersθthat
maximizethelog-likelihoodfunction,formulatedas:
(cid:88)
max logP (R|C), (1)
θ
θ
(C,R)∈D
3https://openai.com/index/hello-gpt-4o
4https://moshi.chat
3where(C,R)representsthecontext-responsepairsinthedatasetDandP (R|C)istheprobability
θ
oftheresponseRgiventhecontextC andparametersθ. Morespecifically,ifthespokendialogue
systemismodeledbyanautoregressivelanguagemodelwheretheresponseRisgeneratedtokenby
token,thetraininglossL(θ)foreachsampleisexpressedas:
T
(cid:88)
L(θ)=− logP (r |R ,C), (2)
θ t 1:t−1
t=1
whereR =[r ,r ,...,r ]andT isthesequencelength.Duringtheinferencephase,themodel
1:t−1 1 2 t−1
can only predict the next token autoregressively based on the previous output within the current
channel,withoutinformationfromotherchannels.
Inmodelingafullduplexspokendialoguesystemwithinanautoregressivelanguagemodel,themodel
needstopredictthenexttokenr intheresponseRnotonlybasedonthecontextCandthegenerated
t
responsehistoryR =[r ,r ,...,r ]inthecurrentchannel,butalsobyutilizinginformation
1:t−1 1 2 t−1
S = [s ,s ,...,s ] from another channel simultaneously. Here we extend the modeling
1:t−1 1 2 t−1
approachusedforsimplexorhalfduplexdialoguesystemstoaccommodatetherequirementsoffull
duplexmodeling(FDM).ThetraininglossL(θ)isnowformulatedas:
T
(cid:88)
L(θ)=− logP (r |R ,S ,C) (3)
θ t 1:t−1 1:t−1
t=1
AkeypointinFDMisthatthesequenceS isproducedinrealtimeandunpredictably. Taking
the full duplex speech language model as an example, at the inference step t − 1, the current
speakingchannelgeneratesoutputr andlisteningchannelacquiredinputs arefedintothe
t−1 t−1
modelsimultaneously,influencingthepredictionofthespeakingchannel’snextstepoutputr . This
t
modelingapproachendowsthesystemwithafullduplexability,enablingittoeffectivelyleverage
themulti-channelinformationduringdialogue,therebyimprovingtheaccuracyandfluencyofthe
real-timeinteractioncapability.
4 ProposedLSLM
ThecoredifferencebetweenLSLMandpreviousspeechlanguagemodelsliesinitscapabilityto
simultaneouslyspeakandlisten. WefirstintroducethespeakingcapabilityofLSLM,followedbyits
listeningcapability,andfinally,wediscussvariousfusionmethodsthatintegratethesecapabilities,
endowingLSLMwithfullduplexability.
4.1 SpeakingAbility
TosimulatethespeakingabilityoftheLSLM,weutilizeanautoregressivetoken-basedTTSmodel.
Unlike VALL-E-styled models that combine autoregressive (AR) and non-autoregressive (NAR)
approacheswithmulti-layerresidualvectorquantization(RVQ)tokens,ourmodelemploysasingle
layerofdiscreteaudiotokens. Thisdesignbettermeetstherequirementsforreal-timeinteraction,
as it eliminates the need to wait for the completion of AR token synthesis before performing
NARoperations. GiventargetspeechXR,anSSLencoderEncisutilizedtoobtainacontinuous
embeddingR,whichcanbewrittenas:
R=Enc(XR). (4)
TotrainanautoregressiveTTSmodelbasedondiscretetokens,wequantizethespeechembeddingR,
denotedby:
Rq =Qnt(R), (5)
whereQntisthediscretizationoperationandRq arethediscretetokens. Giventhecontextinforma-
tionC,inthisscenariothetextcontenttobesynthesized,themodelsynthesizesthecorresponding
speechdiscretetokensautoregressively. Weminimizethenegativelog-likelihoodofthetargetse-
quencetotrainthedecoder-onlymodel,conditionedontheprecedingtokensandthecontext. The
lossfunctionisdefinedas:
t (cid:88)EOS
L(θ )=− logP(rq|Rq ,C;θ ), (6)
S t 1:t−1 S
t=1
4Context token Speaking token Listening token
IRQ
LSLM
......
Streaming SSL Encoder
Noise Noise + Voice
Figure2: ProposedLSLM.Themodelcontainsadecoder-onlyTransformertogeneratespeaking
tokensandastreamingSSLencodertoprocesslisteningtokens. Aninterruptiontoken(IRQ)isadded
toallowthemodeltoterminateearlyifaturn-takingoccurs.
whereθ aretheparameterstomodelspeakingability,t representsthetimestepatwhichthe
S EOS
end-of-sequencetokenisreached,rq isthetargetdiscretetokenattimestept,Rq denotesthe
t 1:t−1
sequenceofallprevioustokensuptotimestept−1,andC isthetextcontenttobesynthesized.
Duringinference,themodelsamplesrˆqfromaconditionaldistributionbasedonthealreadygenerated
t
tokensRˆq andthecontextC. Theprocessisdescribedbythefollowingequation:
1:t−1
rˆq ∼P(rq|Rˆq ,C;θ ). (7)
t t 1:t−1 S
AvocoderDecisemployedtorecoverthespeechsignalXˆR fromdiscretetokensRˆq,donatedby:
XˆR =Dec(Rˆq,A), (8)
whereAistheacousticpromptprovidingthetimbreofthesynthesizedspeech. Thisdecoupling
of timbre from content allows the AR model to focus more on semantic information rather than
paralinguisticinformation.
4.2 ListeningAbility
GiventheaudioinputXS ofthelisteningchannel,thesameSSLencoderEncinEquation4isused
toobtainacontinuousembeddingS,whichcanbewrittenas:
S =Enc(XS), (9)
whereXS canbeavarietyofsoundsignals,includingenvironmentalnoiseandhumanspeech.Unlike
trainingthespeakingability,whichinvolvesadiscretizationmodule,thelisteningchannelembedding
S isfedintotheneuralnetworkend-to-endviaaprojectionmoduleProj,whichcanbewrittenas:
Sp =Proj(S), (10)
wherethelistenedaudiosignalismappedtoaspacethatcanbeprocessedbytheARmodel.
4.3 FDMAbility
LSLM has two channels: speaking and listening. At time step t, all previous information of the
speakingchannelRq andtheprocessedinformationofthelisteningchannelSp areconsidered
1:t−1 1:t−1
5bythemodelsimultaneously. HerewereviseEquation6asfollows:
(cid:40)
−(cid:80)tIRQlogP(rq|Rq ,Sp ,C;θ ) ifturn-taking,
L(θ )= t=1 t 1:t−1 1:t−1 LS (11)
LS −(cid:80)tEOSlogP(rq|Rq ,Sp ,C;θ ) otherwise.
t=1 t 1:t−1 1:t−1 LS
whereθ aretheparameterstomodeltheproposedLSLMwithlistening-while-speakingability. In
LS
additiontotheEOStoken,weaddaninterruptiontokenIRQtothetokenizervocabularytoallowthe
modeltoterminateearlyifturn-takingoccurs. Forexample,ifahumaninterrupts,themodelshould
stopspeakingwithinadetectionintervalµsecondsaftertheinterruptionstarts. Duringinference,the
modelsamplesrˆq fromaconditionaldistributionbasedonthealreadygeneratedtokensRˆq ,the
t 1:t−1
contextC,andmostimportant,real-timelistenedaudiotokensSp . Therevisedformulafrom
1:t−1
Equation8iswrittenasfollows:
rˆq ∼P(rq|Rˆq ,Sp ,C;θ ), (12)
t t 1:t−1 1:t−1 LS
inwhich,anessentialrequirementfortheSSLencoderEncisthatitisstreaming. Thus,LSLMcan
obtainreal-timeaudiofeaturesduringinference. ThisisdetailedfurtherinSection5.1.
TocomprehensivelyexploretheintegrationofalisteningchanneltotheproposedLSLM,wetryto
fusethelisteningchannelandthespeakingchannelwithearly,middle,andlatemethods,asshownin
Figure3.
Early Fusion integrates the listening and speaking channels at the input embeddings before
autoregressiveprediction.
MiddleFusion mergesthelisteningandspeakingchannelsateachTransformerblock. Specifically,
inadditiontothehiddenstatesofthespeakingchannelandpositionalembeddings, thelistening
channelisadditionallyaddedtotheinputofeachTransformerblock.
LateFusion combinesthechannelsattheoutputlogitsbeforethesoftmaxoperation.
Listening token Speaking token Hidden feature
Transformer Decoder Layer
Transformer Decoder Layer Transformer Decoder Layer
Early Fusion Middle Fusion Late Fusion
Figure3: DifferentmodeldesignstointegratethelisteningchanneltotheproposedLSLM.
5 Setup
5.1 ModelDetails
ThebackboneoftheproposedLSLMemploysadecoder-onlyTransformerarchitectureconsistingof
12Transformerblocks,12attentionheads,768embeddingdimensions,and3072feed-forwardlayer
dimensions,resultingin106Mparameters. SSLencodervq-wav2vec[2]isemployedtoextractaudio
featuresandfurtherconvertspeechfeaturestodiscretetokens. vq-wav2vec,afullyconvolutional
self-supervised pre-trained model with 20 layers of 1D convolutional neural networks with 34M
parameters,isnaturallysuitableforstreamingaudiofeatureextraction. Asimplelinearlayerserves
astheprojectionmoduletoadaptthelisteningchannelfeaturestotheARmodel. AGAN-based
token-to-waveformvocoder[12]isutilizedtorecoverdiscreteaudiotokenstospeechwaveform.
65.2 DataDetails
WeevaluatetheproposedLSLMundertwofullduplexmodeling(FDM)settings: command-based
FDMandvoice-basedFDM.Table1summarizesthedatasetsandexperimentalsettings. Forthe
TTSdatasets,weutilizetheLibriTTSdataset[47]with585hoursofspeech-textpairsfortraining
andvalidation. LibriTTS-testsetB[12]isadoptedfortesting,whichcontains500utterancessampled
fromthetest-cleansubsetofLibriTTSwith37unseenspeakers. Backgroundnoiseisuniformly
sourcedfromtheFreesoundportionoftheMUSANdataset[34], whichincludeshigh-frequency
noisesuchastelephoneringingandsoundsoftheexplosion,aswellaslow-frequencynoisesuchas
whitenoiseandtrafficnoise. Themodelneedstodistinguishthehumanvoicefromthenoise,soasto
avoidturning-takingwithanyrandominputsignalsandavoidtrivialsolutions. Differentinterruption
dataisconstructedbasedontheFDMsettings.
Command-based FDM. In this setting, LSLM can only be interrupted by specific keywords.
Timbreof22boutiquespeakersfromSEED-TTS[31]isusedtosynthesizethecommand"Honey"
forthecommand-basedFDM.
Voice-basedFDM. Inthissetting,LSLMcanbeinterruptedbyavarietyofdifferentwords. The
SpeechCommandsDataset[47]isasetofone-secondaudio,eachcontainingasinglespokenEnglish
word. Wesplitthedatasetintotraining,validation,andtestsetsinan8 : 1 : 1ratio,resultingin
51,088,6,798,and6,835piecesofdata,respectively. Inaddition,weuseaspeakerindependence
setting,whichguaranteesthatthespeakersinthetestsetdonotappearinthetrainingset,simulating
morechallengingandrealisticscenarios.
Table1: DatadetailsinvolvedintrainingLSLM.SDmeansspeakerdependence,whileSImeans
speakerindependencehere.
Command-basedFDM(SD) Voice-basedFDM(SI)
train LibriTTS-train[47]
TTS val LibriTTS-dev-clean/other[47]
test LibriTTS-testsetB[12]
train SpeechCommandsDataset-train[43]
val Say_Honey SpeechCommandsDataset-dev[43]
Interruption
test SpeechCommandsDataset-test[43]
Noise all FreesoundportionofMUSAN[34]
5.3 TrainingandInferenceDetails
WetrainthemodelwithTTS,interruption,andnoisedatasetsfor20epochs. Foreachsample,noise
isaddedwitha50%probability,andinterruptionwitha50%probability,tothelisteningtokens. Ifa
sampleisselectedtoincludeaninterruption,wemodifythesentencetooutputtheIRQtokenµ=0.5
secondsafterthestartoftheinterruptionandthenstopoutputtingtheremainingspeakingtokens.
Thisensuresthatthemodelcancorrectlyhandledifferentaudiosignalcombinationsinthelistening
channel. TheoptimizationstrategyinvolvesusingAdamW[23]withamaxlearningrateof5×10−4
withoutweightdecayandabatchsizeof4. Thelearningrateschedulerinvolvesawarm-upphasefor
thefirst5,000steps,followedbyacosinedecayofthelearningrate. Validationisperformedatthe
endofeachepoch,andthecheckpointwiththelowestlossisselectedforinference. Thegeneration
processemploysTop-Psamplingwithatop-pvalueof0.99andatemperatureof1.0.
6 Experiments
6.1 EvaluationMetrics
TTScapabilityevaluation. Weevaluatewhetherthespeechgenerationcapabilityisaffectedby
thefullduplexmodelingintheproposedLSLM.Theworderrorrate(WER)comparingthegenerated
7speechtotheoriginaltextisconsideredastheTTScapabilityevaluationmetricsusingWhisperlarge
v35[30].
Interactivecapabilityevaluation. Interactivitycapabilityevaluationaimstomeasurehowwell
theproposedLSLMrespondstoreal-timeandunpredictableinputfromthelisteningchannel. A
successfulturn-takingisdefinedasthemodelstoppingspeakingwithinthe[0,2µ]interval(1second
inoursetting)aftertheinterruptionbegins. Basedonthis, wecategorizetheoutcomesintofour
cases: interruptionandhit(TP),interruptionandmiss(FN),nointerruptionandhit(FP),andno
interruptionandmiss(TN).Fromthesecases, weconstructaconfusionmatrixandcalculatethe
Precision,Recall,andF1score. Thesemetricsconsiderboththesuccessrateofturn-taking(Recall)
and the rate of misjudgments (Precision), providing a comprehensive evaluation of the model’s
interactivitycapabilities.
6.2 Experimentsresults
Weconductaseriesofexperimentstoevaluatethecommand-basedandvoice-basedFDMforboth
TTScapabilityandinteractivecapability. ForTTScapability, weuseatestsetconsistingof500
utterances,referredtoasLibriTTS-testsetB[12],withoutanyinterruptionsinthelisteningchannel.
TheprimarymetricforthisevaluationisWER.Fortheinteractivecapabilityevaluation,weemploya
setof1000utterancesdividedintotwoequalparts: 500utteranceswithinterruptionsatarandom
timestepand500utteranceswithoutinterruptions. InteractivecapabilityismeasuredusingPrecision,
Recall,andF1Score.
Additionally, we test the models under two listening channel conditions: without noise, donated
asClean,andwithnoise,donatedasNoise. ForthebaselineVanillaTTSmodel,sinceitdoesnot
involve a listening channel, the input is inherently clean. By comparing the clean scenarios, we
assesswhethertheintrinsicTTScapabilityisaffected. Additionally,integratingnoisyexternalinputs
providesabettersimulationofreal-worldscenarios.
Command-basedFDM. Forcommand-basedFDM,wetestthethreearchitecturesdescribedin
Section4.3tofusethelisteningchannelandthespeakingchannel,whichareearlyfusion(LSLM ),
EF
middlefusion(LSLM ),andlatefusion(LSLM ). TheresultsareshowninTable2. ForTTS
MF LF
capability,ThebaselineVanillaTTSmodelwithoutalisteningchannelachievesaWERof4.28%.
LSLM outperforms LSLM and LSLM with a WER of 4.05% in clean conditions and
MF EF LF
maintainsarelativelylowWERof4.51%innoisyconditions. TheTTSabilityofLSLM shows
EF
anotabledecrease,likelyduetothefusionofinputembeddings,makingitdifficultforthemodel
todistinguishtheinformationofthelisteningandspeakingchannels,negativelyimpactingthenext
tokenprediction. Forinteractivecapability,allthreearchitecturesperformwellwithanoracleclean
listeningchannel. However,LSLM showsanotabledropinperformanceundernoisyconditions,
LF
withtheF1scorefallingto94.89%. Observingthatthelatefusionmethodappearstomainlyaffect
theprecisionscorewhenthelisteningchannelisnoisy,suggeststhattheLSLM modelreduces
LF
thediscriminationofnoiseandhumanvoice,leadingtomisjudgmentsofinterruptions. Insummary,
themiddlefusionapproachdemonstratessuperiorperformanceinTTScapabilityandcompetitive
performanceininteractivecapability. Therefore,LSLM isconcludedtobethebest-performing
MF
modelamongthosetested.
Voice-based FDM. We utilized a more diverse set of interruption commands compared to the
command-basedFDMandinvolvedunseenspeakersinthetestingprocedures. Thebestconfiguration
from the command-based FDM, the LSLM model, was selected to evaluate the voice-based
MF
FDMcapability. TheresultsareshowninTable3. LSLMshowsahigherWERof5.33%inclean
conditionsand8.50%innoisyconditionscomparedtotheVanillaTTSmodel,demonstratingthe
challengesposedbythereal-worldturn-takingproblem. Comparingtheresultswiththecommand-
basedFDMusingtheLSLM F model,wefindthatthevoice-basedsettingfacesgreaterchallenges
M
inmaintaininghighperformance,especiallyundernoisyconditionswithPrecisionat87.69%,Recall
at82.77%,andanF1scoreof85.15%.Thediversesetofinterruptioncommandsandtheinvolvement
ofunseenspeakersaddcomplexity,resultinginhighererrorrates.
5https://github.com/openai/whisper
8Table 2: Experiments results on command-based FDM. Early fusion (LSLM ), middle fusion
EF
(LSLM ),andlatefusion(LSLM )areconsidered.
MF LF
TTSCapability InteractiveCapability
Model ListeningChannel
WER(%)↓ Precision(%)↑ Recall(%)↑ F1(%)↑
VanillaTTS -(Clean) 4.28 - - -
Clean 33.56 98.00 98.20 98.10
LSLM
EF Noise 34.99 97.20 97.20 97.20
Clean 4.05 97.80 98.19 98.00
LSLM
MF Noise 4.51 97.58 97.18 97.38
Clean 4.37 97.99 97.80 97.89
LSLM
LF Noise 6.87 93.06 96.79 94.89
Table3: Experimentsresultsonvoice-basedFDM.LSLMhereutilizesthearchitectureofmiddle
fusion.
TTSCapability InteractiveCapability
Model ListeningChannel
WER(%)↓ Precision(%)↑ Recall(%)↑ F1(%)↑
VanillaTTS -(Clean) 4.28 - - -
Clean 5.33 95.21 95.78 95.50
LSLM
Noise 8.50 87.69 82.77 85.15
Visualization. To investigate the turn-taking internal mechanism of LSLM, we visualize the
probabilitydistributionofIRQtokensatdifferenttimestepsduringthegenerationprocess. Given
thattheIRQtokenprobabilitydistributionvariessignificantlyinorderofmagnitudeacrossdifferent
timesteps,weutilizealogarithmicscaleforprobabilitytoenhancetheclarityofthevisualization.
AsillustratedinFigure4,theprobabilityoftheIRQtokenremainsbelow1×10−3whenthemodel
isnotinterrupted. Whenthelisteningchannelstartstoreceivethereal-timeturn-takingsignal,LSLM
senseswhetheritisaninterruptionoranoise. Afteraveryshorttime,theIRQtokenprobability
beginstoincrease. Shortlythereafter,thisprobabilityrisestoalevelwheretheIRQtokenissampled
bythemodelduringgeneration.
IRQ Probability
101 Start/End Interruption
Detection Interval
102
103
104
105
106
107
108
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
Time (seconds)
Figure4: IllustrationoftheprobabilitydistributionofIRQtokens(beinginterrupted)overtime. The
logarithmicscaleprobabilityisusedforclearvisualization.
6.3 AblationStudy
Inthissection,weconductanablationstudyonLSLMwithmiddlefusionarchitecturetoevaluatethe
impactofdifferenttrainingmethodsontheperformanceofTTScapabilityandinteractivecapability.
Thetrainingmethodsarecategorizedastrainingfromscratch(✗), loadingthepre-trainedmodel
9
)elacS
goL(
ytilibaborPandfixingtheparameters(✓),andloadingthepre-trainedmodelandcontinuingtraining(✚). The
detailedresultsarepresentedinTable4.
ThevanillaTTSmodel,trainedfromscratch,achievesaWERof4.28%concerningTTScapability.
Fortheinteractivecapability, thevanillaTTSmodeldoesnothavealisteningchannel, henceno
metricsareavailable. FortheLSLMmodel,thebestperformanceisobservedwhenboththeTTS
backboneandstreamingSSLencoderareloadedandcontinuetraining(✚ &✚),achievingthelowest
WERof4.05%andhighestPrecisionof97.80%,Recallof98.19%,andF1Scoreof98.00%. Some
conclusionscanalsobedrawnfromtheseexperiments. Forexample,theSSLencoderofthelistening
channelperformsbetterwhenitcanbecontinuedtrainingthanfixedtheparameters. Onepotential
reason is that the SSL encoder has not encountered diverse noise during pre-training, creating a
bottleneck for extracting audio with mixed human voice and noise when using fixed pre-trained
parameters.
Table4: AblationstudyonLSLMtoevaluatetheimpactofdifferenttrainingmethods. ✗means
trainingfromscratch,✓meansloadthepre-trainingmodelandfixtheparameters,✚ meansloadthe
pre-trainingmodelandcontinuetraining. LSLMhereutilizesthearchitectureofmiddlefusion.
TrainingMethod TTSCapability InteractiveCapability
Model
Speaking Listening WER(%)↓ Precision(%)↑ Recall(%)↑ F1(%)↑
VanillaTTS ✗ - 4.28 - - -
✗ ✓ 4.82 97.80 97.99 97.89
✗ ✚ 4.67 95.60 95.98 95.79
LSLM ✓ ✓ 6.64 97.89 83.60 90.18
✓ ✚ 4.64 97.60 98.18 97.89
✚ ✓ 4.46 96.43 92.54 94.44
✚ ✚ 4.05 97.80 98.19 98.00
7 Conclusion
Inthispaper,weaddressthechallengesofenhancingreal-timeinteractionbyintroducingfullduplex
modeling(FDM)ininteractivespeechlanguagemodels(iSLM).Weintroducelisten-while-speaking
languagemodel(LSLM),aninnovativeend-to-endmodeldesignedtohandlereal-timeturn-taking.
LSLMintegratesatoken-baseddecoder-onlyTTSmodelforspeechgenerationandastreamingSSL
encoderforaudioinput,enablingsimultaneouslisteningandspeaking. Weproposethreestrategies
forfusingduplexsignals: earlyfusion,middlefusion,andlatefusion. Amongthese,MiddleFusion
demonstratesasuperiorbalancebetweenspeechgenerationandreal-timeinteractioncapabilities.
TheproposedLSLMisevaluatedintwosettings: command-basedFDMandvoice-basedFDM.Our
experimentsshowthatLSLMisrobusttonoisyenvironmentsandresponsivetodiverseinstructions
fromunseenspeakers,achievingeffectiveduplexcommunicationwithminimalimpactonsystem
performance. Ourworkisaninitialexplorationintofullduplexinteractivespeechlanguagemodels,
andthereisstillalongwaytogotoachievesmoothhuman-computerspeechinteraction. Thereis
alottoexploreinthefuture,suchasdevelopingspeech-inspeech-outdialoguesystemswithfull
duplexmodelingability,incorporatingspeaker-followingcapabilitytoidentifyinterruptingspeakers,
andexploringaudiovisualco-guidanceforimprovedturn-taking.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. GPT-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] AlexeiBaevski,SteffenSchneider,andMichaelAuli. vq-wav2vec: Self-supervisedlearningof
discretespeechrepresentations. InProc.ICLR,2020.
[3] ZalánBorsos,RaphaëlMarinier,DamienVincent,EugeneKharitonov,OlivierPietquin,Matt
Sharifi,DominikRoblek,OlivierTeboul,DavidGrangier,MarcoTagliasacchi,etal. AudioLM:
alanguagemodelingapproachtoaudiogeneration. Proc.TASLP,2023.
10[4] PeikunChen,SiningSun,ChanghaoShan,QingYang,andLeiXie. Streamingdecoder-only
automaticspeechrecognitionwithdiscretespeechunits: Apilotstudy. Proc.Interspeech,2024.
[5] QianChen,YunfeiChu,ZhifuGao,ZeruiLi,KaiHu,XiaohuanZhou,JinXu,ZiyangMa,Wen
Wang,SiqiZheng,etal. LauraGPT:Listen,attend,understand,andregenerateaudiowithgpt.
arXivpreprintarXiv:2310.04673,2023.
[6] XiChen,SongyangZhang,QibingBai,KaiChen,andSatoshiNakamura. LLaST:Improved
end-to-end speech translation system leveraged by large language models. arXiv preprint
arXiv:2407.15415,2024.
[7] ZhehuaiChen,HeHuang,AndreiAndrusenko,OleksiiHrinchuk,KrishnaCPuvvada,JasonLi,
SubhankarGhosh,JagadeeshBalam,andBorisGinsburg. SALM:Speech-augmentedlanguage
modelwithin-contextlearningforspeechrecognitionandtranslation. InProc.ICASSP,2024.
[8] ZhehuaiChen,HeHuang,OleksiiHrinchuk,KrishnaCPuvvada,NithinRaoKoluguri,Piotr
Z˙elasko,JagadeeshBalam,andBorisGinsburg. BESTOW:Efficientandstreamablespeech
languagemodelwiththebestoftwoworldsingptandt5. arXivpreprintarXiv:2406.19954,
2024.
[9] YunfeiChu,JinXu,XiaohuanZhou,QianYang,ShiliangZhang,ZhijieYan,ChangZhou,and
JingrenZhou. Qwen-audio: Advancinguniversalaudiounderstandingviaunifiedlarge-scale
audio-languagemodels. arXivpreprintarXiv:2311.07919,2023.
[10] YunfeiChu,JinXu,QianYang,HaojieWei,XipinWei,ZhifangGuo,YichongLeng,Yuan-
jun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint
arXiv:2407.10759,2024.
[11] KeqiDeng,GuangzhiSun,andPhilipCWoodland. Wav2prompt: End-to-endspeechprompt
generationandtuningforllminzeroandfew-shotlearning. arXivpreprintarXiv:2406.00522,
2024.
[12] Chenpeng Du, Yiwei Guo, Feiyu Shen, Zhijun Liu, Zheng Liang, Xie Chen, Shuai Wang,
HuiZhang,andKaiYu. UniCATS:Aunifiedcontext-awaretext-to-speechframeworkwith
contextualvq-diffusionandvocoding. InProc.AAAI,2024.
[13] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi
Zheng,YueGu,ZiyangMa,etal. CosyVoice: Ascalablemultilingualzero-shottext-to-speech
synthesizerbasedonsupervisedsemantictokens. arXivpreprintarXiv:2407.05407,2024.
[14] YuanGong,HongyinLuo,AlexanderHLiu,LeonidKarlinsky,andJamesGlass. Listen,think,
andunderstand. Proc.ICLR,2024.
[15] Hongkun Hao, Long Zhou, Shujie Liu, Jinyu Li, Shujie Hu, Rui Wang, and Furu Wei.
Boosting large language model for speech synthesis: An empirical study. arXiv preprint
arXiv:2401.00246,2023.
[16] Chao-WeiHuang,HuiLu,HongyuGong,HirofumiInaguma,IliaKulikov,RuslanMavlyu-
tov,andSravyaPopuri. Investigatingdecoder-onlylargelanguagemodelsforspeech-to-text
translation. Proc.Interspeech,2024.
[17] Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-
AnhNguyen,MorganeRivière,AbdelrahmanMohamed,EmmanuelDupoux,etal. Text-free
prosody-awaregenerativespokenlanguagemodeling. InProc.ACL,2022.
[18] MateuszŁajszczak,GuillermoCámbara,YangLi,FatihBeyhan,ArentvanKorlaar,FanYang,
Arnaud Joly, Álvaro Martín-Cortinas, Ammar Abbas, Adam Michalski, et al. BASE TTS:
Lessonsfrombuildingabillion-parametertext-to-speechmodelon100khoursofdata. arXiv
preprintarXiv:2402.08093,2024.
[19] KushalLakhotia,EugeneKharitonov,Wei-NingHsu,YossiAdi,AdamPolyak,BenjaminBolte,
TuAnhNguyen,JadeCopet,AlexeiBaevski,AbdelrahmanMohamed,etal. Ongenerative
spokenlanguagemodelingfromrawaudio. Proc.TACL,2021.
11[20] Zheng Lian, Haiyang Sun, Licai Sun, Jiangyan Yi, Bin Liu, and Jianhua Tao. AffectGPT:
Dataset and framework for explainable multimodal emotion recognition. arXiv preprint
arXiv:2407.07653,2024.
[21] Guan-TingLin,Cheng-HanChiang,andHung-yiLee. Advancinglargelanguagemodelsto
capturevariedspeakingstylesandrespondproperlyinspokenconversations. Proc.ACL,2024.
[22] Ting-EnLin,YuchuanWu,FeiHuang,LuoSi,JianSun,andYongbinLi. Duplexconversation:
Towardshuman-likeinteractioninspokendialoguesystems. InProc.SIGKDD,2022.
[23] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InProc.ICLR,2019.
[24] ZiyangMa,GuanrouYang,YifanYang,ZhifuGao,JiamingWang,ZhihaoDu,FanYu,Qian
Chen,SiqiZheng,ShiliangZhang,etal. Anembarrassinglysimpleapproachforllmwithstrong
asrcapacity. arXivpreprintarXiv:2402.08846,2024.
[25] PaarthNeekhara,ShehzeenHussain,SubhankarGhosh,JasonLi,RafaelValle,RohanBadlani,
andBorisGinsburg. Improvingrobustnessofllm-basedspeechsynthesisbylearningmonotonic
alignment. Proc.Interspeech,2024.
[26] TuAnhNguyen,EugeneKharitonov,JadeCopet,YossiAdi,Wei-NingHsu,AliElkahky,Paden
Tomasello,RobinAlgayres,BenoitSagot,AbdelrahmanMohamed,etal. Generativespoken
dialoguelanguagemodeling. Proc.TACL,2023.
[27] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelsto
followinstructionswithhumanfeedback. Proc.Neurips,2022.
[28] Jing Pan, Jian Wu, Yashesh Gaur, Sunit Sivasankaran, Zhuo Chen, Shujie Liu, and Jinyu
Li. Cosmic: Dataefficientinstruction-tuningforspeechin-contextlearning. arXivpreprint
arXiv:2311.02248,2023.
[29] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training. 2018.
[30] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya
Sutskever. Robustspeechrecognitionvialarge-scaleweaksupervision. InProc.ICML,2023.
[31] SeedSpeechTeam. Seed-TTS: A family of high-quality versatile speech generation models.
arXivpreprintarXiv:2406.02430,2024.
[32] SeedSpeechTeam. Seed-ASR: Understanding diverse speech and contexts with llm-based
speechrecognition. arXivpreprintarXiv:2407.04675,2024.
[33] FrankSeide,MorrieDoulaty,YangyangShi,YasheshGaur,JuntengJia,andChunyangWu.
SpeechReaLLM–real-timestreamingspeechrecognitionwithmultimodalLLMsbyteaching
theflowoftime. arXivpreprintarXiv:2406.09569,2024.
[34] DavidSnyder,GuoguoChen,andDanielPovey. MUSAN:Amusic,speech,andnoisecorpus.
arXivpreprintarXiv:1510.08484,2015.
[35] ChangliTang,WenyiYu,GuangzhiSun,XianzhaoChen,TianTan,WeiLi,LuLu,ZejunMa,
andChaoZhang. SALMONN:Towardsgenerichearingabilitiesforlargelanguagemodels. In
Proc.ICLR,2024.
[36] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[37] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
12[38] Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, and Shinji Watanabe.
Decoder-only architecture for streaming end-to-end speech recognition. Proc. Interspeech,
2024.
[39] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Proc.Neurips,2017.
[40] ChengyiWang,SanyuanChen,YuWu,ZiqiangZhang,LongZhou,ShujieLiu,ZhuoChen,
YanqingLiu,HuamingWang,JinyuLi,etal. Neuralcodeclanguagemodelsarezero-shottext
tospeechsynthesizers. arXivpreprintarXiv:2301.02111,2023.
[41] PengWang,SongshuoLu,YaohuaTang,SijieYan,YuanjunXiong,andWeiXia. Afull-duplex
speechdialogueschemebasedonlargelanguagemodels. arXivpreprintarXiv:2405.19487,
2024.
[42] TianruiWang,LongZhou,ZiqiangZhang,YuWu,ShujieLiu,YasheshGaur,ZhuoChen,Jinyu
Li,andFuruWei. VioLA:Unifiedcodeclanguagemodelsforspeechrecognition,synthesis,
andtranslation. arXivpreprintarXiv:2305.16107,2023.
[43] PeteWarden. Speechcommands: Apublicdatasetforsingle-wordspeechrecognition. Dataset
availablefromhttp://download.tensorflow.org/data/speech_commands_v0,2017.
[44] Yaoxun Xu, Hangting Chen, Jianwei Yu, Qiaochu Huang, Zhiyong Wu, Shi-Xiong Zhang,
GuangzhiLi,YiLuo,andRongzhiGu. Secap: Speechemotioncaptioningwithlargelanguage
model. InProc.AAAI,2024.
[45] Guanrou Yang, Ziyang Ma, Fan Yu, Zhifu Gao, Shiliang Zhang, and Xie Chen. Mala-asr:
Multimedia-assistedllm-basedasr. Proc.Interspeech,2024.
[46] WenyiYu,ChangliTang,GuangzhiSun,XianzhaoChen,TianTan,WeiLi,LuLu,ZejunMa,
and ChaoZhang. Connectingspeech encoder andlargelanguage modelfor ASR. In Proc.
ICASSP,2024.
[47] HeigaZen,VietDang,RobClark,YuZhang,RonJWeiss,YeJia,ZhifengChen,andYonghui
Wu. LibriTTS:Acorpusderivedfromlibrispeechfortext-to-speech. Proc.Interspeech,2019.
[48] DongZhang,ShiminLi,XinZhang,JunZhan,PengyuWang,YaqianZhou,andXipengQiu.
SpeechGPT: Empowering large language models with intrinsic cross-modal conversational
abilities. InProc.EMNLP,2023.
[49] XinrongZhang,YingfaChen,ShengdingHu,XuHan,ZihangXu,YuanweiXu,WeilinZhao,
MaosongSun,andZhiyuanLiu. Beyondtheturn-basedgame:Enablingreal-timeconversations
withduplexmodels. arXivpreprintarXiv:2406.15718,2024.
13