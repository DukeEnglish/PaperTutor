QUASI-BAYES MEETS VINES
David Huk Yuanhe Zhang
Department of Statistics Department of Statistics
University of Warwick University of Warwick
David.Huk@warwick.ac.uk Yuanhe.Zhang@warwick.ac.uk
Mark Steel Ritabrata Dutta
Department of Statistics Department of Statistics
University of Warwick University of Warwick
m.steel@warwick.ac.uk Ritabrata.Dutta@warwick.ac.uk
ABSTRACT
Recently proposed quasi-Bayesian (QB) methods [Fong et al., 2023] initiated
a new era in Bayesian computation by directly constructing the Bayesian
predictive distribution through recursion, removing the need for expensive
computations involved in sampling the Bayesian posterior distribution. This
has proved to be data-efficient for univariate predictions, but extensions to
multiple dimensions rely on a conditional decomposition resulting from pre-
defined assumptions on the kernel of the Dirichlet Process Mixture Model,
which is the implicit nonparametric model used. Here, we propose a different
way to extend Quasi-Bayesian prediction to high dimensions through the
use of Sklar’s theorem by decomposing the predictive distribution into one-
dimensional predictive marginals and a high-dimensional copula. Thus, we
use the efficient recursive QB construction for the one-dimensional marginals
and model the dependence using highly expressive vine copulas. Further, we
tune hyperparameters using robust divergences (eg. energy score) and show
that our proposed Quasi-Bayesian Vine (QB-Vine) is a fully non-parametric
density estimator with an analytical form and convergence rate independent
ofthedimensionofdatainsomesituations. Ourexperimentsillustratethatthe
QB-Vine is appropriate for high dimensional distributions (∼64), needs very
few samples to train (∼200) and outperforms state-of-the-art methods with
analytical forms for density estimation and supervised tasks by a considerable
margin.
4202
nuJ
81
]LM.tats[
1v46721.6042:viXra1 Introduction
The estimation of joint densities p(x) is a cornerstone of machine learning as a looking
glass into the underlying data-generating process of multivariate data x. Methods that
support explicit density evaluation are crucial in probabilistic modelling, with applications
in variational methods [Kingma and Welling, 2013, Rezende et al., 2014], Importance
Sampling [Arbel et al., 2021, Matthews et al., 2022], Sequential Monte Carlo [Gu et al.,
2015], Markov Chain Monte Carlo (MCMC) [Song et al., 2017, Hoffman et al., 2019]
and simulation-based inference [Papamakarios et al., 2019, Lueckmann et al., 2021]. A
prominent example are Normalising Flows (NF) Papamakarios et al. [2017], Durkan et al.
[2019],Papamakariosetal.[2021],leveragingdeepnetworkswithinvertibletransformations
for analytical expressions and sampling. Despite impressive performances, they require
meticulous manual hyperparameter tuning and large amounts of data to train. Bayesian
methods are another attractive approach for analytical density modelling where the central
object of interest is the predictive density p(n)(x), with the Dirichlet Process Mixture Model
(DPMM) Hjort et al. [2010] as the canonical nonparametric choice. However, obtaining
analytical predictive densities commonly relies on computationally expensive MCMC
methods scaling poorly to high-dimensions1.
Recently, Fong et al. [2023] heralded a revolution in Bayesian methods with efficient
constructions of Quasi-Bayesian (QB) sequences of predictive densities through recursion
alone, liberating Bayes from the reliance on MCMC. The seminal univariate Recursive
Bayesian Predictive (R-BP) [Hahnet al., 2018], its multivariate extension[Fong et al., 2023]
andtheAutoRegressiveBayesianPredictive(AR-BP)[Ghalebikesabietal.,2022]areallQB
modelstargetingthepredictivemeanoftheDPMM,withananalyticalrecursiveformdriven
by bivariate copula updates. Notably, the AR-BP demonstrated superior density estimation
capabilities compared to a suite of competitors on varied supervised and unsupervised
datasets, is orders of magnitude faster than standard Bayesian methods and is data-efficient,
i.e. does not require large amounts of training data to be effective. In multivariate QB
predictives, analytical expressions are derived by enforcing assumptions on the DPMM
kernel structure. The kernel of the R-BP is set to be independent across dimensions, where
this strong assumption is too constraining for more complex data. This is relaxed in the
AR-BP through an autoregressive form of the kernel, where each kernel mean is a similarity
function of previous dimensions modelled through kernels or deep autoregressive networks,
depending on the complexity of the data. The former relies on a fixed form that is often too
simplistic while the latter loses the appeal of a data-efficient predictive like in the R-BP.
In this paper, we introduce the Quasi-Bayesian Vine (QB-Vine) for density estimation and
supervised learning, obtained by applying Sklar’s theorem [Sklar, 1959] to the joint predic-
tive, thereby dissecting it into univariate marginal predictive densities and a multivariate
copula. Marginal predictives are modelled with the data-efficient univariate R-BP while the
1Forreference,withdthedimensionalityoftheparameterspace,RandomWalkMetropolis-HastingsscaleslikeO(d2)[Roberts
etal.,1997],Metropolis-adjustedLangevinalgorithmlikeO(d5/4)[RobertsandRosenthal,1998],andHamiltonianMonteCarlo
likeO(d4/3)[Beskosetal.,2013]
2Marginal predictive recursion Vine copula model
2
1 3
1,2 2,3
Figure 1: Overview of the Quasi-Bayesian Vine model. The joint predictive density
p(n)(x ,...,x ) is modelled nonparametrically through a copula decomposition into uni-
1 d
variate marginal predictives and a multivariate copula. Left: Univariate predictive densities
(n)
p are modelled with a data-efficient Quasi-Bayesian recursion using observed samples
i
{x(k) }n alone, bypassing posterior integration steps. Right: A vine copula model decom-
i k=1
d(d−1)
poses the high-dimensional copula into two-dimensional copulas nonparametrically
2
estimatedwithkerneldensitycopulaestimatorsrepresentedbyagraphicalstructurebetween
dimensions
multivariate copula is modelled with a highly flexible vine copula, see the diagram in Figure
1. The main contributions of our work are as follows:
• This decomposition frees us from the need to assume specific kernel structures of
the DPMM, but preserves the data-efficiency of QB methods while making it more
effective on high-dimensional data.
• Under the assumption of well-identified simplified vine copula model, we show that
the QB-Vine attains a convergence rate that is independent of dimension.
• The decomposition of the joint density and use of energy score to tune hyperpa-
rameters makes our algorithm amenable to efficient parallelisation with significant
computational gains.
Our paper is structured as follows. In Section 2 we introduce Quasi-Bayesian prediction,
recapitulating the R-BP construction. In Section 3 we formulate the Quasi-Bayesian Vine
model. We provide a succinct survey of related work in Section 4 and compare related
methodstotheQB-VineinSection5onarangeofdatasetsfordensityestimation,regression
and classification, achieving state-of-the-art performance with our model. We conclude with
a discussion in Section 6.
32 Quasi-Bayesian prediction
Notation. Let p(x) be a multivariate probability density function over X ⊆ Rd, from which
we observe i.i.d. samples D = {xk}K ∼ p(x). Similarly, let p (x ),...,p (x ) be the
P k=1 1 1 d d
marginal densities of p(x), each over (a subset of) R with corresponding i.i.d. samples
D = {x k}K ∼ p (x ) for i = 1,...,d, and assumed to all be continuous. Further, let P
P i i k=1 i i
and P ,...,P be the respective cumulative distribution functions (cdf’s) of the previously
1 d
mentioned densities, in order of appearance. Finally, when discussing predictive densities,
we will use a superscript, plain for data (e.g. xn) and in parentheses for functions (e.g. p(n))
to indicate the predictive at the nth step, distinguishing them from the subscripts kept for
dimension.
Bayesian predictive densities as copula updates. Consider the univariate Bayesian pre-
dictive density p(n) for a future observation x given seen i.i.d. data x1:n ∈ R with a likelihood
f of the data and a posterior π(n) for the model parameters θ after n observations. By Bayes
rule, we have:
(cid:82)
f(x|θ)·f(xn|θ)·π(n−1)(θ|x1:n−1) dθ
p(n)(x|x1:n) = . (1)
p(n−1)(xn|x1:n−1)
As identified by Hahn et al. [2018], Fong et al. [2023], Holmes and Walker [2023], multiply-
ing and dividing by the predictive from the previous step p(n−1), we arrive at
Jointdensityforx,xn
(cid:122)(cid:90) (cid:125)(cid:124) (cid:123)
f(x|θ)·f(xn|θ)·π(n−1)(θ|x1:n−1) dθ
p(n)(x|x1:n) = p(n−1)(x|x1:n−1)·
p(n−1)(xn|x1:n−1)·p(n−1)(x|x1:n−1) (2)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Marginalforxn Marginalforx
(cid:16) (cid:17)
= p(n−1)(x|x1:n−1)·c(n) P(n−1)(x),P(n−1)(xn)
which identifies the involvement of copulas (see Appendix A, Elidan [2013], Größer and
Okhrin [2022]) in this recursive equation for the predictive densities. The second term on
the right-hand side of (2) is seen to be a symmetric bivariate copula density function with
the property that c(n)(·,·) → 1 to ensure p(n) converges asymptotically with n. It can be
shown that every univariate Bayesian model can be written in this form Hahn et al. [2018],
and so has a unique copula sequence characterising its predictive updates by de Finetti’s
theorem [De Finetti, 1937, Hewitt and Savage, 1955]. The choice of the predictive sequence
then corresponds to an implicit choice of likelihood and prior [Berti et al., 2023, Garelli
et al., 2024].
Recursive Bayesian Predictives. Due to the integration over the posterior density often
being intractable in practice, identifying the update copulas c(n) analytically is generally
impossible. Therefore, Hahn et al. [2018] propose a nonparametric density estimator
termedrecursiveBayesianpredictive(R-BP)asaDirichletProcessMixtureModel(DPMM)
inspired recursion emulating (2). They derive the correct Bayesian update under a DPMM
4for step 1 and use it for all future steps m > 1 (derivations shown in Appendix B.2). The
update copula of the R-BP is a mixture between the independent and Gaussian copula,
thereby deviating from the true (unknown) form of the Bayesian recursion copulas for a
DPMM. For an initial choice of predictive density p(0) and distribution P(0), the obtained
analytical expression for the R-BP recursion has the following predictive density
(cid:104) (cid:105)
p(n)(x) = p(n−1)(x)· (1−α )+α ·c (P(n−1)(x),P(n−1)(xn)) (3)
n n ρ
with c ∈ [0,1] being a bivariate Gaussian copula with covariance ρ. The corresponding cdf
ρ
also admits an analytical expression as follows
P(n)(x) = (1−α )·P(n−1)(x)+α H (P(n−1)(x),P(n−1)(xn)). (4)
n n ρ
Here, for Φ the standard univariate Gaussian distribution,
(cid:32) (cid:33)
Φ−1(u)−ρ·Φ−1(v)
H (u,v) = Φ (5)
ρ (cid:112)
1−ρ2
is a conditional Gaussian copula distribution with covariance ρ treated as a hyperparameter
and where α = (2 − 1) 1 is a sequence of weights converging to 0 (See supplement
k k k+1
E of Fong et al. [2023] for a more detailed explanation of the weights). This univariate
R-BP model has been shown to converge to a limiting distribution P(∞) with density p(∞)
termed the martingale posterior [Fong et al., 2023, Fortini and Petrone, 2023, Holmes
and Walker, 2023, Berti et al., 2006, Garelli et al., 2024] which is a quasi-Bayesian object
describing current uncertainty (See Appendix B for an introduction). Frequentist results
such as Kullback-Leibler convergence to the true density are given in Hahn et al. [2018].
An extension to the multivariate case was studied by Fong et al. [2023] and refined by
Ghalebikesabietal.[2022]fordatawithmorecomplexdependencestructures. Therecursive
formulation of the R-BP with an analytical form ensures fast updates of predictives whilst
the use of copulas bypasses the need to evaluate a normalising constant (as is the case in
Newton’s algorithm [Newton et al., 1998, Newton, 2002]). Consequently, this approach is
free from the reliance on MCMC to approximate a posterior density, making it much faster
than regular DPMMs. While this formulation does not correspond to a Bayesian model, as
argued by Berti et al. [2004, 2023], Fong et al. [2023], Holmes and Walker [2023], if the
recursive updates are conditionally identically distributed (as is the case for the recursion
of (4)), they still exhibit desirable Bayesian characteristics such as coherence, regularization,
and asymptotic exchangeability, motivating the Quasi-Bayesian name as used in Fortini and
Petrone [2020].
3 Quasi-Bayesian Vine prediction
WeproposetheQuasi-BayesianVine(QB-Vine)forefficientlymodelingahigh-dimensional
predictive density p(n) (and distribution P(n)). The efficiency is achieved by splitting the
joint density into predictive marginals for each dimension and a high-dimensional copula;
then using two different nonparametric density estimation schemes for the marginals and
the vine, correspondingly the Quasi-Bayes recursion (described in Equation 3 and 4) for
5each of the marginals and a vine copula to model the high-dimensional dependency. We first
start by describing how we decompose the joint predictive density adapting Sklar’s theorem
(Sklar [1959], see Appendix A),
Theorem 3.1. Let P(n) be an d-dimensional predictive distribution function with continuous
marginal distributions P(n) ,P(n) ,...,P(n) . Then there exists a copula distribution C(n) such
1 2 d
that for all x = (x ,x ,...,x ) ∈ Rd:
1 2 d
P(n)(x ,...,x ) = C(n)(P(n) (x ),...,P(n) (x )) (6)
1 d 1 1 d d
And if a probability density function (pdf) is available:
p(n)(x ,...,x ) = p(n) (x )·...·p(n) (x )·c(n)(P(n) (x ),...,P(n) (x )) (7)
1 d 1 1 d d 1 1 d d
(n) (n)
where p (x ),...,p (x ) are the marginal predictive probability density functions (pdf),
1 1 d d
and c(n) : [0,1]d → R is the copula pdf.
The decomposition of (7) has two consequences for modelling p(n). Firstly, as the predic-
(n)
tives p are unconditional marginal densities, their estimation can be done independently.
i
Secondly,byapplying(7)totwoconsecutivepredictivedensitiesp(m−1) andp(m),weobtain
a recursive update for predictive densities with two parts, of the form:
(cid:16) (cid:17)
p(m) (cid:89)d (cid:110) p(m) (cid:111) c(m) P 1(m) (x 1),...,P d(m) (x d)
= i · . (8)
(cid:16) (cid:17)
p(m−1) p(m−1) c(m−1) P(m−1) (x ),...,P(m−1) (x )
i=1 i 1 1 d d
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
Independentrecursions
Implicitrecursiononcopulas
This decomposition fruitfully isolates updates to marginal predictive densities from updates
to their dependence structure, allowing us to model each recursion separately; the marginal
predictives follow a univariate recursion a la (2) while the copulas are free to follow another
recursive form. Further, as we are only interested in the joint predictive p(n), once marginal
predictives are obtained, we only need to fit a single copula c(n) at the final iteration n to
recover the joint predictive through (7). Crucially, we do not need an analytical form for
copula updates, leaving them as an implicit recursion on copula densities. Motivated by this
insight, we formulate the Quasi-Bayesian Vine as an estimation problem with two parts, the
recursive estimation of marginal predictive densities and the estimation of a vine copula to
model the high-dimensional dependency.
Marginal predictive density estimation We model marginal predictive densities p (n)(x )
i i
and distributions P (n)(x ), ∀i = 1,...,d independently of each other. We use the univariate
i i
R-BP approach described in Section 2 to recursively obtain the analytical expression for
(0)
both. For each dimension separately, starting with an initial density p and distribution
i
(0)
P , we follow the updates (3) for the density and (4) for the distribution.
i
Simplified vine copulas for high-dimensional dependence After estimating marginal
(cid:16) (cid:17)
predictives, we model the joint density of u := P (n)(x ),...,u := P (n)(x ) with a
1 1 1 d d d
6multivariate copula. We consider a highly flexible vine copula which decomposes the
d(d−1)
joint copula density c(u ,...,u ) into bivariate conditional copulas to capture the
1 d 2
dependence structure.
Vine copulas are a class of copulas that provide a divide-and-conquer approach to high-
d(d−1)
dimensional modelling by decomposing the joint copula density into bivariate copula
2
terms. The main ingredient of a vine copula decomposition is the following identity as a
consequence of (7):
p (x |x ) = c (P (x ),P (x ))·p (x ) (9)
a|b a b a,b a a b b a a
where a,b are subsets of dimensions from {1,...,d}. Vine copulas rely on a conditional
(cid:81)d
factorisation p(x ,...,x ) = p (x |x ) to which they repeatedly apply (9), rewrit-
1 d i=1 i|<i i <i
ing any conditional densities as copulas, thereby splitting the joint density into the d
d(d−1)
marginaldensitiesand bivariatecopulascalledpaircopulas. Thepaircopulasforeach
2
i ̸= j ∈ {,...,d}, take as input pairs of conditional distributions (P (x ),P (x ))
i|S i|S j|S j|S
ij ij ij ij
where S ⊆ {1,...,d}\{i,j}∪∅ is decided by the choice of the vine. A vine copula model
ij
thus has the form
d(d−1)/2
(cid:89)
c(u ,...,u ) = c (P (x ),P (x )|S ). (10)
1 d ij i|S i|S j|S j|S ij
ij ij ij ij
i̸=j
We note that these pair copulas start as unconditional bivariate copulas and later capture
higher orders of multivariate dependence by conditioning on the set S itself. This decompo-
sition is valid but only unique up to the permutation of indexes. We provide an example of a
three-dimensional vine copula decomposition and an overview in Appendix A.3, referring
thereadertoCzado[2019],CzadoandNagler[2022]foranintroduction. Inpractice,weuse
asimplifiedvinecopulamodel[NaglerandCzado,2016,Nagleretal.,2017]whichremoves
the conditional dependence of each of the copula c on S . This is an approximation
ij ij
reducing the complexity of the model for dependency structure,
but provides a significant computation efficiency by reducing the size of tree space and
Nagler and Czado [2016] provides a dimension-independent convergence rate when the
simplified vine assumption is true, making simplified vine copulas greatly appealing for
high-dimensional models. Under this simplified vine copula model assumption, we use
nonparametric Kernel Density Estimator (KDE) for each of the bivariate unconditional pair-
copulas2. Then c becomes a two-dimensional KDE copula with the following expression:
ij
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:80)K
ϕ Φ−1(u)−Φ−1(u );0,b ·ϕ Φ−1(v)−Φ−1(v );0,b
k=1 i,k j,k
c (u,v) = (11)
ij (cid:16) (cid:17) (cid:16) (cid:17)
ϕ Φ−1(u);0,b ·ϕ Φ−1(v);0,b
where ϕ(.;0,b) is the pdf of a normal with mean 0 and variance b, Φ−1 is the inverse standard
normal cdf. Samples {(u ,v )}K are easily obtained by iteratively fitting KDE pair
i,k j,k k=1
copulas on observed samples {(u ,...,u )}K [Czado, 2019].
1,k d,k k=1
2ForfurtherdetailonKDE-basedvinecopulas,seeAppendicesA.2,A.3andNaglerandCzado[2016],Nagleretal.[2017].
7Choice of Hyperparameters. We begin by choosing a Cauchy distribution as the initial
(0) (0)
predictive distribution P and the corresponding density p ∀i = 1,...,d, with details
i i
provided in Appendix E. The hyperparameter ρ for the R-BP recursion for each marginal
predictive is chosen in an automatic data-dependent manner, by minimising the energy score
[Pacchiardi et al., 2024] between observations and samples from our model conditional on
previous observed data. As the R-BP is sensitive to the ordering of the data, we follow Fong
et al. [2023], Ghalebikesabi et al. [2022] by averaging the resulting R-BP marginal over 10
permutations of the data (see Tokdar et al. [2009], Dixit and Martin [2019] for a discussion
regarding the need of averaging over permutations). We assume a the same variance
parameter b for all the KDE pair copula estimators in the simplified vine and is chosen using
10-fold cross-validation, in a data-dependent manner minimizing the same energy score.
The assumption of a common bandwidth b is motivated by mapping all pair copulas to a
Gaussian space, which results in a common distance used on the latent pair copula spaces.
Another hyperparameter is the specific vine decomposition (the grouping of dimensions
in (10), see Appendix A.3) for which we use a regular vine structure [Panagiotelis et al.,
2017], selecting the best pair-copula decomposition with a modified Bayesian Information
Criterion suited for vines [Nagler et al., 2019].
Benefits of the energy score To note, unlike previous R-BP works [Hahn et al., 2018,
Fong et al., 2023, Ghalebikesabi et al., 2022] here we minimize the energy score to choose
the hyper-parameters rather than the log-score, both of which are strictly proper scoring
rules (see Appendix C and [Pacchiardi et al., 2024]) and define statistical divergences. This
choice was motivated by the robustness properties of the energy score [Pacchiardi et al.,
2021].
Further this choice also provides us some significant computational gain. We only need
(n)
samples from P to get an unbiased estimate of the energy score, which can be done
i
very efficiently through inverse probability sampling, hence halving the time to tune hyper-
(n)
parameters compared to using the log-score, which also requires densities p . Due to the
i
computational independence along each dimensions ensuing from Sklar’s copula decompo-
sition, this selection of ρ (and model fitting) can further be parallelised across dimensions
(and permutations of the data) to exploit the power of distributed computing. Similarly, the
cross-validation of the copula estimation can also be performed in parallel.
QB-VineforRegression/classificationtasks Ourframeworkcanaccommodateregression
and classification tasks in addition to density estimation, by rewriting the conditional density
as following:
(cid:110) (cid:111)
(cid:81)d
p (y)· p (x ) ·c(y,x ,...,x )
p(y,x) y i=1 i i 1 d c(y,x 1,...,x d)·p y(y)
p(y|x) = = = . (12)
(cid:110) (cid:111)
p(x) (cid:81)d
p (x ) ·c(x ,...,x )
c(x 1,...,x d)
i=1 i i 1 d
The estimation of (12) is comprised of estimating the d+1 marginals for y,x ,...,x and
1 d
the two copulas c(y,x) and c(x). We specify separate ρ across marginal densities as well
as separate bandwidths for copulas to be estimated with the methods described above. We
8note that for a copula decomposition to be unique, we require that the marginals involved be
continuous. This assumption is violated in the case of classification for binary outcomes y.
As such, we make use of an approximation that transforms y to a continuous scale by setting
negative examples to −10, and positive examples to 10 and adds standard Gaussian noise
to all examples, breaking ties on a distribution scale (a common approach taken in similar
contexts [Journel and Xu, 1994, Liu, 2019, Harris et al., 2022]). The rationale behind this
approximation is that by setting the two classes far apart on a marginal scale, we ensure no
overlaps occur, thereby maintaining a clear cut-off between the classes on the distribution
scale. Indeed, the separating boundary between the classes on a distribution scale will be
the percentile q = T 0 where T and T are the numbers of negative and positive samples,
T +T 0 1
1 0
respectively, in the training set. Consequently, we create different clusters in the copula
[0,1]d hypercube according to the separation on the distributional scale, facilitating the
identification of patterns in the data. We note that other approaches exist in the literature
[Chen, 2016, Chen and Hanson, 2017, Czado and Nagler, 2022] for classification with
copulas which our framework can be extended to.
Approximation error of Quasi-Bayesian Vine To quantify the approximation power of
the QB-Vine, we provide the following stochastic boundedness [Bishop et al., 2007] result
for univariate R-BP distributions with respect to the limiting martingale posterior P(∞). To
the best of our knowledge, this is the first convergence rate result for the R-BP.
Lemma 3.2. (R-BP predictive distribution convergence) The error of the distribution
function P(n)(x) in (4) is stochastically bounded with
(cid:12) (cid:12) (cid:16) (cid:17)
sup(cid:12)P(∞)(x)−P(n)(x)(cid:12) = O n−1/2 .
(cid:12) (cid:12) p
x∈X
Proof. We provide a proof in Appendix D.1.
In comparison to univariate Kernel Density Estimation with a mean-square optimal band-
(cid:112)
width b = O(n−1/5), which convergence at a rate O (n−2/5 ln(n)), we can see that the
n a.s.
marginal R-BP has a better rate with sample size. In what follows, we assume that the true
copula is a simplified vine of which we know the decomposition (a standard assumption in
the vine copula litterature [Nagler and Czado, 2016, Czado and Nagler, 2022, Spanhel and
Kurz, 2019]). We strengthen marginal guarantees with the theory on vine copulas to obtain
the following convergence result for the estimate of the copula density. In the statement
of the theorem, we consider marginal distributions {P(∞) }d and {P(n) }d are implicitly
i i=1 i i=1
applied to x for respective copulas.
Theorem 3.3. (Convergence of Quasi-Bayesian Vine) Assuming a correctly identified
simplified vine structure for c(∞)(u), and using univariate R-BP marginal distributions with
a simplified vine copula, the copula estimator error is stochastically bounded ∀x ∈ Rd with
|c(∞)(x)−c(n)(x)| = O (n−r) (13)
p
where n−r is the convergence rate of the KDE pair-copula.
Proof. We provide a proof in Appendix D.2.
9(cid:0) (cid:1)
For a bivariate KDE pair-copula estimator with optimal bandwidth b = O n−1/6 , we
n
obtain n−r = n−1/3 [Nagler and Czado, 2016]. From Stone [1980], we note the optimal
convergence rate of a nonparametric estimator is n−p/(2p+d) where p is the number of times
the estimator is differentiable. Therefore, as d increases, we expect large benefits from using
a vine copula decomposition for the QB-Vine. When the simplifying assumption does not
hold, the vine copula converges to a partial vine approximation of the true copula, as defined
in Spanhel and Kurz [2019]. Together, these two results guarantee accurate samples from
P(n)(x)byinverseprobabilitysamplingarguments. ByTheorem3.3,thecopulac(n) ensures
samples u = (u ,...,u ) on the [0,1]d hypercube have a dependence structure representative
1 d
of the data. Then, marginal distributions {P(n) }d recover dependent samples x ∈ Rd by
i i=1
evaluating the inverse of the distribution at u dimension-wise.
i
4 Related Work
Our method shares similarities with existing work on QB predictive density estimation with
analytical forms. The pivotal works of Newton et al. [1998], Newton [2002] and the ensuing
Predictive Recursion (PR) [Ghosh and Tokdar, 2006, Martin and Ghosh, 2008, Martin and
Tokdar, 2009, Tokdar et al., 2009, Martin, 2012, Ghosal and Van der Vaart, 2017, Martin,
2021]proposearecursivesolutiontothesameproblembutarerestrainedtolowdimensional
settings due to the numerical integration of a normalising constant over a space scaling with
d. A sequential importance sampling strategy for PR is proposed in Dixit and Martin [2023]
termed as PRticle Filter. The R-BP of Hahn et al. [2018] and the multivariate extensions in
Fong et al. [2023], Ghalebikesabi et al. [2022] also have a recursive form driven by bivariate
copula updates. In the multivariate case, imposing assumptions on the kernel structure
leads to a conditional factorisation of the joint predictive which recovers bivariate copula
updates. In Ghalebikesabi et al. [2022], an autoregressive Bayesian predictive (AR-BP) is
used, where the dependence is captured by dimension-wise similarity functions modelled
with kernels or deep autoregressive networks. The former relies on assumptions that might
be too simplistic to capture complex data while the latter loses the appeal of a data-efficient
predictive like in the R-BP. The Quasi-Bayesian Vine retains the advantages of the bivariate
copula-based recursion for marginal predictives and circumvents the need for assumptions
on the DPMM kernel. We achieve this via approximating the high-dimensional dependency
through a simplified vine copula which is highly flexible and does not use a deep network
to preserve data-efficiency, all the while maintaining an analytical expression. A relevant
benchmark are the NFs of Papamakarios et al. [2017], Durkan et al. [2019] with analytical
densities with a solid performance across data types and tasks.
5 Experiments
In this section, we compare our QB-Vine model against competing methods supporting
density evaluation with a closed-form expression. Further details on the experiments are
included in Appendix E, with an ablation study and comparison with sample size and
dimension. Code is included in the supplementary material.
10Table 1: Average log predictive score (lower is better) with error bars corresponding
to two standard deviations over five runs for density estimation on datasets analysed by
Ghalebikesabi et al. [2022]. We note that as dimension increases, the QB-Vine outperforms
all benchmarks.
WINE BREAST PARKIN IONO BOSTON
n/d 89/12 97/14 97/16 175/30 506/13
KDE 13.69 10.45 12.83 32.06 8.34
±0.00 ±0.24 ±0.27 ±0.00 ±0.00
DPMM 17.46 16.26 22.28 35.30 7.64
±0.60 ±0.71 ±0.66 ±1.28 ±0.09
(Diag)
DPMM (Full) 32.88 26.67 39.95 86.18 9.45
±0.82 ±1.32 ±1.56 ±10.22 ±0.43
MAF 39.60 10.13 11.76 140.09 56.01
±1.41 ±0.40 ±0.45 ±4.03 ±27.74
RQ-NSF 38.34 26.41 31.26 54.49 −2.20
±0.63 ±0.57 ±0.31 ±0.65 ±0.11
PRticle Filter 23.89 25.98 34.79 79.22 27.18
±0.93 ±1.06 ±3.95 ±9.87 ±3.12
R-BP 13.57 7.45 9.15 21.15 4.56
±0.04 ±0.02 ±0.04 ±0.04 ±0.04
R -BP 13.32 6.12 7.52 19.82 −13.50
d ±0.01 ±0.05 ±0.05 ±0.08 ±0.59
AR-BP 13.45 6.18 8.29 17.16 −0.45
±0.05 ±0.05 ±0.11 ±0.25 ±0.77
AR -BP 13.22 6.11 7.21 16.48 −14.75
d ±0.04 ±0.04 ±0.12 ±0.26 ±0.89
ARnet-BP 14.41 6.87 8.29 15.32 −5.71
±0.11 ±0.23 ±0.17 ±0.35 ±0.62
QB-Vine 13.76 4.67 4.93 −16.08 −31.04
±0.13 ±0.31 ±0.20 ±2.12 ±1.02
Density estimation We evaluate the QB-Vine on density estimation benchmark UCI
datasets [Asuncion and Newman, 2007] with small sample sizes ranging from 89 to 506
and dimensionality varying from 12 to 30, adding results for the QB-Vine and PRticle
Filter to the experiments of Ghalebikesabi et al. [2022]. We report the log predictive score
LPS= n1 (cid:80)n ktest−p(n train)(x k) on a held-out test dataset of size n
test
comprised of half the
test
samples with the other half used for training, averaging results over five runs with random
partitionseachtime. WecomparetheQB-Vineagainstthefollowingmodels: KernelDensity
Estimation [Parzen, 1962], DPMM [Rasmussen, 1999] with a diagonal (Diag) and full (Full)
covariance matrix for each mixture component, MAF [Papamakarios et al., 2017], RQ-NSF
[Durkan et al., 2019] as well as the closely related PRticle Filter [Dixit and Martin, 2023],
R-BP [Fong et al., 2023] and AR-BP [Ghalebikesabi et al., 2022]. For the last two Bayesian
predictivemodels, weaddasubscriptdtoindicatethattheρhyperparameterpossiblydiffers
acrossdimensions,andthenetsuffixindicatesanetwork-basedselectionofρfordimensions.
We observe in Table 1 that our QB-Vine method comfortably outperforms all competitors as
the dimension increases, matching the performance of other Bayesian predictive models for
the lower dimensional WINE dataset. Our method’s relative performance increases with the
dimension of the data, particularly achieving a much smaller LPS for IONO - the dataset
with the largest dimensions and a relatively small sample size. We accredit this performance
to the copula decomposition as that is our main distinguishing factor from the Bayesian
predictive models.
116
QBVine
MAF
7 RQ-NSF
R-BP
Rd-BP
8
AR-BP
ARd-BP
ARnet-BP
9
10
11
0 200 400 600 800
Training sample size
Figure 2: Density estimation on the Digits data (n = 1797,d = 64) with reduced training
sizes for the QB-Vine against other models fitted on the full training set. The QB-Vine
achieves competitive performance for training sizes as little as n = 50 and outperforms all
competitors once n > 200.
High-dimensional image dataset We further evaluate the QB-Vine on the digits dataset
(n =1797, d=64) as a high-dimensional example with a relatively low sample size. The
high contrast between n and d makes the problem suited for assessing the data efficiency
and convergence of the QB-Vine. We compare with the two NF models as their high model
capacity is a good fit for image data, as well as all the Bayesian predictive methods of
Section 5, from the study of Ghalebikesabi et al. [2022]. We report the average LPS in bits
perdimension(bpd)withstandarderrorsoverfiverunswithrandompartitions,usinghalfthe
sample size to train models and the other half to evaluate the LPS. Additionally, we report
the average LPS of the QB-Vine, obtained in the same way except for the training set size
beingreduced(to30,50,100,200,300,400,500). Figure2depictstheQB-Vine’sperformance
for different-sized training sets. When trained on the full train set, the QB-Vine outperforms
all competitors by a considerable margin. Furthermore, our method is competitive with as
little as 50 training samples and outperforms all benchmarks past a training size of 200,
demonstrating its data-efficiency and convergence speed. A complete numerical table is
reported in Appendix E.
Regression and classification We further demonstrate our method’s effectiveness on su-
pervised learning tasks, with three datasets for regression and two datasets for classification,
adding to the study of Ghalebikesabi et al. [2022]. For classification, we transform the
binary values to continuous ones to preserve copula assumptions, as detailed in Section 3.
We report the conditional LPS = 1 (cid:80)n test−p(n train)(y |x )
n k k k
test
overatestsetofsizen madeupofhalfthesampleswiththeconditionalestimatortrained
test
on the other half of the data. We compare our model against a Gaussian Process [Williams
and Rasmussen, 1995], a linear Bayesian model (Linear) [Minka, 2000], a one-hidden-
layer multilayer perceptron (MLP), as well the R-BP and AR-BP variants for supervised
tasks [Fong et al., 2023, Ghalebikesabi et al., 2022]. The QB-Vine outperforms competing
12
dpb
ni
SPLTable 2: Average LPS (lower is better) with error bars corresponding to two standard
deviations over five runs for supervised tasks analysed by Ghalebikesabi et al. [2022]. The
QB-Vine performs favourably against benchmarks, with relative performance improving as
samples per dimension decrease.
Regression Classification
BOSTON CONCR DIAB IONO PARKIN
n/d 506/13 1,030/8 442/10 351/33 195/22
Linear 0.87 0.99 1.07 0.33 0.38
±0.03 ±0.01 ±0.01 ±0.01 ±0.01
GP 0.42 0.36 1.06 0.30 0.42
±0.08 ±0.02 ±0.02 ±0.02 ±0.02
MLP 1.42 2.01 3.32 0.26 0.31
±1.01 ±0.98 ±4.05 ±0.05 ±0.02
R-BP 0.76 0.87 1.05 0.26 0.37
±0.09 ±0.03 ±0.03 ±0.01 ±0.01
R -BP 0.40 0.42 1.00 0.34 0.27
d ±0.03 ±0.00 ±0.02 ±0.02 ±0.03
AR-BP 0.52 0.42 1.06 0.21 0.29
±0.13 ±0.01 ±0.02 ±0.02 ±0.02
AR -BP 0.37 0.39 0.99 0.20 0.28
d ±0.10 ±0.01 ±0.02 ±0.02 ±0.03
ARnet-BP 0.45 −0.03 1.41 0.24 0.26
±0.11 ±0.00 ±0.07 ±0.04 ±0.04
QB-Vine −0.81 0.54 0.87 −1.85 −0.76
±1.26 ±0.34 ±0.20 ±1.16 ±0.28
methods on all datasets except CONCR. We believe the lower performance on CONCR
is due to the high number of samples relative to the dimension, preventing our approach
from fully exploiting the vine copula decomposition. Once again, the performance of the
QB-Vinemoreclearlyexceedsthatofcompetitorsasdimensionsincrease. TheQB-Vinehas
higher standard errors than other methods (except MLP), which we posit is the consequence
ofourconditionalestimatorinSection3beingdefinedasaratio,inflatingthevariationinthe
LPS. However, we highlight that an overly precise inference is more misleading/dangerous
than an overly uncertain one.
6 Discussion
We introduced the Quasi-Bayesian Vine, a joint Bayesian predictive density estimator with
an analytical form and easy to sample. This extends the existing works on Quasi-Bayesian
predictive densities, by using Sklar’s theorem to decompose the predictive density into
predictive marginals and a copula to model the high-dimensional dependency. This decom-
position also provides us a way for two-part estimation procedure, employing recursive
density estimation as in the Quasi-Bayesian works for the marginals and fitting a simplified
vine copula for the dependence, resulting in a convergence rate independent of dimension
for certain joint densities. We empirically demonstrate the advantage of QB-Vine on a range
of datasets compared to other benchmark methods, showing excellent modeling capabilities
in large dimensions with only a few training data.
However, there is potential for further improvements. The non-uniqueness of the vine
decomposition results in a search over a vast model space during estimation. In addition,
the hyperparameter selection of the KDE pair copulas could be ameliorated. Finally, the
13main assumption is the use of a simplified vine copula which is an approximation to the
true distribution. From a practical point of view, however, it offers fast computations and
outperforms competitors as shown in experiments. Other, future directions of this work
include incorporation of more effective copula models, or copulas accommodating different
dependence structures [Vatter and Nagler, 2018, Nagler et al., 2022, Huk et al., 2023a].
Another exciting direction are developments of new recursive Quasi-Bayes methods that
can be merged into a Quasi-Bayesian Vine model [Garelli et al., 2024].
References
Pierre Alquier, Badr-Eddine Chérief-Abdellatif, Alexis Derumigny, and Jean-David Ferma-
nian. Estimation of copulas via maximum mean discrepancy. Journal of the American
Statistical Association, pages 1–16, 2022.
Michael Arbel, Alex Matthews, and Arnaud Doucet. Annealed flow transport Monte Carlo.
In International Conference on Machine Learning, pages 318–330. PMLR, 2021.
Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Nicolas Chapados, and Alexandre
Drouin. TACTis-2: Better, faster, simpler attentional copulas for multivariate time series.
In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=xtOydkE1Ku.
Arthur Asuncion and David Newman. UCI machine learning repository, 2007.
Tim Bedford and Roger M Cooke. Probability density decomposition for conditionally
dependent random variables modeled by vines. Annals of Mathematics and Artificial
intelligence, 32:245–268, 2001.
Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshmi-
narayanan, Stephan Hoyer, and Rémi Munos. The Cramer distance as a solution to
biased Wasserstein gradients. arXiv preprint arXiv:1705.10743, 2017.
Patrizia Berti, Luca Pratelli, and Pietro Rigo. Limit theorems for a class of identically
distributed random variables. 2004.
Patrizia Berti, Luca Pratelli, and Pietro Rigo. Almost sure weak convergence of random
probability measures. Stochastics and Stochastics Reports, 78(2):91–97, 2006.
Patrizia Berti, Emanuela Dreassi, Fabrizio Leisen, Luca Pratelli, and Pietro Rigo. A
probabilistic view on predictive constructions for Bayesian learning. Statistical Science, 1
(1):1–15, 2023.
Alexandros Beskos, Natesh Pillai, Gareth Roberts, Jesus-Maria Sanz-Serna, and Andrew
Stuart. Optimal tuning of the hybrid Monte Carlo algorithm. 2013.
YvonneMBishop,StephenEFienberg,andPaulWHolland. Discretemultivariateanalysis:
Theory and practice. Springer Science & Business Media, 2007.
Arthur Charpentier, Jean-David Fermanian, and Olivier Scaillet. The estimation of copulas:
Theory and practice. Copulas: From theory to application in finance, 35, 2007.
Yuhui Chen. A copula-based supervised learning classification for continuous and discrete
data. Journal of Data Science, 14(4):769–782, 2016.
14YuhuiChenandTimothyHanson. Copularegressionmodelsfordiscreteandmixedbivariate
responses. Journal of Statistical Theory and Practice, 11:515–530, 2017.
David G Clayton. A model for association in bivariate life tables and its application in
epidemiological studies of familial tendency in chronic disease incidence. Biometrika, 65
(1):141–151, 1978.
Claudia Czado. Analyzing dependent data with vine copulas. Lecture Notes in Statistics,
Springer, 222, 2019.
ClaudiaCzadoandThomasNagler. Vinecopulabasedmodeling. AnnualReviewofStatistics
and Its Application, 9:453–477, 2022.
A Philip Dawid, Monica Musio, and Laura Ventura. Minimum scoring rule inference.
Scandinavian Journal of Statistics, 43(1):123–138, 2016.
Bruno De Finetti. La prévision: ses lois logiques, ses sources subjectives. In Annales de
l’institut Henri Poincaré, volume 7, pages 1–68, 1937.
VaidehiDixitandRyanMartin. Permutation-baseduncertaintyquantificationaboutamixing
distribution. arXiv preprint arXiv:1906.05349, 2019.
Vaidehi Dixit and Ryan Martin. A PRticle filter algorithm for nonparametric estimation of
multivariate mixing distributions. Statistics and Computing, 33(4):1–14, 2023.
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows.
Advances in neural information processing systems, 32, 2019.
Gal Elidan. Copulas in machine learning. In Copulae in Mathematical and Quantitative
Finance: Proceedings of the Workshop Held in Cracow, 10-11 July 2012, pages 39–60.
Springer, 2013.
Matthias Fischer, Christian Köck, Stephan Schlüter, and Florian Weigert. An empirical
analysis of multivariate copula models. Quantitative Finance, 9(7):839–854, 2009.
Edwin Fong, Chris Holmes, and Stephen G Walker. Martingale posterior distributions.
Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(5):1357–
1391, 2023.
Sandra Fortini and Sonia Petrone. Quasi-Bayes properties of a procedure for sequential
learning in mixture models. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 82(4):1087–1114, 2020.
Sandra Fortini and Sonia Petrone. Prediction-based uncertainty quantification for exchange-
able sequences. Philosophical Transactions of the Royal Society A, 381(2247):20220142,
2023.
David T. Frazier, Jeremias Knoblauch, and Christopher Drovandi. The impact of loss
estimation on Gibbs measures, 2024.
Samuele Garelli, Fabrizio Leisen, Luca Pratelli, and Pietro Rigo. Asymptotics of predictive
distributions driven by sample means and variances, 2024.
Gery Geenens, Arthur Charpentier, and Davy Paindaveine. Probit transformation for
nonparametric kernel estimation of the copula density. 2017.
15Sahra Ghalebikesabi, Chris Holmes, Edwin Fong, and Brieuc Lehmann. Density estimation
with autoregressive Bayesian predictives. arXiv preprint arXiv:2206.06462, 2022.
Subhashis Ghosal and Aad Van der Vaart. Fundamentals of nonparametric Bayesian
inference, volume 44. Cambridge University Press, 2017.
Jayanta K Ghosh and Surya T Tokdar. Convergence and consistency of Newton’s algorithm
for estimating mixing distribution. In Frontiers in statistics, pages 429–443. World
Scientific, 2006.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and
estimation. Journal of the American statistical Association, 102(477):359–378, 2007.
Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alex Smola. A
kernel method for the two-sample-problem. Advances in neural information processing
systems, 19, 2006.
Joshua Größer and Ostap Okhrin. Copulae: An overview and recent developments. Wiley
Interdisciplinary Reviews: Computational Statistics, 14(3):e1557, 2022.
Shixiang Shane Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive sequential
Monte Carlo. Advances in neural information processing systems, 28, 2015.
Ingrid Hobæk Haff, Kjersti Aas, and Arnoldo Frigessi. On the simplified pair-copula
construction—simply useful or too simplistic? Journal of Multivariate Analysis, 101(5):
1296–1310, 2010.
P Richard Hahn, Ryan Martin, and Stephen G Walker. On recursive Bayesian predictive
distributions. Journal of the American Statistical Association, 113(523):1085–1093, 2018.
Lucy Harris, Andrew TT McRae, Matthew Chantry, Peter D Dueben, and Tim N Palmer.
A generative deep learning approach to stochastic downscaling of precipitation forecasts.
Journal of Advances in Modeling Earth Systems, 14(10):e2022MS003120, 2022.
Edwin Hewitt and Leonard J Savage. Symmetric measures on Cartesian products. Transac-
tions of the American Mathematical Society, 80(2):470–501, 1955.
Nils Lid Hjort, Chris Holmes, Peter Müller, and Stephen G Walker. Bayesian nonparamet-
rics, volume 28. Cambridge University Press, 2010.
Marius Hofert, Martin Mächler, and Alexander J McNeil. Archimedean copulas in high
dimensions: Estimators and numerical challenges motivated by financial applications.
Journal de la Société Française de Statistique, 154(1):25–63, 2013.
MariusHofert,IvanKojadinovic,MartinMächler,andJunYan. Elementsofcopulamodeling
with R. Springer, 2018.
Matthew Hoffman, Pavel Sountsov, Joshua V Dillon, Ian Langmore, Dustin Tran, and
Srinivas Vasudevan. Neutralizing bad geometry in Hamiltonian Monte Carlo using neural
transport. arXiv preprint arXiv:1903.03704, 2019.
Chris Holmes and Stephen G Walker. Statistical inference with exchangeability and martin-
gales. Philosophical Transactions of the Royal Society A, 381(2247):20220143, 2023.
16David Huk, Rilwan A Adewoyin, and Ritabrata Dutta. Probabilistic rainfall downscaling:
Joint generalized neural models with censored spatial Gaussian copula. arXiv preprint
arXiv:2308.09827, 2023a.
David Huk, Lorenzo Pacchiardi, Ritabrata Dutta, and Mark Steel. David Huk, Lorenzo
Pacchiardi, Ritabrata Dutta and Mark Steel’s contribution to the discussion of ‘Martingale
posterior distributions’ by Fong, Holmes and Walker. Journal of the Royal Statistical
Society Series B: Statistical Methodology, 85(5):1405–1406, 2023b.
Tim Janke, Mohamed Ghanmi, and Florian Steinke. Implicit generative copulas. Advances
in Neural Information Processing Systems, 34:26028–26039, 2021.
Harry Joe. Dependence modeling with copulas. CRC press, 2014.
André G Journel and Wenlong Xu. Posterior identification of histograms conditional to
local data. Mathematical Geology, 26:323–359, 1994.
Sanket Kamthe, Samuel Assefa, and Marc Deisenroth. Copula flows for synthetic data
generation. arXiv preprint arXiv:2101.00598, 2021.
Leonid V Kantorovich. Mathematical methods of organizing and planning production.
Management science, 6(4):366–422, 1960.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
Chun Kai Ling, Fei Fang, and J Zico Kolter. Deep Archimedean copulas. Advances in
Neural Information Processing Systems, 33:1535–1545, 2020.
Weiwei Liu. Copula multi-label learning. Advances in Neural Information Processing
Systems, 32, 2019.
Jan-Matthis Lueckmann, Jan Boelts, David Greenberg, Pedro Goncalves, and Jakob Macke.
Benchmarking simulation-based inference. In International conference on artificial
intelligence and statistics, pages 343–351. PMLR, 2021.
Ryan Martin. Convergence rate for predictive recursion estimation of finite mixtures.
Statistics & Probability Letters, 82(2):378–384, 2012.
Ryan Martin. A survey of nonparametric mixing density estimation via the predictive
recursion algorithm. Sankhya B, 83:97–121, 2021.
Ryan Martin and Jayanta K Ghosh. Stochastic approximation and Newton’s estimate of a
mixing distribution. Statistical Science, pages 365–382, 2008.
Ryan Martin and Surya T Tokdar. Asymptotic properties of predictive recursion: robustness
and rate of convergence. 2009.
Alex Matthews, Michael Arbel, Danilo Jimenez Rezende, and Arnaud Doucet. Continual
repeated annealed flow transport Monte Carlo. In International Conference on Machine
Learning, pages 15196–15219. PMLR, 2022.
Thomas Minka. Bayesian linear regression. Technical report, Citeseer, 2000.
Thomas Nagler and Claudia Czado. Evading the curse of dimensionality in nonparametric
density estimation with simplified vine copulas. Journal of Multivariate Analysis, 151:
69–89, 2016.
17Thomas Nagler, Christian Schellhase, and Claudia Czado. Nonparametric estimation of
simplified vine copula models: comparison of methods. Dependence Modeling, 5(1):
99–120, 2017.
Thomas Nagler, Christian Bumann, and Claudia Czado. Model selection in sparse high-
dimensional vine copula models with an application to portfolio risk. Journal of Multi-
variate Analysis, 172:180–192, 2019.
Thomas Nagler, Daniel Krüger, and Aleksey Min. Stationary vine copula models for
multivariate time series. Journal of Econometrics, 227(2):305–324, 2022.
Michael A Newton. On a nonparametric recursive estimator of the mixing distribution.
Sankhya¯: The Indian Journal of Statistics, Series A, pages 306–322, 2002.
Michael A Newton, Fernando A Quintana, and Yunlei Zhang. Nonparametric Bayes
methods using predictive updating. In Practical nonparametric and semiparametric
Bayesian statistics, pages 45–61. Springer, 1998.
Lorenzo Pacchiardi and Ritabrata Dutta. Score matched neural exponential families for
likelihood-free inference. The Journal of Machine Learning Research, 23(1):1745–1815,
2022.
Lorenzo Pacchiardi, Sherman Khoo, and Ritabrata Dutta. Generalized Bayesian likelihood-
free inference. arXiv preprint arXiv:2104.03889, 2021.
Lorenzo Pacchiardi, Rilwan A Adewoyin, Peter Dueben, and Ritabrata Dutta. Probabilistic
forecasting with generative networks via scoring rule minimization. Journal of Machine
Learning Research, 25(45):1–64, 2024.
Anastasios Panagiotelis, Claudia Czado, Harry Joe, and Jakob Stöber. Model selection for
discrete regular vine copulas. Computational Statistics & Data Analysis, 106:138–152,
2017.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for
density estimation. Advances in neural information processing systems, 30, 2017.
George Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast
likelihood-free inference with autoregressive flows. In The 22nd international conference
on artificial intelligence and statistics, pages 837–848. PMLR, 2019.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and
Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference.
Journal of Machine Learning Research, 22(57):1–64, 2021.
Emanuel Parzen. On estimation of a probability density function and mode. The annals of
mathematical statistics, 33(3):1065–1076, 1962.
Carl Rasmussen. The infinite Gaussian mixture model. Advances in Neural Information
Processing Systems, 12, 1999.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In International conference on
machine learning, pages 1278–1286. PMLR, 2014.
18Gareth O Roberts and Jeffrey S Rosenthal. Optimal scaling of discrete approximations
to Langevin diffusions. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 60(1):255–268, 1998.
GO Roberts, A Gelman, and WR Gilks. Weak convergence and optimal scaling of random
walk Metropolis algorithms. The annals of applied probability, 7(1):110–120, 1997.
Ulf Schepsmeier, Jakob Stoeber, Eike Christian Brechmann, Benedikt Graeler, Thomas
Nagler, Tobias Erhardt, Carlos Almeida, Aleksey Min, Claudia Czado, Mathias Hofmann,
et al. Package ‘vinecopula’. R package version, 2(5), 2015.
Jayaram Sethuraman. A constructive definition of Dirichlet priors. Statistica sinica, pages
639–650, 1994.
Phillip Si, Zeyi Chen, Subham Sekhar Sahoo, Yair Schiff, and Volodymyr Kuleshov. Semi-
autoregressive energy flows: exploring likelihood-free training of normalizing flows. In
International Conference on Machine Learning, pages 31732–31753. PMLR, 2023.
M Sklar. Fonctions de répartition à n dimensions et leurs marges. In Annales de l’ISUP,
volume 8, pages 229–231, 1959.
Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for
mcmc. Advances in neural information processing systems, 30, 2017.
Fabian Spanhel and Malte S Kurz. Simplified vine copula models: Approximations based
on the simplifying assumption. Electronic Journal of Statistics, 13:1254–1291, 2019.
Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Gert RG Lanckriet, and Bern-
hard Schölkopf. A note on integral probability metrics and φ-divergences. arXiv preprint
arXiv:0901.2698, 2009.
Charles J Stone. Optimal rates of convergence for nonparametric estimators. The annals of
Statistics, pages 1348–1360, 1980.
Gábor J Székely and Maria L Rizzo. A new test for multivariate normality. Journal of
Multivariate Analysis, 93(1):58–80, 2005.
Surya T Tokdar, Ryan Martin, and Jayanta K Ghosh. Consistency of a recursive estimate of
mixing distributions. The Annals of Statistics, pages 2502–2522, 2009.
Thibault Vatter and Thomas Nagler. Generalized additive models for pair-copula construc-
tions. Journal of Computational and Graphical Statistics, 27(4):715–727, 2018.
Christopher Williams and Carl Rasmussen. Gaussian processes for regression. Advances in
neural information processing systems, 8, 1995.
A Copulas
Copulas are a widely adopted tool in statistics and machine learning for modelling densities,
permitting the construction of joint densities in a two-step estimation process. One firstly
estimates the marginal densities as if they were independent of each other, and secondly,
models the copula which accounts for the dependence between dimensions. This approach
is motivated through Sklar’s theorem:
19Theorem A.1 (Sklar Sklar [1959]). Let P be an d-dimensional distribution function with
continuous marginal distributions P ,P ,...,P . Then there exists a copula distribution C
1 2 d
such that for all x = (x ,x ,...,x ) ∈ Rd:
1 2 d
P(x ,...,x ) = C(P (x ),...,P (x )) (14)
1 d 1 1 n d
And if a probability density function (pdf) is available:
p(x ,...,x ) = p (x )·...·p (x )·c(P (x ),...,P (x )) (15)
1 d 1 1 d d 1 1 d d
where p (x ),...,p (x ) are the marginal pdfs, and c(P (x ),...,P (x )) is the copula pdf.
1 1 d d 1 1 d d
If the marginal distributions are absolutely continuous, the copula is unique. Consequently,
one can decompose the estimation problem of learning p(x) by first learning all the
marginals {p }d , and in a second step learning an appropriate copula model c(u ,...,u ),
i i=1 1 d
where u := P (x ),i ∈ {1,...d} are the images of the x under the cdf of each dimension.
i i i i
By applying cdf transformations marginally, the copula is agnostic to the differences
between dimensions such as axis scaling, and purely focuses upon capturing the dependence
structure among them.
Most parametric copula models are only suited for two-dimensional dependence modelling
and greatly suffer from the curse of dimensionality [Fischer et al., 2009, Hofert et al., 2013].
The Gaussian copula is a popular parametric choice as it is well-studied and can be fitted
quickly even to moderate dimensions [Hofert et al., 2018]. However, it lacks the desired
flexibility to capture more complex dependencies involving multiple dimensions. Among
nonparametric copulas, Kernel Density Estimator (KDE) copulas [Geenens et al., 2017]
are commonly used. They apply an inverse Gaussian distribution to the observed {u }d to
i i=1
map them to a latent space and perform regular KDE on the latent density. However, this
KDE copula method suffers from the poor scaling of KDE estimators in higher dimensions.
Finally, deep learning copula models remain a nascent line of research and typically are
morecomputationallyexpensiveandsample-dependentduetotheirrelianceonlargemodels,
with only a handful of candidate solutions such as Ling et al. [2020], Kamthe et al. [2021],
Janke et al. [2021], Ashok et al. [2024]. As such, current copula models are mostly limited
to low to medium-dimensional modelling [Joe, 2014].
A.1 Gaussian copula
A popular parametric copula model is the Gaussian copula. It assumes that the dependence
betweendimensionsisidenticaltothatofaGaussiandistributionwithmean0andcovariance
matrix Σ:
N (Φ−1(u ),...,Φ−1(u );0,Σ)
d 1 d
c(u ,...,u ) = . (16)
1 d (cid:81)d
N(Φ−1(u );0,1)
i=1 i
As such its only parameters are the off-diagonal entries of the covariance matrix Σ. In the
case of d = 2, there is a single parameter to estimate for the Gaussian copula.
20A.2 Gaussian Kernel Density Estimator copulas
As the marginal distribution of a copula is uniform in [0,1], the support of the copula
estimator is restricted to [0,1]d and must satisfy the uniform marginal condition. It is
fairly difficult to build such an estimator that fulfills both desiderata with high expressivity.
Gaussian Kernel Density Estimation for copulas [Charpentier et al., 2007, Geenens et al.,
2017] is a popular approach for such nonparametric copula models, which models the
copula on a latent Gaussian space. We explain the approach in the following.
By the inverse sampling theorem, for any R-valued continuous random variable X, applying
the corresponding cumulative distribution function F to X results in F(X) being uniformly
distributed in [0,1]. Thus, we can transform a uniform random variable into any continuous
distribution using its inverse cumulative distribution function. In the copula estimation stage,
samples from the copula already have uniform marginal distributions, meaning we can apply
any inverse distribution F−1 to each marginal sample value and obtain a corresponding
latent marginal distribution. If F−1 is the inverse standard normal distribution, then the
latent distribution for each marginal will be normal and R-valued with no uniformity
restrictions.
Gaussian KDE for copulas applies inverse standard normal distributions to each marginal,
resulting in a latent representation of the samples on a Gaussian space. As such, one can
employ regular Gaussian KDE to estimate the copula density on this latent space. In the
case of two-dimensional copulas, the ensuing estimator has the following expression:
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:80)K
ϕ Φ−1(u)−Φ−1(u );0,b ·ϕ Φ−1(v)−Φ−1(v );0,b
k=1 k k
cˆ(u,v) = , (17)
(cid:16) (cid:17) (cid:16) (cid:17)
ϕ Φ−1(u);0,b ·ϕ Φ−1(v);0,b
where (u,v) and (u ,v ) are both in [0,1]2,{(u ,v )}K are observed copula samples, and Φ
k k k k k=1
and ϕ are respectively the Gaussian distribution and density with mean 0 and variance b > 0.
A.3 Vine copulas
A vine copula is an efficient dependency modelling method which decomposes d-
dimensional copula estimation into d(d−1)/2 bivariate copula estimation via structured
conditioning [Bedford and Cooke, 2001]. Here we illustrate the decomposition by a 3-
dimensional copula density c for the random vector (U, V, W):
U,V,W
(cid:18) (cid:12) (cid:19)
(cid:12)
c U,V,W(u,v,w) = c U,V(u,v)·c V,W(v,w)·c
U,W|V
C U|V(u|v), C W|V(w|v)(cid:12)v , (18)
(cid:12)
where
• C is the copula of (U, V), C is the copula of (V, W);
U,V V,W
• C is the copula of (C (U|v),C (W|v)) conditional on V = v;
U,W|V=v U|V W|V
21• C is the conditional distribution of C on V, and C is the conditional
U|V U,V W|V
distribution of C on V.
V,W
Generally,thedistributionofC willchangewithdifferentvaluesv andthiswillmake
U,W|V=v
the model relatively complex. Therefore, it is common to use the simplifying assumption by
ignoring the conditioning of pair copulas and simply model them as unconditional bivariate
densities:
(cid:18) (cid:19)
c (u,v,w) = c (u,v)·c (v,w)·c C (u|v), C (w|v) . (19)
U,V,W U,V V,W U,W|V U|V W|V
The rationale of the simplified assumption is studied in Haff et al. [2010]. In this paper,
we mainly focus on the regular vine copula (R-vine) with a simplified assumption. The
construction of an R-vine copula has two basic ingredients: (1) a valid tree structure of all
random variables, (2) the choice of family for bivariate copulas.
Before we introduce the tree structure which is valid to construct a R-vine copula, let
us first rigorously define the random variable of a copula we want to estimate. Suppose
U = (U ,U ,...,U ) is a d-dimensional random variable which is distributed as a copula C,
1 2 d
then we have that each marginal random variable U is uniformly distributed in [0,1]-scale.
i
For notational simplicity, we will use the index of a random variable as its notation instead.
Denote T = {T ,...,T } as a sequence of (d−1) trees in terms of T = (V ,E ). Here we
1 d−1 i i i
use a set of two nodes to represent the corresponding edge in T , i.e., e = (a,b) if node a and
i
b in E are linked. To construct a valid R-vine copula, T satisfies the following conditions:
i
• T is a tree with a set of edges E and a set of nodes V = {1,...,d};
1 1 1
• T is a tree with a set of edges E and a set of nodes V = V(E ) for i = 2,3,...,d−1,
i i i i−1
where V(E) denoted that the pair of nodes which are linked by an edge in E is treated
as a new node.
• If two nodes in E are linked by an edge in V , then they must be linked to one
i+1 i+1
common node in T .
i
For ∀e = (a,b) ∈ T with i ≥ 2, we define
i
c(e) = a∩b, a(e) = a\c(e), b(e) = b\c(e). (20)
Finally, we rigorously define the R-vine copula as follows.
Definition A.2 (Regular Vine Copulas). A d-dimensional copula C is a regular vine copula
if there exists a tuple (T ,C) such that
• T is a regular vine tree sequence with (d−1) trees;
• C = {C : e ∈ E ,i ∈ [d−1],C is a bivariate copula} is a family of bivariate copulas
e i e
for each edge;
• For ∀e ∈ E and ∀i ∈ [d − 1], then C is corresponding to the copula of
i e
(cid:18) (cid:19)(cid:12)
(cid:12)
a(e),b(e) (cid:12)c(e).
(cid:12)
22Therefore, the density function of R-vine C can be expressed as
c (u ,...,u )
(T,C) 1 d
d−1
(cid:89)(cid:89) (21)
= c (C (v |v ),C (v |v )).
(a(e),b(e))|c(e) a(e)|c(e) a(e) c(e) b(e)|c(e) b(e) c(e)
i=1e∈E
i
Here we illustrate an R-vine copula density in five dimensions where we use different colors
for the pair copulas corresponding to each of the d−1 = 4 trees.
c (u ,u ,u ,u ,u ) =c(u ,u )·c(u ,u )·c(u ,u )·c(u ,u )
T,C 1 2 3 4 5 1 2 1 3 2 4 3 5
·c (u ,u )·c (u ,u )·c (u ,u )
1,5|3 1|3 5|3 2,3|1 2|1 3|1 1,4|2 1|2 4|2
(22)
·c (u ,u )·c (u ,u )
2,5|1,3 2|1,3 5|1,3 3,4|1,2 3|1,2 4|1,2
·c (u ,u ).
4,5|1,2,3 4|1,2,3 5|1,2,3
Specifying the tree structure for an R-vine decomposition is essential and plays an important
role in pair-copula estimation through conditioning. An excellent overview is given in
Czado [2019]. Notably, an R-vine decomposition is not unique for a given joint copula pdf.
An appealing tree selection algorithm is proposed in Nagler et al. [2019], where authors
derive a modified BIC criterion that prioritizes sparse trees while being consistent when the
√
dimension d grows at most at the rate of n where n is the sample size.
B Martingale Posterior Distributions
HereweexplainmartingaleposteriordistributionsasajustificationoftheBayesianapproach
through a focus on prediction.
B.1 The Bayesian Choice as a Consequence of Predictive Uncertainty
A common goal in statistics is the inference of a parameter or quantity θ by analysing data
in the form of observations (x ,...,x ), n ∈ N. The rationale for learning from data is
1 n
that each observation provides information about the underlying process and parameter θ,
without which statistical analysis would be redundant. Indeed, consider a decision-maker
basing their decision on their belief about θ (in an i.i.d. setting). Having observed data
(x ,...,x ) and given the opportunity to observe an additional data point x , they would
1 n n+1
be assumed to accept, as they could refine their beliefs on θ. This process of updating one’s
beliefs based on data is at the core of the Bayesian approach. Equipped with an initial guess
about the parameter of interest θ captured by the prior π(θ), the goal is the inference about
the distribution of the parameter given observed data (x ,...,x ) which is encoded in the
1 n
posterior density π(n)(θ|(x ,...,x )).
1 n
For the decision-maker to refuse, it would mean that the additional observation has no
significant effect on their belief. This implies, as identified by Fong et al. [2023] and
HolmesandWalker[2023],thatthereisapointwhereadditionalobservations(x ,...,x )
n+1 N
provide no benefit to the knowledge update of θ. Inspecting the Bayesian posterior in terms
of observed data x = (x ,...,x ) and possible future observations to be made x ,
1:n 1 n n+1:∞
23written as
(cid:90)
π(n)(θ|x ) = π(n)(θ,x |x ) dx , (23)
1:n n+1:∞ 1:n n+1:∞
one can expand the right-hand integrand by including the predictive density p for future data,
obtaining
(cid:90)
π(n)(θ|x ) = π (θ|x )·p(x |x ) dx . (24)
1:n ∞ 1:∞ n+1:∞ 1:n n+1:∞
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Bayesestimate predictivedensity
Having rewritten the posterior density in this way, it becomes apparent that the uncertainty
in the value of θ, which is given by the Bayes estimate, is a consequence of the uncertainty
surrounding the imputation of missing observations x through the predictive. With this
n+1:∞
insight, unlike the traditional Bayesian construction of prior-likelihood, Fong et al. [2023]
proceed to replace the predictive density p with a predictive mechanism using all available
data to impute the missing x (or at least impute x for a sufficiently large N), and
n+1:∞ n+1:N
replace the Bayes estimate π(∞) with an appropriate functional of the complete data x .
1:∞
This predictive mechanism used to impute the unobserved data x given observed x
n+1:∞ 1:n
directly leads to the martingale posterior as the limiting distribution of functionals when the
unobserved data has been imputed.
B.2 Constructing a martingale posterior for the DPMM
As explained in Section 2, given a sequence of observations (x ) from the data generating
i i≥1
process, the Bayesian approach wants to obtain an infinite sequence of copula densities
(c ) to recursively update the Bayesian predictive distribution. However, finding such
i i≥1
an infinite sequence of copulas isn’t feasible in practice and the ensuing copula family is
determined by the prior-likelihood pair one implicitly chooses. For example, from Hahn
et al. [2018], we have the following copula sequences:
• If we select the likelihood and prior as l(y;θ) = θe−θy and π(θ) = e−θ, then
(n+1)[(1−u)−1−1/n(1−v )−1−1/n]
n
c (u,v ) = , (25)
n n
n[(1−u)−1/n +(1−v )−1/n −1]n+2
n
which is a sequence of Clayton copula [Clayton, 1978] with parameter n−1.
• If we select the likelihood and prior as l(y|θ) = ϕ(y;θ,1) and π(θ) = ϕ(θ;0,τ−1), then
ϕ (Φ−1(u),Φ−1(v );0,Σ )
2 n ρ
c (u,v ) = n , (26)
n n ϕ(Φ−1(u);0,1)ϕ(Φ−1(v );0,1)
n
where ϕ is the pdf of a bivariate normal distribution, ϕ is the pdf of normal distribu-
2
tion, and
(cid:20) (cid:21)
1 ρ
Σ = n , ρ = (n+τ)−1. (27)
ρ n
n ρ 1
n
Therefore, we obtain a sequence of Gaussian copulas with parameters {ρ } .
n n≥1
Further, if we additionally put a conjugate prior on the variance parameter σ2 of the
likelihood l(y|θ) = ϕ(y;θ,σ2), then we will recover a sequence of Student-t copulas.
24The issue of model mis-specification naturally arises here due to the selection of prior-
likelihood pair. Fortunately, here we can employ the DPMM which has relatively high
flexibility due to its nonparametric nature.For the DPMM, suppose the first observation x
1
arrives, then we can obtain the first predictive via the updating kernel
E[f (x)f (x )]
G G 1
k (x,x ) = , (28)
1 1
E[f (x)]·E[f (x )]
G G 1
where p (x) = E[f (x)]. Then, we can derive the numerator as
0 G
E[f (x)f (x )]
G G 1
(cid:34) (cid:35)
∞ ∞
(cid:88)(cid:88)
=E w w ϕ(x;θ ,1)ϕ(x ;θ ,1)
j k j 1 k
j=1 k=1
(cid:32) (cid:34) (cid:35)(cid:33)
∞
(cid:88)
= 1−E w2 E [ϕ(x;θ,1)] E [ϕ(x ;θ,1)]
i G 0 G 0 1
(29)
i=1
(cid:34) (cid:35)
∞
(cid:88)
+E w2 E [ϕ(x;θ,1)ϕ(x ;θ,1)]
i G 0 1
i=1
(cid:32) (cid:34) (cid:35)(cid:33) (cid:34) (cid:35)
∞ ∞
(cid:88) (cid:88)
= 1−E w2 p (x)p (x )+E w2 E [ϕ(x;θ,1)ϕ(x ;θ,1)] .
i 0 0 1 i G 0 1
i=1 i=1
The first equality follows from the stick-breaking representation [Sethuraman, 1994] of the
DP, we can formulate G as
∞
(cid:88)
G(·) = w δ (·) (30)
i θ
i
i=1
where
(cid:89)
i.i.d. i.i.d.
w = v (1−v ), v ∼ Beta(1,a), θ ∼ G . (31)
i i j i i 0
j<i
(cid:80)∞
Then,thesecondequalityfollowsfromtheconditionthat w = 1almostsurely. Denote
α =
E(cid:2)(cid:80)∞ w2(cid:3)
, then we can write the updating kernel
asi=1 i
i=1 i
E [ϕ(x;θ,1)ϕ(x ;θ,1)]
G 1
k (x,x ) = (1−α)+α· 0
1 1
p (x)p (x )
0 0 1
ϕ (Φ−1(u),Φ−1(v );0,Σ ) (32)
2 1 ρ
= (1−α)+α·
ϕ(Φ−1(u);0,1)ϕ(Φ−1(v );0,1)
1
= (1−α)+α·c (Φ−1(u),Φ−1(v )),
ρ 1
where
(cid:20) (cid:21)
1 ρ
• covariance matrix Σ = ,
ρ
ρ 1
25• Φ−1 is the inverse cdf of standard normal distribution,
• u = P (x), v = P (x ),
0 1 0 1
• p (·) = ϕ(·;0,1+τ−1),
0
• c is a bivariate Gaussian copula density function with the correlation parameter
ρ
ρ = 1/(1+τ).
Therefore, we can see that the copula c (u,v ) for the first updating step is a mixture of
1 1
independent copula and Gaussian copula. However, we will lose the tractable form of the
copula from the second updating step. Following Hahn et al. [2018], instead of deriving the
explicit rule for the sequence of copula, we fix the correlation parameter ρ and set α to be a
(0,1)-valued decreasing sequence (α ) .
i i≥1
C Strictly Proper Scoring Rules
In probabilistic machine learning, a Scoring Rule (SR) measures the appropriateness of a
distribution P in modelling an observation x ∈ X through a score written as S(P,x). For
(hyper) parameter estimation, suppose we aim to model the underlying distribution of an
observation x using a family of distributions P parametrized by θ, then we can use a SR S
θ
to select the suitable θ. If we assume that x ∼ Q, then we can obtain the expected SR S via
taking expectation w.r.t. x as
(cid:104) (cid:105)
S(P,Q) = E S(P,x) . (33)
x∼Q
Following Gneiting and Raftery [2007], we call S strictly proper if S(P,Q) is minimized if
and only if P = Q, i.e. for ∀P ∈ P with P ̸= Q such that
S(Q,Q) < S(P,Q). (34)
Considering our settings of marginal distributions, we have observed data x i. ∼i.d. P∗, and
1:n
we aim to model P∗ using P . More explicitly, we don’t need to use P directly in the
θ θ
expected SR, instead we normally use its probability density function or samples from this
distribution to evaluate. In general, we optimise
θ∗ = argminE S(P ,Q)
θ∈Θ θ
θ∈Θ
= argminS(P ,Q).
θ
θ∈Θ
Since we do not have the complete population of Q, we use the empirical SR Sˆinstead, i.e.
θˆ∗ = argminE Sˆ (P ,Q), (35)
θ∈Θ θ
θ∈Θ
where Sˆ (P ,Q) = 1 (cid:80)n S(P ,x ). Under mild conditions, it can be proven that θˆ∗ → θ∗
θ n k=1 θ k
asymptotically in Dawid et al. [2016]. For any positive definite kernel k(·, ·), the Kernel
Score [Gneiting and Raftery, 2007] is given by
S (P ) = E[k(Y,Y′)]−2·E[k(Y,x)], (36)
k θ,x
26where Y,Y′ ∼ P . If we set k(x,y) = −||x−y||β, then we obtain the energy score which
θ 2
is strictly proper if E ||Y||β < ∞. The energy Score is defined as
Y∼P θ 2
Sβ (P ,x) = 2·E||Y−x||β −E||Y−Y′||β , for β ∈ (0,2]. (37)
E θ 2 2
Here YandY′ are i.i.d. samples from P . Practically, given finite samples
θ
i.i.d.
y (θ),...,y (θ) ∼ P where y(θ) denotes a sample from P which is a differentiable
1 m θ θ
function of θ, the unbiased estimate of the energy Score via Monte Carlo approximation is
m
2 (cid:88) 1 (cid:88)
Sˆβ (y (θ),x) = ||y (θ)−x||β − ||y (θ)−y (θ)||β . (38)
E 1:m m j 2 m(m−1) j k 2
j=1 k̸=j
Similarly, we can derive the unbiased gradient of Sβ (y (θ),x) w.r.t. θ which is crucial for
E 1:m
any gradient descent algorithm. For notational convenience, we define
g(Y,Y′,x) = 2·||Y−x||β −||Y−Y′||β , (39)
2 2
then Sβ (P ,x) = E [g(Y,Y′,x)]. Next, we have that
E θ Y,Y′∼P θ
β
∇ S (P ,x)
θ E θ
=∇ E [g(Y,Y′,x)]
θ Y,Y′∼P
θ
(cid:104) (cid:105)
=E ∇ g(Y,Y′,x)
Y,Y′∼P θ
θ
m m
1 (cid:88)(cid:88)
≃ ∇ g(y (θ),y (θ),x)·δ (40)
θ j k {j̸=k}
m(m−1)
j=1 k=1
m
2 (cid:88) 1 (cid:88)
β β
= ∇ ||y (θ)−x|| − ∇ ||y (θ)−y (θ)||
m θ j 2 m(m−1) θ j k 2
j=1 k̸=j
β
=∇(cid:98)θS E(y 1:m(θ),x).
Furthermore,ifP andQbotharedistributionsofR-valuedrandomvariables,thentheenergy
θ
Score will be reduced to
Sβ (P ,x) = 2·E|Y −x|β −E|Y −Y′|β, for β ∈ (0,2], (41)
E θ
where Y,Y′ ∼ P . Notice that this will become the Continuous Ranked Probability Score
θ
(CRPS) [Székely and Rizzo, 2005] when β = 1.
The energy Score is a strictly proper scoring rule [Gneiting and Raftery, 2007, Dawid
et al., 2016, Pacchiardi and Dutta, 2022] and is a special instance of the maximum mean
discrepancy [Gretton et al., 2006] as well as a statistical divergence. It has been used as
an effective objective for copula estimation [Janke et al., 2021, Alquier et al., 2022, Huk
et al., 2023a] and even for R-BP marginal predictives as shown in Huk et al. [2023b]. It has
also enjoyed success as a objective for Normalising Flows [Si et al., 2023] and generative
models [Pacchiardi et al., 2024]. The energy Score is the only objective among Wasserstein
p-metric [Kantorovich, 1960] that supports unbiased gradient evaluations [Bellemare et al.,
2017, Pacchiardi and Dutta, 2022], has a known optimisation-free solution and features a
faster convergence rate than similar integral probability metrics [Sriperumbudur et al., 2009,
Frazier et al., 2024].
27D Proofs
D.1 Lemma 3.2
Proof. We follow the Definition 14.4-3 of stochastic boundedness from Bishop et al. [2007].
Begin by choosing δ ∈ (0,1). From Proposition 1 in Fong et al. [2023], we have for ϵ > 0
and M > n, over the supremum of x ∈ X:
(cid:32) (cid:33)
(cid:16)(cid:12) (cid:12) (cid:17) ϵ2
P (cid:12)P(M)(x)−P(n)(x)(cid:12) ≥ ϵ ≤ 2exp −
(cid:12) (cid:12) 2ϵα n+1 + 1 (cid:80)M α2
3 2 i=n+1 i
(cid:32) (cid:33)
(cid:16)(cid:12) (cid:12) (cid:17) ϵ2
⇔ lim P (cid:12)P(M)(x)−P(n)(x)(cid:12) ≤ ϵ ≥ lim 1−2exp − .
M→∞ (cid:12) (cid:12) M→∞ 2ϵα n+1 + 1 (cid:80)M α2
3 2 i=n+1 i
(42)
Next, we choose a value ϵ (where the subscript shows that this quantity is dependent on n)
n
to have the appropriate probability on the right-hand side by enforcing:
(cid:32) (cid:33)
ϵ2
δ = 2exp − n
2ϵ nα n+1 + 1 (cid:80)∞ α2
3 2 i=n+1 i
(43)
(cid:18) (cid:19) (cid:18) (cid:19) ∞
δ 2α 1 δ (cid:88)
⇔ 0 = ϵ2 +log n+1 ϵ + log α2
n 2 3 n 2 2 i
i=n+1
with solution
(cid:113)
−log(cid:0) δ(cid:1) 2α n+1 + (cid:2) log(cid:0) δ(cid:1) 2α n+1(cid:3)2 −2log(cid:0) δ(cid:1)(cid:80)∞ α2
2 3 2 3 2 i=n+1 i
ϵ = . (44)
n
2
Due to δ ∈ (0,1) and α > 0∀i, we have:
i
(cid:113)
−log(cid:0) δ(cid:1) n1/22α n+1 + (cid:2) log(cid:0) δ(cid:1) n1/22α n+1(cid:3)2 −2log(cid:0) δ(cid:1) n(cid:80)∞ α2
|ϵ | = n−1/2 2 3 2 3 2 i=n+1 i . (45)
n
2
With the choice of α = (2− 1)( 1 ), we see that lim n−aα is bounded for all powers
i i i+1 n→∞ i
a ≥ −1. As such, both −log(cid:0) δ(cid:1) n1/22α n+1 and (cid:2) log(cid:0) δ(cid:1) n1/22α n+1(cid:3)2 will safely be bounded
2 3 2 3
for large enough n. Similarly, due to the choice of α , we have
(cid:80)∞
(α )2 = O(n−1),
i i=n+1 i
meaning
n−a(cid:80)∞
(α )2 will be bounded for large enough n as long as a ≥ −1. Hence
i=n+1 i
−2log(cid:0) δ(cid:1) n(cid:80)∞
α2 will also be bounded for large enough n.
2 i=n+1 i
Consequently, for our choice of δ, there exists a finite K > 0 and a finite N > 0 such that:
(cid:18)(cid:12) (cid:12) (cid:19)
(cid:12)P(∞)(x)−P(n)(x)(cid:12)
supP (cid:12) (cid:12) ≤ K ≥ 1−δ ∀n > N. (46)
(cid:12) n−1/2 (cid:12)
x∈X
28D.2 Theorem 3.3
We prove the statement for general densities, which then naturally extends to predictive
densities. The proof of this result is largely an adaptation of the simplified vine copula
convergence result (Theorem 1 in Nagler and Czado [2016]) but where no rate on marginal
densities is required. As such, our proof shares an identical approach until the last part,
where we deviate. We have a weaker result for the convergence of distributions and yet
show in what follows that convergence of our copula estimator can be obtained even without
convergence guarantees on marginal densities.
Notation used in the proof We follow vine copula notation from Appendix A.3 and use
superscripts with parenthesis to now differentiate between samples instead of predictive
steps, following notational conventions of the literature. We define h-functions as the
conditional distribution functions for pair copulas
(cid:90) u
h (u | v) := c (s,v)ds, for (u,v) ∈ [0,1]2. (47)
j e|ℓ e;D e′ j e,ℓ e;D e′
0
Further, we refer to the true unobserved samples of pair-copulas as
(cid:16) (cid:17) (cid:16) (cid:17)
(i) (i) (i) (i) (i) (i)
U := F | D X | X , U := F X | X , (48)
j e|D e je e j e D e k e|D e k e|D e k e D e
for i = 1,...,n. We also denote estimators and quantities obtained by application to these
unobserved samples with a bar superscript, for example:
(cid:16) (cid:17)
(1) (n)
c¯ (u,v) := c¯ u,v,U ,...,U . (49)
j e,k e;D e j e,k e;D e j e|D e k e|D e
Finally, denote with a hat superscript all quantities and estimators obtained by using
Uˆ(i)
:=
l
Fˆ (X(i) ) instead of the true unobserved samples used in Equation (48).
l l
Assumptions For completeness, we state assumptions about the marginal distribution
estimator Pˆ as well as bivariate copula estimators cˆ, even though our practical choices
from the main text respect these. We begin by stating the assumption about our marginal
distribution estimator denoted Pˆ:
• A1: The marginal distribution function estimator has the following convergence rate:
(cid:16) (cid:17)
(cid:12) (cid:12)
sup(cid:12)Pˆ (x)−P(x)(cid:12) = O
p
n−1/2 . (50)
x∈X
Next, we state our assumptions about the pair copula estimator for completeness:
• A2: For all e ∈ E ,m = 1,...,d − 1, with −r the convergence rate of a bivariate
m
KDE copula estimator, it holds:
(a) for all (u,v) ∈ (0,1)2,
(cid:0) (cid:1)
c¯ (u,v)−c (u,v) = O n−r , (51)
j ,k ;D j ,k ;D p
e e e e e e
29(b) for every δ ∈ (0,0.5],
sup
(cid:12) (cid:12)h¯
j |k ;D (u | v)−h j |k ;D (u |
v)(cid:12)
(cid:12) = o a.s.
(cid:0) n−r(cid:1)
,
e e e e e e
(u,v)∈[δ,1−δ]2
(52)
sup
(cid:12) (cid:12)h¯
k |j ;D (u | v)−h k |j ;D (u |
v)(cid:12)
(cid:12) = o a.s.
(cid:0) n−r(cid:1)
.
e e e e e e
(u,v)∈[δ,1−δ]2
• A3: For all e ∈ E ,m = 1,...,d−1, it holds:
m
(a) for all (u,v) ∈ (0,1)2,
c (u,v)−c¯ (u,v) = O (a ), (53)
(cid:98)j ,k ;D j ,k ;D p e,n
e e e e e e
(b) for every δ ∈ (0,0.5],
(cid:12) (cid:12)
sup (cid:12) (cid:12)(cid:98)h
j |k ;D
(u | v)−h¯
j |k ;D
(u | v)(cid:12)
(cid:12)
= O
a.s.
(a e,n),
e e e e e e
(u,v)∈[δ,1−δ]2
(54)
(cid:12) (cid:12)
sup (cid:12) (cid:12)(cid:98)h
k |j ;D
(u | v)−h¯
k |j ;D
(u | v)(cid:12)
(cid:12)
= O
a.s.
(a e,n),
e e e e j e
(u,v)∈[δ,1−δ]2
where
(cid:12) (cid:12) (cid:12) (cid:12)
a
e,n
:= sup (cid:12) (cid:12)U(cid:98) j(i |)
D
−U j(i |)
D
(cid:12) (cid:12)+(cid:12) (cid:12)U(cid:98) k(i)
|D
−U k(i)
|D
(cid:12) (cid:12). (55)
i=1,...,n e e e e e e e e
• A4: For all e ∈ E ,m = 1,...,d−1, the pair copula densities c are continu-
m j ,k ;D
e e e
ously differentiable on (0,1)2.
We note that A1 is satisfied by our marginal predictive estimator, as proved in D.1 while A2,
A3, A4 are all satisfied by the KDE pair copula estimator, as shown in Nagler and Czado
[2016].
Proofstrategy Weperformtheproofinthreeparts. Toobtainthefinalresult,wefirstprove
the convergence of pseudo observations to true observations through induction. We then
rely on the aforementioned convergence to show that feasible pair-copula density estimators
cˆ , and conditional distribution function estimators Fˆ and Fˆ are pointwise
j e,k e;D e j e|D e k e|D e
consistent. Lastly, these two results are combined to obtain the convergence of the joint
copula estimator cˆ to the true multivariate copula c.
Part 1: Convergence of pseudo observations we start by proving a convergence rate of
samplesonthecopulaspaceobtainedthroughmarginaldistributionstotheirtrueunobserved
equivalent. That is, ∀ e ∈ E ,...,E ,i = 1,...,n,
1 d−1
U(cid:98)
j(i |)
D
−U
j(i |)
D
= O
p(cid:0) n−r(cid:1)
, U(cid:98)
k(i)
|D
−U
k(i)
|D
= O
p(cid:0) n−r(cid:1)
. (56)
e e e e e e e e
Starting with e ∈ E (the conditioning set D is empty), as a consequence of A1 we obtain
1 e
the bound
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)U(cid:98) j( ei) −U j( ei)(cid:12)
(cid:12)
= (cid:12) (cid:12)F(cid:98)(X
j
e)−F (X
j
e)(cid:12)
(cid:12)
≤ sup (cid:12) (cid:12)F(cid:98)(x
j
e)−F (x
j
e)(cid:12)
(cid:12)
= O p(cid:0) n−r(cid:1) . (57)
x ∈Ω
je Xje
30To obtain the second part of (56) one can use identical arguments, providing the initial
inductive hook. Next, assuming (56) holds for all e ∈ E with 1 ≤ m ≤ d−2, we extend the
m
induction to e ∈ E . Recalling that pseudo-observations e′ ∈ E are equal to Uˆ(i)
m+1 m+1 j |D ∪k
e e e
or
Uˆ(i)
for some e ∈ E , it follows by multiple triangle inequalities that
k |D ∪j m
e e e
(cid:12) ˆ(i) (i) (cid:12) (cid:12) ˆ (cid:8)ˆ(i) ˆ(i) (cid:9) (cid:8) (i) (i) (cid:1) (cid:12)
(cid:12)U −U (cid:12) = (cid:12)h U |U −h U |U }(cid:12)
j e|D e∪k e j e|D e∪k e j e|k e;D e j e|D e k e|D e j e|k e;D e j e|D e k e|D e
(cid:12) ˆ (cid:8)ˆ(i) ˆ(i) (cid:9) (cid:8)ˆ(i) ˆ(i) (cid:9)(cid:12)
≤ (cid:12)h U |U −h U |U (cid:12)
j e|k e;D e j e|D e k e|D e j e|k e;D e j e|D e k e|D e
(cid:12) (cid:8)ˆ(i) ˆ(i) (cid:9) (cid:8)ˆ(i) ˆ(i) (cid:9)(cid:12)
+ (cid:12)h U |U −h U |U (cid:12)
j e|k e;D e j e|D e k e|D e j e|k e;D e j e|D e k e|D e
(cid:12) (cid:8)ˆ(i) ˆ(i) (cid:9) (cid:8) (i) (i) (cid:9)(cid:12)
+ (cid:12)h U |U −h U |U (cid:12)
j e|k e;D e j e|D e k e|D e j e|k e;D e j e|D e k e|D e
= H +H +H
1,n 2,n 3,n
(cid:8) (i) (i) (i) (i) (cid:9)
Notice that for δ := min U ,U ,1 − U ,1 − U > 0, we have that all re-
i j |D k |D j |D k |D
e e e e e e e e
alisations (U(i) ,U(i) ) are contained within [δ ,1 − δ ]2 almost surely. Similarly, all
j |D k |D i i
e e e e
realisations (Uˆ(i) ,Uˆ(i) ) are in [δ /2,1−δ /2]2 for sufficiently large n as a consequence
j |D k |D i i
e e e e
of (56). Combining this with A2 (b) and A3 (b), with large enough n:
(cid:12) (cid:12)
ˆ
H
1,n
≤ sup (cid:12)h
j |k ;D
(u|v)−h
j |k ;D
(u|v)(cid:12) = O p(a e,n),
e e e e e e
(u,v)∈[δ /2,1−δ /2]2
i i
(cid:12) (cid:12)
H
2,n
≤ sup (cid:12)h
j |k ;D
(u|v)−h
j |k ;D
(u|v)(cid:12) = O p(n−r),
e e e e e e
(u,v)∈[δ /2,1−δ /2]2
i i
and by another application of (56),
a e,n = sup |Uˆ j(i |) D −U j(i |) D (cid:12) (cid:12)+(cid:12) (cid:12)Uˆ k(i) |D −U k(i) |D (cid:12) (cid:12) = O p(n−r),
i=1,...,n e e e e e e e e
giving H = O (n−r). To complete part 1, we want to show that H = O (n−r). We
1,n p 3,n p
write the gradient of h as ∇h and use a first-order Taylor approximation of
j |k ;D j |k ;D
e e e e e e
h
(cid:0) Uˆ(i) |Uˆ(i) (cid:1)
around
(cid:0) U(i) ,U(i) (cid:1)
to get
j e|k e;D e j e|D e k e|D e j e|D e k e|D e
(cid:32) (cid:33) (cid:32) (cid:33)
(cid:12) ˆ(i) (i) (cid:12) ˆ(i) (i)
U −U U −U
H 3,n ≤ (cid:12) (cid:12) (cid:12)∇⊤h j e|k e;D e(cid:0) U j( ei |) D e|U k( ei) |D e(cid:1) Uˆj (e i)|D e −Uj (e i| )D e (cid:12) (cid:12) (cid:12)+o a.s. Uˆj (e i)|D e −Uj (e i| )D e
k |D k |D k |D k |D
e e e e e e e e
getting the desired result,. and hence the first equality of (56) by yet another application of
(56). The second equation follows by identical steps, completing the induction.
Part 2: consistency of conditional CDF and pair-copula density estimators Following
similar steps to as in Part 1, one can obtain that for all e ∈ E ,...,E , and all x ∈ Ω , the
1 d−1 X
CDF estimators are bounded as
Fˆ (cid:0) x |x (cid:1) −F (cid:0) x |x (cid:1) = O (n−r),
j e|D e j e D e j e|D e j e D e p (58)
Fˆ (cid:0) x |x (cid:1) −F (cid:0) x |x (cid:1) = O (n−r).
k e|D e k e D e k e|D e k e D e p
31To bound pair-copula density estimators, we apply the triangle inequality to obtain
(cid:12) (cid:0) (cid:1) (cid:0) (cid:1)(cid:12)
(cid:12)cˆ u,v −c u,v (cid:12) (59)
j ,k ;D j ,k ;D
e e e e e e
(cid:12) (cid:0) (cid:1) (cid:0) (cid:1)(cid:12) (cid:12) (cid:0) (cid:1) (cid:0) (cid:1)(cid:12)
≤ (cid:12)cˆ u,v −c u,v (cid:12)+(cid:12)c u,v −c u,v (cid:12) (60)
j ,k ;D j ,k ;D j ,k ;D j ,k ;D
e e e e e e e e e e e e
= R +R . (61)
n,1 n,2
Assumption A3 (a) coupled with (56) bounds R while R is bounded byA2 (a),
n,1 n,2
completing the second part.
Part 3: Consistency of the vine copula estimator Up to now, our steps have mirrored
those of Nagler and Czado [2016]. With the following we differentiate ourselves by noticing
that to get a bound on the copula estimator alone, no marginal densities are required:
d−1
cˆ(x) = (cid:89) (cid:89) cˆ (cid:8) Fˆ (x |x ), Fˆ (x |x )(cid:9)
j e,k e;D e j e|D e j e D e k e|D e k e D e
k=1e∈E
k
d−1 (cid:20) (cid:21)
= (cid:89) (cid:89) c (cid:8) Fˆ (x |x ), Fˆ (x |x )(cid:9) +O (n−r)
j e,k e;D e j e|D e j e D e k e|D e k e D e p
k=1e∈E
k
d−1 (cid:20) (cid:21)
(cid:89) (cid:89) (cid:8) (cid:9)
= c F (x |x ), F (x |x ) +O (n−r)+O (n−r)
j e,k e;D e j e|D e j e D e k e|D e k e D e p p
k=1e∈E
k
= c(x)+O (n−r).
p
where the first line is a consequence of pair-copula estimator convergence in Part 2, and
the second equality is a consequence of (58) and the fact that c is continuously
j ,k ;D
e e e
differentiable. This concludes the proof.
E Experiments and practical details
Details of the UCI datasets are discussed in Ghalebikesabi et al. [2022]. In experiments, We
use the implementation of vine copulas from Schepsmeier et al. [2015] through a Python
interface. We follow the data pre-processing of Ghalebikesabi et al. [2022] to make results
comparable.
Hyperparameter search In our experiments on small UCI datasets, we use a grid search
over 50 values from 0.1 to 0.99 to select ρ, independently across dimensions, selecting
possibly different values for each. To select the KDE pair copula bandwidth we use a
10-fold cross-validation to evaluate the energy score for 50 values between 2 and 4, as these
ranges were appraised to give the best fits on preliminary runs on train data. We note the
hyperparameter selection of ρ also supports gradient-based optimisation (see Appendix C).
For energy score evaluations, with marginal predictives we sample 100 observations and
32Table 3: Average LPS (in bpd, lower is better) over five runs with standard errors for the
Digits dataset.
Model DIGITS
n/d 1797/64
MAF −8.76
±0.10
RQ-NSF −6.17
±0.13
R-BP −8.80
±0.00
R -BP −7.46
d ±0.12
AR-BP −8.66
±0.03
AR -BP −7.46
d ±0.18
ARnet-BP −7.72
±0.28
QB-Vine(30) −6.56
±0.13
QB-Vine(50) −8.19
±0.14
QB-Vine(100) −8.54
±0.17
QB-Vine(200) −9.03
±0.17
QB-Vine(300) −10.24
±0.14
QB-Vine(400) −10.43
±0.13
QB-Vine(500) −10.75
±0.03
QB-Vine(full) −10.11
±0.19
compare them to the training data, while for the copula we simulate 100 samples from the
joint to compare with the energy score against training data.
For the PRticle filter we took an initial sample size of d·n to accommodate for different
dimensions while not being overcome by computational burden. The Kernel used is a
standard multivariate Gaussian kernel.
Compute We ran all experiments on an Intel(R) Core(TM) i7-9700 Processor. In total our
experiments for the QB-Vine took a combined 15 hours with parallelisation across 8 cores,
or 120 hours on a single core. The Digits dataset on 8 cores took us 6 hours to run with
5 different train and test splits. Other datasets require about half an hour for five runs in
parallel, while the Gaussian Mixture study had a total time of 4 hours. The PRticle Filter
takes about two hours on all density estimation tasks combined. The RQ-NSF experiments
on Gaussian Mixture Models took about 4 hours combined. Our total compute time is
therefore the equivalent of 126 hours on a single core. Our implementation of the QB-Vine
is not fully efficient so the computational times are rough upper bounds.
Selection of P(0) in practice In practice, the initial choice of P(0) is made to reflect the
support and spread of the data. As we standardize our data to be mean 0 and have standard
deviation 1, a natural choice is the standard Gaussian N(0,1). However, given distribution
transformations are used throughout the recursion, if observations fall in the tails of the
predictivedensity,numericaloverflowmightmakethemredundant,loweringtheaccuracyof
ourapproach. Therefore,itisdesirabletohaveheaviertailsthanthoseofthetruedistribution
to capture outliers accurately. This coincides with the theory on such recursion requiring
heavier tails for the initial predictive compared to those of the data, see the assumptions on
P(0) in Tokdar et al. [2009], Martin [2021] and Hahn et al. [2018], Fong et al. [2023]. As
such, our default choice is a standard Cauchy distribution.
Forexperiments,wetestedNormal,Cauchy,andUniform(overtherangeofthetrainingsam-
ples plus a margin) initial guesses. Generally, the Cauchy distribution is a well-performing
choice and obtained the best NLL in all but two experiments. We give a summary of initial
33density p choices for different experiments in Table 4 for density estimation and Table 5 for
0
regression and classification.
Dataset WINE BREAST PARKIN IONO BOSTON
Choice of p Cauchy Cauchy Cauchy Normal Cauchy
0
Table 4: Choice of p for different density estimation experiments.
0
Dataset BOSTON(reg) CONCR(reg) DIAB(reg) IONO(class) PARKIN(class)
Choiceofp Normal Cauchy Cauchy Cauchy Cauchy
0
Table 5: Choice of p for different regression and classification experiments.
0
Marginal Sampling Here we briefly introduce the basic idea of the inverse sampling
method for marginal predictive densities via linear interpolation. In general, suppose Z ∼ F
withthecorrespondingcumulativedistributionfunctionF,wedenoteF−1 asthegeneralized
inverse function of F, given u ∼ U[0,1], then we can compute z = F−1(u) as a sample from
F.
Suppose we have access to the cdf of a univariate random variable, meaning that we can
evaluate the cdf F value given any location on the axis, and we have a set of samples
z i. ∼i.d. F. Our goal is to approximate the generalized inverse F−1 cdf then we can sample
1:n
the realisations of this univariate random variable as many as we want. Additionally, we
assume that the random variable is bounded and the closed support is R = [min(z ) −
1:n
η,max(z ) + η] where η is a positive hyperparameter we manually tune as the range of
1:n
extrapolation.
The idea is to construct a context set which contains (K +2) pairs of evaluation value on
the axis and its associated cdf value and then we can use this set to fit a linear interpolator to
approximate the inverse cdf of our desired distribution. More precisely, we can use a fixed
grid of K points, y¯ = {y¯ ,...,y¯ } where y¯ < ... < y¯ , within R and compute c¯ = F(y¯)
K 1 K 1 K i i
for i = 1,...,K. Additionally, we set
y¯ = min(z )−η, c¯ = 0,
0 1:n 0
y¯ = max(z )+η, c¯ = 1.
K+1 1:n K+1
Then, we obtain a context set C(R) = {(y¯,c¯)}K+1. Since the cdf is strictly non-decreasing
i i i=0
and bounded within [0,1], then we have that
c¯ ≤ c¯ ≤ ... ≤ c¯ ≤ c¯ .
0 1 K K+1
Next, we propose to fit the approximation of the generalized inverse cdf F(cid:98)−1 via the linear
interpolation using the context set C(R) we constructed. Mathematically, given a value
c¯∈ (0,1) we want to evaluate, then
K
(cid:88) y¯ (c¯ −c¯)+y¯ (c¯−c¯ )
y¯= F(cid:98)−1(c¯) = j j+1 j+1 j ·δ ∼ F.
{c¯ <c¯≤c¯ }
c¯ −c¯ j j+1
j+1 j
j=0
34By consequence, we can easily obtain the gradient of y¯w.r.t. c¯as
K
dy¯ (cid:88) y¯ −y¯
j+1 j
= ·δ .
{c¯ <c¯≤c¯ }
dc¯ c¯ −c¯ j j+1
j+1 j
j=0
(n)
Since we can evaluate the cdf values via P for j = 1,...,d, then we can use the linear
j
(1:n)
interpolation method we proposed. For ∀j ∈ 1,...,d, given the set of observations x , we
j
define the support for the jth marginal as
(cid:104) (cid:105)
(1:n) (1:n)
R = min(x )−η,max(x )+η .
j j j
Following a similar procedure, we can construct a context set CK(R ) with a grid of K
j
(n)
points via P . Next, we can fit a linear interpolator denoted as LI (·) using the context
j j
set CK(R ). Then,
j
(n)
LI (ϵ) ∼ P , where ϵ ∼ U[0,1].
j j
E.1 Comparison to normalising flow on Gaussian Mixture Model
We assess the performance of the QB-Vine on a mixture of 4 non-isotropic Gaussians across
a range of dimensions and sample sizes. We simulate n d-dimensional data points from
4
(cid:88)
p(y) = π ·ϕ(y;µ ,Σ ),
k k k
k=1
where (π ,π ,π ,π ) = (0.2,0.3,0.1,0.4) and
1 2 3 4
µ i. ∼i.d. U[−50,50]d, Σ i. ∼i.d. Wishart(d,I ).
k k d
We compare the QB-Vine with the RQ-NSF as a benchmark off-the-shelf estimator. The
hyperparameters for the RQ-NSF were chosen to give the best performance on train data,
to be 100000 epochs, 0.0001 learning rate, 1 flow step, 8 bins, 2 blocks, and 0.2 dropout
probability in common. For the number of hidden features, we set 16 for d = 10, 32 for
d = 30, 64 for d = 50, and 128 for d = 100. Our results in Table 6 show that the QB-Vine
consistently outperforms the RQ-NSF for the dimensions and sample sizes considered.
35Table 6: Comparison of LPS for QB-Vine (our method) and RQ-NSF on GMM with 4
clustersforchangingnandd. ResultsforourQB-Vinemethodareshownasthetopnumbers
of each row, and RQ-NSF values as the bottom numbers of each row.
d \ n 50 100 300 500 103
3.98 1.73 2.15 0.94 2.43
10 ±0.23 ±0.29 ±0.06 ±0.31 ±0.17
36.47 17.14 12.82 7.10 7.91
±4.87 ±1.51 ±0.36 ±0.26 ±0.11
- 17.94 11.04 12.87 9.85
30 ±1.06 ±0.35 ±0.17 ±0.40
- 91.09 50.51 48.50 34.98
±7.54 ±2.20 ±0.73 ±0.31
- - 38.59 25.82 26.14
50 ±4.31 ±0.06 ±0.01
- - 115.64 112.16 71.43
±3.06 ±2.05 ±1.65
- - - - 78.20
100 ±0.23
- - - - 268.88
±1.37
36