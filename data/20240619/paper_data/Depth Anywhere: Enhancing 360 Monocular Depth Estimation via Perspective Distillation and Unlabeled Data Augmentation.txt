Depth Anywhere: Enhancing 360 Monocular Depth
Estimation via Perspective Distillation and Unlabeled
Data Augmentation
Ning-HsuWang Yu-LunLiu
albert.nhwang@gmail.com DepartmentofComputerScience
NationalYangMingChiaoTungUniversity
yulunliu@cs.nycu.edu.tw
Abstract
Accuratelyestimatingdepthin360-degreeimageryiscrucialforvirtualreality,au-
tonomousnavigation,andimmersivemediaapplications. Existingdepthestimation
methodsdesignedforperspective-viewimageryfailwhenappliedto360-degree
imagesduetodifferentcameraprojectionsanddistortions,whereas360-degree
methodsperforminferiorduetothelackoflabeleddatapairs. Weproposeanew
depth estimation framework that utilizes unlabeled 360-degree data effectively.
Ourapproachusesstate-of-the-artperspectivedepthestimationmodelsasteacher
models to generate pseudo labels through a six-face cube projection technique,
enablingefficientlabelingofdepthin360-degreeimages. Thismethodleverages
theincreasingavailabilityoflargedatasets.Ourapproachincludestwomainstages:
offline mask generation for invalid regions and an online semi-supervised joint
trainingregime. WetestedourapproachonbenchmarkdatasetssuchasMatter-
port3DandStanford2D3D,showingsignificantimprovementsindepthestimation
accuracy,particularlyinzero-shotscenarios. Ourproposedtrainingpipelinecan
enhanceany360monoculardepthestimatoranddemonstrateseffectiveknowledge
transferacrossdifferentcameraprojectionsanddatatypes. Seeourprojectpage
forresults: albert100121.github.io/Depth-Anywhere.
1 Introduction
In recent years, the field of computer vision has seen a surge in research focused on addressing
thechallengesassociatedwithprocessing360-degreeimages. Thewidespreaduseofpanoramic
imagery across various domains, such as virtual reality, autonomous navigation, and immersive
media,hasunderscoredtheneedforaccuratedepthestimationtechniquestailoredspecificallyfor
360-degreeimages. However,existingdepthestimationmethodsdevelopedforperspective-view
imagesencountersignificantdifficultieswhenapplieddirectlyto360-degreedataduetodifferences
incameraprojectionanddistortion. Whilemanymethodsaimtoaddressdepthestimationforthis
cameraprojection,theyoftenstruggleduetothelimitedavailabilityoflabeleddatasets.
Toovercomethesechallenges,thispaperpresentsanovelapproachfortrainingstate-of-the-art(SOTA)
depthestimationmodelson360-degreeimagery. Withtherecentsignificantincreaseintheamountof
availabledata,theimportanceofbothdataquantityandqualityhasbecomeevident. Researchefforts
onperspectiveperceptualmodelshaveincreasinglyfocusedonaugmentingthevolumeofdataand
developingfoundationmodelsthatgeneralizeacrossvarioustypesofdata. Ourmethodleverages
SOTA perspective depth estimation foundation models as teacher models and generates pseudo
labelsforunlabeled360-degreeimagesusingasix-facecubeprojectionapproach. Bydoingso,we
efficientlyaddressthechallengeoflabelingdepthin360-degreeimagerybyleveragingperspective
modelsandlargeamountsofunlabeleddata.
4202
nuJ
81
]VC.sc[
1v94821.6042:viXraError Map Error Map
Ground Truth BiFuse++ BiFuse++ (p)
Figure1: Ourproposedtrainingpipelineimprovesexisting360monoculardepthestimators.
This figure demonstrated the improvement of our proposed training pipeline tested on the Stan-
ford2D3D[2]datasetinazero-shotsetting.
Ourapproachconsistsoftwokeystages: offlinemaskgenerationandonlinejointtraining. During
theofflinestage,weemployacombinationofdetectionandsegmentationmodelstogeneratemasks
forinvalidregionssuchasskyandwatermarksinunlabeleddata. Subsequently,intheonlinestage,
weadoptasemi-supervisedlearningstrategy,loadinghalfofthebatchwithlabeleddataandtheother
halfwithpseudo-labeleddata. Throughjointtrainingwithbothlabeledandpseudo-labeleddata,our
methodachievesrobustdepthestimationperformanceon360-degreeimagery.
To validate the effectiveness of our approach, we conduct extensive experiments on benchmark
datasetssuchasMatterport3DandStanford2D3D.Ourmethoddemonstratessignificantimprovements
indepthestimationaccuracy,particularlyinzero-shotscenarioswheremodelsaretrainedonone
datasetandevaluatedonanother. Furthermore,wedemonstratetheefficacyofourtrainingtechniques
with different SOTA 360-degree depth models and various unlabeled datasets, showcasing the
versatilityandeffectivenessofourapproachinaddressingtheuniquechallengesposedby360-degree
imagery.
Ourcontributionscanbesummarizedasfollows:
• Weproposeanoveltrainingtechniquefor360-degreeimagerythatharnessesthepowerof
unlabeleddatathroughthedistillationofperspectivefoundationmodels.
• We introduce an online data augmentation method that effectively bridges knowledge
distillationacrossdifferentcameraprojections.
• Ourproposedtrainingtechniquessignificantlybenefitandinspirefutureresearchon360-
degreeimagerybyshowcasingtheinterchangeabilityofstate-of-the-art(SOTA)360models,
perspectiveteachermodels,andunlabeleddatasets. Thisenablesbetterresultsevenasnew
SOTAtechniquesemergeinthefuture.
2 RelatedWork
360monoculardepth. Depthestimationfor360-degreeimagespresentsuniquechallengesdue
totheequirectangularprojectionandinherentdistortion. Variousapproacheshavebeenexploredto
addresstheseissues:
• DirectlyApply: Somemethodsdirectlyapplymonoculardepthestimationtechniquesto
360-degreeimagery. OmniDepth[55]leveragessphericalgeometryandincorporatesSph-
Conv[31]toimprovedepthpredictionwithdistortion. [56,37]usesphericalcoordinates
toovercomedistortionwithextrainformation. SliceNet[21]andACDNet[54]propose
advancednetworkarchitecturestailoredforomnidirectionalimages. EGFormer[50]intro-
ducesatransformer-basedmodelthatcapturesglobalcontextefficiently,while [33,32]
focusesonintegratinggeometricpriorsintothelearningprocess.
2
BGR
htpeD• Cube: Other approaches use cube map projections to mitigate distortion effects. 360-
SelfNet [34] is the first work to self-supervised 360 depth estimation leveraging cube-
padding[7]. BiFuse[35]anditsimprovedversionBiFuse++[36]aretwo-brancharchi-
tecturesthatutilizecubemapsandequirectangularprojections. UniFuse[11]combines
equirectangularandcubemapprojectionsandsimplifiesthearchitecture.
• Tangent Image: Tangent image projections are also popular. 360MonoDepth [26] and
OmniFusion[16]convertequirectangularimagesintoaseriesoftangentimages,whichare
thenprocessedusingconventionaldepthestimationnetworks. PanoFormer[29]employsa
transformer-basedarchitecturetohandletangentimages,whileSphereNet[9]andHRD-
Fuse[1]enhancedepthpredictionbycollaborativelylearningfrommultipleprojections.
Unlabeled / Pseudo labeled data. Utilizing unlabeled or pseudo-labeled data has become a
significanttrendtomitigatethelimitationsoflabeleddatascarcity. Techniqueslike [15,57,30,44]
leveragelargeamountsofunlabeleddatatoimprovemodelperformancethroughsemi-supervised
learning. Inthecontextof360-degreedepthestimation,ourapproachgeneratespseudolabelsfrom
pre-trainedperspectivemodels,whicharethenusedtotrain360-degreedepthmodelseffectively.
Zero-shotmethods. Zero-shotlearningmethodsaimtogeneralizetonewdomainswithoutad-
ditionaltrainingdata. [6,42]targetthisdirectlywithincreasingtrainingdata.,MiDaS[24,4,23]
and Depth Anything [45] are notable for their robust monocular depth estimation across diverse
datasetsleveragingaffine-invariantloss. [47]takesastepfurthertoinvestigatezero-shotonmetric
depth. Marigold[12]leveragesdiffusionmodelswithimageconditioningandup-to-scalerelative
depth denoising to generate detailed depth maps. ZoeDepth [3] further these advancements by
incorporatingscaleawarenessanddomainadaptation. [10,38]leveragecameramodelinformation
toadaptcross-domaindepthestimation.
Foundationmodels. FoundationmodelshaverevolutionizedvariousfieldsinAI,includingnat-
ural language processing and image-text alignment. In computer vision, models like CLIP [22]
demonstrateexceptionalgeneralizationcapabilities. [19]proposedafoundationvisualencoderfor
downstreamtaskssuchassegmentation,detection,depthestimation,etc. [13]proposedamodelthat
cancutoutmasksforanyobjects. Ourworkleveragesapre-trainedperspectivedepthestimation
foundationmodel[45]asateachermodeltogeneratepseudolabelsfor360-degreeimages,enhancing
depthestimationbyutilizingthevastknowledgeembeddedinthesefoundationmodels.
3 Methods
In this work, we propose a novel training approach for 360-degree monocular depth estimation
models. Our method leverages a perspective depth estimation model as a teacher and generates
pseudolabelsforunlabeled360-degreeimagesusinga6-facecubeprojection. Figure2illustrates
ourtrainingpipeline,incorporatingtheuseofSegmentAnythingtomaskoutskyandwatermark
regionsinunlabeleddataduringtheofflinestage. Subsequently,weconductjointtrainingusingboth
labeledandunlabeleddata,allocatinghalfofthebatchtoeach. Theunlabeleddataissupervised
usingpseudolabelsgeneratedbyDepthAnything,astate-of-the-artperspectivemonoculardepth
foundationmodel. Withthebenefitofourteachermodel,the360-degreedepthmodeldemonstrates
anobservableimprovementonzero-shotdataset1.
3.1 UnleashingthePowerofUnlabel360data
Datasetstatistics. 360-degreedatahasbecomeincreasinglyavailableinrecentyears. However,
compared to perspective-view depth datasets, labeling depth ground truths for 360-degree data
presentsgreaterchallenges. Consequently,theavailabilityoflabeleddatasetsfor360-degreedatais
considerablysmallerthanthatofperspectivedatasets.
Table 1 presents the data quantities available in some of the most popular 360-degree datasets,
including Matterport3D [5], Stanford2D3D [2], and Structured3D [51]. Additionally, we list a
multi-modaldataset,SpatialAudioGen[20],whichconsistsofunlabeled360-degreedatausedinour
experiments. Notably,theamountoflabeledandunlabeleddatausedintheperspectivefoundation
model,DepthAnything[45],issignificantlylarger,with1.5millionlabeledimages[17,40,8,46,
43, 39] and 62 million unlabeled images [27, 49, 41, 48, 28, 14, 52, 13], making the amount in
360-degreedatasetsapproximately170timessmaller.
3360 model Losson GT
Labeled data Depth prediction Ground truth
Weight sharing (a) Supervised training with labeled data
360 model
Equi. to cube
Unlabeled data
Same randomrotation
Depth
SAM
Equi. to cube Anything
Same random
rotation Pseudo GT
Equi. to cube Losson pseudo GT
Valid pixel mask with valid masks Valid masks
(b) Distillation with unlabeled data (c) Random rotation
Figure2: TrainingPipeline. Ourproposedtrainingpipelineinvolvesjointtrainingonbothlabeled
360 data with ground truth and unlabeled 360 data. (a) For labeled data, we train our 360 depth
modelwiththelossbetweendepthpredictionandgroundtruth. (b)Forunlabeleddata,wepropose
todistillknowledgefromapre-trainedperspective-viewmonoculardepthestimator. Inthispaper,
weuseDepthAnything[45]togeneratepseudogroundtruthfortraining. However,moreadvanced
techniquescouldbeapplied. Theseperspective-viewmonoculardepthestimatorsfailtoproduce
reasonableequirectangulardepthasthereexistsadomaingap. Therefore,wedistillknowledgeby
inferringsixperspectivecubefacesandpassingthemthroughperspective-viewmonoculardepth
estimators. Toensurestableandeffectivetraining,weproposegeneratingavalidpixelmaskwith
SegmentAnything[13]whilecalculatingloss. (c)Furthermore,weaugmentrandomrotationon
RGBbeforepassingitintoDepthAnything,aswellasonpredictionsfromthe360depthmodel.
Table1: 360monoculardepthestimationlacksalargeamountoftrainingdata. Thenumberof
imagesin360-degreemonoculardepthestimationdatasetsalongsideperspectivedepthdatasetsfrom
theDepthAnythingmethodology.
Perspective Equirectangular
Labeled 1.5M Unlabeled 62M Labeled 34K Unlabeled 344K
Datacleaningandvalidpixelmaskgeneration Unlabeleddataoftencontainsinvalidpixelsin
regionssuchastheskyandwatermark,leadingtounstabletrainingorundesiredconvergence. To
addressthisissue,weappliedtheGroundingSAM[25]methodtomaskouttheinvalidregions. This
approachutilizesGroundedDINOv2[18]todetectproblematicregionsandappliestheSegment
Anything[13]modeltomaskouttheinvalidpixelsbysegmentingwithintheboundingbox. While
DepthAnything[45]alsoemploysapre-trainedsegmentationmodel,DINOv2,toselectskyregions.
Brandlogosandwatermarksfrequentlyappearafterfisheyecamerastitching. Therefore,additional
labelsareappliedtoenhancetherobustnessofourtrainingprocess. Wealsoremoveallimageswith
lessthan20percentofvalidpixelstostablizeourtrainingprogress.
Perspective foundation models (teacher models). To tackle the challenges posed by limited
dataandlabelingdifficultiesin360-degreedatasets,weleveragealargeamountofunlabeleddata
alongside state-of-the-art perspective depth foundation models. Due to significant differences in
camera projection and distortion, directly applying perspective models to 360-degree data often
yieldsinferiorresults. Previousworkshaveexploredvariousmethodsofprojectionforconverting
equirectangulartoperspectivedepthasstatedinSec2. Amongthese,cubeprojectionandtangent
projectionarethemostcommontechniques. Weselectedcubeprojectiontoensurealargerfieldof
viewforeachpatch,enablingbetterobservationofrelativedistancesbetweenpixelsorobjectsduring
theinferenceoftheperspectivefoundationmodelandenhancingknowledgedistillation.
Inourapproach,weapplyprojectiontounlabeled360-degreedataandthenrunDepthAnythingon
theseprojectedpatchesofperspectiveimagestogeneratepseudo-labels. Weexploretwodirections
forpseudo-labelsupervision:projectingthepatchtoequirectangularandcomputinginthe360-degree
domainorprojectingthe360-degreedepthoutputfromthe360modeltopatchesandcomputingin
4Figure3: ValidPixelMasking. WeusedGrounded-Segment-Anything[25]tomaskoutinvalid
pixelsbasedontwotextprompts: “sky”and“watermark.” Theseregionslackdepthsensorground
truth labels in all previous datasets. Unlike Depth Anything [45], which sets sky regions as 0
disparity,wefollowgroundtruthtrainingtoignoretheseregionsduringtrainingfortworeasons:
(1)segmentationmaymisclassifyandsetotherregionsaszero,leadingtonoisylabeling,and(2)
watermarksarepost-processingregionsthatlackgeometricalmeaning.
theperspectivedomain. Sincetrainingisconductedinanup-to-scalerelativedepthmanner,stitching
thepatchofperspectiveimagesbacktoequirectangularwithanalignedscaleisanadditionalresearch
aspect. Weopttocomputethelossintheperspectivedomain,facilitatingfasterandeasiertraining
withouttheneedforadditionalalignmentoptimization.
3.2 RandomRotationProcessing
DirectlyapplyingDepthAnythingoncube-projectedunlabeleddatadoesnotyieldimprovements
duetoignoranceofcrosscube-facerelation,leadingtocubeartifacts(Figure4). Thisissuearises
fromtheseparateestimationofperspectivecubefaces,wheremonoculardepthisestimatedbased
onsemanticinformation,potentiallylackingacomprehensiveunderstandingoftheentirescene. To
addressthis,weproposearandomrotationpreprocessingstepinfrontoftheperspectivefoundation
model.
AsdepictedinFigure2,therotationisappliedtoequirectangularprojectionRGBimagesusinga
randomrotationmatrix,followedbycubeprojection. Thisresultsinamorediversesetofcubefaces,
capturingrelativedistancesbetweenceilings,walls,windows,andotherobjectsmoreeffectively.
Withtheproposedrandomrotationtechnique,knowledgedistillationbecomesmorecomprehensive
asthepointofviewisnotstatic. Theinferencebytheperspectivefoundationmodelisperformedon
thefly,withparametersfrozenduringthetrainingofthe360model.
Inordertoperformrandomrotation,weapplyarotationmatrixontheequirectangularcoordinates,
notedas(θ,ϕ),androtationmatrixasR.
(θˆ,ϕˆ)=R·(θ,ϕ). (1)
For equirectangular to cube projection, the field-of-view (FoV) of each cube face is equal to 90
degrees;eachfacecanbeconsideredasaperspectivecamerawhosefocallengthisw/2,andallfaces
sharethesamecenterpointintheworldcoordinate. Sincethesixcubefacessharethesamecenter
point,theextrinsicmatrixofeachcameracanbedefinedbyarotationmatrixR . pisthenthepixel
i
onthecubeface
p=K·RT ·q, (2)
i
where,
(cid:34)q (cid:35) (cid:34)sin(θ)·cos(ϕ)(cid:35) (cid:34)w/2 0 w/2(cid:35)
x
q = q = sin(ϕ) ,K = 0 w/2 w/2 , (3)
y
q cosθ·cosϕ 0 0 1
z
where θ and ϕ are longitude and latitude in equirectangular projection and q is the position in
Euclideanspacecoordinates.
5Figure4: CubeArtifact.Asshowninthecenterrowofthefigure,anundesiredcubeartifactappears
whenweapplyjointtrainingwithpseudogroundtruthfromDepthAnything[45]directly. Thisissue
arisesfromindependentrelativedistanceswithineachcubefacecausedbyastaticpointofview.
Ignoringcross-cuberelationshipsresultsinpoorknowledgedistillation. Toaddressthis,asshown
inFigure2(c),werandomlyrotatetheRGBimagebeforeinputtingitintoDepthAnything. This
enablesbetterdistillationofdepthinformationfromvaryingperspectiveswithintheequirectangular
image.
3.3 LossFunction
The training process closely resembles that of MiDaS, Depth Anything, and other cross-dataset
methods. Ourgoalistoprovidedepthestimationforany360-degreeimages. Followingprevious
approachesthattrainedonmultipledatasets,ourtrainingobjectiveistoestimaterelativedepth. The
depthvaluesarefirsttransformedintodisparityspaceusingtheformula1/dandthennormalizedto
therange[0,1]foreachdisparitymap.
Toadapttocross-datasettrainingandpseudogroundtruthsfromthefoundationmodel,weemployed
the affine-invariant loss, consistent with prior cross-dataset methodologies. This loss function
disregardsabsolutescaleandshiftsforeachdomain,allowingforeffectiveadaptationacrossdifferent
datasetsandmodels.
HW
1 (cid:88)
L1= ρ(d∗,d ), (4)
HW i i
i=1
whered∗ andd arethepredictionandgroundtruth,respectively. ρrepresentstheaffine-invariant
i i
meanabsoluteerrorloss:
ρ(d∗,d )=|dˆ∗−dˆ|. (5)
i i i i
Here,dˆ anddˆ∗arethescaledandshiftedversionsofthepredictiond∗andgroundtruthd :
i i i i
d −t(d)
dˆ = i , (6)
i s(d)
wheret(d)ands(d)areusedtoalignthepredictionandgroundtruthtohavezerotranslationand
unitscale:
HW
1 (cid:88)
t(d)=median(d), s(d)= |d −t(d)|. (7)
HW i
i=1
4 Experiments
These notations apply for all tables: M: Matterport3D [5], SF: Stanford2D3D [2], ST: Struc-
tured3D[51],SP:Spatialaudiogen[20],-allindicatesusingtheentiretrain,validation,andtestsets
ofthespecificdataset,and(p)denotesusingpseudogroundtruthgeneratedbyDepthAnything[45].
Duetospacelimits,weprovidetheexperimentalsetupintheappendix,includingimplementation
detailsandevaluationmetrics.
6Table2: Matterport3DBenchmark. Theuppersectionlists360methodstrainedwithmetricdepths
inmetersusingBerHuloss. Allnumbersaresourcedfromtheirrespectivepapers. Thelowersection
includesselectedmethodsretrainedwithrelativedepth(disparity)usingaffine-invariantloss.
Method Loss Train Test AbsRel↓ δ ↑ δ ↑ δ ↑
1 2 3
BiFuse BerHu M M - 0.8452 0.9319 0.9632
UniFise BerHu M M 0.1063 0.8897 0.9623 0.9831
SliceNet BerHu M M - 0.8716 0.9483 0.9716
BiFuse++ BerHu M M - 0.879 0.9517 0.9772
HRDFuse BerHu M M 0.0967 0.9162 0.9669 0.9844
UniFuse Affine-Inv M M 0.10187 0.89311 0.97044 0.98902
UniFuse Affine-Inv M,ST-all(p) M 0.08938 0.91143 0.97512 0.99078
BiFuse++ Affine-Inv M M 0.0938 0.9142 0.97363 0.9894
BiFuse++ Affine-Inv M,ST-all(p) M 0.085 0.917 0.976 0.991
4.1 Baselines
Recentstate-of-the-artmethods[1,50,35,36,11,21,29]haveemerged. WechoseUniFuseand
BiFuse++asourbaselinemodelsforexperiments,asmanyoftheaforementionedmethodsdidnot
releasepre-trainedmodelsorprovidetrainingcodeandimplementationdetails. It’sworthnotingthat
PanoFormer[29]isnotincludedduetoincorrectevaluationcodeandresults,andEGFormer[29]is
notincludedsinceitsexperimentsaremainlyconductedonotherdatasetsandbenchmarks. Both
selectedmodelsarere-implementedwithaffine-invariantlossondisparityforafaircomparisonandto
demonstrateimprovement. WeconductexperimentsontheMatterport3Dbenchmarktodemonstrate
improvementwithinthesamedataset/domainandapplyzero-shotevaluationontheStanford2D3D
testsettodemonstratethegeneralizationabilityofourproposedtrainingtechnique.
4.2 Benchmarksevaluation
Weconductedourin-domainimprovementexperimentonthewidelyused360-degreedepthbench-
mark,Matterport3D[5],toshowcasetheresultsofperspectivefoundationmodeldistillationonthe
twoselectedbaselinemodels,UniFuse[11]andBiFuse++[36]. InTable2,welistthemetricdepth
evaluationresultsfromstate-of-the-artmethodsonthisbenchmark. Subsequently,wepresentthe
re-trainedbaselinemodelsusingaffine-invariantlossondisparitytoensureafaircomparisonwith
theiroriginaldepthmetrictraining. Finally,wedemonstratetheimprovementachievedwithresults
trainedonthelabeledMatterport3DtrainingsetandtheentireStructured3Ddatasetwithpseudo
groundtruth.
4.3 Zero-shotevaluation
Our goal is to estimate depths for all 360-degree images, making zero-shot performance crucial.
Followingpreviousworks[35,11],weadoptedtheirzero-shotcomparisonsetting,wheremodels
trainedontheentireMatterport3D[5]datasetaretestedontheStanford2D3D[2]testset. InTable3,
theuppersectionlistsmethodstrainedwithmetricdepthgroundtruth,withnumberssourcedfrom
their respective papers. The lower section includes models trained with affine-invariant loss on
disparitygroundtruth. AsshowninFigure5,[11,36]demonstrategeneralizationimprovementswith
alowererrorontheStanford2D3Ddataset.
Depth Anything [45] and Marigold [12] are state-of-the-art zero-shot depth models trained with
perspectivedepths. AsshowninTable3,duetothedomaingapanddifferentcameraprojections,
foundationmodelstrainedwithperspectivedepthcannotbedirectlyappliedto360-degreeimages.
We demonstrated the zero-shot improvement on UniFuse [11] and BiFuse++ [36] with models
trainedontheentireMatterport3D[5]datasetwithgroundtruthandtheentireStructured3D[51]or
SpatialAudioGen[20]datasetwithpseudogroundtruthgeneratedusingDepthAnything[45].
AsStructured3Dprovidesgroundtruthlabelsforitsdataset,wealsoevaluateourmodelsonitstest
settoassesshowwelltheyperformwithpseudolabels. Table4showstheimprovementsachieved
ontheStructured3Dtestsetwhenusingmodelstrainedwithpseudolabels. It’sworthnotingthat
evenwhenthe360modelistrainedonpseudolabelsfromSpatialAudioGen,itperformssimilarly
7Table3: Zero-shotEvaluationonStanford2D3D.Weperformzero-shotevaluationswithmodels
trainedonotherdatasets. FollowingBiFuseandUniFuse’ssettings,wetrainthe360modelsonthe
entireMatterport3DdatasetandthentestonStanford3D’stestset.
Method Loss train test AbsRel↓ δ ↑ δ ↑ δ ↑
1 2 3
BiFuse BerHu M-all SF 0.1195 0.8616 - -
UniFuse BerHu M-all SF 0.0944 0.9131 - -
BiFuse++ BerHu M-all SF 0.107 0.914 0.975 0.989
DepthAnything Affine-Inv Pers. SF 0.248 0.635 0.899 0.97
Marigold Affine-Inv Pers. SF 0.195 0.692 0.942 0.982
UniFuse Affine-Inv M-all SF 0.09 0.914 0.976 0.99
UniFuse Affine-Inv M-all,ST-all(p) SF 0.086 0.924 0.977 0.99
UniFuse Affine-Inv M-all,SP-all(p) SF 0.09 0.92 0.978 0.99
BiFuse++ Affine-Inv M-all SF 0.09 0.921 0.976 0.99
BiFuse++ Affine-Inv M-all,ST-all(p) SF 0.082 0.931 0.979 0.991
BiFuse++ Affine-Inv M-all,SP-all(p) SF 0.858 0.926 0.979 0.991
Figure5: Zero-shotqualitativewithUnFuse[11](left2columns)andBiFuse++[36](right2
columns)testedonStanford2D3D.
well. Thisdemonstratesthesuccessofourdistillationtechniqueandthemodel’sabilitytogeneralize
acrossdifferentdatasets.
4.4 Qualtativeresultsinthewild
WedemonstratedthequalitativeresultsinFigure7andFigure6360-degreeimagesthatwereeither
capturedbyusordownloadedfromtheinternet1. Theseexamplesshowcasethezero-shotcapability
ofourmodelwhenappliedtodataoutsidetheaforementioned360-degreedatasets.
1StigNygaard,https://www.flickr.com/photos/stignygaard/49659694937,CCBY2.0DEED
DominicAlves,https://www.flickr.com/photos/dominicspics/28296671029/,CCBY2.0DEED
LucaBiada,https://www.flickr.com/photos/pedroscreamerovsky/6873256488/,CCBY2.0DEED
LucaBiada,https://www.flickr.com/photos/pedroscreamerovsky/6798474782/,CCBY2.0DEED
8
BGR
TG
esUFinU
rorrE
)p(
esUFinU
rorrE
RGB
GT
BiFuse++
Error
BiFuse++
(p)
ErrorTable4: Structured3DTestSet. WedemonstratetheimprovementontheStructured3Dtestset
usingpseudogroundtruthfortraining. Thelowersectionshowsenhancementswithmodelstrained
onpseudogroundtruthfromMatterport3DandSpatialAudioGen,indicatingsimilarimprovements.
ThishighlightsthesuccessfuldistillationofDepthAnything.
Method Loss train test AbsRel↓ δ ↑ δ ↑ δ ↑
1 2 3
UniFuse Affine-Inv M-all ST 0.202 0.759 0.932 0.97
UniFuse Affine-Inv M-all,ST-all(p) ST 0.13 0.887 0.953 0.977
UniFuse Affine-Inv M-all,SP-all(p) ST 0.152 0.864 0.946 0.972
Figure6: Generalizationabilityinthewildwithdepthmapvisualization: Weshowcasezero-shot
qualitativeresultsusingacombinationofimagescapturedbyusandrandomlysourcedfromthe
internet to assess the model’s generalization ability. For privacy reasons, we have obscured the
camera-manintheimages.
Figure7:Generalizationabilityinthewildwithpointcloudvisualization:Weshowcasezero-shot
qualitativeresultsinpointcloudusingacombinationofimagescapturedbyusandrandomlysourced
fromtheinternettoassessthemodel’sgeneralizationability.
5 Conclusion
Ourproposedmethodsignificantlyadvances360-degreemonoculardepthestimationbyleveraging
perspectivemodelsforpseudo-labelgenerationonunlabeleddata. Theuseofcubeprojectionwith
random rotation and affine-invariant loss ensures robust training and improved depth prediction
accuracywhilebridgingthedomaingapbetweenperspectiveandequirectangularprojection. By
effectively addressing the challenges of limited labeled data with cross-domain distillation, our
approach opens new possibilities for accurate depth estimation in 360 imagery. This work lays
the groundwork for future research and applications, offering a promising direction for further
advancementsin360-degreedepthestimation.
Limitations Ourworkfaceslimitationsduetoitsheavyrelianceonthequalityofunlabeleddata
andpseudolabelsfromperspectivefoundationmodels. Theresultsaresignificantlyimpactedbydata
quality(Sec.3.1). Withoutdatacleaning,thetrainingprocessresultedinNaNvalues.
9
BGR
esuFinU
)p(
esuFinUReferences
[1] HaoAi,ZidongCao,Yan-PeiCao,YingShan,andLinWang. Hrdfuse: Monocular360deg
depth estimation by collaboratively learning holistic-with-regional depth distributions. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
13273–13282,2023.
[2] I.Armeni,A.Sax,A.R.Zamir,andS.Savarese. Joint2D-3D-SemanticDataforIndoorScene
Understanding. ArXive-prints,February2017.
[3] ShariqFarooqBhat,ReinerBirkl,DianaWofk,PeterWonka,andMatthiasMüller. Zoedepth:
Zero-shottransferbycombiningrelativeandmetricdepth. arXivpreprintarXiv:2302.12288,
2023.
[4] ReinerBirkl,DianaWofk,andMatthiasMüller. Midasv3.1–amodelzooforrobustmonocular
relativedepthestimation. arXivpreprintarXiv:2307.14460,2023.
[5] AngelChang,AngelaDai,ThomasFunkhouser,MaciejHalber,MatthiasNiessner,Manolis
Savva,ShuranSong,AndyZeng,andYindaZhang. Matterport3d: Learningfromrgb-ddatain
indoorenvironments. InternationalConferenceon3DVision(3DV),2017.
[6] WeifengChen,ZhaoFu,DaweiYang,andJiaDeng. Single-imagedepthperceptioninthewild.
Advancesinneuralinformationprocessingsystems,29,2016.
[7] Hsien-TzuCheng,Chun-HungChao,Jin-DongDong,Hao-KaiWen,Tyng-LuhLiu,andMin
Sun. Cubepaddingforweakly-supervisedsaliencypredictionin360{\deg}videos. arXiv
preprintarXiv:1806.01320,2018.
[8] JaehoonCho,DongboMin,YoungjungKim,andKwanghoonSohn. Diml/cvlrgb-ddataset:
2mrgb-dimagesofnaturalindoorandoutdoorscenes. arXivpreprintarXiv:2110.11590,2021.
[9] BenjaminCoors,AlexandruPaulCondurache,andAndreasGeiger. Spherenet: Learningspher-
icalrepresentationsfordetectionandclassificationinomnidirectionalimages. InProceedings
oftheEuropeanConferenceonComputerVision(ECCV),pages518–533,2018.
[10] V. Guizilini, I. Vasiljevic, D. Chen, R. Ambrus, and A. Gaidon. Towards zero-shot scale-
awaremonoculardepthestimation. In2023IEEE/CVFInternationalConferenceonComputer
Vision(ICCV),pages9199–9209,LosAlamitos,CA,USA,oct2023.IEEEComputerSociety.
doi: 10.1109/ICCV51070.2023.00847. URLhttps://doi.ieeecomputersociety.org/
10.1109/ICCV51070.2023.00847.
[11] HualieJiang,ZheSheng,SiyuZhu,ZilongDong,andRuiHuang. Unifuse: Unidirectional
fusionfor360◦panoramadepthestimation. IEEERoboticsandAutomationLetters,2021.
[12] BingxinKe,AntonObukhov,ShengyuHuang,NandoMetzger,RodrigoCayeDaudt,andKon-
radSchindler. Repurposingdiffusion-basedimagegeneratorsformonoculardepthestimation.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR),2024.
[13] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderC.Berg,Wan-YenLo,PiotrDollár,andRossGirshick.
Segmentanything. arXiv:2304.02643,2023.
[14] AlinaKuznetsova,HassanRom,NeilAlldrin,JasperUijlings,IvanKrasin,JordiPont-Tuset,
ShahabKamali,StefanPopov,MatteoMalloci,AlexanderKolesnikov,TomDuerig,andVittorio
Ferrari. Theopenimagesdatasetv4: Unifiedimageclassification,objectdetection,andvisual
relationshipdetectionatscale. IJCV,2020.
[15] Dong-HyunLee. Pseudo-label: Thesimpleandefficientsemi-supervisedlearningmethod
for deep neural networks. 2013. URL https://api.semanticscholar.org/CorpusID:
18507866.
[16] YuyanLi,YuliangGuo,ZhixinYan,XinyuHuang,DuanYe,andLiuRen. Omnifusion: 360
monocular depth estimation via geometry-aware fusion. In 2022 Conference on Computer
VisionandPatternRecognition(CVPR),NewOrleans,USA,June2022.
10[17] ZhengqiLiandNoahSnavely. Megadepth: Learningsingle-viewdepthpredictionfrominternet
photos. InComputerVisionandPatternRecognition(CVPR),2018.
[18] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,Jianwei
Yang,HangSu,JunZhu,etal. Groundingdino: Marryingdinowithgroundedpre-trainingfor
open-setobjectdetection. arXivpreprintarXiv:2303.05499,2023.
[19] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil
Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell
Howes,Po-YaoHuang,HuXu,VasuSharma,Shang-WenLi,WojciechGaluba,MikeRabbat,
Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal,
PatrickLabatut,ArmandJoulin,andPiotrBojanowski. Dinov2: Learningrobustvisualfeatures
withoutsupervision,2023.
[20] Timothy Langlois Pedro Morgado, Nuno Vasconcelos and Oliver Wang. Self-supervised
generationofspatialaudiofor360degvideo. InNeuralInformationProcessingSystems(NIPS),
2018.
[21] GiovanniPintore,MarcoAgus,EvaAlmansa,JensSchneider,andEnricoGobbetti. SliceNet:
deepdensedepthestimationfromasingleindoorpanoramausingaslice-basedrepresentation.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR),pages11536–11545,June2021.
[22] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
[23] RenéRanftl,AlexeyBochkovskiy,andVladlenKoltun.Visiontransformersfordenseprediction.
ICCV,2021.
[24] RenéRanftl,KatrinLasinger,DavidHafner,KonradSchindler,andVladlenKoltun. Towards
robustmonoculardepthestimation: Mixingdatasetsforzero-shotcross-datasettransfer. IEEE
TransactionsonPatternAnalysisandMachineIntelligence,44(3),2022.
[25] TianheRen, ShilongLiu, AilingZeng, JingLin, KunchangLi, HeCao, JiayuChen, Xinyu
Huang,YukangChen,FengYan,ZhaoyangZeng,HaoZhang,FengLi,JieYang,HongyangLi,
QingJiang,andLeiZhang. Groundedsam: Assemblingopen-worldmodelsfordiversevisual
tasks,2024.
[26] ManuelRey-Area,MingzeYuan,andChristianRichardt. 360MonoDepth: High-resolution360
monoculardepthestimation. InCVPR,2022.
[27] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,Zhiheng
Huang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal. Imagenetlargescalevisual
recognitionchallenge. Internationaljournalofcomputervision,115:211–252,2015.
[28] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,GangYu,XiangyuZhang,JingLi,and
JianSun. Objects365: Alarge-scale,high-qualitydatasetforobjectdetection. InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),October2019.
[29] ZhijieShen,ChunyuLin,KangLiao,LangNie,ZishuoZheng,andYaoZhao. Panoformer:
Panoramatransformerforindoor360depthestimation. InEuropeanConferenceonComputer
Vision,pages195–211.Springer,2022.
[30] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raf-
fel, EkinDogusCubuk, AlexeyKurakin, andChun-LiangLi. Fixmatch: Simplifyingsemi-
supervisedlearningwithconsistencyandconfidence.Advancesinneuralinformationprocessing
systems,33:596–608,2020.
[31] Yu-ChuanSuandKristenGrauman. Learningsphericalconvolutionforfastfeaturesfrom360
imagery. Advancesinneuralinformationprocessingsystems,30,2017.
11[32] Cheng Sun, Chi-Wei Hsiao, Ning-Hsu Wang, Min Sun, and Hwann-Tzong Chen. Indoor
panoramaplanar3dreconstructionviadivideandconquer. InCVPR,2021.
[33] ChengSun,MinSun,andHwann-TzongChen. Hohonet: 360indoorholisticunderstanding
withlatenthorizontalfeatures. InCVPR,2021.
[34] Fu-En Wang, Hou-Ning Hu, Hsien-Tzu Cheng, Juan-Ting Lin, Shang-Ta Yang, Meng-Li
Shih, Hung-Kuo Chu, and Min Sun. Self-supervised learning of depth and camera motion
from 360° videos. In Asian Conference on Computer Vision, 2018. URL https://api.
semanticscholar.org/CorpusID:53290169.
[35] Fu-EnWang,Yu-HsuanYeh,MinSun,Wei-ChenChiu,andYi-HsuanTsai. Bifuse: Monocular
360 depth estimation via bi-projection fusion. In The IEEE/CVF Conference on Computer
VisionandPatternRecognition(CVPR),June2020.
[36] Fu-EnWang,Yu-HsuanYeh,Yi-HsuanTsai,Wei-ChenChiu,andMinSun. Bifuse++: Self-
supervisedandefficientbi-projectionfusionfor360°depthestimation. IEEETransactionson
PatternAnalysisandMachineIntelligence,45(5):5448–5460,2023. doi: 10.1109/TPAMI.2022.
3203516.
[37] Ning-HsuWang,BolivarSolarte,Yi-HsuanTsai,Wei-ChenChiu,andMinSun. 360sd-net: 360
stereodepthestimationwithlearnablecostvolume. In2020IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pages582–588.IEEE,2020.
[38] Ning-HsuWang,RenWang,Yu-LunLiu,Yu-HaoHuang,Yu-LinChang,Chia-PingChen,and
KevinJou. Bridgingunsupervisedandsuperviseddepthfromfocusviaall-in-focussupervision.
InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages12621–
12631,2021.
[39] QiangWang,ShizhenZheng,QingsongYan,FeiDeng,KaiyongZhao,andXiaowenChu. Irs:
Alargenaturalisticindoorroboticsstereodatasettotraindeepmodelsfordisparityandsurface
normalestimation. In2021IEEEInternationalConferenceonMultimediaandExpo(ICME),
pages1–6.IEEE,2021.
[40] WenshanWang,DelongZhu,XiangweiWang,YaoyuHu,YuhengQiu,ChenWang,YafeiHu,
AshishKapoor,andSebastianScherer. Tartanair: Adatasettopushthelimitsofvisualslam.
In2020IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),pages
4909–4916.IEEE,2020.
[41] TobiasWeyand,AndreAraujo,BingyiCao,andJackSim. Googlelandmarksdatasetv2-alarge-
scalebenchmarkforinstance-levelrecognitionandretrieval. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages2575–2584,2020.
[42] Ke Xian, Chunhua Shen, ZHIGUO CAO, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo.
Monocular relative depth perception with web stereo data supervision. 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 311–320, 2018. URL
https://api.semanticscholar.org/CorpusID:52860134.
[43] KeXian,JianmingZhang,OliverWang,LongMai,ZheLin,andZhiguoCao. Structure-guided
rankinglossforsingleimagedepthprediction. InTheIEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),June2020.
[44] QizheXie,Minh-ThangLuong,EduardHovy,andQuocVLe. Self-trainingwithnoisystudent
improvesimagenetclassification. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10687–10698,2020.
[45] LiheYang, BingyiKang, ZilongHuang, XiaogangXu, JiashiFeng, andHengshuangZhao.
Depthanything: Unleashingthepoweroflarge-scaleunlabeleddata. InCVPR,2024.
[46] YaoYao,ZixinLuo,ShiweiLi,JingyangZhang,YufanRen,LeiZhou,TianFang,andLong
Quan. Blendedmvs: A large-scale dataset for generalized multi-view stereo networks. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
1790–1799,2020.
12[47] WeiYin,ChiZhang,HaoChen,ZhipengCai,GangYu,KaixuanWang,XiaozhiChen,and
ChunhuaShen. Metric3d: Towardszero-shotmetric3dpredictionfromasingleimage. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages9043–9053,
2023.
[48] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction
ofalarge-scaleimagedatasetusingdeeplearningwithhumansintheloop. arXivpreprint
arXiv:1506.03365,2015.
[49] FisherYu,HaofengChen,XinWang,WenqiXian,YingyingChen,FangchenLiu,Vashisht
Madhavan,andTrevorDarrell. Bdd100k: Adiversedrivingdatasetforheterogeneousmul-
titasklearning. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages2636–2645,2020.
[50] Ilwi Yun, Chanyong Shin, Hyunku Lee, Hyuk-Jae Lee, and Chae Eun Rhee. Egformer:
Equirectangulargeometry-biasedtransformerfor360depthestimation. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision(ICCV),pages6101–6112,October
2023.
[51] JiaZheng,JunfeiZhang,JingLi,RuiTang,ShenghuaGao,andZihanZhou. Structured3d:
Alargephoto-realisticdatasetforstructured3dmodeling. InProceedingsofTheEuropean
ConferenceonComputerVision(ECCV),2020.
[52] BoleiZhou,AgataLapedriza,AdityaKhosla,AudeOliva,andAntonioTorralba. Places: A
10millionimagedatabaseforscenerecognition. IEEETransactionsonPatternAnalysisand
MachineIntelligence,2017.
[53] TinghuiZhou,MatthewBrown,NoahSnavely,andDavidG.Lowe. Unsupervisedlearningof
depthandego-motionfromvideo. In2017IEEEConferenceonComputerVisionandPattern
Recognition(CVPR),pages6612–6619,2017. doi: 10.1109/CVPR.2017.700.
[54] ChuanqingZhuang,ZhengdaLu,YiqunWang,JunXiao,andYingWang. Acdnet: Adaptively
combineddilatedconvolutionformonocularpanoramadepthestimation. InProceedingsofthe
AAAIConferenceonArtificialIntelligence,volume36,pages3653–3661,2022.
[55] NikolaosZioulis,AntonisKarakottas,DimitriosZarpalas,andPetrosDaras. Omnidepth: Dense
depthestimationforindoorssphericalpanoramas. InProceedingsoftheEuropeanConference
onComputerVision(ECCV),pages448–465,2018.
[56] NikolaosZioulis,AntonisKarakottas,DimitrisZarpalas,FedericAlvarez,andPetrosDaras.
Sphericalviewsynthesisforself-supervised360odepthestimation. InInternationalConference
on3DVision(3DV),September2019.
[57] BarretZoph, GolnazGhiasi, Tsung-YiLin, YinCui, HanxiaoLiu, EkinDogusCubuk, and
QuocV.Le. Rethinkingpre-trainingandself-training. ArXiv,abs/2006.06882,2020. URL
https://api.semanticscholar.org/CorpusID:219635973.
A Appendix/supplementalmaterial
A.1 Experimentalsetup
Implementationdetails. Ourworkisdividedintotwostages: (1)offlinemaskgenerationand
(2) online joint training. (1) In the first stage, we use Grounded-Segment-Anything [25], which
combines state-of-the-art detection and segmentation models. We set the BOX_THRESHOLD and
TEXT_THRESHOLDto0.3and0.25,respectively,followingtherecommendationsoftheofficialcode.
We use “sky” and “watermark” as text prompts. All pixels with these labels are set to False to
formourvalidmaskforthesecondstageoftraining. (2)Inthesecondstage,eachbatchconsists
ofanequalmixoflabeledandunlabeleddata. Wefollowthebackbonemodel’sofficialsettings
forbatchsize, learningrate, optimizer, augmentation, andotherhyperparameters, changingonly
thelossfunctiontoaffine-invariantloss. UnlikeDepthAnything,whichsetsinvalidskyregionsto
zerodisparity,weignoretheseinvalidpixelsduringlosscalculation,consistentwithgroundtruth
13trainingsettings. Weaveragethelossforgroundtruthandpseudogroundtruthduringupdates. All
ourexperimentsareconductedonasingleRTX4090,bothofflineandonline. However,iffuture
360-degreestate-of-the-artmethodsorperspectivefoundationmodelsrequirehigherVRAMusage,
thecomputationalresourcerequirementsmayincrease.
Metrics. In line with previous cross-dataset works, all evaluation metrics are presented in per-
centage terms. The primary metric is Absolute Mean Relative Error (AbsRel), calculated as:
1 (cid:80)M |a −d |/d , where M is the total number of pixels, a is the predicted depth, and d
M i=1 i i i i i
isthegroundtruthdepth. Thesecondmetric,δ accuracy,measurestheproportionofpixelswhere
j
max(ai/di,di/ai)iswithin1.25j. Duringevaluations,wefollow[11,35,36]toignoreareaswhere
groundtruthdepthvaluesarelargerthan10orequalto0. Giventheambiguousscaleofself-training
results,weapplymedianalignmentafterconvertingdisparityoutputtodepthbeforeevaluation,as
perthemethodusedin[53]:
median(dˆ)
d′ =d· , (8)
median(d)
wheredisthepredicteddepthfrominversedisparityanddˆisthegroundtruthdepth. Thisensuresa
faircomparisonbyaligningthemediandepthvaluesofpredictionsandgroundtruths.
A.2 Morequalitative
Wedemonstrateadditionalzero-shotqualitativeresultsinFigure8. In-domainresultsontheMatter-
port3DtestsetsareshowcasedinFigure9andFigure10.
Figure8: MorequalitativetestedonStanford2D3Dwithzero-shotsetting.
A.3 Datasetstatistic
AsdescribedinSec.3.1ofthemainpaper,thereisasignificantdifferenceinthenumberofimages
betweentheperspectiveandequirectangulardatasets. Detailedstatisticsofthedatasetsarelistedin
Table5.
14
BGR
TG
esuFinU
)p(
esuFinU
++esuFiB
)p(
++esuFiBFigure9: In-domainqualitativewithUniFuse.
Figure10: In-domainqualitativewithBiFuse++.
15
BGR
TG
esUFinU
)p(
esUFinU
BGR
TG
++esuFiB
)p(
++esuFiBTable5: 360monoculardepthestimationlacksalargeamountoftrainingdata. Thistablelists
datasetsusedin360-degreemonoculardepthestimationalongsideperspectivedepthdatasetsfrom
theDepthAnythingmethodology. Thevolumeoftrainingdatafor360-degreeimagery(rightcolumn)
issignificantlysmallerthanthatforperspectiveimagery(leftcolumn), byabout200times. This
highlightstheneedforusingperspectivedistillationtechniquestoenhancethelimiteddataavailable
for360-degreedepthestimation. Groundtruth(GT)labelsarenotedwhereapplicable,showingthe
availableresourcesfortraininginthesedomains.
Perspective Equirectangular
Dataset Venue #ofimages GTlabels Dataset Venue #ofimages GTlabels
MegaDepth[17] CVPR2018 128K ✓ Stanford2D3D[2] arXiv2017 1.4K ✓
TartanAir[40] IROS2020 306K ✓ Matterport3D[5] 3DV2017 10.8K ✓
DIML[8] arXiv2021 927K ✓ Structured3D[51] ECCV2020 21.8K ✓
BlendedMVS[46] CVRP2020 115K ✓ SpatialAudioGen[20] NeurIPS2018 344K -
HRWSI[43] CVPR2020 20K ✓
IRS[39] ICME2021 103K ✓
ImageNet-21K[27] IJCV2015 13.1M -
BDD100K[49] CVPR2020 8.2M -
GoogleLandmarks[41] CVPR2020 4.1M -
LSUN[48] arXiv2015 9.8M -
Objects365[28] ICCV2019 1.7M -
OpenImagesV7[14] IJCV2020 7.8M -
Places365[52] TPAMI2017 6.5M -
SA-1B[13] ICCV2023 11.1M -
16