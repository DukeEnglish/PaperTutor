From RAGs to rich parameters: Probing how language models utilize
external knowledge over parametric information for factual queries
HiteshWadhwa1,*,RahulSeetharaman1,*,SomyaaAggarwal1,*,ReshmiGhosh2,
SamyadeepBasu2,3,SoundararajanSrinivasan2,WenlongZhao1,ShreyasChaudhari1,
EhsanAghazadeh1
1UniversityofMassachusetts,Amherst,2Microsoft,3UniversityofMaryland,CollegePark
*
EqualContributions|| Correspondence:reshmighosh@microsoft.com
Abstract internalparametersoftheLMstoupdateorcorrect
knowledge. However,amechanisticunderstanding
Retrieval Augmented Generation (RAG) en-
of how RAG context influences LM predictions
richestheabilityoflanguagemodelstoreason
overpriorknowledgehasn’tbeenstudiedtilldate. usingexternalcontexttoaugmentresponsesfor
And the rise of RAG usage necessitates us to un-
agivenuserprompt. Thisapproachhasrisenin
popularityduetopracticalapplicationsinvari- derstand quantitatively the interplay between the
ousapplicationsoflanguagemodelsinsearch, LM’spriorknowledgeandtheexternalinformation
question/answering, andchat-bots. However, retrievedduringinference, forpreventingdriftin
the exact nature of how this approach works modelreasoning.
isn’t clearly understood. In this paper, we
mechanisticallyexaminetheRAGpipelineto
highlightthatlanguagemodelstake“shortcut”
and have a strongbias towardsutilizing only
thecontextinformationtoanswerthequestion,
while relying minimally on their parametric
memory. Weprobethismechanisticbehavior
inlanguagemodelswith: (i)CausalMediation
Analysistoshowthattheparametricmemoryis
minimallyutilizedwhenansweringaquestion
and (ii) Attention Contributions and Knock-
outstoshowthatthelasttokenresidualstream
donotgetenrichedfromthesubjecttokenin
thequestion, butgetsenrichedfromotherin-
Figure1: SetupofafactualQAsystemwithRAG,utilized
formativetokensinthecontext. Wefindthis
inthispaper,forunderstandingtheusefulnessofparameteric
pronounced “shortcut” behaviour true across
knowledgestoredinLlaMaandPhi.
bothLLaMaandPhifamilyofmodels.
Inthispaper,weaimtoanalyzeandinterpret
1 Introduction
thedependencyofLMsonparametricknowledge
With the burgeoning use of Language Models versustheretrievedinformationpresentedviaRAG.
(LMs) in many industrial applications, retrieval Towardsthisgoal,werelyonestablishedmethods
AugmentedGeneration(RAG)hasbecomepopu- oflocatingfactualknowledgestoredinthemodel
lar as a mechanism of providing additional con- parameters.
text for effective reasoning to mitigate halluci- Wefindthat: (i). Parametricknowledgeismini-
nations. Yet, the usefulness of RAG to pro- mallyusedwithinMultiLayerPerceptrons(MLPs)
vide meaningful information in comparison to inthepresenceofretrievedcontext. and(ii). The
model priors is an under-explored area of re- lasttokenresidualstream,crucialforformingthefi-
search. Ontheotherhand,knowledgelocalization naloutput,derivesmoreenrichedinformationfrom
andeditingtechniques(Wangetal.,2024b)(Wang theattributetokenpresentexplicitlyinthecontext
et al., 2024a)(Gupta et al., 2024b)(Gupta et al., ratherthanfromthesubjecttokenwithinthequery.
2024a)(Sharma et al., 2024)(Conmy et al., Theseinsightshighlightapronounced"shortcut"
2023)(Wu et al., 2024b) in LMs such as ROME behavior in LMs, where the models prioritize ex-
(Meng et al., 2022a) and MEMIT (Meng et al., ternal context over internal knowledge. Through
2022b) are traditionally focused on adjusting the this analysis, our work contributes to the a novel
4202
nuJ
81
]LC.sc[
1v42821.6042:viXraunderstandingofthemechanismsunderlyingLMs’ Finally, the Indirect Effect (IE) of a spe-
preferencefortheinformationprovidedviaRAG. cific hidden state h(l) is defined as the differ-
i
encebetweenthecorruptedrunandthecorrupted-
2 RelatedWork
(l)
with-restoration run probabilities: IE(h ) =
i
RAG systems (Lewis et al., 2021) have become P∗ (h(l) )[y]−P∗[y]andbyaveragingtheseef-
clean i
popular in practical natural language systems fectsoverasample,theAverageIndirectEffect
as they significantly improve the performance (AIE)iscomputedforallhiddenstates,providinga
of LM applications by integrating external con- quantitativemeasureoftheirimportanceinfactual
text(Shao et al., 2023)(Singh et al., 2023)(Inges- prediction.
tAI, 2023)(Kaddour et al., 2023) (Chen et al.,
2024)(Renetal.,2023)However,utilizingRAGs
3.2 AttentionKnockoutandContribution
can also have nuanced outcomes such as gener-
Mechanism
ation of inconsistent predictions, even with per-
TheAttentionContribution(Yuksekgonuletal.,
fect retrieval results(Hagström et al., 2023).(Wu
2024),focusesontheroleofattentionmechanisms
et al., 2024a) explore the role of RAG in re-
in shaping the output of language models. This
ducing hallucinations and enhancing accuracy in
approachinvestigateshowattentionweights,par-
largelanguagemodelssuchasGPT-4,buildingon
ticularly from the subject token in a query to the
priorwork(Lewisetal.,2021)(Shusteretal.,2021)
lasttoken position, contribute to themodel’spre-
thatleverageexternalretrievalsystemstomitigate
dictions. By examining the norm of these atten-
modelerrors. EventhoughRAGmodelsareexten-
(ℓ)
tion weights ∥a ∥, we observe what tokens the
sively used, and their shortcomings documented, i,T
last token pays the most attention to, during the
only (Wu et al., 2024a) delves into the balance
generationprocess. SeeappendixCfornormcal-
between a model’s internal knowledge and exter-
culationdetails. TheAttentionKnockoutmech-
nallyretrievedinformation,examiningtheirpracti-
anism (Geva et al., 2023) identifies critical atten-
calvalue. However,asystematicmechanisticex-
tion edges in transformer-based models that are
plorationofmodel’spreferenceforRAG-provided
essential for maintaining prediction quality. The
informationovertheirparametricknowledgecon-
processinvolvesidentifyingcriticaledgeswhose
tributionhasnotyetbeenconducted,tothebestof
removal significantly degrades the model’s pre-
ourknowledge. Ourstudymechanisticallyprobes
diction quality. To test the importance of these
intotheinternalworkingsoflargelanguagemodels
edges, attention weights between two positions
andhowtheyexhibita"shortcutmechanism"when
r and c at a layer l are set to negative infinity:
theyareprovidedwithnon-parametricknowledge
Ml+1,j = −∞ ∀j ∈ [1,H]
viaaRAGsystem. rc
This prevents the source position xl from at-
r
3 ProbingMechanisms tendingtothetargetpositionxl,blockinginforma-
c
Tomechanisticallyinterprettheknowledgecontri- tionflowatthatlayer. Thedegradationinpredic-
butions towards factual reasoning by LLMs and tionqualityafterblockingattentionedgesidentifies
SLMs,weusethreemethodsforcausalmediation, whichedgesarecriticalforinformationflow.
describedasfollows:
4 DatasetsandModels
3.1 CausalTracing
Causaltracing(Mengetal.,2022a)identifiesspe- 4.1 Models
cifichiddenstatesthatsignificantlyinfluencefac- Foracomprehensivemechanisticprobing,welever-
tualpredictions. Theapproachinvolvesthreesteps age two state-of-the-art LMs, Phi-2 (2.7B) (Li
-acleanrun, corruptedrunandacorrupted-with- et al., 2023) and LLaMA-2 (7B) (Touvron et al.,
restoration run. The corrupted run involves cor- 2023)models,whichweretrainedondifferentcor-
ruptingacertainspanofthetext,andrunningthe pora. Difference in parametreic knowledge be-
forwardpassofthemodel. Intherestorationrun, tween two different family of models, allows us
activationsfromthecleanrunarepatchedoneby tocomprehensivelyprobetheinfluenceofRAGfor
oneintothecorruptedrun,andtheincreaseinan- factualqueriesinscenariosinvolvingthesemodels.
swerprobabilityisobserved;themostcrucialacti- Alsochosingopen-sourceLMsenablesusmeasure
vationsarethuscausallydetermined. causalmediationeasily.(a)LLaMa-2withRAGvs.LLaMa-2Vanilla (b)Phi-2withRAGvs.Phi-2Vanilla
Figure 2: Language models minimally rely on the MLP parametric memory inthe presence of retrieved
context. Fromlefttoright: AverageIndirectEffectfromMLPsaftercorruptingsubject+contextforscenariobased
onRAGandsubjectforvanilla-case. Here,FST=FirstSubjectToken,MST=MiddleSubjectTokens,LST=Last
SubjectToken,FSST=FirstSubsequentToken,FT=FurtherTokens,LT=LastToken. Onaverage5timesdecrease
inAIEisobservedforLSTwithRAGvs. vanilla,signallingdecreaseinusageofMLPwhenRAGcontextpresent.
4.2 Dataset prompts2 forbothscenarios,i.e,vanillavs. RAG
Inthispaper,wescopetheanalysistodeterminethe tounderstandfactprediction. ForRAGscenarios,
influenceofexternalinformationprovidedbyRAG theentirecontextalongwithsubjectiscorrupted
contextagainstmodelpriors,toonlyfactualquery as part of causal tracing, whereas for the vanilla
predictionsfromaforementionedLMs. Thus,we case only subject is corrupted. Figure 2 presents
utilizetheKnownsFactDatasetof1209factual thedecreaseinAIEinpresenceofRAGoftheLST
queries,introducedin(Mengetal.,2022a). Each ascomparedtovanilla(noRAG)setting.
recordinthedatasetisof(s,r,o)formatofsubject, WeanalyzetheAverageIndirectEffectofMLPs
relationandobject/attribute,respectively1. representing subject tokens and compare against
FortheRAGdataset,wesyntheticallygenerate vanillavs. RAGcontextscenariosforLlama-2(7B)
RAGcontextforeachqueryfromtheKnowns-Fact for50examplesfromtheknownsfactdataset,and
datasetusingGPT4. Thiswasdonetocontrolthe find that the AIE decreases 5 times (from 0.2
variablessuchaslengthofeachsegmentwithinthe to 0.0375),provingthatsubjecttokenswithinthe
RAGcontextandthepresenceofattributeorobject. querydoesnotelicittheparametricmemorywhen
Furtherdetailsonpromptsusedandsamplesfrom thecontextispresent. Similarly,forthecaseofa
datasetinAppendixA.Inthescopeofthiswork, smallerlanguagemodelsuchasPhi-2,wehavea
we work with a vanilla setting, where no RAG similarobservationwherewefindthatthelanguage
contextispresentforqueriestogetenriches,and modeldoesnotusetheparametricmemory. This
a RAG setting. The generation was made sure is in contrast to a non-RAG, vanilla case where
to follow our constraints using quality assurance the subject token has a high AIE and serves as a
techniqueswhichregeneratedthecontextwhenthe hotspotoffactualretrievalfromparametricmem-
constraints were not satisfied. The code can be ory. In addition to the MLPs, we also perform
foundhereinAppendixE causaltracingonattentionlayers,detailsofwhich
canbefoundinAppendixF
5 EmpiricalResults
5.2 Finding2: Lasttokenresidualstream
5.1 Finding1: Languagemodelsminimally
obtainsmoreenrichedinformationfrom
useparametricmemoryinthepresenceof
thecontext,ratherthansubjecttokenin
context
query
Westartbymechanisticallyprobingthecontribu-
Inspired by findings of a strong attention contri-
tionsofvariousMLPlayersforLlama-2(7B)and
bution from the Subject Token (ST) in the query
Phi-2forarepresentativesetofrandomlysampled
question to the Last Token (LT) position for fac-
1subjectpartoftheuserquery.Forexample,foruserquery: tualqueriesin(Yuksekgonuletal.,2024),wetryto
"TheSpaceNeedleislocatedinthecityof"thesubjectwill
bedefinedas"TheSpaceNeedle". Whenwesayattribute 2Werandomlyselectasmallsubset,50promptsascausal
orobject, wemeantheanswertothatquerywhichwillbe tracingwithRAGcontexttakessignificanttimetoexperiment
presentonlyonceinthecontextgeneratedplacedatthefirst with,intheorderofa4-5hoursfor20wordsegmentsof5
segment.ExamplecanbefoundinAppendixB. countFigure3: Thelasttokenresidualstreamobtainslessenrichedinformationfromthesubjecttokeninthequery
inthepresenceofretrievedcontext.(a)SubjectTokencontributionforRAGvsvanillainLlama-2,(b)Comparisonof
subjectandattributecontributionsw/RAGforLlama-2,(c)SubjectcontributionforRAGvsvanillainPhi-2,(d)Comparisonof
subjectandattributecontributionsw/RAGforPhi-2.4a.and4cindicatessubjectcontributionistwiceaslowerincaseofRAG
ascomparedtovanilla.4band4dshowsthatattributetoken’sattentioncontributionis5timesmorethanthesubjectcontribution.
uncover any signal of relevant information trans- themagainstST.ThecontrolledRAGcontextwe
fer between subject token and the last token po- generated syntheticallyensures there is only one
sitioninLMsforfactualqueries.Wecomputethe ATpresentinthecontext. WefindinFig3.b,and
Attention Contributions from ST 3 to the LT for 3.d, when compared against Attention Contribu-
LlaMa-2andPhi-2forvanillaandRAGscenarios tionsofATpresentinRAGcontext,againstSTin
forall1209factualqueriesinKnownsFactDataset. thequery,AThasalargerinfluenceinfactpredic-
Wefindthat70%ofthelayersdon’tcontributeto tions. ForLlaMa-2,themeanattentioncontribution
the final token prediction and therefore resulting atATis7.1242,whileatSTis5.6094. ForPhi-2,
in almost 0 contribution to the Last Token (LT). itis20.8902and10.6650,respectively,i.e,2times
Thereby, as shown in Figure 3 we extract the top higherthanatST.
5%oftheAttentionContributionsfromtheSTto
To validate this finding further, we use Atten-
theLTforvanillavs. RAGscenariosusingLlaMA
tionKnockouts(Gevaetal.,2023)tomeasurethe
andPhitoamplifythedifference. Weobservethat
changeinprobabilityofthepredictedtoken(objec-
Specifically for Fig3.a and Fig 3.b, the Attention
t/attribute),whentheattentionweightsfromtheST
ContributionsfromSubjectTokendecreaseinthe
inthequerytothelasttokenisknockedoff. Figure
presenceofRAGindicating,thelargerinfluenceof
4presentsthatfortheRAGscenario,knockingoff
RAGcontextinpredictingfacts.ForLLaMa-2,the
attentionweightsfromthesubjectinquerytothe
meanattentioncontributionforRAGcaseis5.6094
last token leads to a probability drop of less than
vs. 9.0054 in vanilla setting. For Phi, Attention
5 percent in both LLaMa-2 and Phi-2. However,
ContributionatSTis10.6650forRAGvs. 72.5961
weobserveamuchstrongerdropintheprobability
inthevanillacase,which7timeslarger.
oftheoriginalpredictedtoken,(20%)inLLaMa-2
and25%inPhi-2. Theseresultshighlightthatin
presence of RAG context, the last token residual
streamignoresinformationfromthesubjecttoken
position in the query and instead solely relies on
thetokencontributionsfromthecontext. Addition-
ally,weperformknockoutsinthevanillasettingon
thesubjecttoken(detailsinAppendixD.)
Figure 4: In the presence of retrieved context, knock-
ing out attention weights from the subject in query to
Main Takeaway: In the presence of re-
the last token has minimal effect. (Left) Llama2 (Right)
Phi2. [Knockingoutattributetokensdecreasesprobability trieved RAG context, language models in-
upto25%inPhi2and20%inLlama2andonly5%probability ternallyrelyprimarilyonthecontext,while
isreducedonknockingoutsubjecttokenattention.]
minimallyusingtheparametricmemoryto
answeraquestion.
Additionally,wealsoanalyzeAttentionContri-
butions for Attribute Tokens (AT)4, and compare
3STreferstothesubjecttokensoftheuserquery.
4Attributetokensreferstotheexpectedanswerofthequery
beingasked,presentintheRAGcontext,whichisalsothe sameastheobject6 DiscussionandConclusions ationsinauto-regressivelanguagemodels. Preprint,
arXiv:2304.14767.
This paper is the first study to utilize three dif-
ferentmechanisticprobingmethodstounderstand
Akshat Gupta, Anurag Rao, and Gopala Anu-
the benefits of using RAG context as an external manchipalli. 2024a. Model editing at scale leads
knowledge source to complement the parametric togradualandcatastrophicforgetting. arXivpreprint
arXiv:2401.07453.
knowledgestoredinthemodelsaspriorforfactual
queries. Ourworkexplorestheutilityofparamet-
Akshat Gupta, Dev Sajnani, and Gopala Anu-
ricmemory,andtheinterplaybetweenparametric manchipalli.2024b. Aunifiedframeworkformodel
and non-parametric memory in the process of re- editing. arXivpreprintarXiv:2403.14236.
trievalaugmentedgeneration.Wefindthatparamet-
Lovisa Hagström, Denitsa Saynova, Tobias Norlund,
ricmemorybecomeslesscriticalforfactualrecall
MoaJohansson,andRichardJohansson.2023. The
when RAG context is augmented to the prompt.
effectofscaling,retrievalaugmentationandformon
Throughattentioncontributions, attentionknock- thefactualconsistencyoflanguagemodels. InCon-
outs and causal traces, we specifically observe a ferenceonEmpiricalMethodsinNaturalLanguage
Processing.
reducedrelianceonthesubjecttoken,andtheMLP
activationsassociatedwithit,whenthecontextis
IngestAI.2023. Retrieval-augmentedgeneration(rag):
augmentedwithRAG. Enhancingllmswithexternalknowledge.
7 LimitationsandFutureWork JeanKaddour,JoshuaHarris,MaximilianMozes,Her-
bieBradley,RobertaRaileanu,andRobertMcHardy.
Our study is limited by the analysis using short 2023. Challengesandapplicationsoflargelanguage
models. arXivpreprintarXiv:2307.10169.
RAG-basedcontext. Handlingreallylongcontext
currentlyincursaprohibitivelylargecomputational
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
overhead in causal tracing. We plan to study the Petroni,VladimirKarpukhin,NamanGoyal,Hein-
impact of long context and the impact of subject richKüttler, MikeLewis, WentauYih, TimRock-
tokenandattributetokenwithrespecttoposition täschel, Sebastian Riedel, and Douwe Kiela. 2021.
Retrieval-augmented generation for knowledge-
andthetendencytoexhibitproximityandrecency
intensivenlptasks. Preprint,arXiv:2005.11401.
bias (Liu et al., 2023) in a future work. In addi-
tion,similaranalysisofinstructiontunedmodels Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie
and models that are finetuned on objectives like DelGiorno,SuriyaGunasekar,andYinTatLee.2023.
Textbooksareallyouneedii: phi-1.5technicalre-
RLHFisatopicforfuturework. Thecurrentstudy
port. arXivpreprintarXiv:2309.05463.
involves a well controlled setting where attribute
token is present only once in the context and the NelsonF.Liu,KevinLin,JohnHewitt,AshwinParan-
context itself is synthetically generated and well- jape,MicheleBevilacqua,FabioPetroni,andPercy
Liang. 2023. Lost in the middle: How language
formed. Retrievedoutputs,inpracticeisverynoisy
modelsuselongcontexts. ArXiv:2307.03172.
andoftensensitivetothequalityoftheretrievers,
rankers,andthehyperparametersused. Examining KevinMeng,DavidBau,AlexAndonian,andYonatan
thoseisalsoanaturalextensionofthiswork. Belinkov. 2022a. Locating and editing factual as-
sociationsingpt. AdvancesinNeuralInformation
ProcessingSystems,35:17359–17372.
References
Kevin Meng, Arnab Sen Sharma, Alex Andonian,
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Yonatan Belinkov, and David Bau. 2022b. Mass-
2024. Benchmarking large language models in editing memory in a transformer. arXiv preprint
retrieval-augmentedgeneration. InProceedingsof arXiv:2210.07229.
theAAAIConferenceonArtificialIntelligence,vol-
ume38,pages17754–17762. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin
Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,
ArthurConmy,AugustineMavor-Parker,AengusLynch, andHaifengWang.2023. Investigatingthefactual
Stefan Heimersheim, and Adrià Garriga-Alonso. knowledgeboundaryoflargelanguagemodelswith
2023. Towardsautomatedcircuitdiscoveryformech- retrievalaugmentation. Preprint,arXiv:2307.11019.
anisticinterpretability. AdvancesinNeuralInforma-
tionProcessingSystems,36:16318–16352. C. Shao, T. Kim, and Z. Gao. 2023. Eragent: En-
hancingretrieval-augmentedlanguagemodelswith
MorGeva,JasmijnBastings,KatjaFilippova,andAmir improvedaccuracy,efficiency,andpersonalization.
Globerson.2023. Dissectingrecalloffactualassoci- arXivpreprintarXiv:2405.06683.Arnab Sen Sharma, David Atkinson, and David Bau. Kamar, and Besmira Nushi. 2024. Attention satis-
2024. Locating and editing factual associations in fies: Aconstraint-satisfactionlensonfactualerrors
mamba. arXivpreprintarXiv:2404.03646. oflanguagemodels. Preprint,arXiv:2309.15098.
KurtShuster,SpencerPoff,MoyaChen,DouweKiela,
A SampleDatafromKnownFacts
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. Preprint, Dataset
arXiv:2104.07567.
{
A.Singh,M.Sachan,andK.Guu.2023. Improvingthe
"known_id": 14,
domainadaptationofretrievalaugmentedgeneration
"subject": "Eavan Boland",
(rag) models for open domain question answering.
TransactionsoftheAssociationforComputational "attribute": "Dublin",
Linguistics. "template": "{} was born in",
"prediction": " Dublin, Ireland, in 1971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
He is the",
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti "prompt": "Eavan Boland was born in",
Bhosale,DanBikel,LukasBlecher,CristianCanton "relation_id": "P19"
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu, }
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
CynthiaGao,VedanujGoswami,NamanGoyal,An-
thonyHartshorn,SagharHosseini,RuiHou,Hakan B SampleDatafromsynthetically
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
generatedGPT4DatasetwithRAG
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
contexts
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
{"index": 14,
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- "user_query": "Eavan Boland was born in",
stein,RashiRungta,KalyanSaladi,AlanSchelten, "object": "Dublin",
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
"response": ["Boland was born in Dublin,
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Ireland, 1944, and became a leading voice
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan, in contemporary Irish poetry,
Melanie Kambadur, Sharan Narang, Aurelien Ro- exploring women's",
driguez,RobertStojnic,SergeyEdunov,andThomas
"Her birthplace greatly influenced her
Scialom.2023. Llama2: Openfoundationandfine-
works, emphasizing historical narratives
tunedchatmodels. Preprint,arXiv:2307.09288.
and the role of women in Irish society
Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, through poetry.",
Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi
"Boland's early life in Ireland shaped her
Yang, Jindong Wang, and Huajun Chen. 2024a.
poetic voice, focusing on national
Detoxifying large language models via knowledge
editing. arXivpreprintarXiv:2403.14472. identity, gender issues, and
personal history.",
XiaohanWang,ShengyuMao,NingyuZhang,Shumin "Educated at Trinity College, her
Deng,YunzhiYao,YueShen,LeiLiang,JinjieGu,
surroundings nurtured her literary
andHuajunChen.2024b. Editingconceptualknowl-
edge for large language models. arXiv preprint genius, leading to a profound
arXiv:2403.06259. impact on modern literature.",
"Despite her global travels and
Kevin Wu, Eric Wu, and James Zou. 2024a. How
international teaching positions, her
faithful are rag models? quantifying the tug-of-
Irish roots remained central to
warbetweenragandllms’internalprior. Preprint,
arXiv:2404.10198. her thematic concerns in poetry"]
}
ZhengxuanWu,AtticusGeiger,ThomasIcard,Christo-
pherPotts,andNoahGoodman.2024b. Interpretabil-
InitialQuery:
ityatscale: Identifyingcausalmechanismsinalpaca.
AdvancesinNeuralInformationProcessingSystems,
36. EavanBolandwasbornin
MertYuksekgonul,VarunChandrasekaran,ErikJones,
SuriyaGunasekar,RanjitaNaik,HamidPalangi,Ece QueryAugmentedwithRAGcontext:Informationisbelow:—————- Generate five 20-word segments based on
EavanBolandwasborninDublin,Ireland, the following sentence: [user query] [ob-
1944, and became a leading voice in ject]
contemporary Irish poetry, exploring
women’s TheRAG-likedatasetofaugmentedcontextsis
Herbirthplacegreatlyinfluencedherworks, createdsyntheticallybypromptingGPT-4. Wealso
emphasizing historical narratives and the experimented with an actual RAG pipeline, with
role of women in Irish society through documentsfromwikipediaalongwiththeexisting
poetry. queryset. HoweverweobservedthatusingaRAG
Boland’s early life in Ireland shaped her pipeline comes with its own disadvantages with
poeticvoice,focusingonnationalidentity, respecttocontrollability. Giventhesensitityofthe
genderissues,andpersonalhistory. outputmeasureslikeAIE,probabilities,etctoin-
EducatedatTrinityCollege,hersurround- putsandtheirperturbations,usingaRAGpipeline
ingsnurturedherliterarygenius,leadingto addsmorevariability,asretrieveddocumentscan
aprofoundimpactonmodernliterature. benoisyandextremelysensitivetotheunderlying
Despiteherglobaltravelsandinternational retrievalmodelanditshyperparameters.
teachingpositions,herIrishrootsremained
centraltoherthematicconcernsinpoetry. C Background
Giventhecontextinformationandnotprior
C.1 AttentionContribution
knowledge,completethefollowing:
(Yuksekgonuletal.,2024)introducedSAT-Probe,
EavanBolandwasbornin topredictconstraintsatisfactionandfactualerrors
byleveragingself-attentionpatternstodetermineif
generatedtextadherestospecifiedconstraintsand
Prompt used for generation of synthetic measuringthecontributionofdifferentcomponents
dataset: tothemodel’spredictions.
SystemPromptforGPT-4 And Attention to Constraints is achieved by
1. identify constraint tokens within the input, 2.
(ℓ)
trackingtheattentionweightsA (ℓislayer,iis
i,j
Youareanexpertdatagenerationbot,spe- query token and j is constraint token), 3. aggre-
cializingingenerating20wordsegments. gatingattentionweightsacrosslayersandheadsto
- You generate these 20-word segments determineattentioncontributionA (whereCk
Ck,T
by consolidating information/knowledge isconstrainttokens&T istheentiretokenset).
AROUNDasentencethattheuserprovides, Finally, the norm of attention contributions
thatis: [userquery][object]. (ℓ)
∥a ∥ from constraint tokens c to target token T
i,T
-Whilegeneratingthesefive20-wordseg-
atlayerℓismeasuredbyaggregatingthesenorms
ments based on the sentence provided by
acrossalllayersandheadstoformacomprehensive
theuser,here: [userquery][object],make
metricforattentioncontribution.
sure that only 1 of the 5 segments has the
[object] explicitly mentioned. FOLLOW a(ℓ,h) = A(ℓ,h) (x(ℓ−1)W(ℓ,h) )W(ℓ,h)
c,T c,T c V O
THISINSTRUCTIONSTRICTLY.
- Also make sure that none of these seg- (ℓ,h)
where a indicates the attention contribution
c,T
mentscontain: [userquery]. Doublecheck
from a constraint token c through head h to the
tomakesurethisinstructionisstrictlyfol-
finaltokenT. Thetotalcontributionis:
lowed.
- Also make sure that these segments fol- a(ℓ) = (cid:88) a(ℓ,h)
c,T c,T
lowtheformatofanarrayofsegments,i.e,
h
[segment1,segment2,segment3,segment4,
Formultipleconstrainttokens,themaximumvalue
segment5]
isconsidered:
(ℓ,h) (ℓ,h) (ℓ,h) (ℓ,h)
A = maxA and a = max∥a ∥
UserPromptforGPT-4 C,T c,T C,T c,T
c∈C c∈CCorrelationwithFactualCorrectness
Analyzethecorrelationbetweentheaggregated
attentionnormsandthefactualcorrectnessofthe
model’s outputs. Higher attention norms to con-
strainttokensarefoundtocorrelatewithincreased
factualaccuracy,providingapredictivemeasurefor
Figure7: LLama-2causaltraceonAttention
evaluatingthereliabilityofthemodel’sresponses.
D AttentionKnockouts
E Qualitychecksonthegenerated
syntheticdata
Theattentionknockouts(Gevaetal.,2023)study
the impact knocking out attention from a token Ourdatagenerationprocesscomprisesprompting
position i to j, where i ≤ j for an autoregressive GPT-4 to generate synthetic RAG context. The
model. Morespecifically,(Gevaetal.,2023)study quality check primarily involves verifying the at-
theimpactofknockingoutattentionfromthelast tributetokenoccursexactlyoncewithinthegener-
tokentothesubjecttoken,withpromptsfromthe atedcontext. Thefollowingpieceofcodeisused
Knowns1000dataset,whichisadatasetofqueries toperformtheverification.
in the form of (s,r,o) triples. In addition to the
1 def isEntryOkay(entry):
attentionknockoutsintheRAGsetting,weimple- 2 user_query = entry[’user_query’]
3 object_value = entry[’object’]
menttheattentionknockoutsonthesubjecttoken 4 response = entry[’response’]
5
inthevanillasetting. 6 # Check if object is present only once in the
response
7 object_count = response.count(object_value)
8
9 # Check if user query is not present in the response
10 query_in_response = user_query in response
11 return object_count == 1 and not query_in_response
F CausalTracing
Thefollowingpositionsaretrackedwhileplotting
theAverageIndirectEffect(AIE).Firstsubjectto-
ken(FST),MiddleSubjectToken(MST),Lastsub-
Figure5: AttentionknockoutsinLLaMa-vanillaset-
ting jecttoken(LST),FurtherSubsequenttoken(FSST),
Furthertokens(FT),Lasttoken(LT).Thelasttoken
iscrucialtostudy,asitisprojectedontoavocab-
ulary during decoding. The last token residual is
whereinformationgetswrittenduringfactualrecall
(bothRAGandnon-RAG).Thelastsubjecttoken
positionsarehotspotsofparametricknowledgeand
factualrecallinthevanillanon-RAGsetting. Be-
sides, due to causal attention, last subject token
(LST)isequippedwithcontextaboutFirst(FST)
andMiddlesubjecttokens(MST)aswell.Further
Figure6: AttentionknockoutsinPhi-vanillasetting
tokens(FT),FurtherSubsequenttokens(FSST)are
notfoundtohavesignificantcausalimpactinboth
Figure5and6showtheattentionknockouton RAGandthenon-RAGsettings.
the subject token in the vanilla setting. In the ab- InadditiontocausaltracingonMLPs,wealso
senceofaddedRAGcontext,weobservea95per- perform causal tracing on the attention modules,
cent decrease in attribute probability in LLaMa whichwepresentinthissectionin7and8
and nearly a 60 percent decrease in the attribute Weobservefairlysimilartracesforattentionin
probabilityinPhi-2. Intheabsenceofexternalcon- the RAG vs non-RAG settings. The last token is
text,themodelisreliantonparametricmemoryto crucialinbothsettings,thuseffectivelyestablishing
answerthefactualquery,andhencethelargeproba- thatallinformationrequiredforthetaskiswritten
bilitydroponknockingoutsubjecttokenattention. tothelasttoken’sresidualstream,withthesourceFigure8: Phi-2causaltraceonAttention
beingsubjectinthenon-RAGcase,andthesource
beingtheattributetokenintheRAGsetting.
Toapplynoisetothetokenembeddings,weuse
theautomaticsphericalgaussiannoise,thedefault
setting used in (Meng et al., 2022a). The noise
is sampled from a gaussian distribution of mean
0andstandarddeviationν whereν = 3σ, where
σ is the standard deviation of a sample of token
embeddings.