JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 1
ChangeViT: Unleashing Plain Vision Transformers
for Change Detection
Duowang Zhu∗, Xiaohu Huang∗, Haiyan Huang, Zhenfeng Shao†, and Qimin Cheng
Abstract—Changedetectioninremotesensingimagesisessen-
ResNet18 VGG16 UNet ViT-S(DeiT) ViT-S(DINO) ViT-S(DINOv2)
tial for tracking environmental changes on the Earth’s surface.
90
Despite the success of vision transformers (ViTs) as back-
85
bones in numerous computer vision applications, they remain
80
underutilized in change detection, where convolutional neural
networks (CNNs) continue to dominate due to their powerful 75
feature extraction capabilities. In this paper, our study uncovers 70
ViTs’ unique advantage in discerning large-scale changes, a 65
capability where CNNs fall short. Capitalizing on this insight, 60
we introduce ChangeViT, a framework that adopts a plain ViT 55
backbone to enhance the performance of large-scale changes.
50
This framework is supplemented by a detail-capture module LEVIR-CD WHU-CD CLCD
that generates detailed spatial features and a feature injector
(a) Performancecomparisonofdifferentchangedetectors.
that efficiently integrates fine-grained spatial information into
high-level semantic learning. The feature integration ensures
that ChangeViT excels in both detecting large-scale changes 6.00 LEVIR-CD 6.00 WHU-CD 5.12 6.00 CLCD
and capturing fine-grained details, providing comprehensive 4.00 4.00 4.00
c tlh ea s,ng Ce hd anet ge ec Vti io Tn aa cc hr io es vs esdi sv te ar ts ee -os fc -ta hle es -a. rW t i pt eh ro fu ot rmbe al nls cean od n w thh ri es e- 2.00 0.01 0.76 2.00 0.41 1.20 2.00 0.17 0.74 1.81
0.00 0.00 0.00
popular high-resolution datasets (i.e., LEVIR-CD, WHU-CD, -0.02 Size Size Size
-2.00 -2.00 -2.00 -0.92
and CLCD) and one low-resolution dataset (i.e., OSCD), which -2.17 -2.14
underscores the unleashed potential of plain ViTs for change -4.00 -2.87 -4.00 -3.86 -4.00
detection. Furthermore, thorough quantitative and qualitative -6.00 -6.00 -5.05 -6.00
analysesvalidatetheefficacyoftheintroducedmodules,solidify-
(b) Performance comparison between CNN and ViT models for various
ingtheeffectivenessofourapproach.Thesourcecodeisavailable
changesizes.
at https://github.com/zhuduowang/ChangeViT.
Index Terms—Change Detection, Vision Transformer. Fig.1. (a)Performancecomparisonofdifferentchangedetectorsacrossthree
datasets,categorizedasCNN-basedand ViT-basedmodels.(b)Performance
comparison (∆IoU (%)) between a CNN (ResNet18) and a ViT (ViT-S
I. INTRODUCTION
(DINOv2))modelfordetectingchangeswithvarioussizes.Thehorizontalaxis
CHANGE detection plays a crucial role in the field of incrementally reflects the change sizes, progressing from smallest to largest
changes.The∆IoUvaluespresentedarecalculatedbysubtractingtheCNN’s
remote sensing, employing pairs of bi-temporal images
performancefromthatoftheViTforeachsizecategory.
taken of the same geographic area at different times to track
changesontheEarth’ssurfaceovertime[1].Ithasbeenwidely
appliedinvariousapplicationssuchasdisasterassessment[2],
in various computer vision tasks, e.g., object detection [12],
urban planning [3], arable land protection [4], and environ-
imagesegmentation[13],imagematting[14],andposeestima-
mental management [5]. In recent years, convolutional neural
tion[15],whichexhibitsuperiorperformancethanCNN-based
networks (CNNs) have emerged as the primary backbone
methods benefiting from the long-range modeling capability.
choice for state-of-the-art change detectors [2], [6], [7], [8],
While transformers have been explored in the context of
[9], [10], as they can extract rich hierarchical features for
change detection in some preliminary studies [6], [16], [17],
detecting changes with different sizes.
[18], [19], their performance has not yet matched that of the
Over the past few years, Vision Transformers (ViTs) [11]
leading CNN models. Therefore, this paper aims to study the
have de facto substituted CNNs as the dominant backbones
potential benefits of ViTs for change detection, striving to
Duowang Zhu, Haiyan Huang and Zhenfeng Shao are with the unleash their effectiveness in this area.
State Key Laboratory of Information Engineering in Surveying, Map-
To assess the efficacy of ViTs in the change detection task,
ping and Remote Sensing, Wuhan University, Wuhan 430079, China
(email: zhuduowang@whu.edu.cn; huanghaiyan@whu.edu.cn; shaozhen- we first conduct a comprehensive performance comparison
feng@whu.edu.cn). between change detectors utilizing ViTs and three established
XiaohuHuangiswiththeUniversityofHongKong,Pokfulam,HongKong
CNN architectures as backbones, i.e., ResNet18 [20], VGG16
(e-mail:huangxiaohu@connect.hku.hk)
Qimin Cheng is with the School of Electronic Information and Commu- [21], and UNet [22]. This evaluation spans three well-known
nications,HuazhongUniversityofScienceandTechnology,Wuhan430074, datasets, i.e., LEVIR-CD [23], WHU-CD [24], and CLCD
China(e-mail:chengqm@hust.edu.cn).
∗:Equalcontribution. [25], as depicted in Fig. 1(a). Additionally, we explore the
†:Correspondingauthor. influenceofvariousmodelinitializationsbyincorporatingpre-
4202
nuJ
81
]VC.sc[
1v74821.6042:viXra
)%(UoIΔ
)%(UoI
)%(UoIΔ )%(UoIΔJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 2
trained weights from DeiT [26], DINO [27], and DINOv2 effectively unleash the capacity of plain ViTs in the field of
[28] into our analysis. Specifically, ResNet18, VGG16, UNet, change detection.
and ViT-S (DeiT) are pre-trained on ImageNet-1k with su- The main contributions of this paper can be summarized as
pervised training, while ViT-S(DINO) and ViT-S(DINOv2) follows:
are pre-trained with self-supervised training on ImageNet- • We thoroughly investigate the performance of plain
1k, ImageNet-22k, and Google Landmarks, etc. The results ViTs and identify their aptitude for detecting large-
indicate that: (1) CNN models significantly outperform all scale changes. Motivated by this finding, we introduce
ViTs across all datasets, regardless of whether supervised or ChangeViT, a simple yet effective framework which
self-supervised learning is used, highlighting the dominance utilizes plain ViT as the primary feature extractor for the
of CNNs in change detection tasks. (2) Even with identical change detection task.
datainitialization(i.e.,ImageNet-1k),theperformanceofViTs • Toenhancethedetectionofchangesacrossvarioussizes,
remains inferior to that of CNN-based models. we integrate a detail-capture module, specifically intro-
To delve deeper into the models’ capabilities, we perform duced to address the limitations of ViTs when identi-
an in-depth analysis of a ViT (ViT-S with DeiT pre-training) fying small objects. Furthermore, we introduce a feature
and a CNN model (ResNet18 with ImageNet-1k pre-training) injectortomergetheextracteddetailedfeaturesintohigh-
in detecting changes across various object sizes, which is level ones from the ViT, ensuring comprehensive feature
illustrated in Fig. 1(b). We organize the test samples from representation within the model.
each dataset by the proportion of pixels occupied by different • ChangeViT achieves state-of-the-art performance on four
objectswithintheimages.Specifically,wefirstsorttheimages populardatasets,i.e.,LEVIR-CD,WHU-CD,CLCD,and
in ascending order based on the ratio of pixels occupied by OSCD, demonstrating the superiority of the proposed
changing objects to the total number of pixels in the image. method. Moreover, thorough quantitative and qualitative
Then, we evenly divide this ordered sequence into five cate- analyses validate the efficacy of the modules we have
gories,rangingfromthesmallesttothelargestproportions.We introduced, further solidifying the effectiveness of our
calculate the average performance difference between the ViT approach.
and CNN models within each category. The results show that
though ViTs lag behind CNNs in detecting smaller changes,
II. RELATEDWORK
they demonstrate enhanced reliability for larger objects across
A. Change Detection
all datasets. These insights suggest that while ViTs cannot
capturefine-graineddetailsaseffectivelyasCNNs,theyexcel Regarding the network architecture, existing change de-
in detecting large-scale changes. Therefore, this previously tection methods employing deep learning can be generally
untapped benefit has the potential to effectively mitigate the categorized into two groups: CNN-based and transformer-
limitations inherent in CNN architectures. based.
Building upon the insights gathered from our preceding CNN-based Methods. CNN-based change detection ap-
analysis, we propose ChangeViT, a simple yet effective proacheshavebeenthemainstreamframeworkintheliterature
framework that leverages the plain ViT framework as its core [7], [8], [9], [33], [34], [35], [36], [37], [38], [39] for a long
tocapturelarge-scaleobjectinformation.Thisiscoupledwith time, known for their hierarchical feature modeling capabili-
a detail-capture module specifically used to focus on fine- ties.Theseworksprimarilyfocusonmulti-scalefeatureextrac-
grained features. The detail-capture module functions as an tion, difference modeling, lightweight architecture designing,
auxiliary network, incorporating selected layers (C2-C4) from and foreground-background class imbalance. For instance,
ResNet18 [20], which offers a more compact footprint (2.7M methods in [35], [39] utilize fully convolutional networks to
parameters) compared to a complete CNN model (11.2M pa- capture hierarchical features for learning multi-scale feature
rameters). To seamlessly inject these fine-grained details into representations.Foradequatedifferentialfeaturemodeling,ap-
the feature representation of ViTs, we establish connections proaches in [33], [36] incorporate the attention mechanism to
between ViT’s representations and fine-grained features. This establish relational dependencies among bi-temporal features.
integration is accomplished by considering ViT features as In contrast, Changer [34] introduces a parameter-free method,
queries and merging the fine-grained features by applying the which simply exchanges the characteristics of each phase to
cross-attention mechanism. capture and perceive each other’s information. Methods in
Through extensive experiments on four widely recognized [8], [9] focus on designing efficient and effective network
datasets, i.e., LEVIR-CD [23], WHU-CD [24], CLCD [25], architectures,utilizinglightweightfeatureextractors[40],[41]
and OSCD [29], ChangeViT achieves the state-of-the-art as backbones. Several studies [7], [37] address the significant
performance across the board. In addition, we combine the challengeposedbyforeground-backgroundclassimbalanceby
proposed modules with various hierarchical transformers, i.e., developinginnovativelossfunctionsthatprioritizeforeground
Swin Transformer [30], PVT [31], and PiT [32]. Consistently alterations while minimizing interference from background
across these architectures, the proposed modules enhance per- noise (e.g., seasonal variations, climate changes).
formance, thereby further confirming their efficacy. Notably, Transformer-based Methods. Recently, Vision Trans-
despite the plain ViT’s perceived limitations compared to former [11] and its variants [30], [31], [42] have surpassed
theseadvancedhierarchicalnetworks,ChangeViToutperforms CNN in various visual tasks and became the dominant back-
methodsthatutilizethesecomplexmodels,showcasingthatwe bone [12], [14], [15], [43]. Motivated by these achievements,JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 3
Feature
ViT 1/16
Injector
Change Map
1/8
1/4
Detail
1/2
Capture
Loss
Fig. 2. Overview of the proposed ChangeViT. bi-temporal images I1 and I2 are firstly fed into shared ViT to extract high-level semantic features and
detail-capture module to extract low-level detailed information. Subsequently, a feature injector is introduced to inject the low-level details into high-level
features.Finally,adecoderisutilizedtopredictchangedprobabilitymaps.
several works [6], [16], [17], [18], [19], [44], [45] have [43] introduces a pre-training-free adapter that injects prior
explored the application of transformers in change detec- knowledge to ViT without redesigning its architecture for
tion tasks. Some of these methods [18], [45] utilize pure various dense prediction tasks. Similarly, SimpleClick [13]
transformers, while others [6], [16], [17], [44] adopt CNN- and ViTPose [15] apply vanilla ViT as the feature extractor
Transformer hybrid architectures. Methods in [18], [45] in- to acquire single-scale features. For image matting, ViTMatte
troduce hierarchical transformer networks based on the swin [14] is the first work to unleash the potential of ViT with
transformer [30]. The others typically follow a paradigm in concise adaption.
which features extracted by CNN serve as semantic tokens, Inspiredbytheaboveworks,weaimtounleashthepotential
followedbycontextualrelationmodelingbetweenbi-temporal of the plain ViT model, enabling it to adapt well to change
tokens using transformer blocks. The method introduced in detection tasks.
[19]exhibitsanefficienttuningstrategythatinvolvesfreezing
the parameters of the Transformer encoder while introducing III. PROPOSEDMETHOD
additional trainable parameters. However, this method fails to
The overall architecture is illustrated in Fig. 2. For the bi-
deliveroptimalresultsduetoaninadequateexplorationofthe
temporal images I ∈ RH×W×3 and I ∈ RH×W×3, they
strengths and limitations of the transformers. This precludes a 1 2
are parallelly fed into a ViT and a detail-capture module. The
more effective application of the model’s capabilities, thereby
capping the potential gains in performance.
ViT extracts high-level features F Vt ∈R 1H 6×W 16×C4, where t∈
{1,2} represents two phases, while the detail-capture module
Different from the previous approaches mainly using hi-
erarchical networks, the proposed ChangeViT applies the
acquires fine-grained multi-scale features F Ct
i
∈ R 2H i×W 2i×Ci
(i ∈ {1,2,3}, t ∈ {1,2}). To enhance the detection of
plain ViT as the cornerstone feature extractor, which we find
intricate details within high-level features, we introduce a
has previously unidentified potential in detecting large-scale
feature injector aimed at integrating low-level fine-grained
changes.
information into F . Finally, a multi-scale feature fusion
V
decoder is applied to predict the changed probability map
B. Plain ViT for Downstream Tasks P ∈RH×W×1.
ViT[11]isaplain,non-hierarchicalarchitecture,whichisa
powerfulalternativetostandardCNNforimageclassification. A. Feature Extraction
Duetothesignificantcomputationaloverheadofself-attention The feature extractor is composed of a plain ViT, and a
in ViT, subsequent works focus on designing more efficient detail-capture module which is described as follows:
architectures,suchasSwin[30],PVT[31]andPiT[32].These Plain ViT. Bi-temporal images I , I are fed into patch
1 2
works inherit some designs from CNN, including hierarchical embedding layer, dividing them into non-overlapping 16 ×
structures, sliding windows, and convolutions. Recently, re- 16 patches. These patches are then flattened and projected to
searchers have begun to study the potential of ViT for various D-dimension tokens, and the feature resolution is reduced to
downstream tasks motivated by the emergence of large pre- 1/16oftheoriginalimages.Afterwards,positionembeddingis
trained models, e.g., DeiT [26], DINO [46], DINOv2 [28], addedtothesetokens,whicharepassedthroughLtransformer
MAE [47] and CLIP [48]. The plain ViT has already made layers. Each layer consists of a layer normalization (LN), a
remarkable progress in dense prediction [12], [13], [43], pose multi-headself-attention(MHSA)andafeed-forwardnetwork
estimation [15], image matting [14], etc. ViTDet [12] is the (FFN). The formulation of these layers is given by Eq. 1 and
first to employ plain, non-hierarchical ViT as the backbone Eq. 2:
for object detection with minimal adaptation, i.e., building a F′t,i+1 =Ft,i+MHSA(LN(Ft,i)), (1)
V V V
simple feature pyramid for single-scale features and aiding a
few cross windows for information propagation. ViT-Adapter Ft,i+1 =F′t,i+1+FFN(LN(F′t,i+1)). (2)
V V V
redoceDJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 4
where i denotes the output of the ith transformer layer. The
final output of the ViT backbone is represented as Ft ∈
V Q
R 1H 6×W 16×C4, where C
4
equals to D .
Detail-capture. As discussed in Sec. I, ViT demonstrates
proficiency in detecting large changes but exhibits reduced
K,V
effectiveness with smaller ones. Addressing this challenge,
K,V
we introduce a detail-capture module designed to compensate
K,V Cross-Attn
for the absence of fine-grained local cues crucial for change
Block
detection. This module comprises three residual convolutional
blocks(C2-C4)adaptedfromResNet18[20].Uponprocessing
(a) Using ViT’s features as query and detailed features as key and value for
theinputimagesthroughthedetail-capturemodule,three-scale featureinjector.
detailedfeaturesaregenerated,i.e.,1/2,1/4and1/8,denoted
as F Ct
i
∈R 2H i×W 2i×Ci (i∈{1,2,3}).
K,V
B. Feature Injector
In the change detection task, preserving detailed spatial Q
features is crucial as they can help detect small objects.
Q
Ensuring the effective transmission of low-level details to
Q Cross-Attn
high-level semantic features is paramount. Block
Therefore, we introduce a feature injector, composed of
three cross-attention blocks [49], as illustrated in Fig. 3(a). It (b) Using ViT’s features as key and value and detailed features as query for
considers the low-level features as the key and value vectors featureinjector.
andthehigh-levelfeatureasthequeryvector.Intuitively,thisis
reasonable as it allows the feature injector to gather the most sF ci ag l. e3. deI tl al iu ls et dra fti eo an tuo ref sth ae cqfe ua it ru er de fi rn oj mecto thr es. dF eC tai il( -i ca∈ pt{ u1 re,2 m,3 o} du) ld ee ,n wot he ilm eu Flt Vi-
relevant information based on the provided key information denotestheViT’sfeaturelackingdetailedinformation.(a)LetFV asthequery
andintegrateitintothequery.Byenablingcross-layerfeature
vector,andFCi asthekeyandvaluevectorstocapturedetailedfeaturesfor
propagation, detailed information can be incorporated into the
tV oiT re. fi(b n) eU fes ain tug reF sV foa rs Vth iTe .keyandvaluevectors,andFCi asthequeryvector
high-level representations of the ViT, denoted as Ft . The
Ft is computed as follows:
VE
VE where F it ∈ {F Ct 1,F Ct 2,F Ct 3,F Vt E} (t ∈ {1,2}), MLP is a
Ft,i =CrossAttn(Ft,Ft ), (3) three-layer2Dconvolutionalnetworkwithkernelsizeof3×3
VE V Ci
alongwithReLUactivationfunction,⃝c denotesconcatenating
Ft =FC(Ft,1⃝cFt,2⃝cFt,3). (4) onchanneldimension,and|·|meansabsolutevalueoperation.
VE VE VE VE
To restore the original resolution of the changed map,
where i∈{1,2,3} denotes the index of low-level layers, Ft
V we use a simple cascade upsampling operation, which is
as query and Ft as key and value, respectively. The FC is a
Ci represented as follows:
2Ddepth-wiseconvolutionwiththekernelsizeof1×1and⃝c
denotes concatenation operation along the channel dimension. F Di+1 ←Deconv 4×4(Conv 1×1(F Di))+F Di+1, (6)
Additionally, we explore an alternative approach to feature
whereConv isa2Dconvolutionwithakernelsizeof1×1
1×1
injector,asdepictedinFig.3(b),whichconsidersthelow-level
to reduce the channel dimension, and Deconv denotes 2D
4×4
features as query, and the ViT’s semantic information as key
deconvolution to upsample the feature map with a kernel size
and value to refine the ViT’s representation according to the
of 4×4 and stride size of 2×2.
characteristics of the hierarchical detailed features.
Finally,aclassificationlayerisappliedtotransformtheshal-
lowestfeaturesF intochangemapsP,whichisformulated
D4
C. Decoder and Optimization as Eq. 7:
P =Sigmoid(Conv (F )). (7)
Compared to existing methods [6], [7], [36], [44], which 3×3 D4
employ complex techniques to model difference information where Conv is a 2D convolution with the kernel size of
3×3
and predict the change probability map, we chose a simpler 3×3, Sigmoid function maps the feature map to (0,1) and
decoder to better demonstrate the learning capabilities of then transforms to a binary map given a predefined threshold
ChangeViT. Specifically, we use a straightforward feature fu- (i.e., 0.5), i.e., P ∈{0,1}H×W.
sionlayertocapturedifferencesbetweenbi-temporalfeatures. As mentioned in prior works [7], [9], the proportion of
A cascade convolutional layer, followed by an upsampling changed targets is significantly lower than that of unchanged
operation, is employed to progressively aggregate differential ones. Following the above works, we adopt binary cross-
featuresfromdeeptoshallowlayers,ultimatelyrestoringthem entropy (BCE) and dice loss (Dice) [50] to alleviate the class
to the original resolution of H×W. The difference modeling imbalanceproblem.ThechangedetectionlossL isdefined
total
is formulated as Eq. 5: as Eq. 8:
F =MLP(F1⃝cF2⃝c|F1−F2|), (5) L =L (P,Y)+L (P,Y), (8)
Di i i i i total bce dice
CF
CFJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 5
The BCE and Dice losses are formulated as follows: 256×256 patches. Consequently, the dataset is divided into
7120 pairs for training, 1024 pairs for validation, and 2048
pairs for testing.
N
1 (cid:88)
L (P,Y)=− [Y log P +(1−Y )log (1−P )], 2) WHU-CD: This publicly available dataset [24] focuses
bce N i 2 i i 2 i
i=1 onbuildingchangedetectionandincludeshigh-resolution(0.2
2(cid:80)N P Y +ϵ m) bi-temporal aerial images, totaling 32507×15354 pixels.
L dice(P,Y)=1− (cid:80)N
(P
)2i= +1 (cid:80)i Ni
(Y
)2+ϵ. (9) It primarily encompasses areas affected by earthquakes and
i=1 i i=1 i subsequent reconstruction, mainly involving building renova-
whereidenotesthei-thpixel,N isthenumberoftotalpixels, tions. Adhering to the standard procedure detailed in [33],
Y denotes the ground truth and ϵ (i.e., 1e-5) is a smooth term the dataset images are divided into 256×256 non-overlapping
utilized to avoid zero division. patches. The dataset is partitioned into 5947 training pairs,
744 validation pairs, and 744 test pairs.
IV. EXPERIMENTS 3) CLCD: The CLCD [25] dataset consists of cropland
change samples, including buildings, roads, lakes, etc. The
We conducted extensive experiments on three widely used
bi-temporal images in CLCD were collected by Gaofen-2 in
high-resolution datasets, namely LEVIR-CD [23], WHU-CD
Guangdong Province, China, in 2017 and 2019, respectively,
[24], and CLCD [25], as well as one challenging low-
withspatialresolutionsrangingfrom0.5to2m.Followingthe
resolution dataset, OSCD [29], to demonstrate the effective-
standard procedure detailed in [6], each image in the dataset
ness of the proposed method. To better understand each
is segmented into 256×256 patches. Consequently, the CLCD
componentofChangeViT,weconductextensivediagnosticex-
dataset is divided into 1440, 480, and 480 pairs for training,
perimentsinSec.IV-E.Otherwisestated,weuseChangeViT-S
validation, and testing, respectively.
for experiments on the three high-resolution datasets.
4) OSCD: The OSCD dataset [29] is a relatively low-
resolutiondatasetwitharesolutionrangingfrom10mto60m.
A. Implementation Details
ItwascapturedbytheSentinel-2satellitesinvariouscountries
We adopt vanilla ViT [11] as our primary backbone, withdifferentlevelsofurbanizationandhasexperiencedurban
specifically incorporating its tiny and small variants, thereby growth or changes. This resolution enables the detection of
constructingtwomodelsnamedChangeViT-TandChangeViT- large buildings in the image pairs. However, smaller changes
S.WeuseDeiT[26]andDINOv2[28]pre-trainedweightsfor such as the appearance of small buildings, extensions of
initialization, respectively. Our models are implemented using existing buildings, or additions of lanes to roads may not be
the PyTorch framework [51] and executed on a computing obvious, making diverse change detection challenging. The
platform consisting of a single NVIDIA GeForce RTX 3090 dataset consists of 24 regions of approximately 600×600
GPU paired with an Intel(R) Xeon(R) Gold 6138 CPU. For pixels. In accordance with common practice, each image in
optimization, we opt for the Adam optimizer [52], with beta the dataset is cropped into 256×256 patches. As a result, the
values set to (0.9, 0.99) and a weight decay of 1e-4. Initially, OSCD dataset is divided into 75 training pairs and 28 test
the learning rate is 2e-4 and gradually reduces according to pairs.
a scheduled reduction formula: (1-(curr iter/max iter))α× lr,
where α is set to 0.9 and max iter is set to 80K iterations on
LEVIR-CD and WHU-CD, 40K for CLCD dataset, and 10K C. Evaluation Metrics and Compared Methods
for OSCD, respectively. The batch size remains at 16 across
1) Evaluation Metrics: Following the widely used evalu-
all experiments. To augment the training data and bolster the
ation protocols in the change detection task, we use three
model’s robustness, we apply random flipping and cropping
accuracy metrics, i.e., F1 score (F1), intersection over union
dataenhancementapproaches.ThechanneldimensionsofF
Ci (IoU) and overall accuracy (OA), to evaluate our proposed
are set to 64, 128, and 256, respectively. Furthermore, we
method. They as formulated as follows:
ensureconsistencyandfairnessincomparisonbymeticulously
aligning the experimental settings of the compared methods TP
P = ,
with those specified in the original paper. TP +FP
TP
R= ,
TP +FN
B. Datasets
2PR
1) LEVIR-CD: This dataset [23] comprises 637 high- F1= ,
P +R
resolution (1024×1024, 0.5 m/pixel) bi-temporal image pairs,
TP
sourced from Google Earth. The images represent 20 diverse IoU = ,
TP +FN +FP
regions across various Texan cities, including Austin, Lake-
TP +TN
way, Bee Cave, Buda, Kyle, Manor, Pflugervilletx, Dripping OA= . (10)
TP +TN +FN +FP
Springs, and others. The dataset, with annotations for 31333
individualbuildingchanges,spansimagescapturedfrom2002 whereTP,FP,TN,andFNindicatetruepositive,falsepositive,
to2018invariouslocations.Followingthecroppingmethodol- true negative, and false negative, respectively. For all the
ogyestablishedin[6],eachimageissegmentedinto16distinct metrics, a higher value means better detection performance.JOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 6
TABLEI
PERFORMANCECOMPARISONOFDIFFERENTCHANGEDETECTIONMETHODSONLEVIR-CD,WHU-CD,ANDCLCDDATASETS,RESPECTIVELY.THE
BESTRESULTSAREHIGHLIGHTEDINBOLDANDTHESECONDBESTRESULTSAREUNDERLINED.ALLRESULTSOFTHETHREEEVALUATIONMETRICS
AREDESCRIBEDASPERCENTAGES(%).
LEVIR-CD WHU-CD CLCD
Method #Params(M) FLOPs(G)
F1 IoU OA F1 IoU OA F1 IoU OA
DTCDSCN[38] 41.07 20.44 87.43 77.67 98.75 79.92 66.56 98.05 57.47 40.81 94.59
SNUNet[53] 12.04 54.82 88.16 78.83 98.82 83.22 71.26 98.44 60.82 43.63 94.90
ChangeFormer[18] 41.03 202.79 90.40 82.48 99.04 87.39 77.61 99.11 61.31 44.29 94.98
BIT[16] 3.55 10.63 89.31 80.68 98.92 83.98 72.39 98.52 59.93 42.12 94.77
ICIFNet[33] 23.82 25.36 89.96 81.75 98.99 88.32 79.24 98.96 68.66 52.27 95.77
DMINet[36] 6.24 14.42 90.71 82.99 99.07 88.69 79.68 98.97 67.24 50.65 95.21
GASNet[7] 23.59 23.52 90.52 83.48 99.07 91.75 84.76 99.34 63.84 46.89 94.01
AMTNet[6] 24.67 21.56 90.76 83.08 98.96 92.27 85.64 99.32 75.10 60.13 96.45
EATDer[44] 6.61 23.43 91.20 83.80 98.75 90.01 81.97 98.58 72.01 56.19 96.11
ChangeViT-T 11.68 27.15 91.81 84.86 99.17 94.53 89.63 99.57 77.31 63.01 96.67
ChangeViT-S 32.13 38.80 91.98 85.16 99.19 94.84 90.18 99.59 77.57 63.36 96.79
2) Compared Methods: To verify the effectiveness of the TABLEII
proposed method, nine representative and open-source meth- PERFORMANCECOMPARISONOFDIFFERENTCHANGEDETECTION
METHODSONTHEOSCDDATASET.THEBESTRESULTSARE
ods are selected for comparison which are described as fol-
HIGHLIGHTEDINBOLDANDTHESECONDBESTRESULTSARE
lows: UNDERLINED.ALLRESULTSOFTHETHREEEVALUATIONMETRICSARE
DESCRIBEDASPERCENTAGES(%).
a) DTCDSCN [38]: A dual task-constrained deep siamese
convolutional network is introduced which can accomplish
OSCD
change detection and semantic segmentation. It applies chan- Method
F1 IoU OA
nel and spatial attention to improve the interactive feature DTCDSCN[38] 36.13 22.05 94.50
representation. SNUNet[53] 27.02 15.62 93.81
ChangeFormer[18] 38.22 23.62 94.53
b) SNUNet [53]: The bi-temporal differential features are
BIT[16] 29.58 17.36 90.15
extracted by the densely connected siamese network which ICIFNet[33] 23.03 13.02 94.61
focuses not only on high-level semantic features but also on DMINet[36] 42.23 26.76 95.00
the low-level fine-grained features. GASNet[7] 10.71 5.66 91.52
AMTNet[6] 10.25 5.40 94.29
c) ChangeFormer [18]: Multi-scale long-range features are EATDer[44] 54.23 36.98 93.85
extracted by a hierarchical swin-transformer encoder and de- ChangeViT-T 55.13 38.06 95.01
coder with a multi-layer perception. ChangeViT-S 55.51 38.42 95.05
d) BIT [16]: The bi-temporal images are represented as
semantic tokens, then using a transformer encoder to model
contexts and a transformer decoder to refine the context-rich D. Comparison with State-of-the-Art Approaches
tokens.
As illustrated in Tab. I, we compare ChangeViT with
e) ICIFNet [33]: An intra-scale cross-interaction and inter- the previous methods on three high-resolution datasets, i.e.,
scale feature fusion network that jointly captures spatio- LEVIR-CD,WHU-CD,andCLCD.Notably,allthecompared
temporal contextual information and obtains short-long range methods employed hierarchical backbone as primary feature
representations of bi-temporal features. extractors. Specifically, DTCDSCN, BIT, ICIFNet, DMINet,
f) DMINet [36]: An inter-temporal joint-attention module GASNet and AMTNet apply ResNet [20] or its variants [54]
whichconsistsofself-attentionandcross-attentionblock,aims as backbones, SNUNet and EATDer apply nested UNet [22]
to model the global relations of input images. and stack non-local blocks [55], respectively. In contrast,
our approach employs a non-hierarchical, plain ViT as the
g) GASNet [7]: This is a CNN-transformer model that
core backbone which includes ViT-T and ViT-S, coupling
uses CNN as the backbone to extract multi-scale features
with a lightweight detail-capture module which serves as
andemploystransformerencoder-decodertomodelcontextual
an auxiliary network. From Tab. I, we can summarize the
information.
following valuable findings: (1) ChangeViT consistently out-
h) AMTNet [6]: A global-aware network that models rela-
performs the existing works across all datasets and evaluation
tions between scene and foreground, is proposed to solve the
metrics, despite utilizing the tiny backbone of ViT, which
class imbalance problem of change detection task.
demonstrates its effectiveness. (2) The primary feature ex-
i) EATDer [44]: An edge-assisted detector incorporates an tractor in ViTs, despite being non-hierarchical, demonstrates
edge-awaredecodertointegratetheedgeinformationobtained competitiveperformancewhencomparedtohierarchical-based
by the encoder, thereby enhancing the feature representation methods. This underscores the robust feature extraction and
of changed regions. representationcapabilitiesthatlarge-scalepre-trainingViTcanJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 7
TABLEIII
STUDYTHEEFFECTIVENESSOFPROPOSEDMODULESWITHDIFFERENTTRANSFORMERARCHITECTURESONTHREEDATASETS,RESPECTIVELY.THE
CHECKMARK(✓)DENOTESCOMBININGWITHOURPROPOSEDMODULES.ALLRESULTSAREDESCRIBEDASPERCENTAGES(%).
LEVIR-CD WHU-CD CLCD
Backbone Ours
F1 IoU OA F1 IoU OA F1 IoU OA
89.40 80.83 98.94 93.03 86.98 99.22 73.80 58.47 96.33
Swin-S
✓ 90.18 82.11 99.01 94.04 88.75 99.30 75.41 60.52 96.40
84.60 73.31 98.38 87.36 77.55 98.89 70.25 54.15 95.76
PVT-S
✓ 87.26 77.40 98.68 89.09 80.32 98.92 71.95 56.19 95.90
84.94 73.83 98.38 87.34 77.53 98.89 70.01 53.86 95.88
PiT-S
✓ 87.20 77.31 98.63 89.50 81.00 98.94 72.80 57.23 95.93
82.39 70.05 98.25 84.70 73.46 98.82 69.05 52.74 95.75
ViT-S
✓ 91.98 85.16 99.19 94.84 90.18 99.59 77.53 63.30 96.76
offer, fully realizing its potential. (3) Notably, ChangeViT-T When integrated with our proposed modules, all transformers
and ChangeViT-S exhibit significant performance gains over exhibit performance improvements, indicating the efficacy of
theSOTAmethod(i.e.,AMTNet)by3.99%and4.54%IoUon ourapproachregardlessoftransformerarchitectures.(3)ViT-S
the WHU-CD dataset. This finding is sensible given that the achievessignificantperformancegainsoverhierarchicaltrans-
changes in WHU-CD vary widely, with fewer medium-sized formers when equipped with our proposed modules, suggest-
objects compared to smaller and larger ones. This observation ing that our modules effectively mitigate ViT-S’s limitations
aligns with the results illustrated in the middle of Fig. 1(b), in capturing detailed information to detect smaller objects.
underscoringtheefficacyofourproposedmethodincapturing Effectiveness of proposed modules. To investigate the
globalfeaturesandextractingfine-grainedspatialinformation. effectiveness of the proposed modules, we conduct compre-
(4) With an increase in the size of the primary feature extrac- hensive diagnostic experiments on three datasets. As shown
tor, ChangeViT demonstrates enhanced performance. Notably, in Tab. IV, we take various combinations of components
the detail-capture module, comprising just 2.7M parameters, into account and explore the contribution of each module.
standsoutforitslightweightnaturewhencomparedtothetotal We apply ViT as baseline, which consists of a plain ViT
parameter count of each model (i.e., 11.68M and 32.13M). and a decoder. Coupling with detail-capture module, ViT can
OurproposedChangeViTachievesasuperiorbalancebetween unleash its potential and improve 8.81%, 8.60%, and 6.18%
efficiency and effectiveness compared to previous methods, F1onthreedatasetscomparedtothebaseline,whichindicates
underscoring its superiority. that the detail-capture module can supplement the detailed
As shown in Tab. II, we also compare ChangeViT with spatialinformation,whichisessentialforthechangedetection
several existing methods on the low-resolution dataset, i.e., task. Furthermore, when combined with the feature injector,
OSCD. The targets in the OSCD are relatively smaller than there are additional performance gains of 0.78%, 1.54%, and
thoseinhigh-resolutiondatasets,exacerbatingtheforeground- 2.17% in F1, indicating the effectiveness of incorporating
background imbalance issue and making it challenging to detailed information at higher levels. In summary, all of our
detect smaller targets. From Tab. II, the following key points proposedmodulesareessentialandeffectiveintheChangeViT
can be noted: (1) The proposed ChangeViT outperforms all framework.
compared methods across three evaluation metrics, despite Impact of multiple scales. To investigate the necessity
utilizing tiny or small models of ViT, demonstrating its ef- of capturing multiple scales in the detail-capture module,
fectiveness on the low-resolution dataset. (2) GASNet and we conduct experiments using multi-scale features, i.e., 1/2,
AMTNet perform poorly on this dataset, likely due to their 1/4, 1/8. As shown in Tab. V, we can get the following
inefficiency in detecting small targets. Although GASNet key observations: (1) Single-scale features often yield subpar
introduces a foreground-awareness module to address the results, while the amalgamation of multi-scale features leads
category imbalance between the foreground and background, to enhanced performance. (2) An interesting finding is that
it still underperforms in detecting changes in low-resolution high-level features or their combinations can achieve better
remote sensing images. performance than low-level features. (3) Furthermore, the in-
clusionofthree-scalefeaturesresultsinmutualimprovements,
indicatingthatmulti-scalefeaturesleveragespatialcuesacross
E. Diagnostic Study
complementary levels.
Effectiveness with different architectures. In Tab. III, Impact of pre-trained weights. To investigate the im-
we investigate the effectiveness of the proposed modules pact of pre-trained weights on ChangeViT, we apply various
with different architectures, including hierarchical (i.e., Swin- model initialization approaches, including random initializa-
S [30], PVT-S [31], PiT-S [32]) and non-hierarchical (i.e., tion and several publicly available large-scale pre-trained
ViT-S [11]) transformers. Key observations from the table weights derived from both supervised and self-supervised
include: (1) Without combining with our proposed modules, trainingstrategiesonvariousdatasets.AsillustratedinTab.VI,
the non-hierarchical ViT-S underperformers the other hier- we observed the following key points: (1) Both ChangeViT-
archical methods across all metrics on three datasets. (2) T and ChangeViT-S exhibit improved detection accuracyJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 8
TABLEIV
STUDYTHEEFFECTIVENESSOFPROPOSEDMODULESINCHANGEVITONTHREEDATASETS,RESPECTIVELY.DCANDFIDENOTETHEDETAIL-CAPTURE
MODULEANDFEATUREINJECTOR,RESPECTIVELY.ALLRESULTSAREDESCRIBEDASPERCENTAGES(%).
Model LEVIR-CD WHU-CD CLCD
ViT DC FI F1 IoU OA F1 IoU OA F1 IoU OA
✓ 82.39 70.05 98.25 84.70 73.46 98.82 69.18 52.88 95.68
✓ 88.12 78.76 98.80 90.20 82.15 99.24 69.72 53.51 95.98
✓ ✓ 91.20 83.82 99.11 93.30 87.43 99.46 75.36 60.46 96.62
✓ ✓ ✓ 91.98 85.16 99.19 94.84 90.18 99.59 77.53 63.30 96.76
TABLEV
INVESTIGATIONONTHEIMPACTOFMULTIPLESCALESINTHEDETAIL-CAPTUREMODULEONTHREEDATASETS,RESPECTIVELY.ALLRESULTSARE
DESCRIBEDASPERCENTAGES(%).
Scales LEVIR-CD WHU-CD CLCD
1/8 1/4 1/2 F1 IoU OA F1 IoU OA F1 IoU OA
✓ 91.32 84.03 99.12 94.20 89.04 99.55 76.43 61.85 96.49
✓ 91.08 83.62 99.10 92.90 86.74 99.45 73.25 57.79 96.31
✓ 89.43 80.89 98.94 90.87 83.28 99.29 70.82 54.82 95.79
✓ ✓ 91.56 84.43 99.15 94.25 89.07 99.58 77.30 63.07 96.62
✓ ✓ 91.45 94.24 99.14 94.02 88.67 99.44 77.10 62.73 96.56
✓ ✓ 90.94 83.39 99.09 92.49 86.04 99.41 75.22 60.28 96.46
✓ ✓ ✓ 91.98 85.16 99.19 94.84 90.18 99.59 77.53 63.30 96.76
TABLEVI
STUDYTHEIMPACTOFDIFFERENTPRE-TRAINEDWEIGHTSOFVITONTHREEDATASETS,RESPECTIVELY.ALLRESULTSAREDESCRIBEDAS
PERCENTAGES(%).
Training LEVIR-CD WHU-CD CLCD
Model Backbone Pretrain Pre-trainedData
Strategy F1 IoU OA F1 IoU OA F1 IoU OA
DMINet[36] ResNet18 - ImageNet(1k) Supervised 90.71 82.99 99.07 88.69 79.68 98.97 67.24 50.65 95.21
GASNet[7] ResNet34 - ImageNet(1k) Supervised 90.52 83.48 99.07 91.75 84.76 99.34 63.84 46.89 94.01
AMTNet[6] ResNet50 - ImageNet(1k) Supervised 90.76 83.08 98.96 92.27 85.64 99.32 75.10 60.13 96.45
RandomInit - - 91.58 84.47 99.15 93.78 88.29 99.51 76.91 62.49 96.66
ChangeViT-T ViT(Tiny)
DeiT-T[26] ImageNet(1k) Supervised 91.81 84.86 99.17 94.53 89.63 99.57 77.31 63.01 96.67
RandomInit - - 90.82 83.19 99.09 93.65 88.06 99.50 75.05 60.06 96.59
DeiT-S[26] ImageNet(1k) Supervised 91.78 84.81 99.17 94.73 89.99 99.58 77.24 62.69 96.68
ChangeViT-S ViT(Small) DINO-S[27] ImageNet(w/olabels) Self-supervised 91.68 84.64 99.16 94.70 89.94 99.58 77.05 62.67 96.65
ImageNet(1k,22k)&
DINOv2-S[28] Self-supervised 91.98 85.16 99.19 94.84 90.18 99.59 77.53 63.30 96.76
GoogleLandmarks
when utilizing pre-trained weights compared to random ini- Sizeofchangesv.s.performance.AsdepictedinFig.4(a),
tialization. (2) DINOv2-S provides the most effective pre- we conduct experiments on three datasets using the detail-
trained weights for the ChangeViT-S model, benefiting from capture module, ViT-S, and our proposed method to quan-
large-scale data pre-training. (3) When DMINet, GASNet, titatively analyse the performance of each method under
AMTNet, and ChangeViT are pre-trained on the same data, different change sizes. The detail-capture module and ViT-S
i.e., ImageNet-1k, the proposed ChangeViT outperforms all bothintegratewithadecoderwhichisthesameasChangeViT.
the CNN-based methods, demonstrating the effectiveness of Theresultsindicatethatthedetail-capturemoduleexcelsatde-
transferring the priorities of large pre-trained ViT models to tecting smaller changed targets, while the ViT-S demonstrates
the change detection task. superiority in detecting larger ones. Our method capitalizes
on ViT’s powerful feature expression while leveraging a
Choice of query, key and value. Two experiments are
detail-capture module for fine-detail information mining. This
conducted to investigate different modeling approaches in the
comprehensive approach enables superior performance across
feature injector, as shown in Tab. VII. In the first experiment,
targets of all sizes.
F serves as query, and F serves as key and value, yielding
V C
the best performance. This result is consistent with the con- Qualitative results.Wepresentrepresentativevisualization
jecture mentioned in Sec. III-B, suggesting that the feature results on three datasets, comparing the performance of the
injector effectively captures low-level value information most detail-capture module, ViT-S, and our proposed method to
relevant to the high-level query and reintegrates it back to demonstrate the effectiveness of ChangeViT. As shown in
the query. Therefore, through cross-attention, high-level fine- Fig. 4 (b), the first row in each dataset presents the test
grainedfeaturescanseamlesslymergewithlow-levelfeatures. results for smaller targets, while the second row correspondsJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 9
𝑰𝟏 𝑰𝟐 GroundTruth ResNet18 ViT-S ChangeViT-S(Ours)
ResNet18 ViT-S ChangeViT-S(Ours)
100
90
80
70
60
50
40
LEVIR-CD Size
100
90
80
70
60
50
40
30
20
WHU-CD Size
80
70
60
50
40
30
20
10
0
CLCD Size
(a)Size of changes v.s. performance. (b)Qualitative comparison of detecting changes with different sizes.
Fig.4. (a)Eachdatasetissplitintofiveintervalsonaveragebasedonthechangesizes.Thehorizontalaxisincrementallyreflectsthechangesizes,progressing
fromsmallertolargerchanges.(b)Thepredictedmapwithintheredboxindicatesapoordetectionoutcome.
TABLEVII
STUDYTHEIMPACTOFDIFFERENTMODELINGAPPROACHESINTHEFEATUREINJECTORONTHREEDATASETS,RESPECTIVELY.ALLRESULTSARE
DESCRIBEDASPERCENTAGES(%).
LEVIR-CD WHU-CD CLCD
Query Key&Value
F1 IoU OA F1 IoU OA F1 IoU OA
FV FC 91.98 85.16 99.19 94.84 90.18 99.59 77.53 63.30 96.76
FC FV 91.78 84.80 99.17 94.60 89.75 99.58 75.84 61.08 96.58
to larger targets. From the results, we can see that the detail- features into ViT’s high-level semantic representations, en-
capture module excels at detecting smaller targets, whereas hancing ChangeViT’s capability to detect changes of diverse
ViT-S demonstrates superiority in detecting larger targets. sizes. (2) In detecting dense objects, regardless of their size,
The fundamental distinction lies in the local receptive field ChangeViT consistently delineates clear boundaries compared
of CNN, enabling them to extract intricate local features, to prior methods. This underscores ChangeViT’s effectiveness
while ViT possesses a global receptive field, facilitating the incapturingbothglobalsemanticinformationandlocalspatial
extractionofcomprehensiveglobalinformation.Theproposed details of neighboring objects.
method efficiently integrates global and local information,
resulting in superior performance. V. CONCLUSION
Toqualitativelycomparewithpreviousmethods,weprovide In this paper, we present a simple yet effective frame-
comprehensivesamplesencompassingsmall,large,sparse,and work, namely ChangeViT, that leverages the plain ViT as
dense targets, as illustrated in Fig. 5. From these samples, its primary feature extractor to capture large-scale changes.
several key observations emerge intuitively: (1) Our proposed Coupledwithadetail-capturemodulededicatedtofine-grained
methodconsistentlyoutperformsallcomparedmethodsacross spatialfeatures,ChangeViTseamlesslyintegratesthesedetails
various change sizes. This is attributed to the robust global into ViT’s feature representation through the cross-attention
modeling capabilities of ViT and the detail-capture module’s mechanism. Experimental results demonstrate ChangeViT’s
capacity to extract intricate spatial information. Additionally, supremacy over meticulously designed hierarchical models
a feature injector integrates low-level fine-grained spatial across all evaluation metrics on four widely adopted datasets,
)%(UoI
)%(UoI
)%(UoI
DC-RIVEL
DC-UHW
DCLCJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 10
𝑰 𝟏 𝑰 𝟐 Ground Truth
D
T C D S C N SN U Net ChangeFormer BIT ICIFNet D MINet GASNet AMTNet EATDer ChangeViT-S
Fig. 5. Qualitative comparison of different methods on the three datasets. White represents a true positive, black is a true negative, green indicates a false
positive,andredisafalsenegative.Fewergreenandredpixelsrepresentbetterperformance.Forbetterclarity,pleasezoominonthefigure.
highlighting the untapped potential of vanilla ViTs for change semantic change detection framework: From natural disasters to man-
detection. Furthermore, comprehensive diagnostic analyses made disasters,” Remote Sensing of Environment, vol. 265, p. 112636,
2021.
and visualization results provide insights into the contribution
[3] S.W.Wang,L.Munkhnasan,andW.-K.Lee,“Landuseandlandcover
of each module. We aim for this study to offer valuable in- changedetectionandpredictioninbhutan’shighaltitudecityofthimphu,
sightstotheresearchcommunityandignitefurtherexploration usingcellularautomataandmarkovchain,”EnvironmentalChallenges,
vol.2,p.100017,2021.
into leveraging vanilla ViTs for other related computer vision
[4] R. S. Lunetta, J. F. Knight, J. Ediriwickrema, J. G. Lyon, and L. D.
tasks, such as change caption.
Worthy,“Land-coverchangedetectionusingmulti-temporalmodisndvi
data,” in Geospatial Information Handbook for Water Resources and
WatershedManagement,VolumeII. CRCPress,2022,pp.65–88.
REFERENCES [5] R.E.Kennedy,P.A.Townsend,J.E.Gross,W.B.Cohen,P.Bolstad,
Y. Wang, and P. Adams, “Remote sensing change detection tools
[1] R. J. Radke, S. Andra, O. Al-Kofahi, and B. Roysam, “Image change for natural resource managers: Understanding concepts and tradeoffs
detectionalgorithms:asystematicsurvey,”IEEEtransactionsonimage in the design of landscape monitoring projects,” Remote sensing of
processing,vol.14,no.3,pp.294–307,2005. environment,vol.113,no.7,pp.1382–1396,2009.
[2] Z. Zheng, Y. Zhong, J. Wang, A. Ma, and L. Zhang, “Building [6] W.Liu,Y.Lin,W.Liu,Y.Yu,andJ.Li,“Anattention-basedmultiscale
damageassessmentforrapiddisasterresponsewithadeepobject-based transformernetworkforremotesensingimagechangedetection,”ISPRS
DC-RIVEL
DC-UHW
DCLCJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 11
JournalofPhotogrammetryandRemoteSensing,vol.202,pp.599–609, Learning robust visual features without supervision,” arXiv preprint
2023. arXiv:2304.07193,2023.
[7] R.Zhang,H.Zhang,X.Ning,X.Huang,J.Wang,andW.Cui,“Global- [29] R.C.Daudt,B.LeSaux,A.Boulch,andY.Gousseau,“Urbanchange
awaresiamesenetworkforchangedetectiononremotesensingimages,” detectionformultispectralearthobservationusingconvolutionalneural
ISPRS Journal of Photogrammetry and Remote Sensing, vol. 199, pp. networks,” in IGARSS 2018-2018 IEEE International Geoscience and
61–72,2023. RemoteSensingSymposium. Ieee,2018,pp.2115–2118.
[8] Y.Feng,Y.Shao,H.Xu,J.Xu,andJ.Zheng,“Alightweightcollective- [30] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and
attentionnetworkforchangedetection,”inProceedingsofthe31stACM B.Guo,“Swintransformer:Hierarchicalvisiontransformerusingshifted
InternationalConferenceonMultimedia,2023,pp.8195–8203. windows,”inProceedingsoftheIEEE/CVFinternationalconferenceon
[9] Z.Li,C.Tang,X.Liu,W.Zhang,J.Dou,L.Wang,andA.Y.Zomaya, computervision,2021,pp.10012–10022.
“Lightweightremotesensingchangedetectionwithprogressivefeature [31] W.Wang,E.Xie,X.Li,D.-P.Fan,K.Song,D.Liang,T.Lu,P.Luo,
aggregationandsupervisedattention,”IEEETransactionsonGeoscience and L. Shao, “Pyramid vision transformer: A versatile backbone for
andRemoteSensing,vol.61,pp.1–12,2023. densepredictionwithoutconvolutions,”inProceedingsoftheIEEE/CVF
[10] Z. Zheng, A. Ma, L. Zhang, and Y. Zhong, “Change is everywhere: internationalconferenceoncomputervision,2021,pp.568–578.
Single-temporal supervised object change detection in remote sensing [32] B.Heo,S.Yun,D.Han,S.Chun,J.Choe,andS.J.Oh,“Rethinkingspa-
imagery,”inProceedingsoftheIEEE/CVFinternationalconferenceon tialdimensionsofvisiontransformers,”inProceedingsoftheIEEE/CVF
computervision,2021,pp.15193–15202. InternationalConferenceonComputerVision,2021,pp.11936–11945.
[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, [33] Y. Feng, H. Xu, J. Jiang, H. Liu, and J. Zheng, “Icif-net: Intra-scale
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gellyetal., cross-interaction and inter-scale feature fusion network for bitemporal
“An image is worth 16x16 words: Transformers for image recognition remote sensing images change detection,” IEEE Transactions on Geo-
atscale,”arXivpreprintarXiv:2010.11929,2020. scienceandRemoteSensing,vol.60,pp.1–13,2022.
[12] Y. Li, H. Mao, R. Girshick, and K. He, “Exploring plain vision [34] S.Fang,K.Li,andZ.Li,“Changer:Featureinteractioniswhatyouneed
transformer backbones for object detection,” in European Conference for change detection,” IEEE Transactions on Geoscience and Remote
onComputerVision. Springer,2022,pp.280–296. Sensing,2023.
[13] Q. Liu, Z. Xu, G. Bertasius, and M. Niethammer, “Simpleclick: [35] R.C.Daudt,B.LeSaux,andA.Boulch,“Fullyconvolutionalsiamese
Interactive image segmentation with simple vision transformers,” in
networksforchangedetection,”in201825thIEEEInternationalCon-
Proceedings of the IEEE/CVF International Conference on Computer ferenceonImageProcessing(ICIP). IEEE,2018,pp.4063–4067.
Vision,2023,pp.22290–22300. [36] Y. Feng, J. Jiang, H. Xu, and J. Zheng, “Change detection on remote
sensing images using dual-branch multilevel intertemporal network,”
[14] J. Yao, X. Wang, S. Yang, and B. Wang, “Vitmatte: Boosting image
IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp.
mattingwithpre-trainedplainvisiontransformers,”InformationFusion,
1–15,2023.
vol.103,p.102091,2024.
[37] J.Zhang,Z.Shao,Q.Ding,X.Huang,Y.Wang,X.Zhou,andD.Li,
[15] Y. Xu, J. Zhang, Q. Zhang, and D. Tao, “Vitpose: Simple vision
“Aernet: An attention-guided edge refinement network and a dataset
transformerbaselinesforhumanposeestimation,”AdvancesinNeural
for remote sensing building change detection,” IEEE Transactions on
InformationProcessingSystems,vol.35,pp.38571–38584,2022.
GeoscienceandRemoteSensing,2023.
[16] H. Chen, Z. Qi, and Z. Shi, “Remote sensing image change detection
[38] Y. Liu, C. Pang, Z. Zhan, X. Zhang, and X. Yang, “Building change
with transformers,” IEEE Transactions on Geoscience and Remote
detectionforremotesensingimagesusingadual-taskconstraineddeep
Sensing,vol.60,pp.1–14,2021.
siamese convolutional network model,” IEEE Geoscience and Remote
[17] B.Jiang,Z.Wang,X.Wang,Z.Zhang,L.Chen,X.Wang,andB.Luo,
SensingLetters,vol.18,no.5,pp.811–815,2020.
“Vct: Visual change transformer for remote sensing image change
[39] R.C.Daudt,B.LeSaux,A.Boulch,andY.Gousseau,“Urbanchange
detection,”IEEETransactionsonGeoscienceandRemoteSensing,2023.
detectionformultispectralearthobservationusingconvolutionalneural
[18] W. G. C. Bandara and V. M. Patel, “A transformer-based siamese net-
networks,” in IGARSS 2018-2018 IEEE International Geoscience and
work for change detection,” in IGARSS 2022-2022 IEEE International
RemoteSensingSymposium. Ieee,2018,pp.2115–2118.
GeoscienceandRemoteSensingSymposium. IEEE,2022,pp.207–210.
[40] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
[19] Y.Zhao,Y.Zhang,Y.Dong,andB.Du,“Adaptingvisiontransformer
“Mobilenetv2:Invertedresidualsandlinearbottlenecks,”inProceedings
forefficientchangedetection,”arXivpreprintarXiv:2312.04869,2023.
of the IEEE conference on computer vision and pattern recognition,
[20] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
2018,pp.4510–4520.
recognition,”inProceedingsoftheIEEEconferenceoncomputervision
[41] Y. Tang, K. Han, J. Guo, C. Xu, C. Xu, and Y. Wang, “Ghostnetv2:
andpatternrecognition,2016,pp.770–778.
enhancecheapoperationwithlong-rangeattention,”AdvancesinNeural
[21] K.SimonyanandA.Zisserman,“Verydeepconvolutionalnetworksfor InformationProcessingSystems,vol.35,pp.9969–9982,2022.
large-scaleimagerecognition,”arXivpreprintarXiv:1409.1556,2014.
[42] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao,
[22] O.Ronneberger,P.Fischer,andT.Brox,“U-net:Convolutionalnetworks Z.Zhang,L.Dongetal.,“Swintransformerv2:Scalingupcapacityand
for biomedical image segmentation,” in Medical Image Computing resolution,” in Proceedings of the IEEE/CVF conference on computer
and Computer-Assisted Intervention–MICCAI 2015: 18th International visionandpatternrecognition,2022,pp.12009–12019.
Conference,Munich,Germany,October5-9,2015,Proceedings,PartIII [43] Z.Chen,Y.Duan,W.Wang,J.He,T.Lu,J.Dai,andY.Qiao,“Vision
18. Springer,2015,pp.234–241. transformeradapterfordensepredictions,”InternationalConferenceon
[23] H. Chen and Z. Shi, “A spatial-temporal attention-based method and LearningRepresentations,2023.
a new dataset for remote sensing image change detection,” Remote [44] J.Ma,J.Duan,X.Tang,X.Zhang,andL.Jiao,“Eatder:Edge-assisted
Sensing,vol.12,no.10,p.1662,2020. adaptive transformer detector for remote sensing change detection,”
[24] S.Ji,S.Wei,andM.Lu,“Fullyconvolutionalnetworksformultisource IEEETransactionsonGeoscienceandRemoteSensing,2024.
buildingextractionfromanopenaerialandsatelliteimagerydataset,” [45] C.Zhang,L.Wang,S.Cheng,andY.Li,“Swinsunet:Puretransformer
IEEE Transactions on geoscience and remote sensing, vol. 57, no. 1, networkforremotesensingimagechangedetection,”IEEETransactions
pp.574–586,2018. onGeoscienceandRemoteSensing,vol.60,pp.1–13,2022.
[25] M.Liu,Z.Chai,H.Deng,andR.Liu,“Acnn-transformernetworkwith [46] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,and
multiscale context aggregation for fine-grained cropland change detec- A.Joulin,“Emergingpropertiesinself-supervisedvisiontransformers,”
tion,” IEEE Journal of Selected Topics in Applied Earth Observations inProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
andRemoteSensing,vol.15,pp.4297–4306,2022. vision,2021,pp.9650–9660.
[26] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and [47] K.He,X.Chen,S.Xie,Y.Li,P.Dolla´r,andR.Girshick,“Maskedau-
H. Je´gou, “Training data-efficient image transformers & distillation toencodersarescalablevisionlearners,”inProceedingsoftheIEEE/CVF
through attention,” in International conference on machine learning. conference on computer vision and pattern recognition, 2022, pp.
PMLR,2021,pp.10347–10357. 16000–16009.
[27] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,and [48] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
A.Joulin,“Emergingpropertiesinself-supervisedvisiontransformers,” G.Sastry,A.Askell,P.Mishkin,J.Clarketal.,“Learningtransferable
inProceedingsoftheIEEE/CVFinternationalconferenceoncomputer visual models from natural language supervision,” in International
vision,2021,pp.9650–9660. conferenceonmachinelearning. PMLR,2021,pp.8748–8763.
[28] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov, [49] C.-F.R.Chen,Q.Fan,andR.Panda,“Crossvit:Cross-attentionmulti-
P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., “Dinov2: scalevisiontransformerforimageclassification,”inProceedingsoftheJOURNALOFLATEXCLASSFILES,VOL.XX,NO.XX 12
IEEE/CVFinternationalconferenceoncomputervision,2021,pp.357– Zhenfeng Shao received the Ph.D. degree in pho-
366. togrammetryandremotesensingfromWuhanUni-
[50] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional versity,Wuhan,China,in2004.Since2009,hehas
neural networks for volumetric medical image segmentation,” in 2016 beenaFullProfessorwiththeStateKeyLaboratory
fourth international conference on 3D vision (3DV). Ieee, 2016, pp. of Information Engineering in Surveying, Mapping
565–571. andRemoteSensing,WuhanUniversity.Hehasau-
[51] A.Paszke,S.Gross,S.Chintala,G.Chanan,E.Yang,Z.DeVito,Z.Lin, thoredorcoauthoredmorethan50peer-reviewedar-
A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in ticlesininternationaljournals.Hisresearchinterests
pytorch,”NIPSWorkshops,2017. include high-resolution image processing, pattern
[52] D.P.KingmaandJ.Ba,“Adam:Amethodforstochasticoptimization,” recognition,andurbanremotesensingapplications.
arXivpreprintarXiv:1412.6980,2014. Dr. Shao was a recipient of the Talbert Abrams
[53] S. Fang, K. Li, J. Shao, and Z. Li, “Snunet-cd: A densely connected Award for the Best Paper in Image Matching from the American Society
siamesenetworkforchangedetectionofvhrimages,”IEEEGeoscience for Photogrammetry and Remote Sensing in 2014 and the New Century
andRemoteSensingLetters,vol.19,pp.1–5,2021. Excellent Talents in University from the Ministry of Education of China
[54] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in in 2012. Since 2019, he has been serving as an Associate Editor for the
Proceedings of the IEEE conference on computer vision and pattern PhotogrammetricEngineering&RemoteSensing(PE&RS)specializingin
recognition,2018,pp.7132–7141. smartcities,photogrammetry,andchangedetection.
[55] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-
works,”inProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,2018,pp.7794–7803.
Qimin Cheng receivedthePh.D.degreeincartog-
raphy and geographic information system from the
Institute of Remote Sensing Applications, Chinese
AcademyofSciences,Beijing,China,in2004.She
iscurrentlyaProfessorwiththeHuazhongUniver-
sityofScienceandTechnology,Wuhan,China.Her
researchinterestsincludeimageretrievalandanno-
Duowang Zhu received the B.S. and M.S. degree
tation,andremotesensingimagesunderstandingand
inSchoolofElectronicInformationandCommuni-
analysis.
cations from Huazhong University of Science and
Technology (HUST), Wuhan, China, in 2020 and
2022, respectively. He is currently pursuing the
Ph.D. degree with the State Key Laboratory of
InformationEngineeringinSurveying,Mappingand
Remote Sensing (LIESMARS), Wuhan University,
Wuhan, China. His current research areas include
computervisionandremotesensingimageprocess.
Xiaohu Huang received the B.S. and M.S. degree
inSchoolofElectronicInformationandCommuni-
cations from Huazhong University of Science and
Technology (HUST), Wuhan, China, in 2020 and
2023, respectively. Now, he is pursuing the Ph.D
degree in the University of Hong Kong (HKU),
Hong Kong, China. His current research areas in-
cludecomputervisionandmachinelearning.
HaiyanHuangreceivedtheM.S.degreeHuazhong
University of Science and Technology, Wuhan,
China, in 2022. She is currently studying for the
Ph.D. degree with the State Key Laboratory of
InformationEngineeringinSurveying,Mappingand
RemoteSensing,WuhanUniversity.Herresearchin-
terestsincludehighspatialresolutionremotesensing
imageunderstandingandanalysis.