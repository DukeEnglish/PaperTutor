Generating Educational Materials with Different Levels of
Readability using LLMs
Chieh-YangHuang JingWei Ting-HaoKennethHuang
cyhuang@lexile.com jwei@lexile.com txh710@psu.edu
MetaMetricsInc. MetaMetricsInc. PennsylvaniaStateUniversity
Durham,NorthCarolina,USA Durham,NorthCarolina,USA UniversityPark,Pennsylvania,USA
Figure1:Given(A)asourcetext,(B)areadabilitylevelforthesourcetext,and(C)anintendedreadabilitylevel,theleveled-text
generationaimsatrewritingthesourcetextintheintendedreadabilitylevelwhilepreservingthemeaning.Notethatin
thispaper,weuseawidelyadoptedreadabilitylevelintheeducationcontext,theLexileScale.Intheprovidedexample,the
sentencesarelongerandthewordsarelesscommonintheresultingtext.
ABSTRACT suchasNewsela[2],SimpleEnglishWikipedia[3],andCommon-
Thisstudyintroducestheleveled-textgenerationtask,aimingto LIT[1],amongothers,havedevelopedtextsatdifferentlevelsof
rewriteeducationalmaterialstospecificreadabilitylevelswhile complexityforeducationalpurposes.
preservingmeaning.WeassessthecapabilityofGPT-3.5,LLaMA-2 Theprocessofrewritingtextstodifferentlevelstypicallyin-
70B,andMixtral8x7B,togeneratecontentatvariousreadability volvesiterativeeditingtoensurethattherevisedtextsmeetthe
levelsthroughzero-shotandfew-shotprompting.Evaluating100 desireddifficultycriteria.Thisreadabilityassessmentisbasedon
processededucationalmaterialsrevealsthatfew-shotprompting variouslinguisticfeatures,withsentencelengthandwordfrequency
significantlyimprovesperformanceinreadabilitymanipulationand identifiedaskeyfactorsinpreviousstudies[11].Althoughthis
informationpreservation.LLaMA-270Bperformsbetterinachiev- processappearsstraightforward,accuratelyadjustingtheseele-
ingthedesireddifficultyrange,whileGPT-3.5maintainsoriginal mentstoachievethetargetreadingdifficultyischallenging.This
meaning.However,manualinspectionhighlightsconcernssuch taskbecomesevenmorecomplexforyounglearners,wherefactors
asmisinformationintroductionandinconsistenteditdistribution. suchasdecodability[19],informationload[15],andotherelements
Thesefindingsemphasizetheneedforfurtherresearchtoensure playasignificantrole[10,11].
thequalityofgeneratededucationalcontent. Thispaperintroducestheleveled-textgenerationtask.As
showninFigure1,given(A)asourcetext,(B)thereadabilitylevel
ofthesourcetext,and(C)thedesiredreadabilitylevel,thegoalis
KEYWORDS
torewritethegiventextsothattheresultingtext(i)iswithinthe
EducationalMaterialGeneration,TextReadability,TextGeneration, desiredreadabilityleveland(ii)alsopreservestheoriginalmeaning.
LargeLanguageModel Asourfirstattemptatsolvingtheleveledtextgenerationtask,we
assessthecapabilityofthreeLLMs,includingGPT-3.5[21],LLaMA-
270B[26],andMixtral8x7B[13],togeneratecontentatvarious
1 INTRODUCTION levelsofreadabilitythroughzero-shotandfew-shotprompting
techniques.Ourevaluationinvolved100educationalmaterialspro-
Priorresearchhasshownthatstudentslearnmoreeffectivelyfrom
cessedbyGPT-3.5,LLaMA-270B,andMixtral8x7B.Thefindings
readingmaterialsthatmatchtheirlevelofreadability,optimallybal-
indicatethatprovidingafewexampleswillsignificantlyimprove
ancingimprovementandcognitiveload[8,20,28].Consequently,
performanceinbothmanipulatingthecontentreadabilityandpre-
curatingeducationalcontenttomeetthediversereadingabilitiesof
servingthesameinformation.Thismightalsosuggestthatthe
studentsisacrucialsteptowardbetterlearningoutcomes.Platforms
4202
nuJ
81
]LC.sc[
1v78721.6042:viXraIn2Writing2024,May11,2024,Honolulu,Hawii Chieh-YangHuang,JingWei,andTing-HaoKennethHuang
conceptoftheleveled-textgenerationandLexilescaledoesnot spokenlanguagelearning;Juryetal.[14]evaluatedLLM-generated
inherentlyexist,asshowingexamplescouldhelpthemodelunder- workedexamplesinanintroductoryprogrammingcourse;andXiao
standthetaskandrealizewhatthetextsshouldlooklikewithin etal.[27]appliedLLMstogeneratereadingcomprehensionexer-
thedesireddifficultylevel.WhencomparingdifferentLLMs,we cises.DespitetheadoptionofLLMsinvariousfields,thisparticular
foundthatLLaMA-270Bwasthemosteffectiveinachievingthe workfocusesonleveragingtheirpotentialforlanguagelearning
desireddifficultyrange,whileGPT-3.5excelledinmaintainingthe byrewritingtextstodifferentreadabilitylevels.
meaningoftheoriginaltext.
Wealsoconductedamanualqualityinspectionof10articles
3 BENCHMARKTHELEVELED-TEXT
consideringpotentialconcernsifthegeneratedtextswereused
GENERATIONTASK
aslearningmaterials.Ouranalysisrevealedafewissues:(i)the
potentialforintroducingmisinformation,particularlyintheformof Toestablishafirstbenchmarkfortheleveled-textgenerationtask,
alteredquotationsandfactualinaccuracies,and(ii)theinconsistent we compiled a parallel dataset comprising 30K pairs of leveled
distributionofeditswithinthetext,leadingtoanunevenreadability textsandexperimentedwithitusingthreeLLMs,namelyGPT-3.5,
level throughout the article. These findings highlight the need LLaMA-2 70B, and Mixtral 8x7B. We evaluate the performance
forfurtherresearchanddevelopmenttoaddresstheseconcerns basedontwoaspects:(i)manipulatingreadabilityand(ii)content
andensurethequalityofthegeneratededucationalcontent.This preservation.
preliminarysurveyhelpedusidentifyfivecriticalpointsforfuture NotethatwechosetheLexilescaleasatooltoassessreadability,
advancesinleveledtextgenerationtasks. givenitswideadoptionineducationalsettingstomeasuretext
complexity.TheLexileFrameworkevaluatesastudent’sreading
2 RELATEDWORK skillsandthecomplexityofreadingmaterialsonthesamescale[20].
Thismakesiteasierforstudentstochoosebooksthatarejustright
This task is related to (i) text readability manipulation and (ii)
fortheirreadinglevel,helpingthemtoenhancetheirlanguage
educationalcontentgeneration.
skillsmoreeffectively.GivenitswideadoptioninK-12educational
settingstomeasuretextcomplexity,wedecidedthattheLexile
TextReadabilityManipulation. Mostofthetasksrelatedtomanip-
measurewasthebestfitforourreadabilityanalysis.
ulatingtextreadabilityfocusontextsimplification.S.Bautistaetal.
[23]simplifiedtextsthrough(i)rule-basedconversiontosentence
structuresand(ii)replacingdifficultwordswitheasiersynonyms. 3.1 TaskDefinition
BingelandSøgaard[6]introducedastructuredapproachtotextsim-
AsshowninFigure1,given(A)asourcetext,(B)areadabilitylevel
plificationusingconditionalrandomfieldsondependencygraphsto
ofthesourcetext,and(C)anintendedreadabilitylevelofthetarget
predictcompressionsandparaphrases.Swainetal.[25]developed
text,theleveled-textgenerationtaskshouldrewritethesourcetext
anefficienttextsimplificationtechniqueusingtheWordNetmodel
tomeetcharacteristicswithintheintendedreadabilitylevel,such
inNLTK.AlkaldiandInkpen[5]trainedtheirownreadabilityclas-
asthevocabularyused,sentencestructure,sentencelength,andso
sifieranddesignedareinforcementlearningframeworktotraina
on.
textsimplificationmodelbasedonaGRUsequence-to-sequence
TheLexileFrameworksuggeststhatreadabilitymeasuresshould
modelwithattention.AlkaldiandInkpen[5]’swork,however,is
relyondifferentfeaturesfordifferentgroupsofstudents.Forearly-
currentlyonlytrainedinsentence-levelsimplification.Recentad-
levellearners(usuallylowerthangrade4oraLexilelevelof750L),
vancementsinLLMshaveshownpromisingresults.Fengetal.[9]
readability relies on many specific features, such as decodabil-
investigatedzero-shotandfew-shotlearningcapabilitiesofLLMs,
ity[19],informationload[15],phoneticfeatures,wordstructure,
demonstratingsuperiorperformance.Maddelaetal.[18]introduced
sentencecomplexity,andsoon[10,11].Forupper-levellearners,
ahybridapproachcombininglinguistically-motivatedruleswith
thereadabilitymeasurementmostlyreliesonwordfrequencyand
aneuralparaphrasingmodel.Otherstudiesfocusedonimproving
sentencelength[11].Inthistask,wewouldliketoseeifLLMscan
transparencyandexplainability.CristinaGarbaceaetal.[7]pro-
learnthecharacteristicsofthetargetLexilelevelandrewritethe
posedastructuredpipeline,analyzingtextcomplexityprediction
sourcetextaccordingly.
andcomplexcomponentidentification.
Whilethesestudieshavemadesignificantcontributions,further
3.2 EvaluationMetrics.
researchisneededongeneratingtextsatspecificreadabilitylevels
(includingbothincreasinganddecreasingtextreadability)while Weevaluatetwoaspects:(i)whetherthemodelcouldcorrectly
preservingmeaning.Ourworkontheleveled-textgenerationtask rewritethetextstotheintendedLexilescoreand(ii)whetherthe
benchmarkstheLLMsperformanceandexploresthepossibilityof modelcouldstillpreservethesameinformation.
thisnewtopic. To evaluate whether the resulting texts are aligned with the
intendedLexilescore,wefirstusedtheLexileAnalyzertomeasure
EducationalContentGeneration. Recentstudieshaveexplored
theresultingtextsandobtainedtheresultingLexilescore.Several
thepotentialoflargelanguagemodels(LLMs)ingeneratingeduca-
metricswerethencalculatedbasedontheintendedandtheresulting
tionalcontent.Leikeretal.[16]investigatedtheuseofLLMsfor
Lexilescore:
creatingadultlearningcontentatscale;MacNeiletal.[17]focused
onautomaticallygeneratingcomputersciencelearningmaterials; (1) MeanAbsoluteError(MAE),representingtheabsolutede-
Gaoetal.[12]specificallyinvestigatedtheapplicationofLLMsto viationbetweentheintendedandtheresultingLexilescores;GeneratingEducationalMaterialswithDifferentLevelsofReadabilityusingLLMs In2Writing2024,May11,2024,Honolulu,Hawii
(2) MatchRate,indicatingtheproportionofinstanceswhere Wetriedpromptingtechniqueswithbothzero-shotlearningand
the resulting Lexile score was within a ±50 range of the few-shotlearning.Forzero-shotlearning,wedefinedtheLexile
intendedscore;and scoreandthenprovidedthesourcetext,sourceLexilescore,and
(3) DirectionalAccuracy,reflectingtheproportionofinstances targetLexilescoretothemodel(seeAppendixAfortheactual
wheretheresultingLexilescoremovedintheintendeddi- promptused.)
rection(towardeasierormoredifficultlevels). However,thedefinitionofLexilescorecanstillbevaguefor
Tomeasureinformationpreservation,weusedBERTScores[29]1, LLMs.Toaddressthis,wealsotriedfew-shotlearning,whereactual
semanticsimilarity[22]2,andnormalizededitdistancetoassess examplesfromthetrainingsetwerepresentedtoteachLLMswhat
contentpreservationbetweenthesourcetextsandtheresulting thetextwithinaparticularreadabilitylevelshouldlooklike.
texts. Thefew-shotlearningexamplesforeachsamplewerechosen
basedonthecorrespondingLexilescoresofthesourceandtarget
3.3 Dataset texts.WeidentifiedtrainingsampleswhosesourceandtargetLexile
scoreswerebothwithina50-pointrangeofthetargetsample’s
Westartedwithaleveled-textcorpus,acollectionofarticlesfrom
correspondingscores:
variouslanguage-learningbooksorganizedinto1,690sets.Each
setcontainsarticlesthatsharethesametitle,meaningtheycover Atrainingsampleisqualifiedif:
the same topic but are written at different readability levels to (cid:26)|𝐿𝑒𝑥𝑖𝑙𝑒 𝑡𝑟𝑎𝑖𝑛−𝑠𝑜𝑢𝑟𝑐𝑒 −𝐿𝑒𝑥𝑖𝑙𝑒 𝑠𝑎𝑚𝑝𝑙𝑒−𝑠𝑜𝑢𝑟𝑐𝑒| ≤50(cid:27) (2)
suitvariousreadingabilities.Thesearticlesrangefromtwotosix
|𝐿𝑒𝑥𝑖𝑙𝑒 𝑡𝑟𝑎𝑖𝑛−𝑡𝑎𝑟𝑔𝑒𝑡 −𝐿𝑒𝑥𝑖𝑙𝑒 𝑠𝑎𝑚𝑝𝑙𝑒−𝑡𝑎𝑟𝑔𝑒𝑡| ≤50
versions per set, with an average of 825 words per article. The
collectioncanberepresentedasfollows: Fromthequalifyingtrainingsamples,weselectedtheshortest𝑛
LeveledTextCorpus={{𝐴1 1,𝐴1 2},{𝐴2 1,𝐴2 2,...𝐴2 6},...,{𝐴𝑛 1,...,𝐴 𝑚𝑛 }} samplestominimizethecontextsizeoftheprompt.Inthefew-shot
prompt,weincludedthesourcetext,sourceLexilescore,target
(1)
Inthisrepresentation,𝐴𝑖
𝑗 referstoaspecificlanguagelearning
text,andtargetLexilescoreasexamplesforin-contextlearning
article,where𝑖 indicatesthesetindexand 𝑗 isthearticleindex (seeAppendixAfortheactualpromptused).
withinthatset.Articlesinthesamegroup(𝑖)discussthesametopic
3.5 BenchmarkResults
butatdifferentcomplexitylevels.Weassignedareadabilityscore
toeacharticlebyrunningtheLexileanalyzer[20]. Table1showsthebenchmarkresults.Notethatforfew-shotlearn-
Thedatasetwasthensplitintothreepartsaccordingtotheset ing,wetried1-shot,3-shot,and5-shotsamples,butonlythebest-
index:90%fortraining(1521sets),5%forvalidation(84sets),and5% performingonewasreported.
fortesting(85sets).Toformaparalleldatasetfortheleveled-text Ourfindingssuggestthatprovidingafewexamplessignificantly
generationtask,wepermutedallpairsofarticleswithineachset. improves performance, as few-shot learning outperforms zero-
Thisnewparalleldatasetconsistsofthefollowingfields: shotlearning.Thisphenomenonalsoindicatesthattheconceptof
“leveled-textgeneration”withrespecttotheLexilescaledoesnot
(1) Sourcetext:Theoriginalarticlethatneedstoberewritten
inherentlyexistintheseLLMs.TofullyutilizeLLMs,itisnecessary
tomatchadifferentreadabilitylevel.
toteachthemwhatLexilemeansandwhatthetextswouldpossibly
(2) SourceLexilescore:Thereadabilityscoreofthesourcetext,
looklikeateachlevel.
usedtoguidetheadjustmentprocessintermsofsimplifying
WhencomparingdifferentLLMs,wefoundthatLLaMA-270B
orcomplicatingthetext.
performsbestinadjustingreadability,whereasGPT-3.5demon-
(3) Targettext:Therewrittenarticle,adjustedtothedesired
stratessuperiorperformanceinpreservingcontentandmeaning.
readabilitylevel.
However,amajorconcernisthatalthoughLLaMA-270Bachieves
(4) Target Lexile score: The intended readability score for
thebestLexileScoremeasure(lowestMAE=172.9;highestmatch
therewrittenarticle,servingasagoalfortheleveled-text
rate=22.22%),itsnormalizededitdistanceisalsothehighest,mean-
generationtask.
ingthatitproducestheleastedittothecontent(seeSection4:Bias
Afterpreprocessing,thetrain,validation,andtestsetscomprised
inCurrentModelsforthediscussionofthedesirededitbehavior).
29,990,1680,and1700pairsofleveledtexts,respectively.Notethat
AnotherconcernwhenusingLLaMA-270Bisitssmallercontext
althoughourtestsetcomprised1700instances,weonlyselected
size(4Ktokens),whichcancausethetasktofailiftheprompt
100samplesforthispreliminaryanalysis.
exceedsthecontextsizelimitation.
InFigure2andFigure3,wepresentscatterplotstoillustrate
3.4 Leveled-TextGenerationwithLLMs
thedistributionofintendedLexileversusresultingLexilescores
Asthefirstattempttosolvetheleveled-textgenerationtask,weex- andintendedLexileshiftversusresultingLexileshift.Thesefigures
perimentedwithpromptingpopularLLMs,namelyGPT-3.5,LLaMA- providevaluableinsightsintothebehaviorofthelanguagemodels.
270B,andMixtral8x7B.Thepurposeofthismethodistoexplore Thered-shadedarearepresentstheregionwhereresultingscores
thecapabilitiesofLLMsingeneratingleveledtextsandtoestablish fallwithin±50pointsoftheintendedscores.InbothFigure2and
abaselineforfutureimprovements. Figure3,wecanclearlyobservethatahigherproportionofthedata
pointsarelocatedabovethered-shadedarea,indicatingthatthe
1Weusedthemicrosoft/deberta-xlarge-mnlimodeltoobtaintheembedding.
generatedtextsaregenerallymorecomplexthantheintendedlevel.
2Weusedsentencetransformerwiththesentence-transformers/all-mpnet-base-v2
model Althoughthisbiashasbeenidentified,moreresearchisnecessaryIn2Writing2024,May11,2024,Honolulu,Hawii Chieh-YangHuang,JingWei,andTing-HaoKennethHuang
LexileScore BERTScore Semantic Normalized
Method Model #Shot Support
Similarity↑ EditDistance
MAE↓ Match↑ Direction↑ Precision↑ Recall↑ F1↑
GPT-3.5 0 100 257.6 15.00% 80.00% 79.87% 76.54% 78.03% 0.893 0.941
Zero-shot LLaMA-270B 0 100 206.5 15.15% 71.72% 75.49% 71.82% 73.56% 0.894 0.947
Mixtral8x7B 0 100 256.0 11.00% 79.00% 74.69% 73.74% 74.18% 0.894 0.951
GPT-3.5 3 100 205.3 15.00% 75.00% 82.85% 80.18% 81.45% 0.937 0.934
Few-shot LLaMA-270B 1 99 172.9 22.22% 86.87% 72.96% 71.25% 73.01% 0.887 0.949
Mixtral8x7B 3 100 210.9 12.00% 83.00% 72.89% 69.88% 71.27% 0.935 0.929
Table1:Performancecomparisonfordifferentmodels.InLexilescoremeasurements,MAEreferstothemeanabsoluteerror
betweentheresultedLexilescoreandtheintentedLexilescore;MatchmeasureswhethertheresultedLexilescoresfallwithin
therangeoftheintentedLexilescore±50;andDirectionmeasureswhethertheresultedLexilescoremovestowardtheintended
direction(i.e.,easierorharder).
tounderstanditsunderlyingcausesanddeveloppotentialsolutions differentlevelsiscrucialyetchallenging.Webelievehumanin-
tomitigateitseffects. volvementisessential,butmoreresearchisneededtoexplorehow
suchinvolvementcanbedone.
4 DETAILEDINSPECTIONANDDISCUSSION
ImportantInformationShouldRemainUnchanged. Thereisaneed
Weconductedamanualinvestigationof10samplesselectedfrom
amongcontentcreatorstoretainspecificpiecesofinformation,
thetestset.Wereviewedthegeneratedtextsanddiscussedpotential
suchaskeyterms(e.g.,“Photosynthesis”),essentialsentences(e.g.,
issuesinusingtheseoutputsaseducationalmaterials.
aquote),orparticularsectionsdeemedmoreimportantthanothers.
CurrentLLMsmayaddressthisrequirementthroughprompten-
ProblemswithTextShorteningandLengthening. Changingcon-
gineering.However,developinganintuitiveinterfacethatallows
tentreadabilitymayinvolveshorteningorlengtheningthegiven
usersto(i)highlightareasoftextthatshouldremainunchanged
text.LLMsaregoodatshorteningtextswithadvancedprompting
and(ii)verifywhetherthegeneratedtextsmeetthesecriteriais
techniquessuchasChain-of-Density[4].Expandingtexts,however,
essential.
requirestheintroductionofnewinformation.Whilethisisless
problematicfornarrativegenres,itposessignificantchallengesfor
factualcontentlikescienceandnews,wheremaintainingaccuracy LimitationsandBiasinCurrentModels. Weidentifiedbiasesin
andminimizingmisinformationarecrucial.Inourreviewof30 thethreeLLMs.First,wefoundatendencyforthemodelstoproduce
generatedarticles,wenoticedseveralmodificationstoquotations, shortertextsthantheoriginals(Original:825words,Few-Shot:350-
whichisnotideal.RecentstudiessuggestthatRetrieval-Augmented 500words),regardlessofwhethertheintentionwastosimplify
Generation(RAG)canmitigatehallucinations[24],butimplement- orcomplexifythetexts.Thisbiasmayhavebeeninfluencedby
inganRAGsystemthatintegratesup-to-date,externalinformation theuseofshorterfew-shotsamples.However,asimilarpattern
remainsachallenge. emergedinzero-shotscenarios(Original:825words,Zero-Shot:
500-600words),suggestingapossiblebiasinoff-the-shelfLLMs.
LimitationsinLeveled-TextGeneration. Leveled-textgeneration, Second,thedistributionofeditswithinarticlesoftenappeared
particularlyforscientificmaterials,mightnotbefeasibleforallLex- uneven,withsomeparagraphsremainingunchangedwhileothers
ilelevels,especiallyforyoungerlearners.Paraphrasingsentences underwentsignificantrevisions(thisappearsmorefrequentlyin
orselectingfrequentlyappearingwordshavetheirlimitsinchang- thetextsgeneratedbyLLaMA-270B).Thisinconsistentediting
ingtextdifficulty.Forexample,explaining“photosynthesis”tokids patternisunsuitableforeducationalmaterials,evenifthearticles
might require a new explanation that involves age-appropriate achievethedesiredreadability.Moreresearchisneededtoidentify
analogiesorvisuals.Thus,theleveled-textgenerationtask,which thecausesanddevelopsolutions.
aimstorewritetexts,maynotadequatelyaddresssuchneeds.To
significantlychangetheoriginalcontenttoachievethedesired
5 CONCLUSION
readability,amoresophisticatedLLMmightbeabletocompletethe
task,butitisoutsidethescopeofthecurrentleveled-textgeneration Ourinvestigationintoleveled-textgenerationusingLLMsunder-
task. scoresthepotentialandchallengesofautomatingeducationalcon-
tentcreation.WhileLLaMA-270Bshowspromiseinadjustingtext
IncorporatingEducationalLessons. Ineducationalcontexts,ma- complexity,GPT-3.5isbetteratmaintainingcontentmeaning.Fu-
terialsusuallycomewithlearningobjectives,suchasgrammar,vo- tureresearchshouldaddressthenuanceddemandsofeducational
cabulary,knowledge,etc.Integrationoftheseeducationalelements content,includingaccuraterepresentationofinformation,integra-
intorewrittentextsremainsanunresolvedchallenge.Moreover, tionoflearningobjectives,andretentionofkeyinformation,to
determining the appropriate learning objectives for students at enhanceleveled-textgenerationprocess.GeneratingEducationalMaterialswithDifferentLevelsofReadabilityusingLLMs In2Writing2024,May11,2024,Honolulu,Hawii
(a)Zero-shotGPT-3.5 (b)Zero-shotLLaMA-270B (c)Zero-shotMixtral8x7B
(d)Few-shotGPT-3.5 (e)Few-shotLLaMA-270B (f)Few-shotMixtral8x7B
Figure2:ScatterplotscomparingintendedandresultingLexilescoresfortextgeneratedbyGPT-3.5,LLaMA-270B,andMixtral
8x7Bmodelsinzero-shotandfew-shotsettings.Thered-shadedarearepresentstheregionwhereresultingscoresarewithin
±50pointsoftheintendedscores.Ahigherproportionofdatapointsfallabovetheredarea,indicatingthattheresultingLexile
scorestendtoskewhigherthantheintendedscores,suggestingatendencyforthemodelstogenerateslightlymorecomplex
textthanthetargetdifficultylevel,regardlessofthespecificmodelorpromptingapproachused.
REFERENCES
[11] JillFitzgerald,JeffElmore,HeatherKoons,ElfriedaHHiebert,KimberlyBowen,
[1] [n.d.].CommonLit. https://www.commonlit.org/en/texts/celebrating-chinese- EleanorESanford-Moore,andAJacksonStenner.2015.Importanttextcharac-
new-year/paired-texts CommonLITidentifiedtextpairingsbasedonsimilar teristicsforearly-gradestextcomplexity.JournalofEducationalPsychology107,
themes,topic,orwritingstyle.. 1(2015),4.
[2] [n.d.].Newsela. https://newsela.com/ [12] YingmingGao,BaorianNuchged,YaLi,andLinkaiPeng.2023.AnInvestigation
[3] [n.d.].SimpleEnglishWikipedia. https://simple.wikipedia.org/wiki/Main_Page ofApplyingLargeLanguageModelstoSpokenLanguageLearning. Applied
TheSimpleEnglishWikipediaisaSimpleEnglishlanguageversionofWikipedia, Sciences14,1(dec262023),224.
anonlineencyclopedia,writteninalanguagethatiseasytounderstandbutis [13] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,Blanche
stillnaturalandgrammatical.. Savary,ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBou
[4] GriffinAdams,AlexanderFabbri,FaisalLadhak,EricLehman,andNoémieEl- Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint
hadad.2023.Fromsparsetodense:GPT-4summarizationwithchainofdensity arXiv:2401.04088(2024).
prompting.arXivpreprintarXiv:2309.04269(2023). [14] BreannaJury,AngelaLorusso,JuhoLeinonen,PaulDenny,andAndrewLuxton-
[5] WejdanAlkaldiandDianaInkpen.2023.TextSimplificationtoSpecificReadability Reilly.2024.EvaluatingLLM-generatedWorkedExamplesinanIntroductory
Levels.Mathematics11,9(apr262023),2063. ProgrammingCourse.InProceedingsofthe26thAustralasianComputingEducation
[6] JoachimBingelandAndersSøgaard.2016.TextSimplificationasTreeLabeling. Conference.ACM.
InProceedingsofthe54thAnnualMeetingoftheAssociationforComputational [15] ThomasKLandauerandSusanTDumais.1997.AsolutiontoPlato’sproblem:
Linguistics(Volume2:ShortPapers).AssociationforComputationalLinguistics. Thelatentsemanticanalysistheoryofacquisition,induction,andrepresentation
[7] CristinaGarbacea,MengtianGuo,SamuelCarton,andQ.Mei.2020.AnEmpirical ofknowledge.Psychologicalreview104,2(1997),211.
StudyonExplainablePredictionofTextComplexity:PreliminariesforText [16] DanielLeiker,SaraFinnigan,AshleyRickerGyllen,andMutluCukurova.2023.
Simplification.arXiv.org(2020). PrototypingtheuseofLargeLanguageModels(LLMs)foradultlearningcontent
[8] Fariba Rahimi Esfahani Ehsan Namaziandost and Sheida Ahmadi. 2019. creationatscale.(2023).
Varying levels of difficulty in L2 reading materials in the EFL class- [17] StephenMacNeil,AndrewTran,JuhoLeinonen,PaulDenny,JoanneKim,Arto
room: Impact on comprehension and motivation. Cogent Education Hellas,SethBernstein,andSamiSarsa.2022. AutomaticallyGeneratingCS
6, 1 (2019), 1615740. https://doi.org/10.1080/2331186X.2019.1615740 LearningMaterialswithLargeLanguageModels.InProceedingsofthe54thACM
arXiv:https://doi.org/10.1080/2331186X.2019.1615740 TechnicalSymposiumonComputerScienceEducationV.2.ACM.
[9] YutaoFeng,JipengQiang,YunLi,YunhaoYuan,andYiZhu.2023. Sentence [18] MounicaMaddela,FernandoAlva-Manchego,andWeiXu.2021.ControllableText
SimplificationviaLargeLanguageModels.(2023). SimplificationwithExplicitParaphrasing.InProceedingsofthe2021Conference
[10] JillFitzgerald,JeffElmore,ElfriedaHHiebert,HeatherHKoons,KimberlyBowen, oftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
EleanorESanford-Moore,andAJacksonStenner.2016.Examiningtextcomplex- HumanLanguageTechnologies.AssociationforComputationalLinguistics.
ityintheearlygrades.PhiDeltaKappan97,8(2016),60–65. [19] ShailajaMentonandElfriedaHHiebert.1999.LiteratureAnthologies:TheTask
forFirst-GradeReaders.(1999).
[20] MetaMetrics.2022. LexileFrameworkforReadingDevelopmentandValidity
Evidence. https://hubsupport.lexile.com/Images/Lexile%20Framework%20for%In2Writing2024,May11,2024,Honolulu,Hawii Chieh-YangHuang,JingWei,andTing-HaoKennethHuang
(a)Zero-shotGPT-3.5 (b)Zero-shotLLaMA-270B (c)Zero-shotMixtral8x7B
(d)Few-shotGPT-3.5 (e)Few-shotLLaMA-270B (f)Few-shotMixtral8x7B
Figure3:ScatterplotscomparingintendedandresultingLexileshiftsfortextgeneratedbyGPT-3.5,LLaMA-270B,andMixtral
8x7Bmodelsinzero-shotandfew-shotsettings.TheLexileshiftiscalculatedasthedifferencebetweentheintendedorresulting
LexilescoreandthesourceLexilescore.Datapointsfallingwithinthefirstandthirdquadrantsindicatethecorrectdirection
ofchangeintextcomplexity.However,theoveralldistributionofpointsstillexhibitsaskewtowardshigherdifficultylevels,
suggestingthatthemodelstendtogeneratetextthatisslightlymorecomplexthantheintendedshift,regardlessofthespecific
modelorpromptingapproachemployed.
20Reading%20Validity%20Evidence_2022.pdf [29] TianyiZhang*,VarshaKishore*,FelixWu*,KilianQ.Weinberger,andYoavArtzi.
[21] OpenAI.2022. ChatGPT:Alargelanguagemodel. https://openai.com/blog/ 2020.BERTScore:EvaluatingTextGenerationwithBERT.InInternationalConfer-
chatgpt/Knowledgecutoff:September2021. enceonLearningRepresentations.https://openreview.net/forum?id=SkeHuCVFDr
[22] NilsReimersandIrynaGurevych.2019.Sentence-BERT:SentenceEmbeddings
usingSiameseBERT-Networks.InProceedingsofthe2019ConferenceonEm-
piricalMethodsinNaturalLanguageProcessing.AssociationforComputational A PROMPT
Linguistics. https://arxiv.org/abs/1908.10084
[23] S.Bautista,PabloGervás,andR.I.Madrid.2009.FeasibilityAnalysisforSemiAuto- Here,weprovidethepromptweusedfortheexperiments.
maticConversionofTexttoImproveReadability.InformationandCommunication Thezero-shotpromptweusedisasfollow.Wequicklyintro-
TechnologiesandAccessibility(2009).
ducetheLexilemeasurementandthenprovide(i){SOURCE-TEXT},
[24] KurtShuster,SpencerPoff,MoyaChen,DouweKiela,andJasonWeston.2021.
RetrievalAugmentationReducesHallucinationinConversation.InFindings (ii){SOURCE-LEXILE},and(iii){TARGET-LEXILE}astheinforma-
oftheAssociationforComputationalLinguistics:EMNLP2021,Marie-Francine tion.
Moens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(Eds.).Association
forComputationalLinguistics,PuntaCana,DominicanRepublic,3784–3803. ALexilemeasureisdefinedas“thenumericrepresentation
https://doi.org/10.18653/v1/2021.findings-emnlp.320 ofanindividual’sreadingabilityoratext’sreadability(or
[25] DebabrataSwain,MrunmayeeTambe,PreetiBallal,VishalDolase,KajolAgrawal,
andYogeshRajmane.2019.LexicalTextSimplificationUsingWordNet.Springer difficulty),”wherelowerscoresreflecteasierreadabilityand
Singapore,114–122. higherscoresindicateharderreadability.
[26] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yas-
mineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos-
ale,etal.2023. Llama2:Openfoundationandfine-tunedchatmodels. arXiv Inthistask,wearetryingtorewriteagiventextintothe
preprintarXiv:2307.09288(2023). targetLexilelevelandkeeptheoriginalmeaningandinfor-
[27] ChangrongXiao,SeanXinXu,KunpengZhang,YufangWang,andLeiXia.
mation.Giventheoriginaldraft(Lexile={SOURCE-LEXILE}):
2023. EvaluatingReadingComprehensionExercisesGeneratedbyLLMs:A
ShowcaseofChatGPTinEducationApplications.InProceedingsofthe18th
WorkshoponInnovativeUseofNLPforBuildingEducationalApplications(BEA [TEXTSTART]
2023).AssociationforComputationalLinguistics.
[28] Ya-HanYang,Hsi-ChinChu,andWen-TaTseng.2021.Textdifficultyinextensive {SOURCE-TEXT}
reading:Readingcomprehensionandreadingmotivation.(2021). [TEXTEND]GeneratingEducationalMaterialswithDifferentLevelsofReadabilityusingLLMs In2Writing2024,May11,2024,Honolulu,Hawii
Rewritetheabovetextand{TASK}tothedifficultylevelof [TEXTSTART]
Lexile={TARGET-LEXILE}. {SOURCE-TEXT}
Thefew-shotpromptweusedisasfollow.Therearethree [TEXTEND]
sectionsintheprompt,introduction,example,andtask(separated
by“START-INTRO”tags).Thesectiontagwillnotappearinthe RewrittenTextofLexile={TARGET-LEXILE}
finalprompt.Forfew-shotlearningwithmorethan1shot,onlythe [TEXTSTART]
examplesectionwillberepeatedseveraltimes. {TARGET-TEXT}
[TEXTEND]
((START-INTRO))
((END-EXAMPLE))
ALexilemeasureisdefinedas"thenumericrepresentation
ofanindividual’sreadingabilityoratext’sreadability(or
((START-TASK))
difficulty),"wherelowerscoresreflecteasierreadabilityand
Now,giventheoriginaltext(Lexile={SOURCE-LEXILE}):
higherscoresindicateharderreadability.
[TEXTSTART]
{SOURCE-TEXT}
Inthistask,wearetryingtorewriteagiventextintothe
[TEXTEND]
targetLexilelevelandkeeptheoriginalmeaningandinfor-
mation.
Rewritetheabovetextand{TASK}tothedifficultylevelof
((END-INTRO))
Lexile={TARGET-LEXILE}.Donotinclude[TEXTSTART]
and[TEXTEND]inyourresponse.Thanks.
((START-EXAMPLE))
((END-TASK))
Hereisanexample.
Lexile={SOURCE-LEXILE}