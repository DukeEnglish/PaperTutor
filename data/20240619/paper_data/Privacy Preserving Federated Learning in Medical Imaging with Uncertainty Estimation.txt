Privacy Preserving Federated Learning in Medical Imaging with Uncertainty
Estimation
NikolasKoutsoubisa,c,∗∗,YasinYilmazc,RaviP.Ramachandrand,MatthewSchabathb,GhulamRasoola
aDepartmentofMachineLearning,MoffittCancerCenter,Tampa,FL
bDepartmentofCancerEpidemiology,MoffittCancerCenter,Tampa,FL
cElectricalEngineeringDepartment,UniversityofSouthFlorida,FL,
dElectrical&ComputerEngineeringDepartment,RowanUniversity,NJ
Abstract
Machinelearning(ML)andArtificialIntelligence(AI)havefueledremarkableadvancements,particularlyinhealth-
care. Withinmedicalimaging,MLmodelsholdthepromiseofimprovingdiseasediagnoses,treatmentplanning,and
post-treatmentmonitoring. Variouscomputervisiontaskslikeimageclassification,objectdetection,andimageseg-
mentationarepoisedtobecomeroutineinclinicalanalysis.However,privacyconcernssurroundingpatientdatahinder
theassemblyoflargetrainingdatasetsneededfordevelopingandtrainingaccurate,robust,andgeneralizablemodels.
FederatedLearning(FL)emergesasacompellingsolution,enablingorganizationstocollaborateonMLmodeltrain-
ingbysharingmodeltraininginformation(gradients)ratherthandata(e.g.,medicalimages).FL’sdistributedlearning
frameworkfacilitatesinter-institutionalcollaborationwhilepreservingpatientprivacy. However,FL,whilerobustin
privacy preservation, faces several challenges. Sensitive information can still be gleaned from shared gradients that
are passed on between organizations during model training. Additionally, in medical imaging, quantifying model
confidence/uncertaintyaccuratelyiscrucialduetothenoiseandartifactspresentinthedata. Uncertaintyestimation
in FL encounters unique hurdles due to data heterogeneity across organizations. This paper offers a comprehensive
reviewofFL,privacypreservation,anduncertaintyestimation,withafocusonmedicalimaging. Alongsideasurvey
ofcurrentresearch,weidentifygapsinthefieldandsuggestfuturedirectionsforFLresearchtoenhanceprivacyand
addressnoisymedicalimagingdatachallenges.
Keywords: FederatedLearning,MedicalImaging,PrivacyPreservation,UncertaintyEstimation,Review
1. Introduction These ML models enable research scientists and clinical
care teams to comprehend and interpret complex health-
Notably over the last decade, machine learning (ML) care data accurately and efficiently. One key component
approacheshavebeenleveragedfortheanalysisofmedi- intrainingeffectiveMLmodelsiscuratinglargedatasets
calimagingtoimprovethepredictionofrisk,earlydetec- necessary for training in the selected domain. This crit-
tion, diagnosis, treatment, and survival outcomes of nu- ical requirement presents a problem in the analysis of
merous diseases [1, 2, 3, 4, 5, 6, 7, 8]. ML models have medicalimagingduetoprivacyregulationsregardingpri-
beenusedinclinicalresearchapplicationsleveragingra- vatehealthdata,suchastheHealthInsurancePortability
diological data, such as CT, MRI, PET, and more [9]. andAccountabilityAct(HIPAA)[10]intheUSAandthe
General Data Protection Regulation (GDPR)[11] in Eu-
rope. Theseregulationsaredesignedtokeeppatientdata
∗Correspondingauthor:NikolasKoutsoubis
secure and private, which makes it challenging to curate
∗∗email:Niko.Koutsoubis@moffitt.org
PreprintsubmittedtoMedicalImageAnalysis June19,2024
4202
nuJ
81
]GL.sc[
1v51821.6042:viXraandcombinelarge-scaletrainingdatasetsacrossmultiple accuracies based on different levels of privacy. An in-
sites. herenttrade-offexistsinmanyprivacypreservationtech-
AconventionalmethodoftrainingMLmodelsiscen- niques where, as privacy increases, model performance
tralized learning, which involves pooling data at a sin- decreases. Sidestepping this trade-off is explored in fur-
gle location from all sources (e.g., sites). This may be therdetailinSection3.
challenging for medical datasets. A solution to circum- AnotherkeyareainwhichFLneedstoexcelinmedi-
vent centralized learning that has risen in popularity in calimagingisuncertaintyestimation,whichistheprocess
recentyearsisFederatedLearning(FL).FLwasfirstpro- of measuring the reliability of a prediction/classification
posedbyGooglefortrainingMLmodelsonedgedevices made by an ML model [19]. ML models in the medical
without sharing client data [12]. FL provides a method domainwillultimatelybeusedtoaidcliniciansinthedi-
fortrainingondatafrommultiplesiteswithoutdataever agnosis and treatment of potentially life-threatening dis-
leavingthelocalsite.Thisallowslarge-scalemodeltrain- eases. Hence, it is of utmost importance that the models
ingwithoutviolatingprivacyregulationsthatoftenhinder have a way to notify users when they make uncertain or
transferring and sharing data across sites. The overarch- low-confidence predictions. Uncertainty estimation is a
ingpremisebehindFListhatratherthantransferringdata, widely studied field [20]. However, FL presents unique
FL transfers training information (gradients) updates be- challenges for uncertainty estimation. The non-i.i.d. na-
tween sites. This permits multiple sites to act as clients tureofdatainmanyFLapplications,dataimbalanceson
andtrainaglobalmodellocatedatthecentralserver. The the client side, and variable computational overhead re-
global model is expected to outperform all local models quire modifications to traditional uncertainty estimation
on all data as it will learn from all local models without methods to work in FL environments successfully [19].
sharingprivatedata. Various methods have been explored in recent years and
Inreal-worldsettings,however,datadistributionsmay willbediscussedfurtherinSection4.
differ across sites attributed to patient demographics, lo- FLholdsthepotentialtosignificantlyimprovetherole
cation, andotherfactors[13]. Thisviolatestheindepen- of ML models in the medical imaging domain, helping
dentandidenticallydistributed(i.i.d)assumptionofdata cliniciansbetterdiagnoseandtreatpatients. However,FL
andposestechnicalchallengesforeffectivelearningina alonecannotworkinthemedicalimagingdomaindueto
distributed FL setting. Many advances in FL techniques heterogeneousdatasets,privacyregulations,andconcerns
havecomeaboutinrecentyears, modifyingthemethods aboutconfidenceinthemodeloutput.Extensiveworkhas
usedtooptimizeFL’sabilitytolearnfromheterogeneous beenconductedinrecentyearstomitigateandsolvethese
data while limiting communication costs to improve ac- challengestoelevateFLasamainstreammethodfortrain-
curacy and efficiency. However, retaining client data lo- ingMLmodelsinmedicalimaging[21,22,23]. Solving
callyisnotenoughtoguaranteedataprivacy. Ithasbeen theseissueswouldallowlarge-scalemodelstobetrained
shown in multiple implementations of FL that through onawidevarietyofdata,significantlyimprovingtheutil-
carefully planned methods, private data can be extracted ity of these models for clinicians and researchers alike.
fromthecommunicationsthattakeplaceinanFLsetting. This work presents a comprehensive review of the state-
Communications such as gradient updates can be used of-the-artmethodsofFLinthemedicalimagingdomain.
to reconstruct patient data through attacks such as gra- A summary of the aspects of FL and the topics covered
dient dis-aggregation [14], model inversion attack [15], inthispapercanbeseeninFigures1and2,respectively.
andothermethods[16]. Furtherenhancementsinprivacy Theprimarycontributionsofthisworkinclude:
preservationarecriticalforFLtobeaparadigm-shifting
• Areviewofthecurrentstate-of-the-art(last5years)
technology to improve ML models in medical imaging.
FL methods proposed in the medical imaging do-
Methodssuchasdifferentialprivacy(DP)[17]andhomo-
main to deal with non-i.i.d. data in real-world set-
morphicencryption(HE)[18]havebeenleveragedtoim-
tings.
provecommunicationssecurity. Theadvantagesofthese
privacy preservation methods are that they can provide • Areviewofthestate-of-the-artprivacypreservation
mathematicalguaranteesandshowtheoreticalmaximum methodsextendedtoFLtoguaranteedataprivacy.
2Figure1:AnoverviewofFL,privacypreservation,anduncertaintyestimationispresented.
• A review of uncertainty estimation methods effec- and send gradient information to a central server to up-
tively applied in FL settings, enabling trustworthy date a global model that, in theory, can outperform all
andreliablemodeldevelopment. local models on all data [12]. FL is particularly power-
ful and applicable in medical ML research because the
• Exploration of a number of real-world use cases of
dataneverhastoleavelocalsites,sidesteppingmanypri-
FLinthemedicalimagingdomainandwhatcanbe
vacy regulations to protect private health data [22]. Ad-
learnedfromthesesuccessstories.
ditionally, medical imaging research often requires large
• Current challenges in FL, data privacy, uncertainty volumes of data, frequently reaching terabytes or more,
estimation,andpotentialopportunitiesanddirection makingdatatransferchallengingforcentralizedtraining.
forfutureresearch. ThefirstnotableFLalgorithm,FedAvg,trainslocalmod-
els across various clients and then averages the gradient
The paper is organized as follows: Section 2 presents updates at the central server to update the global model
a review of FL methods. Section 3 reviews the current [12]. The ML models trained with FL can perform at
stateofprivacypreservationinFL.Section4exploresre- the same level of accuracy compared to those obtained
cent advancements in uncertainty estimation in FL. Sec- using traditional centralized learning in many real-world
tion5coversthereal-worldapplicationsofFLinmedical medical applications [36]. We have identified four main
imaging. Section 6 covers some current challenges and challenges/bottlenecksthatstillexistincreatingareliable
opportunitiesforresearchinFLformedicalimaging. Fi- FL framework for training medical imaging ML models
nally, Section 7 concludes the paper. A repository with [37,19]:
linkstopapersreviewedinthisworkcanbefoundinthis
AwesomeList. • Privacy and security challenges: While data re-
mainslocaltoeachsite,privatedatacanstillbeex-
tracted from the gradient updates sent to and from
2. FederatedLearning(FL)
thecentralserverorbetweensites[37,19].Thiscalls
FL was first proposed in [12] with the FedAvg algo- forenhancedprivacypreservationtechniques,which
rithmfortrainingmodelsonedgedeviceswithoutexpos- willbediscussedinSection3.
ingprivatedata. ThisledtoaparadigmshiftinhowML
models could be trained on sensitive and private data in • Heterogeneous and non-i.i.d. data distribution:
distributed settings [34, 35]. The general idea of FL is Duetovariationsindemographics,location,medical
that multiple clients train local ML models on their data imaging equipment, and a variety of other factors,
3Figure2:Summaryoftopicscoveredinthisreview
data between sites often violate the i.i.d. assump- ortheexistingmethodsmustbeadaptedforFL.The
tion, which can inhibit the models’ ability to learn current state of progress on the uncertainty estima-
[37, 19]. Various strategies are being developed to tionisdiscussedinSection4.
addressthisissue.
A summary of the topics covered in this section can be
• Significant communication overhead: Since data seen in Figure 3, and a table of comparisons as well as
never leaves the client’s site, there must be signif- ashortdescriptionofeachFLalgorithmreviewedispre-
icant communication between clients and the cen- sentedinTable1.
tral server. This could be costly, especially with
large gradient updates being sent very frequently. 2.1. FLAlgorithms
Communication-efficient FL frameworks must be FL frameworks can be divided into three main cate-
designedforFLtosucceedinmedicalimagingtasks gories[37]:
[37,19].
• Horizontal FL - The dataset of each client has a
• Uncertainty estimation: The models used in med- largeroverlapofdatafeaturesthansites. Thismeans
ical imaging will aid clinicians in the diagnosis therearemoreshareddatafeaturesandfewershared
and treatment of potentially life-threatening dis- users. Horizontal FL focuses on the feature dimen-
eases. Therefore, a method for quantifying the un- sionofthedataandextractspartswiththesamechar-
certaintyinmodelpredictionmustbeintegratedinto acteristics but different users for joint training[38].
FLframeworks[37,19]. Duetothenon-i.i.d. nature Horizontal FL finds its usage in fields such as key-
of the data in FL, new methods must be developed, word spotting, emoji prediction, and blockchain
4Figure3:SummaryofFLtopics
[39, 40, 41, 12]. Horizontal FL offers significant culttocoordinateupdatesandmaintainmodelqual-
benefits in terms of privacy and data security [38]. ity. Moreover,HorizontalFLissusceptibletosecu-
It enables collaborative model training without ex- ritythreats,includingmodelpoisoningandinference
posing individual data points, thereby safeguarding attacks,whichcancompromisethemodel’sintegrity
sensitiveinformationandenhancingprivacy. Byag- andpotentiallyrevealsensitiveinformation.
gregating insights from diverse sources, horizontal
FL improves model accuracy and robustness due to • Vertical FL - VFL is characterized by a scenario
the abundance of data. This approach also reduces where client datasets have more overlapping users
the risks associated with centralized data storage, than overlapping data features. Each client dataset
suchasbreachesandmisuse,andsupportsregulatory has more shared users, but data features are rarely
compliance efforts, like GDPR, by keeping data lo- duplicated. Vertical FL is based on feature dimen-
calized and within regulatory boundaries. Horizon- sionsandrequiresdataalignmentbasedoncommon
tal FL faces several challenges that can impact its users during joint training. After aligning samples
efficiency and effectiveness [42]. The frequent ex- from each participant’s data, training is conducted
change of model updates between participants and on the selected datasets [43]. Vertical FL has seen
the central server, known as communication over- usageinthemedicaldomain, financialdomain, and
head,canbebandwidth-intensive. Additionally,het- malware detection [44, 45, 46] Vertical FL proves
erogeneity in data distribution, device capabilities, to be exceptionally efficient in scenarios that de-
and network connectivity can hinder model conver- mand the integration of datasets to uncover new in-
gence and performance. As the number of partici- sights, thereby facilitating cross-industry collabora-
pantsgrows,scalabilityissuesarise,makingitdiffi-
tions [44]. Additionally, its alignment with regu-
5Table1: FLalgorithms
Algorithm Ref Central Local Summary
Server Forget-
ting
FedAvg [12] yes no Trainlocalmodelsacrossvariousclientsandthenaveragethegradientupdates
atthecentralservertoupdatetheglobalmode;firstproposedmethodofFL.
FedProx [24] yes no Excelsinheterogeneoussettings;generalizationoftheFedAvgalgorithm;allows
forpartialupdatestobesenttotheserverinsteadofsimplydroppingthemfrom
afederatedround;addsproximaltermthatpreventsanyoneclientfromhaving
toomuchofanimpactontheglobalmodel.
FedBN [25] yes no Addressestheissueofnon-i.i.d.databyleveragingbatchnormalization;follows
asimilarproceduretoFed-Avgbutassumeslocalmodelshavebatchnormlayers
andexcludestheirparametersfromtheaveragingstep.
TCT [26] yes yes Train-Convexify-Train: Learn features with an off-the-shelf method (i.e., Fe-
davg)andthenoptimizeaconvexifiedproblemobtainedusingthemodel’sem-
piricalneuraltangentkernelapproximation;involvestwostageswherethefirst
stagelearnsusefulfeaturesfromthedata,andthesecondstagelearnstousethese
featurestogenerateawell-performingmodel.
FedAP [27] yes no Learnssimilaritiesbetweenclientsbycalculatingdistancesbetweenbatchnor-
malizationlayerstatisticsobtainedfromapre-trainedmodel;thesesimilarities
areusedtoaggregateclientmodels;eachclientpreservesitsbatchnormalization
layerstomaintainpersonalizedfeatures;theserveraggregatesclientmodelpa-
rametersweightedbyclientsimilaritiesinapersonalizedmannertogeneratea
uniquefinalmodelforeachclient.
FedGen [28] yes no Learnsageneratormodelontheservertoensembleusermodels’predictions,
creatingaugmentedsamplesthatencapsulateconsensualknowledgefromuser
models; generate augmented samples that are shared with users to regularize
localmodeltraining,leadingtobetteraccuracyandfasterconvergence.
FOLA [29] yes yes BayesianfederatedlearningframeworkutilizingonlineLaplaceapproximation
toaddresslocalcatastrophicforgettinganddataheterogeneity; maximizesthe
posteriorsoftheserverandclientssimultaneouslytoreduceaggregationerror
andmitigatelocalforgetting.
FCCL [30] yes yes Federatedcross-correlationalandcontinuallearningusesunlabeledpublicdata
toaddressheterogeneityacrossmodelsandnon-i.i.ddata,enhancingmodelgen-
eralizability;constructsacross-correlationmatrixonmodeloutputstoencourage
classinvarianceanddiversity;employsknowledgedistillation,utilizingboththe
updatedglobalmodelandthetrainedlocalmodeltobalanceinter-domainand
intra-domainknowledgetomitigatelocalforgetting.
Swarm [31] no yes Modelparametersaresharedviaaswarmnetwork,andthemodelisbuiltinde-
Learn- pendentlyonprivatedataattheindividualsites;onlypre-authorizedclientsare
ing allowedtoexecutetransactions;on-boardingnewclientscanbedonedynami-
cally.
ProxyFL [32] yes no Clientsmaintaintwomodels,aprivatemodelthatisneversharedandapublicly
sharedproxymodelthatisdesignedtopreservepatientprivacy;proxymodels
allowforefficientinformationexchangeamongclientswithoutneedingacen-
tralizedserver;clientscanhavedifferentmodelarchitectures.
FogML [33] no no Fogcomputingnodesresideonthelocalareanetworksofeachsite;fognodes
canpre-processdataandaggregateupdatesfromthelocallytrainedmodelsbe-
foretransmitting,reducingdatatrafficoversendingrawdata.
latory compliance mandates makes it an attractive dards. The requirement for precise data alignment
option for industries looking to leverage collective acrossdifferentdatasetsintroducescomplexity, par-
data insights while maintaining strict privacy stan- ticularlywithlarge-scaledatafrommultiplesources,
6making the process challenging [44]. Vertical FL the conditional distribution Pi(x|y) varies across clients
incurs significant communication overhead during and P(y)isthesame, wherefeaturesare xandlabelsare
model training, which can strain bandwidth and la- y on each client. FedBN uses the same premise as Fe-
tency, thereby acting as a potential bottleneck. Its dAvg,sendinglocalupdatesandaveragingatacoordina-
applicabilityisalsosomewhatlimited,asitnecessi- tor. However, FedBN assumes local models have batch
tatesconditionswheredatasetssharethesamesam- normalization layers and excludes their parameters from
ple space but differ in feature space, restricting its theaveragingstep.
use to specific scenarios. Moreover, despite its ad-
vantagesinprivacypreservation,VerticalFLremains
2.1.3. Train-Convexify-Train
vulnerabletosecuritythreats,includinginferenceat-
Despite advancements offered by FedBN, Yu et al.
tacks [47], where adversaries could potentially ex-
pointoutthechallengesduetothenon-convexityofdata
tract sensitive information from the model updates,
[26]. The authors point out that local models with dif-
therebyposingarisktodataprivacy.
ferent local optima can cause the global model to strug-
• Federated Transfer Learning - In many real-world gle to converge and hinder accuracy improvement [26].
scenarios, the datasets owned by each client can They find that the early layers of an FL model do learn
vary considerably. Federated transfer learning ad- useful features, but the final layers do not make use of
dressesthesesituationsbyenablingtheconstruction them. Thatis,federatedoptimizationappliedtothisnon-
ofaneffectiveglobalmodeldespiteminimaloverlap convex problem distorts the learning of the final layers.
in dataset features and samples. Federated transfer Tosolvethisissue,theyproposeaTrain-Convexify-Train
learning facilitates the development of models with procedure, which involves learning features with an off-
limiteddataandfewerlabelswhileadheringtodata the-shelfmethod(i.e.,Fedavg)andthenoptimizingacon-
privacyandsecurityregulations. vexified problem obtained using the model’s empirical
neural tangent kernel (eNTK) approximation. This tech-
nique provided up to 37% improvements in accuracy on
2.1.1. FedProx
dissimilardataoverFedAvgalone. Theconvexityaspect
A variety of extensions to the original FedAvg algo-
attemptstocomputeaconvexapproximationofthemodel
rithm have been proposed, such as FedProX [24]. Fed-
usingitseNTKbasedontheconceptoftheneuraltangent
ProXisanalgorithmforFLthatexcelsinheterogeneous
kernel (NTK) [48]. The eNTK approximates the fine-
datasettingsandservesasageneralizationoftheFedAvg
tuningofapre-trainedmodel. Train-Convexify-Trainhas
algorithm, and FedAvg is considered a special case of
twostages[26]:
FedProX[24]. FedProXallowspartialupdatestobesent
totheserverinsteadofsimplydroppingthemfromafed-
• Stage 1 - Extract eNTK features from a trained Fe-
eratedroundandaddingaproximaltermthatpreventsany
dAvgmodel. FedAvgisfirstusedtotrainthemodel
client from contributing too much to the global model,
foranumberofcommunicationrounds. Then,each
therebyincreasingmodelstability.
clientlocallycomputessub-sampledeNTKfeatures.
2.1.2. FedBN • Stage2-Decentralizedlinearregressionwithgradi-
Another notable high-performing FL algorithm is entcorrection. Givensamplesoneachclientk,first,
FedBN[25]. ThismethodoutperformsbothFedAvgand normalizetheeNTKinputsofallclientswithasingle
FedproX. This method addresses the issue of non-i.i.d. communicationround. Then,solvethelinearregres-
data by leveraging batch normalization. The authors in- sion problem with standard local learning rate and
troducetheconceptoffeatureshiftinFLasanovelcate- localstepsM.
goryofaclient’snon-i.i.ddatadistribution,wherethefol-
lowingtypesoffeatureshiftscanoccur:1)covariateshift: In this procedure, the first stage learns useful features
themarginaldistributionsPi(x)variesacrossclients,even from the data, and the second stage learns to use these
if Pi(y|x) is the same for all client; and 2) concept shift: featurestogenerateawell-performingmodel[26].
72.1.4. FedAP a generator model on the server that ensembles the pre-
Personalized FL involves training personalized mod- diction rules from user models. This generator can pro-
els for various clients to deal with large data hetero- duce augmented samples that convey consensual knowl-
geneity. Personalized FL balances the need for a gen- edgefromtheusermodels. Thegeneratorissharedwith
eralized model and the benefits of localized, personal- theusersandprovidesadditionalsamplesthatregularize
ized models, makingit apromising approach inapplica- local model training. This distills the aggregated knowl-
tions where data privacy and customization are key con- edge into the user models. Sharing the lightweight gen-
cerns. One notable personalized FL algorithm is FedAP erator introduces minimal communication overhead and
[27]. FedAPlearnssimilaritiesbetweenclientsbycalcu- increases security because the full model is not shared.
latingdistancesbetweenbatchnormalizationlayerstatis- FedGen achieves better accuracy and faster convergence
tics obtained from a pre-trained model. These similar- than state-of-the-art methods under heterogeneous set-
ities are used to aggregate client models. Each client tings. Benefits are especially notable as the data hetero-
preserves its batch normalization layers to maintain per- geneityincreases.Tomodelnon-iiddatadistributions,the
sonalized features. The server aggregates client model authorsfollowtheworkdonebyLinetal.[57]andHsuet
parameters weighted by client similarities in a personal- al. [58],usingaDirichletdistributionDir(α)inwhicha
ized manner to generate a unique final model for each smallerαindicateshigherdataheterogeneity,asitmakes
client. The authors evaluated FedAP on five health- thedistributionmorebiasedforauserk.
care datasets across modalities [27] including the pub-
lic human activity recognition dataset, PAMAP2 [49], 2.1.6. FederatedOnlineLaplaceApproximation(FOLA)
COVID-19chestscandataset[50], MedMNIST,MedM- In addition to data heterogeneity, another challenging
NISTv2[51,52],thelivertumorsegmentationbenchmark issue in FL is local catastrophic forgetting. That is, the
[53], and OrganAMNIST, OrganCMNIST, OrganSM- local models forget the specific attributes of their data
NIST [54]. FedAP achieves more than 10% accuracy when they are updated with the global model because
over state-of-the-art FL models. FedAP converges faster the global weights overwrite the model weights. Catas-
thanothermethods,10roundsvs. morethan400rounds trophic forgetting is also a challenge in continual learn-
forFedBNwhilebeingrobusttovaryinghyperparameters ing [59, 60]. To combat this problem and data hetero-
and different degrees of non-i.i.d data distribution shifts geneity, a Bayesian FL algorithm with online Laplace
amongclients. approximation is proposed by [29]. Federated Online
Laplace Approximation (FOLA) operates by integrating
2.1.5. FedGeN Bayesian principles with an online approximation ap-
Knowledgedistillationisanotheremergingmethodfor proach, thereby effectively estimating probabilistic pa-
dealing with the challenge of data heterogeneity in FL. rameters of both global and local models in real-time.
BasedonworkdonebyHintonetal.[55].Yangetal.[56] This approach addresses aggregation error and local for-
implemented knowledge distillation for multi-organ seg- getting by efficiently approximating Gaussian posterior
mentation in a federated paradigm. Knowledge distilla- distributions in a distributed setting. A Gaussian prod-
tioninvolvesextractingusefulknowledgefromanensem- uct method is used to construct a global posterior on the
bleofmodels. ThishasanaturalextensiontoFLasmul- serversideandaprioriterationstrategytoupdatethelo-
tipleclientscanserveasanensemblefromwhichknowl- cal posteriors on the client sides, both of which are easy
edge can be distilled. One successful implementation of tooptimize. Successfullymaximizingtheseposteriorsof
Federated knowledge distillation is FedGen [28]. Most theserverandclientssimultaneouslyreducesaggregation
knowledge distillation methods require a proxy dataset errorandlocalforgetting[29].
to distill the knowledge. FedGen proposes a data-free
method, thereby removing the need for this proxy and 2.1.7. Federated Cross-Correlation and Continual
making knowledge distillation more accessible to appli- Learning(FCCL)
cations where a proxy dataset cannot be created due to AnothermethodthataddresseslocalforgettingisFed-
lack of data or privacy restrictions [28]. FedGen learns eratedCross-CorrelationandContinualLearning(FCCL)
8[30]. To handle heterogeneity across models and non- fromtraditionalFListhatProxyFLallowseachclientto
i.i.ddata,FCCLleveragesunlabeledpublicdataandcon- havemodelheterogeneity,meaningthateachclient’spri-
structsacross-correlationmatrixonthemodels’logitout- vatemodelcanhaveanyarchitecture. Theproxymodels
puts. This encourages correlation among the same logit alsoutilizeDPtoimproveprivacy. ProxyFLcanoutper-
dimensions(classinvariance)whilede-correlatingdiffer- formexistingalternativeswithmuchlesscommunication
ent dimensions (class diversity) to learn a more gener- overheadandstrongerprivacy.
alizable representation. To alleviate catastrophic forget-
tingduringlocalupdates,FCCLemploysknowledgedis-
tillation using the updated global model (to retain inter- 2.2.3. Fog-FL
domain information learned from others) and a trained Fog-FLenhancestheefficiencyandreliabilityofcom-
local model (to retain intra-domain information) without putingusingadecentralizedcomputinginfrastructurebe-
leaking privacy. This continually balances knowledge, tween the data source and the cloud, known as fog
helpingtoalleviatecatastrophicforgetting. computing[33]. It provides computing, storage, and net-
working services closer to where the data is generated,
2.2. DecentralizedFLalgorithms i.e.,atthenetworkedge. Fogcomputingnodesresideon
thelocalareanetworksofeachmedicalinstitution. These
ManyFLalgorithmsutilizeacentralserverwherelocal
Fogcomputingnodescanpre-processdataandaggregate
model updates are sent from clients to update the local
updates from the locally trained models before transmit-
model.However,manyFLimplementationsdonotutilize
tingthemtothecentralFLserverinthecloud. Fogcom-
a central server [31, 32, 33]. This subsection describes
puting reduces the data traffic and latency compared to
somerecentdecentralizedFLalgorithms.
sendingallrawdatadirectlytothecloud.Italsoenhances
privacy and security as sensitive data stays on the local
2.2.1. SwarmLearning
network.
Swarmlearning[31]isadecentralizedlearningmethod
that unites edge computing with blockchain-based peer-
to-peer networking, eliminating the need for a central 2.3. SoftwareFrameworksforFLImplementation
coordinating server. This approach combines decen-
tralized hardware infrastructures and distributed ML us- An FL project involves many moving parts, requiring
ingstandardizedengineswithapermissionedblockchain coordination between all clients and the central server.
to securely onboard members, dynamically elect the Tofacilitatethisprocess,severalopen-sourceframeworks
leader, and merge model parameters. Model parameters have been developed to aid in setting up and managing
are shared via a swarm network, allowing independent federatedruns.
model building on private data at individual sites. Secu-
rity and confidentiality are ensured by the permissioned
blockchain(makingeachclientwell-definedasapartici- 2.3.1. OpenFL
pant),whichrestrictsexecutiontopre-authorizedclients. OpenFL is an open-source Python library for FL[61].
Newclientscanbedynamicallyonboarded. This framework supports both Tensorflow and PyTorch
projects,andtheworkflowisdefinedbyafederationplan
2.2.2. ProxyFL thatallsitesagreeuponbeforebeginning. OpenFLusesa
ProxyFL [32] was proposed by Kalra et al. as a staticnetworktopologywithclientsconnectingtoacen-
communication-efficient scheme for decentralized FL. tralaggregatingserveroverencryptedchannels. OpenFL
Each client maintains two models: a private model that was designed with medical imaging in mind and is set
isneversharedandapubliclysharedproxymodelthatis up for horizontal FL but can be extended to other types.
designedtopreserveprivacy. Usingproxymodelsallows OpenFL allows easy migration of centralized ML mod-
forefficientinformationexchangeamongclientswithout els into a federated training pipeline and is designed for
needing a centralized server. One massive step forward real-worldscalabilityandtrustedexecution.
92.4. NVIDIAMONAI,FLARE,andClara using the CIFAR-10 [66] dataset and ResNet56 model
and explored FL with different numbers of clients (N)
NVIDIAMONAI,FLARE,andClaraarethreeintegral
andtwotypesofdatadistributions,i.i.dandnon-i.i.d[65].
frameworks developed by NVIDIA to advance FL and
medicalimaging[62],[63],[64]. MedicalOpenNetwork
forAI(MONAI)isanopen-sourceframeworkoptimized
for healthcare, providing domain-specific tools and deep
learningmodelstostreamlinethedevelopmentofmedical 3. PrivacyPreservationinFL
imagingsolutions.ItintegratesseamlesslywithFederated
Learning Application Runtime Environment (FLARE), Ensuringthesecureprocessingofprotectedandidenti-
anotheropen-sourceSDKbyNVIDIAdesignedtofacil- fiableinformationisacriticalpriorityinthemedicalfield.
itate FL. FLARE supports various FL architectures and Federalregulationsstrictlyprohibitthesharingofpatient
incorporatesrobustprivacy-preservingtechniqueslikeDP data to prevent privacy breaches. FL addresses this is-
andHE.Clara,specificallyClaraTrain,isamedicalimag- suebykeepingdatalocalizedateachsite. However,even
ing platform that leverages FLARE to enable FL within with data remaining local, privacy can still be compro-
its ecosystem. Some key components of this NVIDIA mised. The gradient updates exchanged between clients
FLAREinclude[64]: and the server can potentially reveal information about
the training data, leading to privacy leaks. The attacks
• An FL simulator for rapid development and proto-
likethegradientdis-aggregationattack[14]highlightthe
typing.
need for enhanced privacy measures in FL to safeguard
• Adashboardforsimplifiedprojectmanagement,se- sensitive information effectively. The topics covered in
cureprovisioning,anddeploymentorchestration. thissectionaresummarizedinFigure4andTable2.
• ReferenceFLalgorithmslikeFedavg, fedproX,and
FedOpt,withworkflowslikescatterandgather. 3.1. DifferentialPrivacy(DP)
• PrivacypreservationoptionslikeDP,HE,andothers. Oneofthemostpopularmethodsforprivacypreserva-
tionisDP[17],whichintroducesnoiseintothegradients
• Specification-basedAPIforcustomimplementations
topreventprivateinformationleakage. DPhasbeenused
in medical imaging applications [76]. The DP method
• TightintegrationwithframeworkslikeMONAI.
provides mathematical guarantees of privacy. However,
the guarantees come at the cost of accuracy and conver-
2.5. ConvergenceofModelLearninginFL
gence[77,78,79].
MLmodelstrainedusingfederatedrunscanstruggleto
convergeduetothenon-i.i.dnatureofthemodeltraining
data. Conventional ML training convergence analysis 3.1.1. NoisingbeforemodelaggregationFL(nbAFL)
methodsarenotnecessarilysuitedforFLsettings. Huang nbAFL adds artificial noise to parameters at the client
et al. propose FL Neural Tangent Kernel (FL-NTK) side before aggregation to ensure DP and then proposes
[65] to perform convergence analysis of FL algorithms. K-random scheduling to find optimal convergence [67].
FL-NTK analyzes the convergence and generalization In K-random scheduling, K clients are chosen randomly
of FL algorithms in the context of over-parameterized toparticipateintheaggregationprocess,ensuringnotall
Rectified Linear Unit (ReLU) neural networks. The informationiscommunicatedineveryround. Thismakes
authorsshowthatFL-NTKconvergestoaglobal-optimal it harder for attackers to extract useful information. An
solution at a linear rate with properly tuned learning optimal value of K must be found for a given privacy
parameters, such as quartic width [65]. The framework level. This trade-off is often referred to as privacy bud-
offers insights into different FL optimization and ag- get allocation. nbAFL can balance the privacy level and
gregation methods. The authors conducted experiments thedesiredaccuracybasedontheapplication.
10Table2:PrivacyPreservationMethodsinFL.
Algorithm Ref Differential Homomorphic Summary
Privacy Encryption
(DP) (HE)
NbAFL [67] yes no NoisingbeforeaggregationFL(NbAFL)UsesK-randomschedulingtooptimize
theprivacyandaccuracytrade-offbyintroducingartificialnoiseintotheparam-
etersofeachclientbeforeaggregation.
Adaptivepri- [68] yes no AdaptiveallocationoftheprivacybudgetacrossFLiterations;theaimistoopti-
vacy mizetheuseoftheprivacybudgetbasedonthedatadistributionandthemodel’s
learning status; higher privacy budget allocated earlier in training, and lower
budgetlatertooptimizeprivacybudgetutilization.
FedOpt [69] yes yes UtilizessparsecompressionandHEforsecuregradientaggregationandDPfor
enhancedprivacy.
SHEFL [70] yes yes SomewhathomomorphicallyencryptedFL(SHEFL);onlycommunicatingen-
cryptedweights;allmodelupdatesareconductedinanencryptedspace.
Hybrid Ap- [71] yes yes CombiningDPwithsecuremultipartycomputationenablesthismethodtore-
proach ducethegrowthofnoiseinjectionasthenumberofpartiesincreaseswithout
sacrificingprivacy;thetrustparameterallowsformaintainingasetleveloftrust.
PrivateKT [72] yes no Private knowledge transfer method that uses a small subset of public data to
transferknowledgewithlocalDPguarantee;selectspublicdatapointsbasedon
informativenessratherthanrandomlytomaximizetheknowledgequality.
Multi- [73] yes no Providesprivacyguaranteesovermultipletrainingrounds;developsastructured
RoundSecAgg usersectionstrategythatguaranteesthelong-termprivacyofeachuse.
LDS-FL [74] no no Maintaintheperformanceofaprivatemodelpreservedthroughparameterre-
placementwithmulti-userparticipationtoreducetheefficiencyofprivacyat-
tacks.
DeTrust-FL [75] no no Providessecureaggregationofmodelupdatesinadecentralizedtrustsetting;
implementsadecentralizedfunctionalencryptionschemewhereclientscollab-
oratively generate decryption key fragments based on an agreed participation
matrix.
11Figure4:SummaryofprivacypreservationmethodsinFL.
3.1.2. Adaptiveprivacybudgetallocation [80]implement a two-step method for DP. First, multiple
One method for a well-designed privacy budget was patchesareextracted,andamosaicisformedfortraining
proposed by Nampelle et al. [68]. The authors demon- usingamemorynetworkandanattention-basedmultiple
stratethatstrategiccalibrationoftheprivacybudgetinDP instant learning algorithm that provides privacy bounds
can uphold robust performance while providing substan- locally. The local models with DP are then aggregated
tialprivacyguarantees. Theyproposeanadaptiveprivacy at the central server. The method was tested on simu-
budgetallocationstrategyforFLroundsthatbestupdates lated real-world data in both i.i.d and non-i.i.d. settings.
the privacy budget in each round based on the data dis- Contrarily, Choudhury et al. [81] found that although
tributionandmodellearningprogress. Thekeyaspectof DP guarantees a given level of privacy as set by its pa-
theirmethodsistheadaptiveallocationoftheprivacybud- rameters,itsignificantlydeterioratestheutilityoftheFL
getacrossFLiterations. Theaimistooptimizetheuseof model. The model’s performance can only be preserved
theprivacybudgetbasedonthedatadistributionandthe with a very large number of sites, on the order of 103,
model’slearningstatus.Thestrategyistoallocatemoreof but suffers severely in cases with fewer sites. Such an
thebudgetintheearlieriterationsofFL,wherethemod- assumption of large-scale setup is unrealistic for health-
els learn more from the data. Later iterations have less care applications, where sites are typically hospitals or
privacybudgetallocatedasthegradientshavelessinfor- providers, and each site may not have sufficient data for
mationaboutthedata. Thisdesignoptimizesthetrade-off independentlytrainingdeeplearningmodels.
betweenlearningandprivacy.
3.2. HomomorphicEncryption(HE)
3.1.3. Privacy-performancetrade-offs While DP has proven useful, HE has also been exten-
Differentially private FL can provide comparable per- sivelyexploredinFL[82,83,84].HEisaformofencryp-
formance to centralized learning [80]. The authors in tion that allows computations (mathematical operations)
12tobecarriedoutonciphertexts,generatingencryptedre- SHEFL are on par with regular FL while providing pri-
sultsthat,whendecrypted,matchtheresultofoperations vacyguarantees, showingthat encryptiondoes notnega-
performed on the plain data. Thus, the data can be en- tively impact accuracy [70]. The methods only encrypt
cryptedandsharedwithathirdpartyforprocessingwith- the vulnerable areas of the FL with a less than 5% in-
outthethirdpartyhavingaccesstothedecrypteddata. crease in train time or compute. The authors show the
encryption/decryption process is negligible compared to
3.2.1. FedOpt backpropagation. Whenfacedwithaninversionattack,a
FedOpt [69] provides a communication-efficient normalFLalgorithmcouldhaveitsdatareconstructedin
method for privacy preservation in FL. This method 120iterations,butwithSHELF,thedatawassecureafter
uses a novel sparse compression algorithm to reduce 40,000iterations[70].
communication overhead by extending top-k gradient
compression with a downstream compression mecha- 3.3. Othermethodsofprivacypreservation
nism. The authors adopt lightweight HE for efficient In addition to DP and HE, other methods have been
and secure aggregation of gradients, using additive HE used to preserve privacy in FL, such as using the afore-
without key-switching to increase plain-text space [69]. mentionedmethodsinconjunctionwithothertechniques
The authors also employ DP. FedOpt is robust to user tooptimizesecurity.
dropouts during training, with little impact on accuracy.
Evaluations show that FedOpt outperforms state-of-the- 3.3.1. Ahybridapproach
artapproacheslikeFedAvgandPPDLinmodelaccuracy, A hybrid approach to privacy-preserving FL is pro-
communicationefficiency,andcomputationoverhead. posed in [71] that uses DP and secure multi-party com-
putationtobalancethetrade-offbetweenprivacyandac-
3.2.2. SomewhatHomomorphicEncryption(SHE) curacy. Combining DP with secure multiparty computa-
SHE is a subset method of HE [85] and is a type of tion enables this method to reduce the growth of noise
encryptionthatallowsforalimitednumberofarithmetic injection as the number of parties increases without sac-
operationsonencrypteddata.UnlikeFullyHomomorphic rificing privacy while maintaining a pre-defined rate of
Encryption (FHE), which supports unlimited operations, trustwithatuneabletrustparameterthatcanaccountfor
SHEhasconstraintsonthenumberandtypeofoperations variousscenarios. Thetrustparametertreferstothemin-
thatcanbeexecuted.SHEisgenerallymoreefficientthan imum number of honest, non-colluding parties the sys-
FHE because it deals with a restricted set of operations temassumes[71]. Thisparametercapturesthedegreeof
[85]. Thismakesitmorepracticalforapplicationswhere possible adversarial knowledge by specifying the maxi-
thecomputationaloverheadofFHEwouldbeprohibitive mumnumberofcolludingpartiesthesystemcantolerate
[85]. whilestillprovidingprivacyguarantees. Thenoiseadded
by each honest party depends on t. As t decreases (less
3.2.3. Somewhat Homomorphically Encrypted FL trust),morenoiseneedstobeaddedbyeachhonestparty
(SHEFL) toaccountformorepotentialcolluders. Thethresholden-
Truhn et al. leveraged SHE and proposed SHEFL, cryptionschemeusest tosetthethresholdsothatnoset
whichenablesmultiplepartiestoco-trainMLmodelsfor of parties less than this threshold can decrypt data. This
pathologyandradiologyimagessecurely,reachingstate- prevents smaller colluding groups from learning honest
of-the-artperformancewithprivacyguaranteeswhilere- parties’data. Thetrustparametertisusefulinpreventing
quiring negligible extra computational resources [70]. dishonestpartiesfromactingasclientsandgainingaccess
SHEFL provides a solution to privacy concerns by only tohonestclients’data.
communicatingencryptedweights,andmodelupdatesare
conductedinanencryptedspace. Theauthorsimplement 3.3.2. PrivateKT
SHEFL on two clinical use cases - segmenting brain tu- One method that leverages DP to implement private
mors[86]andpredictingbiomarkersfromhistopathology knowledge transfer is PrivateKT [72], a private knowl-
slides in colorectal cancer[87]. The models trained with edge transfer method that uses a small subset of public
13datatotransferknowledgewithlocalDPguarantees.This 3.3.3. Multi-RoundSecAgg
methodactivelyselectspublicdatapointsbasedonthein- So et al. point out that many privacy preservation
formationcontentsratherthanrandomlytomaximizethe methods only provide privacy guarantees for a single
knowledgequality. Theknowledgetransfermethodcon- communication round [73]. The authors propose Multi-
tainsthreesteps: RoundSecAgg, which provides privacy guarantees over
multiple training rounds. The authors also introduce a
• Knowledge Extraction: The clients train their mod-
newmetrictoquantifytheprivacyguaranteesofFLover
elsonlocalprivatedata,thenmakepredictionswith
multiple training rounds and develop a structured user
their models on a small set of specifically selected
section strategy that guarantees the long-term privacy of
public data points (KT data) [72]. This process ex-
each user. Multi-RoundSecAgg contains two compo-
tracts knowledge from private data and uses it to
nents: (1)afamilyofsetsofusersthatsatisfythemulti-
makepredictionsaboutpublicdata.
round privacy requirement, and (2) a set from this fam-
• KnowledgeExchange: TheclientslocallyaddDPto ily to satisfy a fairness guarantee. The authors found a
the public data predictions using a randomized re- trade-off between long-term privacy guarantees and the
sponse mechanism to guarantee DP. These DP pre- number of participants. As the average number of users
dictionsarethensenttothecentralserver[72]. increases, long-termprivacybecomesweaker[73]. Ran-
dom user selection schemes are shown to provide very
• Knowledge Aggregation: The central server aggre- weakmulti-roundprivacy. Aftersufficientrounds(linear
gatesDPpredictionsfromallclientsandstoresthem
in number of users), the server can reconstruct all user
inaknowledgebuffer[72].
models. Multi-RoundSecAgg is a structured user selec-
knowledgetransfercansecurelytransferdatabetween tion strategy with provable multi-round privacy. It par-
models and also provide uncertainty estimation through titionsusersintobatchesthatalwaysparticipatetogether.
its functionality. Two methods are implemented to im-
Multi-RoundSecAggprovidesatrade-offbetweenprivacy
prove the effectiveness of knowledge transfer on a small and convergence rate. More privacy reduces the average
amountofpublicdata. number of users per round, slowing down training. The
authors show that structured user selection is necessary
• Importance Sampling - The model’s uncertainty is forlong-termprivacy[73].
measured on each public data point using informa-
tion entropy, and a higher sampling probability is 3.3.4. Loss Differential Strategy for Parameter replace-
assigned to data with higher uncertainty [72]. This ment(LDS-FL)
maximizestheinformationandqualityoftheknowl- One method for privacy preservation that takes a dif-
edgesampledinasmalldataset[72]. ferent approach entirely is the loss differential strategy
for parameter replacement (LDS-FL) [74]. The key idea
• KnowledgeBuffer -TheserverstorestheDPaggre-
of this strategy is to maintain the performance of a pri-
gatedpredictionsfromclientsinabufferthatmain-
vatemodelpreservedthroughparameterreplacementwith
tains a history of past aggregated knowledge [72].
multi-userparticipation. LDS-FLintroducesapublicpar-
Thisbufferisusedtofine-tunetheglobalmodel,en-
ticipant that shares parameters to enable other private
codinghistoricalknowledgetohelpmitigatethelim-
participants to construct loss differential models without
itationsofasmalldataset[72]. Aknowledgebuffer
exposing private data. These satisfy an inequality that
is typically implemented during the knowledge ag-
bounds loss on public, private, and other data. Wang
gregationstep.
et al. propose a loss differential strategy (LDS) where
PrivateKT is tested on MNIST [88], METText, and private participants replace some public parameters with
a Kaggle X-ray image dataset for pneumonia detection. theirowntocreatemodelsthatresistprivacyattacks.This
Underastrictprivacybudget,PrivateKTreducestheper- balances privacy and accuracy [74]. The authors for-
formance gap with centralized learning by up to 84% mallyprovetheprivacyguaranteesoftheLDSapproach
comparedtootherFLmethods[72]. againstmembershipinferenceattacks. Experimentsshow
14that LDS-FL reduces attack accuracy while maintaining • Communicationprotocols
modelaccuracy. Themulti-roundLDSalgorithmenables
participantstoiterativelyconstructlossdifferentialmod- • Neuralnetworkmodels
els in a privacy-preserving and convergent way during • Datafortrainingandtesting
FL [74]. Comprehensive experiments on image datasets
demonstratethatLDS-FLreducesattackaccuracybyover The APPFL framework provides users with the tools
10% on MNIST while reducing model accuracy by just toconducttheirexperimentswithFLandallowsforflex-
0.17% [74]. LDS-FL outperforms DP defenses in accu- ibility in model choice and the ability to implement cus-
racyandattackresistance. ThismethoddoesnotuseDP tommodels[89].APPFLalsoprovidesacommunication-
or HE but rather provides an alternative method for pre- efficient inexact alternating direction method of mul-
servingprivacy, suggestingotherwaystosolvetheissue tipliers (IIADMM) based on the Alternating Direction
ofprivacypreservationinFL. MethodofMultipliers(ADMM)[90]. TheIIADMMal-
gorithm significantly reduces the amount of information
3.3.5. DeTrust-FL transferred between the server and the clients compared
tosimilaralgorithms.
DeTrust-FL[75]proposesasolutiontoenhancethepri-
vacyofFLinadecentralizedsettingandprovidessecure
3.4.2. Privacy-preserving Medical Image Analysis
aggregationofmodelupdatesinadecentralizedtrustset-
(PriMIA)
ting. DeTrust-FL improves other PPFL methods by not
PriMIAisanopen-sourcesoftwareframeworkfordif-
relyingonacentralizedtrustedauthorityandvulnerability
ferentiallyprivate,securelyaggregatedFLandencrypted
toinferenceattackslikedis-aggregationattacks. DeTrust
inference on medical imaging data [91]. The authors
usesadecentralizedfunctionalencryptionschemewhere
tested PriMIA using a real-life case study on pediatric
clientscollaborativelygeneratedecryptionkeyfragments
chest X-rays. They found their privacy-preserving fed-
based on an agreed participation matrix. using a partici-
erated model was on par with local non-securely trained
pationmatrixprovidestransparencyandcontroloverthe
models. They theoretically and empirically evaluate the
aggregation process, as all participants know what they
framework’s performance and privacy guarantees and
agree to. Detrust-FL incorporates batch partitioning to
demonstratethattheprotectionsprovidedpreventthere-
prevent dis-aggregation attacks and encrypts model up-
construction of usable data by a gradient-based model
dateswithroundlabelstopreventreplayattacks. Theau-
inversion attack [91]. The authors successfully employ
thorsshowthatDeTrust-FLachievesstate-of-the-artcom-
munication efficiency and reduces reliance on a central- the trained model in an end-to-end encrypted remote in-
ferencescenariousingsecuremulti-partycomputationto
izedtrustentity[75].
preventthedisclosureofthedataandthemodel.
3.4. PrivacypreservingFLFrameworks
This section presents Frameworks that have been cre- 4. UncertaintyEstimationinFL
ated to streamline the process of privacy preservation in
AnothercriticalareainFLformedicalimagingisun-
FL.
certainty quantification or estimation. Once data privacy
is ensured, assessing the quality of the model becomes
3.4.1. ArgonnePrivacy-PreservingFramework(APPFL)
a crucial focus for researchers. For FL to excel in the
APPFL provides an open-source Python package that
medicalimagingfield,itisessentialtohaveamethodfor
providestoolsforuserstorunFLexperimentswithaddi-
measuringhowcertainthemodelisaboutitspredictions.
tionalprivacypreservationtools[89]. Therearefivemain
Additionally, there should be a mechanism to alert a hu-
componentsofthisframework:
man operator when the model’s certainty falls below ac-
• FLalgorithms ceptable levels. The unique challenge in FL for medical
imaging arises from its non-i.i.d nature, which compli-
• DPschemes cates the quantification of certainty. This complexity is
15Figure5:SummaryofuncertaintyestimationmethodsinFL.
further exacerbated by the phenomenon where local cer- models. Thesemodelsarethenusedforfinalpredic-
taintymightbehigh,butglobalcertaintyislow,andvice tion. However, themainideaofFLislostheredue
versa. This section will discuss various methods to im- tothelackofcommunication[19].
plement uncertainty estimation in FL settings. Figure 5
• Ensemble of global models: In this approach, the
and Table 3 summarize the uncertainty estimation meth-
idea of FL is preserved, however computational
odspresentedinthissection.
overheadisincreased[19].EachworkertrainsS ML
models with different random initialization seeds to
4.1. ModelEnsembling
traineachmodel.ForeachS model,anFLworkflow
Model ensembling is a popular uncertainty estimation
isexecuted. Thiscanquicklybecomecomputation-
method and involves running inference with an ensem-
allyexpensiveasS increases[19].
ble of models and taking the average [106]. This natu-
rally extends to FL because of the distributed nature of • Ensemblebasedonmultiplecoordinators: These
the FL setup involving multiple clients that can serve as methodssplittheworkersintosubgroupsandassign
multiple models. The approach in [19] Integrates mul- a coordinator to each subgroup [19]. FL is carried
tiple ensembling methods into an uncertainty estimation outasnormalamongthesubgroups,andtheoutputs
frameworkforFL.ThevariationsofFLensemblingused of each subgroup are averaged to produce the final
include[19]: prediction.
• Ensemble of local models: This method is a naive Each method presents advantages and challenges, ne-
way of incorporating deep ensemble-based uncer- cessitating careful consideration when used in FL appli-
tainty estimation into FL [19]. This method treats cationsinreal-worldsettings.
each worker’s local model as an ensemble member. Theensembleoflocalmodelsemphasizesprivacyand
Notalltheworkerscommunicatewiththecoordina- simplicitybytreatingeachworker’smodelasanindepen-
tor,whichleadstoamnumberofseparatelytrained dent ensemble member. While this approach maximizes
16Table3: UncertaintyEstimationMethodsinFL.
Algorithm Ref Conformal Distilled Bayesian CalibrationSummary
Predic- Predic-
tion tion
(CP)
Fed- [92] no no no no ExtendsensemblingmethodstoFL;characterizesuncertainty
ensemble inpredictionsbyusingthevarianceinthepredictionsasamea-
sureofknowledgeuncertainty.
DP- [93] yes no no no DifferentiallyPrivateFederatedAverageQuantileEstimation
fedCP (DP-fedCP);themethodisdesignedtoconstructpersonalized
CPsetsinanFLscenario.
FCP [94] yes no no no FederatedCP,aframeworkforextendingCPtoFLthatad-
dressesthenon-i.i.d.natureofdatainFL.
FedPPD [95] no yes no no Framework for FL with uncertainty, where, in every round,
eachclientinferstheposteriordistributionoveritsparameters
andtheposteriorpredictivedistribution(PPD);PPDissentto
theserver.
FedUA [96] no yes no no Fed uncertainty aware - Each client’s uncertainty is quanti-
fied; asamplequalityevaluatorselectshigh-qualitysamples
forglobalmodeltraining;knowledgedistillationsusedinthe
aggregationprocesstotransferinter-classrelationshipsfrom
the local models and suppress noise from incomplete client
data.
FedBNN [97] no no yes no FLframeworkbasedontrainingacustomizedlocalBayesian
modelforeachclient.
pFL [98] no no yes no ThepersonalizedFL(pFL)trainspersonalizedlocalmodelsto
catertothedatasetswhilestillbeingabletolearnfromalarger
datapool.
Self-FL [99] no no yes no Self-awarepersonalizedFLmethodthatusesintra-clientand
inter-clientuncertaintyestimationtobalancethetrainingofits
localpersonalmodelandglobalmodel.
pFedBays [100] no no yes no Weight uncertainty is introduced in client and server neural
networks; to achieve personalization, each client updates its
localdistributionparametersbybalancingitsconstructioner-
roroverprivatedata.
Fedpop [101] no no yes no Eachclienthasalocalmodelcomposedoffixedpopulation
parametersthataresharedacrossclients, aswellasrandom
effectsthatexplainheterogeneityinthelocaldata.
FedFA [102] no no no no Featureanchorsareusedtoalignfeaturesandcalibrateclassi-
fiersacrossclientssimultaneously;thisenablesclientmodels
tobeupdatedinasharedfeaturespacewithconsistentclassi-
fiersduringlocaltraining.
FedAG [103]) no no no no Byintroducingweightuncertaintyintheaggregationstepof
FedAvgalgorithm,theenddevicescancalculateprobabilistic
predictionsbutonlyhavetolearnconventional,deterministic
models.
CCVR [104] no no no yes Classifier calibration with Virtual Representation (CCVR)
Foundagreaterbiasinrepresentationslearnedinthedeeper
layersofamodeltrainedwithFL;theyshowthattheclassi-
fiercontainsthegreatestbiastowardlocalclientdataandthat
classificationperformancecanbegreatlyimprovedwithpost-
trainingclassifiercalibrationcalibration
FedCSPC [105] no no no yes Thismethodtakesadditionalprototypeinformationfromthe
clientstolearnaunifiedfeaturespaceontheserversidewhile
maintainingclearboundaries.
17data privacy and is straightforward to implement, it di- [92].TheserversendsoneoftheKmodelstoeveryclient
vergesfromthecollaborativeessenceofFL.Itmayresult in each training round to train on local data. The server
ininconsistentmodelperformanceduetoisolatedtraining thenaggregatestheupdatedmodelfromeachclient; this
environments. Conversely, the ensemble of global mod- way,theburdenonclientsisnotincreased,andallKmod-
elsalignswiththecollaborativelearningprincipleofFL, els eventually see all the clients’ data. To obtain uncer-
enhancing model robustness by integrating diverse per- tainty predictions in an ensemble of models, the sample
spectives. However, this method significantly increases variance can be used to estimate the uncertainty. Fed-
computationalandcommunicationdemands,posingscal- ensemble can appropriately characterize knowledge un-
ability challenges as the number of clients grows. The certainty on regions without labeled data. Fed-ensemble
third approach, employing multiple coordinators, offers enhancesexistingFLtechniquesbysystematicallyquan-
improvedscalabilitybydistributingtheworkloadandtai- tifyinguncertaintyandincreasingmodelcapacitywithout
loring learning strategies within subgroups. However, raisingcommunicationcosts.UnlikeFedavg,whichtends
this method introduces additional complexity in coordi- to be overconfident in predictions, Fed-ensemble offers
nationandriskslearningfragmentationacrosssubgroups. convergenceguaranteesandeffectivelymanagesdatahet-
Tonavigatethesetrade-offs, consideringhybridoradap- erogeneity through ensembling, outperforming methods
tiveensemblingstrategiesthatbalancecomputationalef- thatrelyonstrongregularizers.
ficiency with the benefits of collaborative learning could
bebeneficial.Ultimately,selectinganensemblingmethod
should be guided by the application’s specific needs, in-
4.2. ConformalPrediction(CP)
cludingprivacyrequirements,availablecomputationalre-
sources,anddataheterogeneity.
CP is another method for uncertainty estimation that
4.1.1. Fed-ensemble has been extensively explored in FL. The idea was first
Fed-ensemble[92]extendsensemblingmethodsforFL proposed in [109] and then improved upon by [110]
using a different approach. Instead of aggregating local around the turn of the century. CP is a statistical frame-
modelstoupdateasingleglobalmodel,thismethoduses workthatisusedtoprovidereliableandvalidconfidence
randompermutationstoupdateagroupof K modelsand measuresforthepredictionsmadebyMLmodels.CPbe-
obtains predictions using model averaging. This method gins by defining a nonconformity measure, which quan-
imposesnoadditionalcomputationalcostsandcanreadily
tifieshowdifferentanewexampleisfromasetofprevi-
beutilizedwithinestablishedFLalgorithms. Theauthors ouslyseenexamples[110]. Thismeasureisbasedonan
empiricallyshowthattheproposedapproachperformssu- ML algorithm trained on a dataset. The non-conformity
perior to other methods on many datasets. It also excels ofanexamplecanbesomethinglikethedistancefroma
inheterogeneoussettings,whichisconsistentwithmany decision boundary in classification or the error of a pre-
FLapplicationslikemedicalimaging. Fed-ensemblecan dictioninregression.
characterize uncertainty in predictions by using the vari- Foranewdatasample,CPgeneratespredictionregions
ance in the predictions as a measure of knowledge un- (or sets) likely to contain the true label or value. This is
certainty. Shi et. al [92] propose performing ensemble donebyconsideringallpossiblelabelsforthenewexam-
FLthatupdates K modelsoverlocaldatasets. Pointpre- ple, calculating the nonconformity score for each label,
dictions are obtained by model averaging. The authors and comparing these scores to the scores from the cali-
showthattheFed-ensembleexcelsatuncertaintyquantifi- bration set. Lu et al. [111] point out that since CP is
cationwhentestedonCIFAR-10[66]CIFAR-100[107], primarily a post-processing method for uncertainty esti-
MNIST,andtheOpenimagesv4dataset[108]inbothho- mation, integrating it into an FL framework is generally
mogeneousandheterogeneoussettings. UsingNTK,they straightforward. Theauthorsalsocorrelateclassentropy
showthatpredictionsatnewdatapointsfromallK mod- withpredictionsetsizetodeterminetaskuncertainty. CP
elsconvergetosamplesfromthesame,limitingtheGaus- can produce confidence predictions for any ML model
sian process in sufficiently over-parameterized regimes thatoutputsascorefunction.
184.2.1. Differentialy Private Federated Average Quantile • Step 1 - For each client, the authors perform ap-
Estimation(DP-FedAvgQE) proximate Bayesian inference for the posterior dis-
DP-FedAvgQE brings CP and DP to FL and provides tribution of the client model weights using Markov
theoretical privacy guarantees to ensure additional secu- Chain Monte Carlo (MCMC) sampling [95]. This
rity[93]. DP-FedAvgQEprovidedstrongbenchmarkson produces a set of samples from the client’s poste-
ImageNet [112] and CIFAR-10 [66] datasets and simu- rior, and these samples are used as teacher models,
lated data experiments. DP-FedAvgQE takes advantage whicharedistilledintoastudentmodel. Theauthors
ofimportanceweightingtoaddressthelabelshiftbetween use stochastic gradient Langevin dynamics (SGLD)
agentseffectively. sampling since it provides an online method to dis-
till these posterior samples efficiently into a student
model.
4.2.2. FederatedConformalPrediction(FCP)
FCPisanothermethodforextendingCPtoFLthatad- • Step 2 - For each client, the authors distill the
dressesthenon-i.i.d. natureofdatainFL[94]. Theinher- MCMC samples (teacher models) directly into the
entheterogeneityofFLdatasetsviolatesthefundamental PPD, which is the student model [95]. Notably, in
tenetofexchangeabilitybetweenthecalibrationdatadis- this distillation-based approach, the PPD for each
tributionandthetestdatadistributionduringinferencein clientisrepresentedsuccinctlybyasingledeepneu-
CP,implyingthatthecalibrationandtestdatahaveidenti- ralnetworkinsteadofviaanensembleofdeepneu-
caldistributions[113].Tosolvethisviolation,theauthors ralnetworks. Thismakesthepredictionstagemuch
proposeusingpartialexchangeability,whichisageneral- fasterthantypicalBayesianapproaches.
ization of exchangeability [94]. FCP makes no assump-
tions between clients P1,...,PK.. Specifically, this as-
4.3.2. FedUncertaintyAwareFedUA
sumptiondoesnotrequireindependenceoridenticaldis-
FedUAprovidesanotherapproachtodistillpredictions
tributionsamongclients. FCPprovidesrigoroustheoret-
focusing on non-i.i.d. data while limiting communica-
ical guarantees and excellent empirical performance on
tion bandwidth [96]. This framework implements two
multiplecomputervisionandmedicalimagingdatasets.
core components: (1) uncertainty measurement to quan-
tify each client’s uncertainty and (2) a sample quality
4.3. DistilledPredictions evaluatortoselecthigh-qualitysamplesforglobalmodel
training.Knowledgedistillationisusedintheaggregation
The distilled prediction method leverages knowledge
processtotransferinter-classrelationshipsfromthelocal
distillationtoquantifyuncertainty[95].
models and suppress noise from incomplete client data.
TheauthorsempiricallyshowthatFedUAimprovesaccu-
4.3.1. Federated Posterior Predictive Distribution racycomparedtootherFLmodelswhilelimitingcommu-
(FedPPD) nication costs on image classification tasks. The authors
FedPPDisaframeworkforFLwithuncertaintyestima- alsoreportedthattheuncertaintymeasurementusingfea-
tionwhere,ineveryround,eachclientinferstheposterior ture space density was more robust to native data uncer-
distribution over its parameters and the posterior predic- taintythansoftmaxentropy.
tive distribution (PPD) [95]. The estimated PPD is sent Knowledge distillation provides two promising im-
to the server. Making predictions at test time does not provements for FL: it can alleviate overfitting on client-
require computationally expensive Monte-Carlo averag- side data by sifting through informative and valuable in-
ing over the posterior distribution because this approach formation for learning, mitigating the bias caused by in-
maintains the PPD in the form of a single deep neural complete or over-trained data on a given client. Knowl-
network. Moreover, this approach makes no restrictive edge distillation also allows the global model to learn
assumptions, such as the form of the clients’ posterior inter-class relationships, which helps to transfer knowl-
distributions or their PPDs. FedPPD follows a two-step edge from a general multi-purposed model to a specific
process[95]: target-oriented model. For the sample evaluator, the
19method used when finding quality samples can be de- has been trained, it can be personalized for individ-
scribed as “samples that do not reach consensus among ualusersorclients.
local models should be taken with a higher priority”.
• Personalized model learning: With personalized
Thesesamplesaremoreimportantforoptimizingthelo-
modellearninginafederatedsetting,thefocusshifts
calmodelontheserverside.Foruncertaintyestimation,a
to training individual models for each site from the
singledeterministicmodelisusedtoquantifyuncertainty
outset,leveragingthelocaldatawhilestilloccasion-
byestimatingfeaturespacedensityforeachclientmodel.
ally sharing insights or parameters (in a privacy-
Foranewinputsample,featuresareextractedandevalu-
preserving manner) across the network to improve
atedtogettheprobabilitydensityfunctionintheclient’s
themodelscollectively.
feature space. A lower density indicates a higher uncer-
tainty and vice versa. Leveraging knowledge distillation ThestudybyZhangetal. [98]showsthatpersonaliza-
isapowerfulwayofimplementinguncertaintyestimation tioninFLimprovesclassificationaccuracyandincreases
intoanFLframeworkwithnon-i.i.ddata[96]. the quality of estimated uncertainty [98]. Thus, person-
alization is a promising research direction in local client
4.4. BayesianFL
deployment and uncertainty quantification for healthcare
A popular class of models for providing uncertainty applications[98]. Bayesianmethodsareheavilyusedfor
quantificationinMLbelongstoBayesianorprobabilistic creatingpFLalgorithms[98].
models[114]. ThesemodelsutilizeBayesianmethodsto
giveprobabilisticpredictionsratherthanpointpredictions 4.4.3. Self-FL
[115]. Self-aware personalized FL (Self-FL) uses intra-client
and inter-client uncertainty to balance the training of lo-
4.4.1. FederatedBayesianNeuralNetwork(FedBNN)
cal personal and global models [99]. Larger inter-client
The probabilistic predictions can give insight into
variation implies more personalization is needed. Self-
modeluncertainty[97]. TheauthorspresentaunifiedFL
FL uses uncertainty-driven local training steps and ag-
frameworkbasedontrainingacustomizedlocalBayesian
gregation rules instead of conventional local fine-tuning
modelforeachclient. Thesemodelscanlearnintheab-
and sample size-based aggregation. The authors inter-
senceoflargelocaldatasets.TheBayesiannatureofthese
pret personalized FL through a two-level Bayesian hier-
modelsallowsforincorporatingsupervisionintheformof
archical model perspective to characterize client-specific
priordistributions. Theauthorsusethepriorofthefunc-
andglobally-sharedinformation. Themethodusesuncer-
tionaloutputspaceofthenetworktoaidincollaboration
tainty to drive client-side training with an adaptive num-
acrossheterogeneousclients[97].
ber of local steps and server-side aggregation (variance-
weighted averaging). The authors evaluate their method
4.4.2. PersonalizedFL(pFL)
using synthetic data, images (MNIST, FEMNIST [117],
In some settings with heterogeneous data, it makes
CIFAR10 [66]), text (Sent140 [118]), and audio (wake-
sensetopersonalizelocalmodelstocatertotheirrespec-
word detection) and show robust personalization capa-
tive datasets while still being able to learn from a larger
bility under data heterogeneity. Some key advantages
datapool,liketheworkdoneby[116]formulti-contrast
of the Self-FL model are principled connections to hi-
MRIsynthesis.ThispracticeisknownaspersonalizedFL
erarchicalBayesianmodelingandbuilt-inauto-tuningof
(pFL) and can be carried out in two primary ways with
hyper-parameters for each client, all while maintaining
Bayesiantechniquesaccordingto[98]:
the same computation and communication overhead as
• Global model personalization: The global model FedAvg[99].
personalization strategy begins with the training of
a global model on data distributed across many de- 4.4.4. PersonalizedfederatedlearningwithBayesianin-
vices or nodes. The model is trained by aggregat- ference(pFedBays)
ing locally computed updates from each node with- In pFedBays, weight uncertainty is introduced in neu-
out sharing the data itself. Once this global model ral networks for clients and the server [100]. To achieve
20personalization, each client updates its local distribution tingproblemwhereclientshavehighlyheterogeneousand
parameters by balancing its construction error over pri- smalldatasets[101].
vatedata anditsKullback–Leibler(KL)divergencewith
global probability distribution from the server. pFed- 4.5. OthermethodsforUncertaintyEstimationinFL
Bays method tackles two issues in FL: training on non- Somemethodstakeadifferentapproachtoquantifying
i.i.ddataacrossclientsandoverfittingduetolimiteddata uncertaintyinFLsetups.
[100].pFedBaysformulatesboththelocalclients’models
andtheglobalservermodelasBayesianneuralnetworks, 4.5.1. FeatureAnchors
where the parameters are modeled as probability distri- Zhouetal. proposeusingfeatureanchorstoalignfea-
butions rather than point estimates. This helps address turesandclassifiersforheterogeneousdataintheirframe-
overfitting. Theserveroptimizestofindaglobaldistribu- work[102]. FedFAisdesignedtoaddressthechallenges
tionthatisclosetothelocaldistributionsbyminimizing posedbyheterogeneousdata.Thismethodutilizesfeature
KL divergence. Each client balances minimizing a local anchors to align features and calibrate classifiers across
data fit term and the KL divergence from the global dis- clients simultaneously. This enables client models to be
tribution to find its personalized distribution. The global updated in a shared feature space with consistent classi-
distribution acts as a prior for the local models. By re- fiersduringlocaltraining.Theauthorsexplainthevicious
placing the prior distribution with a trained global dis- andvirtuouscyclesofFLwithheterogeneousdata:
tribution, the authors find a relatively good distribution
without making assumptions about the prior distribution • Vicious Cycle: In traditional FL approaches, data
[100]. Thisiscriticalbecauseestimatingapriorinmany heterogeneity leads to feature inconsistency across
real-world scenarios is not feasible [100]. The authors client models. This inconsistency causes classifier
provideatheoreticalanalysisboundingthegeneralization updatestodiverge,forcingfeatureextractorstomap
errorandshowingtheconvergencerateisminimaxopti- tomoreinconsistentfeaturespaces. Thiscycleofin-
maluptoalogarithmicfactor. BeingaBayesiannetwork, creasing divergence in classifiers and inconsistency
uncertaintyestimatesfortheparameterscanbemonitored in features degrades the performance and conver-
tounderstandthemodel’sconfidenceinitspredictions. genceofthefederatedmodel.
• Virtuous Cycle: FedFA introduces feature anchors
4.4.5. Fedpop
tobreakthisviciouscycle. Byaligningfeaturesand
The Fedpop [101] framework recasts the method of
calibrating classifiers across clients, FedFA creates
pFL into a population modeling paradigm. Clients inte-
avirtuouscycle. Thealignedfeaturesandclassifiers
grate fixed common population parameters with random
promoteconsistencyinclientfeaturesandclassifiers.
effects, expanding data heterogeneity. Each client has
This alignment ensures that client models are up-
a local model composed of fixed population parameters
datedinashared,consistentfeaturespacewithsim-
sharedacrossclientsandrandomeffectsthatexplainhet-
ilarclassifiers,leadingtoimprovedperformanceand
erogeneityinthelocaldata. Kotelevskiietal. developed
morestableconvergence.
anewstochasticoptimizationalgorithmbasedonMCMC
to perform inference under this model [101]. The algo- The FedFA framework integrates feature anchor loss
rithm allows uncertainty estimation, handles issues like to minimize local objective functions. This mechanism
client drift, and works well even with limited client par- is designed to align class-specific features and dimin-
ticipation. The authors show that, in practice, the added ish the distance within classes at the client level [102].
computationalcostfromtheMonteCarlochainisnegligi- Moreover, the FedFA algorithm encompasses a server-
ble. FedPop allows for uncertainty estimation by having sidecomponentwherebothclassfeatureanchorsandthe
eachclientmodelinvolveafixedeffectparametershared globalmodelundergoaggregation. Thisprocessemploys
across clients and a low-dimensional random effect pa- a weighted averaging scheme akin to that of the FedAvg
rameter sampled for each client. Introducing a common algorithm,facilitatingtheintegrationoflocalupdatesinto
prior on the local parameters addresses the local overfit- acohesiveglobalmodel.
214.5.2. FedAvg-GaussianFedAG modeluncertainty,anditwillbediscussedinthenextpart
FedAvg-Gaussian FedAG takes a Gaussian approach ofpaper.
to generating probabilistic predictions in FL [103]. By
introducing weight uncertainty in the aggregation step 4.6. Calibration
of the FedAvg algorithm, the end devices can calcu-
Calibration is another method of dealing with uncer-
late probabilistic predictions but only have to learn con-
tainty estimation by correcting an ML model’s tendency
ventional, deterministic models. This allows for uncer-
to be overconfident in incorrect predictions due to the
tainty estimation in an FL framework. The key idea in
Softmax function [120]. By calibrating the confidence,
FedAGistotreattheprobabilitydistributionofthelocal
a better assumption about the quality of a prediction can
model weights from different devices as an approxima-
bemade[120].
tion of the posterior distribution over the global model
weights [103]. This allows the global model to make
4.6.1. Classifier Calibration with Virtual Representation
probabilisticpredictions. Forlinearmodels, FedAGper-
(CCVR)
forms on par with Bayesian linear regression. For neu-
CCVR calibrates a global model to improve perfor-
ral networks, FedAG outperforms variational inference
mance on non-i.i.d data in heterogeneous settings [104].
methodsandapproachestheperformanceofdeepensem-
The authors found a greater bias in representations
bles for probabilistic predictions after several rounds of
training[103]. FedAGprovidesanefficientandprivacy- learned in the deeper layers of a model trained with FL.
They show that the classifier contains the greatest bias
preserving way to enable probabilistic predictions in FL
and that post-calibration can greatly improve classifica-
settings, with performance competitive to non-federated
tion performance. Specifically, the classifiers learned on
methods. FedAGhascomparableaccuracytonon-FLand
different clients show the lowest feature similarity. The
non-distributedlearningframeworks. Therearetwovari-
classifiers tend to get biased toward the classes over-
ationstothisalgorithm:
represented in the local client data, leading to poor per-
• MonteCarloandnon-parametricbootstrapping: formance in under-represented classes. This classifier
bias is a key reason behind performance degradation on
In Monte Carlo and non-parametric bootstrapping,
non-i.i.d federated data. Regularizing the classifier dur-
Msetsofweightsarerandomlydrawnfromthepos-
ing federated training brings minor improvements [104].
terior weight distributions learned during federated
However, post-training calibration of the classifier sig-
aggregation. Each of these M weight sets is used
nificantlyimprovesclassificationaccuracyacrossvarious
to generate a prediction on the test input. These M
FL algorithms and datasets [104]. CCVR generates vir-
predictionsareaggregated(bytakingmeanandvari-
tual representations using Gaussian probability distribu-
ance)toapproximateapredictivedistribution.
tionsfittedonclientfeaturestatistics.CCVRthenretrains
• Non-parametric bootstrapping: This approach the classifier on these virtual representations while fix-
ingthefeatureextractor. Experimentalresultsshowstate-
usesthelocalweightupdatesfromclientdevicesdi-
of-the-artaccuraciesoncommonbenchmarkdatasetslike
rectly, rather than drawing samples from the fitted
CIFAR-10. CCVRisbuiltontopoftheoff-the-shelffea-
posterior probability distributions, to generate pre-
ture extractor and requires no transmission of the repre-
dictions. Eachclient’sweightupdateisuseddirectly
sentations of the original data, thus raising no additional
to generate a prediction. These predictions approx-
privacyconcerns.
imate the predictive distribution. Non-parametric
bootstrapping is conceptually similar to bootstrap
aggregating [119], where re-sampling the training 4.6.2. FedCSPC
dataisreplacedbyre-samplingtheclientweights. FedCSPCmethodaddressestheissueofheterogeneous
datadistributionsacrossclientsinFL[105]. Thismethod
FedFAuseditsfeatureanchorstocalibratethemodel’s takes additional prototype information from the clients
classifier. Calibration is a different way of dealing with to learn a unified feature space on the server side while
22maintaining clear boundaries. There are two main mod- The FeTS-1.0 study opened the door for FL in a medi-
ules to this framework: (1) The Data Prototypical Mod- cal environment. Participants were given a U-net [124]
eling (DPM) module and (2) the Cross-Silo Prototypi- modelandtaskedwithfindingthebestmethodforweight
calCalibration(CSPC)module[105]. TheDPMmodule aggregation. TheFeTS-1.0challengealsofocusedonthe
uses clustering to model representation distributions for real-world evaluation of the consensus model to show if
each client and generate class-specific prototypes for the it could perform well on real unseen data. The success
server.Thishelpscapturediversitywithineachclass.The of this first challenge paved the way for the FeTS-2.0
CSPCmoduleontheserveralignstheheterogeneouspro- Challenge[125],wheretheobjectivewastoaddressout-
totypefeaturesfromdifferentclientsintoaunifiedspace. of-sample generalizability for rare Glioblastoma cancer
It uses an augmented contrastive learning approach with boundary detection. Due to this disease’s rarity and pri-
positivemixingandhardnegativeminingtoimprovero- vacy concerns regarding medical data, it is a challenge
bustness. Knowledge-based predictions using the cali- to gather a large amount of data to train a model on this
bratedexemplarprototypesfromtheunifiedspacetosup- task[125].Traditionalapproachestothisprobleminvolve
plementthenetworkpredictions. FedCSPCalleviatesthe sharingmulti-sitedata[125],butcentralizingsuchdatais
featuregapbetweendatasources,whichcansignificantly often difficult or infeasible due to various limitations re-
improve generalization ability.The authors test the pro- garding privacy. The study presented in this paper is the
posed framework on CIFAR10, CIFAR100 [107], Tiny- largestFLprojecttodate,incorporatingdatafrom71sites
ImageNet [121], and VireoFood172 [122] datasets. The around the globe [125]. With this approach, the authors
proposedCSPCmoduleisanorthogonalimprovementto createdthelargestGlioblastomadatasetwith6,214sam-
client-basedmethodsandhasaplug-and-playdesignthat ples. The authors reported a 33% improvement in seg-
makes it easy to integrate into existing FL frameworks. mentingthesurgicallytargetableportionofthetumorand
CalibrationisanattractivemethodforFLasitintroduces a23%improvementforthecompletetumorcomparedtoa
little additional communication overhead and can effec- publiclytrainedmodel[125]. Thisresearchdemonstrates
tively provide quality information about model certainty thatFLenhancestheefficacyofMLmethodologieswithin
[105]. themedicalsector,reinforcingthenotionthatFLcouldbe
atransformativetechnologyforamplifyingtheimpactof
MLinmedicalimaging.
5. Real-WorldApplications
6. ChallengesandOpportunities
WiththeinfluxofresearchinthefieldofFLformedical
imaging, some successful real-world applications show- While there has been significant progress in FL in re-
caseFL’spotentialforthemedicalimagingdomain[123]. cent years, some challenges remain to be solved. These
The Federated Tumor Segmentation (FeTS-1.0) Chal- challengespresentpotentialopportunitiesforresearchers
lenge[123]wasthefirstreal-worldFLchallengeformed- tofurtherexploreandimprovethestateofFLformedical
icalimages. Thegoalsforthischallengewere: imaging. One particular challenge is the inherent trade-
off between privacy and security in FL. Further research
• Theidentificationoftheoptimalweightaggregation into the efficient allocation of the privacy budget to en-
approach towards training a consensus model that hancemodelperformancewithoutcompromisingprivacy
has gained knowledge via FL from multiple geo- is a key area that requires further research. In addition
graphically distinct institutions while their data are to privacy and security, communication efficiency must
alwaysretainedwithineachinstitution. alsobeconsidered. Alternatenoiseadditionmethodsare
also a possible route for increasing the effectiveness of
• The federated evaluation of the generalizability of DP, as current methods may not be optimal. Another
braintumorsegmentationmodels“inthewild”,i.e., trade-off that still presents a challenge is personalization
ondatafrominstitutionaldistributionsthatwerenot versusoverfitting. PersonalizationinFLcanincreaseac-
partofthetrainingdatasets. curacy but risks affecting uncertainty estimation perfor-
23manceduetooverfitting. Methodsforoptimizingperson- 8. Acknowledgements
alizationtobalanceoverfittingareopenareasforresearch.
Computational efficiency remains an issue for many as- This work was funded by NSF grants 2234468 and
2234836. Thecontentistheresponsibilityoftheauthors
pects of FL, particularly with deep ensembles like fed-
ensemble. Finding more computationally efficient meth-
anddoesnotreflecttheofficialviewsoftheNationalSci-
enceFoundation
odscouldprogressFLfurther. uncertaintyestimationfor
out-of-distributionandnoisylabelsisanunder-researched
area,andthereisaneedtoinvestigatehowuncertaintyes- References
timationcanbeleveragedtoaddresstheseissues. Explor-
[1] B. Erickson, P. Korfiatis, Z. Akkus, and T. Kline,
ing generative AI models to provide application-specific
“Machine learning for medical imaging,” Radio-
alignment datasets could be a promising direction. Gen-
graphics,2017.
erativeAIcouldmakeupforalackofdatabyproviding
simulateddata. Conformalpredictionhasbeenshownto
[2] J. Latif, C. Xiao, A. Imran, and S. Tu, “Medical
perform well for uncertainty estimation in FL, but little
imagingusingmachinelearninganddeeplearning
research has been conducted on conformal prediction in
algorithms: Areview,”inICOMET,2019.
a personalized setting, making it an open research area.
EnsemblemodeshavebeenintegratedintoFLandcould [3] A. M. Barraga´n-Montero et al., “Artificial intelli-
potentially address and detect client drift, anomalies, or gence and machine learning for medical imaging:
fairnesschallengesduringmodeltraining.Applyingdata- Atechnologyreview,”PhysicaMedica,2021.
free knowledge transfer methods could improve practi-
[4] M. Willemink et al., “Preparing medical imaging
cability in scenarios where shared datasets are not avail-
dataformachinelearning,”Radiology,2020.
able,providingasecurewaytotransferknowledgeacross
clients. [5] J.Jager,T.Gremeaux,T.Gonzalez,S.Bonnafous,
C. Debard, M. Laville, and H. Vidal, “Adipose
tissue-derived stem cells promote monocyte re-
cruitmentinadiposetissueandliver,” MolMetab,
7. Conclusion vol.3,no.4,pp.417–425,2014.
[6] C. Wu, H. Ying, F. Grinnell, G. Bryant-
Greenwood,R.Riha,J.Nguyen,Z.Li,M.Parsons,
Machine learning holds the potential to dramatically B. Parry, D. Rotstein, A. Lightfoot, and S. Cas-
improvetheeffectivenessofmedicalimagingfordisease sar,“Vitamindreceptorlocalizationandactivityin
diagnoses and treatment. However, to succeed, methods earlyhumanfetaldevelopment,”JClinEndocrinol
needtobeimplementedtoaddressbothprivacyconcerns Metab,vol.100,no.12,pp.E1568–E1575,2015.
anduncertaintyestimation. FLisapowerfulsolutionfor
[7] G.Sahay,D.Y.Alakhova,andA.V.Kabanov,“En-
trainingonmultipleprivatedatasetswithoutexposingany
docytosis of nanomedicines,” J Control Release,
private data, and enhanced privacy preservation and un-
vol.145,no.3,pp.182–195,2010.
certaintyestimationmethodscanbeaneffectiveapproach
for training large medical imaging models. This paper [8] N.Bettencourt,J.P.Ferreira,I.P.Culotta,J.J.Mc-
provided a comprehensive review of the current state of Murray,S.Jacob,J.L.Rouleau,K.Swedberg,S.J.
FL algorithms, privacy preservation, and uncertainty es- Pocock,S.D.Solomon,F.Zannad,andP.Rossig-
timation in the context of medical imaging. Significant nol, “Impact of intensive versus standard blood
progresshasbeenmadeinrecentyearstomakeFLviable pressure lowering in chronic kidney disease pa-
for the medical imaging domain, with work being done tients with and without diabetes mellitus: A sub-
tooptimizetheaggregationprocess,privacypreservation, analysisofthesprintstudy,”Hypertension,vol.76,
anduncertaintyestimation. no.4,pp.979–987,2020.
24[9] S. Dayarathna, K. T. Islam, S. Uribe, G. Yang, Programming, part II (ICALP 2006), pp. 1–12,
M.Hayat,andZ.Chen,“Deeplearningbasedsyn- Springer,2006.
thesis of mri, ct and pet: Review and analysis,”
[18] C. Gentry, A Fully Homomorphic Encryption
MedicalImageAnalysis,vol.92,p.103046,2024.
Scheme. PhDthesis,StanfordUniversity,2009.
[10] “Health Insurance Portability and Accountability
[19] F.Linsner,L.Adilova,S.Da¨ubener,M.Kamp,and
Act of 1996.” Pub. L. No. 104-191, 110 Stat.
A.Fischer,“Approachestouncertaintyquantifica-
1936, 1996. Available from: U.S. Government
Printing Office, via: https://www.govinfo. tion in federated deep learning,” in PKDD/ECML
Workshops,2021.
gov/content/pkg/PLAW-104publ191/pdf/
PLAW-104publ191.pdf. [20] A.F.Psaros,X.Meng,Z.Zou,L.Guo,andG.Kar-
niadakis, “Uncertainty quantification in scientific
[11] “General Data Protection Regulation (GDPR).”
machinelearning: Methods, metrics, andcompar-
Regulation(EU)2016/679oftheEuropeanParlia-
isons,”JournalofComputationalPhysics,2022.
ment and of the Council of 27 April 2016, 2016.
Availablefrom: https://eur-lex.europa.eu/ [21] E.Darzidehkalani, M.Ghasemi-rad, andP.V.van
eli/reg/2016/679/oj. Ooijen, “Federated learning in medical imaging:
Part ii: Methods, challenges, and considerations,”
[12] B. McMahan, E. Moore, D. Ramage, S. Hamp-
Journal of the American College of Radiology,
son,andB.A.y.Arcas,“Communication-Efficient
vol.19,no.4,pp.P755–765,2022.
Learning of Deep Networks from Decentralized
Data,” in Proceedings of the 20th International [22] G. Kaissis, M. Makowski, D. Ru¨ckert, and
ConferenceonArtificialIntelligenceandStatistics R. Braren, “Secure, privacy-preserving and feder-
(A.SinghandJ.Zhu,eds.),vol.54ofProceedings atedmachinelearninginmedicalimaging,”Nature
of Machine Learning Research, pp. 1273–1282, MachineIntelligence,vol.2,062020.
PMLR,20–22Apr2017.
[23] N. Mouhni, A. Elkalay, M. Chakraoui, A. Ab-
[13] L. Qu, N. Balachandar, and D. Rubin, “An ex- dali, A. Ammoumou, and I. Amalou, “Federated
perimental study of data heterogeneity in feder- learning for medical imaging: An updated state
atedlearningmethodsformedicalimaging,”arXiv, oftheart,”IngenieriedesSystemesd’Information,
2021. AvailableatarXiv:2107.08371. vol.27,no.1,pp.117–122,2022.
[14] M. Lam, G. Wei, D. Brooks, V. J. Reddi, [24] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Tal-
and M. Mitzenmacher, “Gradient disaggregation: walkar, and V. Smith, “Federated optimization in
Breaking privacy in federated learning by re- heterogeneousnetworks,”2020.
constructing the user participant matrix,” CoRR,
vol.abs/2106.06089,2021. [25] X. Li, M. Jiang, X. Zhang, M. Kamp, and
Q. Dou, “Fedbn: Federated learning on non-iid
[15] R. Wu, X. Chen, C. Guo, and K. Q. Weinberger, features via local batch normalization,” ArXiv,
“Learning to invert: Simple adaptive attacks for vol.abs/2102.07623,2021.
gradient inversion in federated learning,” arXiv,
[26] Y.Yu,A.Wei,S.Karimireddy,Y.Ma,andM.Jor-
2022.
dan, “Tct: Convexifying federated learning using
[16] M. Jere, T. Farnan, and F. Koushanfar, “A taxon- bootstrappedneuraltangentkernels,”072022.
omyofattacksonfederatedlearning,”IEEESecu-
rity&Privacy,2021. [27] W.Lu,J.Wang,Y.Chen,X.Qin,R.Xu,D.Dim-
itriadis,andT.Qin,“Personalizedfederatedlearn-
[17] C. Dwork, “Differential privacy,” in 33rd Interna- ingwithadaptivebatchnormforhealthcare,”IEEE
tional Colloquium on Automata, Languages and TransactionsonBigData,pp.1–1,2022.
25[28] Z. Zhu, J. Hong, and J. Zhou, “Data-free knowl- [36] M. Sheller, B. Edwards, G. Reina, J. Martin,
edgedistillationforheterogeneousfederatedlearn- S. Pati, A. Kotrotsou, M. Milchenko, W. Xu,
ing,” Proceedings of machine learning research, D. Marcus, R. Colen, and S. Bakas, “Feder-
vol.139,pp.12878–12889,072021. ated learning in medicine: facilitating multi-
institutionalcollaborationswithoutsharingpatient
[29] L. Liu, X. Jiang, F. Zheng, H. Chen, G.-J. Qi, data,”ScientificReports,vol.10,072020.
H. Huang, and L. Shao, “A bayesian federated
learning framework with online laplace approxi- [37] J. Wen, Z. Zhang, Y. Lan, Z.-s. Cui, J. Cai, and
mation.,” IEEE transactions on pattern analysis W. Zhang, “A survey on federated learning: chal-
andmachineintelligence,vol.PP,2021. lenges and applications,” International Journal of
MachineLearningandCybernetics,2022.
[30] W. Huang, M. Ye, and B. Du, “Learn from oth-
ers and be yourself in heterogeneous federated [38] D. Liu, L. Bai, T. Yu, and A. Zhang, “Towards
learning,”in2022IEEE/CVFConferenceonCom- method of horizontal federated learning: A sur-
puter Vision and Pattern Recognition (CVPR), vey,” 2022 IEEE 5th International Conference on
pp.10133–10143,2022. BigDataandArtificialIntelligence(BDAI),2022.
[39] D. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht,
[31] S. Warnat-Herresthal, H. Schultze, K. Shas-
and J. Dureau, “Federated learning for keyword
try, S. Manamohan, S. Mukherjee, V. Garg,
spotting,” in Proceedings of IEEE International
R.Sarveswara,K.Ha¨ndler,P.Pickkers,N.A.Aziz,
Conference on Acoustics, Speech and Signal Pro-
M. Breteler, E. Giamarellos-Bourboulis, M. Kox,
cessing,pp.6341–6345,2019.
M. Becker, S. Cheran, M. Woodacre, E. Goh,
J. Schultze, and H. Grundmann, “Swarm learning
[40] S. Ramaswamy, R. Mathews, K. Rao, and
fordecentralizedandconfidentialclinicalmachine
F. Beaufays, “Federated learning for emoji pre-
learning,”012021.
diction in a mobile keyboard,” arXiv preprint
arXiv:1906.04329,2019.
[32] S.Kalra,J.Wen,J.C.Cresswell,M.Volkovs,and
H.R.Tizhoosh,“Decentralizedfederatedlearning
[41] A.Fallah,A.Mokhtari,andA.Ozdaglar,“Person-
through proxy model sharing,” Nature Communi-
alized federated learning with theoretical guaran-
cations,vol.14,p.2899,May2023.
tees: A model-agnostic meta-learning approach,”
inAdvancesinNeuralInformationProcessingSys-
[33] M.Butt, N.Tariq, M.Ashraf, H.S.Alsagri, S.A.
tems,vol.33,2020.
Moqurrab,H.A.A.Alhakbani,andY.A.Alduray-
wish, “A fog-based privacy-preserving federated [42] K. Zhang, X. Song, C. Zhang, and C. Yu, “Chal-
learningsystemforsmarthealthcareapplications,” lenges and future directions of secure federated
Electronics,vol.12,no.19,2023. learning:asurvey,”FrontiersinComputerScience,
vol.16,no.5,p.165817,2022.
[34] J. Wu, Q. Xia, and Q. Li, “Efficient privacy-
preserving federated learning for resource- [43] Y. Liu, Y. Kang, T. Zou, Y. Pu, Y. He, X. Ye,
constrained edge devices,” in IEEE International Y. Ouyang, Y. Zhang, and Q. Yang, “Vertical fed-
ConferenceonMobileAd-HocandSmartSystems eratedlearning,”arXivpreprintarXiv:2211.12814,
(MASS),2021. 2022.
[35] J. Mills, J. Hu, and G. Min, “Communication- [44] L.Yang,D.Chai,J.Zhang,Y.Jin,L.Wang,H.Liu,
efficientfederatedlearningforwirelessedgeintel- H. Tian, Q. Xu, and K. Chen, “A survey on ver-
ligence in iot,” IEEE Internet of Things Journal, tical federated learning: From a layered perspec-
vol.7,no.7,pp.5986–5994,2020. tive,”arXivpreprintarXiv:2304.01829,2023.
26[45] A. Khan, M. T. Thij, and A. Wilbik, 3d region proposal network,” IEEE Transactions
“Communication-efficient vertical federated onMedicalImaging,vol.38,no.8,pp.1885–1898,
learning,”Algorithms,vol.15,no.8,p.273,2022. 2019.
[46] D. Serpanos and G. Xenos, “Vertical federated [55] G. Hinton, O. Vinyals, and J. Dean, “Distilling
learning in malware detection for smart cities,” in theknowledgeinaneuralnetwork,”arXivpreprint
IEEEInternationalConferenceonSecurity,2023. arXiv:1503.02531,2015.
[47] C. Fu, X. Zhang, S. Ji, J. Chen, J. Wu, S. Guo, [56] D.Yangetal.,“Federatedlearningwithknowledge
J.Zhou,A.X.Liu,andT.Wang,“Labelinference distillationformulti-organsegmentationwithpar-
attacks against vertical federated learning,” 2022. tially labeled datasets,” Medical Image Analysis,
DBLPconferenceproceedings. vol.95,p.102967,2024.
[48] A. Jacot, F. Gabriel, and C. Hongler, “Neural [57] T.Lin,L.Kong,S.U.Stich,andM.Jaggi,“Ensem-
tangent kernel: Convergence and generalization bledistillationforrobustmodelfusioninfederated
in neural networks,” Proceedings of the National learning,”arXivpreprintarXiv:2006.07242,2020.
AcademyofSciences,vol.115,no.34,pp.E7665–
[58] T.-M. H. Hsu, H. Qi, and M. Brown, “Measur-
E7671,2018.
ing the effects of non-identical data distribution
[49] A. Reiss and D. Stricker, “Introducing a new for federated visual classification,” arXiv preprint
benchmarked dataset for activity monitoring,” in arXiv:1909.06335,2019.
2012 16th International Symposium on Wearable
[59] H.Khan,N.C.Bouaynaya,andG.Rasool,“Brain-
Computers,pp.108–109,IEEE,2012.
inspired continual learning: Robust feature distil-
[50] U.Sait,K.G.Lal,S.Prajapati,R.Bhaumik,T.Ku- lation and re-consolidation for class incremental
mar, S. Sanjana, and K. Bhalla, “Curated dataset learning,”IEEEAccess,2024.
for covid-19 posterior-anterior chest radiography
[60] H.Khan,N.Bouaynaya,andG.Rasool,“TheIm-
images(x-rays),”2020.
portance of Robust Features in Mitigating Catas-
[51] J. Yang, R. Shi, and B. Ni, “Medmnist classifica- trophic Forgetting,” in accepted for publication in
tion decathlon: A lightweight automl benchmark 28thIEEESymposiumonComputersandCommu-
for medical image analysis,” in 2021 IEEE 18th nications (ISCC 2023), 2023. https://arxiv.
International Symposium on Biomedical Imaging org/abs/2306.17091.
(ISBI),pp.191–195,IEEE,2021.
[61] Patrick Foley and Micah J Sheller and Brandon
[52] J. Yang, R. Shi, D. Wei, Z. Liu, L. Zhao, B. Ke, Edwards and Sarthak Pati and Walter Riviera and
H. Pfister, and B. Ni, “MedMNIST v2-a large- MansiSharmaandPrakashNarayanaMoorthyand
scalelightweightbenchmarkfor2dand3dbiomed- Shih-hanWangandJasonMartinandParsaMirhaji
ical image classification,” Scientific Data, vol. 10, and Prashant Shah and Spyridon Bakas, “Openfl:
no.1,p.41,2023. the open federated learning library,” Physics in
Medicine&Biology,vol.67,p.214001,oct2022.
[53] P. Bilic, P. F. Christ, E. Vorontsov, G. Chle-
bus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.- [62] MONAITeam,“MONAIDeploy,”2023. available
A. Heng, J. Hesser, et al., “The liver tumor at: https://monai.io/deploy.html. Last ac-
segmentation benchmark (lits),” arXiv preprint cessedonFeb1,2023.
arXiv:1901.04056,2019.
[63] NVIDIATeam,“NVIDIAClara,”2023. available
[54] X. Xu, F. Zhou, B. Liu, D. Fu, and X. Bai, “Effi- at: https://www.nvidia.com/en-us/clara/.
cientmultipleorganlocalizationinctimageusing LastaccessedonFeb1,2023.
27[64] H. Roth, Y. Cheng, Y. Wen, I. Yang, Z. Xu, Y.- [71] S. Truex, N. Baracaldo, A. Anwar, T. Steinke,
T.Hsieh,K.Kersten,A.Harouni,C.Zhao,K.Lu, H.Ludwig,R.Zhang,andY.Zhou,“Ahybridap-
Z.Zhang,W.Li,A.Myronenko,D.Yang,S.Yang, proach to privacy-preserving federated learning,”
N.Rieke,A.Quraini,C.Chen,D.Xu,andA.Feng, AISec’19,(NewYork,NY,USA),p.1–11,Associ-
“Nvidia flare: Federated learning from simulation ationforComputingMachinery,2019.
toreal-world,”102022.
[72] T.Qi,F.Wu,C.Wu,L.He,Y.Huang,andX.Xie,
[65] B. Huang, X. Li, Z. Song, and X. Yang, “Fl-ntk: “Differentially private knowledge transfer for fed-
A neural tangent kernel-based framework for fed- eratedlearning,”NatureCommunications,vol.14,
erated learning analysis,” in Proceedings of the p.3785,June2023.
38thInternationalConferenceonMachineLearn-
[73] J.So, R.E.Ali, B.Gu¨ler, J.Jiao, andA.S.Aves-
ing(M.MeilaandT.Zhang,eds.),vol.139ofPro-
timehr, “Securing secure aggregation: Mitigating
ceedingsofMachineLearningResearch,pp.4423–
multi-roundprivacyleakageinfederatedlearning,”
4434,PMLR,18–24Jul2021. AAAI’23/IAAI’23/EAAI’23,AAAIPress,2023.
[66] A.KrizhevskyandG.Hinton,“Cifar-10(canadian [74] T. Wang, Q. Yang, K. Zhu, J. Wang, C. Su, and
instituteforadvancedresearch),” tech.rep., Cana- K. Sato, “Lds-fl: Loss differential strategy based
dianInstituteforAdvancedResearch,2009. federated learning for privacy preserving,” IEEE
Transactions on Information Forensics and Secu-
[67] K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang,
rity,vol.19,pp.1015–1030,2024.
F. Farokhi, S. Jin, T. Q. S. Quek, and H. Vin-
centPoor,“Federatedlearningwithdifferentialpri- [75] R.Xu,N.Baracaldo,Y.Zhou,A.Anwar,S.Kadhe,
vacy:Algorithmsandperformanceanalysis,”IEEE and H. Ludwig, “Detrust-fl: Privacy-preserving
Transactions on Information Forensics and Secu- federated learning in decentralized trust setting,”
rity,vol.15,pp.3454–3469,2020. 2022 IEEE 15th International Conference on
CloudComputing(CLOUD),pp.417–426,2022.
[68] K.B.Nampalle,P.Singh,U.Narayan,andB.Ra-
man,“Visionthroughtheveil: Differentialprivacy [76] M. Y. Lu, R. J. Chen, D. Kong, J. Lipkova,
infederatedlearningformedicalimageclassifica- R. Singh, D. F. Williamson, T. Y. Chen, and
tion,”062023. F. Mahmood, “Federated learning for computa-
tionalpathologyongigapixelwholeslideimages,”
[69] M. Asad, A. Moustafa, and T. Ito, “Fedopt: MedicalImageAnalysis,vol.76,p.102298,2022.
Towards communication efficiency and privacy
[77] W. Li, F. Milletar`ı, D. Xu, N. Rieke, J. Hancox,
preservation in federated learning,” Applied Sci-
W.Zhu,M.Baust,Y.Cheng,S.Ourselin,M.Car-
ences,vol.10,pp.1–17,042020.
doso, and A. Feng, “Privacy-preserving federated
[70] D.Truhn,S.T.Arasteh,O.L.Saldanha,G.Mu¨ller-
braintumoursegmentation,”Hindawi,2019.
Franzes,F.Khader,P.Quirke,N.P.West,R.Gray,
[78] P. Nanayakkara, J. Bater, X. He, J. Hullman, and
G. G. Hutchins, J. A. James, M. B. Loughrey, J.Duggan,“Visualizingprivacy-utilitytrade-offsin
M. Salto-Tellez, H. Brenner, A. Brobeil, T. Yuan, differentiallyprivatedatareleases,”Proceedingson
J. Chang-Claude, M. Hoffmeister, S. Foersch,
PrivacyEnhancingTechnologies,2022.
T. Han, S. Keil, M. Schulze-Hagen, P. Isfort,
P. Bruners, G. Kaissis, C. Kuhl, S. Nebelung, and [79] L. Xu, C. Jiang, Y. Qian, J. Li, Y. Zhao,
J. N. Kather, “Encrypted federated learning for and Y. Ren, “Privacy-accuracy trade-off in
secure decentralized collaboration in cancer im- differentially-private distributed classification: A
ageanalysis,”MedicalImageAnalysis,p.103059, gametheoreticalapproach,”IEEETransactionson
2023. BigData,2021.
28[80] M.Adnan,S.Kalra,J.C.Cresswell,G.W.Taylor, [89] M. Ryu, Y. Kim, K. Kim, and R. K. Mad-
and H. Tizhoosh, “Federated learning and differ- duri, “Appfl: Open-source software framework
entialprivacyformedicalimageanalysis,”Nature, forprivacy-preservingfederatedlearning,”in2022
2021. IEEE International Parallel and Distributed Pro-
cessing Symposium Workshops (IPDPSW), (Los
[81] O.Choudhury,A.Gkoulalas-Divanis,T.Salonidis, Alamitos,CA,USA),pp.1074–1083,IEEECom-
I.Sylla,Y.Park,G.Hsu,andA.K.Das,“Differen-
puterSociety,jun2022.
tialprivacy-enabledfederatedlearningforsensitive
healthdata,”ArXiv,vol.abs/1910.02578,2019. [90] S. Boyd, N. Parikh, E. Chu, B. Peleato, and
J. Eckstein, “Distributed optimization and statisti-
[82] S. Dhiman, S. Nayak, G. K. Mahato, A. Ram, callearningviathealternatingdirectionmethodof
andS.K.Chakraborty,“Homomorphicencryption multipliers,” Foundations and Trends in Machine
based federated learning for financial data secu- Learning,vol.3,no.1,pp.1–122,2011.
rity,”inIEEEInternationalConferenceonInnova-
[91] G.Kaissis,A.Ziller,J.Passerat-Palmbach,T.Ryf-
tionsinComputerScienceandEngineering(I3CS),
fel, D. Usynin, A. Trask, I. Lima, J. Man-
2023.
cuso, F. Jungmann, M.-M. Steinborn, A. Saleh,
[83] D. Stripelis, H. Saleem, T. Ghai, N. J. Dhinagar, M. Makowski, D. Rueckert, and R. Braren, “End-
U. Gupta, C. Anastasiou, G. V. Steeg, S. Ravi, to-end privacy preserving deep learning on multi-
M.Naveed, P.M.Thompson, andJ.Ambite, “Se- institutionalmedicalimaging,”NatureMachineIn-
cure neuroimaging analysis using federated learn- telligence,vol.3,pp.1–12,062021.
ingwithhomomorphicencryption,”inSPIEMedi-
[92] N. Shi, F. Lai, R. A. Kontar, and M. Chowdhury,
calImaging,2021.
“Fed-ensemble: Ensemble models in federated
learning for improved generalization and uncer-
[84] K.Burlachenko,A.Alrowithi,F.A.Albalawi,and
tainty quantification,” IEEE Transactions on Au-
P.Richta´rik,“Federatedlearningisbetterwithnon-
tomationScienceandEngineering,pp.1–0,2023.
homomorphic encryption,” in Proceedings of the
ACMSymposiumonCloudComputing,2023.
[93] V. Plassier, M. Makni, A. Rubashevskii,
E. Moulines, and M. Panov, “Conformal pre-
[85] A.Acar,H.Aksu,A.S.Uluagac,andM.Conti,“A
diction for federated uncertainty quantification
surveyonhomomorphicencryptionschemes:The-
underlabelshift,”inSEFM,vol.11724ofLecture
oryandimplementation,”2017.
NotesinComputerScience,pp.183–202,Springer,
[86] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy- 2023.
Cramer,K.Farahani,J.Kirby,Y.Burren,N.Porz,
[94] C.Lu,Y.Yu,S.P.Karimireddy,M.I.Jordan,and
J. Slotboom, R. Wiest, et al., “The multi-
R.Raskar,“Federatedconformalpredictorsfordis-
modalbraintumorimagesegmentationbenchmark
tributeduncertaintyquantification,”inProceedings
(brats),” IEEE transactions on medical imaging,
of the 40th International Conference on Machine
vol.34,no.10,pp.1993–2024,2015.
Learning,ICML’23,JMLR.org,2023.
[87] T. C. G. A. Network, “Comprehensive molecular [95] S. Bhatt, A. Gupta, and P. Rai, “Federated learn-
characterizationofhumancolonandrectalcancer,” ing with uncertainty via distilled predictive distri-
Nature,vol.487,no.7407,pp.330–337,2012. butions,”2023.
[88] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, [96] S.-M. Lee and J.-L. Wu, “Fedua: An uncertainty-
“Gradient-based learning applied to document awaredistillation-basedfederatedlearningscheme
recognition,” Proceedings of the IEEE, vol. 86, for image classification,” Information, vol. 14,
no.11,pp.2278–2324,1998. p.234,Apr2023.
29[97] D.Makhija,J.Ghosh,andN.Ho,“Privacypreserv- federated learning with non-iid data,” MM ’23,
ing bayesian federated learning in heterogeneous (NewYork,NY,USA),p.3099–3107,Association
settings,”arXivpreprintarXiv:2306.07959,2023. forComputingMachinery,2023.
[98] Y.Zhang,T.Xia,A.Ghosh,andC.Mascolo,“Un- [106] B.Sanderson,“Uncertaintyquantificationinmulti-
certainty quantification in federated learning for modelensembles,”OxfordResearchEncyclopedia
heterogeneoushealthdata,”inInternationalWork- ofClimateScience,2018.
shop on Federated Learning for Distributed Data
[107] A. Krizhevsky, “Learning multiple layers of
Mining,2023.
features from tiny images. university of toronto
[99] H. Chen, J. Ding, E. W. Tramel, S. Wu, (2012),” http://www.cs.toronto.edu/
A. K. Sahu, S. Avestimehr, and T. Zhang, “Self- kriz/cifar.html, last accessed, vol. 5, p. 13,
aware personalized federated learning,” ArXiv, 2022.
vol.abs/2204.08069,2022.
[108] A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings,
[100] X. Zhang, Y. Li, W. Li, K. Guo, and Y. Shao, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov,
“Personalized federated learning via variational M. Malloci, A. Kolesnikov, et al., “The open im-
Bayesian inference,” in Proceedings of the 39th ages dataset v4,” International Journal of Com-
International Conference on Machine Learning puterVision,pp.1–26,2020.
(K.Chaudhuri,S.Jegelka,L.Song,C.Szepesvari,
[109] A. Gammerman, V. Vovk, and V. Vapnik, “Learn-
G.Niu, andS.Sabato, eds.), vol.162ofProceed-
ing by transduction,” in Proceedings of the Four-
ings of Machine Learning Research, pp. 26293–
teenth Conference on Uncertainty in Artificial In-
26310,PMLR,17–23Jul2022.
telligence, UAI’98, (San Francisco, CA, USA),
p. 148–155, Morgan Kaufmann Publishers Inc.,
[101] N. Kotelevskii, M. Vono, A. Durmus, and
1998.
E. Moulines, “Fedpop: A bayesian approach
for personalised federated learning,” in Ad-
[110] C.Saunders,A.Gammerman,andV.Vovk,“Trans-
vances in Neural Information Processing Systems
ductionwithconfidenceandcredibility.,”pp.722–
(S.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,
726,011999.
K.Cho,andA.Oh,eds.),vol.35,pp.8687–8701,
CurranAssociates,Inc.,2022. [111] C. Lu and J. Kalpathy-Cramer, “Distribution-free
federated learning with conformal predictions,”
[102] T. Zhou, J. Zhang, and D. H. Tsang, “Fedfa: ArXiv,vol.abs/2110.07661,2021.
Federated learning with feature anchors to align
features and classifiers for heterogeneous data,” [112] O. Russakovsky, J. Deng, H. Su, J. Krause,
IEEE Transactions on Mobile Computing, pp. 1– S. Satheesh, S. Ma, Z. Huang, A. Karpathy,
12,2023. A. Khosla, M. S. Bernstein, A. C. Berg, and
L. Fei-Fei, “Imagenet large scale visual recogni-
[103] A. T. Thorgeirsson and F. Gauterin, “Probabilis- tionchallenge,”Int.J.Comput.Vis.,vol.115,no.3,
tic predictions with federated learning,” Entropy, pp.211–252,2015.
vol.23,2020.
[113] V.Vovk,A.Gammerman,andG.Shafer,Algorith-
[104] M. Luo, F. Chen, D. Hu, Y. Zhang, J. Liang, and micLearninginaRandomWorld.StatisticsforEn-
J. Feng, “No fear of heterogeneity: Classifier cal- gineeringandInformationScience,Springer,2005.
ibration for federated learning with non-iid data,”
ArXiv,vol.abs/2106.05001,2021. [114] K. E. Brown and D. A. Talbert, “A simple di-
rect uncertainty quantification technique based on
[105] Z. Qi, L. Meng, Z. Chen, H. Hu, H. Lin, and machinelearningregression,”FLAIRSConference,
X. Meng, “Cross-silo prototypical calibration for 2022.
30[115] M.Swaminathan,O.W.Bhatti,Y.Guo,E.Huang, segmentation,” in Medical Image Computing and
and O. Akinwande, “Bayesian learning for uncer- Computer-Assisted Intervention – MICCAI 2015,
taintyquantification,optimization,andinversede- (Cham), pp. 234–241, Springer International Pub-
sign,” IEEE Transactions on Microwave Theory lishing,2015.
andTechniques,2022.
[125] S. Pati, U. Baid, B. Edwards, M. Sheller, S.-
[116] O. Dalmaz, M. U. Mirza, G. Elmas, M. Ozbey, H. Wang, G. A. Reina, P. Foley, A. Gruzdev,
S. U. Dar, E. Ceyani, K. K. Oguz, S. Aves- D. Karkada, C. Davatzikos, et al., “Federated
timehr,andT.C¸ukur,“Onemodeltounitethemall: learningenablesbigdataforrarecancerboundary
Personalized federated learning of multi-contrast detection,”Naturecommunications,vol.13,no.1,
mri synthesis,” Medical Image Analysis, vol. 94, p.7346,2022.
p.103121,2024.
[117] S. Caldas, S. M. K. Duddu, P. Wu, T. Li,
J.Konecˇny`,H.B.McMahan,V.Smith,andA.Tal-
walkar, “Federated extended mnist (femnist).”
https://github.com/TalwalkarLab/leaf,
2018.
[118] A. Go, R. Bhayani, and L. Huang, “Sen-
timent140.” http://help.sentiment140.com/
for-students,2009.
[119] L.Breiman,“Baggingpredictors,”MachineLearn-
ing,vol.24,no.2,pp.123–140,1996.
[120] T. Pearce, A. Brintrup, and J. Zhu, “Understand-
ing softmax confidence and uncertainty,” arXiv
preprintarXiv:2106.04972,2021.
[121] “Tiny imagenet.” https://tiny-imagenet.
herokuapp.com/.
[122] “Vireofood-172.”https://fvl.fudan.edu.cn/
dataset/vireofood172/list.htm.
[123] S.Pati,U.Baid,M.Zenk,B.Edwards,M.Sheller,
G.A.Reina,P.Foley,A.Gruzdev,J.Martin,S.Al-
barqouni, Y. Chen, R. T. Shinohara, A. Reinke,
D. Zimmerer, J. B. Freymann, J. S. Kirby, C. Da-
vatzikos, R. R. Colen, A. Kotrotsou, D. Marcus,
M. Milchenko, A. Nazeri, H. Fathallah-Shaykh,
R. Wiest, A. Jakab, M.-A. Weber, A. Mahajan,
L. Maier-Hein, J. Kleesiek, B. Menze, K. Maier-
Hein,andS.Bakas,“Thefederatedtumorsegmen-
tation(fets)challenge,”2021.
[124] O. Ronneberger, P. Fischer, and T. Brox, “U-
net: Convolutionalnetworksforbiomedicalimage
31