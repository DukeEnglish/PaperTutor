Synergizing Foundation Models and Federated Learning: A Survey
ShenghuiLi1 FanghuaYe2 MengFang3 JiaxuZhao4
Yun-HinChan5 EdithC.-H.Ngai5∗ ThiemoVoigt1,6
1 UppsalaUniversity,Sweden. shenghui.li@it.uu.se
2 UniversityCollegeLondon,UnitedKingdom. fanghua.ye.19@ucl.ac.uk
3 UniversityofLiverpool,UnitedKingdom. mfang@liverpool.ac.uk
4 EindhovenUniversityofTechnology,theNetherlands. j.zhao@@tue.nl
5 TheUniversityofHongKong,China. {chngai@eee,chanyunhin@connect}.hku.hk
6 ResearchInstitutesofSweden,Sweden. thiemo.voigt@angstrom.uu.se
Abstract fromtheirpre-trainingonextensivedatasets(Gu-
nasekar et al., 2023), which imbues them with a
The recent development of Foundation Mod-
profound understanding of language, vision, and
els(FMs),representedbylargelanguagemod-
multimodaldata.
els,visiontransformers,andmultimodalmod-
els, has been making a significant impact on Whilegeneral-purposeFMscanleverageopenly
both academia and industry. Compared with accessibledatafromtheInternet,domain-specific
small-scalemodels,FMshaveamuchstronger FMsrequireproprietarydata. Itis,however,chal-
demandforhigh-volumedataduringthepre- lengingtocollectvastamountsofproprietarydata
trainingphase. AlthoughgeneralFMscanbe
andperformcentralizedpre-trainingorfine-tuning
pre-trainedondatacollectedfromopensources
for domain-specific FMs, due to privacy restric-
suchastheInternet,domain-specificFMsneed
tions (Jo and Gebru, 2020; GDPR, 2016; CCPA,
proprietarydata,posingapracticalchallenge
2023). Particularlyindomainssuchaslaw,health-
regardingtheamountofdataavailabledueto
privacyconcerns. FederatedLearning(FL)is care,andfinance,wheredataisinherentlyprivacy-
acollaborativelearningparadigmthatbreaks sensitive, there is a pressing need for stringent
the barrier of data availability from different privacy safeguards. Furthermore, given that data
participants. Therefore, itprovidesapromis-
often constitutes a pivotal asset for enterprises,
ing solution to customize and adapt FMs to
its widespread distribution is prohibitive. Conse-
a wide range of domain-specific tasks using
quently, there is an urgent need for novel strate-
distributeddatasetswhilstpreservingprivacy.
giestohandledataavailabilityandfacilitatemodel
Thissurveypaperdiscussesthepotentialsand
challenges of synergizing FL and FMs and training,therebyunlockingthepotentialofdomain-
summarizescoretechniques,futuredirections, specificFMswhilstrespectingdataprivacy.
andapplications. Aperiodicallyupdatedpaper Toaddressthechallengesassociatedwithdata
collectiononFM-FLisavailableathttps://
privacyinmodeltraining,FederatedLearning(FL)
github.com/lishenghui/awesome-fm-fl.
(McMahanetal.,2017)hasemergedasapromising
1 Introduction paradigm. FLfacilitatescollaborativemodeltrain-
ingacrossdecentralizedclientswithouttheneedto
The landscape of Artificial Intelligence (AI) has sharerawdata,thusensuringprivacypreservation.
beenrevolutionizedbytheemergenceofFounda- Concretely,FLencompassesperiodicinteractions
tionModels(FMs)(Bommasanietal.,2021),such between the server and decentralized clients for
asBERT(Devlinetal.,2019),GPTseries(Brown theexchangeoftrainablemodelparameterswith-
et al., 2020; OpenAI, 2022, 2024), and LLaMA out the requirement for private client data. Rec-
series (Touvron et al., 2023a,b) in Natural Lan- ognizingsuchabenefit,integratingFMswithFL
guageProcessing(NLP);ViTs(Dosovitskiyetal., presentsacompellingsolutionfordomain-specific
2021)andSAM(Kirillovetal.,2023)inComputer FMs(Zhuangetal.,2023;Yuetal.,2023d).
Vision(CV);CLIP(Radfordetal.,2021),DALL-
DespitethepotentialsynergiesbetweenFLand
E (Ramesh et al., 2021), Gemini (Google, 2023),
FMs,thefieldisstillnascent,lackingacomprehen-
and GPT-4o in multimodal applications. These
siveunderstandingofchallenges,methodologies,
FMshavebecomepivotalinamyriadofAIappli-
anddirections. Thissurveyaimstobridgethisgap
cationsacrossdiversedomains. Theirsuperbcapa-
byprovidingathoroughexplorationoftheintegra-
bilitytogeneralizeacrosstasksanddomainsstems
tionofFMsandFL.Wedelveintothemotivations
∗CorrespondingAuthor. andchallengesofcombiningthesetwoparadigms,
4202
nuJ
81
]GL.sc[
1v44821.6042:viXrahighlightrepresentativetechniques,anddiscussap- by Ramesh et al. (2021), generates images from
plicationsandfuturedirections. Byelucidatingthe textualdescriptions,expandingthepossibilitiesof
intersection of FL and FMs, we aim to catalyze creative image generation. Additionally, models
further research and innovation in this burgeon- like GAto (Reed et al., 2022), exhibit versatility
ingarea,ultimatelyadvancingthedevelopmentof by being applicable across various tasks such as
privacy-aware,domain-specificFMs. conversationalagents,roboticcontrol,andgaming.
Thepapercontinuesasfollows: Thenextsection
2.2 FederatedLearning
introducesbackgroundonFMsandFL.Section3
presentsthemotivationandchallengesforsynergiz- FL(McMahanetal.,2017)isalearningparadigm
ingFMsandFL.Section4highlightsrepresenta- that enables a collection of clients to collabora-
tivetechniques. Section5explorestheapplications tively learn a shared global model by leveraging
acrossvariousdomains. Beforeconcluding,wedis- their private datasets in a distributed manner, as-
cussrepresentativefuturedirectionsinSection6. sistedbythecoordinationofacentralserver. The
generalgoalofFListofindaparametersetθ that
2 Background minimizesthefollowingdistributedoptimization
objective:
2.1 FoundationModels
An FM is a model that can be adapted to a wide 1 (cid:88)
minF(θ) := F (θ), (1)
k
arrayoftasksthroughfine-tuningafterinitialpre- θ K
k∈[K]
training (Bommasani et al., 2021). The lifecycle
ofFMstypicallyinvolvespre-trainingonextensive whereK representsthetotalnumberofclientsand
generic data to establish the basis of their abili- F (θ) = E [ℓ(θ;z)]denotestheexpectedrisk
k z∼D k
ties(Bubecketal.,2023),followedbyadaptation ofthek-thclient. Here,D isthedatadistribution
k
todownstreamtaskssuchasdomain-specificques- forthek-thclient,andℓ(·;·)isauser-specifiedloss
tionanswering(Zhangetal.,2023e),andultimately function.
applicationinvariousdomains. ThemostrepresentativealgorithmsintheFLlit-
FMshavesparkedasignificantparadigmshiftin eraturearetheFedAvg-familyalgorithms(McMa-
variousfieldsofAIsuchasNLP,CV,speechand hanetal.,2017;Reddietal.,2021). Thestandard
acoustics, and beyond. In the realm of NLP, the FedAvginvolvesperiodicinteractionsbetweenthe
mostprominentexampleisLargeLanguageMod- serveranddecentralizedclientstoexchangetrain-
els(LLMs)withsubstantialparametersizes(Zhao ablemodelparameters. Inthisprocess,eachclient
etal.,2023). Thesemodels,suchasChatGPTand independently trains the model on its local data
GPT-4(OpenAI,2022,2024),demonstrateexcep- and sends the model updates to a central server.
tionalabilitiesinnaturallanguageunderstanding Theserveraggregatestheseupdatesbycomputing
andgeneration,enablingthemtocomprehendand theiraveragetoupdatetheglobalmodel,whichis
respondtouserinputswithremarkablecontextual subsequentlyredistributedtotheclientsforfurther
relevance. Thiscapabilityprovesinvaluableinap- iterations. Many variants have been proposed to
plicationslikecustomerservice,virtualassistants, tackle issues such as convergence and local data
and chatbots, where effective communication is heterogeneity (Diao et al., 2021). For example,
paramount. Moreover, LLMs eliminate the need FedProx(Lietal.,2020)andFedDyn(Acaretal.,
fortrainingmodelsfromscratchforspecifictasks, 2021)introduceregularizertermstopenalizeclient
beitmachinetranslation,documentsummarization, updatesthatarefarawayfromtheservermodel. A
textgeneration,orotherlanguage-relatedtasks. generalframeworkFedOpt(Reddietal.,2021)uni-
In the realm of CV and other modalities, FMs fies adaptive optimizers (Adam, Yogi, etc.) and
havealsomaderemarkableprogress. VisionTrans- demonstrates superior convergence speed when
formers(ViTs)(Dosovitskiyetal.,2021)segment comparedtothenaiveFedAvg.
images into distinct patches, which serve as in- FL offers an efficient privacy-preserving way
putsfortransformerarchitectures. SAM(Kirillov to train models on large-scale and diverse data
et al., 2023) can segment anything in images ac- (Kairouz et al., 2021), leading to its application
cordingtotheinputprompts. CLIP(Radfordetal., acrossvariousdomainssuchashealthcare(Lincy
2021) bridges the gap between text and images and Kowshalya, 2020; Rieke et al., 2020; Joshi
through contrastive learning. DALL·E, proposed etal.,2022), finance(Chatterjeeetal.,2023;Liuetal.,2023b),andsmartcities(Ramuetal.,2022; Efficiency Challenges. Efficiency challenges
Pandyaetal.,2023). stem from the mismatch between the significant
resourcedemandsofFMtrainingandthelimited,
3 FM-FL:Motivation&Challenges heterogeneous system resources (e.g., mobile de-
vices)withinFLsystems,suchascommunication
In this section, we first motivate the synergy of
bandwidth,computationalpower,andmemory(Su
FMsandFL(Section3.1),thensummarizethekey
etal.,2023). ThecommunicationbottleneckofFL
challenges(Section3.2).
isinducedbyfrequentlyexchangingtraininginfor-
3.1 Motivation mationbetweentheserverandclientsoverlimited
bandwidth channels (Kairouz et al., 2021). The
TheintegrationofFMsandFLrepresentsacom-
substantial number of parameters in FMs further
pelling collaboration that leverages each other’s
exacerbatesthisburden,thushinderingthetraining
strengths to address their respective limitations,
process.
embodyingacomplementaryrelationship(Zhuang
etal.,2023;LiandWang,2024).
Adaptability Challenges. Adaptability chal-
FLexpandsdataavailabilityforFMs. Bylever- lenges arise from the adaptation of an FM to a
specificdownstreamtask(e.g.,byfine-tuning)in
aging data from a wide range of sources in a
FL settings. Key challenges include data hetero-
privacy-preserving manner, FL makes it possible
geneityandresourceheterogeneity. Performance
to build models on sensitive data in specific do-
degradationinFL,attributedtoheterogeneousdata
mains,suchashealthcare(LincyandKowshalya,
distributionsamongclients,isawell-recognizedis-
2020; Joshi et al., 2022; Rieke et al., 2020) and
sue(Kairouzetal.,2021;Lietal.,2022). Arecent
finance(Chatterjeeetal.,2023;Liuetal.,2023b).
study(Babakniyaetal.,2023a)hasshownthatsuch
Thisenhancesthediversityandvolumeoftraining
performancepenaltyisevenmoresubstantialwhen
data,improvingmodelrobustnessandadaptability.
fine-tuningFMs. ForNLPtasks,dataheterogene-
Moreover,FLenablestheintegrationofpersonal
ity can manifest as variations in language, style,
and task-specific data, allowing FMs to be cus-
topic,orsentimentacrossdatasetsheldbydifferent
tomized for personal applications. For instance,
clients. Inmulti-modalscenarios,thechallengeis
Googlehastrainednext-word-predictionlanguage
evenmorepronouncedduetotheinherentdiversity
modelsonmobilekeyboardinputdatawithFLto
in data types (e.g., text, images, and audio) (Yu
improveuserexperience(Xuetal.,2023;Bonawitz
et al., 2023a). Addressing data heterogeneity in-
etal.,2021).
volvesnotjustidentifyingandmeasuringitbutalso
FMsboostFLwithfeaturerepresentationand developingalgorithmsthatarerobusttosuchdiver-
few-shotlearningcapabilities. Bypre-training sity,ensuringthatthemodelcanlearneffectively
onlarge-scalegenericdata,FMsacquireessential fromvarieddatacontributionswithoutcompromis-
knowledgeandunderstandingcapabilities(Brown ing on performance. In terms of resource hetero-
et al., 2020), providing multiple benefits to FL. geneity,thememoryandcomputationalresources
Firstly, they benefit FL systems by offering ad- ofthedevicesfordifferentparticipantsmaybedi-
vancedfeaturerepresentationsandlearningcapa- verse(Diaoetal.,2021),whichcouldcausedelays
bilities from the outset. Secondly, leveraging the formodelsynchronizationandinactivationofsome
pre-learnedknowledgeofFMscanacceleratethe participants,i.e.,stragglers,makingitchallenging
FLprocess,enablingefficientandeffectiveadapta- toleveragethefullpotentialofFMsinFLsettings.
tiontospecifictaskswithminimaladditionaltrain-
ing. Thirdly,FMs’powerfulgenerativecapabilities Trustworthiness Challenges. Trustworthiness
could help FL overcome the data heterogeneity challengesemphasizetheconcernsregardingpri-
challengebysynthesizingextradata,thusacceler- vacy,security,andethicalconsiderationsinthelife-
atingmodelconvergence(Huangetal.,2024). cycleofFM-FL,fromthepre-trainingandmodel
adaptation to the application stages. We present
3.2 CoreChallenges
tworepresentativechallengesfromthisperspective:
Inthispart,wediscusschallengesemergingfrom (1)intellectualproperty: IntellectualProperty(IP)
the FM-FL marriage in three aspects: efficiency, protectioninFM-FLprimarilyinvolvesattributing
adaptability,aswellastrustworthiness. ownershiprightsforbothmodelsanddata. Fromtheserver’sperspective,broadcastingapre-trained terms. BitFit has inspired a series of studies
model to multiple nodes for fine-tuning poses IP in FedPEFT (Bu et al., 2022; Sun et al., 2022a;
protectionandsecurityrisks(e.g.,modeltheft),ne- Zhang et al., 2023f), demonstrating the superior
cessitatingmeasurestosafeguardIPrightsanden- communication efficiency of only updating the
suremodelintegrity(Kangetal.,2024);(2)privacy biastermswhilestillachievingcompetitiveperfor-
leakage: AlthoughFLdoesnotimmediatelyshare mance. Moresophisticatedmethodsstrivetofind
data, studies have shown that it may not always sparsesubnetworksforpartialfine-tuning. Among
guaranteesufficientprivacypreservation(Geiping them,variousmethods(Seoetal.,2021;Lietal.,
etal.,2020),asmodelparameters(e.g.,weightsor 2021a;Tamirisaetal.,2024)advocatefortheLot-
gradients)mayleaksensitiveinformationtomali- teryTicketHypothesis(LTH)(FrankleandCarbin,
ciousadversaries(Zhuetal.,2019). (3)Poisoning 2019),positingthatadensenetworkcontainsmany
Attacks: FL systems are inherently vulnerable to subnetworks whose inference capabilities are as
attacksduetotheirwideattacksurfaceandreliance accurateasthatoftheoriginalnetwork. FedSelect
onnetworkcommunication(Lietal.,2023b). Poi- (Tamirisaetal.,2024)isarepresentativemethod
soningattacksarecarriedoutbymaliciouspartici- thatencouragesclientstofindoptimalsubnetworks
pants,aimingtobiastheglobalmodeltothedesire basedonLTHandcontinuallyfine-tunesthesede-
ofattackers. rivedsubnetworkstoencapsulatelocalknowledge.
As another important aspect, RaFFM (Yu et al.,
4 Techniques
2023c)proposestoprioritizespecializedsalientpa-
rametersbyrankingthemusingsalienceevaluation
Recent work has begun to address challenges as-
metricssuchastheℓ andℓ norms.
sociatedwithadaptingpre-trainedFMstospecific 1 2
downstreamtasksinFLsettings. Inthissection,we
AdditiveMethods. Insteadoffine-tuningasub-
surveyFM-FLtechniquesonthreeaspects,namely
setofmodelparameters,additivemethodsincorpo-
efficiency(Section4.1),adaptability(Section4.2),
ratelightweighttrainableblocksintofrozenFMs
andtrustworthiness(Section4.3). Asillustratedin
andtunetheadditionalparametersformodeladap-
Figure1, wefurtherrefinethemaccordingtothe
tation. Thesemethodsnotonlyenhancecomputa-
keyfeaturesofdifferentmethods.
tionalandcommunicationalefficiencybutalsoin-
4.1 Efficiency troduceanextrabenefit: personalization(Luetal.,
2023a), i.e., the integration of these supplemen-
Therehasbeenaconsiderablefocusondeveloping
taryparametersallowsforthecustomizationofhet-
resource-efficientapproaches. Thispartdescribes
erogeneous models tailored to specific local data
techniquesthatimproveresourceefficiency.
characteristicsoruserpreferences. Keybranches
4.1.1 Parameter-EfficientFine-Tuning withinadditivemethodsincludeadaptertuningand
prompt tuning. Adapter tuning integrates small-
Federated Parameter-Efficient Fine-Tuning (Fed-
scaleneuralnetworks(knownas“adapters”)into
PEFT),originatingfromthefine-tuningpractices
the pre-trained models (Houlsby et al., 2019; Hu
ofFMs(Lesteretal.,2021;Huetal.,2022;Liand
et al., 2022). On the other hand, prompt tun-
Liang,2021),isasuiteoftechniquesdesignedto
ingincorporatestrainabletask-specificcontinuous
reduce both the computational load and the asso-
promptvectorsattheinputlayer(Liuetal.,2023a;
ciatedcommunicationoverheads(Malaviyaetal.,
Dongetal.,2023). Moredetailsonthesemethods
2023; Woisetschläger et al., 2024). In alignment
areprovidedinAppendixA.
with existing FM fine-tuning taxonomies (Lialin
etal.,2023;Dingetal.,2023),wepresentFedPEFT
Reparameterization-based Methods. The hy-
methods in three categories: selective methods,
pothesis behind reparameterization-based meth-
additive methods, and reparameterization-based
ods is that fine-tuning adaptations can be re-
methods.
parameterized into optimization within low-rank
SelectiveMethods. Selectivemethodsfine-tune subspaces (Aghajanyan et al., 2021). Low-Rank
a small subset of the parameters, leaving the ma- Adaptation (LoRA) (Hu et al., 2022), as a popu-
jority unchanged. In the field of LLMs, a promi- larPEFTmethodfromtheareaofLLMs,reduces
nent example of such methods is BitFit (Ben Za- thenumberoftrainableparametersfordownstream
ken et al., 2022), which only fine-tunes the bias tasksbyrepresentingtheweightupdateswithtwoSelective RaFFM(Yuetal.,2023c),FedBF(Zhangetal.,2023f)
Parameter-Efficient
Additive FedCLIP(Luetal.,2023a),FedDAT(Chenetal.,2024)
Fine-Tuning
Reparameterization-based HETLORA(Choetal.,2024),FedDPA(Yangetal.,2024b)
Efficiency Sparsification PruneFL(Jiangetal.,2023c),FLASH(Babakniyaetal.,2023b)
(§4.1) ModelCompression
Quantization FedSplitBERT(Litetal.,2022)
Zeroth-Order BAFFLE(Fengetal.,2023b),FedZeN(Maritanetal.,2023),FedKSeed(Qinetal.,2024),
Optimization FwdLLM(Xuetal.,2024a),ZooPFL(Luetal.,2023b),FedMeZO(Lingetal.,2024)
Domain-AdaptivePre-Training FMTDA(Yaoetal.,2022),FEDBFPT(Wangetal.,2023)
Domain-Centric
Multi-DomainAdaptation FedAPT(Suetal.,2024),DiPrompT(Baietal.,2024b)
Personalization FedDAT(Chenetal.,2024),Fed-MNMT(Liuetal.,2023d) Adaptability
Client-Centric
(§4.2)
ClientClustering FedLFC(Guoetal.,2024b),FL-TAC(Pingetal.,2024)
Resource-Heterogeneous FedRA(Suetal.,2023),HETLORA(Choetal.,2024)
System-Centric
SplitLearning FedBERT(Tianetal.,2022),FedSplitX(Shinetal.,2023b)
Watermarking WAFFLE(Tekguletal.,2021),DUW(Yuetal.,2023b)
IPProtection
Black-BoxTuning Fed-BBPT(Linetal.,2023),pFedGPT(Ruietal.,2024)
Privacy-PreservingTechniques DP-FTRL(Xuetal.,2023),DP-LoRA(Liuetal.,2023c)
Trustworthiness
PrivacyProtection
(§4.3)
PrivacyAttack FILM(Guptaetal.,2022),DRA(Zhangetal.,2024c)
PoisoningAttacks Fed-EBD(Lietal.,2024c)
AttackRobustness
DefenseTechniques ClippedClustering(Lietal.,2023b),Fed-FA(Zhangetal.,2023d)
Figure1: Taxonomyofresearchinfoundationmodelswithfederatedlearning.
smallermatrices(calledupdatematrices)through
low-rankdecomposition(Dingetal.,2023). When
optimizingaparametermatrixW ∈ Rm×n,theup-
dateequationcanbewrittenas: W ← W+∆W.
The core idea of LoRA is to freeze the original
matrixWwhileapproximatingtheparameterup-
date∆Wbylow-rankdecompositionmatrices,i.e.,
∆W = A·B⊤,whereA ∈ Rm×k andB ∈ Rn×k
arethetrainableparametersfortaskadaptationand
k ≪ min(m,n) is the reduced rank. The train-
able parameter size is then reduced from mn to
k(m+n). The major benefit of LoRA is that it
can largely save memory and storage usage. A
straightforwardwaytoperformfederatedfinetun-
Figure2: TaxonomyofFederatedParameter-Efficient
ing with LoRA is to train the LoRA modules A
Fine-Tuning(FedPEFT).Apartfromefficiency,some
andBwithhomogeneousrankk acrossallclients methods also account for other considerations, such
withstandardFLsuchasFedAvg(McMahanetal., asdataandresourceheterogeneitychallengesthatare
2017). Servalstudieshaveshownthatthismethod identifiedinSection3.2andblack-boxtuning(seeSec-
can achieve an outstanding level of trade-off be- tion4.3).
tweenperformanceandcommunicationoverhead
forawiderangeofFMs,includinglanguagemod-
els(Zhangetal.,2024b,2023f), vision-language long to multiple overlapping categories. To com-
models(Nguyenetal.,2024), andspeech-to-text parethecommunicationefficiencyofdifferentFed-
models(Duetal.,2024). PEFTmethods,Table1givesabriefoverviewof
experimentalevaluationsfromrepresentativestud-
Comparison of FedPEFT methods. Figure 2 ies. Comparedtofull-modelfine-tuning,FedPEFT
depictsthetaxonomyofFedPEFTwithrepresen- methods only require 0.1%-30% communication
tative methods. Note that some methods may be- overhead. We note that the differences can be at-
LF-MFtributed to several factors, including model com- 2020). Alternatively, many studies opt for two-
plexityandimplementationdetails. pointgradientestimatorsthatcanyieldamoresta-
bleandreliableapproximation(Spall,1992;Mal-
4.1.2 ModelCompression
ladi et al., 2023a; Lin et al., 2023; Ling et al.,
Modelcompressionreferstothetechniquesused 2024). Thestandardtwo-pointgradientestimator
to reduce the size of models, thereby improving estimatesthegradientonaminibatchB as
resourceefficiency(ShahandLau,2023).
L(θ+ϵz;B)−L(θ−ϵz;B)
∇ˆL(θ;B) = z.
Sparsification. Modelsparsificationmethodsre-
2ϵ
ducecommunicationburdenbyonlytransmittinga (3)
subsetofFMparametersacrossthenetwork(Jiang Based on the above gradient estimation frame-
etal.,2023c). Typicalmethodsfocusonidentifying works, recent work, such as that by Xu et al.
andcultivatinghigh-potentialsubnetworks(Fran- (2024a); Lu et al. (2023b), has initiated prelimi-
kleandCarbin,2019;Tsouvalasetal.,2023). naryexplorationsintothedeploymentofbothFed-
PEFT and full-model fine-tuning of billion-sized
Quantization. Quantizationiswell-established
FMs,likeLLaMA,onmobiledevices. Thenaive
inboththeFMandFLdomains(Xuetal.,2024b;
ZOOmethodsremainimpracticalfortraininglarge
Reisizadeh et al., 2020), which involves decreas-
FMs in standard FL frameworks such as FedAvg,
ing the precision of floating-point parameters for
astheystillresultinasignificantcommunication
mitigatingthestorage,computational,andcommu-
burden for model aggregation. In light of this,
nication demands. Quantization is orthogonal to
FedKSeed(Qinetal.,2024)wasproposedtofur-
otherresource-efficienttechniques,makingitfea-
therreducecommunicationoverheadsbetweenthe
sible to combine them for greater efficiency and
serverandclientsbyusingjustafewrandomseeds
flexibility(Litetal.,2022).
andscalargradients,requiringonlyafewthousand
bytesforcommunication.
4.1.3 Zeroth-OrderOptimization
AlthoughZOOmethodshaveshownpromisein
In contrast to the use of gradient descent in most
resource-efficientFL(Lingetal.,2024),theygen-
FLoptimizationalgorithms,aparticularlineofre-
erallyrequiremanyiterationstoachievestrongper-
searchadvocatesfortheremovalofBackPropaga-
formance(Malladietal.,2023b). Comparedtothe
tion(BP)(Malladietal.,2023a)infavorofZeroth-
well-established BP-based optimization, ZOO is
OrderOptimization(ZOO)(Fangetal.,2022;Li
stillintheearlystagesofdevelopment,particularly
andChen,2021). BP-freemethodsconservemem-
forFM-FLsettings,necessitatingfurtherresearch
oryneededforcomputinggradientsandminimize
andoptimization.
communication overhead for model aggregation
(Qinetal.,2024),makingFMsmoreaccessiblefor
4.2 Adaptability
lower-enddevices,therebyenhancingtheirappli-
Adaptationreferstotheprocessoftailoringapre-
cabilityindiversehardwareenvironments.
trained FM to perform effectively across varying
ZOO methods primarily rely on perturbation
FLsettingsandscenarios. Thismainlyincludesthe
methodstoestimategradientswithforwardprop-
agation. Given a model with parameters θ ∈ Rd capabilitytolearnfromdifferentdomains,caterto
individualuserneeds,andworkacrossdiversede-
andalossfunctionL,atypicalgradientestimator
viceswhileretainingoverallperformanceandeffi-
estimatesthegradientonaminibatchB as
ciency. Wefocusonthreekeyaspectsofadaptation,
L(θ+ϵz;B)−L(θ;B) namelydomain-centricadaptation,client-centric
∇ˆL(θ;B) = z, (2)
2ϵ adaptation,andsystem-centricadaptation.
where z ∈ Rd with z ∼ N(0,I ) and ϵ is the 4.2.1 Domain-CentricAdaptation
d
perturbation scale (Duchi et al., 2015). It re-
Domain-centric adaptation focuses on adapting
quiresonlytwoforwardpassesthroughthemodel
FMs within specific domains by addressing the
tocomputetheestimationofgradient, servingas
domaindiversityacrossclientdatasets.
amemory-efficientalternativetoBP.However,Eq.
(2)providesabiasedgradientestimation,leading Domain-AdaptivePre-Training. Despitebeing
toacertaindegreeofinformationloss(Liuetal., heavily reliant on large-scale and public datasetsTable1: ComparisonofFederatedParameter-EfficientFine-Tuning(FedPEFT)Methods.
#Full #Train. Training Comm.
Category RepresentativeWork Modality Model
Params. Params. Accel. Cost
RaFFM(Yuetal.,2023c) Txt. BERT-Large(2019) 336M 100M 6.13× 29.8%
Selective
FedBF(Zhangetal.,2023f) Txt. Roberta-Base(2019) 125M 0.66M 1.6%
FedAP(Zhangetal.,2023f) Txt. Roberta-Base(2019) 125M 2M 1.6%
FedCLIP(Luetal.,2023a) Vis.-Txt. ViT-B/32(2020a) 150M 0.53M 3.5%
FedDAT(Chenetal.,2024) Vis.-Txt. ALBEF(2021b) 290M 2.86M 9.9%
Adapter
C2A(Kimetal.,2023) Txt. DistilBERT(2020) 66M 0.06M 0.1%
Fed-MNMT(Liuetal.,2023d) Txt. mBART-50(2020) 611M 8M 1.3%
Additive
AdaFL(Caietal.,2023) Txt. BERT(2019) 110M 0.61M 1.63× 0.6%
PromptFL(Guoetal.,2023) Vis.-Txt. ViT-B/16(2021) 87M 0.87M 2.38× 0.9%
MFPT(Zhaoetal.,2024b) Txt. XLM-RoBERTa(2020) 270M 1.2M 0.4%
Prompt
FedAPT(Suetal.,2024) Vis.-Txt. ViT-B/32(2020a) 88M 2.8M 3.2%
FedSP(Dongetal.,2023) Txt. GPT2-XL(2019) 1.6B 111M 0.5%
SLoRA(Babakniyaetal.,2023a) Txt. DistilBERT(2020) 67M 0.7M 13.47× 5.8%
LP-FL(Jiangetal.,2023a) Txt. BERT-Large(2019) 336M 100M 30%
Reparameterization-based
FedMS(Wuetal.,2023c) Vis.-Txt. ViT-B/16(2021) 87M 8.6M 10%
Methods
pFedS2T(Duetal.,2024) Aud. Whisper(2023) 254M 10.1M 4%
FFA-LoRA(Sunetal.,2024b) Txt. RoBERTa-Large(2019) 355M 0.39M 0.1%
for their initial training, FMs often require fur- 4.2.2 Client-CentricAdaptation
therDomain-AdaptivePre-Training(DAPT)with
Client-centric adaptation refers to the process of
domain-specificdatafortasksthatnecessitatespe-
tailoringanFMtomeetthespecificneedsorpref-
cializedknowledge(Gururanganetal.,2020;Guo
erencesofindividualclientswhileleveragingthe
and Yu, 2022). In domains like healthcare, FL
decentralizedandprivacy-preservingnatureofFL.
allowsforthecontinuedpre-trainingofthesemod-
Particularly,wediscusstwotypesofpopularper-
els using sensitive, domain-specific data without
sonalizedmethodsasfollows:
compromising privacy. Based on this idea, Jiang
etal.(2023b)proposedFFDAPT,acomputational- Personalization. Adapter-basedmethodsintro-
efficientfurtherpre-trainingalgorithmthatfreezes ducesmall,trainableadaptersintothefrozenpre-
a portion of consecutive layers while optimizing trained FMs, allowing for client-specific model
therestofthelayers. Similarly,Wangetal.(2023) adaptation without altering the original FL. Fed-
proposedFEDBFPTthatbuildsalocalmodelfor DAT(Chenetal.,2024)leveragesadual-adapter
each client, progressively training the shallower structure,withpersonalizedadaptersfocusingon
layersoflocalmodelswhilesamplingdeeperlay- client-specific knowledge and a global adapter
ers,andaggregatingtrainedparametersonaserver maintaining client-agnostic knowledge. FedDAT
tocreatethefinalglobalmodel. executesbi-directionalknowledgedistillationbe-
tweenpersonalizedadaptersandtheglobaladapter
toregularizetheclient’supdatesandpreventover-
fitting. Prompt-basedmethodsinvolveusingclient-
Multi-Domain Adaptation. Given that client
specificsoftpromptstoguidethemodel’sresponse.
datamaybelongtovariousdomainsinreal-world
pFedPG(Yangetal.,2023a)trainsapromptgener-
FL scenarios, some efforts (Feng et al., 2023c;
atortoexploitunderlyingclient-specificcharacter-
Su et al., 2024) have been devoted to facilitating
isticsandproducepersonalizedpromptsforeach
multi-domaincollaborativeadaptation.Fengetal.
client,therebyenablingefficientandpersonalized
(2023c) applied a pre-trained CLIP to the multi-
adaptation.
domainscenarioandproposedanadaptiveprompt
tuning method that uses domain-specific keys to ClientClustering. Thisbranchofstudyaimsto
generate prompts for each test sample. Further- clusterclientsbasedontheunderlyingrelationships
more,Suetal.(2024)employedknowledgedistil- andtailorFMsfortheclientgroupwithsimilardata
lationtoselectivelydistillglobalknowledgebased distributions,thusreducingthenegativeimpactof
onanentropymeasure,improvingthegeneraliza- data heterogeneity and improving accuracy. Guo
tionacrossdifferentdomains. et al. (2024b) proposed a FedPEFT-based frame-work for multilingual modeling, which employs eratedatthefirstlayeroftheservermodelisthen
languagefamilyclusteringtoalleviateparameter transmittedbacktotheclientforfurtherbackprop-
conflictsofLoRAtuning. agation. Along this line, FedBERT (Tian et al.,
2022)proposestoleveragesplitlearningfortrain-
4.2.3 System-CentricAdaptation
ing the BERT model, showing the feasibility of
System-centricaimstoimproveadaptabilityatthe
traininglargeFMsinFLsettings. FedSplitX(Shin
systemlevel. Thisinvolveshandlingresourcehet-
et al., 2023b) is a more fine-grained method that
erogeneityintheFLsystemswhileensuringtrain-
allowsmultiplepartitionpointsformodelsplitting,
ingefficiencyandmodelutility.
accommodating more diverse client capabilities.
ComparedtoconventionalFL,splitlearningscales
Resource-Heterogeneous Methods. Cross-
better with the size of FMs as it communicates
device FL systems may be composed of devices
only small-sized smashed data instead of model
equipped with heterogeneous resources, leading
parameters(Singhetal.,2019). Despiteitsmerits,
to disparities where certain devices exhibit more
splitlearningishighlydependentonthenetwork
efficient model training than others (Chen et al.,
connectionquality. Giventhatserver-clientinterac-
2024). Toaddressthisissue,severalmethodshave
tionsoccurateverystepoftheoptimizationprocess
beendevelopedtocustomizemodelarchitectures
(Zhengetal.,2023),communicationdelayscause
for resource-heterogeneous FL systems. In FL
amoresignificantimpactonefficiency.
environmentspossessingheterogeneousresources,
LoRA-based FedPEFT exhibits distinctive flexi-
4.3 Trustworthiness
bility and adaptation in fine-tuning frozen FMs
Thislineofworkaimstoenhancetrustworthiness
without overburdening client devices. Su et al.
throughouttheFM-FLlifecycle,coveringavariety
(2023) suggested assigning LoRA adapters to
ofkeyaspectsincluding,butnotlimitedto,IPpro-
varying numbers of layers for heterogeneous
tection,privacyprotection,andattackrobustness.
clients according to a randomly generated mask
matrix. Analternativeandmoretargetedideaisto
4.3.1 IPProtection
choose diverse LoRA ranks across clients based
ExistingIPprotectioninvolvessafeguardingown-
on their system capabilities. Bai et al. (2024a)
ershipofFMsfromunauthorizeduse(e.g.,model
proposed FlexLoRA to adjust local LoRA ranks
theft) (Tekgul et al., 2021). We discuss the fol-
dynamically. FlexLoRAreconstructstheuniform
lowing two mainstream IP protection strategies:
full-sized LoRA module ∆W for server-side
watermarkingandblack-boxtuning.
model aggregation followed by an SVD-based
parameter redistribution. However, concurrent Watermarking. Watermarkingisawell-known
research by Cho et al. (2024) has empirically deterrencetechnologyformodelIPprotectionby
demonstrated that the reconstruct-redistribute providingtheidentitiesofmodelownerstodemon-
methodsuffersfromperformancelosscomparedto strateownershipoftheirmodels(Adietal.,2018).
homogeneousLoRA.Instead,theyproposed HET- Tekgul et al. (2021) proposed WAFFLE, the first
LORA(Choetal.,2024)thatutilizeszero-padding solutionthataddressestheownershipproblemby
to align module size before aggregation. It then injectingawatermarkintotheglobalmodelinFL
truncatestheglobalLoRAmodulesforthespecific environments. Recently,Yuetal.(2023b)proposed
rankofthenextselectedclients. DUW that embeds a client-unique key into each
client’slocalmodel,aimingtoidentifytheinfringer
SplitLearning. Splitlearningaddressesthere-
ofaleakedmodelwhileverifyingtheFLmodel’s
source heterogeneity between servers and clients
ownership.
bysplittingalargemodelatacutlayerintoclient
and server models (Thapa et al., 2022). For each Black-BoxTuning. Black-BoxTuning(BBT)is
trainingstep,theoutputtensor,so-calledsmashed a set of ZOO-based methods that fine-tune FMs
data,fromtheclientmodelandthecorresponding without direct access to model parameters (Sun
labelsaretransmittedovertotheserver. Theserver etal.,2022c,b). BBTmethodsareoftenadditive,
continues the forward propagation by processing introducing additional parameters while keeping
the smashed data through its remaining layers; it theoriginalmodelfrozen(seeSection4.1.1). Fed-
thencomputesthelossusingthetransmittedlabel BBPT(Linetal.,2023)isageneralprompttuning
andperformsbackpropagation. Thegradientgen- framework that facilitates the joint training of aglobal lightweight prompt generator across mul- of popular LLMs, including BERT, DistilBERT,
tiple clients. FedBPT (Sun et al., 2024a) adopts andOpenAI’sGPTs. Intermsofdatareconstruc-
aclassicevolutionary-basedZOOmethod,CMA- tionattacks,Guptaetal.(2022)presentedanattack
ES (Hansen and Ostermeier, 2001), for training FILM,whichrecoversprivatetextdatabyextract-
anoptimalpromptthatimprovestheperformance inginformationfromgradientstransmittedduring
of frozen FMs. ZooPFL (Lu et al., 2023b), on trainingdespiteemployingaDPmechanism.
the other hand, applies coordinate-wise gradient
4.3.3 AttackRobustness
estimate to learn input surgery that incorporates
Due to the distributed characteristic of optimiza-
client-specificembeddings. BBTallowsforlocal
tion, FL is vulnerable to poisoning attacks (Lyu
fine-tuning of FMs while not infringing IP con-
et al., 2022; Rodríguez-Barroso et al., 2023),
straints. However, current research in this line is
whereincertainparticipantsmaydeviatefromthe
limitedtofew-shotlearningwithsmalldatasetsfor
prescribed update protocol and upload arbitrary
LLMfine-tuning(Sunetal.,2022b),whilelarger
parameterstothecentralserver.
datasetsandothermodalitiesremainunexplored.
PoisoningAttacks. Dependingontheadversar-
4.3.2 PrivacyProtection
ialgoals,poisoningattacksinFLcanbeclassified
ProtectingprivacyinFM-FLrequiresbothdesign-
astargeted anduntargeted (Jereetal.,2020). Tar-
ingprotectivemeasuresandstudyingprivacyattack
getedattacks,likebackdoorattacks,aimtomanip-
strategies.
ulatetheglobalmodeltogenerateattacker-desired
Privacy-Preserving Techniques. Differential misclassificationsforsomeparticularsamples(Xie
Privacy(DP)isatheoreticalframeworkthatgov- et al., 2020; Bagdasaryan et al., 2020). In con-
erns privacy boundaries and manages the trade- trast,untargetedattacksseektodegradethemodel’s
offbetweenprivacyandmodelconvergence(Wei overallperformanceindiscriminately(Fangetal.,
et al., 2020; Xu et al., 2023). DP-based FL ap- 2020). Inadditiontothewell-recognizedattackson
proachesoftenaddartificialnoise(e.g.,Gaussian conventionalFLstudies(Lietal.,2023b,2024b),
noise)toparametersattheclients’sidebeforeag- FM-FL also faces potential threats from compro-
gregatingtopreventinformationleakage(Xuetal., misedpre-trainedFMs(Lietal.,2023c). Thus,The
2023). Besides,DPiscompatiblewithmostFed- attacker can introduce backdoors to downstream
PEFT methods. For instance, Sun et al. (2024b) taskswithoutpriorknowledge(Shenetal.,2021).
showed that DP noise can even be amplified by Specifically,Lietal.(2023d)proposedFed-EBD
thelocally“semi-quadratic”natureofLoRA-based that introduces a backdoor-compromised FM to
methods,motivatingtheintegrationofLoRAwith generateapublic,syntheticdatasetforFLtraining.
DPtoimproveresourceefficiencywhilemaintain- The clients’ models, pre-trained on this dataset,
ingdataprivacy(Liuetal.,2023c). Inadditionto inheritthebackdoorthroughoutthetraining.
DP,SecureMulti-PartyComputation(SMPC)(Mu-
Defense Techniques. As for defenses, robust
gunthan et al., 2019) and Homomorphic Encryp-
aggregation rules are widely applied to make an
tion (HE) (Zhang et al., 2020) are also effective
attack-resilientestimationofthetrueupdatesand
privacy-preservingmechanisms. However,theydo
excludetheinfluenceofmaliciousupdates(Blan-
notscalewellenoughforlarge-scaledeployments
chard et al., 2017; Yin et al., 2018; Chen et al.,
inFM-FL.
2017;Lietal.,2023a). Otherresearchdirections
Privacy Attack. Privacy attacks in FM-FL in- include trust-based strategies (Cao et al., 2021;
volveextractingsensitiveinformationfromthedata Xu et al., 2022; Park et al., 2021) and variance-
usedintraining,eventhoughthedataitselfisnot reduced algorithms (Gorbunov et al., 2023; Wu
directlyshared. Majorattacksincludemembership et al., 2020b). Although these techniques have
inference attack and data reconstruction attack, beenwidelyexaminedinvariousFLsettings,their
wheretheformeraimstodeterminewhetheraspe- effectivenesshasyettobeexploredintheFM-FL
cificdatasampleisinavictimclient’strainingset, paradigm.
and the latter strives to reconstruct original input
5 ApplicationsofFM-FL
datafromthemodelparametersorgradients(Ren
etal.,2024). Regardingmembershipinferenceat- Inthispart,webrieflyreviewtherecentprogress
tacks,Vuetal.(2024)revealedthevulnerabilities onFM-FLapplications. Table2listsrepresentativeTable2: AlistofrepresentativestudiesontheapplicationsofFM-FL.Abbreviations:LoRATuning(LT),AdapterTuning(AT),
Full-ParameterTuning(FT),SelectiveTuning(ST),PromptTuning(PT).
Domain/Application Task RepresentativeWork Modality Backbone
LanguageUnderstanding FedKC(Wangetal.,2022) (cid:37) (cid:37) Txt. mBERT FT
Multi-Tasks PMMFL(Welleretal.,2022) (cid:37) (cid:37) Txt. mBERT FT
MultilingualNLP MachineTranslation Fed-MNMT(Liuetal.,2023d) (cid:37) (cid:37) Txt. mBART-50 AT
MachineTranslation FL-MetaSend(Chuetal.,2024) (cid:37) (cid:37) Txt. M2M-100 ST
Multi-Tasks MFPT(Zhaoetal.,2024b) (cid:33) (cid:37) Txt. XLM-RoBERTa PT
Speech-to-Text pFedS2T(Duetal.,2024) (cid:37) (cid:33) Aud. Conformer/Whisper LT
Speech
SpeechRecognition FedASR(Jiaetal.,2023) (cid:33) (cid:33) Aud. RNN-T AT
SpeechRecognition FedE2EASR(Azametal.,2023a) (cid:37) (cid:33) Aud. CTC-AED FT
General PPLR(Zhaoetal.,2024a) (cid:37) (cid:33) Txt. LLaMA-7B/LongFormer FT
Recommendation General TransFR(Zhangetal.,2024a) (cid:33) (cid:33) Txt. DistBERT AT
General GPT-FedRec(Zengetal.,2024) (cid:37) (cid:37) Txt. ChatGPT NA
MentalHealthPrediction FedTherapist(Shinetal.,2023a) (cid:33) (cid:37) Txt. BERT&LLaMa-7B LT
Healthcare
MRIReconstruction FedPR(Fengetal.,2023a) (cid:37) (cid:37) Vis. SwinTransformers PT
workonspecificapplicationsanddomains. Consideringtheadverseeffectofconflictingpa-
rametersfromdiverselanguagesduringfederated
5.1 FM-FLforMultilingualNLP
fine-tuning,recentstudieshaveexploitedclustering
MultilingualNLPreferstothetechniquesthathan- strategiestoalleviatethisissue. Forinstance,Wang
dlemultiplenaturallanguages(Piresetal.,2019), et al. (2022) applied k-means clustering on each
oftentoperformequallywellacrossthem(Wuand client’s data to obtain representative knowledge,
Dredze, 2020). Earlier research (Johnson et al., specifically the clustered data centroids. These
2017)hasshownthatparametersharingamongdif- centroids were then shared across clients for lo-
ferentlanguagesbooststhemodel’sperformance cal training, enriching training data and address-
in multilingual NLP, especially for low-resource ingthechallengesassociatedwithdataheterogene-
languages for which significantly less content is ity. Another compelling strategy along this line
available. However, real-world multilingual text is language family-based clustering. Liu et al.
dataisoftendistributedacrossdevicesorregions, (2023d) explored various clustering strategies to
witheachclient(user)accessingonlyalimitedsub- groupadapterparameterstomitigatethenegative
set of languages, where transferring the data to a effectsofmultilingualdataheterogeneity,showing
central server is often problematic or prohibited thatlanguagefamily-basedclusteringsignificantly
duetoprivacyissues(Wangetal.,2022). Thanks outperformstheotherclusteringstrategies. Simi-
toitsinherentprivacy-preservingcharacteristic,FL larly,Guoetal.(2024b)proposedfine-tuningFMs
holds promise in breaking the barriers of cross- withLoRAandlanguagefamily-basedclustering
lingual modeling and data isolation by allowing toaddresstheheterogeneityissueofmultilingual
modelstolearnfromdecentralizeddatasets. modeling.
The pioneer work by Weller et al. (2022) has General downstream tasks include language
firstlydemonstratedthatfine-tuningpre-trainedlan- modeling (Wang et al., 2022), machine transla-
guagemodelswithFLcanperformsimilarlytopre- tion(Liuetal.,2023d;Chuetal.,2024),andtext
trained models fine-tuned with the standard cen- classification (Weller et al., 2022). In addition,
tralized method under multilingual NLP settings. somestudiesalsofocusonmorespecificapplica-
Varioussubsequentstudieshavefocusedonadapt- tionssuchasmedicaltranscriptanalysis(Manoel
ingpre-trainedFMsthroughFedPEFTtechniques et al., 2023) and hate speech detection (Akshay
suchasadaptertuning(Liuetal.,2023d),prompt andRahul,2024). Theseadvancementsillustrate
tuning(Zhaoetal.,2024b),andLoRA(Guoetal., theapplicabilityofFM-FLacrossawiderangeof
2024b),aimingtoenhancetrainingefficiency. scenariosinmultilingualNLP.
eciveD-nO
noitazilanosreP
gninuT-eniF5.2 FM-FLforSpeech as adapter tuning (Zhang et al., 2024a) and split
learning (Zhao et al., 2024a) can be employed to
WiththedevelopmentofAI,researchershavealso
improve resource efficiency. Apart from parame-
carried out many studies on speech-related FMs,
ter fine-tuning, LLMs can also be adapted to as-
e.g.,wav2vec2.0(Baevskietal.,2020)andWhis-
sist the recommendation in a zero-shot paradigm
per (Radford et al., 2023). In this field, the adap-
throughpromptengineering(i.e.,withoutparame-
tation of FMs often relies on FL to facilitate sce-
tertuning)(Gaoetal.,2023). Forexample,Zeng
narios where the audio data is privacy-sensitive.
et al. (2024) proposed GPT-FedRec, a two-stage
Comparedtootherdatamodalities,speech-related
FRframeworkthatleveragesChatGPTforitspow-
FM-FLapplicationsespeciallyattractexcessiveat-
erfulzero-shotgeneralizationability. Firstly,GPT-
tention to the aspects of on-device training and
FedRec facilitates hybrid retrieval by collabora-
personalization,motivatedbythefollowingcon-
tivelytrainingIDandtextretrievers,afterwhichthe
siderations: (1)Audiodataiscontinuallygenerated
retrievedresultsaretransformedintotextprompts
onend-devicessuchasmobilephones,andowned
andsubmittedtoGPTforre-rankinginthesecond
by individual users—thus it should be processed
stage. Additionally,Guoetal.(2024a)employeda
locally,ratherthanbeingtransferredelsewhere;(2)
pre-trainedBERTtoobtaintherepresentationvec-
AlthoughFLtakesadvantageofalluserdatatocol-
torsofitemdescriptions,whicharethenfedintoa
lectivelytrainonemodelthatmaximizesspeaker-
recommendersystemasaugmentedinput.
independentaccuracy,suchaone-model-fits-allso-
lutioncanbesub-optimalforindividualusers(Jia
5.4 FM-FLforHealthcare
etal.,2023). SpecifictasksinthisfieldincludeAu-
tomatic Speech Recognition (ASR) (Azam et al.,
FMs,especiallyLLMs,havebeenfoundtoexcelin
2023b)andSpeech-to-Text(S2T)(Duetal.,2024).
healthcareapplications,showcasingimpressiveca-
pabilitiesintaskslikementalhealthanalysis(Yang
5.3 FM-FLforRecommendation
etal.,2023b),diseasediagnosis(Panagouliasetal.,
Federated Recommendation (FR) strives to cap- 2024), and drug discovery (Chenthamarakshan
tureunderlyinguserpreferencesandrecommend etal.,2023). However, itraisesprivacyconcerns
appropriateinformationtouserswhilesafeguard- touploadthehealthinformationofpatients(Tang
ing data privacy (Bobadilla et al., 2013; Zhang et al., 2023) into a commercial server that sup-
et al., 2023a). Typical FR systems consist of a ports the FMs. Meanwhile, FL has consistently
serverandmultipleclients,whereclientsrepresent receivedwidespreadattentioninthehealthcaredo-
individual users or local data servers possessing main (Lincy and Kowshalya, 2020; Rieke et al.,
smallerdatasetsandretainingprivateuserinforma- 2020; Joshi et al., 2022), driven by the need for
tion(Ammad-Ud-Dinetal.,2019). Theseclients collaborativemodeltrainingacrossdifferentmedi-
collaboratetotrainaglobalmodelwhileensuring calinstitutionswithoutcompromisingpatientdata
their data privacy protection by abstaining from privacy. By breaking the barriers of private data
directdatasharing(Zengetal.,2024;Zhangetal., availability, the FM-FL paradigm shows the po-
2023a). Recently, LLM-based recommendations tentialtofurtherharnessthepowerofFMsinthe
havebeengainingincreasingattention(Wuetal., healthcaredomain.
2023b) due to their strong capacities in language A recent study (Shin et al., 2023a) presents a
understandinganddomaingeneralization. Theben- mobilementalhealthmonitoringsystem,FedTher-
efits are mainly twofold: (1) LLMs mitigate the apist,whichleveragesuserspeechandkeyboardin-
cold-startissuebyutilizingtextualdescriptionsto puttofine-tuneFMswithFL,demonstratingsupe-
makerecommendationswithouttheneedforexten- rioraccuracyinmentalhealthpredictiontaskssuch
sivehistoricaldata(Zhangetal.,2023c);(2)The asdepression,stress,andmoodprediction. Another
inherenttransferabilityofLLMsallowsthemtoap- representativestudy(Fengetal.,2023a)focuseson
plycross-domainknowledgeandsideinformation Magnetic Resonance Imaging (MRI) reconstruc-
toimproveaccuracyandrelevanceacrossdiverse tion, whichinvolvesretrievingacomplex-valued
itemsanduserinterests(Gaoetal.,2023). imagefromitsunder-sampledsignal. Theauthors
One straightforward way to adapt FMs for FR adoptedanFMpre-trainedonpublicdatasetsand
is by fine-tuning them with historical user-item trainedvisualpromptsfromdecentralizedclinical
data. Morespecifically,FedPEFTtechniquessuch datasetsviaapersonalizedFLmechanism,therebyreducingcommunicationcostsandachievingcom- vanced generative FMs (Wu et al., 2023a). The
petitiveperformanceonlimitedlocaldata. stronggenerativecapabilityofFMsoffersthead-
Despitetheefforts,ithasbeenshownthatFMsin vantageofrapidlyautomatingthecreationofinex-
healthcareriskgeneratingmisleadinginformation haustiblesyntheticdata. Thiscapabilitypositions
due to their imperfect understanding of complex AIGCasavaluablesupplementarydatasourcefor
medicaldata(Jeblicketal.,2024). model training and evaluation in many tasks (Xu
etal.,2024c). Despitesomeefforts(Zhangetal.,
6 FutureDirections
2023b), more potential opportunities and chal-
lengesforAIGC-aidedFLhaveyettobeexplored.
Althoughrecentworkhasalreadybeguntoaddress
thechallengesdiscussedinSection3.2,manycrit- 7 Conclusions
ical open directions are yet to be explored. Here,
Inthissurvey,wehavemeticulouslysurveyedthe
weoutlineseveralrepresentativeones.
intersectionofFMandFL.Weidentifiedcorechal-
MultimodalFM-FL. Withthedevelopmentof lengesinefficiency,adaptability,andtrustworthi-
mobiletechnologyandIoTinfrastructures(Brunete nessandproposedacomprehensivetaxonomyof
etal.,2021),numerousedgedevicesproducedata techniquesinresponsetothesechallenges. Inad-
fromarangeofmodalities,suchassensory,visual, dition,wediscussedfuturedirectionsandapplica-
andaudio. IntheeraofFMs,thesuccessofLLMs tionsinthisresearchfield,hopingtoattractmore
and their multimodal derivatives (Ramesh et al., breakthroughsinfutureresearch.
2021;Google,2023;OpenAI,2024)havedemon-
Limitations
stratedthepotentialofmultimodalFMs. Thepo-
tentialopportunitiesandchallengesformultimodal FMandFLareveryfast-movingfields. Wehave
FM-FLhaveyettobeexplored. putalotofeffortintoincludingthelatestresearch
efforts in the community in this survey. There-
ContinualLearning. Continuallearningenables
fore,webelievethatoursurveywillhelptoinspire
modelstoadapttonewdataovertime,improving
andpushfurtherresearchandinnovationinthese
their performance and accuracy. By incorporat-
importantareas. Oursurveydoesnotfocusonex-
ing new data into the model training process, FL
perimental evaluation of the available ideas and
and FMs can continuously improve and adapt to
systems. We believe that would be an important
changingenvironmentsanduserneeds(Yangetal.,
nextstepthatweareleavingforfuturework.
2024a). Futuredirectionsmayinvolveleveraging
transferlearningtechniquesincontinuallearning
forFLandFMs. Modelscantransferknowledge References
from previous tasks or domains to new ones, en-
Durmus Alp Emre Acar, Yue Zhao, Ramon Matas,
ablingmoreefficientadaptation(Goodetal.,2023). MatthewMattina,PaulWhatmough,andVenkatesh
Saligrama. 2021. Federated learning based on dy-
EfficientFederatedBlack-BoxTuning. Insce- namicregularization. InInternationalConferenceon
narioswheregradientaccessisunavailable,prelim- LearningRepresentations.
inaryeffortshavefocusedonfederatedfine-tuning
Yossi Adi, Carsten Baum, Moustapha Cisse, Benny
black-boxFMs(Linetal.,2023;Sunetal.,2024a; Pinkas,andJosephKeshet.2018. Turningyourweak-
Lu et al., 2023b; Rui et al., 2024) utilizing ZOO. nessintoastrength: Watermarkingdeepneuralnet-
However, ZOO’s noticeably slower convergence works by backdooring. In 27th USENIX Security
Symposium(USENIXSecurity18),pages1615–1631,
rates,especiallyinhigh-dimensionalcontextscom-
Baltimore,MD.USENIXAssociation.
pared to gradient-based methods (Golovin et al.,
2020), indicateanimportantdirectionforfurther Armen Aghajanyan, Sonal Gupta, and Luke Zettle-
moyer.2021. Intrinsicdimensionalityexplainsthe
research. Theimpactoftheseslowerconvergence
effectivenessoflanguagemodelfine-tuning. InPro-
ratesonoverallefficiencyandcomputationalload
ceedingsofthe59thAnnualMeetingoftheAssocia-
withinFL,particularlyconcerninglarge-scaleFMs, tionforComputationalLinguisticsandthe11thInter-
has not been adequately investigated and under- nationalJointConferenceonNaturalLanguagePro-
cessing(Volume1: LongPapers),pages7319–7328,
stood.
Online.AssociationforComputationalLinguistics.
FLwithAI-GeneratedContent. AI-Generated
SinghAkshayandThakurRahul.2024. Generalizable
Content(AIGC)denotescontentproducedviaad- multilingualhatespeechdetectiononlowresourceindian languages using fair selection in federated EladBenZaken,YoavGoldberg,andShauliRavfogel.
learning. In Proceedings of the 2024 Conference 2022. BitFit: Simpleparameter-efficientfine-tuning
of the North American Chapter of the Association fortransformer-basedmaskedlanguage-models. In
for Computational Linguistics: Human Language Proceedings of the60th Annual Meeting of the As-
Technologies. sociationforComputationalLinguistics(Volume2:
ShortPapers),pages1–9,Dublin,Ireland.Associa-
Muhammad Ammad-Ud-Din, Elena Ivannikova, tionforComputationalLinguistics.
Suleiman A. Khan, Were Oyomno, Qiang Fu,
KuanEeikTan,andAdrianFlanagan.2019. Feder-
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guer-
ated collaborative filtering for privacy-preserving
raoui, and Julien Stainer. 2017. Machine learning
personalized recommendation system. arXiv
withadversaries: Byzantinetolerantgradientdescent.
preprintarXiv:1901.09888.
InProceedingsofthe31stInternationalConference
onNeuralInformationProcessingSystems.
SheikhShamsAzam, TatianaLikhomanenko, Martin
Pelikan, andJan“Honza”Silovsky.2023a. Impor-
J.Bobadilla,F.Ortega,A.Hernando,andA.Gutiérrez.
tanceofsmoothnessinducedbyoptimizersinfl4asr:
2013. Recommendersystemssurvey. Knowledge-
Towardsunderstandingfederatedlearningforend-to-
BasedSystems,46:109–132.
endasr. In2023IEEEAutomaticSpeechRecognition
andUnderstandingWorkshop(ASRU),pages1–8.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli,
SheikhShamsAzam,MartinPelikan,VitalyFeldman, Russ Altman, Simran Arora, Sydney von Arx,
Kunal Talwar, Jan Silovsky, and Tatiana Likhoma- MichaelSBernstein,JeannetteBohg,AntoineBosse-
nenko.2023b. Federatedlearningforspeechrecog- lut,EmmaBrunskill,etal.2021. Ontheopportuni-
nition: Revisitingcurrenttrendstowardslarge-scale tiesandrisksoffoundationmodels. arXivpreprint
ASR. InInternationalWorkshoponFederatedLearn- arXiv:2108.07258.
ingintheAgeofFoundationModelsinConjunction
withNeurIPS2023. KallistaBonawitz,PeterKairouz,BrendanMcMahan,
andDanielRamage.2021. Federatedlearningand
Sara Babakniya, Ahmed Elkordy, Yahya Ezzeldin,
privacy: Buildingprivacy-preservingsystemsforma-
Qingfeng Liu, Kee-Bong Song, MOSTAFA EL-
chinelearninganddatascienceondecentralizeddata.
Khamy, and Salman Avestimehr. 2023a. SLoRA:
Queue,19(5):87–114.
Federatedparameterefficientfine-tuningoflanguage
models. In International Workshop on Federated
Tom Brown et al. 2020. Language models are few-
LearningintheAgeofFoundationModelsinCon-
shot learners. In Advances in Neural Information
junctionwithNeurIPS2023.
Processing Systems, volume 33, pages 1877–1901.
CurranAssociates,Inc.
SaraBabakniya,SouvikKundu,SauravPrakash,Yue
Niu,andSalmanAvestimehr.2023b. Revisitingspar-
sityhuntinginfederatedlearning: Whydoessparsity AlbertoBrunete,ErnestoGambao,MiguelHernando,
consensusmatter? TransactionsonMachineLearn- andRaquelCedazo.2021. Smartassistivearchitec-
ingResearch. ture for the integration of iot devices, robotic sys-
tems,andmultimodalinterfacesinhealthcareenvi-
AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed, ronments. Sensors,21(6):2212.
andMichaelAuli.2020. wav2vec2.0: Aframework
forself-supervisedlearningofspeechrepresentations. Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George
InAdvancesinNeuralInformationProcessingSys- Karypis.2022. Differentiallyprivatebias-termonly
tems,volume33,pages12449–12460.CurranAsso- fine-tuning of foundation models. In Workshop
ciates,Inc. on Trustworthy and Socially Responsible Machine
Learning,NeurIPS2022.
EugeneBagdasaryan,AndreasVeit,YiqingHua,Deb-
orah Estrin, and Vitaly Shmatikov. 2020. How to
SébastienBubeck,VarunChandrasekaran,RonenEldan,
backdoorfederatedlearning. InInternationalCon-
Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter
ferenceonArtificialIntelligenceandStatistics,pages
Lee,YinTatLee,YuanzhiLi,ScottLundberg,Har-
2938–2948.PMLR.
shaNori,HamidPalangi,MarcoTulioRibeiro,and
YiZhang.2023. Sparksofartificialgeneralintelli-
JiamuBai,DaoyuanChen,BingchenQian,LiuyiYao,
gence: Earlyexperimentswithgpt-4. arXivpreprint
and Yaliang Li. 2024a. Federated fine-tuning of
arXiv:2303.12712.
large language models under heterogeneous lan-
guage tasks and client resources. arXiv preprint
arXiv:2402.11505. DongqiCai,YaozongWu,ShangguangWang,FelixXi-
aozhu Lin, and Mengwei Xu. 2023. Efficient fed-
SikaiBai,JieZhang,ShuaichengLi,SongGuo,Jingcai erated learning for modern nlp. In Proceedings of
Guo,JunHou,TaoHan,andXiaochengLu.2024b. the29thAnnualInternationalConferenceonMobile
Diprompt: Disentangledprompttuningformultiple Computing and Networking, ACM MobiCom ’23,
latent domain generalization in federated learning. New York, NY, USA. Association for Computing
arXivpreprintarXiv:2403.08506. Machinery.XiaoyuCao,MinghongFang,JiaLiu,andNeilZhen- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
qiangGong.2021. Fltrust: Byzantine-robustfeder- Kristina Toutanova. 2019. BERT: Pre-training of
atedlearningviatrustbootstrapping. InISOCNet- deepbidirectionaltransformersforlanguageunder-
work and Distributed System Security Symposium standing. InProceedingsofthe2019Conferenceof
(NDSS). theNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTech-
CCPA.2023. Californiaconsumerprivacyact(ccpa). nologies,Volume1(LongandShortPapers),pages
4171–4186,Minneapolis,Minnesota.Associationfor
PushpitaChatterjee,DebashisDas,andDandaBRawat. ComputationalLinguistics.
2023. Useoffederatedlearningandblockchainto-
wards securing financial services. arXiv preprint EnmaoDiao,JieDing,andVahidTarokh.2021. Het-
arXiv:2303.12944. ero{fl}: Computationandcommunicationefficient
federatedlearningforheterogeneousclients. InInter-
HaokunChen, YaoZhang, DenisKrompass, Jindong nationalConferenceonLearningRepresentations.
Gu,andVolkerTresp.2024. Feddat:Anapproachfor
foundationmodelfinetuninginmulti-modalhetero- Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei,
geneousfederatedlearning. ProceedingsoftheAAAI Zonghan Yang, Yusheng Su, Shengding Hu, Yulin
ConferenceonArtificialIntelligence,38(10):11285–
Chen, Chi-Min Chan, Weize Chen, et al. 2023.
11293. Parameter-efficient fine-tuning of large-scale pre-
trained language models. Nature Machine Intelli-
Yudong Chen, Lili Su, and Jiaming Xu. 2017. Dis- gence,5(3):220–235.
tributed statistical machine learning in adversarial
settings: Byzantinegradientdescent. Proceedingsof
ChenheDong,YuexiangXie,BolinDing,YingShen,
theACMonMeasurementandAnalysisofComputing
andYaliangLi.2023. Tunablesoftpromptsaremes-
Systems.
sengers in federated learning. In Findings of the
AssociationforComputationalLinguistics: EMNLP
VijilChenthamarakshan,SamuelC.Hoffman,C.David
2023,pages14665–14675,Singapore.Association
Owen,PetraLukacik,ClaireStrain-Damerell,Daren
forComputationalLinguistics.
Fearon, Tika R. Malla, Anthony Tumber, Christo-
pher J. Schofield, Helen M.E. Duyvesteyn, Wan-
Alexey Dosovitskiy, Lucas Beyer, Alexander
wisa Dejnirattisai, Loic Carrique, Thomas S. Wal-
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
ter,GavinR.Screaton,TetianaMatviiuk,Aleksandra
Thomas Unterthiner, Mostafa Dehghani, Matthias
Mojsilovic,JasonCrain,MartinA.Walsh,DavidI.
Minderer, Georg Heigold, Sylvain Gelly, Jakob
Stuart,andPayelDas.2023. Acceleratingdrugtarget
Uszkoreit, and Neil Houlsby. 2021. An image
inhibitordiscoverywithadeepgenerativefoundation
is worth 16x16 words: Transformers for image
model. ScienceAdvances,9(25):eadg7865.
recognitionatscale. InInternationalConferenceon
LearningRepresentations.
YaeJeeCho,LuyangLiu,ZhengXu,AldiFahrezi,and
GauriJoshi.2024. Heterogeneousloraforfederated
Yichao Du, Zhirui Zhang, Linan Yue, Xu Huang,
fine-tuningofon-devicefoundationmodels. arXiv
YuqingZhang,TongXu,LinliXu,andEnhongChen.
preprintarXiv:2401.06432.
2024. Communication-efficientpersonalizedfeder-
ated learning for speech-to-text tasks. In ICASSP
Yun-WeiChu,Dong-JunHan,andChristopherG.Brin-
2024 - 2024 IEEE International Conference on
ton. 2024. Only send what you need: Learning to
Acoustics,SpeechandSignalProcessing(ICASSP),
communicateefficientlyinfederatedmultilingualma-
pages10001–10005.
chinetranslation. InCompanionProceedingsofthe
ACM on Web Conference 2024, page 1548–1557,
New York, NY, USA. Association for Computing JohnCDuchi,MichaelIJordan,MartinJWainwright,
Machinery. andAndreWibisono.2015. Optimalratesforzero-
orderconvexoptimization: Thepoweroftwofunc-
AlexisConneau,KartikayKhandelwal,NamanGoyal, tionevaluations. IEEETransactionsonInformation
Vishrav Chaudhary, Guillaume Wenzek, Francisco Theory,61(5):2788–2806.
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer,andVeselinStoyanov.2020. Unsupervised TaoFan,YanKang,GuoqiangMa,WeijingChen,Wen-
cross-lingualrepresentationlearningatscale. InPro- bin Wei, Lixin Fan, and Qiang Yang. 2023. Fate-
ceedings of the 58th Annual Meeting of the Asso- llm: A industrial grade federated learning frame-
ciationforComputationalLinguistics,pages8440– work for large language models. arXiv preprint
8451, Online. Association for Computational Lin- arXiv:2310.10049.
guistics.
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil
WenlongDeng,ChristosThrampoulidis,andXiaoxiao Gong. 2020. Local model poisoning attacks to
Li.2024. Unlockingthepotentialofprompt-tuning byzantine-robustfederatedlearning. In29thUSENIX
inbridginggeneralizedandpersonalizedfederated Security Symposium (USENIX Security 20), pages
learning. CVPR. 1605–1622.Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Eduard Gorbunov, Samuel Horváth, Peter Richtárik,
Shi, Colin N. Jones, and Yong Zhou. 2022. andGauthierGidel.2023. Variancereductionisan
Communication-efficientstochasticzeroth-orderop- antidotetobyzantines: Betterrates,weakerassump-
timizationforfederatedlearning. IEEETransactions tions and communication compression as a cherry
onSignalProcessing,70:5058–5073. onthetop. InInternationalConferenceonLearning
Representations.
FedML.2023. Fedllm.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
Chun-Mei Feng, Bangjun Li, Xinxing Xu, Yong Liu, CésarTeodoroMendes,AllieDelGiorno,Sivakanth
HuazhuFu,andWangmengZuo.2023a. Learning Gopi,MojanJavaheripi,PieroKauffmann,Gustavo
federatedvisualpromptinnullspaceformrirecon- de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,
struction. In2023IEEE/CVFConferenceonCom- HarkiratSinghBehl,XinWang,SébastienBubeck,
puterVisionandPatternRecognition(CVPR),pages RonenEldan,AdamTaumanKalai,YinTatLee,and
8064–8073. YuanzhiLi.2023. Textbooksareallyouneed. arXiv
preprintarXiv:2306.11644.
Haozhe Feng, Tianyu Pang, Chao Du, Wei Chen,
Shuicheng Yan, and Min Lin. 2023b. Does feder- Lei Guo, Ziang Lu, Junliang Yu, Quoc Viet Hung
ated learning really need backpropagation? arXiv Nguyen,andHongzhiYin.2024a. Prompt-enhanced
preprintarXiv:2301.12195. federatedcontentrepresentationlearningforcross-
domain recommendation. In Proceedings of the
Xiachong Feng, Xiaocheng Feng, Xiyuan Du, Min- ACM on Web Conference 2024, WWW ’24, page
Yen Kan, and Bing Qin. 2023c. Adapter-based se- 3139–3149, New York, NY, USA. Association for
lective knowledge distillation for federated multi- ComputingMachinery.
domain meeting summarization. arXiv preprint
arXiv:2308.03275. TaoGuo,SongGuo,JunxiaoWang,XueyangTang,and
WenchaoXu.2023. Promptfl: Letfederatedpartici-
JonathanFrankleandMichaelCarbin.2019. Thelottery pantscooperativelylearnpromptsinsteadofmodels-
ticket hypothesis: Finding sparse, trainable neural federatedlearninginageoffoundationmodel. IEEE
networks. InInternationalConferenceonLearning TransactionsonMobileComputing,pages1–15.
Representations.
XuGuoandHanYu.2022. Onthedomainadaptation
Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, andgeneralizationofpretrainedlanguagemodels: A
Haofen Wang, and Jiawei Zhang. 2023. Chat- survey. arXivpreprintarXiv:2211.03154.
rec: Towards interactive and explainable llms-
augmented recommender system. arXiv preprint Zhihan Guo, Yifei Zhang, Zhuo Zhang, Zenglin Xu,
arXiv:2303.14524. andIrwinKing.2024b. FedLFC:Towardsefficient
federatedmultilingualmodelingwithLoRA-based
GDPR.2016. Regulation(eu)2016/679oftheeuropean languagefamilyclustering. InFindingsoftheAsso-
parliamentandofthecouncil. ciationforComputationalLinguistics: NAACL2024,
pages1519–1528,MexicoCity,Mexico.Association
JonasGeiping,HartmutBauermeister,HannahDröge, forComputationalLinguistics.
andMichaelMoeller.2020. Invertinggradients-how
easyisittobreakprivacyinfederatedlearning? In Samyak Gupta, Yangsibo Huang, Zexuan Zhong,
AdvancesinNeuralInformationProcessingSystems, TianyuGao,KaiLi,andDanqiChen.2022. Recov-
volume33,pages16937–16947.CurranAssociates, eringprivatetextinfederatedlearningoflanguage
Inc. models. InAdvancesinNeuralInformationProcess-
ingSystems,volume35,pages8130–8143.Curran
DanielGolovin,JohnKarro,GregKochanski,Chansoo Associates,Inc.
Lee,XingyouSong,andQiuyiZhang.2020. Gradi-
entlessdescent: High-dimensionalzeroth-orderopti- Suchin Gururangan, Ana Marasovic´, Swabha
mization. InInternationalConferenceonLearning Swayamdipta,KyleLo,IzBeltagy,DougDowney,
Representations. and Noah A. Smith. 2020. Don’t stop pretraining:
Adapt language models to domains and tasks. In
Jack Good, Jimit Majmudar, Christophe Dupuy, Jix- Proceedings of the 58th Annual Meeting of the
uanWang,CharithPeris,ClementChung,Richard Association for Computational Linguistics, pages
Zemel,andRahulGupta.2023. Coordinatedreplay 8342–8360,Online.AssociationforComputational
sampleselectionforcontinualfederatedlearning. In Linguistics.
Proceedings of the 2023 Conference on Empirical
MethodsinNaturalLanguageProcessing: Industry DavidHa,AndrewM.Dai,andQuocV.Le.2017. Hy-
Track, pages 331–342, Singapore. Association for pernetworks. InInternationalConferenceonLearn-
ComputationalLinguistics. ingRepresentations.
Gemini Team Google. 2023. Gemini: a family of NikolausHansenandAndreasOstermeier.2001. Com-
highlycapablemultimodalmodels. arXivpreprint pletely derandomized self-adaptation in evolution
arXiv:2312.11805. strategies. EvolutionaryComputation,9(2):159–195.Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, YuangJiang,ShiqiangWang,VíctorValls,BongJunKo,
MiZhang,HongyiWang,XiaoyangWang,Praneeth Wei-HanLee,KinK.Leung,andLeandrosTassiulas.
Vepakomma,AbhishekSingh,HangQiu,Xinghua 2023c. Model pruning enables efficient federated
Zhu,JianzongWang,LiShen,PeilinZhao,YanKang, learningonedgedevices. IEEETransactionsonNeu-
YangLiu,RameshRaskar,QiangYang,MuraliAn- ralNetworksandLearningSystems,34(12):10374–
navaram,andSalmanAvestimehr.2020. Fedml: A 10386.
research library and benchmark for federated ma-
chinelearning. arXivpreprintarXiv:2007.13518. Eun Seo Jo and Timnit Gebru. 2020. Lessons from
archives: Strategiesforcollectingsocioculturaldata
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, in machine learning. In Proceedings of the 2020
Bruna Morrone, Quentin De Laroussilhe, Andrea ConferenceonFairness,Accountability,andTrans-
Gesmundo,MonaAttariyan,andSylvainGelly.2019. parency,FAcctT’20,pages306–316,NewYork,NY,
Parameter-efficient transfer learning for NLP. In USA.AssociationforComputingMachinery.
Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings MelvinJohnson, MikeSchuster, QuocV.Le, Maxim
of Machine Learning Research, pages 2790–2799. Krikun,YonghuiWu,ZhifengChen,NikhilThorat,
PMLR. FernandaViégas,MartinWattenberg,GregCorrado,
MacduffHughes,andJeffreyDean.2017. Google’s
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Multilingual Neural Machine Translation System:
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and EnablingZero-ShotTranslation. Transactionsofthe
WeizhuChen.2022. LoRA:Low-rankadaptationof Association for Computational Linguistics, 5:339–
largelanguagemodels. InInternationalConference 351.
onLearningRepresentations.
MadhuraJoshi,AnkitPal,andMalaikannanSankara-
subbu.2022. Federatedlearningforhealthcaredo-
XuminHuang,PeichunLi,HongyangDu,JiawenKang,
main-pipeline,applicationsandchallenges. ACM
Dusit Niyato, Dong In Kim, and Yuan Wu. 2024.
Trans.Comput.Healthcare,3(4).
Federatedlearning-empoweredai-generatedcontent
inwirelessnetworks. IEEENetwork,pages1–1.
PeterKairouzetal.2021. Advancesandopenproblems
in federated learning. Foundations and Trends in
Katharina Jeblick, Balthasar Schachtner, Jakob Dexl,
MachineLearning,14(1–2):1–210.
AndreasMittermeier,AnnaTheresaStüber,Johanna
Topalis,TobiasWeber,PhilippWesp,BastianOliver
YanKang,TaoFan,HanlinGu,XiaojinZhang,Lixin
Sabel, Jens Ricke, et al. 2024. Chatgpt makes
Fan,andQiangYang.2024. Groundingfoundation
medicineeasytoswallow: anexploratorycasestudy
modelsthroughfederatedtransferlearning: Agen-
onsimplifiedradiologyreports. Europeanradiology,
eralframework. arXivpreprintarXiv:2311.17431.
34(5):2817–2825.
YeachanKim,JunhoKim,Wing-LamMok,Jun-Hyung
MalharSJere,TylerFarnan,andFarinazKoushanfar.
Park,andSangKeunLee.2023. Client-customized
2020. Ataxonomyofattacksonfederatedlearning.
adaptationforparameter-efficientfederatedlearning.
IEEESecurity&Privacy,19(2).
In Findings of the Association for Computational
Linguistics: ACL2023,pages1159–1172,Toronto,
Junteng Jia, Ke Li, Mani Malek, Kshitiz Malik, Jay
Canada.AssociationforComputationalLinguistics.
Mahadeokar,OzlemKalinli,andFrankSeide.2023.
Jointfederatedlearningandpersonalizationforon-
AlexanderKirillov,EricMintun,NikhilaRavi,Hanzi
deviceasr. In2023IEEEAutomaticSpeechRecog-
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,
nitionandUnderstandingWorkshop(ASRU),pages
Spencer Whitehead, Alexander C Berg, Wan-Yen
1–8.
Lo, etal.2023. Segmentanything. arXivpreprint
arXiv:2304.02643.
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire
Cardie,SergeBelongie,BharathHariharan,andSer- Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan
NamLim.2022. Visualprompttuning. InComputer Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie,
Vision–ECCV2022,pages709–727,Cham.Springer Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.
NatureSwitzerland. Federatedscope-llm: Acomprehensivepackagefor
fine-tuninglargelanguagemodelsinfederatedlearn-
JingangJiang,XiangyangLiu,andChenyouFan.2023a. ing. arXivpreprintarXiv:2309.00363.
Low-parameter federated learning with large lan-
guagemodels. arXivpreprintarXiv:2307.13896. BrianLester,RamiAl-Rfou,andNoahConstant.2021.
The power of scale for parameter-efficient prompt
Lekang Jiang, Filip Svoboda, and Nicholas Donald tuning. InProceedingsofthe2021Conferenceon
Lane.2023b. FDAPT:Federateddomain-adaptive EmpiricalMethodsinNaturalLanguageProcessing,
pre-trainingforlanguagemodels. InInternational pages3045–3059,OnlineandPuntaCana,Domini-
WorkshoponFederatedLearningintheAgeofFoun- can Republic. Association for Computational Lin-
dationModelsinConjunctionwithNeurIPS2023. guistics.AngLi,JingweiSun,BinghuiWang,LinDuan,Sicheng Xi Li, Chen Wu, and Jiaqi Wang. 2024c. Unveil-
Li, Yiran Chen, and Hai Li. 2021a. Lotteryfl: ing backdoor risks brought by foundation models
Empower edge intelligence with personalized and in heterogeneous federated learning. In Advances
communication-efficientfederatedlearning. In2021 in Knowledge Discovery and Data Mining, pages
IEEE/ACMSymposiumonEdgeComputing(SEC), 168–181,Singapore.SpringerNatureSingapore.
pages68–79.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
GuanghaoLi,WansenWu,YanSun,LiShen,Baoyuan
Optimizing continuous prompts for generation. In
Wu,andDachengTao.2024a. Visualpromptbased
Proceedingsofthe59thAnnualMeetingoftheAsso-
personalized federated learning. Transactions on
ciationforComputationalLinguisticsandthe11th
MachineLearningResearch.
InternationalJointConferenceonNaturalLanguage
Processing (Volume 1: Long Papers), pages 4582–
JunnanLi,RamprasaathSelvaraju,AkhileshGotmare,
4597.AssociationforComputationalLinguistics.
ShafiqJoty,CaimingXiong,andStevenChuHong
Hoi.2021b. Alignbeforefuse: Visionandlanguage
Zan Li and Li Chen. 2021. Communication-efficient
representationlearningwithmomentumdistillation.
decentralizedzeroth-ordermethodonheterogeneous
InAdvancesinNeuralInformationProcessingSys-
data. In202113thInternationalConferenceonWire-
tems, volume 34, pages 9694–9705. Curran Asso-
lessCommunicationsandSignalProcessing(WCSP),
ciates,Inc.
pages1–6.
QinbinLi,YiqunDiao,QuanChen,andBingshengHe.
2022. Federatedlearningonnon-iiddatasilos: An Vladislav Lialin, Vijeta Deshpande, and Anna
experimentalstudy. In2022IEEE38thInternational Rumshisky.2023. Scalingdowntoscaleup: Aguide
ConferenceonDataEngineering(ICDE),pages965– to parameter-efficient fine-tuning. arXiv preprint
978. arXiv:2303.15647.
Shenghui Li, Edith Ngai, and Thiemo Voigt. 2023a. Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu
Byzantine-robustaggregationinfederatedlearning Huang, Li Shen, and Dacheng Tao. 2023. Effi-
empowered industrial iot. IEEE Transactions on cient federated prompt tuning for black-box large
IndustrialInformatics,19(2):1165–1175. pre-trainedmodels. CoRR,abs/2310.03123.
Shenghui Li, Edith Ngai, Fanghua Ye, Li Ju, Tianru
M Lincy and A Meena Kowshalya. 2020. Early de-
Zhang,andThiemoVoigt.2024b. Blades: Aunified
tection of type-2 diabetes using federated learning.
benchmarksuiteforbyzantineattacksanddefenses
International Journal ofScientific Researchin Sci-
infederatedlearning. In2024IEEE/ACMNinthIn-
ence,EngineeringandTechnology,12:257–267.
ternationalConferenceonInternet-of-ThingsDesign
andImplementation(IoTDI).
Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang
Shenghui Li, Edith C.-H. Ngai, and Thiemo Voigt. Li, and Ying Shen. 2024. On the convergence of
2023b. Anexperimentalstudyofbyzantine-robust zeroth-orderfederatedtuninginlargelanguagemod-
aggregation schemes in federated learning. IEEE els. arXivpreprintarXiv:2402.05926.
TransactionsonBigData,pages1–13.
ZhengyangLit, ShijingSit, JianzongWang, andJing
TianLi,AnitKumarSahu,ManzilZaheer,MaziarSan- Xiao.2022. Federatedsplitbertforheterogeneous
jabi, Ameet Talwalkar, and Virginia Smith. 2020. textclassification. In2022InternationalJointCon-
Federatedoptimizationinheterogeneousnetworks. ferenceonNeuralNetworks(IJCNN),pages1–8.
In Proceedings of Machine Learning and Systems,
volume2,pages429–450.
PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,
HiroakiHayashi,andGrahamNeubig.2023a. Pre-
XiLiandJiaqiWang.2024. Positionpaper: Assessing
train, prompt, and predict: A systematic survey of
robustness,privacy,andfairnessinfederatedlearning
promptingmethodsinnaturallanguageprocessing.
integrated with foundation models. arXiv preprint
ACMComput.Surv.,55(9).
arXiv:2402.01857.
Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan
XiLi, SongheWang, ChenWu, HaoZhou, andJiaqi
Zhang, Alfred O. Hero III, and Pramod K. Varsh-
Wang.2023c. Backdoorthreatsfromcompromised
ney. 2020. A primer on zeroth-order optimization
foundationmodelstofederatedlearning. InInterna-
in signal processing and machine learning: Princi-
tionalWorkshoponFederatedLearningintheAge
pals,recentadvances,andapplications. IEEESignal
ofFoundationModelsinConjunctionwithNeurIPS
ProcessingMagazine,37(5):43–54.
2023.
Xi Li, Chen Wu, and Jiaqi Wang. 2023d. Unveil- TaoLiu,ZhiWang,HuiHe,WeiShi,LiangliangLin,
ing backdoor risks brought by foundation models RanAn,andChenhaoLi.2023b. Efficientandsecure
inheterogeneousfederatedlearning. arXivpreprint federatedlearningforfinancialapplications. Applied
arXiv:2311.18350. Sciences,13(10).Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Andrea Manoel, Mirian del Carmen Hipolito Garcia,
Gao, Shan Zhong, and Meikang Qiu. 2023c. Dif- TalBaumel,ShizeSu,JialeiChen,RobertSim,Dan
ferentiallyprivatelow-rankadaptationoflargelan- Miller, Danny Karmon, and Dimitrios Dimitriadis.
guagemodelusingfederatedlearning. arXivpreprint 2023. Federated multilingual models for medical
arXiv:2312.17493. transcriptanalysis. InProceedingsoftheConference
onHealth,Inference,andLearning,volume209of
YiLiu,XiaohanBi,LeiLi,SishuoChen,WenkaiYang, ProceedingsofMachineLearningResearch,pages
andXuSun.2023d. Communicationefficientfeder- 147–162.PMLR.
atedlearningformultilingualneuralmachinetrans-
lationwithadapter. InFindingsoftheAssociation AlessioMaritan,SubhrakantiDey,andLucaSchenato.
for Computational Linguistics: ACL 2023, pages 2023. Fedzen: Towardssuperlinearzeroth-orderfed-
5315–5328,Toronto,Canada.AssociationforCom- erated learning via incremental hessian estimation.
putationalLinguistics. arXivpreprintarXiv:2309.17174.
Brendan McMahan, Eider Moore, Daniel Ramage,
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
Seth Hampson, and Blaise Aguera y Arcas. 2017.
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Communication-efficientlearningofdeepnetworks
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
fromdecentralizeddata. InArtificialintelligenceand
Roberta: A robustly optimized bert pretraining ap-
statistics,pages1273–1282.PMLR.
proach. arXivpreprintarXiv:1907.11692.
VaikkunthMugunthan,AntigoniPolychroniadou,David
WangLu,XixuHu,JindongWang,andXingXie.2023a.
Byrd,andTuckerHybinetteBalch.2019. Smpai: Se-
Fedclip: Fastgeneralizationandpersonalizationfor
curemulti-partycomputationforfederatedlearning.
CLIPinfederatedlearning. IEEEDataEng.Bull.,
In Proceedings of the NeurIPS 2019 Workshop on
46(1):52–66.
Robust AI in Financial Services, volume 21. MIT
PressCambridge,MA,USA.
WangLu,HaoYu,JindongWang,DamienTeney,Hao-
han Wang, Yiqiang Chen, Qiang Yang, Xing Xie, DuyPhuongNguyen,J.PabloMunoz,andAliJannesari.
andXiangyangJi.2023b. Zoopfl: Exploringblack- 2024. Flora:Enhancingvision-languagemodelswith
box foundation models for personalized federated parameter-efficientfederatedlearning. arXivpreprint
learning. arXivpreprintarXiv:2310.05143. arXiv:2404.15182.
LingjuanLyu,HanYu,XingjunMa,ChenChen,Lichao OpenAI.2022. Chatgpt.
Sun,JunZhao,QiangYang,andPhilipS.Yu.2022.
Privacyandrobustnessinfederatedlearning: Attacks OpenAI.2024. Gpt-4technicalreport. arXivpreprint
and defenses. IEEE Transactions on Neural Net- arXiv:2303.08774.
worksandLearningSystems,pages1–21.
DimitriosP.Panagoulias,MariaVirvou,andGeorgeA.
Tsihrintzis.2024. Evaluatingllm–generatedmulti-
ShubhamMalaviya,ManishShukla,andSachinLodha.
modaldiagnosisfrommedicalimagesandsymptom
2023. Reducingcommunicationoverheadinfeder-
analysis. arXivpreprintarXiv:2402.01730.
atedlearningforpre-trainedlanguagemodelsusing
parameter-efficientfinetuning. InProceedingsofThe
Sharnil Pandya, Gautam Srivastava, Rutvij Jhaveri,
2ndConferenceonLifelongLearningAgents,volume
M.RajasekharaBabu,SwetaBhattacharya,Praveen
232ofProceedingsofMachineLearningResearch,
Kumar Reddy Maddikunta, Spyridon Mastorakis,
pages456–469.PMLR.
Md.JalilPiran,andThippaReddyGadekallu.2023.
Federated learning for smart cities: A comprehen-
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex
sive survey. Sustainable Energy Technologies and
Damian, Jason D. Lee, Danqi Chen, and Sanjeev
Assessments,55:102987.
Arora.2023a. Fine-tuninglanguagemodelswithjust
forwardpasses. InWorkshoponEfficientSystemsfor
Jungwuk Park, Dong-Jun Han, Minseok Choi, and
FoundationModels@ICML2023.
JaekyunMoon.2021. Sageflow: Robustfederated
learningagainstbothstragglersandadversaries. In
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex
AdvancesinNeuralInformationProcessingSystems,
Damian, Jason D Lee, Danqi Chen, and Sanjeev
volume34,pages840–851.CurranAssociates,Inc.
Arora.2023b. Fine-tuninglanguagemodelswithjust
forwardpasses. InAdvancesinNeuralInformation SiqiPing,YuzhuMao,YangLiu,Xiao-PingZhang,and
ProcessingSystems,volume36,pages53038–53075. WenboDing.2024. FL-TAC:Enhancedfine-tuning
CurranAssociates,Inc. in federated learning via low-rank, task-specific
adapterclustering. InICLR2024WorkshoponLarge
Sourab Mangrulkar, Sylvain Gugger, Lysandre De- LanguageModel(LLM)Agents.
but, Younes Belkada, Sayak Paul, and Benjamin
Bossan. 2022. Peft: State-of-the-art parameter- Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
efficient fine-tuning methods. https://github. How multilingual is multilingual bert? arXiv
com/huggingface/peft. preprintarXiv:1906.01502.ZhenQin,DaoyuanChen,BingchenQian,BolinDing, ChaoRen,HanYu,HongyiPeng,XiaoliTang,AnranLi,
YaliangLi, andShuiguangDeng.2024. Federated YulanGao,AlysaZiyingTan,BoZhao,XiaoxiaoLi,
full-parametertuningofbillion-sizedlanguagemod- ZengxiangLi,andQiangYang.2024. Advancesand
elswithcommunicationcostunder18kilobytes. In openchallengesinfederatedlearningwithfoundation
Proceedingsofthe41thInternationalConferenceon models. arXivpreprintarXiv:2404.15381.
MachineLearning.
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Mil-
Chen Qiu, Xingyu Li, Chaithanya Kumar Mummadi, letari,HolgerRRoth,ShadiAlbarqouni,Spyridon
Madan Ravi Ganesh, Zhenzhen Li, Lu Peng, and Bakas, Mathieu N Galtier, Bennett A Landman,
Wan-YiLin.2024. Federatedtext-drivenpromptgen- KlausMaier-Hein,etal.2020. Thefutureofdigital
erationforvision-languagemodels. InTheTwelfth healthwithfederatedlearning. NPJdigitalmedicine,
International Conference on Learning Representa- 3(1):119.
tions.
Nuria Rodríguez-Barroso, Daniel Jiménez-López,
AlecRadford,JongWookKim,ChrisHallacy,Aditya MVictoriaLuzón,FranciscoHerrera,andEugenio
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas- Martínez-Cámara.2023. Surveyonfederatedlearn-
try, Amanda Askell, Pamela Mishkin, Jack Clark, ingthreats: Concepts,taxonomyonattacksandde-
etal.2021. Learningtransferablevisualmodelsfrom fences,experimentalstudyandchallenges. Informa-
naturallanguagesupervision. InInternationalconfer- tionFusion,90:148–173.
enceonmachinelearning,pages8748–8763.PMLR.
HolgerR.Roth,ZiyueXu,Yuan-TingHsieh,Adithya
AlecRadford,JongWookKim,TaoXu,GregBrock-
Renduchintala,IsaacYang,ZhihongZhang,Yuhong
man,ChristineMcleavey,andIlyaSutskever.2023.
Wen, Sean Yang, Kevin Lu, Kristopher Kersten,
Robustspeechrecognitionvialarge-scaleweaksu-
Camir Ricketts, Daguang Xu, Chester Chen, Yan
pervision. InProceedingsofthe40thInternational
Cheng,andAndrewFeng.2024. Empoweringfed-
Conference on Machine Learning, volume 202 of
eratedlearningformassivemodelswithnvidiaflare.
ProceedingsofMachineLearningResearch,pages
arXivpreprintarXiv:2402.07792.
28492–28518.PMLR.
WangRui,YuTong,ZhangRuiyi,KimSungchul,Rossi
AlecRadfordandJeffreyWu.2019. Rewonchild,david
Ryan A., Zhao Handong, Wu Junda, Mitra Sub-
luan, dario amodei, and ilya sutskever. 2019. Lan-
rata, Yao Lina, and Henao Ricardo. 2024. Person-
guage models are unsupervised multitask learners.
alizedfederatedlearningfortextclassificationwith
OpenAIblog,1(8):9.
gradient-freeprompttuning. InProceedingsofthe
2024ConferenceoftheNorthAmericanChapterof
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
theAssociationforComputationalLinguistics: Hu-
Gray,ChelseaVoss,AlecRadford,MarkChen,and
manLanguageTechnologies.
IlyaSutskever.2021. Zero-shottext-to-imagegen-
eration. In International Conference on Machine
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Learning,pages8821–8831.PMLR.
Thomas Wolf. 2020. Distilbert, a distilled version
Swarna Priya Ramu, Parimala Boopalan, Quoc-Viet of bert: smaller, faster, cheaper and lighter. arXiv
Pham, Praveen Kumar Reddy Maddikunta, Thien preprintarXiv:1910.01108.
Huynh-The, Mamoun Alazab, Thanh Thi Nguyen,
Sejin Seo, Seung-Woo Ko, Jihong Park, Seong-Lyun
andThippaReddyGadekallu.2022. Federatedlearn-
Kim, and Mehdi Bennis. 2021. Communication-
ingenableddigitaltwinsforsmartcities: Concepts,
efficient and personalized federated lottery ticket
recentadvances,andfuturedirections. Sustainable
learning. In2021IEEE22ndInternationalWorkshop
CitiesandSociety,79:103663.
onSignalProcessingAdvancesinWirelessCommu-
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, nications(SPAWC),pages581–585.
ZacharyGarrett,KeithRush,JakubKonecˇný,Sanjiv
Kumar,andHughBrendanMcMahan.2021. Adap- Suhail Mohmad Shah and Vincent K. N. Lau. 2023.
tivefederatedoptimization. InInternationalConfer- Modelcompressionforcommunicationefficientfed-
enceonLearningRepresentations. eratedlearning. IEEETransactionsonNeuralNet-
worksandLearningSystems,34(9):5937–5951.
ScottReedetal.2022. Ageneralistagent. Transactions
onMachineLearningResearch. FeaturedCertifica- Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li,
tion,OutstandingCertification. Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin,
andTingWang.2021. Backdoorpre-trainedmodels
AmirhosseinReisizadeh,AryanMokhtari,HamedHas- cantransfertoall. InProceedingsofthe2021ACM
sani, Ali Jadbabaie, and Ramtin Pedarsani. 2020. SIGSACConferenceonComputerandCommunica-
Fedpaq: Acommunication-efficientfederatedlearn- tionsSecurity,CCS’21.ACM.
ingmethodwithperiodicaveragingandquantization.
In Proceedings of the Twenty Third International JaeminShin,HyungjunYoon,SeungjooLee,Sungjoon
ConferenceonArtificialIntelligenceandStatistics, Park, Yunxin Liu, Jinho Choi, and Sung-Ju Lee.
volume 108 of Proceedings of Machine Learning 2023a. FedTherapist: Mentalhealthmonitoringwith
Research,pages2021–2031.PMLR. user-generatedlinguisticexpressionsonsmartphonesviafederatedlearning. InProceedingsofthe2023 Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding.
Conference on Empirical Methods in Natural Lan- 2024b. ImprovingloRAinprivacy-preservingfeder-
guageProcessing, pages11971–11988, Singapore. atedlearning. InTheTwelfthInternationalConfer-
AssociationforComputationalLinguistics. enceonLearningRepresentations.
JiyunShin,JinhyunAhn,HongguKang,andJoonhyuk RishubTamirisa, JohnWon, ChengjunLu, RonArel,
Kang.2023b. Fedsplitx: Federatedsplitlearningfor andAndyZhou.2024. Fedselect: Customizedselec-
computationally-constrainedheterogeneousclients. tionofparametersforfine-tuningduringpersonalized
arXivpreprintarXiv:2310.14579. federatedlearning. InFederatedLearningandAna-
lyticsinPractice: Algorithms,Systems,Applications,
andOpportunities.
AbhishekSingh,PraneethVepakomma,OtkristGupta,
andRameshRaskar.2019. Detailedcomparisonof
Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and
communicationefficiencyofsplitlearningandfeder-
Xia Hu. 2023. Does synthetic data generation of
atedlearning. arXivpreprintarXiv:1909.09145.
llms help clinical text mining? arXiv preprint
arXiv:2303.04360.
J.C.Spall.1992. Multivariatestochasticapproximation
usingasimultaneousperturbationgradientapprox-
YuqingTang,ChauTran,XianLi,Peng-JenChen,Na-
imation. IEEETransactionsonAutomaticControl,
manGoyal,VishravChaudhary,JiataoGu,andAn-
37(3):332–341.
gelaFan.2020. Multilingualtranslationwithexten-
siblemultilingualpretrainingandfinetuning. arXiv
ShangchaoSu,BinLi,andXiangyangXue.2023. Fe-
preprintarXiv:2008.00401.
dra: Arandomallocationstrategyforfederatedtun-
ing to unleash the power of heterogeneous clients. Buse G. A. Tekgul, Yuxi Xia, Samuel Marchal, and
arXivpreprintarXiv:2311.11227. N. Asokan. 2021. Waffle: Watermarking in feder-
atedlearning. In202140thInternationalSymposium
ShangchaoSu,MingzhaoYang,BinLi,andXiangyang onReliableDistributedSystems(SRDS),pages310–
Xue. 2024. Federated adaptive prompt tuning for 320.
multi-domain collaborative learning. Proceedings
of the AAAI Conference on Artificial Intelligence, Chandra Thapa, Pathum Chamikara Ma-
38(13):15117–15125. hawagaArachchige,SeyitCamtepe,andLichaoSun.
2022. Splitfed: Whenfederatedlearningmeetssplit
Guangyu Sun, Matias Mendieta, Jun Luo, Shandong learning. Proceedings of the AAAI Conference on
Wu,andChenChen.2023. Fedperfix: Towardspar- ArtificialIntelligence,36(8):8485–8493.
tialmodelpersonalizationofvisiontransformersin
federatedlearning. InProceedingsoftheIEEE/CVF YuanyishuTian,YaoWan,LingjuanLyu,DezhongYao,
InternationalConferenceonComputerVision,pages HaiJin,andLichaoSun.2022. Fedbert: Whenfeder-
4988–4998. atedlearningmeetspre-training. ACMTrans.Intell.
Syst.Technol.,13(4).
GuangyuSun,MatiasMendieta,TaojiannanYang,and
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
ChenChen.2022a. Conqueringthecommunication
Martinet,Marie-AnneLachaux,TimothéeLacroix,
constraintstoenablelargepre-trainedmodelsinfed-
BaptisteRozière,NamanGoyal,EricHambro,Faisal
eratedlearning. arXivpreprintarXiv:2210.01708.
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang,
arXiv:2302.13971.
DaguangXu,YiranChen,andHolgerRRoth.2024a.
Fedbpt: Efficientfederatedblack-boxprompttuning
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
for large language models. In Proceedings of the
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
41thInternationalConferenceonMachineLearning.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
TianxiangSun,ZhengfuHe,HongQian,YunhuaZhou,
tion and fine-tuned chat models. arXiv preprint
XuanjingHuang,andXipengQiu.2022b. BBTv2:
arXiv:2307.09288.
Towardsagradient-freefuturewithlargelanguage
models. InProceedingsofthe2022Conferenceon Vasileios Tsouvalas, Yuki Asano, and Aaqib Saeed.
EmpiricalMethodsinNaturalLanguageProcessing, 2023. Federated fine-tuning of foundation mod-
pages3916–3930,AbuDhabi,UnitedArabEmirates. els via probabilistic masking. arXiv preprint
AssociationforComputationalLinguistics. arXiv:2311.17299.
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Minh Vu, Truc Nguyen, Tre’ Jeter, and My T. Thai.
Huang,andXipengQiu.2022c. Black-boxtuningfor 2024. Analysisofprivacyleakageinfederatedlarge
language-model-as-a-service. InProceedingsofthe languagemodels. InProceedingsofThe27thInter-
39thInternationalConferenceonMachineLearning, national Conference on Artificial Intelligence and
volume 162 of Proceedings of Machine Learning Statistics, volume 238 of Proceedings of Machine
Research,pages20841–20855.PMLR. LearningResearch,pages1423–1431.PMLR.EricWang.2023. Alpaca-lora. Zhaoxian Wu, Qing Ling, Tianyi Chen, and Geor-
gios B. Giannakis. 2020b. Federated variance-
HaoyuWang,HandongZhao,YaqingWang,TongYu,
reducedstochasticgradientdescentwithrobustness
JiuxiangGu,andJingGao.2022. Fedkc: Federated
tobyzantineattacks. IEEETransactionsonSignal
knowledgecompositionformultilingualnaturallan-
Processing,68:4583–4596.
guage understanding. In Proceedings of the ACM
WebConference2022,WWW’22,page1839–1850,
ChulinXie,KeliHuang,Pin-YuChen,andBoLi.2020.
New York, NY, USA. Association for Computing
Dba: Distributedbackdoorattacksagainstfederated
Machinery.
learning. InInternationalConferenceonLearning
Representations.
Xin’aoWang,HuanLi,KeChen,andLidanShou.2023.
Fedbfpt: Anefficientfederatedlearningframework
YuexiangXie,ZhenWang,DaweiGao,DaoyuanChen,
forbertfurtherpre-training. InProceedingsofthe
Liuyi Yao, Weirui Kuang, Yaliang Li, Bolin Ding,
Thirty-SecondInternationalJointConferenceonArti-
andJingrenZhou.2023. Federatedscope: Aflexible
ficialIntelligence,IJCAI-23,pages4344–4352.Inter-
federatedlearningplatformforheterogeneity. Proc.
nationalJointConferencesonArtificialIntelligence
VLDBEndow.,16(5):1059–1072.
Organization. MainTrack.
KangWei,JunLi,MingDing,ChuanMa,HowardH. ChangXu,YuJia,LiehuangZhu,ChuanZhang,Guoxie
Yang,FarhadFarokhi,ShiJin,TonyQ.S.Quek,and Jin,andKashifSharif.2022. Tdfl: Truthdiscovery
H.VincentPoor.2020. Federatedlearningwithdif- based byzantine robust federated learning. IEEE
ferentialprivacy: Algorithmsandperformanceanaly- Transactions on Parallel and Distributed Systems,
sis. IEEETransactionsonInformationForensicsand 33(12).
Security,15:3454–3469.
Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li,
Orion Weller, Marc Marone, Vladimir Braverman, and Shangguang Wang. 2024a. Fwdllm: Effi-
DawnLawrie,andBenjaminVanDurme.2022. Pre- cientfedllmusingforwardgradient. arXivpreprint
trained models for multilingual federated learning. arXiv:2308.13894.
InProceedingsofthe2022ConferenceoftheNorth
AmericanChapteroftheAssociationforComputa- Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie
tionalLinguistics: HumanLanguageTechnologies, Yi, Daliang Xu, Qipeng Wang, Bingyang Wu,
pages1413–1421,Seattle,UnitedStates.Association Yihao Zhao, Chen Yang, Shihe Wang, Qiyang
forComputationalLinguistics. Zhang,ZhenyanLu,LiZhang,ShangguangWang,
Yuanchun Li, Yunxin Liu, Xin Jin, and Xuanzhe
Herbert Woisetschläger, Alexander Isenko, Shiqiang
Liu. 2024b. A survey of resource-efficient llm
Wang, Ruben Mayer, and Hans-Arno Jacobsen.
andmultimodalfoundationmodels. arXivpreprint
2024. Asurveyonefficientfederatedlearningmeth-
arXiv:2401.08092.
ods for foundation model training. arXiv preprint
arXiv:2401.04472.
MinruiXu,HongyangDu,DusitNiyato,JiawenKang,
BichenWu,ChenfengXu,XiaoliangDai,AlvinWan, Zehui Xiong, Shiwen Mao, Zhu Han, Abbas Ja-
PeizhaoZhang,ZhichengYan,MasayoshiTomizuka, malipour,DongInKim,XueminShen,VictorC.M.
Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Leung, and H. Vincent Poor. 2024c. Unleashing
2020a. Visual transformers: Token-based image thepowerofedge-cloudgenerativeaiinmobilenet-
representation and processing for computer vision. works: Asurveyofaigcservices. IEEECommunica-
arXivpreprintarXiv:2006.03677. tionsSurveys&Tutorials,pages1–1.
JiayangWu, WenshengGan, ZefengChen, Shicheng ZhengXu,YanxiangZhang,GalenAndrew,Christopher
Wan, and Hong Lin. 2023a. Ai-generated content Choquette,PeterKairouz,BrendanMcmahan,Jesse
(aigc): Asurvey. arXivpreprintarXiv:2304.06632. Rosenstock, and Yuanbo Zhang. 2023. Federated
learningofgboardlanguagemodelswithdifferential
Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang,
privacy. In Proceedings of the 61st Annual Meet-
HongchaoGu,TingjiaShen,ChuanQin,ChenZhu,
ingoftheAssociationforComputationalLinguistics
HengshuZhu,QiLiu,HuiXiong,andEnhongChen.
(Volume5: IndustryTrack),pages629–639,Toronto,
2023b. Asurveyonlargelanguagemodelsforrec-
Canada.AssociationforComputationalLinguistics.
ommendation. arXivpreprintarXiv:2305.19860.
Fu-En Yang, Chien-Yi Wang, and Yu-Chiang Frank
Panlong Wu, Kangshuo Li, Ting Wang, and Fangxin
Wang.2023a. Efficientmodelpersonalizationinfed-
Wang.2023c. Fedms: Federatedlearningwithmix-
eratedlearningviaclient-specificpromptgeneration.
tureofsparselyactivatedfoundationsmodels. arXiv
In2023IEEE/CVFInternationalConferenceonCom-
preprintarXiv:2312.15926.
puterVision(ICCV),pages19102–19111.
ShijieWuandMarkDredze.2020. Arealllanguages
createdequalinmultilingualBERT? InProceedings KailaiYang,ShaoxiongJi,TianlinZhang,QianqianXie,
ofthe5thWorkshoponRepresentationLearningfor Ziyan Kuang, and Sophia Ananiadou. 2023b. To-
NLP,pages120–130,Online.AssociationforCom- wardsinterpretablementalhealthanalysiswithlarge
putationalLinguistics. languagemodels. arXivpreprintarXiv:2304.03347.XinYang,HaoYu,XinGao,HaoWang,JunboZhang, HongleiZhang,FangyuanLuo,JunWu,XiangnanHe,
andTianruiLi.2024a. Federatedcontinuallearning andYidongLi.2023a. Lightfr: Lightweightfeder-
viaknowledgefusion: Asurvey. IEEETransactions atedrecommendationwithprivacy-preservingmatrix
onKnowledgeandDataEngineering,page1–20. factorization. ACMTrans.Inf.Syst.,41(4).
YiyuanYang,GuodongLong,TaoShen,JingJiang,and JianyiZhang,SaeedVahidian,MartinKuo,Chunyuan
Michael Blumenstein. 2024b. Dual-personalizing Li,RuiyiZhang,TongYu,GuoyinWang,andYiran
adapter for federated foundation models. arXiv Chen. 2024b. Towards building the federatedgpt:
preprintarXiv:2403.19211. Federatedinstructiontuning. InICASSP2024-2024
IEEEInternationalConferenceonAcoustics,Speech
Chun-HanYao,BoqingGong,HangQi,YinCui,Yukun andSignalProcessing(ICASSP),pages6915–6919.
Zhu,andMing-HsuanYang.2022. Federatedmulti-
targetdomainadaptation. In2022IEEE/CVFWin- JieZhang,XiaohuaQi,andBoZhao.2023b. Federated
terConferenceonApplicationsofComputerVision generativelearningwithfoundationmodels. arXiv
(WACV),pages1081–1090. preprintarXiv:2306.16064.
Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi JunjieZhang,RuobingXie,YupengHou,WayneXin
Li,YindaXu,YaxinDu,YanfengWang,andSiheng Zhao,LeyuLin,andJi-RongWen.2023c. Recom-
Chen.2024. Openfedllm: Traininglargelanguage mendationasinstructionfollowing:Alargelanguage
models on decentralized private data via federated modelempoweredrecommendationapproach. arXiv
learning. arXivpreprintarXiv:2402.06954. preprintarXiv:2305.07001.
DongYin,YudongChen,RamchandranKannan,and ZhiyuanZhang,DeliChen,HaoZhou,FandongMeng,
Peter Bartlett. 2018. Byzantine-robust distributed Jie Zhou, and Xu Sun. 2023d. Fed-fa: Theoreti-
learning: Towardsoptimalstatisticalrates. InInter- callymodelingclientdatadivergenceforfederated
nationalConferenceonMachineLearning.PMLR. languagebackdoordefense. InAdvancesinNeural
InformationProcessingSystems,volume36,pages
QiyingYu,YangLiu,YimuWang,KeXu,andJingjing 62006–62031.CurranAssociates,Inc.
Liu.2023a. Multimodalfederatedlearningviacon-
trastive representation ensemble. In The Eleventh Zhuo Zhang, Xiangjing Hu, Jingyuan Zhang, Yating
International Conference on Learning Representa- Zhang, Hui Wang, Lizhen Qu, and Zenglin Xu.
tions. 2023e. FEDLEGAL:Thefirstreal-worldfederated
learningbenchmarkforlegalNLP. InProceedings
ShuyangYu,JunyuanHong,YiZeng,FeiWang,Ruoxi of the 61st Annual Meeting of the Association for
Jia,andJiayuZhou.2023b. Wholeakedthemodel? ComputationalLinguistics(Volume1: LongPapers),
trackingIPinfringersinaccountablefederatedlearn- pages3492–3507,Toronto,Canada.Associationfor
ing. InNeurIPS2023WorkshoponRegulatableML. ComputationalLinguistics.
Sixing Yu, J Pablo Muñoz, and Ali Jannesari. 2023c. Zhuo Zhang, Jintao Huang, Xiangjing Hu, Jingyuan
Bridging the gap between foundation models and Zhang, Yating Zhang, Hui Wang, Yue Yu, Qifan
heterogeneous federated learning. arXiv preprint Wang,LizhenQu,andZenglinXu.2024c. Revisiting
arXiv:2310.00247. datareconstructionattacksonreal-worlddatasetfor
federated natural language understanding. In Pro-
Sixing Yu, J Pablo Muñoz, and Ali Jannesari. 2023d. ceedingsofthe2024JointInternationalConference
Federated foundation models: Privacy-preserving onComputationalLinguistics,LanguageResources
and collaborativelearning for large models. arXiv andEvaluation(LREC-COLING2024),pages14080–
preprintarXiv:2305.11414. 14091,Torino,Italia.ELRAandICCL.
Huimin Zeng, Zhenrui Yue, Qian Jiang, and Dong ZhuoZhang,YuanhangYang,YongDai,QifanWang,
Wang. 2024. Federated recommendation via hy- Yue Yu, Lizhen Qu, and Zenglin Xu. 2023f. Fed-
bridretrievalaugmentedgeneration. arXivpreprint PETuning: When federated learning meets the
arXiv:2403.04256. parameter-efficienttuningmethodsofpre-trainedlan-
guage models. In Findings of the Association for
Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, ComputationalLinguistics: ACL2023,pages9963–
FengYan,andYangLiu.2020. BatchCrypt:Efficient 9977, Toronto, Canada. Association for Computa-
homomorphic encryption for Cross-Silo federated tionalLinguistics.
learning. In2020USENIXAnnualTechnicalConfer-
ence(USENIXATC20), pages493–506.USENIX Jujia Zhao, Wenjie Wang, Chen Xu, Zhaochun Ren,
Association. See-Kiong Ng, and Tat-Seng Chua. 2024a. Llm-
based federated recommendation. arXiv preprint
Honglei Zhang, He Liu, Haoxuan Li, and Yidong Li. arXiv:2402.09959.
2024a. Transfr: Transferablefederatedrecommen-
dation with pre-trained language models. arXiv WanruZhao, YihongChen, RoysonLee, XinchiQiu,
preprintarXiv:2402.01124. Yan Gao, Hongxiang Fan, and Nicholas DonaldLane.2024b. Breakingphysicalandlinguisticbor- fine-tuning, it achieves comparable performance
ders: Multilingualfederatedprompttuningforlow- but with 1000× less parameter storage and com-
resource languages. In The Twelfth International
munication(Jiaetal.,2022). Avariationofprompt
ConferenceonLearningRepresentations.
tuning, FedPerfix (Sun et al., 2023) uses a local
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, adaptertogeneratetheprefixesandaggregatethe
XiaoleiWang,YupengHou,YingqianMin,Beichen
originalself-attentionlayers.
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
Dependingontargetmodalities,prompttuning
survey of large language models. arXiv preprint
arXiv:2303.18223. in current literature can be further classified into
threecategories:
FeiZheng,ChaochaoChen,LingjuanLyu,andBinhui
Yao.2023. Reducingcommunicationforsplitlearn-
• TextualPromptTuning. Task-specificpromptem-
ingbyrandomizedtop-ksparsification. InProceed-
beddingsarecombinedwiththeinputtextembed-
ingsoftheThirty-SecondInternationalJointConfer-
enceonArtificialIntelligence,IJCAI’23. dings,whicharesubsequentlyfedintolanguage
models. Thesesoftpromptsserveasinstructive
LigengZhu,ZhijianLiu,andSongHan.2019. Deep
contexts to influence the generation process of
leakagefromgradients. In AdvancesinNeuralIn-
formation Processing Systems, volume 32. Curran LLMsbysteeringtheprobabilitydistributionof
Associates,Inc. thenexttoken(Dongetal.,2023).
WeimingZhuang,ChenChen,andLingjuanLyu.2023.
• Visual PromptTuning. Takinginspirationfrom
When foundation model meets federated learning:
advances in efficiently tuning LLMs, prompts
Motivations,challenges,andfuturedirections. arXiv
preprintarXiv:2306.15546. are also introduced in the input space of vision
models (Jia et al., 2022). Naive implementa-
A AdditionalDetailsofAdapterTuning
tions introduce prompts at the pixel level, act-
ing as a form of data augmentation (Li et al.,
A.1 AdapterTuning
2024a). Alternatively,onecouldalsoinsertthe
Adapter tuning integrates small-scale neural net-
promptsaslatentvectorsforthefirstTransformer
works (known as “adapters”) into the pre-trained
layer(Dengetal.,2024;Yangetal.,2023a). Nev-
models(Houlsbyetal.,2019;Huetal.,2022). A
ertheless,anempiricalstudy(Jiaetal.,2022)has
straightforwardimplementationofadaptertuning
suggested that it is easier for visual prompts to
istocollaborativelytrainasharedadapteramong
learn condensed task-dependent signals in the
all clients in the FedAvg manner, as highlighted
latentinputspaceofTransformers.
by Sun et al. (2022a). Based on FedAvg, Fed-
CLIP(Luetal.,2023a)incorporatesanattention- • Textual-Visual Prompt Tuning. Unlike single-
basedadapterfortheimageencoderinCLIPmod- modal FMs, vision-language FMs can process
els (Radford et al., 2021). In the domain of mul- and interpret both visual data and textual infor-
tilingualmachinetranslation,wheredifferentlan- mation,endowingthemwithpowerfulrepresen-
guagepairsexhibitsubstantialdiscrepanciesindata tationabilityandtransferability(Radfordetal.,
distributions, Fed-MNMT (Liu et al., 2023d) ex- 2021). Basedonvision-languageFMslikeCLIP,
ploresclusteringstrategiesthatgroupadapterpa- textual-visualprompttuningshowspromisingca-
rameters and makes inner-cluster parameters ag- pabilitiesinFL(Guoetal.,2023),especiallyin
gregationforalleviatingtheundesirableeffectof cross-domainscenarios,wherethemodelneeds
datadiscrepancy. Anotherrepresentativeapproach togeneralizeacrossvarieddomainsandunseen
namedC2A(Kimetal.,2023)employshypernet- classes(Qiuetal.,2024).
works(Haetal.,2017)togenerateclient-specific
adapters by conditioning on the client’s informa-
B LibrariesandBenchmarks
tion,maximizingtheutilityofsharedmodelparam-
eters while minimizing the divergence caused by This part briefly introduces a series of available
dataheterogeneity. librariesandbenchmarksfordevelopingandexam-
iningFM-FLtechniques. Anoverviewisprovided
A.2 PromptTuning
inTable3.
Prompttuningincorporatestrainabletask-specific
continuouspromptvectorsattheinputlayer(Liu • FederatedScope-LLM(Kuangetal.,2023)isan
etal.,2023a;Dongetal.,2023). Comparedtofull open-source package for fine-tuning LLMs viaTable3: AlistofexistingFM-FLlibrariesandbenchmarks.MissingorinapplicabledetailsdenotedbyN/A.(cid:33)denotesa
strongfocusorpresence;(cid:37)indicatesnofocusorabsence; signifiesamoderatefocusorpartialinclusion.
(cid:71)(cid:35)
Library/Benchmark FLBackend Description
FederatedScope-LLM(Kuangetal.,2023) FederatedScope (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) Anend-to-endbenchmarkforefficientfine-tuningLLMswithFL
NVIDIAFLARE(Rothetal.,2024) NVFlare (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) Scalableandefficientfine-tuningLLMswithFL
FATE-LLM(Fanetal.,2023) FATE (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) FocusesonIPandprivacyprotectioninfederatedLLM
FedLLM(FedML,2023) FedML (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) AnMLOps-supportedtrainingpipelinebasedonFedML
OpenFedLLM(Yeetal.,2024) N/A (cid:33) (cid:37) (cid:35)(cid:71) (cid:37) N/A (cid:37) AnLLMframeworkfocusingonFLinstructiontuning/alignment
Shepherd(Zhangetal.,2024b) N/A (cid:33) (cid:37) (cid:33)(cid:35)(cid:71) (cid:37) (cid:37) (cid:37) FederatedinstructiontuningbasedonHuggingFace
FedPETuning(Zhangetal.,2023f) FedLab (cid:33) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) AbenchmarkcomprisingfourFedPEFTmethods
FedLegal(Zhangetal.,2023e) FedLab (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) AbenchmarkcomprisingsixlegalNLPtasksunderFLsettings
FL.BuiltontopofapopularFLbackendFeder- • FedPETuning(Zhangetal.,2023f)isapioneer-
atedScope(Xieetal.,2023),itsupportsfederated ingfederatedbenchmarkforfourrepresentative
fine-tuningofLLMsundervariousFLscenarios, FedPEFTmethods,coveringadaptertuning,pre-
includingFedPEFTandmodelpersonalization. fixtuning,LoRA,andBitFit.
• FedLegal(Zhangetal.,2023e)istheveryfirst
• NVIDIA FLARE (Roth et al., 2024) is an FL
real-worldFLbenchmarkforlegalNLP,which
frameworkthatallowsresearchersanddatascien-
comprisesfivelegalNLPtasksandoneprivacy
tiststoseamlesslymovetheirmachinelearning
taskbasedonthedatafromChinesecourts.
and deep learning workflows into a federated
paradigm.
• FATE-LLM (Fan et al., 2023) is an industrial-
gradeFLframeworkforLLM.ApartfromFed-
PEFT,itprovidesaprivacyhubintegratingsev-
eralIPprotectionandprivacy-preservingmecha-
nismstoprotectmodelsecurityanddataprivacy.
• FedLLM (FedML, 2023) is an MLOps-
supportedtrainingpipelinebuiltupontheFedML
AIplatform(Heetal.,2020). FedLLMiscom-
patiblewithpopularLLMlibrariessuchasHug-
gingFaceandDeepSpeedtosupportalargerange
ofFMsanddatasets.
• OpenFedLLM (Ye et al., 2024) is a federated
tuningframeworkforLLMs,whichcoversappli-
cationsofinstructiontuningandvaluealignment,
diverseFLbaselines,trainingdatasets,andeval-
uationdatasets.
• Shepherd(Zhangetal.,2024b)isalightweight
federatedtuningframework. Thelocaltraining
processofShepherdisbuiltupontheimplemen-
tationsofAlpaca-LoRA(Wang,2023),andHug-
gingFace’sPEFT(Mangrulkaretal.,2022),en-
ablingefficientfine-tuning.
troppuSMLL
troppuSMFladoMitluM
TFEPdeF
gniniarTeciveD-nO
deretsulC&detubirtsiD
ycavirPlaitnereffiD