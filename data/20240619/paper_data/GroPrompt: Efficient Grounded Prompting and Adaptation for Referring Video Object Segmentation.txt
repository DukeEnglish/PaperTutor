GroPrompt: Efficient Grounded Prompting and Adaptation
for Referring Video Object Segmentation
Ci-SiangLin1,2∗† I-JiehLiu1∗ Min-HungChen2 Chien-YiWang2 SifeiLiu2
Yu-ChiangFrankWang1,2
1GraduateInstituteofCommunicationEngineering,NationalTaiwanUniversity,Taiwan
2NVIDIA
{d08942011, r11942087, ycwang}@ntu.edu.tw, {minghungc, chienyiw, sifeil}@nvidia.com
Abstract theleftswimmingtowardsthetopright”),whichcouldnot
be easily recognized from a single frame. To address this
Referring Video Object Segmentation (RVOS) aims to challenging task, many works [15, 21, 27, 30, 37, 38, 41,
segment the object referred to by the query sentence 42,44]havebeenproposed. URVOS[37]ispioneeringasa
throughout the entire video. Most existing methods re- unifiedframeworkforreferringvideosegmentation,which
quire end-to-end training with dense mask annotations, introduces memory attention modules to retrieve relevant
which could be computation-consuming and less scalable. informationfromthepreviousframeandencouragetempo-
In this work, we aim to efficiently adapt foundation seg- ralconsistency.WiththerapiddevelopmentofTransformer,
mentation models for addressing RVOS from weak su- ReferFormer[42]adoptsencoderanddecoderlayersinthe
pervision with the proposed Grounded Prompting (Gro- Transformermodelandviewslanguageasqueriestoattend
Prompt) framework. More specifically, we propose Text- tothereferredobject, andaninstancematchingstrategyis
AwarePromptContrastiveLearning(TAP-CL)toenhance utilized to achieve object tracking. Recent works like FS-
the association between the position prompts and the re- RVOS[21]andOnlineRefer[41]furtherextendRVOSinto
ferring sentences with only box supervisions, including the few-shot setting and online pipeline to handle limited
Text-ContrastivePromptLearning(TextCon)andModality- samplesandongoingvideosinreal-worldscenarios,respec-
Contrastive Prompt Learning (ModalCon) at frame level tively. Nevertheless, most existing methods require end-
and video level, respectively. With the proposed TAP- to-endtrainingforvision-languagemodels,whichcouldbe
CL, our GroPrompt framework can generate temporal- computationallyexpensiveandtime-consuming.Moreover,
consistentyettext-awarepositionpromptsdescribingloca- therequirementofdensemaskannotationsfortrainingim-
tionsandmovementsforthereferredobjectfromthevideo. pedesthescalabilityofthoseapproaches.
The experimental results in the standard RVOS bench-
marks (Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences, Recently, foundation segmentation models [20, 40, 58]
and JHMDB-Sentences) demonstrate the competitive per- has been proposed. By leveraging numerous training data
formanceofourproposedGroPromptframeworkgivenonly and employing large-scale model architectures, they can
boundingboxweaksupervisions. produce high-quality object masks according to various
prompts such as points or boxes, and have shown over-
whelming generalizability on various datasets, setting su-
1.Introduction periorbenchmarksforsegmentationtasks. However, there
are still challenges in the RVOS problem not addressed
ReferringVideoObjectSegmentation(RVOS),aimstoseg-
by those foundation models. For example, SAM [20] is
menttheobjectreferredtobyasentencequerythroughout
trained solely with images and their associated masks, not
the entire video. In contrast to RIS, RVOS is particularly
tailored to handle natural language descriptions and video
facedwithdynamicvisualchallenges,suchaspositionand
datainRVOS.WhileitispossibletoadaptSAMtothetask
size variation, pose deformation, object occlusion or exit,
of RVOS by incorporating grounding models (e.g., [25])
andscenevariation. Moreover, thereferringsentencemay
to generate text-associated position prompts and tracking
contain long-term motions or actions (e.g., “a gold fish on
models (e.g., [10]) to capture object motions across video
*Equalcontribution. frames, such naive combination of off-the-shelf models
†WorkdoneduringaninternshipatNVIDIA. has shown to be suboptimal [23], as they are individually
4202
nuJ
81
]VC.sc[
1v43821.6042:viXratrained for different tasks. Therefore, a question arises: andvideo-level,respectively.
“How can we effectively exploit foundation segmentation
models to address RVOS?” We argue that the RVOS prob- • Thederivedpositionpromptswouldbeutilizedtoinstruct
lemcanbedecomposedintoreferring, video, andsegmen- image-basedfoundationsegmentationmodelstoproduce
tationfactors,andleavethesegmentationproblemtofoun- object masks, enabling efficient adaptation to referring
dationsegmentationmodels. Weonlyfocusonaddressing video object segmentation with 7× fewer trainable pa-
thereferringandvideofactorsascurrentfoundationmodels rameterscomparedwithSOTAs.
canalreadytackletosegmentationproblemeffectively.
In this paper, we aim to efficiently adapt image- 2.RelatedWork
based foundation segmentation models for addressing
2.1.ReferringVideoObjectSegmentation
referring video object segmentation from weak super-
vision. To achieve this goal, we propose a novel ReferringVideoObjectSegmentation(RVOS)[15,27,30,
Grounded Prompting (GroPrompt) framework, which ad- 38,41,42,44]strivestosegmenttheobjectdescribedbya
vances vision-language learning to produce temporal- free-form sentence query across the entire video duration.
consistent yet text-aware position prompts for segmen- Recently, ReferFormer [42] views language as queries to
tation purposes. More specifically, we propose Text- payattentiontothereferredobjectbyadoptinganencoder-
AwarePromptContrastiveLearning(TAP-CL)toenhance decoder style in the transformer. However, this work only
the association between the position prompts and the re- supports offline training and inference, limiting its usage
ferring sentences with only box supervisions, including in real-world scenarios. More recently, OnlineRefer [41]
Text-ContrastivePromptLearning(TextCon)andModality- further proposes an online RVOS setting to deal with the
Contrastive Prompt Learning (ModalCon) at frame level issues about offline limits, which makes it more possi-
andvideolevel,respectively. ForTextCon,weenforceour ble to adapt to real-world scenarios. Nevertheless, most
GroPromptframeworktogeneratedistinctpositionprompts existing methods require end-to-end training for vision-
for different referring sentences within each video frame. language models, which could be computationally expen-
As for the ModalCon, given that the sentence descrip- sive and time-consuming. Moreover, the requirement of
tion may contain long-term motions or actions spanning densemaskannotationsfortrainingimpedesthescalability
across different moments, we propose to align the whole ofthoseapproaches. Instead,weproposetoexploitfounda-
sequenceofpositionpromptsandthecorrespondingobject tionsegmentationmodelswithouttext-andtemporal-aware
with the input text for each video clip. With the proposed prompting, which is trained without mask annotations and
TAP-CL,ourGroPromptframeworkcangeneratetemporal- supportsonlinesettings.
consistentyettext-awarepositionpromptsdescribingloca-
tionsandmovementsforthereferredobjectfromthevideo. 2.2.FoundationSegmentationModels
More importantly, our derived position prompts would be
Inrecentyears,foundationvisionmodelshavegainedmas-
utilized to instruct image-based foundation segmentation
sive attention given their remarkable generalization ca-
models to produce object masks, enabling efficient adap-
pabilities on various downstream tasks. More recently,
tation to referring video object segmentation without re-
SAM [20] has introduced a foundation model specifically
quiring dense mask annotations. The experimental results
tailored for segmentation tasks. SAM allows specific po-
in the standard RVOS benchmarks (Ref-YouTube-VOS,
sition prompts (e.g., points, boxes, etc.) to demonstrate
Ref-DAVIS17, A2D-Sentences, and JHMDB-Sentences)
the zero-shot ability on the open vocabulary segmentation
demonstrate the competitive performance of our proposed
tasks with novel image distributions. Several works have
GroPrompt framework given only bounding box weak su-
studied the versatility of SAM, including remote sensing
pervisions.
images [5, 39], medical image analysis [6, 9, 28, 43], and
Wehighlightthecontributionsofthispaperasfollows: adaptationtovideo-basedtrackingtask[10,35,49],etc.
• We propose a novel Grounded Prompting (GroPrompt) For adaptation to tracking tasks with SAM, SAM-
framework, which performs efficient prompting and PT[35]designsapoint-basedpromptenhancementforthe
adapts image-based segmentation models to address original SAM point prompt to support classic video ob-
referring video object segmentation without additional ject segmentation tasks, while neglecting the importance
finetuning. of text prompt for advanced referring video object seg-
mentation. Another example SAM-Track [10] attempts
• To generate temporal-consistent yet text-aware position to utilize SAM for segmentation and detection of objects
prompts for segmentation purposes, we propose to while the DeAOT [51] module captures the motion across
jointly perform Text-Contrastive Prompt Learning and frames for tracking the objects. Though it is possible to
Modality-Contrastive Prompt Learning at frame-level combine text-grounding detection models (e.g., Ground-ing DINO [25]) with SAM-Track to tackle RVOS, Ref- 3.2.EfficientGroundedPromptingandAdaptation
SAM [23] has studied the possible concerns and indicates
Recent foundation segmentation models [20, 40, 58] have
theunsatisfactoryperformancecomparedwithcurrentSO-
presentedoverwhelmingperformanceonvarioussegmenta-
TAs in RVOS tasks. Different from the above, we pro-
tiontasks. Whenpromptedbypointsorboundingboxesin-
posetemporal-awarepromptingwithfoundationsegmenta-
dicatingthepositions,thesefoundationmodelswouldpro-
tionmodels(e.g.,SAM)totackleRVOSproblems.
duce high-quality object masks as desired. However, ex-
isting foundation segmentation models are mainly trained
3.ProposedMethod
from general image data and therefore have limited abil-
3.1.Overview ity to comprehend video content or complex text descrip-
tions. Toadaptimage-basedfoundationsegmentationmod-
Problem Definition. For the sake of completeness, we
elstoaddressreferringvideoobjectsegmentation,ourpro-
first define the problem setting and notations used in this
posedGroPromptframeworkisdesignedtolearnandgen-
paper. In Referring Video Object Segmentation (RVOS),
erate position prompts for the target object from the input
weassumethatthetrainingdatacontainasetofN videos,
video frames and the referring sentences. In this way, our
whereeachvideoV = {I }T isasequenceofT frames
t t=1 GroPrompt framework enables efficient model adaptation
and is associated with a set of referring sentences S =
withoutadditionalfinetuningforfoundationmodels,avoid-
{Si}M describingM distinctobjects. ThegoalofRVOS
i=1 ingpossibleoverfittingissueswhilereducingcomputational
is to produce segmentation masks for the referred objects.
costandtime. Wenowdetailourlearningschemebelow.
Different from previous works [30, 41, 42] which require
dense mask annotations for training, we assume that we
only have access to box-level annotations Bˆi = {Bˆi}T 3.2.1 Weakly-SupervisedPositionPrompts
t t=1
fortheT framescorrespondingtotheithreferringsentence To produce precise position prompts for segmentation, we
Si, whereeachboundingboxBˆ ti = (xˆi t,yˆ ti,hˆi t,wˆ ti)isrep- advancevision-languagelearningtogenerateboundingbox
resentedbythecoordinateofthecenterpointandtheheight proposalsforthereferredobject. AsillustratedinFigure1,
andwidth. our GroPrompt framework first employs a Transformer-
basedimage-textencodertoextractvisualfeaturesandlin-
FrameworkOverview. Undertheabovesetting,ourgoal guisticfeaturesforeachframeI tandthereferringsentence
istoefficientlyadaptimage-basedfoundationsegmentation Si,respectively. Inspiredby[25],weadoptthequerygen-
models for addressing referring video object segmentation erationmechanismtoobtainasetofobjectqueriesQi t. By
from such weak supervision. To achieve efficient model taking visual features and linguistic features as keys and
adaptation,weproposeanovelGroundedPrompting(Gro- values,thederivedobjectqueriesQi t wouldperformcross-
Prompt) framework, which advances vision-language attentionthroughthecross-modalitydecodertogeneratethe
learning to produce temporal-consistent yet text-aware po- boxproposalB ti. Withtheground-truthboundingboxBˆ ti,
sition prompts for segmentation purposes. As shown the standard box loss L box is formulated by the regression
in Figure 1, our proposed GroPrompt framework is de- lossandgeneralizedIoUlossL g [36]:
signed to generate the bounding box proposal by taking
(cid:34) T (cid:35)
object queries to perform cross-modal attention at each L =E (cid:88) λ ∥Bi−Bˆi∥ +λ L (Bi,Bˆi) (1)
frame. Such proposals then serve as position prompts box V,Si r t t 1 g g t t
i=1
to instruct foundation segmentation models to segment
the referred object. To facilitate the position prompts where λ and λ are hyper-parameters for the two loss
r g
to be text- and temporal-aware, we propose Text-Aware terms, respectively. Here, since there is typically only
Prompt Contrastive Learning (TAP-CL), including: 1) one target object in referring segmentation tasks, we sim-
Text-Contrastive Prompt Learning (TextCon) at the frame ply select the output proposal Bi with the highest confi-
t
level, which encourages the output proposals to be dis- dence score at each frame instead of using the Hungar-
tinct when taking different referring sentences as input; 2) ian loss [3] for matching. It is worth noting that we do
Modality-ContrastivePromptLearning(ModalCon),which not need mask loss for training like most existing RVOS
aims to align the output proposal sequence and its corre- works[15,27,30,38,41,42,44].
sponding object with the input text for each video clip.
With the proposed TAP-CL, our GroPrompt framework
3.2.2 Text-AwarePromptContrastiveLearning
wouldproducetemporal-consistentyettext-awareposition
prompts for the referred object, enabling efficient adapta- In referring segmentation tasks, the sentence descriptions
tion from weak supervision without additional finetuning couldbeambiguous. Forexample, thesentence“Aperson
forfoundationmodels. surfing” in Figure 1 refers to the person alone rather than(a) 𝑆𝑗 A surfboard. (b)
𝑆𝑖 A person surfing.
𝐼𝑡
Image-Text Encoder
Proposal 𝐼1 𝐼𝑡 𝑆𝑗 A surfboard.
Generation
𝑄𝑖/𝑗
𝑡 𝑆𝑖 A person surfing.
Cross-Modality Decoder Image Prompt Image Prompt Text
Encoder Encoder Encoder Encoder Encoder
box
𝐼𝑡
Prompt Encoder 𝑓1 𝑝 1𝑖 𝑓𝑡 𝑝 𝑡𝑖
SeP gr mom enp tt ae td
ion
𝑝 𝑡𝑖 𝑝 𝑡𝑗 Image Encoder
Prompted Attention & Pooling
avg. pooling
Mask Decoder
𝑓𝑖 𝑧𝑖 𝑧𝑗
Attract
Repel
Figure1.OverviewofourproposedGroPromptframework.In(a),ourproposalgenerationtakeseachframeI andthereferringsentence
t
SitoderiveobjectqueriesQi andproducethepromptembeddingpi forsegmentation,withanothersentenceSj asinputforperforming
t t
Text-ContrastivePromptLearning.In(b),tohandlesentencedescriptionscontaininglong-termmotionsoractionsinreferringvideoobject
segmentation,weuniquelypresentModality-ContrastivePromptLearningtoalignthetextwiththereferredobjectatthevideolevel.
both the person and the surfboard. To mitigate such text We note that to preserve the latent space learned by foun-
ambiguityinnaturallanguage,weproposetoperformText- dation models for segmentation, we choose to freeze the
ContrastivePromptLearning(TextCon)attheframelevelto prompt encoder during training. Under the guidance of
generatedistinctproposalsfordifferentreferringsentences. the prompt encoder, our proposed TextCon enforces the
Apart from the text ambiguity, the sentence descriptions distinctness of the proposals while enhancing the position
in referring video object segmentation often contain long- promptstobetext-aware.
termmotionsoractions. Sentenceslike“agoldfishonthe
left swimming towards the top right” require considering Modality-Contrastive Prompt Learning. In addition
all the frames as a whole to perform video segmentation. to the prompt embedding pi derived in Text-Contrastive
t
Toalignthetextwiththereferredobjectatthevideolevel, PromptLearning, wealsoutilizetheimageencodertoex-
we uniquely present Modality-Contrastive Prompt Learn- tract the visual features f . With the cross-attention per-
t
ing(ModalCon). Thelearningschemeisdetailedbelow. formed at each frame by taking the prompt embedding pi
t
asthequeryandvisualfeaturesf askeysandvalues, fol-
t
Text-Contrastive Prompt Learning. Formally, in addi- lowedbyanaveragepoolinglayerfortemporalaggregation,
tiontotheinputsentenceS , weforwardanothersentence thevideo-levelcontentfeaturefiwouldbeencodedforthe
i
Sj throughourGroPromptframeworktoobtaintheoutput referredobject.AsforthereferringsentencesSiandSj,we
proposal Bj for another object at each frame. To perform derivethesentence-levellinguisticfeatureszi andzj from
t
contrastivelearning, weleveragethepromptencoderfrom the text encoder. Then, the video-level triplet contrastive
the foundation segmentation models to extract the prompt lossLv wouldbecomputedasfollows:
contra
embeddingspi,pj,andpˆi fortheproposalsBi andBj and
theground-trut thbt oundint gboxBˆi,respectivelt
y.
Bytat
king
Lv
contra
=E V,Si,Sj[max(0,dp−dn)],
t (3)
pi, pˆi, and pj as the anchor, positive, and negative sam- where dp =∥fi−zi∥ and dn =∥fi−zj∥ .
t t t 2 2
ple, the frame-level triplet contrastive loss Lf would
contra Notethattheprompt,image,andtextencodersareallfrozen
becomputedasfollows:
duringtrainingtopreservetheirpretrainedsemanticspaces
(cid:34) (cid:88)T (cid:35) whileavoidingoverfitting.
Lf =E max(0,dp−dn) ,
contra V,Si,Sj t t Finally,wedefinethetotallossfunctionL as:
(2) total
t=1
where dp t =∥pi t−pˆi t∥ 2 and dn t =∥pi t−pj t∥ 2. L total =L box+L contra, (4)Ref-YouTube-VOS Ref-DAVIS17
Method Publication Referring&VideoTrainingData
J&F J F J&F J F
URVOS[37] ECCV’20 RefYT 47.2 45.3 49.2 51.5 47.3 56.0
MTTR[1] CVPR’22 RefYT 55.3 54.0 56.6 - - -
ReferFormer[42] CVPR’22 RefC,RefYT 62.9 61.3 64.6 61.1 58.1 64.1
MANet[7] ACMMM’22 RefYT 55.6 54.8 56.5 - - -
LOCATER[24] TPAMI’23 RefYT 56.5 54.8 58.1 - - -
VLT[12] TPAMI’23 RefC,RefYT 63.8 61.9 65.6 61.6 58.9 64.3
R2-VOS[22] ICCV’23 RefC,RefYT 61.3 59.6 63.1 - - -
HTML[15] ICCV’23 RefC,RefYT 63.4 61.5 65.2 62.1 59.2 65.1
OnlineRefer[41] ICCV’23 RefC,RefYT 63.5 61.6 65.5 64.8 61.6 67.7
SgMg[30] ICCV’23 RefC,RefYT 65.7 63.9 67.4 63.3 60.6 66.0
TempCD[38] ICCV’23 RefC,RefYT 65.8 63.6 68.0 64.6 61.6 67.6
SOC[27] NeurIPS’23 RefC,RefYT 67.3 65.3 69.3 65.8 62.5 69.1
LoSh[55] arXiv’23 RefC,RefYT 64.2 62.5 66.0 62.5 59.5 65.4
RefSAM[23] arXiv’23 RefC,RefYT 62.1 60.9 63.3 69.5 65.9 73.2
EPCFormer[4] arXiv’23 RefYT,AVOS 65.0 62.9 67.2 - - -
UniNEXT[47] CVPR’23 RefC,RefYT,G,La,T,YT,B,V,O 66.2 64.0 68.4 66.7 62.3 71.1
DEVA[8] ICCV’23 RefC,RefYT,YT,D,O 66.0 - - 66.3 - -
UniRef[44] ICCV’23 RefC,RefYT,RefD,YT,O,LV 67.4 65.5 69.2 66.3 62.9 69.7
MUTR[48] arXiv’23 RefC,RefYT,AVSB 68.4 66.4 70.4 68.0 64.8 71.3
WRVOS[56] arXiv’23 RefYT(box+1st-framemask) 46.6 45.6 47.6 47.3 44.6 50.0
Grounded-SAM[25] arXiv’23 RefC(box) 62.3 61.0 63.6 65.2 62.3 68.0
GroPrompt(Ours) - RefC(box),RefYT(box) 65.5 64.1 66.9 70.6 67.8 73.3
Table 1. Quantitative comparison to state-of-the-art methods on the validation split of Ref-YouTube-VOS and Ref-DAVIS17. RefYT:
Ref-YouTube-VOS,RefD:Ref-DAVIS,RefC:RefCOCO[29,54],AVOS:Audio-VOS[32],AVSB:AVSBench[57],YT:YouTube-VOS
2019[46],D:DAVIS17[33],O:OccludedVIS[34],LV:Long-termVOS[16],G:GOT-10K[17],La: LaSOT[13],T:TrackingNet[31],
B:BDD100K[53],V:VIS19[50].
whereL =λ Lf +λ Lv ,andλ andλ are more than 1,500 expressions. A2D Sentences [14] and J-
contra f contra v contra f v
hyper-parametersforthetwocontrastiveloss,respectively. HMDBSentences[14]areextendedfromtheA2D[45]and
With the proposed TAP-CL, our GroPrompt framework J-HMDB[18]datasetswithsentencesdescribingtheactors
would produce temporal-consistent yet text-aware bound- andactionsappearinginthevideocontent. A2DSentences
ing box proposals, allowing video segmentation by taking contains3,036trainingvideosand746testingvideoswith
the learned proposals to prompt image-based foundation atotalof6,656sentences, whileJ-HMDBSentencescon-
segmentation models. It is worth repeating that, the above tains 928 video clips of 21 different actions and 928 sen-
learning scheme does not require any dense mask annota- tences.
tions. Furthermore, our proposed GroPrompt framework
learns to prompt instead of finetuning foundation models, EvaluationMetrics. FortheRef-Youtube-VOSandRef-
enabling efficient adaptation to referring video object seg- DAVIS datasets, we follow the standard protocol and
17
mentationfromweaksupervision. adoptthefollowingevaluationmetrics: regionsimilarityJ
(averageIoU),contouraccuracyF (averageboundarysim-
4.Experiments
ilarity),andtheirmeanvalueJ&F. Sincetheannotations
oftheRef-Youtube-VOSvalidationsetarenotpubliclyre-
4.1.DatasetsandEvaluationMetrics
leased,weevaluatetheresultsontheofficialserver. Asfor
Datasets. We conduct experiments on four RVOS Ref-DAVIS , we use the official code for evaluation. For
17
benchmark datasets: Refer-Youtube-VOS [37], Refer- A2D Sentences and J-HMDB Sentences, we adopt Preci-
DAVIS [19], A2D Sentences [14], and J-HMDB Sen- sion@K,OverallIoU,andMeanIoUforevaluation. Over-
17
tences[14]. Refer-Youtube-VOSisalarge-scaledatasetfor all IoU is the ratio between the total intersection and the
RVOS, with 3,975 videos, 7,451 objects, and 27,899 ex- union area over all the testing data, and Mean IoU is the
pressions. Refer-DAVIS is augmented from the popular averagedIoUoverthetestingdata. Precision@Kmeasures
17
video object segmentation dataset, DAVIS [2]. It con- thepercentageoftestingdatawithIoUscorehigherthana
17
tains 90 videos (60 for training and 30 for testing) with thresholdK,whereK∈[0.5,0.6,0.7,0.8,0.9].Method Publication P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 OverallIoU MeanIoU
CMSA+CFSA[52] TPAMI’21 48.7 43.1 35.8 23.1 5.2 61.8 43.2
MTTR[1] CVPR’22 75.4 71.2 63.8 48.5 16.9 72.0 64.0
ReferFormer[42] CVPR’22 83.1 80.4 74.1 57.9 21.2 78.6 70.3
LOCATER[24] TPAMI’23 70.9 64.0 52.5 35.1 10.1 69.0 59.7
TempCD[38] ICCV’23 - - - - - 76.6 68.6
HTML[15] ICCV’23 84.0 81.5 75.8 59.2 22.8 79.5 71.2
OnlineRefer[41] ICCV’23 83.1 80.2 73.4 56.8 21.7 79.6 70.5
LoSh[55] arXiv’23 85.3 80.7 74.3 57.7 21.1 78.9 71.3
WRVOS[56] arXiv’23 62.9 58.0 49.8 35.0 11.0 66.3 53.9
Grounded-SAM[25] arXiv’23 80.8 78.7 73.9 59.8 23.8 71.0 68.9
GroPrompt(Ours) - 83.9 81.5 75.9 60.5 23.4 77.3 71.3
Table2.ThequantitativeevaluationonA2D-Sentences,withPrecision@K,OverallIoUandMeanIoU.
Method Publication P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 OverallIoU MeanIoU
CMSA+CFSA[52] TPAMI’21 76.4 62.5 38.9 9.0 0.1 62.8 58.1
MTTR[1] CVPR’22 93.9 85.2 61.6 16.6 0.1 70.1 69.8
ReferFormer[42] CVPR’22 96.2 90.2 70.2 21.0 0.3 73.0 71.8
LOCATER[24] TPAMI’23 89.3 77.2 50.8 10.6 0.2 67.3 66.3
TempCD[38] ICCV’23 - - - - - 70.6 69.6
OnlineRefer[41] ICCV’23 96.1 90.4 71.0 21.9 0.2 73.5 71.9
SgMg[30] ICCV’23 - - - - - 73.7 72.5
SOC[27] NeurIPS’23 96.9 91.4 71.1 21.3 0.1 73.6 72.3
LoSh[55] arXiv’23 96.3 90.1 70.4 20.5 0.2 73.2 72.5
EPCFormer[4] arXiv’23 97.6 93.1 72.6 23.0 0.0 74.0 73.1
WRVOS[56] arXiv’23 82.0 67.3 41.4 8.9 0.1 63.2 62.7
Grounded-SAM[25] arXiv’23 95.3 89.4 70.0 23.1 0.3 71.9 71.7
GroPrompt(Ours) - 96.8 90.8 71.0 23.2 0.3 73.3 72.4
Table3.ThequantitativeevaluationonJHMDB-Sentences,withPrecision@K,OverallIoUandMeanIoU.
4.2.ImplementationDetails art methods on Refer-Youtube-VOS [37] and Refer-
DAVIS [19]. AsshowninTable1, weseethatourGro-
We follow from [41, 42] to train our model on the Ref- 17
Prompt framework achieves 65.5% and 70.6% in J&F
YouTube-VOSdataset,anddirectlyevaluatebythevalida-
on Refer-Youtube-VOS and Refer-DAVIS , respectively.
tionsetprovidedbyRef-YouTube-VOSandRef-DAVIS . 17
17
ComparedwithRefSAM[4],ourGroPromptframeworkis
Forourdetailedmodelarchitecture,ourimage-textencoder
3.4% and 1.1% higher on the two datasets. This validates
comprises Swin-Transformer [26] for the image features
that our learned position prompts would properly instruct
andBERT[11]forthetextfeatures. Besides,wesetupour
foundationsegmentationmodelstoperformreferringvideo
cross-modality decoder with 6 cross-attention transformer
object segmentation. While UniRef [44] and MUTR [48]
layers.Forthesegmentationpart,wetakeSAMasourmain
achieve competitive performance on Refer-Youtube-VOS,
segmentor to take our special text-aware position prompt
these methods require large-scale referring or video data
as input. Thus, the prompt encoder, image encoder, and
for training. Compared to WRVOS [56], which observes
maskdecoderarefollowedbySAMinoursetting. Weset
the learning rate to 0.0001 and train our framework for 12 box-levelsupervisionplusthemaskannotationforthefirst
epochs. Following [25], we set λ and λ as 5 and 2 re- frame, ourGroPromptframeworkisover20%higherwith
r g
spectively. Asforλ andλ ,weuse0.01and0.1onRef- box-levelsupervisiononly. Similarresultsareobservedon
f v
Youtube-VOSand0.0001and0.001onA2DSentences,re- A2D Sentences [14] and J-HMDB Sentences [14]. In Ta-
ble 2, our method reports 71.3% in Mean IoU. As for J-
spectively. We implement our framework in PyTorch and
trainthemodelon8NVIDIAV100GPUs.
HMDBSentences,weachieve72.4%inTable3.
4.3.QuantitativeandQualitativeComparisons In Figure 2 and 3, we also provide qualitative com-
To evaluate our proposed GroPrompt framework, we parisons with ReferFormer [42] and OnlineRefer [41] on
first provide quantitative comparisons with state-of-the- Refer-DAVIS andRefer-Youtube-VOS[37]. Weobserve
17Expression:Aman in the middle wearing a blue belt teaching judo
Figure2. Qualitativecomparisonsofthestate-of-the-artmethodsonRefer-DAVIS ,where“GT-bbox+SAM”representstheresultby
17
takingground-truthboundingboxestopromptSAM.
Figure3.Qualitativecomparisonsofthestate-of-the-artmethodsonRefer-Youtube-VOS.
that, our method outperforms OnlineRefer and the pro- boundingboxes. Fromtheaboveexperiments, wevalidate
ducedboundingboxproposalsareclosetotheground-truth that our proposed GroPrompt framework would produce
tupnI
remroFrefeR
refeRenilnO
sruO
MAS
+
xobb
TGMethod Weaksup. Online Decoupled Addi.TrainingVideos Method Box J&F J F
ReferFormer[42] NoNeed
WRVOS[56] ✓ NoNeed Grounded-SAM[25] 69.9 65.2 62.3 68.0
OnlineRefer[41] ✓ NoNeed GroPrompt(L boxonly) 73.2 69.8 67.0 72.7
DEVA[8] ✓ YT,D,O GroPrompt(L +L ) 74.4 70.6 67.8 73.3
box contra
GroPrompt(Ours) ✓ ✓ ✓ NoNeed
GTBox+SAM(upperbound) 100.0 83.6 80.1 87.2
Table4. SettingcomparisonswithrecentRVOSmethods. “Weak
sup.”:Trainedmainlywithbox-levelweaksupervisions,“Online”: Table6. AblationstudiesofthelossfunctionsonRef-DAVIS17.
Onlinemethodratherthanofflinemethod,“Decoupled”: Decou- “Box”:IoUscoresoftheboundingboxes(positionprompts).
pledsegmentationinsteadofend-to-endtraining,“Addi. Training
4.5.AblationStudies
Videos”: Additional video datasets for training. YT: YouTube-
VOS2019[46],D:DAVIS17[33],O:OccludedVIS[34]. To verify the effectiveness of our proposed loss functions,
we conduct ablation studies by taking the ground-truth
Method #oftrainableparameters Ref-YTVOS Ref-DAVIS boundingboxestocomputetheIoUscoresofthepredicted
ReferFormer[42] ∼112M 62.9 61.1 box proposals on Ref-DAVIS17. From Table 6, we see
OnlineRefer[41] ∼221M 63.5 64.8 that when only L is considered, the box and segmen-
DEVA[8] ∼112M 66.0 66.3 box
tation score J&F would improve 3.3% and 4.6% com-
GroPrompt(Ours) ∼15M 65.5 70.6
pared to Grounded-SAM [25]. If we further apply our
Table5.EfficiencycomparisonswithrecentRVOSmethods,along proposed L to perform contrastive learning at frame
contra
withtheJ&F scoresonRef-YouTube-VOSandRef-DAVIS17. levelandvideolevel,theboxandsegmentationscorewould
improve to 74.4% and 70.6%, which are 1.2% and 0.8%
positionpromptsfromweaksupervision,enablingefficient higher.Finally,ifwedirectlytaketheground-truthboxesto
adaptationofimage-basedfoundationsegmentationmodels promptSAM,thesuperiorperformanceof83.6%inJ&F
foraddressingreferringvideoobjectsegmentation. wouldbeobserved. Thisdemonstratesthatimagesegmen-
tation could be mostly solved by SAM, and therefore how
4.4.SettingandEfficiencyComparisons to generate proper prompts to instruct foundation segmen-
tation models for referring segmentation tasks would now
In Table 4, we compare the setting of our proposed Gro-
be of interest. From the above experiments, we confirm
Prompt framework with recent RVOS methods. From this
that our proposed loss functions would learn precise posi-
table, weseethatWRVOS[56]attemptstoaddressRVOS
tion prompts (box proposals) from the referring sentence
from box-level weak supervision plus the ground-truth
and the input video, allowing efficient adaptation of foun-
mask for the first frame, while OnlineRefer [41] extends
dationmodelsforaddressingRVOS.
ReferFormer [42] with query propagation to handle ongo-
ing videos under the online setting. However, these meth- 5.Conclusion
ods require end-to-end training for vision-language mod- In this work, we propose the Grounded
els, which could be computationally expensive and time- Prompting (GroPrompt) framework to efficiently adapt
consuming. On the other hand, assuming that additional foundation segmentation models for addressing RVOS
videodataareaccessible, DEVA[8]decouplesRVOSinto from weak supervision. More specifically, we propose
image segmentation and temporal propagation to increase Text-Aware Prompt Contrastive Learning (TAP-CL) to
the scalability. Compared to these works, our proposed enhance the association between the position prompts
GroPromptframeworkdecouplesRVOSintoproposalgen- and the referring sentences with only box supervisions,
eration and prompted segmentation with no need for addi- includingText-ContrastivePromptLearning(TextCon)and
tional video data for training. In this decoupled manner, Modality-Contrastive Prompt Learning (ModalCon) at
ourframeworkcanlearnproperpromptsfromweaksuper- frame level and video level, respectively. With the pro-
vision for foundation segmentation models and could also posed TAP-CL, our GroPrompt framework can generate
beappliedtoonlinesettings. temporal-consistent yet text-aware position prompts de-
InTable5,wealsoprovideefficiencycomparisonswith scribing locations and movements for the referred object
recentworks. Weseethatthenumberoftrainableparame- from the video. With no need of additional finetuning for
tersofourmethodisover7timesfewerthanDEVA.Thisis foundation segmentation models, we are able to produce
becausethatourproposedGroPromptframeworklearnsto precise masks for the referred object in the video. The
promptfoundationmodelsforefficientadaptationinsteadof experimental results in the standard RVOS benchmarks
trainingavision-languagemodelend-to-end.Togetherwith (Ref-YouTube-VOS, Ref-DAVIS17, A2D-Sentences, and
thequantitativecomparisonsinTable1,wevalidatethatour JHMDB-Sentences) demonstrate the competitive perfor-
proposed GroPrompt framework is preferable in terms of mance of our proposed GroPrompt framework given only
performance,setting,andefficiency. boundingboxweaksupervision.References computervisionandpatternrecognition,pages5374–5383,
2019. 5
[1] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.
[14] KirillGavrilyuk,AmirGhodrati,ZhenyangLi,andCeesGM
End-to-endreferringvideoobjectsegmentationwithmulti-
Snoek.Actorandactionvideosegmentationfromasentence.
modaltransformers. InCVPR,pages4985–4995,2022. 5,
InCVPR,2018. 5,6
6
[15] Mingfei Han, Yali Wang, Zhihui Li, Lina Yao, Xiaojun
[2] Sergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis,
Chang, and Yu Qiao. Html: Hybrid temporal-scale mul-
Yuhua Chen, Luc Van Gool, Federico Perazzi, and Jordi
timodal learning framework for referring video object seg-
Pont-Tuset. The2018davischallengeonvideoobjectseg-
mentation. In Proceedings of the IEEE/CVF International
mentation. arXivpreprintarXiv:1803.00557,2018. 5
ConferenceonComputerVision,pages13414–13423,2023.
[3] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas
1,2,3,5,6
Usunier,AlexanderKirillov,andSergeyZagoruyko.End-to-
[16] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang,
endobjectdetectionwithtransformers. InEuropeanconfer-
Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos:
enceoncomputervision,pages213–229.Springer,2020. 3
A benchmark for long-term video object segmentation. In
[4] Jiajun Chen, Jiacheng Lin, Zhiqiang Xiao, Haolong Fu, ProceedingsoftheIEEE/CVFInternationalConferenceon
Ke Nai, Kailun Yang, and Zhiyong Li. Epcformer: ComputerVision,pages13480–13492,2023. 5
expression prompt collaboration transformer for univer-
[17] LianghuaHuang,XinZhao,andKaiqiHuang. Got-10k: A
sal referring video object segmentation. arXiv preprint
largehigh-diversitybenchmarkforgenericobjecttrackingin
arXiv:2308.04162,2023. 5,6
thewild.IEEEtransactionsonpatternanalysisandmachine
[5] Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, intelligence,43(5):1562–1577,2019. 5
WenyuanLi,ZhengxiaZou,andZhenweiShi. Rsprompter: [18] Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia
Learning to prompt for remote sensing instance segmenta- Schmid, and Michael J Black. Towards understanding ac-
tionbasedonvisualfoundationmodel,2023. 2 tionrecognition. InProceedingsoftheIEEEinternational
[6] Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao, conferenceoncomputervision,pages3192–3199,2013. 5
ShangzhanZhang,YanWang,ZejianLi,LingyunSun,Papa [19] Anna Khoreva, Anna Rohrbach, and Brent Schiele. Video
Mao,andYingZang.Samfailstosegmentanything?–sam- object segmentation with referring expressions. In Pro-
adapter: Adaptingsaminunderperformedscenes: Camou- ceedings of the European Conference on Computer Vision
flage,shadow,andmore,2023. 2 (ECCV)Workshops,pages0–0,2018. 5,6
[7] Weidong Chen, Dexiang Hong, Yuankai Qi, Zhenjun Han, [20] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
ShuhuiWang,LaiyunQing,QingmingHuang,andGuorong ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
Li. Multi-attentionnetworkforcompressedvideoreferring head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
object segmentation. In Proceedings of the 30th ACM In- thing. arXivpreprintarXiv:2304.02643,2023. 1,2,3
ternational Conference on Multimedia, pages 4416–4425, [21] Guanghui Li, Mingqi Gao, Heng Liu, Xiantong Zhen, and
2022. 5 Feng Zheng. Learning cross-modal affinity for referring
[8] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexan- video object segmentation targeting limited samples. In
der Schwing, and Joon-Young Lee. Tracking anything ProceedingsoftheIEEE/CVFInternationalConferenceon
with decoupled video segmentation. In Proceedings of the ComputerVision,pages2684–2693,2023. 1
IEEE/CVF International Conference on Computer Vision, [22] XiangLi,JingluWang,XiaohaoXu,XiaoLi,BhikshaRaj,
pages1316–1326,2023. 5,8 andYanLu.Robustreferringvideoobjectsegmentationwith
[9] Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, cyclicstructuralconsensus.InProceedingsoftheIEEE/CVF
TianbinLi,HaoyuWang,YanzhouSu,ZiyanHuang,Jilong InternationalConferenceonComputerVision,pages22236–
Chen, Lei Jiangand Hui Sun, Junjun He, Shaoting Zhang, 22245,2023. 5
MinZhu,andYuQiao. Sam-med2d,2023. 2 [23] Yonglin Li, Jing Zhang, Xiao Teng, and Long Lan.
[10] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Refsam: Efficiently adapting segmenting anything model
ZongxinYang,WenguanWang,andYiYang. Segmentand for referring video object segmentation. arXiv preprint
trackanything. arXivpreprintarXiv:2305.06558,2023. 1,2 arXiv:2307.00997,2023. 1,3,5
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina [24] Chen Liang, Wenguan Wang, Tianfei Zhou, Jiaxu Miao,
Toutanova. Bert: Pre-training of deep bidirectional Yawei Luo, and Yi Yang. Local-global context aware
transformers for language understanding. arXiv preprint transformerforlanguage-guidedvideosegmentation. IEEE
arXiv:1810.04805,2018. 6 TransactionsonPatternAnalysisandMachineIntelligence,
[12] HenghuiDing,ChangLiu,SuchenWang,andXudongJiang. 2023. 5,6
Vlt: Vision-language transformer and query generation for [25] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
referringsegmentation. IEEETransactionsonPatternAnal- Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun
ysisandMachineIntelligence,2022. 5 Zhu, etal. Groundingdino: Marryingdinowithgrounded
[13] HengFan,LitingLin,FanYang,PengChu,GeDeng,Sijia pre-training for open-set object detection. arXiv preprint
Yu,HexinBai,YongXu,ChunyuanLiao,andHaibinLing. arXiv:2303.05499,2023. 1,3,5,6,8
Lasot: Ahigh-qualitybenchmarkforlarge-scalesingleob- [26] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng
jecttracking.InProceedingsoftheIEEE/CVFconferenceon Zhang, Stephen Lin, and Baining Guo. Swin transformer:Hierarchical vision transformer using shifted windows. In [38] JiajinTang,GeZheng,andSibeiYang. Temporalcollection
Proceedings of the IEEE/CVF international conference on anddistributionforreferringvideoobjectsegmentation. In
computervision,pages10012–10022,2021. 6 ProceedingsoftheIEEE/CVFInternationalConferenceon
[27] Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yi- ComputerVision,pages15466–15476,2023. 1,2,3,5,6
tong Wang, Yansong Tang, Xiu Li, and Yujiu Yang. Soc: [39] Di Wang, Jing Zhang, Bo Du, Minqiang Xu, Lin Liu,
Semantic-assisted object cluster for referring video object Dacheng Tao, and Liangpei Zhang. SAMRS: Scaling-up
segmentation. arXiv preprint arXiv:2305.17011, 2023. 1, remotesensingsegmentationdatasetwithsegmentanything
2,3,5,6 model. InThirty-seventhConferenceonNeuralInformation
[28] JunMa,YutingHe,FeifeiLi,LinHan,ChenyuYou,andBo ProcessingSystemsDatasetsandBenchmarksTrack,2023.
Wang. Segmentanythinginmedicalimages. arXivpreprint 2
arXiv:2304.12306,2023. 2 [40] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,
[29] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Chunhua Shen, and Tiejun Huang. Seggpt: Towards seg-
Camburu, Alan L Yuille, and Kevin Murphy. Generation menting everything in context. In Proceedings of the
andcomprehensionofunambiguousobjectdescriptions. In IEEE/CVF International Conference on Computer Vision,
ProceedingsoftheIEEEconferenceoncomputervisionand 2023. 1,3
patternrecognition,pages11–20,2016. 5 [41] Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu
[30] Bo Miao, Mohammed Bennamoun, Yongsheng Gao, and Zhang, and Jianbing Shen. Onlinerefer: A simple online
Ajmal Mian. Spectrum-guided multi-granularity referring baselineforreferringvideoobjectsegmentation.InProceed-
videoobjectsegmentation. InProceedingsoftheIEEE/CVF ings of the IEEE/CVF International Conference on Com-
International Conference on Computer Vision, pages 920– puterVision,pages2761–2770,2023. 1,2,3,5,6,8
930,2023. 1,2,3,5,6
[42] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping
[31] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-
Luo. Language as queries for referring video object seg-
subaihi, and Bernard Ghanem. Trackingnet: A large-scale
mentation. In Proceedings of the IEEE/CVF Conference
dataset and benchmark for object tracking in the wild. In
on Computer Vision and Pattern Recognition, pages 4974–
ProceedingsoftheEuropeanconferenceoncomputervision
4984,2022. 1,2,3,5,6,8
(ECCV),pages300–317,2018. 5
[43] Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei
[32] Wenwen Pan, Haonan Shi, Zhou Zhao, Jieming Zhu, Xi-
Wang,YanwuXu,YuemingJin,andTalArbel.Medicalsam
uqiang He, Zhigeng Pan, Lianli Gao, Jun Yu, Fei Wu, and
adapter: Adaptingsegmentanythingmodelformedicalim-
QiTian. Wnet:Audio-guidedvideoobjectsegmentationvia
agesegmentation. arXivpreprintarXiv:2304.12620, 2023.
wavelet-basedcross-modaldenoisingnetworks. InProceed-
2
ingsoftheIEEE/CVFConferenceonComputerVisionand
[44] JiannanWu,YiJiang,BinYan,HuchuanLu,ZehuanYuan,
PatternRecognition,pages1320–1331,2022. 5
andPingLuo. Segmenteveryreferenceobjectinspatialand
[33] FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams,Luc
temporalspaces. InProceedingsoftheIEEE/CVFInterna-
VanGool,MarkusGross,andAlexanderSorkine-Hornung.
tional Conference on Computer Vision, pages 2538–2550,
Abenchmarkdatasetandevaluationmethodologyforvideo
2023. 1,2,3,5,6
objectsegmentation. InProceedingsoftheIEEEconference
[45] Chenliang Xu, Shao-Hang Hsieh, Caiming Xiong, and Ja-
oncomputervisionandpatternrecognition,pages724–732,
son J Corso. Can humans fly? action understanding with
2016. 5,8
multipleclassesofactors. InProceedingsoftheIEEEcon-
[34] JiyangQi,YanGao,YaoHu,XinggangWang,XiaoyuLiu,
ference on computer vision and pattern recognition, pages
XiangBai,SergeBelongie,AlanYuille,PhilipHSTorr,and
2264–2273,2015. 5
SongBai. Occludedvideoinstancesegmentation: Abench-
mark. International Journal of Computer Vision, 130(8): [46] NingXu,LinjieYang,YuchenFan,DingchengYue,Yuchen
2022–2039,2022. 5,8 Liang, Jianchao Yang, and Thomas Huang. Youtube-vos:
Alarge-scalevideoobjectsegmentationbenchmark. arXiv
[35] Frano Rajicˇ, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Mar-
preprintarXiv:1809.03327,2018. 5,8
tinDanelljan,andFisherYu. Segmentanythingmeetspoint
tracking. arXiv:2307.01197,2023. 2 [47] BinYan,YiJiang,JiannanWu,DongWang,PingLuo,Ze-
[36] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir huan Yuan, and Huchuan Lu. Universal instance percep-
Sadeghian, Ian Reid, and Silvio Savarese. Generalized in- tion as object discovery and retrieval. In Proceedings of
tersection over union: A metric and a loss for bounding theIEEE/CVFConferenceonComputerVisionandPattern
boxregression. InProceedingsoftheIEEE/CVFconference Recognition,pages15325–15336,2023. 5
oncomputervisionandpatternrecognition,pages658–666, [48] Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei
2019. 3 Zhang, Hongyang Li, Yu Qiao, Zhongjiang He, and Peng
[37] SeongukSeo, Joon-YoungLee, andBohyungHan. Urvos: Gao. Referred by multi-modality: A unified temporal
Unifiedreferringvideoobjectsegmentationnetworkwitha transformer for video object segmentation. arXiv preprint
large-scale benchmark. In Computer Vision–ECCV 2020: arXiv:2305.16318,2023. 5,6
16th European Conference, Glasgow, UK, August 23–28, [49] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing
2020, Proceedings, Part XV 16, pages 208–223. Springer, Wang,andFengZheng. Trackanything: Segmentanything
2020. 1,5,6 meetsvideos,2023. 2[50] LinjieYang,YuchenFan,andNingXu. Videoinstanceseg-
mentation. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pages5188–5197,2019. 5
[51] ZongxinYangandYiYang. Decouplingfeaturesinhierar-
chicalpropagationforvideoobjectsegmentation. Advances
inNeuralInformationProcessingSystems,35:36324–36336,
2022. 2
[52] LinweiYe, MrigankRochan, ZhiLiu, XiaoqinZhang, and
Yang Wang. Referring segmentation in images and videos
withcross-modalself-attentionnetwork. TPAMI,2021. 6
[53] FisherYu,HaofengChen,XinWang,WenqiXian,Yingying
Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-
rell. Bdd100k: Adiversedrivingdatasetforheterogeneous
multitask learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
2636–2645,2020. 5
[54] LichengYu,PatrickPoirson,ShanYang,AlexanderCBerg,
andTamaraLBerg. Modelingcontextinreferringexpres-
sions. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14,
2016,Proceedings,PartII14,pages69–85.Springer,2016.
5
[55] Linfeng Yuan, Miaojing Shi, and Zijie Yue. Losh: Long-
shorttextjointpredictionnetworkforreferringvideoobject
segmentation. arXivpreprintarXiv:2306.08736,2023. 5,6
[56] Wangbo Zhao, Kepan Nan, Songyang Zhang, Kai Chen,
Dahua Lin, and Yang You. Learning referring video ob-
ject segmentation from weak annotation. arXiv preprint
arXiv:2308.02162,2023. 5,6,8
[57] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun,
Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong,
MengWang,andYiranZhong. Audio–visualsegmentation.
In European Conference on Computer Vision, pages 386–
403.Springer,2022. 5
[58] XueyanZou,JianweiYang,HaoZhang,FengLi,LinjieLi,
JianfengGao,andYongJaeLee.Segmenteverythingevery-
whereallatonce. arXivpreprintarXiv:2304.06718, 2023.
1,3