TRACKING THE PERSPECTIVES OF
INTERACTING LANGUAGE MODELS
HaydenHelm† BrandonDuderstadt YoungserPark
NomicAI NomicAI CenterforImagingSciences
JohnsHopkinsUniversity
CareyE.Priebe
Dept. ofAppliedMath. &Statistics
JohnsHopkinsUniversity
ABSTRACT
Largelanguagemodels(LLMs)arecapableofproducinghighqualityinformation
atunprecedentedrates. Asthesemodelscontinuetoentrenchthemselvesinsoci-
ety,thecontenttheyproducewillbecomeincreasinglypervasiveindatabasesthat
are,inturn,incorporatedintothepre-trainingdata,fine-tuningdata,retrievaldata,
etc. ofotherlanguagemodels. Inthispaperweformalizetheideaofacommuni-
cationnetworkofLLMsandintroduceamethodforrepresentingtheperspective
ofindividualmodelswithinacollectionofLLMs. Giventhesetoolswesystem-
atically study information diffusion in the communication network of LLMs in
varioussimulatedsettings.
…
…
…
(a)Fullyconnected. (b)Intra-classonly. (c)Vulnerable. (d)General.
Figure1:Examplesofcommunicationnetworksoflanguagemodelsanddatabases. Theedgestruc-
ture and model intitializations directly impact the evolution of the perspectives of the models and
theoverallhealthofthesystem.
1 INTRODUCTION
Thesuccessoflargepre-trainedmodelsinnaturallanguageprocessing(Devlinetal.,2018),com-
puter vision (Oquab et al., 2023), signal processing (Radford et al., 2023), among other domains
(Jumperetal.,2021)acrossvariouscomputingandhumanbenchmarkshasbroughtthemtothefore-
frontofthetechnology-centricworld. Giventheirabilitytoproducehuman-expertlevelresponses
foralargesetofknowledge-basedquestions(Touvronetal.,2023;Achiametal.,2023),thecontent
theyproduceisoftenpropagatedthroughoutforumsthathaveinfluenceoverothermodelsandhu-
manusers(Brinkmannetal.,2023). Assuch, itisimportanttodevelopsufficientframeworksand
complementarytoolstounderstandhowinformationproducedbythesemodelsaffectsthebehavior
of other models and human users. We refer to a system where a model can potentially influence
othermodelsasasystemofinteractinglanguagemodels.
Beyondtheir ability toinfluenceinformationon human-modelforums, systemsofinteracting lan-
guage models are interesting in their own right – insofar as an individual model is an intriguing
proxyforanindividualhuman(Helmetal.,2023;Kosinski,2023),asystemofinteractinglanguage
modelsisanintriguingproxyforhumancommunities. Systemsofinteractinglanguagemodelsare
†correspondingauthor;[first-name]@nomic.ai
1
4202
nuJ
71
]IA.sc[
1v83911.6042:viXrathus an alluring alternative or complement to studying human communities in the social sciences.
Forexample,itisofteninfeasibleorunethicaltosubjectentirecommunitiestodifferentinformation
paradigms to understand how individuals within the community – as well as the community itself
–changeinresponsetoanintervention. Theseissuesarelessprominentforsystemsofinteracting
languagemodels.Further,thereispotentialforgreatercontrolincommunitymembershipandcross-
communityinteractions,whichmayimprovereproducibilityandmitigatetheeffectsofsociological
confounders.
Inthispaper,westudyinformationdiffusioninasystemofinteractinglanguagemodels.Theframe-
work and methods that we develop can be applied to monitoring information diffusion in human-
modelforumsandtothetreatmentofsystemsofinteractinglanguagemodelsquantitativelyasproxy
human communities. The current standard (Perez et al., 2024) for studying information diffusion
in a system of interacting language models requires i) parameterizing models with different sys-
temprompts,contexts,weights,orcollectionsofdata,ii)providinganenvironmentortemplatefor
model-to-modelormodel-to-datasetinteractions, andiii)analyzinghowtheoutputsofthemodels
changeafterasequenceofinteractions.
Forexample, researchersincludedescriptionsofdesiredmodelbehaviororpersonalityinthesys-
tem prompt – e.g., “You have opinion A” is included in the system prompt for model 1 and “You
haveopinionB”isincludedinthesystempromptformodel2,etc. –topromotediversityinmodel
response(Parketal.,2023;Chuangetal.,2023;Papachristou&Yuan,2024). Whiletheintended
model response diversity is achieved, previous studies have failed to quantitatively assess the ef-
fectofdifferentmodelinitializationsand, instead, relyonqualitativechecks. Similarly, analyzing
changesinmodelresponsesasthesystemevolveshaspreviouslybeenlimitedtohumaninspection
of responses (Park et al., 2023), or classification of responses into a few classes (Chuang et al.,
2023).
We introduce the perspective space of a collection of models to address the gap in quantitative
methods for studying the diversity and evolution of model responses. The perspective space is an
embedding-based representation of a collection of models designed to capture the relative differ-
encesinmodelresponsesforafixedsetofprompts. Themethodcanbeusedtostudyinformation
diffusionandgeneralsystemdynamicsbyqueryingeachmodelwiththesamesetofqueriesateach
timestep. Todemonstratetheeffectivenessoftheperspectivespaceforunderstandingmodel-level
diversityandforanalyzingmodel-levelandsystemdynamics,weformalizethesystemofinteract-
inglanguagemodelsasagraph.Theformalizationenablessystematicstudyoftheeffectofdifferent
communicationstructuresoninformationdiffusionthatisotherwisenotpossible.
Ourcontributionistwo-fold: i)Wemodelasystemofinteractinglanguagemodelsasagraphand
systematically study the effect of different communication structures on information diffusion. ii)
Weintroducetheperspectivespaceasamethodtoquantitativelyanalyzeinformationdiffustionina
populationoflanguagemodels.
2 A COMMUNICATION NETWORK OF LLMS
Consider a system that consists of a collection of language of models F = {f ,...,f } and
1 n
databases D = {D ,...,D }. Given a set of prompts X, systems deploying model f ∈ F may
1 n′
usethedatabaseD ∈D–viafine-tuning,contextretrieval,etc. –toproducemorerelevantoutputs
withrespecttoX. Theoutputsoftheupdatedmodelmaybeusedtoupdatea(potentiallydifferent)
databaseD′ ∈ D. Theupdateddatabasecanthenbeusedasafine-tuning, retrieval, etc. database
fora(potentiallydifferent)modelf′ ∈ F. Thissetofinteractionsbetweenamodelandadatabase
mayoccuracrossvariousmodelsandvariousdatabasesinthesystem.
Asdescribed,thissystemcanbemodeledasagraphG=(V,E)whereV =F∪Dandthedirected
edge(v,v′)isinEifvertexvhasinfluenceonvertexv′.Forexample,theedge(D,f)existsiff has
accesstoDforretrievalaugmentationorifitcanuseasubsetofDasfine-tuningdata. Conversely,
theedge(f,D)existsiftheoutputoff caninfluencethecontentofdatasetD.
OurprimaryinterestisthedynamicsofasystemofinteractingLLMsanddatabaseswherethevertex
andedgesetsareindexedbyadiscretevariablet ∈ {1,...,T}. Therearemanywayscomponents
ofthegraphmayvaryintinsuchasystem. Forexample,thedatasetD(t) ∈ V(t) maybeupdated
basedontheoutputsofthemodelf(t) ∈V(t)orthemodelf(t)maychangeafterfine-tuningonthe
2Figure2: Two2-dperspectivespacesoffifteenmodels(5modelseachfromthreeclasses,encoded
by color). An evaluation set containing prompts relevant to the differences in the models (left) is
bettersuitedtoinduceadiscriminativeperspectivespacethananevaluationsetcontaining“orthog-
onal”prompts.
contents of the dataset D(t). In both cases V(t) ̸= V(t+1). Similarly, external factors such as the
termsofuseforadatasetmaychangetodisallowitsuseforretrievalaugmentationoramodelmay
losewrite-accesstoadataset. InbothcasesE(t) ̸=E(t+1). Figure1illustratessimpleexamplesof
systemsofLLMsasgraphs, includingthreestructuresthatarestudiedinthesimulatedsettingsin
Section4.
3 DEFINING A PERSPECTIVE SPACE WITH SURROGATE DATA KERNELS
Thesystem-of-LLMs-as-a-graphperspectiveprovidesaframeworktosystematicallystudytheeffect
of different vertex sets and edge structures on the flow of information through the system as a
function of t. The framework does not, however, provide a method to track the information flow.
For this, we introduce an adaptation of the embedding-based data kernel presented in (Duderstadt
etal.,2023). Forourpurposes,anembeddingfunctiongisamappingtoreal-valuedvectors.
3.1 THEDATAKERNEL&ITSSURROGATE
WeletX={x ,...,x }beacollectionofpromptswithx∈X andf(X)={f (x ),...,f(x )}
1 m θ 1 m
bethecorrespondingsetofresponseswithf(x) ∈ X′. Givenanembeddingfunctiong associated
i
withf , recallthatthedatakernelA(g ,X)oftheevaluationdatasetXfromtheperspectiveoff
i i i
capturestheintrinsicgeometryoftheembeddingspacewithrespecttoX. Thedatakernelenables
datum-level and global comparisons of two models with potentially different architectures, sizes,
etc. wheredirectcomparisonofg (X) = [g (x ),...,g (x )]⊤ ∈ Rm×p andg (X) ∈ Rm×p′ is
i i 1 i m j
otherwisenotpossible.
Themethodologycanbeextendedtocomparetheembeddingspacesofmultiplemodelsf ,...,f
1 n
at once by considering the pairwise distance matrix of the corresponding data kernels. In particu-
lar, the classical multi-dimensional scaling (Torgerson, 1952)) of the n×n matrix M with entries
M = ||A(g ,X) − A(g ,X)|| yieldsd-dimensionalEuclideanrepresentationsofthemodel
ij i j F
f withrespecttoX. Afterthistransformation, inferencemethodsdesignedforEuclideanobjects
i
canbeusedformodel-levelanalysis.
Thedatakernel,asdefinedin(Duderstadtetal.,2023),requiresthemodelf tohaveanassociated
i
embeddingfunctiong .Unfortunately,forsomestate-of-the-artLLMssuchasOpenAI’sGPTseries,
i
Anthropic’sClaudeseries,etc.,anassociatedembeddingfunctionisunavailableandthedatakernel
cannotbeconstructed. Torectifythis,wereplaceamodel’sassociatedembeddingfunctionwitha
3surrogateembeddingfunctiong˜:X′ →RpthatisnotnecessarilyrelatedtoanyoftheLLMsunder
study.
Thesurrogateembeddingfunctionisnotadrop-and-replacesolutionformodelcomparisons,how-
ever,sincetheembeddingg˜(X)isindependentoff .Instead,wequerythemodelwiththeelements
i
of X and embed the responses f (X) with g˜: the surrogate data kernel A(g˜,f (X)) is simply
i i
g˜(f (X))∈Rm×p.
i
3.2 THEPERSPECTIVESPACE
As with the original data kernel, we can use the surrogate data kernel to compare the responses
frommultiplemodelssimultaneouslyviatheCMDSofthepairwisedistancematrixM˜ withentries
M˜ = ||g˜(f (X))−g˜(f (X))|| .WeletZ ∈Rddenotethed-dimensionalvectorrepresentation
ij i j F i
off .
i
SincetherepresentationsZ ,...,Z areafunctionofthedifferencesinthemodelresponses–or
1 n
“perspectives” – f (X),...,f (X), we refer to the subspace populated by {Z ,...,Z } as the
1 n 1 n
perspective space of F with respect to X. The information that is captured by the perspective
spacedependsong˜andX. Inparticular,g˜needstobeabletodistinguishbetweenconceptsthatare
intendedtobedistinguished.Forexample,arandommappingfromX′toRpislikelyinsufficientfor
comparingmodels,general-purposeembeddingfunctions(Reimers&Gurevych,2019;Nussbaum
etal.,2024)shouldbesufficientforcapturingthemajorityofsignal,anddomain-specificembedding
functions(Risch&Krestel,2019)shouldbeusedwhenthedifferenceinmodelsishighlynuanced.
Similarly, X should contain prompts that the models are expected to have meaningfully different
responses. We demonstrate this in Figure 2 where g˜ is fixed, F consists of 15 models (5 each
from three different classes) and X is chosen to be relevant to the difference in classes (left) or
“orthogonal”tothedifferenceinclasses(right). Theperspectivespaceismorediscriminative(i.e.,
the models from a given class cluster better) when X contains prompts relevant to the class-wise
differences. Moredetailsrelatedtothemodelsshowninthetwoperspectivespacesareprovidedin
AppendixB.
Theperspectivespacethatincludestheentirehistoryofasystemcanbelearnedbyconsideringthe
CMDSofthe|F|T × |F|T pairwisedistancematrixwithentries||g˜(f(t)(X))−g˜(f(t′)(X))|| for
i j F
alli,j ∈ {1,...,|F|}andallt,t′ ∈ {1,...,T}. Weusethisperspectivespacewhenstudying
the systems below. The methodology can be extended to instances where only a partial history of
thesystemisobservedviaout-of-samplemethods(Bengioetal.,2003;Levinetal.,2018).
4 SIMULATING SYSTEMS OF INTERACTING LLMS
WenextsimulatethreedifferentsystemsofinteractingLLMstodemonstratetheeffectivenessofthe
perspectivespaceanditsderivativesforcapturingmodelandsystemdynamicsfordifferentunder-
lyingcommunicationstructures. Theinitialmodelsineachsystemarebasedonaninstanceofthe
410-millionparametermodelfromthePythiasuite(Bidermanetal.,2023)thathasbeeninstruction-
tunedusingDatabricks’Dolly15k(Conoveretal.,2023). Foreachsystemwefurtherfine-tunethe
basemodelonrandomquestion-pairsfromsettingspecifictopicsfromYahoo!Answers(YA)dataset
(Zhangetal.,2015)topromoteresponsevariation. Weprovidedetailsontheinstruction-tuningof
the base model and the fine-tuning of the initial models in Appendix A and Appendix B, respec-
tively. Weuseall-MiniLM-L6-v2,asentenceembeddingfunctionfrom(Reimers&Gurevych,
2019) based on (Wang et al., 2020b) hosted on the HuggingFace Hub (Wolf et al., 2020), as the
surrogate embedding function and the implementation of CMDS from Graspologic (Chung et al.,
2019).
InthethreeCaseStudies(C.S.)weconsider,eachmodelinteractswithanothermodelinthesystem
ateacht. Aninteractionconsistsofmodeliaskingmodelj ̸= iarandomsetofquestionsfroma
fixedquestionbankandfine-tuningmodeliusingtheresultingquestion-answerpairsasfine-tuning
data. For a given t, the underlying communication structure E(t) determines which set of model
interactions are possible for model i. In particular, the interviewed model j is randomly selected
fromthesetofmodelssuchthat(f ,f ) ∈ E(t). Thefixedquestionbankisusedastheevaluation
j i
settoinducetheperspectivespace.
4No disruption
disruption
0 10 20 30 40 50 0 10 20 30 40 50
time time
Figure3:Trackingindividualperspective(left)andsystem-leveldynamics(right)ofcommunication
networks of chat-based language models with (bottom left) and without (top left) a disruption in
communicationstructure.
Whileeachsystemthatwestudytechnicallyconsistsofmodelsanddatabases,eachdatasetisasso-
ciatedwithonlyasinglemodel.Forconveniencewediscussthesystemsasifthemodelsthemselves
aredirectlyconnected. Oursetting–wheremodelsaresequentiallytrainedoneachothersoutputs
withoutintervention–canbeviewedasageneralizationofasinglemodelsequentiallytrainedon
itsownoutputsasstudiedin(Shumailovetal.,2024).
Attheendofeachsimulationsettingweprovideexamplesthatmotivatedthecasestudy.
C.S.1: DISRUPTINGTHECOMMUNICATIONNETWORK
Wefirststudyasystemwith|F|=25modelsfine-tunedondifferent400randomsamplesfromYA
withtopic“Society&Culture”undertwodifferentsystemevolutions. Forthefirstsystemevolution
the underlying communication structure is un-
restricted (i.e., E(t) fully connected, see Fig- 4 No disruption
ure 1 “fully connected”) for all t. For the sec- disruption
ondsystemevolutiontheunderlyingcommuni- 3
cation structure is unrestricted for t < t∗ and
is then local-only (i.e., (f ,f ) ∈ E(t) only if 2
i j
model i is model j’s nearest neighbor in per-
1
spective space after the interactions at t − 1)
1.0
thereafter. We refer to the shift from unre-
strictedcommunicationtolocalcommunication
asadisruptioninthecommunicationstructure. 0.5
At each time t model i asks 50 random ques-
tions from a question bank of 400 questions 0.0
from YA with topic “Society & Culture”. The 0 10 20 30 40 50
time
initial 1-d perspectives of the models are rel-
atively close to each other, as can be seen at
t = 0 in both the top left and bottom left fig- Figure4: Estimatednumberofclustersfoundvia
ures of Figure 3. As the system evolves for GMMwithBIC(top)andsequentialARIofclus-
t < t∗, we observe the models “exploring” ter labels (bottom) for disrupted and undisrupted
theperspectivespace. Forthesystemthatdoes systems. Thenumberofclustersinbothsystems
notexperienceadisruption(topleft),theexplo- stabilize, indicating the presence of model sinks.
ration in perspective eventually stagnates and Modelsinksareunstableinasystemwithnodis-
each model appears to oscillate between three ruptionandstableinasystemwithadisruption.
different global perspective “sinks”, one near
the top of the figure, one in the middle of the figure, and one near the bottom of the figure. For
the system that experiences a disruption at t∗ = 21 (bottom left) the exploration in perspective
spacesimilarlystops,thoughthemodelsdonotoscillatebetweenglobalsinksand,instead,persist
in local sinks. The existence of multiple model sinks in both evolutions generalizes the behavior
5
evitcepsrep
No
disruption
disruption
rorrim−osi
sretsulc
fo
rebmuN
)1−t
,t(IRAobservedin(Shumailovetal.,2024),wherethesequenceofasinglemodelsequentiallytrainedon
itsownoutputconvergestoadegeneratemodelsink.
The difference in local and global sinks is quantified in Figure 4, where we report the number of
clustersateachtandthesimilarityofsequentialclusterlabels. WeuseGaussianMixtureModeling
withtheBayesianInformationCriterion(BIC)toestimatethenumberofclusters(Fraley&Raftery,
2002)andadjustedRandindex(ARI)tomeasureclusterlabelsimilarity. Wefindthatthenumber
ofclustersforbothsystemseventuallystabilizesandthattheARIbetweensequentialclusterlabels
is lower for the global communication network after stabilization, which signifies higher cluster
instability.
Wequantifythegeneralevolutionofthesystemsviathe“iso-mirror”(Athreyaetal.,2022)inthe
rightfigureofFigure3. Theiso-mirrorisasystem-levelsummaryofthedynamicsthattakesinto
accountthecollectionofmodel-leveldynamics. Inoursetting,theiso-mirrorcorrespondingtothe
systemthatdoesnotexperienceadisruptionisunstablethroughoutt. Theiso-mirrorcorresponding
to the disrupted system, however, clearly changes behavior at t∗ and remains constant throughout
theremainderofitsevolution.
Motivatingexamples. ThiscasestudywaslargelymotivatedbytheCOVID-19pandemic(Zuzul
et al., 2023) where social distancing, work from home, and social pods changed the latent com-
munication structure for entire communities. It is also relevant to communication networks for
range-limited devices where the definition of “local” depends on the geographical location of the
device(Wangetal.,2020a).
C.S.2: DIFFUSIONOFANADVERSARIALPERSPECTIVE
Figure 5: The evolution of 1-d perspectives of five interacting models where two models interact
with an “adversarial” model every other interaction (top). Given enough nodes to influence, the
adversarialmodelcancompromisetheentirenetwork–ascapturedbythedifferencebetweenthe
average 1-d perspective of the non-adversarial models and the 1-d perspective of the adversarial
modelforvariousamountsoftargetmodelsandvariousattackfrequencies(bottom).
Wenextconsiderasystemwith|F|=6modelswherefiveofthemodelsarefine-tunedonarandom
setof1000question-answerpairsfromYAwithtopic“Society&Culture”andthesixthisfine-tuned
onarandomsetof1000question-answerpairsfromYAwithtopic“Science&Mathematics”. We
refer to the model trained on data with topic “Science & Mathematics” as an “adversarial” model
sinceitdoesnotsharethesameinitialperspectiveastheotherfiveinexpectation. Anon-adversarial
model is referred to as a “target” model at time t if there is an edge from the adversarial model
to it in E(t). Target models are randomly selected at the beginning of the evolution of the system
6andremaintargetsthroughoutasimulation. Theevaluationsetconsistsof200questionsfromthe
“Science&Mathematics”topic. Ateachiterationmodeliasksmodelj 100questions.
For this experiment E(t) oscillates between two states. The first is a base state where the non-
adversarialsubnetworkisfullyconnectedandtherearenoedgestoorfromtheadversarialmodel.
The second is a “vulnerable” state where there is an edge from the adversarial model to all tar-
get models, there are no other in-bound edges to the adversarial or target models, the non-target
non-adversarial subnetwork is fully connected, and there are edges from the target models to the
non-targetmodels(seeFigure1“vulnerable”). Wesimulatesystemsthathaveavulnerablecommu-
nicationnetworkonceeverytwo,fiveorteniterations.
Thetrajectoriesofthe1-dperspectivesofthemodelsinthesystemwithavulnerablecommunication
everyotheriterationareshowninthetopofFigure5forsystemswith0,1,2and5targets. Wealso
reporttheaverageperspectiveofthetargetedmodelsandtheaverageperspectiveofthenon-targeted
modelsforeachsystem.
Forthesystemwithnotargets(topleft)weobservesimilarbehaviortothefirstcasestudyunderno
disruption: themodelsinitiallyexploretheperspectivespaceandeventuallysettleinamodelsink.
Forthesystemwithasingletargetweseethetargetedmodel(topcenterleft)oscillatebetweenthe
adversarialperspectiveandtheaverageperspectiveofthenon-targetedmodels. Non-targetmodels
that interact with the target models immediately after the communication network was vulnerable
are similarly pulled towards the adversarial perspective but to a lesser extent. Together these two
effectslimittheperspectiveexplorationofthemodelsinthesystemandeliminatethepresenceof
themodelsink.
Forthesystemwithtwotargets(topcenterright)thetargetedmodelsoscillatebetweentheadversar-
ialperspectiveandtheaveragenon-targetperspectivebuttheoscillationsdampenasthenon-target
model pespectives start to drift towards the adversarial perspective. By t = 20 the average non-
targetperspectiveisclosertotheadversarialperspectivethanitsownstartingposition. Thatis,the
entiresystemofLLMshasbeencompromisedbytheadversarialmodeltargetingonlyaminorityof
themodelsinthesystem. Theaverageperspectiveofmodelsinasystemwithfivetargets(topright)
quicklyapproachestheadversarialperspective.
Inthissettingwesummarizesystembehaviorviapolarizationdefinedasthedifferenceintheaver-
ageperspectiveofnon-adversarialmodelsandtheperspectiveoftheadversarialmodelnormalized
by this difference at t = 0. We report the polarization for five initialization for vulnerable com-
municationfrequenciesoftwo, five, andteninthebottomofFigure5. Forexample, foranattack
frequency of two we see that polarization neatly summarizes our observations. In particular, the
polarization increases when there are no target models, the polarization is relatively stable when
there is a single target, the polarization slowly drifts towards zero when there are two targets, and
thepolarizationquicklyapproacheszerowhentherearefivetargets. Thesystemismoresusceptible
whenmoremodelsaretargetedforattackfrequenciesoffiveandten,aswell.
Thetrendacrossattackfrequenciesforafixednumberoftargetmodelsindicatesthatgivenenough
time between attacks the average model perspective is able to recover. This is likely due to the
interaction mechanic involving a random subset of the evaluation questions – instead of the entire
set–thatenablessystem-levelperspectivehomeostasis.
Motivatingexample. Thiscasestudywasdesignedtomimicinformationdiffusioninthepresence
ofsimplepropagandamachinesandtostudyhow“attacks”onaminorityaffectstheentiresystem.
C.S.3: MITIGATINGORPROMOTINGPOLARIZATION
In our last case study we consider a system of |F| = 10 models where five of the models are
fine-tunedon1000randomquestion-answerpairsfromYAwithtopic“Society&Culture”andthe
other five are fine-tuned on 1000 random question-answer pairs from YA with topic “Science &
Mathematics”. Weletthetopicinwhichthefine-tuningdataissampledfromparameterizemodel
“class”. The evaluation set consists of 200 questions from each class. An interaction consists of
modeliaskingmodelj 100questions.
In this experiment we consider two different communication structures: unrestricted communica-
tionwhereE(t) isfullyconnectedandintra-classonlycommunicationwhereE(t) consistsoftwo
7Figure 6: The evolution of 1-d perspective space representations of ten models from two classes
under different underlying communication structures – unrestricted (left, top) and intra-class only
(left,bottom).Class-wiseaverage1-dperspectives(bolded)areintertwinedthroughouttheevolution
of the system with unrestricted communication and diverge with intra-class only communication.
Polarizationcapturesthisdifferenceinbehaviorovermultipleiterationsoftheexperiment(right).
unconnectedclass-wisefullyconnectedsubnetworks(seeFigure1“intra-classonly”).Asystemhas
the same communication structure for the entirety of its evolution. The top left figure of Figure 6
shows1-dperspectivesofthemodelsinthesystemwithunrestrictedcommunication. Boldedlines
represent the class average. As with fully connected communication network settings in the other
case studies, we observe a period of perspective exploration before stabilizing. Notably, the two
class-meansstayintertwinedthroughouttheentiretyoftheevolutionofthesystem.
ThebottomleftfigureofFigure6showstheevolutionof1-dperspectiveswithintra-classonlycom-
munication.Undertheintra-classonlyregimeweseethatthetwoclassesexploredifferentregionsof
theperspectivespaceandeventuallysettleintotwosinkswithamuchgreaterdistancebetweenthem
then the class-wise differences at t = 0. The polarization of the class-wise averages captures the
distancingoftheperspective“echochambers”,asreportedintherightfigureofFigure6.Indeed,the
polarization increased by 15x on average over four different simulation initializations under intra-
class only communication. Average polarization is near zero by the end of the simulations under
unrestrictedcommunication.
Motivatingexample. Thiscasestudywasdesignedtoinvestigatetheeffectofextremeunderlying
communicationnetworksontwopartysystems.
5 RELATED WORK
Ourworkiscloselyrelatedtosimulatinggroupsofcomputationalagentstostudysociologicaland
culturalphenomena(Steels,1990;Wagneretal.,2003)andtocontinuallearning(Vogelsteinetal.,
2020;Geisaetal.,2021). TheformerhasseenrenewedinterestwiththerecentsuccessesofLLMs.
Inparticular,LLMsare–asofthiswriting–thecomputationaltoolthatproduceslanguageartifacts
mostsimilartooursand,assuch,areanintriguingprospectformulti-agentsociologicalandcultural
simulations.Recentworkhasincludedobjective-lessbehavioralstudies(Parketal.,2023),studying
theformationofsocialnetworks(Papachristou&Yuan,2024),trackingopiniondynamicsviaclassi-
ficationofLLMresponse(Chuangetal.,2023),andanalyzingdocumentcollaboration(Perezetal.,
2024). Our work extends these by introducing a framework to systematically study interventions
andbyintroducingaquantitativemethodfortrackingtheevolutionofagentperspectives.
Continuallearning(Thrun,1995;1998)islargelyconcernedwithhowasingleagentadaptstopre-
viouslyunseeninferencetaskswhileavoiding“catastrophicallyforgetting”(McCloskey&Cohen,
1989;Kirkpatricketal.,2017)previoustasks. Oursettingisslightlydifferent,sincewehavemul-
tiple agents and no explicit task – though a large movement in perspective space is likely highly
correlatedtochangeinperformanceonlanguagebenchmarksrelatedtotheevaluationset. Indeed,
large enough movements in perspective space and the emergence of model sinks when training a
modelrecursivelyisrelatedtocatastrophicforgetting(Shumailovetal.,2024).
86 CONCLUSION
We introduced a system-of-LLMs-as-a-graph to enable systematic interventions to a system of in-
teracting LLMs and the perspective space to quantitatively study the corresponding evolution of
the system. We used these tools to highlight differences in paired systems across three case stud-
ies. For the particular interaction mechanic and update function that we used in our simulations,
the model behaviors in perspective space consistently demonstrated initial model exploration and,
in most cases, the emergence and persistence of model sinks. Further, we used derivatives of the
perspectivespacesuchastheiso-mirror,polarization,andclusteringtohighlightdifferencesinthe
evolutionofpairedsystems.
Forexample,weobserveddifferencesintheiso-mirror(stableversusunstableafterdisruption)and
clustering(globalsinksversuslocalsinksafterdisruption)inthefirstcasestudy;differencesinthe
sensitivityoftheaverageperspectiveofnon-adversarialmodelstoanadversarialperspectiveacross
numberofvictimsandfrequencyofattackinthesecondcasestudy;anddifferencesinthebehavior
ofpolarizationoftwoclassesofmodelsinthethirdcasestudy.
7 LIMITATIONS
A system of interacting language models is a complicated system and, as such, analysis of them
will often require simplification of aspects of the system. Our case studies are no expection. For
example, the interaction mechanic (i.e., each model interacts with exactly one of its neighbors at
timet)andupdatefunction(i.e.,updatemodelweightsviafine-tuning)usedinthesimulationsare
moreproof-of-conceptthanfinal-productinthattheydonotreflectourbeliefsonhowindividuals
withinacommunityinteractor“update”themselves,norarecurrentlydeployedmodelsconstantly
updated. While we do not attempt to enumerate all possible improvements here, we believe that
it is imperative to work closely with social and cognitive scientists to understand the appropriate-
ness of considering systems of LLMs as a proxy for human communities or online forums before
generalizingobservedsimulatedbehaviortohuman-facingcommunities. Futureworkalongthese
lineswillincludetwomajorfronts:i)designingcomprehensivestatisticalframeworkstounderstand
the appropriateness of using a system of interacting LLMs as a proxy for various social settings
andii)extendingsimulationsettingstoincludemoresociologicallyplausibleinteractionandupdate
mechanics.
Further,thesimulationstudieshereinarebutthreesystemconfigurationsworthconsidering.Indeed,
ofimmediateinterestisanextensiontohierarchicalsocialstructuresobservedinlargecommercial
and government institutions where the perspective space can be used to understand the effect of
information injection, re-organizations, third-party seminars, etc. on individual-level, team-level,
andorganization-leveldynamics.
Therearealsolimitationsrelatedtotheanalysisofeachofthethreecasestudieswepresented. For
example, for the first case study we only investigated the difference between system behavior of
globalcommunicationandglobaltohyper-localcommunication. Morenuancedinvestigationsinto
the effect of the number of models, the effect of the initializations of the models, the effect of the
definitionof“local”,etc. isnecessarytounderstandhowtheempiricalobservationsmaygeneralize
totherealworld. Similarly,forthesecondcasestudyweonlyconsideredasinglestaticadversarial
model. Amorerealisticsimulationmayincludemultipledynamicadversarialmodels. Forthethird
casestudy,ifthisanalysisistobeusedtounderstandpolarizationofpoliticalparties,itisnecessary
tounderstandtheeffectofcross-partycommunication,howeverrareitmaybe. We,again,believe
that it is necessary to comprehensively explore each of these experiments before making claims
aboutitsapplicabilitytosocietyandhuman-modelforums.
Lastly,weintroducetheperspectivespaceanddemonstratethatitissensitivetoevaluationset. We
do not, however, comprehensively explore or discuss potential applications or alternative model-
basedsimilarities. SimilarmethodshavebeenusedWeexpecttheperspectivespacetobeusefulfor
variousmodel-levelinferencetasks,assimilarmethodshavebeensuccessfullyusedforclassifica-
tion(Chenetal.,2022)andchange-pointdetection(Chenetal.,2023)inneuroscienceapplications.
We also expect the model-based similarity most effective for capturing model differences will be
systemandtaskdependent(Eatonetal.,2008;Zamiretal.,2018;Helmetal.,2020).
9Acknowledgements. We would like to thank Avanti Athreya, Henry Farrell, Hahrie Han, Teresa
Huang,VinceLyzinski,HarveyMcGuinness,andTimWangfortheirhelpfulfeedbackanddiscus-
sionsthroughoutthedevelopmentofthismanuscript.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
Avanti Athreya, Zachary Lubberts, Youngser Park, and Carey E Priebe. Discovering underlying
dynamicsintimeseriesofnetworks. arXivpreprintarXiv:2205.06877,2022.
Yoshua Bengio, Jean-franc¸cois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Roux, and
Marie Ouimet. Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clus-
tering. Advancesinneuralinformationprocessingsystems,16,2003.
StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal.
Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InInternational
ConferenceonMachineLearning,pp.2397–2430.PMLR,2023.
Levin Brinkmann, Fabian Baumann, Jean-Franc¸ois Bonnefon, Maxime Derex, Thomas F. Mu¨ller,
Anne-MarieNussberger,AgnieszkaCzaplicka,AlbertoAcerbi,ThomasL.Griffiths,JosephHen-
rich, Joel Z. Leibo, Richard McElreath, Pierre-Yves Oudeyer, Jonathan Stray, and Iyad Rah-
wan. Machine culture. Nature Human Behaviour, 7(11):1855–1868, November 2023. ISSN
2397-3374. doi: 10.1038/s41562-023-01742-2. URL http://dx.doi.org/10.1038/
s41562-023-01742-2.
GuodongChen,HaydenSHelm,KateLytvynets,WeiweiYang,andCareyEPriebe. Mentalstate
classificationusingmulti-graphfeatures. FrontiersinHumanNeuroscience,16:930291,2022.
Tianyi Chen, Youngser Park, Ali Saad-Eldin, Zachary Lubberts, Avanti Athreya, Benjamin D
Pedigo, Joshua T Vogelstein, Francesca Puppo, Gabriel A Silva, Alysson R Muotri, et al. Dis-
coveringachangepointinatimeseriesoforganoidnetworksviatheiso-mirror. arXivpreprint
arXiv:2303.04871,2023.
Yun-ShiuanChuang,AgamGoyal,NikunjHarlalka,SiddharthSuresh,RobertHawkins,SijiaYang,
DhavanShah,JunjieHu,andTimothyTRogers. Simulatingopiniondynamicswithnetworksof
llm-basedagents. arXivpreprintarXiv:2311.09618,2023.
JaewonChung, BenjaminDPedigo, EricWBridgeford, BijanKVarjavand, HaydenSHelm, and
JoshuaTVogelstein. Graspy: Graphstatisticsinpython. JournalofMachineLearningResearch,
20(158):1–7,2019.
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick
Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly open
instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/
12/dolly-first-open-commercially-viable-instruction-tuned-llm.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
Brandon Duderstadt, Hayden S Helm, and Carey E Priebe. Comparing foundation models using
datakernels. arXivpreprintarXiv:2305.05126,2023.
Eric Eaton, Marie Desjardins, and Terran Lane. Modeling transfer relationships between learn-
ing tasks for improved inductive transfer. In Machine Learning and Knowledge Discovery in
Databases: European Conference, ECML PKDD 2008, Antwerp, Belgium, September 15-19,
2008,Proceedings,PartI19,pp.317–332.Springer,2008.
10ChrisFraleyandAdrianERaftery. Model-basedclustering,discriminantanalysis,anddensityesti-
mation. JournaloftheAmericanStatisticalAssociation,97(458):611–631,2002. doi: 10.1198/
016214502760047131. URLhttps://doi.org/10.1198/016214502760047131.
AliGeisa, RonakMehta, HaydenSHelm, JayantaDey, EricEaton, JefferyDick, CareyEPriebe,
and Joshua T Vogelstein. Towards a theory of out-of-distribution learning. arXiv preprint
arXiv:2109.14501,2021.
Hayden Helm, Carey E Priebe, and Weiwei Yang. A statistical turing test for generative models.
arXivpreprintarXiv:2309.08913,2023.
Hayden S Helm, Ronak D Mehta, Brandon Duderstadt, Weiwei Yang, Christoper M White, Ali
Geisa, Joshua T Vogelstein, and Carey E Priebe. A partition-based similarity for classification
distributions. arXivpreprintarXiv:2011.06557,2020.
JohnJumper, RichardEvans, AlexanderPritzel, TimGreen, MichaelFigurnov, OlafRonneberger,
KathrynTunyasuvunakool,RussBates,AugustinZˇ´ıdek,AnnaPotapenko,etal. Highlyaccurate
proteinstructurepredictionwithalphafold. Nature,596(7873):583–589,2021.
JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiA
Rusu,KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,etal.Overcom-
ingcatastrophicforgettinginneuralnetworks. Proceedingsofthenationalacademyofsciences,
114(13):3521–3526,2017.
Michal Kosinski. Evaluating large language models in theory of mind tasks. arXiv e-prints, pp.
arXiv–2302,2023.
KeithLevin,FredRoosta,MichaelMahoney,andCareyPriebe. Out-of-sampleextensionofgraph
adjacency spectral embedding. In International Conference on Machine Learning, pp. 2975–
2984.PMLR,2018.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequentiallearningproblem. InPsychologyoflearningandmotivation,volume24,pp.109–165.
Elsevier,1989.
ZachNussbaum,JohnX.Morris,BrandonDuderstadt,andAndriyMulyar. Nomicembed:Training
areproduciblelongcontexttextembedder,2024.
Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learning
robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
MariosPapachristouandYuanYuan. Networkformationanddynamicsamongmulti-llms,2024.
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and
MichaelSBernstein.Generativeagents:Interactivesimulacraofhumanbehavior.InProceedings
ofthe36thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology,pp.1–22,2023.
Je´re´my Perez, Corentin Le´ger, Marcela Ovando-Tellez, Chris Foulon, Joan Dussauld, Pierre-Yves
Oudeyer,andCle´mentMoulin-Frier. Culturalevolutioninpopulationsoflargelanguagemodels,
2024.
AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,andIlyaSutskever.
Robustspeechrecognitionvialarge-scaleweaksupervision. InInternationalConferenceonMa-
chineLearning,pp.28492–28518.PMLR,2023.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
networks. arXivpreprintarXiv:1908.10084,2019.
Julian Risch and Ralf Krestel. Domain-specific word embeddings for patent classification. Data
TechnologiesandApplications,53(1):108–122,2019.
IliaShumailov, ZakharShumaylov, YirenZhao, YarinGal, NicolasPapernot, andRossAnderson.
Thecurseofrecursion: Trainingongenerateddatamakesmodelsforget,2024.
11LucSteels. Cooperationbetweendistributedagentsthroughself-orcamsation. InProceedingsofthe
firstEuropeanworkshoponmodellingautonomousagentsinamulti-agentworld.Citeseer,1990.
Sebastian Thrun. Is learning the n-th thing any easier than learning the first? Advances in neural
informationprocessingsystems,8,1995.
SebastianThrun. Lifelonglearningalgorithms. InLearningtolearn,pp.181–209.Springer,1998.
WarrenSTorgerson. Multidimensionalscaling: I.theoryandmethod. Psychometrika,17(4):401–
419,1952.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
layBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfounda-
tionandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
Joshua T Vogelstein, Hayden S Helm, Ronak D Mehta, Jayanta Dey, Weiwei Yang, Bryan Tower,
WillLeVine,JonathanLarson,ChrisWhite,andCareyEPriebe. Ageneralapproachtoprogres-
sivelearning. Preprintathttps://arxiv.org/abs/2004.12908,2020.
KyleWagner,JamesAReggia,JuanUriagereka,andGeraldSWilkinson.Progressinthesimulation
ofemergentcommunicationandlanguage. AdaptiveBehavior,11(1):37–69,2003.
FangxinWang,MiaoZhang,XiangxiangWang,XiaoqiangMa,andJiangchuanLiu. Deeplearning
foredgecomputingapplications:Astate-of-the-artsurvey. IEEEAccess,8:58322–58336,2020a.
doi: 10.1109/ACCESS.2020.2982411.
WenhuiWang, FuruWei, LiDong, HangboBao, NanYang, andMingZhou. Minilm: Deepself-
attentiondistillationfortask-agnosticcompressionofpre-trainedtransformers. AdvancesinNeu-
ralInformationProcessingSystems,33:5776–5788,2020b.
ThomasWolf, LysandreDebut, VictorSanh, JulienChaumond, ClementDelangue, AnthonyMoi,
Pierric Cistac, Tim Rault, Re´mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-
of-the-artnaturallanguageprocessing,2020.
Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio
Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE con-
ferenceoncomputervisionandpatternrecognition,pp.3712–3722,2018.
XiangZhang,JunboZhao,andYannLeCun. Character-levelconvolutionalnetworksfortextclas-
sification. Advancesinneuralinformationprocessingsystems,28,2015.
TionaZuzul,EmilyCoxPahnke,JonathanLarson,PatrickBourke,NicholasCaurvina,NehaParikh
Shah, Fereshteh Amini, Jeffrey Weston, Youngser Park, Joshua Vogelstein, Christopher White,
andCareyE.Priebe. Dynamicsilos:Increasedmodularityinintra-organizationalcommunication
networksduringthecovid-19pandemic,2023.
12A INSTRUCTION-TUNING PYTHIA-410M-DEDUPED
ThebasemodelthatweusedinthecasestudiesinSection4wasaninstruction-tunedversionofthe
410millionparametermodelfromthePythiasuite(Bidermanetal.,2023). Forinstruction-tuning,
weaddedthreespecialtokenstoitstokenizer’svocabulary,“###End”,“###Instruction:”,and“###
Response:”,andfine-tunedthemodelwithasubsetofDatabricks’Dolly15k(Conoveretal.,2023).
Each datum consists of an instruction, context, response, and category. We kept only data in the
OpenQA,Brainstorm,GeneralQA,andCreativeWritingcategoriesandthathadaresponselength
lessthan100characters. Thisfilteringleftuswith1559instruction-responsepairs. Weformatteda
particularexampleasfollows:
###Instruction: {instruction}
###Response: {response}
###End
Wefine-tunedthemodelontheformatteddatausingAdamwithalearningrateof5×10−5 anda
batchsizeof8for10epochs. Thefinalcross-entropylossonthetrainingdatawas≈0.26.
B CASE-STUDY SPECIFIC FINE-TUNING
For each of the case studies we further fine-tuned the instruction-tuned base model to promote
response variation. For this, we used the data from the Yahoo! Answers (YA) dataset introduced
in (Zhang et al., 2015), where each datum consists of a topic, a question title, question content, a
list of answers, and a best answer. Given data from a particular topic, we further filtered the data
by considering only examples with best answers less than 200 characters, with best answers that
containedonlyasinglesentence,andwithquestiontitlesthatcontainedonlyasinglequestion. We
formatteddatafromYAasfollows:
###Instruction: {questiontitle}
###Response: {bestanswer}
###End
Unlessotherwisespecified, fine-tuningisdoneusingAdamwithalearningrateof5×10−5. The
initialmodelsweretrainedfor3epochs. Themodelupdatesafteraninteractionconsistedofonlya
singleepochwithalearningrateof10−5.
B.1 CASESTUDY1: STOCHASTICALLYEQUIVALENTMODELS
For case study 1, we randomly selected 400 examples with the topic “Society & Culture” that we
usedasboththeevaluationsetintheexperimentandasapoolofdatausedforfurthersampling. In
particular,werandomlysampled200samplesfromthesetof40025timesandusedthe25subsets
asfine-tuningdatafordifferent“stochasticallyequivalent”models.
B.2 CASESTUDIES2&3: TWOCLASSES
Forcasestudies2&3,weconsideredfiltereddatafromtopics“Society&Culture”and“Science&
Mathematics”. Foreachtopicwerandomlysampled1000examples10timestouseforfine-tuning.
Forcasestudy2, werandomlyselectedasinglemodelfine-tunedon“Science&Mathematics”to
betheadversarialmodel. Thismodelwastheadversarialmodelforallsysteminstances. Wethen
randomlyselected5modelsfine-tunedon“Society&Culture”datatobenon-adversarialmodels.
Thenon-adversarialmodelschangedwitheachsysteminstance.
Forcasestudy3,werandomlyselected5modelsfromeachclassforeverysysteminstance.
13