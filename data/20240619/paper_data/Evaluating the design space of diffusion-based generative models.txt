Evaluating the design space of diffusion-based
generative models
YuqingWang YeHe
SchoolofMathematics SchoolofMathematics
GeorgiaInstituteofTechnology GeorgiaInstituteofTechnology
ywang3398@gatech.edu yhe367@gatech.edu
MoleiTao
SchoolofMathematics
GeorgiaInstituteofTechnology
mtao@gatech.edu
Abstract
Mostexistingtheoreticalinvestigationsoftheaccuracyofdiffusionmodels,albeit
significant,assumethescorefunctionhasbeenapproximatedtoacertainaccuracy,
and then use this a priori bound to control the error of generation. This article
insteadprovidesafirstquantitativeunderstandingofthewholegenerationprocess,
i.e.,bothtrainingandsampling. Moreprecisely,itconductsanon-asymptoticcon-
vergenceanalysisofdenoisingscorematchingundergradientdescent. Inaddition,
arefinedsamplingerroranalysisforvarianceexplodingmodelsisalsoprovided.
Thecombinationofthesetworesultsyieldsafullerroranalysis,whichelucidates
(again,butthistimetheoretically)howtodesignthetrainingandsamplingpro-
cessesforeffectivegeneration.Forinstance,ourtheoryimpliesapreferencetoward
noisedistributionandlossweightingthatqualitativelyagreewiththeonesusedin
Karrasetal.[31]. Italsoprovidessomeperspectivesonwhythetimeandvariance
scheduleusedinKarrasetal.[31]couldbebettertunedthanthepioneeringversion
inSongetal.[52].
1 Introduction
Diffusionmodelshavenowadaysadvancedinvariancedomains,includingcomputervision[20,7,
28,29,41,56], naturallanguageprocessing[6,36,40], variousmodelingtasks[16,44,58], and
medical,biological,chemicalandphysicalapplications[3,18,46,55,23,59](seemoresurveysin
[57,11,15]).Therearethreemainformulationsofdiffusionmodels:denoisingdiffusionprobabilistic
models(DDPM)[48,27],score-basedgenerativemodels(SGM)[49,50],andscore-basedmodels
through SDEs [52, 51]. Regarding score-based approaches, Karras et al. [31] provided a unified
empiricalunderstandingofthederivationsofmodelparameters,improvingittothenewstate-of-the-
artperformance. Karrasetal.[32]furtherupgradeditbyredesigningthenetworkarchitecturesand
replacingtheweightsofthenetworkwithanexponentialmovingaverage. Asdiffusionmodelsgain
widerusage,effortstounderstandandenhancetheirbehaviorbecomeincreasinglymeaningful.
Inthemeantime,therearealotoftheoreticalworkstryingtoanalyzediffusionmodels. Existing
literaturecanbedividedintotwocategories,focusingseparatelyonsamplingandtrainingprocesses.
Samplingworks[17,13,8,24,12,19]assumethescoreerroriswithinacertainaccuracythreshold
andanalyzethediscrepancybetweenthedistributionofthegeneratedsamplesandthetrueonethrough
discretizationofbackwarddynamics. Meanwhile,trainingworks[47,25,9,14,42,25]concentrate
Preprint.Underreview.
4202
nuJ
81
]GL.sc[
1v93821.6042:viXraonminimizingthedistancebetweentheapproximatedscoreandthetruescore. Nevertheless,the
trainingbehaviorofdiffusionmodelsremainslargelyunexploredduetotheunboundednoisydata
structure and complicated network architectures used in practice. More detailed discussions of
theoreticalworksareinSection1.1.
However,asKarrasetal.[31]indicates,theperformanceofdiffusionmodelsalsoreliesontheinter-
actionbetweeneachcomponentintrainingandsampling,suchasthenoisedistribution,weighting,
timeandvarianceschedules,etc. Whilestudyingindividualprocessesisessential,itprovideslimited
insightintothecomprehensiveunderstandingofthewholemodel. Therefore,motivatedbyobtaining
deepertheoreticalunderstandingofhowtomaximizetheperformanceofdiffusionmodels,thegoal
ofthispaperistoestablishthefullgenerationerroranalysis,combiningoptimizationandsampling,
topartiallyinvestigatethedesignspaceofdiffusionmodels.
Inthispaper,weconsiderthevarianceexplodingsetting[52],whichisalsothefoundationofthe
continuousforwarddynamicsinKarrasetal.[31].Ourmaincontributionsaresummarizedasfollows:
• Fordenoisingscorematchingobjective,weestablishtheexponentialconvergenceundergradient
descentintoaneighbourhoodoftheminimum(Theorem1).Weemployahigh-dimensionalsetting,
wherethewidthmandthedatadimensiondareofthesameorder,anddevelopanewmethodfor
provingthekeylowerboundofgradientunderthesemi-smoothnessframework[1,37].
• Weextendthesamplingerroranalysisin[8]tovarianceexplodingcase(Theorem2),underonly
thefinitesecondmomentassumption(Assumption3)ofthedatadistribution. Ourresultappliesto
variousvarianceandtimeschedules,andimpliesasharpalmostlinearcomplexityintermofdata
dimensionunderoptimaltimeschedule.
• Weobtainfullerroranalysisfordiffusionmodels,combiningtrainingandsampling(Corollary1).
• Wequalitativelyderivethetheoryforchoosingthenoisedistributionandweightinginthetraining
objective,whichcoincideswithKarrasetal.[31](Section4.1). Moreprecisely,ourtheoryimplies
thattheoptimalrateisobtainedwhenthetotalweightingexhibitsasimilar“bell-shaped”pattern
usedinKarrasetal.[31].
• Wedevelopatheoryofchoosingtimeandvarianceschedulesbasedonbothtrainingandsampling
(Section4.2).Indeed,whenthescoreerrordominates,i.e.,theneuralnetworkislesstrainedandnot
veryclosetothetruescore,polynomialschedule[31]ensuressmallererror;whensamplingerror
dominates,i.e.,thescorefunctioniswellapproximated,exponentialschedule[52]ispreferred.
1.1 Relatedworks
Sampling. Alotoftheoreticalworkshavebeen
donetoanalyzethediffusionmodelsbyquan-
tifyingthesamplingerrorfromsimulatingthe
backward SDEs, assuming the score function
can be approximated within certain accuracy.
Most existing works [17, 13, 8] have been fo- Figure1: Structureofthispaper.
cused on the variance preserving (VP) SDEs,
whosediscretizationscorrespondtoDDPM.Bentonetal.[8]obtainedthestate-of-the-artconver-
genceresultfortheVPSDE-baseddiffusionmodelsassumingthedatadistributionhasfinitesecond
moment: theiterationcomplexityisalmostlinearinthedatadimensionandpolynomialintheinverse
accuracy,underexponentialtimeschedule. However,alimitedamountofworks[34,24]analyzethe
varianceexploding(VE)SDEs,whosediscretizationscorrespondtoScorematchingwithLangevin
dynamics(SMLD)[49]. Toourbestknowledge,GaoandZhu[24]obtainedthestate-of-the-artresult
assumingthedatadistributionhasboundedsupport:theiterationcomplexityispolynomialinthedata
dimensionandtheinverseaccuracy,underdifferentchoicesoftimeschedules. Incontrast,ourwork
extendstheresultin[8]totheVESDE-baseddiffusionmodels. Assumingthedatadistributionhas
finitesecondmoment,weobtaintheiterationcomplexitythatispolynomialinthedatadimensionand
theinverseaccuracy,underdifferentchoicesoftimeschedules. Ourcomplexityimprovescomplexity
inGaoandZhu[24]byafactorofthedatadimension. Undertheexponentialtimeschedule,our
complexity is almost linear in the data dimension, which recovers the state-of-the art result for
VPSDE-baseddiffusionmodels.
Training. Toourbestknowledge,theonlyworksthatconsidertheoptimizationerrorofthediffusion
modelsareShahetal.[47]andHanetal.[25]. Shahetal.[47]employedtheDDPMformulation
andconsideredtheinitialdistributiontobemixturesoftwosphericalGaussianswithvariousscales
ofseparationaswellasK sphericalGaussianswithawarmstart. Thenthescorefunctioncanbe
2analyticallysolvedandtheymodeleditinateacher-studentframeworksolvedbygradientdescent.
They also provided the sample complexity bound under these specific settings. In contrast, our
resultsworkforgeneralinitialdistributionswherethetruescoreisunknownandcombinetraining
withsamplingerroranalysis. Hanetal.[25]consideredthetwo-layerReLUneuralnetworkwith
thelastlayerfixedunderGDandusedtheneuraltangentkernel(NTK)approachtoprovethefirst
generalizationerror. TheyassumedtheGrammatrixofthekernelispositivedefiniteandtheinitial
distributionhascompactsupport,andthebounddoesnotshowtimecomplexity. Theyalsouniformly
sampledthetimepointsinthetrainingobjective. Incontrast,weusethedeepReLUnetworkwithL
layertrainedbyGDandproveinsteadofassumingthegradientislowerboundedbytheobjective
function. Moreover,weobtainthetime-dependentboundfortheoptimizationerror,andourboundis
validforgeneraltimeandvarianceschedules,whichallowsustoobtainafullerroranalysis. There
arealsoworksstudyinggeneralapproximationandgeneralizationofthescorematchingproblem
[9,14,42,25].
Convergenceofneuralnetworks. Theconvergenceanalysisofneuralnetworksundergradient
descent has been a longstanding challenging problem. One line of research employs the neural
tangentkernel(NTK)approach[22,21,5,53,39], wherethegeneralizationperformancecanbe
well-understood. However,existingworksinthisdirectioncanonlydealwitheitherscalaroutputor
vectoroutputbutwithonlyonelayertrainedfortwo-layernetworks,whichishighlyinsufficientfor
diffusionmodels.Anotherlineofresearchdirectlyquantifiesthelowerboundofthegradient[1,37,2]
anduseasemi-smoothnesspropertytoproveexponentialconvergence. Ourresultsalignwiththis
directionwhiledevelopinganewmethodforprovingthelowerboundofthegradient. Seemore
discussionsinSection3.1.
1.2 Notations
Wedenote∥⋅∥tobetheℓ2normforbothvectorsandmatrices,and∥⋅∥ tobetheFrobeniusnorm.
F
For thediscretetimepoints, weuse t todenotethe timepointforforwarddynamicsand t← for
i i
backwarddynamics. Fortheorderofterms,wefollowthetheoreticalcomputerscienceconventionto
useO(⋅),Θ(⋅),Ω(⋅). Wealsodenotef ≲giff ≤CgforsomeuniversalconstantC.
2 Basicsofdiffusion-basedgenerativemodels
Inthissection,wewillintroducethebasicforwardandbackwarddynamicsofdiffusionmodelsand
thedenoisingscorematchingsetting.
2.1 Forwardandbackwardprocesses
Consideraforwarddiffusionprocessthatpushesanin√itialdistributionP 0toGaussian
dX =−f X dt+ 2σ2dW , (1)
t t t t t
wheredW istheBrownianmotion,X ∼P ,andX ∈Rd. Undermildassumptions,theprocess
t 0 0 t
canbereversedandthebackwardprocessisdefinedasfollows
√
dY t=(f T−tY t+2σ T2 −t∇logp T−t(Y t))dt+ 2σ T2 −tdW˜ t, (2)
where Y 0 ∼ P T, and p t is the density of P t. Then Y T−t and X t have the same distribution with
densityp [4],whichmeansthedynamics(2)willpushtheGaussiandistributionbacktotheinitial
t
distributionP . Toapplythebackwarddynamicsforgenerativemodeling,themainchallengeliesin
0
approximatingtheterm∇logp T−t(Y t))whichiscalledscorefunction. Itiscommontouseaneural
networktoapproximatethisscorefunctionandlearnitviatheforwarddynamics(1);then,samples
canbegeneratedbysimulatingthebackwarddynamics(2).
2.2 Denoisingscorematching
In order to learn the score function, a typical way is to start with the following score matching
objective[e.g.,30]
1 T
L conti(θ)= 2∫
t0
w(t)E Xt∼Pt∥S(θ;t,X t)−∇ xlogp t(X t)∥2dt (3)
where S(θ;t,X ) is a θ-parametrized neural network, w(t) is some weighting function, and the
t
subscriptmeansthisisthecontinuoussetup.Ideallyonewouldliketooptimizethisobjectivefunction
toobtainθ;however,p ingeneralisunknown,andsoisthetruescorefunction∇ logp . Oneof
t x t
thesolutionsisdenoisingscorematchingproposedbyVincent[54],whereone,insteadofdirectly
matchingthetruescore,leveragesconditionalscoreforwhichinitialconditionisfixedsothatp t∣0is
analyticallyknown.
3Moreprecisely,giventhelinearityofforwarddynamics(1),itsexactsolutionisexplicitlyknown:
Let µ t = ∫ 0t f sds, and σ¯ t2 = 2∫ 0t e2µs−2µtσ s2ds. Then the solution is X t = e−µtX 0+σ¯ tξ, where
ξ ∼ N(0,I). We also have X t∣X
0
∼ N(e−µtX 0,σ¯ t2I) and g t(x∣y) = (2πσ¯ t2)−d/2exp(−∥x−
e−µty∥2/(2σ¯ t2)),whichisthedensityofX t∣X 0. Thentheobjectivecanberewrittenas
1 T 1 T
L conti(θ)= 2∫
t0
w(t)E X0E Xt∣X0∥S(θ;t,X t)−∇logg t(X t∣X 0)∥2dt+ 2∫
t0
w(t)Cdt
1 T 1 1 T
= 2∫
t0
w(t)
σ¯
tE X0E ξ∥σ¯ tS(θ;t,X t)+ξ∥2dt+ 2∫
t0
w(t)Cdt (4)
whereC =E Xt∥∇logp t∥2−E X0E Xt∣X0∥∇logg t(X t∣X 0)∥2. Forcompleteness,wewillprovidea
detailedderivationoftheseresultsinAppendixAandemphasizethatitisjustareviewofexisting
resultsinournotation. Throughoutthispaper,weadoptthevarianceexplodingsetting[52],where
f =0andhenceµ =0,whichalsoalignswiththesetupofEDM[31].
t t
3 Erroranalysisfordiffusion-basedgenerativemodels
Inthissection,wewillquantifythebehaviorofbothtrainingandsampling,andthenintegratethem
intoamorecomprehensivegenerationerroranalysis. Fortraining, weprovethenon-asymptotic
convergencebydevelopinganewmethodforobtainingalowerboundofgradient,togetherwith
exploitingthesemi-smoothnesspropertyoftheobjectivefunction[1,37]. Forsampling,weextend
the existing analysis in [8] to the variance exploding setting, obtaining a time/variance schedule
dependenterrorboundunderminimalassumptions. Intheend,weintegratethetwoaspectintoafull
generationerroranalysis.
3.1 Training
Inthissection,weemployapracticalimplementation(timediscretized)ofdenoisingscorematching
objective,representthescorebydeepReLUnetwork,andestablishtheexponentialconvergenceof
GDtrainingdynamicsintoaneighbourhoodoftheminimum.
Trainingobjectivefunction. Consideraquadraturediscretizationofthetimeintegralin(3)based
ondeterministic1collocationpoints0<t <t <t <⋯<t =T. Then
0 1 2 N
L (θ)≈L¯(θ)+C¯,
conti
whereC¯ =∑N j=1w(t j)(t j−t j−1)C,and
1 N 1
L¯(θ)=
2
∑w(t j)(t j−t j−1)
σ¯
E X0E ξ∥σ¯ tjS(θ;t j,X tj)+ξ∥2. (5)
j=1 tj
We define β
j
= w(t j)(t
j
−t j−1) σ¯1
tj
to be the total weighting and further consider the empirical
versionofL¯(5). Denotetheinitialdatatobe{x }n withx ∼P ,andthenoisetobe{ξ }N with
i i=1 i 0 ij j=1
ξ
ij
∼N(0,I d). Thentheinputdataoftheneuralnetworkis{X ij}n i=, 1N ,j=1={e−µtjx i+σ¯ tjξ ij}n i=, 1N
,j=1
andtheoutputdatais{ξ /σ¯ }n,N ifσ¯ ≠0. Consequently,L¯(θ)(5)canbeapproximatedby
ij tj i=1,j=1 tj
thefollowing
1 n N
L¯ (θ)= ∑∑β ∥σ¯ S(θ;t ,e−µtx +σ¯ ξ )+ξ ∥2. (6)
em
2n
j tj j i tj ij ij
i=1j=1
We will use (6) as the training objective function in our analysis. For simplicity, we also denote
f(θ;i,j)=β j∥σ¯ tjS(θ;t j,e−µtx i+σ¯ tjξ ij)+ξ ij∥2andthenL¯ em(θ)= 21 n∑n i=1∑N j=1f(θ;i,j).
Architecture. Theanalysisofdiffusionmodeltrainingisingeneralverychallenging. Oneobvious
factoristhecomplicatedarchitectureusedinpracticelikeU-Net[45]andtransformers[43,36]. In
thispaper,wesimplifythearchitectureandconsiderdeepfeedforwardnetworks. Althoughitisstill
farfrompracticalusage, notethissimplestructurecanalreadyprovideinsightsaboutthedesign
space,asshowninlatersections,andismorecomplicatedthanexistingworks[25,47]relatedtothe
trainingofdiffusionmodels(seeSection1.1). Moreprecisely,weconsiderthestandarddeepReLU
networkwithbiasabsorbed:
S(θ;t j,X ij)=W L+1σ(W L⋯W 1σ(W 0X ij)),
1OtherwiseitisnolongerGDtrainingbutstochasticGD.
4whereθ=(W 0,W,W L+1),W 0∈Rm×d,W L+1∈Rd×m,W ℓ∈Rm×mforℓ=1,⋯,L,andσ(⋅)isthe
ReLUactivation.
Algorithm. LetW =(W ,⋯,W ). Weconsiderthegradientdescent(GD)algorithmasfollows
1 L
W(k+1)=W(k)−h∇L¯ (W(k)),
(7)
em
where h > 0 is the learning rate. We fix W 0 and W L+1 throughout the training process and only
updateW ,⋯,W ,whichisacommonlyusedsettingintheconvergenceanalysisofneuralnetworks
1 L
toavoidinvolvedtechnicalcomputationwhilestillmaintainingtheabilityoftheneuralnetworkto
learnviatheLtrainedlayers[1,10,25]. Wealsodenoteθ(k)=(W 0,W(k),W L+1).
Initialization.WeemploythesameinitializationasinAllen-Zhuetal.[1],whichistoset(W(0)
) ∼
ℓ ij
N(0, 2)forℓ=0,⋯,L,i,j =1,⋯,m,and(W(0) ) ∼N(0,1)fori=1⋯,d,j =1⋯,m.
m L+1 ij d
Forthis setup, themain challenge inourconvergence analysisfordenoisingscorematchinglies
inthenatureofthedata. 1)TheoutputdataisanunboundedGaussianrandomvector,andcannot
berescaledasassumedinmanytheoreticalworks(forexample,Allen-Zhuetal.[1]assumedthe
outputdatatobeofordero(1)). 2)TheinputdataX isthesumoftwoparts: x whichfollows
ij i
fromtheinitialdistributionP ,andaGaussiannoiseσ¯ ξ . Therefore,anyassumptionontheinput
0 tj ij
dataneedstoagreewiththisnoisyandunboundednature, andcommonlyusedassumptionslike
dataseparability[1,37]cannolongerbeused. 3)Thedenoisingobjectivehasanon-zerominimum,
whichmeansthereisnowaythatwecanassumeinterpolationofthemodel.
Todealwiththeaboveissues,weinsteadmakethefollowingassumptions.
Assumption1(Onnetworkhyperparametersandinitialdataoftheforwarddynamics). Weassume
thefollowingholds:
1. Datascaling: ∥x ∥=Θ(d1/2)foralli.
i
2. High-dimensionaldata: Theinputdimensionisthesameorderasthenetworkwidthd=Θ(m),
andbothneedtobelargeinthesensethatm=Ω(poly(n,N,L,T/t )).
0
Weremarkthatthefirstassumptionfocusesonlyontheinitialdatax insteadofthewholesolutionof
i
theforwarddynamicsX whichincorporatestheGaussiannoise. Also,thisassumptionisindeednot
ij
farawayfromreality;forexample,itholdswithprobabilityatleast1−O(exp(−Ω(d))forGaussian
randomvectorsfollowingN(0,I). Thesecondassumptionrequireshigh-dimensionaldataandisto
ensurethattheaddedGaussiannoiseiswellseparatedwithhighprobability(seeLemma..). Notethis
isdifferentfromthedataseparabilityassumption[1,37]sincethereisnoguaranteeoftheseparability
ofx ∼P intheinputX .
i 0 ij
Wealsomakethefollowingassumptionsonthehyperparametersofthedenoisingscorematching.
Assumption2(Onthedesignofdiffusionmodels). Weassumethefollowingholds:
1. Weighting: ∑N j=1w(t j)(t j−t j−1)σ¯
tj
<O(N).
2. Variance: σ¯ >0andσ¯ =Θ(1).
t0 tN
The first assumption is to guarantee that the weighting function w(t) is properly scaled. This
expressionw(t j)(t j−t j−1)σ¯
tj
isobtainedfromprovingtheupperandlowerboundsofthegradient
of(6),andisdifferentfromthetotalweightingβ definedabove. Inthesecondassumption,σ¯ >0
i t0
ensurestheoutputξ /σ¯ iswell-defined. Theσ¯ =Θ(1)guaranteesthatthescalesofthenoise
ij tj tN
σ¯ ξ andtheinitialdatax areofthesameorderattheendoftheforwardprocess,namely,theinitial
tj ij i
datax iseventuallypush-forwardedtonearGaussianwiththepropersize. Therefore,Assumption2
i
alignswithwhathasbeenusedinpractice(seeSection4andKarrasetal.[31],Songetal.[52]for
examples).
Thefollowingtheoremsummarizesourconvergenceresultforthetrainingofthescorefunction.
Theorem1(ConvergenceofGD). Leth=Θ( 1 ⋅
minjw(tj)(tj−tj−1)σ¯tj
).
m2d2nNL2 maxjw(tj)(tj−tj−1)σ¯tj∑ kw(tj)(tj−tj−1)/σ¯tj
Define a set of indices to be G(s) = {j = 1⋯,N∣f(θ(s);i,j) ≥ f(θ(s);i′,j′)foralli′,j′}. Then
givenAssumption1and2,fork=O(d3/4nN1/2L1/2logm3/2log(nN)),withprobabilityatleast
1−exp(−Ω(logd)),
5√
L¯ (W(k+1))−L¯∗ ≤C
(L3/2 logmn2N2
⋅
max jw(t j)(t j−t j−1)σ¯
tj)
em em 5 m1/2−4c min jw(t j)(t j−t j−1)σ¯
tj
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
Eni
√
k m logd
+∏(1−C 6hw(t j∗(s))(t j∗(s)−t j∗(s)−1)σ¯
t
j∗(s)⋅( d1/2+4cn2N))⋅(L¯ em(W(0))−L¯∗ em)
s=0
where L¯∗ is the minimum of L¯ (⋅), the universal constants C ,C > 0, c ∈ (1/10,1/8), and
em em 5 6
j∗(s)=argmax j∈G(s)w(t j)(t j−t j−1)σ¯ tj.
The above theorem implies that for denoising score matching objective, GD has exponen-
tial convergence into a neighbourhood of the minimum. For example, if we simply take
j∗ = min jw(t j)(t
j
− t j−1)σ¯ tj, then L¯ em(W(k+1)) − L¯∗
em
is further upper bounded by
√ k+1
(1−C 6hw(t j∗)(t j∗−t j∗−1)σ¯
t j∗
⋅( d1m /2+4l co ng 2d N)) ⋅(L¯ em(W(0))−L¯∗ em)+E ni. TheE
ni
isdue
to the lack of interpolation ability of the model and is obtained via Gronwall’s inequality. Note
interpolationisacommonlyusedsettingintheconvergenceanalysisofneuralnetworks[38,1,37,
e.g.];ifsuchasettingisremoved,theminimumofloss(6)isthenunknowntous. Consequently,
itcannotbeutilizedinanalyzingtheupperboundofthegradient, whichmakestheboundbeing
especially loose near the minimum. The rate of convergence can be interpreted in the following
way: 1)atthekthiteration,wecollectalltheindicesofthetimepointsintoG(k) wheref(θ;i,j)
h ina ds icth ee sm ana dxi dm eu nm oteva tl hu ee; in2 d) ew xe tothe bn ec jh ∗o (o ks )e
,
t ah ne dm oa bx ti am inum theof dw ec( at yj) r( at tj io− ft oj r−1 t) hσ¯ etj nea xm to in teg raa tl il os nuc ah
s
√
1−C 6hw(t j∗(k))(t j∗(k)−t j∗(k)−1)σ¯
t
j∗(k)⋅( d1m /2+4l co ng 2d N).
TheproofofTheorem1isinAppendixB,wheretheanalysisframeworkisadaptedfromAllen-Zhu
etal.[1]. Forthelowerboundofgradient,whichisthekeyoftheproof,wedevelopanewmethodto
dealwiththedifficultiesindenoisingscorematchingsetting(seethediscussionsearlythissection).
Ournewmethodconsistsofthedecouplingofthegradientfromthegeometricpointofview. See
AppendixB.1formoredetails.
3.2 Sampling
Inthissection,weprovetheconvergenceofthebackwardprocessinthevarianceexplodingsetting,
whichisanextensiontoBentonetal.[8]. Forthesimplificationofnotations,wedefinethebackward
timeschedulet←
k
=T −t N−k. Then0=t←
0
<t←
1
<⋯<t←
N
=T −δ.
Generationalgorithm. WeconsidertheexponentialintegratorschemeofthebackwardSDE(2)
withf ≡02. Thegenerationalgorithmcanbepiecewiselyexpressedasacontinuous-timeSDE:for
t
anyt∈[t←,t← ), √
k k+1
dY¯ t=2σ T2 −ts(θ;T −t← k,Y¯ t← k)dt+ 2σ T2 −tdW¯ t. (8)
Initialization. Denoteq ∶=Law(Y¯)forallt∈[0,T −δ]. WechoosetheGaussianinitialization,
t t
q =N(0,σ¯2).
0 T
Ourconvergenceresultreliesonthefollowingassumption.
Assumption3. Thedistributionphasafinitesecondmoment: E x∼p[∥x∥2]=m2 2<∞.
Nextwestatethemainconvergenceresult,whoseproofisprovidedinAppendixC.
Theorem2. UnderAssumption3,foranyδ∈(0,1)andT >1,wehave
m2 N−1
KL(p δ∣q T−δ)≲ σ¯ T22 + k∑ =0γ kσ T2 −t← kE Y t← k∼p T−t← k[∥s(θ;T −t← k,Y t← k)−∇lnp T−t← k(Y t← k)∥2]
(cid:176) ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
EI ES
+dN k∑ =− 01 γ k∫
t←
kt← k+1 σσ
¯
TT 44 −− ttdt+ m
σ¯
T22 2 +(m2 2+d)N k∑ =− 11 (1−e−σ¯ T2 −t← k)σ¯ T4 −t← k
σ¯
T2− −σ¯
t←
kT2 −− 1t σ¯← k T4+1 −σ¯
t←
kT2 −t← k−1 . (9)
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
ED
whereγ ∶=t← −t←forallk=0,1,⋯,N −1isthestepsizeofthediscretizationof (2).
k k+1 k
2Theexponentialintegratorschemeisdegeneratesincef ≡ 0. Timediscretizationisappliedwhenwe
t
evaluatethescoreapproximations{s(θ;t,Y¯)}.
t
6Theorem2isanextensionoftheconvergenceresultinBentonetal.[8]ofVPSDE-baseddiffusion
modeltoVESDE-baseddiffusionmodel. SameasBentonetal.[8],ourresultonlyrequirethedata
distributiontohavefinitesecondmoment,anditachievesthesharpalmostlineardatadimension
dependenceundertheexponentialtimeschedule.However,ourresultappliestovarieschoicesoftime
schedule,whichenablesustoinvestigatingthedesignspaceofthediffusionmodel,aswewilldiscuss
in Section 4. On the other hand, Gao and Zhu [24] obtains similar results as ours under various
time schedules and variance schedules for VESDE-based diffusion models. However, their data
assumption(compactsupport)isstrongerthanours(finitesecondmoment). Moreimportantly,under
differenttimeschedules,ourresultimpliesiterationcomplexitieswithafactordimprovementover
complexitiesinGaoandZhu[24]. AdetaileddiscussiononcomplexitiesisgiveninAppendixF.1.
TheE ,E ,E in(9)representthethreetypesoferrors: initializationerror,discretizationerror,and
I D S
scoreestimationerror,respectively. TheE quantifiestheerrorbetweentheinitialdensityofthe
I
samplingalgorithmq andtheidealinitializationp ,whichisthedensitywhentheforwardprocess
0 T
stopsattimeT. E istheerrorstemmedfromthediscretizationofthebackwarddynamics. The
D
scoreerrorE characterizestheerroroftheestimatedscorefunctionandthetruescore,andisrelated
S
totheoptimizationerrorofL¯ . However,inTheorem2,populationlossisneededinsteadofthe
em
empiricalversionL¯ (6). Besidesthis,theweightingγ σ2 isnotnecessarilythesameasthe
em k T−t←
totalweightinginL¯ (6)β ,dependingonchoicesofw(t )ank dtimeandvarianceschedules(see
em j j
moreinSection4). Wewilllateronintegratetheoptimizationerror(Theorem1)intothisscoreerror
E toobtainafullerroranalysisinSection3.3.
S
Remark1(sharpnessofdependenceindandm2). Inoneofthemostsimplestcases,whenthedata
2
distributionisGaussian,thescorefunctionisexplicitlyknown. HenceKL(p δ∣q T−δ)canbeexplicitly
computedaswell,whichverifiesthatthedependenceofparametersdandm2issharpinE andE .
2 I D
Based on the convergence result in Theorem 2, we can discuss iteration complexities of the VE
generationalgorithmandtheoptimalchoiceofstepsizeunderdifferentvarianceschedule{σ¯ t} t∈[0,T].
AdetaileddiscussionwillbeprovidedinSection4.2.
3.3 Fullerroranalysis
In this section, we combine the analyses from the previous two sections to obtain an end-to-end
generationerrorbound. Thisisobtainedbyapplyingthetimeandvariancescheduleinsamplingto
thetrainingobjective.
Themaintheoremisstatedinthefollowing.
Corollary1. UnderthesameconditionsasTheorem1and2,wehave
σ2
KL(p δ∣q T−δ)≲E
I
+E D+m kax w(ttN N− −k
k)
⋅(ϵ n+∣L¯∗ em+C¯∣+E
ni
√
+N ∏GD (1−C 6hw(t j∗(s))(t j∗(s)−t j∗(s)−1)σ¯
t
j∗(s)⋅( d1m /2+4l co ng 2d N))⋅(L¯ em(W(0))−L¯∗ em))
s=0
whereE ,E aredefinedinTheorem2,N isthetotalnumberofiterationsusedintraining,and
I D GD
ϵ n=∣L¯(WNGD)−L¯ em(WNGD)∣→0asn→∞.
Inthistheorem,thediscretizationerrorE andinitializationerrorE arethesameasTheorem2.
D I
ForthescoreerrorE ,notethatthetimeschedulesusedintrainingandsamplingarenotnecessarily
S
thesameandthereforelogicallyweneedtofirstreschedulethetimepointsinthetrainingobjective
bytheoneusedinsamplingandthenapplyTheorem1. Indeed,ourtraininganalysisisvalidfor
generaltimeschedulesandthereforecandirectlyfitintothesamplingerroranalysis,whichisin
contrasttoexistingworks[25,47](seemorediscussionsinSection1.1). Wewilldiscusstheeffect
ofmax kσ t2 N−k/w(t N−k)underdifferenttimeandvarianceschedulesinSection4. Thetrainingerror
neededinE isthepopulationerrorbetweenthenetworkandthetruescore. Therefore,besides
S
theoptimizationerroroftheempiricaltrainingobjectiveinTheorem1,weneedϵ tomeasurethe
n
distancebetweentheempiricallossandpopulationlossattheendoftheGDiteration, whichby
centrallimittheorem,convergesto0asn→∞.Additionally,thereisalso∣L¯∗ +C¯∣thatcharacterizes
em
thegeneralizationabilityofthemodel. Inthispaper,wefocusontheoptimizationandsamplingand
willleavethegeneralizationpartforfutureanalysis.
74 Theory-basedunderstandingofthedesignspaceanditsrelationtoexisting
empiricalcounterparts
Thissectiontheoreticallyexploresthechoiceofparametersinbothtrainingandsamplingandshow
thattheyagreewiththeonesusedinEDM[31]andSongetal.[52]underdifferentcircumstances.
4.1 Choiceoftotalweightingfortraining
Inthissection,wefocusondevelopingtheoptimaltotalweightingβ forthetrainingobjective(6).
j
Wequalitativelyshowthatthe“bell-shaped”weighting,whichistheoneusedinEDM[31],willlead
totheoptimalrateofconvergenceintwosteps: 1)∥σ¯ S(θ;t ,X )+ξ ∥isinversely“bell-shaped”
tj j tj ij
with respect to j, and 2) f(θ;i,j) = β ∥σ¯ S(θ;t ,X )+ξ ∥ should be close to each other for
j tj j tj ij
anyi,j.
4.1.1 Inversely“bell-shaped”loss∥σ¯ S(θ;t ,X )+ξ ∥withrespecttoj.
tj j tj ij
Proposition1. Consider∥σ¯ S(θ;t ,X )+ξ ∥. Then
tj j tj ij
1. ∀ϵ >0,∃δ >0,s.t.,when0≤σ¯ <δ ,∥σ¯ S∗(t ,X +σ¯ ξ)+ξ ∥>∥ξ ∥−ϵ .
1 1 tj 1 tj j 0 tj ij ij 1
2. ∀ϵ >0,∃M >0,s.t.,whenσ¯ >M,∥σ¯ S∗(t ,X +σ¯ ξ)+ξ ∥≥M2(∥S(t ,ξ )∥−ϵ ).
2 tj tj j 0 tj ij j ij 2
3. ∀ϵ > 0, there exists some neural network S, s.t., ∥σ¯ tjS(e−µtjx i+σ¯ tjξ ij)+ξ ij∥ ≤ ϵ. where
i=1,⋯,n,andj =N ,⋯,N forsomeN >0andN <N.
1 2 1 2
Thispropositionindicatesthatwhenσ¯ iseitherverysmallorverylarge,∥σ¯ S(θ;t ,X )+ξ ∥
tj tj j tj ij
isawayfrom0;whenσ¯ fallswithinthemiddlerange,∥σ¯ S(θ;t ,X )+ξ ∥canbesufficiently
tj tj j tj ij
small. Therefore,thefunction∥σ¯ S(θ;t ,X )+ξ ∥isroughlyaninversely“bell-shaped”curve
tj j tj ij
with respect to j. Additionally, 3 in Proposition 1 is obtained from the universal approximation
theoremandthereforethenetworkcanbeeitherwideenoughwithfixeddepthorwithm=Θ(d)but
deepenough[33];thelattercaseisclosertoourarchitectureusedinThm.1butweremarkthatthis
propositionservesasanintuitiveunderstandingofthelossandthereisagapbetweenourchoiceof
depthandbiasandthetrueapproximatednetworkduetothelimitationofexistingprooftechniques.
4.1.2 Ensuringcomparablevaluesoff(θ;i,j)foroptimalrateofconvergence.
Corollary2. UnderthesameconditionsasTheorem1,forsomelargeK >0,if∣f(θ(k+K);i,j)−
f(θ(k+K);l,s)∣≤ϵholdsforallk>0andall(i,j),(k,s),withsomesmalluniversalconstantϵ>0,
thenwehave,forsomeC >0,
7
L¯ (W(k+K))−L¯∗ ≤E
em em ni
√
k
m logd
+(1−C 7h j=m 1,a ⋯x ,Nw(t j)(t j−t j−1)σ¯
tj
⋅( d1/2+4cn2N)) ⋅(L¯ em(W(K))−L¯∗ em).
The above corollary characterizes the asymptotic behavior of the con-
vergenceofGD.Inparticular,whentheiterationisneartheminimizer, 7
if f(θ(k);i,j)’s are almost the same for any i,j, then the decay ratio 6
of the next iteration is minimized. More precisely, the index set G(k) 5
definedinTheorem1isroughlythewholeset{1,⋯,N},andtherefore M4
w(t j∗)(t
j∗
−t j∗−1)σ¯
t j∗
canbetakenasthemaximumvalueoverallj,
-DE3
whichconsequentlyleadstotheoptimalrate. 2
1
4.1.3 “Bell-shaped”weighting: ourtheoryandEDM.
0
Combining the above two aspects, the optimal rate of convergence 10!2 100 102
<7
leads to the choice of total weighting β such that f(θ;i,j) =
j
β j∥σ¯ tjS(θ;t j,X tj)+ξ ij∥ is close to each other; as a result, the total Figure 2: EDM weight-
weightingshouldbechosenasa“bell-shaped”curvewithrespecttoj ingβ .
EDM
accordingtotheshapeofthecurvefor∥σ¯ S(θ;t ,X )+ξ ∥.
tj j tj ij
Before comparing our weighting and the one used in EDM [31], let us first recall that the EDM
trainingobjective3canbewrittenasE σ¯∼ptrainE y,nλ(σ¯)∥D θ(y+n;σ¯)−y∥2
1 −(logσ¯−Pmean)2 σ¯2+σ2
=
Z
∫ e 2Ps2 td
σ2
dataE X0,ξ∥σ¯s(θ;t,X t)+ξ∥2dσ¯, (10)
1 data
3InKarrasetal.[31],theyuseP =−1.2, P =1.2, σ =0.5, σ¯ =0.002, σ¯ =80.
mean std data min max
8whereZ 1isanormalizationconstant,andwedenoteβ EDM(σ¯)=e−(logσ¯ 2− PP s2m tdean)2 ⋅ σ¯ σ¯2 2+ σσ 2d2 ata ⋅σ¯2tobe
data
thetotalweightingofEDM.
Figure2exhibitsthetotalweightingofEDMβ withrespecttoσ¯. Asisshowninthepicture,
EDM
thisisa“bell-shaped”curve4,whichcoincideswithourchoiceoftotalweightingintheabovetheory.
When σ¯ is small, according to Proposition 1, the lower bound of ∥σ¯ S(θ;t ,X )+ξ ∥ cannot
tj j tj ij
vanishandthereforeneedsthesmallestweightingoverallσ¯. Whenσ¯ isverylarge,Proposition1
showsthat∥σ¯ S(θ;t ,X )+ξ ∥couldstillbelargebutcanbedeterminedbytheneuralnetwork;
tj j tj ij
therefore,whilestillrequiringsmallweighting,itcouldbelargerthantheonesusedinsmallσ¯’s.
Whenσ¯ takesthemiddlevalue,thescaleoftheoutputdataξ /σ¯ isroughlythesameastheinput
ij j
dataX andthereforemakesiteasierfortheneuralnetworktofitthedata, whichadmitslarger
ij
weighting.
4.2 Choiceoftimeandvarianceschedules
In this section, we discuss the choice of time and variance schedules based on the three errors
E ,E ,E intheerroranalysisofSection3.3andconsiderthetwosituations: whenE dominates,
S D I S
polynomialschedule[31]ispreferable;whenE +E dominates,exponentialschedule[52]isbetter.
D I
4.2.1 WhenscoreerrorE dominates
S
AsisshowninCorollary1,themainimpactofdifferenttimeandvarianceschedulesonscoreerror
E
S
appearsinthetermmax kσ t2 N−k/w(t N−k),whenthescorefunctionisapproximatedtoacertain
accuracy. Itremainstocomputew(t)undervariouschoicesofschedules.
Generalruleofconstructingw(t). Toensurefaircomparisonsbetweendifferenttimeandvariance
schedules,wemaintainafixedthetotalweightinginthetrainingobjective. Additionally,tofacilitate
comparisonswithpracticalusage,weadoptthetotalweightinginEDM,i.e.,β =C β (σ¯ ),
j 3 EDM tj
forsomeuniversalconstantC >0. ThereasonforusingtheEDMtotalweightingisthataccording
3
to Section 4.1, our total weighting β should be “bell-shaped” with respect to j, which agrees
j
qualitativelywiththeoneusedinEDM.
Polynomialschedule[31]vsexponentialschedule[52]. Weapplythetwoschedules(seeTable1)
u hase vd ets he ap tar
1
2a (te σ¯l my at xo −w σ¯e mig ah xt (in σ¯σ¯g 2m2a inn )d 1c /Nom )p isut le arm gea rx tk haσ nt2
N (− σ¯k
m/w ax( −t N (− σ¯k m1)
/
aρi xn −Ta σ¯b m1/l aρe
x
N−2 σ¯.
m1/
iW
ρ
n)h ρe )n ,mN ea> ni1 n0 g, tw he
e
max
polynomialtimescheduleusedinEDMisbetterthantheexponentialscheduleinVE.
Table1: Polynomialscheduleandexponentialschedule. σ¯ =0.002, σ¯ =80, ρ=7[31].
min max
Variancescheduleσ¯ Timeschedulet
t k
Poly. [31] t
(σ¯1/ρ −(σ¯1/ρ −σ¯1/ρ)N−k)ρ
max max min N
√ N−k
Exp. [52] t σ¯2 (σ¯ m2 in) N
max σ¯2
max
Table2: Comparisonsbetweendifferentschedules.
E (scoreerror)dominates E +E (samplingerror)dominates
S D I
max σ2 /w(t ) Choice N Choice
k tk k
Poly. [31] C 4(σ¯ max−(σ¯ m1/ aρ x− σ¯ m1/ aρ x N−σ¯ m1/ iρ n)ρ ) (cid:33) Ω(m2 2 d∨dρ2(σ¯ σ¯m ma inx)1/ρ σ¯ m2 ax)
Exp. [52] C ⋅ 1(σ¯ −σ¯ (σ¯ m2 in)1/N) Ω(m2 2∨dln(σ¯max)2σ¯2 ) (cid:33)
4 2 max max σ¯ m2
ax
d σ¯min max
4.2.2 WhendiscretizationerrorE andinitializationerrorE dominate
D I
Inthissection,wecomparethetwodifferentschedulesinTable1bystudyingtheiterationcomplexity
ofthesamplingalgorithm,i.e.,numberoftimepointsN,whenE +E dominates.
D I
Generalrulesofcomparison. Weconsiderthecasewhenthediscretizationandinitializationerrors
areboundedbythesamequantityϵ,i.e.,E +E ≲ε.ThenaccordingtoTheorem2andCorollary1,
I D
4Thishorizontalaxisisinlog-scaleandtheplotinregularscaleisalittlebitskewed,notpreciselya“bell”
shape.However,weremarkthatthetrendofthecurvestillmatchesourtheory.
9wecomputetheiterationcomplexityofachievingthiserrorusingthetwoschedulesinTable1. More
detailsareprovidedinAppendixF.1.
Polynomialschedule[31]vsexponentialschedule[52]. AsisshowninthelastcolumnofTable2,
theiterationcomplexityunderexponentialschedule[52]hasthepoly-logarithmicdependenceon
theratiobetweenmaximalandminimalvariance(σ¯ /σ¯ )5,whichisbetterthanthecomplexity
max min
underpolynomialschedule[31],whichispolynomiallydependentonσ¯ /σ¯ . Bothcomplexities
max min
arederivedfromTheorem2bychoosingdifferentparameters.
Remark2(Optimalρinthepolynomialschedule[31]). Forfixedσ¯ /σ¯ ,theoptimalρthat
max min
minimizes the iteration complexity is ρ = 1ln(σ¯max). In [31], it was empirically observed that
2 σ¯min
withfixediterationcomplexity, thereisanoptimalvalueofρthatminimizestheFID.Ourresult
indicatesthat,forσ¯ andσ¯ ,hencethedesiredaccuracyinKLdivergencebeingfixed,there
max min
isanoptimalvalueofρthatminimizestheiterationcomplexitytoreachthefixedaccuracy. Even
thoughweconsideradifferentmetric,ourresultprovidesaquantitativesupportofthephenomenon
observedin[31].
AcknowledgmentsandDisclosureofFunding
TheauthorsaregratefulforthepartiallysupportbyNSFDMS-1847802,Cullen-PeckScholarship,
andGT-EmoryHumanity.AIAward.
References
[1] ZeyuanAllen-Zhu,YuanzhiLi,andZhaoSong. Aconvergencetheoryfordeeplearningvia
over-parameterization.InInternationalconferenceonmachinelearning,pages242–252.PMLR,
2019.
[2] ZeyuanAllen-Zhu,YuanzhiLi,andZhaoSong. Ontheconvergencerateoftrainingrecurrent
neuralnetworks. Advancesinneuralinformationprocessingsystems,32,2019.
[3] NamrataAnandandTudorAchim. Proteinstructureandsequencegenerationwithequivariant
denoisingdiffusionprobabilisticmodels. arXivpreprintarXiv:2205.15019,2022.
[4] BrianDOAnderson. Reverse-timediffusionequationmodels. StochasticProcessesandtheir
Applications,12(3):313–326,1982.
[5] SanjeevArora,SimonDu,WeiHu,ZhiyuanLi,andRuosongWang.Fine-grainedanalysisofop-
timizationandgeneralizationforoverparameterizedtwo-layerneuralnetworks. InInternational
ConferenceonMachineLearning,pages322–332.PMLR,2019.
[6] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg.
Structureddenoisingdiffusionmodelsindiscretestate-spaces. AdvancesinNeuralInformation
ProcessingSystems,34:17981–17993,2021.
[7] DmitryBaranchuk,AndreyVoynov,IvanRubachev,ValentinKhrulkov,andArtemBabenko.
Label-efficientsemanticsegmentationwithdiffusionmodels. InInternationalConferenceon
LearningRepresentations,2022. URLhttps://openreview.net/forum?id=SlxSY2UZQT.
[8] JoeBenton,ValentinDeBortoli,ArnaudDoucet,andGeorgeDeligiannidis. Nearlyd-linear
convergenceboundsfordiffusionmodelsviastochasticlocalization.InTheTwelfthInternational
ConferenceonLearningRepresentations,2024.
[9] AdamBlock,YoussefMroueh,andAlexanderRakhlin. Generativemodelingwithdenoising
auto-encodersandlangevinsampling. arXivpreprintarXiv:2002.00107,2020.
[10] QiCai,ZhuoranYang,JasonDLee,andZhaoranWang. Neuraltemporal-differencelearning
convergestoglobaloptima. AdvancesinNeuralInformationProcessingSystems,32,2019.
5Theexponentialtimescheduleunderthevarianceschedulein[31]alsohasthepoly-logarithmicdependence
onσ¯ /σ¯ .Underbothvarianceschedulesin[31]and[52],itcanbeshownthatexponentialtimeschedule
max min
isoptimal.DetailsareprovidedinAppendixF.1.
10[11] HanqunCao,ChengTan,ZhangyangGao,YilunXu,GuangyongChen,Pheng-AnnHeng,and
StanZLi. Asurveyongenerativediffusionmodels. IEEETransactionsonKnowledgeand
DataEngineering,2024.
[12] Hongrui Chen and Lexing Ying. Convergence analysis of discrete diffusion model: Exact
implementationthroughuniformization. arXivpreprintarXiv:2402.08095,2024.
[13] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative
modeling: User-friendly bounds under minimal smoothness assumptions. In International
ConferenceonMachineLearning,pages4735–4763.PMLR,2023.
[14] MinshuoChen,KaixuanHuang,TuoZhao,andMengdiWang. Scoreapproximation,estimation
and distribution recovery of diffusion models on low-dimensional data. In International
ConferenceonMachineLearning,pages4672–4712.PMLR,2023.
[15] Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang. An overview of diffusion
models: Applications, guided generation, statistical rates and optimization. arXiv preprint
arXiv:2404.07771,2024.
[16] NanxinChen,YuZhang,HeigaZen,RonJWeiss,MohammadNorouzi,andWilliamChan.
Wavegrad:Estimatinggradientsforwaveformgeneration.InInternationalConferenceonLearn-
ingRepresentations,2021. URLhttps://openreview.net/forum?id=NsMLjcFaO8O.
[17] SitanChen,SinhoChewi,JerryLi,YuanzhiLi,AdilSalim,andAnruRZhang. Samplingisas
easyaslearningthescore: theoryfordiffusionmodelswithminimaldataassumptions. arXiv
preprintarXiv:2209.11215,2022.
[18] HyungjinChungandJongChulYe. Score-baseddiffusionmodelsforacceleratedmri. Medical
imageanalysis,80:102479,2022.
[19] ValentinDeBortoli. Convergenceofdenoisingdiffusionmodelsunderthemanifoldhypothesis.
arXivpreprintarXiv:2208.05314,2022.
[20] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.
Advancesinneuralinformationprocessingsystems,34:8780–8794,2021.
[21] SimonDu,JasonLee,HaochuanLi,LiweiWang,andXiyuZhai. Gradientdescentfindsglobal
minima of deep neural networks. In International conference on machine learning, pages
1675–1685.PMLR,2019.
[22] SimonSDu,XiyuZhai,BarnabasPoczos,andAartiSingh.Gradientdescentprovablyoptimizes
over-parameterizedneuralnetworks. arXivpreprintarXiv:1810.02054,2018.
[23] ChenruDuan,YuanqiDu,HaojunJia,andHeatherJKulik. Accuratetransitionstategeneration
withanobject-awareequivariantelementaryreactiondiffusionmodel. NatureComputational
Science,3(12):1045–1055,2023.
[24] XuefengGaoandLingjiongZhu. Convergenceanalysisforgeneralprobabilityflowodesof
diffusionmodelsinwassersteindistances. arXivpreprintarXiv:2401.17958,2024.
[25] YinbinHan,MeisamRazaviyayn,andRenyuanXu. Neuralnetwork-basedscoreestimationin
diffusionmodels: Optimizationandgeneralization. InTheTwelfthInternationalConferenceon
LearningRepresentations,2024. URLhttps://openreview.net/forum?id=h8GeqOxtd4.
[26] Ye He, Kevin Rojas, and Molei Tao. Zeroth-order sampling methods for non-log-concave
distributions:Alleviatingmetastabilitybydenoisingdiffusion.arXivpreprintarXiv:2402.17886,
2024.
[27] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advances
inneuralinformationprocessingsystems,33:6840–6851,2020.
[28] JonathanHo,ChitwanSaharia,WilliamChan,DavidJFleet,MohammadNorouzi,andTim
Salimans. Cascadeddiffusionmodelsforhighfidelityimagegeneration. JournalofMachine
LearningResearch,23(47):1–33,2022.
11[29] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and
DavidJFleet. Videodiffusionmodels. AdvancesinNeuralInformationProcessingSystems,
35:8633–8646,2022.
[30] AapoHyvärinenandPeterDayan. Estimationofnon-normalizedstatisticalmodelsbyscore
matching. JournalofMachineLearningResearch,6(4),2005.
[31] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-basedgenerativemodels. AdvancesinNeuralInformationProcessingSystems,35:
26565–26577,2022.
[32] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli
Laine. Analyzingandimprovingthetrainingdynamicsofdiffusionmodels. arXivpreprint
arXiv:2312.02696,2023.
[33] Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In
Conferenceonlearningtheory,pages2306–2327.PMLR,2020.
[34] HoldenLee,JianfengLu,andYixinTan.Convergenceforscore-basedgenerativemodelingwith
polynomialcomplexity. AdvancesinNeuralInformationProcessingSystems,35:22870–22882,
2022.
[35] YongjaeLeeandWooChangKim. Conciseformulasforthesurfaceareaoftheintersectionof
twohypersphericalcaps. KAISTTechnicalReport,2014.
[36] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto.
Diffusion-lmimprovescontrollabletextgeneration. AdvancesinNeuralInformationProcessing
Systems,35:4328–4343,2022.
[37] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradientdescentonstructureddata. Advancesinneuralinformationprocessingsystems,31,
2018.
[38] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu
activation. Advancesinneuralinformationprocessingsystems,30,2017.
[39] XinLiu,ZhisongPan,andWeiTao. Provableconvergenceofnesterov’sacceleratedgradient
methodforover-parameterizedneuralnetworks. Knowledge-BasedSystems,251:109277,2022.
[40] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by
estimatingtheratiosofthedatadistribution. arXivpreprintarXiv:2310.16834,2023.
[41] ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefano
Ermon. SDEdit: Guidedimagesynthesisandeditingwithstochasticdifferentialequations. In
InternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.
net/forum?id=aBsCjcPu_tE.
[42] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal
distributionestimators. InInternationalConferenceonMachineLearning,pages26517–26582.
PMLR,2023.
[43] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedings
oftheIEEE/CVFInternationalConferenceonComputerVision,pages4195–4205,2023.
[44] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen. Hierarchical
text-conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,
2022.
[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks
for biomedical image segmentation. In Medical image computing and computer-assisted
intervention–MICCAI2015: 18thinternationalconference,Munich,Germany,October5-9,
2015,proceedings,partIII18,pages234–241.Springer,2015.
12[46] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom
Blundell, Pietro Lió, Carla Gomes, Max Welling, et al. Structure-based drug design with
equivariantdiffusionmodels. arXivpreprintarXiv:2210.13695,2022.
[47] KulinShah,SitanChen,andAdamKlivans. Learningmixturesofgaussiansusingtheddpm
objective. AdvancesinNeuralInformationProcessingSystems,36:19636–19649,2023.
[48] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsuper-
visedlearningusingnonequilibriumthermodynamics. InInternationalconferenceonmachine
learning,pages2256–2265.PMLR,2015.
[49] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data
distribution. Advancesinneuralinformationprocessingsystems,32,2019.
[50] YangSongandStefanoErmon.Improvedtechniquesfortrainingscore-basedgenerativemodels.
Advancesinneuralinformationprocessingsystems,33:12438–12448,2020.
[51] YangSong,ConorDurkan,IainMurray,andStefanoErmon. Maximumlikelihoodtraining
of score-based diffusion models. Advances in neural information processing systems, 34:
1415–1428,2021.
[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon,
and Ben Poole. Score-based generative modeling through stochastic differential equations.
InternationalConferenceonLearningRepresentations,2021.
[53] Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff
bound. arXivpreprintarXiv:1906.03593,2019.
[54] PascalVincent. Aconnectionbetweenscorematchinganddenoisingautoencoders. Neural
computation,23(7):1661–1674,2011.
[55] JosephLWatson,DavidJuergens,NathanielRBennett,BrianLTrippe,JasonYim,HelenE
Eisenach,WoodyAhern,AndrewJBorst,RobertJRagotte,LukasFMilles,etal. Denovo
designofproteinstructureandfunctionwithrfdiffusion. Nature,620(7976):1089–1100,2023.
[56] JundeWu, RAOFU,HuihuiFang, YuZhang, YehuiYang, HaoyiXiong, HuiyingLiu, and
YanwuXu. Medsegdiff: Medicalimagesegmentationwithdiffusionprobabilisticmodel. In
MedicalImagingwithDeepLearning,2023. URLhttps://openreview.net/forum?id=
Jdw-cm2jG9.
[57] LingYang,ZhilongZhang,YangSong,ShendaHong,RunshengXu,YueZhao,WentaoZhang,
BinCui,andMing-HsuanYang. Diffusionmodels: Acomprehensivesurveyofmethodsand
applications. ACMComputingSurveys,56(4):1–39,2023.
[58] Jongmin Yoon, Sung Ju Hwang, and Juho Lee. Adversarial purification with score-based
generativemodels. InInternationalConferenceonMachineLearning,pages12062–12072.
PMLR,2021.
[59] YuchenZhu,TianrongChen,EvangelosATheodorou,XieChen,andMoleiTao. Quantum
stategenerationwithstructure-preservingdiffusionmodel. arXivpreprintarXiv:2404.06336,
2024.
13Appendix
A Derivationofdenoisingscorematchingobjective
Inthissection,wewillderivethedenoisingscorematchingobjective,i.e. showtheequivalenceof(3)
and(4). Forsimplicity,wedenoteS tobetheneuralnetworkweuseS(θ;t,X ).
θ t
Consider
E Xt∼Pt∥S(θ;t,X t)−∇logp t∥2=E Xt[∥S θ∥2−2⟨S θ,∇logp t⟩]+E Xt∥∇logp t∥2, (11)
wherep isthedensityofX .
t t
Sincep t(x)=∫ p 0(y)q t(x∣y)dy,whereq t(⋅)isthedensityofX t∣X 0,thenwehave
E Xt⟨S θ,∇logp t⟩=∫ S θ⊺∇logp t⋅p tdx
t
=∫ S θ⊺∇p tdx
t
=∬ S θ⊺∇q t(x∣y)p 0(y)dxdy
=∬ S θ⊺∇logq t(x∣y)p 0(y)q t(x∣y)dxdy
=E X0∼P0E Xt∣X0∼Qt⟨S θ,∇logq t(x t∣x 0)⟩.
Then
(11)=E X0∼P0E Xt∣X0∼Qt[∥S θ∥2−2⟨S θ,∇logq t(x t∣x 0)⟩]+E Xt∥∇logp t∥2
=E X0E Xt∣X0∥S θ−∇logq t∥2+C,
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
(∆)
whereC =E Xt∥∇logp t∥2−E X0E Xt∣X0∥∇logq t(x t∣x 0)∥2.
Moreover,X t∣X 0∼N(e−µtX 0,σ¯ t2I),anditsdensityfunctionis
q
(x∣y)=(2πσ¯2)−d/2exp(−∥x−e−µty∥2
).
t t 2σ¯2
t
Then
(∆)=E X0E Xt∣X0∥S θ−∇logq t∥2
=E X0E Xt∣X0∥S θ−∇
x(−∥X t− 2e σ¯−µ 2tX 0∥2 )∥2
t
=E X0E Xt∣X0∥S θ+
X t− σ¯e− 2µtX 0∥2
t
2
ϵ
=E E ∥S + t ∥ .
X0 ϵt θ σ¯2
t
Letξ= ϵt ∼N(0,I). Then
σ¯t
1
(∆)=E E σ¯ ⋅ ∥σ¯ S +ξ∥2
X0 ξ t σ¯2 t θ
t
1
= E E ∥σ¯ S +ξ∥2
σ¯
X0 ξ t θ
t
B Proofsfortraining
Inthissection,wewillproveTheorem1.
14Beforeintroducingtheconcreteproof,wefirstredefinethedeepfullyconnectedfeedforwardnetwork
r =W X ,q =σ(r ),
ij,0 0 ij ij,0 ij,0
r ij,ℓ=W ℓq ij,ℓ−1,q ij,ℓ=σ(r ij,ℓ), forℓ=1,⋯,L
S(θ;t j,X ij)=W L+1q
ij,L
whereW 0∈Rm×d,W L+1∈Rd×mandW ℓ∈Rm×m;σistheReLUactivation. Wealsodenoteq ij,−1
tobeX .
ij
WealsofollowthenotationinAllen-Zhuetal.[1]anddenoteD ∈Rm×mtobeadiagonalmatrix
i,ℓ
and(D i,ℓ) kk =1 (Wℓqij,ℓ−1) k>0fork=1,⋯,m. Then
q ij,ℓ=D ij,ℓW ℓq ij,ℓ−1
Fortheobjective(6),thegradientw.r.t. tothekthrowofW forℓ=1,⋯,Listhefollowing
ℓ
1 n N
∇
(Wℓ)
kL¯ em(θ)= n∑∑w(t j)(t j−t j−1)
i=1j=1
[( ·W „„„„„„„„„L „„„„„„„„„+ „„„„„„„„„1 „„„„„„„„D „„„„„„„„„„„„„„„i „„„„„j „„„„„„, „„„„L „„„„„„„„„„„W „„„„„„„‚L „⋯ „„„„„„„„„„„„„„„D „„„„„„„„„„„„„„„i „„„„„j „„„„„„, „„„„ℓ „„„„„„„W „„„„„„„„„„„„„„„„„ℓ „„„„„„+ „„„„„„¶1)⊺(σ¯ tjW L+1q ij,L+ξ ij)] kq ij,ℓ−11 (Wℓqij,ℓ−1) k>0
Rij,ℓ+1
Throughouttheproof,weusebothL¯ (θ)andL¯ (W)torepresentthesamevalueoftheloss
em em
function,andweleta=b= 1.
2
ProofofTheorem1. FirstbyLemma4,
L¯ em(W(0))=O(d2a∑w(t j)(t j−t j−1)/σ¯ tj)
j
√
Also,∥∇L¯ (θ)∥≤ Lmax ∥∇ L¯ (θ)∥. Thenwehave
em ℓ Wℓ em
k−1
∥W(k)−W(0)∥≤ ∑h∥∇L¯ (W(i))∥
em
i=0
√ √
≤O( md2a−1NLm jaxw(t j)(t j−t j−1)σ¯ tj)hkm iax L¯ em(W(i))
√ √
≤O( md2a−1NLm jaxw(t j)(t j−t j−1)σ¯
tj
da)hk ∑w(t j)(t j−t j−1)/σ¯
tj
∶=ω
j
Let h = Θ( 1 ⋅
minjw(tj)(tj−tj−1)σ¯tj
) and
m2d2nNL2 maxjw(tj)(tj−tj−1)σ¯tj∑ kw(tj)(tj−tj−1)/σ¯tj
k = O( √
(logm)3/2
log(nN)) =
hm2d5/4L2N1/2 maxjw(tj)(tj−tj−1)σ¯tj∑kw(tj)(tj−tj−1)/σ¯tj
O(d3/4nN1/2L1/2logm3/2log(nN)). Thenω=O( logm3/2 )andbyLemma8,
d3a/2m3/2L3/2
L¯ (W(k+1))−L¯∗
em em
≤L¯ (W(k))−L¯∗ −h∥∇L¯ (W(k))∥2
em em em
√ √ √
+h L¯
em
∑w(t j)(t j−t j−1)σ¯ tjO(ω1/3L2 mlogmda/2−1/2)∥∇L¯ em(W(k))∥
j
√ √ √
+h2 L¯
em
∑w(t j)(t j−t j−1)σ¯ tjO(L2 mda−1/2)∥∇L¯ em(W(k))∥2
j
√
m logdd2b−1.5−4c
≤(1−hw(t j∗)(t j∗+1−t j∗)σ¯
t j∗
⋅Ω(
n2N
))(L¯ em(W(k))−L¯∗ em)
L3/2√ logmn2N3/2√ √ m√ logdd2b−1.5−4c
+hL¯∗
em m1/2−4c
m jaxw(t j)(t j−t j−1)σ¯
tj
∑w(t j)(t j−t j−1)σ¯ tjO(
n2N
)
j
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
hg
15wherethesecondinequalityfollowsfromLemma7with
√
m logdd2b−1.5−4c
∥∇ WLL¯ em(θ(k))∥2=Ω(
n2N
w(t j∗)(t j∗−t j∗−1)σ¯
t
j∗)L¯ em(θ(k)).
BydiscreteGronwall’sinequality,wehave
L¯ (W(k+1))−L¯∗
em em
√
k m logdd2b−1.5−4c
≤∏(1−hw(t j∗(s))(t j∗(s)+1−t j∗(s))σ¯
t
j∗(s)⋅Ω(
n2N
))⋅(L¯ em(W(0))−L¯∗ em)
s=0
√
k−1 k m logdd2b−1.5−4c
+hg ∑ ∏ (1−hw(t j∗(l))(t j∗(l)+1−t j∗(l))σ¯
t
j∗(l)⋅Ω(
n2N
))
s=0l=s+1
√
k m logdd2b−1.5−4c
≤∏(1−hw(t j∗(s))(t j∗(s)+1−t j∗(s))σ¯
t
j∗(s)⋅Ω(
n2N
))⋅(L¯ em(W(0))−L¯∗ em)
s=0
k−1 m√ logdd2b−1.5−4c k−s−1
+hg s∑ =0(1−hm jinw(t j)(t j−t j−1)σ¯
tj
⋅Ω(
n2N
))
√
k m logdd2b−1.5−4c
≤∏(1−hw(t j∗(s))(t j∗(s)+1−t j∗(s))σ¯
t
j∗(s)⋅Ω(
n2N
))⋅(L¯ em(W(0))−L¯∗ em)
s=0
√
+O(L3/2 logmn2N2
⋅
max jw(t j)(t j−t j−1)σ¯
tj)
m1/2−4c min jw(t j)(t j−t j−1)σ¯
tj
wherec∈(1/10,1/8).
B.1 Proofoflowerboundofthegradientattheinitialization
Inthissection,wewillshowthemainpartoftheconvergenceanalysis,whichisthefollowinglower
boundofthegradient.
Lemma1(Lowerbound). Withprobability1−exp(−Ω(logd)),wehave
√
m logdd2b−1.5−4c
∥∇L¯ em(θ(0))∥2≥C 6(
n2N
w(t j∗)(t j∗−t j∗−1)σ¯
t
j∗)L¯ em(θ(0))
√
where (i∗,j∗) = argmax∥ w(tj)( σt ¯tj j−tj−1) (σ¯ tjW L+1q ij,L+ξ ij)∥, and C
6
> 0 is some universal
constant.
Proof. Themainideaoftheproofoflowerboundistodecoupletheelementsinthegradientand
incorporategeometricview. Wefocuson∇ L¯ (θ).
WL em
Step1: Rewrite∇ (WL) kL¯ em(θ)tobethe(i∗,j∗)thtermg 1plustherestnN −1termsg 2.
√
Let(i∗,j∗)=argmax∥ w(tj)( σt ¯tj j−tj−1) (σ¯ tjW L+1q ij,L+ξ ij)∥. Let
⊺
g ij,L=w(t j)(t j−t j−1)(W L+1)k (σ¯ tjW L+1q ij,L+ξ ij)q ij,L−1.
Then
∇
(WL)
kL¯ em(θ)
1 ⊺
= nw(t j∗)(t j∗+1−t j∗)(W L+1)k (σ¯ t j∗W L+1q i∗j∗,L+ξ i∗j∗)q i∗j∗,L−11 (WLq i∗j∗,L−1) k>0
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
∇
1
1 ⊺
+
n
∑ w(t j)(t j−t j−1)(W L+1)k (σ¯ tjW L+1q ij,L+ξ ij)q ij,L−11 (WLqij,L−1) k>0
(i,j)≠(i∗,j∗)
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
∇
2
16Alsodefine
1 ⊺
∇ 1,s= nw(t j∗)(t j∗+1−t j∗)σ¯ t j∗ (W L+1)k W L+1q i∗j∗,L(q i∗j∗,L−1) s1 (WLq i∗j∗,L−1) k>0
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
∇
11,s
1 ⊺
+ nw(t j∗)(t j∗+1−t j∗)(W L+1)k ξ i∗j∗(q i∗j∗,L−1) s1 (WLq i∗j∗,L−1) k>0
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
∇
12,s
1 ⊺
∇ 2,s=
n
∑ w(t j)(t j−t j−1)σ¯ tj(W L+1)k W L+1q ij,L(q ij,L−1) s1 (WLqij,L−1) k>0
(i,j)≠(i∗,j∗)
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
∇
21,s
1 ⊺
+
n
∑ w(t j)(t j−t j−1)(W L+1)k ξ ijq ij,L−1(q ij,L−1) s1 (WLqij,L−1) k>0
(i,j)≠(i∗,j∗)
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
∇
22,s
√
Ourgoalistoshowthatwithhighprobability,thereareatleastO(m logd)numberofrowsksuch
d1/2+4c
that∇ 11,s≥0,∇ 12,s≥0,∇ 21,s≥0,∇ 22,s≥0. Thenwecanlowerbound[∇ (WL) kL¯ em(θ)]2 s by∇2 1,
whichcanbeeventuallylowerboundedbyL¯ (θ).
em
Step 2: Consider [∇ (WL) kL¯ em(θ)] s. For (g 2) s, first take (q ij,L−1) s = 1 for all (i,j) ≠ (i∗,j∗).
Thenweonlyneedtoconsider
∇′ 2,s= n1 ∑ w(t j)(t j−t j−1)(W L+1)k⊺ (σ¯ tjW L+1q ij,L+ξ ij)1 (WLqij,L−1) k>0
(i,j)≠(i∗,j∗)
whichisindependentofs. For∇ 1,sinceq i∗,j∗,L−1 ≥0whichdoesnotaffectthesignofthisterm,
wecanalsofirsttake(q i∗,j∗,L−1) s=1foralls.
Step3: Wefocuson∇ and∇ andwewouldliketopickthenon-zeroelementsinthistwoterms.
11 21
Moreprecisely,let
N ={s∣(q ) >0,s=1,⋯,m},
1 i∗j∗,L s
⎧ ⎫
⎪⎪ ⎪⎪
N 2=⎨ ⎪⎪s∣ ∑ w(t j)(t j−t j−1)σ¯ tj1 (WLqij,L−1) k>0(q ij,L) s>0,s=1,⋯,m⎬ ⎪⎪
⎩ (i,j)≠(i∗,j∗) ⎭
Letα ij =w(t j)(t j−t j−1)σ¯ tj1 (WLqij,L−1) k>0≥0. Then
∑ w(t j)(t j−t j−1)σ¯ tj1 (WLqij,L−1) k>0(q ij,L) s
(i,j)≠(i∗,j∗)
= ∑ α ij(q ij,L) s= ∑ α ijσ(W Lq ij,L−1) s.
(i,j)≠(i∗,j∗) (i,j)≠(i∗,j∗)
If
∑ α ij(W Lq ij,L−1) s=(W L)
s
∑ α ijq ij,L−1>0,
(i,j)≠(i∗,j∗) (i,j)≠(i∗,j∗)
thentheremustbeatleastonepairof(i,j)s.t. α ij(W Lq ij,L−1)
s
=α ijσ(W Lq ij,L−1)
s
>0,which
implies∑ α (q ) >0. Therefore,itsufficestoconsider
(i,j)≠(i∗,j∗) ij ij,L s
N 1={s∣(q i∗j∗,L) s=(W L) sq i∗j∗,L−1>0,s=1,⋯,m},
⎧ ⎫
⎪⎪ ⎪⎪
N 2′ =⎨ ⎪⎪s∣(W L)
s
∑ α ijq ij,L−1>0⎬ ⎪⎪.
⎩ (i,j)≠(i∗,j∗) ⎭
Since(q ij,L−1) s≥0,wehave
⟨q i∗j∗,L−1, ∑ α ijq ij,L−1⟩≥0,
(i,j)≠(i∗,j∗)
⎛ ⎞ π
i.e.,∠ q i∗j∗,L−1, ∑ α ijq ij,L−1 ≤
⎝ ⎠ 2
(i,j)≠(i∗,j∗)
17ByLemma2andProposition2,wehave
⎛ ⎞
P (W L) sq i∗j∗,L−1>0,(W L)
s
∑ α ijq ij,L−1>0
⎝ ⎠
(i,j)≠(i∗,j∗)
⎛ (W ) (W ) ⎞
=P ⎝∥(WL )s ∥q i∗j∗,L−1>0, ∥(WL )s
∥
∑ α ijq ij,L−1>0
⎠
L s L s (i,j)≠(i∗,j∗)
1
≥ .
4
Also(W ) ’sarei.i.d.multivariateGaussian. ByChernoffbound,
L s
m m
P(∣N ∩N′∣∈(δ ,δ ))≤1−2e−Ω(m)
1 2 1 4 2 4
forsomesmallδ ≤ 1 andδ ≤4,i.e.,∣N ∩N′∣=Θ(m)withprobabilityatleast1−2e−Ω(m).
1 4 2 1 2
Step 4: Next we condition on N 1 ∩ N 2′ and consider (W L+1)k⊺ W L+1q i∗j∗,L and
⊺
(W L+1)k W L+1∑ (i,j)≠(i∗,j∗)α ijq ij,L.
Fix (W L+1)k, we would like to consider ∑ s≠k(W L+1)k⊺ (W L+1)su
s
for some u = (u 1,⋯,u m)⊺
withu ≥0. BygeneralHoeffding’sinequality,
i
⊺ ⊺
P(∣∑(W L+1)k (W L+1)su s∣≥u k(W L+1)k (W L+1)k)
s≠k
≤2exp⎛
−
cu2 k∥(W L+1)k∥4 ⎞
⎝ max i,j∥(W L+1) ij∥2 ψ2∥u∥2∥(W L+1)k∥2⎠
=2exp(−Ω(du2 k∥(W L+1)k∥2
)),
∥u∥2
wheretheOrlicznormofGaussian∥(W L+1) ij∥2
ψ2
=O(1/d).
Defineevent
⎧ ⎫
A
=⎪⎪ ⎨d(q i∗j∗,L)2 k∥(W L+1)k∥2 ≥Ω(logd),d(∑ (i,j)≠(i∗,j∗)α ijq ij,L)2 k∥(W L+1)k∥2 ≥Ω(logd)⎪⎪
⎬.
k ⎪⎪ ⎩ ∥q i∗j∗,L∥2 ∥∑ (i,j)≠(i∗,j∗)α ijq ij,L∥2 ⎪⎪ ⎭
√
Step5: NextwewouldliketoshowthateventA holdswithprobabilityatleastΩ( logd)forsome
k d1/2+4c
c∈(1/10,1/8).
Since(q ij,L)
k
=σ((W L) kq ij,L−1)and(∑ (i,j)≠(i∗,j∗)α ijq ij,L)
k
≥(W L) k∑ (i,j)≠(i∗,j∗)α ijq ij,L−1,
wefurtherconsiderthefollowingevents
√ √
A1 ={ (W L) k ⋅ q ij,L−1 ≥ c √logd , (W L) k ⋅ ∑ (i,j)≠(i∗,j∗)α ijq ij,L−1 ≥ c √logd }
k ∥(W L) k∥ ∥q ij,L−1∥ d ∥(W L) k∥ ∥∑ (i,j)≠(i∗,j∗)α ijq ij,L−1∥ d
1
A2 ={∥(W ) ∥2≥ }
k L k 2
1
A3
k
={∥(W L+1) k∥2≥ 2}
A4
={∥q ij,L−1∥2
=O(1)}
k ∥q ∥2
ij,L
A5
={∥∑ (i,j)≠(i∗,j∗)α ijq ij,L−1∥2
=O(1)}
k ∥∑ α q ∥2
(i,j)≠(i∗,j∗) ij ij,L
ForA1,byProposition2,
k
intersectionoftwohypersphericalcaps
P(A1)= .
k theareaofthesphere
18ByLemma3,
√
logd
P(A1)≥Ω( ).
k d1/2+4c
ForA2,since(W ) ∼N(0,2/m)and(W ) ∈Rm,byBernsteininequality,withprobabilityat
k L ij L k
least1−2e−Ω(m),wehave∥(W ) ∥2≥ 1,i.e.,
L k 2
P(A2)≥1−2e−Ω(m)
.
k
Similarly,withprobabilityatleast1−2e−Ω(d),wehave
P(A2)≥1−2e−Ω(d)
.
k
ForA5,
k
∑ α ijq ij,L= ∑ α ijσ(W Lq ij,L−1)
(i,j)≠(i∗,j∗) (i,j)≠(i∗,j∗)
= ∑ α ijD ij,L−1W Lq ij,L−1.
(i,j)≠(i∗,j∗)
Fix D ij,L−1 and consider the kth element. Then (∑ (i,j)≠(i∗,j∗)α ijq ij,L) k =
(W L) k∑ (i,j)≠(i∗,j∗)α ij(D ij,L−1) kkq ij,L−1 ∼ N(0, m2∥∑ (i,j)≠(i∗,j∗)α ij(D ij,L−1) kkq ij,L−1∥2).
ByBernstein’sinequality,withprobabilityatleast1−2e−Ω(m),wehave
∑
k
((W L)
k
(i,j)≠∑ (i∗,j∗)α ij(D ij,L−1) kkq ij,L−1)2≤C˜m
kaxXXXXXXXXXXXX(i,j)≠∑
(i∗,j∗)α ij(D ij,L−1) kkq
ij,L−1XXXXXXXXXXXX2
forsomelargeuniversalconstantC˜ >0. Therefore,byunionbound,wehave
XXXXXXXXXXXX(i,j)≠∑
(i∗,j∗)α ijq
ij,LXXXXXXXXXXXX2
≤C˜
1XXXXXXXXXXXX(i,j)≠∑
(i∗,j∗)α ijq
ij,L−1XXXXXXXXXXXX2
withprobabilityatleast1−Θ(nN)e−Ω(m),whereC˜ >0isauniversalconstant.
1
Similarly,bytakingoneofα =1andotherα =0forl≠i,s≠j,weobtainthesameresultforA4.
ij ls k
Intheend,combiningalltheresultsabove,wehave,withprobabilityatleast1−Θ(nN)e−Ω(m)
√
logd
P(A )≥P(A1)P(A2)P(A3)≥Ω( )
k k k k d1/2+4c
⊺
Step 6: Next we would like to deal with the sign of (W L+1)k ξ i∗j∗ and
(W L+1)k⊺ ∑ (i,j)≠(i∗,j∗)α ijξ ij. More precisely, let ξ
1
= ∥ξ ξi i∗ ∗j j∗ ∗∥, ξ
2
= ∥∑ ∑( (i i, ,j j) )≠ ≠( (i i∗ ∗, ,j j∗ ∗) )α αi ij jξ ξi ij j∥.
Then ξ ,ξ independently follow from the Uniform distribution on the unit hypersphere by
1 2
Proposition2.
Firstconsidertheanglebetweenξ andξ . Foranyfixedξ ,theprobabilityof∠(ξ ,ξ )≥π−θis
1 2 1 1 2
onahypersphericalcap. ByLeeandKim[35],theareaofthehyperspericalcapis
1 d−1 1
Aθ(1)= A (1)I ( , ).
d 2 d sin2θ 2 2
Then
Aθ(1) 1 d−1 1 1 θd−1
P(∠(ξ ,ξ )≥π−θ)= d = I ( , )∝ √ .
1 2 A d(1) 2 sin2θ 2 2 2√ π d−1
2
Letθ= π <1.Thenwithprobabilityatleast1−exp(−Ω(d)),wehave∠(ξ ,ξ )≤ 3π.ByLemma2,
4 1 2 4
P((W
L+1)k⊺
ξ 1≥0,(W
L+1)k⊺
ξ 2≥0)=
π−∠(ξ 1,ξ 2)
≥
1
.
2π 8
19⊺ ⊺
DenotetheeventB k ={(W L+1)k ξ i∗j∗ ≥0, (W L+1)k ∑ (i,j)≠(i∗,j∗)α ijξ ij ≥0}givenξ ij.
Step7: Combiningtheabove6steps,wewouldliketoobtainthelowerboundofthegradient.
For each k, consider (q ij,L−1) s for (i,j) ≠ (i∗,j∗) and denote q s = {(q ij,L−1) s} (i,j)≠(i∗,j∗) =
{σ((W L−1) sq ij,L−2)} (i,j)≠(i∗,j∗). Let q¯ s = {(W L−1) sq ij,L−2} (i,j)≠(i∗,j∗) and q¯ s ∼ N(0,QQ⊺),
whereeachrowofQisq⊺ for(i,j)≠(i∗,j∗). Thus,q isq¯ projectedtothepositiveorthant.
ij,L−2 s s
Let1=(1,1,⋯,1)∈RnN−1. Therefore,if⟨β ,1⟩≥0forsomeβ ∈RnN−1,thereexistsauniversial
k k
constantc >0,s.t.
1
P(⟨β ,q ⟩≥0)≥c ,
k s 1
i.e.,P(∇ ≥0)≥c .
2 1
Wecanthentaketheminimumofc overalli,j andk,whichisstillauniversalconstantawayfrom
1
0.
Moreover,considerwithA ’sandB ’swheretheyareindependentwitheachotherandapplythe
k k √
Chernoffbounds,wehave,withprobabilityatleast1−exp(−Ω(m logd)
√
d1/2+4c
m logd
∣{A andB }∣≥C
k k 5 d1/2+4c
forsomeconstantC >0.
5
CombinealloftheaboveandapplytheClaim9.5inAllen-Zhuetal.[1],weobtain,withprobability
atleast1−exp(−Ω(logd)),
√
1 1 m logd
∥∇ WLL¯ em(θ(0))∥2
F
≥ n2C 6w(t j∗)2(t j∗−t j∗−1)2 d∥σ¯
t
j∗W L+1q i∗j∗,L+ξ i∗j∗∥2∥q i∗j∗,L−1∥2
d1/2+4c
√
md2b logd 1
≥C
6 d3/2+4c
w(t j∗)(t j∗−t j∗−1)σ¯
t
j∗n2NL¯ em(θ(0)),
whereC >0issomeuniversalconstant,andthesecondinequalityfollowsfromthedefinitionof
6
i∗,j∗.
B.1.1 Geometricideasusedintheproof
Proposition2. Considerw∼N(0,I),wherew∈Rn. Then∥w∥and w areindependentrandom
∥w∥
variablesand w ∼Unif(Sn−1).
∥w∥
Lemma 2. Let w ∼ Unif (Sn−1), where Sn−1 = {x ∈ Rn∣∥x∥ = 1}. Then for two fixed vector
v ,v ∈Rn,
1 2
π−∠(v ,v )
P(w⊺ v ≥0,w⊺ v ≥0)= 1 2 .
1 2
2π
Proof. Sincew∼Unif(Sn−1),weonlyneedtoconsidertheareaoftheevent. Itisobviousthatthe
set{w∈Sn−1∣w⊺v }isasemi-hypersphere. Therefore,weonlyneedtoconsidertheintersectionof
i
twosemi-hypersphere,i.e.,
areaof{w∈Sn−1∣w⊺v }∩ areaof{w∈Sn−1∣w⊺v }
P(w⊺ v ≥0,w⊺ v ≥0)= 1 2
1 2
areaofthehypersphere
π−∠(v ,v )
= 1 2 .
2π
Next we would like to show the probability when the lower bound of the two inner product are
non-zero. WefollowthenotationsanddefinitionsinLeeandKim[35].Considertheunithypersphere
inRm,Sm−1={x∈Rm∣∥x∥=1}. TheareaofSm−1is
2πm/2
A (1)= .
m Γ(m/2)
20DenoteC (1,v,arccos(x))tobetheareaofthehypersphericalcap{w∈Sm−1∣w⊺v≥x}forfixed
m
v∈Sm−1.
Lemma 3. Let w ∼ Unif (Sm−1), where Sm−1 = {x ∈ Rm∣∥x∥ = 1}. Then for two fixed vector
v ,v ∈Sm−1s.t. v⊺v ≥0,
1 2 1 2
√ √ √
clogd clogd logd
P(w⊺ v ≥ √ ,w⊺ v ≥ √ )≥Ω( ),
1
d
2
d
d1/2+4c
forsomec>0.
Proof. Sincewisuniformlydistributedonthesphere,wehave
√ √
clogd clogd
P(w⊺ v ≥ √ ,w⊺ v ≥ √ )
1 2
d d
√ √
areaofC (1,v ,arccos c √logd)∩areaofC (1,v ,arccos c √logd)
m 1 m 2
d d
= .
areaofthehypersphere
Since θ = arccos(v⊺v ) ≤ π and the colatitude angle of each hyperspherical cap θ =
v 1 1 2 i
arccos(w⊺v )< π fori=1,2,thisintersectionofthehypersphericalcapsfallsintoCase8in[35],
i 2
i.e.,
√ √
P(w⊺ v ≥
c √logd
,w⊺ v ≥
c √logd
)=
J mθv−θmin,θ1(1)+J mθmin,θ2(1)
,
1 d 2 d A m(1)
whereθ =arctan( 1 − 1 ). Intheaboveexpression,
min sinθv tanθv
J mθmin,θ2(1)= Γπ (mm 2 2− −1 1)∫ θmθ i2 nsin(ϕ)m−2I 1−(ta tn aθ nm ϕin)2(m 2−2 ,1 2)dϕ,
whereI (a,b)istheregularizedincompletebetafunction. Sincetheincompletebetafunction
y
ya a(1−b)y
B(y;a,b)∝ (1+ +O(y2))
a a+1
andthecompletebetafunction
B(a,b)∝Γ(b)a−b,
wehave
m−2 1 ym 2−2
I ( , )≥ √ .
y 2 2 π(m−2)1/2
2
Then
m−2 1
sin(ϕ)m−2I ( , )
1−(tanθmin)2
2 2
tanϕ
m−2
(sinϕ)2−(tanθ )2(cosϕ)2 2 1
≥sin(ϕ)m−2( min ) √
(sinϕ)2 π(m−2)1/2
2
m−2 1
∝(1−(1+tanθ )(π/2−ϕ)2) 2 √
min π(m−2)1/2
2
wherethelastrowisbyseriesexpansionnearϕ=π/2. Also,bytheasymptoticapproximationwhen
x→+∞,
Γ(x+α)∼Γ(x)xα.
21Therefore,
J mθmin,θ2(1)
∝
πm 2−1 ∫ θθ m2 in(1− √(1+tanθ min)(π/2−ϕ)2)m 2−2 dϕ⋅Γ(m/2)
A (1) π(m−2)1/22πm/2Γ(m−1)
m 2 2
=
∫ θθ m2 in(1−(1+tan √θ min)(π/2−ϕ √)2)m 2−2 dϕ⋅Γ(m 2−1)(m 2−1)1/2
π(m−2)1/22 πΓ(m−1)
2 2
=
1
∫
θ2
(1−(1+tanθ
min)(π/2−ϕ)2)m 2−2
dϕ
√
2π θmin
. Letθ = π − c √logd . Also,sinceθ ≤ π,wehaveθ ≤ π. Then
2 2 d v 2 min 4
√
J mθ Ami mn, (θ2 1( )1) ≥C 0 √∫
π
2−π 2− 2√ √cc√
l
dolo d gg dd (1−2(π/2−ϕ)2)m 2−2dϕ
m−2
clogd 8clogd 2
≥C √ (1− )
0
d d
√
clogd
=C √ e−4clogd
1
d
√
clogd
=C
1 d1/2+4c
where C > 0 and C > 0 are some universal constants; the second inequality follows from ϕ ≥
√ 0 1
π − 2 √clogd ;thethirdequalityfollowsfromm=Θ(d). Similarly, Jmθv−θmin,θ1(1) islowerbounded
2 d Am(1)
bythesameorderofd. Thisfinishestheproof.
B.2 Proofsrelatedtorandominitialization
ConsiderW
=W(0)
inthissection.
i i
Lemma 4. If ϵ ∈ (0,1), with probability at least 1 − O(nN)e−Ω(min(ϵ2d4b−1,ϵd2b)), ∥X ∥2 ∈
ij
[∥e−µtjx i∥2+σ¯ t2 jd−ϵσ¯ t2 jd2b,∥e−µtjx i∥2+σ¯ t2 jd+ϵσ¯ t2 jd2b]foralli = 1,⋯,nandj = 0,⋯,N −1.
Moreover,withprobabilityatleast1−O(L)e−Ω(mϵ2/L)overtherandomnessofW fors=0,⋯,L,
s
we have ∥q ∥ ∈ [∥X ∥(1−ϵ),∥X ∥(1+ϵ)] for fixed i,j. Therefore, with probability at least
ij,ℓ ij ij
1−O(nNL)e−Ω(min(mϵ2/L,ϵ2d4b−1,ϵd2b)),wehaveΩ(db)=∥q ∥=O(da).
ij,ℓ
Proof. Consider 1 X =
e−µtjx
+ξ .Sinceξ ∼N(0,I),∥ 1 X ∥2followsfromthenoncentral
σ¯tj ij σ¯tj i ij ij σ¯tj ij
χ2distributionandE∥ 1 X
∥2=d+∥e−µtjx
∥2. ByBersteininequality,
σ¯tj ij σ¯tj i
1 2 1 2 t2
P(∣∥ X ∥ −E∥ X ∥ ∣≥t)≤2exp(−cmin( ,t))
ij ij
σ¯ σ¯ d
tj tj
t2
i.e.,P(∣∥e−µtjx i+σ¯ tjξ ij∥2−(σ¯ t2 jd+∥e−µtjx i∥2)∣≥σ¯ t2 jt)≤2exp(−cmin( d,t))
Therefore,withprobabilityatleast1−O(nN)e−Ω(min(ϵ2d4b−1,ϵd2b)),∥X ij∥2∈[∥e−µtjx i∥2+σ¯ t2 jd−
ϵσ¯ t2 jd2b,∥e−µtjx i∥2+σ¯ t2 jd+ϵσ¯ t2 jd2b]foralli=1,⋯,nandj =0,⋯,N −1,whereϵ∈(0,1). The
secondpartoftheLemmafollowsthesimilarproofinLemma7.1ofAllen-Zhuetal.[1]. Thelast
partfollowsfromunionboundandAssumption1.
Lemma5(Upperbound). UndertherandominitializationofW fori=0,⋯,L,withprobabilityat
i
least1−O(nNL)e−Ω(min(mϵ2/L,ϵ2d4b−1,ϵd2b)),wehave
∥∇ WℓL¯ em(θ(0))∥2=O(md2a−1Nm jaxw(t j)(t j−t j−1)σ¯ tj)L¯ em(θ(0)).
22Proof. Foranyℓ=1,⋅,L,wehave
∥∇ L¯ (θ)∥2
Wℓ em F
m
= ∑∥∇
(Wℓ)
kL¯ em(θ)∥2
k=1
m 1 n N
= ∑∥ ∑∑w(t j)(t j−t j−1)
n
k=1 i=1j=1
2
×[(W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺(σ¯ tjW L+1q ij,L+ξ ij)] kq ij,ℓ−11 (Wℓqij,ℓ−1) k>0∥
N n N m
≤
n
∑∑w(t j)2(t j−t j−1)2∑∥(W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺ k(σ¯ tjW L+1q ij,L+ξ ij)∥2⋅∥q ij,ℓ−1∥2
i=1j=1 k=1
mN n N
≤C 7d2a
d n
∑∑w(t j)2(t j−t j−1)2⋅∥σ¯ tjW L+1q ij,L+ξ ij∥2
i=1j=1
mN
≤C 7d2a
d
m jaxw(t j)(t j−t j−1)σ¯ tjL¯ em(θ)
where the first inequality follows from Young’s inequality; the second inequality follows from
Lemma4andLemma7.4inAllen-Zhuetal.[1];C >0
7
B.3 Proofsrelatedtoperturbation
ConsiderWper=W(0) +W′fori=1,⋯,Linthissection. WefollowthesameideainAllen-Zhu
i i i
etal.[1]toconsiderthenetworkvalueofperturbedweightsateachlayer. Weusethesuperscript
“per”todenotestheperturbedversion,i.e.,
rper =W X ,qper =σ(rper),
ij,0 0 ij ij,0 ij,0
rper =Wperqper ,qper =σ(rper), forℓ=1,⋯,L
ij,ℓ ℓ ij,ℓ−1 ij,ℓ ij,ℓ
S(θper;t j,X ij)=W L+1q ip je ,Lr
WealsosimilarlydefinethediagonalmatrixDper fortheabovenetwork.
ij,ℓ
ThefollowingLemmameasurestheperturbationofeachlayer. ThelemmadiffersfromLemma8.2
inAllen-Zhuetal.[1]byascaleofda. Forsakeofcompleteness,westateitinthefollowingandthe
proofcanbesimilarlyobtained.
Lemma 6. Let ω ≤ 1 for some large C > 1. With probability at least 1 −
C7L9/2log3mda
exp(−Ω(damω2/3L)),forany∆W s.t. ∥∆W∥≤ω,wehave
1. rper−r canbedecomposedtotwopartrper−r =r′ +r′ ,where∥r′ ∥=
ij,ℓ ij,ℓ √ ij,ℓ ij,ℓ ij,ℓ,1 ij,ℓ,2 ij,ℓ,1
O(ωL3/2da)and∥r i′ j,ℓ,2∥ ∞=O(ωL5/2 logmdam−1/2).
2. ∥Dper−D ∥ =O(mω2/3L)and∥(Dper−D )rper∥=O(ωL3/2da).
ij,ℓ ij,ℓ 0 ij,ℓ ij,ℓ ij,ℓ
√
3. ∥rper−r ∥and∥qper−q ∥areO(ωL5/2 logmda).
ij,ℓ ij,ℓ ij,ℓ ij,ℓ
B.4 Proofsrelatedtotheevolutionofthealgorithm
Lemma7(Upperandlowerboundsofgradientafterperturbation). Let
√
ω=O(
L¯∗
em
logd
⋅
min jw(t j)(t j−t j−1)σ¯
tj ).
L9(logm)2n2N2d2a−2b+1/2+4c max{max jw(t j)(t j−t j−1)σ¯ tj,∑ j(w(t j)(t j−t j−1)σ¯ tj)2}
Assumea−b+4c< 1. Considerθpers.t. ∥θper−θ∥≤ω. Thenwithprobabilityatleast1−e−Ω(logd),
2
∥∇ WℓL¯ em(θper)∥2=O(md2a−1Nm jaxw(t j)(t j−t j−1)σ¯ tj)L¯ em(θper),
√
m logdd2b−1.5−4c
∥∇ WLL¯ em(θper)∥2=Ω(
n2N
w(t j∗)(t j∗−t j∗−1)σ¯
t
j∗)min{L¯ em(θ),L¯ em(θper)},
23forℓ=1,⋯,L.
Proof. Considerthefollowingterms
1 n N
∇
(Wℓ)
kL¯ em(θ)= n∑∑w(t j)(t j−t j−1)
i=1j=1
×[(W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺(σ¯ tjW L+1q ij,L+ξ ij)] kq ij,ℓ−11 (Wℓqij,ℓ−1) k>0 (12)
1 n N
∇p (Wer
ℓ)
kL¯ em(θ)=
n
i∑ =1j∑ =1w(t j)(t j−t j−1)
×[(W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺(σ¯ tjW L+1q ip je ,Lr +ξ ij)] kq ij,ℓ−11 (Wℓqij,ℓ−1) k>0, (13)
1 n N
∇
(Wℓ)
kL¯ em(θper)= n∑∑w(t j)(t j−t j−1)
i=1j=1
×[(W L+1D ip je ,r LW Lper⋯D ip je ,r ℓW ℓp +e 1r)⊺(σ¯ tjW L+1q ip je ,Lr +ξ ij)] kq ip je ,ℓr −11 (W ℓperq ip je ,r ℓ−1) k>0 (14)
Then
∥∇ L¯ (θ)−∇perL¯ (θ)∥2
Wℓ em Wℓ em F
m
= k∑ =1∥∇ (Wℓ) kL¯ em(θ)−∇p (Wer ℓ) kL¯ em(θ)∥2
N n N m
≤
n
∑∑w(t j)2(t j−t j−1)2∑∥(W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺ k(σ¯ tjW L+1(q ij,L−q ip je ,Lr))∥2⋅∥q ij,ℓ−1∥2
i=1j=1 k=1
mN n N
≤C 8d2a
d n
∑∑w(t j)2(t j−t j−1)2⋅∥σ¯ tjW L+1(q ij,L−q ip je ,Lr)∥2
i=1j=1
m N √
≤C 8′ d2a dN ∑w(t j)2(t j−t j−1)2(ωL5/2 logmda)2
j=1
√
m logdd2b−1.5−4c
≤C˜ 8(
n2N
w(t j∗)(t j∗−t j∗−1)σ¯
t
j∗)L¯∗
em
wherethefirsttwoinequalitiesfollowthesameastheproofofLemma5;thethirdinequalityfollows
fromLemma6;thelastinequalityfollowsfromthedefinitionofω.
Also,wehave
∥∇p (Wer
ℓ)
kL¯ em(θ)−∇
(Wℓ)
kL¯ em(θper)∥
1 n N
≤∥ ∑∑w(t j)(t j−t j−1)
n
i=1j=1
×[(W L+1D ip je ,r LW Lper⋯D ip je ,r ℓW ℓp +e 1r−W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺(σ¯ tjW L+1q ip je ,Lr +ξ ij)]
k
× q ip je ,ℓr −11 (W ℓperq ip je ,r ℓ−1) k>0∥
1 n N
+∥ n∑∑w(t j)(t j−t j−1)[(W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺(σ¯ tjW L+1q ip je ,Lr +ξ ij)]
k
i=1j=1
×(q ij,ℓ−11
(Wℓqij,ℓ−1)
k>0−q ip je ,ℓr −11
(W ℓperq ip je ,r ℓ−1)
k>0)∥
24Then
∥∇perL¯ (θ)−∇ L¯ (θper)∥2
Wℓ em Wℓ em F
N n N
≤2
n
∑∑w(t j)2(t j−t j−1)2⋅∥q ip je ,ℓr −1∥2
i=1j=1
m
×∑∥(W L+1D ip je ,r LW Lper⋯D ip je ,r ℓW ℓp +e 1r−W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺ k(σ¯ tjW L+1q ip je ,Lr +ξ ij)∥2
k=1
N n N
+2
n
i∑ =1j∑ =1w(t j)2(t j−t j−1)2⋅∥q ij,ℓ−11 (Wℓqij,ℓ−1) k>0−q ip je ,ℓr −11 (W ℓperq ip je ,r ℓ−1) k>0∥2
m
×∑∥(W L+1D ij,LW L⋯D ij,ℓW ℓ+1)⊺ k(σ¯ tjW L+1q ip je ,Lr +ξ ij)∥2
k=1
mN n N
≤C 9(ω2L5logmd2a)(L3/2logmm1/2)
d n
∑∑w(t j)2(t j−t j−1)2⋅∥σ¯ tjW L+1q ip je ,Lr +ξ ij∥2
i=1j=1
mN n N
+C 9′(ω2L5logmd2a)
d n
∑∑w(t j)2(t j−t j−1)2⋅∥σ¯ tjW L+1q ip je ,Lr +ξ ij∥2
i=1j=1
√
m logdd2b−1.5−4c
≤C˜ 9(
n2N
w(t j∗)(t j∗−t j∗−1)σ¯
t
j∗)L¯ em(θper)
wherethefirstinequalityfollowsfromYoung’sinequalityandtheabovedecomposition;thesecond
inequalityfollowsfromLemma7.4,8.7inAllen-Zhuetal.[1](withs=O(damω2/3L)=O(logm);
notethesetwolemmascanbeextendedtos=O(logm)byutilizingthepropertyofGaussianand
generalHoeffding’sinequality)andLemma6;thelastinequalityfollowsfromthedefinitionofω.
For upper bound, we only need to consider ∇p (Wer ℓ) kL¯ em(θ) and ∇ (Wℓ) kL¯ em(θper). By similar
argumentasLemma5,withprobabilityatleast1−O(nNL)e−Ω(min(mϵ2/L,ϵ2d4b−1,ϵd2b)),wehave
∥∇p (Wer
ℓ)
kL¯ em(θ)∥2=O(md2a−1Nm jaxw(t j)(t j−t j−1)σ¯ tj)L¯ em(θper).
Then
∥∇ L¯ (θper)∥2
Wℓ em
≤2∥∇perL¯ (θ)∥2 +2∥∇perL¯ (θ)−∇ L¯ (θper)∥2
Wℓ em F Wℓ em Wℓ em F
=O(md2a−1Nm jaxw(t j)(t j−t j−1)σ¯ tj)L¯ em(θper).
Also,
∥∇ L¯ (θper)∥2
W em
≥∥∇ L¯ (θper)∥2
WL em
1
≥ ∥∇ L¯ (θ)∥2−∥∇ L¯ (θ)−∇perL¯ (θ)∥2 −∥∇perL¯ (θ)−∇ L¯ (θper)∥2
3 WL em Wℓ em Wℓ em F Wℓ em Wℓ em F
√
m logdd2b−1.5−4c
=Ω(
n2N
w(t j∗)(t j∗−t j∗−1)σ¯
t
j∗)min{L¯ em(θ),L¯ em(θper)}.
Notewheninterpolationisnotachievable,thislowerboundisalwaysawayfrom0,whichmeansthe
currenttechniquecanonlyevaluatethelowerboundoutsideaneighbourhoodoftheminimizer. More
advancedmethodisneededandweleaveitforfutureinvestigation.
25Lemma8(semi-smoothness). Letω=Ω.Withprobabilityatleast1−e−Ω(logm)overtherandomness
ofθ(0),wehaveforallθs.t. ∥θ−θ(0)∥≤ω,andallθpers.t. ∥θper−θ∥≤ω,
L¯ (θper)≤L¯ (θ)+⟨∇L¯ (θ),θper−θ⟩
em em em
√ √ √
+ L¯ em(θ) ∑w(t j)(t j−t j−1)σ¯ tjO(ω1/3L2 mlogmda/2−1/2)∥θper−θ∥
j
√ √
√
+ L¯ em(θ) ∑w(t j)(t j−t j−1)σ¯ tjO(L2 mda−1/2)∥θper−θ∥2
j
Proof. Bydefinition,
L¯ (θper)−L¯ (θ)−⟨∇L¯ (θ),θper−θ⟩
em em em
1 n N
= n∑∑w(t j)(t j−t j−1)(σ¯ tjW L+1q ij,L+ξ ij)⊺ W L+1
i=1j=1
L
×(q ip je ,Lr −q ij,L−∑D ij,LW ij,L⋯W ij,ℓ+1D ij,ℓ(W ip je ,ℓr−W ij,ℓ)q ij,ℓ)
ℓ=1
1
+
2σ¯
w(t j)(t j−t j−1)∥σ¯ tjW L+1(q ip je ,Lr −q ij,L)∥2.
tj
Similar to the proof of Theorem 4 in Allen-Zhu et al. [1], we obtain the desired bound by using
Cauchy-Schwartzinequalityandweomit∑ jw(t j)(t j−t j−1)σ¯
tj
intheboundduetoAssumption2.
Note, in our case, due to the order of input data, we choose s = O(damω2/3L) = O(logm) in
Allen-Zhu et al. [1] (see more discussions in the proof of Lemma 7) and therefore the bound is
slightlydifferentfromtheirs.
C Proofsforsampling
In this section, we prove Theorem 2. The proof includes two main steps: 1. decomposing
KL(p δ∣q T−δ)intotheinitializationerror,thescoreestimationerrorsandthediscretizationerrors;
2. estimatingtheinitializationerrorandthediscretizationerrorbasedonourassumptions. Inthe
followingcontext,weintroducetheproofofthesetwostepsseparately.
ProofofTheorem2. Step1: Theerrordecompositionfollowsfromtheideasin[13]ofstudying
VPSDE-baseddiffusionmodels. AccordingtothechainruleofKLdivergence,wehave
KL(p δ∣q T−δ)≤KL(p T∣q 0)+E y∼pT[KL(p δ∣T(⋅∣y)∣q T−δ∣0(⋅∣y))],
Applythechainruleagainforatacrossthetimeschedule(T −t← k) 0≤k≤N−1,thesecondtermcanbe
writtenas
E y∼pT[KL(p δ∣T(⋅∣y)∣q T−δ∣0(⋅∣y))]
N−1
≤ k∑ =0E yk∼p T−t← k[KL(p T−t← k+1∣T−t← k(⋅∣y k)∣q t← k+1∣t← k(⋅∣y k))]
≤
21N
k∑
=− 01
∫
t←
kt←
k+1 σ T2 −tE[∥s(θ;T −t← k,Y t← k)−∇logp T−t(Y t)∥2]dt
N−1 t←
≤ ∑ ∫ k+1 σ T2 −tE[∥s(θ;T −t← k,Y t←)−∇logp T−t←(Y t←)∥2]dt
k=0 t← k k k k
N−1 t←
+ ∑ ∫ k+1 σ T2 −tE[∥∇logp T−t←(Y t←)−∇logp T−t(Y t)∥2]dt,
k=0 t← k k k
26wherethesecondinequalityfollowsfromLemma9. Therefore,theerrordecompositionwritesas
N−1 t←
KL(p δ∣q T−δ)≲KL(p T∣q 0)+ ∑ ∫ k+1 σ T2 −tE[∥s(θ;T −t← k,Y t←)−∇logp T−t←(Y t←)∥2]dt
k=0 t← k k k k
N−1 t←
+ ∑ ∫ k+1 σ T2 −tE[∥∇logp T−t←(Y t←)−∇logp T−t(Y t)∥2]dt (15)
k=0 t← k k k
where the three terms in (15) quantify the initialization error, the score estimation error and the
discretizationerror,respectively.
Step2:Inthisstep,weestimatethethreeerrortermsinStep1. First,recallthatp =p∗N(0,σ¯2I )
T T d
andq =N(0,σ¯2I ),hencetheinitializationerrorKL(p ∣q )canbeestimatedasfollows,
0 T d T 0
KL(p ∣q )=KL(p∗N(0,σ¯2I )∣N(0,σ¯2I ))≲ m2 2, (16)
T 0 T d T d σ¯2
T
wheretheinequalityfollowsfromLemma10. HencewerecoverthetermE in(9).
I
Next,sinceσ isnon-decreasingint,thescoreestimationerrorcanbeestimatedas
t
N−1 t←
∑ ∫ k+1 σ T2 −tE[∥s(θ;T −t← k,Y t←)−∇logp T−t←(Y t←)∥2]dt
k=0 t← k k k k
N−1
≤ ∑ γ kσ T2 −t←E[∥s(θ;T −t← k,Y t←)−∇logp T−t←(Y t←)∥2]. (17)
k=0 k k k k
Hence,werecoverthetermE in(9).
S
Last,weestimatedthediscretizationerrorterm. OurapproachismotivatedbyanalysesofVPSDEs
in[8,26]. WedefinesaprocessL
t
∶= ∇logp T−t(Y t). Thenwecanrelatediscretizationerrorto
quantitiesdependingonL t,andthereforeboundthediscretizationerrorviapropertiesof{L t} 0≤t≤T.
AccordingtoLemma12,wehave
N−1 t←
∑ ∫ k+1 σ T2 −tE[∥∇logp T−t←(Y t←)−∇logp T−t(Y t)∥2]dt
k=0 t← k k k
N−1 t← t N−1 t←
≤2d ∑ ∫ k+1 ∫ σ T2 −tσ T2 −uσ¯ T−4 −ududt+ ∑ ∫ k+1 σ T2 −tdtσ¯ T−4 −t←E[tr(Σ T−t←(X T−t←))]
k=0 t← k t← k k=0 t← k k k k
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶ ·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
N1 N2
N−1 t←
− ∑ ∫ k+1 σ T2 −tσ¯ T−4 −tE[tr(Σ T−t(X T−t))]dt.
k=0 t← k
·„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„‚„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„„¶
N3
Sincet↦σ tisnon-decreasingandt↦E[tr(Σ T−t(X T−t))]isnon-increasing,wehave
N−1 T−t← T−u N−1 T−t←
N 1=2d ∑ ∫ k ∫ σ t2dtσ u2σ¯ u−4du≤2d ∑ γ k∫ k σ t4σ¯ t−4dt,
k=0 T−t← k+1 T−t← k+1 k=0 T−t← k+1
N−1 t←
N 2= ∑ ∫ k+1 σ T2 −tdtσ¯ T−4 −t←E[tr(Σ T−t←(X T−t←))],
k=0 t← k k k k
N−1 t←
N 3≤− k∑ =0∫
t←
kk+1 σ T2 −tσ¯ T−4 −tdtE[tr(Σ T−t← k+1(X T−t← k+1))].
27Therefore,weobtain
N−1 t←
∑ ∫ k+1 σ T2 −tE[∥∇logp T−t←(Y t←)−∇logp T−t(Y t)∥2]dt
k=0 t← k k k
N−1 T−t← N−1 t←
≤2d ∑ γ k∫ k σ t4σ¯ t−4dt+ ∑ ∫ k+1 σ T2 −tdtσ¯ T−4 −t←E[tr(Σ T−t←(X T−t←))]
k=0 T−t← k+1 k=0 t← k k k k
N−1 t←
− k∑ =0∫
t←
kk+1 σ T2 −tσ¯ T−4 −tdtE[tr(Σ T−t← k+1(X T−t← k+1))]
N−1 T−t← t←
=2d ∑ γ k∫ k σ t4σ¯ t−4dt+∫ 1 σ T2 −tdtσ¯ T−4E[tr(Σ T(X T))]
k=0 T−t← k+1 0
N−1 t← t←
+ ∑ (∫ k+1 σ T2 −tσ¯ T−4 −t←dt−∫ k σ T2 −tσ¯ T−4 −tdt)E[tr(Σ T−t←(X T−t←))]. (18)
k=1 t← k k t← k−1 k k
TheabovebounddependsonE[tr(Σ (X ))],henceweestimateE[tr(Σ (X ))]fordifferentvalues
t t t t
oft.
First,wehave
E[tr(Σ (X ))]=E[E[∥X ∥2∣X ]−∥E[X ∣X ]∥2]≤E[∥X ∥2]=m2.
t t 0 t 0 t 0 2
Meanwhile,
E[tr(Σ (X ))]=E[tr(Cov(X −X ∣X ))]=E[E[∥X −X ∥2∣X ]]−∥E[X −X ∣X ]∥2
t t 0 t t 0 t t 0 t t
≤E[∥X −X ∥2]=σ¯2d
0 t t
Therefore, E[tr(Σ t(X t))] ≤ min(m2 2,σ¯ t2d) ≲ (1−e−σ¯ t2)(m2 2+d). Plugthisestimationinto(18)
andweget
N−1 t←
∑ ∫ k+1 σ T2 −tE[∥∇logp T−t←(Y t←)−∇logp T−t(Y t)∥2]dt
k=0 t← k k k
N−1 T−t← t←
≲d ∑ γ k∫ k σ t4σ¯ t−4dt+∫ 1 σ T2 −tdtσ¯ T−4m2
2
k=0 T−t← k+1 0
+(m2 2+d)N ∑−1 (1−e−σ¯ T2 −t← k)(∫ t← k+1 σ T2 −tσ¯ T−4 −t←dt−∫ t← k σ T2 −tσ¯ T−4 −tdt)
k=1 t← k k t← k−1
≲dN k∑ =− 01 γ k∫ TT −t−
←
kt +← k 1σ t4σ¯ t−4dt+m2 2σ¯ T−2++(m2 2+d)N k∑ =− 11 (1−e−σ¯ T2 −t← k)σ¯ T4 −t← k
σ¯
T2− −σ¯
t←
kT2 −− 1t σ¯← k T4+1 −σ¯
t←
kT2 −t← k−1,
wherethelastinequalityfollowsfromthedefinitionofσ¯ andintegrationbyparts. Theproofof
t
Theorem2iscompleted.
Lemma 9. Let {Y t} 0≤t≤T be the solution to (2) with f t ≡ 0 and p← t+s∣s(⋅∣y) be the conditional
distributionofY s+tgiven{Y s=y}. Let{Y¯ t} 0≤t≤T bethesolutionto(9)q t+s∣s(⋅∣y)betheconditional
distributionofY¯ s+tgiven{Y¯ s=y}. Thenforanyfixedt∈(0,γ k],wehave
1
E y∼p← t← kKL(p← t← k+t∣t← k(⋅∣y)∣q t← k+t∣t← k(⋅∣y))≤ 2σ T2 −tE[∥s(θ;T −t← k,Y t← k)−∇logp T−t(Y t)∥2]
ProofofLemma9. Accordingto[13,Lemma6],wehave
KL(p← t←+t∣t←(⋅∣y)∣q t←+t∣t←(⋅∣y))
k k k k
p← (x∣y)
≤−2σ T2 −t∫ p← t← k+t∣t← k(x∣y)∥∇log
q
tt ←← k ++ tt ∣∣ tt ←← k (x∣y)∥2dx
k k
p← (x∣y)
+2σ T2 −tE p←
t← k+t∣t←
k(x∣y)[⟨∇logp T−t(x)−s(θ;T −t← k,Y t← k),∇log
q
tt ←← k ++ tt ∣∣ tt ←← k (x∣y)⟩]
k k
1
≤ 2σ T2 −tE p←
t← k+t∣t←
k(x∣y)[∥∇logp T−t(x)−s(θ;T −t← k,Y t← k)∥2],
28wherethelastinequalityfollowsfromYoung’sinequality. Therefore,Lemma9isprovedaftertaking
anotherexpectation.
Lemma 10. For any probability distribution p satisfying Assumption 3 and q being a centered
multivariatenormaldistributionwithcovariancematrixσ2I ,wehave
d
m2
KL(p∗q∣q)≤ 2 .
2σ2
ProofofLemma10.
KL(p∗q∣q)≤∫ KL(q(⋅−y)∣q(⋅))p(dy)=∫ KL(N(y,σ2I d)∣N(0,σ2I d))p(dy)
1 m2
= 2∫ ln(1)−d+tr(I d)+∥y∥2σ−2p(dy)= 2σ2 2,
wheretheinequalityfollowsfromconvexityofKL(⋅∣q)andthesecondidentityfollowsfromKL-
divergencebetweenmultivariatenormaldistributions.
Lemma 11. Let {X t} 0≤t≤T be the solution to (1) with f t ≡ 0 and p 0∣t(⋅∣x) be the conditional
distributionofX given{X =x}. Define
0 t
m t(X t)∶=E X∼p0∣t(⋅∣Xt)[X], Σ t(X t)=Cov X∼p0∣t(⋅∣Xt)(X). (19)
Let{Y t} 0≤t≤T bethesolutionto(2)withf t ≡0andq 0∣t(⋅∣x)betheconditionaldistributionofY 0
given{Y =x}. Define
t
m¯ t(Y t)∶=E X∼q 0∣t(⋅∣Yt)[X], Σ¯ t(Y t)=Cov X∼q 0∣t(⋅∣Yt)(X). (20)
Thenwehaveforallt∈(0,T),
√
dm¯ t(Y t)= 2σ T−tσ¯ T−2 −tΣ¯ t(Y t)dW˜ t,
d
and E[Σ (X )]=2σ2σ¯−4E[Σ (X )2].
dt t t t t t t
ProofofLemma11. We first represent ∇logp (X ) and ∇2logp (X ) via m (X ) and Σ (X ).
t t t t t t t t
Since{X t} 0≤t≤T solves(1),X t =X 0+σ¯ tξ with(X 0,ξ)∼p⊗N(0,I d). Therefore,accordingto
Bayesrule,wehave
1
∇logp t(X t)=
p (X
)∫ ∇logp t∣0(X t∣x)p 0,t(x,X t)dx
t t
=E x∼p0∣t(⋅∣Xt)[σ¯ t−2(X t−x)]
=−σ¯−2(X −m (X )), (21)
t t t t
wherethesecondidentityfollowsfromthefactthatp t∣0(⋅∣x)=N(x,σ¯ t2I d). Thelastidentityfollows
fromthedefinitionofm (X )inLemma11. Similarly,accordingtoBayesrule,wecancompute
t t
∇2logp (X )
t t
1
=
p (X
)∫ ∇2logp t∣0(X t∣x)p 0,t(x,X t)dx
t t
1 ⊺
+
p (X
)∫ (∇logp t∣0(X t∣x))(∇logp t∣0(X t∣x)) p 0,t(x,X t)dx
t t
1 ⊺
−
p (X
)2(∫ ∇logp t∣0(X t∣x)p 0,t(x,X t)dx)(∫ ∇logp t∣0(X t∣x)p 0,t(x,X t)dx)
t t
=−σ¯−2I +σ¯−4Σ (X ), (22)
t d t t t
where the second identity follows from the fact that p t∣0(⋅∣x) = N(x,σ¯ t2I d) and the definition of
Σ (X )inLemma11.
t t
29AccordingtoBayesrule,wehave
1∥X −x∥2
p 0∣t(dx∣X t)∝exp(−
2
t
σ¯2
)p(dx)
t
and
1∥Y −x∥2
q 0∣t(dx∣Y t)=Z−1exp(−
2
σ¯t
2
)p(dx)
T−t
1
=Z−1exp(− σ¯−2 ∥x∥2+σ¯−2 ⟨x,Y ⟩)p(dx)
t 2 T−t T−t t
∶=Z−1exp(h (x))p(dx), (23)
t t
whereZ t=∫ exp(h t(x))p(dx)isa(random)normalizationconstant. Fromtheabovecomputations,
wecanseethatq 0∣t(dx∣Y t)=p 0∣T−t(dx∣X T−t)forallt∈[0,T]. Therefore,wehave
m¯ t(Y t)=E X∼q0∣t(⋅∣Yt)[X]=m T−t(X T−t), Σ¯ t(Y t)=Cov X∼q0∣t(⋅∣Yt)(X)=Σ T−t(X T−t),
wheretheidentitiesholdindistribution. Therefore,toprovethefirststatement,itsufficestocompute
dm¯ (Y ). Todoso,wefirstcomputedh (x),d[h(x),h(x)] ,dZ anddlogZ .
t t t t t t
dh t(x)=σ¯ T−3 −tσ¯˙ T−t∥x∥2dt−2σ¯ T−3 −tσ¯˙ T−tσ¯⟨x,Y t⟩dt+σ¯ T−2 −t⟨x,dY t⟩. (24)
AccordingtothedefinitionofY and(21),wehave
t
√
dY t=2σ T2 −t∇logp T−t(Y t)dt+ 2σ T2 −tdW˜
t
√
=−2σ2 σ¯−2 (Y −m¯ (Y ))dt+ 2σ2 dW˜ .
T−t T−t t t t T−t t
Therefore
d[h(x),h(x)] =σ¯−4 ∣x∣2[dY,dY] =2σ2 σ¯−4 ∥x∥2. (25)
t T−t t T−t T−t
Apply(24)and(25)andweget
1
dZ t=∫ exp(h t(x))(dh t(x)+ d[h(x),h(x)] t)p(dx)
2
=σ¯ T−3 −tσ¯˙ T−tE q0∣t(⋅∣Yt)[∥x∥2]Z tdt−2σ¯ T−3 −tσ¯˙ T−t⟨Y t,m¯ t(Y t)⟩Z tdt
+σ¯ T−2 −t⟨m¯ t(Y t),dY t⟩Z t+σ T2 −tσ¯ T−4 −tE
q
0∣t(⋅∣Yt)[∥x∥2]Z tdt, (26)
and
1
dlogZ =Z−1dZ − Z−2d[Z,Z]
t t t 2 t t
=−2σ¯ T−3 −tσ¯˙ T−t⟨Y t,m¯ t(Y t)⟩dt+σ¯ T−2 −t⟨m¯ t(Y t),dY t⟩−σ T2 −tσ¯ T−4 −t∥m¯ t(Y t)∥2dt. (27)
IfwefurtherdefineR (Y )∶= q0∣t(dx∣Yt) =Z−1exp(h (x)). Wehave
t t p(dx) t t
1
dR (Y )=dexp(logR (Y ))=R (Y )d(logR (Y ))+ R (Y )d[logR (Y ),logR (Y )]
t t t t t t t t t t t t t t
2
1
=−R (Y )d(logZ )+R (Y )dh (x)+ R (Y )d[h (x)−logZ ,h (x)−logZ ] (28)
t t t t t t t t t t t t
2
With(24),(25),(26),(27)and(28),wehave
dm¯ t(Y t)=d∫ xR t(Y t)p(dx)
1
=∫ x(−d(logZ t)+dh t(x)+ 2d[h t(x)−logZ t,h t(x)−logZ t])q 0∣t(dx∣Y t)
√
= 2σ T−tσ¯ T−2 −tΣ¯ t(Y t)dW˜ t, (29)
30wheremosttermscancelinthelastidentity. Therefore,thefirststatementisproved. Next,weprove
thesecondstatement. Wehave
d d d
E[Σ T−t(X T−t)]= E[Σ¯ t(Y t)]= E[Σ T−t(X T−t)]
dt dt dt
d
= dtE Yt∼pT−t[E q0∣t(⋅∣Yt)[x⊗ 2]−m¯ t(Y t)⊗ 2]
d d
= E [x⊗ 2]− E[m¯ (Y )⊗ 2]
dt
q0
dt
t t
=−E[−2m¯ (Y )dm¯ (Y )⊺+d[m¯ (Y ),m¯ (Y )⊺]]
t t t t t t t t
=2σ¯ T−3 −tσ¯˙ T−tE[Σ¯ t(Y t)2]dt
=−2σ T2 −tσ¯ T−4 −tE[Σ t(X T−t)2],
wherethesecondlastidentityfollowsfrom(29)andthelastidentityfollowsfromthedefinitionofσ¯ .
t
Last,wereversethetimeandget
d
E[Σ (X )]=2σ2σ¯−4E[Σ (X )2].
dt t t t t t t
Theproofiscompleted.
Lemma12. UndertheconditionsinLemma11, let{Y t} 0≤t≤T bethesolutionto(2)withf t ≡ 0.
DefineL t∶=∇logp T−t(Y t),thenforanyt∈[t← k,t← k+1),wehave
t
E[∥L t−L t←∥2]=2d∫ σ T2 −uσ¯ T−4 −udu+σ¯ T−4 −t←E[tr(Σ T−t←(X T−t←))]−σ¯ T−4 −tE[tr(Σ T−t(X T−t))]
k t← k k k
k
ProofofLemma12. First,accordingtothedefinitionofL andY ,itfollowsfromItô’slemmathat
t t
√
dL t=∇2logp T−t(Y t)(2σ T2 −t∇logp T−t(Y t)dt+ 2σ T−tdW˜ t) (30)
+∆(∇logp T−t(Y t))σ T2 −tdt+
d(∇lo dg tp T−t)
(Y t)dt (31)
√
= 2σ T2 −t∇2logp T−t(Y t)dW˜ t, (32)
where the last step follows from applying the Fokker Planck equation of (1) with f ≡ 0, i.e.,
t
∂ p =σ2∆p . MostofthetermsarecancelledafterapplyingtheFokkerPlanckequation. Now,for
t t t t
fixeds>0andt>s,defineE ∶=E[∥L −L ∥2]. ApplyItô’slemmaand(30),wehave
s,t t s
dE =2E[⟨L −L ,dL ⟩]+d[L]
s,t t s t t
√
=2E[⟨L t−L s, 2σ T2 −t∇logp T−t(Y t)dW˜ t⟩]+2σ T2 −tE[∥∇2logp T−t(Y t)∥2 F]dt
=2σ T2 −tE[∥∇2logp T−t(Y t)∥2 F]dt, (33)
where∥A∥ denotestheFrobeniusnormofanymatrixA. Accordingto(22),wehave
F
dE
dts,t =2σ T2 −tE[∥∇2logp T−t(Y t)∥2 F]=2σ T2 −tE[∥∇2logp T−t(X T−t)∥2 F]
=2σ T2 −tE[∥−σ¯ T−2 −tI d+σ¯ T−4 −tΣ T−t(X T−t)∥2 F]
=2dσ T2 −tσ¯ T−4 −t−4σ T2 −tσ¯ T−6 −tE[tr(Σ T−t(X T−t))]+2σ T2 −tσ¯ T−8 −tE[tr(Σ T−t(X T−t)2)]
d
=2dσ T2 −tσ¯ T−4 −t−4σ T2 −tσ¯ T−6 −tE[tr(Σ T−t(X T−t))]−σ¯ T−4 −tdtE[tr(Σ T−t(X T−t))],
31wherethelastidentityfollowsfromtheproofofLemma11. Therefore,foranyt∈[t←,t← ),we
k k+1
have
t t
E t←,t=2d∫ σ T2 −uσ¯ T−4 −udu−4∫ σ T2 −uσ¯ T−6 −uE[tr(Σ T−u(X T−u))]du
k t← t←
k k
t d
−∫ t←σ¯ T−4 −uduE[tr(Σ T−u(X T−u))]
k
t t
=2d∫ σ T2 −uσ¯ T−4 −udu−4∫ σ T2 −uσ¯ T−6 −uE[tr(Σ T−u(X T−u))]du
t← t←
k k
−σ¯ T−4 −tE[tr(Σ T−t(X T−t))]+σ¯ T−4 −t←E[tr(Σ T−t←(X T−t←))]
k k k
t
+4∫ σ T2 −uσ¯ T−6 −uE[tr(Σ T−u(X T−u))]
t←
k
t
=2d∫ σ T2 −uσ¯ T−4 −udu−σ¯ T−4 −tE[tr(Σ T−t(X T−t))]+σ¯ T−4 −t←E[tr(Σ T−t←(X T−t←))]
t← k k k
k
Theproofiscompleted.
D Fullerroranalysis
ProofofCorollary1. WeonlyneedtodealwithE . Byapplyingthesameschedulestotraining
S
objective,weobtain
N−1 σ2 1
E
S
= k∑
=0
w(ttN N− −k
k)
⋅w(t N−k)(t N−k−t N−k−1)
σ¯
tN−kE X0E ξ∥σ¯ tN−ks(θ;t N−k,X tN−k)+ξ∥2
N−1 σ2
+ k∑
=0
w(ttN N− −k
k)
⋅w(t N−k)(t N−k−t N−k−1)⋅C
σ2
≤max tN−k ⋅(L¯(W)+C¯).
k w(t N−k)
Togetherwith
L(W(k))=L¯(W(k))+C¯ ≤∣L¯(W(k))−L¯ (W(k))∣+∣L¯ (W(k))−L¯∗ ∣+∣L¯∗ +C¯∣,
em em em em
wehavetheresult.
E ProofsforSection4.1
E.1 Proofof“bell-shaped”curve
ProofofProposition1. For1and2,theproofissimplytheϵ−δ (ϵ−N)versionofdefinitionfor
limit. For2,thecontinuityandpositivehomogeneityofReLUfunctionisalsoneeded.
For3,considerthedatasetwithinputdataX
in={e−µtjx
i+σ¯ tjξ ij} ij,andtheoutputdataX out=
{−ξij} fori=1,⋯,nandj =N ,⋯,N ,whereN >0andN <T. TheN andN ischosen
σ¯tj ij 1 2 1 2 1 2
sothatC ∥ξ ∥ ≤ ∥ξij∥ ≤ C ∥ξ ∥,where 1 = O(1),C = O(1). Byimplicitfunctiontheorem,
3 ij σ¯tj 4 ij C3 4
thereexistsacontinuouslydifferentiablefunctionG,s.t. G(e−µtjx i+σ¯ tjξ ij)=− σ¯ξi tj j. Thenbythe
universalapproximationtheoremwithm=Θ(d)[33],foranyC ϵ>0,thereexistsaneuralnetwork
3
F,s.t.
sup ∥F(x)−G(x)∥≤C ϵ
3
x∈Xin
ξ
∥F(e−µtjx i+σ¯ tjξ ij)+ σ¯ij∥≤C 3ϵ
tj
∥σ¯ tjF(e−µtjx i+σ¯ tjξ ij)+ξ ij∥≤m jaxσ¯
tj
⋅C 3ϵ≤ϵ
32E.2 Proofofoptimalrate
ProofofCorollary2. If∣f(θ(k);i,j)−f(θ(k);l,s)∣≤ϵforalli,j,l,sandk>K,thenbyLemma1
and7,wechoosethemaximumf(θ(k);i,j)forthelowerbound,whichisoforderO(ϵ)awayfrom
theotherf(θ(k);i,j)′s. Therefore,wecantakej∗(k)=argmax f(θ(k);i,j)andabsorbtheO(ϵ)
j
errorinconstantfactors. Thentheresultnaturallyfollows.
E.3 ProofofcomparisonsofE
S
RecallthatthetrainingobjectiveofEDMisdefinedinthefollowing
1 −(logσ¯−Pmean)2 σ¯2+σ2
E σ¯∼ptrainE y,nλ(σ¯)∥D θ(y+n;σ¯)−y∥2=
Z
∫ e 2Ps2 td ⋅ σ¯2σ2data ⋅σ¯2E X0,ξ∥σ¯s(θ;t,X t)+ξ∥2dσ¯.
1 data
Letβ =C β ,i.e.,
j 1 EDM
w(t ) −(logσ¯tk−Pmean)2 σ¯2 +σ2
σ¯
k (t k−t k−1)=C 1⋅e 2Ps2
td
⋅ σ¯tk
2
σ2data ⋅σ¯ t2
k
tk tk data
σ¯ −(logσ¯tk−Pmean)2 σ¯2 +σ2
w(t k)=C 1⋅
t
k−t tk
k−1
⋅e 2Ps2
td
⋅ σ¯tk
t2 kσ d2
ad ta ata ⋅σ¯ t2
k
EDM.Considerσ¯ =tandt
=(σ¯1/ρ −(σ¯1/ρ −σ¯1/ρ)N−k)ρ
fork=0,⋯,N. Then
t k max max min N
t −(logtk−Pmean)2 t 2+σ2
w(t k)=C 1⋅
t
k−k
t k−1
⋅e 2Ps2
td
⋅ k
σ d2
atd aata
(σ¯1/ρ −(σ¯1/ρ −σ¯1/ρ)N−k)ρ
max max min N
=C ⋅
1 (σ¯1/ρ −(σ¯1/ρ −σ¯1/ρ)N−k)ρ −(σ¯1/ρ −(σ¯1/ρ −σ¯1/ρ)N−k+1)ρ
max max min N max max min N
⋅e−(log(σ¯m1/ aρ x−(σ¯m1/ aρ x− 2σ¯ Pm1 s2/ tiρ dn)N N−k)ρ −Pmean)2
⋅
(σ¯ m1/ aρ x−(σ¯ m1/ aρ x−σ¯ m1/ iρ n)N N−k)2ρ +σ d2
ata
σ2
data
Thenthemaximumof σ t2 k = tk appearsatk=N
w(tk) w(tk)
max
σ t2
k =max
t
k =
σ d2 atae(Pmean− 2lo Pg s2 tdσ¯max)2 ⋅⎛
σ¯
−⎛
σ¯1/ρ −
σ¯ m1/ aρ x−σ¯ m1/ iρ n⎞ρ ⎞
k w(t k) k w(t k) C 1(σ¯ m2 ax+σ d2 ata) ⎝ max ⎝ max N ⎠ ⎠
√ N−k
Songetal.[52]. Considerσ¯ = tandt =σ¯2 (σ¯ m2 in) N fork=0,⋯,N. Then
t k max σ¯2
max
√ t −(log√ tk−Pmean)2 t +σ2
w(t k)=C 1⋅
t
k−tk
k−1
⋅e 2Ps2
td
⋅ k
σ d2
atd aata
=C 1⋅
σ¯2 (σ¯ m2
inσ¯ )m Na Nx −k( −σ¯σ¯ mm σ¯ain 2x)N N (−k
σ¯ m2
in)N− Nk+1
⋅e−⎛ ⎝logσ¯max(σ¯σ¯ mm 2ai Pn x s2) tdN N−k −Pmean⎞ ⎠2
⋅
σ¯ m2 ax( σ¯σ¯ m2m2 ai σn
x
d2) atN aN−k +σ d2 ata
max σ¯2 max σ¯2
max max
Then
(Pmean−logσ¯max)2
max σ t2 k =max 1 = σ d2 atae 2P s2 td ⋅ 1 (σ¯ −σ¯ (σ¯ m2 in)1/N)
k w(t k) k 2w(t k) C 1(σ¯ m2 ax+σ d2 ata) 2 max max σ¯ m2
ax
33F ProofsforSection4.2
F.1 ProofwhenE +E dominates.
I D
UndertheEDMchoiceofvariance,σ¯ = tforallt ∈ [0,T],andstudytheoptimaltimeschedule
t
whenE +E dominates. First,itfollowsfromTheorem2that
D I
m2 N−1 γ2
E +E ≲ 2 +d ∑ k
I D T2 (T −t←)2
k=0 k
γ2 γ3 γ2 γ3
+(m2+d)( ∑ k + k + ∑ k + k )
2 (T −t←)4 (T −t←)5 (T −t←)2 (T −t←)3
T−t←≥1 k k T−t←<1 k k
k k
Basedontheabovetimescheduledependenterrorbound,wequantifytheerrorsunderpolynomial
timescheduleandexponentialtimeschedule.
Polynomialtimeschedule. weconsiderT −t←=(δ1/a+(N −k)h)awithh= T1/a−δ1/a anda>1,
k N
γ
k
=a(δ1/a+(N −k−θ)h)a−1hforsomeθ∈(0,1). Wehaveγ k/h∼a(T −t← k)a a−1 and
E +E ≲
m2
2 +
da2Ta1 +(m2+d)(a2Ta1
+
a3Ta2
)
I D T2 δa1N 2 δa1N δa2N2
Therefore,toobtainE +E ≲ε,itsufficestorequireT =Θ(m2 )andtheiterationcomplexity
I D ε1/2
N =Ω(a2(m 2 )a1 m2 2+d )
δε1
2
ε
Forfixedm ,δandε,optimalvalueofathatminimizestheiterationcomplexityN isa= 1ln( m2 ).
2 2 δε1/2
Onceweletδ=σ¯ ,T =σ¯ =Θ(m2 )anda=ρ,theiterationcomplexityis
min max ε1/2
N
=Ω(m2 2∨d ρ2(σ¯ max)1/ρ
σ¯2 ),
d σ¯ max
min
anditiseasytoseethatourtheoreticalresultsupportswhat’sempiricallyobservedinEDMthatthere
isanoptimalvalueofρthatminimizestheFID.
Exponentialtimeschedule. weconsiderγ =κ(T −t←)withκ= ln(T/δ) ,wehave
k k N
m2 dln(T/δ)2 ln(T/δ)2 ln(T/δ)3
E +E ≲ 2 + +(m2+d)( + )
I D T2 N 2 N N2
Therefore,toobtainE +E ≲ε,itsufficestorequireT =Θ(m2)andtheiterationcomplexity
I D 1
ε2
m2+d m
N =Ω( 2 ln( 2 )2)
ε δε1
2
√
Whenm ≤O( d),theexponentialtimescheduleisasymptoticoptimal,henceitisbetterthanthe
2
polynomialtimeschedulewhentheinitilizationerroranddiscretizationerrordominate. Oncewelet
δ=σ¯ ,T =σ¯ =Θ(m2 )anda=ρ,theiterationcomplexityis
min max ε1/2
N =Ω(m2 2∨d ln(σ¯ max)2 σ¯2 ).
d σ¯ max
min
√
Nowweadoptthevarianceschedulein[52],σ¯ = tforallt∈[0,T],itfollowsfromTheorem2
t
that
m2 N−1 γ2 γ2 γ2
E +E ≲ 2 +d ∑ k +(m2+d)( ∑ k + ∑ k )
I D T (T −t←)2 2 (T −t←)3 (T −t←)2
k=0 k T−t←≥1 k T−t←<1 k
k k
Polynomialtimeschedule. weconsiderT −t←=(δ1/a+(N −k)h)awithh= T1/a−δ1/a anda>1,
k N
γ
k
=a(δ1/a+(N −k−θ)h)a−1hforsomeθ∈(0,1). Wehaveγ k/h∼a(T −t← k)a a−1 and
E +E ≲
m2
2 +
da2Ta1 +(m2+d)a2Ta1
I D T δa1N 2 δa1N
34Therefore,toobtainE +E ≲ε,itsufficestorequireT
=Θ(m2
2)andtheiterationcomplexity
I D ε
m2 1 m2+d
N =Ω(a2( 2)a 2 )
δε ε
Onceweletδ=σ¯2 ,T =σ¯2 =Θ(m2 2)anda=ρ,theiterationcomplexityis
min max ε
N
=Ω(m2 2∨d ρ2(σ¯ max)2/ρ
σ¯2 ).
d σ¯ max
min
ComparedtoexponentialtimeschedulewiththeEDMchoiceofvarianceschedule,thisiteration
complexityisworseuptoafactor(σ¯max)1/ρ
.
σ¯min
Exponentialtimeschedule. weconsiderγ =κ(T −t←)withκ= ln(T/δ) ,wehave
k k N
m2 dln(T/δ)2 ln(T/δ)2
E +E ≲ 2 + +(m2+d)
I D T N 2 N
Therefore,toobtainE +E ≲ε,itsufficestorequireT
=Θ(m2
2)andtheiterationcomplexity
I D ε
m2+d m2
N =Ω( 2 ln( 2)2)
ε δε
Onceweletδ=σ¯2 ,T =σ¯2 =Θ(m2 2)anda=ρ,theiterationcomplexityis
min max ε
N =Ω(m2 2∨d ln(σ¯ max)2 σ¯2 ).
d σ¯ max
min
ComparedtoexponentialtimeschedulewiththeEDMchoiceofvarianceschedule,thisiteration
complexity has the same dependence on dimension parameters m ,d and the minimal/maximal
2
varianceσ¯ ,σ¯ .
min max
OptimalityofExponentialtimeschedule. Forsimplicity,weassumem2=O(d). Thenunderboth
2
schedulesin[31]and[52],E sonlydependentonT,andareindependentofthetimeschedule. Both
I
E ssatisfy
D
N−1 γ2
E ≲d ∑ k ≲ε
D (T −t←)2
k=0 k
L Sie nt cβ ek x= ↦ln (( 1TT −−− t et ← k← k −+1 x) )2∈ is(0 c, o∞ nv) e. xT oh ne tn heTγ − dk t o← k ma= in1 x− ∈e− (β 0k ,∞an )d ,∑ acδ c< oT r− dt i← k n< gT tβ hk
e
J= enln se( nT ’s/δ i) nei qs ufi ax lie td y,.
∑
δ<T−t← k<T
(T−γ tk2
← k)2
reachesitsminimumwhenβ
k
areconstant-valuedforallk,whichimpliesthe
exponentialscheduleisoptimaltominimizeE ,henceoptimaltominimizeE +E .
D D I
35