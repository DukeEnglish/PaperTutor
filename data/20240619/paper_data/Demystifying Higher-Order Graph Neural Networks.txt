1
Demystifying Higher-Order Graph
Neural Networks
Maciej Besta1†, Florian Scheidl1, Lukas Gianinazzi1, Shachar Klaiman2, Ju¨rgen Mu¨ller2, Torsten Hoefler1
1ETHZurich 2BASFSE
Abstract—Higher-ordergraphneuralnetworks(HOGNNs)areanimportantclassofGNNmodelsthatharnesspolyadicrelations
betweenverticesbeyondplainedges.Theyhavebeenusedtoeliminateissuessuchasover-smoothingorover-squashing,to
significantlyenhancetheaccuracyofGNNpredictions,toimprovetheexpressivenessofGNNarchitectures,andfornumerousother
goals.AplethoraofHOGNNmodelshavebeenintroduced,andtheycomewithdiverseneuralarchitectures,andevenwithdifferent
notionsofwhatthe“higher-order”means.ThisrichnessmakesitverychallengingtoappropriatelyanalyzeandcompareHOGNN
models,andtodecideinwhatscenariotousespecificones.Toalleviatethis,wefirstdesignanin-depthtaxonomyandablueprintfor
HOGNNs.Thisfacilitatesdesigningmodelsthatmaximizeperformance.Then,weuseourtaxonomytoanalyzeandcomparethe
availableHOGNNmodels.TheoutcomesofouranalysisaresynthesizedinasetofinsightsthathelptoselectthemostbeneficialGNN
modelinagivenscenario,andacomprehensivelistofchallengesandopportunitiesforfurtherresearchintomorepowerfulHOGNNs.
IndexTerms—Higher-OrderGraphNeuralNetworks,Higher-OrderGraphConvolutionNetworks,Higher-OrderGraphAttention
Networks,Higher-OrderMessagePassingNetworks,K-HopGraphNeuralNetworks,HierarchicalGraphNeuralNetworks,Nested
GraphNeuralNetworks,HypergraphNeuralNetworks,SimplicialNeuralNetworks,CellComplexNetworks,CombinatorialComplex
Networks,SubgraphNeuralNetworks.
✦
1 INTRODUCTION many classes of such data models were proposed, includ-
Graphneuralnetworks(GNNs)[62],[73],[125],[141],[251], ing hypergraphs (HGs), simplicial complexes (SCs), or cell
[280], [285] are a powerful class of deep learning (DL) complexes(CCs)[52].Simultaneously,inrecentyears,there
modelsforclassificationandregressionoverinterconnected hasbeenanincreasinginterestinhigher-ordergraphrepresen-
graph datasets. They have been used to study human in- tationlearning(HOGRL),withmanyhigher-ordergraphneural
teractions,analyzeproteinstructures,designchemicalcom- network (HOGNN) models being proposed. These models
pounds, discover drugs, identify intruder machines, model have gained wide recognition as they have been proven to
relationships between words, find efficient transportation befundamentallymorepowerfulthanGNNmodelsdefined
routes, and many others [2], [38], [58], [112], [112], [149], onplaingraphs[256],forexampleintermsofwhatgraphs
[181],[203],[212],[228],[244],[264],[267],[268],[277],[281]. theycandistinguishbetween.
Many successful GNN models have been proposed, for Many HOGNNs have been introduced, but the term
example Graph Convolution Network (GCN) [150], Graph “higher-order” has been used in so many different set-
Attention Network (GAT) [237], Graph Isomorphism Net- tings, that it is not clear how to reason about, let alone
work (GIN) [255], or Message-Passing (MP) Neural Net- compare, different HOGNNs. A potential HOGNN model
works [114]. These GNN models are designed for “plain can be based on any of the available HOGDMs (HGs, SCs,
graphs” where relations are only dyadic, i.e., only defined CCs, etc.) and it can harness any of the available GNN
for vertex pairs [126], [252], [280]. While being simple, such mechanisms (convolution, attention, MP, etc.). Moreover,
pairwise relations can be insufficient to adequately capture manyworksintroduceHOintoplaingraphs,byconsidering
relationships encoded in data [20], [21]. For example, con- convolution over “higher-order neighboring vertices” or
siderthefollowingtwocasesofco-authorshiprelations:(a) graph motifs. Some models even use hierarchical nested
threeauthorsworktogether(asagroupofthree)onasingle graphdatamodelsanduseconvolution,attention,orMP,on
paper,and(b)everytwooftheseauthorsworkasapairon suchnestedgraphs.Alltheseaspectsresultsinaverylarge
separate papers [263]. These cases cannot be distinguished space of potential HOGNNs, hindering the understanding
when using a plain graph, because they are both modeled offundamentalprinciplesbehindthesemodels,differences
as a clique over three vertices. Other such examples can be betweenmodels,andtheirprosandcons.
found in social networks (e.g., a group of friends forms a To address these issues, we analyze different aspects of
polyadicrelation[6]),inpharmaceuticalinteractionnetworks HOGNNs and derive a taxonomy, in which we formalize
(e.g.,multi-druginteractions[229]),inneuroscience[115]or and define new classes of graph data models, relations
ecology [118]. To capture such relationships, going beyond between them, and the corresponding classes of HOGNNs
pairwiseinteractionsisnecessary. (contribution #1). The taxonomy comes with an accom-
Higher-ordergraphdatamodels(HOGDMs)addressthisby panying blueprint recipe for generating new HOGNNs
explicitlyencodingpolyadicinteractionsintothegraphdata (contribution #2). We use our taxonomy to study over 100
model (GDM). HOGDMs have been intensely studied, and HOGNNrelatedschemes(contribution #3)andwediscuss
their expressiveness, time complexities, and applications.
†Correspondingauthor OurworkwillhelptodesignmorepowerfulfutureGNNs.
4202
nuJ
81
]GL.sc[
1v14821.6042:viXra2
1.1 ScopeofthisWorkvs.RelatedSurveys&Analyses to transform the input, i.e., the graph structure A and the
We focus specifically on HOGNNs based on the MP input features X, into the output feature matrix Y (unless
paradigm. We exclude works related to early non-GNN specified otherwise, X models vertex features). In this pro-
higher-order learning, such as the work by Scho¨lkopf et cess,intermediatehiddenlatentvectorsareoftencreated.One
al. [284]. We also do not focus on spectral graph learning updatesthesehiddenfeaturesiteratively,usuallymorethan
beyondwhatisrelatedtoHOGNNsandMP[213]. once.AsingleiterationiscalledaGNNlayer.Finally,output
Our work complements existing surveys. These works
featurevectorsareusedindownstreamMLtasks,suchasnode
analyze HOGDMs in the context of complex physical pro- classification.
cesses [22], [52], [234] and signal processing [213]. A recent A single GNN layer consists of a graph-related operation
workontopologicaldeeplearning[122],[123],[192]covers (usually sparse), an operation related to traditional neural
a very broad set of topics related to topological neural networks (usually dense), and a non-linear activation (e.g.,
networks. Another work [9] focuses on broad representa- ReLU [150]) and/or normalization. An example sparse
tion learning for hypergraphs. Some other results provide operation is the graph convolution [150] in which each
blueprints for a limited class of HOGNNs, for example vertex v generates a new feature vector by summing and
Node-based Subgraph GNNs [100] or others [80], [136]. transforming the features of its neighbors. Example dense
We complement these papers by providing a taxonomy of operationsappliedtothefeaturevectorsareMLPsorlinear
a broad set of HOGNNs together with an accompanying transformations.
generalblueprintfordevisingnewHOGNNs. 2.2.1 Localvs.GlobalFormulations
Many GNN models are specified with a so called local
2 BACKGROUND, NAMING, NOTATION
formulation. Here, to obtain the latent feature vector hi of
Wefirstestablishconsistentnamingandnotation. a given node i in a given GNN layer, one aggregates the
2.1 PlainGraphDataModel&BasicNotation featurevectorsoftheneighborsN(i)ofiusingapermutation
(cid:76)
invariantaggregator ,suchassumormax.Intheprocess,
The fundamental graph data model is a tuple G = (V,E);
feature vectors of the neighbors of i may be transformed
V isasetofvertices(nodes)andEisasetofedges;|V|=n
by a function ψ. Finally, the aggregator outcome is usually
and|E|=m.Wewillbereferringtoitasaplaingraph(PG)
furthermodifiedwithanotherfunctionφ.Insummary,one
or, when it is clear from the context, as graph. This model,
by default, does not incorporate any explicit higher-order
obtainsafeaturevectorh( il+1) ofavertexiinthenextGNN
structureinformation.Ifedgesmodeldirectedrelations,we layerl+1as  
use a directed graph G = (V,E) in which the edges are a h(l+1) =φh(l), (cid:77) ψ(cid:16) h(l),h(l)(cid:17)  (1)
i i i j
subsetoforderedvertexpairsE ⊆V ×V.N(v)denotesthe
j∈N(i)
set of vertices adjacent to vertex (node) v, d v is v’s degree, Different forms of ψ are a basis of three GNN
and d is the maximum degree in G. The adjacency matrix
classes: Convolutional GNNs (C-GNNs), Attentional GNNs
A ∈ {0,1}n×n of a graph determines the connectivity of
(A-GNNs), and Message-Passing GNNs (MP-GNNs). ψ re-
vertices:A(i,j)=1⇔(i,j)∈E.
(l)
turns a product of h with – respectively – a fixed
WecalltwographsG
1
=(V 1,E 1),G
2
=(V 2,E 2)isomor- j
scalar coefficient (C-GNNs), a learnable function that
phic if there exists a bijection φ: V 1 → V 2 that preserves
returns a scalar coefficient (A-GNNs), and a learn-
edges, i.e., {φ(u),φ(v)} ∈ E 2 ⇔ {u,v} ∈ E 1, for all
able function that returns a vector coefficient (MP-
u,v ∈ V 1. An isomorphism of directed graphs requires
GNNs)[61].Forexample,intheseminalGCNmodel[150],
(φ(u),φ(v))∈E 2 ⇔(u,v)∈E 1 forallu,v ∈V 1. (cid:18) (cid:18) (cid:19)(cid:19)
Theinput,output,andlatentfeaturevectorofavertexi h(l+1) = ReLU W(l)× (cid:80) √1 h(l) . Here, (cid:76)
aredenotedwith,respectively,xi,y i,hi ∈Rk,k isthenum- sui
ms N(i) ∪ {i} ≡ N(cid:98)(i),
ψj∈N r(cid:98) e( ti u) rnsdid aj pj
roduct of each
ber of features1. These vectors can be grouped in matrices, (cid:112)
denoted respectively as X,Y,H ∈ Rn×k. If needed, we use neighbor j’s feature vector with a scalar 1/ d id j, and φ
isalinearprojectionfollowedbyReLU.”
theiterationindex(l)todenotethelatentfeaturesinagiven
iteration(e.g.,aGNNlayer)l(e.g.,h(l) ,H(l)). Some GNN models also have global linear algebraic
i formulations, in which one employs operations on whole
We denote multisets with double brackets {{·}}. For a
matrices X, H, A, and others. For example, in the GCN
ase nt oV n, -w nee gd ate in vo ete ini tt es gp eo rw ner ∈se Zt >b 0y l2 eV
t
[= n]{ =W {: 1,W ...⊆ ,nV }.} A.Fo
∼
=r m izeo dd ae dl, jaH ce( nl+ cy1) m= atrR ixeL wU ith(A(cid:98) seH lf( ll o)W ops(l .)),whereA(cid:98) isthenormal-
B is an isomorphism between A and B. 1 is the indicator
function,i.e.,1
p
=1ifpistrueand1
p
=0otherwise. 3 WHY HIGHER-ORDER GNNS?
2.2 GraphNeuralNetworks(GNNs) GNNs have attained state-of-the-art results in many graph
tasks.Yet,therearesometasksthatstandardMP-GNNs(c.f.
Each vertex and often each edge are associated with input
Eq.1)strugglewith.Forexample,Figure1showstwonon-
featurevectorsthatcarrytask-relatedinformation.Forexam-
ple, if nodes and edges model papers and citations, then
isomorphic graphs G 1,G 2. Colors indicate node features
and, for each node, we portray its one-hop neighborhood
a node input feature vertex could be a one-hot encoding,
intheadjacentsurfaceenclosedbydashedlines.
which determines the presence of words in the paper ab-
In the best case, a node v ∈ V retains all features of
stract. When developing a GNN model, one specifies how
the nodes in its one-hop neighbourhood N(v) in a single
(cid:76)
iteration. Since a permutation-invariant aggregation is
1Forclarity,weusethesamesymbolforthenumberofinput,output,andlatent
features;thisdoesnotimpactthegeneralityofanyinsightsinthiswork. applied to incoming messages ψ(xv,xu), a node only sees3
G G two vertices can be adjacent if they both belong to one
1 2
v 1 v 2 v 1 v 2 v 1 v 2 v 1 v 2 triangle (or another specified subgraph). Other examples
v 3 v 4 v 3 v 3 of node adjacency notions are based on shortest paths or
v v v v
1 2 1 2 the probability of co-occurrences in a random walk. In
v hypergraphs, an example of adjacency between two nodes
1
v 1 v 2 v 3 v 4 v 3 can also be based on the number of hyperedges these two
v 3 v 4 v 3 v 4 v 3 v 4 v 5 v 2 nodesareincidenton.
v 5 v 6 v 4 v 3 v 4 The third fundamental HOGDM element is the spec-
v 6 ification of potential distinguished substructures among
v v v v vertices, links, and adjacencies. For example, in Subgraph
5 6 5 6
v 3 v 4 v 4 v 4 GNNs (detailed in Section 5.5), one uses message-passing
v 5 v 6 v 5 v 6 v 5 v 6 v 5 v 6 over graphs with distinguished collections of vertices and
edges. Such structures are usually specified by appropri-
All 1-hop neighborhoods for both G1 and G2 are iden�cal, ately extending a given GDM definition; in the above ex-
even though these two graphs have different structures. ampleofSubgraphGNNs,adefinitionofspecificvertexor
Fig.1:Computationalstructureofaggregationsforagivenvertexinsimple1-hop edgecollections.Thesestructuresareoftenassignedfeature
GNNs(itincludes1-hopneighborsandagivenvertex).GraphsG1andG2are vectors, which are updated in each GNN layer similarly to
non-isomorphicbutcannotbedistinguishedbyasimpleGCN.
thoseofverticesandedges.
the set of transformed features {ψ(xv,xu) : u ∈ N(v)} and One can also introduce HO by associating additional
cannot associate them with specific nodes. Thus, the most information with vertices, links, and distinguished sub-
information about features that can be obtained through structures.Forexample,onecouldenhanceeachvertexwith
one iteration is xn vew = (xv,{xu : u ∈ N(v)}). Note that thenumberoftrianglesitispartof.
for the graphs G 1,G 2, the one-hop neighborhoods of the Finally, while an individual vertex usually models a
corresponding vertices look the same. The same will hold fundamental entity (as in plain graphs or in hypergraphs),
after each further MP step (only the messages change). it can also model a nested graph, as in nested GNNs. For
Thus, the readout function will map these graphs to the example,whenmodellingmoleculesandtheirinteractions,
same value. However, the graphs are not isomorphic. For thehigher-levelgraphrepresentsmoleculesasverticesand
example,G 2containstwotriangles,whileG 1containsnone. theirinteractionsas(hyper-)edges.Moreover,eachmolecule
This problem has sparked an increased interest in GRL isrepresentedasaplaingraphinwhichatomsformvertices
designs that incorporate polyadic relationships, aka higher- andatomicbondsformedges.
orderstructures,intotheirlearningarchitecture.
4.2 HigherOrderinGNNArchitectures
4 TAXONOMY & BLUEPRINT The central part of specifying an HOGNN architecture is
determining the message-passing (MP) channels that will
In general, when analyzing or constructing a HOGNN ar-
be used in GNN layers. We refer to this step as wiring.
chitecture, one must consider the details of the harnessed
Formally,wiringcreatesasetoftuplesW ={(x,y)}where
graph data model (GDM) and the details of the neural
x,y can be any parts of the used GDM, such as vertices,
architecture that harnesses a given GDM. We first discuss
links,andanysubstructures.ThesetuplesformMPchannels
the HO aspects of the GDM (Section 4.1) and later those
that are then used to exchange information in each GNN
of the GNN architecture based on that GDM (Section 4.2).
layer.Theexchangeddataarefeaturevectors;thus,xandy
We then describe how prescribing these aspects forms a
fromeachelementofW havefeaturevectors.
blueprintfornewHOGNNarchitectures(Section4.3).
An important aspect of an imposed wiring is its fla-
4.1 HigherOrderinGraphDataModels
vor. Similarly to MP over plain graphs, we distinguish
Figure2illustratetheblueprintofHOGDMs.Thetaxonomy convolutional,attentional,andgeneralmessage-passingflavors.
ofHOGDMsisillustratedinFigure3andinTable1. However, while in the GNNs over plain graphs it was
The first fundamental element of any HOGDM is a straightforwardtodefinetheseflavorsbasedonthedifferent
specificationoflinksbetweenvertices.Forexample,inPGs, forms of ψ (see Section 2.2.1), in HOGNNs, it becomes
alinkisanorderedtupleoftwovertices(indirectedPGs)or morecomplicated,becausemodelformulationscanbevery
asetoftwovertices(inundirectedPGs).Inhypergraphs,a complex. For example, in HOGNNs based on hypergraphs
link(calledahyperedge)isasetofarbitrarilymanyvertices. orsimplicialcomplexes,exchangingamessagebetweentwo
In cell complexes, one uses cells which form a hierarchy of verticesmayinvolvegeneratingmultiplefeaturevectorsas-
connections that can be pictured with Hasse diagrams (see signedtointermediatestepswithinoneGNNlayer.Forthis,
theredpartofFigure3). we propose the following definition: an HOGNN model is
Another fundamental GDM element is the adjacency: convolutional, attentional, or general message-passing if –
thespecificationofhowvertices,links,andpotentiallyother respectively – all the functions used by the model return
parts of a given GDM connect. The adjacency always spec- fixed scalar coefficients, at least one of these functions is
ifies what is connected (e.g., vertices, subgraphs) and how learnableandallthefunctionsreturnscalarcoefficients,and
it is connected (e.g., vertices are adjacent when they share at least one of these functions is learnable and it returns
an edge). In PGs, adjacency is fully determined by links. a vector coefficient. Note that these flavors can be used
However, one can make it richer. For example, in a GDM simultaneously. For example, in Motif-graphs, one could
that we refer to as “Motif-Graph” and detail in Section 5.6, haveconvolutionalmessage-passingalongedge-basedadja-4
Blueprint of a higher-order Graph Data Model (HOGDM)
1 Links 2 Substructures 3 Adjacencies & their subjects 4 Informa�on 5 Nes�ng
1 Links 3 Adjacencies
The specifica�on of basic structures that connect ver�ces The specifica�on of how ver�ces and links connect
Edge Hyperdge Cell Edges are 1-cells Incidence
Adjacency hyperedge e 2-cell c2
via edges
vertex v
1-cell c1
Two ver�ces are vertex w
adjacent via a
shared edge - no v is incident on e w is incident on c1
Closed polygons are 2-cells higher order e is incident on v c1 is incident on c2
A standard dyadic edge between A hyperedge is an A cell is similar to a hyperedge; the main
two ver�ces (no higher order) arbitrary set of ver�ces difference is that cells form a hierarchy
Boundary adjacencies
2 Substructures Boundary adjacency Co-boundary adjacency
hyperedge e hyperedge e
The specifica�on of dis�nguished structures that go beyond ver�ces and links
Node-tuple oO f r vd ee rr �i cn eg s Subgraph cS au nb og vra ep rlh as p Subgraph-tuple O surd be gr ri an pg h o sf vertex v vertex v
v is boundary adjacent e is co-boundary
to e (same as incidence) adjacent to v
Lower & upper adjacency
vertex v1
hyperedge e2
vertex v2
Node-tuple is an Tuples can A dis�nguishedsubgraph is a designated Subgraph-tuple is an ordered tuple
ordered tuple of ver�ces. overlap subset of ver�ces and edges of dis�nguished subgraphs e1 is lower adjacent to e2
hyperedge e1 v1 is upper adjacent to v2
Adjacencies of ver�ces via dis�nguished structures Adjacencies of structures
Node-tuple adjacency Subgraph/Mo�f adjacency Node-tuple adjacency Subgraph/Mo�f adjacency Ordered subgraph/Mo�f adjacency
vertex v1 vertex v1 vertex v1
v1 vertex v2
vertex v1 subgraph s
vertex v2
node-tuple nt node-tuple nt1 subgraph s1 subgraph s1
node-tuple nt2
v2 subgraph s2 subgraph s2
v1 to i s v a 2d vj ia ac nen tt v1 t ois va 2d vja iac e snt n nt t1 2 i vs i aa d vj 1a c ae nn dt vt 2o s1 t ois sa 2d vja iac e vnt s1 t ois sa 2d vja iac e vnt
Other adjacencies Combina�ons of adjacencies Different types of adjacencies can be used together
There are numerous other ways to define adjacencies. A few examples: In exis�ng HOGDMs, node-tuple links, subgraph links, and subgraph-tuple links are used together with standard edges.
Node-tuples + edges Subgraphs + edges Subgraph-tuples + edges
- Two ver�ces can be adjacent if v1 Edge
they are both located on a given
number of shortest paths
- Two ver�ces can be adjacent if
they have a high probability of
co-occurence on a random walk
- Two ver�ces that are incident
on two or more hyperedges, v2
are adjacent
- ...
Note that all these HO forms of links are constructed on top of standard edges, i.e.,
a subgraph link or a node-tuple link can come with connec�ons that duplicate exis�ng edges
4 Informa�on 5 Nes�ng
Using addi�onal informa�on There are numerous ways to define nes�ng. A few examples:
1 Nes�ng in graphs Nes�ng in hypergraphs
Example: how
2 many triangles
each vertex
is a part of.
2 2
1
Fig.2:Ablueprintofahigher-ordergraphdatamodel(HOGDM).5
Links: edges Links: hyperedges Links: cells
Sec�on 2.1 Use cells
Plain Graph (PG) Use hyperdges instead of edges
instead of edges
Links:
→ plain edges
Sec�on 5.1 Sec�on 5.3
Adjacencies & subjects: Sec�on 5.7 Hypergraph (HG) Cell Complex (CC)
→ plain edges Nested Graphs
(between ver�ces). Links:
Links: → hyperedges. Links:
→ edges → 1-cells
Adjacencies & subjects: Adjacencies & subjects:
→ as in Plain Graphs → incidence (between Adjacencies & subjects:
Add nes�ng ver�ces & hyperedges), → incidence (between
→ boundary (between n-cells and (n+1)-cells).
ver�ces & hyperedges).
add subgraphs add ordering
+
+
Sec�on 5.5 Sec�on 5.4
For each hyperdge, Make is possible
Graph with Subgraph- Graph with Node- make all subsets for ver�ces to be
-Collec�ons -Tuple-Collec�ons of ver�ces also directly adjacent
(SCol-Graph) (NT-Col-Graph) form a hyperdge to any n-cells, for
n ≥ 1
Links:
→ plain edges. Links: Sec�on 5.3
→ plain edges.
Combinatorial
Adjacencies & subjects:
→ (be p twla ein e e nd vg ee rs �ces), A →d pja lace inn eci de gs e & s subjects: Sec�on 5.2 Complex (CbC)
→ subgraphs (between ver�ces), Simplicial Complex
(between ver�ces), → node-tuples (SC) Links:
→ subgraph adjacencies (between ver�ces), → any n-cells, for n ≥ 1
(between subgraphs), → down adjacencies Links:
→ ... (between node-tuples), → hyperedges Adjacencies & subjects:
→ ... → incidence (between
Adjacencies & subjects: n-cells and (n+1)-cells,
→ incidence (between and between 0-cells and
ver�ces & hyperedges), any n-cells for n ≥ 1).
→ boundary (between
ver�ces & hyperedges).
Make subgraphs
combine subgraphs + ordering
recurring
Sec�on 5.5
Plain Graphs vs. Hypergraphs vs. Cell/Combinatorial Complexes
Graph with Subgraph- using Hasse Diagrams
-Tuple-Collec�ons
(ST-Col-Graph) Plain Graph Sec. 2.1 Hypergraph Sec. 5.1
Sec�on 5.6
3 4
Mo�f-Graphs Links: 3 4
1
→ plain edges, 1
L →in pk ls a: in edges. → subgraphs. 2 5 2 5
Adjacencies & subjects: A plain graph can be represented
Adjacencies & subjects: → plain edges as a bipar�te structure.
→ (be s tu wb eg er nap vh es r�ces). ( → (b be e st tuw wbe ege ern nap v vhe esr r� �c ce es s) ), , diH aa gs rase m: 1 2 3 4 5 A arc s e sh a p tay n rr p ub eae cisl p tr s e ug ao nr rr ta eb �ep .e td eh H 1asse 2 diag 3ram: 4 5
→ subgraph tuple edges:
adjacencies(between hyperedges:
subgraphs),
→ ...
Cell Complex Sec. 5.3 Combinatorial Sec. 5.3
Complex
3 4 3 4
1 1
A sd ed le in ctf eo d o sn t rc uo cu tn ut rs e sof 2 5 2 5
(e.g., triangles) that A cell complex can be represented A combinatorial complex can be
each vertex belongs to as a mul�-par�te structure. Any represented as a mul�-par�te structure.
n-cell can only connect Unlike in cell complexes, an n-cell can
Sec�on 5.6 to (n-1) or (n+1)-cells. connect to any other cells.
G -Cr oa up nh t w s (i Sth C nS tu -b Gg rara pp hh s- ) 1 2 1 H 1ass 2e dia 3gram 4: 5 H 1asse 2 dia 3gram 4: 5
L →in pk ls a: in edges. 1 2 1 1-cells: 1-cells:
Adjacencies & subjects:
→ links (between ver�ces). 1 2-cells: 2-cells:
Fig.3:Ataxonomyofhigher-ordergraphdatamodels(HOGDMs).6
Usedlinks Harnessed Distinguished Additional
GDMname,examplereference,andabbreviation Nesting
betweenvertices adjacencynotions substructures information
Hypergraph[12] HG hyperedge Incidence,boundary — — —
NestedHypergraph[257]⋆ Nested-HG hyperedge Incidence,boundary — — Yes
SimplicialComplex[57] SC hyperedge/cell Incidence,boundary simplices — —
CellComplex[55] CC cell Incidence,boundary — — —
CombinatorialComplex[122] CbC cell Incidence,boundary — — —
GraphwithNode-Tuple-Collections[187]⋆ NT-Col-Graph edge,node-tuple edge,tupleadjacency nodetuples — —
GraphwithSubgraph-Collections[256]⋆ SCol-Graph edge,subgraph edge,subgraphadjacency subgraphs — —
GraphwithSubgraph-Tuple-Collections[197]⋆ ST-Col-Graph edge,subgraph-tuple subgraph-tupleadjacency subgraph-tuples — —
GraphwithMotifs[206]⋆ Motif-Graph edge motifadjacency motif — —
GraphwithSubgraph-Counts[59]⋆ SCnt-Graph edge edge — subgraphcount —
NestedGraph[240]⋆ Nested-Graph edge edge — — Yes
TABLE1:Comparisonofconsideredhigher-ordergraphdatamodels(HOGDMs)withrespecttothetaxonomyintroducedinSection4.1.“⋆”indicatesagraph
datamodelformallystatedinthiswork.
cenciesand,inaddition,attentionalmessage-passingalong Next, one specifies the details of the HO neural model,
adjacenciesdefinedbybeinginacommontriangle. as discussed in Section 4.2. This includes details of wiring
Anotherrelevantaspectoftheharnessedwiringpattern, and of how the feature vectors are transformed between
is whether it uses multi-hop channels. These are wiring GNN layers, and the specifics of harnessed lifting(s) and
channels that connect vertices or links which are multiple lowering(s).AtypicalHOGNNpipelineisinFigure5.
stepsofadjacencyawayfromeachother.Suchchannelsmay
5 HIGHER-ORDER GRAPH DATA MODELS
beusefulwhendealingwithissuessuchasoversmoothing.
AtechnicalaspectofconstructingaHOGNNiswhether We now investigate GDMs used in HOGNNs. First, we
tousethelocalortheglobalformulation(orwhetherusea analyze the existing established GDMs: hypergraphs (HGs)
combination of both), when constructing the MP channels. (Section 5.1) as well as their specialized variants, namely,
As we illustrate in Section 6, most HOGNN architectures simplicial complexes (SCs) (Section 5.2) and cell complexes
use either the local formulation, or a mixture of local and (CCs) (Section 5.3). Next, we introduce new HOGDMs that
global formulations, when prescribing MP channels. The formally capture data models used in various existing
localformulationisusuallyeasiertodevelop,buttheglobal HOGNNs. These are graphs equipped with node-tuple collec-
formulationmayresultinlowerrunningtimesoftheGNN tions (NT-Col-Graphs) (Section 5.4), graphs equipped with sub-
computation, as it makes it easier to take advantage of graph collections (SCol-Graphs) (Section 5.5), graphs equipped
featuressuchasvectorization[46]. with subgraph-tuple collections (ST-Col-Graphs) (Section 5.5),
Finally,whenbuildingaHOGNNarchitecture,onemust
graphsequippedwithmotifs(Motif-Graphs)(Section5.6),graphs
considerhowtotransformtheinputdatasetinto,andfrom,
equippedwithsubgraphcounts(SCnt-Graphs)(Section5.6),and
anHOformat.Inmanydatasets,thedataisstoredasaplain
Nested-GDMs(Section5.7).
graph [134], [184]. We refer to a transformation from the WesummarizetheconsideredGDMsintermsofthepro-
plaintotheHOformataslifting.Wealsodenoteamapping videdGDMblueprintfromSection4.1inTable1.Ingeneral,
from an HOGDM to a PG as lowering. When conducting a HGs, SCs, and CCs introduce HO by harnessing different
lifting, one usually does not want to lose (or change) any notions of links and adjacencies beyond plain dyadic inter-
structural information. For example, one usually wants to actions. NT-Col-Graphs, SCol-Graphs, and ST-Col-Graphs
preserveisomorphismproperties.Formally,wehave utilize certain distinguished structures. Motif-Graphs har-
ness motif-based forms of adjacency. SCnt-Graphs count
Definition 4.1 (Graphdatamodellifting). LetGbetheclass
subgraphsasdistinguishedinformation.Finally,somemod-
of graphs and K a higher-order graph data model equipped with
elsusenestingofgraphsorhypergraphswithinvertices.
a notion of isomorphism. A GDM lifting from G to K is a
InPGs,linksareedgesbetweentwovertices.InHGsand
map f: G → K that preserves isomorphisms. That is, for any
their specialized variants (SCs, CCs), HO is introduced by
G 1,G
2
∈G,G 1andG 2areisomorphicifandonlyiff(G 1)and
making edges being able to link more than two vertices. In
f(G 2)areisomorphic.
NT-Col-graphs,SCol-graphs,andST-Col-graphs,oneintro-
Incontrast,loweringsgenerallyintroducealossofinfor- duces HO by distinguishing collections of substructures in
mation,aswewillseeinSection5.1.2. additiontoharnessingedgesbetweenvertices.
We discuss in more detail how all these HO aspects 5.1 Hypergraphs
are harnessed by different existing HOGNNs in Section 6.
Hypergraphsgeneralizeplaingraphsbyallowingarbitrary
Figure4illustratesthetaxonomyofHOarchitectures.
subsetsofentitiestoformhyperedges.
4.3 Blueprint&PipelineforHOGNNs
Definition 5.1. A HG with features is a tuple H = (V,E,x)
In our blueprint for creating HOGNNs with desired prop- comprised of a set of nodes V, hyperedges E ⊂ 2V, and features
erties, one first specifies a HOGDM by selecting the HO x: V ∪E →Rk.
aspects described in Section 4.1. This includes selecting
a form of links, adjacencies, distinguished substructures, This enables modeling complex and diverse forms of
additional information, and nesting. Many of possible se- polyadic relationships, used broadly in various domains
lectionsresultinalreadyexistingHOGDMs,forexample,if andproblemssuchasclusteringorpartitioning.
usinghyperedgesaslinks,oneobtainsahypergraphasthe We call two HGs H 1 = (V 1,E 1,x 1),H 2 = (V 2,E 2,x 2)
GDM. Many other selections would result in novel GDMs, isomorphic, if there is a bijective node relabeling φ: V 1 →
with potentially more powerful expressiveness properties. V 2 suchthatallhyperedgesandfeaturesarepreserved,i.e.,7
Neural MP on Hypergraphs (Sec�on 6.1) These models mostly rely on the incidence based adjacency for communica�on channels
Neural MP on Simplicial Complexes (Sec�on 6.2)
These models mostly rely on the boundary adjacency for communica�on channels
HO GNN Neural MP on Cell Complexes (Sec�on 6.3)
architectures
(Sec�on 6)
Neural MP on NT-Col-Graphs (Sec�on 6.4) These models mostly rely on the down adjacency for communica�on channels
Neural MP on SCol-Graphs & ST-Col-Graphs (Sec�on 6.5) Ego-nets (Sec�on 7.5.1)
Reconstruc�on-based (Sec�on 7.5.2)
Neural MP on Mo�f-Graphs & SCnt-Graphs (Sec�on 6.6) Subgraph-adjacency based (Sec�on 7.5.3)
Fig.4:Ataxonomyofhigher-orderGNNarchitectures.
1 Input graph dataset 2 Poten�al li�ing 3 HOGNN model
neighborhoods”.Theseare–respectively–co-boundaryand
upperadjacenciesofagivenvertex.
Different forms of adjacency in HGs can be modeled
with plain graph incidence within plain graphs. First,
4 Downstream tasks boundary adjacency and regular incidence are equivalent.
When a node is connected to an edge, the edge is a co-
boundary adjacency of the node. When two edges share a
node,theyarelower-adjacent.Standardadjacencybetween
nodesinagraphcorrespondstotheupperadjacency.
5.1.2 LoweringHypergraphs
Fig. 5: A typical HOGNN data pipeline. (1) The input dataset is a set of one
ormoregraphs{G = (V,E,x)}.(2)TheinputisliftedtoaselectedHOGDM Lowering a HG prescribes how to map a given HG into
(HOGDMsaredetailedinSection5).VectorsintheredovalsindicateRk-valued
a plain graph First, while in graphs nodes are related by
features.IntheHOGDM,wecommonlyhavefeaturesforhigher-orderstructures,
forexample,edgesandhyperedges.(3)TheconstructedHOGNNarchitecture sharing a single edge, in HGs, nodes can potentially share
transformsfeatures(HOGNNsaredetailedinSection6).(4)Finalfeaturesarefed
multiple hyperedges. This relation gives rise to a canonical
todownstreamtasks,forexample,nodeprediction.
mappingofHGstoedge-weightedgraphs.Here,weassign
∀ xe 2∈ (φ2V (1 v)e )∈ anE d1
∀
e⇐ ∈E⇒
1
x
1{ (φ e)(v =) x: 2v ({∈ φ(e v} )∈ :vE ∈2, e∀ }v )∈ .V1 x 1(v) = a
n uu
sw
em
de big
te
oh rt
o
lew
f
ahu rnv ypt no
e or
dea dn egy
re
ep
s
pa
t
ri
h
er
se
eo
y
nf
s
tn aho
ta
id
r
oee n.s sTu bh, yiv
s
fim∈
rsa
tV
pp
apig
n
piv
g
lye hn
ina
gb sy
b
itet eh
tn
oe
Tosimplifyournotation,foranyvertexv inV wewrite
the HG and then employing a GNN designed for edge-
v ∈ H and for every hyperedge e in E, we write e ∈ H.
weightedgraphs[15],[97].
Moreover,fortheprecisionofthefollowingGDMconcepts,
An HG H = (V,E) can also be mapped to a bipartite
wealsodefineforeachvertexv ∈ V asetvˆ= {v},andfor
eachhyperedgee∈E wedefineeˆ≡e. graph G H = (V˜ = V ∪E,E˜), in which a node-hyperedge
pair {u,e} for u ∈ V,e ∈ E belongs to E˜ if u ∈ e [131].
5.1.1 Higher-OrderFormsofAdjacency
featuresofincidentnodes[131].
The HG definition gives rise to multiple “higher-order”
Twootherschemesarecliqueexpansionandstarexpan-
flavors of node and hyperedge adjacency. Two basic flavors
sion[12],[196].Intheformer,oneconvertseachhyperedge
are incidence and boundary adjacency. They were originally etoacliquebyaddingedgesbetweenanytwoverticesine.
defined for cell complexes (CCs) [55]; we adapt them for In the latter, in place of each hyperedge e, one adds a new
HGs. vertex v e and then connects each existing vertex contained
For a vertex v and hyperedge e, if v ∈ e, then v is inetov e,effectivelycreatingastar.
incidentoneandeisincidentonv.Thedegreeofavertexor
All of the above-mentioned lowerings involve informa-
hyperedgeisthenumberofhyperedgesorverticesincident tion loss in general. For example, clique expansion applied
onit,respectively. to hyperedges e 1 = {v 1,v 2} and e 2 = {v 1,v 2,v 3} would
Definition5.2. LetH=(V,E)beaHG,andb,c∈V ∪E.We erase information about the presence of two distinctive
callbboundary-adjacentoncandwriteb≺cifˆb⊊cˆ. hyperedges, instead resulting in a plain clique connecting
verticesv 1,v 2,v 3 [12].
Boundary adjacency gives rise to four closely related
ThesedifferentHGloweringshavevastlydifferentstor-
forms of adjacency between node-sets. For c ∈ V ∪E we
age requirements. With edge-weighted graphs, the repre-
define
sentation takes O(|V| + |E|) space. Let k be the largest
B(c)={b∈V ∪E :b≺c} boundaryadjacencies, degree of any edge in the HG. Then, the representation
C(c)={d∈V ∪E :c≺d} co-boundaryadjacencies, as a bipartite graph or using the star expansion takes
(cid:80)
N ↓(c)={b∈V ∪E :∃τ :τ ≺b,τ ≺c} loweradjacencies, O(|V| + e∈E|e|) ⊆ O(|V| + k|E|k) ⊆ O(|V|(1 + |E|)).
N ↑(c)={d∈V ∪E :∃τ :c≺τ,d≺τ} upperadjacencies. ThecliqueexpansiontakesO(k|V|)space.
The HG definition is very generic, and several varia-
These notions of adjacency are sometimes referred tions that make it more restrictive have been proposed.
to using different names. For example, the HyperSAGE Two subclasses of HGs are particularly popular: simplicial
model[12]introducesthenotionsof“intra”and“inter-edge complexes(SCs)andcellcomplexes(CCs).8
5.2 SimplicialComplexes edgesbetweenthepairswhichdonotinteract.Similarly,in
SCs restrict the HG definition to ensure stronger assump- drug treatments, certain multi-drug interactions may only
tions on the polyadic relationships. Specifically, in SCs, appearinthepresenceofmorethantwodrugs[229].
vertices are also connected with hyperedges; however, if a The introduction of cell complexes (CCs) came to ad-
given set of vertices is connected with a hyperedge, then dress the above limitations by creating a hierarchy which
all possible subsets of these vertices also need to form a hy-
isconstructedbyattachingtheboundariesofn-dimensional
peredge.Thisassumptionisusefulin,e.g.,socialnetworks,
spherestocertain(n−1)-cellsinthecomplex[55].Thisgen-
wheresubsetsoffriendgroupsoftenalsoformsuchgroups. eralizesandremovesthestrictdependencyofthehierarchy
withthenumberofverticesasisthecaseinSCs.Specifically,
Definition 5.3. A simplicial complex (SC) with features
we can now construct cell complexes by using vertices
H = (V,E,x) is a HG in which for any hyperedge e ∈ E,
(0-cells), edges (1-cells), and surfaces (2-cells), which can
anynonemptysubsetd⊂ecorrespondstoavertexorhyperedge
already cover the most common applications. The formal
d=cˆforsomec∈H;aswithHGs,wehavex: V ∪E →Rk.
definitionofregularCCsisasfollows:
All forms of generalized adjacencies (Section 5.1.1) are Definition 5.6 ( [55]). A real regular CC is a higher-
directlyapplicablealsotoSCs.Anyplaingraphsatisfiesthe dimensional real subset X ⊂ Rd with a finite partition P =
definition of an SC since the endpoints of each edge are {X σ} σ∈PX ofX intoso-calledcellsX σ ofX,suchthat
nodesinthegraph. 1) (Boundary-subset condition:)ForanypairofcellsX σ,X τ,we
We also need further notions, used in GNNs based on haveX τ ∩X σ ̸= ∅iffX τ ⊆ X σ.Thisconditionenforcesaposet
SCs.Forp∈Z ⩾0,p-node-setsinanHGHareallsetsofnodes structureofthesubspacesofaspace,i.e.,τ ≤σiffX τ ⊆X σ.
composedofp+1distinctnodes:Hp :={{v 0,...,v p}∈H: 2) (Homeomorphismcondition)EverycellishomeomorphictoRn
forsomen.
t c|{ o av ln l0 o, th. d. ee. s, m,v 1 ap - x} n i| o m= d ae lp -s p+ et1 ∈s} t. ZoIn >e 0dan g se uS s cC , h, 2t - th n he o ad t0 e- Hn -so e cd t ose n- ts to aet it ns r sic ao anr gr pe l -es nsp o.o dWn ed e
-
3) p φ(R h tie s os mt tr hi φ ect io ni fo ten a ricc olo ron s oed fdi tt hbi iao sln l b) i a: n lF lRo isr n aσev hte oor my X eσ oσ m∈ s ou rP c pX h hit st h mh ae tr oe t nhi tes oa r Xeh s σto r ,m ice to iom no or- f
set the dimension of H. Generalizing the notion of edge di-
rection in simple graphs, node-sets can have an orientation where X σ is the closure of a cell X σ, i.e., all points in X σ
o: H→{±1}.ForanarbitraryfixedorderV =(v 1,...,v n) togetherwithalllimitpointsofX σ.Ahomeomorphismfrom
o of rieth ne ten do id fe (s v, i0w ,e ..s .a ,y
v
ipa )n ao pd pe e- ase rst ( inv i0 a,. p. o. s, iv tii vp e) pis erp mos ui tt aiv tie ol ny ua og uiv se mn ap par tt hit ai ton trap n∈ sfoP rmto
s
pa g toiv Ren dpd (o am ndain itsR id np vi es rsa ec io sn at li sn o-
of (v 1,...,v n) and negatively oriented otherwise. Orien- continuous). For full details, see Mendelson’s Introduction
tation in SCs is incorporated into certain simplicial GNN toTopology[180].
methodsbychangingthesignofmessages[57],[117]. The cells in a CC form a multipartite structure where
n-cells are connected to (n + 1)-cells, which are in turn
5.2.1 LiftingPGstoSCs
connectedto(n+2)-cells,andsoon,seeFig. 1in[121]for
A commonly used technique to transform a PG into an SC
adepictionofthismultipartitestructureofaCC.Thisleads
istoformahyperedgefromeachcliqueinG.
toamultipartitestructurewhereverticesareonlyconnected
Example 5.4. For a simple graph G = (V,E) and a maximal to1-cellsdirectly.ThisisincontrasttotheSCwherevertices
clique size k, we define a clique complex (CqC) lifting to be are fully connected to all other cells. Different forms of
a transformation, in which we construct a simplicial complex adjacencyinCCs,HGs,andScs,areshowninFigure6.
H = (V,E) by setting E = {{v 0,...,v l} : l ∈ [k − As in SCs, a CC inherits the notion of isomorphism as
1],{v 0,...,v l}arefullyconnectedinG}. well as the generalized forms of adjacency between node-
sets,asspecifiedforHGs.
The resulting HG is indeed an SC because each hyperedge
A cell’s dimension is the dimension of the space it is
is defined by a fully connected subset of nodes in G. Thus,
homeomorphic to. We call cells of dimension p the p-cells
anysubcollectionofthesenodesisalsofullyconnected.
of the complex. The dimension of a CC is the maximum
Proposition5.5. CqCliftingpreservesisomorphisms. dimension of its constituents. Note that the dimension of a
Note that all forms of representing HGs with PGs (Sec-
cellc = {v i0,...,v ip}neednotberelatedtothenumberof
nodesitcomprises.Forexample,2-cellsassociatedwithcy-
tion5.1.2)directlyapplytoSCsaswell. clesoflengthk ⩾4havedimension2butcompriseknodes.
5.3 CellComplexes Using the nomenclature from before, p-node-sets need not
The combinatorical constraints of SCs, as described in the be p-cells. As a matter of fact, an SC is a CC for which every
previoussection,mayresultinaverylargenumberofcells p-node-sethasdimensionpforp∈Z ⩾0.
in order to represent different HO structures. Consider, for In contrast to SCs, CCs can encode p-dimensional sub-
example, the molecules portrayed in Figures 7 and 6. In structures containing more than p+1 vertices. Thus, they
ordertocapturethefulleffectofthemolecularrings(cycles) are more flexible and better suited for learning on certain
we would need to go well beyond the commonly used 2- datasets, for example, molecular datasets [56]. Similarly
simplices. Furthermore, the strict requirement that subsets to SCs, representations for CCs are learned through MP
of hyperedges necessarily form a hyperedge as well can be along boundary adjacency (see Definition 5.2) and related
difficult to satisfy in different complex systems. For exam- adjacencytypes[120].
ple,somepairsofsubstancesonlyreactinthepresenceofa Consider the HG H portrayed in Figure 7 with nodes
catalyst.Thisrelationshipcanbecapturedbyahypergraph markedinblue,edgesinblackandhigher-orderhyperedges
with a hyperedge connecting three substances but lacking inorangeandpurple,respectively.Notethatthehyperedges9
will focus on liftings that map cycles in a graph to 2-cells.
Wedistinguishbetweentwotypesofcycles.
Definition 5.7. A cycle in a graph G = (V,E) is a collection
(a) Hypergraphs and cells. Cell com-(b)Incidences.The1-cellchasincident of nodes v i0,...,v ip that are connected by a walk v i0 → v i1 →
p cole mx pC lexw Sit wh it1 h-c 2e -l cl elc ls(l (e rf it g) h, ts ).implicialn edo gd ee ss .,2-cellshasincidentnodesand ··· → v ip → v i0. An induced cycle is a cycle that does not
containanypropersub-cycles.
When lifting cycles to 2-cells, we will specify the max-
imum length of induced cycles k and regular cycles
ind-cycle
k toconsider.Sinceanyinducedcycleisalsoacycle,we
cycle
(c) Boundary adjacencies. Incident(d) Co-boundary adjacencies. The 1- willhavek ⩽k .
nodes form boundaries of c, incidentcell c has a cycle, the 2-cell s a 3- cycle ind-cyc
edgestheboundariesofs. dimensionalnode-setasco-boundary. Definition 5.8. Consider a graph G = (V,E), k , k ,
cl ind-cycle
k
cycle
∈ Z >0. We define a (k cl,k ind-cycle,k cycle)-cell lifting by
attaching a p-cell to every clique of size p + 1 ⩽ k , a 2-cell
cl
toanyinducedcycleoflengthatmostk anda2-celltoany
ind-cycle
(e)Upperadjacencies.Forc,theseare(f)Lower adjacencies.Forc,theseare non-inducedcycleoflengthatmostk cycle.
theedgeswiththecycleasco-boundary,edgeswhichsharenodesasboundaries,
forsthe2-cellswiththe3Dnode-setasfors,2-cellswhichshareedgeswiths Proposition 5.9. The cell lifting defined above is 1-skeleton-
co-boundary. asboundaries. preservingandisomorphism-preserving.
Fig.6:Adjacencynotionsrelatedtoboundary-adjacencyintwohypergraphs.As 5.4 GraphswithNode-TupleCollections
higher-ordergraphsweconsideracellcomplexC withnodesas0-cells,edges
as1-cellsanda2-dimensionalhyperedgecorrespondingtothecycle,anda3- Hypergraphs and their specialized variants, simplicial and
dimensionalsimplicialcomplexS.Subfigure6ahighlightsa1-cellc ∈ C,and
cell complexes, are restricted, because hyperedges are sub-
a2-cells ∈ S,respectively,ingreen.Subfigures6b-6edisplaytheadjacencies
ofcandsinorange(inSubfigure6dtheco-boundaryofsisthree-dimensional setsofnodesandarethusfundamentallyunordered.Moreover,
thereforecolouredbrowntodistinguishitfromtwo-dimensionfaces.) they cannot contain one node multiple times, i.e., they are
not multisets. However, as we will discuss in detail in Sec-
tion8,harnessingorderofnodesormultisetpropertiescan
are associated with cycles in the underlying graph. If we
enable distinguishing non-isomorphic graphs that would
consider the HG as a topological space, we observe that a
otherwisebeindistinguishablebyWLschemes.
node-settouchesanotherifandonlyifthereisanincidence
We now formally introduce graphs with node-tuple-
relation between them. Moreover, each node-set c is home-
collections (NT-Col-graphs) that address the above issues by
omorphictoRdc forsomed c (i.e.,thereisacontinuousmap usingorderednode-tuplesinsteadofhyperedgestoencode
that transforms c to Rdc, see [180]). These are two of the
higher-orderstructures.Whiletuplesofnodeshavealready
definingpropertiesofCCs.
been used in HOGNNs [187], we are the first to formally
defineNT-col-graphsasaseparatedatamodel,asapartof
ourblueprint,inordertofacilitatedevelopingHOGNNs.
Definition 5.10. A node-tuple-collection C = (V,E,T,x) is
agraphG=(V,E)togetherwithacollectionofnodetuplesT⊆
∪kmaxVk,possiblyendowedwithfeaturesx: V ∪E∪T→Rd.
k=2
We denote one k-tuple of nodes as v = (v i1,...,v ik) ∈
Vk. Note that one may use node-tuples of length two,
which possibly do not appear as edges in the underlying
plaingraphG(inparticular,ifthegraphisundirected).An
Fig.7:ExamplecellcomplexHwithfeatures.Nodescorrespondto0-cells,edges examplenode-tuple-collectionisdepictedinFigure8.
to1-cellsandinducedcyclesto2-cells.Theverticalarraysvisualisethefeatures
ofthehighlightedcells.
5.3.1 LiftingPGstoCCs
(v1,v1,v4) (v4,v4)
Next, we relate PGs to CCs via lifting transformations. For
(v1,v3) (v3,v1)
(v3,v4) v4
this,weextendthenotionofHGtomulti-HGHbyallowing
v1 v3
somehyperedgestooccurmultipletimes(possiblywithdif- (v1,v2,v3)
ferentfeatures).Wedefinethep-skeletonofaCCHtobeall
cells in H with dimension at most p, denoted H⩽p. We call
a lifting map f from PGs to CCs 1-skeleton-preserving if for
any graph G, the 1-skeleton of f(G) (identified as a multi-
graph composed of nodes V = {0-cells} and multiedges v2
E = {{1-cells}}) is isomorphic to G. In particular, any
liftingthatattachesmultiple1-cellstoapairofnodesisnot Fig.8:Exampleofanode-tuple-collectionC.Nodesintheunderlyinggraphare
depictedindarkblueandnode-tuplesasdirectededgesorpaths.
1-skeleton-preserving.Whiletheonly1-skeleton-preserving
liftingfromPGstoSCsistheCqClifting[56,page5],there One can extend the notion of adjacency in different
aremultiple1-skeleton-preservingliftingsofPGstoCCs.We ways to accommodate node-tuple collections. For example,10
Morris et al. [187] in their k-GNN architecture introduce
down adjacency between node-tuples that is similar to
lower-adjacencies:
Definition 5.11 (Downadjacency). ForanNT-col-graphC =
(G,T),adown-adjacencyisdefinedas
N ⇓(v)={w∈Vk :∃!j ∈{1,...,k}:w
j
̸=v j}.
However, since the number of down-adjacencies scales
exponentiallyink,theauthorsalsoproposeanMParchitec-
ture in which only messages from local down adjacencies
Fig.9:Exampleofasubgraph-collectionC withsubgraphsinred,yellowand
areprocessed.Formally,wehave
blue.Theedgesintheunderlyinggrapharedisplayedbycontinuouslinesand
thevirtualedgeswhichappearinsubgraphsbutnotintheunderlyinggraph,by
Definition5.12(Localdownadjacency). ForanNT-col-graph
dottedlines.
C =(G,T),wedefinealocaldown-adjacencytobe
N loc,⇓(v)={u∈Vk :∃!j :v
j
̸=u j,{v j,u j}∈E}.
5.6 Motif-Graphs&SCnt-Graphs
5.5 GraphswithSubgraph[-Tuple]Collections In PGs, links are edges between two vertices. In HGs and
Next, we formally introduce a GDM implicitly used by their specialized variants (SCs, CCs), HO is introduced by
several neural architectures where the “first-class citizens” making edges being able to link more than two vertices, as
are arbitrary subgraphs. In these schemes, plain graphs are well as extending adjacency notions to links. In NT-Col-
enrichedwithacollectionofsubgraphsselectedwithcertain graphs,SCol-graphs,andST-Col-graphs,oneintroducesHO
criteria specific to a given architecture. These subgraphs by distinguishing collections of substructures in addition to
are not necessarily induced, and may even introduce new harnessing edges between vertices. Now, we describe another
“virtual” edges not present in the original graph dataset. wayofintroducingHOintoGDMs:definingadjacenciesusing
Such architectures capture substructures of an input graph substructuresconsistingofdyadicinteractions.Thesesubstruc-
to achieve expressivity beyond the 1-WL-test [256]. Some turesarecalledmotifs[25],[32],forexampletriangles[225].
achieve this by learning subgraph representations [51], Thekeyformalnotionshereareamotifandtheunderlying
[148],[193],[204],[209],[230],[275],[282],whichtheysubse- conceptsofagraphletandorbit.
quently transform to graph representations. Others predict
Definition 5.14. Let G = (V,E) be a graph. A graphlet in
the properties of subgraphs themselves. For example, in
G is a subgraph H = (V H,E H) such that V H ⊂ V,E H ⊂
medical diagnostics, given a collection of phenotypes from {1..(cid:0)VH(cid:1)
}∩E.Agraphletautomorphismisapermutationofthe
ararediseasedatabase,thetaskistopredictthecategoryof 2
nodes in V H that preserves edge relations. A set of nodes V H in
diseasethatbestfitsthisphenotype[5].
a graphlet H define an orbit if the action of the automorphism
Inspired by these different use cases, we introduce a
group is transitive, that is, for any u,v ∈ V H there exists an
GDMcalledthegraphwithasubgraph-collection(SCol-graph).
automorphismφonV
H
suchthatφ(u) = v.Finally,amotifis
Definition 5.13. Agraphwithasubgraph-collection(SCol- identifiedwiththeisomorphismequivalenceclassofagraphletor
graph) is a tuple C = (V,E,S,x) comprising a simple graph anorbit.
G = (V,E), a collection of (possibly non-induced) subgraphs S
We will refer to a GDM that is based on “motif-driven”
withV
H
⊆ V,E
H
⊆ V
H
×V
H
foreveryH = (V H,E H) ∈ S,
adjacencies as Motif-Graphs. We distinguish two classes of
andfeaturesx: V ∪E∪S →Rd.
suchGDMs.
To the best of our knowledge, notions of incidence and In one class, one uses motifs to define “motif induced”
boundary-adjacencyasinDef.5.2havenotbeendefinedfor neighborhoods, in which - intuitively - the neighbors of a
SCol-graphs. Instead, the adjacency is usually very specific givenvertexaredeterminedbasedonwhethertheysharean
toagivenscheme,wewillrefercollectivelytoitasthesub- edgethatisincludedinagivenselectedmotif.Forexample,
graph adjacency. Some architectures associate subgraphs in HONE [206], one first chooses a collection of motifs S
with single nodes, subsequently defining subgraph adja- andbuildsanewedge-weightedgraphW M foreverymotif
cencyviaadjacencyofthecorrespondingnodes[209],[275]. M ∈ S. Given an input graph G, the weight of an edge
Another approach is to measure the number of nodes or e ∈ W M is the number of subgraphs of G which contain
edgesthatsubgraphsshareanddefineadjacenciesbetween e and are isomorphic to M. The motif-weighted graphs
them based on these overlaps [230]. Others use even more W M for M ∈ S can then be used to learn node or graph
complex notions of adjacency to construct MP channels representations[157],[206].Thesameformsofmotif-based
betweensubgraphsortheirconnectedcomponents[5].Fig- adjacency are used to design MotifNet, a GNN model that
ure9showsanexampleofasubgraph-collection. seamlessly works with directed graphs [183]. This simple
Similarly to NT-Col-graphs, one can also impose an notionofmotif-basedadjacencycanalsohavemorecomplex
ordering of subgraphs in SCol-graphs. This has been pro- forms. For example, HONE also proposes a “higher-order
posed in a recent work by Qian et al. [197]. We refer to the form”ofthismotifadjacency:amoreabstractconceptwhere
resultingunderlyingGDMasgraphwithasubgraph-tuple adjacencyisnotjustaboutdirectnode-to-nodeconnections
collection (ST-Col-graph). Analogously to NT-Col-Graphs, but involves the role of nodes across different instances of
one can then extend the notion of down adjacency into the thesamemotiforacrossdifferentmotifs.Forexample,two
realmofST-Col-graphs. nodes might be considered adjacent in this higher-order11
sense if they frequently participate in similar motifs, even betweenthemaredefinedbyincidence;theseedgesareused
iftheyarenotdirectlyconnected. aschannelsinMP.
In another class of Motif-Graphs, the key idea is to GivenanHGwithfeaturesH=(V,E,x),duringoneMP
extend the features of vertices with information on how step,nodeandhyperedgefeaturesareupdatedasfollows:
manyselectedmotifseachvertexbelongsto.Thisideahasbeen
 
exploredfortrianglesandformoregeneralsubgraphs,and
(cid:75)
it has been shown to improve a GRL method’s ability to xn eew=φE xe, ψV (xu)  , (2)
distinguishnon-isomorphicgraphs[59].
u∈V:u⊆em(cid:124) essag(cid:123) e(cid:122) u→(cid:125)
e
 
5.7 NestedGDMs
(cid:79)
Finally,someHOGDMsincorporatenesting.InsuchaGDM,
me=ψE xe, u∈V:u⊆emψ
(cid:124)
esV sag(cid:123)( e(cid:122)x uu →(cid:125)) e , (3)
adistinguishedpartofagraph(orahypergraph)ismodeled
 
as a “higher-order vertex” with some inner structure that (cid:77)
forms a graph itself. This GDM has been implicitly used
xn vew=φV  xv, mf  , (4)
f∈E:v⊆f (cid:124)(cid:123)(cid:122)(cid:125)
bydifferentGNNarchitectures,includingGraphofGraphs messagef→v
Neural Network [240], Graph-in-Graph Network [162], hi- for v ∈ V, e ∈ E, learnable functions φ V,φ E,ψ V,ψ E,
erarchical graphs [159], and Neural Message Passing with and possibly distinct permutation-invariant aggregators
Recursive Hypergraphs [257]. The details of such a formu- (cid:74) ,(cid:78) ,(cid:76) .Thus,messagesfirsttravelfromnodestoincident
lation (e.g., whether or not to connect such higher-order hyperedges, and these hyperedges aggregate the messages
vertices explicitly to plain vertices) depends on a specific toobtainnewfeaturesxnewforthemselves,using(cid:74) (Eq(2)).
e
architecture.AmotivatingexampleusecaseforsuchaGDM Next, each hyperedge builds a message (Eq. (3)), possibly
isanetworkofinteractingproteins:whileanindividualpro- using a different aggregation scheme (cid:78) . This message is
tein has a graph structure, one can also model interactions then sent to all incident nodes, and these messages can be
betweendifferentproteinsasagraph. further aggregated using a yet another scheme (cid:76) . In con-
trasttoMPonplaingraphs,thisarchitectureperformstwo
6 HIGHER-ORDER GNN ARCHITECTURES
levelsofnestedaggregationsperMPstep,sincecomputing
We now analyze HOGNN architectures that harness differ- xn vew requiresknowingme.
entHOGDMs.Mostsucharchitectureshaveastructuresim- TherearenumerousIMParchitecturesthatharnessHGs,
ilartotheMPframework(seeSec.2.2),exceptthat,instead for example Hypergraph Convolution [15], Hypergraph
ofonlyupdatingthefeaturesofnodes,theyalsoupdatethe Neural Networks [97], Hypergraph Attention [15], Hyper-
featuresofhigher-orderstructures.Forexample,asubgraph GCN [258], Hypergraph Networks with Hyperedge Neu-
orahyperdgecanhavetheirownfeaturevectors. rons [91], Hyper-SAGNN [277], and others [10], [13], [102],
We first discuss HOGNN architectures that – as their [103],[171],[223],[243],[273].
underlying GDM – harness HGs (Section 6.1), SCs (Sec- 6.2 NeuralMPonSimplicialComplexes(SCs)
tion 6.2), CCs (Section 6.3), NT-Col-Graphs (Section 6.4),
AprimarymodeofwiringinHOGNNmodelsbasedonSCs
SCol-Graphs and ST-Col-Graphs (Section 6.5), and Motif-
isthroughboundaryadjacenciesandrelatedneighborhood
Graphs as well as SCnt-Graphs (Section 6.6). We then dis-
notions.Inthismode,aavertexorahyperedgecanreceive
cuss other orthogonal aspects of HOGNNs, namely MP
messages from its boundary-, co-boundary-, upper-, and
flavors (Section 6.7.1), local vs. global formulations (Sec-
lower adjacencies, or a subset thereof. We describe the MP
tion 6.7.2), multi-hop wiring (Section 6.7.3), and nesting
architecturefirstintroducedbyBodnaretal.[57].
(Section6.7.4).
LetH = (V,E,x)beanSC.Duringonestepofmessage
Formoreinsightfuldiscussions,inthefollowingsection
passing,nodeandsimplexfeaturesareupdatedasfollows:
we also present in more detail representative HOGNN ar-
chit Te ac btu lere 2s cf oro mm pad ri eff se sr ee ln et cc tela dss re es pro ef ss ec nh tae tm ive es.
HOGNNmod-
mB
c
= (cid:77) ψB(xc,xb), mC
c
= (cid:77) ψC(xc,xb),
b∈B(c) b∈C(c)
e rels s. pW ece tivil elu Mst Prat be luh eo pw rinth tse .s We em so ed le el cs
t
a or ne ee mxp or de es lse fd roi mn t eh ae ci hr mN c↑ = (cid:77) ψ ↑(xc,xd,xδ),
MP flavor (convolutional, attentional, general) and from a
d∈N↑(c),δ∈C(c)∩C(d)
correspondingGDM. mN c↓ = (cid:77) ψ ↓(xc,xd,xδ),
d∈N↓(c),δ∈B(c)∩B(d)
6.1 NeuralMPonHypergraphs(HGs) xn cew=φ(cid:16) xc,mB c,mC c,mN c↑,mN c↓(cid:17) ,
InHOGNNmodelsbasedonHGs,thedominantapproach
for wiring is to define message passing channels by in- for learnable maps φ,ψ B,ψ C,ψ ↑,ψ ↓ and potentially dif-
(cid:76)
cidence; we call such models incidence based message ferent permutation-invariant aggregators . We call mod-
passing models or IMP for short. Here, information flows els using this set of update equations boundary-adjacency
between nodes and hyperedges. This architecture appears basedmessagepassingmodelsorBAMPforshort.
inmultipleHGneuralnetworkmodels[15],[97],[131]and In BAMP, compared to IMP, messages may have a
wasdescribedinageneralformbyHeydariandLivi[131]. smaller reach. For example, two edges that neither share
Note that an alternative perspective on such models is that nodes nor co-boundaries will not exchange messages in
one can see an HG as a plain bipartite graph, as there are boundary-adjacencymessage-passing(BAMP),while,inin-
two node sets: HG nodes and HG hyperedges, and edges cidencemessagepassing(IMP),theywilliftheyareincident12
HoGDM Wiring Flavour Obj Exampleupdateequations Eq.ref Examples
v∈V xn vew=φV (cid:16) xv,(cid:76) f∈E:v⊆fmf(cid:17) .
hypergraph IMP gen me=φE(cid:16) xe,(cid:78) u∈V:u⊆eψV (xu)(cid:17) . [131]
e∈E xn eew=φE(cid:16) xe,(cid:74) u∈V:u⊆eψV (xu)(cid:17) .
v∈V xn vew= √ de1 g(u)|f∈E1 :v⊆f|(cid:80) f∈E:v⊆fa(xv,xf)mfΘ.
hypergraph IMP att me=ωe|u∈V1 :u⊆e|(cid:80)
u∈V:u⊆e
√ de1 g(u)a(xe,xv)xu. [15]
a(xv,xf)= (cid:80) e∈Ee :x vp ⊆( eσ e( xsi pm (σ(x (v siΘ m,x (f xvΘ Θ) ,) x) eΘ))).
hypergraph IMP conv
v∈V xn vew= √ de1 g(u)|f∈E1 :v⊆f|(cid:80) f∈E:v⊆fmfΘ.
[97] [15],[131]
me=ωe|u∈V1 :u⊆e|(cid:80)
u∈V:u⊆e
√ de1 g(u)xu.
c∈H xn cew=φ(cid:16) xc,mB c,mN c↑(cid:17)
cellcomplex BAMP gen mB
c
=(cid:76) b∈B(c)ψB(xc,xb) [56] [120]
mN c↑ =(cid:76) d∈N↑(c),δ∈C(c)∩C(d)ψ ↑(xc,xd,xδ)
e∈H1 xn eew=αp(x˜e)·x˜e
x˜e=φ(cid:16) xe,mN c↑,mN c↓(cid:17)
cellcomplex BAMP att [116]
mN c↑ =(cid:76) f∈N↑α ↑(xe,x f)ψ ↑(xe)
mN c↓ =(cid:76) f∈N↓α ↓(xe,x f)ψ ↓(xe)
c∈H xn
cew=σ(cid:16)(cid:80)
d∈N↑(c)β cdx
dΘ(cid:17)
cellcomplex BAMP conv β =1 + |C(c)∩C(d)| [120] [56]
cd {c=d} (cid:113) degN↑(c)·degN↑(d)
degN↑(c)=(cid:80) |C(c)∩C(d)|
d∈H
c∈H xn cew=φ(cid:16) xc,mB c,mC c,mN c↑,mN c↓(cid:17)
mB
c
=(cid:76) d∈B(c)ψB(xc,xd)
simplicialcomplex BAMP gen mC
c
=(cid:76) d∈C(c)ψC(xc,xd) [57]
mN c↑ =(cid:76) d∈N↑(c),δ∈C(c)∩C(d)ψ ↑(xc,xd,xδ)
mN c↓ =(cid:76) d∈N↓(c),δ∈C(c)∩C(d)ψ ↓(xc,xd,xδ)
simplicialcomplex BAMP att
c∈H xn cew=φ(cid:16)(cid:80) d∈N↑(c)αU c,dΘ Uxd,(cid:80) d∈N↓(c)αL c,dΘ Lxd(cid:17)
[117]
αN = exp(σ(sim(xcΘN,xdΘN)))
c,d (cid:80) e∈Nexp(σ(sim(xcΘN,xdΘN)))
simplicialcomplex BAMP conv
c∈H xn cew=σ(cid:16)(cid:80) d∈N↑(c)Θ Uxd+(cid:80) d∈N↓(c)Θ Lxd+Θ Sxc(cid:17)
[262] [69],[92]
Otherformulationsarecomplex
andomittedduetospaceconstraints
node-tuple-c DAMP gen v∈T Sofarunexplored -
node-tuple-c DAMP att v∈T Sofarunexplored -
node-tuple-c DAMP conv v∈T xn vew=σ(cid:16) Θ1xv+(cid:80) u∈N↓(v)Θ2xu(cid:17) [187] [186]
TABLE2:SelectionofHOGNNswhichcanbeexpressedasMParchitectures.Welisttheusedhigher-ordergraphdatamodel(HoGDM),themessagepassingwiring
scheme(Wiring),themessagepassingflavour(Flavour),whichobjectshavetheirfeaturesupdatedduringmessagepassing(Obj),andtheupdateequation.Inthe
updateequations,(cid:76),(cid:78),(cid:74) refertopermutation-invariantaggregators,Θarelearnablematrices,ψ,φarelearnablefunctions,simisareal-valuedsimilaritymeasure
fortwovectorsofequalsize,forexample,aninnerproduct.13
to a common hyperedge. Let us consider the effect that a Definition6.1(Down-adjacencymessagepassingfornode–
nodeuhasonthefeatureupdateofnodevinthetwoarchi- tuplecollections). Givenanode-tuple-collectionC =(G,T,x),
tectures. InIMP,ufirstsendsitsmessagetoallhyperedges and a neighbourhood notion given by down- or local down-
itisincidenton.Theseaggregatethemessagestheyreceive adjacency, N ∈ {N ↓,N loc,↓}, we update the features of node-
fromtheirincidentnodes.Thenallhyperedgescontainingv tuplesinonemessagepassingstepbyaggregatingmessagesfrom
send their message to v. Thus, the contribution of u to the anode-tuple’sneighbours
feature update of v depends on how many co-incidences u  
and v share, and to what extent the feature of u is retained (cid:77)
xnew =φx , ψ(x ,x ).
aftertheaggregations.InBAMP,uandvexchangemessages v  v v u 
(cid:124) (cid:123)(cid:122) (cid:125)
if and only if they share an edge. The message from u to v u∈N(v) messageu→v
containsthefeaturesofu,v,andthefeatureoftheirshared Down-adjacency message passing (DAMP) is similar
edge e. These features are then transformed by ψ ↑. Finally, to BAMP when using only lower adjacency. However, in
themessagesfromallupperadjacenciesofvareaggregated, DAMP N ↓,N loc,↓ refer to down and local down adjacency
forming the collective message mN c↑, which is fed into the respectively (see Definition 5.12), while in BAMP N ↓ de-
final update. There are also variants of BAMP which use notesloweradjacency(seeDefinition5.2).
nestedaggregations[120]. 6.5 NeuralMPonSCol-Graphs&ST-Col-Graphs
There are numerous examples of MP based on SCs; ex-
Finally, we turn to architectures for subgraph-collections.
amplemodelsincludeMessagepassingsimplicialnetworks
Thereisalargebodyofresearchonthistopicandnumerous
(MPSNs) [57], Simplicial 2-complex convolutional neural
models have been proposed [5], [51], [60], [86], [100], [193],
networks(S2CNNs)[69],Simplicialattentionnetworks[117]
[209], [230], [275], [282]. In general, the architectures based
(SATs),orSCCNN[261]andtheirspecializedvariant[262].
onsubgraph-collectionsusuallyproceedasfollows:
TheseareallBAMP-basedmodels.
1) Constructacollectionofsubgraphsfromaninputgraph.
6.3 NeuralMPonCellComplexes(CCs) 2) Specifythewiringschemeforsubgraphs.
Neural MP on CCs uses, as the primary adjacency scheme, 3) Learn representations for the subgraphs, harnessing the
the BAMP adjacency. However, unlike in SCs, it usually specifiedwiringpattern.
uses a restricted version of BAMP. For example, CW Net- 4) Aggregate subgraph representations into a graph repre-
works [56] employ MP on cell complexes by sending mes- sentation.
sagesfromboundary-adjacentandupper-adjacentcells.The We identify three classes of methods for constructing
representationofacellcisupdatedaccordingto subgraph collections and learning their representations.
Based on this, we further classify these HOGNN architec-
m( Bl+1)(c)= (cid:77) ψ B(cid:16) H( cl),H( bl)(cid:17) (5) t gu er ne es rain lto sut bh gre re apt hyp wes i: rir ne gcon apst pru roc ati co hn e- sb .a Is ned r, ece og no s- tn re ut c, tia on nd
-
b∈B(c) basedschemes,oneremovesnodesoredgesandlearnsrep-
m( Nl+ ↑1)(c)= (cid:77) ψ ↑(cid:16) H( cl),H( dl),H( δl)(cid:17) (6) resentations for the resulting induced subgraphs [51], [86],
[193], [204]. In ego-net schemes, subgraphs are constructed
d∈N↑(c),δ∈C(c)∩C(d)
(cid:16) (cid:17) by selecting vertices according to some scheme, and then
H(l+1) =φ H(l),m(l+1)(c),m(l+1)(c) (7)
c c B N↑ buildingsubgraphsusingmulti-hopneighborhoodsofthese
vertices[51],[209],[275],[282].Ingeneralsubgraphwiring,
forlearnablefunctionsφ,ψ B,ψ ↑ andpermutation-invariant
(cid:76) theapproachistopickspecificsubgraphsofinterest[5],for
aggregators .AnembeddingontheCCKcanbeobtained
example subgraphs that are isomorphic to chosen template
bypoolingmultisetsofcellrepresentations
graphs[230].
(cid:16) (cid:17)
HK = POOL {{H( cL)}} dim=j :j =0,...,dim max . The subgraph wiring scheme usually simply follows
thegeneralsubgraphadjacency,andisarchitecture-specific.
Interestingly, this model considers only boundary and up- Some schemes do not consider adjacency between sub-
per adjacency, while omitting co-boundary and lower ad- graphsatallandinsteadsimplypoolsubgraphrepresenta-
jacency. The authors show that this does not harm the tionsintoagraphrepresentation[86],[193],therebytreating
expressivityofthemodelintermsofitsabilitytodistinguish themasbagsofsubgraphssimilartoDeepSets[269].
non-isomorphicCCs. Finally, Qian et al. [197] have proposed a scheme that
There are other models for MP on CCs, including Cell imposes an ordering between subgraphs, in addition to its
complex neural networks (CCNNs) [120] or Cell Attention subgraphadjacencyusedforwiring.
Networks(CANs)[116]. 6.6 NeuralMPonMotif-Graphs&SCnt-Graphs
6.4 NeuralMPonNT-Col-Graphs Neural MP implemented in HOGNNs based on Motif-
Graphs follows the motif-adjacency defined in the spe-
Node-tuples act as a tool for capturing information across
cific work. Interestingly, it is usually defined and imple-
different parts of the graph beyond that available when
mented using global formulations [157], [205]. As such,
using plain graphs; they have been used to strengthen the
theseHOGNNsdonotexplicitlyharnessmessagesbetween
expressive power of GNNs. The proposed wiring pattern
motifs. Instead, they implicitly provide this mechanism in
by Morris et al. [186], [187], called the Down-Adjacency
their matrix-based formulations for constructing embed-
Message Passing (DAMP), is based on down adjacencies
dings.Instead,modelsthatarebasedonSCnt-Graphsoften
(seeSection5.4).
explicitlyusewiringbasedontheinputgraphstructure[59].14
6.7 Discussion theotherdoesnot.Thereexistseveralsucharchitectures[1],
[63],[96],[101],[165],[206],[261].
Webrieflydiscussdifferentaspectsrelatedtoalltheabove-
summarizedHOGNNs. Manymulti-hoparchitecturesarebasedonPGs,andthe
higher order in these architectures solely involves harness-
6.7.1 WiringFlavors
ingmulti-hopneighborhoods.ThisincludesMixHop[1]and
Thechoiceofwiringflavorisimportantforperformance(la- SIGN[101].Somemulti-hopbasedarchitecturesarerelated
tency, throughput), expressiveness, and overall robustness. to GDMs beyond PGs, for example SCCNN [261], which
The convolutional flavor is simplest to implement and use, combinesMixHopwithSCs.
and it is usually advantageous for high performance, as
Interestingly, multi-hop architectures are often formu-
one can precompute the local formulation coefficients for
lated using the global approach. This is because there is a
a given input graph. However, it was shown to have less
correspondence between obtaining the information about
competitiveexpressivenessthanotherflavors. Ontheother k-hop neighbors and computing the k-th power of the
hand, attentional and general MP flavors are usually more
adjacency matrix. This has been harnessed in, for example,
complicated and harder to implement efficiently, because
SIGN[101].
any of the functional building blocks (ψ, φ, ⊕) can return
6.7.4 Nesting
scalarsorvectorsthatmustbelearnt.
Thereexistseveralmodelsthatusenestingexplicitly.Mod-
We observe that all three wiring flavors (convolutional,
els from this category can be referred to with differ-
attentional, and general MP) are widely used in the neu-
ent words (besides “nested”), for example “hierarchical”
ral architectures based on a hypergraph or its variants.
or “recursive”. These models mostly harness PGs as the
Specifically,therehavebeenexplicitHOGNNswithallthese
underlying GDM. This includes Hierarchical GNNs [74],
flavorsforHOGNNsbasedongeneralHGs,SCs,andCCs;
GoGNNs [240], and others [99], [162], [257]. However, one
see Table 2. However, HOGNNs based on other GDMs, do
architecture introduces the concept of Recursive Hyper-
not consider all these flavors. For example, architectures
graphs [169]. The wiring in these models usually follows
basedonNT-Col-Graphshaveonlyconsideredtheconvolu-
a multi-level approach, where messages are consecutively
tional flavor so far. Exploring these flavors for such model
exchangedamongverticesatdifferentlevelsofnesting.
categorieswouldbeaninterestingdirectionforfuturework.
6.7.2 Localvs.GlobalFormulations
7 EXAMPLE HOGNN ARCHITECTURES
Both types of formulations come with tradeoffs. Global We also provide more details about selected representative
formulations can harness methods from different domains HOGNNarchitectures.
(e.g., linear algebra and matrix computations), for exam- 7.1 HOGNNsonHypergraphs
ple communication avoidance. They may also be easier
Hypergraph Convolution [15] is the first convolutional
to vectorize, because they deal with whole feature and
hypergraph architecture, designed for learning node repre-
adjacencymatrices(insteadofindividualvectorsasinlocal
sentationsbasedoncommonhyperedges.
formulations). On the other hand, local formulations can
In hypergraph neural networks [97], node features are
be programmed more effectively because one focuses on a
updated according to the following hyperedge convolu-
“local” view from a single vertex, which is often easier to
tionaloperation:
grasp. Moreover, such formulations may also be easier to
schedulemoreflexiblyonlow-endcomputeresourcessuch
(cid:16) (cid:17)
asserverlessfunctionsbecausefunctionsinquestionoperate H(l+1) =σ D−1/2BWD−1BTD−1/2 H(l)Θ(l) (8)
V V E V V
onsinglevertices/edgesinsteadofwholematrices.
Many of the considered models across all the GDMs where
are formulated using the local approach. This is especially • an HG H = (V,E) has an incidence matrix B and hyper-
visible with models based on HGs, SCs, CCs and NT-Col- edgeweightsw(e)storedinadiagonalmatrixW ∈Rm×m
Graphs, as illustrated in Table 2. These models directly fol- withW
jj
=w(e j)forj =1,...,m;
lowtheIMP(Section6.1),BAMP(Section6.2),andDAMP- • D V ∈ Rn×n denote the diagonal node degree matrix
based (Section 6.4) wiring patterns, which are intrinsically with (D V) ii = (cid:80)m j=1w(e j)B ij = (BW) i,: equal to the
locally formulated. However, some models use a mixture weightedsumofhyperedgesthenodev i isincidenton,
of the local and global formulations, for example, S2CNNs • D E ∈Rm×misthediagonalhyperedgedegreematrixwith
or CCNNs. Finally, few models are globally formulated. (D E) jj =(cid:80)n i=1B ij countingthenumberofnodesthatare
TwonotableexceptionsaretheHypergraphConvolutionor incidentonhyperedgee
j
∈E,
HONE. • Θ(l) isalearnablematrix,
6.7.3 Multi-HopChannels • The convolution operator D V−1/2BWD E−1BTD V−1/2 is a
Multi-hop channels are often referred to as channels intro- normalised version of the hyperedge relatedness BBT
ducing “higher-order neighborhoods” [1], [101]. For exam- whichinentry(BBT) ij countsthenumberofhyperedges
ple, in MixHop [1] messages are passed multiple hops in apairofnodesv i,v j aresimultaneouslyincidenton.
everymessagepassingstep.Thisway,anodecanseenodes Applyingthismodeltounweightedgraphs,onerecovers
beyond its immediate neighbourhood. Recalling Figure 1, graph convolutional networks (GCNs) [150]. The diagonal
if every node saw its 2-hop neighbourhood in every step, edge degree matrix simplifies to D E = 2I, W = I, more-
thiswouldsufficetodistinguishthetwographs:onegraph over,wehaveBBT =A+D V.ThusEquation8transforms
hasa2-hopneighbourhoodwhichcontainsatriangle,while to15
1)-simplices, lower and upper-adjacent p-simplices and co-
H(l+1)
=σ(cid:16)
D−1/2BWD−1BTD−1/2
H(l)Θ(l)(cid:17) boundaryadjacent(p+1)-simplicesrespectively.
V V E V V In the simplicial network methods considered above,
(cid:16) (cid:17)
=σ 21(I+(D V)−1/2A(D V)−1/2)H( Vl)Θ(l) adjacencies between simplices and the resulting feature
representation updates are tied to the structure of the SC,
which corresponds to the GCN node feature update up to given by functions of the boundary operator. Simplicial
normalization[150]. attentionnetworks[117](SATs)generalisesGATs[237]and
HypergraphAttention(HAT)[15]buildsonthisconvo- allow these adjacencies to be learned. For two p-simplices
lutionalarchitectureandfurtherproposesanattentionmod- c,d in an SC K, their relative orientation o c,d ∈ {±1} is
uleforlearningaweightedincidencematrix.Theattentional 1 if they have equal orientation and −1 otherwise. SATs
moduleinlayerliscomputedaccordingto considerattentioncoefficientsα↑ ,α↓ forupperandlower
c,d c,d
adjacenciestoupdatesimplexfeatures
B ia jtt,(l) = (cid:80) k:{vk,e vx i}p ⊆(cid:16) eσ je(cid:16) xs pim (cid:0) σ(x (cid:0)v si iΘ m( (l) (, xx ve kj ΘΘ (( ll )) ,) x(cid:17) e(cid:17) jΘ(l))(cid:1)(cid:1) H( cl+1) =φ  d∈(cid:88) N↑(c)α c↑ ,, d(l)Θ( Ul) H( dl), d∈(cid:88) N↓(c)α c↓ ,, d(l)Θ( Ll) H( dl) 
forasimilarityfunctionsim(l) oftwovectorsandlearn- for an update function φ which aggregates its two inputs
able parameter matrices Θ(l). This similarity function can and performs further computations, for example given by
begivenby,forexample,aninnerproductwithalearnable an MLP. The attention coefficients are computed layerwise
vectora(l) ∈R2dl,i.e.,sim(l)(x,y)=(a(l))T(x∥y).
andupdatedaccordingto
Notethatitisessentialforhyperedgesandnodestohavea
representation in the same domain, so their inner product
α c↑ ,, d(l) =o c,d·softmax(ATT(Θ( Ul) H( cl),Θ( Ul) H( dl)))
can be computed (more specifically, since the similarity α c↓ ,, d(l) =o c,d·softmax(ATT(Θ( Ll) H( cl),Θ( Ll) H( dl)))
function takes two elements from the same domain as
input). where ATT is an attention function, for example an inner
Althoughcloselyrelated,HATdoesnotdirectlygeneral- product. Note that the learnable matrices
Θ(l),Θ(l)
used to
U L
izeGAT[237].WhileHATusesanattentionmoduletolearn computetheattentioncoefficientscoincidewiththoseinthe
theincidencematrix,GATusesittolearnadjacencybetween featureupdates.
nodes. However, an HG H = (V,E) can be expressed SCCNN [261] and their specialized variant [262] are
as a bipartite graph G H = (V GH,E GH) with node sets also BAMP - they use Hodge Laplacians as the basis of
V GH = V ⊔E and {v i,e j} ∈ E GH if v i ⊆ e j. Hypergraph theirsimplicialconvolutions(thereforeboundary-adjacency
attention then corresponds to GAT on the bipartite graph insimplicialcomplexes)andcombinethiswithMixHop.
G H. 7.3 HOGNNsonCellComplexes
7.2 HOGNNsonSimplicialComplexes Cellcomplexneuralnetworks(CCNNs)[120]aredesigned
Message passing simplicial networks (MPSNs) [57]intro- similarly to CW Networks [56], but differ in two main as-
duce the MP framework for SCs. Here, messages are sent pects.Firstly,theyrestrictboundary-adjacencytocellswith
from upper and boundary adjacencies and, in addition, contiguousdimensions.Thisalsotransferstothederivative
from from lower and co-boundary adjacencies. The MP notions of adjacency, such that only cells of equal dimen-
communicationchannelsusedareBAMPonly. sion can be lower or upper-adjacent. Secondly, in CCNNs
Simplicial 2-complex convolutional neural networks messages are only sent from only upper-adjacent cells. In
(S2CNNs) [69] focus on SCs of dimension at most 2. In particular, the highest-dimensional cells do not receive any
addition to upper and lower adjacencies, boundary adja- messages are their representations are not updated. Sim-
cencies are considered for the feature updates. They addi- ilarly to hypergraph message passing networks [131] the
tionally normalise the adjacency and boundary-adjacency aggregationisnested.Featureupdatesforacellcaregiven
matrices [212]. For better readibility we omit the details of by:
thenormalisationandcollectivelydenotethenormalisation
functionsbyΦ(·).Thefeatureupdatesarethengivenby  
(cid:16)(cid:104) (cid:105)(cid:17)   
H H( K
(
Kl l+ +0 11 1)
)
= =σ σ(cid:16)(cid:104)Φ Φ( (B B1 1TB )H1T
(
K) lH
)
0Θ( Kl) 0
( 0l
,Θ
)
1( 0l ,) 0 ∥Φ(B 1)H( Kl) 1Θ( 1l ,) 0 , H( cl+1) =φ   


H( cl), d(cid:77) ∈(1 N) ↑ψ 1  H c(l),H( dl), e∈C((cid:77) c(2 )∩) C(d)(cid:124)ψ 2 M(cid:16)
(cid:123)
eH →(cid:122)( e dl)(cid:17) (cid:125)     



∥Φ(B 1TB 1,B 2B 2T)H( Kl) 1Θ( 1l ,)
1
(cid:124) M(cid:123) d(cid:122)
→c
(cid:125)
(cid:105)(cid:17)
∥Φ(B 2)H( Kl) 2Θ( 2l ,)
1
, invafo rir anl te aa grn ga reb gle atof ru sn (cid:76)cti (o 1n ),s (cid:76)φ (2, )ψ .1 T, hψ e2 aua tn hd orsp ae lsr om pu rt oat pio on se-
(cid:16)(cid:104) (cid:105)(cid:17)
H( Kl+ 21) =σ Φ(B 2T)H( Kl) 1Θ( 1l ,)
2
∥Φ(B 2TB 2)H( Kl) 2Θ( 2l ,)
2
a convolutional architecture based on a cellular adjacency
matrix. Letting nˆ = |K − K dim(K)| denote the number of
for learnable parameter matrices Θ(l) in every layer l. This cells in a CC K omitting cells of the highest dimension,
i,j
framework can be generalised to any order p of simplices, the cell adjacency matrix AK ∈ Znˆ×nˆ tracks the number
byconsideringfunctionsofB pT,B pTB p,B p+1B pT +1andB p+1 of common coboundaries of two cells. For i,j ∈ [1 : nˆ],
for computing updates based on boundary adjacent (p − AK
ij
= |C(c i) ∩ C(c j)|. The diagonal cell degree matrix16
is defined by DK = (cid:80)nˆ AK for i = 1,...,nˆ. Letting buildagraphrepresentation.Anotherwaytoseethisisthat
ii j=1 ij
AˆK = I nˆ + (DK)−1/2AK(DK)−1/2, the convolutional cell the GNN performs message passing between subgraphs,
complexnetwork(CCXN)updatereads wherethemessagepassingchannelsbetweenthesubgraphs
(cid:16) (cid:17) are given by the connectivity of the nodes they are centred
H( Kl+1) =σ AˆKH( Kl+1)Θ(l) at.Furtherarchitectures[199],[282]withasimilarapproach
havebeenproposed.
foralearnableparametermatrixΘ(l),resemblingtheupdate
The Ego-GNN [209] model learns node representations
inGCN[150]. basedonmessage-passingwithin1-ego-nets.Conceptually,
7.4 HOGNNsonNT-Col-Graphs node features are updated in two steps. First, a predefined
Two special cases of the neural architectures based on number p of message-passing steps are performed within
DAMP and local DAMP are the k-GNN and the local k- each 1-ego-net. We refer to this learnable function as the
GNN architecture [187]. Here, a convolution-type archi- ego-net encoder and denote the representation of nodes in
tecture is proven to have the same expressive power as theego-netforvertexv ibyinstepl+1byφp ego,i(H( Vl)).Then
thegeneralmessage-passingmodel.Ink-GNNs,node-tuple theinformationisaggregatedacrossego-netsbycomputing
representations are learned according to messages from a node v’s new representation H(l+1) as the average over
v
lower-adjacencies: representations of v in its neighbours’ 1-ego-nets. Thus the
updateisgivenby
 
H( vl+1) =σΘ( 1l) H( vl)+ (cid:88) Θ( 2l) H( ul)  (9) (cid:80) φp (H(l))
u∈N↓(v) H( vl i+1) = j∈B d1 e(v gi ()
v
)eg +o,j
1
vi .
i
for learnable parameter matrices Θ(l),Θ(l) . In local k-
1 2
GNNs, the feature update is as in Equation 9, except that The authors show that in contrast to standard GNN
thesummationisoverlocalneighbourhoods. methods, Ego-GNNs can identify closed triangles in a
In k-invariant-graph-networks (k-IGNs) [176], the col- graph.Moreover,theyshowthattheirarchitectureisstrictly
lective feature representation Hout for node-k-tuples is more powerful than the WL-test at distinguishing non-
Vk
computed by feeding feature matrices through node- isomorphicgraphs.
permutation-equivariant linear layers L i followed by non- The GNN-as-kernel (GNN-AK) method [282] is based
linearities σ. The output is then transformed by a node- on similar ideas as Ego-GNNs and NGNNs, allowing the
permutation-invariantlayerhfollowedbyanMLP: incorporation of the node representations in different sub-
graphs and using subgraph embeddings based on pooling
Ho Vu kt = MLP◦h◦L d◦σ◦···◦σ◦L 1(X Vk)
the representations of a base GNN applied to an r-ego-
Theyauthorshaveproventhatk-IGNsareatleastaspower- net. Let φ( ul)(B r(v)) denote the feature representation of
fulask-WLatdistinguishingnon-isomorphicgraphs[175]. node u computed by the base GNN applied to B r(v) in
Later it was shown that they actually have the same layer l. Their basic architecture GNN-AK updates node
power [105]. Moreover, the form of linear equivariant and representations based on their ego-net embedding and the
invariant layers is well understood. The authors have pro- node’srepresentationinitsownego-net:
vided bases for these linear spaces and proven that their
dimensions are independent of the number of nodes and
H( vl+1)|centroid =φ( vl)(B r(v))
insteadonlydependonk. H( vl+1)|subgraph = (cid:77) (cid:16) φ( ul)(B r(v))(cid:17)
Other models based on the NT-Col-Graph are k-WL-
u∈Br(v)
GNN, delta-k-GNN, and delta-k-LGNN+ [186], or the ar- (cid:16) (cid:17)
chitecturebyMaronetal.[175]. H( vl+1) = FUSE H( vl+1)|centroid,H v(l+1)|subgraph
7.5 HOGNNsonSCol-Graphs&ST-Col-Graphs (cid:76)
where refers to a permutation-invariant aggregator and
7.5.1 Ego-NetArchitectures
FUSE to concatenation or sum. They show that using any
Ego-GNNs [209] build a subgraph collection from the one-
MPNN as base GNN, this model is strictly more powerful
hopneighbourhoodsNˆ
v
={v}∪N
v
foreverynodev ∈V.
than the WL-test. Moreover, GNN-AK can distinguish cer-
For each neighbourhood,
Nˆ
v, an individual graph neural tain graphs that 3-WL cannot. They propose a second vari-
network(GNN)istrained,yieldingforeverynodev ∈ V a ant, GNN-AK+, which incorporates node representations
representationxN vˆ u intheego-netofitsneighboursu ∈ N v. fromdifferentego-netsandthedistanced u,v betweennodes
Anewnoderepresentationisobtainedbyaveragingxn vew = intothenoderepresentationupdates.
MEAN({x vNˆ u : u ∈ Nˆ v}). Finally, a GNN takes these new
node features as input and learns a graph representation. H v(l+1)|centroid =φ( vl)(B r(v))
N ape ps rte od acg hr .ap Inh itn iae lu lyr ,al thn ee ytw bo urk ils d(N aG suN bN gs r) ap[2 h7 -5 c] ou lls ee cs tia onsim fri ola mr H( vl ,+ ga1 t) e| dsubgraph = (cid:77) σ(d u,v)φ( ul)(B r(v))
the r-hop neighbourhood B r(v) of every node v ∈ V for
u∈B ′r(v)
s x
s
co
t
eB
e
nm r
p
t(
,
re v e)
n
dr f oo a∈ dr teB
s
vZ .r> v( A0 v. i) GnN u
h
Ne s eix Nrn it t, g it stoh
h
tne
e
hey
ef
nGl ee aN ta
t
rr uN an
r
ineps eu e
x
drb Bg ws rr u
(
ia
v
tb )p hgh r
o
ta hfr p ee th sp
h
e.r ee I fns ese autn th
b
ut ea
g
rt erni
a
so e pn tx
h
os t
H(
vlH +( v 1l , )+ ga =1 t) e| dc Fo Unt Sex Et (cid:16)=
d
uu ,∈ v(cid:77) B ,Hr(v
(
vl) +σ 1( )d |cu en,v tr) oiφ d,( vl H)(
(
vB
l ,+
gr
a1
t(
)
eu
|
dsu))
bgraph,H( vl ,+ ga1 t) e| dcontext(cid:17)17
for a sigmoid function σ and possibly two distinct canbepooledintoagraphrepresentation.Theauthorsshow
permutation-invariant aggregators
(cid:76) ,(cid:76)′
. They show that thatwhenusingappropriateGNNarchitecturesforthebase
GNN-AK+ has the same power as GNN-AK and slightly and the outer GNN, NGNNs are strictly more powerful
improvesonGNN-AK’sperformance. thantheWL-test.Moreover,theydiscusshowhigher-order
7.5.2 Reconstruction-BasedArchitectures methodsoperatingonnode-tuplescanbeincorporatedinto
this nested approach. Since k-GNNs consider O(nk) node
In DropGNN [193], a subgraph is constructed by deleting
tuples,applyingthemethodtosubgraphsofsizecreduces
nodes of the input graph independently and uniformly at
thenumberoftuplestoO(nck).
random. In this fashion, one obtains a collection of node-
In GoGNN [240], L-graph representations are learned
deleted subgraphs. To build features for these subgraphs,
and then fed to a graph-attention network [237] which
they are fed to a GNN, which yields subgraph represen-
learns representations based on neighbourhoods in the
tations. Finally, these representations are aggregated into a
H-graph. They apply their work to predicting chemical-
single graph representation with a set encoder similar to
chemicalanddrug-druginteractions.
Deep Sets [269]. Once the subgraph representations have
The SEAL-AI/CI framework [159] has been devised
been computed, no other structural information is used to
to solve L-graph classification by learning two classifiers
build the graph representation. Drop Edge [204] uses a re-
corresponding to the two levels. In every update step,
latedapproach:itactsasaregularGNN,exceptthat,during
the information learned by one classifier is fed to the
the MP steps, single edges are removed uniformly at ran-
other. This approach has been applied to social networks
dom.Thusnodefeatureupdatestakeplaceinedge-removed
in which L-graphs corresponded to social sub-groups that
subgraphs.Thismethoddoesnotexplicitlylearnrepresenta-
wereconnectedbycommonmembersandtheobjectivewas
tionsforsubgraphs-itsimplyusesthenoderepresentations
todistinguishbetweengamingandnon-gaminggroups.
obtained at the end of the training to build a graph rep-
resentation.ReconstructionGNNs[86]learnrepresentations 8 EXPRESSIVENESS IN HOGNNS
for a graph from representations of fixed-size subgraphs
Wealsooverviewapproachesforanalyzingtheexpressive-
of G. In the first step, one picks k ∈ {1,...,|V| − 1}
nessofHOGNNs.
and builds a subgraph-collection by sampling subgraphs
8.1 ApproachesforExpressivenessAnalysis
of G with k nodes. Then a GNN is applied to these node-
There are several ways to formally compare the power of
induced subgraphs, yielding subgraph representations. Fi-
differentHOGNNs[211].Oneapproachistoconsiderwhich
nally, these are transformed and aggregated into a graph
pairsofinputgraphscanbedistinguishedbyagivenGNN
representation, for example, using a set encoder [269]. A
variant of ReconstructionGNNs uses all size-k subgraphs. (Section 8.1.1). Another approach investigates whether a
However, this is only feasible for small k ∈ {1,2} or large GNN can count selected subgraphs (Section 8.1.2). Finally,
k ∈ {n−2,n−1}. Like DropGNN, ReconstructionGNNs one can also analyze which function classes respective
GNNscanapproximate(Section8.1.3).
omittheinterrelationalstructureofsubgraphs.
8.1.1 IsomorphismBasedExpressiveness
7.5.3 GeneralSubgraph-AdjacencyBasedSchemes
The idea underlying isomorphism-based expressiveness is
The third class of methods focuses on more specific sub-
toconsiderwhichnon-isomorphicgraphsagivenHOGNN
graphs. Subgraph neural networks [5] start with a given
candistinguish.Thegraphisomorphism(GI)problemisthe
subgraph collection. The goal is to learn a representation
computational task of discerning pairs of non-isomorphic
for each subgraph which can be used to make predictions.
simple graphs. Dating back to the 1960s, classifying its
They devise a message passing scheme based on a sophis-
complexityremainsunsolved.Thestate-of-the-artalgorithm
ticated subgraph adjacency structure. In Autobahn [230],
byBabai[14],[130]runsinquasipolynomialtime.However,
one covers a graph with a chosen class of subgraphs, for
formanysubclassesofgraphs,polynomial-timealgorithms
example, paths and cycles. The method then applies local
areknown,forexample,forplanargraphs[133],trees[144],
operations based on the node intersections of subgraphs.
circulant graphs [190] and permutation graphs [83]. In
Subgraph representations are then aggregated into graph
the expressiveness analysis of HOGNNs, one usually com-
representations using a permutation-invariant aggregator.
paresanarchitecture’sabilitytodistinguishnon-isomorphic
Finally,thearchitectureapplieslocalconvolutionstoupdate
graphstoaheuristicforGI-testing.
thegraphrepresentation.
TheWeisfeiler-Lehmantest(WL-test)[249]isanefficient
7.6 HOGNNsonNestedGDMs
GIheuristicbasedoniterativecolourrefinement.
Nested graph neural networks (NGNNs) [275] follow a
Definition 8.1 (Weisfeiler-Lehman test). Given a vertex-
similar approach as Ego-GNNs, however, they use a dif-
coloured graph G = (V,E,c(0)) with colour function
ferent aggregation scheme. Their architecture is composed
c(0): V →Σ,vertexcoloursforv ∈V areupdatedaccordingto
of two levels: the base GNN and the outer GNN. The
(cid:16) (cid:17)
base GNN learns a representation for the r-ego-nets of c( vl+1) = HASH c( vl),{{c( ul) :u∈N v}}
every vertex by applying a GNN architecture on the r-
ego-nets. Instead of keeping the representations of single
forabijectivefunctionHASHtoΣ.Ineverystepl,thehistogram
nodes in every ego-net B r(v), they are aggregated into ofnodescolours{{c( vl) : v ∈ V}}definesthecolourofthegraph
a single ego-net representation Hv,ego for every v ∈ V. c( Gl) . When the graph colour c( Gl) = c( Gl+1) remains equal in two
The outer GNN treats the ego-net representations Hv,ego consecutivesteps,thealgorithmterminates.Twographsarenon-
as node-representations and performs message-passing to isomorphiciftheirfinalgraphcoloursaredistinct.Otherwise,the
updatetheego-netfeatures.Finally,theseupdatedfeatures isomorphismtestisinconclusive.18
The WL-test terminates after at most O((|m| + The k-FWL test uses multisets of neighbourhood colour
|n|)log|V|) iterations [147]. Recent work has characterised tuples in which the tuples are indexed by nodes w ∈ V
the types of non-isomorphic graphs that can be distin- and the entries of the tuples are down adjacencies indexed
guished by the WL-test [11]. For an overview of the power bycoordinatesj ∈ {1,...,k}.Thissubtledifferencemakes
andlimitationsoftheWL-test,wereferto[147]. onemethodmorepowerfulthantheotheratdistinguishing
Ashasbeennotedbyseveralauthors,oneiterationinthe non-isomorphic graphs, namely k-FWL has the same dis-
WL-testcloselyresemblesmessagepassingwhenreplacing criminativepoweras(k+1)-WL[70]forallk ⩾2.Fork =1,
colourswithvertexfeatures: bothalgorithmssimplifytothe1-WL-test(8.1).Inparticular,
  1-WL and 2-WL have the same distinguishing power. The
(cid:77) k-FWL test is known to terminate after O(k2nk+1logn)
xn vew =φ xv, ψ(xv,xu) .
steps[70],whilewearenotawareofsuchresultsfork-WL.
(cid:124) (cid:123)(cid:122) (cid:125)
u∈Nvmessageu→v
The(k+1)-WL-testhasstrictlyhigherdiscriminativepower
ThemaindifferencebetweenMP-GNNsandtheWL-testis than k-WL for every k ⩾ 2 [70]. Thus the k-WL-test for
that feature values are relevant in GNNs, while only the k ∈Z >0formafamilyofgraphisomorphismheuristicswith
distributionorhistogramorcoloursmattersintheWL-test. strictly increasing distinguishing powers for the increasing
Moreover, information may be lost by the transformations
k ⩾3increases.WecallthistheWLhierarchy.
ψ,φ and the aggregation (cid:76) . Xu et al. [256] have shown Xu et al. [256] show that MP-GNNs have the same
that MP-GNNs are at most as powerful as the WL-test at
discriminativepowerasthe1-WL-testiff,φandthegraph-
distinguishingnon-isomorphicsimplegraphs. pooling function are injective [256]. Morris et al. [185],
Multiple variants of the Weisfeiler-Lehman test (WL- [187] show a similar upper bound for DAMP architectures
test) have been introduced, often forming the basis for onisomorphism-type-liftednode-tuple-collections(seeDef-
deep learning architectures [185], [187]. This includes tests inition 6.1), for example k-GNNs. They found that the
for simple graphs [247], simplicial complexes, cell com- discriminative power of DAMP methods on k-node-tuple-
plexes[56],node-tuple-collections[70],andothers[19].The collections is upper bounded by the discriminative power
testsfollowasimilarapproach:startingwithacollectionof
ofthek-WL-test.Moreover,thereexistDAMParchitectures
objects with colours and channels connecting them, every on k-node-tuple-collections which attain the same power.
object sends its colour along all outgoing channels to its
SomeotherGNNmethodsonnode-k-tuple-collections,such
adjacencies. Once all colours have been received, the new as k-invariant graph networks (IGNs) [176] have the same
colour is defined by a function of the current colour and discriminative power as k-WL [105], while δ-k-GNNs have
multisetsofreceivedcolours.Thealgorithmterminatesonce lowerexpressivepower.
the graph colours are equal in two consecutive steps. The
Intheδ-k-WL-test[186],thecolourfunctionadditionally
most prominent higher-order variants are the two k-WL- stores whether the neighbour is a local lower adjacency,
tests [70], which apply colour refinement to node-tuple- i.e., C u(l) is replaced by (C u(l),1 u∈N↓,loc(v)) in the equation
collections derived from the isomorphism-type lifting of above.Theauthorsprovethatthisvariationofthek-WL-test
simplegraphs. is strictly more powerful at distinguishing non-isomorphic
graphsthank-WL.Toreducethecomputationalcostofthis
Definition 8.2 (k-Weisfeiler-Lehman tests). Let G = algorithm, which scales as Ω(nk), the authors propose the
(V,E,c(0)) be a vertex-coloured graph with colour function
local k-WL test, denoted k-LWL. In k-LWL, only colours
c(0): V →Σandk ∈Z >0.
from lower local adjacencies are sent. While this reduces
1) Inthek-WL-test,coloursareupdatedaccordingto
the computational cost for some graphs, one also loses the
c( vl+1) =HASH(cid:16) c( vl),(cid:16) {{c( ul) :u∈N jk-WL(v)}}:j ∈[k](cid:17)(cid:17) e thx ip sr ,e ts hs ei yve pn re os ps og su ea ar nan et nee hs ano cf eδ d-k k-W -LWLa Ln +d tek s- tW
.
IL t. isTo bare sem ded ony
2) In the k-folklore-Weisfeiler-Lehman (FWL)-test, colours are k-LWL,butinthei-thiteration,thecolourmessagefromthe
updatedaccordingto localneighbourxwhichdiffersinthej-thcoordinatehasan
(cid:16) (cid:110)(cid:110)(cid:16) (cid:17) (cid:111)(cid:111)(cid:17) additionalargument
c( vl+1) =HASH c( vl), c( ul) :u∈N wk-FWL(v) ,w∈V
where #j i(v,x)=|{w:w∈N ↓(v):C w(i) =C x(i)}|,
N
jk-WL(v)=(cid:8)
(v i1,...,v ij−1,w,v ij+1,...,v
ik):w∈V(cid:9)
that is, the number of lower adjacencies of v that have
N
wk-FWL(v)=(cid:0)
(v i1,...,v ij−1,w,v ij+1,...,v ik):j
∈[1:k](cid:1)
the same colour as x. The authors show that in connected
graphs,k-LWL+hasthesamepowerasδ-k-WLandthatthe
The graph colour is defined as the histogram of node-tuple
connectednessconditioncanbeliftedbyaddinganauxiliary
colours. The algorithms terminate when the graph colour of two
vertex. Although k-LWL+ has a better runtime than δ-k-
consecutivestepsremainsunchanged.Asbefore,graphsarenon-
WL, its run-time still scales as Ω(nk). The authors further
isomorphic if their final colours differ. Otherwise, the test is
proposesamplingtechniquestoaddressit.
inconclusive.
The WL hierarchy is a widely used framework for
The two variants of the WL-test on k-node tuples differ measuring the expressive power of GNNs. However, some
in the employed neighbourhoods. The k-WL-test works GNN architectures do not align with the hierarchy, for
with tuples of neighbourhood colour multisets, in which example,graphsubstructurenetworks(GSNs)[59],message
multisets are collections of down adjacencies indexed by passing simplicial networks (MPSNs) [57], CW-networks
nodesandtuplesareindexedbycoordinatesj ∈{1,...,k}. (CWNs) [56] or Autobahn [230]. The Weisfeiler-Lehman19
(WL) hierarchy is built around plain graphs and NT-Col- additional pre-processing. The efficiency of hypergraph-
graphs. Next, we will see an alternative approach to mea- basedmethods,includingmethodsonsimplicialcomplexes
and cell complexes, heavily depends on the chosen lifting.
suring the expressiveness of GNNs, based on counting
With certain liftings, it has been shown that MPSN [57]
isomorphicsubgraphs.
and CWN [56] attain a strictly higher expressiveness than
8.1.2 SubstructureCountBasedExpressiveness 1-WL and are provably not less powerful than 3-WL. Sim-
Some approaches aim at harnessing substructure counts as ilarly, the message passing complexity of subgraph-based
methods depends on the chosen subgraph-collection and
a measure of GNN expresiveness [59]. Specifically, a given
certainsubgraph-basedmodelsprovablybeat1-WLandare
GNNismoreexpressivethananotherGNN,ifitcancount
nolesspowerfulthan3-WL.Asubcollectionofmodelsonk-
– for a specified subgraph S and for each vertex v or edge node-tuple-collections,forexample,k-GNN[187]havebeen
e – what is the count of S that v or e belong to. Recent proven to have the same expressiveness as k-WL for any
work has improved the understanding of substructure- k ⩾ 2. In k-GNNs all node-tuples v = (v i1,...v ik) ∈ Vk,
basedexpressivenessandledtonewarchitectures.Chenet areconsideredgiving|V|k node-tuplesintotal.Eachnode-
al. [76] analysed the ability of established methods to per- tuplesendsitsfeaturetoallitsdown-adjacencies
form subgraph counting for various classes of subgraphs. (cid:8) (cid:9)
For example, they found that MP-GNNs and 2-IGNs can
N ↓(v)= (v i1,...,v ij−1,w,v ij+1,...,v ik):w∈V,j ∈[1:k] .
perform subgraph counting of star-shaped patterns, while Since|N ↓(v)|=k·|V|k−1,assumingthatmessagesaresent
they fail at subgraph-counting for connected patterns with bidirectionally,weobtainatotalofk·|V|2k−1messagessent
3ormorenodes.Higher-orderk-IGNscanperforminduced ineveryiteration.Figure10displaystheexpressivenessand
subgraph-count for patterns consisting of at most k nodes. message passing complexity for a selection of simple and
GSNs [59] employ MP on plain graphs with augmented higher-orderGRL(HoGRL)methods.Notethatthemessage
node or edge features encoding the occurrence of a node, passing complexity scales polynomially in the number of
respectively edge in subgraph orbits (see Definition 5.14). nodesandfork-GNNexponentiallyintheparameterk.
With particular choices of subgraphs, the authors demon-
9 WORKS RELATED TO HIGHER-ORDER GNNS
stratethatGSNsarestrictlymorepowerfulthantheWLtest
andnolesspowerfulthanthe2-FWLtestatdistinguishing Wealsosummarizeseveralaspectsthatarenotthefocusof
non-isomorphicgraphs. thiswork,butarestillrelatedtoHOGNNs.
There have been efforts into combining HO with tem-
8.1.3 FunctionApproximationBasedExpressiveness
poral graph data models [93]. This includes graph data
Whilediscriminatoryframeworksarecommonlyused,GRL
modelssuchasd-dimensionalDeBruijngraphs[156],[215],
methods have also been analysed for their ability to ap-
memory networks [207], HO Markov models for temporal
proximate function classes. Maron et al. [177] have ex-
networks [24], [194], [253], motif-based process representa-
plored this question for continuous permutation-invariant
tions [216], multi-layer and multiplex networks [151], hy-
functions F from the class of graphs G n with n nodes pergraphs[75],[245],[260],[266],andothers[286].
to R and identified neural architectures (k-IGNs) which
Another line of works is related to random walks on
are universal approximators of F under certain conditions.
HOGDMs as a way of harnessing HO for learning graph
Interestingly, the universal approximation of permutation-
representations. This includes works dedicated to hyper-
invariantcontinuousfunctionsandtheabilitytodistinguish
graphs [4], [72], [81], [84], [128], [129], [168], [214], [277],
non-isomorphicgraphsareequivalent[78].Thatis,aclassof
hyper-networks [236], and simplicial complexes [53], [119],
permutation-invariantcontinuousfunctionsF fromG ntoR
[270].
candistinguishallnon-isomorphicgraphsinG n ifandonly Finally,therehavebeeneffortsintoharnessingHOinthe
if F is a universal approximator of permutation invariant
contextofheterogeneousgraphs[94],[226].Insuchgraphs,
functions from G n to R. For a more in-depth discussion of nodesandedgesmaybelongtodifferentclasses[170],[227],
expressivenessingraphrepresentationlearning,wepointto
[271].
theliterature[105],[106],[211].
10 RESEARCH OPPORTUNITIES
8.2 Expressivenessvs.ComputationalComplexity
WenowreviewfutureresearchdirectionsinHOGNNs.
We also discuss the relation between expressiveness and
Exploring New HO Graph Data Models. One avenue
computational complexity of HOGNNs. We focus on the
of future works lies in developing new HOGDMs beyond
WL hierarchy related expressiveness since most published
thoseinTable1,whichcouldbetterencapsulateHOinterac-
results work with it. In our analysis, we only consider
tionsbetweenentitiesinawaythatisbothcomputationally
bounds on the number of messages passed, neglecting the
efficient and representative of the underlying complexities
complexityofthetransformationsinvolvedinMP.
of the data. For example, one could consider novel sub-
In general, MP architectures on graphs, such as
GCN [150], GraphSAGE [124], or GAT [237] are efficient, structuresforMotif-Graphs,suchasdifferentformsofdense
onemessagepassingstepinvolvingΘ(|E|)messages.How- subgraphs[44],[49],[111],[113],[158],[173],[210].
ever, their expressiveness is upper-bounded by the 1-WL- Simplicial 2-complex convolutional neural networks
test. A subclass of message passing architectures such as (S2CNNs) [69] Moreover, by incorporating powers of these
GIN[256]or1-IGNs[176]attainthesameexpressivenessas
operators, one would obtain a framework that generalises
1-WL, while also sending only Θ(|E|) messages. Methods
SCNN[262]andmultihopdiffusionmodelsthathavebeen
with augmented node and edge features, e.g., GSN [59]
devisedforgraphs[1].
maintain a low message passing complexity, while being
strictly more expressive than 1-WL and not less powerful Exploring New HO Neural Architectures & Models.
than3-WL.Notethatthisgaininexpressivepowerrequires Similarly to exploring new HOGDMs, one can also study20
Weisfeiler-Lehman-hierarchy Message Complexity
GRL on node-tuple-
collections
k-GNN, …
𝑘-WL 𝑂(𝑘 𝑉 2𝑘−1)
HoGRL: hypergraphs,
subgraph-collections
GSN, CWN, MPSN, …
3-WL
GRL on graphs
1-WL/
𝑂(|𝐸|)
Special MPNNs (GIN, …)
2-WL
General MPNN (GCN, …)
Fig.10:Overviewofexpressivepowerandmessagecomplexity(#exchangedmessages)inHOGRLmodels.Theshapesontheleftrepresentsubclassesofgraphs,
whicharedistinguishablebyaGIheuristicoragraphrepresentationlearning(GRL)method.ThecirclesofincreasingradiusrepresenttheWL-hierarchy,while
purpleandyellowshapesindicatesubclassesofgraphsthatmethodscandistinguish,whichdonotalignwiththeWL-hierarchy.Thebluecloudisastrictsubset
of1-WL-distinguishablegraphs,whichstandsfordifferentsubsetsofgraphswhichstandardmessagepassingneuralnetworks(MPNNs)candistinguish.Theright
columndepictsmessagecomplexity.ThegreencloudindicatesamessagecomplexitygreaterthanO(|E|),butlowerthanO(k|V|2k−1)forlargek.Thecentral
columnshowsGRLmethodsforgraphsandhigher-ordergraphdatamodels(HoGDMs),withedgesindicatingtheexpressivepower,respectively,andthemessage
complexityofthemethods.
new HOGNN models and model classes. One direction in retrieval and manipulation, ensuring that these advanced
this avenue would be to explore novel forms of communi- models can be effectively utilized in a variety of computa-
cation channels beyond those described in Section 6. Here, tionalenvironments.
one specific idea would be to harness the attentional and Parallel and Distributed HO Neural Architectures.
message-passingflavorsformodelsbasedonmorecomplex Parallel and distributed computing offers a pathway to
GDMs such as Nested-Graphs, Nested-Hypergraphs, and tacklethescalabilitychallengesinherentinprocessinglarge-
others. Another direction would be to harness mechanisms scale,complexgraphdata.Constructingalgorithmsthatcan
beyondtheHOGNNblueprint,forexamplenewactivation efficiently distribute the workload of HO neural networks
functions [219]. Finally, mechanisms based on randomiza- across multiple processors or nodes can significantly re-
tioncouldprovefruitfulinordertoalleviatecomputecosts duce training and inference times. Research could explore
whileofferingatunableaccuracytradeoff. novel parallelization strategies, communication protocols,
Developing Processing Frameworks for HOGNNs. and synchronization mechanisms tailored to the unique
There exists a plethora of frameworks for GNNs [16], [18], demandsofHOgraphprocessing,aimingtooptimizecom-
[71], [79], [98], [132], [135], [140], [160], [161], [163], [172], putational efficiency and resource utilization. One could
[179],[231],[232],[238],[239],[241],[242],[246],[250],[272], also investigate effective integration of prompting with
[274], [278], [279], [283], [287]. A crucial direction would be distributed-memory infrastructure and paradigms, such as
to construct such frameworks for HOGNNs, focusing on remote direct memory access (RDMA) [34]–[36], [89], [90],
research into system design and architecture, programma- [108]orserverlessprocessing[17],[85],[142],[174],[178].
bility,productivity,andothers. Exploring Hardware-Acceleration for HO Neural Ar-
Integrating HOGNNs into Graph Databases and chitectures.Understandingenergyandperformancebottle-
Graph Frameworks. A related research direction is to aug- necksandmitigatingthemwithspecializedtechniquessuch
ment the capabilities of modern graph databases [7], [8], as processing-in-memory [3], [41], [109], [188], [189], [217],
[29]–[31], [87], [104], [127], [143], [152] and dynamic graph dedicated NoCs [33], [47], [110], [139], FPGAs [47], [88],
frameworks [28], [82], [208] with HOGNNs. While initial [182], or even quantum devices [195], [224] could enable
designs for such systems combined with GNNs exist [40], much more scalable models and model execution under
[200], HOGNNs have still not being considered for such a stringentconditions.
setting. This integration would facilitate the storage, man- Global Formulations for HOGNNs. There have been
agement, and analysis of HO graph data, providing re- effortsintobuildingglobalformulationsforGNNs[23],[39],
searchersandpractitionerswiththetoolsneededtodeploy [46], [235] and general graph computing [42], [45], [64]–
HOGNNs in real-world applications. Research opportuni- [68], [107], [145], [146], [153], [220], [259]. This facilitates
ties include the development of standardized APIs, query applying mechanisms such as communication avoidance
languages, and optimization techniques for efficient data or vectorization [154], [155], [221], [222]. Constructing and21
implementingsuchformulationsforHOGNNswouldbean innovationprogramunderthegrantagreementNo.101070141
interestingdirection. (ProjectGLACIATION).
Temporal HOGNNs.Animportantvenueofresearchis
REFERENCES
toinvestigatehowtocombineHOandtemporalgraphs[27],
[173]. This could involve using HO for more accurate [1] S. Abu-El-Haija, B. Perozzi, A. Kapoor, N. Alipourfard, K. Ler-
temporal-relatedpredictions. man, H. Harutyunyan, G. V. Steeg, and A. Galstyan. MixHop:
Higher-OrderGraphConvolutionalArchitecturesviaSparsified
Summarization & Compression. Graph compression Neighborhood Mixing. arXiv:1905.00067 [cs, stat], June 2019.
and summarization is an important topic that received arXiv:1905.00067.
widespreadattention[37],[48],[50],[166],[191],[202],[233], [2] R.Achanta,A.Shaji,K.Smith,A.Lucchi,P.Fua,andS.Su¨sstrunk,
editors. SLICSuperpixels. EPFL,2010.
[276]. GNNs have been used to summarize and compress
[3] J.Ahn,S.Yoo,O.Mutlu,andK.Choi. Pim-enabledinstructions:
graphs[54],[164],[218].HOGNNsimposehierarchicalcom- a low-overhead, locality-aware processing-in-memory architec-
pute patterns over the graph structure, potentially offering ture. In Computer Architecture (ISCA), 2015 ACM/IEEE 42nd
novelcapabilitiesformoreeffectivegraphcompressionand AnnualInternationalSymposiumon,pages336–348.IEEE,2015.
[4] S. G. Aksoy, C. Joslyn, C. O. Marrero, B. Praggastis, and
summarization.
E. Purvine. Hypernetwork science via high-order hypergraph
IntegratingHOGNNsintoLLMPipelines.GNNshave walks. EPJDataScience,9(1):16,2020.
alsobeenusedtogetherwithLLMsandbroadgenerativeAI [5] E.Alsentzer,S.G.Finlayson,M.M.Li,andM.Zitnik. Subgraph
NeuralNetworks.TechnicalReportarXiv:2006.10538,arXiv,Nov.
pipelines[77],[95],[137],[138],[167],[198],[201],[254].An
2020. arXiv:2006.10538[cs,stat]type:article.
excitingresearchdirectionistoexplorehowHOGNNs,and
[6] U.Alvarez-Rodriguez,F.Battiston,G.F.deArruda,Y.Moreno,
morebroadly,howHGDMscouldbeharnesstoenhance,for M.Perc,andV.Latora. Evolutionarydynamicsofhigher-order
example, the structured reasoning LLM schemes [26], [43], interactionsinsocialnetworks.NatureHumanBehaviour,5(5):586–
595,May2021. Number:5Publisher:NaturePublishingGroup.
[248],[265].
[7] R.AnglesandC.Gutierrez. SurveyofGraphDatabaseModels.
inACMComput.Surv.,40(1):1:1–1:39,2008.
11 CONCLUSION
[8] R. Angles and C. Gutierrez. An Introduction to Graph Data
Graph Neural Networks (GNNs) have enabled graph- Management. InGraphDataManagement,FundamentalIssuesand
RecentDevelopments,pages1–32.2018.
drivenbreakthroughtsinareassuchasdrugdesign,mathe-
[9] A. Antelmi, G. Cordasco, M. Polato, V. Scarano, C. Spagnuolo,
maticalreasoning,andtransportation.HigherOrderGNNs andD.Yang. Asurveyonhypergraphrepresentationlearning.
(HOGNNs)hasemergedasacrucialclassofGNNmodels, ACMComputingSurveys,56(1):1–38,2023.
offering higher expressive power and accuracy of predic- [10] R.Aponte,R.A.Rossi,S.Guo,J.Hoffswell,N.Lipka,C.Xiao,
G.Chan,E.Koh,andN.Ahmed. Ahypergraphneuralnetwork
tions.However,asaplethoraofHOGNNmodelshavebeen
frameworkforlearninghyperedge-dependentnodeembeddings.
introduced, together with a variety of different graph data arXivpreprintarXiv:2212.14077,2022.
models (GDMs), architectures, and mechanisms employed, [11] V.Arvind,J.Ko¨bler,G.Rattan,andO.Verbitsky.OnthePowerof
it makes it very challenging to appropriately analyze and ColorRefinement. InA.KosowskiandI.Walukiewicz,editors,
FundamentalsofComputationTheory,LectureNotesinComputer
compareHOGNNmodels.
Science,pages339–350,Cham,2015.SpringerInternationalPub-
This paper addresses these challenges by introducing a lishing.
blueprintandanaccompanyingtaxonomyofHOGNNs,fo- [12] D.Arya,D.K.Gupta,S.Rudinac,andM.Worring. Hypersage:
Generalizinginductiverepresentationlearningonhypergraphs.
cusingonformalfoundations.WemodelageneralHOGNN
arXivpreprintarXiv:2010.04558,2020.
schemeasacompositionofanHOGDM,thespecificationof [13] D. Arya, D. K. Gupta, S. Rudinac, and M. Worring. Adaptive
message-passing(MP)channelsimposedontothatHOGDM neuralmessagepassingforinductivelearningonhypergraphs.
(which prescribes how the feature vectors are transformed arXivpreprintarXiv:2109.10683,2021.
[14] L. Babai. Graph Isomorphism in Quasipolynomial Time, Jan.
betweenGNNlayers),andthespecificsofliftingandlower-
2016. arXiv:1512.03547[cs,math].
ingtransformationsthatareusedtoconverttheinputplain [15] S.Bai,F.Zhang,andP.H.S.Torr. Hypergraphconvolutionand
graph into, and back from, an HO form. The taxonomy is hypergraphattention. PatternRecognition,110:107637,Feb.2021.
thenusedtosurveyandanalyzeexistingdesigns,dissecting [16] Y.Bai,C.Li,Z.Lin,Y.Wu,Y.Miao,Y.Liu,andY.Xu. Efficient
dataloaderforfastsampling-basedgnntrainingonlargegraphs.
them into fundamental aspects. Our blueprint facilitates
IEEETPDS,2021.
developingnewHOGNNarchitectures.
[17] I. Baldini, P. Castro, K. Chang, P. Cheng, S. Fink, V. Ishakian,
We also conduct an analysis of HOGNN methods in N. Mitchell, V. Muthusamy, R. Rabbah, A. Slominski, et al.
Serverless computing: Current trends and open problems. In
termsoftheirexpressivenessandcomputationalcosts.Our
ResearchAdvancesinCloudComputing,pages1–20.Springer,2017.
investigation results in different insights into the tradeoffs
[18] M. F. Balın et al. Mg-gcn: Scalable multi-gpu gcn training
between them. We also provide insights into open chal- framework. arXivpreprintarXiv:2110.08688,2021.
lengesandpotentialresearchdirections,navigatingthepath [19] I.Batatia,D.P.Kovacs,G.Simm,C.Ortner,andG.Csa´nyi.Mace:
Higher order equivariant message passing neural networks for
forfutureresearchavenuesintomoreeffectiveHOGNNs.
fast and accurate force fields. Advances in Neural Information
ProcessingSystems,35:11423–11436,2022.
ACKNOWLEDGEMENTS
[20] F. Battiston, E. Amico, A. Barrat, G. Bianconi, G. Ferraz de Ar-
This project received funding from the European Research ruda,B.Franceschiello,I.Iacopini,S.Ke´fi,V.Latora,Y.Moreno,
M. M. Murray, T. P. Peixoto, F. Vaccarino, and G. Petri. The
Council(ProjectPSAP,No.101002047),andtheEuropeanHigh-
physicsofhigher-orderinteractionsincomplexsystems. Nature
Performance Computing Joint Undertaking (JU) under grant Physics, 17(10):1093–1098, Oct. 2021. Number: 10 Publisher:
agreement No. 955513 (MAELSTROM). This project was sup- NaturePublishingGroup.
[21] F.Battiston,G.Cencetti,I.Iacopini,V.Latora,M.Lucas,A.Pata-
ported by the ETH Future Computing Laboratory (EFCL),
nia, J.-G. Young, and G. Petri. Networks beyond pairwise
financedbyadonationfromHuaweiTechnologies.Thisproject
interactions: structure and dynamics. Physics Reports, 874:1–92,
receivedfundingfromtheEuropeanUnion’sHEresearchand Aug.2020. arXiv:2006.01764.22
[22] F.Battiston,G.Cencetti,I.Iacopini,V.Latora,M.Lucas,A.Pata- L. Gianinazzi, I. Stefan, J. Go´mez Luna, M. Copik, L. Kapp-
nia, J.-G. Young, and G. Petri. Networks beyond pairwise Schwoerer, S. Di Girolamo, M. Konieczny, N. Blach, O. Mutlu,
interactions: Structure and dynamics. Physics Reports, 874:1–92, and T. Hoefler. Sisa: Set-centric instruction set architecture for
2020. graphminingonprocessing-in-memorysystems. InMICRO-54:
[23] J.Bazinska,A.Ivanov,T.Ben-Nun,N.Dryden,M.Besta,S.Shen, 54thAnnualIEEE/ACMInternationalSymposiumonMicroarchitec-
andT.Hoefler. Cachedoperatorreordering:Aunifiedviewfor ture,pages282–297,2021.
fastgnntraining. arXivpreprintarXiv:2308.12093,2023. [42] M.Besta,F.Marending,E.Solomonik,andT.Hoefler.Slimsell:A
[24] A. R. Benson, D. F. Gleich, and J. Leskovec. Tensor spectral vectorizablegraphrepresentationforbreadth-firstsearch.In2017
clustering for partitioning higher-order network structures. In IEEE International Parallel and Distributed Processing Symposium
Proceedings of the 2015 SIAM International Conference on Data (IPDPS),pages32–41.IEEE,2017.
Mining,pages118–126.SIAM,2015. [43] M. Besta, F. Memedi, Z. Zhang, R. Gerstenberger, N. Blach,
[25] A. R. Benson, D. F. Gleich, and J. Leskovec. Higher-order P.Nyczyk,M.Copik,G.Kwas´niewski,J.Mu¨ller,L.Gianinazzi,
organization of complex networks. Science, 353(6295):163–166, et al. Topologies of reasoning: Demystifying chains, trees, and
2016. graphsofthoughts. arXivpreprintarXiv:2401.14295,2024.
[26] M.Besta,N.Blach,A.Kubicek,R.Gerstenberger,L.Gianinazzi, [44] M. Besta, C. Miglioli, P. S. Labini, J. Teˇtek, P. Iff, R. Kanak-
J.Gajda,T.Lehmann,M.Podstawski,H.Niewiadomski,P.Ny- agiri, S. Ashkboos, K. Janda, M. Podstawski, G. Kwas´niewski,
czyk,etal. Graphofthoughts:Solvingelaborateproblemswith N.Gleinig,F.Vella,O.Mutlu,andT.Hoefler. Probgraph:High-
largelanguagemodels. InAAAI,2023. performanceandhigh-accuracygraphminingwithprobabilistic
[27] M. Besta, A. C. Catarino, L. Gianinazzi, N. Blach, P. Nyczyk, setrepresentations. InSC22:InternationalConferenceforHighPer-
H. Niewiadomski, and T. Hoefler. Hot: Higher-order dynamic formanceComputing,Networking,StorageandAnalysis(ACM/IEEE
graph representation learning with efficient transformers. In Supercomputing),pages1–17.IEEE,2022.
LearningonGraphsConference.PMLR,2023. [45] M.Besta,M.Podstawski,L.Groner,E.Solomonik,andT.Hoefler.
[28] M. Besta, M. Fischer, V. Kalavri, M. Kapralov, and T. Hoefler. Topushortopull:Onreducingcommunicationandsynchroniza-
Practice of streaming processing of dynamic graphs: Concepts, tioningraphcomputations.InProceedingsofthe26thInternational
models,andsystems.IEEETransactionsonParallelandDistributed SymposiumonHigh-PerformanceParallelandDistributedComputing
Systems(TPDS),2021. (HPDC),pages93–104,2017.
[29] M.Besta,R.Gerstenberger,N.Blach,M.Fischer,andT.Hoefler. [46] M.Besta,P.Renc,R.Gerstenberger,P.SylosLabini,T.Chen,L.Gi-
Gdi: A graph database interface standard. Technical report, aninazzi,F.Scheidl,K.Szenes,A.Carigiet,P.Iff,G.Kwasniewski,
TechnicalReport.Availableathttp://spcl.inf.ethz.ch/Research/ R. Kanakagiri, C. Ge, S. Jaeger, J. Was, F. Vella, and T. Hoefler.
Parallel Programming/GDI/gdi v0.1.pdf,2023. High-performance and programmable attentional graph neural
[30] M.Besta,R.Gerstenberger,M.Fischer,M.Podstawski,J.Mu¨ller, networks with global tensor formulations. In Proceedings of the
N.Blach,B.Egeli,G.Mitenkov,W.Chlapek,H.Niewiadomski, InternationalConferenceonHighPerformanceComputing,Network-
M. Michalewicz, and T. Hoefler. The graph database interface: ing,StorageandAnalysis(ACM/IEEESupercomputing),2023.
Scaling online transactional and analytical graph workloads to [47] M.Besta,D.Stanojevic,J.D.F.Licht,T.Ben-Nun,andT.Hoefler.
hundredsofthousandsofcores. InProceedingsoftheInternational Graphprocessingonfpgas:Taxonomy,survey,challenges. arXiv
ConferenceonHighPerformanceComputing,Networking,Storageand preprintarXiv:1903.06697,2019.
Analysis(ACM/IEEESupercomputing),2023. [48] M. Besta, D. Stanojevic, T. Zivic, J. Singh, M. Hoerold, and
[31] M.Besta,R.Gerstenberger,E.Peter,M.Fischer,M.Podstawski, T.Hoefler. Log(graph)anear-optimalhigh-performancegraph
C. Barthels, G. Alonso, and T. Hoefler. Demystifying graph representation.InProceedingsofthe27thinternationalconferenceon
databases:Analysisandtaxonomyofdataorganization,system parallelarchitecturesandcompilationtechniques(PACT),pages1–13,
designs, and graph queries. ACM Computing Surveys (CSUR), 2018.
2023. [49] M. Besta, Z. Vonarburg-Shmaria, Y. Schaffner, L. Schwarz,
[32] M. Besta, R. Grob, C. Miglioli, N. Bernold, G. Kwasniewski, G. Kwasniewski, L. Gianinazzi, J. Beranek, K. Janda, T. Holen-
G.Gjini,R.Kanakagiri,S.Ashkboos,L.Gianinazzi,N.Dryden, stein,S.Leisinger,P.Tatkowski,E.Ozdemir,A.Balla,M.Copik,
and T. Hoefler. Motif prediction with graph neural networks. P. Lindenberger, P. Kalvoda, M. Konieczny, O. Mutlu, and
InProceedingsofthe28thACMSIGKDDConferenceonKnowledge T. Hoefler. Graphminesuite: Enabling high-performance and
DiscoveryandDataMining,pages35–45,2022. programmable graph mining algorithms with set algebra. In
[33] M. Besta, S. M. Hassan, S. Yalamanchili, R. Ausavarungnirun, InternationalConferenceonVeryLargeDataBases(VLDB),2021.
O. Mutlu, and T. Hoefler. Slim noc: A low-diameter on-chip [50] M. Besta, S. Weber, L. Gianinazzi, R. Gerstenberger, A. Ivanov,
networktopologyforhighenergyefficiencyandscalability.ACM Y. Oltchik, and T. Hoefler. Slim graph: Practical lossy graph
SIGPLANNotices,53(2):43–55,2018. compression for approximate graph processing, storage, and
[34] M.BestaandT.Hoefler. Faulttoleranceforremotememoryac- analytics.InProceedingsoftheInternationalConferenceforHighPer-
cessprogrammingmodels. InProceedingsofthe23rdinternational formanceComputing,Networking,StorageandAnalysis(ACM/IEEE
symposium on High-performance parallel and distributed computing Supercomputing),pages1–25,2019.
(HPDC),pages37–48,2014. [51] B.Bevilacqua,F.Frasca,D.Lim,B.Srinivasan,C.Cai,G.Balamu-
[35] M. Besta and T. Hoefler. Accelerating irregular computations rugan, M. M. Bronstein, and H. Maron. Equivariant Subgraph
with hardware transactional memory and active messages. In AggregationNetworks,Mar.2022. arXiv:2110.02910[cs,stat].
Proceedingsofthe24thInternationalSymposiumonHigh-Performance [52] C.Bick,E.Gross,H.A.Harrington,andM.T.Schaub. Whatare
ParallelandDistributedComputing(HPDC),pages161–172,2015. higher-order networks? arXiv:2104.11329 [nlin, stat], Mar. 2022.
[36] M.BestaandT.Hoefler. Activeaccess:Amechanismforhigh- arXiv:2104.11329.
performance distributed data-centric computations. In Proceed- [53] J. C. W. Billings, M. Hu, G. Lerda, A. N. Medvedev, F. Mottes,
ingsofthe29thACMonInternationalConferenceonSupercomputing A. Onicas, A. Santoro, and G. Petri. Simplex2vec embeddings
(ICS),pages155–164,2015. forcommunitydetectioninsimplicialcomplexes. arXivpreprint
[37] M.BestaandT.Hoefler. Surveyandtaxonomyoflosslessgraph arXiv:1906.09068,2019.
compression and space-efficient graph representations. arXiv [54] M.Blasi,M.Freudenreich,J.Horvath,D.Richerby,andA.Scherp.
preprintarXiv:1806.01799,2018. Graphsummarizationwithgraphneuralnetworks.arXivpreprint
[38] M. Besta and T. Hoefler. Parallel and distributed graph neural arXiv:2203.05919,2022.
networks: An in-depth concurrency analysis. arXiv preprint [55] C.Bodnar,F.Frasca,N.Otter,Y.Wang,P.Lio,G.F.Montufar,and
arXiv:2205.09702,2022. M.Bronstein. Weisfeilerandlehmangocellular:Cwnetworks.
[39] M. Besta and T. Hoefler. Parallel and distributed graph neural Advances in Neural Information Processing Systems, 34:2625–2640,
networks: An in-depth concurrency analysis. IEEE Transactions 2021.
onPatternAnalysisandMachineIntelligence(TPAMI),2023. [56] C. Bodnar, F. Frasca, N. Otter, Y. G. Wang, P. Lio`, G. Montu´far,
[40] M.Besta,P.Iff,F.Scheidl,K.Osawa,N.Dryden,M.Podstawski, and M. Bronstein. Weisfeiler and Lehman Go Cellular: CW
T.Chen,andT.Hoefler. Neuralgraphdatabases. InLearningon Networks. Technical Report arXiv:2106.12575, arXiv, Jan. 2022.
GraphsConference,pages31–1.PMLR,2022. arXiv:2106.12575[cs,stat]type:article.
[41] M.Besta,R.Kanakagiri,G.Kwasniewski,R.Ausavarungnirun, [57] C.Bodnar,F.Frasca,Y.G.Wang,N.Otter,G.Montu´far,P.Lio`,and
J. Bera´nek, K. Kanellopoulos, K. Janda, Z. Vonarburg-Shmaria, M.Bronstein. WeisfeilerandLehmanGoTopological:Message23
Passing Simplicial Networks. arXiv:2103.03212 [cs], June 2021. ofdynamicknowledgegraphs. InIEEEICDE,pages1563–1565,
arXiv:2103.03212. 2017.
[58] A.Bordes,S.Chopra,andJ.Weston. QuestionAnsweringwith [83] C. J. Colbourn. On testing isomorphism of permu-
SubgraphEmbeddings,Sept.2014. tation graphs. Networks, 11(1):13–21, 1981. eprint:
[59] G. Bouritsas, F. Frasca, S. Zafeiriou, and M. M. Bronstein. Im- https://onlinelibrary.wiley.com/doi/pdf/10.1002/net.3230110103.
provingGraphNeuralNetworkExpressivityviaSubgraphIso- [84] C.Cooper,A.Frieze,andT.Radzik. Thecovertimesofrandom
morphism Counting. Technical Report arXiv:2006.09252, arXiv, walks on random uniform hypergraphs. Theoretical Computer
July2021. arXiv:2006.09252[cs,stat]type:article. Science,509:51–69,2013.
[60] G. Bouritsas, F. Frasca, S. Zafeiriou, and M. M. Bronstein. Im- [85] M.Copik,G.Kwasniewski,M.Besta,M.Podstawski,andT.Hoe-
provingGraphNeuralNetworkExpressivityviaSubgraphIso- fler. Sebs:Aserverlessbenchmarksuiteforfunction-as-a-service
morphismCounting,July2021. computing. In Proceedings of the 22nd International Middleware
[61] M.M.Bronstein,J.Bruna,T.Cohen,andP.Velicˇkovic´.Geometric Conference,pages64–78,2021.
DeepLearning:Grids,Groups,Graphs,Geodesics,andGauges. [86] L.Cotta,C.Morris,andB.Ribeiro. ReconstructionforPowerful
arXiv:2104.13478[cs,stat],May2021. arXiv:2104.13478. GraphRepresentations,Dec.2021. arXiv:2110.00577[cs].
[62] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Van- [87] A.Davoudian,L.Chen,andM.Liu. AsurveyonNoSQLstores.
dergheynst. Geometric deep learning: going beyond euclidean ACMComputingSurveys(CSUR),51(2):40,2018.
data. IEEESignalProcessingMagazine,34(4):18–42,2017.
[88] J. de Fine Licht, M. Besta, S. Meierhans, and T. Hoefler. Trans-
[63] R.Brossard,O.Frigo,andD.Dehaene. Graphconvolutionsthat
formations of high-level synthesis codes for high-performance
canfinallymodellocalstructure. arXivpreprintarXiv:2011.15069, computing. IEEETransactionsonParallelandDistributedSystems
2020. (TPDS),32(5):1014–1029,2020.
[64] A.BulucandJ.R.Gilbert. Challengesandadvancesinparallel
[89] S.DiGirolamo,D.DeSensi,K.Taranov,M.Malesevic,M.Besta,
sparse matrix-matrix multiplication. In 2008 37th International
T. Schneider, S. Kistler, and T. Hoefler. Building blocks for
ConferenceonParallelProcessing,pages503–510.IEEE,2008.
network-accelerated distributed file systems. In SC22: Inter-
[65] A. Buluc and J. R. Gilbert. Linear algebraic primitives for parallel national Conference for High Performance Computing, Networking,
computingonlargegraphs. Citeseer,2010. Storage and Analysis (ACM/IEEE Supercomputing), pages 1–14.
[66] A. Buluc¸ and J. R. Gilbert. The combinatorial blas: Design, IEEE,2022.
implementation,andapplications. IJHPCA,25(4):496–509,2011.
[90] S.DiGirolamo,K.Taranov,A.Kurth,M.Schaffner,T.Schneider,
[67] A.Buluc¸,J.R.Gilbert,andC.Budak. Solvingpathproblemson J. Bera´nek, M. Besta, L. Benini, D. Roweth, and T. Hoefler.
thegpu. ParallelComputing,36(5-6):241–253,2010. Network-acceleratednon-contiguousmemorytransfers. InPro-
[68] A. Buluc¸ and K. Madduri. Parallel breadth-first search on ceedingsoftheInternationalConferenceforHighPerformanceComput-
distributedmemorysystems. InProceedingsof2011International ing,Networking,StorageandAnalysis(ACM/IEEESupercomputing),
ConferenceforHighPerformanceComputing,Networking,Storageand pages1–14,2019.
Analysis,page65.ACM,2011.
[91] Y.Dong,W.Sawin,andY.Bengio. Hnhn:Hypergraphnetworks
[69] E.Bunch,Q.You,G.Fung,andV.Singh. Simplicial2-Complex withhyperedgeneurons. arXivpreprintarXiv:2006.12278,2020.
ConvolutionalNeuralNets. arXiv:2012.06010[math],Dec.2020.
[92] S. Ebli, M. Defferrard, and G. Spreemann. Simplicial Neural
arXiv:2012.06010.
Networks. arXiv:2010.03633 [cs, math, stat], Dec. 2020. arXiv:
[70] J.-Y.Cai,M.Fu¨rer,andN.Immerman. Anoptimallowerbound 2010.03633.
onthenumberofvariablesforgraphidentification. Combinator-
[93] T.Eliassi-Rad,V.Latora,M.Rosvall,andI.Scholtes.Higher-order
ica,12(4):389–410,Dec.1992.
graphmodels:Fromtheoreticalfoundationstomachinelearning
[71] Z.Cai,X.Yan,Y.Wu,K.Ma,J.Cheng,andF.Yu.Dgcl:anefficient
(dagstuhlseminar21352).InDagstuhlReports,volume11.Schloss
communicationlibraryfordistributedgnntraining. InEuroSys,
Dagstuhl-Leibniz-Zentrumfu¨rInformatik,2021.
2021.
[94] H.Fan,F.Zhang,Y.Wei,Z.Li,C.Zou,Y.Gao,andQ.Dai.Hetero-
[72] T.Carletti,F.Battiston,G.Cencetti,andD.Fanelli.Randomwalks
geneoushypergraphvariationalautoencoderforlinkprediction.
onhypergraphs. PhysicalreviewE,101(2):022308,2020.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
[73] I. Chami, S. Abu-El-Haija, B. Perozzi, C. Re´, and K. Murphy.
44(8):4125–4138,2021.
Machine learning on graphs: A model and comprehensive tax-
[95] W.Fan,S.Wang,J.Huang,Z.Chen,Y.Song,W.Tang,H.Mao,
onomy. arXivpreprintarXiv:2005.03675,2020.
H.Liu,X.Liu,D.Yin,etal. Graphmachinelearningintheera
[74] C. Chen, K. Li, W. Wei, J. T. Zhou, and Z. Zeng. Hierarchical
oflargelanguagemodels(llms). arXivpreprintarXiv:2404.14928,
graphneuralnetworksforfew-shotlearning. IEEETransactions
2024.
onCircuitsandSystemsforVideoTechnology,32(1):240–252,2021.
[96] J.Feng,Y.Chen,F.Li,A.Sarkar,andM.Zhang. Howpowerful
[75] H. Chen, R. A. Rossi, K. Mahadik, S. Kim, and H. Eldardiry.
arek-hopmessagepassinggraphneuralnetworks. Advancesin
Hypergraphneuralnetworksfortime-seriesforecasting. In2023
NeuralInformationProcessingSystems,35:4776–4790,2022.
IEEE International Conference on Big Data (BigData), pages 1076–
[97] Y.Feng,H.You,Z.Zhang,R.Ji,andY.Gao. HypergraphNeural
1080.IEEE,2023.
Networks,Feb.2019.Number:arXiv:1809.09401arXiv:1809.09401
[76] Z. Chen, L. Chen, S. Villar, and J. Bruna. Can graph neural
[cs,stat].
networks count substructures? Advances in neural information
processingsystems,33:10383–10395,2020. [98] M.FeyandJ.E.Lenssen.Fastgraphrepresentationlearningwith
pytorchgeometric. arXivpreprintarXiv:1903.02428,2019.
[77] Z.Chen,H.Mao,H.Li,W.Jin,H.Wen,X.Wei,S.Wang,D.Yin,
W.Fan,H.Liu,etal. Exploringthepotentialoflargelanguage [99] M.Fey,J.-G.Yuen,andF.Weichert. Hierarchicalinter-message
models(llms)inlearningongraphs. ACMSIGKDDExplorations passing for learning on molecular graphs. arXiv preprint
Newsletter,25(2):42–61,2024. arXiv:2006.12179,2020.
[78] Z. Chen, S. Villar, L. Chen, and J. Bruna. On the equivalence [100] F. Frasca, B. Bevilacqua, M. Bronstein, and H. Maron. Un-
between graph isomorphism testing and function approxima- derstanding and extending subgraph gnns by rethinking their
tion with GNNs. arXiv:1905.12560 [cs, stat], May 2019. arXiv: symmetries. Advances in Neural Information Processing Systems,
1905.12560. 35:31376–31390,2022.
[79] Z.Chen,M.Yan,M.Zhu,L.Deng,G.Li,S.Li,andY.Xie.fusegnn: [101] F.Frasca,E.Rossi,D.Eynard,B.Chamberlain,M.Bronstein,and
accelerating graph convolutional neural network training on F.Monti. Sign:Scalableinceptiongraphneuralnetworks. arXiv
gpgpu. InIEEE/ACMICCAD,2020. preprintarXiv:2004.11198,2020.
[80] E. Chien, C. Pan, J. Peng, and O. Milenkovic. You are allset: [102] G. Fu, P. Zhao, and Y. Bian. p-laplacian based graph neural
Amultisetfunctionframeworkforhypergraphneuralnetworks. networks. InInternationalConferenceonMachineLearning,pages
arXivpreprintarXiv:2106.13264,2021. 6878–6917.PMLR,2022.
[81] U.ChitraandB.Raphael. Randomwalksonhypergraphswith [103] S. Fu, W. Liu, Y. Zhou, and L. Nie. Hplapgcn: Hypergraph
edge-dependent vertex weights. In International conference on p-laplacian graph convolutional networks. Neurocomputing,
machinelearning,pages1172–1181.PMLR,2019. 362:166–174,2019.
[82] S. Choudhury, K. Agarwal, S. Purohit, B. Zhang, M. Pirrung, [104] S. K. Gajendran. A survey on NoSQL databases. University of
W. Smith, and M. Thomas. Nous: Construction and querying Illinois,2012.24
[105] F. Geerts. The expressive power of kth-order invariant graph [128] K.Hayashi,S.G.Aksoy,C.H.Park,andH.Park. Hypergraph
networks,July2020. Number:arXiv:2007.12035arXiv:2007.12035 random walks, laplacians, and clustering. In Proceedings of the
[cs,math,stat]. 29thacminternationalconferenceoninformation&knowledgeman-
[106] F. Geerts and J. L. Reutter. Expressiveness and Approxima- agement,pages495–504,2020.
tion Properties of Graph Neural Networks. Technical Report [129] M.Hein,S.Setzer,L.Jost,andS.S.Rangapuram.Thetotalvaria-
arXiv:2204.04661, arXiv, Apr. 2022. arXiv:2204.04661 [cs] type: tiononhypergraphs-learningonhypergraphsrevisited.Advances
article. inNeuralInformationProcessingSystems,26,2013.
[107] E.Georganas,A.Buluc¸,J.Chapman,L.Oliker,D.Rokhsar,and [130] H.A.Helfgott,J.Bajpai,andD.Dona. Graphisomorphismsin
K. Yelick. Parallel de bruijn graph construction and traversal quasi-polynomialtime,Oct.2017. arXiv:1710.04574[math].
fordenovogenomeassembly. InProceedingsoftheInternational [131] S. Heydari and L. Livi. Message Passing Neural Net-
ConferenceforHighPerformanceComputing,Networking,Storageand works for Hypergraphs, Apr. 2022. Number: arXiv:2203.16995
Analysis,pages437–448.IEEEPress,2014. arXiv:2203.16995[cs].
[108] R. Gerstenberger, M. Besta, and T. Hoefler. Enabling highly- [132] L.Hoang,X.Chen,H.Lee,R.Dathathri,G.Gill,andK.Pingali.
scalable remote memory access programming with mpi-3 one Efficientdistributionfordeeplearningonlargegraphs,2021.
sided. In Proceedings of the International Conference on High Per- [133] J. E. Hopcroft and J. K. Wong. Linear time algorithm for iso-
formanceComputing,Networking,StorageandAnalysis(ACM/IEEE morphismofplanargraphs(PreliminaryReport). InProceedings
Supercomputing),pages1–12,2013. ofthesixthannualACMsymposiumonTheoryofcomputing,STOC
[109] S.Ghose,A.Boroumand,J.S.Kim,J.Go´mez-Luna,andO.Mutlu. ’74,pages172–184,NewYork,NY,USA,Apr.1974.Association
Processing-in-Memory: A Workload-driven Perspective. IBM forComputingMachinery.
JRD,2019. [134] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta,
[110] L. Gianinazzi, T. Ben-Nun, M. Besta, S. Ashkboos, Y. Bau- andJ.Leskovec. OpenGraphBenchmark:DatasetsforMachine
mann, P. Luczynski, and T. Hoefler. The spatial computer: A LearningonGraphs. InAdvancesinNeuralInformationProcessing
model for energy-efficient parallel computation. arXiv preprint Systems,volume33,pages22118–22133.CurranAssociates,Inc.,
arXiv:2205.04934,2022. 2020.
[111] L. Gianinazzi, M. Besta, Y. Schaffner, and T. Hoefler. Parallel [135] Y.Huetal.Featgraph:Aflexibleandefficientbackendforgraph
algorithmsforfindinglargecliquesinsparsegraphs. InProceed- neuralnetworksystems. arXivpreprintarXiv:2008.11359,2020.
ingsofthe33rdACMSymposiumonParallelisminAlgorithmsand [136] J. Huang and J. Yang. Unignn: a unified framework for graph
Architectures(SPAA),pages243–253,2021. andhypergraphneuralnetworks.arXivpreprintarXiv:2105.00956,
[112] L. Gianinazzi, M. Fries, N. Dryden, T. Ben-Nun, M. Besta, and 2021.
T. Hoefler. Learning combinatorial node labeling algorithms. [137] X.Huang,K.Han,Y.Yang,D.Bao,Q.Tao,Z.Chai,andQ.Zhu.
arXivpreprintarXiv:2106.03594,2021. Gnnsasadaptersforllmsontext-attributedgraphs. InTheWeb
[113] D.Gibson,R.Kumar,andA.Tomkins. Discoveringlargedense Conference2024.
subgraphsinmassivegraphs. InVLDB,pages721–732,2005. [138] X.Huang,K.Han,Y.Yang,D.Bao,Q.Tao,Z.Chai,andQ.Zhu.
[114] J.Gilmer,S.S.Schoenholz,P.F.Riley,O.Vinyals,andG.E.Dahl. Cangnnbegoodadapterforllms? InProceedingsoftheACMon
Neuralmessagepassingforquantumchemistry. InInternational WebConference2024,pages893–904,2024.
ConferenceonMachineLearning,pages1263–1272.PMLR,2017. [139] P.Iff,M.Besta,M.Cavalcante,T.Fischer,L.Benini,andT.Hoefler.
[115] C. Giusti, R. Ghrist, and D. S. Bassett. Two’s company, three Hexamesh: Scaling to hundreds of chiplets with an optimized
(or more) is a simplex: Algebraic-topological tools for under- chiplet arrangement. In Design Automation Conference (DAC),
standinghigher-orderstructureinneuraldata. TechnicalReport 2023.
arXiv:1601.01704,arXiv,Jan.2016.arXiv:1601.01704[math,q-bio] [140] Z.Jiaetal. Improvingtheaccuracy,scalability,andperformance
type:article. ofgraphneuralnetworkswithroc. MLSys,2020.
[116] L. Giusti, C. Battiloro, L. Testa, P. Di Lorenzo, S. Sardellitti, [141] T.Jin,L.Cao,B.Zhang,X.Sun,C.Deng,andR.Ji. Hypergraph
and S. Barbarossa. Cell attention networks. arXiv preprint inducedconvolutionalmanifoldnetworks.InIJCAI,pages2670–
arXiv:2209.08179,2022. 2676,2019.
[117] C. W. J. Goh, C. Bodnar, and P. Lio`. Simplicial Attention [142] E. Jonas, J. Schleier-Smith, V. Sreekanti, C.-C. Tsai, A. Khandel-
Networks. TechnicalReportarXiv:2204.09455,arXiv,Apr.2022. wal,Q.Pu,V.Shankar,J.Carreira,K.Krauth,N.Yadwadkar,etal.
arXiv:2204.09455[cs,math]type:article. Cloud programming simplified: A berkeley view on serverless
[118] J. Grilli, G. Baraba´s, M. J. Michalska-Smith, and S. Allesina. computing. arXivpreprintarXiv:1902.03383,2019.
Higher-orderinteractionsstabilizedynamicsincompetitivenet- [143] R.K.Kaliyar. Graphdatabases:Asurvey. InICCCA,pages785–
workmodels. Nature,548(7666):210–213,Aug.2017. 790,2015.
[119] C. Hacker. k-simplex2vec: a simplicial extension of node2vec. [144] P. Kelly. A congruence theorem for trees. Pacific Journal of
arXivpreprintarXiv:2010.05636,2020. Mathematics, 7(1):961–968, Mar. 1957. Publisher: Mathematical
[120] M. Hajij, K. Istvan, and G. Zamzmi. Cell Complex Neural SciencesPublishers.
Networks,Mar.2021. arXiv:2010.00743[cs,math,stat]. [145] J.Kepner,P.Aaltonen,D.Bader,A.Buluc¸,F.Franchetti,J.Gilbert,
[121] M. Hajij, G. Zamzmi, T. Papamarkou, A. Guzm’an-S’aenz, D. Hutchison, M. Kumar, A. Lumsdaine, H. Meyerhenke, et al.
T.Birdal,andM.T.Schaub. Combinatorialcomplexes:Bridging Mathematicalfoundationsofthegraphblas. In2016IEEEHigh
the gap between cell complexes and hypergraphs. 2023 57th Performance Extreme Computing Conference (HPEC), pages 1–9.
AsilomarConferenceonSignals,Systems,andComputers,pages799– IEEE,2016.
803,2023. [146] J.Kepner,D.Bader,A.Buluc¸,J.Gilbert,T.Mattson,andH.Meyer-
[122] M.Hajij,G.Zamzmi,T.Papamarkou,N.Miolane,A.Guzma´n- henke.Graphs,matrices,andthegraphblas:Sevengoodreasons.
Sa´enz,andK.N.Ramamurthy.Higher-orderattentionnetworks. ProcediaComputerScience,51:2453–2462,2015.
arXivpreprintarXiv:2206.00606,2022. [147] S.Kiefer. PowerandlimitsoftheWeisfeiler-Lemanalgorithm.
[123] M.Hajij,G.Zamzmi,T.Papamarkou,N.Miolane,A.Guzma´n- [148] D. Kim and A. Oh. Efficient Representation Learning of Sub-
Sa´enz,K.N.Ramamurthy,T.Birdal,T.K.Dey,S.Mukherjee,S.N. graphs by Subgraph-To-Node Translation. Technical Report
Samaga,etal. Topologicaldeeplearning:Goingbeyondgraph arXiv:2204.04510, arXiv, Apr. 2022. arXiv:2204.04510 [cs] type:
data. article.
[124] W.Hamilton,Z.Ying,andJ.Leskovec. InductiveRepresentation [149] E.-S.Kim,W.Y.Kang,K.-W.On,Y.-J.Heo,andB.-T.Zhang. Hy-
Learning on Large Graphs. In Advances in Neural Information pergraphAttentionNetworksforMultimodalLearning. In2020
ProcessingSystems,volume30.CurranAssociates,Inc.,2017. IEEE/CVF Conference on Computer Vision and Pattern Recognition
[125] W.L.Hamilton. GraphRepresentationLearning. page141. (CVPR),pages14569–14578,June2020.
[126] W.L.Hamilton,R.Ying,andJ.Leskovec. RepresentationLearn- [150] T. N. Kipf and M. Welling. Semi-Supervised Classifica-
ing on Graphs: Methods and Applications. Technical Report tion with Graph Convolutional Networks. Technical Report
arXiv:1709.05584, arXiv, Apr. 2018. arXiv:1709.05584 [cs] type: arXiv:1609.02907, arXiv, Feb. 2017. arXiv:1609.02907 [cs, stat]
article. type:article.
[127] J.Han,E.Haihong,G.Le,andJ.Du.SurveyonNoSQLdatabase. [151] M. Kivela¨, A. Arenas, M. Barthelemy, J. P. Gleeson, Y. Moreno,
In 2011 6th international conference on pervasive computing and and M. A. Porter. Multilayer networks. Journal of complex
applications,pages363–366.IEEE,2011. networks,2(3):203–271,2014.25
[152] V. Kumar and A. Babu. Domain suitable graph database se- Neugraph: parallel deep neural network computation on large
lection: A preliminary report. In 3rd International Conference on graphs. InUSENIXATC,2019.
Advances in Engineering Sciences & Applied Mathematics, London, [173] S.Ma,R.Hu,L.Wang,X.Lin,andJ.Huai. Fastcomputationof
UK,pages26–29,2015. densetemporalsubgraphs. InICDE,pages361–372.IEEE,2017.
[153] G. Kwasniewski, T. Ben-Nun, L. Gianinazzi, A. Calotoiu, [174] Z. Mao, R. Wang, H. Li, Y. Huang, Q. Zhang, X. Liao, and
T. Schneider, A. N. Ziogas, M. Besta, and T. Hoefler. Pebbles, H. Ma. Ermer: a serverless platform for navigating, analyzing,
graphs, and a pinch of combinatorics: Towards tight i/o lower and visualizing escherichia coli regulatory landscape through
boundsforstaticallyanalyzableprograms. InProceedingsofthe graphdatabase. NucleicAcidsResearch,2022.
33rdACMSymposiumonParallelisminAlgorithmsandArchitectures [175] H.Maron,H.Ben-Hamu,H.Serviansky,andY.Lipman.Provably
(SPAA),pages328–339,2021. PowerfulGraphNetworks. arXiv:1905.11136[cs,stat],June2020.
[154] G. Kwasniewski, M. Kabic, T. Ben-Nun, A. N. Ziogas, J. E. [176] H.Maron,H.Ben-Hamu,N.Shamir,andY.Lipman. Invariant
Saethre, A. Gaillard, T. Schneider, M. Besta, A. Kozhevnikov, andEquivariantGraphNetworks.arXiv:1812.09902[cs,stat],Apr.
J.VandeVondele,etal.Ontheparalleli/ooptimalityoflinearal- 2019. arXiv:1812.09902.
gebrakernels:Near-optimalmatrixfactorizations. InProceedings [177] H.Maron,E.Fetaya,N.Segol,andY.Lipman. OntheUniver-
oftheInternationalConferenceforHighPerformanceComputing,Net- salityofInvariantNetworks. TechnicalReportarXiv:1901.09342,
working,StorageandAnalysis(ACM/IEEESupercomputing),pages arXiv,May2019. arXiv:1901.09342[cs,stat]type:article.
1–15,2021. [178] G. McGrath and P. R. Brenner. Serverless computing: Design,
[155] G.Kwasniewski,M.Kabic´,M.Besta,J.VandeVondele,R.Solca`, implementation,andperformance.In2017IEEE37thInternational
andT.Hoefler.Red-bluepebblingrevisited:nearoptimalparallel ConferenceonDistributedComputingSystemsWorkshops(ICDCSW),
matrix-matrix multiplication. In Proceedings of the International pages405–410.IEEE,2017.
ConferenceforHighPerformanceComputing,Networking,Storageand [179] V.Md,S.Misra,G.Ma,R.Mohanty,E.Georganas,A.Heinecke,
Analysis(ACM/IEEESupercomputing),pages1–22,2019. D.Kalamkar,N.K.Ahmed,andS.Avancha. Distgnn:Scalable
[156] T.LaRock,V.Nanumyan,I.Scholtes,G.Casiraghi,T.Eliassi-Rad, distributedtrainingforlarge-scalegraphneuralnetworks. arXiv
and F. Schweitzer. Hypa: Efficient detection of path anomalies preprintarXiv:2104.06700,2021.
intimeseriesdataonnetworks. InProceedingsofthe2020SIAM [180] B. Mendelson. Introduction to Topology: Third Edition. Courier
internationalconferenceondatamining,pages460–468.SIAM,2020. Corporation,Apr.2012. Google-Books-ID:FWFmoEUJSwkC.
[157] J. B. Lee, R. A. Rossi, X. Kong, S. Kim, E. Koh, and A. Rao. [181] C.Meng,S.C.Mouli,B.Ribeiro,andJ.Neville.SubgraphPattern
Graph convolutional networks with motif-based attention. In Neural Networks for High-Order Graph Evolution Prediction.
Proceedingsofthe28thACMinternationalconferenceoninformation page10.
andknowledgemanagement,pages499–508,2019. [182] S.Mittal. Asurveyoffpga-basedacceleratorsforconvolutional
[158] V.E.Lee,N.Ruan,R.Jin,andC.Aggarwal. Asurveyofalgo- neural networks. Neural computing and applications, 32(4):1109–
rithms for dense subgraph discovery. In Managing and Mining 1139,2020.
GraphData,pages303–336.Springer,2010. [183] F.Monti,K.Otness,andM.M.Bronstein.Motifnet:amotif-based
[159] J. Li, Y. Rong, H. Cheng, H. Meng, W. Huang, and J. Huang. graphconvolutionalnetworkfordirectedgraphs. In2018IEEE
Semi-SupervisedGraphClassification:AHierarchicalGraphPer- DataScienceWorkshop(DSW),pages225–228.IEEE,2018.
spective. arXiv:1904.05003[cs],Apr.2019. [184] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and
[160] S.Lietal. Pytorchdistributed:Experiencesonacceleratingdata M.Neumann.TUDataset:Acollectionofbenchmarkdatasetsfor
paralleltraining. arXivpreprintarXiv:2006.15704,2020. learningwithgraphs,July2020. arXiv:2007.08663[cs,stat].
[185] C.Morris,Y.Lipman,H.Maron,B.Rieck,N.M.Kriege,M.Grohe,
[161] Z. Lin,C. Li, Y. Miao, Y. Liu,and Y. Xu. Pagraph: Scaling gnn
M. Fey, and K. Borgwardt. Weisfeiler and leman go machine
trainingonlargegraphsviacomputation-awarecaching.InACM
learning:Thestorysofar. arXivpreprintarXiv:2112.09992,2021.
SoCC,2020.
[186] C. Morris, G. Rattan, and P. Mutzel. Weisfeiler and leman
[162] F. Liu, B. Yang, C. You, X. Wu, S. Ge, A. Woicik, and S. Wang.
go sparse: Towards scalable higher-order graph embeddings.
Graph-in-graphnetworkforautomaticgeneontologydescription
AdvancesinNeuralInformationProcessingSystems,33:21824–21840,
generation. InProceedingsofthe28thACMSIGKDDConferenceon
2020.
KnowledgeDiscoveryandDataMining,pages1060–1068,2022.
[187] C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen,
[163] H. Liu, S. Lu, X. Chen, and B. He. G3: when graph neural
G. Rattan, and M. Grohe. Weisfeiler and Leman Go Neural:
networksmeetparallelgraphprocessingsystemsongpus.VLDB,
Higher-orderGraphNeuralNetworks.arXiv:1810.02244[cs,stat],
2020.
Nov.2021. arXiv:1810.02244.
[164] N.Liu,X.Wang,L.Wu,Y.Chen,X.Guo,andC.Shi. Compact
[188] O.Mutluetal. ProcessingDataWhereItMakesSense:Enabling
graph structure learning via mutual information compression.
In-MemoryComputation. MicPro,2019.
InProceedingsoftheACMWebConference2022,pages1601–1610,
[189] O.Mutlu,S.Ghose,J.Go´mez-Luna,andR.Ausavarungnirun. A
2022.
modernprimeronprocessinginmemory.InEmergingComputing:
[165] S.Liu,L.Chen,H.Dong,Z.Wang,D.Wu,andZ.Huang.Higher-
FromDevicestoSystems:LookingBeyondMooreandVonNeumann,
order weighted graph convolutional networks. arXiv preprint
pages171–243.Springer,2022.
arXiv:1911.04129,2019.
[190] M. Muzychuk. A Solution of the Isomorphism
[166] Y.Liu,T.Safavi,A.Dighe,andD.Koutra. Graphsummarization Problem for Circulant Graphs. Proceedings of the
methods and applications: A survey. ACM Computing Surveys London Mathematical Society, 88(1):1–41, 2004. eprint:
(CSUR),51(3):1–34,2018.
https://onlinelibrary.wiley.com/doi/pdf/10.1112/S0024611503014412.
[167] Z.Liu,X.He,Y.Tian,andN.V.Chawla.Canwesoftpromptllms [191] S. Navlakha, R. Rastogi, and N. Shrivastava. Graph summa-
forgraphlearningtasks?InCompanionProceedingsoftheACMon rization with bounded error. In Proceedings of the 2008 ACM
WebConference2024,pages481–484,2024. SIGMOD international conference on Management of data, pages
[168] X. Lu, N. S. Islam, M. Wasi-Ur-Rahman, J. Jose, H. Subramoni, 419–432.ACM,2008.
H.Wang,andD.K.Panda. High-performancedesignofhadoop [192] M.Papillon,S.Sanborn,M.Hajij,andN.Miolane. Architectures
rpcwithrdmaoverinfiniband. In201342ndInternationalConfer- of topological deep learning: A survey on topological neural
enceonParallelProcessing,pages641–650.IEEE,2013. networks. arXivpreprintarXiv:2304.10031,2023.
[169] Y.Luo. Shine:Subhypergraphinductiveneuralnetwork. arXiv [193] P. A. Papp, K. Martinkus, L. Faber, and R. Wattenhofer.
preprintarXiv:2210.07309,2022. DropGNN: Random Dropouts Increase the Expressiveness of
[170] A.Ma,X.Wang,J.Li,C.Wang,T.Xiao,Y.Liu,H.Cheng,J.Wang, GraphNeuralNetworks,Nov.2021. arXiv:2111.06283[cs].
Y.Li,Y.Chang,etal. Single-cellbiologicalnetworkinferenceus- [194] T.P.PeixotoandM.Rosvall. Modellingsequencesandtemporal
ingaheterogeneousgraphtransformer. NatureCommunications, networkswithdynamiccommunitystructures. Naturecommuni-
14(1):964,2023. cations,8(1):582,2017.
[171] J.Ma,M.Wan,L.Yang,J.Li,B.Hecht,andJ.Teevan. Learning [195] J. Preskill. Quantum computing in the nisq era and beyond.
causal effects on hypergraphs. In Proceedings of the 28th ACM Quantum,2:79,2018.
SIGKDD Conference on Knowledge Discovery and Data Mining, [196] L. Pu and B. Faltings. Hypergraph learning with hyperedge
pages1202–1212,2022. expansion. In Machine Learning and Knowledge Discovery in
[172] L. Ma, Z. Yang, Y. Miao, J. Xue, M. Wu, L. Zhou, and Y. Dai. Databases: European Conference, ECML PKDD 2012, Bristol, UK,26
September 24-28, 2012. Proceedings, Part I 23, pages 410–425. [218] N.Shabani,J.Wu,A.Beheshti,Q.Z.Sheng,J.Foo,V.Haghighi,
Springer,2012. A. Hanif, and M. Shahabikargar. A comprehensive survey on
[197] C.Qian,G.Rattan,F.Geerts,M.Niepert,andC.Morris.Ordered graphsummarizationwithgraphneuralnetworks. IEEETrans-
subgraphaggregationnetworks. AdvancesinNeuralInformation actionsonArtificialIntelligence,2024.
ProcessingSystems,35:21030–21045,2022. [219] S. Sharma, S. Sharma, and A. Athaiya. Activation functions in
[198] Y. Qiao, X. Ao, Y. Liu, J. Xu, X. Sun, and Q. He. Login: A neuralnetworks. TowardsDataSci,6(12):310–316,2017.
large language model consultedgraph neural network training [220] E. Solomonik, M. Besta, F. Vella, and T. Hoefler. Scaling be-
framework. arXivpreprintarXiv:2405.13902,2024. tweenness centrality using communication-efficient sparse ma-
[199] E. Ranjan, S. Sanyal, and P. P. Talukdar. ASAP: Adaptive trix multiplication. In Proceedings of the International Conference
Structure Aware Pooling for Learning Hierarchical Graph Rep- forHighPerformanceComputing,Networking,StorageandAnalysis
resentations.TechnicalReportarXiv:1911.07979,arXiv,Feb.2020. (ACM/IEEESupercomputing),pages1–14,2017.
arXiv:1911.07979[cs,stat]type:article. [221] E. Solomonik, E. Carson, N. Knight, and J. Demmel. Tradeoffs
[200] H.Ren,M.Galkin,M.Cochez,Z.Zhu,andJ.Leskovec. Neural betweensynchronization,communication,andworkinparallel
graphreasoning:Complexlogicalqueryansweringmeetsgraph linearalgebracomputations. Technicalreport,2014.
databases. arXivpreprintarXiv:2303.14617,2023. [222] E.Solomonik,D.Matthews,J.Hammond,andJ.Demmel. Cy-
[201] X. Ren, J. Tang, D. Yin, N. Chawla, and C. Huang. A clops tensor framework: Reducing communication and elimi-
survey of large language models for graphs. arXiv preprint nating load imbalance in massively parallel contractions. In
arXiv:2405.08011,2024. 2013IEEE27thInternationalSymposiumonParallelandDistributed
Processing,pages813–824.IEEE,2013.
[202] M.Riondato,D.Garc´ıa-Soriano,andF.Bonchi. Graphsumma-
rization with quality guarantees. Data Mining and Knowledge [223] Y.Song,Y.Gu,T.Li,J.Qi,Z.Liu,C.S.Jensen,andG.Yu.Chgnn:
Discovery,31(2):314–349,2017. A semi-supervised contrastive hypergraph learning network.
arXivpreprintarXiv:2303.06213,2023.
[203] T. M. Roddenberry and S. Segarra. HodgeNet: Graph Neural
[224] D.S.Steiger,T.Ha¨ner,andM.Troyer. Projectq:anopensource
NetworksforEdgeData. arXiv:1912.02354[cs,eess],Dec.2019.
software framework for quantum computing. Quantum, 2:49,
[204] Y. Rong, W. Huang, T. Xu, and J. Huang. DropEdge: Towards
2018.
Deep Graph Convolutional Networks on Node Classification,
[225] A. Strausz, F. Vella, S. Di Girolamo, M. Besta, and T. Hoe-
Mar.2020. arXiv:1907.10903[cs,stat].
fler. Asynchronous distributed-memory triangle counting and
[205] R. A. Rossi, N. K. Ahmed, and E. Koh. Higher-order Network
lcc with rma caching. In 2022 IEEE International Parallel and
RepresentationLearning.InCompanionProceedingsoftheTheWeb
DistributedProcessingSymposium(IPDPS),2022.
Conference 2018, WWW ’18, pages 3–4, Republic and Canton of
[226] X. Sun, H. Yin, B. Liu, H. Chen, J. Cao, Y. Shao, and N. Q.
Geneva,CHE,Apr.2018.InternationalWorldWideWebConfer-
Viet Hung. Heterogeneous hypergraph embedding for graph
encesSteeringCommittee.
classification. In Proceedings of the 14th ACM international con-
[206] R. A. Rossi, N. K. Ahmed, E. Koh, S. Kim, A. Rao, and
ferenceonwebsearchanddatamining,pages725–733,2021.
Y. A. Yadkori. HONE: Higher-Order Network Embeddings.
[227] Y.SunandJ.Han. Miningheterogeneousinformationnetworks:
arXiv:1801.09303[cs,stat],May2018. arXiv:1801.09303.
principlesandmethodologies. SynthesisLecturesonDataMining
[207] M. Rosvall, A. V. Esquivel, A. Lancichinetti, J. D. West, and
andKnowledgeDiscovery,3(2):1–159,2012.
R.Lambiotte.Memoryinnetworkflowsanditseffectsonspread-
[228] E.Tekin,V.M.Savage,andP.J.Yeh.Measuringhigher-orderdrug
ingdynamicsandcommunitydetection. Naturecommunications,
interactions:Areviewofrecentapproaches. CurrentOpinionin
5(1):4630,2014.
SystemsBiology,4:16–23,Aug.2017.
[208] S. Sakr, A. Bonifati, H. Voigt, A. Iosup, K. Ammar, R. Angles,
[229] E.Tekin,V.M.Savage,andP.J.Yeh.Measuringhigher-orderdrug
W. Aref, M. Arenas, M. Besta, P. A. Boncz, et al. The future
interactions:Areviewofrecentapproaches. CurrentOpinionin
isbiggraphs:acommunityviewongraphprocessingsystems.
SystemsBiology,4:16–23,Aug.2017.
CommunicationsoftheACM,64(9):62–71,2021.
[230] E. H. Thiede, W. Zhou, and R. Kondor. Autobahn:
[209] D. Sandfelder, P. Vijayan, and W. L. Hamilton. Ego-GNNs: Automorphism-based Graph Neural Nets, Feb. 2022.
ExploitingEgoStructuresinGraphNeuralNetworks.InICASSP arXiv:2103.01710[cs].
2021 - 2021 IEEE International Conference on Acoustics, Speech
[231] J. Thorpe, Y. Qiao, J. Eyolfson, S. Teng, G. Hu, Z. Jia, J. Wei,
and Signal Processing (ICASSP), pages 8523–8527, June 2021.
K.Vora,R.Netravali,M.Kim,etal.Dorylus:Affordable,scalable,
arXiv:2107.10957[cs].
andaccurategnntrainingoverbillion-edgegraphs.arXivpreprint
[210] A. E. Sariyuce, C. Seshadhri, A. Pinar, and U. V. Catalyurek. arXiv:2105.11118,2021.
Findingthehierarchyofdensesubgraphsusingnucleusdecom- [232] C. Tian, L. Ma, Z. Yang, and Y. Dai. Pcgcn: Partition-centric
positions. In Proceedings of the 24th International Conference on processing for accelerating graph convolutional network. In
WorldWideWeb,pages927–937,2015. IPDPS,pages936–945.IEEE,2020.
[211] R. Sato. A Survey on The Expressive Power of Graph Neural [233] Y.Tian,R.A.Hankins,andJ.M.Patel. Efficientaggregationfor
Networks,Oct.2020. arXiv:2003.04078[cs,stat]. graphsummarization. InProceedingsofthe2008ACMSIGMOD
[212] M. T. Schaub, A. R. Benson, P. Horn, G. Lippner, and A. Jad- international conference on Management of data, pages 567–580.
babaie.RandomWalksonSimplicialComplexesandthenormal- ACM,2008.
izedHodge1-Laplacian. SIAMReview,62(2):353–391,Jan.2020. [234] L.Torres,A.S.Blevins,D.Bassett,andT.Eliassi-Rad. Thewhy,
[213] M. T. Schaub, Y. Zhu, J.-B. Seby, T. M. Roddenberry, and how, and when of representations for complex systems. SIAM
S.Segarra. Signalprocessingonhigher-ordernetworks:Livin’on Review,63(3):435–485,2021.
theedge...andbeyond. SignalProcessing,187:108149,2021. [235] A.Tripathy,K.Yelick,andA.Buluc¸. Reducingcommunication
[214] T.Schnake,O.Eberle,J.Lederer,S.Nakajima,K.T.Schu¨tt,K.-R. ingraphneuralnetworktraining. InACM/IEEESupercomputing,
Mu¨ller, and G. Montavon. Higher-order explanations of graph 2020.
neuralnetworksviarelevantwalks. IEEEtransactionsonpattern [236] K. Tu, P. Cui, X. Wang, F. Wang, and W. Zhu. Structural
analysisandmachineintelligence,44(11):7581–7596,2021. deepembeddingforhyper-networks. InProceedingsoftheAAAI
[215] I. Scholtes, N. Wider, R. Pfitzner, A. Garas, C. J. Tessone, and ConferenceonArtificialIntelligence,volume32,2018.
F. Schweitzer. Causality-driven slow-down and speed-up of [237] P.Velicˇkovic´,G.Cucurull,A.Casanova,A.Romero,P.Lio`,and
diffusioninnon-markoviantemporalnetworks.Naturecommuni- Y.Bengio.GraphAttentionNetworks.arXiv:1710.10903[cs,stat],
cations,5(1):5024,2014. Feb.2018. arXiv:1710.10903.
[216] A. C. Schwarze and M. A. Porter. Motifs for processes on [238] R. Waleffe, J. Mohoney, T. Rekatsinas, and S. Venkataraman.
networks.SIAMJournalonAppliedDynamicalSystems,20(4):2516– Marius++: Large-scale training of graph neural networks on a
2557,2021. singlemachine. arXivpreprintarXiv:2202.02365,2022.
[217] V.Seshadri,D.Lee,T.Mullins,H.Hassan,A.Boroumand,J.Kim, [239] C.Wan,Y.Li,C.R.Wolfe,A.Kyrillidis,N.S.Kim,andY.Lin.
M. A. Kozuch, O. Mutlu, P. B. Gibbons, and T. C. Mowry. Pipegcn: Efficient full-graph training of graph convolutional
Ambit:In-memoryacceleratorforbulkbitwiseoperationsusing networkswithpipelinedfeaturecommunication. arXivpreprint
commodity dram technology. In Proceedings of the 50th Annual arXiv:2203.10428,2022.
IEEE/ACM International Symposium on Microarchitecture, pages [240] H.Wang,D.Lian,Y.Zhang,L.Qin,andX.Lin. GoGNN:Graph
273–287.ACM,2017. of Graphs Neural Network for Predicting Structured Entity In-27
teractions. In Proceedings of the Twenty-Ninth International Joint [264] H.Yao,F.Wu,J.Ke,X.Tang,Y.Jia,S.Lu,P.Gong,J.Ye,andZ.Li.
Conference on Artificial Intelligence, pages 1317–1323, July 2020. Deep Multi-View Spatial-Temporal Network for Taxi Demand
arXiv:2005.05537[cs]. Prediction,Feb.2018.
[241] L. Wang, Q. Yin, C. Tian, J. Yang, R. Chen, W. Yu, Z. Yao, and [265] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and
J.Zhou.Flexgraph:aflexibleandefficientdistributedframework K. Narasimhan. Tree of thoughts: Deliberate problem solving
forgnntraining. InEuroSys,2021. with large language models. Advances in Neural Information
[242] M.Wang,D.Zheng,Z.Ye,Q.Gan,M.Li,X.Song,J.Zhou,C.Ma, ProcessingSystems,36,2024.
L.Yu,Y.Gai,etal. Deepgraphlibrary:Agraph-centric,highly- [266] J. Yi and J. Park. Hypergraph convolutional recurrent neural
performantpackageforgraphneuralnetworks.arXiv:1909.01315, network. In Proceedings of the 26th ACM SIGKDD international
2019. conferenceonknowledgediscovery&datamining,pages3366–3376,
[243] P.Wang,S.Yang,Y.Liu,Z.Wang,andP.Li. Equivarianthyper- 2020.
graphdiffusionneuraloperators.arXivpreprintarXiv:2207.06680, [267] R.Ying,R.He,K.Chen,P.Eksombatchai,W.L.Hamilton,and
2022. J. Leskovec. Graph Convolutional Neural Networks for Web-
Scale Recommender Systems. In Proceedings of the 24th ACM
[244] R.Wang,B.Li,S.Hu,W.Du,andM.Zhang. KnowledgeGraph
SIGKDD International Conference on Knowledge Discovery & Data
Embedding via Graph Attenuated Attention Networks. IEEE
Access,8:5212–5224,2020.
Mining,pages974–983,July2018.
[268] B.Yu,H.Yin,andZ.Zhu.Spatio-TemporalGraphConvolutional
[245] S. Wang, Y. Zhang, X. Lin, Y. Hu, Q. Huang, and B. Yin. Dy-
Networks:ADeepLearningFrameworkforTrafficForecasting.
namichypergraphstructurelearningformultivariatetimeseries
In Proceedings of the Twenty-Seventh International Joint Conference
forecasting. IEEETransactionsonBigData,2024.
onArtificialIntelligence,pages3634–3640,July2018.
[246] Y.Wangetal. Gnnadvisor:Anefficientruntimesystemforgnn
[269] M.Zaheer,S.Kottur,S.Ravanbakhsh,B.Poczos,R.Salakhutdi-
accelerationongpus. InOSDI,2021.
nov,andA.Smola. DeepSets,Apr.2018. arXiv:1703.06114[cs,
[247] Z.Wang,Q.Cao,H.Shen,B.Xu,andX.Cheng. TwinWeisfeiler-
stat].
Lehman:HighExpressiveGNNsforGraphClassification. Tech-
[270] H. Zhan, K. Zhang, Z. Chen, and V. S. Sheng. Simplex2vec
nicalReportarXiv:2203.11683,arXiv,Mar.2022.arXiv:2203.11683
backward:Fromvectorsbacktosimplicialcomplex. InProceed-
[cs]type:article.
ings of the 32nd ACM International Conference on Information and
[248] J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V. KnowledgeManagement,pages4405–4409,2023.
Le,D.Zhou,etal. Chain-of-thoughtpromptingelicitsreasoning [271] C. Zhang, D. Song, C. Huang, A. Swami, and N. V. Chawla.
inlargelanguagemodels.Advancesinneuralinformationprocessing Heterogeneousgraphneuralnetwork. InKDD,pages793–803,
systems,35:24824–24837,2022.
2019.
[249] B. Y. Weisfeiler and A. A. Leman. The Reduction of a Graph [272] D. Zhang et al. Agl: a scalable system for industrial-purpose
to Canonical Form and the Algebra which Appears therein. graphmachinelearning. arXivpreprintarXiv:2003.02454,2020.
Nauchno-TechnicheskayaInformatsia,9:11,1968. [273] J. Zhang, F. Li, X. Xiao, T. Xu, Y. Rong, J. Huang, and
[250] Y.Wu,K.Ma,Z.Cai,T.Jin,B.Li,C.Zheng,J.Cheng,andF.Yu. Y. Bian. Hypergraph convolutional networks via equivalency
Seastar:vertex-centricprogrammingforgraphneuralnetworks. between hypergraphs and undirected graphs. arXiv preprint
InEuroSys,2021. arXiv:2203.16939,2022.
[251] Z.Wuetal. Acomprehensivesurveyongraphneuralnetworks. [274] L. Zhang, Z. Lai, S. Li, Y. Tang, F. Liu, and D. Li. 2pgraph:
IEEETransactionsonNeuralNetworksandLearningSystems,2020. Acceleratinggnntrainingoverlargegraphsongpuclusters. In
[252] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. IEEECLUSTER,2021.
A Comprehensive Survey on Graph Neural Networks. IEEE [275] M.ZhangandP.Li. NestedGraphNeuralNetworks,Oct.2021.
TransactionsonNeuralNetworksandLearningSystems,32(1):4–24, arXiv:2110.13197[cs].
Jan.2021. arXiv:1901.00596. [276] N. Zhang, Y. Tian, and J. M. Patel. Discovery-driven graph
[253] J. Xu, T. L. Wickramarathne, and N. V. Chawla. Represent- summarization. In Data Engineering (ICDE), 2010 IEEE 26th
ing higher-order dependencies in networks. Science advances, InternationalConferenceon,pages880–891.IEEE,2010.
2(5):e1600028,2016. [277] R. Zhang, Y. Zou, and J. Ma. Hyper-sagnn: a self-attention
[254] J. Xu, Z. Wu, M. Lin, X. Zhang, and S. Wang. Llm and gnn based graph neural network for hypergraphs. arXiv preprint
arecomplementary:Distillingllmformultimodalgraphlearning. arXiv:1911.02613,2019.
arXivpreprintarXiv:2406.01032,2024. [278] W.Zhang,Y.Shen,Z.Lin,Y.Li,X.Li,W.Ouyang,Y.Tao,Z.Yang,
[255] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are and B. Cui. Gmlp: Building scalable and flexible graph neural
graphneuralnetworks? arXivpreprintarXiv:1810.00826,2018. networkswithfeature-messagepassing. arXiv:2104.09880,2021.
[279] W.Zhang,Y.Shen,Z.Lin,Y.Li,X.Li,W.Ouyang,Y.Tao,Z.Yang,
[256] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How Powerful are
Graph Neural Networks? arXiv:1810.00826 [cs, stat], Feb. 2019. and B. Cui. Pasca: A graph neural architecture search system
underthescalableparadigm. InACMTheWebConf,2022.
arXiv:1810.00826.
[280] Z. Zhang, P. Cui, and W. Zhu. Deep Learning on Graphs: A
[257] N.Yadati. Neuralmessagepassingformulti-relationalordered
Survey. IEEE Transactions on Knowledge and Data Engineering,
and recursive hypergraphs. Advances in Neural Information Pro-
34(01):249–270,Jan.2022. Publisher:IEEEComputerSociety.
cessingSystems,33:3275–3289,2020.
[281] C.Zhao,Y.Xin,X.Li,H.Zhu,Y.Yang,andY.Chen.AnAttention-
[258] N. Yadati, M. Nimishakavi, P. Yadav, V. Nitin, A. Louis, and
BasedGraphNeuralNetworkforSpamBotDetectioninSocial
P.Talukdar. Hypergcn:Anewmethodfortraininggraphconvo-
Networks. AppliedSciences,10(22):8160,Jan.2020.
lutionalnetworksonhypergraphs.Advancesinneuralinformation
[282] L.Zhao,W.Jin,L.Akoglu,andN.Shah.FromStarstoSubgraphs:
processingsystems,32,2019.
UpliftingAnyGNNwithLocalStructureAwareness,Apr.2022.
[259] C. Yang, A. Buluc¸, and J. D. Owens. Graphblast: A high-
arXiv:2110.03753[cs,stat].
performancelinearalgebra-basedgraphframeworkonthegpu.
[283] D.Zheng,X.Song,C.Yang,D.LaSalle,Q.Su,M.Wang,C.Ma,
ACM Transactions on Mathematical Software (TOMS), 48(1):1–51,
and G. Karypis. Distributed hybrid cpu and gpu training for
2022.
graphneuralnetworksonbillion-scalegraphs. arXiv:2112.15345,
[260] D. Yang, B. Qu, J. Yang, and P. Cudre´-Mauroux. Lbsn2vec++: 2021.
Heterogeneoushypergraphembeddingforlocation-basedsocial [284] D.Zhou,J.Huang,andB.Scho¨lkopf.Learningwithhypergraphs:
networks. IEEETransactionsonKnowledgeandDataEngineering, Clustering, classification, and embedding. Advances in neural
34(4):1843–1855,2020. informationprocessingsystems,19,2006.
[261] M. Yang and E. Isufi. Convolutional learning on simplicial [285] J.Zhouetal. Graphneuralnetworks:Areviewofmethodsand
complexes. arXivpreprintarXiv:2301.11163,2023. applications. AIOpen,1:57–81,2020.
[262] M.Yang,E.Isufi,andG.Leus. SimplicialConvolutionalNeural [286] J.Zhu,B.Li,Z.Zhang,L.Zhao,andH.Li.High-ordertopology-
Networks.InICASSP2022-2022IEEEInternationalConferenceon enhanced graph convolutional networks for dynamic graphs.
Acoustics,SpeechandSignalProcessing(ICASSP),pages8847–8851, Symmetry,14(10):2218,2022.
May2022. ISSN:2379-190X. [287] R.Zhuetal. Aligraph:Acomprehensivegraphneuralnetwork
[263] R.Yang,F.Sala,andP.Bogdan. Efficientrepresentationlearning platform. arXivpreprintarXiv:1902.08730,2019.
forhigher-orderdatawithsimplicialcomplexes. InLearningon
GraphsConference,pages13–1.PMLR,2022.