On the Content Bias in Fre´chet Video Distance
SongweiGe1 AniruddhaMahapatra2,3 GauravParmar2
Jun-YanZhu2 Jia-BinHuang1
1UniversityofMaryland,CollegePark 2CarnegieMellonUniversity 3AdobeResearch
https://content-debiased-fvd.github.io/
x x x
t t t
(a)MediumSpatial&NoTemporal (b)SmallSpatial&SevereTemporal
ReferenceVideos
Corruption Corruption
FVD=317.10 FVD=310.52
Figure1.FVDisbiasedtowardsper-framequalitythantemporalconsistency.FVD[72],acommonlyusedvideogenerationevaluation
metric,shouldideallycapturebothspatialandtemporalaspects.However,ourexperimentsrevealastrongbiastowardindividualframe
quality.(a)First,weapplymildspatialdistortionsthroughlocalwarping,whichresultsinanFVDscoreof317.10.(b)Next,weinduce
slightlylessspatialcorruptionsbutseveretemporalinconsistenciesbyalteringeachframedifferently.Thesechangescreateartifactsthatare
noticeabletohumansandevidentinthespatiotemporalx-tslice,asseeninthebottomrow,butsurprisinglyleadtoalower(better)FVD
scoreof310.52.Thisdiscrepancyhighlightsthemetric’sbiastowardsindividualframequality.Weencouragereaderstoviewthevideos
withAcrobatReaderorvisitourwebsitetoobservetheinconsistencies.
Abstract fromtherecentlarge-scaleself-supervisedvideomodelsis
lessbiasedtowardimagequality. Finally,werevisitafew
real-worldexamplestovalidateourhypothesis.
Fre´chet VideoDistance(FVD), aprominentmetricfor
evaluating video generation models, is known to conflict
withhumanperceptionoccasionally. Inthispaper,weaim 1.Introduction
toexploretheextentofFVD’sbiastowardper-framequal-
ityovertemporalrealismandidentifyitssources. Wefirst Videogeneration[5,9,12,21,22,30,62]hasrecentlyac-
quantifytheFVD’ssensitivitytothetemporalaxisbydecou- complishedunprecedentedadvancesdrivenbythescalable
plingtheframeandmotionqualityandfindthattheFVD models [29, 65] and growing training data [4, 57]. With
increasesonlyslightlywithlargetemporalcorruption. We rapidprogress,itisincreasinglycrucialtoevaluatethemodel
thenanalyzethegeneratedvideosandshowthatviacareful performanceaccurately. Despitethefruitfulliteratureonas-
samplingfromalargesetofgeneratedvideosthatdonot sessingvideoquality[40,58,63,81]anddesigningimage
containmotions,onecandrasticallydecreaseFVDwithout generationevaluationmetrics[28,38,47,48,55],automat-
improvingthetemporalquality. BothstudiessuggestFVD’s icallyevaluatingthequalityanddiversityofthegenerated
biastowardsthequalityofindividualframes. Wefurtherob- videos, has received less attention [33, 44]. In this paper,
servethatthebiascanbeattributedtothefeaturesextracted wefocusonanalyzingthebiasofFre´chetVideoDistance
from a supervised video classifier trained on the content- (FVD) [72], one of the most frequently used metrics for
biaseddataset. WeshowthatFVDwithfeaturesextracted videoevaluation.
4202
rpA
81
]VC.sc[
1v19321.4042:viXra
soediV
ecilst-xFVDextendstheimagegenerationmetricFre´chetIncep- recognition on the Kinetics-400 dataset [14]. The feature
tionDistance(FID)[28]tomeasurethequalityanddiversity spaceisformedastheoutputofthelogitlayer. Asaresult,
ofgeneratedvideoswithrespecttothetrainingset. GivenN thesefeaturesfocusonextractingthesemanticinformation
featuresf forasetofgeneratedandrealvideos,whichare abouthumanactionsinthevideos.
i
columnvectorsextractedfromapretrainedvideonetwork, UsingI3Dfeaturesthusraisesseveralpracticalconcerns
wefitamultivariateGaussianwiththemeanµ= 1 (cid:80) f , thatcouldunderminethemetric’sreliability. First,theKi-
andcovarianceΣ = 1 (cid:80) (f −µ)(f −µ)T. TheN perfi ori - netics dataset [14] predominantly comprises videos with
N i i i
manceofthevideogeneratoristhenmeasuredastheFre´chet humans as the protagonists. However, video content di-
distance[19]betweenthetwoGaussiandistributions: vergingfromtypicalKinetics-400categories,suchastime-
(cid:16) (cid:17) lapselandscapevideos[86]orfirst-personridingandbiking
FVD=∥µ r−µ g∥2 2+Tr Σ r+Σ g−2(Σ rΣ g)1 2 , (1) videos[11],maynotproduceameaningfulfeaturerepresen-
tation.Second,themodelstrainedontheKineticsdatasetare
where(µ ,Σ )and(µ ,Σ )denotethemeanandcovari-
r r g g shownbiasedtotheappearanceofobjectsandbackgrounds
anceforrealandgenerateddata.
instead of motions [16, 32, 42, 43, 59, 75]. For example,
EarlierstudieshaveconfirmedthatFVDreliablyreflects
torecognizetheaction“playingsaxophone”,itissufficient
the model performance in various cases, such as training
todetectthepresenceofasaxophonesincethisistheonly
convergence[92], hyperparametertuning[11,30], andar-
category where a saxophone is presented. Therefore, the
chitecturedesign[31,64]. However,severalrecentstudies
featuresmaynotcapturethemusician’smotion. Previous
have reported cases where FVD scores contradict human
worksalsoshowthatdescentclassificationaccuracycanbe
judgment [11, 20, 64]. A recurrent argument is that FVD
achievedwithoutmodelingthetemporalaspect[6,14].
tendstovaluetheimagequalityofindividualframesmore
Toverifyourhypothesis,wecomputeFVDscoresusing
thantherealismofmotion. Werefertosuchbiasasthecon-
featuresextractedfromaself-supervisedmodel[76]trained
tentbias,whichisinspiredbythevideogenerationworksin
ondiversedatasetandperformthesameanalysis. Overall,
decouplingcontentandmotion[34,69,71,73,79].
our experiments show that the FVD, computed with I3D
AsshowninFigure1,wemotivateouranalysiswitha
features,isstronglybiasedtothecontentoverthemotion,
simple,controlledsetting,wherethemetricdivergesfrom
whileusingfeaturescomputedwithamodeltrainedinaself-
humanperceptionwhenweighingspatialandtemporalqual-
supervisedmannerhelpsmitigatesuchbiastoalargeextent.
ities. Specifically,givenasetofreferencevideos,wecreate
Ourevaluationcodeanddataareavailableonhttps://
two sets of distortion. In (a), we locally warp the frames
content-debiased-fvd.github.io/.
ineachvideouniformly,whilein(b),wedistorttheframes
differentlybutwithslightlyreducedseverity. Thelattercre- 2.RelatedWork
atesadditionaltemporalartifacts. TheFVDmetric,however,
favorsvideoset(b),whilemosthumanswouldpickvideo Video generation. Various types of generative models
set(a)tobemoresimilartothereferencevideosduetothe havebeenproposedforvideogenerationsuchasGANs[54,
significanttemporalinconsistencypresentedinvideoset(b). 60, 69, 71, 73, 74, 79], Autoregressive models [3, 15, 18,
Buildinguponthissimpleexample,wepresentthefirst 20, 37, 50, 67, 82, 83, 87–90], and implicit neural repre-
systematicstudytoquantifythecontentbiasandunderstand sentations [64, 91]. Following the recent success of text-
itsimpactusingbothsyntheticandreal-worldsettings. We to-imageDiffusionModels[49,51,53],severalworksaim
firstdistortvideossothattheframequalitydeterioratesto to achieve high-quality results for the text-to-video task.
thesamelevelwhilethetemporalconsistencyiseitherintact Theseworksleveragediffusionprocesseitherinthepixel
or,intheothercase,significantlydecreased. Bycomparing space[21,30,62]orlatentspace[2,8,9,24,25,36,45,77,
FVDsonthesedistortedvideos,wecanquantifytherelative 78,80,80,84,85,95]orboth[93]. Reliablyevaluatingthe
sensitivityoftheFVDmetrictothetemporalconsistency. abovemodelsremainsachallenge. Currentworksprimarily
Next,followingthepreviousworkonFIDanalysis[39],we relyonFVD[72]andhumanperceptualstudy. Whileauser
probetheperceptualnullspaceintheFVDmetric. Without studycanreflecthumanpreferencemoreaccurately,FVD
improvingthetemporalqualityofthegeneratedvideos,we servesasamorescalableevaluationprotocol. Inthispaper,
canstillgreatlyreducetheFVDscores. Lastly,werevisit weaimtobetterunderstandwhataspectstheFVDmetric
afewreal-worldexampleswhereFVDpresentsanotable valuesmore. Specifically,weanalyzeitssensitivitytothe
contentbias. spatialversustemporalquality.
Where does the content bias originate from? Previous
studies show that the alignment of the FID metric to hu- Evaluation metrics for image and video generation.
manperceptiondependsonthechoiceoftheextractedfea- Manystudieshavefocusedonunderstandingandimprov-
tures[1,10,39,46]. Inpractice,FVDemploysanInflated ingtheevaluationmetricsforimagegeneration,suchasIn-
3DConvNet(I3D)model[14],originallytrainedforaction ception Score [56], FID [28, 39, 46, 48], Perceptual PathSpatialOnly Spatiotemporal
Spatially
Distorted Videos
Samedistortion Spatial
applied to each frame FVD
Reference
Videos
(undistorted)
Differentdistortion Spatiotemporal
to each frame FVD
Spatio-Temporally
Distorted Videos
OriginalVideos DistortedVideos
Figure2. AnalyzingtheFVD’ssensitivitytotemporalconsis-
tency. Wedistort the same set of videos inspatial only or spa- Figure3. Visualizationofthespatialandspatiotemporalcor-
tiotemporalmannerssothattheresultingvideoshavesimilarframe ruptions.Bothcorruptionsyieldsimilarframequality,whilethe
qualityyetonlydifferintemporalquality.BycomparingtheFVD spatiotemporal corruption induces additional temporal inconsis-
scoresofthetwodistortedvideosets,weaimatquantifyingthe tencyinthevideo.BycomparingtheFVDofthespatiotemporal
temporalsensitivityofthemetric. corruptionwiththespatialcorruption, weanalyzethetemporal
sensitivityofthemetric.BestviewedwithAcrobatReader.Please
checkourwebsiteforvideos.
Length [35], and precision and recall [38, 55]. Among
plingthetwoaspectsisnon-trivial[32]. Forexample,poor
them, FID is the most commonly adopted one, using the
framequalitywouldhinderthecreationofanaturalmotion.
Inception-V3featureextractor[68]trainedontheImageNet
Asaresult,previousapproachessuggestcreatingvideosby
dataset[17]. However, itcansometimesdivergefromhu-
stitchingrealframestogether,albeitinanincorrectorderor
manjudgment,especiallyontheout-of-domaindatasetslike
fromdifferentvideos[32,59,72].
humanfaces[46,96].
Whilethismethodisusefulforanalyzingvideodatasets,
Toaddresstheaboveissue,researchershaveintroduced
itmaynotbeidealforunderstandingavideogenerationmet-
several variants [7, 38, 47, 55] and performed analysis to
ric. Thisisbecausegeneratedvideosrarelycontainframes
understandFID[1,10,39,48]. Forinstance,Kynka¨a¨nniemi
fromirrelevantvideosorarrangeframesinincorrectorder.
etal.[39]studytheroleoftrainingdataclassesintheFID
In contrast, we carefully design distortions that simulate
metricandadvocatetheuseoftheCLIPmodelasthefeature
realscenariostoquantifyFVD’ssensitivitytospatialand
extractorinstead. KID[7]isproposedtoimproveFIDusing
temporalvideoquality.
the squared Maximum Mean Discrepancy (MMD) with a
polynomialkernel. KIDrelaxestheGaussianassumptionin Videodistortionmethods. Weillustrateourmethodfor
FIDandrequiresfewersamplestocompute. Clean-FID[48] addingdistortionsinFigure2. Weapplytworelevantdis-
shows that the aliasing issue caused by the preprocessing tortionstothesamesetofrealvideos,aimingtosynthesize
steps could significantly affect the FID scores. Similarly, videoswithsimilarframequalitydegradationbutlargedif-
Skorokhodovetal.[64]studiesthe“low-level”preprocess- ferencesintemporalquality. Weemployconventionalimage
ingoperationsinFVD,suchasresizingandframesampling distortionmethods[27,94]includingelastictransformation
strategies. However, theanalysisandimprovementofthe andmotionblur. Theelastictransformationlocallystretches
FVDaremuchlessexploredthanthoseofFID. and contrasts the frames, while the motion blur averages
imagepixelsalongaspecificdirectionofmotion. Weapply
3.QuantifyingtheTemporalSensitivityofFVD thesamedistortionstoeachframetoachieveaconsistent
framequalitydrop.
We examine the significance of temporal quality and con-
To create spatiotemporal corruptions, as shown in Fig-
sistency in FVD calculation. Recent studies suggest that
ure 3, we apply randomly sampled elastic transformation
modelstrainedontheKineticsdatasetsmaynotfullylever-
parametersorblurkernelsforeachframe. Thisprocedureal-
agethemotioninformation[6,32,43,59],raisingasimilar
lowsustointroducetemporalinconsistencywhileproducing
questionaboutwhethertheI3DfeaturesinFVDtrulycap-
similarframequalitytospatialcorruptions.
turethemotionqualityofvideos. Onewaytounderstand
videomotionvs. contentistoundermineoneaspectthrough Experimentalsetups. Afterdistortingthevideosusingspa-
eitherspatialortemporaldistortion. However,fullydecou- tialonlyandspatiotemporalmethods,wecomputetheFVDTable1.AnalyzingFVDtemporalsensitivitywithvideodistortions.Weapplyspatialonlyorspatiotemporaldistortionstothevideos.
ThetwovideosetssharesimilarframequalityasassessedbyFID.WethususetheFVDratiotomeasurethetemporalsensitivityofFVD.
Metric Distortion UCF-101 SkyTime-lapse FaceForensics Taichi-HD SSv2 Kinectics-400
Spatial 133.15 79.11 80.42 169.76 100.65 112.22
FID
Spatiotemporal 133.69 79.35 79.57 170.10 100.62 112.85
(+0.4%) (+0.3%) (−1.1%) (+0.2%) (−0.0%) (+0.6%)
Spatial 1460.18 211.08 354.49 1016.78 594.68 996.71
FVD
Spatiotemporal 1705.27 286.39 367.35 1201.35 678.08 1155.53
(+16.8%) (+35.7%) (+3.6%) (+18.2%) (+14.0%) (+15.9%)
I3D VideoMAE-v2-K710 VideoMAE-v2-SSv2 VideoMAE-v2-PT I3D TimeSformer-K400 VideoMAE-v1-K400 VideoMAE-v2-K710
05 05
05 05
600 600 100
200
80
400 400 150
60
100
40
200 200
20 50
0 0 0 0
(a)MotionBlur (b)ElasticTransformation (a)MotionBlur (b)ElasticTransformation
Figure4.FVDsensitivitywithdifferentvideofeatureextractors.
WeshowthatbysubstitutingtheI3Dfeatureswithonescomputed ViTArchitecture ✗ ✓ ✓ ✓
from the VideoMAE-v2 model, the temporal sensitivity can be Self-SupervisedObjective ✗ ✗ ✓ ✓
significantlyimprovedforbothkindsofdistortions. Large-scaleTraining ✗ ✗ ✗ ✓
Figure5. TheoriginofFVDsensitivity. Weshowthetempo-
scoreofeachvideosetwithrespecttotheoriginalvideos.
ral sensitivity achieved by using VideoMAE features is mainly
Weapplydistortioninfivepredefinedcorruptionlevels[27]
attributedtotheself-supervisedobjective.
andcomputetheaverage. Toverifythatthetwodistorted
setshavesimilarframequality,wecomputeFID[48]onthe
frames extracted from each set against the original image
tiotemporallyondifferentdatasetsinTable1. Weprovide
frames. Finally,weusetherelativeratiobetweenchangesin
moreresultsinSupplementaryMaterialSectionB.Themin-
FVDandFIDtomeasuretemporalsensitivity.
imalFIDdifferencebetweenthespatialandspatiotemporal
We perform the experiments on several standard
distortionvideosvalidatesourclaimthatthetwodistorted
video datasets, including Kinetics-400 [14], Something-
videosetssharesimilarframequality.
Something-v2 [23], UCF-101 [66], Sky Time-lapse [86],
Regarding the FVD scores of the spatially distorted
and FaceForensics [52] datasets. Motivated by the previ-
videos, several datasets that are not in the distribution of
ous finding that the unsupervised models trained on large
theI3Dtrainingdata,suchasSkyTime-lapse,FaceForen-
datasetsoftenproducemorereliablefeaturesinFID[39,46],
sics,andSSv2,generallyyieldmuchsmallerFVDvalues
wealsocomputeFVDusingaself-supervisedvideomodel
comparedtoin-distributiondatasetslikeTaichi-HD,UCF-
VideoMAE-v2[76],whichistrainedonamixedsetofunla-
101,andKinectics-400. Giventhesamelevelofdistortion
beleddatasetswiththeMaskedAutoencoders(MAE)recon-
isappliedacrossdifferentdatasets,thissuggeststhatFVD
structionobjective[26]. Duetothelargegapbetweenthe
islesssensitivetodistortionontheout-of-domaindata. We
pertaininganddownstreamtasks,theMAEmodelsareoften
inspectFVD’stemporalsensitivitybasedonitsincreasesin-
furtherfine-tunedonthedownstreamtasks.
ducedbytemporalinconsistency. Specifically,wecompute
In our experiments, we explore a pretrained model
therelativechangeofFVDbetweenspatialandspatiotempo-
VideoMAE-v2-PT andtwomodelsfine-tunedonKinetics-
ralcorruptions. WefindthatFVDsometimesfailstodetect
710dataset[41](VideoMAE-v2-K710)andSSv2dataset[23]
thetemporalqualitydecrease. Forexample, thetemporal
(VideoMAE-v2-SSv2). More details about the dataset and
inconsistencyintheFaceForensicsdatasetonlyraisesFVD
experimentalsetupsareincludedinSupplementaryMaterial
by3%.
SectionA.
To grasp the significance of the FVD increase due to
FVD temporal sensitivity. We present FID and FVD temporalinconsistency,wecompareitwiththeFVDvalues
scoresobtainedfromthevideosdistortedspatiallyorspa- computedusingVideoMAE-v2modelsinFigure4,where
)%(
oitar
egnahc
DVF
)%(
oitar
egnahc
DVFwereporttheaverageFVDscoresacrossmultipledatasets. 2048real videos
WhenusingtheVideoMAEmodelstoextractfeatures,we 𝑓!"#$%
notice a much more pronounced increase of FVD on the I3D 𝑓&"#$% 𝒩(𝜇!,Σ!)
videos with temporal inconsistency. For example, when 𝑓&"’#($)%
theelastictransformationisadoptedtointroducetemporal Weighted
Fréchet Distance
inconsistency,FVDwithVideoMAE-v2-PTincreasesfive K generated video candidates
timesmorethantheoriginalFVDwiththeI3Dmodel. 𝑓!*#+#"$,#- 𝑤!
ComparedtoVideoMAEvariants,theVideoMAEmodel I3D 𝑓&*#+#"$,#- 𝑤" 𝒩(𝜇"𝑤,Σ"(𝑤))
Video 1 Video K
fine-tunedontheSSv2datasetconsistentlyexhibitsgreater 𝑓.*#+#"$,#- 𝑤#
Gradient descent to
temporal sensitivity than the one fine-tuned on the K710 update weights w
dataset,atleastthreefold. Thisdifferencecanbeattributed
totheSSv2dataset’semphasisonmotion,wheredifferent Figure6.ProbingtheperceptualnullspaceinFVD.Wesample
videossharesimilarvisualcontent,whiledifferencesonly a 8× larger set of fake videos and compute a weight for each
ariseinfine-grainedmotioncues. BothVideoMAE-v2-SSv2 candidatevideobyoptimizingtheweightedFrechetdistance.We
thenusetheweightstosample2,048videostocomputethefinal
andVideoMAE-v2-PT exhibitlargersensitivitytothetem-
FVDscore,whichisusedtomeasuretheperceptualnullspace.
poral quality. Computing FVD with VideoMAE-v2-SSv2
featureseffectivelycapturesmildtemporalqualitydecrease
inducedbyMotionBlur,whereasusingVideoMAE-v2-PT
4.ProbingthePerceptualNullSpaceinFVD
featuresprovesmoresensitivetotemporaldistortionintro-
ducedbyElasticTransformation. Overall,theyallexhibit HavingobservedFID’sinsensitivitytotemporalqualityin
moresensitivitytotemporalcorruptionscomparedtoFVD our synthetic experiments, a natural question arises: how
computedwiththeI3Dmodel. doesthisbiasimpactpracticalevaluation? Toaddressthis
question,weleverageananalysismethodintroducedtoun-
Wheredoesthecontentbiasoriginatefrom? Multiple derstandundesiredbehaviorinFID[39]. Thistoolexamines
factorscouldcontributetotheincreasedtemporalsensitiv- the perceptual null space, where the quality of generated
itywhenusingfeaturescomputedfromtheVideoMAE-v2 images remains similar while the FID score can be effec-
model. Thesefactors may encompass themodel architec- tivelyadjusted. Inourscenario,wegeneratealargesetof
ture,trainingobjectives,modelcapacity,andthedataset. To candidatevideosfromthesamemodelandcarefullyselecta
unraveltheseintricacies,wefurtherdelveintoacompara- subsettolowertheFVDscore.
tivestudywithtwoothermodels,VideoMAE-v1[70]and Asthecoreassumptionofthemethod,thesamplessyn-
TimeSFormer[6]models. thesizedbythesamemodelshouldexhibitrelativelysimilar
The VideoMAE-v1 model uses a smaller ViT model visual quality. However, we observe that the quality may
size while sharing the same objective as the VideoMAE- sometimesvaryfordifferentgenerativevideos. Therefore,
v2 model, which helps us demystify the training scales. wefurtherextendtheanalysistounderstandtheconceptof
Due to limited computing resources, we cannot train the temporalperceptualnullspace,wherewehard-constrainthe
VideoMAE-v2ViTmodelfromscratchwiththesupervised temporalqualityofthegeneratedvideostobethesameby
objective. Instead, we train the smaller ViT with the size usingfrozenvideos.
ofVideoMAE-v1usingtherecipefromTimeSFomer. Both
Resampling method. We adopt the resampling tech-
modelsaretrainedontheKinetics-400dataset,sharingthe
niqueproposedbyKynka¨a¨nniemietal.[39]. Givenasetof
sametrainingdatasetastheI3Dmodel. ForVideoMAE-v2,
weusetheVideoMAE-v2-K710model,asitsharesthemost K, where K > N, generated videos, we assign a weight
w ∈ Rforeachvideo, aimingtominimizetheweighted
similar fine-tuning dataset, Kinetics-710, with other mod- k
FVD.Giventheweights,theFVDdefinedinEquation1is
els. Notethatithastheleasttemporalsensitivityamongthe
threevariants,asshowninFigure4. Weuseittomakeafair reformulatedwiththeweightedmeanµ g(w)= (cid:80) (cid:80)k kex exp pw wk kfk,
com Wp ear si uso mn mr aeg ria zr ed ti hng edth ise tifi nn ce ti- otu nn si bn eg twda et ea nse tt h.
esemodelsin
andcovarianceΣ g(w)= (cid:80) kexp (cid:80)wk k(f ek x− pµ w) k(fk−µ)T as:
thetableofFigure5. Weperformourtemporalsensitivity (cid:16) (cid:17)
analysisandreporttheFVDratiowithdifferentfeatureex-
∥µ r−µ g(w)∥2 2+Tr Σ r+Σ g(w)−2(Σ rΣ g(w))21 .
tractorsinFigure5. Themajorimprovementinthetemporal (2)
sensitivity arises when comparing the VideoMAE-v1 and ThisweightedFVDservesastheobjectiveforoptimizingw.
TimeSFormermodels. Basedontheseobservations,wecon- Aftertheoptimizationprocess,theresamplingisperformed
cludethattheself-supervisedtrainingobjectivecontributes withtheprobability (cid:80)e kx ep xw pk wk. 2,048videosaresampled
themosttomitigatingthecontentbias. fromthecandidatesettocomputethenewFVDscore,which(a)FVDscores (b)FVDdecreasepercentage (a)Candidatevideoswiththesmallestweights.
byresampling
Figure7. FVDdecreaseinducedbyresamplingacrossdiffer-
entmodelsanddatasets.WecompareFVDcomputedfromthe
VideoMAE-v2-SSv2andI3Dfeatures.Eachdotrepresentsavideo
generationmodeltrainedonaspecificdataset.(a)Wenoticeanon-
monotonicrelationshipbetweentheFVDcomputedwithI3Dand
VideoMAE-v2features. Forexample,themodelshighlightedin
redhavesimilarFVDscorescomputedwithI3Dfeaturesbutdiffer-
entscoreswhencomputedwithVideoMAE-v2features.(b)After
resampling,theFVDwithVideoMAEfeaturesgenerallydecreases
lessthantheFVDwithI3Dfeatures,asmostdotsarelocatedinthe
(b)Candidatevideoswiththelargestweights.
bottomrightarea(highlightedingrey).
Figure9.Candidatevideoswiththelargestandsmallestweights.
WevisualizetheresamplingresultsofDIGANtrainedontheTaichi-
wedenoteasFVD*.
HDdatasetwiththe32largest(mostlikelytoselect)andleast(least
likelytoselect)weights.Weobserveclearqualitydegradationin
Experimental setups. We experiment with several
thesampleswiththesmallestweights. BestviewedwithAcrobat
videogenerationmodels,includingGANs[64,91],Trans-
Reader.Pleasecheckthewebsiteforvideos.
former[20],andDiffusionmodels[92]. Asthesemodelsare
evaluatedondifferentbenchmarks,wealsotestwithdiffer-
scoresbeforeresamplinginFigure7(a). Wefirstobserve
entdatasets,includingUCF-101[66],SkyTime-lapse[86],
anon-monotonicrelationshipbetweentheFVDcomputed
Taichi-HD [61], and FaceForensics [52]. We use the of-
withI3DandVideoMAE-v2features,potentiallyleadingto
ficial scripts and checkpoints to sample the videos. For
differentmodelrankingswhenusingthesetwofeatures. For
eachexperiment,wegeneratea8×largercandidateset,i.e.,
FVDafterresampling,wereportthechangeratio
FVD∗−FVD
K = 16,384 videos. To optimize the weights w, we use FVD
inFigure7(b). Itillustratesasignificantdrop(40%−70%)
gradientdescentwithaninitiallearningrateof0.01anda
inFVDcomputedwithI3Dfeaturesafterresampling,while
linear scheduler that decreases the learning rate by 0.1 at
FVDcomputedwithVideoMAE-v2featuresexperiencesa
every100steps. Weperformoptimizationfor300steps,at
moremoderatedrop(25%−60%).
whichpointtheweightedFVDscoresoftenconverge.
Wenoteclearqualitydifferencesafterinspectingvideos
We are especially interested in how much we can re-
withthehighestandlowestweightsobtainedfromoptimiz-
ducetheFVDscorewithoutimprovingthetemporalquality.
ingweightedFVD.AnexampleofvideosgeneratedbyDI-
To achieve this, we convert each generated video into a
GANtrainedontheTaichi-HDdatasetisshowninFigure9.
frozen video by repeating its first frame 16 times. By do-
Weattributethedifferentobservationsfromtheoriginalpa-
ing so, we enforce all the videos to contain no motion so
per[39]totheunstableperformanceofthevideogenerator.
thatthetemporalqualitycannotbeimprovedthroughresam-
Since the video generator sometimes generates nonrealis-
pling. Motivatedbythetemporalsensitivitypresentedby
ticvideos,resamplingisbeneficialinselectingagroupof
theVideoMAE-v2models,wealsoperformtheexperiments
higher-qualityvideos,yieldingsmallerFVDscores.
with the VideoMAE-v2-SSv2 model as the feature extrac-
tor,wheretheresamplingisdonebyoptimizingitsspecific Temporalperceptualnullspace. Table2showsextensive
weights. Further details about the video generation mod- results,includingtheFVDoforiginalvideoswithmotions,
elsandexperimentalsetupsareincludedinSupplementary theFVDoffrozenvideos,andtheweightedFVDoffrozen
MaterialSectionA. videosafterresampling. Despitetheabsenceofmotionin
the generated videos, one can still reduce FVD by up to
Observationontheresamplingresults. WeshowFVD halfbyselectivelychoosingfromthecandidatevideoswhenTable2. ResultsofprobingthetemporalperceptualnullspaceofFVD.WereportFVDsofnormalandfrozengeneratedvideosby
randomsampling(FVD)andresamplingtominimizeweightedFVD(FVD*).WecolortheFVDdifferenceforbettervisualization:<20%,
20%−40%and>40%.ThedropofFVDonthefrozengeneratedvideosindicatesthevolumeofthenullspacewhereFVDcanbereduced
withoutgeneratingameaningfulmotion.Thegraybackgroundindicatesthesampleswhereresamplingfrozenvideoscanobtainsimilaror
evenbetterFVDthantherandomgenerationresultswithmotions.
I3DFeatures VideoMAE-v2-SSv2Features
Model Dataset FVD FVD FVD* FVD FVD FVD*
w/omotion w/omotion w/omotion w/omotion
DIGAN[91] UCF-101[66] 562.36 1303.13 715.96(−45.1%) 378.19 951.59 859.57(−9.7%)
DIGAN[91] SkyTime-lapse[86] 157.13 230.64 115.55(−49.9%) 174.79 408.17 362.84(−11.1%)
DIGAN[91] Taichi-HD[61] 132.26 461.79 276.88(−40.0%) 313.84 578.61 523.20(−9.6%)
TATS[20] UCF-101[66] 329.92 1157.69 616.25(−46.8%) 388.79 908.95 805.88(−11.3%)
TATS[20] SkyTime-lapse[86] 125.62 279.75 126.32(−54.8%) 213.33 375.74 353.15(−6.0%)
TATS[20] Taichi-HD[61] 124.16 475.99 312.19(−34.4%) 274.81 587.31 530.86(−9.6%)
StyleGAN-V[64] SkyTime-lapse[86] 56.63 206.56 104.27(−49.5%) 219.85 503.22 456.24(−9.3%)
StyleGAN-V[64] FaceForensics[52] 56.22 353.79 242.04(−31.6%) 194.68 547.24 520.98(−4.8%)
PVDM[92] UCF-101[66] 348.81 1135.61 605.09(−46.7%) 369.14 1032.90 898.48(−13.0%)
PVDM[92] SkyTime-lapse[86] 59.95 182.77 94.87(−48.1%) 142.50 429.06 395.79(−7.8%)
evaluatingtheSkyTime-lapsedataset. Intheworstcase,the Table3.StyleGAN-vmodel[64]withLSTMasthemotioncodes
sameorevensmallerFVDscorescanbeachievedcompared trainedontheSkyTime-lapsedatasetgeneratecollapsedmotions,
whereasFVDcomputedonthe128framesfavorstheresults.We
withrandomlyselectedgeneratedvideoswithmotions.
showthatcomputingFVDwithVideoMAE-v2featurescalibrates
Thesefindingshighlightthepronouncedcontentbiasin-
theconclusion.
herentintheFVDmetric. Conversely,whencomputingthe
featuresforFVDusingtheVideoMAE-v2model,whichis
Frame#FVDFeature StylegGAN-vw/LSTMcodes
sensitive to temporal quality, the gap significantly dimin-
ishes,andtheFVDscorescanhardlybedecreasedthrough I3D 120.11 136.65(+16.54%)
resampling. ThisemphasizesthattheFVDwithVideoMAE- 16 VideoMAE-SSv2 223.96 247.25(+23.29%)
v2hasamuchsmallertemporalperceptualnullspace. More VideoMAE-K710145.37 154.29(+8.92%)
resultsareavailableinSupplementaryMaterialSectionB. I3D 190.82 172.71(−18.11%)
128 VideoMAE-SSv2 332.80 616.74(+283.94%)
5.CaseStudy: LongVideoGeneration VideoMAE-K710155.51 191.48(+35.97%)
Our experiments have revealed that FVD does not suffi-
cientlyaccountformotioningeneratedvideos. Wenowdive
motioncollapse(Figure11b).
intotwocasestudiesfrompreviousworkswhereFVDscores
We follow the original study’s evaluation protocol and
contradicthumanperception[20,64,91]. Inbothcases,the
computetheFVDmetricbyfeedingallthe128framesto
videogenerationmodelsaretrainedontheSkyTime-lapse
theI3Dmodel,termedasFVD . NotethattheI3Dmodel
dataset,whichisout-of-domainfortheI3Dmodel,andFVD 128
wasinitiallytrainedon64frames,whiletheglobalaverage
hasbeenshownnottoperformwell. Inaddition,bothtasks
poolingandconvolutionalarchitectureallowittobeapplied
generatelongervideosthanthestandard16-framesetting,
to any video length. We also compute FVD with the
making the motion artifacts more perceptible to humans. 128
VideoMAE.SincetheVideoMAEusesaViTwithfixed-size
Nevertheless,FVDfailstocapturethesemotionartifactsin
positionalencoding,weperforminterpolationofpositional
bothexperiments.
encodings,similartoDINO[13].
Case study I [64]. To synthesize long videos, the Contrary to the visual evidence, we observe the same
StyleGAN-vmodel[64]employsconvolutionallayerswith trendasnotedbytheauthorsthatFVD computedusing
128
largereceptionfieldstopredicttheparametersoftheFourier theI3DmodelislowerfortheLSTMvariant,comparedto
temporalencoding.Wereproduceoneofitsbaselinesbysub- theoriginalStyleGAN-v,asshowninTable3. Uponcom-
stitutingsuchtemporalencodingwithanLSTMlayer. For puting FVD using VideoMAE-v2 features, both using
128
generating128-framevideos,thedefaultStyleGAN-Vsyn- SSv2 and K710, we have them to be in accordance with
thesizesrealisticmotions(Figure11a),whereasthebaseline human preference, i.e., FVD for the LSTM baseline is
128
withcontinuousLSTMcodesleadstovideoswithnoticeable muchworsecomparedtotheoriginalStyleGAN-v.(a)DefaultStyleGAN-v. (a)Frames0-16.
(b)StyleGAN-vwithLSTMmotioncodes. (b)Frames128-144.
Figure11.VideosgeneratedbyStyelGAN-vanditsLSTMvari- Figure 13. Videos generated by DIGAN at different extrap-
ant.ThedefaultStyleGAN-vsynthesizesnaturalmotions,while olated time steps. The initial 16 frames generated by DIGAN
thevariantwithLSTMmotioncodesgeneratesrepeatedpatterns. exhibitnaturalmotions,whiletheextrapolatedframescontainperi-
BestviewedwithAcrobatReader. Pleasecheckthewebsitefor odicartifacts.BestviewedwithAcrobatReader.Pleasecheckthe
videos. websiteforvideos.
6.Discussion
Table4.DIGAN[91]trainedontheSkyTime-lapsedatasetwith
Inthispaper, wehavestudiedthebiasoftheFVDonthe
extrapolated time steps generate periodic artifacts, whereas the
frame quality. With experiments spanning from synthetic
FVDmetricfavorstheresults.WeshowthatcomputingFVDwith
VideoMAE-v2featurescalibratestheconclusion. videodistortion,toresamplingvideogenerationresults,to
investigatingreal-worldexamples,wehaveconcludedthat
FVDFeature Frames0-16 Frames128-144 FVDishighlyinsensitivetothetemporalqualityandconsis-
tencyofthegeneratedvideos. Wehaveverifiedthehypoth-
I3D 155.58 141.82(−8.84%)
esisthatthebiasoriginatesfromthecontent-biasedvideo
VideoMAE-SSv2 133.37 150.61(+12.9%)
featuresandshowthatself-supervisedfeaturescanmitigate
VideoMAE-K710 250.70 259.29(+3.43%)
the issues in all the experiments. We hope our work will
drawmoreattentiontostudyingvideogenerationevaluation
anddesigningbetterevaluationmetrics.
CasestudyII[20,91]. Thoughtrainedon16-framevideo
clips,DIGANcanbeappliedtogeneratelongervideosbyex-
Limitations. SeveralcriticalaspectsofFVDremainun-
trapolatingthetemporalencodings.However,intheprevious
derexplored. For example, in addition to the longer time
study[20],theauthorshaveobservedthatmotionartifactsin
duration,existingmethodsalsogeneratemegapixelresolu-
theformofrepeatedchangesinthediagonaldirection,while
tionvideos[9,21,30,62]. However,tocomputeFVD(I3D
FVDcomputedonthe16-framechunks,favortheartifacts.
orVideoMAE-v2),thevideomustberesizedtoalower(e.g.,
Specifically,toevaluatethelongvideogenerationresults, 224x224)resolution. Inaddition,manymethodschooseto
they compute FVD at strides of k frames, 16 frames at a generatevideosnotlimitedtothesquareaspectratio,e.g.,
time, for the entire video. We compute FVD on frames 16:9,whiletheFVDmetricalwaysrequiresasquarevideoas
0→16,64→82,128→144,andsoon. Thevisualization theinput. ComputingFVDusingVideoMAE-v2islimited
ofvideosat0 → 16and128 → 144Figure13showmore bythequadraticcostofattentionlayers,causingissuesin
periodic artifacts at 128 → 144 compared to 0 → 16. In evaluatinglongervideogeneration. Usingefficientnetworks
contrast,fromTable4,weseethatFVDcomputedusingI3D canreducethecost,whichweleaveasfuturework.
featuresfavors128→144frames,thoughFVDcomputed
Acknowledgment. We thank Angjoo Kanazawa, Alek-
usingVideoMAEfeaturesforbothSSv2andK710follow
sanderHolynski,DeviParikh,andYogeshBalajifortheir
humanjudgment.
early feedback and discussion. We thank Or Patashnik,
Recentprogressinphotorealisticvideogenerationusing
Richard Zhang, Hanyu Wang, and Hadi Alzayer for their
Diffusion models has enabled the creation of videos with
helpfulcomments,paperreading,andcodereviewing. We
extended durations (≥ 100 frames) [9, 21, 30, 62]. As a
thankIvanSkorokhodovforhishelpwithreproducingthe
result,evaluatinglongvideogenerationresultshasbecome
StyleGAN-vexperiments. Thisworkispartlysupportedby
increasinglyimportant. However,accordingtothetworeal-
NSFgrantNo. IIS-239076,thePackardFellowship,aswell
worldcasestudiesabove,FVDwithI3Dfeaturesdoesnot
asNSFgrantsNo. IIS-1910132andIIS-2213335.
reliablydetectmotionartifactsinlongvideos.References [16] Jinwoo Choi, Chen Gao, Joseph CE Messou, and Jia-Bin
Huang. Whycan’tidanceinthemall?learningtomitigate
[1] MotasemAlfarra,JuanCPe´rez,AnnaFru¨hstu¨ck,PhilipHS
scenebiasinactionrecognition. InNeurIPS,2019. 2
Torr,PeterWonka,andBernardGhanem. Ontherobustness
[17] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi
ofqualitymeasuresforgans. InECCV,2022. 2,3
Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase.
[2] JieAn,SongyangZhang,HarryYang,SonalGupta,Jia-Bin
InCVPR,pages248–255,2009. 3
Huang, JieboLuo, andXiYin. Latent-shift: Latentdiffu-
[18] EmilyDentonandRobFergus. Stochasticvideogeneration
sionwithtemporalshiftforefficienttext-to-videogeneration.
with a learned prior. In ICML, pages 1174–1183. PMLR,
arXivpreprintarXiv:2304.08477,2023. 2
2018. 2
[3] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,
[19] DCDowsonandBVLandau. Thefre´chetdistancebetween
RoyHCampbell,andSergeyLevine. Stochasticvariational
multivariate normal distributions. Journal of multivariate
videoprediction. InICLR,2018. 2
analysis,12(3):450–455,1982. 2
[4] MaxBain,ArshaNagrani,Gu¨lVarol,andAndrewZisserman.
[20] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan
Frozenintime:Ajointvideoandimageencoderforend-to-
Pang,DavidJacobs,Jia-BinHuang,andDeviParikh. Long
endretrieval. InICCV,pages1728–1738,2021. 1
videogenerationwithtime-agnosticvqganandtime-sensitive
[5] OmerBar-Tal, HilaChefer, OmerTov, CharlesHerrmann, transformer. InECCV,2022. 2,6,7,8,13
RoniPaiss,ShiranZada,ArielEphrat,JunhwaHur,Guanghui
[21] SongweiGe,SeungjunNah,GuilinLiu,TylerPoon,Andrew
Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer
Tao,BryanCatanzaro,DavidJacobs,Jia-BinHuang,Ming-
Michaeli,OliverWang,DeqingSun,TaliDekel,andInbar
YuLiu,andYogeshBalaji. Preserveyourowncorrelation:A
Mosseri. Lumiere:Aspace-timediffusionmodelforvideo
noisepriorforvideodiffusionmodels.InICCV,2023.1,2,8
generation,2024. 1
[22] RohitGirdhar,MannatSingh,AndrewBrown,QuentinDuval,
[6] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
SamanehAzadi,SaiSakethRambhatla,AkbarShah,XiYin,
space-timeattentionallyouneedforvideounderstanding? In
DeviParikh,andIshanMisra. Emuvideo:Factorizingtext-
ICML,2021. 2,3,5,13
to-videogenerationbyexplicitimageconditioning,2023. 1
[7] MikolajBinkowski,DanicaJ.Sutherland,MichalArbel,and
[23] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-
ArthurGretton. Demystifyingmmdgans. InICLR,2018. 3
ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
[8] AndreasBlattmann, TimDockhorn, SumithKulal, Daniel ValentinHaenel,IngoFruend,PeterYianilos,MoritzMueller-
Mendelevitch, MaciejKilian, DominikLorenz, YamLevi, Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and
ZionEnglish, VikramVoleti, AdamLetts, VarunJampani, Roland Memisevic. The ”something something” video
andRobinRombach. Stablevideodiffusion:Scalinglatent databaseforlearningandevaluatingvisualcommonsense.In
videodiffusionmodelstolargedatasets,2023. 2 ICCV,2017. 4,13
[9] AndreasBlattmann,RobinRombach,HuanLing,TimDock- [24] JiaxiGu,ShicongWang,HaoyuZhao,TianyiLu,XingZhang,
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. ZuxuanWu,SongcenXu,WeiZhang,Yu-GangJiang,and
Alignyourlatents:High-resolutionvideosynthesiswithla- HangXu. Reuseanddiffuse:Iterativedenoisingfortext-to-
tentdiffusionmodels. InCVPR,2023. 1,2,8 videogeneration,2023. 2
[10] AliBorji. Prosandconsofganevaluationmeasures: New [25] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera
developments. ComputerVisionandImageUnderstanding, Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose´ Lezama.
215:103329,2022. 2,3 Photorealisticvideogenerationwithdiffusionmodels,2023.
[11] TimBrooks,JanneHellsten,MiikaAittala,Ting-ChunWang, 2
Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, [26] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr
andTeroKarras. Generatinglongvideosofdynamicscenes. Dolla´r,andRossGirshick. Maskedautoencodersarescalable
NeurIPS,35:31769–31781,2022. 2 visionlearners. InCVPR,pages16000–16009,2022. 4
[12] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,Yufei [27] DanHendrycksandThomasDietterich.Benchmarkingneural
Guo,LiJing,DavidSchnurr,JoeTaylor,TroyLuhman,Eric networkrobustnesstocommoncorruptionsandperturbations.
Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. InICLR,2019. 3,4,13
Videogenerationmodelsasworldsimulators,2024. 1 [28] MartinHeusel,HubertRamsauer,ThomasUnterthiner,Bern-
[13] MathildeCaron,HugoTouvron,IshanMisra,Herve´Je´gou, hardNessler,andSeppHochreiter. Ganstrainedbyatwo
JulienMairal,PiotrBojanowski,andArmandJoulin. Emerg- time-scaleupdateruleconvergetoalocalnashequilibrium.
ingpropertiesinself-supervisedvisiontransformers.InICCV, InNeurIPS,2017. 1,2
2021. 7 [29] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
[14] Joao Carreira and Andrew Zisserman. Quo vadis, action sionprobabilisticmodels. InNeurIPS,2020. 1
recognition?anewmodelandthekineticsdataset. InCVPR, [30] JonathanHo,WilliamChan,ChitwanSaharia,JayWhang,
pages6299–6308,2017. 2,4,13 RuiqiGao,AlexeyGritsenko,DiederikPKingma,BenPoole,
[15] LluisCastrejon,NicolasBallas,andAaronCourville. Im- Mohammad Norouzi, David J Fleet, et al. Imagen video:
provedconditionalvrnnsforvideoprediction.InICCV,pages Highdefinitionvideogenerationwithdiffusionmodels.arXiv
7608–7617,2019. 2 preprintarXiv:2210.02303,2022. 1,2,8[31] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan, Chan,andYingShan. Evalcrafter:Benchmarkingandevalu-
Mohammad Norouzi, and David J Fleet. Video diffusion atinglargevideogenerationmodels,2023. 1
models. arXivpreprintarXiv:2204.03458,2022. 2 [45] ZhengxiongLuo,DayouChen,YingyaZhang,YanHuang,
[32] De-An Huang, Vignesh Ramanathan, Dhruv Mahajan, LiangWang,YujunShen,DeliZhao,JingrenZhou,andTie-
LorenzoTorresani,ManoharPaluri,LiFei-Fei,andJuanCar- niu Tan. Videofusion: Decomposed diffusion models for
losNiebles.Whatmakesavideoavideo:Analyzingtemporal high-qualityvideogeneration. InCVPR,2023. 2
informationinvideounderstandingmodelsanddatasets. In [46] StanislavMorozov,AndreyVoynov,andArtemBabenko. On
CVPR,2018. 2,3 self-supervisedimagerepresentationsforganevaluation. In
[33] ZiqiHuang, YinanHe, JiashuoYu, FanZhang, Chenyang ICLR,2020. 2,3,4
Si,YumingJiang,YuanhanZhang,TianxingWu,Qingyang [47] MuhammadFerjadNaeem,SeongJoonOh,YoungjungUh,
Jin,NattapolChanpaisit,YaohuiWang,XinyuanChen,Limin YunjeyChoi,andJaejunYoo. Reliablefidelityanddiversity
Wang,DahuaLin,YuQiao,andZiweiLiu. Vbench:Compre- metricsforgenerativemodels. InICML,pages7176–7185.
hensivebenchmarksuiteforvideogenerativemodels,2023. PMLR,2020. 1,3
1 [48] GauravParmar,RichardZhang,andJun-YanZhu. Onaliased
[34] YunseokJang,GunheeKim,andYaleSong.Videoprediction resizingandsurprisingsubtletiesinganevaluation. InCVPR,
with appearance and motion conditions. In ICML, pages 2022. 1,2,3,4
2225–2234,2018. 2 [49] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
andMarkChen. Hierarchicaltext-conditionalimagegenera-
[35] TeroKarras, SamuliLaine, andTimoAila. Astyle-based
tionwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):
generatorarchitectureforgenerativeadversarialnetworks. In
CVPR,2019. 3 3,2022. 2
[50] MarcAurelioRanzato,ArthurSzlam,JoanBruna,Michael
[36] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
Mathieu,RonanCollobert,andSumitChopra. Video(lan-
vosyan, Roberto Henschel, Zhangyang Wang, Shant
guage)modeling:abaselineforgenerativemodelsofnatural
Navasardyan, and Humphrey Shi. Text2video-zero: Text-
videos. arXivpreprintarXiv:1412.6604,2014. 2
to-imagediffusionmodelsarezero-shotvideogenerators. In
[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
CVPR,2023. 2
Patrick Esser, and Bjo¨rn Ommer. High-resolution image
[37] DanKondratyuk,LijunYu,XiuyeGu,Jose´Lezama,Jonathan
synthesiswithlatentdiffusionmodels. InCVPR,2022. 2
Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari,
[52] AndreasRo¨ssler,DavideCozzolino,LuisaVerdoliva,Chris-
Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang
tianRiess,JustusThies,andMatthiasNießner. FaceForen-
Chiu, JoshDillon, IrfanEssa, AgrimGupta, MeeraHahn,
sics++: Learning to detect manipulated facial images. In
AnjaHauth,DavidHendon,AlonsoMartinez,DavidMin-
ICCV,2019. 4,6,7,13
nen,DavidRoss,GrantSchindler,MikhailSirotenko,Kihyuk
[53] ChitwanSaharia,WilliamChan,SaurabhSaxena, LalaLi,
Sohn,KrishnaSomandepalli,HuishengWang,JimmyYan,
JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael
Ming-HsuanYang,XuanYang,BryanSeybold,andLuJiang.
GontijoLopes,BurcuKaragolAyan,TimSalimans,etal.Pho-
Videopoet:Alargelanguagemodelforzero-shotvideogen-
torealistictext-to-imagediffusionmodelswithdeeplanguage
eration,2024. 2
understanding. InNeurIPS,2022. 2
[38] TuomasKynka¨a¨nniemi,TeroKarras,SamuliLaine,Jaakko
[54] MasakiSaito,EiichiMatsumoto,andShuntaSaito. Temporal
Lehtinen, and Timo Aila. Improved precision and recall
generativeadversarialnetswithsingularvalueclipping. In
metricforassessinggenerativemodels. InNeurIPS,2019. 1,
ICCV,2017. 2
3
[55] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier
[39] Tuomas Kynka¨a¨nniemi, Tero Karras, Miika Aittala, Timo
Bousquet,andSylvainGelly. Assessinggenerativemodels
Aila,andJaakkoLehtinen. Theroleofimagenetclassesin
viaprecisionandrecall. InNeurIPS,2018. 1,3
fre´chetinceptiondistance. InICLR,2022. 2,3,4,5,6
[56] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
[40] DingquanLi,TingtingJiang,andMingJiang. Qualityassess-
Cheung,AlecRadford,andXiChen. Improvedtechniques
mentofin-the-wildvideos. InProceedingsofthe27thACM
fortraininggans. InNeurIPS,2016. 2
InternationalConferenceonMultimedia,pages2351–2359,
[57] ChristophSchuhmann,RomainBeaumont,RichardVencu,
2019. 1
Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo
[41] KunchangLi, YaliWang, YinanHe, YizhuoLi, YiWang,
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
LiminWang, andYuQiao. Uniformerv2: Spatiotemporal
man,PatrickSchramowski,SrivatsaRKundurthy,Katherine
learningbyarmingimagevitswithvideouniformer,2022. 4
Crowson,LudwigSchmidt,RobertKaczmarczyk,andJenia
[42] YingweiLi,YiLi,andNunoVasconcelos.Resound:Towards Jitsev. LAION-5b:Anopenlarge-scaledatasetfortraining
action recognition without representation bias. In ECCV, nextgenerationimage-textmodels. InThirty-sixthConfer-
2018. 2 enceonNeuralInformationProcessingSystemsDatasetsand
[43] Xin Liu, Silvia L Pintea, Fatemeh Karimi Nejadasl, Olaf BenchmarksTrack,2022. 1
Booij,andJanCVanGemert. Noframeleftbehind: Full [58] KalpanaSeshadrinathan,RajivSoundararajan,AlanConrad
videoactionrecognition. InCVPR,2021. 2,3 Bovik,andLawrenceKCormack. Studyofsubjectiveand
[44] YaofangLiu,XiaodongCun,XueboLiu,XintaoWang,Yong objectivequalityassessmentofvideo. IEEEtransactionson
Zhang, HaoxinChen, YangLiu, TieyongZeng, Raymond ImageProcessing,19(6):1427–1441,2010. 1[59] LauraSevilla-Lara,ShengxinZha,ZhichengYan,Vedanuj Towardsgoodpracticesfordeepactionrecognition.InECCV,
Goswami,MattFeiszli,andLorenzoTorresani. Onlytime 2016. 2
cantell: Discoveringtemporaldatafortemporalmodeling. [76] LiminWang,BingkunHuang,ZhiyuZhao,ZhanTong,Yinan
InProceedingsoftheIEEE/CVFWinterConferenceonAp- He,YiWang,YaliWang,andYuQiao.Videomaev2:Scaling
plicationsofComputerVision(WACV),pages535–544,2021. videomaskedautoencoderswithdualmasking. InCVPR,
2,3 pages14549–14560,2023. 2,4,13
[60] XiaoqianShen,XiangLi,andMohamedElhoseiny.Mostgan- [77] WenjingWang,HuanYang,ZixiTuo,HuiguoHe,Junchen
v:Videogenerationwithtemporalmotionstyles. InCVPR, Zhu,JianlongFu,andJiayingLiu. Videofactory:Swapatten-
2023. 2 tioninspatiotemporaldiffusionsfortext-to-videogeneration.
[61] AliaksandrSiarohin,Ste´phaneLathuilie`re,SergeyTulyakov, arXivpreprintarXiv:2305.10874,2023. 2
Elisa Ricci, and Nicu Sebe. First order motion model for [78] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya
imageanimation. InNeurIPS,2019. 6,7,13 Zhang,ChangxinGao,andNongSang. Videolcm: Video
[62] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn, latentconsistencymodel,2023. 2
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, [79] YaohuiWang,PiotrBilinski,FrancoisBremond,andAntitza
OranGafni,etal. Make-a-video: Text-to-videogeneration Dantcheva. G3an:Disentanglingappearanceandmotionfor
withouttext-videodata. arXivpreprintarXiv:2209.14792, videogeneration. InCVPR,2020. 2
2022. 1,2,8 [80] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,
[63] ZeinaSinnoandAlanConradBovik. Large-scalestudyof Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo
perceptualvideoquality. IEEETransactionsonImagePro- Yu, PeiqingYang, etal. Lavie: High-qualityvideogener-
cessing,28(2):612–627,2018. 1 ationwithcascadedlatentdiffusionmodels. arXivpreprint
arXiv:2309.15103,2023. 2
[64] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-
[81] ZhouWang,LigangLu,andAlanCBovik. Videoqualityas-
seiny. Stylegan-v: Acontinuousvideogeneratorwiththe
price,imagequalityandperksofstylegan2. InCVPR,2022. sessmentbasedonstructuraldistortionmeasurement. Signal
processing:Imagecommunication,19(2):121–132,2004. 1
2,3,6,7,13
[82] DirkWeissenborn,OscarTa¨ckstro¨m,andJakobUszkoreit.
[65] JiamingSong,ChenlinMeng,andStefanoErmon. Denoising
Scalingautoregressivevideomodels. InICLR,2019. 2
diffusionimplicitmodels. InICLR,2021. 1
[83] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei
[66] KhurramSoomro,AmirRoshanZamir,andMubarakShah.
Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva:
Ucf101:Adatasetof101humanactionsclassesfromvideos
Generating open-domain videos from natural descriptions.
inthewild.arXivpreprintarXiv:1212.0402,2012.4,6,7,13
arXivpreprintarXiv:2104.14806,2021. 2
[67] NitishSrivastava,ElmanMansimov,andRuslanSalakhudi-
[84] JinboXing,MenghanXia,YongZhang,HaoxinChen,Xin-
nov. Unsupervisedlearningofvideorepresentationsusing
taoWang,Tien-TsinWong,andYingShan. Dynamicrafter:
lstms. InICML,2015. 2
Animatingopen-domainimageswithvideodiffusionpriors,
[68] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
2023. 2
Shlens,andZbigniewWojna. Rethinkingtheinceptionarchi-
[85] ZhenXing,QiDai,HanHu,ZuxuanWu,andYu-GangJiang.
tectureforcomputervision. InCVPR,2016. 3
Simda: Simplediffusionadapterforefficientvideogenera-
[69] YuTian,JianRen,MengleiChai,KyleOlszewski,XiPeng,
tion,2023. 2
DimitrisN.Metaxas,andSergeyTulyakov. Agoodimage
[86] WeiXiong,WenhanLuo,LinMa,WeiLiu,andJieboLuo.
generatoriswhatyouneedforhigh-resolutionvideosynthesis.
Learningtogeneratetime-lapsevideosusingmulti-stagedy-
InICLR,2021. 2,13
namicgenerativeadversarialnetworks. InCVPR,2018. 2,4,
[70] ZhanTong,YibingSong,JueWang,andLiminWang. Video-
6,7,13
mae:Maskedautoencodersaredata-efficientlearnersforself-
[87] WilsonYan,YunzhiZhang,PieterAbbeel,andAravindSrini-
supervisedvideopre-training. NeurIPS,35:10078–10093,
vas. Videogpt:Videogenerationusingvq-vaeandtransform-
2022. 5,13
ers. arXivpreprintarXiv:2104.10157,2021. 2,13
[71] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
[88] Wilson Yan, Danijar Hafner, Stephen James, and Pieter
Kautz.Mocogan:Decomposingmotionandcontentforvideo
Abbeel. Temporallyconsistenttransformersforvideogenera-
generation. InCVPR,2018. 2
tion,2023.
[72] ThomasUnterthiner,SjoerdVanSteenkiste,KarolKurach, [89] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose´ Lezama, Han
RaphaelMarinier,MarcinMichalski,andSylvainGelly. To- Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-
wardsaccurategenerativemodelsofvideo:Anewmetric& HsuanYang,YuanHao,IrfanEssa,andLuJiang. Magvit:
challenges. arXivpreprintarXiv:1812.01717,2018. 1,2,3 Maskedgenerativevideotransformer.InCVPR,pages10459–
[73] RubenVillegas,JimeiYang,SeunghoonHong,XunyuLin, 10469,2023.
and Honglak Lee. Decomposing motion and content for [90] LijunYu,Jose´Lezama,NiteshB.Gundavarapu,LucaVersari,
naturalvideosequenceprediction. InICLR,2017. 2 Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta,
[74] CarlVondrick,HamedPirsiavash,andAntonioTorralba.Gen- XiuyeGu,AlexanderG.Hauptmann,BoqingGong,Ming-
eratingvideoswithscenedynamics. NeurIPS,2016. 2 HsuanYang,IrfanEssa,DavidA.Ross,andLuJiang. Lan-
[75] LiminWang,YuanjunXiong,ZheWang,YuQiao,DahuaLin, guagemodelbeatsdiffusion–tokenizeriskeytovisualgen-
XiaoouTang,andLucVanGool.Temporalsegmentnetworks: eration,2023. 2[91] SihyunYu,JihoonTack,SangwooMo,HyunsuKim,Junho
Kim,Jung-WooHa,andJinwooShin.Generatingvideoswith
dynamics-awareimplicitgenerativeadversarialnetworks. In
ICLR,2022. 2,6,7,8,13,14
[92] SihyunYu,KihyukSohn,SubinKim,andJinwooShin.Video
probabilisticdiffusionmodelsinprojectedlatentspace. In
CVPR,2023. 2,6,7,13
[93] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui
Zhao,LingminRan,YuchaoGu,DifeiGao,andMikeZheng
Shou. Show-1: Marryingpixelandlatentdiffusionmodels
fortext-to-videogeneration,2023. 2
[94] RichardZhang,PhillipIsola,AlexeiA.Efros,EliShechtman,
andOliverWang. Theunreasonableeffectivenessofdeep
featuresasaperceptualmetric. InCVPR,2018. 3
[95] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models. arXiv preprint
arXiv:2211.11018,2022. 2
[96] SharonZhou,MitchellGordon,RanjayKrishna,AustinNar-
comey,LiFFei-Fei,andMichaelBernstein. Hype:Abench-
markforhumaneyeperceptualevaluationofgenerativemod-
els. InNeurIPS,2019. 3A.ExperimentDetails AdditionalImplementationDeatils. ToquantifytheFVD
temporalsensitivitywithvideodistortionmethods,wefol-
In this section, we discuss the additional details of the
lowthecommonpractice[69,87,91]tosample2,048clips
datasets, video generation models, and other experiment
ofresolution128×128fromeachvideodataset. Weapply
setups. We will release our code to compute FVD with
thedistortioninfivepre-definedcorruptionlevelsfollowing
VideoMAE-v2backbonefeaturesandpre-computedfeatures
thepreviousstudy[27]. Toprobetheperceptualnullspace
forcommonlyusedvideodatasets.
inFVD,wecasttheextractedfeaturesandweightstofloat64
to stabilize the optimization process and avoid numerical
issues.
Dataset. We conduct our analysis on six datasets, in-
Inaddition,tocomputeViTencoderfeaturesofVideo-
cluding two widely used video understanding bench-
MAE [70, 76] and TimeSFormer [6] models, we follow
marksKinetics-400[14]andSomething-Something-v2[23],
theconventiontoexploitthepre-logitfeatures. Toextract
threevideogenerationbenchmarksFaceForencis[52],Sky
featuresfromthepre-trainedVideoMAEencoder-decoder
Time-lapse [86], and Taichi-HD [61], and the UCF-101
architecture,wetaketheoutputofthepenultimatelayerin
dataset[66]thathasbeenusedforbothtasks.
theencoderandaverageacrossallthepatches,whichuses
Kinetics-400 [14] (K400) contains 267,000 videos of
essentiallytheoutputfromthesamelayerasthefine-tuned
10 seconds in 400 action classes. Something-Something-
VideoMAEmodel. Toreducememorycostswhencomput-
v2[23](SSv2)consistsof220,000videosof2−6seconds
ing FVD using VideoMAE-v2 models, we cast all the
128
in174classesofhumansperformingbasicactionswithev-
features to float16, as the FVD score difference between
eryday objects. UCF-101 [66] has 13,320 videos of, on
usingfloat16andfloat32isneglectableandoftenlessthan
average, 7 seconds in 101 classes of human actions. Sky
0.03%.
Time-lapse[86](Sky)collects2,647time-lapsevideosof
AllofourexperimentsareperformedonasingleNVIDIA
theskyindifferentperiodsandundervariousweathercon-
RTXA6000GPUexceptforreproducingtheStyleGAN-v
ditions. FaceForencis [52] (FFS) contains 1,000 human
variants,wherewefollowtheofficialreceipttotrainonfour
talking videos collected from YouTube to facilitate Deep-
NVIDIARTXA6000GPUs.
fake detection. We follow the official instructions to pro-
cess the videos to extract the face region and obtain 704
B.AdditionResults
videos. Taichi-HD [61] (Taichi) is a video dataset of 280
longYouTubevideosrecordingapersonperformingTaichi, QuantifyingthetemporalsensitivityofFVD. Weexpand
whichispreprocessedinto3,335shortclips.Notethatvideo Table1inthemainpapertoincludetheFVDscoresonthe
generationmodels[20,91]trainedonthisdatasetoftensam- sixdatasetswitheitherspatial(S)orspatiotemporal(ST)dis-
pleeveryfourframestoattainlargermotionineachtraining tortionusingfeaturesfromtheI3Dmodel,threeVideoMAE-
clip. v2variants,twoTimeSformermodels,andVideoMAE-v2
modelsinTables5and6. ByinspectingthespatialFVDs
computedwithVideoMAE-v2featuresondifferentdatasets,
VideoGenerationModels. Ourgeneratedvideosarefrom wenoticethattheyvarylessthantheFVDscoresusingthe
four video generation models, DIGAN [91], TATS [20], I3Dfeatures,highlightingtheirgeneralizationcapacity. We
StyleGAN-v[64],andPVDM[92]. DIGAN[91]isaGAN- also explore the TimeSformer model trained on the SSv2
basedmodelthatleveragesimplicitneuralrepresentations dataset. ComparedwiththeonetrainedontheK400dataset
and computation-efficient discriminators. TATS [20] ex- reported in the main paper, it is generally more sensitive
tends VQGAN [? ] for long video generation by design- totemporalqualitychangeduetothedataset. However,it
ing time-agnostic VAE and hierarchical transformer. DI- is still on par with the I3D model as both share the same
GANandTATS-basemodelsaretrainedon16videoframes supervisedobjective.
of 128 × 128 resolution. StyleGAN-v [64] extends the
renownedStyleGANarchitecture[? ] forvideogeneration
ProbingtheperceptualnullspaceinFVD. Weexpand
byemployingimplicitneuralrepresentations. PVDM[92]
Table7inthemainpapertoincludeFVDandFVD*onall
exploitsalatentdiffusionarchitectureandefficienttriplane
themodelsanddatasetcomputedwiththeI3Dmodeland
representation. StyleGAN-vandPVDMareevaluatedwith
threeVideoMAE-v2variantsinTable7.
resolution256×256andvideolength16and128. When
computing FVD scores, all four methods generate 2,048
videos. WefollowStyleGAN-v[64]tosavethegenerated Practicalexamples. WeexpandTable4inthemainpa-
videoswithoutsevereJPEGcompressionandsamplerandom per by showing the FVD changes on all the consecutive
clipsfromtherealvideos. Wecanreproducethereported 16 frames of the extrapolated generation results using DI-
FVDscores,asshowninTable. 2inthemainpaper. GANinFigure14. WenoticethatwithlongerframesbeingTable5.ResultsofanalyzingthetemporalsensitivityofFVD.WereportFVDsofsyntheticvideoscreatedfromrealvideosusingspatial
onlyorspatiotemporaldistortions,wherethetwosetsproducesimilarframequalityasassessedbyFIDandonlydifferintemporalquality.
ThistableincludestheresultsoftheI3DmodelandthreeVideoMAE-v2variants.
Dataset Distortion Type FID FVD FVD FVD FVD
I3D VideoMAE-v2-K710 VideoMAE-v2-SSv2 VideoMAE-v2-PT
S 133.15 1460.18 121.37 277.10 18.33
MotionBlur
ST 133.69(+0.4%) 1705.27(+16.8%) 147.91(+21.9%) 868.31(+213.4%) 39.21(+113.8%)
UCF-101
S 175.47 979.48 167.21 221.83 7.95
Elastic
ST 176.46(+0.6%) 1694.95(+73.0%) 321.96(+92.5%) 1186.91(+435.0%) 58.89(+640.6%)
S 79.11 211.08 88.80 127.99 14.22
MotionBlur
ST 79.35(+0.3%) 286.39(+35.7%) 252.01(+183.8%) 733.41(+473.0%) 35.73(+151.2%)
Sky
S 72.32 149.23 105.04 142.49 6.97
Elastic
ST 72.52(+0.3%) 333.48(+123.5%) 438.19(+317.2%) 1056.40(+641.4%) 60.60(+769.0%)
S 80.42 354.49 95.73 199.90 13.61
MotionBlur
ST 79.57(−1.1%) 367.35(+3.6%) 178.96(+87.0%) 717.08(+258.7%) 23.75(+74.4%)
FFS
S 161.55 589.07 192.01 164.82 11.14
Elastic
ST 161.30(−0.2%) 891.50(+51.3%) 442.62(+130.5%) 969.28(+488.1%) 54.42(+388.4%)
S 169.76 1016.78 100.83 382.37 25.22
MotionBlur
ST 170.10(+0.2%) 1201.35(+18.2%) 177.51(+76.0%) 1217.34(+218.4%) 47.73(+89.3%)
TaiChi
S 182.99 688.55 100.93 161.51 5.81
Elastic
ST 183.21(+0.1%) 1252.72(+81.9%) 372.14(+268.7%) 1467.06(+808.3%) 66.34(+1042.6%)
S 100.65 594.68 89.31 144.95 16.96
MotionBlur
ST 100.62(−0.0%) 678.08(+14.0%) 135.98(+52.3%) 502.09(+246.4%) 29.93(+76.5%)
SSv2
S 143.16 622.87 216.12 211.98 9.74
Elastic
ST 143.91(+0.5%) 980.44(+57.4%) 351.48(+62.6%) 746.91(+252.4%) 48.07(+393.7%)
S 112.22 996.71 92.11 257.01 17.67
MotionBlur
ST 112.85(+0.6%) 1155.53(+15.9%) 126.96(+37.8%) 785.58(+205.7%) 34.34(+94.3%)
K400
S 146.70 675.53 151.50 241.15 8.61
Elastic
ST 146.68(−0.0%) 1189.37(+76.1%) 300.02(+98.0%) 1087.20(+350.8%) 55.01(+539.0%)
providingalargervalue. However,FVDscorescomputed
withtheI3Dbackboneareconsistentlylessthanfromframes
0-16.
Figure 14. DIGAN [91] trained on the Sky Time-lapse dataset
generatesperiodicartifactswhenusingextrapolatedtimesteps.We
showthepercentagechangeofFVDcomputedonevery16frames
comparedwiththefirst16frames.
generated, the motion artifacts become more pronounced.
Asaconsequence,FVDscorescomputedwithVideoMAE
featuresproperly capturethereducedtemporal qualitybyTable6.ResultsofanalyzingthetemporalsensitivityofFVD.WereportFVDsofsyntheticvideoscreatedfromrealvideosusingspatial
onlyorspatiotemporaldistortions,wherethetwosetsproducesimilarframequalityasassessedbyFIDandonlydifferintemporalquality.
ThistableincludestheresultsoftheI3Dmodel,twoTimeSformervariants,andVideoMAE-v1model.
Dataset Distortion Type FVD FVD FVD FVD
I3D TimeSfomer-k400 TimeSfomer-SSv2 VideoMAE-v1-k400
Spatial 1460.18 265.77 311.85 26.44
MotionBlur
Spatiotemporal 1705.27(+16.8%) 275.09(+3.5%) 336.51(+7.9%) 46.58(+76.2%)
UCF-101
Spatial 979.48 260.65 261.27 31.82
Elastic
Spatiotemporal 1694.95(+73.0%) 313.36(+20.2%) 398.29(+52.4%) 79.85(+150.9%)
Spatial 211.08 154.19 133.98 19.39
MotionBlur
Spatiotemporal 286.39(+35.7%) 169.46(+9.9%) 147.69(+10.2%) 62.33(+221.4%)
Sky
Spatial 149.23 123.58 137.43 23.47
Elastic
Spatiotemporal 333.48(+123.5%) 186.33(+50.8%) 249.93(+81.9%) 99.02(+321.8%)
Spatial 354.49 240.65 327.82 21.56
MotionBlur
Spatiotemporal 367.35(+3.6%) 241.97(+0.5%) 311.36(−5.0%) 37.32(+73.1%)
FFS
Spatial 589.07 314.96 390.37 32.61
Elastic
Spatiotemporal 891.50(+51.3%) 392.19(+24.5%) 472.37(+21.0%) 102.92(+215.6%)
Spatial 1016.78 342.29 437.08 26.44
MotionBlur
Spatiotemporal 1201.35(+18.2%) 373.35(+9.1%) 499.24(+14.2%) 56.77(+114.7%)
TaiChi
Spatial 688.55 278.37 276.86 20.88
Elastic
Spatiotemporal 1252.72(+81.9%) 365.80(+31.4%) 465.57(+68.2%) 105.27(+404.2%)
Spatial 594.68 166.16 167.52 21.23
MotionBlur
Spatiotemporal 678.08(+14.0%) 169.68(+2.1%) 184.42(+10.1%) 32.65(+53.8%)
SSv2
Spatial 622.87 265.63 186.04 38.13
Elastic
Spatiotemporal 980.44(+57.4%) 296.53(+11.6%) 245.35(+31.9%) 84.95(+122.8%)
Spatial 996.71 203.54 237.63 18.73
MotionBlur
Spatiotemporal 1155.53(+15.9%) 211.09(+3.7%) 254.55(+7.1%) 33.01(+76.2%)
K400
Spatial 675.53 214.95 206.19 25.40
Elastic
Spatiotemporal 1189.37(+76.1%) 251.35(+16.9%) 297.65(+44.4%) 65.24(+156.9%)Table7.ResultsofprobingtheperceptualnullspaceofFVD.WereportFVDsofnormalandfrozengeneratedvideosbyrandomsampling
(FVD)andsamplingtomatchallthefringefeatures(FVD*).WecolortheFVDdifferenceforbettervisualization:<20%,20%−40%
and>60%.ThedropofFVDonthefrozengeneratedvideosindicatesthevolumeofthenullspacewhereFVDcanbereducedwithout
generatingameaningfulmotion.I3Dhasthelargestperceptualnullspace.
FeatureExtractor I3D VideoMAE-v2-K710 VideoMAE-v2-SSv2 VideoMAE-v2-PT
Model Dataset FVD FVD* FVD FVD* FVD FVD* FVD FVD*
NormalGeneratedVideosvs. RealVideos
DIGAN UCF-101 562.36 220.89(−60.7%) 358.80 160.13(−55.4%) 378.19 260.77(−31.0%) 2.77 2.67(−3.9%)
DIGAN Sky 157.13 54.39(−65.4%) 86.58 61.93(−28.5%) 174.79 128.00(−26.8%) 4.72 3.71(−21.5%)
DIGAN Taichi 132.26 65.72(−50.3%) 58.72 24.45(−58.4%) 313.84 194.17(−38.1%) 4.00 3.66(−8.5%)
TATS UCF-101 329.92 120.58(−63.5%) 176.98 72.95(−58.8%) 388.79 226.39(−41.8%) 7.92 7.12(−10.1%)
TATS Sky 125.62 38.42(−69.4%) 100.27 59.83(−40.3%) 213.33 105.69(−50.5%) 18.11 7.87(−56.5%)
TATS Taichi 124.16 64.17(−48.3%) 37.16 26.08(−29.8%) 274.81 126.53(−54.0%) 5.88 5.34(−9.2%)
StyleGAN-V Sky 56.63 31.73(−44.0%) 180.97 55.54(−69.3%) 219.85 148.11(−32.6%) 10.04 8.78(−12.5%)
StyleGAN-V FFS 56.22 25.87(−54.0%) 77.28 61.02(−21.0%) 194.68 135.30(−30.5%) 1.08 1.04(−3.7%)
PVDM UCF-101 348.81 113.99(−67.3%) 116.01 90.40(−22.1%) 369.14 172.35(−53.3%) 4.51 3.69(−18.2%)
PVDM Sky 59.95 22.94(−61.7%) 141.48 75.12(−46.9%) 142.50 57.04(−60.0%) 3.63 2.33(−35.7%)
FrozenGeneratedVideosvs. RealVideos
DIGAN UCF-101 1303.13 715.96(−45.1%) 357.61 175.13(−51.0%) 951.59 859.57(−9.7%) 12.61 12.23(−3.1%)
DIGAN Sky 230.64 115.55(−49.9%) 175.47 142.86(−18.6%) 408.17 362.84(−11.1%) 13.23 12.16(−8.1%)
DIGAN Taichi 461.79 276.88(−40.0%) 132.96 52.00(−60.9%) 578.61 523.20(−9.6%) 4.40 4.18(−4.9%)
TATS UCF-101 1157.69 616.25(−46.8%) 247.80 107.41(−56.7%) 908.95 805.88(−11.3%) 14.66 13.68(−6.7%)
TATS Sky 279.75 126.32(−54.8%) 172.00 140.37(−18.4%) 375.74 353.15(−6.0%) 21.28 15.76(−25.9%)
TATS Taichi 475.99 312.19(−34.4%) 164.58 64.69(−60.7%) 587.31 530.86(−9.6%) 4.63 4.42(−4.4%)
StyleGAN-V Sky 206.56 104.27(−49.5%) 224.80 91.71(−59.2%) 503.22 456.24(−9.3%) 23.17 21.60(−6.8%)
StyleGAN-V FFS 353.79 242.04(−31.6%) 171.38 147.76(−13.8%) 547.24 520.98(−4.8%) 14.08 14.22(+0.9%)
PVDM UCF-101 1135.61 605.09(−46.7%) 250.52 211.34(−15.6%) 1032.90 898.48(−13.0%) 12.95 12.34(−4.7%)
PVDM Sky 182.77 94.87(−48.1%) 198.50 140.77(−29.1%) 429.06 395.79(−7.8%) 11.54 11.03(−4.4%)