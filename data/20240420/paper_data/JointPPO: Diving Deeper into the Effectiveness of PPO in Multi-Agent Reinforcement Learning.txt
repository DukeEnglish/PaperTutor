JointPPO: Diving Deeper into the Effectiveness of PPO in Multi-Agent
Reinforcement Learning
ChenxingLiu and GuizhongLiu
SchoolofInformationandCommunicationsEngineering,Xi’anJiaotongUniversity
lcx459455791@stu.xjtu.edu.cn,liugz@xjtu.edu.cn
Abstract control [Schrittwieser et al., 2020; Wu et al., 2023], apply-
ing RL to multi-agent systems, known as Multi-Agent Re-
While Centralized Training with Decentralized
inforcementLearning(MARL),emergesasapromisingand
Execution (CTDE) has become the prevailing
challenging research area. In MARL, all the agents consis-
paradigm in Multi-Agent Reinforcement Learning
tentlyinteractwiththeenvironment,optimizingtheirpolicies
(MARL), it may not be suitable for scenarios in
in a trial-and-error manner, with the goal of maximizing the
whichagentscanfullycommunicateandshareob-
expectedcumulativerewards.
servationswitheachother. Fullycentralizedmeth-
The existence of more than one learning agent in MARL
ods, also know as Centralized Training with Cen-
incurs greater uncertainty and learning instability, making
tralized Execution (CTCE) methods, can fullyuti-
it a challenging problem [Nguyen et al., 2020]. To tackle
lize observations of all the agents by treating the
thischallenge,existingMARLalgorithmsprimarilyfallinto
entire system as a single agent. However, tradi-
three categories: fully independent methods, fully central-
tionalCTCEmethodssufferfromscalabilityissues
izedmethods,andcentralizedtrainingwithdecentralizedex-
due to the exponential growth of the joint action
ecution (CTDE) methods. In fully independent methods, as
space. To address these challenges, in this paper
depicted in Figure 1(a), each agent trains and acts indepen-
we propose JointPPO, a CTCE method that uses
dently, making decisions solely based on its own observa-
ProximalPolicyOptimization(PPO)todirectlyop-
tion[Tampuuetal.,2017;deWittetal.,2020]. Thisallows
timize the joint policy of the multi-agent system.
fordirectimplementationofsingle-agentRLalgorithms,but
JointPPO decomposes the joint policy into con-
still suffers from learning instability. CTDE methods, on
ditional probabilities, transforming the decision-
the other hand, allow agents to access the environment state
making process into a sequence generation task.
or all the observations during training while still maintain-
A Transformer-based joint policy network is con-
ing decentralized execution during interactions, as depicted
structed, trained with a PPO loss tailored for the
in Figure 1(c) [Lowe et al., 2017; Foerster et al., 2018;
joint policy. JointPPO effectively handles a large
Rashid et al., 2020b]. While both of these methods have
joint action space and extends PPO to multi-agent
achievedgoodperformance,theyarelimitedtodecentralized
settingwiththeoreticalclarityandconciseness.Ex-
execution, which means that agents share little information
tensive experiments on the StarCraft Multi-Agent
witheachotherandmakedecisionindependently. Suchlim-
Challenge (SMAC) testbed demonstrate the supe-
itation makes them less appropriate for scenarios with suf-
riorityofJointPPOoverstrongbaselines. Ablation
ficient communication, as the information available in other
experimentsandanalysesareconductedtoexplores
agents’observationsisignored. Additionally,thewidespread
thefactorsinfluencingJointPPO’sperformance.
useofparameter-sharingtechniquesinthesemethodsleadsto
suboptimal outcome and homogeneous behavior, potentially
1 Introduction causingfailureincomplexscenarios[Kubaetal.,2021].
Multi-AgentSystems(MAS)arecomplexsystemscomposed Alternatively, fully centralized methods, also known as
ofmultipleagentsthatcooperatewitheachotherinteracting centralizedtrainingwithcentralizedexecution(CTCE)meth-
with a common environment [Dorri et al., 2018]. Such sys- ods, mitigate these limitations by treating the entire multi-
tems are ubiquitous in various real-world scenarios, includ- agent system as a single agent and making full use of all
ingtrafficlightcontrol[Wuetal.,2020],finance[Leeetal., theagents’observations[Liuetal.,2021;Chenetal.,2022].
2007], and robots coordination [Han et al., 2020]. In this Figure1(b)illustratesCTCEmethodsthatbehaveasacentral
paper, we aim at developing an intelligent decision-making controller which integrates all agents’ observation and des-
method for fully cooperative MAS, in which agents act as a ignatestheiractions. Currentcentralizedmethods, however,
unifiedteamtotacklecomplextasks. facechallengesduetotheexponentialgrowthofjointaction
While Reinforcement Learning (RL) has shown remark- spaceswithincreasingnumberofagents.
able success in achieving intelligent decision-making and In this paper, we address those challenges in two steps:
4202
rpA
81
]AM.sc[
1v13811.4042:viXra(𝑎𝑡1,𝑎𝑡2,...,𝑎𝑡𝑛) ~ 𝝅(∙|𝑜𝑡1,𝑜𝑡2,...,𝑜𝑡𝑛)
Central Controller Centralized Critic
Training Phase
𝑎𝑡1~𝜋1(∙|𝑜𝑡1) 𝑎𝑡2~𝜋2(∙|𝑜𝑡2) 𝑎𝑡𝑛~𝜋𝑛(∙|𝑜𝑡𝑛) 𝑎𝑡1 𝑜𝑡1 𝑎𝑡2 𝑜𝑡2 𝑎𝑡𝑛 𝑜𝑡𝑛
Agent 1 Agent 2
...
Agent n
𝑎𝑡1~𝜋1(∙|𝑜𝑡1) 𝑎𝑡2~𝜋2(∙|𝑜𝑡2) 𝑎𝑡𝑛~𝜋𝑛(∙|𝑜𝑡𝑛)
... ...
Agent 1 Agent 2 Agent n Agent 1 Agent 2 Agent n
𝑎𝑡1 𝑜𝑡1 𝑎𝑡2 𝑜𝑡2 𝑎𝑡𝑛 𝑜𝑡𝑛
𝑎𝑡1 𝑜𝑡1 𝑎𝑡2 𝑜𝑡2 𝑎𝑡𝑛 𝑜𝑡𝑛 Execution Phase
Environment Environment Environment
(a) Fullyindependent(DTDE)paradigm.(b) Fullycentralized(CTCE)paradigm. (c) CTDEparadigm.
Figure1:DifferentlearningparadigmsinMARL.
First, we decompose the joint policy of the multi-agent sys- 2 RelatedWorks
temintoconditionalprobabilities,transformingthedecision-
Inrecentyears,therehasbeensignificantprogressinMARL.
makingprocessintoasequencegenerationtask,andpropose
FullyindependentmethodIQL[Tampuuetal.,2017]firstex-
a general framework that solves MARL with any sequence
ploredextendingRLtomulti-agentsettingbyapplyingDQN
generation model. Then we propose JointPPO, a CTCE
to each agent independently. Fully centralized methods, on
method designed to directly optimize the joint policy. As
the other hand, have received less attention since they suf-
aninstanceoftheproposedframework,JointPPOcontainsa
fer from scalability issues due to the exponential growth of
jointpolicynetworkwhichactsasacentralcontroller,taking
jointactionspace. Existingfullycentralizedmethodsusually
all the agents’ observations as input and generating agents’
adoptaninformationexchangemechanismtohandlethelarge
actions sequentially at each decision time. A PPO loss tai-
action space [Liu et al., 2021; Chen et al., 2022]. However,
loredforthejointpolicyisdesignedforthenetworktraining.
in practice they do not exhibit the expected stronger perfor-
Consideredthatthenetworkarchitecturedesignisnotthepri-
mancethanCTDEmethods.
maryfocusofthispaper,weadopttheTransformerstructure
TheCTDEparadigmreachesacompromisebetweenfully
introducedinthestate-of-the-artalgorithmMAT[Wenetal.,
independent and centralized approaches, attracting great at-
2022] with some modifications as our joint policy network.
tention of the MARL community [Oliehoek et al., 2008;
In this way, JointPPO simplifies MARL to single-agent RL
Kraemer and Banerjee, 2016]. Numerous works have
andeffectivelysolvesitbyleveragingtheTransformermodel.
emerged within the CTDE paradigm, encompassing both
Mostimportantly,JointPPObringstheadvantagesofPPOto
value factorization methods and policy gradient methods.
MARLwiththeoreticalclarityandconciseness.
Mostvaluefactorizationmethodsweredesignedtosatisfythe
We extensively evaluate JointPPO on StarCraft Multi-
IGM (Indicidual-Global-Max) condition [Son et al., 2019].
Agent Challenge (SMAC) testbed [Samvelyan et al., 2019]
VDN [Sunehag et al., 2017] first conducted value factoriza-
across various maps, encompassing both homogeneous and
tion by approximating the joint value function as a sum of
heterogeneous scenarios. JointPPO consistently demon-
individual ones. QMIX [Rashid et al., 2020b] extended this
stratessuperiorperformanceanddataefficiencycomparedto
withamonotonicityassumptionandanon-negative-weighted
strong baselines. It achieves nearly 100% win rates across
mixer network. Subsequent efforts usually built upon the
all the test maps and exhibits an remarkable advantage in
structureintroducedinQMIX,furtherapproachingtheIGM
terms of cost for victory such as killed allies. Comprehen-
condition or introducing additional components [Son et al.,
siveablationexperimentsandanalysesarefurtherconducted
2019;Mahajanetal.,2019;Wangetal.,2020a;Rashidetal.,
to explore the elements influencing JointPPO’s training pro-
2020a]. However, these value factorization methods face a
cessandfinalperformance.
commonchallengecausedbythemismatchbetweentheop-
Tosumup,thecontributionsofthisworkareasfollows:
timaljointvaluefunctionandindividualvaluefunctionsdur-
• We explicitly decompose the joint policy of the multi- ing training. Such mismatch necessitates more iterations to
agent system into conditional probabilities, and sum- recoverthesatisfactionofIGM,resultinginlowsampleeffi-
mariseageneralframeworkofsolvingMARLusingany ciency.
sequencegenerationmodel. Among the various policy gradient methods, trust region
methods, represented by Trust Region Policy Optimization
• We propose JointPPO as an instance of the proposed
(TRPO)[Schulmanetal.,2015a]andProximalPolicyOpti-
framework. JointPPO effectively handles the high-
mization (PPO) [Schulman et al., 2017], stand out for their
dimensionaljointactionspacebyleveragingtheTrans-
supreme performance with theoretically-justified monotonic
formermodel.
policy improvement [Kakade and Langford, 2002]. Numer-
• As a CTCE method, JointPPO’s performance demon- ous studies have tried to extend this advantage to the multi-
strates the feasibility of addressing MARL by simpli- agent setting. While IPPO [de Witt et al., 2020] applied
fying it into single-agent RL, which facilitates the ef- PPOindependentlytoeachagent,MAPPO[Yuetal., 2022]
fectiveintegrationoftechniquesandresearchoutcomes introduced centralized critics and comprehensively explored
fromsingle-agentRLintothedomainofMARL. factors that influences its performance. HAPPO [Kuba etal., 2021] presented an Multi-Agent Advantage Decompo- 3.2 Multi-AgentTransformer
sition Theorem and proposed a sequential update scheme.
The state-of-the-art algorithm, Multi-Agent Transformer
MAT[Wenetal., 2022], themostrelevantworkstothispa- (MAT)[Wenetal.,2022],firsteffectivelysolveMARLprob-
per,wasderivedfromtheMulti-AgentAdvantageDecompo-
lem by transforming it into a sequence generation problem
sitionTheoremandintroducedanovelapproachofleveraging
andleveragingtheTransformermodeltomaptheinputofthe
thesequencemodeltogenerateagents’actionssequentially. agents’observations(cid:0) o1,...,on(cid:1)
totheoutputoftheagents’
t t
There are also lots of recent work following the sequential
actions
(cid:0) a1,...,an(cid:1)
. It consists of an encoder and a de-
updateschemeortheaction-dependentscheme[Wangetal., t t
coder, separately used to learn a valid representation of the
2023b; Kuba et al., 2022; Li et al., 2021; Bertsekas, 2021;
jointobservationsandtooutputtheagents’actionsinanauto-
Li et al., 2023]. However, those methods either suffer from
regressivemanner. MAT’straininglossisdesignedbasedon
sampleinefficiencyorrequiresophisticatedtheoreticalanal-
the Multi-Agent Advantage Decomposition Theorem, which
yses,whicharesusceptibletovulnerabilities. Incontrast,we
decomposes the joint advantage function of the multi-agent
startfromtheperspectiveofCTCE,proposeapracticalalgo-
systemintoindividualadvantagefunctionsandimpliesase-
rithm JointPPO that uses PPO to directly optimize the joint
quential update scheme. However, we argue that the loss in
policy. SinceJointPPOisderivedwithoutanyassumptionof
itsimplementationdoesnotstrictlyadheretothementioned
valuedecompositionorcreditassignment,itnaturallyinher-
theorem,asitgeneratesagents’theactionssequentiallyrather
itsthetheoreticalguaranteeofmonotonicimprovementfrom
thanupdatingtheagents’actionssequentially. Besides,MAT
PPOforthejointpolicy.
remainsatthestageofoptimizingindividualpolicies,without
considering a direct optimization for the joint policy. All of
3 Preliminaries
those lead to a mismatch between its theoretical foundation
3.1 PODMP andpracticalimplementation.
Despitethoseconcerns,MAT’suseoftheTransformerhas
Weconsiderthedecision-makingprobleminthefullycoop-
beenasignificantcontributionandsuccess. Therefore,inthis
erative multi-agent systems described by Partially Observ-
paper,weadoptitsTransformerarchitecturewithsomemod-
ableMarkovDecisionProcesses(POMDP)[Kaelblingetal.,
ificationstoconstructourjointpolicynetwork. Mostimpor-
1998]. An n-agent POMDP can be formalized as a tuple
tantly,weredesignthePPOlossfromafullycentralizedper-
⟨N,S,O,A,P,R,γ⟩, where N = {1,...,n} is the set of
spective, aiming to provide theoretical clarity and concise-
agentsandS istheglobalstatespaceoftheenvironment. We
ness.
denotethelocalobservationandactionspaceofagentibyOi
andAi respectively,andsubsequently,theCartesianproduct
4 Method
O = O1×,...,×On represents the joint observation space
of the system and A = A1×,...,×An represents the joint In this section, we present details of JointPPO in three sub-
actionspace. P : S ×A×S → [0,1]isthetransitionfunc- sections: problem modeling, Transformer-based joint policy
tiondenotingthestatetransitionprobability. R:S×A→R network, and joint PPO loss. In problem modeling, we dis-
representstherewardfunctionwhichgivesrisetotheinstan- cussthedecompositionofthejointpolicyandproposeagen-
taneousrewardandγ ∈[0,1)isthediscountfactorthatgives eralframeworkthatsolvesMARLwithsequencegeneration
smallerweightstofuturerewards. model. In the next two subsections, we present the detail of
InPOMDP,eachagenti ∈ N onlyhaveaccesstotheob- JointPPO in term of its network architecture and loss func-
servation oi ∈ Oi to the environment rather than the global tion,asaninstanceoftheproposedframework.
t
state s ∈ S. At each time step t, all agents i ∈ N choose
t
their actions ai ∈ Ai which may be discrete or continu- 4.1 ProblemModeling
t
ous, and all the actions together forming the joint action Asmentionedearlier,thegoalforMARListolearnanopti-
a t = (cid:0) a1 t,...,an t(cid:1) ∈ A. Executing the joint action a t, the mal joint policy π∗(a t|o t) that maximizes the expected ac-
agentsstimulatetheenvironmentintothenextstateaccording cumulatedreturn(Eq.(1)). Mostexistingmethodshandlethe
to the transition function P and, at the same time, receive a jointpolicyπbydecomposingitintoindependentindividual
scalarteamrewardr t =R(s t,a t). Repeatingtheabovepro- policies:
cess, agents consistently interact with the environment and
collect the rewards. We define the joint policy π(a t|o t) as
π(a |o )=π(cid:0) a1,a2,...,an|o (cid:1)
=(cid:89)n
πi(cid:0) ai|o (cid:1) . (2)
a conditional probability of the joint action a t given all the t t t t t t t t
agents’ observations o =
(cid:0) o1,...,on(cid:1)
∈ O, and return i=1
t t t
G wh( eτ r) e=
τ
d(cid:80) en∞ t o= t0 esγ tt hr et a ss amth pe lea dcc tu ram jeu cl ta ot re yd .d Ti hsc eo gu on ate ld ofre Mw Aar Rds L, N vio dt ua ab ll py, olt ih ce yr ,e suar ce hs ae sv πer ia (cid:0)l ad i|i off ie (cid:1)re on rt πf io (cid:0)r am iu |τla it (cid:1)i ,o wns hio cf ht dh ie ffi en rd ii n-
t t t t
istolearnanoptimaljointpolicyπ∗ thatmaximizestheex- theavailableinformationusedfordecisionmaking.However,
pectedreturn: the underlying assumption remains unchanged: the agents’
actionsareindependentofeachother.
π∗ =argmaxE [G(τ)]
π Whilesuchindependenceassumptionfacilitatesdecentral-
π
(cid:34) ∞ (cid:35) (1) ized execution, it has some shortcomings. First, as high-
=argmaxE (cid:88) γtR(cid:0) s ,a | (cid:1) . lightedintheintroduction,decentralizedexecutionisnotuni-
π
π t t at∼π(at|ot)
versally suitable. It is restricted to scenarios where commu-
t=0𝑉𝑉(𝒐𝒐𝑡𝑡) 𝜋𝜋𝑚𝑚 (�|𝒐𝒐𝒕𝒕,𝑎𝑎𝑡𝑡1:𝑚𝑚−1 )
average pooling
critic feed forward add & norm
feed forward
feed forward
𝒐𝒐�𝑡𝑡=(𝑜𝑜�𝑡𝑡1 ,𝑜𝑜�𝑡𝑡2 ,...,𝑜𝑜�𝑡𝑡𝑛𝑛 ) add & norm
add & norm
cross-attention multi-head attention
feed forward slef-attention block
block
add & norm 𝒐𝒐�𝑡𝑡
add & norm
masked
multi-head attention observation action multi-head attention
embedding embedding
1 2 𝑛𝑛 1 2 𝑚𝑚−1
𝒐𝒐𝑡𝑡=(𝑜𝑜𝑡𝑡,𝑜𝑜𝑡𝑡,...,𝑜𝑜𝑡𝑡) 𝑎𝑎𝑡𝑡,𝑎𝑎𝑡𝑡,...,𝑎𝑎𝑡𝑡
Figure2:ArchitectureoftheTransformer-basedpolicynetwork.
nicationamongagentsislimited. Incontrast,real-worldsit- State transition
uations often involve agents with robust communication ca-
pabilities, enabling them to freely share observations. This
𝑠𝑠𝑡𝑡+1~𝑃𝑃(�|𝑠𝑠𝑡𝑡,𝑎𝑎𝑡𝑡)
sharing of observations enhances agents’ awareness to the 𝑠𝑠𝑡𝑡 𝑠𝑠𝑡𝑡+1
environmentandcanleadtobettercooperation,whiledecen-
tralizedexecutionneglectsthis. Second,thereexistsituations
𝒐𝒐𝑡𝑡 𝒐𝒐𝑡𝑡+1
where the agents’ actions exhibit interdependence. Agents 1 1
in a system can sometimes be divided into dominant agents
𝑎𝑎𝑡𝑡~𝜋𝜋 (�|𝒐𝒐𝒕𝒕)
1 Action
andassistantagents[Wangetal.,2023a;Zhangetal.,2022; 𝑎𝑎𝑡𝑡 2 2 1 generation
Ruanetal.,2022b;Duetal.,2022]. Theactionsofthedom- 𝑎𝑎𝑡𝑡~𝜋𝜋 (�|𝒐𝒐𝒕𝒕,𝑎𝑎𝑡𝑡)
2
inantagentscarrygreaterimportance,grantingthempriority 𝑎𝑎𝑡𝑡
indecision-making. Subsequently,theassistantagentsmake … 𝑛𝑛 𝑛𝑛 1:𝑛𝑛−1
𝑎𝑎𝑡𝑡~𝜋𝜋 (�|𝒐𝒐𝒕𝒕,𝑎𝑎𝑡𝑡 )
decisions based on the dominant agents’ actions, playing a 𝑛𝑛
supportiverole.Suchacooperativepatternisalsocommonin Joint a𝑎𝑎c𝑡𝑡tion
humansociety,whereindividualactionsarenotindependent 𝒂𝒂𝑡𝑡=(𝑎𝑎𝑡𝑡1 ,𝑎𝑎𝑡𝑡2 ,...,𝑎𝑎𝑡𝑡𝑛𝑛 )
Figure3:Actiongenerationprocess.
but rather correlated, challenging the independence assump-
tion.
Therefore, we propose an alternative decomposition also contributes to handling the exponential growth of the
methodforthejointpolicythatdoesnotrelyontheassump- jointactionspace.
tion of independence among agents’ actions. Formally, we Whileourproposedframeworkdoesnotnecessitatethein-
decomposethejointpolicyintoconditionalprobabilities: dependence assumption, it does require a pre-specified or-
π(a |o )=π(cid:0) a1,a2,...,an|o (cid:1) (3) der for the actions generation, implying some dependencies
t t t t t t among the agents. We view this order as prior knowledge
n
=(cid:89) πi(cid:0) ai|o ,a1:i−1(cid:1) , (4) aboutthesystemandwewilldiscussthisfurtherintheabla-
t t t tionexperiments.
i=1
where πi(cid:0) ai|o ,a1:i−1(cid:1) , called the conditional individual 4.2 Transformer-BasedJointPolicyNetwork
t t t
policy,istheconditionalprobabilityoftheith agent’saction Having transformed the decision-making process into a
giventhejointobservationandtheprecedingagents’actions. sequence generation task, we can take advantage of
In this way, the decision-making process of MAS is explic- any state-of-the-art sequence models. The Transformer
itly transformed into a sequence generation task. Given the model [Vaswani et al., 2017], originally designed for ma-
joint observation o at each time step, the actions of agents chine translation tasks, has exhibited strong sequence mod-
t
aregeneratedsequentially,asillustratedinFigure3. elingabilities. Hence,weadopttheTransformerarchitecture
Anysequencegenerationmodelhasthepotentialtotackle introducedinMATtoconstructourjointpolicynetwork.
this task. Consequently, we propose a generalized frame- AsillustratedinFigure2,ourTransformer-basedjointpol-
work, outlined in Algorithm 1, designed to solves MARL icy network consists of an encoder, a decoder, and a cen-
withanysequencegenerationmodel. Thisframeworkoffers tralized critic. The encoder, whose parameters are denoted
the benefit of simplifying MARL into single-agent RL. Be- by ϕ, plays a vital role of learning an effective representa-
sides,theapplicationofpowerfulsequencegenerationmodel tion of the original observations oˆ =
(cid:0) oˆ1,...,oˆn(cid:1)
. This is
t t tAlgorithm1AGeneralFrameworkofSolvingMARLUsing joint observation value function V (oˆ), which is approxi-
ψ t
SequenceGenerationModel mated by the centralized critic, to estimate the joint advan-
Input: Number of agents n, the action generation order tagefunction, followingtheGeneralizedAdvantageEstima-
(i ,...,i ). tion(GAE)[Schulmanetal.,2015b]as:
1 n
Initialize: Acentralizedcritic,asequencegenerationmodel, h
andareplaybuffer. A(o ,a )=(cid:88) (γλ)lδ , (5)
t t t+l
1: repeat l=0
2: Consistently interact with the environment using the whereδ =r +γV (oˆ )−V (oˆ)istheTDerrorattime
t t ψ t+1 ψ t
sequencegenerationmodelasthejointpolicynetwork, step t and h is GAE steps. Similar to IPPO [de Witt et al.,
whichtakesallagents’observationasinputandgener- 2020],weformulatethecritic’slossastheerrorbetweenthe
atesnactionssequentiallyintheorderofi 1,i 2,...,i n. predicted joint observation value V ψ(oˆ t) and its estimated
Collect interaction data and input the data into replay valuebasedontherealcollectedrewards:
3:
b Tu raff ie nr t.
hecentralizedcriticusingthesampleddatafrom L =
1 T (cid:88)−1 min(cid:20)(cid:16)
V
(oˆ)−Vˆ(cid:17)2 ,(cid:16)
V (oˆ)+
thereplaybuffertoapproximatethevaluefunction. critic T ψ t t ψold t
t=0
4: Trainthejointpolicynetworkusingthesampleddate (cid:17)2(cid:21)
fromthereplaybufferandthevaluefunctionapproxi- clip(V (oˆ)−V (oˆ),−ϵ,+ϵ)−Vˆ ,
ψ t ψold t t
matedbycentralizedcritic.
(6)
5: untilThedesiredperformanceisachieved
where Vˆ = A +V (oˆ) and ψ are the old parameters
Output: Atrainedjointpolicynetwork. t t ψ t θold
beforetheupdate. Eq.(6)restrictstheupdateofthecentral-
izedvalue functiontowithina trustregion, preventingfrom
overfitting to previous data as the data distribution continu-
achieved through a self-attention block consisting of a self-
ally changes with the evolving policy. Since the critic takes
attention mechanism, Multi-Layer Perceptrons (MLPs), and
theencodedobservationoˆ asinput,thislossalsocontributes
residualconnections. Suchcomputationalblocktakesallthe t
agents’originalobservationso =
(cid:0) o1,...,oi(cid:1)
asinput, in-
tothetrainingoftheencodertolearnanexpressiverepresen-
t t t tationoftheobservations.
tegratestask-relatedinformationandoutputstheencodedob-
servations oˆ =
(cid:0) oˆ1,...,oˆn(cid:1)
. Then those coded observa-
As for policy training, we employ PPO on the generated
t t t joint policy. The joint policy π (a |oˆ) is computed fol-
tionsarepassedthroughthecentralizedcritic,whoseparam- θ t t
lowing Eq. (4) by multiplying the generated conditional lo-
etersaredenotedbyψ,tocalculatethejointobservationvalue
cal policies. Formally, the joint PPO loss is constructed as
V (oˆ),whichisthenusedtocalculatethejointPPOloss.
ψ t follows:
The decoder, whose parameters are denoted by θ, acts
T−1
as a sequence generator that generates the agent’s action 1 (cid:88) (cid:16) (cid:17)
a t =(cid:0) a1 t,...,ai t(cid:1) inanauto-regressionmanner.Specifically, L policy =− T min α tA t,clip(α t,1±ϵ)A t , (7a)
thisprocessbeginswithaninputofainitialtokenaswellas t=0
the encoded observation oˆ , and output of the first agent’s where
t
conditional individual policy π θ1(cid:0) a1 t|oˆ t(cid:1) , which is actually
α =
π θ(a t|oˆ t)
(7b)
the probability distribution over possible actions. The first t π (a |oˆ)
agent’s action a1
t
is sampled from this distribution, encoded (cid:81)θo nld πt i(cid:0)t ai|oˆ,a1:i−1(cid:1)
asaone-hotvector,andthenfedbackintothedecoderasthe = i=1 θ t t t .
second token. Subsequently, the second agent’s action a2
t
is (cid:81)n i=1π θi old(cid:0) ai t|oˆ t,a t1:i−1(cid:1)
sampledaccordingtotheoutputconditionalindividualpolicy Eq. (7a) presents a direct use of PPO on the joint policy
π θ2(cid:0) a2 t|oˆ t,a1 t(cid:1) . This process continues until all the agents’ π θ(a t|oˆ t), whose update is restricted to within the trust re-
actionsaregenerated,togetherformingthejointaction. The gion. ThetheoreticaladvantageofPPOguaranteesamono-
decoderblockconsistsofamaskedself-attentionmechanism, tonicimprovementofthejointpolicy. Inthiswayweextend
amaskedcross-attentionmechanism,MLPsandsomeresid- PPOtomulti-agentsettingsmoothly. Havingconstructedthe
ual connections. The masked cross-attention mechanism is loss function for both the critic and the policy, the overall
used to integrate the encoded observation, where “masked” learninglosscanbecomputedby:
indicatesthattheattentionmatrixisanuppertriangularma-
n
ptr ri ex ce en ds inu grin gg ent eh ra at tee dac ah cta iog ne snt a’s j,ja <ct ii .on ai t depends only on its L(θ,ϕ,ψ)=L critic+λ 1L policy+λ 2(cid:88) H(π θi), (8)
t i=1
whereH(πi)denotestheentropyoftheconditionalindivid-
4.3 JointPPOLoss θ
ual policy πi, serving to prevent from early convergence to
θ
For network training, we use PPO algorithm to directly op- suboptimal solutions, and λ ,λ are the weight parameters.
1 2
timize the joint policy. To achieve this, an accurate ap- JointPPO is a fully centralized method as it operates on all
proximationofthejointobservationvaluefunctionV (o )is variables at the joint level, and that is also the origin of its
t
necessary, so we build the loss functions for the critic and name. ThecompletepseudocodeforJointPPOisprovidedin
the policy separately. For the critic’s loss, We first use the supplementarymaterials.Table1:PerformanceEvaluationsofwinrateandstandarddeviationontheSMACtestbed.
Task Type Difficulty JointPPO MAT MAPPO HAPPO Steps
5m vs 6m Homogeneous Hard 89.06(0.03) 72.81(0.17) 85.62(0.05) 61.56(0.07) 1e7
8m vs 9m Homogeneous Hard 98.44(0.01) 97.81(0.02) 97.50(0.01) 65.31(0.03) 5e6
10m vs 11m Homogeneous Hard 99.69(0.00) 98.12(0.02) 93.75(0.03) 75.31(0.09) 5e6
27m vs 30m Homogeneous SuperHard 100.00(0.00) 95.63(0.04) 91.25(0.04) 0.00(0.00) 1e7
6h vs 8z Homogeneous SuperHard 92.5(0.04) 97.81(0.01) 77.50(0.17) 0.08(0.05) 1e7
MMM Heterogeneous Easy 99.69(0.01) 96.88(0.00) 97.50(0.02) 0.00(0.00) 2e6
3s5z Heterogeneous Hard 96.88(0.01) 92.19(0.05) 96.25(0.02) 31.56(0.18) 3e6
MMM2 Heterogeneous SuperHard 97.19(0.02) 88.44(0.08) 89.06(0.07) 0.01(0.01) 1e7
3s5z vs 3s6z Heterogeneous SuperHard 91.56(0.03) 95.63(0.03) 62.81(0.04) 88.12(0.07) 2e7
(a) Learningcurvesofwinrate.
(b) Learningcurvesofthenumberofkilledallies.
Figure4:ComparisonofJointPPOagainstbaselinesonfourSMACmaps.
5 Experiments in [Wang et al., 2020b], we compute the win rates over 32
evaluation games after each training iteration and take the
Inthissection,weevaluateJointPPOacrossvarioustasksin
median of the final ten evaluation win rates as the perfor-
theStarCraftMulti-AgentChallenge(SMAC)testbed.
manceforeachseed. However,evaluatingalgorithmssolely
5.1 SMACTestbed basedonwinratesisnotenough,asthereexitsituationsthat
twoalgorithmwithsamewinratesmaydifferintermsofthe
SMAC(StarCraftMulti-AgentChallenge)[Samvelyanetal.,
costpaidforthevictory, suchasthenumberofkilledallies.
2019]isatestbedforMARLthatoffersadiversesetofStar-
Therefore,wefurtherrecordthenumberofkilledalliesinthe
Craft II unit micromanagement tasks of varying difficulties.
evaluation game as an additional performance metric. More
In these tasks, a collaborative team of algorithm controlled
detailsarepresentedinsupplementarymaterials.
unitsneedtodefeatanenemyteamofunitscontrolledbythe
built-inAI.TheunitsinSMACarealsodiverse. Inhomoge-
5.2 JointPPO’sPerformance
neoustasks,theunitscomprisingateamareofthesametype,
whereas heterogeneous tasks mean the opposite. Successful WepresenttheperformanceofJointPPOonseveralrepresen-
strategiesoftenrequireprecisecoordinationamongtheunits, tative tasks, covering both homogeneous and homogeneous
executingtacticssuchasfocusedattackorkitingtogainpo- settings. We compare JointPPO with PPO-based algorithms
sitionaladvantages. MAPPO, HAPPO, and SOTA algorithm MAT. We use the
Forourexperiments,weusegameversion4.6,andallthe samehyperparametersofthesebaselinealgorithmsfromtheir
evaluation results are averaged over 5 random seeds. For originalpaperstoensuretheirbestperformanceandfaircom-
eachrandomseed, followingtheevaluationmetricproposed parisons. The evaluation results and learning curves of win 0 0 0   0 0 0   0 0 0   0 0 0 
               
               
   
           
   
           
     H S R F K   
     H  H  0  0 S  S  H  H R  R  D  G F  F  Q  L D K  K  Q            /     L Q H           -  0  0 R  $  $ L Q  7  7 W  B 3  Z 3  L 2  W K B V D P H B S D U D P H W H U           Q       R          B       H       Q       W       U R S \ B O R V V           '  ,  5 Q H  D Y I  Q H D  G U X  V  R O  H W  P    2  2   2 U  U G  G  U H  G H U  H U  U
         &   O L S S L Q J  3 D U D P H W H   U                 ( Q   Y    L U R Q P H Q W  6   W H  
 S V
          H 
 
         ( Q   Y    L U R Q P H Q W  6   W H  
 S V
          H 
 
         ( Q   Y    L U R Q P H Q W  6   W H  
 S V
          H 
 
(a) (b) (c) (d)
Figure5:Resultsofablationexperiments.(a):Effectoftrainingepochsandclippingparameter.(b):ComparisonbetweenJointPPO,original
MATandMATwiththesamehyperparameterasJointPPO.(c):ComparisonofJointPPOwithdifferentweightedentropyloss.(d):Compar-
isonofJointPPOwithdifferentactiongenerationorder.
ratesarepresentedinTable1andFigure4(a). JointPPOex- rameter λ (Eq. 8). A well tuned λ is supposed to strike a
2 2
hibits competitive performance with baseline algorithms in balance between exploration and exploitation. Here, we in-
termsoffinalwinratesandsampleefficiency. Remarkably,it vestigatehowentropylosswithdifferentweightsaffectsthe
achievesnearly100%testwinratesacrossalltestmaps, in- trainingprocess.Figure5(c)showsthat,asmallerweightcan
cludingthesuperhardheterogeneoustaskMMM2,whichis result in faster convergence while a larger one does the op-
noteasilysolvedbyexistingmethods. Additionally,wesur- posite. However,whenit’ssettoosmall,suchasanextreme
prisinglyobservethatJointPPOdemonstratesalowercostof case, zero, it brings a lower final win rate which indicates a
killedalliedforvictoryinmosttasks(Figure4(b)),indicating suboptimalconvergence. Therefore,inallofourexperiment,
betteroptimality,whichweattributetoitsdirectoptimization theweightλ issetasancompromiseas0.1.
2
ofthejointpolicy.
ActionGenerationOrder
5.3 AblationStudies Asmentionedearlierthatourproposedframeworkrequiresa
designatedactiongenerationorder.Intheaboveexperiments,
We conduct ablation experiments and analyses on factors
forconvenience, weusethedefaultordergivenbytheenvi-
that’s important in JointPPO’s implementation including:
ronment. HerewestudyhowthisorderinfluencesJointPPO.
PPO training epochs and clipping parameter, entropy loss,
Weconductexperimentswithinverseorderandrandomorder
and action generation order. Each factor is studied through
compared to the default order. Figure 5(d) shows that these
aseriesofexperimentsonthesuperhardheterogeneoustasks
threesettingshavelittledifferenceinperformance,whichval-
MMM2.
idatesthatJointPPOisquiterobusttotheorderofagents.De-
PPOTrainingEpochsandClippingParameter spitethis,weareinterestedintheideaofintroducinglearned
Thesetwoparametersareamongthemostinfluentialhyper- dependency such as [Ruan et al., 2022a], so that the algo-
parametersaffectingJointPPO’straining. Weconductexper- rithmcanautomaticallyadjustthegenerationorderbasedon
iments with different combination of these two parameters. thesituation,whichweleaveforfuturework.
TheresultsarepresentedinFigure5(a). Generally, thefinal
winratesarepositivelycorrelatedwithtrainingepochs,while 6 Conclusion
negatively correlated with the clipping parameter. However,
In this paper, we decompose the joint policy of the multi-
as the training epoch further increases, the training process
agent system into conditional probabilities and introduce a
will crush, which was observed during experiments. This
framework that solves MARL using sequence generation
trendvalidatestheexplanationin[Yuetal.,2022]thatgreater
model. By leveraging the Transformer model, the pro-
trainingepochsbringhighersampleefficiency,butmayhurt
posed CTCE algorithm, JointPPO, effectively handles high-
the optimality of convergence due to an overfitting on old
dimensionaljointactionspacesanddemonstratecompetitive
data. A larger clipping parameter may cause instability, as
performanceonSMAC.Forfuturework,weareinterestedin
seenintheexperimentwithepoch=5,clippingparameter=0.2.
extending this work with learned agents’ dependencies and
WefurtherconductexperimentsusingMATwiththesame
integratingwiththeprosperousdevelopmentsinsingle-agent
trainingepochsandclippingparameterasJointPPO.Wecom-
RLforfurtheradvancements.
pareitsperformancewithJointPPOandoriginalMATinFig-
ure5(b). TheresultsshowthatMATwiththesameparame-
References
tersperformworsethantheitsoriginalsettings,therebyelim-
inatingtheimpactofparametertuningontheJointPPO’sim- [Bertsekas,2021] Dimitri Bertsekas. Multiagent reinforce-
provedperformance. ment learning: Rollout and policy iteration. IEEE/CAA
JournalofAutomaticaSinica,8(2):249–272,2021.
EntropyLoss
The entropy loss is a crucial component contributing to the [Chenetal.,2022] Yiqun Chen, Wei Yang, Tianle Zhang,
learning loss. Its strength is determined by the weight pa- ShiguangWu,andHongxingChang.Commander-soldiers
 H W D 5  Q L :  H W D 5  Q L :  H W D 5  Q L :  H W D 5  Q L :reinforcement learning for cooperative multi-agent sys- in marl via trust-region decomposition. arXiv preprint
tems. In 2022 International Joint Conference on Neural arXiv:2102.10616,2021.
Networks(IJCNN),pages1–7.IEEE,2022.
[Lietal.,2023] Chuming Li, Jie Liu, Yinmin Zhang,
[deWittetal.,2020] Christian Schroeder de Witt, Tarun Yuhong Wei, Yazhe Niu, Yaodong Yang, Yu Liu, and
Gupta, Denys Makoviichuk, Viktor Makoviychuk, Wanli Ouyang. Ace: Cooperative multi-agent q-learning
Philip HS Torr, Mingfei Sun, and Shimon Whiteson. with bidirectional action-dependency. In Proceedings of
Is independent learning all you need in the starcraft theAAAIconferenceonartificialintelligence,volume37,
multi-agentchallenge? arXivpreprintarXiv:2011.09533, pages8536–8544,2023.
2020.
[Liuetal.,2021] Bo Liu, Qiang Liu, Peter Stone, Ani-
[Dorrietal.,2018] AliDorri, SalilSKanhere, andRajaJu- mesh Garg, Yuke Zhu, and Anima Anandkumar. Coach-
rdak. Multi-agent systems: A survey. Ieee Access, player multi-agent reinforcement learning for dynamic
6:28573–28593,2018. team composition. In International Conference on Ma-
[Duetal.,2022] Wei Du, Shifei Ding, Chenglong Zhang, chineLearning,pages6860–6870.PMLR,2021.
andZhongzhiShi.Multiagentreinforcementlearningwith [Loweetal.,2017] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean
heterogeneous graph attention network. IEEE Transac- Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-
tionsonNeuralNetworksandLearningSystems,2022. agentactor-criticformixedcooperative-competitiveenvi-
[Foersteretal.,2018] Jakob Foerster, Gregory Farquhar, ronments. Advancesinneuralinformationprocessingsys-
Triantafyllos Afouras, Nantas Nardelli, and Shimon
tems,30,2017.
Whiteson. Counterfactualmulti-agentpolicygradients. In [Mahajanetal.,2019] Anuj Mahajan, Tabish Rashid,
Proceedings of the AAAI conference on artificial intelli- Mikayel Samvelyan, and Shimon Whiteson. Maven:
gence,volume32,2018. Multi-agent variational exploration. Advances in neural
[Hanetal.,2020] RuihuaHan,ShengduoChen,andQiHao. informationprocessingsystems,32,2019.
Cooperative multi-robot navigation in dynamic environ- [Nguyenetal.,2020] Thanh Thi Nguyen, Ngoc Duy
ment with deep reinforcement learning. In 2020 IEEE Nguyen, and Saeid Nahavandi. Deep reinforcement
International Conference on Robotics and Automation learning for multiagent systems: A review of challenges,
(ICRA),pages448–454.IEEE,2020. solutions, and applications. IEEE transactions on
[Kaelblingetal.,1998] Leslie Pack Kaelbling, Michael L cybernetics,50(9):3826–3839,2020.
Littman, and Anthony R Cassandra. Planning and acting [Oliehoeketal.,2008] Frans A Oliehoek, Matthijs TJ
inpartiallyobservablestochasticdomains. Artificialintel- Spaan, and Nikos Vlassis. Optimal and approximate q-
ligence,101(1-2):99–134,1998. valuefunctionsfordecentralizedpomdps. JournalofAr-
[KakadeandLangford,2002] ShamKakadeandJohnLang- tificialIntelligenceResearch,32:289–353,2008.
ford. Approximately optimal approximate reinforcement [Rashidetal.,2020a] TabishRashid,GregoryFarquhar,Bei
learning. In Proceedings of the Nineteenth International Peng, and Shimon Whiteson. Weighted qmix: Expand-
ConferenceonMachineLearning,pages267–274,2002. ingmonotonicvaluefunctionfactorisationfordeepmulti-
[KraemerandBanerjee,2016] Landon Kraemer and agent reinforcement learning. Advances in neural infor-
Bikramjit Banerjee. Multi-agent reinforcement learning mationprocessingsystems,33:10199–10210,2020.
as a rehearsal for decentralized planning. Neurocomput- [Rashidetal.,2020b] Tabish Rashid, Mikayel Samvelyan,
ing,190:82–94,2016.
ChristianSchroederDeWitt,GregoryFarquhar,JakobFo-
[Kubaetal.,2021] Jakub Grudzien Kuba, Ruiqing Chen, erster, and Shimon Whiteson. Monotonic value function
Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, factorisationfordeepmulti-agentreinforcementlearning.
and Yaodong Yang. Trust region policy optimisation The Journal of Machine Learning Research, 21(1):7234–
in multi-agent reinforcement learning. arXiv preprint 7284,2020.
arXiv:2109.11251,2021. [Ruanetal.,2022a] Jingqing Ruan, Yali Du, Xuantang
[Kubaetal.,2022] Jakub Grudzien Kuba, Xidong Feng, Xiong,DengpengXing,XiyunLi,LinghuiMeng,Haifeng
Shiyao Ding, Hao Dong, Jun Wang, and Yaodong Zhang,JunWang,andBoXu.Gcs:graph-basedcoordina-
Yang. Heterogeneous-agent mirror learning: A contin- tionstrategyformulti-agentreinforcementlearning.arXiv
uum of solutions to cooperative marl. arXiv preprint preprintarXiv:2201.06257,2022.
arXiv:2208.01682,2022. [Ruanetal.,2022b] Jingqing Ruan, Linghui Meng, Xuan-
[Leeetal.,2007] Jae Won Lee, Jonghun Park, O Jangmin, tangXiong,DengpengXing,andBoXu. Learningmulti-
JongwooLee,andEuyseokHong. Amultiagentapproach agent action coordination via electing first-move agent.
to q-learning for daily stock trading. IEEE Transactions In Proceedings of the International Conference on Auto-
on Systems, Man, and Cybernetics-Part A: Systems and mated Planning and Scheduling, volume 32, pages 624–
Humans,37(6):864–877,2007. 628,2022.
[Lietal.,2021] WenhaoLi,XiangfengWang,BoJin,Junjie [Samvelyanetal.,2019] Mikayel Samvelyan, Tabish
Sheng,andHongyuanZha. Dealingwithnon-stationarity Rashid, Christian Schroeder De Witt, Gregory Farquhar,Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, [Wangetal.,2023b] Xihuai Wang, Zheng Tian, Ziyu Wan,
Philip HS Torr, Jakob Foerster, and Shimon Whiteson. Ying Wen, Jun Wang, and Weinan Zhang. Order mat-
The starcraft multi-agent challenge. arXiv preprint ters: Agent-by-agent policy optimization. arXiv preprint
arXiv:1902.04043,2019. arXiv:2302.06205,2023.
[Schrittwieseretal.,2020] Julian Schrittwieser, Ioannis [Wenetal.,2022] Muning Wen, Jakub Kuba, Runji Lin,
Antonoglou, Thomas Hubert, Karen Simonyan, Laurent WeinanZhang,YingWen,JunWang,andYaodongYang.
Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Multi-agent reinforcement learning is a sequence model-
Demis Hassabis, Thore Graepel, et al. Mastering atari, ingproblem. AdvancesinNeuralInformationProcessing
go, chess and shogi by planning with a learned model. Systems,35:16509–16521,2022.
Nature,588(7839):604–609,2020.
[Wuetal.,2020] Tong Wu, Pan Zhou, Kai Liu, Yali Yuan,
[Schulmanetal.,2015a] John Schulman, Sergey Levine, Xiumin Wang, Huawei Huang, and Dapeng Oliver Wu.
PieterAbbeel,MichaelJordan,andPhilippMoritz. Trust Multi-agent deep reinforcement learning for urban traffic
regionpolicyoptimization. InInternationalconferenceon lightcontrolinvehicularnetworks. IEEETransactionson
machinelearning,pages1889–1897.PMLR,2015. VehicularTechnology,69(8):8243–8256,2020.
[Schulmanetal.,2015b] John Schulman, Philipp Moritz, [Wuetal.,2023] PhilippWu,AlejandroEscontrela,Danijar
SergeyLevine,MichaelJordan,andPieterAbbeel. High- Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer:
dimensional continuous control using generalized advan- Worldmodelsforphysicalrobotlearning. InConference
tageestimation. arXivpreprintarXiv:1506.02438,2015. onRobotLearning,pages2226–2240.PMLR,2023.
[Schulmanetal.,2017] John Schulman, Filip Wolski, Pra- [Yuetal.,2022] ChaoYu,AkashVelu,EugeneVinitsky,Ji-
fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox- axuanGao,YuWang,AlexandreBayen,andYiWu. The
imal policy optimization algorithms. arXiv preprint surprisingeffectivenessofppoincooperativemulti-agent
arXiv:1707.06347,2017. games. Advances in Neural Information Processing Sys-
tems,35:24611–24624,2022.
[Sonetal.,2019] Kyunghwan Son, Daewoo Kim, Wan Ju
Kang,DavidEarlHostallero,andYungYi. Qtran: Learn- [Zhangetal.,2022] Feiye Zhang, Qingyu Yang, and Dou
ingtofactorizewithtransformationforcooperativemulti- An. A leader-following paradigm based deep reinforce-
agentreinforcementlearning. InInternationalconference mentlearningmethodformulti-agentcooperationgames.
onmachinelearning,pages5887–5896.PMLR,2019. NeuralNetworks,156:1–12,2022.
[Sunehagetal.,2017] PeterSunehag,GuyLever,Audrunas
Gruslys, WojciechMarianCzarnecki, ViniciusZambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z
Leibo, Karl Tuyls, et al. Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296,2017.
[Tampuuetal.,2017] Ardi Tampuu, Tambet Matiisen, Do-
rian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru,
Jaan Aru, and Raul Vicente. Multiagent cooperation and
competition with deep reinforcement learning. PloS one,
12(4):e0172395,2017.
[Vaswanietal.,2017] AshishVaswani,NoamShazeer,Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you
need. Advancesinneuralinformationprocessingsystems,
30,2017.
[Wangetal.,2020a] JianhaoWang,ZhizhouRen,TerryLiu,
Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agentq-learning. arXivpreprintarXiv:2008.01062,
2020.
[Wangetal.,2020b] Tonghan Wang, Tarun Gupta, Anuj
Mahajan, Bei Peng, Shimon Whiteson, and Chongjie
Zhang. Rode: Learning roles to decompose multi-agent
tasks. arXivpreprintarXiv:2010.01523,2020.
[Wangetal.,2023a] JiaoWang,MingruiYuan, YunLi,and
Zihui Zhao. Hierarchical attention master–slave for het-
erogeneous multi-agent reinforcement learning. Neural
Networks,162:359–368,2023.A PseudocodeofJointPPO Table3:Differenthyper-parametersusedforJointPPOintheexper-
iments.
Algorithm2JointPPO
PPO PPO policyloss lrdecay
maps
Input: Number of agents N, actions generation order epochs clip coefficient strategy
(i ,...,i ),batchsizeB,episodesK,stepsperepisodeT.
1 N
5m vs 6m 15 0.05 5 linear
Initialize: Encoder’s parameters ϕ, decoder’s parameters θ,
8m vs 9m 15 0.1 5 linear
replaybufferB.
10m vs 11m 15 0.1 5 linear
1: fork =0,1,...,K−1do
27m vs 30m 15 0.1 5 linear
2: Initializetheenvironmentandstartanepisode.
6h vs 8z 15 0.05 2 linear
3: fort=0,1,...,T −1do
MMM 15 0.2 5 linear
4: (//TheInteractionPhase)
5: Collectobservationso t =(cid:8) oi t(cid:9)n i=1. M3 Ms5 Mz 2 1 10 5 00 .. 02 5 5 5 expl oin ne ea nr tial
6: Input the collected observations to the joint pol-
3s5z vs 3s6z 10 0.1 2 linear
icy network. Get the predicted joint observation
value V (o ) and generated conditional local poli-
ϕ t
cies (cid:8) πi(cid:0) ai|o ,a1:i−1(cid:1)(cid:9) according to the or- C DetailsofExperimentalResults
t t t i=1:n
der.
In this section, we present details of the experiment results,
7: Sampleagents’actionsandexecutethejointaction
a =
(cid:0) a1,...,ai(cid:1)
totheenvironment. Receivethe
including the training curves of win rates and number of
t t t killedalliesacrossalltestmapsinFigure6andFigure7. We
teamrewardr andstimulatetheenvironmenttothe
t alsopresentthedetailedresultsofablationstudyontheinflu-
nextstate.
8:
Inserttuple(cid:0)
o t,V ϕ(o
t),(cid:8) πi(cid:9)
,a t,r
t(cid:1)
intoB.
e nn ac le wo infp rp ao tee ap no dc ah va en rd agc eli wpp inin rg ap tear fa om ree ate cr h. sW ete or fe pc ao rr ad mt eh te erfi s-
,
9: endfor
seeninTable4. Thefinalwinrateisthewinratedescribed
10: (//TheTrainingPhase)
abovewhichreflectstheoptimalityoftheconvergence,while
11: SamplearandomminibatchofBfromB.
theaveragewinrate,herewerefertothewinrateaveraged
12: CalculatethelossaccordingtoEq.(8)andupdatenet-
over the entire training process from scratch, which reflects
workparametersϕandθwithgradientdescent.
thelearningrateandthesampleefficiency. Bothkindsofwin
13: endfor
rateareaveragedover5randomseeds.
Output: AtrainedTransformer-basedjointpolicynetwork.
Table4:PPOEpochsandClippingParameterAblations
Clipping
B Hyper-parameterSettingsforExperiments 0.05 0.1 0.15 0.2
Epochs
5 90.0(46.8) 85.3(55.6) 88.1(64.6) 73.1(51.4)
During experiments, the implementations of MAT, MAPPO
10 93.4(53.6) 94.1(66.9) 92.2(66.2) 91.3(68.1)
and HAPPO are consistent with their official repositories. 15 96.9(62.7) 92.8(67.1) 90.6(61.6) 88.1(60.7)
Here we list the hyper-parameter adopted in the implemen-
tationofJointPPOfordifferenttasksinTable2andTable3,
especiallyintermsoftheppoepochs,ppoclip,learningrate
decay strategy, and the coefficient parameter λ of the PPO
1
loss,whichcorelatestoitsproportionintheoveralllearning
loss.
Table2:Commonhyper-parametersusedforJointPPOintheexper-
iments.
hyper-parameters value
learningrate 5e-4
batchsize 3200
discountfactor 0.99
entropycoef 0.01
hiddenlayernum 1
hiddenlayerdim 64
attentionblocknum 1
optimizer Adam
learningratedecay True
usevaluenormalization TrueFigure6:PerformancecomparisononSMACtasksintermsofwinrate.Figure7:PerformancecomparisononSMACtasksintermsofthenumberofkilledallies.