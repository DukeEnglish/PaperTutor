When LLMs are Unfit Use FastFit:
Fast and Effective Text Classification with Many Classes
AsafYehudai12 ElronBandel1
1IBMResearch,2HebrewUniversityofJerusalem
{first.last}@ibm.com
Abstract
Train Time 🏆
We present FastFit, a method, and a Python
~ 20 Min
package design to provide fast and accurate
few-shotclassification,especiallyforscenarios
withmanysemanticallysimilarclasses. Fast-
~2 Min
Fitutilizesanovelapproachintegratingbatch ~ 30 Sec
contrastivelearningandtoken-levelsimilarity
Throughput (Samples/Second)
score. Comparedtoexistingfew-shotlearning
>1K >1K >1K
packages,suchasSetFit,Transformers,orfew-
shotpromptingoflargelanguagemodelsvia 2.5 3
APIcalls,FastFitsignificantlyimprovesmulti- Accuracy
classclassificationperformanceinspeedand
88.6
accuracyacrossFewMany,ournewlycurated 84.8 86.9
80.1
Englishbenchmark,andMultilingualdatasets.
FastFitdemonstratesa3-20ximprovementin 59.6
trainingspeed,completingtraininginjustafew
seconds. TheFastFitpackageisnowavailable
Mistral Flan Classifier SetFit FastFit
onGitHubandPyPi,presentingauser-friendly
solutionforNLPpractitioners.
Code: https://github.com/IBM/fastfit Figure1: FastFitachievesSOTAclassificationresults
Data: https://huggingface.co/FastFit combinedwithfasttrainingandhighthroughput. Out-
preformingotherfine-tuningmethodsandstrongLLMs.
1 Introduction
Few-shotclassificationpresentsauniquechallenge, Incontrast,theapproachoffine-tuningsmaller
especiallywhendealingwithamultitudeofclasses language models capitalizes on their adaptability
thatsharesimilarsemanticmeanings. Expanding to specific tasks, as demonstrated to be effective
thetrainingdatacanbebothtime-consumingand in recent works. However, these methods can be
costly. Toaddressthischallenge,twoprimarycat- challengingtodeployastheyrequirearchitectural
egories of tools have been developed: few-shot adjustments(Yehudaietal.,2023)or, likeSetFit,
prompting of large language models (LLMs) via mayprovelesssuitableforclassificationwithmany
API calls, or packages designed for fine-tuning classes(Tunstalletal.,2022).
smaller language models using the limited avail- In this work, we present FastFit, a fast and ac-
abledata. However,werecognizethedrawbacks curatemethod,andapip-installablePythonpack-
ofapplyingbothapproachesinpractice. agedesignedforfine-tuningsmalllanguagemod-
Few-shot prompting of LLMs leverages their elsinfew-shotclassificationtasksinvolvingmany
multitaskingabilitiestotackledatascarcity. How- classes. Through various experiments, on our
ever, in the presence of many classes, LLMs en- newly curated FewMany benchmark, we demon-
counterthreemajorchallenges: (1)LLMsstruggle strate that FastFit training is significantly faster,
toincorporatedemonstrationsofallclasseswithin providinga3-20xspeedup. Thisenablestraining
their context window. (2) Utilization of the long withinseconds,asillustratedinFig. 1. FastFitout-
contextfortheclassificationtaskcanbechalleng- performsearlierpackages,includingSetFit,Trans-
ing (Liu et al., 2023). (3) Inference time is slow former,andmulti-taskmodelslikeFLAN,orlarger
duetomodelsize,andpromptlength. LLMslikeLLama-70B,inbothEnglishandMulti-
4202
rpA
81
]LC.sc[
1v56321.4042:viXralingualsettings. label_column_name and text_column_name pa-
The core contribution facilitating this speedup rametersoftheFastFitTrainer. Ourtraineralso
andimprovementliesinFastFit’suseofbatchcon- supports training with either CLS or token-level
trastivetraining,recognizedforitsefficiencyand similarity metrics, set by the sim_rep parameter.
effectiveness(Khoslaetal.,2021). Thistechnique Thetrainerallowstomodifythenumberofaugmen-
bringssame-classtextscloserwhilepushingapart tationrepetitionswiththenum_repeatsparameter.
allothertexts. FastFitalsoincorporatestoken-level Thenaftertraining,wecaneasilysavethemodel:
textsimilaritymeasuresthatleveragefine-grained
•••
information (Zhang et al., 2020; Khattab and Za-
haria, 2020). Additionally, we integrate text aug- model.save_pretrained("fast-fit")
mentationtechniquestoenhancetherobustnessof
thetrainingprocess(Gaoetal.,2021).
Andlaterloaditforinference,SeeApp. §A.
The FastFit package is easy to install and use,
interfacing with standard training APIs (See §2).
3 Method
WehopethatFastFitwillhelpmaketextclassifica-
tion easier and faster for the benefit of the whole Givenafew-shottextclassificationdatasetcontain-
community. ingtextsandtheircorrespondingclassesdenoted
as {x ,y }N , let C = {c }M represent all pos-
i i i=1 j j=1
2 TheFastFit Library sibleclasses. Ourtaskistoclassifyeachx i intoa
classy ∈ C. Toachievethisgoalweaimtoencode
i
The FastFit Python package is available on PyPI bothtextsandclassnamesintoasharedembedding
andcanbeinstalledwith: space,wheretheyarerepresentedclosely,accord-
ing to a similarity metric S, when they belong to
•••
the same class and are represented further apart
$ pip install fast-fit whentheydonot. Toaccomplishthis,weoptimize
thefollowingbatchcontrastiveloss:
To utilize FastFit, import the FastFit trainer,
whichinheritsfromtheHuggingFace(HF)trainer. (cid:88) −1 (cid:88) eS(xb,xp)/τ
L = log
ThisenablesFastFittobecustomizable,inheriting |P(b)| (cid:80) eS(xb,xa)/τ
b∈[B] p∈P(b) a∈[B]\b
all parameters from the HF trainer. FastFit sup- (1)
portsloadingdatasetseitherbydirectlypassingthe Here,{x }B representsabatchofB texts,and
b b=1
datasetorprovidingfilepaths. P(b)referstothesetoftextsinthesameclassas
Here is a simple code example of loading and b in the batch, given by P(b) = {c ∈ [B],|,y =
c
training FastFit. In App. §A, we provide a com- y }. The function S is the similarity metric, and
b
pletecodeexample. τ isascalartemperatureparameterregulatingthe
penaltyfornegativetexts.
•••
Foreachtextinthebatch,weaugmentthebatch
by including its class name as an additional ex-
from fastfit import FastFitTrainer
ample. Additionally, we repeat the texts in the
trainer = FastFitTrainer(
batch r times as a data augmentation technique,
model_name_or_path=
"roberta-large", followingGaoetal.(2021)bytreatingthedropout
text_column_name="text", as a minimal augmentation at the representation
label_column_name="label",
level. This method has demonstrated significant
dataset=dataset,
) success in generating sentence embeddings, and
we leverage it here to enhance representation for
model = trainer.train()
textclassification.
results = trainer.evaluate()
In our data-scarce setting, we employ fine-
grainedtoken-levelsimilaritymetrics,leveraging
As FastFit utilizes example texts and class textualdetails. Thisapproach,successfulinworks
names,itexpectsthedatatohavetextandlabel likeBERT-ScoreandColBERT,definesthesimi-
fieldsortomaptheexistingfieldstothemusingthe laritymetricbetweentextsx andx asthesumof
i jcosinesimilaritiesbetweenx andthemostsimilar versions, where small is MPNet (110M parame-
i
tokensinx . Specifically,withtokensdenotedas ters)(Songetal.,2020),andlargeisRoberta-large
j
x1,...,xn and x1,...,xm respectively, the simi- (355M parameters) (Liu et al., 2019b) or equiva-
i i j j
larityscoreiscomputedasfollows: lent.
StandardClassifier. Asimpleyetstrongbase-
line is a standard fine-tuning of an encoder-only
n
S(x ,x ) = (cid:88) mm axE (xk)·E (xl) (2) model. Since we assume no validation sets, we
i j θ i θ j
l=1 usebestpracticesasdescribedinpreviousworks,
k=1
and train for 40 epochs, with a learning rate of
where E (xk) is a dense representation of token 1e−5,andbatchsizeof16(Linetal.,2023). We
θ i
xk producedbyaparametricencodermodelwith recoveredrunsthatdidn’tconverge.
i
parametersθ. SetFit. SentenceTransformerFine-tuning(Set-
Duringinference,whenprovidedwithanewtext, Fit)(Tunstalletal.,2022)isatwo-stagemethodfor
x weclassifyittothemostsimilarclassy ∈ C trainingaSentenceTransformermodel(Reimers
u i
withrespecttoasimilaritymetricS. Thismethod and Gurevych, 2019), specifically designed for
draws inspiration from the way inference is con- few-shot classification tasks. In the first stage,
ducted in retrieval systems, eliminating the need the encoder undergoes fine-tuning using triplet
foraclassificationheadandaligningthetraining loss, and in the second stage, the classification
andinferenceobjectives. head is trained. For the small model we use
paraphrase-mpnet-base-v21, and for the large
4 FewManyBenchmark model, we used all-Roberta-Large-v12, both
trainedwithsentencetransformerobjectivebefore.
To rigorously evaluate the capabilities of models
Wetrainedthemodelwithalearningrateof1e−5,
in few-shot text classification with many classes,
a batch size of 16, for one epoch, based on the
we introduce the FewMany benchmark, a collec-
parametersdefinedinSetFit’spaper.
tion of eight diverse classification datasets, each
Flan. Flanlanguagemodelsarefine-tunedona
featuringatleast50classes. Thebenchmarkspans
diverse range of NLP tasks and datasets, making
severaldomains,includingintentdetection,topic
themadaptableforvariousNLPtasksinafew-shot
classification,questionclassification,andproduct
manner. Here, we experimented with Flan-XXL
classification. EachdomaininFewManypresents
(11B) and Flan-ul2 (20B) models. These models
a unique input type, such as short informal user
havea4Ktokenscontextwindow.
queries,arguments,claims,long-formWikipedia
Llama. Llama-2-chatisasetoflargelanguage
articles, questions, and product descriptions. By
modelsdevelopedforconversationalapplications
coveringawidespectrumofcases, FewManyen-
and has strong multi-task few-shot capabilities.
ablesacomprehensiveevaluationofmodelperfor-
Here, we experimented with a Llama model that
mance in distinguishing between many semanti-
supportsa4Ktokenscontextwindow.
callysimilarclasses,oftenwithsubtledistinctions.
Mistral. Mistral is a strong 7B open-source
Inthiswork,weconductexperimentsonFewMany
largelanguagemodel. Here,weusedtheinstruct-
under 5-shot and 10-shot scenarios, where the k-
tunedversion. Mistralsupportsan8Ktokenscon-
shotscenarioreferstoatrainingsetwithk exam-
textwindow.
ples per class. Further details and data statistics
canbefoundinAppendixB.
5.2 ExperimentalSetup
5 Experiments Training Setup. We fine-tune the FastFit model
with a learning rate of 1e−5, a batch size of 32,
5.1 Baselines and a maximum sequence length of 128 tokens,
WecompareFastFitwithafewclassificationmeth- for 40 epochs. We used AdamW optimizer, 16-
ods,includingfine-tuningmethods,likeStandard bitfloating-point(FP16)precision,andapplied4
and SetFit classifiers, and few-shot promoting of batchrepetitionsthatactasaugmentations.
LLMsincludingFlan-XXL(Weietal.,2022),Flan- ForallLLMs,wefitthemaximumpossiblenum-
ul2(Tayetal.,2023),llama-2-70b-chat(Touvron ber of examples into their context window. For
et al., 2023), and Mistral-7b (Jiang et al., 2023). 1ST-MPNet
Forallfine-tuningmethods,weusesmallandlarge 2ST-Roberta-LargeMethod Model C150 AP106 B77 AT71 DB70 HU64 CS55 T50 Average
5-shot
S 91.3 47.5 81.0 95.4 82.5 82.2 86.1 80.3 80.8
FastFit
L 93.4* 50.9* 85.2* 96.2 83.1* 84.6* 87.5 84.8* 83.2*
S 89.0 45.9 77.3 94.8 79.0 80.0 84.1 79.5 78.7
SetFit
L 90.4 48.2 81.7 95.6 80.1 81.9 87.8 83.9 81.2
S 86.3 30.4 68.2 95.1 70.5 73.9 82.6 63.4 71.3
Classifier
L 92.0 44.5 79.7 96.0 76.8 79.4 88.2 73.3 78.7
10-shot
S 93.5 54.5 86.4 95.9 87.8 85.8 88.5 84.1 84.6
FastFit
L 95.3* 57.5 88.8* 96.5 88.7* 87.9* 89.4 88.0* 86.5*
S 90.9 53.6 84.8 95.5 85.9 85.1 87.7 83.7 83.4
SetFit
L 88.4 53.6 86.4 95.7 85.8 85.4 88.8 86.4 83.8
S 91.5 46.9 80.2 95.5 82.1 83.1 86.5 78.0 80.5
Classifier
L 94.5 57.1 87.4 96.6 87.0 86.0 90.9 86.8 85.8
Table 1: Accuracy results of FastFit and baselines on 5/10-shot text classification. Results show that FastFit
outperformsSetFitandastandardclassifier. Moreover,FastFitsmalliscomparabletoSetFitlarge. Resultswith*
arestatisticallysignificantbyt-test(p<0.05)comparedtothelargestandardclassifier.
Model C150 B77 AT71 HU64 CS55 T50 Avg.
Flan-ul2 80.3 71.5 97.3 76.2 89.4 65.6 80.1
Flan-XXL 82.1 72.1 97.0 74.9 49.0 84.9 76.7
Llama-2-13B-chat 53.0 42.6 77.0 53.2 54.8 49.6 55.0
Llama-2-70B-chat 60.8 45.7 88.9 62.8 57.9 37.7 59.0
Mistral-7B 63.5 46.8 87.0 71.7 58.8 29.5 59.6
FastFit 93.4 85.2 96.2 84.6 87.5 84.8 88.6
SetFit 90.4 81.7 95.6 81.9 87.8 83.9 86.9
Classifier 92.0 79.7 96.0 79.4 88.2 73.3 84.8
Table2: AccuracyresultsofafewLLMsmodelson6testsetsfromFewMany. TheFlanmodelsoutperformthe
otherLLMsondatasetsfromFewMany. Llama-70BscoreshigherthanLlama-13BandiscomparabletoMistralon
thesedatasets. Forcomparison,wepresentthe5-shotresultsofthefine-tuningmethods.
AP106andDB70testsetsevena1-shotexample datasetsunder5/10-shotsettings. FastFitlargeout-
donotfitintothecontext. HencewecompareLLM performsSetFitby2%andthestandardclassifier
resultsontheremainingsixtestsets. by 4.5%, in the 5-shot case. In the 10-shot case,
EvaluationSetup. Few-shotevaluationscanbe FastFitoutperformsSetFit,andastandardclassifier
noisyduetovariationsinthesmalldatasets(Dodge by2.7%and0.7%,respectively. Moreover,FastFit
et al., 2020; Zhang et al., 2021). To address this small is comparable to SetFit large in 5-shot and
challenge, we perform all our experiments using outperformsitin10-shot. Notably,FastFitshows
5 random training split variations and report the greaterimprovementinthe5-shotcasecomparedto
meanresults. the10-shotcaseandforthesmallmodelcompared
tothelargeone.
5.3 Results
Table2displaystheresultsoffew-shotprompt-
In Table 1, we present the results of FastFit, Set- ing for several LLMs. The Flan models exhibit
Fit,andthestandardclassifierforFewManyeight higherperformancethanotherLLMs,likelyduetoMethod Size En De Ja Es Fr Zh Average
5-shot
S 72.3 65.0 68.7 65.9 68.0 68.4 68.1
FastFit
L 77.6* 70.5* 73.7* 71.7* 73.1* 73.7* 73.4*
SetFit S 67.9 62.2 66.8 64.0 65.0 66.7 65.4
S 61.2 56.8 59.7 58.4 59.8 61.4 59.5
Classifier
L 66.4 56.0 65.3 56.6 60.0 61.9 61.0
10-shot
S 77.6 70.5 73.7 71.7 73.1 73.7 73.4
FastFit
L 79.2* 74.8* 77.4 74.1* 75.7* 74.9* 76.0*
SetFit S 74.7 69.8 73.5 71.4 72.0 72.9 72.4
S 72.2 67.7 71.0 68.6 69.7 70.0 69.9
Classifier
L 77.5 71.2 74.3 71.3 72.5 72.7 73.3
Table3: AccuracyresultsforFastFitandbaselinesacrosssixlanguages,under5/10-shotsettings. Resultsshowthat
FastFitconsistentlyoutperformsSetFitandthestandardclassifier. Notably,FastFitsmallconsistentlysurpasses
SetFit’ssmallandstandardlargeclassifiers.Resultsmarkedwithanasterisk(*)arestatisticallysignificantaccording
tot-test(p<0.05)whencomparedtothelargestandardclassifier.
thepresenceofmanyclassificationdatasetsinthe 6.2 Baselines
Flan dataset3. This observation aligns with find-
Formultilingualtraining,weutilizedparaphrase-
ingsinzero-shotclassification(Gretzetal.,2023).
multilingual-mpnet-base-v2asasmallmodeland
Llama-70B outperforms Llama-13B, and is com-
XLM-Roberta-Largeasalargemodel. Bothmod-
parabletoMistral-7B’sperformance,possiblydue
els underwent pretraining in a large number of
to Mistral’s larger context length, allowing it to
languages. Notably, to the best of our knowl-
incorporatemoreexamplesperclass.
edge,thereisnomultilingualsentencetransformer
The results suggest that in our setting, where modelequivalenttoRoberta-LargeforSetFittrain-
numerous classes are present, even the best- ing. MonolingualandXLM-Roberta-Largemodels
performingLLMswetested(Flan’s)underperform were tested, but they yielded lower performance
comparedtolargestandardclassifiersandfacechal- than the small model; hence, their results are de-
lengescomparedtoFastFit. It’simportanttonote tailedinAppendix§D.InEnglishexperiments,we
that,duetothemodel’ssizeandthelengthofthe maintained the use of monolingual models (see
few-shotprompt,inferencetimecanbeslow,with §5.1),conductingtrainingandevaluationwiththe
throughput exceeding 1 second per input, in con- samesetupoutlinedin§5.2.
trasttoabout1millisecondwithFastFit.
6.3 Results
6 MultilingualExperiments In Table 3, we present the results on MASSIVE
in 5/10-shot scenarios using FastFit, SetFit, and
6.1 Datasets the standard classifier. FastFit consistently out-
performsbothSetFitandthestandardclassifierin
To evaluate FastFit’s multilingual classification both5-shotand10-shotsettings,acrosssmalland
abilitiesweadoptAmazonMultilingualMASSIVE largemodels. Inthe5-shotscenario,FastFitlarge
dataset (FitzGerald et al., 2022). From the 51 achievesan8%improvementoverSetFitsmalland
availablelanguages,weselectedsixtypologically a12.4%improvementoverthestandardclassifier.
diverse languages: English, Japanese, German, Meanwhile,FastFitsmallshowsa2.7%improve-
French, Spanish, and Chinese. MASSIVE is a ment over SetFit small and a 7.1% improvement
paralleldataset,with60classes(SeeApp. §B). over the standard classifier. In the 10-shot case,
FastFitlargeoutperformsSetFitsmallby3.6%and
the standard large classifier by 2.7%. Similarly,
3Tothebestofourknowledge,theFlandatasetincludes
onlyT50fromourtestsets FastFitsmallexhibitsimprovementsof1.9%andformances within a few seconds before reaching
aplateau. Notably,bothsmallandlargeSentence
Transformer(ST)modelsexhibithigherinitialper-
formanceandfasterconvergencethantheirnon-ST
basemodelcounterparts. WecanalsoseethatFast-
Fit achieves state-of-the-art results on FewMany,
above81.2,within30secondsasillustratedinFig.
1.
5 Shots
80
Figure2: Trainingtimes(sec)forFastFit,SetFit,and 70
standardclassifierwithMPNetmodel. FastFittraining 60
is3-20xfaster.
50
3.5%overSetFitsmallandthestandardclassifier, 40
respectively. Model
30
Large (ST)
It is noteworthy that FastFit demonstrates im-
Large
20
provement when scaling from a small to a large Small (ST)
model,withgainsof5.3%and2.6%inthe5-shot 10 Small
and 10-shot settings, respectively. This enhance- 0 5 10 15 20 25 30
Train Time (Second)
menthighlightsthefactthatFastFitisnotmodel-
specific and thus is highly flexible for different Figure 3: Average 5-shot Accuracy on the FewMany
sizesandtypesofmodels,unlikeSetFit. Suchflex- benchmarkofvariousFastFitmodelsovertrainingtime,
ibility is particularly crucial in few-shot settings measuredinseconds,trainedonanNvidiaA100-80GB
where limited examples are available, highlight- GPU.
ingthepotentialtotrainenhancedclassifiersusing
domain-orlanguage-specificmodels. Moreover,if 8 Ablation&FullTraining
unlabeledorpairwisedataisavailable,usingitfor
To further examine the contribution of some of
pretrainingcanleadtoevenfurtherimprovement.
our method modifications, we compare training
with CLS and token-level similarity metrics, as
7 FastTraining
well as training with a different number of batch
Training Times for FastFit, SetFit, and the stan- repetitions. Weconducttheseexperimentsonthree
dardclassifierareillustratedinFigure2. Results datasets: Hwu64,Banking77,andClinc150,with5
areaverageacrossalllanguagesinMASSIVE,and randomsplits,andaveragetheirresults. Weassess
all 5 seeds. FastFit exhibits faster training times theeffectofthesemodificationsforbothsmalland
comparedtobothSetFitandthestandardclassifier, largemodels,with5and10shots.
witha3-20xdecrease,andatrainingtimeofabout In Table 4, we present the differences in per-
30 seconds (See more results at App. §E). This formance caused by our changes; full results are
canbeattributedtoacombinationoftechnicaland available in App. §F. The Token-level similarity
methodologicalfactors. Theimprovedimplemen- metric proves beneficial across all settings, with
tationincludespre-trainingtokenizationandFP16 amorepronouncedeffectforsmallermodelsand
training. Furthermore,themethodologicaladvan- when less data is available (5-shot compared to
tage stems from using batch contrastive training, 10-shot). Concerning the number of repetitions,
whichleveragesin-batchexamplesasnegatives,in weobservethat,inmostcases,addingrepetitions
contrasttothetripletlossutilizedbySetFit. helps. Additionally,itappearsthatoverall,fourrep-
Convergence. Figure 3 presents the average etitionsaremoreeffectivethantwo. Regardingthe
FewManyaccuracy resultsper training second of relationshipbetweenthenumberofshotsandthe
FastFit for 5-shot with both small and large, and effectivenessofrepetition,noclearconnectionis
regularandSTbackbonemodels. Resultsdemon- apparent. Whileanincreaseinthenumberofshots
strateFastFitrapidconvergence,achievingtopper- enhanceseffectivenessinsmallmodels,theoppo-
ycaruccA
ynaM-weFModel Shot SimilarityLevel Repetitions Model EN DE JP ES FR CN Avg.
Classifier-B 88.3 85.7 83.9 86.9 86.3 84.9 86.0
Token 2 4
mT5-BT2T 87.9 86.2 83.5 86.7 86.9 85.2 86.1
FastFit-S 5 1.33 -0.28 0.09 mT5-BEnc 89.0 86.8 85.8 86.8 87.2 85.8 86.9
FastFit-S 88.8 87.4 87.0 87.9 87.6 86.7 87.6
FastFit-S 10 0.85 0.09 0.24
FastFit-L 89.5 88.5 88.5 87.4 88.5 86.7 88.2
FastFit-L 5 0.65 0.72 1.04
FastFit-L 10 0.36 0.55 0.78
Table6: FastFitandbaselinesaccuracyresultsonMAS-
SIVEwithfulldatatraining.
Table 4: FastFit ablation experiments; Accuracy dif-
ferencesintrainingwithtoken-levelversusCLSsim-
ilarity metrics and increasing augmentations repeti- 9 RelatedWork
tions. Token-levelenhancementsaremoreprominentin
smallermodels,especiallyinthe5-shotsetting. Forfine-tuningbaselines,wefocusonreadilyavail-
able methods. , including SetFit with its pack-
Model C150 B77 H64 Avg.
age, a standard classifier accessible through HF
Classifier-L 96.8 93.7 92.1 94.2 Transformers(Wolfetal.,2019),orLLMsthrough
FastFit-S 97.1 93.8 92.7 94.5 API calls. However, there are various few-shot
FastFit-L 97.5 94.2 93.0 94.9 classifiers,andwewillbrieflydiscussacoupleof
them. QAID(Yehudaietal.,2023)proposedpre-
Table5: FastFitaccuracyresultswhentrainingonthe andfine-tuningtrainingstageswithunsupervised
fulldata. andsupervisedloss,usingColBERTarchitecture,
achievingSOTAresults. T-Few(Liuetal.,2022),a
parameter-efficientfine-tuningmethodbasedonT0
siteisobservedforlargemodels,wheretheeffect
(Sanhetal.,2021),claimstobebetterandcheaper
decreases. Nevertheless,itseemsthat,ingeneral,
thanIn-ContextLearning.
largermodelsbenefitmorefrombatchrepetition.
Regarding few-shot prompting of LLMs ap-
Althoughourprimaryfocusisfew-shotclassifi-
proaches, a question arises about whether our re-
cation,wealsowantedtoexaminetheeffectiveness
sults will withstand stronger LLMs or improved
of FastFit when training on the full dataset. We
promptingtechniques. AccordingtoLoukasetal.
conducted two sets of experiments. In the first,
(2023) we can deduce that FastFit outperforms
we compared FastFit-small, FastFit-large, and a
GPT4(OpenAIetal.,2023)withafractionofthe
large standard classifier on Hwu64, Banking77,
cost. Additionally,Miliosetal.(2023)demonstrate
and Clinc150. In the second, we compared Fast-
that retrieval-based few-shot prompts can lead to
Fit-smallandFastFit-largewithafewbase-sized
improvedresults. However,it’sworthnotingthat
multilingualbaselinemodelsonMASSIVE,using
currently,thesemodelsremainslowandcostly.
thesetofsixlanguagesmentionedin§6.1. These
baselinesarebasedontheMASSIVEpaper,where
Classifier-BandmT5-BEncoderarestandardclas- 10 Conclusions
sifiersbasedonXLM-R-BASEandmT5-Basewith
270Mand258Mparameters,respectively. mT5-B In this paper, we introduce FastFit, a novel few-
T2Tisatext-2-textclassifierwith580Mparame- shottextclassificationmethodaccompaniedbya
ters. Pythonpackage. Forourtask,wecuratedtheFew-
ResultsinTable5demonstratethatwhentrain- Many benchmark. Our results demonstrate that
ingonallthedata,FastFit-Smalloutperformsthe FastFitoutperformslargelanguagemodels(LLMs)
largeClassifier,andFastFit-Largeperformseven suchasFlan-XXLandLlama-2-chat-70B,aswell
better. FromTable6,wecanseethatFastFit-Small as fine-tuning methods, including both standard
outperforms all other baselines even with fewer andSetFitclassifiers,readilyavailableinexisting
than half the number of parameters. Moreover, packages. Notably, FastFit exhibits fast training
FastFit-Large further improves performances by andinference. Weprovideevidencethatthesere-
0.6%onaverage. TheseresultsindicatethatFast- sultsholdforbothMultilingualandfull-datatrain-
Fitisnotonlyafastfew-shotclassifierbutcanalso ingsetups. WehopethatFastFit’sspeedandsim-
outperformevenlargerclassifierswhentrainingon plicitywillenhanceitsusability.
thefulldataset.References Maschinot,CeLiu,andDilipKrishnan.2021. Super-
visedcontrastivelearning.
RoyBar-Haim,IndrajitBhattacharya,FrancescoDin-
uzzo,AmritaSaha,andNoamSlonim.2017. Stance Stefan Larson, Anish Mahendran, Joseph J Peper,
classificationofcontext-dependentclaims. InPro- Christopher Clarke, Andrew Lee, Parker Hill,
ceedings of the 15th Conference of the European JonathanKKummerfeld, KevinLeach, MichaelA
Chapter of the Association for Computational Lin- Laurenzano, Lingjia Tang, et al. 2019. An evalua-
guistics: Volume 1, Long Papers, pages 251–261, tiondatasetforintentclassificationandout-of-scope
Valencia,Spain.AssociationforComputationalLin- prediction. arXivpreprintarXiv:1909.02027.
guistics.
Xin Li and Dan Roth. 2002. Learning question clas-
Iñigo Casanueva, Tadas Temcˇinas, Daniela Gerz, sifiers. In COLING 2002: The 19th International
MatthewHenderson,andIvanVulic´.2020. Efficient ConferenceonComputationalLinguistics.
intentdetectionwithdualsentenceencoders. arXiv
preprintarXiv:2003.04807. Yen-TingLin,AlexandrosPapangelis,SeokhwanKim,
SungjinLee,DevamanyuHazarika,MahdiNamaz-
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali ifar, Di Jin, Yang Liu, and Dilek Z. Hakkani-Tür.
Farhadi,HannanehHajishirzi,andNoahSmith.2020. 2023. Selective in-context data augmentation for
Fine-tuningpretrainedlanguagemodels: Weightini- intent detection using pointwise v-information. In
tializations,dataorders,andearlystopping. ConferenceoftheEuropeanChapteroftheAssocia-
tionforComputationalLinguistics.
Jack FitzGerald, Christopher Hench, Charith Peris,
ScottMackie,KayRottmann,AnaSanchez,Aaron HaokunLiu,DerekTam,MohammedMuqeeth,JayMo-
Nash,LiamUrbach,VisheshKakarala,RichaSingh, hta,TenghaoHuang,MohitBansal,andColinRaffel.
Swetha Ranganath, Laurie Crist, Misha Britan, 2022. Few-shot parameter-efficient fine-tuning is
Wouter Leeuwis, Gokhan Tur, and Prem Natara- betterandcheaperthanin-contextlearning. ArXiv,
jan. 2022. Massive: A 1m-example multilin- abs/2205.05638.
gualnaturallanguageunderstandingdatasetwith51
typologically-diverselanguages. NelsonF.Liu,KevinLin,JohnHewitt,AshwinParan-
jape,MicheleBevilacqua,FabioPetroni,andPercy
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Liang. 2023. Lost in the middle: How language
SimCSE:Simplecontrastivelearningofsentenceem- modelsuselongcontexts.
beddings. In Proceedings of the 2021 Conference
onEmpiricalMethodsinNaturalLanguageProcess- XingkunLiu,ArashEshghi,PawelSwietojanski,and
ing,pages6894–6910,OnlineandPuntaCana,Do- Verena Rieser. 2019a. Benchmarking natural lan-
minican Republic. Association for Computational guageunderstandingservicesforbuildingconversa-
Linguistics. tionalagents. arXivpreprintarXiv:1903.05566.
Shai Gretz, Roni Friedman, Edo Cohen-Karlik, As- YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
safToledo,DanLahav,RanitAharonov,andNoam dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Slonim. 2019. A large-scale dataset for argument Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
qualityranking: Constructionandanalysis. Roberta: A robustly optimized bert pretraining ap-
proach.
Shai Gretz, Alon Halfon, Ilya Shnayderman, Orith
Toledo-Ronen, Artem Spector, Lena Dankin, Yan- Lefteris Loukas, Ilias Stogiannidis, Odysseas Dia-
nis Katsis, Ofir Arviv, Yoav Katz, Noam Slonim, mantopoulos,ProdromosMalakasiotis,andStavros
andLiatEin-Dor.2023. Zero-shottopicaltextclas- Vassos. 2023. Making llms worth every penny:
sification with LLMs - an experimental study. In Resource-limitedtextclassificationinbanking. Pro-
FindingsoftheAssociationforComputationalLin- ceedings of the Fourth ACM International Confer-
guistics:EMNLP2023,pages9647–9676,Singapore. enceonAIinFinance.
AssociationforComputationalLinguistics.
AristidesMilios,SivaReddy,andDzmitryBahdanau.
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen- 2023. In-contextlearningfortextclassificationwith
sch,ChrisBamford,DevendraSinghChaplot,Diego manylabels. ArXiv,abs/2309.10954.
delasCasas,FlorianBressand,GiannaLengyel,Guil-
OpenAI,:,JoshAchiam,StevenAdler,SandhiniAgar-
laumeLample,LucileSaulnier,LélioRenardLavaud,
wal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAle-
Marie-AnneLachaux,PierreStock,TevenLeScao,
man,DiogoAlmeida,JankoAltenschmidt,SamAlt-
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
man,ShyamalAnadkat,RedAvila,IgorBabuschkin,
andWilliamElSayed.2023. Mistral7b.
SuchirBalaji,ValerieBalcom,PaulBaltescu,Haim-
OmarKhattabandMateiZaharia.2020. Colbert: Effi- ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,
cientandeffectivepassagesearchviacontextualized Jake Berdine, Gabriel Bernadett-Shapiro, Christo-
lateinteractionoverbert. pherBerner,LennyBogdonoff,OlegBoiko,Made-
laineBoyd,Anna-LuisaBrakman,GregBrockman,
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron TimBrooks,MilesBrundage,KevinButton,Trevor
Sarna, Yonglong Tian, Phillip Isola, Aaron Cai,RosieCampbell,AndrewCann,BrittanyCarey,Chelsea Carlson, Rory Carmichael, Brooke Chan, lipeCerónUribe,AndreaVallone,ArunVijayvergiya,
CheChang,FotisChantzis,DerekChen,SullyChen, ChelseaVoss,CarrollWainwright,JustinJayWang,
Ruby Chen, Jason Chen, Mark Chen, Ben Chess, AlvinWang,BenWang,JonathanWard,JasonWei,
ChesterCho,CaseyChu,HyungWonChung,Dave CJWeinmann,AkilaWelihinda,PeterWelinder,Ji-
Cummings, Jeremiah Currier, Yunxing Dai, Cory ayiWeng,LilianWeng,MattWiethoff,DaveWillner,
Decareaux,ThomasDegry,NoahDeutsch,Damien Clemens Winter, Samuel Wolrich, Hannah Wong,
Deville, Arka Dhar, David Dohan, Steve Dowl- Lauren Workman, Sherwin Wu, Jeff Wu, Michael
ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Wu,KaiXiao,TaoXu,SarahYoo,KevinYu,Qim-
Tyna Eloundou, David Farhi, Liam Fedus, Niko ingYuan,WojciechZaremba,RowanZellers,Chong
Felix, Simón Posada Fishman, Juston Forte, Is- Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
abella Fulford, Leo Gao, Elie Georges, Christian Zheng,JuntangZhuang,WilliamZhuk,andBarret
Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Zoph.2023. Gpt-4technicalreport.
Rapha Gontijo-Lopes, Jonathan Gordon, Morgan
Grafstein, ScottGray, RyanGreene, JoshuaGross, NilsReimersandIrynaGurevych.2019. Sentence-bert:
ShixiangShaneGu,YufeiGuo,ChrisHallacy,Jesse Sentenceembeddingsusingsiamesebert-networks.
Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-
hannesHeidecke,ChrisHesse,AlanHickey,Wade VictorSanh,AlbertWebson,ColinRaffel,StephenH.
Hickey,PeterHoeschele,BrandonHoughton,Kenny Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Hsu,ShengliHu,XinHu,JoostHuizinga,Shantanu Chaffin,ArnaudStiegler,TevenLeScao,ArunRaja,
Jain,ShawnJain,JoanneJang,AngelaJiang,Roger Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Jiang,HaozhunJin,DennyJin,ShinoJomoto,Billie Thakker,ShanyaSharmaSharma,ElizaSzczechla,
Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, TaewoonKim,GunjanChhablani,NihalV.Nayak,
Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Debajyoti Datta, Jonathan D. Chang, Mike Tian-
Keskar,TabarakKhan,LoganKilpatrick,JongWook JianJiang,HanWang,MatteoManica,ShengShen,
Kim, ChristinaKim, YongjikKim, HendrikKirch- Zheng-XinYong,HarshitPandey,RachelBawden,
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, ThomasWang,TrishalaNeeraj,JosRozen,Abheesht
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon- Sharma,AndreaSantilli,ThibaultFévry,JasonAlan
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal Fries,RyanTeehan,StellaBiderman,LeoGao,Tali
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Bers,ThomasWolf,andAlexanderM.Rush.2021.
Leike, Jade Leung, Daniel Levy, Chak Ming Li, Multitaskpromptedtrainingenableszero-shottask
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz generalization. ArXiv,abs/2110.08207.
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
AnnaMakanju,KimMalfacini,SamManning,Todor KaitaoSong, XuTan, TaoQin, JianfengLu, andTie-
Markov, Yaniv Markovski, Bianca Martin, Katie YanLiu.2020. Mpnet: Maskedandpermutedpre-
Mayer,AndrewMayne,BobMcGrew,ScottMayer trainingforlanguageunderstanding.
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier
Menick, Luke Metz, Andrey Mishchenko, Pamela Garcia, Jason Wei, Xuezhi Wang, Hyung Won
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Chung, Siamak Shakeri, Dara Bahri, Tal Schuster,
Mossing,TongMu,MiraMurati,OlegMurk,David HuaixiuStevenZheng,DennyZhou,NeilHoulsby,
Mély,AshvinNair,ReiichiroNakano,RajeevNayak, andDonaldMetzler.2023. Ul2: Unifyinglanguage
ArvindNeelakantan,RichardNgo,HyeonwooNoh, learningparadigms.
LongOuyang,CullenO’Keefe,JakubPachocki,Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat- Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
tistaParascandolo,JoelParish,EmyParparita,Alex bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Passos,MikhailPavlov,AndrewPeng,AdamPerel- Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
man,FilipedeAvilaBelbutePeres,MichaelPetrov, Bhosale,DanBikel,LukasBlecher,CristianCanton
Henrique Ponde de Oliveira Pinto, Michael, Poko- Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow- JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
ell, Alethea Power, Boris Power, Elizabeth Proehl, CynthiaGao,VedanujGoswami,NamanGoyal,An-
RaulPuri,AlecRadford,JackRae,AdityaRamesh, thonyHartshorn,SagharHosseini,RuiHou,Hakan
CameronRaymond,FrancisReal,KendraRimbach, Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry- IsabelKloumann,ArtemKorenev,PunitSinghKoura,
der,MarioSaltarelli,TedSanders,ShibaniSanturkar, Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
GirishSastry,HeatherSchmidt,DavidSchnurr,John anaLiskovich,YinghaiLu,YuningMao,XavierMar-
Schulman, Daniel Selsam, Kyla Sheppard, Toki tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
Shyam,SzymonSidor,EricSigler,MaddieSimens, stein,RashiRungta,KalyanSaladi,AlanSchelten,
JordanSitkin,KatarinaSlama,IanSohl,Benjamin Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Sokolowsky, Yang Song, Natalie Staudacher, Fe- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lipePetroskiSuch,NatalieSummers,IlyaSutskever, lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
JieTang,NikolasTezak,MadeleineThompson,Phil ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre- Melanie Kambadur, Sharan Narang, Aurelien Ro-
ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe- driguez,RobertStojnic,SergeyEdunov,andThomasScialom.2023. Llama2: Openfoundationandfine- A FullCodeExample
tunedchatmodels.
LewisTunstall,NilsReimers,UnsoEunSeoJo,Luke AnydatasetcanbeloadeddirectlyfromHugging-
Bates, Daniel Korat, Moshe Wasserblat, and Oren
faceHub,Forexample:
Pereg. 2022. Efficient few-shot learning without
prompts.
•••
JasonWei, MaartenBosma, VincentY.Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
from datasets import load_dataset
drew M. Dai, and Quoc V. Le. 2022. Finetuned
languagemodelsarezero-shotlearners.
dataset = load_dataset(
"FastFit/banking77"
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
)
Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RémiLouf,MorganFuntowicz,
andJamieBrew.2019. Huggingface’stransformers:
State-of-the-artnaturallanguageprocessing. ArXiv,
ThenFastFitlibrarycansampleitdowntothe5
abs/1910.03771.
or10-shotformat:
AsafYehudai,MatanVetzler,YosiMass,KorenLazar,
DoronCohen,andBoazCarmeli.2023. Qaid: Ques-
tionansweringinspiredfew-shotintentdetection. •••
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. from fastfit import sample_dataset
Weinberger,andYoavArtzi.2020. Bertscore: Evalu-
atingtextgenerationwithbert. dataset["train"] =sample_dataset(
dataset["train"],
TianyiZhang,FelixWu,ArzooKatiyar,KilianQWein- label_column="label",
num_samples_per_label=5
berger,andYoavArtzi.2021. Revisitingfew-sample
)
{bert} fine-tuning. In International Conference on
LearningRepresentations.
Then once the data is ready it can be serve as
inputtotheFast-Fittrainertogetherwithotherim-
portantinputs:
•••
from fastfit import FastFitTrainer
trainer = FastFitTrainer(
model_name_or_path=
"roberta-large",
label_column_name="label_text",
text_column_name="text",
dataset=dataset,
)
model = trainer.train()
results = trainer.evaluate()
Thenwecansavethemodel:
•••
model.save_pretrained("fast-fit")
Andcouldbeloadedforinferencewith:••• datasets are: Hwu64 (Liu et al., 2019a), Bank-
ing77(Casanuevaetal.,2020),andClinc150(Lar-
from fastfit import FastFit
son et al., 2019). Many classes are semantically
from transformers import (
AutoTokenizer, similar,makingtheclassificationtasksmuchharder.
pipeline Twodatasetsfocusontopicclassificationinthede-
)
bate context are: Argument Topic (Gretz et al.,
model = FastFit.from_pretrained( 2019)andClaimStance(Bar-Haimetal.,2017)
"fast-fit"
containing arguments associated with debatable
)
topics,wherethedebatabletopicisthegoldlabel.
tokenizer = \ DBpediadatasetresultsfromaprojectaimingto
AutoTokenizer.from_pretrained(
extractstructuredcontentfromWikipediaarticles.
"roberta-large"
) HereweusethedatasetwithL2(L1-L3)levelfor
textclassificationwith70classes. TheTrecdataset
classifier = pipeline(
(LiandRoth,2002)presentsthetaskofquestion
"text-classification",
model=model, classification, with 50 fine-grain classes describ-
tokenizer=tokenizer ingtheanswertype. AmazonProducts(AP)isa
)
classification data of product descriptions to 106
print(classifier("Hello World!")) product categories. We conduct our experiments
in5/10-shotscenarioswhereinthek-shotscenario
thetrainingsetconsistedofk examplesperclass.
Dataset #Train #Vaild #Test #Classes #Domains
5 Shots
Clinc150 15,000 3,000 4,500 150 10
80 BankingG77 8,622 1,540 3,080 77 1
Hwu64 8,954 1,076 1,076 64 21
ArgumentTopic 6,640 949 1,898 71 -
60 ClaimStance 1,675 - 480 55 -
DBpedia 240,942 36,003 60,794 70 -
AmazonProducts 22,036 2,459 6,148 106 -
40 Trec 5,452 - 500 50 -
MASSIVE 11,514 2,033 2,974 60 18
20
Table 7: Data statistics of the few-shot classification
datasets.
10 Shots
80
C FullFewManyResults
60
LLMs in-context examples. All LLMs, except
Mistral,haveacontextwindowof4Ktokens. For
Model
40
Large (ST) Clinc150,Banking77,andArgumentTopic,wefit
Large
1 example into their context; for Trec and Claim
20 Small (ST)
Small Stance,wefit2examples;andforHwu64,wefit3
examples. Mistral,withan8Kcontextwindow,al-
0 5 10 15 20 25 30
Train Time (Second) lowsfor2examplesforClinc150,3forBanking77
and Argument Topic, and 5 examples for all the
Figure4: Average5and10shotAccuracyontheFew-
remainingtestsets.
ManybenchmarkofvariousFastFitmodelsovertrain-
Table10presentsthecomprehensiveresultsof
ing time, measured in seconds, trained on an Nvidia
FewManyacrossits8diversetestsets,covering5-
A100-80GBGPU.
and10-shotsettings,withsmall(MPNet)andlarge
(RoBERTa) models, as well as regular and Sen-
B DataStatistics
tence Transformers (ST) backbone models. The
WeconstructandexperimentwithFewManybench- results demonstrate that FastFit consistently out-
mark,containing8datasetswithatleast50classes, performsSetFitandstandardclassifiersonaverage
See Table 7, for full data statistics. Three En- across all settings. In the 5-shot setting, FastFit
glish intent detection few-shot text classification achieves 2% and 4.5% higher scores than SetFit
ycaruccA
ynaM-weFandstandardclassifiers,respectively. Similarly,in thetrainingtimesforthedifferentmethods,models,
the 10-shot case, it surpasses them by 2.7% and anddatasets.
0.7%. Furthermore,weobservethatlargeandST
modelsconsistentlyoutperformtheirsmallandreg-
ularcounterparts.
Table11showstheperformancedifferencesbe-
tweenmodelswithandwithoutSTfor5-and10-
shots,usingbothsmallandlargemodels. There-
sultsareaveragedacrossallFewManytestsets. We
observethatthedifferenceisconsistentlymoresig-
nificantfor5-shotcomparedto10-shot,indicating
thatwhenfewerexamplesareavailable,theback-
bone model becomes more advantageous. More-
over, the difference is more pronounced for the
largemodelsinFastFitandSetFit,suggestingthat
largeSTmodelsenableevengreaterimprovement.
Finally,wenotethatthedifferencesaresmallerfor
FastFit comparedtoSetFit,implyingthatFastFit
is less reliant on ST backbone models than Set-
Fit. Thesefindingsareconsistentwithourresults
fromthemultilingualexperimentandhighlightthe
adaptabilityofFastFit.
D MultilingualResults
Figure5: Trainingtimes(sec)forFastFit,SetFit,and
In Table 9, we present the experimental results standardclassifier. FastFittrainingisfasterbothforthe
using various backbone models for SetFit. We smallmodel(top)andforthelargemodel(bottom).
evaluatedthreemodels: (1)Monolingualsentence-
transformer (ST) large, referred to as ST-L. (2) Model Small Large
RegularMultilingualRoBERTa-large,denotedas
5-shot 10-shot 5-shot 10-shot
XLM-R-L or simply L. (3) RoBERTa-Base Mul-
FastFit 00:01:16∗ 00:02:11∗ 00:02:44∗ 00:05:20∗
tilingual sentence-transformer model, labeled as SetFit 00:12:44 00:49:38 00:25:36 01:34:18
ST-XB. Classifier 00:02:20 00:04:31 00:05:44 00:11:04
The results indicate that ST-L encounters diffi-
Table8: TrainingTimesforFastFit,SetFit,andStan-
culties with all non-English datasets, resulting in
dardClassifier: FastFittimesareindicatedwitha∗to
overall inferior performance. XLM-R-L exhibits
denote that, as illustrated in Figure 3, they converge
lowerproficiencyinEnglishbutdemonstratesim-
afterapproximately30secondsandpromptlyreacha
proved results across all other languages. Lastly, plateauthereafter. Comprehensivetrainingtimesforall
ST-XB,withacomparablesizetothesmallmodels modelsanddatasetsarepresentedinTable14.
(125Mvs. 110M),achievedsimilar,albeitslightly
lower,results. ThesefindingsunderscoreSetFit’s
dependenceonSTpre-trainedmodelsandhighlight
itslimitationswhensuchamodelisunavailable,as
inthisexperiment.
E TrainingRunTimesResults
Here we present more training run time results
for FastFit, SetFit, and a standard classifier. Fig.
5 presents the run time for the small and large
settings. Tab. 8 shows the average training run
time results. Table 4 presents the convergence in
both5-and10-shotcases. Table14furthershowsF AblationResults
Here,wepresenttheresultsfortheablationsasso-
ciatedwithTable4. Thefirstablationisdesigned
tomeasuretheeffectofthesimilaritymetrics. Ta-
ble 12 shows the results of the experiments with
both CLS and token-level similarity metrics. In
Table 13, we present the results without augmen-
tationrepetitions(1),andwith2and4repetitions.
Both ablations support our claim that the token-
levelsimilaritymetricandanincreasednumberof
augmentationrepetitionshelp.
G ShortVideo
Clickhereforourshortpresentation.Method Model En De Ja Es Fr Zh Average
5-shot
S 72.3 65.0 68.7 65.9 68.0 68.4 68.1
FastFit
L 77.6* 70.5* 73.7* 71.7* 73.1* 73.7* 73.4*
SetFit S 67.9 62.2 66.8 64.0 65.0 66.7 65.4
ST-L 74.0 50.3 41.3 53.6 52.1 39.6 51.8
L 66.1 60.8 64.8 50.1 61.3 43.6 57.8
ST-XB 74.0 62.3 64.8 62.0 62.3 65.1 65.1
10-shot
S 77.6 70.5 73.7 71.7 73.1 73.7 73.4
FastFit
L 79.2* 74.8* 77.4 74.1* 75.7* 74.9* 76.0*
SetFit S 74.7 69.8 73.5 71.4 72.0 72.9 72.4
ST-L 78.3 61.4 53.4 64.0 63.2 48.3 61.4
L 74.5 69.1 72.5 69.7 70.7 59.2 69.3
ST-XB 78.3 68.7 72.9 70.1 70.5 72.3 72.1
Table9: AccuracyresultsforFastFitandbaselinesacrosssixlanguages,under5/10-shotsettings. Resultswithfew
SetFitversionsbutnoonesurpassesSetFitsmall. Weexperimentingherewithsentence-transformer(ST)large
monolingual,multilingualbase,andnon-STmultilinguallarge.Method Model C150 AP106 B77 AT71 D70 HU64 CS55 T50 Average
5-shot
S 89.4 46.7 80.2 95.7 81.3 81.3 85.8 73.9 79.3
S(ST) 91.3 47.5 81.0 95.4 82.5 82.2 86.1 80.3 80.8
FastFit
L 91.9 50.0 83.1 96.1 81.5 82.6 85.9 80.5 81.4
L(ST) 93.4 50.9 85.2 96.2 83.1 84.6 87.5 84.8 83.2
S 87.8 43.0 75.7 94.3 79.2 76.3 84.0 74.9 76.9
S(ST) 89.0 45.9 77.3 94.8 79.0 80.0 84.1 79.5 78.7
SetFit
L 84.5 45.9 79.2 94.7 80.1 79.5 84.5 78.3 78.3
L(ST) 90.4 48.2 81.7 95.6 80.1 81.9 87.8 83.9 81.2
S 75.9 25.7 58.1 94.3 67.6 61.4 73.5 54.2 63.8
S(ST) 86.3 30.4 68.2 95.1 70.5 73.9 82.6 63.4 71.3
Classifier
L 89.9 46.0 74.9 95.6 78.4 77.2 86.1 66.9 76.9
L(ST) 92.0 44.5 79.7 96.0 76.8 79.4 88.2 73.3 78.7
10-shot
S 93.3 53.9 85.9 96.3 86.6 86.0 87.9 82.9 84.1
S(ST) 93.5 54.5 86.4 95.9 87.8 85.8 88.5 84.1 84.6
FastFit
L 94.1 56.8 87.8 96.4 87.0 86.2 88.2 86.3 85.4
L(ST) 95.3 57.5 88.8 96.5 88.7 87.9 89.4 88.0 86.5
S 90.0 53.1 84.4 95.2 84.9 84.0 87.4 83.0 82.8
S(ST) 90.9 53.6 84.8 95.5 85.9 85.1 87.7 83.7 83.4
SetFit
L 78.5 52.5 84.4 94.3 85.0 83.2 86.1 84.6 81.1
L(ST) 88.4 53.6 86.4 95.7 85.8 85.4 88.8 86.4 83.8
S 88.1 43.6 75.6 95.3 80.1 75.9 84.1 68.0 76.3
S(ST) 91.5 46.9 80.2 95.5 82.1 83.1 86.5 78.0 80.5
Classifier
L 93.5 57.7 86.1 96.6 87.3 85.4 88.9 83.1 84.8
L(ST) 94.5 57.1 87.4 96.6 87.0 86.0 90.9 86.8 85.8
Table10: AccuracyresultsforFastFitandbaselinesacrossFewManybenchmarktasks,under5/10-shotsettings.
Smallmodel(S)isMPNetand(L)isRoBERTaLarge,SentenceTransformersmodelsaremarkedwith(ST).
Method 5-shot 10-shot
Base ST Diff Base ST Diff
SmallModel
FastFit 79.3 80.8 +1.5 84.1 84.6 +0.5
SetFit 76.9 78.7 +1.8 82.8 83.4 +0.6
Classifier 63.8 71.3 +7.5 76.3 80.5 +4.2
LargeModel
FastFit 81.4 83.2 +1.8 85.4 86.5 +1.1
SetFit 78.3 81.2 +2.9 81.1 83.8 +2.7
Classifier 76.9 78.7 +1.8 84.8 85.8 +1.0
Table11: AccuracyresultswithSentenceTransformers(ST)regularbackbonemodelforFastFitandbaselineson
FewMany,under5/10-shotandsmall/largemodel. Thedifference(Diff)columnrepresentstheimprovementdueto
theuseofSTbackbonemodel. ModelSisMPNetandLisRoBERTaLarge.Method Shots Sim. C150 B77 H64 Average
metric
5 CLS 88.9 78.6 78.5 82.0
5 TOK. 90.2 80.0 79.7 83.3
FastFit-small
10 CLS 92.4 84.7 83.8 86.9
10 TOK. 93.3 85.4 84.7 87.8
5 CLS 91.6 81.7 82.4 85.2
5 TOK. 92.3 82.9 82.4 85.9
FastFit-large
10 CLS 94.1 87.6 86.3 89.4
10 TOK. 94.8 88.0 86.4 89.7
Table12: AblationresultswithCLSandtoken-levelsimilaritymetrics. Theaverageresultsthatscoredthehighest
foreachmodelsizeandshotnumberarehighlightedinbold.
Method Size Shots Repet. C150 B77 H64 Average
1 90.3 80.3 79.1 83.2
FastFit S 5 2 89.8 79.8 79.2 82.9
4 90.2 80.0 79.7 83.3
1 93.3 85.3 84.1 87.6
FastFit S 10 2 93.2 85.3 84.5 87.6
4 93.3 85.4 84.7 87.8
1 91.6 82.0 81.0 84.8
FastFit L 5 2 92.0 82.4 82.3 85.6
4 92.3 82.9 82.4 85.9
1 94.2 87.3 85.2 88.9
FastFit L 10 2 94.6 87.7 86.1 89.5
4 94.8 88.0 86.4 89.7
Table13: Ablationresultswithvaryingrepetitionnumbers. Theboldedvaluesrepresentthehighest-scoringaverage
resultsforeachmodelsizeandshotnumber.Model Shots C150 AP106 B77 AT71 D70 CS55 HU64 T50 Average
FastFitSmall(ST)(ML) 10 00:03:41 00:03:55 00:02:59 00:02:13 00:03:20 00:01:35 00:01:29 00:01:15 00:02:34
FastFitSmall(ST)(ML) 5 00:01:49 00:02:22 00:01:29 00:01:08 00:01:40 00:00:49 00:00:44 00:00:39 00:01:20
FastFitSmall(ST) 10 00:03:26 00:03:50 00:02:55 00:02:07 00:03:26 00:01:23 00:01:25 00:01:10 00:02:28
FastFitSmall(ST) 5 00:01:43 00:02:11 00:01:27 00:01:04 00:01:43 00:00:43 00:00:42 00:00:35 00:01:16
FastFitSmall 10 00:03:28 00:03:49 00:02:56 00:02:05 00:03:26 00:01:24 00:01:23 00:01:09 00:02:28
FastFitSmall 5 00:01:42 00:02:12 00:01:27 00:01:04 00:01:42 00:00:43 00:00:41 00:00:35 00:01:16
FastFitLarge(ST) 10 00:07:30 00:08:36 00:06:18 00:04:18 00:07:40 00:03:06 00:02:50 00:02:24 00:05:20
FastFitLarge(ST) 5 00:03:36 00:04:58 00:03:07 00:02:12 00:03:50 00:01:35 00:01:24 00:01:13 00:02:44
FastFitLarge(ML) 10 00:07:53 00:09:06 00:06:39 00:04:51 00:07:48 00:03:29 00:03:08 00:02:46 00:05:42
FastFitLarge(ML) 5 00:03:49 00:05:26 00:03:18 00:02:27 00:03:54 00:01:47 00:01:31 00:01:23 00:02:57
FastFitLarge 10 00:07:26 00:08:39 00:06:17 00:04:20 00:07:39 00:03:04 00:02:48 00:02:25 00:05:20
FastFitLarge 5 00:03:35 00:04:58 00:03:07 00:02:11 00:03:50 00:01:34 00:01:23 00:01:14 00:02:44
Model Shots C150 AP106 B77 AT71 D70 CS55 HU64 T50 Average
SetFitSmall(ST)(ML) 10 02:28:32 00:42:54 00:40:07 00:34:20 00:40:28 00:17:50 00:26:42 00:14:44 00:45:42
SetFitSmall(ST)(ML) 5 00:36:46 00:15:04 00:10:08 00:08:30 00:10:02 00:04:49 00:06:43 00:04:03 00:12:01
SetFitSmall(ST) 10 02:14:22 00:38:36 00:34:47 00:31:11 01:45:01 00:15:10 00:24:44 00:13:10 00:49:38
SetFitSmall(ST) 5 00:32:48 00:13:39 00:08:48 00:07:51 00:25:12 00:04:13 00:05:51 00:03:34 00:12:44
SetFitSmall 10 02:10:12 00:37:33 00:35:45 00:30:50 01:42:14 00:15:45 00:23:20 00:12:56 00:48:34
SetFitSmall 5 00:32:46 00:13:09 00:08:56 00:07:47 00:25:24 00:04:11 00:05:43 00:03:29 00:12:41
SetFitLarge(ST) 10 04:17:39 01:25:54 01:13:47 01:02:28 02:46:13 00:33:44 00:47:50 00:26:48 01:34:18
SetFitLarge(ST) 5 01:10:34 00:30:01 00:18:27 00:15:39 00:41:24 00:08:51 00:12:19 00:07:37 00:25:36
SetFitLarge(ML) 10 04:50:47 01:36:26 01:24:32 01:11:00 04:51:33 00:36:50 00:52:49 00:30:11 01:59:16
SetFitLarge(ML) 5 01:12:07 00:33:54 00:20:46 00:17:41 01:09:16 00:09:59 00:13:08 00:07:59 00:30:36
SetFitLarge 10 04:18:11 01:25:50 01:13:44 01:02:42 04:25:02 00:32:15 00:47:25 00:26:36 01:46:28
SetFitLarge 5 01:04:12 00:30:21 00:18:42 00:15:45 01:05:24 00:08:42 00:11:50 00:07:18 00:27:47
Model Shots C150 AP106 B77 AT71 D70 CS55 HU64 T50 Average
ClassifierSmall(ST)(ML) 10 00:08:39 00:04:20 00:04:28 00:04:06 00:04:03 00:03:01 00:03:41 00:02:45 00:04:23
ClassifierSmall(ST)(ML) 5 00:04:19 00:02:34 00:02:14 00:02:03 00:02:01 00:01:34 00:01:50 00:01:26 00:02:15
ClassifierSmall(ST) 10 00:08:54 00:04:28 00:04:36 00:04:14 00:04:09 00:03:06 00:03:48 00:02:51 00:04:31
ClassifierSmall(ST) 5 00:04:27 00:02:39 00:02:18 00:02:07 00:02:05 00:01:37 00:01:54 00:01:30 00:02:20
ClassifierSmall 10 00:08:56 00:04:27 00:04:34 00:04:13 00:04:09 00:03:05 00:03:47 00:02:51 00:04:30
ClassifierSmall 5 00:04:27 00:02:38 00:02:18 00:02:07 00:02:05 00:01:37 00:01:54 00:01:29 00:02:19
ClassifierLarge(ST) 10 00:21:52 00:11:00 00:11:14 00:10:20 00:10:14 00:07:35 00:09:19 00:06:59 00:11:04
ClassifierLarge(ST) 5 00:10:58 00:06:33 00:05:40 00:05:12 00:05:08 00:03:57 00:04:41 00:03:39 00:05:44
ClassifierLarge(ML) 10 00:22:44 00:11:23 00:11:43 00:10:44 00:10:35 00:07:54 00:09:42 00:07:16 00:11:30
ClassifierLarge(ML) 5 00:11:23 00:06:46 00:05:54 00:05:26 00:05:19 00:04:07 00:04:51 00:03:47 00:05:57
ClassifierLarge 10 00:21:52 00:11:00 00:11:17 00:10:21 00:10:15 00:07:38 00:09:20 00:06:59 00:11:05
ClassifierLarge 5 00:10:59 00:06:33 00:05:41 00:05:13 00:05:09 00:03:58 00:04:42 00:03:40 00:05:44
Table14: AverageTrainingtimeover10seedsforthedifferentmethods.