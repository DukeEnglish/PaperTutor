PublishedasaconferencepaperatICLR2024
SOHES: SELF-SUPERVISED OPEN-WORLD
HIERARCHICAL ENTITY SEGMENTATION
ShengcaoCao1∗ JiuxiangGu2 JasonKuen2 HaoTan2 RuiyiZhang2
HandongZhao2 AniNenkova2 Liang-YanGui1 TongSun2 Yu-XiongWang1
1UniversityofIllinoisUrbana-Champaign 2AdobeResearch
{cao44,lgui,yxw}@illinois.edu
{jigu,kuen,hatan,ruizhang,hazhao,nenkova,tsun}@adobe.com
SAM
60.8 (supervised SOTA)
-21%
46.1 45.9 45.8
-28% -37% -18%
33.3 33.5 31.1
26.0 29.1 26.2 27.8 18.1
22.5
17.1
-85%
11.4
SA-1B LVIS EntitySeg ADE20K PACO-LVIS
Figure1: SOHESboostsopen-worldentitysegmentationwithself-supervisiononvariousim-
age datasets. Compared to prior state of the art, SOHES significantly reduces the gap between
self-supervisedmethodsandthesupervisedSegmentAnythingModel(SAM)(Kirillovetal.,2023),
yetusingonly2%unlabeledimagedataasSAM.
ABSTRACT
Open-world entity segmentation, as an emerging computer vision task, aims at
segmenting entities in images without being restricted by pre-defined classes,
offering impressive generalization capabilities on unseen images and concepts.
Despite its promise, existing entity segmentation methods like Segment Any-
thingModel(SAM)relyheavilyoncostlyexpertannotators. Thisworkpresents
Self-supervisedOpen-worldHierarchicalEntitySegmentation(SOHES),anovel
approach that eliminates the need for human annotations. SOHES operates in
three phases: self-exploration, self-instruction, and self-correction. Given a pre-
trainedself-supervisedrepresentation,weproduceabundanthigh-qualitypseudo-
labelsthroughvisualfeatureclustering. Then,wetrainasegmentationmodelon
the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student
mutual-learning procedure. Beyond segmenting entities, SOHES also captures
their constituent parts, providing a hierarchical understanding of visual entities.
Using raw images as the sole training data, our method achieves unprecedented
performance in self-supervised open-world segmentation, marking a significant
milestonetowardshigh-qualityopen-worldentitysegmentationintheabsenceof
human-annotatedmasks. Projectpage: https://SOHES.github.io.
1 INTRODUCTION
Open-worldentitysegmentation(Qietal.,2022;2023)isanemergingvisiontaskforlocalizingse-
manticallycoherentvisualentitieswithouttheconstraintsofpre-definedclasses. Thistask,incon-
trasttotraditionalsegmentation(Longetal.,2015;Chenetal.,2017;Heetal.,2017;Kirillovetal.,
2019),aimsatcreatingsegmentationmasksforvisualentitiesinclusiveofboth“things”(countable
∗WorkdoneduringaninternshipatAdobeResearch.
1
4202
rpA
81
]VC.sc[
1v68321.4042:viXra
RA
ksaM
DOSSAH
)ATOS
.verP(
SEHOS )sruO(PublishedasaconferencepaperatICLR2024
objects such as persons and cars) and “stuff” (amorphous regions such as sea and sky) (Kirillov
et al., 2019; Qi et al., 2022), without regard for class labels. The inherent inclusivity and class-
agnostic nature enable open-world entity segmentation to perform strongly on unfamiliar entities
fromunseenimagedomains,afrequentreal-worldchallengeinapplicationssuchasimageediting
androbotics. AprominentmodelforthistaskisSegmentAnythingModel(SAM)(Kirillovetal.,
2023),whichhasgarneredenthusiasticattentionforitsimpressiveperformanceinopen-worldseg-
mentation. However, the efficacy of models like SAM depends on the avilability of extensively
annotated datasets. To illustrate, SAM is trained on SA-1B (Kirillov et al., 2023), a vast dataset
comprising11millionimagesandanenormousamountof1billionsegmentationmasks. Whileau-
tomatedsegmentationplaysacentralroleinbuildingSA-1B,humanexpertiseandmanuallaborare
similarlyimportant,whereittakes14to34secondstoannotateamask.Meanwhile,itischallenging
forhumanannotatorstoproducesegmentationmasksataconsistentgranularity,becausethereisno
universally agreed definition of objects and parts. This reliance on intricately annotated datasets
andconsiderablehumaneffortraisesacompellingquestion: Canwedevelopahigh-qualityopen-
world segmentation model using pure self-supervision? The prospect of learning from unlabeled
rawimageswithouttheneedforexpertannotationsishighlyappealing.
In fact, self-supervised visual representation learning (Chen et al., 2020; He et al., 2020; Caron
etal.,2021;Heetal.,2022b)hasalreadyshownpromise.Suchmodelscaneffectivelyexploituseful
trainingsignalsfrompurelyunlabeledimages, resultinginhigh-qualityvisualrepresentationsthat
arecomparablewiththoseachievedviasupervisedlearning. However,mainstreamself-supervised
representationlearningapproachestypicallylearnholisticrepresentationsforwholeimages,without
distinguishingindividualentitiesnorunderstandingregion-levelstructures. Asaresult,theycannot
bedirectlyusedtoachieveopen-worldentitysegmentation.Ourkeyinsighttobridgethisgapisthat
anintelligentmodelcannotonlylearnrepresentationsfromobservations,butcanalsoself-evolveto
exploretheopenworld,instructandgeneralizeitself,continuouslyrefineandcorrectitspredictions
inaself-supervisedmanner,andultimatelyachieveopen-worldsegmentation.
Followingthiskeyinsight,weproposeSelf-supervisedOpen-worldHierarchicalEntitySegmenta-
tion (SOHES), a novel approach consisting of three phases – 1) Self-exploration: Starting from
apre-trainedself-supervisedrepresentationDINO(Caronetal.,2021),wegenerateinitialpseudo-
labelstolearnfrom. Byclusteringvisualfeaturesbasedonsimilarityandlocality, wecandiscern
semanticallycoherentcontinuousregionsthatlikelyrepresentvisuallymeaningfulentities. 2)Self-
instruction: Ourinitialpseudo-labelsareconstrainedbythefixedvisualrepresentation. Torefine
the segmentation, we train a Mask2Former (Cheng et al., 2022) segmentation model on the ini-
tialpseudo-labels. Eventhoughtheinitialpseudo-labelsarenoisy, learningasegmentationmodel
on them can “average out” the noises, thus predicting more accurate masks. 3) Self-correction:
Buildinguponthesemoreaccuratepredictions,weemployateacher-studentmutual-learningframe-
work(Tarvainen&Valpola,2017;Liuetal.,2020)tofurtherreducetheearly-stagenoisesandadapt
themodelforopen-worldsegmentation. Throughoutthethreephases,werelysolelyontherawim-
ages, without any human annotations. Equally significantly, due to the compositional nature of
things and stuff in natural scenes, our model learns not just to segment entities but also their con-
stituent parts and finer subparts of these parts. During the self-exploration phase, we generate a
hierarchical structure of each visual entity from individual parts to the whole. This hierarchical
segmentation approach enriches our understanding of visual elements in an open-world context,
ensuringamorecomprehensiveandversatileapplication.
Tosummarize,ourkeycontributionsinclude:
• WeproposeSelf-supervisedOpen-worldHierarchicalEntitySegmentation(SOHES)toaddress
theopen-worldsegmentationchallenge. Wedemonstratethepotentialofhigh-qualityopen-world
segmentationbyadaptingself-supervisedrepresentationsandlearningsolelyfromunlabeleddata.
• Wedevelopamethodtogenerateover100segmentationmasksperimageashigh-qualitypseudo-
labelsbyclusteringself-supervisedvisualfeatures.
• Welearntosegmententitiesandtheirconstituentpartsandperformhierarchicalassociationbe-
tweenvisualentities. Thishierarchicalsegmentationapproachprovidesamulti-granularityanal-
ysisofvisualentitiesincomplexscenes.
• Weachievenewstate-of-the-artperformanceinself-supervisedopen-worldsegmentation,which
enhancesmaskaveragerecall(AR)onvariousdatasets(e.g., improvingARonSA-1B(Kirillov
etal.,2023)from26.0to33.3)andclosestheperformancegapbetweenself-supervisedandsu-
pervisedparadigms,asillustratedinFigure1.
2PublishedasaconferencepaperatICLR2024
2 RELATED WORK
Open-worldvisualrecognition. Open-worldrecognition(Scheireretal.,2012;Bendale&Boult,
2015; 2016) aims to recognize and classify visual concepts in an evolving environment where the
model encounters unfamiliar objects, which challenges traditional models trained to recognize a
fixedsetofclasses. Thetaskhasbeenextendedfromclassificationtodetection(Bansaletal.,2018;
Dhamijaetal.,2020;Josephetal.,2021;Jaiswaletal.,2021;Kimetal.,2022),segmentation(Hu
et al., 2018; Wang et al., 2021; 2022a; Kalluri et al., 2023), and tracking (Liu et al., 2022). In
particular,open-worldentitysegmentation(Qietal.,2022;2023)segmentsentitiesintosemantically
meaningful regions without regard for class labels. In this work, we further expand the scope to
wholeentitiesandtheirconstituentparts.
Self-supervisedobjectlocalization/discovery. Localizingobjectsfromimagesinaself-supervised
manner requires learning the concept of objects from visual data without any human annotations.
Early explorations (Vo et al., 2019; 2020; 2021) formulate an optimization problem on a graph,
wherethenodesareobjectproposals(e.g.,byselectivesearch(Uijlingsetal.,2013))andtheedges
areconstructedbasedonvisualsimilarities. Followingtheobservationthatthesegmentationofthe
most prominent object can emerge from DINO (Caron et al., 2021), Sime´oni et al. (2021; 2023);
Wang et al. (2022b) learn object detectors from saliency-based pseudo-labels. Meanwhile, Wang
et al. (2022c; 2023) generate pseudo-labels by extending NormCut (Shi & Malik, 2000), and Cao
et al. (2023) cluster semantically coherent regions into pseudo-labels. We share a common multi-
phase learning paradigm with these prior methods, where pseudo-labels are first discovered from
self-supervisedrepresentations, andthenadetection/segmentationmodelislearned. However, we
contributenovelandimproveddesignstoeachphase,including1)aglobal-to-localclusteringalgo-
rithmforhigh-qualitypseudo-labeling,2)ahierarchicalrelationlearningmodule,and3)ateacher-
studentself-correctionphase.
3 APPROACH
Inthissection,wefirstprovideanoverviewofSelf-supervisedOpen-worldHierarchicalEntitySeg-
mentation(SOHES)andthenpresentitsthreelearningphasesinthefollowingsubsections. Build-
ing upon and significantly enhancing the pseudo-label discovery and learning paradigm in prior
self-supervised object discovery work (Sime´oni et al., 2023; Wang et al., 2023; Cao et al., 2023),
SOHESconsistsofthreephases: self-exploration,self-instruction,andself-correction,asshownin
Figure2. 1)InPhase1self-exploration,westartfromapre-trainedself-supervisedrepresentation
DINO (Caron et al., 2021) with a ViT-B/8 (Dosovitskiy et al., 2020) architecture, and initiate our
exploration on unlabeled raw images. Our strategy is based on agglomerative clustering (Hastie
et al., 2009), and organizes image patches into semantically consistent regions automatically. 2)
Withthesepseudo-labels,webeginPhase2self-instruction. Wetrainasegmentationmodelcom-
posed by a DINO pre-trained ViT backbone (Caron et al., 2021), ViT-Adapter (Chen et al., 2022)
(for generating multi-scale features from ViT), and Mask2Former (Cheng et al., 2022) (for the fi-
nal mask prediction). Through self-instruction, our segmentation model can learn from common
visual entities in different images and generalize better than the initial pseudo-labels produced by
thefrozenViTbackbone. 3)InthefinalPhase3self-correction,weexploitmoreself-supervision
Phase 1: Self-exploration Phase 2: Self-instruction Phase 3: Self-correction
Generate initial pseudo-labels Learn from initial pseudo-labels Improve over initial pseudo-labels
Unlabeled Teacher
Raw Images Segmentation Model Segmentation Model Predict
Initial Supervise Teacher's
Pseudo-labels DINO ViT Mask2 Initialize EMA Pseudo-labels
Frozen Cluster ViT-Adapter Former Student
DINO ViT Segmentation Model
Supervise Supervise
(Initial Branch) (Teacher Branch)
Figure 2: Three phases of SOHES. In the first self-exploration phase, we cluster visual features
from pre-trained DINO to generate initial pseudo-labels on unlabeled images. Then in the self-
instruction phase, a segmentation model learns from the initial pseudo-labels. Finally, in the self-
correctionphase,weadoptateacher-studentframeworktofurtherrefinethesegmentationmodel.
3PublishedasaconferencepaperatICLR2024
Step 2: Local Step 3: Mask
Step 1: Global Clustering Step 4: Hierarchy Analysis
Re-clustering Refinement
Zoom in
Merge
Mix Refine Split
No valid
Extract
Merge
in mte ar sn kal
Features
Merge
Figure3: Self-explorationphaseforgeneratinginitialpseudo-labels. Thisphaseconsistsoffour
steps. Wefirstmergeimagepatchesintoregionswithhighvisualfeaturesimilarities,thenzoomin
onthesmallcandidateregionsandre-clusterthelocalimagestobetterdiscoversmallentities. After
that,werefinethemaskdetailsandidentifythehierarchicalstructureamongthemasks.
signals to liftthe limit induced by noisesin the initial pseudo-labels. Inspired by semi-supervised
learning(Tarvainen&Valpola,2017;Liuetal.,2020),weemployateacher-studentmutual-learning
framework,allowingthestudenttolearnfromtheimprovedpseudo-labelsgeneratedbytheteacher.
3.1 SELF-EXPLORATION: GENERATEINITIALPSEUDO-LABELS
Intheself-explorationphase,wegenerateinitialpseudo-labelswithseveralstepsdelicatelydesigned
to include potential entities and their constituent parts of diverse categories. We take a global-
to-local perspective to first create candidate regions at the global level, and then investigate local
imagestoaccuratelydiscoversmallentities. Inparticular, webeginbyclusteringpatch-levelself-
supervisedfeaturestogenerateapoolofcandidateregions,thenfilterandrefinesuchcandidatesinto
initial pseudo-labeled masks, and finally analyze the hierarchical structure among them. Figure 3
depictsthisprocesswithvisualexamples.
Step1isaglobalclusteringprocedure,whichmergesimagepatchesintosemanticallymeaningful
regions. Given an unlabeled image with resolution S × S, we use DINO ViT-B/8 to extract its
visualfeatures{f ,...,f }correspondingtoeach8×8patch. Then,wemergethesepatches
1 S×S
8 8
inabottom-up, iterativemanner. Theinitialseedregionsareexactlythese8×8patches. Ineach
iteration, we find the pair of adjacent regions (i,j) with the highest cosine feature similarity (f ·
i
f )/(∥f ∥ ·∥f ∥ ). Thesetworegionsiandj aremergedintoanewregionk. Thevisualfeature
j i 2 j 2
ofthemergedregioniscomputedasf = aifi+ajfj,wherea ,a aretheareasoftheregionsi,j.
k ai+aj i j
Afterreplacingregionsiandj withthenewmergedregionk,wecontinuewiththenextiteration.
Wesetaseriesofmergingthresholdsθ > ··· > θ ascriterionforstoppingthemerg-
merge,1 merge,m
ingprocedure. Ingeneral, thehighestcosinefeaturesimilarity(amongallunmergedregionpairs)
decreasesasmoreregionsaremerged. Whenthehighestcosinefeaturesimilaritygoesbelowone
threshold θ (t ∈ {1,...,m}), we record the merging results that have been obtained so far.
merge,t
Consequently,wecangeneratemsetsofregions,coveringvariousgranularitylevels. Wemixthese
sets into a pool of regions that may overlap with each other. Non-maximal suppression (NMS) is
appliedtoremoveduplicateregions. Thethresholds{θ }m canbedeterminedbasedonthe
merge,t t=1
desirednumberofpseudo-labelsperimage(seeAppendixD).
Step 2 is local re-clustering. In the first step, we have generated a large pool of image regions
that may correspond to valid visual entities. However, many small regions tend to be noisy and
lackmeaningfulcontent. Weadoptaglobal-to-localperspectivetore-examinetheregionsthatare
smallerthanθ %ofthetotalimagearea. Foreachsmallcandidateregion,wecropalocalimage
small
around it, resize it to S′ × S′, and re-cluster it with the same procedure as in Step 1 to obtain
subregionsofthelocalcrop. Subregionsthatintersectwiththeboundariesofthecroparediscarded,
because they are incomplete within the local crop context. The remaining subregions, along with
regions larger than θ % of the whole image (from Step 1), form our initial pseudo-labels. By
small
“zoomingin”onthesmallcandidateregionsandrepeatingtheclusteringprocedureatafinerscale,
wecanbetterremovenoisypseudo-labelsandimprovethequalityoftheremainingones.
4PublishedasaconferencepaperatICLR2024
Whole Mask 5 𝑄 ×𝑊"
Query 1 0 0 1 1 0 0
Query 2 0 0 0 0 0 1
Part Mask 2 Mask 1 Query 3 ×𝑊! 0 0 0 0 0 0
Query 4 0 0 0 0 0 0
Query 5 1 1 1 1 0 1
Query 6 0 0 0 0 0 0
Subpart Mask 6 Mask 3 Mask 4
Figure 4: Ancestor relation prediction in the self-instruction phase. The prediction target, a
binary matrix of ancestor relations, is constructed from the hierarchical structure identified in the
self-explorationphase. TheancestorpredictionheadusestwolinearmappingsW ,W totransform
1 2
thequeryfeaturesQandlearnstopredictthetargetancestors.
InStep3,weleveragetheoff-the-shelfmaskrefinementmodelCascadePSP(Chengetal.,2020)to
furtherrefinetheboundariesofthepseduo-labelmasks. WecomputethemaskIoUs(intersection-
over-union)betweenthepseudo-labelsbeforeandafterundergoingtherefinementstep,andremove
theonesthathavepoorIoUsbecausetheyarelikelynoisysamples.
Finally,Step4focusesonidentifyingthehierarchicalstructureembeddedwithinthesetofpseudo-
labels,whichisrepresentedasaforeststructure(i.e.,setoftrees)wheretherootsarewholeentities,
and their descendants are parts and subparts, etc. We test each pair of pseudo-labels i and j to
determinetheirhierarchicalrelation: If1)overθ %pixelsofpseudo-labeliarealsoinpseudo-
cover
label j (meaning that i is covered by j), and 2) less than θ % pixels of pseudo-label j are in
cover
pseudo-labeli(meaningthatjislargerthani),thenpseudo-labeljisanancestorofiinthehierarchy
forest. The smallest ancestor of i is the direct parent of i. By testing the pixel coverage between
pseudo-labels,wecanfigureoutthehierarchicalstructureofourpseudo-labels.
3.2 SELF-INSTRUCTION: LEARNFROMINITIALPSEUDO-LABELS
In the self-instruction phase, we need to address two problems: 1) The initial pseudo-labels from
thepreviousself-explorationphasecontainnoises. Howtoleverageself-supervisedlearningsignals
to “average out” the noises? 2) Existing general-purpose segmentation heads cannot predict the
hierarchicalrelationsamongmasks. Howtolearnthehierarchyforestfromthepreviousphase?
To address the first problem, we train a segmentation model to learn and generalize from the ini-
tialpseudo-labels. Throughthisprocedure,thesegmentationmodelcanobservevalidentitiesfrom
pseudo-labels which are more frequent than noises, and thus accurately segments unseen images.
The model is composed of a ViT-based backbone and a Mask2Former (Cheng et al., 2022) seg-
mentationmodel. Inparticular,thebackboneisconstructedbythesameDINO(Caronetal.,2021)
pre-trainedViT,andViT-Adapter(Chenetal.,2022)forproducingmulti-scalevisualfeaturemaps.
TheViTbackboneisnotfixed,andthuswecanadaptitsfeaturesforthesegmentationtask.
To accomplish the hierarchical segmentation task, we attach a novel ancestor prediction head to
Mask2Former, which predicts the hierarchical relations among the predicted masks. In parallel to
the existing mask and class prediction heads, our ancestor prediction head operates on the query
featuresQ ∈ RN×C,whereN isthenumberofqueriesandC isthequeryfeaturedimension. As
showninFigure4, thelearningtargetoftheancestorpredictionisanon-symmetricbinarymatrix
representing the ancestor relations P ∈ {0,1}N×N, where P = 1 represents that mask i is an
i,j
ancestorofmaskj,andP = 0otherwise. Itisworthnotingthatamaskimayhavenoancestors
i,j
(asarootinthehierarchyforest),ifmaskiisawholeentity;maskimayalsohavemorethanone
ancestor(asadeepnodeintheforest),ifmaskiisapartofanotherpart. Theancestorpredictionis
formulatedas:
(cid:16) √ (cid:17)
Pˆ =sigmoid (QW )(QW )⊤/ C ∈RN×N, (1)
1 2
whereW ,W ∈RC×C arelearnableweightsfortwolineartransformations. Weusetwodifferent
1 2
linearmappingssincetheancestorrelationsareasymmetric. Theyareoptimizedviaabinarycross-
entropy(BCE)lossL = BCE(Pˆ,P). Atinferencetime, wecanemploytopologicalsorting
ancestor
toreconstructtheforeststructurefromthebinaryancestorrelationpredictions. Differentfromprior
transformer-based hierarchical segmentation methods like GroupViT (Xu et al., 2022) which are
5PublishedasaconferencepaperatICLR2024
Teacher Branch
Weakly Teacher's
Augmented Pseudo-labels
Image 𝒯-.’/(𝐼!) 𝑌&.’01.*
U Imnl aa gb ee l 𝐼e !d ℳT ",e Θa "c %h ’e ()r
%*
𝐿!"#$%"&
Strongly Student's
Augmented EMA Predictions
Image 𝒯)&*+$,(𝐼!) ℳ𝒯!"#$%&(𝐼’),Θ!"()*%"
U Imnl aa gb ee l 𝐼e "d ℳS ",t Θud !"e #n $t
%&"
Optimize 𝐿!*!#)
Strongly Student's
Self-explore Augmented Predictions
Image 𝒯)&*+$,(𝐼") ℳ𝒯!"#$%&(𝐼+),Θ!"()*%"
Initial 𝐿’(’!’#)
Pseudo-labels
𝑌#$%&%’(
Initial Branch
Figure 5: Teacher-student mutual-learning in the self-correction phase. We initialize both the
teacher and student with the segmentation model learned in the self-instruction phase, which pro-
ducesbettersegmentationpredictionsthantheinitialpseudo-labels. Thestudentreceivessupervi-
sion from the teacher’s pseudo-labels and the initial pseudo-labels. The teacher is updated as the
exponentialmovingaverage(EMA)ofthestudent.
constrainedbythepre-definednumberofhierarchicallevels,ourmethodisabletopredictavariable
numberoflevelsandentities.
3.3 SELF-CORRECTION: IMPROVEOVERINITIALPSEUDO-LABELS
Althoughwehaveelaboratelybuiltapseudo-labelingprocessintheself-explorationphase,itisstill
basedonafixedself-supervisedvisualrepresentationthatisnotoptimizedforimagesegmentation.
Consequently, there may still be initial pseudo-labels that are noisy and can negatively affect our
model. Meanwhile, we observe that the segmentation model learned through the self-instruction
phase can predict masks that are more reliable and accurate than the clustering results from the
self-explorationphase. Motivatedbythisobservation,inthefinalself-correctionphase,wefurther
bootstrapourmodelbylearningfromitselfandmitigatingtheimpactofnoisesintheinitialpseudo-
labels. To achieve self-correction, we adopt a semi-supervised approach that is based on teacher-
studentmutual-learning(Tarvainen&Valpola,2017;Liuetal.,2020).
Theself-correctionphasestartsoffbyinitializingtwoseparatesegmentationmodelswhichareexact
clonesofthesegmentationmodelproducedbytheself-instructionphase. Wedenoteonesegmenta-
tionmodelasthestudentmodelM(·,Θ ),whichisactivelyupdatedthroughgradientdescent;
student
theothersegmentationmodelistheteachermodelM(·,Θ ),whichisupdatedeveryiterationas
teacher
anexponentialmovingaverage(EMA)ofthestudent:Θ ←mΘ +(1−m)Θ ,where
teacher teacher student
m ∈ (0,1)isthemomentum. Thestudentreceivessupervisionfromboththeinitialpseudo-labels
andtheteacher’spseudo-labels. Thus,thetotallossiscomputedas:
L =L (M(T (I ),Θ ),Y )+L (M(T (I ),Θ ),Y ), (2)
total seg strong 1 student initial seg strong 2 student teacher
whereI andI aretwoimagebatches,Y istheinitialpseudo-labelsonI ,Y istheteacher’s
1 2 initial 1 teacher
pseudo-labelsbythresholdingthepredictionsM(T (I ),Θ ),T andT denotestrong
weak 2 teacher strong weak
and weak data augmentations respectively, and L is the segmentation loss which is composed
seg
ofaclassificationlossL ,amaskpredictionlossL ,andourancestorpredictionlossL .
cls mask ancestor
Figure5illustratesthestepsinourteacher-studentlearningapproach.
Toobtainmorereliablesupervisionfromtheteacher’spredictionsM(T (I ),Θ ),wekeep
weak 2 teacher
only those masks with confidence scores exceeding θ to form the pseudo-labels Y . We
score teacher
observe that the teacher model tends to be less confident when segmenting smaller entities. If the
thresholdθ isfixedacrossallmasks,itwouldresultintoofewpseudo-labelswithsmallareas,
score
and consequently, the student’s small entity segmentation performance and overall performance
wouldbeimpaired. Therefore,weleverageadynamicthreshold:
θ =(1−(1−a)γ)(θ −θ )+θ , (3)
score score,large score,small score,small
where a ∈ (0,1) represents the area ratio of the predicted mask to the whole image, γ > 1 is
a hyper-parameter, and θ < θ are the pre-defined thresholds for the smallest and
score,small score,large
largestmask,respectively.Thisdynamicthresholdacrossdifferentscalesallowsustobetterbalance
small,medium,andlargeentitiesintheteacher’spseudo-labels,andencouragesthestudentmodel
tosegmentsmallentitiesmoreaccurately.
6PublishedasaconferencepaperatICLR2024
Table1: Zero-shotevaluationonvariousimagedatasets. SOHESsetsnewstate-of-the-artself-
supervisedopen-worldentitysegmentationperformance. Thecollectionoftheevaluationdatasets
represents diverse classes in an open world and includes both whole entities and parts. Mean-
while,usingjust2%unlabeledimagesasSAM,SOHESsignificantlyclosesthegapbetweenself-
supervisedmethodsandthesupervisedSAM.Theevaluationmetricisaveragerecall(AR).
Datasetsw/WholeEntities Datasetsw/Parts
Supervision Method
COCO LVIS ADE Entity SA-1B PtIN PACO
Supervised SAM(Kirillovetal.,2023) 49.6 46.1 45.8 45.9 60.8 28.3 18.1
FreeSOLO(Wangetal.,2022b) 11.6 5.9 7.3 8.0 2.2 13.8 2.4
Self- CutLER(Wangetal.,2023) 28.1 20.2 26.3 23.1 17.0 28.7 8.9
supervised HASSOD(Caoetal.,2023) 28.3 22.5 27.8 26.2 26.0 32.6 11.4
SOHES(Ours) 30.5 29.1 31.1 33.5 33.3 36.0 17.1
ImprovementoverHASSOD +2.2 +6.6 +3.3 +7.3 +7.3 +3.4 +5.7
ReducedGapvs.SAM -10% -28% -18% -37% -21% ∗ -85%
∗SOHESoutperformsSAMonPartImagenet.
4 EXPERIMENTS
In this section, we thoroughly evaluate SOHES on various datasets and examine the ViT-based
backboneimprovementfordownstreamtasks. Weperformaseriesofablationstudyexperimentsto
demonstratetheefficacyofmodulesandstepsin SOHES.Wealsodiscusslimitationsof SOHES
inAppendixE.AdditionalqualitativeresultsareshowninAppendixF.
4.1 TRAININGANDEVALUATIONDATA
We train our SOHES model on the SA-1B (Kirillov et al., 2023) dataset. In SA-1B, there are 11
millionimagesequallysplitinto1,000packs. Unlessotherwisespecified,weuse20packsofraw
images(2%)fortraining,and1differentpack(0.1%)forevaluation.
For evaluation purposes, we test SOHES on various image datasets with segmentation mask an-
notations in a zero-shot manner (i.e., no further training on evaluation datasets). The diversity in
theevaluationdatasetscansimulatethechallengeofunseenentityclassesandimagedomainsinan
open-worldsetting. Sincetheannotationsineachdatasetmayonlycoverentitiesfromapre-defined
listofclasses,thecommonlyusedMS-COCOstyleaverageprecision(AP)metricforclosed-world
detection/segmentation would incorrectly penalize open-world predictions that cannot be matched
with ground truths in known classes. More details of the AP metric are discussed in Appendix B.
Followingpriorwork(Kimetal.,2022;Wangetal.,2022a;Liuetal.,2022;Caoetal.,2023),we
mainlyconsidertheaveragerecall(AR)metricforupto1,000predictionsperimagewhencompar-
ingdifferentmethods. OtherimplementationdetailsareinAppendixA.
4.2 OPEN-WORLDENTITYSEGMENTATION
WeevaluateSOHESonavarietyofdatasets,includingMS-COCO(Linetal.,2014),LVIS(Gupta
etal.,2019), ADE20K(Zhouetal.,2017), EntitySeg(Qietal.,2023), andSA-1B(Kirillovetal.,
2023). These datasets include naturalimages of complex scenes, in which multiple visual entities
of diverse classes present and are labeled with segmentation masks. Thus, the collection of such
evaluation datasets can faithfully reflect the performance of an open-world segmentation model.
Wecomparewithrecentself-supervisedmethodsFreeSOLO(Wangetal.,2022b),CutLER(Wang
et al., 2023), and HASSOD (Cao et al., 2023). We aim to close the gap between self-supervised
methodsandthesupervisedstate-of-the-artmodelSAM(Kirillovetal.,2023). Notably,weuseonly
2% images as SAM for training SOHES, and we do not require any human annotations on these
images.
As summarized in Table 1 and Table 4, SOHES consistently outperforms the prior state-of-the-
art HASSOD by large margins (e.g., +7.3 AR on SA-1B and EntitySeg). Meanwhile, SOHES
significantlyclosesthegapbetweenself-supervisedmethodsandsupervisedmethods. Forinstance,
usingonly2%unlabeleddatainSA-1B,SOHESalreadyachievesoverhalfARofSAM.SOHES
alsoreducesthegapbetweenself-supervisedmethodsandSAMonSA-1Brelativelyby37%.
4.3 PARTSEGMENTATION
Inadditionaltowholeentities, SOHESalsolearnstosegmenttheirconstituentpartsandsubparts.
To evaluate our hierarchical segmentation results, we compare them with the ground-truth mask
7PublishedasaconferencepaperatICLR2024
Figure 6: Downstream performance of ViT-
basedbackbones. WefreezetheViTandfine- 10
tune a ViT-Adapter and a lightweight segmen- Resolution = 224
tation/detection head on ADE20K/MS-COCO. 5 Resolution = 448
The backbone further fine-tuned in SOHES is 32 16 8
Patch size
moreadaptedtodense-predictiontasks.
Figure 7: Mask quality of the initial pseudo-
Backbone ADE20KmIoU MS-COCOAP labels produced by DINO backbones with
Pre-trainedbyDINO 35.2 22.7 differentpre-trainingconfigurations.Asmall
Fine-tunedbySOHES 39.6 24.4
pre-training patch size leads to better fine-
grainedfeaturesandhigh-qualitypseudo-labels.
annotationsofobjectparts(e.g.,headsandtailsofanimals)intwodatasets,PartImageNet(Heetal.,
2022a)andPACO-LVIS(Ramanathanetal.,2023),andsummarizetheresultsinTable1andTable5.
Comparedwithpriorself-supervisedbaselines,SOHESmoreaccuratelylocalizesmeaningfulparts
ofentities,andalmostdoublesCutLER’sperformanceonPACO(8.9AR→17.1AR).Impressively,
SOHES outperformsSAM onPartImageNetandisonparwithSAMonPACO.Thereasonisthat
SOHEScanpredictmorepartsandsubpartsthanSAM(seeFigure14),whicharethefocusofthe
twodatasets’annotations.
4.4 IMPROVEDBACKBONEFEATURES
Throughourself-instructionandcorrectionphases, weadaptself-supervisedrepresentationDINO
toanopen-worldsegmentationmodel. Consequently,ourfine-tunedvisualbackbonecanbetterfit
intootherdensepredictiontasks. Totestsuchabilities, wecomparea)theViT-B/8backbonepre-
trained by DINO and b) the backbone further tuned in SOHES, in downstream tasks of semantic
segmentationonADE20KandobjectdetectiononMS-COCO.Thedownstreamfine-tuningisper-
formed in a minimalistic style, mimicking the linear probing (Chen et al., 2020; He et al., 2020)
in self-supervised representation learning. For semantic segmentation, we directly attach a linear
classifieronthefeaturemapsfromViT-Adapter; forobjectdetection, weattachthesimplestReti-
naNet (Lin et al., 2017) detection head on the ViT-Adapter. We keep the ViT parameters frozen
duringthesupervisedfine-tuning. Table6summarizestheresults,demonstratingthatSOHEScan
adapttheViT-basedbackbonetogeneratebetterfeaturesfordensepredictiondownstreamtasks.
4.5 ABLATIONSTUDY
Inthissubsection,weablatethedesignchoicesinSOHESonSA-1B,andprovideourinsightsfor
future research in self-supervised open-world segmentation. Further details about the choices of
hyper-parametersandmodelarchitecturesarediscussedinAppendixD.
DINObackbone.Inrecentself-supervisedobjectlocalization/discoverywork(Sime´onietal.,2023;
Wangetal.,2023), researchersprefertheViTbackbonepre-trainedbyDINO,inparticularViT-B
withpatchsize8. WehavealsoobservedthataViTbackbonewithpatchsize8leadstobettermask
quality in SOHES. To investigate this, we use DINO to pre-train ViT-B backbones with varying
patch size and input resolution configurations, with a shorter 100-epoch training schedule (DINO
originally pre-trains on ImageNet (Deng et al., 2009) for 300 epochs). Then, we repeat our self-
explorationphasewiththesebackbones,andtheresultingmaskqualitycomparisonissummarized
inFigure7andTable7.Fromthiscomparison,wecanobservethatthesmallpatchsizeispositively
correlatedwiththemaskquality. Whenthepatchsizedecreasesfrom32to8,theARsignificantly
improves. Meanwhile,theinputresolutiondoesnotinfluencethemaskqualityasmuch. Thesmall
patchsizemaybettersupporttheViTtocapturepixel-aligneddetailsforlocalizingentites,andthus
is more suited in our self-supervised segmentation task. It is worth noting that we cannot further
reducethepatchsizeduetocomputationalconstraints.Wheneverthepatchsizeishalved,ViTneeds
toprocess4×patches,andperform16×computationinself-attention. Therefore,theoff-the-shelf
DINOViT-B/8isthebestchoiceinourtask.
Steps in self-exploration. In our self-exploration phase, we have delicately designed a series of
steps to generate, select, and refine the pseudo-labels. We summarize the impact of the design
choicesinTable2. Inthefirstglobalclusteringstep,ifweuseonefixedmergingthresholdθ ,a
merge
largerθ leadstomoremasksperimageandbettercoverageofentities(increasingAR),andalso
merge
introducesnoises(oscillatingAP).Wechoosetomixtheresultswithdifferentthresholdstogether
8
RA
ksaMPublishedasaconferencepaperatICLR2024
Table2: Impactofeachstepinself-exploration. Wemixtheglobalclusteringresultsfrommul-
tiplemergingthresholds, adoptthelocalre-clustering, andusetheoff-the-shelfCascadePSPmask
refinementmoduletoobtainthebestinitialpseudo-labels.
Time/Img MaskQuality
Step Choice Masks/Img
(sec) AP AR AR AR AR
S M L
θ =0.1 1 4.9 0.2 0.0 0.0 0.3 0.1
merge
θ =0.2 3 4.9 0.8 0.1 0.1 0.6 0.3
merge
θ =0.3 9 4.5 0.9 0.4 0.6 2.0 1.0
merge
1 θ =0.4 23 3.8 0.6 0.7 1.5 5.4 2.6
merge
θ =0.5 58 2.7 1.4 1.2 3.4 11.1 5.4
merge
θ =0.6 131 2.2 0.6 1.5 6.0 15.3 8.1
merge
Mixw/NMS 148 5.3 1.1 1.9 6.5 17.2 9.1
2 w/localre-clustering 115 8.4 2.0 5.1 10.1 17.5 11.6
DenseCRF(Kra¨henbu¨hl&Koltun,2011) 61 18.2 4.7 3.5 9.5 20.7 12.0
3 CRM(Shenetal.,2022) 71 18.7 2.7 4.7 13.5 20.2 14.1
CascadePSP(Chengetal.,2020) 101 15.2 4.7 6.0 15.8 22.6 16.4
Table3:Impactofthedynamicthresholdinself-correction.Ifthevanillateacher-studentlearning
is employed in the self-correction phase (row 2), the imbalance between small and large entity
segmentation is intensified which leads to worse overall performance. Our dynamic threshold for
filtering the teacher’s pseudo-labels (row 3) can encourage the student’s predictions for small and
mediumentitiesandimprovetheoverallAR.
MaskQuality
Training
AP AR AR AR AR
S M L
Phase2 12.8 8.0 33.7 43.0 32.6
Phase2+3w/odynamicthreshold 10.9 6.3 32.5 45.3 32.5
Phase2+3w/dynamicthreshold 12.9 8.6 35.2 42.0 33.3
andremoveduplicates,whichprovidesthebestARandonlyslightlyincreasesthenumberofmasks
compared with the largest θ . In the second local re-clustering step, we significantly improve
merge
therecallforsmallentities,relativelyby168%. Thisstepensuresthatourmodelreceivesadequate
supervisionfromsmallentities. Wealsoimprovetheoverallrecallby2.5AR.Finally,inthethird
refinement step, we adopt CascadePSP (Cheng et al., 2020) because it can best boost the overall
mask quality. The other two options, DenseCRF (Kra¨henbu¨hl & Koltun, 2011) and CRM (Shen
et al., 2022), are also viable, but they change the pseudo-labels more aggressively, leading to the
removal of many potential entities. Overall, each step in our self-exploration phase contributes to
the high-quality initial pseudo-labels for SOHES. Notably, we can parallelize the processing for
eachimageandaccelerateself-explorationwithmorecomputenodes.
Self-correction. In our self-correction phase, we adopt a teacher-student mutual-learning frame-
work from semi-supervised learning, to continuously improve the segmentation model by itself.
However, as shown in Table 3, the initial attempt of the mutual-learning with a fixed confidence
thresholdleadstoworseperformance. Infact,theimbalanceddistributionisreinforcedduringthis
procedure, as indicated by the decreased AR for small and medium entities and increased AR for
largerentities. Therefore,weneedadynamicthresholdthatallowsmoresmallandmediumpseudo-
labelsfromtheteachermodel,andbalancesthestudent’spredictionforentitiesofdifferentscales.
Withthedynamicthreshold,wecanimproveAR andAR relativelyby7.5%and4.5%,withan
S M
acceptablecostof2.3%AR . Consequently,theoverallARisimprovedby0.7.
L
5 CONCLUSION
We present SOHES, a self-supervised approach towards open-world entity segmentation with hi-
erarchical structures. Through three phases of self-evolution, a self-supervised learner is adapted
toanopen-worldsegmentationmodel. Byrecognizingandlocalizingentitiesandtheirconstituent
parts in an open world with superior mask quality, SOHES substantially closes the gap between
self-supervisedandsupervisedmethods,andsetsthenewstateoftheartonvariousdatasets.
9PublishedasaconferencepaperatICLR2024
Acknowledgement. ThisworkwassupportedinpartbyNSFGrant2106825,NIFAAward2020-
67021-32799, and the Jump ARCHES endowment through the Health Care Engineering Systems
Center.ThisworkusedNVIDIAGPUsatNCSADeltathroughallocationsCIS220014,CIS230012,
andCIS230013fromtheAdvancedCyberinfrastructureCoordinationEcosystem: Services&Sup-
port (ACCESS) program, which is supported by NSF Grants #2138259, #2138286, #2138307,
#2137603,and#2138296.
REFERENCES
Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran. Zero-shot
objectdetection. InECCV,2018.
AbhijitBendaleandTerranceBoult. Towardsopenworldrecognition. InCVPR,2015.
AbhijitBendaleandTerranceBoult. Towardsopensetdeepnetworks. InCVPR,2016.
ZhaoweiCaiandNunoVasconcelos. CascadeR-CNN:Delvingintohighqualityobjectdetection.
InCVPR,2018.
ShengcaoCao,DhirajJoshi,Liang-YanGui,andYu-XiongWang. HASSOD:Hierarchicaladaptive
self-supervisedobjectdetection. InNeurIPS,2023.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herve´ Je´gou, Julien Mairal, Piotr Bojanowski, and
ArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InICCV,2021.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and
fullyconnectedcrfs. TPAMI,40(4):834–848,2017.
TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor
contrastivelearningofvisualrepresentations. InICML,2020.
Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision
transformeradapterfordensepredictions. InICLR,2022.
BowenCheng,IshanMisra,AlexanderGSchwing,AlexanderKirillov,andRohitGirdhar.Masked-
attentionmasktransformerforuniversalimagesegmentation. InCVPR,2022.
Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung Tang. CascadePSP: Toward class-
agnosticandveryhigh-resolutionsegmentationviaglobalandlocalrefinement. InCVPR,2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchicalimagedatabase. InCVPR,2009.
AkshayDhamija,ManuelGunther,JonathanVentura,andTerranceBoult. Theoverlookedelephant
ofobjectdetection: Openset. InWACV,2020.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszko-
reit,andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionat
scale. InICLR,2020.
GolnazGhiasi,YinCui,AravindSrinivas,RuiQian,Tsung-YiLin,EkinDCubuk,QuocVLe,and
BarretZoph. Simplecopy-pasteisastrongdataaugmentationmethodforinstancesegmentation.
InCVPR,2021.
AgrimGupta,PiotrDollar,andRossGirshick. LVIS:Adatasetforlargevocabularyinstanceseg-
mentation. InCVPR,2019.
Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of
statisticallearning: Datamining,inference,andprediction. Springer,2009.
10PublishedasaconferencepaperatICLR2024
JuHe,ShuoYang,ShaokangYang,AdamKortylewski,XiaodingYuan,Jie-NengChen,ShuaiLiu,
ChengYang,QihangYu,andAlanYuille. PartImageNet: Alarge,high-qualitydatasetofparts.
InECCV,2022a.
KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGirshick. MaskR-CNN. InICCV,2017.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervisedvisualrepresentationlearning. InCVPR,2020.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dolla´r, and Ross Girshick. Masked
autoencodersarescalablevisionlearners. InCVPR,2022b.
RonghangHu, PiotrDolla´r, KaimingHe,TrevorDarrell, andRossGirshick. Learningtosegment
everything. InCVPR,2018.
Ayush Jaiswal, Yue Wu, Pradeep Natarajan, and Premkumar Natarajan. Class-agnostic object de-
tection. InWACV,2021.
K J Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian. Towards open
worldobjectdetection. InCVPR,2021.
TarunKalluri,WeiyaoWang,HengWang,ManmohanChandraker,LorenzoTorresani,andDuTran.
Open-world instance segmentation: Top-down learning with bottom-up supervision. arXiv
preprintarXiv:2303.05503,2023.
Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and Weicheng Kuo. Learning open-
worldobjectproposalswithoutlearningtoclassify. IEEERoboticsandAutomationLetters,7(2):
5453–5460,2022.
AlexanderKirillov,KaimingHe,RossGirshick,CarstenRother,andPiotrDolla´r.Panopticsegmen-
tation. InCVPR,2019.
AlexanderKirillov, EricMintun, NikhilaRavi, HanziMao, ChloeRolland, LauraGustafson, Tete
Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla´r, and Ross Girshick.
Segmentanything. InICCV,2023.
PhilippKra¨henbu¨hlandVladlenKoltun. EfficientinferenceinfullyconnectedCRFswithGaussian
edgepotentials. InNeurIPS,2011.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolla´r,andCLawrenceZitnick. MicrosoftCOCO:Commonobjectsincontext. InECCV,2014.
Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDolla´r.Focallossfordenseobject
detection. InICCV,2017.
YangLiu, IdilEsenZulfikar, JonathonLuiten, AchalDave, DevaRamanan, BastianLeibe, Aljosˇa
Osˇep,andLauraLeal-Taixe´. Openingupopenworldtracking. InCVPR,2022.
Yen-ChengLiu, Chih-YaoMa, ZijianHe, Chia-WenKuo, KanChen, PeizhaoZhang, BichenWu,
Zsolt Kira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. In ICLR,
2020.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. InCVPR,2015.
LuQi, JasonKuen, YiWang, JiuxiangGu, HengshuangZhao, PhilipTorr, ZheLin, andJiayaJia.
Open-worldentitysegmentation. TPAMI,45(7):8743–8756,2022.
Lu Qi, Jason Kuen, Tiancheng Shen, Jiuxiang Gu, Weidong Guo, Jiaya Jia, Zhe Lin, and Ming-
HsuanYang. High-qualityentitysegmentation. InICCV,2023.
Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui
Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, Amir Mousavi, Yiwen Song, Abhi-
manyu Dubey, and Dhruv Mahajan. PACO: Parts and attributes of common objects. In CVPR,
2023.
11PublishedasaconferencepaperatICLR2024
WalterJScheirer,AndersondeRezendeRocha,ArchanaSapkota,andTerranceBoult.Towardopen
setrecognition. TPAMI,35(7):1757–1772,2012.
TianchengShen,YuechenZhang,LuQi,JasonKuen,XingyuXie,JianlongWu,ZheLin,andJiaya
Jia. Highqualitysegmentationforultrahigh-resolutionimages. InCVPR,2022.
JianboShiandJitendraMalik. Normalizedcutsandimagesegmentation. TPAMI,22(8):888–905,
2000.
Oriane Sime´oni, Gilles Puy, Huy V Vo, Simon Roburin, Spyros Gidaris, Andrei Bursuc, Patrick
Pe´rez,RenaudMarlet,andJeanPonce. Localizingobjectswithself-supervisedtransformersand
nolabels. InBMVC,2021.
Oriane Sime´oni, Chloe´ Sekkat, Gilles Puy, Anton´ın Vobecky`, E´loi Zablocki, and Patrick Pe´rez.
Unsupervisedobjectlocalization:Observingthebackgroundtodiscoverobjects. InCVPR,2023.
AnttiTarvainenandHarriValpola. Meanteachersarebetterrolemodels: Weight-averagedconsis-
tencytargetsimprovesemi-superviseddeeplearningresults. InNeurIPS,2017.
JasperRRUijlings,KoenEAVanDeSande,TheoGevers,andArnoldWMSmeulders. Selective
searchforobjectrecognition. IJCV,104:154–171,2013.
HuyVVo,FrancisBach,MinsuCho,KaiHan,YannLeCun,PatrickPe´rez,andJeanPonce. Unsu-
pervisedimagematchingandobjectdiscoveryasoptimization. InCVPR,2019.
Huy V Vo, Patrick Pe´rez, and Jean Ponce. Toward unsupervised, multi-object discovery in large-
scaleimagecollections. InECCV,2020.
VanHuyVo,ElenaSizikova,CordeliaSchmid,PatrickPe´rez,andJeanPonce. Large-scaleunsuper-
visedobjectdiscovery. InNeurIPS,2021.
Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran. Unidentified video objects: A benchmark
fordense,open-worldsegmentation. InICCV,2021.
Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and Du Tran. Open-world instance seg-
mentation: Exploitingpseudogroundtruthfromlearnedpairwiseaffinity. InCVPR,2022a.
XinlongWang,ZhidingYu,ShaliniDeMello,JanKautz,AnimaAnandkumar,ChunhuaShen,and
JoseMAlvarez. FreeSOLO:Learningtosegmentobjectswithoutannotations. InCVPR,2022b.
XudongWang,RohitGirdhar,StellaXYu,andIshanMisra. Cutandlearnforunsupervisedobject
detectionandinstancesegmentation. InCVPR,2023.
YangtaoWang,XiShen,YuanYuan,YumingDu,MaomaoLi,ShellXuHu,JamesLCrowley,and
DominiqueVaufreydaz.TokenCut:Segmentingobjectsinimagesandvideoswithself-supervised
transformerandnormalizedcut. InCVPR,2022c.
Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan. Adan: Adaptive nesterov
momentumalgorithmforfasteroptimizingdeepmodels.arXivpreprintarXiv:2208.06677,2022.
Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong
Wang. GroupViT:Semanticsegmentationemergesfromtextsupervision. InCVPR,2022.
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene
parsingthroughADE20Kdataset. InCVPR,2017.
12PublishedasaconferencepaperatICLR2024
A IMPLEMENTATION DETAILS
Self-exploration. We use DINO (Caron et al., 2021) pre-trained ViT-B/8 as the feature extractor
togeneratepatch-levelvisualfeatures. Duringtheglobalclusteringstep, wefirstresizeunlabeled
imagestoresolutionS ×S = 1,024×1,024, andcluster8×8patcheswithmergingthresholds
{θ }m = {0.6,0.5,0.4,0.3,0.2,0.1}, which are decided based on the number of pseudo-
merge,t t=1
labels(seeFigure8).Theninthelocalre-clusteringstep,wefurtherinvestigateregionssmallerthan
θ % = 1/1,024ofthetotalimagearea,andcroplocalimagesaroundthem. Thelocalimageis
small
resizedtoS′ ×S′ = 256×256anditssubregionsareclustered. Inthenextstep, weusetheoff-
the-shelfCascadePSP(Chengetal.,2020)modeltorefinethemasks. Inthefinalstepofhierarchy
analysis,thecoveragethresholdissettoθ %=90%.
cover
Self-instruction. We learn a segmentation model composed of DINO (Caron et al., 2021) pre-
trained ViT-B/8, ViT-Adapter (Chen et al., 2022), Mask2Former (Cheng et al., 2022), and our an-
cestor prediction head. The model is trained on 8 compute nodes, each equipped with 8 NVIDIA
A100GPUs. Thetotalbatchsizeis128, andthenumberoftrainingstepsis40,000. Weoptimize
themodelwiththeAdanoptimizer(Xieetal.,2022)andabaselearningrateof0.0008. Thetotal
trainingtimeisabout3days.
Self-correction. Theteacher-studentmutuallearningstartsaftertheself-instructionphase. Ittrains
themodelforadditional5,000iterations. Theteacherisupdatedastheexponentialmovingaverage
of the student, with momentum m = 0.9995. The loss terms from the initial pseudo-labels and
the teacher’s pseudo-labels are weighted equally. In the dynamic threshold, we set θ =
score,large
0.7,θ =0.3,γ =200.
score,small
B DEFICIENCY OF AP METRIC IN OPEN-WORLD SEGMENTATION
TheMS-COCO(Linetal.,2014)styleaverageprecision(AP)isaprevalentmetricforevaluating
object detection and instance segmentation models in the traditional closed-world setting with a
pre-defined scope of categories. However, for the open-world segmentation task, the AP metric
becomesmisleadingandcannotaccuratelyreflecttheopen-worldmodel’strueperformance: When
evaluating open-world segmentation models (which try to segment “everything”) on datasets with
closed-world annotations (such as MS-COCO/LVIS which only include a pre-defined, limited set
ofentityclasses),APwouldpenalizemodelpredictionsthatareactuallyvalidentities,butjustnot
annotatedbythedataset.
Consequently, when the dataset annotations cannot cover all entities, AP becomes misleading for
judging the performance of open-world models. As an example, the AP of SAM (Kirillov et al.,
2023) is lower than CutLER (Wang et al., 2023) on MS-COCO (6.1 vs. 9.8, Table 4) and PartIm-
ageNet (3.4 vs. 5.3, Table 5), which definitely cannot imply SAM is a model inferior to CutLER.
The lower AP of SAM is due to insufficient annotations in these two datasets, not the capability
of SAM. Meanwhile, on datasets which explicitly try to mitigate these annotation limitations like
SA-1B (Kirillov et al., 2023), AP can be more reliable than on traditional closed-world datasets
likeMS-COCO(Linetal.,2014). Forinstance,APonSA-1B(themostdenselyannotateddataset
evaluated in our work, see Table 4) matches qualitative comparison and follows the same trend as
AR, and our AP (12.9) significantly surpasses CutLER’s AP (7.8). Prior work (Cao et al., 2023)
hasmadeasimilarobservationaboutthedeficiencyofMS-COCOAPevaluationinthecontextof
self-supervisedobjectdetection.
Ingeneral, wechoosetheaveragerecall(AR)asthemainmetricinsteadofAP,becauseARdoes
notpenalizevalidopen-worldpredictions. NotethatthischoiceofARoverAPisalsocommonly
adoptedintheopen-worldliterature. Examplesinclude(butarenotlimitedto)detection(Kimetal.,
2022),segmentation(Wangetal.,2022a),andtracking(Liuetal.,2022).
C ADDITIONAL EVALUATION RESULTS
WeprovideadditionalevaluationresultswithmoremetricsinTable4andTable5,asasupplement
toTable1inthemainpaper.
13PublishedasaconferencepaperatICLR2024
Table4: Detailedzero-shotevaluationresultsonimagedatasetswithannotationsofwholeentities.
MaskQuality
Method AP AR AR AR AR
S M L
MS-COCO(Linetal.,2014)
SAM(Kirillovetal.,2023) 6.1 33.4 59.6 64.1 49.6
FreeSOLO(Wangetal.,2022b) 4.3 0.5 11.5 31.2 11.6
CutLER(Wangetal.,2023) 9.8 13.1 31.6 49.3 28.1
HASSOD(Caoetal.,2023) 6.0 14.0 34.1 45.2 28.3
SOHES(Ours) 2.1 19.8 31.0 48.8 30.5
LVIS(Guptaetal.,2019)
SAM(Kirillovetal.,2023) 6.7 31.1 71.3 74.6 46.1
FreeSOLO(Wangetal.,2022b) 1.9 0.2 9.2 31.7 5.9
CutLER(Wangetal.,2023) 3.6 11.3 31.1 46.2 20.2
HASSOD(Caoetal.,2023) 4.2 12.7 36.1 47.8 22.5
SOHES(Ours) 1.9 19.8 39.4 59.2 29.1
ADE20K (Zhouetal.,2017)
SAM(Kirillovetal.,2023) 7.8 31.6 59.2 62.5 45.8
FreeSOLO(Wangetal.,2022b) 2.3 0.5 9.3 27.1 7.3
CutLER(Wangetal.,2023) 5.2 15.3 34.7 44.5 26.3
HASSOD(Caoetal.,2023) 7.0 16.2 36.7 46.7 27.8
SOHES(Ours) 2.6 21.8 37.2 49.0 31.1
EntitySeg(Qietal.,2023)
SAM(Kirillovetal.,2023) 14.8 11.0 25.0 55.6 45.9
FreeSOLO(Wangetal.,2022b) 3.0 0.1 1.2 10.7 8.0
CutLER(Wangetal.,2023) 7.7 6.1 15.3 27.2 23.1
HASSOD(Caoetal.,2023) 6.1 7.6 20.1 30.2 26.2
SOHES(Ours) 5.0 9.1 21.6 39.6 33.5
SA-1B(Kirillovetal.,2023)
SAM(Kirillovetal.,2023) 38.9 20.0 59.9 82.2 60.8
FreeSOLO(Wangetal.,2022b) 1.5 0.0 0.2 6.9 2.2
CutLER(Wangetal.,2023) 7.8 4.9 13.9 28.5 17.0
HASSOD(Caoetal.,2023) 13.8 12.9 22.8 38.3 26.0
SOHES(Ours) 12.9 8.6 35.2 42.0 33.3
Table5: Detailedzero-shotevaluationresultsonimagedatasetswithannotationsofobjectparts.
MaskQuality
Method AP AR AR AR AR
S M L
PartImageNet(Heetal.,2022a)
SAM(Kirillovetal.,2023) 3.4 25.4 29.3 28.5 28.3
FreeSOLO(Wangetal.,2022b) 3.3 0.6 7.7 26.4 13.8
CutLER(Wangetal.,2023) 5.3 13.2 26.5 38.1 28.7
HASSOD(Caoetal.,2023) 4.5 19.0 32.9 38.7 32.6
SOHES(Ours) 1.2 30.3 32.4 42.3 36.0
PACO-LVIS(Ramanathanetal.,2023)
SAM(Kirillovetal.,2023) 1.0 11.9 34.6 41.1 18.1
FreeSOLO(Wangetal.,2022b) 0.2 0.1 5.8 21.3 2.4
CutLER(Wangetal.,2023) 0.2 5.0 18.7 25.1 8.9
HASSOD(Caoetal.,2023) 0.4 6.4 24.2 31.4 11.4
SOHES(Ours) 0.4 12.0 29.0 41.9 17.1
14PublishedasaconferencepaperatICLR2024
D ADDITIONAL ABLATION STUDY RESULTS
SinceSOHESisaself-supervisedapproach,wedonotbaseourselectionofhyper-parametersona
posteriorimodelperformance. Instead,wechoosehyper-parametersbyconsideringsimplecriteria
such as computation constraints or small-scale experiments. In this section, we detail the design
choicesinSOHES.
Mergingthresholds. Inthefirststepofourself-explorationphase,weclusterpatchesintocoherent
regionsandstopatpre-setmergingthresholdsθ >···>θ .Wechoosethesethresholds
merge,1 merge,m
based on a practical computation constraint: pseudo-labels per image. When there are too many
pseudo-labels,dataloading,augmentation,andpre-processingwouldbecomeabottleneckinmodel
training.Therefore,wecontrolthenumberofpseudo-labelsperimagetobeunder200.Asshownin
Figure8,thequantityofpseudo-labelsgrowssignificantlywhenthemergingthresholdislarger. In
ordernottoexceed200masksperimage,wesetthemergingthresholdsas0.6,0.5,0.4,0.3,0.2,0.1.
300
250
Figure 8: Relation between the number of
200 pseudo-labels and the merging thresholds.
Asthemergingthresholdincreases,thenumber 150
of pseudo-labels rapidly grows. To control the
100 numberofpseudo-labelsperimage, wechoose
mergingthresholdsnolargerthan0.6.
50
0
0.2 0.4 0.6 0.8
Merging threshold
Local re-clustering threshold. In the second step of self-exploration, we use a threshold θ %
small
to select small regions that require a local re-examination. Pseudo-labels with areas smaller than
θ %ofthewholeimageareprocessedvialocalre-clustering,andthisstepimprovesthecoverage
small
ofsmallentities. Wechooseθ %=1/1,024because1)thecommonlyadoptedMS-COCOstyle
small
evaluation(Linetal.,2014)definessmallobjectsasobjectswhoseareaissmallerthan32×32pixels;
2)ourimagesareresizedto1,024×1,024intheself-explorationphase; 3)(32×32)/(1,024×
1,024)=1/1,024. Infact,ifthethresholdθ %islarger,thecoverageofsmallentitiescouldnot
small
besignificantlyimprovedfurther,andtheprocessingtimewouldbelonger,asshowninTable6a.
Table6: Impactofhyper-parametersθ %andθ %.
small cover
(a)Choiceofthelocalre-clusteringthresholdθ %.We (b)Choiceofthecoveragethresholdθ %.
small cover
set θ % = 1/1,024 by considering the relative areas of We set θ % = 0.9 for robust hierarchi-
small cover
smallentities.Usingalargerθ %doesnotfurtherimprove calrelationsbetweenourpseudo-labels.When
small
thecoverageofsmallentities,butintroducesadditionalcom- θ %varieswithin[0.6,0.95],thehierarchi-
cover
putationoverheads. calrelationsarestable.
θ % AR Time/Img(sec) RelativeChange
small S θ %
cover w.r.t. θ =0.9
0(Nolocalre-clustering) 1.9 0.0 cover
1/2,048 4.3 6.0 0.6 9%
1/1,024 5.1 8.4 0.7 7%
1/512 5.1 10.0 0.8 4%
0.85 2%
0.9 0%
0.95 4%
1.0 36%
Coveragethreshold. Inthefinalstepofself-exploration,weanalyzethehierarchicalrelationsbe-
tween pseudo-labels. The pairwise test involves a coverage threshold θ %. For robustness, we
cover
15
egami
rep
slebal-oduesPPublishedasaconferencepaperatICLR2024
choose θ % = 0.9 to allow an ancestor pseudo-label to not necessarily cover all of its descen-
cover
dants’ pixels. Indeed, when θ % ∈ [0.6,0.95], the induced hierarchical relations are relatively
cover
stable,asshowninTable6b.
DINObackbone. AsasupplementtoFigure7inthemainpaper,Table7summarizesthedetailed
statisticsofthemaskqualityofinitialpseudo-labelsproducedbyDINObackbonespre-trainedwith
differentconfigurations. Italsoincludesthemaskqualityaftereachself-explorationstep.
Table 7: Comparison of DINO backbones pre-trained with different configurations. A small
pre-trainingpatchsizeiscriticalforproducingfine-grainedvisualfeaturesandhigh-qualitypseudo-
labelmasks.Thesteps1,2,and3refertoglobalclustering,localre-clustering,andmaskrefinement,
respectively.
Pre-training MaskQuality
Step Masks/Image
Resolution PatchSize AP AR AR AR AR
S M L
1 144 0.8 1.6 5.3 14.7 7.6
224 8 2 124 1.3 4.1 7.8 15.0 9.4
3 105 3.2 5.0 12.6 20.6 13.8
1 108 0.8 1.3 4.5 13.0 6.6
224 16 2 75 1.7 3.5 6.9 13.3 8.3
3 65 4.0 4.2 11.3 17.0 11.9
1 43 0.6 0.7 2.6 5.8 3.3
224 32 2 17 0.9 0.3 2.2 5.9 3.0
3 14 1.8 0.4 3.4 6.6 3.9
1 114 1.0 1.2 4.2 14.6 6.9
448 8 2 103 1.6 3.1 6.4 14.8 8.5
3 82 3.4 3.9 10.8 19.1 12.2
1 102 0.8 1.3 3.9 13.1 6.3
448 16 2 84 1.5 3.8 6.7 13.4 8.3
3 71 3.8 4.6 10.8 17.1 11.8
1 48 0.7 0.8 2.2 7.1 3.5
448 32 2 25 1.0 1.1 2.5 7.2 3.7
3 20 2.4 1.4 4.1 8.3 4.9
Segmentationhead. In SOHES,wechooseMask2Former(Chengetal.,2022)asoursegmenta-
tionheadmainlyformakinghierarchicalpredictions.InCascadeMaskR-CNN(Cai&Vasconcelos,
2018)usedbypriorwork(Wangetal.,2023;Caoetal.,2023),eachproposalispredictedindepen-
dently, and therefore, analyzing hierarchical relations between entities would be challenging. In
contrast,theattentionmodulesinMask2Formerallowinformationexchangeamongqueries,sowe
canbuildourancestorpredictionheadonMask2Former(seeFigure4). Foramorecomprehensive
comparisonwithpriorwork,wetrainasegmentationmodelbasedonCascadeMaskR-CNNusing
SOHESwithouttheentityhierarchy.AsshowninTable8,thismodelstillsignificantlyoutperforms
CutLERandHASSODwiththesamesegmentationhead.
WhenusingtheMask2Formersegmentationhead,wecanextenditwithourproposedancestorpre-
diction head to perform the additional task of hierarchical relation predictions. This module can
beconsideredasanadd-ontotheoriginalsegmentationheadwithminimalinfluenceonthemask
quality,sincethisancestorpredictionheadoperatesinparalleltothemaskpredictionhead. Infact,
the segmentation performance of a SOHES model trained without the ancestor prediction head is
closetothatofthestandardSOHESmodel(e.g.,onSA-1B,33.0MaskARwithoutancestorpredic-
tionvs.33.3MaskARwithancestorprediction), buttheformermodelcannotpredicthierarchical
relations.
16PublishedasaconferencepaperatICLR2024
Table8: ComparisonofmodelstrainedwiththeCascadeMaskR-CNNsegmentationhead. In
themainpaper,wechoosetheMask2Formersegmentationheadtomodelhierarchicalrelationsbe-
tweenpredictions,whilepriormethodsusuallyuseCascadeMaskR-CNN.WiththesameCascade
MaskR-CNNsegmentationhead,SOHESstilloutperformspriorwork.
MaskQuality(AR)
Method SegmentationHead
LVIS EntitySeg SA-1B
CutLER(Wangetal.,2023) 20.2 23.1 17.0
CascadeMaskR-CNN
HASSOD(Caoetal.,2023) 22.5 26.2 26.0
(Cai&Vasconcelos,2018)
SOHES(Ours) 27.0 29.2 31.9
SOHES(Ours) Mask2Former(Chengetal.,2022) 29.1 33.5 33.3
E LIMITATIONS
In Figure 9, we visualize some failure cases of SOHES: 1) When there are discontinuous entities
or occlusion (e.g., sky separated by foreground objects), SOHES may not correctly associate the
disconnectedsegmentsofthesameentity. Thereasonisthatintheself-explorationphase,weonly
mergeadjacentregions.Themodelrarelyobservesoneentityseparatedinmultipledisconnectedre-
gions.Weobservethatthecopy-pastedataaugmentation(Ghiasietal.,2021)cansimulateocclusion
and partially mitigate this issue, so we have adopted such data augmentation in SOHES training.
2) SOHES oftenproducesimprecisesegmentationmasksforlettersandcharacters. Theboundary
isnotperfectlyalignedwithstrokesandthemaskisoftenlargerthantheletter. 3)Whenthereare
blurrybackgrounds, SOHES tendsnottopredictamaskforthebackground. Similarfailurecases
canbeobservedwhenapplyingpriormethods(Wangetal.,2023;Caoetal.,2023;Kirillovetal.,
2023) to images affected by occlusion, text overlays, or blurring. We aim to resolve these issues
withimprovedpseudo-labelingstrategiesinfuturework.
Figure9:LimitationsofSOHES.Ourmethodsometimesfailtoaccuratelysegmentdiscontinuous
oroccludedentities(left),lettersorcharacters(middle),andblurredbackground(right).
F QUALITATIVE RESULTS
InFigures10,11,12,13,and14,wevisualizethesegmentationresultsofSOHES,andqualitatively
compare SOHES with the supervised model SAM on the evaluation datasets we have used in the
mainpaper.
17PublishedasaconferencepaperatICLR2024
Figure10:QualitativeresultsonMS-COCOimages.Ineachgroupofimages,fromtoptobottom
weshowtheoriginalinputimage, segmentationbySAM(Kirillovetal.,2023), andsegmentation
bySOHES.Asaself-supervisedmethod,SOHEScanachieveresultsthatarecomparabletothose
producedbythesupervisedmodelSAM.
18PublishedasaconferencepaperatICLR2024
Figure11: QualitativeresultsonADE20Kimages. Ineachgroupofimages,fromtoptobottom
weshowtheoriginalinputimage, segmentationbySAM(Kirillovetal.,2023), andsegmentation
bySOHES.Asaself-supervisedmethod,SOHEScanachieveresultsthatarecomparabletothose
producedbythesupervisedmodelSAM.
19PublishedasaconferencepaperatICLR2024
Figure12: QualitativeresultsonEntitySegimages. Ineachgroupofimages,fromtoptobottom
weshowtheoriginalinputimage, segmentationbySAM(Kirillovetal.,2023), andsegmentation
bySOHES.Asaself-supervisedmethod,SOHEScanachieveresultsthatarecomparabletothose
producedbythesupervisedmodelSAM.
20PublishedasaconferencepaperatICLR2024
Figure 13: Qualitative results on SA-1B images. In each group of images, from top to bottom
weshowtheoriginalinputimage, segmentationbySAM(Kirillovetal.,2023), andsegmentation
bySOHES.Asaself-supervisedmethod,SOHEScanachieveresultsthatarecomparabletothose
producedbythesupervisedmodelSAM.
21PublishedasaconferencepaperatICLR2024
Figure 14: Qualitative results on PartImageNet images. In each group of images, from top to
bottomweshowtheoriginalinputimage,segmentationbySAM(Kirillovetal.,2023),andsegmen-
tationbySOHES.Asaself-supervisedmethod,SOHEScanachieveresultsthatarecomparableto
thoseproducedbythesupervisedmodelSAM.
22