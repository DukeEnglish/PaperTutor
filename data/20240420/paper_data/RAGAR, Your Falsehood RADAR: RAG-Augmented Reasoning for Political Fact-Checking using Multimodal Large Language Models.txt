RAGAR, Your Falsehood RADAR:
RAG-Augmented Reasoning for Political Fact-Checking using
Multimodal Large Language Models
MohammedAbdulKhaliq1,3,PaulYu-ChunChang2∗,
MingyangMa2,BernhardPflugfelder2,FilipMiletic´1∗
1InstituteforNaturalLanguageProcessing,UniversityofStuttgart,
2appliedAIInitiativeGmbH,3appliedAIInstituteforEuropegGmbH
{mohammed.abdul-khaliq, filip.miletic}@ims.uni-stuttgart.de,
{p.chang, m.ma, b.pflugfelder}@appliedai.de
Abstract
The escalating challenge of misinformation,
particularly in the context of political dis-
course,necessitatesadvancedsolutionsforfact-
checking. Weintroduceinnovativeapproaches
to enhance the reliability and efficiency of
multimodalfact-checkingthroughtheintegra-
tionofLargeLanguageModels(LLMs)with
Retrieval-augmentedGeneration(RAG)-based
advancedreasoningtechniques. Thisworkpro-
posestwonovelmethodologies,ChainofRAG
(CoRAG)andTreeofRAG(ToRAG).Theap-
proaches are designed to handle multimodal
claims by reasoning the next questions that
need to be answered based on previous evi-
dence. Ourapproachesimprovetheaccuracy
of veracity predictions and the generation of
explanationsoverthetraditionalfact-checking
approachofsub-questiongenerationwithchain
ofthoughtveracityprediction. Byemploying
multimodalLLMsadeptatanalyzingbothtext
and images, this research advances the capa-
bilityofautomatedsystemsinidentifyingand
counteringmisinformation.
1 Introduction
Figure1:Anoverviewofthefact-checkingpipelinecon-
trastingthebaselineSub-QuestionGenerationapproach
Intheageofdigitalinformation,therapiddissem-
fromtheRAGAR:ChainofRAGandRAGAR:Tree
inationofnews,bothgenuineandfabricated,has
of RAG approach followed by a veracity explanation
becomeadefiningfeatureofthemodernera. The
generatedbyaVeracityPredictionmodule.
term“FakeNews",morepreciselyusedtoidentify
misinformation,disinformation,oracombination
ofboth(Aïmeuretal.,2023),hasgainedsignificant
for the spread of false information at an unprece-
attention. InthecaseofUSpolitics,forexample,
dented scale. A study conducted in 2018 found
suchnewscanhaveprofoundimplicationsonpub-
thatfalseinformationspreadssixtimesfasterthan
lic opinion and electoral outcomes. The urgency
thetruthonplatformslikeTwitter(Vosoughietal.,
toaddressthespreadoffalsenarrativesisfurther
2018). Suchrapidpropagationoffalsehoodsposes
motivatedbytheapproaching2024USPresidential
a significant threat to the democratic process, as
elections.
voters may base their decisions on inaccurate or
Socialmediahasdemocratizedinformationdis-
misleading information.Images have also been a
semination,allowinganyonewithaninternetcon-
widespreadsourceoffakenews. Ithasbeenshown
nectiontobroadcasttheirviews. Whilethishasled
thatfakenewsstoriesthatincludedimagesspread
toamoreopendiscourse,ithasalsopavedtheway
furtherthantheonesthatcontainedonlytext(Zan-
*CorrespondingAuthor nettou et al., 2018). Hence, one crucial line of
4202
rpA
81
]LC.sc[
1v56021.4042:viXraexisting research involves checking cross-modal 2 RelatedWork
consistencyandunifiedembeddingrepresentation
2.1 Retrieval-AugmentedGeneration(RAG)
of multimodal features to interpret images (Yao
forFact-Checking
etal.,2023a).
To combat hallucination in text generation by
LLMslikeBERT(Devlinetal.,2018)andBART
LLMs, current fact-checking pipelines often im-
(Lewis et al., 2020) have advanced detection, la-
plementaRAGapproachwhereintheLLMisfed
beling,andgenerationofveracityexplanationsto
datafromexternalsourcestoenhanceitsresponse.
combatfakenews(Dasetal.,2023). Multimodal
ThisenablestheLLMtoconditionitsgeneration
LLMs that can interpret and understand images
basedontheexternaldataandisnolongerlimited
have also been developed. Although LLMs have
byitsknowledgecutoff.
drawbacks,researchisbeingconductedtoremove
Pengetal.(2023)presenttheLLM-Augmenter
themandenableLLMstogeneratefactuallycorrect
system,whichcombinesexternalknowledgeinte-
explanations. Forinstance,adoptionofreasoning
grationandautomatedfeedbackmechanismswith
techniques like Chain of Thought have been ex-
a set of plug-and-play modules to enhance LLM
plored in the automated fact-checking domain to
performance. Further, Chern et al. (2023) de-
combathallucinationsandimprovetheveracityof
velop a factuality detection framework to assess
responses(Panetal.,2023;Chernetal.,2023).
LLM-generated text spanning multiple tasks and
domains. FortheKBQAtask,theauthorsmakeuse
Morerecently,Retrieval-AugmentedGeneration
of Google Search API to extract relevant knowl-
(RAG)hasalsobeenadoptedinfact-checkingus-
edge and parse the result. Similarly, Pan et al.
ingLLMs. RAGallowsLLMstoaccessup-to-date
(2023) propose a program-guided reasoning ap-
information without needing to be constantly re-
proachleveragingthein-contextlearningabilityof
trained. RecentstudieshaveexploredusingRAG
largelanguagemodels. ChainofThoughtreason-
withLLMsforautomatedfact-checkingofclaims.
ingisemployedtoguidethemodelinanswering
Commonapproaches(Asaietal.,2024;Zengand
complextasksZhangandGao(2023)proposeaHi-
Gao,2024)involveconvertingtheinputclaiminto
erarchicalStep-by-Step(HiSS)promptingmethod.
relevantphrasequerieswhichareusedtoquerya
Thisapproachinvolvesbreakingdownaclaiminto
searchengineforinformation. Theinformationis
sub-claims thereby creating a hierarchy and then
thenusedtoassesstheveracityoftheclaim.
sequentially verifying each one through multiple
Nevertheless,amergerbetweenreasoningtech- question-answeringstepsusingevidenceretrieved
niques such as that of Chain of Thought or more from the web. Concurrent research is being con-
advancedapproachessuchasTreeofThought(Yao ductedtoimprovethecapabilitiesofcontext-based
etal.,2023c)andRAGstillneedstobeexplored questiongenerationusingRAGevidence. Zengand
in the domain of automated fact-checking. To Gao(2024)arguethattheapproachforgenerating
thisend,wepresenttwomultimodalfact-checking justificationsforfact-checkingclaimsneedstobe
approaches based on utilizing the RAG response abletocapturethecomplexityofgeneratingjusti-
to motivate the next follow-up step to determine ficationsbasedonretrievedevidence. Toaddress
the veracity of political claims. We term these this, they propose a novel methodology focusing
approaches as RAG-augmented Reasoning (RA- onretrievingevidencetosupportorrefuteclaims,
GAR). Our RAGAR approaches employ multi- ratherthanrelyingsolelyonsummarization.
modalLLMs,adeptatcomprehendingtextualand
2.2 MultimodalFact-CheckingusingLLMs
visualelements,therebyincorporatinginformation
frombothmodalitieswhenverifyingtheclaim. The Multimodality is an underexplored field in fact-
twoRAGARapproachesweintroduceare(i)Chain checking(Alametal.,2022). Priorapproachesin
ofRAG(CoRAG)and(ii)TreeofRAG(ToRAG). evaluatingmultimodalclaims,particularlytextand
Ahigh-leveloverviewofthetwoapproachescan images,includecross-modalconsistencychecking
be seen in Figure 1. To our knowledge, this is orincorporatingacommonembeddingformulti-
thefirstworkincorporatingmultimodalLLMsand modalfeatures(Yaoetal.,2023b). Withtheadvent
utilizingmultimodalRAGcoupledwithreasoning ofmultimodalinstruction-finetunedLLMssuchas
inthefact-checkingdomaintohandlemultimodal GPT4-V and LLaVA, newer approaches towards
claims. assessingmultimodalfeaturesandassociatingthemFigure2: AdetailedoverviewoftheMultimodalFact-checkingpipeline
withtextualinputcanbedevelopedforgenerating Thisreducesourtestsamplesto794. Wepick300
veracitylabelsandexplanationsforclaims. test samples from this set of 794 and ensure that
Guoetal.(2023)introduceImg2Prompt,which thesetestsamplesareselectedatrandomfromsup-
leveragesLLM-agnosticmodelstogeneratetextual ported or refuted labels only, discarding the NEI
prompts from images. These prompts effectively labels. Therefore, our test dataset contains 150
guide LLMs in generating meaningful responses supported and150refuted multimodalclaims.
toVQAqueries,showcasingthemethod’sefficacy The supported and refuted labels signify that
inhandlingzero-shotlearningscenarios. Yaoetal. sufficientweb-basedinformationexistsforverify-
(2023a) develop an end-to-end multimodal fact- ingclaimswithhighconfidence. Conversely,the
checkingandexplanationgenerationpipelineThe NEI(NotEnoughInformation)labelcorrespondsto
authorsconstructamultimodaldatasetusingPoli- PolitiFact’sHalfTruedesignation,whichindicates
tiFact and Snopes, including the source text and a claim is partially accurate but omits significant
imagesoftheoriginalclaimsthatthesearticlesad- details or contexts. This study does not explore
dress. Themultimodalevidenceretrievalencodes anyenhancementsinLLMreasoningtounderstand
andrerankseachsentenceinthedocumentcorpus the context or the nature of claims more deeply.
in relation to the claim. A CLIP (Radford et al., Furthermore, the interpretation of NEI by LLMs,
2021)encodingisusedforimages,andthensimi- which may occur due to inadequate data for fact-
larityiscomputed. Anattentionmodelisusedfor checkingorunsuccessfulevidenceretrieval,differs
multimodalclaimverification,whileBART(Lewis significantlyfromPolitiFact’s"HalfTrue."
etal.,2020)isusedforexplanationgeneration.
4 MultimodalFact-CheckingPipeline
3 Dataset
Our fact-checking pipeline comprises 4 parts: (i)
WeusetheMOCHEGdataset(Yaoetal.,2023b) Multimodal Claim Generation, (ii) Multimodal
forourexperiments. MOCHEGprovidesasetof Evidence Retrieval, (iii) LLM-based and RAG-
21,184multimodalclaims. Theclaimsaresourced augmented Reasoning for Fact-checking, and fi-
fromtwofact-checkingwebsites, PolitiFact1 and nallythe(iv)VeracityPredictionandExplanation.
Snopes2. FollowingHanselowskietal.(2019),the ThepipelineisvisualizedinFigure2.
MOCHEG dataset simplifies the labels from the
twowebsitesinto: supported,refutedandNEI(Not 4.1 MultimodalClaimGeneration
EnoughInformation). Givenaninputclaimastext,anassociatedimage
MOCHEG provides a test set with 2,007 mul- andthedateoftheclaim,thetaskoftheclaimgen-
timodal claims. We further prune this set to only eration module is to generate a suitable response
includeclaimsthatwerefact-checkedbyPolitiFact. detailingboththeimageandtextualclaiminforma-
1https://www.politifact.com/ tion. WemakeuseofGPT-4V(OpenAI,2023)as
2https://www.snopes.com/ ourmultimodalLLMforthistask.Figure3: ChainofRAGandTreeofRAGpipeline
Thegeneratedresponseisdividedintotwosec- toremovethosesearchresultsthatoriginatefrom
tions;claimwhichsimplycontainstheoriginaltext “www.politifact.com”,“www.snopes.com”,and
claim and image context which contains the rele- “www.factcheck.org” since it is likely that they
vantdetailsextractedfromtheimagebyGPT-4V already contain answers to the claim and would
andusedtocontextualizethetextualclaiminlight thus impact the fairness of the experiment. We
oftheimageinformation. also remove the following social media web-
The image context is able to shed light on as- sites: “www.tiktok.com”, “www.facebook.com”,
pectssuchasthespeakerthattheclaimisquoting, “www.twitter.com”, “www.youtube.com” to fur-
extractnumericalinformationfromthefiguresand therimprovethequalityofretrievedresults.
highlight relevant textual data mentioned in the Mostoftheimagesinourdatasetcontainfaces
image. In addition to this, the contextualization ofpoliticians,picturesfromdebates/events,govern-
furtherprovidesdetailsintowhethertheimageis mentbuildingsetc. Insuchasituation,theimage
ofanyimportancetothetextclaimornot. itself may not provide much additional informa-
tion over the text claim. However, it is useful to
4.2 MultimodalEvidenceRetrieval
determinethemetadatabehindtheimagesuchas
The questions generated by the LLM-based or the date when the claim was made or the venue
RAG-augmented Reasoning techniques to fact- whereitwasmade. Forthispurpose,wemakeuse
checktheclaimserveasinputforthemultimodal ofSerpAPI4 toconductreverseimagesearchover
evidenceretrieval. Thetaskofevidenceretrievalis theassociatedimages. Weextractthecaptionsfor
toretrieverelevanttextsnippetsfromtheinternet theimagesfromthefirst10websitesanduseitas
and use the details associated with the image to additionalinformation.
helpanswerthequestion.
4.3 LLM-BasedandRAG-Augmented
Fortext-basedevidenceretrieval,wemakeuse
ReasoningforFact-Checking
of the DuckDuckGo Search tool provided by
LangChain3. We retrieve the top 10 results from 4.3.1 RAG-AugmentedReasoning: Chainof
theAPIanduseittoanswerthequestion. Wealso RAG(CoRAG)
restrictthetimeframeofsearchbyonlycollecting ChainofRAG(CoRAG)buildsupongeneralRAG
articlespublishedinthetwoyearswindowbefore approachesbyusingsequentialfollow-upquestions
theclaimwasfact-checkedbyPolitiFact. Tomimic augmentedfromtheRAGresponsetoretrievefur-
areal-timefact-checkingscenario,wemakesure ther evidence. An overview of the approach is
3https://www.langchain.com/ 4https://serpapi.com/Algorithm1ChainofRAG(CoRAG)
1: Input:ClaimC,ImageContextI,ImageCaptionsIC
2: Q←GenerateFirstQuestion(C,I)
3: QAPairs←[] ▷InitializeanemptylistforQ-Apairs
4: counter←0
5: followUpNeeded←True
6: whilecounter<6andfollowUpNeededdo
7: ifQuestionAboutImage(Q)then
8: A←ImageQA(Q,I,IC) ▷Usingimage,question,andcaptions
9: else
10: A←WebQA(Q) ▷Standardevidenceretrieval
11: endif
12: QAPairs.append((Q,A)) ▷StoretheQ-Apairofthisiteration
13: followUpNeeded←FollowupCheck(Q,A)
14: iffollowUpNeededthen
15: Q←FollowupQuestion(QAPairs)
16: endif
17: counter←counter+1
18: endwhile
19: returnQAPairs ▷ReturnsthelistofQ-Apairs
providedinAlgorithm1. TheinputtotheCoRAG step. Anoverviewoftheapproachisprovidedin
module is the claim and image context from the Algorithm2. TheinputtotheToRAGmoduleisthe
multimodalclaimgeneration. TheLLMfirstgen- claimandimagecontextfromthemultimodalclaim
eratesaquestionthatisintendedtoansweranas- generationmodule. Uponreceivingthisinput,the
pectoftheclaim. Wefollowazero-shotapproach ToRAG approach branches into three, with each
whereintheLLMisnotprovidedwithanyexample branchaskingauniquequestiontofact-checkthe
question/answerpairstoinfluenceorguidetherea- claim.
soningprocess. Thegeneratedquestionispassed
Oncethethreestartingquestionshavebeengen-
to the multimodal evidence retriever to reteieve
erated,theToRAGapproachmakesuseoftheevi-
evidencetoinformtheRAGanswer.
denceretrievertoretrieveandgenerateanswersfor
Oncetheanswerisgenerated,theCoRAGpro- these questions. The three question-answer pairs
cessundergoesanearlyterminationcheck,which are then passed into an elimination prompt from
wetermasfollow-upcheck. Thefollow-upcheck whichonlyonequestion-answerpairischosenas
takesasinputtheLLMgeneratedclaimaswellas thecandidateevidence. Thisselectionisbasedon
thegeneratedquestion-answerpair(s)andchecks the criteria of relevance, detail, additional infor-
whether enough information to answer the claim mation, and answer confidence as determined by
hasbeengathered. Iftheresponsefromthefollow- GPT-4(seeAppendixA.3).
upcheckis“Yes”,itasksafollow-upquestionthat
The candidate evidence is the best question-
isalreadyinformedofallthepreviouslyanswered
answer pair that was returned from the question
question-answerpairs.
answer elimination and now acts as the basis for
A follow-up check occurs after each question-
thefollow-upquestiontobeasked. Yetagain,three
answer generation step. If the follow-up check
follow-upquestionsaregeneratedsimultaneously
prompt finds sufficient evidence in the questions
based on the information from the candidate evi-
andanswersgeneratedbytheCoRAGprocess,it
dence. The evidence retriever fetches answers to
terminatesandpassestheevidencetotheveracity
thesequestions,andtheLLMgeneratestheanswer
prediction and explanation generation module of
basedontheevidenceretrieved. Anewcandidate’s
thepipeline. Wealsosetaconstraintofamaximum
evidenceischosenbytheeliminationpromptand
ofsixquestions,inwhichcasetheCoRAGprocess
isaddedtotheexistinglistofcandidateevidences.
terminatesevenifitdoesnothaveenoughevidence
Thecandidateevidencelist,therefore,storesonly
forthefact-check.
the best of the three question-answer pairs from
eachstep. Upongatheringsufficientinformationto
4.3.2 RAG-AugmentedReasoning: Treeof
fact-checktheclaimasdeterminedbythefollow-
RAG(ToRAG)
upcheckpromptorreachingamaximumofsixcan-
Tree of RAG (ToRAG) builds upon our CoRAG didateevidencequestion-answerpairs,theToRAG
approachbycreatingquestionbranchesateachstep process terminates, and the list of candidate evi-
ofthereasoning. Basedontheretrievedevidence, denceispassedontotheveracitypredictionmodule
thebestquestion-answerbranchisselectedateach togeneratetheveracitylabelandtheexplanation.Algorithm2TreeofRAG(ToRAG)
1: Input:ClaimC,ImageContextI,ImageCaptionsIC
2: BestQAPairs←[] ▷InitializeanemptylistforbestQ-Apairs
3: Questions←GenerateFirstQuestions(C,I) ▷Generatesthreequestions
4: counter←0
5: followUpNeeded←True
6: whilecounter<6andfollowUpNeededdo
7: QAPairs←[] ▷Initializesanemptylistforquestion-answerpairs
8: forQinQuestionsdo
9: ifQuestionAboutImage(Q)then
10: A←ImageQA(Q,I,IC) ▷Usingimage,question,andcaptions
11: else
12: A←WebQA(Q) ▷Standardevidenceretrieval
13: endif
14: QAPairs.append((Q,A))
15: endfor
16: (BestQ,BestA)←QAElimination(QAPairs)
17: BestQAPairs.append((BestQ,BestA)) ▷StoresthebestQ-Apairofthisiteration
18: followUpNeeded←FollowupCheck(BestQAPairs)
19: iffollowUpNeededthen
20: Questions←GenerateFollowupQuestions(BestQAPairs) ▷Generatesthreefollow-upquestions
21: else
22: break
23: endif
24: counter←counter+1
25: endwhile
26: returnBestQAPairs ▷ReturnsallcollectedbestQ-Apairs
4.4 VeracityPredictionandExplanation results from our default metric, which treats the
predictionasbinary,eithercorrectorincorrect. In
Thetaskoftheveracitypredictionandexplanation
Section5.2weevaluateexplanationgenerationby
(herebyreferredtosimplyasveracityprediction)
conductingahumanannotationstudytocompare
moduleistogenerateaveracitylabelofeithersup-
betweenthegeneratedexplanationandgoldexpla-
ported or refuted based on the information avail-
nation.
ableinthequestion-answerpairs. Itisalsoableto
generateafailed labelwhenitisdeemedtohave Weemployasub-questiongenerationfollowed
insufficientinformationinthequestion-answerpair by chain of thought veracity prediction approach
toeithersupportorrefutetheclaim. (SubQ+CoT VP)asourbaselinetocompareourRA-
Wemakeuseofthreevariantsofveracitypredic- GAR approaches. This baseline is based off of
tionprompts(seeAppendixA.4). Thestandardve- recent approaches in fact-checking using LLMs
racityprompt(Standard )simplytakestheclaim (Panetal.,2023;Chernetal.,2023). Weadaptthe
VP
andevidencepairsasinputandoutputstherating approachtohandlemultimodalclaimsaswell.
andtheexplanationwithoutanyinducedreasoning
5.1 DefaultMetric: F1ScoresonBinary
rationale. Thezeroshotchainofthoughtveracity
Prediction
prediction prompt (CoT ) makes use of a “Lets
VP
think step by step” phrase to guide the model to Thisevaluationischaracterizedbyabinarycorrect
follow chain of thought reasoning rationale. Fi- or incorrect label prediction for the claim. Table
nally, a Chain of Verification (Dhuliawala et al., 1 shows the results of all of our approaches for
2023) veracity prediction prompt (CoVe) is em- thisevaluationcriteria. SubQ+CoT isourworst-
VP
ployedwhichfirstconstructsverificationquestions performingapproachwithaweightedF1of0.71,
basedontheoriginalfact-checkexplanation. Using while Tree of RAG + CoT + CoVe is our best-
VP
RAG,theanswerstothesequestionsaregenerated. performingapproachwithaweightedF1of0.85.
Theanswersalongwiththeoriginalfact-checkare WithintheChainofRAGimplementations,Chain
passed to a correction check prompt. In case of ofRAG+CoT +CoVeperformsthebestwith
VP
corrections to the original fact-check, a new fact- a weighted F1 score of 0.81. We also notice that
checkisgeneratedalongwithanewveracitylabel thescoresforrefuted classpredictionsareconsis-
ifnecessary. tently better than the scores for supported class
predictions.
5 EvaluationandResults
SubQ+CoT performs the worst when com-
VP
Inthissection,weexplaintheresultsoftwoeval- pared to our RAGAR approaches. It lags behind
uation metrics we employ across the set of 300 the next best method, CoRAG by 0.08 F1 points
multimodal claims. In Section 5.1, we detail the and16correctpredictionsforthesupportedlabelAPPROACHES SUPPORTED(F1) REFUTED(F1) #FAILED WEIGHTEDF1
SubQ+CoT 0.66 0.77 50|22 0.71
VP
CoRAG+Standard 0.74 0.81 31|15 0.77
VP
CoRAG+CoT 0.73 0.82 38|14 0.77
VP
CoRAG+CoT +CoVe 0.78 0.83 21|8 0.81
VP
ToRAG+Standard 0.82 0.86 16|5 0.84
VP
ToRAG+CoT 0.82 0.85 19|9 0.83
VP
ToRAG+CoT +CoVe 0.84 0.86 9|4 0.85
VP
Table1: F1ResultsoftheDefaultMetricevaluation. The#FAILEDcolumncontainsthenumberofsupported|
refutedclaimsthatwerepredictedasfailedtogivethereaderanideaofhowmanyfailedpredictionscontributedto
beingconsideredincorrect.
Figure4: Numberof1/2/3ratingsreceivedforexplanationsbyeachapproach
andfallsbehindby0.06weightedF1points. This mationthatwasprovidedtotheCoRAGapproach.
poor performance is attributed to the CoT ap- Incaseofdiscrepanciesorerrors,itisabletocor-
VP
proach’s inability to gain sequential information rect those errors and output a new and improved
and its lack of context-building pertinent to fact- fact-checkthatcorrectspreviouslyincorrectlabels.
checkingtheclaim. Sincethesub-questionsgener- ToRAG-based approaches shine as the best-
atedbySubQ+CoT VParebasedsolelyontheclaim, performing approaches, incorporating a branch
theanswerstothesequestionsthatarequerieddur- question-answering architecture and using only
ing the evidence retrieval are not interconnected the best, most descriptive, and relevant question-
and neither do they follow through from one an- answerpairsateachstep. Wenoticethatasimple
other. As such, important sequential details, for ToRAG approach, even without CoVe, is already
example, identifying the speaker making a claim averycapableapproachtopredictingthegoldla-
andthenverifyingiftheclaimwasactuallysaidby belscorrectly. InasimilarveintoCoRAG,incor-
theidentifiedspeaker,inwhatcontextthespeaker porating CoT does not improve the prediction
VP
saidit,andwhetheritwasdeemedtobeaccurate, capability of the approach which we again base
etc. areaspectsthatSubQ+CoT VP overlooks. onGPT-4’sinternalreasoningbeingverycapable
as it is. Incorporating CoVe also does not see a
Incorporating CoT on the question-answer
VP
significantboosttoToRAGasitdidtoCoRAGin-
pairs generated by CoRAG did not show us any
dicatingtheQAeliminationpromptinToRAGhas
improvement. Weattributethistotheinternalrea-
alreadydoneagoodjobineliminatingerroneous
soning capabilities of GPT-4, which are the best
orirrelevantquestion-answerpairs.
in class amongst available LLMs. However, we
areabletoimprovetheperformancebycombining
5.2 EvaluatingExplanationGeneration
theCoVeapproachwiththeresultfromCoRAG+
CoT ,whichimprovesthemodel’sabilitytopre- We evaluate explanation generation by compar-
VP
dicttheveracitylabelcorrectlyforaconsiderable ingtheLLM-generatedfact-checkedexplanation
amountofsupported andrefuted cases. Thechain with the corresponding ruling outline from the
ofverificationapproachisabletoaskpointedques- MOCHEG dataset. Three voluntary annotators
tions that further test the credibility of the infor- aged21-24withaC1/C2Englishproficiencyarechosen for this annotation task. They are asked majorityoftheratingsbeingannotatedas3across
to rate the explanations generated by each of the thedifferentapproacheslendscredencetothequal-
approachesonascaleof1-3where3indicatesthat ity of the explanation and also to the efficacy of
allinformationinthegoldexplanationispresentin the underlying retrieval system in retrieving the
thegeneratedexplanationwhile1indicatesthatall relevantevidencetofact-checktheclaim.
information in the generated explanation is miss-
6 Conclusion
ingfromthepredictedexplanation. Thecomplete
annotationinstructionsareprovidedinAppendix
This paper introduces and validates two novel
A.11.
Large Language Models (LLMs) for multimodal
Werandomlysampleasetof50claimsdivided political fact-checking: Chain of RAG (CoRAG)
into 25 supported and 25 refuted. All the claims and Tree of RAG (ToRAG). These methods ad-
rated are those where the gold veracity label and dressmisinformationinpoliticaldiscoursethrough
the predicted veracity label match, i.e., the LLM text and images, showing superior accuracy over
correctly predicted the veracity of the claim as traditionalfact-checkingapproaches. CoRAGem-
supported/refuted. The Krippendorff’s α (Hayes ploysasequentialquestioningstrategyforthorough
andKrippendorff,2007)inter-annotatoragreement claimexploration,whileToRAGusesabranching
scoresareintherangeof0.53-0.75acorsstheap- followedbyevidenceeliminationforpreciseverac-
proacheswiththemeanbeing0.60whichwecon- ityprediction. Thestudyevaluatesthesemethods
sidersufficientagreementgiventhenatureofthe withrigorousexperimentsandahumanannotation
task. study,highlightingtheireffectivenessandpotential
AscanbeseeninFigure4,theannotatorspro- for future research in combating misinformation.
vide a rating of 3 for an overwhelming majority Ourworkcontributestothefieldbydemonstrating
of explanations generated across methods. This the viability of RAG-augmented reasoning (RA-
shows that these generated explanations did en- GAR) techniques in political fact-checking and
suretocoverallthepointsmentionedinthePoli- suggestsfurtheravenuesforresearchinevidence
tiFactfact-check. Inaddition,theannotatorsalso retrievalandextensiontoothermisinformationdo-
reported that the LLM-generated fact-check was mains.
moredetailedanddivulgedmoreinformationthan
7 Limitations
the ruling outline. We attribute this to the fact
thattherulingoutlineprovidedintheMOCHEG We experimented with three tools for extracting
datasetisasummaryofthePolitiFactarticleand relevantwebresultsfornaturallanguagequestions;
onlygoesoversomepointsoftheclaimforquick DuckDuckGo Search, You.com5 and Tavily AI6.
reading,whereastheLLM-generatedfact-checked Across the three tools, we notice that the search
was prompted to explain the reason regarding results vary when prompted with the same ques-
its veracity label. Additionally, it can be seen tionsmultipletimes. Thisvarianceinresults,even
thatSubQ+CoT VP hadsignificantlymore1-rated though the question remains the same or similar,
claimscomparedtoanyothermodels,whichindi- is problematic since it affects the final result and
catesthattheSubQ+CoT VP fact-checkmissedout makesithardtocompareapproachessinceoneap-
oncertainpointsordidnotelaborateonthepoint proachmayhavegottenthecorrectresponseand
accuratelytomotivatetheveracity. theotheronemayhavehadanincorrectresponse.
Further analysis indicates that there is also a Infact,thisiswheretheToRAGapproachbenefits
higher number of explanations in the supported the most since it can analyze answers for slight
cases that are rated as 2 as compared to the re- variationsinquestionsandpickthebestquestion-
futed case. Thisindicatesthatcertaininformation answerpairthatdivulgesthemostinformation.
wasmissingfromtheexplanationthatwasgener-
ated. When asked about this after the annotation
References
task,theannotatorsrespondedthatwhiletheLLM-
generated explanation did mention certain points FirojAlam,StefanoCresci,TanmoyChakraborty,Fab-
fromthePolitiFactrulingoutline,italsodetailed rizioSilvestri,DimiterDimitrov,GiovanniDaSan
thesameclaimwithothervaluablepointsthatwere 5https://you.com/
notmentionedbyPolitiFact. Overall,however,the 6https://tavily.com/Martino,ShadenShaar,HamedFirooz,andPreslav fornaturallanguagegeneration,translation,andcom-
Nakov.2022. Asurveyonmultimodaldisinforma- prehension. InProceedingsofthe58thAnnualMeet-
tion detection. In Proceedings of the 29th Inter- ingoftheAssociationforComputationalLinguistics,
nationalConferenceonComputationalLinguistics, pages7871–7880,Online.AssociationforComputa-
pages6625–6643,Gyeongju,RepublicofKorea.In- tionalLinguistics.
ternationalCommitteeonComputationalLinguistics.
OpenAI.2023. Gpt-4v: Amultimodaltransformerfor
AkariAsai,ZeqiuWu,YizhongWang,AvirupSil,and visionandlanguage.
HannanehHajishirzi.2024. Self-RAG:Learningto
retrieve,generate,andcritiquethroughself-reflection. LiangmingPan,XiaobaoWu,XinyuanLu,AnhTuan
InTheTwelfthInternationalConferenceonLearning Luu,WilliamYangWang,Min-YenKan,andPreslav
Representations. Nakov. 2023. Fact-checking complex claims with
program-guided reasoning. In Proceedings of the
EsmaAïmeur,SabrineAmri,andGillesBrassard.2023. 61stAnnualMeetingoftheAssociationforCompu-
Fake news, disinformation and misinformation in tationalLinguistics(Volume1: LongPapers),pages
socialmedia: areview. SocialNetworkAnalysisand 6981–7004,Toronto,Canada.AssociationforCom-
Mining,13(1):30. putationalLinguistics.
I-ChunChern,SteffiChern,ShiqiChen,WeizheYuan, BaolinPeng,MichelGalley,PengchengHe,HaoCheng,
KehuaFeng,ChuntingZhou,JunxianHe,Graham YujiaXie,YuHu,QiuyuanHuang,LarsLiden,Zhou
Neubig, and Pengfei Liu. 2023. FacTool: Factual- Yu,WeizhuChen,andJianfengGao.2023. Check
ity detection in generative AI – a tool augmented yourfactsandtryagain: Improvinglargelanguage
frameworkformulti-taskandmulti-domainscenar- modelswithexternalknowledgeandautomatedfeed-
ios. arXivpreprintarXiv:2307.13528. back. arXivpreprintarXiv:2302.12813.
AnubrataDas,HoujiangLiu,VenelinKovatchev,and AlecRadford,JongWookKim,ChrisHallacy,Aditya
MatthewLease.2023. Thestateofhuman-centered Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
NLPtechnologyforfact-checking. InformationPro- try, Amanda Askell, Pamela Mishkin, Jack Clark,
cessing&Management,60(2):103219. GretchenKrueger,andIlyaSutskever.2021. Learn-
ingtransferablevisualmodelsfromnaturallanguage
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and supervision. InProceedingsofthe38thInternational
KristinaToutanova.2018. Bert: Pre-trainingofdeep Conference on Machine Learning, volume 139 of
bidirectionaltransformersforlanguageunderstand- ProceedingsofMachineLearningResearch,pages
ing. arXivpreprintarXiv:1810.04805. 8748–8763.PMLR.
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018.
RobertaRaileanu,XianLi,AsliCelikyilmaz,andJa- The spread of true and false news online. Science,
sonWeston.2023. Chain-of-verificationreduceshal- 359(6380):1146–1151.
lucinationinlargelanguagemodels. arXivpreprint
arXiv:2309.11495. Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-
HeeCho,andLifuHuang.2023a. End-to-endmulti-
ZixianGuo,BowenDong,ZhilongJi,JinfengBai,Yi- modalfact-checkingandexplanationgeneration: A
wenGuo, andWangmengZuo.2023. Textsasim- challengingdatasetandmodels. InProceedingsof
agesinprompttuningformulti-labelimagerecogni- the 46th International ACM SIGIR Conference on
tion. InProceedingsoftheIEEE/CVFConference ResearchandDevelopmentinInformationRetrieval.
onComputerVisionandPatternRecognition,pages ACM.
2808–2817.
Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-
AndreasHanselowski,ChristianStab,ClaudiaSchulz, HeeCho,andLifuHuang.2023b. End-to-endmulti-
ZileLi, andIrynaGurevych.2019. Arichlyanno- modalfact-checkingandexplanationgeneration: A
tated corpus for different tasks in automated fact- challengingdatasetandmodels. InProceedingsof
checking. InProceedingsofthe23rdConferenceon the 46th International ACM SIGIR Conference on
ComputationalNaturalLanguageLearning(CoNLL), ResearchandDevelopmentinInformationRetrieval,
pages493–503,HongKong,China.Associationfor SIGIR’23,page2733–2743,NewYork,NY,USA.
ComputationalLinguistics. AssociationforComputingMachinery.
AndrewF.HayesandKlausKrippendorff.2007. An- Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
sweringthecallforastandardreliabilitymeasurefor Thomas L. Griffiths, Yuan Cao, and Karthik
codingdata. CommunicationMethodsandMeasures, Narasimhan. 2023c. Tree of thoughts: Deliberate
1:77–89. problemsolvingwithlargelanguagemodels. arXiv
preprintarXiv:2305.10601.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad,AbdelrahmanMohamed,OmerLevy, SavvasZannettou,TristanCaulfield,JeremyBlackburn,
Veselin Stoyanov, and Luke Zettlemoyer. 2020. Emiliano De Cristofaro, Michael Sirivianos, Gian-
BART:Denoisingsequence-to-sequencepre-training lucaStringhini,andGuillermoSuarez-Tangil.2018.On the origins of memes by means of fringe web
communities. In Proceedings of the Internet Mea-
surementConference2018,IMC’18,page188–202,
New York, NY, USA. Association for Computing
Machinery.
Fengzhu Zeng and Wei Gao. 2024. Justilm: Few-
shot justification generation for explainable fact-
checking of real-world claims. arXiv preprint
arXiv:2401.08026.
XuanZhangandWeiGao.2023. Towardsllm-based
fact verification on news claims with a hierarchi-
calstep-by-steppromptingmethod. arXivpreprint
arXiv:2310.00305.
A Appendix
A.1 InstructionstoAnnotators
Figure5: AnnotationInstructionsA.2 GeneralPromptsintheRAGAR
Approaches
Figure6:Promptforinitialquestion-generation,Follow-
upCheckandFollow-upQuestioncommontoallRA-
GARapproaches
A.3 PromptsSpecifictoTreeofRAG
Figure7: PromptforQAEliminationA.4 PromptsforVeracityPrediction
A.4.1 StandardVeracityPredictionPrompt
Figure8: PromptforStandardVeracityprediction
A.4.2 ZeroShotChainofThoughtVeracity
Prediction
Figure 9: Prompt to get the CoT Veracity Prediction
fromthequestion-answerpairsandtheclaim
A.4.3 ChainofVerificationVeracity
Prediction
Figure10: PipelineoftheCoVeVeracityPredictionFigure11: CoVeVerificationQuestionsprompt
Figure12: CoVeCorrectionsPrompt