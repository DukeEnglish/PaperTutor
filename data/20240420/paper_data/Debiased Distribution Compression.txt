Debiased Distribution Compression
LingxiaoLi1 RaazDwivedi2 LesterMackey3
Abstract Remarkably,moderncompressionmethodscansummarize
adistributionmoresuccinctlythani.i.d.sampling. Forex-
Modern compression methods can summarize a
ample, kernelthinning(KT)(DwivediandMackey,2021;
target distribution P more succinctly than i.i.d.
2022), Compress++ (Shetty et al., 2022), recombination
sampling but require access to a low-bias input
(Hayakawa et al., 2023), and randomly pivoted Cholesky
sequencelikeaMarkovchainconvergingquickly
(Epperly and Moreno, 2024) all provide O(cid:101)(1/m) approx-
to P. We introduce a new suite of compression
imation error using m points, a significant improvement
methods suitable for compression with biased
over the Ω(1/√m) approximation provided by i.i.d. sam-
input sequences. Given n points targeting the pling from P. However, each of these constructions relies
wrongdistributionandquadratictime,SteinKer-
onaccesstoanaccurateinputsequence,likeani.i.d.sam-
nel Thinning (SKT) returns √n equal-weighted plefromPoraMarkovchainconvergingquicklytoP.
points with O(cid:101)(n−1/2) maximum mean discrep-
ancy(MMD)toP. Forlarger-scalecompression Much more commonly, one only has access to n biased
tasks, Low-rank SKT achieves the same feat in samplepointsapproximatingawrongdistributionQ. Such
sub-quadratic time using an adaptive low-rank biases are a common occurrence in Markov chain Monte
debiasingprocedurethatmaybeofindependent Carlo (MCMC)-based inference due to tempering (where
interest. Fordownstreamtasksthatsupportsim- onetargetsalesspeakedandmoredisperseddistributionto
plex or constant-preserving weights, Stein Re- achievefasterconvergence,Gramacyetal.,2010),burn-in
combination and Stein Cholesky achieve even (where the initial state of a Markov chain biases the dis-
greater parsimony, matching the guarantees of tribution of chain iterates, Cowles and Carlin, 1996), or
SKTwithasfewaspoly-log(n)weightedpoints. approximate MCMC (where one runs a cheaper approxi-
Underlying these advances are new guarantees mateMarkovchaintoavoidtheprohibitivecostsofanex-
forthequalityofsimplex-weightedcoresets,the act MCMC algorithm, e.g., Ahn et al., 2012). The Stein
spectral decay of kernel matrices, and the cov- thinning (ST) method of Riabiz et al. (2022) was devel-
ering numbers of Stein kernel Hilbert spaces. opedtoprovideaccuratecompressionevenwhentheinput
In our experiments, our techniques provide suc- samplesequenceprovidesapoorapproximationtothetar-
cinct and accurate posterior summaries while get. ST operates by greedily thinning the input sample to
overcoming biases due to burn-in, approximate minimizethemaximummeandiscrepancy(MMD,Gretton
MarkovchainMonteCarlo,andtempering.
etal.,2012)toP.However,STisonlyknowntoprovidean
O(1/√m)approximationtoP; thisguaranteeisnobetter
thanthatofi.i.d.samplingandafarcryfromtheO(cid:101)(1/m)
1.Introduction errorachievedwithunbiasedcoresetconstructions.
Inthiswork,weaddressthisdeficitbydevelopingnew,ef-
Distribution compression is the problem of summarizing
ficientcoresetconstructionsthatprovablyyieldbetter-than-
a target probability distribution P with a small set of rep-
i.i.d.errorevenwhentheinputsampleisbiased. ForPon
resentative points. Such compact summaries are particu-
Rd,ourprimarycontributionsarefourfoldandsummarized
larly valuable for tasks that incur substantial downstream
in Tab. 1. First, for the task of equal-weighted compres-
computationcostspersummarypoint,likeorganandtissue
sion, we introduce Stein Kernel Thinning (SKT, Alg. 1),
modelinginwhicheachsimulationconsumesthousandsof
a strategy that combines the greedy bias correction prop-
CPUhours(Niedereretal.,2011).
erties of ST with the unbiased compression of KT to pro-
1MIT CSAIL 2Cornell Tech 3Microsoft Research New duce √n summary points with error O(cid:101)(n−1/2) in O(n2)
England.Correspondenceto:LingxiaoLi<lingxiao@mit.edu>, time. Incontrast,STwouldrequireΩ(n)pointstoguaran-
Raaz Dwivedi <dwivedi@cornell.edu>, Lester Mackey tee this error. Second, for larger-scale compression prob-
<lmackey@microsoft.com>.
lems, we propose Low-rank SKT (Alg. 3), a strategy that
combinesthescalablesummarizationofCompress++with
Preliminarywork.Underreview.
1
4202
rpA
81
]LM.tats[
1v09221.4042:viXraDebiasedDistributionCompression
Table1: Methodsfordebiaseddistributioncompression. Foreachmethod, wereportthesmallestcoresetsizemand
runningtime, uptologarithmicfactors, sufficienttoguaranteeO(cid:101)(n−1/2)MMD
kP
toPgivena LOGGROWTH kernelkP
andnslow-growinginputpoints = (x )n fromafast-mixingMarkovchaintargetingQwithtailsnolighterthanP
Sn i i=1
(seeThm.1andDef.3). Forgenericslow-growing ,identicalguaranteesholdforexcessMMD (2)relativetothebest
Sn kP
simplexreweightingof .
n
S
Method CompressionType CoresetSizem Runtime Source
SteinThinning(Riabizetal.,2022) equal-weighted n d kPn2 App.D.1
(cid:40)
Greedy (Alg.1) d n2 Thm.3
SteinKernelThinning equal-weighted √n kP
Low-rank (Alg.3) d n1.5 Thm.5
kP
(cid:40)
Greedy d n2
SteinRecombination (Alg.5) simplex-weighted poly-log(n) kP Thm.6
Low-rank d n+n1.5
kP
(cid:40)
Greedy d n2
SteinCholesky (Alg.7) constant-preserving poly-log(n) kP Thm.7
Low-rank d n+n1.5
kP
anewlow-rankdebiasingprocedure(Alg.2)tomatchthe ingthisnotation,weviewallalgorithmparametersexcept
SKTguaranteesinsub-quadratico(n2)time. δ as functions of n. For A Rn×n and v Rn, diag(A)
∈ ∈
and diag(v) are n n diagonal matrices with A and v
Third, for the task of simplex-weighted compression, in × ii i
respectivelyasthei-thdiagonalentry.
whichsummarypointsareaccompaniedbyweightsinthe
simplex, we propose greedy and low-rank Stein Recombi-
nation(Alg.5)constructionsthatmatchtheguaranteesof 2.DebiasedDistributionCompression
SKTwithasfewaspoly-log(n)points.Finally,forthetask
Throughout, we aim to summarize a fixed target distribu-
of constant-preserving compression, in which summary
tionPonRdusingasequence ≜(x )n ofpotentially
points are accompanied by real-valued weights summing biasedcandidatepointsinRd.1S Cn orrectii ngi= fo1
runknownbi-
to 1, we introduce greedy and low-rank Stein Cholesky
asesin requiressomeauxiliaryknowledgeofP. Forus,
(Alg. 7) constructions that again match the guarantees of Sn
thisknowledgecomesintheformofakernelfunctionkP
SKTusingasfewaspoly-log(n)points.
withknownexpectationunderP. Withoutlossofgeneral-
Underlyingtheseadvancesarenewguaranteesforthequal- ity,wecantakethiskernelmeantobeidenticallyzero.2
ityofsimplex-weightedcoresets(Thms.1and2),thespec-
Assumption 1 (Mean-zero kernel). For some p 1/2,
tral decay of kernel matrices (Cor. B.1), and the covering E x∼P[kP(x,x)p]< andPkP 0. ≥
numbersofSteinkernelHilbertspaces(Prop.1)thatmay ∞ ≡
be of independent interest. In Sec. 5, we employ our new Givenatargetcompressionsizem,ourgoalistooutputan
procedurestoproducecompactsummariesofcomplextar- weight vector w Rn with w m, 1⊤w = 1, and
get distributions given input points biased by burn-in, ap- o(m−1/2) (better-∈ than-i.i.d.) m∥ ax∥ i0 m≤ um mean n discrepancy
proximateMCMC,ortempering. (MMD)toP:
Notation WeassumeBorel-measurablesetsandfunctions (cid:113)
and define [n] ≜ 1,...,n , ∆
n−1
≜ w Rn : w MMD kP((cid:80)n i=1w iδ xi,P)≜ (cid:80)n i,j=1w iw jkP(x i,x j).
0,1⊤w =1 , x {≜ i:x} =0 ,and{ x ∈ p ≜(cid:80) x≥ p
} ∥ ∥0 |{ i ̸ }| ∥ ∥p i| i |
for x Rd and p 1. For x Rd, δ denotes the delta We consider three standard compression tasks with
x
m Hie la bs eu r∈ r te spa at cx e. (RW Ke H≥ Sle )t oH fak kd ee rnn eo∈ lte kt :h Re drepr Ro dducin Rg (k Ae rr on ne -l ∥ mw ∥ p0 oss≤ iblm y. repIn eae teq dua pl- ow ine ti sgh ft re od mcom np ar nes dsi ao sn sigo nn se ese al ce hct as
s F wz o ra r ij tn ea, m µ19 fe5 a0 ≜su) ra (cid:82)end fµ (∥ a xf n )∥ d dk µse (d xpe a )n ro a at t ne e dlt yh µe µ kR - (iK n xt )H egS ≜r× an b (cid:82)o lr e km k (→ xo a ,f n yf d )d∈ f µ,H (w yk )e.
.
w o cove mei rg ph
S
rt eno sssf ia om t1 nis; fi wb ee esc wa au lls
∈
oe w∆of anr n−e yp 1e w∩ats (, N mt 0h ∆)eS n ni .n −Id 1nu ,c s ae im nd dpw l iee nxig - cwh ot e niv sge th ac t nt eo td -r
∈
The divergence of a differentiable matrix-valued function preservingcompressionwesimplyenforce1⊤w =1.
n
(cid:80)
Ais( A(x)) = ∂ A (x). Forrandomvariables
∇x · j i xi ij 1Our coreset constructions will in fact apply to any sample
(X n) n∈N,wesayX n=O(f(n,δ))holdswithprobability space,butouranalysiswillfocusonRd.
≥
1 −δ if Pr(X n ≤Cf(n,δ))
≥
1 −δ for a constant C 2ForPkP ̸≡ 0,thekernelkP′(x,y) = kP(x,y)−PkP(x)−
independentof(n,δ)andallnsufficientlylarge. Whenus- PkP(y)+PPkPsatisfiesPkP′ ≡0andMMD kP′ =MMD kP.
2DebiasedDistributionCompression
fiW xh oe fn am na ik ni fing nib teig sO eqs ut ea nte cm een ∞ts, ≜we (w
x
iil )l i∈tr Ne .at WSn ea as lst ohe wp rr ie te- (b) sP -O tiL mY eG scR oO nW tiT nuH o( us− sd l1 y, d( i1 ff+ ered s nℓ t) id a) bli eft (h De eb f.a Bse .2k )e fr on re sl >k 1is
.
kP( n[J], n[J]) ≜ [kP(x i,S x j)]
i,j∈J
fortheprincipalker-
S S
nelsubmatrixwithindicesJ [n]. Notably, the popular Gaussian (Ex. B.1) and inverse
⊆
multiquadric (Ex. B.2) base kernels satisfy the LOG-
2.1.Kernelassumptions GROWTH preconditions, while Mate´rn, B-spline, sinc,
sech,andWendland’scompactlysupportedkernelssatisfy
ManypracticalSteinkernelconstructionsareavailablefor
generatingmean-zerokernelsforatargetP(Chwialkowski
the POLYGROWTH precondition (Dwivedi and Mackey,
2022, Prop. 3). To our knowledge, Prop. 1 provides the
et al., 2016; Liu et al., 2016; Gorham and Mackey, 2017;
first covering number bounds and eigenvalue decay rates
Gorham et al., 2019; Barp et al., 2019; Yang et al., 2018;
forthe(typicallyunbounded)Steinkernelsk .
Afzali and Muthukumarana, 2023). We will use the most p
prominentoftheseSteinkernelsasarunningexample:
2.2.Inputpointdesiderata
Definition 1 (Stein kernel). Given a differentiable base
kernel k and a symmetric positive semidefinite matrix M, Our primary desideratum for the input points is that they
the Stein kernel k : Rd Rd R for P with positive canbedebiasedintoanaccurateestimateofP. Indeed,our
p
differentiableLebesgueden× sityp→ isdefinedas high-levelstrategyfordebiasedcompressionistofirstuse
kP todebiastheinputpointsintoamoreaccurateapprox-
k (x,y)≜ 1 (p(x)Mk(x,y)p(y)). imationof P andthen compressthatapproximationinto a
p p(x)p(y)∇x ·∇y
·
more succinct representation. Fortunately, even when the
input targets a distribution Q = P, effective debiasing
While our algorithms apply to any mean zero kernel, our Sn ̸
isoftenachievableviasimplexreweighting,i.e.,bysolving
guaranteesadapt totheunderlying smoothnessof theker-
theconvexoptimizationproblem
nels.Ournextdefinitionandassumptionmakethisprecise.
aD R ne d dfi →n εit >Rio 0n w ,i2 tt hh( eBC cko ov v≜e er ri
{
in nfg g∈n nu uHm mkb be e:r r∥). f ∥F ko (r A≤a ,ε1k )}e , ir san te hsl e etk mA i: n⊂R imd R u× md, ww iO tP hT M∈ Mar Dg Om Pi Tn w ≜∈ M∆n M−1 D(cid:80) kPn i (,j (cid:80)=1
n
i=w 1i ww Oj Pk TP i( δx xi i, ,x Pj ))
.
(1)
k
N
cardinalityofallsets k satisfying For example, Hodgkinson et al. (2020, Thm. 1b) showed
C ⊂B
thatsimplexreweightingcancorrectforbiasesduetooff-
(cid:83)
g :sup h(x) g(x) ε .
Bk ⊂ h∈C{ ∈Bk x∈A| − |≤ } targeti.i.d.orMCMCsampling. Ournextresult(provedin
App.C.2)significantlyrelaxestheirconditions.
Assumption (α,β)-kernel. For some C > 0, all r > 0
d
and ε (0,1), and (r) ≜ x Rd : x r , a Theorem 1 (Debiasing via simplex reweighting). Con-
∈ B2 { ∈ ∥ ∥2 ≤ }
kernelkiseitherPOLYGROWTH(α,β),i.e., siderakernelkP satisfyingAssum.1with HkP separable,
andsuppose(x )∞ aretheiteratesofahomogeneousϕ-
i i=1
log k( 2(r),ε) C d(1/ε)α(r+1)β, irreduciblegeometricallyergodicMarkovchain(Gallegos-
N B ≤
Herradaetal.,2023,Thm.1)withstationarydistributionQ
withα<2orLOGGROWTH(α,β),i.e.,
and initial distribution absolutely continuous with respect
log k( 2(r),ε) C dlog(e/ε)α (r+1)β.
toP. IfE x∼P[ dd QP (x)2q−1kP(x,x)q] < ∞forsomeq > 1
N B ≤ thenMMD =O(n−1/2)inprobability.
OPT
InCor.B.1weshowthattheeigenvaluesofkernelmatrices
Remark 1.
HkP
is separable whenever kP is continuous
(SteinwartandChristmann,2008,Lem.4.33).
withPOLYGROWTHandLOGGROWTHkernelshavepoly-
nomial and exponential decay respectively. Dwivedi and
Since n points sampled i.i.d. from P have Θ(n−1/2) root
Mackey(2022,Prop.2)showedthatallsufficientlydiffer-
mean squared MMD (see Prop. C.1), Thm. 1 shows that
entiable kernels satisfy the POLYGROWTH condition and
a debiased off-target sample can be as accurate as a di-
that bounded radially analytic kernels are LOGGROWTH.
rect sample from P. Moreover, Thm. 1 applies to many
Ournextresult,provedinApp.B.2,showsthataSteinker-
practical examples. The simplest example of a geomet-
nel k can inherit the growth properties of its base kernel
p rically ergodic chain is i.i.d. sampling from Q, but ge-
evenifk isitselfunboundedandnon-smooth.
p ometric ergodicity has also been established for a va-
Proposition 1 (Stein kernel growth rates). A Stein kernel riety of popular Markov chains including random walk
k pwithsup ∥x∥2≤r∥∇logp(x) ∥2 =O(rdℓ)ford ℓ ≥0is Metropolis (Roberts and Tweedie, 1996, Thm. 3.2), inde-
pendent Metropolis-Hastings (Atchade´ and Perron, 2007,
(a) LOGGROWTH(d+1,2d+δ)foranyδ >0ifthebase Thm. 2.2), the unadjusted Langevin algorithm (Durmus
kernelkisradiallyanalytic(Def.B.3)and and Moulines, 2017, Prop. 8), the Metropolis-adjusted
3DebiasedDistributionCompression
Langevinalgorithm(DurmusandMoulines,2022,Thm.1), time by generating weights with O(n−1/2 kP n) excess
∥ ∥
Hamiltonian Monte Carlo (Durmus et al., 2020, Thm. 10 MMD (Liu and Lee, 2017). We instead employ a more
and Thm. 11), stochastic gradient Langevin dynamics (Li efficient,greedydebiasingstrategybasedonSteinthinning
et al., 2023, Thm. 2.1), and the Gibbs sampler (Johnson, (ST).Afternrounds,SToutputsanequal-weightedcoreset
2009). Moreover,forQabsolutelycontinuouswithrespect ofsizenwithO(n−1/2 kP n)excessMMD(Riabizetal.,
to P, the importance weight dP is typically bounded or 2022, Thm. 1). Moreo∥ ver,∥ while the original implemen-
dQ
slowly growing when the tails of Q are not much lighter tationofRiabizetal.(2022)hascubicruntime,ourimple-
thanthoseofP. mentation(Alg.D.1)basedonsufficientstatisticsimproves
theruntimetoO(n2d )whered denotestheruntimeof
Remarkably, under more stringent conditions, Thm. 2 kP kP
asinglekernelevaluation.3
(proved in App. C.3) shows that simplex reweighting can
decreaseMMDtoPataneven-faster-than-i.i.d.rate. Theequal-weightedoutputofSTservesastheperfectinput
Theorem 2 (Better-than-i.i.d. debiasing via simplex for the kernel thinning (KT) algorithm which compresses
reweighting). Consider a kernel kP satisfying Assum. 1 an equal-weighted sample of size n into a coreset of any
sbw oui mt th io enp qQ= >w 32 ,ita th hn ed dd nQPp Eo b [i Mn ot us Mn( d Dx ei 2d) .∞ i= ]I1 f =d Er o[a k (w nPn ( −x 1i 1. )i , ..d x. 1)fr qo ]m <a ∞dist fr oi r- tt g oa er tg ine K ct lT usi dz a ee lg aom bri at≤ h sem lin ns eli in g Sh TO tl c( y on rt2 o ed sk etaP tr) og feti t sm iM ze e. M mW D ie nea r trd hoa erp Kt to Tt -h P Se Wat Aa nr d P-
OPT step(seeAlg.D.3). Combiningthetworoutinesweobtain
TheworkofLiuandLee(2017,Thm.3.3)alsoestablished Stein Kernel Thinning (SKT), our first solution for equal-
o(n−1/2)MMDerrorforsimplexreweightingbutonlyun- weighteddebiaseddistributioncompression:
derauniformlyboundedeigenfunctionsassumptionthatis
oftenviolated(Minh,2010,Thm.1,Zhou,2002,Ex.1)and Algorithm1SteinKernelThinning(SKT)
difficulttoverify(SteinwartandScovel,2012). Input:mean-zerokernelkP,points n,outputsizem,KT
S
failureprobabilityδ
Ourremainingresultsmakenoparticularassumptionabout
theinputpointsbutratherupperboundtheexcessMMD
n′ m2⌈log
2
mn⌉
←
w SteinThinning(kP, n,n′)
∆MMD kP(w)≜ MM MM DD Ok PP T((cid:80) i∈[n]w iδ xi,P) (2) w RS e← K tuT
r←
n:K we Sr Kn Te ∈lT ∆hi nn −ni 1n ∩gS ( (k
N
mP 0,
)S
nn,n′ ▷,w he, nm ce,δ ∥)
w
SKT ∥0
≤m
−
of a candidate weighting w in terms of the input point ra-
diusR
n
≜max
i∈[n]
∥x
i
∥2∨1andkernelradius ∥kP
∥n
≜ O beu tr ten r-e tx ht ar ne -s iu .il .t d, .p er xo cv ee sd si MnA Mp Dp. wD h.3 en, esh vo erw ts ht eh ra at dS iK i(T Ryie al nd ds
max i∈[n]kP(x i,x i). Whiletheseresultsapplytoanyinput n
points,wewillconsiderthefollowingrunningexampleof
∥kP ∥n)andkernelcoveringnumberexhibitslowgrowth.
slow-growinginputpointsthroughoutthepaper. Theorem3(MMDguaranteeforSKT). GivenakernelkP
Definition3(Slow-growinginputpoints). Wesay isγ- satisfyingAssums.1and(α,β)-kernel,SteinKernelThin-
n
slow-growing if R
n
= O((logn)γ) for some γ S 0 and ning(Alg.1)outputsw SKTinO(n2d kP)timesatisfying
≥
∥kP
∥n
=O(cid:101)(1).
∆MMD kP(w SKT)=O(cid:0)√∥kP∥ mn inℓδ (· mlo ,g √n n· )RnβGα m(cid:1)
Notably, is 1-slow-growing with probability 1 when
n
S
pk oP i( nx t, sx a) reis dp rao wly nno fm roi mally
a
b ho ou mn od ge ed neb oy u∥ sx ϕ∥ -2 ira ren dd ut ch ibe leinp gu e-t withprobabilityatleast1 −δ,whereℓ δ ≜log2( δe)and
ometrically ergodic Markov chain with a sub-exponential (cid:40)
ata nr dge Mt aQ c, kei. ye ,., 20E 2[e 1c ,∥ Px r∥ o2 p] .<
2).∞
Forfo ar Ss to eim ne kec rn> elk0 (D (Dw ei fv .e 1d )i
,
G m≜ mlog(em) PL OO LG YG GR RO OW WT TH H(α (α, ,β β),
).
p
by Prop. B.3, k (x,x) is polynomially bounded by x
p ∥ ∥2 Example1. UndertheassumptionsofThm.3withγ-slow-
if k(x,x), k(x,x) , and logp(x) are all
polynomiall∥ y∇ box u∇ ny
dedby
∥ x2
.
Mo∥ re∇
over,
l∥ o2
gp(x)
growing input points (Def. 3), LOGGROWTH kP, and a
∥ ∥2 ∥∇ ∥2 coresetsizem √n,SKTdeliversO(cid:101)(m−1)excessMMD
is automatically polynomially bounded by ∥x ∥2 when with high prob≤ ability, a significant improvement over the
logp is Lipschitz or, more generally, pseudo-Lipschitz
∇ Ω(m−1/2)errorrateofi.i.d.sampling.
(Erdogduetal.,2018,Eq.(2.5)).
Remark2. Whenm < √n,wecanuniformlysubsample
2.3.DebiasedcompressionviaSteinKernelThinning or, in the case of MCMC inputs, standard thin (i.e., keep
onlyevery n -thpointof)theinputsequencedowntosize
Off-the-shelf solvers based on mirror descent and Frank
m2
Wolfecansolvetheconvexdebiasingprogram(1)inO(n3) 3Often,d =Θ(d)asinthecaseofSteinkernels(App.I.1).
kP
4DebiasedDistributionCompression
m2 beforerunningSKTtoreduceruntimewhileincurring Algorithm2Low-rankDebiasing(LD)
ao ln gl oy riO th( mm− in1 tr) oe dx uc ce es ds ie nrr So er. c.T 3h .esameholdsfortheLSKT Input: mean-zerokernelkP,points
Sn
=(x i)n i=1,rankr,
AMDstepsT,adaptiveroundsQ
w(0) (1,..., 1) Rn
← n n ∈
3.AcceleratedDebiasedCompression forq =1toQdo
w˜ Resample(w(q−1),n)
To enable larger-scale debiased compression, we next in- ←
I,F WeightedRPCholesky(kP, n,w˜,r)
troduceasub-quadratic-timeversionofSKTbuiltviaanew K′ ← FF⊤+diag(kP( n, n)) S diag(FF⊤)
low-rank debiasing scheme and the near-linear-time com- w(q)← AMD(K′,T,w˜,AS GGS =1−
)
q>1
pressionalgorithmofShettyetal.(2022). if(w← (q))⊤K′w(q) >w˜⊤K′w˜thenw(q) w˜
←
endfor
3.1.Fastbiascorrectionvialow-rankapproximation Return: w w(Q) ∆
LD n−1
← ∈
At a high level, our approach to accelerated debiasing in-
volves four components. First, we form a rank-r approx-
imation FF⊤ of the kernel matrix K = kP( n, n) in Theorem 4 (Debiasing guarantee for LD). Under As-
O(nrd +nr2)timeusingaweightedextensionS (WeS ighte- sum. (α,β)-params, Low-rank Debiasing (Alg. 2) takes
kP
dRPCholesky,Alg.F.1)oftherandomlypivotedCholesky O((d kP+r+T)nr)timetooutputw LDsatisfying
algorithmofChenetal.(2022,Alg.2.1). Second,wecor-
(cid:18)(cid:113) (cid:113) (cid:19)
rectthediagonaltoformK′ =FF⊤+diag(K FF⊤). ∆MMD (w )=O ∥kP∥nmax(logn,1/δ) + nHn,r
Third,wesolvethereweightingproblem(1)with− K′ sub- kP LD n δ
stituted for K using T iterations of accelerated entropic
mirror descent (AMD, Wang et al., 2023, Alg. 14 with
withprobabilityatleast1 δ,foranyδ (0,1)andH
n,r
− ∈
ϕ(w)=(cid:80) w logw ). TheaccelerationensuresO(1/T2) definedin(46)thatsatisfies
i i i
suboptimality after T iterations, and each iteration takes

onlyO(nr)timethankstothelow-rankplusdiagonalap- O(cid:0) √r(R n2β )α1(cid:1) POLYGROWTH(α,β),
proximation. Finally, we repeat this three-step procedure H n,r= O(cid:0) √rexr
p(
(cid:0)0.83√ r−2.39(cid:1) α1 )(cid:1)
LOGGROWTH(α,β).
Qtimes,eachtimeusingtheweightsoutputtedbytheprior − CdRnβ
round to update the low-rank approximation K(cid:98). On these
Example2. UndertheassumptionsofThm.4withγ-slow-
subsequentadaptiverounds,WeightedRPCholeskyapprox-
imates the leading subspace of a weighted kernel matrix
growing input points (Def. 3), LOGGROWTH kP, T =
Θ(√n), and r = (logn)2(α+βγ)+ϵ for any ϵ > 0, LD
diag(√w˜)Kdiag(√w˜) before undoing the row and col-
umnreweighting. Sinceeachround’sweightsarecloserto
delivers O(cid:101)(n−1/2) excess MMD with high probability in
optimal,thisadaptiveupdatinghastheeffectofupweight-
O(cid:101)(n1.5)time.
ingmorerelevantsubspacesforsubsequentdebiasing. For
added sparsity, we prune the weights outputted by the 3.2.FastdebiasedcompressionviaLow-rankSteinKT
prior round using stratified residual resampling (Resam-
To achieve debiased compression in sub-quadratic time,
ple,Alg.E.3,DoucandCappe´,2005). OurcompleteLow-
we next propose Low-rank SKT (Alg. 3). LSKT debi-
rankDebiasing(LD)scheme,summarizedinAlg.2,enjoys
ases the input using LD, converts the LD output into an
o(n2)runtimewheneverr = o(n1/2), T = O(n1/2), and
equal-weighted coreset using Resample, and finally com-
Q=O(1).
binesKTwiththedivide-and-conquerCompress++frame-
Moreover, our next result, proved in App. F.1, shows that work (Shetty et al., 2022) to compress n equal-weighted
LDprovidesi.i.d.-levelprecisionwheneverT √n,Q= pointsinto√ninnear-lineartime.
≥
O(1),andr growsappropriatelywiththeinputradiusand
kernelcoveringnumber. Algorithm3Low-rankSteinKernelThinning(LSKT)
Assumption (α,β)-params. The kernel kP satisfies As- Input: mean-zerokernelkP,points Sn =(x i)n i=1,rankr,
sums.1and(α,β)-kernel,theoutputsizeandrankm,r AGMstepsT,adaptiveroundsQ,oversamplingparam-
(C √dR nβ+1 +2√log2)2, the AMDstepcount T √n, an≥ d eterg,failureprob. δ
thealo dg a2
ptiveroundcountQ=O(1).4
≥ w ←Low-rankDebiasing(kP, Sn,r,T,Q)
n′ 4⌈log 4n⌉,m √n′▷outputsize√n m<2√n
← ← ≤
4To unify the presentation of our results, Assum. (α,β)- w Resample(w,n′)
p
t ih
na
e
pra uum
tn
ids
se
rc
r
eo
s
ltn eas vnt ar
d
na
i
ti nn tgs ota ahl
a
gl
t
ic vto
h
em
e
ncm
ao
lgo
n
on
d ri
ia
t
til hog mno sr .i ath rem enin fop ru ct ep da ora nm lye wte hrs enw ti hth
e
Rw L e← S tuKT
rn← :
wK LT SK-C To ∈m ∆p nre −s 1s+ ∩+ ((k
N
mP 0, )nSn ▷, hn e′, nw ce,g ∥, w3δ L)
SKT ∥0
≤m
5DebiasedDistributionCompression
Our next result (proved in App. F) shows that LSKT can Swap-LS (Alg. G.2), a new, line-search version of KT-
providebetter-than-i.i.d.excessMMDino(n2)time. SWAP (Dwivedi and Mackey, 2021, Alg. 1b) that greed-
ily improves MMD to P while maintaining both the spar-
Theorem 5 (MMD guarantee for LSKT). Under As-
sity and simplex constraint of its input. Finally, we opti-
sum. (α,β)-params, Low-rank SKT (Alg. 3) with g
∈ mizetheweightsoftheremainingsupportpointsusingany
[log log(n+1) + 3.1,log (√n/logn)] and δ (0,1)
outp2 uts w in O((d +4 r+T)nr+d n1.5) t∈ ime sat- cubic-timequadraticprogrammingsolver.
LSKT kP kP
isfying,withprobabilityatleast1 δ, In Prop. G.1 we show that RT runs in time O((d +
− kP
m)nm+m3logn) and nearly preserves the MMD of its
∆MMD kP(w LSKT) inputwhenevermgrowsappropriatelywiththekernelcov-
=O(cid:18)(cid:113) ∥kP∥nmax(1/δ,ℓδ(logn)nγβGα√ n) +(cid:113) nHn,r(cid:19)
,
eringnumber. CombiningRTwithSteinThinningorLow-
n δ rankDebiasing in Alg. 5, we obtain Stein Recombination
(SR)andLow-rankSR(LSR),ourapproachestodebiased
forG ,H asinThms.3and5. simplex-weightedcompression. Remarkably,SRandLSR
m n,r
can match the MMD error rates established for SKT and
Example 3. Under the assumptions of Thm. 5 with γ-
LSKTusingsubstantiallyfewercoresetpoints,asournext
slow-growinginputpoints(Def.3),LOGGROWTHkP,T =
Θ(√n), and r = (logn)2(α+βγ)+ϵ for any ϵ > 0, LSKT result(provedinApp.G.2)shows.
delivers O(cid:101)(n−1/2) excess MMD with high probability in
Algorithm5(Low-rank)SteinRecombination(SR/LSR)
O(cid:101)(n1.5)timewithacoresetofsizem [√n,2√n).
∈ Input: mean-zero kernel kP, points n, output size m,
S
rankr,AGMstepsT,adaptiveroundsQ
4.WeightedDebiasedCompression (cid:40)
w
Low-rankDebiasing(kP, Sn,r,T,Q) iflow-rank
The prior sections developed debiased equal-weighted ← SteinThinning(kP, n) otherwise
S
coresetswithbetter-than-i.i.d.compressionguarantees. In w
SR
RecombinationThinning(kP, n,w,m)
this section, we match those guarantees with significantly Retu← rn: w ∆ with w S m
SR n−1 SR 0
smallerweightedcoresets. ∈ ∥ ∥ ≤
4.1.Simplex-weightedcoresetsviaSteinRecombination Theorem 6 (MMD guarantee for SR/ LSR). Under As-
sum. (α,β)-params, Stein Recombination (Alg. 5) takes
Algorithm4RecombinationThinning(RT) O(d kPn2+(d kP+m)nm+m3logn)tooutputw SR,andLow-
rankSRtakesO((d +r+T)nr+(d +m)nm+m3logn)
In wpu eit g: htsm wea ∈n-z ∆e nro −1k ,e or un te pl utk sP iz, ep moints Sn = (x i)n i=1, t ai sm ie nto To hu mt .pu 4t ,w
eL aS cR
hk .P oM fo tr he eov foe lr l, of wor ina gk nP y bδ
ou∈
nd( s0, h1 o) lda snd (sH
epn a,r
-
w˜ Resample(w,n)
← rately)withprobabilityatleast1 δ:
I,F WeightedRPCholesky(kP, n,w˜,m 1) −
w ▷′ F← ⊤← w˜R =eco Fm ⊤b win ′a ,t wio ′n ∈([F ∆, n1 −n 1]⊤ ,, aw n˜ d)S ▷ ∥w[F ′ ∥, 01 n ≤]− ⊤ m∈Rm×n ∆MMD kP(w SR)=O(cid:16)(cid:113) ∥kP∥ n(l nogn∨ δ1) + nH δn,m(cid:17) and
w w′ ′′ ′[← J]K ←T-S aw rga mp- iL nS w( ′k ∈P ∆, |S J|−n 1,w w′ ′, ⊤S kP PL ( SX n); [JJ ], S← n{ [Ji ]: )ww i ′′′ ▷> u0 s} e ∆MMD kP(w LSR)=O(cid:16)(cid:113) ∥kP∥ n(l nogn∨ δ1) + n(Hn,m δ+Hn,r)(cid:17) .
anyO(J3 )quadraticprogrammingsolver
| | Example4. InstantiatetheassumptionsofThm.6withγ-
Return: w w′′ ∆ with w m
RT n−1 RT 0
← ∈ ∥ ∥ ≤ slow-growinginputpoints(Def.3),LOGGROWTHkP,and
a heavily compressed coreset size m = (logn)2(α+βγ)+ϵ
Inspired by the coreset constructions of Hayakawa et al. for any ϵ > 0. Then SR delivers O(cid:101)(n−1/2) excess MMD
(2022; 2023), we first introduce a simplex-weighted com- withhighprobabilityinO(n2)time,andLSRwithr=m
pression algorithm, RecombinationThinning (RT, Alg. 4), andT =Θ(√n)achievesthesameinO(cid:101)(n1.5)time.
suitable for summarizing a debiased input sequence. To
produce a coreset given input weights w ∆ , RT
∈ n−1 4.2.Constant-preservingcoresetsviaSteinCholesky
first prunes small weights using Resample and then uses
WeightedRPCholesky to identify m 1 test vectors that For applications supporting negative weights, we next
−
capture most of the variability in the weighted kernel introduce a constant-preserving compression algorithm,
matrix. Next, Recombination (Alg. G.1) (Tchernychova, CholeskyThinning (CT, Alg. 6), suitable for summarizing
2016, Alg. 1) identifies a sparse simplex vector w′ with a debiased input sequence. CT first applies WeightedR-
∥w′
∥0 ≤
m that exactly matches the inner product of its PCholesky to a constant-regularized kernel kP(x,y) + c
input with each of the test vectors. Then, we run KT- to select an initial coreset and then uses a combination of
6DebiasedDistributionCompression
KT-Swap-LSandclosed-formoptimalconstant-preserving withhighprobabilityinO(n2)time,andLSCwithr=m
reweighting to greedily refine the support and weights. andT =Θ(√n)achievesthesameinO(cid:101)(n1.5)time.
TheregularizedkernelensuresthatWeightedRPCholesky,
Remark 3. While we present our results for a target pre-
originally developed for compression with unconstrained
weights,alsoyieldsahigh-qualitycoresetwhenitsweights
cision of 1/√n, a coarser target precision of 1/√n
0
for
n < n can be achieved more quickly by standard thin-
areconstrainedtosumto1,andourCTstandaloneanalysis 0
ning the input sequence down to size n before running
(Prop. H.1) improves upon the runtime and error guaran- 0
SR,LSR,SC,orLSC.
teesofRT.InAlg.7,wecombineCTwithSteinThinning
or Low-rankDebiasing to obtain Stein Cholesky (SC) and
Low-rankSC(LSC),ourapproachestodebiasedconstant-
5.Experiments
preserving compression. Our MMD guarantees for SC
and LSC (proved in App. H.2) improve upon the rates of
We next evaluate the practical utility of our procedures
Thm.6.
when faced with three common sources of bias: (1)
burn-in, (2) approximate MCMC, and (3) tempering. In
Algorithm6CholeskyThinning(CT)
all experiments, we use a Stein kernel k with an in-
p
Input: mean-zero kernel kP, points Sn = (x i)n i=1, verse multiquadric (IMQ) base kernel k(x,y) = (1+
c I,←w Fei Ag Vht E Ws R ew A igG∈ hE t(∆ eL dn Ra− r Pg1 Ce, s ho t ou m lt ep su e knt yts (ri i kz ee Ps +m of c( ,kP n( ,x wi, ,x mi) )) ;n i w=1)
0
n
∥
∥
th·x
∥
e− M iny pd∥ ui2 M s t.t/ a Tnσ oc2 e v) a− a rm1 y/ o2 on ugf to psr ut t1σ M00 Me 0q Dpu oa pl in ret to s cis st t ih a oe n nd ,m a wre d ed t fii ha rin sn tn sp e ta adi nr fw dro ai rs m de
← S ←
w[I] ←argmin w′∈R|I|:(cid:80) iw i′=1w′⊤kP( Sn[I], Sn[I])w′ thintheinputtosizen 0 ∈{210,212,214,216,218,220 }be-
w KT-Swap-LS(kP, n,w,CP); I i:w
i
=0 fore applying any method, as discussed in Rems. 2 and 3.
← S ←{ ̸ }
w Re[I tu] r← n:a wrgmin w w′∈R|I| R:(cid:80) ni ww ii′ t= h1w w′⊤kP( Sn m[I ,], 1S ⊤n w[I])w =′
1
F foo rr mlow =-r ran =k nor τw
.
e Wig hh et ned cc oo mre ps ae rt inm ge wth eo id gs h, tw ede cs oh ro ew ser te ss ,u wlt es
CT ← ∈ ∥ CT ∥0 ≤ n CT optimally reweight every coreset. We report the median
over 5 independent runs for all error metrics. We imple-
ment our algorithms in JAX (Bradbury et al., 2018) and
Algorithm7(Low-rank)SteinCholesky(SC/LSC)
referthereadertoApp.Iforadditionalexperimentdetails.
Input: mean-zero kernel kP, points Sn, output size m,
Correctingforburn-in TheinitialiteratesofaMarkov
rankr,AGMstepsT,adaptiveroundsQ
(cid:40) chain are biased by its starting point and need not accu-
w
Low-rankDebiasing(kP, Sn,r,T,Q) iflow-rank ratelyreflectthetargetdistributionP.Classicalburn-incor-
← SteinThinning(kP, n) otherwise rectionsuseconvergencediagnosticstodetectanddiscard
S
w
SC
CholeskyThinning(kP, n,w,m) these iterates but typically require running multiple inde-
Retu← rn: w Rnwith w S mand1⊤w =1 pendentMarkovchains(CowlesandCarlin,1996). Alter-
SC ∈ ∥ SC ∥0 ≤ n SC
natively, ourproposeddebiasedcompressionmethodscan
beusedtocorrectforburn-ingivenjustasinglechain.
Theorem 7 (MMD guarantee for SC / LSC). Under
Assum. (α,β)-params, Stein Cholesky (Alg. 7) takes We test this claim using an experimental setup from Ri-
O(d kPn2+(d kP+m)nm+m3)timetooutputw SC,andLow- abiz et al. (2022, Sec. 4.1) and the 6-chain “burn-in ora-
rankSCtakesO((d +r+T)nr+(d +m)nm+m3)timeto cle” diagnostic of Vats and Knudson (2021). We aim to
kP kP
outputw . Moreover,foranyδ (0,1),withprobability compress a posterior P over the parameters in the Good-
LSC
∈
atleast1 δ,eachofthefollowingboundshold: win model of oscillatory enzymatic control (d = 4) us-
−
ing n=2 106 points from a preconditioned Metropolis-
∆MMD kP(w SC)=2MMD OPT adjustedL×
angevinalgorithm(P-MALA)chain. Werepeat
(cid:113)
+O(cid:0) ∥kP∥ nlogn + H n,m′(cid:1) and thisexperimentwiththreealternativeMCMCalgorithmsin
δn δ App.I.3. OurprimarymetricisMMD toPwithM =I,
∆MMD (w )=2MMD
kP
kP LSC OPT but,forexternalvalidation,wealsomeasuretheenergydis-
(cid:113)
+O(cid:0) ∥kP∥ n(logn∨1/δ) + H n,m′ + nHn,r(cid:1) tance (Riabiz et al., 2022, Eq. 11) to an auxiliary MCMC
δn δ δ2 chainoflengthn. Trajectoryplotsofthefirsttwocoordi-
nates (Fig. 1, left) highlight the substantial burn-in period
forH asinThm.4andm′ ≜m+log2 2√mlog2+1.
n,r − for the Goodwin chain and the ability of LSKT to mimic
Example5. InstantiatetheassumptionsofThm.7withγ- the 6-chain burn-in oracle using only a single chain. In
slow-growinginputpoints(Def.3),LOGGROWTHkP,and Fig. 1 (right), for both the MMD metric and the auxiliary
a heavily compressed coreset size m = (logn)2(α+βγ)+ϵ energydistance,ourproposedmethodsconsistentlyoutper-
for any ϵ > 0. Then SC delivers O(cid:101)(n−1/2) excess MMD formSteinthinningandmatchthequalityof6-chainburn-
7DebiasedDistributionCompression
Equal-Weighted Simplex-Weighted Constant-Preserving
101 101 101
100 100 100
10−1 10−1 10−1
0.1540 0.1540 0.1540
Burn-inOracle+Standard Burn-inOracle+Standard Burn-inOracle+Standard
SteinThinning SteinThinning SteinThinning
0.1535 SteinKernelThinning 0.1535 SteinRecombination(τ=0.4) 0.1535 SteinCholesky(τ=0.4)
Low-rankSKT(τ=0.4) SteinRecombination(τ=0.5) SteinCholesky(τ=0.5)
0.1530 Low-rankSKT(τ=0.5) 0.1530 Low-rankSR(τ=0.4) 0.1530 Low-rankSC(τ=0.4)
Burn-inOracle+Compress++ Low-rankSR(τ=0.5) Low-rankSC(τ=0.5)
Burn-inOracle+RT Burn-inOracle+CT
0.1525 0.1525 0.1525
0.1520 0.1520 0.1520
0.1515 0.1515 0.1515
102 103 102 103 102 103
CoresetSizem CoresetSizem CoresetSizem
Figure1: Correctingforburn-in. Left: Beforeselectingcoresets(orange),theburn-inoracleuses6independentMarkov
chainstodiscardburn-in(red)whileLSKTidentifiesthesamehigh-densityregion(blue)with1chain. Right: Usingonly
onechain,ourmethodsconsistentlyoutperformtheSteinandstandardthinningbaselinesandmatchthe6-chainoracle.
in removal paired with unbiased compression. The spike peaked and more dispersed distribution Q, is a popular
inbaselineenergydistancefortheconstant-preservingtask technique to improve the speed of MCMC convergence.
canbeattributedtotheselectionofoverlylargeweightval- Onecancorrectforthesamplebiasusingimportancesam-
uesduetopoormatrixconditioning;thesimplex-weighted pling, but this requires knowledge of the tempered den-
task does not suffer from this issue due to its regularizing sityandcanintroducesubstantialvariance(Gramacyetal.,
nonnegativityconstraint. 2010).Alternatively,onecanuseconstructionsofthiswork
tocorrectfortemperingduringcompression; thisrequires
Correcting for approximate MCMC In posterior in-
no importance weighting and no knowledge of Q. To test
ference,MCMCalgorithmstypicallyrequireiteratingover
this proposal, we compress the cardiac calcium signaling
every datapoint to draw each new sample point. When
modelposterior(d = 38)ofRiabizetal.(2022,Sec. 4.3)
datasets are large, approximating MCMC using datapoint
with M = I and n = 3 106 tempered points from
mini-batchescanreducesamplingtimeatthecostofpersis- ×
a Gaussian random walk Metropolis-Hastings chain. As
tent bias and an unknown stationary distribution that pro-
discussedbyRiabizetal., compressionisessentialinthis
hibits debiasing via importance sampling. Our proposed
settingastheultimateaimistopropagateposterioruncer-
methodscancorrectforthesebiasesduringcompressionby
tainty through a human heart simulator, a feat which re-
computingfull-datasetscoresonasmallsubsetofn stan-
0 quires over 1000 CPU hours for each summary point re-
dardthinnedpoints.Toevaluatethisprotocol,wecompress
tained. Our methods perform on par with Stein thinning
aBayesianlogisticregressionposteriorconditionedonthe
forequal-weightedcompressionandyieldsubstantialgains
ForestCovtypedataset(d=54)usingn=224 approximate
over Stein (and standard) thinning for the two weighted
MCMCpointsfromthestochasticgradientFisherscoring
compressiontasks.
sampler (Ahn et al., 2012) with batch size 32. Follow-
ing Wang et al. (2024), we set M = 2logp(x ) at
mode
thesamplemodex anduse220 sur− ro∇ gategroundtruth 6.ConclusionsandFutureWork
mode
pointsfromtheNoU-turnSampler(Hoffmanetal.,2014)
We have introduced and analyzed a suite of new proce-
toevaluateenergydistance. Wefindthatourproposalsim-
duresforcompressingabiasedinputsequenceintoanaccu-
prove upon standard thinning and Stein thinning for each
rate summary of a target distribution. For equal-weighted
compression task, not just in the optimized MMD metric
compression,Steinkernelthinningdelivers√npointswith
(Fig.2,top)butalsointheauxiliaryenergydistance(Fig.2,
O(cid:101)(n−1/2)MMDinO(n2)time,andlow-rankSKTcanim-
middle)andwhenmeasuringintegrationerrorforthemean
(Fig.I.4).
provethisrunningtimetoO(cid:101)(n3/2). Forsimplex-weighted
andconstant-preservingcompression,Steinrecombination
Correcting for tempering Tempering, targeting a less-
8
ecnatsiDygrenE
DMM
ecnatsiDygrenE
DMM
ecnatsiDygrenE
DMMDebiasedDistributionCompression
Equal-Weighted(Approx. MCMC) Simplex-Weighted(Approx. MCMC) Constant-Preserving(Approx. MCMC)
10−1 10−1 10−1
3.32 3.32 3.32
StandardThinning StandardThinning StandardThinning
SteinThinning SteinThinning SteinThinning
3.30 SteinKernelThinning 3.30 SteinRecombination(τ=0.4) 3.30 SteinCholesky(τ=0.4)
Low-rankSKT(τ=0.4) SteinRecombination(τ=0.5) SteinCholesky(τ=0.5)
3.28 Low-rankSKT(τ=0.5) 3.28 Low-rankSR(τ=0.4) 3.28 Low-rankSC(τ=0.4)
Low-rankSR(τ=0.5) Low-rankSC(τ=0.5)
3.26 3.26 3.26
3.24 3.24 3.24
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
CoresetSizem CoresetSizem CoresetSizem
Equal-Weighted(Tempering) Simplex-Weighted(Tempering) Constant-Preserving(Tempering)
StandardThinning StandardThinning
103 103 SteinThinning 103 SteinThinning
SteinRecombination(τ=0.4) SteinCholesky(τ=0.4)
StandardThinning SteinRecombination(τ=0.5) SteinCholesky(τ=0.5)
102 SteinThinning 102 Low-rankSR(τ=0.4) 102 Low-rankSC(τ=0.4)
SteinKernelThinning Low-rankSR(τ=0.5) Low-rankSC(τ=0.5)
Low-rankSKT(τ=0.4)
101 Low-rankSKT(τ=0.5) 101 101
100 100 100
102 103 102 103 102 103
CoresetSizem CoresetSizem CoresetSizem
Figure2: CorrectingforapproximateMCMC(top)andtempering(bottom). Forposteriorinferenceovertheparam-
eters of Bayesian logistic regression (d=54, top) and a cardiac calcium signaling model (d=38, bottom), our concise
coresetconstructionscorrectforapproximateMCMCandtemperingbiaseswithoutneedforexplicitimportancesampling.
andSteinCholeskyprovideenhancedparsimony,matching Acknowledgments
theseguaranteeswithasfewaspoly-log(n)points. Recent
WethankMarinaRiabizformakingtheMarkovchaindata
work has identified some limitations of score-based dis-
used in Sec. 5 available and Jeffrey Rosenthal for helpful
crepancies,likeSteinkernelMMDs,anddevelopedmodi-
discussionsconcerningthegeometricergodicityofMarkov
fiedobjectivesthataremoresensitivetotherelativedensity
chains.
of isolated modes (Liu et al., 2023; Be´nard et al., 2024).
A valuable next step would be to extend our construc-
tions to provide compression guarantees for these mod- References
ified discrepancy measures. Other opportunities for fu-
E. Afzali and S. Muthukumarana. Gradient-free
ture work include marrying the better-than-i.i.d. guaran-
kernel conditional stein discrepancy goodness
tees of this work with the non-myopic compression of
of fit testing. Machine Learning with Appli-
Teymur et al. (2021), the control-variate compression of
cations, 12:100463, 2023. ISSN 2666-8270.
Chopin and Ducrocq (2021), and the online compression
doi: https://doi.org/10.1016/j.mlwa.2023.100463.
ofHawkinsetal.(2022).
URL https://www.sciencedirect.com/
science/article/pii/S2666827023000166.
BroaderImpactStatement
S.Ahn,A.Korattikara,andM.Welling.Bayesianposterior
This paper presents work with the aim of advancing the
sampling via stochastic gradient fisher scoring. arXiv
fieldofMachineLearning. Therearemanypotentialsoci-
preprintarXiv:1206.6380,2012.
etalconsequencesofourwork,nonewhichwefeelmustbe
specificallyhighlightedhere. N.Aronszajn.Theoryofreproducingkernels.Transactions
of the American mathematical society, 68(3):337–404,
1950.
9
DMM
ecnatsiDygrenE
DMM
DMM
ecnatsiDygrenE
DMM
DMM
ecnatsiDygrenE
DMMDebiasedDistributionCompression
Y.F.Atchade´andF.Perron.Onthegeometricergodicityof M.K.CowlesandB.P.Carlin. Markovchainmontecarlo
metropolis-hastingsalgorithms. Statistics,41(1):77–84, convergencediagnostics: acomparativereview. Journal
2007. of the American Statistical Association, 91(434):883–
904,1996.
A. Barp, F.-X. Briol, A. Duncan, M. Girolami, and
L.Mackey. Minimumsteindiscrepancyestimators. Ad- A.Daxetal.Low-rankpositiveapproximantsofsymmetric
vances in Neural Information Processing Systems, 32, matrices. AdvancesinLinearAlgebra&MatrixTheory,
4(03):172,2014.
2019.
R.DoucandO.Cappe´.Comparisonofresamplingschemes
A. Barp, C.-J. Simon-Gabriel, M. Girolami, and
for particle filtering. In ISPA 2005. Proceedings of the
L. Mackey. Targeted separation and convergence with
4thInternationalSymposiumonImageandSignalPro-
kerneldiscrepancies. arXivpreprintarXiv:2209.12835,
cessingandAnalysis,2005.,pages64–69.IEEE,2005.
2022.
R.Douc,E.Moulines,P.Priouret,andP.Soulier. Markov
C. Be´nard, B. Staber, and S. Da Veiga. Kernel stein dis- chains,volume1. Springer,2018.
crepancy thinning: a theoretical perspective of patholo-
giesandapracticalfixwithregularization. Advancesin A.DurmusandE.Moulines. Nonasymptoticconvergence
NeuralInformationProcessingSystems,36,2024. analysisfortheunadjustedlangevinalgorithm. 2017.
A. Durmus and E´. Moulines. On the geometric con-
P.Billingsley. Convergenceofprobabilitymeasures. John
vergence for mala under verifiable conditions. arXiv
Wiley&Sons,2013.
preprintarXiv:2201.01951,2022.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, A. Durmus, E´. Moulines, and E. Saksman. Irreducibil-
C.Leary,D.Maclaurin,G.Necula,A.Paszke,J.Vander-
ityandgeometricergodicityofhamiltonianmontecarlo.
Plas, S. Wanderman-Milne, and Q. Zhang. JAX: com-
TheAnnalsofStatistics,48(6):3545–3564,2020.
posable transformations of Python+NumPy programs,
2018. URLhttp://github.com/google/jax. R.DwivediandL.Mackey.Kernelthinning.arXivpreprint
arXiv:2105.05842,2021.
R. C. Bradley. Basic properties of strong mixing condi-
R.DwivediandL.Mackey.Generalizedkernelthinning.In
tions.asurveyandsomeopenquestions. 2005.
International Conference on Learning Representations,
C. Carmeli, E. De Vito, and A. Toigo. Vector valued re- 2022.
producing kernel hilbert spaces of integrable functions
E. Epperly and E. Moreno. Kernel quadrature with ran-
and mercer theorem. Analysis and Applications, 4(04):
domly pivoted cholesky. Advances in Neural Informa-
377–408,2006.
tionProcessingSystems,36,2024.
T. K. Chandra. De la valle´e poussin’s theorem, uniform M. A. Erdogdu, L. Mackey, and O. Shamir. Global non-
integrability,tightnessandmoments. Statistics&Prob- convex optimization with discretized diffusions. Ad-
abilityLetters,107:136–141,2015. vances in Neural Information Processing Systems, 31,
2018.
Y.Chen,E.N.Epperly,J.A.Tropp,andR.J.Webber.Ran-
domly pivoted cholesky: Practical approximation of a M.A.Gallegos-Herrada,D.Ledvinka,andJ.S.Rosenthal.
kernelmatrixwithfewentryevaluations. arXivpreprint Equivalencesofgeometricergodicityofmarkovchains,
arXiv:2207.06503,2022. 2023.
B. Ghojogh, A. Ghodsi, F. Karray, and M. Crowley. Kkt
N. Chopin and G. Ducrocq. Fast compression of mcmc
conditions, first-order and second-order optimization,
output. Entropy,23(8):1017,2021.
anddistributedoptimization: tutorialandsurvey. arXiv
K. Chwialkowski, H. Strathmann, and A. Gretton. A ker- preprintarXiv:2110.01858,2021.
nel test of goodness of fit. In M. F. Balcan and K. Q.
J.GorhamandL.Mackey. Measuringsamplequalitywith
Weinberger, editors, Proceedings of The 33rd Interna-
kernels. InInternationalConferenceonMachineLearn-
tional Conference on Machine Learning, volume 48
ing,pages1292–1301.PMLR,2017.
of Proceedings of Machine Learning Research, pages
2606–2615, New York, New York, USA, 20–22 Jun J. Gorham, A. B. Duncan, S. J. Vollmer, and L. Mackey.
2016.PMLR. URLhttps://proceedings.mlr. Measuring sample quality with diffusions. The Annals
press/v48/chwialkowski16.html. ofAppliedProbability,29(5):2884–2928,2019.
10DebiasedDistributionCompression
R.Gramacy,R.Samworth,andR.King. Importancetem- H.Q.Minh. Somepropertiesofgaussianreproducingker-
pering. StatisticsandComputing,20:1–7,2010. nelhilbertspacesandtheirimplicationsforfunctionap-
proximationandlearningtheory. ConstructiveApproxi-
A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scho¨lkopf,
mation,32:307–338,2010.
andA.Smola. Akerneltwo-sampletest. TheJournalof
MachineLearningResearch,13(1):723–773,2012. S. Niederer, L. Mitchell, N. Smith, and G. Plank. Simu-
latinghumancardiacelectrophysiologyonclinicaltime-
C.Hawkins,A.Koppel,andZ.Zhang. Online,informative
scales. Frontiersinphysiology,2:14,2011.
mcmcthinningwithkernelizedsteindiscrepancy. arXiv
preprintarXiv:2201.07130,2022. V.I.PaulsenandM.Raghupathi.Anintroductiontothethe-
ory of reproducing kernel Hilbert spaces, volume 152.
S. Hayakawa, H. Oberhauser, and T. Lyons. Positively
Cambridgeuniversitypress,2016.
weightedkernelquadratureviasubsampling. Advances
in Neural Information Processing Systems, 35:6886– D.Phan, N.Pradhan, andM.Jankowiak. Composableef-
6900,2022. fectsforflexibleandacceleratedprobabilisticprogram-
ming in numpyro. arXiv preprint arXiv:1912.11554,
S. Hayakawa, H. Oberhauser, and T. Lyons. Sampling-
2019.
basednystro¨mapproximationandkernelquadrature. In
Proceedings of the 40th International Conference on
I. Pinelis. Exact lower and upper bounds on the incom-
MachineLearning,ICML’23.JMLR.org,2023. pletegammafunction.arXivpreprintarXiv:2005.06384,
2020.
L. Hodgkinson, R. Salomone, and F. Roosta. The repro-
ducingsteinkernelapproachforpost-hoccorrectedsam-
Y. Pitcan. A note on concentration inequalities for u-
pling. arXivpreprintarXiv:2001.09266,2020.
statistics. arXivpreprintarXiv:1712.06160,2017.
M.D.Hoffman,A.Gelman,etal. Theno-u-turnsampler:
Q. Qin. Geometric ergodicity of trans-dimensional
adaptively setting path lengths in hamiltonian monte
markov chain monte carlo algorithms. arXiv preprint
carlo. J.Mach.Learn.Res.,15(1):1593–1623,2014.
arXiv:2308.00139,2023.
A. A. Johnson. Geometric ergodicity of Gibbs samplers.
M. Riabiz, W. Y. Chen, J. Cockayne, P. Swietach, S. A.
universityofminnesota,2009.
Niederer,L.Mackey,andC.J.Oates. ReplicationData
for: Optimal Thinning of MCMC Output, 2020. URL
L. Li, J.-G. Liu, and Y. Wang. Geometric ergodic-
https://doi.org/10.7910/DVN/MDKNWM. Ac-
ity of sgld via reflection coupling. arXiv preprint
cessedonMar23,2021.
arXiv:2301.06769,2023.
Q.LiuandJ.Lee.Black-boximportancesampling.InArti- M. Riabiz, W. Y. Chen, J. Cockayne, P. Swietach, S. A.
ficialIntelligenceandStatistics,pages952–961.PMLR, Niederer,L.Mackey,andC.J.Oates. Optimalthinning
2017. of mcmc output. Journal of the Royal Statistical Soci-
etySeriesB:StatisticalMethodology,84(4):1059–1081,
Q.Liu,J.Lee,andM.Jordan. Akernelizedsteindiscrep- 2022.
ancy for goodness-of-fit tests. In International confer-
enceonmachinelearning,pages276–284.PMLR,2016. G.O.RobertsandR.L.Tweedie. Geometricconvergence
andcentrallimittheoremsformultidimensionalhastings
X.Liu,A.B.Duncan,andA.Gandy.Usingperturbationto and metropolis algorithms. Biometrika, 83(1):95–110,
improve goodness-of-fit tests based on kernelized stein 1996.
discrepancy. InA.Krause,E.Brunskill,K.Cho,B.En-
gelhardt,S.Sabato,andJ.Scarlett,editors,Proceedings A.Shetty, R.Dwivedi, andL.Mackey. Distributioncom-
ofthe40thInternationalConferenceonMachineLearn- pressioninnear-lineartime.InInternationalConference
ing, volume 202 of Proceedings of Machine Learning onLearningRepresentations,2022.
Research,pages21527–21547.PMLR,23–29Jul2023.
I.SteinwartandA.Christmann. Supportvectormachines.
F. Merleve`de, M. Peligrad, and S. Utev. Sharp conditions SpringerScience&BusinessMedia,2008.
forthecltoflinearprocessesinahilbertspace. Journal
I. Steinwart and C. Scovel. Mercer’s theorem on general
ofTheoreticalProbability,10(3):681–693,1997.
domains: Ontheinteractionbetweenmeasures,kernels,
S.P.MeynandR.L.Tweedie. Markovchainsandstochas- and rkhss. Constructive Approximation, 35:363–417,
ticstability. SpringerScience&BusinessMedia,2012. 2012.
11DebiasedDistributionCompression
H.-W. Sun and D.-X. Zhou. Reproducing kernel hilbert
spaces associated with analytic translation-invariant
mercerkernels. JournalofFourierAnalysisandAppli-
cations,14(1):89–101,2008.
M.Tchernychova. Caratheodorycubaturemeasures. PhD
thesis,UniversityofOxford,2016.
O. Teymur, J. Gorham, M. Riabiz, and C. Oates. Op-
timal quantisation of probability measures using maxi-
mummeandiscrepancy. InInternationalConferenceon
Artificial Intelligence and Statistics, pages 1027–1035.
PMLR,2021.
D.VatsandC.Knudson. Revisitingthegelman–rubindi-
agnostic. StatisticalScience,36(4):518–529,2021.
M. J. Wainwright. High-dimensional statistics: A non-
asymptoticviewpoint,volume48. Cambridgeuniversity
press,2019.
C.Wang,Y.Chen,H.Kanagawa,andC.J.Oates. SteinΠ-
importance sampling. Advances in Neural Information
ProcessingSystems,36,2024.
J.-K. Wang, J. Abernethy, and K. Y. Levy. No-regret dy-
namicsinthefenchelgame: Aunifiedframeworkforal-
gorithmicconvexoptimization. MathematicalProgram-
ming,pages1–66,2023.
J. Wellner et al. Weak convergence and empirical pro-
cesses: withapplicationstostatistics. SpringerScience
&BusinessMedia,2013.
J. Yang, Q. Liu, V. Rao, and J. Neville. Goodness-of-fit
testingfordiscretedistributionsviasteindiscrepancy. In
International Conference on Machine Learning, pages
5561–5570.PMLR,2018.
F.Zhang. TheSchurcomplementanditsapplications,vol-
ume4. SpringerScience&BusinessMedia,2006.
D.-X.Zhou.Thecoveringnumberinlearningtheory.Jour-
nalofComplexity,18(3):739–767,2002.
12DebiasedDistributionCompression
AppendixContents
A AppendixNotation 14
B SpectralAnalysisofKernelMatrices 14
B.1 Boundingthespectrumofkernelmatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B.2 SpectraldecayofSteinkernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.2.1 Caseofdifferentiablebasekernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2.2 Caseofradiallyanalyticbasekernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2.3 ProofofProp.1:Steinkernelgrowthrates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C ADebiasingBenchmark 24
C.1 MMDofunbiasedi.i.d.samplepoints. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.2 ProofofThm.1:Debiasingviasimplexreweighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.3 ProofofThm.2:Better-than-i.i.d.debiasingviasimplexreweighting . . . . . . . . . . . . . . . . . . . . . . . . . . 25
C.4 ProofofThm.C.1:Debiasingviai.i.d.simplexreweighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D SteinKernelThinning 33
D.1 SteinThinningwithsufficientstatistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
D.2 KernelThinningtargetingP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
D.3 ProofofThm.3:MMDguaranteeforSKT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
E ResamplingofSimplexWeights 36
F AcceleratedDebiasedCompression 40
F.1 ProofofThm.4:DebiasingguaranteeforLD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
F.2 ThinningwithKT-Compress++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
F.3 ProofofThm.5:MMDguaranteeforLSKT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
G Simplex-WeightedDebiasedCompression 47
G.1 MMDguaranteeforRT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
G.2 ProofofThm.6:MMDguaranteeforSR/LSR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
H Constant-PreservingDebiasedCompression 50
H.1 MMDguaranteeforCT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
H.2 ProofofThm.7:MMDguaranteeforSC/LSC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
I ImplementationandExperimentalDetails 53
I.1 O(d)-timeSteinkernelevaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
I.2 Defaultparametersforalgorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
I.3 Correctingforburn-indetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
I.4 CorrectingforapproximateMCMCdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
I.5 Correctingfortemperingdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
13DebiasedDistributionCompression
A.AppendixNotation
Forthepointsequence =(x ) ,wedefineS ≜ 1 (cid:80) δ . Foraweightvectorw Rn,wedefinethesupport
Sn i i∈[n] n n i∈[n] xi ∈
supp(w) ≜ i [n] : w = 0 andthesignedmeasureSw ≜ (cid:80) w δ . ForamatrixK Rn×n andw ∆ ,
{ ∈ i ̸ } n i∈[n] i xi ∈ ∈ n−1
wedefinetheweightedmatrixKw ≜ diag(√w)Kdiag(√w). Forpositivesemidefinite(PSD)matrices(A,B), weuse
A B(resp. A B)tomeanA B(resp. B A)isPSD.ForasymmetricPSD(SPSD)matrixM,weletM1/2denote
⪰ ⪯ − −
asymmetricmatrixsquarerootsatisfyingM =M1/2M1/2. ForA Rn×m,wedenote A ≜sup ∥Ax∥ p. Wewill
use1 todenotetheindicatorfunctionforaneventE. ∈ ∥ ∥p x̸=0 ∥x∥ p
E
Notationsusedonlyinaspecificsectionwillbeintroducedwithin.
B.SpectralAnalysisofKernelMatrices
Thegoalofthissectionistodevelopspectralboundsforkernelmatrices.
InApp.B.1,wetransfertheboundsoncoveringnumbersfromthedefinitionofPOLYGROWTHorLOGGROWTHkernels
toboundsontheeigenvaluesofthekernelmatrices. Thissetsthetheoreticalfoundationforthealgorithmsinlatersections
astheirerrorguaranteesrelyonthefastdecayofeigenvaluesofkernelmatrices.
In App. B.2, we show that Stein kernels are POLYGROWTH (resp. LOGGROWTH) provided that their base kernels are
differentiable(resp. radiallyanalytic). HenceweobtainspectralboundsforawiderangeofSteinkernels.
Notation ForanormedspaceE, weuse todenoteitsnorm, (p,r) ≜ x E : x p r todenotethe
∥·∥E BE { ∈ ∥ − ∥E ≤ }
closedballofradiusr centeredatpinE withtheshorthand (r) ≜ (0,r)and ≜ (1). WhenE isanRKHS
E E E E
B B B B
withkernelk, forbrevityweusek inplaceofE inthesubscript. LetF( , )denotethespaceoffunctionsfrom to
X Y X
,andB(E,F)denotethespaceofboundedlinearfunctionsbetweennormedspacesE,F. ForasetA,weuseℓ (A)
∞
Y todenotethespaceofboundedR-valuedfunctionsonAequippedwiththesup-norm f ≜ sup f(x). Weuse
∥ ∥∞,A x∈A| |
E (cid:44) F todenotetheinclusionmap. euseλ (T)todenotetheℓ-thlargesteigenvalueofanoperatorT.
ℓ
→
B.1.Boundingthespectrumofkernelmatrices
WefirstintroducethegeneralMercerrepresentationtheoremfromSteinwartandScovel(2012),whichshowstheexistence
of a discrete spectrum of the integral operator associated with a continuous square-integrable kernel. The theorem also
providesaseriesexpansionofthekernel,i.e.,theMercerrepresentation,intermsoftheeigenvaluesandeigenfunctions.
LemmaB.1(GeneralMercerrepresentation(SteinwartandScovel,2012)). Considerakernelk : Rd Rd Rthatis
(cid:82) × →
jointlycontinuousinbothinputsandaprobabilitymeasureµsuchthat k(x,x)dµ(x)< . Thenthefollowingholds.
∞
(a) Theinclusion (cid:44) 2(µ)isacompactoperator,i.e., isacompactsubsetof 2(µ). Inparticular,thisinclusion
k k
H →L B L
iscontinuous.
(b) TheHilbert-spaceadjointoftheinclusion (cid:44) 2(µ)isthecompactoperatorS : 2(µ) definedas
k k,µ k
H →L L →H
S f
≜(cid:82)
k(,x)f(x)dµ(x). (3)
k,µ
·
WealsohaveS∗ ≜ (cid:44) 2(µ). Hencetheoperator
k,µ Hk →L
T ≜S∗ S : 2(µ) 2(µ) (4)
k,µ k,µ k,µ L →L
isalsocompact.
(c) There exist λ ∞ with λ λ 0 and ϕ ∞ such that ϕ ∞ is an orthonormal system in
{ ℓ }ℓ=1 1 ≥ 2 ≥ ··· ≥ { ℓ }ℓ=1 ⊂ Hk { ℓ }ℓ=1
2(µ)and λ ∞ (resp. ϕ ∞ )consistsoftheeigenvalues(resp. eigenfunctions)ofT witheigendecomposi-
L { ℓ }ℓ=1 { ℓ }ℓ=1 k,µ
tion,forf 2(µ),
∈L
T f
=(cid:80)∞
λ f,ϕ ϕ
k,µ ℓ=1 ℓ ⟨ ℓ ⟩L2(µ) ℓ
withconvergencein 2(µ).
L
14DebiasedDistributionCompression
(d) Wehavethefollowingseriesexpansion
k(x,x′)=(cid:80)∞ λ ϕ (x)ϕ (x′), (5)
ℓ=1 i ℓ ℓ
wheretheseriesconvergenceisabsoluteanduniforminx,x′onallA A suppµ suppµ.
× ⊂ ×
ProofofLem.B.1. Part (a) and (b) follow respectively from Steinwart and Scovel (2012, Lem. 2.3 and 2.2). Part (c)
follows from part (a) and Steinwart and Scovel (2012, Lem. 2.12). Finally, part (d) follows from Steinwart and Scovel
(2012,Cor.3.5).
Wewillusethefollowinglemmaregardingtherestrictionofcoveringnumbers.
LemmaB.2(Coveringnumberispreservedinrestriction). Forakernelk : Rd Rd RandasetA Rd,wehave
× → ⊂
(A,ϵ)= (A,ϵ),fork ,therestrictedkernelofktoA(PaulsenandRaghupathi,2016,Sec.5.4).
Nk Nk|A |A
ProofofLem.B.2. Itsufficestoshowthata(k,A,ϵ)covercanbeconvertedtoacoverof(k ,A,ϵ)ofthesamecardinality
A
|
andviceversa.
(cid:110) (cid:111)
Let bea(k ,A,ϵ)cover. Foranyf ,wehave f =inf f˜ :f˜ ,f˜ =f 1(Paulsenand
C
⊂Bk|A |A
∈C ∥
∥k|A
∥
∥k ∈Hk |A
≤
Raghupathi,2016,Corollary5.8). Moreover,theinfimumisattainedbysomef˜ suchthat f˜ = f 1and
∈Hk
∥
∥k
∥
∥k|A
≤
f˜ =f. Nowform ˜= f˜:f . Foranyh˜ ,thereexistsf suchthat
A k
| C { ∈C} ∈B ∈C
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)h˜ f(cid:13) ϵ = =(cid:13)h˜ f˜(cid:13) ϵ,
(cid:13) A (cid:13) (cid:13) (cid:13)
| − ∞,A ≤ ⇒ − ∞,A ≤
so ˜isa(k ,A,ϵ)cover.
A
C |
Fortheotherdirection,let ˜ bea(k,A,ϵ)cover. Define = f˜ :f˜ ˜ . Since f˜ f˜ ,we
C
⊂Bk
C {
|A ∈C}⊂Hk|A
∥
|A ∥k|A
≤∥
∥k
have . Foranyh ,againbyPaulsenandRaghupathi(2016,Corollary5.8),thereexistsh˜ suchthat
C
⊂Bk|A ∈BkA] ∈Hk
h˜ = h 1,sothereexistsf˜ ˜suchthat
∥
∥k
∥
∥k|A
≤ ∈C
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)h˜ f˜(cid:13) ϵ = (cid:13)h f˜ (cid:13) ϵ,
(cid:13) (cid:13) (cid:13) A(cid:13)
− ∞,A ≤ ⇒ − | ∞,A ≤
Hence isa(k,A,ϵ)cover.
C
ThegoalfortherestofthissectionistotransfertheboundsofthecoveringnumberinthedefinitionofaPOLYGROWTH
or LOGGROWTH kernel from Assum. (α,β)-kernel to bounds on entropy numbers (Steinwart and Christmann, 2008,
Def.6.20)thatarecloselyrelatedtoeigenvaluesoftheintegraloperator(4).
DefinitionB.1(Entropynumberofaboundedlinearmap). ForaboundedlinearoperatorS : E F betweennormed
spacesE,F,forℓ N,theℓ-thentropynumberofS isdefinedas →
∈
(cid:110) (cid:111)
e (S)≜inf ϵ>0: s ,...,s S( )suchthatS( ) (cid:83)2ℓ−1 (s ,ϵ) .
ℓ ∃ 1 2ℓ−1 ∈ BE BE ⊂ i=1 BF i
Thefollowinglemmashowstherelationbetweencoveringnumbersandentropynumbers.
Lemma B.3 (Relation between covering number and entropy number). Suppose a kernel k is jointly continuous and
A Rdisbounded. Thenforanyϵ>0,
⊂
e ( (cid:44) ℓ (A)) ϵ.
⌈log 2Nk(A,ϵ)⌉+1 Hk|A
→
∞
≤
ProofofLem.B.3. First,theassumptionimpliesk isaboundedkernel,sobySteinwartandChristmann(2008,Lemma
A
|
4.23),theinclusion (cid:44) ℓ (A)iscontinuous. Bythedefinitionof (A,ϵ),byaddingarbitraryelementsintothe
Hk|A
→
∞ Nk|A
coverifnecessary,thereexistsa(k |A,A,ϵ)coverof
Bk|A
ofcardinality2⌈log 2(Nk|A(A,ϵ))⌉ ≥Nk|A(A,ϵ). Hence
e ( (cid:44) ℓ (A)) ϵ.
⌈log 2Nk|A(A,ϵ)⌉+1 Hk|A
→
∞
≤
Theclaimfollowssince (A,ϵ)= (A,ϵ)byLem.B.2.
Nk|A Nk
15DebiasedDistributionCompression
Proposition B.1 (ℓ ∞-entropy number bound for POLYGROWTH or LOGGROWTH k). Suppose a kernel k satisfies As-
sum.(α,β)-kernel. LetC >0denotetheconstantthatappearsintheAssum.(α,β)-kernel. Define
d
L k(r)≜ loC gd 2rβ. (6)
Thenforanyr >0andℓ Nthatsatisfiesℓ>L (r+1)+1,wehave
k
∈
(cid:16) (cid:17)1
 Lk ℓ( −r+ 11) α ifkisPOLYGROWTH(α,β), and
e ℓ( Hk|B2(r) (cid:44) →ℓ ∞( B2(r))) ≤exp(cid:18)
1
(cid:16)
ℓ−1
(cid:17) α1(cid:19)
ifkisLOGGROWTH(α,β).
− Lk(r+1)
ProofofProp.B.1. By Lem. B.3 and the fact that e is monotonically decreasing in ℓ by definition, if ℓ
ℓ
≥
log ( (r),ϵ)+1forsomeϵ>0,then
2Nk B2
e ( (cid:44) ℓ ( (r))) e ( (cid:44) ℓ ( (r))) ϵ. (7)
ℓ Hk|B2(r)
→
∞ B2
≤
⌈log 2Nk(B2(r),ϵ)⌉+1 Hk|B2(r)
→
∞ B2
≤
ForthePOLYGROWTHcase,byitsdefinition,theconditionℓ ≥log 2Nk( B2(r),ϵ)+1ismetifϵ ∈(0,1)and
(cid:16) (cid:17)1
ℓ Cd (1/ϵ)α(r+1)β +1 ϵ Lk(r+1) α .
≥ log2 ⇐⇒ ≤ ℓ−1
(cid:16) (cid:17)1
Hence(7)holdswithϵ= Lk(r+1) α,aslongasϵ (0,1),soℓneedstosatisfy
ℓ−1 ∈
(cid:16) (cid:17)1
1> Lk(r+1) α ℓ>L (r+1)+1.
ℓ−1 ⇐⇒ k
Similarly,fortheLOGGROWTHcase,theconditionℓ ≥log 2Nk( B2(r),ϵ)+1ismetifϵ ∈(0,1)and
(cid:18) (cid:16) (cid:17)1(cid:19)
ℓ Cd (log(1/ϵ)+1)α(r+1)β +1 ϵ exp 1 ℓ−1 α .
≥ log2 ⇐⇒ ≤ − Lk(r+1)
(cid:18) (cid:16) (cid:17)1(cid:19)
Hence(7)holdswithϵ=exp 1 ℓ−1 α ,aslongasϵ (0,1),soℓneedstosatisfy
− Lk(r+1) ∈
(cid:18) (cid:16) (cid:17)1(cid:19)
1>exp 1 ℓ−1 α ℓ>L (r+1)+1.
− Lk(r+1) ⇐⇒ k
Next, weshowthatwecantransferboundsonentropynumberstoobtainboundsfortheeigenvaluesofkernelmatrices,
which will become handy when we develop sub-quadratic-time algorithms in Sec. 3. We rely on the following lemma,
whichsummarizestherelevantfactsfromSteinwartandChristmann(2008,AppendixA).
LemmaB.4(Eigenvalueisboundedbyentropynumber). LetkbeajointlycontinuouskernelandPbeadistributionsuch
thatE x∼P[k(x,x)]< ,andrecallthatλ ℓ()denotestheℓ-thlargesteigenvalueofalinearoperator. Then,forallℓ N,
∞ · ∈
λ ℓ(T k,P) ≤4e2 ℓ(
Hk
(cid:44) →L2(P)).
ProofofLem.B.4. ForanyboundedlinearoperatorS : betweenHilbertspaces and ,wehavea (S)
1 2 1 2 ℓ
H → H H H ≤
2e (S),wherea istheℓ-thapproximationnumberdefinedinSteinwartandChristmann(2008,(A.29)).Recalltheoperator
ℓ ℓ
S∗ = (cid:44) 2(P)from(3),whichiscompact(inparticularbounded)byLem.B.1(a). Thus
k,P Hk
→L
s (S∗ )=a (S∗ ) 2e (S∗ ),
ℓ k,P ℓ k,P
≤
ℓ k,P
wherethefirstequalityfollowsfromtheparagraphbelowSteinwartandChristmann(2008,(A.29)))ands isℓ-thsingular
ℓ
numberofanoperator(SteinwartandChristmann,2008,(A.25)). ThenusingtheidentitiesmentionedunderSteinwartand
Christmann (2008, (A.25)) and Steinwart and Christmann (2008, (A.27)) and that all operators involved are compact by
Lem.B.1(b),wehave
λ ℓ(T k,P)=λ ℓ(S k∗ ,PS k,P)=s ℓ(S k∗ ,PS k,P)=s2 ℓ(S k∗ ,P) ≤4e2 ℓ(S k∗ ,P).
16DebiasedDistributionCompression
Thepreviouslemmaallowsustoboundeigenvaluesofkernelmatricesbyℓ -entropynumbers.
∞
PropositionB.2(Eigenvalueofkernelmatrixisboundedbyℓ -entropynumber). Letk beajointlycontinuouskernel.
∞
Define K ≜ k( , ) for the sequence of points = (x ,...,x ) Rd. For any w ∆ , recall the notation
n n n 1 n n−1
Sw =(cid:80) wS δ ,S Kw =diag(√w)Kdiag(√w),S andR =1+sup ⊂ x . Thenfor∈ allℓ N,
n i∈[n] i xi n i∈[n]∥ i ∥2 ∈
λ ℓ(Kw)( =i) λ ℓ(T k,Sw n)( ≤ii) 4e2 ℓ( Hk|B2(Rn−1) (cid:44) →ℓ ∞( B2(R n −1))). (8)
ProofofProp.B.2. Without loss of generality, we assume w > 0 for all i [n], since otherwise, we can consider a
i
∈
smallersetofpointsbyremovingtheoneswithzeroweights.
Proofofequality(i)fromdisplay(8) Notethat 2(Sw)isisometrictoRn.LetK ≜k( , )denotethekernelmatrix.
L n Sn Sn
TheactionofT k,Sw isgivenby,fori [n],
n ∈
(cid:80)
T k,Sw nf(x i)= j∈[n]w jk(x i,x j)f(x j),
so in matrix form, T k,Swf = Kdiag(w)f, and hence T k,Sw = Kdiag(w). If λ ℓ is an eigenvalue of Kdiag(w) with
n n
eigenvectorv ,then
ℓ
Kdiag(w)v =λ v diag(√w)Kdiag(w)v =λ diag(√w)v
ℓ ℓ ℓ ℓ ℓ ℓ
⇐⇒
diag(√w)Kdiag(√w)(diag(√w)v )=λ diag(√w)v ,
ℓ ℓ ℓ
⇐⇒
whereweusedw i >0foralli [n]. HencetheeigenspectrumofT k,Sw agreeswiththatofdiag(√w)Kdiag(√w).
∈ n
Proofofbound(ii)fromdisplay(8) ByLem.B.4,wehaveλ ℓ(T k,Sw n)
≤
4e2 ℓ( Hk|B2(Rn−1) (cid:44)
→
L2(Sw n)). Finally,using
Def. B.1, we have e ( (cid:44) 2(Sw)) e ( (cid:44) ℓ ( (R 1))) because Sw is supported in
2(R
n
1)andthefℓ acH tk th|B a2 t(Rn− L1 2) (P→
)
L ∞n for≤ anyPℓ .Hk|B2(Rn−1) → ∞ B2 n − n
B − ∥·∥ ≤∥·∥
Combiningthetoolsdevelopedsofar,wehavethefollowingcorollaryforboundingtheeigenvaluesofPOLYGROWTHand
LOGGROWTHkernelmatrices.
Corollary B.1 (Eigenvalue bound for POLYGROWTH or LOGGROWTH kernel matrix). Suppose a kernel k satisfies As-
sum.(α,β)-kernel. Let =(x ,...,x ) Rdbeasequenceofpoints. Foranyw ∆ ,usingthenotationL from
n 1 n n−1 k
S ⊂ ∈
(6),foranyℓ>L (R )+1,wehave
k n
 (cid:16) (cid:17)2
4 Lk ℓ( −R 1n) α POLYGROWTH(α,β) and
λ ℓ(Kw) (cid:18) (cid:16) (cid:17)1(cid:19)
≤4exp 2 2 ℓ−1 α LOGGROWTH(α,β).
− Lk(Rn)
ProofofCor.B.1. TheclaimfollowsbyapplyingProp.B.2andProp.B.1.
B.2.SpectraldecayofSteinkernels
The goal of this section is to show that a Stein kernel k satisfies Assum. (α,β)-kernel provided that the base kernel is
p
sufficientlysmoothandtoderivetheparametersα,β forPOLYGROWTHandLOGGROWTHcases.
ForaSteinkernelk withpreconditioningmatrixM,wedefine
p
S
p(r)≜max(cid:16)
1,sup
∥x∥
2≤r(cid:13)
(cid:13)M1/2
∇logp(x)(cid:13)
(cid:13)
2(cid:17)
. (9)
WestartbynotingausefulalternativeexpressionforaSteinkernelwhereweonlyneedaccesstothedensityviathescore
logp.
∇
PropositionB.3(AlternativeexpressionforSteinkernel). TheSteinkernelk hasthefollowingalternativeform:
p
k (x,y)= logp(x),M logp(y) k(x,y)+ logp(x),M k(x,y) +
p y
⟨∇ ∇ ⟩ ⟨∇ ∇ ⟩ (10)
logp(y),M k(x,y) +tr(M k(x,y)),
x x y
⟨∇ ∇ ⟩ ∇ ∇
where k(x,y)denotesthed dmatrix(∂ ∂ k(x,y)) .
∇x ∇y
×
xi yj i,j∈[d]
17DebiasedDistributionCompression
ProofofProp.B.3. Wecompute
(cid:80)
( (p(x)Mk(x,y)p(y))) = M (∂ p(x)k(x,y)p(y)+p(x)∂ k(x,y)p(y)).
∇x · j i∈[d] ij xi xi
(cid:80) (cid:0) (cid:1)
(p(x)Mk(x,y)p(y))= M ∂ p(x)∂ p(y)k(x,y)+∂ p(x)∂ k(x,y)p(y)
∇y ·∇x · i,j∈[d] ij xi yj xi yj
(cid:80) (cid:0) (cid:1)
+ M p(x)∂ p(y)∂ k(x,y)+p(x)∂ ∂ k(x,y)p(y) .
i,j∈[d] ij yj xi xi yj
Thefourtermsinthelastequationcorrespondtothefourtermsin(10).
In whatfollows, wewill makeuse ofa matrix-valued kernelK : Rd Rd Rd×d which generatesan RKHS of
K
× → H
vector-valuedfunctions. SeeCarmelietal.(2006)foranintroductiontovector-valuedRKHStheory.
Our next goal is to build a Hilbert-space isometry between the direct sum Hilbert space ⊕d and to represent
Hk Hkp
functionsin usingfunctionsfrom .
Hkp Hk
Lemma B.5 (Preconditioned matrix-valued RKHS from a scalar kernel). Let k : Rd Rd R be kernel and be
k
the corresponding RKHS. Let M Rd×d be an SPSD matrix. Consider the map ι :× ⊕d→ F(Rd,Rd) definH ed by
k
(f ,...,f ) [x M1/2(f (x)∈ ,...,f (x))], where ⊕d is the direct-sum HilbertH space o→ f d copies of Then ι
1 d 1 d k k
(cid:55)→ (cid:55)→ H H
isaHilbert-spaceisometryontoavector-valuedRKHS withmatrix-valuedreproducingkernelgivenbyK(x,y) =
K
H
k(x,y)M.
ProofofLem.B.5. Defineγ :Rd F(Rd, ⊕d)via
k
→ H
γ(x)(α)≜k(x, )M1/2α.
·
Wehave
(cid:13) (cid:13)
γ(x)(α) k(x, ) (cid:13)M1/2(cid:13) α ,
∥ ∥Hk⊕d ≤∥ · ∥k 2∥ ∥2
so γ(x) is bounded. Since γ(x) is also linear, we have γ(x) B(Rd, ⊕d). Let γ(x)∗ : ⊕d Rd denote the
k k
Hilbert-spaceadjointofγ(x). Thenforany(f ,...,f )
⊕∈d,α RdH
,wehave
H →
1 d k
∈H ∈
γ(x)∗(f ,...,f ),α = (f ,...,f ),γ(x)(α)
⟨
1 d
⟩ ⟨
1 d ⟩Hk⊕d
= (f ,...,f ),k(x, )M1/2α
⟨
1 d
·
⟩Hk⊕d
= (f (x),...,f (x)),M1/2α
1 d
⟨ ⟩
= M1/2(f (x),...,f (x)),α .
1 d
⟨ ⟩
Henceγ(x)∗(f ,...,f ) = M1/2(f (x),...,f (x)),soι(f ,...,f )(x) = γ(x)∗(f ,...,f ). ByCarmelietal.(2006,
1 d 1 d 1 d 1 d
Proposition2.4),weseethatιisapartialisometryfrom ⊕dontoavector-valuedRKHSspacewithvreproducingkernel
k
K(x,y)=γ(x)∗γ(y):Rd Rd. Forα Rd,previouH scalculationimplies
→ ∈
γ(x)∗γ(y)(α)=γ(x)∗(k(y, )M1/2α)=M1/2k(y,x)M1/2α=k(x,y)M.
·
Lemma B.6 (Stein operator is an isometry). Consider a Stein kernel k with base kernel k and preconditioning matrix
p
M. Then,theSteinoperator definedby (v)≜ 1 (pv)isanisometryfrom withK ≜kM to .
Sp Sp p∇· HK Hkp
Proof. ThisfollowsfromBarpetal.(2022,Theorem2.6)appliedtoK.
Theprevioustwolemmasshowthat ιisaHilbertspaceisometryfrom ⊕dto .Notethat (v)= logp,h +
Sp
◦
Hk Hkp Sp
⟨∇ ⟩
h. Hence,weimmediatelyhave
∇·
(cid:110) (cid:111)
= logp,M1/2f + (M1/2f):f =(f ,...,f ) ⊕d . (11)
Hkp
⟨∇ ⟩ ∇·
1 d ∈Hk
WenextbuildadivergenceRKHSwhichisoneofthesummandsusedtoform .
Hkp
18DebiasedDistributionCompression
Lemma B.7 (Divergence RKHS). Let k : Rd Rd R be a continuously differentiable kernel. Let M be an SPSD
matrix. Define ⊗2 (Mk):Rd Rd Rvia× →
∇ · × →
⊗2 (Mk)(x,y)≜ (Mk(x,y))=tr(M k(x,y)). (12)
y x x y
∇ · ∇ ·∇ · ∇ ∇
Then ⊗2 (Mk)isakernel,anditsRKHS hasthefollowingexplicitform
∇⊗2·(Mk)
∇ · H
(cid:110) (cid:111)
= = (M1/2f):f =(f ,...,f ) ⊕d , (13)
∇⊗2·(Mk) K 1 d k
H ∇·H ∇· ∈H
whereK =Mk. Moreover, : isanisometry.
K ∇⊗2·(Mk)
∇· H →H
ProofofLem.B.7. Firstofall,bySteinwartandChristmann(2008,Corollary4.36),everyf iscontinuouslydiffer-
k
∈H
entiable,so∂ f exists. ByLem.B.5, iswell-definedandtherightequalityin(13)holds.
xi ∇·HK
Defineγ :Rd F(R, )via
K
→ H
γ(x)(t)≜t(cid:80)d
∂ K(x, )e ,
i=1 xi · i
wheree RdistheithstandardbasisvectorinRd;byBarpetal.(2022,LemmaC.8)wehave∂ K(x, )e .Note
i
∈
xi
·
i ∈HK
that
(cid:13) (cid:13)
γ(x)(t) = t (cid:13)(cid:80)d ∂ K(x, )e (cid:13) ,
∥ ∥K | |(cid:13) i=1 xi · i(cid:13) K
soγ(x) B(R, ). Theadjointγ(x)∗ B( ,R)mustsatisfy,foranyh ,
K K K
∈ H ∈ H ∈H
(cid:68) (cid:69)
tγ(x)∗h= h,γ(x)(t) = h,t(cid:80)d ∂ K(x, )e =t h,
⟨ ⟩K i=1 xi · i K ∇·
where we used the fact (Barp et al., 2022, Lemma C.8) that, for c Rd, h , ∂ K(x, )c,h = c⊤∂ h(x). So
we find γ(x)∗(h) = h(x). By Carmeli et al. (2006,
Proposit∈
ion 2.4),
∈ theH mK ap⟨ Axi
:
· F⟩ (Rd,R)x di
efined by
K
∇ · H →
A(h)(x)=γ∗(x)(h)= h(x),i.e.,A= ,isapartialisometryfrom toanRKHS withreproducingkernel
K ∇·K
∇· ∇· H H
(cid:16) (cid:17)
γ(x)∗γ(y)= (cid:80)d ∂ K(x, )e (y)= K(x,y)= ⊗2 (Mk)(x,y).
∇· i=1 xi · i ∇y ·∇x · ∇ ·
The following lemma shows that we can project a covering of consisting of arbitrary functions to a covering using
k
B
functionsonlyin whileinflatingthecoveringradiusbyatmost2.
k
B
LemmaB.8(ProjectionofcoveringsintoRKHSballs). Letkbeakernel,A Rd beaset,andϵ > 0. Let beasetof
⊂ C
functionssuchthatforanyf ,thereexistsg suchthat f g ϵ. Then
∈Bk ∈C ∥ − ∥∞,A ≤
(A,2ϵ) .
k
N ≤|C|
Proof. Wewillbuilda(k,A,2ϵ)covering ′ asfollows. Foranyh ,ifthereexistsh′ with h′ h ϵ,
C ∈ C ∈ Bk ∥ − ∥∞,A ≤
thenweincludeh′ in ′. Byconstruction, ′ . Then,foranyf ,byassumption,thereexistsg suchthat
k
C |C | ≤ |C| ∈ B ∈ C
f g ϵ. Byconstruction,thereexistsg′ ′suchthat g′ g ϵ. Thus
∥ − ∥∞,A ≤ ∈C ∥ − ∥∞,A ≤
f g′ f g + g g′ 2ϵ.
∥ − ∥∞,A ≤∥ − ∥∞,A ∥ − ∥∞,A ≤
HenceC′isa(k,A,2ϵ)covering.
Wearenowreadytoboundthecoveringnumbersofk bythoseofkand ⊗2 (Mk). Ourkeyinsighttowardsthisendis
p
∇ ·
thatanyelementin canbedecomposedasasumoffunctionsoriginatedfrom andafunctionfromthedivergence
Hkp Hk
RKHS .
∇⊗2·(Mk)
H
19DebiasedDistributionCompression
LemmaB.9(UpperboundingcoveringnumberofSteinkernelwiththatofitsbasekernel). Letk beaSteinkernelwith
p
densitypandpreconditioningmatrixM. ForanyA Rd,ϵ ,ϵ >0,
1 2
⊂
(A,ϵ) (A,ϵ )d (A,ϵ ),
Nkp ≤Nk 1 N∇⊗2·(Mk) 2
(cid:13) (cid:13)
forϵ=2(√dϵ 1sup x∈A(cid:13)M1/2 ∇logp(x)(cid:13)+ϵ 2).
ProofofLem.B.9. Let be a (k,A,ϵ ) covering and be a ( ⊗2 (Mk),A,ϵ ) covering. Define b ≜
k 1 ∇⊗2·(Mk) 2
C C ∇ ·
M1/2 logp. Form
∇
(cid:110) (cid:111)
≜ b,f˜ +g˜:f˜=(f˜,...,f˜) ( )d,g˜ F(Rd,R).
1 d k ∇⊗2·(Mk)
C ⟨ ⟩ ∈ C ∈C ⊂
Then
|C| ≤
|Ck |d(cid:12) (cid:12) C∇⊗2·(Mk)(cid:12) (cid:12). Let K ≜ kM. For any h
∈
Bkp, by (11), we can find f = (f 1,...,f d)
∈
Hk⊕d with
f suchthat
i k
∈H
h= ι(f)= logp,M1/2f + (M1/2f)= b,f + (M1/2f).
p
S ◦ ⟨∇ ⟩ ∇· ⟨ ⟩ ∇·
Sinceιand areisometries,wehavef . Since,foreachi,
Sp ∈BHk⊕d
(cid:113)
f (cid:80)d f 2 = f 1,
∥ i ∥k ≤ j=1∥ j ∥k ∥ ∥Hk⊕d ≤
wehavef . ByLem.B.7, : isalsoanisometry,so (M1/2f) . Thusthere
i k K ∇⊗2·(Mk) ∇⊗2·(Mk)
∈ B ∇· H → H ∇· ∈ B
existf˜ foreachiandg˜ suchthat
i k ∇⊗2·(Mk)
∈C ∈C
(cid:13) (cid:13) (cid:13)f i −f˜ i(cid:13) (cid:13) (cid:13) ∞,A ≤ϵ 1, (cid:13) (cid:13) ∇·(M1/2f) −g˜(cid:13) (cid:13) ∞,A ≤ϵ 2.
Let
h˜(x)≜ b,f˜ +g˜ .
⟨ ⟩ ∈C
Thenforx A,
∈
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)h(x) h˜(x)(cid:12)=(cid:12) b(x),f(x) f˜(x) + (M1/2f(x)) g˜(x)(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
− ⟨ − ⟩ ∇· −
(cid:113)
≤∥b(x)
∥
(cid:80)d i=1(f i(x) −f˜ i(x))2+(cid:12) (cid:12) ∇·(M1/2f(x)) −g˜(x)(cid:12) (cid:12)
√dϵ b(x) +ϵ .
1 2
≤ ∥ ∥
Hence
(cid:13) (cid:13)
(cid:13)h h˜(cid:13) √dϵ sup b(x) +ϵ ≜ϵ .
(cid:13) − (cid:13) ∞,A ≤ 1 x∈A∥ ∥ 2 3
Notethat thatweconstructedisnotnecessarilycontainedin . ByLem.B.8,wecangeta(k ,A,2ϵ )coveringand
C
Bkp p 3
wearedone.
Corollary B.2 (Log-covering number bound for Stein kernel). Let k be a Stein kernel and A Rd. For any r > 0,
p
⊂
ϵ>0,
log Nkp(A,ϵ) ≤dlog Nk(cid:16) A, 4√ dSϵ p(r)(cid:17) +log N∇⊗2·(Mk)(cid:0) A, 4ϵ(cid:1) ,
whereS isdefinedin(9).
p
Proof. ThisisdirectfromLem.B.9withϵ 1 = 4√ dSϵ p(r),ϵ 2 = 4ϵ.
20DebiasedDistributionCompression
B.2.1.CASEOFDIFFERENTIABLEBASEKERNEL
DefinitionB.2(s-timescontinuouslydifferentiablekernel). Akernelkiss-timescontinuouslydifferentiablefors Nif
allpartialderivatives∂α,αkexistandarecontinuousforallmulti-indicesα Ndwith α s. ∈
∈ 0 | |≤
Proposition B.4 (Covering number bound for k with differentiable base kernel). Suppose k is a Stein kernel with an
p p
s-times continuously differentiable base kernel k for s 2. Then there exist a constant C > 0 depending only on
d
≥
(s,d,k,M)suchthatforanyr >0,ϵ (0,1),
∈
log ( (r),ϵ) C rdSd/s (r)(1/ϵ)d/(s−1).
Nkp B2
≤
d p
ProofofProp.B.4. Since k is s-times continuously differentiable, the divergence kernel ⊗2 (Mk) is (s 1)-times
∇ · −
continuouslydifferentiable.ByDwivediandMackey(2022,Proposition2(b)),thereexistsconstantsc ,c dependingonly
1 2
on(s,d,k,M)suchthat,foranyr >0,ϵ ,ϵ >0,
1 2
log ( (r),ϵ ) c rd(1/ϵ)d/s,
k 2 1 1
N B ≤
log ( (r),ϵ ) c rd(1/ϵ)d/(s−1).
∇⊗2·(Mk) 2 2 2
N B ≤
ByCor.B.2withA= (r),wehave,foranyr >0andϵ (0,1),
2
B ∈
log ( (r),ϵ) c drd(4√dS (r))d/s(1/ϵ)d/ϵ+c rd(4/ϵ)d/(s−1)
Nkp B2
≤
1 p 2
C rdSd/s (r)(1/ϵ)d/(s−1)
d p
≤
forsomeC >0dependingonlyon(s,d,k,M).
d
B.2.2.CASEOFRADIALLYANALYTICBASEKERNEL
ForasymmetricpositivedefiniteM Rd×d,wedefine,forx Rd,
∈ ∈
x ≜√x⊤M−1x.
∥ ∥M
Definition B.3 (Radially analytic kernel). A kernel k is radially analytic if k satisfies k(x,y) = κ( x y 2 ) for a
symmetric positive definite matrix M Rd×d and a function κ : R R real-analytic everywhere∥ with− co∥ nM vergence
≥0
∈ →
radiusR >0suchthatthereexistsaconstantC >0forwhich
κ κ
(cid:12) (cid:12)
(cid:12)1κ(j) (0)(cid:12) C (2/R )j, forallj N , (14)
(cid:12)j! + (cid:12) ≤ κ κ ∈ 0
whereκ(j)indicatesthej-thright-sidedderivativeofκ.
+
ExampleB.1(Gaussiankernel). ConsidertheGaussiankernelk(x,y)=κ( ∥x −y ∥2 M)withκ(t)=e− 2σt 2 whereσ >0.
Notetheexponentialfunctionisreal-analyticeverywhere,andsoisκ. Sinceκ(t)=(cid:80)∞ (−t/2σ2)j ,wefind 1κ(j)(0)=
j=0 j j!
(−1)j . Hence(14)holdswithC =1andR =2inf (j(2σ2)j)1/j =4σ2.
j(2σ2)j κ κ j≥0
ExampleB.2(IMQkernel). Considertheinversemultiquadrickernelk(x,y) = κ( x y 2 )withκ(t) = (c2 +t)−β
∥ − ∥M
where c,β > 0. By Sun and Zhou (2008, Example 3), κ is real-analytic everywhere with C = c−2β(2β +1)β+1 and
κ
R =c2.
κ
Asimplecalculationyieldsthefollowinglemma.
Proposition B.5 (Expression for k with a radially analytic base kernel). Suppose a Stein kernel k has a symmetric
p p
positivedefinitepreconditioningmatrixandabasekernelk(x,y)=κ( x y 2 )whereκistwice-differentiable. Then
∥ − ∥M
k (x,y)= logp(x),M logp(y) κ( x y 2 )
p ⟨∇ ∇ ⟩ ∥ − ∥M −
2κ′ ( x y 2 ) x y, logp(x) logp(y) (15)
+ ∥ − ∥M ⟨ − ∇ −∇ ⟩−
4κ′′( x y 2 ) x y 2 2dκ′ ( x y 2 ).
+ ∥ − ∥M ∥ − ∥M − + ∥ − ∥M
Inparticular,
k p(x,x)=(cid:13) (cid:13)M1/2 ∇logp(x)(cid:13) (cid:13)2 2κ(0) −2dκ′ +(0).
21DebiasedDistributionCompression
ProofofProp.B.5. Fromk(x,y)=κ( x y 2 )=κ((x y)⊤M−1(x y)),wecompute,using(12),
∥ − ∥M − −
k(x,y)= 2κ′ ( x y 2 )M−1(x y)
∇y − + ∥ − ∥M −
k(x,y)= 2κ′ ( x y 2 )M−1 4κ′′( x y 2 )M−1(x y)((x y)M−1)⊤
∇x ∇y − + ∥ − ∥M − + ∥ − ∥M − −
⊗2 (Mk)(x,y)=tr(M k(x,y))= 4κ′′( x y 2 ) x y 2 2dκ′ ( x y 2 ). (16)
∇ · ∇x ∇y − + ∥ − ∥M ∥ − ∥M − + ∥ − ∥M
Theform(15)thenfollowsfromapplyingProp.B.3.
Wenextshowthatthedivergencekernel ⊗2 (Mk)isradiallyanalyticgiventhatkis.
∇ ·
LemmaB.10(Convergenceradiusofdivergencekernel). Letkbearadiallyanalytickernelwiththecorrespondingreal-
analyticfunctionκ,convergenceradiusR withconstantC ,andasymmetricpositivedefinitematrixM. Then
κ κ
⊗2 (Mk)(x,y)=κ ( x y 2 ),
∇ · ∇⊗2·(Mk) ∥ − ∥M
whereκ :R Risthereal-analyticfunctiondefinedas
∇⊗2·(Mk) ≥0
→
κ (t)≜ 4κ′′(t)t 2dκ′ (t).
∇⊗2·(Mk) − + − +
Moreover,κ hasconvergenceradiuswithconstant
∇⊗2·(Mk)
R κ ∇⊗2·(Mk) = 4R d+κ 8, C κ ∇⊗2·(Mk) =4dC κ/R κ.
ProofofLem.B.10. Thefirststatementregardingtheformofκ directlyfollowsfrom(16). Next,iterativediffer-
∇⊗2·(Mk)
entiationyields,forj N ,
0
∈
κ(j) (t)= (2d+4j)κ(j+1) (t) 4κ(j+2) (t)t.
∇⊗2·(Mk) − + − +
Thus
(cid:12) (cid:12)
(cid:12)1κ(j) (0)(cid:12)= 2d+4jκ(j+1) (0)
(cid:12)j! ∇⊗2·(Mk) (cid:12) j! +
=
(2d+4j)(j+1)κ(j+1)
(0)
(j+1)! +
(2d+4j)(j+1)C (2/R )j+1. (17)
κ κ
≤
Forj 1,
≥
(cid:12) (cid:12)
(cid:12)1κ(j) (0)(cid:12) (2C /R )(cid:0) ((2d+4j)(j+1))1/j2/R (cid:1)j
(cid:12)j! ∇⊗2·(Mk) (cid:12) ≤ κ κ κ
(2C /R )((2(2d+4) 2/R )j.
κ κ κ
≤ ·
whereweusedthefactthat((2d+4j)(j+1))1/j isdecreasinginj.Forj =0,(17)isjust2dC 2/R .Henceκ
κ κ ∇⊗2·(Mk)
·
isanalyticwithC κ ∇⊗2·(Mk) =4dC κ/R κandR κ ∇⊗2·(Mk) = 4R d+κ 8.
Wewillusethefollowinglemmarepeatedly.
Lemma B.11 (Covering number of radially analytic kernel with M-metric). Let k be a radially analytic kernel with
0
k (x,y) = κ( x y 2 ). ForanysymmetricpositivedefiniteM Rd×d,considertheradiallyanalytickernelk(x,y) ≜
0 ∥ − ∥2 ∈
κ( x y 2 ). ThenforanyA Rdandϵ>0,wehave
∥ − ∥M ⊂
(M−1/2(A),ϵ)= (A,ϵ).
Nk Nk0
Inparticular,foranyr >0,
( (r),ϵ) ( (r M1/2 ),ϵ).
Nk B2 ≤Nk0 B2
∥
∥2
22DebiasedDistributionCompression
Proof. Notethatk(x,y)=k (M−1/2x,M−1/2y).ByPaulsenandRaghupathi(2016,Theorem5.7), = f M−1/2 :
0 k
H { ◦
f , andmoreover = f M−1 : f . Let bea(k ,A,ϵ)covering. Form = h M−1/2 : h
∈
Hk0} Bk
{ ◦ ∈
Bk0} C0 0
C { ◦ ∈
. Foranyelementf M−1/2 wheref ,thereexistsh suchthat f h ϵ. Thus
C0 }⊂Bk ◦ ∈Bk ∈Bk0 ∈C0 ∥ − ∥∞,A ≤
(cid:13) (cid:13)
(cid:13)f M−1/2 h M−1/2(cid:13) = f h ϵ.
◦ − ◦ ∞,M−1/2(A) ∥ − ∥∞,A ≤
Thus (M−1/2(A),ϵ) (A,ϵ). ByconsideringM−1inplaceofM,wegetourdesiredequality.
Nk ≤Nk0
Forthesecondstatement,bylettingA=M1/2 (r),wehave
2
B
( (r),ϵ)= (M1/2 (r),ϵ) ( (r M1/2 ),ϵ),
Nk B2 Nk0 B2 ≤Nk0 B2
∥
∥2
whereweusethefactthatM1/2 (r) (r M1/2 ).
2 2 2
B ⊂B ∥ ∥
Inthenextlemma,werephrasetheresultfromSunandZhou(2008,Theorem2)forboundingthecoveringnumberofa
radiallyanalytickernel.
Lemma B.12 (Covering number bound for radially analytic kernel). Let k be a radially analytic kernel with k(x,y) =
κ( x y 2 ). Then,thereexistapolynomialP(r)ofdegree2dandaconstantC dependingonlyon(κ,d)suchthatfor
∥ − ∥2
anyr >0,ϵ (0,1/2),
∈
log ( (r),ϵ) P(r)(log(1/ϵ)+C)d+1.
k 2
N B ≤
ProofofLem.B.12. Let R ,C denote the constants for κ as in (14). By and Sun and Zhou (2008, Theorem 2) with
κ κ
R=1,D =2r,andLem.B.2,forϵ (0,1/2),wehave
∈
log ( (r),ϵ) N (
(r),r†/2)(cid:0) 4log(1/ϵ)+2+4log(cid:0)
16√C
+1(cid:1)(cid:1)d+1
,
k 2 2 2 κ
N B ≤ B
where r† = min(√ Rκ,(cid:112) R +(2r)2 2r), and N ( (r),r†/2) is the covering number of (r) as a subset of Rd,
2d κ − 2 B2 B2
whichcanbefurtherboundedby(Wainwright,2019,(5.8))
N ( (r),r†/2) (cid:0) 1+ 4r(cid:1)d .
2 B2 ≤ r†
Ifr† =(cid:112) R +(2r)2 2r,then r = r = r(√Rκ+(2r)2+2r) r(√ Rκ+4r) whichisaquadraticpolynomial
κ − r† √Rκ+(2r)2−2r Rκ ≤ Rκ
inr. HenceforaconstantC >0andapolynomialP(r)ofdegree2dthatdependonlyon(κ,d),wehavetheclaim.
PropositionB.6(Coveringnumberboundfork withradiallyanalyticbasekernel). Supposek isaSteinkernelwitha
p p
preconditioning matrix M and a radially analytic base kernel k based on a real-analytic function κ. Then there exist a
constantC >0andapolynomialP(r)ofdegree2ddependingonlyon(κ,d,M)suchthatforanyr >0,ϵ (0,1),
∈
(cid:16) (cid:17)d+1
log ( (r),ϵ) logSp(r) +C P(r). (18)
Nkp B2 ≤ ϵ
ProofofProp.B.6. Recallk(x,y)=κ( x y 2 ). Considerk (x,y)≜κ( x y 2 ). Forϵ (0,1/2),byLem.B.11,
∥ − ∥M 0 ∥ − ∥2 1 ∈
wehave
log ( (r/ M1/2 ),ϵ ) log ( (r),ϵ /2).
Nk B2
∥
∥2 1
≤
Nk0 B2 1
ThusbyLem.B.12,thereexistsapolynomialP (r)ofdegree2dandaconstantC dependingonlyon(κ,d,M)suchthat
k k
log ( (r),ϵ ) P (r)(log(1/ϵ )+C )d+1
k 2 1 k 1 k
N B ≤
Similarly, for ϵ (0,1/2), by Lem. B.10 and Lem. B.12, we have, for a constant C > 0 and a polynomial
2 ∇⊗2·(Mk)
∈
P (r)ofdegree2dthatdependonlyon(κ,d,M),
∇⊗2·(Mk)
log ( (r),ϵ ) P (r)(log(1/ϵ )+C )d+1.
∇⊗2·(Mk) 2 2 ∇⊗2·(Mk) 2 ∇⊗2·(Mk)
N B ≤
23DebiasedDistributionCompression
Foragivenϵ
∈
(0,1),letϵ 1 = 4√ dSϵ
p(r)
andϵ 2 = 4ϵ. ThensinceS p
≥
1,wehaveϵ 1,ϵ 2
∈
(0,1/2). ByCor.B.2with
A= (r),weobtain,foraconstantsC >0andapolynomialP(r)ofdegree2dthatdependonlyon(κ,d,M),
2
B
log ( (r),ϵ) P(r)(log(1/ϵ)+logS (r)+C)d+1.
Nkp B2
≤
p
Hence(18)isshown.
WhenlogS (r)growspolynomiallyinr,weapplyProp.B.6toimmediatelyobtainthefollowing.
p
CorollaryB.3. UndertheassumptionofProp.B.6,supposeS (r)=O(poly(r)).Thenforanyδ >0,thereexistsC >0
p d
suchthat
log ( (r),ϵ) C log(e/ϵ)d+1 (r+1)2d+δ.
Nkp B2
≤
d
ProofofCor.B.3. ThisimmediatelyfollowsfromProp.B.6byusingδ >0toabsorbthelogS (r)=O(rδ)term.
p
B.2.3.PROOFOFPROP.1: STEINKERNELGROWTHRATES
This follows from Prop. B.4 and Cor. B.3, and by noticing that if sup logp(x) is bounded by a degree d
∥x∥ 2≤r∥∇ ∥2 ℓ
polynomial,thensois
(cid:13) (cid:13) (cid:13) (cid:13)
S p(r)=sup
∥x∥
2≤r(cid:13)M1/2 ∇logp(x)(cid:13)
2
≤(cid:13)M1/2(cid:13) 2sup
∥x∥
2≤r∥∇logp(x) ∥2.
C.ADebiasingBenchmark
C.1.MMDofunbiasedi.i.d.samplepoints
Westartbyshowingthatsequenceofnpointssampledi.i.d.fromPachievesΘ(n−1)squaredMMD toPinexpectation.
kP
PropositionC.1(MMDofunbiasedi.i.d.samplepoints). LetkP beakernelsatisfyingAssum.1withp 1. Let
n
=
(x ) beni.i.d.samplesfromP. Then ≥ S
i i∈[n]
E[MMD (S ,P)2]= E x∼P[kP(x,x)].
kP n n
ProofofProp.C.1. Wecompute
E[MMD kP(S n,P)2]=E[(cid:80)
i,j∈[n]
n1 2kP(x i,x j)]= n1
2
(cid:80) i,j∈[n]E[kP(x i,x j)]= n1E[kP(x 1,x 1)],
whereweusedthefactthatkPismean-zerowithrespecttoPandtheindependenceofx i,x
j
fori=j.
̸
C.2.ProofofThm.1: Debiasingviasimplexreweighting
We make use of the self-normalized importance sampling weights wSNIS = dP (x )/(cid:80) dP (x ) for j [n] in our
j dQ j i∈[n] dQ i ∈
proofs. Noticethat(wSNIS,...,wSNIS)⊤ ∆ andhence
1 n ∈ n−1
MMD MMD (wSNISδ ,P)= ∥(cid:80)n i=1 dd QP(xi)kP(xi,·)∥kP = ∥ n1 (cid:80)n i=1 dd QP(xi)kP(xi,·)∥kP.
OPT ≤ kP i xi (cid:80)n i=1 dd QP(xi) n1 (cid:80)n i=1 dd QP(xi)
IntroducetheboundedinprobabilitynotationX =O (g )tomeanPr(X /g >C ) ϵforalln N foranyϵ>0.
n p n n n ϵ ϵ
| | ≤ ≥
ThenweclaimthatundertheconditionsassumedinThm.1,
∥n1 (cid:80)n
i=1
dd QP (x i)kP(x i, ·)
∥kP
=O p(n− 21) and n1 (cid:80)n
i=1
dd QP (x i) →1almostsurely, (19)
so that by Slutsky’s theorem (Wellner et al., 2013, Ex. 1.4.7), we have MMD
OPT
= O p(n− 21) as desired. We prove the
claims in (19) in two main steps: (a) first, we construct a weighted RKHS and then (b) establish a central limit theorem
(CLT)thatallowsustoconcludebothclaimsfrom(19)
24DebiasedDistributionCompression
ConstructingaweightedandseparableRKHS DefinethekernelkQ(x,y)≜ dd QP (x)kP(x,y) dd QP (y)withHilbertspace
HkQ
= dd QP
HkP
and the elements ξ
i
≜ kQ(x i, ·) = dd QP (x i)kP(x i, ·) dd QP ( ·)
∈
HkQ
for each i
∈
N. By Paulsen and
Raghupathi (2016, Prop. 5.20), any element in is represented by dPf for some f and moreover, f dPf
HkQ dQ
∈
HkP
(cid:55)→
dQ
preserves inner product between the two RKHSs, i.e., f,g = dPf, dPg for f,g , which in turn implies
⟨ ⟩kP ⟨dQ dQ ⟩kQ ∈ HkP
f = dPf . Asaresult,wealsohavethat
∥
∥kP ∥dQ ∥kQ
∥n1 (cid:80)n
i=1
dd QP (x i)kP(x i, ·)
∥kP
= ∥n1 (cid:80)n
i=1
dd QP (x i)kP(x i, ·) dd QP ( ·)
∥kQ
= ∥n1 (cid:80)n i=1ξ
i
∥kQ. (20)
Since HkP isseparable,thereexistsadensecountablesubset(f n) n∈N ⊂HkP. Forany dd QPf ∈HkQ,thereexists {n k }k∈N
suchthatlim f f =0. Since dPf dPf = dP (f f) = f f duetoinner-product
k→∞ ∥ nk − ∥kP ∥dQ nk − dQ ∥kQ ∥dQ nk − ∥kQ ∥ nk − ∥kP
preservation,wethushavelim k→∞ ∥dd QPf nk
−
dd QPf ∥kQ = lim k→∞ ∥f nk −f ∥kP0,so( dd QPf n) n∈N isdensein HkQ,show-
ingthat isseparable.
HkQ
H irra er dr ui cs ir be lecu gr er oe mn ec te rio cf alt lh ye ec rgh oa din ic( Mx i a) ri k∈ oN vcL hae it nµ w1 id then so tat te ioth ne ard yis dt ir si tb ru ibti uo tn ioo nf Qx ,1 i. tS isin ac le soS p∞ os= itiv( ex i () M∞ i= e1 ynis aa ndho Tm wo eg ee dn iee ,o 2u 0s 1ϕ 2-
,
Ch.10)bydefinitionandaperiodicbyDoucetal.(2018,Lem.9.3.9). Moreover,since isϕ-irreducible,aperiodic,and
∞
S
geometricallyergodicinthesenseofGallegos-Herradaetal.(2023,Thm.1)andµ isabsolutelycontinuouswithrespect
1
toP,wewillassume,withoutlossofgenerality,that isHarrisrecurrent(MeynandTweedie,2012,Ch.9),since,by
∞
S
Qin(2023,Lem.9), isequaltoageometricallyergodicHarrischainwithprobability1.
∞
S
CLTfor √1 n(cid:80)n i=1ξ
i
Wenowshowthat √1 n(cid:80)n i=1ξ iconvergestoaGaussianrandomelementtakingvaluesin HkQ. We
separatetheproofintwoparts: firstwhentheinitialdistributionµ =Qandnextwhenµ =Q.
1 1
̸
Case1: µ = Q Whenµ = Q, isastrictlystationarychain. ByBradley(2005,Thm.3.7and(1.11)),since is
1 1 ∞ ∞
geometricallyergodic,itsstrongmiS xingcoefficients(α˜ i) i∈Nsatisfyα˜
i
CρiforsomeC >0andρ [0,1)andalliS N.
≤ ∈ ∈
Sinceeachξ i isameasurablefunctionofx i,thestrongmixingcoefficients(α i) i∈N of(ξ i) i∈N satisfyα i α˜ i Cρi for
eachi N. Consequently,(cid:80) i2/δα < forδ =2q 2>0. Notethatwealsohave ≤ ≤
∈
i∈N i
∞ −
E z∼Q[ ∥kQ(z, ·) ∥2 k+ Qδ]=E z∼Q[kQ(z,z)q]=E z∼Q[ dd QP (z)2qkP(z,z)q]=E x∼P[ dd QP (x)2q−1kP(x,x)q]< ∞,
E xi∼Q[ξ i] = E xi∼P[kP(x i, ·)] = 0 and that
HkQ
is separable. Since
S∞
is a strictly stationary chain, we conclude that
( Mξ i e) ri l∈ evN e`i ds ea es ttr aic l.tl (y 1s 9t 9at 7i ,on Ca or ry .c 1e )n ,t ae nre dd hs ee nq cu een (cid:80)ce nof
H
ξ
/k √Q- nva clu oe nd vera rgn ed som inv da ir si ta rib bl ues tis oa nti ts ofy ain Gg at uh se sc iao nnd ri at nio dn os mne ee ld emed et no ti tn av ko ink ge
i=1 i
valuesin .
HkQ
Case2: µ = Q Since ispositiveHarrisand(cid:80)n ξ /√nsatisfiesaCLTfortheinitialdistributionµ = Q,Meyn
andTweed1 ie̸ (2012,Prop.S 1∞ 7.1.6)impliesthat(cid:80)n ξi /= √1 ni alsosatisfiesthesameCLTforanyinitialdistrib1
utionµ .
i=1 i 1
Putting the pieces together for (19) Since, for any initial distribution for x 1, the sequence
((cid:80)n
i=1ξ i/√n) n∈N con-
verges in distribution and that is separable and (by virtue of being a Hilbert space) complete, Prokhorov’s theorem
(Billingsley,2013,Thm.5.2)imH pk lQ
iesthatthesequenceisalsotight,i.e.,
(cid:80)n
ξ /√n=O (1). Consequently,
∥ i=1 i ∥kQ p
∥n1 (cid:80)n i=1 dd QP (x i)kP(x i, ·) ∥kP ( =20) ∥n1 (cid:80)n i=1ξ i ∥kQ = √1 n ·∥(cid:80) √n i= n1ξi ∥kQ =O p(n− 21),
as desired for the first claim in (19). Moreover, the strong law of large numbers for positive Harris chains (Meyn and
Tweedie,2012,Thm.17.0.1(i))impliesthat n1 (cid:80)
i∈[n]
dd QP (x i)convergesalmostsurelytoE z∼Q[ dd QP (z)] = 1asdesiredfor
thesecondclaimin(19).
C.3.ProofofThm.2: Better-than-i.i.d.debiasingviasimplexreweighting
WestartwithThm.C.1,provedinApp.C.4,thatboundsMMD intermsoftheeigenvaluesoftheintegraloperatorof
OPT
thekernelkP. OurproofmakesuseofaweightconstructionfromLiuandLee(2017, Theorem3.2), butisanon-trivial
generalizationoftheirproofaswenolongerassumeuniformboundsontheeigenfunctions,andinsteadleveragetruncated
variationsofBernstein’sinequality(Lems.C.2andC.3)toestablishsuitableconcentrationbounds.
25DebiasedDistributionCompression
TheoremC.1(Debiasingviai.i.d.simplexreweighting). ConsiderakernelkPsatisfyingAssum.1withp=2.Let(λ ℓ)∞
ℓ=1
b we itt hh le ad we Qcre sa us ci hng ths ae tq Pue isnc ae bso of le ui tg ee lynv ca ol nu te ins uo of uT sk wP, iP thd re efi sn pe ed ctin to(4 Q). aL ne dt Sdn Pbeaseq Mue fn oc re so of mn
e∈
M2 >N 0i. .i. Fd. ur tha en rd mo om rev ,a ar sia sub mle es
∥dQ ∥∞
≤
thereexistconstantsδ n,B
n
>0suchthatPr( kP
n
>B n)<δ n. ThenforallL Nsuchthatλ
L
>0,wehave
∥ ∥ ∈
E[MMD2 kP(Sw nOPT,P)]
≤
8 nM (cid:16) 2 nM E x∼P[ λk LP2(x,x)] +(cid:80) ℓ>Lλ ℓ(cid:17) +ϵ nE[k P2(x 1,x 1)], (21)
where
ϵ2 ≜nexp(cid:16) −3n (cid:17) +2exp(cid:0) −n (cid:1) +2exp(cid:16) n (cid:17) +δ . (22)
n 16MBn/λL 16M2 −64M2(E x∼P[kP(x,x)]+Bn/12)/λL n
Given Thm. C.1, Thm. 2 follows, i.e., we have E[MMD2 (SwOPT,P)] = o(n−1), as long as we can show (i)
kP n
E[k2(x ,x )] < , whichinturnholdswhenq > 3asassumedinThm.2, and(ii)findsequences(B )∞ , (δ )∞ ,
P 1 1 ∞ n n=1 n n=1
and(L n)∞ n=1suchthatPr( ∥kP
∥n
>B n)<δ nforallnandthefollowingconditionsaremet:
(a)
E x∼P[kP2(x,x)]
=o(n);
λLn
(b) Bn =O(nβ),forsomeβ <1;
λLn
(cid:80)
(c) λ =o(1);
ℓ>Ln ℓ
(d) δ =o(n−2).
n
WenowproceedtoestablishtheseconditionsundertheassumptionsofThm.2.
Condition (d) By the de La Valle´e Poussin Theorem (Chandra, 2015, Thm. 1.3) applied to the Q-integrable function
x kP(x,x)q (which is a uniformly integrable family with one function), there exists a convex increasing function G
(cid:55)→
suchthatlim
t→∞
G( tt) = ∞andE[G(kP(x 1,x 1)q)]< ∞. Thus,
Pr(cid:0) kP(x 1,x 1)>n3/q(cid:1) =Pr(cid:0) kP(x 1,x 1)q >n3(cid:1) =Pr(cid:0) G(kP(x 1,x 1))q >G(n3)(cid:1)
E[G(kP(x1,x1))q] =o(n−3),
≤ G(n3)
wherethelaststepuseslim G(t) = . Hencebytheunionbound,
t→∞ t ∞
Pr(cid:0) kP
n
>n3/q(cid:1) =Pr(cid:0) max i∈[n]kP(x i,x i)>n3/q(cid:1) nPr(cid:0) kP(x 1,x 1)>n3/q(cid:1) =o(n−2).
∥ ∥ ≤
Hence if we set B = nτ for τ ≜ 3/q < 1, there exists (δ )∞ such that δ = o(n−2). This fulfills (d) and that
n n n=1 n
Pr( kP
n
>B n)<δ n.
∥ ∥
Toproveremainingconditions, withoutlossofgenerality, weassumethatλ > 0forallℓ N, sinceotherwisewecan
ℓ
(cid:80) ∈
chooseL tobe,foralln,thelargestℓsuchthatλ >0. Then λ =0andallotherconditionsaremet.
n ℓ ℓ>Ln Ln
(cid:80)
Condition(c) IfL ,then(c)isfulfilledsince λ < ,whichfollowsfromLem.B.1(d)andthat
n →∞ ℓ ℓ ∞
(cid:80) ℓλ
ℓ
=(cid:80)∞ ℓ=1λ iE x∼P[ϕ ℓ(x)2]=(cid:80)∞ ℓ=1λ iE x∼P[ϕ ℓ(x)2]=E x∼P[(cid:80)∞ ℓ=1λ iϕ ℓ(x)2]=E x∼P[kP(x,x)]< ∞.
Conditions(a)and(b) Notethatthecondition(a)issubsumedby(b)sinceE x∼P[k P2(x,x)] < . Itremainstochoose
∞
(L n)∞
n=1
tosatisfy(b)suchthatlim n→∞L
n
= ∞. DefineL
n
≜max {ℓ ∈N:λ
ℓ
≥nτ− 21 }. ThenL
n
iswell-definedfor
n ≥( λ1 1)1−2 τ,sinceforsuchnwehaveλ
1
≥nτ− 21. Henceforn ≥( λ1 1)1−2 τ,wehave
λB Ln
n ≤
nn τ−τ
21
=nτ+ 21,
so(b)issatisfiedwithβ = τ+ 21 <1. Sinceτ <1,L nisnon-decreasinginnandnτ− 21 decreasesto0. Sinceeachλ
ℓ
>0,
wethereforehavelim L = .
n→∞ n
∞
26DebiasedDistributionCompression
C.4.ProofofThm.C.1: Debiasingviai.i.d.simplexreweighting
WewillslowlybuilduptowardsprovingThm.C.1. FirstnoticeE x∼P[k P2(x,x)] < impliesE x∼P[kP(x,x)] < , so
Lem.B.1holds. FixanyL Nsatisfyingλ >0. Sinceniseven,wecandefine ∞≜[n/2]and ≜[n] . W∞ ewill
L 0 1 0
∈ D D \D
use and todenotethesubsetsof withindicesin and respectively.Let(ϕ )∞ beeigenfunctions
corrS esD p0 ondinS gD to1 theeigenvalues(λ )∞ S bn yLem.B.1(c),soD t0 hat(ϕD )1 ∞ isanorthonormaℓ lsℓ y= s1 te⊂ mH ofkP 2(P).
ℓ ℓ=1 ℓ ℓ=1 L
Westartwithausefullemma.
hL ae vm em PfaC =.1 0.(
HkP
consistsofmean-zerofunctions). LetkP beakernelsatisfyingAssum.1. Thenforanyf
∈
HkP, we
Proof. Fix f . By Steinwart and Christmann (2008, Thm 4.26), f is P integrable. Consider the linear operator I
thatmapsf
∈PH
f.kP
Since
(cid:55)→
|I(f) |= |Pf |≤P |f |=(cid:82) |⟨f,kP(x, ·) ⟩kP|dP ≤(cid:82) ∥f ∥kP(cid:112) kP(x,x)dP= ∥f ∥kPE x∼P[kP(x,x)21].
Hence I is a continuous linear operator, so by the Riez representation theorem (Steinwart and Christmann, 2008,
Thm.A.5.12),thereexistsg suchthatI(h)= h,g foranyh .
∈HkP
⟨
⟩kP ∈HkP
BySteinwartandChristmann(2008,Thm.4.21),theset
H
pre
≜(cid:8)(cid:80)n i=1α ikP( ·,x i):n ∈N,(α i)
i∈[n]
⊂R,(x i)
i∈[n]
⊂Rd(cid:9)
isdensein . NotethatH consistsofmeanzerofunctionsunderPbylinearity. Sothereexistsf convergingtof in
whereH eak cP hf hasPf =pre I(f )= f ,g =0. Since n
HkP n n n
⟨
n ⟩kP
lim f,g f ,g =lim f f ,g lim f f g =0,
n→∞ |⟨ ⟩kP −⟨ n ⟩kP| n→∞ |⟨ − n ⟩kP|≤ n→∞ ∥ − n ∥kP∥ ∥kP
wehavePf = f,g =0.
⟨
⟩kP
Inparticular,theassumptionE x∼P[k P2(x,x)]< ofThm.C.1impliesE x∼P[kP(x,x)1 2]< ,soLem.C.1holds.
∞ ∞
Step1. Buildcontrolvariateweights
FixanyL 1andh ,andlethˆ denotetheeigen-expansiontruncatedapproximationofhbasedon ,
≥
∈HkP D0 D0
hˆ (x)≜(cid:80)L βˆ ϕ (x) for βˆ ≜ 2 (cid:80) h(x )ϕ (x )ξ(x ).
D0 ℓ=1 ℓ,0 ℓ ℓ,0 n i∈D0 i ℓ i i
Then
E[βˆ ℓ,0]=E(cid:2) n2 (cid:80) i∈D0h(x i)ϕ ℓ(x i)ξ(x i)(cid:3) = ⟨h,ϕ
ℓ
⟩L2(P). (23)
Next,definethecontrolvariate
(cid:16) (cid:17)
Zˆ [h]= 2 (cid:80) ξ(x )(h(x ) hˆ (x )) . (24)
0 n i∈D1 i i − D0 i
whichsatisfies
(cid:104) (cid:105)
E[Zˆ 0[h]]=E x∼P h(x) −(cid:80)L ℓ=1E[βˆ ℓ,0]ϕ ℓ(x) =0, (25)
since functions in have mean 0 with respect to P (Lem. C.1). Similarly, we define Zˆ [h] by swapping and .
HkP 1 D0 D1
ThenweformZˆ[h]≜ Zˆ 0[h]+Zˆ 1[h]. WecanrewriteZˆ[h]asaquadratureruleover (LiuandLee,2017,LemmaB.6)
2 Sn
Zˆ[h]=(cid:80)
w h(x ), (26)
i∈[n] i i
wherew isdefinedas(whoserandomnessdependsontherandomnessin )
i n
S
(cid:26) 1ξ(x ) 2 (cid:80) ξ(x )ξ(x ) Φ (x ),Φ (x ) , i ,
w
i
≜ n 1ξ(xi )− n 22 (cid:80)j∈D1 ξ(xi )ξ(xj )⟨ ΦL (xi ),ΦL (xj )⟩ ,∀ i∈D0
,
(27)
n i − n2 j∈D0 i j ⟨ L i L j ⟩ ∀ ∈D1
27DebiasedDistributionCompression
andΦ (x)≜(ϕ (x),...,ϕ (x)).
L 1 L
Step2. ShowE[MMD2 (Sw,P)]=o(n−1)
kP n
WefirstboundthevarianceofthecontrolvariateZˆ 0[h]forh=ϕ
ℓ′
forℓ′ N. Letusfixℓ′ N. From(24),wecompute
∈ ∈
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:104) (cid:105)
E[Zˆ [h]2]= 4 E (cid:80) ξ(x )(h(x ) hˆ (x )) = 4 E (cid:80) ξ(x )2(h(x ) hˆ (x ))2
0 n2 i∈D1 i i − D0 i n2 i∈D1 i i − D0 i
= n2E[E x∼Q[ξ(x)2(h(x) −hˆ D0(x))2 |SD0]]
= n2E[E x∼P[ξ(x)(h(x) −hˆ D0(x))2 |SD0]]
≤
2 nME[E x∼P[(h(x) −hˆ D0(x))2 |SD0]],
whereinthesecondequality,thecrosstermsarezeroduetotheindependenceofpointsx andtheequality(25). Bythe
i
definitionofhˆ ,wecompute
D0
E x∼P[(h(x) −hˆ D0(x))2 |SD0]=E
x∼P(cid:20)(cid:16)
ϕ ℓ′(x) −(cid:80) ℓ≤Lβˆ ℓ,0ϕ
ℓ(x)(cid:17)2(cid:12)
(cid:12) (cid:12)
(cid:12)SD0(cid:21)
(cid:104) (cid:12) (cid:105)
=E x∼P ϕ2 ℓ′(x)+(cid:80) ℓ≤Lβˆ ℓ2 ,0ϕ2 ℓ(x) −2ϕ ℓ′(x)(cid:80) ℓ≤Lβˆ ℓ,0ϕ ℓ(x)(cid:12) (cid:12) SD0
=1+(cid:80) ℓ≤Lβˆ ℓ2 ,0−2(cid:80) ℓ≤Lβˆ ℓ,01
ℓ′=ℓ
=1+(cid:80) ℓ≤Lβˆ ℓ2 ,0−2βˆ ℓ′,01 ℓ′≤L,
whereweusethefactthat(ϕ ℓ)∞
ℓ=1
isanorthonormalsystemin L2(P). By(23)withh = ϕ ℓ′,wehaveE[βˆ ℓ′,0] = 1. On
theotherhand,wecanbound,againusingtheorthonormalityof(ϕ )∞ ,
ℓ ℓ=1
E[βˆ ℓ2 ,0]=E(cid:104)(cid:0) n2 (cid:80) i∈D0ϕ ℓ(x i)ϕ ℓ′(x i)ξ(x i)(cid:1)2(cid:105) = n4 2E(cid:2)(cid:80) i∈D0(ϕ ℓ(x i)ϕ ℓ′(x i)ξ(x i))2(cid:3)
≤
2 nME x∼P[(ϕ ℓ(x)ϕ ℓ′(x))2].
Thusforallℓ′ N,
∈
(cid:16) (cid:17) (cid:16) (cid:17)
E[Zˆ 0[ϕ ℓ′]2]
≤
2 nM 1+ 2 nM (cid:80) ℓ≤LE x∼P[(ϕ ℓ(x)ϕ ℓ′(x))2] −21 ℓ′≤L
≤
2 nM 2 nM (cid:80) ℓ≤LE x∼P[(ϕ ℓ(x)ϕ ℓ′(x))2]+1 ℓ′>L .
SinceZˆ[h]= Zˆ 0[h]+Zˆ 1[h] and(a+b)2 a2+b2 fora,b R,and,bysymmetry,E[Zˆ [h]2]=E[Zˆ [h]2],wehave
2 2 ≤ 2 ∈ 0 1
(cid:16) (cid:17)
E[Zˆ[ϕ ℓ′]2]
≤
2 nM 2 nM (cid:80) ℓ≤LE x∼P[(ϕ ℓ(x)ϕ ℓ′(x))2]+1
ℓ′>L
. (28)
Nowwehave
(cid:104) (cid:105) (cid:104) (cid:105)
E[MMD2 kP(Sw n,P)]=E (cid:80) i,j∈[n]w iw jkP(x i,x j) =E (cid:80) i,j∈[n]w iw j(cid:80)∞ ℓ′=1λ ℓ′ϕ ℓ′(x i)ϕ ℓ′(x j)
(cid:104) (cid:105)
=E (cid:80)∞ ℓ′=1(cid:80) i,j∈[n]w iw jλ ℓ′ϕ ℓ′(x i)ϕ ℓ′(x j)
(cid:20) (cid:16) (cid:17)2(cid:21)
=E (cid:80)∞ ℓ′=1λ ℓ′ (cid:80) i∈[n]w iϕ ℓ′(x i)
(cid:20)(cid:16) (cid:17)2(cid:21)
=(cid:80)∞ ℓ′=1λ ℓ′E (cid:80) i∈[n]w iϕ ℓ′(x i) =(cid:80)∞ ℓ′=1λ ℓ′E[Zˆ[ϕ ℓ′]2],
wherethesecondandthirdequalitiesareduetotheabsoluteconvergenceoftheMercerseries(Lem.B.1(d)), thefourth
equalityfollowsfromTonelli’stheorem(SteinwartandChristmann,2008,Thm.A.3.10),andthelaststepisdueto(26).
Pluggingin(28),wehave
(cid:16) (cid:17)
E[MMD2 kP(Sw n,P)]
≤
2 nM 2 nM (cid:80)∞ ℓ′=1(cid:80) ℓ≤Lλ ℓ′E x∼P[(ϕ ℓ(x)ϕ ℓ′(x))2]+(cid:80) ℓ>Lλ
ℓ
.
Sincetheeigenvaluesarenonnegativeandnon-increasing,wecanwrite,by(5),
k P2(x,x)=(cid:0)(cid:80)∞ ℓ=1λ ℓϕ ℓ(x)2(cid:1)2 ≥(cid:80)∞ ℓ′=1(cid:80) ℓ≤Lλ ℓ′λ ℓ(ϕ ℓ(x)ϕ ℓ′(x))2 ≥λ L(cid:80)∞ ℓ′=1(cid:80) ℓ≤Lλ ℓ′(ϕ ℓ(x)ϕ ℓ′(x))2.
28DebiasedDistributionCompression
ThusbyTonelli’stheorem(SteinwartandChristmann,2008,Thm.A.3.10),
(cid:80)∞ ℓ′=1(cid:80) ℓ≤Lλ ℓ′E x∼P[(ϕ ℓ(x)ϕ ℓ′(x))2]=E x∼P(cid:104) (cid:80)∞ ℓ′=1(cid:80) ℓ≤Lλ ℓ′(ϕ ℓ(x)ϕ ℓ′(x))2(cid:105)
≤
E x∼P[ λk LP2(x,x)].
Finally,wehave
E[MMD2 (Sw,P)] 2M (cid:16) 2M E x∼P[kP2(x,x)] +(cid:80) λ (cid:17) . (29)
kP n ≤ n n λL ℓ>L ℓ
Step3. Meetthenon-negativeconstraint
Wenowshowthattheweights(27)arenonnegativeandsumclosetoonewithhighprobability. Fori ,wehave
0
∈D
w = 1ξ(x )(1 T ) for T ≜ 2 (cid:80) ξ(x ) Φ (x ),Φ (x ) .
i n i − i i n j∈D1 j ⟨ L i L j ⟩
OurfirstgoalistoderiveanupperboundforPr(min w <0). Definetheevent
i∈D0 i
E
n
≜ kP
n
B
n
, (30)
{∥ ∥ ≤ }
soPr(E nc)<δ nbytheassumptionon ∥kP ∥n. Then
Pr(cid:0) min w <0,E (cid:1) =Pr(cid:0) max T >1,E (cid:1) nPr(T 1 >1), (31)
i∈[n] i n i∈[n] i n
≤
1 En
where we applied the union bound and used the fact that T 1 has the same law for different i. To further bound
i En
Pr(T 1 >1),wewillusethefollowinglemma.
1 En
LemmaC.2(TruncatedBernsteininequality). LetX ,...,X bei.i.d. randomvariableswithE[X ] = 0andE[X2] <
1 n 1 1
. ForanyB >0,t>0,
∞
(cid:16) (cid:17) (cid:16) (cid:17)
Pr 1 (cid:80) X 1 >t exp −nt2 .
n i∈[n] i Xi≤B ≤ 2(E[X 12]+B 3t)
ProofofLem.C.2. FixanyB >0andt>0anddefine,foreachi [n],Y ≜X 1 .ThenY B,
∈
i i Xi≤B i
≤
E[Y ]=E[X 1 ] E[X 1 ]+E[X 1 ]=E[X ]=0, and
i i Xi≤B
≤
i Xi≤B i Xi>B i
E[Y2]=E[X21 ] E[X2]=E[X2].
i i Xi≤B ≤ i 1
Nowwecaninvokethenon-positivityofE[Y ],theone-sidedBernsteininequality(Wainwright,2019,Prop.2.14),andthe
i
relationE[Y2] E[X2]toconcludethat
i ≤ 1
(cid:16) (cid:17) (cid:16) (cid:17) (cid:18) (cid:19) (cid:16) (cid:17)
Pr 1 (cid:80) Y >t Pr 1 (cid:80) (Y E[Y ])>t exp −nt2 exp −nt2 .
n i∈[n] i ≤ n i∈[n] i − i ≤ 2( n1 (cid:80) i∈[n]E[Y i2]+B 3t) ≤ 2(E[X 12]+B 3t)
Forj ,defineX ≜ξ(x ) Φ (x ),Φ (x ) andnotethat
1 j j L 1 L j
∈D ⟨ ⟩
E[X
j
x 1]=E x∼Q[ξ(x) Φ L(x 1),Φ L(x) x 1]=E x∼P[ Φ L(x 1),Φ L(x) x 1]=0
| ⟨ ⟩| ⟨ ⟩|
E[X j2 |x 1]=E[ξ(x j)2 ⟨Φ L(x 1),Φ L(x j) ⟩2 |x 1] ≤ME x∼P[ ⟨Φ L(x 1),Φ L(x) ⟩2 |x 1]
(cid:104) (cid:12) (cid:105)
=ME x∼P (cid:80) ℓ,ℓ′≤Lϕ ℓ(x 1)ϕ ℓ′(x 1)ϕ ℓ(x)ϕ ℓ′(x)(cid:12) (cid:12)x 1
=M(cid:80) ℓ,ℓ′≤Lϕ ℓ(x 1)ϕ ℓ′(x 1)E x∼P[ϕ ℓ(x)ϕ ℓ′(x)]
=M Φ (x ) 2.
∥ L 1 ∥2
Sinceλ λ 0,foranyx Rd,wecanbound Φ (x) 2via
1 ≥ 2 ≥···≥ ∈ ∥ L ∥2
∥Φ L(x) ∥2
2
=(cid:80) ℓ≤Lϕ ℓ(x)2
≤
(cid:80) ℓ≤L λλ Lℓϕℓ(x)2
≤
(cid:80)∞ ℓ=1 λλ Lℓϕℓ(x)2 = kP λ(x L,x), (32)
29DebiasedDistributionCompression
whereweappliedLem.B.1(d)forthelastequality. Thus
(cid:113)
X M Φ (x ) Φ (x ) M Φ (x ) kP(xj,xj),
| j |≤ ∥ L 1 ∥2∥ L j ∥2 ≤ ∥ L 1 ∥2 λL
(cid:113)
soifweletB ≜ B λLnM ∥Φ L(x 1) ∥2,then
(cid:110) (cid:111)
(cid:84)
E
n
= sup i∈[n]kP(x i,x i) ≤B
n
⊂
j∈D1{|X
j
|≤B }.
SinceT = 2 (cid:80) X ,wehaveinclusionsofevents
1 n j∈D1 j
(cid:110) (cid:111)
T 1 >1 = T >1 E 2 (cid:80) X 1 >1 .
{ 1 En } { 1 }∩ n ⊂ n j∈D1 j Xj≤B
ThusLem.C.2witht=1andconditionedonx implies
1
(cid:16) (cid:12) (cid:17)
Pr(T 1 >1x ) Pr 2 (cid:80) X 1 >1(cid:12)x
1 En | 1 ≤ n j∈D1 j Xj≤B (cid:12) 1
(cid:32) (cid:33)
exp −n .
(cid:113)
≤ 4(M∥ΦL(x1)∥2 2+ B λLnM∥ΦL(x1)∥ 2/3)
Onevent kP(x 1,x 1) B
n
,by(32),wehave
{ ≤ }
(cid:113)
∥Φ L(x 1)
∥2 ≤
B λLn.
Hence
(cid:18) (cid:19)
Pr(T 1 >1x )1 exp −n .
1 En | 1 kP(x1,x1)≤Bn ≤ 16MBn
3 λL
Ontheotherhand, kP(x 1,x 1)>B
n
/ E n,so
{ }∈
Pr(T 1 >1x )1 =0
1 En
|
1 kP(x1,x1)>Bn
Thus
(cid:18) (cid:19)
Pr(T 1 >1)=E[Pr(T 1 >1x )] exp −n .
1 En 1 En | 1 ≤ 16MBn
3 λL
Combiningthelastinequalitywith(31),wehave:
(cid:18) (cid:19)
Pr(cid:0) min w <0,E (cid:1) nexp −n . (33)
i∈[n] i n ≤ 16MBn
3 λL
Step4. Meetthesum-to-oneconstraint
Let
(cid:16) (cid:17)
S ≜(cid:80) w =(cid:80) 1ξ(x ) 1 2 (cid:80) ξ(x ) Φ (x ),Φ (x ) .
i∈D0 i i∈D0 n i − n j∈D1 j ⟨ L i L j ⟩
WenowderiveaboundforPr(S <1/2 t/2)fort (0,1). Let
− ∈
S ≜ 1 (cid:80) ξ(x ), S ≜ 2 (cid:80) (cid:80) ξ(x )ξ(x ) Φ (x ),Φ (x ) ,
1 n i∈D0 i 2 −n2 i∈D0 j∈D1 i j ⟨ L i L j ⟩
soS =S +S . NotethatE[S ]=1/2andE[S ]=0since and aredisjoint. LetE bethesameeventdefinedas
1 2 1 2 0 1 n
in(30). Fort (0,t/2)tobedeterminedlaterandt
≜t/2D
t
,weD
have,bytheunionbound
1 2 1
∈ −
Pr(S <1/2 t/2,E ) Pr(S <1/2 t ,E )+Pr(S < t ,E ).
n 1 1 n 2 2 n
− ≤ − −
ByHoeffding’sinequalityandtheassumptionξ(x) M,wehave
≤
Pr(S
1
<1/2 −t 1,E n) ≤Pr(cid:16) n2 (cid:80)
i∈D0
ξ( 2xi) −1/2< −t 1(cid:17) ≤exp(cid:16) − (2 M(n // 22 )) 2t2 1(cid:17) =exp(cid:16) − M4n 2t2 1(cid:17) . (34)
TogiveaconcentrationboundforPr(S < t ,E ),wewillusethefollowinglemma.
2 2 n
−
30DebiasedDistributionCompression
LemmaC.3(U-statisticBernstein’sinequality). Leth: Rbeafunctionboundedabovebyb>0. Assumen
2Nandletx ,...,x bei.i.d.randomvariablestakingvaX lu× esX in→ .Denotem ≜E[h(x ,x )]andσ2 ≜Var[h(x ,x )∈ ].
1 n X h 1 2 h 1 2
Let =[n/2]and =[n] [n/2]. Define
0 1
D D \
U ≜ 1 (cid:80) (cid:80) h(x ,x ).
(n/2)2 i∈D0 j∈D1 i j
Then
(cid:16) (cid:17)
Pr(U m >t) exp −nt2 .
− h ≤ 4(σ2+bt)
h 3
ProofofLem.C.3. WeadapttheprooffromPitcan(2017,Section3)asfollows. Letk ≜n/2. DefineV : n Ras
X →
V(x ,...,x )≜ 1 (cid:80) h(x ,x ).
1 n k i∈[k] i i+k
Thennotethat
U = 1 (cid:80) V ,
k! σ∈perm(k) σ
V ≜V(x ,...,x ),
σ σ1 σk
whereperm(k)isthesetofallpermutationsof[k];thisisbecauseeveryh(x ,x )termfori ,j willappearin
i j 0 1
∈D ∈D
thesummationanequalnumberoftimes. Forafixedσ perm(k),therandomvariableV(x ,...,x ,x ,...,x )
∈
σ1 σk k+1 n
isasumofki.i.d.termsh(x ,x ). DenoteV =V(x ,...,x ). Foranys>0,wehave,byindependence,
σi i+k 1 n
(cid:104) (cid:16) (cid:17)(cid:105)
E[es(V−mh)]=E exp ks (cid:80) i∈[k](h(x i,x i+[k]) −m h)
=(cid:0)E(cid:2) exp(cid:0)s(h(x
,x ) m
)(cid:1)(cid:3)(cid:1)k
k 1 2 − h
Bytheone-sidedBernstein’slemmaWainwright(2019,Prop.2.14)appliedto h(x1,x2) whichisupperboundedby b with
k k
variance σ h2 ,wehave
k2
E(cid:104) exp(cid:16) sh(x1,x2)−mh(cid:17)(cid:105) exp(cid:16) s2σ h2/2 (cid:17)
,
k ≤ k(k−bs)
3
fors [0,3k/b). Next,byMarkov’sinequalityandJensen’sinequality,
∈
Pr(U m
h
>t)=Pr(cid:0) es(U−mh) >est(cid:1) E[es(U−mh)]e−st
− ≤
(cid:104) (cid:16) (cid:17)(cid:105)
=E exp 1 (cid:80) s(V m ) e−st
(n/2)! σ∈perm(n/2) σ − h
(cid:104) (cid:105)
E 1 (cid:80) exp(s(V m )) e−st
≤ (n/2)! σ∈perm(n/2) σ − h
=E[es(V−mh)]e−st.
Therefore,
Pr(U −m h >t) ≤exp(cid:16) 2(s k2 −σ h b2 s) −st(cid:17) .
3
Now,wegetthedesiredboundifwepicks= k2t [0,3k/b)andsimplify.
kσ2+ktb ∈
h 3
Let
h(x,x′)≜ξ(x)ξ(x′) Φ (x),Φ (x′)
L L
⟨ ⟩
h¯(x,x′)≜h(x,x′)1 .
h(x,x′)≤M2Bn
λL
31DebiasedDistributionCompression
Then
(cid:16) (cid:17)
Pr(S < t ,E )=Pr 1 (cid:80) (cid:80) h(x ,x )>2t ,E
2 − 2 n (n/2)2 i∈D0 j∈D1 i j 2 n
(cid:16) (cid:17)
Pr 1 (cid:80) (cid:80) h¯(x ,x )>2t , (35)
≤ (n/2)2 i∈D0 j∈D1 i j 2
wherethelastinequalityusedthefactthat,fori ,j ,
0 1
∈D ∈D
(cid:110) (cid:111)
E n ⊂{max(kP(x i,x i),kP(x j,x j)) ≤B n
}⊂
h(x i,x j) ≤M2B λLn ,
using(32). Wefurthercompute
m h¯ =E[h¯(x 1,x 2)] ≤E[h(x 1,x 2)]=E[ξ(x 1)ξ(x 2) ⟨Φ L(x 1),Φ L(x 2) ⟩]
=(cid:80) E[ξ(x )ξ(x )ϕ (x )ϕ (x )]
ℓ≤L 1 2 ℓ 1 ℓ 2
=(cid:80) ℓ≤L(E x∼P[ϕ ℓ(x)])2 =0,
and
σ2 =Var[h¯(x ,x )] E[h¯(x ,x )2] E[h(x ,x )2]
h¯ 1 2
≤
1 2
≤
1 2
(cid:104) (cid:105)
=E (ξ(x )ξ(x ) Φ (x ),Φ (x ) )2
1 2 L 1 L 2
⟨ ⟩
M2E (x,x′)∼P×P[ Φ L(x),Φ L(x′) 2]
≤ ⟨ ⟩
(cid:104) (cid:105)
=M2E (x,x′)∼P×P (cid:80) ℓ,ℓ′≤Lϕ ℓ(x)ϕ ℓ′(x)ϕ ℓ(x′)ϕ ℓ′(x′)
=M2(cid:80) ℓ,ℓ′≤L(E[ϕ ℓ(x)ϕ ℓ′(x)])2 =LM2.
Since E x∼P[kP(x,x)] = (cid:80) ℓλ
ℓ
≥
Lλ L, we have L
≤
∥kP λ∥2 L L2(P), so that σ h¯2
≤
M2∥k λP L∥2 L2(P). Applying Lem. C.3 to h¯,
whichisboundedbyM2B λLn andusingthefactthatm h¯ ≤0,wehave
(cid:16) (cid:17) (cid:16) (cid:17)
Pr (n/1 2)2 (cid:80) i∈D0(cid:80) j∈D1h¯(x i,x j)>2t 2 ≤Pr (n/1 2)2 (cid:80) i∈D0(cid:80) j∈D1h¯(x i,x j) −m h¯ >2t 2
 
exp −n(2t2)2 . (36)
≤  4(cid:32) M2∥k λP L∥2 L2(P)+2M2B λLnt2/3(cid:33)
Thuscombining(34),(35),(36),weget
 
Pr(S <1/2 −t/2,E n) ≤exp(cid:16) − M4n 2t2 1(cid:17) +exp (cid:32) M2∥k λP L∥2 L2(− P)n +t 22 2 M2B λLnt2/3(cid:33) .
Finally,bysymmetryandtheunionbound,fort (0,1),t (0,t/2)andt =t/2 t ,wehave
2 1
∈ ∈ −
(cid:16) (cid:80) (cid:17) (cid:0)(cid:80) (cid:1) (cid:0)(cid:80) (cid:1)
Pr w <1 t,E Pr w <1/2 t/2,E +Pr w <1/2 t/2,E
i∈[n] i − n ≤ i∈D0 i − n i∈D1 i − n
=2Pr(S <1/2 t/2,E )
n
−
  
≤2 exp(cid:16) − M4n 2t2 1(cid:17) +exp (cid:32) M2∥k λP L∥2 L2(− P)n +t 22 2 M2B λLnt2/3(cid:33)  . (37)
Step5. Puttingitalltogether
32DebiasedDistributionCompression
Definetheevent
(cid:110) (cid:111)
F = min w 0,(cid:80) w 1 .
n i∈[n] i ≥ i∈[n] i ≥ 2
Then,bytheunionbound,
Pr(Fc) Pr(cid:0) min w <0,E (cid:1) +Pr(cid:16) (cid:80) w < 1,E (cid:17) +Pr(Ec).
n ≤ i∈[n] i n i∈[n] i 2 n n
Applying(33)and(37)toboundthelastexpressionwitht=1/2,t =t =1/8,wehavePr(Fc) ϵ2 forϵ definedin
1 2 n ≤ n n
(22). OntheeventF ,ifwedefinew+ ∆ via
n n−1
∈
w i+ ≜ (cid:80) i∈w [ni ]wi,
thenw i+ =αw ifori ∈[n]andα≜ (cid:80) i∈1 [n]wi ≤2. Letw˜ ∈∆ n−1betheweightdefinedbyw˜ 1 =1andw˜ i =0fori>1.
Sincew
OPT
isthebestsimplexweight,wehaveMMD2 kP(Sw nOPT,P) ≤min(MMD2 kP(Sw n+,P),MMD2 kP(Sw n˜,P)). Hence
E(cid:2) MMD2 kP(Sw nOPT,P)(cid:3) =E(cid:2) MMD2 kP(Sw nOPT,P)1 Fn(cid:3) +E(cid:2) MMD2 kP(Sw nOPT,P)1
F
nc(cid:3)
≤E(cid:104) MMD2 kP(Sw n+,P)1 Fn(cid:105) +E(cid:2) MMD2 kP(Sw n˜,P)1
F
nc(cid:3) .
Forthefirstterm,wehavethebound
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
E MMD2 kP(Sw n+,P)1
Fn
=E (cid:80) i,j∈[n]w i+w j+kP(x i,x j)1
Fn
=E α2(cid:80) i,j∈[n]w iw jkP(x i,x j)1
Fn
≤4E(cid:104) (cid:80) i,j∈[n]w iw jkP(x i,x j)(cid:105)
≤
8 nM (cid:16) 2 nM E x∼P[ λk LP2(x,x)] +(cid:80) ℓ>Lλ ℓ(cid:17) ,
whereweapplied(29)forthelastinequality. Forthesecondterm,bytheCauchy-Schwartzinequality,
(cid:115)
E(cid:2) MMD2 kP(Sw n˜,P)1
F
nc(cid:3) ≤(cid:112) Pr(F nc)
E(cid:20)(cid:16)
(cid:80) i,j∈[n]kP(x i,x j)w˜ iw˜
j(cid:17)2(cid:21)
≤(cid:112)
Pr(F
nc)(cid:112)E[kP(x
1,x 1)2].
Puttingeverythingtogetherweobtain(21).
D.SteinKernelThinning
Inthissection,wedetailourSteinthinningimplementationinApp.D.1,ourkernelthinningimplementationandanalysis
inApp.D.2,andourproofofThm.3inApp.D.3.
D.1.SteinThinningwithsufficientstatistics
Foraninputpointsetofsizen,theoriginalimplementationofSteinThinningofRiabizetal.(2022)takesO(nm2)time
tooutputacoresetofsizem. InAlg.D.1,weshowthatthisruntimecanbeimprovedtoO(nm)usingsufficientstatistics.
The idea is to maintain a vector g Rn such that g = 2kP(S n,S n)w where w is the weight representing the current
∈
coreset.
D.2.KernelThinningtargetingP
OurKernelThinningimplementationisdetailedinAlg.D.2.SinceweareabletodirectlycomputeMMD (Sw,P),weuse
kP n
KT-Swap(Alg.D.3)inplaceofthestandard KT-SWAP subroutine(DwivediandMackey,2022,Algorithm1b)tochoose
candidate points to swap in so as to greedily minimize MMD (Sw,P). To facilitate our subsequent SKT analysis, we
kP n
restatetheguaranteesofKT-SPLIT(DwivediandMackey,2022,Theorem2)inthesub-Gaussianformatof(Shettyetal.,
2022,Definition3).
33DebiasedDistributionCompression
AlgorithmD.1SteinThinning(ST)withsufficientstatistics
Input: kernelkPwithzero-meanunderP,inputpoints
n
=(x i) i∈[n],outputsizem
w 0 Rn S
← ∈
j ←argmin i∈[n]kP(x i,x i)
w 1
j
←
g 2kP( n,x j)▷maintainsufficientstatisticsg =2kP( n, n)w
← S S S
fort=1tom 1do
−
j ←argmin i∈[n]{tg i+kP(x i,x i)
}
w t w+ 1 e
← t+1 t+1 j
g
←
t+t 1g+ t+2 1kP( Sn,x j)
endfor
Return: w
AlgorithmD.2KernelThinning(KT)(adaptedfromDwivediandMackey(2022,Alg.1))
Input: kernel kP with zero-mean under P, input points
Sn
= (x i) i∈[n], multiplicity n′ with log
2
n m′
∈
N, weight w
∈
∆ n−1 ∩(N n0 ′)n,outputsizemwith n m′ ∈2N,failureprobabilityδ
S indexsequencewherek [n]appearsn′w times
k
t ← log n′ N ∈
← 2 m ∈
(I(ℓ))
ℓ∈[2t]
KT-SPLIT(kP, n[S],t,δ/n′) ▷ KT-SPLIT is from Dwivedi and Mackey (2022, Algorithm 1a) and we set
← S
δ =δforalli
i
I(ℓ) S[I(ℓ)]foreachℓ [2t]
← ∈
I KT-Swap(kP, n,(I(ℓ)) ℓ∈[2t])
← S
w simplexweightcorrespondingtoI▷w = numberofoccurrencesofiinI
KT ← i |I|
Return: w
KT
∈∆
n−1
∩(N m0)n▷Hence ∥w
KT ∥0
≤m
LemmaD.1(Sub-GaussianguaranteeforKT-SPLIT). Let nbeasequenceofnpointsandkakernel. Foranyδ (0,1)
and m N such that log n N, consider the KT-SPLS IT algorithm (Dwivedi and Mackey, 2022, Algorithm 1∈ a) with
∈ 2 m ∈
k = k,thinningparametert = log n,andδ = δ tocompress to2t coresets (i) whereeach (i) hasm
split 2 m i n Sn {Sout}i∈[2t] Sout
points. Denotethesignedmeasureϕ(i) ≜ 1 (cid:80) δ 2t (cid:80) δ . Thenforeachi [2t], onanevent (i) with
n x∈Sn x − n x∈S o( ui t) x ∈ Eequi
P( (i) ) 1 δ,ϕ(i) =ϕ˜(i)forarandomsignedmeasureϕ˜(i)5suchthat,foranyδ′ (0,1),
Eequi ≥ − 2 ∈
(cid:18)(cid:13) (cid:13) (cid:113) (cid:19)
Pr (cid:13)ϕ˜(i)k(cid:13) a +v log(cid:0)1(cid:1) δ′,
(cid:13) (cid:13) Hk ≥ n,m n,m δ′ ≤
where
a
n,m
≜ m1
(cid:18) 2+(cid:114)
8 3∥k
∥nlog(cid:16)
6(log 2
δmn)m(cid:17)
log(4 Nk( B2(R
n),m−1))(cid:19)
,
(cid:114)
v
n,m
≜ m1 8 3∥k
∥nlog(cid:16)
6(log 2
δmn)m(cid:17)
.
ProofofLem.D.1. Fixi [2t], δ (0,1)andn,m Nsuchthatt = log n N. Defineϕ ≜ ϕ(i). Bytheproofof
∈ ∈ ∈ 2 (cid:16)m ∈ (cid:17)
DwivediandMackey(2022,Thms.1and2),thereexistsanevent withPr c δ suchthat,onthisevent,ϕ=ϕ˜
Eequi Eequi ≤ 2
whereϕ˜isasignedmeasuresuchthat,foranyδ′ (0,1),withprobabilityatleast1 δ′,
∈ −
(cid:13) (cid:13) (cid:113)
(cid:13)ϕ˜k(cid:13) inf 2ϵ+ 2t 8 k log(cid:0)6tn(cid:1)(cid:2) log 4 +log (A,ϵ)(cid:3) .
(cid:13) (cid:13) Hk ≤ ϵ∈(0,1),A:Sn⊂A n 3∥ ∥n 2tδ δ′ Nk
5Thisisthesignedmeasurereturnedbyrepeatedapplicationsofself-balancingHilbertwalk(SBHW)(DwivediandMackey,2021,
Algorithm3).AlthoughSBHWreturnsanelementofH ,bytracingthealgorithm,thereturnedoutputisequivalenttoasignedmeasure
k
(cid:80) (cid:80)
viathecorrespondence c k(x ,·)⇔ c δ .TheusageofsignedmeasuresisconsistentwithShettyetal.(2022).
i∈[n] i i i∈[n] i xi
34DebiasedDistributionCompression
AlgorithmD.3KT-Swap(modifiedDwivediandMackey(2022,Alg.1b)tominimizeMMDtoP)
Input: kernelkPwithzero-meanunderP,inputpoints
n
=(x i) i∈[n],candidatecoresetindices(I(ℓ))
ℓ∈[L]
(cid:12) (cid:12) S
m (cid:12)I(0)(cid:12)▷allcandidatecoresetsareofthesamesize
←
I ←I(ℓ∗)forℓ∗ ∈argmin ℓ∈[L]MMD kP( Sn[I(ℓ)],P)▷selectthebestKT-SPLITcoreset
I
ST
indexsequenceofSteinThinning(kP, n,m)▷addSteinthinningbaseline
← S
C= I,I ▷shortlistedcandidates
ST
{ }
forI Cdo
g ←∈ 0 ∈Rn▷maintainsufficientstatisticsg =(cid:80) j∈[m]kP(x Ij, Sn)
Kdiag (kP(x i,x i))
i∈[n]
←
forj =1tomdo
g ←g+kP(x Ij, Sn)
endfor
forj =1tomdo
∆=2(g −kP(x Ij, Sn))+Kdiag▷thisisthechangeinMMD2 kP( Sn[I],P)ifweweretoreplaceI
j
k argmin ∆
← i∈[n] i
g =g −kP(x Ij, Sn)+kP(x k, Sn)
I k
j
←
endfor
endfor
Return: I=argmin MMD ( [I],P)
I∈C kP Sn
(cid:13) (cid:13)
Notethaton , (cid:13)ϕ˜k(cid:13) = ϕk . WechooseA = (R )andϵ = 2t = m−1, sothat, withprobabilityatleast
Eequi (cid:13) (cid:13) Hk ∥ ∥Hk B2 n n
1 δ′,usingthefactthat√a+b √a+√bfora,b 0,
− ≤ ≥
(cid:13) (cid:13) (cid:113)
(cid:13)ϕ˜k(cid:13) 2t+1 + 2t 8 k log(cid:0)6tn(cid:1)(cid:2) log 4 +log ( (R ),m−1)(cid:3) (38)
(cid:13) (cid:13) Hk ≤ n n 3∥ ∥n 2tδ δ′ Nk B2 n
2t+1 + 2t(cid:113) 8 k log(cid:0)6tn(cid:1)(cid:104)(cid:113) log 1 +(cid:112) log4 ( (R ),m−1)(cid:105)
≤ n n 3∥ ∥n 2tδ δ′ Nk B2 n
(cid:113)
a +v
log(cid:0)1(cid:1)
,
≤ n,m n,m δ′
fora ,v inLem.D.1.
n,m n,m
CorollaryD.1(MMDguaranteefor KT-SPLIT). Let
∞
beaninfinitesequenceofpointsinRd andk akernel. Forany
δ (0,1)andn,m Nsuchthatlog n N,consiS dertheKT-SPLITalgorithm(DwivediandMackey,2022,Algorithm
∈ ∈ 2 m ∈
1a) with parameters k = k and δ = δ to compress to 2t coresets (i) where t = log n, each with m
split i n Sn {Sout}i∈[2t] 2 m
points. Thenforanyi [2t],withprobabilityatleast1 δ,
∈ −
MMD k(S n,S( oi u) t)
≤
m1
(cid:18) 2+(cid:114)
8 3∥k
∥nlog(cid:16)
6(log 2
δmn)m(cid:17)(cid:0)
log Nk( B2(R n),m−1)+log8
δ(cid:1)(cid:19)
. (39)
ProofofCor.D.1. Fixi [2t]. Bytakingδ′ = δ in(38),weobtain(39). Thisoccurswithprobability
∈ 2
Pr(cid:16)
MMD (S ,S(i) )<a +v
(cid:113) log(cid:0)1(cid:1)(cid:17)
k n out n,m n,m δ′
=1
Pr(cid:16)
MMD (S ,S(i) ) a +v
(cid:113) log(cid:0)1(cid:1)(cid:17)
− k n out ≥ n,m n,m δ′
1
Pr(cid:16)
(i),MMD (S ,S(i) ) a +v
(cid:113) log(cid:0)1(cid:1)(cid:17) Pr(cid:16) (i)c(cid:17)
≥ − Eequi k n out ≥ n,m n,m δ′ − Eequi
1
Pr(cid:18)(cid:13) (cid:13)ϕ˜(i)k(cid:13)
(cid:13) a +v
(cid:113) log(cid:0)1(cid:1)(cid:19) Pr(cid:16) (i)c(cid:17)
≥ − (cid:13) (cid:13) Hk ≥ n,m n,m δ′ − Eequi
1 δ δ =1 δ.
≥ − 2 − 2 −
35DebiasedDistributionCompression
D.3.ProofofThm.3: MMDguaranteeforSKT
Thm.3willfollowsdirectlyfromAssum.(α,β)-kernelandthefollowingstatementforagenericcoveringnumber.
TheoremD.1. LetkPbeakernelsatisfyingAssum.1.Let ∞beaninfinitesequenceofpoints.Thenforaprefixsequence
S
Snofnpoints,m ∈[n],andn′ ≜m2 ⌈log 2 mn ⌉,SKToutputsw SKTinO(n2d kP)timethatsatisfies,withprobabilityatleast
1 δ,
−
(cid:114) (cid:32) (cid:114) (cid:33)
∆MMD kP(w SKT)
≤
(cid:16) 1+l no ′gn′(cid:17) ∥kP ∥n+ m1 2+ 8 3∥k ∥nlog(cid:16) 6(log 2 δn m′)m(cid:17)(cid:0) log Nk( B2(R n),m−1)+log8 δ(cid:1) .
ProofofThm.D.1. TheruntimeofSKTcomesfromthefactthatallofSteinThinning(withoutputsizen),KT-SPLIT,and
KT-SwaptakeO(d n2)time.
kP
ByRiabizetal.(2022,Theorem1),SteinThinning(whichisadeterministicalgorithm)fromnpointston′ pointshasthe
followingguarantee
(cid:16) (cid:17)
MMD2 kP(Sw n†,P) ≤MMD2 kP(Sw nOPT,P)+ 1+l no ′gn′ ∥kP ∥n,
wherewedenotetheoutputweightofSteinThinningasw†. Using√a+b √a+√bfora,b 0,wehave
≤ ≥
(cid:114)
(cid:16) (cid:17)
MMD kP(Sw n†,P) ≤MMD kP(Sw nOPT,P)+ 1+l no ′gn′ ∥kP ∥n.
Fixδ ∈(0,1). ByCor.D.1withk=kPandt=log
2
n m′ ,withprobabilityatleast1 −δ,wehave,foranyi ∈[2t],
(cid:32) (cid:114) (cid:33)
MMD kP(Sw n†,S( oi u) t)
≤
m1 2+ 8 3∥k ∥nlog(cid:16) 6(log 2 δn m′)m(cid:17)(cid:0) log Nk( B2(R n),m−1)+log8 δ(cid:1) ,
where S( oi u)
t
is the i-th coreset output by KT-SPLIT. Since KT-Swap can only decrease the MMD to P, we have, by the
triangleinequalityofMMD ,
kP
MMD kP(Sw nSKT,P) ≤MMD kP(S( o1 ut),P) ≤MMD kP(S( o1 ut),Sw n† )+MMD kP(Sw n†,P),
whichgivesthedesiredbound.
Thm. 3 now follows from Thm. D.1, the kernel growth definitions in Assum. (α,β)-kernel, n n′ 2n, and that
log (n′ )m n′. ≤ ≤
2 m ≤
E.ResamplingofSimplexWeights
Integral to many of our algorithms is a resampling procedure that turns a simplex-weighted point set of size n into an
equal-weightedpointsetofsizemwhileincurringatmostO(1/√m)MMDerror. Themotivationforwantinganequal-
weightedpointsetistwo-fold: First,inLSKT,weneedtoprovideanequal-weightedpointsettoKT-Compress++,butthe
outputofLDisasimplexweight. Secondly,wecanexploitthefactthatnon-zeroweightsareboundedawayfromzeroin
equal-weightedpointsetstoprovideatighteranalysisofWeightedRPCholesky. Whilei.i.d.resamplingalsoachievesthe
O(1/√m)goal,wechooseResample(Alg.E.3),astratifiedresidualresamplingalgorithm(DoucandCappe´,2005,Sec.
3.2,3.3). Inthissection,wederiveanMMDboundforResampleandshowthatitisbetterinexpectationthanusingi.i.d.
resamplingorresidualresamplingalone.
LetDinvbetheinverseofthecumulativedistributionfunctionofthemultinomialdistributionwithweightw,i.e.,
w
(cid:110) (cid:111)
Dinv(u)≜min i [n]:u (cid:80)i w .
w ∈ ≤ j=1 j
36DebiasedDistributionCompression
AlgorithmE.1i.i.d.resampling
Input: Weightsw ∆ ,outputsizem
n−1
w′ 0 Rn ∈
← ∈
forj =1tomdo
DrawU Uniform([0,1))
j
∼
I Dinv(U )
wj ′← ww ′ +j 1
Ij ← Ij m
endfor
Return: w′ ∈∆
n−1
∩(N m0)n
AlgorithmE.2Residualresampling
Input: Weightsw ∆ ,outputsizem
n−1
∈
w′ ⌊mwi⌋, i [n]
ri ← m m(cid:80) ∀ ∈ mw N
← − i∈[n]⌊ i ⌋∈
η mwi−⌊mwi⌋, i [n]
i ← r ∀ ∈
forj =1tordo
DrawU Uniform([0,1))
j
∼
I Dinv(U )
j ← η j
w′ w′ + 1
Ij ← Ij m
endfor
Return: w′ ∈∆
n−1
∩(N m0)n
Proposition E.1 (MMD guarantee of resampling algorithms). Consider any kernel k, points = (x ,...,x ) Rd,
n 1 n
S ⊂
andaweightvectorw ∆ .
n−1
∈
(a) UsingthenotationfromAlg.E.1,letX,X′ beindependentrandomvariableswithlawSw. Then,theoutputweight
n
vectorwi.i.d. ≜w′ofAlg.E.1satisfies
E[MMD2(Swi.i.d.,Sw)]= Ek(X,X)−Ek(X,X′). (40)
k n n m
(b) Using the notation from Alg. E.2, let R,R′ be independent random variables with law Sη. Then, the output weight
n
vectorwresid ≜w′ofAlg.E.2satisfies
E[MMD2(Swresid,Sw)]= r(Ek(R,R)−Ek(R,R′)). (41)
k n n m2
(c) Using the notation from Alg. E.3, let R ≜ x and R′ be an independent copy of R . Let R be an independent
j Ij j j
randomvariablewithlawSη. Then,theoutputweightvectorwsr ≜w′ofAlg.E.3satisfies
n
E[MMD2(Swsr,Sw)]=
rEk(R,R)−(cid:80) j∈[r]Ek(Rj,R j′)
. (42)
k n n m2
AlgorithmE.3Stratifiedresidualresampling(Resample)
Input: Weightsw ∆ ,outputsizem
n−1
∈
w′ ⌊mwi⌋, i [n]
ri ← m m(cid:80) ∀ ∈ mw N
← − i∈[n]⌊ i ⌋∈
η mwi−⌊mwi⌋, i [n]
i ← r ∀ ∈
forj =1tordo
DrawU Uniform([j,j+1))
j ∼ r r
I Dinv(U )
j ← η j
w′ w′ + 1
Ij ← Ij m
endfor
Return: w′ ∈∆
n−1
∩(N m0)n
37DebiasedDistributionCompression
ProofofProp.E.1(a). LetX ≜x . Asrandomsignedmeasures,wehave
i Ii
Sw′ Sw = 1 (cid:80) δ (cid:80) w δ .
n − n m i∈[m] Xi − i∈[n] i xi
Hence
MMD2(Sw′,Sw)=((Sw′ Sw) (Sw′ Sw))k
k n n n − n × n − n
= m1
2
(cid:80) i,i′∈[m]k(X i,X i′)
−
m2 (cid:80) i∈[m],i′∈[n]w i′k(X i,x i′)+(cid:80) i,i′∈[n]w iw i′k(x i,x i′).
SinceeachX iisdistributedtoSw
n
andX iandX
i′
areindependentfori ̸=i′,takingexpectation,wehave
E[MMD2(Sw′,Sw)]= 1Ek(X,X)+ m−1Ek(X,X′) 2Ek(X,X′)+Ek(X,X′).
k n n m m −
Thisgivesthebound(40).
ProofofProp.E.1(b). LetR ≜x . Asrandomsignedmeasures,wehave
j Ij
(cid:16) (cid:17)
Sw′ Sw = (cid:80) ⌊mwi⌋δ + 1 (cid:80) δ (cid:80) w δ
n − n i∈[n] m xi m j∈[r] Rj − i∈[n] i xi
(cid:16) (cid:17)
= 1 (cid:80) δ (cid:80) w ⌊mwi⌋ δ
m j∈[r] Rj − i∈[n] i − m xi
= 1 (cid:80) δ r (cid:80) η δ .
m j∈[r] Rj − m i∈[n] i xi
Hence
MMD2(Sw′,Sw)=((Sw′ Sw) (Sw′ Sw))k
k n n n − n × n − n
= m1
2
(cid:80) j,j′∈[r]k(R j,R j′)
−
m2r
2
(cid:80) j∈[r],i∈[n]η ik(R j,x i)+ mr2
2
(cid:80) i,i′∈[n]η iη jk(x i,x j). (43)
SinceeachR
j
isdistributedtoSη nandR
j
andR
j′
areindependentforj ̸=j′,takingexpectation,wehave
E[MMD2(Sw′,Sw)]= r Ek(R,R)+ r(r−1)Ek(R,R′) 2r2Ek(R,R′)+ r2 Ek(R,R′).
k n n m2 m2 − m2 m2
Thisgivesthebound(41).
ProofofProp.E.1(c). Werepeatthesamestepsfromthepreviouspartoftheprooftoget(43). Inthecaseof(c),R ’sare
j
notidenticallydistributedsotheanalysisisdifferent. LetR′ beanindependentcopyofR. Takingexpectationof(43),we
have
m2E[MMD2 k(Sw n′,Sw n)]=(cid:80) j∈[r]Ek(R j,R j)+(cid:80) j∈[r](cid:80) j′∈[r]\{j}Ek(R j,R j′) −2r(cid:80) j∈[r]Ek(R j,R)+r2Ek(R,R′).
Note
(cid:80) Ek(R ,R )=(cid:80) r(cid:82) k(x ,x )du=r(cid:82)1 k(x ,x )du=rEk(R,R),
j∈[r] j j j∈[r] [ rj,j+ r1) D ηinv(u) D ηinv(u) 0 D ηinv(u) D ηinv(u)
D
whereweusedthefactthatx =RforU Uniform([0,1]). Similarly,wededuce
Dinv(U)
η ∼
(cid:16) (cid:17)
(cid:80) j∈[r](cid:80) j′∈[r]\{j}Ek(R j,R j′)=(cid:80)
j∈[r]
(cid:80) j′∈[r]Ek(R j,R j′ ′) −Ek(R j,R j′)
=(cid:80) (rEk(R ,R′) Ek(R ,R′))
j∈[r] j − j j
=r2Ek(R,R′) (cid:80) Ek(R ,R′),
− j∈[r] j j
andalso
(cid:80) Ek(R ,R)=rEk(R,R′).
j∈[r] j
Combiningterms,weget
m2EMMD2(Sw′,Sw)=rEk(R,R)+r2Ek(R,R′) (cid:80) Ek(R ,R′) 2r2Ek(R,R′)+r2Ek(R,R′)
k n n − j∈[r] j j −
=rEk(R,R) (cid:80) Ek(R ,R′),
− j∈[r] j j
whichyieldsthedesiredbound(42).
38DebiasedDistributionCompression
Thenextpropositionshowsthatstratifyingtheresidualsalwaysimprovesuponusingi.i.d.samplingorresidualresampling
alone. Weneedthefollowingconvexitylemma.
LemmaE.1(ConvexityofsquaredMMD). Letkbeakernel. Let = (x ,...,x )beanarbitrarysetofpoints. The
n 1 n
functionE :Rn Rdefinedby S
k
→
E (w)≜ Swk 2 =(cid:80) w w k(x ,x )
k ∥ n ∥Hk i,j∈[n] i j i j
isconvexonRn.
ProofofLem.E.1. Sincekisakernel,theHessian 2E =2k( , )isPSD,andhenceE isconvex.
k n n k
∇ S S
PropositionE.2(StratifiedresidualresamplingimprovesMMD). UndertheassumptionsofProp.E.1,wehave
E[MMD2(Swi.i.d.,Sw)] E[MMD2(Swresid,Sw)] E[MMD2(Swsr,Sw)].
k n n ≥ k n n ≥ k n n
ProofofProp.E.2. LetK ≜k( , ). Toshowthefirstinequality,notethatsinceη = mw−⌊mw⌋,byProp.E.1,
Sn Sn r
E[MMD2(Swresid,Sw)]= r(Ek(R,R)−Ek(R,R′))
k n n m2
=
r((cid:80) i∈[n]Kiiηi−(cid:80) i,j∈[n]Kijηiηj)
m2
(cid:18) (cid:16) (cid:17) (cid:16) (cid:17)⊤ (cid:16) (cid:17)(cid:19)
= 1 (cid:80) K w ⌊mwi⌋ m w ⌊mw⌋ K w ⌊mw⌋ .
m i∈[n] ii i − m − r − m − m
Hence
E[MMD2(Swi.i.d.,Sw)] E[MMD2(Swresid,Sw)]
k n n − k n n
(cid:18) (cid:16) (cid:17)⊤ (cid:16) (cid:17) (cid:19)
=1 (cid:80) K ⌊mwi⌋ + m w ⌊mw⌋ K w ⌊mw⌋ w⊤Kw
m i∈[n] ii m r − m − m −
(cid:16) (cid:17)
=1 (1 θ)(cid:80) K ξ +θη⊤Kη w⊤Kw ,
m − i∈[n] ii i −
whereweletξ ≜ m ⌊mw⌋ andθ ≜ r. Notethatw =θη+(1 θ)ξ. ByLem.E.1andJensen’sinequality,wehave
m−r m m −
w⊤Kw =E (w) θE (η)+(1 θ)E (ξ)=θη⊤Kη+(1 θ)ξ⊤Kξ θη⊤Kη+(1 θ)(cid:80) K ξ ,
k ≤ k − k − ≤ − i∈[n] ii i
where the last inequality follows from Prop. E.1(a) with w = ξ and the fact that MMD is nonnegative. Hence we have
shown
E[MMD2(Swi.i.d.,Sw)] E[MMD2(Swresid,Sw)] 0,
k n n − k n n ≥
asdesired.
Forthesecondinequality,byProp.E.1,wecompute
(cid:16) (cid:17)
E[MMD2(Swresid,Sw)] E[MMD2(Swsr,Sw)]= r 1(cid:80) Ek(R ,R′) Ek(R,R′) .
k n n − k n n m2 r j∈[r] j j −
Notethat
Ek(R,R′)=(cid:82) (cid:82) k(x ,x )dudv =E (cid:0) (Dinv) Uniform[0,1)(cid:1) ,
[0,1) [0,1) D ηinv(u) D ηinv(v) k η #
whereweusedT µtodenotethepushforwardmeasureofµbyT. Similarly,
#
1(cid:80) Ek(R ,R′)= 1(cid:80) (cid:82) (cid:82) k(x ,x )dudv
r j∈[r] j j r j∈[r] [ rj,j+ r1) [ rj,j+ r1) D ηinv(u) D ηinv(v)
= 1(cid:80) E (cid:0) (Dinv) Uniform(cid:2)j,j+1(cid:1)(cid:1)
r j∈[r] k η # r r
E (cid:0) (Dinv) Uniform[0,1)(cid:1) =Ek(R,R′),
≤ k η #
whereinthelastinequalityweappliedJensen’sinequalitysinceE isconvexbyLem.E.1. Hencewehaveshown
k
E[MMD2(Swresid,Sw)] E[MMD2(Swsr,Sw)]
0
k n n − k n n ≥
andtheproofiscomplete.
39DebiasedDistributionCompression
F.AcceleratedDebiasedCompression
Inthissection,weprovidesupplementaryalgorithmicdetailsanddeferredanalysesforLSKT(Alg.3). InWeightedRPC-
holesky(Alg.F.1),weprovidedetailsfortheweightedextensionofChenetal.(2022,Alg.2.1)thatisusedextensivelyin
ouralgorithms. ThedetailsofAMD(Wangetal.,2023,Alg.14)areprovidedinAlg.F.2. InApp.F.1,wegivetheproof
ofThm.4fortheMMDerrorguaranteeofLD(Alg.2). InApp.F.2,weprovidedetailsonKT-Compress++modifiedfrom
Compress++(Shettyetal.,2022)tominimizeMMDtoP. Finally,Thm.5isprovedinApp.F.3.
AlgorithmF.1WeightedRandomlyPivotedCholesky(WeightedRPCholesky)(extensionofChenetal.(2022,Alg.2.1))
Input: kernelk,points =(x )n ,simplexweightsw ∆ ,rankr
k˜(i,j)≜k(x i,x j)√w iS √n w
j
▷rei wi e= i1 ghtedkernelmatrixfu∈ nction n−1
F 0 ,S ,d (k˜(i,i))
n×r i∈[n]
← ←{} ←
fori=1to rdo
(cid:80)
Samples d/ d
∼ j∈[n] j
S S s
g
← k˜(∪ :,{ s)}
F(:,1:i 1)F(s,1:i 1)⊤
← − − −
F(:,i) g/√g
s
←
d d F(:,i)2▷F(:,i)2denotesavectorwithentry-wisesquaredvaluesofF(:,i)
← −
d max(d,0)▷numericalstabilityfix,helpfulinpractice
←
endfor
F diag((1/√w i) i∈[n])F ▷undoweighting;treat1/√w
i
=0ifw
i
=0
Re← turn: S [n]with S =randF Rn×r
⊂ | | ∈
AlgorithmF.2AcceleratedEntropicMirrorDescent(AMD)(modificationofWangetal.(2023,Alg.14))
Input: kernelmatrixK Rn×n,numberofstepsT,initialweightw ∆ ,aggressiveflagAGG
0 n−1
η 1 ifAGG∈ else 1 ∈
v
← 8 ww 0⊤diag(K) 8maxi∈[n]Kii
0 0
←
fort=1to T do
β 2
t ← t+1
z (1 β )w +β v
t t t−1 t t−1
← −
g 2tηKz ▷thisisγ f(z )inWangetal.(2023,Alg.14)forf(w)=w⊤Kw
t t t
← ∇
v v exp( g)▷component-wiseexponentiationandmultiplication
t t−1
v ← v / v· ▷− v =argmin g,w +Dϕ (w)forϕ(w)=(cid:80) w logw
t ← t ∥ t ∥1 t w∈∆n−1⟨ ⟩ vt−1 i∈[n] i i
w (1 β )w +β v
t t t−1 t t
← −
endfor
Return: w ∆
T n−1
∈
F.1.ProofofThm.4: DebiasingguaranteeforLD
(cid:16) (cid:17)
Westartwithausefullemmathatboundsw⊤(K Kˆ)wbytr K Kˆ foranysimplexweightsw.
− −
LemmaF.1. ForanyPSDmatrixA Rn×nandw ∆ ,wehave
n−1
∈ ∈
w⊤Aw tr(Aw) max A λ (A),
i∈[n] ii 1
≤ ≤ ≤
whereλ (A)denotesthelargesteigenvalueofA.
1
ProofofLem.F.1. Notethat
w⊤Aw =√w⊤ diag(√w)Adiag(√w)√w =√w⊤ Aw√w.
Theconditionthatw ∆ implies √w =1,sothat
∈ n−1 ∥ ∥2
√w⊤ Aw√w λ (Aw) tr(Aw).
1
≤ ≤
40DebiasedDistributionCompression
Toseetr(Aw) maxi [n]A ,notethattr(Aw)=(cid:80) A w max A sincew ∆ ..
≤ ∈ ii i∈[n] ii i ≤ i∈[n] ii ∈ n−1
Sinceλ (A) = sup x⊤Ax,ifweleti∗ ≜ argmin A ,thenthesimplexweightwith1onthei∗-thentryhas
1 x:∥x∥ =1 i∈[n] ii
2
two-norm1,soweseethatmax A λ (A).
i∈[n] ii 1
≤
Ournextlemmaboundsthesuboptimalityofsurrogateoptimizationofalow-rankplusdiagonalapproximationofK.
LemmaF.2(Suboptimalityofsurrogateoptimization). LetkP beakernelsatisfyingAssum.1. Let
n
= (x 1,...,x n)
S ⊂
Rd beasequenceofpoints. DefineK ≜kP( n, n) Rn×n. SupposeK(cid:98) Rn×n isanotherPSDmatrixsuchthatK
S S ∈ ∈ ⪰
K(cid:98). DefineD ≜diag(K K(cid:98)),thediagonalpartofK K(cid:98),andformK′ ≜K(cid:98) +D. Letw′ argmin w′⊤K′w′.
− − ∈ w∈∆n−1
Thenforanyw ∆ ,
n−1
∈
(cid:16) (cid:17)
MMD2 kP(Sw n,P) ≤MMD2 kP(Sw nOPT,P)+tr (K −K(cid:98))w +max i∈[n](K −K(cid:98)) ii+(w⊤K′w −w′⊤K′w′). (44)
ProofofLem.F.2. SinceK =K′+(K K(cid:98)) Dbyconstruction,wehave
− −
w⊤Kw =w⊤K′w+w⊤(K K(cid:98))w w⊤Dw
− −
w⊤K′w+w⊤(K K(cid:98))w
≤ −
=(w⊤K′w w′⊤K′w′)+w′⊤K′w′+w⊤(K K(cid:98))w
− −
(cid:16) (cid:17)
(w⊤K′w w′⊤K′w′)+w′⊤K′w′+tr (K K(cid:98))w ,
≤ − −
whereweusedthefactthatD 0andLem.F.1. Next,bythedefinitionofw′,wehave
⪰
w′⊤K′w′ (w )⊤K′w =(w )⊤(K′ K)w +(w )⊤Kw
OPT OPT OPT OPT OPT OPT
≤ −
=(w OPT)⊤(D (K K(cid:98)))w OPT+(w OPT)⊤Kw
OPT
− −
(w )⊤Dw +(w )⊤Kw
OPT OPT OPT OPT
≤
max i∈[n](K K(cid:98)) ii+(w OPT)⊤Kw OPT,
≤ −
whereweusedthefactK K(cid:98) inthepenultimatestepandLem.F.1inthelaststep. Hencewehaveshownourclaim.
⪰
Lem. F.2 shows that to control MMD2 (Sw,P), it suffices to separately control the approximation error in terms of
(cid:16) (cid:17)
kP n
tr K Kˆ and the optimization error (w⊤K′w w′⊤K′w′). The next result establishes that using WeightedRPC-
− −
(cid:16) (cid:17)
holesky,wecanobtainpolynomialandexponentialdecayboundsfortr K Kˆ inexpectationdependingonthekernel
−
growthofkP.
PropositionF.1(ApproximationerrorofWeightedRPCholesky). Letk beakernelsatisfyingAssum.(α,β)-kernel. Let
be an infinite sequence of points in Rd. For any w ∆ , let F be the low-rank approximation factor output by
∞ n−1
S ∈
WeightedRPCholesky(k, Sn,w,r). DefineK ≜k( Sn, Sn). Ifr ≥(C √dR lonβ g+ 21 +√log2)2 −lo1 g2,then,withtheexpectation
takenovertherandomnessinWeightedRPCholesky,
E(cid:2) tr(cid:0)
(K
FF⊤)w(cid:1)(cid:3)
H , (45)
n,r
− ≤
whereH isdefinedas
n,r
H
≜(cid:40) 8(cid:80)n ℓ=U(r)(Lk( (cid:16)ℓRn))α2
(cid:17)
POLYGROWTH(α,β),
(46)
n,r 8(cid:80)n ℓ=U(r)exp 1 −( Lk(ℓ Rn))α1 LOGGROWTH(α,β),
forL definedin(6)and
k
(cid:36)(cid:114) (cid:37)
U(r)≜ r+ lo1 g2 1 . (47)
log2 − log2
Moreover,H satisfiestheboundsinThm.4.
n,r
41DebiasedDistributionCompression
ProofofProp.F.1. Recall the notation L k(R n) = C lod gR 2nβ from (6). Define q ≜ U(r) so that q is the biggest integer for
whichr 2q+q2log2. Thelowerboundassumptionofr ischosensuchthatq > L (R ) > 0. ByChenetal.(2022,
k n
≥
Theorem3.1)withϵ=1,wehave
E(cid:2) tr(cid:0) (K FF⊤)w(cid:1)(cid:3) 2(cid:80)n λ (Kw). (48)
− ≤ ℓ=q+1 ℓ
Sinceq >L (R ),wecanapplyCor.B.1toboundλ (Kw)forℓ q+1andobtain(45)sinceH (46)isconstructed
k n ℓ n,r
≥
tomatchtheboundswhenapplyingCor.B.1to(48). ItremainstojustifytheboundsforH inThm.4.
n,r
IfkisPOLYGROWTH(α,β),byAssum.(α,β)-kernelwehaveα<2. Hence
H
n,r
=8(cid:80)n ℓ=q(cid:16) Lk( ℓRn)(cid:17) α2 ≤8L k(R n)α2 (cid:82) q∞ −1ℓ− α2dℓ=8L k(R n)α2(q −1)1− α2 =O(cid:16) √r(R rn2β )α1(cid:17) ,
whereweusedthefactthat(cid:82) q∞ −1ℓ− α2dℓ=(q −1)1− α2 forα<2,L k(R n)=O(R nβ),andq =Θ(√r).
IfkisLOGGROWTH(α,β),then
(cid:18) (cid:16) (cid:17)1(cid:19)
H =8(cid:80)n exp 1 ℓ α =8e(cid:80)n cℓ1/α 8e(cid:82)∞ cℓ1/α,
n,r ℓ=q − Lk(Rn) ℓ=q ≤ ℓ=q−1
wherec≜exp(cid:0) L (R )−1/α(cid:1) (0,1). Definingm≜ logc>0andq′ =q 1,wehave
k n
− ∈ − −
(cid:82)∞ cx1/α dx=(cid:82)∞ exp(cid:0) mx1/α(cid:1) dx=αq′(mq′1/α)−αΓ(α,mq′1/α)=αm−αΓ(α,mq′1/α), (49)
x=q′ x=q′ −
whereΓ(α,x)≜(cid:82)∞ tα−1e−tdtistheincompletegammafunction. Sinceα>0,byPinelis(2020,Thm.1.1),wehave
x
Γ(α,mq′1/α) (mq′1/α+b)α−(mq′1/α)α e−mq′1/α,
≤ αb
where b is a known constant depending only on α. By the equivalence of norms on R2, there exists C > 0 such that
α
(x+y)α C (xα+yα)foranyx,y >0. Hence
α
≤
Γ(α,mq′1/α) (mq′1/α+b)α e−mq′1/α Cα(mαq′+bα)e−mq′1/α.
≤ αb ≤ αb
Hencefrom(49)wededuce
(cid:80)∞ cℓ1/α C (q′b−1+bα−1m−α)e−mq′1/α. (50)
ℓ=q′ ≤ α
Sincem= logc=L (R )−1/α,wecanboundtheexponentby
k n
−
√
mq′1/α = (L (R )−1q′)1/α = (q′log2)1/α (0.83 r−2.39)1/α,
− − k n − CdRnβ ≤− CdRnβ
(cid:114)
whereweusedthefactthatq′log2=(q 1)log2 ( r+ lo1 g2 1 2)log2 0.83√r 2.39. Ontheotherhand,
− ≥ log2 − log2 − ≥ −
sinceq′ =q 1 L(R )=m−α,wecanabsorbthebα−1m−1termin(50)intoqandfinallyobtaintheboundsforH
n n,r
− ≥
inThm.4.
Thelastpieceofouranalysisinvolvesboundingtheoptimizationerror(w⊤K′w w′⊤K′w′)in(44).
−
Lemma F.3 (AMD guarantee for debiasing). Let K Rn×n be an SPSD matrix. Let f(w) ≜ w⊤Kw. Then the final
∈
iteratex ofNesterov’s1-memorymethod(Wangetal.,2023,Algorithm14)afterT stepswithobjectivefunctionf(w),
T
norm = , distance-generating function ϕ(x) = (cid:80)n x logx , and initial point w = (1,..., 1) ∆
∥·∥ ∥·∥1 i=1 i i 0 n n ∈ n−1
satisfies
f(w ) f(w )
16lognmaxi∈[n]Kii,
T − OPT ≤ T2
wherew
OPT
∈argmin x∈Rnf(x).
42DebiasedDistributionCompression
ProofofLem.F.3. We apply Wang et al. (2023, Theorem 14). Hence it remains to determine the smoothness constant
L>0suchthat,forallx,y ∆ ,
n−1
∈
f(x) f(y) L x y ,
∥∇ −∇ ∥∞ ≤ ∥ − ∥1
and an upper bound for the Bregman divergence Dϕ (w ) = (cid:80)n w logwOPTi = (cid:80)n w lognw . To
w0 OPT i=1 OPTi (w0)i i=1 OPTi OPTi
determineL,note f(w)=2Kw,sowehave,foranyx,y ∆ ,
n−1
∇ ∈
f(x) f(y) =2 K(x y) =2max K (x y)
∥∇ −∇ ∥∞ ∥ − ∥∞ i∈[n] | i,: − |
(cid:0) (cid:1) (cid:0) (cid:1)
2max K x y =2 max K x y =2 max K x y ,
≤ i∈[n] ∥ i,: ∥∞∥ − ∥1 i∈[n] ii ∥ − ∥1 i∈[n] ii ∥ − ∥1
whereweusedthefactthatthelargestentryinanSPSDmatrixappearsonitsdiagonal. Thuswecantakethesmoothness
constanttobe
L=2max K .
i∈[n] ii
ToboundDϕ (w ),notethatbyJensen’sinequality,
w0 OPT
Dϕ (w)=(cid:80)n w lognw log(cid:0)(cid:80)n nw2(cid:1) =logn+log w 2 logn,
w0 i=1 i i ≤ i=1 i ∥ ∥2 ≤
whereweusedthefactthat w 2 w =1forw ∆ .
∥ ∥2 ≤∥ ∥1 ∈ n−1
Withthesetoolsinhand,weturntotheproofofThm.4. FortheruntimeofLD,itfollowsfromthefactthatWeightedRP-
CholeskytakesO((d +r)nr)timeandonestepofAMDtakesO(nr)time.
kP
Theerroranalysisisdifferentforthefirstadaptiveiterationandtheensuingadaptiveiterations. Roughlyspeaking,wewill
showthatthefirstadaptiveiterationbringstheMMDgaptothedesiredlevel,whiletheensuingiterationsdonotintroduce
anexcessiveamountoferror.
Step1. Bound∆MMD (w(1))
kP
Let K ≜ kP( n, n) and F denote the low-rank approximation factor generated by WeightedRPCholesky. Denote
S S
K(cid:98) ≜ FF⊤. Then K′ = K(cid:98) +diag(K K(cid:98)). First, note that since w(0) = (1,..., 1), Resample returns w˜ = w(0)
− n n
with probability one. By Lem. F.2, we have, using √a+b √a + √b for a,b 0 repeatedly and Lem. F.1 that
(cid:16) (cid:17) ≤ ≥
tr (K K(cid:98))w λ 1(K K(cid:98))andmax i∈[n](K K(cid:98))
ii
λ 1(K K(cid:98)),
− ≤ − − ≤ −
(cid:113) (cid:113)
MMD kP(Sw n(1),P) ≤MMD kP(Sw nOPT,P)+ 2λ 1(K −K(cid:98))+ w(1)⊤ K′w(1) −w′⊤K′w′
(cid:113) (cid:113)
≤MMD kP(Sw nOPT,P)+ 2λ 1(K −K(cid:98))+ 16log Tn 2∥kP∥n,
whereweappliedLem.F.1andLem.F.3inthelastinequality. Fixδ (0,1). ByMarkov’sinequality,wehave
∈
(cid:18)(cid:113) (cid:113) (cid:19)
Pr λ 1(K −K(cid:98))>
E[λ1(K δ−K(cid:98))]
≤δ.
Thismeansthatwithprobabilityatleast1 δ,wehave
−
(cid:113) (cid:113)
MMD kP(Sw n(1),P) ≤MMD kP(Sw nOPT,P)+ 2E[λ1( δK−K(cid:98))] + 16log Tn 2∥kP∥n.
NotethatthelowerboundconditiononrinAssum.(α,β)-paramsimpliesthelowerboundconditioninProp.F.1. Hence,
(cid:16) (cid:17)
by Prop. F.1 with w = ( n1,..., n1) and using the identity λ 1(K −K(cid:98))
≤
tr K −K(cid:98) while noting that a factor of n
appears,wehave
(cid:113) (cid:113)
MMD kP(Sw n(1),P) ≤MMD kP(Sw nOPT,P)+ 2nH δn,r + 16∥kP T∥n 2logn.
43DebiasedDistributionCompression
Step2. Boundtheerroroftheremainingiterations
Fixδ >0. Thepreviousstepshowsthat,withprobabilityatleast1 δ,
− 2
(cid:113) (cid:113)
MMD kP(Sw n(1),P) ≤MMD kP(Sw nOPT,P)+ 4nH δn,r + 16∥kP T∥n 2logn.
Fixq >1,andletw˜betheresampledweightdefinedintheq-thiterationinAlg.2. Withoutlossofgenerality,weassume
w˜ >0foralli> 0,sinceifw =0thenindexiisirrelevantfortherestofthealgorithm. Thus,thankstoResample,we
i i
havew˜ 1/nforalli [n]. Leta/bdenotetheentry-wisedivisionbetweentwovectors. Asinthepreviousstepofthe
i
≥ ∈
proof,weletK ≜ kP( n, n),F bethelow-rankfactoroutputbyWeightedRPCholesky(kP, n,w˜,r),andK(cid:98) = FF⊤.
Foranyw ∆ ,recS alltS henotationKw ≜diag(√w)Kdiag(√w). Thenwehave S
n−1
∈
w⊤Kw =(w/√w˜)⊤diag(Kw˜)(w/√w˜)
=(w/√w˜)⊤(diag(√w˜)K(cid:98)diag(√w˜))+diag(√w˜)(K K(cid:98))diag(√w˜)))(w/√w˜)
−
=w⊤K(cid:98)w+(w/√w˜)⊤(diag(√w˜)(K K(cid:98))diag(√w˜)))(w/√w˜)
−
(cid:16) (cid:17)
w⊤K(cid:98)w+max i∈[n](1/w˜ i)tr diag(√w˜)(K K(cid:98))diag(√w˜)
≤ −
(cid:16) (cid:17)
w⊤K(cid:98)w+ntr (K K(cid:98))w˜ . (51)
≤ −
Notethat
K′ =K(cid:98) +diag(K K(cid:98))=K+(K(cid:98) K)+diag(K K(cid:98)).
− − −
SinceK′ K(cid:98),wehave
⪰
w(q)⊤ K(cid:98)w(q) w(q)⊤ K′w(q) w˜⊤K′w˜, (52)
≤ ≤
wherethelastinequalityfollowsfromtheifconditioningattheendofAlg.2. Inaddition,
w˜⊤K′w˜ =w˜⊤(K+(K(cid:98) K)+diag(K K(cid:98)))w˜
− −
w˜⊤Kw˜+w˜⊤diag(K K(cid:98))w˜
≤ −
⊤
=w˜⊤Kw˜+√w˜ diag((K K(cid:98))w˜)√w˜
−
(cid:16) (cid:17)
w⊤Kw˜+tr (K K(cid:98))w˜ ,
≤ −
(cid:13) (cid:13)
whereweusedthefactthatK K(cid:98) and(cid:13)√w˜(cid:13) = 1. Pluggingthepreviousinequalityinto(52)andtheninto(51)with
(cid:13) (cid:13)
⪰ 2
w =w(q),weget
(cid:16) (cid:17)
w(q)⊤ Kw(q) w˜⊤Kw˜+(n+1)tr (K K(cid:98))w˜ . (53)
≤ −
Takingsquare-rootonbothsidesusing√a+b √a+√bfora,b 0andthetriangleinequality,weget
≤ ≥
(cid:114)
(cid:16) (cid:17)
MMD kP(Sw n(q),P) ≤MMD kP(Sw n˜,P)+ (n+1)tr (K −K(cid:98))w˜
(cid:114)
(cid:16) (cid:17)
≤MMD kP(Sw n(q−1),P)+MMD kP(Sw n(q−1),Sw n˜)+ (n+1)tr (K −K(cid:98))w˜ .
ByMarkov’sinequality,wehave
Pr(cid:32)
MMD
(Sw(q−1),Sw˜)>(cid:114) 4QE(cid:104) MMD2 kP(Sw n(q−1),Sw n˜)(cid:105)(cid:33)
δ
kP n n δ ≤ 4Q
Pr(cid:18)(cid:114) tr(cid:16)
(K
K(cid:98))w˜(cid:17) >(cid:113) 4QE[tr((K−K(cid:98))w˜)](cid:19)
δ .
− δ ≤ 4Q
44DebiasedDistributionCompression
ByProp.E.1(c),wehave
(cid:104) (cid:105) (cid:104) (cid:104) (cid:12) (cid:105)(cid:105)
E MMD2 (Sw(q−1),Sw˜) =E E MMD2 (Sw(q−1),Sw˜)(cid:12)w(q−1) ∥kP∥n.
kP n n kP n n (cid:12) ≤ n
Thusbytheunionbound,withprobabilityatleast1 δ ,usingProp.F.1(recalllow-rankapproximationK(cid:98) isobtained
− 2Q
usingw˜),wehave
(cid:113) (cid:113)
MMD (Sw(q),P) MMD (Sw(q−1),P)+ 4Q∥kP∥n + 4Q(n+1)Hn,r. (54)
kP n ≤ kP n nδ δ
Finally,applyingunionboundandsumminguptheboundsforq =1,...,Q,weget,withprobabilityatleast1 δ,
−
(cid:113) (cid:113) (cid:18)(cid:113) (cid:113) (cid:19)
∆MMD (w(q)) 2nHn,r + 16∥kP∥nlogn +(Q 1) 4Q∥kP∥n + 4Q(n+1)Hn,r .
kP ≤ δ T2 − nδ δ
ThismatchesthestatedasymptoticboundinThm.4.
F.2.ThinningwithKT-Compress++
ForcompressionwithtargetdistributionP,wemodifytheoriginalKT-Compress++algorithmof(Shettyetal.,2022,Ex.6):
in HALVE and THIN ofCompress++,weuse KT-SPLIT withkernelkP without KT-SWAP (soourversionofCompress++
outputs 2g coresets, each of size √n), followed by KT-Swap to obtain a size √n coreset. We call the resulting thinning
algorithmKT-Compress++. WeshowinLem.F.4andCor.F.1thatKT-Compress++satisfiesanMMDguaranteesimilar
tothatofquadratic-timekernelthinning.
AlgorithmF.3KT-Compress++(modifiedShettyetal.(2022,Alg.2)tominimizeMMDtoP)
Input: kernel kP with zero-mean under P, input points
n
= (x i) i∈[n], multiplicity n′ with n′ 4N, weight w
∆ n−1
∩(N
n0
′)n,thinningparameterg,failureprobabilityS
δ
∈ ∈
S indexsequencewherek [n]appearsn′w times
k
← ∈
(I(ℓ))
ℓ∈[2g]
Compress++(g, n[S])▷Shettyetal.(2022,Ex.6)withKTsubstitutedwithKT-SPLITinHALVEandTHIN.
← S
I(ℓ) S[I(ℓ)]foreachℓ [2g]
← ∈
I KT-Swap(kP, n,(I(ℓ)) ℓ∈[2g])
← S
w simplexweightscorrespondingtoI▷w = numberofoccurrencesofiinI
C++ ← i |I|
Return: w C++ ∈∆ n−1 ∩(√N 0 n)n▷Hence ∥w C++ ∥0 ≤√n
LemmaF.4(Sub-gaussianguaranteeforCompress++). Let beasequenceofnpointswithn 4N. Foranyδ (0,1)
n
S ∈ ∈
and integer g log log(n+1) + 3.1 , consider the Compress++ algorithm (Shetty et al., 2022, Algorithm 2) with
≥ ⌈ 2 ⌉
thinning parameter g, halving algorithm HALVE(k) ≜ symmetrized6(KT-SPLIT(k, ,1, n2 k δ)) for an input
(cid:16) (cid:17) · 4n2g(g+(βn+1)2g)
of n
k
≜ 2g+1+k√n points and β
n
≜ log
2
nn
0
, and with thinning algorithm THIN ≜ KT-SPLIT(k, ·,g, g+(βng +1)2gδ).
Then this instantiation of Compress++ compresses to 2g coresets ( (i) ) of √n points each. Denote the signed
Sn Sout i∈[2g]
(cid:16) (cid:17)
measure ϕ(i) ≜ n1 (cid:80) x∈Snδ
x −
√1 n(cid:80)
x∈S o( ui
t)δ x. Then for each i
∈
[2g], on an event Ee( qi u)
i
with Pr Ee( qi u)
i ≥
1
−
2δ,
ϕ(i) =ϕ˜(i)forarandomsignedmeasureϕ˜(i)suchthat,foranyδ′ (0,1),
∈
Pr(cid:18)(cid:13) (cid:13)ϕ˜(i)k(cid:13)
(cid:13) a′
(cid:16) 1+(cid:113) log(cid:0)1(cid:1)(cid:17)(cid:19)
δ′,
(cid:13) (cid:13) Hk ≥ n δ′ ≤
where
a′
n
= √4 n(cid:32) 2+(cid:115) 8 3∥k ∥nlog(cid:18) 6√ n(g+(lo δg 22n−g)2g)(cid:19) log(cid:0) 4 Nk(cid:0) B2(R n),n−1/2(cid:1)(cid:1)(cid:33) .
6Anyhalvingalgorithmcanbeconvertedintoanunbiasedonebysymmetrization,i.e.,returningeithertheoutputhalforitscomple-
mentwithequalprobability(Shettyetal.,2022,Remark3).
45DebiasedDistributionCompression
ProofofLem.F.4. This proof is similar to the one for Shetty et al. (2022, Ex. 6) but with explicit constant tracking and
is self-contained, invoking only Shetty et al. (2022, Thm. 4) which gives MMD guarantees for Compress++ given the
sub-GaussianparametersofHALVEandTHIN.
Recallthatn isthenumberofinputpointsforthehalvingsubroutineatrecursionlevelk inCompress++,andβ isthe
k n
totalnumberofrecursionlevels. Let
C
denotetheoutputof COMPRESS (Shettyetal.,2022,Alg.1)ofsize2g√n. Fix
δ,δ′ (0,1). Suppose we use HALVS E(k) ≜ symmetrized(KT-SPLIT(k, ,1,γ kδ)) for an input of n
k
points for γ
k
to be
determ∈ ined. SupposeweuseTHIN ≜ KT-SPLIT(k, ,g,γ′δ)forγ′ tobed· etermined;thisisthekernelthinningstagethat
·
thins 2g√n points to 2g coresets, each with √n points. Since the analysis is the same for all coresets, we will fix an
arbitrarycoresetwithoutsuperscriptinthenotation.
By Lem. D.1, with notation t ≜ log 1, there exist events , , and random signed measures ϕ , ϕ˜ , ϕ , ϕ˜ for
δ′ Ek,j ET k,j k,j T T
0 k β andj [4k]suchthat
n
≤ ≤ ∈
(cid:16) (cid:17)
(a) Pr c γkδ andPr( c) γ′δ,
Ek,j ≤ 2 ET ≤ 2
(b) 1 ϕ =1 ϕ˜ and1 ϕ =1 ϕ˜ ,
Ek,j k,j Ek,j k,j ET T ET T
(c) Wehave
Pr(cid:18)(cid:13) (cid:13) (cid:13)ϕ˜ k,jk(cid:13) (cid:13) (cid:13)
Hk
≥a nk +v nk√t(cid:12) (cid:12) (cid:12) (cid:12){ϕ˜ k′,j′ }k′>k,j′≥1, {ϕ˜ k′,j′ }k′,j′<j(cid:19) ≤e−t
Pr(cid:18)(cid:13) (cid:13) (cid:13)ϕ˜ Tk(cid:13) (cid:13) (cid:13)
Hk
≥a′ n+v n′√t(cid:12) (cid:12) (cid:12) (cid:12)S C(cid:19) ≤e−t,
where,byLem.D.1,andbyincreasingthesub-Gaussianconstantsifnecessary,wehave
(cid:18) (cid:114) (cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)(cid:19)
a nk ≜v nk ≜a nk,nk/2 = n2 k 2+ 8 3∥k ∥nlog 3 γn kk δ log 4 Nk B2(R n), n2 k ,
a′
n
≜v n′ ≜a 2g√ n,√
n
= √1 n(cid:18) 2+(cid:114) 8 3∥k ∥nlog(cid:16) 6g γ√ ′δn(cid:17) log(cid:0) 4 Nk(cid:0) B2(R n),n−1/2(cid:1)(cid:1)(cid:19) , and
(cid:104) (cid:12) (cid:105)
(d) E ϕ˜ k,jk(cid:12)
(cid:12)
ϕ˜
k′,j′
k′>k,j′≥1, ϕ˜
k′,j′ k′,j′<j
=0.
{ } { }
Hence on the event = (cid:84) , these properties hold simultaneously. We will choose γ and γ′ such that
E
k,jEk,j ∩ET
{
k }k
Pr( c) δ. Bytheunionbound,
E ≤ 2
(cid:16) (cid:17)
Pr( c) Pr( c)+(cid:80)βn (cid:80)4k Pr c γ′δ +(cid:80)βn 4kγkδ. (55)
E ≤ ET k=0 j=1 Ek,j ≤ 2 k=0 2
Ontheevent ,weapplyShettyetal.(2022,Thm.4,Rmk.7)togetasub-GaussianguaranteeforMMD (S ,S ). We
k n out
wanttochoosE eγ k,γ′ suchthattherescaledquantitiesζ˜
H
≜ n 20a
n0
andζ˜
T
≜ √na′
n
satisfyζ˜
H
= ζ˜
T
(Shettyetal.,2022,
Eq.(13)),whichimpliesthatweneed
√
3n0 = 6g n γ0 = 2g. (56)
γ0δ γ′δ ⇐⇒ γ′ g
Henceifwetakeγ′ = g+(βng +1)2g andγ k = 4n2g(g+n (β2 k n+1)2g),then(56)holdsandtheupperboundin(55)becomes 2δ.
Note that n a is non-decreasing in n , so by Shetty et al. (2022, Theorem 4, Remark 7), Compress++(δ,g) outputs a
k nk k
signedmeasureϕthat,ontheevent withPr( c) δ,equalsanothersignedmeasureϕ˜thatsatisfies,foranyδ′ (0,1),
E E ≤ 2 ∈
(cid:18)(cid:13) (cid:13) (cid:113) (cid:19)
Pr (cid:13)ϕ˜k(cid:13) aˆ +vˆ log(cid:0)1(cid:1) δ′,
(cid:13) (cid:13) Hk ≥ n n δ′ ≤
whereaˆ ,vˆ satisfymax(aˆ ,vˆ ) 4a′ wheneverg log log(n+1)+3.1 .
n n n n ≤ n ≥⌈ 2 ⌉
46DebiasedDistributionCompression
Corollary F.1 (MMD guarantee for Compress++). Let be an infinite sequence of points in Rd and k a kernel. For
∞
any δ (0,1) and n N such that n 4N, considerS the Compress++ with the same parameters as in Lem. F.4 with
∈ ∈ ∈
g log log(n+1)+3.1 . Thenforanyi [√n],withprobabilityatleast1 δ,
≥⌈ 2 ⌉ ∈ −
MMD k(S n,S( oi u) t)
≤
√4 n(cid:32) 2+(cid:115) 8 3∥k ∥nlog(cid:18) 6√ n(g+(lo δg 22n−g)2g)(cid:19) log(cid:0) 4 Nk(cid:0) B2(R n),n−1/2(cid:1)(cid:1)(cid:33) (cid:16) 1+(cid:113) log2 δ(cid:17) .
Proof. AfterapplyingLem.F.4withδ′ = δ andfollowingthesameargumentasintheproofofCor.D.1,wehave,with
2
probabilityatleast1 δ,
−
(cid:16) (cid:113) (cid:17)
MMD (S ,S(i) ) a′ 1+ log2 .
k n out ≤ n δ
Pluggingintheexpressionofa′ fromLem.F.4givestheclaimedbound.
n
F.3.ProofofThm.5: MMDguaranteeforLSKT
Firstofall,theclaimedruntimefollowsfromtheruntimeofLD(Thm.4),theO(d 4gnlogn)runtimeofCompress++,
kP
andtheO(d n1.5)runtimeofKT-Swap.
kP
Withoutlossofgeneralityassumen 4N. Fixδ (0,1). Letw⋄ denotetheoutputofLD,andwsr denotetheoutputof
Resample,bothregardedasrandomv∈ ariables. By∈ Thm.4,wehave,withprobabilityatleast1 δ,
− 3
(cid:18)(cid:113) (cid:19) (cid:18)(cid:113) (cid:19)
MMD kP(Sw n⋄,P)=MMD kP(Sw nOPT,P)+O nH δn,r +O ∥kP∥nmax n(logn,1/δ) . (57)
ByProp.E.1(c)withk=kP,wehavetheupperbound
E(cid:2) MMD2 (Swsr,Sw⋄ )(cid:3) =E(cid:2)E(cid:2) MMD2 (Swsr,Sw⋄ )(cid:12) (cid:12)w⋄(cid:3)(cid:3) ∥kP∥n.
kP n n kP n n ≤ n
Thus,byMarkov’sinequality,
(cid:18) (cid:113) (cid:19)
Pr MMD (Swsr,Sw⋄ ) 3∥kP∥n δ.
kP n n ≥ nδ ≤ 3
Hence,withprobabilityatleast1 δ,wehave
− 3
(cid:113)
MMD (Swsr,Sw⋄ ) 3∥kP∥n. (58)
kP n n ≤ nδ
Let S( oi u)
t
denotethe i-thcoresetoutput by THIN inKT-Compress++(Alg. F.3). ByCor. F.1withk = kP, wehave, with
probabilityatleast1 δ,
− 3
(cid:18)(cid:113) (cid:19)
MMD (Swsr,S(i) )=O ∥kP∥nlognlog(eNkP(B2(Rn),n−1/2))) log e .
kP n out n δ
SinceKT-SwapcanneverincreaseMMD (,P),wehave,bythetriangleinequality,
kP
·
MMD kP(Sw nLSKT,P) ≤MMD kP(S( o1 ut),P) ≤MMD kP(S( o1 ut),Sw nsr )+MMD kP(Sw nsr,Sw n⋄ )+MMD kP(Sw n⋄,P). (59)
Bytheunionbound,withprobabilityatleast1 δ,thebounds(57),(58),(59)hold,sothattheclaimisshownbyadding
−
togethertheright-handsidesoftheseboundsandapplyingAssum.(α,β)-kernel.
G.Simplex-WeightedDebiasedCompression
In this section, we provide deferred analyses for RT and SR/LSR, as well as the algorithmic details of Recombination
(Alg.G.1)andKT-Swap-LS(Alg.G.2).
47DebiasedDistributionCompression
AlgorithmG.1Recombination(rephrasingofTchernychova(2016,Alg. 1)thattakesO(m3logn)time)
Input: matrixA Rm×nwithm<nandonerowofAallpositive,anonnegativevectorx Rn .
∈ 0 ∈ ≥0
functionFindBFS(A,x )
0
▷TherequirementofAandx arethesameastheinput. ThissubroutinetakesO(n3)time.
0
x x
0
←
U,S,V⊤ SVD(A)▷anyO(n3)-timeSVDalgorithmthatgivesUSV⊤ =A
V (V⊤← ) ▷V R(n−m)×nsothatthenullspaceofAisspannedbytherowsofV
m+1:n
← ∈
fori=1to n mdo
−
v V
i
k← argmin xj ▷ This must succeed because Av = 0 and A has an all-positive row, so one of the
← j∈[n]:vj>0 vj
coordinatesofvmustbepositive.
x x xkv▷Thiszerosoutthek-thcoordinateofxwhilestillensuringxisnonnegative.
← − vk
forj =i+1to n mdo
V V Vj,kv− ▷ V n−m remainindependentandhave0onthek-thcoordinate.
j ← j − vk { j }j=i+1
endfor
endfor
return: x Rn suchthatAx=Ax and x m.
∈ ≥0 0 ∥ ∥0 ≤
endfunction
x x
0
←
while x >2mdo
∥ ∥0 (cid:106) (cid:107)
Divide {i ∈[n]:x i >0 }into2mindexblocksI 1,...,I 2m,eachofsizeatmost ∥ 2x m∥ 0 .
A A x Rm, i [2m]
i
←
:,Ii Ii
∈ ∀ ∈
FormAˆtobethem 2mmatrixwithcolumnsA ▷Hence,onerowofAcontainsallpositiveentries.
i
xˆ
FindFBS(Aˆ,1×
)▷ xˆ
nandAˆxˆ=(cid:80)
A xˆ
=(cid:80)
A
=(cid:80)
A x =Ax.
← 2m ∥ ∥0 ≤ i i i :,Ii Ii
fori=1 to 2mdo
x xˆ x if xˆ >0 else 0
Ii
←
i
·
Ii i
endfor
▷Aftertheupdate,thesupportofxshrinksby2whileitmaintainsthatAx=Ax .
0
endwhile
if x m+1then
∥ ∥0 ≥
I i [n]:x >0
i
←{ ∈ }
x =FindBFS(A ,x )
I :,I I
endif
Return: x Rn suchthatAx=Ax and x m.
∈ ≥0 0 ∥ ∥0 ≤
G.1.MMDguaranteeforRT
Proposition G.1 (RT guarantee). Under Assums. 1 and (α,β)-kernel, given w
∈
∆
n−1
and that m
≥
(C √dR lonβ g+ 21 +
√log2)2 1 +1, RecombinationThinning (Alg. 4) outputs w ∆ with w m in O((d +m)nm+
− log2 RT ∈ n−1 ∥ RT ∥0 ≤ kP
m3logn)timesuchthatwithprobabilityatleast1 δ,
−
(cid:113) (cid:113)
MMD kP(Sw nRT,P) ≤MMD kP(Sw n,P)+ 2∥k nP δ∥n + 2nHn δ,m−1, (60)
whereH isdefinedin(46).
n,r
ProofofProp.G.1. The runtime follows from the O((d + m)nm) runtime of WeightedRPCholesky, the O(d nm)
kP kP
runtime of KT-Swap-LS, and the O(m3logn) runtime of Recombination (Tchernychova, 2016) which dominates the
O(m3)weightoptimizationstep.
Recallw′ ∆ fromRT.TheformationofF inAlg.4isidenticaltotheformationofF (withr =m 1)inAlg.2for
n−1
∈ −
q >1. Thusby(51)withw =w′,K =kP( n, n),
S S
w′⊤Kw′ w′⊤FF⊤w′+ntr(cid:0) (K FF⊤)w˜(cid:1) ,
≤ −
48DebiasedDistributionCompression
AlgorithmG.2KT-SwapwithLinearSearch(KT-Swap-LS)
Input: kernelkPwithzero-meanunderP,inputpoints
n
=(x i) i∈[n],weightsw ∆ n−1,fmt SPLX,CP
S ∈ ∈{ }
S i [n]:w =0
i
←{ ∈ ̸ }
▷Maintaintwosufficientstatistics: g =KwandD =w⊤Kw.
functionAdd(g,D,i,t)
g g+tkP( n,x i)
← S
D D+2tg i+t2kP(x i,x i)
←
return: (g,D)
endfunction
functionScale(g,D,α)
g αg
←
D α2D
←
return: (g,D)
endfunction
Kdiag kP( n, n)
g 0
←Rn S S
← ∈
D 0
←
foriin Sdo
(g,D) Add(g,D,i,w )
i
←
endfor
foriin Sdo
(cid:80)
if w =1thencontinue;▷Wecannotswapioutif w =0!
i j̸=i j
▷Firstzerooutw .
i
(g,D) Add(g,D,i, w )
i
(g,D)←
Scale(g,D,
−1
)
← 1−wi
w =0
i
▷Nextperformlinesearchtoaddbackapoint.
α = (D g)./(D 2g+Kdiag); ▷α = argmin MMD2 (Stei+(1−t)w,P) = argmin (1 t)2D+2t(1 t)g+
− − i t kP n t − −
t2Kdiag
iffmt=SPLXthen
α=clip(α,0,1);▷Clippingαto[0,1]. Thiscorrespondstoargmin MMD2 (Stei+(1−t)w,P).
t∈[0,1] kP n
endif
D′ (1 α)2D+2α(1 α)g+α2Kdiag▷multiplicationsareelement-wise
← − −
k argmin D′
← i i
(g,D) Scale(g,D,1 α )
k
← −
(g,D) Add(g,D,k,α )
k
←
endfor
Return: w ∆
n−1
∈
whereK =kP( n, n). Byconstructionofw′usingRecombination,wehaveF⊤w˜ =F⊤w′. SinceK FF⊤,wehave
S S ⪰
w′⊤Kw′ w˜⊤FF⊤w˜+ntr(cid:0) (K FF⊤)w˜(cid:1) w˜⊤Kw˜+ntr(cid:0) (K FF⊤)w˜(cid:1) .
≤ − ≤ −
We recognize the right-hand side is precisely the right-hand side of (53) aside from having a multiplier of n instead of
n+1infrontofthetraceandthatF isrankm 1. Nowapplying(54)withQ= 1,w(q) =w′,w(q−1) =w,r =m 1,
− 2 −
andnoticingthatKT-Swap-LSandthequadratic-programmingsolveattheendcannotdecreasetheobjective, weobtain
(60)withprobabilityatleast1 δ. NotethatthelowerboundofminAssum.(α,β)-paramsmakesr =m 1satisfythe
− −
lowerboundforrinProp.F.1.
G.2.ProofofThm.6: MMDguaranteeforSR/LSR
The claimed runtime follows from the runtime of SteinThinning (Alg. D.1) or LD (Thm. 4) plus the runtime of RT
(Prop.G.1).
49DebiasedDistributionCompression
NotethelowerboundforminAssum.(α,β)-paramsimpliesthelowerboundconditioninProp.G.1. ForthecaseofSR,
weproceedasintheproofofThm.3anduseProp.G.1. ForthecaseofLSR,weproceedasintheproofofThm.5and
useThm.4andProp.G.1.
H.Constant-PreservingDebiasedCompression
Inthissection,weprovidedeferredanalysesforCTandSC/LSC.
H.1.MMDguaranteeforCT
PropositionH.1(CTguarantee). UnderAssums.1and(α,β)-kernel,givenw ∈∆ n−1 andm ≥(C √dR lonβ g+ 21 + √ lo2 g2)2
−
1 , CT outputs w Rn with 1⊤w = 1 and w m in O((d +m)nm+m3) time such that, for any
log2 CT ∈ n CT ∥ CT ∥0 ≤ kP
δ (0,1),withprobability1 δ,
∈ −
(cid:113)
MMD kP(Sw nCT,P) ≤2MMD kP(Sw n,P)+ 4H n δ,m′,
whereH isdefinedin(46)andm′ ≜m+log2 2√mlog2+1.
n,m
−
ProofofProp.H.1. TheruntimefollowsfromtheO((d +m)nm)runtimeofWeightedRPCholesky,theO(nm)runtime
kP
ofKT-Swap-LS,andtheO(m3)runtimeofmatrixinversioninsolvingthetwominimizationproblemsusing(64).
To improve the clarity of notation, we will use w⋄ to denote the input weight w to CT. For index sequences I,J [n]
⊂
and a kernel k, we use k(I,J) to indicate the matrix k( [I], [J]) = [k(x ,x )] , and similarly for a function
n n i j i∈I,j∈J
f :Rn R,weusef(I)todenotethevector(f(x )) S . S
i i∈I
→
Recalltheregularizedkernelisk
c
≜kP+c. Supposefornowthatc>0isanarbitraryconstant. LetIdenotetheindices
outputbyWeightedRPCholeskyinCT.Let
wc ≜argmin MMD2 (Sw,Sw⋄ ).
w:supp(w)⊂I kc n n
Notethatwcisnotaprobabilityvectorandmaynotsumto1.
Step1. BoundMMD2 (Swc,Sw⋄ )intermsofWeightedRPCholeskyapproximationerror
kc n n
WestartbyusinganargumentsimilartothatofEpperlyandMoreno(2024,Prop. 3)toexploittheoptimalityconditionof
wc. Since
argmin MMD2 (Sw,Sw⋄ )=argmin w⊤k (I,I)w 2w⋄⊤k ( ,I)w ,
w:supp(w)⊂I kc n n w:supp(w)⊂I I c I − c Sn I
byoptimality,wcsatisfies,
k (I,I)wc =Sw⋄k (I).
c I n c
We comment that the index sequence I returned by WeightedRPCholesky makes k (I,I) invertible with probability 1:
c
bytheGuttmanrankadditivityformulaofSchurcomplement(Zhang,2006,Eq.(6.0.4)),eachiterationofWeightedRPC-
holeskychoosesapivotwithanon-zerodiagonalandthusincreasestherankofthelow-rankapproximationmatrix,which
isspannedbythecolumnsofpivots,by1. Hence
Swck ()=k (, )wc =k (,I)wc =k (,I)k (I,I)−1k (I,I)wc
n c · c · Sn c · I c · c c I
=k (,I)k (I,I)−1Sw⋄k (I)=Sw⋄k (),
c · c n c n cI ·
wherek (x,y)≜k (x,I)k (I,I)−1k (I,y). Then
cI c c c
MMD2 kc(Sw nc,Sw n⋄ )=(cid:13) (cid:13)Sw n⋄k
c
−Sw nck c(cid:13) (cid:13)2
kc
=(cid:13) (cid:13)Sw n⋄k
c
−Sw n⋄k cI(cid:13) (cid:13)2
kc
=w⋄⊤(k
c
−k cI)( Sn, Sn)w⋄.
RecalltheindexsetIconsistsofthempivotsselectedbyWeightedRPCholeskyontheinputmatrix
K⋄ ≜k ( , )w⋄.
c c Sn Sn
50DebiasedDistributionCompression
Define
K(cid:98) c⋄ ≜k cI( Sn, Sn)w⋄.
Thus,byLem.F.1,
MMD2 kc(Sw nc,Sw n⋄ )=w⋄⊤(k
c
−k cI)( Sn, Sn)w⋄ =√w⋄⊤ (K c⋄ −K(cid:98) c⋄)√w⋄ ≤λ 1(K c⋄ −K(cid:98) c⋄) ≤tr(cid:16) K c⋄ −K(cid:98) c⋄(cid:17) .
(cid:16) (cid:17)
Step2. Boundtr K⋄ K(cid:98)⋄ usingthetraceboundoftheunregularizedkernel
c − c
Let A denotethebestrank-rapproximationofanSPSDmatrixA Rn×ninthesensethat
(cid:74) (cid:75)r ∈
A ≜argmin tr(A X). (61)
(cid:74) (cid:75)r X X∈ =R Xn× ⊤n −
A⪰X⪰0
rank(X)≤r
BytheEckart-Young-Mirskytheoremappliedtosymmetricmatrices(Daxetal.,2014,Theorem19),thesolutionto(61)
isgivenbyr-truncatedeigenvaluedecompositionofA,sothat
tr(A A
)=(cid:80)n
λ (A).
−(cid:74) (cid:75)r ℓ=r+1 ℓ
Letq ≜U(m)whereUisdefinedin(47),sothatbyChenetal.(2022,Thm. 3.1)withϵ=1,wehave
(cid:104) (cid:16) (cid:17)(cid:105) (cid:16) (cid:17)
E tr K⋄ K(cid:98)⋄ 2tr K⋄ K⋄ .
c − c ≤ c −(cid:74) c (cid:75)q
Weknowonespecificrank-qapproximationofK⋄:
c
K(cid:101) c⋄ ≜ K⋄ q−1+diag(√w⋄)c1 n1⊤
n
diag(√w⋄),
(cid:74) (cid:75)
whichsatisfies
K c⋄ −K(cid:101) c⋄ =K⋄+diag(√w⋄)c1 n1⊤
n
diag(√w⋄) −K(cid:101) c⋄ =K⋄ −(cid:74)K⋄ (cid:75)q−1.
Thusbythevariationaldefinitionin(61),wehave
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
tr K c⋄ −(cid:74)K c⋄
(cid:75)q
≤tr K c⋄ −K(cid:101) c⋄ =tr K⋄ −(cid:74)K⋄
(cid:75)q−1
=(cid:80)n ℓ=qλ ℓ(K⋄).
Notethelastbounddoesnotdependonc. Thetailsumofeigenvaluesinthelastexpressionisthesame(uptoaconstant
multiplier)astheonein(48)exceptforanoff-by-1differenceinthesummationindex. Asimplecalculationshowsthat
for m′ ≜ m + log2 2√mlog2+1, we have U(m′) = U(m) 1. Another simple calculation shows that m
− − ≥
(C √dR nβ+1 +√2 )2 1 impliesthatm′satisfiesthelowerboundrequirementofrinProp.F.1. Thus,arguingasinthe
log2 log2 −log2
proofthatfollows(48),weget
(cid:104) (cid:16) (cid:17)(cid:105)
E tr K c⋄ −K(cid:98) c⋄ ≤H n,m′.
Thussofarwehaveshown
(cid:104) (cid:16) (cid:17)(cid:105)
E[MMD2 kc(Sw nc,Sw n⋄ )] ≤E tr K c⋄ −K(cid:98) c⋄ ≤H n,m′.
ByMarkov’sinequality,withprobabilityatleast1 δ,wehave
−
(cid:113)
MMD (Swc,Sw⋄ ) H n,m′.
kc n n ≤ δ
RecallthatMMD (µ,ν)= (µ ν)k forsignedmeasuresµ,ν. Bythetriangleinequality,wehave
k ∥ − ∥k
MMD
(Swc,P)
MMD
(Swc,Sw⋄
)+MMD
(Sw⋄,P)
kc n ≤ kc n n kc n
=MMD
(Swc,Sw⋄
)+MMD
(Sw⋄,P),
kc n n kP n
51DebiasedDistributionCompression
where we used that fact that (cid:80) w⋄ = 1 to get the identity MMD (Sw⋄,P) = MMD (Sw⋄,P). Hence, with
i∈[n] i kc kP
probabilityatleast1 δ,
−
(cid:113)
MMD (Swc,P) MMD (Sw⋄,P)+ H n,m′. (62)
kc n ≤ kP n δ
Step3. Incorporatingsum-to-oneconstraint
Wenowturnwcintoaconstant-preservingweightwhilenotinflatingtheMMDbymuch. Define
w1 ≜argmin w:supp(w)⊂I,(cid:80) i∈[n]wi=1MMD2 kP(Sw n,P). (63)
Notew1istheweightrightbeforeKT-Swap-LSstepinCT.LetK
I
=kP(I,I). Let1 Idenotethe I-dimensionalall-one
| |
veector. TheKarush-Kuhm-Tuckercondition(Ghojoghetal.,2021,Sec.4.7)appliedto(63)impliesthat,thesolutionw1
isastationarypointoftheLagrangianfunction
L(w ,λ)≜w⊤K w +λ(1⊤w 1).
I I I I I I −
Then ∇wIL(w I1,λ) = 0implies2K Iw I1 −λ1
I
= 0,sow I1 = λK I− 211I. TheLagrangianmultiplierλisdeterminedbythe
constraint1⊤w =1,sowefind
I I
w1 = K I−11I . (64)
I 1⊤
I
K I−11I
Define
wc,P ≜argmin MMD2 (Sw,P).
w:supp(w)⊂I kc n
Sincewc,PisoptimizedtominimizeMMD toPonthesamesupportaswc,wehave
kc
MMD (Swc,P ,P) MMD (Swc,P).
kc n ≤ kc n
Theoptimalityconditionforwc,Pis
(K +c1 1⊤)w c1 =0,
I I I − I
andhencebytheSherman–Morrisonformula,
wc,P =(K +c1 1⊤)−1c1 =(cid:16) K−1 cK I−11I1⊤ I K I−1(cid:17) c1 = K I−11I .
I I I I I I − 1+c1⊤
I
K I−11I I 1/c+1⊤
I
K I−11I
Let ρ c ≜ 1/c1 +⊤ I 1K ⊤ II− K1 I−1I 11I, so that w Ic,P = ρ cw I1. In particular, w1 and wc,P are scalar multiples of one another. To relate
MMD (Sw1,P)andMMD (Swc,P ,P),notethat
kP n kc n
MMD2 (Sw1,P)=w1⊤ K w1 = w Ic,P⊤KIw Ic,P = w Ic,P⊤(KI+c1I1⊤ I )w Ic,P−c(1⊤ I w Ic)2
kP n I I I ρ2
c
ρ2
c
=
MMD2 kc(Sw nc,P ,P)+2c1⊤
I
w Ic−c−c(1⊤
I
w Ic)2
=
MMD2 kc(Sw nc,P ,P)−c(ρc−1)2
.
ρ2 ρ2
c c
Sofartheargumentdoesnotdependonanyparticularchoiceofc>0. Letusnowdiscusshowtochoosec. Notethat
1⊤
I
K I−11
I
=m√1 mI ⊤ K I−1 √1 mI ≥mλ m(K I−1)
≥
λ1(m
KI) ≥
tr(m
KI) ≥ (cid:80)
i∈[m]m
diag(K)↓
i,
wherediag(K)↓denotethediagonalentriesofK =kP( n, n)sortedindescendingorder. Thus
S S
ρ = 1 1 .
c c1⊤
I
K1 I−11I+1 ≥ (cid:80) i∈[m] md ciag(K)↓ i+1
52DebiasedDistributionCompression
Hencewecanchoosectomakesureρ isboundedfrombelowbyapositivevalue. RecallinCT,wetake
c
(cid:80) diag(K)↓
c= i∈[m] i,
m
sothatρ 1 and
c ≥ 2
MMD2 (Sw1,P)= MMD2 kc(Sw nc,P ,P)−c(ρc−1)2 4MMD2 (Swc,P ,P).
kP n ρ2 c ≤ kc n
Henceby(62)andthefactthatKT-Swap-LSandthefinalreweightinginCTonlyimprovesMMD,wehave,withproba-
bilityatleast1 δ,
−
(cid:113)
MMD kP(Sw nCT,P) ≤MMD kP(Sw n1,P) ≤2MMD kc(Sw nc,P ,P) ≤2MMD kc(Sw nc,P) ≤2MMD kP(Sw n⋄,P)+2 H n δ,m′,
whereweuse(62)inthelastinequality.
H.2.ProofofThm.7: MMDguaranteeforSC/LSC
The claimed runtime follows from the runtime of SteinThinning (Alg. D.1) or LD (Thm. 4) plus the runtime of CT
(Prop.H.1).
NotethelowerboundforminAssum.(α,β)-paramsimpliesthelowerboundconditioninProp.H.1. ForthecaseofSC,
we proceedas inthe proof ofThm. 3and use Prop.H.1. For thecase ofLSC, we proceedas inthe proof ofThm. 5by
invokingThm.4andProp.H.1.
I.ImplementationandExperimentalDetails
I.1.O(d)-timeSteinkernelevaluation
Inthissection,weshowthatfor =(x ) ,eachSteinkernelevaluationk (x ,x )foraradiallyanalyticbasekernel
n i i∈[n] p i j
S
(Def.B.3)canbedoneinO(d)timeaftercomputingcertainsufficientstatisticsinO(nd2+d3)time. LetM Rd×d be
∈
apositivedefinitepreconditioningmatrixfork . LetLbetheCholeskydecompositionofM whichcanbedoneinO(d3)
p
timesothatM =LL⊤. Fromtheexpression(15),wecanachieveO(d)timeevaluationifwecancompute x y 2 and
∥ − ∥M
M logp(x)inO(d)time. ForM logp(x),wecansimplyprecomputeM logp(x )foralli [n]. For x y 2 ,
∇ ∇ ∇ i ∈ ∥ − ∥M
wehave
x y 2 =(x y)⊤M−1(x y)=(x y)⊤(LL⊤)−1(x y)=(cid:13) (cid:13)L−1(x y)(cid:13) (cid:13)2 .
∥ − ∥M − − − − − 2
HenceitsufficestoprecomputeL−1x foralli [n],andwecanprecomputetheinverseL−1inO(d3)time.
i
∈
I.2.Defaultparametersforalgorithms
ForLD,wealwaysuseQ=3.ToensurethattheguaranteesofLem.F.3andThm.4holdwhileachievingfastconvergence
inpractice,wetakethestepsizeofAMDtobe1/(8 ∥kP ∥n)inthefirstadaptiveroundand1/(8(cid:80) i∈[n]w i(q−1)kP(x i,x i))
insubsequentadaptiverounds. WeuseT =7√n 0forAMDinallexperiments.
We implemented our modified versions of KernelThinning and KT-Compress++ in JAX (Bradbury et al., 2018) so that
certainsubroutinescanachieveaspeedupusingjust-in-timecompilationandtheparallelcomputationpowerofGPUs. For
Compress++,weuseg = 4inallexperimentsasinShettyetal.(2022). ForbothKernelThinningandKT-Compress++,
weusechooseδ =1/2asinthegoodpointslibrary.
EachexperimentwasrunwithasingleNVIDIARTX6000GPUandanAMDEPYC751332-CoreCPU.
I.3.Correctingforburn-indetails
WeusethefourMCMCchainsprovidedbyRiabizetal.(2022)thatincludeboththesamplepointsandtheirscores. The
referencechainusedtocomputetheenergydistanceisthesameoneusedinRiabizetal.(2022)fortheenergydistance
andwaskindlyprovidedbytheauthors.
53DebiasedDistributionCompression
InTab.I.1,wecollecttheruntimefortheburn-incorrectionexperiments.
Fig.I.1,Fig.I.2,Fig.I.3,displaytheresultsoftheburn-incorrectionexperimentofSec.5repeatedwiththreeotherMCMC
algorithms: MALA without preconditioning, random walk (RW), and adaptive random walk (ADA-RW). The results of
P-MALAfromSec.5arealsoincludedforcompleteness. Forallfourchains,ourmethodsreliablyachievebetterquality
coresetswhencomparedwiththebaselinemethods.
P-MALA MALA ADA-RW RW
102
101
100
Burn-inOracle+Standard Burn-inOracle+Standard Burn-inOracle+Standard Burn-inOracle+Standard
0.1535 SteinThinning SteinThinning SteinThinning SteinThinning
SteinKernelThinning SteinKernelThinning SteinKernelThinning SteinKernelThinning
Low-rankSKT(τ=0.4) Low-rankSKT(τ=0.4) Low-rankSKT(τ=0.4) Low-rankSKT(τ=0.4)
0.1530 Low-rankSKT(τ=0.5) Low-rankSKT(τ=0.5) Low-rankSKT(τ=0.5) Low-rankSKT(τ=0.5)
Burn-inOracle+Compress++ Burn-inOracle+Compress++ Burn-inOracle+Compress++ Burn-inOracle+Compress++
0.1525
0.1520
102 103 102 103 102 103 102 103
CoresetSizem CoresetSizem CoresetSizem CoresetSizem
Figure I.1: Correcting for burn-in with equal-weighted compression. For each of four MCMC algorithms and using
only one chain, our methods consistently outperform the Stein and standard thinning baselines and match the 6-chain
oracle.
n ST LD(0.5) LD(0.4) KT KT-Compress++ RT(0.5) RT(0.4) CT(0.5) CT(0.4)
0
214 2.50 13.22 12.88 7.31 3.49 0.79 0.60 2.06 1.96
216 8.48 16.15 15.82 20.77 5.90 2.59 1.68 3.66 3.04
218 111.06 32.14 20.60 193.03 11.73 11.16 2.63 6.48 3.67
220 - 314.67 131.31 - 35.99 113.71 11.06 51.14 8.42
TableI.1: Breakdownofruntime(inseconds)fortheburn-incorrectionexperiment(d = 4)ofSec.5. n istheinput
0
sizeafterstandardthinningfromthelengthn = 2 106 chain(Rem.2). Eachruntimeisthemedianof3runs. KTand
×
KT-Compress++outputm=√n 0equal-weightedpoints. RTandCTrespectivelyoutputm=nτ
0
pointswithsimplexor
constant-preservingweightsforτ showninparentheses. Inaddition, LD,RT,andCTusetheranknτ. STandKTtook
0
longerthan30minutesforn =220andhencetheirnumbersarenotreported.
0
I.4.CorrectingforapproximateMCMCdetails
Surrogategroundtruth FollowingLiuandLee(2017),wetookthefirst10,000datapointsandgenerated220surrogate
groundtruthsamplepointsusingNUTS(Hoffmanetal.,2014)fortheevaluation. Togeneratethesurrogategroundtruth
usingNUTS,weusednumpyro(Phanetal.,2019). Ittook12hourstogeneratethesurrogategroundtruthpointsusing
theGPUimplementation,andweestimateitwouldhavetaken200hoursusingtheCPUimplementation.
SGFS For SGFS, we used batch size 32 and the step size schedule η/(1+t)0.55 where t is the step count and η is
theinitialstepsize. Wechoseη from 10.0,5.0,1.0,0.5,0.1,0.05,0.01 ,foundη = 1.0gavethebeststandardthinning
{ }
54
ecnatsiDygrenE
DMMDebiasedDistributionCompression
P-MALA MALA ADA-RW RW
101
100
10−1
0.1535 Burn-inOracle+Standard Burn-inOracle+Standard Burn-inOracle+Standard Burn-inOracle+Standard
SteinThinning SteinThinning SteinThinning SteinThinning
SteinRecombination(τ=0.4) SteinRecombination(τ=0.4) SteinRecombination(τ=0.4) SteinRecombination(τ=0.4)
0.1530 SteinRecombination(τ=0.5) SteinRecombination(τ=0.5) SteinRecombination(τ=0.5) SteinRecombination(τ=0.5)
Low-rankSR(τ=0.4) Low-rankSR(τ=0.4) Low-rankSR(τ=0.4) Low-rankSR(τ=0.4)
Low-rankSR(τ=0.5) Low-rankSR(τ=0.5) Low-rankSR(τ=0.5) Low-rankSR(τ=0.5)
0.1525 Burn-inOracle+RT Burn-inOracle+RT Burn-inOracle+RT Burn-inOracle+RT
0.1520
102 103 102 103 102 103 102 103
CoresetSizem CoresetSizem CoresetSizem CoresetSizem
FigureI.2: Correctingforburn-inwithsimplex-weightedcompression. ForeachoffourMCMCalgorithmsandusing
only one chain, our methods consistently outperform the Stein and standard thinning baselines and match the 6-chain
oracle.
MMD to get a coreset size of m = 210 , and hence we fixed η = 1.0 in all experiments. We used the version of SGFS
(Ahnetal.,2012,SGFS-f)thatinvolvesinversionofd dmatrices—wefoundthefasterversion(SGFS-d)thatinverts
×
onlythediagonalresultedinsignificantlyworsemixing. WeimplementedSGFSinnumpyandranitontheCPU.
Runtime The SGFS chain of length 224 took approximately 2 hours to generate using the CPU. Remarkably, all of
ourlow-rankmethodsfinishwithin10minutesforn = 220, whichisordersofmagnitudefasterthanthetimetakento
0
generatetheNUTSsurrogategroundtruth.
Additionalresults InFig.I.4,weplottheposteriormeanmean-squarederror(MSE)foreachcompressionmethodinthe
approximateMCMCexperimentofSec.5.
I.5.Correctingfortemperingdetails
In the data release of Riabiz et al. (2020), we noticed there were 349 sample points for which the provided scores were
NaNs,soweremovedthosepointsattherecommendationoftheauthors.
55
ecnatsiDygrenE
DMMDebiasedDistributionCompression
P-MALA MALA ADA-RW RW
101
100
0.1550
Burn-inOracle+Standard Burn-inOracle+Standard Burn-inOracle+Standard Burn-inOracle+Standard
0.1545 SteinThinning SteinThinning SteinThinning SteinThinning
SteinCholesky(τ=0.4) SteinCholesky(τ=0.4) SteinCholesky(τ=0.4) SteinCholesky(τ=0.4)
0.1540
SteinCholesky(τ=0.5) SteinCholesky(τ=0.5) SteinCholesky(τ=0.5) SteinCholesky(τ=0.5)
0.1535 Low-rankSC(τ=0.4) Low-rankSC(τ=0.4) Low-rankSC(τ=0.4) Low-rankSC(τ=0.4)
Low-rankSC(τ=0.5) Low-rankSC(τ=0.5) Low-rankSC(τ=0.5) Low-rankSC(τ=0.5)
0.1530 Burn-inOracle+CT Burn-inOracle+CT Burn-inOracle+CT Burn-inOracle+CT
0.1525
0.1520
0.1515
102 103 102 103 102 103 102 103
CoresetSizem CoresetSizem CoresetSizem CoresetSizem
Figure I.3: Correcting for burn-in with constant-preserving compression. For each of four MCMC algorithms and
usingonlyonechain,ourmethodsconsistentlyoutperformtheSteinandstandardthinningbaselinesandmatchthe6-chain
oracle.
Equal-Weighted(Approx. MCMC) Simplex-Weighted(Approx. MCMC) Constant-Preserving(Approx. MCMC)
StandardThinning StandardThinning StandardThinning
10−2 S St te ei in nT Kh erin nn elin Tg
hinning
10−2 S St te ei in nT Reh cin on min bg
ination(τ=0.4)
10−2 S St te ei in nT Chh oin len sin kyg
(τ=0.4)
Low-rankSKT(τ=0.4) SteinRecombination(τ=0.5) SteinCholesky(τ=0.5)
Low-rankSKT(τ=0.5) Low-rankSR(τ=0.4) Low-rankSC(τ=0.4)
Low-rankSR(τ=0.5) Low-rankSC(τ=0.5)
10−3 10−3 10−3
CoresetSizem CoresetSizem CoresetSizem
FigureI.4: Posteriormeanmean-squarederror(MSE)fortheapproximateMCMCcompressionexperimentofSec.5.
MSEiscomputedas ∥Eˆ PZ −(cid:80) i∈[n0]w ix
i
∥2 M/dwhereEˆ PZ isthemeanofthesurrogategroundtruthNUTSsample.
56
ecnatsiDygrenE
DMM
ESM ESM ESM