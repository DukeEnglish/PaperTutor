E-Vote Your Conscience:
Perceptions of Coercion and Vote Buying,
and the Usability of Fake Credentials in Online Voting
Louis-Henri Merino∗, Alaleh Azhir†, Haoqian Zhang∗, Simone Colombo∗,
Bernhard Tellenbach‡, Vero Estrada-Galin˜anes∗, Bryan Ford∗
∗EPFL †MIT ‡Armasuisse
This paper is the extended version of a work published in
the proceedings of the 45th IEEE Symposium on Security and Privacy, May 2024.
Abstract—Online voting is attractive for convenience and ac- Individual verifiability measures [10], [11] attractively
cessibility, but is more susceptible to voter coercion and vote enable voters to verify that their votes are cast as intended.
buying than in-person voting. One mitigation is to give voters Individually-verifiable receipts unfortunately make voting
fake voting credentials that they can yield to a coercer. Fake more susceptible to voter coercion [12], [13].1 An abusive
credentialsappearidenticaltorealones,butcastvotesthatare partner or other coercer might demand the voter’s receipts,
silentlyomittedfromthefinaltally.Animportantunanswered for example [14]. These receipts could also enable un-
question is how ordinary voters perceive such a mitigation: scrupulous well-funded actors to buy votes at scale through
whether they could understand and use fake credentials, and anonymously-funded smart contracts [15]. To resist such
attacks, a secure election system must prevent coercive
whether the coercion risks justify the costs of mitigation.
adversaries from knowing whether a voter complied with
We present the first systematic study of these questions,
their demands, even if the voter is willing to comply, e.g.,
involving 150 diverse individuals in Boston, Massachusetts.
in return for financial compensation [12], [16].
All participants “registered” and “voted” in a mock election:
Mostonlinevotingsystemslackcoercionresistance[6],
120 were exposed to coercion resistance via fake credentials,
[17], [18]. Deniable re-voting permits a voter to override
the rest forming a control group. Of the 120 participants
a coerced vote with a new vote cast later [19]–[21], but
exposed to fake credentials, 96% understood their use. 53%
is vulnerable to coercers who can supervise voters or hold
reportedthattheywouldcreatefakecredentialsinareal-world
onto their credentials or voting devices until the election
voting scenario, given the opportunity. 10% mistakenly voted
closes[14].Estonia’sonlinevotingsystememploysdeniable
with a fake credential, however. 22% reported either personal
re-voting [19], [22], [23], but lacks universal verifiability.
experiencewithordirectknowledgeofcoercionorvote-buying
A strategy proposed by Juels, Catalano and Jakobsson
incidents.Theselatterparticipantsratedthecoercion-resistant
(JCJ) [12] enables voters to create, alongside their real
systemessentiallyastrustworthyasin-personvotingviahand-
voting credential, fake credentials which cast votes that
marked paper ballots. Of the 150 total participants to use
do not count. Fake credentials present usability concerns,
the system, 87% successfully created their credentials without
however, such as whether voters can distinguish their real
assistance; 83% both successfully created and properly used
credential from fake ones, or can create a fake credential
their credentials. Participants give a System Usability Scale
while under coercion [22], [24]. While prior work has
score of 70.4, which is slightly above the industry’s average discussed the usability of fake credentials [22], [24], [25],
score of 68. Our findings appear to support the importance only Neto et al. [26] performed a user study on this topic.
of the coercion problem in general, and the promise of fake Their study involved only university-affiliated participants,
credentialsasapossiblemitigation,butusererrorratesremain however. Further, the voting process they studied lacked
an important usability challenge for future work. individual verifiability: voters could not check whether the
purportedly “real” credential they were issued was in fact
1. Introduction real (as opposed to fake). Prior work also leaves other
unansweredquestions,suchaswhetherordinaryvoterseven
comprehend the coercion threat or believe it is important.
Remote electronic (online) voting systems promise con-
To fill this gap, we conducted a study with 150 in-
venience and increased voter turnout [1], [2]. Online voting
dividuals, recruited at a suburban park in Boston, Mas-
is particularly useful to overseas voters [3] or in crises such
sachusetts, to examine whether voters might plausibly find
asapandemic[4].Oneimportantdevelopmentinelectronic
coercion-resistant online voting with fake credentials to be
voting is universal verifiability, which allows anyone (not
justelectionofficialsandobservers)toverifythatvoteshave
1.We use the term “coercion” broadly to indicate any form of undue
beentalliedcorrectly,whileprotectingvoterprivacy[5]–[9]. influence,includingvotebuyingandvoterintimidation.
1
4202
rpA
81
]CH.sc[
1v57021.4042:viXrausable and trustworthy. 120 of these participants underwent • The first study that assesses whether voters can use a
a credentialing process to obtain real and fake credentials.2 credentialing process that requires voters to identify and
Participants then cast a mock vote using, at least, their real report a misbehaving kiosk to ensure voter verifiability.
credential. The remaining 30 participants engage with the Our prototype, which includes a user interface mockup of
same system, but without any exposure to fake credentials. TRIP and an Android application that simulates activation
Participantsconcludethestudybycompletingasurveyask- and voting, is available at github.com/dedis/trip-usability.
ingthemtosharetheirexperiencesandviewsofthesystem,
as well as their perspectives on and any experiences with 2. Background
coercioningeneral.Ourinstitutionalreviewboardapproved
the study; Section 3 discusses ethics considerations.
This section introduces coercion-resistant voting sys-
The credentialing process is an interactive user- tems, TRIP, and the metrics we used in our study.
interface prototype of Trust-limiting In-Person Registra-
tion (TRIP) [27], a voter-verifiable registration system for
2.1. Online Voting Systems
coercion-resistant online voting via fake credentials. Unlike
prior coercion-resistant systems either deployed [1], [2] or
Nearlyallonlinevotingsystems[5],[6],[17]strivefora
subject to user studies [26], TRIP ensures that a compro-
minimumofverifiabilityandvoterprivacy:establishingvote
mised registrar cannot undetectably manipulate elections
confidentiality, while offering (publicly) verifiable election
by secretly keeping real credentials for themselves, leaving
results.Substantialresearchseekstoachieveotherdesirable
voters with only fake credentials. To achieve verifiability
propertiesaswell,suchascast-as-intended[10],[11],where
without producing receipts usable for coercion, TRIP’s reg-
voters are convinced that their encrypted ballot contains
istration kiosk produces interactive zero-knowledge proof
their intended vote. JCJ [12] proposed and formally defined
transcripts,allofwhicharevalidandcheckable,butaretrue
coercion resistance as another desirable system property: in
proofs only in real credentials, and are false proofs in fake
brief, the inability of an adversary to confirm whether a
credentials. Study participants used TRIP to create real and
coerced voter has complied with their demands, even if the
fakecredentials,thenusedanAndroiddevicewesuppliedto
voter wishes to do so. While JCJ suggested the use of fake
cast a mock non-political vote. A random subset of partici-
voting credentials, other works [19], [28], [29] have since
pants were silently exposed to a “compromised” kiosk that
proposedotherstrategiesduetousabilityconcernswithfake
issuedonlyfakecredentials.Becauseindividualverifiability
credentials [25]. This paper focuses on these concerns.
dependsonvotersbeingabletodetectacompromisedkiosk
A typical online voting system interacts with voters, an
– not just in theory but in practice – our study sheds light
election authority, and observers, and involves setup, reg-
for the first time on whether ordinary voters can effectively
istration, voting, and tallying phases. Performance-oriented
obtain verifiability and coercion resistance at once.
systems research tends to focus on tallying: reducing the
Using the data we collected from the study, we address
costs of shuffling and counting ballots, including the re-
the following four central questions:
moval of fake ballots in coercion-resistant voting systems.
1) Whatarevoters’perceptionsofandexperienceswiththe For usability, however, registration and voting are the more
coercion threat in general? crucial stages because they directly involve voters. This
2) How likely will voters trust a coercion-resistant online studyfocusesprimarilyonregistration,wherevotersengage
voting system, versus other voting methods? withtheelectionauthoritytogeneratetheirvotingmaterials.
3) Can voters use a voter-verifiable credentialing process
to create their real and fake credentials, and identify
2.2. Trust-limiting In-Person Registration
deviations to ensure voter verifiability?
4) Can voters understand and use fake credentials, casting
Sincetheintroductionofcoercionresistance,mostworks
their intended votes with their real one?
have focused on the tallying process [30]–[34], leaving a
Thisisnotalongitudinalstudy,soitcannotassessissues keychallengeunresolved:thedevelopmentofausablevoter
such as whether voters can recall the distinction between registration system that issues voter-verifiable real and fake
theirrealandfakecredentialsoveranextendedtimeperiod. credentials.Voterverifiabilityplaysakeyroleinpreventing
This is one of several limitations we detail in §7. a compromised election authority from monopolizing real
This paper makes the following key contributions: credentials and issuing only fake ones to voters.
• Thefirststudytosystematicallyinvestigatewhethervoters The Trust-limiting In-Person Voter Registration (TRIP)
find coercion-resistant online voting with fake credentials system [27] addresses this problem by leveraging physical
to be understandable, usable and trustworthy. presence of voters across four registration phases (Fig. 1a):
check-in,credentialing,check-out,andactivation.Atcheck-
in, voters identify themselves to a registration official, ob-
2.Thestudyusedtheterm“testcredentials”insteadof“fakecredentials” taining a check-in ticket that gives them access to a privacy
toavoidnegativeconnotationsthatfakecredentialsareinvalidorinherently
booth for credentialing. Comparable to the “ballot selfie”
bad.Theregistrationprocesssuggeststousersthat“testcredentials”may
problem with in-person voting, recording devices could
alsobeusedforpurposesotherthancoercionresistance,suchastotestthe
votingsystem,ortosharewithfriendsorfamilyforeducationalpurposes. compromise coercion resistance in this critical stage, so
2Check-In Credentialing Check-Out Credential Activation
Voter Official Official Voter VSD
Kiosk
Time
…
Authenticate Check-In Create Credentials Credentials Display A Credential Activate Real Credential
Ticket
Supervised Public Environment Supervised Private Environment Unsupervised Private Environment
(a) TRIP Voter Workflow. The voter (1) checks in with an official by authenticating themselves to receive a check-in ticket, (2) enters a supervised
private environment to create their credentials using a kiosk that is unlocked by their check-in ticket, (3) checks out with an official by displaying the
publicpartofanyoneoftheirpapercredentials(Check-OutTicket–Fig.2c),and(4)activatestheircredentialsontheirvotersupportingdevice(VSD).
Symbol Voters may then create fake credentials in two steps:
Inserted Receipt Commit 1) The voter chooses and scans any envelope.
( )
2) The kiosk then prints the receipt all at once.
Symbol Insert Receipt Here Symbol Insert Receipt Here Symbol Insert Receipt Here Symbol
Co (m )mit Cha (l l e )nge Cha (l l e )nge Cha (l l e )nge inseT rto thfi ena reli cz ee ipth te inf sa idk ee tc hr eed ee nn vt eia lol, pev ,o mter as rka ig na gin tht eea cr reo df ef na tn iad
l
Ch Te ic (c k k )- eO tut Tr Wan insp da or went Ch Te ic (c k k )- eO tut Res (p o )nse d tii as lt si ,nc lit miv ie tl ey d. V ono lt yers bym ta hy eic rre tia mte ea in ny thn eum prb iver aco yf bfa ok oe thc .r 4eden-
Res (p o )nse Mark Here MV Aao r rk ete i anr g Mark Here MV Aao r rk ete i anr g Mark Here MV Aao r rk ete i anr g zeroT -kh ne ose wlt ew do gest pe rp os o, fw
,
ch oil me pa rl oso mie ss et sab thli esh pin rog ofa ’n
s
i sn ot ue nra dc nt eiv se
s
Envelope compared to the four-step process. Despite the proof’s
(a)Receipt (b)Envelope (c)Transport (d)Activate soundnessbeingviolated,theproof’scryptographicvalidity
remains unaffected. Therefore, the voter’s real credential is
Figure 2: TRIP Paper Credential. Figures (a) and (b) present
cryptographically concealed among their fake credentials,
thepapercredential’selementsandFigures(c)and(d)presentthe
paper credential’s transport and activate states. differentiated only by the voter’s own distinct markings.
Upon exiting the booth, voters proceed to check-out
wheretheypresentanyoneoftheircredentials,realorfake,
TRIP assumes that voters cannot use electronic devices in to the registration official. The official scans the receipt’s
the booth.3In the booth voters find a kiosk, envelopes, and middle QR code, visible through the envelope’s transparent
a pen. Voters create their real credential in four steps: window, to complete the in-person portion of the process.
1) The voter scans their check-in ticket. Sometime later, voters activate their real credential on
2) ThekioskprintsaQRcodeandsymbolonreceiptpaper. anydevicetheytrustbyplacingthecredentalintheActivate
3) The voter picks any envelope matching the printed sym- state (Fig. 2d) and scanning it. After activation, the voter
bol,andpresentstheQRcodeonittothekiosk’sscanner. discardsthenow-unusablepapercredential.Voterswhohave
4) The kiosk prints two more QR codes on the receipt. no device they trust may activate their real credential on a
These steps establish an interactive zero-knowledge device of a trusted friend or family member.5 Voters may
proof between the kiosk and the voter, ensuring the correct- give away or sell their fake paper credentials, or activate
ness of the real credential. The voter verifies that the kiosk them on a device that is under a coercer’s control.
adherestothesestepswhilethevoter’sdevice—later,during
TRIP’s design does not limit the lifetime of voting
activation—verifies the proof’s cryptographic validity.
credentials. Voters may therefore reuse their activated cre-
To finalize their credential, the kiosk instructs voters
dentials to cast (both real and fake) votes in multiple suc-
to tear off the receipt (Fig. 2a), insert it into the enve-
cessive elections, thereby amortizing the convenience cost
lope (Fig. 2b), and memorably mark the resulting com-
of in-person registration. Election authorities may impose
bination – which we term the real paper credential – to
distinguish it from future fake credentials. When the receipt
4.Whethertoimposeanyparticulartimelimitisapolicydecision.We
is fully inserted and only its middle QR code is visible, the
anticipate that voters spending an inordinate time registering should be
credential is in its transport state (Fig. 2c).
a rare situation, manageable informally by an election official asking “is
anythingwrong?”atsomepointandgentlyescalatingonlyasneeded.
3.The degree to which this rule is enforced is a policy decision, 5.Inhopefully-rarecaseswhereavotercannothideapapercredential
importantbutorthogonaltoTRIP’sdesign.Standardpracticecomparableto fromthecoercer,orhavenoaccesstoanydevicetheytrust,TRIPoffers
in-personvotingwouldbemerelytoforbidtheuseofdevicesinthebooth. anextensionwherevoterscanleavetheboothwithonlyfakecredentials,
Strongerenforcementmightrequirevoterstodepositelectronicdevicesin while delegating their real vote to a designated proxy such as a political
alockerbeforeenteringthebooth,atanobviouscostinconvenience. party.Wedonotexplorethisextensioninthepresentstudy,however.
3an expiration date on voting credentials by policy, perhaps 3. Methodology
aligningwith therenewal cycleof identificationdocuments.
TRIP is designed to replicate the trust assumptions This section presents our study’s design, detailing its objec-
inherent in in-person voting to achieve verifiability and tives, workflow, materials and evaluation metrics.
coercionresistance.Intraditionalin-personvoting,theelec- Ethical Considerations. Before participating, each indi-
tion authority mitigates the risk of coercion by providing vidual received an information sheet outlining the study’s
voters with a privacy booth free from any coercer’s influ- scope, their rights, the expected tasks, and the data to be
ence. This supervised environment creates an untappable collected. We collected data from participants’ interaction
communication channel between voters and the election with our devices and credentials, supplemented with infor-
authority, enabling voters to vote their conscience. Voters, mationfromasurvey.Werespectedparticipants’privacyby
inreturn,collectivelyprotecttheintegrityoftheelectionby supplying all required materials, and by not asking for any
reportinginconsistencieswiththeirballotpriortosubmitting form of ID, although showing ID would be standard in real
it [35]. TRIP, being designed for online voting, leverages a voter registration. Furthermore, we informed participants
similar untappable channel but at registration time rather that this system is not linked to any real voting system.
than voting time. While a privacy booth traditionally hides
a voter’s choices, in TRIP it hides knowledge of how 3.1. Objectives
many credentials the voter created and which one is real.6
Voters can therefore convey their intentions to the election We start by detailing our study’s central questions, along
authority later while casting a vote online—by choosing to with our strategies for addressing them.
cast their vote using a real or fake credential. TRIP relies
What are voters’ perceptions and experiences with co-
on interactive zero-knowledge proofs to ensure the integrity
ercion and how do they rate their trust in this coercion-
of the election process. Voters need not understand zero-
resistant voting system versus other voting methods?
knowledge proofs, but to verify the process’s integrity, they
To answer the former, we ask participants whether they
do need to distinguish the four-step process of creating a
haveexperiencedorknowofsomeonewhohasexperienced
real credential from the two-step process of creating a fake
coercion. The wording “or someone you know” is designed
credential, and to report any inconsistencies they encounter.
to encourage disclosure from those who might otherwise be
hesitant to share their experience. We also ask to rate the
2.3. Standardized Usability Metrics
likelihood of various coercion and vote-buying scenarios,
along with potential perpetrators. For the latter, we ask par-
In this study, we use the System Usability Scale (SUS) and
ticipantstoratethetrustworthinessofthiscoercion-resistant
theUserExperienceQuestionnaire(UEQ)tomeasurepartic-
voting system in comparison to other voting methods.
ipants’ perceptions of system usability and user experience.
SystemUsabilityScale(SUS).Thisscale[36]ismostoften Can voters use a credentialing process to create their
used to measure usability, requiring only ten prompts. We real and fake credentials, and identify deviations during
alteredthefirstpromptfrom“Iwouldliketousethissystem credentialing to ensure voter verifiability? To measure
frequently” to “I would like to use this voter registration success rate, we observe the number of participants who
systemwheneverIrenewmyidentificationsdocuments(i.e., complete the process successfully, without requiring a fa-
every 5-10 years)” to align with TRIP’s intended usage. cilitator. While during the survey, we measure participants’
User Experience Questionnaire (UEQ). This question- experience and perceptions of usability using both the SUS
naire[37],unlikeSUS,assessesnotonlytraditionalusability and UEQ questionnaires. To measure the malicious kiosk
factorsbutalsoaspectsofuserexperience,therebyensuring detection rate, we intentionally make the kiosk misbehave
the system meets user needs. User experience covers a and observe if participants report the anomaly.
participant’s emotional, cognitive, and physical responses Can voters understand and use fake credentials, while
before, during, and after system usage [38]. The UEQ con- still casting their intended votes using their real creden-
sistsof26contrastingattributes,withparticipantsexpressing tial? We gather data about participants’ understanding of
their level of agreement by selecting a value between 1 fake credentials in two phases: first via an interactive quiz
and 7, then re-calibrated to a score between -3 to 3. The onthekiosk,thenlaterviasurveyquestions.Wealsopresent
responsesallowforthemeasurementofsixscales[37],such an instructional video at the beginning of the process to
as Perspicuity, Efficiency, Dependability and Novelty, in prepareandeducatethevoter.Sincerealandfakecredentials
addition to the individual scores obtained for each attribute, function identically, we direct participants to vote using
such as security and practicality. Furthermore, Hinderks their real credential, allowing us to assess whether they can
et al. [38] introduce a Key Performance Indicator (KPI) distinguish their real credential from their fake credentials.
extension to the UEQ, termed UEQ PKI. This extension
allowsparticipantstoexpresstheirviewsonthesignificance 3.2. Study Setup & Workflow
of each of the six UEQ scales via six additional questions.
We now describe the study’s setup and workflow, detailing
6.The design TRIP paper [27] includes a formal proof that coercers
its location, the setup of the workstation and the participant
cannotdistinguishbetweenrealandfakecredentials,oridentifythenumber
offakecredentialscreatedbyavoterwhileinsidethebooth. workflow. The facilitator’s scripts are available in §C.1.
4Real Credential Steps Quiz
Video 1
Honest Kiosk Real Credential Usage Quiz
Video 2
Malicious Kiosk Test Credential Usage Quiz
Video 3
Distinguish Credentials Quiz
Time
Control Group (C) Fake Credential (F) Malicious Kiosk (M)
Security Priming + F (SF) Security Priming + M (SM)
Figure4:StudyFlow.Participantsfirstwatchoneofthreeinstruc-
tionalvideos,theninteractwitheitheranhonestormaliciouskiosk
thatdisplaysthecorrespondingquizzes.Thedistinguishcredentials
quiz is present for those who created at least one fake credential.
Upon completing the credentialing process, the participant
returns to the left side of the table and hands one credential
(real or fake) to the facilitator. The facilitator scans the QR
code through the visible window and returns the credential.
Figure 3: Study Setup: Starting on the left chair, participants (1) Activation and Voting. For activation, the facilitator asks
review the study’s information sheet and sign the consent form,
the participant to envision that they are now at home, and
(2) watch an instructional video on the laptop, (3) are handed a
to use the study’s supplied mobile device to activate their
check-inticketfromthefacilitator,(4)movetotherightchairand
real credential and cast a mock vote. The participant may
interact with the kiosk to create their credential(s), (5) return to
theleftchairforcheck-out,(6)activatetheircredential(s)andcast optionally activate fake credential(s) as well and cast fake
mock votes on the supplied mobile device, and, (7) complete the votes by clicking on ‘cast vote with another credential’.
study’s exit survey on either our laptop or touchscreen tablet. Since the app was designed solely for the purpose of this
study, it is stateless, storing only one credential at a time. If
asked, we explain that in a real setting, one could unlock a
Location and Recruitment. To enhance demographic di- specific credential using its associated user-generated PIN.
versity while ensuring a neutral environment, we conduct Survey and Compensation. Once finished, the participant
our study in a suburban park. We verbally invited passerbys completestheexitsurveyandisthencompensatedwith$20.
toparticipateinthestudyoverthreemonths,andshowcased The entire session typically takes around 30 minutes.
flyers/posters at our study location to encourage engage-
ment. We limited participants to only those who said they
3.3. Study Groups
had prior experience with voter registration.
Study Setup. Our setup (Fig. 3) consists of a table with
To answer our key questions (§3.1), we conduct a between-
two chairs on one side for the participant and a park bench
subjectsstudy,randomlyassigningstudyparticipantstofive
on the other side for the facilitator. On the right side of
equal-sizegroups.Eachgroupexperiencesadistinctvariant
the table, we have the credentialing process with the kiosk,
of the process. To assess the usability of fake credentials in
envelopesandreceiptprinter,whileontheleftside,wehave
particular, our study exposes one control group only to real
the check-in, check-out, activation and voting processes.
credentials,whileexposingtheotherfourgroupstorealand
Participant Enrollment. Whenever an eligible participant fake credentials. To assess the system’s effective voter veri-
agrees to participate, we guide them to the left side of the fiability,weexposetwoofthelattergroupstoa“malicious”
table to review the information sheet and sign the consent kiosk that attempts to “steal” the voter’s real credential by
form. The facilitator addresses any logistical questions, and silently guiding the user through the creation of a fake
refrains from discussing the objectives of the study beyond credential instead of a real one.7 To familiarize participants
“assessing the usability of an online voting system.” The with the correct voting process we rely on instructional
participant is then randomly assigned to one of five groups videos, as detailed in §3.4. To study the tradeoffs between
that we detail in §3.3. To enhance the ecological validity of being more or less explicit about security threats and risks
ourstudy[39],thefacilitatoraskstheparticipanttoenvision in educational materials, we use two contrasting types of
themselves at a government office, as scripted in §C.1. instructional videos. We therefore obtain the following five
InstructionalVideo&Check-In.Thefacilitatorthenstarts groups, each consisting of 30 participants:
playing a video (§3.4) for the participant to watch, the • Control Group (C): Exposure only to the real credential
content of which depends on the participant’s assigned known as “voting credential” and instructional video 1.
group. After the video concludes, the facilitator hands the
participant a check-in ticket and directs them to the booth. 7.Thedeviationinvolvesreorderingsteps2and3,withstep3nowbeing
Credentialing and Check-Out. To simulate an authentic before2.Otherdeviationsarepossibleandperhapsworthstudyinginthe
future,butmostvariationsareeithermoreovertandobvious(e.g.,skipping
booth experience, the facilitator does not interact with the
thecreationofarealcredential)ordetectablebythevoter’spersonaldevice
participant in this stage, intervening only upon request. onactivation(e.g.,thecryptographicvalidityofthezero-knowledgeproof).
5their real credential on a trusted device. For the treatment
groups (F, M, SF, SM), we examine participants’ recall of
thestatedpurposesoffakecredentials.ForgroupsC,F,and
SF,wealsoassesscomprehensionofwhentoselectandscan
an envelope prior to creating their real credential. Finally,
for those electing to create fake credential(s), we evaluate
theirrecallonhowtodifferentiatetheirrealcredentialfrom
fake credentials.
Exit Survey. In the exit survey (§D), we first ask partici-
pants their demographic information. Participants then rate
their experience, including what they liked most and liked
least, along with completing the SUS, UEQ, and UEQ PKI
questionnaires. Participants continue by answering whether
Figure 5: Steps Overview. Various screenshots from the instruc- they noticed anything odd with the credentialing process.
tional video, which demonstrate each phase. Participants exposed to fake credentials answer questions
related to those credentials, such as whether they can recall
their usage. Participants then rate their trust on various
• Fake Credential Group (F): Exposure to real and fake votingmethods:threevariantsofin-personvoting,andthree
voting credentials, and instructional video 2. variants of remote voting, including one for this voting sys-
• Malicious Kiosk Group (M): Exposure to real and fake tem. The survey finishes by asking participants to describe
credentials, instructional video 2, and a malicious kiosk. their perceptions and experiences with coercion and vote-
• Security Priming + F (SF): Exposure to real and fake buying in their own lives. The average time spent on the
voting credentials, and instructional video 3. survey is 17 minutes.
• Security Priming + M (SM): Exposure to real and fake
credentials, instructional video 3, and a malicious kiosk.
3.5. Statistical Methods
3.4. Materials
Throughout this paper, we use an alpha level of 0.05 to
establish statistical significance. For our between-subjects
Thisstudyusesinstructionalvideostoeducatevoters,while
comparison on non-parametric data (e.g., ordinals), we use
quizzesandasurveyassistusinevaluatingtheirunderstand-
the Kruskal-Wallis one-way ANOVA, while for pairwise
ing according to the study flow presented in Figure 4.
comparisons, we use the Dunn test. For our within-subjects
Instructional Videos. We introduce three videos, avail-
comparisons on non-parametric data, we use the Friedman
ableatgithub.com/dedis/trip-usability,eachillustratingtheir
rank sum test, while for pairwise comparisons, we use
assigned conditions: control group (video 1), no security
the Durbin-Conover test. To control for the family-wise
priming (video 2) and security priming (video 3). Video
errorratewhenconductingpairwisestatisticalcomparisons,
1, lasting 3 minutes and 6 seconds, demonstrates only the
we use the Holm-Bonferroni method. We apply Shapiro-
creationofa“voting”credential.Incontrast,videos2and3,
Wilk to assess whether the current data follows a normal
withdurationsof5m50sand6m25srespectively,illustrates
distribution. We employ Cohen’s d to measure effect sizes,
the creation of real and fake credentials (Fig. 5). Videos 2
whichestimatethedegreeofdifferencebetweentwogroups.
(Fig. 10a) and 3 (Fig. 10b) both highlight the differences in
Typically, an effect size of about 0.2 is considered small,
creating real and fake credentials. However, video 3 shows,
while an effect size near 0.8 is considered large. The term
under a conspicuous “BEWARE” sign, how to detect a
“participants” represent the averageviews or experiences of
“hacked” kiosk. Video 3 thus makes a key threat explicit,
those involved in the study.
at the risk of a more unsettling or “scary” presentation.
We use videos as our primary source of instructional
4. Participants’ Perceptions and Experiences
material, as individuals learn better from dynamic visuals
compared to static images [40]. We select whiteboard an-
This section details the demographic profile of our par-
imated videos due to their positive impacts on retention,
ticipants and discusses their experiences with and percep-
engagement and enjoyment, even when conveying complex
tions of coercion. Moreover, we explore their views on this
material [41]. To enhance the effectiveness of our videos,
coercion-resistant system versus other voting methods.
weincorporatethedynamicdrawingprinciple,adoptafirst-
personperspective,andincludenarrationwithsubtitles[42].
Quizzes. To evaluate participants’ understanding of con- 4.1. Demographics
cepts and foster active learning, the kiosk exposes partic-
ipants to up to four unannounced, multiple-choice quizzes Werecruited150participants,aged19to83,withanaverage
(§C.3. The selection of quizzes depends on the participant’s andmedianageof44and36.5,respectively.Figure6depicts
group assignment and actions. For all groups, we evaluate thisagedistribution.Figure11aprovidesabreakdownofthe
participants’understandingthatthey mustkeepandactivate participants’ age distribution across the five study groups,
6contrast, participants perceived C-Forceful as the least
likely scenario with a mean score of -0.34 and a median of
25
-1 (somewhat unlikely). We also see that the distribution
20 of responses for C-Forceful almost inversely mirrors
that of P-Selfie. The calculated Cohen’s d effect sizes
15
further quantify these differences: 0.41 when comparing
10
P-SelfiewithC-Forceful,0.39withP-Ballotand
5 0.26 with P-App. The remaining two coercion scenarios,
P-BallotandP-Appalsobearavisualsimilarityintheir
0
20 25 30 35 40 45 50 55 60 65 70 75 80 85 distributions, demonstrating two distinct peaks at somewhat
Age
likely and extremely unlikely.
Figure 6: Distribution of Participant Ages. The minimum, me- Perceived Sources (Fig. 7b). Participants perceive
dian, mean and maximum ages among all groups were 19, 36.5, S-Family to be the most likely source of coercion
44and83,respectively.Sixparticipantsdidnotdisclosetheirage. (mean score of 0.53 and a median rating of somewhat
likely), with 21% of participants rating it as extremely
likely. Moreover, S-Family is statistically significantly
while Figure 11b provides a breakdown of their gender,
higher than the three other coercion sources. In contrast,
ethnicity and education. We observed distinct participation S-Authorityistheleastlikelysourcewithameanscore
patterns:seniorsprimarilyduringtheday,middle-agedindi- of -0.25. The calculated Cohen’s d effect sizes are: 0.38
vidualsafterwork,andyoungerindividuals(18-35)through- when comparing S-Family with S-Authority, 0.34
out the day. On initial encounter, many individuals had the with S-Employer, and 0.25 with S-Party. For both
first impression that they could register with us for real S-EmployerandS-Partysources,participantshavetwo
online voting; we clarified in our recruitment script (§C.1)
dominant contrasting opinions, with one cohort seeing the
thatthisisonlyausabilitystudyononlinevotingwithmock,
source as somewhat likely and the other as very unlikely.
non-political, elections. Participants’ time availability was Coercion Instances (Fig. 7c). A quarter of participants
the primary recruitment challenge; we estimated the study (26%) report experiencing or knowing of someone who
to last around 30 minutes, and found, after the fact, that has experienced at least one form of voter coercion, while
a typical participant took 35 minutes. A few individuals two-thirds (67%) report no such experiences; the remaining
declined to participate due to their opposition to online 7% preferred not to say. In line with participants’ views,
voting, reconfirming well-known difficulties in overcoming the most commonly reported source of coercion (15%) is
selection bias in user studies. S-Family. Such instances include multiple accounts of
spousal oversight during voting, leading to dictated voting
4.2. Coercion and Vote Buying choices. However, despite being viewed as the least likely
coercionscenario,S-Forcefulisthemostreportedcoer-
We now discuss participants’ perspectives and experiences cion scenario (12%). Around 10% identify S-Employer
with coercion and vote buying based on their survey re- and S-Party as sources of coercion. Reported incidents
sponses. Participants rated the following four coercion sce- include unions dictating votes, co-workers pressuring at-
narios on a 7-point Likert scale [43] (summarized from tendance at undesired political rallies, and prominent party
§D.8): (C)oercing by threatening harm (C-Forceful), members pressuring members to vote along party lines.
(P)urchasing absentee ballots (P-Ballot)8, purchasing Discussion. The substantial reporting rate of recognized
proofsuchasavotertakingaselfiewiththeirballotcontain- coercioninstancesat26%,withfamilymemberscitedasthe
ing the coercer-dictated choices (P-Selfie), and an app predominant coercion source (15%), highlights the need for
offering compensation to vote as directed (P-App). They coercion-resistantstrategiestocounterpersistentadversarial
alsoratedthefollowingfourpotentialsourcesofcoercionon oversight.Insuchcircumstances,thepracticeofdeniablere-
the same scale: a party operative (S-Party), an authority voting, such as the one in use in Estonia [22], falls short.
figure (S-Authority), a family member (S-Family), Family members, typically having substantial unrestricted
and an employer (S-Employer). Ratings are then con- access, can potentially cast a vote on behalf of the targeted
verted to a score between -3 to 3, where -3 is ‘completely relative just before the election concludes. Such a coercer
unlikely’ and 3 is ‘completely likely’. can also realistically stay with the relative or retain their
Perceived Scenarios (Fig. 7a). Among the coercion sce- device or voting credential until the voting process ends.
narios, participants find P-Selfie to be the most likely
scenario with a mean score of 0.51 and a median of 1 4.3. Trust in Voting Methods
(somewhat likely). A quarter (24%) of participants rated
P-Selfie as extremely likely, with this scenario being Wenowinvestigatetheleveloftrustthe120participants
perceived as having a statistically significantly higher like- intreatmentgroupsperceiveinourcoercion-resistantsystem
lihood compared to the other three coercion scenarios. In versusothervotingmethods.Participantsratedtheirlevelof
trust in the following voting methods on a 7 point Likert
8.IllustratedbytheNorthCarolinaballotfraudincident[44]. Scale (summarized from §D.7): (I)n-person voting with
7
stnapicitraP
#c F2 riedman(3)=40.64, p pH= ol7 m-. a8 dj0 .=e 1−
p
.0
2Ho
59
lm
e,
-
−W
a 0dj
8.K =e 7n .d 3a 9ll e= −00 7.09 p,
H
C olmI -9 a5 dj%
.=
[ 90 .. 70 15 e,
−
01 4.00], npairs=150 c F2 riedman(3)= pH4 ol2 m. -3 ad0 j.=, p 6.4= p93 e Ho−. l4 m07 -4 ae dj.− p=0
H0
o9
.
lm0, -2W adj.K =e 1n
p
.d 4Ha
o
9l ll
m
e=
- −a
0d0
j
9.. =0 39 .,
0
C 4eI −95 0%
6
[0.06, 1.00], npairs=150 CS -c Fe on ra cer fio
ul
1H p 2ea n %p e- dH
p
8N
e
1ao
n
%pt e-
d
pHolm-adj.=0.05 3 P-Ballot 8% 85%
3 P-Selfie 6% 86%
P-App 3% 88% 2
TotalCount 44 511
2 UniqueCount 26 112
1 mmean=−0.34 mmean=−0.31
mmean=0.51
mmean=−0.05
01
mmean=0.02
mmean=0.53
mmean=−0.25 mmean=−0.16
SSS -F-o
P
au
a
mr rc
t
iye
ly 1 10 5% % 78 71 %%
0 S-Authority 7% 85%
−1 S-Employer 11% 83%
−1 TotalCount 63 491
−2 UniqueCount 31 108
−2 Unique 39 98
−3 Participants
−3 S−Party S−Family S−Authority S−Employer (c)Coercion Instances.Par-
(n = 150) (n = 150) (n = 150) (n = 150)
C−Forceful P−Ballot P−Selfie P−App ticipants state whether these
(n = 150) (n = 150) (n = 150) (n = 150)
(b) Coercion Sources. Participants perception of likely itemshavehappenedtothem
(a) Coercion Scenarios. Participants perception of likely sources of coercion; family members received the highest or someone they know. The
coercionscenarios;ballotselfiesreceivedthehighestaverage average likelihood rating, while authority figures received remaining participants chose
likelihood,whileforcefulcoercionreceivedthelowest. thelowest. nottodisclose.
Figure 7: Participants’ Views and Experiences with Coercion Scenarios, and Coercion Sources
VotingMethod I- I- I- R- R- IR- in I-BMD and I-Ballot, and least in R-Mail and
Ballot BMD DRE Mail Online Online R-Online, with I-DRE and IR-Online in-between.
Mean 1.38 1.68 1.18 0.72 0.82 1.25
IR-Online. Participants generally regard IR-Online as
Median 2 2 1.5 1 1 2
‘somewhat trustworthy’, as indicated by a mean rating of
Trust Rating Participants(%)
1.25. However, the median participant sees it as ‘highly
Trusted(3) 35 36 26 22 20 20
Highly 25 33 24 24 22 35 trustworthy’, as indicated by a greater median rating of 2.
Somewhat 12 11 18 15 17 16 This ranks IR-Online marginally higher than I-DRE,
Neutral(0) 12 10 17 12 18 17
having a mean trust rating of 1.18, yet remains below
Somewhat 7 5 7 7 9 5
Highly 6 3 4 12 6 4 I-Ballot, exhibiting a higher average of 1.38. More-
Untrusted(-3) 3 3 3 8 7 3 over, although we find no statistically significant difference
VotingMethod PairwiseStatisticalOutcomes(Fig.9b) in trust ratings between the treatment groups, the median
I-Ballot × × ↑ ↑ × trustworthiness rating for the security priming groups SF
I-BMD × ↑ ↑ ↑ ↑
and SM is ‘somewhat trustworthy’, contrasting with groups
I-DRE × ↓ × × ×
R-Mail ↓ ↓ × × × F and M’s ‘highly trustworthy’ rating (Fig. 9d). We also
R-Online ↓ ↓ × × × find no statistically significant difference in trust ratings for
IR-Online × ↓ × × ×
IR-Online and R-Online between the control group
TABLE 1: Voting Methods. This table presents the trust ratings (n = 30) and the treatment groups (n = 120) (Mann-
for various voting methods, as given by the 120 participants
Whitney U test: p=0.969).
exposed to fake credentials, and a summary of the statistical
ParticipantsExperiencingCoercion.Weexaminethetrust
outcomes (Fig. 9b). × means no statistical difference, ↓ means
statistically significantly lower, and ↑ means statistically signifi- ratings from the 33 participants (22%) not in group C who
cantly higher. report personal or known experiences of coercion. These
results, depicted in Figure 9c, show statistically signifi-
cant lower trust scores for R-Mail and R-Online when
hand-marked paper ballot (I-Ballot), in-person voting compared to I-BMD, indicating a general distrust towards
with a ballot marking device (I-BMD), in-person voting remote voting methods. Based on Cohen’s d, the effect
with a direct electronic device (I-DRE), (R)emote voting sizes between I-BMD and R-Mail, as well as I-BMD and
via mail-in ballot (R-Mail), a fully-remote voting sys- R-Online,are0.76and0.69respectively.Simultaneously,
tem where both voter registration and voting are online the score of I-Ballot dropped to match IR-Online,
(R-Online), and the online voting system they just ex- while that of I-DRE rose to match I-BMD. This could
perienced with in-person voter registration (IR-Online). possiblybeinfluencedbytheperceivedriskofballotselfies.
Table 1 presents our summarized results while Figure 9 in Discussion. These findings demonstrate the system’s
Appendix A contains more complete statistical results. promising potential to attain trustworthiness scores on par
Summarized Results. Analyzing the trust scores and the with in-person voting. Presently, the system’s overall score
pairwise statistical outcomes across voting methods, we exceedthoseofin-personvotingwithadirectrecordingde-
identify three cohorts. Participants place the highest trust vice, employed in 11.5% of U.S. jurisdictions in 2020 [45],
8
gnitaR
doohilekiL
Pairwise
test:
Durbin−Conover,
Bars
shown:
significant
gnitaR
doohilekiL
Pairwise
test:
Durbin−Conover,
Bars
shown:
significanteasy positive 17 new neutral 3 (F)ake (M)alicious
Groups (C)ontrol SF SM
interesting positive 13 streamlined positive 3 Creds. Kiosk
complicated negative 8 long negative 2 Rating SystemApprovalRating
good positive 7 fine neutral 2
simple positive 7 fast positive 2 Positive 73% 63% 60% 56% 53%
Neutral 13% 20% 3% 17% 23%
straightforward positive 4 complex negative 2
Negative 13% 17% 37% 27% 23%
great positive 4 convenient positive 2
smooth positive 4 cumbersome negative 2 SystemUsabilityScale
confusing negative 4 efficient positive 2 Score 69.6 70.4 69.9 67.3 62.7
SD 18.6 18.6 17.4 19.8 21.9
TABLE 2: Distribution of Single-Word Summaries with Senti- N 29 28 29 30 29
ment. This table lists the words used more than once by partici- Percentile 55.1 57.8 56.1 47.8 35.0
Usability 69.5 69.8 68.4 67.7 62.5
pants to describe their impressions of the system along with each
Learnability 69.8 73.2 75.9 65.8 63.4
word’s associated sentiment (positive, neutral or negative).
Scale UserExperienceQuestionnaire(Scorevs.Benchmark)
Attractiveness ↑ ↑ ↓ ↓ ↓↓
Perspicuity ↑ ↓ ↓ ↓ ↓↓
and approaches the trust levels associated with in-person Efficiency ↑↑ ↑↑ ↑ ↓ ↓
Dependability ↓ ↑ ↑ ↓ ↓↓
hand-marked paper ballots, used in 68.1% of jurisdictions. Stimulation ↑↑ ↑↑ ↑ ↑ ↓
Additional Findings. In §A.1, we discuss our findings Novelty ↑ ↑↑ ↑↑ ↑↑ ↑
when we contrast the other voting methods with each other, Category MostLiked(%)
particularly the unanticipated lower trust for I-Ballot EaseofUse 20 27 27 43 17
Instructions 27 20 13 13 27
compared to I-BMD. We also discuss the surprisingly low RemoteVoting 23 10 20 10 13
trust rating for R-Mail, despite its use by 46% of voters Security 3 20 10 17 23
QRCodes 7 13 17 3 10
in the 2020 U.S. presidential election [46], and the high Other(Positive) 3 10 10 7 10
trust rating in I-BMD, despite only 6.6% of participants Other 17 0 3 7 0
capableofdetectingamanipulatedBMD-printedballot[35]. Category MostDisliked(%)
Process
Moreover,wepresentourfindingswhenconsideringall150 27 23 30 23 23
Complexity
participants,particularlythestatisticallysignificantlyhigher Credential
20 23 7 17 30
trust in IR-Online compared to R-Online. Handling
Confusion 7 17 17 13 17
Security/Coercion 7 10 23 13 7
In-PersonReg. 7 3 7 0 3
5. Usability of Registration and Voting (None) 17 13 10 23 10
Other 17 10 7 10 10
Types UseErrors(#Events)
In this section, we present the usability results for TRIP,
ToreReceipt 3 1 0 0 1
a voter-verifiable registration process that outputs real and AlmostTear 3 4 0 1 0
fake credentials and relies on voters to identify deviations EnvelopePick 1 2 0 1 1
DiscardedReal
during credentialing to meet voter verifiability. We begin 0 0 0 0 1
Cred.
withqualitativeresults,discussingthesystemaspectspartic- ActivateDifficulty 3 5 6 5 10
ipantsmostlikedanddisliked.Wethenpresenttheobserved Total(48) 10 12 6 7 13
use errors and the number of participants who successfully Levels UseErrors(#Events)
completed the registration and voting process without facil- Mistakes 10 7 5 7 11
Violations 0 5 1 0 2
itator intervention. We continue with the System Usability
Types UseErrors(#RequiredFacilitatorEvents)
ScaleandUserExperienceQuestionnairescores,contrasting
Kiosk 4 1 0 0 2
these against benchmarks and other voting-related studies. Activation 2 3 3 2 2
We conclude with our findings concerning the participants Study-Wide 0 0 0 0 1
who encountered our purposely designed malicious kiosk. Reporting KioskReportingRate
Groups. The control group (C) establishes the baseline Facilitator 0% 0% 10% 0% 47%
Survey 10% 7% 20% 7% 57%
against which we evaluate the four experimental groups (F,
M,SF,andSM).GroupFservesastheexpectednorm,with TABLE 3: Usability Results Overview. The top section up to
“LikeLeast”representsparticipants’perceptionofsystemusability
group M being relevant in the event of a malicious kiosk.
anduserexperience.Themiddlesectionrepresentstheiruseerrors
andfacilitatorinterventionsduringthestudy.Thefinaldistinctta-
5.1. Qualitative Results blerepresentsthekioskreportingrate.↑↑istop25%ofbenchmark
studies, ↑ is top 50%, ↓ is bottom 50% and ↓↓ is bottom 25%.
We present the results from participants’ single-word
summaries and the system aspects they liked or disliked the
most. For the single-word summaries, we classified each We observe that the control group perceived the sys-
word based on sentiment, marking them as positive, neutral tem most positively. As the level of engagement increased,
ornegative.Table2presentsthewordsthatoccurmorethan subsequent groups showed a consistent decline in posi-
once. We also devised categories that best encapsulate the tive ratings. Meanwhile, neutral and negative ratings vary
items expressed and split them into two groups: most liked between 13 to 27%, with the exception of group M (a
and most disliked (Table 3). 37% negative rating). This surge partly derives from partic-
9ipants’ suspicion of the kiosk’s unexpected (and incorrect) two QR codes from the receipt and one QR code from the
behavior,expressingtheirsurprisewithwordslike“hacked” envelope. For those participants who read the instructions
and “suspect”—terms absent from other groups’ feedback, yet still faced challenges, mistakes often involved scanning
including SM. Moreover, while group M shows the fewest onlyasubsetofQRcodesatonceinsteadofallthreesimul-
participants valuing security among the treatment groups, taneously, and placing the credential on the device’s screen
group SM shows the most. Despite these observations, we insteadofonthetabletobescannedbythedevice’scamera.
cannot confirm statistical differences between the groups Considering these activation errors, it naturally follows that
and the ratings (Fisher’s Exact Test, p=0.24). credential handling emerged as the second most disliked
We find a roughly equal divide between those who find aspect of the system among participants.
the system easy to use and those who consider it complex. Success Rate. 95% of participants succeeded in registering
We also observe a roughly equal split between those who and creating their credentials without assistance from the
appreciated the system’s instructional guidance and those facilitator.Thisresultappearstosupportthepracticalityand
who disliked the handling (e.g., scanning, storage) of paper usability of a coercion-resistant voter-verifiable registration
credentials.Wealsofindthatgiventhelackofonlinevoting process like TRIP’s, despite its complexity. Success rate
in the study location, 15% of participants appreciate the drops when we include errors later in the voting pipeline,
ability to vote remotely. A minority (4%) expressed dissat- however.92%ofparticipantsactivatedtheircredentialwith-
isfactionwiththeneedforin-personregistration.Finally,we out help. In combination, 87% accomplished both regis-
observe that participants in the treatment groups valued the tration and activation by themselves, with 19 participants
system’s security at least threefold compared to the control (13%) needing assistance. Counting participants who mis-
group,withhalfofthesecommentspraisingfakecredentials. takenly used their fake credential to cast their vote further
reduces the success rate to 83%, as we discuss in §6.2.
5.2. Use Errors Statistical Analysis. We categorized participants into four
age groups: 18-30, 31-45, 46-65, and 65+. Analysis of use
During the study, we record observational notes for each errors across these age groups using the chi-squared test
participant,includinguseerrorsandfacilitatorinterventions. revealednostatisticallysignificantdifferences(p=0.8791).
We focus on use errors fundamental to the credentialing Further examination employing logistic regression to assess
process, while we discuss interface and device-induced use the interaction effects between age groups, ethnicity, educa-
errors in §A.2. We classify use errors into two categories: tion, and gender on use errors also indicated no statistically
mistakes and violations. We characterize mistakes when significant associations as determined by the Wald test.
participants’ plan or intended action is flawed, typically System Improvements. The study results suggest several
resulting from misinterpretation of instructions. In contrast, potential ways to improve the success rate. A receipt de-
violations arise when participants intentionally disregard or tection mechanism could enable the kiosk to restart creden-
skip instructions. We now present a discussion of process- tial creation if the user prematurely tears off the receipt.
induced use errors we observed, as reported in Table 3. Activation issues might be reduced by reading the QR
Credentialing. During the real credential creation process codesincrementally,insteadofexpectingthreereadableQR
with an honest kiosk (C, F, SF), 5 participants (6%) pre- codesinoneimage.Redisplayingtheactivationinstructions
maturely tore off the receipt after the kiosk printed the first upon unsuccessful activation may help users who skip the
QR code—a mistake that required us to intervene during instructions. An animation showing how to use the device’s
activation. An additional 8 participants (9%) attempted to rear-facing camera to scan credentials might also help.
tear off the receipt but when they encountered resistance Study Comparisons. We compare our success rate with
from the receipt printer, they rectified their mistake by three usability studies containing seven variants of voting
consulting the kiosk display guiding them to pick and scan systems: the only other JCJ-style coercion-resistant voting
an envelope. In terms of envelope selection, 5 participants study we are aware of by Neto et al. [26], a study by Ace-
(6%) initially opted for an envelope that did not correspond myanetal.[47]onthreevoter-verifiablesystems(Helios[6],
withthesymbolonthereceipt;thekioskalertedthemtothis Preˆta` Voter[48],ScantegrityII[49]),andarecentstudyon
mistakeandtheyallsuccessfullyscannedacorrectenvelope STAR-Vote [50], a secure, auditable and transparent ballot
on their second attempt. A single participant misinterpreted marking device for in-person voting. The Neto et al. [26]
the “discard check-in ticket” screen after creating their real between-subjects study involved 80 university-affiliated in-
credential and mistakenly threw away their credential. dividuals aged 18-39 acquiring their real credential using
Activation. The majority of use errors occurred during the one of the following three variants and then casting both
activationphase,after“leavingthegovernmentoffice”,with real and fake votes: (1) in-person acquiring a pen drive, (2)
approximately 19% of participants encountering difficulties remotely via email, and (3) in-person with a password set
and 8% requiring facilitator assistance. Participants com- by the voter. The success rate for the first two variants is
monly skipped the on-screen instructions, a violation error. 100% and the last one is 85%. Acemyan et al. [47] con-
Participants therefore removed the entire receipt from the ducted a within-subjects study with 37 diverse participants
envelopewhenthedevicepromptedtoscanthreeQRcodes. on Helios, Preˆt a` Voter and Scantegrity II where Helios is
These participants likely associated the “three QR codes” an voter-verifiable online voting system while Preˆt a` Voter
to be the three QR codes on the receipt rather than the andScantegrityIIarecoercion-resistant,voter-verifiable,in-
10FWelch(4, 69.89)=0.67, p=0.62, w p2=0.00, CI95% [0.00, 1.00], nobs=145 3
2
100
1
0
Group
−1
80 −2 C
−3 mmean=69.57 mmean=70.45 mmean=69.91
mmean=67.33 mmean=62.67 Perspicuity Efficiency Dependability
F
M
60 3
2 SF
1 SM
40 0
−1
−2
−3
20 Attractiveness Novelty Stimulation
C F M SF SM
(n = 29) (n = 28) G(n r=
o
2 u9 p) (n = 30) (n = 29) UEQ PKI UEQ PKI UEQ PKI
loge(BF01)=3.49, R2Bpoaysetesriiaonr=0.00, CI9H5D%I [0.00, 0.00], rCJZaSuchy=0.71
(a)SystemUsabilityScaleAcrossGroups (b)UserExperienceQuestionnaireMeanScoresAcrossScales
Figure 8: Usability Scales
person voting systems. The success rate for Helios is 60%, of 446 studies involving a range of products and services,
Preˆt a` Voter is 60%, and Scantegrity II is 50%. The study from business/consumer software to hardware devices [53].
on STAR-Vote [50] reported a success rate of 93%. These 446 studies reveal an average SUS score of 68, with
Discussion. TRIP outperforms three out of the seven standard deviation of 12.5. Groups C, F, and M achieve
variants, including prominent coercion-resistant, voter- marginallysuperiorSUSscores,rankinginthe55.1st,57.8th
verifiable, in-person voting systems Preˆt a` Voter and Scant- and 56.1st percentiles, respectively. Conversely, groups SF
egrityII,whichTRIPmostcloselyresemblesasacoercion- andSMunderperformwithscoresrankinginthe47.8thand
resistant, voter-verifiable, in-person registration system. De- 35.0th percentiles. Works such as Bangor et al. [54] have
spite the remaining four variants surpassing TRIP, they ei- proposed adjective ratings based on the SUS score. These
therfallshortinachievingoneormoreoftheseproperties— ratings would classify group F—representing the expected
illustrating the challenge in designing a usable coercion- common case in realistic settings—as “Acceptable,” and
resistant system—or have a homogeneous study population. achieving a “Good” adjective rating.
TRIP substantially improves this success rate to 83%, up SUS: Study Comparisons. We now compare group F’s
from60%(Preˆta`Voter)and50%(ScantegrityII),andcloser SUSscorewithNetoetal.andAcemyanetal.’sstudy[26],
to the 93% success rate that STAR-Vote achieves. [47]. While Neto et al.’s variant 2 and 3 achieve a higher
To our knowledge, NIST has established standards for mean SUS score of 77.5 (81st percentile) and 77.4 (81st
electronic voting [51], but no official success rate exists. percentile), respectively, these were not statistically signifi-
Only a NIST internal document [52] recommends a success cantly higher than group F’s score (Welch one-sided t-Test,
rate of 98% (known as Total Completion Score) but where p=0.094andp=0.051).Netoetal.,unfortunately,didnot
98% only needs to fall within the 95% confidence interval. administer the SUS questionnaire to participants for variant
1, which is the most comparable to TRIP, in that it involves
5.3. Perceived Usability and User Experience the in-person delivery of voter materials. From Acemyan et
al.’s [47] study9, Group F’s mean SUS score of 70.4 was
We now present the system usability scale and the statistically significantly higher than Preˆt a` Voter’s score of
user experience questionnaire scores and compare them to 61 (Welch one-sided t-test, p < 0.05), and Scantegrity II’s
benchmarks, along with the systems mentioned in §5.2. score of 59 (Welch one-sided t-test, p < 0.05) but lacked
SystemUsabilityScale.InFigure8a,wepresenttheSystem conclusive statistical difference from Helios’ score of 76
UsabilityScale(SUS)scoresacrossourstudygroups,while (Welch two-sided t-test, p=0.17).
removing 5 participants due to inconsistent answers (agree- SUS: Discussion.Thecomparablepercentilescoresandthe
ing to most positive and negative items), as suggested by absence of statistical difference between groups C, F, and
Sauro[53].WeobservethatgroupsC,F,andMdemonstrate M suggest that introducing fake credentials do not change
similar average and median scores while groups SF and participants’ perceptions of usability. Further, even with
SM exhibit lower average scores, with group SM having a voters engaging in a nontrivial 4-step protocol, F achieved
scoredecreaseof10%overthecontrolgroup.Despitethese
observations, Welch’s one-way ANOVA reveals no statisti-
callysignificantdifferences(p=0.62).Nonetheless,wecan
9.These numbers are not present in the text [47]; we extract these
compareourSUSmeanscoreswithabenchmarkconsisting numbersfromtheirfigure4andassumea95%confidenceinterval.
11
erocS
SUS
Pairwise
test:
Games−Howell,
Bars
shown:
significant
erocS
naeMan Acceptable rating and achieved statistically significantly with the kiosk and via a survey question probing if they
higher SUS scores than Scantegrity II and Pre´t a` Voter. detected any irregularities while creating their real cre-
User Experience Questionnaire. Figure 8b and Table 7 dential (Appendix D.6). We administered the same survey
illustrate the UEQ scores for each scale (Attractiveness, question to groups F and SF as a control to identify false
Dependability, Efficiency, Novelty, Perspicuity, and Stim- positives rates. Table 3 presents the reporting rates.
ulation), along with the UEQ Key Performance Indicator Kiosk Reporting Rate. Among group M participants, 10%
(KPI) scores, which represent participants’ perception of reported the kiosk’s misbehavior to the facilitator, whereas
an ideal registration experience. Traditional usability as- this rate significantly increased to 47% for group SM.
pects encompass Efficiency, Perspicuity, and Dependability, Further, this difference in reporting rate is statistically sig-
while Novelty and Stimulation relate to user experience. nificant (Chi-squared, χ2 = 8.2079,p < 0.01; Crame´r’s V
Upon comparison, participants view this system as slightly =0.4068).10 Instructionalvideo3,showntogroupSM,thus
surpassing their expectations in Novelty, Stimulation, and substantially increased the reporting rate.
Attractiveness. However, it falls short in the traditional Survey Reporting Rate. A greater percentage of partic-
usabilityaspectsbyapproximatelyafullpoint.Nonetheless, ipants reported the kiosk’s misbehavior in the exit sur-
scores above -2 and 2 are extremely rare due to differing vey: 20% for group M and 57% for group SM. We still
opinions and answer tendencies [55] (e.g., avoidance of observe a statistically significant difference in the report-
extreme responses). Typically, as depicted in Figure 8b, a ing rates between groups M and SM (Chi-squared test,
positive evaluation is a score above 0.8, neutral evaluation χ2 = 7.0505,p < 0.01; Crame´r’s V = 0.3771). Unlike the
is a score between -0.8 and 0.8, and negative evaluation is kiosk reporting rate, however, the survey reporting rate was
a score less than -0.8 [55]. Based on this metric, Group F non-zero for groups C, F, and SF, averaging at 8%; these
has a positive evaluation for each of the scales. participants primarily cited confusion about the process.
UEQ: Benchmark. Similar to SUS, to better assess usabil- Real-World Scenario.Accordingtoparticipants’responses
ity, we need to compare the UEQ scores with other studies. tooursurveyquestions,85%arewillingtowatchaninstruc-
We first compare our scores to the UEQ benchmark [55], tionalvideobeforeparticipatinginvoterregistrationinareal
consisting of 21,175 participants from 468 diverse stud- world context. However, only 59% indicated willingness to
ies. While group F’s scores for efficiency, stimulation and “lock up” their personal devices in a locker before entering
novelty rank in the top quartile, the perspicuity scores are the booth and retrieve them afterwards.
between the 50th and 75th percentiles. This suggests that Study Comparison. Bernhard et al. [35] conducted a study
despite participants finding it more challenging to familiar- among diverse participants to examining the rate at which
izethemselveswiththissystemcomparedtothebenchmark, voters could detect malicious ballot manipulation from bal-
they still complete their tasks without excess effort, while lotmarkingdevices(BMDs).BothourandBernhardetal.’s
perceiving the system as both engaging and innovative. studies involved participants operating under the election
UEQ: Study Comparisons. We now compare our scores authority’s supervision, interacting directly with an official
with other voting-related studies, although the number of device: the BMD in their case and the kiosk in ours. In
studies that administer UEQ is more limited. One compara- both studies, we assess whether participants can visually
ble study conducted by Marky et al. [56] focuses on voter discern anomalous behavior from these devices. In their
verifiability—though not coercion-resistance—through the study, without any guidance, only 6.6% of 31 participants
use of code voting [57] as prominently used in the Swiss reported the error to the facilitator. When participants were
Online Voting System [17]. This approach provides voters askedbeforesubmissioniftheyhadcarefullyreviewedtheir
with physical materials, typically by mail, to cast a vote so ballot, 12.9% of 31 reported the error to the facilitator. In
as to prevent potential vote alteration by the voter’s device. contrast,thereportingrateforTRIPis10%withoutsecurity
Markyetal.teststhreevotingcodevariants—manualcodes, priming, and 47% with priming.
QR codes, and tangibles—with 18 participants. Our interest
lies in the QR-codes variant, as it is most similar to TRIP, 6. Usability of Fake Credentials
andisalsotheoptionfavoredinMarkyetal.’sstudy.Unlike
in TRIP, their scores for user experience (stimulation and
This section evaluates the comprehension and usability of
novelty) are neutral, although their scores for the traditional
fakecredentialsamongthe120participantsexposedtothem.
usability aspects (perspicuity, efficiency and dependability)
are all positive. Marky et al.’s study also achieves higher
6.1. Understanding Fake Credentials
perspicuity and efficiency scores, 2.2 vs. 1.13, and 2 vs.
1.59, respectively, while TRIP achieves a marginally higher
We examine voter comprehension of fake credentials in
dependability score 1.3 vs. 1.2.
two distinct stages: first with a pop quiz presented by the
kiosk, then later in the survey. This dual-stage assessment
5.4. Detecting a Malicious Kiosk
measures understanding of passive instruction and efficacy
We present our findings from participants exposed to our
10.For Crame´r’s V, values around 0.1 are typically considered small,
maliciouskiosk(groupsMandSM).Weassesstheirability
around 0.3 indicate a moderate effect size, and values of 0.5 or greater
to identify the kiosk’s misbehavior during their interaction signifyastrongeffect.
12Continue Remarkably, 99% of participants select the correct option
Continue
QuizAttempts 1 2 3 (Missing (Incorrect Count “Only myself with my pen markings” on their first attempt,
Correct
Option) thereby confirming their apparent comprehension (Table 4).
Options)
Distinguishing Credentials. We assess participants’ actual
RealCredentialSteps 65 21 1 0 3 90
RealCredentialUsage 148 2 0 0 0 150 ability to distinguish their credentials by asking them to
TestCredentialUsage 84 18 4 7 7 120 cast a vote using their real credential after completing voter
DistinguishCredentials 91 1 0 0 0 92
registration.Amongthe92participants,90%(83)accurately
TABLE 4: Quiz Results. This table represents the quiz attempts; identified their real credential. For the 10% (9 participants)
“Continue”signifiesthattheseparticipantscouldnotpassthequiz who failed to do so, we explore the reasons behind this.
after the third attempt. “Missing Correct Options” indicates that
One participant discarded their real credential during
theseparticipantsonlyselectedcorrectanswersbutdidnotsimul-
registration, as mentioned in §5.2. We examined each set
taneouslychooseallcorrectanswers.“IncorrectOption”indicates
ofcredentialstheremainingeightparticipantscreatedtoas-
that these participants selected the incorrect option in one of the
certain whether their real credentials had distinct markings,
quiz attempts and could not rectify it by the third attempt. §3.4
discusses participants’ assignment to quizzes. andfoundthattheydid.Fordetailsabouthowstudypartici-
pants generally marked their credentials, see Appendix A.3.
Although credential marking was apparently not the cause
ofactiveinstruction.Thequizposesthequestion“Whichof of these errors, many other potential causes remain that
the following purposes can you use a test credential for?” we could not identify, such as memory lapses, misreading
(§C.3).Notselecting“Tocastavotethatcountsinelection” instructions,environmentaldistractions,orevendeliberately
underscores the crucial understanding that fake credentials disobeyingourinstructions(e.g.,viewingthefacilitatorasa
cannotbeusedtocastarealvote;theremainingthreecorrect potential coercer, which “by our own game” might suggest
options must be concurrently selected to pass the quiz. voting with a fake credential in the facilitator’s presence).11
Quiz Results: Test Credential Usage. We find that 70% To gain deeper insights into individuals’ ability to recall
of participants answer the quiz correctly on their first at- sensitive data, we consider password-related studies [58],
tempt (Table 4). An additional 15% succeed on the second [59], which indicate a wide range of retention rates from
attempt,including3whoinitiallychosetheincorrectanswer. 23% to 98%. With a 90% success rate, this rate lies in
Thethirdandfinalattemptseestheremaining15%(18par- the upper end of this spectrum and is similar to De´ja`
ticipants),splitasfollows:4participantsanswercorrectly,7 Vu [59], a study conducted on using images for authen-
failtoselectallcorrectoptionssimultaneously,and7choose tication. Voters marking their own credentials incorporate
the incorrect option. In this quiz stage, 92% of participants several known retention-enhancing strategies, such as user-
avoid selecting the incorrect answer in all attempts, thereby generatedcontentandfavoringvisualimageryovertext.The
demonstrating an understanding of fake credentials. De´ja` Vustudyprovidesencouragingevidencethatimagery-
Exit Survey. Among the 8% of participants (3 from second basedmemorydegradationissignificantlylowerthanthatof
attempt + 7 from third attempt) who incorrectly selected PIN/passwords:afteroneweek,only1outof20participants
“To cast a vote that counts in an election” in the quiz, five failed to login with images while 7 to 6 individuals failed
of them correctly identified the use of fake credentials in to login with PIN and passwords, respectively.
the survey. In the remaining five, four could not recall the
We expect that the duration over which TRIP users
purpose of fake credentials, and one only wrote “voting”.
must remember their credential markings should normally
We infer that 4% (5 participants) likely finished the study
be much shorter than the requirements for long-term pass-
without a clear understanding of fake credentials, with one
words: from leaving the privacy booth until credential acti-
participant thinking that fake credentials can be used to
vation on their device. This duration may be only minutes
cast real votes. Among the participants who did not select
if the voter brought their voting device with them, hours
the incorrect option, 89% remembered, in their free-text
or at most days for a credential the voter activates at home
responses,theuseoffakecredentialstoresistcoercion.The
or elsewhere. Voters who do forget which credential is real
remaining 11% wrote about their use to educate others, test
may re-register at any time to obtain fresh credentials.
the system, and even profit by selling their fake credentials.
Confidence in Distinguishing Credentials. Participants
also rated how confident they were in recalling their real
6.2. Usability of Real and Fake Credentials
credential on a 7 point scale, where 1 is “No confidence”
and 7 is “Extremely confident.” 87% of participants gave
Considering that both real and fake credentials are used to
a confidence rating of 5 or more, with 55% of participants
cast votes in the same way—assessed previously in §5.2—
givingtheextremelyconfidentrating.7%rateda4(neutral),
the success of using real or fake credentials ultimately
and ratings 1 and 2 each got one participant.
hinges on the voter’s ability to distinguish between the two.
Fake Credentials In Reality? We ask participants about
In the study, out of the 120 participants exposed to fake
their willingness to create fake credentials if such a system
credentials,92chosetocreateoneormorefakecredentials.
Credential Distinguish Quiz. We initially verify partici-
11.We informally observed a few participants hiding their credentials
pants’ understanding of how to distinguish their credentials
under the table out of the facilitator’s sight, which might suggest such a
by administering the Credential Distinguish quiz (§C.3). ”facilitatorasadversary”perspective.
13existed; 53% of participants affirm that they would. Testi- than group F, despite the only change being an educational
monialsvaryfrom“becauseIhavebeeninsituationswhere brief in the instructional video about the rare possibility of
othersforcedme tovote”, “toarguelesswithpeoplevoting a compromised kiosk (§C.2). This increase in discomfort,
for another candidate” to “[...] a fun souvenir”, and “I’m however, came with a statistically significant improvement
in a demographic where I cannot imagine having someone in detecting a malicious kiosk in group SM compared with
trying to solicit my vote, if they did, I would simply tell groupM(47%vs.10%),withnoincreaseinfalsepositives.
them no without fear.” Additional testimonials, including a Meanwhile, twice as many participants in SM compared to
taxonomy, are available in Appendix A.4. M liked the system’s security the most. These observations
Discussion.Despiteintroducinganunforeseenandunprece- confirm that finding the right balance between security
dentedconceptto120participants,only4%(5participants) education and user comfort is an important challenge.
appear to have finished the study without a reasonable
Limitations. This study has several important limitations.
grasp of the use of fake credentials. Furthermore, although
First, it did not evaluate long-term questions, such as
voters were not required to generate fake credentials, 76%
whether voters can effectively store, manage, and properly
of participant chose to create at least one. This proactive
use credentials on their devices after activation to vote
engagementisfurtherexpressedby53%ofparticipantswho
in successive elections over periods of years. While the
arewillingtocreatefakecredentialsinareal-worldcontext.
problem of remembering which paper credential is real
Lastly, in spite of the identical nature of fake and real
technically ends at credential activation time in TRIP, the
credentials, 90% of participants successfully distinguished
problemofrememberingwhichdeviceholdstherealvoting
between the two when aiming to activate and cast a real
credential–orofrememberingwhichaccount,password,or
vote,onparwithastudyofusingimagesforauthentication.
PINguardsit–remains.Thus,thedetaileddesignandeval-
uation of long-term storage systems for coercion resistance
7. Discussion with fake credentials remains an important area for future
work. Systematically evaluating the usability of long-term
This section primarily offers key takeaways of our findings, credential storage systems may be a particularly difficult
building on the many results we have presented earlier. We challenge, as studies addressing this challenge would nec-
also expand on our study’s limitations. essarily be longitudinal in nature, requiring participants to
TheNeedForCoercion-Resistance.Ourstudyrevealsthat remain involved over an extended period. We know of no
a quarter of participants have either personally experienced deployed voting system that could facilitate such a study.
coercion, or is aware of someone who has. These findings The TRIP system’s accessibility to voters with disabil-
highlight the necessity of coercion-resistance to uphold free ities is also important but beyond the scope of this study.
and fair elections. In essence, votes should not just be In the future, a thorough design for accessibility and corre-
privatebutincorruptible.Thisimportanceisfurtherreflected spondingstudyshouldincludevoterswithdisabilities,which
with these same participants rating (Fig. 9c) statistically would likely yield further insights. Although we aim for
significantly lower trust levels for both mail-in and online diversity,thestudybeingconductedinasinglegeographical
voting methods compared to in-person voting (with BMD). location may affect the generalizability of our findings.
Finally, the study did not replicate a government office
Usability of Coercion-Resistant Voting Systems. Design-
setting, which might influence differences in participants’
ing a usable coercion-resistant voting system is a difficult
views and behaviors. Similarly, the study does not replicate
task.Asseenin§5.2,votingsystemswithoutcoercionresis-
circumstances where a voter is truly under coercion.
tance,suchasSTAR-Vote[18],exhibitmuchhighersuccess
rates than systems that address coercion, such as Preˆt a`
Voter[60]andScantegrityII[49](93%versus60%and50% 8. Related Work
respectively). This disparity has motivated other coercion-
resistant strategies with less intricate tasks such as deniable Inprevioussections(§5.2&§5.3),wecomparedourresults
re-voting [19], [21], where voters only receive a single with other usability studies. This section therefore provides
votingcredentialandperformthesamevotecastingprocess a broader overview of related work.
to override their previous vote. However, this strategy is A few works [24], [25], [61] have considered usability
vulnerable to last-minute coercion, as well as to domestic issues with fake credentials and proposed possible imple-
coercion—the primary source of coercion in our findings— mentations, such as using smart cards. These works did not
where the coercer simply keeps the voting credential or conduct systematic studies with real users, however.
device. The 83% success rate achieved by TRIP shows Civitas [62], strengthens the verifiability of the JCJ
promise in narrowing the success gap between coercion- coercion-resistance scheme by having voters interact with
resistantandnon-coercion-resistantsystems,thoughthisgap multiple registration officials. There are no user studies of
remains an important usability challenge for future work. Civitas, however, and such a study may be difficult due to
The Impact of Security Vigilance on User Experience. thelogisticalchallengespresentedbymultipleregistrars,and
Our findings show an interesting tradeoff between user ex- the absence of a well-defined voter-facing design.
perience and security vigilance. Participants from group SF Other works have looked into using other countermea-
typically rated system usability and user experience lower sures to resist coercion [22], such as deniable re-voting and
14masking. The masking approach [57] provides each voter [7] V. Cortier, D. Galindo, R. Ku¨sters, J. Mu¨ller, and T. Truderung,
with a unique value b, known only to them, which is then “SoK: Verifiability Notions for E-Voting Protocols,” in 2016 IEEE
used to offset their cast vote. However, a usability concern
SymposiumonSecurityandPrivacy,2016.
[8] R.Ku¨sters,J.Liedtke,J.Mu¨ller,D.Rausch,andA.Vogt,“Ordinos:
arises as it remains uncertain if voters can recall and apply
AVerifiableTally-HidingE-VotingSystem,”in2020IEEEEuropean
their assigned value b during vote casting [22]. SymposiumonSecurityandPrivacy,2020.
An important security-related study in voting is one by [9] Y.-X.Kho,S.-H.Heng,andJ.-J.Chin,“AReviewofCryptographic
Distler et al. [63], which examines the impact of displayed ElectronicVoting,”Symmetry,vol.14,no.5,p.858,May2022.
security mechanisms on user experience. They found that [10] J.Benaloh,“BallotCastingAssuranceviaVoter-InitiatedPollStation
Auditing,” in 2007 USENIX/ACCURATE Electronic Voting Technol-
participants exposed to these mechanisms, e.g., messages
ogyWorkshop,Boston,MA,Aug.2007.
like “Encrypting your vote”, have a better user experience
[11] V.Cortier,A.Debant,P.Gaudry,andS.Glondu,“Belenioswithcast
than those who were not exposed to these mechanisms. asintended,”inVoting2023-8thWorkshoponAdvancesinSecure
Their research thus highlights the importance of the visibil- ElectronicVoting,May2023.
ity of security measures. In contrast to their work, we study [12] A.Juels,D.Catalano,andM.Jakobsson,“Coercion-ResistantElec-
the involvement of voters to achieve security measures.
tronicElections,”inTowardsTrustworthyElections:NewDirections
inElectronicVoting,Berlin,Heidelberg,2010,pp.37–63.
[13] J.BenalohandD.Tuinstra,“Receipt-freesecret-ballotelections,”ser.
9. Conclusion STOC’94,May1994.
[14] S. Park, M. Specter, N. Narula, and R. L. Rivest, “Going from bad
to worse: From Internet voting to blockchain voting,” Journal of
This paper presented a study with 150 individuals to
Cybersecurity,vol.7,no.1,Feb.2021.
evaluatetheirexperiencesandperceptionsofcoercion,along
[15] P.Daian,T.Kell,I.Miers,andA.Juels,“On-ChainVoteBuyingand
withtheirviewsofacoercion-resistantsystemwithavoter- theRiseofDarkDAOs,”Jul.2018.
verifiable registration system resilient to coercion via the [16] R.Ku¨sters,T.Truderung,andA.Vogt,“AGame-BasedDefinitionof
use of fake credentials. A quarter of participants had either Coercion-Resistance and Its Applications,” in 23rd IEEE Computer
SecurityFoundationsSymposium,2010,pp.122–136.
personally faced coercion or know of someone who did.
[17] “ProtocoloftheSwissPostVotingSystem:ComputationalProofof
Remarkably, 96% of participants understand the concept
CompleteVerifiabilityandPrivacy,”Tech.Rep.1.0.0,2021.
of fake credentials, with 90% successfully casting a vote
[18] S.Bell,J.Benaloh,M.D.Byrne,D.Debeauvoir,B.Eakin,P.Kortum,
usingtheirrealcredential.Moreover,overhalfofthepartic- N. McBurnett, O. Pereira, P. B. Stark, D. S. Wallach, G. Fisher,
ipantsexposedtofakecredentialsindicatedtheirwillingness J.Montoya,M.Parker,andM.Winn,“STAR-Vote:ASecure,Trans-
parent, Auditable, and Reliable Voting System,” in 2013 Electronic
to use them in reality. These findings show promise in
VotingTechnologyWorkshop/WorkshoponTrustworthyElections.
narrowing the usability gap between voting systems with
[19] D. Achenbach, C. Kempka, B. Lo¨we, and J. Mu¨ller-Quade, “Im-
and without coercion-resistance, and confirm the need for provedCoercion-ResistantElectronicElectionsthroughDeniableRe-
continued research into making verifiable coercion-resistant Voting,” in USENIX Journal of Election Technology and Systems,
voting systems more usable. 2015.
[20] K. Krips and J. Willemson, “On Practical Aspects of Coercion-
Resistant Remote Voting Systems,” in E-Vote-ID 2019: Electronic
Acknowledgements Voting,2019,pp.216–232.
[21] W.Lueks,I.Querejeta-Azurmendi,andC.Troncoso,“VoteAgain:A
Theauthorswouldliketothankthestudyparticipantsfor scalablecoercion-resistantvotingsystem,”in29thUSENIXSecurity
Symposium,2020,pp.1553–1570.
theirinvaluableinsightsandtheirtime,aswellasthereview-
[22] O. Kulyk and S. Neumann, “Human Factors in Coercion Resistant
ersfortheirhelpfulfeedback.Thisprojectwassupportedin
Internet Voting–A Review of Existing Solutions and Open Chal-
part by the armasuisse Science and Technology, the AXA lenges,” in Proceedings of the Fifth International Joint Conference
Research Fund, and the US ONR grant N000141912361. onElectronicVoting. TalTechpress,2020,p.189.
[23] D. Springall, T. Finkenauer, Z. Durumeric, J. Kitcat, H. Hursti,
M. MacAlpine, and J. A. Halderman, “Security Analysis of the
References
Estonian Internet Voting System,” in ACM SIGSAC Conference on
ComputerandCommunicationsSecurity,2014.
[24] S.NeumannandM.Volkamer,“CivitasandtheRealWorld:Problems
[1] P. Ehin, M. Solvak, J. Willemson, and P. Vinkel, “Internet voting
andSolutionsfromaPracticalPointofView,”inSeventhConference
inEstonia2005–2019:Evidencefromelevenelections,”Government
onAvailability,ReliabilityandSecurity,2012.
InformationQuarterly,vol.39,no.4,p.101718,Oct.2022.
[25] E. Estaji, T. Haines, K. Gjøsteen, P. B. Rønne, P. Y. A. Ryan,
[2] P.vandenBesselaar,A.-M.Oostveen,F.DeCindio,andD.Ferrazzi,
andN.Soroush,“RevisitingPracticalandUsableCoercion-Resistant
“ExperimentswithE-VotingTechnology:ExperiencesandLessons,”
RemoteE-Voting,”inE-Vote-ID2020:ElectronicVoting,2020.
SocialScienceResearchNetwork,ScholarlyPaper1433569,2003.
[3] D. Jefferson, A. D. Rubin, B. Simons, and D. Wagner, “A Security [26] A. S. Neto, M. Leite, R. Arau´jo, M. P. Mota, N. C. S. Neto, and
AnalysisoftheSecureElectronicRegistrationandVotingExperiment J.Traore´,“UsabilityConsiderationsForCoercion-ResistantElection
(SERVE),”2004. Systems,”inProceedingsofthe17thBrazilianSymposiumonHuman
[4] R. Krimmer, D. Duenas-Cid, and I. Krivonosova, “Debate: Safe- FactorsinComputingSystems,ser.IHC2018,Oct.2018,pp.1–10.
guarding democracy during pandemics. Social distancing, postal, or [27] L.-H.Merino,S.Colombo,R.Reyes,A.Azhir,H.Zhang,J.Allen,
internet voting—the good, the bad or the ugly?” Public Money & B. Tellenbach, V. Estrada-Galin˜anes, and B. Ford, “TRIP: Trust-
Management,vol.41,no.1,pp.8–10,Jan.2021. Limited Coercion-Resistant In-Person Voter Registration.” [Online].
[5] J. Benaloh, “Verifiable Secret-Ballot Elections,” Ph.D. dissertation, Available:https://arxiv.org/abs/2202.06692
1987. [28] D.Chaum,R.T.Carback,J.Clark,C.Liu,M.Nejadgholi,B.Preneel,
[6] B. Adida, “Helios: Web-based Open-Audit Voting.” in USENIX Se- A. T. Sherman, M. Yaksetig, and F. Zagorski, “VoteXX: A Remote
curitySymposium,vol.17,2008,pp.335–348. VotingSystemthatisCoercionResistant,”Oct.2020.
15[29] M. Backes, M. Gagne´, and M. Skoruppa, “Using mobile device [53] J.Sauro,APracticalGuidetotheSystemUsabilityScale. Measuring
communication to strengthen e-Voting protocols,” in Proceedings of UsabilityLLC,2011.
the 12th ACM Workshop on Workshop on Privacy in the Electronic [54] A. Bangor, P. Kortum, and J. Miller, “Determining what individual
Society,2013. SUS scores mean: Adding an adjective rating scale,” Journal of
[30] T.Krivoruchko,“RobustCoercion-ResistantRegistrationforRemote UsabilityStudies,vol.4,no.3,pp.114–123,2009.
E-voting,” in Proceedings of the IAVoSS Workshop on Trustworthy [55] M.Schrepp,“UserExperienceQuestionnaireHandbookVersion10.”
Elections,2007. [56] K.Marky,M.Schmitz,F.Lange,andM.Mu¨hlha¨user,“Usabilityof
[31] O. Spycher, R. Koenig, R. Haenni, and M. Schla¨pfer, “A New Ap- Code Voting Modalities,” in Extended Abstracts of the 2019 CHI
proachtowardsCoercion-ResistantRemoteE-VotinginLinearTime,” ConferenceonHumanFactorsinComputingSystems,2019.
in Proceedings of the 15th International Conference on Financial [57] P.Y.A.RyanandV.Teague,“PrettyGoodDemocracy,”inSecurity
CryptographyandDataSecurity,2011,pp.182–189. ProtocolsXVII,2013.
[32] R.Arau´jo,S.Foulle,andJ.Traore´,“APracticalandSecureCoercion- [58] A. S. Brown, E. Bracken, S. Zoccoli, and K. Douglas, “Generating
ResistantSchemeforInternetVoting,”inTowardsTrustworthyElec- andrememberingpasswords,”AppliedCognitivePsychology,vol.18,
tions:NewDirectionsinElectronicVoting,2010. no.6,pp.641–651,2004.
[59] R. Dhamija and A. Perrig, “Deja Vu: A User Study Using Images
[33] J.Benaloh,T.Moran,L.Naish,K.Ramchen,andV.Teague,“Shuffle-
Sum:Coercion-ResistantVerifiableTallyingforSTVVoting,”IEEE for Authentication,” in 9th USENIX Security Symposium (USENIX
TransactionsonInformationForensicsandSecurity,2009.
Security00),2000.
[60] D. Khader, Q. Tang, and P. Y. Ryan, “Proving preˆt a` voter receipt
[34] S. G. Weber, R. Araujo, and J. Buchmann, “On Coercion-Resistant
freeusingcomputationalsecuritymodels,”in2013ElectronicVoting
Electronic Elections with Linear Work,” in The Second Conference
TechnologyWorkshop,2013.
onAvailability,ReliabilityandSecurity,2007.
[61] S. Neumann, C. Feier, M. Volkamer, and R. Koenig, “Towards a
[35] M.Bernhard,A.McDonald,H.Meng,J.Hwa,N.Bajaj,K.Chang,
practical JCJ/Civitas implementation,” in Informatik Angepasst an
andJ.A.Halderman,“CanVotersDetectMaliciousManipulationof
Mensch,OrganisationUndUmwelt,2013.
BallotMarkingDevices?”in2020IEEESymposiumonSecurityand
[62] M. R. Clarkson, S. Chong, and A. C. Myers, “Civitas: Toward a
Privacy,May2020,pp.679–694.
Secure Voting System,” in 2008 IEEE Symposium on Security and
[36] J.Brooke,“SUS:A‘QuickandDirty’UsabilityScale,”inUsability
Privacy,May2008,pp.354–368.
EvaluationInIndustry. CRCPress,1996.
[63] V. Distler, M.-L. Zollinger, C. Lallemand, P. B. Roenne, P. Y. A.
[37] B.Laugwitz,T.Held,andM.Schrepp,“Constructionandevaluation Ryan,andV.Koenig,“Security-Visible,YetUnseen?”2019.
of a user experience questionnaire,” in Symposium of the Austrian [64] P. B. Stark, “There is no Reliable Way to Detect Hacked Ballot-
HCIandUsabilityEngineeringGroup. Springer,2008,pp.63–76. Marking Devices,” Election LawJournal Rules, Politicsand Policy,
[38] A. Hinderks, M. Schrepp, F. J. Dom´ınguez Mayo, M. J. Escalona, vol.19,no.3,2019.
and J. Thomaschewski, “Developing a UX KPI based on the user [65] A. W. Appel, R. A. DeMillo, and P. B. Stark, “Ballot-Marking
experiencequestionnaire,”ComputerStandards&Interfaces,2019. DevicesCannotEnsuretheWilloftheVoters,”ElectionLawJournal:
[39] J.KjeldskovandM.B.Skov,“StudyingUsabilityInSitro:Simulating Rules,Politics,andPolicy,vol.19,no.3,pp.432–450,Sep.2020.
Real World Phenomena in Controlled Environments,” International
JournalofHuman–ComputerInteraction,2007.
Appendix A.
[40] R. Ploetzner, S. Berney, and M. Be´trancourt, “When learning from
animations is more successful than learning from static pictures: Miscellaneous Questions
Learningthespecificsofchange,”InstructionalScience,2021.
[41] S. Tu¨rkay, “The effects of whiteboard animations on retention and
subjectiveexperienceswhenlearningadvancedphysicstopics,”Com- A.1. Trust Rating in Other Voting Methods
puters&Education,vol.98,pp.102–114,Jul.2016.
[42] R.E.Mayer,L.Fiorella,andA.Stull,“Fivewaystoincreasetheef- Most Trusted. Participants express the most trust towards
fectivenessofinstructionalvideo,”EducationalTechnologyResearch I-BMD, with 69% deeming it as highly trustworthy or
andDevelopment,vol.68,no.3,pp.837–852,Jun.2020.
higher,andassigningitanaveragetrustscoreof1.68outof
[43] R. Likert, “A technique for the measurement of attitudes.” Archives
ofPsychology,vol.22140,pp.55–55,1932. 3 (Table 1). Moreover, I-BMD is statistically significantly
[44] “FourpeoplepleadguiltyinNorthCarolinaballotprobeof2016and higher than that of all other voting methods, except for
2018elections,”NBCNews,2022. I-Ballot (Fig. 9b), which earns the same level of trust
[45] “Verifier-VerifiedVoting2024,”https://verifiedvoting.org/verifier/. from60%ofparticipantsandanaveragetrustscoreof1.38.
[46] S. Atske, “Pew Research Center - The voting experience in 2020,”
Least Trusted. Participants express the least trust in
Nov.2020.
R-Mail,only46%consideringithighlytrustworthyorbet-
[47] C. Z. Acemyan, P. Kortum, M. D. Byrne, and D. S. Wallach, “Us-
abilityofVoterVerifiable,End-to-endVotingSystems:BaselineData ter (mean score of 0.72). Participants also view R-Online
forHelios,Preˆta` Voter,andScantegrityII,”vol.2,no.3,2014. similarly, yielding similar mistrust (mean score of 0.85).
[48] P.Y.A.Ryan,D.Bismark,J.Heather,S.Schneider,andZ.Xia,“PrEˆt All Participants. Figure 9a depicts the trust ratings across
A` Voter: A Voter-Verifiable Voting System,” IEEE Transactions on
voting methods from all 150 participants (including the
InformationForensicsandSecurity,vol.4,no.4,pp.662–673,2009.
controlgroup),alongwithourstatisticalresults.Participants
[49] D. Chaum, R. Carback, J. Clark, A. Essex, S. Popoveniuc, R. L.
Rivest,P.Y.A.Ryan,E.Shen,andA.T.Sherman,“ScantegrityII: trustin-personregistrationforonlinevotingstatisticallysig-
End-to-EndVerifiabilitybyVotersofOpticalScanElectionsThrough nificantly more than they trust a fully-online voting system.
Confirmation Codes,” IEEE Transactions on Information Forensics
Participants further emphasized this during the study by
andSecurity,vol.4,no.4,pp.611–627,Dec.2009.
asking us whether we would require individuals to verify
[50] C.Z.Acemyan,P.Kortum,M.D.Byrne,andD.S.Wallach,“Sum-
mative Usability Assessments of STAR-Vote: A Cryptographically their eligibility, as we had not required this in the study.
Securee2eVotingSystemThatHasBeenEmpiricallyProventoBe Discussion. 46% of voters cast an absentee ballot in the
EasytoUse,”HumanFactors,pp.866–889,Aug.2022. 2020 U.S. presidential election [46]. Despite this, mail-in
[51] NIST,“VoluntaryVotingSystemGuidelinesVersion2.0,”Tech.Rep.,
voting has the second-lowest trust score (Fig. 9a) or the
Feb.2021.
lowesttrustscore(Fig.9b&Fig.9c).Participantsexpressed
[52] ——,“UsabilityPerformanceBenchmarksFortheVoluntaryVoting
SystemGuidelines,”Tech.Rep.,Aug.2007. a lack of confidence in the U.S. postal service’s capacity
16c F2 riedman(5)=54.52, p=1.64e−10, WKendall=0.07, CI95% [0.05, 1.00], npairs=150 c F2 riedman(5)=38.31, p=3.28e−07, WKendall=0.06, CI95% [0.04, 1.00], npairs=120
pHolm-adj.=5.11e−03 pHolm-adj.=7.15e−03
pHolm-adj.=4.88e−03 pHolm-adj.=0.02
pHolm-adj.=9.83e−06 pHolm-adj.=1.91e−06
pHolm-adj.= p4 H. o3 lm0 -e a− dj.0 =3 p 1H .o 8lm 2- ea −dj. 0= 64. p9 H4 oe lm− -1 a0
dj.=0.01
pHolm-adp j.H =ol 0m .- 0a 2dj.=5.90e−06 pHolm-adj.=0.04
pHolm-adj.=0.01
3
3
2
12
mmean=1.43
mmean=1.71
mmean=1.21
mmean=0.85 mmean=0.75
mmean=1.24
1
mmean=1.38 mmean=1.67
mmean=1.18
mmean=0.72 mmean=0.82
mmean=1.25
0
0
−1
−1
−2 −2
−3 −3
I−Ballot I−BMD I−DRE R−Mail R−Online IR−Online I−Ballot I−BMD I−DRE R−Mail R−Online IR−Online
(n = 150) (n = 150) (n = 150) (n = 150) (n = 150) (n = 150) (n = 120) (n = 120) (n = 120) (n = 120) (n = 120) (n = 120)
Voting Methods Voting Methods
(a) Voting Methods Trust Rating from All Participants. All 150 (b)VotingMethodsTrustRatingfromParticipantsExposedtoFake
participants’trustratingsacrossvotingmethods. Credentials. The trust ratings for various voting methods, as given by
the120participantsexposedtofakecredentials.
c F2 riedman(5)=18.12, p=2.80e−03, WKendall=0.11, CI95% [0.05, 1.00], npairs=33 c K2 ruskal−Wallis(4)=2.72, p=0.61, e o2 rdinal=0.02, CI95% [7.71e−03, 1.00], nobs=150
pHolm-adj.=7.76e−03
pHolm-adj.=7.76e−03 3
3
2
2
m mean=1.63
mmean=1.52 1 m mean=1.20 m mean=1.13 m mean=1.20
mmean=1.24 m mean=1.03
1 mmean=1.03 mmean=1.09
0
mmean=0.09 mmean=0.27 0
−1
−1
−2
−2
−3
−3
C F M SF SM
I (− nB =a 3ll 3o )t (I n− B =M 3D 3) (I n− D = R 3E 3) (R n− =M 3a 3il ) R (− nO =n 3li 3n )e IR (n− O =n 3l 3in )e (n = 30) (n = 30) (n = 30) (n = 30) (n = 30)
Voting Methods Groups
(c)VotingMethodsfromCoercion-HappenedParticipants.Thetrust (d) IR-Online Voting Method Across Groups. The participants’ trust
ratingsforvariousvotingmethods,asgivenbythe39participantswho ratings across groups for the online voting with in-person registration
experiencedcoercionorknowsomeonewhohasexperiencedcoercion. votingmethod.
Figure 9: Voting Methods.
to deliver mail reliably. Despite concerns with BMDs [64], A.2. Use Errors
[65], including a study finding that only 6.6% of partici-
pants can detect a manipulated printed ballot [35], I-BMD
We discuss process-induced errors in §5.2. This section
received the highest trust score across voting methods.
discusses device- or interface-induced errors, which include
premature scanning of check-in tickets, unawareness to an-
swer the quiz, and unfamiliarity with the device’s camera.
17
gnitaR
tsurT
gnitaR
tsurT
Pairwise
test:
Durbin−Conover,
Bars
shown:
significant
Pairwise
test: Durbin−Conover,
Bars
shown:
significant
gnitaR
tsurT
gnitaR
tsurT
Pairwise
test:
Durbin−Conover,
Bars
shown:
significant
Pairwise
test:
Dunn,
Bars
shown:
significantEnvelope Type marking and then level of differentiation based on patterns
Differentiator NumberScribbleSymbolText Count
Symbol Change across credentials. We find six general types of markings:
Explicit 0 0 0 0 30 2 32 • Envelope Symbol: No pen markings.
Implicit 0 0 0 5 4 10 19
Indistin- • Number: Random numbers.
guishable 5 1 10 8 9 4 37 • Scribble: Indecipherable writings, including signatures.
Count 5 1 10 13 43 16 88 • Symbols: Predominantly shapes such as stars or squares
butsometimesincludingsmileyfacesoranimaldrawings.
TABLE 5: Credential Markings. Type of markings that partic-
ipants used to differentiate their fake credentials from their real • Text:Wordslike“Real”or“Test”orparticipants’initials.
credential.“TypeChange”signifiesparticipantsusedistincttypes, • Type Change: Alternation between two or more of the
“Scribble” includes signatures, and “Envelope Symbol” indicates above categories across the real and test credentials.
participantsusednomarkings,insteadrelyingondistinctenvelope Our findings, detailed in Table 5, show that 49% of par-
symbols. Four sets of envelopes are unaccounted for. ticipantsusedTexttodifferentiatetheircredentials,followed
by Type Change at 18%. Furthermore, 42% of participants
markedtheircredentialsindistinguishably,makingitimpos-
Premature Scanning. Of the 150 participants, 56 scanned sible to differentiate the real credential from the fake ones.
their check-in ticket before the kiosk asked them to do so. Participants may lie about their real credential by marking
Thisoccurred51timesingroupsC,FandSF.47(84%)cor- their real one as “Fake” and their fake one as “Real” but
rectedtheirmistakewithoutourinterventionuponrecogniz- participants in our study were not influenced to do so.
ingthecorrectintendedaction,suchaspressing“Continue”
or “Begin”. No participants in groups M or SM required A.4. Fake Credentials in Reality?
our intervention. This pattern occurred mostly in groups C,
F and SF due to an instructional slide, for which groups M We examine whether participants exposed to fake cre-
and SM were not privy due to the kiosk’s malicious setting. dentialswouldbewillingtocreatethemalongsidetheirreal
Upon viewing this instructional slide’s four steps, starting credential if such a system existed in the real world. 53%
with“ScanCheck-inticket”,participantsscannedtheirticket would do so, citing the following reasons:
instead of pressing “Begin” to initiate the process. When • Security (27 participants): “Because I have been in situa-
we were asked to intervene, we guided participants towards tionswhereothersforcedmetovote”,“Ontheoffchance
theintendedaction,typicallybypressing“Begin”,although someone tries to force me to vote in a particular way”.
84% of participants did not require help. These errors • Convenience(15participants):“noharmincreatingatest
may result from a mix of environmental factors, such as credential”, “might come in handy”, and “to argue less
inadequate contrast between the button and the screen in with people voting for another candidate”.
an outside environment, alongside behavioral factors. Such • Education (7 participants): “It might be useful to show
behavioral factors may include participants scanning their my students what the process was like”, “To see that it
check-in ticket before reading the instructions, perhaps due worked and educate family and friends”, and “It would
to over-confidence after seeing the instructional video. To be good for teaching people how to vote. Also might be
address these issues, we propose permitting a combination a fun souvenir”.
of inputs: e.g., interpreting the scanning of the check-in • Other (12 participants): “I would want to create as many
ticket during an instructional slide as an indication that the test credentials as possible before the market became
participant is prepared for subsequent steps. flooded with test credentials”, and “I’d like to test my
Mobile Device-induced. An additional 12 participants had device setup”.
initialdifficultywithactivatingtheircredentialastheyeither • No Reason Given (2 participants)
placed the QR codes directly on the device’s screen or used Thefollowingwerereasonsfornotcreatingatestcredential:
the front-facing camera. In each instance, we intervened • Unnecessary (39 participants): “I’m in a demographic
to instruct the participant to use the back-facing camera. where I cannot imagine having someone trying to solicit
Participants who continued to need assistance beyond this my vote, if they did, I would simply tell them no without
guidance are reported in the main body of the paper. fear,” and “Not interested in the uses.”
Kiosk-Induced. Three participants faced technical difficul- • Cumbersome or Confusing (11 participants): “I don’t
ties with the kiosk device, necessitating our intervention. want to take the risk of being confused between my real
Two of these instances arose from participants not realizing credentials and the fake one. I can use videos or websites
they had to complete a quiz before proceeding, while one to teach someone else how to vote,” and “More to lose,
participant had trouble locating the QR code scanner. and I would mix them up.”
• No Reason Given (7 participants).
A.3. Credential Markings
Appendix B.
We examine how participants mark their credentials to Meta-Review
distinguish their real credentials from their fake ones. We
categorized the set of credentials for 88 participants (four Thefollowingmeta-reviewwaspreparedbytheprogram
sets were unaccounted for), first by looking at the type of committee for the 2024 IEEE Symposium on Security and
18Privacy (S&P) as part of the review process as detailed in voter immediately upon leaving the booth to confiscate
the call for papers. all of their voting credentials. The authors suggest a po-
tential countermeasure for this last case, but all of these
B.1. Summary risks beyond the formal threat model – and potential
mitigationsforthem–remainimportantareasforfurther
This paper presents a field-experiment and usability test study.
oftheTRIP(Trust-limitingIn-PersonRegistration)protocol:
Appendix C.
a voter-verifiable registration system for coercion-resistant
online voting via the idea of fake credentials. The core Study Materials
contributionofthepaperistoprovideasenseofhowusable
the TRIP system is with an experiment that attempts to This section provides details about the study design.
captureaswideofavoter-poolaspossible.Thepaperdetails Additional notation. Considering the differences between
the protocol, discusses the recruitment and study strategy, groups, we adopt specific notations to indicate the text each
andprovidesadetailedanalysisofhowusersinteractedwith groupencounters.Thenotation[X|Y]indicatesthatX isthe
the system, what confusion points and errors occurred, and content present for the control group while Y is the content
howtheseresultsmightbeincorporatedintoaTRIP-enabled present for the treatment groups. On the other hand, [X$Y]
system in the future. indicates that Y is the content present for the malicious
kiosk groups (M, SM) while X is the content present for
B.2. Scientific Contributions the other groups.
C.1. Facilitator’s Scripts
• IndependentConfirmationofImportantResultswithLim-
ited Prior Research
Recruitment. Would you like to participate in a usability
• Provides a Valuable Step Forward in an Established Field
study on online voting? The study takes approximately 30
• Provides a New Data Set For Public Use
minutes and you will be compensated $20 for your time.
• Creates a New Tool to Enable Future Science
To be eligible, you must have registered to vote in the past.
• Establishes a New Research Direction
Thewholeprocessisfictitious,youwouldbevotingonyour
favorite color or favorite sport.
B.3. Reasons for Acceptance
Welcome. Thank you for your interest in our study. Online
voting is also known as electronic voting or e-voting and
1) The paper presents a valuable step forward in an estab-
enables voters to cast votes using their own device in the
lishedfieldbypresentingresultsfromafield-experiment
comfort of their own home. Here is the information sheet.
for a potential online voting scheme. The authors do a
We also have an instructional video for you to watch. At
goodjobofdetailingtheuserstudyandrigorouslytesting
any time during or after the study, you may withdraw from
the TRIP mechanism.
participation by contacting us using the contact info on the
2) The paper also offers researchers insight into leverag-
information sheet. Please review the information sheet and
ing fake credentials for coercion resistance and outlines
signtheconsentformifyouagreetoparticipateinthestudy.
potential pain points of this mechanism in real-world
Iwillkeeptheoriginalsignedconsentform.Youmaytakea
deployments
copy of the consent form or take a picture for your records.
The information sheet is yours to keep.
B.4. Noteworthy Concerns Groups C & F & M Scenario Priming. Imagine that the
year is 2030. Online voting is now commonplace. You are
1) The results illustrate that some voters may encounter renewing your drivers license at a government office. You
difficulties using fake credentials, with 17% of users see a poster announcing that you can vote online. You are
encountering an issue and 10% of users mistakenly interested, so you watch an instructional video on how to
voting with their fake credential. These findings could register for online voting. After watching the video, you go
adverselyaffectthesystem’sdeployabilityandunderline to the clerk to get started.
theneedforfurtherresearchtomitigateandreducesuch Groups SF & SM Scenario Priming. Imagine that the
use errors. year is 2030. Online voting is now commonplace. You are
2) It is unclear how requiring in-person registration may renewing your drivers license at a government office. You
impact voter turnout. This presents a practical consider- see a poster announcing that you can vote online. You
ation for the real-world deployment of this system that are interested, so you watch an instructional video on how
must be examined in future research. to register for online voting. You worry that hackers are
3) The system’s formal threat model excludes coercion trying to manipulate U.S. elections, but the video teaches
threats that could be realistic in practice, such as side- you how to register securely and protect your vote from
channel attacks, electronic surveillance via wearable de- manipulation. After watching the video, you go to the clerk
vices a voter is successfully coerced to sneak into the to get started. Being security conscious, you monitor the
registrationbooth,oracoercerwaitingtostrip-searchthe registrationprocesscarefullyandreportanythingsuspicious.
19Real Credential Steps. While creating your [|real ]voting
credential, when do you pick and scan an envelope to the
kiosk? (Check all that apply)
• After a symbol and three QR codes are printed
• After a symbol and one QR code is printed
• Before anything is printed
Real Credential Storage. What should you do with your
[|real]votingcredentialafterleavingthegovernmentoffice?
(Check all that apply)
• Activate it yourself on a device that you trust.
• Give it to someone else to activate on my behalf.
Test Credential Usage. Which of the following purposes
(a) Video 2. Participants are exposed to a slide that shows, side by side,
thecreationofarealcredentialandofafakecredential. can you use a test credential for? (check all that apply)
• To teach kids about voting.
• To resist pressure from someone to vote a particular way.
• Toselltosomeonewhoofferstobuyyourrealcredential.
• To cast a vote that counts in an election.
Credential Distinguish. Who can distinguish your test cre-
dential from your real credential? (Check all that apply)
• Anyone with a device.
• Only myself with my pen markings.
Appendix D.
Survey and Results
(b)Video 3.Participantsareexposedtoaslidethatwhichdepictshowa
kiosk can trick the voter into creating a fake credential instead of a real This section presents the exit survey with the precise
credential.
wording used, along with any figures or tables that could
Figure10:InstructionalVideoDifferences.Screenshotsfromthe not be incorporated into the main paper. For readability, we
instructional video concerning how to distinguish real and fake omit certain information such as the response type (e.g.,
credentials. numeric, free form, multiple choice) and merge followup
questions to the original question.
Check-In. Here is your check-in ticket, you may now enter
D.1. Background
the booth.
Check-Out.Pleasehandme[|anyoneof]yourcredential[|s]
to perform the check-out process. 1) What is your age?
Activation. Here is your credential. Now imagine that you 2) What is your gender?
have left the government office and are at home. Please use 3) What is your ethnicity?
this device as the device that you trust for online voting. 4) Whatisthehighestdegreeorlevelofeducationyouhave
For the purposes of this study, activate your real credential completed?
and then cast a vote in our mock elections. [|You may then Figure 11a presents the age distribution of participants for
activate your test credentials if you wish] eachgroupandFigure11bpresentsethnicity,educationand
Conclusion.That’sit.Inpractice,youwouldbeabletovote gender distributions.
in all elections for the next 5-10 years on your device until
youneedtorenewyouridentificationdocuments,whereyou D.2. System Usability Qualitative
would repeat this signup process.
1) Please describe your online voting signup experience in
C.2. Instructional Video Slides a single word.
2) What elements about this voter registration system did
Figure 10 presents the distinguish credential slides be- you like most?
tween instructional videos 2 and 3, as introduced in §3.4. 3) What elements about this voter registration system did
you like least?
C.3. Quizzes
D.3. System Usability Scale (SUS)
We present our quizzes and italicize the correct op-
tion(s). In the study, the kiosk shuffles the options for each Please rate each statement on a scale ranging from 1
participant. “Strongly Disagree” to 5 “Strongly Agree”:
20c K2 ruskal−Wallis(4)=1.93, p=0.75, e o2 rdinal=0.01, CI95% [8.74e−03, 1.00], nobs=144 Gender Ethnicity Education
Gender
100%
Female
80 Male
Prefer not to say
Other
75% Ethnicity
African−American
60 Asian
Caucasian
Jewish
Latino or Hispanic
m mean=47.63 m mean=45.86 50% Middle Eastern
Mixed
40 m mean=41.07 m mean=40.97 m mean=44.17 Native American
Prefer not to say
Education
25%
Trade School
Some High School
20 High School
Associate's Degree
C F M SF SM Bachelor's Degree
(n = 30) (n = 27) (n = 29) (n = 29) (n = 29) 0% Master's Degree
Group Ph.D., MD. or similar
(a)AgeAcross Groups.Theagedistributionofparticipantsacrossgroups (b) Gender, Ethnicity and Education. The distribution of
participants,categorizedbygender,ethnicityandeducation.
Figure 11: Participant Demographics
1) I would like to use this voter registration system when- Entry Left Right Scale(notshown)
1 annoying enjoyable Attractiveness
ever I renew my identifications documents (i.e., every
2 notunderstandable understandable Perspicuity
5-10 years) 3 creative dull Novelty
4 easytolearn difficulttolearn Perspicuity
2) I found the system unnecessarily complex. 5 valuable inferior Stimulation
3) I thought the system was easy to use. 6 boring exciting Stimulation
7 notinteresting interesting Stimulation
4) IthinkthatIwouldneedthesupportofatechnicalperson 8 unpredictable predictable Dependability
to be able to use this system. 9 fast slow Efficiency
10 inventive conventional Novelty
5) I found the various functions in this system were well 11 obstructive supportive Dependability
integrated. 12 good bad Attractiveness
13 complicated easy Perspicuity
6) I thought there was too much inconsistency in this sys- 14 unlikable pleasing Attractiveness
15 usual leadingedge Novelty
tem.
16 unpleasant pleasant Attractiveness
7) Iwouldimaginethatmostpeoplewouldlearntousethis 17 secure notsecure Dependability
18 motivating demotivating Stimulation
system very quickly. 19 meetsexpectations doesnotexpectations Dependability
8) I found the system very cumbersome to use. 20 inefficient efficient Efficiency
21 clear confusing Perspicuity
9) I felt very confident using the system. 22 impractical practical Efficiency
10) I needed to learn a lot of things before I could get going 23 organized cluttered Efficiency
24 attractive unattractive Attractiveness
with this system. 25 friendly unfriendly Attractiveness
26 conservative innovative Novelty
TABLE 6: UEQ Questionnaire
D.4. User Experience Questionnaire (UEQ)
D.5. UEQ Key Performance Indicator Extension
(KPI)
This questionnaire consists of pairs of contrasting at-
tributes. The circles between the attributes represent grada-
Pleaseratetheimportanceofthefollowingitemsrelative
tions between the opposites. You can express your agree-
to your use of an ideal voter registration system for online
ment with the attributes by ticking the circle that most
voting from 1 “Not important at all” to 7 “Very Important”.
closely reflects your impression.
Do not consider how important these items are in your
Pleasedecidespontaneously.Don’tthinktoolongabout daily life, but please focus specifically on how important
your decision to make sure that you convey your original they are when you interact with a voter registration system
impression. for online voting.
Table 6 presents the attributes, Table 7 presents the UEQ 1) Theidealvoterregistrationsystemshouldlookattractive,
scores for each scale and Figure 12 presents the itemized enjoyable, friendly and pleasant.
scores for each UEQ attribute. For clarity, we only include 2) I should perform my tasks with the ideal voter registra-
the positive attributes in the figure. tion system fast, efficient and in a pragmatic way.
21
egA
Pairwise
test:
Dunn,
Bars
shown:
significant
)%(
egatnecreP3
2
1
0
−1
−2
−3
attractive clear creative easy easy to learn efficient enjoyable
3
2
1
0
−1 Group
−2
C
−3
F
exciting fast friendly good innovative interesting inventive
M
3
SF
2
SM
1
0
−1
−2
−3
leading edge meets expectations motivating organized pleasant pleasing practical
3
2
1
0
−1
−2
−3
predictable secure supportive understandable valuable
Figure 12: User Experience Questionnaire Itemized Scores.
Sample
Group Type Attractiveness Perspicuity Efficiency Dependability Stimulation Novelty
Size
C UEQ 27 1.21 (1.13) 1.25 (1.59) 1.51 (0.77) 0.98 (0.85) 1.37 (0.87) 1.10 (0.64)
C PKI 27 1.81 (1.70) 2.04 (1.34) 2.37 (1.01) 2.41 (1.10) 1.63 (2.40) 1.63 (2.40)
F UEQ 27 1.49 (1.56) 1.13 (1.84) 1.59 (1.69) 1.30 (1.35) 1.38 (1.66) 1.44 (1.09)
F PKI 27 1.56 (2.64) 2.00 (1.23) 2.63 (1.01) 2.48 (1.11) 1.00 (3.00) 1.15 (2.98)
M UEQ 27 1.16 (1.33) 0.93 (1.96) 1.48 (1.26) 1.25 (1.12) 1.28 (1.11) 1.40 (1.12)
M PKI 27 1.63 (1.78) 2.30 (0.60) 2.67 (0.54) 2.48 (0.64) 1.15 (1.90) 1.22 (2.03)
SF UEQ 24 1.06 (1.73) 1.06 (1.73) 1.04 (1.30) 1.06 (1.33) 1.08 (1.44) 1.48 (0.86)
SF PKI 24 1.92 (1.38) 2.46 (0.87) 2.75 (0.72) 2.79 (0.26) 0.96 (3.26) 1.42 (2.77)
SM UEQ 25 0.55 (1.36) 0.28 (1.54) 0.66 (1.67) 0.49 (1.01) 0.78 (1.14) 0.81 (1.01)
SM PKI 25 1.76 (1.61) 2.32 (0.64) 2.76 (0.44) 2.60 (0.67) 1.08 (2.99) 1.00 (2.42)
TABLE 7: UEQ Scale Scores. This table presents the UEQ and UEQ PKI mean scores (and variance) for each scale, categorized by
group.
3) The ideal voter registration system should be easy to D.6. Test Credentials
understand, clear, simple, and easy to learn.
4) The interaction with the ideal voter registration system
1) Did you notice anything odd while creating your real
shouldbepredictable,secureandmeetsmyexpectations.
credential? Please explain.
5) Using the ideal voter registration system should be in-
2) Do you remember what a test credential is? Please list
teresting, exciting and motivating.
the potential uses for a test credential.
6) The ideal voter registration system should be innovative,
3) Did you create a test credential?
inventive and creatively designed.
4) After creating and marking the credentials as instructed,
how confident are you that you can remember which
credential is your real one?
22
erocS
naeM5) If online voting was an option today to cast votes in • A party operative pressures or coerces you to vote a
political elections, do you imagine yourself creating a particular way or buys your vote.
test credential alongside your real credential? Why or • Anauthority(e.g.,police,municipalworkers)pressur-
why not? ing or coercing voters to cast a vote in a particular
6) If online voting was an option today to cast votes in way or buys your vote.
political elections, do you image yourself watching an • A family member, such as a domestic partner, pres-
instructional video like the one you watched before sures or coerces you to vote a particular way or buys
completing voter registration? Why or why not? your vote.
7) If a government official asks you to leave all of your • An employer pressures or coerces you to vote a par-
personal devices in a locker before entering the booth ticular way or buys your vote.
and then retrieve them after you leave the booth, would 3) For each of the above questions (1) and (2), we asked
you be comfortable with this? participants whether this item “Has happened to you or
someone you know?”
D.7. Voting Methods • Yes
• No
• Describe
Pleaserateyourleveloftrust(e.g.,reliableandaccurate
4) Any additional comments about this user study?
counting and reporting of votes) with each of the following
voting options from 1 “Not trustworthy at all” to 7 “Com-
pletely trustworthy” and explain your reasoning.
1) Votingin-personatapollingplace,andfillingoutapaper
ballot by hand.
2) Voting in-person at a polling place, and submitting your
choices on an electronic kiosk which then prints a ballot
for you to inspect before depositing the ballot in the
ballot box.
3) Voting in-person at a polling place, and submitting your
choices on an electronic kiosk.
4) Votingremotelybyfillingoutapaperballot,andsending
it in the mail.
5) Voting online using an e-voting system that you also
register for online.
6) Voting online using an e-voting system, and registering
for it in-person using a process similar to the one you
just experienced.
Figure 9 presents these results.
D.8. Coercion Scenario
1) Which of the following scenarios do you think might
plausibly happen (or have happened) to you or someone
you know? Please rate the following scenarios from 1
“Not likely at all” to 7 “Extremely likely”
• Someone - such as an authority figure, employer, or
domestic partner - coercing voters to vote in a partic-
ular way, threatening harm if they do not.
• Someone coming to voters’ homes offering them
money if they register to vote by mail and give away
their mail-in ballot when it arrives.
• Someone on the Internet offering money to voters to
recordthemselvesvotingaparticularway,forexample
by taking a “ballot selfie”.
• Ane-votingapplicationthatsendsvotersmoneyifthey
cast votes that the app instructs.
2) Fromwhomdoyouthinkvotebuyingorcoercionismost
likely to happen (or have happened) to you or someone
you know? Please rate the following scenarios from 1
”Not likely at all” to 7 “Extremely likely”
23