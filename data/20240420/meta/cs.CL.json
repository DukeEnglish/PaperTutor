[
    {
        "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
        "authors": "Xingyu FuYushi HuBangzheng LiYu FengHaoyu WangXudong LinDan RothNoah A. SmithWei-Chiu MaRanjay Krishna",
        "links": "http://arxiv.org/abs/2404.12390v1",
        "entry_id": "http://arxiv.org/abs/2404.12390v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12390v1",
        "summary": "We introduce Blink, a new benchmark for multimodal language models (LLMs)\nthat focuses on core visual perception abilities not found in other\nevaluations. Most of the Blink tasks can be solved by humans \"within a blink\"\n(e.g., relative depth estimation, visual correspondence, forensics detection,\nand multi-view reasoning). However, we find these perception-demanding tasks\ncast significant challenges for current multimodal LLMs because they resist\nmediation through natural language. Blink reformats 14 classic computer vision\ntasks into 3,807 multiple-choice questions, paired with single or multiple\nimages and visual prompting. While humans get 95.70% accuracy on average, Blink\nis surprisingly challenging for existing multimodal LLMs: even the\nbest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only\n13.17% and 7.63% higher than random guessing, indicating that such perception\nabilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also\nhighlights that specialist CV models could solve these problems much better,\nsuggesting potential pathways for future improvements. We believe Blink will\nstimulate the community to help multimodal LLMs catch up with human-level\nvisual perception.",
        "updated": "2024-04-18 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12390v1"
    },
    {
        "title": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models",
        "authors": "Aitor OrmazabalChe ZhengCyprien de Masson d'AutumeDani YogatamaDeyu FuDonovan OngEric ChenEugenie LamprechtHai PhamIsaac OngKaloyan AleksievLei LiMatthew HendersonMax BainMikel ArtetxeNishant RelanPiotr PadlewskiQi LiuRen ChenSamuel PhuaYazheng YangYi TayYuqi WangZhongkai ZhuZhihui Xie",
        "links": "http://arxiv.org/abs/2404.12387v1",
        "entry_id": "http://arxiv.org/abs/2404.12387v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12387v1",
        "summary": "We introduce Reka Core, Flash, and Edge, a series of powerful multimodal\nlanguage models trained from scratch by Reka. Reka models are able to process\nand reason with text, images, video, and audio inputs. This technical report\ndiscusses details of training some of these models and provides comprehensive\nevaluation results. We show that Reka Edge and Reka Flash are not only\nstate-of-the-art but also outperform many much larger models, delivering\noutsized values for their respective compute class. Meanwhile, our most capable\nand largest model, Reka Core, approaches the best frontier models on both\nautomatic evaluations and blind human evaluations. On image question answering\nbenchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.\nMeanwhile, on multimodal chat, Core ranks as the second most preferred model\nunder a blind third-party human evaluation setup, outperforming other models\nsuch as Claude 3 Opus. On text benchmarks, Core not only performs competitively\nto other frontier models on a set of well-established benchmarks (e.g. MMLU,\nGSM8K) but also outperforms GPT4-0613 on human evaluation. On video question\nanswering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped\nin production at http://chat.reka.ai . A showcase of non cherry picked\nqualitative examples can also be found at http://showcase.reka.ai .",
        "updated": "2024-04-18 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12387v1"
    },
    {
        "title": "When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes",
        "authors": "Asaf YehudaiElron Bendel",
        "links": "http://arxiv.org/abs/2404.12365v1",
        "entry_id": "http://arxiv.org/abs/2404.12365v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12365v1",
        "summary": "We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.",
        "updated": "2024-04-18 17:48:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12365v1"
    },
    {
        "title": "Large Language Models in Targeted Sentiment Analysis",
        "authors": "Nicolay RusnachenkoAnton GolubevNatalia Loukachevitch",
        "links": "http://arxiv.org/abs/2404.12342v1",
        "entry_id": "http://arxiv.org/abs/2404.12342v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12342v1",
        "summary": "In this paper we investigate the use of decoder-based generative transformers\nfor extracting sentiment towards the named entities in Russian news articles.\nWe study sentiment analysis capabilities of instruction-tuned large language\nmodels (LLMs). We consider the dataset of RuSentNE-2023 in our study. The first\ngroup of experiments was aimed at the evaluation of zero-shot capabilities of\nLLMs with closed and open transparencies. The second covers the fine-tuning of\nFlan-T5 using the \"chain-of-thought\" (CoT) three-hop reasoning framework\n(THoR). We found that the results of the zero-shot approaches are similar to\nthe results achieved by baseline fine-tuned encoder-based transformers\n(BERT-base). Reasoning capabilities of the fine-tuned Flan-T5 models with THoR\nachieve at least 5% increment with the base-size model compared to the results\nof the zero-shot experiment. The best results of sentiment analysis on\nRuSentNE-2023 were achieved by fine-tuned Flan-T5-xl, which surpassed the\nresults of previous state-of-the-art transformer-based classifiers. Our CoT\napplication framework is publicly available:\nhttps://github.com/nicolay-r/Reasoning-for-Sentiment-Analysis-Framework",
        "updated": "2024-04-18 17:16:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12342v1"
    },
    {
        "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment",
        "authors": "Zhaofeng WuAnanth BalashankarYoon KimJacob EisensteinAhmad Beirami",
        "links": "http://arxiv.org/abs/2404.12318v1",
        "entry_id": "http://arxiv.org/abs/2404.12318v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12318v1",
        "summary": "Aligning language models (LMs) based on human-annotated preference data is a\ncrucial step in obtaining practical and performant LM-based systems. However,\nmultilingual human preference data are difficult to obtain at scale, making it\nchallenging to extend this framework to diverse languages. In this work, we\nevaluate a simple approach for zero-shot cross-lingual alignment, where a\nreward model is trained on preference data in one source language and directly\napplied to other target languages. On summarization and open-ended dialog\ngeneration, we show that this method is consistently successful under\ncomprehensive evaluation settings, including human evaluation: cross-lingually\naligned models are preferred by humans over unaligned models on up to >70% of\nevaluation instances. We moreover find that a different-language reward model\nsometimes yields better aligned models than a same-language reward model. We\nalso identify best practices when there is no language-specific data for even\nsupervised finetuning, another component in alignment.",
        "updated": "2024-04-18 16:52:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12318v1"
    }
]