[
    {
        "title": "Evaluating AI for Law: Bridging the Gap with Open-Source Solutions",
        "authors": "Rohan BhambhoriaSamuel DahanJonathan LiXiaodan Zhu",
        "links": "http://arxiv.org/abs/2404.12349v1",
        "entry_id": "http://arxiv.org/abs/2404.12349v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12349v1",
        "summary": "This study evaluates the performance of general-purpose AI, like ChatGPT, in\nlegal question-answering tasks, highlighting significant risks to legal\nprofessionals and clients. It suggests leveraging foundational models enhanced\nby domain-specific knowledge to overcome these issues. The paper advocates for\ncreating open-source legal AI systems to improve accuracy, transparency, and\nnarrative diversity, addressing general AI's shortcomings in legal contexts.",
        "updated": "2024-04-18 17:26:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12349v1"
    },
    {
        "title": "Large Language Models for Synthetic Participatory Planning of Shared Automated Electric Mobility Systems",
        "authors": "Jiangbo Yu",
        "links": "http://arxiv.org/abs/2404.12317v1",
        "entry_id": "http://arxiv.org/abs/2404.12317v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12317v1",
        "summary": "Unleashing the synergies of rapidly evolving mobility technologies in a\nmulti-stakeholder landscape presents unique challenges and opportunities for\naddressing urban transportation problems. This paper introduces a novel\nsynthetic participatory method, critically leveraging large language models\n(LLMs) to create digital avatars representing diverse stakeholders to plan\nshared automated electric mobility systems (SAEMS). These calibratable agents\ncollaboratively identify objectives, envision and evaluate SAEMS alternatives,\nand strategize implementation under risks and constraints. The results of a\nMontreal case study indicate that a structured and parameterized workflow\nprovides outputs with high controllability and comprehensiveness on an SAEMS\nplan than generated using a single LLM-enabled expert agent. Consequently, the\napproach provides a promising avenue for cost-efficiently improving the\ninclusivity and interpretability of multi-objective transportation planning,\nsuggesting a paradigm shift in how we envision and strategize for sustainable\nand equitable transportation systems.",
        "updated": "2024-04-18 16:51:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12317v1"
    },
    {
        "title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
        "authors": "Shreya ShankarJ. D. Zamfirescu-PereiraBjörn HartmannAditya G. ParameswaranIan Arawjo",
        "links": "http://arxiv.org/abs/2404.12272v1",
        "entry_id": "http://arxiv.org/abs/2404.12272v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12272v1",
        "summary": "Due to the cumbersome nature of human evaluation and limitations of\ncode-based evaluation, Large Language Models (LLMs) are increasingly being used\nto assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply\ninherit all the problems of the LLMs they evaluate, requiring further human\nvalidation. We present a mixed-initiative approach to ``validate the\nvalidators'' -- aligning LLM-generated evaluation functions (be it prompts or\ncode) with human requirements. Our interface, EvalGen, provides automated\nassistance to users in generating evaluation criteria and implementing\nassertions. While generating candidate implementations (Python functions, LLM\ngrader prompts), EvalGen asks humans to grade a subset of LLM outputs; this\nfeedback is used to select implementations that better align with user grades.\nA qualitative study finds overall support for EvalGen but underscores the\nsubjectivity and iterative process of alignment. In particular, we identify a\nphenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs,\nbut grading outputs helps users define criteria. What is more, some criteria\nappears \\emph{dependent} on the specific LLM outputs observed (rather than\nindependent criteria that can be defined \\emph{a priori}), raising serious\nquestions for approaches that assume the independence of evaluation from\nobservation of model outputs. We present our interface and implementation\ndetails, a comparison of our algorithm with a baseline approach, and\nimplications for the design of future LLM evaluation assistants.",
        "updated": "2024-04-18 15:45:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12272v1"
    },
    {
        "title": "Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM",
        "authors": "Michelle S. LamJanice TeohJames LandayJeffrey HeerMichael S. Bernstein",
        "links": "http://dx.doi.org/10.1145/3613904.3642830",
        "entry_id": "http://arxiv.org/abs/2404.12259v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12259v1",
        "summary": "Data analysts have long sought to turn unstructured text data into meaningful\nconcepts. Though common, topic modeling and clustering focus on lower-level\nkeywords and require significant interpretative work. We introduce concept\ninduction, a computational process that instead produces high-level concepts,\ndefined by explicit inclusion criteria, from unstructured text. For a dataset\nof toxic online comments, where a state-of-the-art BERTopic model outputs\n\"women, power, female,\" concept induction produces high-level concepts such as\n\"Criticism of traditional gender roles\" and \"Dismissal of women's concerns.\" We\npresent LLooM, a concept induction algorithm that leverages large language\nmodels to iteratively synthesize sampled text and propose human-interpretable\nconcepts of increasing generality. We then instantiate LLooM in a\nmixed-initiative text analysis tool, enabling analysts to shift their attention\nfrom interpreting topics to engaging in theory-driven analysis. Through\ntechnical evaluations and four analysis scenarios ranging from literature\nreview to content moderation, we find that LLooM's concepts improve upon the\nprior art of topic models in terms of quality and data coverage. In expert case\nstudies, LLooM helped researchers to uncover new insights even from familiar\ndatasets, for example by suggesting a previously unnoticed concept of attacks\non out-party stances in a political social media dataset.",
        "updated": "2024-04-18 15:26:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12259v1"
    },
    {
        "title": "E-Vote Your Conscience: Perceptions of Coercion and Vote Buying, and the Usability of Fake Credentials in Online Voting",
        "authors": "Louis-Henri MerinoAlaleh AzhirHaoqian ZhangSimone ColomboBernhard TellenbachVero Estrada-GaliñanesBryan Ford",
        "links": "http://arxiv.org/abs/2404.12075v1",
        "entry_id": "http://arxiv.org/abs/2404.12075v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12075v1",
        "summary": "Online voting is attractive for convenience and accessibility, but is more\nsusceptible to voter coercion and vote buying than in-person voting. One\nmitigation is to give voters fake voting credentials that they can yield to a\ncoercer. Fake credentials appear identical to real ones, but cast votes that\nare silently omitted from the final tally. An important unanswered question is\nhow ordinary voters perceive such a mitigation: whether they could understand\nand use fake credentials, and whether the coercion risks justify the costs of\nmitigation. We present the first systematic study of these questions, involving\n150 diverse individuals in Boston, Massachusetts. All participants \"registered\"\nand \"voted\" in a mock election: 120 were exposed to coercion resistance via\nfake credentials, the rest forming a control group. Of the 120 participants\nexposed to fake credentials, 96% understood their use. 53% reported that they\nwould create fake credentials in a real-world voting scenario, given the\nopportunity. 10% mistakenly voted with a fake credential, however. 22% reported\neither personal experience with or direct knowledge of coercion or vote-buying\nincidents. These latter participants rated the coercion-resistant system\nessentially as trustworthy as in-person voting via hand-marked paper ballots.\nOf the 150 total participants to use the system, 87% successfully created their\ncredentials without assistance; 83% both successfully created and properly used\ntheir credentials. Participants give a System Usability Scale score of 70.4,\nwhich is slightly above the industry's average score of 68. Our findings appear\nto support the importance of the coercion problem in general, and the promise\nof fake credentials as a possible mitigation, but user error rates remain an\nimportant usability challenge for future work.",
        "updated": "2024-04-18 10:57:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12075v1"
    }
]