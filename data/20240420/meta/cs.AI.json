[
    {
        "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
        "authors": "Xingyu FuYushi HuBangzheng LiYu FengHaoyu WangXudong LinDan RothNoah A. SmithWei-Chiu MaRanjay Krishna",
        "links": "http://arxiv.org/abs/2404.12390v1",
        "entry_id": "http://arxiv.org/abs/2404.12390v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12390v1",
        "summary": "We introduce Blink, a new benchmark for multimodal language models (LLMs)\nthat focuses on core visual perception abilities not found in other\nevaluations. Most of the Blink tasks can be solved by humans \"within a blink\"\n(e.g., relative depth estimation, visual correspondence, forensics detection,\nand multi-view reasoning). However, we find these perception-demanding tasks\ncast significant challenges for current multimodal LLMs because they resist\nmediation through natural language. Blink reformats 14 classic computer vision\ntasks into 3,807 multiple-choice questions, paired with single or multiple\nimages and visual prompting. While humans get 95.70% accuracy on average, Blink\nis surprisingly challenging for existing multimodal LLMs: even the\nbest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only\n13.17% and 7.63% higher than random guessing, indicating that such perception\nabilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also\nhighlights that specialist CV models could solve these problems much better,\nsuggesting potential pathways for future improvements. We believe Blink will\nstimulate the community to help multimodal LLMs catch up with human-level\nvisual perception.",
        "updated": "2024-04-18 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12390v1"
    },
    {
        "title": "Lazy Diffusion Transformer for Interactive Image Editing",
        "authors": "Yotam NitzanZongze WuRichard ZhangEli ShechtmanDaniel Cohen-OrTaesung ParkMichaël Gharbi",
        "links": "http://arxiv.org/abs/2404.12382v1",
        "entry_id": "http://arxiv.org/abs/2404.12382v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12382v1",
        "summary": "We introduce a novel diffusion transformer, LazyDiffusion, that generates\npartial image updates efficiently. Our approach targets interactive image\nediting applications in which, starting from a blank canvas or an image, a user\nspecifies a sequence of localized image modifications using binary masks and\ntext prompts. Our generator operates in two phases. First, a context encoder\nprocesses the current canvas and user mask to produce a compact global context\ntailored to the region to generate. Second, conditioned on this context, a\ndiffusion-based transformer decoder synthesizes the masked pixels in a \"lazy\"\nfashion, i.e., it only generates the masked region. This contrasts with\nprevious works that either regenerate the full canvas, wasting time and\ncomputation, or confine processing to a tight rectangular crop around the mask,\nignoring the global image context altogether. Our decoder's runtime scales with\nthe mask size, which is typically small, while our encoder introduces\nnegligible overhead. We demonstrate that our approach is competitive with\nstate-of-the-art inpainting methods in terms of quality and fidelity while\nproviding a 10x speedup for typical user interactions, where the editing mask\nrepresents 10% of the image.",
        "updated": "2024-04-18 17:59:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12382v1"
    },
    {
        "title": "6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction",
        "authors": "Théo GierucMarius KästingschäferSebastian BernhardMathieu Salzmann",
        "links": "http://arxiv.org/abs/2404.12378v1",
        "entry_id": "http://arxiv.org/abs/2404.12378v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12378v1",
        "summary": "Current 3D reconstruction techniques struggle to infer unbounded scenes from\na few images faithfully. Specifically, existing methods have high computational\ndemands, require detailed pose information, and cannot reconstruct occluded\nregions reliably. We introduce 6Img-to-3D, an efficient, scalable\ntransformer-based encoder-renderer method for single-shot image to 3D\nreconstruction. Our method outputs a 3D-consistent parameterized triplane from\nonly six outward-facing input images for large-scale, unbounded outdoor driving\nscenarios. We take a step towards resolving existing shortcomings by combining\ncontracted custom cross- and self-attention mechanisms for triplane\nparameterization, differentiable volume rendering, scene contraction, and image\nfeature projection. We showcase that six surround-view vehicle images from a\nsingle timestamp without global pose information are enough to reconstruct\n360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows,\nfor example, rendering third-person images and birds-eye views. Our code is\navailable at https://github.com/continental/6Img-to-3D, and more examples can\nbe found at our website here https://6Img-to-3D.GitHub.io/.",
        "updated": "2024-04-18 17:58:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12378v1"
    },
    {
        "title": "When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes",
        "authors": "Asaf YehudaiElron Bendel",
        "links": "http://arxiv.org/abs/2404.12365v1",
        "entry_id": "http://arxiv.org/abs/2404.12365v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12365v1",
        "summary": "We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.",
        "updated": "2024-04-18 17:48:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12365v1"
    },
    {
        "title": "Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI using Diffusion Models",
        "authors": "Trevor J. ChanChamith S. Rajapakse",
        "links": "http://arxiv.org/abs/2404.12361v1",
        "entry_id": "http://arxiv.org/abs/2404.12361v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12361v1",
        "summary": "Deep learning methods for accelerated MRI achieve state-of-the-art results\nbut largely ignore additional speedups possible with noncartesian sampling\ntrajectories. To address this gap, we created a generative diffusion\nmodel-based reconstruction algorithm for multi-coil highly undersampled spiral\nMRI. This model uses conditioning during training as well as frequency-based\nguidance to ensure consistency between images and measurements. Evaluated on\nretrospective data, we show high quality (structural similarity > 0.87) in\nreconstructed images with ultrafast scan times (0.02 seconds for a 2D image).\nWe use this algorithm to identify a set of optimal variable-density spiral\ntrajectories and show large improvements in image quality compared to\nconventional reconstruction using the non-uniform fast Fourier transform. By\ncombining efficient spiral sampling trajectories, multicoil imaging, and deep\nlearning reconstruction, these methods could enable the extremely high\nacceleration factors needed for real-time 3D imaging.",
        "updated": "2024-04-18 17:40:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12361v1"
    }
]