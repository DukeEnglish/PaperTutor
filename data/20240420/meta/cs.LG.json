[
    {
        "title": "On the Content Bias in Fréchet Video Distance",
        "authors": "Songwei GeAniruddha MahapatraGaurav ParmarJun-Yan ZhuJia-Bin Huang",
        "links": "http://arxiv.org/abs/2404.12391v1",
        "entry_id": "http://arxiv.org/abs/2404.12391v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12391v1",
        "summary": "Fr\\'echet Video Distance (FVD), a prominent metric for evaluating video\ngeneration models, is known to conflict with human perception occasionally. In\nthis paper, we aim to explore the extent of FVD's bias toward per-frame quality\nover temporal realism and identify its sources. We first quantify the FVD's\nsensitivity to the temporal axis by decoupling the frame and motion quality and\nfind that the FVD increases only slightly with large temporal corruption. We\nthen analyze the generated videos and show that via careful sampling from a\nlarge set of generated videos that do not contain motions, one can drastically\ndecrease FVD without improving the temporal quality. Both studies suggest FVD's\nbias towards the quality of individual frames. We further observe that the bias\ncan be attributed to the features extracted from a supervised video classifier\ntrained on the content-biased dataset. We show that FVD with features extracted\nfrom the recent large-scale self-supervised video models is less biased toward\nimage quality. Finally, we revisit a few real-world examples to validate our\nhypothesis.",
        "updated": "2024-04-18 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12391v1"
    },
    {
        "title": "SOHES: Self-supervised Open-world Hierarchical Entity Segmentation",
        "authors": "Shengcao CaoJiuxiang GuJason KuenHao TanRuiyi ZhangHandong ZhaoAni NenkovaLiang-Yan GuiTong SunYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2404.12386v1",
        "entry_id": "http://arxiv.org/abs/2404.12386v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12386v1",
        "summary": "Open-world entity segmentation, as an emerging computer vision task, aims at\nsegmenting entities in images without being restricted by pre-defined classes,\noffering impressive generalization capabilities on unseen images and concepts.\nDespite its promise, existing entity segmentation methods like Segment Anything\nModel (SAM) rely heavily on costly expert annotators. This work presents\nSelf-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel\napproach that eliminates the need for human annotations. SOHES operates in\nthree phases: self-exploration, self-instruction, and self-correction. Given a\npre-trained self-supervised representation, we produce abundant high-quality\npseudo-labels through visual feature clustering. Then, we train a segmentation\nmodel on the pseudo-labels, and rectify the noises in pseudo-labels via a\nteacher-student mutual-learning procedure. Beyond segmenting entities, SOHES\nalso captures their constituent parts, providing a hierarchical understanding\nof visual entities. Using raw images as the sole training data, our method\nachieves unprecedented performance in self-supervised open-world segmentation,\nmarking a significant milestone towards high-quality open-world entity\nsegmentation in the absence of human-annotated masks. Project page:\nhttps://SOHES.github.io.",
        "updated": "2024-04-18 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12386v1"
    },
    {
        "title": "6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction",
        "authors": "Théo GierucMarius KästingschäferSebastian BernhardMathieu Salzmann",
        "links": "http://arxiv.org/abs/2404.12378v1",
        "entry_id": "http://arxiv.org/abs/2404.12378v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12378v1",
        "summary": "Current 3D reconstruction techniques struggle to infer unbounded scenes from\na few images faithfully. Specifically, existing methods have high computational\ndemands, require detailed pose information, and cannot reconstruct occluded\nregions reliably. We introduce 6Img-to-3D, an efficient, scalable\ntransformer-based encoder-renderer method for single-shot image to 3D\nreconstruction. Our method outputs a 3D-consistent parameterized triplane from\nonly six outward-facing input images for large-scale, unbounded outdoor driving\nscenarios. We take a step towards resolving existing shortcomings by combining\ncontracted custom cross- and self-attention mechanisms for triplane\nparameterization, differentiable volume rendering, scene contraction, and image\nfeature projection. We showcase that six surround-view vehicle images from a\nsingle timestamp without global pose information are enough to reconstruct\n360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows,\nfor example, rendering third-person images and birds-eye views. Our code is\navailable at https://github.com/continental/6Img-to-3D, and more examples can\nbe found at our website here https://6Img-to-3D.GitHub.io/.",
        "updated": "2024-04-18 17:58:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12378v1"
    },
    {
        "title": "Matching the Statistical Query Lower Bound for k-sparse Parity Problems with Stochastic Gradient Descent",
        "authors": "Yiwen KouZixiang ChenQuanquan GuSham M. Kakade",
        "links": "http://arxiv.org/abs/2404.12376v1",
        "entry_id": "http://arxiv.org/abs/2404.12376v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12376v1",
        "summary": "The $k$-parity problem is a classical problem in computational complexity and\nalgorithmic theory, serving as a key benchmark for understanding computational\nclasses. In this paper, we solve the $k$-parity problem with stochastic\ngradient descent (SGD) on two-layer fully-connected neural networks. We\ndemonstrate that SGD can efficiently solve the $k$-sparse parity problem on a\n$d$-dimensional hypercube ($k\\le O(\\sqrt{d})$) with a sample complexity of\n$\\tilde{O}(d^{k-1})$ using $2^{\\Theta(k)}$ neurons, thus matching the\nestablished $\\Omega(d^{k})$ lower bounds of Statistical Query (SQ) models. Our\ntheoretical analysis begins by constructing a good neural network capable of\ncorrectly solving the $k$-parity problem. We then demonstrate how a trained\nneural network with SGD can effectively approximate this good network, solving\nthe $k$-parity problem with small statistical errors. Our theoretical results\nand findings are supported by empirical evidence, showcasing the efficiency and\nefficacy of our approach.",
        "updated": "2024-04-18 17:57:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12376v1"
    },
    {
        "title": "KDk: A Defense Mechanism Against Label Inference Attacks in Vertical Federated Learning",
        "authors": "Marco ArazziSerena NicolazzoAntonino Nocera",
        "links": "http://arxiv.org/abs/2404.12369v1",
        "entry_id": "http://arxiv.org/abs/2404.12369v1",
        "pdf_url": "http://arxiv.org/pdf/2404.12369v1",
        "summary": "Vertical Federated Learning (VFL) is a category of Federated Learning in\nwhich models are trained collaboratively among parties with vertically\npartitioned data. Typically, in a VFL scenario, the labels of the samples are\nkept private from all the parties except for the aggregating server, that is\nthe label owner. Nevertheless, recent works discovered that by exploiting\ngradient information returned by the server to bottom models, with the\nknowledge of only a small set of auxiliary labels on a very limited subset of\ntraining data points, an adversary can infer the private labels. These attacks\nare known as label inference attacks in VFL. In our work, we propose a novel\nframework called KDk, that combines Knowledge Distillation and k-anonymity to\nprovide a defense mechanism against potential label inference attacks in a VFL\nscenario. Through an exhaustive experimental campaign we demonstrate that by\napplying our approach, the performance of the analyzed label inference attacks\ndecreases consistently, even by more than 60%, maintaining the accuracy of the\nwhole VFL almost unaltered.",
        "updated": "2024-04-18 17:51:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.12369v1"
    }
]