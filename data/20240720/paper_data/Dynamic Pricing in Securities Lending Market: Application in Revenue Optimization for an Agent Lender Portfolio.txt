Dynamic Pricing in Securities Lending Market∗
Application in Revenue Optimization for an Agent Lender Portfolio
Jing Xu Yung Cheng Hsu William Biscarri
Quantitative Research Quantitative Research Quantitative Research
J.P. Morgan J.P. Morgan J.P. Morgan
New York, USA New York, USA New York, USA
jing.xu@chase.com yung-cheng.hsu@jpmorgan.com william.biscarri@jpmorgan.com
ABSTRACT book structure, such as cash equities [10, 11], futures [12], and
foreign exchange [13, 14].
Securities lending is an important part of the financial market
structure, where agent lenders help long term institutional investors One market that has seen comparatively less attention is the
to lend out their securities to short sellers in exchange for a lending securities lending (SL) market, which is an important, yet often
fee. Agent lenders within the market seek to optimize revenue by overlooked, component of the modern financial system. In the SL
lending out securities at the highest rate possible. Typically, this market, agent lenders which hold securities on behalf of customers
rate is set by hard-coded business rules or standard supervised compete with one another to lend out these securities to other
machine learning models. These approaches are often difficult to investors, who typically seek to borrow these securities for the
scale and are not adaptive to changing market conditions. Unlike a purposes of shorting. Much past work has focused on analyzing the
traditional stock exchange with a centralized limit order book, the SL market primarily from an auction or game theoretic perspective
securities lending market is organized similarly to an e-commerce [15, 16, 17].
marketplace, where agent lenders and borrowers can transact at any
The usual goal of agent lenders operating within the SL market is
agreed price in a bilateral fashion. This similarity suggests that the
to maximize the total amount of revenue received from lending out
use of typical methods for addressing dynamic pricing problems in
securities to borrowers. Since there is no difference between any
e-commerce could be effective in the securities lending market. We
given security borrowed from different agent lenders, borrowers
show that existing contextual bandit frameworks can be
have no preference for one agent lender over another except in the
successfully utilized in the securities lending market. Using offline
cost of borrowing being offered by each lender. Furthermore, the
evaluation on real historical data, we show that the contextual
number of available shares of any given security available to lend
bandit approach can consistently outperform typical approaches by
in the market often far surpasses the demand of that security to be
at least 15% in terms of total revenue generated.
borrowed. As a result, agent lenders must carefully set the cost of
borrowing the securities that they hold to optimize the total amount
KEYWORDS
of revenue they receive.
Reinforcement Learning, Contextual Bandit, Thompson Sampling,
Traditionally, agent lenders have used rule-based logic derived
Dynamic Pricing, Securities Lending, Financial Markets
from business intuition in combination with human judgement from
ACM Reference format: manual traders to set the rate for borrowing a security. While this
approach can be effective, the hard-coded rules are often static and
Jing Xu, Yung-Cheng Hsu and William Biscarri. 2024. Dynamic Pricing in
may reflect observations or patterns that were valid in the past, but
Securities Lending Market: Application in Revenue Optimization for an
Agent Lender Portfolio. In Proceedings of ACM International Conference which no longer hold. A seminal work which studies these issues
on AI in Finance (ICAIF ’24), November 14–16, 2024, New York, NY, USA. is presented by Duffie et al. [4], however, their approach relies on
8 pages. static beliefs and manual intervention by traders and is not
automated.
1 INTRODUCTION More recently, agent lenders have turned to building machine
learning based models to set lending rates. While more scalable and
A fundamental problem in quantitative finance is the design and
adaptive than rule-based approaches, there are still limitations. The
implementation of automated data-driven processes to operate
first is that there is no ground truth label available for training, as
within financial markets. A particular area of interest is automated
the “correct” lending rate is unobservable, which can make training
trading, where the goal is to create systems capable of interacting
a standard supervised model difficult. The second, and perhaps
with a market without human intervention. This problem has been,
most important, is that a fixed model still may not be able to adapt
and continues to be, extensively studied in multiple markets, but
to changing market conditions.
particular focus has been on markets with a traditional limit orderICAIF’23, November 28-29, 2023, New York, NY USA F. Surname et al.
The goal of this work is to explore the use of automated, dynamic, The borrower is required to provide sufficient collateral, in the form
and data-driven methods for optimizing the actions of an agent of either cash or securities, to compensate the fund if the borrower
lender in the SL market. Unlike the typically studied markets that fails to return the loaned securities in the agreed timeframe, subject
to certain counterparty and liquidity risks [2]. Figure 1 captures the
use a continuous-time double auction mechanism and limit order
key market participants and shows how the securities lending
book to handle order flows and execution in a centralized fashion,
process works.
the SL market is organized more like a standard e-commerce
platform where transactions occur between a buyer and seller
directly. Given this important difference, existing automated
trading approaches in the quantitative finance literature designed
for markets that follow a limit order book structure may not be
appropriate for the SL market. However, the similarities between
the SL market and e-commerce platforms hint that approaches for
dynamic pricing problems may be effective.
A well-studied and successful paradigm for approaching dynamic
pricing problems, is the contextual multi-armed bandit framework
[18, 19, 20]. In this framework, an agent observes context from its
environment and uses that context to sequentially make decisions.
Importantly, the agent quickly receives feedback on the efficacy of
its past decisions and can utilize that information when making Figure 2: Borrower sending multiple requests to single lender for
future decisions. In this paper, we demonstrate that a contextual the same security with different expiration schedule
bandit framework can be effectively applied by an agent lender in
the SL market to dynamically set lending rates and improve Agent lenders play an important role in the lending market. On one
revenue generation over standard methods. In section 2, we cover hand, they lend out securities on behalf of asset owners/managers
to create incremental alpha. On the other hand, they offer broker
details of the SL market. In section 3, we briefly review the
dealers a way to mitigate the consequence of frictions inherent to
standard contextual bandit framework. In section 4, we describe a
OTC markets at the cost of charging a lending fee [4]. To reduce
custom reward function to be used in the contextual bandit model.
such cost a borrower often reaches out to multiple lenders to
Finally, in section 5, we train the model offline using real historical
communicate interests for securities and borrow from the lender
demand data and demonstrate that the contextual bandit approach that offers the lowest fee, especially for hard-to-borrow names. The
can outperform traditional approaches used in the SL market. lending fee dependents heavily on market supply-demand
dynamics. From an agent lender’s perspective, suboptimal pricing
decisions lead to reduction in its own revenue and in some cases
2 BACKGROUND sends the wrong pricing signals to other agency lenders and distort
price in the market.
In this section, we provide a brief introduction to the securities
lending market and the market mechanisms as that result from its 2.2 Trading infrastructure and market mechanism
trading infrastructure.
Trading in the SL markets happens on marketplace platform like
the Next Generation Trading (NGT)1. On the supply side, lenders
actively update executable inventory by publish <Offer rate,
Inventory> on Target Availability (TA). On the demand side,
NGT’s Indication of Interest (IOI) allows borrowers to reach out
to multiple lenders for market discover in a non-exclusive fashion.
Borrowers can also send multiple requests to the same lender with
a different bid over time. Figure 2 shows an example of borrower
submitting multiple requests to one agent lender for the same
security throughout the day, all of which got rejected by the lender.
After IOI, borrowers then set up a pre-defined order of lenders that
Figure 1: Interactions between entities in the securities lending they want to transact with and allow NGT to route borrow requests
process. sequentially.
2.1 Securities Lending and Agent Lenders Trades are automatically accepted or rejected between two private
parties based on few key parameters: Lender’s <Offer rate,
Securities lending works by allowing a fund to temporarily lend Available inventory>, Borrower’s <Bid, Quantity requested>. A
securities that it owns to an approved borrower in return for a fee. request is accepted when bid and offer agrees conditional
1 NGT is a multi-asset trading platform available 24 hours a day for real-time securities
lending trading between lenders and borrowers. As of May 2023, $113.5bn notional
on average traded on NGT each day. https://equilend.com/services/ngt/.Dynamic Pricing for Securities Lending Market ICAIF’23, July 2023, New York, NY USA
availability, reject otherwise. Borrowers and lenders can access to • The algorithm then improves the action selection strategy
transaction history on NGT at a cost. Since there’s no centralized with this new observation. Note that the reward is only
clearing in the marketplace, there’s no notion of market clearing observed for chosen 𝑎 at any timestep t.
𝑡
lending fee. The access to market transaction history allows both
sides to find out where the market is trading at for price discovery2.
The CB algorithm is equipped with a set of policies Π ⊆{𝑋→𝐴},
which describe how actions are chosen given the observed context.
2.3 Properties of Good Pricing Strategy
The objective of a CB algorithm is to learn a policy π ∈ Π which
Realistic preference. A necessary condition for good pricing minimizes,
model is that reward preference assumption should truly reflect
optimization need of client/business. Poorly constructed
𝑇
assumptions could lead to misleading conclusions, sometimes 𝑅𝑒𝑔𝑟𝑒𝑡(𝜋)=∑ [𝑟(𝑥 ,𝑎∗)− 𝑟(𝑥 ,𝜋(𝑥 ))] ,
𝑡 𝑡 𝑡 𝑡 𝑡 𝑡
under the disguise of complicated structure. To evaluate the realism 𝑡=1
of the assumptions, we need check if the preference is
commensurable with the metrics that business cares about. For where
example, in SL market, the business prefers high acceptance rate of 𝜋(𝑥 )=𝑎𝑟𝑔max𝑟(𝑥 ,𝑎 )
𝑡 𝑡 𝑡 𝑡
orders flow and higher price conditional on acceptance. In Section 𝑎𝑡
4, we show that our preference function is designed to balance out
The optimal policy is thus:
hit rate vs ask price.
𝜋∗=𝑎𝑟𝑔min𝑅𝑒𝑔𝑟𝑒𝑡(𝜋)
𝜋
Responsiveness. Another desirable property of good pricing
strategy is the responsiveness to adapt to market dynamics: the 3.2 Related Work
model should include parameters that explicit captures the markets
dynamics allowing it to response to the change directly. For Dynamic pricing strategies for financial market has been
example, when an agent lender owns a dominating share of the extensively studied in the literature. Duffie et al. [4] use traditional
market, the agent lender has tremendous pricing power, the lending statistical methods to dynamically determine the price, fees, and the
fee tends to be more aggressive than when the agent lender has low interests rate in securities lending. The lending fee dynamics is
market share. In section 5, we provide empirical evidence that our driven by the difference in agent believes and bargaining. Their
reward function and market share feature can capture such pricing styled assumptions on believes are static and rarely hold in
power dynamic due to monopoly power. securities lending market as more than 80% of the transactions are
low touch transactions that settles without traders’ intervention.
3 CB FORMULATION & RELATED WORK
More recently, reinforcement learning (RL) has emerged as a
powerful paradigm for sequential decision-making problems,
In this section, we define the K-armed contextual bandit problem where an agent learns to interact with an environment to maximize
formally, and as an example, show how it can model the agent a cumulative reward. Khraishi et al. [5] applied an offline policy
lenders’ offer rate pricing problem. We then discuss related works evaluation onto dynamic pricing for consumer credit with the
and their limitations. classic Q-learning paradigm. Past work has showed evaluation
methods that are guaranteed to be unbiased (Li et al. 2011) or have
3.1 Contextual Bandit Formulation for an Agent Lender low bias (Dud´ık et al. 2012; 2014), but only assuming an infinite
In a competitive market, the revenue optimization problem for the data stream. Other work has focused on evaluators that perform
representative agent lender in the marketplace takes market demand well empirically but lack this unbiasedness (Mary, Preux, and Nicol
as given. Hence, it is safe to assume such lender is not likely to 2014). Li et al. [6] used contextual bandit techniques to make
have any systematic impact on aggregate demand. Following personalized recommendation with the reward function of article
previous work of Li et al. [6], we model the agent lender pricing click through rate. Most RL methods rely on predefined reward
problem as a bandit problem with contextual information. functions, such as immediate profit or transactional success rate.
Formally, a CB algorithm learn through repeated interaction over Khraishi et al. [3] took expected immediate profit as the reward for
T rounds. At each round t = 1, 2, … T: dynamically pricing. Such reward functions can be challenging to
define accurately and may not fully capture the nuances of the
• The algorithm observes the current market environment 𝑥 𝑡 ∈ field.
𝑋 where X is a set of market features the vector 𝑥
𝑡
summarizes information about market supply and demand
dynamics in our setting. 4 REWARD FUNCTION & EVALUATION
• Based on previous observed payoffs, the algorithm chooses
an action 𝑎 ∈𝐴, from a set of actions, A. In our setting ach In this Section, we define a bounded reward function for the agent
𝑡
lender and show that it can be broken down into intuitively
action corresponds to a price level. Given an action, 𝑎 , we
𝑡
explainable components. We then go over the evaluation
receive reward, 𝑟 .
𝑡,𝑎𝑡
methodology.
2 Market VWAF is the value weighted average lending fee of all outstand loans on
NGTICAIF’23, November 28-29, 2023, New York, NY USA F. Surname et al.
4.1 The Agent Lender’s Reward Function 0,𝑏𝑝(𝐵𝑖𝑑𝑠,𝑎𝑠)=0 𝒐𝒓 𝐵𝑖𝑑𝑠<𝛿𝐶
𝐼 (𝐵𝑖𝑑𝑠,𝑎𝑠,𝛿)= { 𝑡 𝑡 𝑡 (4)
𝑏 𝑡 𝑡 1, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒
To effectively use the contextual bandit framework, a sensible
reward metric is critical. Given the nature of the securities lending
market the reward function that is used should meet two criteria. where 𝛅 is a threshold multiplier and 𝐂 is a benchmark value to
First, it should prefer to lend out (or book) a security over not detect spoofing requests, which occurs when a borrower tries to
lending out a security. Second, if a security is lent out, it should book a trade with an extremely low 𝐁𝐢𝐝 to influence the market.
prefer to lend out the security at as high of a rate as possible. Booking Status can be estimated via any model capable of
Therefore, we define the reward as the product of two terms – producing binary predictions.
booking preference and booking practice – and refer to the overall
reward as revenue propensity. Thus, the overall reward for an action gives a context at time t is
𝐫(𝐱 ,𝐚∗) and is equal to Finally, our optimization problem is to
𝐭 𝐭 𝐭
minimize the total regret in term of expected revenue by multiply
Reward. In a nutshell the agent lender’s reward should prefer
to the market value of the loan to the lending fee:
booking over rejection and if a booking has been made, it should
prefer a higher lending fee. Hence, we define the reward as the 𝑅𝑒𝑔𝑟𝑒𝑡(𝜋)=∑𝑇 [𝑟(𝑥,𝑎∗)− 𝑟(𝑥,𝜋(𝑥))] ∗𝑀𝑎𝑟𝑘𝑒𝑡𝑉𝑎𝑙∗𝑎∗ (5)
𝑡=1 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡 𝑡
product of two parts and call it 𝑅𝑒𝑣𝑒𝑛𝑢𝑒 𝑃𝑟𝑜𝑝𝑒𝑛𝑠𝑖𝑡𝑦 ∈[0,1]:
𝜋∗=𝑎𝑟𝑔min𝑅𝑒𝑔𝑟𝑒𝑡(𝜋) (6)
𝜋
𝑅𝑒𝑣𝑒𝑛𝑢𝑒 𝑃𝑟𝑜𝑝𝑒𝑛𝑠𝑖𝑡𝑦=𝑟(𝑥 (𝐵𝑖𝑑𝑠),𝑎𝑠,𝛿) (1)
𝑡 𝑡 𝑡 𝑡 4.2 Offline Evaluation
𝑅𝑒𝑣𝑒𝑛𝑢𝑒 𝑃𝑟𝑜𝑝𝑒𝑛𝑠𝑖𝑡𝑦≡𝐵𝑜𝑜𝑘𝑖𝑛𝑔 𝑃𝑟𝑒𝑓∗𝐵𝑜𝑜𝑘𝑖𝑛𝑔 𝑆𝑡𝑎𝑡𝑢𝑠 (2)
Evaluation for problems with sequential interactions are always
tricky. In our case, we want to measure the cumulative reward for
Booking Preference is a continuous value bounded between 0
a given policy. The ideal situation is to have each policy test in an
and 1. It represents the preference of a match or no match for any
‘online’ on the trading platform, which is not feasible in our
request s.
research setting. Hence, we followed the unbiased offline
evaluation method in Li et al. [6] by using historical logged
Booking Preference ∈[0,1] is defined as:
transaction details. We built a simulator to model the bandit process
0, 𝑖𝑓 𝐵𝑖𝑑 𝑡𝑠< 𝑎 𝑡𝑠 from the logged transaction data, and then evaluated each policy
𝑏𝑝(𝐵𝑖𝑑 𝑡𝑠,𝑎 𝑡𝑠)= { 𝑎𝑡𝑠
, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒
(3)
using the following steps:
𝐵𝑖𝑑𝑡𝑠
1. Initialize the action vector and/or matrix for each action arm
where 𝐵𝑖𝑑𝑠 is the bid from the borrower and 𝑎𝑠 is the proposed 2. Observe a booking request and create the context vector
𝑡 𝑡 using the market supply-demand signals
lending fee from the lender. Intuitively, when there is no match
i.e., when 𝐵𝑖𝑑𝑠is below 𝑎𝑠, the booking preference is zero. When 3. Estimate the Booking Status or Revenue using different
a match a happ𝑡 ens, the bo𝑡 oking preference rewards an action 𝑎𝑠 strategies (elaboration on the strategies could be found
that is closer to 𝐵𝑖𝑑𝑠. Booking preference is intuitively 𝑡 below) by taking the context vector and action
𝑡 vectors/matrices
maximized when, 𝑎𝑠= 𝐵𝑖𝑑𝑠 , as this reflects a scenario in which
𝑡 𝑡 4. Choose the action arm that gives the highest reward (i.e.,
the full extent of the borrower’s willingness to pay has been met.
estimated revenue)
Figure 3 plots the value of booking preference for various level of
5. Update the parameters of the vectors/matrices of the action
𝑎𝑠 as a function of bid to illustrate the tradeoff between hit rate
𝑡 arm that gives the highest estimated revenue
and high booking preference value.
6. Keep looping through step 2, 3, 4, and 5 to balance
exploration and exploitation dynamically
In our experiment we tested 4 exploration/exploit strategies:
• Linear Upper Confidence Bound (LinUCB. With a direct
estimation on the Revenue.): We use the originally LinUCB
presented in Li et al [6], This method creates a confidence
interval around the estimated mean reward of an action and
chooses the action with the highest upper confidence bound.
In this strategy, the reward is just the revenue of a booking
request since there is no need to estimate the booking
preference.
Figure 3: Booking preference as a function of bid for various as • Regularized Logistic Regression [Algorithm 1]: This
t
strategy initializes a vector 𝛽 for each action arm 𝑎 to
Booking Status is binary variable reflecting a match or no
estimate a reward 𝑟̂(𝑥 ,𝑎), which is the Booking Status in
match. Booking Status ∈{0,1} is defined as follows: 𝑡 𝑡
this case, by applying a sigmoid operation on the context 𝑥
𝑡
and 𝛽, with the following variations:Dynamic Pricing for Securities Lending Market ICAIF’23, July 2023, New York, NY USA
• Upper Confidence Bound (LRUCB): Booking Status is 5.1.1 Data
estimated using a sigmoid function with a predefined
We trained our models with historical bid-level data from NGT
confidence bound factor.
autoborrow, so that each observation contains a timestamp,
• Thompson Sampling (LRTS): Booking Status is estimated by quantity, bid, and accept or reject status. Additional data for market
a normal sampling and a sigmoid function. features including market share, utilization, other source of supply,
• Epsilon Greedy (EG): Booking Status is estimated by solely borrower’s bid from the demand side is obtained separately from
one sigmoid function. other sources. For the scope the experiment, we focus on the U.S.
market and securities with a lending fee in the range from 1%- 10%
LRUCB, LRTS, and EG strategies use the same algorithm as the securities with very high lending fee will be manually trader
backbone [Algorithm 1] of logistic regression to estimate the value by the desk and the GC securities will be priced at 25 bp in general.
of Booking Status (for EG) or the mean value of Booking Status
(for LRUCB and LRTS). Once the booking status is estimated, 5.1.2 Contextual Feature Construction
we can estimate the reward of an action by multiplying it with the Our context vector consists of features describing market signals,
booking preference and the market value of a certain security. the agent lender’s information, and demand side information. We
use 4 market supply-demand signals and one demand feature
including Utilization ∈ [0, 1], Agent Lender Mkt Share, Alternative
5 EXPERIMENTS
Supply Signal ∈ [0, 1], Return signal ∈ [0, 1], BID signal scaled ∈
In this section, we use the offline evaluation method of Li et al [6] [0, 1].
to compare the 4 contextual bandit algorithms with 4 non-
• Utilization ∈ [0, 1] stands for the market level demand to
contextual bandit pricing strategies. We describe the experiment
supply ratio
setup briefly followed by discussion of results.
• Agent Lender Mkt Share ∈ [0, 1] is a proxy for the market
power of the agent lender
5.1 Experiment Setup
• Other Source of Supply ∈ [0, 1] is a proxy for the alternative
This subsection gives a detailed description of our experimental source of supply outside NGT
setup, including data, contextual feature construction, performance • Return over Notional ∈ [0, 1] is a proprietary demand signal
evaluation, and competing algorithms. for the agent lender
• BID EWMA Ratio scaled ∈ [0, 1] is a bid level feature used
to gauge borrower interest level
Algorithm 1 Regularized Logistic Regression Algorithm for
Reward Estimation
A good contextual feature should discriminate various market
Initialize 𝛽 (parameters of the estimated distribution of condition, i.e., we should expect to see reward ranking amongst
booking status) ∈ 𝑅5 to 𝑁(0,𝜆) and 𝛼 for LRUCB as the actions to vary over the range of our context. For example, in Figure
confidence bound factor for each action arm 6, we plot the average Booking Preference of 4 type of pricing
for t ∈ T do strategies in our action space vs 2 contextual features -market share
and other sources of supply. The top chart shows that the action
for all a ∈ 𝑨 do
𝑡 with highest booking preference varies across different market
𝑥 ← Observe a new context
𝑡 share levels, i.e., not a single action pareto dominates the rest. The
if using LRUCB
best action in mid-range (40-60%) might not be the best action in
𝑚←𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝛽,𝑥 𝑡) high market power situation (>70%). Moreover, it shows that our
𝑟̂ 𝑡(𝑥 𝑡,𝑎)←𝑈𝐶𝐵(𝑚,𝛼) booking preference penalizes pricing strategy (𝑃 𝑟𝑠 𝑢𝑙𝑒(𝑡) ) that is
end if overly aggressive, but less so when market share is > 85%, in which
else if using LRTS the agent lender will have monopoly pricing power.
𝑚←𝑇ℎ𝑜𝑚𝑝𝑠𝑜𝑛 𝑆𝑎𝑚𝑝𝑙𝑖𝑛𝑔
𝑟̂(𝑥 ,𝑎)←𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝑚)
𝑡 𝑡
end else if
else if using EG
𝑟̂(𝑥 ,𝑎)←𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝛽,𝑥 )
𝑡 𝑡 𝑡
end else if
end for
Choose the arm 𝑎 =max 𝑟̂(𝑥 ,𝑎)
𝑡 𝑡 𝑡
𝑎∈𝐴𝑡
if 𝑎 = the assigned action to the current booking request then
𝑡
𝑢𝑝𝑑𝑎𝑡𝑒 𝛽
end if
end forICAIF’23, November 28-29, 2023, New York, NY USA F. Surname et al.
Figure 4: Comparison of average Booking Preference of 4
actions given the context (x axis - market share (top), other
source of supply (bottom), y axis – Booking Preference)
5.2.1 Method Comparisons Table 1: Contextual bandit-based lending fee optimization
experiment: comparing the simulated revenue based on sample of
We compared contextual bandits-based policies with other non-CB
5-day NGT demand data for 8 distinct pricing strategies.
policies commonly used by agent lenders. We estimated revenue
Contextual bandit-based policies are denoted with a (*), while the
generated by 8 policies trained below.
traditional, non-contextual bandit-based approaches are denoted by
(**). (The anti-spoofing factor 𝛿𝐶 is set to 0.85 in all experiments)
Table 1 shows the revenue generated by each policy based on our
simulation using real demand data from 3 consecutive 5-day
periods April 2021 and another 3 from November 2021. The
following patterns can be seen quite consistently in our results:
Figure 5: Cardinal ranking of aggressiveness of 4 type of
pricing strategies that in the action space. The first, and most encouraging, is that the contextual bandit-based
methods almost uniformly outperform the traditional non-
I. Non-CB pricing policy. These are the pricing strategies that contextual bandit pricing policies. This is perhaps unsurprising, as
commonly used by major agent lenders. The strategies span our the contextual bandit approach can use past and current market
action space for CB setup: conditions to determine which pricing strategy is likely to be most
effective given the current environment.
• ML based supervised learning predictive pricing policy
• Agent Lender rule-based pricing policy
• Agent Lender’s existing book VWAF
• Market VWAF of all agent lenders
Figure 5 shows the cardinal ranking of aggressiveness of the 4
actions in our experiment, from low to high value are 𝑃𝑠 (𝑡)
𝑂𝑤𝑛 𝑣𝑤𝑎𝑓
, 𝑃𝑠 (𝑡) , 𝑃𝑠 (𝑡) and lastly 𝑃𝑠 (𝑡). The higher the
𝑀𝐿 𝑚𝑎𝑟𝑒𝑘𝑡 𝑣𝑤𝑎𝑓 𝑟𝑢𝑙𝑒
rate, the more aggressive the agent lender is in it’s pricing strategy.
Table 2: Number of times each action is selected as the best
II. CB pricing policies. action in 2023 May experiments.
• LinUCB The second is that the contextual bandit approaches provide insight
• LR UCB into which traditional approaches are most effective, and why.
• epsilon-greedy Table 2 shows a count of how often each pricing strategy was
• Thompson Sampling chosen by each contextual bandit method. From this, we can see
that the agent lender’s rule-based rate and ML based rate were
5.3 Results and Discussion selected most often. This likely occurs because rule-based rates are
usually lower than the Bid and the other rates are usually higher
To test each pricing strategy, we use 5-day worth of trading log data than the Bid. Hence, on average, there would be more match
from NGT. The first 4 days are used for training, and the last day between the Bid and the rule-based rates. On the other hand, when
is used for testing. We measure the total estimated revenue in a 5- the Bid are greater than ML based rates, the ML based rates would
day sliding window fashion (i.e., approximately the first 4 days will be very close to Bid, hence, under this scenario, picking ML based
be used for training and the last day will be used for testing), to test rates gives higher booking preference for individual incoming
the consistency of the results. booking requests, rendering higher total revenue.
Finally, among the contextual bandit frameworks we investigated,
the policies which utilized our custom reward function by
estimating the booking status using logistic regression performed
better than the framework which estimated revenue directly. ThisDynamic Pricing for Securities Lending Market ICAIF’23, July 2023, New York, NY USA
likely occurs because without incorporating the booking status, it is [6] Lihong Li, Wei Chu, John Langford, Robert E. Schapire. 2010. A contextual-
implicitly assumed that a price will be accepted, which is likely too bandit approach to personalized news article recommendation. WWW '10:
Proceedings of the 19th international conference on World wide web DOI:
aggressive of an assumption and thus leads to a disproportionate
https://doi.org/10.1145/1772690.1772758
amount of missed bookings. [7] Yuantong Li, Chi-hua Wang, Guang Cheng, Will Sun. 2022. Rate-Optimal
Contextual Online Matching Bandit. DOI:
https://doi.org/10.48550/arXiv.2205.03699
[8] Olivier Chapelle and Lihong Li. 2011. An empirical evaluation of thompson
6 CONCLUSIONS sampling. Advances in neural information processing systems, 24
[9] Aleksandrs Slivkins et al. 2019. Introduction to multi-armed bandits.
Foundations and Trends® in Machine Learning, 12(1-2):1–286.
In this paper we investigated the use of the contextual bandit
[10] Yuriy Nevmyvaka, Yi Feng, and Michael Kearns. 2006. Reinforcement
framework in improving the ability of an agent lending to optimize learning for optimized trade execution. Proceedings of the 23rd international
their pricing in the securities lending market. Using real historical conference on Machine learning - ICML (2006).
demand data from the securities lending market, we benchmarked DOI:https://doi.org/10.1145/1143844.1143929
[11] Zihao Zhang, Stefan Zohren, and Stephen Roberts. 2019. DeepLOB: Deep
the performance of optimal prices chosen by common contextual
Convolutional Neural Networks for Limit Order Books. IEEE Transactions on
bandit frameworks. We demonstrated that contextual bandit Signal Processing 67, 11 (2019), 3001-3012.
frameworks can be effectively used to choose between static prices DOI:https://doi.org/10.1109/tsp.2019.2907260
[12] Guhyuk Chung, Munki Chung, Yongjae Lee, and Woo Chang Kim. 2022.
recommended by supervised learning model as well as three logic-
Market Making under Order Stacking Framework: A Deep Reinforcement
based pricing rules which are commonly used in the industry. In Learning Approach. Proceedings of the Third ACM International Conference
our historical data-based simulation, we have shown that that our on AI in Finance (2022). DOI:https://doi.org/10.1145/3533271.3561789
contextual bandit framework can learn on historical data to [13] Antonio Riva, Lorenzo Bisi, Pierre Liotet, Luca Sabbioni, Edoardo Vittori,
Marco Pinciroli, Michele Trapletti, and Marcello Restelli. 2022. Addressing
recommend an optimal price, without the need of heavy model Non-Stationarity in FX Trading with Online Model Selection of Offline RL
assumption as in any supervised learning model. We also Experts. Proceedings of the Third ACM International Conference on AI in
demonstrated that our models improve previous state-of-art Finance (2022). DOI:https://doi.org/10.1145/3533271.3561780
[14] João Carapuço, Rui Neves, and Nuno Horta. 2018. Reinforcement learning
solutions, providing higher PnL than any single pricing strategies.
applied to Forex trading. Applied Soft Computing 73, (2018), 783-794.
We discussed the main advantages of the Contextual Bandit DOI:https://doi.org/10.1016/j.asoc.2018.09.017
framework, and we demonstrated that it is flexible enough to [15] Emily Diana, Michael Kearns, Seth Neel, and Aaron Roth. 2020. Optimal,
consider multiple pricing strategies at once. For the future work, we truthful, and private securities lending. Proceedings of the First ACM
International Conference on AI in Finance (2020).
would like to explore and improve the model performance on real
DOI:https://doi.org/10.1145/3383455.3422541
time demand data, which require a more sophisticated [16] Mahdi Nezafat and Mark Schroder. 2021. Private Information, Securities
consideration on infrastructure readiness in a production setting. Lending, and Asset Prices. The Review of Financial Studies 35, 2 (2021), 1009-
1063. DOI:https://doi.org/10.1093/rfs/hhab043
[17] Shuaiyu Chen, Ron Kaniel, and Christian C. Opp. 2022. Market Power in the
DISCLAIMER Securities Lending Market. SSRN Electronic Journal (2022).
DOI:https://doi.org/10.2139/ssrn.4100699
This paper was prepared for informational purposes in part by the [18] Jonas Mueller, Vasilis Syrgkanis, and Matt Taddy. 2019. Low-rank bandit
Quantitative Research group of JPMorgan Chase & Co. and its methods for high-dimensional dynamic pricing. Proceedings of the 33rd
International Conference on Neural Information Processing Systems. Curran
affiliates (“J.P. Morgan”) and is not a product of the Research
Associates Inc., Red Hook, NY, USA, Article 1386, 15468–15478.
Department of J.P. Morgan. J.P. Morgan makes no representation [19] Virag Shah, Jose Blanchet, and Ramesh Johari. 2019. Semi-parametric dynamic
and warranty whatsoever and disclaims all liability, for the contextual pricing. Proceedings of the 33rd International Conference on Neural
Information Processing Systems. Curran Associates Inc., Red Hook, NY, USA,
completeness, accuracy or reliability of the information contained Article 212, 2363–2373.
herein. This document is not intended as investment research or [20] Kanishka Misra, Eric M. Schwartz, and Jacob Abernethy. 2018. Dynamic
Online Pricing with Incomplete Information Using Multi-Armed Bandit
investment advice, or a recommendation, offer or solicitation for
Experiments. SSRN Electronic Journal (2018).
the purchase or sale of any security, financial instrument, financial DOI:https://doi.org/10.2139/ssrn.2981814
[21] Maxime C. Cohen, Ilan Lobel, and Renato Paes Leme. 2020. Feature-Based
product or service, or to be used in any way for evaluating the
Dynamic Pricing. Management Science 66, 11 (2020), 4921-4943. DOI:
merits of participating in any transaction, and shall not constitute a
https://doi.org/10.1287/mnsc.2019.3485
solicitation under any jurisdiction or to any person, if such
solicitation under such jurisdiction or to such person would be
unlawful.
REFERENCES
[1] Gary Gensler. 2021. Proposed Updates to Securities Lending Market.
https://www.sec.gov/news/statement/gensler-securities-lending-market-
20211118
[2] JP Morgan Asset management. 2023. https://am.jpmorgan.com/se/en/asset-
management/institutional/insights/securities-lending/
[3] Nathan Foley-Fisher, Stefan Gissler, Stéphane Verani. 2019. Over-the-counter
market liquidity and securities lending. (Review of Economic Dynamics
Volume 33) DOI: https://doi.org/10.1016/j.red.2019.02.005.
[4] Darrell Duffie, Nicolae Gârleanu, Lasse Heje Pedersen. 2002.
Securities lending, shorting, and pricing (Journal of Financial Economics).
https://www.sciencedirect.com/science/article/pii/S0304405X0200226X
[5] Raad Khraishi1, Ramin Okhrati. 2022. Offline Deep Reinforcement Learning
for Dynamic Pricing of Consumer Credit. ICAIF '22: Proceedings of the Third
ACM International Conference on AI in Finance. DOI:
https://doi.org/10.1145/3533271.3561682