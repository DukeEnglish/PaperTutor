1–28
Multi-Label Learning with Stronger Consistency Guarantees
AnqiMao AQMAO@CIMS.NYU.EDU
CourantInstituteofMathematicalSciences,NewYork
MehryarMohri MOHRI@GOOGLE.COM
GoogleResearchandCourantInstituteofMathematicalSciences,NewYork
YutaoZhong YUTAO@CIMS.NYU.EDU
CourantInstituteofMathematicalSciences,NewYork
Abstract
We presenta detailed study of surrogatelosses and algorithmsfor multi-labellearning, sup-
ported by H-consistency bounds. We first show that, for the simplest form of multi-label loss
(thepopularHammingloss), thewell-knownconsistentbinaryrelevancesurrogatesuffersfroma
sub-optimaldependencyon the numberof labels in terms of H-consistency bounds, when using
smooth losses such as logistic losses. Furthermore, this loss function fails to account for label
correlations. To address these drawbacks, we introducea novelsurrogateloss, multi-labellogis-
tic loss, that accounts for label correlations and benefits from label-independent H-consistency
bounds. We thenbroadenouranalysistocovera moreextensivefamilyofmulti-labellosses, in-
cluding all common ones and a new extension defined based on linear-fractional functions with
respect to the confusion matrix. We also extend our multi-label logistic losses to more compre-
hensivemulti-labelcomp-sumlosses,adaptingcomp-sumlossesfromstandardclassificationtothe
multi-label learning. We prove that this family of surrogate losses benefits from H-consistency
bounds,andthusBayes-consistency,acrossanygeneralmulti-labelloss. Ourworkthusproposes
a unified surrogate loss framework benefiting from strong consistency guarantees for any multi-
labelloss, significantlyexpandinguponpreviousworkwhichonlyestablishedBayes-consistency
and for specific loss functions. Additionally, we adapt constrained losses from standard classifi-
cation to multi-label constrained losses in a similar way, which also benefit from H-consistency
boundsandthusBayes-consistencyforanymulti-labelloss. Wefurtherdescribeefficientgradient
computationalgorithmsforminimizingthemulti-labellogisticloss.
1. Introduction
Supervised learning methods oftenassignasinglelabeltoeachinstance. However,real-world data
exhibits a more complex structure, with objects belonging to multiple categories simultaneously.
Consider avideo about sports training, which could be categorized asboth ‘health’ and ‘athletics,’
or a culinary blog post tagged with ‘cooking’ and ‘nutrition’. As a result, multi-label learning
(McCallum, 1999; Schapire and Singer, 2000) has become increasingly important, leading to the
developmentofvariousinterestingandeffectiveapproaches, predominantlyexperimentalinnature,
inrecentyears(ElisseeffandWeston,2001;Dengetal.,2011;PettersonandCaetano,2011;Kapoor
etal.,2012).
Although there is a rich literature on multi-label learning (see (Zhang and Zhou, 2013) and
(Bogatinovskietal.,2022)fordetailedsurveys),onlyafewstudiesfocusonthetheoreticalanalysis
of multi-label learning, particularly the study of the Bayes-consistency of surrogate losses (Zhang,
2004a,b;Bartlettetal.,2006;TewariandBartlett, 2007;Steinwart,2007).
© A.Mao,M.Mohri&Y.Zhong.
4202
luJ
81
]GL.sc[
1v64731.7042:viXraMAO MOHRIZHONG
Gao and Zhou (2011) initiated the study of Bayes-consistency in multi-label learning with re-
spect to Hamming loss and (partial) ranking loss. They provided negative results for ranking loss,
demonstrating thatnoconvexanddifferentiable pairwisesurrogatelossisBayes-consistent forthat
multi-label loss. Theyalso showed that thebinary relevance method, which learns an independent
binaryclassifierforeachofthellabels,isBayes-consistentwithrespecttotheHammingloss. Dem-
bczynski etal.(2011)furtherdemonstrated thatundertheassumption ofconditionally independent
labels, the binary relevance method is also Bayes-consistent with respect to the F measure loss.
β
However, they noted that it can perform arbitrarily poorly when this assumption does not hold.
Dembczynski et al. (2012) provided a positive result for the (partial) ranking loss by showing that
the simpler univariate variants of smooth surrogate losses are Bayes-consistent with respect to it.
Additionally, Zhang et al. (2020) proposed a family of Bayes-consistent surrogate losses for the
F measure by reducing the F learning problem to a set of binary class probability estimation
β β
problems. This approach was motivated by the consistent output coding scheme in (Ramaswamy
et al., 2014) for general multiclass problems. Other works have studied generalization bounds in
multi-label learning (Yu et al., 2014; Wydmuch et al., 2018; Wu and Zhu, 2020; Wu et al., 2021,
2023;Busa-Feketeetal.,2022).
Another related topic is the characterization of the Bayes classifier and corresponding Bayes-
consistentplug-inalgorithminmulti-labellearning. Thisincludesthecharacterization oftheBayes
classifier for subset 0/1 loss and Hamming loss in (Cheng et al., 2010) and the characterization
of the Bayes classifier for F measure in (Dembczynski et al., 2011). Dembczynski et al. (2013);
1
Waegeman et al. (2014) further extended the results in (Dembczynski et al., 2011) by designing
a Bayes-consistent plug-in algorithm for the F measure. Koyejo et al. (2015) characterized the
β
Bayesclassifierforgenerallinearfractionallosseswithrespecttotheconfusionmatrixanddesigned
the corresponding plug-in algorithms in the empirical utility maximization (EUM) framework. In
this framework, the measures are directly defined as functions of the population, in contrast to a
loss function that is defined as a function over a single instance in the decision theoretic analysis
(DTA)framework (Ye et al., 2012). Menon et al. (2019) studied the Bayes-consistency of various
reductionmethodswithrespecttoPrecision@κandRecall@κinmulti-labellearning. However,all
these publications only established Bayes-consistency for specific loss functions. Can we derive a
unifiedsurrogate lossframeworkthatisBayes-consistent foranymulti-label loss?
Furthermore, asAwasthietal. (2022a,b) pointed out,Bayes-consistency isanasymptotic guar-
anteeanddoesnotprovideconvergence guarantees. Italsoappliesonlytothefamilyofallmeasur-
ablefunctionsunliketherestrictedhypothesissetstypicallyusedinpractice. Instead,theyproposed
astronger guarantee known asH-consistency bounds, which are both non-asymptotic and account
for the hypothesis set while implying Bayes-consistency. These guarantees provide upper bounds
onthetarget estimation errorintermsofthesurrogate estimation error. Canweleveragethisstate-
of-the-art consistency guarantee whendesigning surrogatelossfunctions formulti-label learning?
Moreover, one of the main concerns in multi-label learning is label correlations (see (Dem-
bczyn´ski et al., 2012)). For the simplest form of multi-label loss, the popular Hamming loss, the
existing Bayes-consistent binary relevance surrogate failstoaccount forlabelcorrelations. Canwe
designconsistent lossfunctions thateffectivelyaccount forlabelcorrelations aswell?
OurContributions. This paper directly addresses these key questions in multi-label learning.
We present a detailed study of surrogate losses and algorithms for multi-label learning, supported
byH-consistency bounds.
2MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
InSection3,wefirstshow thatforthesimplest form ofmulti-label loss, thepopular Hamming
loss,thewell-knownconsistentbinaryrelevancesurrogate,whenusingsmoothlossessuchaslogis-
ticlosses,suffersfromasub-optimaldependencyonthenumberoflabelsintermsofH-consistency
bounds. Furthermore,thislossfunctionfailstoaccount forlabelcorrelations.
To address these drawbacks, weintroduce a novel surrogate loss, multi-label logistic loss, that
accounts for label correlations and benefits from label-independent H-consistency bounds (Sec-
tion4). Wethenbroadenouranalysistocoveramoreextensivefamilyofmulti-labellosses,includ-
ingallcommononesandanewextension defined based onlinear-fractional functions withrespect
totheconfusion matrix(Section5).
In Section 6, we also extend our multi-label logistic losses to more comprehensive multi-label
comp-sumlosses,adaptingcomp-sumlossesfromstandardclassificationtothemulti-labellearning.
Weprovethatthisfamilyofsurrogate losses benefitsfromH-consistency bounds, andthusBayes-
consistency, across any general multi-label loss. Our work thus proposes a unified surrogate loss
framework that is Bayes-consistent forany multi-label loss, significantly expanding upon previous
workwhichonlyestablished consistency forspecificlossfunctions.
Additionally,weadaptconstrainedlossesfromstandardclassificationtomulti-labelconstrained
losses inasimilarway,whichalsobenefitfromH-consistency bounds andthusBayes-consistency
foranymulti-labelloss(AppendixA).Wefurtherdescribeefficientgradientcomputationalgorithms
forminimizingthemulti-labellogistic loss(Section7).
2. Preliminaries
Multi-label learning. We consider the standard multi-label learning setting. Let X be the input
space and Y = {+1,−1}l the set of all possible labels or tags, where l is a finite number. For
example,Xcanbeasetofimages,andYcanbeasetoflpre-giventags(suchas’flowers’,’shoes’,
or ’books’) that can be associated with each image in the image tagging problem. Let n = ∣Y∣.
For any instance x ∈ X and its associated label y = (y ,...,y ) ∈ Y, if y = +1, we say that label
1 l i
i is relevant to x. Otherwise, it is not relevant. Let l = 1,...,l . Given a sample S drawn
i.i.d. according to some distribution D over X×Y, th[e]goal{of mult}i-label learning is to learn a
hypothesish∶X× l →Rtominimizethegeneralizationerrordefinedbyamulti-labellossfunction
L∶H ×X×Y→[R],
all
R h = E L h,x,y ,
L
( ) (x,y)∼D[ ( )]
where H is the family of all measurable hypotheses. For convenience, we abusively denote the
all
scoring vector by h x = h x,1 ,...,h x,l . Given a hypothesis set H ⊂ H , we denote by
all
R∗ H = inf R(h)the(b(est-in)-class e(rror.))We refer to the difference R h −R∗ H as the
L h∈H L L
esti(ma)tion error, wh(ic)h is termed the excess error when H = H . Letsgn∶t( ↦)1 −(1 ) be the
all t≥0 t<0
signfunction, andlett∶X→R beathreshold function. Thetargetlossfunction Lcanbetypically
+
givenbyafunction LmappingfromY×Ytorealnumbers:
L h,x,y =L h x ,y , (1)
( ) ( ( ) )
whereh x ∶= h x ,...,h x ∈Yisthepredictionfortheinputx∈Xandh x =sign h x,i −
1 l i
t x fo(r a)ny i[ ∈ (l ). As wit(h m)]any multi-label learning algorithms, such as b(ina)ry relev(an(ce, w)e
s(et)th)e threshold f[un]ction t x = 0. There are many multi-label loss functions, such as Hamming
loss,(partial)rankingloss,F( )andthemoregeneralF measureloss,subset0 1loss,precision@κ,
1 β
/
3MAO MOHRIZHONG
recall@κ, etc. (Zhang and Zhou, 2013). Among these, several loss functions are defined based
on the prediction of the hypothesis h x , while others are based on the scoring vector h x . We
will specifically consider the first typ(e o)f multi-label loss in the form given in (1), which(is)based
on some ‘distance’ between the prediction and the true label. This includes all the loss functions
previously mentioned (see Section 5 for a list of several common multi-label losses in this family)
butexcludesthe(partial) rankingloss,whichisdefinedbasedonpairwisescores. Forconvenience,
we may alternatively refer to L or its induced L as the multi-label loss. Without loss of generality,
weassumethatL∈ 0,1 , whichcanbeachievedthroughnormalization. WealsodenotebyL =
max
max y′,yL y′,y . O[uran]alysis isgeneralandadaptstoanymulti-label lossL.
Surro(gate )risk minimization and consistency. Minimizing the multi-label loss L directly is
computationally hard for most hypothesis sets because it is discrete and non-convex. A common
method involves minimizing a smooth surrogate loss function L∶H ×X×Y → R, which is the
all
̃
mainfocusofthispaper. Minimizing asurrogate lossdirectly leadstoanalgorithm formulti-label
learning. A desirable guarantee for the surrogate loss in multi-label learning is Bayes-consistency
(Zhang, 2004a,b; Bartlett et al., 2006; Tewari and Bartlett, 2007; Steinwart, 2007; Gao and Zhou,
2011). That is, minimizing the surrogate loss over the family of all measurable functions leads to
theminimizationofthemulti-labellossoverthesamefamily:
Definition1 AsurrogatelossLissaidtobeBayes-consistent withrespecttoamulti-labellossLif
̃
thefollowingholdsforanydistribution andallgivensequences ofhypotheses h ⊂H :
n n∈N all
{ }
lim R h −R∗ H =0 Ô⇒ lim R h −R∗ H =0 .
(n→+∞ ̃L ( n ) ̃L ( all ) ) (n→+∞ L ( n ) L ( all ) )
AspointedoutbyAwasthietal.(2022a,b)(seealso(LongandServedio,2013;ZhangandAgarwal,
2020; Mao etal., 2023d,c,a,b,e; Awasthi et al., 2023a,b; Mao et al., 2024a,b,c,f,g,e,d; Mohri et al.,
2024; Cortes et al., 2024)), Bayes-consistency is an asymptotic guarantee that cannot provide any
guaranteeforapproximateminimizers;italsoappliesonlytothefamilyofallmeasurablefunctions
anddoesnotconsiderthehypothesissetstypicallyusedinpractice. Instead,theyproposeastronger
guarantee known as H-consistency bounds, which are both non-asymptotic and dependent on the
hypothesis set, and imply Bayes-consistency when H = H . These guarantees provide upper
all
bounds on the target estimation error in terms of the surrogate estimation error. In the multi-label
learning scenario, theycanbeformulated asfollows:
Definition2 A surrogate loss L is said to admit an H-consistency bound with respect to a multi-
̃
labellossLifthefollowingconditionholdsforanydistribution andforallhypothesesh∈H,given
aconcave functionΓ∶R →R withΓ 0 =0:
+ +
( )
R h −R∗ H +M H ≤Γ R h −R∗ H +M H . (2)
L L L ̃L ̃L ̃L
( ) ( ) ( ) ( ( ) ( ) ( ))
Thequantities M H appearing inthe bounds arecalled minimizability gaps, whichmeasure the
̃L
difference between(the)best-in-class error and the expected best pointwise error for a loss function
Landahypothesis setH:
̃
M H =R∗ H −E inf E L h,x,y ≥0.
̃L ( ) ̃L ( ) x[h∈H(y∣x[̃( )])]
These are inherent quantities depending on the distribution and hypothesis set, which we cannot
hope to minimize. Since Γ is concave and Γ 0 = 0, Γ is sub-additive and an H-consistency
( )
4MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
bound(2)impliesthat: R h −R∗ H +M H ≤Γ R h −R∗ H +Γ M H . Therefore,
L L L ̃L ̃L ̃L
( ) ( ) ( ) ( ( ) ( )) ( ( ))
when the surrogate estimation error R h −R∗ H is minimized to ǫ, the target estimation
̃L ̃L
( ( ) ( ))
error R h −R∗ H is upper bounded by Γ ǫ +Γ M H . Theminimizability gaps vanish
L L ̃L
when(H =(H) or(in m))ore general realizable ca(se)s, suc(h as(wh)e)n R∗ H = R∗ H (Steinwart,
all ̃L ̃L all
2007; Awasthi et al., 2022b; Mao et al., 2023f). In these cases, H-co(nsi)stency(boun)ds imply the
H-consistency ofasurrogate lossLwithrespect toamulti-label loss L: R h −R∗ H ≤ǫ Ô⇒
R h − R∗ H ≤ Γ ǫ , for anỹ ǫ ≥ 0. The minimizability gap M H̃L ( i)s upp̃L e(r bo)unded by
L L ̃L
the( ap) proxim( ate) error( A) H =R∗ H −E inf E L h,x,,y ( ) andisgenerally afiner
quantity (Mao et al., 202̃L (3f).)ThũL s,(H)-consx is[tench y∈H ba oll u(ndy s∣x a[̃ re(more in)f]o)r]mative, more favorable,
andstronger thanexcesserrorbounds, andtheyimplythesebounds whenH=H .
all
Next, wewillstudy surrogate loss functions and algorithms formulti-label learning, supported
by H-consistency bounds, the state-of-the-art consistency guarantee for surrogate risk minimiza-
tion.
3. Existingconsistent surrogates fortheHamming loss
Inthesection, weconsider thesimplestformofmulti-label loss,theHammingloss,definedas:
l
∀ (h,x,y )∈H×X×Y, L ham (h,x,y )=L ham (h (x ),y ), whereL ham (y′,y )=∑ i=11 yi≠y i′.
Theexisting Bayes-consistent surrogate loss function istotransform themulti-label learning intol
independentbinaryclassificationtasks(GaoandZhou,2011),definedasforall h,x,y ∈H×X×Y,
( )
l
L
br
h,x,y =∑Φ y ih x,i ,
̃ ( ) i=1 ( ( ))
whereΦ∶R→R isabinarymargin-basedlossfunction,suchasthelogisticlossu↦log 1+e−u .
+
The algorithm that minimizes this surrogate loss is known as binary relevance (Zhang a(nd Zhou),
2013), which learns an independent binary classifier for each of the l labels. Gaoand Zhou (2011,
Theorem 15)showsthatL isBayes-consistent withrespect toL ifΦisBayes-consistent with
br ham
̃
respect toℓ 0−1∶ f,x,y ↦1 y≠sign(f(x)),the binary zero-one loss. Here, weproveastronger result
that L admits( an H-c) onsistency bound with respect to L with a functional form lΓ ⋅ if Φ
admĩ tsbr an H-consistency bounds with respect to ℓ with ham a functional form Γ ⋅ . Let(Fl )be a
0−1
hypothesis setconsistoffunctions mappingfromXtoR. ()
Theorem3 Let H = Fl. Assume that the following F-consistency bound holds in the binary
classification, forsomeconcavefunction Γ∶R→R +:
∀f ∈F, R f −R∗ F +M F ≤Γ R f −R∗ F +M F .
ℓ0−1 ℓ0−1 ℓ0−1 Φ Φ Φ
( ) ( ) ( ) ( ( ) ( ) ( ))
Then,thefollowingH-consistency boundholdsinthemulti-label learning: forallh∈H,
R h −R∗ H +M H
R
L ham
(h )−R∗
L
ham(H )+M
L ham
(H )≤lΓ⎛ ̃L br( ) ̃L br(
l
) ̃L br( )⎞.
⎝ ⎠
5MAO MOHRIZHONG
TheproofisincludedinAppendixB.WesaythatahypothesissetFiscompleteif f x ∶f ∈F =
R,∀x∈X. Thisnotionofcompletenessisbroadlyapplicableandholdsforcommonly{us(ed)hypoth}-
esissetsinpractice, including linear hypotheses, multi-layer feed-forward neural networks, andall
measurable functions. For such complete hypothesis sets F and with smooth functions Φ like the
logisticlossfunction,Γadmitsasquarerootdependencyinthebinaryclassification(Awasthietal.,
2022a;Maoetal.,2024g). Thus,byTheorem3,weobtainthefollowingresult.
Corollary 4 LetH=Fl. AssumethatFiscompleteandΦ u =log 1+e−u . Then,thefollowing
H-consistency boundholdsinthemulti-labellearning: for(all)h∈H,( )
1
R L ham (h )−R∗ L ham(H )+M L ham (H )≤l1 2 (R ̃L br(h )−R ̃∗ L br(H )+M ̃L br(H ))2.
1
Sincet↦t2 issub-additive, theright-hand sideoftheH-consistency boundinCorollary4can
1 1
be further upper bounded by l21 (R
̃L
br(h )−R ̃∗
L
br(H ))2 +l21 (M
̃L
br(H ))2. This implies that when
the estimation error of the surrogate loss L isreduced to ǫ, the corresponding estimation error of
br
̃
1
theHamminglossisupperboundedbyl21 ǫ1 2+l1 2 (M ̃L br(H ))2−M L ham (H ). Inthenearlyrealizable
caseswhereminimizability gapsarenegligible, thisupperboundapproximates to
R
L ham
(h )−R∗
L
ham(H )≤l21 ǫ1 2. (3)
Therefore, asthenumberoflabels lincreases, thebound becomeslessfavorable. Furthermore, the
loss function L clearly fails to account for the inherent correlations among labels. For instance,
br
̃
‘coffee’and’mug’aremorelikelytoco-occurthan‘coffee’and‘umbrella’. Additionally,L isonly
br
̃
Bayes-consistentwithrespecttotheHamminglossandcannotyieldrisk-minimizingpredictionsfor
other multi-label losses suchassubset 0 1loss orF -measure loss(Dembczyn´ski etal.,2012). To
β
addressthesedrawbacks, wewillintrodu/ce anewsurrogatelossinthenextsection.
4. Multi-labellogisticloss
Inthissection,wedefineanewsurrogatelossforHamminglossinmulti-labellearningthataccounts
forlabelcorrelationsandbenefitsfromlabel-independent H-consistencybounds. Thislossfunction
canbeviewedasageneralization ofthe(multinomial)logisticloss(Verhulst,1838,1845;Berkson,
1944, 1951), used in standard classification, to multi-label learning. Thus, we will refer to it as
multi-label logisticloss. Itisdefinedasfollows: forall h,x,y ∈H×X×Y,
( )
L log h,x,y = ∑ 1−L ham y′ ,y log⎛∑ e∑l i=1(y i′′−y i′ )h(x,i) ⎞.
̃ ( ) y′∈Y( ( )) y′′∈Y
⎝ ⎠
This formulation can be interpreted as a weighted logistic loss, where 1−L ⋅,y serves as a
ham
weight vector. Additionally, this formulation accounts for label correla(tions amo(ng t)h)e y s within
i
thelogarithmicfunction.
Thenextresultshowsthatthemulti-label logistic lossbenefits fromafavorable H-consistency
boundwithrespecttoL ,withoutdependency onthenumberoflabelsl. WeassumethatH=Fl
ham
andF iscomplete,conditions thattypically holdinpractice.
6MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
Theorem5 Let H = Fl. Assume that F is complete. Then, the following H-consistency bound
holdsinthemulti-label learning: forallh∈H,
1
R h −R∗ H +M H ≤2 R h −R∗ H +M H 2 .
L ham ( ) L ham( ) L ham ( ) ( ̃L log( ) ̃L log( ) ̃L log( ))
1
Since t ↦ t2 is sub-additive, the right-hand side of the H-consistency bound in Theorem 5
1 1
can be further upper bounded by 2 R h −R∗ H 2 +2 M H 2. This implies that
( ̃L log( ) ̃L log( )) ( ̃L log( ))
whentheestimationerrorofthesurrogatelossL isreduceduptoǫ,thecorresponding estimation
log
̃
1
error of the Hamming loss is upper bounded by 2ǫ1 2 +2 (M ̃L log(H ))2 −M L ham (H ). In the nearly
realizable caseswhereminimizability gapsarenegligible, thisupperboundapproximates to
R
L ham
(h )−R∗
L
ham(H )≤2ǫ1 2. (4)
Therefore, the bound is independent of the number of labels l. This contrasts with the bound for
1
L
br
shownin(3),wherealabel-dependent factorℓ2 replacestheconstantfactor2,makingitsignif-
̃
icantlylessfavorable.
The proof of Theorem 5 is included in Appendix C.2. We first present a general tool (Theo-
rem14)inAppendixC.1,whichshowsthattoderiveH-consistency bounds inmulti-labellearning
with a concave function Γ, it is only necessary to upper bound the conditional regret of the target
multi-labellossbythatofthesurrogatelosswiththesameΓ. ThisgeneralizesAwasthietal.(2022b,
Theorem 2)instandard multi-class classification tomulti-label learning. Next,wecharacterize the
conditional regret ofthetarget multi-label loss, such asHammingloss, in Lemma15found inAp-
pendix C.1, under the given assumption. By using Lemma 15, we upper bound the conditional
regretofL
ham
bythatofthesurrogate lossL
log
withaconcavefunction Γ t =2√t.
When H = H , minimizability gaps̃ M H and M H van(is)h, Theorem 5 implies
all ̃L log( ) L ham ( )
excesserrorbound andBayes-consistency ofmulti-label logistic losswithrespect totheHamming
loss.
Corollary 6 Thefollowingexcesserrorboundholdsinthemulti-labellearning: forallh∈H ,
all
1
R h −R∗ H ≤2 R h −R∗ H 2 .
L ham ( ) L ham( all ) ( ̃L log( ) ̃L log( all ))
Moreover, L isBayes-consistent withrespecttoL .
log ham
̃
It is known that L is only Bayes-consistent with respect to the Hamming loss and can be
br
̃
arbitrarily bad for other multi-label losses such as F -measure loss (Dembczynski et al., 2011).
β
Instead, we will show in the following section that our surrogate loss L adapts to and is Bayes-
log
̃
consistent withrespecttoanextensivefamilyofmulti-labellosses, including theF measureloss.
β
5. Extension: general multi-labellosses
In this section, we broaden our analysis to cover a more extensive family of multi-label losses,
including all common ones and a new extension defined based on linear-fractional functions with
7MAO MOHRIZHONG
respect tothe confusion matrix. Notethat several loss functions aredefined overthespace 0,1 l ,
rather than +1,−1 l. To accommodate this difference, any pair y,y′ ∈ Y = +1,−1 l{can }be
projected on{ to 0,1}l bylettingy = y+1 andy′ = y′+1,where1∈Rl isthevector{ withall} elements
2 2
equalto1. Seve{ralc}ommonmulti-label lossesaredefinedasfollows.
H Fa -m mm eain sug rl eos los: ssL
:(
Ly′ y,y
′,)
y=∑ =l i 1=1 −1 y (i 1≠ +y βi′.
2)y′⋅y
.
β
( )
β2∥y∥1+∥y′
∥1
JS au cb cs ae rt d0
/
d1 isl to as ns c: eL
:(
Ly′, yy
′,)
y=m =1ax −i∈ [l ]1 y i′ y≠ ′y ⋅yi.
y + y′ −y′⋅y
( ) 1 1
Precision@κ: L y′,y = 1− κ1 ∑∥ i∈T∥ y′∥ 1∥ y i=1 subject to y′ ∈ Y κ, where Y κ = y ∈Y∶ y 1 =κ
andT y′ = i∈ l ∶(y′ =1) . ( ) { ∥ ∥ }
i
andR Te( c ya ′) ll@ ={ κ i: ∈L[
(
ly] ∶′, yy
′)
== 1} 1 .− ∥y1
∥1
∑ i∈T (y′ )1 y i=1 subject to y′ ∈ Y κ, where Y κ = {y ∈Y∶ ∥y ∥1 =κ
}
i
M( ore) gen{ eral[ ly] , we ca} n define a multi-label loss based on true positives (TP), true negatives
(TN),falsepositives(FP)andfalsenegatives(FN),whichcanbewrittenexplicitly asfollows:
TP=y′⋅y TN= y −y′⋅y
1
∥ ∥
FP= y′ −y′⋅y, FN=l+y′⋅y− y − y′
1 1 1
∥ ∥ ∥ ∥ ∥ ∥
Similar to (Koyejo et al., 2014, 2015), we now define a general family of multi-label losses as
linear-fractional functions intermsofthesefourquantities:
a +a TP+a FP+a FN+a TN
′ 0 11 10 01 00
L y ,y = . (5)
( ) b 0+b 11TP+b 10FP+b 01FN+b 00TN
It can be shown that the aforementioned Hamming loss, F -measure loss, Jaccard distance, preci-
β
sion and recall all belong to this family. Note that the previous definitions in (Koyejo et al., 2014,
2015) arewithin theempirical utility maximization (EUM)framework (Yeetal., 2012), where the
measures are directly defined as functions of the population. We generalize their definition to the
decision theoretic analysis(DTA)framework,intermsoflossfunctions definedovery andy′.
Moreover, we can consider extending multi-label losses (5) to non-linear fractional functions
of these four quantities, or more generally, toany other forms, aslong as they are defined over the
spaceY×Y.
Another important familyofmulti-label losses isthetreedistance loss, usedincases ofhierar-
chical classes. In many practical applications, the class labels exist within a predefined hierarchy.
Forexample,intheimagetaggingproblem,classlabelsmightincludebroadcategoriessuchas‘an-
imals’or‘vehicles’, whichfurther subdivide into morespecific classes like ‘mammals’and ‘birds’
for animals, or ‘cars’ and ‘trucks’ for vehicles. Each of these subcategories can be divided even
further, showcasing aclearhierarchical structure.
Tree distance: Let T = Y,E,W be a tree over the label space Y, with edge set E and
positive, finite edge lengths sp(ecified by)W. Suppose r ∈ Y is designated as the root node. Then,
L y′,y =theshortest pathlengthinT betweeny andy′.
T
( )
Despite the widespread use of hierarchical classes in practice, to our knowledge, no Bayes-
consistent surrogate has been proposed for the tree distance loss in multi-label learning. Next,
we will show that our multi-label logistic loss can accommodate all these different loss functions,
8MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
includingthetreedistanceloss. Foranygeneralmulti-labellossL,wedefinethemulti-labellogistic
lossasfollows:
∀ (h,x,y
)
∈H×X×Y, ̃L log (h,x,y )= y∑ ′∈Y(1−L (y′ ,y ))log⎛ y∑ ′′∈Ye∑l i=1 (y i′′−y i′ )h (x,i )⎞. (6)
⎝ ⎠
Here, L can be chosen as all the multi-label losses mentioned above. Next, we will show that
L benefits from H-consistency bounds and Bayes consistency with respect to any of these loss
log
̃
functions.
Theorem7 Let H = Fl. Assume that F is complete. Then, the following H-consistency bound
holdsinthemulti-label learning:
1
∀h∈H, R h −R∗ H +M H ≤2 R h −R∗ H +M H 2 .
L ( ) L ( ) L ( ) ( ̃L log( ) ̃L log( ) ̃L log( ))
TheproofofTheorem7isbasicallythesameasthatofTheorem5,moduloreplacingtheHam-
minglossL withageneral multi-label lossL. Weinclude itinAppendix C.3forcompleteness.
ham
When H = H , minimizability gaps M H and M H vanish, Theorem 5 implies excess
all L L
̃log( ) ( )
errorboundandBayes-consistency ofmulti-label logisticlosswithrespecttoanymulti-label loss.
Corollary 8 Thefollowingexcesserrorboundholdsinthemulti-label learning: forallh∈H ,
all
1
R h −R∗ H ≤2 R h −R∗ H 2 .
L ( ) L ( all ) ( ̃L log( ) ̃L log( all ))
Moreover, L isBayes-consistent withrespecttoL.
log
̃
Corollary 8isremarkable, asitdemonstrates thataunified surrogate loss, L ,isBayes-consistent
log
̃
for any multi-label loss, significantly expanding upon previous work which only established con-
sistency for specific loss functions. Furthermore, Theorem 7 provides a stronger guarantee than
Bayes-consistency, whichisbothnon-asymptotic andspecifictothehypothesis setused.
Minimizing the multi-label logistic loss directly leads to the effective algorithm in multi-label
learning. Wefurtherdiscusstheefficiencyandpracticality ofthisalgorithm inSection7,wherewe
describe efficientgradient computation.
6. Extension: multi-label comp-sum losses
Inthissection, wefurther extend ourmulti-label logistic losses tomorecomprehensive multi-label
comp-sum losses, adapting comp-sum losses (Mao etal., 2023f) from standard classification to the
multi-label learning. As shown by Mao et al. (2023f), comp-sum loss is defined via a composi-
tion of the function Ψ and a sum, and includes the logistic loss (Ψ u = log u ) (Verhulst, 1838,
1845; Berkson, 1944, 1951), thesum-exponential loss (Ψ u =u−(1))(Westo(n a)ndWatkins, 1998;
Awasthi et al., 2022b), the generalized cross-entropy loss((Ψ) u = 1 1− 1 ,q ∈ 0,1 ) (Zhang
q uq
( ) ( ) ( )
andSabuncu,2018),andthemeanabsoluteerrorloss(Ψ u =1−1)(Ghoshetal.,2017)asspecial
u
cases. ( )
9MAO MOHRIZHONG
Givenanymulti-label lossL,wewilldefineournovelmulti-label comp-sumlossesasfollows:
∀ (h,x,y )∈H×X×Y, ̃L comp (h,x,y )= y∑ ′∈Y(1−L (y′ ,y ))Ψ⎛ y∑ ′′∈Ye∑l i=1 (y i′′−y i′ )h (x,i )⎞. (7)
⎝ ⎠
Thisformulationcanbeinterpretedasaweightedcomp-sumloss,where 1−L ⋅,y servesas
ham
aweightvector. Additionally, thisformulation accounts forlabelcorrelati(ons amon(gth)e)y swithin
i
the function Ψ. Next, we prove that this family of surrogate losses benefits from H-consistency
bounds, andthusBayes-consistency, acrossanygeneralmulti-label loss.
Theorem9 Let H = Fl. Assume that F is complete. Then, the following H-consistency bound
holdsinthemulti-label learning:
∀h∈H, R h −R∗ H +M H ≤Γ R h −R∗ H +M H ,
L
( )
L
( )
L
( ) (
̃Lcomp(
)
̃Lcomp(
)
̃Lcomp(
))
whereΓ t =2√twhenΨ u =log u oru−1;Γ t =2√nqtwhenΨ u = 1 1− 1 ,q ∈ 0,1 ;
q uq
( ) ( ) ( ) ( ) ( ) ( ) ( )
andΓ t =ntwhenΨ u =1− 1.
u
( ) ( )
Corollary 10 Thefollowingexcesserrorboundholdsinthemulti-label learning:
∀h∈H , R h −R∗ H ≤Γ R h −R∗ H ,
all L
( )
L
(
all
) (
̃Lcomp(
)
̃Lcomp( all
))
whereΓ t =2√twhenΨ u =log u oru−1;Γ t =2√nqtwhenΨ u = 1 1− 1 ,q ∈ 0,1 ;
q uq
( ) ( ) ( ) ( ) ( ) ( ) ( )
and Γ t = nt when Ψ u = 1− 1. Moreover, L with these choices of Ψ are Bayes-consistent
withre(sp)ecttoL. ( )
u ̃comp
The proof of Theorem 9 is included in Appendix C.4. Similar to the proof of Theorem 7, we
makeuseofTheorem14andLemma15inAppendixC.1. However,upperboundingtheconditional
regretofLbythatofthesurrogate lossL fordifferent choices ofΨrequires adistinct analysis
comp
̃
dependingonthespecificformofthefunctionΨ,leadingtovariousconcavefunctionsΓ. Ourproof
isinspiredbytheproofofH-consistencyboundsforcomp-sumlossesin(Maoetal.,2023f)through
theintroduction ofaparameterµandoptimization. However,thenoveltyliesintheadaptationofµ
withaquantity stailoredtomulti-labellossfunctions insteadofthescorevectorhitself.
Note that, as with Ψ u = log u shown in Section 5, for Ψ u = u−1, the bounds are also
independentofthenumb(er)oflabel(sa)ndarefavorable. However,f(or)otherchoicesofΨ,thebounds
exhibitaworsedependency onn,whichcanbeexponential withrespect tol.
In Appendix A, we introduce another novel family of surrogate losses, adapting constrained
losses (Lee et al., 2004) from standard classification to multi-label constrained losses in a similar
way.
7. Efficient Gradient Computation
In this section, we demonstrate the efficient computation of the gradient for the multi-label logis-
tic loss L at any point xj,yj . This loss function is therefore both theoretically grounded in
log
̃ ( )
10MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
H-consistency bounds and computationally efficient. Consider the labeled pair xj,yj and a hy-
pothesis hinH. Theexpression forL h,xj,yj canbereformulated asfollow(s: )
log
̃ ( )
̃L log (h,xj,yj )= y∑ ′∈Y(1−L (y′ ,yj ))log⎛ y∑ ′′∈Ye∑l i=1 (y i′′−y i′ )h (xj,i )⎞
⎝ ⎠
l
=− ∑ 1−L y′ ,yj ∑y i′ h xj,i + ∑ 1−L y′ ,yj log⎛∑e∑l i=1yih (xj,i )⎞.
y′∈Y( ( ))i=1 ( ) y′∈Y( ( )) y∈Y
⎝ ⎠
Let L 1 j = ∑ y∈Y 1−L y,yj , which is independent of h and can be pre-computed. It can also
be inva(ria)nt withr(espect(to j a)n)d isafixed constant for manyloss functions such as theHamming
loss. Next, wewill consider the hypothesis set of linear functions H = x↦w⋅Ψ x,i ∶w ∈Rd ,
whereΨisafeaturemappingfromX× l toRd. Usingtheshorthand w{ forh,we( canr) ewriteL }
log
at xj,yj asfollows: [ ] ̃
( )
l
̃L log (w,xj,yj )=−w⋅⎡ ⎢
⎢
⎢y∑ ′∈Y(1−L (y′ ,yj ))(i∑ =1y i′ Ψ (xj,i ))⎤ ⎥
⎥
⎥+L 1 (j )log (Z w,j ), (8)
⎢ ⎥
⎣ ⎦
whereZ
w,j
=∑ y∈Yew⋅ (∑l i=1yiΨ (xj,i )). Then,wecancomputethegradient of ̃L
log
atanyw∈Rd:
∇ ̃L log (w )=− y∑ ′∈Y(1−L (y′ ,yj ))(∑ i=l 1y i′ Ψ (xj,i ))+L 1 (j )y∑ ∈Yew⋅ (∑l i= Z1 wyi ,Ψ j(xj,i )) (i∑ =l 1y iΨ (xj,i
))
l l
=− y∑ ′∈Y(1−L (y′ ,yj ))(∑ i=1y i′ Ψ (xj,i ))+L 1 (j )y∼E qw[(i∑ =1y iΨ (xj,i ))], (9)
where q is a distribution over Y with probability mass function q y =
ew ⋅(∑l i=1yiΨ(xj,i)).
By
w w
( )
Zw,j
rearranging thetermsin(9),weobtainthefollowingresult.
Lemma11 ThegradientofL atanyw∈Rd canbeexpressed asfollows:
log
̃
l l
∇L
log
w =∑Ψ xj,i L
2
i,j +L
1
j ∑Ψ xj,i Q
w
i
̃ ( ) i=1 ( ) ( ) ( )i=1 ( ) ( )
where L 2 i,j = ∑ y∈Y 1−L y,yj y i, L 1 j = ∑ y∈Y 1−L y,yj , Q w i = ∑ y∈Yq w y y i,
q w (y
)
=( ew ⋅() ∑l i= Z1 wyi ,jΨ(xj,i( )), an( d Z w,) j) = ∑ y∈( Ye) w⋅ (∑l i=1yiΨ( (xj,i ))( . The)) overal( l ) time complexi( ty) for
gradientcomputation isO l .
( )
Here, the evaluation of L i,j , i ∈ l and L j can be computed once and for all, before any
2 1
gradient computation. For(evalu)ation[of]Q i ,(no)tethatitcanbeequivalently writtenasfollows:
w
( )
ew⋅Ψ xj,y l
Q w i = ∑ ̃( ) y i, withΨ xj,y =∑y iΨ xj,i ,
( ) y∈Y∑ y∈Yew⋅Ψ ̃(xj,y
)
̃( ) i=1 ( )
whereΨ xj,y admitsaMarkovianpropertyoforder1(ManningandSchutze,1999;Cortesetal.,
2016).̃ T(hus,as)shownbyCortesetal.(2016,2018),Q i canbeevaluatedefficientlybyrunning
w
( )
11MAO MOHRIZHONG
twosingle-source shortest-distance algorithmsoverthe +,× semiringonanappropriate weighted
finiteautomaton(WFA).Morespecifically, inourcase,(theW)FAcanbedescribedasfollows: there
are l+1 verticeslabeled0,...,l. Therearetwotransitions fromkto k+1 labeledwith+1and
−1.(Thew)eight ofthe transition withlabel +1isexp +w⋅Ψ xj,k , a(nd ex)p −w⋅Ψ xj,k for
the other. 0 is the initial state, and l the final state. T(he oṽ er(all tim)e) complexi(ty of c̃ o(mputi)n)g all
quantities Q i ,i∈ l ,isO l .
w
( ) [ ] ( )
8. Conclusion
We presented a comprehensive analysis of surrogate losses for multi-label learning, establishing
strong consistency guarantees. We introduced a novel multi-label logistic loss that addresses the
shortcomings ofexisting methodsandenjoys label-independent consistency bounds. Ourproposed
family of multi-label comp-sum losses offers a unified framework with strong consistency guar-
antees for any general multi-label loss, significantly expanding upon previous work. Additionally,
we presented efficient algorithms for their gradient computation. This unified framework holds
promise forbroader applications andopens newavenues forfutureresearch inmulti-label learning
andrelatedareas.
References
PranjalAwasthi,AnqiMao,MehryarMohri,andYutaoZhong. H-consistencyboundsforsurrogate
lossminimizers. InInternational ConferenceonMachineLearning, 2022a.
PranjalAwasthi,AnqiMao,MehryarMohri,andYutaoZhong. Multi-class H-consistency bounds.
InAdvancesinneuralinformation processing systems,pages782–795, 2022b.
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. Theoretically grounded loss func-
tionsandalgorithms foradversarial robustness. InInternational Conference onArtificial Intelli-
genceandStatistics, pages10077–10094, 2023a.
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. DC-programming for neural net-
workoptimizations. JournalofGlobalOptimization, 2023b.
Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk
bounds. JournaloftheAmericanStatistical Association, 101(473):138–156, 2006.
JosephBerkson. Applicationofthelogisticfunctiontobio-assay. JournaloftheAmericanStatisti-
calAssociation, 39:357—-365, 1944.
JosephBerkson. WhyIpreferlogitstoprobits. Biometrics,7(4):327—-339, 1951.
Jasmin Bogatinovski, Ljupcˇo Todorovski, Sasˇo Dzˇeroski, and Dragi Kocev. Comprehensive com-
parative study of multi-label classification methods. Expert Systems with Applications, 203:
117215, 2022.
Ro´bert Busa-Fekete, Heejin Choi, Krzysztof Dembczynski, Claudio Gentile, Henry Reeve, and
BalazsSzorenyi. Regretboundsformultilabelclassificationinsparselabelregimes. InAdvances
inNeuralInformation Processing Systems,pages5404–5416, 2022.
12MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
WeiweiCheng,EykeHu¨llermeier,andKrzysztofJDembczynski. Bayesoptimalmultilabelclassifi-
cationviaprobabilistic classifierchains. Ininternational conference onmachinelearning, pages
279–286, 2010.
Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. Structured prediction theory
basedonfactorgraphcomplexity. InAdvancesinNeuralInformationProcessingSystems,2016.
Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, Dmitry Storcheus, and Scott Yang. Efficient
gradientcomputationforstructuredoutputlearningwithrationalandtropicallosses. InAdvances
inNeuralInformation Processing Systems,2018.
Corinna Cortes, Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong. Cardinality-
awaresetprediction andtop-k classification. arXivpreprintarXiv:2407.07140, 2024.
Krzysztof Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke Hu¨llermeier. An exact al-
gorithm for f-measure maximization. In Advances in neural information processing systems,
2011.
KrzysztofDembczynski,WojciechKotłowski,andEykeHu¨llermeier. Consistentmultilabelranking
through univariate loss minimization. In Proceedings of the 29th International Coference on
International Conference onMachineLearning, pages1347–1354, 2012.
Krzysztof Dembczyn´ski, Willem Waegeman, Weiwei Cheng, and Eyke Hu¨llermeier. On label de-
pendence andlossminimization inmulti-label classification. MachineLearning,88:5–45, 2012.
Krzysztof Dembczynski, Arkadiusz Jachnik, Wojciech Kotlowski, Willem Waegeman, and Eyke
Hu¨llermeier. Optimizingthef-measureinmulti-labelclassification: Plug-inruleapproachversus
structuredlossminimization.InInternationalconferenceonmachinelearning,pages1130–1138,
2013.
Jia Deng, Sanjeev Satheesh, Alexander Berg, and Fei Li. Fast and balanced: Efficient label tree
learning for large scale object recognition. In Advances in Neural Information Processing Sys-
tems,2011.
Andre´ Elisseeff and Jason Weston. Akernel method for multi-labelled classification. In Advances
inneuralinformation processing systems,2001.
WeiGaoandZhi-HuaZhou. Ontheconsistencyofmulti-labellearning. InConferenceonlearning
theory, pages341–358, 2011.
Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise for
deepneuralnetworks. InProceedings oftheAAAIconference onartificial intelligence, 2017.
Ashish Kapoor, Raajay Viswanathan, and Prateek Jain. Multilabel classification using bayesian
compressed sensing. InAdvances inneuralinformation processing systems, 2012.
Oluwasanmi O Koyejo, Nagarajan Natarajan, Pradeep K Ravikumar, and Inderjit S Dhillon. Con-
sistent binary classification with generalized performance metrics. In Advances in neural infor-
mationprocessing systems,2014.
13MAO MOHRIZHONG
Oluwasanmi O Koyejo, Nagarajan Natarajan, Pradeep K Ravikumar, and Inderjit S Dhillon. Con-
sistentmultilabelclassification. InAdvancesinNeuralInformation ProcessingSystems, 2015.
Yoonkyung Lee, Yi Lin, and Grace Wahba. Multicategory support vector machines: Theory and
application to the classification of microarray data and satellite radiance data. Journal of the
AmericanStatistical Association, 99(465):67–81, 2004.
PhilLongandRoccoServedio. ConsistencyversusrealizableH-consistency formulticlassclassifi-
cation. InInternational ConferenceonMachineLearning, pages801–809, 2013.
Christopher ManningandHinrichSchutze. Foundations ofstatistical naturallanguageprocessing.
MITpress, 1999.
AnqiMao,ChristopherMohri,MehryarMohri,andYutaoZhong. Two-stagelearningtodeferwith
multipleexperts. InAdvancesinneuralinformation processing systems,2023a.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. H-consistency bounds: Characterization and exten-
sions. InAdvancesinNeuralInformation Processing Systems,2023b.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. H-consistency bounds for pairwise misranking loss
surrogates. InInternational conference onMachinelearning, 2023c.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Ranking with abstention. In ICML 2023 Workshop
TheManyFacetsofPreference-Based Learning, 2023d.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Structured prediction with stronger consistency
guarantees. InAdvancesinNeuralInformation Processing Systems,2023e.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Cross-entropy loss functions: Theoretical analysis
andapplications. InInternational ConferenceonMachineLearning,pages23803–23828, 2023f.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Principled approaches for learning to defer with
multipleexperts. InInternational Symposium onArtificialIntelligence andMathematics, 2024a.
AnqiMao,MehryarMohri,andYutaoZhong. Predictor-rejectormulti-classabstention: Theoretical
analysis andalgorithms. InAlgorithmicLearning Theory,2024b.
AnqiMao,MehryarMohri,andYutaoZhong. Theoreticallygroundedlossfunctionsandalgorithms
forscore-based multi-classabstention. InInternational ConferenceonArtificialIntelligence and
Statistics, 2024c.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. H-consistency guarantees for regression. arXiv
preprintarXiv:2403.19480, 2024d.
AnqiMao,MehryarMohri,andYutaoZhong. Regressionwithmulti-expertdeferral. arXivpreprint
arXiv:2403.19494, 2024e.
AnqiMao,MehryarMohri,andYutaoZhong. Top-kclassificationandcardinality-awareprediction.
arXivpreprintarXiv:2403.19625, 2024f.
14MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
Anqi Mao, Mehryar Mohri, and Yutao Zhong. A universal growth rate for learning with smooth
surrogatelosses. arXivpreprintarXiv:2405.05968, 2024g.
AndrewKachitesMcCallum. Multi-labeltextclassificationwithamixturemodeltrainedbyem. In
AAAI’99workshopontextlearning, 1999.
Aditya K Menon, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Multilabel reductions:
whatismylossoptimising? InAdvancesinNeuralInformation Processing Systems,2019.
Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Anqi Mao, and Yutao Zhong.
Learning to reject with a fixed predictor: Application to decontextualization. In International
ConferenceonLearningRepresentations, 2024.
James Petterson and Tiberio Caetano. Submodular multi-label learning. In Advances in Neural
Information Processing Systems,2011.
Harish G Ramaswamy, Balaji Srinivasan Babu, Shivani Agarwal, and Robert C Williamson. On
the consistency of output code based learning algorithms for multiclass learning problems. In
ConferenceonLearningTheory,pages885–902, 2014.
RobertESchapireandYoramSinger. Boostexter: Aboosting-based systemfortextcategorization.
Machinelearning, 39:135–168, 2000.
Ingo Steinwart. Howto compare different loss functions and their risks. Constructive Approxima-
tion,26(2):225–287, 2007.
AmbujTewariandPeterL.Bartlett.Ontheconsistencyofmulticlassclassificationmethods.Journal
ofMachineLearningResearch,8(36):1007–1025, 2007.
Pierre Franc¸ois Verhulst. Notice sur la loi que la population suit dans son accroissement. Corre-
spondance mathe´matique etphysique, 10:113—-121, 1838.
Pierre Franc¸ois Verhulst. Recherches mathe´matiques sur la loi d’accroissement de la population.
Nouveaux Me´moires de l’Acade´mie Royale des Sciences et Belles-Lettres de Bruxelles, 18:1—-
42,1845.
Willem Waegeman, Krzysztof Dembczyn´ski, Arkadiusz Jachnik, Weiwei Cheng, and Eyke
Hu¨llermeier. On the bayes-optimality of f-measure maximizers. Journal of Machine Learning
Research,15:3333–3388, 2014.
Jason Weston andChris Watkins. Multi-class support vector machines. Technical report, Citeseer,
1998.
Guoqiang WuandJunZhu. Multi-label classification: dohamminglossandsubsetaccuracy really
conflict with each other? In Advances in Neural Information Processing Systems, pages 3130–
3140,2020.
Guoqiang Wu, Chongxuan Li, Kun Xu, and Jun Zhu. Rethinking and reweighting the univariate
losses for multi-label ranking: Consistency and generalization. In Advances in Neural Informa-
tionProcessing Systems,pages14332–14344, 2021.
15MAO MOHRIZHONG
GuoqiangWu,ChongxuanLi,andYilongYin. Towardsunderstanding generalization ofmacro-auc
inmulti-label learning. InInternational Conference onMachine Learning, pages 37540–37570,
2023.
Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov, Ro´bert Busa-Fekete, and Krzysztof Dem-
bczynski. Ano-regretgeneralizationofhierarchicalsoftmaxtoextrememulti-labelclassification.
InAdvancesinneuralinformation processing systems,2018.
Nan Ye, Kian Ming Chai, Wee Sun Lee, and Hai Leong Chieu. Optimizing f-measures: A tale of
twoapproaches. InInternational Conference onMachineLearning, pages289–296, 2012.
Hsiang-FuYu,PrateekJain,PurushottamKar,andInderjitDhillon. Large-scalemulti-labellearning
withmissinglabels. InInternational conference onmachinelearning, pages593–601, 2014.
Min-Ling Zhang and Zhi-Hua Zhou. A review on multi-label learning algorithms. IEEE transac-
tionsonknowledge anddataengineering, 26(8):1819–1837, 2013.
Mingyuan Zhang and Shivani Agarwal. Bayes consistency vs. H-consistency: The interplay be-
tweensurrogatelossfunctionsandthescoringfunctionclass. InAdvancesinNeuralInformation
Processing Systems,2020.
Mingyuan Zhang, Harish Guruprasad Ramaswamy, and Shivani Agarwal. Convex calibrated sur-
rogates for the multi-label f-measure. In International Conference on Machine Learning, pages
11246–11255, 2020.
Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk
minimization. TheAnnalsofStatistics, 32(1):56–85, 2004a.
TongZhang. Statisticalanalysis ofsomemulti-category largemarginclassification methods. Jour-
nalofMachineLearningResearch,5(Oct):1225–1251, 2004b.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
withnoisylabels. InAdvancesinneuralinformation processing systems,2018.
16MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
Contents ofAppendix
A Extension: multi-labelconstrainedlosses 18
B ProofofH-consistency boundsforexistingsurrogate losses(Theorem3) 19
C ProofsofH-consistency boundsfornewsurrogate losses 20
C.1 Auxiliary definitionsandresults(Theorem 14andLemma15) . . . . . . . . . . . . 20
C.2 ProofofTheorem5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 ProofofTheorem7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.4 ProofofTheorem9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.5 ProofofTheorem12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D Futurework 28
17MAO MOHRIZHONG
Appendix A. Extension: multi-label constrained losses
In this section, we introduce another novel family of surrogate losses, adapting constrained losses
(Leeetal.,2004;Awasthietal.,2022b)fromstandardclassificationtomulti-labelconstrainedlosses
inasimilarway. Givenanygeneral multi-labellossL,wedefinemulti-labelconstrained lossesas:
l
∀ (h,x,y )∈H×X×Y, ̃L cstnd (h,x,y )= y∑ ′∈YL (y′ ,y )Φ (− i∑ =1y i′ h (x,i )). (10)
aw nh der the u∑ sy B∈Y ay∑ esl i= -c1 oy nih si(sx te, ni c)y= fo0 r. aN ne yx mt, uw lte i-s lh abo ew llt oh sa st .̃L cstnd also benefit from H-consistency bounds
Theorem12 Let H = Fl. Assume that F is complete Then, the following H-consistency bound
holdsinthemulti-label learning:
∀h∈H, R h −R∗ H +M H ≤Γ R h −R∗ H +M H ,
L ( ) L ( ) L ( ) ( ̃L cstnd( ) ̃L cstnd( ) ̃L cstnd( ))
whereΓ t =2√L maxtwhenΦ u =e−u;Γ t =2√twhenΦ u =max 0,1−u 2;andΓ t =t
whenΦ(u) =max 0,1−u or(Φ)u =min(m)ax 0,1−u ρ ,(1),ρ>0. { } ( )
( ) { } ( ) { { / } }
Corollary 13 Thefollowingexcesserrorboundholdsinthemulti-label learning:
∀h∈H , R h −R∗ H ≤Γ R h −R∗ H ,
all L ( ) L ( all ) ( ̃L cstnd( ) ̃L cstnd( all ))
whereΓ t =2√L maxtwhenΦ u =e−u;Γ t =2√twhenΦ u =max 0,1−u 2;andΓ t =t
when Φ(u) = max 0,1−u or(Φ) u = mi(n)max 0,1−u ρ(,1) , ρ > 0{. Moreo}ver, L ( )with
cstnd
thesech(oic)esofΦa{reBayes}-consis(ten)t withre{spect{toL. / } } ̃
TheproofofTheorem12isincludedinAppendixC.5. AswiththeproofofTheorem9,weuse
Theorem 14 and Lemma 15 from Appendix C.1, and aim to upper bound the conditional regret of
Lbythatofthesurrogate losses L using various concave functions Γ. However, thedifference
comp
̃
lies inour introduction and optimization of aparameter µ tailored toaquantity zthat isspecific to
theformofthemulti-labelconstrained loss.
Theseresultsshowthatincaseswhereminimizabilitygapsvanish,reducingtheestimationerror
ofL toǫresultsintheestimationerroroftargetmulti-labellossLbeingupperboundedbyeither
cstnd
̃
√ǫorǫ,moduloaconstant thatisindependent ofthenumberoflabels.
18MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
Appendix B. ProofofH-consistency bounds forexisting surrogate losses
(Theorem 3)
Theorem3 LetH=Fl. AssumethatthefollowingF-consistency boundholdsinthebinaryclassi-
fication, forsomeconcavefunction Γ∶R→R +:
∀f ∈F, R f −R∗ F +M F ≤Γ R f −R∗ F +M F .
ℓ0−1
( )
ℓ0−1
( )
ℓ0−1
( ) (
Φ
( )
Φ
( )
Φ
( ))
Then,thefollowingH-consistency boundholdsinthemulti-label learning: forallh∈H,
R h −R∗ H +M H
R
L ham
(h )−R∗
L
ham(H )+M
L ham
(H )≤lΓ⎛ ̃L br( ) ̃L br(
l
) ̃L br( )⎞.
⎝ ⎠
Proof Let p y x = P Y = y X = x be the conditional probability of Y = y given X = x.
Given a mul(ti-l∣abe)l surr(ogate los∣s L and)a hypothesis set H, we denote the conditional error by
tC h̃L e(ch o, nx d)it= ioE nay l∣x r[ẽL g(rh et,x by,y ∆)]C,theb he ,s x̃ t-in =-c Class hc ,o xnd −it Cio ∗na Hle ,r xro .r Wby eC c̃∗ L a(nH ex, px r)es= si tn hf eh c∈H onC d̃L it(ih on,x al)e, ra rn od
r
L,H L L
̃ ( ) ̃( ) ̃( )
ofthehamminglossandthesurrogate lossL asfollows:
br
̃
l
C L ham h,x = ∑p y x ∑1 yi≠h x,i
( ) y∈Y ( ∣ )i=1 ( )
l
=∑⎛ ∑ p y x 1 1≠sgn h x,i + ∑ p y x 1 −1≠sgn h x,i ⎞
i=1 y∶yi=+1 ( ∣ ) ( ( )) y∶yi=−1 ( ∣ ) ( ( ))
⎝ ⎠
l
C
L
h,x = ∑p y x ∑Φ y ih x,i
̃br( ) y∈Y ( ∣ )i=1 ( ( ))
l
=∑⎛ ∑ p y x Φ h x,i + ∑ p y x Φ −h x,i ⎞
i=1 y∶yi=+1 ( ∣ ) ( ( )) y∶yi=−1 ( ∣ ) ( ( ))
⎝ ⎠
Let q +1 x = ∑ y∶yi=+1p y x and q −1 x = ∑ y∶yi=+=−1p y x . Let f i = h ⋅,i ∈ F, for
( ∣ ) ( ∣ ) ( ∣ ) ( ∣ ) ( )
all i ∈ l . Then, it is clear that the conditional regrets of L and L can be expressed as the
ham br
corresp[on]ding conditional regretsofℓ andΦunderthisnewintrodũ ced newdistribution:
0−1
l l
∆C L ham,H (h,x )= i∑ =1∆C ℓ0−1,F (f i,x ), ∆C ̃L br,H (h,x )= i∑ =1∆C Φ,F (f i,x ).
Sincewehave∆C f ,x ≤Γ ∆C f ,x undertheassumption, weobtain
ℓ0−1,F
(
i
) (
Φ,F
(
i
))
l l
∆C L ham,H (h,x )=∑ i=1∆C ℓ0−1,F (f i,x )≤ i∑ =1Γ (∆C Φ,F (f i,x
))
1 l
≤lΓ ∑∆C
Φ,F
f i,x (concavity ofΓ)
(l i=1 ( ))
1
=lΓ ∆C h,x .
(l ̃L br,H ( ))
BytakingtheexpectationonbothsidesandusingtheJensen’sinequality, wecompletetheproof.
19MAO MOHRIZHONG
Appendix C. Proofs ofH-consistency bounds fornew surrogate losses
C.1. Auxiliarydefinitionsandresults(Theorem14andLemma15)
Before proceeding withthe proof, wefirstintroduce some notation and definitions. Given amulti-
label surrogate loss L and a hypothesis set H, we denote the conditional error by C h,x =
dE iy ti∣x o[ñL a(lh re, gx r, ey t)b]y, t ∆he C̃ best- hin ,- xcla =ss Ccon hd ,i xtion −a Cl ∗err Hor ,xby .C W̃∗ L (eH th, ex n)p= resin enf h t∈ aH gC ẽL n(eh ra, lx t)h, ea on red̃L m( th ,e wc h) o icn h-
L,H L L
shows that to derive H̃-c(onsis)tencỹb(ound)s iñm(ulti-la)bel learning with a concave function Γ, it is
only necessary to upper bound the conditional regret of the target multi-label loss by that of the
surrogate losswiththesameΓ.
Theorem14 Let L be a multi-label loss and L be a surrogate loss. Given a concave function
̃
Γ∶R
+
→R +. Ifthefollowingcondition holdsforallh∈Handx∈X:
∆C h,x ≤Γ ∆C h,x , (11)
L,H L,H
( ) ( ̃ ( ))
then,foranydistribution andforallhypotheses h∈H,
R h −R∗ H +M H ≤Γ R h −R∗ H +M H .
L L L L L L
( ) ( ) ( ) ( ̃( ) ̃( ) ̃( ))
ProofBythedefinitions, theexpectation oftheconditional regretsforLandLcanbeexpressed as:
̃
E ∆C h,x =R h −R∗ H +M H
L,H L L L
x[ ( )] ( ) ( ) ( )
E ∆C h,x =R h −R∗ H +M H .
x[ ̃L,H ( )] ̃L ( ) ̃L ( ) ̃L ( )
Thus,bytakingtheexpectation onbothsidesof(11)andusingJensen’s inequality, wehave
R h −R∗ H +M H =E ∆C h,x
L L L L,H
( ) ( ) ( ) x[ ( )]
≤E Γ ∆C h,x (Eq. (11))
x[ (
̃L,H
( ))]
≤Γ E ∆C h,x (concavity ofΓ)
(x[
̃L,H
( )])
=Γ R h −R∗ H +M H .
L L L
( ̃( ) ̃( ) ̃( ))
Thiscompletes theproof.
ToderiveH-consistency bounds using Theorem 14, wewillcharacterize theconditional regret
of a multi-label loss L. Forsimplicity, we first introduce some notation. Forany x ∈ X, let y x =
argmin E L y′,y ∈ Y. To simplify the notation further, we will drop the dependen(cy)on
y′∈Y yx
x. Specificall∣y,[w(e use)]y to denote y x and h to denote h x . Additionally, we define c =
h
E L h,y ,c =E L y,y andc( =)E L y′,y ,∀y′( ∈Y).
yx y yx y′ yx
∣ [ ( )] ∣ [ ( )] ∣ [ ( )]
Lemma15 Let H = Fl. Assume that F is complete. Then, the conditional regret of a multi-label
lossLcanbeexpressed asfollows: ∆C h,x =c −c .
L,H h y
( )
20MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
ProofBydefinition, theconditional errorofLcanbeexpressed asfollows:
C h,x = E L h,x,y = E L h x ,y =c .
L h
( ) yx[ ( )] yx[ ( ( ) )]
∣ ∣
Since H = Fl and F is complete, for any x ∈ X, h x ∶h∈H = Y. Then, the best-in-class
conditional errorofLcanbeexpressed asfollows: { ( ) }
C∗ H,x = inf C h,x = inf E L h x ,y = E L y x ,y =c . (12)
L L y
( ) h∈H ( ) h∈Hyx[ ( ( ) )] yx[ ( ( ) )]
∣ ∣
Therefore, ∆C h,x =C h,x −C∗ H,x =c −c .
L,H L L h y
( ) ( ) ( )
Next, by using Lemma15, wewill upper bound the conditional regret ofthe target multi-label
lossLbythatofthesurrogate lossLwithaconcave functionΓ.
̃
C.2. ProofofTheorem5
Theorem5 Let H = Fl. Assume that F is complete. Then, the following H-consistency bound
holdsinthemulti-label learning: forallh∈H,
1
R h −R∗ H +M H ≤2 R h −R∗ H +M H 2 .
L ham ( ) L ham( ) L ham ( ) ( ̃L log( ) ̃L log( ) ̃L log( ))
ProofWewillusethefollowingnotationadaptedtotheHammingloss: c =E L h,y ,c =
h yx ham y
E L y,y andc =E L y′,y ,∀y′ ∈Y. Wewilldenotebys
h,x,y′∣ =[ e∑(l i=1y) i′h]
(x,i)
y ∣x [ ( )] y′ y ∣x [ ham ( )] ( ) ∑ y′′∈Ye∑l i=1y i′′h(x,i)
and simplify notation by using s , thereby dropping the dependency on h and x. It is clear that
y′
s ∈ 0,1 . Then,theconditional errorofL canbeexpressed asfollows:
y′
[ ]
̃log
C
̃L
log(h,x )= yE x⎡ ⎢
⎢
⎢y∑ ′∈Y(1−L (y′ ,y ))log⎛ y∑ ′′∈Ye∑l i=1 (y i′′−y i′ )h (x,i )⎞⎤ ⎥
⎥
⎥
∣ ⎢ ⎝ ⎠⎥
=− ∑⎣ 1−c
y′
log s
y′
⎦
y′∈Y( ) ( )
21MAO MOHRIZHONG
Foranyh≠y,wedefinesµ asfollows: setsµ =s forally′ ≠y andy′ ≠h;definesµ =s −µ;and
y′ y′ h y
letsµ =s +µ. Notethatsµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C ̃L log,H (h,x )≥⎛− y∑ ′∈Y(1−c y′ )log (s y′ )⎞− µin ∈Rf⎛− y∑ ′∈Y(1−c y′ )log (sµ y′ )⎞
⎝ ⎠ ⎝ ⎠
=sup 1−c log s −µ −log s + 1−c log s +µ −log s
h y h y h y
µ∈R{( )[ ( ) ( )] ( )[ ( ) ( )]}
s +s 1−c s +s 1−c
= 1−c y log ( sh 2−y ) c( −c y ) + 1−c h log ( sh 2−y ) c( −c h )
( ) y h y ( ) h h y
( ) ( )
(supremum isattained whenµ∗ = − 1−c h s h+ 1−cy sy)
( 2)−cy−(c
h
)
2 1−c 2 1−c
≥ 1−c y log 2−(
c
−y c) + 1−c h log 2−(
c
−h c)
( ) h y ( ) h y
( ) ( )
(minimumisattained whens =s )
h y
c −c 2
≥
2
( 2−h
c
−y )
c
(alog a2 +a
b
+blog a2 +b
b
≥ 2(a a− +b )b2 ,∀a,b ∈ 0,1 )
h y ( ) [ ]
( )
c −c 2
h y
≥ ( ) .
4
1
Therefore, by Lemma 15, ∆C h,x ≤ 2 ∆C h,x 2. By Theorem 14, we complete
L ham,H
( ) (
̃L log,H
( ))
theproof.
22MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
C.3. ProofofTheorem7
Theorem5 Let H = Fl. Assume that F is complete. Then, the following H-consistency bound
holdsinthemulti-label learning: forallh∈H,
1
R h −R∗ H +M H ≤2 R h −R∗ H +M H 2 .
L ham ( ) L ham( ) L ham ( ) ( ̃L log( ) ̃L log( ) ̃L log( ))
Proof The proof is basically the same as that of Theorem 5, modulo replacing the Hamming loss
L with a general multi-label loss L. We adopt the following notation: c = E L h,y , c =
ham h yx y
E L y,y and c = E L y′,y , ∀y′ ∈ Y. Wewill denote by s h,x,y′ =
∣ [ e∑(l i=1yi′h) (]
x,i)
y ∣x [ ( )] y′ y ∣x [ ( )] ( ) ∑ y′′∈Ye∑l i=1y i′′h(x,i)
and simplify notation by using s , thereby dropping the dependency on h and x. It is clear that
y′
s ∈ 0,1 . Then,theconditional errorofL canbeexpressed asfollows:
y′
[ ]
̃log
C
̃L
log(h,x )= yE x⎡ ⎢
⎢
⎢y∑ ′∈Y(1−L (y′ ,y ))log⎛ y∑ ′′∈Ye∑l i=1 (y i′′−y i′ )h (x,i )⎞⎤ ⎥
⎥
⎥
∣ ⎢ ⎝ ⎠⎥
=− ∑⎣ 1−c
y′
log s
y′
⎦
y′∈Y( ) ( )
Foranyh≠y,wedefinesµ asfollows: setsµ =s forally′ ≠y andy′ ≠h;definesµ =s −µ;and
y′ y′ h y
letsµ =s +µ. Notethatsµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C ̃L log,H (h,x )≥⎛− y∑ ′∈Y(1−c y′ )log (s y′ )⎞− µin ∈Rf⎛− y∑ ′∈Y(1−c y′ )log (sµ y′ )⎞
⎝ ⎠ ⎝ ⎠
=sup 1−c log s −µ −log s + 1−c log s +µ −log s
h y h y h y
µ∈R{( )[ ( ) ( )] ( )[ ( ) ( )]}
s +s 1−c s +s 1−c
= 1−c y log ( sh 2−y ) c( −c y ) + 1−c h log ( sh 2−y ) c( −c h )
( ) y h y ( ) h h y
( (su) premum isattained w( henµ∗ = −)1−c h s h+ 1−cy sy)
( 2)−cy−(c
h
)
2 1−c 2 1−c
≥ 1−c y log 2−(
c
−y c) + 1−c h log 2−(
c
−h c)
( ) h y ( ) h y
( ) ( )
(minimumisattained whens =s )
h y
c −c 2
≥
2
( 2−h
c
−y )
c
(alog a2 +a
b
+blog a2 +b
b
≥ 2(a a− +b )b2 ,∀a,b ∈ 0,1 )
h y ( ) [ ]
( )
c −c 2
h y
≥ ( ) .
4
1
Therefore, by Lemma 15, ∆C h,x ≤ 2 ∆C h,x 2. By Theorem 14, we complete the
L,H
( ) (
̃L log,H
( ))
proof.
23MAO MOHRIZHONG
C.4. ProofofTheorem9
Theorem9 Let H = Fl. Assume that F is complete. Then, the following H-consistency bound
holdsinthemulti-label learning:
∀h∈H, R h −R∗ H +M H ≤Γ R h −R∗ H +M H ,
L
( )
L
( )
L
( ) (
̃Lcomp(
)
̃Lcomp(
)
̃Lcomp(
))
whereΓ t =2√twhenΨ u =log u oru−1;Γ t =2√nqtwhenΨ u = 1 1− 1 ,q ∈ 0,1 ;
q uq
( ) ( ) ( ) ( ) ( ) ( ) ( )
andΓ t =ntwhenΨ u =1− 1.
u
( ) ( )
Proof Recall that we adopt the following notation: c = E L h,y , c = E L y,y and
h yx y yx
c = E L y′,y , ∀y′ ∈ Y. We will denote by s
h,x,y′∣ [ =( e) ∑]l
i=1yi′h(x,i)
∣ [ an(
d
si) m]
plify
y′ y ∣x [ ( )] ( ) ∑ y′′∈Ye∑l i=1y i′′h(x,i)
notationbyusings ,therebydroppingthedependencyonhandx. Itisclearthats ∈ 0,1 . Next,
y′ y′
wewillanalyzecasebycase. [ ]
ThecasewhereΦ u =log u : SeetheproofofTheorem7.
ThecasewhereΦ(u) =u−(1:)Theconditional errorofL canbeexpressed asfollows:
comp
( ) ̃
C h,x
̃Lcomp(
)
= yE x⎡ ⎢
⎢
⎢y∑ ′∈Y(1−L (y′ ,y ))⎛ y∑ ′′∈Ye∑l i=1 (y i′′−y i′ )h (x,i )−1⎞⎤ ⎥
⎥
⎥
∣ ⎢ ⎝ ⎠⎥
⎣ 1 ⎦
= ∑ 1−c y′
s
−1 .
y′∈Y( )( y′ )
Foranyh≠y,wedefinesµ asfollows: setsµ =s forally′ ≠y andy′ ≠h;definesµ =s −µ;and
y′ y′ h y
letsµ =s +µ. Notethatsµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C h,x
̃Lcomp,H
( )
1 1
≥ y∑ ′∈Y(1−c y′ )(s
y′
−1 )− µin ∈Rf⎛ y∑ ′∈Y(1−c y′ )⎛ sµ
y′
−1⎞⎞
⎝ ⎝ ⎠⎠
1 1 1 1
=sup 1−c − + 1−c −
µ∈R{( h )[s h s y−µ] ( y )[s y s h+µ]}
=
1−c
h +
1−c
y −
2−c h−c y+2 (1−c
h
)1
2
(1−c
y
)21
s s s +s
h y h y
(supremum isattained whenµ∗ = −√1−c hs h+√1−cysy)
√1−cy+√1−c
h
≥ 1−c h 21 − 1−c y 1 2 2 (minimumisattainedwhens h =s y = 1 2)
(( ) ( ) )
c −c 2
h y
= ( )
1−c
h
21 + 1−c
y
1
2
2
(( ) ( ) )
c −c 2
h y
≥ ( ) .
4
24MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
1
Therefore, byLemma15,∆C h,x ≤2 ∆C h,x 2. ByTheorem14,wecompletethe
L,H
( ) (
̃Lcomp,H
( ))
proof.
ThecasewhereΦ u = 1 1− 1 ,q ∈ 0,1 : Theconditional errorofL canbeexpressed
as: ( )
q
(
uq
) ( )
̃comp
1
C ̃Lcomp(h,x )=
q
y∑ ′∈Y(1−c y′ )(1− (s y′ )q ).
Foranyh≠y,wedefinesµ asfollows: setsµ =s forally′ ≠y andy′ ≠h;definesµ =s −µ;and
y′ y′ h y
letsµ =s +µ. Notethatsµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C h,x
̃Lcomp,H
( )
1 1
≥ q y∑ ′∈Y(1−c y′ )(1−s y )− µin ∈Rf⎛ q y∑ ′∈Y(1−c y′ )(1− (sµ y′ )q )⎞
⎝ ⎠
1
= sup 1−c −s + s −µ q + 1−c − s q+ s +µ q
h h y y y h
q µ∈R{( )[ ( ) ] ( )[ ( ) ( ) ]}
= 1 q(s h+s y )q ((1−c y )1−1 q + (1−c h )1−1 q )1−q − 1 q(1−c y )sq y− 1 q(1−c h )sq h
1 1
(supremum isattained whenµ∗ = − 1−c h 1−qs h+ 1−cy 1−qsy)
( ) 1 ( )1
1−cy 1−q+ 1−c
h
1−q
≥ qn1 q [2q ((1−c y )1−1 q + (1−c h )1−1 q )1−q − (1−c y )− (1−c h
)]
( ) ( )
(minimumisattained whens =s = 1)
h y n
c −c 2 1 1 1−q
≥ ( h y ) . ( a1−q+b1−q − a+b ≥ q a−b 2,∀a,b∈ 0,1 ,0≤a+b≤1)
4nq
(
2
)
2 4
( ) [ ]
1
Therefore, byLemma15, ∆C
L,H
(h,x
)
≤2n2q (∆C
̃Lcomp,H
(h,x ))2. ByTheorem 14, wecomplete
theproof.
ThecasewhereΦ u = 1− 1 : Theconditional errorofL canbeexpressed as:
( ) (
u
)
̃comp
C ̃Lcomp(h,x )= y∑ ′∈Y(1−c y′ )(1− (s y′ )q ).
Foranyh≠y,wedefinesµ asfollows: setsµ =s forally′ ≠y andy′ ≠h;definesµ =s −µ;and
y′ y′ h y
letsµ =s +µ. Notethatsµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C h,x
̃Lcomp,H
( )
≥ y∑ ′∈Y(1−c y′ )(1−s y )− µin ∈Rf⎛ y∑ ′∈Y(1−c y′ )(1−sµ y′ )⎞
⎝ ⎠
=sup 1−c −s +s −µ + 1−c −s +s +µ
h h y y y h
µ∈R{( )[ ] ( )[ ]}
=s c −c (supremum isattainedwhenµ∗ =s )
h h y y
( )
1
≥ c −c . (minimumisattainedwhens = 1)
n( h y ) h n
25MAO MOHRIZHONG
Therefore, by Lemma 15, ∆C h,x ≤ n∆C h,x . By Theorem 14, we complete the
proof.
L,H
( )
̃Lcomp,H
( )
C.5. ProofofTheorem12
Theorem12 Let H = Fl. Assume that F is complete Then, the following H-consistency bound
holdsinthemulti-label learning:
∀h∈H, R h −R∗ H +M H ≤Γ R h −R∗ H +M H ,
L ( ) L ( ) L ( ) ( ̃L cstnd( ) ̃L cstnd( ) ̃L cstnd( ))
whereΓ t =2√L maxtwhenΦ u =e−u;Γ t =2√twhenΦ u =max 0,1−u 2;andΓ t =t
whenΦ(u) =max 0,1−u or(Φ)u =min(m)ax 0,1−u ρ ,(1),ρ>0. { } ( )
( ) { } ( ) { { / } }
Proof Recall that we adopt the following notation: c = E L h,y , c = E L y,y and
h yx y yx
c
y′
= E
yx
L y′,y , ∀y′ ∈ Y. We will also denote by z h,x∣,[ y′( = ∑)]l i=1y i′h x,i∣ [ an( d si) m] plify
notation b∣ y[u(sing z)] , thereby dropping the dependency o(n h and)x. It is clear(that)the constraint
y′
canbeexpressed as∑ y′∈Yz
y′
=0. Next,wewillanalyze casebycase.
ThecasewhereΦ u =e−u: Theconditional errorofL canbeexpressed asfollows:
cstnd
( ) ̃
C
̃L
cstnd(h,x )= yE x⎡ ⎢
⎢
⎢y∑ ′∈YL (y′ ,y )e∑l i=1y i′h (x,i )⎤ ⎥
⎥
⎥= y∑ ′∈Yc y′ez y′.
∣ ⎢ ⎥
⎣ ⎦
Foranyh≠y,wedefinezµ asfollows: setzµ =z forally′ ≠yandy′ ≠h;definezµ =z −µ;and
y′ y′ h y
letzµ =z +µ. Notethatzµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C
̃Lcomp,H
(h,x )≥ y∑ ′∈Yc y′ez y′ − µin ∈Rf⎛ y∑ ′∈Yc
y′ezµ
y′⎞
⎝ ⎠
=sup c ezy −ez h+µ +c ez h −ezy−µ
y h
µ∈R{ ( ) ( )}
= (√c hez h −√c yezy )2 (supremum isattained whenµ∗ = 1 2log cc hy ee zz hy )
c −c 2
h y
= (minimumisattained whenz =z =0)
c + c h y
(√ y √ h)
1
≥ c −c 2.
h y
4L
max( )
1
Therefore, by Lemma 15, ∆C
L,H
(h,x
)
≤ 2 (L
max
)21 (∆C
̃L cstnd,H
(h,x ))2. By Theorem 14, we
completetheproof.
The case where Φ u = max 0,1−u 2: The conditional error of L can be expressed as
cstnd
follows: ( ) { } ̃
C L h,x = ∑ c y′max 0,1+z y′ 2 .
̃cstnd( ) y′∈Y { }
26MULTI-LABELLEARNING WITH STRONGERCONSISTENCY GUARANTEES
Foranyh≠y,wedefinezµ asfollows: setzµ =z forally′ ≠yandy′ ≠h;definezµ =z −µ;and
y′ y′ h y
letzµ =z +µ. Notethatzµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C h,x
̃L cstnd,H
( )
2
≥ y∑ ′∈Yc y′max {0,1+z y′ }2− µin ∈Rf⎛ y∑ ′∈Yc y′max {0,1+zµ y′
}
⎞
⎝ ⎠
=sup c max 0,1+z 2−max 0,1+z +µ 2 +c max 0,1+z 2−max 0,1+z −µ 2
y y h h h y
µ∈R{ ( { } { } ) ( { } { } )}
≥ 1+z 2 c −c 2 (differentiating withrespecttoµtooptimize)
h y h
( ) ( )
≥ c −c 2 . (minimumisattainedwhenz =0)
h y h
( )
1
Therefore, by Lemma15, ∆C h,x ≤ ∆C h,x 2. By Theorem 14, wecomplete the
L,H
( ) (
̃L cstnd,H
( ))
proof.
ThecasewhereΦ u =max 0,1−u : Theconditional errorofL canbeexpressed as:
cstnd
( ) { } ̃
C L h,x = ∑ c y′max 0,1+z y′ .
̃cstnd( ) y′∈Y { }
Foranyh≠y,wedefinezµ asfollows: setzµ =z forally′ ≠yandy′ ≠h;definezµ =z −µ;and
y′ y′ h y
letzµ =z +µ. Notethatzµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C h,x
̃L cstnd,H
( )
≥ y∑ ′∈Yc y′max {0,1+z y′ }− µin ∈Rf⎛ y∑ ′∈Yc y′max {0,1+zµ y′ }⎞
⎝ ⎠
=sup c max 0,1+z −max 0,1+z +µ +c max 0,1+z 2−max 0,1+z −µ 2
y y h h h y
µ∈R{ ( { } { }) ( { } { } )}
≥ 1+z c −c (differentiating withrespecttoµtooptimize)
h y h
( )( )
≥ c −c . (minimumisattainedwhenz =0)
h y h
( )
Therefore, by Lemma 15, ∆C h,x ≤ ∆C h,x . By Theorem 14, we complete the
proof.
L,H
( )
̃L cstnd,H
( )
The case where Φ u = min max 0,1−u ρ ,1 ,ρ > 0: The conditional error of L can
cstnd
beexpressedas: ( ) { { / } } ̃
C L h,x = ∑ c y′min max 0,1+z y′ ρ ,1 .
̃cstnd( ) y′∈Y { { / } }
27MAO MOHRIZHONG
Foranyh≠y,wedefinezµ asfollows: setzµ =z forally′ ≠yandy′ ≠h;definezµ =z −µ;and
y′ y′ h y
letzµ =z +µ. Notethatzµ canberealized bysomeh′ ∈Hundertheassumption. Then,wehave
y h
∆C h,x
̃L cstnd,H
( )
≥ y∑ ′∈Yc y′min {max {0,1+z y′ /ρ },1 }− µin ∈Rf⎛ y∑ ′∈Yc y′min {max {0,1+zµ y′ /ρ },1 }⎞
⎝ ⎠
=sup c min max 0,1+z ρ ,1 −min max 0,1+ z +µ ρ ,1
y y h
µ∈R{ ( { { / } } { { ( )/ } })
+c min max 0,1+z ρ ,1 −min max 0,1+ z −µ ρ ,1
h h y
( { { / } } { { ( )/ }} )}
≥ c −c . (differentiating withrespecttoµtooptimize)
y h
( )
Therefore, by Lemma 15, ∆C h,x ≤ ∆C h,x . By Theorem 14, we complete the
proof.
L,H
( )
̃L cstnd,H
( )
Appendix D. Future work
WhileourworkproposedaunifiedsurrogatelossframeworkthatisBayes-consistent foranymulti-
label loss, significantly expanding upon previous workwhichonlyestablished consistency forspe-
cificlossfunctions, empirical comparison withsurrogate lossesforspecificlossfunctions couldbe
an interesting direction, which we have left for future work and have already initiated. Moreover,
thepotentialtotheoretically improvesurrogatelossesforspecifictargetlossesisanotherpromising
direction.
28