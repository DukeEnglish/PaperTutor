GroupMamba: Parameter-Efficient and Accurate
Group Visual State Space Model
AbdelrahmanShaker SyedTalalWasim
MohamedbinZayedUniversityofArtificialIntelligence UniversityofBonn
SalmanKhan JuergenGall
MohamedbinZayedUniversityofArtificialIntelligence UniversityofBonn
FahadShahbazKhan
MohamedbinZayedUniversityofArtificialIntelligence;LinköpingUniversity
Abstract
Recent advancements in state-space models (SSMs) have showcased effective
performanceinmodelinglong-rangedependencieswithsubquadraticcomplexity.
However, pure SSM-based models still face challenges related to stability and
achieving optimal performance on computer vision tasks. Our paper addresses
thechallengesofscalingSSM-basedmodelsforcomputervision,particularlythe
instabilityandinefficiencyoflargemodelsizes. Toaddressthis,weintroducea
ModulatedGroupMambalayerwhichdividestheinputchannelsintofourgroups
andappliesourproposedSSM-basedefficientVisualSingleSelectiveScanning
(VSSS)blockindependentlytoeachgroup,witheachVSSSblockscanninginone
ofthefourspatialdirections. TheModulatedGroupMambalayeralsowrapsthe
fourVSSSblocksintoachannelmodulationoperatortoimprovecross-channel
communication.Furthermore,weintroduceadistillation-basedtrainingobjectiveto
stabilizethetrainingoflargemodels,leadingtoconsistentperformancegains. Our
comprehensiveexperimentsdemonstratethemeritsoftheproposedcontributions,
leading to superior performance over existing methods for image classification
on ImageNet-1K, object detection, instance segmentation on MS-COCO, and
semanticsegmentationonADE20K.Ourtinyvariantwith23Mparametersachieves
state-of-the-art performance with a classification top-1 accuracy of 83.3% on
ImageNet-1K,whilebeing26%efficientintermsofparameters,comparedtothe
bestexistingMambadesignofsamemodelsize.Ourcodeandmodelsareavailable
at: https://github.com/Amshaker/GroupMamba.
1 Introduction
Variouscontextmodelingmethodshaveemergedinthedomainsoflanguageandvisionunderstanding.
TheseincludeConvolution[22,66],Attention[59],and,morerecently,StateSpaceModels[17,16].
Transformers with their multi-headed self-attention mechanism [59] have been central to both
languagemodelssuchasGPT-3[2]andvisionmodelssuchasVisionTransformers[10,36].However,
challengesaroseduetothequadraticcomputationalcomplexityofattentionmechanismsparticularly
forlongersequences,leadingtotherecentemergenceofStateSpacemodelssuchasS4[17].
Whilebeingeffectiveinhandlingextendedinputsequencesduetotheirlinearcomplexityintermsof
sequencelengths,S4[17]encounteredlimitationsinglobalcontextprocessingininformation-dense
data,especiallyindomainslikecomputervisionduetothedata-independentnatureofthemodel. Al-
Preprint.Underreview.
4202
luJ
81
]VC.sc[
1v27731.7042:viXraFigure1: Left: ComparisonintermsofParametersvs. Top-1AccuracyonImageNet-1k[9]. Our
GroupMamba-Bachievessuperiortop-1classificationaccuracywhilereducingparametersby36%
comparedtoVMamba[34]. Right: QualitativeresultsofGroupMamba-Tonsemanticsegmentation
(topright),andobjectdetectionandinstancesegmentation(bottomright). Morequalitativeexamples
arepresentedinFigure3andFigure4
ternatively,approachessuchasglobalconvolutions-basedstatespacemodels[14]andLiquidS4[20]
havebeenproposedtomitigatetheaforementionedlimitations. TherecentMamba[16]introduces
theS6architecturewhichaimstoenhancetheabilityofstate-spacemodelstohandlelong-range
dependenciesefficiently. Theselective-scanalgorithmintroducedbyMambausesinput-dependent
state-spaceparameters,whichallowforbetterin-contextlearningwhilestillbeingcomputationally
efficientcomparedtoself-attention.
However,Mamba,specificallytheS6algorithm,isknowntobeunstablefore.g.,imageclassification,
especiallywhenscaledtolargesizes[45]. Additionally,theMambamodelvariantusedinimage
classificationgenerallycalledtheVSS(VisualStateSpace)block,canbemoreefficientintermsof
parametersandcomputerequirementsbasedonthenumberofchannels. TheVSSblockincludes
extensiveinputandoutputprojectionsalongwithdepth-wiseconvolutions,whoseparametersand
computecomplexitiesaredirectlyproportionaltothenumberofchannelsintheinput. Toaddress
thisissue,weproposeaModulatedGroupMambalayerwhichmitigatestheaforementionedissues
inacomputationandparameter-efficientmanner. Themaincontributionsofourpaperare:
1. WeintroduceaModulatedGroupMambalayer,inspiredbyGroupConvolutions,which
enhancescomputationalefficiencyandinteractioninstate-spacemodelsbyusingamulti-
directionscanningmethodforcomprehensivespatialcoverageandeffectivemodelingof
localandglobalinformation.
2. WeintroduceaChannelAffinityModulation(CAM)operator,whichenhancescommuni-
cationacrosschannelstoimprovefeatureaggregation,addressingthelimitedinteraction
inherentinthegroupingoperation.
3. Weemployadistillation-basedtrainingobjectivetofurtherstabilizethetrainingofmodels
withalargenumberofparameters.
4. Webuildaseriesofparameter-efficientgenericclassificationmodelscalled“GroupMamba”,
basedontheproposedModulatedGroupMambalayer. Ourtinyvariantachieves83.3%
top-1accuracyonImageNet-1k[9]with23M parametersand4.6GFLOPs. Additionally,
ourbasevariantachievestop-1accuracyof84.5%with57M parametersand14GFLOPs,
outperformingallrecentSSMmethods(seeFigure1).
22 RelatedWork
ConvolutionalNeuralNetworks(ConvNets)havebeenthepopularchoiceforcomputervisiontasks
sincetheintroductionofAlexNet[29]. ThefieldhasrapidlyevolvedwithseverallandmarkConvNet
architectures[51,55,22,25,56]. Alongsidethesearchitecturaladvances,significanteffortshave
been made to refine individual convolution layers, including depthwise convolution [65], group
convolution[7],anddeformableconvolution[8]. Recently,ConvNeXtvariants[37,63]havetaken
concretestepstowardsmodernizingtraditional2DConvNetsbyincorporatingmacrodesignswith
advancedsettingsandtrainingrecipestoachieveon-parperformancewiththestate-of-the-artmodels.
Inrecentyears,thepioneeringVisionTransformer(ViT)[10]hassignificantlyimpactedthecomputer
vision field, including tasks such as image classification [57, 36, 35, 12], object detection [3, 71,
43,68],andsegmentation[5,50,27]. ViT[10]introducesamonolithicdesignthatapproachesan
imageasaseriesofflattened2Dpatcheswithoutimage-specificinductivebias. Theremarkable
performance of ViT for computer vision tasks, along with its scalability, has inspired numerous
subsequentendeavorstodesignbetterarchitectures. TheearlyViT-basedmodelsusuallyrequire
large-scale datasets (e.g., JFT-300M [54]) for pretraining. Later, DeiT [57] proposes advanced
training techniques in addition to integrating a distillation token into the architecture, enabling
effectivetrainingonsmallerdatasets(e.g.,ImageNet-1K[9]). Sincethen,subsequentstudieshave
designed hierarchical and hybrid architectures by combining CNN and ViT modules to improve
performanceondifferentvisiontasks[53,40,11,49,12]. Anotherlineofworkistomitigatethe
quadraticcomplexityinherentinself-attention,aprimarybottleneckofViTs. Thisefforthasledto
significantimprovementsandmoreefficientandapproximatedvariants[61,49,44,42,28,6,58],
offeringreducedcomplexitywhilemaintainingeffectiveness.
Recently,StateSpaceModels(SSMs)haveemergedasanalternativetoViTs[59],capturingthe
intricate dynamics and inter-dependencies within language sequences [17]. One notable method
inthisareaisthestructuredstate-spacesequencemodel(S4)[17],designedtotacklelong-range
dependencieswhilemaintaininglinearcomplexity. Followingthisdirection,severalmodelshave
beenproposed,includingS5[52],H3[13],andGSS[41]. Morerecently,Mamba[16]introducesan
input-dependentSSMlayerandleveragesaparallelselectivescanmechanism(S6).
Inthevisualdomain,variousworkshaveappliedSSMstodifferenttasks. Inparticularforimage
classification,VMamba[34]usesMambawithbidirectionalscansacrossbothspatialdimensions
inahierarchicalSwin-Transformer[36]styledesigntobuildaglobalreceptivefieldefficiently. A
concurrent work, Vision Mamba (Vim) [70], instead proposed a monolithic design with a single
bidirectional scan for the entire image, outperforming traditional vision transformers like DeiT.
LocalVMamba[26]addressesthechallengeofcapturingdetailedlocalinformationbyintroducinga
scanningmethodologywithindistinctwindows(inspiredfromSwin-Transformer[36]),coupledwith
dynamicscanningdirectionsacrossnetworklayers. EfficientVMamba[46]integratesatrous-based
selectivescanninganddual-pathwaymodulesforefficientglobalandlocalfeatureextraction,achiev-
ingcompetitiveresultswithreducedcomputationalcomplexity. Thesemodelshavebeenappliedfor
imageclassification,aswellasimagesegmentation[33,39,48,15],videounderstanding[67,30,4],
and various other tasks [19, 23, 60, 18, 31]. Their wide applicability shows the effectiveness of
SSMs[17,52,13,41],andinparticularMamba[16],inthevisualdomain. Inthispaper,wepropose
aModulatedGroupMambalayerthatmitigatesthedrawbacksofthedefaultvisionMambablock,
suchaslackofstability[45]andtheincreasednumberofparameterswithrespecttothenumberof
channels.
3 Method
Motivation: Ourmethodismotivatedbasedontheobservationswithrespecttothelimitationsof
existingVisualState-Spacemodels.
• Lack of Stability for Larger Models: We observe from [45] that Mamba [16] based im-
ageclassificationmodelswithanMLPchannelmixerareunstablewhenscaledtoalarge
numberofparameters. ThisinstabilitycanbeseeninSiMBA-L(MLP)[45],whichleads
tosub-optimalclassificationresultsof49%accuracy. Wemitigatethisissuebyintroduc-
ingaModulatedGroupMambadesignalongsideadistillationobjective(aspresentedin
Section3.4)thatstabilizestheMambaSSMtrainingwithoutmodifyingthechannelmixer.
3Figure2: Overviewoftheproposedmethod. TopRow: Theoverallarchitectureofourframework
withaconsistenthierarchicaldesigncomprisingfourstages.BottomRow:Wepresent(b)Thedesign
ofthemodulatedgroupmambalayer. Theinputchannelsaredividedintofourgroupswithasingle
scanningdirectionforeachVSSSblock. Thissignificantlyreducesthecomputationalcomplexity
compared to the standard mamba layer, with similar performance. Channel Affinity Modulation
mechanismisintroducedtoaddressthelimitedinteractionswithintheVSSSblocks. (c)Thedesign
ofVSSSblock. ItconsistsofMambablockwith1DSelectiveScanningblockfollowedbyFFN.(d)
ThefourscanningdirectionsusedforthefourVSSSblocksareillustrated.
• EfficientImprovedInteraction: GiventhecomputationalimpactofMamba-baseddesignon
thenumberofchannels,theproposedModulatedGroupMambalayeriscomputationally
inexpensiveandparameterefficientthanthedefaultMambaandabletomodelbothlocaland
globalinformationfromtheinputtokensthroughmulti-directionscanning. Anadditional
ChannelAffinityModulationoperatorisproposedinthisworktocompensateforthelimited
channelinteractionduetothegroupedoperation.
3.1 Preliminaries
State-Space Models: State-space models (SSMs) like S4 [17] and Mamba [16] are structured
sequence architectures inspired by a combination of recurrent neural networks (RNNs) and con-
volutional neural networks (CNNs), with linear or near-linear scaling in sequence length. De-
rived from continuous systems, SSMs define and 1D function-to-function map for an input
x(t) ∈ RL → y(t) ∈ RL via a hidden state h(t) ∈ RN. More formally, SSMs are described
bythecontinuoustimeOrdinaryDifferentialEquation(ODE)inEquation1.
h′(t)=Ah(t)+Bx(t),
(1)
y(t)=Ch(t),
whereh(t)isthecurrenthiddenstate,h′(t)istheupdatedhiddenstate,x(t)isthecurrentinput,y(t)
istheoutput,A∈RN×N isSSM’sevolutionmatrix,andB∈RN×1,C∈RN×1aretheinputand
outputprojectionmatrices,respectively.
Discrete State-Space Models: To allow these models to be used in sequence modeling tasks in
deeplearning,theyneedtobediscretized,convertingtheSSMfromacontinuoustimefunction-to-
functionmapintoadiscrete-timesequence-to-sequencemap. S4[17]andMamba[16]areamong
thediscreteadaptationsofthecontinuoussystem,incorporatingatimescaleparameter∆toconvert
thecontinuousparametersA,BintotheirdiscreteequivalentsA,B. Thisdiscretizationistypically
4donethroughtheZero-OrderHold(ZOH)methodgiveninEquation2.
A=exp(∆A),
B=(∆A)−1(exp(∆A)−I)·∆B
(2)
h =Ah +Bx ,
t t−1 t
y =Ch .
t t
WhilebothS4[17]andMamba[16]utilizeasimilardiscretizationstepasstatedaboveinEquation2,
MambadifferentiatesitselffromS4byconditioningtheparameters∆∈RB×L×D,B∈RB×L×N
andC∈RB×L×N,ontheinputx∈RB×L×D,throughtheS6SelectiveScanMechanism,whereB
isthebatchsize,Listhesequencelength,andDisthefeaturedimension.
3.2 OverallArchitecture
AsshowninFigure2(a),ourmodelusesahierarchicalarchitecture,similartoSwinTransformer[36],
with four stages to efficiently process images at varying resolutions. Assuming an input image,
I∈RH×W×3,wefirstapplyaPatchEmbeddinglayertodividetheimageintonon-overlapping
patchesofsize4×4andembedeachpatchintoaC -dimensionalfeaturevector.Thepatchembedding
1
layerisimplementedusingtwo3×3convolutionswithastrideof2. Thisproducesfeaturesmaps
ofsize H × W ×C atthefirststage. ThesefeaturemapsarepassedtoastackofourModulated
4 4 1
GroupedMambablocks(asdetailedinSection3.3). Ineachsubsequentstage,adown-sampling
layermergespatchesina2×2region,followedbyanotherstackofourModulatedGroupedMamba
blocks. Hence, feature size at stages two, three and four are H × W ×C , H × W ×C , and
8 8 2 16 16 3
H × W ×C ,respectively.
32 32 4
3.3 ModulatedGroupMambaLayer
WepresenttheoveralloperationsoftheproposedModulatedGroupMambalayer(Figure2(b))for
aninputsequenceX ,withdimensions(B,H,W,C),whereBisthebatchsize,C isthenumberof
in
inputchannelsandH/W arethewidthandheightofthefeaturemap,inEquation3.
X =GroupedMamba(X ,Θ)
GM in
X =CAM(X ,Affinity(X )) (3)
CAM GM in
X =X +FFN(LN(X ))
out in CAM
Here, X is the output of Equation 6, X is the output of Equation 9, LN is the Layer Nor-
GM CAM
malization[1]operation,FFNistheFeed-ForwardNetworkasdescribedbyEquation5,andX
out
isthefinaloutputoftheModulatedGroupMambablock. Theindividualoperations,namelythe
GroupedMamba operator, the VSSS block used inside the GroupedMamba operator, and the
CAMoperator,arepresentedinSection3.3.1,Section3.3.2andSection3.3.3,respectively.
3.3.1 VisualSingleSelectiveScan(VSSS)Block
TheVSSSblock(Figure2(c))isatokenandchannelmixerbasedontheMambaoperator. Mathe-
matically,foraninputtokensequenceZ ,theVSSSblockperformstheoperationsasdescribedin
in
Equation4.
Z′ =Z +Mamba(LN(Z ))
out in in (4)
Z =Z′ +FFN(LN(Z′ ))
out out out
WhereZ istheoutputsequence,MambaisthediscretizedversionoftheMambaSSMoperatoras
out
describedinEquation2.
FFN(LN(Z′ ))=GELU(LN(Z′ )W +b )W +b (5)
out out 1 1 2 2
WhereGELU[24]istheactivationfunctionandW ,W ,b ,andb areweightsandbiasesforthe
1 2 1 2
linearprojections.
53.3.2 GroupedMambaOperator
Considering the motivation presented earlier in Section 3, we aim to design a variant of the
Mamba [16] that is both computationally efficient and can effectively model the spatial depen-
denciesoftheinputsequence. GiventhatMambaiscomputationallyinefficientonlargenumber
of channels C in the input sequence, we propose a grouped variant of the operator, inspired by
GroupedConvolutions. TheGroupedMambaoperationisavariantoftheVSSSblockpresentedin
Section3.3.1,wheretheinputchannelsaredividedintogroups,andtheVSSSoperatorisapplied
separatelytoeachgroup. Specifically,wedividetheinputchannelsintofourgroups,eachofsize C,
4
andanindependentVSSSblockisappliedtoeachgroup. Tobettermodelspatialdependenciesinthe
input,eachofthefourgroupsscansinoneoffourdirectionsacrossthetokensequence: left-to-right,
right-to-left,top-to-bottom,andbottom-to-top,asoutlinedinFigure2(d).
LetG=4bethenumberofgroupsrepresentingfourscanningdirections: left-to-right,right-to-left,
top-to-bottom,andbottom-to-top. WeformfoursequencesfromtheinputsequenceX ,namely
in
X , X , X , and X , each of shape (B,H,W,C), representing one of the four directions
LR RL TB BT 4
specifiedearlier. Thesearethenflattenedtoformasingletokensequenceofshape(B,N,C),where
4
N =W ×H isthenumberoftokensinthesequence. Theparametersforeachofthefourgroups
canbespecifiedbyθ ,θ ,θ ,andθ ,respectively,foreachofthefourgroups,representingthe
LR RL TB BT
parametersfortheVSSSblocks.
Giventheabovedefinitions,theoverallrelationfortheGroupedMambaoperatorcanbewrittenas
showninEquation6.
X =GroupedMamba(X ,Θ)=Concat(VSSS(X ,Θ ),
GM in LR LR
VSSS(X ,Θ ),
RL RL
(6)
VSSS(X ,Θ ),
TB TB
VSSS(X ,Θ ))
BT BT
Where:
• X ,X ,X ,andX representtheinputtensorsscannedintherespectivedirections.
LR RL TB BT
• Θ ,Θ ,Θ ,andΘ representstheparametersoftheVSSSblockforeachdirection.
LR RL TB BT
• TheoutputofeachMambaoperatorisreshapedagainto(B,H,W,C),andconcatenated
4
backtoformthetokensequenceX ,againofthesize(B,H,W,C).
GM
3.3.3 ChannelAffinityModulation(CAM)
Onitsown,theGroupedMambaoperatormayhaveadisadvantageintheformoflimitedinformation
exchange across channels, given the fact that each operator in the group only operates over C
4
channels. Toencouragetheexchangeofinformationacrosschannels,weproposeaChannelAffinity
Modulationoperator,whichrecalibrateschannel-wisefeatureresponsestoenhancetherepresentation
powerofthenetwork. Inthisblock,wefirstaveragepooltheinputtocalculatethechannelstatistics
asshowninEquation7.
ChannelStat(X )=AvgPool(X ) (7)
in in
whereX istheinputtensor,andAvgPoolrepresentstheglobalaveragepoolingoperation. Next
in
comestheaffinitycalculationoperationasshowninEquation8.
Affinity(X )=σ(W δ(W ChannelStat(X ))) (8)
in 2 1 in
whereδandσrepresentnon-linearityfunctions,andW andW arelearnableweights. Theroleofσ
1 2
istoassignanimportanceweighttoeachchanneltocomputetheaffinity. Theresultoftheaffinity
calculationisusedtorecalibratetheoutputoftheGroupedMambaoperator,asshowninEquation9.
X =CAM(X ,Affinity(X ))=X ·Affinity(X ) (9)
CAM GM in GM in
whereX istherecalibratedoutput,X istheconcatenatedoutputofthefourVSSSgroupsfrom
CAM GM
Equation6,X istheinputtensor,andAffinity(X )arethechannel-wiseattentionscoresobtained
in in
fromthechannelaffinitycalculationoperationinEquation8.
63.4 DistilledLossFunction
AsmentionedearlierinthemotivationinSection3,theMambatrainingisunstablewhenscaled
tolargemodels[45]. Tomitigatethisissue,weproposetoutilizeadistillationobjectionalongside
thestandardcross-entropyobjective. Knowledgedistillationinvolvestrainingastudentmodelto
learnfromateachermodel’sbehaviorbyminimizingacombinationoftheclassificationlossand
distillationloss. Thedistillationlossiscomputedusingthecross-entropyobjectivebetweenthelogits
oftheteacherandstudentmodels. Giventhelogits(Z )fromthestudentmodel,logits(Z )froma
s t
teachermodel(RegNetY-16G[47]inourcase),thegroundtruthlabely,andtheharddecisionofthe
teachery =argmax Z (c),thejointlossfunctionisdefinedasshowninEquation10.
t c t
L =αL (Z ,y)+(1−α)L (Z ,y ). (10)
total CE s CE s t
whereL isthecross-entropyobjectiveandαistheweightingparameter. Weexperimentallyshow
CE
in Section 4 that training with this distillation objective stabilizes training, leading to consistent
performancegainsonlargermodelvariants.
4 Experiments
4.1 ImageClassification
Settings: TheimageclassificationexperimentsarebasedonImageNet-1K[9],whichcomprisingof
over1.28milliontrainingimagesand50Kvalidationimages,spanning1,000categories. Follow-
ing[35],wetrainourmodelsforusingtheAdamW[38]optimizerandacosinedecaylearningrate
schedulerfor300epochs,includinga20epochwarm-up. Thetotalbatchsizeissetto1024,with
modelstrainedon8xA100GPUs,eachwith80GBofCUDAmemory. Optimizerbetasaresetto
(0.9,0.999);momentumissetto0.9,andaninitiallearningrateof1×10−3isusedwithaweight
decayof0.05. Labelsmoothingof0.1isusedalongsidethedistillationobjective(seeSection3.4).
Results: Table1presentsacomparisonofourproposedGroupMambamodels(T,S,B)withvarious
state-of-the-artmethods. TheGroupMambamodelsexhibitanotablebalanceofaccuracyandcompu-
tationalefficiency. GroupMamba-Tachievesatop-1accuracyof83.3%with23millionparameters
and 4.5 GFLOPs, outperforming ConvNeXt-T [37] and Swin-T [36] by 1.2% and 2.0%, respec-
tively,withfewerparameters. Additionally,GroupMamba-TsurpassestherecentlyintroducedSSM
models,outperformingVMamba-T[34]andLocalVMamba-T[26]by0.8%and0.6%,respectively,
whileusing26%fewerparametersthanVMamba-T.GroupMamba-S,with34millionparameters
and7.0GFLOPs, achievesanaccuracyof83.9%, surpassingVMamba-S[34], Swin-S[36], and
EfficientVMamba-B[46]. TheperformanceisbetterthanLocalVMamba-S[26]by0.2%with32%
fewerparameters. Furthermore,GroupMamba-Bachievesanaccuracyof84.5%withonly57million
parametersand14GFLOPs,exceedingVMamba-B[34]by0.6%whileusing36%fewerparameters.
4.2 ObjectDetectionandInstanceSegmentation
Settings: WeevaluatetheperformanceofGroupMamba-TforobjectdetectionontheMS-COCO
2017dataset[32].OurmethodisbasedontheMask-RCNN[21]detectorwiththehyperparametersas
usedforSwin[36]. WeusetheAdamW[38]optimizerandtrainMask-RCNNwithGroupMamba-T
backbonefor12epochs. Thebackboneisinitializedandfine-tunedfromtheImageNet-1K[9]. We
useaninitiallearningrateof1×10−4anddecaybyafactorof10atepochs9and11.
Results: Table2showstheresultsofGroupMamba-T,comparingitagainstvariousstate-of-the-art
modelsforobjectdetectionandinstancesegmentationusingtheMaskR-CNNframeworkontheMS-
COCOdataset. OurmodelachievesboxAP(APb)of47.6andmaskAP(APm)of42.9. Itsurpasses
ResNet-50 [22], Swin-T [35], ConvNeXt-T [37]. In addition, GroupMamba-T has competitive
performancecomparedtoVMamba-T[34]andLocalVMamba-T[26],withless20%parameters
comparedtoVMamba-T.InFigure3,wepresentthequalitativeresultsofGroupMamba-Tonsamples
fromtheCOCOvalidationset[32], demonstratingitsperformanceininstancesegmentationand
objectdetection. Ourmodelaccuratelylocalizesobjectsandcorrectlysegmentsthemacrossdiverse
scenesandvaryingscales.
7Table1: PerformancecomparisonofGroupMambamodelswithstate-of-the-artconvolution-
based,attention-based,andSSM-basedmodelsonImageNet-1K[9]. Ourmodelsdemonstrate
superiorperformanceandachieveabettertrade-offbetweenaccuracyandmodelparameters.
Token Image
Method #Param. FLOPs Top-1acc.
mixing size
RegNetY-8G[47] Conv 2242 39M 8.0G 81.7
RegNetY-16G[47] Conv 2242 84M 16.0G 82.9
EffNet-B4[56] Conv 3802 19M 4.2G 82.9
EffNet-B5[56] Conv 4562 30M 9.9G 83.6
EffNet-B6[56] Conv 5282 43M 19.0G 84.0
DeiT-S[57] Attention 2242 22M 4.6G 79.8
DeiT-B[57] Attention 2242 86M 17.5G 81.8
DeiT-B[57] Attention 3842 86M 55.4G 83.1
ConvNeXt-T[37] Conv 2242 29M 4.5G 82.1
ConvNeXt-S[37] Conv 2242 50M 8.7G 83.1
ConvNeXt-B[37] Conv 2242 89M 15.4G 83.8
Swin-T[36] Attention 2242 28M 4.6G 81.3
Swin-S[36] Attention 2242 50M 8.7G 83.0
Swin-B[36] Attention 2242 88M 15.4G 83.5
ViM-S[70] SSM 2242 26M - 80.5
VMamba-T[34] SSM 2242 31M 4.9G 82.5
VMamba-S[34] SSM 2242 50M 8.7G 83.6
VMamba-B[34] SSM 2242 89M 15.4G 83.9
LocalVMamba-T[26] SSM 2242 26M 5.7G 82.7
LocalVMamba-S[26] SSM 2242 50M 11.4G 83.7
EfficientVMamba-B[46] SSM 2242 33M 4.0G 81.8
GroupMamba-T SSM 2242 23M 4.6G 83.3
GroupMamba-S SSM 2242 34M 7.0G 83.9
GroupMamba-B SSM 2242 57M 14G 84.5
4.3 SemanticSegmentation
Settings: WealsoevaluatetheperformanceofGroupMamba-Tforsemanticsegmentationonthe
ADE20K[69]dataset. TheframeworkisbasedontheUperNet[64]architecture,andwefollowthe
samehyperparametersasusedfortheSwin[36]backbone.Morespecifically,weusetheAdamW[38]
optimizerforatotalof160kiterationswithaninitiallearningrateof6×10−5. Thedefaultinput
resolutionusedinourexperimentsis512×512.
Results: The GroupMamba-T model demonstrates favorable performance in semantic segmen-
tation compared to various state-of-the-art methods, as presented in Table 3. GroupMamba-T
achievesamIoUof48.6insingle-scaleand49.2inmulti-scaleevaluation,with49M parameters
and955GFLOPs. ThisoutperformsResNet-50[22],Swin-T[36],andConvNeXt-T[37]. Addition-
ally,GroupMamba-TexceedstheperformanceoftherecentSSMmethods,includingViM-S[70],
VMamba-T[34],andLocalVMamba[26]withfewernumberofparameters. InFigure4,weshow
qualitativeresultsofGroupMamba-TonsamplesfromtheADE20K[69]validationsetforsemantic
segmentation.Thefirstrowshowsthegroundtruthmasks,whilethesecondrowdisplaysthepredicted
masks. Itisnotablethatourmodeldelineatesthemasksaccurately,highlightingtheeffectiveness
ofsemanticsegmentation. ThequantitativeandqualitativeresultsofGroupMambademonstratethe
robustgeneralizationcapabilityofourGroupMambabackbonesacrossdiversedownstreamtasks,
includingsemanticsegmentation,objectdetection,andinstancesegmentation.
8Table 2: Performance comparison for object detection and instance segmentation on MS-
COCO[32]usingMaskR-CNN[21]: APbandAPmsignifyboxAPandmaskAP,respectively.
FLOPs,arecomputedforaninputdimensionof1280×800.
MaskR-CNN1×schedule
Backbone APb APb APb APm APm APm #param. FLOPs
50 75 50 75
ResNet-50[22] 38.2 58.8 41.4 34.7 55.7 37.2 44M 260G
Swin-T[36] 42.7 65.2 46.8 39.3 62.2 42.2 48M 267G
ConvNeXt-T[37] 44.2 66.6 48.3 40.1 63.3 42.8 48M 262G
PVTv2-B2[62] 45.3 67.1 49.6 41.2 64.2 44.4 45M 309G
VMamba-T[34] 47.4 69.5 52.0 42.7 66.3 46.0 50M 270G
LocalVMamba-T[26] 46.7 68.7 50.8 42.2 65.7 45.5 45M 291G
GroupMamba-T 47.6 69.8 52.1 42.9 66.5 46.3 40M 279G
Figure3: QualitativeresultsofGroupMamba-Tforobjectdetectionandinstancesegmentationon
theCOCOvalidationset.
4.4 AblationStudy
Table4illustratestheeffectofeachofourcontributions,comparedtoVMamba-T.Theproposed
GroupMamba-TwithouttheCAMmoduleachievesatop-1accuracyof82.2%inthesecondrow.
IntegratingtheproposedCAMmodule,asshowninthethirdrow,improvesperformanceby0.3%,
with comparable parameters and FLOPs. In the fourth row, we expand the number of channels
to match the parameter count of VMamba-T (without using distillation). Our GroupMamba-T*
surpasses VMamba-T by 0.6% with an equivalent number of parameters. In the final row, we
incorporatetheproposeddistillationlosswithoutexpandingthechannels,whichboostsperformance
by0.8%comparedtoVMamba-Twhileusing26%fewerparameters.
9Table 3: Performance comparison for semantic segmentation on ADE20K [69] using Uper-
Net[64]. Theterms’SS’and’MS’refertoevaluationsconductedatsingle-scaleandmulti-scale
levels,respectively. FLOPsarecomputedforaninputdimensionof512×2048.
method cropsize mIoU(SS) mIoU(MS) #param. FLOPs
ResNet-50[22] 5122 42.1 42.8 67M 953G
DeiT-S+MLN 5122 43.8 45.1 58M 1217G
Swin-T[36] 5122 44.4 45.8 60M 945G
ConvNeXt-T[37] 5122 46.0 46.7 60M 939G
ViM-S[70] 5122 44.9 - 46M -
VMamba-T[34] 5122 48.3 48.6 62M 948G
EfficientVMamba-B[46] 5122 46.5 47.3 65M 930G
LocalVMamba-T[26] 5122 47.9 49.1 57M 970G
GroupMamba-T 5122 48.6 49.2 49M 955G
Figure4: QualitativeresultsofGroupMamba-TforsemanticsegmentationonADE20Kvalidation
set. The first row shows the ground truth for the masks, while the second and second show the
correspondingpredictionsofourmodel.
Table4: AblationstudyonGroupMamba-TfortheproposedCAMmoduleandDistilledLoss.
Method #Param. FLOPs Top-1acc.
VMamba-T[34] 31M 4.9G 82.5
GroupMamba-Tw/oCAM 22M 4.6G 82.2
GroupMamba-TwithCAM 23M 4.6G 82.5
GroupMamba-T*withCAM 31M 5.4G 83.1
GroupMamba-TwithCAM+DistilledLoss 23M 4.6G 83.3
5 Conclusion
In this paper, we tackle the computational inefficiencies and stability challenges associated with
visualSSMsforcomputervisiontasksbyintroducinganovellayercalledtheModulatedGroup
Mamba. Wealsoproposeamulti-directionalscanningmethodthatimprovesparameterefficiencyby
scanninginfourspatialdirectionsandleveragingtheChannelAffinityModulation(CAM)operator
toenhancefeatureaggregationacrosschannels. Tostabilizetraining,especiallyforlargermodels,we
employadistillation-basedtrainingobjective. Ourexperimentalresultsdemonstratethattheproposed
GroupMambamodelsoutperformrecentSSMswhilerequiringfewerparameters.
10References
[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyE.Hinton. Layernormalization. arxivpreprint,
arXiv:1607.06450,2016.
[2] TomBrown,BenjaminMann,NickRyder,etal. Languagemodelsarefew-shotlearners. In
NeurIPS,2020.
[3] NicolasCarion,FranciscoMassa,GabrielSynnaeve,NicolasUsunier,AlexanderKirillov,and
SergeyZagoruyko. End-to-endobjectdetectionwithtransformers. InECCV,2020.
[4] GuoChen,YifeiHuang,JilanXu,BaoqiPei,ZheChen,ZhiqiLi,JiahaoWang,KunchangLi,
TongLu,andLiminWang. Videomambasuite: Statespacemodelasaversatilealternativefor
videounderstanding. arxivpreprint,arXiv:2403.09626,2024.
[5] BowenCheng, IshanMisra, AlexanderG.Schwing, AlexanderKirillov, andRohitGirdhar.
Masked-attentionmasktransformerforuniversalimagesegmentation. InCVPR,2022.
[6] XiangxiangChuetal. Twins: Revisitingthedesignofspatialattentioninvisiontransformers.
InNIPS,2021.
[7] TacoCohenandMaxWelling. Groupequivariantconvolutionalnetworks. InICML,2016.
[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei.
Deformableconvolutionalnetworks. InICCV,2017.
[9] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. ImageNet: Alarge-scale
hierarchicalimagedatabase. InCVPR,2009.
[10] AlexeyDosovitskiyetal. Animageisworth16x16words: Transformersforimagerecognition
atscale. InICLR,2021.
[11] Stéphaned’Ascoli,HugoTouvron,MatthewLLeavitt,AriSMorcos,GiulioBiroli,andLevent
Sagun. Convit: Improvingvision transformerswithsoft convolutionalinductivebiases. In
ICML,2021.
[12] HaoqiFanetal. Multiscalevisiontransformers. InICCV,2021.
[13] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré.
Hungryhungryhippos: Towardslanguagemodelingwithstatespacemodels. InICLR,2023.
[14] DanielY.Fu,HermannKumbong,EricNguyen,andChristopherRé. FlashFFTConv: Efficient
convolutionsforlongsequenceswithtensorcores. arXivpreprint,arXiv:2311.05908,2023.
[15] HaifanGong,LuoyaoKang,YitaoWang,XiangWan,andHaofengLi.nnmamba:3dbiomedical
imagesegmentation,classificationandlandmarkdetectionwithstatespacemodel.arxivpreprint,
arXiv:2402.03526,2024.
[16] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces.
arxivpreprint,arXiv:2312.00752,2023.
[17] AlbertGu,KaranGoel,andChristopherRé.Efficientlymodelinglongsequenceswithstructured
statespaces. InICLR,2022.
[18] HangGuo,JinminLi,TaoDai,ZhihaoOuyang,XudongRen,andShu-TaoXia. Mambair: A
simplebaselineforimagerestorationwithstate-spacemodel. arxivpreprint,arXiv:2402.15648,
2024.
[19] TaoGuo,YinuoWang,andCaiMeng.Mambamorph:amamba-basedbackbonewithcontrastive
featurelearningfordeformablemr-ctregistration. arxivpreprint,arXiv:2401.13934,2024.
[20] RaminHasani,MathiasLechner,Tsun-HuangWang,MakramChahine,AlexanderAmini,and
DanielaRus. Liquidstructuralstate-spacemodels. arXivpreprint,arXiv:2209.12951,2022.
[21] KaimingHe,GeorgiaGkioxari,PiotrDollár,andRossGirshick. Maskr-cnn. InICCV,2017.
[22] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. InCVPR,2016.
[23] XuanhuaHe,KeCao,KeyuYan,RuiLi,ChengjunXie,JieZhang,andManZhou. Pan-mamba:
Effectivepan-sharpeningwithstatespacemodel. arxivpreprint,arXiv:2402.12192,2024.
[24] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arxiv preprint,
arXiv:1606.08415,2016.
11[25] AndrewG.Howardetal. MobileNets:Efficientconvolutionalneuralnetworksformobilevision
applications. arxivpreprint,arXiv:1704.04861,2017.
[26] Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, and Chang Xu. Localmamba:
Visual state space model with windowed selective scan. arxiv preprint, arXiv:2403.09338,
2024.
[27] AlexanderKirillovetal. Segmentanything. InICCV,2023.
[28] NikitaKitaev,LukaszKaiser,andAnselmLevskaya. Reformer: Theefficienttransformer. In
ICML,2020.
[29] AlexKrizhevsky,IlyaSutskever,andGeoffreyE.Hinton. Imagenetclassificationwithdeep
convolutionalneuralnetworks. InNeurIPS,2012.
[30] KunchangLi,XinhaoLi,YiWang,YinanHe,YaliWang,LiminWang,andYuQiao. Video-
mamba: Statespacemodelforefficientvideounderstanding. arxivpreprint,arXiv:2403.06977,
2024.
[31] DingkangLiang,XinZhou,XinyuWang,XingkuiZhu,WeiXu,ZhikangZou,XiaoqingYe,
and Xiang Bai. Pointmamba: A simple state space model for point cloud analysis. arxiv
preprint,arXiv:2402.10739,2024.
[32] Tsung-YiLin, MichaelMaire, SergeBelongie, JamesHays, PietroPerona, DevaRamanan,
C.LawrenceZitnick,andPiotrDollár. Microsoftcoco: Commonobjectsincontext. InECCV,
2014.
[33] JiarunLiu,,etal. Swin-umamba: Mamba-basedunetwithimagenet-basedpretraining. arxiv
preprint,arXiv:2402.03302,2024.
[34] YueLiu,YunjieTian,YuzhongZhao,HongtianYu,LingxiXie,YaoweiWang,QixiangYe,and
YunfanLiu. Vmamba: Visualstatespacemodel. arxivpreprint,arXiv:2401.10166,2024.
[35] ZeLiuetal. SwinTransformerV2: Scalingupcapacityandresolution. InCVPR,2022.
[36] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBaining
Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. In ICCV,
2021.
[37] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSaining
Xie. Aconvnetforthe2020s. InCVPR,2022.
[38] IlyaLoshchilovandFrankHutter. Fixingweightdecayregularizationinadam. arxivpreprint,
arXiv:1711.05101,2017.
[39] JunMa,FeifeiLi,andBoWang. U-mamba: Enhancinglong-rangedependencyforbiomedical
imagesegmentation. arxivpreprint,arXiv:2401.04722,2024.
[40] MuhammadMaazetal. Edgenext: Efficientlyamalgamatedcnn-transformerarchitecturefor
mobile vision applications. In International Workshop on Computational Aspects of Deep
Learningat17thEuropeanConferenceonComputerVision(CADL2022),2022.
[41] HarshMehta,AnkitGupta,AshokCutkosky,andBehnamNeyshabur. Longrangelanguage
modelingviagatedstatespaces. arxivpreprint,arXiv:2206.13947,2022.
[42] SachinMehtaandMohammadRastegari.Separableself-attentionformobilevisiontransformers.
TransactionsonMachineLearningResearch,2023.
[43] DepuMeng,XiaokangChen,ZejiaFan,GangZeng,HouqiangLi,YuhuiYuan,LeiSun,and
JingdongWang. Conditionaldetrforfasttrainingconvergence. InICCV,2021.
[44] Junting Pan et al. Edgevits: Competing light-weight cnns on mobile devices with vision
transformers. InECCV,2022.
[45] BadriN.PatroandVijayS.Agneeswaran. Simba: Simplifiedmamba-basedarchitecturefor
visionandmultivariatetimeseries. arxivpreprint,arXiv:2403.15360,2024.
[46] XiaohuanPei,TaoHuang,andChangXu. Efficientvmamba: Atrousselectivescanforlight
weightvisualmamba. arxivpreprint,arXiv:2403.09977,2024.
[47] I. Radosavovic, R. Kosaraju, R. Girshick, K. He, and P. Dollar. Designing network design
spaces. InCVPR,2020.
12[48] JiachengRuanandSunchengXiang. Vm-unet: Visionmambaunitformedicalimagesegmen-
tation. arxivpreprint,arXiv:,2024.
[49] Abdelrahman Shaker et al. Swiftformer: Efficient additive attention for transformer-based
real-timemobilevisionapplications. InICCV,2023.
[50] AbdelrahmanShakeretal. Efficientvideoobjectsegmentationviamodulatedcross-attention
memory. arXiv:2403.17937,2024.
[51] KarenSimonyanandAndrewZisserman. Verydeepconvolutionalnetworksforlarge-scale
imagerecognition. ICLR,2015.
[52] JimmyTHSmith,AndrewWarrington,andScottWLinderman. Simplifiedstatespacelayers
forsequencemodeling. InICLR,2023.
[53] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish
Vaswani. Bottlenecktransformersforvisualrecognition. InCVPR,2021.
[54] ChenSun,AbhinavShrivastava,SaurabhSingh,andAbhinavGupta. Revisitingunreasonable
effectivenessofdataindeeplearningera. InICCV,2017.
[55] ChristianSzegedyetal. Goingdeeperwithconvolutions. InCVPR,2015.
[56] MingxingTanandQuocV.Le. EfficientNet: Rethinkingmodelscalingforconvolutionalneural
networks. InICML,2019.
[57] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,and
HerveJegou. Trainingdata-efficientimagetransformers&distillationthroughattention. In
ICML,2021.
[58] ZhengzhongTu,HosseinTalebi,HanZhang,FengYang,PeymanMilanfar,AlanBovik,and
YinxiaoLi. Maxvit: Multi-axisvisiontransformer. InECCV,2022.
[59] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,2017.
[60] ChloeWang,OleksiiTsepa,JunMa,andBoWang. Graph-mamba: Towardslong-rangegraph
sequencemodelingwithselectivestatespaces. arxivpreprint,arXiv:2402.00789,2024.
[61] SinongWang,BelindaZ.Li,MadianKhabsa,HanFang,andHaoMa.Linformer:Self-attention
withlinearcomplexity. arXivpreprintarXiv:2006.04768,2020.
[62] WenhaiWangetal. Pvtv2: Improvedbaselineswithpyramidvisiontransformer. InComputa-
tionalVisualMedia,2022.
[63] SanghyunWooetal. Convnextv2: Co-designingandscalingconvnetswithmaskedautoen-
coders. InCVPR,2023.
[64] TeteXiao,YingchengLiu,BoleiZhou,YuningJiang,andJianSun. Unifiedperceptualparsing
forsceneunderstanding. InECCV,2018.
[65] SainingXie,RossGirshick,PiotrDollár,ZhuowenTu,andKaimingHe. Aggregatedresidual
transformationsfordeepneuralnetworks. InCVPR,2017.
[66] Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, and Jianfeng Gao. Focal modulation
networks. InNeurIPS,2022.
[67] YijunYang,ZhaohuXing,andLeiZhu. Vivim: avideovisionmambaformedicalvideoobject
segmentation. arxivpreprint,arXiv:2401.14168,2024.
[68] Hao Zhang et al. Dino: Detr with improved denoising anchor boxes for end-to-end object
detection. InICLR,2022.
[69] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
Sceneparsingthroughade20kdataset. InCVPR,2017.
[70] LianghuiZhu,BenchengLiao,QianZhang,XinlongWang,WenyuLiu,andXinggangWang.
Visionmamba: Efficientvisualrepresentationlearningwithbidirectionalstatespacemodel.
arxivpreprint,arXiv:2401.09417,2024.
[71] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,andJifengDai. Deformabledetr:
Deformabletransformersforend-to-endobjectdetection. InICLR,2021.
13