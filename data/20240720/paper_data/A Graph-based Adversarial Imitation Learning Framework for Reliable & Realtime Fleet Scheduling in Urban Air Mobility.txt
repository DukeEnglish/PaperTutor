A Graph-based Adversarial Imitation Learning Framework for Reliable &
Realtime Fleet Scheduling in Urban Air Mobility
Prithvi Poddar1, Steve Paul2 and Souma Chowdhury3
University at Buffalo, Buffalo, NY, 14260
Abstract—TheadventofUrbanAirMobility(UAM)presents domain of UAM fleet scheduling at the forefront of innova-
the scope for a transformative shift in the domain of urban tions. Following the air-space and aircraft safety constraints
transportation.However,itswidespreadadoptionandeconomic
while being robust against dynamic environmental changes,
viability depends in part on the ability to optimally schedule
mitigating energy footprint, and maximizing profitability
the fleet of aircraft across vertiports in a UAM network,
under uncertainties attributed to airspace congestion, chang- makes developing an optimal scheduling policy challenging.
ing weather conditions, and varying demands. This paper The complexities inherent in UAM fleet scheduling are
presents a comprehensive optimization formulation of the fleet characterized by:
schedulingproblem,whilealsoidentifyingtheneedforalternate
Dynamic environments where real-time factors such as
solutionapproaches,sincedirectlysolvingtheresultinginteger
airspacecongestion,weatherconditions,demanduncertainty
nonlinear programming (INLP) problem is computationally
prohibitivefordailyfleetscheduling.Previousworkhasshown characterized by dynamic traffic patterns, changing weather
the effectiveness of using (graph) reinforcement learning (RL) conditions, and regulatory constraints demand intelligent
approaches to train real-time executable policy models for scheduling solutions that can swiftly adapt to real-time
fleet scheduling. However, such policies can often be brittle on
challenges [1].
out-of-distribution scenarios or edge cases. Moreover, training
State information sharing among all vertiports and eV-
performance also deteriorates as the complexity (e.g., number
of constraints) of the problem increases. To address these TOLs is necessary for trajectory and speed adjustments to
issues,thispaperpresentsanimitationlearningapproachwhere ensure the safety of the operations [3].
the RL-based policy exploits expert demonstrations yielded Scarce resources such as vertiport parking lots, charging
by solving the exact optimization using a Genetic Algorithm.
stations, and air corridors must be optimally allocated to
The policy model comprises Graph Neural Network (GNN)
prevent unnecessary delays and conflicts [4].
basedencodersthatembedthespaceofvertiportsandaircraft,
Transformer networks to encode demand, passenger fare, and Contemporary solutions to such scheduling problems take
transport cost profiles, and a Multi-head attention (MHA) the form of complex nonlinear Combinatorial Optimiza-
based decoder. Expert demonstrations are used through the tion (CO) problems [5], which can be addressed through
Generative Adversarial Imitation Learning (GAIL) algorithm.
classical optimization, heuristic search, and learning-based
Interfaced with a UAM simulation environment involving 8
approaches. While these approaches provide local optimal
vertiports and 40 aircrafts, in terms of the daily profits
earned reward, the new imitative approach achieves better solutions for small UAM fleets [6]–[8], they often present
mean performance and remarkable improvement in the case computational complexities that render them impractical
of unseen worst-case scenarios, compared to pure RL results. for online decision-making and larger fleets. Furthermore,
despite the computational expenses, these methods would
I. INTRODUCTION
still be viable options if we disregard uncertainties in the
In the modern landscape of urban transportation, the eVTOL’s operations (e.g. occasional failures of the air-
emergence of Urban Air Mobility (UAM) introduces a revo- craft) and environmental constraints (like the closure of air
lutionary dimension to the concept of commuting. As cities corridors due to bad weather conditions). However, these
grapple with increasing populations and traffic congestion, factors cannot be disregarded because of the safety concerns
the concept of UAM proposes the use of electric vertical they impose, making it especially challenging to use such
take-off and landing (eVTOL) aircraft [1] as an alternate approaches.
formofautomatedairtransportation.Withaprojectedmarket In the pursuit of addressing these complexities and pre-
sizeof$1.5trillionby2040[2],itseconomicviabilitywillbe sentinganefficientonlinescheduler,weextendourprevious
drivenbytheabilitytooperateasufficientlylargenumberof work in [9] by proposing a specialized Graph Neural Net-
eVTOLs in any given market (high-penetration), placing the works (GNN) [10], [11] based adversarial imitation learning
framework that learns from the optimal schedules (expert
1Graduate Student, Department of Mechanical and Aerospace data) generated by classical optimization algorithms and
Engineering, University at Buffalo, AIAA student member. builds upon existing multi-agent task allocation methods for
prithvid@buffalo.edu
2Graduate Student, Department of Mechanical and Aerospace scaling up to larger UAM fleets. We begin by posing the
Engineering, University at Buffalo, AIAA student member. UAMfleetschedulingproblemasanIntegerNon-LinearPro-
stevepau@buffalo.edu
gramming (INLP) problem with a small number of eVTOLs
3Associate Professor, Department of Mechanical and Aerospace
and vertiports and generate optimal solutions using an elitist
Engineering, University at Buffalo, AIAA Associate Fellow.
soumacho@buffalo.edu Genetic Algorithm. These form the expert demonstrations
4202
luJ
61
]GL.sc[
1v31121.7042:viXraUAM Network
CapTAIN-GAIL CapTAIN-GAIL
eVTOL 0 goes eVTOL 1 goes
to vertiport 1 to vertiport 0
4 2 4 2
1 1
Transition
2
0 0
2
0
0
1 3 1 3
Time = t Time = t+1
3 - Vertiport - Edges in Vertiport Graph - eVTOL Graph - Passenger Fare
- eVTOL - Edges in eVTOL Graph - Passenger Transportation - Corridor Availibility
Cost
- eVTOL taking - Vertiport Graph - Demand Model
decision
Fig. 1: A visual representation of the UAM fleet management problem. CapTAIN-GAIL is the imitation learning policy that
takes as input, the information from the vertiport and eVTOL graphs along with operational costs, passenger demands, and
air-corridor availabilities and decides the next vertiport that the decision-making agent should go to.
for the imitation learning policy. We use the Generative ingconcept.However,aconsiderableportionofthisliterature
Adversarial Imitation Learning (GAIL) [12] algorithm to neglectscrucialguidelinesoutlinedbytheFAA[1]concern-
traintheGNNpolicy.Then,weestablishhowagraphrepre- ing UAM airspace integration, particularly in aspects such
sentation efficiently encodes all the state-space information as air corridors, range/battery constraints, and unforeseen
of the vertiports and the eVTOLs, thus motivating the use events like route closures due to adverse weather or eVTOL
of GNNs over regular neural networks. We also present a malfunctions [7], [13], [14]. Traditional methods, includ-
comparative study between a pure reinforcement learning- ing Integer Linear Programming (ILP) and metaheuristics,
based method and a Genetic Algorithm, that demonstrates proveinadequateforefficientlysolvingrelatedNP-hardfleet
edgecaseswherestandardRLmethodologiesfailtogenerate scheduling problems [15]–[20]. For context, we focus on
an optimal solution and require access to expert data to hour-ahead planning to allow flexibility in adapting to fluc-
train an optimal policy. Finally, we compare our results with tuatingdemandandrouteconditionsaffectedbyweatherand
standard RL-based and Genetic Algorithm based solutions unforeseenaircraftdowntime.Whilelearning-basedmethods
and demonstrate that our method performs better than RL- haveshownpromiseingeneratingpoliciesforcombinatorial
basedmethods,intermsofprofitabilityandwhileitperforms optimization problems with relatable characteristics [11],
equally to a Genetic Algorithm, it is significantly faster in it [21]–[27], their current forms often oversimplify complex-
termsofcomputationaltimeswhen comparedtotheGenetic ities or tackle less intricate problem scenarios. In response,
Algorithm. Fig.1 presents a diagrammatic representation of our paper introduces a centralized learning-based approach
the UAM fleet management problem. tailored to address the complexities and safety guidelines
inherent in UAM fleet scheduling. Our proposed approach
II. RELATEDWORKS
involvesthecreationofasimulationenvironmentthatencap-
The existing body of work in Urban Air Mobility (UAM) sulatesthesecomplexities,facilitatingthetrainingofapolicy
fleet planning has witnessed notable growth, propelling the network for generating hour-ahead sequential actions for
optimization and learning formalism to advance this emerg- eVTOLsacrossageneric12-houroperationalday.Thepolicynetwork integrates a Graph Capsule Convolutional Neural passenger demand Q(i,j,t) and the operational cost R
i,j
Network (GCACPN) [28], [29] for encoding vertiport and and is computed as Fpassenger =R ×Q (i,j,t), where
i,j,t i,j factor
eVTOLstateinformationpresentedasgraphs,aTransformer Q (i,j,t)=max(log(Q i,j,t)/10),1). A constant elec-
factor (
encodernetworkforassimilatingtime-seriesdataondemand tricity pricing, Priceelec=$0.2/kWh is used [31].
and fare, and a feedforward network for encoding passen- The eVTOL vehicle model is considered to be the City
ger transportation cost. Additionally, a Multi-head Attention Airbus eVTOL [32] with a battery model similar to the one
mechanism is employed to fuse the encoded information described in [8]. The aircraft has a maximum cruising speed
andproblem-specificcontext,enhancingsequentialdecision- of 74.5 mph and a passenger seating capacity of 4. The
making [25], [30]. operating cost of this vehicle is $0.64 per mile [8]. It has
a maximum battery capacity of B =110 kWh. Let Bk
max t
III. PROBLEMDESCRIPTIONANDFORMULATION
represent the charge left in eVTOL k at time t and if the
A. UAM Fleet Scheduling as an Integer Non-Linear Pro- eVTOL travels from vertiport i to j, then the charge at the
gramming Problem next time step is B tk
+1
=B tk−Bc i,h jarge , where Bc i,h jarge is the
charge required to travel from i to j.
We begin by posing the UAM fleet scheduling problem
We consider an operating time horizon of T hours (with
as an optimization problem [9], enabling us to use a Ge-
starttimeTstart andendtimeTend)andtheaimoftheUAM
netic Algorithm to generate the expert solution. Consider a
scheduling problem is to maximize the profits earned in the
UAM network comprising N vertiports and N number of
K
T hours. This is achieved by smartly assigning (based on
eVTOLs, each with a maximum passenger carrying capacity
the demand, battery charge, operation cost, etc.) a journey
of C(= 4). Let V and K denote the set of all vertiports
toaneVTOLduringeachdecision-makinginstancet∈T.A
and eVTOLs, respectively, with each vertiport i∈V capable
of accommodating a maximum of
Cpark(=10)
number of
journey is defined as the commute of an eVTOl between
max two vertiports i and j. If i = j, the eVTOL has to wait
eVTOLswhilealsohavingchargingstationsforCcharge(=6)
max for Twait(=15) minutes at the vertiport before assigning a
eVTOLS. Some vertiports might not have charging stations
journeyinthenextdecision-makinginstance.Weassumethat
and are called vertistops (V ⊂V). We consider there to be
S each eVTOL can take off at any time within the T horizon,
4 air corridors between two vertiports with two corridors
and we consider the schedule for a single-day operation
for each direction. Each vertiport i ∈V has an expected
where the operational time begins at 6:00 AM (Tstart) and
take-off delay TTOD that affects every eVTOL taking off,
i ends at 6:00 PM (Tend).
and we consider this estimated take-off delay to be less
Optimization formulation: Based on the notations de-
than 6 minutes at each vertiport. The probability of route
scribed above, the optimization problem is formulated as
closure between any two vertiports i and j is considered
follows(similartoourpreviouswork[9]).ForeveryeVTOL
to be Pclosure(≤0.05) while the probability of an eVTOL k
i,j k ∈V , let Sjour be the set of journeys taken during time
becoming dysfunctional is considered to be P kfail(≤0.005). periode of T k and, Npassengers be the number of passengers
Every episode considers a randomly assigned take-off delay i,l
transported during the trip, l∈Sjour by eVTOL k. Let Bk be
(≤30 mins) that is drawn from a Gaussian distribution with k l
the battery charge of eVTOL k just before its l’th journey,
meanTTODandastandarddeviationof6minutes.Additional
i Vstart and Vend be the respective start and end vertiports
assumptions that are made regarding the problem setup are: k,l k,l
1) The decision-making is done by a central agent that has of eVTOL k during it l’th journey, T kt ,a lkeoff and T kl ,a lnding be
access to the full observation of the states of the eVTOLs the corresponding takeoff time and landing time. Then the
andthevertiports,and2)AneVTOLcantravelbetweenany objectiveoftheoptimizationproblemistomaximizethenet
two vertiports given it has sufficient battery charge and the profit z that can be computed by subtracting the operational
resistive loss of batteries is negligible. costs CO and the cost of electricity CE from the revenue
We define R as the operation cost of transporting a generated. These values are computed as:
ij
passenger from vertiport i to j while Fpassenger is the price
ijt
a passenger is charged for traveling from i to j at time
t, that is based on passenger demand. We follow the de- CO= ∑ ∑ N ip ,lassengers×R i,j ,i=V ks ,t lart,j=V ke ,n ld (1)
mand model as mentioned in our previous work [9]. Let k∈Kl∈Sjour
k
Q(I,j,t) represent the demand between vertiports i and CE = ∑ ∑ Priceelec×Bcharge ,i=Vstart,j=Vend (2)
i,j k,l k,l
j at time t. A subset of vertiports V ⊂V are consid-
B k∈Kl∈Sjour
ered to be high-demand vertiports and we account for two k
peak hours of operation: 8 : 00−9 : 00 am (Tpeak1) and Revenue= ∑ ∑ N ip ,lassengers×F i,p ja ,tssenger
4 : 00−5 : 00 pm (Tpeak2) such that V
B
experience high k∈Kl∈S kjour (3)
demands during both peak hours. The demand is high from ,i=Vstart,j=Vend
k,l k,l
V −V to V during Tpeak1 and vice-versa during Tpeak2.
B B
The passenger fare is computed by adding a variable fare Therefore, the objective function can be formulated as:
Fpassenger to a fixed base fare of Fbase =$5. The variable
fare between two vertiports i and j is dependent on the maxz=Revenue−CO−CC (4)With the following constraints: V (= V) is the set of vertiports, E represents the set
v v
Npassengers=min(C,Q (I,j,t)) of edges/routes between the vertiports, and A v represents
i,l act the adjacency matrix of the graph. To consider the route
(5)
,i=Vstart,j=Vend,t=Ttakeoff∀l∈Sjour
closure probability, a weighted adjacency matrix is com-
k,l k,l k,l k
Bk >Bcharge puted as A v = (1 N×N−Pclosure)×A. The node properties
T kt ,a lkeoff i,j (6) δ it of the vertiport i ∈V v at time t are defined as δ it =
,i=V ks ,t lart,j=V ke ,n ld,∀k∈K,∀l∈S kjour [x i,y i,C ip ,tark,T icharge,T iTOD,I ivstop], where (x i,y i) are the x-y
Cpark≤Cpark ,∀i∈V,∀t∈[Tstart,Tend] (7) coordinates of the vertiport,C ip ,tark is the number of eVTOLs
i,t max currentlyparked,Tcharge
istheearliesttimewhenacharging
V le ,knd ̸=i ,ifAi,V le ,knd =0,∀i∈V,∀k∈K,∀l∈S kjour (8) stationwillbefree,i T iTOD istheexpectedtake-offdelay,and
Ivstop
is a variable that takes the value 1 if the node is a
Where Q (I,j,t)is the actual demand,and A is a matrix i
act vertistop and 0 if the node is a vertiport.
that represents the availability of routes between vertiports.
Graph Representation of the eVTOLs: G =(V ,E ,A )
A =A =1 if the route between i and j is open and =0 e e e e
ij ji
represents the graph that encodes the information about
if the route is closed.
the eVTOL network. V represents the set of eVTOLs, E
e e
B. Generating the Expert Demonstrations represents the set of edges, and A e represents the adjacency
matrix. G is considered to be fully connected and each
GiventheINLPproblemformulationinSec.III-A,weuse e
eVTOL k ∈V is represented by it properties i.e. ψt =
an elitist Genetic Algorithm (GA) to generate the expert (cid:104) e (cid:105) k
solutions for a small number of eVTOLs (N
k
= 40) and x kd,yd k,B tk,T kflight,T kdec,P kfail ,ψ it∈R6.Here,x kd,yd k)represent
vertiports (N =8). The GA uses a population size of 100, the coordinates of the destination vertiport, Bk is the current
t
max iterations of 100, mutation probability of 0.1, elite battery level, Tflight is the next flight time, Tdec is the next
k k
ratio of 0.01, and cross-over probability of 0.5. The GA is decision making time, and Pfail is the probability of failure.
k
implemented in batches of 60 decision variables which are Action Space: At each decision-making time instance,
then simulated sequentially and the objective functions are each agent takes an action from the available action space.
computed. Upon simulating a batch of the GA solution, we The action space consists of all the available vertiports.
achieve an updated state of the environment which is then Therefore the action space will be of size N . During a
K
usedtocomputethenext60decisionvariables.Thisprocess decision-making step, if an agent chooses the vertiport at
is repeated until the episode is over. Each decision variable which it currently is, then it waits for 15 minutes in the
takesanintegervaluebetween1andN.WegeneratetheGA vertiport, until it makes a new decision.
solutions for 100 different scenarios (for each scenario, the Reward:Weconsideradelayedrewardfunctionwherethe
locations of the vertiports stay fixed) which are then used as
agentgetsarewardonlyattheendofanepisode.Thereward
expert demonstrations for the imitation learning algorithm.
is ratio of the profit earned to the maximum possible profit
C. MDP Formulation
inanepisode,i.e.∑ i∈V,j∈V,t∈[Tstart,Tend](Q(i,j,t)×F i,p ja ,tssenger).
Transition Dynamics: Since demand and electricity pric-
Having expressed the UAM fleet scheduling problem as
ingcanvaryfromthatoftheforecastedvalues,thetransition
an INLP problem, we now define it as a Markov Decision
of the states is considered to be stochastic. The transition is
Process(MDP)thatsequentiallycomputestheactionforeach
an event-based trigger. An event is defined as the condition
eVTOL at the decision-making time instantt∈T. The state,
thataneVTOLisreadyfortakeoff.Asenvironmentaluncer-
action, reward, and the transitions are described below:
tainties and communication issues (thus partial observation)
State Space: The state-space at time t consists of infor-
are not considered in this paper, only deterministic state
mation about 1) the vertiports, represented as a graph G ,
V transitions are allowed.
2) the eVTOLs, represented as a graph G , 3) the passenger
e
demandmodelQ,4)passengerfareFpassenger,5)operational
IV. PROPOSEDGRAPH-BASEDADVERSARIALIMITATION
costs R (computed based on the per mile operational cost
LEARNINGAPPROACH
of the eVTOL, the passenger demands and the price for
electricity), and 6) time when it is safe to launch an eVTOL We propose an adversarial imitation learning formulation
to the corridors Tcor∈RN×N×2. to train a transformer-aided GNN-based policy network
Atanydecision-makingtimestep,thestateinformationis called CapTAIN [9] that will assign actions to the eVTOLs.
processed by a transformer-aided multihead-attention graph The policy network takes the state information from all the
capsule convolutional neural network (that was presented eVTOLs and vertiports as input and assigns an action to the
in [9]) that acts as the policy network for the imitation eVTOLs that are ready for take-off. The policy is trained
learning algorithm. Details about the policy network have using the generative adversarial imitation learning (GAIL)
been discussed in IV-A. Further details on the state space [12] algorithm that uses the solutions generated by the GA
are as follows: as the expert demonstrations. The following section presents
Graph Representation of the Vertiport Network: G = further information about the GAIL algorithm and the state
V
(V ,E ,A )representsthevertiportnetworkasagraphwhere encoding and decoding.
v v vDecoder
3 4 Softmax
Vertiport
eVTOL
Graph Linear ( )
Graph
1 2
Batch Normalization
eVTOL Encoder Vertiport Encoder 1 2 3
GCAPCN GCAPCN Feed Forward
Action
State Batch Normalization
Demand Demand Encoder Mean
Model
Discriminator
Transformer
MHA
Passenger Fare
Passenger
Encoder
Fare Model
Transformer Linear ( ) Linear ( ) Linear ( ) State
Passenger Transport Cost
Transport Encoder
Corridor Expert Policy
Cost
Transformer Availability
Flatten
Encoder
Cost Function
CapTAIN
Fig. 2: The learning framework for CapTAIN-GAIL
A. Policy Network B. Training with Generative Adversarial Imitation Learning
The policy network π is trained on the expert demonstra-
We use the Capsule Transformer Attention-mechanism
tions generated by the Genetic Algorithm, using Generative
Integrated Network (CapTAIN) [9] as the policy network
AdversarialImitationLearning(GAIL),animitationlearning
for GAIL. CapTAIN combines graph neural networks and
approachintroducedin[12].Thepolicyislearntusingatwo-
transformers to encode the state-space information and used
player zero-sum game which can be defined as:
a multi-head attention-based decoder to generate the action.
The information from the veriports and eVTOLs (G and argminargmaxE [logD(s,a)]+
V π
G ) are first passed through a Graph Capsule Convolutional π D∈(0,1) (9)
e
Network (GCAPCN), introduced in [28], which takes the E [log(1−D(s,a))]−λH(π)
πE
graphs as the inputs and generates the feature embeddings
Whereπ isthepolicyfollowedbytheexpertdemonstration.
forthevertiportsandeVTOLs.Simultaneously,atransformer E
D is a discriminator that solves a binary classification prob-
architecture is used to compute learnable feature vectors
lemD:S×A →(0,1),whereS representsthestate-space
for the passenger demand model and the passenger fares
and A represents the action-space. D tries to distinguish
(information that can be represented as time-series data).
between the distribution of data generated by π from the
Additionally, a simple feed forward network computes the
expertdatageneratedbyπ ,WhenDcannotdistinguishdata
embeddings for the passenger transportation cost R which E
generatedbythepolicyπ fromthetruedata,thenπ hassuc-
can be represented as a N×N matrix, and another feed
cessfully matched the true data. H(λ)=E [−logπ(a|s)] is
forwardnetworkprocessesthecorridoravailability.Wekeep π
the entropy. Figure 2 shows the overall learning framework.
a track of the time at which it is safe for a new eVTOL
The discriminator (approximated by a neural network)
to enter a corridor, subject to various safety restrictions
and minimum separation, using a N×N×2 tensor, Tcor, tries to maximize the objective, while the policy π tries to
minimize it. GAIL alternates between an Adam gradient [6]
which is flattened and fed into the feed forward network.
step on the parameters w of D with the gradient:
The outputs from the transformer, GCAPCN, and the feed w
f tho erw na prd assn ee dtw to hr rk os ugf horm
a
mth ue ltie -n hc eo ad ded atte em ntb ioe nddi [n 3g 0]w dh ei cc oh deis
r
Eˆ τi[∇ wlogD w(s,a)]+Eˆ D[∇ wlog1−D w(s,a)] (10)
to generate the probabilities of choosing each action, for andaTrustRegionPolicyOptimization(TRPO)[7]gradient
the decision-making agent. Further technical details about step on the parameters θ of πheta which minimizes a cost
t
CapTAIN have been discussed in [9]. function c(s,a)=logD (s,a) with the gradient:
wi+1
etanetacnoC
seitilibaborP
noitcAalgorithms in each test scenario, is used to compare their
Eˆ [∇ logπ (a|s)Q(s,a)]−λ∇ H(π ) (11) performance and Fig.3 plots the difference in the profits
τi θ θ θ θ
earnedbyGAandCapTAINineachofthe100testscenarios.
V. EXPERIMENTSANDRESULTS
It can be seen that GA generates more profits in the
A. Simulation Details majorityofscenarios(GAperformsbetterin66outof100
We use the simulation environment presented in [9] that cases), with GA generating an average profit of $17603
is implemented in Python and uses the Open AI Gym while CapTAIN generating an average profit of $15727.
environment interface. We consider a hypothetical city cov- To test the significance of this difference, we perform a
ering an area of 50×50 sq. miles with 8 vertiports (2 of statistical T-test with the null-hypothesis being that both
which are vertistops) and 40 eVTOLs. The locations of the methods generate the same amount of profit. The p-value
−5
vertiportsremainthesamethroughouttrainingandtestingthe of the test turns out to be 3.08×10 (<0.05) which
algorithm. The rest of the operational details stay the same, means that GA has a significant statistical advantage over
as discussed above. To train the CapTAIN policy network, CapTAIN. This motivates the use of imitation learning for
we use the GAIL implementation from imitation [33], a trainingapolicythatcanmimicGAwhilebeingsignificantly
python package for imitation learning algorithms, and PPO more efficient than GA in terms of computational time.
[34] from stable-baselines3 [35].
D. Evaluating CapTAIN-GAIL
B. Training Details We train the imitation learning policy with a soft start,
We begin by training the CapTAIN policy using just i.e. we use the pre-trained CapTAIN policy from Sec.V-C as
PPO, for 1.5 million steps. Next, we generate the expert the initial policy for GAIL. The generator policy in GAIL is
demonstrations for 100 scenarios (un-seen by CapTAIN trained for 20 iterations with 20000 steps in each iteration.
during training) using a standard elitist Genetic Algorithm Thetrainedimitationlearningpolicyisthentestedonthe100
(GA) (using the parameters as described in Sec.III-B) and testcasesanditsperformanceiscomparedagainstCapTAIN
compare the performance of CapTAIN against GA. These and GA. We further test the generalizability of CapTAIN-
test scenarios are generated by fixing the seed values for GAIL on a new set of 100 unseen test cases that were not a
the random number generators in the simulation. Finally, part of the expert demonstrations.
we train the imitation learning policy (CapTAIN-GAIL) 1) Average Profits: Fig.4 plots the average profits earned
on the expert demonstration, with a soft start by beginning by all three methods, in the test scenarios that were a
the training with the pre-trained weights from CapTAIN, part of the expert demonstrations. It can be noticed that
and finally compare the performance of CapTAIN-GAIL CapTAIN generates the least profit while CapTAIN-GAIL
againstCapTAINandGA.Allthetraining,experiments,and andGAalmostearnequalprofitsonaverage.Amoredetailed
evaluations are computed on a workstation running Ubuntu analysisoftheperformanceofthethreemethodsisdiscussed
22.04.4 LTS and equipped with an Intel Core i9-12900K below.
processor and Nvidia GeForce RTX 3080 Ti graphics pro-
cessor.Furtherevaluationdetailsandresultsarediscussedin 30000
the sections ahead.
25000
20000
20000
15000
15000
10000
10000 5000
5000 CapTAIN-GAIL CapTAIN GA
Algorithm
0 Fig. 4: Comparing the average profits earned by all the 3
methods
5000
10000
2) CapTAIN-GAIL vs. CapTAIN: Fig.5 compares the dif-
1 11 21 31 41 51 61 71 81 91
Test Scenarios ference in the profits earned by CapTAIN-GAIL and Cap-
Fig.3:DifferenceintheprofitsearnedbyGAandCapTAIN, TAIN.CapTAIN-GAILgeneratesmoreprofitsthanCapTAIN
Positive values indicate better performance by GA and in73outof100cases,withtheaverageprofitofCapTAIN-
negative values indicate better performance by CapTAIN. GAILbeing$17587whiletheaverageprofitofCapTAIN
is $15727. To statistically evaluate the significance of this
difference,weconductaT-testwiththenull-hypothesisbeing
C. Comparing CapTAIN vs. Genetic Algorithm thatboththealgorithmsgenerateequalaverageprofits.With
−5
We begin by testing the performance of a policy trained a p-value of 6.55×10 (< 0.05), we have statistical
purely using reinforcement learning (i.e. CapTAIN trained evidence that CapTAIN-GAIL performs significantly better
using PPO) against GA. The daily profit earned by both the than CapTAIN.
)$(
)NIATpaC
-
AG(
stiforp
ni
ecnereffiD
)$(
tiforPgenerate similar amount for profits, and this discrepancy in
10000 the performance of these two algorithms shall remain the
subject of future research.
5000
15000
0
10000
5000
5000
10000
0
1 11 21 31 41 51 61 71 81 91
Test Scenarios 5000
Fig. 5: Difference in the profits earned by CapTAIN-GAIL
10000
and CapTAIN, Positive values indicate better performance
by CapTAIN-GAIL and negative values indicate better per- 15000
101 111 121 131 141 151 161 171 181 191
formance by CapTAIN. Test Scenarios
Fig. 7: Difference in the profits earned by CapTAIN-GAIL
and CapTAIN in completely unseen scenarios. Positive val-
3) CapTAIN-GAIL vs. GA: Next, we evaluate the perfor- ues indicate better performance by CapTAIN-GAIL and
mance of CapTAIN-GAIL against GA. With Fig.6 plotting negative values indicate better performance by GA.
thedifferenceintheprofitsearnedbythesetwomethods,we
can see that CapTAIN-GAIL performs better than GA in 55 4) Generalizability Analysis: To test the generalizability
outofthe100cases,whichisroughlyhalfthenumberoftest of CapTAIN-GAIL, we test it on a new set of 100 unseen
cases. The average profit earned by GA is $17603 while scenariosthatwerenotapartoftheexpertdemonstrationson
the average profit earned by CapTAIN-GAIL is $17587. which it was trained, and compare it’s performance against
UponperformingaT-testwiththenullhypothesisbeingthat CapTAIN. Fig.7 plots the difference in the profits earned by
bothmethodsearnthesameaverageprofit,wegetap-value CapTAIN-GAILandCapTAIN.WhencomparedtoFig.5,we
of 0.97(> 0.05) which indicates that statistically, both notice that CapTAIN-GAIL shows improvement in terms of
methods have a similar performance. This result matches performing better in cases where CapTAIN generates more
ourexpectationssincetheimitationlearningpolicyshouldat profits (which can be seen by the reduced negative values in
least perform similar to the expert when tested on the expert Fig.7).
demonstrations. Further analysing the mean profits earned by both the
methods in the unseen scenarios (as shown in Fig.8) and
10000 comparing it to Fig.4, we notice that CapTAIN-GAIL
achieves better mean performance and remarkably better
5000 bounding of the unseen worst-case scenarios (that can be
seen in the reduce standard deviation between Fig.4 and
0 Fig.8), when compared to CapTAIN. In the unseen sce-
narios, CapTAIN-GAIL generates an average profit of
5000 $17813ascomparedto$17587intheexpertdemonstration
cases while CapTAIN generates $15902. Additionally,
10000 the standard deviation of the profits earned, goes down
1 11 21 31 41 51 61 71 81 91 from $3363 in the expert demonstration cases to $3161
Test Scenarios
in the unseen cases. This shows that CapTAIN-GAIL is
Fig. 6: Difference in the profits earned by CapTAIN-GAIL
generalizable across scenarios that are unseen in the expert
and GA, Positive values indicate better performance by
demonstrations.ConductingaT-testwiththenull-hypothesis
CapTAIN-GAIL and negative values indicate better perfor-
being that both the methods generate the same average
mance by GA. −5
profits, gives a p-value of 3.09×10 (<0.05). Which
meansthatCapTAIN-GAILperformsstatisticallybetterthan
Even though both CapTAIN-GAIL and GA perform sim- CapTAIN.
ilarly in terms of the average profits they generate, it shall 5) Further Analysis: To further analyse the performance
be noticed in Fig.6 that the difference in the profit values difference among the three methods, we look at their com-
are quite large. This indicates that in cases where GA putation times as well as track the average number of idle
performs better, it outperforms CapTAIN-GAIL by a large decisions and flight decisions taken by the three methods.
margin while the opposite is also true, i.e. CapTAIN-GAIL The computing time for GA is calculated by adding the
outperforms GA by a large margin, in the cases where it total time required to generate the solutions for an entire
performs better. One would ideally assume the difference episode. Similarly, the computation times for CapTAIN and
in the profits to be close to 0, given that both the methods CapTAIN-GAIL are calculated based on the total forward-
)$(
)NIATpaC
-
LIAG-NIATpaC(
ecnereffiD
tiforP
)$(
)AG
-
LIAG-NIATpaC(
stiforp
ni
ecnereffiD
)$(
)NIATpaC
-
LIAG-NIATpaC(
ecnereffiD
tiforP30000 Algorithm
200
CapTAIN-GAIL
25000 CapTAIN
150 GA
20000
100
15000
10000 50
5000
0
# idle decisions # flight decision
CapTAIN-GAIL CapTAIN
Metric
Algorithm
(a) Comparing the number of idle decisions and number of flight
Fig. 8: Comparing the average profits earned by CapTAIN-
decisions taken by the 3 algorithms.
GAIL and CapTAIN in the unseen test cases.
Algorithm
200
CapTAIN-GAIL
propagation time for the entire episode. It was found that CapTAIN
GA took an average of 783.48±267.54 seconds per episode 150
while CapTAIN and CapTAIN-GAIL took 2.57±2.24 sec-
onds and 1.86±2.25 seconds respectively. Thus, there is a
significantadvantageofusingapolicytrainedusingimitation 100
learning when compared to an optimization solver like GA,
in terms of computational time required.
50
Additionally, Fig.9a shows the average idle decisions and
flight decisions taken by all the three algorithms in the sce-
narios present in the expert demonstrations. It can be noted 0
thatGAtakethemostnumberofflightdecisionsandtheleast # idle decisions # flight decision
Metric
number of idle decisions where as CapTAIN takes the least
numberofflightdecisionsandmostnumberofidledecisions. (b)Comparingthenumberofidledecisionsandflightdecisionstaken
Meanwhile, the decisions taken by CapTAIN-GAIL closely by the 2 algorithms in the unseen scenarios.
resemble those taken by GA, which is expected given that Fig. 9: Comparing the number of idle decisions and flight
we have already seen that CapTAIN-GAIL performs similar decisions taken by different algorithms under seen and un-
to the GA. seen test scenarios.
In comparison, Fig.9b show the decisions taken by
CapTAIN-GAIL and CapTAIN in the unseen test cases. It
can be noted that the decisions remain fairly the same for
both the algorithms. Additionally, the number of outliers in gorithm or GA (that is impractical to be used directly/online
the unseen cases for CapTAIN-GAIL are lower than that for due to its high computing cost). While prior work had
the expert demonstration cases. On an average, CapTAIN- shownthatgraphRLisuniquelycapableoflearningpolicies
GAIL takes 51.84 idle decisions and 187.29 fight decisions with performance clearly better than standard RL or other
in the seen cases and 52.36 idle decisions and 189.48 flight online approaches, there is still a significant optimality gap
decisions in the unseen cases, while CapTAIN takes 72.11 when compared with partly-converged results from a global
idle decisions and 179.96 fight decisions in the seen cases optimizer such as GA. The imitation learning extension
and 71.8 idle decisions and 178.39 flight decisions in the presented here (CapTAIN-GAIL) was found to successfully
unseen cases. This further demonstrates the generalizability reduce this optimality gap, with the initial policy seeded
of CapTAIN-GAIL. by the earlier CapTAIN results. Our case studies involved
schedulingtheflightsof40aircraftacross8vertiports.When
VI. CONCLUSIONANDFUTUREWORK
tested over 100 seen scenarios, CapTAIN-GAIL generated
In this paper, we developed CapTAIN-GAIL, a graph solutions with performance that is statistically similar to the
neural network based policy for Urban Air Mobility fleet solutions generated by the GA. Both across seen (during
scheduling, which presents itself as a challenging combi- training) and unseen scenarios the new CapTAIN-GAIL
natorial optimization problem. CapTAIN-GAIL is trained results (in terms of the profit metric) are found to be sig-
using generative-adversarial imitation learning (GAIL), with nificantly better than those by CapTAIN, thereby supporting
(offline) expert demonstrations generated by a Genetic Al- our hypothesis of taking an imitation learning approach
)$(
tiforPto reduce the optimality gap and improve performance. [12] J. Ho and S. Ermon, “Generative adversarial imitation
Notably CapTAIN-GAIL learnt to take more flight decisions learning,” in Advances in Neural Information Processing
Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
than staying idle decisions for the UAM aircraft, compared
R. Garnett, Eds., vol. 29. Curran Associates, Inc., 2016.
to CapTAIN, which might be partly responsible for the [Online]. Available: https://proceedings.neurips.cc/paperfiles/paper/
improved performance. Interestingly, when comparing the 2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf
[13] M. Fernando, R. Senanayake, H. H. Choi, and M. Swany, “Graph
performance of CapTAIN-GAIL against GA, for the small
attention multi-agent fleet autonomy for advanced air mobility,” 07
set of cases where their performance differed, the difference 2023.
was quite high, the cause of which needs to be further [14] S. Paul and S. Chowdhury, “A graph-based reinforcement learning
framework for urban air mobility fleet scheduling,” 2023. [Online].
explored in the future. Moreover, to allow more efficient
Available:https://par.nsf.gov/biblio/10345357
use of costly expert demonstrations from GA, future work [15] N.KamraandN.Ayanian,“Amixedintegerprogrammingmodelfor
should look at approaches that can in situ identify when timed deliveries in multirobot systems,” in 2015 IEEE International
Conference on Automation Science and Engineering (CASE), 2015,
and where imitation is needed similar to the concept of
pp.612–617.
adaptivesequentialsamplinginoptimization,asopposedpre- [16] C. E. Miller, A. W. Tucker, and R. A. Zemlin, “Integer
computing a fixed set of expert demonstrations. programming formulation of traveling salesman problems,” J.
ACM, vol. 7, pp. 326–329, 1960. [Online]. Available: https:
ACKNOWLEDGMENTS //api.semanticscholar.org/CorpusID:2984845
[17] H.Mu¨hlenbein,“Parallelgeneticalgorithms,populationgeneticsand
This work was supported by the Office of Naval Research combinatorialoptimization,”inParallelism,Learning,Evolution,J.D.
(ONR) award N00014-21-1-2530 and the National Science Becker, I. Eisele, and F. W. Mu¨ndemann, Eds. Berlin, Heidelberg:
SpringerBerlinHeidelberg,1991,pp.398–406.
Foundation (NSF) award CMMI 2048020. Any opinions,
[18] Y. Peng, B. Choi, and J. Xu, “Graph learning for combinatorial
findings, conclusions, or recommendations expressed in this optimization: A survey of state-of-the-art,” Data Science and
paper are those of the authors and do not necessarily reflect Engineering, vol. 6, no. 2, pp. 119–141, Jun 2021. [Online].
Available:https://doi.org/10.1007/s41019-021-00155-3
the views of the ONR or the NSF.
[19] A.E.Rizzoli,R.Montemanni,E.Lucibello,andL.M.Gambardella,
REFERENCES
“Ant colony optimization for real-world vehicle routing problems,”
Swarm Intelligence, vol. 1, no. 2, pp. 135–151, Dec 2007. [Online].
[1] “Urban air mobility (uam) concept of operations 2.00.pdf,”
Available:https://doi.org/10.1007/s11721-007-0005-x
2023. [Online]. Available: https://www.faa.gov/sites/faa.gov/files/
[20] X. Wang, T.-M. Choi, H. Liu, and X. Yue, “Novel ant colony
Urban%20Air%20Mobility%20%28UAM%29%20Concept%20of%
optimizationmethodsforsimplifyingsolutionconstructioninvehicle
20Operations%202.00.pdf
routing problems,” IEEE Transactions on Intelligent Transportation
[2] T.Reed,“Inrixglobaltrafficscorecard,”2019.
Systems,vol.17,no.11,pp.3132–3141,2016.
[3] P. D. Krivonos, “Communication in aviation safety: Lessons learned
andlessonsrequired,”2007. [21] “Exploratorycombinatorialoptimizationwithreinforcementlearning,”
[4] L. Sun, P. Wei, and W. Xie, “Fair and risk-averse urban air vol. 34, pp. 3243–3250, Apr. 2020. [Online]. Available: https:
mobility resource allocation under uncertainties,” 2023. [Online]. //ojs.aaai.org/index.php/AAAI/article/view/5723
Available: http://dx.doi.org/10.2139/ssrn.4343979http://dx.doi.org/10. [22] E. B. Khalil, H. Dai, Y. Zhang, B. N. Dilkina, and L. Song,
2139/ssrn.4343979 “Learning combinatorial optimization algorithms over graphs,”
[5] S.-I. Hwang and S.-T. Cheng, “Combinatorial optimization in real- ArXiv, vol. abs/1704.01665, 2017. [Online]. Available: https:
time scheduling: Theory and algorithms,” Journal of Combinatorial //api.semanticscholar.org/CorpusID:3486660
Optimization, vol. 5, no. 3, pp. 345–375, Sep 2001. [Online]. [23] R.A.Jacob,S.Paul,W.Li,S.Chowdhury,Y.R.Gel,andJ.Zhang,
Available:https://doi.org/10.1023/A:1011449311477 “Reconfiguringunbalanceddistributionnetworksusingreinforcement
[6] P.PradeepandP.Wei,“Heuristicapproachforarrivalsequencingand learning over graphs,” in 2022 IEEE Texas Power and Energy Con-
schedulingforevtolaircraftinon-demandurbanairmobility,”in2018 ference(TPEC),2022,pp.1–6.
IEEE/AIAA37thDigitalAvionicsSystemsConference(DASC),2018, [24] Y.KaempferandL.Wolf,“Learningthemultipletravelingsalesmen
pp.1–7. problemwithpermutationinvariantpoolingnetworks,”032018.
[7] S. H. Kim, “Receding horizon scheduling of on-demand urban air [25] W. Kool, H. van Hoof, and M. Welling, “Attention, learn to
mobility with heterogeneous fleet,” IEEE Transactions on Aerospace solve routing problems!” in International Conference on Learning
andElectronicSystems,vol.56,no.4,pp.2751–2761,2020. Representations,2018.[Online].Available:https://api.semanticscholar.
[8] S. A. M. Shihab, P. Wei, J. Shi, and N. Yu, “Optimal evtol org/CorpusID:59608816
fleet dispatch for urban air mobility and power grid services,” [26] Z. Li, Q. Chen, and V. Koltun, “Combinatorial optimization
in AIAA AVIATION 2020 FORUM, 2020. [Online]. Available: with graph convolutional networks and guided tree search,” in
https://arc.aiaa.org/doi/abs/10.2514/6.2020-2906 Advances in Neural Information Processing Systems, S. Bengio,
[9] S. Paul, J. Witter, and S. Chowdhury, “Graph learning-based fleet H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
scheduling for urban air mobility under operational constraints, R. Garnett, Eds., vol. 31. Curran Associates, Inc., 2018.
varying demand & uncertainties,” in Proceedings of the 39th [Online]. Available: https://proceedings.neurips.cc/paperfiles/paper/
ACM/SIGAPP Symposium on Applied Computing, ser. SAC ’24. 2018/file/8d3bba7425e7c98c50f52ca1b52d3735-Paper.pdf
New York, NY, USA: Association for Computing Machinery, 2024, [27] A. Nowak, S. Villar, A. S. Bandeira, and J. Bruna, “Revised note
p. 638–645. [Online]. Available: https://doi.org/10.1145/3605098. on learning algorithms for quadratic assignment with graph neural
3635976 networks,”2018.
[10] S. Paul and S. Chowdhury, “A Scalable Graph Learning Approach [28] S. Verma and Z.-L. Zhang, “Graph capsule convolutional neural
to Capacitated Vehicle Routing Problem Using Capsule Networks networks,”2018.
and Attention Mechanism,” ser. International Design Engineering [29] R. A. Jacob, S. Paul, S. Chowdhury, Y. R. Gel, and J. Zhang,
TechnicalConferencesandComputersandInformationinEngineering “Real-time outage management in active distribution networks
Conference, vol. Volume 3B: 48th Design Automation Conference using reinforcement learning over graphs,” Nature Communications,
(DAC), 08 2022, p. V03BT03A045. [Online]. Available: https: vol. 15, no. 1, p. 4766, Jun 2024. [Online]. Available: https:
//doi.org/10.1115/DETC2022-90123 //doi.org/10.1038/s41467-024-49207-y
[11] S. Paul, P. Ghassemi, and S. Chowdhury, “Learning scalable [30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
policies over graphs for multi-robot task allocation using capsule Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,”
attention networks,” 2022 International Conference on Robotics in Advances in Neural Information Processing Systems, I. Guyon,
and Automation (ICRA), pp. 8815–8822, 2022. [Online]. Available: U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
https://api.semanticscholar.org/CorpusID:248562542 and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017.[Online]. Available: https://proceedings.neurips.cc/paperfiles/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[31] “Electricityrates(updatedjune2024)—electricchoice,”https://www.
electricchoice.com/electricity-prices-by-state/,2024.
[32] “Cityairbus nextgen - urban air mobility - airbus,” https://www.
airbus.com/en/innovation/low-carbon-aviation/urban-air-mobility/
cityairbus-nextgen.
[33] A. Gleave, M. Taufeeque, J. Rocamonde, E. Jenner, S. H.
Wang, S. Toyer, M. Ernestus, N. Belrose, S. Emmons, and
S. Russell, “imitation: Clean imitation learning implementations,”
arXiv:2211.11972v1 [cs.LG], 2022. [Online]. Available: https:
//arxiv.org/abs/2211.11972
[34] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
O. Klimov, “Proximal policy optimization algorithms,” ArXiv, vol.
abs/1707.06347,2017.[Online].Available:https://api.semanticscholar.
org/CorpusID:28695052
[35] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and
N. Dormann, “Stable-baselines3: Reliable reinforcement learning
implementations,” Journal of Machine Learning Research, vol. 22,
no. 268, pp. 1–8, 2021. [Online]. Available: http://jmlr.org/papers/
v22/20-1364.html