Mixed-Projection ADMM
Predictive Low Rank Matrix Learning under Partial
Observations: Mixed-Projection ADMM
Dimitris Bertsimas dbertsim@mit.edu
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
Nicholas A. G. Johnson nagj@mit.edu
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
Editor: TBD
Abstract
We study the problem of learning a partially observed matrix under the low rank assump-
tion in the presence of fully observed side information that depends linearly on the true
underlying matrix. This problem consists of an important generalization of the Matrix
Completion problem, a central problem in Statistics, Operations Research and Machine
Learning, that arises in applications such as recommendation systems, signal processing,
system identification and image denoising. We formalize this problem as an optimization
problem with an objective that balances the strength of the fit ofthe reconstruction to the
observed entries with the ability of the reconstruction to be predictive of the side informa-
tion. Wederiveamixed-projectionreformulationoftheresultingoptimizationproblemand
present a strong semidefinite cone relaxation. We design an efficient, scalable alternating
direction method of multipliers algorithm that produces high quality feasible solutions to
the problem of interest. Our numerical results demonstrate that in the small rank regime
(k ≤15),ouralgorithmoutputssolutionsthatachieveonaverage79%lowerobjectivevalue
and90.1%lowerℓ reconstructionerrorthanthesolutionsreturnedbytheexperiment-wise
2
bestperformingbenchmarkmethod. Theruntimeofouralgorithmiscompetitivewithand
often superior to that of the benchmark methods. Our algorithm is able to solve problems
with n=10000 rows and m=10000 columns in less than a minute.
Keywords: Matrix Completion; Rank; Mixed-Projection; ADMM;
1. Introduction
In many real world applications, we are faced with the problem of recovering a (often large)
matrixfroma(oftensmall)subsetofitsentries. Thisproblem,knownasMatrixCompletion
(MC), has gained significant attention due to its broad range of applications in areas such
as signal processing (Candes and Plan, 2010), system identification (Liu, 2019) and image
denoising (Ji et al., 2010). The fundamental task in MC is to accurately reconstruct the
missing entries of a matrix given a limited number of observed entries. The challenge is
particularly pronounced when the number of observed entries is small relatively to the
dimension of the matrix, yet this is the common scenario in practice.
OneofthemostprominentusesofMCisinrecommendationsystems,wherethegoalisto
predictuserpreferencesforitems(e.g.,movies,products)basedonapartiallyobserveduser-
1
4202
luJ
81
]LM.tats[
1v13731.7042:viXraBertsimas and Johnson
item rating matrix (Ramlatchan et al., 2018). The Netflix Prize competition highlighted
thepotentialofMCtechniques, wheretheobjectivewastopredictmissingratingsinauser-
movie matrix to improve recommendation accuracy (Bell and Koren, 2007). The success
of such systems hinges on the assumption that the underlying rating matrix is low rank,
meaning that the preferences of users can be well-approximated by a small number of
factors. Indeed, it has been well studied that many real world datasets are low rank (Udell
and Townsend, 2019).
In many practical applications, in addition to a collection of observed matrix entries we
additionallyhaveaccesstoauxiliarysideinformationthatcanbeleveragedwhenperforming
the reconstruction. For example, in a recommendation system, side information might
consist of social network data or item attributes. The vast majority of existing approaches
to MC in the presence of side information incorporate the side information by making
additional structural restrictions on the reconstructed matrix beyond the usual low rank
assumption (see, for example, Xu et al. (2013); Chiang et al. (2015); Bertsimas and Li
(2020)). In this work, we take an alternate approach by assuming that the side information
can be well modelled as a linear function of the underlying full matrix. In this setting, the
side information can be thought of as labels for a regression problem where the unobserved
matrix consists of the regression features. This assumption is in keeping with ideas from
the predictive low rank kernel learning literature (Bach and Jordan, 2005) (note however
that low rank kernel learning assumes a fully observed input matrix).
Formally, let Ω ⊆ [n]×[m] denote a collection of revealed entries of a partially observed
matrix A ∈ Rn×m, let Y ∈ Rn×d denote a matrix of side information and let k denote a
specified target rank. We consider the problem given by
(cid:88)
min (X −A )2+λ∥Y −Xα∥2 +γ∥X∥
ij ij F ⋆
X∈Rn×m,α∈Rm×d
(i,j)∈Ω (1)
s.t. rank(X) ≤ k,
whereλ,γ > 0arehyperparametersthatinpracticecaneithertakeadefaultvalueorcanbe
cross-validated by minimizing a validation metric (Owen and Perry, 2009) to obtain strong
out-of-sample performance (Bousquet and Elisseeff, 2002). We assume that the ground
truth matrix A has low rank and that the side information can be well approximated as
Y = Aα + N for some weighting matrix α and noise matrix N. The first term in the
objective function of (1) measures how well the observed entries of the unknown matrix are
fit by the estimated matrix X, the second term of the objective function measures how well
the side information Y can be represented as a linear function of the estimated matrix X
and the final term of the objective is a regularization term. To the best of our knowledge,
Problem (1) has not previously been directly studied despite its very natural motivation.
1.1 Contribution and Structure
In this paper, we tackle (1) by developing novel mixed-projection optimization techniques
(Bertsimas et al., 2022). We show that solving (1) is equivalent to solving an appropriately
defined robust optimization problem. We develop an exact reformulation of (1) by com-
bining a parametrization of the X decision variable as the product of two low rank factors
with the introduction of a projection matrix to model the column space of X. We derive a
2Mixed-Projection ADMM
semidefinite cone convex relaxation for our mixed-projection reformulation and we present
an efficient, scalable alternating direction method of multipliers (ADMM) algorithm that
produces high quality feasible solutions to (1). Our numerical results show that across all
experimentsinthesmallrankregime(k ≤ 15), ouralgorithmoutputssolutionsthatachieve
on average 79% lower objective value in (1) and 90.1% lower ℓ reconstruction error than
2
the solutions returned by the experiment-wise best performing benchmark method. For the
5 experiments with k > 15, the only benchmark that returns a solution with superior qual-
ity than that returned by our algorithm takes on average 3 times as long to execute. The
runtime of our algorithm is competitive with and often superior to that of the benchmark
methods. Our algorithm is able to solve problems with n = 10000 rows and m = 10000
columns in less than a minute.
The rest of the paper is laid out as follows. In Section 2, we review previous work
that is closely related to (1). In Section 3, we study (1) under a robust optimization lens
and investigate formulating (1) as a two stage optimization problem where the inner stage
is a regression problem that can be solved in closed form. We formulate (1) as a mixed-
projection optimization problem in Section 4 and present a natural convex relaxation. In
Section 5, we present and rigorously study our ADMM algorithm. Finally, in Section 6
we investigate the performance of our algorithm against benchmark methods on synthetic
data.
Notation: We let nonbold face characters such as b denote scalars, lowercase bold faced
characters such as x denote vectors, uppercase bold faced characters such as X denote
matrices, and calligraphic uppercase characters such as Z denote sets. We let [n] denote
the set of running indices {1,...,n}. We let 0 denote an n-dimensional vector of all 0’s,
n
0 denote an n × m-dimensional matrix of all 0’s, and I denote the n × n identity
n×m n
matrix. We let Sn denote the cone of n×n symmetric matrices and Sn denote the cone of
+
n×n positive semidefinite matrices
2. Literature Review
In this section, we review a handful of notable approaches from the literature that have
been employed to solve MC and to solve general low rank optimization problems. As an
exhaustiveliteraturereviewofMCmethodsisoutsideofthescopeofthispaper,wefocusour
reviewonahandfulofwellstudiedapproacheswhichwewillemployasbenchmarkmethods
in this work. We additionally give an overview of the ADMM algorithmic framework which
is of central relevance to this work. For a more detailed review of the MC literature, we
refer the reader to Ramlatchan et al. (2018) and Nguyen et al. (2019).
2.1 Matrix Completion Methods
2.1.1 Iterative-SVD
Iterative-SVD is an expectation maximization style algorithm (Dempster et al., 1977) that
generates a solution to the MC problem by iteratively computing a singular value decom-
position (SVD) of the current iterate and estimating the missing values by performing a
regression against the low rank factors returned by SVD (Troyanskaya et al., 2001). This
is one of a handful of methods in the literature that leverage the SVD as their primary al-
3Bertsimas and Johnson
gorithmic workhorse (Billsus et al., 1998; Sarwar et al., 2000). Concretely, given a partially
observed matrix {X } where Ω ⊆ [n]×[m] and a target rank k ∈ N , Iterative-SVD
ij (i,j)∈Ω +
proceeds as follows:
1. Initialize the iteration count t ← 0 and initialize missing entries of X ,(i,j) ∈/ Ω with
ij
(cid:80) X
the row average X = l:(i,l)∈Ω il.
ij |{l:(i,l)∈Ω}|
2. Compute a rank k SVD X = U Σ VT of the current iterate where U ∈ Rn×k,Σ ∈
t t t t t t
Rk×k,V ∈ Rm×k.
t
3. For each (i,j) ∈/ Ω, estimate the missing value (X ) by regressing all other entries
t+1 ij
in row i against all except the jth row of V . Concretely, letting x˜ = (X ) ∈ Rm−1
t t i,⋆\j
denote the column vector consisting of the ith row of X excluding the jth entry,
t
letting V˜ = (V ) ∈ R(m−1)×k denote the matrix formed by eliminating the jth row
t ⋆\j,⋆
from V and letting vˆ = (V ) ∈ Rk denote the column vector consisting of the jth
t t j,⋆
row of V , we set (X ) = vˆT(V˜TV˜)−1V˜Tx˜.
t t+1 ij
4. Terminate if the total change between X and X is less than 0.01. Otherwise,
t t+1
increment t and return to Step 2.
2.1.2 Soft-Impute
Soft-Impute is a convex relaxation inspired algorithm that leverages the nuclear norm
as a low rank inducing regularizer (Mazumder et al., 2010). This approach is one of a
broad class of methods that tackle MC from a nuclear norm minimization lens (Fazel,
2002; Cand`es and Tao, 2010; Candes and Recht, 2012). Seeking a reconstruction with
minimum nuclear norm is typically motivated by the observation that the nuclear norm
ball given by B = {X ∈ Rn×n : ∥X∥ ≤ k} is the convex hull of the nonconvex set
⋆
X = {X ∈ Rn×n : rank(X) ≤ k,∥X∥ ≤ 1}, where ∥ · ∥ denotes the spectral norm.
σ σ
Moreover, several conditions have been established under which nuclear norm minimization
methods are guaranteed to return the ground truth matrix (Cand`es and Tao, 2010; Candes
and Recht, 2012) though such conditions tend to be strong and hard to verify in practice.
Soft-Impute proceeds by iteratively replacing the missing elements of the matrix with those
obtained from a soft thresholded low rank singular value decomposition. Accordingly, sim-
ilarly to Iterative-SVD, Soft-Impute relies on the computation of a low rank SVD as the
primary algorithmic workhorse. The approach relies on the result that for an arbitrary ma-
trixX,thesolutionoftheproblemmin 1∥X−Z∥2 +λ∥Z∥ isgivenbyZˆ = S (X)where
Z 2 F ⋆ λ
S (·) denotes the soft-thresholding operation (Donoho et al., 1995). Explicitly, Soft-Impute
λ
proceeds as follows for a given regularization parameter λ > 0 and termination threshold
ϵ > 0:
1. Initialize the iteration count t ← 0 and initialize Z = 0 .
t n×m
2. Compute Z = S (P (X) + P⊥(Z )) where P (·) denotes the operation that
t+1 λ Ω Ω t Ω
projectsontotherevealedentriesofX whileP⊥(·)denotestheoperationthatprojects
Ω
onto the missing entries of X.
3. Terminate if
∥Zt−Zt+1∥2
F. Otherwise, increment t and return to Step 2.
∥Zt∥2
F
4Mixed-Projection ADMM
2.1.3 Fast-Impute
Fast-Impute is a projected gradient descent approach to MC that has desirable global
convergence properties (Bertsimas and Li, 2020). Fast-Impute belongs to the broad class of
methodsthatsolveMCbyfactorizingthetargetmatrixasX = UVT whereU ∈ Rn×k,V ∈
Rm×k andperformingsomevariantofgradientdescent(oralternatingminimization)onthe
matrices U and V (Koren et al., 2009; Jain and Netrapalli, 2015; Zheng and Lafferty, 2016;
Jin et al., 2016). We note that we leverage this common factorization in the approach
to (1) presented in this work. Gradient descent based methods have shown great success.
Despitethenon-convexityofthefactorization,ithasbeenshownthatinmanycasesgradient
descent and its variants will nevertheless converge to a globally optimal solution (Chen and
Wainwright, 2015; Ge et al., 2015; Sun and Luo, 2016; Ma et al., 2018; Bertsimas and Li,
2020). Fast-Impute takes the approach of expressing U as a closed form function of V after
performing the facorization and directly performs projected gradient descent updates on
V with classic Nesterov acceleration (Nesterov, 1983). Moreover, to enhance scalability of
theirmethod,BertsimasandLi(2020)designastochasticgradientextensionofFast-Impute
that estimates the gradient at each update step by only considering a sub sample of the
rows and columns of the target matrix.
2.2 Low Rank Optimization Methods
2.2.1 ScaledGD
ScaledGD is a highly performant method to obtain strong solutions to low rank matrix
estimation problems that take the following form:
1
min f(X) = ∥A(X)−y∥2 s.t. rank(X) ≤ k,
X∈Rn×m 2 2
where A(·) : Rn×m → Rl models some measurement process and we have y ∈ Rl (Tong
et al., 2021). ScaledGD proceeds by factorizing the target matrix as X = UVT and
iteratively performing gradient updates on the low rank factors U,V after preconditioning
the gradients with an adaptive matrix that is efficient to compute. Doing so yields a
linear convergence rate that is notably independent of the condition number of the low
rank matrix. In so doing, ScaledGD combines the desirable convergence rate of alternating
minimizationwiththedesirablelowper-iterationcostofgradientdescent. Explicitly,letting
L(U,V) = f(UVT), ScaledGD updates the low rank factors as:
U ← U −η∇ L(U ,V )(VTV)−1,
t+1 t U t t
V ← V −η∇ L(U ,V )(UTU)−1,
t+1 t V t t
where η > 0 denotes the step size.
2.2.2 Mixed-Projection Conic Optimization
Mixed-projectionconicoptimizationisarecentlyproposedmodellingandalgorithmicframe-
work designed to tackle a broad class of matrix optimization problems (Bertsimas et al.,
2022, 2023c). Specifically, this approach considers problems that have the following form:
5Bertsimas and Johnson
min ⟨C,X⟩+λ·rank(X)+Ω(X)
X∈Rn×m (2)
s.t. AX = B, rank(X) ≤ k, X ∈ K,
where C ∈ Rn×m is a cost matrix, λ > 0, k ∈ N , A ∈ Rl×n,B ∈ Rl×m,K denotes a proper
+
cone in the sense of Boyd et al. (2004) and Ω(·) is a frobenius norm regularization function
or a spectral norm regularization function of the input matrix. The main workhorse of
mixed-projection conic optimization is the use of a projection matrix to cleverly model the
rank terms in (2). This can be viewed as the matrix generalization of using binary variables
to model the sparsity of a vector in mixed-integer optimization. Bertsimas et al. (2022)
show that for an arbitrary matrix X ∈ Rn×m, we have
rank(X) ≤ k ⇐⇒ ∃P ∈ Sn : P2 = P,Tr(P) ≤ k,X = PX.
Introducing projection matrices allows the rank functions to be eliminated from (2) at the
expense of introducing non convex quadratic equality constraints. From here, most existing
works that leverage mixed-projection conic optimization have either focused on obtaining
strong semidefinite based convex relaxations (Bertsimas et al., 2022, 2023c) or have focused
on obtaining certifiably optimal solutions for small and moderately sized problem instances
(Bertsimas et al., 2023a,b). In this work, we leverage the mixed-projection framework to
scalably obtain high quality solutions to large problem instances.
2.3 Alternating Direction Method of Multipliers
Alternating direction method of multipliers (ADMM) is an algorithm that was originally
designed to solve linearly constrained convex optimization problems of the form
min f(x)+g(z) s.t. Ax+Bz = c, (3)
x∈Rn,z∈Rm
where we have A ∈ Rl×n,B ∈ Rl×m,c ∈ Rl and the functions f and g are assumed
to be convex (Boyd et al., 2011). The main benefit of ADMM is that it can combine
the decomposition benefits of dual ascent with the desirable convergence properties of the
method of multipliers. Letting y ∈ Rl denote the dual variable, the augmented lagrangian
of (3) is given by
ρ
L (x,z,y) = f(x)+g(z)+yT(Ax+Bz−c)+ ∥Ax+Bz−c∥2,
ρ 2 2
where ρ > 0 is the augmented lagrangian parameter. ADMM then proceeds by iteratively
updatingtheprimalvariablex, updatingtheprimalvariablez andtakingagradientascent
step on the dual variable y. Explicitly, ADMM consists of the following updates:
1. x ← argmin L(x,z ,y ),
t+1 x t t
2. z ← argmin L(x ,z,y ),
t+1 z t+1 t
3. y ← y +ρ(Ax +Bz −c).
t+1 t t+1 t+1
6Mixed-Projection ADMM
Under very mild regularity conditions on f,g and L, it is well known that ADMM is guar-
anteed to produce a sequence of primal iterates that converges to the optimal value of (3)
and a sequence of dual iterates that converge to the optimal dual variable (note that there
is no guarantee of primal variable convergence) (Boyd et al., 2011). Importantly, although
ADMM was originally designed for linearly constrained convex optimization, it has often
been applied to non convex optimization problems and yielded empirically strong results
(Xu et al., 2016). This observation has motivated work to explore the theoretical conver-
gence behavior of ADMM and its variants on specific classes of non convex optimization
problems (Guo et al., 2017; Wang et al., 2019; Wang and Zhao, 2021).
3. Formulation Properties
In this section, we rigorously investigate certain key features of (1). Specifically, we estab-
lish an equivalence between (1) and an appropriately defined robust optimization problem.
Moreover,weillustratethat(1)canbereducedtoanoptimizationproblemoveronlyX and
establish that the resulting objective function is not convex, not concave and non-smooth.
Finally, we study how efficient evaluations of the reduced problem objective function can
be performed.
3.1 Equivalence Between Regularization and Robustness
Real-world datasets frequently contain inaccuracies and missing values, which hinder the
ability of machine learning models to generalize effectively to new data when these incon-
sistencies are not appropriately modelled. Consequently, robustness is a crucial quality for
machine learning models, both in theory and application (Xu et al., 2009; Bertsimas and
den Hertog, 2020). In this section, we show that our regularized problem (1) can be viewed
as a robust optimization (RO) problem. This finding justifies the inclusion of the nuclear
norm regularization term in (1) and is in a similar flavor as known results from the robust
optimization literature in the case of vector (Bertsimas and Copenhaver, 2018) and matrix
(Bertsimas et al., 2023a) problems.
Proposition 1 Problem (1) is equivalent to the following robust optimization problem:
(cid:88)
min max (X −A )2+λ∥Y −Xα∥2 +⟨X,∆⟩
ij ij F
X∈Rn×m,α∈Rm×d∆∈U
(i,j)∈Ω (4)
s.t. rank(X) ≤ k ,
0
where U = {∆ ∈ Rn×m : ∥∆∥ ≤ γ}
σ
Proof To establish this result, it suffices to argue that max ⟨X,∆⟩ = γ∥X∥ . This
∆∈U ⋆
equivalence follows immediately from the fact that the nuclear norm is dual to the spectral
norm. So as to keep this manuscript self contained, we present a proof of this equivalence
below.
Consider any matrix ∆¯ ∈ Rn×m such that ∥∆¯∥ ≤ γ. Let X = UΣVT be a singular
σ
valuedecompositionofX whereweletr = rank(X)andwehaveU ∈ Rn×r,Σ ∈ Rr×r,V ∈
7Bertsimas and Johnson
Rm×r. We have
r
(cid:88)
⟨X,∆¯⟩ = Tr(∆¯TUΣVT) = Tr(VT∆¯TUΣ) = ⟨UT∆¯V,Σ⟩ = Σ (UT∆¯V)
ii ii
i=1
r r r
(cid:88) (cid:88) (cid:88)
= Σ UT∆¯V ≤ Σ σ (∆¯) ≤ γ Σ = γ∥X∥
ii i i ii 1 ii ⋆
i=1 i=1 i=1
where we have used the fact that Σ is a diagonal matrix and the columns of U and V have
unit length. Thus, we have shown that γ∥X∥ is an upper bound for max ⟨X,∆⟩. To
⋆ ∆∈U
show that the upper bound is always achieved, consider the matrix ∆˜ = γUVT ∈ Rn×m
where U and V are taken from a singular value decomposition of X. Observe that
∥∆˜∥ = γ∥UVT∥ ≤ γ =⇒ ∆˜ ∈ U.
σ σ
We conclude by noting that
⟨X,∆˜⟩ = Tr(VΣUTγUVT) = γTr(VTVΣUTU) = γTr(IΣI) = γ∥X∥ .
⋆
Proposition1impliesthatsolvingthenuclearnormregularized(1)isequivalenttosolvingan
unregularized robust optimization problem that protects against adversarial perturbations
that are bounded in spectral norm. This result is not surprising given the duality of norms,
yet is nevertheless insightful.
3.2 A Partial Minimization
Let g(X,α) denote the objective function of (1). Note that g(X,α) is bi-convex in (X,α)
but is not jointly convex due to the product Xα. Observe that we can simplify (1) by
performing a partial minimization in α. For any X, the problem in α requires finding
the unconstrained minimum of a convex quadratic function. The gradient of g with re-
spect to α is given by ∇ g(X,α) = 2λXT(Xα − Y). Setting ∇ g(X,α) to 0 yields
α α
α⋆ = (XTX)†XTY as a minimizer of g over α. Letting f(X) correspond to the partially
minimized objective function of (1), we have
(cid:88)
f(X) = ming(X,α) = (X −A )2+λ∥(I −X(XTX)†XT)Y∥2 +γ∥X∥
ij ij n F ⋆
α
(i,j)∈Ω
= (cid:88) (X −A )2+λTr(cid:0) YT(I −X(XTX)†XT)Y(cid:1) +γ∥X∥
ij ij n ⋆
(i,j)∈Ω
We note that α⋆ corresponds to the well studied ordinary least squares solution. When
XTX has full rank, α is the unique minimizer of g. If XTX is rank deficient, α⋆ corre-
sponds to the minimizer with minimum norm.
Though we have simplified the objective function of (1), f(X) is not a particularly well
behaved function. We formalize this statement in Proposition (2).
Proposition 2 The function f(X) is in general neither convex nor concave and is non-
smooth.
8Mixed-Projection ADMM
Proof To illustrate that f(X) is in general neither convex nor concave, suppose that
Ω = ∅, n = 2 and m = d = λ = γ = 1. In this setting, we have x,y ∈ R2×1. Assuming that
x ̸= 0 , we can write the objective function as
2
f(x) = Tr(yT(I −x(xTx)−1xT)y)+∥x∥
2 ⋆
yTxxTy
= yTy− +∥x∥
xTx 2
(yTx)2 √
= yTy− + xTx.
xTx
For x = 0 , the objective value f(0 ) is equal to yTy. Let y = 1 and consider the line in
2 2 2
R2 defined by X = {x ∈ R2 : x = x +1}. The restriction of f(x) to the line defined by
2 1
X is a univariate function given by
(2t+1)2 (cid:112)
f (t) = 2− + 2t2+2t+1
X 2t2+2t+1
where t ∈ R is a dummy variable. Observe that we have f (−1) = f (0) = 2,f (−0.5) =
√ X X X
2+ 2 and f (−4) = f (3) = 5.04. Thus, the point (−0.5,f (0.5)) lies above the chord
2 X X X
connecting (−1,f (−1)) and (0,f (0)), so f (t) is not a convex function. Moreover, the
X X X
point (−1,f (−1)) lies below the chord connecting (−4,f (−4)) and (−0.5,f (−0.5)), so
X X X
f (t) is not a concave function. Since a function is convex (respectively concave) if and
X
only if its restriction to every line is convex (respectively concave), we have established that
f(X) is neither convex nor concave since f (t) is neither convex nor concave. To conclude
X
the proof of Proposition 2, note that the non-smooth property of f(X) follows immediately
from the non-smooth property of the nuclear norm function.
Althoughtheaboveclosedformpartialminimizationinαeliminatesm×dvariablesform
(1), this comes at the expense of introducing a m×m matrix pseudo-inverse term into the
objective function which can be computationally expensive to evaluate. Efficient evaluation
ofanobjectivefunctioniscrucialinmanyoptimizationproblemstoquicklymeasuresolution
quality. A plethora of modern optimization techniques require iterative objective function
evaluations. As a result, the computational cost of evaluating an objective function can
quickly become the bottleneck of an algorithm’s complexity. Directly evaluating f(X)
naively requires O(|Ω|) operations for the first term, O(m2n + m3 + n2d) for the second
term (forming the matrix XTX is O(m2n), taking the pseudo-inverse is O(m3), computing
the products involving Y is O(n2d)) and requires O(mnmin(m,n)) for the third term (the
nuclear norm can be evaluated by computing a singular value decomposition of X). We
observe that computing the second term of f(X) involving the pseudo-inverse dominates
the complexity calculation. Indeed, the overall complexity of evaluating f(X) naively is
O(m2n+m3+n2d).
Fortunately, it is possible to make evaluations of f(X) without explicitly forming the
productXTX ortakingapseudo-inverse. Proposition3illustratesthatitsuffices(interms
of computational complexity) to take a singular value decomposition of X. Moreover, a
largeclassofoptimizationalgorithmsrequireonlyfunctionevaluationsforfeasiblesolutions.
If we consider only those values of X that are feasible to (1), it is sufficient (in terms of
9Bertsimas and Johnson
computational complexity) to take a rank k truncated singular value decomposition of X
to make functions evaluations of f(X).
Proposition 3 The function f(X) can equivalently be written as
f(X) = (cid:88) (X −A )2+λTr(cid:0) YT(I −UUT)Y(cid:1) +γ∥X∥ ,
ij ij n ⋆
(i,j)∈Ω
where X = UΣVT is a singular value decomposition of X where we let r = rank(X) and
we have U ∈ Rn×r,Σ ∈ Rr×r,V ∈ Rm×r.
Proof To establish the result, it suffices to show that
Tr(cid:0) YTX(XTX)†XTY(cid:1)
=
Tr(cid:0) YTUUTY(cid:1)
.
Let X = UΣVT be a singular value decomposition of X where r = rank(X) and U ∈
Rn×r,Σ ∈ Rr×r,V ∈ Rm×r. Observe that
Tr(cid:0) YTX(XTX)†XTY(cid:1)
=
Tr(cid:0) YTUΣVT(VΣUTUΣVT)†VΣUTY(cid:1)
=
Tr(cid:0) YTUΣVT(VΣ2VT)†VΣUTY(cid:1)
=
Tr(cid:0) YTUΣVTVΣ−2VTVΣUTY(cid:1)
=
Tr(cid:0) YTUΣΣ−2ΣUTY(cid:1)
=
Tr(cid:0) YTUUTY(cid:1)
,
where we have repeatedly invoked the property that UTU = VTV = I .
r
In light of Proposition 3, evaluating f(X) for feasible solutions requires still requires O(|Ω|)
operations for the first term, but the second term can be evaluated using O(kn(m + d))
operations(performingatruncatedsingularvaluedecompositionisO(knm)andcomputing
the products involving Y is O(knd)) and the third term can be evaluated using O(knm)
operations (by performing a truncated singular value decomposition) for an overall com-
plexity of O(kn(m+d)). This is significantly less expensive than the O(m2n+m3 +n2d)
complexity of naive direct evaluation of f(X) introduced previously.
4. An Exact Mixed-Projection Formulation
In this section, we reformulate (1) as a mixed-projection optimization problem and further
reduce the dimension of the resulting problem in a commonly studied manner by parame-
terizing X as the matrix product of two low dimensional matrices. Thereafter, we illustrate
how to employ the matrix generalization of the perspective relaxation (Gu¨nlu¨k and Lin-
deroth, 2012; Bertsimas et al., 2022, 2023c,a) to construct a convex relaxation of (1).
We first note that given the result of Section 3.2, we can rewrite (1) as an optimization
problem only over X as follows:
min (cid:88) (X −A )2+λTr(cid:0) YT(I −X(XTX)†XT)Y(cid:1) +γ∥X∥
ij ij n ⋆
X∈Rn×m
(i,j)∈Ω (5)
s.t. rank(X) ≤ k.
10Mixed-Projection ADMM
Observe that the matrix X(XTX)†XT is the linear transformation that projects vectors
onto the subspace spanned by the columns of the matrix X. Drawing on ideas presented
in Bertsimas et al. (2022, 2023c,a), we introduce an orthogonal projection matrix P ∈ P
k
to model the column space of X where P = {P ∈ Sn : P2 = P,tr(P) ≤ η} for η ≥ 0.
η
We can express the desired relationship between P and X as X = PX since projecting
a matrix onto its own column space leaves the matrix unchanged. This gives the following
reformulation of (1):
min (cid:88) (X −A )2+λTr(cid:0) YT(I −P)Y(cid:1) +γ∥X∥
ij ij n ⋆
X∈Rn×m,P∈Rn×n
(i,j)∈Ω (6)
s.t. (I −P)X = 0 , P ∈ P .
n n×m min(k,rank(X))
Observe that the matrix pseudo-inverse term has been eliminated from the objective func-
tion, however we have introduced the bilinear constraint X = PX which is non convex
in the optimization variables as well as the non convex constraint P ∈ P . We
min(k,rank(X))
now have the following result:
Proposition 4 Problem (6) is a valid reformulation of (5).
Proof We show that given a feasible solution to (6), we can construct a feasible solution
to (5) that achieves the same objective value and vice versa.
Consider an arbitrary feasible solution (X¯,P¯) to (6). Since P¯X¯ = X¯ and P¯ ∈
P , we have rank(X¯) ≤ k. We claim that X¯ achieves the same objective
min(k,rank(X¯))
value in (5) as (X¯,P¯) achieves in (6). To show this, it suffices to illustrate that for all
(X¯,P¯) feasible to (6) we have H(X¯) := X¯(X¯TX¯)†X¯T = P¯. The matrix P¯ is an or-
thogonal projection matrix since it is symmetric and satisfies P¯2 = P¯. Moreover, since
rank(P¯) = rank(X¯) and P¯X¯ = X¯ we know that P¯ is an orthogonal projection onto the
subspace spanned by the columns of X¯. Similarly, it can easily be verified that H(X¯)
is symmetric and satisfies H(X¯)2 = H(X¯), rank(H(X¯)) = rank(X¯) and H(X¯)X¯ = X¯.
Thus, H(X¯) is also an orthogonal projection matrix onto the subspace spanned by the
columns of X¯. To conclude, we invoke the property that given a subspace V ⊂ Rn the
orthogonal projection onto V is uniquely defined. To see this, suppose P and P are two
1 2
orthogonal projections onto V. Let l = dim(V). Let {e }l be an orthogonal basis for V
i i=1
and let {e }n be an orthogonal basis for V⊥. Since P is an orthogonal projection onto
i l+1 1
V, we have P e = e for all 1 ≤ i ≤ l and P e = 0 for all l+1 ≤ i ≤ n. However, the
1 i i 1 i n
same must hold for P which implies that P = P .
2 1 2
Consider an arbitrary feasible solution X¯ to (5). Let r = rank(X¯) and X¯ = U¯Σ¯V¯T
be a singular value decomposition of X¯ where we have U¯ ∈ Rn×r,Σ¯ ∈ Rr×r,V¯ ∈ Rm×r.
Define P¯ = U¯U¯T. By construction, we have P¯ ∈ P since r ≤ k. Moreover, it
min(k,rank(X¯))
is easy to verify that
P¯X¯ = U¯U¯TU¯Σ¯V¯T = U¯Σ¯V¯T = X¯,
where we have used the property U¯TU¯ = I . Finally, Proposition 3 immediately implies
r
that (X¯,P¯) achieves the same objective in (6) as X¯ achieves in (5). This completes the
proof.
Optimizing explicitly over the space of n×m matrices can rapidly become prohibitively
11Bertsimas and Johnson
costly in terms of runtime and memory requirements. Accordingly, we adopt the common
approach of factorizing X ∈ Rn×m as UVT for U ∈ Rn×k,V ∈ Rm×k. This leads to the
following formulation:
min (cid:88) ((UVT) −A )2+λTr(cid:0) YT(I −P)Y(cid:1) + γ (∥U∥2 +∥V∥2)
U∈Rn×k,V∈Rm×k, ij ij n 2 F F
P∈Rn×n (i,j)∈Ω
s.t. (I −P)U = 0 , P ∈ P .
n n×k min(k,rank(UVT))
(7)
Notice that we have replaced n×m optimization variables with k×(n+m) optimization
variables, anoftensignificantdimensionreductioninpractice. Attentivereadersmayobject
that though this is true, we have introduced n2 decision variables through the introduction
of the projection matrix variable P which nullifies any savings introduced through the
factorization of X. Note, however, that it is possible to factor any feasible projection
matrix as P = MMT for some M ∈ Rn×k. In Section 5, we leverage this fact so that the
presence of the projection matrix incurs a cost of n×k additional variables rather than n2
variables. We have the following result:
Proposition 5 Problem (7) is a valid reformulation of (6).
Proof We show that given a feasible solution to (7), we can construct a feasible solution
to (6) that achieves the same or lesser objective value and vice versa.
Consider an arbitrary feasible solution (U¯,V¯,P¯) to (7). Let X¯ = U¯V¯T. We will show
that (X¯,P¯) is feasible to (6) and achieves the same or lesser objective as (U¯,V¯,P¯) does
in (7). Feasibility of (7) implies that P¯ ∈ P = P and also that
min(k,rank(U¯V¯T)) min(k,rank(X¯))
(I −P¯)X¯ = (I −P¯)U¯V¯T = 0 V¯T = 0 ,
n n n×k n×m
thus the solution (X¯,P¯) is certainly feasible for (6). To see that (X¯,P¯) achieves the same
or lesser objective value, it suffices to argue that ∥X¯∥ ≤ 1(∥U¯∥2 +∥V¯∥2). This follows
⋆ 2 F F
immediatelyfromthefollowinglemmaestablishedbyMazumderetal.(2010)(seeAppendix
A.5 in their paper for a proof):
Lemma 6 For any matrix Z, the following holds:
1
∥Z∥ = min (∥U∥2 +∥V∥2).
⋆ U,V:Z=UVT 2 F F
If rank(Z) = k ≤ min(m,n), then the minimum above is attained at a factor decomposition
U VT . Letting Z = L Σ RT denote a singular value decomposition of Z,
n×k m×k n×m n×k k×k m×k
1 1
the minimum above is attained at U = L Σ2 ,V = R Σ2 .
n×k n×k k×k m×k m×k k×k
Consider now an arbitrary feasible solution (X¯,P¯) to (6). Let X¯ = LΣRT be a
singular value decomposition of X¯ where L ∈ Rn×k,Σ ∈ Rk×k,R ∈ Rm×k and define
U¯ = LΣ21 ,V¯ = RΣ1 2. Feasibility of (X¯,P¯) in (6) implies that P¯ ∈ P
min(k,rank(X¯))
=
12Mixed-Projection ADMM
P . Moreover, since the columns of L form an orthogonal basis for the
min(k,rank(U¯V¯T))
columns space of X¯, the condition (I −P¯)X¯ = 0 implies that
n n×m
(I n−P¯)U¯ = (I n−P¯)LΣ1 2 = 0 n×kΣ1 2 = 0 n×k.
Thus, the solution (U¯,V¯,P¯) is feasible to (7). Moreover, by Lemma 6 we have 1(∥U¯∥2 +
2 F
∥V¯∥2) = ∥X¯∥ so (U¯,V¯,P¯) achieves the same objective in (7) as (X¯,P¯) does in (6). This
F ⋆
completes the proof.
In the remainder of the paper, we will relax the constraint P ∈ P to
min(k,rank(UVT))
P ∈ P anddevelopascalablealgorithmtoobtainhighqualityfeasiblesolutions. Explicitly,
k
we consider the problem given by:
min (cid:88) ((UVT) −A )2+λTr(cid:0) YT(I −P)Y(cid:1) + γ (∥U∥2 +∥V∥2)
U∈Rn×k,V∈Rm×k, ij ij n 2 F F
P∈Rn×n (i,j)∈Ω
s.t. (I −P)U = 0 , P ∈ P .
n n×k k
(8)
It is straightforward to see that the optimal value of (8) is no greater than the optimal
value of (7). Unfortunately, the converse does not necessarily hold. To see why the optimal
value of (8) can be strictly less than that of (7) in certain pathological cases, suppose we
had k = n = m, Ω = ∅. In this setting, letting P¯ = I , U = 0 and V¯ = 0 , the
n n×k m×k
solution (U¯,V¯,P¯) would be feasible to (8) and achieve an objective value of 0. However
the optimal value of (7) would be strictly greater than 0 in this setting as long as Y ̸= 0.
Although (8) is a relaxation of (1), we will see in Section 6 that the solutions we will obtain
to (8) will be high quality solutions for (1), the main problem of interest.
4.1 A Positive Semidefinite Cone Relaxation
Convex relaxations are useful in non convex optimization primarily for two reasons. Firstly,
given the objective value achieved by an arbitrary feasible solution, strong convex relax-
ations can be used to upperbound the worst case suboptimality of said solution. Secondly,
convex relaxations can often be used as building blocks for global optimization procedures.
In this section, we present a natural convex relaxation of (8) that leverages the matrix
generalization of the perspective relaxation (Gu¨nlu¨k and Linderoth, 2012; Bertsimas et al.,
2022, 2023c,a).
Rather than working directly with (8), consider the equivalent formulation (6) with
P replaced by P . Before proceeding, we will assume knowledge of an upper
min(k,rank(X)) k
bound M ∈ R on the spectral norm of an optimal X to (6). Tighter bounds M are
+
desirable as they will lead to stronger convex relaxations of (6). We note that it is always
possible to specify such an upper bound M without prior knowledge of an optimal solution
to (6). To see this, note that setting X = 0 in (6) produces an objective value of
n×m
(cid:80) A2 + λ∥Y∥2. Thus, any X such that γ∥X∥ > (cid:80) A2 + λ∥Y∥2 cannot
(i,j)∈Ω ij F ⋆ (i,j)∈Ω ij F
possibly be optimal to (6). Finally, since the nuclear norm is an upper bound on the
13Bertsimas and Johnson
spectral norm of a matrix, we must have
(cid:80) A2 +λ∥Y∥2
(i,j)∈Ω ij F
∥X∥ ≤ ,
σ
γ
(cid:80) A2+λ∥Y∥2
for any matrix X that is optimal to (6). We can therefore take M = (i,j)∈Ω ij F.
γ
Notice that the non convexity in (6) is captured entirely by the bilinear constraint
(I −P)X = 0 and the quadratic constraint P2 = P. In keeping with the approach
n n×m
presented in Bertsimas et al. (2023c,a), we leverage the matrix perspective to convexify the
bilinear term and solve over the convex hull of the set P . Recalling that the nuclear norm
k
is semidefinite representable, we have the following formulation:
min (cid:88) (X −A )2+λTr(cid:0) YT(I −P)Y(cid:1) + γ (Tr(W )+Tr(W ))
ij ij n 1 2
X∈RP n, ×W m1 ,∈ WR 2n ∈× Rn m, ×m (i,j)∈Ω 2
s.t. I ⪰ P ⪰ 0, Tr(P) ≤ k,
n
(cid:18) (cid:19) (cid:18) (cid:19)
MP X W X
⪰ 0, 1 ⪰ 0.
XT MI XT W
m 2
(9)
We now have the following result:
Proposition 7 Problem (9) is a valid convex relaxation of (8).
Proof Problem(9)isclearlyaconvexoptimizationproblem. Wewillshowthattheoptimal
value of (9) is a lower bound on the optimal value of (8) by showing that given any optimal
solution to (8), we can construct a feasible solution to (9) that achieves the same objective
value.
Consider any optimal solution (U¯,V¯,P¯) to (8). From the proof of Proposition 5, we
know that the solution (X¯,P¯) where X¯ = U¯V¯T is feasible to (6) (where we replace the
constraint P ∈ P with P ∈ P ) and must also be optimal. Let X¯ = LΣRT
min(k,rank(UVT)) k
be a singular value decomposition of X¯ with L ∈ Rn×k,Σ ∈ Rk×k and R ∈ Rm×k. Let
W¯ = LΣLT and W¯ = RΣRT. We claim that (X¯,P¯,W¯ ,W¯ ) is feasible to (9) and
1 2 1 2
achieves the same objective value as (X¯,P¯) does in (6).
From the feasibility of P¯ in (8), we know that P¯ ∈ P which implies I ⪰ P¯ ⪰ 0 and
k n
Tr(P¯) ≤ k. By the generalized Schur complement lemma (see Boyd et al. (1994), Equation
2.41), we know that
(cid:18) MP¯ X¯ (cid:19)
⪰ 0 ⇐⇒ MI ⪰ 0, and MI −X¯T(MP¯)†X¯ ⪰ 0.
X¯T MI m m
m
We trivially have MI ⪰ 0. To see that the second condition holds, note that since P¯
m
is a projection matrix and P¯X¯ = X¯, we have X¯T(MP¯)†X¯ = 1 X¯TP¯X¯ = 1 X¯TX¯.
M M
Furthermore, since X¯ is optimal to (6), we have ∥X¯∥ ≤ M by assumption. Thus, we have
σ
1
∥X¯∥ ≤ M =⇒ ∥X¯TX¯∥ ≤ M2 =⇒ M2I ⪰ X¯TX¯ =⇒ MI ⪰ X¯TX¯.
σ σ m m
M
14Mixed-Projection ADMM
Finally, observe that
(cid:18) W¯ X¯ (cid:19) (cid:18) LΣLT LΣRT(cid:19) (cid:18) L(cid:19) (cid:18) L(cid:19)T
1 = = Σ .
X¯T W¯ RΣLT RΣRT R R
2
(cid:18) W¯ X¯ (cid:19)
Since Σ is a diagonal matrix with non negative entries, the matrix 1 is certainly
X¯T W¯
2
positive semidefinite. Thus we have shown that (X¯,P¯,W¯ ,W¯ ) is indeed feasible to (9).
1 2
To conclude the proof, we note that
γ γ γ
(Tr(W¯ )+Tr(W¯ )) = (Tr(LΣLT)+Tr(RΣRT)) = (Tr(LTLΣ)+Tr(RTRΣ))
1 2
2 2 2
γ
= (Tr(Σ)+Tr(Σ)) = γ∥X¯∥ ,
⋆
2
thus (X¯,P¯,W¯ ,W¯ ) achieves the same objective value in (9) as (X¯,P¯) achieves in (6).
1 2
In general, an optimal solution to (9) will have P ∈/ P . We briefly note that to obtain
k
a stronger convex relaxation, one could leverage eigenvector disjunctions (Bertsimas et al.,
2023b; Saxena et al., 2010) to iteratively cut off solutions to (9) with P ∈/ P and form
k
increasingly tighter disjunctive approximations to the set P .
k
5. Mixed-Projection ADMM
In this section, we present an alternating direction method of multipliers (ADMM) algo-
rithm that is scalable and obtains high quality solutions for (8). Rather than forming the
augmentedlagrangiandirectlyfor(8), wefirstmodifyourproblemformulationbyintroduc-
ing a dummy variable Z ∈ Rn×k that is an identical copy of U. Additionally, rather than
directlyenforcingtheconstraintP ∈ P , weintroduceanindicatorfunctionpenaltyI (P)
k P k
into the objective function where I (x) = 0 if x ∈ X, otherwise I (x) = ∞. Explicitly, we
X X
consider the following problem:
min (cid:88) ((UVT) −A )2+λTr(cid:0) YT(I −P)Y(cid:1) + γ (∥U∥2 +∥V∥2)+I (P)
U,Z∈Rn×k, ij ij n 2 F F P k
(i,j)∈Ω
V∈Rm×k,
P∈Rn×n
s.t. (I −P)U = 0 , U −Z = 0 .
n n×k n×k
(10)
Itistrivialtoseethat(10)isequivalentto(8). Wewillseeinthissectionthatworkingwith
formulation (10) leads to an ADMM algorithm with favorable decomposition properties.
IntroducingdualvariablesΦ,Ψ ∈ Rn×k fortheconstraints(I −P)U = 0 andU−Z =
n n×k
0 respectively, the augmented lagrangian L for (10) is given by:
n×k
L(U,V,P,Z,Φ,Ψ) = (cid:88) ((UVT) −A )2+λTr(cid:0) YT(I −P)Y(cid:1)
ij ij n
(i,j)∈Ω
γ
+ (∥U∥2 +∥V∥2)+I (P)+Tr(ΦT(I −P)Z) (11)
2 F F P k n
ρ ρ
+Tr(ΨT(Z −U))+ 1 ∥(I −P)Z∥2 + 2 ∥Z −U∥2,
2 n F 2 F
15Bertsimas and Johnson
where ρ ,ρ > 0 are non-negative penalty parameters. In what follows, we show that
1 2
performing a partial minimization of the augmented lagrangian (11) over each of the primal
variables U,V,P,Z yields a subproblem that can be solved efficiently. We present each
subproblem and investigate the complexity of computing the corresponding subproblem
solutions.
5.1 Subproblem in U
First, suppose we fix variables V,P,Z,Φ,Ψ and seek to minimize L(U,V,P,Z,Φ,Ψ)
over U. Eliminating terms that do not depend on U, the resulting subproblem is given by
min (cid:88) ((UVT) −A )2+ γ ∥U∥2 −Tr(ΨTU)+ ρ 2 ∥Z −U∥2.
U∈Rn×k ij ij 2 F 2 F (12)
(i,j)∈Ω
We now have the following result:
Proposition 8 The optimal solution U¯ for (12) is given by
U¯ = [2VTW V +(γ +ρ )I ]−1[2VTW A +Ψ +ρ Z ], (13)
i,⋆ i 2 k i i,⋆ i,⋆ 2 i,⋆
for each i ∈ {1,...,n} where each W ∈ Rm×m is a diagonal matrix satisfying (W ) = 1
i i jj
if (i,j) ∈ Ω, otherwise (W ) = 0. Here, the column vectors U¯ ∈ Rk,A ∈ Rm,Ψ ∈
i jj i,⋆ i,⋆ i,⋆
Rk,Z ∈ Rk denote the ith row of the matrices U¯,A,Ψ,Z respectively, where the unknown
i,⋆
entries of A are taken to be 0.
Proof Let f(U) denote the objective function of (12). With {W }n defined as in Propo-
i i=1
sition 8, observe that we can write f(U) as
n n n n
f(U) = (cid:88) ∥W (VU −A )∥2+ γ (cid:88) ∥U ∥2−(cid:88) ΨT U + ρ 2 (cid:88) ∥Z −U ∥2
i i,⋆ i,⋆ 2 2 i,⋆ 2 i,⋆ i,⋆ 2 i,⋆ i,⋆ 2
i=1 i=1 i=1 i=1
n (cid:20) (cid:21) n
= (cid:88) ∥W (VU −A )∥2+ γ ∥U ∥2−ΨT U + ρ 2 ∥Z −U ∥2 = (cid:88) g (U),
i i,⋆ i,⋆ 2 2 i,⋆ 2 i,⋆ i,⋆ 2 i,⋆ i,⋆ 2 i
i=1 i=1
where we define g (U) = ∥W (VU −A )∥2+γ∥U ∥2−ΨT U +ρ2∥Z −U ∥2. Thus,
i i i,⋆ i,⋆ 2 2 i,⋆ 2 i,⋆ i,⋆ 2 i,⋆ i,⋆ 2
we have shown that f(U) is separable over the rows of the matrix U. Each function g (U)
i
is a (strongly) convex quadratic. Thus, we can minimize g (U) by setting its gradient to 0.
i
For any fixed row i ∈ {1,...,n}, we can differentiate and obtain
∇ g (U) = 2VTW (VU −A )+γU −Ψ −ρ (Z −U ).
Ui,⋆ i i i,⋆ i⋆ i,⋆ i,⋆ 2 i,⋆ i,⋆
By equating the gradient ∇ g (U) to 0 and rearranging, we obtain that the optimal vec-
Ui,⋆ i
tor U¯ is given by (13). This completes the proof.
i,⋆
Observe that since the matrix VTW V is positive semidefinite and γ + ρ > 0, the
i 2
matrix inverse [2VTW V +(γ +ρ )I ]−1 is well defined for all i ∈ {1,...,n}. Computing
i 2 k
the optimal solution to (12) requires computing n different k ×k matrix inverses (where
in general k ≪ min{m,n}). Computing a single k×k matrix inverse requires O(k3) time
16Mixed-Projection ADMM
and forming the matrix product VTW V requires O(k2m) time for a given i. Thus, the
i
complexity of computing the optimal solution for a single column is O(k3 +k2m). Notice
that each column of U¯ can be computed independently of the other columns. We leverage
this observation by developing a multi-threaded implementation of the algorithm presented
in this section. Letting w denote the number of compute threads available, computing the
(cid:16) (cid:17)
optimal solution U¯ of (12) requires O k3n+k2mn time (the term min{w,n} in the denomi-
min{w,n}
nator reflects that fact that increasing the number of available compute threads beyond the
number of columns of U¯ does not yield additional reduction in compute complexity).
5.2 Subproblem in V
Now, suppose we fix variables U,P,Z,Φ,Ψ and seek to minimize L(U,V,P,Z,Φ,Ψ)
over V. Eliminating terms that do not depend on V, the resulting subproblem is given by
(cid:88) γ
min ((UVT) −A )2+ ∥V∥2.
V∈Rm×k ij ij 2 F (14)
(i,j)∈Ω
We now have the following result:
Proposition 9 The optimal solution V¯ for (14) is given by
V¯ = [2UTW U +γI ]−1[2UTW A ], (15)
j,⋆ j k j ⋆,j
for each j ∈ {1,...,m} where each W ∈ Rn×n is a diagonal matrix satisfying (W ) = 1
j j ii
if (i,j) ∈ Ω, otherwise (W ) = 0. Here, the column vector V¯ ∈ Rk denotes the jth row
j ii j,⋆
of V¯ while the column vector A ∈ Rn denotes the jth column of A where the unknown
⋆,j
entries of A are taken to be 0.
Proof ThisprooffollowstheproofofProposition8. Letf(V)denotetheobjectivefunction
of (14). With {W }m defined as in Proposition 9, observe that we can write f(V) as
j j=1
m m
(cid:88) γ (cid:88)
f(V) = ∥W (UV −A )∥2+ ∥V ∥2
j j,⋆ ⋆,j 2 2 j,⋆ 2
j=1 j=1
m (cid:20) (cid:21) m
(cid:88) γ (cid:88)
= ∥W (UV −A )∥2+ ∥V ∥2 = g (V),
j j,⋆ ⋆,j 2 2 j,⋆ 2 j
j=1 j=1
where we define g (V) = ∥W (UV −A )∥2+ γ∥V ∥2. Thus, we have shown that f(V)
j j j,⋆ ⋆,j 2 2 j,⋆ 2
is separable over the rows of the matrix V. Each function g (V) is a (strongly) convex
j
quadratic. Thus, we can minimize g (V) by setting its gradient to 0. For any fixed row
j
j ∈ {1,...,m}, we can differentiate and obtain
∇ g (V) = 2UTW (UV −A )+γV .
Vj,⋆ j j j,⋆ ⋆,j j,⋆
By equating the gradient ∇ g (V) to 0 and rearranging, we obtain that the optimal vec-
Vj,⋆ j
tor U¯ is given by (15). This completes the proof.
i,⋆
17Bertsimas and Johnson
Observe that since the matrix UTW U is positive semidefinite and γ > 0, the matrix
j
inverse [2UTW U +γI ]−1 is well defined for all j ∈ {1,...,m}. Computing the optimal
j k
solution to (14) requires computing m different k×k matrix inverses. Forming the matrix
productUTW U requiresO(k2n)timeforagivenj. Thus,thecomplexityofcomputingthe
j
optimal solution for a single column is O(k3+k2n). Notice that, similarly to the solution of
(12), eachcolumnofV¯ canbecomputedindependentlyoftheothercolumns. Asbefore, we
leverage this observation in our multi-threaded implementation of the algorithm presented
in this section. Letting w denote the number of compute threads available, computing the
(cid:16) (cid:17)
optimal solution V¯ of (14) requires O k3m+k2mn time.
min{w,m}
The optimal solution V¯ to (14) reveals that the frobenius norm regularization term
on V in (8) (which emerges from the nuclear norm regularization term on X in (1)) has
computational benefits. Indeed, if we had γ = 0, it is possible that the matrix UTW U
j
be singular at certain iterates of our ADMM algorithm, in which case the corresponding
matrix inverse would be undefined. This observation is in keeping with several recent works
in the statistics, machine learning and operations research literatures where the presence of
aregularizationpenaltyintheobjectivefunctionyieldsimprovedoutofsampleperformance
as well as benefits in computational tractability (see for example Bertsimas et al. (2020,
2021, 2023c,a); Bertsimas and Johnson (2024)).
5.3 Subproblem in P
Now, suppose we fix variables U,V,Z,Φ,Ψ and seek to minimize L(U,V,P,Z,Φ,Ψ)
over P. Eliminating terms that do not depend on P, the resulting subproblem is given by
ρ
min −λTr(YTPY)−Tr(ΦTPZ)+ 1 ∥(I −P)Z∥2
P∈Sn
+
2 n F
(16)
s.t. P2 = P, Tr(P) ≤ k.
We now have the following result:
Proposition 10 Let MΣMT be a rank k truncated singular value decomposition for the
matrix given by:
(cid:16) ρ 1 (cid:17)
λYYT + 1 ZZT + (ΦZT +ZΦT) ,
2 2
where Σ ∈ Rk×k,M ∈ Rn×k,MTM = I .The optimal solution P¯ for (16) is given by
k
P¯ = MMT.
Proof Let f(P) denote the objective function of (16). Observe that for any P that is
feasible to (16), we can write f(P) as:
ρ
f(P) = −λTr(YTPY)−Tr(ΦTPZ)+ 1 ∥(I −P)Z∥2
2 n F
ρ
= −λTr(YYTP)−Tr(ZΦTP)+ 1 Tr(ZZT(I −P))
n
2
ρ ρ
= 1 Tr(ZZT)−⟨λYYT +ZΦT + 1 ZZT,P⟩.
2 2
18Mixed-Projection ADMM
Thus, it is immediately clear that a solution will be optimal to (16) if and only if it is
optimal to the problem given by:
max ⟨C,P⟩
P∈Sn
+ (17)
s.t. P2 = P, Tr(P) ≤ k,
wherewedefinethematrixC ∈ Rn×n asC = λYYT+ZΦT+ρ1ZZT. LetC¯ = 1(C+CT)
2 2
denotethesymmetricpartofC. ObservethatforanysymmetricmatrixP,wecanconsider
⟨C¯,P⟩ in place of ⟨C,P⟩ since we have
n n n n−1 n
(cid:88)(cid:88) (cid:88) (cid:88) (cid:88)
⟨C,P⟩ = P C = P C + P (C +C )
ij ij ii ii ij ij ji
i=1 j=1 i=1 i=1j=i+1
n n−1 n n n
(cid:88) (cid:88) (cid:88) (cid:88)(cid:88)
= P C¯ +2 P C¯ = P C¯ = ⟨C¯,P⟩.
ii ii ij ij ij ij
i=1 i=1j=i+1 i=1 j=1
LetC¯ = MΣMT beafullsingularvaluedecompositionofC¯ withM,Σ ∈ Rn×n,MTM =
MMT = I . The matrix Σ is the diagonal matrix of (ordered) singular values of C¯ and
n
we let σ denote the ith singular value. Any feasible matrix P to (17) can be written as
i
P = LLT where L ∈ Rn×k,LTL = I . Thus, for any P feasible to (17) we express the
k
objective value as:
n
(cid:88)
⟨C¯,P⟩ = Tr(MΣMTLLT) = Tr(Σ(MTLLTM)) = σ ∥(MTL) ∥2.
i i,⋆ 2
i=1
Let N = MTL ∈ Rn×k. Note that we have NTN = LTMMTL = LTL = I , which
k
implies that the columns of N are orthonormal. This immediately implies that we have
NT N = ∥(MTL) ∥2 ≤ 1. Moreover, we have
i,⋆ i,⋆ i,⋆ 2
n n n k k n k
(cid:88) (cid:88) (cid:88)(cid:88) (cid:88)(cid:88) (cid:88)
∥(MTL) ∥2 = NT N = N2 = N2 = 1 = k.
i,⋆ 2 i,⋆ i,⋆ ij ij
i=1 i=1 i=1 j=1 j=1 i=1 j=1
We can therefore upper bound the optimal objective value of (17) as
n k
(cid:88) (cid:88)
⟨C¯,P⟩ = σ ∥(MTL) ∥2 ≤ σ .
i i,⋆ 2 i
i=1 i=1
To conclude the proof, notice that by taking P¯ = M¯M¯T where M¯ ∈ Rn×k is the matrix
that consists of the first k columns of M we can achieve the upper bound on (17):
k
(cid:88)
⟨C¯,P¯⟩ = Tr(MΣMTM¯M¯T) = Tr(M¯TMΣMTM¯) = σ .
i
i=1
19Bertsimas and Johnson
To compute the optimal solution of (16), we need to compute a rank k singular value
decomposition of the matrix C¯ = (cid:0) λYYT + ρ1ZZT + 1(ΦZT + ZΦT)(cid:1) which requires
2 2
O(kn2) time since C¯ ∈ Rn×n. Moreover, explicitly forming the matrix C¯ in memory from
its constituent matrices Y,Z,Φ requires O(n2(d+k)) operations. Thus, naively computing
the optimal solution to (16) has complexity O(n2(d+k)) where the bottleneck operation
from a complexity standpoint is explicitly forming the matrix C¯.
Fortunately, it is possible to compute the optimal solution to (16) more efficiently.
Observe that we can equivalently express the matrix C¯ as C¯ = F FT where F ,F ∈
1 2 1 2
Rn×(d+3k) are defined as
(cid:16)√ (cid:113) (cid:113) (cid:113) (cid:17)
F = λY ρ1Z 1Φ 1Z ,
1 2 2 2
(cid:16)√ (cid:113) (cid:113) (cid:113) (cid:17)
F = λY ρ1Z 1Z 1Φ .
2 2 2 2
Computing a truncated singular value decomposition requires only computing repeated
matrix vector products. Therefore, rather than explicitly forming the matrix C¯ in memory
at a cost of O(n2(d + k)) operations, in our implementation we design a custom matrix
class where matrix vector products between C¯ and arbitrary vectors x ∈ Rn are computed
by first evaluating the matrix vector product ν = FTx and subsequently evaluating the
2
matrix vector product C¯x = F ν. In so doing, we can evaluate matrix vector products C¯x
1
inO(n(d+k))timeratherthanO(n2)time(ingeneral, wewillhaved+k ≪ n). Computing
a truncated singular value decomposition of C¯ with this methodology of evaluating matrix
vector products requires only O(k2n + knd) operations. Thus, our custom matrix class
implementationavoidsneedingtoexplicitlyforC¯ inmemoryandallowstheoptimalsolution
to (16) to be computed in O(k2n+knd) time.
5.4 Subproblem in Z
Now, suppose we fix variables U,V,P,Φ,Ψ and seek to minimize L(U,V,P,Z,Φ,Ψ)
over Z. Eliminating terms that do not depend on Z, the resulting subproblem is given by
ρ ρ
Z∈m Ri nn ×kTr(ΦT(I n−P)Z)+Tr(ΨTZ)+ 21 ∥(I n−P)Z∥2
F
+ 22 ∥Z −U∥2 F. (18)
We now have the following result:
Proposition 11 The optimal solution Z¯ for (18) is given by
1 (cid:16) ρ (cid:17)(cid:16) (cid:17)
Z¯ = I + 1 P ρ U −(I −P)Φ−Ψ
n 2 n
ρ +ρ ρ
1 2 2 (19)
1 (cid:16) ρ (cid:17)
1
= ρ U −Φ+PΦ−Ψ+ρ PU − PΨ .
2 1
ρ +ρ ρ
1 2 2
Proof Let f(Z) denote the objective function of (18). The function f(Z) is a convex
quadratic, thus it can be minimized by setting its gradient to 0. Differentiating f(Z), we
obtain:
∇ f(Z) = (I −P)TΦ+Ψ+ρ (I −P)T(I −P)Z +ρ (Z −U).
Z n 1 n n 2
20Mixed-Projection ADMM
Moreover, for any matrix P for which the augmented lagrangian (11) takes finite value, we
will have P ∈ P which implies that PT = P and P2 = P. We can therefore simplify
k
∇ f(Z) as:
Z
∇ f(Z) = (I −P)Φ+Ψ+ρ (I −P)Z +ρ (Z −U).
Z n 1 n 2
By equating the gradient ∇ f(Z) to 0 and rearranging, we obtain that the optimal matrix
Z
Z¯ is given by:
(cid:16) (cid:17)−1(cid:16) (cid:17)
Z¯ = ρ (I −P)+ρ I ρ U −(I −P)Φ−Ψ
1 n 2 n 2 n
To conclude the proof, it remains to show that (cid:0) ρ (I −P)+ρ I (cid:1)−1 = 1 (cid:0) I + ρ1P(cid:1) .
1 n 2 n ρ1+ρ2 n ρ2
Let P = MMT where M ∈ Rn×k,MTM = I . Such a matrix M is guaranteed to exist
k
for any P ∈ P . We have
k
ρ (I −P)+ρ I = ρ (I −MMT)+ρ I
1 n 2 n 1 n 2 n
= (ρ +ρ )I +M(−ρ I )MT
1 2 n 1 n
1 1
(cid:18)
1 1
(cid:19)−1
= I − M MTM − I MT
ρ +ρ n (ρ +ρ )2 ρ +ρ ρ k
1 2 1 2 1 2 1
(cid:18) (cid:19)
1 1 −ρ (ρ +ρ )
= I − M 1 1 2 I MT
ρ +ρ n (ρ +ρ )2 ρ k
1 2 1 2 2
1 (cid:16) ρ (cid:17)
1
= I + P ,
n
ρ +ρ ρ
1 2 2
where the third equality follows from the Woodbury matrix inversion lemma (see Pe-
tersen et al. (2008), Section 3.2.2). As a sanity check, one can verify that the product
of (cid:0) ρ (I −P)+ρ I (cid:1) and 1 (cid:0) I +ρ1P(cid:1) is indeed the n dimensional identity matrix.
1 n 2 n ρ1+ρ2 n ρ2
Evaluating the optimal solution to (18) requires only matrix-matrix multiplications.
Computing the products of PΦ,PU,PΨ in the definition of Z¯ from (19) requires O(kn2)
operations. Thus, the naive cost of forming Z¯ is O(kn2). However, notice that if we had
a factored representation of the matrix P as P = MMT with M ∈ Rn×k, for any matrix
R ∈ Rn×k we could compute matrix-matrix products PR by first computing S = MTR
and thereafter computing PR = MS for a total complexity of O(k2n). One might object
that this ignores the time required to compute such a matrix M. However, observe that
in computing a matrix P that is optimal to (16), we in fact must already generate such a
matrix M (see proposition 10). In fact, in our implementation we never explicitly form a
n×n matrix P as it suffices to only store a copy of its low rank factorization matrix M.
Thus, the optimal solution to (18) can be evaluated in O(k2n) time.
5.5 An ADMM Algorithm
Having illustrated that the partial minimization of the lagrangian (11) across each of the
primal variables (Problems (12), (14), (16), (18)) can be solved efficiently, we can now
present the overall approach Algorithm 1.
We initialize primal iterates U 0 = Z 0 = LΣ21 ,P 0 = LLT,V 0 = RΣ1 2 where LΣR
denotes a rank k truncated singular value decomposition of A (the missing entries of A
21Bertsimas and Johnson
Algorithm 1: Mixed-Projection ADMM
Data: n,m,k ∈ Z+,Ω ⊂ [n]×[m],{A } ,λ,γ ∈ R+. Tolerance parameter
ij (i,j)∈Ω
ϵ > 0. Maximum iteration parameter T ∈ Z+
Result: (U¯,V¯,P¯) that is feasible to (8).
(U 0,P 0,V 0,Z 0) ←−
(LΣ1 2,LLT,RΣ21 ,LΣ1
2) where LΣR is a rank k truncated
SVD of A and missing entries are filled in with 0;
(Φ ,Ψ ) ←− (1 ,1 );
0 0 n×k n×k
t ←− 0;
while t < T and max{∥(I −P )Z ∥2,∥Z −U ∥2} > ϵ do
n t t F t t F
(U ,P ) ←− argmin L(U,V ,P,Z ,Φ ,Ψ );
t+1 t+1 U,P t t t t
(V ,Z ) ←− argmin L(U ,V,P ,Z,Φ ,Ψ );
t+1 t+1 V,Z t+1 t+1 t t
Φ ←− Φ +ρ (I −P )Z ;
t+1 t 1 t+1 t+1
Ψ ←− Ψ +ρ (Z −U );
t+1 t 2 t+1 t+1
t ←− t+1;
end
return (U ,V ,P )
t t t
are filled in with 0s) and we initialize dual iterates Φ = Ψ = 1 . Observe that the
0 0 n×k
subproblems (12) and (16) can be solved simultaneously. Similarly, the subprobems (14)
and(18)canbesolvedsimultaneously. AteachiterationofAlgorithm1, wefirstupdatethe
iterates U ,P by solving problems (12) and (16) with (V ,Z ,Φ ,Ψ ) fixed. Next, we
t+1 t+1 t t t t
update the iterates V ,Z by solving problems (14) and (18) with (U ,P ,Φ ,Ψ )
t+1 t+1 t+1 t+1 t t
fixed. Finally, we update the dual iterates Φ,Ψ by taking a gradient ascent step. The
gradientsoftheaugmentedlagrangian(11)withrespecttoΦandΨaregivenbytheprimal
residuals(I −P )Z andZ −U respectively. Weuseρ andρ respectivelyasthe
n t+1 t+1 t+1 t+1 1 2
step size. We proceed until the squared norm of each primal residual is below a numerical
tolerance parameter ϵ or until we reach an input maximum number of iterations T. We
know have the following result:
Proposition 12 Assume that the number of compute threads w is less than min{n,m}.
The per iteration complexity of Algorithm 1 is O(cid:0) k2n+knd+ k3(n+m)+k2nm(cid:1) .
w
Proof The result follows from the complexity analysis of problems (12), (14), (16) and
(18).
6. Computational Results
We evaluate the performance of Algorithm 1 implemented in Julia 1.7.3. Throughout,
we fix ρ = ρ = 10, set the maximum number of iterations T = 20 and set the num-
1 2
ber of compute threads w = 24. Note that given the novelty of Problem (1), there are
no pre-existing specialized methods to benchmark against. Accordingly, we compare the
performance of Algorithm 1 against well studied methods for the very closely related MC
problem as well as a highly performant generic method for low rank matrix optimization
22Mixed-Projection ADMM
problems. The MC methods we consider are Fast-Impute (Bertsimas and Li, 2020), Soft-
Impute (Mazumder et al., 2010) and Iterative-SVD (Troyanskaya et al., 2001) which we
introduced formally in Section 2.1. We utilize the implementation of Fast-Impute made
publicly available by (Bertsimas and Li, 2020) while we use the implementation of Soft-
Impute and Iterative-SVD from the python package fancyimpute 0.7.0 (Rubinsteyn and
Feldman, 2016). The matrix optimization method we consider is ScaledGD (scaled gradi-
entdescent)(Tongetal.,2021)whichweintroducedformallyinSection2.2.1andimplement
ourselves. All experiments were performed using synthetic data on MIT’s Supercloud Clus-
ter (Reuther et al., 2018), which hosts Intel Xeon Platinum 8260 processors. To bridge
the gap between theory and practice, we have made our code freely available on GitHub at
github.com/NicholasJohnson2020/LearningLowRankMatrices.
To evaluate the performance of Algorithm 1, Fast-Impute, Soft-Impute, Iterative-SVD
and ScaledGD on synthetic data, we consider the objective value achieved by a returned
solution in (1), the ℓ reconstruction error between a returned solution and the ground
2
truth, the numerical rank of a returned solution and the execution time of each algorithm.
Explicitly, let Xˆ ∈ Rn×m denote the solution returned by a given method (where we define
Xˆ = UˆVˆT if the method outputs low rank factors Uˆ,Vˆ) and let Atrue ∈ Rn×m denote the
ground truth matrix. We define the the ℓ reconstruction error of Xˆ as
2
∥Xˆ −Atrue∥2
ERR (Xˆ) = F.
ℓ2 ∥Atrue∥2
F
We compute the numerical rank of Xˆ by calling the default rank function from the Julia
LinearAlgebra package. We aim to answer the following questions:
1. How does the performance of Algorithm 1 compare to existing methods such as Fast-
Impute, Soft-Impute, Iterative-SVD and ScaledGD on synthetic data?
2. How is the performance of Algorithm 1 affected by the number of rows n, the number
of columns m, the dimension of the side information d and the underlying rank k of
the ground truth?
3. Empircially, which subproblem solution update is the computational bottleneck of
Algorithm 1?
6.1 Synthetic Data Generation
To generate synthetic data, we specify a number of rows n ∈ Z , a number of columns
+
m ∈ Z , a desired rank k ∈ Z with k < min{n,m}, the dimension of the side information
+ +
d ∈ Z , a fraction of missing values α ∈ (0,1) and a noise parameter σ ∈ R that controls
+ +
the signal to noise ratio. We sample matrices U ∈ Rn×k,V ∈ Rm×k,β ∈ Rm×d by drawing
each entry U ,V ,β independently from the uniform distribution on the interval [0,1].
ij ij ij
Furthermore, wesampleanoisematrixN ∈ Rn×d bydrawingeachentryN independently
ij
from the univariate normal distribution with mean 0 and variance σ2. We let A = UVT
and we let Y = Aβ + N. Lastly, we sample ⌊α · n · m⌋ indices uniformly at random
from the collection I = {(i,j) : 1 ≤ i ≤ n,1 ≤ j ≤ m} to be the set of missing indices,
which we denote by Γ. The set of revealed entries can then be defined as Ω = I \Γ. We
23Bertsimas and Johnson
fix α = 0.9,σ = 2 throughout our experiments and report numerical results for various
different combinations of (n,m,d,k).
6.2 Sensitivity to Row Dimension
We present a comparison of Algorithm 1 with ScaledGD, Fast-Impute, Soft-Impute and
Iterative-SVDaswevarythenumberofrowsn. Intheseexperiments,wefixedm = 100,k =
5, and d = 150 across all trials. We varied n ∈ {100,200,400,800,1000,2000,5000,10000}
and we performed 20 trials for each value of n. For ScaledGD, we set the step size to be
η = 1 where σ (A) denotes the largest singular value of the input matrix A where
10σ1(A) 1
we fill the unobserved entries with the value 0. Letting f(U ,V ) denote the objective value
t t
achieved after iteration t of ScaledGD, we terminate ScaledGD when either t > 1000 or
f(Ut−1,Vt−1)−f(Ut,Vt) < 10−3. In words, we terminate ScaledGD after 1000 iterations or
f(Ut−1,Vt−1)
after the relative objective value improvement between two iterations is less than 0.1%.
 2 E M H F W L Y H  9 D O X H  Y V  1  /   5 H F R Q V W U X F W L R Q  ( U U R U  Y V  1
    
 $ O J R U L W K P  
      
 I D V W , P S X W H
    
        V R I W , P S X W H
 6 9 '     
      
    
      
           
           
      
                                                       
 1  1
 5 D Q N  R I  5 H W X U Q H G  6 R O X W L R Q  Y V  1  ( [ H F X W L R Q  7 L P H   P V   Y V  1
    
   
       
  
    
  
    
  
 
                                                       
 1  1
Figure 1: Objective value (top left), ℓ reconstruction error (top right), fitted rank (bottom
2
left)andexecutiontime(bottomright)versusnwithm = 100,k = 5andd = 150.
Averaged over 20 trials for each parameter configuration.
24
 H X O D 9  H Y L W F H M E 2
 Q R L W X O R 6  G H Q U X W H 5  I R  N Q D 5
 U R U U (  Q R L W F X U W V Q R F H 5   /
  V P   H P L 7  Q R L W X F H [ (Mixed-Projection ADMM
We report the objective value, ℓ reconstruction error, fitted rank and execution time
2
for Algorithm 1, Fast-Impute, Soft-Impute and Iterative-SVD in Figure 1. We additionally
reporttheobjectivevalue,reconstructionerrorandexecutiontimeforScaledGD,Algorithm
1, Fast-Impute, Soft-Impute and Iterative-SVD in Tables 1, 2 and 3 of Appendix A. In
Figure 2, we plot the average cumulative time spent solving subproblems (12), (14), (16),
(18) during the execution of Algorithm 1 versus n. Our main findings from this set of
experiments are:
1. Algorithm 1 systematically produces higher quality solutions than ScaledGD, Fast-
Impute, Soft-Impute and Iterative-SVD (see Table 1), sometimes achieving an objec-
tive value that is an order of magnitude superior than the next best method. On
average, Algorithm 1 outputs a solution whose objective value is 86% lesser than the
objective value achieved by the best performing alternative method (Fast-Impute).
We remind the reader that Fast-Impute, Soft-Impute and Iterative-SVD are methods
designedforthegenericMCproblemandarenotcustombuilttosolve(1)soitshould
not come as a surprise that Algorithm 1 significantly outperforms these 3 methods in
terms of objective value. ScaledGD however has explicit knowledge of the objective
function of (1) along with its gradient, yet surprisingly produces the weakest average
objective value across these experiments. We note that we use the default hyper-
parameters for ScaledGD recommended by the authors of this method (Tong et al.,
2021). We observe that the objective value achieved by all methods increases linearly
as the number of rows n increases.
2. In terms of ℓ reconstruction error, Algorithm 1 again systematically produces so-
2
lutions that are of higher quality than ScaledGD, Fast-Impute, Soft-Impute and
Iterative-SVD (see Table 2), often achieving an error that is an order of magnitude
superior than the next best method. On average, Algorithm 1 outputs a solution
whose ℓ reconstruction error is 92% lesser than the reconstruction error achieved by
2
the best performing alternative method (Soft-Impute in all but one parameter con-
figuration). This is especially noteworthy since Algorithm 1 is not designed explicitly
with reconstruction error minimization as the objective, unlike Fast-Impute and Soft-
Impute, and suggests that the side information Y is instrumental in recovering high
quality low rank estimates of the partially observed data matrix.
3. We observe that the fitted rank of the solutions returned by Algorithm 1, ScaledGD
and Fast-Impute always matched the specified target rank as would be expected, but
surprisingly the solutions returned by Soft-Impute and Iterative-SVD were always
of full rank despite the fact that these methods were provided with the target rank
explicitly. This is potentially due to a numerical issues in the computation of the rank
due to presence of extremely small singular values.
4. The runtime of Algorithm 1 is competitive with that of the other methods. The
runtime of Algorithm 1 is less than of Soft-Impute and Iterative-SVD but greater
than that of Fast-Impute. For experiments with n ≤ 2000, Table 3 illustrates that
ScaledGD was the method with the fastest execution time (however as previously
mentioned the returned solutions were of low quality). The runtime of Algorithm 1,
Fast-Impute, Soft-Impute and iterate SVD appear to grow linearly with n.
25Bertsimas and Johnson
5. Figure 2 illustrates that the computation of the solution for (12) is the computational
bottleneck in the execution of Algorithm 1 in this set of experiments, followed next
by the computation of the solution for (16). Empirically, we observe that the solution
time of (12), (14), (16) and (18) appear to scale linearly with the number of rows n.
This observation is consistent with the computational complexities derived for each
subproblem of Algorithm 1 in Section 5.
 $ O J R U L W K P    8 S G D W H  ( [ H F X W L R Q  7 L P H  Y V  1
 8
 9
 3
      =
   
   
   
   
 
                           
 1
Figure 2: CumulativetimespentsolvingeachsubproblemofAlgorithm1versusnwithm =
100,k = 5 and d = 150. Averaged over 20 trials for each parameter configuration.
6.3 Sensitivity to Column Dimension
Here, we present a comparison of Algorithm 1 with ScaledGD, Fast-Impute, Soft-Impute
and Iterative-SVD as we vary the number of columns m. We fixed n = 1000,k = 5, and
d = 150 across all trials. We varied m ∈ {100,200,400,800,1000,2000,5000,10000} and we
performed 20 trials for each value of m.
We report the objective value, ℓ reconstruction error, fitted rank and execution time
2
for Algorithm 1, Fast-Impute and Soft-Impute in Figure 3. We additionally report the
26
  V P   H P L 7  Q R L W X F H [ (Mixed-Projection ADMM
  H   2 E M H F W L Y H  9 D O X H  Y V  0  /   5 H F R Q V W U X F W L R Q  ( U U R U  Y V  0
   
 $ O J R U L W K P       
 I D V W , P S X W H
        
 V R I W , P S X W H
    
   
    
        
    
   
    
        
                                                       
 0  0
 5 D Q N  R I  5 H W X U Q H G  6 R O X W L R Q  Y V  0  ( [ H F X W L R Q  7 L P H   P V   Y V  0
    
      
          
      
   
     
   
     
   
     
   
                                                       
 0  0
Figure 3: Objective value (top left), ℓ reconstruction error (top right), fitted rank (bottom
2
left) and execution time (bottom right) versus m with n = 1000,k = 5 and
d = 150. Averaged over 20 trials for each parameter configuration.
objective value, reconstruction error and execution time for ScaledGD, Algorithm 1, Fast-
Impute, Soft-Impute and Iterative-SVD in Tables 4, 5 and 6 of Appendix A. In Figure 4,
we plot the average cumulative time spent solving subproblems (12), (14), (16), (18) during
the execution of Algorithm 1 versus m. Our main findings from this set of experiments are
a follows:
1. Hereagain,Algorithm1systematicallyproduceshigherqualitysolutionsthanScaledGD,
Fast-Impute, Soft-Impute and Iterative-SVD (see Table 4). On average, Algorithm 1
outputsasolutionwhoseobjectivevalueis62%lesserthantheobjectivevalueachieved
by the best performing alternative method (Fast-Impute). Here again, ScaledGD pro-
duces the weakest average objective value across these experiments. We observe that
the objective value achieved by each method appears to increase super-linearly as the
number of columns m increases.
2. In terms of ℓ reconstruction error, Algorithm 1 again systematically produces so-
2
lutions that are of higher quality than ScaledGD, Fast-Impute, Soft-Impute and
27
 Q R L W X O R 6  G H Q U X W H 5  I R  N Q D 5
 H X O D 9  H Y L W F H M E 2
  V P   H P L 7  Q R L W X F H [ (
 U R U U (  Q R L W F X U W V Q R F H 5   /Bertsimas and Johnson
Iterative-SVD(seeTable5), oftenachievinganerrorthatisanorderofmagnitudesu-
perior than the next best method. On average, Algorithm 1 outputs a solution whose
ℓ reconstructionerroris90%lesserthanthereconstructionerrorachievedbythebest
2
performing alternative method (Soft-Impute in all but one parameter configuration).
3. The fitted rank of the solutions returned by Algorithm 1, ScaledGD and Fast-Impute
always matched the specified target rank, but the solutions returned by Soft-Impute
and Iterative-SVD were always of full rank despite the fact that these methods were
provided with the target rank explicitly.
4. The runtime of Algorithm 1 exhibits the most favorable scaling behavior among the
methods tested in these experiments. For instances with m ≥ 2000, Table 6 shows
that Algorithm 1 had the fastest runtime. For instances with m < 2000, ScaledGD
had the fastest execution time but produced low quality solutions. The runtime of all
methods tested grow super-linearly with m.
5. Figure 4 illustrates that the computation of the solution for (12) and (14) are the
computational bottlenecks in the execution of Algorithm 1 in this set of experiments
while the computation of the solution for (18) and (16) appear to be a constant
function of m. This observation is consistent with the complexity analysis performed
for each subproblem of Algorithm 1 in Section 5. Indeed, this analysis indicated that
solve times for (18) and (16) are independent of m while the solve times for (12) and
(14) scale linearly with m when the number of threads w satisfies w < m.
6.4 Sensitivity to Side Information Dimension
We present a comparison of Algorithm 1 with ScaledGD, Fast-Impute, Soft-Impute and
Iterative-SVD as we vary the dimension of the side information d. In these experiments, we
fixedn = 1000,m = 100andk = 5acrossalltrials. Wevariedd ∈ {10,50,100,150,200,250,500,1000}
and we performed 20 trials for each value of d.
We report the objective value, ℓ reconstruction error, fitted rank and execution time
2
for Algorithm 1, Fast-Impute, Soft-Impute and Iterative-SVD in Figure 5. We additionally
reporttheobjectivevalue,reconstructionerrorandexecutiontimeforScaledGD,Algorithm
1, Fast-Impute, Soft-Impute and Iterative-SVD in Tables 7, 8 and 9 of Appendix A. In
Figure 6, we plot the average cumulative time spent solving subproblems (12), (14), (16),
(18) during the execution of Algorithm 1 versus d. Our main findings from this set of
experiments are:
1. Just as in Sections 6.2 and 6.3, Algorithm 1 systematically produces higher quality
solutions than ScaledGD, Fast-Impute, Soft-Impute and Iterative-SVD (see Table 7).
On average, Algorithm 1 outputs a solution whose objective value is 85% lesser than
theobjectivevalueachievedbythebestperformingalternativemethod(Fast-Impute).
ScaledGDproducestheweakestaverageobjectivevalueacrosstheseexperiments. The
objective value achieved by each method appears to increase linearly as the dimension
d of the side information increases.
28Mixed-Projection ADMM
 $ O J R U L W K P    8 S G D W H  ( [ H F X W L R Q  7 L P H  Y V  0
 8
 9
     
 3
 =
     
     
    
    
    
    
 
                           
 0
Figure 4: Cumulative time spent solving each subproblem of Algorithm 1 versus m with
n = 1000,k = 5 and d = 150. Averaged over 20 trials for each parameter
configuration.
2. Intermsofℓ reconstructionerror,justasinSections6.2and6.3Algorithm1produces
2
solutions that are of higher quality than ScaledGD, Fast-Impute, Soft-Impute and
Iterative-SVD (see Table 8), often achieving an error that is an order of magnitude
superior than the next best method. On average, Algorithm 1 outputs a solution
whose ℓ reconstruction error is 93% lesser than the reconstruction error achieved by
2
thebestperformingalternativemethod(Soft-Impute). TheperformanceofAlgorithm
1 improves as d increases, consistent with the intuition that recovering the partially
observed matrix A becomes easier as more side information becomes available.
3. The runtime of Algorithm 1 is competitive with that of the other methods. The
runtime of Algorithm 1 is less than of Soft-Impute and Iterative-SVD but greater
thanthatofFast-Impute. Table9illustratesthatScaledGDwasthefastestperforming
method, however its solutions were of the lowest quality. The runtime of Algorithm
1 and ScaledGD grows with d while Fast-Impute, Soft-Impute and iterate SVD are
29
  V P   H P L 7  Q R L W X F H [ (Bertsimas and Johnson
 2 E M H F W L Y H  9 D O X H  Y V  '  /   5 H F R Q V W U X F W L R Q  ( U U R U  Y V  '
           
 $ O J R U L W K P  
        I D V W , P S X W H
    
 V R I W , P S X W H
      
 6 9 '
           
      
    
      
    
      
      
                                             
 '  '
 5 D Q N  R I  5 H W X U Q H G  6 R O X W L R Q  Y V  '  ( [ H F X W L R Q  7 L P H   P V   Y V  '
   
   
   
  
   
  
   
      
   
  
   
                                             
 '  '
Figure 5: Objective value (top left), ℓ reconstruction error (top right), fitted rank (bottom
2
left) and execution time (bottom right) versus d with n = 1000,m = 100 and
k = 5. Averaged over 20 trials for each parameter configuration.
constant with d which should be expected as these methods do not act on the side
information matrix Y.
4. Figure 6 illustrates that the computation of the solution for (16) is the computational
bottleneck in the execution of Algorithm 1 in this set of experiments, followed next
by the computation of the solution to (12). The solution times for (12), (14) and (18)
appear constant as a function of d. This is consistent with the complexity analysis
from Section 5 which found that the solve time for (16) is linear in d while the solve
time for the 3 other subproblems are independent of d.
6.5 Sensitivity to Target Rank
We present a comparison of Algorithm 1 with ScaledGD, Fast-Impute, Soft-Impute and
Iterative-SVDaswevarytherankoftheunderlyingmatrixk. Intheseexperiments,wefixed
30
 H X O D 9  H Y L W F H M E 2
 Q R L W X O R 6  G H Q U X W H 5  I R  N Q D 5
 U R U U (  Q R L W F X U W V Q R F H 5   /
  V P   H P L 7  Q R L W X F H [ (Mixed-Projection ADMM
 $ O J R U L W K P    8 S G D W H  ( [ H F X W L R Q  7 L P H  Y V  '
   
 8
 9
 3
     =
   
   
   
   
  
 
                      
 '
Figure 6: Cumulative time spent solving each subproblem of Algorithm 1 versus d with
n = 1000,m = 100 and k = 5. Averaged over 20 trials for each parameter
configuration.
n = 1000,m = 100 and d = 150 across all trials. We varied k ∈ {5,10,15,20,25,30,35,40}
and we performed 20 trials for each value of d.
We report the objective value, ℓ reconstruction error, fitted rank and execution time
2
for Algorithm 1, Fast-Impute, Soft-Impute and Iterative-SVD in Figure 7. We additionally
reporttheobjectivevalue,reconstructionerrorandexecutiontimeforScaledGD,Algorithm
1, Fast-Impute, Soft-Impute and Iterative-SVD in Tables 10, 11 and 12 of Appendix A. In
Figure 8, we plot the average cumulative time spent solving subproblems (12), (14), (16),
(18) during the execution of Algorithm 1 versus k. Our main findings from this set of
experiments are as follows:
1. UnlikeinSections6.2, 6.3and6.4, Algorithm1onlyproducedhigherqualitysolutions
than all benchmark methods in 3 out of 8 of the tested parameter configurations
where k ≤ 15 (see Table 10). Fast-Impute was the best performing method in 3
configurations and Soft-Impute was best in the remaining 2 configurations. ScaledGD
produces the weakest average objective value across these experiments.
31
  V P   H P L 7  Q R L W X F H [ (Bertsimas and Johnson
  H   2 E M H F W L Y H  9 D O X H  Y V  .  /   5 H F R Q V W U X F W L R Q  ( U U R U  Y V  .
 $ O J R U L W K P       
 I D V W , P S X W H
    
   V R I W , P S X W H
 6 9 '     
      
    
      
    
      
                           
 .  .
 5 D Q N  R I  5 H W X U Q H G  6 R O X W L R Q  Y V  .  ( [ H F X W L R Q  7 L P H   P V   Y V  .
        
    
  
    
  
    
       
    
  
   
                           
 .  .
Figure 7: Objective value (top left), ℓ reconstruction error (top right), fitted rank (bottom
2
left) and execution time (bottom right) versus k with n = 1000,m = 100 and
d = 150. Averaged over 20 trials for each parameter configuration.
2. In terms of ℓ reconstruction error, Algorithm 1 again produced higher quality solu-
2
tions than all benchmark methods in 3 out of 8 of the tested parameter configurations
where k ≤ 15 (see Table 11). Fast-Impute produced solutions achieving the lowest
error in the other 5 parameter configurations.
3. The runtime of Algorithm 1 is competitive with that of the other methods. Table 12
illustrates that ScaledGD was the fastest performing method, however its solutions
were of the lowest quality. The runtime of Algorithm 1 is most competitive with
Soft-Impute and Iterative-SVD for small values of k. Though Fast-Impute is the best
performing method in terms of objective in 3 out of 8 configurations and the best in
terms of ℓ error in 5 out of 8 configurations, it takes on average 3 times as long as
2
Algorithm 1 to execute.
4. Figure 8 illustrates that the computation of the solution for (12) is the computational
bottleneck in the execution of Algorithm 1 in this set of experiments, followed next
by the computation of the solution to (14) and (18).
32
 Q R L W X O R 6  G H Q U X W H 5  I R  N Q D 5
 H X O D 9  H Y L W F H M E 2
 U R U U (  Q R L W F X U W V Q R F H 5   /
  V P   H P L 7  Q R L W X F H [ (Mixed-Projection ADMM
 $ O J R U L W K P    8 S G D W H  ( [ H F X W L R Q  7 L P H  Y V  .
 8
 9
    
 3
 =
    
   
   
   
   
 
                      
 .
Figure 8: Cumulative time spent solving each subproblem of Algorithm 1 versus k with
n = 1000,m = 100 and d = 150. Averaged over 20 trials for each parameter
configuration.
6.6 Summary of Findings
We now summarize our findings from our numerical experiments. In Sections 6.2-6.5, we
see that across all experiments using synthetic data and target rank k ≤ 15, Algorithm 1
produces solutions that achieve on average 79% lower objective value and 90.1% lower ℓ
2
reconstruction error than the solutions returned by the experiment-wise best performing
benchmark method. In the regime where k > 15, we see in Section 6.5 that Fast-Impute
outperforms Algorithm 1. We see that the execution time of Algorithm 1 is competitive
with and often notably faster than the benchmark methods on synthetic data. Importantly,
intheregimek > 15, althoughFast-ImputereturnshigherqualitysolutionsthanAlgorithm
1, the former has an execution time that is on average 3 times as long as our method. Our
computational results are consistent with the complexity analysis performed in Section 5
for Problems (12), (14), (16) and (18). We observe that solution time for (16) becomes the
bottleneck as the target rank k scales, otherwise the solution time for (12) is the bottleneck.
33
  V P   H P L 7  Q R L W X F H [ (Bertsimas and Johnson
7. Conclusion
In this paper, we introduced Problem (1) which seeks to reconstruct a partially observed
matrix that is predictive of fully observed side information. We illustrate that (1) has
a natural interpretation as a robust optimization problem and can be reformulated as a
mixed-projection optimization problem. We derive a semidefinite cone relaxation (9) to (1)
and we present Algorithm 1, a mixed-projection alternating direction method of multipliers
algorithm that obtains scalable, high quality solutions to (1). We rigorously benchmark the
performance of Algorithm 1 on synthetic data against benchmark methods Fast-Impute,
Soft-Impute,Iterative-SVDandScaledGD.Wefindthatacrossallexperimentswithk ≤ 15,
Algorithm 1 outputs solutions that achieve on average 79% lower objective value in (1) and
90.1%lowerℓ reconstructionerrorthanthesolutionsreturnedbytheexperiment-wisebest
2
performing benchmark method. For the 5 experiments with k > 15, Fast-Impute returns
superior quality solutions than Algorithm 1, however the former takes on average 3 times as
long as Algorithm 1 to execute. The runtime of Algorithm 1 is competitive with and often
superior to that of the benchmark methods. Algorithm 1 is able to solve problems with
n = 10000 rows and m = 10000 columns in less than a minute. Future work could expand
the mixed-projection ADMM framework introduced in this work to incorporate positive
semidefinite constraints and general linear constraints. Additionally, future work could
empirically investigate the strength of the semidefinite relaxation (9) and could explore
how to leverage this lower bound to certify globally optimal solutions.
34Mixed-Projection ADMM
References
Francis R Bach and Michael I Jordan. Predictive low-rank decomposition for kernel meth-
ods. In Proceedings of the 22nd international conference on Machine learning, pages
33–40, 2005.
Robert M Bell and Yehuda Koren. Lessons from the netflix prize challenge. Acm Sigkdd
Explorations Newsletter, 9(2):75–79, 2007.
Dimitris Bertsimas and Martin S Copenhaver. Characterization of the equivalence of ro-
bustification and regularization in linear and matrix regression. European Journal of
Operational Research, 270(3):931–942, 2018.
Dimitris Bertsimas and Dick den Hertog. Robust and adaptive optimization. Dynamic Ideas
LLC, 2020.
Dimitris Bertsimas and Nicholas A. G. Johnson. Compressed sensing: A discrete op-
timization approach. Machine Learning, 2024. URL https://doi.org/10.1007/
s10994-024-06577-0.
Dimitris Bertsimas and Michael Lingzhi Li. Fast exact matrix completion: A unified op-
timization framework for matrix completion. Journal of Machine Learning Research, 21
(231):1–43, 2020.
Dimitris Bertsimas, Jean Pauphilet, and Bart Van Parys. Rejoinder: Sparse regression:
scalable algorithms and empirical performance. 2020.
Dimitris Bertsimas, Ryan Cory-Wright, and Jean Pauphilet. A unified approach to mixed-
integer optimization problems with logical constraints. SIAM Journal on Optimization,
31(3):2340–2367, 2021.
Dimitris Bertsimas, Ryan Cory-Wright, and Jean Pauphilet. Mixed-projection conic opti-
mization: A new paradigm for modeling rank constraints. Operations Research, 70(6):
3321–3344, 2022.
Dimitris Bertsimas, Ryan Cory-Wright, and Nicholas A. G. Johnson. Sparse plus low rank
matrix decomposition: A discrete optimization approach. Journal of Machine Learning
Research, 24(267):1–51, 2023a. URL http://jmlr.org/papers/v24/21-1130.html.
Dimitris Bertsimas, Ryan Cory-Wright, Sean Lo, and Jean Pauphilet. Optimal low-rank
matrix completion: Semidefinite relaxations and eigenvector disjunctions. arXiv preprint
arXiv:2305.12292, 2023b.
DimitrisBertsimas, RyanCory-Wright, andJeanPauphilet. Anewperspectiveonlow-rank
optimization. Mathematical Programming, articles in advance, pages 1–46, 2023c.
Daniel Billsus, Michael J Pazzani, et al. Learning collaborative information filters. In Icml,
volume 98, pages 46–54, 1998.
Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. Journal of Machine
Learning Research, 2:499–526, 2002.
35Bertsimas and Johnson
Stephen Boyd, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan. Linear
matrix inequalities in system and control theory. SIAM, 1994.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cam-
bridge university press, USA, 2004.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed
optimization and statistical learning via the alternating direction method of multipliers.
Foundations and Trends® in Machine learning, 3(1):1–122, 2011.
Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization.
Communications of the ACM, 55(6):111–119, 2012.
Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the
IEEE, 98(6):925–936, 2010.
Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal
matrix completion. IEEE transactions on information theory, 56(5):2053–2080, 2010.
Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient
descent: Generalstatisticalandalgorithmicguarantees. arXivpreprintarXiv:1509.03025,
2015.
Kai-YangChiang, Cho-JuiHsieh, andInderjitSDhillon. Matrixcompletionwithnoisyside
information. Advances in neural information processing systems, 28, 2015.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from in-
complete data via the em algorithm. Journal of the royal statistical society: series B
(methodological), 39(1):1–22, 1977.
DavidLDonoho,IainMJohnstone,G´erardKerkyacharian,andDominiquePicard. Wavelet
shrinkage: asymptopia? Journal of the Royal Statistical Society: Series B (Methodologi-
cal), 57(2):301–337, 1995.
Maryam Fazel. Matrix rank minimization with applications. PhD thesis, PhD thesis, Stan-
ford University, 2002.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online
stochastic gradient for tensor decomposition. In Conference on learning theory, pages
797–842. PMLR, 2015.
Oktay Gu¨nlu¨k and Jeff Linderoth. Perspective reformulation and applications. In Mixed
Integer Nonlinear Programming, pages 61–89. Springer, 2012.
Ke Guo, Deren Han, David ZW Wang, and Tingting Wu. Convergence of admm for multi-
block nonconvex separable optimization models. Frontiers of Mathematics in China, 12:
1139–1162, 2017.
Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with finite samples.
In Conference on Learning Theory, pages 1007–1034. PMLR, 2015.
36Mixed-Projection ADMM
Hui Ji, Chaoqiang Liu, Zuowei Shen, and Yuhong Xu. Robust video denoising using low
rank matrix completion. In 2010 IEEE computer society conference on computer vision
and pattern recognition, pages 1791–1798. IEEE, 2010.
Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Provable efficient online matrix com-
pletion via non-convex stochastic gradient descent. Advances in Neural Information Pro-
cessing Systems, 29, 2016.
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recom-
mender systems. Computer, 42(8):30–37, 2009.
Qi Liu. Power network system identification and recovery based on the matrix completion.
In Journal of Physics: Conference Series, volume 1237, page 032059. IOP Publishing,
2019.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in non-
convex statistical estimation: Gradient descent converges linearly for phase retrieval and
matrix completion. In International Conference on Machine Learning, pages 3345–3354.
PMLR, 2018.
RahulMazumder, TrevorHastie, andRobertTibshirani. Spectralregularizationalgorithms
for learning large incomplete matrices. Journal of Machine Learning Research, 11(80):
2287–2322, 2010. URL http://jmlr.org/papers/v11/mazumder10a.html.
Yurii Nesterov. A method for solving the convex programming problem with convergence
rate o (1/k2). In Dokl akad nauk Sssr, volume 269, page 543, 1983.
Luong Trung Nguyen, Junhan Kim, and Byonghyo Shim. Low-rank matrix completion: A
contemporary survey. IEEE Access, 7:94215–94237, 2019.
Art B. Owen and Patrick O. Perry. Bi-cross-validation of the SVD and the nonnegative
matrix factorization. The Annals of Applied Statistics, 3(2):564 – 594, 2009.
Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical
University of Denmark, 7(15):510, 2008.
Andy Ramlatchan, Mengyun Yang, Quan Liu, Min Li, Jianxin Wang, and Yaohang Li. A
survey of matrix completion methods for recommendation systems. Big Data Mining and
Analytics, 1(4):308–323, 2018.
Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David
Bestor, Bill Bergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones,
Anna Klein, Lauren Milechin, Julia Mullen, Andrew Prout, Antonio Rosa, Charles Yee,
andPeterMichaleas.Interactivesupercomputingon40,000coresformachinelearningand
data analysis. In 2018 IEEE High Performance extreme Computing Conference (HPEC),
pages 1–6. IEEE, 2018.
Alex Rubinsteyn and Sergey Feldman. fancyimpute: An imputation library for python,
2016. URL https://github.com/iskandr/fancyimpute.
37Bertsimas and Johnson
Badrul Sarwar, George Karypis, Joseph Konstan, and John T Riedl. Application of dimen-
sionality reduction in recommender system-a case study. 2000.
Anureet Saxena, Pierre Bonami, and Jon Lee. Convex relaxations of non-convex mixed
integer quadratically constrained programs: extended formulations. Mathematical pro-
gramming, 124(1):383–411, 2010.
RuoyuSunandZhi-QuanLuo. Guaranteedmatrixcompletionvianon-convexfactorization.
IEEE Transactions on Information Theory, 62(11):6535–6579, 2016.
Tian Tong, Cong Ma, and Yuejie Chi. Accelerating ill-conditioned low-rank matrix estima-
tion via scaled gradient descent. Journal of Machine Learning Research, 22(150):1–63,
2021.
Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie, Robert
Tibshirani, David Botstein, and Russ B Altman. Missing value estimation methods for
dna microarrays. Bioinformatics, 17(6):520–525, 2001.
Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank?
SIAM Journal on Mathematics of Data Science, 1(1):144–160, 2019.
Junxiang Wang and Liang Zhao. Nonconvex generalization of alternating direction method
of multipliers for nonlinear equality constrained problems. Results in Control and Opti-
mization, 2:100009, 2021.
Yu Wang, Wotao Yin, and Jinshan Zeng. Global convergence of admm in nonconvex nons-
mooth optimization. Journal of Scientific Computing, 78:29–63, 2019.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of
support vector machines. Journal of Machine Learning Research, 10(7), 2009.
Miao Xu, Rong Jin, and Zhi-Hua Zhou. Speedup matrix completion with side information:
Application to multi-label learning. Advances in neural information processing systems,
26, 2013.
Zheng Xu, Soham De, Mario Figueiredo, Christoph Studer, and Tom Goldstein. An empir-
ical study of admm for nonconvex problems. arXiv preprint arXiv:1612.03349, 2016.
Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix com-
pletion using burer-monteiro factorization and gradient descent. arXiv preprint
arXiv:1605.07051, 2016.
38Mixed-Projection ADMM
Appendix A. Supplemental Computational Results
Table 1: Comparison of the objective value of ScaledGD, Algorithm 1, Fast-Impute, Soft-
Impute and SVD versus n with m = 100,k = 5 and d = 150. Averaged over 20
trials for each parameter configuration.
Objective
N ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
100 249262.99 655.46 5530.48 19893.08 28677.73
200 306738.68 1280.82 9756.14 24251.93 44054.89
400 417643.27 2483.49 14321.46 28932.85 61112.62
800 421032.49 4813.05 31520.96 41179.38 93119.34
1000 522586.08 6010.34 33557.75 47701.68 107851.28
2000 563033.20 11975.48 83669.84 76566.52 167458.68
5000 1226489.68 30060.61 273747.04 172093.66 364065.64
10000 1973665.62 60082.27 521189.88 319915.98 642759.84
Table 2: Comparison of the reconstruction error of ScaledGD, Algorithm 1, Fast-Impute,
Soft-Impute and SVD versus n with m = 100,k = 5 and d = 150. Averaged over
20 trials for each parameter configuration.
ℓ Reconstruction Error
2
N ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
100 100.22460 0.01520 0.07540 0.21330 0.30090
200 58.37210 0.00695 0.12800 0.11770 0.21050
400 33.92500 0.00412 0.08980 0.07390 0.15980
800 14.97890 0.00328 0.23990 0.05160 0.13010
1000 12.66500 0.00312 0.18600 0.04950 0.12620
2000 5.54420 0.00304 0.07990 0.04410 0.11670
5000 2.47260 0.00282 0.15040 0.03720 0.10550
10000 1.32070 0.00267 0.10920 0.03510 0.10240
39Bertsimas and Johnson
Table 3: Comparison of the execution time of ScaledGD, Algorithm 1, Fast-Impute, Soft-
Impute and SVD versus n with m = 100,k = 5 and d = 150. Averaged over 20
trials for each parameter configuration.
Execution Time (ms)
N ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
100 10.84 53.47 99.95 141.42 115.26
200 41.11 73.84 121.05 187.26 163.11
400 54.95 113.89 152.16 262.95 246.42
800 68.00 184.11 195.05 389.63 334.16
1000 44.11 241.16 211.63 442.05 366.53
2000 124.47 340.84 311.32 759.58 575.79
5000 813.53 770.11 484.58 1795.58 1298.63
10000 18828.21 1730.00 863.84 3871.53 2713.26
Table 4: Comparison of the objective value of ScaledGD, Algorithm 1, Fast-Impute, Soft-
Impute and SVD versus m with n = 1000,k = 5 and d = 150. Averaged over 20
trials for each parameter configuration.
Objective
M ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
100 530097.31 6014.93 44337.08 47334.20 103403.04
200 2483913.82 6131.52 12159.08 29560.77 114448.43
400 14226534.31 6361.90 13875.31 21942.31 90652.40
800 99356634.18 6800.63 22152.43 37924.99 87895.33
1000 105451997.06 7126.60 24435.40 46327.69 128499.51
2000 591164404.77 13964.62 44333.51 95044.30 815807.16
5000 4002087935.12 41679.23 96985.27 308044.93 11294104.83
10000 9826251365.01 117558.76 197362.37 968255.00 60913874.17
40Mixed-Projection ADMM
Table 5: Comparison of the reconstruction error of ScaledGD, Algorithm 1, Fast-Impute,
Soft-Impute and SVD versus m with n = 1000,k = 5 and d = 150. Averaged over
20 trials for each parameter configuration.
ℓ Reconstruction Error
2
M ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
100 13.68740 0.00322 0.14530 0.04960 0.12560
200 40.58900 0.00154 0.00590 0.01260 0.06640
400 127.71450 0.00075 0.00340 0.00340 0.02240
800 508.24550 0.00036 0.00340 0.00310 0.00460
1000 443.26620 0.00029 0.00310 0.00300 0.00350
2000 1292.61610 0.00012 0.00330 0.00290 0.00300
5000 3658.18810 0.00004 0.00310 0.00270 0.00540
10000 4559.27360 0.00002 0.00320 0.00270 0.00650
Table 6: Comparison of the execution time of ScaledGD, Algorithm 1, Fast-Impute, Soft-
Impute and SVD versus m with n = 1000,k = 5 and d = 150. Averaged over 20
trials for each parameter configuration.
Execution Time (ms)
M ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
100 44.16 222.21 193.16 444.89 365.74
200 54.32 237.37 223.32 953.32 760.11
400 80.95 311.32 315.47 1466.32 1511.79
800 121.37 637.53 360.53 2198.21 2564.00
1000 154.89 728.58 434.47 2611.58 3009.21
2000 4652.11 1181.47 5127.37 5308.16 6062.89
5000 28587.11 12645.16 40526.16 32824.79 35015.21
10000 108255.05 39569.37 156399.42 82361.37 86762.84
41Bertsimas and Johnson
Table 7: Comparison of the objective value of ScaledGD, Algorithm 1, Fast-Impute, Soft-
Impute and SVD versus d with n = 1000,m = 100 and k = 5. Averaged over 20
trials for each parameter configuration.
Objective
D ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
10 11691.78 475.17 4222.64 3367.41 7710.45
50 96771.27 2067.20 14565.83 16511.97 37203.79
100 229740.41 4033.46 28967.06 31000.81 68634.31
150 532018.26 6057.23 37244.64 45581.92 106504.32
200 734648.30 7994.51 42289.45 64771.47 141252.05
250 1195065.81 9984.20 71433.91 83752.90 183720.00
500 4165782.61 20094.24 120114.14 158458.64 361740.57
1000 13578263.54 40191.29 230467.93 294273.13 668550.27
Table 8: Comparison of the reconstruction error of ScaledGD, Algorithm 1, Fast-Impute,
Soft-Impute and SVD versus d with n = 1000,m = 100 and k = 5. Averaged over
20 trials for each parameter configuration.
ℓ Reconstruction Error
2
D ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
10 0.60780 0.00690 0.15210 0.04840 0.12530
50 1.41600 0.00471 0.09780 0.05020 0.12730
100 5.03590 0.00382 0.24280 0.05130 0.12790
150 14.00330 0.00326 0.19790 0.05030 0.12650
200 22.26870 0.00276 0.08640 0.04840 0.12510
250 39.41630 0.00245 0.19210 0.04830 0.12580
500 177.68800 0.00165 0.15700 0.04860 0.12640
1000 679.11770 0.00104 0.09360 0.04990 0.12660
42Mixed-Projection ADMM
Table 9: Comparison of the execution time of ScaledGD, Algorithm 1, Fast-Impute, Soft-
Impute and SVD versus d with n = 1000,m = 100 and k = 5. Averaged over 20
trials for each parameter configuration.
Execution Time (ms)
D ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
10 84.00 199.89 222.68 465.11 386.53
50 80.79 193.05 226.00 459.53 385.00
100 108.63 245.11 226.53 457.84 380.11
150 113.47 246.16 218.47 452.21 380.79
200 117.32 318.26 243.05 455.32 387.05
250 152.79 362.63 229.63 487.21 412.79
500 176.21 365.37 274.26 439.05 358.95
1000 138.74 449.42 255.32 467.95 392.00
Table 10: Comparison of the objective value of ScaledGD, Algorithm 1, Fast-Impute, Soft-
Impute and SVD versus k with n = 1000,m = 100 and d = 150. Averaged over
20 trials for each parameter configuration.
Objective
K ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
5 514330.09 6021.76 41376.50 45858.20 106390.58
10 1892278.99 7393.65 255228.60 318398.98 805396.86
15 5213393.44 14104.62 115383.43 1112396.97 2495972.46
20 10196279.89 328671.00 101812.52 2628073.04 4910386.51
25 16816442.74 1069103.04 116005.02 4526388.13 7541300.54
30 - 34567679.83 10695127.17 6864577.33 10634436.81
35 39536651.09 187701091.79 144464.25 9424715.13 14192827.60
40 - 723504611.03 191276652.97 12529277.05 18290215.22
43Bertsimas and Johnson
Table 11: Comparison of the reconstruction error of ScaledGD, Algorithm 1, Fast-Impute,
Soft-Impute and SVD versus k with n = 1000,m = 100 and d = 150. Averaged
over 20 trials for each parameter configuration.
ℓ Reconstruction Error
2
K ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
5 12.95500 0.00314 0.10940 0.05090 0.12900
10 5.41720 0.00288 0.24270 0.08970 0.22520
15 3.45210 0.00871 0.01940 0.14010 0.30200
20 2.34590 0.04900 0.00903 0.18270 0.34220
25 1.73610 0.11890 0.00678 0.21480 0.36130
30 - 0.20030 0.00562 0.23810 0.37240
35 1.16780 0.17330 0.00488 0.25260 0.37740
40 - 0.22320 0.00452 0.26550 0.38020
Table 12: Comparison of the execution time of ScaledGD, Algorithm 1, Fast-Impute, Soft-
Impute and SVD versus k with n = 1000,m = 100 and d = 150. Averaged over
20 trials for each parameter configuration.
Execution Time (ms)
K ScaledGD Algorithm 1 Fast-Impute Soft-Impute SVD
5 55.79 227.47 205.16 460.11 374.79
10 80.79 298.63 823.21 477.95 361.95
15 107.89 502.21 1817.16 509.68 346.05
20 95.53 713.42 2420.89 533.79 324.79
25 111.53 1110.89 3586.05 569.68 297.58
30 - 1353.95 4435.63 591.21 280.37
35 107.21 1822.16 6212.63 640.05 271.00
40 - 2281.95 8168.68 645.11 263.37
44