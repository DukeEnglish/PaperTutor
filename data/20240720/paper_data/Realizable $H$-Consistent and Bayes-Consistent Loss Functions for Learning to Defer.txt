1–34
Realizable H-Consistent and Bayes-Consistent Loss Functions for
Learning to Defer
AnqiMao AQMAO@CIMS.NYU.EDU
CourantInstituteofMathematicalSciences,NewYork
MehryarMohri MOHRI@GOOGLE.COM
GoogleResearchandCourantInstituteofMathematicalSciences,NewYork
YutaoZhong YUTAO@CIMS.NYU.EDU
CourantInstituteofMathematicalSciences,NewYork
Abstract
We present a comprehensive study of surrogate loss functions for learning to defer. We in-
troduce a broad family of surrogate losses, parameterized by a non-increasing function Ψ, and
establishtheirrealizableH-consistencyundermildconditions. Forcostfunctionsbasedonclassi-
ficationerror,wefurthershowthattheselossesadmitH-consistencyboundswhenthehypothesis
set is symmetric and complete, a property satisfied by common neural network and linear func-
tionhypothesissets. Ourresultsalsoresolveanopenquestionraisedinpreviouswork(Mozannar
etal.,2023)byprovingtherealizableH-consistencyandBayes-consistencyofaspecificsurrogate
loss. Furthermore,weidentifychoicesofΨthatleadtoH-consistentsurrogatelossesforanygen-
eralcostfunction,thusachievingBayes-consistency,realizableH-consistency,andH-consistency
bounds simultaneously. We also investigate the relationship between H-consistency bounds and
realizable H-consistency in learning to defer, highlighting key differences from standard classi-
fication. Finally, we empirically evaluate our proposed surrogate losses and compare them with
existingbaselines.
1. Introduction
Inmanypracticalscenarios,combiningexpertinsightswithestablishedmodelscanyieldsignificant
enhancements. These experts can be human domain specialists or more complex, albeit resource-
intensive,models. Forexample,modernlanguageanddialoguemodelsarepronetoproducinghal-
lucinationsorinaccurateinformation. Thequalityoftheirresponsescanbesignificantlyenhanced
bydelegatinguncertainpredictionstomorespecializedoradvancedpre-trainedmodels. Thisprob-
lemisparticularlycrucialforlargelanguagemodels(LLMs),asnotedin(Weietal.,2022;Bubeck
etal.,2023). Thesameprincipleappliestoothergenerativesystems,likethoseforimagesorvideos,
andtolearningmodelsindiverseapplicationssuchasimageclassification, annotation, andspeech
recognition. Thus,thetaskoflearningtodefer(L2D)withexpertshasbecomeincreasinglycritical
acrossawidearrayofapplications.
Directly optimizing the deferral loss function, which is the target loss in L2D, is computa-
tionally intractable for many choices of the hypothesis set. Therefore, a common approach is to
optimizeasurrogatelossthatfacilitatestheoptimizationofthedeferrallossfunction. Recentwork
in L2D has proposed several surrogate losses (Mozannar and Sontag, 2020; Verma and Nalisnick,
2022;Mozannaretal.,2023;Maoetal.,2024a)andstudiedtheirconsistencyguarantees,including
Bayes-consistency, realizable H-consistency, and H-consistency bounds (see definitions in Sec-
tion 3.2). In particular, Mozannar and Sontag (2020) proposed the first Bayes-consistent surrogate
© A.Mao,M.Mohri&Y.Zhong.
4202
luJ
81
]GL.sc[
1v23731.7042:viXraMAOMOHRIZHONG
lossbygeneralizingthecross-entropylossforL2D.VermaandNalisnick(2022)proposedanalter-
native Bayes-consistent surrogate loss by generalizing the one-versus-all loss for L2D. Mozannar
et al. (2023) showed that these surrogate losses are not realizable H-consistent. They proposed an
alternativesurrogatelossthatisrealizableH-consistent,buttheywereunabletoproveordisprove
whethertheproposedsurrogatelossisBayes-consistent. Allthesurrogatelossesmentionedabove
and their consistency guarantees hold only for cost functions based on classification error. Mao
etal.(2024a)generalizedthesurrogatelossin(MozannarandSontag,2020)toincorporategeneral
cost functions and any multi-class surrogate losses. They provided H-consistency bounds for the
novelfamilyofsurrogatelosses,offeringastrongerguaranteethanBayes-consistency.
However, none of these surrogate losses satisfies all these guarantees simultaneously. In par-
ticular, arecentAISTATSnotableawardpaperbyMozannaretal.(2023)leftopentheproblemof
finding surrogate losses that are both Bayes-consistent and realizable H-consistent when the cost
functionfortheexpertisitsclassificationerror. Theproblembecomesevenmorechallengingwhen
consideringmoregeneralandrealisticcostfunctions.
We present a comprehensive analysis of surrogate loss functions for L2D. Our contributions
address the limitations of previous approaches and provide a unified framework for designing sur-
rogate losses with strong theoretical guarantees. In Section 4, we first introduce a broad family of
surrogate losses for L2D, derived from first principles (Section 4.1). This family is parameterized
by a non-increasing function Ψ, which provides some flexibility in tailoring the loss function to
specificrequirements. WeestablishthatundermildconditionsonΨ,thesesurrogatelossesachieve
realizableH-consistency,akeyguaranteeformanyapplications(Section4.2).
Next, for cost functions based on classification error, we further establish that our surrogate
loss functions admit H-consistency bounds when the hypothesis set is symmetric and complete
(Section 4.3). This result holds for commonly used neural network and linear function hypoth-
esis sets, further strengthening the applicability of our results. Additionally, our results resolve
an open question raised by Mozannar et al. (2023) by proving the realizable H-consistency and
Bayes-consistencyoftheirproposedsurrogateloss, whichtheauthorshadleftasanopenquestion
(Section4.4).
In Section 4.3, we further identify specific choices of Ψ, such as the one corresponding to the
mean absolute error loss, that lead to H-consistent surrogate losses for any general cost function.
TheselossfunctionsareadaptedtogeneralcostfunctionsandbenefitfromBayes-consistency(Sec-
tion4.4),realizableH-consistency,andH-consistencyboundssimultaneously.
In Section 5, we also study the relationship between H-consistency bounds and realizable H-
consistency in the context of L2D, highlighting key distinctions from the standard classification
setting. Finally,wefurtherreporttheresultsofexperimentswithournewsurrogatelossesandtheir
comparisonwiththebaselinesindifferentsettings(Section6).
WediscusstherelatedworkinSection2andthenbeginwiththepreliminariesinSection3.
2. Relatedwork
Theapproachofsingle-stagelearningtodefer,whereapredictorandadeferralfunctionaretrained
together, was pioneered by Cortes, DeSalvo, and Mohri (2016a,b, 2023) and further developed in
subsequentstudiesonabstention,wherethecostisconstant(Charoenphakdeeetal.,2021;Caoetal.,
2022; Li et al., 2023; Cheng et al., 2023; Mao et al., 2024c,b; Mohri et al., 2024) and on deferral,
wherethecostcanvarydependingontheinstanceandthelabel(MozannarandSontag,2020;Verma
2REALIZABLELEARNINGTODEFER
andNalisnick,2022;Mozannaretal.,2023;Vermaetal.,2023;Caoetal.,2023;Maoetal.,2023a,
2024a). In this approach, the deferral function determines whether to defer to an expert for each
input. This approach has been shown to be superior to confidence-based approaches, where the
decisiontoabstainordeferisbasedsolelyonthemagnitudeofthepredictor’svalue(Chow,1957,
1970;BartlettandWegkamp,2008;YuanandWegkamp,2010,2011;Ramaswamyetal.,2018;Ni
et al., 2019; Jitkrittum et al., 2023); and to selective classification approaches, where the selection
rateisfixedandacostfunctionmodeledbyanexpertcannotbetakenintoaccount(El-Yanivetal.,
2010;El-YanivandWiener,2012;WienerandEl-Yaniv,2011,2012,2015;GeifmanandEl-Yaniv,
2017, 2019; Acar et al., 2020; Gangrade et al., 2021; Zaoui et al., 2020; Jiang et al., 2020; Shah
etal.,2022).
Madras et al. (2018) initiated the learning to defer (L2D) problem scenario, which integrates
human expert decisions into the cost function. This approach has been further explored in subse-
quent studies (Raghu et al., 2019; Wilder et al., 2021; Pradier et al., 2021). Mozannar and Sontag
(2020) introduced the first Bayes-consistent surrogate loss for L2D, which was further refined in
(Raman and Yee, 2021; Liu et al., 2022). Verma and Nalisnick (2022) proposed an alternative
Bayes-consistentsurrogateloss,theone-versus-allloss,whichwaslaterexaminedwithinabroader
family of loss functions (Charusaie et al., 2022). Cao et al. (2023) proposed an asymmetric soft-
maxfunction,whichcaninduceavalidprobabilityestimatorforlearningtodefer. Mozannaretal.
(2023) showed that the surrogate losses in (Mozannar and Sontag, 2020; Verma and Nalisnick,
2022) are not realizable H-consistent. They proposed an alternative surrogate loss that is realiz-
ableH-consistent,buttheywereunabletoproveordisprovewhethertheproposedsurrogatelossis
Bayes-consistent. All the surrogate losses mentioned above and their consistency guarantees hold
only for cost functions based on classification error. Mao et al. (2024a) generalized the surrogate
lossin(MozannarandSontag,2020)toincorporategeneralcostfunctionsandanymulti-classsurro-
gatelosses. TheyprovidedH-consistencyboundsforthenovelfamilyofsurrogatelosses,offering
astrongerguaranteethanBayes-consistency.
Additionalstudieshavefocusedonpost-hocmethods,withOkatietal.(2021)suggestinganal-
ternativeoptimizationtechniquebetweenthepredictorandrejector,Narasimhanetal.(2022)offer-
ingcorrectionsforunderfittingsurrogatelosses(Liuetal.,2024),andCharusaieandSamadi(2024)
providingaunifyingpost-processingframeworkformulti-objectiveL2Dbasedonageneralization
of the Neyman-Pearson Lemma (Neyman and Pearson, 1933). The L2D framework or variations
thereof have found applications in diverse scenarios, spanning regression, reinforcement learning,
and human-in-the-loop systems, among others (De et al., 2020, 2021; Straitouri et al., 2021; Zhao
etal.,2021;Joshietal.,2021;Gaoetal.,2021;Mozannaretal.,2022;Hemmeretal.,2023;Chen
etal.,2024;Palombaetal.,2024). Morerecently,theproblemoflearningtodeferwithmultipleex-
pertshasbeenanalyzedinseveralpublications(Hemmeretal.,2022;Keswanietal.,2021;Kerrigan
etal.,2021;Straitourietal.,2022;BenzandRodriguez,2022;Vermaetal.,2023;Maoetal.,2023a,
2024a,e;Tailoretal.,2024). Meanwhile, Maoetal.(2023a)alsoproposedatwo-stagelearningto
defer framework. They introduced two-stage surrogate losses that are both Bayes-consistent and
realizable H-consistent with constant costs. However, realizable H-consistency does not hold for
costfunctionsbasedonclassificationerror. Aswith(MozannarandSontag,2020;VermaandNal-
isnick,2022;Mozannaretal.,2023),ourworkfocusesonthesingle-stageandsingle-expertsetting,
andweplantoexploreasimilarapproachinamulti-expert/two-stagesettinginthefuture.
3MAOMOHRIZHONG
3. Preliminaries
Westartwiththedefinitionsandnotationsusedinthelearning-to-deferscenarioconsideredinthis
paper. We will then introduce consistency guarantees, including Bayes consistency, Realizable H-
consistency,andH-consistencybounds. Finally,wewillreviewexistingconsistentsurrogatelosses
forL2D.
3.1. Learningtodefer: problemsetup
Let X be an input space and Y = [n] ∶= {1,...,n} be the label space in the standard multi-class
classification setting. We study the learning to defer (L2D) scenario, where a learner can either
predictalabelfromYordefertoanexpert.
To model this, we introduce an augmented label space Y = {1,...,n,n+1}, where the label
n+1 corresponds to deferral. An expert is a fixed predictor g∶X ×Y → R. The goal of L2D is to
select a predictor h out of a hypothesis set H of functions mapping from X ×Y to R with small
expected deferral loss. Let h(x) denote the prediction of h on input x ∈ X, defined as h(x) =
argmax y∈Yh(x,y),thatisthelabelintheaugmentedlabelspaceYwiththehighestscore,withan
arbitrary but fixed deterministic strategy for breaking ties. Then, the deferral loss function L is
def
definedasfollows:
∀(x,y)∈X ×Y, L def(h,x,y)=1 h(x)≠y1 h(x)∈[n]+c(x,y)1 h(x)=n+1,
where c(x,y) is the the cost of deferring on input x with true label y. If the deferral option is
selected, that is h(x) = n+1, the deferral cost c(x,y) is incurred. Otherwise, the prediction of
h is within the standard label space, h(x) ∈ [n], and the loss incurred coincides with the standard
zero-oneclassificationloss,1 .
h(x)≠y
Thechoiceofthecostfunctioncisflexible. Forexample,thecostcanbedefinedastheexpert’s
classificationerror: c(x,y)=1 g(x)≠y,asinpreviouswork(MozannarandSontag,2020;Vermaand
Nalisnick,2022;Mozannaretal.,2023). Here, g(x) = argmax y∈[n]g(x,y)isthepredictionmade
by the expert g. More generally, it can incorporate the inference cost for the expert (Mao et al.,
2024a): c(x,y) = α1 g(x)≠y +β,withα,β > 0. Weassume,withoutlossofgenerality,thatthecost
isboundedby1: 0≤c(x,y)≤1,whichcanbeachievedthroughnormalizationinpractice.
3.2. Consistencyguarantees
Directly optimizing the deferral loss function, which is the target loss in L2D, is generally com-
putationally intractable for for complex hypothesis sets H. Therefore, a common approach is to
optimize a surrogate loss that facilitates the optimization of the deferral loss function. A natural
learning guarantee for such surrogate losses is Bayes-consistency (Zhang, 2004a; Bartlett et al.,
2006;Zhang,2004b;TewariandBartlett,2007;Steinwart,2007):
Definition1(Bayes-consistency) A surrogate loss L is Bayes-consistent with respect to L , if
def
minimizingthesurrogatelossoverthefamilyofallmeasurablefunctionsleadstotheminimization
ofthedeferralloss:
nl →im +∞E L(h n)−E∗ L(H all)=0 (cid:212)⇒ nl →im +∞E L def(h n)−E∗ L def(H all)=0.
4REALIZABLELEARNINGTODEFER
Here,givenadistributionDoverX ×YandalossfunctionL∶H ×X ×Y →R,wedenotebyE L(h)
the generalization error of a hypothesis h ∈ H, E L(h) = E (x,y)∼D[L(h,x,y)], and by E∗ L(H )
the best-in-class generalization error, E∗ L(H ) = inf h∈HE L(h). Bayes-consistency assumes that
theoptimizationoccursoverthefamilyofallmeasurablefunctions,H . However,inpractice,the
all
hypothesissetofinterestistypicallyarestrictedone,suchasafamilyofneuralnetworks. Therefore,
ahypothesis-dependentlearningguarantee,suchasH-consistencybounds(Awasthietal.,2022a,b)
(see also (Awasthi et al., 2023a,b; Mao et al., 2023b,e,f; Zheng et al., 2023; Mao et al., 2023c,d,
2024g,f,d;Cortesetal.,2024))andrealizableH-consistency(LongandServedio,2013;Zhangand
Agarwal, 2020), is more informative and relevant. Realizable H-consistency, defined as follows,
requires that a minimizer of the surrogate loss over the given hypothesis set H also minimizes the
targetloss,providedthattheunderlyingdistributionisrealizable.
Definition2(RealizableH-consistency) A surrogate loss L is realizable H-consistent with re-
spect to L def, if for any distribution over which there exists a predictor h∗ ∈ H achieving zero
deferralloss,E
L
(h∗)=0,minimizingthesurrogatelossalsoleadstoazero-errorsolution:
def
hˆ ∈argminE L(h) (cid:212)⇒ E L (hˆ )=0.
def
h∈H
Note that realizable H-consistency does not imply Bayes-consistency, even if we set H = H all in
Definition 2, since Bayes-consistency requires that the relationship holds for all distributions, not
just realizable ones. H-consistency bounds, on the other hand, always imply Bayes-consistency.
Given a hypothesis set H, a surrogate loss L admits an H-consistency bound, if for some non-
decreasing concave function Γ∶R → R with Γ(0) = 0, a bound of the following form holds for
+ +
anyhypothesish∈Handanydistribution:
E
L
def(h)−E∗
L
def(H )+M
L
def(H )≤Γ(E L(h)−E∗ L(H )+M L(H )), (1)
whereM L(H )istheminimizabilitygap,definedasthedifferencebetweenthebest-in-classgeneral-
izationerrorandtheexpectedpointwiseinfimumloss: M L(H )=E∗ L(H )−E x[inf h∈HE y∣x[L(h,x,y)]].
The minimizability gap can be upper-bounded by the approximation error and vanishes when H =
H (Awasthietal.,2022a,b). Thus,anH-consistencyboundimpliesBayes-consistency. Therela-
all
tionshipbetweenthetwohypothesis-dependentlearningguarantees—realizableH-consistencyand
H-consistencybounds—dependsonthetargetlossadoptedinthespecificlearningscenario. InSec-
tion5,wewilldemonstratethatinthestandardmulti-classclassificationsetting,anH-consistency
boundisastrongernotionthanrealizableH-consistency. However,inL2D,theseguaranteesdonot
implyoneanother.
3.3. Existingsurrogatelosses
Here, we will review several consistent surrogate losses used in L2D. For convenience, we use
̃c(x,y) = 1 g(x)≠y to denote the cost when it specifically represents the expert’s classification error,
andusec(x,y)whenitrepresentsageneralcostfunction.
MozannarandSontag(2020)proposedthefirstBayes-consistentsurrogatelossbygeneralizing
thecross-entropylossforL2D,withcostfunctionsbasedonclassificationerror,whichisdefinedas
⎛
eh(x,y)
⎞ ⎛
eh(x,n+1)
⎞
L CE(h,x,y)=−log ⎝∑y′∈Yeh(x,y′)⎠−(1−̃c(x,y))log ⎝∑y′∈Yeh(x,y′)⎠.
5MAOMOHRIZHONG
Verma and Nalisnick (2022) proposed an alternative one-vs-all surrogates loss with cost functions
basedonexpert’sclassificationerror,thatisBayes-consistentaswell:
L OvA(h,x,y)=Φ(h(x,y))+∑ Φ(−h(x,y′ ))+(1−̃c(x,y))[Φ(h(x,n+1))−Φ(−h(x,n+1))],
y′ ∈Y
y′ ≠y
whereΦisastrictlyproperbinarycompositeloss(ReidandWilliamson,2010),suchasthelogistic
loss t ↦ log(1+e−t ). L CE and L OvA are not realizable H-consistent. Instead, Mozannar et al.
(2023)proposedthefollowinglossfunctionthatisrealizableH-consistentwhenHisclosedunder
scaling:
⎛eh(x,y)+(1−̃c(x,y))eh(x,n+1)⎞
L RS(h,x,y)=−2log
⎝
∑y′∈Yeh(x,y′)
⎠.
However,theywereunabletoproveordisprovewhetherthesurrogatelossL isBayes-consistent.
RS
All the surrogate losses mentioned above and their consistency guarantees hold only for cost
functions based on the classification error: ̃c(x,y) = 1 g(x)≠y. Mao et al. (2024a) generalized the
surrogatelossL toincorporategeneralcostfunctionsandanymulti-classsurrogatelosses:
CE
L general(h,x,y)=ℓ(h,x,y)+(1−c(x,y))ℓ(h,x,n+1).
Here, ℓ is a Bayes-consistent surrogate loss for the multi-class zero-one loss over the augmented
labelsetY. Inparticular,ℓcanbechosenasacomp-sumloss(Maoetal.,2023f),forexample,the
generalizedcrossentropyloss(seeSection4.1). AsshownbyMaoetal.(2024a), L benefits
general
fromH-consistencybounds,whichimpliesitsBayes-consistency.
4. Novelsurrogatelosses
In this section, we introduce a new family of surrogate losses for L2D that benefit from Bayes-
consistency,realizableH-consistencyandH-consistencybounds,startingfromfirstprinciples.
4.1. Derivationfromfirstprinciples
Observe that for any (x,y) ∈ X ×Y, we have 1 h(x)=n+1 = 1 h(x)≠y1 h(x)=n+1, since h(x) = n+1
impliesh(x) ≠ y. Thus, usingadditionally1 h(x)∈[n] = 1 h(x)≠n+1, thedeferrallosscanberewritten
asfollowsforall(x,y)∈X ×Y:
L def(h,x,y)=1 h(x)≠y1 h(x)∈[n]+c(x,y)1
h(x)=n+1
=1 h(x)≠y1 h(x)≠n+1+c(x,y)1 h(x)≠y1
h(x)=n+1
=1 h(x)≠y1 h(x)≠n+1+c(x,y)1 h(x)≠y(1−1 h(x)≠n+1)
=c(x,y)1 h(x)≠y+(1−c(x,y))1 h(x)≠y∧h(x)≠n+1. (2)
Next,wewillderivethenewsurrogatelossesforL2Dbyreplacingtheindicatorfunctionsin(2)
withsmoothlossfunctions. Thefirstindicatorfunction1 isjustthemulti-classzero-oneloss.
h(x)≠y
Thus,anaturalchoiceistoreplaceitwithasurrogatelossinstandardmulti-classclassification. We
6REALIZABLELEARNINGTODEFER
willspecificallyconsiderthefamilyofcomp-sumlosses(Maoetal.,2023f),definedasfollowsfor
any(h,x,y)∈H ×X ×Y:
⎛
eh(x,y)
⎞
ℓ comp(h,x,y)=Ψ ⎝∑y′∈Yeh(x,y′)⎠,
whereΨ∶[0,1]→R ∪{+∞}isanon-increasingfunction. Forexample,bytakingΨ(t)=−log(t),
+
1 (1−tq )withq ∈ (0,1), 1−t, we obtain the logistic loss (Verhulst, 1838, 1845; Berkson, 1944,
q
1951),thegeneralizedcrossentropyloss(ZhangandSabuncu,2018),andthemeanabsoluteerror
loss(Ghoshetal.,2017),respectively:
⎡
⎢
eh(x,y) ⎤
⎥
Logisticloss: ℓ log(h,x,y)=−log⎢
⎢
⎢
⎣∑y′∈Yeh(x,y′)⎥
⎥
⎥
⎦
q
1⎡
⎢
⎡
⎢
eh(x,y) ⎤
⎥
⎤
⎥
Generalizedcrossentropyloss: ℓ gce(h,x,y)= q⎢
⎢
⎢
⎣1−⎢
⎢
⎢
⎣∑y′∈Yeh(x,y′)⎥
⎥
⎥
⎦
⎥
⎥
⎥
⎦
eh(x,y)
Meanabsoluteerrorloss: ℓ mae(h,x,y)=1− ∑y′∈Yeh(x,y′).
For any (h,x,y) ∈ H ×X ×Y, the confidence margin ρ h(x,y) is defined by ρ h(x,y) = h(x,y)−
max y′∈Y,y′≠yh(x,y′). Thus, the second indicator function 1
h(x)≠y∧h(x)≠n+1
can be expressed as
followsintermsoftheconfidencemargin:
1
h(x)≠y∧h(x)≠n+1
=1
(h(x,y)≤max y′∈Y,y′≠yh(x,y′))∧(h(x,n+1)≤max y′∈Y,y′≠n+1h(x,y′))
=1
(ρ h(x,y)≤0)∧(ρ h(x,n+1)≤0)
=1
max{ρ h(x,y),ρ
h(x,n+1)}≤0.
Note that the first indicator function can also be written in terms of margin: 1 h(x)≠y = 1 ρ h(x,y)≤0.
Unlikethefirstindicatorfunction,whichpressesh(x,y)tobethelargestscoreamongY,thatisthe
marginρ h(x,y)tobepositive,thesecondindicatorfunctiononlyenforcesh(x,y)orh(x,n+1)to
be the largest score among Y, that is the maximum of two margins, max{ρ h(x,y),ρ h(x,n+1)},
to be positive. This condition can be further strengthened by requiring the sum of two margins,
ρ h(x,y)+ρ h(x,n+1),tobepositive. Inviewofthisobservation,weadoptthefollowingmodified
comp-sumsurrogatelossforthesecondindicatorfunction:
⎛eh(x,y)+eh(x,n+1)⎞
ℓ̃ comp(h,x,y)=Ψ
⎝
∑y′∈Yeh(x,y′)
⎠,
whereΨ∶[0,1]→R +∪{+∞}isanon-increasingfunction. Inotherwords,ℓ̃
comp
replacestheterm
eh(x,y) inthesoftmaxfunctioninℓ
comp
withthesumeh(x,y)+eh(x,n+1). Theeffectistoencourage
thesumofthetwomargins,ρ h(x,y)+ρ h(x,n+1),tobepositive,ratherthanjustthesinglemargin
ρ h(x,y). Followingthisprinciple,wederivethefollowingexpressionforanewfamilyofsurrogate
losses,L ,dubbedrealizableL2D:
RL2D
L RL2D(h,x,y)=c(x,y)ℓ comp(h,x,y)+(1−c(x,y))ℓ̃ comp(h,x,y). (3)
7MAOMOHRIZHONG
Table1: AnewfamilyofsurrogatelossesL forL2D.
RL2D
Ψ(t) L
RL2D
−log(t) −c(x,y)log[ ∑y′e ∈Yh( ex h,y (x) ,y′) q]−(1−c(x,y))log[eh ∑(x y, ′y ∈) Y+ ee hh (( xx ,, yn ′+ )1) q]
1 q(1−tq ) c(x q,y) [1−[ ∑y′e ∈Yh( ex h,y (x) ,y′)] ]+ (1−c( qx,y)) [1−[eh ∑(x y, ′y ∈) Y+ ee hh (( xx ,, yn ′+ )1) ] ]
1−t c(x,y)(1− ∑y′e ∈Yh( ex h,y (x) ,y′))+(1−c(x,y))(1− eh ∑(x y, ′y ∈) Y+ ee hh (( xx ,, yn ′+ )1) )
ForthechoicesofΨ(t)=−log(t), 1 (1−tq )withq ∈(0,1)and1−t,weobtainthenewsurrogate
q
losses for L2D in Table 1. In the next sections, we will prove both realizable H-consistency guar-
anteesandH-consistencyboundsforthisfamilyofsurrogatelosses,whichimplytheirexcesserror
boundsandBayes-consistencyaswell.
4.2. RealizableH-consistency
Here,weshowthatL isrealizableH-consistentwithrespecttoL . Wesaythatahypothesis
RL2D def
setHisclosedunderscalingif,h∈H
(cid:212)⇒
αh∈Hforanyα∈R.
Theorem3 Assume that H is closed under scaling. Suppose that Ψ is non-increasing, Ψ(2 ) > 0
3
and lim t→1Ψ(t) = 0. Then, the surrogate loss L RL2D is realizable H-consistent with respect to
L .
def
The proof, detailed in Appendix A, begins by establishing an upper bound on the deferral loss
in terms of the comp-sum loss: L def ≤ L ΨR (L 2 32D ). Letting hˆ be the minimizer of L RL2D and α be any
realnumber,wethenshowthatE L def(hˆ )≤ Ψ(1 2 3)E LRL2D(αh∗). Thegeneralizationerroristhensplit
byconditioningonwhetherh∗(x)isthedeferralclass(n+1)ornot. Finally,wedemonstratethat
each conditional term converges to zero as α tends to +∞, and apply the monotone convergence
theoremtocompletetheproof.
4.3. H-Consistencybounds
Here, we show that L admits an H-consistency bound with respect to L , which implies its
RL2D def
Bayes-consistency as well. We say that a hypothesis set is symmetric if there exists a family F of
functionsf mappingfromXtoRsuchthat
{[h(x,1),...,h(x,n+1)]∶h∈H }={[f 1(x),...,f n+1(x)]∶f 1,...,f
n+1
∈F },
foranyx∈X. WesaythatahypothesissetHiscompleteifforany(x,y)∈X ×Y,thesetofscores
generatedbyitspansacrosstherealnumbers: {h(x,y)∣h∈H }=R. Commonneuralnetworkand
linearfunctionhypothesissetsareallsymmetricandcomplete. Wefirstconsiderthecasewherethe
costisexpert’sclassificationerror.
Theorem4 Assume that H is symmetric and complete and that c(x,y) = 1 g(x)≠y. Then, for all
h∈Handanydistribution,thefollowingH-consistencyboundholds:
E
L
def(h)−E
L
def(H )+M
L
def(H )≤Γ(E LRL2D(h)−E LRL2D(H )+M LRL2D(H )),
8REALIZABLELEARNINGTODEFER
√ √
whereΓ(t) = 2twhenΨ(t) = −log(t)andΓ(t) = 2(n+1)qtwhenΨ(t) = 1 (1−tq )withq ∈
q
(0,1).
Theproof,detailedinAppendixB.3andB.4,establishesstrongconsistencyguaranteesforour
newsurrogatelossL RL2D(Theorem4). Wefirstintroducey
max
=argmax y∈Yp(x,y),thelabelwith
thehighestconditionalprobability. Wethenshowthatfor anyhypothesishandinputx, ify is
max
not the predicted label h , the conditional error ofh is lower bounded by a modified hypothesis
max
h (obtained by swapping the scores of y max and h max). Next, for hypotheses where y max = h max,
welowerboundtheirconditionalregretintermsoftheconditionalregretofthedeferrallossusing
a new hypothesis h . This proof is novel and significantly different from existing approaches for
µ
establishing H-consistency bounds in either the standard or deferral settings (Mao et al., 2023f,
2024a).
The next result further shows that when Ψ(t) = 1−t, our surrogate losses benefit from H-
consistencyboundsforanygeneralcostfunction.
Theorem5 Assume that H is symmetric and complete. Suppose that Ψ(t) = 1−t. Then, for all
h∈Handanydistribution,thefollowingH-consistencyboundshold:
E
L
def(h)−E
L
def(H )+M
L
def(H )≤(n+1)(E LRL2D(h)−E LRL2D(H )+M LRL2D(H )).
The proof is included in Appendix B.2. Theorem 4 provides stronger consistency guarantees
for our new surrogate loss L RL2D with Ψ(t) = 1−t since it holds for any general cost function.
TheproofideaissimilartothatofTheorem4,albeitwithmorecasestoanalyzeduetothegeneral
cost function. This occurs when lower bounding the conditional regret of a hypothesis h, which
satisfies y max = h max, in terms of the conditional regret of the deferral loss by introducing a new
hypothesis h . The additional cases necessitate a more stringent condition for the guarantee, such
µ
thatthefunctionsΨ(t)=−log(t)andΨ(t)= 1 (1−tq )donotapply.
q
4.4. ExcesserrorboundsandBayes-consistency
For the family of all measurable functions H = H all, the minimizability gaps vanish. In this case,
Theorems4and5implythefollowingexcesserrorboundsandBayes-consistencyguarantees.
Corollary6 Suppose that c(x,y) = 1 g(x)≠y. For all h ∈ H all and any distribution, the following
excesserrorboundshold:
E
L
def(h)−E
L
def(H all)≤Γ(E LRL2D(h)−E LRL2D(H all)),
√ √
whereΓ(t) = 2twhenΨ(t) = −log(t)andΓ(t) = 2(n+1)qtwhenΨ(t) = 1 (1−tq )withq ∈
q
(0,1). Furthermore, the surrogate loss L
RL2D
is Bayes-consistent with respect to L
def
in these
cases.
Corollary7 SupposethatΨ(t)=1−t. Forallh∈H
all
andanydistribution,thefollowingexcess
errorboundshold:
E
L
def(h)−E
L
def(H all)≤(n+1)(E LRL2D(h)−E LRL2D(H all)).
Furthermore,thesurrogatelossL isBayes-consistentwithrespecttoL inthiscase.
RL2D def
9MAOMOHRIZHONG
Therefore, Theorem 3 and Corollary 6 show that L is both realizable H-consistent and
RL2D
Bayes-consistent with respect to L . This solves the open problem raised by Mozannar et al.
def
(2023).
In particular, for cost functions based on classification error, c(x,y) = 1 g(x)≠y, our surrogate
loss L RL2D with Ψ(t) = −log(t) coincides with the surrogate loss L RS in (Mozannar et al., 2023),
moduloaconstant. ThisaffirmativelyanswersthequestionofwhethertheirsurrogatelossisBayes-
consistent when c(x,y) = 1 g(x)≠y. However, their surrogate loss cannot be shown to be Bayes-
consistent for a general cost function. In contrast, our surrogate losses L RL2D with Ψ(t) = 1−t
areadaptabletogeneralcostfunctionsandbenefitfrombothH-consistencyboundsandrealizable
H-consistencyguarantees. Wealsoprovideamoregeneralfamilyofcomp-sumlossfunctionswith
Ψ(t)= 1 (1−tq )thatbenefitfrombothH-consistencyboundsandrealizableH-consistencywhen
q
c(x,y)=1 g(x)≠y.
5. RelationshipbetweenH-consistencyboundsandrealizableH-consistency
Here, we discuss the relationship between H-consistency bounds and realizable H-consistency.
First,realizableH-consistencydoesnotimplyH-consistencybounds,sinceH-consistencybounds
require that the relationship holds for all distributions, not just realizable ones. Moreover, H-
consistency bounds provide non-asymptotic guarantees, while realizable H-consistency provides
onlyasymptoticguarantees. Second, H-consistencyboundsimplyrealizableH-consistencyinthe
standardmulti-classclassificationsetting. Thisisbecauseminimizabilitygapsvanishunderthere-
alizable assumption in standard case. In particular, for comp-sum losses, the following holds (see
AppendixCforproof).
Theorem8 Assume that there exists a zero error solution h∗ ∈ H with E ℓ0−1(h∗) = 0 and H is
closedunderscaling. Assumethatlim t→1Ψ(t)=0. Then,theminimizabilitygapofcomp-sumloss
ℓ
comp
vanishes: M ℓcomp(H )=0.
However,inthedeferralsetting,thisrelationshipnolongerholds: H-consistencyboundscannot
implyrealizableH-consistency. Inparticular,Maoetal.(2023f)showedthatL benefitsfromH-
CE
consistencybounds,whileMozannaretal.(2023)showedthatitisnotrealizableH-consistent. The
lossfunctionin(Madrasetal.,2018)isnotBayes-consistent,andthusdoesnothaveH-consistency
boundguarantees,butisactuallyrealizableH-consistent(Mozannaretal.,2023).
6. Experiments
Inthissection,weempiricallyevaluateourproposedsurrogatelossesandcomparethemwithexist-
ingbaselines.
Experimental settings. We follow the setting of Mozannar et al. (2023) and conduct experi-
ments on three real-world datasets: CIFAR-10H (Battleday et al., 2020), HateSpeech (Davidson
et al., 2017), and COMPASS (Dressel and Farid, 2018). For these three datasets, we adopt the
same model class as that in (Mozannar et al., 2023, Table 1). Each dataset is randomly split into
70%,10%,and20%fortraining,validation,andtesting,respectively.
As with (Mozannar et al., 2023), we choose the cost function to be the expert’s classification
error: c(x,y) = 1 g(x)≠y. We compare our surrogate to four baselines as described in Section 3.3:
thecross-entropysurrogateL from(MozannarandSontag,2020),theone-vs-allsurrogateL
CE OvA
10REALIZABLELEARNINGTODEFER
Table2: Comparisonofsystemaccuracy,acceptedaccuracyandcoverage;mean±standarddevia-
tionoverthreeruns. RealizableL2Doutperformsoriscomparabletobaselinesinallthesettings.
Method Dataset SystemAccuracy AcceptedAccuracy Coverage
MozannarandSontag(2020)(L CE) 91.60±0.15 94.61±0.67 44.55±1.68
VermaandNalisnick(2022)(L OvA) 92.18±0.10 95.43±0.36 58.56±3.18
Mozannaretal.(2023)(L RS)
HateSpeech
91.83±0.63 95.37±0.72 54.78±3.70
Maoetal.(2024a)(L general) 92.05±0.04 96.28±0.35 46.74±2.80
RealizableL2D(L RL2D,q=0.7) 92.20±0.54 96.06±0.39 57.85±0.76
RealizableL2D(L RL2D,q=1) 91.97±0.29 96.57±0.69 53.25±2.49
MozannarandSontag(2020)(L CE) 66.33±0.47 73.65±1.83 55.17±9.51
VermaandNalisnick(2022)(L OvA) 66.33±1.31 71.03±5.10 53.33±4.73
Mozannaretal.(2023)(L RS)
COMPASS
66.00±2.27 63.20±4.23 69.50±10.8
Maoetal.(2024a)(L general) 66.67±0.62 76.25±2.42 48.33±5.31
RealizableL2D(L RL2D,q=0.7) 66.17±2.01 69.33±3.03 55.67±5.95
RealizableL2D(L RL2D,q=1) 66.83±0.85 69.02±2.42 54.83±0.62
MozannarandSontag(2020)(L CE) 96.27±0.51 98.77±0.71 64.33±6.13
VermaandNalisnick(2022)(L OvA) 96.25±0.45 98.74±0.54 67.88±6.16
Mozannaretal.(2023)(L RS)
CIFAR-10H
96.63±0.18 98.23±0.78 66.63±1.80
Maoetal.(2024a)(L general) 96.75±0.55 98.65±0.80 65.68±3.36
RealizableL2D(L RL2D,q=0.7) 96.80±0.25 98.37±0.20 76.77±3.63
RealizableL2D(L RL2D,q=1) 96.57±0.05 98.34±0.24 77.37±2.43
from(MozannarandSontag,2020),therealizablesurrogateL from(Mozannaretal.,2023),and
RS
the general surrogate L from (Mao et al., 2024a). For L , we choose Φ as the logistic
general OvA
loss, following (Verma and Nalisnick, 2022). For L , we choose ℓ as the generalized cross
general
entropylosswithq =0.7,following(Maoetal.,2024a). ForourRealizableL2DsurrogateL RL2D,
we consider two choices: ℓ as the generalized cross entropy loss with q = 0.7, following (Zhang
and Sabuncu, 2018; Mao et al., 2024a), and ℓ as the mean absolute error loss (q = 1). Among
these, L , L and L are Bayes-consistent but not realizable H-consistent; L , L
CE OvA general RS RL2D
withq =0.7andL
RL2D
withq =1arebothBayes-consistentandrealizableH-consistent,asshown
in Sections 4.2 and 4.4. Note that in this case, L is a special case of L when Ψ is chosen
RS RL2D
as t ↦ −log(t). We use the same optimizer, learning rate, and number of epochs as chosen in
(Mozannaretal.,2023), andweselectthemodelthatachievesthehighestsystemaccuracy, thatis
average[1−L def(h,x,y)],onavalidationset.
Evaluation. For the three real-world datasets, we report the system accuracy, that is average
value of [1−L def(h,x,y)] on the test data. For completeness, we also include the accepted accu-
racy,thatistheaveragevalueof[1 h(x)≠y1 h(x)∈[n]]. Thismetricconsidersonlyincorrectpredictions
(h(x)≠y)andmeasuresthefractionofthosewherethesystem’soutput(h(x))fallswithinthevalid
rangeofpossibleoutputs([n]). Wealsoreportthecoverage,thatistheaveragevalueof[1 h(x)∈[n]]
on the test set, or the fraction of test instances where the system’s prediction falls within the valid
range([n]). Foreachmetric,weaverageresultsoverthreerunsandreportthemeanaccuracyalong
withthestandarddeviationforbothourproposedmethodsandthebaselineapproaches.
Results. Table 2 shows that for the real-world datasets, L RL2D with q = 0.7, and L RL2D with
q =1eitheroutperformorarecomparabletothebestbaselineintermsofsystemaccuracyoneach
dataset. ThisperformanceissupportedbyourH-consistencyboundsandBayes-consistencyresults
11MAOMOHRIZHONG
for our Realizable L2D surrogate with respect to the deferral loss L , as shown in Sections 4.3
def
and4.4. Table2alsoshowsthatL achievesreasonablecoverageandacceptableaccuracy. The
RL2D
systemaccuracy,coverage,andstandarddeviationsofthebaselinesmatchthosein(Mozannaretal.,
2023). Moreover,L RS,L RL2Dwithq =0.7,andL RL2Dwithq =1performdifferentlyacrossvarious
datasets: L
RL2D
withq =0.7outperformstheothersonHateSpeechandCIFAR-10H,whileL
RL2D
with q = 1 outperforms the others on COMPASS. Note that in this case, L RS is a special case of
L
RL2D
whenΨischosenast↦−log(t). TheseresultsshowthatRealizableL2Dcanbenefitfrom
theflexibilityinthechoiceofΨ.
7. Conclusion
Weintroducedabroadfamilyofsurrogatelossesandalgorithmsforlearningtodefer,parameterized
by a non-increasing function. We established their realizable H-consistency properties under mild
conditions and proved that several of these surrogate losses benefit from H-consistency bounds
for cost functions based on classification error and general cost functions, which also imply their
Bayes-consistency. Thisresearchnotonlyresolvesanopenquestionposedinpreviousworkbutalso
lays the groundwork for comparing various consistency notions in learning to defer and standard
classification. Lookingforward,ourapproachoffersapromisingavenueforanalyzingmulti-expert
andtwo-stagesettings.
References
Durmus Alp Emre Acar, Aditya Gangrade, and Venkatesh Saligrama. Budget learning via brack-
eting. In International Conference on Artificial Intelligence and Statistics, pages 4109–4119,
2020.
PranjalAwasthi,AnqiMao,MehryarMohri,andYutaoZhong. H-consistencyboundsforsurrogate
lossminimizers. InInternationalConferenceonMachineLearning,2022a.
PranjalAwasthi,AnqiMao,MehryarMohri,andYutaoZhong. Multi-classH-consistencybounds.
InAdvancesinneuralinformationprocessingsystems,2022b.
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. Theoretically grounded loss func-
tionsandalgorithmsforadversarialrobustness. InInternationalConferenceonArtificialIntelli-
genceandStatistics,pages10077–10094,2023a.
Pranjal Awasthi, Anqi Mao, Mehryar Mohri, and Yutao Zhong. DC-programming for neural net-
workoptimizations. JournalofGlobalOptimization,2023b.
Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss.
JournalofMachineLearningResearch,9(8),2008.
Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk
bounds. JournaloftheAmericanStatisticalAssociation,101(473):138–156,2006.
RuairidhMBattleday,JoshuaCPeterson,andThomasLGriffiths. Capturinghumancategorization
of natural images by combining deep networks and cognitive models. Nature communications,
11(1):5418,2020.
12REALIZABLELEARNINGTODEFER
NinaLCorveloBenzandManuelGomezRodriguez. Counterfactualinferenceofsecondopinions.
InUncertaintyinArtificialIntelligence,pages453–463,2022.
JosephBerkson. Applicationofthelogisticfunctiontobio-assay. JournaloftheAmericanStatisti-
calAssociation,39:357—-365,1944.
JosephBerkson. WhyIpreferlogitstoprobits. Biometrics,7(4):327—-339,1951.
Se´bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-
mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Earlyexperimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023.
Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, Jinjie Gu, Bo An, Gang Niu, and Masashi
Sugiyama. Generalizingconsistentmulti-classclassificationwithrejectiontobecompatiblewith
arbitrarylosses. InAdvancesinneuralinformationprocessingsystems,2022.
Yuzhou Cao, Hussein Mozannar, Lei Feng, Hongxin Wei, and Bo An. In defense of softmax
parametrization for calibrated and consistent learning to defer. In Advances in Neural Infor-
mationProcessingSystems,2023.
Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. Classification
with rejection based on cost-sensitive classification. In International Conference on Machine
Learning,pages1507–1517,2021.
Mohammad-AminCharusaieandSamiraSamadi. Aunifyingpost-processingframeworkformulti-
objectivelearn-to-deferproblems. arXivpreprintarXiv:2407.12710,2024.
Mohammad-AminCharusaie,HusseinMozannar,DavidSontag,andSamiraSamadi. Sampleeffi-
cient learning of predictors that complement humans. In International Conference on Machine
Learning,pages2972–3005,2022.
Guanting Chen, Xiaocheng Li, Chunlin Sun, and Hanzhao Wang. Learning to make adherence-
awareadvice. InInternationalConferenceonLearningRepresentations,2024.
Xin Cheng, Yuzhou Cao, Haobo Wang, Hongxin Wei, Bo An, and Lei Feng. Regression with
cost-basedrejection. InAdvancesinNeuralInformationProcessingSystems,2023.
C Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on information
theory,16(1):41–46,1970.
C.K.Chow. Anoptimumcharacterrecognitionsystemusingdecisionfunction. IEEET.C.,1957.
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In International
ConferenceonAlgorithmicLearningTheory,pages67–82,2016a.
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. In Advances in
NeuralInformationProcessingSystems,pages1660–1668,2016b.
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Theory and algorithms for learning with
rejectioninbinaryclassification. AnnalsofMathematicsandArtificialIntelligence,pages1–39,
2023.
13MAOMOHRIZHONG
Corinna Cortes, Anqi Mao, Christopher Mohri, Mehryar Mohri, and Yutao Zhong. Cardinality-
awaresetpredictionandtop-k classification. arXivpreprintarXiv:2407.07140,2024.
Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech
detectionandtheproblemofoffensivelanguage. InProceedingsoftheinternationalAAAIcon-
ferenceonwebandsocialmedia,volume11,pages512–515,2017.
AbirDe,ParamitaKoley,NiloyGanguly,andManuelGomez-Rodriguez. Regressionunderhuman
assistance. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2611–2620,
2020.
AbirDe,NastaranOkati,AliZarezade,andManuelGomezRodriguez. Classificationunderhuman
assistance. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 5905–5913,
2021.
Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science
advances,4(1):eaao5580,2018.
Ran El-Yaniv and Yair Wiener. Active learning via perfect selective classification. Journal of
MachineLearningResearch,13(2),2012.
Ran El-Yaniv et al. On the foundations of noise-free selective classification. Journal of Machine
LearningResearch,11(5),2010.
Aditya Gangrade, Anil Kag, and Venkatesh Saligrama. Selective classification via one-sided pre-
diction. In International Conference on Artificial Intelligence and Statistics, pages 2179–2187,
2021.
Ruijiang Gao, Maytal Saar-Tsechansky, Maria De-Arteaga, Ligong Han, Min Kyung Lee, and
MatthewLease. Human-aicollaborationwithbanditfeedback. arXivpreprintarXiv:2105.10614,
2021.
YonatanGeifmanandRanEl-Yaniv. Selectiveclassificationfordeepneuralnetworks. InAdvances
inneuralinformationprocessingsystems,2017.
YonatanGeifmanandRanEl-Yaniv. Selectivenet: Adeepneuralnetworkwithanintegratedreject
option. InInternationalconferenceonmachinelearning,pages2151–2159,2019.
Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise for
deepneuralnetworks. InProceedingsoftheAAAIconferenceonartificialintelligence,2017.
Patrick Hemmer, Sebastian Schellhammer, Michael Vo¨ssing, Johannes Jakubik, and Gerhard
Satzger. Formingeffectivehuman-aiteams: Buildingmachinelearningmodelsthatcomplement
thecapabilitiesofmultipleexperts. arXivpreprintarXiv:2206.07948,2022.
PatrickHemmer,LukasThede,MichaelVo¨ssing,JohannesJakubik,andNiklasKu¨hl. Learningto
deferwithlimitedexpertpredictions. arXivpreprintarXiv:2304.07306,2023.
Wenming Jiang, Ying Zhao, and Zehan Wang. Risk-controlled selective prediction for regres-
sion deep neural network models. In 2020 International Joint Conference on Neural Networks
(IJCNN),pages1–8,2020.
14REALIZABLELEARNINGTODEFER
Wittawat Jitkrittum, Neha Gupta, Aditya K Menon, Harikrishna Narasimhan, Ankit Rawat, and
Sanjiv Kumar. When does confidence-based cascade deferral suffice? In Advances in Neural
InformationProcessingSystems,2023.
ShalmaliJoshi,SonaliParbhoo,andFinaleDoshi-Velez. Pre-emptivelearning-to-deferforsequen-
tialmedicaldecision-makingunderuncertainty. arXivpreprintarXiv:2109.06312,2021.
Gavin Kerrigan, Padhraic Smyth, and Mark Steyvers. Combining human predictions with model
probabilitiesviaconfusionmatricesandcalibration. AdvancesinNeuralInformationProcessing
Systems,34:4421–4434,2021.
Vijay Keswani, Matthew Lease, and Krishnaram Kenthapadi. Towards unbiased and accurate de-
ferraltomultipleexperts. InProceedingsofthe2021AAAI/ACMConferenceonAI,Ethics,and
Society,pages154–165,2021.
XiaochengLi,ShangLiu,ChunlinSun,andHanzhaoWang. Whenno-rejectionlearningisoptimal
forregressionwithrejection. arXivpreprintarXiv:2307.02932,2023.
JessieLiu,BlancaGallego,andSebastianoBarbieri. Incorporatinguncertaintyinlearningtodefer
algorithmsforsafecomputer-aideddiagnosis. Scientificreports,12(1):1762,2022.
Shuqi Liu, Yuzhou Cao, Qiaozhen Zhang, Lei Feng, and Bo An. Mitigating underfitting in learn-
ing to defer with consistent losses. In International Conference on Artificial Intelligence and
Statistics,pages4816–4824,2024.
PhilLongandRoccoServedio. ConsistencyversusrealizableH-consistencyformulticlassclassifi-
cation. InInternationalConferenceonMachineLearning,pages801–809,2013.
DavidMadras,ElliotCreager,ToniannPitassi,andRichardZemel. Learningadversariallyfairand
transferablerepresentations. arXivpreprintarXiv:1802.06309,2018.
AnqiMao,ChristopherMohri,MehryarMohri,andYutaoZhong. Two-stagelearningtodeferwith
multipleexperts. InAdvancesinneuralinformationprocessingsystems,2023a.
AnqiMao, Mehryar Mohri, andYutaoZhong. H-consistencybounds: Characterizationandexten-
sions. InAdvancesinNeuralInformationProcessingSystems,2023b.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. H-consistency bounds for pairwise misranking loss
surrogates. InInternationalconferenceonMachinelearning,2023c.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Ranking with abstention. In ICML 2023 Workshop
TheManyFacetsofPreference-BasedLearning,2023d.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Structured prediction with stronger consistency
guarantees. InAdvancesinNeuralInformationProcessingSystems,2023e.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Cross-entropy loss functions: Theoretical analysis
andapplications. InInternationalConferenceonMachineLearning,2023f.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Principled approaches for learning to defer with
multipleexperts. InInternationalSymposiumonArtificialIntelligenceandMathematics,2024a.
15MAOMOHRIZHONG
AnqiMao,MehryarMohri,andYutaoZhong. Predictor-rejectormulti-classabstention: Theoretical
analysis and algorithms. In International Conference on Algorithmic Learning Theory, pages
822–867,2024b.
AnqiMao,MehryarMohri,andYutaoZhong.Theoreticallygroundedlossfunctionsandalgorithms
forscore-basedmulti-classabstention. InInternationalConferenceonArtificialIntelligenceand
Statistics,pages4753–4761,2024c.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. H-consistency guarantees for regression. arXiv
preprintarXiv:2403.19480,2024d.
AnqiMao,MehryarMohri,andYutaoZhong. Regressionwithmulti-expertdeferral. arXivpreprint
arXiv:2403.19494,2024e.
AnqiMao,MehryarMohri,andYutaoZhong.Top-kclassificationandcardinality-awareprediction.
arXivpreprintarXiv:2403.19625,2024f.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. A universal growth rate for learning with smooth
surrogatelosses. arXivpreprintarXiv:2405.05968,2024g.
Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Anqi Mao, and Yutao Zhong.
Learning to reject with a fixed predictor: Application to decontextualization. In International
ConferenceonLearningRepresentations,2024.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MITPress,secondedition,2018.
Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In
InternationalConferenceonMachineLearning,pages7076–7087,2020.
Hussein Mozannar, Arvind Satyanarayan, and David Sontag. Teaching humans when to defer to a
classifierviaexemplars. InProceedingsoftheAAAIConferenceonArtificialIntelligence,pages
5323–5331,2022.
Hussein Mozannar, Hunter Lang, Dennis Wei, Prasanna Sattigeri, Subhro Das, and David Sontag.
Whoshouldpredict? exactalgorithmsforlearningtodefertohumans. InInternationalConfer-
enceonArtificialIntelligenceandStatistics,pages10520–10545,2023.
HarikrishnaNarasimhan,WittawatJitkrittum,AdityaKrishnaMenon,AnkitSinghRawat,andSan-
jiv Kumar. Post-hoc estimators for learning to defer to an expert. In Advances in Neural Infor-
mationProcessingSystems,pages29292–29304,2022.
JerzyNeymanandEgonSharpePearson. Ix.ontheproblemofthemostefficienttestsofstatistical
hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing
PapersofaMathematicalorPhysicalCharacter,231(694-706):289–337,1933.
ChenriNi,NontawatCharoenphakdee,JunyaHonda,andMasashiSugiyama. Onthecalibrationof
multiclass classification with rejection. In Advances in Neural Information Processing Systems,
pages2582–2592,2019.
16REALIZABLELEARNINGTODEFER
NastaranOkati,AbirDe,andManuelRodriguez. Differentiablelearningundertriage. Advancesin
NeuralInformationProcessingSystems,34:9140–9151,2021.
FilippoPalomba,AndreaPugnana,Jose´ ManuelAlvarez,andSalvatoreRuggieri. Acausalframe-
workforevaluatingdeferringsystems. arXivpreprintarXiv:2405.18902,2024.
Melanie F Pradier, Javier Zazo, Sonali Parbhoo, Roy H Perlis, Maurizio Zazzi, and Finale Doshi-
Velez.Preferentialmixture-of-experts: Interpretablemodelsthatrelyonhumanexpertiseasmuch
aspossible. AMIASummitsonTranslationalScienceProceedings,2021:525,2021.
Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, and Sendhil Mul-
lainathan. The algorithmic automation problem: Prediction, triage, and human effort. arXiv
preprintarXiv:1903.12220,2019.
NaveenRamanandMichaelYee.Improvinglearning-to-deferalgorithmsthroughfine-tuning.arXiv
preprintarXiv:2112.10768,2021.
Harish G Ramaswamy, Ambuj Tewari, and Shivani Agarwal. Consistent algorithms for multiclass
classificationwithanabstainoption. ElectronicJournalofStatistics,12(1):530–554,2018.
MarkDReidandRobertCWilliamson.Compositebinarylosses.TheJournalofMachineLearning
Research,11:2387–2422,2010.
Abhin Shah, Yuheng Bu, Joshua K Lee, Subhro Das, Rameswar Panda, Prasanna Sattigeri, and
GregoryWWornell. Selectiveregressionunderfairnesscriteria. InInternationalConferenceon
MachineLearning,pages19598–19615,2022.
IngoSteinwart. Howtocomparedifferentlossfunctionsandtheirrisks. ConstructiveApproxima-
tion,26(2):225–287,2007.
Eleni Straitouri, Adish Singla, Vahid Balazadeh Meresht, and Manuel Gomez-Rodriguez. Rein-
forcementlearningunderalgorithmictriage. arXivpreprintarXiv:2109.11328,2021.
EleniStraitouri,LequnWang,NastaranOkati,andManuelGomezRodriguez. Provablyimproving
expertpredictionswithconformalprediction. arXivpreprintarXiv:2201.12006,2022.
Dharmesh Tailor, Aditya Patra, Rajeev Verma, Putra Manggala, and Eric Nalisnick. Learning to
defertoapopulation: Ameta-learningapproach. InInternationalConferenceonArtificialIntel-
ligenceandStatistics,pages3475–3483,2024.
AmbujTewariandPeterL.Bartlett.Ontheconsistencyofmulticlassclassificationmethods.Journal
ofMachineLearningResearch,8(36):1007–1025,2007.
Pierre Franc¸ois Verhulst. Notice sur la loi que la population suit dans son accroissement. Corre-
spondancemathe´matiqueetphysique,10:113—-121,1838.
Pierre Franc¸ois Verhulst. Recherches mathe´matiques sur la loi d’accroissement de la population.
Nouveaux Me´moires de l’Acade´mie Royale des Sciences et Belles-Lettres de Bruxelles, 18:1—-
42,1845.
17MAOMOHRIZHONG
RajeevVermaandEricNalisnick. Calibratedlearningtodeferwithone-vs-allclassifiers. InInter-
nationalConferenceonMachineLearning,pages22184–22202,2022.
RajeevVerma,DanielBarrejo´n,andEricNalisnick. Learningtodefertomultipleexperts: Consis-
tentsurrogatelosses,confidencecalibration,andconformalensembles. InInternationalConfer-
enceonArtificialIntelligenceandStatistics,pages11415–11434,2023.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol
Vinyals,PercyLiang,JeffDean,andWilliamFedus.Emergentabilitiesoflargelanguagemodels.
CoRR,abs/2206.07682,2022.
YairWienerandRanEl-Yaniv. Agnosticselectiveclassification. InAdvancesinneuralinformation
processingsystems,2011.
Yair Wiener and Ran El-Yaniv. Pointwise tracking the optimal regression function. Advances in
NeuralInformationProcessingSystems,25,2012.
YairWienerandRanEl-Yaniv. Agnosticpointwise-competitiveselectiveclassification. Journalof
ArtificialIntelligenceResearch,52:171–201,2015.
Bryan Wilder, Eric Horvitz, and Ece Kamar. Learning to complement humans. In International
JointConferencesonArtificialIntelligence,pages1526–1533,2021.
Ming Yuan and Marten Wegkamp. Classification methods with reject option based on convex risk
minimization. JournalofMachineLearningResearch,11(1),2010.
MingYuanandMartenWegkamp. SVMswitharejectoption. InBernoulli,2011.
AhmedZaoui,ChristopheDenis,andMohamedHebiri. Regressionwithrejectoptionandapplica-
tiontoknn. InAdvancesinNeuralInformationProcessingSystems,pages20073–20082,2020.
Mingyuan Zhang and Shivani Agarwal. Bayes consistency vs. H-consistency: The interplay be-
tweensurrogatelossfunctionsandthescoringfunctionclass. InAdvancesinNeuralInformation
ProcessingSystems,2020.
Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk
minimization. TheAnnalsofStatistics,32(1):56–85,2004a.
TongZhang. Statisticalanalysisofsomemulti-categorylargemarginclassificationmethods. Jour-
nalofMachineLearningResearch,5(Oct):1225–1251,2004b.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
withnoisylabels. InAdvancesinneuralinformationprocessingsystems,2018.
Jason Zhao, Monica Agrawal, Pedram Razavi, and David Sontag. Directing human attention in
eventlocalizationforclinicaltimelinecreation. InMachineLearningforHealthcareConference,
pages80–102,2021.
Chenyu Zheng, Guoqiang Wu, Fan Bao, Yue Cao, Chongxuan Li, and Jun Zhu. Revisiting dis-
criminative vs. generative classifiers: Theory and implications. In International Conference on
MachineLearning,pages42420–42477,2023.
18REALIZABLELEARNINGTODEFER
ContentsofAppendix
A ProofofrealizableH-consistency 20
B ProofofH-consistencybounds 21
B.1 Auxiliarylemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2 Ψ(t)=1−t . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B.3 Ψ(t)=−log(t) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.4 Ψ(t)= 1 (1−tq ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
q
C ProofofTheorem8 33
D Futurework 34
19MAOMOHRIZHONG
AppendixA. ProofofrealizableH-consistency
Theorem3 Assume that H is closed under scaling. Suppose that Ψ is non-increasing, Ψ(2 ) > 0
3
and lim t→1Ψ(t) = 0. Then, the surrogate loss L RL2D is realizable H-consistent with respect to
L .
def
ProofWefirstprovethatforevery(h,x,y)∈H ×X ×Y,thefollowinginequalityholds:
L RL2D(h,x,y)
L def(h,x,y)≤
Ψ(2
)
.
3
Wewillanalyzecasebycase.
1. CaseI:Ifh(x)∈[n](deferraldoesnotoccur):
(a) If1
h(x)≠y
=1,thenwemusthave
eh(x,y) 1 eh(x,y)+eh(x,n+1) 2
L def(h,x,y)=1,
∑y′∈Yeh(x,y′)
≤ 2,
∑y′∈Yeh(x,y′)
≤
3
1 2 2
(cid:212)⇒ L RL2D(h,x,y)≥c(x,y)Ψ( )+(1−c(x,y))Ψ( )≥Ψ( )L def(h,x,y).
2 3 3
(b) If1
h(x)≠y
=0,thenwemusthave
L RL2D(h,x,y)≥0=L def(h,x,y).
2. CaseII:Ifh(x)=n+1(deferraloccurs): thenwemusthave
eh(x,y) 1
L def(h,x,y)=c(x,y),
∑y′∈Yeh(x,y′)
≤
2
1 2
(cid:212)⇒ L RL2D(h,x,y)≥c(x,y)Ψ( )≥Ψ( )L def(h,x,y).
2 3
This concludes that L def(h,x,y) ≤ LRL2 ΨD (( 2 3h ),x,y). Next, we prove that L RL2D is realizable H-
consistent under the assumptions. Consider a distribution and an expert under which there exists
a zero error solution h∗ ∈ H with E L (h∗) = 0. Let hˆ be the minimizer of the surrogate loss:
def
hˆ ∈argmin h∈HE LRL2D(h). Letαbeanyrealnumber. Then,thefollowinginequalityholds:
1
E L def(hˆ )≤ Ψ( 32 )E LRL2D(hˆ ) (L def ≤ Ψ(1 2 3)L RL2D)
1
≤
Ψ(2
)E LRL2D(αh∗ ) (hˆ ∈argmin h∈HE LRL2D(h)andHisclosedunderscaling)
3
1
=
Ψ(2
)E [L RL2D(αh∗,x,y)∣h∗ (x)=n+1]P (h∗ (x)=n+1)
3
1
+
Ψ(2
)E [L RL2D(αh∗,x,y)∣h∗ (x)∈[n]]P (h∗ (x)∈[n]).
3
20REALIZABLELEARNINGTODEFER
Forthefirsttermconditionalonh∗(x)=n+1,wemusthaveh∗(x,n+1)>max y∈Yh∗(x,y)and
c(x,y)=0sincethedataisrealizable. Therefore,
lim E [L RL2D(αh∗,x,y)∣h∗ (x)=n+1]P (h∗ (x)=n+1)
α→+∞
= αl →im +∞E⎡ ⎢ ⎢ ⎢
⎢
⎣Ψ⎛ ⎝eαh∗ ∑(x y, ′y ∈) Y+ eαe hα ∗h (x∗ ( ,yx ′, )n+1)⎞ ⎠∣h∗ (x)=n+1⎤ ⎥ ⎥ ⎥
⎥
⎦P (h∗ (x)=n+1)
=E [0∣h∗ (x)=n+1]P (h∗ (x)=n+1) (lim t→1Ψ(t)=0andmonotoneconvergencetheorem)
=0.
For the second term conditional on h∗(x) ∈ [n], we must have h∗(x,y) > max y′∈Y,y′≠yh(x,y′)
sincethedataisrealizable. Therefore,
lim E [L RL2D(αh∗,x,y)∣h∗ (x)∈[n]]P (h∗ (x)∈[n])
α→+∞
⎡
⎢ ⎛
eαh∗ (x,y)
⎞
= αl →im +∞E⎢ ⎢
⎢
⎣c(x,y)Ψ ⎝∑y′∈Yeαh∗(x,y′)⎠
+(1−c(x,y))Ψ⎛ ⎝eαh∗ ∑(x y, ′y ∈) Y+ eαe hα ∗h (x∗ ( ,yx ′, )n+1)⎞ ⎠∣h∗ (x)∈[n]⎤ ⎥ ⎥
⎥
⎥
⎦P (h∗ (x)∈[n])
=E [0∣h∗ (x)∈[n]]P (h∗ (x)∈[n]) (lim t→1Ψ(t)=0andmonotoneconvergencetheorem)
=0.
Combining the two analyses, we conclude that E L (hˆ ) = 0 and thus L RL2D is realizable H-
def
consistentwithrespecttoL .
def
AppendixB. ProofofH-consistencybounds
Before delving into the proof, we first establish some essential notation and definitions. Let L
represent a deferral surrogate loss and H denote a hypothesis set. We define the conditional error
as C L(h,x) = E y∣x[L(h,x,y)], the best-in-class conditional error as C∗ L(H,x) = inf h∈HC L(h,x),
andtheconditionalregretas∆C L,H(h,x)=C L(h,x)−C∗ L(H,x). Weproceedtopresentageneral
theorem demonstrating that, to establish H-consistency bounds (1) with a concave function Γ, it
sufficestolowerboundtheconditionalregretofthesurrogatelossbythatofthedeferralloss,using
thesamefunctionΓ.
Theorem9 Ifthefollowingholdsforallh∈Handx∈X,forsomeconcavefunctionΓ:
∆C
L
,H(h,x)≤Γ(∆C L,H(h,x)), (4)
def
then,forallhypothesesh∈Handforanydistribution,
E
L
def(h)−E∗
L
def(H )+M
L
def(H )≤Γ(E L(h)−E∗ L(H )+M L(H )).
21MAOMOHRIZHONG
ProofWecanexpresstheexpectationsoftheconditionalregretsforL andLasfollows:
def
E x[∆C
L
def,H(h,x)]=E
L
def(h)−E∗
L
def(H )+M
L
def(H
)
E [∆C L,H(h,x)]=E L(h)−E∗ L(H )+M L(H ).
x
Then,byusing(4)andtakingtheexpectation,weobtain:
E
L
def(h)−E∗
L
def(H )+M
L
def(H )=E x[∆C
L
def,H(h,x)]
≤E [Γ(∆C L,H(h,x))] (Eq. (4))
x
≤Γ(E [∆C L,H(h,x)]) (concavityofΓ)
x
=Γ(E L(h)−E∗ L(H )+M L(H )).
Thus,theproofiscomplete.
Next, to prove H-consistency bounds using Theorem 9, we will characterize the conditional
regretofthedeferrallossL inthefollowingsection.
def
B.1. Auxiliarylemma
To simplify the presentation, we introduce the following notation. For any y ∈ Y, define p(x,y) =
P (Y = y ∣ X = x) as the conditional probability that Y = y given X = x. For brevity, we will
omit the dependency on x in our notation. We denote by h y = h(x,y) for any y ∈ Y. We also
denotebyp
y
=p(x,y)andq
y
=p(x,y)c(x,y)foranyy ∈Y,andp
n+1
=∑y∈Yp(x,y)(1−c(x,y)).
⎧
Note that p(x,y)(1−c(x,y)) = p y −q y, ∀y ∈ Y. Let p h = p h(x) = ⎪⎪ ⎨
⎪⎪
⎩p ph n( +x 1) h h( (x x) )∈ =[ nn +] 1.. Let
y max = argmax y∈Yp y and h max = argmax y∈Yh y. Note that both y max and h max are in the label
spaceY,whileh(x)isintheaugmentedlabelspaceY. Wecharacterizetheconditionalregretofthe
deferrallossL asfollows.
def
Lemma10 AssumethatHissymmetricandcomplete. Then,theconditionalregretofthedeferral
lossL
def
canbeexpressedasfollows: ∆C
L
def,H(h,x)=max{p ymax,p n+1}−p h.
ProofWecanwritetheconditionalerrorofthedeferrallossasfollows:
C
L
(h,x)
def
= ∑p(x,y)L def(h,x,y)
y∈Y
= ∑p(x,y)1 h(x)≠y1 h(x)∈[n]+∑p(x,y)c(x,y)1 h(x)=n+1
y∈Y y∈Y
=(1−p h(x))1 h(x)∈[n]+(1−p n+1)1
h(x)=n+1
=1−p h.
Since H is symmetric and complete, for any x ∈ X, {h(x)∶h∈H } = Y. Then, the best-in-class
conditionalerrorofL canbeexpressedasfollows:
def
C∗ L def(H,x)= hin ∈Hf C L def(h,x)=1−max{p n+1,p ymax} (5)
22REALIZABLELEARNINGTODEFER
Therefore,∆C
L
def,H(h,x)=C
L
def(h,x)−C∗
L
def(H,x)=max{p ymax,p n+1}−p h.
Next, we will present the proofs separately in the following sections, by lower bounding the
conditionalregretofthesurrogatelossLbythatofthedeferrallossL usingLemma10.
def
B.2. Ψ(t)=1−t
Theorem11 AssumethatHissymmetricandcomplete. Then,forallh∈Handanydistribution,
thefollowingH-consistencyboundholds:
E
L
def(h)−E
L
def(H )+M
L
def(H )≤n(E LRL2D(h)−E LRL2D(H )+M LRL2D(H )).
ProofWecanwritetheconditionalerrorofthesurrogatelossasfollows:
C LRL2D(h,x)
= ∑p(x,y)L RL2D(h,x,y)
y∈Y
⎛
eh(x,y)
⎞ ⎛
eh(x,y)+eh(x,n+1)⎞
= ∑p(x,y)c(x,y) 1− +∑p(x,y)(1−c(x,y)) 1−
y∈Y ⎝
∑y′∈Yeh(x,y′)⎠
y∈Y ⎝
∑y′∈Yeh(x,y′)
⎠
⎛
ehy
⎞ ⎛
ehy +ehn+1⎞
= y∑ ∈Yq y ⎝1− ∑y′∈Yeh y′⎠+ y∑ ∈Y(p y−q y) ⎝1− ∑y′∈Yeh y′ ⎠.
ByLemma10,theconditionalregretofthedeferrallosscanbeexpressedas
∆C
L
def,H(h,x)=max{p ymax,p n+1}−p h.
Next,wewillshowthattheconditionalregretofthesurrogatelosscanbelowerboundedasfollows:
1
∆C LRL2D,H(h,x)=C LRL2D(h)−C∗ LRL2D(H )≥ n+1(∆C L def,H(h,x)). (6)
We first prove that for any hypothesis h and x ∈ X, if y max ≠ h max, then the conditional error of h
⎧ ⎪⎪⎪⎪h
hmax
y =y
max
canbelowerboundedbythatofh,whichsatisfiesthath(x,y)=⎨ ⎪⎪⎪⎪ ⎩h hy ymax y ot= heh rwm ia sx e.. Indeed,
C LRL2D(h)−C LRL2D(h)=q ymax⎛ ⎝1− ∑e yh ′∈y Ym eax
h
y′⎞ ⎠+(p
ymax
−q ymax)⎛ ⎝1− eh ∑ym ya ′x ∈Y+ ee hh yn ′+1⎞
⎠
⎛
eh
hmax ⎞ ⎛
eh
hmax
+ehn+1⎞
+q hmax ⎝1− ∑y′∈Yeh y′⎠+(p hmax −q hmax) ⎝1− ∑y′∈Yeh y′ ⎠
⎛
eh
hmax ⎞ ⎛
eh
hmax
+ehn+1⎞
−q ymax ⎝1− ∑y′∈Yeh y′⎠−(p ymax −q ymax) ⎝1− ∑y′∈Yeh y′ ⎠
⎛
ehymax
⎞ ⎛
ehymax +ehn+1⎞
−q hmax ⎝1− ∑y′∈Yeh y′⎠−(p hmax −q hmax) ⎝1− ∑y′∈Yeh y′ ⎠
1
= ∑y′∈Yeh y′(p ymax −p hmax)(eh hmax −ehymax)≥0.
23MAOMOHRIZHONG
Therefore, we only need to lower bound the conditional regret of hypothesis h satisfying y max =
h max. Next, we will analyze case by case. Note that when (p ymax −p n+1)(h ymax −h n+1) > 0, we
have∆C
L
def,H(h,x)=max{p ymax,p n+1}−p
h
=0.
1. Case I: If p ymax −p n+1 ≥ 0 and h ymax −h n+1 ≤ 0: we define a new hypothesis h µ such that
h
µ(x,y)=⎧ ⎪⎪⎪⎪ ⎨l lo og g( (e eh hn ym+1 ax+ −µ µ)
)
y
y
= =y nm +ax
1 ,whereehymax ≥µ≥0. Then,wecanlowerbound
⎪⎪⎪⎪
⎩h(x,y) otherwise.
the conditional regret of L RL2D by using ∆C LRL2D,H(h,x) ≥ C LRL2D(h)−C∗ LRL2D(h µ) for
anyehymax ≥µ≥0:
∆C LRL2D,H(h,x)
≥ sup (C LRL2D(h)−C∗ LRL2D(h µ))
ehymax≥µ≥0
⎛ ⎛
ehymax
⎞ ⎛
ehymax +ehn+1⎞
≥ ehyms au xp ≥µ≥0⎝q ymax ⎝1− ∑y′∈Yeh y′⎠+(p ymax −q ymax) ⎝1− ∑y′∈Yeh y′ ⎠
⎛ eh y′ +ehn+1⎞
+ y′∈Y,y∑ ′≠ymax(p y′ −q y′) ⎝1−
∑y′∈Yeh y′ ⎠
⎛ ehn+1 +µ⎞ ⎛ ehn+1 +eh hmax⎞⎞
−q ymax ⎝1− ∑y′∈Yeh y′⎠−(p ymax −q ymax) ⎝1− ∑y′∈Yeh y′ ⎠⎠
⎛ eh y′ +ehymax −µ⎞
− y′∈Y,y∑ ′≠ymax(p y′ −q y′) ⎝1−
∑y′∈Yeh y′ ⎠
1
= ∑y′∈Yeh y′ ehyms au xp ≥µ≥0(q ymax(ehn+1 +µ−ehymax)+(p n+1−p ymax +q ymax)(ehymax −µ−ehn+1))
ehn+1
=(p ymax −p n+1) ∑y′∈Yeh y′ (µ=ehymax achievesthemaximum)
1
≥ n+1(p ymax −p n+1) (bytheassumptionh n+1 ≥h ymax =h hmax)
1
= n+1(∆C L def,H(h,x)) (bytheassumptionp ymax ≥p n+1 andh ymax −h n+1 ≤0)
2. CaseII:Ifp ymax −p n+1 ≤ 0andh ymax −h n+1 ≥ 0: wedefineanewhypothesish µ suchthat
h µ(x,y) =
⎧ ⎪⎪⎪⎪ ⎨l lo og g( (e eh hn ym+1 ax− +µ µ)
)
y
y
= =y nm +ax
1 , where ehn+1 ≥ µ ≥ 0. Then, we can lower bound
⎪⎪⎪⎪
⎩h(x,y) otherwise.
the conditional regret of L RL2D by using ∆C LRL2D,H(h,x) ≥ C LRL2D(h)−C∗ LRL2D(h µ) for
24REALIZABLELEARNINGTODEFER
anyehn+1 ≥µ≥0:
∆C LRL2D,H(h,x)
≥ sup (C LRL2D(h)−C∗ LRL2D(h µ))
ehn+1≥µ≥0
⎛ ⎛
ehymax
⎞ ⎛
ehymax +ehn+1⎞
≥ ehns +u 1≥p µ≥0⎝q ymax ⎝1− ∑y′∈Yeh y′⎠+(p ymax −q ymax) ⎝1− ∑y′∈Yeh y′ ⎠
⎛ eh y′ +ehn+1⎞
+ y′∈Y,y∑ ′≠ymax(p y′ −q y′) ⎝1−
∑y′∈Yeh y′ ⎠
⎛ ehn+1 −µ⎞ ⎛ ehn+1 +eh hmax⎞⎞
−q ymax ⎝1− ∑y′∈Yeh y′⎠−(p ymax −q ymax) ⎝1− ∑y′∈Yeh y′ ⎠⎠
⎛ eh y′ +ehymax +µ⎞
− y′∈Y,y∑ ′≠ymax(p y′ −q y′) ⎝1−
∑y′∈Yeh y′ ⎠
1
= ∑y′∈Yeh y′ ehns +u 1≥p µ≥0(q ymax(ehn+1 −µ−ehymax)+(p n+1−p ymax +q ymax)(ehymax +µ−ehn+1))
ehymax
=(p n+1−p ymax)
∑y′∈Yeh y′
(µ=ehn+1 achievesthemaximum)
1
≥ n+1(p n+1−p ymax) (bytheassumptionh hmax =h ymax ≥h n+1)
1
= n+1(∆C L def,H(h,x)) (bytheassumptionp n+1 ≥p ymax andh ymax −h n+1 ≥0)
Thisprovestheinequality(6). ByTheorem9,wecompletetheproof.
25MAOMOHRIZHONG
B.3. Ψ(t)=−log(t)
Theorem12 AssumethatHissymmetricandcomplete. Assumethatc(x,y)=1 g(x)≠y. Then,for
allh∈Handanydistribution,thefollowingH-consistencyboundholds:
√
E
L
def(h)−E
L
def(H )+M
L
def(H )≤2 E LRL2D(h)−E LRL2D(H )+M LRL2D(H ).
ProofWecanwritetheconditionalerrorofthesurrogatelossasfollows:
C LRL2D(h,x)
= ∑p(x,y)L RL2D(h,x,y)
y∈Y
⎛
eh(x,y)
⎞
⎛eh(x,y)+eh(x,n+1)⎞
=−∑p(x,y)c(x,y)log −∑p(x,y)(1−c(x,y))log
y∈Y
⎝∑y′∈Yeh(x,y′)⎠
y∈Y ⎝
∑y′∈Yeh(x,y′)
⎠
⎛
ehy
⎞
⎛ehy +ehn+1⎞
=− y∑ ∈Yq ylog
⎝∑y′∈Yeh
y′⎠− y∑ ∈Y(p y−q y)log
⎝ ∑y′∈Yeh y′
⎠.
ByLemma10,theconditionalregretofthedeferrallosscanbeexpressedas
∆C
L
def,H(h,x)=max{p ymax,p n+1}−p h.
Next,wewillshowthattheconditionalregretofthesurrogatelosscanbelowerboundedasfollows:
∆C LRL2D,H(h,x)=C LRL2D(h)−C∗ LRL2D(H )≥ 21 (∆C L def,H(h,x))2 . (7)
We first consider the case where g(x) ≠ y max. Otherwise, it would be straightforward to see
thattheboundholds. Inthecasewhereg(x)≠y max,wehaveq
ymax
=p ymax. Wefirstprovethatfor
any hypothesis h and x ∈ X, if y max ≠ h max, then the conditional error of h can be lower bounded
26REALIZABLELEARNINGTODEFER
⎧ ⎪⎪⎪⎪h
hmax
y =y
max
bythatofh,whichsatisfiesthath(x,y)=⎨ ⎪⎪⎪⎪ ⎩h hy ymax y ot= heh rwm ia sx e.. Indeed,
C LRL2D(h)−C LRL2D(h)
⎛
ehymax
⎞
⎛ehymax +ehn+1⎞
=−q ymaxlog
⎝∑y′∈Yeh
y′⎠−(p
ymax
−q ymax)log
⎝ ∑y′∈Yeh y′ ⎠
⎛
eh
hmax ⎞
⎛eh
hmax
+ehn+1⎞
−q hmaxlog
⎝∑y′∈Yeh
y′⎠−(p
hmax
−q hmax)log
⎝ ∑y′∈Yeh y′ ⎠
⎛
eh
hmax ⎞
⎛eh
hmax
+ehn+1⎞
+q ymaxlog
⎝∑y′∈Yeh
y′⎠+(p
ymax
−q ymax)log
⎝ ∑y′∈Yeh y′ ⎠
⎛
ehymax
⎞
⎛ehymax +ehn+1⎞
+q hmaxlog
⎝∑y′∈Yeh
y′⎠+(p
hmax
−q hmax)log
⎝ ∑y′∈Yeh y′ ⎠
eh
hmax
eh
hmax
+ehn+1
=(q ymax −q hmax)log( ehymax)+(p ymax −q ymax −p hmax +q hmax)log( ehymax +ehn+1)
eh
hmax
+ehn+1
≥(p ymax −p hmax)log( ehymax +ehn+1)
≥0.
Therefore, we only need to lower bound the conditional regret of hypothesis h satisfying y max =
h max. Sincec(x,y)=1 g(x)≠y,wehavep ymax ≥p n+1 =p g(x). Notethatwhen(p ymax−p n+1)(h ymax−
h n+1)>0,wehave∆C
L
def,H(h,x)=max{p ymax,p n+1}−p
h
=0. Whenh ymax−h
n+1
≤0,wedefine
a new hypothesis h µ such that h µ(x,y) =
⎧ ⎪⎪⎪⎪ ⎨ll oo gg (( ee hh yn m+1 ax+ −µ µ)
)
y
y
= =y nm +ax
1 , where ehymax −ehn+1 ≤
⎪⎪⎪⎪
⎩h(x,y) otherwise.
µ≤ehymax. Then,wecanlowerboundtheconditionalregretofL RL2Dbyusing∆C LRL2D,H(h,x)≥
27MAOMOHRIZHONG
C LRL2D(h)−C∗ LRL2D(h µ)foranyehymax −ehn+1 ≤µ≤ehymax:
∆C LRL2D,H(h,x)
≥ sup (C LRL2D(h)−C∗ LRL2D(h µ))
ehymax≥µ≥ehymax−ehn+1
⎛ ⎛
ehymax
⎞
⎛ehymax +ehn+1⎞
≥ ehymax≥µ≥s eu hyp max−ehn+1⎝−q ymaxlog ⎝∑y′∈Yeh y′⎠−(p ymax −q ymax)log ⎝ ∑y′∈Yeh y′ ⎠
⎛eh y′ +ehn+1⎞
− y′∈Y,y∑ ′≠ymax(p y′ −q y′)log
⎝ ∑y′∈Yeh y′ ⎠
⎛ehn+1 +µ⎞ ⎛ehn+1 +ehymax⎞⎞
+q ymaxlog
⎝∑y′∈Yeh
y′⎠+(p
ymax
−q ymax)log
⎝ ∑y′∈Yeh y′ ⎠⎠
⎛eh y′ +ehymax −µ⎞
+ y′∈Y,y∑ ′≠ymax(p y′ −q y′)log
⎝ ∑y′∈Yeh y′ ⎠
⎛ ehn+1 +µ eh y′ +ehymax −µ⎞
= ehymax≥µ≥s eu hyp max−ehn+1⎝q ymaxlog
ehymax
+ y′∈Y,y∑ ′≠ymax(p y′ −q y′)log
eh y′ +ehn+1 ⎠
⎛ ehn+1 +µ ehymax −µ⎞
≥ ehymax≥µ≥s eu hyp max−ehn+1⎝q ymaxlog
ehymax
+ y′∈Y,y∑ ′≠ymax(p y′ −q y′)log
ehn+1
⎠
(ehymax −ehn+1 ≤µ≤ehymax)
ehn+1 +µ ehymax −µ
= ehymax≥µ≥s eu hyp max−ehn+1(q ymaxlog ehymax +(p n+1−(p ymax −q ymax))log ehn+1 ).
Bydifferentiatingwithrespecttoµ,weobtainthat
µ=
q ymaxehymax −(p n+1−(p
ymax
−q ymax))ehn+1
q ymax +(p n+1−(p ymax −q ymax))
achievesthemaximum. Pluggingitintotheexpression,wehave
∆C LRL2D,H(h,x)
≥q ymaxlog⎡ ⎢ ⎢
⎢ ⎣ehymax[q
ym[e axhy +ma (x
p
n+ +e 1h −n+ (1 p] yq mym axax
−q
ymax))]⎤ ⎥ ⎥
⎥
⎦
+(p n+1−(p ymax −q ymax))log⎡ ⎢ ⎢
⎢
⎣[ ee hh ny +m 1a [x
q
y+ me axhn ++1 (] p( np +n 1+ −1− (p( yp my am xax −−
q
yq my am xa )x )) ])⎤ ⎥ ⎥
⎥
⎦.
This can be further lower bounded by taking the minimum over h ∈ H, where the minimum is
attainedwhenehymax =ehn+1 Therefore,
∆C LRL2D,H(h,x)
2q
≥q ymaxlog[
q
ymax
+(p
n+1−y (m pa yx
max
−q
ymax))]
+(p n+1−(p
ymax
−q ymax))log[
q
ym2 ax(p +n (+ p1 n− +1(p −y (m pax ym− axq y −m qax ym)) ax))].
28REALIZABLELEARNINGTODEFER
ByapplyingPinsker’sinequality(Mohrietal.,2018,PropositionE.7),weobtain
∆C LRL2D,H(h,x)
≥[q ymax +p n+1−(p ymax −q ymax)]
2
1 q ymax 1 p n+1−(p ymax −q ymax) 1
× [∣ − ∣+∣ − ∣]
2 q ymax +p n+1−(p ymax −q ymax) 2 q ymax +p n+1−(p ymax −q ymax) 2
1 (p
ymax
−p n+1)2
≥
2q ymax +p n+1−(p ymax −q ymax)
1
≥ 2(p ymax −p n+1)2 (q ymax +p n+1−(p ymax −q ymax)≤1)
= 1 2(∆C L def,H(h,x))2 (bytheassumptionp ymax ≥p n+1 andh ymax ≤h n+1)
Thisprovestheinequality(7). Inthecasewhereg(x)=y max,wehavep
n+1
=p ymax. ByLemma10,
theconditionalregretofthedeferrallosscanbeexpressedas∆C
L
def,H(h,x)=p n+1−p h.Ifh(x)=
n+1,thenwehave∆C
L
,H(h,x)=0. Otherwise,whenh(x)≠n+1,wecanproceedinthesimilar
def
wayasabove,bydefininganewhypothesish µ suchthath
µ(x,y)=⎧ ⎪⎪⎪⎪ ⎨ll oo gg (( ee hh hn (+ x1 )+ −µ µ)
)
y
y
= =h n( +x)
1 .
⎪⎪⎪⎪
⎩h(x,y) otherwise
Then,wecanlowerboundtheconditionalregretofL RL2Dbyusing∆C LRL2D,H(h,x)≥C LRL2D(h)−
C∗ LRL2D(h µ), by applying the same derivation as above, modulo replacing y
max
with h(x). This
leadstotheinequality(7)aswell. ByTheorem9,wecompletetheproof.
29MAOMOHRIZHONG
B.4. Ψ(t)= 1 (1−tq )
q
Theorem13 AssumethatHissymmetricandcomplete. Assumethatc(x,y)=1 g(x)≠y. Then,for
allh∈Handanydistribution,thefollowingH-consistencyboundholds:
√
E
L
def(h)−E
L
def(H )+M
L
def(H )≤2 (n+1)α (E LRL2D(h)−E LRL2D(H )+M LRL2D(H )).
ProofWecanwritetheconditionalerrorofthesurrogatelossasfollows:
C LRL2D(h,x)= ∑p(x,y)L RL2D(h,x,y)
y∈Y
q
1 ⎛ ⎛ eh(x,y) ⎞ ⎞
= ∑p(x,y)c(x,y) 1−
q
y∈Y ⎝
⎝∑y′∈Yeh(x,y′)⎠
⎠
q
1 ⎛ ⎛eh(x,y)+eh(x,n+1)⎞ ⎞
+ ∑p(x,y)(1−c(x,y)) 1−
q
y∈Y ⎝ ⎝
∑y′∈Yeh(x,y′)
⎠ ⎠
q q
1 ⎛ ⎛ ehy ⎞ ⎞ 1 ⎛ ⎛ehy +ehn+1⎞ ⎞
= q y∑ ∈Yq y ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠+ q y∑ ∈Y(p y−q y) ⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠.
ByLemma10,theconditionalregretofthedeferrallosscanbeexpressedas
∆C
L
def,H(h,x)=max{p ymax,p n+1}−p h.
Next,wewillshowthattheconditionalregretofthesurrogatelosscanbelowerboundedasfollows:
∆C LRL2D,H(h,x)=C LRL2D(h)−C∗ LRL2D(H )≥ 2(n1 +1)q(∆C L def,H(h,x))2 . (8)
We first consider the case where g(x) ≠ y max. Otherwise, it would be straightforward to see that
theboundholds. Inthecasewhereg(x)≠y max,wehaveq
ymax
=p ymax. Wefirstprovethatforany
hypothesis h and x ∈ X, if y max ≠ h max, then the conditional error of h can be lower bounded by
30REALIZABLELEARNINGTODEFER
⎧ ⎪⎪⎪⎪h
hmax
y =y
max
thatofh,whichsatisfiesthath(x,y)=⎨ ⎪⎪⎪⎪ ⎩h hy ymax y ot= heh rwm ia sx e.. Indeed,
q(C LRL2D(h)−C LRL2D(h))
q q
⎛ ⎛
ehymax
⎞ ⎞ ⎛
⎛ehymax +ehn+1⎞
⎞
=q ymax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠+(p ymax −q ymax) ⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠
q q
⎛ ⎛
eh
hmax ⎞ ⎞ ⎛
⎛eh
hmax
+ehn+1⎞
⎞
+q hmax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠+(p hmax −q hmax) ⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠
q q
⎛ ⎛
eh
hmax ⎞ ⎞ ⎛
⎛eh
hmax
+ehn+1⎞
⎞
−q ymax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠−(p ymax −q ymax) ⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠
q q
⎛ ⎛
ehymax
⎞ ⎞ ⎛
⎛ehymax +ehn+1⎞
⎞
−q hmax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠+(p hmax −q hmax) ⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠
⎡
⎢⎛ ⎛
ehymax ⎞q
⎞ ⎛ ⎛
eh
hmax
⎞q ⎞⎤
⎥
=(q ymax −q hmax)⎢ ⎢ ⎢ ⎣⎝1− ⎝∑y′∈Yeh y′⎠ ⎠− ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠⎥ ⎥ ⎥
⎦
⎡
⎢⎛
⎛ehymax +ehn+1⎞q
⎞ ⎛
⎛eh
hmax
+ehn+1⎞q ⎞⎤
⎥
+(p ymax −q ymax −p hmax +q hmax)⎢ ⎢ ⎢ ⎣⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠− ⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠⎥ ⎥ ⎥
⎦
⎡
⎢⎛
⎛ehymax +ehn+1⎞q
⎞ ⎛
⎛eh
hmax
+ehn+1⎞q ⎞⎤
⎥
≥(p ymax −p hmax)⎢ ⎢ ⎢ ⎣⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠− ⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠⎥ ⎥ ⎥
⎦
≥0.
Therefore, we only need to lower bound the conditional regret of hypothesis h satisfying y max =
h max. Sincec(x,y)=1 g(x)≠y,wehavep ymax ≥p n+1 =p g(x). Notethatwhen(p ymax−p n+1)(h ymax−
h n+1) > 0, we have ∆C L def,H(h,x) = max{p ymax,p n+1}−p h = 0. When h ymax −h n+1 ≤ 0, we
define a new hypothesis h µ such that h µ(x,y) =
⎧ ⎪⎪⎪⎪ ⎨l lo og g( (e eh hn ym+1 ax+ −µ µ)
)
y
y
= =y nm +ax
1 , where ehymax −
⎪⎪⎪⎪
⎩h(x,y) otherwise.
ehn+1 ≤ µ ≤ ehymax. Then, we can lower bound the conditional regret of hypothesis h by using
31MAOMOHRIZHONG
∆C LRL2D,H(h,x)≥C LRL2D(h)−C∗ LRL2D(h µ)foranyehymax −ehn+1 ≤µ≤ehymax:
∆C LRL2D,H(h,x)
≥ sup (C LRL2D(h)−C∗ LRL2D(h µ))
ehymax≥µ≥ehymax−ehn+1
q q
1 ⎛ ⎛ ⎛ ehymax ⎞ ⎞ ⎛ ⎛ehymax +ehn+1⎞ ⎞
≥ q ehymax≥µ≥s eu hyp max−ehn+1⎝q ymax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠+(p ymax −q ymax) ⎝1− ⎝ ∑y′∈Yeh y′ ⎠ ⎠
q
⎛ ⎛eh y′ +ehn+1⎞ ⎞
+ y′∈Y,y∑ ′≠ymax(p y′ −q y′) ⎝1−
⎝ ∑y′∈Yeh y′ ⎠ ⎠
q q
⎛ ⎛ehn+1 +µ⎞ ⎞ ⎛ ⎛ehn+1 +ehymax⎞⎞
−q ymax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠−(p ymax −q ymax) ⎝1− ⎝ ∑y′∈Yeh y′ ⎠⎠
q
⎛ ⎛eh y′ +ehymax −µ⎞ ⎞⎞
− y′∈Y,y∑ ′≠ymax(p y′ −q y′) ⎝1−
⎝ ∑y′∈Yeh y′ ⎠ ⎠⎠
q q
1 ⎛ ⎛ ⎛ ehymax ⎞ ⎞ ⎛ ⎛ ehn+1 ⎞ ⎞
≥ q ehymax≥µ≥s eu hyp max−ehn+1⎝q ymax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠+ y′∈Y,y∑ ′≠ymax(p y′ −q y′) ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠
q q
⎛ ⎛ehn+1 +µ⎞ ⎞ ⎛ ⎛ehymax −µ⎞ ⎞⎞
−q ymax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠− y′∈Y,y∑ ′≠ymax(p y′ −q y′) ⎝1− ⎝∑y′∈Yeh y′ ⎠ ⎠⎠
(ehymax −ehn+1 ≤µ≤ehymax)
q q
1 ⎛ ⎛ ⎛ ehymax ⎞ ⎞ ⎛ ⎛ ehn+1 ⎞ ⎞
= q ehymax≥µ≥s eu hyp max−ehn+1⎝q ymax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠+(p n+1−(p ymax −q ymax)) ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠
q q
⎛ ⎛ehn+1 +µ⎞ ⎞ ⎛ ⎛ehymax −µ⎞ ⎞⎞
−q ymax ⎝1− ⎝∑y′∈Yeh y′⎠ ⎠−(p n+1−(p ymax −q ymax)) ⎝1− ⎝∑y′∈Yeh y′ ⎠ ⎠⎠
Bydifferentiatingwithrespecttoµ,weobtainthat
1 1
µ=
(p n+1−(p
ymax
−q ymax))q−1ehymax −(q ymax)q−1ehn+1
1 1
(q ymax)q−1 +(p n+1−(p ymax −q ymax))q−1
32REALIZABLELEARNINGTODEFER
achievesthemaximum. Pluggingitintotheexpression,wehave
∆C LRL2D,H(h,x)
q q
1⎛ ⎛ ehymax ⎞ ⎛ ehn+1 ⎞
≥ q ⎝−q ymax ⎝∑y′∈Yeh y′⎠ −(p n+1−(p ymax −q ymax)) ⎝∑y′∈Yeh y′⎠
q
⎡ ⎤
⎢ 1 ⎥
+q ymax⎢ ⎢ ⎢
⎢
⎢
⎢∑[ ye ′∈h Yym ehax y′+ [qe yqh m−1n
1
a+ x1 +]( (p pn n+ +1 1− −( (p py ym ma ax x− −q qy ym ma ax x) )) )q q− −11 1]⎥ ⎥ ⎥
⎥
⎥
⎥
⎣ ⎦
q
⎡ ⎤
⎢ 1 ⎥
+(p n+1−(p ymax −q ymax))⎢ ⎢ ⎢
⎢ ⎢ ⎢∑y′∈Yeh y′[q yq
m[ −1e
1
axhy +ma (x p+ n+e 1h −n+ (1 p] yq myq m a− x1 ax
−q ymax))q−1
1]⎥ ⎥ ⎥
⎥ ⎥
⎥
⎞
⎠
⎣ ⎦
This can be further lower bounded by taking the minimum over h ∈ H, where the minimum is
attainedwhenehn+1 =ehymax =ehy forally ∈Y. Therefore,
∆C LRL2D,H(h,x)≥ (n+2 1)q⎛ ⎜ ⎜⎡ ⎢ ⎢ ⎢ ⎢q y1 m−1 q ax +(p n+1−(p 2ymax −q ymax))1−1 q ⎤ ⎥ ⎥ ⎥ ⎥1−q − p n+1− 2p ymax⎞ ⎟ ⎟
⎝⎢ ⎥ ⎠
⎣ ⎦
(minimumisattainedwhenehn+1 =ehymax =ehy,∀y ∈Y)
1
≥ 2(n+1)q(p ymax −p n+1)2
(q
ymax
+(p n+1−(p
ymax
−q ymax))≤1andbyanalyzingtheTaylorexpansion)
= 2(n1 +1)q(∆C L def,H(h,x))2 (p ymax ≥p n+1 andh ymax ≤h n+1)
Thisprovestheinequality(8). Inthecasewhereg(x)=y max,wehavep
n+1
=p ymax. ByLemma10,
theconditionalregretofthedeferrallosscanbeexpressedas∆C
L
def,H(h,x)=p n+1−p h.Ifh(x)=
n+1,thenwehave∆C
L
,H(h,x)=0. Otherwise,whenh(x)≠n+1,wecanproceedinthesimilar
def
wayasabove,bydefininganewhypothesish µ suchthath
µ(x,y)=⎧ ⎪⎪⎪⎪ ⎨ll oo gg (( ee hh hn (+ x1 )+ −µ µ)
)
y
y
= =h n( +x)
1 .
⎪⎪⎪⎪
⎩h(x,y) otherwise
Then,wecanlowerboundtheconditionalregretofL RL2Dbyusing∆C LRL2D,H(h,x)≥C LRL2D(h)−
C∗ LRL2D(h µ), by applying the same derivation as above, modulo replacing y
max
with h(x). This
leadstotheinequality(8)aswell. ByTheorem9,wecompletetheproof.
AppendixC. ProofofTheorem8
Theorem8 Assume that there exists a zero error solution h∗ ∈ H with E ℓ0−1(h∗) = 0 and H is
closedunderscaling. Assumethatlim t→1Ψ(t)=0. Then,theminimizabilitygapofcomp-sumloss
ℓ
comp
vanishes: M ℓcomp(H )=0.
33MAOMOHRIZHONG
ProofBydefinitionandtheLebesguedominatedconvergencetheorem,wehave
eαh∗ (x,y)
M ℓcomp(H )≤E∗ ℓcomp(H )≤ αl →im +∞E [Ψ( ∑y′∈Yeαh∗(x,y′))]=0.
Thiscompletestheproof.
AppendixD. Futurework
While we presented a comprehensive study of surrogate loss functions for learning to defer, our
work focused on the standard single-expert and single-stage setting, aligning with previous work
(Mozannaretal.,2023). However,aninterestingdirectionistoextendourapproachtomulti-expert
(Vermaetal.,2023)andtwo-stagesettings(Maoetal.,2023a),whichwehaveleftforfuturework.
34