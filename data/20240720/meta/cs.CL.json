[
    {
        "title": "Latent Causal Probing: A Formal Perspective on Probing with Causal Models of Data",
        "authors": "Charles Jin",
        "links": "http://arxiv.org/abs/2407.13765v1",
        "entry_id": "http://arxiv.org/abs/2407.13765v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13765v1",
        "summary": "As language models (LMs) deliver increasing performance on a range of NLP\ntasks, probing classifiers have become an indispensable technique in the effort\nto better understand their inner workings. A typical setup involves (1)\ndefining an auxiliary task consisting of a dataset of text annotated with\nlabels, then (2) supervising small classifiers to predict the labels from the\nrepresentations of a pretrained LM as it processed the dataset. A high probing\naccuracy is interpreted as evidence that the LM has learned to perform the\nauxiliary task as an unsupervised byproduct of its original pretraining\nobjective. Despite the widespread usage of probes, however, the robust design\nand analysis of probing experiments remains a challenge. We develop a formal\nperspective on probing using structural causal models (SCM). Specifically,\ngiven an SCM which explains the distribution of tokens observed during\ntraining, we frame the central hypothesis as whether the LM has learned to\nrepresent the latent variables of the SCM. Empirically, we extend a recent\nstudy of LMs in the context of a synthetic grid-world navigation task, where\nhaving an exact model of the underlying causal structure allows us to draw\nstrong inferences from the result of probing experiments. Our techniques\nprovide robust empirical evidence for the ability of LMs to learn the latent\ncausal concepts underlying text.",
        "updated": "2024-07-18 17:59:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13765v1"
    },
    {
        "title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": "Zhuo ChenJiawei LiuHaotan LiuQikai ChengFan ZhangWei LuXiaozhong Liu",
        "links": "http://arxiv.org/abs/2407.13757v1",
        "entry_id": "http://arxiv.org/abs/2407.13757v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13757v1",
        "summary": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination\nproblems and real-time constraints of large language models, but it also\ninduces vulnerabilities against retrieval corruption attacks. Existing research\nmainly explores the unreliability of RAG in white-box and closed-domain QA\ntasks. In this paper, we aim to reveal the vulnerabilities of\nRetrieval-Enhanced Generative (RAG) models when faced with black-box attacks\nfor opinion manipulation. We explore the impact of such attacks on user\ncognition and decision-making, providing new insight to enhance the reliability\nand security of RAG models. We manipulate the ranking results of the retrieval\nmodel in RAG with instruction and use these results as data to train a\nsurrogate model. By employing adversarial retrieval attack methods to the\nsurrogate model, black-box transfer attacks on RAG are further realized.\nExperiments conducted on opinion datasets across multiple topics show that the\nproposed attack strategy can significantly alter the opinion polarity of the\ncontent generated by RAG. This demonstrates the model's vulnerability and, more\nimportantly, reveals the potential negative impact on user cognition and\ndecision-making, making it easier to mislead users into accepting incorrect or\nbiased information.",
        "updated": "2024-07-18 17:55:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13757v1"
    },
    {
        "title": "LLMs as Function Approximators: Terminology, Taxonomy, and Questions for Evaluation",
        "authors": "David Schlangen",
        "links": "http://arxiv.org/abs/2407.13744v1",
        "entry_id": "http://arxiv.org/abs/2407.13744v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13744v1",
        "summary": "Natural Language Processing has moved rather quickly from modelling specific\ntasks to taking more general pre-trained models and fine-tuning them for\nspecific tasks, to a point where we now have what appear to be inherently\ngeneralist models. This paper argues that the resultant loss of clarity on what\nthese models model leads to metaphors like \"artificial general intelligences\"\nthat are not helpful for evaluating their strengths and weaknesses. The\nproposal is to see their generality, and their potential value, in their\nability to approximate specialist function, based on a natural language\nspecification. This framing brings to the fore questions of the quality of the\napproximation, but beyond that, also questions of discoverability, stability,\nand protectability of these functions. As the paper will show, this framing\nhence brings together in one conceptual framework various aspects of\nevaluation, both from a practical and a theoretical perspective, as well as\nquestions often relegated to a secondary status (such as \"prompt injection\" and\n\"jailbreaking\").",
        "updated": "2024-07-18 17:49:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13744v1"
    },
    {
        "title": "Scaling Granite Code Models to 128K Context",
        "authors": "Matt StalloneVaibhav SaxenaLeonid KarlinskyBridget McGinnTim BulaMayank MishraAdriana Meza SoriaGaoyuan ZhangAditya PrasadYikang ShenSaptha SurendranShanmukha GuttulaHima PatelParameswaran SelvamXuan-Hong DangYan KoyfmanAtin SoodRogerio FerisNirmit DesaiDavid D. CoxRuchir PuriRameswar Panda",
        "links": "http://arxiv.org/abs/2407.13739v1",
        "entry_id": "http://arxiv.org/abs/2407.13739v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13739v1",
        "summary": "This paper introduces long-context Granite code models that support effective\ncontext windows of up to 128K tokens. Our solution for scaling context length\nof Granite 3B/8B code models from 2K/4K to 128K consists of a light-weight\ncontinual pretraining by gradually increasing its RoPE base frequency with\nrepository-level file packing and length-upsampled long-context data.\nAdditionally, we also release instruction-tuned models with long-context\nsupport which are derived by further finetuning the long context base models on\na mix of permissively licensed short and long-context instruction-response\npairs. While comparing to the original short-context Granite code models, our\nlong-context models achieve significant improvements on long-context tasks\nwithout any noticeable performance degradation on regular code completion\nbenchmarks (e.g., HumanEval). We release all our long-context Granite code\nmodels under an Apache 2.0 license for both research and commercial use.",
        "updated": "2024-07-18 17:46:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13739v1"
    },
    {
        "title": "Baba Is AI: Break the Rules to Beat the Benchmark",
        "authors": "Nathan CloosMeagan JensMichelangelo NaimYen-Ling KuoIgnacio CasesAndrei BarbuChristopher J. Cueva",
        "links": "http://arxiv.org/abs/2407.13729v1",
        "entry_id": "http://arxiv.org/abs/2407.13729v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13729v1",
        "summary": "Humans solve problems by following existing rules and procedures, and also by\nleaps of creativity to redefine those rules and objectives. To probe these\nabilities, we developed a new benchmark based on the game Baba Is You where an\nagent manipulates both objects in the environment and rules, represented by\nmovable tiles with words written on them, to reach a specified goal and win the\ngame. We test three state-of-the-art multi-modal large language models (OpenAI\nGPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail\ndramatically when generalization requires that the rules of the game must be\nmanipulated and combined.",
        "updated": "2024-07-18 17:30:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13729v1"
    }
]