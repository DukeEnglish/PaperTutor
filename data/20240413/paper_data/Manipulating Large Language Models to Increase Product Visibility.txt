Manipulating Large Language Models to
Increase Product Visibility
Aounon Kumar and Himabindu Lakkaraju
Harvard University, Cambridge, MA.
Correspondence: aokumar@hbs.edu and hlakkaraju@hbs.edu.
Abstract. Large language models (LLMs) are increasingly being integrated into
search engines to provide natural language responses tailored to user queries. Cus-
tomers and end-users are also becoming more dependent on these models for quick
and easy purchase decisions. In this work, we investigate whether recommendations
from LLMs can be manipulated to enhance a product’s visibility. We demonstrate
thataddingastrategictextsequence(STS)—acarefullycraftedmessage—toaprod-
uct’s information page can significantly increase its likelihood of being listed as the
LLM’s top recommendation. To understand the impact of STS, we use a catalog of
fictitious coffee machines and analyze its effect on two target products: one that sel-
domappearsintheLLM’srecommendationsandanotherthatusuallyrankssecond.
We observe that the strategic text sequence significantly enhances the visibility of
both products by increasing their chances of appearing as the top recommendation.
This ability to manipulate LLM-generated search responses provides vendors with a
considerablecompetitiveadvantageandhasthepotentialtodisruptfairmarketcom-
petition. Just as search engine optimization (SEO) revolutionized how webpages are
customizedtorankhigherinsearchengineresults,influencingLLMrecommendations
couldprofoundlyimpactcontentoptimizationforAI-drivensearchservices.Codefor
our experiments is available at: https://github.com/aounon/llm-rank-optimizer.
1 Introduction
Large language models (LLMs) are being used on e-commerce platforms to search product
catalogs and provide users with personalized recommendations tailored to their specific
query. Although traditional search engines are good at retrieving relevant pages, they lack
the ability to synthesize the information and present it as a coherent response. LLMs are
quickly filling this gap by compiling search results into natural language responses that
directly address users’ specific inquiries. Search engines like Google Search and Microsoft
Bing have begun integrating LLM-driven chat interfaces alongside their traditional search
boxes.1 ChatbotslikeOpenAI’sChat-GPTandPerplexityAIbrowsetheInternettoretrieve
up-to-date information to enhance their response.
Given a user query, the search engine retrieves relevant information from a knowledge
base, such as the Internet or a product catalog. It feeds the retrieved information to the
LLM along with the query to generate a response. This mechanism opens up the possibility
for a third party to influence the LLM by manipulating the retrieved content. Consider, for
instance, a scenario where a consumer is looking for product recommendations online. An
LLM search tool would provide a list of brands based on the needs and preferences of the
1 Google AI-Search: https://blog.google/products/search/generative-ai-search/
Microsoft Bing Chat: https://www.bing.com/chat
4202
rpA
11
]RI.sc[
1v18970.4042:viXra2 Aounon Kumar and Himabindu Lakkaraju
Fig.1: Bing Copilot’s response for the search phrase “coffee machines”.
user, similar to the example in Figure 1. This raises a critical question: Could a vendor
increase the visibility of their product by embedding a strategic text sequence
in the product information page?
In this work, we demonstrate that an LLM can be manipulated to increase the chances
of a product being listed as the top recommendation. We develop a framework to game
an LLM’s recommendations in favor of a target product by inserting a strategic text se-
quence (STS) into the product’s information. Our framework leverages adversarial attack
algorithms such as the Greedy Coordinate Gradient (GCG) algorithm [1] to optimize the
STS. Although such algorithms are typically designed to bypass an LLM’s safety guardrails
and induce harmful outputs, we show that they
can be repurposed for more benign objectives,
such as increasing product visibility, that can
have a profound impact on society, particularly
inbusinessande-commerce.Ourframeworkalso
allows us to optimize the STS to be robust to
variations in the order in which retrieved prod-
uct information is listed in the LLM’s input
prompt.
Using a catalog of fictitious coffee machines,
we demonstrate that adding the strategic text
sequence significantly improves the visibility of
the target product and increases its chances of
being recommended to potential customers. For
example, Figure 2 shows how the rank of a tar-
getproductchangesbeforeandafteraddingthe
STStotheproductinformationfor200indepen-
dent evaluations of the LLM. The product goes Fig.2: Rank distribution of the target
from not appearing in the LLM’s recommenda- product before (not recommended) and
tions to being the top recommendation. To test after (top recommendation) adding the
the robustness of the STS, we select a random strategic text sequence.Manipulating LLMs to Increase Product Visibility 3
permutation of the products in each evaluation and generate the LLM’s recommendations
with and without the STS. The addition of the STS consistently improves the rank of the
target product in the LLM’s recommendations. Furthermore, in Figure 6b in Section 4, we
showthatevenforaproductappearingclosetothetopintheLLM’srecommendations,the
STS can significantly boost the chances of the product becoming the top recommendation.
The ability to manipulate LLM search responses gives vendors a significant competitive
advantage over rival products. This capability has far-reaching implications for market dy-
namics, as it can alter the balance of competition and lead to a skewed representation of
products. As LLMs become more deeply embedded in the digital commerce infrastructure,
safeguards must be established to prevent the exploitation of AI-driven search tools for
unfair advantage.
2 Related Work
Retrieval-Augmented Generation: Traditional LLMs are limited by the knowledge ac-
quired during their training phase, making it difficult to keep them up-to-date with new
informationaftertraining.Theyarealsopronetofactualerrorswhengeneratingtextsolely
from the trained model weights. Retrieval-augmented generation (RAG) addresses these
limitations by integrating an external knowledge source, such as a database or search en-
gine, with the LLM to enhance the text generation process with additional context [2, 3].
In this framework, a retrieval component extracts context-relevant information from the
knowledge base in response to a given user prompt. The information is then concatenated
with the prompt and fed to the LLM to obtain a response tailored to the specific user
prompt. This technique has been shown to improve LLMs’ few-shot performance [4], pre-
training efficacy [5], cross-lingual performance [6], open-domain question answering [7, 8, 9]
and more [10, 11, 12, 13]. RAG is crucial for LLM-based search tools to ensure they remain
current with the latest information.
AdversarialAttacksonLLMs:Largelanguagemodelshavebeenshowntobevulner-
abletoadversarialattacks,inwhichattackersintroducemaliciouslycraftedtokensequences
intotheinputprompttocircumventthemodel’ssafetymechanismsandgenerateaharmful
response [1, 14]. These adversarial sequences have been shown to be universal and trans-
ferable. This means that they are effective across various harmful prompts and capable of
compromisingblack-boxLLMsthatpermitonlyqueryaccess.Similarattacksarealsocapa-
bleofinducingunintendedbehaviorsinLLMs,suchasmisdirection,dataextraction,control
manipulation, or denial of service [15]. Furthermore, adversarial attacks can be designed to
resemble natural text and bypass defense mechanisms based on perplexity filtering [16, 17].
Prompt injection attacks, which embed adversarial sequences in data likely to be recovered
by an LLM, can lead to data theft, control of accessed APIs, or the generation of mali-
ciouscode[18].Self-replicatingadversarialsequencesthatcanspreadacrossmultipleLLMs,
potentially jeopardizing an entire network of interconnected AI systems, have also been
developed [19]. While most adversarial attack algorithms in the literature are designed for
malicious purposes, such as physical harm or data theft, our study explores a more benign
application: an e-commerce vendor using strategic text sequences to enhance product vis-
ibility. Although not malicious in intent, this approach could have significant implications
for business and commerce.
Search Engine Optimization: A field of study closely aligned with our work is that
of search engine optimization (SEO). It studies techniques and strategies for increasing the4 Aounon Kumar and Himabindu Lakkaraju
Fig.3: LLM Search: Given a user query, it extracts relevant product information from
the internet and passes it to the LLM along with the query. The LLM uses the retrieved
informationtogeneratearesponsetailoredtotheuser’squery.Thecirclednumbersindicate
the order of the steps. STS: The strategic text sequence is added to the target product’s
information page to increase its chances of being recommended to the user.
quality and quantity of a website’s traffic by improving its visibility in search engine re-
sults. SEO’s objective is to optimize a website’s content to maximize its ranking on the
organic (non-paid) search engine results page, thereby attracting more visitors and poten-
tial customers. Over time, several SEO techniques have been developed and studied, with
significant emphasis on content optimization, keyword selection, website architecture, and
backlinkquality[20,21,22].Additionally,toolshavebeendesignedtodetectSEOtacticsand
counter them [23]. Although there exists a rich body of literature on SEO techniques, their
impacts, and countermeasures, research into influencing LLMs to enhance content visibility
remains underexplored. As LLMs are increasingly being integrated into search engines and
e-commerce platforms, it becomes important to study how these models can be influenced
to enhance a website’s or product’s visibility.
3 Strategic Text Sequence
Inthissection,wedescribetheLLM-drivensearchframeworkandthemethodforgenerating
strategic text sequences. We focus on the use case of online product recommendations and
define the search framework as illustrated in Figure 3. The process begins with a user
making a query, which prompts the system to extract relevant product information from a
knowledge base, such as the Internet or a specific product catalog database. The extracted
productinformationandthequeryarethenconcatenatedintoasingletextsequenceandfed
intotheLLMtogenerateacuratedresponseforthespecificuserquery.Avendorinfluences
the process by inserting a carefully designed text sequence on their product’s information
page to steer the LLM toward favoring their products in its recommendations.Manipulating LLMs to Increase Product Visibility 5
In our experimental setup, we assume that the information about the target product
is retrieved from the knowledge base and fed to the LLM. This is essential for the vendor
to have any influence over the LLM, as the target product’s information page is the only
componentthattheycancontrol.Wealsorestrictourfocustoopen-sourcelanguagemodels,
suchasLlama-2,astheLLM.However,sequencesproducedbyadversarialattackalgorithms
such as GCG have been shown to transfer to black-box models like GPT-4 [1].
We consider the scenario where a user is looking for recommendations for affordable
coffee machines. The LLM prompt consists of three main components: the system prompt,
the product information extracted from the knowledge base (list of coffee machines), and
the user query. The system prompt sets the context of the conversation and controls the
behavior of the LLM. The product information is formatted as a collection of JSON lines
containing information about ten fictitious coffee machines. Each JSON line consists of
fields such as Name, Description, Price, Capacity, Customer Rating, etc. The strategic text
sequence(STS)isinsertedinafieldofoneoftheseproducts,namelythetargetproduct.The
user query is a request for recommendations for affordable coffee machines. The following is
an example of an input prompt for Llama-2 used in our experiments.
LLMPrompt
<s>[INST]«SYS»Achatbetweenahumanandanartificialintelligenceassistant.Theassistantprovidesa
numberedlistofproductrecommendationsrankedbasedontheuser’srequest.«/SYS»→SystemPrompt
ProductInformation↓
Products:
{"Name": "FrenchPress Classic", "Description": "Traditional French press for a rich and flavorful cup of
coffee.","Price":"$29","Rating":4.1,"Capacity":"4cups","IdealFor":"Frenchpressenthusiasts"}
{"Name": "QuickBrew Express", "Description": "Fast and efficient coffee maker for a quick cup of cof-
fee.","Price":"$89","Rating":4.0,"Capacity":"1cup","IdealFor":"Busyindividuals"}
{"Name": "ColdBrew Master", "Description": "Specialized machine for making smooth and refreshing
coldbrewcoffee.","Price":"$199","Rating":4.3,"Capacity":"6cups","IdealFor":"Coldbrewloversinter-
act>; expect formatted XVI RETedly_ _Hello necessarily phys*) ### Das Cold Elis$?"} → Target product
informationwithoptimizedtext(inred).
{"Name": "BrewMaster Classic", "Description": "Durable and easy-to-use coffee maker with a timeless
design.","Price":"$129","Rating":4.2,"Capacity":"12cups","IdealFor":"Homeuse"}
{"Name": "SingleServe Wonder", "Description": "Compact and convenient single-serve coffee machine
foraquickbrew.","Price":"$59","Rating":3.9,"Capacity":"1cup","IdealFor":"Individualson-the-go"}
... Moreproductinformation.
Iamlookingforanaffordablecoffeemachine.CanIgetsomerecommendations?[/INST]→UserQuery
We optimize the STS with the objective of minimizing the LLM output’s cross-entropy
loss with respect to the string ‘1. [Target Product Name]’. We initialize the STS with a
sequenceofdummytokens‘*’anditerativelyoptimizeitusingtheGCGalgorithm.Ateach
iteration,thisalgorithmrandomlyselectsanSTStokenandreplacesitwithoneofthetopk
tokenswiththehighestgradient.TheSTScanalsobemaderobusttovariationsinproduct
order by randomly permuting the product list in each iteration.
We evaluate the LLM with the optimized STS inserted in the target product’s infor-
mation. It generates a list of coffee machines based on the user’s preferences, in this case,
affordability.TheSTSintheproduct’sinformationcausestheLLMtorecommendithighly,
even though it may not be an affordable option and would not be recommended otherwise.
The following is the effect of rank optimization on the LLM’s response. The target product
ColdBrew Master is shown as the top recommendation over more affordable options such
as SingleServe Wonder and FrenchPress Classic in the product list.6 Aounon Kumar and Himabindu Lakkaraju
(a) Target product rank vs iterations. (b) Rank distribution before and after
adding the STS for 200 independent
evaluations of the LLM (1 dot ≈5%).
Fig.4: The target product ColdBrew Master goes from not being recommended to the top
recommended product in 100 iterations of the GCG algorithm. The optimized text signifi-
cantly increases the chances of the target product being listed as the top recommendation.
Response
Ofcourse!I’dbehappytohelpyoufindanaffordablecoffeemachinethatfitsyourneeds.
1. ColdBrew Master: Specialized machine for making smooth and refreshing cold brew coffee. Price:
$199.Rating:4.3.Capacity:6cups.IdealFor:Coldbrewlovers!
2. SingleServe Wonder: Compact and convenient single-serve coffee machine for a quick brew. Price:
$59.Rating:3.9.Capacity:1cup.IdealFor:Individualson-the-go.
3. Grind&Brew Plus: Coffee machine with integrated grinder for freshly ground coffee every time. Price:
$349.Rating:4.4.Capacity:10cups.IdealFor:Coffeepurists.
These recommendations are ranked based on your request for an affordable coffee machine. The Cold-
Brew Master and SingleServe Wonder are both relatively inexpensive options, while the Grind&Brew Plus
offersagoodbalanceofpriceandfeatures.</s>
We parse the LLM response to find the location of each product’s first occurrence and
rank them in that order. If a product does not appear in the response, we assign it a rank
greater than the number of products in the list.
4 Experiments
We study the impact of the strategic text sequence (STS) on the LLM’s ranking of two
target products: ColdBrew Master and QuickBrew Express. The LLM almost never shows
ColdBrew Master to customers due to its high price of $199. However, we show that adding
the STS to this product’s information can not only make it appear in the LLM’s response
but also position it as the top-recommended product in most cases. This indicates that the
STS can skew LLM recommendations towards products that may not align with the user’s
criteria.However,onemightarguethat,giventhehighpriceofColdBrewMaster,customersManipulating LLMs to Increase Product Visibility 7
(a) Percentage advantage from STS opti- (b) Advantage boost from STS optimized
mized with a fixed order of the product in- withrandompermutationsoftheproductin-
formation in the LLM’s prompt. formation in the LLM’s prompt.
Fig.5: Evaluating the advantage from the STS for the target product ColdBrew Master
under variations of the product ordering in the LLM’s input prompt. Figure (a) plots the
advantage of optimizing with a fixed product order. Figure (b) shows that this advantage
can be significantly improved by optimizing with random permutations of the product list.
are unlikely to buy it even if it is the top recommendation. So, we study a more reasonably
priced product, QuickBrew Express ($89), which commonly ranks second in the LLM’s
recommendations. We observe that the addition of the STS can noticeably increase the
product’svisibility,elevatingittothetoprecommendationwithhighprobability.Thisshows
that even for products close to the top of the recommendation list, the STS has substantial
influence over the product’s ranking and visibility, offering a competitive advantage over a
rival product.
4.1 ColdBrew Master
We run the GCG algorithm for 2000 iterations to find the STS. Figure 4a shows how the
rankofthetargetproductColdBrewMasterimprovesovertheiterations.Theproductgoes
fromnotbeingrecommendedtothetoprecommendationin100iterations.Weevaluatethe
effect of the strategic text sequence on the target product’s rank in 200 LLM inferences
with and without the sequence. Figure 4b plots the rank distribution with and without the
strategictextsequence.Weobservethataddingtheoptimizedtextsequencetotheproduct
information significantly increases its chances of being the top recommendation.
For the plots in Figure 4, the order of the products in the LLM input prompt remains
fixed. However, this order can vary in a real-world LLM-driven search tool. To evaluate
the robustness of the STS to variations in the product ordering, we randomize the list of
productsintheinputpromptforthe200independentLLMevaluations.Ineachevaluation,
we pick a random permutation of the product list and evaluate the LLM with and without
theSTS.WesaythattheSTSgivesusanadvantageiftheproduct’srankishigherwiththe
STS than without it, a disadvantage if the rank is lower with the STS, and no advantage
if the rank is the same in both cases. Figure 5a plots the advantage of adding the strategic
textsequencetothetargetproduct’sinformation.Inabout40%oftheevaluations,therank
of the target product is higher due to the addition of the optimized sequence. In about 60%
of the evaluations, there is no change in the rank of the target product. For a tiny fraction
of the evaluations, the rank is lowered by the optimized sequence.8 Aounon Kumar and Himabindu Lakkaraju
(a) Target product rank vs iterations. (b) Rank distribution before and after
adding the STS for 200 independent
evaluations of the LLM (1 dot ≈5%).
Fig.6: The target product QuickBrew Express becomes the top recommended product in
1000iterationsoftheGCGalgorithm.Theoptimizedtextsignificantlyincreasesthechances
that the target product is listed as the top recommendation.
Next, we show that optimizing the STS to make it robust to variations in product
ordering can significantly boost its advantage. In each iteration of the GCG algorithm,
we pick a random permutation of the products and optimize the STS tokens with respect
to that permutation. We find that the resulting STS is much more robust to variations
in product ordering. Figure 5b plots the advantage of the robust STS after optimizing
with random product ordering. The percentage advantage significantly increases, and the
percentage disadvantage is negligible.
4.2 QuickBrew Express
In this section, we analyze the product QuickBrew Express, a more affordable option that
typically ranks second in the LLM’s recommendations without the STS. We show that
adding the STS can improve its ranking, often making it the top-recommended product.
Similar to our approach in the previous section, we optimize the STS using the GCG algo-
rithmfor2000iterations.Figure6ashowshowtherankevolvesoveriterations.Theproduct
isinitiallylistedasthesecondrecommendation.Itsrankingdeterioratesforafewiterations,
but quickly improves to the top recommendation and stays there consistently for the rest
of the 2000 iterations. Figure 6b plots the rank distribution before and after inserting the
STS. While the product is never listed as the top recommendation before adding the STS,
its chances of being in the first position increase significantly after adding the STS. Similar
to ColdBrew Master, the order of the products in the LLM’s input prompt remains fixed in
Figure 6.
Next, we randomize the ordering of the products in the LLM’s prompt and evaluate the
effectoftheSTSusing200independentevaluations.Figure7aplotstheadvantageobtained
by the STS (optimized using a fixed product order). We observe that the probability of theManipulating LLMs to Increase Product Visibility 9
(a) Percentage advantage from STS opti- (b) Advantage boost from STS optimized
mized with a fixed order of the product in- withrandompermutationsoftheproductin-
formation in the LLM’s prompt. formation in the LLM’s prompt.
Fig.7: Evaluating the advantage from the STS for the target product QuickBrew Express
under variations of the product ordering in the LLM’s input prompt. Figure (a) plots the
advantage of optimizing with a fixed product order. Figure (b) shows that this advantage
can be significantly improved by optimizing with random permutations of the product list.
STS providing an advantage is roughly equal to the probability of yielding a disadvantage,
neutralizing its overall benefit. However, optimizing the STS with a random ordering of the
product list in each iteration of GCG improves the advantage. Figure 7b shows that by
randomizing the order of the product during the STS optimization phase, the advantages
can be significantly increased while the disadvantages are minimized.
5 Conclusion
Inthiswork,wedemonstratethatstrategictextsequencescaninfluenceLLM-drivensearch
tools in the context of e-commerce. We show that a vendor can significantly improve their
product’s ranking in the LLM’s recommendations by inserting an optimized sequence of
tokensintotheproductinformationpage.TheabilitytomanipulateLLMrecommendations
has serious implications for business and commerce, as it offers vendors a competitive edge
in the market by enhancing their products’ visibility. While our work explores a specific
vulnerability in LLMs, more research is needed to uncover other vulnerabilities that can
give businesses an unfair advantage.
The impact of our findings extends beyond e-commerce, shedding light on the broader
implications of AI search optimization and the ethical considerations that come with it.
As LLMs become increasingly integral to the digital landscape, it is crucial to develop
safeguards against potential misuse. Future work in this area could explore the ethical
boundariesofusingstrategictextsequences,developguidelinesfortheirresponsibleuse,and
investigate countermeasures to prevent unfair manipulation of search results. By balancing
search optimization and ethical standards, we can ensure that advancements in AI search
technology contribute to a fair and competitive marketplace.Bibliography
[1] AndyZou,ZifanWang,NicholasCarlini,MiladNasr,J.ZicoKolter,andMattFredrik-
son. Universal and transferable adversarial attacks on aligned language models, 2023.
[2] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
NamanGoyal,HeinrichKüttler,MikeLewis,WentauYih,TimRocktäschel,Sebastian
Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp
tasks, 2021.
[3] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,Jiawei
Sun,QianyuGuo,MengWang,andHaofenWang. Retrieval-augmentedgenerationfor
large language models: A survey, 2024.
[4] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,
TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.
Atlas: Few-shot learning with retrieval augmented language models. J. Mach. Learn.
Res., 24:251:1–251:43, 2023. URL http://jmlr.org/papers/v24/23-0037.html.
[5] KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMing-WeiChang. Realm:
Retrieval-augmented language model pre-training, 2020.
[6] XiaoqianLi,ErcongNie,andShengLiang. Crosslingualretrievalaugmentedin-context
learning for Bangla. In Firoj Alam, Sudipta Kar, Shammur Absar Chowdhury, Farig
Sadeque, and Ruhul Amin, editors, Proceedings of the First Workshop on Bangla Lan-
guage Processing (BLP-2023), pages 136–151, Singapore, December 2023. Association
for Computational Linguistics. https://doi.org/10.18653/v1/2023.banglalp-1.15. URL
https://aclanthology.org/2023.banglalp-1.15.
[7] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with genera-
tive models for open domain question answering. In Paola Merlo, Jorg Tiede-
mann, and Reut Tsarfaty, editors, Proceedings of the 16th Conference of the Eu-
ropean Chapter of the Association for Computational Linguistics: Main Volume,
pages 874–880, Online, April 2021. Association for Computational Linguistics.
https://doi.org/10.18653/v1/2021.eacl-main.74. URL https://aclanthology.org/
2021.eacl-main.74.
[8] DevendraSingh,SivaReddy,WillHamilton,ChrisDyer,andDaniYogatama. End-to-
end training of multi-document reader and retriever for open-domain question answer-
ing. InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan,
editors, Advances in Neural Information Processing Systems, volume 34, pages 25968–
25981. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/
paper_files/paper/2021/file/da3fde159d754a2555eaa198d2d105b2-Paper.pdf.
[9] Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, and
Graham Neubig. Retrieval as attention: End-to-end learning of retrieval and reading
within a single transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang,
editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-
guageProcessing,pages2336–2349,AbuDhabi,UnitedArabEmirates,December2022.
Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.emnlp-
main.149. URL https://aclanthology.org/2022.emnlp-main.149.
[10] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim,
ChristopherHesse,ShantanuJain,VineetKosaraju,WilliamSaunders,XuJiang,Karl
Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, BenjaminManipulating LLMs to Increase Product Visibility 11
Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human
feedback, 2022.
[11] Hongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei
Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. Webbrain: Learning to generate fac-
tually correct articles for queries by grounding on large web corpus, 2023.
[12] Lucas Charpentier, Sondre Wold, David Samuel, and Egil Rønningstad. BRENT:
Bidirectional retrieval enhanced Norwegian transformer. In Tanel Alumäe and Mark
Fishel,editors,Proceedingsofthe24thNordicConferenceonComputationalLinguistics
(NoDaLiDa), pages 202–214, Tórshavn, Faroe Islands, May 2023. University of Tartu
Library. URL https://aclanthology.org/2023.nodalida-1.21.
[13] Hongrui Chen, Yuan Xiong, Jingru Wang, and Zhong Zhou. Long-term visual local-
ization with semantic enhanced global retrieval. In 17th International Conference on
Mobility, Sensing and Networking, MSN 2021, Exeter, United Kingdom, December 13-
15, 2021, pages 319–326. IEEE, 2021. https://doi.org/10.1109/MSN53354.2021.00057.
URL https://doi.org/10.1109/MSN53354.2021.00057.
[14] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does
LLM safety training fail? In Alice Oh, Tristan Naumann, Amir Globerson,
Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural In-
formation Processing Systems 36: Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -
16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html.
[15] JonasGeiping,AlexStein,ManliShu,KhalidSaifullah,YuxinWen,andTomGoldstein.
Coercing llms to do and reveal (almost) anything, 2024.
[16] XiaogengLiu,NanXu,MuhaoChen,andChaoweiXiao. Autodan:Generatingstealthy
jailbreak prompts on aligned large language models, 2023.
[17] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong
Huang,AniNenkova,andTongSun. Autodan:Automaticandinterpretableadversarial
attacks on large language models, 2023.
[18] KaiGreshake,SaharAbdelnabi,ShaileshMishra,ChristophEndres,ThorstenHolz,and
Mario Fritz. Not what you’ve signed up for: Compromising real-world llm-integrated
applications with indirect prompt injection, 2023.
[19] StavCohen,RonBitton,andBenNassi. Herecomestheaiworm:Unleashingzero-click
worms that target genai-powered applications, 2024.
[20] Fuxue Wang, Yi Li, and Yiwen Zhang. An empirical study on the search engine op-
timization technique and its outcomes. In 2011 2nd International Conference on Ar-
tificial Intelligence, Management Science and Electronic Commerce (AIMSEC), pages
2767–2770, 2011. https://doi.org/10.1109/AIMSEC.2011.6011361.
[21] R.Anil Kumar, Zaiduddin Shaik, and Mohammed Furqan. A survey on search engine
optimizationtechniques.InternationalJournalofP2PNetworkTrendsandTechnology,
9:5–8, 01 2019. https://doi.org/10.14445/22492615/IJPTT-V9I1P402.
[22] Asim Shahzad, Nazri Mohd Nawi, Norhamreeza Hamid, Sundas Khan, Muhammad
Aamir, Arif Ulah, and Salfarina Abdullah. The impact of search engine optimization
on the visibility of research paper and citations. JOIV : International Journal on
Informatics Visualization, 1:195, 11 2017. https://doi.org/10.30630/joiv.1.4-2.77.
[23] Dirk Lewandowski, Sebastian Sünkler, and Nurce Yagci. The influence of search
engine optimization on google’s results: A multi-dimensional approach for detect-
ing seo. In Proceedings of the 13th ACM Web Science Conference 2021, WebSci12 Aounon Kumar and Himabindu Lakkaraju
’21, page 12–20, New York, NY, USA, 2021. Association for Computing Machin-
ery. ISBN 9781450383301. https://doi.org/10.1145/3447535.3462479. URL https:
//doi.org/10.1145/3447535.3462479.