Differentially Private Reinforcement Learning with Self-Play
Dan Qiao1 and Yu-Xiang Wang1
1Department of Computer Science, UC Santa Barbara
danqiao@ucsb.edu, yuxiangw@cs.ucsb.edu
Abstract
We study the problem of multi-agent reinforcementlearning (multi-agent RL) with differen-
tialprivacy(DP)constraints. Thisiswell-motivatedbyvariousreal-worldapplicationsinvolving
sensitive data,where itis criticalto protectusers’private information. We firstextendthe defi-
nitionsofJointDP(JDP)andLocalDP(LDP)totwo-playerzero-sumepisodicMarkovGames,
where both definitions ensure trajectory-wiseprivacy protection. Then we design a provably ef-
ficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type
bonuses. The algorithm is able to satisfy JDP and LDP requirements when instantiated with
appropriate privacy mechanisms. Furthermore, for both notions of DP, our regretbound gener-
alizesthebestknownresultunderthesingle-agentRLcase,whileourregretcouldalsoreduceto
the bestknownresultformulti-agentRLwithoutprivacyconstraints. Tothebestofourknowl-
edge,these arethe firstline ofresultstowardsunderstanding trajectory-wiseprivacy protection
in multi-agent RL.
1
4202
rpA
11
]GL.sc[
1v95570.4042:viXraContents
1 Introduction 3
1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Problem Setup 5
2.1 Markov Games and Regret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Differential Privacy in Multi-agent RL . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3 Algorithm 8
4 Main results 11
5 Privatizers for JDP and LDP 11
5.1 Central Privatizer for Joint DP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
5.2 Local Privatizer for Local DP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.3 The post-processing step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.4 Some discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
6 Proof overview 15
7 Conclusion 16
A Extended related works 22
B Proof of main theorems 22
B.1 Properties of private estimations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2 Proof of UCB and LCB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.3 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.4 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C Missing proof in Section 5 31
21 Introduction
This paper considers the problem of multi-agent reinforcement learning (multi-agent RL), wherein
several agents simultaneously make decisions in an unfamiliar environment with the goal of maxi-
mizing their individual cumulative rewards. Multi-agent RL has been deployed not only in large-
scale strategy games like Go [Silver et al., 2017], Poker [Brown and Sandholm, 2019] and MOBA
games [Ye et al., 2020], but also in various real-world applications such as autonomous driving
[Shalev-Shwartz et al., 2016], negotiation [Bachrach et al., 2020], and trading in financial markets
[Shavandi and Khedmati, 2022]. In these applications, the learning agent analyzes users’ private
feedback in order to refine its performance, where the data from users usually contain sensitive
information. Take autonomous driving as an instance, here a trajectory describes the interaction
between the cars in a neighborhood duringa fixed time window. At each timestamp, given the cur-
rent situation of each car, the system (central agent) will send a command for each car to take (e.g.
speed up, pull over), and finally the system gathers the feedback from each car (e.g. whether the
driving is safe, whether the customer feels comfortable) and enhances its policy. Here, (situation,
command, feedback) corresponds to (state, action, reward) in a Markov Game where the state and
reward of each user are considered as sensitive information. Therefore, leakage of such information
is not acceptable. Regrettably, it has been demonstrated that without the implementation of pri-
vacy safeguards, learning agents tend to inadvertently memorize details from individual training
data points [Carlini et al., 2019], regardless of their relevance to the learning process [Brown et al.,
2021]. This susceptibility exposes multi-agent RL agents to potential privacy threats.
To handle the above privacy issue, Differential privacy (DP) [Dwork et al., 2006] has been widely
considered. The output of a differentially private reinforcement learning algorithm cannot be dis-
cerned from its output in an alternative reality where any specific user is substituted, which effec-
tively mitigates the privacyrisks mentioned earlier. However, it is shown[Shariff and Sheffet,2018]
that standard DP will lead to linear regret even under contextual bandits. Therefore, Vietri et al.
[2020] considered a relaxed surrogate of DP: Joint Differential Privacy (JDP) [Kearns et al., 2014]
for RL. Briefly speaking, JDP protects the information about any specific user even given the
output of all other users. Meanwhile, another variant of DP: Local Differential Privacy (LDP)
[Duchi et al., 2013] has also been extended to RL by Garcelon et al. [2021] due to its stronger pri-
vacy protection. LDP requires that the raw data of each user is privatized before being sent to the
agent. Although following works [Chowdhury and Zhou, 2022, Qiao and Wang, 2023c] established
near optimal results under these two notions of DP, all of the previous works focused on the single-
agent RL setting while the solution to multi-agent RL with differential privacy is still unknown.
Therefore it is natural to question:
Question 1.1. Isit possible to design a provably efficientself-play algorithm to solve Markov games
while satisfying the constraints of differential privacy?
Our contributions. In this paper, we answer the above question affirmatively by proposing
a general algorithm for DP multi-agent RL: DP-Nash-VI (Algorithm 1). Our contributions are
threefold and summarized as below.
• We first extend the definitions of Joint DP (Definition 2.2) and Local DP (Definition 2.3) to
the multi-agent RL setting. Both notions of DP focus on protecting the sensitive information
of each trajectory, which is consistent with the counterparts under single-agent RL.
3AlgorithmsforMarkovGames Regretwithoutprivacy Regretunderǫ-JDP Regretunderǫ-LDP
DP-Nash-VI(OurAlgorithm1) O(√H2SABT) O(√H2SABT+H3S2AB/ǫ) O(√H2SABT+S2AB√H5T/ǫ)
NashVI[Liuetal.,2021] O(√H2SABT)∗ N.A. N.A.
Lowerbounds Ω( H2S(A+e B)T)[BaiandJin,2020] Ωe H2S(A+B)T+HS(A+B) Ωe H2S(A+B)T+√HS(A+B)T
e ǫ ǫ
AlgorithmsforMDPs(B=1) p Regretwithoutprivacy (cid:16)pRegretunderǫ-JDP (cid:17) (cid:18) p Regretunderǫ-LDP (cid:19)
e e
PUCB[Vietrietal.,2020] O(√H3S2AT) O(√H3S2AT+H3S2A/ǫ)⋆ N.A.
LDP-OBI[Garcelonetal.,2021] O(√H3S2AT) N.A. O(√H3S2AT+S2A√H5T/ǫ)†
Private-UCB-VI[ChowdhuryandZhou,2022] eO(√H3SAT) eO(√H3SAT+H3S2A/ǫ) O(√H3SAT+S2A√H5T/ǫ)
DP-UCBVI‡[QiaoandWang,2023c] eO(√H2SAT) O(√H2SAT+H3S2A/ǫ) eO(√H2SAT+S2A√H5T/ǫ)
e e e
Table 1: Comparison of our results (ineblue) to existing weork regarding regretewithout privacy (i.e.
the privacy budget is infinity), regret under ǫ-Joint DP and regret under ǫ-Local DP. In the above,
S is the number of states, A,B are the number of actions for both players, H is the planning
horizon and K is the number of episodes (T = HK is the number of steps). Markov decision
processes (MDPs) is a special case of Markov Games whereB = 1. : This result is the bestknown
∗
regret boundwhen there is no privacy concern. ⋆: More discussions about this bound can be found
in Chowdhury and Zhou [2022]. : The original regret bound in Garcelon et al. [2021] is derived
†
under the setting of stationary MDP, and can be directly transferred to the bound here by adding
√H to the first term. : This algorithm achieved the best known results under single-agent MDPs,
‡
and our Algorithm 1 can obtain the same regret bounds under this setting.
• We design a new algorithm DP-Nash-VI (Algorithm 1) based on optimistic Nash value it-
eration and privatization of Bernstein-type bonuses. The algorithm can be combined with
any Privatizer (for JDP or LDP) that possesses a corresponding regret bound (Theorem 4.1).
Moreover, when there is no privacy constraint (i.e. the privacy budget is infinity), our regret
reduces to the best known regret for non-private multi-agent RL.
• Under the constraint of ǫ-JDP, DP-Nash-VI achieves a regret of O(√H2SABT+H3S2AB/ǫ)
(Theorem 5.2). Compared to the regret lower bound (Theorem 5.3), the main term is nearly
optimal while the additional cost due to JDP has optimal depeendence on ǫ. Under the ǫ-
LDP constraint, DP-Nash-VI achieves a regret of O(√H2SABT+S2AB√H5T/ǫ) (Theorem
5.5), where the dependence on K,ǫ is optimal according to the lower bound (Theorem 5.6).
The pair of results strictly generalizes the best kneown results for single-agent RL with DP
[Qiao and Wang, 2023c].
1.1 Related work
Wecompareourresultswithexistingworksondifferentiallyprivatereinforcementlearning[Vietri et al.,
2020, Garcelon et al., 2021, Chowdhury and Zhou, 2022, Qiao and Wang, 2023c] and regret min-
imization under Markov Games [Liu et al., 2021] in Table 1, while more discussions about dif-
ferentially private learning algorithms are deferred to Appendix A. Notably, all existing DP RL
algorithms focus on the single-agent case. In comparison, our algorithm works for the more general
two-player setting and our results directly match the best known regret bounds [Qiao and Wang,
2023c] when applied to the single-agent setting.
Recently, several works provide non-asymptotic theoretical guarantees for learning Markov Games.
Bai and Jin[2020]developedthefirstprovably-efficientalgorithmsinMGsbasedonoptimisticvalue
iteration, and the result is improved by Liu et al. [2021] using model-based approach. Meanwhile,
model-free approaches are shown to break the curse of multiagency and improve the dependence
4on action space [Bai et al., 2020, Jin et al., 2021, Mao et al., 2022, Wang et al., 2023, Cui et al.,
2023]. However, all these algorithms base on the original data from users, and thus are vulnera-
ble to various privacy attacks. While several works [Hossain and Lee, 2023, Hossain et al., 2023,
Zhao et al.,2023b,Gohari et al.,2023]studytheprivatization ofcommunications between multiple
agents, none of them provide regret guarantees. In comparison, we design algorithms that provably
protect the sensitive information in each trajectory, while achieving near-optimal regret bounds
simultaneously.
Technically speaking, we follow the idea of optimistic Nash value iteration and privatization of
Bernstein-type bonuses. Optimistic Nash value iteration aims to construct both upper bounds
and lower bounds for value functions, which could guide the exploration. Such idea has been
applied by previous model-based approaches [Bai and Jin, 2020, Xie et al., 2020, Liu et al., 2021]
to derive tight regret bounds. To satisfy the privacy guarantees, we are required to construct the
UCB and LCB privately. In this work, we privatize the transition kernel estimate and construct
a private bonus function for our purpose. Among different bonuses, we generalize the approach in
Qiao and Wang [2023c] and directly operate on the Bernstein-type bonus, which could enable tight
regret analysis while the privatization is more technically demanding due to the variance term. To
handle this, we first privatize the visitation counts such that they satisfy several nice properties,
then we use these counts to construct private transition estimates and private bonuses. Lastly, we
manage to prove UCB and LCB, and bound the private terms by their non-private counterparts to
complete the regret analysis.
2 Problem Setup
In this paper, we consider reinforcement learning under Markov Games (MGs) [Shapley, 1953]
with the guarantee of Differential Privacy (DP) [Dwork et al., 2006]. Below we introduce MGs and
define DP under multi-agent RL.
2.1 Markov Games and Regret
Markov Games (MGs) are the generalization of Markov Decision Processes (MDPs) to the multi-
player setting, where each player aims to maximize her own reward. We consider two-player zero-
sum episodic MGs, denoted by a tuple = ( , , ,H, P H , r H ,s ), where is the
MG S A B { h }h=1 { h }h=1 1 S
state space with S = , and are the action space for the max-player (who aims to maximize
|S| A B
the total reward) and the min-player (who aims to minimize the total reward) respectively with
A = ,B = . Besides, H is the horizon while the non-stationary transition kernel P ( s,a,b)
h
|A| |B| ·|
givesthedistributionofthenextstateifaction (a,b)istakenatstatesandtimesteph. Inaddition,
we assume that the reward function r (s,a,b) [0,1] is deterministic and known1. For simplicity,
h
∈
we assume each episode starts from a fixed initial state s . Then at each time step h [H], two
1
∈
players observe s and choose their actions a and b simultaneously, after which both
h h h
∈ A ∈ B
players observe the action of their opponent and receive reward r (s ,a ,b ), the environment will
h h h h
transit to s P ( s ,a ,b ).
h+1 h h h h
∼ ·|
Markov policy, value function. A Markov policy µ of the max-player can be seen as a series
1This assumption is without loss of generality since the uncertainty of reward is dominated by that of transition
kernel.
5of mappings µ = µ H , where each µ maps each state s to a probability distribution
{ h }h=1 h ∈ S
over actions , i.e. µ : ∆( ). A Markov policy ν for the min-player is defined similarly.
h
A S → A µ,ν
Given a pair of policies (µ,ν) and time step h [H], the value function V () is defined as
∈ h ·
Vµ,ν (s) = E [ H r s = s] while the Q-value function Qµ,ν (, , ) is defined as Qµ,ν (s,a,b) =
h µ,ν t=h t | h h · · · h
E [ H r s ,a ,b = s,a,b] for all s,a,b. According to the definitions, the following Bellman
µ,ν t=h t | hPh h
equation holds:
P
µ,ν µ,ν
Q (s,a,b) = [r +P V ](s,a,b),
h h h h+1
Vµ,ν (s)= [E Qµ,ν ](s), (h,s,a,b).
h µ,ν h ∀
Best responses, Nash equilibrium. For any policy µ of the max-player, there exists a best
response policy ν†(µ) of the min-player such that Vµ,ν†(µ) (s) = inf Vµ,ν (s) for all (s,h). For
h ν h
simplicity, we denote Vµ,† := Vµ,ν†(µ) . Also, µ†(ν) and V†,ν can be defined by symmetry. It is
h h h
shown [Filar and Vrieze, 2012] that there exists a pair of policies (µ⋆,ν⋆) that are best responses
against each other, i.e.
µ⋆,† µ⋆,ν⋆ †,ν⋆
V (s)= V (s) = V (s), (s,h) [H].
h h h ∀ ∈ S ×
The pair of policies (µ⋆,ν⋆) is called the Nash equilibrium of the Markov game, which further
satisfies the following minimax property: for all (s,h) [H],
∈ S ×
µ,ν µ⋆,ν⋆ µ,ν
supinfV (s) = V (s)= infsupV (s).
µ ν h h ν µ h
The value functions of (µ⋆,ν⋆) are called Nash value functions and we denote V⋆ = Vµ⋆,ν⋆ ,Q⋆ =
h h h
µ⋆,ν⋆
Q for simplicity. Intuitively speaking, Nash equilibrium means that no player could gain more
h
from updating her own policy.
Learning objective: regret. Following previous works [Bai and Jin, 2020, Liu et al., 2021], we
aim to minimize the regret, which is defined as below:
K
†,νk µk,†
Regret(K) = V (s ) V (s ) ,
1 1 − 1 1
Xk=1h i
where K is the number of episodes the agent interacts with the environment and (µk,νk) are the
policies executed by the agent in the k-th episode. Note that any sub-linear regret bound can be
transferred to a PAC guarantee according to the standard online-to-batch conversion [Jin et al.,
2018].
2.2 Differential Privacy in Multi-agent RL
For RL with self-play, each trajectory corresponds to the interaction between a pair of users and
the environment. The interaction generally follows the protocol below. At time step h of the
k-th episode, the users send their state sk to a central agent , then sends back a pair of
h M M
actions (ak,bk) for the users to take, and finally the users send their reward rk to . Following
h h h M
previousworks[Vietri et al.,2020,Chowdhury and Zhou,2022,Qiao and Wang,2023c], herewelet
6= (u , ,u ) denote the sequence of K unique2 pairs of users who participate in the above RL
1 K
U ···
protocol. Besides, each pair of users u is characterized by the sk,rk H information they would
k { h h}h=1
respond to all (AB)H3 possible sequences of actions from the agent. Let ( ) = (ak,bk) H,K
M U { h h }h,k=1,1
denote the whole sequence of actions suggested by the agent . Then a direct adaptation of
M
differential privacy [Dwork et al., 2006] is defined below, which says that ( ) and all other pairs
M U
excluding u together will not disclose much information about user u .
k k
Definition 2.1 (Differential Privacy (DP)). For any ǫ > 0 and δ [0,1], a mechanism :
∈ M U →
( )KH is (ǫ,δ)-differentially private if for any possible user sequences and ′ that is different
A×B U U
on one pair of users and any subset E of ( )KH,
A×B
P[ ( ) E] eǫ P[ ( ′) E]+δ.
M U ∈ ≤ · M U ∈
If δ = 0, we say that is ǫ-differentially private (ǫ-DP).
M
Unfortunately, privately recommending actions to the pair of users u while protecting their own
k
stateandrewardinformationisshowntobeimpracticalevenforthesingle-playersetting. Therefore,
weconsiderarelaxedversionofDP,knownasJointDifferential Privacy (JDP)[Kearns et al.,2014].
JDP says that for all pairs of users u , the recommendation to all other pairs excluding u will not
k k
disclosethesensitiveinformationaboutu . AlthoughbeingweakerthanDP,JDPcouldstillprovide
k
meaningful privacy protection by ensuring that even if an adversary can observe the interactions
between all other users and the environment, it is statistically hard to reconstruct the interaction
between u and the environment. JDP is first studied by Vietri et al. [2020] under single-agent
k
reinforcement learning, and we extend the definition to the two-player setting.
Definition 2.2 (Joint Differential Privacy (JDP)). For any ǫ > 0, a mechanism : (
M U → A×
)KH is ǫ-joint differentially private if for any k [K], any user sequences and ′ that is
B ∈ U U
different on the k-th pair of users and any subset E of ( )(K−1)H,
A×B
P[ ( ) E] eǫ P[ ( ′) E],
−k −k
M U ∈ ≤ · M U ∈
where ( ) E means the sequence of actions sent to all pairs of users excluding u belongs to
−k k
M U ∈
set E.
In the example of autonomous driving, JDP ensures that even if an adversary observes the inter-
actions between cars within all time windows except one, it is hard to know what happens during
the specific time window. While providing strong privacy protection, JDP requires the central
agent to have access to the real trajectories from users. However, in various scenarios the users
M
are not even willing to directly share their data with the agent. To address such circumstances,
Duchi et al. [2013] developed a stronger notion of privacy named Local Differential Privacy (LDP).
Now that when considering LDP, the agent can not observe the state of users, we consider the
following protocol specific for LDP: at the beginning of the k-th episode, the agent first sends
M
a policy pair π = (µ ,ν ) to the pair of users u , after running π and getting a trajectory X ,
k k k k k k
u privatizes their trajectory to X′ and sends it back to . We present the definition of Local
k k M
DP below, which generalizes the LDP under single-agent reinforcement learning by Garcelon et al.
2Uniquenessis assumed wlog, as for a returninguser pair one can group them with theirprevious occurrences.
3At each time step h ∈ [H], the agent suggests actions to both players, and thus there are AB possibilities for
each time step h.
7[2021]. Briefly speaking, Local DP ensures that it is impractical for an adversary to reconstruct
the whole trajectory of u even if observing their whole response.
k
Definition 2.3 (Local Differential Privacy (LDP)). For any ǫ > 0, a mechanism is ǫ-local
M
differentially private if for any possible trajectories X,X′ and any possible set
E (X)X is any possible trajectory , f
⊆ {M | }
f P[ (X) E] eǫ P[ (X′) E].
M ∈ ≤ · M ∈
f f
In the example of autonomous driving, LDP ensures that the system can only observe a private
version of the interactions between cars instead of the raw data.
Remark 2.4. Note that here our definitions of JDP and LDP both provide trajectory-wise privacy
protection, which is consistent with previous works [Chowdhury and Zhou, 2022, Qiao and Wang,
2023c]. Moreover, underthe special case where the min-player plays a fixedand known deterministic
policy (or equivalently, only contains a single action and B = 1), the Markov Game setting
B
reduces to a single-agent Markov decision process while our JDP and LDP directly matches previous
definitions for the MDP setting. Therefore, our setting strictly generalizes previous works and
requires novel techniques to handle the min-player.
Remark 2.5. In the following sections we will show that LDP is consistent with sub-linear regret
bounds, while it is known that we can not derive sub-linear regret bounds under the constraint of
DP. We remark that there is no contradictory since here the RL protocols for DP and LDP are
different. As a result, here a guarantee of LDP does not directly imply a guarantee of DP and the
two notions are indeed not directly comparable.
3 Algorithm
In this part, we introduce DP-Nash-VI (Algorithm 1). Note that the algorithm takes Privatizer as
an input. We analyze the regret of Algorithm 1 for all Privatizers satisfying the Assumption 3.1
below, which includes the cases where the Privatizer is chosen as Central (for JDP) or Local (for
LDP).
We firstintroducethedefinition of visitation counts, whereNk(s,a,b) = k−11(si,ai,bi = s,a,b)
h i=1 h h h
denotes the visitation count of (s,a,b) at time step h until the beginning of the k-th episode.
Similarly, we let Nk(s,a,b,s′) = k−11(si,ai,bi,si = s,a,b,s′) bP e the visitation count of
h i=1 h h h h+1
(h,s,a,b,s′) before the k-th episode. In multi-agent RL without privacy constraints, such visi-
P
tation counts are sufficient for estimating the transition kernel P H and updating the explo-
{ h }h=1
ration policy, as in previous model-based approaches [Liu et al., 2021]. However, these counts
base on the original trajectories from the users, which could reveal sensitive information. There-
fore, with the concern of privacy, we can only incorporate these counts after a privacy-preserving
step. In other words, we use a Privatizer to transfer the original counts to the private version
Nk(s,a,b),Nk(s,a,b,s′). We make thefollowing Assumption 3.1 for Privatizer, which says that the
h h
private counts are close to real ones. Privatizers for JDP and LDP that satisfy Assumption 3.1 will
bee proposedein Section 5.
Assumption 3.1 (Private counts). For any privacy budget ǫ > 0 and failure probability β [0,1],
∈
there exists some E > 0 such that with probability at least 1 β/3, for all (h,s,a,b,s′,k)
ǫ,β
− ∈
8Algorithm 1 Differentially Private Optimistic Nash Value Iteration (DP-Nash-VI)
1: Input: Number of episodes K, privacy budget ǫ, failure probability β and a Privatizer (can be
either Central or Local).
2: Initialize: Private counts N1(s,a,b) = N1(s,a,b,s′) = 0 for all (h,s,a,b,s′). Set up the
h h
confidence bound E w.r.t the Privatizer, the minimal gap ∆ = H and universal constants
ǫ,β
C ,C > 0. ι =log(30HSABeK/β). e
1 2
3: for k = 1,2, ,K do
4: Vk () =· V·· k () = 0.
H+1 · H+1 ·
5: for h = H,H 1, ,1 do
− ···
6: for (s,a,b) do
∈ S ×A×B
7: Compute private transition kernel Pk( s,a,b) as in (1).
h ·|
8: Compute γk(s,a,b) = C1 Pk(Vk Vk )(s,a,b).
h H · h h+1 e− h+1
9: Compute private bonus Γk
he
(s,a,b) = C 2v
uVar Pe hk(·|s,a,b)
"
N
k(V sk h ,a+ ,1 b)+ 2Vk h+1 !(·) #·ι
+ C N2H k(S sE ,aǫ ,, bβ )·ι +
u h h
C2H2Sι . t
Nk(s,a,b) e e
h
10: U eCB Qk h(s,a,b) = min {r h(s,a,b) + s′P hk(s′ |s,a,b)
·
Vk h+1(s′) + γ hk(s,a,b) +
Γk(s,a,b),H .
h } P
11: LCBQk h(s,a,b) = max {r h(s,a,b)+ s′P hk(s′ |s,ea,b) ·Vk h+1(s′) −γ hk(s,a,b) −Γk h(s,a,b),0 }.
12: end for
P
13: for s do e
14: Com∈ pS ute the policy πk(, s) = CCE(Qk (s, , ),Qk(s, , )).
h · ·| h · · h · ·
15: Compute the value functions Vk h(s)= E
π
hkQk h(s), Vk h(s)= E
π
hkQk h(s).
16: end for
17: end for
18: Deploy policy πk = (πk, ,πk ) and get trajectory (sk,ak,bk,rk, ,sk ).
1 ··· H 1 1 1 1 ··· H+1
19: Update the private counts to Nk+1 via Privatizer.
20: if (Vk
1
−Vk 1)(s 1)< ∆ then
21: ∆ = (Vk
1
−Vk 1)(s 1) and πoute = πk = (π 1k,
···
,π Hk ).
22: end if
23: end for
24: Return: The marginal policies of πout: (µout,νout).
9[H] [K], the Nk(s,a,b,s′) and Nk(s,a,b) from Privatizer satisfies:
×S ×A×B×S × h h
(1) Nk(s,a,b,s′) Nk(s,a,b,s′) E , Nk(s,a,b) Nk(s,a,b) E . Nk(s,a,b,s′) > 0.
| h − h | ≤ ǫ,β | h − h | ≤ ǫ,β h
(2) Nk(s,a,b) = Nk(s,a,b,e s′) Nk(s,a,b). e
h s′∈S h ≥ h
e e e
Given the privatPe counts satisfying Assumption 3.1, the private estimate of transition kernel is
e e
defined as below.
Nk(s,a,b,s′)
Pk(s′ s,a,b) = h , (h,s,a,b,s′,k). (1)
h | Nk(s,a,b) ∀
h
e
e
Remark 3.2. Assumption 3.1 is a generaleization of Assumption 3.1 of Qiao and Wang [2023c] to
the two-player setting. The assumption (2) guarantees that the private transition kernel estimate
Pk( s,a,b) is a valid probability distribution, which enables our usage of Bernstein-type bonus.
h ·|
Besides, P is close to the empirical transition kernel based on original visitation counts according
teo Assumption (1).
e
Algorithmic design. Following previous non-private approaches [Liu et al., 2021], DP-Nash-VI
(Algorithm 1) maintains a pair of value functions Q and Q which are the upper bound and lower
bound of the Q value of the current policy when facing best responses (with high probability).
More specifically, we use private visitation counts Nk to construct a private estimate of transition
h
kernel Pk (line 7) and a pair of private bonus γk (line 8) and Γk (line 9). Intuitively, the first term
h h h
of Γk is derived from Bernstein’s inequality while tehe second term is the additional bonus due to
h
e k
differential privacy. Next we do value iteration with bonuses to construct the UCB function Q
h
(line 10) and the LCB function Qk (line 11). The policy πk for the k-th episode is calculated using
h
the CCE function (discussed below) and we run πk to collect a trajectory (line 14,18). Finally, the
Privatizertransfersthenon-privatecountstoprivateonesforthenextepisode(line19). Theoutput
policy πout is chosen as the policy πk with minimal gap (Vk Vk)(s ) (line 21). Decomposing the
1 − 1 1
output policy, the output policy (µout,νout) for both players are the marginal policies of πout, i.e.
µout( s) = πout(,b s) and νout( s) = πout(a, s) for all (h,s) [H] .
h ·| b∈B h · | h ·| a∈A h ·| ∈ ×S
Coarse CoPrrelated Equilibrium (CCEP). Intuitively speaking, CCE of a Markov Game is a
potentially correlated policy where no player could benefit from unilateral unconditional deviation.
As a computationally friendly relaxation of Nash Equilibrium, CCE has been applied by previ-
ous works [Xie et al., 2020, Liu et al., 2021] to design efficient algorithms. Formally, for any two
functions Q(, ),Q(, ) : [0,H], CCE(Q,Q) returnsa policy π ∆( )such that
· · · · A×B → ∈ A×B
E Q(a,b) maxE Q(a′,b),
(a,b)∼π (a,b)∼π
≥ a′
E Q(a,b) minE Q(a,b′).
(a,b)∼π (a,b)∼π
≤ b′
Since Nash Equilibrium (NE) is a special case of CCE and a NE always exists, a CCEalways exists.
Moreover, a CCEcan bederived inpolynomial timevia linear programming. Note thatthepolicies
given by CCE can be correlated for the two players, therefore deploying such policy requires the
cooperation of both players (line 18).
104 Main results
We firststate theregretanalysis of DP-Nash-VI (Algorithm 1)basedonAssumption3.1, which can
be combined with any Privatizers. The proof of Theorem 4.1 is sketched in Section 6 with details
in the Appendix. Note that (µk,νk) denote the marginal policies of πk for both players.
Theorem 4.1. For any privacy budget ǫ > 0, failure probability β [0,1] and any Privatizer
∈
satisfying Assumption 3.1, with probability at least 1 β, the regret of DP-Nash-VI (Algorithm 1)
−
is bounded by
K
Regret(K)= V†,νk (s ) Vµk,† (s ) O √H2SABT +H2S2ABE , (2)
1 1 − 1 1 ≤ ǫ,β
Xk=1h i (cid:16) (cid:17)
e
where K is the number of episodes and T = HK.
Under the special case wherethe privacy budgetǫ (i.e. there is no privacy concern), plugging
→ ∞
E = 0 in Theorem 4.1 will imply a regret bound of O(√H2SABT). Such result directly matches
ǫ,β
the best known result for regret minimization without privacy constraints [Liu et al., 2021] and
nearlymatchesthelowerboundofΩ( H2S(A+B)T)e[Bai and Jin,2020]. Furthermore,underthe
special case of single-agent MDP (where B = 1), our result reduces to Regret(K) O(√H2SAT +
p ≤
H2S2AE ). Suchresultmatches thebestknownresultunderthesamesetofconditions (Theorem
ǫ,β
4.1ofQiao and Wang[2023c]). Therefore,Theorem4.1isageneralization ofthebesteknownresults
underMARL[Liu et al.,2021]andDifferentially Private (single-agent) RL[Qiao and Wang,2023c]
simultaneously.
PAC guarantee. Recall that we output a policy πout whose marginal policies are (µout,νout). We
highlight that the outputpolicy for each player is a single Markov policy that is convenient to store
and deploy. Moreover, as a corollary of the regret bound, we give a PAC bound for the output
policy.
Theorem 4.2. For any privacy budget ǫ > 0, failure probability β [0,1] and any Privatizer that
∈
satisfies Assumption 3.1, if the number of episodes satisfies that
K Ω H3SAB +min K′ H2S2ABEǫ,β α , withprobability 1 β, (µout,νout)isα-approximate
≥ α2 | K′ ≤ −
Nash, (cid:16)i.e., n o(cid:17)
e †,νout µout,†
V (s ) V (s ) α. (3)
1 1 − 1 1 ≤
The proof is deferred to Appendix B.4. Here the second term of the sample complexity bound4
ensures that the additional cost due to DP is bounded by O(α). The detailed PAC guarantees
under the special cases where the Privatizer is either Central or Local will be provided in Section
5.
5 Privatizers for JDP and LDP
In this section, we propose Privatizers that provide DP guarantees (JDP or LDP) while satisfying
Assumption 3.1. The proofs for this section can be found in Appendix C.
4The presentation here is because the term E ǫ,β is indeed dependent of thenumberof episodes K.
115.1 Central Privatizer for Joint DP
Given the number of episodes K, the Central Privatizer applies K-bounded Binary Mechanism
[Chan et al., 2011] to privatize all the visitation counter streams Nk(s,a,b), Nk(s,a,b,s′), thus
h h
protecting the information of all single users. Briefly speaking, Binary mechanism takes a stream
of partial sums as input and outputs a surrogate stream satisfying differential privacy, while the
errorfor each item scales only logarithmically on thelength ofthestream5. Hereinmulti-agent RL,
for each (h,s,a,b), the stream Nk(s,a,b) = k−11(si,ai,bi = s,a,b) can beconsidered as
{ h i=1 h h h }k∈[K]
the partial sums of 1(si,ai,bi = s,a,b) . Therefore, after observing 1(sk,ak,bk = s,a,b) at the
{ h h h } P h h h
end of episode k, the Binary Mechanism will output a private version of k 1(si,ai,bi = s,a,b).
i=1 h h h
However, Binary Mechanism alone does not satisfy (2) of Assumption 3.1, and a post-processing
P
step is required. To sum up, we let the Central Privatizer follow the workflow below:
Given the privacy budget for JDP ǫ > 0,
(1)For all (h,s,a,b,s′), we apply Binary Mechanism (Algorithm 2 in Chan et al. [2011]) with in-
put parameter ǫ′ = ǫ to privatize all the visitation counter streams Nk(s,a,b) and
2HlogK { h }k∈[K]
Nk(s,a,b,s′) . We denote the output of Binary Mechanism by Nk.
{ h }k∈[K] h
(2)The private counts Nk are derived through the procedure in Section 5.3 with
h b
E = O(H log(HSABK/β)2).
ǫ,β ǫ
e
Our Central Privatizer satisfies the privacy guarantee below.
Lemma 5.1. For any possible ǫ,β, the Central Privatizer satisfies ǫ-JDP and Assumption 3.1 with
E = O(H).
ǫ,β ǫ
Combining Lemma 5.1 with Theorem 4.1 and Theorem 4.2, we have the following regret & PAC
e
guarantee under ǫ-JDP.
Theorem 5.2 (Results under JDP). For any possible ǫ,β, with probability 1 β, the regret from
−
running DP-Nash-VI (Algorithm 1) instantiated with Central Privatizer satisfies:
Regret(K) O √H2SABT +H3S2AB/ǫ . (4)
≤
(cid:16) (cid:17)
Moreover, if the number of episodes K iselarger than Ω H3SAB + H3S2AB , with probability 1 β,
α2 ǫα −
the output policy (µout,νout) is α-approximate Nash. (cid:16) (cid:17)
e
Similar to the single-agent (MDP) setting (B = 1), the additional cost due to JDP is a lower order
term under the most prevalent regime where the privacy budget ǫ is a constant. When applied
to the single-agent case, our regret matches the best known regret O √H2SAT +H3S2A/ǫ
[Qiao and Wang, 2023c]. Moreover, when compared to the regret lower(cid:16)bound below, our mai(cid:17)n
term is nearly optimal while the lower order term has optimal dependenece on ǫ.
Theorem 5.3. For any algorithm Alg satisfying ǫ-JDP, there exists a Markov Game such that the
expected regret from running Alg for K episodes (T =HK steps) satisfies:
HS(A+B)
E[Regret(K)] Ω H2S(A+B)T + .
≥ ǫ
(cid:18) (cid:19)
5More details in Chan et al. [2011] and Kairouzp et al. [2021].
e
12Theregretlowerboundresultsfromthelowerboundforthenon-privatelearning[Bai and Jin,2020]
andanadaptation of thelower boundunderJDPguarantees [Vietri et al.,2020]tothemulti-player
setting. Details are deferred to the appendix.
5.2 Local Privatizer for Local DP
At the end of episode k, the Local Privatizer perturbs the statistics calculated from the new tra-
jectory before sending it to the agent. Since the set of original visitation counts σk(s,a,b) =
{ h
1(sk,ak,bk = s,a,b) has ℓ sensitivity H, we can achieve ǫ-LDP by directly addingLaplace
h h h }(h,s,a,b) 1 2
noise, i.e., σk(s,a,b) = σk(s,a,b) + Lap(2H). Similarly, repeating the above perturbation to
h h ǫ
1(sk,ak,bk,sk = s,a,b,s′) will lead to identical results. Therefore, the Local Pri-
{ h h h h+1 }(h,s,a,b,s′)
vatizer withebudget ǫ is as below:
(1)We perturb σk(s,a,b) = 1(sk,ak,bk = s,a,b) and σk(s,a,b,s′) = 1(sk,ak,bk,sk = s,a,b,s′)
h h h h h h h h h+1
by adding independent Laplace noises: for all (h,s,a,b,s′,k),
2H
σk(s,a,b) = σk(s,a,b)+Lap ,
h h ǫ
(cid:18) (cid:19) (5)
2H
σk(s,ea,b,s′) = σk(s,a,b,s′)+Lap .
h h ǫ
(cid:18) (cid:19)
e
(2)Then the noisy counts are derived according to
k−1
Nk(s,a,b) = σi(s,a,b),
h h
i=1
X (6)
b k−1 e
Nk(s,a,b,s′)= σi(s,a,b,s′),
h h
i=1
X
b e
and the private counts Nk are solved through the procedure in Section 5.3 with
h
E = O(H Klog(HSABK/β)).
ǫ,β ǫ
e
Our Local Pprivatizer satisfies the privacy guarantee below.
Lemma 5.4. For any possible ǫ,β , the Local Privatizer satisfies ǫ-LDP and Assumption 3.1 with
E = O(H√K).
ǫ,β ǫ
Combining Lemma 5.4 with Theorem 4.1 and Theorem 4.2, we have the following regret & PAC
e
guarantee under ǫ-LDP.
Theorem 5.5 (Results under LDP). For any possible ǫ,β, with probability 1 β, the regret from
−
running DP-Nash-VI (Algorithm 1) instantiated with Local Privatizer satisfies:
Regret(K) O √H2SABT +S2AB√H5T/ǫ . (7)
≤
(cid:16) (cid:17)
Moreover, if the number of episodes K
ie
s larger than Ω
H3SAB
+
H6S4A2B2
, with probability 1 β,
α2 ǫ2α2 −
the output policy (µout,νout) is α-approximate Nash. (cid:16) (cid:17)
e
13Similar to the single-agent case, the additional cost due to LDP is a multiplicative factor to the
regret bound. When applied to the single-agent case, our regret matches the best known regret
O √H2SAT +S2A√H5T/ǫ [Qiao and Wang, 2023c]. Moreover, we state the lower bound be-
low(cid:16). (cid:17)
e
Theorem 5.6. For any algorithm Alg satisfying ǫ-LDP, there exists a Markov Game such that the
expected regret from running Alg for K episodes (T =HK steps) satisfies:
HS(A+B)T
E[Regret(K)] Ω H2S(A+B)T + .
≥ ǫ
p !
p
e
The lower bound is adapted from Garcelon et al. [2021]. While our regret has optimal dependence
on ǫ and K, the optimal dependence on H,S,A,B remains open.
5.3 The post-processing step
Now we introduce the post-processing step. At the end of episode k, given the noisy counts
Nk(s,a,b) and Nk(s,a,b,s′) for all (h,s,a,b,s′), the private visitation counts are constructed as
h h
following: for all (h,s,a,b),
b b
Nk(s,a,b,s′) = argmin max x Nk(s,a,b,s′)
h s′ h
n os′∈S {x s′} s′∈S s′∈S (cid:12) − (cid:12)
(cid:12) (cid:12)
suce h that
(cid:12)
x s′ −N hk(s,a,b)
(cid:12) ≤
E(cid:12) 4ǫ,β andb x s′
≥
0, ∀s(cid:12) ′. (8)
(cid:12)s X′∈S (cid:12)
(cid:12) (cid:12)
Nk(s,a,b)(cid:12)= Nk(sb ,a,b,s′).(cid:12)
h (cid:12) h (cid:12)
s′∈S
X
e e
Lastly, we add a constant term to each count to ensure that with high probability, no underestima-
tion will happen.
E
Nk(s,a,b,s′)= Nk(s,a,b,s′)+ ǫ,β ,
h h 2S
(9)
E
e Nk(s,a,b) =e Nk(s,a,b)+ ǫ,β .
h h 2
e e
Remark 5.7. Solving problem (8) is equivalent to solving:
min t, s.t. x Nk(s,a,b,s′) t, x 0, s′ ,
s′ h s′
− ≤ ≥ ∀ ∈S
(cid:12) (cid:12)
x
N(cid:12) (cid:12)k(s,a,b
b)
E ǫ,β
,
(cid:12)
(cid:12)
(cid:12) s′ − h (cid:12) ≤ 4
(cid:12)s X′∈S (cid:12)
(cid:12) (cid:12)
b
which is a Linear Pro(cid:12)gramming problem(cid:12) with O(S) variables and O(S) linear constraints. This
(cid:12) (cid:12)
can be solved in polynomial time [Nemhauser and Wolsey, 1988]. Note that the computation of
CCE (line 14 in Algorithm 1) is also a LP problem, therefore the computational complexity of DP-
Nash-VI is dominated by O(HSABK) Linear Programming problems, which is computationally
friendly.
14We summarize the properties of private counts Nk below, which says that the post-processing step
h
ensures that our private transition kernel estimate is a valid probability distribution while only
enlarging the error by a constant factor. e
Lemma 5.8. Suppose Nk satisfies that with probability 1 β , uniformly over all (h,s,a,b,s′,k),
h − 3
it holds that
E
b Nk(s,a,b,s′) Nk(s,a,b,s′) ǫ,β ,
h − h ≤ 4
(cid:12) (cid:12)
(cid:12) (cid:12)b Nk(s,a,b) Nk(s,a,b) (cid:12) (cid:12)E ǫ,β ,
h − h ≤ 4
(cid:12) (cid:12)
then the Nk derived above satisfie(cid:12)sbAssumption 3.1. (cid:12)
h (cid:12) (cid:12)
5.4 Soeme discussions
In this part, we generalize the Privatizers in Qiao and Wang [2023c] (for single-agent case) to the
two-player setting, which enables our usage of Bernstein-type bonuses. Such techniques lead to a
tight regret analysis and a near-optimal “non-private part” of the regret bound eventually.
Meanwhile, theadditionalcost duetoDPhas sub-optimaldependenceonparameters regardingthe
Markov Game. The issue appears even in the single-agent case and is considered to be inherent to
model-based algorithms due to the explicit estimation of private transitions [Garcelon et al., 2021].
The improvement requires new algorithmic designs (e.g., private Q-learning) and we leave those as
future works.
Lastly, the Laplace Mechanism can be replaced with other mechanisms, such as Gaussian Mecha-
nism [Dwork et al., 2014] with approximate DP guarantee (or zCDP). The regret and PAC guar-
antees are readily derived by plugging in the corresponding E to Theorem 4.1 and Theorem
ǫ,β
4.2.
6 Proof overview
In this section, we provide a proof sketch of Theorem 4.1, which can further imply the PAC
guarantee (Theorem 4.2) and the regret bounds under JDP (Theorem 5.2) or LDP (Theorem 5.5).
The proof consists of the following steps:
(1)Bound the difference between the private statistics and their non-private counterparts.
(2)Prove that UCB and LCB hold with high probability.
(3)Bound the regret via telescoping over time steps and replace the private terms by non-private
ones.
Below we explain the key steps in detail. Recall that Nk denotes the real visitation counts, while
h
Nk,Pk are the private visitation counts and private transition kernel respectively.
h h
Step (1). According to Assumption 3.1 and standard concentration inequalities, we provide high
e e
probabilityupperboundsfor Pk( s,a,b) P ( s,a,b) and Pk(s′ s,a,b) P (s′ s,a,b). Besides,
k h ·| − h ·| k1 | h | − h | |
we upper bound the following key term (Pk P ) V⋆ (s,a,b) by
| h − h · h+1 |
e e
O Var V⋆ ()/Nk(s,a,b)+HSE /Nk(s,a,b) . DetailsaredeferredtoAppendixB.1.
P hk(·|s,a,b) h+1 · h e ǫ,β h
(cid:16)q (cid:17)
e b e e
15Step (2). Then we prove that UCB and LCB hold with high probability via backward induction
over timesteps (Appendix B.2). More specifically, the variance term of Γk is the private Bernstein-
h
type bonus, while the difference between the private variance and its non-private counterpart can
be bounded by γk and the lower order terms in Γk.
h h
Step (3). Lastly, the regret can be bounded by telescoping:
K H
Regret(K) O Γk(sk,ak,bk)
h h h h
≤
!
k=1h=1
XX
boundbynon-privateterms
| {z }
O K H Var Ph(·|sk h,ak h,bk h)V hπ +k 1
+
K H HSE ǫ,β 
≤ 
 Xk=1
Xh=1v u
u
N hk(sk h,ak h,bk h)
Xk=1
Xh=1N hk(sk h,ak h,bk h)

e t 
 boundbyCauchy-SchwarzinequalityandL.T.V. ≤H2S2ABEǫ,βι 

O(√H2S|ABT +H2S2{AzBE ). } | {z }
ǫ,β
≤
The details about each inequality above and the lower order terms we ignore are deferred to Ap-
e
pendix B.3.
7 Conclusion
We take the initial steps to study trajectory-wise privacy protection in multi-agent RL. We extend
the definitions of Joint DP and Local DP to the multi-player RL setting. In addition, we design
a provably-efficient algorithm: DP-Nash-VI (Algorithm 1) that could satisfy either of the two DP
constraints with corresponding regret guarantee. Moreover, our regret bounds strictly generalize
the best known results under DP single-agent RL. There are various interesting future directions,
such as improvingtheadditional cost dueto DP viamodel-free approaches and consideringMarkov
Games with function approximations. We believe the techniques in this paper could serve as basic
building blocks.
Acknowledgments
The research is partially supported by NSF Awards #2007117 and #2048091.
References
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. InInternational Conference on Machine Learning, pages
463–474. PMLR, 2020.
Mohammad Gheshlaghi Azar, Ian Osband, and R´emi Munos. Minimax regret bounds for reinforce-
mentlearning. InProceedings of the 34th International Conference on Machine Learning-Volume
70, pages 263–272. JMLR. org, 2017.
16Yoram Bachrach, Richard Everett, Edward Hughes, Angeliki Lazaridou, Joel Z Leibo, Marc Lanc-
tot, Michael Johanson, Wojciech M Czarnecki, and Thore Graepel. Negotiating team formation
using deep reinforcement learning. Artificial Intelligence, 288:103356, 2020.
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In
International Conference on Machine Learning, pages 551–560. PMLR, 2020.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. Advances
in neural information processing systems, 33:2159–2170, 2020.
Borja Balle, Maziar Gomrokchi, and Doina Precup. Differentially private policy evaluation. In
International Conference on Machine Learning, pages 2130–2138. PMLR, 2016.
Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, andKunalTalwar. When is memorization
of irrelevant training data necessary for high-accuracy learning? In ACM SIGACT Symposium
on Theory of Computing, pages 123–132, 2021.
Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):
885–890, 2019.
Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and
lower bounds. In Theory of Cryptography Conference, pages 635–658. Springer, 2016.
Nicholas Carlini,ChangLiu,U´lfarErlingsson,JernejKos,andDawnSong. Thesecretsharer: Eval-
uatingandtestingunintendedmemorization inneuralnetworks. InUSENIXSecurity Symposium
(USENIX Security 19), pages 267–284, 2019.
T-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM
Transactions on Information and System Security (TISSEC), 14(3):1–24, 2011.
Sayak Ray Chowdhury and Xingyu Zhou. Differentially private regret minimization in episodic
markov decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence,
2022.
Sayak Ray Chowdhury, Xingyu Zhou, and Ness Shroff. Adaptive control of differentially private
linear quadratic systems. In2021 IEEE International Symposium on Information Theory (ISIT),
pages 485–490. IEEE, 2021.
Sayak Ray Chowdhury, Xingyu Zhou, and Nagarajan Natarajan. Differentially private reward
estimation with preference feedback. arXiv preprint arXiv:2310.19733, 2023.
Qiwen Cui, Kaiqing Zhang, and Simon Du. Breaking the curse of multiagents in a large state
space: Rl in markov games with independentlinear function approximation. In The Thirty Sixth
Annual Conference on Learning Theory, pages 2651–2652. PMLR, 2023.
Chris Cundy and Stefano Ermon. Privacy-constrained policies via mutual information regularized
policy gradients. arXiv preprint arXiv:2012.15019, 2020.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac
bounds for episodic reinforcement learning. In Advances in Neural Information Processing Sys-
tems, pages 5713–5723, 2017.
17John C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical minimax
rates. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 429–
438. IEEE, 2013.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pages 265–284. Springer, 2006.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found.
Trends Theor. Comput. Sci., 9(3-4):211–407, 2014.
Jerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business
Media, 2012.
EvrardGarcelon,VianneyPerchet,CiaraPike-Burke,andMatteoPirotta. Localdifferentialprivacy
for regret minimization in reinforcement learning. Advances in Neural Information Processing
Systems, 34, 2021.
ParhamGohari, MatthewHale, andUfukTopcu. Privacy-engineered valuedecompositionnetworks
for cooperative multi-agent reinforcement learning. In 2023 62nd IEEE Conference on Decision
and Control (CDC), pages 8038–8044. IEEE, 2023.
MdTamjidHossain andJohnWTLee. Hidingin plainsight: Differential privacy noiseexploitation
for evasion-resilient localized poisoning attacks in multiagent reinforcement learning. In 2023
International Conference on Machine Learning and Cybernetics (ICMLC), pages 209–216. IEEE,
2023.
Md Tamjid Hossain, Hung Manh La, Shahriar Badsha, and Anton Netchaev. Brnes: Enabling
security and privacy-aware experience sharing in multiagent robotic and autonomous systems.
In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
9269–9276. IEEE, 2023.
JustinHsu,ZhiyiHuang,AaronRoth,TimRoughgarden,andZhiweiStevenWu. Privatematchings
andallocations. InProceedings oftheforty-sixth annualACMsymposium onTheoryofcomputing,
pages 21–30, 2014.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(4), 2010.
ChiJin,ZeyuanAllen-Zhu,SebastienBubeck,andMichaelIJordan.Isq-learningprovablyefficient?
In Advances in Neural Information Processing Systems, pages 4863–4873, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pages 2137–
2143. PMLR, 2020.
Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning–a simple, efficient, decentral-
ized algorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021.
Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng
Xu. Practical and private (deep) learning without sampling or shuffling. In International Con-
ference on Machine Learning, pages 5213–5225. PMLR, 2021.
18Michael Kearns, Mallesh Pai, Aaron Roth, and Jonathan Ullman. Mechanism design in large
games: Incentives and privacy. In Proceedings of the 5th conference on Innovations in theoretical
computer science, pages 403–410, 2014.
Jonathan Lebensold, William Hamilton, Borja Balle, and Doina Precup. Actor critic with differen-
tially private critic. arXiv preprint arXiv:1910.05876, 2019.
Chonghua Liao, Jiafan He, and Quanquan Gu. Locally differentially private reinforcement learning
for linear mixture markov decision processes. In Asian Conference on Machine Learning, pages
627–642. PMLR, 2023.
Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement
learning with self-play. In International Conference on Machine Learning, pages 7001–7010.
PMLR, 2021.
Paul Luyo, Evrard Garcelon, Alessandro Lazaric, and Matteo Pirotta. Differentially private ex-
ploration in reinforcement learning with linear representation. arXiv preprint arXiv:2112.01585,
2021.
Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms
for decentralized multi-agent reinforcement learning. In International Conference on Machine
Learning, pages 15007–15049. PMLR, 2022.
George Nemhauser and Laurence Wolsey. Polynomial-time algorithms for linear programming.
Integer and Combinatorial Optimization, pages 146–181, 1988.
Dung Daniel T Ngo, Giuseppe Vietri, and Steven Wu. Improved regret for differentially private
exploration inlinear mdp. InInternational Conference on Machine Learning, pages 16529–16552.
PMLR, 2022.
Hajime Ono and Tsubasa Takahashi. Locally private distributed reinforcement learning. arXiv
preprint arXiv:2001.11718, 2020.
Dan Qiao and Yu-Xiang Wang. Near-optimal deployment efficiency in reward-free reinforcement
learning with linear function approximation. International Conference on Learning Representa-
tions, 2023a.
Dan Qiao and Yu-Xiang Wang. Offline reinforcement learning with differential privacy. Advances
in Neural Information Processing Systems, 2023b.
Dan Qiao and Yu-Xiang Wang. Near-optimal differentially private reinforcement learning. In
International Conference on Artificial Intelligence and Statistics, pages9914–9940. PMLR,2023c.
DanQiaoandYu-XiangWang. Near-optimalreinforcementlearningwithself-playunderadaptivity
constraints. arXiv preprint arXiv:2402.01111, 2024.
Dan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang. Sample-efficient reinforcement learning with
loglog(T) switching cost. In International Conference on Machine Learning, pages 18031–18061.
PMLR, 2022.
Dan Qiao, Ming Yin, and Yu-Xiang Wang. Logarithmic switching cost in reinforcement learning
beyond linear mdps. arXiv preprint arXiv:2302.12456, 2023.
19Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095–
1100, 1953.
Roshan Shariff and Or Sheffet. Differentially private contextual linear bandits. Advances in Neural
Information Processing Systems, 31, 2018.
Ali Shavandi and Majid Khedmati. A multi-agent deep reinforcement learning framework for
algorithmic trading in financial markets. Expert Systems with Applications, 208:118124, 2022.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354–359, 2017.
Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar
Shah, Junaid Qadir, and Salil S Kanhere. Privacy preserving large language models: Chatgpt
case study based vision and framework. arXiv preprint arXiv:2310.12523, 2023.
Giuseppe Vietri, Borja Balle, Akshay Krishnamurthy, and Steven Wu. Private reinforcement learn-
ing with pac and regret guarantees. In International Conference on Machine Learning, pages
9754–9764. PMLR, 2020.
Baoxiang WangandNidhiHegde. Privacy-preservingq-learningwithfunctionalnoiseincontinuous
spaces. Advances in Neural Information Processing Systems, 32, 2019.
YuanhaoWang,QinghuaLiu,YuBai,andChiJin. Breakingthecurseofmultiagency: Provablyeffi-
cient decentralized multi-agent rlwith function approximation. arXiv preprint arXiv:2302.06606,
2023.
Fan Wu, Huseyin A Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, and
Robert Sim. Privately aligning language models with reinforcement learning. arXiv preprint
arXiv:2310.16960, 2023a.
Yulian Wu, Xingyu Zhou, Sayak Ray Chowdhury, and Di Wang. Differentially private episodic
reinforcement learning with heavy-tailed rewards. arXiv preprint arXiv:2306.01121, 2023b.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. In Conference on
learning theory, pages 3674–3682. PMLR, 2020.
Tengyang Xie, Philip S Thomas, and Gerome Miklau. Privacy preserving off-policy evaluation.
arXiv preprint arXiv:1902.00174, 2019.
Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao
Qiu, Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning.
Advances in Neural Information Processing Systems, 33:621–632, 2020.
Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, and Shuai Li. Differentially private temporal
difference learning with stochastic nonconvex-strongly-concave optimization. In Proceedings of
20the Sixteenth ACM International Conference on Web Search and Data Mining, pages 985–993,
2023a.
Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, and Shuai Li. Dpmac: differentially
private communication for cooperative multi-agent reinforcement learning. arXiv preprint
arXiv:2308.09902, 2023b.
FuhengZhao,DanQiao,RachelRedberg,DivyakantAgrawal, AmrElAbbadi,andYu-XiangWang.
Differentially private linear sketches: Efficient implementations and applications. Advances in
Neural Information Processing Systems, 35:12691–12704, 2022.
Xingyu Zhou. Differentially private reinforcement learning with linear function approximation.
Proceedings of the ACM on Measurement and Analysis of Computing Systems, 6(1):1–27, 2022.
21A Extended related works
Differentially private reinforcement learning. Thestream of research on DP RLstarted from
the offline setting. Balle et al. [2016] first studied privately evaluating the value of a fixed policy
from running it for several episodes (the on policy setting). Later, Xie et al. [2019] considered a
more general setting of DP off policy evaluation. Recently, Qiao and Wang [2023b] provided the
first results for offline reinforcement learning with DP guarantees.
Moreefforts focusedonsolvingregretminimization. Underthesettingof tabularMDP,Vietri et al.
[2020] designed PUCB by privatizing UBEV [Dann et al., 2017] to satisfy Joint DP. Besides,
under the constraints of Local DP, Garcelon et al. [2021] designed LDP-OBI based on UCRL2
[Jaksch et al., 2010]. Chowdhury and Zhou [2022] designed a general framework for both JDP and
LDP based on UCBVI [Azar et al., 2017], and improved upon previous results. Finally, the best
known results are obtained by Qiao and Wang [2023c] via incorporating Bernstein-type bonuses.
Meanwhile, Wu et al. [2023b] studied the case with heavy-tailed rewards. Under linear MDP,
the only algorithm with JDP guarantee: Private LSVI-UCB [Ngo et al., 2022] is a private and
low switching 6 version of LSVI-UCB [Jin et al., 2020], while LDP under linear MDP still re-
mains open. Under linear mixture MDP, LinOpt-VI-Reg [Zhou, 2022] generalized UCRL-VTR
[Ayoub et al., 2020] to guarantee JDP, while Liao et al. [2023] also privatized UCRL-VTR for LDP
guarantee. In addition, Luyo et al. [2021] provided a unified framework for analyzing joint and
local DP exploration.
ThereareseveralotherworksregardingDPRL.Wang and Hegde[2019]proposedprivacy-preserving
Q-learning to protect the reward information. Ono and Takahashi [2020] studied the problem of
distributed reinforcement learning under LDP. Lebensold et al. [2019] presented an actor critic
algorithm with differentially private critic. Cundy and Ermon [2020] tackled DP-RL under the pol-
icy gradient framework. Chowdhury et al. [2021] considered the adaptive control of differentially
private linear quadratic (LQ) systems. Zhao et al. [2023a] studied differentially private temporal
difference (TD) learning. Chowdhury et al. [2023] analyzed reward estimation with preference feed-
back underthe constraints of DP. Hossain and Lee [2023], Hossain et al. [2023], Zhao et al. [2023b],
Gohari et al. [2023] focused on the privatization of communications between multiple agents in
multi-agent RL. For applications, DP RL was applied to protect sensitive information in natu-
ral language processing and large language models (LLM) [Ullah et al., 2023, Wu et al., 2023a].
Meanwhile, Zhao et al. [2022] considered linear sketches with DP.
B Proof of main theorems
In this section, we prove Theorem 4.1 and Theorem 4.2.
B.1 Properties of private estimations
We begin with some concentration results about our private transition kernel estimate P that will
be useful for the proof. Throughout the paper, let the non-private empirical transition kernel
e
6ForlowswitchingRL,pleaserefertoQiao et al.[2022],Qiao and Wang[2023a],Qiao et al.[2023],Qiao and Wang
[2024].
22be:
Nk(s,a,b,s′)
Pk(s′ s,a,b) = h , (h,s,a,b,s′,k). (10)
h | Nk(s,a,b) ∀
h
b
In addition, recall that our private transition kernel estimate is defined as below.
Nk(s,a,b,s′)
Pk(s′ s,a,b) = h , (h,s,a,b,s′,k). (11)
h | Nk(s,a,b) ∀
h
e
e
Now we are ready to list the properties beelow. Note that ι = log(30HSABK/β) throughout the
paper.
Lemma B.1. With probability 1 β , for all (h,s,a,b,k) [H] [K], it holds that:
− 15 ∈ ×S ×A×B×
Sι 2SE
Pk( s,a,b) P ( s,a,b) 2 + ǫ,β , (12)
h ·| − h ·| 1 ≤ sNk(s,a,b) Nk(s,a,b)
(cid:13) (cid:13) h h
(cid:13) (cid:13)
e
(cid:13) (cid:13)
Pk( s,a,b) Pk( s,a,b) e 2SE ǫ,βe . (13)
h ·| − h ·| 1 ≤ Nk(s,a,b)
(cid:13) (cid:13) h
(cid:13) (cid:13)
e b
(cid:13) (cid:13)
Proof of Lemma B.1. TheproofisadirectgeneralizationofLeemmaB.2andRemarkB.3inQiao and Wang
[2023c] to the two-player setting.
Lemma B.2. With probability 1 2β, for all (h,s,a,b,s′,k) [H] [K], it holds
− 15 ∈ ×S×A×B×S×
that:
min P (s′ s,a,b),Pk(s′ s,a,b) ι 2E ι
Pk(s′ s,a,b) P (s′ s,a,b) 2 { h | h | } + ǫ,β , (14)
h | − h | ≤ v Nk(s,a,b) Nk(s,a,b)
(cid:12) (cid:12) u u h e h
(cid:12) e (cid:12) t
(cid:12) (cid:12)
Pk(s′ s,a,b) Pk(s′ s,a,b)e 2E ǫ,β . e (15)
h | − h | ≤ Nk(s,a,b)
(cid:12) (cid:12) h
(cid:12) (cid:12)
e b
(cid:12) (cid:12)
Proof of Lemma B.2. TheproofisadirectgeneralizationofLeemmaB.4andRemarkB.5inQiao and Wang
[2023c] to the two-player setting.
Lemma B.3. With probability 1 2β, for all (h,s,a,b,k) [H] [K], it holds that:
− 15 ∈ ×S ×A×B×
Pk P V⋆ (s,a,b) min 2Var Ph(·|s,a,b)V h⋆ +1( ·) ·ι , 2Var P hk(·|s,a,b)V h⋆ +1( ·) ·ι +2HSE ǫ,βι ,
(cid:12)(cid:16)
h − h (cid:17)· h+1
(cid:12)
≤  s N hk(s,a,b) v u
u
bN hk(s,a,b) 

N hk(s,a,b)
(cid:12) e (cid:12) t (16)
(cid:12) (cid:12)
 e 2HSE e  e
Pk Pk V⋆ (s,a,b) ǫ,β . (17)
h
−
h
·
h+1
≤ Nk(s,a,b)
(cid:12)(cid:16) (cid:17) (cid:12) h
(cid:12) (cid:12)
e b
(cid:12) (cid:12)
Proof of Lemma B.3. TheproofisadirectgeneralizationofeLemmaB.6andRemarkB.7inQiao and Wang
[2023c] to the two-player setting.
23According to a union bound, the following lemma holds.
Lemma B.4. Under the high probability event that Assumption 3.1 holds, with probability at least
1 β, the conclusions in Lemma B.1, Lemma B.2, Lemma B.3 hold simultaneously.
− 3
Throughouttheproof,wewillassumethatAssumption3.1andLemmaB.4hold,whichwillhappen
with high probability. Before we prove the main theorems, we present the following lemma which
bounds the two variances.
Lemma B.5 (LemmaC.5ofQiao and Wang[2023b]). For any function V RS such that V
∞
∈ k k ≤
H, it holds that
Var (V) Var (V) √3H Pk( s,a,b) Pk( s,a,b) . (18)
P hk(·|s,a,b) − P hk(·|s,a,b) ≤ · h ·| − h ·| 1
r
(cid:12)q q (cid:12) (cid:13) (cid:13)
In additi(cid:12) (cid:12)on, acce ording to Lemma B.1b , the left ha(cid:12) (cid:12)nd side can(cid:13) (cid:13)beefurther boundbed by (cid:13) (cid:13)
SE
ǫ,β
Var (V) Var (V) 3H . (19)
P hk(·|s,a,b) − P hk(·|s,a,b) ≤ sNk(s,a,b)
(cid:12)q q (cid:12) h
(cid:12) e b (cid:12)
(cid:12) (cid:12)
e
B.2 Proof of UCB and LCB
For notational simplicity, for V RS such that V H, we define
∞
∈ k k ≤
V hkV(s,a,b) = Var
P
hk(·|s,a,b)V( ·), V hV(s,a,b) = Var Ph(·|s,a,b)V( ·). (20)
e e
Then the bonus term Γ can be represented as below (C is the universal constant in Algorithm
2
1).
Vk
Vk h+1+Vk
h+1 (s,a,b) ι
h 2 · C HSE ι C H2Sι
Γk(s,a,b) =C v (cid:18) (cid:19) + 2 ǫ,β · + 2 . (21)
h 2u u ue N hk(s,a,b) N hk(s,a,b) N hk(s,a,b)
t
We state the following lemma that caen bound the lower ordeer term, whicheis helpful for proving
UCB and LCB.
Lemma B.6. Suppose Assumption 3.1 and Lemma B.4 hold, then there exists a universal constant
c > 0 such that: if function g(s) satisfies g (s) (Vk Vk )(s), then it holds that:
1 | | ≤ h+1− h+1
(Pk P )g(s,a,b) c 1 min P (Vk Vk )(s,a,b),Pk(Vk Vk )(s,a,b)
h − h ≤H h h+1− h+1 h h+1− h+1
(cid:12) (cid:12) c H2Sn ι c HSE ι o (22)
(cid:12) (cid:12) e (cid:12) (cid:12) + 1 + 1 ǫ,β . e
Nk(s,a,b) Nk(s,a,b)
h h
e e
24Proof of Lemma B.6. If g (s) (Vk Vk )(s), it holds that:
| | ≤ h+1− h+1
(Pk P )g(s,a,b) Pk P (s′ s,a,b) g (s′)
h h h h
− ≤ − | ·| |
(cid:12) (cid:12) Xs′ (cid:12)(cid:16) (cid:17) (cid:12)
(cid:12) (cid:12) e Pk P (s′ (cid:12) (cid:12)s,a,b) (cid:12) (cid:12) Vek Vk (s′)(cid:12) (cid:12)
≤
h
−
h
| ·
h+1− h+1
Xs′ (cid:12)(cid:16) (cid:17) (cid:12) (cid:16) (cid:17)
(cid:12) (cid:12)
(cid:12) 2e P h(s′ |s,a,b)ι + (cid:12) 2E ǫ,βι Vk Vk (s′)
≤
s
Nk(s,a,b) Nk(s,a,b)!· h+1− h+1 (23)
Xs′ h h (cid:16) (cid:17)
P h(s′ |es,a,b) + H eι + 2E ǫ,βι Vk Vk (s′)
≤ H Nk(s,a,b) Nk(s,a,b)!· h+1− h+1
Xs′ h h (cid:16) (cid:17)
≤Hc 1 P h(Vk h+1−Vk h+1)(se,a,b)+ Nc k1 (H s,e2 aS ,ι
b)
+ c N1H k(S s,E aǫ ,, bβ )ι ,
h h
where the third inequality is because of Lemma B.2. The forth inequality results from AM-GM
e e
inequality. The last inequality holds for some universal constant c .
1
The empirical part with the R.H.S to be Pk can be proven using identical proof according to
h
(14).
e
Then we prove that the UCB and LCB functions are actually upper and lower bounds of the
best responses. Recall that πk is the (correlated) policy executed in the k-th episode and (µk,νk)
for both players are the marginal policies of πk. In other words, µk( s) = πk(,b s) and
h ·| b∈B h · |
νk( s) = πk(a, s) for all (h,s) [H] .
h ·| a∈A h ·| ∈ ×S P
Lemma BP.7. Suppose Assumption 3.1 and Lemma B.4 hold, then there exist universal constants
C ,C > 0 (in Algorithm 1) such that for all (h,s,a,b,k) [H] [K], it holds that:
1 2
∈ ×S ×A×B×
Qk (s,a,b) Q†,νk (s,a,b) Qµk,† (s,a,b) Qk(s,a,b),
h ≥ h ≥ h ≥ h (24)
(Vk (s) V†,νk (s) Vµk,† (s) Vk(s).
h ≥ h ≥ h ≥ h
Proof of Lemma B.7. We provebybackward induction. For each k [K], theconclusion is obvious
∈
for h = H + 1. Suppose UCB and LCB hold for Q value functions in the (h + 1)-th time step,
we first prove the bounds for V functions in the (h+1)-th step and then prove the bounds for Q
functions in the h-th step. For all s , it holds that
∈ S
Vk (s) =E Qk (s)
h+1 πk h+1
h+1
supE Qk (s)
≥ µ µ,ν hk +1 h+1
(25)
supE Q†,νk (s)
≥ µ µ,ν hk +1 h+1
†,νk
=V (s).
h+1
The conclusion Vk (s) Vµk,† (s) can be proven by symmetry. Therefore, it holds that
h+1 ≤ h+1
Vk (s) V†,νk (s) V⋆ (s) Vµk,† (s) Vk (s). (26)
h+1 ≥ h+1 ≥ h+1 ≥ h+1 ≥ h+1
25Next we prove the bounds for Q value functions at the h-th step. For all (s,a,b), it holds that
Qk Q†,νk (s,a,b) min PkVk P V†,νk +γk +Γk (s,a,b),0
h − h ≥ h h+1− h h+1 h h
m(cid:16) in PkV†(cid:17),νk P V†,νk +n γk(cid:16) +Γk (s,a,b),0 (cid:17) o
≥ h h+1 − h h+1 h e h
n(cid:16) (cid:17) o
e
=min Pk P V†,νk V⋆ (s,a,b)+ Pk P V⋆ (s,a,b)+γk(s,a,b)+Γk(s,a,b),0.
 h − h h+1 − h+1 h − h h+1 h h 
 
(cid:16) (cid:17)(cid:16) (cid:17) (cid:16) (cid:17) 
e (i) e (ii)
 (27)
 | {z } | {z }  
The absolute value of term (i) can be bounded as below.
(i) c 1 Pk(Vk Vk )(s,a,b)+ c 1H2Sι + c 1HSE ǫ,βι , (28)
| |≤ H h h+1− h+1 Nk(s,a,b) Nk(s,a,b)
h h
for some universal constantec according to Lemma B.6.
1
e e
The absolute value of term (ii) can be bounded as below.
2Var V⋆ () ι 2Var V⋆ () ι
(ii)
P hk(·|s,a,b) h+1 · ·
+
2HSE ǫ,βι P hk(·|s,a,b) h+1 · ·
+
8HSE ǫ,βι
, (29)
| |≤ v u bN hk(s,a,b) N hk(s,a,b) ≤ v u eN hk(s,a,b) N hk(s,a,b)
u u
t t
wherethe first inequality is because of Lemma B.3 while the second inequality holds dueto Lemma
e e e e
B.5.
We further bound the term Var V⋆ () as below.
P hk(·|s,a,b) h+1 ·
Vk +Vk e
Vk h+1 h+1 VkV⋆ () (s,a,b)
(cid:12) h 2 !− h h+1 · (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)e Vk +Vk 2 e (cid:12) (cid:12) Vk +Vk 2 2
(cid:12) Pk h+1 h+1 Pk V⋆(cid:12) 2 (s,a,b)+ Pk h+1 h+1 (s,a,b) PkV⋆ (s,a,b)
≤(cid:12) h · 2 − h · h+1 (cid:12) (cid:12) h · 2 − h h+1 (cid:12)
(cid:12) ! (cid:12) (cid:12)" ! # (cid:12)
(cid:12) (cid:0) (cid:1) (cid:12) (cid:12) h i (cid:12)
4(cid:12) (cid:12)He Pk Vk Vk (s,ae ,b). (cid:12) (cid:12) (cid:12) (cid:12) e e (cid:12) (cid:12)
≤(cid:12) h · h+1− h+1 (cid:12) (cid:12) (cid:12)
(cid:16) (cid:17) (30)
e
Therefore, the term (ii) can be further bounded as below.
2Var V⋆ () ι
(ii)
P hk(·|s,a,b) h+1 · ·
+
8HSE ǫ,βι
| |≤ v u eN hk(s,a,b) N hk(s,a,b)
u
t
2ι Vk Vk h+1+eVk h+1 (s,a,b)+2ι e4HPk Vk Vk (s,a,b)
v u · h (cid:18) 2 (cid:19) · h · (cid:16) h+1− h+1 (cid:17) + 8HSE ǫ,βι (31)
≤u
u
e N hk(s,a,b)e N hk(s,a,b)
t
v2V hk (cid:18)Vk h+1+ 2Vk h+1 (cid:19)(s,a,b)ι +eP hk · Vk h+1−Vk h+1 (s,a,b)
+
2H2ι e
+
8HSE ǫ,βι
,
≤u u
u
e N hk(s,a,b)
e
(cid:16) H (cid:17) N hk(s,a,b) N hk(s,a,b)
t
e 26 e ewhere the second inequality results from (30) and the third inequality is due to AM-GM inequality.
Combining the upper bounds of (i) and (ii) , there exist universal constants C ,C > 0 such that
1 2
| | | |
(i)+(ii)+γk(s,a,b)+Γk(s,a,b) 0. (32)
h h
≥
Theinequalityimpliesthat Qk Q†,νk (s,a,b) 0. Bysymmetry,wehave Qk Qµk,† (s,a,b)
h − h ≥ h− h ≤
0. As a result, it holds that(cid:16)Qk (s,a,b) (cid:17) Q†,νk (s,a,b) Q⋆(s,a,b) Qµk,† (s(cid:16),a,b) Qk((cid:17)s,a,b).
h ≥ h ≥ h ≥ h ≥ h
According to backward induction, the conclusion holds for all (h,s,a,b,k).
B.3 Proof of Theorem 4.1
Given the UCB and LCB property, we are now ready to prove our main results. We first state the
following lemma that controls the error of the empirical variance estimator.
Lemma B.8. Suppose Assumption 3.1 and Lemma B.4 hold, then there exists a universal constant
c > 0 such that for all (h,s,a,b,k) [H] [K], it holds that
2
∈ ×S ×A×B×
Vk +Vk
Vk h+1 h+1 V Vπk (s,a,b)
(cid:12) h 2 !− h h+1 (cid:12)
(cid:12) (cid:12) (33)
(cid:12) (cid:12)
4(cid:12) (cid:12)He P Vk Vk (s,a,b)+(cid:12) (cid:12)c 2H2SE ǫ,β +c H2 Sι .
≤ h h+1− h+1 Nk(s,a,b) 2 sNk(s,a,b)
(cid:16) (cid:17) h h
Proof of Lemma B.8. According to Lemma B.7,
Vek
(s)
Vπk
(s)
Vke
(s) always holds. Then it
h ≥ h ≥ h
holds that
Vk +Vk
Vk h+1 h+1 V Vπk (s,a,b)
(cid:12) h 2 !− h h+1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)e Vk +Vk 2 (cid:12) (cid:12) 2 Vk +Vk 2 2
(cid:12) Pk h+1 h+1 P Vπ(cid:12)k Pk h+1 h+1 + P Vπk (s,a,b)
≤(cid:12) h 2 − h h+1 − h 2 h h+1 (cid:12)
(cid:12) ! " !# (cid:12)
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17) (cid:12)
(cid:12) (cid:12) Pe k Vk 2 P Vk 2 PkVk e2 + P Vk 2 (s,a,b) (cid:12) (cid:12)
≤(cid:12) h h+1 − h h+1 − h h+1 h h+1 (cid:12) (34)
(cid:12) (cid:12)
(cid:12) (cid:12)e Pk(cid:16) P (cid:17) Vk (cid:16) 2 (s,a(cid:17) ,b)+(cid:16) e P V(cid:17) k 2(cid:16) Vk (cid:17) 2(cid:12) (cid:12) (s,a,b)
≤(cid:12) h
−
h h+1 h h+1
−
h+1 (cid:12)
(cid:12) (cid:12) (cid:12) (cid:20) (cid:21)(cid:12)
(cid:12)(cid:16) (cid:17)(cid:16) (cid:17) (cid:12) (cid:12) (cid:16) (cid:17) (cid:16) (cid:17) (cid:12)
(cid:12) e (i) (cid:12) (cid:12) (ii) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
| + PkVk {2 z P Vk }2 (s| ,a,b)+ P Vk {z 2 P Vk 2 } (s,a,b).
h h+1
−
h h+1 h h+1
−
h h+1
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)(cid:16) (cid:17) (cid:16) (cid:17) (cid:12) (cid:12)(cid:16) (cid:17) (cid:16) (cid:17) (cid:12)
(cid:12) e (iii) (cid:12) (cid:12) (iv) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
The term| (i) can be boun{zded as below due}to|Lemma B.1. {z }
Sι 2H2SE
(i) 2H2 + ǫ,β . (35)
≤ sNk(s,a,b) Nk(s,a,b)
h h
e 27 eThe term (ii) can be directly bounded as below.
(ii) 2HP Vk Vk (s,a,b). (36)
≤
h h+1− h+1
(cid:16) (cid:17)
The term (iii) can be bounded as below due to Lemma B.1.
Sι 4H2SE
(iii) 2H Pk P Vk (s,a,b) 4H2 + ǫ,β . (37)
≤ h − h h+1 ≤ sNk(s,a,b) Nk(s,a,b)
(cid:12)(cid:16) (cid:17) (cid:12) h h
(cid:12) (cid:12)
e
The term (iv) can be dir(cid:12)ectly bounded as(cid:12)below.
e e
(iv) 2HP Vk Vk (s,a,b). (38)
≤ h h+1− h+1
(cid:16) (cid:17)
The conclusion holds according the upper bounds of term (i), (ii), (iii) and (iv).
Finally we prove the regret bound of Algorithm 1.
Proof of Theorem 4.1. Ourproofbase on Assumption3.1 andLemma B.4. We definethefollowing
notations.
∆k = Vk Vk (sk),
h h − h h
ζk = ∆(cid:16)k Qk (cid:17) Qk (sk,ak,bk), (39)
  h h− h − h h h h
 ξk = P V(cid:16)k Vk (cid:17) (sk,ak,bk) ∆k .
h h h+1− h+1 h h h − h+1
Then it holds that ζk an  d ξk are m(cid:16) artingale diffe(cid:17) rences bounded by H. In addition, we use the
h  h
following abbreviations for notational simplicity.
γk = γk(sk,ak,bk),
h h h h h
Γk = Γk(sk,ak,bk),
h h h h h (40)
  Nk = Nk(sk,ak,bk),
 h h h h h

Nk = Nk(sk,ak,bk).
h h h h h


Then we have the following analysis  abo eut ∆k he.
∆k = ζk + Qk Qk (sk,ak,bk)
h h h − h h h h
ζk +2γk +(cid:16) 2Γk +Pk(cid:17) Vk Vk (sk,ak,bk)
≤
h h h h h+1− h+1 h h h
(cid:16) (cid:17)
ζk +2Γk + 1+ 2 eC 1 1+ c 1 P Vk Vk (sk,ak,bk)+ c 1H2Sι + c 1HSE ǫ,βι
≤ h h H ·
"
H · h h+1− h+1 h h h Nk Nk
#
(cid:18) (cid:19) (cid:16) (cid:17) (cid:16) (cid:17) h h
≤ζ hk + 1+ Hc 3 ·P h Vk h+1−Vk h+1 (sk h,ak h,bk h)+ c 3H N2 kSι + c 3H NSE kǫ,βι e e
(cid:16) (cid:17) (cid:16) (cid:17) h h
Vk
Vk h+1+Vk
h+1 (sk,ak,bk)ι e e
h 2 h h h
+c v (cid:18) (cid:19) ,
3u
u ue N hk
t
(i)
e
(41)
| {z }
28where the first inequality holds because of the definition of Q and Q. The second inequality holds
due to the definition of γk and Lemma B.6. The last inequality holds for some universal constant
h
c > 0.
3
The term (i) can be further bounded as below according to Lemma B.8 and AM-GM inequality.
(i)
V hV hπ +k 1(sk h,ak h,bk h)ι
+
4HP h Vk h+1−Vk h+1 (sk h,ak h,bk h)ι
+
H c 2SE ǫ,βι
+c
ι
+
H2ι√c 2S
≤v u
u
N hk v u
u
(cid:16) N hk (cid:17) pN hk 2 sN hk N hk
t V hV hπ +k 1(sek h,ak h,bk h)ι +t c 4P h Vk h+1−Vk h+e1 (sk h,ak h,bk h)
+
c 4H2√Sιe
+
c 4H SE ǫ,βιe
+c
ιe
,
≤v Nk (cid:16) H (cid:17) Nk Nk 4 Nk
u h h ph s h
u
t (42)
e e e e
where the first inequality results from Lemma B.8 and AM-GM inequality on the last term of (33).
The second inequality holds for some universal constant c > 0 according to AM-GM inequality.
4
Plugging in the upper bound of term (i), for some universal constant c > 0, it holds that:
5
∆k ζk + 1+
c
5 ξk + 1+
c
5 ∆k +c
V hV hπ +k 1(sk h,ak h,bk h)ι
+c
ι
+
c 5H2Sι
+
c 5HSE ǫ,βι
.
h ≤ h H h H h+1 5v Nk 5 Nk Nk Nk
(cid:16) (cid:17) (cid:16) (cid:17) u u h s h h h
t (43)
e e e e
Summing ∆k over k [K], we have for some universal constant c > 0, it holds that:
1 ∈ 6
K
∆k
K H
1+
c
5
h−1
ζk+
K H
1+
c
5
h
ξk+c
K H V hV hπ +k 1(sk h,ak h,bk h)ι
1 ≤ H h H h 6 v Nk
Xk=1 Xk=1 Xh=1(cid:16) (cid:17) Xk=1 Xh=1(cid:16) (cid:17) Xk=1 Xh=1u u h
t
(ii) (iii) (iv)
e (44)
| K H{z ι } K| H H2S{ιz+HSE } ι | {z }
ǫ,β
+c +c .
6 6
Nk Nk
k=1h=1s h k=1h=1 h
XX XX
(v) (vi)
e e
The term (ii) and| term{z(iii) }can b|e bounded{bzy Azuma-H}oeffding inequality. With probability
1
2β
, it holds that
− 9
(ii) O √H3Kι , (iii) O √H3Kι . (45)
| |≤ | | ≤
The main term (iv) is bounded as b(cid:16)elow. (cid:17) (cid:16) (cid:17)
K H V Vπk (sk,ak,bk)ι
(iv)
h h+1 h h h
≤ s Nk
k=1h=1 h
XX
K H K H
1
V Vπk (sk,ak,bk)ι (46)
≤v h h+1 h h h · Nk
uk=1h=1 k=1h=1 h
uXX XX
t
O(H2K +H3ι)ι O(HSABι)
≤ ·
=pO √H3SABK +H2√SAB .
(cid:16) (cid:17)
e
29The first inequality is because Nk Nk (Assumption 3.1). The second inequality holds due to
h ≥ h
β
Cauchy-Schwarz inequality. The third inequality holds with probability 1 because of Law
− 9
of total variance and standard ceoncentration inequalities (for details please refer to Lemma 8 of
Azar et al. [2017]).
The term (v) is bounded as below due to pigeon-hole principle.
K H
ι
(v) O(√H2SABKι), (47)
≤ Nk ≤
k=1h=1r h
XX
where the first inequality is because Nk Nk (Assumption 3.1). The last one results from pigeon-
h ≥ h
hole principle.
e
The term (vi) can be bounded as below.
K H H2Sι+HSE ι
(vi) ǫ,β O(H3S2ABι2)+O(H2S2ABE ι2). (48)
≤ Nk ≤ ǫ,β
k=1h=1 h
XX
Combining the upper bounds for term (ii) , (iii) , (iv), (v) and (vi). The regret of Algorithm 1
| | | |
can be bounded as below.
K K
Regret(K) = V†,νk (s ) Vµk,† (s ) Vk (s ) Vk(s )
1 1 − 1 1 ≤ 1 1 − 1 1
Xk=1h i Xk=1h i (49)
K
= ∆k O √H2SABT +H3S2AB+H2S2ABE ,
1 ≤ ǫ,β
Xk=1 (cid:16) (cid:17)
e
where T = HK is the number of steps.
The failure probability is bounded by β (β for Assumption 3.1, β for Lemma B.4, β for terms (ii),
3 3 3
(iii) and (iv)). The proof of Theorem 4.1 is complete.
B.4 Proof of Theorem 4.2
In this part, we provide a proof of the PAC guarantee: Theorem 4.2. The proof directly follows
from the proof of the regret bound (Theorem 4.1).
Proof of Theorem 4.2. Recall that we choose πout = πk such that k = argmin Vk Vk (s ).
k 1 − 1 1
Therefore, we have (cid:16) (cid:17)
V†,νout (s ) Vµout,† (s ) Vk (s ) Vk(s ) 1 O √H3SABK +H2S2ABE , (50)
1 1 − 1 1 ≤ 1 1 − 1 1 ≤ K ǫ,β
(cid:16) (cid:17)
if ignoring the lower order term of the regret bound. e
Therefore, choosing K Ω H3SAB +min K′ H2S2ABEǫ,β α bounds the R.H.S by α.
≥ α2 | K′ ≤
(cid:16) n o(cid:17)
e
30C Missing proof in Section 5
In this section, we provide the missing proof for results in Section 5. Recall that Nk is the real visi-
h
tation count, Nk is the intermediate noisy count calculated by both Privatizers and Nk is the final
h h
private count after the post-processing step. Note that most of the proof here are generalizations
of Appendix Dbin Qiao and Wang [2023c] to the multi-player setting, and here we steate the proof
for completeness.
Proof of Lemma 5.1. Due to Theorem 3.5 of Chan et al. [2011] and Lemma 34 of Hsu et al. [2014],
the release of Nk(s,a,b) satisfies ǫ-DP. Similarly, the release of Nk(s,a,b,s′)
{ h }(h,s,a,b,k) 2 { h }(h,s,a,b,s′,k)
also satisfies ǫ-DP. Therefore, the release of the following private counters Nk(s,a,b) ,
2 { h }(h,s,a,b,k)
Nk(s,a,b,s′) b satisfy ǫ-DP. Due to post-processing (Lemma 2b .3 of Bun and Steinke
{ h }(h,s,a,b,s′,k)
[2016]), the release of both private counts Nk(s,a,b) and Nk(s,a,b,b s′) also
{ h }(h,s,a,b,k) { h }(h,s,a,b,s′,k)
sab tisfies ǫ-DP. Then it holds that the release of all πk is ǫ-DP according to post-processing. Finally,
the guarantee of ǫ-JDP results from Billboared Lemma (Lemma 9 of Hseu et al. [2014]).
For utility analysis, because of Theorem 3.6 of Chan et al. [2011], our choice ǫ′ = ǫ in Binary
2HlogK
Mechanism and a union bound, with probability 1 β, for all (h,s,a,b,s′,k),
− 3
H
Nk(s,a,b,s′) Nk(s,a,b,s′) O log(HSABK/β)2 ,
h − h ≤ ǫ
(cid:12) (cid:12) (cid:18) (cid:19) (51)
(cid:12) (cid:12) H
(cid:12)b Nk(s,a,b) Nk(s,a,b)(cid:12) O log(HSABK/β)2 .
h − h ≤ ǫ
(cid:12) (cid:12) (cid:18) (cid:19)
(cid:12) (cid:12)
b
Together with Lemma 5.(cid:12)8, the Central Privatizer(cid:12) satisfies Assumption 3.1 with E = O H .
ǫ,β ǫ
(cid:0) (cid:1)
Proof of Theorem 5.2. The proof directly results from plugging E = O H into Theeorem 4.1
ǫ,β ǫ
and Theorem 4.2.
(cid:0) (cid:1)
e
Proof of Theorem 5.3. Thefirsttermresultsfromthenon-privateregretlowerboundΩ( H2S(A+B)T)
[Bai and Jin, 2020]. The second term is a direct adaptation of the Ω(HSA/ǫ) lower bound for any
p
algorithms with ǫ-JDP guarantee under single-agent MDP [Vietri et al., 2020].
Proof of Lemma 5.4. The privacy guarantee directly results from properties of Laplace Mechanism
and composition of DP [Dwork et al., 2014].
For utility analysis, because of Corollary 12.4 of Dwork et al. [2014] and a union bound, with
probability 1 β , for all possible (h,s,a,b,s′,k),
− 3
H
Nk(s,a,b,s′) Nk(s,a,b,s′) O Klog(HSABK/β) ,
h − h ≤ ǫ
(cid:12) (cid:12) (cid:18) p (cid:19) (52)
(cid:12) (cid:12) H
(cid:12)b Nk(s,a,b) Nk(s,a,b)(cid:12) O Klog(HSABK/β) .
h − h ≤ ǫ
(cid:12) (cid:12) (cid:18) p (cid:19)
(cid:12) (cid:12)
b
TogetherwithLemma5(cid:12) .8,theLocalPrivatizers(cid:12) atisfiesAssumption3.1withE = O H√K .
ǫ,β ǫ
(cid:16) (cid:17)
e
31Proof of Theorem 5.5. The proof directly results from plugging E = O H√K into Theorem
ǫ,β ǫ
4.1 and Theorem 4.2. (cid:16) (cid:17)
e
Proof of Theorem 5.6. Thefirsttermresultsfromthenon-privateregretlowerboundΩ( H2S(A+B)T)
[Bai and Jin, 2020]. The second term is a direct adaptation of the Ω(√HSAT/ǫ) lower bound for
p
any algorithms with ǫ-LDP guarantee under single-agent MDP [Garcelon et al., 2021].
Proof of Lemma 5.8. For clarity, wedenote thesolution of (8)by N¯k and thereforeNk(s,a,b,s′)=
h h
N¯k(s,a,b,s′)+ Eǫ,β, Nk(s,a,b) = N¯k(s,a,b)+ Eǫ,β.
h 2S h h 2
e
When the condition (two inequalities) in Lemma 5.8 holds, the original counts Nk(s,a,b,s′)
e { h }s′∈S
is a feasible solution to the optimization problem, which means that
E
max N¯k(s,a,b,s′) Nk(s,a,b,s′) max Nk(s,a,b,s′) Nk(s,a,b,s′) ǫ,β .
s′ h − h ≤ s′ h − h ≤ 4
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
Combining w(cid:12)ith the conditionb in Lemma 5(cid:12).8 with r(cid:12)espect to Nk(s,ab ,b,s′), it hol(cid:12)ds that
h
E
N¯k(s,a,b,s′) Nk(s,a,b,s′) N¯k(s,a,b,s′) Nk(s,a,b,s′)b + Nk(s,a,b,s′) Nk(s,a,b,s′) ǫ,β .
h − h ≤ h − h h − h ≤ 2
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
S(cid:12)ince Nk(s,a,b,s′) = N¯k(s,a,(cid:12)b,s(cid:12)′)+ Eǫ,β and N¯k(b s,a,b,s′) (cid:12)0,(cid:12)wb e have (cid:12)
h h 2S h ≥
e Nk(s,a,b,s′) > 0, Nk(s,a,b,s′) Nk(s,a,b,s′) E . (53)
h h h ǫ,β
− ≤
(cid:12) (cid:12)
(cid:12) (cid:12)
e e
For N¯k(s,a,b), according to the constrai(cid:12) nts in the optimization probl(cid:12) em (8), it holds that
h
E
N¯k(s,a,b) Nk(s,a,b) ǫ,β .
h − h ≤ 4
(cid:12) (cid:12)
(cid:12) (cid:12)
Combining with the condition in(cid:12)Lemma 5.8 wb ith respec(cid:12)t to Nk(s,a,b), it holds that
h
E
N¯k(s,a,b) Nk(s,a,b) N¯k(s,a,b) Nk(s,a,b) + Nbk(s,a,b) Nk(s,a,b) ǫ,β .
h − h ≤ h − h h − h ≤ 2
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
Since(cid:12)Nk(s,a,b) = N¯k(s,a,b)(cid:12)+ E (cid:12)ǫ,β, we have b (cid:12) (cid:12)b (cid:12)
h h 2
e Nk(s,a,b) Nk(s,a,b) Nk(s,a,b)+E . (54)
h h h ǫ,β
≤ ≤
Accordingtothelastlineoftheoptimizate ionproblem(8),wehaveN¯k(s,a,b) = N¯k(s,a,b,s′)
h s′∈S h
and therefore,
P
Nk(s,a,b) = Nk(s,a,b,s′). (55)
h h
s′∈S
X
e e
The proof is complete by combining (53), (54) and (55).
32