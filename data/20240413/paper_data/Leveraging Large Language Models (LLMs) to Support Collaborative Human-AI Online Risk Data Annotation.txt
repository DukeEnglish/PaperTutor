LeveragingLargeLanguage Models(LLMs)toSupport Collaborative
Human-AI OnlineRisk Data Annotation
JINKYUNGPARK,VanderbiltUniversity,USA
PAMELAWISNIEWSKI,VanderbiltUniversity,USA
VIVEKSINGH,RutgersUniversity,USA
Inthispositionpaper,wediscussthepotentialforleveragingLLMsasinteractiveresearchtoolstofacilitatecollaborationbetween
humancodersandAItoeffectivelyannotateonlineriskdataatscale.Collaborativehuman-AIlabelingisapromisingapproachto
annotatinglarge-scaleandcomplexdataforvarioustasks.Yet,toolsandmethodstosupporteffectivehuman-AIcollaborationfor
dataannotationareunder-studied.Thisgapispertinentbecauseco-labelingtasksneedtosupportatwo-wayinteractivediscussion
thatcanaddnuanceandcontext,particularlyinthecontextofonlinerisk,whichishighlysubjectiveandcontextualized.Therefore,
weprovidesomeoftheearlybenefitsandchallengesofusingLLMs-basedtoolsforriskannotationandsuggestfuturedirectionsfor
theHCIresearchcommunitytoleverageLLMsasresearchtoolstofacilitatehuman-AIcollaborationincontextualizedonlinedata
annotation.OurresearchinterestsalignverywellwiththepurposesoftheLLMsasResearchToolsworkshoptoidentifyongoing
applicationsandchallengesofusingLLMstoworkwithdatainHCIresearch.Weanticipatelearningvaluableinsightsfromorganizers
andparticipantsintohowLLMscanhelpreshapetheHCIcommunity’smethodsforworkingwithdata.
CCSConcepts:•Human-centeredcomputing→Interactivesystemsandtools.
AdditionalKeyWordsandPhrases:LargeLanguageModel,ResearchTool,Human-AICollaboration,OnlineRiskAnnotation,Con-
versationalAgent
ACMReferenceFormat:
JinkyungPark,PamelaWisniewski,andVivekSingh.2024.LeveragingLargeLanguageModels(LLMs)toSupport Collaborative
Human-AIOnlineRiskDataAnnotation.InCHI2024WorkshoponLLMsasResearchTools:ApplicationsandEvaluationsinHCIData
Work,May12,2024,Honolulu,HI,USA.ACM,NewYork,NY,USA,6pages.
1 INTRODUCTION
Onlineriskexposureisapervasivephenomenonthataffectsmillionsofsocialmediauserseveryday[9,12,32,38].
Giventhemassivescaleofonlinecontentgeneration,thedevelopmentandimplementationofmachinelearning(ML)
basedriskdetectiontoolstoautomaticallyidentifyandmitigatevariousonlineriskshasbeenaccelerating(e.g.,[5,14,
24,27,28,33,35]).TodeveloptheseAI-basedsystems,Human-ComputerInteraction(HCI),socialcomputing,andML
researchoftenemployateamofhumancoderstocompleteground-truthdataannotations(e.g.,classifyingwhether
messagesareriskyornot)throughcollaborativetasksamonghumanannotators[2,7,20],inwhichacodingscheme
is pre-defined, a group ofcodersis trained withthe scheme to reconcile disagreement, and inter-coder agreement
isevaluated[25].Thisoftenincludescrowdsourcedworkers[11],ateamofresearchersandresearchassistants[28,
34],and/ordomainexperts[27].Thisannotationprocessinvolvesanintensiveandcollaborativeprocessoftraining,
consensus-building,andqualitycontrolamongmultiplecoders;therefore,itcanbecostly,time-consuming,andstress-
inducing,whilestillyieldingunevenlevelsofinter-coderagreement[4,29].
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot
madeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforthird-party
componentsofthisworkmustbehonored.Forallotheruses,contacttheowner/author(s).
©2024Copyrightheldbytheowner/author(s).
ManuscriptsubmittedtoACM
1
4202
rpA
11
]CH.sc[
1v62970.4042:viXraCCAI2024,May12,2024,Honolulu,HI,USA JinkyungPark,PamelaWisniewski,andVivekSingh
Alternatively,proxiesareusedtolabelonlineriskcollectivelyinsteadofannotatingindividualriskcases.Forin-
stance, existing literature on misinformation detection that relied on source-level labels assumes that all new arti-
clesfromagivensourcesharethesamecredibilitylevel(reliablevs.unreliable)dependingonthereputationofthe
source[8,10,26].However,inarecentstudy,researchersshowedonlyabout50%alignmentbetweensourceleveland
articlelevel labelsforthecredibilityofpoliticalnews articlesand calledfor approachestobalance thequalityand
quantityofthegroundtruthannotation[27].Assuch,thereisagrowingneedforinnovativeandefficientmethods
tosupporthumancodersinlabelinglargecorporaofonlinedata,whichcanhaveasignificantmethodologicalimpact
onHCIandsocialcomputingresearch.Inthispositionpaper,wediscussthepotentialoftheuseofLargeLanguage
Models(LLMs)-basedConversationalAgents(CAs)asAI-basedco-codersforaccuratelyannotatingonlineriskdata.
OurpositionpaperishighlyrelevanttotheLLMsasResearchToolsworkshopasourfocusonLLMsasresearchtools
tofacilitateonlineriskdataannotationalignsverywellwiththecoretopicofinterestfortheworkshop(i.e.,LLMsto
reshapetheHCIcommunity’ssuiteofmethodsforworkingwithdata).
2 LLMSASCOLLABORATIVEAGENTSFORONLINERISKANNOTATION
AftertherecentreleaseofvariousLargeLanguageModel(LLM)-basedConversationalAgents(CAs)(e.g.,ChatGPT[21]),
researchcommunitiesareincreasinglyexperimentingwithdataannotationtaskssuchasannotatingpoliticalstance
andsentimentoftextualdata[1,15,18,36].EmergingliteraturesuggeststhatLLM-basedCAscanbeusefulfortext
classificationtasks,owingtotheiraccuracyandabilitytoflexiblyadapttotasks.Forinstance,Zhangetal.[36]show
thatChatGPTwasabletoannotatethepoliticalstanceofthetweetswithanaverage accuracyabove70.Similarly,
Aminetal.[1]evaluatedhowaccuratelyChatGPTclassifiessentiment,personality,andsuicideideationingiventexts.
TheresultsshowedthatChatGPToutperformedotherrobustlanguagemodelsforaccuratelyclassifyingthesentiment
ofthetext(i.e.,positive,neutral,andnegativeclasses).Atthesametime,challengeshavebeendocumentedintheuse
ofLLM-based CAsinannotating textualdataformorecontextualizedconstructs.Forinstance, theperformanceof
ChatGPTforclassifyingthefivepersonalityandsuicideideationclassificationswaslowerthantheotherpre-trained
languagemodels(e.g.,BERT)[1].TheobservationsfrompriorstudiesindicatethatChatGPTisageneralistmodelthat
canperformmanydifferenttextclassificationtaskswithoutspecializedtraining,yetdedicatedtrainingisrequiredto
achievegoodresultsonspecifictaskssuchasannotatingcontextualizedonlineriskdata.
ResearchershavealsoexploredthepotentialofLLM-basedCAsastoolsforidentifyingthemesandgroupingthe
textualdataintoidentifiedthemes(e.g.,groundedthematicanalysis)toreducetimeandlaborforsuchanalysis[37].
Forinstance,Zhangetal.developedGPT-baseddataanalysistoolstoidentifymajorthemesintextualdataandhigh-
lightedthepotentialofleveragingLLMsforaugmentingefficiencyinqualitativedataanalysis.Assuch,priorstudies
highlighted thepotentialbenefitsoftheuseofLLMsasAI-based toolstofacilitatetextannotationtasks.However,
mostofthepriorworkshedlightonthepotentialofLLM-based CAsfortextannotationtaskstoalternatehuman
laborforthesametasks;hence,leavingpotentialforco-labelingtaskswithhumancodersunder-explored.Thisgapis
pertinentbecauseco-labelingtasksneedtosupportatwo-wayinteractivediscussionthatcanaddnuanceandcontext,
andhelpgeneratearationaleforthevariousdecisions,particularlyinthecontextofonlineriskbehavior,whichis
highlycontextualinnature[3].Whilemodernconversationalagentshaveshowntheabilitytointeractwithhumans
andworkwithexamples[16,19],theirperformanceincollaborativetextannotationexerciseswheredifferentfacetsof
co-labelingareimportantisasyetunderstudied.Here,weconsiderLLMs-basedCAsaspotentiallyusefulco-labeling
agents,whichcouldsupporthigh-qualitytextannotationswithexplanations.Particularly,wesituatethisdiscussion
inthedataannotationofonlinerisks.
2LeveragingLargeLanguageModels(LLMs)toSupportCollaborativeHuman-AIOnlineRiskDataAnnotaCtiConAI2024,May12,2024,Honolulu,HI,USA
Theconceptofonlineriskishighlysubjectiveandnuanced,andcontextualsensitivityshouldbeconsideredwhen
identifyingonlinerisk[17].Forinstance,oftentimes,socialmediausersusesarcasmorevenmorecreativewaysto
getaroundcontentmoderation[33].Thisphenomenonhasbecomeincreasinglycommoninuser-generatedcontent
onsocialmediaplatforms[31].Hence,newer waystoinnovate andre-thinklabelinginsuchsettingscouldhavea
significantmethodologicalimpactonHCIandsocialcomputingresearch.Toaddresstheissuesofannotatingsubjective
andsubtleonlineriskdatausingLLM-basedCAs,Huangetal.[13]experimentedwiththeChatGPTtoannotatehate
speechintweetsintothreecategories:implicithatespeech,non-hatespeech,anduncertain.Theresultsshowedthat
ChatGPTcorrectlyrecognizedimplicithatespeechin80%ofthetweetswithexplanationsbetterthanthoseprovidedby
humancoders(i.e.,crowdsourcedannotators).TheresultsfrompriorstudiesshedlightonthepotentialofLLM-based
CAsfortextannotationtaskstoalternatehumanlaborforthesametasks.However,onceagain,itshouldbenoted
thattheconceptofriskandannotatingonlineriskcanbehighlydependentonavarietyoffactorsincludingcultural
imprint, personality, politicalorientation, and contextual knowledge, hence, can lead to disagreement even among
human coders.It isimportant thatthis insight doesnot lead to capitulationin the face ofcomplexitybut,instead,
inspiresbettermethodsofautomatedanalysis.Therefore,thecombinationofmanualandautomatedcontentanalysis
issuggested asthegoldstandard foridentifying online risk[6].Here, wedonotconsider LLMs-based CAsaspre-
trainedclassifiersfortextannotationbutratheraspotentiallyusefulco-labelingagents,whichundertherightsettings
offrequentandnuancedinteractioncouldsupporthigh-qualityannotationswithexplanations.Suchanapproachif
feasibleandsuccessful,couldallowforscalabilityincontentanalysis.Further,thiscanopendoorstounderstanding
nuancesthatmightbelostevenbyhumancoders,whoalsohavelimitedworldawarenessandcognitiveabilities.
3 DESIGNCONSIDERATIONS
WeconsiderLLM-basedCAsaspromisingco-annotationpartnersbecausetheyaresystemsenabledwiththeabilityto
interactwiththeusersusingnaturalhumandialogue[30]haveshownpromisingresultsintextannotationtasksdueto
theiraccuracyandadaptability(e.g.,[1,13,15,18,36,37]).Theoverarchingquestionthatweareinterestedinaddressing
is‘‘How canwe designandimplementasystemthat facilitatescollaborative dataannotationbetweenresearchersand
conversationalagents?” WereflectonsomeofthepotentialchallengeswiththeuseofLLMsincontextualizedand/or
sensitiveonlineriskdataannotationtasksandprovidedesignconsiderationsforbuildingLLMs-basedresearchtools
toeffectivelyfacilitatehuman-AIcollaborativeonlineriskdataannotation.
• Interactivity:InteractivityistheheartofLLMs.Partially,interactionbetweenhumansandAIthroughLLMs
helpsalleviateoneofthemajorissuesinherenttoAI-basedsystems,transparency.Tothisend,“Howcanwe
leverageinteractiveconversationbetweenhumansandAItosupporthighlynuancedcontextualizedonlinerisk
dataannotationtasksatscale?”
• Context-Awareness:LLMsworkbytokenizingthetextsintowordsandprocessingthem[23].Therefore,how
theyunderstandriskyinteractioncandifferfromhowhumansdo.Thedifferencecanbepertinent,especiallyfor
onlineriskdataasriskishighlysubjectiveandnuanced.Therefore,understandinghowLLMsprocesstextual
dataandmakesenseofinformationprocessedfromtextualdataisacriticalareaforfurtherexploration.
• PromptDesign:TheresponsesgeneratedbyLLMs-basedmodelsarehighlydependentontheprompts.There-
fore,thequalityofpromptscouldbethefoundationforeffectivehuman-AIcollaborationfordataannotation
tasks.Recently,variouspromptingtuningapproacheshavebeenproposedandexaminedtoelicitresponsesthat
3CCAI2024,May12,2024,Honolulu,HI,USA JinkyungPark,PamelaWisniewski,andVivekSingh
areconsistentandcontext-aware.Therefore,weask“Howcanwedesignpromptsthatcanbestsupporthighly
nuancedandcontextualizedonlineriskdataannotationtasks?”
• Consistency:LLMsareknowntoprovideinconsistent responsesevenwiththesamepromptsgiven forthe
same query, and the reasons why they generate inconsistent responses are still in a black box. One way to
increase consistency is through well-designed promptsgiven for specific tasks. Another optionmight be to
combineresponsesfrommultipleiterationstoidentifyconsistenttrendsintheoutput.Theremainingquestion
hereis“HowcanwedesignLLMsworkflowstogenerateonlinerisklabelswithhighconsistency?”
• UserInterface:TherearepotentialfeaturestomaximizethebenefitofLLMs-basedonlineriskdataannota-
tiontoolsbyhelping(e.g.,featurestofeedinteractionlogsbetweenresearchersandAIintoprompts).Atthe
sametime,acomplexuserinterfacewithadvancedfeaturesmayincreasetheburdenforresearcherstotrain
themselves to use the systems. Moreover, it can make the data annotation process lengthy and prone to er-
rors.Therefore, astreamlined user interfacewhileproviding asetoffeaturestosupporteffective human-AI
collaborationforonlineriskdataannotationtasksisneeded.
• DataPrivacyandSecurity:WhenusingLLMstoprocessonlineriskdata,aprimaryconcernforresearchers
isdataprivacyand security.Forinstance, OpenAI’spoliciesexplicitlystipulatethatdatasubmittedbyusers
throughtheirAPIwillnotbeusedtotraintheirmodels[22].Yet,weacknowledgethattheconversationlogs
betweenresearchers andAIcanpotentiallybestoredintheOpenAIAPIserverforfurtherusage.Therefore,
researchers should also consider building LLM-based data annotation toolswith private servers so that the
trainingdatasetisnotsharedviatheweb.
4 CONCLUSION
OurresearchinterestsalignverywellwiththepurposesoftheLLMsasResearchToolsworkshoptoidentifyongoing
applications and challenges of using LLMs to workwith data in HCI research. We anticipate learning more about
organizers’andparticipants’ground-breakingresearchideastoreshapetheHCIcommunity’smethodsforworking
withdata.Inaddition,participatingintheworkshopwouldbeextremelybeneficialforustohaveadiscourseoncritical
and ethical perspectives of the application ofLLMs in HCI research. While we have identified some ofthe design
condensationsofLLM-baseddataannotationtoolstosupporthuman-AIcollaboration,wehopethatparticipatingin
theworshipwillhelpusaddresssomeoftheremainingchallengesandcomeupwithadditionaldesignimplications.
Finallyattendingtheworkshopwillbeavaluableopportunitytointeractwithandgaininsightsfromorganizersand
participants,whichcouldpotentiallyleadtofuturecollaborationopportunities.
5 ABOUTTHEAUTHORS
JinkyungParkisapostdoctoralscholarintheDepartmentofComputerScienceatVanderbiltUniversity.Herresearch
focusesonHuman-ComputerInteractiontopromoteonlinesafetyforyouthandvulnerablepopulations.
PamelaWisniewskiisanassociateprofessorintheDepartmentofComputerScienceatVanderbiltUniversity.Her
workliesattheintersectionofHuman-ComputerInteraction,SocialComputing,andPrivacy.Herexpertisehelpsher
empowerendusersandteachstudentstounderstandthevalueofuser-centereddesignandevaluation.
Vivek Singhisanassociateprofessor intheSchoolofCommunicationand Informationat RutgersUniversity. He
designsAIsystemsthatareresponsivetohumanvaluesandneeds.
4LeveragingLargeLanguageModels(LLMs)toSupportCollaborativeHuman-AIOnlineRiskDataAnnotaCtiConAI2024,May12,2024,Honolulu,HI,USA
REFERENCES
[1] MostafaMAmin,ErikCambria,andBjörnWSchuller.2023. WillAffectiveComputingEmergeFromFoundationModelsandGeneralArtificial
Intelligence?AFirstEvaluationofChatGPT.IEEEIntelligentSystems38,2(2023),15–23.
[2] NatãMBarbosaandMonchuChen.2019.Rehumanizedcrowdsourcing:Alabelingframeworkaddressingbiasandethicsinmachinelearning.In
Proceedingsofthe2019CHIConferenceonHumanFactorsinComputingSystems.1–12.
[3] JodyClay-Warner.2003.Thecontextofsexualviolence:Situationalpredictorsofself-protectiveactions.Violenceandvictims18,5(2003),543–556.
[4] KevinCoe,KateKenski,andStephenARains.2014.Onlineanduncivil?Patternsanddeterminantsofincivilityinnewspaperwebsitecomments.
JournalofCommunication64,4(2014),658–679.
[5] JohannesDaxenberger,MarcZiegele,IrynaGurevych,andOliverQuiring.2018.Automaticallydetectingincivilityinonlinediscussionsofnews
media.In2018IEEE14thInternationalConferenceone-Science(e-Science).IEEE,318–319.
[6] KatharinaEsau.2022. ContentAnalysisintheResearchFieldofIncivilityandHateSpeechinOnlineCommunication. InStandardisierteIn-
haltsanalyseinderKommunikationswissenschaft–StandardizedContentAnalysisinCommunicationResearch:EinHandbuch-AHandbook.Springer
FachmedienWiesbadenWiesbaden,451–461.
[7] RStuartGeiger,KevinYu,YanlaiYang,MindyDai,JieQiu,RebekahTang,andJennyHuang.2020.Garbagein,garbageout?Domachinelearning
applicationpapersinsocialcomputingreportwherehuman-labeledtrainingdatacomesfrom?.InProceedingsofthe2020ConferenceonFairness,
Accountability,andTransparency.325–336.
[8] NirGrinberg,KennethJoseph,LisaFriedland,BrionySwire-Thompson,andDavidLazer.2019.FakenewsonTwitterduringthe2016USpresidential
election.Science363,6425(2019),374–378.
[9] Soo-HyeHan,LeAnnMBrazeal,andNataliePennington.2018. Iscivilitycontagious?Examiningtheimpactofmodelinginonlinepolitical
discussions.SocialMedia+Society4,3(2018),2056305118793404.
[10] BenjaminDHorne,WilliamDron,SaraKhedr,andSibelAdali.2018. Assessingthenewslandscape:Amulti-moduletoolkitforevaluatingthe
credibilityofnews.InCompanionProceedingsoftheTheWebConference2018.235–238.
[11] HomaHosseinmardi,SabrinaArredondoMattson,RahatIbnRafiq,RichardHan,QinLv,andShivakantMishra.2015.Analyzinglabeledcyberbul-
lyingincidentsontheinstagramsocialnetwork.InInternationalconferenceonsocialinformatics.Springer,49–66.
[12] MarkHsueh,KumarYogeeswaran,andSannaMalinen.2015. “Leaveyourcommentbelow”:Canbiasedonlinecommentsinfluenceourown
prejudicialattitudesandbehaviors?Humancommunicationresearch41,4(2015),557–576.
[13] FanHuang,HaewoonKwak,andJisunAn.2023. Ischatgptbetterthanhumanannotators?potentialandlimitationsofchatgptinexplaining
implicithatespeech.arXivpreprintarXiv:2302.07736(2023).
[14] SeunghyunKim,AfsanehRazi,GianlucaStringhini,PamelaJWisniewski,andMunmunDeChoudhury.2021.YouDon’tKnowHowIFeel:Insider-
OutsiderPerspectiveGapsinCyberbullyingRiskDetection.InProceedingsoftheInternationalAAAIConferenceonWebandSocialMedia,Vol.15.
290–302.
[15] TajaKuzman,IgorMozetic,andNikolaLjubešic.2023.Chatgpt:Beginningofanendofmanuallinguisticdataannotation?usecaseofautomatic
genreidentification.ArXiv,abs/2303.03953(2023).
[16] VivianLai,SamuelCarton,RajatBhatnagar,QVeraLiao,YunfengZhang,andChenhaoTan.2022.Human-aicollaborationviaconditionaldelega-
tion:Acasestudyofcontentmoderation.InProceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems.1–18.
[17] Anna Litvinenko. 2023. The role of context in incivility research. In Challenges and perspectives of hate speech research, Chris-
tian Strippel, Sünje Paasch-Colberg, Martin Emmer, and Joachim Trebbe (Eds.). Digital Communication Research, Vol. 12. Berlin, 73–85.
https://doi.org/10.48541/dcr.v12.5
[18] YihengLiu,TianleHan,SiyuanMa,JiayueZhang,YuanyuanYang,JiamingTian,HaoHe,AntongLi,MengshenHe,ZhengliangLiu,etal.2023.
SummaryofChatGPT-RelatedResearchandPerspectiveTowardstheFutureofLargeLanguageModels.Meta-Radiology(2023),100017.
[19] MaximilianMackeprang,ClaudiaMüller-Birn,andMaximilianTimoStauss.2019.Discoveringthesweetspotofhuman-computerconfigurations:
Acasestudyininformationextraction.ProceedingsoftheACMonHuman-ComputerInteraction3,CSCW(2019),1–30.
[20] MichaelMuller,ChristineTWolf,JoshAndres,MichaelDesmond,NarendraNathJoshi,ZahraAshktorab,AabhasSharma,KristinaBrimijoin,Qian
Pan,EvelynDuesterwald,etal.2021. Designinggroundtruthandthesociallifeoflabels.InProceedingsofthe2021CHIConferenceonHuman
FactorsinComputingSystems.1–16.
[21] OpenAI.2023.IntroducingChatGPT. https://openai.com/blog/chatgpt
[22] OpenAI.2024.Security&Privacy. https://openai.com/security
[23] OpenAI.2024.Whataretokensandhowtocountthem? https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
[24] KadirBulutOzler,KateKenski,SteveRains,YotamShmargad,KevinCoe,andStevenBethard.2020.Fine-tuningformulti-domainandmulti-label
uncivillanguagedetection.InProceedingsoftheFourthWorkshoponOnlineAbuseandHarms.28–33.
[25] CliodhnaO’ConnorandHeleneJoffe.2020.Intercoderreliabilityinqualitativeresearch:debatesandpracticalguidelines.Internationaljournalof
qualitativemethods19(2020),1609406919899220.
[26] JinkyungPark,RahulEllezhuthil,RamanathanArunachalam,LaurenFeldman,andVivekSingh.2022.TowardFairnessinMisinformationDetection
Algorithms.InWorkshopProceedingsofthe16thInternationalAAAIConferenceonWebandSocialMedia.Retrievedfromhttps://doi.org/10.36190.
5CCAI2024,May12,2024,Honolulu,HI,USA JinkyungPark,PamelaWisniewski,andVivekSingh
[27] JinkyungPark,RahulDevEllezhuthil,JosephIsaac,ChristophMergerson,LaurenFeldman,andVivekSingh.2023. MisinformationDetection
AlgorithmsandFairnessacrossPoliticalIdeologies:TheImpactofArticleLevelLabeling.InProceedingsofthe15thACMWebScienceConference
2023.107–116.
[28] JinkyungPark,JoshuaGracie,AshwaqAlsoubai,GianlucaStringhini,VivekSingh,andPamelaWisniewski.2023.TowardsAutomatedDetection
ofRiskyImagesSharedbyYouthonSocialMedia.InCompanionProceedingsoftheACMWebConference2023.1348–1357.
[29] StephenARains,KateKenski,KevinCoe,andJakeHarwood.2017.IncivilityandpoliticalidentityontheInternet:Intergroupfactorsaspredictors
ofincivilityindiscussionsofnewsonline.JournalofComputer-MediatedCommunication22,4(2017),163–178.
[30] MinjinRheu,JiYounShin,WeiPeng,andJinaHuh-Yoo.2021.Systematicreview:Trust-buildingfactorsandimplicationsforconversationalagent
design.InternationalJournalofHuman–ComputerInteraction37,1(2021),81–96.
[31] SergioRojas-Galeano.2017.Onobstructingobscenityobfuscation.ACMTransactionsontheWeb(TWEB)11,2(2017),1–24.
[32] LeonieRösnerandNicoleCKrämer.2016.Verbalventinginthesocialweb:Effectsofanonymityandgroupnormsonaggressivelanguageusein
onlinecomments.SocialMedia+Society2,3(2016),2056305116664220.
[33] FarigSadeque,StephenRains,YotamShmargad,KateKenski,KevinCoe,andStevenBethard.2019. Incivilitydetectioninonlinecomments.In
Proceedingsoftheeighthjointconferenceonlexicalandcomputationalsemantics(*SEM2019).283–291.
[34] VivekKSingh,SouvickGhosh,andChristinJose.2017. Towardmultimodalcyberbullyingdetection.InProceedingsofthe2017CHIConference
ExtendedAbstractsonHumanFactorsinComputingSystems.2090–2099.
[35] AnkeStoll,MarcZiegele,andOliverQuiring.2020. Detectingimpolitenessandincivilityinonlinediscussions:Classificationapproachesfor
Germanusercomments.ComputationalCommunicationResearch2,1(2020),109–134.
[36] BowenZhang,DaijunDing,andLiwenJing.2022. Howwouldstancedetectiontechniquesevolveafterthelaunchofchatgpt? arXivpreprint
arXiv:2212.14548(2022).
[37] HeZhang,ChuhaoWu,JingyiXie,ChanMinKim,andJohnMCarroll.2023.QualiGPT:GPTasaneasy-to-usetoolforqualitativecoding. arXiv
preprintarXiv:2310.07061(2023).
[38] AdamGZimmermanandGabrielJYbarra.2016.Onlineaggression:Theinfluencesofanonymityandsocialmodeling.PsychologyofPopularMedia
Culture5,2(2016),181.
6