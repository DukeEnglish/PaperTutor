GoMAvatar: Efficient Animatable Human Modeling from Monocular Video
Using Gaussians-on-Mesh
JingWen XiaomingZhao ZhongzhengRen AlexanderG.Schwing ShenlongWang
UniversityofIllinoisUrbana-Champaign
{jw116, xz23, zr5, aschwing, shenlong}@illinois.edu
https://wenj.github.io/GoMAvatar/
43FPS
t<latexit sha1_base64="4mSRiAOC1HPbUsbyd7QN48TyFAA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNnGqGW+xWMa6E1DDpVC8hQIl7ySa0yiQ/CEY3878hyeujYjVPU4S7kd0qEQoGEUrNbFfrrhVdw6ySrycVCBHo1/+6g1ilkZcIZPUmK7nJuhnVKNgkk9LvdTwhLIxHfKupYpG3PjZ/NApObPKgISxtqWQzNXfExmNjJlEge2MKI7MsjcT//O6KYbXfiZUkiJXbLEoTCXBmMy+JgOhOUM5sYQyLeythI2opgxtNiUbgrf88ippX1S9y2qtWavUb/I4inACp3AOHlxBHe6gAS1gwOEZXuHNeXRenHfnY9FacPKZY/gD5/MH4xeNAQ==</latexit>
Single Video Gaussians-on-Mesh Animatable Real-time Explicit Geometry
Figure1. GoMAvatartakesamonocularRGBvideo(left)asinputtoestablishanexplicitandaccurate4Drepresentationofadynamic
human. Itcanrenderefficientlyatnovelviewsandposeswithstate-of-the-artquality. Additionally,itisextremelycompact(3.63MBper
subject),efficient(43FPS),andseamlesslycompatiblewiththegraphicspipelinesuchasOpenGL.
Abstract AR/VR, and simulation. Conventional approaches carried
outinMotionCapture(MoCap)studiosareslow,expensive,
We introduce GoMAvatar, a novel approach for real- and cumbersome, due to costly wearable devices [42, 52]
time, memory-efficient, high-quality animatable human andintricatemulti-viewcamerasystems[28,74].Hence,to
modeling. GoMAvatar takes as input a single monocular enablewidespreadpersonaluse,affordablemethodswhich
video to create a digital avatar capable of re-articulation only rely on monocular RGB videos for creating digital
in new poses and real-time rendering from novel view- avatarsaremuchdesired.
points, while seamlessly integrating with rasterization-
Reconstruction of digital humans from monocular
based graphics pipelines. Central to our method is the
videos has been studied intensively recently [16, 25, 64,
Gaussians-on-Mesh (GoM) representation, a hybrid 3D
70, 81]. The key lies in choosing a suitable 3D represen-
modelcombiningrenderingqualityandspeedofGaussian
tation, flexible for articulation, efficient for rendering and
splatting with geometry modeling and compatibility of de-
storage,andcapableofmodelinghigh-qualitygeometryand
formable meshes. We assess GoMAvatar on ZJU-MoCap,
appearance all while being easily integrated into graphics
PeopleSnapshot, and various YouTube videos. GoMA-
pipelines. Despite various proposals, no animated 3D rep-
vatarmatchesorsurpassescurrentmonocularhumanmod-
resentation has met all these needs. Neural fields based
elingalgorithmsinrenderingqualityandsignificantlyout-
avatars [16, 27, 70, 81] offer photorealism, but they are
performs them in computational efficiency (43 FPS) while
challenging to articulate and lack explicit geometry, mak-
beingmemory-efficient(3.63MBpersubject).
ing them less compatible with game engines. Mesh-based
methods [58] excel in articulation and rendering but fall
shortinmodelingtopologicalchangesandhigh-qualityap-
1.Introduction
pearance. Point-basedmethods[88]arelimitedbyincom-
High-fidelity, animatable digital avatar modeling is crucial plete topology and surface geometry. Recent successes of
forvariousapplicationssuchasmoviemaking,healthcare, Gaussian splatting in neural rendering motivate extensions
4202
rpA
11
]VC.sc[
1v19970.4042:viXra
…
…tofree-formdynamicscenes[73],butaknowledgegapex-
30.5
istsinhowtoleverageGaussiansforarticulatablehumans.
GoMAvatar (Ours)
Besides, the lack of explicit surface modeling of Gaussian
30.0
splatshinderstheirbroaderuseindigitalavatarmodeling. MonoHuman
To address these challenges, we present GoMAvatar, a 29.5
novel digital avatar modeling framework. GoMAvatar op- HumanNeRF
eratesonasinglemonocularvideoandyieldsanarticulated
29.0
character that encodes high-fidelity appearance and geom- NeuralBody NeuMan
etry. It is both articulable and memory-efficient, render-
28.5
ing in real-time (see Fig. 1). Central to the framework is
30 100 500 2000 12000
a novel articulated human representation, which we refer
Runtime (ms) [log scale]
to as Gaussians-on-Mesh (GoM) (Sec. 3.1). GoM com- Figure 2. Our approach is simultaneously faster (represented
bines rendering quality and speed of Gaussian splatting by x coordinates of circle centers , smaller is better), memory-
with geometry modeling and compatibility of deformable efficient(representedbycirclesize,smallerisbetter),andrenders
meshes. Specifically, GoM employs Gaussian splats for atahigherquality(representedbyycoordinatesofcirclecenters,
rendering,offeringflexibilityinmodelingrichappearances higherisbetter).ThehorizontalbrownlinedenotesourPSNR.
and enabling real-time performance (Sec. 3.2). GoM uti- 2.RelatedWork
lizes a skeleton-driven deformable mesh, enabling the cre-
ation of compact, topologically complete digital avatars, Representations for novel view synthesis. Several rep-
whileeasingmesharticulationthroughforwardkinematics resentationshavebeenproposedforthetaskofnovelview
(Sec. 3.3). Crucially, to integrate both representations, we synthesis,suchaslightfields[3,17,36],layeredrepresenta-
attach a Gaussian to each mesh face. This method differs tions[61,62,71,90],voxels[41,63],andmeshes[18,23].
from traditional mesh techniques that rely on texturing or Recently, several works also demonstrated the effective-
vertex coloring to enhance rendering. It also differs from ness of an implicit representation, i.e., a neural network,
standardfreeformGaussiansplats,therebybetterregulariz- for a scene [12, 47, 51]. Further, neural radiance fields
ingGaussiansfornovelposes. Furthermore,totackleview (NeRFs) [49] utilize a volume rendering equation [29] to
dependency,wefactorizethefinalRGBcolorintoapseudo optimize the implicit representation, yielding high-quality
albedomaprenderingandapseudoshadingmapprediction. view synthesis. Follow-up works further improve and
Thisentirerepresentationcanbeinferredfromasinglein- demonstratecompellingrenderingresults[4–6,46,57,67,
put video without additional training data (Sec. 3.5). We 82]. Meanwhile, other works use volume rendering equa-
findthisdualrepresentationtobalanceperformanceandef- tions to optimize more explicit representations [7, 50, 80],
ficiency effectively. Importantly, the entire animation and largely accelerating the optimization procedure. Point-
rendering of GoM are fully compatible with graphics en- based rendering (e.g., Gaussian splatting [30, 45, 72]) has
gines,suchasOpenGL. recently been adopted for fast rendering. It models the
scenes as a set of 3D Gaussians, each equipped with ro-
WeconductedextensiveexperimentsontheZJU-MoCap
tation, scale, and appearance-related features, and raster-
data [54], PeopleSnapshot [1] and YouTube videos. Go-
izesbyprojectingthe3DGaussianstothe2Dimageplane.
MAvatar matches or surpasses the rendering quality of
To model dynamic scenes, [45, 72] further extend the 3D
the best monocular human modeling algorithms (GoMA-
Gaussians,addingatimedependency. Toregularizethe3D
vatar reaches 30.37 dB PSNR in novel view synthesis and
Gaussians through time, [45] adds physically-based priors
30.31 dB PSNR in novel pose synthesis). Meanwhile, it
during training, and [72] uses a neural network to predict
is faster than competing algorithms, reaching a rendering
the deformation of Gaussians. Our approach is inspired
speed of 43 FPS on an NVIDIA A100 GPU and remains
by the recent progress of point-based rendering to facili-
compact in memory, only costing 3.63 MB per subject
tatefastrendering. Moreconcretely,wealsouseGaussian
(Fig.2). Tosummarize,ourmaincontributionsare:
splatting for rendering. However, different from previous
• We introduce the Gaussians-on-Mesh representation for approaches, we propose the Gaussians-on-Mesh represen-
efficient, high-fidelity articulated human reconstruction tationthatcombines3DGaussianswithameshrepresenta-
fromasinglevideo,combiningGaussiansplatswithde- tion. Bydoingso,weobtainfastrenderingspeedaswellas
formablemeshesforreal-time,free-viewpointrendering. regularizeddeformationof3DGaussians.
• We design a unique differentiable shading module for Human modeling. Early works to model humans rely on
view dependency, splitting color into a pseudo albedo templates, e.g., SCAPE [2] and SMPL [43]. Later, [56,
map from Gaussian splatting and a pseudo shading map 59, 60, 86] utilize (pixel-aligned) image features to re-
derivedfromthenormalmap. construct human geometry and appearance from a single
)bd(
RNSPimage. However, such human modeling is not animat- G<latexit sha1_base64="JifhsSvrSYXmUYLoQ6J9mMjCPTw=">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFF7oRKtgHtNOSSdM2NJkMSUYpQ//DjQtF3Pov7vwbM+0stPVA4HDOvdyTE0ScaeO6305uZXVtfSO/Wdja3tndK+4fNLSMFaF1IrlUrQBryllI64YZTluRolgEnDaD8XXqNx+p0kyGD2YSUV/gYcgGjGBjpW5HYDNSIrmRd9Ou7BVLbtmdAS0TLyMlyFDrFb86fUliQUNDONa67bmR8ROsDCOcTgudWNMIkzEe0ralIRZU+8ks9RSdWKWPBlLZFxo0U39vJFhoPRGBnUxT6kUvFf/z2rEZXPoJC6PY0JDMDw1ijoxEaQWozxQlhk8swUQxmxWREVaYGFtUwZbgLX55mTTOyt55uXJfKVWvsjrycATHcAoeXEAVbqEGdSCg4Ble4c15cl6cd+djPppzsp1D+APn8wfTL5K8</latexit> oMo
able. ARCH [19, 76] and S3 [77] incorporate reanima-
tioncapabilitiesbuttheyfallshortindeliveringhigh-quality
<latexit sha1_base64="1ujeAb301Ai+7g8rSYIdQcobxpg=">AAACIXicbZDLSgMxFIYzXut4G3XpJlgEF1JmStEui7pwWcFeoB2HTJppYzPJkGSEMsyruPFV3LhQpDvxZUwvC239IfDznXM4OX+YMKq0635ZK6tr6xubhS17e2d3b985OGwqkUpMGlgwIdshUoRRThqaakbaiSQoDhlphcPrSb31RKSigt/rUUL8GPU5jShG2qDAqSYPIsi6N4RpFGSP526e2wvIW0blPA+coltyp4LLxpubIpirHjjjbk/gNCZcY4aU6nhuov0MSU0xI7ndTRVJEB6iPukYy1FMlJ9NL8zhqSE9GAlpHtdwSn9PZChWahSHpjNGeqAWaxP4X62T6qjqZ5QnqSYczxZFKYNawElcsEclwZqNjEFYUvNXiAdIIqxNqLYJwVs8edk0yyXvolS5qxRrV/M4CuAYnIAz4IFLUAO3oA4aAINn8ArewYf1Yr1Zn9Z41rpizWeOwB9Z3z+8tKPg</latexit>po po po
rendering. Recently, efforts on human geometry model-      
j,0 j,1 j,2
ing exploit implicit representations [10, 11, 24, 48, 66]. µ<latexit sha1_base64="Ny2xs9CdII+geSDQTigyTaeQ9m4=">AAAB+HicbVDLSsNAFJ34rPXRqEs3g0VwISWRoi6LblxWtA9oQphMJ+20M5MwD6GWfokbF4q49VPc+TdO2yy09cCFwzn3cu89ccao0p737aysrq1vbBa2its7u3sld/+gqVIjMWnglKWyHSNFGBWkoalmpJ1JgnjMSCse3kz91iORiqbiQY8yEnLUEzShGGkrRW4p4CYanMHgnvY4igaRW/Yq3gxwmfg5KYMc9cj9CropNpwIjRlSquN7mQ7HSGqKGZkUA6NIhvAQ9UjHUoE4UeF4dvgEnlilC5NU2hIaztTfE2PElRrx2HZypPtq0ZuK/3kdo5OrcExFZjQReL4oMQzqFE5TgF0qCdZsZAnCktpbIe4jibC2WRVtCP7iy8ukeV7xLyrVu2q5dp3HUQBH4BicAh9cghq4BXXQABgY8AxewZvz5Lw4787HvHXFyWcOwR84nz8JTJKz</latexit>µ<latexit sha1_base64="Ny2xs9CdII+geSDQTigyTaeQ9m4=">AAAB+HicbVDLSsNAFJ34rPXRqEs3g0VwISWRoi6LblxWtA9oQphMJ+20M5MwD6GWfokbF4q49VPc+TdO2yy09cCFwzn3cu89ccao0p737aysrq1vbBa2its7u3sld/+gqVIjMWnglKWyHSNFGBWkoalmpJ1JgnjMSCse3kz91iORiqbiQY8yEnLUEzShGGkrRW4p4CYanMHgnvY4igaRW/Yq3gxwmfg5KYMc9cj9CropNpwIjRlSquN7mQ7HSGqKGZkUA6NIhvAQ9UjHUoE4UeF4dvgEnlilC5NU2hIaztTfE2PElRrx2HZypPtq0ZuK/3kdo5OrcExFZjQReL4oMQzqFE5TgF0qCdZsZAnCktpbIe4jibC2WRVtCP7iy8ukeV7xLyrVu2q5dp3HUQBH4BicAh9cghq4BXXQABgY8AxewZvz5Lw4787HvHXFyWcOwR84nz8JTJKz</latexit> jj,,⌃⌃ jj
Their use of 3D scans also limits their application. To
address this limitation, human modeling from videos has
received a lot of attention from the community: many <latexit sha1_base64="1ujeAb301Ai+7g8rSYIdQcobxpg=">AAACIXicbZDLSgMxFIYzXut4G3XpJlgEF1JmStEui7pwWcFeoB2HTJppYzPJkGSEMsyruPFV3LhQpDvxZUwvC239IfDznXM4OX+YMKq0635ZK6tr6xubhS17e2d3b985OGwqkUpMGlgwIdshUoRRThqaakbaiSQoDhlphcPrSb31RKSigt/rUUL8GPU5jShG2qDAqSYPIsi6N4RpFGSP526e2wvIW0blPA+coltyp4LLxpubIpirHjjjbk/gNCZcY4aU6nhuov0MSU0xI7ndTRVJEB6iPukYy1FMlJ9NL8zhqSE9GAlpHtdwSn9PZChWahSHpjNGeqAWaxP4X62T6qjqZ5QnqSYczxZFKYNawElcsEclwZqNjEFYUvNXiAdIIqxNqLYJwVs8edk0yyXvolS5qxRrV/M4CuAYnIAz4IFLUAO3oA4aAINn8ArewYf1Yr1Zn9Z41rpizWeOwB9Z3z+8tKPg</latexit>po po po
     
prior efforts utilize implicit representations and differen- <latexit sha1_base64="1ujeAb301Ai+7g8rSYIdQcobxpg=">AAACIXicbZDLSgMxFIYzXut4G3XpJlgEF1JmStEui7pwWcFeoB2HTJppYzPJkGSEMsyruPFV3LhQpDvxZUwvC239IfDznXM4OX+YMKq0635ZK6tr6xubhS17e2d3b985OGwqkUpMGlgwIdshUoRRThqaakbaiSQoDhlphcPrSb31RKSigt/rUUL8GPU5jShG2qDAqSYPIsi6N4RpFGSP526e2wvIW0blPA+coltyp4LLxpubIpirHjjjbk/gNCZcY4aU6nhuov0MSU0xI7ndTRVJEB6iPukYy1FMlJ9NL8zhqSE9GAlpHtdwSn9PZChWahSHpjNGeqAWaxP4X62T6qjqZ5QnqSYczxZFKYNawElcsEclwZqNjEFYUvNXiAdIIqxNqLYJwVs8edk0yyXvolS5qxRrV/M4CuAYnIAz4IFLUAO3oA4aAINn8ArewYf1Yr1Zn9Z41rpizWeOwB9Z3z+8tKPg</latexit>po pjo,0 pjo,1 j,2
     
j,0 j,1 j,2
tiable renderers for either non-animatable [54] or animat-
able[16,22,25,37,39,53,55,64,69,70,75,81,83,89]
Figure3.Gaussians-on-Mesh(GoM).WelearnGaussiansinthe
scene-specifichumanmodelingwhileothereffortsfocuson
localcoordinatesofeachtriangleandtransformthemtotheworld
scene-agnosticmodeling[9,14,21,31,34,35,56,85,87].
coordinatebasedonthetriangle’sshape.Weinitializetherotation
Inthisstudy, ourapproachfocuseson scene-specific mod- r ∈so(3)tozerosandscales ∈R3toonessothatwestart
θ,j θ,j
eling following prior works. Different from the common withaGaussianthat’sthinalongthenormalaxisofthetriangle.
pure implicit representations, we utilize an explicit repre- Meanwhile,theprojectionoftheellipsoid{x:(x−µ )TΣ−1(x−
j j
sentationtermedGaussians-on-Mesh. Theexplicitcanoni- µ )=1}onthetrianglerecoverstheSteinerellipse.SeeSec.3.1
j
calgeometryenablesustoapplywell-definedforwardkine- andtheappendixfordetails.
matics,suchaslinearblendskinning,totransformfromthe
subject of interest, we aim to learn a canonical Gaussians-
canonicalspacetotheobservationspace. Incontrast,meth- on-MeshrepresentationGoMc suchthatwecanrenderthat
ods using implicit representations can only perform map- humaninreal-timegivenanyθ cameraintrinsicsK ∈R3×3,
pinginabackwardmanner,i.e.,fromtheobservationspace
extrinsicsE ∈SE(3),andahumanposeP.Note,hereand
tothecanonicalspace,whichisinherentlyill-posedandam-
below, parameters θ indicate that the corresponding func-
biguous.
tion or variable is learnable and superscript c indicates the
Real-time rendering of animatable human modeling. canonical pose space. To render, we first articulate GoMc
The key to real-time rendering in our approach is the co- θ
totheobservationspacetoobtain
design of an explicit geometry representation and rasteri-
zation: Gaussiansplattingandmeshrasterizationarefaster GoMo =Articulator (GoMc,P), (1)
than volume rendering in general. This principle has been θ θ
explored by prior efforts to accelerate the rendering of where GoMo denotes the Gaussians-on-Mesh representa-
general-purpose NeRFs. Representative approaches pro-
tion in the observation space. To obtain a rendering with
pose to either bake [20, 78, 79] or cache [15] the trained
resolutionH ×W,weformulateaneuralrenderertoyield
implicitrepresentation.Anotherlineofworkexploitsmesh- the human appearance I ∈ RH×W×3 and the alpha mask
basedrasterizationtoboosttheinferencespeed[13,38,78]. M ∈RH×W×1. Formally,
Inspiredbythesuccess,concurrentworksexploreefficient
NeRF rendering for humans [16, 58]. Note, [58] firstly (I,M)=Renderer (K,E,GoMo). (2)
θ
trainsaNeRFrepresentationandthenbakesitintoamesh
forreal-timerendering. However, thesecondbakingstage The final rendering is obtained from a classical alpha-
isshowntoharmtherenderingquality. Incontrast,thepro- composition based on I and M. We will first discuss
posedGaussians-on-Meshrepresentationistrainedend-to- thedetailsoftheGaussians-on-Meshhumanrepresentation
end,achievingasuperiorquality-speedtrade-off. inSec.3.1andtherenderingpipelineinSec.3.2. Thenwe
introduce how to articulate the Gaussians-on-Mesh repre-
3.Gaussians-on-Mesh(GoM)
sentationinSec.3.3.
Inthefollowing,wepresenttheGaussians-on-Mesh(GoM)
3.1.Gaussians-on-MeshRepresentation
representation, how to render it, and its articulation. The
goal of the proposed representation is to combine the ben- ThecoreofourapproachistheGaussians-on-Mesh(GoM)
efits of both Gaussian splatting and meshes while alleviat- representationinthecanonicalspace.Thedesignoftherep-
ing some of their individual shortcomings. Concretely, by resentationismotivatedbythefollowingtwokeyconsidera-
usingGaussiansplatting,weattainahigh-qualityreal-time tions:1)GoMcanberenderedefficientlythroughGaussian
renderingcapability,achieving43FPS.Byutilizingamesh, splatting [30] which eliminates the need of dense samples
weconducteffectivearticulationinaforwardmannerwhile alongraysusedinvolumerendering;2)ByattachingGaus-
alsoregularizingtheunderlyinggeometry. sians to a mesh, we effectively adapt the shapes of Gaus-
Overview. Given a monocular video capturing a human sianstodifferenthumanposesandenableregularization.Formally,ourcanonicalGaussians-on-Meshrepresenta- Ingreaterdetail,Gaussiansplattingisusedtorenderthe
tion is specified via a collection of points and faces with pseudo albedo map I , specified in Eq. (6), and the sub-
GS
associatedattributes: ject mask M, specified in Eq. (2). To obtain the pseudo
shading map S, specified in Eq. (6), we use the normal
GoMc
θ
≜{{v θc ,i}V i=1,{f θ,j}F j=1}. (3) map N
mesh
obtained via standard mesh rasterization. Dur-
ing training, we also use the subject mask M which is
mesh
Here, {vc }V and {f }F represent V vertices and F obtained through the SoftRasterizer [40]. We now discuss
θ,i i=1 θ,j j=1
trianglefacesalongwiththeirrelatedattributesrespectively. thecomputationofI andS.
GS
Wefurtherdefineavertexas Pseudo albedo map I rendering. We render I and
GS GS
M withGaussiansplattinggivenF Gaussiansintheworld
v θc ,i =(pc θ,i,w i), (4) coordinate system {G j ≜ N(µ j,Σ j)}F j=1 and the corre-
spondingcolors{c }F whicharedefinedinEq.(5). F
wherepc ∈R3isthevertexcoordinateandw ∈RJ refers θ,j j=1
θ,i i indicatesthenumberoffaces.
tothelinearblendskinningweightswithrespecttoJ joints.
Importantly, different from the original 3D Gaussian
Wedefinethefaceas
splatting that directly learns Gaussian parameters within
the world coordinate system, we acquire these parame-
f =(r ,s ,c ,{∆ }3 ). (5)
θ,j θ,j θ,j θ,j j,k k=1 ters within the local coordinate frame of each triangle
face. Subsequently, we transform these local Gaussians
r ∈ so(3) and s ∈ R3 define the rotation and scale
θ,j θ,j into the world coordinate system, taking into account the
of the local Gaussian associated with a face. Further,
deformations of the individual faces. This distinctive for-
c ∈ R3 is the color vector. {∆ }3 are the in-
θ,j j,k k=1 mulation allows our Gaussian representation to dynami-
dicesofthethreeverticesbelongingtothej-thface,where
cally adapt to the varying shapes of triangles, which can
∆ ∈ {1,...,V}. Note that we associate Gaussian pa-
j,k change across different human poses due to articulation.
rameterswithfaces. Wewilldelveintothederivationofthe
Concretely, given a face and its local parameters f =
Gaussiandistributionsintheworldcoordinatesforrender- θ,j
(r ,s ,c ,{∆ }3 ), the mean µ of a Gaussian in
inginthefollowingsection. θ,j θ,j θ,j j,k k=1 j
worldcoordinatesisthecentroidoftheface,i.e.,
3.2.Rendering
3
1(cid:88)
µ = po . (8)
Incontrasttodirectlycomputingthefinalcolorasdoneby j 3 ∆j,k
k=1
prior monocular human rendering works [70, 81], render-
ing of the Gaussians-on-Mesh representation decomposes po is the coordinate of the triangle’s vertex. The Gaus-
∆j,k
the RGB image I into the pseudo albedo map I and the sian’scovarianceis
GS
pseudoshadingmapS,i.e.,thefinalimageIisgivenby
Σ =A (R S STRT)AT. (9)
j j j j j j j
I =I ·S. (6)
GS R andS arethematricesencodingrotationr andscale
j j θ,j
s . A isthetransformationmatrixfromlocalcoordinates
Here, I is rendered by Gaussian splatting and S is pre- θ,j j
GS toworldcoordinateswhichisafunctionofthefacevertices,
dictedfromthenormalmapobtainedfrommeshrasteriza-
i.e., A = T({po }3 ). We provide a detailed deriva-
tion. We find this combination of Gaussian splatting and j ∆j,k k=1
tionofA inthesupplementarymaterial. ThroughEq.(8)
mesh rasterization to better capture view-dependent shad- j
and(9),Gaussiansaredynamicallyadaptedtotheshapesof
ing effects than each individual approach while retaining
trianglesofdifferenthumanposes.
efficiency. We use ‘pseudo’ because the decomposition is
Pseudo shading map S prediction. For view-dependent
not perfect. Even though, we will show that the pseudo
shading effects, we predict the pseudo shading map from
shadingmapencodeslightingeffectstosomeextent.
themeshrasterizednormalmapN via
mesh
WeemphasizethatrenderingoperatesontheGoMrep-
resentation in the observation space (see Eq. (2)), i.e., on RH×W×1 ∋S =Shading (γ(N )). (10)
θ mesh
Hereγ(·)denotesthepositionalencoding[49]. Shading
θ
GoMo ≜{{(po,w )}V ,{(r ,s ,c )}F }. (7) isa1×1convolutionalnetworkthatmapseachpixeltoa
i i i=1 θ,j θ,j θ,j j=1
scalingfactor.
Note,theonlydifferencebetweenGoMoandGoMcdefined
θ 3.3.Articulation
inEq.(3)istheuseofobservationspacevertexcoordinates
po.Sec.3.3willprovidemoredetailsabouthowtocompute Different from NeRF-based approaches [16, 70, 81] that
i
pofromthevertexcoordinatesincanonicalspacepc . require the ill-posed backward mapping from observation
i θ,ispacetocanonicalspace,ourarticulationfollowsthemesh’s 3.5.Training
forward articulation,i.e.,fromcanonicalspacetoobserva-
WesupervisethepredictedRGBimageI andsubjectmask
tionspace,takingadvantageofourGaussians-on-Meshrep-
M withground-truthI andM . Ouroveralllossis
resentation. gt gt
ThegoalofthearticulatordefinedinEq.(1)istoobtain
L=L +α L +α L +α L . (14)
I lpips lpips M M reg reg
theGaussians-on-Meshrepresentationinobservationspace,
i.e.,GoMo (seeEq.(7)),giventhecanonicalrepresentation Here,α areweightsforlosses. L andL aretheL1loss
∗ I M
GoMc andahumanposeP. Note, weonlytransformpc on the RGB images and subject masks respectively. L
θ θ,i lpips
topoasalltheotherattributesareshared. istheLPIPSloss[84]betweenpredictedRGBimageI and
i
To transform, linear blend skinning (LBS) is applied ground-truth I . We add additional regularization on the
gt
to warp the vertices to the observation space. For pose- underlyingmeshviaL =
reg
dependent non-rigid motion, we utilize a non-rigid motion
module to deform the canonical vertices before applying L mask+α lapL lap+α normalL normal+α colorL color. (15)
LBS. We refer to the space after non-rigid deformation as
‘thenon-rigidlytransformedcanonicalspace’. L mask = ∥M mesh −M gt∥ is the regularization on the mesh
Linear blend skinning. We adhere to the standard linear silhouette. L lap = N1 (cid:80)N i=1∥δ i∥2 istheLaplaciansmooth-
blend skinning for the transformation of vertices from the ingloss,whereδ iistheLaplaciancoordinateofthei-thver-
non-rigidly transformed canonical space into the observa- tex. L normal is the normal consistency loss that maximizes
tionspaceasR3 ∋po = the cosine similarity of adjacent face normals. Similar to
i
the normal consistency, we apply a color smoothness loss
LBS(pnr,w ,P)=
(cid:80)J j=1w ij(R jppn ir+tp j)
. (11)
denotedasL color,whichpenalizesthedifferencesincolors
i i (cid:80)J wk betweentwoadjacentfaces.
k=1 i
WeinitializetheverticesandfaceswithSMPL[43]. We
Inthisequation,thehumanposeP ={(R jp,tp j)}J j=1isrep- initialize the r
θ,j
and s
θ,j
in Eq. (5) to zeros and ones re-
resentedbytherotationsandtranslationsofJ joints. Each spectivelysothatwestartwithathinGaussianwhosevari-
vertexisassociatedwithLBSweightsdenotedasw i. And anceinthefacenormalaxisissmall. Meanwhile,thepro-
pn irrepresentsthecoordinatesinthenon-rigidlytransformed jectionoftheellipsoid{x : (x−µ j)TΣ− j1(x−µ j) = 1}
canonicalspace,whichwewillelaborateonnext. onthetrianglerecoverstheSteinerellipse(seeFig.3). To
Non-rigid deformation. To transform to the non-rigidly enhance the details, we upsample the canonical GoMc us-
θ
transformed canonical space, we model a pose-dependent ing GoM subdivision during training. We first subdivide
non-rigiddeformationbeforeLBS.Specifically,wepredict theunderlyingmeshbyintroducingnewverticesatthecen-
anoffsetandaddittothei-thcanonicalvertex,i.e., terofeachedge,followedbyreplacingeachfacewithfour
pnr =pc +NRDeformer (cid:0) γ(pc ),P(cid:1) . (12) smaller faces. The properties of each face, as described in
i θ,i θ θ,i Eq.(5),areduplicatedacrossthenewlygeneratedfaces.
NRDeformerreferstoanMLPnetwork. γ(·)denotesthe
sinusoidalpositionalencoding[49]. 4.Experiments
3.4.PoseRefinement We evaluate GoMAvatar on the ZJU-MoCap dataset [54],
the PeopleSnapshot dataset [1] and on YouTube videos,
Human poses are typically estimated from the image and
comparing with state-of-the-art human avatar modeling
hence often inaccurate. Therefore, we follow Human-
methods from monocular videos. We showcase our
NeRF [70] to add a pose refinement module that learns to
method’s rendering quality under novel views and poses,
correct the estimated poses. Specifically, given a human
aswellasitsspeedandgeometry.
posePˆ ={(Rˆp,tp)}J estimatedfromavideoframe,we
j j j=1
predictacorrectiontothejointrotationsvia 4.1.Experimentalsetup
(cid:16) (cid:17)
{ξ }J =PoseRefiner {Rˆp}J . (13) Datasets. We validate our proposed approach on ZJU-
j j=1 θ j j=1
MoCap [54] data, PeopleSnapshot [1] data and Youtube
where ξ j ∈ SO(3). We obtain the updated pose P = videos. ZJU-MoCap: The ZJU-MoCap dataset provides
{(Rp,tp)}J = {(Rˆp · ξ , tp)}J , which is used a comprehensive multi-camera, multi-subject benchmark
j j j=1 j j j j=1
inEq.(11)and(12). for human rendering evaluation. It has 9 dynamic human
It’s important to note that pose refinement occurs only videoscapturedby21synchronizedcameras. Inourpaper,
duringnovelviewsynthesisandthetrainingstagetocom- to ensure a fair comparison, we adhere to the training/test
pensate for the inaccuracies in pose estimation from the splitinMonoHuman[81]andfollowtheirmonocularvideo
videos. Itisnotneededforanimation. human rendering setting. We validate our approach on sixNovelviewsynthesis Novelposesynthesis Inference Memory
PSNR↑ SSIM↑ LPIPS*↓ PSNR↑ SSIM↑ LPIPS*↓ time(ms)↓ (MB)↓
NeuralBody[54] 28.72 0.9611 52.25 28.54 0.9604 53.91 212.3 16.76
HumanNeRF[70] 29.61 0.9625 38.45 29.74 0.9655 34.79 1776.7 245.73
NeuMan[27] 28.96 0.9479 60.74 28.75 0.9406 62.35 3412.5 2.27
MonoHuman[81] 30.26 0.9692 30.92 30.05 0.9684 31.51 5970.0 280.67
Ours 30.37 0.9689 32.53 30.34 0.9688 32.39 23.2 3.63
Table1. QuantitativeresultsonZJU-MoCapdataset. Ourresultsgenerallyprovidethebest(orsecondbest)qualityacrossbothnovel
viewandnovelposerenderingwhilebeingthefastestandhavingthesecondsmallestparametersize.( best, secondbest)
CD↓ NC↑ tency,wecompute1−L2distancebetweennormalsfor1)
eachvertexintheground-truthmesh;and2)itsclosestver-
NeuralBody[54] 5.1473 0.4985
texinthepredictedmesh. Wealsobenchmarktheinference
HumanNeRF[70] 2.8029 0.5039
MonoHuman[81] 2.6303 0.5205 speed in milliseconds (ms) / frame on an NVIDIA A100
Ours 2.8364 0.6201 GPU and the memory cost (the size of parameters used in
inference).
Table 2. Geometry quality evaluation. Our approach provides
thebestnormalconsistencyacrossallmethods,andMonoHuman
4.2.Quantitativeresults
achievesbestqualityinsurfacegeometry.( best, secondbest)
Novelviewsynthesis Inference Tab. 1 presents our results on ZJU-MoCap data follow-
PSNR↑ SSIM↑ LPIPS↓ time(ms)↓ ing MonoHuman’s split. In terms of perceptual per-
Anim-NeRF[8] 28.89 0.9682 0.0206 217.00 formance, our approach achieves PSNR/SSIM/LPIPS*
InstantAvatar[26] 28.61 0.9698 0.0242 71.26 of 30.37/0.9689/32.53 on novel view synthesis and
Ours 30.68 0.9767 0.0213 25.82 30.34/0.9688/32.39 on novel pose synthesis, which is on
Table3. QuantitativeresultsonPeopleSnapshotdataset. Our parwiththetop-performingcompetitivemethodsMonoHu-
approach provides the best results regarding PSNR and SSIM man. Notably, in terms of inference time, our approach
whilebeingthefastestininference.( best, secondbest) achieves a rendering speed of 23.2ms/frame (43 FPS),
subjects (377, 386, 387, 392, 393, and 394) in the dataset. which is 257× faster than MonoHuman, 76× faster than
For each subject, the first 4/5 frames from Camera 0 are HumanNeRF,andmorethan9×fasterthananycompeting
used for training. We use the corresponding frames in the algorithm. Theseresultsindicatethatourproposedmethod
remaining cameras to evaluate novel view synthesis, and enablesreal-timearticulatedneuralhumanrenderingfroma
the last 1/5 frames from all views to evaluate novel pose singlevideo. Meanwhile,ourapproachismemory-efficient
synthesis. PeopleSnapshot: The PeopleSnapshot dataset (3.63MBparameters),whichissmallerthanallcompetitive
provides monocular videos where humans rotate in front methodsexceptNeuMan[27].
of the cameras. We follow the evaluation protocol in In- We also evaluate the Chamfer distance and the nor-
stantAvatar[26]tovalidateourapproach. Wereportresults mal consistency between predicted geometry and pseudo
averagedonfoursubjects(f3c,f4c,m3c,andm4c)andre- ground-truth geometry in Tab. 2. Note that the pseudo
finethetestposes. Youtubevideos: Wequalitativelyvali- ground-truths are generated from NeuS [68] on all view-
date our approach on Youtube dancing videos used in Hu- pointsandarethenfiltered,followingARAH[69]. Ourap-
manNeRF[70]. WegeneratethesubjectmaskswithMedi- proach significantly outperforms NeRF-based approaches
aPipe[44],andtheSMPLposeswithPARE[33]. in terms of normal consistency, which indicates that our
Baselines. We compare with state-of-the-art approaches approach can learn meaningful geometry. Note that our
forsingle-videoarticulatedhumancapturingalgorithms,in- Chamfer distance is slightly worse than HumanNeRF and
cludingNeuralBody[54],HumanNeRF[70],NeuMan[27], MonoHuman. Itispossiblyduetotheuseof3DGaussians,
MonoHuman[81],Anim-NeRF[8]andInstantAvatar[26]. which have thickness in the surface normal direction. The
Similartoourmethod,thesemethodstakeasinputasingle rendered mask is larger than the actual mesh’s silhouette.
videoand3Dskeletonandoutputanarticulatedneuralhu- Hence,ourmeshesareabitsmallerthanthe‘real’meshes.
manrepresentation, thatcanfacilitatebothnovelviewand Following InstantAvatar’s split, we evaluate our ap-
novelposesynthesis. proachonfoursubjectsinPeopleSnapshotdatasetinTab.3.
Evaluationmetrics. WereportPSNR,SSIMandLPIPSor Our approach achieves the PSNR/SSIM/LPIPS/inference
LPIPS* (= LPIPS × 1000) for novel view synthesis and time of 30.68/0.9767/0.0213/25.82ms, significantly out-
novelposesynthesis. Tocomparethegeometry, wereport performing InstantAvatar’s 28.61/0.9698/0.0242/71.26ms.
ChamferDistance(CD)andtheNormalConsistency(NC) Compared to Anim-NeRF’s PSNR/SSIM/LPIPS of
following the protocol in ARAH [69]. For normal consis- 28.89/0.9682/0.0206,ourPSNRandSSIMaresignificantly(a) Ground truth (b) Neural Body (c) HumanNeRF (d) MonoHuman (e) GoMAvatar(ours)
Figure4. Qualitativecomparisontostate-of-the-arts. Ineachpair, werendertheRGBimageandnormalmap. Thenormalmapis
renderedfromtheextractedmesh. Weshowthatourapproachcanproducerealisticdetailsinbothrenderedimagesandgeometry,while
otherapproachesstruggletogenerateasmoothmesh.
(a)Targetpose (b)HumanNeRF(c)MonoHuman (d)Ours
Figure6. Novelposesynthesis. Posesarefromusingposesgen-
eratedfromMDM[65].
ning. Specifically, limited by the resolution of the LBS
weights, the free space is affected by two unrelated body
partsandthusobtainsalargeforegroundscore.Thefloaters
(a)Reference (b)HumanNeRF(c)MonoHuman (d)Ours
aretypicalvolumerenderingartifactsasinotherNeRFrep-
Figure5.QualitativeresultsonYouTubevideos.Thefirstimage
resentations. Incontrast,ourapproachusesexplicitgeom-
isthereferenceimage. Wecomparenovelviewsynthesisinthe
etry and thus does not suffer from both issues. We addi-
firstrowandnovelposesynthesisinthesecondrow.
tionallytestourapproachonYouTubedancingvideosinthe
better, while LPIPS is on par. Also, Anim-NeRF renders firstrowofFig.5.Notethatthehumanposesandmasksare
at a speed of 217ms/frame on an Nvidia A100, while ours predicted and thus inaccurate. However, our method still
achieves25.82ms/frame,being8.4×faster. renders novel views well, while HumanNeRF and Mono-
Humansufferfromimperfectmasksandproducefloaters.
4.3.Qualitativeresults
Novel pose synthesis. We render novel poses generated
Novel view synthesis. We provide a qualitative compar- fromMDM[65],asdepictedinFig.6.Remarkably,ourap-
ison with NeuralBody, HumanNeRF and MonoHuman on proach performs effectively even in extremely challenging
rendered images and normal maps in Fig. 4. The nor- poses characterized by self-penetration, such as sitting. In
malmaps arerenderedfrom theextractedmeshes. As can contrast,bothHumanNeRFandMonoHumanlackthecapa-
be seen from the figure, our approach captures fine de- bilitytohandlesuchself-penetration,duetothevoxel-based
tails, such as facial features and wrinkles, and avoids the inverseblendskinning(seetheincompletelefthands). We
“ghost effect” and “floaters” observed in HumanNeRF’s validateourapproachonnovelviewsynthesisusinganin-
andMonoHuman’soutput(seethearmpitofthesecondsub- the-wild YouTube video, as illustrated in the second row
jectinHumanNeRF’srenderingandfloatersaroundMono- of Fig. 5. Specifically, when rendering the avatar in a leg-
Human’srendering).Theghosteffecttypicallyoccurswhen crossing pose, both HumanNeRF and MonoHuman fail to
two body parts come too close, an artifact due to Human- produce accurate results, whereas our approach success-
NeRF’sandMonoHuman’svoxel-basedinverseblendskin- fullyrenderstheposewithfidelity.Gaussians Mesh PSNR↑ SSIM↑ LPIPS*↓ PSNR↑ SSIM↑ LPIPS*↓ CD↓ NC↑
✓ × 30.06 0.9673 34.13 WorldGaussians 30.34 0.9689 33.99 4.3941 0.6223
× ✓ 28.93 0.9615 38.11 LocalFixedGaussians 30.27 0.9685 34.11 3.0898 0.6247
✓ ✓ 30.36 0.9690 33.28 LocalFlex.Gaussians 30.36 0.9690 33.28 3.0728 0.6366
Table4. Ablationsonscenerepresentationfornovelviewsyn- w/oShading 30.13 0.9684 32.07 3.0177 0.6360
thesis.Gaussians-on-Meshachievesthebestresults. w/Shading 30.36 0.9690 33.28 3.0728 0.6366
w/oSubdivision 30.36 0.9690 33.28 3.0728 0.6366
4.4.Ablationstudies
w/Subdivision 30.37 0.9689 32.53 2.8364 0.6201
Canonical representation. We conduct ablation studies
Table5. AblationStudies. Topsection: locallydeformedGaus-
on the Gaussians-on-Mesh (GoM) representation and 3D
sianshelpimprovebothgeometryandrenderingquality. Middle
Gaussiansormeshesalone,assummarizedinTab.4. Inthe section:ourproposedshadingmoduleenhancesrenderingquality.
3D Gaussians experiment, we only use Gaussian splatting Bottomsection:subdivisionsignificantlyimprovesgeometry.
for rendering, and supervise the rendered image and sub-
jectmaskduringtraining. WeinitializetheGaussians’cen-
troids as the vertices of the canonical T-pose SMPL mesh
anddirectlylearntheircentroids,rotationsandscalesinthe
world coordinates, which differs from the triangle’s local
coordinates used in our approach. We also compare with
justusingamesh: WeinitializeusingthecanonicalSMPL
mesh and attach the pseudo-albedo colors to the vertices.
WerendertheRGBimageandsubjectmaskwithmeshras-
(a)Pseudoshading map (b) Rendered image
terization [40]. We supervise the rendered image and sub-
Figure7.Pseudoshadingmap.Wevisualizethepseudoshading
jectmaskandapplyallregularizationsinEq.(15). Wealso
map and the rendered image for reference. Our approach learns
utilizethecolordecompositioninEq.(6).
view-dependentshadingeffectsasseeninthehighlightedregions.
We find 3D Gaussians alone suffer from overfitting: Thepseudoshadingmapisnormalizedforbettervisualization.
withoutgeometryregularization,Gaussiansaretooflexible
withouttheshadingmoduleinEq.(6)—thatis,bydirectly
and achieve similar rendering quality on training images,
using I as the RGB prediction—our model achieves a
while the outputs are undesirable during inference. When GS
PSNR of 30.13. However, with the shading module in-
using only the mesh, optimization is a known challenge.
cluded,thePSNRincreasesto30.36. Wealsovisualizethe
In contrast, GoM alleviates these issues and combines the
pseudoshadingmap,demonstratingthatourshadingmod-
strengthsofbothrepresentations. GoMproducesthehigh-
ulelearnslightingeffects,asillustratedinFig.7.
estrenderingqualityamongthethreerepresentations.
GoMsubdivision.WeshowinthebottomsectionofTab.5
LocalGaussiansvs.worldGaussians. Wecomparethree
that GoM subdivision enhances the LPIPS* from 33.28 to
choices of attaching Gaussians to the mesh: 1) World
32.53 and reduces the Chamfer distance from 3.0728 to
Gaussians: We associate the Gaussian’s centroid with the
2.8364. Importantly, the geometry significantly improves
face’scentroid(Eq.(8)).However,wedirectlylearnther
θ,j with a more fine-grained mesh. Note, this increases infer-
ands intheworldcoordinates,i.e.,Σ = R S ST,RT,
θ,j j j j j j encetimeto23.2msperframefrom17.5ms.
whereR andS arethematrixencodingsofr ands ;
j j θ,j θ,j
5.Conclusion
2) Local fixed Gaussians: We follow Eqs. (8) and (9) to
computeaGaussian’smeanandcovarianceintheworldco- We introduce GoMAvatar, a framework designed for ren-
ordinates. However,r ands arefixedsothatthevari- deringhigh-fidelity,free-viewpointimagesofahumanper-
θ,j θ,j
anceinthenormalaxisissmall. Meanwhile,theprojection former, using a single input video. At the core of our
of the ellipsoid {x : (x−µ )TΣ−1(x−µ ) = 1} on the method is the Gaussians-on-Mesh representation. Paired
j j j
triangle recovers the Steiner ellipse. 3) Local Gaussians: withforwardarticulationandneuralrendering,ourmethod
WeuseEqs.(8)and(9)totransformtheGaussiansandr rendersquicklywhilebeingmemoryefficient. Notably,the
θ,j
ands arefreevariables. methodhandlesin-the-wildvideoswell.
θ,j
WeshowthecomparisoninthetopsectionofTab.5. In Acknowledgement
termsofrenderingquality,worldGaussiansandlocalGaus- Project supported by Intel AI SRS gift, IBM IIDAI
sians achieve similar performance. But world Gaussians Grant,Insper-IllinoisInnovationGrant,NCSAFacultyFel-
tendtoenlargethescalesinsteadofstretchingthefaces,so lowship, NSF Awards #2008387, #2045586, #2106825,
thegeometryisworse. LocalfixedGaussianscanproduce #2331878, #2340254, #2312102, and NIFA award 2020-
equallygoodgeometry,butloserenderingflexibility. 67021-32799. We thank NCSA for providing computing
ShadingModule.AsshowninthemiddlesectionofTab.5, resources. WethankYimingZuoforhelpfuldiscussions.References [18] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.
Shape,Light&MaterialDecompositionfromImagesusing
[1] Thiemo Alldieck, Marcus A. Magnor, Weipeng Xu, Chris-
MonteCarloRenderingandDenoising.InNeurIPS,2022.2
tian Theobalt, and Gerard Pons-Moll. Video based recon-
[19] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and
structionof3Dpeoplemodels. InCVPR,2018. 2,5
Tony Tung. Arch++: Animation-ready clothed human re-
[2] DragomirAnguelov,PraveenSrinivasan,DaphneKoller,Se-
constructionrevisited. InICCV,2021. 3
bastian Thrun, Jim Rodgers, and James Davis. SCAPE:
[20] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,
shapecompletionandanimationofpeople.ACMTOG,2005.
Jonathan T. Barron, and Paul E. Debevec. Baking Neural
2
Radiance Fields for Real-Time View Synthesis. In ICCV,
[3] Benjamin Attal, Jia-Bin Huang, Michael Zollhoefer, Jo-
2021. 3
hannes Kopf, and Changil Kim. Learning Neural Light
[21] Shou-YongHu,FangzhouHong,LiangPan,HaiyiMei,Lei
FieldswithRay-SpaceEmbedding. InCVPR,2021. 2
Yang,andZiweiLiu. SHERF:GeneralizableHumanNeRF
[4] JonathanT.Barron,BenMildenhall,MatthewTancik,Peter fromaSingleImage. InICCV,2023. 3
Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.
[22] T. Hu, Tao Yu, Zerong Zheng, He Zhang, Yebin Liu, and
Mip-NeRF: A Multiscale Representation for Anti-Aliasing
MatthiasZwicker.HVTR:HybridVolumetric-TexturalRen-
NeuralRadianceFields. InICCV,2021. 2
deringforHumanAvatars. 3DV,2021. 3
[5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
[23] SatoshiIizuka,EdgarSimo-Serra,andHiroshiIshikawa.Let
Srinivasan,andPeterHedman. Mip-NeRF360:Unbounded
therebecolor! ACMTOG,2016. 2
Anti-AliasedNeuralRadianceFields. InCVPR,2022.
[24] Timothy Jeruzalski, Boyang Deng, Mohammad Norouzi,
[6] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
J. P. Lewis, Geo rey E. Hinton, and Andrea Tagliasacchi.
Srinivasan, and Peter Hedman. Zip-NeRF: Anti-Aliased
NASA:NeuralArticulatedShapeApproximation.InECCV,
Grid-BasedNeuralRadianceFields. InICCV,2023. 2
2020. 3
[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
[25] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang.
Hao Su. TensoRF: Tensorial Radiance Fields. In ECCV,
SelfRecon: Self Reconstruction Your Digital Avatar from
2022. 2
MonocularVideo. InCVPR,2022. 1,3
[8] JianchuanChen,YingZhang,DiKang,XuefeiZhe,Linchao
[26] TianjianJiang,XuChen,JieSong,andOtmarHilliges. In-
Bao,andHuchuanLu. AnimatableNeuralRadianceFields
stantavatar: Learning avatars from monocular video in 60
fromMonocularRGBVideo. arXiv,2021. 6 seconds. InCVPR,2023. 6,2
[9] JianchuanChen, WenYi, LiqianMa, XuJia, andHuchuan [27] WeiJiang, KwangMooYi, GolnooshSamei, OncelTuzel,
Lu.GM-NeRF:LearningGeneralizableModel-basedNeural andAnuragRanjan. Neuman: Neuralhumanradiancefield
RadianceFieldsfromMulti-viewImages.InCVPR,2023.3 fromasinglevideo. InECCV,2022. 1,6
[10] XuChen, YufengZheng, MichaelJBlack, OtmarHilliges, [28] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei
andAndreasGeiger. SNARF:DifferentiableForwardSkin- Tan, LinGui, SeanBanerjee, TimothyScottGodisart, Bart
ning for Animating Non-Rigid Neural Implicit Shapes. In Nabbe,IainMatthews,TakeoKanade,ShoheiNobuhara,and
ICCV,2021. 3 YaserSheikh.PanopticStudio:AMassivelyMultiviewSys-
[11] XuChen,TianjianJiang,JieSong,MaxRietmann,Andreas temforSocialInteractionCapture. TPAMI,2017. 1
Geiger,MichaelJ.Black,andOtmarHilliges. Fast-SNARF: [29] JamesT.KajiyaandBrianVonHerzen.RayTracingVolume
A Fast Deformer for Articulated Neural Fields. TPAMI, Densities. InSIGGRAPH,1984. 2
2022. 3
[30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler,
[12] Zhiqin Chen and Hao Zhang. Learning Implicit Fields for andGeorgeDrettakis. 3DGaussianSplattingforReal-Time
GenerativeShapeModeling. InCVPR,2019. 2 RadianceFieldRendering. ACMTOG,2023. 2,3
[13] Zhiqin Chen, Thomas A. Funkhouser, Peter Hedman, and [31] Jaehyeok Kim, Dongyoon Wee, and Dan Xu. You Only
AndreaTagliasacchi. MobileNeRF:ExploitingthePolygon Train Once: Multi-Identity Free-Viewpoint Neural Human
Rasterization Pipeline for Efficient Neural Field Rendering RenderingfromMonocularVideos. arXiv,2023. 3
onMobileArchitectures. InCVPR,2023. 3 [32] Diederik P. Kingma and Jimmy Ba. Adam: A Method for
[14] Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng, StochasticOptimization. arXiv,2014. 2
ZichengLiu,andXinTong. MPS-NeRF:Generalizable3D [33] MuhammedKocabas, Chun-HaoPHuang, OtmarHilliges,
HumanRenderingfromMultiviewImages. TPAMI,2022. 3 andMichaelJBlack. PARE:Partattentionregressorfor3D
[15] Stephan J. Garbin, Marek Kowalski, Matthew Johnson, humanbodyestimation. InICCV,2021. 6,3
JamieShotton,andJulienP.C.Valentin. FastNeRF:High- [34] Youngjoon Kwon, Dahun Kim, Duygu Ceylan, and Henry
FidelityNeuralRenderingat200FPS. InICCV,2021. 3 Fuchs. Neural Human Performer: Learning Generalizable
[16] ChenGeng,SidaPeng,ZhenqiXu,HujunBao,andXiaowei Radiance Fields for Human Performance Rendering. In
Zhou. LearningNeuralVolumetricRepresentationsofDy- NeurIPS,2021. 3
namicHumansinMinutes. InCVPR,2023. 1,3,4 [35] YoungChanKwon,DahunKim,DuyguCeylan,andHenry
[17] StevenJ.Gortler,RadekGrzeszczuk,RichardSzeliski,and Fuchs.NeuralImage-basedAvatars:GeneralizableRadiance
MichaelF.Cohen. Thelumigraph. InSIGGRAPH,1996. 2 FieldsforHumanAvatarModeling. InICLR,2023. 3[36] MarcLevoyandPatHanrahan. LightFieldRendering. In [52] SangIlParkandJessicaKHodgins. Capturingandanimat-
SIGGRAPH,1996. 2 ing skin deformation in human motion. ACM TOG, 2006.
[37] RuilongLi,JulianTanke,MinhVo,MichaelZollhofer,Jur- 1
genGall,AngjooKanazawa,andChristophLassner. TAVA: [53] Sida Peng, Junting Dong, Qianqian Wang, Shang-Wei
Template-free Animatable Volumetric Actors. In ECCV, Zhang,QingShuai,XiaoweiZhou,andHujunBao.Animat-
2022. 3 ableNeuralRadianceFieldsforModelingDynamicHuman
[38] Zhi-HaoLin,Wei-ChiuMa,Hao-YuHsu,Yu-ChiangFrank Bodies. InICCV,2021. 3
Wang, andShenlongWang. Neurmips: NeuralMixtureof [54] SidaPeng, YuanqingZhang, YinghaoXu, QianqianWang,
PlanarExpertsforViewSynthesis. InCVPR,2022. 3 Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Implicit neural representations with structured latent codes
[39] LingjieLiu,MarcHabermann,ViktorRudnev,Kripasindhu
for novel view synthesis of dynamic humans. In CVPR,
Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor:
2021. 2,3,5,6
Neuralfree-viewsynthesisofhumanactorswithposecon-
trol. ACMTOG,2021. 3 [55] Edoardo Remelli, Timur M. Bagautdinov, Shunsuke Saito,
Chenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo,
[40] ShichenLiu,TianyeLi,WeikaiChen,andHaoLi. Softras-
ZheCao,Fabia´nPrada,JasonM.Saragih,andYaserSheikh.
terizer:Adifferentiablerendererforimage-based3dreason-
Drivable Volumetric Avatars using Texel-Aligned Features.
ing. InICCV,2019. 4,8
InSIGGRAPH,2022. 3
[41] StephenLombardi,TomasSimon,JasonM.Saragih,Gabriel
[56] Zhongzheng Ren, Xiaoming Zhao, and Alexander G.
Schwartz,AndreasM.Lehrmann,andYaserSheikh. Neural
Schwing. Class-agnostic Reconstruction of Dynamic Ob-
Volumes: LearningDynamicRenderableVolumesfromIm-
jectsfromVideos. InNeurIPS,2021. 2,3
ages. ACMTOG,2019. 2
[57] ZhongzhengRen,AseemAgarwala,BryanRussell,Alexan-
[42] Matthew Loper, Naureen Mahmood, and Michael J Black.
derG.Schwing,andOliverWang. Neuralvolumetricobject
Mosh:Motionandshapecapturefromsparsemarkers.ACM
selection. InCVPR,2022. 2
TOG,2014. 1
[58] Ignacio Rocco, Iurii Makarov, Filippos Kokkinos, David
[43] MatthewLoper,NaureenMahmood,JavierRomero,Gerard
Novotny´,BenjaminGraham,NataliaNeverova,andAndrea
Pons-Moll,andMichaelJ.Black. SMPL:Askinnedmulti-
Vedaldi. Real-timeVolumetricRenderingofDynamicHu-
personlinearmodel. ACMTOG,2015. 2,5,1
mans. arXiv,2023. 1,3
[44] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-
[59] ShunsukeSaito,ZengHuang,RyotaNatsume,ShigeoMor-
Clanahan,EshaUboweja,MichaelHays,FanZhang,Chuo-
ishima,AngjooKanazawa,andHaoLi. PIFu:Pixel-aligned
Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Medi-
implicitfunctionforhigh-resolutionclothedhumandigitiza-
apipe:Aframeworkforbuildingperceptionpipelines.arXiv,
tion. InICCV,2019. 2
2019. 6
[60] ShunsukeSaito,TomasSimon,JasonSaragih,andHanbyul
[45] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and
Joo.PIFuHD:Multi-levelpixel-alignedimplicitfunctionfor
Deva Ramanan. Dynamic 3d gaussians: Tracking by per-
high-resolution3dhumandigitization. InCVPR,2020. 2
sistentdynamicviewsynthesis. In3DV,2024. 2
[61] Jonathan Shade, Steven J. Gortler, Li wei He, and Richard
[46] RicardoMartin-Brualla,NohaRadwan,MehdiS.M.Sajjadi, Szeliski. Layereddepthimages. InSIGGRAPH,1998. 2
JonathanT.Barron, AlexeyDosovitskiy, andDanielDuck-
[62] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin
worth. NeRFintheWild: NeuralRadianceFieldsforUn-
Huang. 3D Photography Using Context-Aware Layered
constrainedPhotoCollections. InCVPR,2021. 2
DepthInpainting. InCVPR,2020. 2
[47] Lars M. Mescheder, Michael Oechsle, Michael Niemeyer, [63] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias
Sebastian Nowozin, and Andreas Geiger. Occupancy net- Nießner, GordonWetzstein, andMichaelZollho¨fer. Deep-
works: Learning 3D reconstruction in function space. In Voxels: Learning Persistent 3D Feature Embeddings. In
CVPR,2019. 2 CVPR,2019. 2
[48] MarkoMihajlovic´,ShunsukeSaito,AayushBansal,Michael [64] Shih-Yang Su, Frank Yu, Michael Zollhoefer, and Helge
Zollhoefer, and Siyu Tang. COAP: Compositional Articu- Rhodin. A-NeRF: Articulated Neural Radiance Fields for
latedOccupancyofPeople. InCVPR,2022. 3 LearningHumanShape,Appearance,andPose.InNeurIPS,
[49] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, 2021. 1,3
JonathanT.Barron,RaviRamamoorthi,andRenNg.NeRF: [65] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,
Representing Scenes as Neural Radiance Fields for View DanielCohen-Or,andAmitHBermano.Humanmotiondif-
Synthesis. InECCV,2020. 2,4,5 fusionmodel. InICLR,2023. 7
[50] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexan- [66] GarvitaTiwari,NikolaosSarafianos,TonyTung,andGerard
derKeller.Instantneuralgraphicsprimitiveswithamultires- Pons-Moll. Neural-GIF:NeuralGeneralizedImplicitFunc-
olutionhashencoding. ACMTOG,2022. 2 tionsforAnimatingPeopleinClothing. InICCV,2021. 3
[51] JeongJoonPark, PeterFlorence, JulianStraub, RichardA. [67] DorVerbin, PeterHedman, BenMildenhall, ToddE.Zick-
Newcombe, and Steven Lovegrove. DeepSDF: Learning ler,JonathanT.Barron,andPratulP.Srinivasan. Ref-NeRF:
continuous signed distance functions for shape representa- Structured View-Dependent Appearance for Neural Radi-
tion. InCVPR,2019. 2 anceFields. InCVPR,2022. 2[68] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku [82] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Komura,andWenpingWang.Neus:Learningneuralimplicit Koltun. NeRF++: Analyzing and Improving Neural Radi-
surfacesbyvolumerenderingformulti-viewreconstruction. anceFields. arXiv,2020. 2
InNeurIPS,2021. 6 [83] Rui Zhang and Jie Chen. NDF: Neural Deformable Fields
[69] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu forDynamicHumanModelling. InECCV,2022. 3
Tang. ARAH:AnimatableVolumeRenderingofArticulated [84] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
HumanSDFs. InECCV,2022. 3,6 and Oliver Wang. The unreasonable effectiveness of deep
[70] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, featuresasaperceptualmetric. InCVPR,2018. 5
JonathanT.Barron,andIraKemelmacher-Shlizerman. Hu- [85] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei-Ying Lin,
manNeRF: Free-viewpoint Rendering of Moving People YingliangZhang,JingyiYu,andLanXu. HumanNeRF:Ef-
fromMonocularVideo. InCVPR,2022. 1,3,4,5,6,2 ficientlyGeneratedHumanRadianceFieldfromSparseIn-
[71] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon puts. InCVPR,2022. 3
Yenphraphai,andSupasornSuwajanakorn. NeX:Real-time [86] Xiaoming Zhao, Yuan-Ting Hu, Zhongzheng Ren, and
View Synthesis with Neural Basis Expansion. In CVPR, AlexanderG.Schwing. OccupancyPlanesforSingle-view
2021. 2 RGB-DHumanReconstruction. InAAAI,2023. 2
[72] GuanjunWu,TaoranYi,JieminFang,LingxiXie,Xiaopeng
[87] Xiaoming Zhao, Alex Colburn, Fangchang Ma, Miguel
Zhang,WeiWei,WenyuLiu,QiTian,andXinggangWang. A´ngel Bautista, Joshua M. Susskind, and Alexander G.
4dgaussiansplattingforreal-timedynamicscenerendering.
Schwing. Pseudo-Generalized Dynamic View Synthesis
arXiv,2023. 2
fromaVideo. InICLR,2024. 3
[73] GuanjunWu,TaoranYi,JieminFang,LingxiXie,Xiaopeng
[88] Yufeng Zheng, Yifan Wang, Gordon Wetzstein, Michael J.
Zhang,WeiWei,WenyuLiu,QiTian,andWangXinggang.
Black,andOtmarHilliges. PointAvatar: DeformablePoint-
4DGaussianSplattingforReal-TimeDynamicSceneRen-
basedHeadAvatarsfromVideos. InCVPR,2023. 1
dering. arXiv,2023. 2
[89] ZerongZheng,HanHuang,TaoYu,HongwenZhang,Yan-
[74] Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, Rohan
dongGuo,andYebinLiu. StructuredLocalRadianceFields
Bali,DanielleBelko,EricBrockmeyer,LucasEvans,Timo-
forHumanAvatarModeling. InCVPR,2022. 3
thyGodisart,HyowonHa,AlexanderHypes,TaylorKoska,
[90] TinghuiZhou, RichardTucker, JohnFlynn, GrahamFyffe,
Steven Krenn, Stephen Lombardi, Xiaomin Luo, Kevyn
and Noah Snavely. Stereo Magnification: Learning View
McPhail, Laura Millerschoen, Michal Perdoch, Mark Pitts,
SynthesisusingMultiplaneImages. ACMTOG,2018. 2
Alexander Richard, Jason Saragih, Junko Saragih, Takaaki
Shiratori, Tomas Simon, Matt Stewart, Autumn Trimble,
XinshuoWeng,DavidWhitewolf,ChengleiWu,Shoou-IYu,
andYaserSheikh.Multiface:ADatasetforNeuralFaceRen-
dering,2022. 1
[75] HongyiXu,ThiemoAlldieck,andCristianSminchisescu.H-
NeRF:NeuralRadianceFieldsforRenderingandTemporal
ReconstructionofHumansinMotion. InNeurIPS,2021. 3
[76] YuanluXu,BingpengMa,andRuiHuangLiangLin.Person
search in a scene by jointly modeling people commonness
andpersonuniqueness. InACMMM,2014. 3
[77] Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng
Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, and
Raquel Urtasun. S3: Neural shape, skeleton, and skinning
fieldsfor3dhumanmodeling. InCVPR,2021. 3
[78] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,
Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron,
andBenMildenhall.Bakedsdf:Meshingneuralsdfsforreal-
timeviewsynthesis. InSIGGRAPH,2023. 3
[79] AlexYu,RuilongLi,MatthewTancik,HaoLi,RenNg,and
AngjooKanazawa. PlenOctreesforReal-timeRenderingof
NeuralRadianceFields. InICCV,2021. 3
[80] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong
Chen, BenjaminRecht, andAngjooKanazawa. Plenoxels:
RadianceFieldswithoutNeuralNetworks. InCVPR,2022.
2
[81] Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, and
Kwan-Yee Lin. Monohuman: Animatable human neural
field from monocular video. In CVPR, 2023. 1, 3, 4, 5,
6GoMAvatar: Efficient Animatable Human Modeling from Monocular Video
Using Gaussians-on-Mesh
Supplementary Material
Thissupplementarymaterialisorganizedasfollows: Here,{∆ }3 arethethreeindicesoftheverticesonthe
j,k k=1
1. Sec. A provides the detailed derivation of Gaussians’ j-th triangle and hence {p }3 are the coordinates of
∆j,k k=1
local-to-worldtransformation; thevertices.
2. Sec.Bdetailsthewholeinferencepipeline; ThematrixA =[a ,a ,a ]takescareoftheworld
j j,1 j,2 j,3
3. Sec.Cshowsimplementationdetails. Gaussian’s shape deformation. We use a , k ∈ {1,2,3}
j,k
4. Sec. D shows additional results including the quantita- todenotethethreecolumnsofmatrixA . OurdesignofA
j j
tive results broken down per scene and additional abla- isinspiredbytheSteinerellipseofatriangle,theuniqueel-
tionstudies. lipsethathasthemaximumareaofanyellipse.Specifically,
5. Sec.Eshowcasesfailurecasesinourapproach. wedefinea anda asthetwosemi-axesoftheSteiner
j,1 j,2
ellipse:
A. Derivation of Gaussians’ local-to-world
−−−−→ 1 −−−−−−→
transformation a =b p cost + √ p p sint , (21)
j,1 j ∆j,3 0
3
∆j,1 ∆j,2 0
AsdescribedinSec.3.1andSec.3.2,wedefinetherotation −−−−→ π 1 −−−−−−→ π
a =b p cos(t + )+ √ p p sin(t + ),
r θ,j andthescales θ,j inthetriangle’slocalcoordinates. In j,2 j ∆j,3 0 2 3 ∆j,1 ∆j,2 0 2
order to render with Gaussian splatting, we transform the (22)
local Gaussians to the world coordinates. Specifically, the
where
meanistransformedtothecentroidofthetriangle(Eq.(8))
a w thn o ed r dldt eh te t ar ia lc eno dsv fa dor eri m ra in vac ate i to ionm na .mtr ai tx rixis Atr jan (s Ef qo .rm (9e )d ).by Wt ehe nol woca sl h- oto w- t
0
= 21 arctan− b−√2 p−3 −− b →− jp− 2∆−→ −j,3 1·
−
p− p −∆− −− j, −− 1 p−p− ∆ −− →→ j,2 2. (23)
j ∆j,3 3 ∆j,1 ∆j,2
Given a face and its associated local properties f =
θ,j Thethirdcolumna isdefinedalongthenormalvectorof
(r ,s ,c ,{∆ }3 ), wewanttocomputeitsGaus- j,3
θ,j θ,j θ,j j,k k=1 thetriangleface:
sianintheworldcoordinatesG =N(µ ,Σ ).
j j j
TheGaussianinthetriangle’slocalcoordinateis a =ϵ·normalize(a ×a ). (24)
j,3 j,1 j,2
Gˆ j =N(0,Σˆ j), whereΣˆ j =R jS jS jTR jT. (16) We multiply the normal vector with ϵ to make sure the el-
lipsoidisthinalongthesurfacenormal. Wesetϵ=1e−3in
Here,R andS arethematrixformofr ands respec-
j j θ,j θ,j ourexperiments.
tively. Then,wedefineatransformationfromthetriangle’s
Giventhederivationabove,whenthelocalrotationr
θ,j
localcoordinatestoworldcoordinates:
iszeroandthelocalscales isone,i.e,Σˆ = I,thepro-
θ,j j
jectionoftheellipsoid{x:(x−µ )TΣ−1(x−µ )=1}on
f(x)=A jx+b j, (17) j j j
thetrianglerecoverstheSteinerellipse,asshowninFig.3.
where A ∈ R3×3 and b ∈ R3. Therefore, the mean and
j j
covarianceoftheGaussianintheworldcoordinatesare B.InferencePipeline
We present our inference pipeline including the modules
µ =b , (18)
j j
andkeyinputsandoutputsinFig.8.
Σ =A Σˆ AT. (19)
j j j j
C.ImplementationDetails
Local-to-worldaffinetransformationf(x) = A x+b .
j j Architecture details. 1) GoMc in Eq. (3) is initialized
The goal of the local-to-world affine transformation is to θ
with the SMPL mesh [43] under the canonical T-pose; 2)
move and reshape the Gaussian based on the location and
Shading in Eq. (10) is a 4-layer MLP network with
shape of the triangle face. The world Gaussian’s centroid θ
128 channels; 3) NRDeformer in Eq. (12) is a 7-layer
(Eq.(18))isputatthecentroidofthetriangleface,i.e., θ
MLP network with 128 channels; 4) PoseRefiner
θ
3 in Eq. (13) is a 5-layer MLP network with 256 channels.
1(cid:88)
b = p . (20) The detailed architecture of Shading , NRDeformer
j 3 ∆j,k θ θ
k=1 andPoseRefiner θ areshowninFig.9.Articulation (Sec. 3.3) Rendering (Sec. 3.2)
𝑀
Non-rigid Gaussian splatting 𝐼
GoM% motion LBS GoM# !" ⨂ 𝐼
$ module Mesh rasterization 𝑁 !"#$ Pseu mdo o ds uh la eding 𝑆
𝑃 𝐾,𝐸
Figure8. Inferencepipeline. Ourinferencepipelinehastwostages: 1)Articulation: ThisstagetakestheGaussians-on-Mesh(GoM)
representationinthecanonicalspace,denotedasGoMc,andthehumanposeP asinput.Utilizingthenon-rigidmotionmoduleandlinear
θ
blendskinning(LBS),itproducesthetransformedGoMrepresentationintheobservationspace,referredtoasGoMo. 2)Rendering: In
thisstage,thetransformedGoMo,alongwiththecameraintrinsicparametersK andextrinsicparametersE,areemployedasinputs. It
adoptstheGaussiansplattingtogeneratethepseudoalbedomapI andthesubjectmaskM. Meanwhile,throughmeshrasterization,it
GS
producesthenormalmapN whichisthenfedintothepseudoshadingmoduletooutputthepseudoshadingmapS. ThefinalRGB
mesh
imageIisthenobtainedbymultiplyingI withS.
GS
𝛾(𝑁 ) 𝑆 {𝑅(&}) {𝜁})
!"#$ % %’( % %’(
(a) Shading (b) Pose refiner
𝑃
+ 𝑝./
,
𝛾(𝑝- )
*,,
𝛾(𝑝- ) 𝑝-
*,, *,,
(c) Non-rigid deformer
Figure9.Detailedarchitecturesof(a)Shading ,(b)PoseRefiner and(c)NRDeformer .
θ θ θ
Training details. We use Adam optimizer [32] with cess while keeping all other hyperparameters the same as
β = 0.9 and β = 0.999. On ZJU-MoCap, We train ZJU-MoCap.
1 2
the model for 300K iterations. We set the learning rate
of PoseRefiner to 5e−5. The learning rate of the
θ D.AdditionalResults
rest of the model is 5e−4. We kick off the training
of PoseRefiner and NRDeformer after 100K and
θ θ D.1.QuantitativeResultsofPer-sceneBreakdown
150K iterations respectively. For NRDeformer , we fol-
θ
low HumanNeRF [70] to adopt a HanW window during We show the per-scene PSNR, SSIM and LPIPS* on the
training. We set α = 1.0 α = 5.0, α = 1.0 ZJU-MoCap dataset in Tab. 6 and Tab. 7. The per-scene
lpips M reg
in Eq. (14), and α = 10.0, α = 0.1, α = 0.05 breakdownresultsonPeopleSnapshotisshowninTab.8.
lap normal color
inEq.(15).WesubdividetheGoMafter50Kiterations. On
PeopleSnapshot,wetrainthemodelfor200Kiterationsand D.2.QualitativeResultsonPeopleSnapshot
kick off the training of NRDeformer after 100K itera-
θ
tions. WesubdivideGoMonceafter10Kiterations. Wedo We conduct a qualitative comparison on PeopleSnapshot
not refine training poses with PoseRefiner following datasetinFig.10. Asshownbelow, webettercapturetex-
θ
InstantAvatar [26]. On in-the-wild Youtube videos, since turesbettercomparedtoInstantAvatar. Meanwhile,ourap-
the poses are predicted and less accurate, we kick off the proach can capture fine details in geometry, such as wrin-
trainingofPoseRefiner atthestartofthetrainingpro- kles.
θ
128 128 128 128
128 128 128 128 128 128
256
128
256 256 256 256PSNR↑ SSIM↑ LPIPS*↓ PSNR↑ SSIM↑ LPIPS*↓ PSNR↑ SSIM↑ LPIPS*↓
Subject377 Subject386 Subject387
NeuralBody 29.08 0.9679 41.17 29.76 0.9647 46.96 26.84 0.9535 60.82
HumanNeRF 29.79 0.9714 28.49 32.10 0.9642 41.84 28.11 0.9625 37.46
MonoHuman 30.46 0.9781 20.91 32.99 0.9756 30.97 28.40 0.9639 35.06
GoMAvatar(Ours) 30.60 0.9768 23.91 32.97 0.9752 30.36 28.34 0.9635 36.30
Subject392 Subject393 Subject394
NeuralBody 29.49 0.9640 51.06 28.50 0.9591 57.07 28.65 0.9572 55.78
HumanNeRF 30.20 0.9633 40.06 28.16 0.9577 40.85 29.28 0.9557 41.97
MonoHuman 30.98 0.9711 30.80 28.54 0.9620 34.97 30.21 0.9642 32.80
GoMAvatar(Ours) 31.04 0.9708 33.25 28.80 0.9622 37.77 30.44 0.9646 33.56
Table6.Per-scenebreakdowninnovelviewsynthesisonZJU-MoCapdataset.
PSNR↑ SSIM↑ LPIPS*↓ PSNR↑ SSIM↑ LPIPS*↓ PSNR↑ SSIM↑ LPIPS*↓
Subject377 Subject386 Subject387
NeuralBody 29.29 0.9693 39.40 30.71 0.9661 45.89 26.36 0.9520 62.21
HumanNeRF 29.91 0.9755 23.87 32.62 0.9672 39.36 28.01 0.9634 35.27
MonoHuman 30.77 0.9787 21.67 32.97 0.9733 32.73 27.93 0.9633 33.45
GoMAvatar(Ours) 30.68 0.9776 23.41 32.86 0.9737 32.25 28.18 0.9626 36.43
Subject392 Subject393 Subject394
NeuralBody 28.97 0.9615 57.03 27.82 0.9577 59.24 28.09 0.9557 59.66
HumanNeRF 30.95 0.9687 34.23 28.43 0.9609 36.26 28.52 0.9573 39.75
MonoHuman 31.24 0.9715 31.04 28.46 0.9622 34.24 28.94 0.9612 35.90
GoMAvatar(Ours) 31.44 0.9716 33.20 29.09 0.9635 36.02 29.79 0.9638 33.00
Table7.Per-scenebreakdowninnovelposesynthesisonZJU-MoCapdataset.
(a) Ground truth (b) InstantAvatar (c) Ours (d) Reference RGB (e) Predicted normals
Figure10. QualitativeresultsonPeopleSnapshotdataset. On
(c) w/o (d) w/
theleftside,weconductaqualitativecomparisontoInstantAvatar. (a) Input pose (b) Refined pose pose refiner pose refiner
Wealsoshowthegeometrybyrenderingthesurfacenormalson
therightside. Figure11.RobustnesstoSMPLaccuracy.Ourmethodisrobust
toSMPLprediction. Thiscanbeseeninin-the-wildvideos. (a)
Predicted poses have errors. (b) Pose refinement improves erro-
neousSMPLposes,whichiscrucialforin-the-wildvideos(c,d).
D.3.SensitivitytoSMPLAccuracy
Our approach takes the human poses in the input frames
as inputs. The human poses are provided in ZJU-MoCap videos,whichcanbeseeninFig.11(c,d).Withoutthepose
dataset and PeopleSnapshot dataset, while we predict the refinement,theapproachfailstorenderthecorrectshapeof
poseswithPARE[33]forin-the-wildYoutubevideos. the human face. This issue is solved when equipped with
The robustness to SMPL prediction can be seen in in- theposerefinement.
the-wildvideos. InFig.11(a),weshowthatthereareerrors In Tab. 9, we quantitatively assess sensitivity to SMPL
inposepredictioninin-the-wildvideos. However,thepose accuracy on ZJU-MoCap, comparing the original less-
refinement improves erroneous SMPL poses (Fig. 11(b)). accurate SMPL poses to refined versions from Instant-
The pose refinement is crucial for rendering in in-the-wild NVR[16]. Hence,refiningSMPLposeimprovesrenderingPSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
m3c m4c
Anim-NeRF 29.37 0.9703 0.0168 28.37 0.9605 0.0268
InstantAvatar 29.65 0.9730 0.0192 27.97 0.9649 0.0346
GoMAvatar(Ours) 31.74 0.9793 0.0187 29.78 0.9738 0.0282
f3c f4c
Anim-NeRF 28.91 0.9743 0.0215 28.90 0.9678 0.0174
InstantAvatar 27.90 0.9722 0.0249 28.92 0.9692 0.0180
GoMAvatar(Ours) 29.83 0.9758 0.0209 31.38 0.9780 0.0174
Table8.Per-scenebreakdowninnovelviewsynthesisonPeopleSnapshotdataset.
PSNR↑ SSIM↑ LPIPS*↓ CD↓ NC↑
Original 30.37 0.9689 32.53 2.8364 0.6201
Refined 30.86 0.9709 30.91 2.3377 0.6307
Table9.QuantitativeevaluationaboutsensitivitytoSMPLac-
curacy. WetestourapproachwithtwoversionsofSMPLposes
onZJU-MoCapdataset.“Original”referstotheposesprovidedin
theoriginalZJU-MoCapdataset,whichislessaccurate.“Refined”
referstotheimprovedversionfromInstantNVR[16].
(a) (b)
Figure13.Failurecases.
Figure 12. Qualitative comparison between GoM and Gaus-
siansonly.
andgeometryquality,butourmethodwillnotfailwithout.
D.4.AblationonCanonicalRepresentations
Figure14.Novelviewsynthesisonsubjectsindresses.
In Sec.4.4, we conduct a quantitative comparison of the
GoMpresentationand3DGaussiansalone. Here,weshow
the comparison to Gaussians only qualitatively in Fig. 12.
donotcoverthefrontviewoftheperson. Consequently,
Gaussiansonlyyieldsevereartifactsontheboundarywhile
all methods fail to generate a valid rendering from this
ourmethodattainsasharpboundary.
unobservedperspective.
2. As we associate Gaussians with the mesh in the
E.FailureCases
Gaussians-on-Mesh representation, we sometimes can-
Wepresenttwofailurecasesofourapproach: nothandlesignificanttopologychanges.Oneexampleis
1. Ourapproach,alongwithotherstate-of-the-artmethods thewhitebeltontheshortsinSubject377(Fig.13(b)),
optimizedonaper-scenebasis,lackstheabilitytohallu- which dynamically shifts with the person’s movement.
cinateunseenregions.Thislimitationcanbeobservedin Interestingly,whenfittingclotheswithdifferenttopolo-
thefailureforSubject386intheZJU-MoCapdataset,as gies from SMPL, such as dresses, our model can self-
showninFig.13(a). Insubject386,thetrainingframes deform to fit the shapes and yield plausible novel-viewrenderings, even though the topology does not change.
This can be seen in Fig. 14. Addressing topology
changesmayrequireapose-dependenttopologyupdate,
whichweleavetofuturework.