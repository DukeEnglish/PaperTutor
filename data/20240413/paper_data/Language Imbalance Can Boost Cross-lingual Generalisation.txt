Preprint
Language Imbalance Can Boost Cross-lingual Generalisation
AntonScha¨fer,1ShauliRavfogel,2ThomasHofmann,1TiagoPimentel,1,∗ImanolSchlag1,∗
1ETHZu¨rich,2Bar-IlanUniversity
scanton@ethz.ch,shauli.ravfogel@gmail.com,
{thomas.hofmann,tiago.pimentel,imanol.schlag}@inf.ethz.ch
Abstract
Multilingualityiscrucialforextendingrecentadvancementsinlanguage
modellingtodiverselinguisticcommunities.Tomaintainhighperformance
whilerepresentingmultiplelanguages,multilingualmodelsideallyalign
representations, allowing what is learned in one language to generalise
toothers. Priorresearchhasemphasisedtheimportanceofparalleldata
andsharedvocabularyelementsaskeyfactorsforsuchalignment. Inthis
study,weinvestigateanunintuitivenoveldriverofcross-lingualgeneralisa-
tion: languageimbalance. Incontrolledexperimentsonperfectlyequivalent
clonedlanguages,weobservethattheexistenceofapredominantlanguage
during training boosts the performance of less frequent languages and
leadstostrongeralignmentofmodelrepresentationsacrosslanguages. Fur-
thermore,wefindthatthistrendisamplifiedwithscale: withlargeenough
modelsorlongenoughtraining,weobservethatbilingualtrainingdata
witha90⁄ 10languagesplityieldsbetterperformanceonbothlanguagesthan
abalanced50⁄ 50split. Buildingontheseinsights,wedesigntrainingschemes
thatcanimproveperformanceinallclonedlanguages,evenwithoutalter-
ingthetrainingdata. Asweextendouranalysistoreallanguages,wefind
thatinfrequentlanguagesstillbenefitfromfrequentones,yetwhetherlan-
guageimbalancecausescross-lingualgeneralisationthereisnotconclusive.
antonschafer/xling-imbalance
1 Introduction
Inrecentyears,autoregressivelanguagemodels(LMs)pretrainedonmassivetextcorpora
have advanced the state of the art in NLP tasks across the board (Brown et al., 2020;
Touvronetal.,2023a;b;Ko¨pfetal.,2023). Whilemostoftheleadingmodelsaretrained
onEnglishtexts,multilingualcapabilitiesarecrucialtomaketheseadvancesaccessibletoa
broaderuserbasewithdiverselinguisticbackgrounds. Ideally,datainonelanguageshould
improve these multilingual models’ performance in others. Such multilingual models
shouldthusdisplaycross-lingualgeneralisation: byreusingcircuits(Cammarataetal.,
2020;Elhageetal.,2021)andaligningtheirinternalrepresentationsacrosslanguages,they
maygeneraliseconceptslearnedinalanguagetoanother.1
Howcansuchcross-lingualgeneralisationbeachieved? Thishasbeenafocusofmuchprior
work. Onepreviouslyidentifieddriverofcross-lingualgeneralisationisparalleltraining
data; empiricalevidenceshowsthattrainingthemodeloneitherparallelsentencepairs
(Lample&Conneau,2019)oroncorporawhicharecomparableacrosslanguages(Dufter&
Schu¨tze,2020)improvesgeneralisation. Anotherdriverofcross-lingualgeneralisationisthe
∗Jointsupervision.
1A circuit is typically defined as a subgraph of a neural network which performs a specific
computation.E.g.,acircuitcouldberesponsibleforcomputing“greaterthan”comparisonsbetween
numbersinEnglishsentences(Hannaetal.,2024). Ifrepresentationsarealignedacrosslanguages
(in terms of how they encode, e.g., numbers) and circuits are reused, a model should be able to
applywhatitlearnsinonelanguage(e.g.,“greaterthan”comparisonsinEnglish)toperformsimilar
computationsinanotherlanguage(e.g.,French).
1
4202
rpA
11
]LC.sc[
1v28970.4042:viXraPreprint
availabilityofanchorpoints,i.e.,vocabularyelementsthataresharedbetweenlanguages;
these can be naturally occurring subwords (e.g., computer in English and computador in
Portuguesemaysharethesubwordcomp),sharedspecialtokens(e.g.,maskorbossymbols;
Dufter & Schu¨tze, 2020), or even artificially inserted “code-switching” augmentations
(Conneauetal.,2020b;Reid&Artetxe,2022;Ketal.,2020;Fengetal.,2022). Beyondthese
twodrivers,limitedmodelcapacityhasbeenfoundtoimprovegeneralisationbyDufter
&Schu¨tze(2020),buttoconstrainmultilingualcapabilitiesbyChangetal.(2023).
Inthiswork,weidentifyasurprisingnewfactorthatcanboostcross-lingualgeneralisation
abilities: language imbalance. We first conduct experiments in a synthetic setting with
perfectly equivalent cloned languages; this allows us to investigate LMs’ generalisation
abilities in isolation from the effects of languages’ dissimilarities, giving us a rough
upper bound on the generalisation we should expect to see between real language
pairs. In this cloned language setting, we find that having a dominant main language
improvesgeneralisation,significantlyboostingtheperformanceoflessfrequentlanguages.
Furthermore,wefindthatthiseffectbecomesstrongerwhenweeitherincreaseourmodel’s
sizeorwhenwetrainitforlonger. Basedontheseinsights,wedesigntrainingcurricula
thatimproveperformanceinallclonedlanguageswithoutanymodificationstothetraining
data. Inthesecondpartofourpaper,weinvestigatetowhatextentourinsightstransferto
reallanguagepairs. Whilewefindthatlowerresourcelanguagestypicallydobenefitfrom
higher resource ones, the impact of language imbalance on cross-lingual generalisation
ismuchlessclearinthismorerealisticsetting. Overall,ourresultssuggestaninteresting
attributeofmodeltrainingdynamics: insomesettings,havingamainlanguagecanlead
modelcomponentstobesharedacrosslanguages.
2 Cross-lingualgeneralisation
Whilenaturallanguagesdifferwidelyintheirtypologicalproperties,anypairoflanguages
willshareatleastafewgrammaticalandsyntacticpatterns. Furthermore,astheirsemantics
reflecttheunderlyingprocessesofourworld,languagepairsshouldalsohavesimilarities
inthetypesofmessagestheiruserstypicallyconvey. Intuitively,suchsimilaritiessuggest
thatwhatislearnedaboutalanguageL shouldbeusefultomodelanotherlanguageL ,
A B
andviceversa. Theextentofthisgeneralisationdependsnotonlyonhowsimilarthetwo
languagesare,butalsoontheemployedlearningalgorithm. Weanalysesuchgeneralisation
here,withafocusonhowlanguageimbalanceinfluencesmultilingualLMs.
Intuitively, ifamodelgeneraliseswellacrosslanguages, itshouldachievebetterperfor-
manceineachlanguage(intermsof,e.g.,perplexity)thanamonolingualmodeltrained
onthesamedata. Concretely,amodeltrainedonamultilingualdatasetD = D ∪D
multi A B
containinglanguagesL andL shouldperformbetterthanmonolingualmodelstrained
A B
onlyon D or D . Thisbecomesclear whenusingdefinitionsfrom informationtheory:
A B
D containsatleastasmuchinformationaboutL asD . However,suchamultilingual
multi A A
modelcouldalsoperformworse. Thiscouldhappen,forinstance,ifthedatafromdifferent
languagesinterferewitheachotherduringoptimisationthroughconflictinggradientupdate
directions. Itcouldalsohappenifthemodelhaslimitedcapacity: themultilingualmodel
hastorepresentmanylanguages,whichintuitivelyrequiresmorecapacitythanasingleone,
evenifsomeparametersaresharedacrossthem(Conneauetal.,2020a;Pfeifferetal.,2022).
Inanattempttomakemodelsbetteracrossmanylanguages,manymultilingualmodels
thesedaysaretrainedonsomewhatbalanceddata(e.g.,BLOOM;Scaoetal.,2023). Insome
ofthesecases,low-resourcelanguagesareupsampledtoimprovetheirperformanceunder
the model. As mentioned above, however, while balancing languages’ appearance in a
model’strainingsetshouldintuitivelyimproveperformance,thisisnotnecessarilytrue.
In fact, (and perhaps surprisingly) some recent large language models—trained mostly
inmonolingualEnglish-focusedsettings—performreasonablywellinalargesampleof
languages(e.g.,GPT-3andLLaMA;Brownetal.,2020;Touvronetal.,2023a;b;Ahiaetal.,
2023;Wendleretal.,2024). Thesemodels’trainingdataistypicallyhighlyimbalanced,with
onlyasmallfractionbeingcomposedof“non-English”languages.Itisthusunclearwhether
multilingualmodelsindeedbenefitfromtrainingondatasetswithbalancedlanguages.
2Preprint
Insmallertrainingscales,thebenefitsofmultilingualtrainingarebetterunderstood. In
general,ithasbeenfoundthatlow-resourcelanguagestendtobenefitfromdatainhigher-
resourcelanguages,buthigh-resourcelanguagesbenefitmuchlessfromeachother(Con-
neau et al., 2020a; Chang et al., 2023). It is, however, unclear what causes cross-lingual
generalisationinthiscase. Isthemodelinfactabletogeneralisebetterintheimbalancedset-
ting?Ordoesthemodelgeneraliseequallywellinthebalancedcase,butthesmallermodel’s
capacitybottlenecksthoseimprovements,stoppingusfromobservingperformancegains?
Weinvestigatetheroleoflanguageimbalanceincross-lingualgeneralisationhere. Notably,
Wendleretal.(2024)recentlyshowedthatLMsseemtoperforminternalcomputationsin
anabstract“conceptspace”whichisclosesttotheirmainlanguage(English,inthiscase);
representationsarethemmappedbackintotheinputlanguageonlyinthemodels’final
layers. Alabietal.(2024)observeasimilartrendwhenusinglanguageadapters.
3 Experimentalsetup
Inthissection,wedescribethemodelsanddatausedinourexperiments. Wenotethatwe
baseourimplementationonthecodebaseoftheLanguiniKitchen(Stanic´ etal.,2023).
Model. WeuseaGPT-2-styledecoder-onlytransformerarchitectureinallourexperiments
(Radfordetal.,2019). Unlessotherwisenoted,weinstantiateourmodelwith12layersand
ahiddensizeof768,whichresultsin85Mnon-embeddingparameters;thiscorrespondsto
Languini’sgpt-smallconfiguration. Wefollowpreviousworkandtrainourmodelswith
sequencelength512,batchsize128,theAdamoptimiser(Kingma&Ba,2015),andacosine
learningrateschedulefrom6e-4to6e-6with500warmupsteps.
Data. We use Languini’s default datasets to train and evaluate our models. These are
monolingualsplitsfromafilteredversionofthebooks3subsetfromthePile(Gaoetal.,2020).
Thetrainsetconsistsofatotalof23.9Btokens,whilethetestsetcontainsi.i.d. books,with
atotalofroughly11Mtokens. Thisdataistokenisedintoavocabularyofsize16k,obtained
usingaBPEtokenisertrainedwithSentencePiece(Gage,1994;Sennrichetal.,2016;Kudo
&Richardson,2018). ForourexperimentsinFrench,weusetheFrench-PD-Booksdataset
(PleIAs,2024),towhichweapplythepreprocessingpipelineoftheLanguiniKitchen,butfor
French. WetrainaseparateBPEtokeniseronthisFrenchdataset,usinga16k-sizedvocabu-
lary. Forsomeofourexperiments,wetreattheFrenchandEnglishvocabulariesasdisjoint
anddonotmergethem. Ifwemergesubwordsthatoccurinbothvocabularies,wemake
thisclearwiththelabelanchored. Unlessotherwisenoted,wetrainourmodelsfor18,265
steps—i.e.,thefirst1.2Btokensinourdataset;thiscorrespondstoaGPTsmallmodeltrained
for6honanRTX3090GPU,theLanguiniGPTsmall6hsetting(Stanic´etal.,2023).Forexper-
imentswherewecomparehiddenrepresentationsorgradientsonparallelFrench–English
orclonedEnglishsequences,weusedatafromtheEuroparlparallelcorpus(Koehn,2005).
Evaluation. Asourmainevaluationmetric,wereportourmodels’perplexity(PPL)on
thetestset. Toensuresufficientcontextforallpredictions,weuseaslidingwindowwith
stepsof128: wefillina512tokenscontext,ignorethemodel’soutputsontheinitial384,
andevaluateitonlyusingthelast128tokens. Further,wedefinethreemetricsthatallowfor
easycomparisonofmonolingualandmultilingualmodels. Lett andt bethenumberof
A B
tokensamultilingualmodelistrainedoninlanguagesL andL ,respectively. Wedefine
A B
monolingualtokenequivalence(MLTE)asthenumberoftokensthatwouldberequiredbya
monolingualmodel,trainedonlyineitherlanguageL orL ,toachievethesameperplexity
A B
asthemultilingualmodeldoesinthatlanguage. TodetermineMLTE,wefitasimplescaling
lawtopredictperplexityfromthenumberoftrainingtokens(e.g.,t )usingtheresultsfrom
A
ourtrainedmonolingualmodels(seeApp.Bfordetails). Analogously,wedefinemonolin-
gualPPLequivalence(MLPE)astheperplexityamonolingualmodelwouldreachwhen
trainedonthesamenumberofL tokens(i.e.,t )asagivenmultilingualmodel. Finally,
A A
wedefinetokenefficiency(TEff)asthefractionbetweenMLTEandthenumberoftokens
usedformultilingualtraining,e.g.,TEff = MLTEA. Intuitively,ifTEff > 1,performance
A tA
improvesduetomultilinguality,whileifTEff<1,multilingualityhurtsperformance.
3Preprint
4 Clonedlanguages
Inthissection,weexaminethemodel’scapabilitytogeneraliseacrossperfectlyequivalent
cloned languages. We create a cloned language by duplicating the language model’s
vocabulary;thisallowsustoencodeeachsequenceineithertheoriginallanguage(using
the original vocabulary) or in the cloned language (using the cloned vocabulary). This
experimentalparadigmwasoriginallyproposedbyKetal.(2020).2 Formally,letL bean
orig
“original”languagewithavocabularyofsubwordunitsΣ;wedenoteeachsubwordw ∈ Σ.
Thislanguagecanbedescribedbyaprobabilitydistribution p(w ),wherew ∈ Σ∗. We
orig orig
clonelanguage Lbycreatingmultipleinstantiationsofit: L , L ,...L . Theselanguages
1 2 N
havevocabulariesΣ,eachofwhichhassymbolsthatareequivalenttotheoriginalones.3
i
Furthermore,theselanguagesdefineprobabilitydistributionswhichareisometrictothe
originallanguage. Ifwedenoteequivalenceasw =◦ w forw ∈ Σ∗ andw ∈ Σ∗ ,we
i orig i i orig
havew =◦ w =⇒ p(w ) = p(w ).
i orig i orig
Notably,wedonothaveexplicitknowledgeof p(w ). Infact,thisisthedistributionwe
orig
trytoapproximatewhentrainingourlanguagemodel p (w ). Wecan,however,assume
θ orig
accesstosamplesfromp(w )throughourtrainingandtestdatasets.Toassumeasimilarac-
orig
cesstotraining/testsamplesofeachclonedlanguage,wemapeachsequenceinouroriginal
datasetstoaclonedlanguage,randomlysamplingwhichlanguageL tomapeachsequence
n
to. Wedenotethisclonedlanguagesamplingstrategyas p(L ). Thiscanbewrittenas:
n
(cid:41)
T Cr la oi nn e/ dte ls at nd ga ut aa gs eet D= { {w L(( kk )) }} KkK =1 ,, w L(k () k)∼ ∼p p(w (Lo )rig) =⇒ D
n
={w(k) |Ln =L(k)} (1)
k=1
Importantly,clonedlanguagesareperfectlyequivalent,havingthesamesyntax,semantics,
anddistribution. Theydifferonlyinthesymbolsusedtoencodetheirvocabularies. Any
generalisation we observe in this setting should thus serve as an upper bound on the
potential to generalise across non-identical natural languages.4 In other words, if our
modelcannotgeneraliseacrossclonedlanguages,wewouldhavestrongreasontobelieveit
shouldn’tgeneraliseacrossdistinctlanguages. Ifweobservethatamodelcangeneralise
acrossclonedlanguages,however,wemayormaynotobservethesametohappenacross
non-clonedlanguages. We’llinvestigatethelatterinSection5.
4.1 Generalisation
Due to the equivalence of cloned languages, one may expect language models to easily
generaliseacrossthem. Inthatcase,trainingamultilingualmodelondatasetsD andD
1 2
wouldleadtosimilarperformancetotrainingamonolingualmodelontheoriginaldatasetD
(notethat|D| = |D |+|D |). Weperformthisexperimenthere,trainingeithermonolingual
1 2
modelsonEnglish(EN),ormultilingualmodelsonclonedEnglish(EN andEN ),setting
1 2
p(EN ) = p(EN ) =0.5. Perhapssurprisingly,whentraininginthisbalancedmultilingual
1 2
setting,languagemodellingperformanceissignificantlyworsethaninthemonolingual
setting(seeTable1,rows2&4). Infact,onewouldobtainbetterperformancetrainingtwo
monolingualmodelsforhalfasmanystepsthantrainingonthiscombineddata. Training
datainonelanguageseemstohurtperformanceintheotherlanguageinsteadofboostingit.
Thisindicatesthatthemodelisnotabletogeneralisewellacrosslanguagesinthissetting.
Takeaway1. Themodeldoesnotgeneralisewellacrossclonedlanguagesgivena50⁄ 50datasplit.
2Wenotethatpriorworkusingthismethodology(Ketal.,2020;Dufter&Schu¨tze,2020)termedL
2
a“fake”language.SincethereisnodistinctionbetweenL andL ,however,wecallthemclonedlan-
1 2
guagesinstead.Otherrelatedstudieshaveinvestigatedtheeffectofinfinitelymanyclonedlanguages
onLMs’performance(Huangetal.,2023),oremployedduplicatedvocabulariesatthetokenlevelto
studytheirimpactonLMs’memorisationorperformance(Kharitonovetal.,2021;Scha¨feretal.,2024).
3Unlessotherwisenoted,thesevocabulariesaredefinedasdisjointsetsinourexperiments,meaning
thatnoanchorpointsexistacrosslanguages.
4Asformostofourexperimentsweconsiderclonedlanguages’alphabetstobedisjoint,inpractice
ourresultsonlyupperboundthecross-lingualgeneralisationofmodelswithnoanchorpoints(i.e.,
withdisjointvocabularies).
4Preprint
TrainingData PPL TEff
RunType Row #Tokens p(EN1) p(EN2) p(EN3),...,p(EN10) EN1 EN2 EN1 EN2
1 1.2B 100% 0% 0% 21.9 - 1 -
Monolingual 2 1.2B 50% 0% 0% 25.3 - 1 -
3 1.2B 10% 0% 0% 48.4 - 1 -
4 1.2B 50% 50% 0% 26.1 26.1 0.89 0.89
2languages
5 1.2B 90% 10% 0% 22.5 32.8 1.00 2.08
6 1.2B 10% 10% 10%,...,10% 35.5 35.7 1.69 1.67
10languages
7 1.2B 50% 1 1,..., 1 24.6 33.4 1.15 3.56
18 18 18
8 1.2B 100%↰0% 0%↱100% 0% >1B 31.4 - 0.47
Schedule 9 1.2B 90%↰10% 10%↱90% 0% 26.5 24.4 0.83 1.18
10 2.4B 50% 50% 0% 23.3 23.3 0.73 0.73
2xdata 11 2.4B 90%↰10% 10%↱90% 0% 22.8 20.4 0.83 1.60
12 3.6B 50% 50% 0% 22.2 22.2 0.64 0.64
3xdata 13 3.6B 90%↰10% 10%↱90% 0% 21.5 19.3 0.77 1.63
Table1: PerformanceoflanguagemodelstrainedondifferentcompositionsofEN andEN .
1 2
a%↰b%indicatesanimmediatedecreasefroma%downtob%halfwayduringtraining.
↱
Analogously,a% b%indicatesanimmediateincrease.
Figure1: LMperformancebyimbalanceratio. (left)LMperplexity. (right)LMaccuracy
onGLUE;modelswerefine-tunedinEN andevaluatedoneitherEN andEN .
1 1 2
4.2 Languageimbalance
Howdoesthebalanceofthelanguages’dataaffectgeneralisationperformance? Willthe
multilingual model still underperform its monolingual equivalents when trained on an
unevenlanguagedistribution? WhenvaryingtheratioofEN toEN datashownduring
1 2
training(whilekeepingthetotalnumberoftrainingstepsconstant),weobservethatthe
rarer“lowerresource”language,herealwaysEN ,benefitsfromthepresenceofadominant
2
“mainlanguage”. Fig.1(left)showsthat,underhigherimbalance,themodel’sperformance
onEN becomesmuchbetterthanthatofamonolingualmodeltrainedonthesameamount
2
ofEN
2
data. Forexample,whentraininginthe90⁄ 10regime,weobtainaTEff
EN2
ofover2
(seeTable1,row5). Dotheseimprovementstranslatetobettercross-lingualgeneralisation
ondownstreamtasksaswell? Wetestthisbyfine-tuningmodelsontheGLUEbenchmark
(Wang etal., 2019) in EN only, andevaluating them on EN and EN . We observethat
1 1 2
models trained under higher language imbalance indeed have significantly better EN
2
zero-shotperformance(seeFig.1right). Together,theseresultssuggestthatcross-lingual
generalisationisoccurring.
Isthisgeneralisationattainedduetothemodel’sinternalcomputationsbeingsharedacross
languages? To answer this question, we analyse how language imbalance affects the
cross-lingualalignmentofourmodels’representations. Lookingatthecosinesimilarity
of equivalent subwords w
1
=◦
w
2
in EN
1
and EN 2, we find that: (i) in the 50⁄
50
setting,
embeddingsarenotaligned(exhibitinganaveragecosinesimilarityof0.02);(ii)inthe90⁄
10
setting,equivalentsubwordsaremuchmorealigned(withasimilarityof0.28). Further,
analysingmodelstrainedwithotherimbalanceratios,wefindthatsimilarityincreaseswith
imbalance(seeFig.6inApp.Cfortheseresults,aswellasanisotropy-controlbaselines).
5Preprint
TrainingData Layer
p(EN ) p(EN ) 1 2 3 4 5 6 7 8 9 10 11 12
1 2
50% 50% 0.55 0.79 0.83 0.88 0.85 0.83 0.78 0.66 0.56 0.46 0.25 -0.21
90% 10% 0.86 0.93 0.96 0.96 0.96 0.96 0.96 0.95 0.94 0.90 0.67 0.11
∆ 0.31 0.14 0.13 0.09 0.11 0.14 0.18 0.28 0.38 0.44 0.42 0.32
Table2: Hiddenstates’similaritywhenLMisfedequivalentinputsinclonedlanguages.
ComparingthecosinesimilarityofhiddenstateswhentheLMisgivenequivalentsequences
w
1
=◦
w 2,wealsoobservestrongeralignmentforamodeltrainedintheimbalanced90⁄
10
regime,comparedtothe50⁄ 50counterpart(seeTable2). Interestingly,thecosinesimilarity
betweengradientsisalsohigherintheimbalancedsetting: whenprocessingequivalent
sequences,thegradientswithrespecttow orw haveanaveragecosinesimilarityof0.53
1 2
forthemodeltrainedinthe90⁄ 10setting,comparedto0.07inthe50⁄ 50setting(seefullplots
ofsimilaritiespermodelcomponentinApp.F).Thissuggeststhatthegradientupdates
withrespecttoonelanguagemaybenefittheoptimisationprocessofthatlanguage’scloned
counterpartmorewhentrainingunderhigherimbalance.
Takeaway2. Languageimbalanceimprovesgeneralisationandleadstorepresentationswhichare
morealignedacrossclonedlanguages.
4.3 Manylanguages
How does this trend transfer to settings with more than two languages? In such cases,
sharingcircuitsacrosslanguagesmightbeevenmorecrucialduetothemodel’slimited
capacity. Insteadofcloningthelanguageonlyonce,wenowcloneitninetimes,obtaining
intotal10languages. InTable1(rows6&7),wereporttheperformancewhensamplingthe
languagesinabalancedwayandwhenhavingamuchstrongermainlanguage.
Interestingly,whensamplinguniformly,weobtainTEff≈1.7;performanceisthusbetter
than with a monolingual model trained on an equivalent amount of monolingual data
(comparerows6&3). Thisdiffersfromourobservationsforthebilingualsetting,where
uniform language sampling performed worse than the equivalent monolingual models.
Presumably,modellingthismanylanguageseffectivelywithlimitedmodelcapacitymay
leadthemodeltoshareitscircuits,improvingcross-lingualgeneralisation(Dufter&Schu¨tze,
2020). Thelimitofinfinitelanguages(inwhichamodelneverobservesthesamelanguage
morethanonce)wasanalysedbyHuangetal.(2023);interestingly,LMsstillseemtolearn
thelanguage,tosomeextent,eveninthatsetting.
Intheimbalancedsettingwherewesampleastronger“mainlanguage”50%ofthetime,
weobserveevenstrongerperformanceonalllanguages. Despitethemodelseeingonly
roughly 67M tokens in each of the rarer languages (1/18 of all steps), it achieves better
performanceintheselanguagesthanintheuniformsettingwith120Mtokens(1/10ofall
steps)perlanguage.Infact,ontherarerlanguages,themodelachievesTEff≈3.6,matching
theperformanceofamonolingualmodeltrainedon240Mtokens.
Takeaway3. Whentrainingonmanyclonedlanguages,samplingamainlanguagedisproportion-
atelyimprovesgeneralisation.
4.4 Effectofscaling
ModelanddatasizearecrucialfactorsfortheperformanceofLMs.Here,weinvestigatehow
thepreviouslyidentifiedtrendsareaffectedbyscalingthemodelarchitectureortraining
data.Fig.2(left)showsthattheeffectofimbalanceoncross-lingualgeneralisationappearsto
increasewhenwetrainontwiceasmuchdata(2.4Btokensinsteadof1.2B),reachingTEff>
3;thiscorrespondstoa“chinchilla-optimal”setupforourGPTsmallmodel(Hoffmannetal.,
2022). Atthesametime,theTEffofthe50⁄ 50settingseemstobedecreasingunderprolonged
training.Thismightbecausedbytheheightenedimportanceofmodelcapacityunderlonger
6Preprint
Figure2: Performance(PPLandTEff)aswetrainLMswith(left)moredata,or(right)larger
architectures. mini,smallandmediumdenoteGPTsizesinLanguini(Stanic´ etal.,2023),
with11M,85M,and303Mnon-embeddingparameters,respectively.
training,whichmayhaveastrongerimpactonperformancewhenrepresentationsareless
alignedacrosslanguages.Overall,thedisparityineffectivenessbetweentheimbalancedand
balancedsettingsgrowswithlongertraining. Remarkably,whentrainingfor4.8Btokens,
the90⁄ 10settingyieldsbetterperformanceinbothlanguages,comparedtothe50⁄ 50setting.
When decreasing the model size, we also observe higher performance benefits in the
imbalancedsetting(seeFig.2right),potentiallyduetothecapacityargumentdescribed
above. Interestingly,however,theeffectofimbalanceappearstobesignificantlystrongerfor
largermodelsaswell. Whentrainingalargermodelwitharound300Mparameters(GPT
mediuminLanguini;Stanic´ etal.,2023),inthe90⁄ 10setting,weachievebetterperformance
onbothlanguagesthanunderthe50⁄ 50split. Thismightbebecauselargermodelsgenerally
exhibitbettergeneralisationabilitythansmallerones(Brownetal.,2020).
Takeaway 4. Longer training and larger models lead to stronger performance benefits due to
languageimbalance.
4.5 Languagesamplingschedule
Knowingthatlanguageimbalanceboostsgeneralisation,howcanweusethisinsightto
trainbettermodels? Isthereawaytoleverageourinsightsinordertoimproveperformance
ontwolanguages,evenwiththesametrainingdata? InTable1(rows8,9,11,and13),we
reportresultswhentrainingwithalanguagesamplingschedulethatensuresalanguage
imbalancethroughoutalloftraining,butwhichstillleadstoanoverall50⁄ 50splitbetween
EN andEN dataseenbythemodel. WesampleEN withahigherprobabilityduring
1 2 1
thefirsthalfoftraining. Then,wesampleEN 2moreoftentoachieveamarginalsplitof50⁄ 50.
When showing exclusively EN
1
at first, and then showing only EN
2
(100↰0⁄ 0↱100; row 8),
we observe bad overall performance. By the end of training, perplexity on EN is very
1
high,presumablyduetocatastrophicforgetting(McCloskey&Cohen,1989;French,1999).
Further,EN doesnotseemtobenefitfromtheEN data,achievingverylowperformance,
2 1
whichmightbeduetothelowerlearningrateinthesecondhalfoftraining.
Ontheotherhand,ifweavoidcatastrophicforgetting,makingsurethatthemodelencoun-
tersatleastsomesamplesofbothlanguagesateverypointintraining,viaa90↰10⁄ 10↱90split
(firstsamplinglanguageswithratio90⁄ 10,andthenswitchingto10⁄ 90afterhalfoftraining),we
canmitigatetheseissues. Onourstandardtrainingset(1.2Btokens,row9),weobserve
almostequivalentperformancetouniformlanguagesamplingonEN , butsignificantly
1
improvedperformanceonEN . Underlongertraining,thesebenefitsbecomemorepro-
2
nounced: thislanguagescheduleimprovesperformanceonbothlanguagescomparedto
thesimple50⁄ 50setting(comparerow10vs11androw12vs13).
Takeaway5. Comparedtouniformlanguagesampling,animbalancedratiothroughouttraining
canleadtobetterresultsonalllanguages,eveniftheoveralllanguagesplitremainsbalanced.
7Preprint
TrainingData PPL TEff
RunType Row #Tokens p(EN) p(FR) EN FR EN FR
1 1.2B 100% 0% 21.9 - 1 -
2 1.2B 50% 0% 25.3 - 1 -
3 1.2B 10% 0% 48.4 - 1 -
Monolingual
4 1.2B 0% 100% - 16.0 - 1
5 1.2B 0% 50% - 18.4 - 1
6 1.2B 0% 10% - 34.1 - 1
7 1.2B 50% 50% 26.4 19.4 0.85 0.82
Multilingual 8 1.2B 90% 10% 22.5 31.9 1.00 1.05
disjointvocabs 9 1.2B 10% 90% 43.5 16.4 1.10 0.97
10 1.2B 90%↰10% 10%↱90% 29.1 20.5 0.60 0.66
11 1.2B 50% 50% 26.0 19.0 0.91 0.88
12 1.2B 90% 10% 22.5 29.0 1.00 1.27
Multilingual 13 1.2B 10% 90% 39.5 16.5 1.33 0.96
anchoredvocabs 14 1.2B 90%↰10% 10%↱90% 28.9 19.3 0.61 0.83
15 1.2B 90%↰10%↱50%→50% 10%↱90%↰50%→50% 26.4 18.5 0.85 1.00
16 1.2B 95%↰35%→35%→35% 5%↱65%→65%→65% 26.3 18.7 0.86 0.95
17 2.4B 50% 50% 23.0 16.9 0.79 0.76
2xdata 18 2.4B 90%↰10% 10%↱90% 26.1 17.1 0.44 0.70
19 3.6B 50% 50% 21.8 16.0 0.70 0.67
3xdata 20 3.6B 90%↰10% 10%↱90% 25.1 16.2 0.35 0.63
Table 3: Performance of language models trained on different compositions of EN and
FR. a%→b%→c%→d%indicatesafourstagelanguageschedule,switchingimmediately
between,e.g.,c%andd%after75%oftraining.
5 Reallanguages
Toverifywhethertheinsightsfromourcloned-languageexperimentsholdinamorenatural
setting,wenowrunexperimentswithmultilingualmodelsonEnglish(EN)andFrench(FR).
5.1 Generalisation
Intheclonedsetting,weobservednosignificantgeneralisationwhentrainingonabalanced
languagemix(i.e.,TEff<1,representationswereunaligned,andzero-shotGLUEaccuracy
onEN wasbad).Similarly,whensamplingENandFRdatauniformly,weobtainTEff<0.7.
2
Amultilingualmodel’sperformanceisthusworsethanamonolingualmodeltrainedonly
inthesameENorFRdata(seeTable3,row7). Notably,priorworkhasidentifiedanchors
(sharedvocabularyitemsacrosslanguages)helpgeneralisation(Dufter&Schu¨tze,2020).We
thusexperimentwithsimilarlymergingvocabularyitemssharedbetweenENandFR,and
confirmthishelpsperformance(compareTable3,row7vs11). Werunmoreexperiments
analysingtheimpactofanchorpointsinbothclonedandreallanguages,seeApp.D.Note
that,withananchoredvocabulary,generalisationacrossENandFRisnotnecessarilyupper
boundedbyourresultsondisjointclonedlanguages. Infact,inthe50⁄ 50setting,weobservea
marginallyhigherTEffforEN–FRmodelswithananchoredvocabularythanforEN –EN
1 2
modelswhereweuseddisjointvocabularies(compareTable1row4andTable3row11).
5.2 Languageimbalance
Analogous to the cloned setting, we observe that an imbalanced EN/FR ratio leads to
improvedperformance(TEff > 1),ontherarerlanguage(seeTable3,rows7-9&11-13).
Thisisthecaseforboth,a90⁄ 10anda10⁄ 90EN/FRratio. Fig.3showsPPLandTEffinENandFR
asafunctionofthelanguageimbalance. Weobservethatlargeimbalancesgenerallyseemto
yieldTEff>1;theworstTEffisreachedwithabalancedEN/FRratio. Thesetrendsareinline
withourfindingsintheclonedsetting. However,especiallywithdisjointvocabularies,the
observedperformancebenefitsduetogeneralisationarelesssignificant. Presumably,this
isduetoENandFRnotbeingequivalentandthusgenerallyallowinglessgeneralisation.
Does imbalance again improve generalisation due to a better alignment of the model’s
representationsinthetwolanguages? Asintheclonedlanguagessetting,weinvestigate
8Preprint
Figure3: LMperformanceonENandFRbyimbalanceratio.
thecosinesimilaritybetweenthemodels’gradientsintwo“equivalent”sequencesinthe
twolanguages. Forreallanguages,however,wedonothaveaccesstoperfectlyequivalent
sequences. Instead,wemimickthisscenariousingparalleltranslatedsequencesinthetwo
languages,whichshouldcontainroughlysimilarproperties. Perhapssurprisingly,inthis
case,wefindmarginallyhighergradientsimilaritiesinthebalancedthanintheimbalanced
setting(seeApp.F).Thisistrueforbothmodelswithdisjointandanchoredvocabularies.
WethusdonotfindevidencethattheimprovedTEffintheimbalancedsettingiscausedby
astrongeralignmentofmodelupdatesacrosslanguagesinthissetting.
Takeaway6. Imbalancedmultilingualitybooststheperformanceofreallowresourcelanguages.
However,thiseffectisslightlyweakerherethanforclonedlanguages. Further,forreallanguages,we
donotfindevidenceofhighergradientupdatesalignmentduetolanguageimbalance.
5.3 Effectofscaling
Intheclonedsetting,weobservedthatprolongingtrainingsignificantlydecreasedTEffin
the50⁄ 50setting. Wehypothesizedthatthismightbecausedbyastrongerinfluenceofthe
limitedmodelcapacitywithlongertraining,andpoorsharingofrepresentationsbetween
languages. AsENandFRaredistinctlanguagesthatrequireatleastsomelanguagespecific
representations,wemightexpectthistrendtobeevenmorepronouncedfortheselanguages.
However,comparedtotheclonedsetting,prolongingtrainingleadstoasmallerdecline
in TEff in the 50⁄
50
setting here. Presumably, the anchored vocabulary allows for better
generalisationcomparetotheclonedsetting,despitethelanguagesbeingdistinct.
Further,unlikeintheclonedsetting,longertrainingsignificantlydecreasestheTEffofthe
lower-resourcelanguageintheimbalancedsettinghere(seeFig.4). Infact,the90⁄ 10TEffeven
fallsbelow1,approachingtheTEffofthe50⁄ 50setting. Thissuggeststhatlanguageimbalance
mightnotimprovegeneralisationacrossdistinctreallanguages. Still,whenscalingupthe
model,weobserveanincreaseofalmost2xintheTEffofthelower-resourcelanguage(see
Fig.4). Thisisinlinewithourclonedlanguagesobservations,althoughtheeffectisweaker.
Takeaway7. Performancebenefitsforreallow-resourcelanguagestendtodecreaseorvanishwith
longertraining. Largermodels,however,generalisebetteracrosslanguages.
5.4 Languagesamplingschedule
Forequivalentclonedlanguages,wefoundthatanimbalancedlanguagesamplingschedule
canleadtoimprovementsuponsimpleuniformsampling. Ifthisheldforreallanguages
aswell,itcouldhaveimportantpracticalimplicationsforfuturemultilingualLMtraining.
However,whereasa90↰10⁄ 10↱90scheduleyieldedstrongperformanceonclonedlanguages,
matchingoroutperformingthe50⁄ 50setting,thisisnotthecaseforENandFR(seeTable3,
row10vs7androw14vs11). Furthermore,andsimilarlytotheprevioussection,longer
trainingdoesnotmakethisschedulemoreeffective, butinsteadincreasesitsgaptothe
performanceofthe50⁄ 50setting(seerows17-20).
9Preprint
Figure4: Performance(PPLandTEff)ofmodelsonENandFRwithanchoredvocabaswe
trainthemwith(left)moredata,or(right)largerarchitectures.
Thediscrepancybetweentheseresultsandtheonesinclonedlanguagesmightbeexplained
by the reduced effect of imbalance on the generalisation and representation alignment
inreallanguages. Theseschedules maybeenoughto forceLMsto sharecircuitsacross
cloned languages, but not across real ones. Another reason for this discrepancy could
be that, early in training, GPT small models mainly capture local n-gram statistics, but
little semantic information (Chang & Bergen, 2022). While the latter generalises across
language,theformerdoesnot. Hence,amorecarefullydesignedlanguageschedulemight
still be beneficial, although further research would be required.5 To investigate if this
negativeresultwasaparticularpropertyofourchosenschedule, weinvestigatedother
more complex scheduling options. In general, none of the tested schedules appears to
outperformthe50⁄ 50setting(seerows15,16)onbothlanguages. However,morecomplex
4-stageschedules,canobtainbetterperformanceononelanguagewhileincurringaslight
performance drop in the other. Intriguingly, this allows trading off the performance of
differentlanguageswithoutalteringthetrainingdata.
Takeaway8. Forreallanguages,wedonotfindimprovementsonalllanguagesduetothetested
languageschedules. However,theyallowfortradingoffperformanceindifferentlanguages.
6 Conclusion
Weranexperimentstomeasurecross-lingualgeneralisationinbothacontrolledsettingwith
clonedEnglishlanguages,aswellaswithEnglishandFrench. Inbothsettings,wefindthat,
withoutvocabularyoverlap,ourmodelsdonotshowstrongcross-lingualgeneralisation
whentrainedonabalancedlanguageset. However,whentrainingonanimbalancedmixof
languages,weobserveincreasedperformancecomparedtomonolingualsettings.Forcloned
languages,wefindthatthiscanbeexplainedbyahigheralignmentofthemodel’srepresen-
tationsacrosslanguages,whichindicatescircuitreuseandimprovedcross-lingualgeneral-
isation. Yet,suchacorrelationislessevidentinreallanguages. Whileourfindingsallowus
todesignanimbalancedlanguageschedulethatyieldsimprovedperformanceinthecloned
setting,furtherresearchisrequiredtoextendtheseimprovementstoreal-worldsettings.
Acknowledgements
The authors would like to thank Clara Meister, Alexander Ha¨gele, and Jakob Heiss for
feedbackonearlyversionsofthismanuscript.
5Thismightincludeathoroughanalysisoftherelationshipbetweenlanguagescheduleandlearning
rateschedule.
10Preprint
References
OrevaogheneAhia,SachinKumar,HilaGonen,JungoKasai,DavidMortensen,NoahSmith,
andYuliaTsvetkov.Doalllanguagescostthesame?Tokenizationintheeraofcommercial
language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing, pp. 9904–9923, Singapore, 2023. Association for Computational
Linguistics. URLhttps://aclanthology.org/2023.emnlp-main.614.
Jesujoba O Alabi, Marius Mosbach, Matan Eyal, Dietrich Klakow, and Mor Geva. The
hiddenspaceoftransformerlanguageadapters. arXivpreprintarXiv:2402.13137, 2024.
URLhttps://arxiv.org/abs/2402.13137.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,Sandhini
Agarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,Aditya
Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemodels
arefew-shotlearners. InAdvancesinNeuralInformationProcessingSystems,volume33,
pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
NickCammarata,ShanCarter,GabrielGoh,ChrisOlah,MichaelPetrov,LudwigSchubert,
Chelsea Voss, Ben Egan, and Swee Kiat Lim. Thread: Circuits. Distill, 2020. URL
https://distill.pub/2020/circuits.
Tyler A. Chang and Benjamin K. Bergen. Word acquisition in neural language models.
Transactions of the Association for Computational Linguistics, 10:1–16, 2022. URL https:
//aclanthology.org/2022.tacl-1.1.
TylerAChang,CatherineArnett,ZhuowenTu,andBenjaminKBergen. Whenismulti-
lingualityacurse? languagemodelingfor250high-andlow-resourcelanguages. arXiv
preprintarXiv:2311.09205,2023. URLhttps://arxiv.org/pdf/2311.09205.pdf.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek,FranciscoGuzma´n,EdouardGrave,MyleOtt,LukeZettlemoyer,andVeselin
Stoyanov. Unsupervisedcross-lingualrepresentationlearningatscale. InProceedings
ofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.8440–8451.
AssociationforComputationalLinguistics,July2020a. URLhttps://aclanthology.org/
2020.acl-main.747.
AlexisConneau,ShijieWu,HaoranLi,LukeZettlemoyer,andVeselinStoyanov. Emerging
cross-lingualstructureinpretrainedlanguagemodels. InProceedingsofthe58thAnnual
Meeting of the Association for Computational Linguistics, pp. 6022–6034. Association for
ComputationalLinguistics,July2020b. URLhttps://aclanthology.org/2020.acl-main.
536.
PhilippDufterandHinrichSchu¨tze. IdentifyingelementsessentialforBERT’smultilingual-
ity. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP), pp. 4423–4437. Association for Computational Linguistics, November 2020.
URLhttps://aclanthology.org/2020.emnlp-main.358.
NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,
AmandaAskell,YuntaoBai,AnnaChen,TomConerly,NovaDasSarma,DawnDrain,
DeepGanguli,ZacHatfield-Dodds,DannyHernandez,AndyJones,JacksonKernion,
LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,JackClark,JaredKaplan,Sam
McCandlish,andChrisOlah. Amathematicalframeworkfortransformercircuits. Trans-
formerCircuitsThread,2021. URLhttps://transformer-circuits.pub/2021/framework/
index.html.
Yukun Feng, Feng Li, and Philipp Koehn. Toward the limitation of code-switching in
cross-lingualtransfer. InProceedingsofthe2022ConferenceonEmpiricalMethodsinNatural
11Preprint
LanguageProcessing,pp.5966–5971.AssociationforComputationalLinguistics,December
2022. URLhttps://aclanthology.org/2022.emnlp-main.400.
RobertM.French. Catastrophicforgettinginconnectionistnetworks. TrendsinCognitive
Sciences, 3(4):128–135, 1999. URL https://www.sciencedirect.com/science/article/
pii/S1364661399012942.
PhilipGage. Anewalgorithmfordatacompression. CUsersJournal,12(2):23–38,1994.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,
Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor
Leahy. ThePile: An800GBdatasetofdiversetextforlanguagemodeling. arXivpreprint
arXiv:2101.00027,2020. URLhttps://arxiv.org/pdf/2101.00027.pdf.
MichaelHanna,OllieLiu,andAlexandreVariengien. HowdoesGPT-2computegreater-
than?: Interpretingmathematicalabilitiesinapre-trainedlanguagemodel. Advancesin
NeuralInformationProcessingSystems,36,2024. URLhttps://arxiv.org/abs/2305.00586.
pdf.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,
ElizaRutherford,DiegodelasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,
Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals,
Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large
languagemodeltraining. InAdvancesinNeuralInformationProcessingSystems,2022. URL
https://openreview.net/forum?id=iBBcRUlOAPR.
QianHuang,EricZelikman,SarahLiChen,YuhuaiWu,GregoryValiant,andPercyLiang.
Lexinvariantlanguagemodels.InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023. URLhttps://openreview.net/forum?id=NiQTy0NW1L.
KarthikeyanK,ZihanWang,StephenMayhew,andDanRoth. Cross-lingualabilityofmul-
tilingualBERT:Anempiricalstudy. InInternationalConferenceonLearningRepresentations,
2020. URLhttps://openreview.net/forum?id=HJeT3yrtDr.
EugeneKharitonov,MarcoBaroni,andDieuwkeHupkes. HowBPEaffectsmemorizationin
transformers. arXivpreprintarXiv:2110.02782,2021. URLhttps://arxiv.org/abs/2110.
02782.
DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InInterna-
tionalConferenceonLearningRepresentations,SanDiego,CA,USA,2015.
PhilippKoehn. Europarl: Aparallelcorpusforstatisticalmachinetranslation. InProceedings
ofMachineTranslationSummitX:Papers,pp.79–86,Phuket,Thailand,September13-15
2005. URLhttps://aclanthology.org/2005.mtsummit-papers.11.
AndreasKo¨pf,YannicKilcher,DimitrivonRu¨tte,SotirisAnagnostidis,ZhiRuiTam,Keith
Stevens,AbdullahBarhoum,DucMinhNguyen,OliverStanley,Richa´rdNagyfi,Shahul
ES, Sameer Suri, David Alexandrovich Glushkov, Arnav Varma Dantuluri, Andrew
Maguire,ChristophSchuhmann,HuuNguyen,andAlexanderJulianMattick. OpenAs-
sistantconversations-democratizinglargelanguagemodelalignment. InThirty-seventh
ConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2023.
URLhttps://openreview.net/forum?id=VSJotgbPHF.
Taku Kudo and John Richardson. SentencePiece: A simple and language independent
subwordtokenizeranddetokenizerforneuraltextprocessing. InProceedingsofthe2018
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing: SystemDemonstrations,pp.
66–71,2018. URLhttps://arxiv.org/pdf/1808.06226.pdf.
GuillaumeLampleandAlexisConneau. Cross-linguallanguagemodelpretraining. arXiv
preprintarXiv:1901.07291,2019. URLhttps://arxiv.org/abs/1901.07291.
12Preprint
MichaelMcCloskeyandNealJ.Cohen. Catastrophicinterferenceinconnectionistnetworks:
Thesequentiallearningproblem. InPsychologyofLearningandMotivation,volume24,pp.
109–165.AcademicPress,1989.URLhttps://www.sciencedirect.com/science/article/
pii/S0079742108605368.
Toma´sMikolov,KaiChen,GregCorrado,andJeffreyDean. Efficientestimationofword
representationsinvectorspace. In1stInternationalConferenceonLearningRepresentations,
WorkshopTrackProceedings,Scottsdale,Arizona,USA,2013. URLhttp://arxiv.org/abs/
1301.3781.
Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel
Artetxe. Liftingthecurseofmultilingualitybypre-trainingmodulartransformers. In
Proceedingsofthe2022ConferenceoftheNorthAmericanChapteroftheAssociationforCom-
putationalLinguistics: HumanLanguageTechnologies,pp.3479–3495,Seattle,UnitedStates,
July2022.AssociationforComputationalLinguistics. URLhttps://aclanthology.org/
2022.naacl-main.255.
PleIAs. French-PD-Books dataset. https://huggingface.co/datasets/PleIAs/
French-PD-Books,2024. Accessedin01/2024,HuggingFaceDatasetslibrary.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language models are unsupervised multitask learners. OpenAI Blog,
2019. URL https://cdn.openai.com/better-language-models/language models are
unsupervised multitask learners.pdf.
Machel Reid and Mikel Artetxe. PARADISE: Exploiting parallel data for multilingual
sequence-to-sequencepretraining.InProceedingsofthe2022ConferenceoftheNorthAmerican
ChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pp.
800–810,Seattle,UnitedStates,July2022.AssociationforComputationalLinguistics. URL
https://aclanthology.org/2022.naacl-main.58.
TevenLeScao,AngelaFan,ChristopherAkiki,ElliePavlick,SuzanaIlic´,DanielHesslow,
RomanCastagne´,AlexandraSashaLuccioni,Franc¸oisYvon,MatthiasGalle´,Jonathan
Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammana-
manchi,ThomasWang,BenoˆıtSagot,NiklasMuennighoff,AlbertVillanovadelMoral,
OlatunjiRuwase,RachelBawden,StasBekman,AngelinaMcMillan-Major,IzBeltagy,
Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo
Laurenc¸on,YacineJernite,JulienLaunay,MargaretMitchell,ColinRaffel,AaronGokaslan,
Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav,CanwenXu,ChenghaoMou,ChrisEmezue,ChristopherKlamm,ColinLeong,
DanielvanStrien,DavidIfeoluwaAdelani,DragomirRadev,EduardoGonza´lezPonfer-
rada,EfratLevkovizh,EthanKim,EyalBarNatan,FrancescoDeToni,Ge´rardDupont,
Germa´nKruszewski,GiadaPistilli,HadyElsahar,HamzaBenyamina,HieuTran,Ian
Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny
Chim,JesseDodge,JianZhu,JonathanChang,Jo¨rgFrohberg,JosephTobing,Joydeep
Bhattacharjee,KhalidAlmubarak,KimboChen,KyleLo,LeandroVonWerra,LeonWeber,
Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mun˜oz,
MaraimMasoud,Mar´ıaGrandury,MarioSˇasˇko,MaxHuang,MaximinCoavoux,Mayank
Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb,
Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espe-
jel,OnadeGibert,PauloVillegas,PeterHenderson,PierreColombo,PriscillaAmuok,
QuentinLhoest,RhezaHarliman,RishiBommasani,RobertoLuisLo´pez,RuiRibeiro,
Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan
Muhammad,ShanyaSharma,ShayneLongpre,SomaiehNikpoor,StanislavSilberberg,
SuhasPai,SydneyZink,TiagoTimponiTorrent,TimoSchick,TristanThrush,Valentin
Danchev,VassilinaNikoulina,VeronikaLaippala,VioletteLepercq,VrindaPrabhu,Zaid
Alyafeai,ZeerakTalat,ArunRaja,BenjaminHeinzerling,ChengleiSi,DavutEmreTas¸ar,
ElizabethSalesky,SabrinaJ.Mielke,WilsonY.Lee,AbheeshtSharma,AndreaSantilli,An-
toineChaffin,ArnaudStiegler,DebajyotiDatta,ElizaSzczechla,GunjanChhablani,Han
Wang,HarshitPandey,HendrikStrobelt,JasonAlanFries,JosRozen,LeoGao,Lintang
Sutawika,MSaifulBari,MagedS.Al-shaibani,MatteoManica,NihalNayak,RyanTeehan,
13Preprint
SamuelAlbanie,ShengShen,SrulikBen-David,StephenH.Bach,TaewoonKim,TaliBers,
ThibaultFevry,TrishalaNeeraj,UrmishThakker,VikasRaunak,XiangruTang,Zheng-Xin
Yong,ZhiqingSun,ShakedBrody,YallowUri,HadarTojarieh,AdamRoberts,HyungWon
Chung,JaesungTae,JasonPhang,OfirPress,ConglongLi,DeepakNarayanan,Hatim
Bourfoune,JaredCasper,JeffRasley,MaxRyabinin,MayankMishra,MinjiaZhang,Mo-
hammadShoeybi,MyriamPeyrounette,NicolasPatry,NouamaneTazi,OmarSanseviero,
PatrickvonPlaten,PierreCornette,PierreFranc¸oisLavalle´e,Re´miLacroix,SamyamRajb-
handari,SanchitGandhi,ShadenSmith,Ste´phaneRequena,SurajPatil,TimDettmers,
Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun
Subramonian,Aure´lieNe´ve´ol,CharlesLovering,DanGarrette,DeepakTunuguntla,Ehud
Reiter, EkaterinaTaktasheva, EkaterinaVoloshina, EliBogdanov, GentaIndraWinata,
HaileySchoelkopf,Jan-ChristophKalo,JekaterinaNovikova,JessicaZosaForde,Jordan
Clive,JungoKasai,KenKawamura,LiamHazan,MarineCarpuat,MirunaClinciu,Na-
joungKim,NewtonCheng,OlegSerikov,OmerAntverg,OskarvanderWal,RuiZhang,
Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina,
ThomasScialom,TianYun,TomaszLimisiewicz,VerenaRieser,VitalyProtasov,Vladislav
Mikhailov,YadaPruksachatkun,YonatanBelinkov,ZacharyBamberger,ZdeneˇkKasner,
AliceRueda,AmandaPestana,AmirFeizpour,AmmarKhan,AmyFaranak,AnaSantos,
AnthonyHevia,AntigonaUnldreaj,ArashAghagol,ArezooAbdollahi,AychaTammour,
AzadehHajiHosseini,BaharehBehroozi,BenjaminAjibade,BharatSaxena,CarlosMun˜oz
Ferrandis,DanielMcDuff,DanishContractor,DavidLansky,DavisDavid,DouweKiela,
DuongA.Nguyen,EdwardTan,EmiBaylor,EzinwanneOzoani,FatimaMirza,Frankline
Ononiwu,HabibRezanejad,HessieJones,IndraniBhattacharya,IreneSolaiman,Irina
Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra,
Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akin-
lolu,MichaelMcKenna,MikeQiu,MuhammedGhauri,MykolaBurynok,NafisAbrar,
NazneenRajani,NourElkott,NourFahmy,OlanrewajuSamuel,RanAn,RasmusKro-
mann,RyanHao,SamiraAlizadeh,SarmadShubber,SilasWang,SouravRoy,Sylvain
Viguier,ThanhLe,TobiOyebade,TrieuLe,YoyoYang,ZachNguyen,AbhinavRamesh
Kashyap,AlfredoPalasciano,AlisonCallahan,AnimaShukla,AntonioMiranda-Escalada,
AyushSingh,BenjaminBeilharz,BoWang,CaioBrito,ChenxiZhou,ChiragJain,Chuxin
Xu,Cle´mentineFourrier,DanielLeo´nPerin˜a´n,DanielMolano,DianYu,EnriqueMan-
javacas,FabioBarth,FlorianFuhrimann,GabrielAltay,GiyaseddinBayrak,GullyBurns,
Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde,
Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa
Shinzato,MadeleineHahndeBykhovetz,MaikoTakeuchi,MarcPa`mies,MariaACastillo,
MariannaNezhurina,MarioSa¨nger,MatthiasSamwald,MichaelCullan,MichaelWein-
berg,MichielDeWolf,MinaMihaljcic,MinnaLiu,MoritzFreidank,MyungsunKang,
NatashaSeelam,NathanDahlberg,NicholasMichioBroad,NikolausMuellner,Pascale
Fung,PatrickHaller,RamyaChandrasekhar,RenataEisenberg,RobertMartin,Rodrigo
Canalli,RosalineSu,RuisiSu,SamuelCahyawijaya,SamueleGarda,ShlokSDeshmukh,
ShubhanshuMishra,SidKiblawi,SimonOtt,SineeSang-aroonsiri,SrishtiKumar,Stefan
Schweter,SushilBharati,TanmayLaud,The´oGigant,TomoyaKainuma,WojciechKusa,
YanisLabrak,YashShaileshBajaj,YashVenkatraman,YifanXu,YingxinXu,YuXu,Zhe
Tan,ZhongliXie,ZifanYe,MathildeBras,YounesBelkada,andThomasWolf. BLOOM:A
176B-parameteropen-accessmultilinguallanguagemodel.arXivpreprintarXiv:2211.05100,
2023. URLhttps://arxiv.org/abs/2211.05100.
AntonScha¨fer,ThomasHofmann,ImanolSchlag,andTiagoPimentel. Ontheeffectof(near)
duplicatesubwordsinlanguagemodelling. arXivpreprintarXiv:2404.06508,2024. URL
https://arxiv.org/abs/2404.06508.
RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrare
words with subword units. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 1715–1725, Berlin, Germany,
August2016.AssociationforComputationalLinguistics. URLhttps://aclanthology.
org/P16-1162.
Aleksandar Stanic´, Dylan Ashley, Oleg Serikov, Louis Kirsch, Francesco Faccio, Ju¨rgen
Schmidhuber, ThomasHofmann, andImanolSchlag. Thelanguinikitchen: Enabling
14Preprint
languagemodellingresearchatdifferentscalesofcompute.arXivpreprintarXiv:2309.11197,
2023. URLhttps://arxiv.org/abs/2309.11197.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timothe´eLacroix,BaptisteRozie`re,NamanGoyal,EricHambro,FaisalAzhar,Aurelien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a. URL
https://arxiv.org/abs/2302.13971.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes,JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,Yuning
Mao,XavierMartinet,TodorMihaylov,PushkarMishra,IgorMolybog,YixinNie,Andrew
Poulton, JeremyReizenstein, RashiRungta, KalyanSaladi, AlanSchelten, RuanSilva,
EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,RossTaylor,
AdinaWilliams, Jian XiangKuan, Puxin Xu, Zheng Yan, IliyanZarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
SergeyEdunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchat
models.arXivpreprintarXiv:2307.09288,2023b.URLhttps://arxiv.org/pdf/2307.09288.
pdf.
AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelR.Bowman.
GLUE:Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding.
In International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=rJ4km2R5t7.
Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and Robert West. Do Llamas
work in English? On the latent language of multilingual transformers. arXiv preprint
arXiv:2402.10588,2024. URLhttps://arxiv.org/abs/2402.10588.
15Preprint
A Limitationsandavenuesforfutureresearch
Dataandmodelsize. Whileweconductexperimentswithvaryingdata(upto4.8Btokens)
andmodelsize(upto336Mparameters),itisuncertainwhethertheidentifiedtrendsalso
applyatthescaleofmodernlargelanguagemodels. Additionally,formorecapablemodels,
cross-lingual generalisation might be relevant in different aspects, with, e.g., semantics
playingalargerrole. Asthesemanticcontentcommunicatedindifferentlanguagesmight
beeasilytransferable,thismightimpactgeneralisationdynamics.
Languages. We only run experiments on English and French, two Indo-European lan-
guages.Furtherworkcouldconsidermorelanguagesandinvestigatetheimpactoflanguage
similarityinresultsmorebroadly.
Model architecture. We run most of our experiments on a Transformer decoder (we
alsomeasureembeddingalignmentforsimplerWord2Vecmodels). Futureresearchcould
analysetheeffectsofarchitectureinmoredepthtobetterunderstandthedriversofrep-
resentation alignment. Conneau et al. (2020b), e.g., find that shared parameters in the
toplayersleadtobettercross-lingualtransfer. InourWord2Vecexperiments,wedonot
observeimprovementsinrepresentationalignmentduetolanguageimbalance(seeFig.6),
presumablyduetonoparametersbeingsharedbetweenthetwolanguages. Wouldthis
changewhenaddingasharedlayertotheWord2Vecmodel?
Downstreamperformance. Inourevaluationwemainlyrelyonperplexityasametric,
withasingleexperimentonGLUEaccuracy. Itmightbeinsightfultoanalyzeeffectson
downstreamtaskperformancemorebroadly.
Quantifyinggeneralisation. Inthiswork,wemainlymeasurecross-lingualgeneralisation
bycomparingtheperformanceofmultilingualmodelswiththatofmonolingualmodels
trained on the same amount of data in the given language. If a multilingual model on
languages L and L requiresfewer L tokenstoreachagivenperplexityon L thana
A B A A
monolingualmodel,wespeakofcross-lingualgeneralisation,knowingthatperformance
onL musthavebeenboostedbydatainlanguageL . Futureworkcouldformalisethis
A B
measureandaimtomodel/quantifytherelationshipbetweenthenumberoftrainingtokens
seeninalanguageL andperformanceinanotherlanguageL ,dependingonmodelsize,
B A
language imbalance, language similarity, anchor points, and other factors. An accurate
modeloftheserelationshipscouldbeofsubstantialpracticalvalue.
16Preprint
B Fittedscalinglaws
Topredicttheperformanceofmonolingualmodelsdependingontheamountoftokens
theyaretrainedon,wefitapowerlawcurvetopredicttherelationshipbetweennumber
oftrainingtokensandperplexityformodelsofallthreesizesandforbothlanguages(see
Fig.5).
Figure 5: Fitted power laws curves predicting perplexity depending on the fraction of
trainingtokens(comparedtoourstandard1.2Btokens)fordifferentlanguagesandmodel
sizes.
17Preprint
C AlignmentofEN andEN representations
1 2
While,underbalancedlanguagesampling,embeddingsofcorrespondingsubwordsarenot
muchmorealignedthanembeddingsofrandompairs,weobserveanincreaseincosine
similaritywithincreasinglanguageimbalance: from0.02for50⁄ 50to0.28for95⁄ 5(seeFig.6).
Fig.7showsthatthisalignmentishigherforfrequentsubwords. Thisseemsnatural: at
initialisation,subwordembeddingsarerandomandnotaligned. Then,theybecomemore
andmorealignedoverthecourseoftraining.
Interestingly, theembeddingsofasimpleword2vec(Mikolovetal.,2013)modeldonot
showstrongeralignmentunderhigherimbalance. Thismightbeduetoalackofshared
parametersbetweenthelanguages(Conneauetal.,2020b).
Figure6: EmbeddingcosinesimilarityofcorrespondingduplicatesubwordsfromENand
EN andrandompairstocontrolforanisotropy. Left: ourGPTmodel. Right: Word2vec
fake
embeddingstrainedonthesamedata(computedwithGensim).
Figure7: EmbeddingcosinesimilarityofcorrespondingduplicatesubwordswfromEN
1
andw′ fromEN ,byfrequencyoftherarerEN version.
2 2
18Preprint
D Anchorpoints
D.1 Anchorsonclonedlanguages
Asdescribedearlier, previousworksfoundthatanchorpoints—i.e., lexicalitemswhich
overlapbetweenlanguages—canleadtobettergeneralisationandalignmentofrepresenta-
tions(Dufter&Schu¨tze,2020). Inourclonedsetting,wecaninvestigatethisinacontrolled
manner by varying the number of vocabulary elements we duplicate. While in the ex-
perimentsdescribedabovewecreatedEN byduplicatingtheentirevocabulary,wenow
2
duplicateonlyafraction. TheremainingvocabularyissharedbetweenEN andEN . Inthis
1 2
experiment,weobservethatasmallnumberofanchorpointsalreadysignificantlyboosts
modelperformance(seeFig.8),whichindicatesimprovedgeneralisation.
Figure8: Perplexitybypercentageofanchorpoints, i.e., overlapbetweenEN andEN
1 2
vocabularies.
D.2 Anchorsonreallanguages
English and French vocabularies naturally overlap, having common subwords. These
sharedelementspotentiallyactasanchors,facilitatingbettercross-lingualgeneralisation.
However,theeffectivenessofsuchanchorpointsmaybemoderatedbysemanticdifferences;
forinstance,asharedsubwordmightcarryadifferentmeaningorconnotationsinEnglish
andFrench,affectingitsutilityasananchor. Despitethesenuances,anchorpointsappearto
boostgeneralisationbetweenreallanguages: whenwemergetheENandFRvocabularies,
weobtainbetterperformanceonbothlanguages(compareTable3,row7vs11)aswellas
higheralignmentofgradients(seeApp.F).Thisalignswithourfindingsfromthecloned
languagesetting(seeApp.D.1). Giventhesebenefits,itisnaturaltouseananchored(i.e.,
merged)vocabularywhenpossible.6
6Inpractice,thisisusuallyachievedbytrainingatokeniseronmultilingualdata,insteadofmerging
monolinguallytrainedvocabularies.
19Preprint
E Largermodelsandmoredata
Fig.9andFig.10containresultsforthefullarrayofmodel-anddatasetsizecombinations
weranforclonedlanguagesandforEnglishandFrench,respectively.
Figure 9: Performance with balanced and imbalanced EN and EN data for different
1 2
configurationsofmodel-anddatasetsize
20Preprint
Figure10: PerformancewithbalancedandimbalancedENandFRdatafordifferentconfig-
urationsofmodel-anddatasetsize
21Preprint
F Gradientsimilarity
Here,wecomparethecosinesimilarityoftrainedmodels’gradientswithrespecttoparallel
sequencesintwodifferent(possiblycloned)languages. Forclonedlanguages,thealignment
betweengradientsissignificantlyhigherforthemodeltrainedintheimbalanced90⁄ 10setting
(seeFig.11). ForENandFRdata,thisdoesnotseemtobethecase,whetherthevocabulary
isanchored(seeFig.12)ordisjoint(seeFig.13). However,undertheanchoredvocabulary,
the gradient similarities appear to be generally higher, suggesting better cross-lingual
representationalignment.
Figure11: SimilarityofgradientswithrespecttoparallelsequencesinEN andEN for
1 2
modelstrainedinbalancedandimbalancedsettings. Macroaveragefor50⁄ 50: 0.07. Macro
averagefor90⁄ 10: 0.53
Figure12:SimilarityofgradientswithrespecttoparallelsequencesinENandFRformodels
withanchored(i.e.,merged)vocabulary,trainedinbalancedandimbalancedsettings.Macro
averagefor50⁄ 50: 0.17. Macroaveragefor90⁄ 10: 0.14
22Preprint
Figure13:SimilarityofgradientswithrespecttoparallelsequencesinENandFRformodels
withdisjointvocabularies,trainedinbalancedandimbalancedsettings. Macroaveragefor
50⁄ 50: 0.05. Macroaveragefor90⁄ 10: 0.04
23