WESE: Weak Exploration to Strong Exploitation for LLM Agents
XuHuang1, WeiwenLiu2, XiaolongChen1, XingmeiWang1,
DefuLian1∗, YashengWang2, RuimingTang2, EnhongChen1
1UniversityofScienceandTechnologyofChina,Hefei,China
2HuaweiNoah’sArkLab,Shenzhen,China
xuhuangcs,chenxiaolong,xingmeiwang@mail.ustc.edu.cn,{liandefu,cheneh}@ustc.edu.cn,
{liuweiwen8,wangyasheng,tangruiming}@huawei.com
Abstract workstoinvestigatethepotentialofLLMagentsinenhancing
theircapabilitiesforopen-worldtasks.
Recently, large language models (LLMs) have
Benefiting from the capabilities of LLMs in instruction-
demonstratedremarkablepotentialasanintelligent
followingandfew-shotlearning,mostmethodsguideLLMs
agent. However, existing researches mainly focus in decision-making tasks through human-crafted design,
on enhancing the agent’s reasoning or decision- avoiding the costly fine-tuning of LLMs [Wei et al., 2022;
makingabilitiesthroughwell-designedprompten- Wang et al., 2022b; Yao et al., 2022; Kojima et al., 2022].
gineering or task-specific fine-tuning, ignoring the
Existing prompt-engineering approaches primarily consider
procedure of exploration and exploitation. When
two factors: how to incorporate task-relevant information
addressingcomplextaskswithinopen-worldinter-
in the prompt, and how to elicit the reasoning ability of
activeenvironments, thesemethodsexhibitlimita- LLMs through prompts. Task-relevant information encom-
tions. Firstly,thelackofglobalinformationofen-
passestaskdescriptionsandcontextualfeedback,suchasthe
vironments leads to greedy decisions, resulting in
questionandpertinenttaskstatementsinquestion-answering
sub-optimal solutions. On the other hand, irrel-
tasks, along with textual materials retrieved by the agent
evant information acquired from the environment
from the web while problem-solving. To enhance the rea-
notonlyadverselyintroducesnoise,butalsoincurs soning capabilities of LLM agents, methods like CoT [Wei
additional cost. This paper proposes a novel ap- et al., 2022], ReAct [Yao et al., 2022] Reflexion [Shinn et
proach, Weak Exploration to Strong Exploitation al.,2023],etal,inspireLLMstoengageinreasoningbycon-
(WESE),toenhanceLLMagentsinsolvingopen-
structingfew-shotexampleswithexplicitreasoningpaths.
world interactive tasks. Concretely, WESE in-
However,open-worldtasksserveasasimulationofthereal
volvesdecouplingtheexplorationandexploitation
environment, wherein an agent explores and interacts con-
process, employing a cost-effective weak agent to
tinuously with the environment to acquire more information
performexplorationtasksforglobalknowledge. A
forsolvingcomplextasks[Coˆte´ etal.,2019;Shridharetal.,
knowledgegraph-basedstrategyisthenintroduced
2020; Wang et al., 2022a]. There are several characteristics
to store the acquired knowledge and extract task-
ofsuchtasks,makingthemmorechallenging. Theabilityof
relevant knowledge, enhancing the stronger agent
LLM agents is far from optimal due to the following chal-
in success rate and efficiency for the exploitation
lenges:1)Complexity.Eachtaskinvolvesmulti-stepactions
task. Our approach is flexible enough to incorpo-
and each task can have multiple feasible solutions. 2) Un-
ratediversetasks,andobtainssignificantimprove-
certainty. Theagentcannotobtainalltheinformationfrom
ments in both success rates and efficiency across
the initial task description, and additional information must
fourinteractivebenchmarks.
beacquiredthroughexploration. Regardingthesechallenges,
solvingthesetasksnecessitatesmulti-stepexplorationandex-
ploitation by the agent. Exploration involves perceiving the
1 Introduction
environment and obtaining task-relevant information, while
Large language models (LLMs) showcase a myriad of ca- exploitation involves making action decisions based on ex-
pabilities across diverse domains, encompassing human- istingknowledge. Inexistingprompt-basedmethods,explo-
computerconversation,instructionfollowing,reasoning,and rationandexploitationissuesareoftenoverlooked,embedded
few-shotlearning[Zhaoetal., 2023]. Thesecomprehensive within the reasoning process of the LLM [Yao et al., 2022],
abilitiesformarobustfoundation,positioningLLMsasintel- leadingtotwomajorproblems.
ligentagentsinsolvingopen-worldtasks,suchashousehold Firstly,thelackofglobalawarenessoftheenvironmentat
tasksandopen-worldquestion-answeringtasks[Wangetal., the outset solutions results in suboptimal decision-making
2023b;Xietal.,2023]. Recently,therehavebeennumerous bytheLLM.AsillustratedinFigure1(a), thegoalistofind
one aluminum object and test its conductivity. The agent is
∗DefuLianisthecorrespondingauthor. located outside initially. The best trajectory is marked with
4202
rpA
11
]IA.sc[
1v65470.4042:viXraDuringexploitation,weadoptaone-hopknowledgeretrieval
approach, selecting one-hop neighbors of task-relevant enti-
ties from the graph as priors, thereby reducing interference
from irrelevant information. Furthermore, to further mini-
mizeresourceconsumption, weobservethatacost-effective
Trapped Suboptimal
weaker LLM (such as a 7B model) is fully capable of the
less challenging exploratory tasks. Therefore, we propose
the strategy of weak exploration to strong exploitation—
Optimal leveraging the knowledge explored by the weak LLM agent
toenhancetheperformanceofthestrongLLMagent.
(a) ScienceWorld. Lack of global environmental information Ourmaincontributionsaresummarizedasfollows:
causesfailureduetotrappinginalooporsub-optimalsolution.
• Tothebestofourknowledge,thisisthefirstworktoinves-
tigatetheeffectofdecouplingexplorationandexploitation
Question: Who created the show with Wendy Schaal doing the
for LLM agents in open-world tasks. We further propose
voice of Francine?
WESE,leveragingaweakeragenttoenhancethestronger
Massive irrelevant information
Search[Wendy Schaal] agentinacost-effectivemanner.
Wendy Schaal (born July 2, 1954)[1] is an American actress known for her work • Tobetterleveragetheenvironmentalinformationobtained
in Joe Dante films such as Innerspace, The 'Burbs, and Small Soldiers. Her other
film credits include starring in films such as Where the Boys Are '84, Creature, from exploration, we introduce a strategy to compress it
Going Under, and Munchies. She had many guest roles in television series of
t ph re im m ai rd il- y1 9 w8 o0 rs k, em do is nt vn oo it ca eb l ay c a tis n M g,a mril oyn st K ne ols ty a bin l yA vir ow ico il nf. g S i Fn rc ae n c2 i0 n0 e5 S s mh ie t hh a ins Helpful intoaknowledgegraph.Thenwedeviseaone-hopretrieval
t C Sh che hic aa a an g l.i o [m 1, ]Ia l Slt i ce n hd o a ic s a,o l 't sm h fee a d td hy a e ut rge wl he atv e si r s m i oo afn rL r o is ee is dr i S te ocs h aA a cam tl r ee (n sr é sic e Va n T ar l eeD raa iecd y H! ). aaS rnc pdh e a raa c fl rt w oo mra Rs 1ib c 9o h 6ar 4n r d ti o n 1 fo. r( ,W Fre an nd cy in S ec h Sa ma il t, hv )oiced approachtofilterouttheirrelevantinformation.
1 w m9 a o7 s v8 f e, i dvd e u w, r S ii tn c hg h ha w ea rh l li mic vh oe d tt hi m w ere it thH o ha Nr ep er e wpr a p w r oea rn ts t Bh s ee in ar c s C ht re , ep Ctm ae l, i o fIl ot lh i rn ne o ir ai.[ s 2 , a ] a f ttF e r w ro hhm eic rb h pi r t at imh re eu n n ts st h il e s he 2 c Dh. a a( dF r !r a )a cn tec rin ie n ,S Am mit eh r, i can • Experimental results over four open-world interactive
divorced.[3] Schaal studied acting with Viola Spolin in Chicago when she was
nine years old, later moving to Wisconsin and then California when she was 11. benchmarksdemonstratethesuperiorityofWESE,notably
(b) HotPotQA.Thegreensentenceishelpfulwhileothersare in achieving a remarkable balance between effectiveness,
task-irrelevant. efficiencyandcost.
Figure1: Examplesforsub-optimaldecisionsandirrelevantinfor-
2 RelatedWorks
mationinfeedbacks.
2.1 LLMagents
WiththeemergenceofLLMs, theirintelligencehassparked
thewhiteline,wheretheagentgoestothekitchentotakethe
considerable potential in applying LLMs as the brains of
aluminumforkfirstandthengototheworkshop. Whenlack
agents. Existing LLM agent works primarily consider three
ofglobalenvironmentalinformation,theagentprobablygets
key modules: planning, tool usage, and memory [Wang et
trappedinsomeroomduetofailureinfindinganaluminum
al., 2023b]. Planning module aims to empower agent with
object(theredline)orchoosesamoretime-consumingway
the task-decomposition ability, encompassing works on task
(the blue line). Secondly, the knowledge acquired by the
decomposition[Wangetal.,2023c],feedback-drivenadjust-
LLMfromenvironmentalexplorationtendstobeexcessive,
ments[Shinnetal.,2023],andmulti-pathreasoning[Yaoet
including irrelevant information to the task. The presence
al., 2023;Bestaetal., 2023]. Toolusageaimstostrengthen
ofsuchinformationnotonlydisruptsLLMdecision-making
the ability to use external tools [Qin et al., 2023a]. For in-
butalsoincursadditionalcosts. ReferredinFigure1(b), the
stance, Visual ChatGPT [Wu et al., 2023] incorporates vi-
feedback from the environment usually consists of massive
sual models as tools to augment the LLM’s visual capabili-
task-irrelevant information while only one helpful sentence,
ties. ToolLlama [Qin et al., 2023b] fine-tunes Llama’s abil-
i.e. the green line in this example, resulting in extra token
ity to leverage various APIs. The memory module focuses
usageofLLMandanegativeeffectonmakingoptimaldeci-
onstoringfeedbackinformationperceivedfromtheenviron-
sions.
ment, assisting the agent with experience, and fostering the
To address the above limitations, we propose a novel growthoftheagent. InGenerativeAgents[Parketal.,2023],
prompt-based strategy to enhance the LLM agent in this memoriesofsimulatedrolesarestoredastexts,utilizingRAG
work, termed Weak Exploration to Strong Exploitation for relevant pieces. REMEMBER [Zhang et al., 2023] pro-
(WESE).Totacklethefirstlimitation, weintroduceanidea poses a semi-parametric memory, i.e. the Q-value table, to
thatdecouplestheexplorationandexploitation. Specifically, recordrewardsasthevalueandactioninagivenenvironment
we construct two distinct LLM agents for exploration and andtaskasthekey. MemoryBank[Wangetal.,2023d]lever-
exploitation tasks, respectively. In the exploration task, the ages the Ebbinghaus forgetting curve, incorporating update
LLM agent’s goal is to interact with the environment, ex- andforgettingmechanismsintothememorydesign.
ploringpotentiallyhelpfulenvironmentalinformationfortask In our proposed WESE, the knowledge graph is essen-
resolution. Intheexploitationtask,theinformationobtained tially a memory, updating information obtained through ex-
duringexplorationservesasaglobalenvironmentalprior,aid- plorationintothegraph.
ingtheLLMagentinreasoninganddecision-makingtogen-
erate decisions. Regarding the second limitation, we com- 2.2 LLMforopen-worldtasks
press the environmental information acquired by the explo- Open-world tasks represent the simulation of real-world en-
rationagent,structuringitintheformofaknowledgegraph. vironments. Within these tasks, agents engage in contin-uous interactions with the environment to gather pertinent Algorithm1:Graphconstructionalgorithm.
information, subsequently making decisions and taking ac-
Input:KnowledgetripletssetK.
tiontoaccomplishgoals. Open-worldtaskstypicallyexhibit
Output:KnowledgegraphG.
fewerconstraintsontheprocess,placinggreateremphasison
1 EntitysetE ←{};
the final rewards. Representative examples of open-world 2 RelationsetR←{};
tasks include games like “Minecraft” [Wang et al., 2023a; 3 AdjacencymatrixM;
Wang et al., 2023e], where textual information and vi- 4 forx∈Kdo
sual feedback are involved. Another category comprises 5 h,r,t←x;
text-based simulators based on the TextWorld [Coˆte´ et al., 6 E ←E∪{h,t};R←R∪{r};M[h][t]←r;
2019], such as AlfWorld [Shridhar et al., 2020], which in- 7 G.E ←E;G.R←R;G.M ←M;
volves household tasks, ScienceWorld [Wang et al., 2022a],
which involves simple scientific experiments, and question-
answeringtasks[Yangetal.,2018;Thorneetal.,2018]where Algorithm2:Tripletretrievalalgorithm.
agentsneedtointeractwiththewebtoobtainsupportingin-
Input:KnowledgegraphG,TaskT,LLMΘ.
formation,suchasWikipedia. Intacklingsuchtasks,Chain-
Output:TripletssetK.
of-Thought(CoT) [Wei et al., 2022] proposes adding few- 1 Task-relatedentitysetE ←extract(G.E,T;Θ);
shot examples in the prompt, guiding the LLM to solve the 2 K ←{};
task step by step. ReAct [Yao et al., 2022] induces the rea- 3 fore i ∈Edo
soning capability of LLMs by introducing an extra thought 4 fore j ∈E\{e i}do
step. Subsequent methods have built upon ReAct, with en- 5 r←G.M[e i][e j];
hancementssuchastheReflexion[Shinnetal.,2023]mech- 6 ifr̸=emptythen
(cid:8)(cid:0) (cid:1)(cid:9)
anism,allowingagentstolearnfrommistakesinsubsequent 7 K ←K∪ e i,r,e j ;
attempts. Additionally, several methods leverage the coding
capabilities of LLMs, transforming tasks into programming
tasks and guiding LLMs to generate codes as plans, such as
VOYAGER[Wangetal.,2023a]. “cleaning some apples with soap” and the agent’s initial lo-
cationisthehall. Theactuallocationsoftheappleandsoap
3 Methodologies are in the drawer of the table in the hall and on the sink in
the kitchen, respectively. The lack of environmental knowl-
3.1 DecouplingExplorationandExploitation
edgemayleadtheagenttobemisledbytheworldknowledge
Open-world tasks differ from traditional reasoning and of the LLM, going to the kitchen to find the apple. Con-
decision-making tasks. Traditional reasoning [Huang and sequently, substantial efforts traversing every corner of the
Chang, 2022; Sun et al., 2023] or decision-making [Yang et kitchen are wasted, resulting in suboptimal plans and even
al., 2023] tasks typically present all relevant information at failuresduetotrappingintheloop. Therefore,weinvestigate
once, requiring the agent to deduce and make a plan based thestrategytodecoupleexplorationandexploitation,formal-
on the provided information, such as mathematical calcula- izedasfollows:
tions or logical reasoning problems. Conversely, in open- (cid:26)
explore(E,T,s ;Θ,P )∈A , i<N ;
worldtasks,onlythetaskdescriptionisinitiallyspecified. In a = i−1 e e e
i exploit(E,T,s ;Θ,P ,K =∪ {F })∈A ,i≥N .
thiscontext, theagentmustcontinuallyinteractwiththeen- i−1 t j≤N j t e
vironment to obtain supporting information, comprising the whereP andP representthepromptsoftheexplorationand
e t
explorationandexploitationsteps. exploitation task, respectively. N is the maximum number
e
LetEandT representtheenvironmentandthetask,Θde- of steps of exploration, which could also be determined by
note the LLM, and P denote the prompt. The action space the agent, such as terminating the exploration automatically
of the agent is defined as A = A e ∪A t, where A e and A t whenitthinkstheobtainedinformationissufficient.
represent the action set of exploration and exploitation, re- Different from the previous methods, our method places
spectively. Exploration and exploitation are denoted by the thewholeexplorationphasebeforeexploitationexplicitly,as
functions explore(·) and exploit(·). The information given opposed to the alternation of exploration and exploitation.
bytheenvironmentinthei-thstepisdenotedasF i. Regard- In this manner, the agent has extensively explored the en-
ingexistingmethodssuchasReAct[Yaoetal.,2022]where
vironment, acquiring global environmental prior knowledge
explorationandexploitationstepsareembeddedinreasoning, denotedasK =∪ {F }.Exploitationwithglobalknowl-
j≤N j
theactiontakenatstepiisrepresentedasfollows: edgebenefitstheeffectivenessandefficiencyofthesolutions,
whichisempiricallyvalidatedinourexperiments.
a =reason(E,T,s ;Θ,P,K =∪ {F })∈A ∪A .
i i−1 j<i j e t
However,twosubsequentissuesexistfollowingthedecou-
wherereasion(·)denotesthemixofexploreandexploit. plingapproach. Firstly, theinformationobtainedfromenvi-
Within this paradigm, the knowledge K utilized is solely ronmentalfeedbackishugeduetotheextensiveexploration,
the limited information about the environment obtained including a lot of task-irrelevant information. Secondly, the
through partial observations. Particularly, greedy decisions extensive exploration contributes to increased resource con-
aretakenintheinitialstepswhentheagentpossesseslimited sumption, such as token usage. Therefore, we demand an
awarenessoftheenvironment. Forinstance,inatasksuchas efficient mechanism for information transfer between ex-Triplet Extraction Triplet Retrieval
Your goal is to explore Your goal is to exploit
the environment to the information in the
Related Entity
obtain information to memory below to
solve the task. solve the task.
𝑚𝑢𝑔,𝑖𝑛,𝑑𝑟𝑎𝑤𝑒𝑟 2
Here are some 𝑐𝑢𝑝,𝑜𝑛,𝑡𝑎𝑏𝑙𝑒 1 Knowledge Here are some
examples: Memory examples:
{few_shot_examples} {few_shot_examples}
Task: {task_desc} Task: {task_desc}
Exploration Exploitation Memory: {memory}
Knowledge Reward
Exploration Exploitation
Prompt Weak Prompt
Environment
Agent
Strong
Agent
Weak Exploration Strong Exploitation
Figure2:FrameworkofWESE.Theleftpartrepresentstheweakexplorationandtherightpartrepresentsthestrongexploitation.Weemploy
Llama-2-7Bastheweakagentandtext-davinci-003asthestrongagentintheimplementation.
ploration and exploitation and a cost-efficient exploration- Algorithm3:WESEalgorithm.
exploitation strategy. We address the two issues in the sub-
Input:EnvironmentE,TaskT,Initialstates ,WeakLLM
sequentpartsofthissection. 0
Θ ,StrongLLMΘ ,ExplorationpromptP ,
w s e
ExploitationpromptP ,LimitofstepsN ,N .
3.2 KnowledgeCompressionandRetrieval t e t
Output:Planp.
Real-world textual information exhibits inherent sparsity, // Exploration with weak LLM agent.
characterizedbylongsentencesconsistingofplentyofnon- 1 K ←{};i←0;se i ←s 0;
informative conjunctions and adjectives. Environmental 2 fori<N edo
feedback in open-world tasks manifests as such text, where 3 ae i ←explore(E,T,se i;Θ w,P e);
thecumulativeextensiveexplorationyieldlongandunstruc- 4 se i,F i ←step(E,se i,ae i);
′
turedtextualinformation,demonstratingservesparsity. Con- 5 K ←extract(F i;Θ w);
sideringthelimitedcontextwindowoftheLLMandtheex- 6 K ←K∪K′ ;
pensive cost of token usage, it is necessary to compress the 7 i←i+1;
sparse information. Leveraging a knowledge graph (KG) to
8 G K ←construct graph(K); // Alg 1
store information has proved advantageous in enhancing in- // Exploitation with strong LLM agent.
f inor em xa ist ti io nn gd wen os rkit sy [a Pn ad nl ee tv aer l.a ,g 2in 0g 24d ]o .main-specificknowledge 109 Ki ˜← ←0; rs et i tr← ievs e0; tp ri← ple[ t] s;
(G K,T;Θ w); // Alg 2
Consequently, benefiting from the superiority of LLM in
11 fori<N tandF i ̸=Completeddo
relation-extraction tasks [Wadhwa et al., 2023], we extract 12 at i ←exploit(E,T,st i;Θ s,P t,K˜);
t vh ie ronk mno ew ntl ae ldg ke nofr wo lm edgth ee gr re ac pe hiv .ed Spfe ee cd ifib ca ac lk ly,to thf eorm LLa Mn e en x-
-
13 st i,F i ←step(E,st i,at i);
14 i←i+1;
tracts knowledge triplets from the environmental feedback 15 p←p+[at i];
after each exploration step, updating them into the knowl-
edge graph. For example, as for the search result given by
Wikipedia “Since 2005 Wendy Schaal has primarily worked
in voice acting, most notably voicing Francine Smith in the
animated comedy television series American Dad!”, knowl- fusion of entity and relation. For example, giving the triplet
edge triplets are extracted as ⟨Wendy Schaal, voice for, ⟨Bob, favorite fruit, apple⟩ and the question is “What’s the
Francine Smith⟩ and ⟨Francine Smith, character in, Ameri- favorite fruit of Bill?”, the LLM would confuse the relation
canDad!⟩. Notably,theenvironmentalknowledgegraphwe andanswerwithapple. Benefitingfromthegraphstructure,
obtainedistask-relevant, servingasamemoryliketheRan- we adopt a one-hop retrieval method to extract task-related
dom Access Memory(RAM). Actually, a worldwide knowl- informationeasily,illustratedinAlgorithm2. Concretely,we
edgegraphcouldbeleveragedandcontinuallyinourmethod, initiate the process by extracting involved entities from the
servingasageneralmemory. Weleaveitforfurtherwork. taskdescriptionwithLLM.Subsequently,weperformaone-
Nevertheless, it is imperative to acknowledge that not all hopretrievalonthegraphtoobtaintheneighborsoftheseen-
information in the knowledge graph proves useful. The in- tities. Theretrievedknowledgetripletsaretheninjectedinto
troduction of task-irrelevant information has the potential to theprompt,servingastask-relevantknowledgeduringtheex-
lead the hallucination phenomena of LLM, such as the con- ploitationphase,therebyassistingtheLLMintask-solving.3.3 WeakExplorationtoStrongExploitation the ideal solution involves actions such as (go to countertop
Acquiringmorecomprehensiveglobalinformationaboutthe 2, take knife 1, go to sinkbasin 2, clean knife 1, put knife 1
environmentdemandsaconsiderableresourcecostintheex- on countertop 2). These tasks vary in difficulty, with chal-
plorationprocess. However,comparedtoexploitation,explo- lenging tasks encompassing over 50 locations and requiring
rationexhibitslowercomplexity,requiringlessreasoningand morethan50-stepactions,posingchallengesforboththeex-
induction. Concretely,explorationoperationsexhibitlowre- plorationandexploitationprocesses.
quirementsforthelogicandcoherenceofactions,emphasiz-
ingactionspertainingtoenvironmentalobservation. Forex- 120
ample,theexplorationactionsmainlyconsistofseveralsim- 100 Act
Act-WESE
ple actions on decision-making benchmarks, such as “go to 80 Act-SESE
[ROOM]”,“lookaround”,etal,whileexploitationinvolvesa
60
series of coherent operations like (go to sink/stove, put the
40
bowlin/onthesink/stove,activatethesink/stove,wait,deac-
20
tivatethesink/stove). Therefore,weproposetouseaweaker
0
agent for the exploration to mitigate resource consumption,
-19
namely the weak exploration. From the perspective of the
-40
LLMagent,aweakeragentrepresentssubstitutingtheunder- clean heat examine cool puttwo put
lyingLLMforexplorationwithaweakerLLM,i.e. anLLM (a) RelativeimprovementsforAct-basedmethods.
withfewerparameters,therebyreducingcosts. Inourexper-
iments,wecompareperformancebetweenstrongexploration 120
ReAct
andweakexploration. Ourfindingsrevealthataweakerex- 100
ReAct-WESE
plorationhasanegligibleimpactonthefinalsuccessrate,yet 80 ReAct-SESE
itsignificantlylowerscosts. 60
TheframeworkofWESEisillustratedinFigure2. There 40
are three key components in the framework: a weak LLM 20
agent, a strong LLM agent, and a KG-based memory. The 0
wholeprocessconsistsoftheweakexploration(left)andthe -19
strongexploitation(right). Meanwhile,weofferanalgorith- -40
clean heat examine cool puttwo put
mic pseudo-code in Algorithm 3. First, a weak LLM agent
(b) RelativeimprovementsforReAct-basedmethods.
isemployedtoexploretheinteractiveenvironmenttoobtain
informationinline1to7. Thenthoseknowledgetripletsare Figure3: Relativeimprovementsinsuccessrateovervarioustypes
organizedasaknowledgegraphG inline8,asillustratedin oftasksonALFWorld.Thelefttasksaremorecomplicated.
K
Algorithm1. Further,theinvolvedentitiesareextractedfrom
the task with a LLM and the relevant triplets are retrieved TovalidatetheeffectivenessofWESE,weadoptAct[Yao
fromthegraphinline10. Retrievedknowledgeisleveraged et al., 2022] and ReAct [Yao et al., 2022] as baselines. Act
forexploitationinline12,servingasthepriorknowledge. leveragestheideaofCoT,providingLLMswithfew-shotin-
teractiveexamples. ReAct,buildinguponAct,introducesan
4 Experiments extra“THOUGHT”stepwhereLLMscanchoosetoexplicitly
outputtheirthoughtaboutthecurrentstateorgenerateaction.
We employ two categories of interactive open-world tasks
InWESE,weinitiallyuseaWeakLLMforexplorationtoac-
as benchmarks: decision-making and question-answering,
quiretask-relevantknowledge.Wethenleveragetheobtained
whereeachtaskrequiresmulti-stepinteractionswiththeenvi-
knowledgetosolveproblemswithtwobasemethods.Weem-
ronment. We evaluate our methods from three perspectives:
ployLlama-2-7B[Touvronetal.,2023]astheweakLLMand
effectiveness, efficiency and cost, representing whether the
text-davinci-003(withprobablymorethan175billionparam-
agentcancompletethetasks,howmanystepstheagentwould
eters)developedbyOpenAI1 asthestrongLLM.Thelimits
taketofinishthetask,andtheexpensesfortheagenttocom-
ofstepsN ,N arebothsetto50. Ourevaluationfocuseson
pletethetask,respectively. e t
success rates, average steps to complete tasks, and the cost
4.1 DecisionMakingTasks ofOpenAIAPItokensasthreekeymetrics. Additionally,we
introduceavariantofWESE—StrongExplorationtoStrong
Webeginwiththeopen-worlddecision-makingtasks,where
Exploitation(SESE),wheretheweakLLMintheexploration
environmentsarebasedonatext-basedsimulator. Thetasks
processisreplacedwiththestrongLLM,toverifytheeffec-
are about the household, where the agent needs to explore
tivenessofthedecouplingstrategyandexaminetheimpactof
variousroomsandtakeoperationsonseveralobjects.
LLMstrengthonexplorationquality.
ALFWorld
ALFWorld [Shridhar et al., 2020] is a synthetic text-based ScienceWorld
simulated interactive environment. It comprises six types of SimilartoALFWorld, ScienceWorld[Wangetal., 2022a]is
tasks where agents need to interact with the environment to an interactive household environment as well. However, the
generateaseriesofactionstosolvehouseholdtasks. Forex-
ample,inthetask“cleansomeknifeandputitincountertop”, 1https://platform.openai.com/
)%(
tnemevorpmI
evitaleR
)%(
tnemevorpmI
evitaleRTable1: ResultsonALFWorld(134tasks). SRandASareabbreviationsforsuccessrateandaveragestepsofsuccessfultasks,respectively.
SESErepresentsthevariantofWESE—StrongExplorationtoStrongExploitation.TheImprepresentstherelativeimprovementscompared
tobasemethods,i.e.ActandReAct.Theboldandunderlinerepresentthebestandthesecondbestforthesamebasemethod.
Performance Effectiveness Efficiency Cost
Method SR↑ Imp(%) AS↓ Imp(%) Prompt↓ Completion↓ Expense($)↓ Imp(%)
Act 0.43 0.00 10.83 0.00 4,908,548 21,243 98.60 0.00
Act-WESE 0.63 +46.51 7.54 +30.38 3,746,290 19,562 75.32 +23.61
Act-SESE 0.67 +55.81 6.73 +37.86 7,259,508 75,153 146.69 -48.77
ReAct 0.57 0.00 16.64 0.00 7,565,676 43,250 152.18 0.00
ReAct-WESE 0.72 +26.32 13.69 +17.73 5,032,374 41,004 101.47 +33.32
ReAct-SESE 0.75 +31.58 12.41 +25.42 8,996,182 97,286 181.87 -19.51
Table2: ResultsonScienceWorld(296tasks). TR,ARandASareabbreviationsfortotalreward,averagerewardandaveragestepstoget
positivereward,respectively.OthersymbolsareconsistentwithTable1.
Performance Effectiveness Efficiency Cost
Method TR↑ AR↑ Imp(%) AS↓ Imp(%) Prompt↓ Completion↓ Expense($)↓ Imp(%)
Act 4908 16.58 0.00 18.00 0.00 13,554,960 55,817 272.22 0.00
Act-WESE 5198 17.56 5.91 15.68 +12.91 13,491,043 65,952 271.14 +0.40
Act-SESE 5249 17.73 6.94 15.39 +14.49 36,424,190 165,568 731.80 -168.83
ReAct 4454 15.05 0.00 20.00 0.00 17,716,698 84,724 356.03 0.00
ReAct-WESE 5317 17.96 19.34 19.65 +1.77 16,310,632 80,851 327.83 +7.92
ReAct-WESE 5053 17.07 13.42 19.02 +4.92 40,293,571 196,338 809.80 -127.45
tasksinScienceWorldaremorechallenging,involvingscien- tive1.43%and6.89%degradationsineffectivenessandeffi-
tificexperimentssuchasboilingandcreatinganewcolorby ciencycomparedwithSESE.InWESE,theweakLLMagent
mixing primary colors. The environment is more complex, undertakes the exploration process, resulting in cost savings
comprising ten distinct rooms, each with different furnish- for extensive exploration. Besides, benefiting from the re-
ings,andnoteachpairofroomsisconnected. latedtripletsextractedfromtheexploredKG,thestrongLLM
WeconductexperimentsoneighttypesoftaskswithinSci- agentonlyneedstofocusonexploitation,furtherdecreasing
enceWorld,choosingabout30instancesforeachtaskdueto thenumberofsteps,evidencedbythedecreasedcompletion
alimitedbudget.UnlikeALFWorldwheretheagentcangeta tokensandaveragesteps.
rewardof1onlywhenthetaskiscompleted,theagentinSci- WefurtherinvestigatetheimprovementsofWESEonvari-
enceWorld receives partial rewards upon completing crucial oustypesoftasks,showninFigure3. BothWESEandSESE
steps, with the total reward reaching 100. Given the chal- showimprovementsoveralmostalltypesoftasks,furtherin-
lengingnatureofthetasks,achievingafullrewardof100is dicatingtheeffectivenessofthedecouplingstrategy. Inaddi-
rare. Therefore, we utilize the number of steps taken by the tion,theimprovementsin“clean”and“heat”tasksaregreater
agentuntilitfirstobtainsapositiverewardasthemetricfor thanothertasks.Thereasonliesinthatthetwotasksinvolved
efficiency. OthersettingsareconsistentwithALFWorld. more complicated exploitation compared with “put”, where
theagentsneedtofindtheobjectfirstandthencleanorheatit
Results insteadofjustmovingittoanotherplace. Theresultdemon-
TheresultsonALFWorldandScienceWorldareshowninTa- stratesextensiveexplorationbenefitsmoreforcomplextasks.
ble1andTable2,respectively. Weconcludeseveralfindings
basedontheresults. ConsistentwithresultsreportedinRe- 4.2 QuestionAnsweringTasks
Act,ReActoutperformsActontwobenchmarks,showingthe We also validate our WESE on two open-world interac-
superiorityofthe“THOUGHT”step. However,thisadditional tive question-answering benchmarks, i.e., HotPotQA and
step leads to a longer action sequence, resulting in an aver- FEVER.Differentfromtraditionalquestion-answeringtasks
age relative 32.38% increase in average steps. Decoupling wheresupportingsentencesaregiven,thosetasksprovidethe
of exploration and exploitation demonstrates advantages in questiononlyandrequiretheagenttosearchinformationon
effectiveness and efficiency, resulting in SESE outperform- thewebstepbysteptogivethefinalanswer.
ing baselines significantly with average relative 26.94% and
20.67% improvements in terms of success rate (average re- HotPotQA
ward)andaveragesteps.However,thecostofSESEincreases HotPotQA[Yangetal.,2018]isaquestion-answeringdataset
alot dueto theintroduction ofextensive strongexploration, whereeachquestionispairedwithsupportingsentencesfrom
showinganaveragerelative91.14%increaseoverbaselines. Wikipedia articles. In traditional QA tasks, the supporting
WESEshowsabetterbalancebetweeneffectiveness,effi- sentences are given and the remained task is to reason. Re-
ciency,andcost,whichsaves53.83%ofcostswithonlyrela- ferred in ReAct, we use the Wikipedia API with three typesTable3: ResultsonHotPotQA(500tasks). SRandASareabbreviationsforsuccessrateandaveragestepsofsuccessfultasks,respectively.
SESErepresentsthevariantofWESE—StrongExplorationtoStrongExploitation.TheImprepresentstherelativeimprovementscompared
tobasemethods,i.e.ActandReAct.Theboldandunderlinerepresentthebestandthesecondbestforthesamebasemethod.
Performance Effectiveness Efficiency Cost
Method SR↑ Imp(%) AS↓ Imp(%) Prompt↓ Completion↓ Expense($)↓ Imp(%)
CoT 0.318 N/A 1.00 N/A 261,347 25,382 5.73 N/A
Act 0.296 0.00 3.53 0.00 2,390,041 14,236 48.09 0.00
Act-WESE 0.353 +19.26 2.69 +23.80 2,307,421 13,973 46.42 +3.45
Act-SESE 0.361 +21.96 2.58 +26.91 7,522,826 27,1551 155.89 -224.18
ReAct 0.342 0.00 3.17 0.00 3,234,876 65,306 66.00 0.00
ReAct-WESE 0.394 +15.20 2.29 +27.76 2,574,401 67,908 52.85 +19.93
ReAct-SESE 0.416 +21.64 2.11 +33.44 7,338,590 323,401 153.24 -132.17
Table4:ResultsonFEVER(500tasks).ThemeaningsofabbreviationsandsymbolsareconsistentwithTable3.
Performance Effectiveness Efficiency Cost
Method SR↑ Imp(%) AS↓ Imp(%) Prompt↓ Completion↓ Expense($)↓ Imp(%)
CoT 0.61 N/A 1.00 N/A 100,387 11,942 2.25 N/A
Act 0.56 0.00 2.16 0.00 723,646 6,980 14.61 0.00
Act-WESE 0.62 +10.71 1.58 +26.66 723,867 5,937 14.60 +0.11
Act-SESE 0.64 +14.29 1.57 +27.34 2,822,189 122,543 60.89 -316.73
ReAct 0.63 0.00 2.18 0.00 1,074,080 36,040 22.20 0.00
ReAct-WESE 0.68 +7.26 1.62 +25.96 918,905 29,895 18.98 +14.53
ReAct-SESE 0.70 +10.09 1.59 +27.18 3,104,924 162,363 65.35 -194.32
of actions to support interactive information retrieval: (1) Results
SEARCH[ENTITY], which searches the Wikipedia with the The results on HotPotQA and FEVER are shown in Table 3
ENTITYandreturnsthecorrespondingpageifitexists,orsug- and Table 4, respectively. We can conclude several find-
geststop-5similarentities;(2)LOOKUP[KEYWORD],which ings based on the results. Similar to decision-making tasks,
looks up keyword in the page and returns the next sentence ReAct outperforms Act significantly due to the additional
containingtheKEYWORD,simulatingtheCtrl+Ffunctionina “THOUGHT” step. Also, methods equipped with WESE or
webbrowser;(3)FINISH[ANSWER],whichanswerstheques- SESEoutperformbaselinesinbothsuccessrateandthenum-
tion with ANSWER. Once the ANSWER matches the ground ber of taken actions, resulting in average relative improve-
truth,theenvironmentwouldreturnreward1.Wesample500 ments of 19.5% and 28.0%, respectively. Especially, SESE
tasksfromthedevelopmentset. methods surpass WESE slightly with average relative 3.5%
We employ the CoT [Wei et al., 2022], Act and ReAct and3.6%improvementsintermsofsuccessrateandaverage
as baselines and empower Act and ReAct with WESE and steps, while increasing more than twice the expenses. This
SESE.NotethatCoTisaone-stepmethodthatdoesnotsup- furtherdemonstratesthattheweakagentpoweredbyLlama-
portinteractivetasks,weinjectthesupportingsentencesinto 2-7Bisalmostsufficientfortheexplorationtask.
thepromptsandinstructtheLLMtoreasonforthefinalan- Differentfromdecision-makingtasks,question-answering
swer without searching on the web. Also, WESE is not de- tasks require fewer steps due to more information being re-
signed for such a purely reasoning method but for methods turned with one search action. However, our WESE and
involving interactions with the environment. For Act and SESE are still capable of reducing the number of steps, fur-
ReAct, we keep the settings consistent with the original pa- ther showing the advantage of the explored knowledge. As
per. Asthereareprobablylotsofrelatedtripletstothetask- for the cost, thetokens increased in SESE are farmore than
involved entities, we set the limit of retrieved triplets as 10 thoseindecision-makingtasks,whichcanbeattributedtothe
andthelimitsofstepsN ,N as8. Theevaluationforeffec- long-textualfeedbackfromWikipedia.
e t
tiveness,efficiencyandcostisconsistentwiththeALFWorld.
5 Conclusion
FEVER
FEVER [Thorne et al., 2018] is a fact verification dataset, In this paper, we introduce WESE, a cost-effective method
consistingofinstanceswhereeachinstancecomprisesaclaim that enhances LLM agents in open-world interactive tasks.
andajustification(TRUEorFALSEorNOTCLEAR). Weem- Wedecoupletheexplorationandexploitation,employingtwo
ploy the Wikipedia API to construct an interactive environ- agentsforthedistinctprocesses. Toempowerthecommuni-
ment consistent with that in HotPotQA. Other settings are cationbetweenthetwoprocesses,weintroduceaknowledge
kept consistent with HotPotQA, such as the number of re- graph-basedmemorytocompressandstructuretheinforma-
trievedtripletsandthemaximumsteps. tionobtainedinexploration,wheretask-relevantinformationisextractedfromthegraphbyaone-hopretrievalmethod.We References
then propose to leverage a weaker agent for the exploration [Bestaetal.,2023] Maciej Besta, Nils Blach, Ales Ku-
process,formingacost-effectivemannerwithnegligibleper-
bicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna
formancedegradation. Experimentalresultsdemonstratethe
Gajda, Tomasz Lehmann, Michal Podstawski, Hubert
superiorityofWESEineffectiveness,efficiency,andcost.
Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts:
Solving elaborate problems with large language models.
arXivpreprintarXiv:2308.09687,2023.
[Coˆte´ etal.,2019] Marc-Alexandre Coˆte´, Akos Ka´da´r,
Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine,
James Moore, Matthew Hausknecht, Layla El Asri,
Mahmoud Adada, et al. Textworld: A learning environ-
ment for text-based games. In Computer Games: 7th
Workshop,CGW2018,HeldinConjunctionwiththe27th
InternationalConferenceonArtificialIntelligence, IJCAI
2018,Stockholm,Sweden,July13,2018,RevisedSelected
Papers7,pages41–75.Springer,2019.
[HuangandChang,2022] JieHuangandKevinChen-Chuan
Chang. Towards reasoning in large language models: A
survey. arXivpreprintarXiv:2212.10403,2022.
[Kojimaetal.,2022] Takeshi Kojima, Shixiang Shane Gu,
MachelReid,YutakaMatsuo,andYusukeIwasawa. Large
language models are zero-shot reasoners. Advances in
neural information processing systems, 35:22199–22213,
2022.
[Panetal.,2024] ShiruiPan,LinhaoLuo,YufeiWang,Chen
Chen,JiapuWang,andXindongWu. Unifyinglargelan-
guage models and knowledge graphs: A roadmap. IEEE
TransactionsonKnowledgeandDataEngineering,2024.
[Parketal.,2023] Joon Sung Park, Joseph O’Brien, Car-
rie Jun Cai, Meredith Ringel Morris, Percy Liang, and
MichaelSBernstein. Generativeagents: Interactivesim-
ulacra of human behavior. In Proceedings of the 36th
Annual ACM Symposium on User Interface Software and
Technology,pages1–22,2023.
[Qinetal.,2023a] Yujia Qin, Shengding Hu, Yankai Lin,
Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei
Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng
Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun
Zhu,ShihaoLiang,XingyuShen,BokaiXu,ZhenZhang,
YiningYe,BowenLi,ZiweiTang,JingYi,YuzhangZhu,
ZhenningDai,LanYan,XinCong,YaxiLu,WeilinZhao,
Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai
Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji,
ZhiyuanLiu,andMaosongSun. Toollearningwithfoun-
dationmodels,2023.
[Qinetal.,2023b] Yujia Qin, Shihao Liang, Yining Ye,
Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,
XiangruTang,BillQian,etal. Toolllm: Facilitatinglarge
languagemodelstomaster16000+real-worldapis. arXiv
preprintarXiv:2307.16789,2023.
[Shinnetal.,2023] Noah Shinn, Beck Labash, and Ash-
win Gopinath. Reflexion: an autonomous agent with
dynamic memory and self-reflection. arXiv preprint
arXiv:2303.11366,2023.
[Shridharetal.,2020] Mohit Shridhar, Xingdi Yuan, Marc-
Alexandre Coˆte´, Yonatan Bisk, Adam Trischler, andMatthew Hausknecht. Alfworld: Aligning text and [Wangetal.,2023c] Lei Wang, Wanyu Xu, Yihuai Lan,
embodied environments for interactive learning. arXiv Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-
preprintarXiv:2010.03768,2020. Peng Lim. Plan-and-solve prompting: Improving zero-
[Sunetal.,2023] JiankaiSun,ChuanyangZheng,EnzeXie, shotchain-of-thoughtreasoningbylargelanguagemodels.
arXivpreprintarXiv:2305.04091,2023.
Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu,
MingyuDing,HongyangLi,MengzheGeng,etal. Asur- [Wangetal.,2023d] Weizhi Wang, Li Dong, Hao Cheng,
vey of reasoning with foundation models. arXiv preprint Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei.
arXiv:2312.11562,2023. Augmenting language models with long-term memory.
[Thorneetal.,2018] James Thorne, Andreas Vlachos, arXivpreprintarXiv:2306.07174,2023.
Christos Christodoulopoulos, and Arpit Mittal. Fever: [Wangetal.,2023e] Zihao Wang, Shaofei Cai, Guanzhou
a large-scale dataset for fact extraction and verification. Chen,AnjiLiu,XiaojianMa,andYitaoLiang. Describe,
arXivpreprintarXiv:1803.05355,2018. explain, plan and select: interactive planning with llms
[Touvronetal.,2023] Hugo Touvron, Louis Martin, Kevin enables open-world multi-task agents. In Thirty-seventh
Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Conference on Neural Information Processing Systems,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, 2023.
Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Can- [Weietal.,2022] Jason Wei, Xuezhi Wang, Dale Schuur-
ton Ferrer, Moya Chen, Guillem Cucurull, David Es- mans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
iobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Denny Zhou, et al. Chain-of-thought prompting elicits
Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, reasoning in large language models. Advances in Neural
Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan InformationProcessingSystems,35:24824–24837,2022.
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, [Wuetal.,2023] ChenfeiWu,ShengmingYin,WeizhenQi,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
chatgpt: Talking,drawingandeditingwithvisualfounda-
Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,
tionmodels. arXivpreprintarXiv:2303.04671,2023.
Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin
[Xietal.,2023] ZhihengXi,WenxiangChen,XinGuo,Wei
Nie,AndrewPoulton,JeremyReizenstein,RashiRungta,
He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe
Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael
Wang, Senjie Jin, Enyu Zhou, et al. The rise and poten-
Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
tialoflargelanguagemodelbasedagents:Asurvey.arXiv
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan,
preprintarXiv:2309.07864,2023.
Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-
gela Fan, Melanie Kambadur, Sharan Narang, Aurelien [Yangetal.,2018] Zhilin Yang, Peng Qi, Saizheng Zhang,
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas YoshuaBengio,WilliamWCohen,RuslanSalakhutdinov,
Scialom. Llama 2: Open foundation and fine-tuned chat andChristopherDManning. Hotpotqa: Adatasetfordi-
models,2023. verse, explainable multi-hop question answering. arXiv
[Wadhwaetal.,2023] SominWadhwa,SilvioAmir,andBy-
preprintarXiv:1809.09600,2018.
ronCWallace. Revisitingrelationextractionintheeraof [Yangetal.,2023] SherryYang,OfirNachum,YilunDu,Ja-
largelanguagemodels. arXivpreprintarXiv:2305.05003, sonWei,PieterAbbeel,andDaleSchuurmans.Foundation
2023. modelsfordecisionmaking: Problems,methods,andop-
[Wangetal.,2022a] Ruoyao Wang, Peter Jansen, Marc- portunities. arXivpreprintarXiv:2303.04129,2023.
Alexandre Coˆte´, and Prithviraj Ammanabrolu. Science- [Yaoetal.,2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan
world: Is your agent smarter than a 5th grader? arXiv Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
preprintarXiv:2203.07540,2022. React:Synergizingreasoningandactinginlanguagemod-
[Wangetal.,2022b] Xuezhi Wang, Jason Wei, Dale Schu- els. arXivpreprintarXiv:2210.03629,2022.
urmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha [Yaoetal.,2023] ShunyuYao,DianYu,JeffreyZhao,Izhak
Chowdhery,andDennyZhou. Self-consistencyimproves Shafran, Thomas L Griffiths, Yuan Cao, and Karthik
chain of thought reasoning in language models. arXiv Narasimhan. Tree of thoughts: Deliberate problem
preprintarXiv:2203.11171,2022. solving with large language models. arXiv preprint
[Wangetal.,2023a] Guanzhi Wang, Yuqi Xie, Yunfan arXiv:2305.10601,2023.
Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi [Zhangetal.,2023] DanyangZhang,LuChen,SituoZhang,
Fan, and Anima Anandkumar. Voyager: An open- Hongshen Xu, Zihan Zhao, and Kai Yu. Large language
endedembodiedagentwithlargelanguagemodels. arXiv model is semi-parametric reinforcement learning agent.
preprintarXiv:2305.16291,2023. arXivpreprintarXiv:2306.07929,2023.
[Wangetal.,2023b] Lei Wang, Chen Ma, Xueyang Feng, [Zhaoetal.,2023] Wayne Xin Zhao, Kun Zhou, Junyi Li,
Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian
JiakaiTang,XuChen,YankaiLin,etal. Asurveyonlarge Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
languagemodelbasedautonomousagents. arXivpreprint A survey of large language models. arXiv preprint
arXiv:2308.11432,2023. arXiv:2303.18223,2023.