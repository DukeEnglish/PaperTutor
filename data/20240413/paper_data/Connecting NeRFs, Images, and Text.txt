Connecting NeRFs, Images, and Text
FrancescoBallerini PierluigiZamaRamirez RobertoMirabella
SamueleSalti LuigiDiStefano
UniversityofBologna
https://cvlab-unibo.github.io/clip2nerf
Abstract NeRF RETRIEVAL FROM IMAGES NeRF RETRIEVAL FROM TEXT
Neural Radiance Fields (NeRFs) have emerged as a
standard framework for representing 3D scenes and ob- "a 3d model of
a fighter jet"
jects, introducing a novel data type for information ex-
NeRF NeRF NeRF NeRF NeRF NeRF
changeandstorage. Concurrently,significantprogresshas
been made in multimodal representation learning for text
and image data. This paper explores a novel research di- "a 3d model of
a car"
rectionthataimstoconnecttheNeRFmodalitywithother
NeRF NeRF
modalities,similartoestablishedmethodologiesforimages
ZERO-SHOT NeRF CLASSIFICATION NeRF GENERATION FROM IMAGES
and text. To this end, we propose a simple framework
that exploits pre-trained models for NeRF representations Figure 1. Framework applications. Examples of the possi-
alongside multimodal models for text and image process- bletaskswecanperformthankstoourframeworkthatconnects
ing. Our framework learns a bidirectional mapping be- NeRFs,images,andtext.
tween NeRF embeddings and those obtained from corre-
sponding images and text. This mapping unlocks several
novel and useful applications, including NeRF zero-shot els (VLMs) [11, 34]. These models capitalize on large
classificationandNeRFretrievalfromimagesortext. paired databases of images and text to extract rich multi-
modalrepresentations.Bycombiningmodalities,theserep-
resentations obtain a better overall comprehension of im-
1.Introduction agesandtext,allowingforbetterperformanceinexistingvi-
sualandtextualtasks.Moreover,theyunlockedmanynovel
In the Neural Radiance Fields (NeRF) framework [27], a applicationssuchaszero-shotclassification, whereunseen
neural network is trained to construct a volumetric repre- instancesareclassifiedbasedontextualdescriptions,image
sentationofa3Denvironmentfromimages. OnceaNeRF retrievalbyqueryingwithbothtextandimageprompts,vi-
is trained, it enables the generation of novel views of that sual question answering, and many others. In the scenario
environment through ray tracing. They have gained con- inwhichNeRFsareanadditionalinputmodality,anintrigu-
siderable popularity over recent years [29], emerging as a ing research direction is to understand whether and how it
novelapproachfor3Ddatarepresentation. Representinga ispossibletoconnectNeRFstootherinputmodalities,asit
scenewithasingleNeRFdecouplestheactualmemoryoc- was for images and text. By bridging NeRFs with diverse
cupationfromthespatialresolutionandthenumberofob- inputmodalities,wemightunlocknewopportunitiesforin-
servations. Indeed,wecanencodeahypotheticallyinfinite novativeapplications.
numberofimagesatarbitraryresolutionintoafinitenum- Unlikeimagesandtext,whicharewell-studiedinputfor-
ber of network weights. This may potentially lead NeRFs mats, NeRFs present unique challenges as they are neural
tobecomeastandardmeansofstoringandexchanging3D networks, making them less straightforward to process by
information,withentiredatabasesofNeRFsresidingonour conventional frameworks. One naive approach would in-
harddrivesinthefuture. Supportingthisideaistherecent volverenderingimagesoftheobjectrepresentedbyaNeRF.
proliferationofvariousNeRFdatasets[5,15,36]. However,thischoicepresentsvariouschallenges,including
ConcurrentlywiththedevelopmentofNeRFs,therehas lengthy computation times, determining a viewpoint from
beennotableprogressinthefieldofVision-LanguageMod- which to render the object, or deciding on an appropriate
1
4202
rpA
11
]VC.sc[
1v39970.4042:viXrarenderingresolution.Conversely,wewouldencounternone 2.Relatedwork
oftheseissuesbyprocessingtheweightsoftheNeRF.The
Vision-Language Models. During the last few years,
problemofhowtoprocessNeRFweightstoenabledown-
therehasbeenarapidadvancementinvisual-languagemod-
stream tasks has been the focus of the recently proposed
eling. The popularity of Vision Transformer (ViT) [8] has
framework nf2vec [52]. This work learns to encode the
led to numerous studies that utilize ViT to simultaneously
information contained within the network weights into a
learn from vision-language data and achieve outstanding
compactembeddingthatretainssufficientknowledgetobe
performance in downstream tasks [4, 24, 35, 41, 54]. Re-
usedasinputfordownstreamtasks.
searchers have proposed efficient pretraining tasks to en-
Thankstotheavailabilityofgeneralpurposepre-trained
hance the alignment between visual and language modal-
VLMs such as CLIP [34] and pre-trained NeRFs encoders
ities. Contrastive learning is one of the most prominent
suchasnf2vec[52],thispapercaststheproblemofcon-
methods widely adopted in many studies [3, 12, 19, 34].
nectingNeRFs,images,andtextaslearningmappingfunc-
Among these methods, CLIP [34] is one of the most pop-
tionsbetweenthelatentspaceofthenf2vecencoderand
ular. Additionally, there are emerging works that ex-
the latent space of CLIP. In practice, we propose train-
plore unified frameworks to address vision-language tasks
ing two simple Multi-Layer Perceptrons (MLPs) to map a
[25, 39, 44, 45, 47, 48]. Recent works extend multimodal
NeRFembeddingintoaCLIPembeddingorviceversa. In
representation learning to other modalities such as audio
thisway,givenanimageortextinput,wecandiscoverthe
andvideos[11,46,49]. OurworkemploysCLIPtoextract
correspondingNeRForviceversa. Notably,acquiringdata
richmultimodalembeddingsfromimagesandtext.
fortrainingsuchnetworksisstraightforward,aswecandi-
rectlyleveragetherenderingsorground-truthimagesofthe
NeRFs. Moreover, by exploiting a pre-trained model with Neural Radiance Fields. NeRF [27] has emerged as a
multimodaltext-imageembeddingssuchasCLIP,wenatu- valuabletoolforavarietyoftasks, includingviewsynthe-
rally learn the connection between NeRF and texts, avoid- sis [26], generative media [33], robotics [51], and compu-
ingthenecessityofNeRF-textpairings. tational photography [28]. Initially, the base NeRF model
employed an MLP to translate spatial coordinates into
Ourframeworkunlocksmanyinnovativeandcompelling
color and density. Recent advancements substitute or en-
applications,suchasthosedepictedinFig.1. Forinstance,
hanceMLPswithvoxelgrid-likedatastructures[2,10,42].
itispossibletoclassifyNeRFsinazero-shotmannerbased
For instance, Instant NGP [30] utilizes a hierarchical ar-
solely on their weights, or given one or more images de-
rangement of coarse and fine-grained grids stored using
pictinganobject,wecanretrievethemostcloselymatching
hashmaps. Thesestructuresfacilitatetheextractionoffea-
NeRFs. Alternatively,textualqueriescanbeusedtosearch
tures, which are then processed by a compact MLP, re-
for NeRFs stored in our databases. We can even generate
sulting in significantly accelerated training processes. Our
entirelynewNeRFsfromeitherimagesortext.
workemploysNeRFsthatfollowthebaseformulation,i.e.
Despite the simplicity of the architecture, we observe
a single MLP extracting density and color information for
thatourframeworkeffectivelyperformstaskssuchasNeRF
each3Dcoordinate.
zero-shot classification on par with baselines operating on
images obtained from NeRFs without requiring to render
even a single pixel. Moreover, leveraging recent text-to- Deep Learning on Neural Networks. Multiple recent
image conditional generative approaches [53], we propose studies have delved into using neural networks to process
anadaptationtechniquetoapplyourmethodeffectivelyto otherneuralnetworks. Earlyworksinthefieldfocusedon
realimagesevenwhentrainedsolelyonsyntheticdata. forecastingnetworkpropertiessuchasaccuracyandhyper-
parametersdirectlyfromtheirweights[16,17,23,37,43].
Briefly,ourcontributionsare:
Recentstudieshandlenetworksimplicitlyrepresentingdata
• Weinvestigateforthefirsttimetheproblemofconnecting (INRsorNeuralFields). Theseworksperformvisiontasks
NeRFswithimagesandtext. directlyusingnetworkweightsastheinputoroutputdata.
• Weproposethefirstframeworktoachievethisgoal. No- Functa [9] learns priors across an entire dataset using a
tably, this method is easy to train as it requires learning sharednetworkandsubsequentlyencodeseachsampleinto
onlytwosimpleMLPs. a compact embedding employed for downstream discrimi-
• Our idea unlocks many intriguing applications, such as native and generative tasks. The following approaches fo-
zero-shot classification of NeRFs by solely processing cus on processing networks representing individual data,
theirnetworkweightsandretrievingNeRFsfromimages e.g.aspecificobjectorscene. Thefirstframeworkdoingit
ortexts. wasinr2vec[6]. Thisapproachencodesnetworksrepre-
• We propose a technique to adapt our model to perform senting3Dshapesintocompactembeddings,servingasin-
wellonrealimageswhentrainedsolelyonsyntheticdata. putforsubsequenttasks. nf2vec[36]extendsinr2vec
2to NeRFs, performing several tasks directly from NeRFs LOSS
w lee ai rg nh sts h, os wuc th oa ps rc ol ca es ss sifi nc ea uti ro an l, fig ee ln de sra rt eio pn re, so er ntr ee dtri aev sa al. h[ y1] - c l i p 2 n e r f
brid tri-plane representation. Another research direction CLIP
IMAGE
[31, 55–57], recognizing that MLPs exhibit weight space
ENCODER
symmetries[13],proposesinnovativearchitecturestailored
forMLPsbyleveragingnetworksymmetriesasaninductive
LOSS
bias. Otherworks[18,21]exploitGraphNeuralNetworks
to learn network representations. To improve the general-
izationofapproachesprocessingneuralnetworks, [38]ex- nf2vec
ENCODER
plores various strategies for data augmentation directly in
w woei rg kh st res pp ra ec se es n. tiO ngur inf dr ia vm ide uw ao lr ok bja el cs to s.p Ir noc pe as rs tie cs uln ae ru ,r wa el en met -- NeRF n e r f 2 c l i p
ploynf2vectoextractrichembeddingsfromNeRFs.
Figure 2. Feature mapping network training. clip2nerf
is a feature mapping network trained to map image embeddings
3.ConnectingNeRFsandCLIP
ofNeRFviewstoNeRFembeddings. Conversely, nerf2clip
Ourworkaimstolearntheconnectionbetweenimage,text, computesthemappingintheoppositedirection.
andNeRF[27]modalities. Toachievethisgoal,givenrich
multimodal representations extracted by Vision-Language
network weights. These codes can then be processed us-
Models such as CLIP [34] and compact embeddings ex-
ingstandarddeep-learningpipelinestoperformtaskssuch
tractedfromNeRFweightsbynf2vec[52],welearnhow
as classification or segmentation. nf2vec is a represen-
tomapanf2vecembeddingintoaplausibleCLIPembed-
tation learning framework that comprises an encoder and
dingandviceversa. Inthissection, wefirstreporttherel-
a decoder. The encoder consists of a series of linear lay-
evant background knowledge: NeRF, nf2vec, and CLIP
ers with batch normalization and ReLU non-linearity fol-
frameworks. Then, we describe our proposed framework
lowed by a final max pooling. It processes each layer of
depictedinFig.1.
theinputMLPindependently,obtainingonevectorforeach
MLPlayer. Then,thefinalmaxpoolingcompressesallthe
3.1.Preliminaries
layerembeddingsintoone,obtainingthedesiredgloballa-
NeRF. Givenimagesofasceneoranobject, NeRF[27] tentvectorrepresentingtheinputMLP,i.e.theinputNeRF.
allows for novel view synthesis from arbitrary vantage ThedecoderreproducestheoriginalNeRFvaluesgivenas
points. This is achieved by training a neural network, i.e. inputtheembeddingsproducedbytheencoderandaspatial
an MLP, on a set of sparse images collected from differ- coordinate x. Our paper utilizes the pre-trained nf2vec
entviewpoints. WefollowthebaseNeRFformulation[27] encodertoembedNeRFs,keepingitfrozen.
in which a single MLP parameterizes the radiance field of
the scene as a function of continuous 3D coordinates in
CLIP. CLIP (Contrastive Language-Image Pre-training)
space x = (x,y,z). Such a function produces a 4D out-
[34] is a pioneering visual language representation model.
put RGBσ, encoding the RGB color and volume density
The CLIP architecture consists of an image and a text en-
σofeach3Dpointinthescene. σcanbeinterpretedasthe
coder such as ViT [8] and BERT [7], respectively. CLIP
differential probability of a ray terminating at x. Given a
is trained using a contrastive learning objective on a large
NeRF,wecanrenderanimagefromanarbitraryviewpoint
set of data, which encourages the model to assign similar
withadesiredresolutionthroughvolumerendering[27]. In
embeddings to semantically related image-text pairs while
ourpaper,NeRFsareconsideredastandarddataformatand
maximizingthedissimilaritybetweenembeddingsofunre-
theinputtoourframework. WeassumethateachNeRFen-
lated pairs. This procedure enforces a multimodal vision-
codesaspecificobjectorscene. Wewishtoavoidsampling
language latent space in which images and corresponding
anyinformationfromtheNeRFs,suchasrenderingviews,
textual prompts share the same embedding. In our frame-
as it would require vast computational overhead and pose
work,weemploypre-trainedandfrozenCLIPencoders.
manychallenges,suchasthechoiceoftherenderingview-
point. This work aims to extract all information solely by 3.2.Featuremappingnetworks
processingMLPweightsoftheNeRF.
Architecture. TomapCLIPembeddingstonf2vecem-
beddings, we use a simple MLP with GELU [14] activa-
nf2vec. nf2vec[52]canextractcompactembeddings tion function and layer dimensions 512 → 768 → 1024,
fromMLPsthatparametrizeNeRFsbyprocessingonlythe where512and1024arethesizesofCLIPembeddingsand
3nf2vec embeddings, respectively. We call this feature
m Ma Lp Pp ,i dn ug bn be et dw no erk rc fl 2i cp l2 in pe ,wr if th. GSi Em Lil Uar aly c, tiw vae tiu os ne fuan no ctt ih oe nr
nerf2clip
andlayerdimensions1024 → 768 → 512tocomputethe nf2vec
ENCODER
mappingintheoppositedirection.
NeRF
Training. GivenaNeRFandnviewsofanobject,weex- NN
tract the nf2vec embedding v of the NeRF and n CLIP SEARCH
embeddings c of its views. For each view embedding c ,
i i
tw we et er nain itscl oui tp p2 utn 1e 0r 2f 4-to dim ma ex ni sm ioi nz ae lth ve ecc to os rin vˆes aim ndila tr hi ety eb me -- "A 3D MODEL OF A G A L L E R Y
i <CLASS-NAME>"
bedding v of that NeRF. Formally, the clip2nerf loss
foranobjectis:
Figure3.Zero-shotNeRFclassificationmethodoverview.
n (cid:18) (cid:19)
L =
1 (cid:88)
1−
vˆ i·v
clip2nerf
n ∥vˆ ∥∥v∥ Training details. Our feature mapping networks
i
i=1
clip2nerfandnerf2cliparetrainedfor150and100
Instead, wetrainnerf2cliptomapaNeRFembedding epochs, with learning rates 10−5 and 10−3, respectively.
v to the mean embedding of the n views, c= 1 (cid:80)n c , BotharetrainedwiththeAdamWoptimizer[22],one-cycle
n i=1 i
as learning to map a v to every c would create a one-to- learningratescheduler[40],weightdecay10−2,andbatch
i
many correspondence, i.e. not a function. Specifically, we size 64. We perform all our experiments on a single
maximize the cosine similarity between the nerf2clip NVIDIARTX3090GPU.
512-dimensionaloutputcˆandc. Formally:
5.Zero-shotNeRFclassification
cˆ·c
L nerf2clip =1− To perform zero-shot NeRF [27] classification, we build a
∥cˆ∥∥c∥
gallery of CLIP [34] embeddings of sentences of form “A
3Dmodelof<class-name>”,where<class-name>”
Duringtraining,thenviewscanbetheground-truthimages
denotes a class label of ShapeNetRender [50]. In other
usedtotraintheNeRForthoserenderedfromit.
words, the gallery contains one CLIP embedding for each
ShapeNetRender class. We then take a test-set NeRF, en-
4.ExperimentalSettings
code it with the nf2vec [52] encoder, process the results
withnerf2clip,andusetheoutputembeddingtoquery
NeRF framework and dataset. We use the NeRF [27]
datasetofnf2vec[52]. TheNeRFarchitectureconsistsof the gallery. Finally, the predicted label corresponds to the
text of the 1-NN. This procedure is illustrated in Fig. 3.
an MLP with ReLU activation function and 4 hidden lay-
As in our retrieval experiments (Sec. 6), the 1-NN maxi-
erswith256featureseach. Itappliesafrequencyencoding
mizes the cosine similarity with the query. Tab. 1 shows
[27] enc(x) to each input 3D coordinate x. Each NeRF is
classification results for two variants of nerf2clip, one
trained on N = 36 views of a shape of the ShapeNetRen-
trained with ground-truth views (“nerf2clip GT” row)
der dataset [50]. The dataset consists of a NeRF for each
and the other with views rendered from the correspond-
ShapeNetRendershape,foratotalof38653NeRFs,which
ing NeRF (“nerf2clip rendered” row). As an ablation,
we split into training (30946), validation (3847), and test
Tab. 2 shows the effect of training nerf2clip with an
(3860)sets. ForeachNeRF,wehaveaccesstothe36syn-
increasing number of views, i.e. training nerf2clip to
theticimagesusedfortrainingandtheirdepthmaps.
map a NeRF embedding to the mean CLIP embedding of
nNeRFviews,wheren = 1,2...N. Themodelwiththe
Metrics. We evaluate our retrieval experiments (Sec. 6) highestaccuracyinTab.2istheonereportedinTab.1.
with the recall@k metric, i.e. the percentage of queries q As a baseline to compare nerf2clip with, we query
suchthatatleastoneamongthefirstkneighborsofqshare ourgalleryofCLIPclass-labelembeddingswiththemean
the same label as q. The top-k nearest neighbors of q are of the CLIP embeddings of n random rendered views of a
thosewiththehighestcosinesimilaritywithq,sortedfrom test-setNeRF,wheren=1,2...N.Wedonotuseground-
closest to furthest. We call them 1-NN, 2-NN, ..., k-NN. truthviewsasqueries,asweassumetheyareonlyavailable
Ourclassificationexperimentsusethestandardmulti-class at training time. The results are shown in Tab. 1 (“CLIP”
accuracy(Sec.5). rows). nerf2clip, in both its versions, achieves higher
4Method Supervision Rendering Accuracy(%)↑ Time(ms)↓ Method Views Accuracy(%)↑
CLIP[34]1view 73.6 13 1 80.6
CLIP[34]2views 77.7 25
CLIP[34]4views ✗ ✓ 80.5 49 2 81.4
CLIP[34]8views 81.7 97 nerf2cliprendered 4 82.4
CLIP[34]16views 82.4 193 8 82.3
CLIP[34]Nviews 82.4 433 16 83.0
nerf2cliprendered(ours) ✗ ✗ 83.0 2 N 82.7
nerf2clipGT(ours) 84.0
nf2vec[52](oracle) ✓ ✗ 87.3 1 1 82.9
2 83.2
4 84.0
Table1.Zero-shotNeRFclassificationresults. nerf2clipGT
8 83.8
16 83.6
N 82.7
CLIP 84
400
nerf2clip
82 Table2.nerf2cliptrainingablation.Effectofthenumberof
300 viewsusedforthefeaturemappingnetworktrainingontheclassi-
80 ficationaccuracy.
200
78
100 76 C neL rI fP 2clip rendered c l i p 2 n e r f SEN ARN CH GALLERY
74 nerf2clip GT CLIP
0 IMAGE
124 8 16 N 124 8 16 N ENCODER
Views (#)
Figure4. Zero-shotNeRFclassificationresults(plots). Classi- c l i p 2 n e r f
fication time and accuracy as a function of the number of views
usedforthequery. "A 3D MODEL OF A TC EL XIP T NN NeRF
<CLASS-NAME>" ENCODER SEARCH
accuracythanthebaseline,whichplateausat16views. For Figure5.NeRFretrievalmethodoverview.NeRFretrievalfrom
comparison, we also report the classification accuracy of images(top)andtext(bottom).
nf2vec. It is important to note, however, that nf2vec
performs classification in a supervised manner, and there-
fore its accuracy should be regarded as an upper bound to or image query. To achieve this, we first construct offline
ourzero-shotclassificationresults. a gallery of NeRF embeddings obtained by nf2vec [52].
Then, we employ the image or text CLIP [34] encoders
Furthermore, Tab. 1 compares the time required to per-
to generate the corresponding embedding during retrieval.
form zero-shot classification with nerf2clip vs. the
This is processed by clip2nerf, yielding the predicted
CLIPbaseline.Forthebaselines,wereportthesumofview
NeRF embedding. Finally, we employ the NN search to
rendering time, CLIP inference time, and NN search time.
locatetheclosestembeddingwithinthegallery. Thisproce-
The rendering and the CLIP inference times scale linearly
dureisillustratedinFig.5.
with the number of views. For nerf2clip, we report
thesumofnf2vecencoderinferencetime, nerf2clip
6.1.NeRFretrievalfromimages
inference time, and NN search time. Our method is even
fasterthantheCLIPbaselineusingonly1view. The experiments reported here address the scenario in
which a user takes one or more pictures of an object (e.g.
TheresultsofTab.1arealsodepictedinFig.4,wherewe
a product in a store) and uses them to query a database
highlightthefactthat,whiletimeandaccuracyareafunc-
of NeRF objects (e.g. the online store catalog). Thus, we
tionofthenumberofviewsusedtoquerythegalleryforthe
CLIPbaseline,theyareaconstantfornerf2clipatinfer- always employ real or synthetic images as queries, i.e. no
rendered images from NeRFs. In the experiments of this
ence time, as the query is the output of the trained model,
section,webuildthegallerywiththeNeRFembeddingsob-
thus requiring no rendering nor CLIP inference. Further-
more, as already pointed out, nerf2clip achieves the
tainedbynf2veconthetestsetofShapeNetRender[50].
We exclude the queried NeRF from the gallery during re-
highestaccuracyandthefastestinferencetime.
trieval.
6.NeRFRetrieval
Single-viewquery. InTab.3,wereporttheresultsusinga
In the NeRF [27] retrieval application, we aim to identify singleimageasquery(arandomGTviewofthetestsetfor
NeRFs in a database that closely matches a given textual each object). We use the same random images across dif-
5
)sm(
emiT
)%(
ycaruccARecall(%)↑ 96.0
88.5 97.50
Method @1 @5 @10 Time(ms)↓ Memory(MB)↓ 88.0 95.5 97.25
CLIP[34]GTmean 81.4 93.9 96.3 24 8 97.00
CLIP[34]renderedmean 81.2 92.9 96.1 87.5 95.0 96.75
CLIP[34]GTall 83.6 93.0 95.1 331 271 87.0 96.50
CLIP[34]renderedall 81.6 91.7 95.1 94.5
86.5 96.25
clip2nerfGT(ours) 86.1 94.0 96.0
clip2nerfrendered(ours) 84.5 93.3 95.4 25 15 86.0 94.0 96.00
124 8 16 N 124 8 16 N 124 8 16 N
Views (#)
Table 3. NeRF retrieval from images (single-view query re-
sults). Figure 7. NeRF retrieval from images (multi-view query re-
sults).Performanceofourclip2nerffeaturemappingnetwork
asafunctionofthenumberofviewsusedforthequery.
QUERY 1-NN 2-NN 3-NN 4-NN 5-NN
Figure 8. ControlNet generated views. ShapeNetRender [50]
viewsvs.theircounterpartsgeneratedbyControlNet[53].
view. “CLIP rendered all”: the same as the previous one,
yet it employs the rendered views from NeRF instead of
ground-truth images. As shown in Tab. 3, our model ex-
hibitssuperiorperformanceintermsofrecall@1compared
tothebaselineswhilemaintainingcomparableresultsinthe
other metrics. Finally, we show some retrieval examples
in Fig. 6. We render a single reference view to visualize
NeRFs. As we can see, the retrieved NeRFs belong to the
sameobjectclassandresembletheinputimageincolorand
shape.
Figure6.QualitativeresultsofNeRFretrievalfromimageson
Multi-view query. We focus here on the case where a
ShapeNetRender [50]. For each NeRF, we visualize one view
renderedfromavantagepoint. user can acquire multiple pictures of the same object. In
the plots of Fig. 7, we show the retrieval recall@1, re-
call@5,andrecall@10resultswhenvaryingthenumberof
ferenttablerowstocomparemethodsfairly. Wereportthe queryimagesusedforeachobject. Thegalleryisthesame
resultsofourclip2nerfmethod,trainedusingeitherthe as in the single-view scenario, i.e. a NeRF embedding for
ground-truthimages(clip2nerfGT)ortherenderedim- eachobject. Thequeryviewsareselectedrandomlyamong
agesfromtheNeRFs(clip2nerfrendered). Inthisway, theground-truthimagesinShapeNetRender. Werandomly
we simulate the two possible scenarios in which the im- chooseonlytheadditionalviewswhenincreasingthenum-
agesusedtotraintheNeRFsareavailableornotattraining ber of queries. For instance, the experiment with 8 views
time. Moreover, we report the performance of four plau- includestheimagesusedinthe4viewsresults. Toretrieve
siblebaselinestrategiesthatexploitdifferentgalleriesbuilt the NeRF, we pass the n multi-view queries to the CLIP
using CLIP embeddings from images. “CLIP GT mean”: imageencoder,obtainingnembeddings. Eachisprocessed
each gallery element is the average of N = 36 CLIP em- by clip2nerf (the model named clip2nerf GT in
beddings obtained from N object views. “CLIP rendered Tab. 3), and the resulting NeRF embeddings are averaged
mean”: the same as the previous one, yet we employ N togetareferenceembeddingforthatobject. Then,weper-
renderedimagesfromNeRFfromthesameN viewpoints. formtheNNsearchwithinthegallery. Interestingly,when
“CLIP GT all”: for each object in the test set, we store N thenumberofqueryimagesusedforretrievalincreases,the
CLIPembeddingsinthegallery,oneforeachground-truth results improve significantly until 8 views, at which point
6
)%(
1@llaceR
TENEPAHS
TENLORTNOC
)%(
5@llaceR
)%(
01@llaceRQUERY 1-NN 2-NN 3-NN 4-NN 5-NN QUERY 1-NN 2-NN 3-NN 4-NN 5-NN
a 3d
model of
a car
a 3d
model of
a fighter
jet flying
a 3d model
of a lamp
with a
white
shade
a 3d
model of
a pink
sofa
a white
chair
Figure10. QualitativeresultsofNeRFretrievalfromtexton
ShapeNetRender [50]. For each NeRF, we visualize one view
renderedfromavantagepoint.
Figure 9. Qualitative results of NeRF retrieval from real im-
ages. Queries are real images from DomainNet [32]. For each Recall(%)↑
NeRF,wevisualizeoneviewrenderedfromavantagepoint.
Method @1 @5 @10
CLIPGT 80.0 89.6 92.8
Recall(%)↑
CLIPrendered 79.6 89.5 92.0
Method @1 @5 @10 clip2nerfGT(ours) 63.2 75.2 79.0
clip2nerfGTmultimodal(ours) 85.6 91.7 93.3
CLIP[34]GT 75.5 90.4 93.7
CLIP[34]rendered 73.9 89.8 93.8
Table5.NeRFretrievalfromtextresults.
clip2nerfGT(ours) 67.9 80.7 85.6
clip2nerfGTsyn2real(ours) 79.9 87.4 90.1
Table4. NeRFretrievalfromrealimages(adaptationresults). an adaptation protocol based on the recent diffusion-based
GalleryofNeRFsfromShapeNetRender[50]. QueriesfromDo- conditional generative approach, ControlNet [53]. In par-
mainNet[32]. ticular, we generate a new synthetic to real datasets with
ControlNet, using the synthetic object depth map as input
tothegenerativenetwork(seeFig.8). Thesegeneratedim-
performance plateaus. Thus, we can conclude that the in-
agesareaddedtothesyntheticonestotrainclip2nerf.
formationprovidedbytheadditionalviewscanbevaluable
The augmented dataset contains 7 synthetic random views
forretrieval.
and7imagesgeneratedbyControlNetforeachobject. By
training on this dataset, we learned a feature mapping that
Adaptation to real images. In the previous retrieval ex- canbeappliedeffectivelytorealimages. Wereporttheper-
periments, we employed solely synthetic query images. formanceofthisnetworkinthelastrowofTab.4, andwe
However,inapracticalscenario,wewoulduserealimages can observe a remarkable performance improvement w.r.t.
acquiredinthewild. Thus,weevaluatedclip2nerfus- thenetworktrainedwithoutaugmenteddata.Moreover,our
ing the real split of the DomainNet [32] dataset, reporting methodperformscomparablytothebaselineusingtheCLIP
resultsinTab.4. ThegalleryconsistsoftheNeRFembed- galleriesobtainedfromimages(rows1and2vs.4).Finally,
dingsfromShapeNetRender. Wenoteaperformancedrop inSec.6.1,weshowretrievalresultsusingin-the-wildreal
comparedtothecaseoftestingonsyntheticimages, prob- queries. Remarkably,theretrievedNeRFresemblesthege-
ably due to the domain-shift problem. Thus, we propose ometryoftheinputimagewithhighfidelity.
7INPUT GENERATED
c l i p 2 n e r f
CLIP
nf2vec
IMAGE
ENCODER DECODER
c l i p 2 n e r f
"A 3D CLIP
MODEL OF TEXT nf2vec
A CAR" ENCODER DECODER
Figure11.NeRFgenerationmethodoverview.NeRFgeneration
fromimages(top)andtext(bottom).
6.2.NeRFretrievalfromtext Figure12.QualitativeresultsofNeRFgenerationfromimages.
SyntheticimagesfromShapeNetRender[50](top)andrealimages
WeexperimentherewiththeretrievalofNeRFsfromtext.
fromDomainNet[32](bottom).
In this scenario, given a text prompt, we want to find the
correspondingNeRFinourdatabase.
Weemploythesamegalleryofthesingle-viewscenario a 3d model of a car
of NeRF retrieval from images. To obtain a reasonable
querytextfortheinputimages,weusetheBLIP-2[20]cap-
tioner. WereportresultsonShapeNetRenderinTab.5. Our
clip2nerfobtainslowerperformancethanthebaselines
usingtheCLIPgalleries. Werelatethistothefeaturemap-
pingfunction,whichwasnevertrainedontheCLIPtextem- Figure13.QualitativeresultsofNeRFgenerationfromtext.
beddings. Forthisreason,wetrainavariantofourmethod
usingasinputtoclip2nerfeithertheCLIPembeddings
Generation from text. Analogously, our framework al-
obtained from an image or the CLIP embedding of an au-
lows to synthesize new NeRF views from text. Given the
tomatically generated caption with BLIP-2. As shown in
previous generation pipeline, we replace the CLIP image
Tab.5,thismultimodaltrainingparadigmcanevensurpass
encoderwiththeCLIPtextencoder(Fig.11bottom). Qual-
thebaselinesbyamoderatemargin(row1and2vs.row4).
itativeresultsareshowninFig.13.
Finally, we also visualize some qualitative results in
Fig. 10. We note that we can retrieve NeRFs of the class
8.LimitationsandConclusions
described in the text, which contains details presented in
thetextualprompt,e.g.wecorrectlyretrieveNeRFsofajet Our proposed framework effectively connects NeRF, im-
fighterinthesecondrow. ages, and text. We have demonstrated its application in
severalnoveltasks,includingNeRFretrievalorgeneration
7.NeRFgeneration
fromtextandimages,aswellaszero-shotclassificationof
NeRFsusingonlynetworkweights.
Generationfromimages. Anotherapplicationofourap-
However,ourframeworkhasitslimitations. Firstly,the
proachconsistsin,givenaNeRF[27]view,synthesizenew
nf2vec encoder, trained on ShapeNetRender, limits our
viewsoftheobjectbyleveragingthenf2vec[52]decoder.
experimentstoNeRFsofsyntheticobjectsonly. Addition-
Specifically,thisprocedureworksasfollows:weembedthe
ally, theNeRFgenerationisconstrainedbytheprocessing
NeRFviewwiththeCLIP[34]imageencoderandgivethe
capabilitiesofthenf2vecdecoder.
resulting embedding as input to the trained clip2nerf
In the future, expanding our work to include NeRFs
network, which produces a NeRF embedding. The latter
of real objects or scenes would be valuable. Learn-
can be processed by the nf2vec decoder to render arbi-
ing a shared latent space for NeRFs, images, and text,
trary views of the object. Thus, the embedding plus the
e.g. by jointly training the vision, language, and NeRF
decoder can be considered a NeRF architecture. This pro- encoders on larger datasets, could also be a promis-
cedureisillustratedinFig.11(top). Qualitativeresultsare ing direction. We plan to address these limitations by
showninFig.12, bothwithimagesfromShapeNetRender exploring these ideas in future studies and hope that
[50](top)andrealimagesfromDomainNet[32](bottom). our framework inspires further advancements in the field.
8
TUPNI
DETARENEG[12] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRoss
Girshick. Momentumcontrastforunsupervisedvisualrep-
References resentationlearning. InProceedingsoftheIEEE/CVFcon-
ference on computer vision and pattern recognition, pages
[1] Adriano Cardace, Pierluigi Zama Ramirez, Francesco Bal- 9729–9738,2020. 2
lerini, Allan Zhou, Samuele Salti, and Luigi di Stefano. [13] RobertHecht-Nielsen.Onthealgebraicstructureoffeedfor-
Neural processing of tri-plane hybrid neural fields. In The wardnetworkweightspaces. InAdvancedNeuralComput-
Twelfth International Conference on Learning Representa- ers,pages129–135.Elsevier,1990. 3
tions,2024. 3 [14] Dan Hendrycks and Kevin Gimpel. Gaussian error linear
[2] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and units(gelus). arXivpreprintarXiv:1606.08415,2016. 3
Hao Su. Tensorf: Tensorial radiance fields. In European [15] Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, and
ConferenceonComputerVision(ECCV),2022. 2 Chi-KeungTang. Nerf-rpn: Ageneralframeworkforobject
[3] TingChen,SimonKornblith,MohammadNorouzi,andGe- detection in nerfs. In Proceedings of the IEEE/CVF Con-
offreyHinton. Asimpleframeworkforcontrastivelearning ferenceonComputerVisionandPatternRecognition,pages
ofvisualrepresentations.InInternationalconferenceonma- 23528–23538,2023. 1
chinelearning,pages1597–1607.PMLR,2020. 2 [16] FlorianJaeckleandMPawanKumar. Generatingadversar-
[4] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, ialexampleswithgraphneuralnetworks. InUncertaintyin
FaisalAhmed,ZheGan,YuCheng,andJingjingLiu.Uniter: ArtificialIntelligence,pages1556–1564.PMLR,2021. 2
Universal image-text representation learning. In European [17] Boris Knyazev, Michal Drozdzal, Graham W. Taylor, and
conference on computer vision, pages 104–120. Springer, AdrianaRomero. Parameterpredictionforunseendeepar-
2020. 2 chitectures. InAdvancesinNeuralInformationProcessing
[5] LucaDeLuigi,DamianoBolognini,FedericoDomeniconi, Systems,2021. 2
DanieleDeGregorio, MatteoPoggi, andLuigiDiStefano. [18] MiltiadisKofinas,BorisKnyazev,YanZhang,YunluChen,
Scannerf: a scalable benchmark for neural radiance fields. Gertjan J Burghouts, Efstratios Gavves, Cees GM Snoek,
InProceedingsoftheIEEE/CVFWinterConferenceonAp- and David W Zhang. Graph neural networks for learn-
plicationsofComputerVision,pages816–825,2023. 1 ing equivariant representations of neural networks. In The
[6] LucaDeLuigi,AdrianoCardace,RiccardoSpezialetti,Pier- Twelfth International Conference on Learning Representa-
luigi Zama Ramirez, Samuele Salti, and Luigi Di Stefano. tions,2023. 3
Deeplearningonimplicitneuralrepresentationsofshapes. [19] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
In International Conference on Learning Representations Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
(ICLR),2023. 2 Alignbeforefuse:Visionandlanguagerepresentationlearn-
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina ingwithmomentumdistillation. Advancesinneuralinfor-
Toutanova. Bert: Pre-training of deep bidirectional mationprocessingsystems,34:9694–9705,2021. 2
transformers for language understanding. arXiv preprint [20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
arXiv:1810.04805,2018. 3 BLIP-2: Bootstrapping language-image pre-training with
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, frozen image encoders and large language models. arXiv
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, preprintarXiv:2301.12597,2023. 8
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- [21] DerekLim,HaggaiMaron,MarcT.Law,JonathanLorraine,
vain Gelly, et al. An image is worth 16x16 words: Trans- and James Lucas. Graph metanetworks for processing di-
formers for image recognition at scale. arXiv preprint verseneuralarchitectures.InTheTwelfthInternationalCon-
arXiv:2010.11929,2020. 2,3 ferenceonLearningRepresentations,2024. 3
[9] Emilien Dupont, Hyunjik Kim, SM Ali Eslami, [22] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
Danilo Jimenez Rezende, and Dan Rosenbaum. From regularization. arXivpreprintarXiv:1711.05101,2017. 4
data to functa: Your data point is a function and you can [23] JingyueLuandM.PawanKumar.Neuralnetworkbranching
treat it like one. In International Conference on Machine forneuralnetworkverification. InInternationalConference
Learning,pages5694–5725.PMLR,2022. 2 onLearningRepresentations,2020. 2
[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong [24] JiasenLu,DhruvBatra,DeviParikh,andStefanLee.Vilbert:
Chen, BenjaminRecht, andAngjooKanazawa. Plenoxels: Pretraining task-agnostic visiolinguistic representations for
Radiance fields without neural networks. 2022 IEEE/CVF vision-and-languagetasks. Advancesinneuralinformation
Conference on Computer Vision and Pattern Recognition processingsystems,32,2019. 2
(CVPR),2022. 2 [25] JiasenLu,ChristopherClark,RowanZellers,RoozbehMot-
[11] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat taghi, and Aniruddha Kembhavi. Unified-io: A unified
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan modelforvision, language, andmulti-modaltasks. InThe
Misra. Imagebind: Oneembeddingspacetobindthemall. EleventhInternationalConferenceonLearningRepresenta-
In Proceedings of the IEEE/CVF Conference on Computer tions,2022. 2
VisionandPatternRecognition,pages15180–15190,2023. [26] RicardoMartin-Brualla, NohaRadwan, MehdiSMSajjadi,
1,2 Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-
9worth. Nerfinthewild: Neuralradiancefields foruncon- [39] AmanpreetSingh, RonghangHu, VedanujGoswami, Guil-
strainedphotocollections. InProceedingsoftheIEEE/CVF laume Couairon, Wojciech Galuba, Marcus Rohrbach, and
Conference on Computer Vision and Pattern Recognition, Douwe Kiela. Flava: A foundational language and vision
pages7210–7219,2021. 2 alignment model. In Proceedings of the IEEE/CVF Con-
[27] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, ferenceonComputerVisionandPatternRecognition,pages
JonathanTBarron,RaviRamamoorthi,andRenNg. Nerf: 15638–15650,2022. 2
Representingscenesasneuralradiancefieldsforviewsyn- [40] Leslie N Smith and Nicholay Topin. Super-convergence:
thesis. In European conference on computer vision, pages Very fast training of neural networks using large learning
405–421.Springer,2020. 1,2,3,4,5,8 rates. In Artificial intelligence and machine learning for
[28] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, multi-domainoperationsapplications,page1100612.Inter-
PratulPSrinivasan,andJonathanTBarron.Nerfinthedark: nationalSocietyforOpticsandPhotonics,2019. 4
Highdynamicrangeviewsynthesisfromnoisyrawimages. [41] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
In Proceedings of the IEEE/CVF Conference on Computer Wei,andJifengDai. Vl-bert:Pre-trainingofgenericvisual-
VisionandPatternRecognition,pages16190–16199,2022. linguisticrepresentations. arXivpreprintarXiv:1908.08530,
2 2019. 2
[29] AnshMittal. Neuralradiancefields: Past, present, andfu-
[42] ChengSun,MinSun,andHwann-TzongChen.Directvoxel
ture. arXivpreprintarXiv:2304.10050,2023. 1
gridoptimization:Super-fastconvergenceforradiancefields
[30] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexan- reconstruction.InProceedingsoftheIEEE/CVFConference
derKeller.Instantneuralgraphicsprimitiveswithamultires- on Computer Vision and Pattern Recognition, pages 5459–
olution hash encoding. ACM Trans. Graph., 41(4):102:1– 5469,2022. 2
102:15,2022. 2
[43] ThomasUnterthiner,DanielKeysers,SylvainGelly,Olivier
[31] AvivNavon, AvivShamsian, IdanAchituve, EthanFetaya,
Bousquet,andIlyaO.Tolstikhin. Predictingneuralnetwork
GalChechik,andHaggaiMaron. Equivariantarchitectures
accuracyfromweights. arXiv,abs/2002.11448,2020. 2
forlearningindeepweightspaces. InInternationalConfer-
[44] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
enceonMachineLearning,2023. 3
KevinLin,ZheGan,ZichengLiu,CeLiu,andLijuanWang.
[32] XingchaoPeng, QinxunBai, XideXia, ZijunHuang, Kate
Git: A generative image-to-text transformer for vision and
Saenko,andBoWang. Momentmatchingformulti-source
language. Transactions on Machine Learning Research,
domainadaptation.InProceedingsoftheIEEEInternational
2022. 2
ConferenceonComputerVision,pages1406–1415,2019. 7,
[45] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
8
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
[33] BenPoole,AjayJain,JonathanTBarron,andBenMilden-
Hongxia Yang. Ofa: Unifying architectures, tasks, and
hall. Dreamfusion: Text-to-3d using 2d diffusion. In The
modalities through a simple sequence-to-sequence learning
EleventhInternationalConferenceonLearningRepresenta-
framework. InInternationalConferenceonMachineLearn-
tions,2022. 2
ing,pages23318–23340.PMLR,2022. 2
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[46] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xi-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
aohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
Zhou. One-peace: Exploring one general representa-
ingtransferablevisualmodelsfromnaturallanguagesuper-
tion model toward unlimited modalities. arXiv preprint
vision. In International Conference on Machine Learning,
arXiv:2305.11172,2023. 2
pages8748–8763.PMLR,2021. 1,2,3,4,5,6,7,8
[47] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
hammed, SakshamSinghal, SubhojitSom, etal. Imageas
AmandaAskell,PamelaMishkin,JackClark,etal.Learning
a foreign language: Beit pretraining for vision and vision-
transferable visual models from natural language supervi-
sion.InInternationalconferenceonmachinelearning,pages
languagetasks.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages19175–
8748–8763.PMLR,2021. 2
19186,2023. 2
[36] Pierluigi Zama Ramirez, Luca De Luigi, Daniele Sirocchi,
AdrianoCardace,RiccardoSpezialetti,FrancescoBallerini, [48] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
SamueleSalti, andLuigiDiStefano. Deeplearningon3d Tsvetkov,andYuanCao. Simvlm: Simplevisuallanguage
neuralfields. arXivpreprintarXiv:2312.13277,2023. 1,2 model pretraining with weak supervision. In International
[37] Konstantin Schu¨rholt, Dimche Kostadinov, and Damian ConferenceonLearningRepresentations,2021. 2
Borth.Self-supervisedrepresentationlearningonneuralnet- [49] ShengqiongWu,HaoFei,LeigangQu,WeiJi,andTat-Seng
work weights for model characteristic prediction. In Ad- Chua. NExt-GPT:Any-to-anymultimodalLLM,2024. 2
vancesinNeuralInformationProcessingSystems,2021. 2 [50] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir
[38] Aviv Shamsian, Aviv Navon, David W Zhang, Yan Zhang, Mech, and Ulrich Neumann. Disn: Deep implicit surface
Ethan Fetaya, Gal Chechik, and Haggai Maron. Improved network for high-quality single-view 3d reconstruction. In
generalizationofweightspacenetworksviaaugmentations. AdvancesinNeuralInformationProcessingSystems.Curran
arXivpreprintarXiv:2402.04081,2024. 3 Associates,Inc.,2019. 4,5,6,7,8
10[51] LinYen-Chen,PeteFlorence,JonathanTBarron,Tsung-Yi
Lin,AlbertoRodriguez,andPhillipIsola. Nerf-supervision:
Learning dense object descriptors from neural radiance
fields. In2022internationalconferenceonroboticsandau-
tomation(ICRA),pages6496–6503.IEEE,2022. 2
[52] Pierluigi Zama Ramirez, Luca De Luigi, Daniele Sirocchi,
AdrianoCardace,RiccardoSpezialetti,FrancescoBallerini,
SamueleSalti,andLuigiDiStefano. Deeplearningon3D
neuralfields. arXivpreprintarXiv:2312.13277,2023. 2,3,
4,5,8
[53] LvminZhang, AnyiRao, andManeeshAgrawala. Adding
conditional control to text-to-image diffusion models. In
ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages3836–3847,2023. 2,6,7
[54] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,
Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.
Vinvl: Revisiting visual representations in vision-language
models. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages5579–5588,
2021. 2
[55] Allan Zhou, Kaien Yang, Kaylee Burns, Adriano Cardace,
Yiding Jiang, Samuel Sokota, J Zico Kolter, and Chelsea
Finn. Permutationequivariantneuralfunctionals. Advances
inneuralinformationprocessingsystems,37,2023. 3
[56] AllanZhou,KaienYang,YidingJiang,KayleeBurns,Win-
nie Xu, Samuel Sokota, J Zico Kolter, and Chelsea Finn.
Neural functional transformers. Advances in neural infor-
mationprocessingsystems,37,2023.
[57] Allan Zhou, Chelsea Finn, and James Harrison. Universal
neuralfunctionals. arXivpreprintarXiv:2402.05232,2024.
3
11