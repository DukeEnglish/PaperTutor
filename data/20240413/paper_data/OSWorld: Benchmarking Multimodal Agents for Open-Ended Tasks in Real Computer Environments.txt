OSWORLD: Benchmarking Multimodal Agents for
Open-Ended Tasks in Real Computer Environments
TianbaoXie♠DanyangZhang♠JixuanChen♠XiaochuanLi♠SihengZhao♠
RuishengCao♠TohJingHua♠ZhoujunCheng♠DongchanShin♠FangyuLei♠YitaoLiu♠
YihengXu♠ShuyanZhou♣SilvioSavarese♡CaimingXiong♡VictorZhong♢TaoYu♠
♠TheUniversityofHongKong ♣CMU ♡SalesforceResearch ♢UniversityofWaterloo
Abstract
Autonomousagentsthataccomplishcomplexcomputertaskswithminimalhuman
interventionshavethepotentialtotransformhuman-computerinteraction,signif-
icantlyenhancingaccessibilityandproductivity. However,existingbenchmarks
eitherlackaninteractiveenvironmentorarelimitedtoenvironmentsspecificto
certain applications or domains, failing to reflect the diverse and complex na-
ture of real-world computer use, thereby limiting the scope of tasks and agent
scalability. Toaddressthisissue, weintroduce OSWORLD, thefirst-of-its-kind
scalable,realcomputerenvironmentformultimodalagents,supportingtasksetup,
execution-basedevaluation,andinteractivelearningacrossvariousoperatingsys-
temssuchasUbuntu,Windows,andmacOS.OSWORLDcanserveasaunified,
integratedcomputerenvironmentforassessingopen-ended computertasksthat
involvearbitraryapplications. BuildinguponOSWORLD,wecreateabenchmark
of369computertasksinvolvingrealwebanddesktopappsinopendomains,OS
file I/O, and workflows spanning multiple applications. Each task example is
derivedfromreal-worldcomputerusecasesandincludesadetailedinitialstate
setupconfigurationandacustomexecution-basedevaluationscriptforreliable,
reproducibleevaluation. Extensiveevaluationofstate-of-the-artLLM/VLM-based
agentson OSWORLD revealssignificantdeficienciesintheirabilitytoserveas
computerassistants. Whilehumanscanaccomplishover72.36%ofthetasks,the
bestmodelachievesonly12.24%success,primarilystrugglingwithGUIgrounding
andoperationalknowledge. ComprehensiveanalysisusingOSWORLDprovides
valuableinsightsfordevelopingmultimodalgeneralistagentsthatwerenotpossible
withpreviousbenchmarks. Ourcode,environment,baselinemodels,anddataare
publiclyavailableathttps://os-world.github.io.
1 Introduction
Humansinteractwithcomputerstoperformessentialtasksinthedigitalrealm,includingwebbrows-
ing,videoediting,filemanagement,dataanalysis,andsoftwaredevelopment. Thesetaskworkflows
ofteninvolvemultipleapplicationsthroughgraphicaluserinterfaces(GUI)andcommandlineinter-
faces(CLI).Autonomousdigitalagents,poweredbyadvancementsinlargevision-languagemodels
(VLMs),havethepotentialtorevolutionizehowweinteractwithcomputerenvironments[27,41,1].
Byfollowinghigh-levelnaturallanguageinstructions, theseagentscanmakedigitalinteractions
moreaccessibleandvastlyincreasehumanproductivity. However,amajorchallengeindeveloping
suchmultimodalagentsistheabsenceofabenchmarkbasedonarealinteractiveenvironmentthat
coversthediversityandcomplexityofreal-worldcomputeruseacrossvariousoperatingsystems,
interfaces,andapplications,consequentlyrestrictingtaskscopeandagentscalability.
Previousbenchmarksprovidedatasetsofdemonstrationswithoutexecutableenvironments[9,38,
20]. Theirnon-execution-basedevaluationassumesasinglesolutionforeachtaskandwrongfully
penalizes alternative correct solutions. These benchmarks also miss opportunities for essential
Preprint.Underreview.
4202
rpA
11
]IA.sc[
1v27970.4042:viXraTask instruction 1: Update the bookkeeping sheet with my recent transactions over the past few days in the provided folder.
Task instruction 2: ...some details about snake game omitted… Could you help me tweak the code so the snake can actually eat the food?
Task Initial State Setup Config task initial env state setup Virtual Machine(s) get env state
Observation
screenshot a11y-tree Final State
Task Instruction input Agent input
(See examples above) (e.g., GPT-4V) predict Execution-based
OS Arbitrary Apps Interfaces
mouse keyboard Evaluation
Action OSWorld Environment
Figure1: OSWORLDisafirst-of-its-kindscalable,realcomputerenvironmentformultimodalagents,
supportingtasksetup,execution-basedevaluation,andinteractivelearningacrossoperatingsystems.
Itcanserveasaunifiedenvironmentforevaluatingopen-endedcomputertasksthatinvolvearbitrary
apps(e.g.,taskexamplesintheaboveFig). Wealsocreateabenchmarkof369real-worldcomputer
tasksinOSWORLDwithreliable,reproduciblesetupandevaluationscripts.
agentdevelopmentmethodslikeinteractivelearningandreal-worldexploration. Buildingrealistic
interactive environments is a major challenge in developing multimodal agents. Prior work that
introduceexecutableenvironmentssimplifytheobservationandactionspacesofhuman-computer
interactionandlimittaskscopewithinspecificapplicationsordomains,suchaswebnavigationin
afewdomains[41,29,54,62],coding[53]andthecombination[31,50,33]. Agentsdevelopedin
theserestrictedenvironmentscannotcomprehensivelycovercomputertasks,lackingthesupportof
evaluatingtasksincomplex,real-worldscenariosthatrequirenavigatingbetweenapplicationsand
interfacesinopendomains(taskexamplesine.g.,Fig.1).
Toaddressthisgap,weintroduceOSWORLD,thefirst-of-its-kindscalable,realcomputerenvironment
designed for the development of multimodal agents capable of executing a wide range of real
computer tasks beyond isolated interfaces and applications. This executable environment allows
free-formrawkeyboardandmousecontrolofrealcomputerapplicationsandsupportsinitialtask
stateconfiguration,execution-basedevaluation,andinteractivelearningacrossmainstreamoperating
systems (e.g., Ubuntu, Windows, macOS). As shown in Fig. 1, OSWORLD enables evaluation
ofopen-ended computertasksthatinvolvearbitraryapplications,rangingfromimageviewingto
software functionality integration and programming. Thus, OSWORLD can serve as a unified,
realcomputerenvironmentthatallowsuserstodefinetheiragenttaskswithouttheneedtobuild
application/domain-specificsimulatedenvironments.
BuildinguponOSWORLD,wecreateabenchmarkwith369real-worldcomputertasksthatinvolve
widely-usedwebanddesktopappsinopendomains,OSfileI/O,andmulti-appworkflowsthrough
bothGUIandCLI.Eachtaskexampleisbasedonreal-worldcomputerusecasesexperiencedbyreal
usersandoftenrequiresinteractionswithmultipleapplicationsandinterfaces. Toensurereliable,
reproducible assessment within the OSWORLD environment, 9 authors with computer science
backgroundscarefullyannotateeachexamplewithaninitialstatesetupconfigurationtosimulate
humanworkinprogressandacustomexecution-basedevaluationscripttoverifytaskcompletion.
Ourbenchmarkhasatotalof134uniqueevaluationfunctions,whichareordersofmagnitudelarger
thanpriorwork[62],showcasingthecomplexity,diversity,andevaluationchallengesoftasksinour
benchmark. ThehumanperformancestudyindicatesthattaskexamplesfromOSWORLDaremore
time-consumingandchallengingcomparedtothoseinpriorwork.
Weextensivelyevaluatestate-of-the-artLLMandVLM-basedagentbaselines,includingtheGPT-4V
series [37], the Gemini-Pro models [45] and the Claude-3 Opus [3], as well as Mixtral [18] and
CogAgent[16]fromtheopen-sourcecommunity. Theperformanceoftheseexperimentsrangesfrom
0.99%to12.24%,withsubsetsofapplicationsevenreaching0%,forworkflowtasksthatinvolve
2cooperationfrommultipleapps,thehighestperformanceofthebaselineagentisonly6.57%. This
indicatesthatcurrentLLMsandVLMsarefarfromcapableofservingascomputerassistants(§4.2).
ResultsalsoshowthatwhileadditionalknowledgesuchastheaccessibilitytreeandSet-of-Mark
(§4.1)canbehelpful,itcanalsoleadtopotentialmisguidanceandvariesacrossmodels. Wealso
observeperformancechangesintheseagentscomparedtoconsistenthumanperformanceacross
different types of computer tasks. Analysis reveals that VLM-based agents struggle to ground
on screenshots to predict precise coordinates for actions, tend to predict repetitive actions, are
unable to handle noise from unexpected application windows and exhibit limited knowledge of
basicGUIinteractionsanddomain-specificfeaturesofapps(§5.2,§5.4). Feedinghigherresolution
and more trajectory history can help improve the performance by even doubling while requiring
longercontextlengthandefficientmodeling(§5.2). Weopen-sourceOSWORLDenvironmentand
benchmark,includingenvironmentinitialstatesetup,reliableevaluationscripts,documentation,and
ourimplementationofbaselinemodelstopromoteresearchtowardsthegoalofgeneralistcapable
computeragents1. FutureworkcanfocusonenhancingVLMGUIgroundingabilities,including
interactioncommonsenseknowledge,higher-resolutionsupport,andcoordinatesaccuracyformore
robustGUIinteractions. Additionally,effortscanbemadetoimproveagentarchitecturestobetter
handlecomplexcomputertasksthroughexploration,memory,andreflection.
2 OSWORLD Environment
In this section, we will introduce the task definition of autonomous agents, the components and
implementationoftheOSWORLDenvironment,andthesupportedobservationandactionspaces.
2.1 TaskDefinition
AnautonomousdigitalagenttaskcanbeformalizedasapartiallyobservableMarkovdecisionprocess
(POMDP)(S,O,A,T,R)withstatespaceS,observationspaceO(§2.3,includingnaturallanguage
I),actionspaceA(§2.4),transitionfunctionT :S×A→S,andrewardfunctionR:S×A→R.
Given current observation o ∈ O (a natural language instruction observation and a screenshot
t
observation (e.g., computer screenshot), a11y tree, or their combination according to facilities
available), anagentgeneratesexecutableactiona ∈ A(e.g., clickingonthecertainpixelofthe
t
screen—.click(300, 540, button=‘right’),presskeycombination—.hotkey(‘ctrl’,
‘alt’, ‘t’)), which results in a new state s ∈ S (e.g., current Desktop environment) and a
t+1
newpartialobservationo ∈ O (e.g.,currentscreenshot). Theinteractionlooprepeatsuntilan
t+1
action that marks termination (DONE or FAIL, see Sec. 2.4) is generated or the agent reaches the
maxnumberofsteps(e.g.,20inourexperiments). InthisversionofOSWORLD,weimplementan
execution-basedrewardfunctionR:S×A→[0,1](§2.2.3). Therewardfunctionawardsavalue
of1orapositivedecimalunder1atthefinalstepifthestatetransitionsmeettheexpectationsofthe
taskobjective(i.e.,thegoalissuccessfullyachievedorpartiallyachieved),oriftheagentaccurately
predictsfailureforaninfeasibletask. Inallotherscenarios,itreturns0.
2.2 RealComputerEnvironmentInfrastructure
OSWORLDisanexecutableandcontrollableenvironmentthatsupportstaskinitialization,execution-
basedevaluation,andinteractiveagentlearninginarangeofrealoperatingsystems(e.g.,Ubuntu,
Windows, macOS) using virtual machine techniques, shown in the middle and right of Fig. 2.
Virtualmachineoffersasafeisolatedenvironmentandpreventstheagentresultinginirreversible
damaging effect on the real host machine. The snapshot feature also enables efficient reset of
the virtual environment. The environment is configured through a config file (shown in the left
of Fig. 2) for interface initialization during the initialization phase (including downloading files,
openingsoftware,adjustinginterfacelayout)(§2.2.2,highlightedwithredinFig.2),post-processing
during the evaluation phase (activating certain windows, saving some files for easy retrieval of
information,highlightedwithorange),andacquiringfilesandinformationforevaluation(suchas
thefinalspreadsheetfileforspreadsheettasks,cookiesforChrometasks,highlightedwithyellowin
Fig.2),aswellastheevaluationfunctionsandparametersused(§2.2.3,highlightedwithgreenin
Fig.2). SeeApp.A.1formoredetails.
1https://os-world.github.io
3Config Agent
{ "instruction": "Please update my bookkeeping sheet with the recent transactions from the provided folder, detailing
my expenses over the past few days.", Virtual Machine Platform
"config": [{"type": "download", Coordinator
"parameters": {"files": [
{ {" ""p pua art tlh h"" "::
:
"" "h/ /th hto opm mse e:/ //u u/s sde err ri/ /vD Dee e.s sgk kot too ogp pl/ /em r.y ec_ cob emo i/o puk tck _?e 0ie .dp j=i pxn exg gx. "xx ,"l }s ,x", Simulator
Virtual Machine
v Fm lar su kn cc oo mm mm aa nn dd ss, V VM
ir
t1
ual Machine
"url": "https://drive.google.com/uc?id=xxxx"},…]}}, Controller Control Receiver
{"t "y pp ae r" a: m e" to ep re sn "" :, { "path": screen capture status, files, infos…
"/home/user/Desktop/my_bookkeeping.xlsx"}}], accessibility tree
"evaluator": {"postconfig": [{"type": "activate_window",
"parameters": {"window_name": "my_bookkeeping.xlsx - Task
Libre "O rf ef si uc le t "C :a l {c "" t, y. p. e. " :] , "vm_file", Manager Postprocess …
"path": "/home/user/Desktop/my_bookkeeping.xlsx",
"dest": "my_bookkeeping.xlsx"}, Set-up Getter VM
" " "e f ox u p" "p n tp de c ia ec " ot st : nh te s" "d " ": :" c : : o " " m {h m{ pt y" at _t rp by es op _: oe t/ k" a/ k: bd e lr e" ei pc "v il ,e no . gu g _d o g_ o of g li l dl e .e . x" c l, o sm x/ "u c }? ,id=xxx", InteSe rptu rep ter IE nvM teae rlu pt ar ri t ec io ts en r CV oir nt tu ra ol l M Rea cc eh ii vn ee r
"rules": [{
"type": "sheet_fuzzy",
"sheet_idx0": "RNSheet1",
"sheet_idx1": "ENSheet1",
} "rules": [ {"range": [ "A1:A8",... }]}] Rewardby executing eval scripts
Figure2: OverviewoftheOSWORLDenvironmentinfrastructure. Theenvironmentusesaconfigu-
rationfileforinitializingtasks(highlightedinred),agentinteraction,post-processinguponagent
completion (highlighted in orange), retrieving files and information (highlighted in yellow), and
executingtheevaluationfunction(highlightedingreen). Environmentscanruninparallelonasingle
hostmachineforlearningorevaluationpurposes. Headlessoperationissupported.
2.2.1 Overview
OSWORLD environment runs on the host machine. Its Coordinator accepts a configuration file
attheinitializationofacomputertask,runscommandstoautomaticallycreateavirtualmachine
instance,andinitializestherequiredstateforthetaskthroughtheTaskManager. Theconfiguration
filespecifiesthesnapshotofthevirtualmachinetobeused(whichstoresthecompletestateofa
computeratacertainmomentandcanberestoredtothisstateatanytime)andalsoindicatesthe
informationneededforsetup(suchasdownloadingfilesandopeningsomesoftware,makingsome
additionalsettings,etc.). Oncetheenvironmentissetup,agentsstarttointeractwiththeenvironment,
receivingobservationssuchasscreenshots,theaccessibility(a11y)tree,andcustomizedstreamssuch
asterminaloutputs. Agentssubsequentlygenerateexecutableactions(e.g.,.click(300, 540))
that manipulate the keyboard and mouse. Each action of the agent is input into the environment
asacodestring,andtheenvironment’sSimulatorexecutestheminthevirtualmachine. Afterthe
completionofatask,theTaskManagerperformspost-processing(suchasfilesaving,orreopening
certainapps)accordingtothetask’spost-config,retrievesdatatothehostmachine(fetchingimages
orconfigurationfilesfromthevirtualmachineorcloud,etc.),andthenrunsevaluationscriptsto
assessthecompletionofthetask. Multiplevirtualmachinescanrunsimultaneouslyonasinglehost
machine,therebyparallelizingtrainingandevaluation.
2.2.2 InitialTaskEnvironmentSetup
Manyreal-worldscenariosrequiringassistanceoccurnotatthebeginningofdigitalactivities,such
as right after launching an application or when a computer has just been started, but rather at
intermediatestages,suchaswhencertainsoftwareisalreadyopenorthecomputerhasexperienceda
crash. Therefore,weaimtosimulatetheseintermediatestatesascloselyaspossibletoreplicatereal-
worldscenarios. Thenaturalnesswebringinalsoleadstomorechallengesforagentstomodeland
explore. Weadoptedahybridapproachforconfigurationinsteadofsolelyrelyingonexample-wise
snapshotsforrestorationsinceitwouldstoremuchunnecessaryhardwarestateinformation,resulting
ineachexamplerequiringgigabytesofspace. Theprocedureisdividedintothreestages: startthe
VMemulator,preparefiles(downloadthefilesorscriptsfromthecloud,etc. optional),andexecute
reprocessing commands (open files or tabs, change the window size, etc. optional). We provide
convenientAPIstoconfigureinitialconditionsandworldsettings,standardizingourtaskstomake
thisprocessuser-friendlyandeasilyextendableforscaling. FormoredetailsonsetupseeApp.B.5.
2.2.3 Execution-BasedEvaluation
Evaluatingthesuccessfulexecutionofgeneralcomputertaskspresentsasignificantchallenge,asthese
tasksdefyreductiontoauniformpatternormeasurementbyasinglemetric. Toensureathorough
assessment,wedesignexample-specificevaluationmetricsincludingpre-setup,post-processing,and
4
observations actionsTable1: Examplesofourannotatedevaluationscripts,whichinvolveretrievingdatafromconfigura-
tionfiles,theenvironment,andthecloud,andexecutingfunctionstoassessfunctionalcorrectnessand
obtainresults. Theexample-wiseevaluationfacilitatesthediversityoftasksandreliableevaluation
ofcomplex,real-world,open-endedtasks.
InitialState TaskInstruction EvaluationScript(Simplified)
Canyouhelpmecleanupmycom- cookie_data = get_cookie_data(env)
puterbygettingridofallthecook- rule = {"type":"domains",
iesthatAmazonmighthavesaved? "domains":[".amazon.com"]}
is_cookie_deleted(cookie_data, rule)
Rename“Sheet1”to“LARSRe- result = get_file(env)
sources”. Then make a copy of expected = get_file(cloud)
it. Placethecopybefore“Sheet rules = [{"type":"sheet_name"},
2”andrenameitbyappendinga {"type":"sheet_data",
suffix“(Backup)”,... "sheet_idx0":0,
"sheet_idx1":1}...]
compare_table(result, expected, rules)
I’ve drafted an e-mail reminder tree = get_a11y_tree(env)
forthosewhohaven’tpaidtuition. rules = [{"selectors":
Pleasehelpmetocheckouttheire- ["tool-bar[attr|id=MsgHeadersToolbar]
mailsfromthepaymentrecordand label[name=To]
addtothereceiverfield. [attr|class=\"address-pill\"]>
label[attr|class=\"pill-label\"]
[name*=\"fox@someuniversity.edu...]
check_a11y_tree(tree, rules)
dedicatedfunctions,tailoredtothesoftwareinuseandthetask’sspecificrequirements. Thisinvolves
interpreting the software’s internal files, utilizing specific packages, and preemptively setting up
scaffoldingbasedonthesoftware’spermissions(e.g.,openingremotedebuggingportsforChrome
andVLC,creatingextensionsforVSCode). Occasionally,thisprocessmayalsorequireassistance
fromreverseengineeringtools,suchasfordecryptingaccountinformationinThunderbird.
Asaresult,weconstructavastcollectionoffunctionsthatmakefinalwranglingandretrievefiles
anddatainformationofvaryingtypes,categories,andgranularitiesfromthecloudandsoftwarefrom
virtualmachinesaswellasevaluationfunctionscoveringdifferentaspectsandtheircombinations,
inputtingthisinformationasparameterstoassesstheoutcomes. Weshowsomeevaluationexamples
inTab.1. ,demonstratetheretrievalofcookiedatafromvirtualmachines,obtainingfilesfromboth
virtualmachinesandcloudservices,fetchingthecurrentruntimeinterface’saccessibilitytreefrom
thevirtualmachines,anddeterminingsuccessbasedonthisinformationwhetherAmazon’scookies
havebeendeleted, whetherthegeneratedtableisaccurate, andwhetherthecorrectinterfacehas
beenaccessed. Needtonotewhenthetypeoftaskhasreal-timecharacteristics(suchasthenumber
ofcitationsofsomeone’spaper,thecontentofblogs,etc.),weincludedynamicfunctions(suchas
crawlerscripts)insidegettertoobtainthereal-timevaluesatthemomentofevaluationandthenuse
themtocomparewiththeresultsobtainedbytheagentupontaskcompletion. SeemoreinApp.B.6.
2.3 ObservationSpace
The observation space in OSWORLD contains a complete screenshot of the desktop screen,
includingthemouse’spositionandshape,variousapplicationwindows,files,andfoldersthatare
opened in different sizes and orders, maintaining the same perception as a human. Also, to be
alignedwithpreviousagent-buildingwebandmobileresearch[29,26,9,62]thatprovideandsupport
theuseofthewebpage’sDOMandapp’sviewhierarchy,OSWORLDalsoprovidesXML-format
accessibility(a11y)tree(obtainedviaATSPI2onUbuntu,viaPyWinAutoonWindows,etc.),which
cansupportadditionalinformationformodeling. Theserawobservationsallowrichinteractions
betweenmultipleapplicationsbutinducechallengesinlong-horizondecision-makingfromhigh-
resolutionimages(e.g.,4kscreenshots)andstructuredlongtext(e.g.,accessibilitytrees). Formore
detailedinformationonobservationspace,refertoApp.A.2.
2.4 ActionSpace
2https://docs.gtk.org/atspi2/
5Table2: Someexamplesofthemouseandkeyboardac-
Action space A in OSWORLD encom-
passesallmouseandkeyboardactions,
tions A in OSWORLD. See App. A.3 for the complete
list.
including movement, clicks (left-key,
right-key, multiple clicks), dragging,
Function Description
keystrokes,hotkeys,andothers,covering
moveTo(x, y) Movesthemousetothespecifiedcoordinates.
allhuman-computeractionspace. Some click(x, y) Clicksatthespecifiedcoordinates.
actionexamplesareshowninTab.2and write(‘text’) Typesthespecifiedtextatthecurrentcursorlocation.
press(‘enter’) PressestheEnterkey.
the complete action list can be found
hotkey(‘ctrl’, ‘c’) PerformstheCtrl+Chotkeycombination(copy).
in Appendix A.3. We use the widely scroll(200) Scrollsupby200units.
usedmouseandkeyboardcontrollibrary scroll(-200) Scrollsdownby200units.
dragTo(x, y) Dragsthemousetothespecifiedcoordinates.
pyautogui3 forouractionspace. This keyDown(‘shift’) HoldsdowntheShiftkey.
libraryleveragesthehigh-levelprogram- keyUp(‘shift’) ReleasestheShiftkey.
WAIT Agentdecidesitshouldwait.
ming language Python to replicate and FAIL Agentdecidesthetaskisinfeasible.
replay various human inputs into com- DONE Agentdecidesthetaskisfinished.
putersthroughcode,allowingustocon-
structauniversalandcompleterepresentationofactions. Theagentmustgeneratesyntax-correct
pyautoguiPythoncodetopredictvalidactions. Basicactions,suchaspressandmoveTo,canbe
integratedwithinprogramstructures,suchasfor-loops,significantlyimprovingtheexpressiveness
ofanaction. Timingisalsocrucial,ashighlightedinpreviousstudiesonmobiledevices[46],as
wellastheabilitytodeterminewhetherataskisinfeasibleorcompleted. Therefore,weaddthree
specialactionsnamedWAIT,FAIL,andDONEtoenhancetheaforementionedactionspaces. Previous
effortstowardscreatingdomain-specificagents, suchasMiniWoB++[41,29], CC-Net[17], and
WebArena[62,21],havedefinedactionspacesthatincludeclicksandtyping,aswellassomeactions
speciallydesignedforwebbrowsing. However,theydonotmodelallpossibleactionsonacomputer,
leadingtolimitationswhenattemptingactionslikeright-clickingandclickingwiththectrlkeyheld
toselectitems. Thisimposesanupperboundonagentlearningcapabilities.
3 OSWORLD Benchmark
WeintroducetheOSWORLDbenchmark,whichencompasses369realcomputingtasksdefinedand
executedonUbuntu.Additionally,weprovideasetof43tasksforWindowsbuiltonOSWORLDenvi-
ronment4. Theenvironmentpreparation,annotationprocess,datastatistics,andhumanperformance
aredescribedinthissection.
3.1 OperatingSystemandSoftwareEnvironments
OSWORLD supports real operating systems, including Windows, macOS, and Ubuntu, for the
developmentofautomatedcomputeragents. Fordevelopmentpurposes,weofferanextensiveset
ofexamplesonUbuntuanditsopen-sourceapplications,leveragingtheiropen-sourcenatureand
moreaccessibleAPIsfortasksettingandevaluation. Wealsoprovideannotatedtestingexamples
for Windows, focusing on applications with similar functionalities. For the first time, our real
OS environments enable us to define all kinds of computer tasks, including those that involve
interacting with multiple applications (e.g., Chrome and file manager) and interfaces (GUIs and
CLIs). Consideringavailability,thestrengthoftheusercommunity,anddiversity,wemainlyfocus
oneightrepresentativeapplicationsaswellasthebasiconesthesystemprovides: Chromeforweb
browsing,VLCformediaplayback,Thunderbirdforemailmanagement,VSCodeasacodingIDE,
andLibreOffice(Calc,Writer,andImpress)forhandlingspreadsheets,documents,andpresentations
respectively,GIMPforimageediting,andotherbasicOSappsliketerminal,filemanager,image
viewer,andPDFviewer. Eachexampledrawnfromtheseapplicationsseparatelyorincombination
showcases distinct operational logic and necessitates skills including commonsense knowledge,
high-resolutionperception,masteryofsoftwareshortcuts,andtheprecisecontrollingofmouseand
keyboardmovements. Formoredetails,checkApp.B.1andB.2.
3.2 Tasks
Wecreateabenchmarksuiteof369real-worldcomputertasksonUbuntuenvironmentcollected
fromauthorsanddiversesourcessuchasforums,tutorials,guidelines,etc.,toshowthecapability
3https://pyautogui.readthedocs.io/en/latest/
4Duetocopyrightissues,theseWindowstasksrequirefurtheractivationbytheuser.
6foropen-endedtaskcreationwithinOSWORLD. Eachexampleiscarefullyannotatedwithanatural
languageinstruction,asetupconfigurationwithcorrespondingfilesandsetupactionsforinitialization
ofinitialstatesuponourprovidedVMimage,andamanuallycraftedevaluationscripttocheckif
thetaskissuccessfullyexecuted. Wealsoadapt43tasksfromtheUbuntusetforanalyticusage
on Windows. Overall, it takes 9 computer science students (all student authors) over 3 months,
consumingapproximately1800man-hours(650hoursonsingle-apptasks,750hoursonworkflow
tasksand400hoursfordouble-checking).
Taskinstructionsandscenarios Todrawthemostdiverseandclose-to-realityusagecases,weex-
ploreseveraltypesofresources,includingofficialguidelines&tutorials,videopiecesgivingtipsand
tutorialsontheInternet(e.g.,TikTokandYouTube),how-towebsites(e.g.,WikiHow),Q&Aforums
(e.g.,Reddit,Quora,Superuser,&StackOverflow),formalvideocourses(e.g.,CourseraandUdemy),
andpublic-availablepersonalblogs&guidelines. Thedetailedresourcesusedinourbenchmarkare
listedinApp.B.3. Theexamplesareselectedbyjudgingtheirpopularity,helpfulness,anddiversity,
revealedbytheviewsandvotes. Meanwhile,wenoticethatitischallengingtofindenoughexamples
ontheinternetfortasksthatinvolvethecollaborationofmultiplesoftwareapplications.Therefore,the
authorsconductedextensivebrainstorming,combiningsomeexistingexamplesordrawinginspiration
fromdaily-lifescenarios,tocompilethetasks. Theinstructionsandtask-relatedfilesarethencrafted
fromthesereal-worldguidelinesandquestionsbytheauthors. Aftertheselection,eachexample
willbecross-checkedbytheothertwoauthorsonthefeasibility,ambiguity,andalignmentwiththe
source. Wenotonlycollecttasksthatcanbefinished,butalsocollecttheinfeasibleonesthatare
inherentlyimpossibletobecompletedduetodeprecatedfeaturesorhallucinatedfeaturesraisedby
realusers,whichresultsin30infeasibleexamplesinourbenchmark. Additionally,todemonstrate
theunificationabilityofOSWORLDenvironmentforthecreationofopen-endedcomputertasks,we
alsointegrate84examplesfromotherbenchmarksfocusingonsingle-applicationordomain-specific
environmentssuchasNL2Bash[28],Mind2Web[9],SheetCopilot[24],PPTC[13],andGAIA[34].
RefertoApp.B.4formoredetailsandB.8forsampledexamplesfortheshowcase. Atotalofabout
400man-hourswerespenttocollecttheseexamples.
Initialstatesetupconfigs Toconstructtheinitialstate,wepreparethefilesrequiredforthetask
andsetuptheinitialstate. Forthefiles,wetrytoobtainthemfromthesourcesofthetaskswefound,
or,incaseswherethefilesarenotpubliclyavailable,werecreatethemasrealisticallyaspossible
basedonscenarios. Fortheinitialstatesetup,wealsodevelopedsomefunctionsbasedontheAPIs
ofsoftwareandOStocontroltheopeningandresizingofsoftwarewindowsandreimplementsome
functionsthataredifficulttoachievewithAPIsusingpyautogui. Fordifferenttasks,wewriteconfigs
tosetthefilesandinitialstepsinthevirtualmachineandverifythemintheenvironment.Forexample,
thesetupstage(highlightedinredcolor,keyedas“config”)inFigure2involvesdownloadingfiles
intothevirtualmachinetoprepareaclose-to-realityinitialenvironment,andthenopenthefileof
interestwiththecorrespondingapplication. Thesetupstepsforeachexampletakeabout1man-hour
toconstruct.
Execution-basedevaluation Foreachtask,weselecttheappropriategetterfunctions,evaluator
functions,andparameterstocomposetheconfigurationfile. Thegetterfunctionisusedtoextract
keycomponents(e.g.,themodifiedfile,thetextcontentsdisplayedinawindowelement)fromthe
finalstateoftheenvironment,andtheevaluatorfunctionassessessuccessbasedontheextractedkey
components. Ifafunctiondoesnotexist,wewillconstructitandaddittothefunctionlibraryofthe
environment. Aftercompletingeachevaluation,theannotatorconductsinitialtestswithself-designed
testcases. Then,inthehumanevaluationandexperimentrunningphases,eachexampleisfurther
scrutinizedanditerateduponbydifferentindividualsthreetimesfromtheperspectiveofalignment
with the instruction and correctness under different solutions. As a result, we implement nearly
sample-specificexecutableevaluationscripts,resultinginatotalof134uniqueevaluationfunctions
forassessingfunctionalcorrectness—significantlymorethanthepreviousbenchmarks. Theaverage
timespentondevelopingtheevaluationforanexampleanditsexaminationamountstoapproximately
2man-hoursfromgraduatestudents.
Qualitycontrol Onceannotationisfinished,eachexampleisattemptedbytwoauthorswhodid
notparticipateinannotatingthatspecificexample,actingasagentstocompletethetask. Thisprocess
evaluates the current example’s quality and provides feedback to the annotators (such as unclear
instructions or inability to complete the task, crashes in corner cases, serious instances of false
7positivesandnegatives,etc.),andinvolvesjointrevisionsandsupplements. Duringexperimentsfor
humanperformanceandbaselines,wefurtherfixedexamplesfoundtohaveissues,dedicatingover
400man-hoursforfourroundsofchecks. Furtherinvestmentoftimeandamoreredteamingcould
furtherreducefalsepositivesandnegatives,whichwewillleavetofuturework.
3.3 T T b acaa h tsb e ieD vl “ de aa S ttt 3 iu a oa : p s nkpS K s. dt ,a e ut tat y ehi s as k tst t osti ca ”c cots r oi ues plft d yi ec r ros s ignti hlon y tth rbO ee e sS W u trW s ii e cn dO td ioR ao nfwL t seD s .r-. ConfigI um raag tie
o
no D p 3o s .c 8. 7 e %.d 0itD %inoc g. s 4 1e .t 6ti 3n %gs
.
1 3.6% %S lide editing 8.7% O31 ff. i7 c% eSlide settings 4.1% Tab. formattin Pg r 3 o.8 c V% e Tis Ss esi eu rn m tag l ti ii7 n. nz0 ga at l si% o 1 2n . .9 41 %. %9%
Statistic Number Professional
OS6.5% Files 2.2%
Code assist 2.4%
T
- -
-
-
So
uM S
I In
nt pia
t
fnu
pe
el
gl
.g
att lia
tr
se-
aas
iA -
b
sk
tA
e
klps
edp
sp( pU
(W
Wb ou inrn
k
dt flu o)
wow
s)
1
23
80 6
36
41 8
09
(( (
(2(
42 7
81
2
37 2
.0
.
1. .
80
4 6
%%%
% %
)))
) ) File o Mp uls
ti
8 m. e1 di%
a
4.6 D%
ataW analyo
si2
s
8r
.7
9k %.f 4lo %w
M
isc.
5.7%Da
2
Sei
1
ttl iny
g.
s
1 5.7% %Info S quhA eo rE c
ypV
cm 4po
.i
i
1nd
a u
%gie
nl
to
2o o
.
p
7c
p
%so
s
n
2
1.t .4r 6o
%
%l 4.6%
InitialStates 302
Eval.Scripts 134
Figure3: DistributionoftaskinstructionsinOS-
WORLDbasedontheappdomainsandoperation
typestoshowcasethecontentintuitively.
Statistics Tofacilitatetheanalysisandcomprehensionoftheagent’scapabilities,weclusterthe
examplesintothesoftwarecategories. Specifically,thesecategoriesincludeOS,Office(LibreOffice
Calc, Impress, Writer), Daily (Chrome, VLC Player, Thunderbird), Professional (VS Code and
GIMP), and Workflow (tasks involving multiple apps). The main statistics of OSWORLD are
presentedinTab.3andFig.3,showcasingtheoutlineandabroadspectrumoftasks. Specifically,
OSWORLDcontainsatotalof369tasks(andanadditional43tasksonWindowsforanalysis),with
themajority(268tasksor72.6%)aimingatsingleapplicationfunctionalitiesandaremarkablesection
ofworkflow-relatedtasks(101tasksor27.4%). Thedataset’sdiversityisfurtheraffirmedbythe
inclusionoftasksconsideredinfeasible,totaling30tasksor8.1%ofthedataset. Additionally,atotal
of84tasks(22.8%)areintegratedfromrelateddatasets,highlightingthedataset’sapplicabilityin
universalmodeling. Remarkably,thedatasetincorporates302distinctinitialstatesand134different
evaluationscripts,underscoringthecomprehensiveapproachtowardsevaluatingthetasks’complexity
andrequirements. MorestatisticdetailsareavailableinApp.B.4.
Comparison with existing benchmarks OSWORLD is compared with a number of existing
benchmarksinTable4. OSWORLDtakeutilizesrawmouseandkeyboardactionsthatisuniversalto
thecomputerenvironment,ratherthanfocusingonspecificcomputerapplications(e.g.,abrowser[62,
9]),withmultimodalobservationincludingscreenshot(MultimodalSupportcolumn). Thisuniversal
actionspaceenablestheconstructedagentstohandlegeneraltasksinthedigitalworld.Ourexecutable
environmentallowsagentstofreelyexploreduringboththelearningandevaluationphases,rather
thanprovidingonlystaticdemonstrationstoevaluateanagent’spredictionofthenextstep(Executable
Env.column).Moreover,itdoesnotsolelyfocusoninteractionswithinasingleappbutalsoconsiders
interactionsacrossmultipleappsandtheoveralltask(Cross-Appcolumn). Unlikemanyevaluations
that offer the same evaluation script or a few scripts for a certain type of task, the OSWORLD
benchmarkprovidesexample-wise,execution-basedevaluationfortasks. Specifically,thetotalof134
uniqueexecution-basedevaluationfunctionsinourbenchmarkissignificantlymorethanprevious
work,demonstratingthecomplexity,diversity,andevaluationchallengesoftasksinourbenchmark
(#Exec.-basedEval. Func. column). Italsoallowustofreelychooseopen-endedtasksandscale
tonewenvironments,ratherthanstrugglingincraftingnewones. Constructingintermediateinitial
statesastasksetupincreasesrealismandposeschallengestotheagents’explorationcapabilities
(IntermediateInit. Statecolumn).
3.4 HumanPerformance
8Table 4: Comparison of different environments for benchmarking digital agents. The columns
indicate: whethertheyprovideacontrollableexecutableenvironment(Control. Exec. Env.), the
easeofaddingnewtasksinvolvingarbitraryapplicationsinopendomains(EnvironmentScalability),
supportformultimodalagentevaluation(MultimodalSupport),supportforandinclusionofcross-app
tasks(Cross-App),capabilitytostarttasksfromanintermediateinitialstate(IntermediateInit. State),
andthenumberofexecution-basedevaluationfunctions(#Exec.-basedEval. Func.).
Control. Environment Multimodal Cross- Intermediate #Exec.-based
Size Exec.Env.? Scalability? Support? App? Init.State? Eval.Func.
GAIA[34] 466 ✗ - ✗ ✗ ✗ 0
MIND2WEB[9] 2350 ✗ - ✓ ✗ ✓ 0
WEBLINX[32] 2337 ✗ - ✓ ✗ ✓ 0
PIXELHELP[26] 187 ✗ - ✓ ✗ ✗ 0
METAGUI[44] 1125 ✗ - ✓ ✗ ✗ 0
AITW[38] 30k ✗ - ✓ ✗ ✓ 0
OMNIACT[20] 9802 ✗ - ✓ ✗ ✓ 0
AGENTBENCH[31] 1091 Multi-isolated ✗ ✗ ✗ ✗ 7
INTERCODE[53] 1350 Code ✗ ✗ ✗ ✗ 3
MINIWOB++[29] 104 Web ✗ ✓ ✗ ✗ 104
WEBSHOP[54] 12k Web ✗ ✓ ✗ ✗ 1
WEBARENA[62] 812 Web ✗ ✓ ✗ ✗ 5
VWEBARENA[21] 910 Web ✗ ✓ ✗ ✗ 6
WIKIHOW[57] 150 Mobile ✗ ✓ ✗ ✗ 16
ASSISTGUI[12] 100 ✗ ✗ ✓ ✗ ✓ 2
OSWORLD 369 Computer ✓ ✓ ✓ ✓ 134
Weconducthumanevaluationsoneachexample
900 90
inourdataset,withannotatorsbeingcomputer Ours median: 111.94s
sciencemajorcollegestudentswhopossessba- 800 WebArena median: 35.38s
80
sicsoftwareusageskillsbuthavenotbeenex-
700
posed to the samples or software before. We
600 70
recordedthetimerequiredtocompleteeachex-
ampleandwhethertheircompletionoftheex- 500
60
ample was correct. For comparison, we also
400
sampled100examplesfromWebArena[62]un-
300 50
derthesameevaluationsetup.
200
Asillustrated,tasksfromourdatasetgenerally 40
requiredmoretimetocomplete,withamedian 100
completiontimeof111.94seconds(compared 0 30
WebArena Ours
to35.38secondsinWebArena),andasignificant
Figure4: Humanoperationtimeandaccuracyon
numberofexamplesdistributedat900seconds
OSWORLDandWebArena.
orevenmore. Intermsofaccuracy,thehuman
performanceonourtaskswasapproximately72.36%,significantlylowerthanthe88%observed
onthepurewebtaskdataset. Thesefindingshighlightthecomplexityandchallengeoftasksinour
dataset,whichdemandmoretimeandeffort. Theloweraccuracyratefurtherindicatesthatourtasks
requireahigherlevelofunderstandingandproficiency,underscoringtheneedforadvancedmodels
andtechniquestotacklethemeffectively.
4 BenchmarkingLLMandVLMAgentBaselines
Inthissection,wepresenttheimplementationdetailsandexperimentalsettingsforseveralstate-of-
the-artLLMandVLMagentbaselinesonOSWORLDbenchmark,aswellastheirperformance.
4.1 LLMandVLMAgentBaselines
Weadoptstate-of-the-artLLMandVLMfromopen-sourcerepresentativessuchasMixtral[18]and
CogAgent [16], and closed-sourceones fromGPT,Gemini, and Claudefamilieson OSWORLD,
to serve as the foundation of agent. We also explore methods such as the Set-of-Marks aided
approach[52,10],whichhasbeendemonstratedtoimprovespatialcapabilitiesforvisualreasoning.
OurpriorexperimentsfollowingVisualWebArena[21]adoptfew-shotprompting,whichinvolves
using (observation, action) pairs as few-shot examples and inputting the current observation to
9
)s(
emiT
noitarepO
namuH
)%(
ycaruccATable5: SuccessratesofbaselineLLMandVLMagentsonOSWORLD,groupedbytaskcategories:
OS,Office(LibreOfficeCalc,Impress,Writer),Daily(Chrome,VLCPlayer,Thunderbird),Profes-
sional(VSCodeandGIMP)andWorkflow(tasksinvolvingmultipleapps),forgaininginsightsfrom
interfacesandoperationlogic. SeeApp.C.1andC.5formoredetails.
SuccessRate(↑)
Inputs Model
OS Office Daily Profess. Workflow Overall
A11ytree Mixtral-8x7B 12.5% 1.01% 4.79% 6.12% 0.09% 2.98%
GPT-3.5 4.17% 4.43% 2.71% 0.00% 1.62% 2.69%
Gemini-Pro 4.17% 1.71% 3.99% 4.08% 0.63% 2.37%
GPT-4 20.83% 3.58% 25.64% 26.53% 2.97% 12.24%
Screenshot CogAgent 4.17% 0.85% 2.71% 0.00% 0.00% 1.11%
Gemini-ProV 8.33% 3.58% 6.55% 16.33% 2.08% 5.80%
GPT-4V 12.5% 1.86% 7.58% 4.08% 6.04% 5.26%
Claude-3-Opus 4.17% 1.87% 2.71% 2.04% 2.61% 2.42%
Screenshot CogAgent 4.17% 0.85% 2.71% 0.62% 0.09% 1.32%
+A11ytree Gemini-ProV 4.17% 4.43% 6.55% 0.00% 1.52% 3.48%
GPT-4V 16.66% 6.99% 24.50% 18.37% 4.64% 12.17%
Claude-3-Opus 12.5% 3.57% 5.27% 8.16% 1.00% 4.41%
Set-of-Mark CogAgent 4.17% 0.00% 2.71% 0.00% 0.53% 0.99%
Gemini-ProV 4.17% 1.01% 1.42% 0.00% 0.63% 1.06%
GPT-4V 8.33% 8.55% 22.84% 14.28% 6.57% 11.77%
Claude-3-Opus 12.5% 2.72% 14.24% 6.12% 4.49% 6.72%
HumanPerformance 75.00% 71.79% 70.51% 73.47% 73.27% 72.36%
generatetheaction,butthisresultedinpoorperformance(successrateof2.79%underpure-screenshot
setting). Weattributetheresulttoalackofhistoryencodingandchangeinthepromptingscheme.
Therefore,intheexperiments,weopttoutilizethecontextwindowbyprovidingthemostrecent
3observationsandactionsinchatmode, i.e., alternatingbetween“user”promptsand“assistant”
prompts, insteadofthe(observation, action)pairs. Weuseatemperatureof1.0andtop-pof0.9
andtruncatefromthebeginningoftheinputifstillexceedingthemaxtokenslimitrequiredbythe
models. ThepromptsusedintheexperimentsareprovidedinApp.C.1. Weheuristicallyrequest
theagentstocompletethetaskswithinamaxsteplimitof15,whichisenoughformosttasks. We
presentasummaryoftheresultsinTab.5andanalysisinSec.4.2. Weimplementthefollowingfour
typesofinputsettingsonLLMandVLM.
Accessibilitytree Weaimtoevaluatewhetherthecurrentadvancedtext-basedlanguagemodelscan
reasonandgroundthemselvesinthecontexttogeneratethecorrectaction. SincetheoriginalXML
formatofaccessibilitytreecontainsmillionsoftokens, causedbycountlesselements, redundant
attributes, and a mass of markups, we opt to filter out non-essential elements and attributes and
representtheelementsinamorecompacttab-separatedtableformat. Tobespecific,wefilterthe
elementsbytheirtag,visibility,availability,existenceoftextorimagecontents,etc. Thedetailed
filtering method is elaborated on in App. C.3. Only the tag, name, text, position, and size of the
remainingelementsarekeptandconcatenatedbytabcharacterintheinput. Astherawcoordinates
areprovidedwithintheaccessibilitytree,theLLMisrequiredtogrounditsactionpredictionsto
accuratecoordinates.
Screenshot This is the input format that is closest to what humans perceive. Without special
processing,therawscreenshotofthevirtualmachineisdirectlysenttotheVLM.TheVLMisto
understandthescreenshotandpredictcorrectactionswithprecisecoordinates. Therawresolution
ofthescreenissetto1920×1080. Inordertoinvestigatetheimpactofinputresolution,ablation
studiesarealsoconductedwithdifferentresolutionsbymanuallydownsamplingthescreenshot.
Screenshot+accessibilitytree Tocheckifacombinationwiththeaccessibilitytreecanimprove
thecapacityofVLMforspatialgrounding,wetakethissettingbyinputtingbothrawscreenshotsand
asimplifiedaccessibilitytree.
10Set-of-Marks Set-of-Marks(SoM)[52]isaneffectivemethodforenhancingthegroundingcapa-
bilitiesofVLMssuchasGPT-4V,bysegmentingtheinputimageintodifferentsectionsandmarking
themwithannotationslikealphanumerics,masks,orboxes. Weleveragetheinformationfromthe
filteredaccessibilitytreeandmarktheelementsonthescreenshotwithanumberedboundingbox.
FollowingVisualWebArena[21]andUFO[55],wefurthercombinetheannotatedscreenshotwith
thetextmetadatafromaccessibilitytree,includingtheindex,tag,name,andtextoftheelements5.
Insteadofpredictingprecisecoordinates,theVLMissupposedtospecifytheactionobjectbyits
numberindex,whichwillbemappedintoouractionspacebypost-processing. Ablationstudiesare
alsoconductedwithdifferentresolutionsforSoMsetting.
4.2 Results
LLMsandVLMsarestillfarfrombeingdigitalagentsonrealcomputers. Theresultsfrom
Table5showthatwhenonlyusingscreenshotsasinputandadoptingpyautoguiasthecodespace,
thesuccessrateofthemodelisonly5.26%to5.80%evenwiththestrongestVLMsGPT-4Vand
Gemini-Pro-vision. Meanwhile,themostadvancedbatchoflanguagemodels,whenusingthea11y
treeasinput,hasasuccessraterangingfrom2.37%to12.24%. Overall,thesefiguresofperformance
aresignificantlylowerthanthehuman-levelperformancewhichis72.36%overallforindividuals
notfamiliarwiththesoftware. ThesegapsindicatethatcurrentLLMsandVLMsmaystillhavea
significantgapfromhumansinperformance, necessitatingfurtherresearchinthisarea. Another
surprising finding is that although Claude-3 Opus is reported to be competitive with GPT-4V on
commonbenchmarks[2],itfallsfarbehindwhenusedasadigitalagentin OSWORLD. Wewill
presentaqualitativeanalysisandinferreasonsinSec.5.4.
Agentperformancehasmuchhighervariancethanhumanacrossdifferenttypesofcomputer
tasks. OSWORLDiscapableofsimulatingandevaluatingthevarioussoftwaretypesandcombina-
tionscenariosinvolvedinpeople’sdailylivesinanopen-endedmanner. Weobserveperformance
basedonsoftwaretypegroupingandfindthatagentsbasedonLLMsshowsignificantdifferences
across different subsets. As shown in Table 5, performance tends to be better in tasks oriented
towardsCLIinterfaces(suchasOS-typetasks)comparedtothosebasedonGUI(suchasOffice
tasksinvolvingclicksonspreadsheetinterfacesanddocumentprocessing). Moreover,thebiases
betweendifferentmodelsandsettingsareinconsistent,withgapsevenexceeding20%;anotherpoint
isthatperformanceonworkflow-typetasksinvolvingmultiplesoftwareisfarbelowthefiguresona
singlesoftware,generallybelow5%. However,humanperformanceisconsistentacrossthesetasks,
fluctuatingaround70%withoutexceedinga5%variance,formingasignificantcontrastwiththe
models. Thissuggeststhatthewayhumansunderstandandcompletetasksmaydiffersignificantly
fromthecurrentlogicandmethodsbasedonLLMsandVLMs.
A11y tree and SoM’s effectiveness varies by models. The a11y tree contains some attribute
information of visible elements, including window position and size, as well as some semantic
labelsofthewindow. TheperformancegapillustratedinTable5betweenGPT-4VandClaude-3
with additional a11y tree information and under a pure screenshot setup suggests that it still has
significantroomforimprovementinaccuratelyperceivingandreasoningGUIelements. Conclusions
arereversedforGemini-Pro.
WhileapplyingSoMsetting, thereisadeclineforGPT-4Vinperformancecomparedtodirectly
providing the model with screenshots and a11y tree inputs, which contradicts the widely shown
effectivenessofSoMinclassicimageunderstandingtasks[52],aswellasinapplicationareaslike
webagents[61,15]. Wespeculatethatthisisduetothetasksperformedwithinoperatingsystems
havinghigherresolutionandmuchmoreelements, (e.g., thecellsinaspreadtable), leadingtoa
significantamountofnoisethatcounteractstheauxiliaryroleofboundingboxes. Sometasksalso
requireadetailedoperationonthecoordinatelevel,whichcannotbemodeledbytheboundingbox
thatSoMmarks.
VLMagentswithscreenshot-onlysettingshowlowerperformance,butitshouldbetheultimate
configurationinthelongrun. Thesettingthatreliessolelyonscreenshotsexhibitsthelowest
performance, at only 5.26%, among all. Surprisingly, it still achieves a decent outcome when
5Thismetadataissimilartobutkindofdifferentfromthatprovidedinthesinglea11ytreesetting. Tobe
specific,thecoordinatesandsizearereplacedwithelementindex.
11managingworkflowtasks(involvingmultipleapplications)thatinvolvemultipleapplications.Despite
the performance, it is worth mentioning that this is the only configuration that does not require
additional information, such as an accessibility (a11y) tree, making it concise, intuitive, and in
alignmentwithhumanperceptionsincethea11ytreemaynotbewell-supportedacrossallsoftwareor
cannotbeobtainedundernoisyconditions(e.g.,whentheagentisrestrictedtoviewingthecomputer
through peripheral screens), and the massive amount of tokens contained in the a11y tree (even
justtheleafnodescanhavetensofthousandsoftokens)canalsoimposeanadditionalinference
burdenonthemodel. Futureworkonpurelyvision-basedagentscouldleadtostrongergeneralization
capabilitiesand,ultimately,thepotentialforintegrationwiththephysicalworldonalargerscale.
5 Analysis
In this section, we aim to delve into the factors influencing the performance of VLMs in digital
agenttasksandtheirunderlyingbehaviorallogic. Wewillinvestigatetheimpactoftaskattributes
(suchasdifficulty,feasibility,visualrequirement,andGUIcomplexity),inputmeasurements(such
as screenshot resolution, the influence of trajectory history, and the effect of UI layout), explore
whethertherearepatternsintheagent’sperformanceacrossdifferentoperatingsystems,andmakea
qualitativeanalysisintheaspectofmodels,methods,andhumans.Allexperiments,unlessspecifically
mentionedotherwise,areconductedusingGPT-4VundertheSet-of-Marksetting. Sometakeaways
fromtheanalysisare: 1)higherscreenshotresolutiontypicallyleadstoimprovedperformance;2)
encodingmorea11y(text)trajectoryhistorycanboostperformance,whilenotworkingforscreenshots
(image);3)currentVLMsarenotadeptatimage-basedtrajectoryhistorycontext;4)currentVLM
agentsarenotrobusttoUIlayoutandnoise; 5)theperformanceofVLMagentsacrossOSisin
strongcorrelation;6)VLMagentshavecommonerrortypeslikemouse-clickinginaccuracies,limited
domainknowledge,andmoretypesdiscussedinSec.5.4.
5.1 PerformancebyTaskDifficulty,FeasibilityandAppInvolved
Weanalyzethesuccessrateacrossseveraladditionalsubsetsoftasks,assummarizedinTab.6and
willbediscussedinthefollowingsections.
Taskdifficulty Wecategorizethetasksbasedonthe Table 6: Success rate (SR) of GPT-4V
timerequiredforhumancompletionintothreegroups: (SoM)acrossdifferenttypesoftasks.
0∼60s(Easy),60s∼180s(Medium),andgreaterthan
180seconds(Hard),asanindicatorofdifficulty.Across TaskSubset %ofTotal SR(↑)
these groups, the model’s success rate drops as the Easy 28.72% 16.78%
requiredtimeincreases,withtaskstakinglongerthan Medium 40.11% 13.12%
180secondsbecomingalmostimpossibletocomplete Hard 30.17% 4.59%
(consideringwehaveinfeasibleexamplesforagent’s
Infeasible 8.13% 16.67%
luckiness),whereashumanperformanceacrossthese
Feasible 91.87% 13.34%
threegroupsis84.91%,81.08%and49.57%,showing
Single-App 72.63% 13.74%
aslightdeclineofthesametrendbutnottotheextent
Multi-AppWorkflow 27.37% 6.57%
ofbeingunachievable.
Feasibility Wealsodividetasksintogroupsoftasksinfeasible(e.g.,deprecatedfeaturesorhallu-
cinatedfeatures)andtasksfeasible,whichrequirestheagentstohavetheabilitytojudgebasedon
theirownknowledgeandexplorationresults. AsshowninTab.6,weobservethatagentscurrently
performslightlybetterintermsofinfeasibility(16.67%to13.34%),butoverall,theyareatarelatively
lowlevel. Itisnoteworthythatwealsoobserveinsomemethodsandsettings(suchasunderthe
purescreenshotsettingwiththeGemini-Promodel),agentstendtoeasilyoutputFAILandrefuseto
continuetrying. Thissituationleadstosomefalsepositivesininfeasibletasks. Thefocusneedstobe
onimprovingoverallperformance.
Numberofappsinvolved Wealsoexaminedtheperformancebasedonwhetherthetaskinvolved
appssoftwareorwithinasingleapp. AsshowninTab.6,theaverageperformancefortasksinvolving
asingleappislow,at13.74%,butstillmorethandoublethe6.57%observedforsubsetsoftasks
involvingworkflowsacrossmultipleapps.Withinsingle-appscenarios,tasksinvolvingGUI-intensive
Office apps generally performed the worst, with subsets such as LibreOffice Calc often scoring
12zero(weshowmoredetailedresultsinApp.C.5). Thesefindingshighlighttheneedforimproved
collaborationcapabilitiesbetweensoftwareandenhancedproficiencyinspecificscenarios.
5.2 PerformancebyMultimodalObservationVariances
Higherscreenshotresolutiontypicallyleads
to improved performance Despite the sig-
GPT-4V SoM
nificantprogressindisplaytechnology(1080P,
20 GPT-4V Screenshot 2K, and 4K), most VLMs are still trained on
datafarbelowtheseresolutions. Weselectthe
screenshot-onlyinputandSoMsettingtotestthe
method’sperformanceunderdifferentscreenin-
10
putdown-samplingratios(i.e.,0.2,0.4,0.6and
0.8oftheoriginalresolution),toevaluatetheim-
pactofresolutionchangesonmodelrecognition
abilityandaccuracy. Theoutputcoordinatesof
0.2 0.4 0.6 0.8 1.0
themodelforthescreenshotsettingarestillex-
Ratio
pectedtoalignwiththeoriginalresolution(i.e.,
Figure 5: The effect of downsampling on the
1080P).Theeffectsofvaryinginputresolutions
screenshot on performance with down-sampling
onperformanceareshowninFigure5. Forin-
ratiosof0.2,0.4,0.6and0.8andrunonasubset
puts based on pure screenshots, it is observed
(10%)ofexamples.
thatanincreaseinresolutiondirectlycorrelates
withenhancedperformance. Thisissuemayarisefromthediscrepancybetweentheresolutionofthe
screenshotandthecoordinatesoftheoutput. However,thescenarioslightlydiffersonSoM.Interest-
ingly,areductioninresolutionto768×432(down-samplingratioof0.4)leadstoanimprovementin
theagent’sperformanceandfurtherdiminishingtheresolutionevenmoretoadown-samplingratio
of0.2resultsinanoticeabledeclineinperformance.
Longertext-basedtrajectoryhistorycontext
improves performance, unlike screenshot-
0.06
90th Percentile: 6343.60 onlyhistory,butposesefficiencychallenges
Themainexperimentrevealedthedecisiverole
0.04
ofthea11ytreeinperformancewithinthecur-
renttechnologicalcontext. Evenwhenweretain
0.02 keyattributeelementsbasedonheuristicrules
(keep nodes with tags of the document, item,
0.00 0 2000 4000 6000 8000 10000 12000 button,heading,label,etc.),LLMsstillrequire
Tokens asufficientlylargecontexttoprocessthisinfor-
Figure6: Thelengthdistributionofa11ytreeas mationeffectively. Tofurtherunderstandthis,
observationfromsampledtrajectories. we sample some a11y tree observations from
OSWORLD andconductedthestatisticalanal-
15 ysis, as shown in Figure 6. The analysis indi-
catesthatacontextlengthof6000isneededto
accommodateabout90%ofcasesforasingle
GPT-4V SoM
10 GPT-4V Screenshot observation. However,relyingsolelyoncurrent
observationsinherentlyleadstoagentsmaking
repeatederrors. Therefore,weincludecurrent
observationsaswellaspastNroundsofobser-
5
vationsandactionsintheconstructedprompts
(seeappendixformoredetails),toexplorethe
1 2 3 >3 impactonagentperformancewhenNissetto1,
HistoryTrajectoryLength
2,3,andallwhereweputasmuchcontextas
wecan. Theexperimentalresults(asshownin
Figure7: Theeffectoflengthofhistoryonperfor-
Figure7)showtheperformanceincreasewith
mancewiththehistoryencodinglengthof1,2,3,
morehistorycontextforSoM.Futureworkon
and>3andrunonasubset(10%)ofexamples.
constructingmodelswithenhancedcapabilities
forlongercontextsupportandunderstandingreasoning,improvingmodelefficiency,anddesigning
newagentarchitecturesforefficientmemorystoragewillhaveasignificantimpactondigitalagents.
13
)%(etaRsseccuS
)%(
ytisneD
ycneuqerF
)%(etaRsseccuSHowever,wealsonotethattheinclusionofadditionaltrajectoryhistorydoesnotenhanceperformance
underthepurescreenshotsetting. ThissuggeststhatcontemporaryadvancedVLMsmightnotbe
as adept at extracting robust contextual information from images as they are from textual data.
Strengtheningthiscapabilitytoharnessinformationfromimagesconstitutesanimportantavenuefor
futureenhancements.
VLMagentsstrugglewithperturbationofpositionand
sizeofapplicationwindowsandirrelevantinformation
50.79 WecontinuetoadopttheSoMsettingandsampleasubset
of 28 tasks that agents relatively well perform (with a
40
successrateof50.79%)inOSWORLD. Atthebeginning
36.5
ofeachtask, weintroducedisturbancestothewindows
by1)changingthepositionofthewindow;2)changing
25.39 thesizeofthewindowtotheminimal;3)openingsome
20 irrelevant software and maximizing them to clutter the
15.04
screen. Thisprocessgeneratesseveraltimesmoresamples
fromthesubsetoftaskstoobservetheirperformance. We
Original Position Size Clutter find current agents are not robust in handling all these
changes,whichleadstoaperformancedroptoover60%
Figure8: Declineinperformancedueto
to even 80%. Surprisingly, we find agents can switch
windowperturbations.
thewindowtoacertaindegreebutfailtomaximizethe
windowasanintermediatestepandarestuckonotherthings. Thissuggeststhatwhileagentspossess
somecapabilitytonavigatebetweenwindows, theylackacomprehensivestrategyformanaging
windowstateseffectively.
5.3 PerformanceacrossDifferentOperatingSystems
Anotherkeychallengeinbuildinguniversaldigitalagentsisensuringthattheseagentscanmaintain
efficientandconsistentperformanceacrossdifferentoperatingsystemenvironments. Thedifferences
betweenOSandtheirsoftwareecosystemscansignificantlyimpactanagent’sobservationandaction
spaces,leadingtoperformanceuncertainties. Here,weexploreandanalyzethecorrelationbetween
thesuccessofagentsincompletingtasksonWindowsaftermigratingfromUbuntuusingexamples
fromOSWORLD.
WeenhancethefunctionalityoftheOSWORLDenvi-
Table7:Comparisonofmodelperformance
ronmenttosupportsettingupinitialexperimentstates,
andcorrelationacrossoperatingsystems.
finalevaluations,andobtainingobservationssuchasthe
a11ytreeandscreenshotsinWindowsOS.Additionally,
OS SR(%) CorrelationCoefficient
wehavemadeexample-wisefine-tuningmodifications
totheexistingsubsetin OSWORLD formigrationto Ubuntu 4.88
0.7
Windows. WeconductevaluationsusingtheGPT-4V Windows 2.55
screenshot-onlymethodandpresentthecorrelationof
performanceacrossthetwooperatingsystems. AsshowninTab.7, themodel’sperformanceon
UbuntuandWindowsis4.88%and2.55%,respectively,withacorrelationcoefficientof0.7,despite
thedifferencesintheirobservationspaces. Thisimpliesthatinsightsandmethodologiesdeveloped
withintheOSWORLDframeworkcanbeeffectivelytransferredtoWindowsenvironmentswitha
highdegreeofreliability.
5.4 QualitativeAnalysis
Inthissectionwehighlightrepresentativeexamplesofsuccess,failure,andsurprisingoutcomes,
alongside a comparative study between GPT-4V and Claude-3 agents, to elucidate the unique
challengesandinsightsourenvironmentintroduces. SeeApp.Dformoredetails.
Successandfailurecases Wefindagents,particularlybasedonGPT-4V,cansuccessfullysolve
tasksthatinvolvecomplexproblem-solvingorcreativethinking,showcasingtheadvancedunder-
standingandprocessingcapabilitiesofthemodelalready. Onesuccessfultaskisshowninthefirst
rowofFigure9. Theagentisrequestedtoextractsubtitlefilesfromthevideostreamandsavethem
locally. Theagentfirstdividesthescreenintotwoparts,withtheVLCapplicationwindowonthe
14
)%(etaRsseccuSleftandtheterminalwindowopenontheright,andusestheffmpegcommandtwice. Thefirstuse
removesthesubtitlesembeddedintheoriginalvideo,andthesecondusesavestheextractedsubtitles
locally.
Task Instruction: I downloaded an episode of Friends to practice listening, but I don't know how to remove the subtitles. Please
help me remove the subtitles from the video and export it as "subtitles.srt" and store it in the same directory as the video.
Step1: pyautogui.click(activities_x, activities_y) Step 2: pyautogui.typewrite('terminal', interval=0.5) Step 3: pyautogui.click(focus_x, focus_y)
Step 4: pyautogui.typewrite('ffmpeg -i video.mp4 -map Step 5: pyautogui.typewrite('ffmpeg -i video.mp4 -c copy
Step 6: Done
0:s:0 subtitles.srt', interval=0.5) -sn no_subtitles_video.mp4', interval=0.5)
Figure9: Theagentsuccessfullyunderstoodthecomplextaskinstructions,extractedthesubtitlefile
fromthevideo,andgeneratedapurevideowithoutembeddedsubtitles.
Despitethesuccesses,therearenotablefailuresthathighlightthelimitationsofcurrentmodels. In
thetaskof“center-aligningthetitleofthedocument”(Fig.10line1),theagentfailstogroundthe
relativelysimplerequirementof“centeralignmentoftexts”,performingmanyuselessactionssuch
asselectingirrelevantwords,openingirrelevantmenus,etc.
Moreover,wefindthattheagentlackspriorknowledgeinusingsoftware,performingpoorlyinmany
specializedtasks(asshowninFig.16,withGIMP,LibreOfficeCalc,andChromeselected). Taking
GIMPasanexample,fortheinstruction“reducebrightness”theagentdoesnotknowwhichmenuin
thetoolbarisforbrightnessadjustmentandinsteadrandomlytriesuntilexhaustingthemaximum
numberofsteps.
CommonerrorsbyGPT-4Vagents Amongthe550failedexamplesfromdifferentsettingsin
oursample,morethan75%existmouseclickinaccuracies,whichisthemostcommonerror. The
agentfailstoclickthecorrectcoordinatesdespiteplanningdetailedandaccuratestepsintheircode
comments,indicatingstrongplanningbutweakexecutioncapabilities. Mouseclickinaccuracies
lead to two other frequent errors: repetitive clicks and environmental noise dilemma. Repetitive
clicks occur when the agent repeatedly misclicks, adjusts, and fails, consuming too many steps.
Environmentalnoisearisesfromclickingunintendedobjects,causingpop-ups,oropeningunrelated
applications. Due to a lack of prior knowledge about most professional software, it falls into a
mismatch dilemma between the actions taken and the current state, and don’t know how to get
backtonormal. Moreover,theagentlacksbasichuman-likecognitionofwebpages,suchasnot
closingpop-upsinreal-worldwebpagesorbeingattractedbyadvertisementcontent,whichaffects
itsoriginalcorrectjudgment. Failuresalsoarisefrommisinterpretationofinstructionsandvisual
oversight,highlightingtheneedforimprovementinlanguageandvisualprocessing. SeeApp.D.2
forthespecificexecutionprocess.
Discrepanciesintaskdifficultybetweenagentandhuman Weidentifynotabledisparitiesin
theperceiveddifficultyoftasksbetweenhumansandAIagents. Tasksthatareintuitivelysimple
forhumansoftenpresentsubstantialchallengestoagents,andconversely,tasksthathumansfind
demandingcanbemorestraightforwardforagentstoexecute. YoucanfindmoredetailsinFig.19
andApp.D.3.
Tasks where humans outperform agents These tasks mainly involve text-based and design-
relatedwork,suchas“boldthefontonthisslideandaddnotes”or“eraseallthehighlightedmarks
inthisdocument”(Fig.10Line2). SincetheInternetlackssuchfine-graineddataasthesoftware
15Task Instruction: help me center align the heading in LibreOffice.
Step 1: pyautogui.click(focux_x, focus_y) Step 2: pyautogui.moveto(coor_x, coor_y) Step 3: pyautogui.click(menu_x, menu_y) Step 4: Failed (Meaningless actions)
Task Instruction: erase all the highlighted marks in this document
Step 1: pyautogui.click(libreoffice_writer) Step 2: pyautogui.mouseDown() Step 3: pyautogui.hotkey('ctrl', 'a') Step 4: Failed ( Did not find the right entrance)
Task Instruction: use GIMP to cut out the 2s to 4s part of a video
Step 1: pyautogui.hotkey('ctrl', 'atl', 't') Step 2: pyautogui.click(focus_x, focus_y) Step 3: pyautogui.typewrite('ffmpeg -ss …', Step 4: Done, but doesn't follow the
interval=0.05) instruction
Figure10: Screenshotsofthethreeexamplesmentionedinthequalityanalysis. Thefirstlineisan
exampleofGPT-4Vfailingataverysimpletask,thesecondlineisoneexamplewhereagentsface
moredifficultythanhumans,andthethirdlineisoneexamplethatismoredifficultforhumansthan
foragents.
executionprocess,theagentalsolacksthecorrespondingtrainingprocess,soitsgroundingabilityis
notgoodenough.ThelackofunderstandingofGUIlogicalsocausespoorperformanceonoperations
likeselectingandscrolling.
Taskswhereagentsoutperformhumans Tasksthattheagentconsiderssimplebuthumansfind
difficultareconcentratedin“codesolvabilitytasks”,suchas“monitorthesystemCPUfor30sand
outputtheresults”and“forcecloseaprocess”. ThesetasksrequirelittleornoGUIinteractionand
canbecompletedbyexecutingcomplexcodesandinstructions. It’sworthnotingthatcompleting
throughcodesometimesmismatcheswithhumaninstructions. Inthetask"useGIMPtocutoutthe
2sto4spartofavideo,(Fig.10Line3)"theagentused“ffmpeg”commandtocompletethevideo
cropping,ignoringthe“useGIMP”requirementintheinstructions.
Surprisingly,wediscoveredthatagentsareaspronetoinefficiencyinmechanicallyrepetitivetasks,
suchascopying,pasting,andbatcheditingofExcelsheets,ashumans. Humansfrequentlycommit
carelesserrorsduringexecution. TheshortcomingsinagentsstemeitherfromtheabsenceofanAPI
orfrominsufficienttrainingdatarelatedtotheAPI,hinderingtheirabilitytoefficientlyprocesstasks
inbatches. Furthermore,sluggishresponsetimescancausetaskstoeithertimeoutorsurpassthe
maximumallowedsteps.
Comparative analysis: Claude-3 vs. GPT-4V Although Claude outperforms GPT-4 in many
benchmarkssuchasGSM8K,HumanEval,etc.,inourmainexperiment,wefindthatClaudehas
anaverageloweraccuracyratecomparedtoGPT-4Vby2.84%to7.76%. WefindthatClaudecan
providesatisfactoryhigh-levelsolutions,butitsgroundingabilitycontainshallucinationsindetail.
Forinstance,Claudewouldinterpretdouble-clickingafileasselectingitinsteadofopeningit,treat
columnBinLibreOfficeCalcsoftwareascolumnC,andentertextintheVSCodetextreplacement
boxwithoutclickingonglobalreplace. ThisshowsthatClaudecanalignwellwithhumanplanning
inproblem-solving,butlacksexcellentgroundingabilitywhenitcomestoexecution. Detailscanbe
seeninFig.20andApp.D.4.
6 RelatedWork
Benchmarksformultimodalagents Testingdigitalinteractionagentsmainlyspanscodingen-
vironments,webscenarios,andmobileapplications. Inthecodingdomain,severalworksprovide
16frameworksanddatasetsforevaluatingagentsacrossprogramminglanguagesandsoftwareengineer-
ingactivities[53,19,23,42]. Forwebbrowsing,platformshavebeendevelopedforagentstointeract
withwebinterfacesthroughkeyboardandmouseactions,alongsidedatasetsfocusingonopen-ended
webtasksandrealisticwebnavigation[41,29,54,9,62,21]. Mobiledeviceinteractionresearch
aimsatimprovingaccessibility,withsimulatorsformobileUIinteractionsandplatformsdedicatedto
InfoUItasks[26,44,47,46,38,57,49,56,48]. Further,environmentsconnectingtorealcomputers
anddatasetsforGUIgrounding,albeitwithoutinteractivecapability,haveemerged[12,8,36,20].
Comprehensive task evaluation across different aspects also sees innovations [31, 34]. Differing
frompreviousendeavorsfocusingonsingularenvironmentsorlackingexecutability, OSWORLD
integratesaninteractivesetupenablingagentstoengagewithoperatingsystemsopenly,supportedby
adiversearrayoftasksandpreciseevaluationscriptswithinafullycontrollablesetting,markingit
asacompetitivebenchmarkingrealismandreliability,aswellasanenvironmentforlearningand
evaluatinggeneral-purposedigitalagent(SeeTab.4forcomparison).
Vision-languagemodelsformultimodalagents ManyexistingworksonGUIinteractionutilize
someformofstructureddata(suchasHTML,accessibilitytrees,viewhierarchies)asagrounding
source[9,14,26,35,60,43,58,62]. However,sourcecodeoftentendstobeverbose,non-intuitive,
andfilledwithnoise. Inmanycases,itiseveninaccessibleorunavailableforuse,makingmulti-
modality or even vision-only perception a must. To take screenshots as input, there are already
specialized,optimizedmulti-modalmodelsavailablethataresuitedfortasksonweb[4,11,17,22,40]
andmobiledevices[16,59]. Additionally,general-purposefoundationmodels[5,25,30,63]also
demonstratesignificantpotentialformulti-modaldigitalagents. Thedevelopmentofprompt-based
methods [12, 15, 51, 61], as well as visual reasoning paradigms, have also further facilitated the
performanceofdigitalagentsinwebpages,mobileapps,anddesktop. Toinvestigatehowwelldo
currentmodelsandmethodsperformindigitalagenttasks,ourpaperevaluatestheresultsoftext-only,
vision-only,andmulti-modalinputaswellasacrossmultiplemethods,demonstratingthatexisting
multi-modalmodelsarefarfromcapablecomputeragents. Specifically, thereisampleroomfor
improvementinlong-horizonplanning,screenshotdetailsperception,pixelcoordinatelocating,and
worldknowledge.
7 ConclusionandFutureWork
Inconclusion,theintroductionofOSWORLDmarksasignificantstepforwardinthedevelopmentof
autonomousdigitalagents,addressingcriticalgapsinexistinginteractivelearningenvironments. By
providingarich,realisticsettingthatspansmultipleoperatingsystems,interfaces,andapplications,
OSWORLDnotonlybroadensthescopeoftasksdigitalagentscanperformbutalsoenhancestheir
potentialforreal-worldapplication. Despitethepromiseshownbyadvancementsinvision-language
models,evaluationswithinOSWORLDrevealnotablechallengesinagents’abilities,particularlyin
GUIunderstandingandoperationalknowledge,pointingtoessentialareasforfutureresearchand
development.
Weidentifyseveralpotentialdirectionsforcommunitydevelopmentandprogresstowardgeneral-
purposeagentsforcomputeroperation:
EnhancingVLMcapabilitiesforefficientandrobustGUIinteractions Forfoundationmodel
development,weneedtoboosttheefficiencyofourmodels,enablingthemtoprocessmuchlonger
contexts and perform inference computations efficiently, akin to the robotics community [6, 7]
to better handle real-world cases. Enhancements in VLMs’ GUI grounding capabilities that are
robusttoapplicationwindowschangesarealsosought,focusingontheaccurateunderstandingand
generationofpreciseactionsalignedwithgiveninstructions. Moreover,amplifyingVLMs’ability
tocomprehendcontextintheformofimagesisapivotalgoal,sinceitiscrucialtoenablehistory
encodingusingimagessothatwecanbuildmemoryandreflectionuponthat. Theseimprovements
mayrequiremoreeffortsintheupstreampre-trainingstage,downstreamfine-tuningstage,andeven
inthemodelstructureitself,aspointedoutinpreviouswork[9,16,32].
Advancingagentmethodologiesforexploration, memory, andreflection Thenext-levelap-
proachencompassesdesigningmoreeffectiveagentarchitecturesthataugmenttheagents’abilitiesto
exploreautonomouslyandsynthesizetheirfindings. Theagentsfacechallengesinleveraginglengthy
rawobservationandactionrecords.It’sfascinatingtoexplorenovelmethodsforencodingthishistory,
17incorporatingefficientmemoryandreflectionsolutionstocondensecontextualinformationandaid
theagentinextractingkeyinformation. Additionally,integratingknowledgegroundinginto(V)LLM
agentsthroughmemorymechanismsisapromisingavenueaswell.Moreover,practiceGUIassistants
alsorequirefeaturesofpersonalizationandcustomization. Thesefeaturesrelyontechniquessuch
asuserprofilingandretainingmemoriesfromlong-termuser-assistantinteractions. Additionally,
craftingprotocolsspecificallyfordigitalagentsoperatingwithinGUIandCLIinterfacesaimsat
facilitatingefficientactionsisalsoanessentialthingforthefeasibilityofgeneral-purposedigital
agentsinthemid-shortterm.
Addressingthesafetychallengesofagentsinrealisticenvironments Thesafetyofagentsis
a critical issue if applying a built agent in fully realistic environments, the developed universal
digitalagentcouldpotentiallybeusedtobypassCAPTCHAsystemsinthefuture,asnotedin[39].
However, due to the currently limited capabilities of agents, we have not observed any harmful
anddamagingbehaviorsduringourexperiments,anautomaticagenthastheopportunitytodamage
patentrights,abuseaccounts,attempttoexploitsoftwarevulnerabilitiestocreateviruses,orengage
inattacks. Currently,weadoptvirtualmachinestomakeitdifficultfordevelopingdigitalagents
tocauseirreversibledamagetoourhostmachines. However,therestilllacksareliablemetricto
assessthesafetyofanagentdevelopedinanisolatedenvironment. Thecurrentevaluationfunctions
mainlyfocusontheresultscloselyregardingthetaskinstructions,assessonlythecorrectnessoftask
completion,andpaylittleattentiontopotentialunnecessarydamagingactionsofagents. Owingto
thecomplexityofacompletecomputerenvironment,wedidn’tworkoutanefficientwaytodetect
thelatentsideeffectsoftheagent. Consequently,howtoassessandcontrolpotentialbehaviorsin
openandrealenvironmentsthroughenvironmentalconstraintsandagenttrainingisanimportant
furtherdirectionofresearch.
Expandingandrefiningdataandenvironmentsforagentdevelopment Intermsofdatasetsand
environments,wecanbroadenthescopetocovermorespecializeddomains,includingreal-sector
needsinhealthcare,education,industry,transportation,andpersonalizedrequirements. Effortscan
bemadetoensureourenvironment’sseamlessdeploymentacrossvarioushardwareandsoftware
settings. Thevarianceofa11ytreequalityacrossdifferentapplicationsisalsonoticed. Although
theproblemisnotremarkableintheapplicationscurrentlyincluded,thereisnoguaranteeofthat
theapplicationdevelopersobeythea11yconventionandofferclearandmeaningfuldescriptionsfor
GUIelements. Moreintelligentapproachestofilteringredundanta11ytreeelementsandhandling
latentlymissingelementsdeservecarefulinvestigationaswell. Wealsohighlightthenecessityofa
painlessdatacollectionmethod,allowingfortheeffortlessacquisitionofcomputeroperationdata
anditstransformationintoagentcapabilities.
Acknowledgements
WethankSidaWang,PeterShaw,ChenHenryWu,PengchengYin,ShunyuYao,XingHanLu,Siva
Reddy,RuoxiSun,ZhiyuanZeng,andLeiLifortheirhelpfulfeedbackonthiswork.
References
[1] Adept. ACT-1: TransformerforActions. https://www.adept.ai/act,2022.
[2] Anthropic. Introducingthenextgenerationofclaude. https://www.anthropic.com/news/
claude-3-family,2023. Accessed: 2024-03-26.
[3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www-
cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,
2024.
[4] GillesBaechler,SrinivasSunkara,MariaWang,FedirZubach,HassanMansoor,VincentEtter,
VictorCa˘rbune,JasonLin,JindongChen,andAbhanshuSharma. Screenai: Avision-language
modelforuiandinfographicsunderstanding. arXivpreprintarXiv:2402.04615,2024.
[5] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXivpreprintarXiv:2308.12966,2023.
18[6] AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,JosephDabis,ChelseaFinn,
KeerthanaGopalakrishnan,KarolHausman,AlexHerzog,JasmineHsu,etal. Rt-1: Robotics
transformerforreal-worldcontrolatscale. arXivpreprintarXiv:2212.06817,2022.
[7] AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoro-
manski,TianliDing,DannyDriess,AvinavaDubey,ChelseaFinn,etal. Rt-2: Vision-language-
actionmodelstransferwebknowledgetoroboticcontrol. arXivpreprintarXiv:2307.15818,
2023.
[8] KanzhiCheng,QiushiSun,YougangChu,FangzhiXu,YantaoLi,JianbingZhang,andZhiyong
Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint
arXiv:2401.10935,2024.
[9] XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamuelStevens,BoshiWang,HuanSun,and
YuSu. Mind2web: Towardsageneralistagentfortheweb. arXivpreprintarXiv:2306.06070,
2023.
[10] D.Dupont. GPT-4V-Act: GPT-4VariantforActiveLearning. GitHubrepository,2023. URL
https://github.com/ddupont808/GPT-4V-Act.
[11] HirokiFuruta,OfirNachum,Kuang-HueiLee,YutakaMatsuo,ShixiangShaneGu,andIzzeddin
Gur. Multimodalwebnavigationwithinstruction-finetunedfoundationmodels. arXivpreprint
arXiv:2305.11854,2023.
[12] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu,
WeichenZhang,PeiyiWang,XiangwuGuo,etal. Assistgui: Task-orienteddesktopgraphical
userinterfaceautomation. arXivpreprintarXiv:2312.13108,2023.
[13] YiduoGuo,ZekaiZhang,YaoboLiang,DongyanZhao,andDuanNan. Pptcbenchmark:Evalu-
atinglargelanguagemodelsforpowerpointtaskcompletion. arXivpreprintarXiv:2311.01767,
2023.
[14] IzzeddinGur,HirokiFuruta,AustinHuang,MustafaSafdari,YutakaMatsuo,DouglasEck,
andAleksandraFaust. Areal-worldwebagentwithplanning,longcontextunderstanding,and
programsynthesis. arXivpreprintarXiv:2307.12856,2023.
[15] HongliangHe,WenlinYao,KaixinMa,WenhaoYu,YongDai,HongmingZhang,Zhenzhong
Lan, andDongYu. Webvoyager: Buildinganend-to-endwebagentwithlargemultimodal
models. arXivpreprintarXiv:2401.13919,2024.
[16] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,
ZihanWang,YuxiaoDong,MingDing,etal. Cogagent:Avisuallanguagemodelforguiagents.
arXivpreprintarXiv:2312.08914,2023.
[17] Peter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia,
AlistairMuldal,JoshAbramson, PetkoGeorgiev,AdamSantoro, andTimothyLillicrap. A
data-drivenapproachforlearningtocontrolcomputers.InInternationalConferenceonMachine
Learning,pages9466–9482.PMLR,2022.
[18] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,Chris
Bamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,
etal. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
[19] CarlosEJimenez,JohnYang,AlexanderWettig,ShunyuYao,KexinPei,OfirPress,andKarthik
Narasimhan.Swe-bench:Canlanguagemodelsresolvereal-worldgithubissues? arXivpreprint
arXiv:2310.06770,2023.
[20] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem
Alshikh,andRuslanSalakhutdinov.Omniact:Adatasetandbenchmarkforenablingmultimodal
generalistautonomousagentsfordesktopandweb. arXivpreprintarXiv:2402.17553,2024.
[21] JingYuKoh,RobertLo,LawrenceJang,VikramDuvvur,MingChongLim,Po-YuHuang,
Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena:
Evaluatingmultimodalagentsonrealisticvisualwebtasks. arXivpreprintarXiv:2401.13649,
2024.
19[22] KentonLee,MandarJoshi,IuliaRalucaTurc,HexiangHu,FangyuLiu,JulianMartinEisensch-
los,UrvashiKhandelwal,PeterShaw,Ming-WeiChang,andKristinaToutanova. Pix2struct:
Screenshotparsingaspretrainingforvisuallanguageunderstanding. InInternationalConfer-
enceonMachineLearning,pages18893–18912.PMLR,2023.
[23] BowenLi,WenhanWu,ZiweiTang,LinShi,JohnYang,JinyangLi,ShunyuYao,ChenQian,
Binyuan Hui, Qicheng Zhang, et al. Devbench: A comprehensive benchmark for software
development. arXivpreprintarXiv:2403.08604,2024.
[24] HongxinLi,JingranSu,YuntaoChen,QingLi,andZhaoxiangZhang. Sheetcopilot: Bring-
ing software productivity to the next level through large language models. arXiv preprint
arXiv:2305.19308,2023.
[25] LeiLi,ZhihuiXie,MukaiLi,ShunianChen,PeiyiWang,LiangChen,YazhengYang,Benyou
Wang,andLingpengKong. Silkie: Preferencedistillationforlargevisuallanguagemodels.
arXivpreprintarXiv:2312.10665,2023.
[26] YangLi,JiacongHe,XinZhou,YuanZhang,andJasonBaldridge. Mappingnaturallanguage
instructionstomobileuiactionsequences. arXivpreprintarXiv:2005.03776,2020.
[27] J.C.R.Licklider.Man-computersymbiosis.IRETransactionsonHumanFactorsinElectronics,
HFE-1(1):4–11,1960. doi: 10.1109/THFE2.1960.4503259.
[28] XiVictoriaLin,ChenglongWang,LukeZettlemoyer,andMichaelDErnst. Nl2bash: Acorpus
andsemanticparserfornaturallanguageinterfacetothelinuxoperatingsystem. arXivpreprint
arXiv:1802.08979,2018.
[29] EvanZheranLiu,KelvinGuu,PanupongPasupat,TianlinShi,andPercyLiang. Reinforcement
learningonwebinterfacesusingworkflow-guidedexploration.arXivpreprintarXiv:1802.08802,
2018.
[30] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. arXiv
preprintarXiv:2304.08485,2023.
[31] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang
Ding,KaiwenMen,KejuanYang,etal. Agentbench: Evaluatingllmsasagents. arXivpreprint
arXiv:2308.03688,2023.
[32] XingHanLù,ZdeneˇkKasner,andSivaReddy. Weblinx: Real-worldwebsitenavigationwith
multi-turndialogue. arXivpreprintarXiv:2402.05930,2024.
[33] ChangMa,JunleiZhang,ZhihaoZhu,ChengYang,YujiuYang,YaohuiJin,ZhenzhongLan,
LingpengKong,andJunxianHe. Agentboard: Ananalyticalevaluationboardofmulti-turnllm
agents. arXivpreprintarXiv:2401.13178,2024.
[34] GrégoireMialon,ClémentineFourrier,CraigSwift,ThomasWolf,YannLeCun,andThomas
Scialom. Gaia: abenchmarkforgeneralaiassistants. arXivpreprintarXiv:2311.12983,2023.
[35] ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,Christo-
pherHesse,ShantanuJain,VineetKosaraju,WilliamSaunders,etal. Webgpt: Browser-assisted
question-answeringwithhumanfeedback. arXivpreprintarXiv:2112.09332,2021.
[36] RunliangNiu,JindongLi,ShiqiWang,YaliFu,XiyuHu,XueyuanLeng,HeKong,YiChang,
andQiWang. Screenagent: Avisionlanguagemodel-drivencomputercontrolagent. arXiv
preprintarXiv:2402.07945,2024.
[37] ROpenAI. Gpt-4technicalreport.arxiv2303.08774. ViewinArticle,2:13,2023.
[38] ChristopherRawles,AliceLi,DanielRodriguez,OrianaRiva,andTimothyLillicrap. Android
inthewild: Alarge-scaledatasetforandroiddevicecontrol. arXivpreprintarXiv:2307.10088,
2023.
[39] Andrew Searles, Yoshimichi Nakatsuka, Ercan Ozturk, Andrew Paverd, Gene Tsudik, and
Ai Enkoji. An empirical study & evaluation of modern {CAPTCHAs}. In 32nd USENIX
SecuritySymposium(USENIXSecurity23),pages3081–3097,2023.
20[40] PeterShaw,MandarJoshi,JamesCohan,JonathanBerant,PanupongPasupat,HexiangHu,
UrvashiKhandelwal,KentonLee,andKristinaToutanova. Frompixelstouiactions: Learning
tofollowinstructionsviagraphicaluserinterfaces. arXivpreprintarXiv:2306.00245,2023.
[41] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of
bits: Anopen-domainplatformforweb-basedagents. InInternationalConferenceonMachine
Learning,pages3135–3144.PMLR,2017.
[42] ChengleiSi,YanzheZhang,ZhengyuanYang,RuiboLiu,andDiyiYang. Design2code: How
fararewefromautomatingfront-endengineering?,2024.
[43] AbishekSridhar,RobertLo,FrankFXu,HaoZhu,andShuyanZhou. Hierarchicalprompting
assistslargelanguagemodelonwebnavigation. arXivpreprintarXiv:2305.14257,2023.
[44] LiangtaiSun,XingyuChen,LuChen,TianleDai,ZichenZhu,andKaiYu. Meta-gui: Towards
multi-modalconversationalagentsonmobilegui. arXivpreprintarXiv:2205.11029,2022.
[45] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[46] DanielToyama,PhilippeHamel,AnitaGergely,GheorgheComanici,AmeliaGlaese,Zafarali
Ahmed,TylerJackson,ShiblMourad,andDoinaPrecup.Androidenv:Areinforcementlearning
platformforandroid. arXivpreprintarXiv:2105.13231,2021.
[47] SagarGubbiVenkatesh,ParthaTalukdar,andSriniNarayanan. Ugif: Uigroundedinstruction
following. arXivpreprintarXiv:2211.07615,2022.
[48] JunyangWang,HaiyangXu,JiaboYe,MingYan,WeizhouShen,JiZhang,FeiHuang,and
JitaoSang. Mobile-agent:Autonomousmulti-modalmobiledeviceagentwithvisualperception.
arXivpreprintarXiv:2401.16158,2024.
[49] HaoWen,YuanchunLi,GuohongLiu,ShanhuiZhao,TaoYu,TobyJia-JunLi,ShiqiJiang,
YunhaoLiu,YaqinZhang,andYunxinLiu. Empoweringllmtousesmartphoneforintelligent
taskautomation. arXivpreprintarXiv:2308.15272,2023.
[50] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing
Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan
Shin, Caiming Xiong, and Tao Yu. Openagents: An open platform for language agents in
the wild. CoRR, abs/2310.10634, 2023. doi: 10.48550/ARXIV.2310.10634. URL https:
//doi.org/10.48550/arXiv.2310.10634.
[51] AnYan,ZhengyuanYang,WanrongZhu,KevinLin,LinjieLi,JianfengWang,JianweiYang,
YiwuZhong,JulianMcAuley,JianfengGao,etal. Gpt-4vinwonderland: Largemultimodal
modelsforzero-shotsmartphoneguinavigation. arXivpreprintarXiv:2311.07562,2023.
[52] JianweiYang,HaoZhang,FengLi,XueyanZou,ChunyuanLi,andJianfengGao. Set-of-mark
promptingunleashesextraordinaryvisualgroundingingpt-4v.arXivpreprintarXiv:2310.11441,
2023.
[53] JohnYang,AksharaPrabhakar,KarthikNarasimhan,andShunyuYao. Intercode:Standardizing
andbenchmarkinginteractivecodingwithexecutionfeedback.arXivpreprintarXiv:2306.14898,
2023.
[54] ShunyuYao,HowardChen,JohnYang,andKarthikNarasimhan. Webshop: Towardsscalable
real-worldwebinteractionwithgroundedlanguageagents. AdvancesinNeuralInformation
ProcessingSystems,35:20744–20757,2022.
[55] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang,
QingweiLin,SaravanRajmohan,etal. Ufo: Aui-focusedagentforwindowsosinteraction.
arXivpreprintarXiv:2402.07939,2024.
[56] ChiZhang,ZhaoYang,JiaxuanLiu,YuchengHan,XinChen,ZebiaoHuang,BinFu,andGang
Yu. Appagent: Multimodal agents as smartphone users. arXiv e-prints, pages arXiv–2312,
2023.
21[57] Danyang Zhang, Lu Chen, and Kai Yu. Mobile-env: A universal platform for training and
evaluationofmobileinteraction. arXivpreprintarXiv:2305.08144,2023.
[58] DanyangZhang,LuChen,SituoZhang,HongshenXu,ZihanZhao,andKaiYu.Largelanguage
modelsaresemi-parametricreinforcementlearningagents. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[59] ZhuoshengZhangandAstonZhang. Youonlylookatscreens: Multimodalchain-of-action
agents. arXive-prints,pagesarXiv–2309,2023.
[60] ZihanZhao,LuChen,RuishengCao,HongshenXu,XingyuChen,andKaiYu.Tie:Topological
information enhanced structural reading comprehension on web pages. In Proceedings of
the 2022 Conference of the North American Chapter of the Association for Computational
Linguistics: HumanLanguageTechnologies,pages1808–1821,2022.
[61] BoyuanZheng,BoyuGou,JihyungKil,HuanSun,andYuSu. Gpt-4v(ision)isageneralist
webagent,ifgrounded. arXivpreprintarXiv:2401.01614,2024.
[62] ShuyanZhou,FrankFXu,HaoZhu,XuhuiZhou,RobertLo,AbishekSridhar,XianyiCheng,
YonatanBisk,DanielFried,UriAlon,etal. Webarena:Arealisticwebenvironmentforbuilding
autonomousagents. arXivpreprintarXiv:2307.13854,2023.
[63] DeyaoZhu, JunChen, XiaoqianShen, XiangLi, andMohamedElhoseiny. Minigpt-4: En-
hancingvision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprint
arXiv:2304.10592,2023.
22A Detailsof OSWORLD Environment
A.1 EnvironmentInfrastructure
AscomparedtocorecommonlyusedtechniqueslikeDocker6,virtualmachinescanoperatetheir
ownkernelandsystem,enablingcompatibilitywithawidevarietyofoperatingsystems(suchas
Windows,macOS,Linux,etc.) acrossdifferentCPUhardwaretypes(x64,ARM,etc.),andsupports
trainingandevaluationinamultiprocessmanneronbothheadlessserversandpersonalcomputers.
A.2 ObservationSpace
Weimplementthreekindsofobservation: completescreenshot,accessibilitytreeandterminal
output. Wealsoimplementavideorecorderoftheenvironmentbutdon’tputitintoourmodeling
duetotheagent’sabilitylimitations. OSWORLDsupportsobservationrefactoringandextendingif
needed,suchasgettingdatafromcertainopeningapplicationsthatwewanttofocuson.
A.2.1 Screenshot
Toalignwiththeperceptionofahumanuser,wecaptureascreenshotoftheentirecomputerscreen.
Includingthemousecursoralsoproveshelpfulincertaincaseswheremouseinformationiscrucial.
Forscreenresolution,wedefaultto1920×1080,asitisthemostcommonlyusedscreenresolution
accordingtoInternetUsersScreenResolutionRealtimeStatisticsfor20237. Thisresolutionalso
offersa16:9aspectratio. OSWORLDalsosupportsmodifyingtheresolutionofvirtualmachinesto
avoidpotentialmemorizationofabsolutepixelvaluesandtoassiststudiesontopicslikegeneralization
acrossdifferentresolutions.
A.2.2 AccessibilityTree
Anaccessibilitytree(ora11ytree,samelogictokubernetesandk8s),referstoanintricatestructure
generatedbythebrowserorOSaccessibilityAPIsthatrendersarepresentativemodeloftheweb
content,providingameansofinteractionforassistivetechnologies.Eachnodewithintheaccessibility
treehostsimportantinformationaboutaUIelement. Thiscouldrangefromthenatureoftheobject
(abutton,checkbox,orparagraphoftext),itscurrentstate(checkedorunchecked,forcheckboxes),
andevenitsspatialorientationonthescreen.
DifferentoperatingsystemsemployvariedaccessibilityAPIsandtoolstoconstructandmanipulate
the accessibility tree. These include Microsoft Active Accessibility (MSAA) and User Interface
Automation(UIA)forWindows,NSAccessibilityProtocolandmacOSAccessibilityInspectorfor
macOS,andAssistiveTechnologyServiceProviderInterface(ATSPI)8forGNOMEdekstopused
onUbuntu. WeadoptpyatspitogettheaccessibilitytreeonUbuntuandpywinautoonWindows.
WefurtherconvertitintoXMLformatformessagepassing. PartialpiecesoftheXMLformatted
accessibilitytreeareshowninFigure11.
ToolssuchasAccercisercanhelpvisualizethecorrespondingrelationshipoftreenodesandGUI
componentsintheaccessibilitytreeasshowninFigure12.
A.3 ActionSpace
Weimplementtwokindsofactionspace:pyautoguiandcomputer_13.Wemainlyusepyautogui
actionspace,sinceitsavestokensfordescribingactionspacedefinitioninprompting,comparedwith
self-designedactions.
A.3.1 PYAUTOGUI
pyautoguiisanopen-source,cross-platformPythonmoduleutilizedforprogrammaticallycontrol-
lingthemouseandkeyboard. Itcancontrolsimplemovements,clicks,aswellaskeyboardinputs,
andcanprovidescreencapturefeaturesorlocatewherethescreen-specificimagesresidethatcan
6https://www.docker.com/
7https://www.screenresolution.org/year-2023/
8https://docs.gtk.org/atspi2/
23Figure11: RawXMLformattedaccessibilitytreevisualization.
Figure 12: Use Accerciser to visualize components in accessibility tree. This example shows
thecorrespondingrelationshipbetweenthe“Print”button(whichisaprintericononthebarofthe
LibreOfficeappasshownontheleft)andthe“Print”treenode(whichisthe“Print”oftheAccerciser
AccessibilityExplorerasshownontheright).
beusefulforGUIautomation. ComparedwithotherPythonpackagessuchasxdotool9,mouse10
andkeyboard11,pyautoguihasbettercompatibilityacrossdifferentOSesasfoundinourearly
attempts,sothatweadaptthatasourcorecomponentofcomputercontrollingaswellasanofficial
validactionspace.
Herein,wewilldemonstratesomeusecasesofpyautoguiandillustratehowitcanbewieldedasan
actionspace.
MouseControllingFunctions
import pyautogui
# move mouse to XY coordinates over num_second seconds
pyautogui.moveTo(x, y, duration=num_seconds)
# drag mouse to XY
pyautogui.dragTo(x, y, duration=num_seconds)
9https://github.com/jordansissel/xdotool
10https://github.com/boppreh/mouse
11https://github.com/boppreh/keyboard
24pyautogui.click(x=moveToX, y=moveToY, clicks=num_of_clicks, button='left')
pyautogui.rightClick(x=moveToX, y=moveToY)
pyautogui.middleClick(x=moveToX, y=moveToY)
pyautogui.doubleClick(x=moveToX, y=moveToY)
pyautogui.tripleClick(x=moveToX, y=moveToY)
pyautogui.scroll(amount_to_scroll, x=moveToX, y=moveToY)
pyautogui.mouseDown(x=moveToX, y=moveToY, button='left')
pyautogui.mouseUp(x=moveToX, y=moveToY, button='left')
KeyboardControllingFunctions
import pyautogui
# useful for entering text, newline is Enter
pyautogui.typewrite('Hello world!\n', interval=secs_between_keys)
pyautogui.typewrite(['a', 'b', 'c', 'left', 'backspace', 'enter', 'f1'], interval=secs_between_keys)
pyautogui.hotkey('ctrl', 'c') # ctrl-c to copy
pyautogui.hotkey('ctrl', 'v') # ctrl-v to paste
pyautogui.keyDown(key_name)
pyautogui.keyUp(key_name)
pyautoguiasanActionSpace Giventhevariouscontrolsitprovides,pyautoguicanreadilybeusedas
anactionspaceinbuildingautomationsoftwareortestinginterfaceswithminoradjustments.Moreformally,an
actioniswithintheactionspacewhenitmeetsthesyntaxofpyautoguiorisoneofthreespecialactionsWAIT,
FAIL,andDONE.Thismightincludeactionslikeclickingatacertainlocation,enteringtextorkeysequences,
orevenrestingforaspan(Pause).Eachactioncouldbemappedtoanidentifyinglabelornumber,forminga
discreteactionspace.Forexample:
import pyautogui
def perform_action(action):
if action == 0:
pyautogui.moveTo(100, 100)
elif action == 1:
pyautogui.write('Hello world!', interval=0.25)
else:
pyautogui.pause(1)
Inthisscheme,the"perform_action"functionconstitutestheactionspace,whereeachuniqueactionisassociated
withauniqueinteger(itsactionID).ThefunctioninterpretstheseactionIDsandperformsthecorresponding
action,formingarudimentarydiscreteactionspace.
Oneinterestingfindingisthatlanguagemodelsgeneratescreenshotlocatefunctionslike:
pyautogui.locateOnScreen('Apple.png')
Whenthereisinsufficientgroundingevidence(suchaswhennoscreenshotisinputted,theaccessibilitytreelacks
aspecificelement,orthemultimodalmodelcannotcomprehendtheuserinterface),employingthisfunctionto
retrievethecorrecticonimagecouldpresentaninterestingmethod.
A.3.2 COMPUTER_13
Tofacilitatepotentialreinforcementlearningapplications,wehavecreatedavariantofpyautogui,which
wenamedcomputer_13. Inthisvariant,wewrappyautoguiintoafiniteactionclasswithparameterized
enumeration,suchthatitfeatures13actiontypes,excludingthreespecialonesfortaskprocesscontrol.Utilizing
thisstructuredapproachallowsmoreeffectivereinforcementlearningbyprovidingadistinctandfinitesetof
actionstobelearnedandoptimized.AssummarizedinTable8,eachactiontypehascertainparameters,detailed
inthecollection,confirmingthetype,range,andwhethereachparameterisoptionalforthataction.
25Table8: Actiontypesandparametersdefinedinactionspacecomputer_13,avariancewecreated
forthepotentialreinforcementlearningresearchbasedonourenvironment.
ActionType Parameters Note
MOVE_TO x,y Movethecursortothespecifiedposition
CLICK button, Clicktheleftbuttonifthebuttonnotspecified,otherwiseclick
x,y, thespecifiedbutton; clickatthecurrentpositionifxandy
num_clicks arenotspecified,otherwiseclickatthespecifiedposition
MOUSE_DOWN button Presstheleftbuttonifthebuttonnotspecified,otherwisepress
thespecifiedbutton
MOUSE_UP button Releasetheleftbuttonifthebuttonnotspecified,otherwise
releasethespecifiedbutton
RIGHT_CLICK x,y Rightclickatthecurrentpositionifxandyarenotspecified,
otherwiserightclickatthespecifiedposition
DOUBLE_CLICK x,y Doubleclickatthecurrentpositionifxandyarenotspecified,
otherwisedoubleclickatthespecifiedposition
DRAG_TO x,y Dragthecursortothespecifiedpositionwiththeleftbutton
pressed
SCROLL dx,dy Scrollthemousewheelupordown
TYPING text Typethespecifiedtext
PRESS key Pressthespecifiedkeyandreleaseit
KEY_DOWN key Pressthespecifiedkey
KEY_UP key Releasethespecifiedkey
HOTKEY keys Pressthespecifiedkeycombination
WAIT - Waituntilthenextaction
FAIL - Decidethetaskcannotbeperformed
DONE - Decidethetaskisdone
B Detailsof OSWORLD Benchmark
B.1 OperatingSystemSelection
Asof2023,themostpopulardesktopoperatingsystemsareWindows(69.5%),macOS(20.4%),ChromeOS
(3.2%),andLinux(3.1%)12. WhileWindowsandmacOSdominatethemarketshareandboasttherichest
softwareecosystems,theirclosed-sourcenatureraisespotentialcopyrightconcernsfordirectusage.ChromeOS,
beingaweb-basedoperatingsystem,heavilydependsonaGoogleaccountforitsfunctionalities,renderingit
lesssuitableforapublicbenchmark.
Incontrast,Linuxdesktopsofferawealthofopen-sourcesoftwareformostdailytasks,supportedbyanactive
communityforbothbasicandadvanceduse.EssentialapplicationssuchasOfficeSuite,browsers,emailclients,
multimediaapps,andthousandsofgamesandapplicationsareeitherpre-installedorreadilyavailablethroughthe
softwarecenterofthedistribution.Consequently,weselectUbuntu,themostrepresentativeLinuxdesktopOS,
asthefoundationforthemainpartofourbenchmarkintendedforpublicuse.Additionally,wehavedeveloped
componentstofacilitateagentinteractiononaWindowsvirtualmachineandhavecreatedarelativelysmallset
ofexamplesfocusingontheMicrosoftOfficesuite,includingExcel,PowerPoint,andWord.Thisservesasa
counterparttotheLibreOfficesuiteavailableonUbuntu.Thesecomponentscanbeutilizedin-houseorofficially
withthepurchaseofalicense. RegardingmacOS,theoretically,itisillegaltoinstallmacOSonnon-Apple
devices,whichleadsustorefrainfromdevelopingourbenchmarkonthisplatformtoavoidcopyrightissues.
B.2 SoftwareSelection
Duetothehighcostofobtainingoperationandevaluationscriptannotationdata,wehavechosenarepresentative
setofsoftwarefortheexamplesofUbuntupart.Weadoptstandardsthatconsider:1)Availability-thesoftware
mustbeavailableonUbuntu22.04;2)Open-source-thesoftwareshouldbeopen-sourcedwithanappropriate
license to prevent copyright issues; 3) Popularity - the software should take a high download number and
frequencyofrecommendationsinblogsandtutorials;4)Strongusercommunityandgoodsupportresources
-itispreferabletohaveanactiveandrobustusercommunityaswellasofficialdocuments,whichcanserve
asampleresourcesfortaskcollectionandagentlearning;5)Diversityofcategories-thesoftwareshouldbe
diversetoadequatelyrepresentandcoverawiderangeofreal-worldcases.
12https://www.statista.com/statistics/218089/global-market-share-of-windows-7/
26Asaresult,wehaveshortlistedthesoftwareintotwocategories: generalusageandprofessionalusage. For
general usage, we have VLC for media playback, Chrome for web browsing, and Thunderbird for email
management.Forprofessionalusage,wehaveVSCodeasacodingIDE,andLibreOffice(Calc,Writer,and
Impress)forhandlingspreadsheets,documents,andpresentationsrespectively,alongwithGIMPforimage
editing.Thisbringsourtotaltoeightdifferenttypesofsoftware.
B.3 TaskExampleSources
WedetailthetaskexamplesourcesinTable9.
Table9: TaskExampleResources
App Resources Link
UbuntuDocumentations https://help.ubuntu.com/
AskUbuntu https://askubuntu.com/
OS SuperUser https://superuser.com/
StackOverflow https://stackoverflow.com
YouTube https://www.youtube.com/
LibreOfficeHelp https://help.libreoffice.org/
MicrosoftTechCommunity https://techcommunity.microsoft.com/
libreofficehelp.com https://www.libreofficehelp.com/
Redditr/LibreOfficeCal https://www.reddit.com/r/LibreOfficeCalc/
Redditr/Excel https://www.reddit.com/r/Excel/
Calc
SuperUser https://superuser.com/
Medium https://medium.com/
Quora https://www.quora.com/
YouTube https://www.youtube.com/
RyanandDebi&TorenPersonalSite https://www.ryananddebi.com/
LibreOfficeHelp https://help.libreoffice.org/
LibreOfficeForum https://ask.libreoffice.org/
libreofficehelp.com https://www.libreofficehelp.com/
SuperUser https://superuser.com/
Writer StackOverflow https://stackoverflow.com
AskUbuntu https://askubuntu.com/
Quora https://www.quora.com/
YouTube https://www.youtube.com/
SearchStarPersonalSite https://seekstar.github.io/
LibreOfficeHelp https://help.libreoffice.org/
libreofficehelp.com https://www.libreofficehelp.com/
Redditr/LibreOffice https://www.reddit.com/r/LibreOffice/
SuperUser https://superuser.com/
Impress
StackOverflow https://stackoverflow.com
TechnicalTips https://technical-tips.com/
JustClickHere https://justclickhere.co.uk/
TikTok https://www.tiktok.com/
VLCDocumentation https://docs.videolan.me
VLCHelp.com https://www.vlchelp.com/
VideoLAN’sWiki https://wiki.videolan.org/
UbuntuDocumentations https://help.ubuntu.com/
VLC Redditr/Fedora https://www.reddit.com/r/Fedora/
SuperUser https://superuser.com/
Medium https://medium.com/
YouTube https://www.youtube.com/
Dedoimedo https://www.dedoimedo.com/index.html
ThunderbirdSupport https://support.mozilla.org/en-US/products/thunderbird
Redditr/Thunderbird https://www.reddit.com/r/Thunderbird/
Redditr/Automation https://www.reddit.com/r/automation/
SuperUser https://superuser.com/
Thunderbird
WikiHow https://www.wikihow.com/
Quora https://www.quora.com/
BitRecover https://www.bitrecover.com/
AdSigner https://www.adsigner.com/
Chrome GoogleChromeHelp https://support.google.com/chrome
Continuedonnextpage
27Table9–continuedfrompreviouspage
App Resources
Redditr/Chrome https://www.reddit.com/r/Chrome/
SuperUser https://superuser.com/
WikiHow https://www.wikihow.com/
in5steps.com https://in5stepstutorials.com/
How-ToGeek https://www.howtogeek.com/
Medium https://medium.com/
Quora https://www.quora.com/
YouTube https://www.youtube.com/
LaptopMag https://www.laptopmag.com
SuperUser https://superuser.com/
StackOverflow https://stackoverflow.com
VSCode Quora https://www.quora.com/
YouTube https://www.youtube.com/
CampbellMuscleLabGitHub https://campbell-muscle-lab.github.io/
Redditr/GIMP https://www.reddit.com/r/GIMP/
SuperUser https://superuser.com/
StackOverflow https://stackoverflow.com
GIMP
Quora https://www.quora.com/
Make-Use-Of https://www.makeuseof.com/
YouTube https://www.youtube.com/
UniPathMarketplace https://marketplace.uipath.com/
sync.blue https://www.sync.blue/
DeviceTests https://devicetests.com/
MakeTechEasier https://www.maketecheasier.com/
Unix&LinuxStackExchange https://unix.stackexchange.com/
GeeksforGeeks https://www.geeksforgeeks.org/
ILoveFreeSoftware https://www.ilovefreesoftware.com/
TheGeekDiary https://www.thegeekdiary.com/
Zyxware https://www.zyxware.com/
Workflow
GNOMEDiscourse https://discourse.gnome.org/
It’sFOSS https://itsfoss.com/
SuperUser https://superuser.com/
StackOverflow https://stackoverflow.com
LibreOfficeForum https://ask.libreoffice.org/
ImpressExtractNotes https://github.com/danielrcollins1/ImpressExtractNotes
Medium https://medium.com/
YouTube https://www.youtube.com/
KelvinSmithLibrary https://case.edu/library/
B.4 TaskExamplesCollection
HereweshowthedetailedstatisticsofOSWORLDbenchmark,includingthemainsetonUbuntu(369intotal)
andtheanalyticsetonWindows(43intotal).
Table 10: Detailed statistics of OSWORLD benchmark suite about examples number, average
instructiontokens,infeasibleinstructionsandintegratedinstructions.
OS Calc Impress Writer VLC Thunderbird Chrome VSCode GIMP Workflow Overall
Examples 24 47 47 23 17 15 46 23 26 101 369
Avg.Inst.Tokens 22.38 33.30 25.19 35.30 35.82 34.07 22.07 20.78 16.23 51.24 33.36
#Infeasible 5 1 0 1 3 1 3 5 10 1 30
#Integrated 7 19 30 0 0 0 26 0 0 2 84
Table11: DetailedstatisticsofWindowsanalyticsetbenchmarksuite. Thissetcontainsnoinfeasible
tasksandintegratedtasks.
Excel Word PPT Workflow Overall
Examples 11 9 7 16 43
Avg.Inst.Tokens 19.45 21.44 21.86 47.57 32.48
28Figure13: Comparisonofinstructionsdistribution. Alldatasetsaresampledto300tomakeafair
comparison. Thehyper-parametersoft-SNEarerandomlysampledforeachplot.
We also visualize the intent distribution (We obtain sentence embeddings for instructions using OpenAI’s
embeddingmodel,andthenapplyt-SNEtoreducethedimensionalitytotwodimensionsforvisualization.)and
compareitwithotherbenchmarkswhichalsofocusonthedigitalagent.Werandomlysample300examples
fromeachdatasetandrandomlychoosethreedifferenthyperparametersfort-SNE.Visualizationresultsare
showninFigure13. Fromthefigure,wecanobservethatthesemanticdistributionoftheinstructionsalone
hasreachedthemostcomprehensivelevel.Additionally,ourenvironmentremainscontrollableandexecutable,
offeringamorereliableevaluation.Itisalsonoticeablethattheclusteringcentersoftheotherthreeareclosely
positioned,whereasthepointsinourdistributionapproachesareinconsistentwiththeirs,indicatingthatwecan
serveasauniquechoiceforamorecomprehensiveassessmentofthecapabilitiesoffutureintelligentagents.
B.5 InitialStateSetupDetails
Thesetupoftheinitialstatecontainsthreestages:1)Startemulator.Thespecifiedvirtualmachineisactivated
and automatically reverted to the corresponding snapshot, which records the initial system settings of the
machine.2)Preparefiles(Optional).Thefileorsoftwarethatspecifiestheinitialstateofthetasktobeexecuted
isdownloadedtothevirtualmachineandopened. Thesystemisconfiguredtofirstdownloadthefilestothe
hostthroughadirectlinkandthenuploadthemtotheVMviaaLANconnection. Specifically,someinitial
filesaresetupforOS-relatedtasksbymanipulatingthefilesystemdirectlyfromthecommandline.3)Execute
reprocessingcommands(Optional). Fortasksthatrequireadditionalpreprocessing,task-specificoperations
areexecutedafterthecompletionofthefirsttwophases.Forexample,takingthecurrentlyopenLibreOffice
Impressfiletopagefive,clickinginthecenterofthescreentoreturntothemaininterface,etc. Weprovide
convenientAPIstoconfigureinitialconditionsandworldsettings,standardizingourtaskstomakethisprocess
user-friendlyandeasilyextendable.
B.6 EvaluationConfigurationDetails
Inthissection,wewillshowdetailsofpreparationsfortheevaluationoftheselectedapps(LibreOffice–Calc,
WriterandImpress,Thunderbird,VLCMediaPlayer,Chrome,VSCode,GIMP)andOS(UbuntuandWindows).
B.6.1 Ubuntu
LibreOffice: Calc,Writer,andImpress LibreOfficeisapopularopen-sourcefully-featuredofficesuite
forLinuxdesktops.Ourbenchmarkisbuiltuponversion7.3.7.2,theversionpre-installedinUbuntu22.04.Calc,
Writer,andImpressareselectedtobuildtasksonthem.Asthemajorityoftasksaretoconductalittlerevision
toafile,weevaluatethesetasksmainlybycheckingthefinalresultfile(inxlsx,docx,orpptxformat). The
checkcanbedonebycomparingtheresultfilewithagoldenreference,orinspectingsomeparticularattributes
orsettingsofthefile,e.g.,,pagestyle,freezing,andlocale.Usually,thexlsx,docx,andpptxfilesaremainly
accessedthroughopenpyxl13,python-docx14,andpython-pptx15.Forsomepropertiesnotsupportedbythe
currentlibraries,wealsolookthemupdirectlyviaparsingtheOfficeOpenXMLformat16.
Thunderbird Thunderbirdisapopularopen-sourcefully-featuredemailclientforLinuxdesktops.Version
115.6.0ofThunderbirdispre-installedinUbuntu22.04.Wecraftedanaccountprofiletosetupafeasibleinitial
state.EvaluationforThunderbirdconfigurationsismainlyperformedbyreadingvariousconfigurationsordata
13https://openpyxl.readthedocs.io/en/stable/
14https://github.com/python-openxml/python-docx
15https://github.com/scanny/python-pptx
16https://learn.microsoft.com/en-us/office/open-xml/about-the-open-xml-sdk
29filesintheprofilefolder.Anopen-sourcereverseengineeringtoolFirefoxDecrypt17isadoptedtodecryptthe
storedaccountinformationforevaluation.Theinvolvedaccountinformationisjustforexamplesandcontains
noinformationabouttherealperson.Besides,therearetasksinstructingtohelptocomposeanewemail.In
thesecases,theaccessibilitytreeisleveragedtoinspectthecontentsinthecomposingwindowbeforereally
sendingit.
VLCMediaPlayer VLCMediaPlayerisapopularopen-sourcecross-platformmultimediaplayerand
frameworkthatplaysmostmultimediafiles.TheevaluationforVLCMediaPlayerismultifold,rangingfrom
utilizingVLCHTTPinterface18,readingtheVLCconfigurationfile,comparingfinalresultfiles,andleveraging
accessibilitytreetoinspectthedesiredcontent.
Chrome GoogleChromeisoneofthemostpopularandpowerfulcross-platformwebbrowsersdeveloped
onGoogle’sfreeandopen-sourcesoftwareprojectChromium.TheevaluationofChromeismainlybasedon
theutilizationofPlaywright19,abrowserautomationlibrarytocontrolChromium,Firefox,andWebKitwith
asingleAPI.ToconnectPlaywrightrunningonhostmachinewithChromerunningonvirtualmachine,port
transferringtoolsocat20isleveraged.AdditionalinformationsuchastheHTMLsourcecodesofwebsitesisalso
leveragedintheevaluationofsometasks.
VSCode VSCodeisapopularopen-sourcemulti-functionalcross-platformeditorforsource-codeediting.
TheevaluationofVSCodetasksisprimarilydividedintotwodifferentcategories. Onesubsetoftasksis
predominantly oriented towards file manipulation. In the context of these tasks, a comparative analysis is
conductedbetweentheresultantfileandananticipatedreferencegoldfile.Anothersubsetoftasksiscentered
aroundhowtoutilizetheintrinsicfunctionalitiesoftheVSCodesoftwareitself,suchasmodifyingcolorthemes,
initiatingworkspacesessions,andmodifyingsettings.Intheseinstances,itbecomesimportanttoextractrelevant
internalinformationandconfigurationsfromtheVSCodeenvironment.
Inthecourseofthisresearch,weprincipallyleveragethecapabilitiesofferedbytheVSCodeExtensionAPI21
andinformationinthesettingsJSONfile22toobtaintherequisiteinternalsignalfortheevaluationprocess.Our
methodologyinvolvesthedevelopmentofacustomVSCodeextension,itsinstallationwithintheVSCode
softwaredeployedonourvirtualmachine,andthesubsequentinvocationoftheextension’scommandeachtime
anevaluationisrequired,aswellascheckingwhetherthesettingsJSONhasthecorrectvalueforaspecifickey.
GIMP GIMPisanopen-sourcerastergraphicseditorusedforimagemanipulation,editing,free-formdrawing,
formattranscoding,andmorespecializedtasks.TheevaluationforGIMPtasksisalsomainlydividedintotwo
differentcategories,justliketheVSCodeevaluation. Onetypeoftaskismainlyorientedtofileoperations.
Inthesetasks,theresultingfilesarecomparedandanalyzedwiththeexpectedreferencegoldenfiles,mainly
relyingonsomepowerfulimageprocessinglibrariessuchaspillow23. Anothercategoryoftasksrevolves
aroundtakingadvantageoftheinherentcapabilitiesoftheGIMPsoftwareitself.Intheseinstances,weprimarily
readGIMP’sconfigurationfilestoobtaininternalinformationtoevaluatethetasks.
B.7 Windows
Microsoft Office: Excel, Word, and PowerPoint MicrosoftOfficeisthemostpopularofficesuite
onWindowsdesktops.ThesethreecomponentssharethesamefunctionswiththecorrespondingLibreOffice
componentsbyandlarge.Theyareusedtoeditxlsx,docx,andpptxfiles,respectively.Thus,theevaluationfor
LibreOfficetaskscanbereusedforMicrosoftOfficetasks.
Thunderbird Thunderbirdisacross-platformemailclient.OnlythestructureofprofilefolderonWindows
issightlydifferentfromthatonLinux.Wethusrevisedtheaccountprofileandreuseittosetupthesameinitial
stateonWindows.
Chrome Chromeisacross-platformwebbrowser.ToevaluatetasksonChrome,onlytheporttransferring
toolneedstobereplacedwithNcat24.OtherconfigurationsandtheevaluationscanbesharedwithLinux-version
tasks.
17https://github.com/unode/firefox_decrypt
18https://wiki.videolan.org/Control_VLC_via_a_browser/
19https://playwright.dev/
20http://www.dest-unreach.org/socat/,https://linux.die.net/man/1/socat
21https://code.visualstudio.com/api
22https://code.visualstudio.com/docs/getstarted/settings#_settingsjson
23https://pypi.org/project/pillow/
24http://www.dest-unreach.org/socat/
30B.8 MoreTaskExamples
Inthissection,wecurateacollectionofexamplesfromvariousappsets,eachcharacterizedbydistinctoperational
logicandrequiringdifferentcapabilities.Theseexamplesarecarefullychosentoillustratethediversechallenges
andrequirementsencounteredwheninteractingwithdifferenttypesofapplications.
Table12: MoreExampleShowcasefromEachSubsetofDomains.
Related Abilities
Instruction(s) Screenshot
App(s) Needed
I want to install Spotify on knowledge
OS mycurrentsystem. Couldyou of OS; omit
pleasehelpme? distractions
massive
Check the names in column elements;
“Names with duplicates” and knowledge
Calc puttheuniqueonesincolumn tricks or
“UniqueNames”.Keeptheorigi- reasoning
nalorder. over long
actions
Ihavealookuptablefortheof-
ficers of each branch. Please, massive
here is another table in which elements;
Calc I need to fill with the officer knowledge of
namesaccordingtheheadoffice formulas and
(i.e.,thebranchname).Helpme functions
tocompletethis.
massive
Givenapartialcalendar,please
elements;
highlightalltheweekends(Sat-
commonsense
Calc urday & Sunday) by setting
reasoning;
the cell background as red
software
(#ff0000).
tricks
software
knowledge;
imagine
Iclosedtheslidepannelonthe
about UI
Impress left and idk how to get it back
layouts;
pleasehelp
overcome
typos in
instruction
Continuedonnextpage
31Table12–continuedfrompreviouspage
Related Abilities
TaskInstruction ScreenshotofInitialState
App(s) Needed
On it Whenever I launch a Li-
breOfficeImpress, itusesboth
screens, one for current slide
andnextslideandanotherfor
reason from
actual presentation. What I
unprofessional
Impress want is to use only one moni-
phenomenon
torwhichshowspresentation.I
expression
dontwantthescreenwithCur-
rentslideandNextslidesothat
itcanbeusedforotherpurposes.
HowshouldIachievethis?
locate the
position
Copythescreenshot1.pngfrom
of cursor;
Writer thedesktoptowheremycursor
switch from
islocated
desktop and
app
Can you help me clean up my
computer by getting rid of all
understanding
thetrackingthingsthatAmazon
the
Chrome might have saved? I want to
unprofessional
make sure my browsing is pri-
expression
vate and those sites don’t re-
memberme.
I am reading lecture note in
PDF while a music video is
running in VLC media player. understanding
But I find I need to switch to the
theplayereverytimeIneedto reference
pause/start.Couldyouhelpme from
VLC
changethesettingtoallowpaus- unprofessional
ing the video using keyboard expression;
shortcutwithoutminimizingthe software
PDF reader? I want to focus knowledge
onthelecturenoteanddon’tbe
disturbedbytheappswitching.
Hey, could you turn this video
software
therightwayupforme? And
knowledge;
onceit’sflippedaround,could
VLC spatial
yousaveitformewiththename
judgment
‘1984_Apple.mp4’onthemain
ability
screenwhereallmyfilesare?
Continuedonnextpage
32Table12–continuedfrompreviouspage
Related Abilities
TaskInstruction ScreenshotofInitialState
App(s) Needed
Create a local folder called
"Promotions"andcreateafilter
software
Thunderbird to auto move the inbox emails
knowledge
whose subject contains “dis-
count”tothenewfolder
Attach the my AWS bill to the file
email.Thebillisstoredat /aws- management;
Thunderbird
bill.pdf.Don’tcloseitorsendit. extra
Ihaven’tfinishallthecontents. requirement
deep-hided
I’ve got a bunch of email ac- feature,
countsinThunderbird,andit’s need to be
a hassle to check them one by explored
Thunderbird
one. Canyoushowmehowto even by
setupaunifiedinboxsoIcan human users;
seeallmyemailsinoneplace? pop-up
window
software
knowledge
to deal with
Please modify VS Code’s set- settings;
VSCode tingstodisableerrorreporting reasoning to
forPythonmissingimports. understand
the cause
and solution
of the error
software
knowledge
to deal with
Please help me install the Extensions;
VSCode autoDocstringextensioninVS reasoning to
Code. search and
install the
extension
successfully
Couldyoumakethebackground precise and
GIMP of this image transparent for intricate
me? operations
Continuedonnextpage
33Table12–continuedfrompreviouspage
Related Abilities
TaskInstruction ScreenshotofInitialState
App(s) Needed
spatial
perception
and
Helpmechoosetheyellowtrian-
reasoning,
GIMP gleandpositionitatthecenter
as well
ofmypicture.
as precise
control of
actions
software
knowledge
to undergo
Could you help me create an
sophisticated
AnimatedGIFfromavideofile
Multiple processes
usingVLCandGIMPfromthe
and ability
(VLC+GIMP)
source of video “src.mp4”, 5-
to process
secondclipbeginningat00:03?
multi-step
procedure
successfully
ability to
selectively
Helpmeexportcharts,graphor export
otherimagesfromdocxfilesre- charts,
Multiple ceivedinemail“LectureDocu- graphs and
ment” in Notes folder and up- images from
(ThunderBird+
load these png files to the fig- docx file;
Writer+Chrome) ures/folderinGoogleDrivefor software
lateruse(usenumberstoname knowledge
them). for google
drive file
upload
ability
Couldyouhelpmeextractdata to do
inthetablefromanewinvoice table data
Multiple
uploaded to my Google Drive, extraction;
(Chrome+Calc) then export it to a Libreoffice export data
calc.xlsxfileinthedesktop? to .xlsx
file
C DetailsofBaselineMethods
C.1 Hyper-ParameteroftheBaselineAgents
Weutilizetheversionsofgpt-3.5-turbo-16k,gpt-4-0125-preview,andgpt-4-vision-preview,re-
spectivelyforGPTresults,needtobenotedthatresultcouldbechangedfromtimesinceitisclose-sourced.
Wealsoemploythegemini-proandgemini-pro-visionversionsfortheGeminimodelsForalllanguage
models,wesetthetemperatureparameterto1.0,andtop_pto0.9,andthemaximumnumberoftokensfor
generationissetto1500.Wesetthemaximumstepsofinteractionto15andthemaximumtimelimitsto30
minutesforalltaskssincetheagentcouldleadtoastuckenvironmentundersomeunexpectedcases.
34C.2 PromptDetails
C.2.1 PromptforA11yTree,ScreenshotandtheirCombinationSetting
You are an agent which follow my instruction and perform desktop computer
tasks as instructed.
(cid:44)→
You have good knowledge of computer and good internet connection and assume
your code will run on a computer for controlling the mouse and keyboard.
(cid:44)→
For each step, you will get an observation of an image, which is the
screenshot of the computer screen and you will predict the action of the
(cid:44)→
computer based on the image.
(cid:44)→
You are required to use `pyautogui` to perform the action grounded to the
observation, but DONOT use the `pyautogui.locateCenterOnScreen` function
(cid:44)→
to locate the element you want to operate with since we have no image of
(cid:44)→
the element you want to operate with. DONOT USE `pyautogui.screenshot()`
(cid:44)→
to make screenshot.
(cid:44)→
Return one line or multiple lines of python code to perform the action each
time, be time efficient. When predicting multiple lines of code, make
(cid:44)→
some small sleep like `time.sleep(0.5);` interval so that the machine
(cid:44)→
could take; Each time you need to predict a complete code, no variables
(cid:44)→
or function can be shared from history
(cid:44)→
You need to to specify the coordinates of by yourself based on your
observation of current observation, but you should be careful to ensure
(cid:44)→
that the coordinates are correct.
(cid:44)→
You ONLY need to return the code inside a code block, like this:
```python
# your code here
```
Specially, it is also allowed to return the following special code:
When you think you have to wait for some time, return ```WAIT```;
When you think the task can not be done, return ```FAIL```, don't easily say
```FAIL```, try your best to do the task;
(cid:44)→
When you think the task is done, return ```DONE```.
My computer's password is 'password', feel free to use it when you need sudo
rights.
(cid:44)→
First give the current screenshot and previous things we did a short
reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER
(cid:44)→
EVER RETURN ME ANYTHING ELSE.
(cid:44)→
Fora11ytreesettinganda11ytree+screenshotsetting,thepromptsarebasicallythesame,justreplacethe
screenshotwordswitha11ytreewords.
C.2.2 PromptforSoMSetting
You are an agent which follow my instruction and perform desktop computer
tasks as instructed.
(cid:44)→
You have good knowledge of computer and good internet connection and assume
your code will run on a computer for controlling the mouse and keyboard.
(cid:44)→
For each step, you will get an observation of the desktop by 1) a screenshot
with interact-able elements marked with numerical tags; and 2)
(cid:44)→
accessibility tree, which is based on AT-SPI library. And you will
(cid:44)→
predict the action of the computer based on the image and text
(cid:44)→
information.
(cid:44)→
You are required to use `pyautogui` to perform the action grounded to the
observation, but DONOT use the `pyautogui.locateCenterOnScreen` function
(cid:44)→
to locate the element you want to operate with since we have no image of
(cid:44)→
the element you want to operate with. DONOT USE `pyautogui.screenshot()`
(cid:44)→
to make screenshot.
(cid:44)→
35You can replace x, y in the code with the tag of the element you want to
operate with. such as:
(cid:44)→
```python
pyautogui.moveTo(tag_3)
pyautogui.click(tag_2)
pyautogui.dragTo(tag_1, button='left')
```
When you think you can directly output precise x and y coordinates or there
is no tag on which you want to interact, you can also use them directly.
(cid:44)→
But you should be careful to ensure that the coordinates are correct.
Return one line or multiple lines of python code to perform the action each
time, be time efficient. When predicting multiple lines of code, make
(cid:44)→
some small sleep like `time.sleep(0.5);` interval so that the machine
(cid:44)→
could take; Each time you need to predict a complete code, no variables
(cid:44)→
or function can be shared from history
(cid:44)→
You need to to specify the coordinates of by yourself based on your
observation of current observation, but you should be careful to ensure
(cid:44)→
that the coordinates are correct.
(cid:44)→
You ONLY need to return the code inside a code block, like this:
```python
# your code here
```
Specially, it is also allowed to return the following special code:
When you think you have to wait for some time, return ```WAIT```;
When you think the task can not be done, return ```FAIL```, don't easily say
```FAIL```, try your best to do the task;
(cid:44)→
When you think the task is done, return ```DONE```.
My computer's password is 'password', feel free to use it when you need sudo
rights.
(cid:44)→
First give the current screenshot and previous things we did a short
reflection, then RETURN ME THE CODE OR SPECIAL CODE I ASKED FOR. NEVER
(cid:44)→
EVER RETURN ME ANYTHING ELSE.
(cid:44)→
C.3 AccessibilityTreeFiltering
Sincetheoriginaltreeislarge(usuallyover1milliontokensinXMLformat),wefiltertheaccessibilitytree
nodesbytheirtags,visibilities,availabilities,etc.TheconcreterulesareillustratedinthefollowingTable13.
Table13: Criteriaforkeepinga11ytreenodesonUbuntuandWindowsplatforms
Condition Ubuntu Windows
NodeTags document∗, ∗item, ∗button, SameasUbuntu
∗heading, ∗label, ∗scrollbar,
∗searchbox, ∗textbox, ∗link,
∗tabelement,∗textfield,∗textarea,
∗menu, alert, canvas, check-
box, combo-box, entry, icon,
image, paragraph, scroll-bar,
section, slider, static, table-cell,
terminal, text, netuiribbontab,
start, trayclockwclass, tray-
dummysearchcontrol, uiimage,
uiproperty,uiribboncommandbar
Showing True NotApplicable
Visible True True
Enabled or Editable or Expand- True True
ableorCheckable
HasNameorTextorImage True True
36Condition Ubuntu Windows
Position >=0 >=0
Size >0 >0
C.4 Set-of-MarkImplementationDetails
Our methodology involves an initial transformation of the original observational image acquired from our
environmentintothestandardizedSoM formatandbeputtedintoVLMstogetherwithatableofthemarks
withmetadatainformationsuchastagsandnames. Thisformatconsistsofboundingboxesthatboundthe
sub-imagesofinterest,eachassociatedwithacorrespondingintegermark.Notably,ourapproachincorporates
theutilizationoftheaccessibilitytreetoidentifytheboundingboxesassociatedwithallclickablebuttonswithin
thecurrentimageobservation,insteadofusingsegmentationmodelsliketheoriginalSoMpaper.Someexamples
ofscreenshotsafterapplyingSoMareshowninFigure14.Wecanobservetheemergenceofsomelow-quality,
unmodelabletasks,andevenmisleadingboundingboxes,dependingonthelevelofsupportfromthesoftware
ecosystem.ThiscouldbeanotherreasonforthepoorperformanceofSoM.Futureimprovementscouldbemade
inthisregard.
Figure14: ShowcaseofexamplescreenshotsmarkedbySoMacrossdifferentappGUI.
C.5 FullResultsofBaselineMethods
Here we show the break-down results of baseline methods from different LLMs and VLMs for follow-up
reference.
WehavealsocompiledthedistributionofstepstakenbytheGPT-4Vmodelunderourfoursettings:Accessibility
Tree(A11yTree), Screenshot, ScreenshotcombinedwithAccessibilityTree(Screenshot+A11yTree), and
Set-of-Mark. This data (as shown in Fig. 15) provides potential clues for future work. Overall, there are
observabledifferencesinhowmanystepstheagentchoosestoexecuteandwhenitdecidestoterminateunder
differentsettings.Moredetailedcontrolandanalysisofthesebehaviorscanbeexploredinsubsequentresearch
efforts.
D ExamplesofQualitativeAnalysis
Herewegivethespecificcodeexecutionprocessfortheexamplesinthequalitativeanalysisin§5.4andadd
moreerrorcauseanalysis,tryingtogetinsightsforfurtherimprovement.
37A11y Tree Screenshot
100
200
80
150
60
100
40
20 50
0 0
1 2 3 4 5 6 7 8 9 10 11 12 13 14>=15 1 2 3 4 5 6 7 8 9 10 11 12 13 14>=15
Screenshot+A11y Tree Set-of-Mark
120
200
100 175
150
80
125
60 100
40 75
50
20
25
0 0
1 2 3 4 5 6 7 8 9 10 11 12 13 14>=15 1 2 3 4 5 6 7 8 9 10 11 12 13 14>=15
Figure15: DistributionofstepstakenbytheGPT-4Vbasedagentsacrossfourdifferentsettings.
Table14: DetailedsuccessratesofbaselineLLMandVLMagentsonOSWORLD,dividedbyapps
(domains): OS,LibreOfficeCalc,LibreOfficeImpress,LibreOfficeWriter,Chrome,VLCPlayer,
Thunderbird,VSCode,GIMPandWorkflowwhichiscomprehensivewithmultipleapps,forgaining
insightsfrominterfacesandoperationlogics.
SuccessRate(↑)
Inputs Model
OS Calc ImpressWriter VLC TB ChromeVSC GIMP Workflow
A11y Mixtral-8x7B 12.50 0.00 0.39 4.34 10.22 6.67 2.17 8.69 3.85 0.10
GPT-3.5 4.17 2.13 6.77 4.35 6.53 0.00 2.17 0.00 0.00 1.62
Gemini-Pro 4.17 0.00 2.13 4.35 12.41 0.00 2.17 0.00 7.69 0.63
GPT-4 20.83 0.00 6.77 4.35 23.53 26.67 26.09 30.43 23.08 2.97
Screen CogAgent 4.17 0.00 0.00 4.34 6.53 0.00 2.17 0.00 0.00 0.00
Gemini-ProV 8.33 0.00 6.77 4.35 12.41 0.00 6.52 8.70 23.08 2.08
GPT-4V 12.50 0.00 2.52 4.35 18.34 0.00 6.52 0.00 7.69 6.04
Claude-3-Opus 4.17 0.00 2.52 4.34 6.53 0.00 2.17 0.00 3.84 2.61
Screen CogAgent 4.17 2.17 0.00 4.35 6.53 0.00 2.17 0.00 0.00 0.10
+A11y Gemini-ProV 4.17 2.13 6.77 4.35 18.30 0.00 4.35 0.00 0.00 1.52
GPT-4V 16.67 0.00 6.77 21.73 24.18 33.33 21.74 21.74 15.38 4.59
Claude-3-Opus 12.5 2.13 4.65 4.34 18.30 0.00 2.17 8.69 7.69 0.99
SoM CogAgent 4.17 2.17 0.00 4.34 6.53 0.00 2.17 0.00 0.00 0.00
Gemini-ProV 4.17 0.00 0.39 4.34 6.53 0.00 0.00 0.00 0.00 0.63
GPT-4V 8.33 8.51 6.38 13.04 10.66 33.33 23.91 17.39 11.54 6.57
Claude-3-Opus 12.5 2.13 0.39 8.70 6.53 13.33 17.39 0.00 11.54 4.49
HumanPerformance 75.00 61.70 80.85 73.91 70.59 46.67 78.26 73.91 73.08 73.27
38
tnuoC
tnuoC
tnuoC
tnuoCD.1 SuccessandFailureCases
SuccessTask: IdownloadedanepisodeofFriendstopracticelistening,butIdon’tknowhow
toremovethesubtitles. Pleasehelpmeremovethesubtitlesfromthevideoandexportitas
"subtitles.srt"andstoreitinthesamedirectoryasthevideo.
# Action 1
import pyautogui
import time
# Move the mouse to the top bar of the media player window to access the menu
pyautogui.moveTo(800, 10)
# Assuming the menu is at the top of the screen
pyautogui.click()
time.sleep(0.5)
# Action 2
# Move the mouse to the Activities corner to open the GNOME Activities Overview
pyautogui.moveTo(10, 10)
# Assuming the Activities corner is at the top-left
pyautogui.click()
# Type `terminal' to search for the Terminal application
pyautogui.write(`terminal')
time.sleep(0.5)
# Press Enter to open the Terminal application
pyautogui.press(`enter')
# Action 3
# Focus the Terminal window and type the command to extract subtitles
pyautogui.click(640, 360)
# Clicking in the center of the screen to focus the Terminal
time.sleep(0.5)
pyautogui.write(`ffmpeg -i video.mp4 -map 0:s:0 subtitles.srt')
pyautogui.press(`enter')
# Action 4
# Focus the Terminal window and type the command
to create a new video without subtitles
pyautogui.click(1100, 350)
# Clicking inside the Terminal window to focus it
pyautogui.write(`ffmpeg -i video.mp4 -c copy -sn no_subtitles_video.mp4')
Done!
FailureTask: HelpmecenteraligntheheadinginLibreOffice.
# Action 1
import pyautogui
import time
# Move the mouse to the LibreOffice Writer document icon and double-click
pyautogui.moveTo(1100, 720, duration=1)
pyautogui.doubleClick()
# Action 2
# Move the mouse to the beginning of the heading text
and click and drag to select the heading
pyautogui.moveTo(200, 230, duration=1)
pyautogui.mouseDown()
pyautogui.moveTo(800, 230, duration=1)
pyautogui.mouseUp()
# Move the mouse to the center align button in the toolbar and click
pyautogui.moveTo(400, 150, duration=1)
39pyautogui.click()
# Action 3
# Assuming the text is still selected,
move the mouse to the center align button in the toolbar and click
pyautogui.moveTo(400, 150, duration=1)
pyautogui.click()
# Action 4
# Move the mouse to the center align button in the toolbar and click
pyautogui.moveTo(340, 80, duration=1)
pyautogui.click()
Failed.
Task Instruction: tone down the brightness of the photo
Step 1: pyautogui.click(focus_x, focus_y) Step 2: pyautogui.click(color_x, color_y) Step 3: pyautogui.click(cancel_x, cancel_y) Step 4: pyautogui.click(x, y), Failed
Task Instruction: fill in the location of these meetings in the form
+
+
Step 1: pyautogui.click(focus_x, focus_y) Step 2: pyautogui.typewrite('Scottsdale', interval=0.5) Step 3: pyautogui.select(c11_x, c11_y) … Step 4: Failed
…
Task Instruction: make Bing the main search engine
Step 1: pyautogui.click(google_x, google_y) Step 2: pyautogui.click(settings_x, Step 3 :pyautogui.typewrite( Step 4: pyautogui.hotkey('tab', 'enter'), Failed
settings_y) 'chrome://settings/searchEngines')
Figure16: Thefigureshowsexamplesofusageerrorsinthreetypesofsoftware. Thefirstrowisfor
GIMP,thesecondrowisforLibreOfficeCalc,andthethirdrowisforGoogleChrome. Detailedcode
executionprocessesareprovidedbelow.
GIMPTask: Couldyoutonedownthebrightnessofmyphoto?
# Action 1
import pyautogui
import time
# Coordinates for the image icon on the desktop
# Adding half of the width and height to get the center
image_icon_x, image_icon_y = 1793 + 60, 920 + 17
# Double-click on the image icon
pyautogui.doubleClick(image_icon_x, image_icon_y)
# Action 2
# Coordinates for the `Colors' menu
# Adding half of the width and height to get the center
colors_menu_x, colors_menu_y = 344 + 28, 64 + 12
# Click on the `Colors' menu
pyautogui.click(colors_menu_x, colors_menu_y)
40# Action 3
# Coordinates for the `Brightness-Contrast...' option in the `Colors' dropdown
# Assuming each menu item is approximately 28 pixels apart
brightness_contrast_option_x, brightness_contrast_option_y
= 344 + 28, 64 + 12 + 28 * 6
# Click on the `Brightness-Contrast...' option
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
# Action 4
# Coordinates for the `Cancel' button in the `Shadows-Highlights' dialog
# Adding half the width and height to get the center
cancel_button_x, cancel_button_y = 375 + 42, 625 + 16
# Click the `Cancel' button
pyautogui.click(cancel_button_x, cancel_button_y)
# Coordinates for the `Brightness-Contrast...' option in the `Colors' dropdown
# Adjusted index for Brightness-Contrast option
brightness_contrast_option_x, brightness_contrast_option_y
= 344 + 28, 64 + 12 + 28 * 5
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
# Action 5
# Coordinates for the `Cancel' button in the `Exposure' dialog
# Adding half the width and height to get the center
cancel_button_x, cancel_button_y
= 375 + 42, 391 + 16
pyautogui.click(cancel_button_x, cancel_button_y)
# Coordinates for the `Brightness-Contrast...' option in the `Colors' dropdown
# Adjusted index for Brightness-Contrast option
brightness_contrast_option_x, brightness_contrast_option_y
= 344 + 28, 64 + 12 + 28 $*$ 6
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
# Action 6
# Coordinates for the `Cancel' button in the `Color Balance' dialog
# Adding half the width and height to get the center
cancel_button_x, cancel_button_y
= 375 + 42, 588 + 16
pyautogui.click(cancel_button_x, cancel_button_y)
# Since the `Brightness-Contrast...' option was not correctly opened before,
we will use an estimated position based on the dropdown menu.
# Assuming `Brightness-Contrast...' is the second option in the `Colors' menu.
brightness_contrast_option_x, brightness_contrast_option_y
= colors_menu_x, colors_menu_y + 28 * 2
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
# Action 7
# Coordinates for the `Cancel' button in the `Color Temperature' dialog
# Adding half the width and height to get the center
cancel_button_x, cancel_button_y = 375 + 42, 403 + 16
pyautogui.click(cancel_button_x, cancel_button_y)
# Since the `Brightness-Contrast...' option was not correctly opened before,
we will use an estimated position based on the dropdown menu.
# Assuming `Brightness-Contrast...' is the second option in the `Colors' menu.
brightness_contrast_option_x, brightness_contrast_option_y
= colors_menu_x, colors_menu_y + 28 * 2
# Click on the `Brightness-Contrast...' option
pyautogui.click(brightness_contrast_option_x, brightness_contrast_option_y)
# ... More Wrong Duplicate Actions; Can not find the right coordination.
# Failed
41LibreofficecalcTask: Inowwanttocountthemeetingcitiesofthethreemachinelearning
conferencesinthepasttenyearsfrom2013to2019(including2013and2019). Ihavelisted
thenamesandyearsoftheconferencesinexcel. Pleasefillinthevacantlocations.
# Action 1
# Move to cell C2 which is the first empty cell under `City' for ICLR 2013
pyautogui.click(319, 222)
# Coordinates of cell C2
time.sleep(0.5)
# Type the city name for ICLR 2013
pyautogui.write(`Scottsdale')
# Press Enter to move to the next cell
pyautogui.press('enter')
# Action 2
# Move to cell C3 which is the city cell for ICML 2013
pyautogui.click(319, 268)
# Coordinates of cell C3
# Type the city name for ICML 2013
pyautogui.write(`Atlanta')
# Press Enter to move to the next cell
pyautogui.press('enter')
# Action 3
# Move to cell C4 which is the city cell for NeurIPS 2013
pyautogui.click(319, 289)
# Coordinates of cell C4
# Type the city name for NeurIPS 2013
pyautogui.write(`Lake Tahoe')
# Press Enter to move to the next cell
pyautogui.press('enter')"
... More copy-and-paste operations
# Action 15
# Move to cell C16 which is the city cell for NeurIPS 2017
pyautogui.click(319, 541)
# Coordinates of cell C16
# Type the city name for NeurIPS 2017
pyautogui.write(`Long Beach')
# Press Enter to move to the next cell
pyautogui.press('enter')
Failed.
ChromeTask:CanyoumakeBingthemainsearchthingywhenIlookstuffupontheinternet?
# Action 1
pyautogui.click(x=1280, y=50)
# Approximate coordinates for the `More' button
pyautogui.move(0, 150)
# Moving the cursor down to `Settings' in the drop-down menu
pyautogui.click()
# Clicking on `Settings'
# Action 2
pyautogui.click(tag_31)
# Click on the `Finish update' button to
approximate the location of `More' button
pyautogui.move(-50, 0)
# Move left from the `Finish update' button to approximate the `More' button
42pyautogui.click()
# Click on the `More' button
time.sleep(0.5)
pyautogui.move(0, 150)
# Move down to the `Settings' option in the drop-down menu
pyautogui.click()
# Click on `Settings'
# Action 3
# Click on the `Customise Chrome' button which should be the `More' button
time.sleep(1)
# Wait for the dropdown menu to appear
# The `Settings' option is usually located towards the bottom of
the dropdown menu, so I'll move the cursor down to reach it
pyautogui.move(0, 200)
# Move down to the `Settings' option in the drop-down menu
pyautogui.click()
# Click on `Settings'
# Action 4
pyautogui.click(tag_47)
# Click on the `Close' button to close the `Customise Chrome' menu
pyautogui.click(tag_25)
# Click on the `New Tab' button to open a new tab
time.sleep(1)
# Wait for the new tab to open
pyautogui.typewrite(`chrome://settings/searchEngines')
# Type the direct URL to the search engine settings page
pyautogui.press(`enter')
# Action 5
# Simulate pressing the `Tab' key multiple times to navigate through the options
for _ in range(10):
# Assuming it might take around 10 tabs to reach the search engine list
pyautogui.press(`tab')
time.sleep(0.5)
# Wait for the focus to move to the next element
# Simulate pressing the `Enter' key to select an option,
which would be done once we reach the Bing option
pyautogui.press('enter')
# Action 6
pyautogui.click(tag_42)
# Click on the `Cancel' button to exit the edit mode for the '360' search engine
Failed.
43D.2 CommonErrorsbyGPT-4VAgents
Inthefollowingtwofigures(Figure18andFigure17),wetakethewebpagetasksasexamplestoshowthetwo
mostcommontypesoferrorsinGPT4-V:mouseclickinaccuraciesandinadequatehandlingofenvironmental
noise.
Task Instruction: On next Monday, look up a flight from Mumbai to Stockholm.
Figure17: Theerrorshowninthefigureisduetomouseclickinaccuracies. Theagentwassupposed
to click on the product category images of the Nike official store and follow the instructions to
searchforwomen’sjerseyspricedover60dollars. However,duetoacoordinatecalculationerror,it
mistakenlyclickedonthefavoritebutton,creatingasignificantdiscrepancybetweenthecurrentstate
andthetargetstate. Underthesecircumstances,theagentisunabletobacktracktothepreviousstate
andstartover.
Task Instruction: Browse the list of women's Nike jerseys over $60.
Figure18: Theerrorshowninthefigureisduetoinadequatehandlingofenvironmentalnoise. When
theagentencounterswebpageswithcookiesorpop-upads,itsunfamiliaritywiththestructureof
suchpagesleadstoattemptstointeractdirectlywiththewebpagewithoutbypassingthecookiesor
pop-ups. Sincethepop-upsorcookierequestsarenotclosed,allactionsareineffective.
44D.3 DiscrepanciesinTaskDifficultybetweenAgentandHuman
Task Instruction: bold the font on this slide and add note
Step 1: pyautogui.click(focus_x, focus_y) Step 2: pyautogui.click(bold_x, bold_y) Step 3: pyautogui.typewrite('Team Members') Step 4: pyautogui.click(bold_x, bold_y) , Failed
Task Instruction: monitor the system CPU for 30s and output the results
Step 1: pyautogui.click(terminal_x, terminal_y) Step 2: pyautogui.click(focus_x, focus_y) Step 3: pyautogui.typewrite('sar 1 30 > …') Step 4: Done
Task Instruction: force close a process
Step 1: pyautogui.hotkey('ctrl', 'alt', 't') Step 2: pyautogui.click(focus_x, focus_y) Step 3: pyautogui.typewrite('pkill soffice.bin') Step 4: Done
Figure19: Supplementaryexamplesoftasksperformedbyhumansandagents.
Human>AgentTask: eraseallthehighlightedmarksinthisdocument
# Action 1
import pyautogui
import time
# Move to the `Character Highlighting Color' button and click
pyautogui.moveTo(1020, 122)
pyautogui.click()
# Action 2
# Assuming the `No Color' option is the first option in the dropdown
# Move the mouse slightly below the `Character Highlighting Color' button
to click on `No Color'
pyautogui.moveTo(1020, 180)
time.sleep(0.5)
pyautogui.click()
# Action 3
# Move to the `Save' button and click
pyautogui.moveTo(184, 108)
time.sleep(0.5)
pyautogui.click()
# Failed.
Human>AgentTask: Onpage2,addanotesametothetitleintotheslide. Makethefontof
titlebold.
# Action 1
# Click on the vertical scrollbar to scroll down and reveal more slides
pyautogui.click(tag_43)
# Action 2
# Click on the second slide thumbnail to select it
45pyautogui.click(tag_2)
# Action 3
# Click on the text box to add text
pyautogui.click(tag_42)
pyautogui.typewrite(`Team Members', interval=0.1)
# Type the title
pyautogui.hotkey(`ctrl', `a')
# Select all text
pyautogui.hotkey(`ctrl', `b')
# Make the text bold
# Action 4
# Click on the Bold button to apply bold formatting
pyautogui.click(tag_68)
Failed.
Agent>HumanTask: useGIMPtocutoutthe2sto4spartofavideo
# Action 1
import pyautogui
import time
# Move the mouse to the assumed terminal icon position
at the bottom of the dock and click to open it
# Assuming the terminal icon is at the bottom of the dock
pyautogui.moveTo(70, 1350, duration=1)
pyautogui.click()
# Type the command to use ffmpeg to trim the video
ffmpeg_command = `ffmpeg -ss 00:00:02 -to 00:00:04 -i /home/user/fullvideo.mp4
-c copy /home/user/trimmedvideo.mp4'
pyautogui.write(ffmpeg_command, interval=0.05)
pyautogui.press('enter')
# Action 2
# Close the search dialog
pyautogui.press('esc')
# Open the terminal using a keyboard shortcut (Ctrl+Alt+T is a common shortcut
for opening a terminal in many Linux distributions)
pyautogui.hotkey('ctrl', 'alt', 't')
# Action 3
# Move the mouse to the terminal and click to focus on the terminal window
pyautogui.moveTo(260 + 722/2, 103 + 434/2, duration=1)
pyautogui.click()
# Type the command to use ffmpeg to trim the video
pyautogui.write(ffmpeg_command, interval=0.05)
pyautogui.press(`enter')
# Done, but not follow the demanding in instruction.
Agent>HumanTask: IwanttounderstandtheresourceusageofmyUbuntusystemunder
normalworkloads. Pleaseusethe‘sar‘commandinthe‘sysstat‘toolkittomonitorsystem
activity,evaluatethestatusonceeverysecondfor30seconds,outputtheresultsto"System
ResourcesReport.txt"underDesktop.
# Action 1
import pyautogui
import time
46# Press the keyboard shortcut to open the terminal
pyautogui.hotkey(`ctrl', `alt', `t')
time.sleep(1)
# Wait a moment for the terminal to open
# Action 2
# Click inside the terminal to focus on it
pyautogui.click(600, 300)
# Assuming the terminal is in the upper left area of the screen
n# Type the command to monitor system activity
pyautogui.typewrite(`sar 1 30 > ~/Desktop/System Resources Report.txt')
# Action 3
# Click on the `System Resources Report.txt' file to select it
pyautogui.click(1850, 940)
# Coordinates taken from the label position and size
time.sleep(0.5)
# Right-click to open the context menu
pyautogui.click(1850, 940, button=`right')
# Move to the `Open With Other Application' option in the context menu
pyautogui.move(0, 70)
# Assuming `Open With Other Application' is 70 pixels
down from the right-click position
# Click on `Open With Other Application'
pyautogui.click()
time.sleep(0.5)
# Type `gedit' to select the text editor
pyautogui.typewrite(`gedit')
# Press `Enter' to open the file with gedit
pyautogui.press(`enter')
# Action 4
# Click inside the terminal to ensure it's in focus
pyautogui.click(500, 200)
# Coordinates are estimated to be within the terminal window
# Type the command to open the file with gedit
pyautogui.typewrite(`gedit ~/Desktop/System Resources Report.txt')
Done!
Agent>HumanTask: Hey,myLibreOfficeWriterseemstohavefrozenandIcan’tgetitto
closenormally. Canyouhelpmeforcequittheapplicationfromthecommandline? I’mon
UbuntuandIdon’twanttorestartmycomputerorloseanyotherworkIhaveopen.
# Action 1
import pyautogui
import time
# Open a terminal window
pyautogui.hotkey(`ctrl', `alt', `t')
time.sleep(1)
# Type the command to kill all instances of LibreOffice Writer
pyautogui.typewrite(`killall soffice.bin')
# Action 2
# Open a terminal window
pyautogui.hotkey(`ctrl', `alt', `t')
time.sleep(1)
# Type the command to kill all instances of LibreOffice Writer
pyautogui.typewrite(`killall soffice.bin')
# Action 3
47# Ensure the terminal window is focused
pyautogui.click(500, 200)
# Assuming the terminal is at this position
# Type the pkill command to forcefully kill the LibreOffice Writer application
pyautogui.typewrite(`pkill -9 soffice.bin')
Done!
D.4 ComparativeAnalysis: Claude-3vs. GPT-4V
Task Instruction: find the path of the file named "secret.docx" and copy it to the clipboard.
Step 1: pyautogui.click(folder_x, folder_y) Step 2: pyautogui.click(grid_x, grid_y) Step 3: pyautogui.typewrite('secret.docx') Step 4: pyautogui.doubleClick(x, y), Failed
copy the data in Cell B6 in this Libreoffice Calc file and search it in the Chrome browser.
Step 1: pyautogui.click(C6_x, C6_y) Step 2: pyautogui.click(chrome_x, chrome_y) Step 3: pyautogui.click(C6_x’, C6_y’) … Step 4: pyautogui.typewrite('Dewitt'), Failed
change all the places in this document …that say \"text\" to \"test\".
Step 1: pyautogui.click(focus_x, focus_y) Step 2: pyautogui.hotkey('control', 'f') … Step 3: pyautogui.hotkey('control', 's') … Step 4: 'Wait', Failed
Figure20: ScreenshotsoferrorexampleanalysisofClaude-3Opus.
Claude3FailedTask1: Irememberthereisafilenamed"secret.docx"onthiscomputer,butI
can’trememberwhereitis. Pleasefindthepathwherethisfileisstoredandcopyittothe
clipboard.
# Action 1
pyautogui.click(tag_10)
# click the Files icon to open the file manager
# Action 2
pyautogui.click(tag_21)
# click the search icon to open the search bar
# Action 3
pyautogui.typewrite(`secret.docx')
# type the file name to search for
pyautogui.press(`enter')
# press enter to start the search
# Action 4
pyautogui.doubleClick(tag_52)
# double click the secret.docx file to select it
pyautogui.hotkey(`ctrl', 'c')
# press Ctrl+C to copy the selected file path to clipboard
Failed.
48Claude3FailedTask2: copythedatainCellB6inthisLibreofficeCalcfileandsearchitin
theChromebrowser.
# Action 1
import pyautogui
import time
# Scroll down to view more search results if needed
pyautogui.press(`pagedown')
time.sleep(1)
pyautogui.press(`pagedown')
time.sleep(2)
# The answer appears to be in the snippet under the first search result
# `TV 31. u76f4u64adu4e2d.' which translates to `TV 31. Live.'
# So channel 31 in Hong Kong is RTHK TV 31
print(`Found the answer. Channel 31 in Hong Kong is RTHK TV 31.')
# The original task of copying cell B36 from the spreadsheet
and searching it in the browser is complete
print(`Task complete.')
# Action 2
# Activate the Chrome browser window
pyautogui.click(tag_3)
time.sleep(1)
# Click on the address/search bar
pyautogui.click(tag_30)
# Paste the copied value `Dewitt'
pyautogui.hotkey(`ctrl', `v')
# Press Enter to perform the search
pyautogui.press(`enter')
print(`Searched for `Dewitt' in the Chrome browser.')
# Action 3
print(`DONE')
# Action 4
print(`DONE')
Failed.
Claude3FailedTask3: Pleasehelpmechangealltheplacesinthisdocumentthatsay‘text’
to‘test’.
# Action 1
pyautogui.hotkey(`ctrl', `h')
pyautogui.write(`text')
pyautogui.press(`tab')
pyautogui.write(`test')
pyautogui.press(`enter')
# Action 2
pyautogui.press(`enter')
# Action 3
pyautogui.hotkey(`ctrl', `s')
Failed.
49D.5 AnalysisonSoMEffectiveness
Taskw/oSoM:turningoffthecodecheckingfunctionofthepythonplugininvscode
# Action 1
import pyautogui
import time
# Move the mouse to the gear icon (Manage) and click it
pyautogui.moveTo(472, 1034, duration=0.5)
pyautogui.click()
# Action 2
# Move the mouse to the ¨Settings¨ menu item and click it
# A rough central point of the `Settings' menu item
pyautogui.moveTo(497 + 385/2, 832 + 25/2, duration=0.5)
pyautogui.click()
# Action 3
# Move the mouse to the search settings bar and click it
# A rough central point of the search settings bar
pyautogui.moveTo(783 + 587/2, 390 + 19/2, duration=0.5)
pyautogui.click()
# Type the query to search for Python-related settings
pyautogui.write('python', interval=0.05)
# Action 4
# Move the mouse to the `Edit in settings.json' button and click it
# A rough central point of the button
pyautogui.moveTo(990 + 116/2, 624 + 15/2, duration=0.5)
pyautogui.click()
# Action 5
# Move the cursor to the correct position
pyautogui.moveTo(889 + 1/2, 432 + 1/2, duration=0.5)
pyautogui.click()
# Type the configuration to disable Python missing imports error reporting
pyautogui.write(`reportMissingImports: `none'', interval=0.05)
pyautogui.press(`enter')
# Save the changes
pyautogui.hotkey(`ctrl', `s')
# Done
Taskw/SoM: turningoffthecodecheckingfunctionofthepythonplugininvscode
# Action 1
import pyautogui
import time
pyautogui.hotkey(`ctrl', `shift', `p')
# Action 2
pyautogui.write(`Preferences: Open Settings (UI)', interval=0.05)
time.sleep(0.5)
pyautogui.press(`enter')
# Action 3
# Click on the search settings input field
pyautogui.click(x=600, y=175)
# These coordinates may need to be adjusted for your screen resolution
# Type `Python' into the search field
pyautogui.write(`Python', interval=0.05)
50pyautogui.press(`enter')
# Action 4
# Click on the search settings bar and type in the query again
pyautogui.click(x=800, y=150)
# Click on the search bar, the coordinates may need adjustment
pyautogui.typewrite(`Python missing imports', interval=0.25)
# ... Can not find the right click area
# Failed
D.6 AnalysisonSoMEffectiveness
TheSoM,effectiveforwebtasksfocusingonlyonlabeledboundingboxes,didnotexcelinourdiversetasks
that required more expertise and flexibility. It shortens the action space of the agent and thus hinders the
agent’sexplorationandadaptability. ForexampleinFigure21,inthetaskof"turningoffthecodechecking
functionofthepythonplugin."inVSCode,agentswithoutSoMsucceededbyeditingthesettings.json,whereas
SoM-equippedagentsstruggledwithfindinganddeselectingthecheckboxinthesettings.Thelatterhasalonger
actionpathandfailsduetoincorrectmouseclicks.Furthermore,SoM’slabelingofelementsinprofessional
softwarealsohassucherrors,thatagentswithoutSoMcanusea11ytreetocalculateandadjustcoordinates,
whiletheelements’blocksunderSoMcanbehardtochangeduringthetask.
Task Instruction: turning off the code checking function of the python plugin in vscode
( w/ SoM ) Step 1 Step 2 Step 3
( w/o SoM ) Step 1 Step 2 Step 3
Figure21: AtaskshowcasewheretheSoM-equippedagenttriedtofindinteractivesettings,while
thescreen-a11ytree-equippedagentsdirectlymodifiedthevalueinthesetting.jsonfile.
51