Any2Point: Empowering Any-modality Large Models
for Efficient 3D Understanding
YiwenTang*1,3,JiamingLiu*2,DongWang1,ZhigangWang1
ShanghangZhang2,BinZhao1,3,XuelongLi1,4
1ShanghaiAILab2PekingUniversity
3NorthwesternPolytechnicalUniversity4ChinaTelecom
Abstract
Largefoundationmodelshaverecentlyemergedasaprominentfocusofinterest,
attaining superior performance in widespread scenarios. Due to the scarcity of
3D data, many efforts have been made to adapt pre-trained transformers from
vision to 3D domains. However, such 2D-to-3D approaches are still limited,
duetothepotentiallossofspatialgeometriesandhighcomputationcost. More
importantly,theirframeworksaremainlydesignedfor2Dmodels,lackingageneral
any-to-3Dparadigm. Inthispaper,weintroduceAny2Point,aparameter-efficient
methodtoempowerany-modalitylargemodels(vision,language,audio)for3D
understanding. Givenafrozentransformerfromanysourcemodality,weproposea
3D-to-any(1Dor2D)virtualprojectionstrategythatcorrelatestheinput3Dpoints
totheoriginal1Dor2Dpositionswithinthesourcemodality. Thismechanism
enablesustoassigneach3Dtokenwithapositionalencodingpairedwiththepre-
trainedmodel,whichavoids3Dgeometrylosscausedbythetrueprojectionand
bettermotivatesthetransformerfor3Dlearningwith1D/2Dpositionalpriors.Then,
withineachtransformerblock,weinsertanany-to-3Dguidedadaptermodulefor
parameter-efficientfine-tuning. Theadapterincorporatespriorspatialknowledge
from the source modality to guide the local feature aggregation of 3D tokens,
compelling the semantic adaption of any-modality transformers. We conduct
extensiveexperimentstoshowcasetheeffectivenessandefficiencyofourmethod.
Thecodeisreleasedathttps://github.com/Ivan-Tang-3D/Any2Point.
1 Introduction
Drivenbythegrowingvolumeofmodelparametersandtrainingdata,largefoundationmodelshave
gainedunprecedentedattentioninadiversearrayofdomainsandtasks. Numerouslargemodelshave
beenpre-trainedfornaturallanguageprocess,includingBERTDevlinetal.[2018],T5Raffeletal.
[2020],andGPTseriesOpenAI[2023],FloridiandChiriatti[2020],aswellasvisualunderstanding
likeDINOV2Oquabetal.[2023],MAEHeetal.[2022],Weietal.[2022],Xieetal.[2022],and
ViT-22BDehghanietal.[2023]. ExistingworksHuetal.[2021],Liuetal.[2021],Chenetal.[2022],
Jiaetal.[2022]alsoexploreefficientfine-tuningtechniquestotransferpre-trainedlargemodelstoa
varietyofdownstreamtasks,consistentlyachievingexcellentperformance. Meanwhile,3Dvisual
understandingZhangetal.[2022b],Daietal.[2017],Qietal.[2017a],Guoetal.[2023b]isalso
asignificanttopic,withitsrichgeometricrepresentationcontributingtothedevelopmentofmany
applications(e.g.,roboticsLietal.[2023],Guoetal.[2023c]andautonomousdrivingYangetal.
[2023],Panetal.[2023]). Unfortunately,duetoalackoflarge-scale3Ddata,theeffortstowards3D
foundationalmodalaresignificantlylaggingcomparedtolanguageand2Dvision. Specifically,the
acquisitionandannotationofhigh-quality3Ddatarequiresexpensiveresourcesandhumanlabor,
whilesynthetic3Ddatatrainingfallsshortofdistributiondiversityandreal-worldapplications.
4202
rpA
11
]VC.sc[
1v98970.4042:viXraFigure1: OverviewofAny2Point. Weproposeageneralframeworkforany-to-3Dlearning,which
issharedforanymodalitieswithparameter-efficientfine-tuning.
Therefore,somepreviousworkshavetransferredpre-trainedmodelsfromothermodalities(mainly
2Dvision)to3Dmodality,leveragingsufficientpre-trainedknowledgefromdiversesources. We
categorizeexisting2D-to-3Dworksintotwogroups. 1)Datamodalitytransformation. Thistypeof
approachinvolvesprojecting3Dpointcloudsinto2DimagesWangetal.[2022],Zhangetal.[2022b],
Zhuetal.[2023],whicharesubsequentlyfedinto2Dpre-trainedmodels. Despitethepromising
performance on downstream tasks, the process of modality transformation inevitably causes the
lossofspatialinformationin3Ddata,hinderingthefullpotentialfor3Dunderstanding. 2)Cross-
modalityknowledgedistillation. Theseapproachesinvolvethepre-trainingknowledgetransfer
from2Dorvision-languagemodelstoanewlytrained3DmodelZhangetal.[2023b],Dongetal.
[2022],Xueetal.[2023].Theyarenotonlyrequiredtoforwardpropagateboththe2Dand3Dmodels
duringtraining,butalsohighlyrelyonthelarge-scalepaired2D-3Ddata. Thisleadstosubstantial
computationcostsanddataengineering,limitingtheirabilityforefficientimplementation. Besides
theaforementionedissues,moreimportantly,currentmethodsmostlyfocusonthemodeladaption
from2Dvisionto3Dpointclouds,ratherthanasharedmethodologyforothermodalities. Therefore,
weposeaquestion:canwedevelopageneralany-to-3Dparadigmthatempowersany-modalitylarge
modelsforefficientandeffectivepointcloudunderstanding?
Toaddressthisissue,weproposeAny2Point,aunifiedany-to-3Dframeworkthattransfersany1D
(language)or2D(image/audio)largemodelsto3DdomainswithParameter-EfficientFine-Tuning
(PEFT),asshowninFigure1. Differentfrompriormethods,ourAny2Pointavoidsthepointcloud
projection,therebymitigatingthe3Dinformationloss,anddirectlyfine-tunespre-trainedmodels
from source modalities, which saves resources by knowledge distillation. Specifically, given an
any-modalitypre-trainedtransformer,wefirstintroducea3D-to-any(1Dor2D)virtualprojection
mechanism. This mechanism establishes a positional mapping between the input 3D points and
their virtually projected 1D lines or 2D planes. This enables us to encode 3D coordinates using
theoriginalpositionalembeddingsofthesourcemodalityofpre-trainedlargemodels. Inthisway,
wenolongerneedtoconductatrueprojectionlosing3Dgeometries,whilebetterpromotingthe
pre-trainedtransformertoacquire3Dfeatureswiththeiroriginal1D/2Dpositionalpriors. Then,
foreachtransformerblock,weinsertanany-to-3DguidedadaptermoduleforPEFT.Thisadapter
leverages the 1D/2D spatial guidance to aggregate the local semantics of 3D tokens, facilitating
fine-grainedfeatureinteraction. Afterward,weperformanadaptiveensembleforthe3Dfeatures
guidedbydifferent1D/2Dpriors,whichattainssuperior3Drepresentations.
Extensive experiments across various tasks demonstrate that our Any2Point framework achieves
superiorperformancecomparedtocurrent3Dpre-trainedmodels,whileutilizingonly1.0%ofthe
trainableparameters. Usingthepre-trainedCLIPTextEncoderRadfordetal.[2021],Any2Point
fine-tunesonly0.8Mparametersandattains91.9%onScanObjectNNUyetal.[2019],outperforming
thepreviousstate-of-the-art(SOTA)3Dpre-trainedmodelby+1.3%,and94.3%onModelNet40Wu
etal.[2015]. Furthermore,Any2Pointalsoachievescomparableresultsandefficiencybyutilizing
otherpre-trainedmodelsOquabetal.[2023],Touvronetal.[2021],Gongetal.[2022],Girdharetal.
2[2023],Liuetal.[2019]ofdifferentmodalities,including2Dvision,language,andaudio,validating
therobustnessofourapproach. Thecontributionsofourpaperareasfollows:
• To enable a general any-to-3D transferring framework, we propose Any2Point, which
empowersany-modalitypre-trainedlargemodels(e.g.,2Dvision,language,andaudio)for
efficient3Dunderstanding.
• Weintroducetwotechniques,i.e.,3D-to-anyvirtualprojectionandany-to-3Dguidedadapter,
toeffectivelyovercometheissueswithincurrentmethods,suchas3Dgeometrylossand
excessiveresourcecost.
• Any2PointachievessuperiorperformancecomparedtopreviousSOTA3Dpre-trainedmod-
elsacrossvarioustasks. Notably,thesecompetitiveresultsremainconsistentbyleveraging
pre-trainedmodelsfromdifferentmodalities,e.g.,2Dvision,language,andaudio.
2 RelatedWork
2.1 LargeModels
Large-scale pre-trained models have achieved remarkable downstream performance in language,
2Dvision,andaudioprocessing. Inthefieldofnaturallanguagefield,BERTDevlinetal.[2018]
revolutionizednaturallanguageunderstandingbypre-trainingdeepbidirectionalrepresentationsfrom
unlabeledtext. Buildingonthis,RoBERTaLiuetal.[2019]enhancesBERTbyoptimizingtraining
and data scale, significantly boosting performance across language understanding benchmarks.
GPTFloridiandChiriatti[2020],Radfordetal.[2019]pioneersingeneratingcoherent,contextually
relevant text using a transformer model pre-trained on a diverse corpus of text. CLIP Radford
etal.[2021]furtherbridgesvisualandlinguisticinformationbypre-trainingonavastcollectionof
image-textpairs. In2Dvision,DeiTTouvronetal.[2021]achievesefficientimageclassificationwith
transformers,usingdataaugmentationandknowledgedistillationforminimaldatareliance. DINO
V2 Oquab et al. [2023] also advances self-supervised learning with vision transformers through
innovativeself-distillation,requiringnolabels. MAEHeetal.[2022]furtherproposesanasymmetric
encoderanddecodertoreconstructimagesfrommaskeddata. Foraudioprocessing,ASTGongetal.
[2021]transformsaudiorecognitiontospectrogramsbyapplyingavisiontransformer. SSASTGong
etal.[2022]leveragesself-supervisedlearningonunlabeleddatawithtransformersforenhanced
audio classification performance. ImageBind Girdhar et al. [2023] unifies multi-modal space by
jointly training on multi-modal data including audio, improving multi-modal understanding and
generation. Ourmethodfirstutilizestheabundantknowledgefromlargemodelsofanymodalities
andachieves3Dunderstandingcapacity.
2.2 2D-to-3DTransferLearning
Theparadigmof2D-to-3Dtransferlearningaimstoleveragetherichcontextualandtexturalknowl-
edgeinthe2Ddomaintoboost3Dunderstanding. Someworksproposespecificdesignsfor3D
learningguidedby2Dpre-trainedknowledge,andachievepromising3Dunderstandingperformance.
Image2PointXuetal.[2022]proposestotransfer2Dsemanticsto3Dbyconvolutionallayerinflating.
ACTDongetal.[2022]employspre-trained2DandlanguageTransformersascross-modalteachers
for3Dlearningviadiscretevariationalautoencodingandprompttuning. ULIPXueetal.[2023]
enhances3Dunderstandingperformancebyunifyingimage,text,and3Dpointcloudrepresenta-
tions. ReConQietal.[2023]leveragescontrastivecross-modallearningandgenerativemodelsfor
knowledgetransfer. Meanwhile,PointCLIPV1Zhangetal.[2022b]andV2Zhuetal.[2023]first
adoptCLIP’s2Dpre-trainedknowledgeondifferent3Ddownstreamtasksviaprojecting3Dpoint
cloudsto2Dimagesasinputtothepre-trainedbackbone. P2PWangetal.[2022]alsoproposes
2D-to-3Dprojectionthroughalearnablecoloringmodule. Ourapproachskipsthestepofprojecting
pointclouds,whichreducesthelossof3Dgeometricinformation,andfine-tunespre-trainedmodels
insteadofusingcomputationallyexpensiveknowledgedistillation.
2.3 Parameter-EfficientFine-tuning
Thepre-trainingandfine-tuningparadigmhasbeenprovenhighlyeffectiveinvarioustaskssuch
as2Dvisualrecognition,languageunderstanding,text-to-imagegeneration,andaudiorecognition.
3Figure 2: Overall Pipeline of Any2Point. For efficiently fine-tuning Any-modality pre-trained
models,ourAny2Pointframeworkcontainstwocomponents: a3D-to-anyVirtualProjection,which
pairsthepre-trainedpositionalencodingswith3Dtokenstoavoidthe3Dgeometricinformationloss,
andanAny-to-3DGuidedAdaptertoeffectivelygrasplocalstructures.
However,fullyfine-tuningthewholemodelbecomesimpracticalasmodelvolumeincreasesexpo-
nentially. Incontrast,theparameter-efficientfine-tuning(PEFT)methodsHuetal.[2021],Jiaetal.
[2022],Chenetal.[2022],Houlsbyetal.[2019],Liuetal.[2023]aimatonlyupdatingatinypart
ofthemodel’sparameterswhilefreezingtherestparts,whichhavebeenproventobeeffectiveand
efficientonmultiplepopularpre-trainedmodelssuchasBERTDevlinetal.[2018],GPTFloridiand
Chiriatti[2020],ViTDosovitskiyetal.[2020],CLIPRadfordetal.[2021],andStableDiffusionRom-
bachetal.[2022]. ThePEFTapproachescanbegenerallycategorizedintothreeprincipalstreams
namelyprompttuningJiaetal.[2022],Yangetal.[2024],reparameterizationHuetal.[2021],and
adaptersChenetal.[2022],Houlsbyetal.[2019],Zhangetal.[2023a]. Thesetechniquescustomize
pre-trainedmodelsforspecifictasksbyfine-tuningprompts,modifyingmodelparameterswithout
alteringtheoriginalarchitecture,andinsertinglightweighttrainablelayers,respectively. Recently,
Point-PEFTTangetal.[2024]firstintroducedPEFTtechniquesinto3Ddomains. Inthispaper,the
Any2PointframeworkutilizesPEFTtechniquestotransferAny-Modalitypre-trainedmodelsto3D
understandingtasksatalowcomputationalandstoragecost.
3 Any2Point
InSection3.1,wefirstprovideaparadigmoverviewofAny2Point,includingtheproblemdefinition
andnetworkarchitecture.Then,inSection3.2and3.3,werespectivelyelaborateonthemethodologies
ofourproposedtwotechniquesforadaptingany-modalitylargemodelsfor3Ddomains.
3.1 MethodOverview
ProblemDefinition. Givenapre-trainedtransformerfromanymodality,e.g.,vision,language,and
audio,ourobjectiveistoempoweritwith3Dunderstandingcapabilitiesinaneffectiveandefficient
manner. Insteadofemployingfullfine-tuningon3Ddata,weseekaparameter-efficientsolutionwith
thesourcetransformersfrozen,sincetheirlarge-scaleparametersmightcausehighcomputationcost
andover-fittingissuesonthelimited3Ddataset. Wegenerallydividethesourcemodelsintotwo
categoriesaccordingtotheirpre-trainingdatadimension,denotedas1Dand2Dtransformers. The
1Dtransformersarespecializedinprocessingsequentialdata,exemplifiedbylanguagemodelslike
RoBERTaLiuetal.[2019],T5Raffeletal.[2020],andCLIP’stextencoderRadfordetal.[2021].
The2Dtransformersareexpertat2Dspatialdata,includingvisionmodels,e.g.,DINOv2Oquabetal.
[2023]andDeiTTouvronetal.[2021],andaudiomodels,e.g.,ImageBindAudioEncoderGirdhar
etal.[2023]andSSASTGongetal.[2022].
ModelPipeline. TheoverallparadigmofAny2PointisdepictedinFigure2. Toencodetheinput
pointcloud,wediscardtheoriginalembeddingmodulesinsourcetransformers,e.g.,tokenizersin
1Dlanguagemodelsandconvolutionsin2Dvision/audiomodels,andemploya3Dmini-network
forpointcloudtokenization. Ontopofthis,theencoded3Dtokensarefedfirstintoa3D-to-any
virtualprojectionmoduleforpositionalencoding,andthenintothefrozen1D/2Dtransformerwith
any-to-3Dguidedadapters. Theformermechanismaimstoassigneach3Dtokenwithpositional
informationwithinthesourcemodality,andthelatterisdesignedforadaptive1D/2D-guided3D
4Figure 3: 3D-to-any Virtual Projection. To prevent the loss of 3D geometric information, the
moduleassigns3Dtokenswiththepositionalencodingsthatarepairedwiththepre-trainedmodel.
representation learning, which we will detail in the following sections. Note that, as the source
transformers are kept frozen, only the initial tokenization network and the inserted adapters are
learnableforparameter-efficientfine-tuning.
3.2 3D-to-anyVirtualProjection
Manycurrent2D-to-3DmethodsZhuetal.[2023],Zhangetal.[2022b],Wangetal.[2022]project
3Dpointcloudsintomulti-viewimagestomeettheinputmodalityofpre-trained2Dmodels. This
dimensionreductionprocesspotentiallyleadstotheinformationlossof3Dgeometriesanddeep
measurements,enablinginsufficient3Dfeatureencoding. Inaddition,theseapproachesaremerely
validatedonthelargemodelswithin2Dimages,withoutconsideringothermodalitieslikelanguage
andaudio. Therefore,weproposea3D-to-anyvirtualprojectionstrategythatmitigatesthegeometric
loss,andisgeneralizabletoany1D/2Dpre-trainedmodels,asshowninFigure3.
Tokenizationin3DSpace. Toavoidanyinformationdegradation,wedirectlytokenizetheinput
pointcloudwithinthe3Dspaceforthesubsequent1D/2Dtransformer. Specifically,weemploya3D
mini-networkcontainingsmall-scaleparameters,whichisalighter-weightvariantofPoint-PNZhang
etal.[2023c],Zhuetal.[2024]. ThetokenizationprocessinvolvesFarthestPointSampling(FPS)Qi
et al. [2017a] for point number downsampling, k-Nearest Neighbor (k-NN) algorithm for local
aggregation,andlearnablelinearlayersforfeatureencoding. Afterthis,wetransformtherawpoint
cloudsintohigh-dimensionalvectors,obtainingN 3Dtokensas{T }N ,with{p3D}N denoting
i i=1 i i=1
their3Dcoordinates.
MotivationsforVirtualProjection. Positionalencodings(PEs)serveastheonlyindicatorfor
positionalinformationtothetransformermodel,sincetheinnerattentionmechanismispermutation-
invariant,treatingeverytokenatdifferentordersallthesame. Therefore,astraightforwardwayfor
1D/2Dtransformerstocomprehend3Dpositionalinformationistointegratenew3DPEswith3D
tokens. However,thesourcetransformersarepre-trainedpairedwiththeiroriginalPEsin1D/2D
space,whichleadstosemanticdiscrepancybetweenthefrozen1D/2Dweightsandnewlylearned3D
PEs. Toaddressthisissue,wevirtuallyproject3Dtokensintothesourcemodality,andobtainthe
corresponding1D/2DPEsforbetteraligningwiththetransformers.
3D-to-2DVirtualProjection. For2Dtransformersin2Dvisionandaudiomodalities,wevirtually
projecteach3Dcoordinate, e.g., p3D, intoM views, derivingthecorresponding2Dcoordinates
i
as {p2D}M . The M different perspectives are capable of providing diverse positional relations
ij j=1
within2Dspace. WeadoptasimpleprojectioninPointCLIPZhangetal.[2022b]withoutlearnable
parameters. Importantly, wedonottrulyproducetheprojectedmulti-viewimages, butonlyaim
5Figure4: Any-to-3DGuidedAdapter. Insertedintoeverytransformerblock,theadapterleverages
the1D/2D-guidedLocalAggregationmoduletocapture3DlocalsemanticsandutilizestheAdaptive
Any-to-3DEnsembletoobtainhigh-qualityfeatures.
to obtain the virtual 2D positions. Then, according to the original 2D PEs within pre-trained
transformers,weassigneach3Dtoken,e.g.,T ,withM differentPEs,denotedas{PE2D(p2D)}M .
i ij j=1
3D-to-1DVirtualProjection. Similarly,for1Dtransformersinlanguagemodality,wevirtually
projectthe3Dcoordinatesintodifferent1Dlines. Toalignthenumberwith2Dmodality,wealso
selectM linespassingthroughthecenterofthepointcloudwithM uniformrotationangles. For
simplicity,wesupposethepointcloudcenterastheorigin,theunitdirectionvectorsofM linesas
{⃗v1D}M ,andthepointcoordinate,p3D,vectorizedasp⃗3D. Then,the1Dcoordinateofpointiin
j j=1 i i
linej isformulatedbythedotproductionof
p1D =⃗v1D·p⃗3D, (1)
ij j i
denotingtheprojectedlength. Inthisway,werefertotheoriginal1DPEs,andassigneach3Dtoken,
e.g.,T ,withM differentPEsas{PE1D(p1D)}M .
i ij j=1
Encoding3DPositionsin1D/2DPEs. Afteracquiringthecorresponding1D/2DPEs,weaverage
themasanoverallpositionalindicator,andincorporateitwiththe3Dtoken,e.g.,T ,by
i
M
Tin =T + 1 (cid:88) PE1D/2D(p1D/2D). (2)
i i M ij
j=1
Withthisapproach,weinjectsufficientpositionalinformationofthesourcemodalityinto3Dtokens
tobettercollaboratewiththefrozentransformer,whilemitigatingtheinformationlossofthetrue
projection.
3.3 Any-to-3DGuidedAdapter
Differentfromexistingdistillation-basedmethodsZhangetal.[2023b],Guoetal.[2023a]traininga
new3Dnetwork,wedirectlyfeedtheencoded3Dtokens{Tin}N tothepre-trained1D/2Dtrans-
ij i=1
former. AlthoughthePEsof3Dtokenshavebeenalignedwiththesourcemodel,theentirelyfrozen
weightspre-trainedbyothermodalitiesarestillrestrictedtolearningsuperior3Drepresentations.
Consideringthis,weintroducealearnableany-to-3Dguidedadapterwithineachtransformerblock,
asshowninFigure4. TheadaptersareinsertedaftertheFeed-ForwardNetworks(FFNs),andfurther
incorporate1D/2D-priorknowledgeforparameter-efficientfine-tuning.
6MotivationsforInsertingAdapters. Theself-attentionmechanismswithinsourcetransformers
normallyfocusonlong-rangetokeninteractioninglobalcontexts,whichlackslocalfeatureextraction.
However,thedetailedspatialgeometriesarealsosignificantforthefine-grainedunderstandingof
3Dshapes. Tocomplementthegap,weutilizetheproposedadapterlayersforspecificallycapturing
3Dsemanticswithinlocalneighborhoods. Inaddition,asthesourcetransformersarepoweredby
1D/2DPEsasdiscussedabove,thenaiveFPSandk-NNfor3Dlocalgroupingmightcausepositional
discrepancy. Therefore, we further design a 1D/2D-guided aggregation strategy and an adaptive
any-to-3Densembleapproachforrobust3Dfine-grainedencoding.
1D/2D-guidedLocalAggregation. Withintheadapter, wefirstgroup3Dtokensintodifferent
localneighborhoodsguidedby1D/2Dpositionalpriors,whichbetteraligntheadopted1D/2DPEs.
ForM differentviews/lines,weconductM concurrentlocalaggregationprocesstomakethebestof
differentprojectionperspectives. Specifically,for2Dtransformers,wedivideeachvirtuallyprojected
image,e.g.,thej-thview,intouniformlocal2Dpatches,andgroupthe3Dtokenswithinthesame
patch into a neighborhood, according to their 2D positions {p2D}N . For 1D transformers, we
ij i=1
similarlydivideeachvirtuallyprojectedline,e.g.,thej-thdirection,intouniformlocal1Dsegments,
andgroupthe3Dtokenswithindifferentsegmentsreferringtotheir1Dpositions{p1D}N . Ontop
ij i=1
ofthis,weadoptaself-attentionlayerfor3Dtokenswithineach1D/2Dneighborhoods,performing
localfeatureinteractionguidedby1D/2Dpriors. Thenweemploytheoperationsofpoolingand
propagationtopropagatethelocalaggregatedfeaturetoeverypointswithinthesameneighborhood.
Adaptive Any-to-3D Ensemble. After the parallel local aggregation, we obtain M sets of 3D
tokens, each representing a 2D view or 1D line. As different projection perspectives normally
showcasedifferentsignificancefor3Drepresentations,weproposeanadaptiveany-to-3Densemble
approachtoaggregatetheM featuresforeachtoken. Wedenotethei-th3DtokenwithM setsof
featuresatthisstageas{F }M . Toproperlyindicatetherelativeimportanceofeachview/line,
ij j=1
weadditionallyemploya3DfeaturetransformationbranchindependentoftheM 2D-guidedlocal
aggregation. This non-parametric branch only contains the local grouping in 3D space, feature
averagepoolingwithinlocalgroups,andpropagationoperations,convertingthe3Dtokenbeforethe
adapterintoafeaturebaselineforadaptiveensemble,denotedasB . Then,wecalculatetherelative
i
weightsfordifferentviews/linesbythecosinesimilarity,andfinallyaggregatetheirfeaturestoobtain
thefinaloutputas
M
1 (cid:88)
Tout = Sim(B ,F ). (3)
i M i ij
j=1
Withtheensemblestrategy,weintegrateM differentfeatureswithdynamicweights,enablingthe
adaptertoadaptivelydeterminewhichview/lineismorecritical,contributingtohigh-qualityadapted
features.
4 Experiments
In this section, we conduct extensive experiments on the ScanObjectNN Uy et al. [2019] and
ModelNet40Wuetal.[2015]datasets. Wefirstintroducethefine-tuningsettingsandimplementation
details in Section 4.1. Then, in Section 4.2, we present the main experiment of transferring any-
modality large models (language, 2D image and audio) to 3D classification tasks. Finally, in
Section4.3,weconductablationstudiestoevaluateeachcomponentwithinourproposedAny2Point
framework.
4.1 ExperimentalSettings
ScanObjectNN. TheScanObjectNNdatasetUyetal.[2019]consistsofreal-world3Dobjectscans,
categorized into 15 distinct classes. We select the most challenging PB-T50-RS split to test the
performanceoftheAny2Pointframeworkwithoutthevotingstrategy. Forallmodels,weemploy
theAdamWoptimizerLoshchilovandHutter[2017]andtheCosineAnnealingscheduler. Theinitial
learningrateissetto5e-4,withaweightdecayfactorof0.05. Wefine-tunethemodelfor300epochs
withabatchsizeof32. Fordataaugmentation,weuseRandomscaling,translation,androtation. For
language,2Dvision,andaudiomodalities,werespectivelyselecttheCLIPTextEncoderRadford
etal.[2021],DINOV2Oquabetal.[2023],andImageBindAudioEncoderGirdharetal.[2023]
7Table1: Comparisonsonaccuracywithpreviousmethodson3Dclassificationdatasets. We
report the pre-training modality (Pre-train), the number of learnable parameters (#Param) on the
"PB-T50-RS"splitofScanObjectNN(SCAN.)andModelNet40(MN.).†indicatesutilizingthevoting
strategy.
Method Pre-train #Param(M) SCAN.(%) MN.(%)
Point-NNZhangetal.[2023c] N/A 0.0 64.9 81.8
PointNetQietal.[2017a] N/A 3.5 68.0 89.2
PointNet++Qietal.[2017b] N/A 1.5 77.9 90.7
DGCNNWangetal.[2019] N/A 1.8 78.1 92.9
PointMLPMaetal.[2022] N/A 12.6 85.4 94.1
Point-PNZhangetal.[2023c] N/A 0.8 87.1 93.8
PointNeXtQianetal.[2022] N/A 1.4 87.7 94.0
Point-BERTYuetal.[2022] 3D 22.1 83.1 92.7
w/Point-PEFTTangetal.[2024] 3D 0.6 85.0 93.4
Point-MAEPangetal.[2022] 3D 22.1 85.2 93.2
Point-M2AEZhangetal.[2022a] 3D 15.3 86.4 93.4
P2P-HorNetWangetal.[2022] 2D 1.2 89.3 94.0†
ACTDongetal.[2022] 3D+2D 22.1 88.2 93.7
I2P-MAEZhangetal.[2023b] 3D+2D 12.9 90.1 93.7
ReConQietal.[2023] 3D+2D+Language 43.6 90.6 94.1
Audio 0.8 87.0 92.7
Any2Point 2D 0.8 87.7 93.2
Language 0.9 91.9 94.3
aspre-trainedmodels. Forthesethreemodels,thetransformerarchitectureisthesame: a12-block
encoder with 768 feature channels and 1,024 input point number. The hyperparameter M in the
3D-to-anyVirtualProjectionissetto6withidenticalanglesfortheAny-ModalityTransformers. To
matchtheshapeoftheoriginalPEswithinpre-trainedmodels,wevirtuallyproject3Dpointsintoa
1Dlinesegmentoflength77withalinesizeof2inthelanguagemodality;a2Dplanemeasuring
512x512withapatchsizeof26inthe2Dvisionmodality;anda2Dplanesized192x304witha
patchsizeof16intheaudiomodality.
ModelNet40. TheModelNet40datasetWuetal.[2015]consistsof40categoriesofsynthesized3D
CADmodels,with9,843trainingsamplesand2,468testsamples.InourexperimentsonModelNet40,
weadoptthesamefine-tuningsettingsandthesamepre-trainedmodelsasinScanObjectNN.For
dataaugmentation,weutilizedefaultrandomscalingandtranslation. Notably,duringthetesting
process,wedonotemploythevotingstrategy.
4.2 QuantitativeAnalysis
The results are shown in Table 1. It is observed that: (i) On the 3D real-world object dataset
ScanObjectNN, the Any2Point framework achieves 91.9%, 87.7%, and 87.0% accuracy based
on Language (CLIP-Text), 2D Vision (DINO V2-B), and Audio (ImageBind-Audio) modalities,
respectively. ComparedtothepreviousSOTAmethod(ReCon),1Dlanguagepre-trainedAny2Point
achieves a 1.3% improvement with only 0.9M learnable parameters. For the 2D (Vision/Audio)
modalities, Any2Point significantly outperforms Point-M2AE, which is the SOTA method pre-
trainedonlyon3Ddatasets, by0.6%and1.3%, respectively. Thisrevealsthatourframeworkis
capableoffullyexploitingpre-trainedknowledgefromothermodalitiestosolve3Drecognitiontasks.
(ii)Onthe3DsyntheticobjectdatasetModelNet40, acrosstheLanguage, 2DVision, andAudio
modalities,ourAny2Pointframeworkattains94.3%,93.2%,and92.7%. Ourframeworkexclusively
utilizesonepre-trainedmodelinthe1Dlanguagemodality, achievinga0.2%improvementover
thepreviousSOTAmethod(ReCon),andreducing42.7Mlearnableparameters. For2Dmodalities,
Any2Pointdemonstratesperformanceonparwithmodelspre-trainedexclusivelyon3Ddatasets. (iii)
Surprisingly,whetherontheScanObjectNNortheModelNet40dataset,theAny2Pointframework
maintainsaperformancetrendwhere1Dmodality(language)outperforms2Dmodalities(imageand
8Table2:AblationStudyonDifferentPEFTMethods.Wereportthenumberoflearnableparameters
(#P)andclassificationaccuracy(%)ofCLIP-Text(1D.)andDINOV2(2D.)onthe"PB-T50-RS"
splitoftheScanObjectNNdataset.
Method #P(M) 1D.(%) 2D.(%)
FullFine-Tuning 86.3 79.9 85.3
PromptTuningJiaetal.[2022] 0.4 89.1 86.4
AdapterTuningHoulsbyetal.[2019] 0.4 89.6 85.9
LoRAHuetal.[2021] 0.9 86.3 85.1
Any2Point 0.8 91.9 87.7
Table3: AblationStudyonMainComponents. Tovalidatetheeffectivenessof3D-to-anyVirtual
Projection(V.P.)andAny-to-3DGuidedAdapter(G.A.).
3D-to-anyV.P. Any-to-3DG.A. #P(M) 1D.(%) 2D.(%)
- - 0.3 88.7 86.1
✓ - 0.3 89.3 86.6
- ✓ 0.8 90.9 87.6
✓ ✓ 0.8 91.9 87.7
audio).Largelanguagemodelsprovideabundantspatialandsemanticinformationinlow-dimensional
spacestoassistin3Dlearning. ThistrendisfurthervalidatedintheupcomingSection4.3.
4.3 AblationStudy
Inthissection,weconductextensiveablationstudiestoexploretheeffectivenessofdifferentcom-
ponentswithinourAny2Pointframework. WeadoptCLIP-Text(1D)andDINOV2(2D)asthe
pre-trainedtransformer,andreporttheclassificationaccuracy(%)onthe"PB-T50-RS"splitofthe
ScanObjectNNdataset.
ComparisonwithtraditionalPEFTmethods. AsdemonstratedinTable2,ourAny-to-3DGuided
AdaptersignificantlyoutperformstraditionalPEFTtechniqueswhenutilizingpre-trainedmodelsfrom
1Dor2Dmodalities. IncomparisontoPromptTuningJiaetal.[2022],itachievesimprovementsof
2.8%and1.3%;comparedtoAdapterTuningHoulsbyetal.[2019],itachievesimprovementsof2.3%
and1.8%;andincontrasttoLow-RankAdaptation(LoRA)Huetal.[2021],itachievesimprovements
of5.6%and2.6%,respectively. TheexperimentalresultsdemonstratethatourAny-to-3DGuided
Adaptercanefficientlymineandintegratepre-trainedknowledgefromothermodalitiestounderstand
thesemanticsof3Dobjects. Unlikeothermethods,ourframeworkleverages1D/2Dspatialguidance
toaggregatethelocalsemanticsof3Dtokens,capturingthelocalfine-grainedinformationof3D
objects.
Effectiveness of Main Components. As shown in Table 3, to substantiate the efficacy of our
proposedmethods,weconductedablationexperimentsbyprogressivelyincorporatingeachcompo-
nentintothebaseline. Thefirstrowindicatesthebaselineconfiguration,whichconsistsofthe3D
tokenizer,thepre-trainedtransformer,andthetaskhead,withupdatesappliedonlytothetokenizer
andhead. Introducingthe3D-to-anyVirtualProjectionresultedinperformanceimprovementsto
89.3%inthe1Dmodalityand86.6%inthe2Dmodality. Thissuggeststhatusingvirtualprojection,
rather than true projection, helps mitigate the loss of 3D spatial information caused by modality
conversion. Following the inclusion of the Any-to-3D Guided Adapter, performance in the 1D
modalitysurgedto90.9%,whileinthe2Dmodality,itroseto87.6%,withafocusonlocalstructures
leadingtogreaterimprovements. Introducingbothaforementionedmethodssimultaneouslyledtoa
surgeinperformanceto91.9%inthe1Dmodalityandariseto87.7%inthe2Dmodality,effectively
showcasingtheeffectivenessofourcomprehensiveframework.
Effectsof3D-to-anyVirtualProjection. InTable4, weinvestigatedtheeffectsofemploying
different positional encoding methods on the Any2Point framework. The first row indicates the
9Table4: AblationStudyon3D-to-anyVirtualProjection. Sinusoidal,Learnableand3D-to-any
V.P. refer to sinusoidal positional encoding, learnable positional encoding and 3D-to-any Virtual
Projection.
Sinusoidal Learnable 3D-to-anyV.P. 1D.(%) 2D.(%)
- - - 90.9 87.6
✓ - - 87.4 86.0
- ✓ - 90.5 86.5
- - ✓ 91.9 87.7
Table5: AblationStudyonAny-to-3DGuidedAdapter. Tovalidatetheeffectivenessof1D/2D-
guidedLocalAggregation(L.A.)andAdaptive(Ada.) Any-to-3DEnsemble(Ens.).
1D/2D-guidedL.A. Ada. Any-to-3DEns. #P(M) 1D.(%) 2D.(%)
- - 0.25 89.3 86.6
✓ - 0.8 90.2 86.8
✓ ✓ 0.8 91.9 87.7
Table6: MoreResultsonScanObjectNN.
Method Pre-train Model #Param(M) SCAN.(%)
Audio SSASTGongetal.[2022] 0.8 87.1
Any2Point 2D DeiTTouvronetal.[2021] 0.8 87.3
Language RoBERTaLiuetal.[2019] 0.9 89.7
absenceofanypositionalencoding.Introducingsinusoidalpositionalencodingorlearnablepositional
encodingledtoacertaindegreeofperformancedegradation. Thisisduetotheconflictbetween
thenewlyintroducedpositionalinformationandtheinherentsemanticswithinthesourcemodality
transformer. On the other hand, employing 3D-to-any Virtual Projection resulted in respective
improvements of 1.0% and 0.1% accuracy. The results demonstrate that using original 1D/2D
positionalpriorscanpromotethepre-trainedtransformertoacquire3Dfeatures.
ComponentsofAny-to-3DGuidedAdapter. AsshowninTable5,weconductablationexperi-
mentsbyincrementallyaddingcomponentstotheAny-to-3DGuidedAdapter. Thefirstrowsignifies
thebaselineadapter,consistingofonlyanMLPwithbottlenecklayers. Byincorporating1D/2D-
guidedLocalAggregation,composedoflocalaggregationin1D/2Dspaces,self-attentioninteractions,
pooling,andpropagation,ourapproachachievesperformancegainsof0.9%and0.2%. Leveraging
thepositionalpriorsfromthepre-trainedmodelfacilitatesminingfine-grained3Dstructuralinforma-
tionfromdifferentperspectives. TheAdaptiveAny-to-3DEnsemblebringsfurtherimprovements
of1.7%and0.9%for1Dand2Dmodalities,effectivelyintegratingparallelfeaturesinaccordance
with3Dstructuralfeatures. Theexperimentsdemonstratetheeffectivenessofeachcomponentin
ourAny-to-3DGuidedAdaptertogather3Dlocalgeometricinformation,complementingtheglobal
attentioninthepre-trainedmodel.
MoreResultsonPerformanceTrend. TofurthervalidateourpreviousfindingsthattheAny2Point
framework,basedon1DLanguagepre-trainedmodels,significantlyoutperformsthosebasedon2D
modalities(Vision/Audio)inthe3Dobjectrecognitiontask,weconductadditionalexperimentsin
Table6. Onthe"PB-T50-RS"splitofScanObjectNNdataset,weselectRoBERTa(1D),DeiT(2D
Vision),andSSAST(Audio)asthepre-trainedmodels,withfine-tuningsettingsconsistentwithour
previousexperiments. Thesemodelsachieveperformanceof89.7%,87.3%,and87.1%,respectively.
Theperformancetrendacrossmodalitiesisobserved: 1Dlanguage>2DVision>2DAudio. We
suspectthatduetothepre-trainingdata,largelanguagemodelspossessstrongersemanticinformation
comparedtoothermodalities,whichisbeneficialforthedeepunderstandingofdifferent3Dobjects.
10Figure5: VisualizationofDifferentPositionalEncodingMethods. Forthe1D/2Dmodalities,we
visualizetheattentionscoresofthe[CLS]tokentootherpointcloudtokens,utilizingsinusoidal
positionalencoding,learnablepositionalencoding,and3D-to-anyVirtualProjection. Theredcolor
indicateshighervalues.
Figure 6: Visualization of Effects of Any-to-3D Guided Adapter.For 1D/2D modalities, we
visualizetheclustersofthesimilaritiesbetweenthe[CLS]tokenandotherpointfeatures,withthe
numberofclusterssetto3. ItisconductedforthecompleteAny-to-3DGuidedAdapter,forreplacing
theAdaptiveAny-to-3DEnsemblewithmaxmeanpooling(MaxmeanPool.),andforperforming
localaggregationsolelybasedon3Dpositions(3D-guidedL.A.).
5 Visualization
Inthissection,weopttovalidatetheefficacyoftheproposed3D-to-anyVirtualProjectionandthe
Any-to-3DGuidedAdapterbyvisualizingontheScanObjectNNtestset,utilizingtheCLIP-Text
Encoder(1D)Radfordetal.[2021]andDINOV2(2D)Oquabetal.[2023].
5.1 DifferentPositionalEncodingMethods
Our3D-to-anyVirtualProjectionfullyexploitsthepositionalencodingpairedwiththepre-trained
model,injectingthesourcemodalityspatialknowledgeintothe3Dtokensduringfine-tuning. In
Figure5,whenusingsinusoidalpositionalencodings,learnablepositionalencodings,andour3D-
to-anyVirtualProjectionrespectively,wevisualizetheattentionscoresofthe[CLS]tokentoother
pointcloudtokens. Asillustrated,forthe1Dlanguagemodality,learnablepositionalencodingsgrasp
uselessinformation. AfterapplyingthecommonlyusedsinusoidalpositionalencodingsinLarge
LanguageModels,theyfailtocapturethecritical3Dsemantics. However,ourmethodfocusesmore
onthesalientobjectparts,suchasthearmrestsandwheelsofchairs,andthelegsoftables. Forthe
2Dvisualmodality,learnableencodingsareslightlybetterthansinusoidalpositionalencodings,as
2Dpre-trainedmodelsmainlyadoptthelearnableencodingmethod. Meanwhile,ourmethoddirectly
recognizesthewholeobjectanditskeyparts,forexample,givinghighweightstothechair’sbackrest.
115.2 EffectsofAny-to-3DGuidedAdapter
TheAny-to-3DGuidedAdaptercapturesthe3Dfine-grainedinformationthroughinteractionswithin
the local regions of the source modality. In Figure 6, we visualize the clustering results of the
similaritiesbetweenthe[CLS]tokenandotherpointtokenfeatures,utilizingthecompleteAny-to-3D
GuidedAdapter,replacingtheAdaptiveAny-to-3DEnsemblewithmaxmeanpooling,andfurther
onlyusing3Dpositionalinformation. Asshown,forsimpleobjectslikechairs(1strow),ourmethod
effectivelydistinguishesbetweenthechair’sbackrest,armrests,seat,andwheels,whereasremoving
componentsfailstocapturethedifferencesbetweenkeyparts. Formorechallengingobjectslike
shelves(2nd row),removinganycomponentsleadstosemanticconfusionoftheobject,whileour
approach clearly differentiates the shelf’s base, middle layer, and backrest. These experiments
indicatethateachcomponentwithintheAdaptereffectivelyutilizesthepositionalinformationfrom
differentmodalitiestopromotethe3Dstructureextraction.
6 AdditionalAblationStudy
In this section, we conduct ablation studies to explore the effectiveness of different components
andhyper-parameters,whicharenotdiscussedinthemaintext. WeselectCLIP-TextRadfordetal.
[2021](1D)andDINOV2Oquabetal.[2023](2D)aspre-trainedmodelstocomparetheaccuracy
(%)onthe"PB-T50-RS"splitofScanObjectNNUyetal.[2019].
Table7: AblationStudyonAdapterPositionsandDepths. The‘After’,‘Before’,and‘Parallel’
denoteblockstructureofputtingtheAny-to-3DGuidedAdapterafter,before,andinparalleltothe
FFNlayerrespectively. Theinsertiondepthdenotesthenumberofearlierblocks. Wereportthe
classificationaccuracyofCLIP-TextRadfordetal.[2021](1D.)andDINOV2Oquabetal.[2023]
(2D.)onthe"PB-T50-RS"splitofScanObjectNNUyetal.[2019].
PositionsofAny-to-3DGuidedAdapter
Insertion 1D. 2D.
After Before Parallel Depth (%) (%)
✓ - - 3 90.4 86.9
✓ - - 6 90.6 87.1
✓ - - 9 90.2 87.2
✓ - - 12 91.9 87.7
- ✓ - 3 89.9 86.2
- ✓ - 6 91.1 86.4
- ✓ - 9 90.4 86.4
- ✓ - 12 91.3 87.0
- - ✓ 3 90.3 86.7
- - ✓ 6 91.5 86.8
- - ✓ 9 90.4 86.8
- - ✓ 12 90.7 87.1
6.1 TheAdapterPositionsandDepths.
InTable7,wefurtherconductedablationstudiesonthepositionsanddepthsoftheproposedAny-to-
3DGuidedAdapter.AsshowninTable7,thebestperformanceisachievedwhentheadapterisplaced
aftertheFeedForwardNetworks(FFNs)andatadepthof12. Thisisbecause,whenplacedafterthe
FFNlayers,thegloballyinteractedpointcloudfeaturesundergolocalaggregationwithintheadapter,
extractingthefine-grainedstructuresofthe3Dpointclouds. Moreover,deeperinsertionallowsthe
adaptertoleveragebothlow-levelandhigh-levelpre-trainedknowledgetoprocessthepointcloud
information. Itisimportanttonotethatforallpre-trainedmodels,weinsertedtheAny-to-3DGuided
AdapteraftertheFFNlayersofallblocks.
6.2 TheComponentsof3D-to-anyVirtualProjection.
InTable8,wevalidatedtheimpactofdifferentprojectionmethodsandthenumberofprojectionviews
onthepre-trainedmodelsfordifferentmodalities. AsshowninTable8,forthe1D/2Dmodalities,
12Table8:AblationStudyontheComponentsof3D-to-anyVirtualProjection. The2DProj.V1and
2DProj.V2denotetheprojectionmethodsusedinPointCLIPV1Zhangetal.[2022b]andPointCLIP
V2Zhuetal.[2023],respectively. The1DProj. referstotheprojectionmethoddepictedinthemain
text. Meanwhile,weablatetheprojectionviewnumber(Num.) onthepre-trainedmodels.
2DProj.V1 2DProj.V2 1DProj. ViewNum. 1D.(%) 2D.(%)
✓ - - 4 - 86.2
✓ - - 6 - 87.7
✓ - - 8 - 87.2
- ✓ - 4 - 86.4
- ✓ - 6 - 87.1
- ✓ - 8 - 86.5
- - ✓ 4 90.6 -
- - ✓ 6 91.9 -
- - ✓ 8 89.8 -
Table 9: Ablation Study on the Different Local Neighborhood Sizes. We conduct ablation
experimentsfordifferentcombinationsof1Dlinesizes,2Dpatchsizes,and3Dgridsizes.
PatchSize LineSize GridSize 1D.(%) 2D.(%)
16 - 0.08 - 86.5
26 - 0.08 - 87.2
34 - 0.08 - 86.4
16 - 0.16 - 87.0
26 - 0.16 - 87.7
34 - 0.16 - 86.0
- 1 0.08 90.9 -
- 2 0.08 91.9 -
- 3 0.08 90.8 -
- 1 0.16 88.9 -
- 2 0.16 89.3 -
- 3 0.16 91.1 -
Table 10: Ablation Study on the 1D/2D-guided Local Aggregation. To further emphasize the
significanceof1D/2D-guidedLocalAggregation(L.A.),weconductadditionalexperimentsonthe
"PB-T50-RS"splitoftheScanObjectNNUyetal.[2019]dataset.
3D-guidedL.A. 2D-guidedL.A. 1D-guidedL.A. 1D.(%) 2D.(%)
✓ - - - 86.2
- ✓ - - 87.7
✓ - - 87.4 -
- - ✓ 91.9 -
theoptimalperformanceisobtainedwhenthenumberofviewsissetto6. Meanwhile,forthe2D
modality,thesimpleprojectioninPointCLIPZhangetal.[2022b]performsbetterthanthemore
complex projection in PointCLIP V2 Zhu et al. [2023]. The findings suggest that employing an
appropriatenumberofprojectionviewssufficientlycapturesthediversityandcomplexityinherent
in3Ddatainlow-dimensionalspaces. Furthermore,asimpleprojectionmethodprovesadequate
forrepresentingthefine-grainedstructureanddatacharacteristicsof3Dpointclouds. Itisworth
notingthatweusesimpleprojectionZhangetal.[2022b]and6viewsforallpre-trainedmodelsof
anymodalities.
13Figure7: VisualizationofDifferent3DPositionalEmbeddings. Forthe1D/2Dmodalities,we
visualizethecosinesimilaritybetweenthe3Dpositionalembeddings(PEs)ofselectedpointand
thoseofothertokensontheairplaneandlamp,respectively. Thefirstcolumnrepresentstheaveraging
of3DPEsobtainedfrom6views,whilethesubsequentcolumnsshowthe3DPEsunderdifferent
views. TheblackcircleareainthefirstcolumnrepresentstherangewhereIselectrandompoint. The
redcolorindicateshighersimilarity.
6.3 TheInfluencesofDifferentLocalNeighborhoodSizes.
InTable9,weinvestigatedtheperformanceimpactofvariouscombinationsof1Dlinesizes,2D
patchsizes,and3DgridsizesWuetal.[2022]duringthe1D/2D-guidedlocalaggregationstage. As
demonstratedinTable9,whenthe1Dlinesizeand2Dpatchsizearesettomoderatevaluesof2and
26,respectively,remarkableperformanceisattained. Thesefindingsindicatethatanappropriatelocal
aggregationsizeenhancesthemodel’scomprehensionof3Dlocalinformation,whereasexcessively
largeorsmallsizescouldleadtothelossofcriticalfeatures. Fordiversetasks,wehaveconsistently
employedsimilarlocalaggregationsizesforpre-trainedtransformersof1D/2Dmodalities.
6.4 TheImportanceof1D/2D-guidedLocalAggregation.
InTable10,wevalidatedtheadvantagesofutilizingpositionalpriorsfromthesourcemodalitiesthat
arecompatiblewiththepre-trainedmodel,byinjectingthepositionalpriorsofdifferentmodalities
(1D/2D/3D) into the local aggregation step within the Any-to-3D Guided Adapter. As shown in
Table10,comparedtousingtheoriginal3Dspatialknowledge,ourmethodexhibitsaperformance
improvement of 1.5%-4.5% for 2D/1D modalities. This demonstrates that our proposed method
effectivelyaddressestheissueofpositionaldiscrepancybetweentheoriginal3Dpositionsandthe
pre-trainedmodel.
7 AdditionalVisualization
WeconductvisualizationexperimentsonthetestsplitoftheModelNet40datasetWuetal.[2015]
utilizingtheCLIP-TextRadfordetal.[2021](1D)andDINOV2Oquabetal.[2023](2D).
7.1 TheSignificanceofEncoding3DPositionsin1D/2DPEs
Todemonstratetheeffectofourmethodthatuses1D/2DPositionalEmbeddings(PEs)fromsource
modalitiestoencode3Dpositions,werandomlyselectedonepointona3Dobject(insidetheblack
circleinthefirstcolumn)andcomputedthecosinesimilaritybetweenthepositionalembeddingof
14thepointandthoseofothertokens. Wecompared3DPEsassignedunderdifferentviews(thesix
columnsontheright)andthefinal3DPEsobtainedbyaveragingoverMviews(thefirstcolumn),
whereMissetto6. AsshowninFigure7,thefirstcolumnshowshighersimilarityinareascloser
toourselectedpoint,whileinothercolumns,highsimilarityareasarescatteredatfartherlocations.
For example, inthe firstand secondrows, weselecteda pointon thenoseof aplane, and inthe
thirdandfourthrows,weselectedapointonthebaseofalamp. Inourmethod(thefirstcolumn),
similarity values decreases with distance, showing a transition from strong to weak, whereas in
theothercolumns,thedistributionappearsirregular. Itindicatesthatourproposed3Dpositional
embeddingsimplicitlyestablishspatialrelationshipsin3Dspace.
8 Conclusion
Inconclusion,ourpaperproposesAny2Pointtoenableageneralany-to-3Dtransferringframework,
empoweringany-modalitypre-trainedlargemodels(e.g.,2Dvision,language,andaudio)forefficient
3Dunderstanding. WithinAny2Pointframework,weintroducetwotechniques,named3D-to-any
virtualprojectionandany-to-3Dguidedadapter,toextract3Dstructureknowledgewhileefficiently
fine-tuningpre-trainedmodels. Thisenablesustoovercomeissueswithincurrentmethods,such
as3Dgeometrylossandexcessiveresourcecost. Ourextensiveexperimentsacrossvarioustasks
demonstratethesuperiorperformanceandefficiencyofAny2PointcomparedtopreviousSOTA3D
pre-trainedmodels,achievingremarkableresultswithonlyafractionofthetrainableparameters.
References
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.
Adaptformer: Adaptingvisiontransformersforscalablevisualrecognition. AdvancesinNeural
InformationProcessingSystems,35:16664–16678,2022.
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nießner. Scannet: Richly-annotated3dreconstructionsofindoorscenes. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages5828–5839,2017.
MostafaDehghani,JosipDjolonga,BasilMustafa,PiotrPadlewski,JonathanHeek,JustinGilmer,
AndreasPeterSteiner,MathildeCaron,RobertGeirhos,IbrahimAlabdulmohsin,etal. Scaling
visiontransformersto22billionparameters. InInternationalConferenceonMachineLearning,
pages7480–7512.PMLR,2023.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
RunpeiDong,ZekunQi,LinfengZhang,JunboZhang,JianjianSun,ZhengGe,LiYi,andKaisheng
Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d
representationlearning? arXivpreprintarXiv:2212.08320,2022.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929,2020.
LucianoFloridiandMassimoChiriatti. Gpt-3: Itsnature,scope,limits,andconsequences. Minds
andMachines,30:681–694,2020.
RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,MannatSingh,KalyanVasudevAlwala,Armand
Joulin,andIshanMisra. Imagebind: Oneembeddingspacetobindthemall. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages15180–15190,2023.
YuanGong,Yu-AnChung,andJamesGlass. Ast: Audiospectrogramtransformer. arXivpreprint
arXiv:2104.01778,2021.
YuanGong,Cheng-ILai,Yu-AnChung,andJamesGlass. Ssast: Self-supervisedaudiospectrogram
transformer. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume36,pages
10699–10709,2022.
15ZiyuGuo,RenruiZhang,LongtianQiu,XianzhiLi,andPheng-AnnHeng. Joint-mae: 2d-3djoint
maskedautoencodersfor3dpointcloudpre-training. arXivpreprintarXiv:2302.14007,2023a.
ZiyuGuo,RenruiZhang,XiangyangZhu,YiwenTang,XianzhengMa,JiamingHan,KexinChen,
Peng Gao, Xianzhi Li, Hongsheng Li, et al. Point-bind & point-llm: Aligning point cloud
withmulti-modalityfor3dunderstanding,generation,andinstructionfollowing. arXivpreprint
arXiv:2309.00615,2023b.
ZoeyGuo,YiwenTang,RayZhang,DongWang,ZhigangWang,BinZhao,andXuelongLi. Viewre-
fer: Graspthemulti-viewknowledgefor3dvisualgrounding. In2023IEEE/CVFInternational
ConferenceonComputerVision(ICCV),pages15326–15337.IEEEComputerSociety,2023c.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencodersarescalablevisionlearners.InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages16000–16009,2022.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,
AndreaGesmundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransferlearningfor
nlp. InInternationalConferenceonMachineLearning,pages2790–2799.PMLR,2019.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021.
MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie,SergeBelongie,BharathHariharan,and
Ser-NamLim. Visualprompttuning. InEuropeanConferenceonComputerVision,pages709–727.
Springer,2022.
XiaoqiLi,MingxuZhang,YiranGeng,HaoranGeng,YuxingLong,YanShen,RenruiZhang,Jiaming
Liu,andHaoDong. Manipllm: Embodiedmultimodallargelanguagemodelforobject-centric
roboticmanipulation. arXivpreprintarXiv:2312.16217,2023.
JiamingLiu,SenqiaoYang,PeidongJia,MingLu,YandongGuo,WeiXue,andShanghangZhang.
Vida: Homeostatic visual domain adapter for continual test time adaptation. arXiv preprint
arXiv:2306.04344,2023.
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.
P-tuningv2: Prompttuningcanbecomparabletofine-tuninguniversallyacrossscalesandtasks.
arXivpreprintarXiv:2110.07602,2021.
YinhanLiu, MyleOtt, Naman Goyal, JingfeiDu, MandarJoshi, DanqiChen, OmerLevy, Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach. arXivpreprintarXiv:1907.11692,2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local
geometryinpointcloud: Asimpleresidualmlpframework. arXivpreprintarXiv:2202.07123,
2022.
OpenAI. GPT-4technicalreport,2023.
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learning
robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
MingjiePan,JiamingLiu,RenruiZhang,PeixiangHuang,XiaoqiLi,LiLiu,andShanghangZhang.
Renderocc: Vision-centric3doccupancypredictionwith2drenderingsupervision. arXivpreprint
arXiv:2309.09502,2023.
Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked
autoencodersforpointcloudself-supervisedlearning. InEuropeanconferenceoncomputervision,
pages604–621.Springer,2022.
16CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas. Pointnet: Deeplearningonpointsets
for3dclassificationandsegmentation. InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pages652–660,2017a.
CharlesRuizhongtaiQi,LiYi,HaoSu,andLeonidasJGuibas. Pointnet++:Deephierarchicalfeature
learningonpointsetsinametricspace. Advancesinneuralinformationprocessingsystems,30,
2017b.
ZekunQi,RunpeiDong,GuofanFan,ZhengGe,XiangyuZhang,KaishengMa,andLiYi. Contrast
withreconstruct: Contrastive3drepresentationlearningguidedbygenerativepretraining. arXiv
preprintarXiv:2302.02318,2023.
GuochengQian,YuchenLi,HouwenPeng,JinjieMai,HasanHammoud,MohamedElhoseiny,and
BernardGhanem. Pointnext: Revisitingpointnet++withimprovedtrainingandscalingstrategies.
AdvancesinNeuralInformationProcessingSystems,35:23192–23204,2022.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pages
8748–8763.PMLR,2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. TheJournalofMachineLearningResearch,21(1):5485–5551,2020.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pages10684–10695,2022.
YiwenTang,RayZhang,ZoeyGuo,XianzhengMa,BinZhao,ZhigangWang,DongWang,and
XuelongLi. Point-peft: Parameter-efficientfine-tuningfor3dpre-trainedmodels. InProceedings
oftheAAAIConferenceonArtificialIntelligence,volume38,pages5171–5179,2024.
HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHervé
Jégou. Trainingdata-efficientimagetransformers&distillationthroughattention. InInternational
conferenceonmachinelearning,pages10347–10357.PMLR,2021.
Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung.
Revisitingpointcloudclassification: Anewbenchmarkdatasetandclassificationmodelonreal-
worlddata. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages
1588–1597,2019.
YueWang,YongbinSun,ZiweiLiu,SanjayESarma,MichaelMBronstein,andJustinMSolomon.
Dynamicgraphcnnforlearningonpointclouds. ACMTransactionsonGraphics(tog),38(5):
1–12,2019.
ZiyiWang, XuminYu, YongmingRao, JieZhou, andJiwenLu. P2p: Tuningpre-trainedimage
modelsforpointcloudanalysiswithpoint-to-pixelprompting. Advancesinneuralinformation
processingsystems,35:14388–14402,2022.
Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer.
Maskedfeaturepredictionforself-supervisedvisualpre-training. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages14668–14678,2022.
Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2:
Groupedvectorattentionandpartition-basedpooling. AdvancesinNeuralInformationProcessing
Systems,35:33330–33342,2022.
ZhirongWu,ShuranSong,AdityaKhosla,FisherYu,LinguangZhang,XiaoouTang,andJianxiong
Xiao. 3dshapenets: Adeeprepresentationforvolumetricshapes. InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition,pages1912–1920,2015.
17ZhendaXie,ZhengZhang,YueCao,YutongLin,JianminBao,ZhuliangYao,QiDai,andHanHu.
Simmim: Asimpleframeworkformaskedimagemodeling. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages9653–9663,2022.
ChenfengXu,ShijiaYang,TomerGalanti,BichenWu,XiangyuYue,BohanZhai,WeiZhan,Peter
Vajda,KurtKeutzer,andMasayoshiTomizuka.Image2point:3dpoint-cloudunderstandingwith2d
imagepretrainedmodels. InEuropeanConferenceonComputerVision,pages638–656.Springer,
2022.
Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu,
JuanCarlosNiebles,andSilvioSavarese. Ulip: Learningaunifiedrepresentationoflanguage,
images,andpointcloudsfor3dunderstanding. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages1179–1189,2023.
SenqiaoYang,JiamingLiu,RayZhang,MingjiePan,ZoeyGuo,XiaoqiLi,ZehuiChen,PengGao,
YandongGuo,andShanghangZhang. Lidar-llm: Exploringthepotentialoflargelanguagemodels
for3dlidarunderstanding. arXivpreprintarXiv:2312.14074,2023.
SenqiaoYang,JiaruiWu,JiamingLiu,XiaoqiLi,QizheZhang,MingjiePan,YuluGan,ZehuiChen,
andShanghangZhang. Exploringsparsevisualpromptfordomainadaptivedenseprediction. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages16334–16342,
2024.
Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-
training3dpointcloudtransformerswithmaskedpointmodeling. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages19313–19322,2022.
RenruiZhang,ZiyuGuo,PengGao,RongyaoFang,BinZhao,DongWang,YuQiao,andHong-
shengLi. Point-m2ae: multi-scalemaskedautoencodersforhierarchicalpointcloudpre-training.
Advancesinneuralinformationprocessingsystems,35:27061–27074,2022a.
RenruiZhang,ZiyuGuo,WeiZhang,KunchangLi,XupengMiao,BinCui,YuQiao,PengGao,and
HongshengLi. Pointclip: Pointcloudunderstandingbyclip. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages8552–8562,2022b.
RenruiZhang,JiamingHan,ChrisLiu,PengGao,AojunZhou,XiangfeiHu,ShilinYan,PanLu,
HongshengLi,andYuQiao.Llama-adapter:Efficientfine-tuningoflanguagemodelswithzero-init
attention. arXivpreprintarXiv:2303.16199,2023a.
RenruiZhang,LiuhuiWang,YuQiao,PengGao,andHongshengLi. Learning3drepresentations
from 2d pre-trained models via image-to-point masked autoencoders. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages21769–21780,2023b.
RenruiZhang,LiuhuiWang,YaliWang,PengGao,HongshengLi,andJianboShi. Startingfrom
non-parametricnetworksfor3dpointcloudanalysis. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages5344–5353,2023c.
XiangyangZhu,RenruiZhang,BoweiHe,ZiyuGuo,ZiyaoZeng,ZipengQin,ShanghangZhang,
andPengGao. Pointclipv2: Promptingclipandgptforpowerful3dopen-worldlearning. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages2639–2650,
2023.
XiangyangZhu, RenruiZhang, BoweiHe, ZiyuGuo, JiamingLiu, HanXiao, ChaoyouFu, Hao
Dong,andPengGao. Notimetotrain: Empoweringnon-parametricnetworksforfew-shot3d
scenesegmentation. arXivpreprintarXiv:2404.04050,2024.
18