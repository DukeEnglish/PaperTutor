[
    {
        "title": "GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo",
        "authors": "Jiang WuRui LiHaofei XuWenxun ZhaoYu ZhuJinqiu SunYanning Zhang",
        "links": "http://arxiv.org/abs/2404.07992v1",
        "entry_id": "http://arxiv.org/abs/2404.07992v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07992v1",
        "summary": "Matching cost aggregation plays a fundamental role in learning-based\nmulti-view stereo networks. However, directly aggregating adjacent costs can\nlead to suboptimal results due to local geometric inconsistency. Related\nmethods either seek selective aggregation or improve aggregated depth in the 2D\nspace, both are unable to handle geometric inconsistency in the cost volume\neffectively. In this paper, we propose GoMVS to aggregate geometrically\nconsistent costs, yielding better utilization of adjacent geometries. More\nspecifically, we correspond and propagate adjacent costs to the reference pixel\nby leveraging the local geometric smoothness in conjunction with surface\nnormals. We achieve this by the geometric consistent propagation (GCP) module.\nIt computes the correspondence from the adjacent depth hypothesis space to the\nreference depth space using surface normals, then uses the correspondence to\npropagate adjacent costs to the reference geometry, followed by a convolution\nfor aggregation. Our method achieves new state-of-the-art performance on DTU,\nTanks & Temple, and ETH3D datasets. Notably, our method ranks 1st on the Tanks\n& Temple Advanced benchmark.",
        "updated": "2024-04-11 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07992v1"
    },
    {
        "title": "Connecting NeRFs, Images, and Text",
        "authors": "Francesco BalleriniPierluigi Zama RamirezRoberto MirabellaSamuele SaltiLuigi Di Stefano",
        "links": "http://arxiv.org/abs/2404.07993v1",
        "entry_id": "http://arxiv.org/abs/2404.07993v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07993v1",
        "summary": "Neural Radiance Fields (NeRFs) have emerged as a standard framework for\nrepresenting 3D scenes and objects, introducing a novel data type for\ninformation exchange and storage. Concurrently, significant progress has been\nmade in multimodal representation learning for text and image data. This paper\nexplores a novel research direction that aims to connect the NeRF modality with\nother modalities, similar to established methodologies for images and text. To\nthis end, we propose a simple framework that exploits pre-trained models for\nNeRF representations alongside multimodal models for text and image processing.\nOur framework learns a bidirectional mapping between NeRF embeddings and those\nobtained from corresponding images and text. This mapping unlocks several novel\nand useful applications, including NeRF zero-shot classification and NeRF\nretrieval from images or text.",
        "updated": "2024-04-11 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07993v1"
    },
    {
        "title": "GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh",
        "authors": "Jing WenXiaoming ZhaoZhongzheng RenAlexander G. SchwingShenlong Wang",
        "links": "http://arxiv.org/abs/2404.07991v1",
        "entry_id": "http://arxiv.org/abs/2404.07991v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07991v1",
        "summary": "We introduce GoMAvatar, a novel approach for real-time, memory-efficient,\nhigh-quality animatable human modeling. GoMAvatar takes as input a single\nmonocular video to create a digital avatar capable of re-articulation in new\nposes and real-time rendering from novel viewpoints, while seamlessly\nintegrating with rasterization-based graphics pipelines. Central to our method\nis the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering\nquality and speed of Gaussian splatting with geometry modeling and\ncompatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and\nvarious YouTube videos. GoMAvatar matches or surpasses current monocular human\nmodeling algorithms in rendering quality and significantly outperforms them in\ncomputational efficiency (43 FPS) while being memory-efficient (3.63 MB per\nsubject).",
        "updated": "2024-04-11 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07991v1"
    },
    {
        "title": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models",
        "authors": "Moreno D'IncàElia PeruzzoMassimiliano ManciniDejia XuVidit GoelXingqian XuZhangyang WangHumphrey ShiNicu Sebe",
        "links": "http://arxiv.org/abs/2404.07990v1",
        "entry_id": "http://arxiv.org/abs/2404.07990v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07990v1",
        "summary": "Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.",
        "updated": "2024-04-11 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07990v1"
    },
    {
        "title": "Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding",
        "authors": "Yiwen TangJiaming LiuDong WangZhigang WangShanghang ZhangBin ZhaoXuelong Li",
        "links": "http://arxiv.org/abs/2404.07989v1",
        "entry_id": "http://arxiv.org/abs/2404.07989v1",
        "pdf_url": "http://arxiv.org/pdf/2404.07989v1",
        "summary": "Large foundation models have recently emerged as a prominent focus of\ninterest, attaining superior performance in widespread scenarios. Due to the\nscarcity of 3D data, many efforts have been made to adapt pre-trained\ntransformers from vision to 3D domains. However, such 2D-to-3D approaches are\nstill limited, due to the potential loss of spatial geometries and high\ncomputation cost. More importantly, their frameworks are mainly designed for 2D\nmodels, lacking a general any-to-3D paradigm. In this paper, we introduce\nAny2Point, a parameter-efficient method to empower any-modality large models\n(vision, language, audio) for 3D understanding. Given a frozen transformer from\nany source modality, we propose a 3D-to-any (1D or 2D) virtual projection\nstrategy that correlates the input 3D points to the original 1D or 2D positions\nwithin the source modality. This mechanism enables us to assign each 3D token\nwith a positional encoding paired with the pre-trained model, which avoids 3D\ngeometry loss caused by the true projection and better motivates the\ntransformer for 3D learning with 1D/2D positional priors. Then, within each\ntransformer block, we insert an any-to-3D guided adapter module for\nparameter-efficient fine-tuning. The adapter incorporates prior spatial\nknowledge from the source modality to guide the local feature aggregation of 3D\ntokens, compelling the semantic adaption of any-modality transformers. We\nconduct extensive experiments to showcase the effectiveness and efficiency of\nour method. Code and models are released at\nhttps://github.com/Ivan-Tang-3D/Any2Point.",
        "updated": "2024-04-11 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.07989v1"
    }
]