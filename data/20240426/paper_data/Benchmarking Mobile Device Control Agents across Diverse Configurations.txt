ICLR2024WorkshoponGenerativeModelsforDecisionMaking
BENCHMARKING MOBILE DEVICE CONTROL AGENTS
ACROSS DIVERSE CONFIGURATIONS
JuyongLee1 TaywonMin2 MinyongAn3 ChangyeonKim1 KiminLee1
1KAIST 2SeoulNationalUniversity 3YonseiUniversity
ABSTRACT
Developingautonomousagentsformobiledevicescansignificantlyenhanceuser
interactionsbyofferingincreasedefficiencyandaccessibility. However,despite
thegrowinginterestinmobiledevicecontrolagents,theabsenceofacommonly
adoptedbenchmarkmakesitchallengingtoquantifyscientificprogressinthisarea.
In this work, we introduce B-MoCA: a novel benchmark designed specifically
forevaluatingmobiledevicecontrolagents. Tocreatearealisticbenchmark,we
developB-MoCAbasedontheAndroidoperatingsystemanddefine60common
daily tasks. Importantly, we incorporate a randomization feature that changes
variousaspectsofmobiledevices,includinguserinterfacelayoutsandlanguage
settings, to assess generalization performance. We benchmark diverse agents,
includingagentsemployinglargelanguagemodels(LLMs)ormulti-modalLLMs
aswellasagentstrainedfromscratchusinghumanexpertdemonstrations. While
these agents demonstrate proficiency in executing straightforward tasks, their
poorperformanceoncomplextaskshighlightssignificantopportunitiesforfuture
researchtoenhancetheireffectiveness. Oursourcecodeispubliclyavailableat
https://b-moca.github.io.
1 INTRODUCTION
Autonomous agents controlling digital devices have great potential benefits. For example, these
agentscanimprovetheaccessibilityofuserinteractions,especiallyforuserswithphysicaldisabilities
orthosefacingchallengesinoperatingdevices,orboostproductivitybyautomatingtediousjobs.
Thisleadstoincreasedinterestindevelopingagentsformobiledevicecontrol,anddiverseapproaches
havebeenintroduced,includingagentsbasedonlargelanguagemodels(LLMs;Wenetal.2023;Yan
etal.2023)andagentstrainedwithhumandemonstrations(Sunetal.,2022;Lietal.,2023),toward
assistiveagentsthatcanunderstandthescreenlayoutofthedevicesandmanipulatetheuserinterface
(UI)tofollowhumaninstructions.
Despiterecentprogressindevelopingmobiledevicecontrolagentsbasedonrealsystems,suchas
Androidemulators(Toyamaetal.,2021;Shvoetal.,2021;Zhangetal.,2023),priorworksoften
overlookseveralimportantproperties. Oneistestinggeneralizationabilityacrossdiversedevice
configurations,whichiscrucialindeployingagentsinrealdevices.Moreover,practicaltasksessential
forlife(suchascreatinganalarmormakingemergencycalls)areoftenneglectedbecauseofthe
challengesindefiningawiderangeofpracticaltaskswithrobustsuccesscriteriainvariousdevice
settings. Thelackofaunifiedbenchmarkencompassingtheseimportantpropertieshasimpeded
scientificprogressinthisfield.
Inthiswork,weintroduceB-MoCA:aBenchmarkdesignedforevaluatingMobiledeviceControl
Agentsacrossdiverseconfigurations,basedonAndroidemulators(seeFigure1). Akeyfeatureof
B-MoCAissupportingnumerouscustomizationtomirrordiversedeviceconfigurations,including
variationsiniconplacements,sizes,wallpapers,languages,anddevicetypes. Utilizingthisfeature,
userscaneasilycreatediverseenvironmentswithvariousconfigurationstoevaluategeneralization
ability. Additionally,wedefine60practicaltasksgroundedinrealisticscenarios,suchasopening
specificapplications,initializingsearchesovertheweb,andadjustingdevicesettings. Toensure
reliableevaluation,B-MoCAprovidesrule-basedsuccessdetectors,whicharebasedonpre-defined
taskcompletioncriteria.
1
4202
rpA
52
]CH.sc[
1v06661.4042:viXraICLR2024WorkshoponGenerativeModelsforDecisionMaking
LLM with
Text Action
or
MLLM with
Daily Tasks
Text Action
or
Mobile Device Mobile Device
Vision-Language Control Agent Environment
Model with
UI-based Action
Algorithmic Designs Diverse Device Setups
Figure1: IllustrationofB-MoCA.Wepresentarealisticbenchmarkforassessingtheperformances
ofmobiledevicecontrolagentsinexecutingeverydaytasks. Toanalyzegeneralizationability,we
introduce a randomization feature that changes various device attributes. We benchmark agents
leveragingLLMsorMLLMsaswellasagentswithvision-languagemodelstrainedfromscratch.
WebenchmarkvariousmethodsforbuildingmobiledevicecontrolagentsinB-MoCA.Thebaselines
includeagentsemployingtext-onlylargelanguagemodels(LLMs)ormulti-modalLLMs(MLLMs),
whichbenefitfromextensiveknowledgeobtainedthroughpre-training. Weconsiderbothclosed-
sourcemodels,suchasGPT-4(Achiametal.,2023)andGemini(Geminietal.,2023),andopen-
sourcemodels,suchasLlama2(Touvronetal.,2023)andLlama3. Additionally,wetrainagents
fromscratchthatdirectlyinteractwithdeviceUIsusingbehaviorcloning(BC;Pomerleau1988).
Inourexperiments,wefindthattheagentsexhibitfundamentalskillsinmobiledevicecontrol,such
assolvingstraightforwardtasksorcompletingtasksintrainingenvironments. However,theystruggle
inmorechallengingscenarios,suchashandlingmoredifficulttasksorgeneralizingtounseendevice
configurations. Specifically,theagentsemployingLLMsorMLLMsshowhighrobustnessacross
diversedeviceconfigurations,whiletheyfallshortonmultiplesequentialdecision-making. Agents
trained with BC, on the other hand, successfully mimic expert behaviors but lack generalization
abilityintestenvironmentswithunseendeviceconfigurations. Westudytheeffectofdifferentdesign
choices on leveraging foundation models, including few-shot learning and the visual prompting
method. Wealsoanalyzetheeffectofusingpre-trainedrepresentationmodelsorutilizingdifferent
numbersoftrainingdeviceenvironmentswhiletrainingagentsfromscratch. Ourextensiveanalyses
revealthelimitationsofexistingmethodsinmobiledevicecontrol,callingforfutureresearch.
Weopen-sourceallthesourcecodesandrelevantmaterialsforeasyreproductionofourenvironments
andexperiments. WehopeB-MoCAhelpsfutureresearchersidentifychallengesinbuildingassistive
agentsandeasilycomparetheefficacyoftheirmethodsoverthepriorwork.
2 B-MOCA
Inthissection,weintroduceB-MoCA:abenchmarkdesignedtoevaluatetheperformanceofmobile
devicecontrolagentsondiversedeviceconfigurationsinexecutingcommondailytasks.
2.1 DESIGNFACTORS
Tocreatearealisticbenchmarkformobiledevicecontrolagents,webuildourbenchmarkbasedon
Android,awidelyusedopen-sourceoperatingsystem. Inthisbenchmark,weframedevicecontrol
as a sequential decision-making problem, reflecting the multi-step nature of the real interactions
(Section 2.2). Designing a meaningful benchmark for mobile device control poses a significant
challenge, particularly in defining practical tasks like opening applications or adjusting device
settings. Toaddressthis,weconsider60basictasksthatinvolvecommonlyusedapplicationslike
ChromeandCalendar,ensuringrelevancetoeverydaylife. Eachtaskisequippedwithasuccess
detectortoevaluatetheagent’sperformanceinaccuratelycompletingthetask(Section2.3).
2ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Figure2: ExamplesofthehomescreenimagesfromenvironmentsinB-MoCA.Therandomized
features span icon location, font size, wallpaper, language, and device type and challenge the
generalizationabilityofagents.
Giventhediversenatureofusermobiledevicesetups,suchasvariationsiniconplacements,wall-
paper choices, languages, and device types, it is important to test the generalization abilities of
device-controlagentsacrossdiversesetups. Toassessgeneralizationperformance,weincorporate
arandomizationfeatureinourbenchmark. Thisfeatureisdesignedtosimulatevariousreal-world
scenariosbychangingvariousaspectsofmobiledevices,suchasuserinterfacelayoutsandwallpapers
(Section2.4).
2.2 PROBLEMFORMULATION
InB-MoCA,weformulatethedevicemanagementtaskasasequentialdecision-makingproblem,
whereanagentinteractswithanenvironment. Formally,givenataskinstructionc,anagentreceives
anobservationo andtakesanactiona basedonitspolicya ∼π(·|o ,c)ateachtimestept. The
t t t t
environment(i.e.,Androidemulator)returnsasuccesssignalr andtheenvironmenttransitionsto
t
thenextobservationo .
t+1
Observations, which capture the UI elements, can be represented as either screen pixels, screen
descriptionsderivedfromtheAndroidviewhierarchy,oracombinationofboth. Theactionspace
comprisesadual-gesture,similartoRawlesetal.(2023),whichconsistsofapairof(x,y)screen
locationsfortouchandlift. Thedual-gestureactionisidentifiedastappingthescreenwhen
thetwolocationsareidenticalwithinaspecifiedthresholdorswipingthescreenwhenthedistance
betweenthetwolocationsexceedsthisthreshold. Additionally,theagentcanpressnavigationbuttons
(i.e.,back,home,andoverview)bytouchingthecorrespondingbuttonlocationsonthescreen. We
notethatourbenchmarksupportstext-basedactions,enablingtheutilizationoftheLLMsorMLLMs
(seeSection3.1fordetails).
WereferthereadersforfurtherdetailsontheenvironmentimplementationtoAppendixA.1.
2.3 DAILYTASKS
Our B-MoCA includes 60 tasks essential for managing digital devices, providing functionalities
usefulindailyroutines.Eachtaskisdesignedtobegroundedinrealisticsituations,suchassettingthe
alarmorenablingairplanemode. Thetasksspanvariousapplicationsandrequireagentstointeract
withdiverseUIelements,suchasapplicationicons,checkboxes,toggleswitches,inputfields,and
sliders. Foracomprehensivelistoftasks,wereferreaderstoAppendixB.1.
Taskcompletionisdeterminedbyarule-basedsuccessdetectorimplementedusingAndroidDebug
Bridge(ADB).ThissuccessdetectormonitorslogsfromADBandidentifiesthesuccessfulcompletion
basedonpre-definedcriteria. ThesecriteriaareestablishedbyexaminingADBlogsfromhuman
demonstrationsforeachtaskandselectingthelogproducedwhenthetargettaskiscompleted. With
thepre-definedcriteria,then,thesuccessdetectorautomaticallyfindsthematchingregularexpression
intheADBlogstosignalthetaskcompletion. Thesuccesssignaliswiththevalueof+1whenthe
taskiscompleted,and0otherwise. Anepisodeterminatesasasuccessifthesuccessdetectorsignals
completion,orasafailureiftheagentexceedsamaximumsteplimitwithoutmeetingthecriteria.
3ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Task instruction
XML Text observation
Parser Large Language Model
Action Agents
Converter Text action
Multimodal
XML observation Multimodal
Parser Large Language Model
Action Agents
Converter Text action
Image observation
Vision-Language-UI
Agents
Dual-gesture action
Figure3: Illustrationofbaselineagents. LLMagentsandMLLMagentsinteractwithenvironments
throughadditionalXMLparserandactionconverter,toobtaintextdescriptionsandmanipulateUIs
withtextactions. VLUIagentsdirectlyleveragetheUIswithscreenimagesanddual-gestureactions.
2.4 ENVIRONMENTRANDOMIZATION
Inmobiledevicecontrol,developingagentsthatcangeneralizeacrossvariousdevicesetupsiscrucial.
Toevaluatetheirgeneralizationability,B-MoCAincorporatesarandomizationfeaturethatchanges
iconplacementsandsizes,wallpapers,languages,anddevicetypes. Userscanselectthedevicetype
fromadevicelistthatincludespopularmodelslikePixel3,Pixel4,Pixel6,andWGXATablet. They
canalsospecifythelocalestosetthelanguageandregion,choosewallpapersfromaselectionof
customimages,andactivatedarkmodeforfurtherenvironmentalvariation. Moreover,thesizesof
iconsandtextcanvarybetweensmall, medium, andlarge. Lastly, applicationscanberandomly
placedonthehomescreentosimulatereal-worldusagepatterns.
Usingrandomizationfeatures,wecreate45uniqueenvironmentsinB-MoCA,withexamplesshown
inFigure2. Toassessthegeneralizationability, wedividethe45distinctenvironmentsintotwo
sets: 35fortrainingand10fortesting. Weemploydomainrandomization(Tobinetal.,2017)to
trainagents,enablingthemtoperformtasksrobustlyacrossdiversedeviceconfigurations. Wethen
evaluatetheperformanceontestenvironments,whichincludeunseendevicesetups. Adetailedlistof
environmentdeviceconfigurationsweprepareisavailableinAppendixA.2.
3 BASELINES
Inthiswork,webenchmarkvariousapproachesforbuildingmobiledevicecontrolagents: LLM
agents, MLLMagents, andVision-Language-UI(VLUI)agents(seeFigure3). LLMagentsand
MLLMagentsaredevelopedusingfoundationmodelslikeLLMsandMLLMs,respectively(Sec-
tion3.1). VLUIagents,whichconsistofvision-languageencoders,aretrainedfromscratchusing
humanexpertdemonstrations(Section3.2).
3.1 LLMAGENTSANDMLLMAGENTS
UtilizingfoundationmodelssuchasLLMsandMLLMs,whichcontainextensiveknowledgeandhave
emergentcapabilities,becomesamajordirectionindevelopingmobiledevicecontrolagents(Wen
etal.,2023;Yanetal.,2023). Inthiswork,webenchmarktwotypesofagentsthatemploydifferent
foundationmodels: LLMs(e.g.,GPT-4)andMLLMs(e.g.,GPT-4V).LLMagentsutilizeonlythe
textdescriptionsofthescreenlayouttogeneratetextactions,whileMLLMagentsprocessbothtext
andvisualinputs.
TofacilitatetheinteractionsofLLMandMLLMagentswithanAndroidemulator,wedefineanXML
parser(Zhangetal.,2023;Yangetal.,2023b). ThisXMLparserconvertstheUIelements,fromthe
AndroidviewhierarchyofthescreenpresentedinXMLformat,intoalistoftextdescriptions. The
descriptionincludesthelocationoftheboundingbox,ifnecessary. Additionally,wedefineasetof
possibleactionoptions,asdetailedinTable1,thatcanbeconvertedintoacorrespondingdual-gesture
4ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Actionoption Description Role:Youareanagentthatistrainedtoperformdailytasks
ondigitaldevices,suchassmartphones[...]
Operateadual-gestureaction
dual-gesture(*) witharguments(*). Actionspace:Youneedtoselectanactionoption[...]
Goal:[...]
TapUIelementlabeled
tap(numeric tag)
withnumeric tag. (Optional)Few-shotexamples:[...]
swipe(direction) Swipetodirection. Outputformat:Youroutputshouldfollowthegivenformat
•Description:Describewhatyouobserveintheinput
press("HOME") Presshomebutton.
•Thought:Tocompletethegiventask,whatisthenextstep
press("BACK") Pressbackbutton. •Action:Thefunctioncallwiththecorrectparameters
press("OVERVIEW") Pressoverviewbutton. Observation:[...]
Table1: Asetofactionoptionsfortext-based Figure4:Anoverviewofpromptforthetext-based
agents. Additionaloptionsareconvertedinto agents, with abbreviated relevant information as
correspondingdual-gestureactions. [...]. ThecompletepromptisatAppendixC.1.
action.1 TheseactionoptionsincludetappingtheUIelementbychoosingthenumerictags,swiping
thescreeninpre-defineddirections(up,down,left,right),andpressingthebuttonwiththenames.
With these text-based observations and actions, we prompt the foundation models to explain the
agents’role,actionspacedefinition,goal,(optional)few-shotexamples,andcurrentobservation. Our
prompts,outlinedinFigure4,alsoincorporatetheChain-of-Thoughttechnique(Weietal.,2022)to
enhancethereasoningabilityoftheagentsbyenforcingacertainoutputformat.
3.2 VLUIAGENTS
DespitethepromisingresultsofLLMs,leveragingthesefoundationmodelspresentsseveralchal-
lenges such as the necessity of auxiliary interfaces or difficulties in fine-tuning. Thus, we also
investigateanothertypeofagentthatcanbetrainedfromscratch: VLUIagents, namedafterthe
1
vision-languagemodelwithUIactions. CharacterizedbytheirdirectinteractionwithdeviceUIsin
ahuman-likemanner,theseagentscansignificantlybenefitfromtheeasyincorporationofhuman
demonstrationsfortraining,potentiallyimprovinglearningefficiency.
Tobedetailed,VLUIagentstakeataskinstructionandscreenimagesastheinputandproducea
dual-gestureactionastheoutput. Inputembeddingsareextractedusingvisionandlanguageencoders
andatransformer(Vaswanietal.,2017)moduleisutilizedtoprocesstheseembeddingsandgenerate
thedual-gestureactions. Specifically,wetrainadeterministicmulti-taskpolicyπ (a |o ,c)using
θ t t
BC(Pomerleau1988;Schaal1996). Theparametersθofthepoliciesareoptimizedtoimitatethe
humanexpertdemonstrationsD ={(o ,a∗,c)}byminimizingthefollowingobjectivewithmean
t t
squarederrorfunctionL(·):
(cid:88)
L(π (a |o ,c),a∗).
θ t t t
(ot,a∗ t,c)∼D
WereferreaderstoAppendixC.2formoredetailsonthearchitectureofVLUIagents.
4 EXPERIMENTS
Wedesignourexperimentstoinvestigatethefollowingresearchquestions:
• Canbaselineagentsperformdailytasksonmobiledevices? (Section4.2)
• Whatarethedistinctivecharacteristicsofeachagent? (Section4.2)
• WhataretheeffectsofdifferentdesignchoicesforLLMorMLLMagents? (Section4.3)
• Howcrucialisthepre-trainingortrainingdatadiversityforVLUIagents? (Section4.4)
1Toconverttextactionstodual-gestureactions,wedefinetheactionconverter.Weanalyzetheefficacyof
theactionoptionsinAppendixE.1.
5ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Airplane Alarm1 Alarm2
LLM (Gemini-Pro)
LLM (GPT-4)
MLLM (Gemini-Pro-V)
MLLM (GPT-4V)
VLUI
0 30 60 90 0 30 60 90 0 30 60 90
Success Rates (%) Success Rates (%) Success Rates (%)
Brightness Call 911 Language
LLM (Gemini-Pro)
LLM (GPT-4)
MLLM (Gemini-Pro-V)
MLLM (GPT-4V)
VLUI
0 30 60 90 0 30 60 90 0 30 60 90
Success Rates (%) Success Rates (%) Success Rates (%)
Figure5: Averagesuccessratesofthebaselineagentsinthetestenvironments. Wereportthemean
andstandarderroracrossthreeruns. LLMagentsareinthree-shotlearning,andMLLMagentsare
withoutSoMpromptingandinone-shotlearningforGemini-Pro-V(duetomaximumcontext
length)orthree-shotlearningforGPT-4V.Thetext-basedagentswithGPT-4orGPT-4Vshowthe
bestperformancesonAirplane,Alarm1,andBirhgtness,whileVLUIagentsshowbetter
performancesonAlarm2,Call 911,andLanguage.
4.1 EXPERIMENTALSETUP
Inourexperiments,weevaluateLLMagents,MLLMagents,andVLUIagentsusingsixrepresenta-
tivetasks: named,Airplane,Alarm1,Alarm2,Brightness,Call 911,andLanguage.
Thesetasksareselectedtocovernavigatingmultiplepagesintargetapplicationsandmanipulating
diverseUIelementswhichvaryinconfigurationacrossdifferentdevicesettings. Forexample,on
Alarm2,theagentsneedtoreachthealarmtabintheclockapplicationandadapttovaryingshapes
ofclockUIinashapeofeitherrectangleorcirclewithdifferentsizeoptions. Wedisplayexemplary
expertdemonstrationsonthesetasksinAppendixB.2.Foreachtask,thetaskinstructionisasfollows:
• Airplane:“turnonairplanemode” • Brightness:“decreasethescreenbrightnessinsetting”
• Alarm1:“turnonalarmat9am” • Call 911:“call911”
• Alarm2:“createanalarmat10:30am” • Language:“gotothe‘addalanguage’pageinsetting”
ForLLMagents,weemploytheclosed-sourcemodelsGemini-Pro(Geminietal.,2023)andGPT-4
(GPT-4-0125-preview; Achiam et al. 2023).2 We study LLM agents with both zero-shot
andfew-shotlearningcases. Forfew-shotlearning,wesampleexamplesfrom210humanexpert
demonstrations(seeAppendixD.1fordatasetcollection). ForMLLMagents,weleverageGemini-
Pro-V and GPT-4V (GPT-4-vision-preview). We report MLLM agents in only few-shot
learningandinvestigatevisuallygroundingtheagentswithSet-of-Mark(SoM)prompting(Yang
et al., 2023a). We provide more details on the configurations for LLM and MLLM Agents in
AppendixC.3. ForVLUIagents,wetrainmulti-taskpolicieswhereeachpolicyperformsallsixtasks.
ThepoliciesaretrainedwithBCusingthe210humanexpertdemonstrations.3 Wereferthereaders
toAppendixC.4formoredetailsonthetrainingproceduresofVLUIagents.
For each evaluation, we measure the success rates of the agents in the 10 test environments and
computetheaveragesuccessrates. Thesesuccessratesareautomaticallycomputedbytherule-based
successdetector. Wereportthemeanandstandarderroracrossthreedifferentruns.
4.2 MAINRESULTS
Figure 5 shows the success rates of LLM agents, MLLM agents, and VLUI agents in test envi-
ronments. LLMagentsandMLLMagentsutilizetheirpre-trainedbaseknowledgeandfew-shot
2We include experiments with open-source models of Llama 2 (Touvron et al., 2023), Llama 3, and
AgentLM(Zengetal.,2023)inAppendixE.2.
3WealsoincludeexperimentalresultsofVLUIagentstrainedwithofflinereinforcementlearningbyemploy-
ingthesuccesssignalsasrewardsinAppendixE.3.
6ICLR2024WorkshoponGenerativeModelsforDecisionMaking
LLM LLM MLLM MLLM
(zero-shot) (few-shot) (w/oSoM) (w/SoM)
Desired
Action Airplane 53±03 73±12 80±06 83±03
Alarm1 42±13 67±03 60±15 62±09
Alarm2 00±00 00±00 23±03 17±03
Executed
Action Brightness 73±12 73±09 87±03 83±03
Call 911 00±00 03±03 53±03 33±09
Language 27±06 43±09 43±09 47±17
(a) (b) (c)
Table2: Successratesoftext-basedagentswith
Figure6:Thecommonfailuremodesoftheagents. different prompting methods. While few-shot
(a)LLMagentsfailtocompletesequentialsteps, exampleshelpLLMagentswithGPT-4,weob-
(b)MLLMagentsmissdetailsintheimages,and servenosignificantgainfromSoMprompting
(c)VLUIagentstapthewrongiconlocations. forMLLMagentswithGPT-4V.
examplestocompletesimpletaskswithhighperformances(e.g.,morethan70%onAirplaneand
BrightnesswithGPT-4orGPT-4V),buttheirsuccessratessignificantlydropasthetasksbecome
complex(e.g.,lessthan30%onAlarm2evenwithGPT-4orGPT-4V).VLUIagents,ontheother
hand,imitatethebehaviorsofexpertsandexhibitaveragesuccessratesofhigherthan50%onall
tasks,except47%onAlarm2. However,allmethodsstillshowlowperformances(lessthan60%)
oncomplextasks(i.e.,Alarm2andCall 911),whichcallsfornewalgorithms.
Weprovidemoreremarksoneachagenttypebelow.
Robustness of LLM agents and MLLM agents Both types of agents employing foundation
modelshaveshownrobustperformancesindiversedeviceconfigurations. Itisstraightforwardthat
theseagentsarerobusttotherandomizationoverthevisualappearances,suchasiconlocationsor
fontsize,asthelocationsoftheUIelementsaredescribedintheAndroidviewhierarchy. Inaddition,
LLMagentswithbothGemini-ProandGPT-4arerobusttolanguagechanges, withdescriptions
of UI elements in different languages. Particularly, these agents generalize to languages in test
environments,e.g.,KoreanandEgyptianArabic,whicharenotincludedinthefew-shotexamples.
Remaining challenges for LLM agents While exhibiting robust performances across diverse
devicesettings,severallimitationsofLLMagentsareobserved. First,theagentsfacedifficultieswith
long-horizontasks,whichrequirecompletingaprecisesequenceofmultipleactions. Forexample,
onCall 911, theagentsoftenmakemistakeswhiletypingthesequenceof9-1-1, asshownin
Figure6(a). Second, theagentsstruggletoleveragefew-shotexamplesadaptively. Forinstance,
onBrightness,weobserveLLMagentsnaivelycopyingthefew-shotexamplesfromdifferent
deviceconfigurationswithoutadjustingthemtothecurrentenvironment.
Efficacyofmulti-modalinputforMLLMagents Weconfirmtheeffectivenessofimageinput
withMLLMagentsemployingGPT-4V,aslargeincreasesinsuccessratesareobservedonAlarm2
andCall 911comparedtoLLMagentswithGPT-4. However,MLLMagentssharethechallenges
ofLLMagentsinaccuratelyexecutingcomplextasks. Moreover,theystillfallshortinunderstanding
details of visual input, such as the small interface for setting AM/PM on Alarm2 as shown in
Figure6(b). MLLMagentswithGemini-Pro-VshowsignificantlylowerperformancesthanLLM
agentswithGemini-Pro,assumablyduetothelongercontextlengthofmulti-modalinputs. These
resultsindicatetheremainingheadroominleveragingmulti-modalinputsmoreefficiently.
GeneralizationabilityofVLUIagents WeobservetrainingVLUIagentswithBCcanleadto
highperformancesonmanycomplextaskswhereMLLMagentsfail. Theseagentsperformrobustly
tounseenwallpapers,asbeingtrainedwithmultipledifferentbackgroundimages. Also,theycan
generalizetheiractionstounseendevices,e.g.,Pixel4,eventhoughtheyaretrainedonlyonasingle
device type, i.e., Pixel3. However, VLUI agentsbegin failingtocomplete thetaskswithsevere
visualchangesinducedbyunseendeviceconfigurations. Whiletheyexhibithigherthan90%success
rates in training environments, the performance degrades to less than 70% in test environments
(seeAppendixD.2formoredetails). Specifically,theysufferfromhandlingunseenlocationsofUI
elements,asshowninFigure6(c). Webelievethesefindingsrevealtheimportanceofdiversityin
trainingdatafromrandomizedenvironments(seeSection4.4formorediscussions).
7ICLR2024WorkshoponGenerativeModelsforDecisionMaking
70 Randomly initialized Pre-trained 70 7 Envs 21 Envs 35 Envs
40
40
10 10
Airplane Alarm1 Alarm2 Brightness Call911 Language Airplane Alarm1 Alarm2 Brightness Call911 Language
Figure7: SuccessratesofVLUIagentswithvi- Figure8:SuccessratesofVLUIagentswithvary-
sualencodersrandomlyinitializedorpre-trained. ingnumbersoftrainingenvironments. Thesuc-
Pre-traininghelpstheperformancesoftheagents. cessratesescalatewithmoreenvironments.
4.3 INVESTIGATIONSONDESIGNCHOICESFORLLMAGENTSANDMLLMAGENTS
TheperformanceofLLMagentsheavilyreliesonhowtheinputpromptsaretailored. Considering
onlytheleafUIelementsofAndroidviewhierarchytodescribethescreenlayout,similartoprior
work(Lietal.,2020;Yangetal.,2023b)forexample,mightresultinmeaninglessdescriptionsin
certainapplications(e.g.,thesettingapplicationonAirplaneandLanguage). Inthiswork,we
haveleveragedthetextattributesofalltheavailablenodestoavoidsuchcollapse,whilewebelieve
therecanbemoresimpleyetexpressiverepresentationmethods.
Inaddition,weobservethatfew-shotexamplescansignificantlyimprovetheperformanceofLLM
agents with GPT-4 compared to zero-shot cases. As shown in Table 2, equipping prompt with
few-shotexamplesimprovestheperformancefrom42%to67%onAlarm1andfrom27%to43%
onLanguage. However,employingfew-shotexamplesdoesnotalwayshelpagents,asshownon
Alarm2orBrightness. Wenotethatnaiveexploitationofexpertdemonstrationsmightleadto
excessiveincreasesincomputationalcostandhighlightthenecessityofefficientfew-shotprompting.
Moreover,weinvestigatetheeffectofcommonvisualpromptingmethodsforMLLMagentswith
GPT-4V.ToenhancethevisualgroundingabilityofMLLMs,priorstudies(Yanetal.,2023;Yang
etal.,2023b)haveactivelyadoptedSoMprompting,whereeachUIelementintheinputimageis
markedwithnumerictags. However,wefindthatSoMpromptingcanoftensignificantlydegradethe
performanceofMLLMagentsonAlarm2andCall 911asshowninTable2. Wehypothesize
thatthenumerictagsmaycauseconfusionwhenoverlaidonUIelementswithnumbers,suchasdial
buttonsorclockinterfaces. ForexamplesoftheinputsusedinSoMprompting,seeAppendixD.3.
4.4 EFFECTSOFPRE-TRAINEDENCODERSANDDATADIVERSITYFORVLUIAGENTS
ThemainchallengeofVLUIagentsisthelackofgeneralizationabilityasmentionedinSection4.2.
Hence,weexaminethedifferentalgorithmicdesignsfortherepresentationmodelofVLUIagentsand
theeffectsoftrainingdiversityonperformancerobustness. Wealsoincludeanadditionalexperiment
withvaryingmodelsizesofvisualencodersinAppendixE.4.
First,wecompareVLUIagentsintwodifferentdesigns: visualencoderswithparametersrandomly
initializedandvisualencoderspre-trainedwithImageNet(Krizhevskyetal.,2017). Asshownin
Figure7,weobservesignificantimprovementsinsuccessrateswithpre-training,e.g.,from37%to
63%onLanguage. Theseresultsdemonstratethebenefitofemployingpre-trainedrepresentation
models,andweexpectfurtherimprovementscanbeinducedbyleveragingmoreAndroid-specific
imagesforpre-training(Sunetal.,2022;Rawlesetal.,2023).
Furthermore,wetrainVLUIagentsbyprogressivelyincreasingthenumberoftrainingenvironments
(seeAppendixD.1formoredetailsoftheexperimentsetting). AsshowninFigure8,asthenumber
of training environments increases, the performance of VLUI agents escalates. Specifically, the
agents exhibit success rates of 20%, 40%, and 63% on Language with the number of training
environments7,21,and35,respectively. Webelievethisverifiestheefficacyoftheenvironment
randomizationfeatureincorporatedinourbenchmarktowardmorepracticalagents.
8
)%(
setaR
sseccuS
)%(
setaR
sseccuSICLR2024WorkshoponGenerativeModelsforDecisionMaking
5 RELATED WORK
Foundationmodelsfordecision-makingsystem Inspiredbythestrongemergentpropertiesof
foundationmodels(Brownetal.,2020;Weietal.,2022),manyresearcheshaveadoptedLLMsto
developdecision-makingsystem(Yaoetal.,2023;Shinnetal.,2023). Inrobotlearning,forexample,
LLMshavebeenwidelyequippedforreasoning,planning,manipulation,andnavigation(Driessetal.,
2023;Liangetal.,2023;Huangetal.,2023).Furthermore,agentswithLLMshaveshowncapabilities
ofperforminginterestingtasksinnumeroussimulatedworlds,includinggameenvironments(Wang
etal.,2023;Tanetal.,2024)andvirtualreality(Qianetal.,2023;Yangetal.,2024). Inrecentdays,
focusingonpracticalness, solvingcomputertaskswithfoundationmodelshasalsobeenactively
explored (Nakano et al., 2021; Furuta et al., 2023). We further study the abilities of foundation
modelstocontrolmobiledevicestowardassistiveagentsinreallife.
Developingassistiveagentfordevicecontrol Foragentsthateffectivelyunderstandandmanipu-
latetheUIelements,alargebodyofworkhasleveragedthestructuralinformation,suchasdocument
object model in HTML or Android view hierarchy (Branavan et al., 2010; Gur et al., 2019). In
addition,methodsforequippingagentswiththeabilitytounderstandinformation-richscreenimages
havebeenwidelyinvestigated,mainlywithvision-basedreinforcementlearning(Liuetal.,2018;
Humphreys et al., 2022; Shaw et al., 2023). Recently, diverse strategies to build device control
agentswithfoundationmodelsareintroduced,includingpromptingmethods(Wenetal.,2023;Kim
etal.,2023),instruction-tuning(Furutaetal.,2023),fine-tuningwithimages(Zhan&Zhang,2023;
Hongetal.,2023),andvisualprompting(Yanetal.,2023;Yangetal.,2023b). Here,wepresentan
elaborateanalysisofthemainmethodsforbuildingmobiledevicecontrolagents.
Benchmarkfordecision-makingagents Therehavebeencontinuouseffortstobuildreliablebench-
marksforsequentialdecision-makinginvideogames(Bellemareetal.,2013),locomotion(Brockman
etal.,2016),androboticmanipulation(Jamesetal.,2020). Lately,researchershaveproposedbench-
marksforsolvingdevicecontroltasks,viewingitasanotherdecision-makingproblem. Forexample,
Yaoetal.(2022)andZhouetal.(2024)havepresentedbenchmarksimulatingwebplatforms,while
Toyamaetal.(2021),Shvoetal.(2021),andZhangetal.(2023)havesuggestedRLenvironments
adoptingAndroidemulators. Inthiswork,inspiredbyspecial-purposebenchmarksquantifyingthe
robustnessoftheagents(Cobbeetal.,2020;Stoneetal.,2021),wenewlyproposeabenchmarkwith
therandomizationfeature.
6 DISCUSSION & CONCLUSION
WepresentB-MoCA,anewbenchmarkdesignedforevaluatingmobiledevicecontrolagents. Our
benchmarkprovidesdiversetasksapplicabletoeverydayroutinesandenvironmentsthatsimulate
numerousdeviceconfigurations. WeconductextensiveexperimentsanddemonstratethatB-MoCA
can serve as a standardized platform for developing different types of agents in a unified setting.
Finally,wementionseverallimitationsandpromisingfuturedirectionsofthiswork:
• TaskswithtexttypingWhilewedefinetheactionspaceswithdual-gestureactions,texttyping
bytouchingthesoftkeyboarddemandsexcessivelylonginteractions. Inthefuture, weplan
to include tasks requiring text typing, such as web search or e-mail sending, with advanced
interfaces.
• Open-endedtasksandrewardmodelingSincetheADB-basedsuccessdetectordoesnotcapture
the semantics of agent behaviors, tasks with ambiguous success criteria are hard to evaluate.
Alternatively,webelieveemployingtherewardmodellearnedfromdemonstrations(Fanetal.,
2022)canbeusedforintegratingopen-endedtasks.
• MoreonLLMagentsFoundationmodelscanbeemployedindifferentways,suchasusingthem
asahigh-levelplannertooperateasetofpre-definedAPIs(Chen&Li,2024)orneuralnetwork
policies(Ahnetal.,2022)aslow-levelactors. Also,astrainingVLUIagentswithdemonstrations
resultsinhighperformances,fine-tuningLLMsishighlypromising.
Towardpracticalmobiledevicecontrolagents,wehopethatB-MoCAstandsasavaluableplatform
withhelpfulresourcesforfutureinnovations.
9ICLR2024WorkshoponGenerativeModelsforDecisionMaking
IMPACT STATEMENT
Thisstudyproposesabenchmarkdesignedtoassessinteractivemobiledevicemanagementagents,
withsocialopportunitiestoenhanceuseraccessibilityandaidthosefacingdisabilities. Wecaution
usersaboutprivacyconcernswhilewetrytoeliminatesuchpotentialsduringtaskdesigns. Noting
theimportanceofresearchforpreventingmalicioususagesofdevicecontrolagents,weemphasize
B-MoCAasausefultestbed.
ACKNOWLEDGMENTS
We thank Dongjun Lee, Kyuyoung Kim, and Ahjeong Seo for providing sincere suggestions for
improving our work. This work was supported by Institute for Information & communications
TechnologyPromotion(IITP)grantfundedbytheKoreagovernment(MSIT)(No.2019-0-00075
ArtificialIntelligenceGraduateSchoolProgram(KAIST)).
REFERENCES
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,Chelsea
Finn,ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,etal. Doasican,notasisay:
Groundinglanguageinroboticaffordances. InTheConferenceonRobotLearning,2022.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluationplatformforgeneralagents. JournalofArtificialIntelligenceResearch,47:253–279,
2013.
SRKBranavan,LukeZettlemoyer,andReginaBarzilay. Readingbetweenthelines: Learningtomap
high-levelinstructionstocommands. InAssociationforComputationalLinguistics,2010.
GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,and
WojciechZaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. InConferenceonNeuralInformationProcessingSystems,2020.
WeiChenandZhiyuanLi. Octopusv2: On-devicelanguagemodelforsuperagent. arXivpreprint
arXiv:2404.01744,2024.
KarlCobbe,ChrisHesse,JacobHilton,andJohnSchulman. Leveragingproceduralgenerationto
benchmarkreinforcementlearning. InInternationalconferenceonmachinelearning,2020.
DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal
languagemodel. InternationalConferenceonMachineLearning,2023.
LinxiFan,GuanzhiWang,YunfanJiang,AjayMandlekar,YuncongYang,HaoyiZhu,AndrewTang,
De-AnHuang,YukeZhu,andAnimaAnandkumar. Minedojo: Buildingopen-endedembodied
agentswithinternet-scaleknowledge. InConferenceonNeuralInformationProcessingSystems,
2022.
HirokiFuruta,OfirNachum,Kuang-HueiLee,YutakaMatsuo,ShixiangShaneGu,andIzzeddinGur.
Instruction-finetunedfoundationmodelsformultimodalwebnavigation. InInternationalConfer-
enceonLearningRepresentations2023WorkshoponMathematicalandEmpiricalUnderstanding
ofFoundationModels,2023.
TeamGemini,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapable
multimodalmodels. arXivpreprintarXiv:2312.11805,2023.
10ICLR2024WorkshoponGenerativeModelsforDecisionMaking
IzzeddinGur,UlrichRueckert,AleksandraFaust,andDilekHakkani-Tur. Learningtonavigatethe
web. InInternationalConferenceonLearningRepresentations,2019.
WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,Zihan
Wang,YuxiaoDong,MingDing,etal. Cogagent: Avisuallanguagemodelforguiagents. arXiv
preprintarXiv:2312.08914,2023.
ChenguangHuang,OierMees,AndyZeng,andWolframBurgard. Visuallanguagemapsforrobot
navigation. InInternationalConferenceonRoboticsandAutomation,2023.
PeterCHumphreys,DavidRaposo,TobiasPohlen,GregoryThornton,RachitaChhaparia,Alistair
Muldal,JoshAbramson,PetkoGeorgiev,AdamSantoro,andTimothyLillicrap. Adata-driven
approachforlearningtocontrolcomputers. InInternationalConferenceonMachineLearning,
2022.
Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot
learningbenchmark&learningenvironment. IEEERoboticsandAutomationLetters,2020.
GeunwooKim, PierreBaldi, andStephenMcAleer. Languagemodelscansolvecomputertasks.
ConferenceonNeuralInformationProcessingSystems,2023.
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InInternational
ConferenceforLearningRepresentations,2017.
Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit
q-learning. InInternationalConferenceonLearningRepresentations,2022.
AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepconvolu-
tionalneuralnetworks. CommunicationsoftheACM,60(6):84–90,2017.
SaschaLange,ThomasGabel,andMartinRiedmiller.Batchreinforcementlearning.InReinforcement
learning: State-of-the-art,pp.45–73.Springer,2012.
SergeyLevine,AviralKumar,GeorgeTucker,andJustinFu. Offlinereinforcementlearning: Tutorial,
review,andperspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
WeiLi,Fu-LinHsu,WillBishop,FolawiyoCampbell-Ajala,OrianaRiva,andMaxLin. Uinav: A
makerofuiautomationagents. arXivpreprintarXiv:2312.10170,2023.
Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning:
Generatingnaturallanguagedescriptionformobileuserinterfaceelements. InConferenceon
EmpiricalMethodsinNaturalLanguageProcessing,2020.
JackyLiang,WenlongHuang,FeiXia,PengXu,KarolHausman,BrianIchter,PeteFlorence,and
AndyZeng. Codeaspolicies: Languagemodelprogramsforembodiedcontrol. InInternational
ConferenceonRoboticsandAutomation,2023.
EvanZheranLiu, KelvinGuu, PanupongPasupat, TianlinShi, andPercyLiang. Reinforcement
learningonwebinterfacesusingworkflow-guidedexploration. InInternationalConferenceon
LearningRepresentations,2018.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.InConference
onNeuralInformationProcessingSystems,2023.
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcementlearningwithofflinedatasets. arXivpreprintarXiv:2006.09359,2020.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answeringwithhumanfeedback. arXivpreprintarXiv:2112.09332,2021.
JanPetersandStefanSchaal. Reinforcementlearningbyreward-weightedregressionforoperational
spacecontrol. InInternationalConferenceonMachinelearning,2007.
11ICLR2024WorkshoponGenerativeModelsforDecisionMaking
DeanAPomerleau. Alvinn: Anautonomouslandvehicleinaneuralnetwork. InConferenceon
NeuralInformationProcessingSystems,1988.
Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li,
Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software
development. arXivpreprintarXiv:2307.07924,2023.
Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy P Lillicrap. An-
droidinthewild: A large-scale dataset for android device control. In Conference on Neural
InformationProcessingSystemsDatasetsandBenchmarksTrack,2023.
StefanSchaal. Learningfromdemonstration. ConferenceonNeuralInformationProcessingSystems,
1996.
PeterShaw,MandarJoshi,JamesCohan,JonathanBerant,PanupongPasupat,HexiangHu,Urvashi
Khandelwal,KentonLee,andKristinaToutanova. Frompixelstouiactions: Learningtofollow
instructionsviagraphicaluserinterfaces.InConferenceonNeuralInformationProcessingSystems,
2023.
NoahShinn,BeckLabash,andAshwinGopinath. Reflexion: anautonomousagentwithdynamic
memoryandself-reflection. arXivpreprintarXiv:2303.11366,2023.
MaayanShvo,ZhimingHu,RodrigoToroIcarte,IqbalMohomed,AllanD.Jepson,andSheilaA.
McIlraith. Appbuddy: Learningtoaccomplishtasksinmobileappsviareinforcementlearning. In
CanadianConferenceonArtificialIntelligence,2021.
AustinStone,OscarRamirez,KurtKonolige,andRicoJonschkowski. Thedistractingcontrolsuite–a
challengingbenchmarkforreinforcementlearningfrompixels. arXivpreprintarXiv:2101.02722,
2021.
Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards
multi-modalconversationalagentsonmobilegui. ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,2022.
Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. InInternationalConferenceonMachineLearning,2019.
WeihaoTan,ZiluoDing,WentaoZhang,BoyuLi,BohanZhou,JunpengYue,HaochongXia,Jiechuan
Jiang,LongtaoZheng,XinrunXu,etal. Towardsgeneralcomputercontrol: Amultimodalagent
forreddeadredemptioniiasacasestudy. arXivpreprintarXiv:2403.03186,2024.
JoshTobin,RachelFong,AlexRay,JonasSchneider,WojciechZaremba,andPieterAbbeel. Do-
mainrandomizationfortransferringdeepneuralnetworksfromsimulationtotherealworld. In
InternationalConferenceonIntelligentRobotsandSystems,2017.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
DanielToyama,PhilippeHamel,AnitaGergely,GheorgheComanici,AmeliaGlaese,ZafaraliAhmed,
TylerJackson,ShiblMourad,andDoinaPrecup. Androidenv: Areinforcementlearningplatform
forandroid. arXivpreprintarXiv:2105.13231,2021.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Conference on Neural Information
ProcessingSystems,2017.
GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,and
AnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels. In
ConferenceonNeuralInformationProcessingSystems,2023.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. InConference
onNeuralInformationProcessingSystems,2022.
12ICLR2024WorkshoponGenerativeModelsforDecisionMaking
HaoWen,YuanchunLi,GuohongLiu,ShanhuiZhao,TaoYu,TobyJia-JunLi,ShiqiJiang,Yunhao
Liu, Yaqin Zhang, and Yunxin Liu. Empowering llm to use smartphone for intelligent task
automation. arXivpreprintarXiv:2308.15272,2023.
AnYan,ZhengyuanYang,WanrongZhu,KevinLin,LinjieLi,JianfengWang,JianweiYang,Yiwu
Zhong,JulianMcAuley,JianfengGao,etal. Gpt-4vinwonderland: Largemultimodalmodelsfor
zero-shotsmartphoneguinavigation. arXivpreprintarXiv:2311.07562,2023.
Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark
promptingunleashesextraordinaryvisualgroundingingpt-4v. arXivpreprintarXiv:2310.11441,
2023a.
Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining Xie. V-irl: Grounding virtual
intelligenceinreallife. arXivpreprintarXiv:2402.03310,2024.
ZhaoYang,JiaxuanLiu,YuchengHan,XinChen,ZebiaoHuang,BinFu,andGangYu. Appagent:
Multimodalagentsassmartphoneusers. arXivpreprintarXiv:2312.13771,2023b.
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
real-worldwebinteractionwithgroundedlanguageagents. ConferenceonNeuralInformation
ProcessingSystems,2022.
ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao.
React: Synergizing reasoning and acting in language models. In International Conference on
LearningRepresentations,2023.
AohanZeng,MingdaoLiu,RuiLu,BowenWang,XiaoLiu,YuxiaoDong,andJieTang.Agenttuning:
Enablinggeneralizedagentabilitiesforllms. arXivpreprintarXiv:2310.12823,2023.
ZhuoshengZhanandAstonZhang. Youonlylookatscreens: Multimodalchain-of-actionagents.
arXivpreprintarXiv:2309.11436,2023.
DanyangZhang,LuChen,andKaiYu. Mobile-env: Auniversalplatformfortrainingandevaluation
ofmobileinteraction. arXivpreprintarXiv:2305.08144,2023.
Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,
YonatanBisk,DanielFried,UriAlon,etal. Webarena: Arealisticwebenvironmentforbuilding
autonomousagents. InInternationalConferenceonLearningRepresentations,2024.
13ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Appendix:
BenchmarkingMobileDeviceControlAgentacrossDiverseConfigurations
A ENVIRONMENT DETAILS
A.1 ENVIRONMENTIMPLEMENTATIONANDINTERFACE
Environment B-MoCA is based on Android OS for real-system interactive evaluation. The
environment is simulated with Android virtual devices, containing the device hardware profile,
systemimage,storagearea,andotherrelevantproperties. Thedynamicsoftheenvironments,suchas
thetransitionrules,aregovernedbyAndroidOSandapplications.
Each environment is represented as an Android device, running on top of the Android emulator.
To be more specific, we define each environment as a snapshot, a stored image of the Android
virtualdevice. Eachsnapshotisbuiltbysavinganimageofthetargetdeviceaftertheconfigurations.
Theseconfigurationsincluderandomizingthefeaturesofenvironmentsbyplacingiconsinrandom
locations,settingdotsperinch(DPI),modifyingwallpapers,andchangingthelanguage. Inaddition,
ourconfigurationprocessincludesadjustingseveraldevicesettingsforaccurateevaluation,suchas
changingthedatabaseofapplications.
Tofacilitateinteractionsbetweentheenvironmentandagents,wedevelopasetofinterfaces. These
interfacesencompassvariousfunctionalities: toprovidethetaskdescriptionsintexttotheagents,to
capturescreenshotsofthevirtualdevice,toprovidetheAndroidviewhierarchyinXMLformatand
parsethetextdescriptionofthescreen,toextractdual-gestureactionsfromtext-basedactions,andto
deliverthedual-gestureactiontotheAndroidemulator.
Interaction Frequency The Android emulators run asynchronously independent of the agent
thatisinteractingwiththeenvironments. However,thisasynchronicitybetweentheagentandthe
environmentmaycauseseveralissuessuchasincompletetransitionoftheenvironmentsordelayed
success signals. To alleviate the issue, we adjust the interaction frequency between agents and
environments. Specifically,thisadjustmentisoperatedbyforcingtheagenttowaitapre-defined
timebeforefetchingthescreeninformationfromtheenvironment. Inourexperiments,wefixthe
interactionfrequencyduringevaluationtobe1/3Hzacrossalltypesofagents.
Observationspace Theobservationspaceiscomprisedofeitherascreenimage,atextdescription
ofthescreeninXMLformatsbasedontheAndroidviewhierarchy,orboth.
The screen images are used for multi-modal large language model (MLLM) agents and vision-
language-UI(VLUI)agents. Eachimageisresizedintoaresolutionof1024×2048forMLLM
agentsand128×256forVLUIagents.
ThetextdescriptionsareusedforagentswithLLMsandMLLMs. Tobuildthetextdescription,the
Androiddebugbridge(ADB)UIAutomatorisemployedforacquiringtheAndroidviewhierarchy
inXMLformat. Apre-definedparser,then, convertsthelistofUIelementswithattributesinan
XMLfileintoasetoftextdescriptionsofUIelements. Thedescriptionincludesthenumerictag
of UI elements, a short description of the elements including class name or content descriptions
(e.g.,“com.google.android.apps.nexuslauncher.id title weather text Sunny,1◦C”foraviewinthe
homescreendescribingtheweather). Also,weoptionallyprovidetheboundingboxx-ycoordinates
specifyingthelocationoftheelements,suchasthesliderinterface. Moreover,wedefinetheparserto
capturethedescriptionsofallthenodesintheAndroidviewhierarchy. Thisisbecauseweobserve
thatmanyUIelementsareomittedifweonlyparseleafnodes,resultinginmeaninglessdescriptions
asdiscussedinSection4.3.
Actionspace Theactionspaceoftheagentsisdefinedasasetofdual-gestureactions{a| a =
(y ,x ,y ,x )∈R4},similartoRawlesetal.(2023). Eachvalueofdual-gestureactionais
touch touch lift lift
normalizedtobeinbetween[−1,1]withrespecttothescreenresolutions. Theformertwovalues
specifythelocationofthescreentotouch,whilethelattertwovaluesdeterminethelocationofthe
screentolift. Thisdefinitionenablesinterpretingusefulactionsindigitaldevicecontrol,i.e.,tapping
14ICLR2024WorkshoponGenerativeModelsforDecisionMaking
orswipingthescreens,inapreciseandcompressivemanner. Also,ourinterfaceallowspressingthe
navigatorbuttonsavailablebytouchingthescreentosupporttheessentialactionsformanipulating
Androiddevices.
Specifically,weimplementaninterfacethatdetermineswhethertheactionisatap,swipe,orpressing
ofnavigationbuttonsi.e.,back,home,andoverview. Theactionparsinginterfaceconvertstheaction
intotaps,swipes,orpressingbuttonsfollowingtheruleasfollows:
• Theactionistapping,ifd((x ,y ),(x ,y ))<threshold
touch touch lift lift
– ThetappingistopressBACKbutton,if(x ,y )=(0.95,0.22)
touch touch
– ThetappingistopressHOMEbutton,if(x ,y )=(0.95,0.50)
touch touch
– ThetappingistopressOVERVIEWbutton,if(x ,y )=(0.95,0.78)
touch touch
• Theactionisswiping,ifd((x ,y ),(x ,y ))≥threshold,
touch touch lift lift
wherethethresholdvalueisdefinedas0.14. Thisvalueisadjustablebyusers,whilewefindthat
thevalueof0.14ensuresproperinteractionsoverUIelements,e.g.,tappingthetargetapplication
icon,inallofourexperiments. Thesespecificvaluesaretestedtobeconsistentacrossdifferentdevice
types,ensuringthatthepositionscorrespondtothecorrectbuttonsinallB-MoCAenvironments.
ForLLMagentsandMLLMagents,wefurtherdefineactionoptions. Followingtheactionspace
definition,theactionoptionsaredesignedtobecompatiblewithadual-gestureaction. Weprompt
theLLMagentstooutputactionsamongsixpossibleoptions: rawdual-gestureaction,tap,swipe,
press(“HOME”),press(“BACK”),andpress(“OVERVIEW”).Theseactionoptionsareconvertedinto
acorrespondingdual-gestureactionbyanadditionalactionextractorwedefineasbelow:
• Forthedual-gestureaction,weconvertthetextactionintothefourfloatingpointsbyrounding
eachvalueintotheseconddecimalpoint.
• Fortapactions,theLLMagentoutputsanintegervaluespecifyingthenumerictagassignedto
theUIelement. Giventhetappingactionwithanumerictag,theparserconvertstheactionintoa
tappingdual-gestureactionwiththeboundingboxinformationofthechosenUIelement.
• Forswipeactions,adirection‘up’,‘down’,‘left’and‘right’isconvertedinto(0.8,0.5,0.2,0.5),
(0.2,0.5,0.8,0.5),(0.5,0.2,0.5,0.8),and(0.5,0.8,0.5,0.2),respectively.
• For the action press(“HOME”), press(“BACK”), and press(“OVERVIEW”), we convert the
outputstodual-gestureactionsinthesamewayasVLUIagents.
Duringtheevaluation,weignoretheactioninthewrongformatbyskippingthetransitionofthe
environmentsbutpenalizingtheagentsbyincrementingthestepstaken,yetweobservebothGemini
andGPTmodels(aswellasvisionversionofthem)rarelymakemistakesontheformat.
A.2 TRAININGANDTESTENVIRONMENTSCONFIGURATIONS
Weconstruct45uniqueenvironmentsinB-MoCA,where35environmentsarefortrainingand10
environmentsarefortesting. Eachenvironmentisprovidedwithauniqueidentification(ID)number,
todistinguishtheenvironmentseasily. Table3showsthelistofthedeviceconfigurationsandthe
homescreenimagesofexemplaryenvironments.
Toconstructenvironments,weusepopulardevicetypes: Pixel3,Pixel4,Pixel4XL,Pixel6,and
WGXATablet. Fortrainingenvironments,onlyPixel3isemployed. Forevaluationenvironments,
weusealldevicetypesPixel3,Pixel4,Pixel4XL,Pixel6,andWGXATablet. Inthesemodels,
wealtertheiconandfontsizesbychangingthedotsperinch(DPI)valuesofthedevices. Foreach
devicetype,wepreparethreedifferentsizesthatuserscanselect. We,then,changethewallpaper
with13imagescollectedfromafreelicenseimagewebsite. Thesewallpaperimagefilesareshared
intheopen-sourcerepository. Wealsocustomizethebackgroundimageswiththedarkthememode.
Ifthedarkthememodeisactivated,thedeviceprovidesscreenimageswithlight-darkcolorreversed.
Forinstance,thewallpaperoftheapplicationlistpageiswhiteinthedefaultsetting,whileitbecomes
blackwithdarkthememodeactivated. Furthermore,weincorporatechangesinlocale,specifying
thelanguageandlocationofthedevices. 12differentlocalesareusedfor35trainingenvironments,
whileweincludethreemorelocalesforthetestenvironments.
15ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Table3: Thedeviceconfigurationofeachenvironmentwiththehomescreenimage
ID 000 001 002 003 004
Devicetype Pixel3 Pixel3 Pixel3 Pixel3 Pixel3
DPI 330 330 440 440 550
Locale en-US en-US en-US en-US en-US
Wallpaper 000.jpg 000.jpg 000.jpg 000.jpg 000.jpg
Darktheme - - - - -
ID 005 006 007 008 009
Devicetype Pixel3 Pixel3 Pixel3 Pixel3 Pixel3
DPI 440 440 330 440 550
Locale en-US en-US en-US en-US en-US
Wallpaper 000.jpg 000.jpg 001.jpg 002.jpg 001.jpg
Darktheme - - ✓ ✓ -
ID 010 011 012 013 014
Devicetype Pixel3 Pixel3 Pixel3 Pixel3 Pixel3
DPI 330 440 550 440 440
Locale en-US en-US en-US en-US en-US
Wallpaper 002.jpg 008.jpg 003.jpg 010.jpg 013.jpg
Darktheme - ✓ ✓ - ✓
Continuedonnextpage
16ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Table3: Thedeviceconfigurationofeachenvironmentwiththehomescreenimage(Continued)
ID 015 016 017 018 019
Devicetype Pixel3 Pixel3 Pixel3 Pixel3 Pixel3
DPI 330 440 440 550 330
Locale en-US en-US en-US en-US en-US
Wallpaper 008.jpg 007.jpg 004.jpg 010.jpg 013.jpg
Darktheme - - ✓ ✓ -
ID 020 021 022 023 024
Devicetype Pixel3 Pixel3 Pixel3 Pixel3 Pixel3
DPI 440 330 440 550 330
Locale en-US es-US es-US fr-CA fr-CA
Wallpaper 004.jpg 001.jpg 002.jpg 001.jpg 002.jpg
Darktheme - ✓ ✓ - -
ID 025 026 027 028 029
Devicetype Pixel3 Pixel4 Pixel4 Pixel4 Pixel5
DPI 440 550 440 440 330
Locale zh-hans-CN zh-hans-CN hi-IN ja-JP ru-MD
Wallpaper 008.jpg 003.jpg 010.jpg 013.jpg 008.jpg
Darktheme ✓ ✓ - ✓ -
Continuedonnextpage
17ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Table3: Thedeviceconfigurationofeachenvironmentwiththehomescreenimage(Continued)
ID 030 031 032 033 034
Devicetype Pixel3 Pixel3 Pixel3 Pixel3 Pixel3
DPI 440 440 550 330 440
Locale ar-AE de-DE ak-GH pt-BR pt-PT
Wallpaper 007.jpg 004.jpg 010.jpg 013.jpg 004.jpg
Darktheme - ✓ ✓ - -
ID 100 101 102 103 104
Devicetype Pixel3 Pixel3 Pixel3 Pixel3 Pixel3
DPI 440 330 440 550 440
Locale en-US en-US en-US en-US fr-CA
Wallpaper 000.jpg 000.jpg 009.jpg 012.jpg 009.jpg
Darktheme - - ✓ - ✓
ID 109 105 106 107 108
Devicetype WXGA Pixel3 Pixel4 Pixel5 Pixel6
Tablet
DPI 160 550 440 550 700
Locale ar-EG ko-KR en-US en-US ur-PK
Wallpaper 012.jpg 009.jpg 012.jpg 005.jpg 011.jpg
Darktheme - ✓ - ✓ -
18ICLR2024WorkshoponGenerativeModelsforDecisionMaking
B TASK DETAILS
B.1 LISTOFDAILYTASKS
B-MoCApresents60dailytasksthatarecommonineverydaylife. Thetasksaredesignedtooperate
indiverseenvironmentsseamlesslyandcovercommonlyusedapplications. Dailytaskseffectively
simulateawiderangeofessentialskillsformobiledevicecontrolproblems,suchasmanipulatingUI
elements(includingapplicationicons,checkboxes,andsliders),andcanbeemployedforevaluating
mobiledevicecontrolagents’capabilitiesinperformingtasksthatmirrorourdailyactivities.
InTable4,weincludethedetailedlistoftaskswiththemaximumsteplimitandthesuccesscriteria.
Thesuccesscriteriaaredefinedintheformofregularexpressionandareemployedbytherule-based
successdetector.Thesuccesscriteria(filter)specifiesthetargetapplicationoractivity,andthesuccess
criteria(regex)referstotheregularexpressionweuse. Wealsodefinethemaximumsteplimits,
whicharesetfortherigorousevaluationoftheagents’proficiencyoneachtask.
B.2 EXAMPLEOFDEMONSTRATIONONREPRESENTATIVETASKS
Inourexperiments,weselectsixrepresentativetasks. Thetasksareselectedtocoverawiderangeof
functionalities,suchasnavigatingpages(e.g.,tabintheclockapplicationordifferentsettingpages
inthesettingapplication)andmanipulatingvariousUIelements(e.g.,checkbox,slider,timepickers,
etc.). Oneachtask,wedisplaythesuccessfuldemonstrationinFigure9.
Table4: Comprehensivelistoftasks.
Step
Taskinstruction Successcriteria(filter) Successcriteria(regex)
limit
4 “openthecalendarapp” ActivityTaskManager ˆ(.*?)START(.*?)com.android.calendar
4 “openthecameraapp” ActivityTaskManager ˆ(.*?)Startproc(.*?)com.android.camera
4 “openthechromeapp” ActivityTaskManager ˆ(.*?)START(.*?)com.google.android.apps.chrome
4 “opentheclockapp” ActivityTaskManager ˆ(.*?)START(.*?)com.android.deskclock
4 “openthecontactapp” ActivityTaskManager ˆ(.*?)Startproc(.*?)com.android.contacts
4 “openthefilemanagerapp” ActivityTaskManager ˆ(.*?)START(.*?)files.FilesActivity
4 “openthegmailapp” ActivityTaskManager ˆ(.*?)START(.*?)com.google.android.gm
4 “openthemapapp” ActivityTaskManager ˆ(.*?)START(.*?)com.google.android.maps.MapsActivity
4 “openthemessageapp” ActivityTaskManager ˆ(.*?)START(.*?)com.google.android.apps.messaging
4 “openthephoneapp” Dialer ˆ(.*?)MainActivity.onCreate
4 “openthephotosapp” ActivityTaskManager ˆ(.*?)START(.*?)com.google.android.apps.photos
4 “opentheplaymusicapp” ActivityTaskManager ˆ(.*?)START(.*?)com.android.music
4 “openthesettingapp” ActivityManager ˆ(*.?)Startproc(.*?)com.android.settings.Settings
4 “opentheyoutubeapp” ActivityTaskManager ˆ(.*?)START(.*?)com.google.android.youtube
4 “turnonalarmat9am” AlarmClock ˆ(.*?)Creatednewalarminstance
5 “deletealarmat9am” AlarmClock ˆ(.*?)Removedalarm
5 “gotothealarmpageinclock” AlarmClock ˆ(.*?)Events:[Alarm][ShowTab][Tap]
5 “gotothestopwatchpageinclock” AlarmClock ˆ(.*?)Events:[Stopwatch][ShowTab][Tap]
5 “gotothetimerpageinclock” AlarmClock ˆ(.*?)Events:[Timer][ShowTab][Tap]
5 “listaudiofilesinfilemanager” DirectoryFragment ˆ(.*?)Showingdirectory(.*?)audio(.*?)root
5 “listimagefilesinfilemanager” DirectoryFragment ˆ(.*?)Showingdirectory(.*?)images
5 “listvideofilesinfilemanager” DirectoryFragment ˆ(.*?)Showingdirectory(.*?)videos
Continuedonnextpage
19ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Table4: Comprehensivelistoftasks. (Continued)
5 “listdownloadfilesinfilemanager” DirectoryFragment ˆ(.*?)Showingdirectory(.*?)download
5 “activatetheinsertpageincontact” ActivityTaskManager ˆ(.*?)START(.*?)INSERT(.*?)ContactEditorActivity
5 “activatetheeditpageincontact” ActivityTaskManager ˆ(.*?)START(.*?)EDIT(.*?)ContactEditorActivity
5 “activatethesearchbarinchrome” AndroidIME ˆ(.*?)LatinIme.onActivate(.*?)android.chrome
5 “activatethesearchbarinmap” AndroidIME ˆ(.*?)LatinIme.onActivate(.*?)apps.map
5 “activatethesearchbarinyoutube” AndroidIME ˆ(.*?)LatinIme.onActivate(.*?)android.youtube
5 “activatethesearchbaringoogle” AndroidIME ˆ(.*?)LatinIme.onActivate(.*?)android.googlequicksearchbox
5 “activatethesearchbarinmessage” AndroidIME ˆ(.*?)LatinIme.onActivate(.*?)apps.messaging
5 “startchattinginmessage” BugleUsageStatistics ˆ(.*?)BUGLECREATE(.*?)DEFAULT
5 “pressthecallbuttonindial” Telecom ˆ(.*?)LogUtils(.*?)EventRecordaddedasCall
5 “turnonairplanemode” PhoneGlobals ˆ(.*?)Turningradiooff(.*?)airplane
5 “turnoffairplanemode” PhoneGlobals ˆ(.*?)Turningradioon(.*?)airplane
5 “turnonwifi” WifiService ˆ(.*?)setWifiEnabled(.*?)com.android.settings(.*?)enable=true
5 “turnoffwifi” WifiService ˆ(.*?)setWifiEnabled(.*?)com.android.settings(.*?)enable=false
5 “startthestopwatchinclock” AlarmClock ˆ(.*?)Start
5 “pausethestopwatchinclock” AlarmClock ˆ(.*?)Pause
5 “resetthestopwatchinclock” AlarmClock ˆ(.*?)Reset
5 “gotosearchhistoryinchrome” ActivityTaskManager ˆ(.*?)START(.*?)chrome.browser.history.HistoryActivity
5 “gototrashpageinphoto” ActivityTaskManager ˆ(.*?)START(.*?)apps.photos(.*?)TrashPhotosActivity
5 “gotosmartpairingpageinyoutube” ActivityTaskManager ˆ(.*?)START(.*?)youtube.mdx.smartpairing.PairWithTvActivity
6 “increasemediavolumeinsetting” vol.Events ˆ(.*?)MEDIA
6 “increasecallvolumeinsetting” vol.Events ˆ(.*?)CALL
6 “increaseringvolumeinsetting” vol.Events ˆ(.*?)MUSIC
6 “increasealarmvolumeinsetting” vol.Events ˆ(.*?)ALARM
“decreasescreenbrightnessinset-
6 DisplayPowerController ˆ(.*?)Brightness(.*?)changing(.*?)manual
ting”
6 “toggledarkthemeinsetting” SettingsProvider ˆ(.*?)content(.*?)settings(.*?)dark(.*?)mode
6 “togglevibrateforcallsinsetting” SettingsProvider ˆ(.*?)vibrate(.*?)when(.*?)ringing
6 “gotoappinfolistinsetting” SettingsActivity ˆ(.*?)Switching(.*?)android.settings(.*?)ManageApplications
6 “gotobluetoothsetting” PrefCtrlListHelper ˆ(.*?)android.settings.bluetooth.BluetoothDevice
“goto‘addalanguage’pageinset-
7 ActivityTaskManager ˆ(.*?)LocalePicker
ting”
9 “call911” Telecom ˆ(.*?)Emergencynumberdetected
10 “turnoffthecallinprocess” Telecom ˆ(.*?)InCallController(.*?)onCallRemoved
11 “createalarmat06:30am” ConditionProviders.SCP ˆ(.*?)nextUserAlarmTime(.*?)06:30:00
11 “createalarmat10:30am” ConditionProviders.SCP ˆ(.*?)nextUserAlarmTime(.*?)10:30:00
11 “createalarmat13:30pm” ConditionProviders.SCP ˆ(.*?)nextUserAlarmTime(.*?)13:30:00
11 “createalarmat17:30pm” ConditionProviders.SCP ˆ(.*?)nextUserAlarmTime(.*?)17:30:00
11 “createalarmat20:30pm” ConditionProviders.SCP ˆ(.*?)nextUserAlarmTime(.*?)20:30:00
11 “createalarmat23:30pm” ConditionProviders.SCP ˆ(.*?)nextUserAlarmTime(.*?)23:30:00
20ICLR2024WorkshoponGenerativeModelsforDecisionMaking
(a) Airplane (b) Alarm1
(c) Alarm2
(d) Brightness
(e) Call 911
(f) Language
Figure9: Examplesofhumanexpertdemonstrationsofsixrepresentativetasks. Theblueandred
cursorslinkedwithawhitearrowidentifytheswipingaction,whiletheredcursoraloneidentifies
thetappingaction.
C AGENT DETAILS
C.1 PROMPTDETAILSFORLLMAGENTSANDMLLMAGENTS
FortheagentsemployingLLMsorMLLMs,weuseacompletepromptformatdescribedinTable5.
The role description informs the agents with general instructions about the problem, i.e., device
control problem. The possible actions are provided as callable functions, options of tapping an
elementinthelist,swipingthescreen,andpressingthethreenavigationbuttonsoverthescreenwith
actionpress(“BACK”),actionpress(“HOME”),andactionpress(“OVERVIEW”).Theoutputformat
isdesignedtointegratetheChain-of-Thought(CoT)technique(Weietal.,2022).
21ICLR2024WorkshoponGenerativeModelsforDecisionMaking
Youareanagentthatistrainedtoperformdailytasksondigitaldevices,suchassmartphones.Youaregivena
goaloftaskinstructiontoaccomplishandadescriptionofscreenfromAndroidviewhierarchy,whichcontains
elements’numerictaganddescription.
BasedonthegoaloftaskinstructionandUIelementslist,youneedtoselectanactionoptionbycallingoneof
thefollowingfunctionstocontrolthedigitaldevice:
1. dual-gesture(touchy: float,touchx: float,lifty: float,liftx: float): Thisfunctionisusedtooperate
adual-gestureaction. Adual-gesturecomprisesoffourfloatingpointnumericvalues,inbetween0and1
indicatinganormalizedlocationofthescreenineachofx-ycoordinates.Adual-gestureactionisinterpreted
astouchingthescreenatthelocationof(touchy,touchx)andliftingatthelocationof(lifty,liftx). The
dual-gestureactionindicatesatappingactionifthetouchandliftlocationsareidenticalbutaswipingactionif
theydiffer.Asimpleusecaseisdual-gesture(0.5,0.5,0.5,0.5)totapthecenterofthescreen.
2.tap(numerictag:int):ThisfunctionisusedtotapanUIelementshownonthedigitaldevicescreen.”numeric
tag”isatagassignedtoanUIelementshownonthedigitaldevicescreen.Asimpleusecasecanbetap(5),
whichtapstheUIelementlabeledwiththenumber5.
3.swipe(direction:str):Thisfunctionisusedtoswipeonthedigitaldevicescreen,”direction”isastringthat
representsoneofthefourdirections:up,down,left,right.”direction”mustbewrappedwithdoublequotation
marks.Asimpleusecaseisswipe(”up”)whichcanbeusedtoopentheapplistinthehomescreen.
4.press(”HOME”):topresshomebutton.
5.press(”BACK”):topressbackbutton.
6.press(”OVERVIEW”):topressoverviewbutton.
Goal:[taskinstruction].
[fewshotprompt]
Now,giventheparseduiautomatorxml,youneedtothinkandcallthefunctionneededtoproceedwiththetask.
Youroutputshouldincludethreepartsinthegivenformat:
-Description:<Describewhatyouobserveintheinput>
-Thought:<Tocompletethegiventask,whatisthenextstepIshoulddo>
-Action:<Thefunctioncallwiththecorrectparameterstoproceedwiththetask.Youcannotoutputanything
elseexceptafunctioncall>
Youcanonlytakeoneactionatatime,sopleasedirectlycallthefunction.
Pleasenevertakeactionbesideoptionsprovided.
Table5: PromptsusedfortheLLMAgentsandMLLMagents. Partsfor[...] arefilledinaccording
todifferentexperiments,whilethefew-shotexamplesareoptional.
Forfew-shotlearningofagentswithfoundationmodels,weincludeapre-definednumberofexamples
hintingcorrectactionstotake. Specifically,tobuildapromptwithfew-shotexamples,the[fewshot
prompt]partinTable5isreplacedwiththetextillustratingthehumandemonstration. Table6shows
anexemplaryfewshowprompts,withonetransitionofthehumanexpertdemonstration.
Belowillustratesexemplaryhumandemonstration(s),withformat:
-Instruction:<Theinstructionoftask>
-Observation:<Anobservationfromenvironment>
-Action:<Anactiontakenbythehumanexpert>
-NextObservation:<Thenextobservationfromenvironmentaftertheactionistaken>
-Reward:<Arewardafteractionisexecuted>.
-Instruction:turnonalarmat9am
-Observation:[’numerictag’:0,’description’:’android.view.ViewAppslist’,[...]’numerictag’:27,’descrip-
tion’:’android.widget.FrameLayout’]
-Action:swipe(”up”)
-Nextobservation: [’numerictag’: 0,’description’: ’android.view.ViewAppslist’,[...] ’numerictag’: 30,
’description’:’android.widget.FrameLayout’]
-Reward:0.0
Table 6: An exemplary few-shot prompt with one transition of human expert demonstration for
text-basedagents. Theabbreviated[...] partsarefilledwithalistofdescriptionsforUIelements.
22ICLR2024WorkshoponGenerativeModelsforDecisionMaking
C.2 ARCHITECTUREDESIGNFORVLUIAGENTS
The network architecture for VLUI agents is composed of three components: encoder, attention
module,andactionhead. Giventhetaskinstructioncandthevisualscreeno ∈R3×256×128ateach
t
timestept,VLUIagentsgenerateactiona ∈R4intheformofdual-gestureaction.
t
VLUI agents use visual and text encoders to represent screen images o and task instruction c,
t
respectively. Thevisualencoderembedsvisualfeaturee ∈ Rd fromtheobservationo ,andthe
textencoderextractsfeaturese ∈ Rd fromthetaskinstro ut ctionc. Forthevisualencodet r,weuse
c
EfficientNet-b0(Tan&Le,2019)pre-trainedwithImageNetfollowedbyanadaptionlayerusinga
fullyconnectedlayertoadapttheoutputchanneltohiddendimensiond(Liuetal.,2023). Forthe
textencoder,weuseapre-trainedtextencoderofText-to-TextTransferTransformer(Zhan&Zhang,
2023)whichistrainedwithadatasetcomposedofdemonstrationsforsolvingAndroiddevicecontrol
problems(Rawlesetal.,2023). Thetextencoderiskeptfrozenduringthetrainingprocess. The
hiddendimensiondissettoequalthevalueof768forbothvisualembeddingandtextembedding.
The attention module, then, fuses the visual feature e and text feature e into a single vision-
languageembeddinge ∈Rd. Especially,weuseMulo tit -headattentionlayerc (Vaswanietal.,2017)
fused
forcross-attention,withe givenasqueryande givenaskeyandvalue. Giventhefusedfeature
c ot
e ,theactionheadspredicttheactiona . Theactionheadconsistsoffullyconnected(FC)layers
fused t
withthelastlayerhavinganoutputdimensionof4,accountingforthedimensionofa∈R4. The
sequenceofthreeFClayersfollowsoutputdimensionsof(1024,1024,4). Weapplythetanhlayerto
thepredictedaction,observingimprovedperformanceswithnormalizationoftheactionvalues.
C.3 CONFIGURATIONDETAILSFORLLMAGENTSANDMLLMAGENTS
For the experiments in Section 4.2 and Section 4.3, we set the configurations for the foundation
models. Weuseatemperatureof0.1, atop-pof1, andatop-kof1forGemini-ProandGemini-
Pro-V.Wesetthetemperaturetobe0.0andtop-pwiththedefaultvalueof1(asalteringonlyeither
temperatureortop-pfromthedefaultsettingissuggested)forGPT-4andGPT-4V.
C.4 TRAININGDETAILSFORVLUIAGENTS
FortheexperimentsinSection4.2andSection4.4,wetrainVLUIagentswithbehaviorcloning(BC)
over4Kstepswithabatchsizeof512,sampledfromacollectionof210humandemonstrations.
WeusetheAdamoptimizer(Kingma&Ba,2017)withalearningrateof3e-4andadoptacosine
annealinglearningratescheduler. EachtrainingisconductedonasingleNVIDIARTXA6000GPU
andtakesapproximatelyonehour.
D EXPERIMENT DETAILS
D.1 DATASETCOLLECTION
For the few-show learning of LLM and MLLM agents and training of VLUI agents, we collect
humanexpertdemonstrations. Thecollectors(graduatestudents)areinstructedtocompletethesix
representativetasksineachtrainingenvironment. Thedefinitionsofactionspaceforthecollected
demonstrationareintwomodes: theactionspacedefinedwithactionoptionsandtheactionspace
asasetofdual-gestureactions. TheendofeachepisodeisdeterminedbytheADB-basedsuccess
detector,andweexcludethedemonstrationswithfailures.
FortheexperimentsinSection4.2,weexploittrainingenvironmentswithidentifying(ID)numbers
from000to034. Hence,atotalnumberof210trajectoriesofdemonstrationsareprepared. Foragents
using foundation models, each transition (task instruction, observation, action, next observation,
reward)inthetrajectoriesissampledasafew-shotexample,similartopriormethods(Zhangetal.,
2023; Rawles et al., 2023). For VLUI agents, each triplet (task instruction, observation, action)
in the trajectories is used as a data point for composing the training batch. For the experiments
in Section 4.4, we leverage varying numbers of training environments 7, 21, and 35 where the
correspondingidentifying(ID)numbersoftheenvironmentsarefrom000to006,from000to021,
andfrom000to034,respectively. Thetotalnumberofdemonstrationsforeachsettingis42,126,
and210,respectively.
23ICLR2024WorkshoponGenerativeModelsforDecisionMaking
D.2 PERFORMANCESOFVLUIAGENTSINTRAININGENVIRONMENTS
90
60 Train
Test
30
Airplane Alarm1 Alarm2 Brightnes Cs
all
911 Language
Figure10: SuccessratesofVLUIagentstrainedwithBContrainingandtestenvironments. The
differencesbetweenthesuccessratesdemonstratetheheadroomforthegeneralizationability.
Figure10displaysthedifferencesinthesuccessratesofVLUIagentsintrainingandtestenvironments.
ThechallengeswithdiversedeviceconfigurationsdegeneratetheperformancesoftheVLUIagents,
fromhigherthan90%inthetrainingenvironmentstolessthan70%inthetestenvironments.
D.3 EXAMPLESOFVISUALINPUTSWITHSOMPROMPTINGFORMLLMAGENTS
InSection4.3,weinvestigatetheeffectsofSoMpromptingthatseveralpriorworks(Yanetal.,2023;
Yangetal.,2023b)adopted. Figure11presentsseveralexamplesofvisualinputsusedforanalysis.
(a) (b) (c) (d)
Figure11: ExamplesofvisualinputsforMLLMagentswithSoMprompting. Theoverlaidnumeric
tags for selecting icons at (a) the home screen or (b) the menu screen of applications list can be
beneficialforMLLMagents,whilethetagson(c)dialbuttonsor(d)clockUImayconfusethem.
E ADDITIONAL EXPERIMENTS
E.1 MLLMAGENTSWITHDUAL-GESTUREACTIONS
We have implemented additional interfaces for agents employing foundation models and action
options,asintroducedinSection3.1. However,itisstillquestionablewhetherdefiningactionoptions
istrulydesirablefortheseagents. Toanswerthis,weconductacomparisonbetweenagentsonly
generatingactionsinthedual-gestureactionformatandtheagentsusingadditionalactionoptions.
Inthisexperiment,weexamineMLLMagentsemployingGPT-4Vinazero-shotmannerontwo
selectedtasks(AirplaneandAlarm 1)withonlyonerunforsimplicity.
AsshowninTable7,weobservethattheagentsbenefitfromemployingadditionalactionoptions. In
theexperiments,theagentswithoutadditionaloptionsexhibitseveralsuccessfultrialsonAlarm 1,
byincludingtheboundingboxlocationofallUIelementsintheobservationpromptfortheseagents.
However,weobservethattheagentslackgeneratingdiversedual-gestureactionsbutonlyperform
tappingactions. Withtheseresults,weexaminetheproficiencyofLLMagentsandMLLMagents
withtheactionoptionsinSection4.2.
24
)%(
setaR
sseccuSICLR2024WorkshoponGenerativeModelsforDecisionMaking
MLLMagents MLLMagents
(dual-gestureactions) (actionoptions)
Airplane 00 30
Alarm 1 30 50
Table7: SuccessratesofMLLMagentswithdifferentactionspaces. MLLMagents(dual-gesture
actions)generatetheactionsinonlydual-gestureactionformat,andMLLMagents(actionoptions)
leveragetheadditionalactionoptionswedefine.
E.2 LLMAGENTSWITHOPEN-SOURCEMODELS
Whileemploymentoffoundationmodelsformobiledevicecontrolagentsisgaininginterests(Wen
et al., 2023; Yang et al., 2023b), many approaches still rely on closed-source models. However,
leveragingclosed-sourcefoundationmodelslieswithseverelimitations,suchasdifficultiesinfine-
tuning. Instead,onecanemployopen-sourceLLMswhichcanbenefitfromhighflexibilityinusage.
Inthisexperiment,weexaminetheproficiencyofLLMagentswithopen-sourcemodels.
Westudyopen-sourcemodels: Llama2-chat(abbreviatedasLlama2)(Touvronetal.,2023),Llama3,
andAgentLM(Zengetal.,2023). Llama2andLlama3areopen-sourcemodelsthathaveshown
compatibleperformanceswithseveralclosed-sourcedmodels,andAgentLMisaninstruction-tuned
versionofLlama2inacollectionofnumerousagenttasks(includingwebtasks). ForLlama2and
AgentLM, we use 7b and 13b size models. For Llama3, we use an 8b size model. We set the
temperaturevaluetobe0.1andreportacrossthreedifferentruns.
Table 8 show the success rates of LLM with open-source models. We observe that these agents
severely lack the proficiency in performing tasks that we select. While the agents can perform
sub-tasksofopeningthetargetapplicationonthehomescreenorenteringthemenuscreeninsome
trials,asobservedintherollouts,theyfailtocompletetheinstructedtasksinlimitedallowedsteps.
Theopen-sourcemodelsalsostrugglewithgeneratingactionsintheformatweinstruct,whileclosed-
sourcemodelsrarelygenerateactionsinthewrongformat. Withthesepilottestresults,weprimarily
focusonexaminingtheefficacyoftrainingagentsfromscratchwithVLUIagents.
LLMagents LLMagents LLMagents LLMagents LLMagents LLMagents LLMagents
(Llama2-7b) (Llama2-13b) (AgentLM-7b) (AgentLM-13b) (Llama3-8b) (Gemini-Pro) (GPT-4)
Airplane 00±00 00±00 00±00 00±00 63±00 87±07 53±03
Alarm 1 00±00 00±00 00±00 00±00 00±00 27±03 42±13
Brightness 00±00 00±00 00±00 00±00 17±03 05±03 73±12
Table8: SuccessratesofLLMagentswithopen-sourcemodelsLlama2andAgentLMinzero-shot
scenario. TheagentsdonotcompleteanytasksthatLLMagentswithclosed-sourcemodelsofGPT-4
orGemini(inzero-shot)haveachieved.
E.3 VLUIAGENTSWITHREINFORCEMENTLEARNING
WestudyVLUIagentstrainedusingreinforcementlearning(RL)algorithms,byusingthesuccess
signal r as a sparse reward. Formally, the RL agent is trained to maximize the expected return,
t
denotedasfollows:
(cid:20) T (cid:21)
(cid:88)
E γtr ,
πθ t
t=0
whereγ ∈[0,1]isthediscountfactorandT isthelengthofepisode. Inthisexperiment,wefocus
onofflineRLsetting(Langeetal.,2012;Levineetal.,2020),wheretheagentlearnsfromapre-
collecteddatasetgeneratedbysomebehaviorpolicy. Specifically,WeutilizetheimplicitQ-learning
(IQL;Kostrikovetal.2022)algorithm.
IQL is one of the popular actor-critic algorithms. The actor network π parameterized with θ
inferstheactiona ateachtimestept,giventheobservationo andtaskinstructionc ,Thecritic
t t t
networkQparameterizedwithϕestimatesthevalueofactionpredictedbytheactor. IQLleverages
expectileregressionforrobustvalueestimationandimprovesthepolicyusingadvantage-weighted
25ICLR2024WorkshoponGenerativeModelsforDecisionMaking
regression(Peters&Schaal,2007;Nairetal.,2020). Inparticular,IQLintroducesaseparatevalue
network V parameterized with ψ for robust learning. The loss function for the critic in IQL is
formulatedas:
L V(ψ)=E
(ot,a∗
t)∼D(cid:104) Lτ 2(cid:0) Q ϕˆ(o t,a t)−V ψ(o t)(cid:1)(cid:105) ,
L (ϕ)=E
(cid:104)(cid:0)
r +γ·V (o )−Q (o
,a∗)(cid:1)2(cid:105)
,
Q (ot,a∗,rt,ot+1)∼D t ψ t+1 ϕ t t
withLτ(u)definedtobe|τ −1(u<0)|·u2,avaluefunctionV parameterizedwithψ,Q-function
2
Qparameterizedwithϕ,andthedatasetofhumandemonstrationsD ={(o ,a∗,r ,o )}. Then,
t t t t+1
theactoristrainedwithadvantage-weightedbehavioralcloningobjectivedefinedas:
L π(θ)=E
(ot,a∗
t)∼D(cid:104) exp(cid:0) β·A(o t,a t)(cid:1) ·(cid:0) a t−a∗ t(cid:1)2(cid:105) ,
withactionpredictiona predictedbytheactornetworkπ,theadvantageA(o ,a )=Q (o ,a )−
t t t ϕˆ t t
V (o ),andaninversetemperatureβ ∈[0,∞).
ψ t
ThepolicyarchitectureforVLUIagentsfollowsthesamearchitectureofVLUIagentstrainedwith
BC,describedinAppendixC.2. Similarly,thehyperparametersforoptimizersandothertraining
detailsremainthesame,exceptthatweiteratethetrainingover20Kstepstoensuretheconvergence
oftraining. Forthetraining,weemploy35trainingenvironments,namely210successfulhuman
expertdemonstrations.
Figure12showsthesuccessratesofVLUIagentstrainedwithIQL,comparedwithVLUIagents
trainedwithBC.WeobservethattheagentstrainedwithIQLdonotexhibitcompatibleperformances
withtheagentstrainedwithBCacrossalltasks. Weassumetheseresultsoriginatedfromtraining
instability due to sparse rewards, as more training steps are required for the convergence of IQL
training. However, asobservedinKostrikovetal.(2022), weexpectthatofflineRLcanprovide
potentialbenefitsovervanillaBCtraining,suchasutilizingfailuredemonstrations. Weleavetraining
VLUIagentswithIQLmoreefficientlyandwithhigherproficienciesasfuturework.
E.4 VLUIAGENTWITHREPRESENTATIONMODELOFVARYINGCAPACITY
WeconducttheeffectofrepresentationmodelswithvaryingcapacitiesontherobustnessofVLUI
agents. Specifically,wecomparetheVLUIagentsequippedwithvisualencodersusingEfficientNet-
b0,EfficientNet-b3,andEfficientNet-b7(withincreasingnumbersofparameterswithvaluesof5.3M,
12M,and66M,respectively),whicharepre-trainedwithImageNet.
Figure13demonstratestheexperimentalresults. Toillustrate,ourexperimentindicatesnosignificant
improvementsbyemployingrepresentationmodelswithincreasedmodelsizes. Withtheseresults,
weexpectthatincreasingthetrainingdatadiversityismoredesirablethanincreasingthemodelsizes
inourcurrentbenchmarksetting. Also,weaddthatthehighermodelcapacitycanbebeneficialfor
developingmulti-taskpolicieswithagreaternumberoftasks.
90 BC IQL 90 EfficientNet-b0 EfficientNet-b3 EfficientNet-b7
50 50
10 10
Airplane Alarm1 Alarm2 Brightness Call911 Language Airplane Alarm1 Alarm2 Brightness Call911 Language
Figure12: SuccessratesofVLUIagentstrained Figure 13: Success rates ov VLUI agents with
with BC and IQL. Training with IQL does not varying size of visual encoders. We do not ob-
result in as high performances as training with servesignificantbenefitsbyincreasingthemodel
BC,presumablyduetotraininginstability. capacitiesofrepresentationmodels.
26
)%(
setaR
sseccuS
)%(
setaR
sseccuS