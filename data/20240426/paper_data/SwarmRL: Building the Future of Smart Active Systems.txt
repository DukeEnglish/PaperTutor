SWARMRL: BUILDING THE FUTURE OF SMART ACTIVE
SYSTEMS
SamuelTovey†,ChristophLohrmann†,TobiasMerkt
DavidZimmer,KonstantinNikolaou,SimonKoppenhöfer
AnnaBushmakina,JonasScheunemann,ChristianHolm
InstituteforComputationalPhysics
UniversityofStuttgart
70569,Stuttgart,Germany
{stovey, clohrmann, holm}@icp.uni-stuttgart.de
ABSTRACT
This work introduces SwarmRL, a Python package designed to study intelligent active particles.
SwarmRLprovidesaneasy-to-useinterfacefordevelopingmodelstocontrolmicroscopiccolloids
usingclassicalcontrolanddeepreinforcementlearningapproaches. Thesemodelsmaybedeployed
insimulationsorreal-worldenvironmentsunderacommonframework. Weexplainthestructure
of the software and its key features and demonstrate how it can be used to accelerate research.
With SwarmRL, we aim to streamline research into micro-robotic control while bridging the gap
betweenexperimentalandsimulation-drivensciences. SwarmRLisavailableopen-sourceonGitHub
athttps://github.com/SwarmRL/SwarmRL.
Keywords ActiveMatter,DeepReinforcementLearning,HPC,Micro-robotics
1 Introduction
Masteringcontrolofmicrorobotsatthemicroscopicscalehasthepotentialforinsightsandthedevelopmentofnew
technologiescapableofchangingtheworld. Whetheritbeanimprovedunderstandingofbacterialnavigationstrategies
ordirectcontrolovermicroscopicrobots,numerousfields,includingconstruction[Hsuetal.,2016],plantpollination
and ecosystem defense [Stefanec et al., 2022], and search and rescue [Murphy et al., 2008], will be enhanced by
advancementsinmicro-scalecontrolresearch. Onefieldinparticularthatwillseesignificantchangesismedicine,
whereitisexpectedthatmicroroboticagentswillbecapableofadvancedtreatmentstrategiesincludingtargetedcancer
therapies[Schmidtetal.,2020],assistedanddrugdelivery[NelsonandPané,2023,Felfouletal.,2016,Hosseinidoust
etal.,2016,Zhuangetal.,2015], assistedfertilisation[Medina-Sánchezetal.,2016,Khaliletal.,2014], amongst
many others [Nelson et al., 2010, Li et al., 2022]. The two most significant challenges in realising this future are
developingthetoolsandmaterialstobuildsuchcapablemicroscopicagentsandprogrammingthemtoachievecomplex
taskswithminimalobservedinput. Theseagentsshouldnotonlybecapableofperformingcomplexactionssuchas
swimming,pushing,andperhaps,communications,buttheyshoulddosowithoutconstantinputfromasupervisor.
Thefirstofthesechallengeshasbeen,andisbeing,extensivelyinvestigated. Currently,therearemanyapproachesfor
designingmobilemicrorobots[Dabbaghetal.,2022]. Initialapproachesincludedthelight-drivenJanusparticles[Su
etal.,2019]andcoils[Wangetal.,2019]andmagneticallydrivendevicestakingonnumerousshapes[Shenetal.,
2023,Dhatt-Gauthieretal.,2023]. Recentapproacheshavealsoincludedusinglivingcellstoproducemotiononthis
microscopicscale,theXenobotsbeingaprimeexample[Kriegmanetal.,2020,Blackistonetal.,2021]. However,the
controloftheseagents,oncebuiltanddeployed,isacomplicatedproblem. Duetothesizeoftheseagents,directly
2Theseauthorscontributedequally
4202
rpA
52
]OR.sc[
1v88361.4042:viXraSwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
installingcomputationalprocessingpowerontothemis,thusfar,intractable. Therefore,simplisticalgorithmsthataim
toencodereactionarybehaviourintotheagentsareoftenconstructed,somethingthatwouldrequirelimitedon-board
processing. Examplesofthisincludetheuseofmicrorobotstoperformchemotaxis[Hartletal.,2021]bychangingtheir
shapesuponexposuretovaryingfields,objectmanipulation[Kimetal.,2016]throughtheuseofanexternalmagnetic
field,andreproducingswarmingbehaviourbyapplyingsimplerotationsandtranslationstoindividualagentsbyasetof
rulestriggeredbychangesinlocalenvironment[Bäuerleetal.,2020,Lavergneetal.,2019]. Morerecentapproaches
haveutilisedreinforcementlearningapproachestofurtherpushinthedirectionoflearnedcontrolstrategies. Qinetal.
[2023]utilisedaQ-learningalgorithmtolearnswimmingstrategiesingatedmicroswimmers,Borraetal.[2022]used
anactor-criticframeworktolearncompetitivecapture-evasionstrategiesfromhydrodynamiccues,andMuiños-Landin
etal.[2021]investigatedactor-criticlearnednavigationstrategiesundertheinfluenceofstochasticenvironments. Inall
cases,demonstratingunderwhatconditionsmicrorobotscanperformcomplextasksinvariousenvironmentsbringsus
astepclosertorealisingthefullpotentialofthistechnology.
Itisclearthatresearchinthedirectionofmicro-roboticcontrolisbothpromisingandexcitinglymulti-disciplinary;
however,duetothenatureofthetechnologyrequired,whetheritbecomplexexperimentalequipment,deepknowledge
ofstate-of-the-artmachinelearningalgorithmsandtheirimplementation,ortheexpertisetoconstructphysicallyrealistic
simulations,theentrybarriercanbehighforspecialistswhofocusononlyoneoftheseareas. Forthisreason,we
havedevelopedtheSwarmRLPythonpackagetocombineeachcontrolworkflowelementunderacommonframework
andacceleratealgorithmdevelopmentanddeployment. SwarmRLenablesresearcherstoapplycontrolstrategiesto
particlesinsimulationorexperiments. Thesestrategiescanbebuiltfromsetsofrulesthatarerigidlyprogrammedand
utilisetheenvironmentoftheparticles,ortheycanbetrainedusingtheactor-criticreinforcementlearningapproach,
offeringcompleteflexibilityinhowtheagentsarecontrolled,whatactionstheyarecapableof,andwhattasksthey
mustperform. Beyond thecustomizability, whereverpossible, itisbuiltontopoftheJAXBradburyetal.[2018]
ecosystemtomaximiseperformanceandissuitedfordeploymentonlargedistributedcomputeclusters. Thispaper
aimstointroducetheSwarmRLsoftwareanddemonstratehowitcanbeappliedtoperformfrontierresearch. Webegin
withanoverviewofthetheoryunderpinningthesoftware,explicitlydiscussingmicroscaleactivematter,simulatingit
inaphysicallyrealisticmanner,andthereinforcementlearningwehavebuilttocontrolit. Afterwards,thearchitecture
ofthesoftwareisdescribed,detailinghoweachpieceofthesoftwarefitstogetherandwhatcanbedonewiththem.
Finally, we discuss some of the unique features of SwarmRL, including performance and visualisation capabilities,
beforeconcludingwithanoutlookoftheproject. WithSwarmRL,wehopetoenablescientistsfromdiversebackgrounds
tocontributetoanddevelopthefieldofmicro-roboticsresearchandrealisethepotentialofthistechnology.
2 Theory
SwarmRLharnessestoolsfromarangeoffields,thelargestofwhichareactivemattersimulationandreinforcement
learning.Inordertounderstandandbetterutilizethesoftwareinfrastructure,itisimportanttoalsohaveanunderstanding
ofthetheoryitimplements. Inthissection,weintroducetheimportanttheoreticalconceptsonwhichSwarmRLisbuilt,
beforeexploringthearchitectureofthepackage.
2.1 ActiveMatter
Biologicalandartificialactivesystems
Active matter refers to systems that consume energy from their surroundings and are thus internally driven out of
equilibrium [Romanczuk et al., 2012]. All biological and human-made machines fall under this term. However,
in the context of this work, we focus on the branch of active systems that consists of small active particles that
useenergytoperformpersistent,directedmotion. Thismuchmorenarrowdefinitioncoversplentyofexamplesin
biologyandengineering. ManybacterialikeEscherichiacoli[WadhwaandBerg,2022],archaealikeThermoplasma
volcanium[Jarrelletal.,2021]andsmalleukaryoteslikeChlamydomonasreinhardtii[SilflowandLefebvre,2001]
use molecular motors and organelles like archaella, flagella or pili to self-propel. Artificial microswimmers can
bemadefromcolloidalparticlesthatuse,e.g.,catalyticreactions[Howseetal.,2007],self-generatedtemperature
gradients [Jiang et al., 2010] or magnetic fields [Mandal et al., 2018] to generate propulsion. In some more rare
cases,theactiveparticlescanself-propelandself-steer,i.e.,usetorquetoactivelychangetheirdirection. Forbacteria,
tumbling[BergandBrown,1972,Darntonetal.,2007]isawell-knownmechanismforchangeofdirection. Some
artificialmicroswimmerscanalsobesteeredby,e.g.,changeinlaserfocus[Bäuerleetal.,2020]ormagneticfield
direction[Carlsenetal.,2014]. Inalltheseexamples,theactiveparticleisofmicrometersizeandsuspendedinaliquid,
makingtranslationalandrotationalBrownianmotionanon-negligiblefactorthatcompeteswiththedeterministicactive
motion.
2SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
Simulation
Activemattersystemswithself-propelledsmallparticlesareusuallymodelledwithintheframeworkofstochastic
dynamicsoftheindividualconstituents. Weusethetwo-orthreedimensionaloverdampedLangevinequations
(cid:113)
r˙ (t)=γ−1(cid:2) Fact(t)ˆe (t)+F (r ,{r })(cid:3) + 2k Tγ−1ξt(t), (1)
i t i i i i j B t i
(cid:20) (cid:113) (cid:21)
ˆe˙ (t)= γ−1(cid:2) Mact(t)nˆ (t)+M(r ,{r })(cid:3) + 2k Tγ−1ξr(t) ×ˆe (t), (2)
i r i i i j B r i i
wherer denotesthepositionofparticlei,γ thetranslational(rotational)frictioncoefficient,Facttheactivedriving
i t(r)
forcethatmodelsself-propulsion,ˆetheunitvectorrepresentingthedirectionoftheparticle,F (r ,{r })aconservative
i i j
forcefrominteractionsbetweenparticleianditsenvironmentaswellasfrominteractionswithotherparticlesj,k
B
(cid:68) (cid:69)
theBoltzmannconstant,T theabsolutetemperature,ξt,(r) thetranslational(rotational)noisewith ξt,(r)(t) = 0
i
(cid:68) (cid:69)
and ξt,(r)(t)⊗ξt,(r)(t′) = δ δ(t−t′)1, where ⟨·⟩ denotes an ensemble average (expectation value) and 1 the
i j ij
unity matrix, Mact an active steering torque, nˆ a unit vector perpendicular toˆe that sets the steering direction and
M (r ,{r })aninteractiontorqueanalogoustoF . WestressthetimedependenceofFactandMactbecauseSwarmRL
i i j i
is designed to focus on control and decision-making on the level of single particles. Collective behavior and task
fulfillmentaretobeachievedbytheparticles’actionsandwhattheycancontrolratherthanbyexternalfieldsactingon
allparticles.
2.2 ReinforcementLearning
Asasub-fieldofML,RLisconcernedwithhowintelligentagentscantakeactionsinanenvironmenttomaximizea
cumulativereward. Itallowstheseagentstolearnoptimalprotocolstoachieveagiventaskorasetoftaskswithout
explicitlytellingthemhow. Thisisoftentimesusefulwhenataskistoocomplexforasimplealgorithmorcontrol
system. Itcanalsobeusedwhenthefocusoftheinvestigationistheemergenceofastrategyforaproblem. RLis,
however,data-inefficientandisthususedmostlyforproblemswheredataexistsabundantlyandwheretheactionspace
isrelativelysmallsuchasautonomousdriving[Kiranetal.,2021],videogames[Mnihetal.,2015],androbotics[Ibarz
et al., 2021]. It reached great popularity in 2016 when Deepmind’s AlphaGo won against the top Go player Lee
Sedol[Silveretal.,2016]. Whenusingitforspecifictasks,however,RLcanachieveremarkableresults. In2022,
itsolveda50-year-oldopenquestioninmathematicshowtomostefficientlyusematrixmultiplication[Fawzietal.,
2022]; thisdiscoverylikelyimprovestheefficiencyofmachinelearningsignificantlyduetoitswidespreaduseof
matrixmultiplication. ItisfurtherincreasinglyusedinPhysicsresearchforsophisticatedtaskssuchasquantumerror
correction[Zengetal.,2023]andthemagneticcontroloftokamakplasmas[Degraveetal.,2022]. Thissectionprovides
anintroductiontoRLandexplainsthetrainingapproachcurrentlyimplementedinSwarmRL,namely,deepActor-Critic
(AC).
2.2.1 Actor-CriticReinforcementLearning
TheRLtrainingparadigmisconcernedwithiterativelyimprovinganagent’sdecision-makingsuchthatitcanachievea
pre-definedtaskasefficientlyaspossible. Inpractice,thisisperformedbyplacingagentsintoanenvironmentwith
whichtheycaninteractbyperformingactions,a . Todecideonsuchanaction,theagentreceivesastatedescription,s ,
t t
thatdescribestheenvironmentwithsufficientinformation. Themodelusedtodecideontheactionisreferredtoasthe
policy,π :s →a ,oftentakentobeaneuralnetworkwithparameters,θthatisthenreferredtoasanactor. These
θ t t
actorstypicallyproduceaprobabilitydistribution,whichcanthenbesampledtoselectanactiongiventhestate. After
anactionistaken,anewstateisproducedalongwithareward,quantifyingtheagent’sprogresstowardsachieving
itstask,aprocessoutlinedinFigure1. Fundamentally,thegoaloftraininginRListofindtheoptimalpolicythat,
throughoutatrajectory,τ,maximisesacostfunction,J(π ),i.e.,
θ
(cid:90)
π∗ =argmaxJ(π )=argmax P (τ|π )·R(τ)=argmax⟨R(τ)⟩ , (3)
π
θ
π τ
θ
π
τ∼πθ
whereR(τ)isthecumulativerewardcomputedfromatrajectory,P(τ|π )istheprobabilityofatrajectorygiventhe
θ
currentpolicy,andtheangledbracketsdenoteanaverageovermanytrajectoriessampledundertheπ policy. This
θ
isachievedbyencouragingthepolicytoincreasetheprobabilityofselectingactionsthatreturnedgoodrewardsand
discouraging those that didn’t. In practice, training is done throughout discrete-time episodes where state, policy
probabilities,andrewarddataarecollectedandusedtoupdatethepolicyviathegradientascentalgorithm
′
θ =θ+η·∇ J(π ), (4)
θ θ
3SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
Agent
Environment
Figure1: GraphicaloverviewoftheRLworkflow. Theagentselectsanaction,a ,basedinitscurrentstate,s ,andacts
t t
withintheenvironment. Thisactionyieldsanewstate,s andanassociatedreward,r .
t+1 t+1
withlearningrateη,and
(cid:42) T (cid:43)
(cid:88)
∇ J = ∇ logπ (a |s )·r (5)
θ θ θ t t t
t τ∼πθ
with time step, t, and instantaneous reward, r . Typically, to ensure convergence in the long-term rewards and to
t
encouragenear-termoverlong-termprogress,anexpectedreturnsfunction,G ,willbeusedintheform
t
T
(cid:88) ′
G = γt −tr , (6)
t t′
t′=t
where T is the final time step of the episode, and γ is a discount factor set as a hyperparameter during training
influencingtheimportanceoflate-timerewards. Whilethisapproachcanbesufficientintrainingagents,itcanbreak
downinstochasticenvironmentsandhigh-dimensionalspaces[Nachumetal.,2017]. Toaddressthis,SuttonandBarto
[2018]showedthatintroducingatrainablebaselineorvaluefunction,Vπ tocomputeanadvantagefunction
Aπ =G −Vπ, (7)
t t t
canresultinthestabilizationoftheexpectedreturnsandimprovedconvergencetimeinthetraining,resultinginthe
reformulationofEquation5to
(cid:42) T (cid:43)
(cid:88)
∇ J = ∇ logπ (a |s )·Aπ . (8)
θ θ θ t t t
t τ∼πθ
When this value function is implemented as a neural network, it is often referred to as a critic, thus actor-critic
reinforcementlearning. Thecritic’sroleistodeterminethevalueofthestateinwhichtheagentcurrentlyfindsitself.
Whileseveraldefinitionsexist,inSwarmRL,thevalueisdefinedasthetheoreticalreturnspossibleifonestartsinastate,
s andfollowspolicyπ . Itimprovesthispredictionbytrainingonthetrueexpectedreturns,Equation6,computed
t θ
during episodes through a desired regression algorithm. Therefore, in Equation 8, the advantage function guides
trainingbyencouragingselectedactionsthatarebetterthanorequaltothoseexpectedfromthevaluefunctionand
discouragingthoseconsideredbelowwhatistheoreticallypossible. Forthisreason,ACRLtrainingisoftenconsidered
adversarialinnature,withtheactorlearningtobeatthecriticandthecritic,inturn,learningthecorrectvalueofthe
encounteredstates. ThetrainingprocedureoutlinedhereisreferredtoasVanillaPolicyGradientduetoitbeingthe
simplestformofactor-critictraining. Morecomplexmethodsextendonthemethoddiscussedhere,suchastrustregion
policyoptimisation[Schulmanetal.,2017a]andproximalpolicyoptimisation[Schulmanetal.,2017b],thelatterof
whichisalsoimplementedinSwarmRL.
2.2.2 Multi-AgentReinforcementLearning
AsSwarmRLspecializesincontrollingmicroscopicagents,therewillbemanyoccasionsinwhichmultipleagentsmust
besimultaneouslycontrolledandtrained. Thisisaddressedbymulti-agentreinforcementlearning(MARL)[Zhang
4SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
etal.,2021]. InMARL,N autonomousagentsinteractwiththeenvironmentandeachother. Thisextensionbrings
several new aspects to the problem, such as cooperation and competition among agents and various information
structures. Inthefollowing,wewilladdresstheseaspectsbriefly.
Cooperative,CompetitiveandMixedSetting Inafullycooperativesetting,allagentsshareacommonreward,
i.e., r1 = r2 = ... = rN = r or a team-average reward where r = 1 (cid:80) r [Zhang et al., 2018]. The latter
N i∈N i
isageneralizedversionoftheformer. Differentrewardfunctionsforeachagentallowmoreheterogeneityamong
agents[Zhangetal.,2021]. AfullycompetitivesettinginMARListypicallymodelledasazero-sumMarkovgame
with(cid:80) ri =0[Littman,1994]. Mostliteraturefocusesontwoplayerswhereoneagent’srewardistheother’sloss,
i∈N
somethingleveragedin,forexample,predator-preystylegames. Amixedsettingimposesnorestrictionsonthegoal
andtherelationshipamongagents[HuandWellman,2003].
InformationStructures AnothercriticalchallengeinMARListhequestionwhoknowswhatduringtrainingand
execution. Thisinvolvesboththedecision-makingprocessaswellastheoptimizationprocess. Threedifferentforms
ofinformationstructurearegenerallyaccepted. Thefirstisthecentralizedsettingwithaso-calledcentralcontroller
aggregating joint observations, actions, and rewards and allowing for centralized learning. The execution, on the
otherhand,caneitherbecentralizedordecentralized. Acentralizedexecutedpolicyproducesajointactionforall
agents, while decentralized produces individually executed policies. The latter is the popular learning scheme of
centralized-learning-decentralized-execution[Oliehoeketal.,2016,Hansenetal.,2004]. Ontheotherside,thereis
thefullydecentralizedsettinginwhicheachagentactsbasedonlyonlocalobservation. Thissettingresultsinthe
so-calledindependentlearningscheme[Tan,1993]. Acombinationofbothisadecentralizedsettingwithnetworked
agents[Zhangetal.,2021]. Thisallowsagentstosharelocalinformationwiththeirneighbours. Thechoiceofsetting
dependsontheproblemathand.
3 Architecture
SwarmRLhasbeendesignedtolowertheentrybarrierforscientiststoaccessstate-of-the-artreinforcementlearning
algorithmsefficiently. Italsooffersawidedegreeofcustomizationandchoiceofalgorithmsthatcanbeusedduring
experimentstomaximisetheexperienceduser’sflexibility.Toachievebothofthesegoals,SwarmRLisconstructedusing
amodular,objectorientedprogrammingstructurewherealmostallpipelinecomponentsarewrittenasimplementations
ofanabstractparentthatdefinestheinterfacethroughwhichthecomponentscommunicate. ThePythonprogramming
languagehasbeenusedthroughoutasitismostcommonamongstscientificdisciplines,however,wheneverpossible,
librarieshavebeenusedtoimproveperformancethroughvectorizedmaps(vmaps),multi-threading,GPUuse,andJust
InTime(JIT)compiling[Bradburyetal.,2018]. Acoarse-grainedflowchartofSwarmRLispresentedinFigure2where
eachmajorcomponentofthearchitectureisshownwithitscorrespondingconnections. ThestructureofSwarmRL
isbuiltaroundtheinteractionbetweentheEngine,whetherthatbesimulationorrealexperiment,andtheAgents,
the algorithms used to control the particles in the simulation. Connecting the Agents to the Engine is the Force
Function. ForceFunctionsareeitherconstructeddirectly,orthroughaTrainerinthecaseswheretrainableAgents
are in use. Finally, the Agent holds Tasks, Actions, and Observables, each of which provide information and
functionalitytothealgorithmbeingused. Therestofthissectionisdedicatedtoexplainingeachimportantcomponent
ofSwarmRL,wheretheyfitintothedifferentcapabilitiesofthesoftware,andhowtheycanbeadaptedorextendedfor
individualimplementations. Itshouldbenotedthat,toreducespaceandimprovereadability,allcomments,typehints,
andmethodsnotdirectlyrelatedtothediscussionhavebeenremovedfromcodesamples.
3.1 Engine
TheEngineclassofSwarmRLisoneofitsmostimportantcomponents. ItrepresentstheenvironmentoftheAgents
andpropagatesthestateaccordingtothegivenactions. SwarmRLshipswithanengineconnectedtotheESPResSo
simulation software [Weik et al., 2019] to solve Equations (1) and (2) numerically. Through ESPResSo, we also
offertheunderdampedLangevinequationandacouplingtoexplicithydrodynamicswithlatticeBoltzmannasthe
solveroftheNavier-Stokesequations. SwarmRLalsoshipswithrudimentaryexamplesforconnectingitdirectlyto
anexperiment. Directexperimentalconnectionsareoftenkeptprivateoutofrespecttotheinstitutionsthathostthe
experiments. However,mostimplementationsthusfarusetheTCPcommunicationsprotocoltosendactionsandreceive
stateinformationtohardware. FutureworkonSwarmRLaimstounifythiscommunicationprocedureusingtheROS
ecosystem[Macenskietal.,2022]. Incaseswhereusersmustconnecttheirownengines,beitsimulationorexperiment,
theyinheritfromtheEngineparentclass. TheymustimplementthetwomethodsoutlineinCodeSample1.
1 class Engine:
2
5SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
Micro-swimmer
Type
Task
Actions
Observable
Colloids Engine Force Function Agent
Update Policy
Exploration
Strategy
Trainer
Sampling
Strategy
Network
Intrinsic
Reward
Figure 2: Overview of the SwarmRL software architecture. Light green boxes on the right-hand side of the figure
representmoduleswithdefaultsettingsthatcanbeadjustedbyusersbutdonotneedtobe,darkgreenboxesindicate
settings that need to be included in a system definition, blue boxes correspond to those directly handling colloid
intelligenceandproperties,theengineisinorange,andgreyrepresentsclassesthattalktotheengine.
3 def integrate(
4 self, n_slices, force_model,
5 ):
6 raise NotImplementedError
7
8 def get_particle_data(self):
9 raise NotImplementedError
CodeSample1: SwarmRLEngineparentclass.
Theintegratemethodisusedtostepforwardeitherinsimulationorinanexperiment. Ittakesasargumentstheforce
model,whichholdsthecontrolalgorithmsfortheparticlesintheexperiment,seesection3.5,andnumberofslicesto
beloopedover,thatis,thenumberoftimestheforcemodeliscalledfornewactionstobeappliedtotheparticles. In
asimulation,thetimein-betweensuccessivetimeslicesisfilledwithintegratorsteps,applyingthechosenactionat
eachtime-step. Incontrast,inexperiment,thiswouldbeapplyingtheactionsforafixedperiodbetweenslices. When
instantiated,theseparameters,time-steportimebetweenactions,aregiventothespecificengine. Thesecondmethod
thatmustbeimplementedisget_particle_data. Thismethodmustcollectinformationabouttheparticlesinthe
systemandgiveitbacktotheuseruniformlysothatallpartsofSwarmRLcanworkwiththem. Initscurrentstate,
SwarmRLdefinescolloidswithpropertiesoutlinedinCodeSample2.
1 class Colloid:
2 pos
3 director
4 id
5 velocity
6 type
CodeSample2: SwarmRLColloiddataclass.
6SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
Here,allinformationrelevanttothemodelsisassignedtoacolloidclassandusedtopredictnewactionsandtocompute
rewards.
3.2 Agents
SwarmRLwasbuilttoserviceavarietyofsimulationsandRLstudies. Alargecomponentinenablingthis,istheAgent
class,whichmanageshowagentsinthesimulationsarecontrolled,whattheysee,andwhattheyaretaskedwithdoing.
TheAgentcollectsthecomponentsintroducedintheprevioussectionsandrepresentsthefullsetofsensing,control,
andobjectivesofthecolloidsinquestion. ThisinformationcanthenbeusedbySwarmRLtoeffectivelydeploythese
agentsinthesimulationenvironment. Codesamples3and4demonstratetheconstructionoftwodifferentagentswhich
couldbothbeaddedtoasinglesimulationwithinSwarmRL.
1 ac_agent = srl.agents.ActorCriticAgent(
2 agent_type=0,
3 network=srl.networks.FlaxNetwork(...),
4 task=srl.tasks.RotateRod(),
5 observable=srl.observables.VisionCones(),
6 actions=action_dict
7 )
CodeSample3: Actor-CriticAgent
8 ac_agent = srl.agents.Lymburn(
9 agent_type=1,
10 homing_location=[500., 500., 500.]
11 )
CodeSample4: ClassicalAgent
Intheaboveexamples,theactor-criticreinforcementlearningapproachcontrolsonegroupofparticles. Thismeans
theirpolicycanbetrainedandadaptedtonewenvironments. Theotherexampleusesaso-calledClassicalAgent,in
particular,onebasedontheswarmingagentsof[Lymburnetal.,2021].
3.3 ClassicalControl
SwarmRL,allowsuserstocontroltheactionsoftheparticlesinthesystemusingbothRLandclassicalalgorithms.
Classicalalgorithms,inthiswork,canbeimplementedwithouttrainingandtypicallyfollowadefinedsetofrules.
Suchsetsofruleshaveoftenbeenstudiedintheliteraturetoreproducebehaviourinmicroswimmers[Bäuerleetal.,
2020,Lavergneetal.,2019]. ClassicalagentsareimplementeddirectlyintotheAgentchildclassonthesoftware
side. However,theymustfollowthesamestructure,namely,ateachtimeslice,theinformationofallparticlesinthe
systemispassedintotheclassandinturn,itmustreturnanactionforeachparticle. Initscurrentstate,SwarmRLoffers
classicalalgorithmsforthepublishedmodelsofLymburnetal.[2021],Bäuerleetal.[2020],andLavergneetal.[2019].
FutureworkaimsatincludingModelPredictiveControl(MPC)algorithmsintotheSwarmRLecosystem.
3.4 MachineLearningControl
The machine learning side of agent control is driven by multi-agent reinforcement learning. However, due to the
complexnatureofthisapproach,severaldesigndecisionhavebeenmadetobalanceflexibilitywithlimitingcomplexity.
ThemainworkhorseofthemachinelearningdrivencontrolistheNetworkclass,whichrequirestwomethodstobe
implemented,compute_actionwhichcomputestheactionoftheagent/sunderstudy,and__call__,whichreturns
actionprobabilitiesand,inthecaseofactor-criticapproaches,alsothepredictedstatevalues.
Architecture
SwarmRL currently utilizes the Flax neural network library [Heek et al., 2023] exclusively. In order to provide an
interfaceforcomplexarchitectures,SwarmRLrequiresonlyasingleFlaxmoduletobepassedtotheFlaxNetwork
class. Thismodulemustreturnboththeactionprobabilities,andthepredictedvaluefunctionincaseofactor-critic
training. Thisapproachallowsimplementingverycomplexneuralnetworkarchitecturessuchasgraphmodels,withthe
sameAPIinterfaceassimpledenseactorsandcritics. AnexampleoftwocommonlyusedapproachesislistedinCode
Samples5and6.
7SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
VanillaGradientUpdate[Suttonetal.,1999]
PolicyUpdate
ProximalPolicyOptimization[Schulmanetal.,2017b]
ExpectedReturns
Returns
GeneralizedAdvantageEstimation[Schulmanetal.,2018]
CategoricalDistribution
SamplingStrategy
GumbelTrick[Gumbel,1954]
Table1: OverviewofRLalgorithmoptionsavailableinSwarmRL
12 class DisjointActorCritic:
13
14 def __call__(self, x):
15 # Perform the actor forward pass.
16 actor_values = Dense(12)(x)
17 actor_values = relu(actor_values)
18 actor_output = Dense(4)(actor_values)
19
20 # Perform the critic forward pass.
21 critic_values = Dense(12)(x)
22 critic_values = relu(critic_values)
23 critic_output = Dense(1)(critic_values)
24
25 return actor_output, critic_output
CodeSample5: Disjointactor-critic
26 class CombinedActorCritic:
27
28 def __call__(self, x):
29 # Perform the shared layer computation.
30 shared_layer = Dense(12)(x)
31 shared_layer = relu(shared_layer)
32
33 # Compute the actor output.
34 actor_output = Dense(4)(shared_layer)
35
36 # Compute the critic output.
37 critic_output = Dense(1)(shared_layer)
38
39 return actor_output, critic_output
CodeSample6: Combinedactor-critic
ThesecodeblocksshowadisjointandcombinedactorcriticapproachbundledintothesameFlaxnetworkstructure.
Inthisway,manycomplexneuralnetworkarchitecturescanbeconstructedunderasinglemoduleincludinggraphor
transformermodules.
Apartfromthearchitecture,severalcomponentsareessentialtotrainingACmodels: thepolicyupdatealgorithm,the
approachusedintheexpectationcomputation,andthestrategyforsamplingactionsfromthelearneddistribution.
Actions
ActionsinSwarmRLaredictatedbydataclasseswhichcanbereadbyanengineandturnedintoforces,torques,or
othervaluesinasimulationorexperiment. Atthetimeofthisreport,SwarmRLhasbeenfocusedonmicro-robotics
withstandarddirectioncontrol,therefore,theclassisbuiltfromasimplesetofattributesoutlinedinCodeSample7.
1 class Action:
2 force = 0.0
3 torque= np.zeros((3,))
4 new_direction = None
CodeSample7: SwarmRLActiondataclass.
8SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
Forceisappliedalongtheagent’sdirectoraxis,torquedescribestheagent’srotation,andnew_directionprovidesa
sometimessimplerapproachtoatorquecalculation. Fromthistemplate,wecreateanactionbyspecifyingtheclass’s
force,torque,andnewdirectionattributesandaddthemtoadictionarytobepassedtoanagent. Forexample,ifthe
agentshouldbecapableoftranslationforwards,translationbackwards,clockwise,andcounterclockwiserotation,the
setofactionscanbepassedtotheAgentasinCodeSample8.
1 agent_action_dictionary = {
2 "Forwards": Action(force=10.0),
3 "Backwards": Action(force=-10.0),
4 "CCW": Action(torque=np.array(0., 0., 10.))
5 "CW": Action(torque=np.array(0., 0., -10.))
6 }
CodeSample8: ExampleActiondictionary.
3.5 ForceFunction
TheinteractionmodelinSwarmRLisresponsibleforconsumingdatafromthesimulationandsendingbackcomputed
actions for each of the agents. SwarmRL can control agents using machine learning and standard control theory
approaches. TheparentclassforthemodelsrequiresonlyasinglefunctiontobecalledbytheEngine: calc_action,
which takes a list of agent objects as input and returns a list of actions. This is the only occasion in which the
environment communicates with the control algorithms and the communication must adhere to this very ’narrow’
interface. ThisdesignchoiceenablesnoviceSwarmRLuserstoswitchoutenvironmentswithoutadaptingtheirRLsetup
andexperienceduserstoeasilyimplementnewenvironmentsaccordingtotheirneeds.
Agentinformationispassedthroughasadictionarywhencreatingamodel,demonstratedinCodeSample9.
1 interaction_model = ForceFunction(
2 agents={
3 "0": ActorCriticAgent,
4 "1": ClassicalAgent
5 }
6 )
CodeSample9: ExampleForceFunctioninstantiationforaclassicalandactor-criticRLagent.
Inthecaseabove,theexperimenthasatleasttwotypesofparticles.Oneiscontrolledusinganactor-criticreinforcement
learningapproachwhiletheothertakesactionscomputedbyaclassical,non-trainablealgorithm. Inthisway,SwarmRL
usershavecompletecontrolovertheagentsintheirsimulationsandexperiments.
3.6 Tasks
AsthegoalofSwarmRListobeusedforthestudyofmicro-scaleroboticorbiologicalsystemstolearnabouttheir
behaviourandcontrol,itisoftenimportanttomeasurehowwellthesystemisdoing. InRL,thisisoftenreferredtoas
therewardwhichismaximisedduringtraining. Forclassicalalgorithms,ataskcanalsobeusedasameasureofhow
welltheapproachisdoingorpassedintothemodelasfeedback. InSwarmRL,systemperformancemeasurementis
handledbytheTaskclass. Theparentclassofthetaskisbuiltupofseveralmethodsthatneedtobeimplementedinthe
caseofacustomtask. ThereducedclassarchitectureisdisplayedinCodeSample10.
1 class Task:
2 @property
3 def kill_switch(self):
4 pass
5 def __call__(self, colloids):
6 raise NotImplementedError("Implemented in child class.")
CodeSample10: SwarmRLTaskparentclass.
Theaboveclasscontainstwofundamentalcomponents,thekillswitchandthecallmethod. Asitsnamesuggests,the
killswitchisusedtoendthecurrentrun. IfthispropertyissettoTrue,SwarmRLwillstopthecurrentrunningengine
andeitherrestartit,orjustfinish. Thisisused,forexample,inepisodictraining,wherethetrainingshouldstopandthe
engineshouldresetafteranamountoftimeorwhenacriterionisreached. However,thekillswitchcanalsobeusedfor
morecriticalcases,suchasinanexperimentwhenitappearsthattheagentswouldperformdamagingactions. The
9SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
secondcomponent,thecallfunction,isusedtocomputeavaluedescribingthecurrentstateofthistask,ideally,large
positivevaluesmeanthetaskisbeingachieved,whereasnegativevaluesmeanitisactivelynotbeingachieved. For
example,whentrainingRLagentstorotateanobject,thecallmethodwouldcomputetherotationspeedandreturn
somepositivevalueifitwasincreasingorlarge,andanegativevalueifitweredecreasing.
Attimes,wewantagentstoperformmorethanjustonetask,inthesecases,twooptionsexist. Thefirstistowrite
acustomtaskthatprovidesfeedbackontwodifferentobjectssuchashowfastsomethingisrotatingandhowfarit
hasbeenpushedtoadesiredlocation. However,thesetasksoftenquicklybecomeconvolutedanddifficulttoreuse.
Therefore,SwarmRLprovidestheMultitaskingclasswhichtakesalistofsingletasksasanargumentandapplies
themtotheengine. Thisapproachismoremodularandallowsforfastertestingofstrategies.
3.7 Observables
AcriticalcomponentofanyRLalgorithmandanimportantpartofclassicalagentcontrolapproachesistheirstate
description. Ratherthanpassingallstateinformation(e.g.,allpositionsanddirectionsoftheagents)tothedecision
makingmodel,theobservableusuallyselectsfromorcondensestheinformationconsiderably. Thismimicsthelimited
sensing capabilities that agents might have. For example, agents might only be able to retrieve information about
otherparticleswithinafinitesensingradiusormightnotbeabletotellthedirectionoftheirneighbours. Investigating
theamountofinformationneededforanagenttostillbeabletoperformitstaskisveryrelevantforthedesignof
cost-efficientmicroagents. InSwarmRL,thedescriptionoftheenvironmentassensedbytheagentsiscomputedby
Observables.AllobservablesimplementedinSwarmRLinheritfromasimpleparentclassoutlinedinCodeSample11.
1 class Observable:
2 def compute_observable(self, colloids):
3 raise NotImplementedError("Implemented in child class.")
CodeSample11: SwarmRLObservableparentclass.
Theclassrequiresimplementingonemethod,compute_observable,whichtakesalistofparticleobjectsasinput
andreturnsthestatedescriptionvectorforeachofthem. Thisstatevectoristhenpassedontothemodelsbytheforce
function. AswiththeTaskclass,weoftenwanttoconstructagentscapableofsensingtheworldinmultipleways. To
achievethis,wealsoprovideaMultiSensingclasswhichtakesalistofobservablesandcomputesasingleoutput.
3.8 IntrinsicReward
In RL agents can be rewarded intrinsically to either encourage the exploration behavior of an agent or to assist in
buildingitsknowledgeabouttheenvironment[Pathaketal.,2017]. Asthenamesuggests,suchamethodisintrinsicto
theagentandisthereforeimplementedasaseparatesub-moduleIntrinsicRewardwhichtheagentcanbeequipped
with.
RandomNetworkDistillation
Randomnetworkdistillation(RND)wasintroducedin2018byBurdaetal.[2018]asanefficientmethodforexploring
thestatespaceofreinforcementlearningagents. Ithassincebeenstudiedmorebroadlyforitsapplicationanddata
selectionforMLapplications[Finkbeineretal.,2023]anditsimplicationsonunderstandingdatarequirementsfor
neuralnetworks[Toveyetal.,2023].
SwarmRLprovidesRNDReward,amoduleimplementingRND,defininganintrinsicrewardforagents,encouragingthem
toexploreunseenregionsoftheirstatespace. GoingbeyondtheoriginalversionfromBurdaetal.[2018],SwarmRL
providestwoimplementationsofRND,differingintheircapabilitytomemorizepreviousstates. Forbothversions
ofRND,adefaultsetupisprovidedinswarmrl/intrinsic_reward/rnd_configs.py. Utilizingsuchconfigurationthe
implementationofRNDisshowninCodeSample12.
40 rnd_config = RNDConfig(...)
41
42 ac_agent = srl.agents.ActorCriticAgent(
43 ...,
44 intrinsic_reward=RNDReward(rnd_config)
45 )
CodeSample12: RandomNetworkDistillationasIntrinsicReward
TheimplementationoftheRNDbackendisbasedontheZnNLpythonpackage[NikolaouandTovey,2021],offeringa
flexiblebutsimpleinterfaceforcomplextrainingalgorithmsforneuralnetworks.
10SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
3.9 ExplorationPolicy
Inadditiontothesamplingstrategywhichcanselectactionsfromitsdistributionsthatdonothavethelargestprobability,
SwarmRLalsoprovidesdirectaccesstoexplorationpoliciestofurtherguidetheagentsintonew,unexploredregionsof
theirenvironment.
RandomExploration
The simplest form of exploration policy is the RandomExploration class, which an exploration probability can
parameterize,ζ anddecayrate,ϵ. Theexplorationratedictateshowlikelyitisforanagenttotakeanexplorationaction
overapolicyaction,andthedecayrateslowlydecreasesthisprobabilityasthetrainingevolvesby
ζ′ =exp−ϵ Tt ζ, (9)
wheretissimulationtime,andT isthetimeforasingletrainingepisode.
3.10 Trainer
ThetrainerismoduleinSwarmRLresponsibleforhandlingtheupdatesofthemodelsandtheirinteractionwiththe
environmentwhiledevelopingapolicy. Inthesimplestcase,thetrainerdeploysthemodelsintheenvironment,collects
therewardsoveranepisode,andperformsanetworkupdate,asoutlinedinCodeSample13. However,inmanycases
more is demanded during RL training and even how rewards are collected can change depending on the problem.
Therefore,inSwarmRL,wehaveimplementedseveralvariationsoftheTrainer.
ContinuousvsEpisodic
Ausermustfirstchoosewhethertoperformepisodicorcontinuoustraining. Inepisodictraining,agentsaredeployed
intheenvironmentforafixednumberofstepsoruntilsomeconditionismet. Afterthistime,arewardiscomputed
based on their final state or trajectory during the episode. This reward is then used to update the networks before
the environment is reset and the process is started again. In this approach, the episodes should be long enough to
allowtheagentstoachievetheirtasksbutshortenoughtomaximizeefficiency. Asimilarbutalternativeapproachis
semi-episodictraining,whereintheagentsareupdatedcontinuouslyduringthetrainingbutaftersometime,oragain,
onceaconditionismet,theenvironmentisresetandthetrainingcontinues. Theseoptionsareavailablethroughthe
EpisodicTrainerpackagedintoSwarmRL.TheEpisodicTrainer,displayedinCodeSamples13and14,allows
userstodefinehowoftentheenvironmentshouldberesetasafunctionofRL-episodes,ortopassataskwhichcan
forceanenvironmentreset.
1 rl_trainer = srl.trainers.ContinuousTrainer(
2 [protocol_1, protocol_2],
3 loss,
4 )
5 rl_trainer.perform_rl_training(
6 system_runner=srl.Engine(...),
7 n_episodes=5000,
8 episode_length=50,
9 )
CodeSample13: ContinuousTraining
10 rl_trainer = srl.trainers.EpisodicTrainer(
11 [protocol_1, protocol_2],
12 loss,
13 )
14 rl_trainer.perform_rl_training(
15 get_engine=engine_fn,
16 n_episodes=10,
17 reset_frequency=30, # > 1 is semi-episodic
18 episode_length=100,
19 )
CodeSample14: EpisodicTraining
Continuoustrainingisaparadigminwhichagentsaredeployedintoanenvironmentandareupdatedafterafixedamount
oftimewithoutresettingtheenvironment. Thisonlinetrainingisusefulforsituationswheretrainingmustoccurinthe
11SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
Figure3: ExamplerenderingsusingSwarmRLandtheZnVisvisualizationengine. (left)Asnapshotfromareservoir
computing experiment using a reinforcement learning controlled swarm. (center) Colloids are trained to perform
chemotaxisinthesamefashionasbacteria.Hereanopen-sourcebacteria3dmodelfilehasbeenusedinsideofSwarmRL
toconveyrealism. ThemeshwascreatedbySketchfabartistandrewfrueh,whoseexplicitpermissionwasrequestedand
grantedbeforebeingusedinthispublication. Themodelislicensedundercreativecommons4.0. (right)Demonstration
ofcolloidsrotatingarod,controlledusinganRLalgorithm.
realworldorwherethetimeitwilltaketoachieveataskisnotwellknown. SwarmRLofferstheContinuousTrainer
classtoperformthiskindoftraining. ItshouldbenotedthatifaTaskisprovidedtothecontinuoustrainercapable
ofresettingtheenvironment,thetrainingwillend. Thiscanalsobehelpfulincaseswhereanendtotrainingcanbe
strictlyidentified.
4 Performance
Reinforcement learning typically does not require expensive neural networks, and classical control algorithms are
limitedincomplexitybythenumberofparticlestheycontrol. However,speedbecomescriticalwhenconsideringthe
numberofiterationstheyhavetogothroughduringtraininganddeployment. Therefore,SwarmRLhasbeenbuiltwitha
performance-firstmentality,leveragingtheJAXecosystem[Heeketal.,2023]heavily. JAXisaPythonlibrarythat,
amongotherthings,re-implementstheNumPylibrary[Harrisetal.,2020]withmulti-threading,GPUdeployment,
andXLA-enhancedJITcompilecapabilities. MostvariablesinSwarmRLarewrittenusingJAXarraysortheirPyTree
counterparts. PyTreesextendthefunctionalityJAXhasforNumPyobjectstodatastructureslikeclasses. Therefore,
mostoftheclassobjectsincludingtheColloidclasscanbeplacedontoaGPUorparallelizedover. Furthermore,
largepartsofcodeareJITcompiledtoensureoptimalperformance.
5 SoftwarePractices
In order to ensure the longevity and continued usability of the SwarmRL software, we adhere to several software
practices. SwarmRListhoroughlytestedwithunit,integration,andfunctionaltestsinplace. Overall,wehave87%
coverageonthecodewithallclasseshavingatleastoneunittest. ThecodefollowstheNumPydoc-stringformatandis
extensivelydocumentedwhichishostedathttps://swarmrl.github.io/SwarmRL.ai/. Beforenewcodeisadded
totheproject,alltestsmustpassandnewtestsshouldbeincluded. TheprojectishostedonGitHubwhereallthese
standardsareenforcedautomatically. Furthermore,werequireatleastonereviewfromacode-owner,thatis,amember
ofthecoreSwarmRLteam,toacceptnewchangestothesoftware.
6 Visualization
Visualizationisanessentialpartofthescientificprocess,particularlyinsimulationswhereoneissearchingforemergent
strategy. It is not always clear from plots alone if the agents have learned what they should do. For this reason,
SwarmRLinterfacescloselywiththeZnVis[Tovey,2021]visualizationlibrary. ZnVis,bydesign,handlestheoutput
datafromSwarmRLandisdevelopedalongsidetheproject. Thepackagecanusecustommeshfilesfortheagents,
exportrenderedvideos, andcapturestill-framerenderingsusingtheMitsubaengine[Jakobetal.,2022]. Figure3
illustratesthevisualizationcapabilitiesofthepackageusedonSwarmRLexperiments.
12SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
7 Conclusion
We have introduced SwarmRL, a Python package aimed at accelerating research into control and understanding of
microscopicactiveparticles,whetherbiologicalorartificial. SwarmRLallowsresearcherstoimplementstate-of-theart
controlalgorithmsdrivenbyreinforcementlearningorclassicalpolicies,inbothsimulatedandexperimentsettings.
AsaPythonpackage,SwarmRLhasafamiliarinterfaceformanyscientists,andissimpleenoughtousewithlimited
backgroundinsoftware. Beyonditsfunctionality,wehavebuiltSwarmRLtobeperformant,capableofrunningon
manycores, GPUs, andscalabletoHPCclusters. OurcurrentfocusisextendingSwarmRLtohandlemorecontrol
algorithms,e.g.,MPC,Q-learning,additionalexperimentimplementations,andadditionalengines. Overall,ourgoalis
thatSwarmRLallowscientiststospendmoretimecreatingnewideas,buildingnewtechnology,andchangingthefield,
ratherthanimplementingandstayinguptodatewithstateof-of-theartmachinelearningmethods.
8 Acknowledgements
C.HandS.TacknowledgefinancialsupportfromtheGermanFundingAgency(DeutscheForschungsgemeinschaft
DFG) under Germany’s Excellence Strategy EXC 2075-390740016. C.H and S.T acknowledge financial support
fromtheGermanFundingAgency(DeutscheForschungsgemeinschaftDFG)underthePriorityProgramSPP2363,
“UtilizationandDevelopmentofMachineLearningforMolecularApplications-MolecularMachineLearning”Project
No. 497249646. C.HandC.LacknowledgefundingbytheDeutscheForschungsgemeinschaft(DFG,GermanResearch
Foundation)underProjectNumber327154368-SFB1313. TheauthorsacknowledgesupportbythestateofBaden-
WürttembergthroughbwHPCandtheGermanResearchFoundation(DFG)throughgrantINST35/1597-1FUGG.
TheauthorsacknowledgesupportfromtheDeutscheForschungsgemeinschaft(DFG,GermanResearchFoundation)
ComputeClustergrantno. 492175459. S.TwouldliketoacknowledgeAnnalenaDanielsforherdetailedreviewand
commentsonthemanuscript.
References
T.Bäuerle,R.C.Löffler,andC.Bechinger. Formationofstableandresponsivecollectivestatesinsuspensionsofactive
colloids. NatureCommunications,11(1):2547,May2020. ISSN2041-1723. doi: 10.1038/s41467-020-16161-4.
URLhttps://doi.org/10.1038/s41467-020-16161-4.
H.C.BergandD.A.Brown. Chemotaxisinescherichiacolianalysedbythree-dimensionaltracking. Nature,239
(5374):500–504,1972.
D.Blackiston,E.Lederer,S.Kriegman,S.Garnier,J.Bongard,andM.Levin. Acellularplatformforthedevelopment
ofsyntheticlivingmachines. ScienceRobotics, 6(52):eabf1571, 2021. doi: 10.1126/scirobotics.abf1571. URL
https://www.science.org/doi/abs/10.1126/scirobotics.abf1571.
F.Borra,L.Biferale,M.Cencini,andA.Celani. Reinforcementlearningforpursuitandevasionofmicroswimmers
at low reynolds number. Phys. Rev. Fluids, 7:023103, Feb 2022. doi: 10.1103/PhysRevFluids.7.023103. URL
https://link.aps.org/doi/10.1103/PhysRevFluids.7.023103.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas,
S.Wanderman-Milne,andQ.Zhang. JAX:composabletransformationsofPython+NumPyprograms,2018. URL
http://github.com/google/jax.
Y.Burda,H.Edwards,A.Storkey,andO.Klimov. Explorationbyrandomnetworkdistillation,2018.
R.W.Carlsen,M.R.Edwards,J.Zhuang,C.Pacoret,andM.Sitti.Magneticsteeringcontrolofmulti-cellularbio-hybrid
microswimmers. LabonaChip,14(19):3850–3859,2014.
S.R.Dabbagh,M.R.Sarabi,M.T.Birtek,S.Seyfi,M.Sitti,andS.Tasoglu. 3d-printedmicrorobotsfromdesignto
translation. NatureCommunications,13(1):5875,Oct2022. ISSN2041-1723. doi: 10.1038/s41467-022-33409-3.
URLhttps://doi.org/10.1038/s41467-022-33409-3.
N.C.Darnton,L.Turner,S.Rojevsky,andH.C.Berg. Ontorqueandtumblinginswimmingescherichiacoli. Journal
ofbacteriology,189(5):1756–1764,2007.
J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds, R. Hafner, A. Abdolmaleki,
D. de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature,
602(7897):414–419,2022.
K. Dhatt-Gauthier, D. Livitz, Y. Wu, and K. J. M. Bishop. Accelerating the design of self-guided microrobots
in time-varying magnetic fields. JACS Au, 3(3):611–627, 2023. doi: 10.1021/jacsau.2c00499. URL https:
//doi.org/10.1021/jacsau.2c00499.
13SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
A.Fawzi,M.Balog,A.Huang,T.Hubert,B.Romera-Paredes,M.Barekatain,A.Novikov,F.J.R.Ruiz,J.Schrittwieser,
G. Swirszcz, D. Silver, D. Hassabis, and P. Kohli. Discovering faster matrix multiplication algorithms with
reinforcementlearning. Nature,610(7930):47–53,2022. doi: 10.1038/s41586-022-05172-4. URLhttps://doi.
org/10.1038/s41586-022-05172-4.
O.Felfoul,M.Mohammadi,S.Taherkhani,D.deLanauze,Y.ZhongXu,D.Loghin,S.Essa,S.Jancik,D.Houle,
M. Lafleur, L. Gaboury, M. Tabrizian, N. Kaou, M. Atkin, T. Vuong, G. Batist, N. Beauchemin, D. Radzioch,
and S. Martel. Magneto-aerotactic bacteria deliver drug-containing nanoliposomes to tumour hypoxic regions.
Nature Nanotechnology, 11(11):941–947, Nov 2016. ISSN 1748-3395. doi: 10.1038/nnano.2016.137. URL
https://doi.org/10.1038/nnano.2016.137.
J.Finkbeiner,S.Tovey,andC.Holm. Generatingminimaltrainingsetsformachinelearnedpotentials,2023.
E.J.Gumbel. Statisticaltheoryofextremevaluesandsomepracticalapplications.lecturesbyemitj.gumbel.national
bureauofstandards,washington,1954.51pp.diagrams.40cents. TheAeronauticalJournal,58(527):792–793,1954.
doi: 10.1017/S0368393100099958.
E.A.Hansen,D.S.Bernstein,andS.Zilberstein. Dynamicprogrammingforpartiallyobservablestochasticgames. In
AAAI,volume4,pages709–715,2004.
C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor,
S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del Río,
M. Wiebe, P. Peterson, P. Gérard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and
T.E.Oliphant. Arrayprogrammingwithnumpy. Nature,585(7825):357–362,Sep2020. ISSN1476-4687. doi:
10.1038/s41586-020-2649-2. URLhttps://doi.org/10.1038/s41586-020-2649-2.
B.Hartl,M.Hübl,G.Kahl,andA.Zöttl. Microswimmerslearningchemotaxiswithgeneticalgorithms. Proceedings
oftheNationalAcademyofSciences,118(19):e2019683118,2021. doi: 10.1073/pnas.2019683118. URLhttps:
//www.pnas.org/doi/abs/10.1073/pnas.2019683118.
J.Heek,A.Levskaya,A.Oliver,M.Ritter,B.Rondepierre,A.Steiner,andM.vanZee. Flax: Aneuralnetworklibrary
andecosystemforJAX,2023. URLhttp://github.com/google/flax.
Z. Hosseinidoust, B. Mostaghaci, O. Yasa, B.-W. Park, A. V. Singh, and M. Sitti. Bioengineered and biohybrid
bacteria-basedsystemsfordrugdelivery. AdvancedDrugDeliveryReviews,106:27–44,2016. ISSN0169-409X.
doi: https://doi.org/10.1016/j.addr.2016.09.007. URLhttps://www.sciencedirect.com/science/article/
pii/S0169409X16302629. Biologically-inspireddrugdeliverysystems.
J.R.Howse,R.A.Jones,A.J.Ryan,T.Gough,R.Vafabakhsh,andR.Golestanian. Self-motilecolloidalparticles:
fromdirectedpropulsiontorandomwalk. Physicalreviewletters,99(4):048102,2007.
A. Hsu, A. Wong-Foy, B. McCoy, C. Cowan, J. Marlow, B. Chavez, T. Kobayashi, D. Shockey, and R. Pelrine.
Applicationofmicro-robotsforbuildingcarbonfibertrusses. In2016InternationalConferenceonManipulation,
AutomationandRoboticsatSmallScales(MARSS),pages1–6,2016. doi: 10.1109/MARSS.2016.7561729.
J.HuandM.P.Wellman. Nashq-learningforgeneral-sumstochasticgames. Journalofmachinelearningresearch,4
(Nov):1039–1069,2003.
J.Ibarz,J.Tan,C.Finn,M.Kalakrishnan,P.Pastor,andS.Levine. Howtotrainyourrobotwithdeepreinforcement
learning: lessonswehavelearned. TheInternationalJournalofRoboticsResearch,40(4–5):698–721,Jan.2021.
ISSN1741-3176. doi: 10.1177/0278364920987859. URLhttp://dx.doi.org/10.1177/0278364920987859.
W.Jakob,S.Speierer,N.Roussel,andD.Vicini. DR.JIT. ACMTransactionsonGraphics,41(4):1–19,jul2022. doi:
10.1145/3528223.3530099. URLhttps://doi.org/10.1145%2F3528223.3530099.
K.F.Jarrell,S.-V.Albers,andJ.N.d.S.Machado. Acomprehensivehistoryofmotilityandarchaellationinarchaea.
FEMSmicrobes,2:xtab002,2021.
H.-R.Jiang,N.Yoshinaga,andM.Sano. Activemotionofajanusparticlebyself-thermophoresisinadefocusedlaser
beam. Physicalreviewletters,105(26):268302,2010.
I.S.M.Khalil,V.Magdanz,S.Sanchez,O.G.Schmidt,andS.Misra. Biocompatible,accurate,andfullyautonomous:
asperm-drivenmicro-bio-robot. JournalofMicro-BioRobotics, 9(3):79–86, Aug2014. ISSN2194-6426. doi:
10.1007/s12213-014-0077-9. URLhttps://doi.org/10.1007/s12213-014-0077-9.
H.Kim,J.Ali,U.K.Cheang,J.Jeong,J.S.Kim,andM.J.Kim. Micromanipulationusingmagneticmicrorobots.
JournalofBionicEngineering,13(4):515–524,2016. ISSN1672-6529. doi: https://doi.org/10.1016/S1672-6529(16)
60324-4. URLhttps://www.sciencedirect.com/science/article/pii/S1672652916603244.
B.R.Kiran,I.Sobh,V.Talpaert,P.Mannion,A.A.A.Sallab,S.Yogamani,andP.Pérez. Deepreinforcementlearning
forautonomousdriving: Asurvey,2021.
14SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
S.Kriegman,D.Blackiston,M.Levin,andJ.Bongard. Ascalablepipelinefordesigningreconfigurableorganisms.
ProceedingsoftheNationalAcademyofSciences,117(4):1853–1859,2020. doi: 10.1073/pnas.1910837117. URL
https://www.pnas.org/doi/abs/10.1073/pnas.1910837117.
F.A.Lavergne,H.Wendehenne,T.Bäuerle,andC.Bechinger. Groupformationandcohesionofactiveparticleswith
visualperception-dependentmotility. Science,364(6435):70–74,Apr.2019.
M.Li, A.Pal, A.Aghakhani, A.Pena-Francesch, andM.Sitti. Softactuatorsforreal-worldapplications. Nature
ReviewsMaterials,7(3):235–249,Mar2022. ISSN2058-8437. doi: 10.1038/s41578-021-00389-7. URLhttps:
//doi.org/10.1038/s41578-021-00389-7.
M.L.Littman. Markovgamesasaframeworkformulti-agentreinforcementlearning. InMachinelearningproceedings
1994,pages157–163.Elsevier,1994.
T.Lymburn,S.D.Algar,M.Small,andT.Jüngling. Reservoircomputingwithswarms. Chaos: AnInterdisciplinary
JournalofNonlinearScience,31(3):033121,032021. ISSN1054-1500. doi: 10.1063/5.0039745. URLhttps:
//doi.org/10.1063/5.0039745.
S.Macenski,T.Foote,B.Gerkey,C.Lalancette,andW.Woodall. Robotoperatingsystem2: Design,architecture,
andusesinthewild. ScienceRobotics,7(66):eabm6074,2022. doi: 10.1126/scirobotics.abm6074. URLhttps:
//www.science.org/doi/abs/10.1126/scirobotics.abm6074.
P.Mandal,G.Patil,H.Kakoty,andA.Ghosh. Magneticactivematterbasedonhelicalpropulsion. Accountsofchemical
research,51(11):2689–2698,2018.
M.Medina-Sánchez,L.Schwarz,A.K.Meyer,F.Hebenstreit,andO.G.Schmidt. Cellularcargodelivery: Toward
assistedfertilizationbysperm-carryingmicromotors. NanoLetters,16(1):555–561,2016. doi: 10.1021/acs.nanolett.
5b04221. URLhttps://doi.org/10.1021/acs.nanolett.5b04221. PMID:26699202.
V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,
G.Ostrovski,S.Petersen,C.Beattie,A.Sadik,I.Antonoglou,H.King,D.Kumaran,D.Wierstra,S.Legg,and
D.Hassabis. Human-levelcontrolthroughdeepreinforcementlearning. Nature,518(7540):529–533,2015. doi:
10.1038/nature14236. URLhttps://doi.org/10.1038/nature14236.
S. Muiños-Landin, A. Fischer, V. Holubec, and F. Cichos. Reinforcement learning with artificial microswimmers.
ScienceRobotics,6(52):eabd9285,2021. doi: 10.1126/scirobotics.abd9285. URLhttps://www.science.org/
doi/abs/10.1126/scirobotics.abd9285.
R. R. Murphy, S. Tadokoro, D. Nardi, A. Jacoff, P. Fiorini, H. Choset, and A. M. Erkmen. Search and Rescue
Robotics,pages1151–1173. SpringerBerlinHeidelberg,Berlin,Heidelberg,2008. ISBN978-3-540-30301-5. doi:
10.1007/978-3-540-30301-5_51. URLhttps://doi.org/10.1007/978-3-540-30301-5_51.
O. Nachum, M. Norouzi, K. Xu, and D. Schuurmans. Bridging the gap between value and policy based
reinforcement learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Cur-
ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/
facf9f743b083008a894eee7baa16469-Paper.pdf.
B.J.NelsonandS.Pané. Deliveringdrugswithmicrorobots. Science,382(6675):1120–1122,2023. doi: 10.1126/
science.adh3073. URLhttps://www.science.org/doi/abs/10.1126/science.adh3073.
B. J. Nelson, I. K. Kaliakatsos, and J. J. Abbott. Microrobots for minimally invasive medicine. Annual Review
of Biomedical Engineering, 12(1):55–85, 2010. doi: 10.1146/annurev-bioeng-010510-103409. URL https:
//doi.org/10.1146/annurev-bioeng-010510-103409. PMID:20415589.
K.NikolaouandS.Tovey. ZnNL,July2021. URLhttps://github.com/zincware/ZnNL.
F.A.Oliehoek,C.Amato,etal. AconciseintroductiontodecentralizedPOMDPs,volume1. Springer,2016.
D.Pathak,P.Agrawal,A.A.Efros,andT.Darrell. Curiosity-drivenexplorationbyself-supervisedprediction. In2017
IEEEConferenceonComputerVisionandPatternRecognitionWorkshops(CVPRW),pages488–489,2017. doi:
10.1109/CVPRW.2017.70.
K.Qin,Z.Zou,L.Zhu,andO.S.Pak. Reinforcementlearningofamulti-linkswimmeratlowReynoldsnumbers.
PhysicsofFluids,35(3),032023. ISSN1070-6631. doi: 10.1063/5.0140662. URLhttps://doi.org/10.1063/
5.0140662. 032003.
P.Romanczuk,M.Bär,W.Ebeling,B.Lindner,andL.Schimansky-Geier. Activebrownianparticles. TheEuropean
PhysicalJournalSpecialTopics,202(1):1–162,Mar2012. ISSN1951-6401. doi: 10.1140/epjst/e2012-01529-y.
URLhttps://doi.org/10.1140/epjst/e2012-01529-y.
15SwarmRL:BuildingtheFutureofSmartActiveSystems PRIMEAIpaper
C. K. Schmidt, M. Medina-Sánchez, R. J. Edmondson, and O. G. Schmidt. Engineering microrobots for targeted
cancertherapiesfromamedicalperspective. NatureCommunications,11(1):5618,Nov2020. ISSN2041-1723. doi:
10.1038/s41467-020-19322-7. URLhttps://doi.org/10.1038/s41467-020-19322-7.
J.Schulman,S.Levine,P.Moritz,M.I.Jordan,andP.Abbeel. Trustregionpolicyoptimization,2017a.
J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov. Proximalpolicyoptimizationalgorithms,2017b.
J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.Abbeel. High-dimensionalcontinuouscontrolusinggeneralized
advantageestimation,2018.
H. Shen, S. Cai, Z. Wang, Z. Ge, and W. Yang. Magnetically driven microrobots: Recent progress and future
development. Materials&Design,227:111735,2023. ISSN0264-1275. doi: https://doi.org/10.1016/j.matdes.2023.
111735. URLhttps://www.sciencedirect.com/science/article/pii/S0264127523001508.
C.D.SilflowandP.A.Lefebvre. Assemblyandmotilityofeukaryoticciliaandflagella.lessonsfromchlamydomonas
reinhardtii. Plantphysiology,127(4):1500–1507,2001.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap,
M.Leach,K.Kavukcuoglu,T.Graepel,andD.Hassabis. Masteringthegameofgowithdeepneuralnetworksand
treesearch. Nature,529(7587):484–489,2016. doi: 10.1038/nature16961. URLhttps://doi.org/10.1038/
nature16961.
M.Stefanec,D.N.Hofstadler,T.Krajník,A.E.Turgut,H.Alemdar,B.Lennox,E.S¸ahin,F.Arvin,andT.Schmickl. A
minimallyinvasiveapproachtowards“ecosystemhacking”withhoneybees. FrontiersinRoboticsandAI,9,2022.
ISSN2296-9144. doi:10.3389/frobt.2022.791921. URLhttps://www.frontiersin.org/articles/10.3389/
frobt.2022.791921.
H.Su,C.-A.HurdPrice,L.Jing,Q.Tian,J.Liu,andK.Qian. Janusparticles: design,preparation,andbiomedical
applications. MaterialsTodayBio,4:100033,2019. ISSN2590-0064. doi: https://doi.org/10.1016/j.mtbio.2019.
100033. URLhttps://www.sciencedirect.com/science/article/pii/S2590006419300596.
R.S.SuttonandA.G.Barto. Reinforcementlearning: Anintroduction. MITpress,2018.
R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with
functionapproximation. InS.Solla,T.Leen,andK.Müller,editors,AdvancesinNeuralInformationProcessing
Systems,volume12.MITPress,1999. URLhttps://proceedings.neurips.cc/paper_files/paper/1999/
file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf.
M. Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth
internationalconferenceonmachinelearning,pages330–337,1993.
S.Tovey. ZnVis: AVisualisationPackageforParticleSimulations,2021.
S.Tovey,S.Krippendorf,K.Nikolaou,andC.Holm. Towardsaphenomenologicalunderstandingofneuralnetworks:
data. MachineLearning: ScienceandTechnology,4(3):035040,sep2023. doi: 10.1088/2632-2153/acf099. URL
https://dx.doi.org/10.1088/2632-2153/acf099.
N.WadhwaandH.C.Berg. Bacterialmotility: machineryandmechanisms. Naturereviewsmicrobiology, 20(3):
161–173,2022.
X.Wang,X.-Z.Chen,C.C.J.Alcântara,S.Sevim,M.Hoop,A.Terzopoulou,C.deMarco,C.Hu,A.J.deMello,
P.Falcaro,S.Furukawa,B.J.Nelson,J.Puigmartí-Luis,andS.Pané. Mofbots: Metal–organic-framework-based
biomedicalmicrorobots. AdvancedMaterials,31(27):1901592,2019. doi: https://doi.org/10.1002/adma.201901592.
URLhttps://onlinelibrary.wiley.com/doi/abs/10.1002/adma.201901592.
F. Weik, R. Weeber, K. Szuttor, K. Breitsprecher, J. de Graaf, M. Kuron, J. Landsgesell, H. Menke, D. Sean, and
C.Holm. Espresso4.0–anextensiblesoftwarepackageforsimulatingsoftmattersystems. TheEuropeanPhysical
JournalSpecialTopics,227:1789–1816,2019.
Y.Zeng,Z.-Y.Zhou,E.Rinaldi,C.Gneiting,andF.Nori. Approximateautonomousquantumerrorcorrectionwith
reinforcement learning. Phys. Rev. Lett., 131:050601, Jul 2023. doi: 10.1103/PhysRevLett.131.050601. URL
https://link.aps.org/doi/10.1103/PhysRevLett.131.050601.
K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar. Fully decentralized multi-agent reinforcement learning with
networkedagents,2018.
K.Zhang,Z.Yang,andT.Bas¸ar. Multi-agentreinforcementlearning: Aselectiveoverviewoftheoriesandalgorithms,
2021.
J.Zhuang,R.WrightCarlsen,andM.Sitti. ph-taxisofbiohybridmicrosystems. ScientificReports,5(1):11403,Jun
2015. ISSN2045-2322. doi: 10.1038/srep11403. URLhttps://doi.org/10.1038/srep11403.
16