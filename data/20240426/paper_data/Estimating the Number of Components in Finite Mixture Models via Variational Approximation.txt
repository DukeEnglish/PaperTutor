Estimating the Number of Components in Finite Mixture
Models via Variational Approximation
Chenyang
Wang∗1,
Yun
Yang†1
1Department of Statistics, University of Illinois Urbana-Champaign
Abstract
This work introduces a new method for selecting the number of components in
finite mixture models (FMMs) using variational Bayes, inspired by the large-sample
properties of the Evidence Lower Bound (ELBO) derived from mean-field (MF) vari-
ational approximation. Specifically, we establish matching upper and lower bounds
for the ELBO without assuming conjugate priors, suggesting the consistency of model
selection for FMMs based on maximizing the ELBO. As a by-product of our proof, we
demonstrate that the MF approximation inherits the stable behavior (benefited from
model singularity) of the posterior distribution, which tends to eliminate the extra
components under model misspecification where the number of mixture components
is over-specified. This stable behavior also leads to the n−1/2 convergence rate for
parameter estimation, up to a logarithmic factor, under this model overspecification.
Empirical experiments are conducted to validate our theoretical findings and compare
with other state-of-the-art methods for selecting the number of components in FMMs.
Keywords: Evidencelowerbound; Mean-fieldapproximation; Mixturemodels; Modelselection;
Singular models.
1 Introduction
One of the fundamental problems in statistics and machine learning is model selection,
which attempts to select the most parsimonious model to fit the observed data from a list
of candidate models. To address this problem, various model selection criteria have been
proposed in the literature. Among them, two well-known criteria are the Akaike Information
Criterion (AIC), proposed by Akaike (1973), and the Bayesian Information Criterion (BIC),
introduced by Schwarz (1978). Under suitable conditions, AIC is known to lead to optimal
prediction performance but tends to overestimate the model size; in comparison, BIC, which is
derived as an asymptotic approximation to the log-marginal likelihood (a.k.a. model evidence)
∗cw80@illinois.edu
†yy84@illinois.edu
1
4202
rpA
52
]EM.tats[
1v64761.4042:viXraunder a Bayesian framework, is able to consistently identify the true model (the model with
the smallest size that contains the true data-generating process) as the sample size grows to
infinity under the frequentist perspective, a property known as model selection consistency.
The validity of BIC in approximating the model evidence requires the considered candidate
model to be non-singular, meaning that its Fisher information matrix needs to be invertible
at the true model parameter so that the maximum likelihood estimator (MLE) enjoys the
asymptotic normality property. However, in many practical applications, especially when the
considered model is over-specified, the Fisher information matrix is non-invertible, a situation
referred to as singular models. Here, an over-specified model means that this model contains
the true model, but the number of parameters in the model exceeds that of the true model;
an accompanying notion is an under-specified model, where the model does not contain the
true model.
In this paper, we consider the problem of model selection for a common and representative
type of singular models, the finite mixture model (FMM), whose distribution family consists
of all finite mixtures of probability distributions from a shared parametric family, with a
fixed number of mixture components. FMMs have been extensively studied in the statistical
literature (Titterington et al., 1985; McLachlan et al., 2019; Fru¨hwirth-Schnatter, 2006;
Mengersen et al., 2011; Schlattmann, 2009) and are widely applied in various fields, including
agriculture, astronomy, biology, economics, engineering, and psychology, among others. In an
FMM, the model size is determined by the number of components, which is typically unknown
in practice. Thus, the model selection problem essentially becomes the task of identifying
the minimal number of components required in the FMM to adequately fit the data. Over-
specified FMMs are singular due to the non-identifiability of model parameters; in other words,
different parameters can correspond to the same probability distribution. Over-specifying
the number of components in an FMM also has the drawback of deteriorating the statistical
efficiency of parameter estimation. For example, in a two-component Gaussian mixture model
that is over-specified with the true model being a single Gaussian, the convergence rate of the
MLE for estimating the location parameter decreases from the parametric rate n−1/2 to n−1/4,
as shown by Chen (1995); more generally, this rate of convergence further slows down with
the inclusion of additional components (Ho and Nguyen, 2016a; Heinrich and Kahn, 2018).
Extensions of BIC-type model selection criteria to singular models date back to a series
of seminal works (Watanabe, 2001; Yamazaki and Watanabe, 2003; Yamazaki and Watanbe,
2012) by Watanabe. In these works, tools from algebraic geometry, particularly those based on
the resolution of singularities, are employed to determine the leading terms in the asymptotic
expansion of the model evidence. The leading terms now depend on the real log canonical
threshold (RLCT) and its multiplicity of the model; see Section 2.3 for a brief review or
the recent book (Watanabe, 2018) for further information. However, unlike the usual BIC
that only depends on the considered model, the RLCT also requires knowledge of the true
parameter value and therefore cannot be used in practice for model selection. Recently,
2Drton and Plummer (2017) proposed a singular Bayesian information criterion (sBIC) for
model selection in the context of singular models. This method involves solving a system of
equations, the unique solution of which is shown to provide valid asymptotic approximations
to the model evidences of candidate models. However, this approach requires knowledge of
the real log canonical threshold (RLCT) and its multiplicity for all candidate models, the
determination of which poses a highly challenging problem even in field of algebraic geometry.
For example, for multi-variate Gaussian mixture models, their complete characterizations
remain unresolved.
In this paper, we propose and analyze a new model selection approach for singular
models based on variational Bayes (VB). VB has become a popular alternative to traditional
sampling-based algorithms like MCMC for approximate Bayesian computation, owing to its
computational scalability and good generalization performance in many applications. Our
proposed approach draws inspiration from the recent work of Zhang and Yang (2024), which
shows that for non-singular models, the evidence lower bound (ELBO) derived from the
mean-field (MF) approximation in variational Bayes can recover the leading terms in model
evidence. Notably, the model selected by the ELBO tends to converge asymptotically with
the one chosen by the BIC as the sample size increases. Furthermore, the calculation of the
ELBO is a straightforward by-product of the Coordinate Ascent Variational Inference (CAVI)
algorithm (see, e.g., Chapter 10 of Bishop, 2006) for implementing the MF approximation.
In particular, this approach eliminates the need for a case-by-case analysis to determine the
effective number of parameters required in computing the BIC for non-singular models. In
this work, we also consider the use of the ELBO from MF approximation for model selection
in FMMs. Theoretically, we find that while the ELBO can no longer recover the leading
term involving the RLCT in the model evidence, it still contains a term that penalizes
over-specified models, leading to consistent model selection.
1.1 Related literature
In this subsection, we briefly review more recent works closely related to ours and defer a more
extensive literature review and discussions to Appendix A. For Bayesian finite mixture models,
Rousseau and Mengersen (2011) studied the asymptotic behavior of posterior distributions in
over-specified Bayesian FMMs, and proved that under certain conditions on the prior, the
posterior tends to empty the extra components. Nguyen (2013) and Guha et al. (2021) derived
the contraction rate of posterior distributions on the parameters of interest in Bayesian FMMs
under model misspecification; more reviews of literature on mixture model estimation can
also be found therein. For variational Bayes (VB), recent works (Alquier and Ridgway, 2020;
Pati et al., 2018; Yang et al., 2020; Zhang and Gao, 2020; Wang and Blei, 2019; Zhang and
Yang, 2024) have provided general conditions under which VB leads to estimation consistency
of the model parameter or data distribution, and derived the corresponding convergence
3rate. In particular, the theoretical framework developed in Yang et al. (2020) and Zhang
and Yang (2024) covers Bayesian latent variable models, including FMMs. Moreover, the
theory in Pati et al. (2018) implies an n−1/2 convergence rate, up to a logarithmic factor, for
over-specified FMMs under the Hellinger metric, a result that will also play a key role in our
proof. In the context of applying VB to Bayesian FMMs, Watanabe and Watanabe (2006)
derived upper and lower bounds for the ELBO (also called stochastic complexity therein)
from the MF approximation for Bayesian Gaussian mixture models; also see Section 2.4 for
further details. However, the gap between these two bounds does not ensure model selection
consistency based on the ELBO. Watanabe and Watanabe (2007) extended this analysis to
general FMMs, but the issue of the gap remains unresolved. More recently, Bhattacharya
et al. (2020) has shown that the ELBO from MF approximation recovers the leading term
involving RLCT for a singular model in its normal-crossing form. However, while Hironaka’s
theorem in algebraic geometry guarantees the existence of a reparametrization that can
locally transform the posterior distribution of any singular model into its normal-crossing
form, explicitly finding such a transformation is typically challenging.
1.2 Our contributions
In this study, we investigate the large sample properties of the ELBO derived from the
MF variational approximation for FMMs under model misspecification. Specifically, we
establish matching upper and lower bounds for the ELBO for a general class of FMMs whose
constituting mixture components are exponential families. Our refined result will imply the
consistency of model selection based on the ELBO. Moreover, unlike earlier analysis relying
on conjugate priors to simplify the analysis, we only require the smoothness of the priors and
employ Laplace approximation to analyze the conditional posterior distribution of parameters
given latent class indicator variables. In our proof, we first establish a lower bound for the
ELBO by constructing specific feasible solutions to the VB optimization problem, similar to
those used by Watanabe and Watanabe (2007). Our main technical contribution comes from
our upper bound analysis. We utilize the variational risk bound under the Hellinger metric
given by Pati et al. (2018) to identify certain important behaviors of the mixing weights,
yielding a substantially improved upper bound that matches the lower bound up to high-order
terms. Interestingly, our findings indicate that, unlike singular models in normal-crossing
forms (Bhattacharya et al., 2020), directly applying MF to Bayesian FMMs does not recover
the leading term in the model evidence, although it still results in consistent model selection.
In addition, as a by-product of our analysis, we prove a corollary demonstrating that the MF
variational approximation to the posterior distribution in Bayesian FMMs inherits the same
interesting and stable behavior of tending to empty the extra components under model over-
specification (Rousseau and Mengersen, 2011). This stable behavior also leads to the n−1/2
convergence rate for parameter estimation, up to a logarithmic factor, for those statistically
4identifiable components that do not empty out (see Section 3.3). It is also worth mentioning
that our results do not require strong identifiability, as assumed by Rousseau and Mengersen
(2011) and Manole and Khalili (2021), and are applicable to weakly identifiable FMMs (Ho
and Nguyen, 2016a). Through experiments conducted on a location Gaussian mixture model
and a real dataset, we empirically validate our theoretical findings and demonstrate that our
model selection method is computationally efficient and tends to have higher power than
existing state-of-the-art methods — it can correctly select the true number of components in
FMMs using fewer samples.
The rest of the paper is organized as follows. Section 2 outlines the notation, provides
background knowledge, formulates the problems we aim to address in this work and introduce
our model selection method in finite mixture models. Section 3 lists our main assumptions
and presents our main theoretical results about model selection via ELBO maximization
and parameter estimation via mean-field variational approximation. Section 4 showcases
simulated experiments of the Gaussian mixture model under various settings to complement
our theoretical findings and reports results from a real data analysis. Finally, Section 5
concludes the paper with a discussion. Proofs and additional literature reviews are deferred
to the appendices in the supplementary material.
2 Background and New Method
In this section, we begin with an overview of the notation, followed by a review of necessary
background, and then introduce the problem formulation.
2.1 Notation
Throughout the paper, random variables are denoted by capital letters, while their realizations
are represented by corresponding lowercase letters. For two probability measures P and Q,
let p and q be their Radon-Nikodym derivatives with respect to a σ-finite measure µ, usually
chosen as the Lebesgue measure or the counting measure. We denote the total variation and
(cid:82)
the (squared) Hellinger distance between p and q respectively as d (p,q) = 1 |p−q|dµ and
(cid:82) √ √ TV 2
h2(p,q) = ( p− q)2dµ. Moreover, their Kullback-Leibler (KL) divergence is denoted as
(cid:82)
D (p∥q) = plog(p/q)dµ. We use N(µ,Σ) to denote a (multivariate) normal distribution
KL
with mean µ ∈ Rd and variance-covariance matrix Σ ∈ Rd×d. Additionally, for a vector
a ∈ Rd, we use |a| to denote its Euclidean ℓ -norm. Throughout this paper, we use C to
2
denote a generic positive constant whose value may vary from line to line. We also write
a ≳ b to mean a ≥ Cb, a ≲ b to mean a ≤ Cb and a ≍ b to mean a ≳ b and a ≲ b.
52.2 Singular models
In statistical learning, many classical techniques and conclusions are applicable primar-
ily to regular models, such as the Laplace approximation, and the convergence rate and
asymptotic normality of the maximum likelihood estimations (MLE). A statistical model
(cid:8) (cid:12) (cid:9)
M = p(·|θ)(cid:12)θ ∈ Θ with parameter space Θ is called regular (or non-singular) if its pa-
rameter θ is identifiable and p(·|θ) has a well-defined and positive definite Fisher information
matrix I(θ) = E [−∇2logp(X|θ)] at each parameter θ ∈ Θ. In contrast, we say a model
θ
is singular when at least one of the conditions for regularity is not met. Many common
statistical models are singular, such as finite mixture models, hidden Markov models, factor
models, and neural networks. Mixture models, characterized by their ability to represent
complex data distributions through convex combinations of simpler distributions from a
shared parametric family, are a representative type of singular model. In this article, we
focus exclusively on finite mixture models.
(cid:8) (cid:12) (cid:9)
Concretely, let us start from a simple parametric family G = g(·; η)(cid:12)η ∈ Ω pa-
rameterized by parameter η ∈ Ω, whose parameter space Ω is a subset of Rd, such as a
location-scale family. For an integer K, a mixture model with K components in G is model
M = (cid:8) p(·|θ)(cid:12) (cid:12)θ = (w ,η )K , η ∈ Ω, w ≥ 0,k ∈ [K] and (cid:80)K w = 1(cid:9) with
K k k k=1 k k k=1 k
K
(cid:88)
p(x|θ) = w g(x; η ), θ = (w ,η )K ∈ RdK+K, (1)
k k k k k=1
k=1
where w = (w ,w ,...,w ) ∈ [0,1]K is called the mixing weight (vector) parameter, and,
1 2 K
by slightly abusing the notation, we use η = (η ,η ,...,η ) ∈ ΩK to denote the parameters
1 2 K
associated with the K mixture components. We also denote the corresponding (discrete)
mixing distribution as G(θ) =
(cid:80)K
w δ , where δ denotes the Dirac mass at η . Generally,
k=1 k η k η k k
mixture models may lose identifiability, since different parameters may lead to the same
data distribution p(·|θ). However, G(θ) is always identifiable, that is, there is a one-to-one
correspondence between G(θ) and p(·|θ). This serves as one of the key assumptions in
our theory (see Section 3.1). For example, the following two (Gaussian) mixture models
correspond to the same standard normal distribution: p(x|θ ) = 1·N(0,1)+0·N(1,1) and
1
p(x|θ ) = 1 ·N(0,1)+ 1 ·N(0,1). Fortunately, they still have the same mixing distribution
2 2 2
G(θ ) = G(θ ) = δ .
1 2 0
The loss of identifiability may lead to a non-invertible Fisher information matrix, resulting
in decreased statistical efficiency — even when an identifiable submodel is considered, the
best point estimator may suffer from a slow convergence rate as the sample size n increases.
For example, Chen (1995) demonstrated such a phenomenon when the mixing distribution is
restricted to the form G(θ) = 2δ + 1δ for h ∈ R, and g satisfies certain regular conditions
3 −h 3 2h
to make the reduced parameter h identifiable. However, even for this identifiable submodel,
6the MLE for h still converges at the slow rate of n−1/4 when the true parameter h∗ = 0.
Therefore, it is crucial to correctly specify the number K of mixture components before
conducting statistical inference for finite mixture models, which motivates the problem of
model selection for K in this paper.
2.3 Model selection criteria for singular models
The Bayesian Information Criterion (BIC) is widely used for model selection for regular
parametric models. Let Xn = (X ,...,X ) be n i.i.d. observations from the true model p∗(x),
1 n
which may or may not belong to the considered model M. Under this setup, the BIC for a
model M is derived as an asymptotic approximation to the so-called model evidence, or the
log-marginal likelihood in a Bayesian framework, defined as
(cid:90)
d
F(M) = log π(θ|M)p(Xn|θ,M) dθ = logp(Xn|θ(cid:98) ,M)− logn + O (1), (2)
M P
2
ΘM
(cid:124) (cid:123)(cid:122) (cid:125)
BICformodelM
where Θ ∈ Rd denotes its parameter space, π(θ|M) the prior density, p(Xn|θ,M) the
M
likelihood function, and θ(cid:98) = argmax p(Xn|θ,M) is the MLE of θ under model
M θ∈ΘM
M. The BIC in the preceding display consists of two terms: a model goodness-of-fit term
and an additional term of −d logn resulting from the integration over Rd, which penalizes
2
larger models that may lead to overfitting. It is a classical result (Schwarz, 1978) that
selecting a model based on maximizing BIC enjoys the model selection consistency property.
Unfortunately, for singular models with non-invertible Fisher information matrices, the
asymptotic expansion (2) for the model evidence no longer holds, as the effective dimension
of the parameter space is no longer equal to the number d of parameters.
In a seminal work, Watanabe (2001) employed algebraic geometry tools to show that
for a singular model M, the asymptotic behavior of the log-marginal likelihood can be
characterized through
F(M) = logp∗(Xn)−λ logn+(m −1)loglogn+O (1), (3)
M M P
where λ is called the real log canonical threshold (RLCT) of model M that determines the
M
effective number of parameters, and m is its multiplicity. Leveraging this general result,
M
Drton and Plummer (2017) proposed a model selection method by maximizing a singular
Bayesian information criterion (sBIC), which is computed by solving a system of equations
whose unique solution approximates the leading terms in F(M). However, this approach
requires knowledge of the real log canonical threshold (RLCT) and its multiplicity for all
candidate models, the determination of which is a highly nontrivial problem even in the field
of algebraic geometry within pure mathematics.
72.4 Selecting number of components in mixture models via varia-
tional approximation
To handle the analytically intractable integral in F(M), variational inference attempts to
approximate F(M) defined in (2) from below by utilizing Jensen’s inequality (to −log(t)),
(cid:90) π(θ|M)p(Xn|θ,M) (cid:90) π(θ|M)p(Xn|θ,M)
log q (θ)dθ ≥ q (θ)log dθ := L(q ),
θ θ θ
q (θ) q (θ)
ΘM θ ΘM θ
where q (θ) is a distribution from a tractable distribution family Γ for approximating the
θ
posterior distribution p(θ|Xn,M). We will focus on the commonly used mean-field family
Γ in this work, where q factorizes over the (blocks of) components of θ. The evidence
MF θ
lower bound (ELBO) is then defined as the best approximation to F(M) by maximizing the
lower bound L(q ) over q ∈ Γ . In the rest of the paper, when we consider a fixed model
θ θ MF
M, we may suppress M in our notation for simplicity.
Now let us specialize the above general framework to finite mixture models (1). As a
commonpractice, wewillaugmentthemixturemodelintoahierarchicalmodelviaintroducing
latent (class indicator) variables to facilitate efficient computation. In fact, a key property
utilized in our theoretical analysis is that the conditional distribution of observation given
latent variable is a regular parametric model. Let Sn = (S ,...,S ) denoting the collection of
1 n
all latent variables, with each S being discrete and taking values from {1,..,K}, indicating
i
which mixture component that X comes from. Then we have the hierarchical form of the
i
iid
model as [X |S = k] ∼ g(x; η ) and S are i.i.d. with P(S = k) = w , for i ∈ [n] and
i i k i i k
k ∈ [K]. Moreover, we can express the joint distribution of Xn given θ as
n
(cid:88) (cid:89) (cid:88)
p(Xn|θ) = p(Xn|sn,θ)p(sn|θ) = p(X |s ,η)p(s |w).
i i i
sn∈[K]n i=1si∈[K]
Let Zn = (θ,Sn) denote the collection of all hidden variables, then the augmented poste-
rior distribution p(Zn|Xn) satisfies p(θ,sn|Xn) ∝ p(Xn,sn|θ)π(θ). Now the variational
inference will use q ∈ Γ to approximate the joint posterior distribution p(Zn|Xn) (by
Zn
replacing θ with Zn therein). To reduce the approximation error, we consider the block MF
(cid:8) (cid:9)
approximation where Γ = q = q ⊗q . Let q = q ⊗q := argmax L(q )
MF Zn θ Sn (cid:98)Zn (cid:98)θ (cid:98)Sn q Zn∈ΓMF Zn
denote the best variational density that approximates p(Zn|Xn), and the corresponding
L(q ) is the ELBO for the mixture model with K components. Then the best variational
(cid:98)Zn
approximation to its posterior p(Zn|Xn) is defined as the minimizer of the following KL
divergence,
(cid:0) (cid:13) (cid:1)
q
(cid:98)Zn
:= arginfD
KL
q Zn(·)(cid:13)p(·|Xn) . (4)
q Zn∈Γ
8Note that for any q , we have the following relationship
Zn
(cid:90)
(cid:0) (cid:13) (cid:1)
D
KL
q Zn(·)(cid:13)p(·|Xn) = log p(Xn|θ)π(θ)dθ−L(q Zn) ≥ 0,
where the first term is the evidence and the second term
(cid:90) p(Xn,sn|θ)π(θ)
L(q ) = q (zn)log dzn. (5)
Zn Zn
q (zn)
Zn
is the corresponding ELBO function, since the KL divergence is non-negative. Therefore,
maximizing the ELBO over q is equivalent to minimizing the KL divergence. In addition,
Zn
the ELBO valued at its maximizer q (zn) is a potential surrogate to the model evidence in
(cid:98)Zn
singular model selection, as long as this approximation retains some characteristics of the
evidence. As aforementioned, a commonly used variational family Γ is the (block) mean-field
(MF) approximation, where q (zn) is factorized as
Zn
q (zn) = q (θ)⊗q (sn). (6)
Zn θ Sn
We will focus on the MF, and use q to denote the maximizer of the ELBO function L(·),
(cid:98)Zn
which, according to the MF definition, can be further decomposed into q = q ⊗q . We
(cid:98)Zn (cid:98)θ (cid:98)Sn
will refer to q
(cid:98)Zn
as the variational posterior (distribution). If we use L(cid:98)
K
= L K(q (cid:98)Zn) to denote
the (maximal) ELBO value under the FMM with K components, then our selected number
of components K(cid:98) via ELBO maximization can be defined via
K(cid:98) = argmaxL(cid:98) . (7)
K
K
Regarding the block MF approximation, the following theorem gives a necessary condition
for q to be the maximizer of the ELBO, as stated in the Theorem 2.1 by Beal (2003). This
(cid:98)Zn
theorem provides a system of distributional equations determining q (θ) and q (sn), which
(cid:98)θ (cid:98)Sn
plays an important role and will be repeatedly used in our proofs.
Theorem 1 Under the block MF approximation, the variational posterior q = q ⊗q
(cid:98)Zn (cid:98)θ (cid:98)Sn
satisfies
(cid:26) (cid:27)
1 (cid:88)
q (θ) = π(θ)exp q (sn)logp(Xn,sn|θ) (8)
(cid:98)θ (cid:98)Sn
C(cid:98)
r sn∈[K]n
(cid:26)(cid:90) (cid:27)
1
and q (sn) = exp q (θ)logp(Xn,sn|θ)dθ , (9)
(cid:98)Sn (cid:98)θ
C(cid:98)
Q
where C(cid:98) and C(cid:98) are the normalization constants.
r Q
Undertheabovesettings, WatanabeandWatanabe(2007)derivedupperandlowerbounds
9for L(q ) under a symmetric Dirichlet prior Diri(ϕ ,...,ϕ ), ϕ ≥ 0, on the mixing weights
(cid:98)Zn 0 0 0
w and a conjugate prior for the parameters η associated with mixture components from an
exponential family. Specifically, if we denote the variational posterior mean (estimator) of θ
(cid:82)
as θ := θq (θ)dθ, then Watanabe and Watanabe (2007) proved that (after some notation
(cid:98)θ
adaptions)
p(Xn|θ)
logp∗(Xn)−λlogn+O (1) ≤ L(q ) ≤ logp∗(Xn)−λlogn+log +O (1)
P (cid:98)Zn p∗(Xn) P
as sample size n tends to infinity, and the leading coefficients λ and λ are given by
 
(K −K∗)ϕ + dK∗+K∗−1, ϕ ≤ d+1, (K −1)ϕ + d, ϕ ≤ d+1,
λ = 0 2 0 2 and λ = 0 2 0 2
dK+K−1, ϕ > d+1; dK+K−1, ϕ > d+1,
2 0 2 2 0 2
where K∗ denotes the true number of components underlying the data-generating process.
When ϕ < (d + 1)/2, there exists a gap between λ and λ. It should be noted that
0
(d+1)/2 also serves as the threshold for ϕ , below which the desirable stable behavior of
0
the posterior distribution and its variational approximation (see Corollary 5) to empty the
extra components can be maintained. This stable behavior is also shown to lead to improved
parameter estimation efficiency (see Theorem 7). In contrast, when ϕ > (d + 1)/2 and
0
K is over-specified, then two or more components can be very close with non-negligible
weights each, leading to unstable and less accurate parameter estimation (for example, see
our numerical results in Section 4.1). In practice, a choice of ϕ less than one is also typically
0
preferred in the literature. For example, a default choice of ϕ in topic models (Blei et al.,
0
2003) is K−1, which encourages a sparse mixing weight w for easier interpretation. Conversely,
a larger ϕ tends to promote a denser mixing weight; in particular, Diri(ϕ ,...,ϕ ) tends to
0 0 0
concentrate around the uniform weights (K−1,...,K−1) as ϕ approaches infinity. Therefore,
0
it is an important theoretical question whether this gap can be eliminated for small ϕ , so
0
that model selection based on maximizing the ELBO is provably consistent. In this work, we
close this gap by proving matching lower and upper bounds for the ELBO under any positive
values of ϕ . The discussion following our Theorem 2, Section 3.3, and our numerical results
0
(e.g., Section 4.1) both suggest that a small ϕ value is indeed preferred for accurate model
0
selection and parameter estimation.
3 Theoretical Results
In this section, we present our main theoretical results. Here, we assume that the dataset
Xn = (X ,...,X ) consists of n observations that are i.i.d. samples from a fixed true data-
1 n
generating process p∗(x) = (cid:80)K∗ w∗g(x;η∗), where K∗ denotes the true component number,
k=1 k k
10and θ∗ = (w∗, η∗)K∗ ∈ RdK∗+K∗ denotes the (unique) true parameters under the mixture
k k k=1
model (1) with K∗ components, where all η ’s are distinct and min w∗ > 0. Let G∗
k k∈[K∗] k
denote the corresponding mixing measure for the truth. Throughout this section, we consider
a finite mixture model with K ≥ K∗ components. In addition, we follow the literature by
(cid:8) (cid:12) (cid:9)
assuming the parametric family G = g(·; η)(cid:12)η ∈ Ω for the mixture components to be an
(cid:8) (cid:9)
exponentialfamilyinthecanonicalform, thatis, g(x;η) = exp ηTT(x)−T (x)−A(η) , where
0
η ∈ Rd is the natural parameter, T(x) is the sufficient statistic and A(η) is the log-partition
function. The Fisher information matrix for this exponential family is I(η) = ∇2A(η), which
implies the convexity of A(η) over its domain Rd. Consequently, the log-likelihood function
logg(x;η) is concave with respect to η for all x. One important assumption of our theory
(see Section 3.1 below) is the relationship between total variation and Wasserstein-type
distances in FMMs, which transforms the closeness of mixture models in density functions
into closeness in their mixing measure or model parameters. Such a relationship can be
verified for most commonly used FMMs; see Section 3.4 for concrete examples. An r-th
order Wasserstein distance (or simply r-Wasserstein distance) between two mixing measures
G(θ) = (cid:80)K w δ and G(θ′) = (cid:80)K′ w′ δ is defined as:
k=1 k η k k=1 k η k′
(cid:0) (cid:1) (cid:16) (cid:88) (cid:17)1/r
W G(θ),G(θ′) = inf q |η −η′|r , (10)
r ij i j
qij
i,j
(cid:8) (cid:9)
where the infimum is taken over all nonnegative probability masses q : i ∈ [K], j ∈ [K′]
ij
satisfying two marginal constraints (cid:80)K q = w′ for j ∈ [K′] and (cid:80)K′ q = w for i ∈ [K].
i=1 ij j j=1 ij i
3.1 Assumptions
Recall that we have the symmetric Dirichlet distribution Diri(ϕ ,...,ϕ ), ϕ > 0 as our prior
0 0 0
for the mixing weights w, whose density takes the form of π(w) = Γ(Kϕ0) (cid:81)K wϕ0−1, for
Γ(ϕ0)K k=1 k
all w = (w ,...,w ) ∈ [0,1]K such that (cid:80)K w = 1, where Γ(·) is the Gamma function.
1 K k=1 k
We also consider independent priors on η, that is, π(η) =
(cid:81)K
π(η ), η ∈ Ω, where for
k=1 k k
each k, the density π(η ) is twice continuously differentiable on Ω. In addition, we make the
k
following assumptions.
Assumption A: Recall that p∗ is the true data-generating probability with mixing measure
G∗ and has K∗ components.
1. (Parameter space compactness) The parameter space Ω of mixture components is a
compact subset of Rd, i.e., there exists a constant L < ∞ such that sup |η−η′| ≤ L.
η,η′∈Ω
2. (Regularity of mixture component family) There exist two positive constants a and b
such that yTI(η)y ∈ [a,b] over all |y| = 1 and all η ∈ Ω, where I(η) = ∇2A(η) is the
Fisher information matrix for the mixture components.
113. (Identifiability of mixing measure) The total variation distance between any FMM p(x|θ)
(cid:0) (cid:1)
with K(≥ K∗) components and the true model p∗ satisfies d p(·|θ),p∗(·) = 0 if and
TV
only if its mixing measure G(θ) satisfies G(θ) = G∗. Moreover, there exist an positive in-
(cid:0) (cid:1) (cid:0) (cid:1)
tegerr andpositiveconstantsεandc suchthatd p(·|θ),p∗(·) ≥ c Wr G(θ),G∗ for
0 TV 0 r
(cid:0) (cid:1)
anyp(·|θ)withmixingmeasureG(θ)(withK components)satisfying W G(θ),G∗ < ε.
r
Assumption A1 is commonly made in the literature for technical simplicity when analyzing
singular models. Assumption A2 is a mild condition with Ω being compact and is satisfied
by all non-singular (i.e., not curved) exponential families. Assumption A3 requires certain
partial identifiability for the true model p∗ with K∗ components. It essentially states that for
any mixing measure G(θ) sufficiently close to G∗, the total variation of the density functions
is lower bounded by some Wasserstein-type distance between the mixing measures. For
example, if the family g(x;η),η ∈ Ω is identifiable in the second-order (see Definition 3.2 in
(Ho and Nguyen, 2016b)), in other words, g(x;η ) and their derivatives with respect to η up
k k
to the second order are linearly independent, then Assumption A3 is satisfied with r = 2.
For families that are not identifiable in the second-order, for instance, the location-scale
Gaussian distributions, verification of Assumption A3 can be done in a case-by-case manner,
where r may also depend on K∗ and K. See Section 3.4 for more examples that satisfy these
assumptions.
It is also worth noting that, with Assumptions A1 and A3, the (localized) inequality
in the second part of Assumption A3 can be made global. Specifically, there exists a
constant c such that d (p(·|θ),p∗(·)) ≥ c Wr(G(θ),G∗) for all possible θ. The proof
1 TV 1 r
is straightforward. Let A := {θ : W (G(θ),G∗) ≥ ε,η ∈ ΩK}. Since W (G(θ),G∗) is
r r
continuous in θ, A is a compact set and d (p(·|θ),p∗(·)) > 0 on A by the first part of
TV
Assumption A3. For the considered exponential family, the total variation is also a continuous
function with respect to θ. Therefore, it attains a strictly positive infimum on A, which
we denote as τ := inf d (p(·|θ),p∗(·)) > 0. Meanwhile, from Assumption A1, we have
θ∈A TV
Wr(G(θ),G∗) ≤ Lr. Therefore, on A, we also have d (p(·|θ),p∗(·)) ≥ (τ/Lr)Wr(G(θ),G∗).
r TV r
Letting c = min{c ,τ/Lr}, it then follows from above and the second part of Assumption
1 0
A3 that for any θ, we have:
(cid:0) (cid:1) (cid:0) (cid:1)
d p(·|θ),p∗(·) ≥ c Wr G(θ),G∗ , (11)
TV 1 r
which provides a global inequality relating the two distance metrics over any compact set.
The above argument allows us to avoid assuming a reverse inequality d (p(·|θ),p∗(·)) ≤
TV
c Wα(G(θ),G∗) for some α > 0 and for any p(·|θ) with mixing measure G(θ) in order to
0 r
extend the local inequality into a global one, as was done in Corollary 3.1 of Ho and Nguyen
(2016b).
123.2 Large sample properties of ELBO
Now we are ready to present our first main result as follows, which provides two-sided bounds
(cid:82)
to the ELBO L(q ) with matching leading terms. Recall that we use θ := θq (θ)dθ to
(cid:98)Zn (cid:98)θ
denote the variational point estimator for θ.
Theorem 2 (Finite-sample two-sided bound of ELBO) Suppose Assumptions A1–A3
hold and K ≥ K∗. Then there exist some positive constants C , C and c independent of n
1 2
such that with probability at least 1−n−c, we have
p(Xn|θ)
logp∗(Xn)−λlogn+C ≤ L(q ) ≤ logp∗(Xn)−λlogn+log +C ,
1 (cid:98)Zn p∗(Xn) 2
where C = KlogΓ(ϕ )−Γ(Kϕ )−C and C = KlogΓ(ϕ )−Γ(Kϕ )+C for some constant
1 0 0 2 0 0
C independent of (n,ϕ ), and λ is given by
0

(K −K∗)ϕ + dK∗+K∗−1, ϕ ≤ d+1,
λ = 0 2 0 2 (12)
dK+K−1, ϕ > d+1.
2 0 2
We note that KlogΓ(ϕ )−Γ(Kϕ ) is an increasing function with respect to both K and
0 0
ϕ . In particular, a large ϕ may cause the constants C and C in the ELBO to rapidly
0 0 1 2
grow with the model size K; as a result, a larger sample size is needed to correctly select the
true model by ELBO maximization. See our numerical results in Section 4 for an empirical
comparison, where at a sample size of n = 50, using ELBO with ϕ = 1 selects the correct
0
model, while using ELBO with ϕ = 6(d = 6) leads to the selection of an over-specified
0
model. Therefore, our theoretical result suggests that in practice a small ϕ value is preferred
0
for accurate model selection with limited data. In Section 3.3 below, we will also demonstrate
the theoretical benefits of using a small ϕ < (d+1)/2 for parameter estimation.
0
There is still a gap ∆ = logp(Xn|θ) − logp∗(Xn) in Theorem 2, which is related to
n
the likelihood ratio test statistic LRT = logp(Xn|θ(cid:98))−logp∗(Xn) through ∆ ≤ LRT, with
n
θ(cid:98)= argmax logp(Xn|θ) denoting the MLE of θ. By utilizing the singular learning theory
θ∈Θ
(Chapter 6.1, Watanabe, 2009), we show in Appendix C.3 that this LRT weakly converges to
a limiting random variable defined through the supreme of a Gaussian process as n → ∞, by
using the invariance property of MLE and a resolution of singularity technique developed in
Watanabe (2009) which transform the log-likelihood function of the singular model into its
canonical normal crossing form. As a result, we obtain the following corollary, ensuring the
gap in our ELBO upper and lower bounds to be at most O (1).
P
Corollary 3 (Asymptotic expansion of ELBO) UnderAssumptionsA1–A3, theasymp-
13totic expansion of ELBO for K ≥ K∗ has the following form:
L(q ) = logp∗(Xn)−λlogn+KlogΓ(ϕ )−Γ(Kϕ )+O (1), as n → ∞.
(cid:98)Zn 0 0 P
Asadirectconsequence,weobtainthefollowingcorollaryaboutmodelselectionconsistency
based on maximizing ELBO. Here, recall that K∗ denotes the (true) number of components
in the data-generating process, and K(cid:98) = argmax L(cid:98) denotes the estimated number of
K K
components via ELBO maximization, with L(cid:98) being the ELBO value under the FMM with
K
K components.
Corollary 4 (Model selection consistency via ELBO maximization) SupposeAssump-
tions A1–A3 hold, then the estimated number of components K(cid:98) in (7) via ELBO maximization
satisfies
P(cid:0)
K(cid:98) =
K∗(cid:12) (cid:12)Xn(cid:1) →p
1, as n → ∞.
This means that model selection via ELBO maximization in FMMs is (asymptotically) consis-
tent.
3.3 Parameter estimation in finite mixture models via variational
approximation
From the Theorem 2, we can also see that the large sample behavior of ELBO exhibits two
different patterns, depending on the value of the prior hyperparameter ϕ for the mixing
0
weight w. For large ϕ that exceeds (d+1)/2, the effective number of parameters in the
0
variational approximation q is (dK +K −1), which matches the degrees of freedom of
(cid:98)Zn
parameter θ = (w,η) (we lose one degree of freedom due to the constraint
(cid:80)K
w = 1).
k=1 k
Therefore, this case can be interpreted as the non-singular regime since ELBO and the regular
BIC asymptotically match each other. The more interesting case occurs when ϕ < (d+1)/2,
0
which corresponds to the singular regime, as the effective number of parameters λ is strictly
smaller than the degrees of freedom (dK +K −1) in the parameterization θ. In fact, the
following corollary as a by-product of our proof suggests that under this singular regime, the
variational approximation q to the posterior distribution on the mixing weight will have an
(cid:98)w
interesting stable behavior of emptying out the (K −K∗) redundant components when K is
overspecified.
Corollary 5 (Stability of mixing weights) Suppose Assumptions A1–A3 holds and there
exists some constant C such that ∆ ≤ C loglogn with probability 1 − o(1). Then it
3 n 3
holds with probability at least 1 − n−c − o(1) that: 1. when ϕ < (d + 1)/2 and for ρ =
0 1
142(C +1)/(d+1−2ϕ ),
3 0
(cid:88)K (K −K∗)[(logn)ρ1 +ϕ ]
0
inf w < ,
σ(k)
σ∈SK n
k=K∗+1
where S is the set of all permutations over [K]; 2. when ϕ > (d + 1)/2 and for ρ =
K 0 2
2(C +1)/(2ϕ −d−1),
3 0
1 ϕ
0
inf w ≥ + .
k∈[K] k (logn)ρ2 n
Our condition ∆ ≤ C loglogn requires a finite sample analysis of the likelihood ratio test
n 3
statistic under singularity, and can be verified in many concrete examples (Watanabe and
Watanabe, 2006, 2007; Rotnitzky et al., 2000; Drton, 2009; Liu and Shao, 2003; Mitchell
et al., 2019); also see Appendix A for a brief review. This corollary indicates that for small ϕ ,
0
the redundant mixture components are emptied out at the rate of n−1 (up to logn factors),
a rate faster than the usual parametric rate of n−1/2. This phenomenon is again a distinctive
feature of the singular regime, where some components of the posterior distribution may
exhibit a faster rate of convergence, depending on the behavior of the prior distribution near
the boundary of the parameter space (Bochkina and Green, 2014). In comparison, under the
regular regime where ϕ > (d+1)/2, the mixing weight point estimates tend to spread out
0
over all K components and are not stable, and the “super-efficiency” property disappears;
this finding is also supported by our numerical results in Section 4. A similar result regarding
this robust behavior for the posterior distribution p(w|Xn) of mixing weights w is also
proved in Rousseau and Mengersen (2011); however, their obtained convergence rate under
small ϕ is n−1/2 up to logarithm terms and requires a stronger identifiability condition on
0
the model (i.e., identifiability in the second-order). Therefore, Corollary 5 indicates that
variational approximation also inherits the stability and (possibly) super-efficiency property
of the posterior distribution in finite mixture models.
When ϕ < (d+1)/2, due to the rapid emptying out speed of redundant components,
0
only K∗ statistically identifiable components survive. It is then natural to ask the question
of whether the convergence rate of the parameters associated with the true components (i.e.,
those K∗ components whose weights do not empty out) can reach the parametric rate n−1/2
(possibly modulo logarithmic terms). The answer is affirmative. Before presenting the result,
we need to first introduce a precise notion of identifiability on the parameter estimation.
Definition 6 (Weak identifiability) The family {g(x;η),η ∈ Ω} is said to be weakly iden-
tifiable, or identifiable in the first order, if g(x;η) is differentiable in η and for any finite K
different {η }K , for some α ∈ R and β ∈ Rd, the following equation holding for almost
k k=1 k k
15every x,
K
(cid:88)(cid:104) (cid:105)
α g(x;η )+βT∇ g(x;η ) = 0,
k k k η k
k=1
implies α = 0 and β = 0 for all 1 ≤ k ≤ K.
k k
This condition of weak identifiability, or identifiability in the first order, is the least
stringent form of identifiability condition for parameter estimation in FMMs in the literature
(e.g., see Ho and Nguyen (2016a)), and is satisfied by most of the commonly used distributions.
The term “weak” is used in contrast to the so-called strong identifiability condition (e.g.,
see Ho and Nguyen (2016b); Jordan et al. (1999); Heinrich and Kahn (2018); Manole and
Khalili (2021)), which is also called identifiability in higher orders (with orders greater than
one). For example, a one-dimensional family {g(x;η),η ∈ Ω} is said to be identifiable in
m-th order (or m-strongly identifiable) if for any finite K different {η }K , for some α ∈ R
k k=1 k
and β(j) ∈ Rd, j ∈ [m], the following equation holding for almost all x,
k
K m
(cid:88)(cid:104) (cid:88) (cid:105)
α g(x;η )+ β(j)g(j)(x;η ) = 0,
k k k k
k=1 j=1
implies α = 0 and β(j) = 0 for all 1 ≤ k ≤ K and j = 1,...,m. Here, g(j)(x;η ) denotes
k k k
the j-th order derivative of g(x;η) with respect to η. Note that the strong identifiability
always implies the weak identifiability. A strong identifiability condition (with order two)
is commonly adopted in the literature to characterize the convergence rate of location or
scale mixture models in the literature (Ho and Nguyen, 2016b; Jordan et al., 1999; Heinrich
and Kahn, 2018; Manole and Khalili, 2021). However, for weakly identifiable models, the
convergence rate of a general FMM must be examined on a case-by-case basis (e.g., see
Ho and Nguyen (2016a)) by proving an inequality similar to our Assumption A3, which
relates the total variation distance to a Wasserstein-type distance. Moreover, the derived
convergence rate also depends on the order r in the inequality (see Assumption A3) and is
typically slower than the square root-n rate for parameters. In contrast, our result below
indicates that, thanks to the stability behavior of mixing weights in the singular regime
(Corollary 5 under ϕ < (d+1)/2), weak identifiability, coupled with Assumption A3 under
0
any order r, leads to a parametric rate for the parameters associated with those identifiable
mixture components. From Section 3.4 below, we will see that some simple FMMs, such as
the location-scale Gaussian family, are only weakly identifiable and not strongly identifiable.
Therefore, our results imply that the variational posterior estimator of θ can achieve the n−1/2
convergence rate for parameter estimation, up to logarithmic factors, for weakly identifiable
FMMs such as the location-scale Gaussian mixture model. This is a result that cannot be
inferred from the existing literature (even for the MLE).
16Theorem 7 (Convergence rate of component parameters) Supposethesameassump-
tions of Corollary 5 hold and the family {g(x;η),η ∈ Ω} is weakly identifiable. If ϕ < (d+1)/2,
0
then there exists a large constant M such that it holds with probability at least 1−n−c −o(1)
that
M logn
inf sup |η −η∗| ≤ √ ,
σ∈SK1≤k≤K∗ σ(k) k n
where S is the set of all permutations over [K].
K
Theorem 7 can be interpreted as a consequence of Corollary 5 where the mixing weights
of redundant components decrease to zero at a sufficiently fast rate. Consequently, despite
the overspecification, we can effectively treat the working models as well-specified models
with the correct number of components. In such well-specified cases, Ho and Nguyen (2016b)
demonstrated in their Theorem 3.1 that d (p(·|θ),p∗(·)) ≥ c W (G(θ),G∗) when W is
TV 0 1 1
small, provided that the family is identifiable in the first order. This leads to a nearly
parametric convergence rate for estimating the mixture component parameters, due to the
nearly parametric convergence rate under the total variation metric. This heuristic argument
illustrates how the stability behavior of mixing weights in the singular regime enhances
parameter estimation efficiency. The numerical results in Section 4.1 also demonstrate the
advantages of using a small ϕ to induce this stability behavior.
0
3.4 Applications to concrete examples
In this subsection, we verify Assumption A and the weak identifiability condition for several
representative finite mixture models, and determine their effective dimensions, λ, arising in
the ELBO from the MF approximation.
Example 1 (Location Gaussian mixture model) In this example, we consider the lo-
cation Gaussian mixture model where the density function of each component is from the
univariate location family of Gaussian distributions {N(µ,σ2);µ ∈ Ω}, where σ2 is assumed to
be known and the canonical parameter in this exponential family is simply η := µ. Moreover,
the Fisher information matrix is 1/σ2 times the identity matrix. For simplicity, if we assume
σ2 = 1 without loss of generality, then
1 (cid:26) x2 µ2(cid:27)
g(x;µ) = √ exp − +µx− .
2π 2 2
According to Theorem 3 in Chen (1995), g(x;µ) is identifiable in the second order, which also
implies the weak identifiability; in fact, it is also identifiable in any finite order according to
Theorem 2.4 in Heinrich and Kahn (2018). Therefore, Assumption A3 is satisfied with r = 2
17by Theorem 3.2 (a) from Ho and Nguyen (2016b), and our Theorem 2 applies with λ being

(K −K∗)ϕ + 2K∗−1, ϕ ≤ 1,
λ = 0 2 0 2
2K−1, ϕ > 1.
2 0 2
Example 2 (Scale exponential mixture model) In this example, we take g(x;η) =
ηe−ηx = exp{ηx + logη} as the (scale) exponential distribution family with rate param-
eter η (or scale parameter η−1), whose fisher information is 1/η2. According to Theorem
2.4 in Heinrich and Kahn (2018), g(x;µ) is also identifiable in any finite order. Therefore,
Assumption A3 is satisfied with r = 2 and the weak identifiability also holds. As a result,
Theorem 2 also applies to the scale exponential mixture model with the same λ as Example 1.
Example 3 (Location-scale Gaussian mixture model) We consider the classical form
of the univariate location-scale Gaussian distribution family where
(cid:26) x2 xµ µ2 1 (cid:27)
g(x;µ,σ2) = exp − + − − log(2πσ2) .
2σ2 σ2 2σ2 2
This family is no longer strongly identifiable due to the algebraic relationship between the two
partial derivatives (up to the second order)
∂2g ∂g
(x;µ,σ2) = 2 (x;µ,σ2).
∂µ2 ∂σ2
For this parametric family, Proposition 2.2 from Ho and Nguyen (2016a) shows that if G(θ)
has at most K components, then for any G(θ) such that W (G(θ),G∗) is sufficiently small,
r
we have
d (p(·|θ),p∗(·)) ≳ Wr(G(θ),G∗),
TV r
for some constant r ≥ 2 depending only on K −K∗. This verifies Assumption A3 with r = r.
Hence, we can apply Theorem 2 to conclude that the effective dimension λ in the ELBO is
(d = 2)

(K −K∗)ϕ + 3K∗−1, ϕ ≤ 3,
λ = 0 2 0 2
3K−1, ϕ > 3.
2 0 2
Moreover, although the location-scale Gaussian distribution family is not strongly identifiable,
it is weakly identifiable, as demonstrated in Ho and Nguyen (2016b). Consequently, all
results from Section 3.3 regarding parameter estimation via mean-field approximation remain
applicable.
18Example 4 (Multinomial mixture model) In the final example, we consider a mixture
of discrete distributions, specifically, the multinomial mixture model that is widely used in
topic modeling. In particular, we take g(x;η) = (cid:0) M (cid:1)(cid:81)d ηxj, x ∈ [M], (cid:80)d x = M.
x1,...,x
d
j=1 j j
(cid:0)
j=1 i
(cid:1)
This is the probability mass function of the multinomial distribution Mult M,(η ,...,η ) with
1 d
a known M and parameters (η ,...,η ), where
(cid:80)d
η = 1 and has an effective dimension
1 d j=1 j
of d−1. A sufficient condition for this multinomial mixture model with K components to be
strongly identifiable is 3K −1 ≤ M according to Corollary 1 in Manole and Khalili (2021).
Therefore, Assumption A3 holds with r = 2 and our Theorem 2 applies with

(K −K∗)ϕ + dK∗−1, ϕ ≤ d,
λ = 0 2 0 2
dK−1, ϕ > d,
2 0 2
where one degree of freedom is lost due to the linear constraint
(cid:80)d
η = 1. The weak
i=1 i
identifiability is again implied by the strong identifiability.
3.5 Technical highlights
In this subsection, we highlight some key technical steps and results in our proofs that could
be interesting in their own right. The main goal of introducing Assumption A3 is to obtain
the following Lemma 8, which implies that when the total variation distance between a K
component mixture model and the true model with K∗ components is very close to zero,
it is always possible to merge its K components into K∗ groups I ,...,I ⊂ [K], so that
1 K∗
the aggregated mixing weights
(cid:8)(cid:80)
w : k ∈
[K∗](cid:9)
are close to the true mixing weights
j∈I j
k
w∗ = {w∗,...,w∗ }.
1 K∗
Lemma 8 Under Assumptions A2 and A3, there exists a positive constant c so that for any
θ, we have
(cid:88)K∗
(cid:12)(cid:88) (cid:12)
d (p(·|θ),p∗(·)) ≥ c inf (cid:12) w −w∗(cid:12),
TV (cid:12) j k(cid:12)
I1,...I K∗
k=1 j∈I k
where the infimum is taken over all index sets I ,I ,...,I that are disjoint subsets of [K].
1 2 K∗
Note that {I ,I ,...,I } is not necessarily a partition of [K]; in other words, the union of
1 2 K∗
these subsets is not necessarily [K]. Lemma 8 establishes a relationship between the total
variation distance between two FMMs and their component weights. Since model selection is
concerned only with the number of components and not with the parameter estimation of
η ’s, this inequality plays a key role and is also sufficient for proving our main conclusions
k
regarding the model selection consistency. Compared to a typical theoretical analysis on
the estimation consistency, an advantage of adopting this lemma is that we no longer need
to require the mixture model to be strongly identifiable (Rousseau and Mengersen, 2011;
19Ho and Nguyen, 2016b; Manole and Khalili, 2021); instead, we only need to assume some
weaker identifiable conditions as in Assumption A3. A proof of this lemma is provided in
Appendix C.
Finally, we remark that in order to improve the ELBO upper bound analysis from
Watanabe and Watanabe (2007), one of the key steps in our proof is to utilize the following
result about the consistency of variational Bayes, which is adapted from Theorem 3.2 in Pati
et al. (2018) and is also applicable to singular models. In particular, this result, combined
with Lemma 8 above, implies the consistency of the estimation of the mixing weights w (after
properly merging similar redundant components) in the mean-field variational approximation.
Lemma 9 (Consistency of variational Bayes) Under Assumption A1, there exist con-
stants (c′, C ) such that it holds with probability at least 1−n−c′,
4
(cid:90) (cid:18) logp∗(Xn)−L(q ) logn(cid:19)
h2(cid:0)
p(x|θ),
p∗(x)(cid:1)
q (θ)dθ ≤ C
(cid:98)Zn
+ .
(cid:98)θ 4
n n
Θ
This lemma is a direct consequence of the result from Pati et al. (2018) since their testing
condition is automatically satisfied for the Hellinger distance with a compact parameter space;
(cid:0) (cid:1) (cid:0) (cid:1)
so we omit the proof. By using the inequality d p(x|θ),p∗(x) ≤ h p(x|θ),p∗(x) , we can
TV
then obtain that, under Assumption A3, at least K∗ of the mixing weights are bounded away
from 0 with high probability under the variational posterior distribution q . This property
(cid:98)w
leads to an improved upper bound of L(q ) (which matches the lower bound) compared to
(cid:98)Zn
the one obtained in Watanabe and Watanabe (2007) (reviewed at the end of Section 2.4).
4 Numerical Studies
In this section, we further demonstrate our theoretical findings about ELBO through simula-
tion experiments, and illustrate and compare the two predicted regimes, namely, the singular
regime where ϕ < (d + 1)/2 and the regular regime where ϕ > (d + 1)/2, in terms of
0 0
their model selection and parameter estimation performances. Additionally, we compare our
proposed method of selecting the number of components in finite mixture models via ELBO
maximization with other state-of-the-art selection methods and apply them to a real dataset.
4.1 Impact of ϕ on ELBO in finite mixture models
0
To study the effectiveness of using the ELBO L(q ) analyzed in Theorem 2 for selecting
(cid:98)Zn
the true number of components, and to examine how different choices of ϕ may impact the
0
selection results, we conduct a numerical experiment using a location Gaussian mixture model
under different settings. The ELBO is computed using the Coordinate Ascent Variational
Inference (CAVI) algorithm (Bishop, 2006). The simulated data Xn = (X ,...,X ) is
1 n
20(a) n = 104 (b) n = 104
(c) n = 105 (d) n = 105
Figure 1: Plots of (L(cid:98)
K
−L(cid:98) K∗)/logn versus K with fixed n.
generated from a d-dimensional Gaussian mixture model with K∗ = 2 components and d = 6.
We use a common variance-covariance matrix for the mixture components, which is the
identity matrix I and treated as known. The true mixing weights are w∗ = w∗ = 1/2 and
√ √ 1 2
the two true location parameters are η∗ = −2/ d·1 and η∗ = 2/ d·1, where 1 denotes
1 2
the d-dimensional all one vector. We employ a symmetric Dirichlet prior Diri(ϕ ,...,ϕ )
0 0
for the mixing weight w and an independent N(0, I) prior for the location parameters
{η }K . We consider K = 5 candidate Gaussian mixture models with component number
k k=1 max
K ∈ {1,2,...,K }. For our implementations, we found that the CAVI algorithm can be
max
sensitive to the initialization of mixing weights w, especially in the regular regime when
ϕ > (d+1)/2. Therefore, in every simulation, the following initialization scheme is applied
0
to enhance the computational stability: we randomly and evenly assign all data points
Xn into K groups at the beginning to initialize the latent class indicators Sn for each
int
K ∈ {1,2,...,K}, and then iteratively update q (sn) and q (θ) until the ELBO value
int Sn θ
converges. For each K, we select the largest ELBO value obtained among different random
initializations (i.e., under different K ) as our final output of L(cid:98) , the ELBO value in the K
int K
components mixture model.
In the first setting, we fix the sample size at n ∈ {104, 105} and let K and ϕ change.
0
We choose ϕ ∈ {1,2,3.5,4.5,6}, where (d+1)/2 = 3.5 corresponds to the critical threshold
0
21(cid:2) (cid:3)
Figure 2: Plots of L(cid:98) −logp(Xn|θ) versus logn with fixed ϕ .
K 0
distinguishing the singular and regular regimes, as predicted in Theorem 2. In particular, our
theory predicted that for K > K∗, the ELBO value L(cid:98) satisfies
K
L(cid:98)
K
−L(cid:98)
K∗
= O P(1)−min{ϕ 0, 3.5}(K −K∗) logn. (13)
Consequently, whenplotting(L(cid:98) K−L(cid:98) K∗)/lognagainstK, theresultingcurvewillapproximate
astraightlinewithaslopeof−min{ϕ , 3.5}. ThenumericalresultsshowninFigure1confirm
0
thistheoreticalprediction. ThetwographsontheleftshowtheELBOvaluesoverallconsidered
K, while the two graphs on the right zoom in on the over-specified range of K ≥ K∗. As we
can see, for all cases, ELBO is maximized at the true model with K = K∗ = 2. Additionally,
under a relatively small sample size, as in Figure 1(b), the slope under ϕ = 6 has a clear
0
deviation from the theoretical value of 3.5 based on equation (13); this situation improves
under the relatively larger sample size, as in Figure 1(d). This observation can be explained
by our discussion after Theorem 2, which suggests that using a large ϕ may not be a good
0
choice for small sample sizes due to higher approximation errors (C ,C ).
1 2
To formally study the impact of ϕ on model selection, we consider a second setting where
0
we fix ϕ and vary n ∈ {10,10·5,10·52,10·53,10·54} and K ∈ {2,3,4,5}. Figure 2 reports
0
(cid:2) (cid:3)
the results, where we plot L(cid:98) −logp∗(Xn) against logn with a fixed ϕ ∈ {1,3.5,6}. As
K 0
can be seen from Figure 2, for ϕ = 1, the ELBO values L(cid:98) is already peaked at K∗ at a
0 K
sample size as small as n = 10. However, for ϕ = 6, a much larger sample size (around
0
e4.5 = 90) is required for the ELBO to peak at the true model with K∗ = 2. This empirical
result again suggests that a smaller value of ϕ is preferred for accurate model selection with
0
limited data.
We also use the setting of a fixed K and varied n to numerically verify the theoretical
dependence of the coefficients of logn in the ELBO on ϕ . According to Theorem 2, if we
0
plot L(q )−logp(Xn|θ) against logn, we expect to see a linear relationship with a slope of
(cid:98)Zn
−λ, which equals to (K −K∗)ϕ +(dK∗ +K∗ −1)/2 for ϕ < 3.5 and (dK +K −1)/2 for
0 0
ϕ > 3.5. The corresponding results are summarized in Figure 3. As we can see, the slopes of
0
22Figure 3: Plots of (L(cid:98)
K
−L(cid:98) K∗)/logn versus ϕ
0
with fixed K.
the lines for larger ϕ = {3.5,4.5,6} exceeding the critical threshold (d+1)/2 = 3.5 remains
0
essentially unchanged across different K. For ϕ < 3.5, the lines becomes less steep compared
0
to ϕ = 3.5 as K increases. Those empirical observations again align well with the prediction
from our theory.
Finally, to examine the stability behavior of the variational approximation q which tends
(cid:98)w
to empty out the redundant mixture components under over-specified K > K∗, we conduct
a last simulation study to report the variational point estimators of the mixing weights
(expectations under q ), denoted as w, under K = 5 and K∗ = 2. In Table 1, we report
(cid:98)w
the (sorted) estimated weights w ’s for each setting. We repeated every setting ten times
k
and recorded the means and standard deviations. The behavior of w is consistent with our
k
predictions from Corollary 5: for small ϕ < 3.5, the mixing weights tend to only concentrate
0
on the first two components; while for large ϕ > 3.5, the mixing weights tend to spread out
over all K = 5 mixture components. Moreover, for large ϕ ≥ 3.5, the estimated weights
tend to be fairly sensitive to the initialization so the standard deviations are higher, which
again suggests that a small ϕ should be used in order to enhance the parameter estimation
0
robustness and accuracy. In addition, in Table 2 we report the estimated Wasserstein
distances between the estimated mixing distribution G(θ) and the true mixing distribution G∗
to compare the convergence rate of component parameters. From the table, we can observe
that the estimation errors in terms of W (G(θ),G∗) (and standard deviations) under both
1
n = 1000 and n = 10000 are substantially smaller under a small ϕ < 3.5 (singular regime)
0
compared to those under a large ϕ > 3.5 (regular regime).
0
23w w w w w
1 2 3 4 5
Mean 0.505 0.484 0.004 0.004 0.004
ϕ = 1
0 Std 0.007 0.006 0.002 0.002 0.002
Mean 0.490 0.461 0.017 0.017 0.017
ϕ = 2
0 Std 0.008 0.013 0.002 0.002 0.002
Mean 0.441 0.356 0.116 0.056 0.030
n = 1000 ϕ = 3.5
0 Std 0.069 0.080 0.081 0.051 0.016
Mean 0.392 0.298 0.155 0.104 0.051
ϕ = 4.5
0 Std 0.057 0.058 0.039 0.037 0.029
Mean 0.343 0.274 0.184 0.123 0.076
ϕ = 6
0 Std 0.051 0.041 0.040 0.028 0.026
Mean 0.503 0.495 <0.001 <0.001 <0.001
ϕ = 1
0 Std 0.003 0.003 <0.001 <0.001 <0.001
Mean 0.502 0.487 0.007 0.002 0.002
ϕ = 2
0 Std 0.004 0.014 0.016 <0.001 <0.001
Mean 0.464 0.418 0.078 0.027 0.023
n = 10000 ϕ = 3.5
0 Std 0.080 0.100 0.103 0.053 0.032
Mean 0.407 0.279 0.149 0.108 0.057
ϕ = 4.5
0 Std 0.083 0.098 0.055 0.044 0.049
Mean 0.384 0.272 0.158 0.109 0.078
ϕ = 6
0 Std 0.076 0.079 0.053 0.046 0.037
Table 1: Estimated mixing weights {w }5 under n ∈ {103,104} and ϕ ∈ {1,2,3.5,4.5,6}.
k k=1 0
n = 1000 n = 10000
ϕ 1 2 3.5 4.5 6 1 2 3.5 4.5 6
0
Mean 0.151 0.222 0.341 0.414 0.422 0.048 0.057 0.098 0.157 0.169
Std 0.023 0.029 0.114 0.056 0.070 0.014 0.012 0.068 0.058 0.050
Table 2: Estimated W (G(θ),G∗) under n ∈ {103,104} and ϕ ∈ {1,2,3.5,4.5,6}.
1 0
4.2 Comparison with other model selection methods
Recently, a method for model selection of finite mixture models (FMMs) named Group-
Sort-Fuse (GSF) has been proposed by Manole and Khalili (2021), which is shown to enjoy
higher consistency and efficiency than existing approaches such as the Merge-Truncate-Merge
(MTM) algorithm provided by Guha et al. (2021). Specifically, GSF is a penalized likelihood
method to simultaneously estimate the number of components and the mixing measure G(θ).
Three commonly used penalties, the Smoothly Clipped Absolute Deviation (SCAD), the
Minimax Concave Penalty (MCP), and the Adaptive Lasso (ALasso), which meet the required
theoretical conditions, are advocated in their paper. In Manole and Khalili (2021), they
already compared their method to the MTM algorithm, which has lower accuracy and incurs
high computational costs since it is based on sampling from the real posterior distribution.
24Therefore, in this simulation study, we will only study and compare the three methods, ELBO
(under ϕ = 1 and ϕ = 5), BIC, and GSF with the three different penalties (denoted as GSF ,
S
GSF and GSF ) for selecting the number of components in a location Gaussian mixture
M A
model. For the model setting, we adopt a similar data-generating model as Manole and
Khalili (2021). Specifically, we consider the six mixture models as summarized in Table 3
(two within each group) in the Appendix B with true number of components K∗ = 2,3,5
and mixture component parameter dimensions d = 2,4,6. The component parameters η∗
k
(location parameters) in Models 1, 3, and 5 are the same as those from Manole and Khalili
(2021), but we change the mixing weights w∗ = (w∗,...,w∗) to be unevenly distributed.
1 5
Models 2, 4, and 6 are less separated (i.e., with closer location parameters {η∗}5 ), with the
k k=1
same component numbers and dimensions as in Models 1, 3, and 5, respectively. For each
model, we assume the identity covariance matrix as known. We randomly generate the data
Xn 100 times with varying sample sizes n = 200,400,600,800 for each model, then apply
each method and record the model selection accuracy (percentages of selecting the correct
model).
The findings are illustrated in Figure 4. A notable observation is that ELBO demonstrates
superiorperformancecomparedtoallothermethodsinscenarioswheretheseparationbetween
mixture components is relatively small (Models 2, 4, and 6). Particularly in Model 6, ELBO
achieves an accuracy exceeding 0.875 with a sample size of 800, while alternative methods
yield accuracies of 0. The tendency of other methods to select smaller models is attributed
to the small (cluster) separation, often resulting in the selection of only one component.
Moreover, in Models 3, and 5, where component weights are unevenly distributed, GSF
exhibits suboptimal performance compared to the findings presented in (Manole and Khalili,
2021), as GSF imposes a large penalty when some component weights approach zero, leading
to model underestimation. Additionally, for ELBO, similar to our earlier results, larger values
ofϕ (regularregime)yieldloweraccuracycomparedtosmallerϕ (singularregime), especially
0 0
when the sample size is small, as demonstrated in Model 2, 4, and 6. However, overall,
even ELBO with a large ϕ still outperforms other methods. In summary, our simulation
0
results for Gaussian mixture models indicate that ELBO generally outperforms GSF under
the following conditions: (1) large number of components and parameter dimensions; (2)
uneven distribution of mixing weights; and (3) similar mixture components or small cluster
separations.
4.3 ELBO and evidence
In this subsection, we empirically compare and calculate the difference between the ELBO
and the true model evidence. Aoyagi (2010) derived the real log canonical threshold (RLCT)
for the univariate location Gaussian distribution. Note that the RLCT determines the
(asymptotic) leading coefficient in front of logn for the true model evidence. The RLCT
25Figure 4: Percentages of selecting the correct model in the multivariate Gaussian mixture
model.
values in (3) are given as
j +j2 +2(K −K∗ +1)
λ = K∗ −1+ with j = max{i : i+i2 ≤ 2(K −K∗ +1)}.
M
4(j +1)
We visualize these metrics by plotting the ELBO, which includes the empirical evidence
(denoted as Evidence ) derived from MCMC sampling and the theoretical evidence (denoted
E
as Evidence ) by plugging-in the theoretical RLCT value; the data-generating model is
T
p∗(x) = 1N(−2,1)+ 1N(2,1) with a sample size of n = 10000. The empirical evidence is
2 2
obtained via the Monte Carlo method using samples generated from the posterior distribution
using a Metropolis-Hastings algorithm, with a Monte Carlo sample size of N = 20000. From
the results in Figure 5, we observe that the ELBO under a relatively small ϕ = 1/4 (singular
regime) tends to be closer to both evidence values, while that under a relatively large ϕ = 1
(regular regime) resembles the BIC, which is consistent with our theoretical predictions.
Moreover, despite the slight discrepancies between ELBO values and evidence values, they
remain close and have the same overall trend as model size grows, ensuring that the ELBO
inherits the consistency of model selection from using the true evidence.
26Figure 5: ELBO and true evidence values for Figure 6: Selected component numbers with
GaussianmixturemodelwithK components. different fractions of the original data
4.4 Real Data Analysis
In this subsection, we apply our method to the old Faithful geyser eruption data, which
contains 272 two-dimensional data points. By examining the scatter plot (Section 4.2.2 in
Ohn and Lin (2023)), it appears that two components are sufficient to represent the data. We
preprocess the data to equalize the variances of the two dimensions by dividing the waiting
time by 15 and setting σ2 = 0.25. We apply each method using 5%, 7.5%, 10%, 15%, and
25% of the entire dataset to determine the minimal sample size required for each method to
correctly select the true number of components. For each fraction, we repeat the subsampling
100 times and report the averaged number of components selected by different methods. As
shown in Figure 6, when ϕ = 1, the (averaged) number of components selected by the ELBO
0
is already very close to 2 with just 14 data points (5% fraction) and remains unchanged when
the fraction exceeds 7.5%. However, with ϕ = 4, a preference for a larger model (6 is the
0
upper limit we set) is observed when the sample size is small. All three penalties of the GSF
algorithm and the BIC show a trend toward correctly selecting the true number of components
as the sample size increases. However, when the sample size is relatively small, BIC tends
to incorrectly select an overly large model compared to the ELBO. This phenomenon could
again be explained by the stability behavior of the ELBO (with a small ϕ), which makes
the variational posterior distribution under an overspecified model θ more accurate than the
maximum likelihood estimator θ(cid:98)(see Theorem 7 and Table 2), resulting in better power to
distinguish true signals (i.e., separation between mixture components) from estimation errors
due to a small size. A deterioration of the ELBO model selection performance observed
in Figure 6 using a larger ϕ = 4 also empirically supports this explanation. A similar
0
overestimation of the model size occurs with the three GSF methods, which could also be
due to the same reason.
275 Discussion
In this paper, we advocate the use of ELBO from mean-field variational Bayes for model
selection in finite mixture models. We analyze the large-sample properties of ELBO by
proving matching lower and upper bounds that improve upon existing results by utilizing the
consistency property of variational Bayes. As a direct consequence, we have established an
asymptotic expansion for the ELBO and proved the consistency of model selection through
ELBO maximization. As a by-product of our proof, we show that the stable behavior of the
posterior distribution, which benefits from model singularity, can be inherited by the mean-
field variational approximation to the posterior. We also derived a parametric convergence
√
rate of logn/ n for component parameters.
As one future direction, we would like to extend the current development to other singular
models such as hidden Markov models, neural networks, and factor models. Another future
direction is to develop a finite sample analysis of the ELBO by providing a more refined
characterization of the likelihood ratio test statistic under singularity, as we briefly commented
after Corollary 5.
References
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle.
In 2nd International Symposium on Information Theory, pages 267–281.
Alquier, P. and Ridgway, J. (2020). Concentration of tempered posteriors and of their
variational approximations. The Annals of Statistics, 48(3):1475–1497.
Alzer, H. (1997). On some inequalities for the gamma and psi functions. Mathematics of
computation, 66(217):373–389.
Aoyagi, M. (2010). A bayesian learning coefficient of generalization error and vandermonde
matrix-type singularities. Communications in Statistics—Theory and Methods, 39(15):2667–
2687.
Beal, M. J. (2003). Variational algorithms for approximate Bayesian inference. University of
London, University College London (United Kingdom).
Bender, C. M. and Orszag, S. A. (2013). Advanced mathematical methods for scientists and
engineers I: Asymptotic methods and perturbation theory. Springer Science & Business
Media.
Bhattacharya, A., Pati, D., and Plummer, S. (2020). Evidence bounds in singular models:
probabilistic and variational perspectives. arXiv preprint arXiv:2008.04537.
28Bickel, P. J. (1993). Asymptotic distribution of the likelihood ratio statistic in a prototypical
non regular problem. Statistics and Probability: A Raghu Raj Bahadur Festschrift.
Bishop, C. (2006). Pattern recognition and machine learning. 2:531–537.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal of
Machine Learning Research, 3(Jan):993–1022.
Bochkina, N. A. and Green, P. J. (2014). The bernstein–von mises theorem and nonregular
models. The Annals of Statistics, 42(5):1850 – 1878.
Chen, J. (1995). Optimal rate of convergence for finite mixture models. The Annals of
Statistics, pages 221–233.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 39(1):1–22.
Drton, M. (2009). Likelihood ratio tests and singularities. The Annals of Statistics, 37(2):979
– 1012.
Drton, M. and Plummer, M. (2017). A bayesian information criterion for singular models.
Journal of the Royal Statistical Society Series B: Statistical Methodology, 79(2):323–380.
Fru¨hwirth-Schnatter, S. (2006). Finite mixture and Markov switching models. Springer.
Ghosh, J. K. and Sen, P. K. (1984). On the asymptotic performance of the log likelihood
ratio statistic for the mixture model and related results.
Guha, A., Ho, N., and Nguyen, X. (2021). On posterior contraction of parameters and
interpretability in bayesian mixture modeling. Bernoulli, 27(4):2159–2188.
Han, W. and Yang, Y. (2019). Statistical inference in mean-field variational bayes. arXiv
preprint arXiv:1911.01525.
Hartigan, J. A. (1985). A failure of likelihood asymptotics for normal mixtures. In Proceedings
of the Barkeley Conference in Honor of Jerzy Neyman and Jack Kiefer, 1985, volume 2,
pages 807–810.
Heinrich, P. and Kahn, J. (2018). Strong identifiability and optimal minimax rates for finite
mixture estimation. The Annals of Statistics, 46:2844 – 2870.
29Ho, N. and Nguyen, X. (2016a). Convergence rates of parameter estimation for some weakly
identifiable finite mixtures. The Annals of Statistics, 44(6):2726 – 2755.
Ho, N. and Nguyen, X. (2016b). On strong identifiability and convergence rates of parameter
estimation in finite mixtures. Electronic Journal of Statistics, 10:271–307.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999). An introduction to
variational methods for graphical models. Machine Learning, 37:183–233.
Liu, X., Pasarica, C., and Shao, Y. (2003). Testing homogeneity in gamma mixture models.
Scandinavian Journal of Statistics, 30(1):227–239.
Liu, X. and Shao, Y. (2003). Asymptotics for likelihood ratio tests under loss of identifiability.
The Annals of Statistics, 31(3):807–832.
Liu, X. and Shao, Y. (2004). Asymptotics for the likelihood ratio test in a two-component
normal mixture model. Journal of Statistical Planning and Inference, 123(1):61–81.
Manole, T. and Khalili, A. (2021). Estimating the number of components in finite mixture
models via the group-sort-fuse procedure. The Annals of Statistics, 49(6):3043–3069.
McLachlan, G. J., Lee, S. X., and Rathnayake, S. I. (2019). Finite mixture models. Annual
Review of Statistics and its Application, 6:355–378.
Mengersen, K. L., Robert, C., and Titterington, M. (2011). Mixtures: estimation and
applications. John Wiley & Sons.
Mitchell, J. D., Allman, E. S., and Rhodes, J. A. (2019). Hypothesis testing near singularities
and boundaries. Electronic Journal of Statistics, 13(1):2150–2193.
Nguyen, X. (2013). Convergence of latent mixing measures in finite and infinite mixture
models. The Annals of Statistics, 41(1):370 – 400.
Ohn, I. and Lin, L. (2023). Optimal bayesian estimation of gaussian mixtures with growing
number of components. Bernoulli, 29(2):1195–1218.
Pati, D., Bhattacharya, A., and Yang, Y. (2018). On statistical optimality of variational bayes.
In International Conference on Artificial Intelligence and Statistics, pages 1579–1588.
Rotnitzky, A., Cox, D. R., Bottai, M., and Robins, J. (2000). Likelihood-based inference with
singular information matrix. Bernoulli, pages 243–284.
30Rousseau, J. and Mengersen, K. (2011). Asymptotic behaviour of the posterior distribution
in overfitted mixture models. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 73(5):689–710.
Schlattmann, P. (2009). Medical applications of finite mixture models.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, pages
461–464.
Titterington, D. M., Smith, A. F., and Makov, U. E. (1985). Statistical analysis of finite
mixture distributions.
Wang, Y. and Blei, D. M. (2019). Frequentist consistency of variational bayes. Journal of
the American Statistical Association, 114(527):1147–1161.
Watanabe, K. and Watanabe, S. (2006). Stochastic complexities of gaussian mixtures in
variational bayesian approximation. The Journal of Machine Learning Research, 7:625–644.
Watanabe, K. and Watanabe, S. (2007). Stochastic complexities of general mixture models
in variational bayesian learning. Neural Networks, 20(2):210–219.
Watanabe, S. (2001). Algebraic analysis for nonidentifiable learning machines. Neural
Computation, 13(4):899–933.
Watanabe, S. (2009). Algebraic geometry and statistical learning theory. Cambridge university
press.
Watanabe, S. (2018). Mathematical theory of Bayesian statistics. CRC Press.
Yamazaki, K. and Watanabe, S. (2003). Singularities in mixture models and upper bounds
of stochastic complexity. Neural Networks, 16(7):1029–1038.
Yamazaki, K. and Watanbe, S. (2012). Stochastic complexity of bayesian networks. arXiv
preprint arXiv:1212.2511.
Yang, Y., Pati, D., and Bhattacharya, A. (2020). α-variational inference with statistical
guarantees. The Annals of Statistics, 48(2):886–905.
Zhang, F. and Gao, C. (2020). Convergence rates of variational posterior distributions. The
Annals of Statistics, 48(4):2180–2207.
Zhang, Y. and Yang, Y. (2024). Bayesian model selection via mean-field variational approxi-
mation. Journal of the Royal Statistical Society Series B: Statistical Methodology, page
qkad164.
31Supplementary Materials for “Model Selection
for Finite Mixture Models via Variational
Approximation”
A More Literature Review
In this appendix, we offer additional details on some of the literature reviewed in the main
paper, along with information on several more related works.
Variational Bayes. Variational inference was introduced by Jordan et al. (1999) for proba-
bility density approximation with intractable integrals and point estimation for parameter
determination. In variational inference, the posterior distribution is approximated by the
closest member relative to the Kullback-Leibler (KL) divergence in a specified family. Among
the various approximating schemes, mean-field approximation, where the variational family
adopts a factorized form, emerges as the most prevalent type of variational inference. It is
characterized by its conceptual simplicity, ease of implementation, and particular suitability
for addressing problems that involve a high number of latent variables. Pati et al. (2018)
relates the Bayes risk to the variational solution for a general distance metric. In addition, for
non-singular models, which include finite mixture models with a known component number,
Han and Yang (2019) proved that the convergence rate of point estimators based on the
variational posterior is n−1/2, and they also provide the asymptotic normality of the optimal
mean-field approximation centered at the maximum likelihood estimation.
Asymptotic properties of likelihood ratio tests in mixture models. The use of
maximum likelihood estimator (MLE) for fitting mixture models has received a lot of
attention from statisticians since the 1960s. Dempster et al. (1977) introduced a general
iterative approach, the expectation–maximization (EM) algorithm, for computing the MLE
in latent variable models. The convergence properties of the MLE for the mixture problem
were then theoretically established. Meanwhile, the testing of homogeneity, in other words,
determiningwhetherthemixturemodelhasonlyonecomponentormore,becameanimportant
research question. For non-singular models, when under the null hypothesis the parameter
is constrained in a subspace of the whole parameter space, the limit distribution of the
likelihood ratio testing statistic (LRTS) Λ is a chi-square distribution. Unlike in non-singular
n
model testing, Hartigan (1985) observed that under homogeneity, the statistic diverges to
infinity if the whole parameter space is unrestricted. Ghosh and Sen (1984) developed the
asymptotic theory for the distribution of the LRTS under this setting. They showed that
in the limit, 2logΛ is distributed as the square of the supremum of a centered Gaussian
n
32process W : S ∈ S admitting continuous sample paths. Also, Bickel (1993) investigated
S
the null behavior of the LRTS for this model. At the beginning of the 21st century, more
mixture models were considered, and the divergence rate of the LRTS with an unconstrained
parameter space was verified. For example, normal mixture models in Liu and Shao (2004)
and gamma mixture models in Liu et al. (2003) were shown to diverge at the rate O(loglogn).
On the other hand, mixture models with restricted parameter spaces were also explored.
While the limit distribution is still related to a Gaussian process, this time the index set
S is compact, so the LRTS is bounded in probability. The general results for models with
restricted parameter spaces under loss of identifiability are provided by Liu and Shao (2003),
and the application to finite mixture models can also be found in Section 4 of the same paper.
Although their method may test the true model against the alternative, it requires prior
knowledge about the true component number to set the null hypothesis, and the exact form
of the Gaussian process still requires a case-by-case analysis.
33B Table 3
Model w∗,η∗ w∗,η∗ w∗,η∗ w∗,η∗ w∗,η∗
1 1 2 2 3 3 4 4 5 5
1 0.3,(0,0)T 0.7,(2,2)T
√ √
2 0.5,( 2,0)T 0.5,(0, 2)T
     
0 2.5 1.5
     
0 1.5 3
     
3 0.2,   0.3,   0.5,  
0  2  2.75
     
0 1.5 2
√     
2 0 0
√
     
0 2 0
     
4 0.3,   0.3,   0.4, √ 
 0   0   2
     
0 0 0
         
0 −1.5 0.25 −0.25 −1
         
0 2.25 1.5 0.5 −1.5
         
         
0  −1  0.75 −2.5 −0.25
5 0.1,   0.3,   0.1,   0.3,  0.2,  
         
0  0  0.25  1.25   1.75 
         
0  0.5  −0.5  0.75  −0.5
         
0 0.75 −1 1.5 2
√         
2 0 0 0 0
√
         
0 2 0 0 0
         
    √     
 0   0   2  0   0 
6 0.2,   0.2,   0.2,   0.2, √  0.2,  
         
 0   0   0   2  0 
        √ 
 0   0   0   0   2
         
0 0 0 0 0
Table 3: Parameter settings for the multivariate Gaussian mixture models
34C Proofs of Main Results
In this appendix, we collect all proofs of the theoretical results in the paper.
C.1 Proof of Lemma 8
Consider the two mixing distributions G = (cid:80)K w δ and G∗ = (cid:80)K∗ w∗δ . Let δ :=
k=1 k η k k=1 k η k∗
min |η∗−η∗| and I′ := {j : |η −η∗| < δ/2} for k ∈ [K∗], then I′’s are disjoint by definition.
j̸=k j k k j k k
Recall that Wr(G,G∗) = inf (cid:80)K (cid:80)K∗ q (cid:12) (cid:12)η −η∗(cid:12) (cid:12)r , where the infimum ranges over all q
r qij i=1 j=1 ij i j ij
satisfying (cid:80) q = w∗ and (cid:80) q = w .
i∈[K] ij j j∈[K∗] ij i
For each k ∈ [K∗] and feasible q , we will show
ij
(cid:12)(cid:88) (cid:12)
Wr(G,G∗) ≥ (cid:12) w −w∗(cid:12)δr. (14)
r (cid:12) i k(cid:12)
i∈I′
k
To see this, we consider two cases. In the first case when (cid:80) w > w∗, we have
i∈I′ i k
k
K
Wr(G,G∗) ≥ inf(cid:88)(cid:88) q (cid:12) (cid:12)η −η∗(cid:12) (cid:12)r ≥ inf(cid:16)(cid:88) q (cid:12) (cid:12)η −η∗(cid:12) (cid:12)r +(cid:88)(cid:88) q (cid:12) (cid:12)η −η∗(cid:12) (cid:12)r(cid:17)
r ij i j ik i k ij i j
qij qij
i∈I′ j=1 i∈I′ i∈I′ j̸=k
k k k
(cid:88)(cid:88)
≥ 0+inf q (δ/2)r. (15)
ij
qij
i∈I′ j̸=k
k
Since (cid:80) (cid:80) q = (cid:80) w and (cid:80) q ≤ (cid:80) q = w∗ for all feasible q ,
i∈I′ j∈[K∗] ij i∈I′ i i∈I′ ik i∈[K] ik k ij
we obtain (cid:80)k (cid:80) q ≥ (cid:80)k w − w∗. Tk his inequality combined with (15) leads to
i∈I′ j̸=k ij i∈I′ i k
bound (14). In tk he second case whk en (cid:80) w ≤ w∗, we have
i∈I′ i k
k
(cid:32) (cid:33)
K
Wr(G,G∗) ≥ inf(cid:88) q (cid:12) (cid:12)η −η∗(cid:12) (cid:12)r = inf (cid:88) q (cid:12) (cid:12)η −η∗(cid:12) (cid:12)r +(cid:88) q (cid:12) (cid:12)η −η∗(cid:12) (cid:12)r .
r ik i k ik i k ik i k
qij qij
i=1 i∈I′ i∈/I′
k k
Since (cid:80) q = w∗ and (cid:80) q ≤ (cid:80) (cid:80) q = (cid:80) w , we can obtain that
i∈[K] ik k i∈I′ ik i∈I′ j∈[K∗] ij i∈I′ i
(cid:80) q ≥ w∗ −(cid:80) w , whichk further impk lies bound (14) duek to the preceding display
i∈/I′ ik k i∈I′ i
k k
and the definition of I .
k
35Finally, since K∗ is finite, we obtain from inequality (14) that
(cid:88)K∗
(cid:12)(cid:88) (cid:12)
Wr(G,G∗) ≥ (K∗)−1 (cid:12) w −w∗(cid:12)δr
r (cid:12) j k(cid:12)
k=1 i∈I′
k
(cid:88)K∗
(cid:12)(cid:88) (cid:12)
(cid:88)K∗
(cid:12)(cid:88) (cid:12)
≳ (cid:12) w −w∗(cid:12)δr ≥ inf (cid:12) w −w∗(cid:12)δr. (16)
(cid:12) j k(cid:12) (cid:12) j k(cid:12)
k=1 i∈I k′
I1,...,I K∗
k=1 i∈I k
The claimed bound then follows by combing above with inequality (11).
C.2 Proof of Theorem 2
The proof is divided into three steps. Firstly, we decompose the ELBO into two parts
and obtain some analytically manageable approximations to both parts. Secondly, we
constructively prove the claimed lower bound by using two instances of q , since for any
Zn
q , L(q ) provides a lower bound to the ELBO. Finally, we prove the claimed upper bound
Zn Zn
based on the key Lemma 9 and our Assumption A2.
Step 1: For the complete-data {Xn,Sn}, we denote the posterior probability of X coming
i
from the kth component as p = q (S = k), and let n =
(cid:80)n
p . Then according to
(cid:98)ik (cid:98)Sn i (cid:98)k i=1 (cid:98)ik
formula (8), the variational posterior distribution q (θ) = q (w)⊗q (η) can be written as
(cid:98)θ (cid:98)w (cid:98)η
K
Γ(n+Kϕ ) (cid:89)
q (w) = 0 w(n (cid:98)k+ϕ0)−1, (17)
(cid:98)w (cid:81)K
Γ(n +ϕ )
k
k=1 (cid:98)k 0 k=1
and
(cid:34) (cid:35)
K n
(cid:89) (cid:88) (cid:0) (cid:1)
q (η) ∝ π(η )exp p ηTT(x )−T (x )−A(η ) . (18)
(cid:98)η k (cid:98)ik k i 0 i k
k=1 i=1
From equations (17) and (18), we observe that q (w) and q (η) are parameterized by (in other
(cid:98)w (cid:98)η
words, only depend on) {p : i ∈ [n],k ∈ [K]}. Additionally, q (sn) is also paramaterized
(cid:98)ik (cid:98)Sn
by {p } due to formula (9). Therefore, we only need to analyze those q (θ) = q (w)⊗q (η)
(cid:98)ik θ w η
taking the form of
K
Γ(n+Kϕ ) (cid:89)
q (w) = 0 w(n k+ϕ0)−1, (19)
w (cid:81)K
Γ(n +ϕ )
k
k=1 k 0 k=1
36and
(cid:34) (cid:35)
K n
(cid:89) (cid:88) (cid:0) (cid:1)
q (η) ∝ π(η )exp p ηTT(X )−T (X )−A(η ) . (20)
η k ik k i 0 i k
k=1 i=1
where n = (cid:80)n p ; and given a q (θ), the optimal q (sn) takes the following form with a
k i=1 ik θ Sn
normalization constant C (also parameterized by {p }),
Q ik
(cid:26)(cid:90) (cid:27)
1
q (sn) = exp q (θ)logp(Xn,sn|θ)dθ . (21)
Sn θ
C
Q
In the rest of the proof, we only consider those q (w), q (η) and q (sn) having the forms as
w η Sn
(19), (20) and (21). Based on these characterizations, we know that maximizing L(q ) over
Zn
q (zn) = q (θ)⊗q (Sn) is equivalent to maximizing L(q ) over all possible combinations
Zn θ Sn Zn
of {p : i ∈ [n],k ∈ [K]} subject to 0 ≤ p ≤ 1 and
(cid:80)K
p = 1 for all i.
ik ik k=1 ik
We now characterize the ELBO function. According to the definition of L(q ) in (5), we
Zn
can decompose the ELBO into two parts as
(cid:90) p(Xn,sn|θ)π(θ)
L(q ) = q (zn)log dzn
Zn Zn
q (zn)
Zn
(cid:90) π(θ) (cid:88) (cid:90) p(Xn,sn|θ)
= q (θ)log dθ+ q (sn) q (θ)log dθ,
θ q (θ) Sn θ q (sn)
θ Sn
sn
where the first part is just −D (q (θ)∥π(θ)) and the second term is logC . Then we can
KL θ Q
reformulate L(q ) as
(cid:98)Zn
(cid:110) (cid:111)
L(q ) = max −D (q (θ)∥π(θ))+logC . (22)
(cid:98)Zn KL θ Q
{p }
ik
We consider the KL divergence part first. Recall that priors on w and η are independent,
which leads to
D (q (θ)∥π(θ)) = D (q (w)∥π(w))+D (q (η)∥π(η)).
KL θ KL w KL η
For the D (q (w)∥π(w)) term, we can use the closed forms of the prior and the variational
KL w
posterior of w to explicitly calculate
(cid:90)
q (w)logw dw = Ψ(n +ϕ )−Ψ(n+Kϕ ), (23)
w k k 0 0
37where Ψ(x) = Γ′(x)/Γ(x) is the so-called di-gamma function. This further leads to
K
(cid:88)
D (q (w)∥π(w)) = (n Ψ(n + ϕ )−logΓ(n +ϕ ))−nΨ(n+Kϕ )
KL w k k 0 k 0 0
k=1
Γ(ϕ )K
0
+logΓ(n+Kϕ )+log , (24)
0
Γ(Kϕ )
0
where the last term causes the undesirable behavior under large ϕ as discussed after Theorem
0
2. To further simplify this expression, we resort to the following two inequalities: for any
x > 0 (Alzer, 1997), we have
1 1
< logx−Ψ(x) < , (25)
2x x
and
(cid:18) (cid:19)
1 1 1
0 ≤ logΓ(x)− (x− )logx−x+ log2π ≤ .
2 2 12x
Applying these two inequalities to (24), we can obtain
(cid:12) 1 1 (cid:88)K (cid:12)
(cid:12)D (q (w)∥π(w))−(Kϕ − )logn+(ϕ − ) log(n +ϕ )(cid:12) < C, (26)
(cid:12) KL w 0 2 0 2 k 0 (cid:12)
k=1
for some constant C independent of n and ϕ .
0
As to the D (q (η)∥π(η)) term, since both the prior π and the variational posterior of
KL η
η are factorized under our setup, we have
k
K
(cid:88)
D (q (η)∥π(η)) = D (q (η )∥π(η )).
KL η KL η k k
k
k=1
For each fixed k ∈ [K], we denote the variational posterior mode (i.e., maximizer of its density
function) as
n
(cid:88) (cid:0) (cid:1)
η = argmax p ηTT(x )+T (x )−A(η ) , (27)
(cid:98)k ik k i 0 i k
η
k i=1
(cid:124) (cid:123)(cid:122) (cid:125)
ℓ (η )
nk k
Note that the critical point η satisfies the first order condition ∇ ℓ (η ) = 0. Under this
(cid:98)k η nk (cid:98)k
k
38notation, we can decompose D (q (η )∥π(η )) as
KL η k k
k
(cid:90)
q (η )
η k
D (q (η )∥π(η )) = q (η )log k
KL η k k η k
k k π(η )
k
(cid:90)
π(η )exp{ℓ (η )−ℓ (η )}
k nk k nk (cid:98)k
= q (η )log
η k (cid:82)
k π(η ) π(η )exp{ℓ (η )−ℓ (η )}dη
k k nk k nk (cid:98)k k
(cid:90)
= q (η )(ℓ (η )−ℓ (η ))dη
η k nk k nk (cid:98)k k
k
(cid:90)
−log π(η )exp{ℓ (η )−ℓ (η )}dη . (28)
k nk k nk (cid:98)k k
For the second term, applying Taylor expansion of ℓ (η ) at η up to the second order, we
nk k (cid:98)k
obtain
n
1 (cid:88)
ℓ (η )−ℓ (η ) = − p (η −η )TI(η )(η −η ),
nk k nk (cid:98)k ik k (cid:98)k (cid:101)ik k (cid:98)k
2
i=1
where η lies on the segment between η and η such that η = η +t (η −η ),t ∈ [0,1].
(cid:101)ik k (cid:98)k (cid:101)ik (cid:98)k i k (cid:98)k i
Then a standard Laplace’s approximation for integrals, e.g., equation (6.4.35) in Bender and
Orszag (2013), implies that with n large enough, the second integral in (28) satisfies
k
(cid:12)(cid:82) (cid:12)
(cid:12) π(η )exp{ℓ (η )−ℓ (η )}dη π(η ) (cid:12) C
(cid:12) k nk k nk (cid:98)k k − (cid:98)k (cid:12) ≤ . (29)
(cid:12) (cid:12) (2π/n k)d 2 (cid:112) I(η (cid:98)k)(cid:12) (cid:12) n k
Therefore, for large n , we have
k
(cid:12) (cid:90) (cid:12)
(cid:12) d (cid:12)
(cid:12)−log π(η k)exp{ℓ nk(η k)−ℓ nk(η (cid:98)k)}dη
k
+ logn k(cid:12) ≤ C. (30)
(cid:12) 2 (cid:12)
For the first term, by plugging in the explicit form of q(η) from (20) into the integral, we
obtain
(cid:90) (cid:82)
π(η )(ℓ (η )−ℓ (η ))exp{ℓ (η )−ℓ (η )}dη
k nk k nk (cid:98)k nk k nk (cid:98)k k
q (η )(ℓ (η )−ℓ (η ))dη = ,
η k nk k nk (cid:98)k k (cid:82)
k π(η )exp{ℓ (η )−ℓ (η )}dη
k nk k nk (cid:98)k k
whose denominator is already analyzed. Using Laplace’s approximation again to the numera-
tor, as well as the fact that the value of the integrand at η vanishes, we obtain
(cid:98)k
(cid:12)(cid:82) (cid:12)
(cid:12) π(η )(ℓ (η )−ℓ (η ))exp{ℓ (η )−ℓ (η )}dη (cid:12) C
(cid:12) k nk k nk (cid:98)k nk k nk (cid:98)k k −0(cid:12) ≤ .
(cid:12) (cid:12) (2π/n k)d 2 (cid:12) (cid:12) n k
39Therefore, by noticing ℓ (η ) ≤ ℓ (η ), we have
nk k nk (cid:98)k
(cid:90)
C
− ≤ q (η )(ℓ (η )−ℓ (η ))dη ≤ 0. (31)
η k nk k nk (cid:98)k k
n k
k
Combining (28), (30) and (31) together, we finally obtain that for any sufficiently large n ,
k
(cid:12) d (cid:12)
(cid:12)D (q (η )∥π(η ))− logn (cid:12) ≤ C. (32)
(cid:12) KL η k k k 2 k(cid:12)
Step 2: Next we prove a lower bound for L(q ). According to the characterization of ELBO
(cid:98)Zn
in (22), every instance of {p } gives a lower bound to L(q ). Let us consider the following
ik (cid:98)Zn
two special constructions:
Construction (I): This construction applies to the case when ϕ > (d+1)/2. For 1 ≤ k ≤
0
K∗ −1, we can always choose {p : i ∈ [n],k ∈ [K∗ −1]} such that
ik
n
(cid:88) n +ϕ
n = p = w∗(n+Kϕ ) −ϕ , i.e. k 0 = w∗,
k ik k 0 0 n+Kϕ k
0
i=1
n
(cid:88)
and p T(x ) = n ∇ A(η∗), i.e. η = η∗,
ik i k η k k (cid:98)k k
i=1
and for K∗ ≤ k ≤ K, we choose {p : i ∈ [n],K∗ ≤ k ≤ K} such that
ik
n+Kϕ (n +ϕ )(K −K∗ +1)
n = w∗ 0 −ϕ , i.e. k 0 = w∗
k K∗K −K∗ +1 0 n+Kϕ K∗
0
n
(cid:88)
and p T(x ) = n ∇ A(η∗ ), i.e. η = η∗ .
ik i k η k K∗ (cid:98)k K∗
i=1
With this construction, all n ’s are proportional to n, so we have from (32) that
k
K
d (cid:88)
D (q (η)∥π(η)) ≤ logn +C.
KL η k
2
k=1
Combiningtheabovewiththeapproximation(26)toD (q (w)∥π(w)),wehavethefollowing
KL w
40upper bound to the KL divergence between q (θ) and π(θ),
θ
K K
1 d (cid:88) 1 (cid:88)
D (q (θ)∥π(θ)) ≤ (Kϕ − )logn+ logn +( −ϕ ) log(n +ϕ )+C
KL θ 0 k 0 k 0
2 2 2
k=1 k=1
K
1 d+1 (cid:88)
≤ (Kϕ − )logn+( −ϕ ) logn +C
0 0 k
2 2
k=1
1 d+1
= (Kϕ − )logn+( −ϕ )Klogn+C
0 0
2 2
dK +K −1
= logn+C. (33)
2
As for the lower bound of logC , we can first rewrite C as
Q Q
n (cid:90)
(cid:89)(cid:88)
C = exp q (θ)logp(x ,s |θ)dθ
Q θ i i
i=1 si
n K (cid:18)(cid:90) (cid:90) (cid:19)
(cid:89)(cid:88)
= exp q (w )logw dw + q (η )logg(x ;η )dη . (34)
w k k k η k i k k
k k
i=1 k=1
Using equations (23) and (25) again, we obtain
(cid:90)
n +ϕ 1
k 0
q (w )logw dw = Ψ(n +ϕ )−Ψ(n+Kϕ ) ≥ log − .
w k k k k 0 0
k n+Kϕ n +ϕ
0 k 0
Additionally, since η∗ = η in this construction, we can apply a Taylor expansion at η∗.
k (cid:98)k k
Concretely, we can write q (η ) explicitly according to (20) as,
η k
k
(cid:90) (cid:82) π(η )logg(x ;η )exp{ℓ (η )−ℓ (η∗)}dη
q (η )logg(x ;η )dη = k i k nk k nk k k .
η k k i k k (cid:82) π(η )exp{ℓ (η )−ℓ (η∗)}dη
k nk k nk k k
Under Assumption A1 and using the fact that n ≍ n, we can apply Laplace’s approximation
k
to approximate the numerator and denominator respectively as,
(cid:12)(cid:82) (cid:112) (cid:12)
(cid:12) π(η )logg(x ;η )exp{ℓ (η )−ℓ (η∗)}dη I(η∗) (cid:12) C
(cid:12) k i k nk k nk k k · k −1(cid:12) ≤ , (35)
(cid:12) (cid:12) (2π/n k)d 2 π(η k∗)logg(x i;η k∗) (cid:12) (cid:12) n k
and
(cid:12)(cid:82) (cid:112) (cid:12)
(cid:12) π(η )exp{ℓ (η )−ℓ (η∗)}dη I(η∗) (cid:12) C
(cid:12) k nk k nk k k · k −1(cid:12) ≤ . (36)
(cid:12) (cid:12) (2π/n k)d 2 π(η k∗) (cid:12) (cid:12) n k
41Therefore, with the choice of w∗ = (n +ϕ )/(n+Kϕ ) for all k < K∗, we have
k k 0 0
K∗−1 (cid:18)(cid:90) (cid:90) (cid:19)
(cid:88)
exp q (w )logw dw + q (η )logg(x ;η )dη
w k k k η k i k k
k k
k=1
K∗−1 (cid:18) (cid:18) (cid:19)(cid:19)
(cid:88) 1 C
≥ exp logw∗ − +logg(x ;η∗)−log 1+
k n +ϕ i k n
k 0 k
k=1
K∗−1 (cid:18) (cid:19)
(cid:88) C
≥ w∗g(x ;η∗) 1− .
k i k n
k=1
The same argument can applies to k ≥ K∗, where by using the fact that n ≍ n, we can
k
obtain that for all K ≥ K∗,
K (cid:18)(cid:90) (cid:90) (cid:19)
(cid:88)
exp q (w )logw dw + q (η )logg(x ;η )dη
w k k k η k i k k
k k
k=K∗
(cid:88)K (cid:18) w∗ 1 (cid:18) C (cid:19)(cid:19)
≥ exp log K∗ − +logg(x ;η∗ )−log 1+
K −K∗ +1 n +ϕ i K∗ n
k 0 k
k=K∗
(cid:18) (cid:19)
C
≥ w∗ g(x ;η∗ ) 1− .
K∗ i K∗ n
According to (34), the constant logC can be bounded from below by
Q
n (cid:32) K∗ (cid:18) (cid:19)(cid:33)
(cid:88) (cid:88) C
logC ≥ log w∗g(x ;η∗) 1− = logp∗(Xn)−C. (37)
Q k i k n
i=1 k=1
Finally, by noticing the formulation of L(q ) as in (22), we can add (33) and (37) together
(cid:98)Zn
to get a lower bound when ϕ > (d+1)/2 as
0
dK +K −1
L(q )−logp∗(Xn) ≥ − logn−C. (38)
(cid:98)Zn
2
Construction (II): This construction applies to the case when ϕ ≤ (d+1)/2. For each k
0
such that 1 ≤ k ≤ K∗, we can always choose {p : i ∈ [n],k ∈ [K∗]} such that
ik
n
(cid:88) n +ϕ
n = p = w∗(n+ϕ )−ϕ , i.e. k 0 = w∗,
k ik k 0 0 n+Kϕ k
0
i=1
n
(cid:88)
and p T(x ) = n ∇ A(η∗);
ik i k η k k
i=1
and for K∗ < k ≤ K, we set n = 0. Then for each k > K∗, q (η ) is the same as prior
k η k
k
42density π(η ), so we have D (q (η )∥π(η )) = 0 for all k > K∗. For each k ≤ K∗, we have
k KL η k k
k
n ≍ n, so (32) leads to
k
K∗
(cid:88) d
D (q (η)∥π(η)) ≤ logn +C.
KL η k
2
k=1
Combining the preceding display with (26), we obtain
(cid:18) dK∗ +K∗ −1 (cid:19)
D (q (θ)∥π(θ)) ≤ +(K −K∗)ϕ logn+C. (39)
KL θ 0
2
Now since the summation over k ∈ [K] for the C in (34) is dominated by its first K∗ terms,
Q
we can lower bound this summation to the sum of the first K∗ terms. Now, since n ≍ n for
k
k ≤ K∗, we can apply (35) and (36) to obtain
K∗ (cid:18)(cid:90) (cid:19) K∗ (cid:18) (cid:19)
(cid:88) (cid:88) C
exp q (θ )log(w g(x ;η ))dθ ≥ w∗g(x ;η∗) 1− .
θ k k k i k k k i k n
k=1 k=1
Putting all the pieces together, we obtain
n (cid:32) K∗ (cid:33) (cid:18) (cid:19) (cid:18) (cid:19)
(cid:88) (cid:88) C 1
logC ≥ log w∗g(x ;η∗) +nlog 1− = logp∗(Xn)−C +O . (40)
Q k i k n n
i=1 k=1
Adding(39)and(40)together,wecanobtainalowerboundtoL(q (zn))forallϕ ≤ (d+1)/2
Zn 0
as
(cid:18) dK∗ +K∗ −1 (cid:19)
L(q )−logp∗(Xn) ≥ − +(K −K∗)ϕ logn−C. (41)
(cid:98)Zn 0
2
Finally, combining (38) and (41) leads to the claimed lower bound of L(q ).
(cid:98)Zn
Step 3: Now we derive an upper bound of the ELBO in (22). We first tackle the logC(cid:98)
Q
term, recall that C(cid:98)
Q
is the normalization constant for the optimal distribution q (cid:98)Sn(sn) in (9).
Note that logw is concave with respect to w . For logg(x;η ), the second order derivative
k k k
with respect to η is the negative fisher information matrix for the mixture component
K
∇2 logg(x;η ) = −∇2 A(η ) = −I(η ),
η k η k k
k k
which is negative definite according to Assumption A1. Therefore, logg(x;η) is concave with
43respect to η . By applying Jensen’s inequality, we can then obtain
k
(cid:90) (cid:90)
q (w )logw dw + q (η )logg(x;η )dη ≤ log(w g(x;η )),
(cid:98)w k k k k (cid:98)η k k k k k k
which combined with (34) leads to
(cid:32) (cid:33)
n K
(cid:88) (cid:88)
logC(cid:98) ≤ log w g(x ;η ) = logp(Xn|θ). (42)
Q k i k
i=1 k=1
The KL divergence D (q (w)∥π(w)) is the same as (26). As to the D (q (η)∥π(η)) term,
KL w KL η
note that for a large n , we can apply the approximation (32). Consequently, we define a
k
set F(cid:98) := {k|1 ≤ k ≤ K,n ≥ M}, where M is a sufficiently large (truncating) constant, to
(cid:98)k
select those indices k for which the approximation is accurate. This approximation, when
combined with the non-negativity of the KL divergence, gives
K
(cid:88) d (cid:88)
D (q (η)∥π(η)) = D (q (η )∥π(η )) ≥ logn −C.
KL (cid:98)η KL (cid:98)η k k (cid:98)k
k 2
k=1 k∈F
Using approximation (26) again, we can bound the entire KL divergence term from below as
(cid:18) (cid:19)
1 d+1 (cid:88)
D (q (θ)∥π(θ)) ≥ (Kϕ − )logn+ −ϕ logn
KL (cid:98)θ 0 0 (cid:98)k
2 2
k∈F
1 (cid:88)
+( −ϕ ) logn −C
0 (cid:98)k
2
k∈/F
(cid:18) (cid:19)
1 d+1 (cid:88)
= (Kϕ − )logn+ −ϕ logn −C. (43)
0 0 (cid:98)k
2 2
k∈F
In the last part of this proof, we will show that at least K∗ of the n ’s are proportional to
(cid:98)k
n, thus are in the set F(cid:98). Using this fact, we can obtain that for each ϕ < (d+1)/2,
0
(cid:18) (cid:19)
1 d+1
D (q (θ)∥π(θ)) ≥ (Kϕ − )logn+ −ϕ K∗logn−C
KL (cid:98)θ 0 0
2 2
(cid:18) dK∗ +K∗ −1 (cid:19)
= +ϕ (K −K∗) logn−C.
0
2
When ϕ > (d+1)/2, by using the fact that 0 < n ≤ n and the lower bound in (43), we can
0 (cid:98)k
44obtain,
(cid:18) (cid:19) K
1 d+1 (cid:88)
D (q (θ)∥π(θ)) ≥ (Kϕ − )logn+ −ϕ logn −C
KL (cid:98)θ 0 0 (cid:98)k
2 2
k=1
(cid:18) (cid:19)
1 d+1
≥ (Kϕ − )logn+ −ϕ Klogn−C
0 0
2 2
dK +K −1
= logn−C.
2
Combining the two lower bounds of D (q (θ)∥π(θ)) obtained above and the bound in (42),
KL (cid:98)θ
we can get an upper bound to L(q )−logp∗(Xn) as
(cid:98)Zn
L(q )−logp(Xn|θ) ≤ −λlogn+C,
(cid:98)Zn
where λ is given by equation (12) in the theorem statement.
It remains to show that at least K∗ of the n ’s are of the same magnitude of n. Noticing
(cid:98)k
that the Hellinger distance

(cid:32) (cid:33)1/2
2
(cid:90) K
(cid:0) (cid:1) (cid:88) (cid:112)
h2 p(x|θ), p∗(x) =  w kg(x;η k) − p∗(x) dx
k=1
 
(cid:32) (cid:33)1/2
(cid:90) (cid:88)K (cid:88)K (cid:112) 
= w g(x;η )+p∗(x)−2 w g(x;η ) p∗(x) dx
k k k k
 
k=1 k=1
is a convex function with respect to every w , then using (11), Lemma 9, Jensen’s inequality
k
(cid:0) (cid:1) (cid:0) (cid:1)
and the inequality d p(x|θ), p∗(x) ≤ h p(x|θ), p∗(x) , we can get
TV
(cid:32) inf (cid:88)K∗ (cid:12) (cid:12)(cid:88) w −w∗(cid:12) (cid:12)(cid:33)2 ≲ (cid:90) h2(cid:0) p(x|w,η),p∗(x)(cid:1) q(η)dη
(cid:12) j k(cid:12) (cid:98)
{I1,...I K∗}
k=1 j∈I k
ΩK
(cid:90)
(cid:0) (cid:1)
≤ h2 p(x|w,η),p∗(x) q(θ)dθ.
(cid:98)
Θ
(cid:18) logp∗(Xn)−L(q ) logn(cid:19)
(cid:98)Zn
≤ C + ,
4
n n
where the convexity is used in the second inequality. Recall that, in Step 2, we proved a
45lower bound to L(q )−logp∗(Xn), which is −λlogn. Thus we can obtain
(cid:98)Zn
(cid:32) (cid:88)K∗
(cid:12)(cid:88)
(cid:12)(cid:33)2
λlogn+logn
inf (cid:12) w −w∗(cid:12) ≲ .
{I1,...I K∗} k=1(cid:12)
j∈I
k
j k(cid:12) n
Since the cardinality of I is at most K, we have that for every k ≤ K∗, there is at least one
k
(cid:112)
w ,j ∈ I such that w ≥ (w∗ −C logn/n)/K. Therefore, we can obtain
j k j k
(cid:16) (cid:112) (cid:17)
n = w (n+Kϕ )−ϕ ≥ w∗ −C logn/n (n+Kϕ )/K −ϕ ≍ n,
(cid:98)j j 0 0 k 0 0
which completes the proof.
C.3 Proof of Corollary 3
According to the classical Hironaka’s Theorem for the resolution of singularity in alge-
braic geometry, which is Theorem 2.3 in Watanabe (2009), the KL divergence K(θ) =
(cid:0) (cid:1)
D p∗(·)∥p(·|θ) for a parametric family {p(·|θ) : θ ∈ Θ} of a singular model, as a func-
KL
tion of θ, can be transformed (via an invertible map) into its normal crossing form (uniquely
determined by analytical properties of K(θ)) around any of its zero point θ∗, that is, any
point θ∗ such that K(θ∗) = 0; see Theorem 2.3 in Watanabe (2009) for the precise statement.
Moreover, Theorem 6.2 in Watanabe (2009) characterized the asymptotic behavior of the
likelihood ratio statistic under a standard normal crossing form. Note that their Fundamental
condition (I) (Definition 6.1) is satisfied by our considered mixtures of the exponential family
under Assumptions A1–A3.
Moreover, we know that the maximum likelihood estimator is invariant under any (one-
to-one) reparameterization of the model (i.e., the invariance property of MLE). Therefore,
the likelihood ratio statistic LRT defined after Theorem 2 coincides with the likelihood ratio
statistic under the normal crossing form analyzed in their Theorem 6.2, which is shown to
converge in distribution as n → ∞ to the supremum of a Gaussian process that is bounded
in probability. These facts imply a bound of LRT = O (1), which combined with Theorem 2
P
leads to Corollary 3.
C.4 Proof of Corollary 4
According to Theorem 2 and the bound ∆ ≤ LRT = O (1) as n → ∞, we can conclude
n P
that for all sufficiently large n and all K ≥ K∗, the probability that the ELBO value L(cid:98)
K
46strictly decreases with K approaches one as n → ∞. Particularly, for K = K∗, we have
dK∗ +K∗ −1 dK∗ +K∗ −1
logp∗(Xn)− logn ≤ L(q )−C ≤ logp(Xn|θ)− logn+C .
(cid:98)Zn 1 2
2 2
On the other hand, when K < K∗ and G(θ) = (cid:80)K w δ , we will show below that the
j=1 j ηj
Wasserstein distance will be bounded from below by a constant order; then we can apply
Lemma 9 to show that the ELBO value L(cid:98)
K
is also strictly less than L(cid:98) K∗. Concretely, recall
thatintheproofofLemma8,wedefinedδ = min |η∗−η∗| > 0andI = {j : |η −η∗| < δ/2}
j̸=k j k k j k
for k ∈ [K]. Since K < K∗ and I ’s are disjoint, there is at least one I that is empty.
k k
Without loss of generality, we may assume I = ∅. This then leads to (according to the proof
1
of Lemma 8)
K K
(cid:88) (cid:88)
Wr(G,G∗) ≥ inf q |η −η∗|r ≥ inf q (δ/2)r = w∗(δ/2)r.
r i1 i 1 i1 1
qi1 qi1
i=1 i=1
(cid:0) (cid:1)
Now an application of Lemma 9, Assumption A3 and the inequality h p(x|θ), p∗(x) ≥
(cid:1)
d (p(x|θ), p∗(x) leads to
TV
logp∗(Xn)−L(q ) logn (cid:90)
(cid:98)Zn + ≳ d2 (cid:0) p(x|θ), p∗(x)(cid:1) q (θ) ≳ w∗2(δ/2)2r.
n n TV (cid:98)θ 1
Θ
Therefore, we have
L(q ) ≤ logp∗(Xn)+logn−cnw∗2(δ/2)2r
(cid:98)Zn 1
for some positive constant c. On the other hand, the ELBO value L(cid:98)
K∗
is at least logp∗(Xn)−
O (logn), which implies that the ELBO value L(cid:98) at K < K∗ is strictly smaller than that
P K
at K = K∗ for all sufficiently large n.
C.5 Proof of Corollary 5
Under the assumption that T (θ) ≤ C loglogn holds with high probability, we have
n 3
−λlogn−C loglogn−C ≤ L(q )−logp(Xn|θ) ≤ −λlogn+C . (44)
3 1 (cid:98)Zn 2
Let us start with the case of ϕ < (d+1)/2. In the proof of Theorem 2, we showed that
0
at least K∗ of the n ’s are proportional to n when ϕ < (d+1)/2. We can then without
(cid:98)k 0
loss of generality assume that it is the first K∗ of n ’s. We will prove the claimed bound by
(cid:98)k
contradictionbelow. Nowsupposethereexistssomek′ > K∗ suchthatw
k′
≥ [(logn)ρ1+ϕ 0]/n,
47then n
(cid:98)k′
= w k′(n+Kϕ 0)−ϕ
0
≥ (logn)ρ1, implying k′ ∈ F(cid:98) (defined in the proof of Theorem 2).
Now using inequality (43), we obtain
(cid:18) (cid:19)
1 d+1 (cid:88)
D (q (θ)∥π(θ)) ≥ (Kϕ − )logn+ −ϕ logn −C
KL (cid:98)θ 0 0 k
2 2
k∈/F
(cid:20) dK∗ +K −1(cid:21)
≥ (K −K∗)ϕ + logn+(C +1)loglogn−C.
0 3
2
Therefore, by further using (22) and (42), we obtain
L(q )−logp(Xn|θ) ≤ −D (q (θ)∥π(θ))+C ≤ −λ logn−(C +1)loglogn−C,
(cid:98)Zn KL (cid:98)θ 1 3
where λ = (K −K∗)ϕ +(dK∗ +K∗ −1)/2. Note that this upper bound is smaller than
1 0
the corresponding lower bound as in inequality (44) for all sufficiently large n, which is a
contradiction. Therefore, we must have w ≤ [(logn)ρ1+ϕ ]/n for all k > K∗, which leads to
k 0
(cid:88)K (cid:88)K (K −K∗)[(logn)ρ1 +ϕ ]
0
inf w ≤ w < .
σ(k) k
σ∈S n
k k=K∗+1 k=K∗+1
For the other case of ϕ > (d + 1)/2, suppose there exists some k′′ such that w ≤
0 k
1/(logn)ρ2 +ϕ 0/n, then we have n
(cid:98)k′′
≤ n/(logn)ρ2. By further using inequality (43), we get
(cid:18) (cid:19)
1 d+1
D (q (θ)∥π(θ)) ≥ (Kϕ − )logn+ −ϕ (Klogn−ρ loglogn)−C,
KL (cid:98)θ 0 0 2
2 2
which implies that, with λ = (dK +K −1)/2, we have
L(q )−logp(Xn|θ) ≤ −λlogn−(C +1)loglogn+C.
(cid:98)Zn 3
This upper bound is again smaller than its corresponding lower bound from (44), which is a
contradiction. Therefore, for all k ∈ {1,...,K}, we must have w ≥ 1/(logn)ρ2 +ϕ /n. This
k 0
completes the proof.
C.6 Proof of Theorem 7
Step 1: In the first step, we establish a relationship between the total variation dis-
tance between two finite mixture distributions and a discrepancy metric between their
respective parameters, similar to the proof of Theorem 3.1 in Ho and Nguyen (2016a).
Concretely, we consider a general finite mixture measure p(x|θ) =
(cid:80)K∗
w g(x;η ), with
k=1 k k
48θ = (w ,...,w ,η ,...,η ), and w = (w ,...,w ), η = (η ,...,η ) where the mixing
1 K∗ 1 K∗ 1 K∗ 1 K∗
weights satisfy w ≥ 0 and
(cid:80)K∗
w ≤ 1 and every η is from the compact set Ω with
k k=1 k k
diameter L < ∞. Note that here p(x|θ) is not necessarily a density function since its integral
can be smaller than 1. Also recall that p∗(x) = (cid:80)K∗ w∗g(x;η∗) is the true finite mixture
k=1 k k
distribution. We consider the following discrepancy metric between the parameters of p(·|θ)
and p∗(·),
K∗
(cid:88)(cid:0) (cid:1)
D(θ) := inf |w −w∗|+w |η −η∗| .
σ(k) k σ(k) σ(k) k
σ∈S K∗
k=1
We will show below that there exists positive constants ε and c such that for any θ such that
D(θ) < ε, it holds
(cid:90)
1 (cid:12) (cid:12)
(cid:12)p(x|θ)−p∗(x)(cid:12)dx > cD(θ). (45)
2
We will prove this inequality by contradiction. If this is not the case, then there exists
some sequence {θm : m ≥ 1} such that D(θm) → 0 as m → ∞ and
(cid:90)
1
lim |p(x|θm)−p∗(x)|dx = 0. (46)
m→∞ D(θm)
Since D(θm) → 0 as m → ∞ and the number of possible permutations of [K∗] is finite, there
is a sub-sequence {θm ℓ} and a permutation σ′ ∈ S
K∗
such that w σm ′(ℓ
k)
→ w k∗ and η σm ′(ℓ
k)
→ η k∗
for all k = 1,...,K∗ as ℓ → ∞. Without loss of generality, we may assume that σ′ is the
identity permutation and substitute the sequence with its sub-sequence, so that we can write
D(θm) = (cid:80)K∗ (cid:0) |w −w∗|+w |η −η∗|(cid:1) for all sufficiently large m. Now we can apply a
k=1 k k k k k
Taylor expansion as follows to obtain:
K∗ K∗
(cid:88) (cid:0) (cid:1) (cid:88)
p(x|θm)−p∗(x) = wm g(x;ηm)−g(x;η∗) + (wm −w∗)g(x;η∗)
k k k k k k
k=1 k=1
K∗ K∗
=
(cid:88) wm(cid:104) (cid:0)
ηm
−η∗(cid:1) ∇g(x;η∗)+O(cid:0)
ηm
−η∗(cid:1)2(cid:105) +(cid:88)
(wm −w∗)g(x;η∗)
k k k k k k k k k
k=1 k=1
K∗ K∗
(cid:88) (cid:0) (cid:1) (cid:88)
= wm ηm −η∗ ∇g(x;η∗)+ (wm −w∗)g(x;η∗)+R , (47)
k k k k k k k m
k=1 k=1
where the remainder term satisfies R /D(θm) → 0 as m → ∞. Note that equation (46) will
m
49then imply
p(x|θm)−p∗(x) (cid:26) (cid:88)K∗ wm(cid:0) ηm −η∗(cid:1) (cid:88)K∗ wm −w∗ (cid:27)
lim = lim k k k ∇g(x;η∗)+ k k g(x;η∗) = 0
m→∞ D(θm) m→∞ D(θm) k D(θm) k
k=1 k=1
for almost every x. Note that the term inside the second limit is a linear combination of finite
many g(x;η∗) and ∇g(x;η∗) terms. Since the parameter space is compact under Assumption
k k
A1, each linear combination coefficient has a convergent sub-sequence. Without loss of
generality, we may substitute the sequence with its sub-sequence, and denote the respective
limits as
(cid:0) (cid:1)
wm ηm −η∗ wm −w∗
α = lim k k k and β = lim k k.
k m→∞ D(θm) k m→∞ D(θm)
Note that at least one of these limiting coefficients {α }K∗ and {β }K∗ does not vanish,
k k=1 k k=1
since otherwise, we will have
D(θm) (cid:80)K∗ wm|ηm −η∗|+(cid:80)K∗ |wm −w∗| (cid:88)K∗
(cid:0) (cid:1)
1 = lim = lim k=1 k k k k=1 k k = |α |+|β | = 0,
m→∞ D(θm) m→∞ D(θm) k k
k=1
which is a contradiction. Therefore, we constructed some {α }K∗ and β = (β ,...,β ) such
k k=1 1 K∗
that they are not all zero, and
(cid:88)K∗
(cid:104) (cid:105) p(x|θm)−p∗(x)
α g(x;η∗)+βT∇g(x;η∗) = lim = 0
k k k m→∞ D(θm)
k=1
for almost every x. Note that the preceding display and the weak identifiability condition
together would imply that α = 0, β =⃗0 for all k = 1,...,K∗, which contradicts the fact
k k
(cid:82)
that at least one of the coefficients is not zero. Therefore, (45) holds. Since if 1 |p(x|θ)−
2
p∗(x)|dx = 0, then (cid:80)K∗ w = 1 and by Assumption A3, D(θ) = 0; if D(θ) = 0, it also
k=1 k
implies (cid:80)K∗ w = 1 and from Assumption A3 again we have 1 (cid:82) |p(x|θ) − p∗(x)|dx = 0.
k=1 k 2
(cid:82)
Therefore, 1 |p(x|θ) − p∗(x)|dx = 0 if and only if D(θ) = 0. Because both D(θ) and
2
(cid:82)
1 |p(x|θ)−p∗(x)|dx are continuous in θ, we can further obtain (following the same argument
2
from which we derived (11)) from equation (45) that for each possible θ,
(cid:90)
1
|p(x|θ)−p∗(x)|dx ≳ D(θ). (48)
2
Step 2: Recall that we have already shown in Theorem 2 and Corollary 5 that when
ϕ < (d+1)/2, there are K∗ of the mixing weights w ’s being bounded away from 0, and the
0 j
50rest K −K∗ of the mixing weights w ’s will empty out at the rate (logn)ρ1/n. Without loss
j
of generality, we may assume that the first K∗ of the weights w ’s do not vanish and denote
j
w = (w )K∗ and η = (η )K∗ . Then for any fixed ϕ < (d+1)/2, we have
K∗ j j=1 K∗ j j=1 0
1 (cid:90) (cid:12)(cid:88)K (cid:88)K∗ (cid:12)
d (p(·|w,η),p∗(·)) = (cid:12) w g(x;η )− w∗g(x;η∗)(cid:12)dx
TV 2 (cid:12) j j k k (cid:12)
j=1 k=1
1 (cid:90) (cid:12)(cid:88)K∗ (cid:88)K∗ (cid:12) 1 (cid:88)K
≥ (cid:12) w g(x;η )− w∗g(x;η∗)(cid:12)dx− w
2 (cid:12) j j k k (cid:12) 2 j
j=1 k=1 j=K∗+1
1 (cid:90) (cid:12)(cid:88)K∗ (cid:88)K∗ (cid:12) C(logn)ρ1
≥ (cid:12) w g(x;η )− w∗g(x;η∗)(cid:12)dx−
2 (cid:12) j j k k (cid:12) n
j=1 k=1
C(logn)ρ1
≳ D(w ,η )− .
K∗ K∗
n
where the last step is due to inequality (48). Recall that we have sup |η −η′| ≤ L from
η,η′∈Ω
Assumption A1, which leads to the bound D(w ,η ) ≤ (cid:80)K∗ (w +w∗ +w L) ≤ 2+L.
K∗ K∗ k=1 k K k
Now since
(cid:0) C(logn)ρ1/n(cid:1)2
> 0, we can then get
2C(L+2)(logn)ρ1
d2 (p(·|w,η),p∗(·)) ≳ D2(w ,η )− .
TV K∗ K∗ n
Step 3: In this step, we show that with high probability, the variational posterior distribu-
tion of each non-emptying out mixture component parameter η is concentrated within a
k
neighborhood of η with radius Mn−1/2. Recall that from Corollary 5, for each non-emptying
k
out component k, we have n ≥ c n for some positive constant c . In this step, we may again
(cid:98)k k k
assume without loss of generality that the first K∗ of the weights w ’s do not vanish. Recall
j
that from Theorem 1, we have that
(cid:34) (cid:35)
n
(cid:88) (cid:0) (cid:1)
q (η ) ∝ π(η )exp p ηTT(x )−T (x )−A(η ) ,
(cid:98)η k k k (cid:98)ik k i 0 i k
i=1
where
(cid:80)n
p = n . Recall that η , defined in equation (27), denotes the η that maximizes
i=1 (cid:98)ik (cid:98)k (cid:98)k k
the exponent in the preceding display. We will first show below that
logn
min |η −η∗| ≲ √ (49)
1≤j≤K∗ (cid:98)j k n
(cid:98)k
holds for each 1 ≤ k ≤ K∗.
51From equations (28) and (29), and Assumption A2, we can get
(cid:34) (cid:35)
n
π(η )exp (cid:88) p (cid:0) ηTT(x )−T (x )−A(η )(cid:1) −ℓ (η ) ≥ π(η )exp(cid:104) − bn (cid:98)k (η −η )2(cid:105) ,
k (cid:98)ik k i 0 i k nk (cid:98)k k 2 k (cid:98)k
i=1
which implies that for any positive number C,
(cid:90)
π(η
)(cid:32) a(cid:0) 2Φ(C)−1(cid:1)(cid:33)d/2
k
q (η )dη ≥ inf ,
(cid:98)η k k
k π(η ) b
{|η k−η (cid:98)k|∞≤C(bn (cid:98)k)−1/2} {|η k−η (cid:98)k|∞≤C(bn (cid:98)k)−1/2} (cid:98)k
where Φ denotes the CDF of the standard normal distribution. In particular, by choosing
C = Φ−1(3/4), the integral above can be bounded from below by c (a/2b)d/2, for some
π
positive constant c when n is sufficiently large. Therefore, for constant C = C(d/bc )1/2,
π k k
we have the following inequality,
(cid:90)
q (η )dη ≥ c (a/2b)d/2.
(cid:98)η k k π
k
{|η k−η (cid:98)k|∞≤C kn−1/2}
Wecannowprove(49)bycontradiction. Supposemin |η −η∗ | ≥ max C logn/n1/2
1≤j≤K∗ (cid:98)j k′ 1≤j≤K∗ j
holds for some k′ ≤ K∗, then each η in the set B := (cid:84)K∗ {η : |η −η | ≤ C n−1/2} cannot
j j=1 j (cid:98)j j
be close to η∗ by the triangle inequality, i.e.,
k
C (logn−1)
min |η −η∗| ≥ min (cid:0) |η −η∗ |−|η −η∗|(cid:1) ≥ min j
1≤j≤K∗ j k 1≤j≤K∗ (cid:98)j k′ j (cid:98)k 1≤j≤K∗ n1/2
holds for any η = (η : j ∈ [K∗]) ∈ B. Since from Step 1, we obtain d2 (p(x|w,η),p∗(x)) ≳
j TV
D2(w K∗,η K∗)−2C(L+2)(logn)ρ1/n, which furhter implies
(cid:90) (cid:90)
(cid:0) (cid:1) (cid:0) (cid:1)
h2 p(x|w,η),p∗(x) q(η)dη ≥ d2 p(x|w,η),p∗(x) q(η)dη
(cid:98) TV (cid:98)
ΩK B
(cid:90)
≳ D2(w ,η )q(η)dη −2CL(logn)ρ1/n
K∗ K∗ (cid:98)
B
≳ (logn−1)n−1/2Q(cid:98)(B)−2CL(logn)ρ1/n
≥
(logn−1)n−1/2(cid:0)
c
(a/2b)d/2(cid:1)K∗
−2CL(logn)ρ1/n.
π
This is a contradiction with Lemma 9, which implies the left-hand side of the preceding
display to be at most O (n−1). This proves inequality (49).
P
Lastly, we show that the variational posterior mode η is close to the variational posterior
(cid:98)k
52mean η for each k ∈ [K∗]. In fact, since η is given by
k k
(cid:82)
η π(η )exp{ℓ (η )−ℓ (η )}dη
k k nk k nk (cid:98)k k
η = ,
k (cid:82) π(η )exp{ℓ (η )−ℓ (η )}dη
k nk k nk (cid:98)k k
we may use the Laplace formula again to obtain
(cid:12)(cid:82) (cid:12)
(cid:12) η π(η )exp{ℓ (η )−ℓ (η )}dη η π(η )(cid:12) C
(cid:12) k k nk k nk (cid:98)k k − (cid:98)k (cid:98)k (cid:12) ≤ .
(cid:12) (cid:12) (2π/n k)d 2 (cid:112) I(η (cid:98)k)(cid:12) (cid:12) n k
This inequality combined with (29) leads to |η −η | ≤ C/n ; and the conclusion follows by
(cid:98)k k k
combining this with (49) and using the fact that n ≍ n for k ∈ [K∗].
(cid:98)k
53