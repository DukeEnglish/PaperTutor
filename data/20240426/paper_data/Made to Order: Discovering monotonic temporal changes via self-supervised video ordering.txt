Made to Order: Discovering monotonic temporal
changes via self-supervised video ordering
Charig Yang1, Weidi Xie1,2, and Andrew Zisserman1
1 Visual Geometry Group, University of Oxford
2 CMIC, Shanghai Jiao Tong University
{charig,weidi,az}@robots.ox.ac.uk
https://charigyang.github.io/order/
Abstract. Our objective is to discover and localize monotonic temporal
changes in a sequence of images. To achieve this, we exploit a simple
proxytaskoforderingashuffledimagesequence,with‘time’servingasa
supervisory signal since only changes that are monotonic with time can
giverisetothecorrectordering.Wealsointroduceaflexibletransformer-
basedmodelforgeneral-purposeorderingofimagesequencesofarbitrary
length with built-in attribution maps. After training, the model success-
fully discovers and localizes monotonic changes while ignoring cyclic and
stochastic ones. We demonstrate applications of the model in multiple
video settings covering different scene and object types, discovering both
object-level and environmental changes in unseen sequences. We also
demonstrate that the attention-based attribution maps function as effec-
tive prompts for segmenting the changing regions, and that the learned
representationscanbeusedfordownstreamapplications.Finally,weshow
that the model achieves the state of the art on standard benchmarks for
ordering a set of images.
Keywords: Ordering · Change detection · Self-supervised learning
Fig.1: Localizing monotonic temporal changes. Top: satellite images (ordered
left to right) taken months apart, containing several changes – some are correlated
monotonically with time (such as urbanization), while others are seasonal/cyclic (such
as water level). Bottom: Our model’s attribution map prediction on the sequence is
able to localize the regions with monotonic time-correlated changes (in green), while
being invariant to the seasonal and sporadic changes. The model is trained with no
manual supervision, generalises to unseen sequences (as here), and the attribution map
can also be used as a prompt to obtain segmentation.
4202
rpA
52
]VC.sc[
1v82861.4042:viXra2 C. Yang et al.
1 Introduction
In the image sequence in Figure 1, there exists numerous changes over time,
thoughmanyoftheseareseasonal,andhencedistractingforlong-termmonitoring
applications. As humans, we not only observe what is changing, but also reason
about which changes are correlated monotonically with time and which ones
are not. In this paper, we introduce a new task of automatically identifying
temporally correlated changes in an image sequence, while being invariant to
otherchanges.Morespecificallywewishtodiscoverandlocalizetheimageregions
where the change is correlated with time. Our motivation is to go beyond just
detecting changes, but also discovering what changes are relevant over a period
of time, and to explore the potential applications that this task could enable.
Toachievethis,weproposeaself-supervisedproxytask,withtimeservingasa
supervisory signal: the task is simply to order a shuffled sequence of video frames.
Theinsightisthatifamodelcanorderthevideoframes,itwouldhavelearnedto
identify relevant (monotonic temporal-correlated) cues while disregarding other
changes. The trained model can then be employed for video analysis applications
where the goal is to identifying changes over time, such as developments or
deforestation in satellite imagery (whilst ignoring seasonal variations) (see Figure
1) or aging signs in medical images. It can also be employed for detecting and
tracking monotonic object motion over time, such as shadows caused by the
movement of the sun or animals moving smoothly across the scene.
It is worth noting that temporal ordering has previously been used as a proxy
task for self-supervised representation learning, with the learnt representations
thenfinetunedfordownstreamtasks,suchasvideoactionrecognition[18,39,46,64]
(though due to the disparity between the proxy and downstream tasks, the
effectiveness of learnt visual representation from temporal ordering has been
unsatisfactory compared to other proxy tasks [14,24,27,28]). In contrast, the
objectiveinthispaperistouseorderingasaproxytasktodirectlytrainamodel
for discovering and localizing monotonic changes in video sequences, without any
subsequentsupervisedfinetuning.Inthissense,wearesimilarinspirittoprevious
works that use self-supervision to directly solve tasks, such as [8,30,34,66] that
targets tracking and segmentation by training on proxy tasks.
Inordertoharnesstheorderingproxytask,weintroduceatransformer-based
modelthatisable(i)toperformorderingonnaturalimagessequences,and,more
importantly, (ii) to provide attribution by localizing the evidence that gives rise
to its prediction. Specifically, we use a DETR-like transformer encoder-decoder
architecture where the queries in the decoder are cast as an ordering index. The
architectureisdesignedtoallowaattributionmaptobeobtaineddirectlyaspart
of the proxy training. Once the model has been trained for a particular setting,
such as change detection in satellite image sequences, then it can generalize to
unseen videos in the same setting, requiring only a forward pass to predict the
localization of the monotonic temporal changes from the attribution map.
To summarize, in this paper, we make the following four contributions: (i) we
introduce a new task of discovering and localizing monotonic temporal changes
in image sequences, and use temporal ordering as a self-supervised proxy lossMade to Order 3
for training. (ii) we introduce a flexible transformer-based model that can be
used for ordering images of different sequence lengths, and also for localizing
evidence. (iii) once trained on a setting (such as satellite images), the model is
able to discover the changes correlating monotonically with time in unseen image
sequences in the same setting, and we demonstrate several situations where this
can be applied. Finally, (iv) we demonstrate that the trained model is able to
order novel image sequences surpassing the performance of previous approaches
for ordering on standard benchmarks.
2 Related Work
Video self-supervised learning has become increasingly popular in computer
vision. Most research in this area focuses on representation learning, with an em-
phasis on downstream performance after supervised fine-tuning or linear probing.
In contrast, there is a less explored direction that goes beyond representation
learning to directly learning a useful task under the self-supervised learning
setting, such as depth[22,70],optical flow[41,45],correspondence[63] andsound
localization [2,3,13,40]. Our work in this paper builds upon such paradigm, that
enables to deploy the model to downstream tasks without the need for labels.
Self-supervised learning from time. This work involves using temporal
ordering as a supervisory signal. Alternative sources of supervision is to leverage
othercues,suchasspeed[6],uniformity[67],andthearrowoftime[64].Theclosest
kin to our work involves using temporal sequencing as supervision [18,19,39,46],
though their primary focus is on representation learning. In this work, we focus
on advocating temporal ordering as a useful task on its own, showing that
localization can emerge by using time as supervisory signal. We highlight the
differences between our work and others in the Supplementary Material.
Ordering has been a longstanding task in computer science, dating back to
sorting algorithms. In machine learning, it is a relevant task in both language [15,
16] and vision [4,46,58,71]. For images, ordering has also been treated as a
pretext for self-supervised pretraining, such as jigsaw puzzles [49], or as a task of
interest,forexample,imagesequencing[4,5,31,57,68].Thereisalsosomeinterest
intheliteraturethatfocusesondifferentiablesortingalgorithms[12,17,25,51,52],
though they mostly focus on algorithmic developments, such as differentiable
loss function and black-box combinatorial optimisation. While in this paper, we
make contributions towards the architecture by introducing a transformer-based
ordering model, which allows ordering of arbitrary-length image sequences, with
built-in visualisation via attribution maps.
Attribution localization, specifically in the case where explicit supervision is
not given, has been of interest in the vision community. In ConvNets, attribution
methodsattempttolookintothenetworktofindoutwhereitisseeing[21,69].In
transformers, several methods have been proposed to look into the attention on
the [CLS] tokens [1]. Instead of these implicit localization, several methods have
also carefully designed the architecture so that localization emerges explicitly
despite not being trained on, such as in sound localization [2,3,13,40]. This work
follows the latter paradigm, while using the self-supervision from ordering.4 C. Yang et al.
Fig.2: Network architecture. For an unordered sequence of F frames each with N
patches, the transformer encoder takes in all FN patches as input, and outputs FN
features. The transformer decoder takes in Q learnable queries, each corresponding to
an ordinal position, and the encoder output for cross-attention, resulting in Q features
for output. A FN ×Q cosine similarity matrix is constructed between all pairs of
features from the encoder and decoder outputs, and the spatial max-pooling over this
matrix reveals the F ×Q order predictions. The ordering can simply then be obtained
by taking an argmax along each query axis. In the example sequence, the hour hand is
correlated monotonically with time, and appears in the attribution map.
Change detection has also been studied in computer vision. Many works look
at changes in the image domain [53,55], and across different applications from
constructionmonitoring[59],satelliteimaging[44],tomedicalimaging[50].Other
works associate short-term changes with motion, and use motion as a cue to
discover moving objects [9,10,35,36,65,66]. We differ from these lines of work
in that we are mostly concerned with temporally coherent changes at different
timescales, which may go beyond object level and not associated with motion.
3 Method
3.1 Problem Formulation
Our goal is to train a vision model to localize the changes in an image sequence
that correlate monotonically with time. As a subsidiary goal, the model should
also be able to order the image sequence.
Formally, given set of images, the model should output an attribution map
Satt ∈RF×H×W and an ordering yorder ∈ZF :yorder,i ∈{0,1,2,...,F −1} as:
yorder,Satt =Φ(I 0,I 1,...,I F−1)
where I ∈ RC×H×W represents the input images. We show that we can train
the model Φ via self-supervised learning on a proxy task, namely, ordering an
arbitrary sequence of F images shuffled from a temporal sequence.
3.2 Ordering architecture
To address this problem, we propose a simple yet novel transformer-based ar-
chitecture, as shown in Figure 2. The architecture comprises a transformer
encoder (Φenc) that encodes the image patches, and a transformer decoder (Φdec)Made to Order 5
that encodes the ordering. To obtain an attribution map, we simply compute the
pairwise cosine similarity between features from the encoder and queries from
the decoder. We can then perform a max pooling operation across patches of the
same image to get the ordering prediction.
Transformer Encoder (Φ ). To process an unordered sequence of F images,
enc
i.e., X ={I ,I ,...,I }, we start by dividing each frame I ∈RC×H×W into
0 1 F−1
2D patches of size (P,P), resulting in N =HW/P2 patches per frame and FN
patches in total. Following the vision transformer approach, we flatten each
patch using a learnable projection layer to D dimensions and add 3D positional
encoding (spatial and frame) to each patch.
It is important to note that the frame positional encoding does not contain
absolute temporal information since the frames are unordered, but it allows
the patches to identify whether they belong to the same frame. As a result,
after patchifying the input sequence, it ends up with a tensor of x∈RF×N×D,
which is then fed into a transformer encoder. The key difference to the standard
vision transformer is that we output all the features, i.e., xenc ∈ RF×N×D
instead of using a [CLS] token. In summary, we can express this procedure as
xenc =Φenc(I 0,I 1,...,I F−1).
Transformer Decoder (Φ ). The transformer decoder is composed of Q
dec
learnable queries q ∈ RQ×D, with each corresponding to an ordering position
(0,1,...,Q−1). The task for the transformer decoder is to align the query vector
with the encoder feature that demonstrates the correct temporal order. These
queriesiterativelyattendthevisualoutputsfromtheencoderwithcross-attention
in the standard transformer decoder. We denote the output of the decoder as,
xdec ∈RQ×D =Φdec(q,xenc). In practice, Q=F.
Cosine similarity matrix (S). Recall that we now possess two sets of fea-
tures: encoder features xenc ∈ RF×N×D and decoder features xdec ∈ RQ×D.
We then compute the pairwise cosine similarity matrix S∈RF×N×Q :[S] =
i,j
cos(xenc,i,xdec,j)∈[−1,1] between each i of the F ×N features in xenc and each
j of the Q features in xdec, where cos(·,·) denotes the cosine similarity function.
Given the similarity matrix, we want to obtain (i) the ordering of the frames
and (ii) the attribution map that indicates the spatial evidence within each
frame that gives rise to ordering. The matrix S ∈ RF×N×Q consists of F ×Q
different spatial maps of size N, each indicating the attention between each pair
of queries (j ∈Q) and images (i∈F).
Order prediction. To obtain the order predictions, we perform spatial max-
pooling over the patches of each frame (along the N dimension), to obtain
yˆ∈RF×Q =max S . This max-pooling is designed to create an information
i∈N i
bottleneck – the query has to attend to the correct token(s) within the correct
image in order to predict the order correctly. The resulting matrix serves as a
predictor for the position of each query in the ordering. We then apply a softmax
along the query axis of the matrix to get the probability scores for each query.
Attributionmap.AmongtheF×Qdifferentspatialmaps,weareonlyinterested
in the ones that correspond to the correct ordering. For each query j ∈Q, we
select one map i ∈ F that has the maximum activation, i.e. i = argmaxyˆ
j6 C. Yang et al.
resulting in Q maps. We then rearrange and resize each map of N patches back
to the original resolution, resulting in Satt ∈RQ×H×W. Notably, this localization
can be achieved without the need for additional fine-tuning, supervision, or
post-hoc attribution methods [1,21].
3.3 Training and inference
Temporal loss. Given the ground-truth order y ∈ ZQ : y ∈ {0,1,...,F −
i
1}, the model can be trained via binary cross-entropy loss. Specifically, we
convert the ground-truth order into a matrix where each column is a one-
(cid:0)010(cid:1)
hot vector indicating its position, e.g. (1,0,2) as 100 . With some notation
001
abuse, we still call this matrix y ∈ ZF×Q. The forward loss is then simply
the elementwise binary cross-entropy between the two matrices: L (y,yˆ) =
f
1 (cid:80) (cid:80) cross-entropy(yˆ ,y ).
FQ i∈F j∈Q ij ij
In practice, we find that allowing reversibility in the loss aids with training,
as many changes are reversible in nature without prior knowledge of the arrow of
time[64](thesequencecouldequallybeorderedfromfirsttolast,orlasttofirst).
To allow this, we calculate the loss as the minimum of the loss for both forward
and backward sequences, i.e. L =min(L (y,yˆ),L (reverse(y),yˆ)). The loss is
r f f
minimized when the sequence is in the correct order, regardless of the direction.
Inference. At inference time, we simply take the argmax along each query axis
as the order prediction, that is, yorder ∈ZQ :yorder,j =argmax
i∈F
yˆ
i,j
In other
words, each query picks the image that contains the maximum activation for its
query, as illustrated in the bottom-right corner of Figure 2.
3.4 Discussion
Generalization to different sequence lengths. Our architecture is designed
to handle sequences of arbitrary, possibly unequal length during training and
inference,withouttheneedtore-designortrainseparatemodelsforeachsequence
length. At training time, we assume there is a maximum number of images, thus
initialize a total of Fmax learnable queries in the transformer, i.e., q∈RFmax×D.
WhilethemodelhandlesasequenceofF images,withF ≤F ,itonlyusesthe
max
first F queries as input to the decoder, ignoring the rest. This approach enables
each query to represent its positioning (0,1,...,F −1), making it generalizable to
different lengths during both training and testing. However, the model will not
generalize to lengths above Fmax.
Avoiding trivial solutions. There are two factors that we need to account for:
camera motion and video compression artifacts. Camera motion can be smooth
or uniform over a short time gap, which can result in an uninteresting cue. To
address this, we apply a small random cropping on each frame in settings where
the time gap between frames is small (i.e. < 1s). This slight jittering helps to
prevent the model from learning trivial solutions. We note that this does not
degrade the performance even if camera motion is absent. Another factor that
can give rise to trivial solutions is inter-frame video compression artifacts. To
address this, we follow conventional wisdom [29,64] and use H.264 formatting for
all videos, thus minimizing compression artifacts and preventing trivial solutions.Made to Order 7
Fig.3: Sequence datasets. From left to right: dynamic Random Dot Stereograms
(RDS)(movingdotscoloredonlyforillustration),movingcamouflagedanimals(MoCA),
timelapse clocks (cropped/full), timelapse scenes, MUDS, CalFire, OASIS-3.
From localization to segmentation.Whiletheattributionmapisuseful,some
applications may benefit from going beyond just localization. Here, we propose
two solutions. We can directly obtain segmentation at patch-level granularity by
thresholding the attribution map, together with minimal post-processing namely
averaging across frames and removing small contours. We note that this method
is crude as it does not distinguish pixels within a patch. Alternatively, we can
use the highest activation points (i.e. centre pixels of patches) as prompt for the
Segment Anything model (SAM) [33], to obtain pixel-level segmentation masks.
We use the pretrained SAM model without fine-tuning.
4 Experiments
4.1 Video datasets
To leverage the inherent temporal information in videos, specifically, we sample a
sequence from a video, shuffle the frames, then train the model with the original
ordering as groundtruth. This approach enables the attribution map to identify
temporally coherent changes that contribute to the ordering while disregarding
otherchanges.Ourstudyexploresvarioussequencetypesacrossmultipledomains,
each with distinct cues of interest, which we summarize in Table 1 and illustrate
with sample sequences in Figure 3.
Dynamic random dot stereograms are a type of image sequence that fea-
tures a box of random dots moving smoothly over a background of random
dots, originally used as a pair to demonstrate stereoscopic motion [47]. While
the individual images may appear random, the box is visible when viewed in
sequence. We generate a synthetic dataset of controllable dynamic random dot
stereograms (Dynamic RDS) to test the model’s ability to detect subtle relative
cues. Since this a synthetic dataset, we know the ground-truth of the box’s
motion, so can compare this with the predictions.
Moving camouflaged animals (MoCA) [36] was constructed from videos of
camouflaged animals. We use this dataset to investigate the use of short-term
object locomotion as a cue, particularly where it is challenging to distinguish
the object from the background. To accomplish this, we follow [66] and focus
on the subset of 88 videos in which the animals are in motion. To evaluate on
localisation, we assume that the change is object-level due to animal motion, and
use the annotated object bounding box as the ground-truth for localization.8 C. Yang et al.
dataset ∆t cue seq (trn/test) EM EW
Dynamic RDS <1s motion ∞ 99.8 99.9
MoCA <1s motion 75/13 82.0 90.6
Clocks (cropped) ∼1m clock 2011/500 62.5 73.0
Clocks (full) ∼1h clock/scene 210/20 55.0 74.3
Timelapse scenes ∼1h scene 130/50 61.8 79.4
MUDS 1mo landscape 60/20 56.4 69.6
CalFire 1mo landscape 800/276 76.6 87.5
OASIS-3 1y brain 100/34 84.3 89.1
Table 1: Dataset attributes and ordering results. This table shows different
datasets and their attributes, as well as the ordering results on the test (unseen)
sequences on exact match (EM) and elementwise (EW) metrics.
Timelapse analog clocks. Our study examines real-world scenes that feature
both absolute cues (time on clocks) and relative cues (scene changes). To accom-
plish this, we utilize the Timelapse dataset [67] in two ways. Firstly, we use the
entire dataset of 2,511 videos that features cropped clocks. Secondly, we create
a subset consisting of 260 outdoor scenes with static cameras where the clock
occupies only a small area of the scene.
Timelapse scenes. We have gathered an outdoor dataset of timelapse videos
from various online sources. The dataset comprises 180 static videos, and our
aim is to investigate the cues that the model can extract from the scene to learn
its order, as there are no specific absolute cues present.
4.2 Temporal image sequences
We also show effectiveness of our approach for image sequences that are collected
over a period of time, including satellite images and longitudinal medical data.
Multi-temporal urban development dataset (MUDS)[61]isadatasetthat
contains 80 aerial satellite image sequences captured monthly over a two-year
period. We aim to identify geographical changes that occur over time, including
deforestation and urbanization, while ignoring other changes. As there are no
existing datasets and benchmarks for this task, we hand-label 60 sub-sequences
on the test set for segmentation masks where changes are monotonic, and call
this evaluation set Monotonic MUDS. Samples are shown in Figure 4a. We
use this dataset to evaluate localization and segmentation performance of the
model trained on the MUDS dataset.
CalFire [43] is a satellite dataset tracking wildfires in California. It also contains
other events, such as snowfall, new construction, changes in water level, that we
aim to discover. We pre-process by removing scenes with significant cloud cover.
OASIS-3 [37] contains longitudinal MRI scans taking 1-4 years apart to study
how brains age. We select sequences with 3 or more scans, resulting in 134
sequences. Following [31], we perform affine registration and use the centre slice.Made to Order 9
(a) Monotonic MUDS benchmark. (b) Image ordering datasets.
Fig.4: (a) To evaluate localization and segmentation performance, we manually anno-
tate the monotonically changing regions (shown in yellow) on the MUDS test set. Each
sequence contains four frames, and the monotonic changes between the first and last
frames are annotated. (b) 4-digit MNIST [38] (left) and SVHN [48] (right). The task is
to order the images by the numbers they contain in increasing order (top to bottom).
4.3 Image ordering datasets
Inaddition,weshowcaseourgeneralorderingcapabilitybyevaluatingourmethod
on standard benchmarks. We compare our ordering performance with related
works [17,25,51,52] on the task of sorting images of numbers in ascending order,
as shown in Figure 4b. What the model has to learn here is different from the
previous datasets, as the ordering is absolute, and not understanding changes.
Multi-digit MNIST [38] dataset is a modified version of the MNIST dataset
in which four digits are concatenated to form a four-digit number. The goal is
to order the image sequences in increasing order. To construct this dataset, we
synthetically combine examples from the corresponding train and test sets of the
MNIST dataset, resulting in a total of 50,000 training and 10,000 testing images.
Street view house numbers (SVHN) [48], was collected from Google Street
View and includes images of house numbers. Similarly, the task is to order these
numbers in increasing order. The dataset consists of 33,402 images for training
and 13,068 for testing. To ensure consistency with previous studies, we followed
the data preprocessing and augmentation methods described in [23].
4.4 Evaluation metrics
Localization. We use a pointing-game evaluation method, this is to follow the
convention of the localization literature in other domains, including audio-visual
localization[2,3]andsaliencymethods[20,21],thatis,ifthepixelwithmaximum
activation in the attribution map contains the change of interest, then it is
positive (1), otherwise it is negative (0). As our method only outputs patch-level
attribution,wesimplyselectthecentrepixelofthepatchasthehighestactivation.
The overall accuracy is then calculated as the average over all sequences.
Segmentation. We use the standard metric for segmentation, i.e., mean inter-
section over union (mIoU), where the mean is the average across all sequences.
Ordering. We use the evaluation metrics for sorting as outlined in [51,52].
These metrics include exact match (EM) and elementwise (EW) accuracy. EM is
considered correct if the entire sequence is ordered correctly, while EW considers
theorderaccuracyperelement.Followingpreviousbenchmarksweevaluatethese10 C. Yang et al.
Fig.5: Ordering and Localization resultsacrossvariousdatasets,wherethemodel
is able to discover and localize various cues across different domains, including object
motion, clocks, scenery, landscape and biological aging. The left column shows the
input (unordered) images. Each column of the similarity matrix represents the model’s
predictionofeachindividualorder(0,1,2,3),wheretheimageintheredboxischosen,
and the attention heat map within the box localizes the change.
at sequence lengths 5, 9 and 16. To test the generalization to different sequence
lengths, we also evaluate the exact match accuracy at a fixed sequence length of
5 at test-time (EM5), regardless of the training sequence length.
4.5 Implementation details
Wespliteachdatasetintodisjointtrainingandtestingsubsets,andthenrandomly
sample frames from each video. We keep the time gap between sampled frames
constant within each video, but vary this across videos to train a robust model.
For image sequences (MUDS, CalFire, OASIS-3) where data collection is less
regular, we relax these constraints and simply randomly sample between all
frames within the sequence. We train each model separately for each dataset.
Architecture. For the encoder, we use a smaller version of TimeSFormer [7]
with a divided space-time attention architecture, consisting of 256 dimensions, 4
heads, 6 layers, and 512 MLP dimensions. For the decoder, we use a standard
transformer decoder with the same parameters, except for 64 dimensions and 3
layers. As a result, the model is lightweight, with only 4M parameters. We use
Adam optimizer [32] with learning rate 1e-4 in all experiments, and batch size
32 sequences with 4 frames per sequence for all video datasets, except 3 frames
for OASIS-3 as this is the minimum MRI scan sessions per subject. For image
ordering, we use batch size 100 with varying numbers of frames. All experiments
are run on a single GPU. The code, datasets, and models will be released.
5 Results
We present ordering results in Section 5.1. Then, as our work cuts across several
domains, we consider several avenues for comparison. In Section 5.2, we compare
our method with existing change detection methods, and show that discovering
monotonic temporal changes is unattainable by previous methods. In Section 5.3,
we compare with other self-supervised proxy tasks and show that (i) our method
works better than previous works in change localization, and (ii) our method
learns better representations that can be finetuned to downstream tasks. Lastly,
we show in Section 5.4 that our model can serve as a general-purpose ordering
method that outperforms existing methods in image ordering benchmarks.Made to Order 11
(a) SAM Prompting. (b) Failure cases.
Fig.6: (a) The attribution map is used to prompt SAM to obtain segmentation masks.
(b) Unorderable sequences, one being too static and the other being too stochastic.
5.1 Results on ordering video frames or image sequence
Video ordering results.Theresultsfororderingaswellasthemainsignalsthat
the model can pick up, are shown by EM and EW in Table 1, with qualitative
results in Figure 5. The model is able to successfully order sequences across
differentdatasets,particularlyincaseswherechangesaresignificant.Itisexpected
that the scores are not perfect, as sampled data from videos is not guaranteed
to contain ordering cues thus rendering the sequence simply unorderable, as
illustrated in the Supplementary Material.
Temporal image sequence ordering results. For satellite image sequences,
the model is able to discover cues that are relevant to ordering, including road
building and forest fires. This illustrates our model’s application on remote
sensing imagery. In MRI scans, we explore the cues for age changes. Our results
show that there are cues in the posterior part of the brain. This is consistent
with the literature [11] that suggest that ventricular enlargement is a prominent
feature, and causes the posterior horn to inflate in response to tissue loss. There
are also some cues along the outline. This is in line with the literature [56,60],
which suggest that brain volume also decreases with old age.
Failure cases. A limitation of our model is that we do not force a one-to-one
matching between queries and images, and this may result in some images being
claimed by multiple different queries or by none at all, as seen in Figure 6b.
This problem can easily be resolved by allowing each image to be predicted once.
However, not being able to order also provides valuable information as not all
real sequences can be ordered – for example, sequences where everything is static
for a period, or very stochastic. Therefore, we treat invalid orderings as a means
to provide information on whether particular frames can be ordered or not.
5.2 Comparison with change detection methods
We compare against three other previous approaches for their ability to detect
monotonic temporal changes on the Monotonic MUDS dataset, namely, a su-
pervised Siamese Networks [26] trained on MUDS dataset for the task of urban
development tracking, by highlighting the differences between buildings; and the
Change Vector Analysis [42], which is a baseline for the task of change detection,
highlighting all changes in an image pair without knowledge their nature. Deep12 C. Yang et al.
Fig.7: Segmentation comparison with other methods: Siamese networks [26] and
CVA [42]. Supervised methods ignore changes other than building, and pixel-based
methods over-segments non-monotonic regions.
Mono-MUDS RDS MoCA
Method loc (acc) ↑ seg (mIoU) ↑ seg (mIoU) ↑loc (acc) ↑
Siamese [26] 73.3 11.1 – –
CVA [42] 71.7 34.6 22.3 69.6
DCVA [54] 70.0 35.5 – –
Ours 83.3 37.9 / 45.1 (SAM) 34.2 75.0
Table 2: Localization and segmentation results via the pointing game accuracy
for localization, and mIoU for segmentation. The methods for segmentation (patch and
SAM)aredescribedinSection3.4.Methodslabelledas“−” areonlytrainedonsatellite
images, hence do not generalize to other domains.
Change Vector Analysis [54] is a learned version of the above CVA that has been
trained specifically on satellite images.
The performance comparison is given in Table 2, and illustrated in Figure 7.
As can be seen, the urban development tracking method [26] under-segments
changes that are correlated with urbanisation, as it is only trained to look
at buildings. The change detection methods of [42,54] over-segments changes
that are non-monotonic as it has no concept of time. Both prior methods have
shortcomings in detecting urban development: the former method (and even the
ground-truthprovidedwiththeoriginaldataset)misseschangeslikeroadbuilding
and deforestation, and the latter includes many erroneous regions such as the
seasonal changes in vegetation and water level. Our model is able to highlight
correctly the monotonic changes while being invariant to other changes. We
furthernotethatourmodeldiscovers suchchangeswithoutanypriorinformation
on what to look for. We also evaluate on two other datasets: RDS and MoCA,
where we have ground-truth for the moving objects in the video; and show that
we obtain favourable results.
Quantitatively, in Table 2, we find that (i) our pointing-game localization
andpatch-levelsegmentationresultsoutperformothermethodsdespiteoperatingMade to Order 13
methods loc (acc) ↑ seg (mIoU) ↑ finetuning (F1) ↑
scratch — — 18.2
S&L [46] 23.3 20.9 15.8
OPN [39] 25.0 24.1 17.1
AoT [64] did not converge —
Ours (AoT proxy) 63.3 26.9 25.5
Ours 83.3 37.9 / 45.1 (SAM) 30.2
Table 3: Comparison with other self-supervised methods (left) on localization
and segmentation on Monotonic MUDS. (right) on fine-tuning performance on building
change detection on MUDS.
only on patch-level (7×7) and not pixel-level granularity, and (ii) segmentation
via SAM prompting further improves the results.
Qualitatively, the results of our localization experiments on various video
datasets are presented in Figure 5. The model is capable of accurately identifying
monotonic changes while remaining invariant to unrelated cues. Notably, these
results were achieved on sequences unseen during training. We additionally show
that our localization map works as a good prompt for the Segment Anything
(SAM) model to obtain object-level changes, as in Figure 6a.
5.3 Comparison on self-supervised proxy tasks
Here, we compare to other self-supervised methods based on time. Please refer
to the Supplementary Material for a discussion on the subtle differences between
the proxy tasks. We include results for training baselines from scratch on MUDS,
and testing on Mono-MUDS in localizing and segmenting monotonic changes.
The results are shown in Table 3, where we conduct three sets of experiments, as
detailed below.
First, we observe that previous methods are extremely crude in localization;
this is expected, as they are all based on conv5 feature (even AoT, via CAM).
Given a 224p input, conv5 has a 13×13 feature map with 195p receptive field.
Our architecture handles localization by design, and is hence more capable
than other methods that use post-hoc attribution methods on top of standard
backbones. We also note that AoT does not train, which is also expected as it
ingests optical flow as input (and in satellite images using flow does not make
sense (change̸=motion)), our method is more flexible in this regard.
Second, we ask if our method is still superior if the architectural gap is closed.
Toachievethis,wealsocomparetheproxytaskinAoT(timedirection)withours
(ordering) under a fairer comparison (using our architecture and RGB input),
and show this in the table under “Ours (AoT proxy)”. We conclude that both
our architecture and proxy task leads to significant improvements.
Third, we investigate representation learning for a target task that requires
both time and localization: change detection of buildings on satellite images
(like [26]). For each method, we pre-train the encoder on MUDS, freeze it, then
train a lightweight head (3× deconvolutions) on top of the same dataset, and14 C. Yang et al.
dataset MNIST SVHN
frames 5 9 16 5 9 16
DSort[51] 83.4|92.6 56.3|86.7 30.5|80.7|86.6 64.1|82.1 24.2|69.6 3.9|59.6|66.8
DSv2[52] 84.9|93.2 63.8|89.1 31.1|82.2|—– 68.5|84.1 39.9|75.8 12.2|65.6|—–
Ptr-Net[62] 91.9|95.6 87.7|95.0 68.9|90.0|1.1 76.3|87.6 48.7|79.4 9.8|63.2|0.1
Ours 93.9|96.7 87.9|95.2 72.2|91.2|92.9 77.3|88.2 53.9|81.0 19.4|67.9|67.6
Table 4: Ordering on image datasets on two standard benchmarks (MNIST and
SVHN) where the task is to order images of numbers in increasing order. Metrics are
(EM|EW|EM5). EM and EW are evaluated at the sequence length the model has been
trained on (5/9/16), whereas EM5 tests generalisation to test length 5.
test on unseen sequences. To keep this fair, we keep the number of parameters
roughly the same across methods. The results (Table 3, right column) show that
our method learns good representations as compared to previous self-supervised
methods in localization tasks.
5.4 Comparison with image ordering methods
We compare to two previous methods on image ordering benchmarks where the
task is to arrange the images in increasing order.
Differentiable sorting networkssuchasDiffSort[51]anditssuccessorDSortv2[52]
employ a parameter-free sorting network to rank scalars. We compare with these
sortingmodels,andshowthatourmodeliscapableasageneralorderingnetwork
with added functionality.
Pointer Networks [62] ranks features by using a recurrent encoder-decoder
network with attention. We note that Ptr-Net is not initially designed for such a
task, but for arranging a set of coordinates. We simply extend pointer networks
by adding an image encoder and task the model to rank the image features from
small to large. We then jointly train this encoder and the pointer network. For
fair comparison, we use the same transformer encoder as our model, and use the
pointer network as the decoder with similar size to our transformer decoder.
The quantitative results are presented in Table 4. Our results demonstrate
that (i) we compare favourably in ordering performance – on both MNIST and
SVHN, our method has the best performance of the four (ii) Ptr-Net does not
automatically generalise to testing with different sequence lengths, while our
method does (as reflected by the poor EM5 accuracy), and (iii) our method also
has the added benefit of having an attribution map.
6 Conclusion
In this paper, we explore using time as a proxy loss for self-supervised training of
models to discover and localize monotonic temporal changes in image sequences.
Possible extensions include discovering more complex temporal changes (sea-
sonal/periodic),orobjectstateandattributechanges.Itwouldalsobeinteresting
to investigate how the model scales with larger datasets and compute, and what
new applications this task can enable. Overall, we hope this paper presents a
valuable starting point for future research and applications in this area.Made to Order 15
Acknowledgements. We thank Tengda Han, Ragav Sachdeva, and Aleksandar
Shtedritski for suggestions and proofreading. This research is supported by the
UK EPSRC CDT in AIMS (EP/S024050/1), and the UK EPSRC Programme
Grant Visual AI (EP/T028572/1).
References
1. Abnar,S.,Zuidema,W.:Quantifyingattentionflowintransformers.arXivpreprint
arXiv:2005.00928 (2020) 3, 6
2. Afouras, T., Owens, A., Chung, J.S., Zisserman, A.: Self-supervised learning of
audio-visual objects from video. In: European Conference on Computer Vision
(ECCV) (2020) 3, 9
3. Arandjelovic, R., Zisserman, A.: Objects that sound. In: European Conference on
Computer Vision (ECCV) (2018) 3, 9
4. Basha, T., Moses, Y., Avidan, S.: Photo sequencing. In: European Conference on
Computer Vision (ECCV) (2012) 3
5. Basha, T.D., Moses, Y., Avidan, S.: Space-time tradeoffs in photo sequencing. In:
Proceedings of the IEEE International Conference on Computer Vision (ICCV)
(December 2013) 3
6. Benaim,S.,Ephrat,A.,Lang,O.,Mosseri,I.,Freeman,W.T.,Rubinstein,M.,Irani,
M., Dekel, T.: Speednet: Learning the speediness in videos. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020) 3
7. Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for
video understanding? In: Proceedings of the International Conference on Machine
Learning (ICML) (July 2021) 10, 7
8. Bian, Z., Jabri, A., Efros, A.A., Owens, A.: Learning pixel trajectories with multi-
scale contrastive random walks. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (2022) 2
9. Bideau, P., Learned-Miller, E.: A detailed rubric for motion segmentation. arXiv
preprint arXiv:1610.10033 (2016) 4
10. Bideau, P., Learned-Miller, E.: It’s moving! a probabilistic model for causal motion
segmentation in moving camera videos. In: European Conference on Computer
Vision (ECCV) (2016) 4
11. Blinkouskaya,Y.,Weickenmeier,J.:Brainshapechangesassociatedwithcerebralat-
rophyinhealthyagingandalzheimer’sdisease.FrontiersinMechanicalEngineering
(2021) 11
12. Brown, A., Xie, W., Kalogeiton, V., Zisserman, A.: Smooth-ap: Smoothing the
path towards large-scale image retrieval. In: European Conference on Computer
Vision (ECCV) (2020) 3
13. Chen,H.,Xie,W.,Afouras,T.,Nagrani,A.,Vedaldi,A.,Zisserman,A.:Localizing
visual sounds the hard way. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (2021) 3
14. Chen,T.,Kornblith,S.,Norouzi,M.,Hinton,G.:Asimpleframeworkforcontrastive
learning of visual representations. In: Proceedings of the International Conference
on Machine Learning (ICML) (2021) 2
15. Chen, X., Qiu, X., Huang, X.: Neural sentence ordering. arXiv preprint
arXiv:1607.06952 (2016) 3
16. Cui, B., Li, Y., Chen, M., Zhang, Z.: Deep attentive sentence ordering network.
In: Proceedings of the Conference on Empirical Methods in Natural Language
Processing (2018) 316 C. Yang et al.
17. Cuturi, M., Teboul, O., Vert, J.P.: Differentiable ranking and sorting using optimal
transport. Advances in Neural Information Processing Systems (2019) 3, 9
18. Fernando,B.,Bilen,H.,Gavves,E.,Gould,S.:Self-supervisedvideorepresentation
learning with odd-one-out networks. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (2017) 2, 3
19. Fernando, B., Gavves, E., Oramas, J.M., Ghodrati, A., Tuytelaars, T.: Modeling
video evolution for action recognition. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 5378–5387 (2015) 3
20. Fong, R., Patrick, M., Vedaldi, A.: Understanding deep networks via extremal
perturbationsandsmoothmasks.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition (2019) 9
21. Fong, R.C., Vedaldi, A.: Interpretable explanations of black boxes by meaningful
perturbation. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (2017) 3, 6, 9
22. Godard,C.,MacAodha,O.,Firman,M.,Brostow,G.J.:Diggingintoself-supervised
monocular depth estimation. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision (2019) 3
23. Goodfellow, I.J., Bulatov, Y., Ibarz, J., Arnoud, S., Shet, V.: Multi-digit number
recognition from street view imagery using deep convolutional neural networks.
arXiv preprint arXiv:1312.6082 (2013) 9
24. Grill, J.B., Strub, F., Altché, F., Tallec, C., Richemond, P.H., Buchatskaya, E.,
Doersch,C.,Pires,B.A.,Guo,Z.D.,Azar,M.G.,Piot,B.,Kavukcuoglu,K.,Munos,
R., Valko, M.: Bootstrap your own latent: A new approach to self-supervised
learning. In: Advances in Neural Information Processing Systems (2021) 2
25. Grover, A., Wang, E., Zweig, A., Ermon, S.: Stochastic optimization of sorting
networks via continuous relaxations. arXiv preprint arXiv:1903.08850 (2019) 3, 9
26. Hafner,S.,Ban,Y.,Nascetti,A.:Urbanchangedetectionusingadual-tasksiamese
network and semi-supervised learning. In: IGARSS 2022-2022 IEEE International
Geoscience and Remote Sensing Symposium (2022) 11, 12, 13
27. Han,T.,Xie,W.,Zisserman,A.:Self-supervisedco-trainingforvideorepresentation
learning. In: Advances in Neural Information Processing Systems (2020) 2
28. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are
scalablevisionlearners.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition (2022) 2
29. Iashin,V.,Xie,W.,Rahtu,E.,Zisserman,A.:Sparseinspaceandtime:Audio-visual
synchronisation with trainable selectors. arXiv preprint arXiv:2210.07055 (2022) 6
30. Jabri, A., Owens, A., Efros, A.A.: Space-time correspondence as a contrastive
random walk. Advances in Neural Information Processing Systems (2020) 2
31. Kim, H., Sabuncu, M.R.: Learning to compare longitudinal images. arXiv preprint
arXiv:2304.02531 (2023) 3, 8
32. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014) 10
33. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything.
arXiv:2304.02643 (2023) 7
34. Lai, Z., Lu, E., Xie, W.: Mast: A memory-augmented self-supervised tracker.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (2020) 2
35. Lamdouar, H., Xie, W., Zisserman, A.: Segmenting invisible moving objects. In:
British Machine Vision Conference (2021) 4Made to Order 17
36. Lamdouar,H.,Yang,C.,Xie,W.,Zisserman,A.:Betrayedbymotion:Camouflaged
object discovery via motion segmentation. In: Proceedings of the Asian Conference
on Computer Vision (2020) 4, 7
37. LaMontagne,P.J.,Benzinger,T.L.,Morris,J.C.,Keefe,S.,Hornbeck,R.,Xiong,C.,
Grant,E.,Hassenstab,J.,Moulder,K.,Vlassenko,A.G.,etal.:Oasis-3:longitudinal
neuroimaging,clinical,andcognitivedatasetfornormalagingandalzheimerdisease.
MedRxiv (2019) 8
38. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to
document recognition. Proceedings of the IEEE (1998) 9
39. Lee,H.Y.,Huang,J.B.,Singh,M.,Yang,M.H.:Unsupervisedrepresentationlearning
by sorting sequences. In: Proceedings of the IEEE International Conference on
Computer Vision (ICCV) (2017) 2, 3, 13
40. Liu, J., Ju, C., Xie, W., Zhang, Y.: Exploiting transformation invariance and
equivariance for self-supervised sound localisation. In: Proceedings of the ACM
International Conference on Multimedia (2022) 3
41. Liu, P., Lyu, M., King, I., Xu, J.: Selflow: Self-supervised learning of optical flow.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (2019) 3
42. Malila, W.A.: Change vector analysis: An approach for detecting forest changes
with landsat. In: LARS symposia (1980) 11, 12
43. Mall, U., Hariharan, B., Bala, K.: Change event dataset for discovery from spatio-
temporal remote sensing imagery. Advances in Neural Information Processing
Systems (2022) 8
44. Mall, U., Hariharan, B., Bala, K.: Change-aware sampling and contrastive learning
for satellite images. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (2023) 4
45. Meister, S., Hur, J., Roth, S.: Unflow: Unsupervised learning of optical flow with
a bidirectional census loss. In: Proceedings of the AAAI conference on artificial
intelligence (2018) 3
46. Misra, I., Zitnick, C.L., Hebert, M.: Shuffle and learn: unsupervised learning using
temporal order verification. In: European Conference on Computer Vision (ECCV)
(2016) 2, 3, 13
47. Neff,R.,Schwartz,S.,Stork,D.G.:Electronicsforgeneratingsimultaneousrandom-
dot cyclopean and monocular stimuli. Behavior Research Methods, Instruments, &
Computers (1985) 7
48. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A.Y.: Reading digits
in natural images with unsupervised feature learning (2011) 9
49. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving
jigsaw puzzles. In: European Conference on Computer Vision (ECCV) (2016) 3
50. Patriarche,J.,Erickson,B.:Areviewoftheautomateddetectionofchangeinserial
imaging studies of the brain. Journal of digital imaging (2004) 4
51. Petersen, F., Borgelt, C., Kuehne, H., Deussen, O.: Differentiable sorting networks
forscalablesortingandrankingsupervision.In:InternationalConferenceonMachine
Learning (ICML) (2021) 3, 9, 14
52. Petersen,F.,Borgelt,C.,Kuehne,H.,Deussen,O.:Monotonicdifferentiablesorting
networks. arXiv preprint arXiv:2203.09630 (2022) 3, 9, 14
53. Sachdeva, R., Zisserman, A.: The change you want to see. In: Proceedings of the
IEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV)(2023)
418 C. Yang et al.
54. Saha, S., Bovolo, F., Bruzzone, L.: Unsupervised deep change vector analysis for
multiple-change detection in vhr images. IEEE Transactions on Geoscience and
Remote Sensing 57(6), 3677–3693 (2019) 12
55. Sakurada, K., Okatani, T.: Change detection from a street image pair using cnn
featuresandsuperpixelsegmentation.In:BritishMachineVisionConference(2015)
4
56. Scahill, R.I., Frost, C., Jenkins, R., Whitwell, J.L., Rossor, M.N., Fox, N.C.: A
longitudinal study of brain volume changes in normal aging using serial registered
magnetic resonance imaging. Archives of neurology 60(7), 989–994 (2003) 11
57. Sevilla-Lara, L., Zha, S., Yan, Z., Goswami, V., Feiszli, M., Torresani, L.: Only
time can tell: Discovering temporal data for temporal modeling. In: Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision (2021) 3
58. Shvetsova, N., Petersen, F., Kukleva, A., Schiele, B., Kuehne, H.: Learning by
sorting: Self-supervised learning with group ordering constraints. International
Conference on Computer Vision (ICCV) (2023) 3
59. Stent, S., Gherardi, R., Stenger, B., Cipolla, R.: Detecting change for multi-view,
long-term surface inspection. In: British Machine Vision Conference (2015) 4
60. Svennerholm, L., Boström, K., Jungbjer, B.: Changes in weight and compositions
of major membrane components of human brain during the span of adult human
life of swedes. Acta neuropathologica (1997) 11
61. Van Etten, A., Hogan, D., Manso, J.M., Shermeyer, J., Weir, N., Lewis, R.:
The multi-temporal urban development spacenet dataset. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021) 8
62. Vinyals, O., Fortunato, M., Jaitly, N.: Pointer networks. Advances in Neural Infor-
mation Processing Systems (2015) 14, 3
63. Wang,X.,Jabri,A.,Efros,A.A.:Learningcorrespondencefromthecycle-consistency
of time. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (2019) 3
64. Wei,D.,Lim,J.J.,Zisserman,A.,Freeman,W.T.:Learningandusingthearrowof
time. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (2018) 2, 3, 6, 13
65. Xie, J., Xie, W., Zisserman, A.: Segmenting moving objects via an object-centric
layered representation. In: Advances in Neural Information Processing Systems
(2022) 4
66. Yang,C.,Lamdouar,H.,Lu,E.,Zisserman,A.,Xie,W.:Self-supervisedvideoobject
segmentation by motion grouping. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision (2021) 2, 4, 7
67. Yang,C.,Xie,W.,Zisserman,A.:It’sabouttime:Analogclockreadinginthewild.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (2022) 3, 8
68. Zarrabi,N.,Avidan,S.,Moses,Y.:Crowdcam:Dynamicregionsegmentation.arXiv
preprint arXiv:1811.11455 (2018) 3
69. Zhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,Torralba,A.:Learningdeepfeatures
for discriminative localization. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (2016) 3
70. Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and
ego-motionfromvideo.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition (2017) 3
71. Zhukov, D., Alayrac, J.B., Laptev, I., Sivic, J.: Learning actionness via long-range
temporal order verification. In: European Conference on Computer Vision (ECCV)
(2020) 3Made to Order 1
The Supplementary Material includes the following sections:
– Sect. A: Datasets in more detail, where the datasets used in the paper
are explained further.
– Sect. B: Code, where we present the pseudocode alongside the attached
code.
– Sect. C: Related work in more detail, where we compare our method
against previous ones and highligh the differences.
– Sect. D: Unorderable sequences, where we discuss the model’s handling
of unorderable sequences.
– Sect. E: Experimental settings, where we perform variations on the
train/test set division.
– Sect. F: Ablation studies,whereweperformvariationsonthearchitecture.
– Sect. G: Qualitative results, for both orderable and unorderable cases.
A Datasets in more detail
Table 5 expands on the main paper’s dataset attributes (Main paper, Table 1) to
give more detailed dataset statistics.
dataset resolutionpatch size# patches# train# test
MNIST 28×112 7 4×16 50000 10000
SVHN 54×54 6 9×9 33402 13068
Dynamic RDS 42×42 7 6×6 ∞ ∞
MoCA 336×336 21 16×16 75 13
Clocks (cropped) 196×196 14 14×14 2011 500
Clocks (full) 320×480 20 16×24 210 50
Timelapse scenes 336×336 21 16×16 130 50
MUDS 196×196 7 28×28 60 20
CalFire 336×336 21 16×16 800 276
OASIS-3 224×224 16 14×14 50 19
Table 5: Dataset attributes. The different video datasets used, and their attributes
in detail.2 C. Yang et al.
B Code
The pseudocode is shown below, with the full code being available on the project
webpage.
class Made2Order(nn.Module):
def __init__(self):
self.query = nn.Parameter([1,q,d]).repeat(b,1,1) # (q=queries, b=bsz)
def forward(self, video):
# input video of dimensions: b f c h w (f=frames, chw=image dimensions)
x = patch_posemb(video) # b (f n) d (n=number of tokens)
x_enc = self.TransformerEncoder(x) # b (f n) d
x_dec = self.TransformerDecoder(self.query, x_enc) # b q d
x_enc = F.normalize(x_enc, 2)
x_dec = F.normalize(x_dec, 2)
S = torch.einsum(‘bik,bjk->bij’, x_enc, x_dec) # b (f n) q
return einops.reduce(S, ‘b (f n) q -> b f q’, ‘max’)
C Related work in more detail
C.1 Self-supervised learning from time
S&L[46] OPN[39] AoT[64] Ours
Input RGB RGB Flow RGB
Filtering Flow Flow Flow no
Evidence post-hoc(pool5) post-hoc(pool5) post-hoc(CAM) built-in
Trainingtaskismiddleframein-betweenshuffleandorderforwardorbackwardshuffleandorder
Goal rep.learning rep.learning rep.learning+apps changelocalization
Table 6: Comparison to related works.Thistablecomparesthispaperwithseveral
relatedworks,namelyShuffleandLearn[46],OrderPredictionNetwork[39],andArrow
of Time [64]. Unlike previous work where the goal was self-supervised representation
learning, our goal is to directly discover the change and predicts its localization.
On motivation. The main goal of Shuffle and Learn [46] and extensions such as
Order Prediction Network [39] is representation learning using self-supervision
as pre-training – the authors had the intuition that change was required and
used a filter for this based on optical flow to select suitable frame-sets in order
to learn good representations. Arrow of Time [64], too, focuses substantially on
representation learning, though based on optical flow as input, and they too filter
away sequences that lack motion using optical flow.
On attribution learning. All existing methods apply attribution methods
post-hoc (S&L and OPN look at the activation on the spatial pooling after conv5Made to Order 3
and AoT uses CAM on conv5 features). As mentioned in the main paper, both
of these post-hoc methods have low accuracy as they rely on 7x7 conv5 features
with very large receptive field. In contrast, we make a contribution in designing
an architecture that has attribution built-in, enabling more precise attribution
(please refer to the respective figures in each paper – Fig. 4 of [46], Fig. 8 of [39]
and Fig. 4 of [64]), and no need for post-processing.
On pretext task. AoT asks the model to distinguish between forward and
reverse order of an otherwise in-order sequence, while S&L fixes the start and
end frames and asks if the middle frame is inside (see Sec. 3.2 of their paper for
details). For our task it does not matter if the sequence is forwards or backwards
– but that is the only goal of AoT, and the AoT goal would not be possible
with shuffled frames. Our task is identical to OPN where the entire sequence is
shuffled and the ordering is predicted, but we add (i) built-in attribution, and (ii)
a more flexible architecture capable of general ordering across a variable number
of frames. Without all these three add ons, our work would essentially boil down
to the same as that of OPN.
C.2 Ordering methods
The other ordering architectures are shown in Figure 8, and their comparison is
shown in Table 7. Our architecture allows for all of (i) ordering of absolute cues
(e.g. images of numbers) (ii) ordering of relative cues (e.g. in natural images),
(iii) ordering with flexible length during training and testing, and (iv) explicit
localization via attribution map. Note that Ptr-Net is not initially designed for
such a task, and we extend the architecture while preserving fair comparison.
DSort Ptr Ours
Absolute order ✓ ✓ ✓
Relative order ✗ ✓ ✓
Flexible length ✓ ✗ ✓
Localization ✗ ✗ ✓
Fig.8: Architectures of related works Table 7: Related works. We show
for ordering, including Differentiable Sort- DiffSort[51],andPointerNetworks[62]
ing [51] and Pointer Networks [62]. are less versatile.
D Unorderable sequences
In real-world scenarios, sequences may not contain monotonic ordering cues
for every frame (such as sequences that are static over several frames, or ones4 C. Yang et al.
where changes are cyclic), and hence it is impossible to order such sequences
using monotonic changes alone – we denote such sequences as unorderable. We
also observe that the model sometimes makes predictions that are inconsistent
by predicting some ordering indices multiple times and others not at all. This
happens when some frames are being claimed by multiple queries and others
claimed by none at all. Here, we investigate the correlation between these two
occurrences and ask whether the model is able to correctly identify unorderable
sequences by making inconsistent predictions.
To do this, we create an unorderable set by including static frames on Mono-
MUDS datset, resulting in 60 unorderable sequences (in addition to the 60
orderable ones) of 4 frames each. We experiment with these 120 sequences, and
report the results in Table 8. We show that there is a strong correlation between
the model predicting inconsistent ordering and the sequence being unorderable.
We also note that this occurs as a result of a design during inference, hence
training is unaffected.
In summary: an inconsistent prediction implies that the set of frames cannot
be ordered, and the model is able to flag such sequences. In the case of videos,
the ground-truth ordering is known, so we are able to easily detect when the
prediction is incorrect or inconsistent. It is also possible to look at the (lack of)
attention in the attribution map to observe the lack of ordering cues. We also
show qualitative examples of these cases in Section G.2.
correctincorrectinconsistent
gt orderable 88.3 10.0 ↑ 1.7 ↓
gt unorderable 0.0 1.7 ↓ 98.3 ↑
Table 8: Unorderable sequences. For an image sequence (0,1,2,3), the model either
makescorrect(0,1,2,3),incorrecte.g.(0,1,3,2),orinconsistente.g.(0,1,3,3)predictions.
We investigate the correlation between the model making inconsistent predictions and
the sequence being unorderable. The model reliably gives inconsistent predictions for
unorderable sequences.
E Experimental settings
In the main paper, we only explore the setting where the train and test sets are
independent video clips within the same dataset, i.e. Figure 9, left. However,
as a self-supervised task, we can explore other settings that are appropriate for
applications.
Testing on the same clips, but at different times (Figure 9, centre). There
may be cases in remote monitoring and surveillance where there exists past data
thatcanbeusedfortraining,andtheadaptationismostlytowardsnewsequences
that are taken under the same or similar settings. In such cases, we can split the
training and testing sets by time instead of by video clips.Made to Order 5
Fig.9: Experimental settings. The main paper only explores the case where the
train and test videos are separate videos (left), all of the same class. Here we explore
two other settings: testing on the same clips but at different time (centre), and testing
on the training set (right).
Testing on the training set (Figure 9, right). Since no annotations are used,
we can also test directly on the training set as an exploration tool to investigate
what cues are used in determining the order.
Experiment. We conduct an experiment by having a common test set, and
perform training on three different training settings, namely: (i) the training set
does not include any videos from the test set, (ii) the training set includes the
videos from the test set, but they do not overlap in time, and (iii) the training
set includes the entire testing set. To keep the experiments fair we use the same
number of clips for each training setting. We test this on MUDS dataset, and
show the resutls in Table 9. It shows that the accuracy increases as the domain
gap between training and test sets decreases.
setting EM↑ EW↑
Base (split by video) 45.1 52.1
Testing on same clips, different time 49.7 59.6
Testing on training set 88.5 91.3
Table 9: Experiment on different settings. This table shows the accuracy for the
commontestsetinMUDSdataset.Metricsareexactmatchandelementwiseaccuracies
(higher is better).
F Ablation studies
In this section, we conduct ablation experiments on different aspects of the
architectural design. As we introduce a new model in order to solve a new task,
there are many variables that are interesting to investigate.
F.1 Transformer Architecture
To investigate the effect of model size on the ordering performance, we exper-
iment by varying the sizes of the encoder and decoder. We do this by vary-6 C. Yang et al.
ing the numbers of (layers/feature dim/heads/feedforward dim) or both the
encoder and decoder (in PyTorch’s nn.Transformer, they are referred to as
(num_layers/d_model/nhead/dim_feedforward). Feature dimensions refers to
the number of features in self-attention or cross-attention within each trans-
former layer, while the feedforward dimension refers to the dimensionality of the
feedforward network applied after attention within each transformer layer.
experiment Encoder Decoder #params EM↑ EW↑ EM5↑
Base(small) 6/256/4/512 3/64/4/512 3.6M 53.9 81.0 78.0
increaseencodersize 2xbase base 19.4M 62.9 85.1 83.8
increasedecodersize base 2xbase 6.4M 53.9 81.5 78.7
increaseboth 2xbase 2xbase 22.2M 62.7 84.9 83.0
decreaseencodersize 0.5xbase base 816k 27.2 68.8 62.9
decreasedecodersize base 0.5xbase 3.3M 51.7 80.1 76.8
decreaseboth 0.5xbase 0.5xbase 515k 25.6 68.0 61.8
Table 10: Ablation studies. We perform ablation studies varying the archi-
tecture of our model. The numbers for the encoder/decoder are (layers/feature
dim/heads/feedforward dim). All ablations are relative to the base size, for exam-
ple, 2x base refers to (12/512/8/1024). We evaluate on SVHN dataset with 9 frames.
Base corresponds to the version implemented in the main paper.
The results are shown in Table 10. We can see from the results that the
encoder size has a significant impact on the accuracy, while the decoder size does
not matter as much. This is interesting, and we think that this arises from the
fact that the encoder is doing most of the work, both in feature extraction and
in ordering. We further investigate this in Section F.3.
F.2 Spatial feature map size
We also experiment varying the spatial feature map size, to look at the trade-off
between the granularity of localization and ordering performance. Small patch
sizes require significantly higher memory consumption.
experiment patchsize #patches EM↑ EW↑ EM5↑
Base(M) 6 9×9 53.9 81.0 78.0
S 3 18×18 47.5 78.2 74.9
L 9 6×6 54.4 81.5 79.8
XL 18 3×3 44.1 76.1 73.1
Table 11: Patch size ablation. We perform ablation studies varying the patch size.
We evaluate on SVHN dataset with 9 frames. Base is used in the main paper.
AsshowninTable13,whilesmallpatchesaregenerallygoodforvisualisation,
as the changes will be more finely localized, it does come with a trade-off that
ordering accuracy drops when the patches are too small.Made to Order 7
F.3 Encoder attention
We also look at the choice of attention used in the encoder. In particular, in the
model of the main paper we use divided space-time attention in the transformer
encoder so that tokens of the same spatial position between frames are allowed
to communicate.
The benefit of allowing communication between the frames in the encoder is
that the ordering can be done within the encoder itself, easing the load on the
decoder.Withoutthis,theencoderwillmerelyactasafeatureextractor,andthe
decoderwillhavetofindawaytorankthesefeatures.Anotherbenefitparticularly
in video ordering is that the patches with the same spatial position across the
frames are able to communicate and compare with each other, highlighting the
differences more easily.
In this section we also experiment with two other settings, where (i) attention
is restricted to within each frame, and (ii) all tokens are allowed to communicate
witheachother(wereferthereadertoFigure2of[7]forillustration).Weevaluate
both on image set ordering (SVHN) and video temporal ordering (MUDS).
experiment GPU mem (GB)↓ SVHN↑ MUDS↑
Base (divided space-time) 8.0/5.3 53.9 | 81.0 56.4 | 69.6
Space only 8.1/5.7 55.8 | 82.1 13.0 | 35.8
Full attention 14.8/9.2 51.5 | 80.6 7.6 | 30.7
Table 12: Encoder attention ablation. We perform ablation studies varying the
encoder attention. We evaluate on SVHN and MUDS with 9 and 4 frames respectively.
Metricsare(EM|EW).WealsoshowtheGPUmemoryusageineachrespectivedataset
(SVHN/MUDS), and show that full attention significantly consumes more memory.
As shown in Table 12, we observe that the results vary significantly with the
datasets. In the case of image set ordering on SVHN, inter-frame communication
matters less presumably because there is no differences between patches of the
same spatial position to highlight, unlike the case of video temporal ordering. We
also observe that space-only attention (6 spatial layers) obtains slightly higher
accuracy than that of divided space-time attention (3 spatial layers, 3 temporal
layers), which reinforces the hypothesis that spatial attention within the image
is more important than across images.
In the case of ordering video frames (MUDS), we found that (i) space-only
attention hinders the model’s ability to communicate with each other, and (ii)
full attention is significantly more difficult to train due to a significantly larger
number of tokens to attend to.
F.4 Number of frames
In this section, we investigate the trade-off for increasing the number of frames
in a sequence. As shown in Table 13, having more frames allows the cues to be8 C. Yang et al.
highlighted more clearly, and allows the model to learn better what changes are
correlated with time and what are not. However, the number of possible orders
will also increase combinatorially which makes the task more difficult.
experiment EM↑ EW↑
Base (4) 62.5 73.0
2 51.3 51.3
3 65.2 73.1
5 37.1 55.0
Table 13: Frame count ablation. We perform ablation studies varying the number
of frames. We evaluate on Timelapse clocks (cropped).
F.5 Choice of objective function
We have two different loss functions (forward and reversible). Consider a scene
of a sky where the sun is moving upwards. Without accounting for reversibility,
it is difficult to distinguish between a sunrise and a sunset in reverse, hence
unidirectional ordering becomes challenging, sometimes impossible. Since our
goal is only to attribute the change, not to determine the forward or backwards
direction, our intuition is that considering reversibility in the sequence allows
the model to be trained better, as this prevents giving the model confusing
training signals. In this case we find that the model is (i) harder to train and (ii)
susceptible to overfitting if we do not allow reversibility in the model, especially
in datasets where changes are reversible.
Table 14 experimental results for the MoCA dataset as an ablation. From the
table, the results show that using reversible losses helps with both (i) training
and (ii) generalisation.
reversible? loss↓ train EM↑ train EW↑ val EM↑ val EW↑
yes 0.119 84.2 91.5 82.0 90.6
no 0.169 76.0 83.9 44.5 53.5
Table 14: Objective function ablation. Ablation on choice of objective function.
We investigate whether the reversible loss helps with training and generalization. We
evaluate on MoCA dataset.Made to Order 9
G Qualitative Results
G.1 More qualitative results
We show more qualitative results in Figure 10. The model is able to find different
cues and use them for ordering.
Fig.10:Qualitativeresults.Weshowmorequalitativeresultsacrossdifferentdatasets.
From top to bottom row: Camouflaged animals: the model uses motion as a cue in
order to localize the moving subject; Timelapse clocks: the model correctly localizes
the clock hands; Timelapse scenes: the model uses either the color of the sky or the
objects such as clouds in ordering; Satellite images: the model is able to identify and
localize structural changes in landscape such as new buildings and cleared land while
being invariant to seasonal changes.10 C. Yang et al.
G.2 Unorderable sequences
We show qualitative examples of unorderable sequences in Figure 11. As we
simply sample frames from video clips, there will be some sequences that cannot
be ordered as they do not contain ordering cues. In particular, this can happen
when: (i) there is insufficient change; (ii) changes are stochastic; or (iii) changes
are seasonal (cyclic).
Fig.11:Unorderablesequences.Thefigureshowsexamplesofunorderablesequences
across different datasets. Those selected are very reasonable: most contain insufficient
cuesfororderingtobedetermined,asthecuesareeither(i)toostatic,(ii)toostochastic,
or (iii) uncorrelated with forward time (e.g. seasonal).