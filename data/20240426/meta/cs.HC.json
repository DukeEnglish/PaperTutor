[
    {
        "title": "Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class",
        "authors": "Mazda MoayeriMichael RabbatMark IbrahimDiane Bouchacourt",
        "links": "http://dx.doi.org/10.1145/3630106.3659039",
        "entry_id": "http://arxiv.org/abs/2404.16717v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16717v1",
        "summary": "Vision-language models enable open-world classification of objects without\nthe need for any retraining. While this zero-shot paradigm marks a significant\nadvance, even today's best models exhibit skewed performance when objects are\ndissimilar from their typical depiction. Real world objects such as pears\nappear in a variety of forms -- from diced to whole, on a table or in a bowl --\nyet standard VLM classifiers map all instances of a class to a \\it{single\nvector based on the class label}. We argue that to represent this rich\ndiversity within a class, zero-shot classification should move beyond a single\nvector. We propose a method to encode and account for diversity within a class\nusing inferred attributes, still in the zero-shot setting without retraining.\nWe find our method consistently outperforms standard zero-shot classification\nover a large suite of datasets encompassing hierarchies, diverse object states,\nand real-world geographic diversity, as well finer-grained datasets where\nintra-class diversity may be less prevalent. Importantly, our method is\ninherently interpretable, offering faithful explanations for each inference to\nfacilitate model debugging and enhance transparency. We also find our method\nscales efficiently to a large number of attributes to account for diversity --\nleading to more accurate predictions for atypical instances. Finally, we\ncharacterize a principled trade-off between overall and worst class accuracy,\nwhich can be tuned via a hyperparameter of our method. We hope this work spurs\nfurther research into the promise of zero-shot classification beyond a single\nclass vector for capturing diversity in the world, and building transparent AI\nsystems without compromising performance.",
        "updated": "2024-04-25 16:29:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16717v1"
    },
    {
        "title": "Benchmarking Mobile Device Control Agents across Diverse Configurations",
        "authors": "Juyong LeeTaywon MinMinyong AnChangyeon KimKimin Lee",
        "links": "http://arxiv.org/abs/2404.16660v1",
        "entry_id": "http://arxiv.org/abs/2404.16660v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16660v1",
        "summary": "Developing autonomous agents for mobile devices can significantly enhance\nuser interactions by offering increased efficiency and accessibility. However,\ndespite the growing interest in mobile device control agents, the absence of a\ncommonly adopted benchmark makes it challenging to quantify scientific progress\nin this area. In this work, we introduce B-MoCA: a novel benchmark designed\nspecifically for evaluating mobile device control agents. To create a realistic\nbenchmark, we develop B-MoCA based on the Android operating system and define\n60 common daily tasks. Importantly, we incorporate a randomization feature that\nchanges various aspects of mobile devices, including user interface layouts and\nlanguage settings, to assess generalization performance. We benchmark diverse\nagents, including agents employing large language models (LLMs) or multi-modal\nLLMs as well as agents trained from scratch using human expert demonstrations.\nWhile these agents demonstrate proficiency in executing straightforward tasks,\ntheir poor performance on complex tasks highlights significant opportunities\nfor future research to enhance their effectiveness. Our source code is publicly\navailable at https://b-moca.github.io.",
        "updated": "2024-04-25 14:56:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16660v1"
    },
    {
        "title": "Comparing Continuous and Retrospective Emotion Ratings in Remote VR Study",
        "authors": "Maximilian WarsinkeTanja KojićMaurizio VergariRobert SpangJan-Niklas Voigt-AntonsSebastian Möller",
        "links": "http://arxiv.org/abs/2404.16487v1",
        "entry_id": "http://arxiv.org/abs/2404.16487v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16487v1",
        "summary": "This study investigates the feasibility of remote virtual reality (VR)\nstudies conducted at home using VR headsets and video conferencing by deploying\nan experiment on emotion ratings. 20 participants used head-mounted displays to\nimmerse themselves in 360{\\deg} videos selected to evoke emotional responses.\nThe research compares continuous ratings using a graphical interface to\nretrospective questionnaires on a digitized Likert Scale for measuring arousal\nand valence, both based on the self-assessment manikin (SAM). It was\nhypothesized that the two different rating methods would lead to significantly\ndifferent values for both valence and arousal. The goal was to investigate\nwhether continuous ratings during the experience would better reflect users'\nemotions compared to the post-questionnaire by mitigating biases such as the\npeak-end rule. The results show significant differences with moderate to strong\neffect sizes for valence and no significant differences for arousal with low to\nmoderate effect sizes. This indicates the need for further investigation of the\nmethods used to assess emotion ratings in VR studies. Overall, this study is an\nexample of a remotely conducted VR experiment, offering insights into methods\nfor emotion elicitation in VR by varying the timing and interface of the\nrating.",
        "updated": "2024-04-25 10:19:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16487v1"
    },
    {
        "title": "CoCoG: Controllable Visual Stimuli Generation based on Human Concept Representations",
        "authors": "Chen WeiJiachen ZouDietmar HeinkeQuanying Liu",
        "links": "http://arxiv.org/abs/2404.16482v1",
        "entry_id": "http://arxiv.org/abs/2404.16482v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16482v1",
        "summary": "A central question for cognitive science is to understand how humans process\nvisual objects, i.e, to uncover human low-dimensional concept representation\nspace from high-dimensional visual stimuli. Generating visual stimuli with\ncontrolling concepts is the key. However, there are currently no generative\nmodels in AI to solve this problem. Here, we present the Concept based\nControllable Generation (CoCoG) framework. CoCoG consists of two components, a\nsimple yet efficient AI agent for extracting interpretable concept and\npredicting human decision-making in visual similarity judgment tasks, and a\nconditional generation model for generating visual stimuli given the concepts.\nWe quantify the performance of CoCoG from two aspects, the human behavior\nprediction accuracy and the controllable generation ability. The experiments\nwith CoCoG indicate that 1) the reliable concept embeddings in CoCoG allows to\npredict human behavior with 64.07\\% accuracy in the THINGS-similarity dataset;\n2) CoCoG can generate diverse objects through the control of concepts; 3) CoCoG\ncan manipulate human similarity judgment behavior by intervening key concepts.\nCoCoG offers visual objects with controlling concepts to advance our\nunderstanding of causality in human cognition. The code of CoCoG is available\nat \\url{https://github.com/ncclab-sustech/CoCoG}.",
        "updated": "2024-04-25 10:10:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16482v1"
    },
    {
        "title": "The Impact of Social Environment and Interaction Focus on User Experience and Social Acceptability of an Augmented Reality Game",
        "authors": "Lorenzo CocchiaMaurizio VergariTanja KojicFrancesco VonaSebastian MollerFranca GarzottoJan-Niklas Voigt-Antons",
        "links": "http://arxiv.org/abs/2404.16479v1",
        "entry_id": "http://arxiv.org/abs/2404.16479v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16479v1",
        "summary": "One of the most promising technologies inside the Extended Reality (XR)\nspectrum is Augmented Reality. This technology is already in people's pockets\nregarding Mobile Augmented Reality with their smartphones. The scientific\ncommunity still needs answers about how humans could and should interact in\nenvironments where perceived stimuli are different from fully physical or\ndigital circumstances. Moreover, it is still being determined if people accept\nthese new technologies in different social environments and interaction\nsettings or if some obstacles could exist. This paper explores the impact of\nthe Social Environment and the Focus of social interaction on users while\nplaying a location-based augmented reality game, measuring it with user\nexperience and social acceptance indicators. An empirical study in a\nwithin-subject fashion was performed in different social environments and under\ndifferent settings of social interaction focus with N = 28 participants\ncompiling self-reported questionnaires after playing a Scavenger Hunt in\nAugmented Reality. The measures from two different Social Environments (Crowded\nvs. Uncrowded) resulted in statistically relevant mean differences with\nindicators from the Social Acceptability dimension. Moreover, the analyses show\nstatistically relevant differences between the variances from different degrees\nof Social Interaction Focus with Overall Social Presence, Perceived\nPsychological Engagement, Perceived Attentional Engagement, and Perceived\nEmotional Contagion. The results suggest that a location-based AR game played\nin different social environments and settings can influence the user\nexperience's social dimension. Therefore, they should be carefully considered\nwhile designing immersive technological experiences in public spaces involving\nsocial interactions between players.",
        "updated": "2024-04-25 10:04:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16479v1"
    }
]