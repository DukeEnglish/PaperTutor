[
    {
        "title": "Estimating the Number of Components in Finite Mixture Models via Variational Approximation",
        "authors": "Chenyang WangYun Yang",
        "links": "http://arxiv.org/abs/2404.16746v1",
        "entry_id": "http://arxiv.org/abs/2404.16746v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16746v1",
        "summary": "This work introduces a new method for selecting the number of components in\nfinite mixture models (FMMs) using variational Bayes, inspired by the\nlarge-sample properties of the Evidence Lower Bound (ELBO) derived from\nmean-field (MF) variational approximation. Specifically, we establish matching\nupper and lower bounds for the ELBO without assuming conjugate priors,\nsuggesting the consistency of model selection for FMMs based on maximizing the\nELBO. As a by-product of our proof, we demonstrate that the MF approximation\ninherits the stable behavior (benefited from model singularity) of the\nposterior distribution, which tends to eliminate the extra components under\nmodel misspecification where the number of mixture components is\nover-specified. This stable behavior also leads to the $n^{-1/2}$ convergence\nrate for parameter estimation, up to a logarithmic factor, under this model\noverspecification. Empirical experiments are conducted to validate our\ntheoretical findings and compare with other state-of-the-art methods for\nselecting the number of components in FMMs.",
        "updated": "2024-04-25 17:00:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16746v1"
    },
    {
        "title": "Conformalized Ordinal Classification with Marginal and Conditional Coverage",
        "authors": "Subhrasish ChakrabortyChhavi TyagiHaiyan QiaoWenge Guo",
        "links": "http://arxiv.org/abs/2404.16610v1",
        "entry_id": "http://arxiv.org/abs/2404.16610v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16610v1",
        "summary": "Conformal prediction is a general distribution-free approach for constructing\nprediction sets combined with any machine learning algorithm that achieve valid\nmarginal or conditional coverage in finite samples. Ordinal classification is\ncommon in real applications where the target variable has natural ordering\namong the class labels. In this paper, we discuss constructing\ndistribution-free prediction sets for such ordinal classification problems by\nleveraging the ideas of conformal prediction and multiple testing with FWER\ncontrol. Newer conformal prediction methods are developed for constructing\ncontiguous and non-contiguous prediction sets based on marginal and conditional\n(class-specific) conformal $p$-values, respectively. Theoretically, we prove\nthat the proposed methods respectively achieve satisfactory levels of marginal\nand class-specific conditional coverages. Through simulation study and real\ndata analysis, these proposed methods show promising performance compared to\nthe existing conformal method.",
        "updated": "2024-04-25 13:49:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16610v1"
    },
    {
        "title": "Automated Model Selection for Generalized Linear Models",
        "authors": "Benjamin SchwendingerFlorian SchwendingerLaura Vana-Gür",
        "links": "http://arxiv.org/abs/2404.16560v1",
        "entry_id": "http://arxiv.org/abs/2404.16560v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16560v1",
        "summary": "In this paper, we show how mixed-integer conic optimization can be used to\ncombine feature subset selection with holistic generalized linear models to\nfully automate the model selection process. Concretely, we directly optimize\nfor the Akaike and Bayesian information criteria while imposing constraints\ndesigned to deal with multicollinearity in the feature selection task.\nSpecifically, we propose a novel pairwise correlation constraint that combines\nthe sign coherence constraint with ideas from classical statistical models like\nRidge regression and the OSCAR model.",
        "updated": "2024-04-25 12:16:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16560v1"
    },
    {
        "title": "Automating the Discovery of Partial Differential Equations in Dynamical Systems",
        "authors": "Weizhen LiRui Carvalho",
        "links": "http://arxiv.org/abs/2404.16444v1",
        "entry_id": "http://arxiv.org/abs/2404.16444v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16444v1",
        "summary": "Identifying partial differential equations (PDEs) from data is crucial for\nunderstanding the governing mechanisms of natural phenomena, yet it remains a\nchallenging task. We present an extension to the ARGOS framework, ARGOS-RAL,\nwhich leverages sparse regression with the recurrent adaptive lasso to identify\nPDEs from limited prior knowledge automatically. Our method automates\ncalculating partial derivatives, constructing a candidate library, and\nestimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL\nin identifying canonical PDEs under various noise levels and sample sizes,\ndemonstrating its robustness in handling noisy and non-uniformly distributed\ndata. We also test the algorithm's performance on datasets consisting solely of\nrandom noise to simulate scenarios with severely compromised data quality. Our\nresults show that ARGOS-RAL effectively and reliably identifies the underlying\nPDEs from data, outperforming the sequential threshold ridge regression method\nin most cases. We highlight the potential of combining statistical methods,\nmachine learning, and dynamical systems theory to automatically discover\ngoverning equations from collected data, streamlining the scientific modeling\nprocess.",
        "updated": "2024-04-25 09:23:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16444v1"
    },
    {
        "title": "Distributionally Robust Safe Screening",
        "authors": "Hiroyuki HanadaSatoshi AkahaneTatsuya AoyamaTomonari TanakaYoshito OkuraYu InatsuNoriaki HashimotoTaro MurayamaLee HanjuShinya KojimaIchiro Takeuchi",
        "links": "http://arxiv.org/abs/2404.16328v1",
        "entry_id": "http://arxiv.org/abs/2404.16328v1",
        "pdf_url": "http://arxiv.org/pdf/2404.16328v1",
        "summary": "In this study, we propose a method Distributionally Robust Safe Screening\n(DRSS), for identifying unnecessary samples and features within a DR covariate\nshift setting. This method effectively combines DR learning, a paradigm aimed\nat enhancing model robustness against variations in data distribution, with\nsafe screening (SS), a sparse optimization technique designed to identify\nirrelevant samples and features prior to model training. The core concept of\nthe DRSS method involves reformulating the DR covariate-shift problem as a\nweighted empirical risk minimization problem, where the weights are subject to\nuncertainty within a predetermined range. By extending the SS technique to\naccommodate this weight uncertainty, the DRSS method is capable of reliably\nidentifying unnecessary samples and features under any future distribution\nwithin a specified range. We provide a theoretical guarantee of the DRSS method\nand validate its performance through numerical experiments on both synthetic\nand real-world datasets.",
        "updated": "2024-04-25 04:29:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.16328v1"
    }
]