Finetuning Language Models to Emit Linguistic
Expressions of Uncertainty
ArslanChaudhry*,SridharThiagarajan* andDilanGorur
*Equalcontributions
Largelanguagemodels(LLMs)areincreasinglyemployedininformation-seekinganddecision-making
tasks. Despitetheirbroadutility,LLMstendtogenerateinformationthatconflictwithreal-worldfacts,
andtheirpersuasivestylecanmaketheseinaccuraciesappearconfidentandconvincing. Asaresult,
end-usersstruggletoconsistentlyaligntheconfidenceexpressedbyLLMswiththeaccuracyoftheir
predictions,oftenleadingtoeitherblindtrustinalloutputsoracompletedisregardfortheirreliability.
Inthiswork,weexploresupervisedfine-tuningonuncertainty-augmentedpredictionsasamethodto
developmodelsthatproducelinguisticexpressionsofuncertainty. Specifically,wemeasurethecalibration
ofpre-trainedmodelsandfine-tunelanguagemodelstogeneratecalibratedlinguisticexpressionsof
uncertainty. Throughexperimentsonvariousquestion-answeringdatasets,wedemonstratethatLLMs
arewell-calibratedinassessingtheirpredictions,andsupervisedfine-tuningbasedonthemodel’sown
confidenceleadstowell-calibratedexpressionsofuncertainty,particularlyforsingle-claimanswers.
Keywords: Calibration, Uncertainty, Linguistic uncertainty, Fine-tuning
Introduction
Large Language Models (LLMs) are emerging as powerful tools that can absorb internet-scale data
in the parametric knowledge of a neural network (Brown, 2020; Hoffmann et al., 2022; Rae et al.,
2021). These models are the foundational blocks of several conversational agents (Achiam et al.,
2023;Anthropic,2024;Dubeyetal.,2024;Teametal.,2023)thatpeopleareincreasinglyrelyingon
for information seeking and decision making tasks. Owing to their natural language interface, these
models are more easily accessible to and interacted by the general public than any other machine
learning models that existed before1. This widespread utility naturally raises questions around the
truthfulness and factuality of the predictions made by these models.
Despite being state-of-the-art on several natural language processing (NLP) tasks (Brown, 2020;
Team et al., 2023), LLMs occasionally produce incorrect predictions – especially on queries that are
outsidethetrainingdistributionofthemodel. Theseinaccuraciesaretrickytodealwithasmodelsdo
not express uncertainty in their generations, making their statements sound highly confident (Huang
et al., 2023; Ji et al., 2023). As such, the end-users have no way of associating the confidence the
modelisexpressinginitspredictionstoitscorrectness. Thislimitstheutilityofthesemodelsinmany
safety-critical applications, such as medicine (Saab et al., 2024; Thirunavukarasu et al., 2023) and
law (Dahl et al., 2024), and precludes the users from reliably using the predictions made by these
models in many information seeking tasks (Passi and Vorvoreanu, 2022; Vasconcelos et al., 2023).
We illustrate this with a mock scenario, as shown in Figure 1.
In this work, we study how to augment an LLM’s prediction with a linguistic expression of
uncertainty, where an uncertainty expression reflects the likelihood that the model’s predictions
are accurate, aggregated across all samples with similar uncertainty levels. For instance, if a model
1AsevidencedbythesurgeofChatGPTusersintomillionsinthefirstfewweeksofitslaunch.
Correspondingauthor(s):arslanch@google.com
Confidential–GoogleDeepMind,donotshareorforwardexternally.
© 2024GoogleDeepMind.Allrightsreserved
4202
peS
81
]LC.sc[
1v08121.9042:viXraFinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
“Beatrix Potter” Beatrix Potter
“Who wrote The
Railway Children?”
“It is unlikely but it
Edith Nesbit
could be Beatrix Potter”
Agent Human
User
Figure 1 | Motivation: The agent provides an incorrect response to a given query. In the response at the
bottom,however,theagentincludesanuncertaintyexpression. Withoutthisuncertaintyexpression,asseenin
theresponseatthetop,thehumanusermightformanincorrectbeliefabouttheworld. Incontrast,withthe
uncertainty-augmentedresponseatthebottom,thehumanuserispromptedtoconsultadditionalresources,
leadingtoamoreaccurateunderstandingoftheworld.
indicates an uncertainty expression such as ‘it is unlikely’ and this corresponds to approximately 30%
confidence, we expect around 30% of those predictions to be correct. In other words, we aim for the
uncertainty-augmented answers to be well-calibrated (Guo et al., 2017). Previous approaches have
used similar measures to avoid answering uncertain questions (Cole et al., 2023). In contrast, our
focusisonalwaysprovidingananswerwhileappropriatelyconveyingtheassociateduncertainty. This
enablestheend-usertodecidewhethertoactonthemodel’sinformation,requestfurtherclarification
from the model, or consult additional sources (see Figure 1). Additionally, we believe that for many
user-facing applications, conveying model uncertainty linguistically is often more effective than
presenting it numerically. This is because a) linguistic expressions, such as ‘I am certain’ or ‘It is
highly unlikely,’ are more intuitive for humans to interpret than raw numbers like 90% or 10%; b)
linguistic expressions integrate more naturally with predictions; c) language can indicate the source
of uncertainty, whether it arises from the model’s limitations or inherent uncertainty (Hüllermeier
and Waegeman, 2021); and d) linguistic expressions are more adaptable for downstream processing,
such as text-to-speech conversion.
We focus on supervised fine-tuning (SFT) with curated datasets as the primary method for
equipping models with linguistic expressions of uncertainty in their predictions. To achieve this, we
firstassessthemodel’sconfidenceinitspredictionsbyqueryingwhethereachpredictionistrueorfalse.
Theconfidencecorrespondstothenormalized2probabilityassignedtothe‘true’token. Buildingonthe
work of Kadavath et al. (2022), we observe that this self-evaluation score is fairly well-calibrated for
single-claim answers across the pre-trained models we tested (Figures 4 and 7). Moreover, applying
mild post-processing, such as isotonic regression on a small calibration set, achieves near-perfect
calibration of both base and instruction-tuned models across various sizes (Figure 4). Next, we
map the confidence scores to linguistic expressions using the framework from Fagen-Ulmschneider
(2019), which was developed based on a survey where human subjects associated probability ranges
with different uncertainty expressions. These linguistic expressions are then combined with the
corresponding predictions to create a fine-tuning dataset. Figure 2 illustrates the overall process
of curating the finetuning dataset. When fine-tuned on this dataset, the resulting models generate
predictions with well-calibrated linguistic expressions of uncertainty (Figure 5).
Overall, our contributions include,
1. We provide a finetuning receipe for equipping models with linguistic expressions of uncertainty.
2. WepresentthecalibrationplotsofGemini1.0smallandmediumsizedmodelsafterpre-training
and alignment phases on three Q/A datasets – TriviaQA (Joshi et al., 2017), AmbigQA (Min
2Thenormalizationconstantisthesumoftheprobablitiesassignedtothe‘true’and‘false’tokens.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 2FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
et al., 2020) and Truthful QA (Lin et al., 2021).
3. Wefindthatpre-trainedmodelsarebettercalibratedthanmodelsfine-tunedforalignment,and
that calibration improves with larger model sizes, consistent with existing literature (Achiam
et al., 2023; Kadavath et al., 2022).
4. We explore various methods for incorporating linguistic expressions of uncertainty into predic-
tions. Our findings indicate that finetuning with predictions augmented by adding uncertainty
expressions after the actual answer results in the most well-calibrated finetuned models.
Related Work
There is a long body of work which studies the ability of machine learning models to express
uncertainty in their predictions (Gawlikowski et al., 2023; Guo et al., 2017). The two broad sources
ofuncertaintytypicallyinfocusareepistemicuncertainty;i.euncertaintystemmingfromthemodel’s
lack of knowledge, and aleatoric uncertainty, uncertainty as a result of inherent ambiguity in the
task (Hüllermeier and Waegeman, 2021). We focus on only the former in this work. For epistemic
uncertainty, in the case of language models, there have been studies (Desai and Durrett, 2020;
Jiang et al., 2021; Kadavath et al., 2022; Tian et al., 2023) which look at the ability of language
models to assess the confidence in the factuality of their predictions, looking at both in-domain and
out-of-domain datasets. Kadavath et al. (2022) show that pre-trained models are reasonably well
calibrated at predicting whether their own output is factual or not in a few-shot setting. We leverage
theirfew-shotpromptingstrategytoelicitnumericalconfidenceofthemodelintheirownpredictions.
Tianetal.(2023)findthatforinstructiontunedmodels,verbalizedprobabilitiesarebetter-calibrated
than conditional probabilities in a setting when models are prompted to output the uncertainty in
their predictions. We refer the reader to (Geng et al., 2024) for a more comprehensive overview of
methods to elicit epistemic uncertainty.
Lin et al. (2022) introduce the notion of linguistic uncertainty in language models, teaching
a pre-trained GPT-3 model to express its uncertainty linguistically along with its prediction on
mathematics tasks. A key difference of our work from theirs is that our uncertainty targets are
obtained in a pointwise manner, whereas they assign the targets based on average task performance
on questions in the same sub-task. We also study the effect of the placement of the uncertainty
estimate(prefix/suffix)onthequalityoftheestimates. Zhouetal.(2023)studytheeffectofprefixed
expression of uncertainty on a pre-existing language model’s generation, finding that expression of
high certainty hurt the accuracy compared to weaker expressions. Mielke et al. (2022) also work
on making models emit linguistic expression of uncertainty, but their process is a two-stage pipeline
whereaseparatecalibratorpredictsthenumericalprobabilityofcorrectness,andthelanguagemodel
thenaddsalinguisticmarkerbasedonthis. Mostsimilartoourworkis(Bandetal.,2024),whowork
on obtaining calibrated long form predictions from LLMs. Their method of obtaining uncertainty
targets relies on a form of self-consistency extended to long form answers, and they also perform an
additional reinforcement learning step after their supervised finetuning stage. They do not study the
quality effect of position of the uncertainty estimates relative to the answer (prefix/postfixed).
Setup and Method
Weinvestigatethequestion-answering(Q/A)taskwhereanLLM,specificallyGemini1.0,incorporates
expressions of uncertainty while generating answers. In a Q/A task, we work with a dataset D =
{𝑥 𝑖,𝑦 𝑖}𝑛 𝑖=1, consisting of 𝑛 examples where 𝑥 represents questions and 𝑦 represents ground-truth
answers. Thesetsofallquestionsandanswersaredenotedby 𝑋 = {𝑥 𝑖}𝑛
𝑖=1
and𝑌 = {𝑦 𝑖}𝑛 𝑖=1,respectively
Given a question 𝑥, the LLM 𝑀(·) produces a prediction ˆ𝑦 = 𝑀(𝑥) through autoregressive decoding.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 3FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
“What was the Elephant LLM “Joseph Warwick” LLM 0.3 Linguistic “Unlikely” LLM* “It is unlikely that its
Man's real name?” Map Joseph Warwick”
(Optional)
Post-processing
Self-Evaluation Interleaving
(Prompt 1) (Prompt 2)
Figure2 | Finetuningdatasetcurationprocess: HereLLMreferstothelanguagemodelthatweareinterested
infinetuning. LLM∗ referestoanoperationthatmixestheuncertaintyexpressionwiththemodelprediction–it
canbeapromptedlanguagemodel(interleavedcase)orsimplyanoperationwhichprefixes/post-fixesthe
answerwiththeexpressionofuncertainty. Givenaquestionontheleft,theLLMproducesarawprediction
andthencomputesitsownconfidenceonthatprediction. Theconfidencescoreisconvertedtoalinguistic
expressionandaugmentedwiththerawprediction. Prompt1andPrompt2aregivenintheappendix.
The dataset D is divided into three non-overlapping subsets: few-shot (D 𝑓𝑠), calibration (D 𝑐𝑎𝑙), and
training (D 𝑡𝑟). These subsets are used in different phases of finetuning dataset curation – D 𝑓𝑠 is used
forcomputingmodelconfidenceinthepredictions,D 𝑐𝑎𝑙 isusedforfittingtheisotonicregressorthatis
then used for post-processing the confidence scores, and D 𝑡𝑟 is used for finetuning (see Algorithm 1
for more details). For each dataset D, we reserve a separate held-out subset D 𝑡𝑒 for evaluation
purposes.
Method
How can we train models to produce accurate expressions of uncertainty? One approach is few-shot
prompting with uncertainty-augmented examples. However, recent literature suggests that LLMs
struggle to generate well-calibrated linguistic expressions of uncertainty through in-context learning
(Zhou et al., 2023). Additionally, in-context learning through few-shot prompting incurs extra infer-
ence costs for processing prompts with each query, making it less optimal from a latency perspective.
To address these issues, we investigate supervised fine-tuning (SFT) on datasets augmented with
uncertaintyexpressionsasanalternativeapproach. Acriticalaspectofcuratingsuchdatasetsisensur-
ing that the uncertainty markers align with the model’s knowledge about the questions. Inconsistent
uncertainty expressions during fine-tuning can lead to hallucinated expressions during testing.
To ensure linguistic expressions of uncertainty are consistent with the model’s knowledge, we
first obtain confidence scores for model-generated samples through self-evaluation, as described
by Kadavath et al. (2022). Specifically, we use a True/False self-evaluation task, where the model
assesses the correctness of its own predictions (see Prompt 1). With the confidence scores in hand,
we apply isotonic regression (Barlow and Brunk, 1972) using a small calibration set, D 𝑐𝑎𝑙, to achieve
nearly perfect calibration of uncertainty estimates. We then map these calibrated confidence scores
to linguistic expressions of uncertainty based on human perception of uncertainty and probabilities
(Fagen-Ulmschneider, 2019), as detailed in Table 1. The final step involves integrating uncertainty
expressions with model predictions in the dataset. We explore three methods for this integration: (1)
Prefixed: placing the uncertainty expression before the prediction (expression, prediction),
(2)Postfixed: placingtheuncertaintyexpressionaftertheprediction(prediction, expression),
and (3) Interleaved: incorporating the uncertainty expression within the prediction (expression
prediction) using a prompted language model (see Prompt 2). The overall process of dataset
curation is illustrated in Figure 2 and Algorithm 1. After curating the dataset D𝑀, we fine-tune the
model (𝑀) to generate linguistic expressions of uncertainty.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 4FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
“Joseph Merrick”
LME
(Reference answer)
(Prompt 4)
LLM*
“Joseph Warwick” False
“It is highly unlikely that the LLM*
name is Joseph Warwick” Brier
Linguistic
“Highly unlikely” 0.09
Disentangling Map
(Prompt 3)
Figure 3 | Evaluationprocess: FinetunedLLMproducesananswerwiththeexpressionofuncertiantyonthe
leftthatissplitbyapromptedLLM∗ intotherawanswer(‘JosephWarwick’)andexpressionofuncertainty
(‘Highlyunlikely’)usingPrompt3. LLM∗thenjudgesthecorrectnessoftherawanswerusingtheLMEPrompt4
anduncertaintyexpressionisconvertedtoafloatequaltotheaverageoftheprobabilityrangetheuncertainty
expressionbelongsto. Basedonthecorrectnessanduncertaintyscore,thefinalmetriciscomputed.
Evaluation
We compute the accuracy (Acc) of a prediction by language model evaluation (LME) whereby a
promptedlanguagemodelcomparesthepredictionwiththegroundtruthanswer(seePrompt 4). To
measure calibration, we plot calibration charts between confidence scores and accuracy (Figure 4)
wherethex-axisisbinnedaccordingtotheprobabilityrangesinthelinguisticexpressionsmap(Fagen-
Ulmschneider, 2019). Further, to summarize the calibration error into a single scalar, we track,
Expected Calibration Error (ECE): weighted average of the difference between the confidence
assigned to examples in the bin, and accuracy of the predictions in the bin.
|𝐵|
∑︁ |𝐵 𝑚| (cid:12) (cid:12)
𝑒𝑐𝑒 = (cid:12)Acc(𝐵 𝑚)−C(𝐵 𝑚)(cid:12),
𝑛
𝑚=1
where |𝐵| are the total bins,
Brier Score: mean squared error between the confidence and correctness verdicts across all
examples
𝑛
brier = 1 ∑︁(cid:0) C(𝑥 𝑖,ˆ𝑦 𝑖)−LME(𝑦,ˆ𝑦)(cid:1)2.
𝑛
𝑖=1
The lower these scores the better.
Once the models are finetuned to generate linguistic expressions of uncertainty, we test the
calibration of their predictions on held-out test sets for each dataset. For this, we first extract
the uncertainty expression from the rest of the prediction using a prompted language model (see
Prompt 3). We then convert these uncertainty expressions into probability estimates using the same
mapping employed for converting probabilities into linguistic expressions in the previous section. We
measurecalibrationusingExpectedCalibrationError(ECE)andBrierScoreandplotcalibrationcharts
based on these probability estimates. The complete evaluation procedure is outlined in Figure 3 and
Algorithm 2.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 5FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
Small | ECE: 0.111 | Brier: 0.138 Small IT | ECE: 0.120 | Brier: 0.141 Medium | ECE: 0.068 | Brier: 0.104 Medium IT | ECE: 0.069 | Brier: 0.108
1.0 1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin Confidence Bin
Small | ECE: 0.010 | Brier: 0.122 Small IT | ECE: 0.015 | Brier: 0.115 Medium | ECE: 0.018 | Brier: 0.096 Medium IT | ECE: 0.022 | Brier: 0.093
1.0 1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin Confidence Bin
(a)Small (b)Small-IT (c)Medium (d)Medium-IT
Figure 4 | TriviaA Calibration Chart: The top-row shows raw calibration scores at temperature=1.0
without any post-processing. The bottom row shows post-processed calibration scores with isotonic
regression. In each plot, the x-axis is the 𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒) of the generated prediction (shown here as
Confidence Bin) and y-axis is probability of that prediction being actually correct (shown here as
Accuracy). Expected Calibration Error (ECE) and Brier Score are reported at the top of each plot.
The error bars show the variance of accuracy in each bin.
Experiments
Datasets
We use standard Q/A datasets – TriviaQA (Joshi et al., 2017), AmbigQA (Min et al., 2020) and
TruthfulQA (Lin et al., 2021). For TriviaQA, we use the wikipedia version. From the train split,
we take 16 examples for the D 𝑓𝑠, 2000 examples for the D 𝑐𝑎𝑙, and remaining examples for the D 𝑡𝑟.
The test set (D 𝑡𝑒) is taken from the dev set. For AmbigQA, we only take the unambiguous set, as we
are focused only on conveying epistemic uncertainty. From the train split, we take 16 examples
for the D 𝑓𝑠, 1000 examples for the D 𝑐𝑎𝑙, and remaining examples for the D 𝑡𝑟. The test set (D 𝑡𝑒) is
taken from the validation set. For TruthfulQA, we split the validation set into a D 𝑓𝑠 of size
8, D 𝑐𝑎𝑙 of size 100, D 𝑡𝑒 of size 100 and D 𝑡𝑟 of size ~600. The prediction is deemed correct if the
LME (Prompt 4) judges it to be similar to any of the possible ground truth answers. We evaluate the
accuracyoftheinterleaving(Prompt2),disentangling(Prompt3),andLME(Prompt4)promptson
a uniformly sampled subset. We manually verify the performance of these prompts on small subsets
of the data, and find that all of these independent tasks are performed at high accuracy, exceeding
95%.
Models
We conduct experiments with the Gemini 1.0 family of models (Team et al., 2023). Specifically, we
usetwovariants,referredtoasSmallandMedium,whichrepresentdifferentarchitecturesizes. Our
study includes both the pre-trained models and those that have been post-trained (aligned). The
post-trained models are denoted with the ‘IT’ marker.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 6
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccAFinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
Small | Postfixed | ECE: 0.040 | Brier: Small IT | Postfixed | ECE: 0.019 | Medium | Postfixed | ECE: 0.022 | Brier:
0.117 Brier: 0.108 0.092
1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin
Small | Postfixed | ECE: 0.067 | Brier: Small IT | Postfixed | ECE: 0.100 | Medium | Postfixed | ECE: 0.061 | Brier:
0.194 Brier: 0.197 0.198
1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin
(a)Small (b)Small-IT (c)Medium
Figure 5 | Calibration Charts of Finetuned Models: Top-row is TriviaQA. Bottom-row is AmibQA.
The model generates post-fixed uncertainty expressions. The x-axis is the 𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒) obtained by
convertingthelinguisticexpressionofuncertaintytoafloatusingTable1(shownhereasConfidence
Bin) and y-axis is probability of that prediction being actually correct (shown here as Accuracy).
No post-processing is done on the 𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒). Expected Calibration Error (ECE) and Brier Score are
reported at the top of each plot. The error bars show the variance of accuracy in each bin.
Finetuning Details
For each question, we independently generate four model samples with a temperature setting of 1.0.
We ensure balance in the curated dataset by setting a maximum number of examples per probability
bin (see Table 1). Although the curated datasets contain model-generated samples with probability
expressions, filtering predictions based on their correctness relative to the ground truth did not yield
a significant performance difference. As a result, we do not apply correctness-based filtering to the
datasets in the experiments discussed in this paper. The statistics for the final curated datasets are
provided in Table 2.
We use a batch size of 32 and train on each dataset for 3 epochs. The Adam optimizer (Kingma,
2014) with a cosine learning rate schedule is employed, where the learning rate is first linearly
warmed up to 5𝑒−7 and then decayed to 5𝑒−8.
Results
CalibrationofGeminimodelsontheself-evaluationtasks: Figure4presentsthecalibrationcharts
for Gemini 1.0 models on the TriviaQA dataset. Several key observations can be made:
1. The Gemini base models exhibit good calibration on the self-evaluation task (Prompt 1).
Confidential–GoogleDeepMind,donotshareorforwardexternally. 7
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccAFinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
Small | Prefixed | ECE: 0.120 | Brier: Small | Postfixed | ECE: 0.040 | Brier: Small | Interleaved | ECE: 0.188 |
0.214 0.117 Brier: 0.245
1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin
Small IT | Prefixed | ECE: 0.069 | Small IT | Postfixed | ECE: 0.019 | Small IT | Interleaved | ECE: 0.092 |
Brier: 0.165 Brier: 0.108 Brier: 0.160
1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin
Medium | Prefixed | ECE: 0.080 | Brier: Medium | Postfixed | ECE: 0.022 | Brier: Medium | Interleaved | ECE: 0.077 |
0.128 0.092 Brier: 0.115
1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin
(a)Prefixed (b)Postfixed (c)Interleaved
Figure 6 | TriviaQA uncertainty augmentation method: Top-row is Gemini Small, middle-row
is Gemini Small-IT and the bottom row is Gemini Medium. The models generate post-fixed
uncertaintyexpressions. Thex-axisisthe 𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒) obtainedbyconvertingthelinguisticexpression
ofuncertaintytoafloatusingTable 1(shownhereasConfidence Bin)andy-axisisprobabilityof
that prediction being actually correct (shown here as Accuracy). No post-processing is done on the
𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒). Expected Calibration Error (ECE) and Brier Score are reported at the top of each plot.
The error bars show the variance of accuracy in each bin.
2. Calibration improves with larger model sizes, and pre-trained models demonstrate better
calibration than instruction-tuned models, aligning with findings reported in the literature for
other LLMs (Achiam et al., 2023; Kadavath et al., 2022).
3. Post-processing confidence scores using isotonic regression leads to significantly improved
calibration.
Results for AmbigQA and TruthfulQA are shown in Figures 7 and 8, respectively. Notably, Gemini
models do not achieve good calibration on TruthfulQA using the self-evaluation task. Although
Confidential–GoogleDeepMind,donotshareorforwardexternally. 8
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccAFinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
post-processing improves calibration somewhat, it is insufficient for effective uncertainty-augmented
dataset curation. Consequently, we exclude the TruthfulQA dataset from our fine-tuning process.
Calibration of finetuned models: We now examine the calibration of linguistic expressions of
uncertainty in finetuned models on held-out test sets. Figure 5 displays the calibration charts for
fine-tuned models that produce post-fixed uncertainty expressions. The figure shows that these fine-
tuned models generate well-calibrated linguistic expressions of uncertainty. This result is consistent
across different model sizes and applies equally to both pre-trained and instruction-tuned models.
Comparing different methods of uncertainty augmentation: Finally, we compare different
methods for augmenting model predictions with uncertainty expressions – prefixed, postfixed, and
interleaved – as previously described. Figure 6 illustrates the performance of these augmentation
methods on the TriviaQA dataset, with similar results for AmbigQA shown in Figure 9. The figure
revealsthatpost-fixeduncertaintyexpressions,wheretheuncertaintyisaddedafterthemainanswer,
resultinthelowestcalibrationerror. Wehypothesizethatthisapproachsimplifiesfine-tuningbecause
the uncertainty expression does not influence the sampling of the main answer during autoregressive
decoding. Incontrast,prefixedorinterleaveduncertaintyexpressions,wheretheuncertaintyisadded
before or within the answer, can impact the answer’s sampling distribution, as discussed by Zhou
et al. (2023), leading to poorer calibration.
Discussion and Conclusion
Inthiswork,weinvestigatedsupervisedfine-tuningonthemodel’sownuncertaintyasapost-training
step to enable models to generate linguistic expressions of uncertainty. We assessed the calibration of
various Gemini 1.0 models and found them to be well-calibrated on the self-evaluation task. We then
usedtheseuncertaintyscorestoaugmentmodelpredictionswithlinguisticexpressionsofuncertainty.
Our findings show that fine-tuning with these augmented predictions results in models that produce
well-calibrated linguistic expressions of uncertainty on held-out test sets. This fine-tuning approach
can be employed as an independent post-training step between supervised fine-tuning (SFT) and
reinforcement learning from human feedback (RLHF). Alternatively, uncertainty-augmented datasets
can be integrated into the SFT process with appropriate system instructions that can guide models to
express uncertainty.
Models capable of generating well-calibrated uncertainty expressions enable users to make in-
formed inferences about the model’s predictions. With linguistic expressions of uncertainty, users
can reliably decide when to trust the model’s predictions and when to seek additional information.
This allows users greater control over how to utilize the model’s outputs, unlike methods that rely
on uncertainty estimates to determine when to abstain from giving a response. By abstaining, these
methods may deprive users of potentially valuable information, even when the model’s responses
are uncertain. Therefore, the ability to produce well-calibrated expressions of uncertainty should be
considered a key objective in the development of user facing foundational models.
Acknowledgements
We extend our gratitude to Greg Wayne, Reed Roberts, and Mehdi Bennani for their early discussions
on the project. We also thank Taylan Cemgil and Jacob Eisenstein for their detailed feedback on the
draft.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 9FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
References
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,
S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
A. Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1, 2024.
N. Band, X. Li, T. Ma, and T. Hashimoto. Linguistic calibration of long-form generations. In Forty-first
International Conference on Machine Learning, 2024. URL https://openreview.net/forum?
id=rJVjQSQ8ye.
R. E. Barlow and H. D. Brunk. The isotonic regression problem and its dual. Journal of the American
Statistical Association, 67(337):140–147, 1972.
T. B. Brown. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165, 2020.
J. R. Cole, M. J. Zhang, D. Gillick, J. M. Eisenschlos, B. Dhingra, and J. Eisenstein. Selectively
answering ambiguous questions. arXiv preprint arXiv:2305.14613, 2023.
M. Dahl, V. Magesh, M. Suzgun, and D. E. Ho. Large legal fictions: Profiling legal hallucinations in
large language models. arXiv preprint arXiv:2401.01301, 2024.
S. Desai and G. Durrett. Calibration of pre-trained transformers. arXiv preprint arXiv:2003.07892,
2020.
A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang,
A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
W. Fagen-Ulmschneider. Perception of probability words, 2019. URL https://waf.cs.illinois.
edu/visualizations/Perception-of-Probability-Words. Accessed: 2024-08-16.
J.Gawlikowski,C.R.N.Tassi,M.Ali,J.Lee,M.Humt,J.Feng,A.Kruspe,R.Triebel,P.Jung,R.Roscher,
et al. A survey of uncertainty in deep neural networks. Artificial Intelligence Review, 56(Suppl 1):
1513–1589, 2023.
J. Geng, F. Cai, Y. Wang, H. Koeppl, P. Nakov, and I. Gurevych. A survey of confidence estimation and
calibration in large language models. In Proceedings of the 2024 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1:
Long Papers), pages 6577–6595, 2024.
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In
International conference on machine learning, pages 1321–1330. PMLR, 2017.
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks,J.Welbl,A.Clark,etal. Trainingcompute-optimallargelanguagemodels. arXivpreprint
arXiv:2203.15556, 2022.
L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, et al. A
survey on hallucination in large language models: Principles, taxonomy, challenges, and open
questions. arXiv preprint arXiv:2311.05232, 2023.
E. Hüllermeier and W. Waegeman. Aleatoric and epistemic uncertainty in machine learning: An
introduction to concepts and methods. Machine learning, 110(3):457–506, 2021.
Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung. Survey of
hallucination in natural language generation. ACM Computing Surveys, 55(12):1–38, 2023.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 10FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
Z. Jiang, J. Araki, H. Ding, and G. Neubig. How can we know when language models know? on
the calibration of language models for question answering. Transactions of the Association for
Computational Linguistics, 9:962–977, 2021.
M.Joshi,E.Choi,D.S.Weld,andL.Zettlemoyer. Triviaqa: Alargescaledistantlysupervisedchallenge
dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.
S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Hatfield-Dodds,
N. DasSarma, E. Tran-Johnson, et al. Language models (mostly) know what they know. arXiv
preprint arXiv:2207.05221, 2022.
D. P. Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv
preprint arXiv:2109.07958, 2021.
S. Lin, J. Hilton, and O. Evans. Teaching models to express their uncertainty in words. arXiv preprint
arXiv:2205.14334, 2022.
S. J. Mielke, A. Szlam, E. Dinan, and Y.-L. Boureau. Reducing conversational agents’ overconfidence
through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:
857–872, 2022.
S. Min, J. Michael, H. Hajishirzi, and L. Zettlemoyer. Ambigqa: Answering ambiguous open-domain
questions. arXiv preprint arXiv:2004.10645, 2020.
S. Passi and M. Vorvoreanu. Overreliance on ai literature review. Microsoft Research, 2022.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv
preprint arXiv:2112.11446, 2021.
K. Saab, T. Tu, W.-H. Weng, R. Tanno, D. Stutz, E. Wulczyn, F. Zhang, T. Strother, C. Park, E. Vedadi,
et al. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416, 2024.
G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.Dai,A.Hauth,
et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
2023.
A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting. Large
language models in medicine. Nature medicine, 29(8):1930–1940, 2023.
K.Tian,E.Mitchell,A.Zhou,A.Sharma,R.Rafailov,H.Yao,C.Finn,andC.D.Manning. Justaskfor
calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned
with human feedback. arXiv preprint arXiv:2305.14975, 2023.
H. Vasconcelos, M. Jörke, M. Grunde-McLaughlin, T. Gerstenberg, M. S. Bernstein, and R. Krishna.
Explanationscanreduceoverrelianceonaisystemsduringdecision-making. ProceedingsoftheACM
on Human-Computer Interaction, 7(CSCW1):1–38, 2023.
K. Zhou, D. Jurafsky, and T. Hashimoto. Navigating the grey area: How expressions of uncertainty
and overconfidence affect language models. arXiv preprint arXiv:2302.13439, 2023.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 11FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
Appendix
Algorithm 1 Dataset curation with linguistic expressions of uncertainty
1: Input: Main Model (𝑀), Augmentation Model (𝐹), Confidence Routine (Confidence), Confidence
ScoretoLinguisticExpressionMap(LinguisticMap),IsotonicRegressionRoutine(IsoReg),Dataset
(D = {D 𝑡𝑟,D 𝑐𝑎𝑙,D 𝑓𝑠})
2: Output: Acurateddataset(D 𝑡𝑀 𝑟)
3: 𝑌ˆ = 𝑀(𝑋) //Computemodelpredictions.
4: 𝐶 𝑐𝑎𝑙 =Confidence(𝑋 𝑐𝑎𝑙,𝑌ˆ 𝑐𝑎𝑙,D 𝑓𝑠) //Computeconfidencescoresoncalibrationset.
5: 𝐶 𝑡𝑟 =Confidence(𝑋 𝑡𝑟,𝑌ˆ 𝑡𝑟,D 𝑓𝑠) //Computeconfidencescoresontrainset.
6: 𝑅=IsoReg(𝑌 𝑐𝑎𝑙,𝑌ˆ 𝑐𝑎𝑙,𝐶 𝑐𝑎𝑙) //Fitanisotonicregressoronthecalibrationset.
7: 𝐶 𝑡∗ 𝑟 = 𝑅(𝐶 𝑡𝑟) //Post-processconfidencescoresusingtheregressor.
8: 𝐸 𝑡𝑟 =LinguisticMap(𝐶 𝑡∗ 𝑟) //Convertconfidencescorestolinguisticexpressionsofuncertainty.
9: D 𝑡𝑀 𝑟 = 𝐹(𝑋 𝑡𝑟,𝑌ˆ 𝑡𝑟,𝐸 𝑡𝑟) //Augmentmodelpredictionswithlinguisticexpressionsofuncertainty.
10: ReturnD 𝑡𝑀 𝑟
Algorithm 2 Evaluation of uncertainty augmented predictions
1: Input: Dataset containing uncertainty augmented predictions D 𝑡𝑒 = (𝑋,𝑌˜), Deaugmentation Model
(DeAug), a map to convert uncertainty statements to floats (InvMap), Routine to compute ECE (ECE),
RoutinetocomputeBrierScore(Brier Score)
2: Output: 𝑒𝑐𝑒,𝑏𝑟𝑖𝑒𝑟
3: 𝑌ˆ,𝐸 = 𝐹(𝑌˜) //Striptheuncertaintystatementsandmainanswers.
4: 𝐶 =InvMap(𝐸) //Mapuncertaintystatementstofloats.
5: 𝑒𝑐𝑒=ECE(𝑌,𝑌ˆ,𝐶)
6: 𝑏𝑟𝑖𝑒𝑟 =Brier Score(𝑌,𝑌ˆ,𝐶)
7: Return𝑒𝑐𝑒,𝑏𝑟𝑖𝑒𝑟
Probability Range Linguistic Expressions
[1.0-1.0] (‘certain’)
[0.87-0.99] (‘almost certain’, ‘highly likely’)
[0.75-0.86] (‘very good chance’, ‘likely’)
[0.63-0.74] (‘i believe’, ‘probably’, ‘probable’)
[0.40-0.62] (‘chances are about even’)
[0.18-0.39] (‘i doubt’, ‘unlikely’)
[0.02-0.17] (‘little chance’,‘chances are slight’,‘improbable’,‘highly unlikely’)
[0.00-0.01] (‘impossible’)
Table 1 | Probability to Linguistic Expression Map
1 You are a highly knowledgeable assistant who is always factual and careful. For a {QUESTION} check if the {PROPOSED_ANSWER} is True
2 or False.
3
4 {QUESTION}: Who was the first president of the US?
5 {PROPOSED_ANSWER}: George Washington
6 Is the Proposed Answer:
7 a) True
8 b) False
9 The proposed Answer is: True
10
11 ...
12
13 {QUESTION}: <query>
14 {PROPOSED_ANSWER}: <model prediction>
15 Is the Proposed Answer:
16 a) True
17 b) False
18 The proposed Answer is:
Prompt 1 | Few-shot prompt for self-evaluation.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 12FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
1 You are a powerful agent that understands probabilities and language. Given a {QUESTION}, {ANSWER} and {PROBABILITY_EXPRESSION}, you
2 modify the {ANSWER} using the appropriate probability expression. Do not modify the core content of the {ANSWER}, and the only
3 change made is to include the {PROBABILITY_EXPRESSION} to the {ANSWER} to produce the {MODIFIED_ANSWER}.
4
5 Here are a few examples:
6
7 Example 1:
8 {QUESTION}: Which English author, who died in 1998, published Katie Hannigan in 1950 and The Bill Bailey Trilogy between 1986 and
9 1997?
10 {ANSWER}: richard adamson
11 {PROBABILITY_EXPRESSION}: little chance
12 {MODIFIED_ANSWER}: There is little chance that the author is Richard Adamson.
13
14 Example 2:
15 {QUESTION}: What chemical element has the symbol S?
16 {ANSWER}: sulphur
17 {PROBABILITY_EXPRESSION}: very good chance
18 {MODIFIED_ANSWER}: There is a very good chance that it is sulphur.
19
20 Example 3:
21 {QUESTION}: What is secreted by the lachrymal glands?
22 {ANSWER}: tears
23 {PROBABILITY_EXPRESSION}: certain
24 {MODIFIED_ANSWER}: It is certainly tears.
25
26 Example 4:
27 {QUESTION}: How many days is the gestation period of a domestic dog (5 days either way)?
28 {ANSWER}: 58-63
29 {PROBABILITY_EXPRESSION}: chances are about even
30 {MODIFIED_ANSWER}: Chances are about even that it is 58-63 days.
31
32 Example 5:
33 {QUESTION}: Which record label signed the Rolling Stones in 1991?
34 {ANSWER}: umg
35 {PROBABILITY_EXPRESSION}: i doubt
36 {MODIFIED_ANSWER}: I doubt that it is umg.
37
38 Example 6:
39 {QUESTION}: What is the capital of India?
40 {ANSWER}: Delhi.
41 {PROBABILITY_EXPRESSION}: impossible
42 {MODIFIED_ANSWER}: It is impossible that it is Delhi.
43
44 Example 7:
45
46 {QUESTION}: {{THE_QUESTION}}
47 {ANSWER}: {{THE_ANSWER}}
48 {PROBABILITY_EXPRESSION}: {{THE_PROBABILITY_EXPRESSION}}
49 {MODIFIED_ANSWER}:
Prompt 2 | Prompt to interleave uncertainty expressions with answers.
1 You are presented with a statement, which may contain a notion of uncertainty expressed linguistically within it. Your task is to
2 extract the {UNCERTAINTY_PHRASE} separately, and remove the uncertainty component from the answer.
3
4 Here is the list of valid uncertainty expressions.
5
6 [certain, almost certain, highly likely, very good chance, likely, i believe, probably, probable, chances are about even,
7 i doubt, unlikely, little chance, chances are slight, improbable, highly unlikely, impossible].
8
9 List the Answer removed of uncertainty in the {ANSWER_WITHOUT_UNCERTAINTY} field. List the uncertainty expression used in the
10 Uncertainty field.
11
12 {ANSWER}: X was certainly not born in 1985.
13 {ANSWER_WITHOUT_UNCERTAINTY}: X was not born in 1985.
14 {UNCERTAINTY_PHRASE}: certainly
15
16 {ANSWER}: The capital of France almost certainly might be paris.
17 {ANSWER_WITHOUT_UNCERTAINTY}: The capital of France is Paris.
18 {UNCERTAINTY_PHRASE}: almost certainly
19
20 {ANSWER}: There is little chance but the fact is correct.
21 {ANSWER_WITHOUT_UNCERTAINTY}: The fact is correct.
22 {UNCERTAINTY_PHRASE}: little chance
23
24 {ANSWER}: It is about even that the coin will be heads.
25 {ANSWER_WITHOUT_UNCERTAINTY}: The coin will be heads.
26 {UNCERTAINTY_PHRASE}: about even
27
28 {ANSWER}: It is impossible that the Sun rises in the West.
29 {ANSWER_WITHOUT_UNCERTAINTY}: The Sun rises in the West.
30 {UNCERTAINTY_PHRASE}: impossible
31
32 {ANSWER}: It is highly unlikely that the coin will be heads.
33 {ANSWER_WITHOUT_UNCERTAINTY}: The coin will be heads.
34 {UNCERTAINTY_PHRASE}: highly unlikely
35
36 {ANSWER}: There is a very good chance that tomato might not be a vegetable.
37 {ANSWER_WITHOUT_UNCERTAINTY}: Tomato is not a vegetable.
38 {UNCERTAINTY_PHRASE}: very good chance
39
40 Here is the {ANSWER} which needs to be separated into {ANSWER_WITHOUT_UNCERTAINTY}, and {UNCERTAINTY_PHRASE}.
41
Confidential–GoogleDeepMind,donotshareorforwardexternally. 13FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
42 {ANSWER}:
Prompt 3 | Prompt to disentangle uncertainty statements from answers.
1 Your task is to determine if two answers to a question are semantically equivalent. Two answers are semantically equivalent if their
2 answer to the question is the same, even if they are rephrases of each other.
3
4 Even if one contains more information than the other, as long as their answer to the question is the same, the answers are considered
5 semantically equivalent.
6
7 For example, for the question ‘Tell me a number‘, the answers ‘five‘ and ‘6‘ are not semantically equivalent as they are different
8 numbers. However, the answers ‘5‘ and ‘A number is five‘ or ‘five‘ are semantically equivalent, since they convey the same thing.
9
10 For a question ‘Tell me the capital of France‘, ‘Its Venice‘ and ‘Venice‘ are semantically equivalent, as they give the same answer.
11 The Answers ‘Paris‘ and ‘Venice‘ are not semantically equivalent. For questions asking factual information, if one answer responds
12 with information disagreeing with the other, they will not be semantically equivalent.
13
14 Another example of not semantically equivalent answers would be ‘Miles‘ and ‘Kilometers‘ for the question ‘Tell me an unit of
15 distance‘, as these two units are not the same.
16
17 Here is the question, and Answer A and Answer B to be compared.
18
19 Respond with ‘YES‘ if they are semantically equivalent, ‘NO‘ otherwise.
20
21 Question: {THE_QUESTION}
22 Answer A: {GOLD_ANSWER}
23 Answer B: {PRED_ANSWER}
24 Semantically equivalent:
Prompt 4 | Language model evaluation.
Model Dataset MaxExamplesPerBin TrainSetSize EvaluationSetsize
TriviaQA 2000 12556 7993
Small AmbigQA 1000 8935 898
TruthfulQA - - -
TriviaQA 2000 11571 7993
Small-IT AmbigQA 1000 8487 898
TruthfulQA 200 581 100
TriviaQA 2000 9283 7993
Medium AmbigQA 1000 4800 898
TruthfulQA 200 440 100
TriviaQA 2000 7590 7993
Medium-IT AmbigQA 1000 3969 898
TruthfulQA 200 518 100
Table 2 | Statistics of the datasets used for finetuning D𝑀
𝑡𝑟
Confidential–GoogleDeepMind,donotshareorforwardexternally. 14FinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
Small | ECE: 0.166 | Brier: 0.220 Small IT | ECE: 0.297 | Brier: 0.302 Medium | ECE: 0.113 | Brier: 0.208 Medium IT | ECE: 0.242 | Brier: 0.271
1.0 1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin Confidence Bin
Small | ECE: 0.022 | Brier: 0.186 Small IT | ECE: 0.016 | Brier: 0.190 Medium | ECE: 0.016 | Brier: 0.191 Medium IT | ECE: 0.022 | Brier: 0.200
1.0 1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin Confidence Bin
(a)Small (b)Small-IT (c)Medium (d)Medium-IT
Figure7|AmbigQACalibrationChart: Thetop-rowshowsrawcalibrationscoresattemperature=1.0
without any post-processing. The bottom row shows post-processed calibration scores with isotonic
regression. In each plot, the x-axis is the 𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒) of the generated prediction (shown here as
Confidence Bin) and y-axis is probability of that prediction being actually correct (shown here as
Accuracy). Expected Calibration Error (ECE) and Brier Score are reported at the top of each plot.
Small | ECE: 0.018 | Brier: 0.242 Small IT | ECE: 0.240 | Brier: 0.301 Medium | ECE: 0.073 | Brier: 0.244 Medium IT | ECE: 0.229 | Brier: 0.244
1.0 1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin Confidence Bin
Small | ECE: 0.032 | Brier: 0.240 Small IT | ECE: 0.037 | Brier: 0.240 Medium | ECE: 0.036 | Brier: 0.220 Medium IT | ECE: 0.024 | Brier: 0.192
1.0 1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin Confidence Bin
(a)Small (b)Small-IT (c)Medium (d)Medium-IT
Figure 8 | TruthfulQA Calibration Chart: The top-row shows raw calibration scores at tempera-
ture=1.0withoutanypost-processing. Thebottom row showspost-processedcalibrationscoreswith
isotonicregression. Ineachplot,thex-axisisthe 𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒) ofthegeneratedprediction(shownhere
as Confidence Bin) and y-axis is probability of that prediction being actually correct (shown here
asAccuracy). ExpectedCalibrationError(ECE)andBrierScorearereportedatthetopofeachplot.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 15
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccAFinetuningLanguageModelstoEmitLinguisticExpressionsofUncertainty
Small | Prefixed | ECE: 0.075 | Brier: Small | Postfixed | ECE: 0.067 | Brier: Small | Interleaved | ECE: 0.079 |
0.222 0.194 Brier: 0.226
1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin
Small IT | Prefixed | ECE: 0.119 | Small IT | Postfixed | ECE: 0.100 | Small IT | Interleaved | ECE: 0.073 |
Brier: 0.214 Brier: 0.197 Brier: 0.210
1.0 1.0 1.0
Perfect Calibration Perfect Calibration Perfect Calibration
Model Calibration Model Calibration Model Calibration
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin
Medium | Prefixed | ECE: 0.026 | Brier: Medium | Postfixed | ECE: 0.061 | Brier: Medium | Interleaved | ECE: 0.091 |
0.202 0.198 Brier: 0.208
1.0 1.0 1.0
Perfect Calibration Perfect Calibration
Model Calibration Model Calibration
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
Perfect Calibration
Model Calibration
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Confidence Bin Confidence Bin Confidence Bin
(a)Prefixed (b)Postfixed (c)Interleaved
Figure 9 | AmbigQA uncertainty augmentation method: Top-row is Gemini Small, middle-row
is Gemini Small-IT and the bottom row is Gemini Medium. The models generate post-fixed
uncertaintyexpressions. Thex-axisisthe 𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒) obtainedbyconvertingthelinguisticexpression
ofuncertaintytoafloatusingTable 1(shownhereasConfidence Bin)andy-axisisprobabilityof
that prediction being actually correct (shown here as Accuracy). No post-processing is done on the
𝑝 𝑚𝑜𝑑𝑒𝑙(𝑡𝑟𝑢𝑒). Expected Calibration Error (ECE) and Brier Score are reported at the top of each plot.
The error bars show the variance of accuracy in each bin.
Confidential–GoogleDeepMind,donotshareorforwardexternally. 16
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA