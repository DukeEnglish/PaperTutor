Recurrent Interpolants for Probabilistic Time Series Prediction
YuChen∗,1 MarinBiloš∗,1 SarthakMittal†,∗,2,3 WeiDeng1 KashifRasul1 AndersonSchneider1
1MorganStanley
2Mila
3UniversitédeMontréal
Abstract
HochreiterandSchmidhuber,1997]ortransformermodels
[Vaswanietal.,2017],havebeenthego-tomethodsfortime
seriesforecasting.Theyarewidelyappliedinfinance,bio-
Sequential models such as recurrent neural net- logicalstatistics,medicine,geophysicalapplications,etc.,
works or transformer-based models became de effectively showcasing their ability to capture short-term
facto tools for multivariate time series forecast- andlong-termsequentialdependencies[Morrilletal.,2021].
inginaprobabilisticfashion,withapplicationsto Thesemethodscanalsoprovideanassessmentofprediction
awiderangeofdatasets,suchasfinance,biology, uncertaintythroughprobabilisticforecastingbyincorporat-
medicine, etc. Despite their adeptness in captur- ingspecificparametricprobabilisticmodelsintotheoutput
ingdependencies,assessingpredictionuncertainty, layeroftheneuralnetwork.Forinstance,apredictorcan
and efficiency in training, challenges emerge in modeltheGaussiandistributionbypredictingbothmean
modelinghigh-dimensionalcomplexdistributions andcovariance.However,theprobabilisticoutputlayeris
and cross-feature dependencies. To tackle these confinedwithinasimpleprobabilityfamilybecausetheden-
issues,recentworksdelveintogenerativemodel- sityneedstobeparameterizedbyneuralnetworks,andthe
ingbyemployingdiffusionorflow-basedmodels. lossmustbedifferentiablewithrespecttoneuralnetwork
Notably,theintegrationofstochasticdifferential parameters.
equationsorprobabilityflowsuccessfullyextends
Tobettercapturesophisticateddistributionsintimeseries
thesemethodstoprobabilistictimeseriesimputa-
modeling and learn both the temporal and cross-feature
tion and forecasting. However, scalability issues
dependencies, a common strategy involves exploring the
necessitate a computational-friendly framework
generativemodelingoftimeseriesusingefficientdistribu-
forlarge-scalegenerativemodel-basedpredictions.
tion transportation plans, especially via diffusion or flow
This work proposes a novel approach by blend-
basedmodels.Forexample,recentworkssuchasLietal.
ingthecomputationalefficiencyofrecurrentneu-
[2020]proposeusinglatentneuralSDEaslatentstatefor
ral networks with the high-quality probabilistic
modelingtimeseriesinastochasticmanner,whileSpantini
modelingofthediffusionmodel,whichaddresses
etal.[2022]summarizenon-linearextensionsofstatespace
challenges and advances generative models’ ap-
modelsusingbothdeterministicandstochastictransforma-
plication in time series forecasting. Our method
tionplans.Tashiroetal.[2021],Bilošetal.[2023],Chen
reliesonthefoundationofstochasticinterpolants
etal.[2023a],Migueletal.[2022],Lietal.[2022]studied
andtheextensiontoabroaderconditionalgener-
the application of diffusion models in probabilistic time
ationframeworkwithadditionalcontrolfeatures,
series imputation and forecasting. The generative model
offering insights for future developments in this
is trained to learn the joint density of the whole time se-
dynamicfield.
rieswindowx∈RK×(Tcontext+Tprediction)withK features.
T isthesizeofthecontextwindow,T isthe
1 INTRODUCTION context prediction
sizeofthepredictionwindow.Duringinference,themodel
performsconditionalgenerationgiventhecontext,similar
Autoregression models [Box et al., 2015], such as recur-
totheinpaintingtaskincomputervision[Songetal.,2021].
rentneuralnetworks[Graves,2013,Sutskeveretal.,2014,
Compared to a recurrent model, where the model size is
Preprint only proportional to the number of features, but not the
∗EqualContribution
lengthofthetimewindow,suchgenerativemodelpredictors
†WorkdoneaspartofaninternshipatMorganStanley
4202
peS
81
]GL.sc[
1v48611.9042:viXramaysufferfromscalabilityissuesbecausethemodelsizeis 2.1 DENOISINGDIFFUSIONPROBABILISTIC
relatedtobothfeaturedimensionandthesizeofthewindow. MODEL(DDPM)
A more computational friendly framework is needed for
largescalegenerativemodel-basedtimeseriesprediction DDPM[Sohl-Dicksteinetal.,2015,Hoetal.,2020]adds
problems. Gaussiannoisetotheobserveddatapointx ∈ Rd atdif-
0
ferentscales0 < β < β < ··· < β suchthatthefirst
Generativemodelingexcelsatmodelingcomplicatedhigh 1 2 N
noisyvaluex isclosetothecleandatax ,andthefinal
dimensiondistributions,butmostmodelsrequirelearninga 1 0
value x is indistinguishable from noise. The generative
mappingfromnoisedistributiontodatadistribution.Ifthe N
modellearnstorevertthisprocessallowingsamplingnew
generativeprocedurestartsfromaninitialdistributionprox-
pointsfromnoise.
imate to the terminal data distribution, it can remarkably
alleviatelearningchallenges,reduceinferencecomplexity, Followingpreviousconvention,wedefineα¯
=(cid:81)n
α ,
n k=1 k
andenhancethequalityofgeneratedsamples,whichisalso withα = 1−β .ThenthetransitionkernelisGaussian
n n
supportedbypreviousstudies[Rubanovaetal.,2019,Rasul andcanbecomputeddirectlyfromx :
0
et al., 2021a,b, Chen et al., 2023b, Deng et al., 2024a,b]. √
Time series data is typically continuous and neighboring q(x n|x 0)=N( α¯ nx 0,(1−α¯ n)I). (1)
timepointsexhibitstrongcorrelations,indicatingthatthe
Theposteriordistributionisavailableinclosedform:
distributionoffuturetimepointsisclosetothatofthecur-
renttimepoint. q(x |x ,x )=N(µ˜ ,β˜ I), (2)
n−1 n 0 n n
Theseobservationsinspirethecreationofatimeseriespre-
whereµ˜ dependsonx ,x andachoiceofβ-scheduler.
diction model under the generative framework that maps n 0 n
The generative model p(x |x ) ≈ q(x |x ,x ) ap-
betweendependentdatapoints:Initiatingthepredictionof n−1 n n−1 n 0
proximatesthereverseprocess.Theactualmodelϵ (x ,n)
futuretimepoint’sdistributionwiththecurrenttimepointis θ 0
isusuallyreparameterizedtopredictthenoiseaddedtoa
morestraightforwardandyieldsbetterquality;meanwhile,
clean data point, from the noisy data point x . The loss
thelongertemporaldependencyisencodedbyarecurrent n
functioncanbesimplywrittenas:
neuralnetworkandtheembeddedhistoryispassedtothe
generativemodelastheguidanceofthepredictionforthe L=E (cid:2) ∥ϵ (x ,n)−ϵ∥2(cid:3) . (3)
ϵ∼N(0,I),n∼U({1,...,N}) θ n 2
futuretimepoints.Thenewframeworkbenefitsfromthe
efficient training and computation inherited from the re- Samplingnewdataisperformedbyfirstsamplingapoint
currentneuralnetwork,whileenjoyingthehighqualityof from the pure noise x ∼ N(0,I) and then gradually
N
probabilisticmodelingempoweredbythediffusionmodel. denoisingitusingtheabovemodeltogetasamplefromthe
datadistribution[Hoetal.,2020].
Ourcontributionsinclude:
• Wefirstextendthetheoryofstochasticinterpolantsto 2.2 SCORE-BASEDGENERATIVEMODEL(SGM)
moregeneralconditionalgenerationframeworkwith
extracontrolfeatures. SGM[Songetal.,2021],likeDDPM,considersapairof
forwardandbackwarddynamicsbetweens∈[0,1]:
• We adopt a conditional stochastic interpolants mod-
dx =f(x ,s)ds+g(s)dw (4)
s s s
ule for the sequential modeling and time series pre-
dx =[f(x ,s)−g(s)2∇ logp(x )]ds+g(s)dw (5)
diction,whichiscomputational-friendlyandachieves s s xs s s
highqualitymodelingofthefuturetimepoint’sdistri-
where ∇ logp(x ) is the so-called score function. The
bution. xs s
forwardprocessusuallyisscheduledassimpleprocesses,
suchasBrownianmotionorOrnstein–Uhlenbeckprocess,
whichcantransportdatadistributiontostandardGaussian
2 BACKGROUND distribution.Thegenerativeprocessisachievedbytheback-
ward process that walks from Gaussian prior distribution
tothedatadistributionofinterest.Now,Equation5givesa
Asweformalizeprobabilistictimeseriesforecastingwithin
waytogeneratenewpointsbystartingatx ∼N(0,I)and
thegenerativeframeworkinSection4,thissectionisdedi- s
solvingtheSDEbackwardintimegivingx asasample
catedtoreviewingcommonlyusedgenerativemethodsand 0
fromdatadistribution.Inpractice,theonlymissingpieceis
theirextensionsforconditionalgeneration.Thesemodels
obtainingthescore.Astandardapproachistoapproximate
willserveasbaselinemodelsinsubsequentsections.Fora
thescorewithaneuralnetwork.
broaderoverviewoftimeseriesforecastingproblems,refer
to[Salinasetal.,2019,Alexandrovetal.,2019],andthe Sinceduringtraining,wehaveaccesstocleandata,thescore
referencestherein. function is available in closed form. The model ϵ (x ,s)
θ s
2learnstoapproximatethescorefromnoisydataonly,result-
1.0
inginalossfunctionsimilartoEquation3:
0.8
(cid:104)
L=E
s∼U(0,1),x0∼Data,xs∼p(xs|x0) 0.6
(6)
(cid:105)
∥ϵ (x ,s)−∇ logp(x |x )∥2 . 0.4
θ s xs s 0 2
0.2
2.3 FLOWMATCHING(FM) 0.0
0.0 0.2 0.4 0.6 0.8 1.0
t
Flow matching [Lipman et al., 2022] constructs a prob- Figure1:α(s),β(s),γ(s),theschedulesofstochasticinter-
ability path by learning the vector field that generates it. polants.
Given a data point x , the conditional probability path
1 tot=1giventhevaluesattwoendsx ∼ρ tox ∼ρ ,
is denoted with p (x|x ) for s ∈ [0,1]. We put the con- 0 0 1 1
s 1 whichprovidesatransportbetweentwodensitiesρ andρ ,
straints on p (x|x ) such that p (x|x ) = N(0,I) and 0 1
s 1 0 1 whilemaintainingthedependencybetweenx andx .
p (x|x ) = N(x ,σ2I), with small σ > 0. That is, the 0 1
1 1 1
distributionp (x|x )correspondstothenoisedistribution
0 1 x =α(s)x +β(s)x +γ(s)z, s∈[0,1],z∼N(0,I)
s 0 1
and the distribution p (x|x ) is centered around the data
1 1 (10)
pointwithsmallvariance.
where ρ(s,x) is the marginal density of x at diffusion
s
Thenthereexistsaconditionalvectorfieldu (x|x )which times.Suchastochasticmappingischaracterizedbyapair
s 1
generates p (x|x ). Our goal is to learn the vector field offunctions:velocityfunctionb(s,x)andscorefunction
s 1
withaneuralnetworkϵ (x,s)whichamountstolearning s(s,x):
θ
thegenerativeprocess.Thiscanbedonebyminimizingthe
s(s,x):=∇logρ(s,x), (11)
flowmatchingobjective:
b(s,x):=E [α˙(s)x +β˙(s)x +γ˙(s)z|x =x].
(cid:104) x0,x1,z 0 1 s
L=E (12)
s∼U(0,1),x1∼Data,x∼ps(x|x0)
(7)
(cid:105)
∥ϵ (x,s)−u (x |x )∥2 . b(s,x),ρ(s,x),ands(s,x)satisfytheequalitybelow,
θ s s 1 2
∂ ρ(s,x)+∇·(b(s,x)ρ(s,x))=0 (13)
GoingbacktoEquation6wenoticethatthetwoapproaches t
havesimilarities.Flowmatchingdiffersinthepathconstruc- s(s,x)=−γ−1(s)E [z|x =x], (14)
z t
tionsanditlearnsthevectorfielddirectly,insteadoflearning
thescore,potentiallyofferingamorestablealternative. whereα(s)andβ(s)schedulethedeterministicinterpolant.
We set α(0) = 1,α(1) = 0,β(0) = 0,β(1) = 1. γ(s)
Onechoiceforthenoisingfunctionistransportingthevalues
schedulesthevarianceofthestochasticcomponentz.We
intonoiseasalinearfunctionoftransporttime:
set γ(0) = γ(1) = 0, so the two ends of the interpolant
arefixedatx andx .Figure1showsoneexampleofthe
x s =sx 1+(1−(1−σ)s)ϵ, ϵ∼N(0,I). (8) interpolantsc0 hedule,1 whereα(s)=(cid:112) 1−γ2(s)cos(1πs),
2
Theprobabilitypathisgeneratedbythefollowingcondi-
β(s)=(cid:112) 1−γ2(s)sin(1πs),γ(s)=(cid:112)
2s(1−s).
2
tionalvectorfieldwhichisavailableinclosed-form:
Thevelocityfunctionb(s,x)andthescorefunctions(s,x)
x −(1−σ)x canbemodeledbyarichfamilyoffunctions,suchasdeep
u (x|x )= 1 . (9)
s 1 1−(1−σ)s neuralnetworks.Themodelistrainedtomatchtheabove
equalitybyminimizingthemeansquarederrorlossfunc-
By learning the field u (x|x ) with a neural network tions,
s 1
ϵ (x,s)wecansamplenewpointsbysamplinganinitial
vθ
aluex 0 fromthenoisedistributionp 0 andsolveanODE L
=(cid:90) 1 E(cid:104)1
∥bˆ(s,x )∥2
0(cid:55)→1toobtainthenewsamplex 1. b 0 2 s (15)
−(cid:0) α˙(s)x +β˙(s)x +γ˙(s)z(cid:1)T bˆ(s,x )(cid:105) ds
0 1 s
2.4 STOCHASTICINTERPOLANTS(SI)
(cid:90) 1 (cid:104)1 (cid:105)
L = E ∥ˆs(s,x )∥2+γ−1zTˆs(s,x ) ds. (16)
Stochasticinterpolants[Albergoetal.,2023b]aimtomodel s 2 s s
0
thedependentcouplingsbetween(x ,x )withtheirjoint
0 1 Moredetailsoftrainingwillbeshowninsection4.
densityρ(x ,x ),andestablishatwo-waygenerativeSDEs
0 1
mappingfromonedatadistributiontoanother.Themethod Duringinference,usually,onesideofthediffusiontrajec-
constructsastraightforwardstochasticmappingfromt=0 toryatt=0ort=1isgiven,thegoalistoinferthesample
38gaussians Circles Moons Rings Swissroll
DDPM (cosine)
DDPM (linear)
RNN RNN
FM
... ...
SGM
Figure2:Stochasticinterpolantsfortimeseriesprediction
using forward SDE in equation 22. Yu: Change rrn input
index. SI (quad, linear)
distributionontheotherside.Theinterpolantinequation10
resultsinelegantforwardandbackwardSDEsandcorre-
SI (sqrt, linear)
spondingFokker-Planckequations,whichofferconvenient
toolsforinference.TheSDEsarecomposedofb(s,x )and
s
s(s,x ),whicharelearnedfromthedata.Foranyϵ(s)≥0,
s SI (sqrt, trig)
definetheforwardandbackwardSDEs
(cid:112)
dx =[b(s,x)+ϵ(s)s(s,x)]ds+ 2ϵ(s)dw (17)
s s
SI (trig, linear)
(cid:112)
dx =[b(s,x)−ϵ(s)s(s,x)]ds+ 2ϵ(s)dwB, (18)
s s
where wB is the backward Brownian motion. The SDEs Figure 3: Examples of model generated samples for syn-
s
satisfytheforwardandbackwardFokker-Plankequations, thetictwo-dimensionaldatasets.
∂ ρ+∇·(b ρ)=ϵ(s)∆ρ,ρ(0)=ρ (19) Acommonlyemployedtechniquetohandlediversecondi-
s F 0
tionsistointegrateconditioninformationthroughfeature
∂ ρ+∇·(b ρ)=−ϵ(s)∆ρ,ρ(1)=ρ . (20)
s B 1 embedding,wheretheembeddingisinjectedintovarious
layersofneuralnetworks[Songetal.,2021,Rombachetal.,
Thesepropertiesimplythatonecandrawsamplesfromthe
2022].Forinstance,conditionalSGMcanbetrainedwith
conditionaldensityρ(x |x )followingtheforwardSDEin
1 0
(cid:104)
equation17startingfromx 0ats=0.Itcanalsodrawsam- L =E
plesfromthejointdensityρ(x 0,x 1)byinitiallydrawinga
cond s∼U(0,1),(x0,ξ)∼Data,xs∼p(xs|x0)
(21)
(cid:105)
samplex 0 ∼ρ 0(iffeasible,forexample,pickonesample ∥ϵ θ(s,x s,ξ)−∇ xslogp(x s|x 0)∥2 2 .
fromthedataset),thenusingtheforwardSDEtogenerate
asamplesx 1ats=1.Themethodguaranteesthatx 1fol- where the data is given by pairs of a sample x 0 and the
lowsmarginaldistributionρ andthesamplepair(x ,x ) correspondingconditionξ.Thissimpleschemeapproach
1 0 1
satisfiesthejointdensityρ(x ,x ).Drawingsamplesusing showcasesitseffectivenessinvarioustasks,achievingstate-
0 1
thebackwardSDEissimilar:onecandrawsamplesfrom of-the-artperformance[Rombachetal.,2022,Zhangetal.].
ρ(x |x )andthejointdensityρ(x ,x )aswell.Detailsof
0 1 0 1 Likewise, SI can be expanded for conditional generation
inferencewillbeshowninsection4.
by substituting the velocity function and score function
withb(x ,s,ξ)ands(x ,s,ξ)[Albergoetal.,2023b].The
s s
3 CONDITIONALGENERATIONWITH modelistrainedusingsamplesoftuples(x 0,x 1,ξ),where
ξistheextraconditionfeature.Consequently,theinference
EXTRAFEATURES
usingforwardorbackwardSDEsbecomes
(cid:112)
Alltheaforementionedmethodscanbeadaptedforcondi- dx =[b(s,x ,ξ)+ϵ(s)s(s,x ,ξ)]ds+ 2ϵ(s)dw
s s s s
tionalgenerationwithadditionalfeatures.Theconditions (22)
mayrangefromsimplecategoricalvalues[Songetal.,2021] (cid:112)
dx =[b(s,x ,ξ)−ϵ(s)s(s,x ,ξ)]ds+ 2ϵ(s)dwB,
tocomplexpromptsinvolvingmultipledatatypes,includ- s s s s
(23)
ingpartialobservationsofasample’sentries(e.g.,image
inpainting, time series imputation) [Tashiro et al., 2021, wherebothvelocityandscorefunctionsdependonthecon-
Song et al., 2021], images [Zheng et al., 2023, Rombach ditionξ.Thelossfunctionsaresimilartoequation15and
etal.,2022],text[Rombachetal.,2022,Zhangetal.],etc. equation16.
4Algorithm1Trainingalgorithm.iisthesampleindex. Algorithm2Inferencealgorithm
Input: Sample 3-tuples (x ,x ,x ). Inter- Input:Sample2-tuples(x ,x ).Trainedmodels:
ti+1 ti ti−P:ti−1 t t−P:t−1
polant schedules: α(s),β(s),γ(s). Models: velocity bˆ, Velocitybˆ,scoreˆs,RNN.Diffusionvarianceϵ.
scoreˆs,RNN. Setx˜ =x .h=RNN(x ).
0 t t−P:t−1
foriteration=1tototaliterationsdo RunSDEintegralfors∈[0,1]following
s i ∼Beta(0.1,0.1). √
x s,i =α(s i)x ti +β(s i)x ti+1+γ(s i)z i dx˜ s =[b(s,x˜ s,h)+ϵs(s,x˜ s,h)]ds+ 2ϵdw s
h =RNN(x )
i ti−P:ti−1
Output:x˜ aspredictionofx .
1 t+1
L
=bat (cid:88)chsize 1 (cid:104)1
∥bˆ(s ,x ,h )∥2
b p Beta(s i) 2 i s,i i 8gaussians Circles Moons Rings Swissroll
i=1
−(cid:0) α˙(s )x +β˙(s )x +γ˙(s )z (cid:1)T bˆ(s ,x ,h )(cid:105) DDPM(cosine) 2.58 0.20 0.20 0.12 0.24
i ti i ti+1 i i i s,i i
DDPM(linear) 0.70 0.18 0.12 0.11 0.14
bat (cid:88)chsize 1 (cid:104)1 SGM 1.10 0.30 0.35 0.32 0.14
L = ∥sˆ(s ,x )∥2
s p (s ) 2 i i s,i FM 0.58 0.10 0.11 0.09 0.15
Beta i
i=1 SI(quad,linear) 0.52 0.15 0.32 0.12 0.16
(cid:105)
+γ−1z Tsˆ(s ,x ) SI(sqrt,linear) 0.59 0.29 0.51 0.22 0.37
i i i s,i
SI(sqrt,trig) 0.75 0.25 0.50 0.48 0.36
Performback-propagationbyminimizingL andL . SI(trig,linear) 0.52 0.13 0.29 0.21 0.16
b s
endfor
Table1:Wassersteindistancebetweenthegeneratedsam-
plesandtruedata.
Regardingthetimeseriespredictiontask,wewillencode
alargecontextwindowastheconditionalinformation,and The proof is in a spirit similar to Theorem 2 in [Albergo
thepredictionorgenerationoffuturetimepointswillrely etal.,2023b]anddetailedinsectionB.Thekeydifference
onsuchaconditionalgenerationmechanism. is that we consider a continuous-time interpretation and
avoidusingcharacteristicfunctions,whichmakestheanaly-
Next,wedemonstratethattheprobabilitydistributionofx
t sismorefriendlytousers.Additionally,thescorefunction
assimulatedbyequation24,resultsinadynamicdensity
∇logρ (x) is optimized in a simple quadratic objective
t
function. This density serves as a solution to a transport
functionasindicatedinTheorem2intheAppendix.
equation25,whichsmoothlytransitionsbetweenρ andρ .
0 1
4 STOCHASTICINTERPOLANTSFOR
Theorem1. (ExtensionofStochasticInterpolantstoArbi-
TIMESERIESPREDICTION
traryJointDistributions).Letρ bethejointdistribution
01
(x ,x )∼ρ andletthestochasticinterpolantbe
0 1 01
Weformulatetimeseriespredictiontasksthroughthecon-
x
t
=α tx 0+β tx 1+γ tz, (24) ditionalprobabilityp(x t+1|x t−P:t).Themodeldiagramis
illustratedinFigure2.Here,x ∈RD representsthemulti-
t
where α = β = 1, α = β = γ = γ = 0, and variatetimeseriesattimetwithDdimensions,x isthe
0 1 1 0 0 1 t+1
α2+β2+γ2 >0forallt∈[0,1].Wedefineρ tobethe predictiontarget,andx isthecontextwindow,where
t t t t t−P:t
time-dependentdensityofx ,whichsatisfiestheboundary P denotesthelengthofthecontextwindow.
t
conditionsatt = 0,1andthetransportequationfollows
For this problem, we employ the conditional Stochas-
that
tic Interpolants (SI) method as follows. In the training
phase, the generative model learns the joint distribution
ρ˙ +∇·(b ρ )=0 (25)
t t t
p(x ,x |x )ofthepair(x ,x )giventhepast
t+1 t t−P:t−1 t+1 t
forallt∈[0,1]withthevelocitydefinedas observationsx t−P:t−1.wherex t ∼ ρ 0 andx t+1 ∼ ρ 1 for
allt,sothemarginaldistributionsareequalρ = ρ .The
0 1
(cid:104) (cid:105)
b t(x|ξ)=E α˙ tx 0+β˙ tx 1+γ˙ tz|x
t
=x,ξ , (26) modelaimstolearnthecouplingrelationbetweenx t+1and
x conditioningonthecontextx .Thisisachieved
t t−P:t−1
where the expectation is based on the density ρ given bytrainingtheconditionalvelocityandscorefunctionsin
01
x =xandtheextrainformationξ. equation22.
s
Thescorefunctionfollowstherelationsuchthat Asthesamplespacesofρ 0 andρ 1 mustbethesame,the
generativemodel cannotdirectly mapthe wholecontext
∇logρ (x)=−γ−1E[z|x =x,ξ]. window x to the target x due to different vector
t t t t−P:t t+1
5Dimension 0 Dimension 5 diffusion
800 Observations 700
Median prediction 600
90.0% prediction interval noise
600 50.0% prediction interval 500
400
400
300
RNN RNN
200
200
100
0 0
... ...
00:00 00:00 00:00 00:00
20-Oct 21-Oct 20-Oct 21-Oct
2006 2006
Dimension 10 Dimension 15
150 Figure5:Time-Gradmodelforconditionaltimeseriespre-
100
125 diction.
80
100
60 5 EXPERIMENTS
75
40
50
20 25 Wefirstverifythemethodonsyntheticdatasets,andthen
0 0 applyittothetimeseriesforecastingtaskswithrealdata.
00:00 00:00 00:00 00:00 BaselinemodelssuchasDDPM,SGM,FM,andSIallin-
20-Oct 21-Oct 20-Oct 21-Oct
2006 2006 volvemodelingfieldfunctions,wheretheinputsarethestate
Dimension 25 Dimension 30
80 175 vector (in the same space of the data samples), diffusion
150 time,andconditionembedding,andtheoutputisthegen-
60
125 eratedsample.Thefieldfunctionscorrespondtothenoise
100 predictionfunctioninDDPM;thescorefunctioninSGM;
40
75 thevectorfieldinFM;andthevelocityandscorefunctions
20 50 inSI.Tomakeafaircomparisonbetweenthesemodels,we
25 usethesameneuralnetworksforthesemodels.Detailsof
0 0 themodelsarediscussedinAppendixC.Ourcodewillbe
00:00 00:00 00:00 00:00 publiclyavailable1.
20-Oct 21-Oct 20-Oct 21-Oct
2006 2006
Figure 4: Forecast paths for SI on Solar dataset showing
5.1 SYNTHETICDATASETS
medianprediction,50thand90thconfidenceintervalscalcu-
latedfrommodelsamples,on6/137variatedimensions.
Wesynthesizeseveraltwo-dimensionaldatasetswithreg-
ular patterns, such as Circles, Moons, etc. Details can be
foundin[Chenetal.,2021,Lipmanetal.,2022]andtheir
published code repositories. Model introduced in section
sizes.Instead,arecurrentneuralnetworkisusedtoencode 2 are compared to the SI as baselines. For diffusion-like
thecontextx intoahistoryprompth .Subsequently, models,weimplementDPPMwithalinearorcosinenoise
t−P:t−1 t
thescorefunctionandvelocityfunctionperformconditional scheduler.Weexploreonsyntheticdatasetstodeterminea
generation diffusing from x with the condition input h goodrangeofhyperparameterswhichwillbeusedinlater
t t
followingequation22. timeseriesexperiments.Thisexperimentisusedtoinves-
tigatethepropertieswithrespecttothevaryingdatasizes,
Thetrainingdatasetconsistsoftuple(x ,x ,x ).
t+1 t t−P:t−1 modelsizes,andtraininglengths.
Itisworthnotingthatthelossvaluesbecomelargerwhens
isclosetotwoends.Toaddressthis,importancesamplingis To fairly compare the generation quality, all models are
leveragedtobetterhandletheintegraloverdiffusiontimein assignedtogeneratedatainthesamesettingbymapping
thelossfunctionsequation15andequation16tostabilize fromstandardGaussiantothetargetdistribution.Theneural
the training, where we use Beta distribution for our pro- networksandhyperparametersarealsosetasthesame,such
posaldistribution.ThealgorithmisoutlinedinAlgorithm1. asbatchsize,trainingepochs,etc.Thegeneratedsamples
AdditionaldetailscanbefoundinAppendixC. fromdifferentmethodsareshowninFigure3.Table1mea-
suresthesamplequalitywithWassersteindistance[Ramdas
Intheinferencephase,theRNNfirstencodesthecontext
etal.,2017].Itshowsthatallthemodelscancapturethetrue
x intothehistoryprompth ,thenSItransportsthe
t−P:t−1 t distribution.Thesameholdswhenweusedifferentmetrics
contextvectorx tothetargetdistributionwiththecondition
t
h t,followingtheforwardSDE.Regardingthemultiplesteps 1github_place_holderWewillpublishthecodeforthe
prediction,werecursivelyrunthestep-by-stepprediction. camera-readyversion.
6Exchangerate Solar Traffic Wiki
Vec-LSTM 0.008±0.001 0.391±0.017 0.087±0.041 0.133±0.002
DDPM 0.009±0.004 0.359±0.061 0.058±0.014 0.084±0.023
FM 0.009±0.001 0.419±0.027 0.038±0.002 64.256±62.596
SGM 0.008±0.002 0.364±0.029 0.071±0.05 0.108±0.026
SI 0.007±0.001 0.359±0.06 0.083±0.005 0.080±0.007
Table2:CRPS-summetriconmultivariateprobabilisticforecasting.Asmallernumberindicatesbetterperformance.
suchasSlicedWassersteinDistance(SWD)[Rabinetal., forecastingtasks.Wefollowthepreprocessingstepsasin
2012]andMaximumMeanDiscrepancy(MMD)[Gretton [Salinasetal.,2019].Theprobabilisticforecastingisevalu-
etal.,2012]. atedbyContinuousRankedProbabilityScore(CRPS-sum)
[Koochalietal.,2022],normalizedrootmeansquareerror
Wealsotest-outdifferentschedulersforastochasticinter-
viathemedianofthesamples(NRMSE),andpoint-metrics
polantmodel.Forexample,“SI(sqrt,linear)”meansweuse
(cid:112) normalizeddeviance(ND).Themetricscalculationispro-
squarerootgamma-functionγ(s)= 2s(1−s))andalin-
videdbygluontspackage[Alexandrovetal.,2019].In
earinterpolant.Othergamma-functionsthatweconsiderare
allofthecasessmallervaluesindicatebetterperformance.
quad:γ(s)=t(1−s),andtrig:γ(s)=sin(πt)2.Weshow
thatmostofthegamma-interpolantfunctioncombinations
Results. TheresultsforCRPS-sumareshowninTable2.
achievegoodresultsmodelingthetargetdistribution.
TheresultsforothermetricsareconsistentwithCRPS-sum
andareshowninTables4and5,inAppendixC.Weoutper-
5.2 MULTIVARIATEPROBABILISTIC formormatchothermodelsonthreeoutoffourdatasets,
FORECASTING onlyonTrafficFMmodelachievesbetterperformance.Note
thatonWikidataFMcannotcapturethedatadistribution.
In this section, we will empirically verify that: 1) SI is Weranasearchoverflowmatchinghyperparameterswith-
asuitablegenerativemoduleforthepredictioncompared outbeingabletogetsatisfyingresults.Therefore,wecon-
withotherbaselineswithdifferentgenerativemethodsunder cludethatstochasticinterpolantsareastrongcandidatefor
thesameframework;2)thewholeframeworkcanachieve conditionalgeneration,inparticularformultivariateproba-
competitiveperformanceintimeseriesforecasting. bilisticforecasting.BycomparingtotheRNN-basedmodel
Vec-LSTM, our model and other baselines such as SGM
Models. ThebaselinemodelsincludeDDPM,SGM,and andDDPMgetbetterperformance,whichimpliesthatcare-
FM-basedgenerativemodelsadoptedforstep-by-step(au- fullymodeltheprobabilitydistributioniscriticalforlarge
toregressive) prediction. DDPM and SGM-based models dimensiontimeseriesprediction.Figure4demonstratesthe
canonlygeneratesamplesbytransportingGaussiannoise quality of the forecast on Solar dataset. We can see that
distributiontodatadistribution.Sowemodifytheframe- ourmodelcanmakeprecisepredictionandcapturetheun-
workbyreplacingthecontexttimepointx twithGaussian certainty,evenwhenthescaleofthedifferentdimensions
noise,asshowninFigure5.Flowmatchingcaneasilyfit variesalot.
into this framework by replacing the denoising objective
withtheflowmatchingobjective.Themodifiedframework
6 CONCLUSIONS
isshowninFigure2.Wemodelthemapfromtheprevious
timeseriesobservationtothenext(forecasted)value.We
This study presentsan innovative methodthat effectively
arguethisisamorenaturalchoicethanmappingfromnoise
mergesthecomputationalefficiencyofrecurrentneuralnet-
foreachtimeseriespredictionstep.RecentVec-LSTMfrom
workswiththehigh-qualityprobabilisticmodelingofthe
[Salinasetal.,2019]iscomparedasapurerecurrentneural
diffusionmodel,specificallyappliedtoprobabilistictimese-
networkmodelwhoseprobabilisticlayerisamultivariate
riesforecasting.Groundedinstochasticinterpolantsandan
Gaussian.
expandedconditionalgenerationframeworkfeaturingcon-
Setup. Thereal-worldtimeseriesdatasetsincludeSolar trolfeatures,themethodundergoesempiricalevaluationon
[Laietal.,2018],Exchange[Laietal.,2018],Traffic2,and bothsyntheticandrealdatasets,showcasingitscompelling
Wiki3whichhavebeencommonlyusedfortheprobabilistic performance.
2https://archive.ics.uci.edu/ml/datasets/
PEMS-SF
3https://github.com/mbohlkeschneider/
gluon-ts/tree/mv_release/datasets
7References AlexGraves. GeneratingSequenceswithRecurrentNeural
Networks. 2013.
MichaelS.Albergo,NicholasM.Boffi,andEricVanden-
Eijnden. StochasticInterpolants:AUnifyingFramework ArthurGretton,KarstenMBorgwardt,MalteJRasch,Bern-
forFlowsandDiffusions. InarXiv:2303.08797v3,2023a. hard Schölkopf, and Alexander Smola. A kernel two-
sampletest. TheJournalofMachineLearningResearch,
Michael S. Albergo, Mark Goldstein, Nicholas M. 13(1):723–773,2012.
Boffi, Rajesh Ranganath, and Eric Vanden-Eijnden.
StochasticInterpolantswithData-DependentCouplings. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising
arXiv:2310.03725v2,2023b. diffusion probabilistic models. In Advances in Neural
InformationProcessingSystems(NeurIPS),2020.
Alexander Alexandrov, Konstantinos Benidis, Michael
Bohlke-Schneider,ValentinFlunkert,JanGasthaus,Tim SeppHochreiterandJürgenSchmidhuber.LongShort-Term
Januschowski,DanielleCMaddix,SyamaRangapuram, Memory. NeuralComputation,9(8),1997.
David Salinas, Jasper Schulz, et al. Gluonts: Proba-
bilistic time series models in python. arXiv preprint Alireza Koochali, Peter Schichtel, Andreas Dengel, and
arXiv:1906.05264,2019. SherazAhmed. Randomnoisevs.state-of-the-artprob-
abilisticforecastingmethods:Acasestudyoncrps-sum
Marin Biloš, Kashif Rasul, Anderson Schneider, Yuriy
discrimination ability. Applied Sciences, 12(10):5104,
Nevmyvaka,andStephanGünnemann. ModelingTempo-
2022.
ralDataasContinuousFunctionswithProcessDiffusion.
In Proc. of the International Conference on Machine
GuokunLai,Wei-ChengChang,YimingYang,andHanxiao
Learning(ICML),2023.
Liu. Modeling long-and short-term temporal patterns
with deep neural networks. In The 41st international
GeorgeE.P.Box,GwilymM.Jenkins,GregoryC.Reinsel,
ACM SIGIR conference on research & development in
andGretaM.Ljung. TimeSeriesAnalysis:Forecasting
informationretrieval,pages95–104,2018.
andControl. WILEY,2015.
Tianrong Chen, Guan-Horng Liu, and Evangelos A XuechenLi,Ting-KamLeonardWong,RickyT.Q.Chen,
Theodorou. Likelihoodtrainingofschr\"odingerbridge andDavidDuvenaud. ScalableGradientsforStochastic
using forward-backward sdes theory. arXiv preprint DifferentialEquations. InProc.oftheInternationalCon-
arXiv:2110.11291,2021. ferenceonArtificialIntelligenceandStatistics(AISTATS),
2020.
YuChen,WeiDeng,ShikaiFang,FengpeiLi,NicoleTian-
jiao Yang, Yikai Zhang, Kashif Rasul, Shandian Zhe, YanLi,XinjiangLu,YaqingWan,andDejingDo. Gener-
Anderson Schneider, and Yuriy Nevmyvaka. Provably ativeTimeSeriesForecastingwithDiffusion,Denoise,
ConvergentSchr\"odingerBridgewithApplicationsto andDisentanglement. InAdvancesinNeuralInformation
Probabilistic Time Series Imputation. In International ProcessingSystems(NeurIPS),2022.
ConferenceonMachineLearning(ICML),2023a.
YaronLipman,RickyTQChen,HeliBen-Hamu,Maximil-
ZehuaChen,GuandeHe,KaiwenZheng,XuTan,andJun
ianNickel,andMattLe. Flowmatchingforgenerative
Zhu.SchrodingerBridgesBeatDiffusionModelsonText-
modeling. arXivpreprintarXiv:2210.02747,2022.
to-SpeechSynthesis. arXivpreprintarXiv:2312.03491,
2023b. JuanMiguel,LopezAlcaraz,andNilsStrodthoff. Diffusion-
basedTimeSeriesImputationandForecastingwithStruc-
ZongleiChen,MinboMa,TianruiLi,HongjunWang,and
turedStateSpaceModels. InTransactionsonMachine
ChongshouLi. LongSequenceTime-SeriesForecasting
LearningResearch,2022.
withDeepLearning:ASurvey. InInformationFusion,
2023c.
JamesMorrill,CristopherSalvi,PatrickKidger,JamesFos-
WeiDeng,YuChen,NicoleTianjiaoYang,HengrongDu, ter,andTerryLyons. NeuralRoughDifferentialEqua-
QiFeng,andRickyT.Q.Chen. ReflectedSchrödinger tionsforLongTimeSeries. InProc.oftheInternational
BridgeforConstrainedGenerativeModeling. InProc.of ConferenceonMachineLearning(ICML),2021.
theConferenceonUncertaintyinArtificialIntelligence
JulienRabin,GabrielPeyré,JulieDelon,andMarcBernot.
(UAI),2024a.
Wassersteinbarycenteranditsapplicationtotexturemix-
WeiDeng,WeijianLuo,YixinTan,MarinBiloš,YuChen, ing.InScaleSpaceandVariationalMethodsinComputer
Yuriy Nevmyvaka, and Ricky T. Q. Chen. Variational Vision:ThirdInternationalConference,SSVM2011,Ein-
SchrödingerDiffusionModels. InProc.oftheInterna- Gedi,Israel,May29–June2,2011,RevisedSelectedPa-
tionalConferenceonMachineLearning(ICML),2024b. pers3,pages435–446.Springer,2012.
8AadityaRamdas,NicolásGarcíaTrillos,andMarcoCuturi. IlyaSutskever,OriolVinyals,andQuocV.Le. Sequence
Onwassersteintwo-sampletestingandrelatedfamilies toSequenceLearningwithNeuralNetworks. InNIPS,
ofnonparametrictests. Entropy,19(2):47,2017. 2014.
KashifRasul,CalvinSeward,IngmarSchuster,andRoland YusukeTashiro,JiamingSong,YangSong,andStefanoEr-
Vollgraf. AutoregressiveDenoisingDiffusionModelsfor mon. CSDI:ConditionalScore-basedDiffusionModels
Multivariate Probabilistic Time Series Forecasting. In forProbabilisticTimeSeriesImputation. InAdvancesin
International Conference on Machine Learning, pages NeuralInformationProcessingSystems(NeurIPS),2021.
8857–8868.PMLR,2021a.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and
KashifRasul,Abdul-SaboorSheikh,IngmarSchuster,Urs
IlliaPolosukhin. Attentionisallyouneed. Advancesin
Bergmann,andRolandVollgraf. MultivariateProbabilis-
neuralinformationprocessingsystems,30,2017.
ticTimeSeriesForecastingviaConditionedNormalizing
Flows.InProc.oftheInternationalConferenceonLearn- Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen,
ingRepresentation(ICLR),2021b. ZiqingMa,JunchiYan,andLiangSun. Transformersin
TimeSeries:aSurvey. Thirty-SecondInternationalJoint
Robin Rombach, Andreas Blattmann, Dominik Lorenz,
ConferenceonArtificialIntelligence,2023.
PatrickEsser,andBjörnOmmer. High-resolutionimage
synthesiswithlatentdiffusionmodels. InProceedingsof Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang,
theIEEE/CVFconferenceoncomputervisionandpattern and In So Kweon. Text-to-image diffusion models in
recognition,pages10684–10695,2022. generative ai: A survey. arxiv 2023. arXiv preprint
arXiv:2303.07909.
OlafRonneberger,PhilippFischer,andThomasBrox. U-
net:Convolutionalnetworksforbiomedicalimageseg- GuangcongZheng,XianpanZhou,XueweiLi,Zhongang
mentation. InMedicalImageComputingandComputer- Qi,YingShan,andXiLi. Layoutdiffusion:Controllable
AssistedIntervention–MICCAI2015:18thInternational diffusionmodelforlayout-to-imagegeneration. InPro-
Conference,Munich,Germany,October5-9,2015,Pro- ceedingsoftheIEEE/CVFConferenceonComputerVi-
ceedings,PartIII18,pages234–241.Springer,2015. sionandPatternRecognition,pages22490–22499,2023.
YuliaRubanova,RickyT.Q.Chen,andDavidDuvenaud.
Latent ODEs for Irregularly-Sampled Time Series. In
Advances in Neural Information Processing Systems
(NeurIPS),2019.
DavidSalinas,MichaelBohlke-Schneider,LaurentCallot,
RobertoMedico,andJanGasthaus. High-dimensional
MultivariateForecastingwithLow-rankGaussianCopula
Processes. Advancesinneuralinformationprocessing
systems,32,2019.
Vladislav Shishkov. Time Diffusion. In
https://github.com/timetoai/TimeDiffusion,2023.
JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,
and Surya Ganguli. Deep Unsupervised Learning us-
ingNonequilibriumThermodynamics. InInternational
ConferenceonMachineLearning(ICML),2015.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-
basedgenerativemodelingthroughstochasticdifferential
equations. InInternationalConferenceonLearningRep-
resentations(ICLR),2021.
AlessioSpantini,RicardoBaptista,andYoussefMarzouk.
CouplingTechniquesforNonlinearEnsembleFiltering.
InSIAMReview,volume64:4,page10.1137,2022.
9A RELATEDWORKS
Aplethoraofpapersfocusonauto-regressionmodels,particularlytransformer-basedmodels.Foramorecomprehensive
review,werefertoWenetal.[2023],Chenetal.[2023c].WhileourworkdoesnotaimtoreplaceRNN-ortransformer-
basedarchitectures,weemphasizethatoneofthemainmotivationsbehindourworkistodevelopaprobabilisticmodule
buildingupontheserecentadvancements.Duetolimitedresources,wedidnotextensivelyexploreallunderlyingtemporal
architecturesbutinsteadselectedrelativelysimplermodelsasdefaults.
The authors were aware of other diffusion-based probabilistic models, as highlighted in the introduction. Unlike our
lightweightmodel,whichmodelsthetransitionbetweenadjacenttimepoints,theseselectedworksmodeltheentiretime
window,requiringbothhighmemoryandcomputationalcomplexity.Withourcomputationbudgetrestrictedtoa32GB
GPUdevice,effectivelytrainingthesediffusionmodelsonlargedatasetswithhundredsoffeaturesischallenging.
Additionally, several relevant works are related to our idea. For instance, Rasul et al. [2021b] incorporates the DDPM
structure,aligningwithourDDPMbaselinestructure.Duringinference,thepredictiondiffusesfrompurenoisetothetarget
distribution.TimeDiffShishkov[2023]introducestwomodificationstotheestablisheddiffusionmodel:duringtrainingit
mixestargetandcontextdataanditaddsanARmodelformorepreciseinitialprediction.Bothofthesecanbeincorporated
intoourmodelaswell.
Theexistingprobabilisticforecastersmodelthedistributionofthenextvaluefromscratch,meaningtheystartwithnormal
distributioninthecaseofnormalizingflowsanddiffusionmodelsoroutputparametricdistributioninthecaseoftransformers
anddeepAR.Weproposemodelingthetransformationbetweenthepreviouslyobservedvalueandthenextvaluewewant
topredict.Webelievethisisamorenaturalwaytoforecastwhichcanbeseenfromrequiringfewersolverstepstoreachthe
targetdistribution.
Thesecondrow(DDPM)inTable2isanexactimplementationofRasuletal.[2021b].Theresultsmightbedifferentdueto
slightlydifferenttrainingsetupsbutallthemodelssharethesametrainingparameterssotherankshouldremainthesame.
WealsoincludeND-sumandNRMSE-sumintheappendixforcompleteness.
DiscussionofVec-LSTMbaseline Intermsoftheneuralnetworkarchitecture,weusesimilararchitecturefortheLSTM
encoder.Buttobeclear,Vec-LSTMSalinasetal.[2019]andourSIframeworkarenotexactlythesamemainlydueto
differentwaysofprobabilisticmodeling.Vec-LSTMconsidersthemultivariateGaussiandistributionforthetimepoints,
wherethemeanandcovariancematricesaremodeledusingseparateLSTMs.Especially,thecovariancematrixismodeled
throughalow-dimensionalstructureΣ(h )=D (h )+V (h )V (h )⊺,whereh isthelatentvariablefromLSTM.The
t t t t t t t t
SIframeworkdoesnotexplicitlymodeltheoutputdistributioninanyparametricformat.Instead,thelatentvariablefrom
RNNoutputisusedastheconditionvariabletoguidethediffusionmodelinEq.22.Thus,thearchitecturesofRNNsinthe
twoframeworksarenotquitestrictlycomparable.
B PROOF:CONDITIONALSTOCHASTICINTERPOLANT
TheproofisinspiritsimilartoTheorem2inAlbergoetal.[2023b].Thekeydifferenceisthatweconsideracontinuous-time
interpretation,whichmakestheanalysismorefriendlytousers.
Proof [ProofofTheorem1]
Giventhe conditional information ξ and x = x simulated fromequation 24, theconditional stochastic interpolantfor
s
equation24followsthat
E[x |x =x,ξ]=E[α x +β x +γ z|x =x,ξ], (27)
t s t 0 t 1 t s
wheretheexpectationtakesoverthedensityfor(x ,x )∼ρ(x ,x |ξ),ξ ∼η(ξ),andz ∼N(0,I).
0 1 0 1
Wenextshowequation27isasolutionofastochasticdifferentialequationasfollows
dE[x |x =x,ξ]=f (x)dt+σ dw , (28)
t s t t t
√
wheref (x)=E[α˙ x +β˙ x |x =x,ξ]andσ = 2γ γ˙.
t t 0 t 1 s t t t
Toprovetheaboveargument,weproceedtoverifythedriftanddiffusiontermsrespectively:
10• Drift:Itisstraightforwardtoverifythedriftf bytakingthegradientoftheconditionalexpectationE[α x +β x |x =
t t 0 t 1 s
x,ξ]withrespecttot.
√
• Diffusion:Forthediffusionterm,theproofhingesonshowingσ = 2γ γ˙,whichboilsdowntoprovethestochastic
√ √t t t
calculus follows that
(cid:82)t
2γ γ˙ dw = γ z. Note that
E[(cid:82)t
2γ γ˙ dw ] = 0. Invoking the Itô isometry, we have
√ 0 s s s t 0 s s s √
Var((cid:82)t 2γ γ˙ dw ) = (cid:82)t 2γ γ˙ ds = (cid:82)t (γ 2)′ds = γ2 (givenγ = 0).Inotherwords,(cid:82)t 2γ γ˙ dw isanormal
0 s s s 0 s s 0 s t 0 0 s s s
randomvariablewithmean0andvariableγ2,whichprovesthatequation27isasolutionofthestochasticdifferential
t
equation28.
DefineΣ =2γ γ˙,weknowtheFokker-Planckequationassociatedwithequation28followsthat
t t t
(cid:18) (cid:19)
∂ρ 1
0= t +∇· f ρ − Σ ∇ρ
∂t t t 2 t t
(cid:18)(cid:18) (cid:19) (cid:19)
∂ρ 1
= t +∇· f − Σ ∇logρ ρ
∂t t 2 t t t
(29)
(cid:18)(cid:18) (cid:19) (cid:19)
∂ρ
= t +∇· E[α˙ x +β˙ x |x =x,ξ]−γ γ˙∇logρ ρ
∂t t 0 t 1 s t t t t
= ∂ρ t +∇·(cid:0) b (x,ξ)ρ (cid:1) ,
∂t t|s t
whereb (x|ξ)=E[α˙ x +β˙ x −γ γ˙∇logρ |x =x,ξ].
t|s t 0 t 1 t t t s
Furthersettings=tandrewriteb ≡b ,wehaveb (x|ξ)=E[α˙ x +β˙ x −γ γ˙∇logρ |x =x,ξ]
t t|t t t 0 t 1 t t t t
Furtherdefineg(i)(x|ξ)=E[x |x =x,ξ],wherei∈{0,1}andg(z)(x|ξ)=E[z|x =x,ξ].Wehavethat
t i t t t
b (x|ξ)=E[α˙ x +β˙ x −γ γ˙∇logρ |x =x,ξ]
t t 0 t 1 t t t t
=α˙ g(0)+β˙ g(1)+γ˙g(z)
t t t
=E[α˙ x +β˙ x +γ˙z|x =x,ξ],
t 0 t 1 t t
wherethefirstequalityfollowsbyequation29andthelastonefollowsbytakingderivativetoequation27w.r.t.thetimet.
Wealsoobservethat∇logρ =−γ−1E[z|x =x].
t t t
Theorem2. Thelossfunctionsusedforestimatingthevectorfieldfollowthat
(cid:90) 1
L (gˆ(i))= E[|gˆ(i)|2−2x ·gˆ(i)]dt,
i i
0
wherei∈{0,1,z},theexpectationtakesoverthedensityfor(x ,x )∼ρ(x ,x |ξ),ξ ∼η(ξ),andz ∼N(0,I).
0 1 0 1
Proof Toshowthelossiseffectivetoestimateg(0),g(1),andg(z).Itsufficestoshow
(cid:90) 1
L (gˆ(0))= E[|gˆ(0)|2−2x ·gˆ(0)]dt,
0 0
0
(cid:90) 1(cid:90) (cid:20) (cid:21)
= |gˆ(0)|2−2E[x |x =x,ξ]·gˆ(0) dxdt,
0 t
0 Rd
(cid:90) 1(cid:90) (cid:20) (cid:21)
= |gˆ(0)|2−2g(0)·gˆ(0) dxdt,
0 Rd
wherethelastequalityfollowsbydefinition.Theuniqueminimizerisattainablebysettinggˆ(0) =g(0).
Theproofofg(1)andg(z)followsasimilarfashion.
11C EXPERIMENTDETAILS
C.1 TIMESERIESDATA
Thetimeseriesdatasetsinclude:SolarLaietal.[2018],ExchangeLaietal.[2018],Traffic4,andWikipedia5.Wefollowthe
preprocessingstepsasinSalinasetal.[2019].DetailsofthedatasetsarelistedinTable3.
Table3:Propertiesofthedatasets.
Datasets Dimension Frequency Totaltimepoints Contextlength
Exchange 8 Daily 6,071 30
Solar 137 Hourly 7,009 24
Traffic 963 Hourly 4,001 24
Wiki 2000 Daily 792 30
TheprobabilisticforecastingisevaluatedbyContinuousRankedProbabilityScore(CRPS-sum)Koochalietal.[2022],
normalized root mean square error via the median of the samples (NRMSE), and point-metrics normalized de-
viance (ND). The metrics calculation is provided by gluonts package Alexandrov et al. [2019] by calling module
gluonts.evaluation.MultivariateEvaluator.
C.2 MODELSANDHYPERPERAMETERS
BaselinemodelssuchasDDPM,SGM,FM,andSIallinvolvemodelingfieldfunctions,wheretheinputsarethestate
vector(inthesamespaceofthedatasamples),diffusiontime,andconditionembedding,andtheoutputisthegenerated
sample.Thefieldfunctionscorrespondtothe"noiseprediction"functioninDDPM;thescorefunctioninSGM;thevector
fieldinFM;thevelocityandscorefunctionsinSI.Tomakefaircomparisonbetweenthesemodels,weusethesameneural
networksforthesemodels.
Inthesyntheticdatasetsexperiments,wemodelthefieldfunctionswitha4-layerResNet,eachlayerhas256intermediate
dimensions.Thebatchsizeis10,000forallmodelsandamodelistrainedwith20,000iterations.Thelearningrateis10−3.
Inthetimeseriesforecastingexperiments,theRNNforthehistoryencoderhas1layerand128latentdimension;The
fieldfunctionismodeledwithUnet-likestructureRonnebergeretal.[2015]with8residualblocks,andeachblockhas64
dimensions.Tostabilizethetraining,wealsousepaired-samplingforthestochasticinterpolantsintroducedby[Albergo
etal.,2023a,AppendixC].
x =α(s)x +β(s)x +γ(s)z
s 0 1
x′ =α(s)x +β(s)x +γ(s)(−z)
s 0 1
s∈[0,1],z∼N(0,I)
Thebaselinemodelsaretrainedwith200epochsand64batchsizewithlearningrate10−3.TheSImodelistrainedwith100
epochsand128batchsizewithlearningrate10−4.Wefindifthelearningrateistoolarge,SImaynotconvergeproperly.
Allthesemodelimplementationdetailscanbefoundinourcoderepositoryatwww:github_place_holder(wewill
makeitpubliclyavailableafteracceptance).
C.3 IMPORTANCESAMPLING
Thelossfunctionsfortrainingthevelocityandscorefunctionsare
4https://archive.ics.uci.edu/ml/datasets/PEMS-SF
55https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release/datasets
12L =(cid:90) 1 E(cid:104)1 ∥bˆ(s,x )∥2−(cid:0) α˙(s)x +β˙(s)x +γ˙(s)z(cid:1)T bˆ(s,x )(cid:105) ds
b 2 s 0 1 s
0 (30)
(cid:90) 1 (cid:104)1 (cid:105)
L = E ∥ˆs(s,x )∥2+γ−1zTˆs(s,x ) ds
s 2 s s
0
Bothlossfunctionsinvolvetheintegraloverdiffusiontimes∈[0,1]intheformof
(cid:90) 1
(cid:88)
L= l(s)ds≈ l(s ), s ∼Uniform[0,1] (31)
i i
0 i
However,thelossvaluesl(s)hasverylargevariance,especiallywhensisnear0or1.Figure6showsanexampleofthe
distributionofl(s)acrossmultiples.Thelargevarianceslowsdowntheconvergenceoftraining.Toovercomethisissue,
weapplyimportancesampling,similartechniqueusedby[Songetal.,2021,Sec.5.1],tostabilizethetraining.Insteadof
drawingdiffusiontimefromuniformdistribution,importancesamplingconsiders,
(cid:90) 1 (cid:88) 1
L= l(s)ds≈ l(s ), s ∼q˜(s) (32)
q˜(s ) i i
0 i i
Ideally,onewantstokeep 1 l(s )asconstantaspossiblesuchthatthevarianceoftheestimationisminimum.Theloss
q˜(si) i
valuel(s)isverylargewhensiscloseto0or1,andl(s)isrelativelyflatinthemiddle,andthedomainofsis[0,1],sowe
chooseBetadistributionBeta(s;0.1,0.1)astheproposaldistributionq˜.AsshowninFigure6,thevaluesof 1 l(s )are
q˜(si) i
plottedagainsttheirs,whichbecomesmoreconcentratedinasmallrange.
Loss with uniform sampling. Weighted Loss with importance sampling.
100
0.12
80 0.10
0.08
60
0.06
40
0.04
20 0.02
0.00
0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
t t
Figure6:Comparisonbetweenuniformsamplingandimportancesampling.Eachdotrepresentthelossofonesamplewith
respecttothediffusiontime.
13
ssol ssolC.4 ADDITIONALFORECASTINGRESULTS
Exchangerate Solar Traffic Wiki
DDPM 0.011±0.004 0.377±0.061 0.064±0.014 0.093±0.023
FM 0.011±0.001 0.445±0.031 0.041±0.002 80.624±89.804
SGM 0.01±0.002 0.388±0.026 0.08±0.053 0.122±0.026
SI 0.008±0.002 0.399±0.065 0.089±0.006 0.091±0.011
Table4:ND-sum.Asmallernumberindicatesbetterperformance.
Exchangerate Solar Traffic Wiki
DDPM 0.013±0.005 0.72±0.08 0.094±0.029 0.123±0.026
FM 0.014±0.002 0.849±0.072 0.059±0.007 165.128±147.682
SGM 0.019±0.004 0.76±0.066 0.109±0.064 0.164±0.03
SI 0.01±0.003 0.722±0.132 0.127±0.003 0.117±0.011
Table5:NRMSE-sum.Asmallernumberindicatesbetterperformance.
C.5 BASELINEMODELUSINGUNCONDITIONALSI
Additionally,weintroducedanewexperimenttoverifythenecessityofconditionalSIoverunconditionalSI.Theuncondi-
tionalSIdiffusesfrompurenoiseanddoesnotutilizethepriordistributionfromtheprevioustimepoint.Inthiscase,the
contextforthepredictionisprovidedexclusivelybytheRNNencoder.Thenewresultsareshowninthefollowingtables.
WhencomparedwiththeconditionalSIframework,theunconditionalmodelshowsslightlyinferiorperformance.
Exchangerate Solar Traffic
SI 0.007±0.001 0.359±0.06 0.083±0.005
VanillaSI 0.010±0.001 0.383±0.010 0.082±0.006
Table6:CRPS-summetriconmultivariateprobabilisticforecasting.Asmallernumberindicatesbetterperformance.
Exchangerate Solar Traffic
SI 0.008±0.002 0.399±0.065 0.089±0.006
VanillaSI 0.010±0.003 0.430±0.113 0.093±0.007
Table7:ND-sum.Asmallernumberindicatesbetterperformance.
Exchangerate Solar Traffic
SI 0.010±0.003 0.722±0.132 0.127±0.003
VanillaSI 0.012±0.003 0.815±0.135 0.132±0.015
Table8:NRMSE-sum.Asmallernumberindicatesbetterperformance.
14