Bundle Adjustment in the Eager Mode
Zitong Zhan1, Huan Xu2, Zihang Fang3, Xinpeng Wei2, Yaoyu Hu4, Chen Wang1
Abstract—Bundle adjustment (BA) is a critical technique Without eager mode, researchers are unable to build dy-
in various robotic applications, such as simultaneous local- namic computational graphs for BA using Python syntax,
ization and mapping (SLAM), augmented reality (AR), and
which limits the flexibility of employing complex control
photogrammetry. BA optimizes parameters such as camera
flows, such as loops and conditionals. As a result, models
posesand3Dlandmarkstoalignthemwithobservations.With
thegrowingimportanceofdeeplearninginperceptionsystems, cannotmakedata-dependentdecisionsduringruntime,which
there is an increasing need to integrate BA with deep learning is crucial for ensuring the robustness of BA, e.g., outlier re-
frameworksforenhancedreliabilityandperformance.However, jectionbasedonerrorpattern.Thisincreasesthedifficultyof
widely-used C++-based BA frameworks, such as GTSAM, g2o,
debugging and experimenting with prototypes. Additionally,
and Ceres, lack native integration with modern deep learning
optimizing BA separately from learning-based models could
libraries like PyTorch. This limitation affects their flexibility,
adaptability, ease of debugging, and overall implementation lead to model compounded errors, especially when the two
efficiency. To address this gap, we introduce an eager-mode components are optimized in different contexts [6].
BA framework seamlessly integrated with PyPose, providing Nevertheless, building BA frameworks in the eager mode
PyTorch-compatible interfaces with high efficiency. Our ap- isextremelychallengingduetotheinvolvementofaseriesof
proach includes GPU-accelerated, differentiable, and sparse
complicated algorithms, such as 2nd-order optimization [11],
operations designed for 2nd-order optimization, Lie group and
Liealgebraoperations,andlinearsolvers.Oureager-modeBA differentiation on Lie manifold [12], sparse Jacobian [13],
on GPU demonstrates substantial runtime efficiency, achieving and sparse linear algebra [14]. Moreover, designing flexible,
an average speedup of 18.5×, 22×, and 23× compared to modular, and extensible interfaces for all these operations
GTSAM, g2o, and Ceres, respectively. The source code will
in the eager mode is highly intricate and demands careful
be made publicly available to benefit the entire community.
planning to ensure both adaptability and performance.
I. INTRODUCTION Inthiswork,wepresentanewBAframeworkintheeager
mode based on PyPose [15], our open-source library for
Bundleadjustment(BA)isafundamentaltechniquein3D
robot learning. PyPose is fully compatible with PyTorch’s
vision, playing a crucial role in various applications such
eager mode and offers highly extensible interfaces for 2nd-
as virtual reality [1], photogrammetry [2], and simultaneous
order optimizations and differentiation on the Lie manifold.
localization and mapping (SLAM) [3]. The primary goal of
However, it currently lacks support for the sparse 2nd-order
BA is to refine sensors’ and environmental parameters, e.g.,
optimization.Tosolvethis,weintroduceAutoDiffandlinear
camera poses and 3D landmarks, so that the parameters are
algebra operations for sparse problems in the eager mode,
best fitted with observations, e.g., image pixel matching [4].
To enhance localization accuracy and preserve semantic addressing sparse Jacobian of Lie group and 2nd-order opti-
information, integrating BA with data-driven methods has mization. Furthermore, we preserved the original interfaces,
become a growing trend [3]–[7]. Achieving this often re- allowinguserstoeasilytakeadvantageofthesenewfeatures
quires implementing BA within deep learning frameworks, by making minimal changes to their existing code. As a
such as PyTorch [8], which operates in the eager mode†. result, their extensibility is retained to the maximum extent,
Remarkably,theeagermodeexecutionhasledtothesuccess ensuringthatuserscaneasilyadaptthemforabroaderscope.
of PyTorch due to its various advantages such as ease of Among them, computing a sparse Jacobian by AutoDiff
use and debugging, as well as the flexibility of Python is difficult in the eager mode. This is because PyTorch’s
syntax without sacrificing much of performance [9]. As a autograd engine exhaustively computes every gradient in
result, researchers have shown an overwhelming preference a dense Jacobian and cannot determine the existence of a
for eager mode programming [10]. Despite these strengths, gradient in advance. Therefore, the Jacobian sparsity pattern
no BA frameworks can function in the eager mode to match required for building the sparse matrix is unknown. To
the flexibility and adaptability of deep learning frameworks, overcome this, we introduce a strategy that automatically
leading to several challenges and drawbacks. traces data manipulation to determine the sparsity pattern.
Additionally, we leverage LieTensor in PyPose to rep-
CorrespondingEmail:{zitongz, chenw}@sairlab.org resent the Lie group and Lie algebra for AutoDiff through
1SpatialAI&Robotics(SAIR)Lab,UniversityatBuffalo,NY14260
the batched camera rotations. We reuse the native sparse
2GeorgiaInstituteofTechnology,GA30332
3NorthviewHighSchool,GA30097 Tensor in PyTorch to represent Jacobian, thus avoiding
4CarnegieMellonUniversity,PA15213 introducing self-defined data structures [16]. We implement
†EagermodeinmachinelearningframeworkssuchasPyTorchenables new sparse linear solvers and basic math operations in the
immediateexecutionofoperationsastheyarecalled,allowingfordynamic
eagermode,ensuringtheentireprocessofBAisefficientand
computational graph construction and providing an intuitive, interactive
developmentexperiencethatsimplifiesdebuggingandexperimentation. highly parallelizable. In summary, our contributions include
4202
peS
81
]OR.sc[
1v09121.9042:viXra• We present a new BA framework in the eager mode, differentiable Gauss-Newton using PyTorch. However, they
showing that optimization traditionally requiring com- ignored sparsity support [21], [22], making them impossible
plexfactorgraphscaneasilybecarriedoutinPython.It to be applied in even moderate-scale problems.
seamlesslyworkswithPyTorchanditsautogradengine, Tothebestofourknowledge,ourBAframeworkprovides
allowing learning-based models to be easily combined. the first exact eager mode 2nd-order optimizer compatible
• We introduce sparsity-aware AutoDiff for 2nd-order withPyTorchandutilizessparsedatastructureforscalability.
optimization, Lie groups and Lie algebras, and linear
C. Bundle Adjustment in Deep Learning
algebra, retaining original PyTorch interface design to
extend their applicability to broader domains. DROID-SLAM[5]isadeeplearning-basedSLAMsystem
• The extensive experiments demonstrated the high ef- that performs recurrent iterative updates of camera pose
ficiency of our BA framework on GPU, surpassing and pixel-wise depth through a dense BA layer, enhancing
GTSAM by 18.5×, g2o by 22×, and Ceres by 23× accuracyandrobustness.ItusestheGauss-Newtonalgorithm
in terms of runtime efficiency, even though our eager for simplicity and implements CUDA kernels from scratch
mode execution trades performance for flexibility. specific to its use case. However, the solution is not gen-
eralizable to all settings nor easy to re-build. iMatching [4]
II. RELATEDWORK
is a self-supervised feature correspondence learning method
A. Eager-mode Programming Interfaces that leverages BA as a supervisory signal to enhance the
Eager mode [9] is an intuitive and user-friendly approach accuracy of feature matching and pose estimation. This
usedindeeplearninglibrariestoexecuteoperations.Italigns approachallowsfeature-matchingmodelstolearnfromarbi-
closely with standard programming practices and interprets trary uninterrupted videos without requiring ground truth of
user commands during runtime. Debugging and interactive camera pose or depth. Although based on PyTorch, its BA
development become feasible because it provides immediate is implemented with GTSAM [16], limiting its extensibility.
feedback in step-by-step execution. On the other hand, non-
eager mode involves defining a computational graph first
III. PRELIMINARIES
and then executing the graph as a whole. This approach can To clarify the challenges of BA in eager mode, we first
optimize performance during compile time, making it faster reviewnon-linearleastsquares(NLS)optimization,usingthe
than eager mode but at the cost of usability. Levenberg-Marquardt (LM) algorithm as an example.
PyTorch [9] is the first machine learning framework ad-
A. Non-linear Least Squares
vocating eager mode usage, renowned for its flexible and
intuitive programming style. It integrates seamlessly with BAjointlyrefinescameraparametersand3Dlandmarksto
the Python ecosystem and prioritizes simplicity and consis- minimize reprojection error between the observed 2D image
tency. The eager mode design has made PyTorch a favorite points and the projected 3D points. In practice, BA is often
among researchers and developers, leading even traditional formulated as a non-linear least squares (NLS) problem:
non-eager mode frameworks to adopt similar programming
C P
models [17]. In terms of maintainability, it values simplicity θ∗ =argmin(cid:88)(cid:88) ∥Π(ζ ,p ,K )−x ∥2, (1)
in its internal implementation to allow for adaptability and θ (cid:124) i j (cid:123)(cid:122) i i (cid:125)j 2
i=1j=1
rapid feature development in the fast-evolving AI field. rij
where Π represents the camera projection model, C is the
B. Factor Graph Non-linear Optimizers
number of camera poses, P is the number of 3D points,
GTSAM [16], g2o [18], and Ceres [14] are generalized ζ ∈ SE(3) is the i-th camera pose, K is camera intrinsic
i i
non-eager mode non-linear optimizers capable of solving parameters, p ∈ R3 denotes 3D scene points, and x
j ij
large scale problems with 2nd-order optimization. All of
represents the observed 2D pixel location of the 3D point
them offer high-accuracy solutions to BA problems. These
p in the image of camera i. The goal of the optimization
libraries are designed for parallel CPU core usage but j .
(1)istorefineparametersθ ={ζ,p}tominimizethesumof
barely utilize the GPUs. DeepLM [19] attempts to build
squared reprojection errors r , thereby ensuring alignment
ij
the Levenberg-Marquardt (LM) algorithm partially based
between the 2D image observations and the 3D geometry.
on PyTorch. It relies on the autograd engine of PyTorch
In this paper, we use quaternion to represent a rotation and
for calculating the Jacobian values, and implements the denote all variables as vectors, thus camera poses ζ ∈R7C,
Jacobian product and the damped linear system using C++ 3D points p∈R3P, and the residual r∈R2PC.
and OpenMP [20] for parallelization on the CPU. Such
an implementation choice makes it hard to maintain and B. Levenberg-Marquard Algorithm
is incompatible with the up-to-date PyTorch 2.0 [8]. To
The LM algorithm combines the Gauss-Newton and gra-
achieve end-to-end differentiability under an eager mode
dientdescentmethodstosolveanNLS.TheLMupdaterule
interface and for a simpler implementation, PyPose [21]
iterativelyadjuststheparametersθ(cameraparametersζ and
provides a variety of non-linear solvers, including Gauss-
3D point locations p) by solving a linear system:
Newton and LM, entirely in PyTorch. gradSLAM [22] is
a SLAM demo project that includes an implementation of (cid:0) J⊤J+λdiag(J⊤J)(cid:1) ∆θ =−J⊤r, (2)between vectors ζ , p , and r [24]. Each block is a sub-
Algorithm 1 The Levenberg-Marquardt algorithm i j ij
Jacobian matrix, comprising the partial derivatives of the
Require: λ (damping),θ (params)
0 reprojection error with respect to a specific camera or point
for t←1 to T do
parameter. Therefore, each residual r is only associated
J← ∂r θt−1 with two blocks defined by J i =j . ∂rij ∈ R2×7 for
∂θ t−1
camera poses and J
=. ∂r[r ijij, ∈ζi] R2×3∂ fζ oi
r 3D points.
A←J⊤J [rij,pj] ∂pj
We represent the Jacobian tensors using the native
A←A+λ·diag(A)
∆θ =solver(A,−J⊤r) sparse bsr(BlockSparseRow)format[25],whichispar-
ticularly designed for matrices with a block sparse structure.
θ ←θ +∆θ
t t−1
This retains the original form of Jacobian in matrix shape,
end for
and allows block J to be directly indexed by the cor-
return θ
t
[rij,·]
responding residual and parameter. Moreover, compared to
otherformatssuchassparse coo(CoordinateList)[26],it
. is more optimized for matrix-matrix operations such as J⊤J
where r is the vectorized reprojection residuals, J = ∂r ∈
∂θ frequently used in LM. Note that PyTorch lacks support for
R(2CP)×(7C+3P) istheJacobianoftheresidualswithrespect basic operations for sparse bsr format, e.g., the matrix-
totheparameters,λisadampingfactor,anddiag(J⊤J)isa
matrix product, thus we implement all related operations
diagonal matrix consisting of the diagonal elements of J⊤J.
in the eager mode so that the entire LM algorithm can be
At each iteration, the parameters are updated as
applied with standard Python operators. By only storing the
sparse blocks, the space complexity can be reduced from
θ =θ +∆θ, (3)
t t−1 O(n2) to O(n), where n = 7C + 3P is the number of
where the damping factor λ can either be fixed or adjusted parameterstooptimize.Thissignificantcomplexityreduction
based on whether the error is reduced, allowing the algo- makes it practical for large-scale problems.
rithm to balance between fast convergence and stability. A It is worth noting that previous BA frameworks such as
simplified version of the LM method is listed in Algorithm GTSAM [16] employ a Jacobian dictionary, where each
1,whereasafullyimplementedversioncanbefoundin[23]. J is stored with an identifier based on the input-output
[rij,·]
symbol (ζ |p ,r ). The existence of J is determined
IV. METHODOLOGY
by
searchii ngj
a
fi aj
ctor graph, and the
o[ pr ti ij m,·]
ization process
Although the LM algorithm 1 is simple, the Jacobian ma- involves updating the Jacobian dictionary. However, this
trix J and the product J⊤J in a BA problem are often large representation is unsuitable for eager mode frameworks,
andhaveasparseblockstructure.Thesecharacteristicsoften which lack the detailed symbol tracking required for graph
render a regular LM implementation [21] inadequate. These searches. Also, the dictionary is a discrete data structure
issues, including sparse Jacobian calculation and efficient that is unfriendly for GPU parallelization. Consequently, the
sparse linear algebra in eager mode, will be addressed in graph search becomes infeasible in the eager mode.
Section IV-A and Section IV-B, respectively. 2) Sparsity-awareAutoDiffintheEagerMode: PyTorch’s
backward AutoDiff [27] streamlines gradient computation
A. Sparse Jacobian
by eliminating the need for user intervention. During the
The Jacobian matrix J in a BA problem is sparse due to backward pass, gradients are automatically calculated by
the unique relationship between 3D landmarks and their 2D traversing the computational graph in reverse and applying
projections, i.e., reprojection residual on each 2D pixel r ij the chain rule [28]. We aim to achieve the same level of
only depends on a single camera pose ζ i and 3D landmarks flexibility in graph traversal to efficiently compute sparse
p j, while all other parameters unrelated to the pixel have Jacobian matrices, which are crucial for many applications.
no gradients. As a result, tracking this sparsity pattern is To ensure high efficiency and flexibility, the sparse block
crucial for efficient computation and optimization. However, Jacobian is constructed by first calculating the Jacobian
PyTorch AutoDiff strategy is designed to handle general blocks J and then placing them into their designated
scenarios and computes every gradient, which is extremely
locations[r inij, t· h]
e J matrix. Although there’s an intuitive solu-
inefficient‡.Wenextanalyzethesparsitypatternandpresent
tion to traverse through each block and perform a backward
an automatic sparsity tracking strategy for the eager mode. pass one by one, it is inevitably slow. Instead, we compute
1) Sparsity Pattern and Representation: Since all camera allblockswithinasinglebackpropagationpassandgenerate
poses ζ ∈ R7C, 3D points p ∈ R3P, and the residuals r ∈ camera and point Jacobian blocks in batch, with stacked
R2PC are vectorized variables, the Jacobian in BA is stored shapes of R(C×P)×2×7 and R(C×P)×2×3, respectively.
in a sparse block structure. A natural way is to partition the
This requires us to trace the contribution of a parameter
large matrix into smaller blocks to highlight the interactions
to all related residuals. Since each camera ζ and point
(·)
p is re-used for in multiple pixel projections x , we
‡For example, the sample “Ladybug” in BAL dataset [13] with 1723 (·) (·)
replicate them in the forward pass to match each r with
cameraposesand156k3DpointswouldproduceadenseJacobianconsum- ij
ing2.6TBmemoryindoublefloatprecisionandneed12TFLOPs. a unique copy of the required parameters (ζ i, p j). ThisLadybug Trafalgar Dubrovnik
Fig.1:QualitativeresultsontheBALdataset.Ourmethodsuccessfullyrecoveredthe3Dgeometryinthescene.Bestvieweddigitally.
allows calculating the camera projections x in batch. More previously unsupported sparse types. This design choice
importantly, the unique copies of the parameters allow us ensures that no modifications to the native eager mode API
to simultaneously yet independently backpropagate gradients or user code are necessary, preserving the ease of use while
from each residual r to ζ and p , forming the blocks providing advanced sparse linear algebra capabilities.
ij i j
J andJ .Inpractice,weusethetensorindexing 2) Matrix-Vector Product: The matrix-vector product is
[rij,ζi] [rij,pj]
operationtensor[indices]toefficientlyreplicatebatch for computing J⊤r, where J⊤ is sparse while r is a dense
samples, resulting in a contiguous data layout in memory vector. This operation, commonly referred to as Sparse
[29].ThissimpletensorusageisdemonstratedinSectionIV- Matrix-Vectorproduct(SpMV)[33],isoneofthefewsparse
C and is particularly effective on high-throughput GPU operations natively supported by PyTorch, which internally
memory. In addition, we overload this operation so that it utilizes the cuSPARSE library for efficient computation.
also records the indices for later use in the backward pass. 3) Diagonal Clamping and Scaling: Diagonal clamp-
To backpropagate the Jacobian blocks in batch, we ing and scaling are essential operations in various nu-
compose PyTorch functions torch.func.jacrev with merical algorithms, especially when adjusting the diagonal
torch.func.vmap. Specifically, we use jacrev to ob- elements of a matrix for stability or regularization pur-
taintheJacobianblockcalculationforasinglepixelresidual. poses. These operations are represented by functions such
We then use vmap to apply this computation to the entire as diagonal clamp(min,max), which clamps the diagonal
batch, generating all Jacobian blocks in parallel. The block elementswithinaspecifiedrangetoensurenumericalstabil-
locations are tracked from the applied indexing operations, ity, and λ·diag(A), which scales the diagonal elements of
where the sources of cameras and points are recorded. By matrix A by a factor λ. Since PyTorch lacks native supports
having both the block locations and block values, we can for these operations with sparse matrices, we implemented
recover the Jacobian in the sparse bsr format. them using custom Triton kernels [34]. This allows diagonal
clamping and scaling directly on sparse matrices.
B. Sparse Linear Algebra Operations
4) Sparse Linear Solvers: Sparse linear solvers play a
To complete the remaining steps of the LM algorithm, crucial role in computing parameter updates within the LM
additional sparse linear algebra operations are necessary. algorithm. The task involves solving a linear system of the
However, PyTorch offers limited support for operations on form Ax = b, which is commonly expressed in code as
sparse tensors. To overcome this limitation, we have devel- x = solver(A, b). In the context of LM, the matrix
oped a set of sparse linear operations that function in eager A = J⊤J+λdiag(J⊤J) is a sparse symmetric positive-
mode. Importantly, unlike libraries such as Ceres, g2o, and definite (SPD) matrix, and the vector b = −J⊤r is dense.
GTSAM, which provide sparse operations tailored specifi- Thegoalineachiterationistofindtheupdate∆θbysolving
cally to their proprietary data structure, our implementation the linear system A∆θ =b, as required by (2).
is general-purpose and can be applied to any application, Linearsolversaregenerallyclassifiedintodirectanditera-
functioning like a standard Python operator. tivemethods.Directsolvers,suchasthoseutilizingCholesky
1) Matrix Multiplication: Matrix multiplication plays a [35] or LU decomposition [36] with pivoting, exploit the
critical role in computing the Jacobian multiplication J⊤J, properties of SPD matrices to compute exact solutions in a
where J⊤ is a sparse matrix in Compressed Sparse Row finite number of steps. These methods are highly efficient
(CSR) [30] or Block Sparse Row (BSR) format. The multi- for small to medium-sized problems. Iterative solvers, like
plicationoftwosparsematrices,commonlyknownasSparse the preconditioned conjugate gradient (PCG) method [37],
General Matrix-Matrix Multiplication (SpGEMM) [31], is approximatethesolutionthroughsuccessiveiterations,which
essential for this computation. We implement the CSR mul- can be more efficient and scalable for large, sparse systems.
tiplication using the cuSPARSE library [32], leveraging its To address varying problem scales, we provide two solver
optimized routines for sparse linear algebra operations. For implementations: (1) Sparse Cholesky Solver: Designed
theBSRformat,wedevelopcustomCUDAkernelstohandle for small-scale systems, this solver leverages the Cholesky
the block-structured sparsity efficiently. decomposition to compute exact solutions efficiently; and
These sparse matrix operations are registered to the Py- (2) Sparse Preconditioned Conjugate Gradient (PCG)
Torch operator dispatcher. Users can perform matrix mul- Solver: Suited for large-scale SPD systems, our PCG solver
tiplication using the PyTorch syntax mat1 @ mat2 with incorporates the preconditioning strategy from [13] to en-hance numerical stability and accelerate convergence. only provides raw images of the wild on Internet photo
Our implementation focuses on concise sparse operators collections.Following[19],wegeneratetheinitialmapusing
and maintains compatibility with the existing PyPose API COLMAP with its bundle adjustment disabled.
for dense linear solvers [38]. This design choice ensures WewillconductexperimentsusingourPCGandCholesky
that our optimizer is easy to deploy in research settings sparse linear solvers, which will be denoted as Ours (PCG)
and can be readily extended to accommodate more complex and Ours (Cholesky). They will be compared against the
nonlinear optimization strategies. By fully exploiting GPU most widely-used BA frameworks, including Ceres [14],
throughput, our approach enables the LM algorithm to be g2o[18], and GTSAM [16]. Ceres is widely regarded as the
seamlessly executed in the eager execution mode, allowing leading BA library, known for its robustness and scalability
for straightforward and efficient code development. to efficiently leverage a large number of CPU cores. Addi-
tionally,toensuretheirbestefficiencyonCPU,wecompiled
C. Minimum Runnable Code for BA in the Eager Mode GTSAM with Intel OneTBB [41], and g2o and Ceres were
Due to the minimal API changes, the users can re-use the built using OpenMP [20], with an optimization flag “-O3”
same code style of dense LM provided by PyPose [21] for applied. All the experiments were conducted on dual-socket
our new sparse LM. A minimum runnable code example for CPUs with 64 physical cores and 512 GB of memory. We
BA in the eager mode with 1 camera and 8 points is listed will also compare with the GPU-based framework DeepLM
below. To automatically balance the convergence rate and [19]. The performance will be presented using an Nvidia
stability, a trust region strategy TrustRegion [39] can be RTX 4090 GPU with double-precision floating point.
applied to dynamically adjust the damping factor λ. For evaluation, we assess the frameworks based on two
metrics: mean squared error (MSE) in pixels to measure
import torch, pypose as pp
from torch import nn, tensor accuracy, and runtime in seconds to evaluate efficiency.
from pypose.optim import LM These metrics allow for a comprehensive comparison of the
from pypose.optim.strategy import TrustRegion
quality and performance of the different BA methods.
from pypose.optim.scheduler import StopOnPlateau
class Residual(nn.Module): B. Overall Performance
def __init__(self, cameras, points):
1) BAL Dataset: The performance comparison on the
super().__init__()
cameras = pp.SE3(cameras) BAL dataset is presented in Table I. Our BA in the eager
self.poses = nn.Parameter(cameras) modearchivesmuchhigherefficiency,i.e.,4.4×,6.65×,and
self.points = nn.Parameter(points) 16× faster than GTSAM, g2o, and Ceres, respectively. It is
def forward(self, observes, K, cidx, pidx): observed that all the BA frameworks except GTSAM can
poses = self.poses[cidx] convergeherefore,theirprecisionintermsofMAEiscompa-
points = self.points[pidx]
rable. We also noticed that Ours (Cholesky) achieves higher
projs = pp.point2pixel(point, poses, K)
return projs - observes efficiency than Ours (PCG) in the scenes of “Trafalgar” and
“Dubrovnik”.Thisisbecause“Trafalgar”haslessnumberof
torch.set_default_device("cuda")
parameters and “Dubrovnik” has an ill-posed linear system
C, P, fx, fy, cx, cy = 1, 8, 200, 200, 100, 100
K = tensor([[fx, 0, cx], [0, fy, cy], [0, 0, 1]]) forPCGtosolve.ThequalitativeresultsareshowninFig.1,
cameras = pp.randn_SE3(C) where a high level of detail is reconstructed.
points = torch.randn(P, 3)
2) 1DSfM Dataset: We present the overall performance
observes = torch.randn(P, 2)
cidx = torch.zeros(P, dtype=torch.int32) on the 1DSfM dataset in Table II, where all frameworks
pidx = torch.arange(P, dtype=torch.int32) share a similar final error. In terms of runtime efficiency,
input = (observes, K, cidx, pidx) our BA framework surpasses GTSAM by 36×, g2o by 43×,
model = Residual(cameras, points) and Ceres by 40× in running speed, further demonstrating a
strategy = TrustRegion(damping=1e-6) consistently high efficiency of our BA in the eager mode.
optimizer = LM(model, strategy=strategy)
3) Scalability: Wenextdemonstratethescalabilityofour
scheduler = StopOnPlateau(optimizer, steps=10)
BAintermsofthenumberofoptimizableparameters.Fig.2
while scheduler.continual(): illustratestheruntimespeedupofourPCGoptimizerrelative
loss = optimizer.step(input) tog2o,GTSAM,andCeresonthe1DSfMdatasamples.The
scheduler.step(loss)
plot reveals a general trend: as the problem scale increases,
our BA demonstrates exponentially increased efficiency,
V. EXPERIMENTS
reaching up to 136×, 166×, and 163× higher efficiency
We next conduct extensive experiments to compare our comparedtog2o,GTSAM,andCeres,respectively.Onsmall
BA in the eager mode with the popular BA frameworks. samples,ourperformanceisboundedbytheefficiencyofthe
Python interpreter and thus is similar to other libraries. De-
A. Datasets, Baseline, Platforms, and Metrics
spiteeagermode’sperformancelimitationswithoutcompile-
We conduct experiments on BA using BAL [13] and time optimization, the significant speedups result from our
1DSfM [40] datasets. The BAL dataset provides the initial sparsity-aware algorithm, which efficiently leverages inher-
estimationsof3Dmapsandcameralocations.While1DSfM ent sparsity and enables effective GPU parallelism.TABLE I: Performance comparison with CPU-based BA frameworks on the BAL dataset.
GTSAM[16] g2o[18] Ceres[14] Ours(PCG) Ours(Cholesky)
Scene Camera Points Pixels
Time↓ Error↓ Time↓ Error↓ Time↓ Error↓ Time↓ Error↓ Time↓ Error↓
Ladybug 1723 156502 678718 12.43 2.540 59.12 1.313 177.15 1.146 1.60 1.120 6.01 1.134
Trafalgar 257 65132 225811 8.47 0.896 7.25 0.863 13.41 0.856 5.81 0.854 1.27 0.853
Dubrovnik 356 226730 1255268 41.80 0.787 28.18 0.789 36.71 0.787 32.10 0.793 6.93 0.791
Overall 62.70 1.408 94.55 0.988 227.27 0.92 39.51 0.922 14.21 0.926
TABLE II: Performance comparison with CPU-based BA frameworks on the 1DSfM dataset.
GTSAM[16] g2o[18] Ceres[14] Ours(PCG) Ours(Cholesky)
Scene Camera Points Pixels
Time↓ Error↓ Time↓ Error↓ Time↓ Error↓ Time↓ Error↓ Time↓ Error↓
UnionSquare 166 3643 39651 9.53 2.365 0.78 2.617 2.15 2.324 1.21 2.358 0.33 2.377
P.delPopolo 317 13294 71055 10.79 3.104 5.64 3.104 8.42 3.102 1.61 2.925 1.16 2.928
EllisIsland 287 17565 64697 8.48 3.473 3.68 3.502 9.28 3.446 1.07 3.449 0.60 3.478
NYCLibrary 265 11247 50103 5.38 2.857 4.50 2.857 2.39 2.855 1.14 2.856 0.53 2.855
M.N.Dame 475 28209 147250 22.82 3.498 16.97 3.444 18.41 3.426 1.30 3.427 1.25 3.426
Gen.markt 745 32940 128472 10.82 4.793 45.45 2.968 30.66 2.922 1.62 2.925 1.16 2.928
Alamo 741 82801 536967 64.73 3.728 32.57 3.817 63.14 3.726 3.25 3.727 3.59 3.727
Yorkminster 64 3432 16351 2.70 2.244 0.19 2.323 1.67 2.059 2.43 2.090 0.59 2.094
RomanForum 905 44245 151704 15.56 3.128 53.20 2.988 34.45 2.982 2.19 2.980 0.97 2.983
V.Cathedral 712 35688 170443 39.06 2.652 55.61 2.667 54.45 2.634 1.90 2.636 2.20 2.636
M.Metropolis 97 4981 21930 2.38 2.612 0.49 2.591 1.50 2.588 0.86 2.598 0.29 2.593
Piccadily 1898 83234 363139 233.57 3.737 454.24 3.484 290.14 3.419 2.53 3.418 13.71 3.423
T.ofLondon 327 13156 58179 9.45 2.303 5.67 2.245 13.87 2.098 1.28 2.108 0.77 2.108
Trafalgar 4159 130027 572171 494.02 3.387 405.56 3.342 486.15 3.311 2.96 3.241 12.40 3.307
Overall 929.28 3.134 1084.55 2.996 1016.67 2.921 25.35 2.910 39.55 2.919
TABLE III: Comparision with GPU-based method DeepLM.
102
DeepLM[19] Ours(Best)
Scene
Time↓ Error↓ Time↓ Error↓
101
Ladybug 5.87 1.121 1.60 1.120
Trafalgar 3.44 0.858 1.27 0.853
Speedup vs. G2O
Dubrovnik 13.10 0.787 6.93 0.791
100 Speedup vs. GTSAM
Speedup vs. Ceres
BALOverall 22.41 0.922 9.80 0.921
G2O Speedup Trend
UnionSquare 1.31 2.330 0.33 2.377 GTSAM Speedup Trend
10−1 Ceres Speedup Trend
P.delPopolo 1.45 3.103 1.16 2.928
EllisIsland 1.46 3.448 0.60 3.478 10k 30k 100k 300k
NYCLibrary 1.40 2.855 0.53 2.855 Number of Optimizable Parameters
M.N.Dame 1.76 3.426 1.25 3.426
Gen.markt 1.77 2.926 1.16 2.928 Fig. 2: Speedup of our BA relative to other frameworks exponen-
Alamo 3.43 3.727 3.25 3.727 tially increases with the number of optimizable parameters.
Yorkminster 1.24 2.089 0.59 2.094
RomanForum 1.65 2.984 0.97 2.983 lacks extensibility to other applications since its sparsity is
V.Cathedral 2.04 2.636 1.90 2.636
addressed by a non-native data structure.
M.Metropolis 1.20 2.589 0.29 2.593
Piccadily 2.28 3.419 2.53 3.418
VI. CONCLUSIONS&DISCUSSIONS
T.ofLondon 1.42 2.103 0.77 2.108
Trafalgar 3.05 3.241 2.96 3.241
Wepresentedahighlyextensibleandscalableeager-mode
1DSfMOverall 25.45 2.92 18.29 2.913 BA framework, which can seamlessly integrate with modern
deep learning frameworks. By leveraging GPU acceleration,
differentiable operations, and sparse linear operations, our
4) ComparisonwithGPU-basedframework: Asshownin new BA framworks offers up to hundreds of times speed-up
Table III, our BA framework achieves both better precision than widely-used CPU-based BA frameworks. Nevertheless,
and higher efficiency than DeepLM [19], the state-of-the- ourcurrentimplementationisnotoptimizedforcomputation
art PyTorch-based BA framework. Specifically, we require on CPUs. Additionally, since memory allocation and re-
56% and 28% less runtime than DeepLM on the BAL and lease are managed by Python’s automatic garbage collection
1DSfM datasets, respectively. Note that although DeepLM mechanism, our BA framework may have a higher memory
is based on PyTorch, it does not support eager mode and consumption than the C++-based frameworks.
pudeepSREFERENCES [17] A. Agrawal, A. N. Modi, A. Passos, A. Lavoie, A. Agarwal,
A.Shankar,I.Ganichev,J.Levenberg,M.Hong,R.Monga,andS.Cai,
[1] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. Lau,
“Tensorflow eager: A multi-stage, python-embedded dsl for machine
F.Gao,Y.Yang,andC.Jiang,“VR-GS:Aphysicaldynamics-aware
learning,”2019.[Online].Available:https://arxiv.org/abs/1903.01855
interactivegaussiansplattingsysteminvirtualreality,”arXivpreprint
[18] R.Ku¨mmerle,G.Grisetti,H.Strasdat,K.Konolige,andW.Burgard,
arXiv:2401.16663,2024.
“G2o:Ageneralframeworkforgraphoptimization,”inIEEEInt.Conf.
[2] X. He, J. Sun, Y. Wang, S. Peng, Q. Huang, H. Bao, and X. Zhou,
onRoboticsandAutomation(ICRA),062011,pp.3607–3613.
“Detector-freestructurefrommotion,”CVPR,2024.
[19] J. Huang, S. Huang, and M. Sun, “Deeplm: Large-scale nonlinear
[3] K. Xu, Y. Hao, S. Yuan, C. Wang, and L. Xie, “AirVO:
least squares on deep learning frameworks using stochastic domain
An illumination-robust point-line visual odometry,” in IEEE/RSJ
decomposition,”inProceedingsoftheIEEEConferenceonComputer
International Conference on Intelligent Robots and Systems (IROS),
VisionandPatternRecognition,2021,pp.10308–10317.
2023.[Online].Available:https://arxiv.org/pdf/2212.07595.pdf
[20] OpenMPArchitectureReviewBoard,“OpenMPapplicationprogram
[4] Z. Zhan, D. Gao, Y.-J. Lin, Y. Xia, and C. Wang, “iMatching:
interface version 3.0,” May 2008. [Online]. Available: http:
Imperative correspondence learning,” in European Conference
//www.openmp.org/mp-documents/spec30.pdf
on Computer Vision (ECCV), 2024. [Online]. Available: https:
[21] C. Wang, D. Gao, K. Xu, J. Geng, Y. Hu, Y. Qiu, B. Li, F. Yang,
//arxiv.org/pdf/2312.02141.pdf
B. Moon, A. Pandey, Aryan, J. Xu, T. Wu, H. He, D. Huang,
[5] Z. Teed and J. Deng, “Droid-slam: Deep visual slam for monocular,
Z. Ren, S. Zhao, T. Fu, P. Reddy, X. Lin, W. Wang, J. Shi,
stereo,andrgb-dcameras,”AdvancesinNeuralInformationProcess-
R. Talak, K. Cao, Y. Du, H. Wang, H. Yu, S. Wang, S. Chen,
ingSystems,vol.34,pp.16558–16569,2021.
A. Kashyap, R. Bandaru, K. Dantu, J. Wu, L. Xie, L. Carlone,
[6] C. Wang, K. Ji, J. Geng, Z. Ren, T. Fu, F. Yang, Y. Guo,
M.Hutter,andS.Scherer,“PyPose:Alibraryforrobotlearningwith
H. He, X. Chen, Z. Zhan, Q. Du, S. Su, B. Li, Y. Qiu,
physics-based optimization,” in IEEE/CVF Conference on Computer
Y. Du, Q. Li, Y. Yang, X. Lin, and Z. Zhao, “Imperative
Vision and Pattern Recognition (CVPR), 2023. [Online]. Available:
learning: A self-supervised neural-symbolic learning framework for
https://arxiv.org/pdf/2209.15428.pdf
robot autonomy,” arXiv preprint arXiv:2406.16087, 2024. [Online]. [22] K.M.Jatavallabhula,G.Iyer,andL.Paull,“▽slam:Denseslammeets
Available:https://arxiv.org/abs/2406.16087
automaticdifferentiation,”in2020IEEEInternationalConferenceon
[7] T.Fu,S.Su,Y.Lu,andC.Wang,“iSLAM:ImperativeSLAM,”IEEE
RoboticsandAutomation(ICRA). IEEE,2020,pp.2130–2137.
Robotics and Automation Letters (RA-L), 2024. [Online]. Available:
[23] “pypose.optim.levenbergmarquardt,” https://pypose.org/docs/main/
https://arxiv.org/pdf/2306.07894.pdf
generated/pypose.optim.LevenbergMarquardt/.
[8] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky,
[24] M. Zheng, N. Chen, J. Zhu, X. Zeng, H. Qiu, Y. Jiang, X. Lu,
B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia,
and H. Qu, “Distributed bundle adjustment with block-based sparse
W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng,
matrix compression for super large scale datasets,” in IEEE/CVF
J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar,
InternationalConferenceonComputerVision(ICCV),2023.[Online].
L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. K.
Available:https://arxiv.org/abs/2307.08383
Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y.
[25] “PyTorch sparse bsr tensor documentation,” https://pytorch.org/docs/
Siraichi, H. Suk, S. Zhang, M. Suo, P. Tillet, X. Zhao, E. Wang,
stable/sparse.html#sparse-bsr-tensor,2024.
K. Zhou, R. Zou, X. Wang, A. Mathews, W. Wen, G. Chanan,
[26] “Sparse Matrix - Coordinate List (COO),” https://en.wikipedia.org/
P.Wu,andS.Chintala,“PyTorch2:Fastermachinelearningthrough
wiki/Sparsematrix#Coordinatelist(COO),2024,accessed:2024-09-
dynamic python bytecode transformation and graph compilation,”
12.
in Proceedings of the 29th ACM International Conference on
[27] “Automatic differentiation with torch.autograd,” https://pytorch.org/
Architectural Support for Programming Languages and Operating
tutorials/beginner/basics/autogradqstutorial.html.
Systems, Volume 2, ser. ASPLOS ’24. New York, NY, USA:
[28] “Automatic Differentiation,” 2024, [Online; accessed 12-September-
Association for Computing Machinery, 2024, p. 929–947. [Online].
2024]. [Online]. Available: https://en.wikipedia.org/wiki/Automatic
Available:https://doi.org/10.1145/3620665.3640366
differentiation#Forwardandreverseaccumulation
[9] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
[29] “Tensor Indexing API,” https://pytorch.org/cppdocs/notes/
T.Killeen,Z.Lin,N.Gimelshein,L.Antiga,A.Desmaison,A.Kopf,
tensorindexing.html,2024,accessed:2024-09-13.
E.Yang,Z.DeVito,M.Raison,A.Tejani,S.Chilamkurthy,B.Steiner,
[30] “PyTorch Sparse CSR Tensor documentation,” https://pytorch.org/
L. Fang, J. Bai, and S. Chintala, “PyTorch: An Imperative Style,
docs/stable/sparse.html#sparse-csr-tensor,2024.
High-Performance Deep Learning Library,” in Advances in Neu-
[31] F. G. Gustavson, “Two fast algorithms for sparse matrices:
ral Information Processing Systems 32, H. Wallach, H. Larochelle,
Multiplication and permuted transposition,” ACM Trans. Math.
A.Beygelzimer,F.d’Alche´Buc,E.Fox,andR.Garnett,Eds. Curran
Softw., vol. 4, no. 3, p. 250–269, sep 1978. [Online]. Available:
Associates,Inc.,2019,pp.8024–8035.
https://doi.org/10.1145/355791.355796
[10] H. He, “The state of machine learning frameworks in 2019,” The
[32] NVIDIACorporation,“cusparselibrary,”https://docs.nvidia.com/cuda/
Gradient,2019.
cusparse/index.html,2024,accessed:2024-09-13.
[11] C.Zach,“Robustbundleadjustmentrevisited,”inComputerVision–
[33] J.H.WilkinsonandC.B.Moler,Matrixcomputations. GBR:John
ECCV 2014, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds.
WileyandSonsLtd.,2003,p.1103–1109.
Cham:SpringerInternationalPublishing,2014,pp.772–787.
[34] Triton Contributors, “Triton language and compiler,” https://github.
[12] J. Sola`, J. Deray, and D. Atchuthan, “A micro lie theory for
com/triton-lang/triton,2024,accessed:2024-09-13.
state estimation in robotics,” 2021. [Online]. Available: https:
[35] “Note sur une me´thode de re´solution des e´quations normales
//arxiv.org/abs/1812.01537
provenant de l’application de la me´thode des moindres carre´s a
[13] S. Agarwal, N. Snavely, S. M. Seitz, and R. Szeliski, “Bundle
un syste`me d’e´quations line´aires en nombre infe´rieur a celui des
adjustmentinthelarge,”inEuropeanConferenceonComputerVision
inconnues.—applicationdelame´thodealare´solutiond’unsyste`me
(ECCV). Springer,2010,pp.29–42.
defini d’e´quations line´aires,” Bulletin ge´ode´sique, vol. 2, no. 1, pp.
[14] S. Agarwal, K. Mierle, and The Ceres Solver Team, “Ceres Solver,”
67–77,1924.[Online].Available:https://doi.org/10.1007/BF03031308
Oct. 2023. [Online]. Available: https://github.com/ceres-solver/
[36] A.Schwarzenberg-Czerny,“Onmatrixfactorizationandefficientleast
ceres-solver
squaressolution.”aaps,vol.110,p.405,Apr.1995.
[15] Z. Zhan, X. Li, Q. Li, H. He, A. Pandey, H. Xiao, Y. Xu, X. Chen,
[37] P. Concus, G. Golub, and G. Meurant, “Block preconditioning for
K. Xu, K. Cao, Z. Zhao, Z. Wang, H. Xu, Z. Fang, Y. Chen,
the conjugate gradient method,” no. LBL-14856, 1982. [Online].
W. Wang, X. Fang, Y. Du, T. Wu, X. Lin, Y. Qiu, F. Yang, J. Shi,
Available:https://escholarship.org/uc/item/0j60b61v
S.Su,Y.Lu,T.Fu,K.Dantu,J.Wu,L.Xie,M.Hutter,L.Carlone,
[38] “Pyposelinearsolver,”https://pypose.org/docs/main/generated/pypose.
S. Scherer, D. Huang, Y. Hu, J. Geng, and C. Wang, “PyPose v0.6:
optim.solver.PINV/.
The imperative programming interface for robotics,” in IEEE/RSJ
[39] D. C. Sorensen, “Newton’s method with a model trust-region
International Conference on Intelligent Robots and Systems (IROS)
modification,” University of North Texas Libraries, UNT Digital
Workshop,2023.[Online].Available:https://arxiv.org/abs/2309.13035
Library, Tech. Rep., September 1980, accessed: September 13,
[16] F. Dellaert and Contributors, “borglab/gtsam,” May 2022. [Online].
2024. [Online]. Available: https://digital.library.unt.edu/ark:/67531/
Available:https://github.com/borglab/gtsam
metadc283479/[40] K.WilsonandN.Snavely,“Robustglobaltranslationswith1dsfm,”in [41] Intel Corporation, “oneapi threading building blocks (onetbb),”
ProceedingsoftheEuropeanConferenceonComputerVision(ECCV), https://www.intel.com/content/www/us/en/developer/tools/oneapi/
2014. onetbb.html,2021,version2021.5.