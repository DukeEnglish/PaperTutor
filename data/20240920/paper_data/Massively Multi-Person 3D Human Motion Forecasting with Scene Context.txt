Massively Multi-Person 3D Human Motion
Forecasting with Scene Context
Felix B Mueller1⋆ , Julian Tanke2, and Juergen Gall2,3
1 Institute of Computer Science and Campus Institute Data Science, University of
Goettingen, Germany
2 Institute of Computer Science, University of Bonn, Germany
3 Lamarr Institute for Machine Learning and Artificial Intelligence, Germany
felix.mueller@cs.uni-goettingen.de
Abstract. Forecasting long-term 3D human motion is challenging: the
stochasticity of human behavior makes it hard to generate realistic hu-
man motion from the input sequence alone. Information on the scene
environmentandthemotionofnearbypeoplecangreatlyaidthegenera-
tionprocess.Weproposeascene-awaresocialtransformermodel(SAST)
toforecastlong-term(10s)humanmotionmotion.Unlikepreviousmod-
els, our approach can model interactions between both widely varying
numbersofpeopleandobjectsinascene.Wecombineatemporalconvo-
lutional encoder-decoder architecture with a Transformer-based bottle-
neckthatallowsustoefficientlycombinemotionandsceneinformation.
We model the conditional motion distribution using denoising diffusion
models.WebenchmarkourapproachontheHumansinKitchensdataset,
whichcontains1to16personsand29to50objectsthatarevisiblesimul-
taneously. Our model outperforms other approaches in terms of realism
and diversity on different metrics and in a user study. Code is available
at https://github.com/felixbmuller/SAST.
Keywords: Human Motion Forecasting · Multi-Person · Long-Term
Forecasting
1 Introduction
Human motion forecasting aims to anticipate how humans may continue their
movement in the future based on past observations [10,15,22,28]. Humans per-
formthistaskinstinctivelytonavigatecomplexmulti-personenvironments[37].
Itisalsoahighlyrelevantcomputervisionproblemasithasvariousdownstream
applications, e.g. in robotics [17,35], healthcare [18], and neuroscience.
Whileshort-termfuturemotionisdominatedbypreservationofmomentum,
long-term future motion forecasting has to take the interdependence of motion
in a multi-person setting into account. Humans interact with and react to each
other all the time [19,36]. The environment is also of crucial importance for
motion forecasting [1], as it predetermines the space of possible behaviors. The
⋆ work done while at University of Bonn.
4202
peS
81
]VC.sc[
1v98121.9042:viXra2 Mueller, Tanke, et al.
Fig.1: Our model forecasting complex realistic motion (fading from to ) in a
kitchen environment based on an input sequence and scene context. The upper
personreturnstothestandingtable aftergrabbingsomethingfromthecupboard
while the lower person walks toward and stops at the large cupboard .
placement of e.g. walkways, doors, and chairs greatly impacts what behaviors
can be expected in certain situations.
Whilelong-termmotionforecastinghasseenaproliferationofworkinrecent
years [5,8,9,11,16,20,23,29,30,40], most approaches still focus on the single
or two-person special case. Especially multi-person situations with a fluctuating
numberofpeopleareunderstudieddespitetheircommonoccurrenceinthewild.
Also,fewworksconsiderthesceneenvironmentdespiteitsrichinformationcon-
tent. TriPod [1] aims to address this by employing two graph attention modules
for human-object and human-human interactions with message passing in be-
tween. However, TriPod does not scale well to sequences longer than a second,
presumably due to its autoregressive approach using RNNs.
We want work towards unconstrained human motion models that are able
to model realistic interactions in the wild and we aim to bridge this gap with
ourscene-awaresocialtransformermodel(SAST)forlong-termforecasting.Our
model allows for
– long-term (10 seconds) motion forecasting,
– versatile interaction modeling for widely varying (1-16) numbers of persons,
– scene-agnosticenvironmentmodelingbasedonlyonvariablenumbers(50+)
of 3D object point clouds, and
– samplingofmultiplerealisticcontinuationsfromtheconditionalmotiondis-
tribution.
Jointlyforecastingmulti-personmotionforalargevaryingnumberofpersons
isdifficulttolearn.Wesimplifythistaskbyforecastingonlyonepersonatatime
during training (with context information from other people). During inference,
weareabletoproducehighlyinterdependentmulti-personmotionbyexchanging
motion information throughout the diffusion process.Massively Multi-Person 3D Human Motion Forecasting with Scene Context 3
To our knowledge, our approach is the first long-term multi-person motion
forecasting model that takes scene context into account. To showcase our flex-
ible modeling approach, we evaluate our model on the Humans in Kitchens
dataset [38] with 1 to 16 persons interacting simultaneously in four different en-
vironments containing 37 to 50 objects, predicting up to 10 seconds of output
motion.
2 Related Work
Human pose forecasting aims to predict a future pose sequence given some pose
history [10,15,22,28]. There has been considerable work on single-person fore-
casting [2,6,12,14,21,25–27]. As multi-person interactions play a crucial role
in forming human behavior, significant work has also been done on modeling
human-human-interactions[1,32,42–44].However,eveninthemulti-personcase,
most prior work focuses on two-person [1,23,29,30] or three-person [40,44] sce-
narios.
Long-Term Forecasting Initially, motion forecasting focused on short-term fore-
casting up to 1 second [1,10,12,15,22,26,28]. Diffusion probabilistic models [13]
sparked the development of both single-person and multi-person models fore-
casting several seconds of realistic motion [5,23,29,30,40], even though non-
diffusion long-term-forecasting approaches have been proposed as well [8,16].
Multi-second forecasting horizons are necessary to allow for diverse downstream
application [16]. But in the area of long-term forecasting, the stochastic nature
of human behavior requires turning away from exact ground truth reconstruc-
tion toward modeling the distribution of plausible motion sequences [4] given
the input sequence and additional context if available.
Motion Synthesis with Context Information There has been some work on com-
bining long-term motion forecasting with guiding signals like speech [29,30] or
action labels [5,9,23]. However, few works take scene context into account, even
thoughitcontainscrucialinformationformanyhumanbehaviorsandispresent
in many in-the-wild scenarios [8,38]. Existing work employing scene context is
either limited to short-term [1] or single-person [8] forecasting.
3 Methodology
3.1 Problem Representation
Motion Sequences GivenP personswithJ joints,themotionsequenceofperson
i consisting of N frames is
X(i),1:N =[j ,j ,...,j ] with j ∈RN (1)
1,1 1,2 J,3 k,d
whereeachjointji isinglobalCartesiancoordinates.Wedenotetheconcurrent
k,d
motion sequences of P persons as
(cid:104) (cid:105)
X = X(1),X(2),...,X(P) ∈RP×J×3×N. (2)4 Mueller, Tanke, et al.
Scene Geometry We describe the scene geometry (e.g. walls, tables, and chairs)
as a set of point clouds whose positions may change over time, for example, by
moving a chair. We use basis point set (BPS) encoding [33] to represent each
object at a specific point in time by a fixed-size vector. BPS encoding is well
suitedtoprovidecompactrepresentationsforobjectsofwidelyvaryingsizesand
shapes. It represents objects by the distance between random fixed basis points
and the closest point on the object surface. The number of basis points equals
the length of the resulting vector representation and can be chosen to balance
between granularity and encoding size. We also concatenate a one-hot encoding
of13objecttypestotheBPSencoding,e.g.chair,table,orcoffeemachine.The
encoding dimensionality is d = 2,061. We define a scene with G objects at
Obj
time n as
(cid:110) (cid:111)
Sn = S(1),n,S(2),n,...,S(G),n with S(i),n ∈RdObj (3)
Forecasting Objective Our goal is to learn a social interaction model that is
able to generate a realistic and plausible future multi-person motion sequence
given pose history and the scene. n denotes the number of frames in the input
sequence,whileN isthenumberofframesinthewholesequence.Weonlypredict
futuremotionforpeoplepresentintheinputsequenceX1:n,evenifmorepeople
enter the scene during the ground truth output sequence. We use zero-velocity
padding if a person enters late during the input sequence or leaves early during
the ground truth output sequence. In this work, we limit our model to static
scene geometry, i.e. the state of the scene at the last input frame.
Xˆn+1:N =SAST(X1:n,Sn) (4)
3.2 Normalization and Scaling
Wedesignourmodeltoforecastthemotionofonepersonatatime(theprimary
person) given environment context (other persons and scene). We align the pri-
maryposesequencesX(i) atthelastframeoftheinputsequencensuchthatthe
mean of both hip joints is at (x,y)=(0,0) and the hip is parallel to the x-axis.
Thisremovesglobaltranslationandrotationfromtheprimaryposesequence.We
do not perform normalization on the z-axis, as all motion in the dataset we are
working with is based on an equal-level floor. This normalization criteria induce
a affine transformation Norm , which we also apply to the environment context,
i
similarto[16].Givenonemulti-personmotionsequenceX andascenecontextS,
wethuscreateP normalizeddatapoints{(x(1),O(1),s(1)),...,(x(P),O(P),s(P))}
withxbeingtherespectiveprimaryposesequence,O beingthecontextofother
persons, and s being the scene context.
x(i) =Norm (X(i)) ∈RJ×3×N (5)
i
O(i) =(cid:104) Norm i(X(j))(cid:12) (cid:12)∀j ̸=i(cid:105) ∈R(P−1)×J×3×N
s(i) =Norm i(S n) ∈RG×dObjMassively Multi-Person 3D Human Motion Forecasting with Scene Context 5
During the diffusion process, pose sequences are disturbed using noise from
anormaldistribution.Toachievedistributionalsimilaritybetweenthedataand
noisedistribution,weperformmin-max-scalingofxto[−3,3].Thisscalingmaps
all motion sequences in the training set to be with 3σ of a unit Gaussian thus
enabling the model to generate all motion present in the training data. We
find min-max-scaling to produce superior qualitative results compared to the
commonly used Normal scaling.
3.3 Diffusion Model
We model the generation of realistic motion sequences as denoising task using
diffusion models. Our model uses latent variables x ,...,x of equal dimen-
1 T
sionality with x ∼q(x ) being a noise-free single-person motion sequence. The
0 0
diffusionprocessq isaMarkovchainspecifyingx ,...,x asprogressivelymore
1 T
noisy versions of x . This can be expressed as closed formula
0
√
q(x |x )=N(x ; a¯ x ,(1−a¯ )I) (6)
t 0 t t 0 t
givensomevariancescheduleβ withα =1−β anda¯
=(cid:81)t
α .Thevariance
t t t t s=1 s
schedule β is chosen such that x ∼N(0,I) approximately holds.
t T
The reverse process p consists of learned denoising transitions p (x |x ).
θ θ t−1 t
Instead of directly learning p , we learn a function f (x ,C,t) predicting the
θ θ t
noise-free motion sequence xˆ . To condition the model on the input sequence
0
and context, we pass a context tuple
C =(x1:n,O,s) (7)
to f consisting of the noise-free input sequence x1:n, other person trajectories
θ
O and scene context s. Given a predicted xˆ , we calculate the reverse step as
0
p (x |x )=N(x ;µ˜ (x ,xˆ ),β˜I) with (8)
θ t−1 t t−1 t t 0 t
√ √
α¯ β α (1−α¯ )
µ˜ (x ,xˆ )= t−1 txˆ + t t−1 x and
t t 0 1−α¯ 0 1−α¯ t
t t
1−α¯
β˜ = t−1β .
t 1−α¯ t
t
Training Given a training datapoint (x,O,s), we use Equation 6 to calculate
the loss of our denoising model f as
θ
√ √
L=∥xn+1:N −f ( a¯ x+ 1−a¯ ϵ,C,t)n+1:N∥ (9)
θ t t
with random noise ϵ ∼ N(0,I) and timestep t ∼ Uniform(1...T). We exclude
padded frames (due to people entering or leaving the scene) from the loss cal-
culation. We use L1 loss, as it is more robust against outliers compared to L2.
This is relevant for human motion datasets, as joint positions are usually esti-
mated with motion capture algorithms [38], introducing some noise in the joint
locations.6 Mueller, Tanke, et al.
skip connections
Pose Pose
Encoder Decoder
Primary Conv Masked Masked Conv
Person Encoder Transformer Transformer Decoder
(noisy) Resblock Decoder Decoder Resblock
x 3 x 3
Scene Transformer P Pr eim rsa or ny
(BPS & Types) Encoder (denoised)
Scene Transformer
Person B
Conv Masked
Encoder Transformer
Resblock Encoder
Person C
Others Transformer
x 3
Others Encoder
Fig.2: Architecture of the denoising model f .
θ
Joint Multi-Person Inference To forecast a multi-person motion sequence, we
sample x ∼ N(0,I) and iteratively denoise it using f and Equation 8. As we
T θ
onlyseeX1:nduringinference,onlyO1:nisknownwhileOn+1:N isunknown.But
to allow for interdependent motion generation, it is crucial that the calculation
off (x(i),C(i),t)fortheprimarypersoniseesthewholemotionsequencesofall
θ t
otherpeopleO(i),1:N.Toallowthisimportantinformationflowbetweendifferent
persons, we perform all inference diffusion processes for X in parallel and de-
normalize the predicted noise-free sequences xˆ of all people from diffusion
0,t−1
step t−1 as
(cid:104) (cid:105)
Xˆ = Norm−1(xˆ(i) )|∀i∈{1,...,P} . (10)
t−1 i 0,t−1
WethenestimateO1:N foreverypersonibasedonXˆ usingEquation5.This
t t−1
O(i),1:N isthenusedinthenextinferencestept.Webootstrapthisprocessusing
t
zero-velocity padding of O1:n for t = 0. This approach provides an iteratively
improving estimation of the behavior of other people in the scene throughout
the diffusion inference.
3.4 Denoising Model
The UNet-style [34] denoising model
xˆ1:N =f (x1:N,(x1:n,O,s),t) (11)
0 θ t
consists of a temporal convolutional encoder e and a temporal convolutional
decoder d, as well as a Transformer-based aggregation module, which combines
pose and scene information in the bottleneck between encoder and decoder. See
Figure 2 for an overview.Massively Multi-Person 3D Human Motion Forecasting with Scene Context 7
Pose Encoder Following [3], we let e be a three-layer temporal convolutional
network,witheachlayerconsistingoftwocausalconvolutionalsubmoduleswith
residual connections. The diffusion step t is encoded using Gaussian Fourier
projection and fed into each convolutional layer. In each layer, we use strided
convolution to halve the temporal resolution.
Two encoder ex and eO are applied to the primary and other pose sequences
respectively. We add the noise-free input sequence x1:n to the noisy motion
sequence x1:N by performing zero-velocity padding to 1 : N and concatenating
t
on the body joint dimension. We use skip connections between each layer of ep
and each layer of d.
skip,h =ex(x1:n∥xn+1:N,t) ∈RN×D (12)
x t
h =[eO(O(1),t)∥...∥eO(O(P−1),t)]∈R((P−1)×N)×D
O
Aggregation Module The aggregation module consists of two Transformer [42]
modules combining h , h , and s.
x O
h=TDecs(h ,TEncs(s)) (13)
x
h′ =TDecO(h,TEncO(h ))
O
TDecs, TDecO, and TEncO(h ) use sinusoidal positional encodings [42] for
O
encoding the order of motion tokens and attention masking to ensure causality.
Pose Decoder The decoder d is a three-layer temporal convolutional network.
The diffusion step is supplied via Gaussian Fourier Projection and we use linear
upsampling in each layer to double the temporal resolution. Skip connections
from ex are incorporated in each layer. The noise-free output sequence is calcu-
lated as
xˆ =d(h′,skip,t). (14)
0
4 Experiments
Metrics Wefollowevaluationproceduresestablishedinpriorworkonlong-term
motion forecasting [8,39,40,44]. To measure local realism, i.e. realism on short
motionsnippets,weusethevelocity-basedNDMS [39]scoresandrealismscores
byamodeltrainedtodiscriminatebetweenrealandsyntheticmotionsamples[8].
We also use the distribution of trajectory lengths as an auxiliary measure for
global motion realism [44], i.e. overall realism of the whole predicted sequence.
We also perform a user study.
In addition to those metrics, we propose to use UMWR, a local metric for
motiondiversitycomplementingNDMS,andmean velocity over time asanother
auxiliary measure for global realism.8 Mueller, Tanke, et al.
Dataset HumansinKitchens[38]isa3Dmotioncapturedatasetofpeopleinter-
acting in a coffee kitchen environment with minimal instructions. The recording
time is 7h over four recordings with 90 unique persons in total and 1 to 16 per-
sonsvisiblesimultaneously.Eachofthefourrecordingstakesplaceinadifferent
coffeekitchen,hencehavingadifferentscenegeometrywith29to50objects.We
follow the evaluation protocol proposed by Tanke et al. [38], using the record-
ings A-C as training data and evaluating on a subset of sequences from D. The
Humans in Kitchens evaluation set focuses on transitional moments. Sequences
were selected so that the end of the input sequence marks the beginning of a
new action (sitting down, standing up, opening the fridge, etc.), which a model
is supposed to predict. The evaluation setup requires models to generalize to a
differentscenegeometrythanseenduringtraining,containingbothmoreobjects
(train:40,test:50)andmoresimultaneouslyvisiblepersons(train:14,test:16).
Baselines We follow [38] in their selection of baselines. MRT [44] is a purely
Transformer-basedarchitectureformulti-personmotionforecasting,whileSiMLPe
[12] and HisRep [26] are single-person models. TriPod [1] is a graph-attention-
based model for multi-person motion forecasting with scene context.
4.1 Model Training
We use Adam with weight decay [24] with a linear learning rate schedule from
2 × 10−7 to 5 × 10−5. We train for 680k training steps with a batch size of
32. For the diffusion process, we use T = 1,000 steps and the cosine variance
schedule [31]. As the pose types in Humans in Kitchens are heavily imbalanced,
weundersamplemotionsequencesthatcontainonlystandingposesby50%.This
removes 15% of the training data.
FortheSceneTransformer,weuse3encoderanddecoderlayerswithd =
Scene
256,d = 1024, and 8 heads. For the Others Transformer, we use 2 encoder
ff
and decoder layers with d = 128,d = 512, and 4 heads, since its input
Others ff
is preprocessed with the convolutional Others Encoder. Our model has 15.3M
parameters.
1
DCT FC U Le
R
t a c n
o
c
FC U Le
R
FC d iom
giS
0
per-joint trajectories per-joint
Fig.3: The realism scoring model calculates a score based on short single-person mo-
tion snippets. It is trained to distinguish real and synthetic motion, the latter is gen-
erated using our model and the baseline models.Massively Multi-Person 3D Human Motion Forecasting with Scene Context 9
Table 1: Local realism (Realism Score, NDMS) and diversity (UMWR) metrics. Re-
alism score at k is the mean realism score on [0,k).
Realism Score ×100 ↑ UMWR ↑
NDMS ↑
2s 4s 6s 8s 10s 2s 4s 6s 8s 10s
MRT 4.23 1.76 1.10 0.85 0.77 0.16 0.09 0.08 0.07 0.07 -
HisRep 8.35 1.10 0.58 0.40 0.30 0.23 0.12 0.07 0.06 0.06 0.06
SiMLPe 18.24 4.16 2.40 1.71 1.33 0.27 0.15 0.07 0.07 0.06 0.06
TriPod 3.39 0.37 0.22 0.17 0.14 0.13 0.18 0.14 0.12 0.11 0.11
Ours 5.75 2.80 2.88 2.55 2.40 0.17 0.41 0.21 0.15 0.14 0.15
4.2 Local Realism and Diversity
Classifier-based Realism Metric To judge the local realism of generated motion,
we train an evaluation model to distinguish real and synthetic motion. We train
on 50% real samples and 10% predicted sequences from SiMLPe, MRT, TriPod,
HisRep, and Ours each. As the classifier is equally well trained to detect syn-
thetic motion for each model, we can assume that a higher mean realism score
is indicative of a model producing more realistic motion than other models.
Theclassifierhasasimplefeed-forwardarchitecture,seeFigure3,andworks
on an input sequence length of 50 frames. See the supplementary material for
implementation details. The classifier has excellent classification performance.
On the validation set (92,997 samples) it achieves an accuracy of 0.997 and an
AreaundertheROCcurveof0.9994,i.e.themodelranksarandomrealsample
higher than a random synthetic sample with 99.94% probability.
NDMS Normalized Directional Motion Similarity [39,40] employs a set of short
real motion snippets as a reference set D for realistic motion. To evaluate a
prediction Xˆ1:N, it is split into short motion snippets x. Each x is matched to
the most similar x˜∈D and a velocity-based score on x and x˜ is calculated.
Following [39], we prepend each predicted sequence with a few frames of its
input sequence to also measure the realism of the transition between input and
predicted sequence. We build the reference set D from the test split. We report
the mean NDMS score over all frames in all predicted sequences.
UMWR We propose using the reference set D of NDMS for judging diversity as
well. The unique motion word ratio is calculated as
(cid:12) (cid:12)(cid:8) NN(χ1:κ),NN(χ2:κ+1),...NN(χ|χ|+1−κ:|χ|)(cid:9)(cid:12)
(cid:12)
UMWR(χ)= (15)
|χ|+1−κ
whereκ=8framesisthemotionsnippetlength,χ=X(i),n+2−κ:N isthesingle-
personsequencewewanttoevaluate(outputsequenceandthelastframesofthe
input sequence), and NN(x) is the function to map x to the most similar x˜∈D.
UMWR scores are in [0,1], with a low UMWR implying that the motion is
less diverse as most of the generated motion is within a very small region in the10 Mueller, Tanke, et al.
GT Ours MRT HisRep SiMLPe TriPod
1.5
1.0
0.5
0.0
0.5
1.0
1.5
1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1
Fig.4: Visualization of ten-second output trajectories rn:N for each model. The last
frame of the input sequences is normalized to (0,0) with persons facing in positive
y-direction. 20 randomly selected trajectories per model displayed.
possible motion space, namely close to a few motion words in D. To calculate
UMWR on subsequences, we use UMWR@ks = UMWR(χ25(k−1):25k) as the
frame rate is 25 frames per second.
Results Table 1 contains the local realism and diversity results. We observe
that our model performs best for sequences of six seconds or longer. SiMLPe
performsverygoodforshortsequences,butperformancerapidlydropsforlonger
sequences.OnNDMS,ourmodelscoresworsethanSiMLPeandHisRep.Interms
ofdiversity,ourmodelperformsbest.TheUMWRscoredecreasesafterthefirst
seconds, but it is consistently higher than for other baseline models.
Table 2: Comparisonbetweenourmod- Table3:Comparisonofthedistribution
elsandfourbaselinesintheuserranking. of trajectory lengths between ground
Users were asked to rank all models and truth and models. For each distribu-
the ground truth from most (1) to least tion, we give the mean, standard devia-
(6)realisticandweprovidethemeanand tion,andWassersteindistanceW tothe
1
standarddeviationbasedon43rankings. ground truth distribution. Best Wasser-
See the supplementary material for de- stein distance and closest mean to the
tails. ground truth is highlighted.
User Ranking Trajectory Dist.
Mean ↓ Std Mean Std W ↓
1
Ground Truth 1.30 0.74 Ground Truth 1.17 1.70 -
MRT [44] 4.44 1.01 MRT 0.20 0.36 0.96
HisRep [26] 4.98 0.96 HisRep 0.26 0.44 0.91
SiMLPe [12] 3.62 1.25 SiMLPe 0.26 0.39 0.91
TriPod [1] 4.37 1.48 TriPod 1.16 0.54 0.72
Ours 2.23 1.02 Ours 0.86 0.62 0.57Massively Multi-Person 3D Human Motion Forecasting with Scene Context 11
0.4
Ground Truth
MRT
0.3 HisRep
SiMLPe
TriPod
0.2
Ours
0.1
0.0
0 50 100 150 200 250
output frames
Fig.5:Frame-wisemeanglobalvelocityforallmodels.Wecalculatethevelocityofthe
hip center in the x- and y-direction and average over all evaluation samples. Outliers
in the ground truth data (few single-frame velocities over 10 m/s) are clipped before
averaging.
4.3 Global Realism and Diversity
To judge the whether models create realistic global behaviour, we compare the
distributionoftrajectoriesgeneratedbymodelstothedistributionoftrajectories
in the ground truth dataset. For each motion sequence X = [j ,j ,...,j ],
1,1 1,2 J,3
we define the 2D root trajectory r as
(cid:20) (cid:21)
(j +j )/2
r = 13,1 14,1 ∈RN×2 (16)
(j +j )/2
13,2 14,2
with j and j being the left and right hip joint.
13 14
For qualitative judgement, we visualize a random subset of trajectories, see
Figure4.Wefindthatourmodelproducestrajectoriesthatarevisuallysimilarto
thegroundtruth.Realistictrajectoriesseemtofollowarandom-walklikepattern
withvaryingtrajectorylengths.TriPoddoesnotproduceshorttrajectories,only
long and regular ones. This is caused by TriPod generating a constant shifting
motion, see the supplemantary material for example videos. We also visualize
the velocity over time for all models (mean over all test samples) in Figure 5.
TriPod matches the ground truth mean velocity most closely, however this is
due to constant shifting motion in its predictions. Our model produces faster-
than-realisticmotioninthefirstthreesecondsandproducesslower-than-realistic
motionafterwards.Otherbaselinesproducehardlyanymotionaftertwoseconds.
A direct quantitative analysis of the trajectory distribution is not feasible
because of the high dimensionality and limited size of the test set. Instead we
analyze the distribution of trajectory length
N
1 (cid:88)
d= ∥rt−rt−1 ∥ . (17)
N −n 2
t=n+1
)s/m(
yticolev
naem12 Mueller, Tanke, et al.
(a) (b) (c) (d)
Fig.6: Samples of Ours creating diverse motion based on a fixed input. In the input
sequence , a person starts to stand up. a) In the ground truth ( to ), the person
kneelsonthesofa towriteonthewhiteboard .b–d)Ours( to )predictswriting
on the whiteboard twice, once stepping on and once stepping over the sofa. The third
prediction shows hesitant standing up motion.
Let µ be the probability distributions over trajectory lengths for the ground
truth and let ν be the distribution for a model M. We sample from µ and ν
M M
by calculating the trajectory length d for all output sequences in the test set
(both ground truth and generated by M). Using this samples, we estimate the
Wasserstein distance between µ and ν , see Table 3. We find that our model
M
resembles the ground truth distribution the most.
4.4 User Study and Qualitative Evaluation
Coveringallaspectsofrealisticmotionwithmetricsistrickyandpriorworkhas
repeatedly reported discrepancies between metrics and subjective evaluation by
users [7,29,41]. We therefore perform a user study to judge the realism of gen-
erated human motion. In the user study, we let humans rank model predictions
as well as ground truth sequences from most to least realistic. Model names
were blinded and we collected 43 rankings from 10 persons. Samples used in the
user study and detailed results can be found in the supplementary material. We
report the results of the user study in Table 2. Our model is ranked highest,
followed by SiMLPe.
This result is consistent with our qualitative observations. SiMLPe is able
to produce the longest smooth continuation of the input sequence among the
baselines, but freezes in an average pose after around two seconds. MRT some-
times generates realistic global motion, but mostly freezes similar to SiMLPe
and HisRep. TriPod produces an unrealistic slow drifting motion for all people
in the scene.
Our model is able to produce a wide variety of realistic motion, like sitting
down, standing up, or directed movement towards an object in the scene. It
alsoproducesrealisticobjectinteractionswithe.g.cupboards,fridges,orwhite-
boards.Promptingourmodelmultipletimesgeneratesmultiplerealisticsamples
from p(Xn+1:N|X1:n,Sn). Figure 6 shows a qualitative example.Massively Multi-Person 3D Human Motion Forecasting with Scene Context 13
But qualitative evaluation also shows some weaknesses of our model: It pro-
duces more high-velocity motion in the first frames of the predicted sequence
than the ground truth would suggest, sometimes causing visible discontinuities
to the input sequence. Motion sequences with long global motion also tend to
have less realistic local limb movement, e.g. missing leg movement.
Table 4: Our model with scene context and other person encoder ablated. Best re-
sult or closest mean trajectory length to the ground truth is bold, the second best
is underlined. Note that the evaluation metrics do not explicitly take human-human
and human-scene interactions into account. We thus also provide a quick overview on
perceived visual quality, more details in Section 4.5
Trajectory Dist. Visual
NDMS ↑ UMWR ↑ Mean Std W ↓ Quality
1
Ground Truth - - 1.17 1.70 -
Ours 0.17 0.20 0.86 0.62 0.57
w/o Scene Context 0.14 0.16 0.63 0.51 0.59 ∼
w/o Other People 0.16 0.19 0.93 0.98 0.36
4.5 Ablations
To study the impact of multi-person and scene context, we ablate the other
person encoder and the scene encoder respectively. We ablate a module by re-
placing its output with constant all-zero vectors during training and inference.
This produces attention weights of zero and thus effectively ignores the module
in the decoder.
WeprovideaquantitativeanalysisinTable4.Asnoneoftheexistingmetrics
are able to explicitly judge human-human or human-scene interactions, we also
provide a qualitative analysis here. Samples of all models are provided in the
supplementary material.
SceneContext Withoutscenecontext,thequalityofhuman-sceneinteractionsis
reduced.Themodeldoesnotproducedirectedmotionwithacleargoalanymore,
i.e. walking to a cupboard and stopping there. While the model is still able to
produce sensible standing up and sitting down motion, sitting down without a
chair beneath a person happens more frequently. This is plausible, as the the
beginningofasittingdownorstandingupmotionisusuallyapparentattheend
of the input sequence, but precise scene information is missing. The evaluation
setofHumansinKitchensisexplicitlydesignedtofocusontransitionalmoments
based on action labels.14 Mueller, Tanke, et al.
Other People Wedonotseeadeteriorationofmotionrealismwhenablatingthe
other person encoder. However, without the other person encoder, our model
is unable to produce synchronized multi-person motion anymore. Our model
sometimes predicts that 2-3 persons sitting next to each other get up even if
the start of the standing up motion is only apparent in the input sequence of
one person, which does not happen when the other person encoder is ablated.
This shows that the joint inference procedure we are using allows to model
interdependent motion and that the information flow between multiple persons
has a precise temporal and spatial resolution.
5 Conclusion
In this work, we propose our scene-aware social transformer model (SAST) that
forecastslong-termhumanmotionconditionedonbothmulti-personinteractions
andscenecontext.Ourarchitecturecombinescausalconvolutionalposeencoder
and decoder with a Transformer-based bottleneck that allows to model interac-
tions between objects and persons on a far larger scale than previously done.
We ablate these context information and see that both scene and multi-person
context provide relevant information for realistic multi-person motion genera-
tion. Our inference procedure can produce highly-synchronized interdependent
motion without the need for jointly forwarding all input sequences through the
model,whichisinvaluableforlargeorvaryingnumbersofpeopleinascene.We
evaluate our approach on the Humans in Kitchens which requires modelling of
widely varying numbers of people and objects in a scene and achieve very good
performance compared to other approaches. However, our model still has limi-
tations: The continuity between input and predicted sequence can be improved
and limb movement realism drops during long global motion.
Future Work To model realistic long-term human motion, we need to efficiently
combineallavailablecontextinformation.Weshouldworktowardflexiblemulti-
modal models that are able to reconcile more context information and guiding
signals like objects, action labels, speech, etc.
Eventhoughhuman-humanandhuman-sceneinteractionsareacentralcom-
ponent of realism for human observers, current realism metrics do not capture
those aspects. To purposefully work toward realistic long-term motion forecast-
ing, we need to improve evaluation protocols to also include environment inter-
actions.
Acknowledgments
This publication was funded by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation) - Project-ID 454648639 - SFB 1528, 1927/5-2
(FOR 2535 Anticipating Human Behavior), and the ERC Consolidator Grant
FORHUE (101044724).Massively Multi-Person 3D Human Motion Forecasting with Scene Context 15
References
1. Adeli, V., Ehsanpour, M., Reid, I.D., Niebles, J.C., Savarese, S., Adeli, E.,
Rezatofighi, H.: Tripod: Human trajectory and pose dynamics forecasting in the
wild. In: 2021 IEEE/CVF International Conference on Computer Vision, ICCV
2021,Montreal,QC,Canada,October10-17,2021.pp.13370–13380.IEEE(2021).
https://doi.org/10.1109/ICCV48922.2021.01314, https://doi.org/10.1109/
ICCV48922.2021.01314
2. Aksan, E., Kaufmann, M., Cao, P., Hilliges, O.: A spatio-temporal transformer
for 3d human motion prediction. In: International Conference on 3D Vision, 3DV
2021, London, United Kingdom, December 1-3, 2021. pp. 565–574. IEEE (2021).
https://doi.org/10.1109/3DV53792.2021.00066, https://doi.org/10.1109/
3DV53792.2021.00066
3. Bai, S., Kolter, J.Z., Koltun, V.: An empirical evaluation of generic convolutional
and recurrent networks for sequence modeling. CoRR abs/1803.01271 (2018),
http://arxiv.org/abs/1803.01271
4. Barquero, G., Escalera, S., Palmero, C.: Belfusion: Latent diffusion for behavior-
driven human motion prediction. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 2317–2327 (2023)
5. Barquero,G.,Escalera,S.,Palmero,C.:Seamlesshumanmotioncompositionwith
blended positional encodings. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 457–469 (2024)
6. Butepage, J., Black, M.J., Kragic, D., Kjellstrom, H.: Deep representation learn-
ing for human motion prediction and classification. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 6158–6166 (2017)
7. Dabral, R., Mughal, M.H., Golyanik, V., Theobalt, C.: Mofusion: A framework
for denoising-diffusion-based motion synthesis. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 9760–9770 (2023)
8. Diller,C.,Funkhouser,T.,Dai,A.:Futurehuman3d:Forecastingcomplexlong-term
3d human behavior from video observations. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). pp. 19902–
19914 (June 2024)
9. Fan,K.,Tang,J.,Cao,W.,Yi,R.,Li,M.,Gong,J.,Zhang,J.,Wang,Y.,Wang,C.,
Ma,L.:Freemotion:Aunifiedframeworkfornumber-freetext-to-motionsynthesis.
arXiv preprint arXiv:2405.15763 (2024)
10. Fragkiadaki,K.,Levine,S.,Felsen,P.,Malik,J.:Recurrentnetworkmodelsforhu-
mandynamics.In:ProceedingsoftheIEEEinternationalconferenceoncomputer
vision. pp. 4346–4354 (2015)
11. Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., Cheng, L.: Generating di-
verseandnatural3dhumanmotionsfromtext.In:ProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 5152–5161 (2022)
12. Guo, W., Du, Y., Shen, X., Lepetit, V., Alameda-Pineda, X., Moreno-Noguer, F.:
Backtomlp:Asimplebaselineforhumanmotionprediction.In:Proceedingsofthe
IEEE/CVFWinterConferenceonApplicationsofComputerVision.pp.4809–4819
(2023)
13. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In:
Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Ad-
vances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual (2020), https://proceedings.neurips.cc/paper/2020/hash/
4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html16 Mueller, Tanke, et al.
14. Holden, D., Saito, J., Komura, T., Joyce, T.: Learning motion manifolds with
convolutional autoencoders. In: SIGGRAPH Asia 2015 technical briefs, pp. 1–4
(2015)
15. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-rnn: Deep learning on
spatio-temporalgraphs.In:Proceedingsoftheieeeconferenceoncomputervision
and pattern recognition. pp. 5308–5317 (2016)
16. Jeong,J.,Park,D.,Yoon,K.J.:Multi-agentlong-term3dhumanposeforecasting
via interaction-aware trajectory conditioning. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 1617–1628 (2024)
17. Kantorovitch, J., Väre, J., Pehkonen, V., Laikari, A., Seppälä, H.: An assistive
household robot–doing more than just cleaning. Journal of Assistive Technologies
8(2), 64–76 (2014)
18. Kidziński, Ł., Yang, B., Hicks, J.L., Rajagopal, A., Delp, S.L., Schwartz, M.H.:
Deep neural networks enable quantitative movement analysis using single-camera
videos. Nature communications 11(1), 4054 (2020)
19. Levinson, S.C., Torreira, F.: Timing in turn-taking and its implications for pro-
cessing models of language. Frontiers in psychology 6, 731 (2015)
20. Li,B.,Ho,E.S.,Shum,H.P.,Wang,H.:Two-personinteractionaugmentationwith
skeletonpriors.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition. pp. 1900–1910 (2024)
21. Li,C.,Zhang,Z.,Lee,W.S.,Lee,G.H.:Convolutionalsequencetosequencemodel
for human dynamics. In: Proceedings of the IEEE conference on computer vision
and pattern recognition. pp. 5226–5234 (2018)
22. Li, Z., Zhou, Y., Xiao, S., He, C., Huang, Z., Li, H.: Auto-conditioned recur-
rent networks for extended complex human motion synthesis. arXiv preprint
arXiv:1707.05363 (2017)
23. Liang,H.,Zhang,W.,Li,W.,Yu,J.,Xu,L.:Intergen:Diffusion-basedmulti-human
motiongenerationundercomplexinteractions.InternationalJournalofComputer
Vision pp. 1–21 (2024)
24. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: 7th Inter-
national Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net (2019), https://openreview.net/forum?
id=Bkg6RiCqY7
25. Lucas, T., Baradel, F., Weinzaepfel, P., Rogez, G.: Posegpt: Quantization-based
3dhumanmotiongenerationandforecasting.In:Avidan,S.,Brostow,G.J.,Cissé,
M., Farinella, G.M., Hassner, T. (eds.) Computer Vision - ECCV 2022 - 17th
European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part
VI. Lecture Notes in Computer Science, vol. 13666, pp. 417–435. Springer (2022).
https://doi.org/10.1007/978-3-031-20068-7_24, https://doi.org/10.1007/
978-3-031-20068-7_24
26. Mao, W., Liu, M., Salzmann, M.: History repeats itself: Human motion predic-
tion via motion attention. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J. (eds.)
Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, Au-
gust 23-28, 2020, Proceedings, Part XIV. Lecture Notes in Computer Science,
vol. 12359, pp. 474–489. Springer (2020). https://doi.org/10.1007/978-3-030-
58568-6_28, https://doi.org/10.1007/978-3-030-58568-6_28
27. Mao,W.,Liu,M.,Salzmann,M.,Li,H.:Learningtrajectorydependenciesforhu-
man motion prediction. In: 2019 IEEE/CVF International Conference on Com-
puter Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,
2019.pp.9488–9496.IEEE(2019).https://doi.org/10.1109/ICCV.2019.00958,
https://doi.org/10.1109/ICCV.2019.00958Massively Multi-Person 3D Human Motion Forecasting with Scene Context 17
28. Martinez,J.,Black,M.J.,Romero,J.:Onhumanmotionpredictionusingrecurrent
neural networks. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 2891–2900 (2017)
29. Mughal,M.H.,Dabral,R.,Habibie,I.,Donatelli,L.,Habermann,M.,Theobalt,C.:
Convofusion:Multi-modalconversationaldiffusionforco-speechgesturesynthesis.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 1388–1398 (June 2024)
30. Ng,E.,Romero,J.,Bagautdinov,T.,Bai,S.,Darrell,T.,Kanazawa,A.,Richard,
A.: From audio to photoreal embodiment: Synthesizing humans in conversations.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 1001–1010 (June 2024)
31. Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion probabilistic models.
In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Confer-
ence on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. Pro-
ceedings of Machine Learning Research, vol. 139, pp. 8162–8171. PMLR (2021),
http://proceedings.mlr.press/v139/nichol21a.html
32. Peng,X.,Shen,Y.,Wang,H.,Nie,B.,Wang,Y.,Wu,Z.:Somoformer:Social-aware
motion transformer for multi-person motion prediction. CoRR abs/2208.09224
(2022). https://doi.org/10.48550/arXiv.2208.09224, https://doi.org/10.
48550/arXiv.2208.09224
33. Prokudin,S.,Lassner,C.,Romero,J.:Efficientlearningonpointcloudswithbasis
point sets. In: 2019 IEEE/CVF International Conference on Computer Vision,
ICCV2019,Seoul,Korea(South),October27-November2,2019.pp.4331–4340.
IEEE (2019). https://doi.org/10.1109/ICCV.2019.00443, https://doi.org/
10.1109/ICCV.2019.00443
34. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: Navab, N., Hornegger, J., III, W.M.W., Frangi, A.F.
(eds.) Medical Image Computing and Computer-Assisted Intervention - MIC-
CAI 2015 - 18th International Conference Munich, Germany, October 5 - 9,
2015, Proceedings, Part III. Lecture Notes in Computer Science, vol. 9351, pp.
234–241. Springer (2015). https://doi.org/10.1007/978-3-319-24574-4_28,
https://doi.org/10.1007/978-3-319-24574-4_28
35. Rösmann, C., Oeljeklaus, M., Hoffmann, F., Bertram, T.: Online trajectory pre-
dictionandplanningforsocialrobotnavigation.In:2017IEEEInternationalCon-
ferenceonAdvancedIntelligentMechatronics(AIM).pp.1255–1260.IEEE(2017)
36. Sacks,H.,Schegloff,E.A.,Jefferson,G.:Asimplestsystematicsfortheorganization
of turn-taking for conversation. language 50(4), 696–735 (1974)
37. Schmidt,R.A.:Anticipationandtiminginhumanmotorperformance.Psycholog-
ical Bulletin 70(6p1), 631 (1968)
38. Tanke, J., Kwon, O., Mueller, F.B., Doering, A., Gall, J.: Humans in kitchens:
A dataset for multi-person human motion forecasting with scene context. In:
Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.)
Advances in Neural Information Processing Systems 36: Annual Conference on
Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
USA, December 10 - 16, 2023 (2023), http://papers.nips.cc/paper_files/
paper/2023/hash/2052b3e0617ecb2ce9474a6feaf422b3-Abstract-Datasets_
and_Benchmarks.html
39. Tanke, J., Zaveri, C., Gall, J.: Intention-based long-term human motion anticipa-
tion. In: 2021 International Conference on 3D Vision (3DV). pp. 596–605. IEEE
(2021)18 Mueller, Tanke, et al.
40. Tanke,J.,Zhang,L.,Zhao,A.,Tang,C.,Cai,Y.,Wang,L.,Wu,P.,Gall,J.,Keskin,
C.:Socialdiffusion:Long-termmultiplehumanmotionanticipation.In:IEEE/CVF
InternationalConferenceonComputerVision,ICCV2023,Paris,France,October
1-6, 2023. pp. 9567–9577. IEEE (2023). https://doi.org/10.1109/ICCV51070.
2023.00880, https://doi.org/10.1109/ICCV51070.2023.00880
41. Tseng, J., Castellon, R., Liu, K.: Edge: Editable dance generation from music.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 448–458 (2023)
42. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
L., Polosukhin, I.: Attention is all you need. In: Guyon, I., von Luxburg, U.,
Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Garnett, R. (eds.)
Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
CA,USA.pp.5998–6008(2017),https://proceedings.neurips.cc/paper/2017/
hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
43. Vendrow,E.,Kumar,S.,Adeli,E.,Rezatofighi,H.:Somoformer:Multi-personpose
forecastingwithtransformers.CoRRabs/2208.14023(2022).https://doi.org/
10.48550/arXiv.2208.14023, https://doi.org/10.48550/arXiv.2208.14023
44. Wang, J., Xu, H., Narasimhan, M., Wang, X.: Multi-person 3d motion pre-
diction with multi-range transformers. In: Ranzato, M., Beygelzimer, A.,
Dauphin, Y.N., Liang, P., Vaughan, J.W. (eds.) Advances in Neural In-
formation Processing Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual.
pp. 6036–6049 (2021), https://proceedings.neurips.cc/paper/2021/hash/
2fd5d41ec6cfab47e32164d5624269b1-Abstract.html
A Example Videos
Please find example videos and information on the user study at
https://github.com/felixbmuller/SAST
The videos in our supplementary material are structured as follows:
User Study By Model All videos used in the user study, sorted by models
(During the actual user study, they were sorted by categories, e.g. coffee
machine, sitting down, fridge, and not by model)
Examples Ours Exampleoutputsrepresentativeofourmodel.Weshowmulti-
pleoutputsforthesameinputtohighlightthatourmodelallowstodirectly
sample multiple realistic continuations (same file name, followed by _sX
Ablations Videos of our NoScene and NoOthers ablations, along with the out-
put of our base model on the same input sequence.
Baselines Example outputs representative of the baseline models
Most filenames follow the schema
CATEGORY_NUMBER[_sX]_[Model]_Text.mp4.Massively Multi-Person 3D Human Motion Forecasting with Scene Context 19
This means that the video was generated using the NUMBERth input sequence of
the evaluation category CATEGORY. This uniquely identifies an input sequence,
i.e.sampleswiththesameCATEGORY_NUMBERweregeneratedonthesameinput.
_sX is a running number, if we sampled multiple outputs for the same input.
Note: All videos show 1 second of input motion followed by 10 seconds of
output motion. Videos with a runtime of 6 seconds are double speed.
Body Half
Left Right
Ground Truth cornflowerblue salmon
Predicted green orange
Table 5: Color scheme for poses in videos
B Implementation Details
B.1 Our Architecture
See Figure 8 for a detailed illustration of the encoder and decoder blocks. We
choose a kernel size of 5 for the Pose Encoder and Pose Decoder, and a kernel
size of 3 for the Others Encoder. We use 32 groups for group normalization
throughout the model.
B.2 Realism Classifier
To reduce input dimensionality, the first fully connected layer uses weight shar-
ing, i.e. the same weights are applied for each body joint. Given a normalized
single-person motion sequence X = [j ,j ,...,j ] with joint trajectories
1,1 1,2 J,3
j ∈RN, RealismClassifier calculates a realism score s∈[0,1] as
i,k
j∗ =DCT(j ) ∀j ∈X
i,k i,k i,k
e =ReLU(FC(1)(j∗ ∥j∗ ∥j∗ )) ∀i∈{1,...,J}
i i,1 i,2 i,3
h=ReLU(FC(2)(e ∥···∥e ))
1 J
s=Sigmoid(FC(3)(h))
We choose a fixed sequence length of N = 50 frames and a hidden dimen-
sionality of 32 for e and 512 for h. Thus, the RealismClassifier has 284,000
i
parameters. We train the model using Adam with weight decay (source) and a
learning rate of 10−3. We train for 6 epochs using a batch size of 16 and bi-
narycrossentropyloss.Ourtrainingdataconsistsof141,299realsequencesand
138,171 generated sequences of our model, SiMLPe, HisRep, TriPod, and MRT.
Duringmodelevaluation,weuseonlymodeloutputthatwasnotusedduring
training of the RealismClassifier.20 Mueller, Tanke, et al.
Fig.7: Scene geometry of the kitchen of the evaluation set D. Walls are light gray .
Tables, drawers, and cupboards are gray . Chairs and sofas are red . Less common
objects (present at least once in each kitchen) include whiteboards (orange ), coffee
machines (blue ), dishwashers (light blue ), sinks (dark blue ), and microwaves
(black ).Thefiguredoesnotcoverthewholesurfaceareaperkitchen,butallobjects
are (partially) visible. There are 50 objects in this scene.Massively Multi-Person 3D Human Motion Forecasting with Scene Context 21
skip
timestep input connection
timestep input
Upsample
Convolution
WeightNorm
ReLU
Convolution
GroupNorm
Gaussian WeightNorm
Dropout
Fourier ReLU
Projection skip Gaussian
connection GroupNorm
Fourier
Linear Projection Dropout
Linear
Strided
Convolution
WeightNorm
Convolution
ReLU
WeightNorm
GroupNorm
ReLU
Dropout
GroupNorm
Dropout
1x1
Conv
1x1
Conv
output
output
Fig.8: Detailed architecture of one encoder block (left) and decoder block (right).
C Additional Metrics
See Table 6 for detailed realism scores.
Realism Score ×100 ↑
2s 3s 4s 5s 6s 7s 8s 9s 10s
SiMLPe 18.24 6.76 4.16 3.06 2.40 2.00 1.71 1.50 1.33
MRT 4.23 2.75 1.76 1.33 1.10 0.95 0.85 0.77 0.77
TriPod 3.39 0.64 0.37 0.27 0.22 0.19 0.17 0.15 0.14
HisRep 8.35 2.00 1.10 0.76 0.58 0.47 0.40 0.34 0.30
Ours 5.75 2.67 2.80 2.99 2.88 2.66 2.55 2.46 2.40
Table 6: Realism scores for different models and output sequence lengths from two
to ten seconds. For sequences longer than two seconds, we calculate the mean over all
two-second subsequences with an offset of 5 frames.