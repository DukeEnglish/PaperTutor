[
    {
        "title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?",
        "authors": "Zhengbo WangJian Liang",
        "links": "http://arxiv.org/abs/2407.18242v1",
        "entry_id": "http://arxiv.org/abs/2407.18242v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18242v1",
        "summary": "Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the \"equivalent gradient.\" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.",
        "updated": "2024-07-25 17:57:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18242v1"
    },
    {
        "title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
        "authors": "Yuxiao QuTianjun ZhangNaman GargAviral Kumar",
        "links": "http://arxiv.org/abs/2407.18219v1",
        "entry_id": "http://arxiv.org/abs/2407.18219v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18219v1",
        "summary": "A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions.",
        "updated": "2024-07-25 17:35:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18219v1"
    },
    {
        "title": "Exploring Scaling Trends in LLM Robustness",
        "authors": "Nikolhaus HoweMichał ZajacIan McKenzieOskar HollinsworthTom TsengPierre-Luc BaconAdam Gleave",
        "links": "http://arxiv.org/abs/2407.18213v1",
        "entry_id": "http://arxiv.org/abs/2407.18213v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18213v1",
        "summary": "Language model capabilities predictably improve from scaling a model's size\nand training data. Motivated by this, increasingly large language models have\nbeen trained, yielding an array of impressive capabilities. Yet these models\nare vulnerable to adversarial prompts, such as \"jailbreaks\" that hijack models\nto perform undesired behaviors, posing a significant risk of misuse. Prior work\nindicates that computer vision models become more robust with model and data\nscaling, raising the question: does language model robustness also improve with\nscale? We study this question empirically, finding that larger models respond\nsubstantially better to adversarial training, but there is little to no benefit\nfrom model scale in the absence of explicit defenses.",
        "updated": "2024-07-25 17:26:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18213v1"
    },
    {
        "title": "Differentiable Quantum Architecture Search in Asynchronous Quantum Reinforcement Learning",
        "authors": "Samuel Yen-Chi Chen",
        "links": "http://arxiv.org/abs/2407.18202v1",
        "entry_id": "http://arxiv.org/abs/2407.18202v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18202v1",
        "summary": "The emergence of quantum reinforcement learning (QRL) is propelled by\nadvancements in quantum computing (QC) and machine learning (ML), particularly\nthrough quantum neural networks (QNN) built on variational quantum circuits\n(VQC). These advancements have proven successful in addressing sequential\ndecision-making tasks. However, constructing effective QRL models demands\nsignificant expertise due to challenges in designing quantum circuit\narchitectures, including data encoding and parameterized circuits, which\nprofoundly influence model performance. In this paper, we propose addressing\nthis challenge with differentiable quantum architecture search (DiffQAS),\nenabling trainable circuit parameters and structure weights using\ngradient-based optimization. Furthermore, we enhance training efficiency\nthrough asynchronous reinforcement learning (RL) methods facilitating parallel\ntraining. Through numerical simulations, we demonstrate that our proposed\nDiffQAS-QRL approach achieves performance comparable to manually-crafted\ncircuit architectures across considered environments, showcasing stability\nacross diverse scenarios. This methodology offers a pathway for designing QRL\nmodels without extensive quantum knowledge, ensuring robust performance and\nfostering broader application of QRL.",
        "updated": "2024-07-25 17:11:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18202v1"
    },
    {
        "title": "Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning",
        "authors": "Sindhura KommuYizhi WangYue WangXuan Wang",
        "links": "http://arxiv.org/abs/2407.18181v1",
        "entry_id": "http://arxiv.org/abs/2407.18181v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18181v1",
        "summary": "Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing\n(scRNA-seq) data is a complex challenge that requires capturing the intricate\nrelationships between genes and their regulatory interactions. In this study,\nwe tackle this challenge by leveraging the single-cell BERT-based pre-trained\ntransformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to\naugment structured biological knowledge from existing GRNs. We introduce a\nnovel joint graph learning approach that combines the rich contextual\nrepresentations learned by pre-trained single-cell language models with the\nstructured knowledge encoded in GRNs using graph neural networks (GNNs). By\nintegrating these two modalities, our approach effectively reasons over boththe\ngene expression level constraints provided by the scRNA-seq data and the\nstructured biological knowledge inherent in GRNs. We evaluate our method on\nhuman cell benchmark datasets from the BEELINE study with cell type-specific\nground truth networks. The results demonstrate superior performance over\ncurrent state-of-the-art baselines, offering a deeper understanding of cellular\nregulatory mechanisms.",
        "updated": "2024-07-25 16:42:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18181v1"
    }
]