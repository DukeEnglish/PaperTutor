[
    {
        "title": "Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal Models: An Empirical Analysis",
        "authors": "Cristian-Alexandru BotocanRaphael MeierLjiljana Dolamic",
        "links": "http://arxiv.org/abs/2407.18251v1",
        "entry_id": "http://arxiv.org/abs/2407.18251v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18251v1",
        "summary": "Assessing the robustness of multimodal models against adversarial examples is\nan important aspect for the safety of its users. We craft L0-norm perturbation\nattacks on the preprocessed input images. We launch them in a black-box setup\nagainst four multimodal models and two unimodal DNNs, considering both targeted\nand untargeted misclassification. Our attacks target less than 0.04% of\nperturbed image area and integrate different spatial positioning of perturbed\npixels: sparse positioning and pixels arranged in different contiguous shapes\n(row, column, diagonal, and patch). To the best of our knowledge, we are the\nfirst to assess the robustness of three state-of-the-art multimodal models\n(ALIGN, AltCLIP, GroupViT) against different sparse and contiguous pixel\ndistribution perturbations. The obtained results indicate that unimodal DNNs\nare more robust than multimodal models. Furthermore, models using CNN-based\nImage Encoder are more vulnerable than models with ViT - for untargeted\nattacks, we obtain a 99% success rate by perturbing less than 0.02% of the\nimage area.",
        "updated": "2024-07-25 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18251v1"
    },
    {
        "title": "VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads",
        "authors": "Orest KupynEugene KhvedcheniaChristian Rupprecht",
        "links": "http://arxiv.org/abs/2407.18245v1",
        "entry_id": "http://arxiv.org/abs/2407.18245v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18245v1",
        "summary": "Human head detection, keypoint estimation, and 3D head model fitting are\nimportant tasks with many applications. However, traditional real-world\ndatasets often suffer from bias, privacy, and ethical concerns, and they have\nbeen recorded in laboratory environments, which makes it difficult for trained\nmodels to generalize. Here, we introduce VGGHeads -- a large scale synthetic\ndataset generated with diffusion models for human head detection and 3D mesh\nestimation. Our dataset comprises over 1 million high-resolution images, each\nannotated with detailed 3D head meshes, facial landmarks, and bounding boxes.\nUsing this dataset we introduce a new model architecture capable of\nsimultaneous heads detection and head meshes reconstruction from a single image\nin a single step. Through extensive experimental evaluations, we demonstrate\nthat models trained on our synthetic data achieve strong performance on real\nimages. Furthermore, the versatility of our dataset makes it applicable across\na broad spectrum of tasks, offering a general and comprehensive representation\nof human heads. Additionally, we provide detailed information about the\nsynthetic data generation pipeline, enabling it to be re-used for other tasks\nand domains.",
        "updated": "2024-07-25 17:58:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18245v1"
    },
    {
        "title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?",
        "authors": "Zhengbo WangJian Liang",
        "links": "http://arxiv.org/abs/2407.18242v1",
        "entry_id": "http://arxiv.org/abs/2407.18242v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18242v1",
        "summary": "Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the \"equivalent gradient.\" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.",
        "updated": "2024-07-25 17:57:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18242v1"
    },
    {
        "title": "Numerical Literals in Link Prediction: A Critical Examination of Models and Datasets",
        "authors": "Moritz BlumBasil EllHannes IllPhilipp Cimiano",
        "links": "http://arxiv.org/abs/2407.18241v1",
        "entry_id": "http://arxiv.org/abs/2407.18241v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18241v1",
        "summary": "Link Prediction(LP) is an essential task over Knowledge Graphs(KGs),\ntraditionally focussed on using and predicting the relations between entities.\nTextual entity descriptions have already been shown to be valuable, but models\nthat incorporate numerical literals have shown minor improvements on existing\nbenchmark datasets. It is unclear whether a model is actually better in using\nnumerical literals, or better capable of utilizing the graph structure. This\nraises doubts about the effectiveness of these methods and about the\nsuitability of the existing benchmark datasets.\n  We propose a methodology to evaluate LP models that incorporate numerical\nliterals. We propose i) a new synthetic dataset to better understand how well\nthese models use numerical literals and ii) dataset ablations strategies to\ninvestigate potential difficulties with the existing datasets. We identify a\nprevalent trend: many models underutilize literal information and potentially\nrely on additional parameters for performance gains. Our investigation\nhighlights the need for more extensive evaluations when releasing new models\nand datasets.",
        "updated": "2024-07-25 17:55:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18241v1"
    },
    {
        "title": "Automated Ensemble Multimodal Machine Learning for Healthcare",
        "authors": "Fergus ImrieStefan DennerLucas S. BrunschwigKlaus Maier-HeinMihaela van der Schaar",
        "links": "http://arxiv.org/abs/2407.18227v1",
        "entry_id": "http://arxiv.org/abs/2407.18227v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18227v1",
        "summary": "The application of machine learning in medicine and healthcare has led to the\ncreation of numerous diagnostic and prognostic models. However, despite their\nsuccess, current approaches generally issue predictions using data from a\nsingle modality. This stands in stark contrast with clinician decision-making\nwhich employs diverse information from multiple sources. While several\nmultimodal machine learning approaches exist, significant challenges in\ndeveloping multimodal systems remain that are hindering clinical adoption. In\nthis paper, we introduce a multimodal framework, AutoPrognosis-M, that enables\nthe integration of structured clinical (tabular) data and medical imaging using\nautomated machine learning. AutoPrognosis-M incorporates 17 imaging models,\nincluding convolutional neural networks and vision transformers, and three\ndistinct multimodal fusion strategies. In an illustrative application using a\nmultimodal skin lesion dataset, we highlight the importance of multimodal\nmachine learning and the power of combining multiple fusion strategies using\nensemble learning. We have open-sourced our framework as a tool for the\ncommunity and hope it will accelerate the uptake of multimodal machine learning\nin healthcare and spur further innovation.",
        "updated": "2024-07-25 17:46:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18227v1"
    }
]