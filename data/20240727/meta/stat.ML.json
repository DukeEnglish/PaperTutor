[
    {
        "title": "Statistical optimal transport",
        "authors": "Sinho ChewiJonathan Niles-WeedPhilippe Rigollet",
        "links": "http://arxiv.org/abs/2407.18163v1",
        "entry_id": "http://arxiv.org/abs/2407.18163v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18163v1",
        "summary": "We present an introduction to the field of statistical optimal transport,\nbased on lectures given at \\'Ecole d'\\'Et\\'e de Probabilit\\'es de Saint-Flour\nXLIX.",
        "updated": "2024-07-25 16:25:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18163v1"
    },
    {
        "title": "Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models",
        "authors": "Sanae LotfiYilun KuangBrandon AmosMicah GoldblumMarc FinziAndrew Gordon Wilson",
        "links": "http://arxiv.org/abs/2407.18158v1",
        "entry_id": "http://arxiv.org/abs/2407.18158v1",
        "pdf_url": "http://arxiv.org/pdf/2407.18158v1",
        "summary": "Large language models (LLMs) with billions of parameters excel at predicting\nthe next token in a sequence. Recent work computes non-vacuous\ncompression-based generalization bounds for LLMs, but these bounds are vacuous\nfor large models at the billion-parameter scale. Moreover, these bounds are\nobtained through restrictive compression techniques, bounding compressed models\nthat generate low-quality text. Additionally, the tightness of these existing\nbounds depends on the number of IID documents in a training set rather than the\nmuch larger number of non-IID constituent tokens, leaving untapped potential\nfor tighter bounds. In this work, we instead use properties of martingales to\nderive generalization bounds that benefit from the vast number of tokens in LLM\ntraining sets. Since a dataset contains far more tokens than documents, our\ngeneralization bounds not only tolerate but actually benefit from far less\nrestrictive compression schemes. With Monarch matrices, Kronecker\nfactorizations, and post-training quantization, we achieve non-vacuous\ngeneralization bounds for LLMs as large as LLaMA2-70B. Unlike previous\napproaches, our work achieves the first non-vacuous bounds for models that are\ndeployed in practice and generate high-quality text.",
        "updated": "2024-07-25 16:13:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.18158v1"
    },
    {
        "title": "Fast convergence of the Expectation Maximization algorithm under a logarithmic Sobolev inequality",
        "authors": "Rocco CaprioAdam M Johansen",
        "links": "http://arxiv.org/abs/2407.17949v1",
        "entry_id": "http://arxiv.org/abs/2407.17949v1",
        "pdf_url": "http://arxiv.org/pdf/2407.17949v1",
        "summary": "By utilizing recently developed tools for constructing gradient flows on\nWasserstein spaces, we extend an analysis technique commonly employed to\nunderstand alternating minimization algorithms on Euclidean space to the\nExpectation Maximization (EM) algorithm via its representation as\ncoordinate-wise minimization on the product of a Euclidean space and a space of\nprobability distributions due to Neal and Hinton (1998). In so doing we obtain\nfinite sample error bounds and exponential convergence of the EM algorithm\nunder a natural generalisation of a log-Sobolev inequality. We further\ndemonstrate that the analysis technique is sufficiently flexible to allow also\nthe analysis of several variants of the EM algorithm.",
        "updated": "2024-07-25 11:08:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.17949v1"
    },
    {
        "title": "Causal Deepsets for Off-policy Evaluation under Spatial or Spatio-temporal Interferences",
        "authors": "Runpeng DaiJianing WangFan ZhouShikai LuoZhiwei QinChengchun ShiHongtu Zhu",
        "links": "http://arxiv.org/abs/2407.17910v1",
        "entry_id": "http://arxiv.org/abs/2407.17910v1",
        "pdf_url": "http://arxiv.org/pdf/2407.17910v1",
        "summary": "Off-policy evaluation (OPE) is widely applied in sectors such as\npharmaceuticals and e-commerce to evaluate the efficacy of novel products or\npolicies from offline datasets. This paper introduces a causal deepset\nframework that relaxes several key structural assumptions, primarily the\nmean-field assumption, prevalent in existing OPE methodologies that handle\nspatio-temporal interference. These traditional assumptions frequently prove\ninadequate in real-world settings, thereby restricting the capability of\ncurrent OPE methods to effectively address complex interference effects. In\nresponse, we advocate for the implementation of the permutation invariance (PI)\nassumption. This innovative approach enables the data-driven, adaptive learning\nof the mean-field function, offering a more flexible estimation method beyond\nconventional averaging. Furthermore, we present novel algorithms that\nincorporate the PI assumption into OPE and thoroughly examine their theoretical\nfoundations. Our numerical analyses demonstrate that this novel approach yields\nsignificantly more precise estimations than existing baseline algorithms,\nthereby substantially improving the practical applicability and effectiveness\nof OPE methodologies. A Python implementation of our proposed method is\navailable at https://github.com/BIG-S2/Causal-Deepsets.",
        "updated": "2024-07-25 10:02:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.17910v1"
    },
    {
        "title": "Superior Scoring Rules for Probabilistic Evaluation of Single-Label Multi-Class Classification Tasks",
        "authors": "Rouhollah AhmadianMehdi GhateeJohan Wahlström",
        "links": "http://arxiv.org/abs/2407.17697v1",
        "entry_id": "http://arxiv.org/abs/2407.17697v1",
        "pdf_url": "http://arxiv.org/pdf/2407.17697v1",
        "summary": "This study introduces novel superior scoring rules called Penalized Brier\nScore (PBS) and Penalized Logarithmic Loss (PLL) to improve model evaluation\nfor probabilistic classification. Traditional scoring rules like Brier Score\nand Logarithmic Loss sometimes assign better scores to misclassifications in\ncomparison with correct classifications. This discrepancy from the actual\npreference for rewarding correct classifications can lead to suboptimal model\nselection. By integrating penalties for misclassifications, PBS and PLL modify\ntraditional proper scoring rules to consistently assign better scores to\ncorrect predictions. Formal proofs demonstrate that PBS and PLL satisfy\nstrictly proper scoring rule properties while also preferentially rewarding\naccurate classifications. Experiments showcase the benefits of using PBS and\nPLL for model selection, model checkpointing, and early stopping. PBS exhibits\na higher negative correlation with the F1 score compared to the Brier Score\nduring training. Thus, PBS more effectively identifies optimal checkpoints and\nearly stopping points, leading to improved F1 scores. Comparative analysis\nverifies models selected by PBS and PLL achieve superior F1 scores. Therefore,\nPBS and PLL address the gap between uncertainty quantification and accuracy\nmaximization by encapsulating both proper scoring principles and explicit\npreference for true classifications. The proposed metrics can enhance model\nevaluation and selection for reliable probabilistic classification.",
        "updated": "2024-07-25 01:46:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.17697v1"
    }
]