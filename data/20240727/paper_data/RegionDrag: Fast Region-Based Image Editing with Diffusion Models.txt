RegionDrag: Fast Region-Based Image Editing
with Diffusion Models
Jingyi Lu1 , Xinghui Li2 , and Kai Han1†
1The University of Hong Kong 2University of Oxford
lujingyi@connect.hku.hk, xinghui@robots.ox.ac.uk, kaihanx@hku.hk
Handle Target Point-based Handle Target Region-based
Point Input DragDiffusion SDE-Drag DiffEditor Region Input Our results
Drag (s) LoRA (s) 45 133 81 45 43 Drag (s) LoRA (s) 1.5s
Run Time ~3min ~2min ~43s Run Time ~1.5s
Fig.1: Comparison of editing results and latency between point-drag-based methods
andourregion-drag-basedmethod.Ourgradient-free,region-basedframeworkreduces
editing time from approximately one minute to about 1.5 seconds for 512×512 resolu-
tion images, while producing results that better align with users’ intentions.
Abstract. Point-drag-basedimageeditingmethods,likeDragDiffusion,
haveattractedsignificantattention.However,point-drag-basedapproach-
es suffer from computational overhead and misinterpretation of user in-
tentions, due to the sparsity of point-based editing instructions. In this
paper, we propose a region-based copy-and-paste dragging method, Re-
gionDrag, to overcome these limitations. RegionDrag allows users to
express their editing instructions in the form of handle and target re-
gions, enabling more precise control and alleviating ambiguity. In addi-
tion, region-based operations complete editing in one iteration and are
much faster than point-drag-based methods. We also incorporate the
attention-swapping technique for enhanced stability during editing. To
validateourapproach,weextendexistingpoint-drag-baseddatasetswith
region-based dragging instructions. Experimental results demonstrate
that RegionDrag outperforms existing point-drag-based approaches in
terms of speed, accuracy, and alignment with user intentions. Remark-
ably, RegionDrag completes the edit on an image with a resolution
of 512×512 in less than 2 seconds, which is more than 100× faster
than DragDiffusion, while achieving better performance. Project page:
https://visual-ai.github.io/regiondrag.
Keywords: Region-BasedDragging·ImageEditing·DiffusionModels
† Corresponding author.
4202
luJ
52
]VC.sc[
1v74281.7042:viXra2 Lu et al.
1 Introduction
Stable Diffusion (SD) [23] is a widely adopted text-to-image generative model
knownforitsefficacyinproducinghigh-fidelityimages.Asitistrainedwithmil-
lions of images, the vast amount of knowledge learned about images enables the
model to edit existing images in a zero-shot manner. A recent line of work that
has gained attention in the community is point-drag-based image editing using
SD [15,18,24]. Point-drag-based methods first allow users to designate several
pointsanddragthemtodesiredpositions.Theytheninvertthelatentrepresen-
tation of the input image to a particular timestep in the diffusion process and
edit the image by enforcing similarities between local latent representations at
theinitialandfinalpositionsofthepoints.Thisisdoneeitherthroughoptimiza-
tion[15–17,24]ordirectcopy-and-paste[18].Theeditedlatentrepresentationis
finally denoised and decoded to the edited image.
Although point-drag-based methods demonstrate encouraging results, they
exhibit several limitations. First, as the editing solely relies on dragging sparse
points,theyhavetointerpolatedozensofintermediatepointsalongthedragging
directions and edit the image iteratively to avoid editing failures. This signifi-
cantly slows down the speed of editing. Second, dragging points cannot always
faithfullyreflectthedesiredeffect.Thisformofinstructionispronetobeingmis-
interpreted by the model, so the editing results may not fully align with users’
actual intentions. For example, moving an object to the left and expanding the
object to the left can equally be represented by dragging points to the left.
Therefore, we propose an alternative form of dragging to address the above
issues. Instead of relying on points, we propose a region-based copy-and-paste
dragging method, RegionDrag. Specifically, we first invert the latent represen-
tation of the unedited image to a specific time step using DDPM inversion [28],
an inverse process to the stochastic DDPM sampling [25] which benefits the
image editing [18]. Users then draw a handle region and a target region (see
Fig. 1), where the former is the part that users intend to drag and the latter
illustratesdesiredpositionsthatusersintendtoachieve.Afterthat,weestablish
a dense mapping between the two regions using our proposed Region-to-Point
Mapping algorithm, and the latent representation covered by the handle region
is mapped to the target region according to the dense mapping. Finally, the
edited latent representation goes through the DDPM sampling process, and the
mapping operation is repeated at multiple time steps. In the meantime, we also
employtheattention-swappingtechniqueproposedin[2]tostabilizetheediting.
ToevaluateRegionDrag,weexpandtwoexistingpoint-drag-basedimageediting
datasets[18,24]byaddingequivalentregion-baseddragginginstructionstoeach
image. Experiments demonstrate that RegionDrag is significantly faster than
point-drag-based methods as it completes editing in a single iteration. In addi-
tion, the paste region offers clearer constraints than point inputs and reduces
chances of misinterpretation, leading to more faithful editing results.
Our contributions can be summarized as follows: (1) We introduce a region-
based image editing method to overcome the limitations of point-drag-based
approaches,utilizingricherinputcontexttobetteraligntheeditingresultswithRegionDrag: Fast Region-Based Image Editing with Diffusion Models 3
theusers’intentions.(2)Byemployingagradient-freecopy-pasteoperation,our
region-based image editing becomes significantly faster than existing methods
(see Fig. 1), completing the dragging in one single iteration. (3) We extend two
point-drag-based datasets with region-based dragging instructions to validate
RegionDrag’s effectiveness and benchmark region-based editing methods.
2 Related Work
Generative Models for Image Editing. Early advancements in image gen-
eration have been driven by generative adversarial networks (GANs) [6,9–11].
However, their practical application in real-world image editing is limited by
the diversity of GAN training data and the effectiveness of GAN inversion tech-
niques [3,4,21,27,29]. The emergence of text-to-image diffusion models leads
to novel image editing techniques that utilize text prompts to modify high-level
characteristicssuchasstyle,motion,orobjectcategories[1,2,5,12].Nevertheless,
imageeditingmethodsbasedonusertextpromptsinherentlystruggletomanip-
ulateimagesatthepixellevel.Draggingmethods[4,14–17,19,24]aimtoaddress
this limitation by controlling the overall posture and shape of objects through
iterative movement and tracking of one or multiple key points. RegionDrag in-
troduces a region-based approach as a superior alternative, offering increased
stability and efficiency for fine-grained image editing tasks.
Image Editing by Dragging Points. When employing drag-based methods
for image editing, users can manipulate images by designating pairs of handle
pointsandtargetpoints.Thepoint-drag-basedmethodsareexpectedtoproduce
an image that meets two criteria: (1) features at the handle points are relocated
to the target points, and (2) the original identity of the edited object is main-
tained. DragGAN [19] first enables editing on GAN-generated images involving
multiple dragging point pairs. Specifically, DragGAN decomposes the editing
processintoseveraliterations,alternatingbetweenmotionsupervisionandpoint
tracking. In the motion supervision phase, DragGAN optimizes the StyleGAN’s
latentcode[10,11]usingthedistancelossbetweentheinitialandtargetpositions
ofthehandlepoints.Followingeachmotionsupervisioniteration,DragGANup-
datesthepositionsofthehandlepointsbyemployingapoint-trackingtechnique.
FreeDrag [14] switches the dragging from pixel space to feature space, enabling
morestableandprecisemanipulation.DragDiffusion[24]extendsthesupervise-
and-track framework of DragGAN [19] to diffusion models. It inverts the latent
representationoftheuneditedimagetoapartiallynoisystatusataselectedtime
step, drags point features by optimizing the latent representation, and denoises
it. RotationDrag [15] further refines the DragDiffusion framework [24] for cases
ofrotatingobjects.Anotherlineofresearch,includingDragonDiffusion[16]and
DiffEditor [17], draws inspiration from classifier-guided generation, extending
feature dragging throughout the entire denoising process rather than confining
it to a single timestep. A method similar to ours, named SDE-Drag [18], elim-
inates memory-intensive backpropagation found in [16,17,24] by copying and
pasting points within the diffusion latent space. Although SDE-Drag demon-4 Lu et al.
strates promising results, it remains time-consuming due to its reliance on in-
terpolating intermediate points along the dragging path to stabilize the editing
process. In this paper, we introduce a simple yet effective region-based frame-
work that utilizes the rich context inherent in region pairs to complete all edits
in just one editing step.
Appearance Consistency for Drag Editing. SD-based image editing shifts
the image’s latent distribution to incorporate user-specified structural, layout,
andshapechanges.However,thisprocesscanalsointroduceundesirablechanges
and artifacts. Mitigating these side effects remains challenging. Existing meth-
ods[15,18,24]oftenrequiretrainingtheSDmodelwithLoRA(Low-RankAdap-
tation)[8]foreachuneditedimagetomaintainstyleandappearanceconsistency.
While LoRA is parameter-efficient through the use of low-rank matrix decom-
position, it requires extensive preparation time and may overly constrain the
image’sappearance,limitingfurtheredits.Additionally,point-drag-basedmeth-
ods often necessitate masking the editing area to reduce input ambiguity and
enhanceimageconsistency.Incontrast,inthispaper,weincorporateatraining-
freeapproachcalledmutualself-attentioncontrol[2],whichpreservestheimage’s
identity by leveraging keys and values in the self-attention blocks of the model.
This approach is seamlessly integrated into the editing pipeline, eliminating the
need for an additional module to maintain the image’s identity. Furthermore,
unlikepoint-drag-basedmethodsthatrelyonamasktoconfinetheeditingarea,
RegionDrageliminatesthissteptoprovideamoreconvenienteditingexperience.
3 RegionDrag
RegionDragenablesuserstoinputhandleandtargetregionpairs,whicharethen
used for editing through two primary steps: (1) copying latent representations
covered by handle regions and storing self-attention features during inversion,
and (2) pasting copied latent representations to target positions and inserting
stored self-attention features during denoising.
This section begins by reviewing the diffusion-based image editing pipeline
and point-drag-based methods in Sec. 3.1. We then discuss the limitations of
point-drag-based methods and introduce how our region-based input addresses
these limitations in Sec. 3.2. Finally, Sec. 3.3 presents our editing pipeline used
to process region-based inputs.
3.1 Preliminary
Diffusion-based image editing involves two main stages: inversion and de-
noising.Uneditedimagesarefirstgraduallyinvertedtoaspecifictimestepinthe
diffusion process. The editing is then performed on the images at this timestep
and the images are finally denoised back to their original image space. The
transition between timesteps in the diffusion process is governed by a sampling
scheduler. A commonly used one is the DDIM scheduler [25]. When an image
latent z at timestep s transitions to z at timestep t, it goes through:
s tRegionDrag: Fast Region-Based Image Editing with Diffusion Models 5
z =f (z )
t s→t s
√
=√
α
(cid:18) z s− 1− √α sϵ θ(z s,s,C)(cid:19) +(cid:112)
1−α −σ2ϵ (z ,s,C)+σ w ,
(1)
t α t s θ s s s
s
where ϵ (z ,s,C) is the noise predicted by the diffusion model, α is a mono-
θ s t
tonically decreasing noise scheduling function dependent on t, w is Gaussian
s
(cid:112) (cid:112)
noise, and σ = η (1−α )/(1−α ) 1−α /α with η = 0. Since η is set to
s t s s t
0, the transition is a deterministic process and it is widely used in many meth-
ods [16,17,23,24]. If η > 0, the sampling becomes a stochastic process known
as the DDPM sampling [7]. We choose DDPM over DDIM as [18] shows that
DDPM’s randomness reduces divergence between unedited and edited image
distributions.
Point-drag-basedmethods,suchasDragDiffusionandSDE-Drag[18,24],edit
an image by relocating its latent representations at user-designated n handle
points h to corresponding target positions t . Due to the limited context
1:n 1:n
available in sparse points, point-drag-based methods divide drag editing into K
sub-steps.Atsub-stepk,thealgorithmdragslatentrepresentationsfromhandle
points h to target positions t , which are found by direct interpolation
k,1:n k,1:n
or an extra point matching step. DragDiffusion optimizes zk by minimizing the
t
ℓ -distance between UNet upblock features F(zk) at handle h and target
1 t k,1:n
t points, respectively denoted as F(zk)[h ] and F(zk)[t ]:
k,1:n t k,1:n t k,1:n
zk+1 =zk−η·∇ ∥F(zk)[h ]−F(zk)[t ]∥ . (2)
t t zk t k,1:n t k,1:n 1
t
DragonDiffusion [16] and DiffEditor [17] are DragDiffusion variants with K =1
andextendoptimizationacrossmultiplediffusiontimesteps.Ontheotherhand,
SDE-Drag copies the latent representations from handle points to target points
using the copy-paste function CP (Eq. (3)), where z[h] and z[t] represent the
latentcodeatpositionshandt.Thisisthenfollowedbyadenoisingandinversion
cycle (Eq. (4)).
CP(z ,z ,h,t)=(z [t]←z [h]), (3)
1 2 2 1
zk+1 =f (cid:0) f (cid:0) CP(zk,zk,h ,t )(cid:1)(cid:1) . (4)
t 0→t t→0 t t k,1:n k,1:n
3.2 From Point-Based to Region-Based Dragging
Although point-drag-based methods offer an intuitive means of user input, the
limited information derived from sparse points poses challenges for models dur-
ingtheeditingprocess.Specifically,pointinstructionscanresultintwoprimary
issues: input ambiguity and slow inference. First, point instructions are in-
herently ambiguous. One dragging action could correspond to multiple plau-
sible editing effects. Consider a user attempting to elongate a bird’s beak in an
image, as depicted in Fig. 2. The user selects a point on the beak and drags it6 Lu et al.
Original Ambiguous results Original
User’s Intention
"extend bird's beak" Region Outome
Point Input w/o sub-steps w/o LoRA Region Input
Fig.2:Overallcomparisonofpoint-basededitingandregion-basedediting,exemplified
bymanipulatingabird’sbeak.Theregion-basedapproachisshowntoprovideamore
user-friendly and less ambiguous editing experience.
towardstheupper-leftcorner.Point-drag-basedmethods,however,mightmisin-
terprettheuser’sgoalasenlargingthebeakormovingtheentirebirdtotheleft,
rather than extending the beak as intended, leading to a misalignment between
the user’s intention and the model’s output.
Second, the complexity involved in point-drag-based editing re-
quires considerable computational overhead. Point-drag-based editing is
challengingbecausethemodelmustdeducechangesacrosstheentireimagefrom
the motion of a single or a few points. To carry out this complex drag operation
while preserving the object’s identity, point-drag-based methods heavily rely on
two computationally intensive steps: training a unique LoRA [8] for each image
and breaking down the dragging process into a series of sub-steps. Particularly,
LoRA helps the model maintain the original image’s identity and step-by-step
dragging boosts the chance of achieving desired editing effects. Otherwise, the
editing results may suffer from significant identity distortion or void editing,
as shown in Fig. 2. The root of this problem lies in that sparse points do not
impose sufficient constraints on editing, so the model has to rely on LoRA to
prevent distortion and iterative editing to provide some degree of additional su-
pervision along the path of the dragging. Consequently, most point-drag-based
methods require several minutes to edit one image, rendering them impractical
for real-world applications.
The simplest solution to these problems is to encourage users to provide a
sufficient number of points. However, such an approach would result in users
spending too much time on designating and dragging points. Therefore, we de-
sign a form of editing that is not only user-friendly but also provides more
informative context to the model, thus avoiding instruction ambiguity, slow in-
ference,andexhaustiveeffortsfromusers.Insteadofrelyingondraggingpoints,
we propose to use region-based operations, where users assign a handle region
H to indicate the area they wish to drag and a target region T to illustrate
the desired position they would like to achieve. We then establish a dense map-
pingbetweenthetworegionsusingourRegion-to-PointMappingalgorithmand
completetheeditingbydirectlycopyingthelatentrepresentationcoveredbythe
handle region to the target region in one inversion and denoising cycle. DespiteRegionDrag: Fast Region-Based Image Editing with Diffusion Models 7
Handle Region Target Region Copy-Paste Inference
Original Image
...
Q Q Q Q
V V V V
K K K K
K K K K
V V V V
Q Q Q Q
User Edit
...
Fig.3: General pipeline of our method. Rich context provided by the region pairs en-
ablesuserstocompleteaccurateeditsinoneinversionanddenoisingcycle.Thelatent
representationofthehandleregioniscopiedandpastedontothetargetregionthrough-
outmultipletimestepsfordragediting.Keysandvalueswithintheself-attentionblocks
are reused to ensure image consistency.
the simplicity of this operation, it addresses ambiguity and overhead from two
perspectives: (1) Region-based operation is more expressive and accurate than
draggingpointsandwouldsignificantlyalleviatetheambiguity.Asdemonstrated
inFig.2,weexpressextendingthebird’sbeakbysimplydrawingalongerbeak,
hence reducing ambiguity presented in point-drag-based inputs. (2) Each region
corresponds to a large number of points after dense mapping, so it provides
strongerconstraintsoneditingresultsthansparsepoints.Asaresult,wedonot
have to interpolate intermediate points along the dragging path and crave extra
supervision, and editing can be completed in one editing step. Moreover, the
handle and target regions can vary in size and take arbitrary shapes allowing
users to define them conveniently.
3.3 Editing Pipeline
We elaborate on our editing pipeline in this section. We first introduce the our
region-based user input, followed by our Region-to-Point Mapping algorithm,
and finally the main working pipeline.
User Input. The handle and target regions can be defined in two ways: (1)
by entering vertices to form a polygon (e.g., a triangle or quadrilateral), or (2)
by brushing out a flexible region using a brush tool. The choice of input form
largely depends on the user’s preferences. Vertices are ideal for editing well-
definedshapes,likemovingawindowonabuilding;abrushtoolisbettersuited
for irregular shapes, like a curved road or human hair.
Region-to-PointMapping. Topreservetheoriginalspatialinformationwhen
copyingandpastinglatentrepresentation,weneedtoestablishadensemapping
between the handle and target region. If the regions are confined to triangular
orquadrilateral shapes, wecan compute atransformation matrixusing affine or
perspective mappings. However, finding a similar transformation for brush-out
noisrevnI
gnisioneD8 Lu et al.
Algorithm 1 Region-to-Point Mapping
Input: Handle region H with N handle points {(xH,yH)}NH, target region T
H i i i=1
with N target points {(xT,yT)}NT.
T i i i=1
Output: List of mapped point pairs P.
Require: Initialize empty list P.
1: for (x,y)∈{(xT,yT)}NT do
i i i=1
2: Horizontal scaling:
3: x′ ←(x−min(xT ))/(max(xT )−min(xT ))
1:NT 1:NT 1:NT
4: x′ ←⌊x′·(max(xH )−min(xH ))+min(xH )⌋
5: Column-by-colu1 m:NH n vertical1 s:N caH ling: 1:NH
6: y′ ←(y−min(yT |xT =x))/(max(yT |xT =x)−min(yT |xT =x))
i i i i i i
7: y′ ←⌊y′·(max(yH |xH =x′)−min(yH |xH =x′))+min(yH |xH =x′)⌋
i i i i i i
8: Add point pair ((x′,y′),(x,y)) to P
9: end for
10: return P
regions with arbitrary shapes is challenging. To address this issue, we propose
an algorithm to numerically find the mapping between two regions.
Let {(xH, yH)}NH and {(xT, yT)}NT be the sets of pixels in handle and
i i i=1 j j j=1
targetregionsrespectively.Webeginbylinearlyscalingthetargetregion’swidth
tomatchthehandleregion’swidth.Foreachpixel(x, y)∈{(xT, yT)}NT inthe
j j j=1
target region, x is adjusted by first subtracting the x-coordinate lower bound-
ary min(xT ) and is normalized using the target region’s x-coordinate range
1:NT
max(xT )−min(xT ). It is then scaled to the handle region’s x-coordinate
1:NT 1:NT
range max(xH )−min(xH ), resulting in x′.
1:NH 1:NH
Thisstepensuresthatbothregionshavethesamenumberofcolumnsofpix-
els.Next,wemapeachtargetregion’sverticalpixelcolumntoitscorresponding
handle region’s column. Each point’s y in the target region is adjusted by sub-
tracting the lower boundary of its vertical column min(yT | xT = x) and is
i i
normalized by its column’s y-coordinate range max(yT | xT = x)−min(yT |
i i i
xT =x),followedbybeingscaledtothecorrespondinghandleregion’scolumn’s
i
y-coordinaterangemax(yH |xH =x′)−min(yH |xH =x′).Thenotationf(·|·)
i i i i
represents a function f (e.g., min or max) that takes a variable and a condition,
and returns the function’s value for the variable among points satisfying the
condition. The whole process is summarised in Algorithm 1.
Main Pipeline. AsillustratedinFig.3,RegionDragutilizestheimageediting
pipeline mentioned in Sec. 3.1. Initially, the latent representation of the image
z is inverted to z , where t′ is a selected timestep prior to maximum timestep
0 t′
T. Each intermediate step z ,z ,...,z is cached during inversion for future use.
0 1 t′
We then duplicate z and denote the copy as z′ . The handle regions of z′
t′ t′ t′
are blended with Gaussian noise ε according to the blending function r (z,H)
α
defined in Eq. (5):
(cid:112)
r (z,H)=(1−H)·z+H ·( 1−α2·z+α·ε), ε∼N(0,I), (5)
αRegionDrag: Fast Region-Based Image Editing with Diffusion Models 9
where H is a binary mask, with the handle region assigned a value of 1 and
α is a blending coefficient that ranges from 0 to 1, which governs the strength
of the blending effect. The noise ε is drawn from a Gaussian distribution with
mean0andidentitycovariancematrix,denotedbyN(0,I).Ifαissettoalower
value, less noise is added, which preserves more of the original image’s features
and details in the handle regions. After blending with Gaussian noise, z′ goes
t′
through the denoising process using the DDPM sampler, and the dragging is
conducted in a copy-paste manner. At a denoising timestep t, we extract latent
representation within the handle region of z and map it to the target region of
t
z′ accordingtothedensemappingcomputedbyeithergeometrictransformation
t
or Algorithm 1. In the meantime, we employ mutual self-attention control [2] to
help maintain the identity of images. In brief, when passing through the self-
attention module of the UNet, the key and value (k′,v′) used when denoising z′
t t t
arereplacedwiththose(k ,v )fromz .Thisallowstheeditedimagetokeepthe
t t t
layoutandidentityoftheoriginalimage,therebystabilizingtheeditingprocess.
After z′ is gradually denoised to z′ (see Fig. 3), it is decoded to the edited im-
t 0
age x′. Although RegionDrag is introduced with a single handle-target region
0
pair, it supports multiple pairs of input {(H ,T )}n , allowing users to specify
i i i=1
several modifications in a single editing session. Dense mapping is individually
constructed for each region pair but collectively used in latent copy-paste oper-
ations. The entire pipeline is summarised in Algorithm 2.
Algorithm 2 Editing Pipeline
Input: Image x , handle and target region pairs {(H ,T )}n .
0 i i i=1
Output: Edited image x′.
0
Require:Diffusionfunctionf Eq.(1),VAEencoderE,VAEdecoderD,region-
s→t
to-point mapping function m, copy-paste function CP Eq. (3), handle resampling
function r Eq. (5), copy-paste time interval (t′,t′′).
α
1: Preparation:
2: {(h ,t )}N ←m({(H ,T )}n )
i i i=1 i i i=1
3: z ←E(x )
0 0
4: Inversion stage:
5: for t=0 to t′−1 do
6: z ,k ,v ←f (z )
t+1 t+1 t+1 t→t+1 t
7: end for
8: z t′
′
←r α(z t′,∪n i=1H i)
9: Denoising stage:
10: for t=t′ down to 1 do
11: if t≥t′′ then
12: z′ ←CP(z ,h ,z′,t )
t t 1:N t 1:N
13: end if
14: z′ ←f (z′,k′ ←k ,v′ ←v )
t−1 t→t−1 t t t t t
15: end for
16: x′ ←D(z′)
0 0
17: return x′
010 Lu et al.
4 Experimental Results
4.1 Datasets
SDE-Drag [18] and DragDiffusion [24] have each introduced a dataset named
DragBenchtoevaluatetheperformanceofpoint-drag-basedmethods.Forclarity,
we refer to the one by SDE-Drag as DragBench-S, which contains 100 samples,
and to the one by DragDiffusion as DragBench-D, which includes 205 samples.
Each sample comprises an image, a descriptive prompt, an optional mask delin-
eating the editing region, and at least one pair of points that reflects the user’s
intention.Empirically,DragBench-Soffersrelativelymoredetailedpromptsand
includesamaskin56ofitssamples,whileDragBench-Discharacterizedbymore
complex intentions and features carefully crafted masks in each of its samples.
Toevaluateregion-basedediting,weintroducetwonewbenchmarks:DragB-
ench-SR and DragBench-DR (R is short for ‘Region’), which are modified ver-
sions of DragBench-S and DragBench-D, respectively. These benchmarks are
consistentwiththeirpoint-drag-basedcounterpartsintermsofimages,prompts,
andmasksbutdifferbyreflectingtheuser’sintentionthroughregionsinsteadof
points.DragBench-SRandDragBench-DRconsistof8and23polygon-annotated
samples, respectively, with the remainders of each dataset using brushes. For a
handle-target region pair, the median number of equivalent point pairs is 267 in
DragBench-SR and 201 in DragBench-DR. We visualize the distribution of the
number of transformed point pairs in Fig. 4.
During the annotation process, we ensure that the region pairs are aligned
with the point pairs to achieve the same editing effect. To simulate how real
users would typically draw handle-target region pairs, we do not meticulously
draw each region. According to our observations, each region-based sample is
annotatedinapproximately10seconds.Thisapproachallowsforamorerealistic
evaluation of the region-based editing methods while maintaining consistency
with the point-based datasets.
4.2 Evaluation Metrics
LPIPS: We follow [24] and use Learned Perceptual Image Patch Similarity
(LPIPS) v0.1 [30] to measure the identity similarity between the edited image
and the original image. LPIPS computes the AlexNet [13] feature distances be-
tween the image pairs. A high LPIPS score indicates that unexpected identity
changes or artifacts occur due to editing. A lower LPIPS score suggests that
the object’s identity has been well-preserved during editing; however, it does
not necessarily imply a better drag edit, as two identical images would yield an
LPIPS score of 0.
Mean Distance (MD):DragDiffusion[24]introducestheMDmetrictoassess
how well an approach moves the handle points’ content to the target points. To
identify where the handle points have been moved to by the method, DragDif-
fusion employs DIFT [26] to find the most similar points to the handle pointsRegionDrag: Fast Region-Based Image Editing with Diffusion Models 11
DragBench-SR User Input User Input
30 DragBench-DR
25
Handle Point
20
15 Target Point
10
Search Mask
5
0
55 403 2981 22026 162755
Number of Equivalent Point Pairs
Fig.4: Log-transformed frequency Fig.5:IllustrationofMDcomputation.
distribution of equivalent point Dragged handle points will be searched
pair counts in DragBench-SR and within the search mask.
DragBench-DR.
h in the entire edited image and denotes them as h′ . It then uses the nor-
1:n 1:n
malized Euclidean distance between DIFT-matched points and the true target
positions t as the metric. However, we believe that we only have to search
1:n
theareaaroundthehandlepointsandtheircorrespondingtruetargetpointsfor
h′ insteadoftheentireimage,toavoidDIFTmistakenlyidentifyingirrelevant
1:n
points in the images as h′ and excessively penalizing certain methods. Formally,
given a dragging point pair {h,t}, we define its searching mask as
(cid:40)
1 if min(d(x,h),d(x,t))< d √(h,t)
M(x)= 2 , (6)
0 otherwise
where d(·,·) calculates the normalized Euclidean distance between two points
(x ,y ) and (x ,y ) by
1 1 2 2
(cid:115)
(cid:18)
x −x
(cid:19)2 (cid:18)
y −y
(cid:19)2
d((x ,y ),(x ,y ))= 2 1 + 2 1 , (7)
1 1 2 2 W H
where W and H are width and height of the image. We provide examples of
searching masks in Fig. 5. MD is then defined as the average of the normalized
distances d¯between the target points and the DIFT-matched points across all
n points:
n
d¯= 1 (cid:88) d(t ,h′). (8)
n i i
i=1
4.3 Implementation Details
Our method is implemented in Python using the HuggingFace [22] and Py-
Torch [20] libraries. We employ Stable Diffusion v1-5 as our diffusion model
with an image size of 512×512, being consistent with previous diffusion-based
dragging methods. A DDPM sampler is utilized for both the inversion and de-
noising sampling processes, configured to use a total of 20 steps. The latent
ycneuqerF12 Lu et al.
Method DragBench-S(R) DragBench-D(R)
Time (↓)MD (↓)LPIPS (↓)MD (↓)LPIPS (↓)
SDE-Drag [18] 126.1 7.5 12.4 8.1 14.9
DragDiffusion [24] 177.7 7.0 18.0 6.7 11.5
DiffEditor [17] 43.1 23.6 17.6 22.1 10.9
Ours 1.5 6.4 9.9 6.6 9.2
Table 1: Comparisons of our method with baseline methods using MD(×100) and
LPIPS(×100) metrics on DragBench-S(R) and DragBench-D(R) datasets. The time
ismeasuredinsecondsandaveragedacrossbothdatasets.Theimagesizeis512×512.
representation is inverted to timestep t′ = 500 out of the total 1000 steps in
SD1-5.Consequently,weperform10inversionstepsand10denoisingsteps.Mu-
tual self-attention control is enabled throughout all timesteps, and the latent
copy-paste operation is terminated at t′′ = 200. The noise weight α is set to 1.
All experimental results are obtained on an NVIDIA Tesla V100 GPU.
4.4 Baselines
We compare RegionDrag with point-drag-based diffusion methods, including
DragDiffusion[24],SDE-Drag[18],andDiffEditor[17].GAN-basedmethodsare
excludedduetotheirlimitationsineditingthediverseimagesintheDragBench-
S and DragBench-D datasets, as they require domain-specific StyleGAN check-
points.Diffusion-basedmethods,whichoutperformGAN-basedmethodsinedit-
ing tasks, are better suited for our evaluation. Execution times are averaged
across both datasets, and all methods are tested on the same device using pub-
licly released code.
4.5 Quantitative Evaluation
To quantitatively evaluate the editing performance of the methods, we employ
LPIPS and Mean Distance as metrics, multiplying both by 100 for illustration
purposes. As demonstrated in Table 1, RegionDrag significantly outperforms
thosecomputationallyexpensivepoint-basedmethodsonbothDragBench-S(R)
and DragBench-D(R) datasets. These results highlight RegionDrag’s superior
performanceinmaintainingimageconsistencywhileachievingcompetitiveedit-
ing results across different datasets. In addition to its effectiveness, RegionDrag
alsoexcelsintermsofefficiency.RegionDragachievesfastinferencespeed,requir-
ingapproximately1.5secondstoedita512×512image,whichis20timesfaster
than the second-fastest method and 100 times faster than DragDiffusion [24].
The inference time for RegionDrag is comparable to generating an image with
20 steps using SD1-5, given that the copy-paste operations introduce negligible
computational overhead.RegionDrag: Fast Region-Based Image Editing with Diffusion Models 13
Point Input DragDiff. SDE-Drag DiffEditor Region Input Our results
Fig.6: Qualitative comparisons with baseline methods. Handle regions and target
regions are respectively denoted by red and blue masks.
4.6 Qualitative Results
Figure 6 compares examples of point-drag-based and region-based editing in-
puts and their corresponding results, demonstrating the effectiveness of Region-
Drag. Our region-based method utilizes the comprehensive context provided by
annotated regions to target desired modifications while preserving the overall
coherence of the image, outperforming point-drag-based editing methods.
4.7 Ablation Study
We argue that the sparsity of point inputs leads to inferior editing results.
To quantitatively demonstrate this, we conducted tests on the DragBench-DR
datasetbyrandomlyselectingsubsetsoftheequivalenttransformedpointswithin
eachsampleandperforminginferenceusingthesesubsets.Wegraduallyreduced
thepercentageofselectedpointstoobservetheimpactontheMDmetric.Asil-
lustratedinFig.7,theresultsexhibitaclearupwardtrendinMDasthepercent-
age of utilized points decreases. This suggests that sparse point inputs provide
weaker constraints on the output compared to region-based inputs, leading to
unsatisfactoryeditingresults.Itconfirmsthebenefitsofemployingregion-based
inputs in RegionDrag.14 Lu et al.
11 Region Input Single-step Multi-step
10
9
8
7
20% 40% 60% 80% 100%
Percentage of Points Used
Fig.7: Ablation study on the Fig.8: Qualitative examples illustrating the im-
impact of the percentage of in- pact of multi-step copy-paste.
putted transformed points.
We copy and paste image’s latent representation over a time interval during
denoising.Tovalidatethisdesign,wecompareittocopy-pasteonlyattheinitial
denoising timestep. Figure 8 shows that editing just at the initial step can yield
unpredictable results because the edits may be lost in subsequent denoising
phases. Multi-step copy-paste solves this by providing extra guidance at smaller
timesteps while preserving image fidelity.
5 Conclusion
In this paper, we have introduced an efficient and effective region-based edit-
ing framework, RegionDrag, for high-fidelity image editing. Unlike existing ap-
proaches that utilize point-drag-based editing, RegionDrag reconsiders the edit-
ing problem from the region perspective. RegionDrag allows for editing in a
single step through copying and pasting the latent representation and self-
attention features of the image, which not only provides excellent efficiency but
alsoachievessuperioreditingperformance.Furthermore,wehaveintroducedtwo
newbenchmarks,DragBench-SRandDragBench-DRbasedonexistingdatasets,
fortheevaluationofregion-basedediting.Experimentalresultshaveconsistently
demonstrated the superior efficiency and editing performance of our method.
Acknowledgments
KaiHanandJingyiLuwouldliketoacknowledgethesupportoftheHongKong
Research Grants Council - General Research Fund (Grant No.: 17211024).
)001x(
ecnatsiD
naeMRegionDrag: Fast Region-Based Image Editing with Diffusion Models 15
References
1. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. In: CVPR (2023)
2. Cao,M.,Wang,X.,Qi,Z.,Shan,Y.,Qie,X.,Zheng,Y.:Masactrl:Tuning-freemu-
tualself-attentioncontrolforconsistentimagesynthesisandediting.arXivpreprint
arXiv:2304.08465 (2023)
3. Creswell, A., Bharath, A.A.: Inverting the generator of a generative adversarial
network. IEEE TNNLS (2018)
4. Endo, Y.: User-controllable latent transformer for stylegan image layout editing.
In: CGF (2022)
5. Epstein, D., Jabri, A., Poole, B., Efros, A., Holynski, A.: Diffusion self-guidance
for controllable image generation. In: NeurIPS (2024)
6. Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville, A., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014)
7. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS
(2020)
8. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 (2021)
9. Kang, M., Zhu, J.Y., Zhang, R., Park, J., Shechtman, E., Paris, S., Park, T.:
Scaling up gans for text-to-image synthesis. In: CVPR (2023)
10. Karras, T., Aittala, M., Laine, S., Härkönen, E., Hellsten, J., Lehtinen, J., Aila,
T.: Alias-free generative adversarial networks. In: NeurIPS (2021)
11. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing
and improving the image quality of stylegan. In: CVPR (2020)
12. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,
M.:Imagic:Text-basedrealimageeditingwithdiffusionmodels.In:CVPR(2023)
13. Krizhevsky,A.,Sutskever,I.,Hinton,G.E.:Imagenetclassificationwithdeepcon-
volutional neural networks. NeurIPS (2012)
14. Ling, P., Chen, L., Zhang, P., Chen, H., Jin, Y.: Freedrag: Point tracking is not
youneedforinteractivepoint-basedimageediting.arXivpreprintarXiv:2307.04684
(2023)
15. Luo, M., Cheng, W., Yang, J.: Rotationdrag: Point-based image editing with ro-
tated diffusion features. arXiv preprint arXiv:2401.06442 (2024)
16. Mou,C.,Wang,X.,Song,J.,Shan,Y.,Zhang,J.:Dragondiffusion:Enablingdrag-
style manipulation on diffusion models. arXiv preprint arXiv:2307.02421 (2023)
17. Mou, C., Wang, X., Song, J., Shan, Y., Zhang, J.: Diffeditor: Boosting accuracy
and flexibility on diffusion-based image editing. arXiv preprint arXiv:2402.02583
(2024)
18. Nie, S., Guo, H.A., Lu, C., Zhou, Y., Zheng, C., Li, C.: The blessing of ran-
domness: Sde beats ode in general diffusion-based image editing. arXiv preprint
arXiv:2311.01410 (2023)
19. Pan, X., Tewari, A., Leimkühler, T., Liu, L., Meka, A., Theobalt, C.: Drag your
gan: Interactive point-based manipulation on the generative image manifold. In:
ACM SIGGRAPH (2023)
20. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-
performance deep learning library. In: NeurIPS (2019)16 Lu et al.
21. Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., Lischinski, D.: Styleclip:
Text-driven manipulation of stylegan imagery. In: ICCV (2021)
22. von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K.,
Davaadorj, M., Wolf, T.: Diffusers: State-of-the-art diffusion models. https://
github.com/huggingface/diffusers (2022)
23. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022)
24. Shi, Y., Xue, C., Pan, J., Zhang, W., Tan, V.Y., Bai, S.: Dragdiffusion: Har-
nessing diffusion models for interactive point-based image editing. arXiv preprint
arXiv:2306.14435 (2023)
25. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.arXivpreprint
arXiv:2010.02502 (2020)
26. Tang,L.,Jia,M.,Wang,Q.,Phoo,C.P.,Hariharan,B.:Emergentcorrespondence
from image diffusion. In: NeurIPS (2024)
27. Wang, T., Zhang, Y., Fan, Y., Wang, J., Chen, Q.: High-fidelity gan inversion for
image attribute editing. In: CVPR (2022)
28. Wu,C.H.,DelaTorre,F.:Unifyingdiffusionmodels’latentspace,withapplications
to cyclediffusion and guidance. arXiv preprint arXiv:2210.05559 (2022)
29. Xia, W., Zhang, Y., Yang, Y., Xue, J.H., Zhou, B., Yang, M.H.: Gan inversion: A
survey. IEEE TPAMI (2022)
30. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018)RegionDrag: Fast Region-Based Image Editing
with Diffusion Models
–Supplementary Material–
Jingyi Lu1 , Xinghui Li2 , and Kai Han1†
1The University of Hong Kong 2University of Oxford
lujingyi@connect.hku.hk, xinghui@robots.ox.ac.uk, kaihanx@hku.hk
1 Effectiveness of Region-Based Inputs
In the main text, we quantitatively demonstrate the effectiveness of using more
point pairs obtained from the region pairs. To complement our quantitative
findings, we present qualitative examples in Fig. 1. The results suggest that
increasing the percentage of transformed points used enhances the quality and
stability of the editing outcomes.
Fig.1: Improved editing quality with a higher percentage of transformed points.
2 Discussion of Noise Weight
In the initial denoising step, the latent representations of the handle regions are
blended with random noise weighted by α, where α ranges from 0 to 1 (Eq.
(5) in main text). As illustrated in Fig. 2, a higher α value retains less of the
original content. Nonetheless, the object in the handle region is not guaranteed
to be removed even when α = 1, as the outcome is influenced by other factors
includingthedenoisingprocess,theattentionswappingmechanism,andthetext
prompt.
† Corresponding author.18 Lu et al.
Original Region edit α= 1 α= 0
Fig.2: Results of applying different noise weight α.
3 More Qualitative Results
In Fig. 3, we showcase additional qualitative outcomes of RegionDrag as it pro-
cesses objects from various domains.
Original Regioninput Ourresults
Fig.3: More qualitative results.
4 More Qualitative Comparisons
Figure 4 presents additional examples comparing point-based editing outcomes
with our region-based editing results. The region-based edits maintain object
identity, resulting in higher-quality modifications that closely adhere to the de-
sired output.Supplementary Material of RegionDrag 19
Pointinput DragDiff. SDE-Drag DiffEditor Regioninput Ourresults
Fig.4: More qualitative comparisons with baseline methods.