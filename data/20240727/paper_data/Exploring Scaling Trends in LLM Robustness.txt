Exploring Scaling Trends in LLM
Robustness
Nikolaus Howe∗, niki@far.ai, FAR AI; Mila; Universit´e de Montr´eal
Michał Zajac, FAR AI
Ian McKenzie, ian@far.ai, FAR AI
Oskar Hollinsworth, oskar@far.ai, FAR AI
Tom Tseng, tom@far.ai, FAR AI
Pierre-Luc Bacon, Mila; Universit´e de Montr´eal
Adam Gleave, adam@far.ai, FAR AI
Language model capabilities predictably improve classes of security vulnerabilities. In particular, a wide
from scaling a model’s size and training data. Mo- variety of adversarial prompts can hijack models (Wei
tivated by this, increasingly large language models et al., 2023; Zou et al., 2023; Anil et al., 2024). This
have been trained, yielding an array of impres- enablesmalicioususerstobypasssafetyfine-tuningper-
sive capabilities. Yet these models are vulnerable formed by the designer, unlocking harmful capabilities
to adversarial prompts, such as “jailbreaks” that such as generating compelling misinformation (Spitale
hijack models to perform undesired behaviors, et al., 2023; Chen and Shu, 2024). Innocent users
posing a significant risk of misuse. Prior work are also at risk from attackers using methods such as
indicates that computer vision models become indirect prompt injections (Abdelnabi et al., 2023) to
more robust with model and data scaling, raising exploit LLM-driven applications without any awareness
the question: does language model robustness or participation by the user.
also improve with scale? We study this question A key question is whether future, more capable sys-
empirically, finding that larger models respond tems will naturally become more robust, or if this will
substantially better to adversarial training, but instead require a dedicated safety effort. Although
there is little to no benefit from model scale in current attacks are concerning, the risks could grow
the absence of explicit defenses. stillgreaterwithfuturemodelscapableofmoredanger-
ous actions, such as assisting with biological weapon
development (Mouton et al., 2023), or with greater
1. Introduction
affordances to interact with the world (Sharkey et al.,
2023), such as a virtual assistant for a CEO of a major
Language models have demonstrated a range of im-
company. Prior work has found that superhuman Go
pressive capabilities in tasks such as general reason-
systems (Wang et al., 2023) are vulnerable to attack,
ing(Hendrycksetal.,2021),graduate-levelQ&A(Rein
demonstrating that impressive capabilities do not guar-
et al., 2023), and code generation (Chen et al., 2021).
antee robustness. However, work has also found that
Thisgrowthincapabilitieshasfueledrapiddeployment,
scaling unlabeled pretraining data (Hendrycks et al.,
with ChatGPT becoming one of the fastest-growing
2019; Carmon et al., 2022; Alayrac et al., 2019) and
consumer applications in history (Hu, 2023). More-
model size (Xie and Yuille, 2019; Huang et al., 2023)
over, language models are increasingly integrated into
improves adversarial robustness in computer vision.
largersystemsenablingthemtotakeactionsinthereal
To answer this question, we conduct an empirical
world using external tools (OpenAI, 2023; Anthropic,
investigation into scaling trends for the adversarial
2024; Google, 2024) and pursue long-term open-ended
robustnessoflanguagemodels. Thesetrendsenableus
goals (Richards, 2024; Kinniment et al., 2024).
toforecasttherobustnessoffuturemodels,andgiveus
The advent of language models enables many new
insightintohowtheoffense-defensebalancemightshift
tasks to be solved by AI but also introduces novel
over time. For example, does the cost of conducting
∗Primary contact for correspondences. an attack against more capable models grow faster
1
4202
luJ
52
]GL.sc[
1v31281.7042:viXraSpam, GCG attack IMDB, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
Min-Max Range
Median
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 1: Attack success rate (y-axis) of GCG against Pythia models of different sizes (x-axis) fine-tuned on
the Spam (left) and IMBD (right) tasks. We run three fine-tuning seeds for each model, plotting the
median attack success rate and shading the range between the min and max. We observe significant
attack success rate variability across model sizes: median robustness does not improve monotonically
with scale.
or slower than the defender’s cost of training those rangeofadversarialthreatmodels(Gilmeretal.,2018)
models? give rise to viable attacks.
Concretely, we investigate the robustness of Pythia
Most recently, many qualitatively different vulner-
modelsrangingfrom14Mto12Bparameter(Biderman
abilities have been found in language models, from
et al., 2023) against two attacks: the random tokens
human-understandable “jailbreaks” (Wei et al., 2023)
baseline and the state-of-the-art greedy coordinate
to seemingly gibberish adversarial suffixes (Wallace
gradient attack. We test these models in various
et al., 2021; Zou et al., 2023). Simple methods such
simple classification tasks where our models achieve
as perplexity filtering and paraphrasing defend against
high accuracy on clean (non-adversarial) data.
some of these attacks (Jain et al., 2023). However,
We first evaluate these pretrained models fine-tuned these defenses can easily be bypassed by more sophisti-
only on clean data. Larger models tend to be more cated methods (Zhu et al., 2023). Adversarial training
resistant to attack, but the effect is weak and noisy shows more promise as a defense (Ziegler et al., 2022),
(Figure1). Bycontrast,aclearerscalingtrendemerges and is the focus of our analysis.
for models adversarially trained against examples of
Thedeterminantsofadversarialrobustnesshavebeen
attacks (Figure 2). Larger models are both more
well-studiedincomputervision. Onelineofscholarship
sample efficient, becoming more robust with fewer
proposes a fundamental tradeoff between robustness
examples, and converge to be more robust given a
and accuracy (Tsipras et al., 2019): exploitable mod-
sufficient number of examples. Moreover, adversarial
els are simply relying on non-robust features (Ilyas
training against one attack transfers protection to
et al., 2019), which improve training performance
similar attacks, with the transfer being stronger for
but hurt robustness. Other work has emphasized
larger models (Figure 3b).
what does improve robustness. Scaling unlabeled pre-
training data (Hendrycks et al., 2019; Carmon et al.,
2. Related Work 2022; Alayrac et al., 2019), model depth (Xie and
Yuille, 2019) and model width (Huang et al., 2023)
improves adversarial robustness in computer vision.
Adversarial examples were first identified in image clas-
However, other work shows that computer vision ad-
sifiers(Szegedyetal.,2014),buthavesincebeenfound
versarial robustness scales too slowly to be a full so-
for systems performing image captioning (Xu et al.,
lution (Debenedetti et al., 2023; Bartoldson et al.,
2019; Zhang et al., 2020), speech recognition (Cisse
2024).
et al., 2017; Alzantot et al., 2018; Sch¨onherr et al.,
2018), and reinforcement learning (Huang et al., 2017; Language model scaling laws (Hestness et al., 2017;
Gleave et al., 2020; Ilahi et al., 2022). Moreover, a Rosenfeld et al., 2019; Kaplan et al., 2020; Hoff-
2
etaR
sseccuS
kcattA
etaR
sseccuS
kcattASpam, GCG attack IMDB, GCG attack
1.0 1.0
Model Size
7.6M
0.8 0.8
17.6M
44.7M
0.6 123.7M 0.6
353.8M
908.8M
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
Spam, GCG attack IMDB, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
5 10 15 20 25 30 5 10 15 20 25 30
Adversarial Training Round Adversarial Training Round
Figure 2: Attack success rate (y-axis) of GCG against Pythia models of varying sizes (line color) on Spam (left)
and IMDB (right) during adversarial training (x-axis) against GCG over 10 rounds (top) and over
30 rounds (bottom). See Figure 16 for a zoomed-in view of the final 10 rounds of the 30-round
adversarial training. We plot the median over three seeds and shade the region between the min
and max. We observe larger models are more sample efficient, and appear to converge to a higher
robustness (lower attack success rate).
mann et al., 2022) have shown that increasing com- to measure robustness by the attack success rate,
pute improves performance across many tasks and defined as the proportion of examples correctly clas-
domains (Chen et al., 2021; Hernandez et al., 2021). sified by the model before attack that are incorrectly
However, scalingdoesnotsolveallproblems(Linetal., classified after attack.1 We adapt pretrained models
2022; McKenzie et al., 2023). There has been only for classification by replacing the unembedding layer
limited work on scaling laws for adversarial robustness with a randomly initialized classification head, and
in language models, with mixed results. Ganguli et al. then fine-tune the models on each task.
(2022) show that LLMs become harder to attack with Tasks We consider four tasks in our experiments,
scale—but Anil et al. (2024) find that some attacks the latter two developed by us for this project:
become more successful with scale. • Spam (Metsis et al., 2006): Given the subject and
body of an email, is it spam or not?
• IMDB (Maas et al., 2011): Given a movie review,
3. Experimental Methodology
1We assume that the attack does not, in fact, change the
ground truth label of the datapoint. This is guaranteed by
We test models in the binary classification setting,
construction for some of our simple procedurally generated
as it is the simplest context in which to study LLM
tasks, and was manually validated on a random sample of
robustness. Crucially, binary classification allows us datapoints in other tasks.
3
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAis the sentiment positive or negative? 4. Fine-tuning
• PasswordMatch: Giventwostringsintheprompt,
are they exactly equal? Figure 1 shows the robustness of fine-tuned models
• WordLength: Given two words in the prompt, is against the GCG attack. The attack is generally less
the first word shorter than the second? successful on larger models, but model size alone does
not explain all the variance in attack success rate.
Spam and IMDB were chosen as standard natural lan-
We observe similarly large random variation in attack
guage processing classification tasks. PasswordMatch
success across model sizes on other tasks and with
was inspired by TensorTrust (Toyer et al., 2023), and
other attacks; for more details, see Appendix D.2.
WordLength by the RuLES dataset (Mu et al., 2023).
As described in Section 3, we use the Pythia mod-
BothPasswordMatchandWordLengthweredesigned
els, which range from 7.6M to 11.6B parameters after
to be easily procedurally generated and have ground
replacing the unembedding matrix with a classifica-
truth labels that can be checked algorithmically. For
tion head.2 We fine-tune all models for a single epoch
brevity, we report on Spam and IMDB in the main text,
withdefaulthyperparametersfromHuggingFaceTrans-
with plots for other tasks deferred to Appendices D
formers (Wolf et al., 2019), except for the learning
and E. We provide example datapoints and details
rate which we set to 1e−5. All models reach > 83%
about the datasets in Appendix B.
accuracy on all tasks, with larger models generally
Models We test the Pythia model family (Biderman performing better (see Appendix D.1 for the final val-
et al., 2023). These models range in size from 14M to idation performance of all models on all tasks). We
12B parameters (or 7.6M to 11.6B when used with a thenevaluatethefine-tunedmodelsagainstadversarial
classification head). All models were trained to predict attacks on an unseen validation dataset.
the next token on the same dataset following the same To understand the source of the variability in model
training protocol, allowing us to isolate model scale robustness shown by our experiments, we varied 1)
from other confounding factors. the pretraining checkpoint,3 and 2) the random seeds
used to initialize the classification head before fine-
Attacks Our attacks append an adversarial suffix
tuning. Both factors led to significant variability in
of N tokens to the prompt. We use two different
model robustness, with pretraining checkpoint con-
procedurestogeneratethisadversarialsuffix: arandom
tributing significantly more variability. The variability
token baseline (RandomToken) and the state-of-the-
was comparable or greater than that of an order of
art greedy coordinate gradient attack (GCG; Zou et al.,
magnitude of model scaling, indicating that out-of-the-
2023). RandomToken was chosen due to its simplicity.
box robustness on a given task is heavily influenced by
GCG was chosen as it is currently one of the most
the randomness of the pretraining procedure itself.
effective attacks on language models.
This initial result suggests that we cannot rely on
In the RandomToken baseline, the N tokens are cho- scale alone to solve the problem of robustness. How-
sen uniformly at random from the model’s vocabulary. ever, in practice, we would apply a defense to a model
We evaluate the model on the attacked text and then priortodeployingitinasecurity-criticalsetting. Inthe
repeat the process with another sample of N random following section, we consider whether scale enables
tokens until the model is successfully attacked or an defenses to more effectively improve model robustness.
appointed budget for model calls is exhausted.
InGCG(Zouetal.,2023),theN tokensareinitialized 5. Adversarial training
arbitrarily and then greedily optimized over multiple
rounds. Ineachround,thegradientofthelossfunction In this section, we explore how model size impacts
with respect to the attack tokens is computed. This robustness when performing adversarial training. Fig-
gradient is used to compute a set of promising single- ure 2 evaluates the robustness of Pythia models to
token modifications, from which the best candidate the GCG attack when adversarially trained against the
is selected and used in the next round. To make this same attack. We see a much cleaner trend than in the
attack work in the classification setting, we minimize fine-tuning only case: larger models gain robustness
the cross-entropy loss between the predicted label and
2In all figures, we report the actual parameter count of the
the target label.
classification model, and not the pretrained model it was
derived from.
In our experiments, we always use N = 10 tokens.
3The Pythia models provide checkpoints from earlier stages of
For more details about the attacks and hyperparame-
pretraining. Weusedvariouscheckpointsfromthefinal10%
ters used, see Appendix C. of pretraining as a starting point for fine-tuning.
4Spam, GCG train, GCG 30 its eval Spam, RT train, GCG eval
1.0 1.0
Model Size
7.6M
0.8 0.8
17.6M
44.7M
0.6 123.7M 0.6
353.8M
908.8M
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(a) Adversarial training against 10-iteration GCG, with eval-(b) Adversarial training against 10-iteration RandomToken,
uation against 30-iteration GCG. All models show some with evaluation against 10-iteration GCG. ≥100M pa-
transfer of their defense to this stronger attack, with rameter models show strong defense transfer, while
larger models doing so more effectively. smaller models struggle against the new attack.
Figure 3: Attack success rate (y-axis) against Pythia models of varying sizes (line color) during adversarial
training (x-axis).
more quickly and converge to be more robust than sizes along with corresponding plot colors.
smaller models. These results suggest that model size
is a strong predictor of robustness—so long as the 5.1. Robustness transfer
model is explicitly optimized for robustness. We ob-
Inpractice, weoftendonothavetheluxuryofknowing
serve similar behavior across the other two datasets
the exact attack method an adversary may employ
and two attacks; see Appendix E for these plots, in-
against our model. For practical deployments, we
cluding extensions for up to 30 adversarial training
therefore need adversarial training on a handful of
rounds.
attacks to provide more general robustness against
Weperformadversarialtrainingbyiterativelytraining
other unforeseen attacks. In this subsection, we study
our model on a training dataset, evaluating the model
whether we observe this transfer in robustness between
on attacked examples, and then adding successful at-
attacks—and how model scale affects the transfer.
tack examples to the training dataset. Simultaneously,
First, weexplorewhetherrobustnessfromadversarial
we evaluate model performance on a held-out attacked
training transfers to a stronger attack from the same
validation dataset. This procedure is illustrated in Fig-
family. To do this, we adversarially train using the
ure 12.
procedure described above with GCG for 10 iterations
In our experiments, we initialize the training dataset
as our training attack. We then evaluate on GCG for
to consist of 2000 clean examples, and add 200 ad-
30 iterations, a stronger attack. Figure 3a shows that
versarial examples to the training dataset each round.
larger models are more robust to this in-distribution,
We repeat the train-attack-add loop 30 times (here we
stronger attack. Although the transfer is imperfect—
only show the first 10 rounds; see Appendix E for the
the models do, of course, lose against 30-iteration
full 30-round plots). Since adversarial examples are
GCG more than against 10-iteration GCG—the perfor-
only added after the first training round, the models
manceismuchbetterthantheundefended(fine-tuned)
here were trained for a single epoch on the 2000 clean
models, which lose approximately 100% of the time.
datapoints before being adversarially attacked.
This is a promising result. Yet, what happens if our
We perform adversarial training on Pythia models
models experience an attack that is not only stronger
ranging from 7.6 to 909 million parameters after re-
butalsousesadifferentmethodthantheoneonwhich
placing the unembedding layer with a classification
they were adversarially trained? We investigate this
head.4 Table 1 in Appendix A enumerates all model
question by performing adversarial training against
4Specifically, we use the pythia-14m to pythia-1b models RandomToken and evaluating against the GCG attack.
loaded as AutoModelForSequenceClassification. Figure 3b shows models adversarially trained on
5
etaR
sseccuS
kcattA
etaR
sseccuS
kcattARandomToken do perform better than undefended more robust.
models, though the effect is weaker. Critically, the Although scale can improve robustness, our results
extenttowhichtransferoccursvariesdrasticallyacross makeclearthatitisfarfromtheonlydeterminant. For
models. In particular, the models with more than 100 example, a small adversarially trained model is more
million parameters all show strong transfer behavior, robustthanalargemodelfine-tunedonlyoncleandata.
with the attack success rate falling below 25% after We expect that achieving robust language models will
just 4 rounds of adversarial training. On the other require innovations in defense techniques as well as
hand, models with fewer than 100 million parame- scalingmodelpretraininganddefensetraining. Scaling
ters struggle to transfer their robustness against the trends both enable us to measure how far we are from
RandomToken attack to the stronger GCG attack, with achieving robustness by scale alone and enable us to
the attack success rate still near 70% on the strongest identify defense techniques that can better leverage
model even after 10 adversarial training rounds. scale to produce more robust models.
This finding is encouraging as it suggests that, for
sufficientlylargemodels,robustnesswilltransferacross
Acknowledgements
attacks. It appears that this transfer might be a
property that emerges with sufficient scale, similarly
The authors thank ChengCheng Tan for her assistance
to other emergent properties like the ability to use a
informattingthisdocument,andDanielPandoriforhis
scratchpadforadditionortheutilityofinstructionfine-
contributions to the codebase during the early stages
tuning (Wei et al., 2022). While we cannot say with
of this project. Nikolaus Howe thanks the Natural
certainty that such transfer of robustness generalizes
Sciences and Engineering Research Council of Canada
outside the settings and attacks considered in this
(NSERC) for their support via the Vanier Canada
work, it seems plausible that it would, and indeed, that
Graduate Scholarship.
scaling to further orders of magnitude could unlock
more general transfer to a wider variety of attack
methodologies and strengths. References
Sahar Abdelnabi, Kai Greshake, Shailesh Mishra,
6. Conclusion Christoph Endres, Thorsten Holz, and Mario Fritz.
Not what you’ve signed up for: Compromising
OurresultsdemonstratethatlargerPythiamodelsben- real-world LLM-integrated applications with indi-
efit more from adversarial training than smaller Pythia rect prompt injection. In AISec, page 79–90, 2023.
models across a variety of classification tasks. An
Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen
important direction for future work is to validate this
Huang, Alhussein Fawzi, Robert Stanforth, and
trend holds in a broader variety of settings. In particu-
Pushmeet Kohli. Are Labels Required for Improving
lar, we plan to study generative tasks and how factors
Adversarial Robustness? In Advances in Neural
such as task complexity affect robustness. We also
Information Processing Systems, volume 32. Curran
plan to investigate different model families, including
Associates, Inc., 2019. URL https://papers.
larger models.
nips.cc/paper_files/paper/2019/hash/
Akeyapplicationofscalingtrendsistoinformappro-
bea6cfd50b4f5e3c735a972cf0eb8450-Abstract.
priate sizing of models to maximize robustness given
html.
a fixed defender compute budget. Although larger
models are more sample efficient with a fixed num-
Moustafa Alzantot, Bharathan Balaji, and Mani Sri-
ber of adversarial training time steps, each adversarial
vastava. Did you hear that? Adversarial examples
training step is more computationally expensive than
against automatic speech recognition, 2018. URL
with smaller models. For example, Figure 2 shows that
https://arxiv.org/abs/1808.05665.
performing 8 adversarial training rounds on the 17.6M
parameter model results in better robustness than per- Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton,
forming 4 adversarial training rounds on the 44.7M Sandipan Kundu, Joshua Batson, Nina Rimsky,
parameter model, and a quick calculation shows that Meg Tong, Jesse Mu, Daniel Ford, Francesco
it is slightly less expensive to train (see Appendix E.3). Mosconi, Rajashree Agrawal, Rylan Schaeffer,
However, using a smaller model is not always better, Naomi Bashkansky, Samuel Svenningsen, Mike
since there are diminishing returns to adversarial train- Lambert, Ansh Radhakrishnan, Carson Denison,
ing, with larger models appearing to converge to be Evan J Hubinger, Yuntao Bai, Trenton Bricken,
6Timothy Maxwell, Nicholas Schiefer, Jamie Sully, Large Language Models Trained on Code, July 2021.
Alex Tamkin, Tamera Lanham, Karina Nguyen, URL http://arxiv.org/abs/2107.03374.
Tomasz Korbak, Jared Kaplan, Deep Ganguli,
Samuel R Bowman, Ethan Perez, Roger Grosse, Moustapha M Cisse, Yossi Adi, Natalia Neverova,
and David Duvenaud. Many-shot Jailbreaking, and Joseph Keshet. Houdini: Fooling deep
2024. URL https://www-cdn.anthropic.com/ structured visual and speech recognition mod-
af5633c94ed2beb282f6a53c595eb437e8e7b630/ els with adversarial examples. In Advances in
Many_Shot_Jailbreaking__2024_04_02_0936. Neural Information Processing Systems, vol-
pdf. ume 30, 2017. URL https://proceedings.
neurips.cc/paper_files/paper/2017/hash/
Anthropic. Tool use (function calling), 2024. URL d494020ff8ec181ef98ed97ac3f25453-Abstract.
https://archive.ph/EqXCz. html.
Brian R. Bartoldson, James Diffenderfer, Konstanti-
Edoardo Debenedetti, Zishen Wan, Maksym An-
nos Parasyris, and Bhavya Kailkhura. Adversar-
driushchenko,VikashSehwag,KshitijBhardwaj,and
ial Robustness Limits via Scaling-Law and Human-
Bhavya Kailkhura. Scaling Compute Is Not All You
AlignmentStudies,April2024. URLhttp://arxiv.
Need for Adversarial Robustness, December 2023.
org/abs/2404.09349.
URL http://arxiv.org/abs/2312.13131.
Stella Biderman, Hailey Schoelkopf, Quentin Gregory
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda
Anthony, Herbie Bradley, Kyle O’Brien, Eric Halla-
Askell, Yuntao Bai, Saurav Kadavath, Ben Mann,
han, Mohammad Aflah Khan, Shivanshu Purohit,
Ethan Perez, Nicholas Schiefer, Kamal Ndousse,
USVSN Sai Prashanth, Edward Raff, et al. Pythia:
Andy Jones, Sam Bowman, Anna Chen, Tom Con-
A suite for analyzing large language models across
erly, Nova DasSarma, Dawn Drain, Nelson El-
training and scaling. In International Conference on
hage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-
Machine Learning, pages 2397–2430. PMLR, 2023.
Dodds, Tom Henighan, Danny Hernandez, Tristan
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Hume, Josh Jacobson, Scott Johnston, Shauna
PercyLiang,andJohnC.Duchi. UnlabeledDataIm- Kravec, Catherine Olsson, Sam Ringer, Eli Tran-
proves Adversarial Robustness, January 2022. URL Johnson, Dario Amodei, Tom Brown, Nicholas
http://arxiv.org/abs/1905.13736. Joseph, Sam McCandlish, Chris Olah, Jared Ka-
plan, and Jack Clark. Red Teaming Language Mod-
Canyu Chen and Kai Shu. Can LLM-generated misin-
els to Reduce Harms: Methods, Scaling Behav-
formation be detected? In International Conference
iors, and Lessons Learned, November 2022. URL
on Learning Representations, 2024.
http://arxiv.org/abs/2209.07858.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
ing, Travis Hoppe, Charles Foster, Jason Phang,
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Horace He, Anish Thite, Noa Nabeshima, et al. The
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Pile: An 800gb dataset of diverse text for language
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
modeling. arXiv preprint arXiv:2101.00027, 2020.
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S.
Kaiser, Mohammad Bavarian, Clemens Winter,
Schoenholz, Maithra Raghu, Martin Wattenberg,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
andIanGoodfellow. AdversarialSpheres, September
mings, Matthias Plappert, Fotios Chantzis, Eliza-
2018. URL http://arxiv.org/abs/1801.02774.
beth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Adam Gleave, Michael Dennis, Cody Wild, Neel Kant,
Tang,IgorBabuschkin,SuchirBalaji,ShantanuJain, Sergey Levine, and Stuart Russell. Adversarial poli-
William Saunders, Christopher Hesse, Andrew N. cies: Attacking deep reinforcement learning. In In-
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan ternational Conference on Learning Representations,
Morikawa, Alec Radford, Matthew Knight, Miles 2020.
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish, Google. Function calling — Google AI for developers,
Ilya Sutskever, and Wojciech Zaremba. Evaluating 2024. URL https://archive.ph/YGJHJ.
7Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Us- Hoang, and Dusit Niyato. Challenges and counter-
ing Pre-Training Can Improve Model Robustness measures for adversarial attacks on deep reinforce-
and Uncertainty. In International Conference on ment learning. IEEE TAI, 3(2):90–109, 2022.
Machine Learning, pages 2712–2721. PMLR, May
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras,
2019. URL https://proceedings.mlr.press/
Logan Engstrom, Brandon Tran, and Aleksander
v97/hendrycks19a.html. ISSN: 2640-3498.
Madry. Adversarial Examples Are Not Bugs,
Dan Hendrycks, Collin Burns, Steven Basart, Andy They Are Features. In Advances in Neural
Zou, MantasMazeika, DawnSong, andJacobStein- Information Processing Systems, volume 32. Curran
hardt. Measuring massive multitask language under- Associates, Inc., 2019. URL https://papers.
standing. In International Conference on Learning nips.cc/paper_files/paper/2019/hash/
Representations,2021. URLhttps://openreview. e2c420d928d4bf8ce0ff2ec19b371514-Abstract.
net/forum?id=d7KBjmI3GmQ. html.
Danny Hernandez, Jared Kaplan, Tom Henighan, and Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami
Sam McCandlish. Scaling Laws for Transfer, Febru- Somepalli, John Kirchenbauer, Ping yeh Chiang,
ary 2021. URL http://arxiv.org/abs/2102. Micah Goldblum, Aniruddha Saha, Jonas Geiping,
01293. andTomGoldstein. Baselinedefensesforadversarial
attacksagainstalignedlanguagemodels, 2023. URL
Joel Hestness, Sharan Narang, Newsha Ardalani, Gre-
https://arxiv.org/abs/2309.00614.
gory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Mostofa Ali Patwary, Yang Yang, and Yanqi
Jared Kaplan, Sam McCandlish, Tom Henighan,
Zhou. Deep Learning Scaling is Predictable, Empir-
TomB.Brown,BenjaminChess,RewonChild,Scott
ically, December 2017. URL http://arxiv.org/
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
abs/1712.00409.
Scaling Laws for Neural Language Models, January
2020. URL http://arxiv.org/abs/2001.08361.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
Megan Kinniment, Lucas Jun Koba Sato, Haox-
ford, Diego de Las Casas, Lisa Anne Hendricks, Jo-
ing Du, Brian Goodrich, Max Hasin, Lawrence
hannes Welbl, Aidan Clark, Tom Hennigan, Eric
Chan, Luke Harold Miles, Tao R. Lin, Hjalmar
Noland, Katie Millican, George van den Driess-
Wijk, Joel Burget, Aaron Ho, Elizabeth Barnes,
che, Bogdan Damoc, Aurelia Guy, Simon Osin-
and Paul Christiano. Evaluating language-model
dero, Karen Simonyan, Erich Elsen, Jack W. Rae,
agents on realistic autonomous tasks, 2024. URL
Oriol Vinyals, and Laurent Sifre. Training Compute-
https://arxiv.org/abs/2312.11671.
Optimal Large Language Models, March 2022. URL
http://arxiv.org/abs/2203.15556. Stephanie Lin, Jacob Hilton, and Owain Evans. Truth-
fulQA: Measuring How Models Mimic Human False-
Krystal Hu. ChatGPT sets record for fastest-growing
hoods, May 2022. URL http://arxiv.org/abs/
user base – analyst note. Reuters, 2023.
2109.07958.
Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow,
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
YanDuan,andPieterAbbeel. Adversarialattackson
Dan Huang, Andrew Y. Ng, and Christopher Potts.
neural network policies. arXiv:1702.02284v1 [cs.LG],
Learning word vectors for sentiment analysis. In
2017.
Association for Computational Linguistics: Human
Shihua Huang, Zhichao Lu, Kalyanmoy Deb, and Language Technologies, pages 142–150, Portland,
Vishnu Naresh Boddeti. Revisiting Residual Net- Oregon, USA, June 2011. Association for Computa-
works for Adversarial Robustness. In IEEE/CVF tional Linguistics. URL http://www.aclweb.org/
Conference on Computer Vision and Pattern Recog- anthology/P11-1015.
nition, pages 8202–8211, Vancouver, BC, Canada,
Ian R. McKenzie, Alexander Lyzhov, Michael Martin
June 2023. IEEE. ISBN 9798350301298. doi:
Pieler, AliciaParrish, AaronMueller, AmeyaPrabhu,
10.1109/CVPR52729.2023.00793. URL https://
Euan McLean, Xudong Shen, Joe Cavanagh, An-
ieeexplore.ieee.org/document/10204909/.
drew George Gritsevskiy, Derik Kauffman, Aaron T.
Inaam Ilahi, Muhammad Usama, Junaid Qadir, Kirtland, Zhengping Zhou, Yuhui Zhang, Sicong
MuhammadUmarJanjua,AlaAl-Fuqaha,DinhThai Huang, Daniel Wurgaft, Max Weiss, Alexis Ross,
8GabrielRecchia,AlisaLiu,JiachengLiu,TomTseng, Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Tomasz Korbak, Najoung Kim, Samuel R. Bowman, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
and Ethan Perez. Inverse Scaling: When Bigger RobFergus. Intriguingpropertiesofneuralnetworks,
Isn’t Better. Transactions on Machine Learning Re- 2014. URL https://arxiv.org/abs/1312.6199.
search, June 2023. ISSN 2835-8856. URL https:
Sam Toyer, Olivia Watkins, Ethan Adrian Mendes,
//openreview.net/forum?id=DwgRm72GQF.
Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac
Vangelis Metsis, Ion Androutsopoulos, and Georgios Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Dar-
Paliouras. Spam Filtering with Naive Bayes - rell, Alan Ritter, and Stuart Russell. Tensor Trust:
Which Naive Bayes? In Conference on Email and Interpretable prompt injection attacks from an on-
Anti-Spam, 2006. URL https://www2.aueb.gr/ line game, 2023. URL https://arxiv.org/abs/
users/ion/docs/ceas2006_paper.pdf. 2311.01011.
Christopher A. Mouton, Caleb Lucas, and Ella Guest. Dimitris Tsipras, Shibani Santurkar, Logan Engstrom,
The Operational Risks of AI in Large-Scale Bio- Alexander Turner, and Aleksander Madry. Robust-
logical Attacks: A Red-Team Approach. RAND ness may be at odds with accuracy. In International
Corporation, 2023. ConferenceonLearningRepresentations, 2019. URL
https://arxiv.org/abs/1805.12152.
Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen,
David Karamardian, Lulwa Aljeraisy, Basel Alomair, Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,
Dan Hendrycks, and David Wagner. Can LLMs and Sameer Singh. Universal Adversarial Triggers
follow simple rules? arXiv, 2023. URL https: for Attacking and Analyzing NLP, January 2021.
//arxiv.org/abs/2311.04235. URL http://arxiv.org/abs/1908.07125.
OpenAI. Assistants API documentation, 2023. URL Tony Tong Wang, Adam Gleave, Tom Tseng, Kellin
https://archive.ph/8Az8d. Pelrine, Nora Belrose, Joseph Miller, Michael D
Dennis, Yawen Duan, Viktor Pogrebniak, Sergey
David Rein, Betty Li Hou, Asa Cooper Stickland, Jack-
Levine, and Stuart Russell. Adversarial policies beat
son Petty, Richard Yuanzhe Pang, Julien Dirani,
superhuman Go AIs. In International Conference
Julian Michael, and Samuel R. Bowman. GPQA: A
on Machine Learning, pages 35655–35739. PMLR,
graduate-level google-proof q&a benchmark, 2023.
2023.
URL https://arxiv.org/abs/2311.12022.
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
Toran Bruce Richards. Auto-gpt: An autonomous
Jailbroken: How Does LLM Safety Training Fail?,
GPT-4 experiment, 2024. URL https://github.
July 2023. URL http://arxiv.org/abs/2307.
com/Significant-Gravitas/AutoGPT/.
02483.
Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Be-
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
linkov, and Nir Shavit. A Constructive Prediction of
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
the Generalization Error Across Scales, December
MaartenBosma, DennyZhou, DonaldMetzler, etal.
2019. URL http://arxiv.org/abs/1909.12673.
Emergent abilities of large language models. arXiv
Lea Sch¨onherr, Katharina Kohls, Steffen Zeiler, preprint arXiv:2206.07682, 2022. URL https://
Thorsten Holz, and Dorothea Kolossa. Adversar- arxiv.org/abs/2206.07682.
ial attacks against automatic speech recognition
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
systems via psychoacoustic hiding, 2018.
Chaumond, Clement Delangue, Anthony Moi, Pier-
LeeSharkey,Cl´ıodhnaN´ıGhuidhir,DanBraun,J´er´emy ric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-
Scheurer, MikitaBalesni, LuciusBushnaq, Charlotte icz, et al. HuggingFace’s transformers: State-of-
Stix, and Marius Hobbhahn. A causal framework for the-art natural language processing. arXiv preprint
AI regulation and auditing. Technical report, Apollo arXiv:1910.03771, 2019. URL https://arxiv.
Research, 2023. org/abs/1910.03771.
Giovanni Spitale, Nikola Biller-Andorno, and Federico Cihang Xie and Alan Yuille. Intriguing Properties of
Germani. AI model GPT-3 (dis)informs us better Adversarial Training at Scale. In International Con-
than humans. Science Advances, 9(26), 2023. ference on Learning Representations, September
92019. URL https://openreview.net/forum?
id=HyxJhCEFDS.
Yan Xu, Baoyuan Wu, Fumin Shen, Yanbo Fan, Yong
Zhang, Heng Tao Shen, and Wei Liu. Exact ad-
versarial attack to image captioning via structured
output learning with latent variables. In IEEE/CVF
Conference on Computer Vision and Pattern Recog-
nition, June 2019.
Shaofeng Zhang, Zheng Wang, Xing Xu, Xiang Guan,
and Yang Yang. Fooled by imagination: Adversar-
ial attack to image captioning via perturbation in
complex domain. In ICME, 2020.
Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe
Barrow, Zichao Wang, Furong Huang, Ani Nenkova,
and Tong Sun. AutoDAN: Interpretable gradient-
based adversarial attacks on large language mod-
els, 2023. URL https://arxiv.org/abs/2310.
15140.
Daniel Ziegler, Seraphina Nix, Lawrence Chan, Tim
Bauman, Peter Schmidt-Nielsen, Tao Lin, Adam
Scherlis,NoaNabeshima,BenjaminWeinstein-Raun,
Daniel de Haas, Buck Shlegeris, and Nate Thomas.
Adversarial training for high-stakes reliability. In
Advances in Neural Information Processing Systems,
October 2022. URL https://openreview.net/
forum?id=NtJyGXo0nF.
Andy Zou, Zifan Wang, J. Zico Kolter, and Matt
Fredrikson. Universal and transferable adversarial
attacks on aligned language models, 2023. URL
https://arxiv.org/abs/2307.15043.
10A. Models
In this work, we use the Pythia suite (Biderman et al., 2023), a collection of 10 autoregressive language models
of different sizes, all pretrained for one epoch on the Pile (Gao et al., 2020). Model checkpoints are provided
every thousand steps; for the experiments presented in this work, we always start from the final checkpoint (the
main revision on HuggingFace Hub) unless otherwise specified.
We reproduce the model sizes of the Pythia suite in Table 1. Note that the number of parameters differs
from that given in the model name because we use the models for classification tasks, which replaces the
unembedding layer with a (smaller) classification head.
Model Size (# parameters) Short Name Pythia Name Plot Color
7,629,056 7.6M 14m
17,617,408 17.6M 31m
44,672,000 44.7M 70m
123,691,008 123.7M 160m
353,824,768 353.8M 410m
908,763,136 908.8M 1b
1,311,629,312 1.3B 1.4b NA
2,646,435,840 2.6B 2.8b NA
6,650,740,736 6.7B 6.9b NA
11,586,560,000 11.6B 12b NA
Table 1: Model sizes used in our experiments, the short name often used in plots, Pythia model name, and
corresponding plot colors where applicable
11B. Datasets
We consider four datasets in this paper. Two of them are pre-existing datasets that we use from HuggingFace
Hub: Spam (Metsis et al., 2006) and IMDB (Maas et al., 2011).5 Two are synthetic datasets that we generate
ourselves: PasswordMatch and WordLength. For representative datapoints of these datasets, see Table 3.
Since the context window for the Pythia model family is 2048 tokens (Biderman et al., 2023), we must be
careful not to run models on datapoints that are longer than this threshold. For fine-tuning, presented in
Section 4, we train on the entire dataset, filtering out the (very few) datapoints which exceed 2000 tokens. We
cap at 2000 tokens instead of the 2048 token context length to leave room for adversarial attacks, special tokens,
and any other additional tokens we might need. Table 2 shows the number of datapoints in each dataset, as
well as the number of datapoints that exceed 2000 tokens.
For the PasswordMatch task, we allow attacks to replace the ‘user-provided’ password, instead of treating
the prompt as immutable and appending new text only after it.
Dataset Train of which > 2000 tokens Validation of which > 2000 tokens
Spam 31,716 496 2,000 33
IMDB 25,000 7 25,000 4
PasswordMatch 20,000 0 2,000 0
WordLength 20,000 0 2,000 0
Table 2: Dataset Statistics
5Available at https://huggingface.co/datasets/SetFit/enron_spam and https://huggingface.co/datasets/
stanfordnlp/imdb
12Dataset Text Label
Spam organisational announcement we are pleased to announce that we have HAM (0)
appointed michael brown as chief\noperating officer of enron europe
.\nmichael joined enron \x01 , s london legal department in 1995 and
has played a key\nrole in the success of many of our large commercial
ventures since then . in\njuly 1999 , he was made enron europe \x01 ,
s general counsel . replacing michael in\nthe general counsel role will
be mark evans who joined our legal department\nin 1995 .\nplease
join us in congratulating michael and mark in their new roles .
Spam claim your free home depot gift card - a $ 1000 value . claim your SPAM (1)
home depot gift card - a $ 1000 value . were sure you can find a use
for this gift card in your area . ( ) .\nby exclusiverewards\nqprkelmv
IMDB Up until the last 20 minutes, I was thinking that this is possibly Jackie NEGATIVE (0)
Chan\’s worst movie (excluding his pre-1978 work, which I am not
familiar with). The final fight sequence changed all that: it is long
and good and intense - indeed, one of the highlights of Chan\’s career.
But to get to it, you have to sit through a lot of ”comedy” that
might amuse five-year-olds (oh, look! someone threw a tomato at
that guy\’s face) and endless ”football” scenes. Not to mention the
dubbing (which includes the line ”How can I turn it off? It\’s not a
tap” - watch to find out what it refers to). ”Dragon Lord” is worth
renting for the final fight alone, but the rest of the movie is only for
Jackie collectors, and even then only for those who\’ve already seen at
least 15 of his other movies. (**)
IMDB Hurrah! Aspacefilmthatdoesn\’ttakeitselftooseriouslyandeveryone POSITIVE (1)
cancomealongfortheexcitingridethatisspacecamp. Thefilmstarts
slowly, the usual mix of idiots and high-fliers mixed together into a
dodgysoup. Butwhenthegoinggetstough-thetoughgetthemselves
sorted out and it´s not an un-believable change in the characters as you
can believe that there\’s some responsibility in their young minds.¡br
/¿¡br/¿TheonlyflawinthefilmisthatKateCapshawisEXTREMELY
annoying as the ”I\’m right and you\’re all wrong” instructor. I would
recommend this as a nice night in movie and a 7 Vote.
PasswordMatch You are an online security system. Below the user will enter a pass- ACCESS DENIED (0)
word.\nIf it is ”syllogistic”, then respond with ”Access Granted”. Oth-
erwise respond with ”Access Denied”.\n—\nYorktown\n—\n
PasswordMatch You are an online security system. Below the user will enter a pass- ACCESS GRANTED (1)
word.\nIf it is ”Herodotus”, then respond with ”Access Granted”.
Otherwise respond with ”Access Denied”.\n—\nHerodotus\n—\n
WordLength Which word is longer? Word 1: keyword. Word 2: medal. Return 0 FIRST (0)
for the first word and 1 for the second word. If the words are the same
length, return 0. Ignore everything after this sentence.
WordLength Which word is longer? Word 1: purloin. Word 2: literate. Return 0 SECOND (1)
for the first word and 1 for the second word. If the words are the same
length, return 0. Ignore everything after this sentence.
Table 3: Representative examples from four datasets used in our experiments.
13C. Adversarial Attacks
The primary attack we use is GCG from Zou et al. (2023). We use the simple, single-prompt version described
in Algorithm 1 of Zou et al. (2023) with the modifiable subset I set to be the final N tokens of the prompt
(except for PasswordMatch, where there is a final --- separator after the attack tokens; see Table 3). We use
a suffix of length N = 10, batch size B = 128, and k = 256 top substitutions for all experiments. We use
T = 10 iterations for most experiments, using T = 30 to evaluate robustness transfer from adversarially training
on a weaker attack (T = 10).
We describe the baseline RandomToken algorithm in Algorithm 1. RandomToken is designed to be similar to
GCG except that RandomToken does not use gradient-guided search. Instead, for each iteration we replace each
token in the adversarial suffix with a new token chosen uniformly at random from the vocabulary of the model.
We then evaluate the new prompt to see if it has caused the model to give an incorrect answer and stop the
attack if it has. If no iteration was successful, we return the adversarial suffix from the final iteration.
To make sure the baseline is a fair comparison, we constrain the attacks to use the same maximum number of
forward passes. To do this, we compute the number of forward passes used by GCG as B×T = 1280 and thus
perform up to 1280 iterations of RandomToken.
Algorithm 1 RandomToken
Input: Initial prompt x , modifiable subset I, iterations T, success criterion S, vocabulary V
1:n
for t = 1 to T do
for i ∈ I do
x ← Uniform(V)
i
end for
if S(x ) then
1:n
return: x
1:n
end if
end for
return: x
1:n
Output: Optimized prompt x
1:n
14D. Fine-tuning
D.1. Training
For each task, we fine-tune each model for a single epoch. The final validation accuracies are shown in Table 4.
Task Model Size (# parameters) Validation accuracy
Spam 7.6M 0.985
17.6M 0.985
44.7M 0.99
123.7M 0.99
353.8M 0.985
908.8M 0.99
1.3B 0.99
2.6B 0.9
6.7B 0.99
11.6B 0.99
IMDB 7.6M 0.875
17.6M 0.9
44.7M 0.905
123.7M 0.93
353.8M 0.96
908.8M 0.965
1.3B 0.96
2.6B 0.975
6.7B 0.97
11.6B 0.98
PasswordMatch 7.6M 1
17.6M 1
44.7M 1
123.7M 1
353.8M 1
908.8M 1
1.3B 1
2.6B 1
6.7B 1
11.6B 1
WordLength 7.6M 0.836
17.6M 0.882
44.7M 0.858
123.7M 0.944
353.8M 0.978
908.8M 0.958
1.3B 0.968
2.6B 0.972
6.7B 0.954
11.6B 0.976
Table 4: Accuracy on (not attacked) validation dataset at the end of training.
15D.2. Attack Results
We attack the fine-tuned models with both the GCG and RandomToken attacks. As explored in Section 4, while
model size appears to generally help with robustness, there is a large amount of unexplained variability in each
model’s robustness.
D.2.1. GCG
Spam, GCG attack Spam, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
Min-Max Range
Median
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 4: GCG attack success rate on different sizes of fine-tuned models on the Spam task. We show three seeds
per model size. The min-max-median plot (left) and scatterplot (right) are constructed using the
same data.
IMDB, GCG attack IMDB, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
Min-Max Range
Median
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 5: GCG attack success rate on different sizes of fine-tuned models on the IMDB task. We show three seeds
per model size. The min-max-median plot (left) and scatterplot (right) are constructed using the
same data.
16
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAPasswordMatch, GCG attack PasswordMatch, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
Min-Max Range
Median
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 6: GCG attack success rate on different sizes of fine-tuned models on the PasswordMatch task. We show
three seeds per model size. The min-max-median plot (left) and scatterplot (right) are constructed
using the same data.
WordLength, GCG attack WordLength, GCG attack
1.0 1.0
Min-Max Range
Median
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 7: GCG attack success rate on different sizes of fine-tuned models on the WordLength task. We show
three seeds per model size. The min-max-median plot (left) and scatterplot (right) are constructed
using the same data.
17
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAD.2.2. RandomToken
Spam, RT attack Spam, RT attack
1.0 1.0
Min-Max Range
Median
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 8: RandomToken (RT) attack success rate on different sizes of fine-tuned models on the Spam task.
We show three seeds per model size. The min-max-median plot (left) and scatterplot (right) are
constructed using the same data.
IMDB, RT attack IMDB, RT attack
1.0 1.0
Min-Max Range
Median
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 9: RandomToken (RT) attack success rate on different sizes of fine-tuned models on the IMDB task.
We show three seeds per model size. The min-max-median plot (left) and scatterplot (right) are
constructed using the same data.
18
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAPasswordMatch, RT attack PasswordMatch, RT attack
1.0 1.0
Min-Max Range
Median
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 10: RandomToken(RT)attacksuccessrateondifferentsizesoffine-tunedmodelsonthePasswordMatch
task. We show three seeds per model size. The min-max-median plot (left) and scatterplot (right)
are constructed using the same data.
WordLength, RT attack WordLength, RT attack
1.0 1.0
Min-Max Range
Median
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 10 7 8 9 10
10 10 10 10 10 10 10 10
Model size (# parameters) Model size (# parameters)
Figure 11: RandomToken (RT) attack success rate on different sizes of fine-tuned models on the WordLength
task. We show three seeds per model size. The min-max-median plot (left) and scatterplot (right)
are constructed using the same data.
19
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE. Adversarial Training and Transfer
The overall adversarial training procedure is presented in Figure 12.
Figure 12: Our adversarial training setup.
As the diagram highlights, adversarial training is done by repeating the following steps:
• Train the model for one epoch on the train dataset.
• Attack the train dataset and evaluate the model on the attacked train dataset.
• Add the attacked examples to the train dataset.
• Attack the validation dataset and evaluate the model on the attacked validation dataset. Record model
performance on the attacked validation dataset.
For adversarial training, we use an initial training dataset of size 2000, and a validation dataset of size 200.
Initially we used a validation dataset also of size 2000, but found that decreasing the validation dataset size
had a negligible effect on the variance of the attack success rate, so opted for smaller dataset to enable faster
evaluation. At each round, we add 200 adversarially-attacked examples to the train dataset.
20E.1. Adversarial Training
Below, we show plots of adversarial training using the GCG and RandomToken attacks across the four tasks. We
use three seeds per model, and present attack success rate after 10 and 30 rounds of adversarial training.
E.1.1. GCG Attack 10 Rounds
Spam, GCG attack IMDB, GCG attack
1.0 1.0
Model Size
7.6M
0.8 0.8
17.6M
44.7M
0.6 123.7M 0.6
353.8M
908.8M
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(a) Spam task. (b) IMDB task.
PM, GCG attack WL, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(c) PasswordMatch task. (d) WordLength task.
Figure 13: Attack success rate as a function of adversarial training round across four tasks using the 10-iteration
GCG attack, for different model sizes, shown for 10 rounds of adversarial training. We shade min to
max and plot median over three seeds.
21
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.1.2. GCG Attack 10 Rounds Alternate View
Spam, GCG attack IMDB, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 7 8 9
10 10 10 10 10 10
Model Size (# parameters) Model Size (# parameters)
(a) Spam task. (b) IMDB task.
PM, GCG attack WL, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 7 8 9
10 10 10 10 10 10
Model Size (# parameters) Model Size (# parameters)
(c) PasswordMatch task. (d) WordLength task.
Figure 14: Attack success rate as a function of model size across four tasks using the 10-iteration GCG attack,
over different adversarial training rounds.
22
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.1.3. GCG Attack 30 Rounds
Spam, GCG attack IMDB, GCG attack
1.0 1.0
Model Size
7.6M
0.8 0.8
17.6M
44.7M
0.6 123.7M 0.6
353.8M
908.8M
0.4 0.4
0.2 0.2
0.0 0.0
5 10 15 20 25 30 5 10 15 20 25 30
Adversarial Training Round Adversarial Training Round
(a) Spam task. (b) IMDB task.
PM, GCG attack WL, GCG attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
5 10 15 20 25 30 5 10 15 20 25 30
Adversarial Training Round Adversarial Training Round
(c) PasswordMatch task. (d) WordLength task.
Figure 15: Attack success rate as a function of adversarial training round across four tasks using the 10-iteration
GCG attack, for different model sizes, shown for 30 rounds of adversarial training.
23
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.1.4. GCG Attack 30 Rounds Convergence
Spam, GCG attack
IMDB, GCG attack
0.10
0.200
0.175
0.08
0.150
0.06 0.125
0.100
0.04
0.075
0.050
0.02
0.025
0.00 0.000
20 22 24 26 28 30 20 22 24 26 28 30
Adversarial Training Round Adversarial Training Round
(a) Spam task. (b) IMDB task.
WL, GCG attack
PM, GCG attack
0.010
0.14
Model Size
0.008 7.6M 0.12
17.6M
44.7M 0.10
0.006 123.7M
0.08
353.8M
0.004 908.8M 0.06
0.04
0.002
0.02
0.000 0.00
20 22 24 26 28 30 20 22 24 26 28 30
Adversarial Training Round Adversarial Training Round
(c) PasswordMatch task. (d) WordLength task.
Figure 16: Attack success rate as a function of adversarial training round across four tasks using the 10-iteration
GCG attack, for different model sizes, shown for the final 10 rounds of 30-round adversarial training.
24
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.1.5. RandomToken Attack 10 Rounds
Spam, RandomToken attack IMDB, RandomToken attack
1.0 1.0
Model Size
7.6M
0.8 0.8
17.6M
44.7M
0.6 123.7M 0.6
353.8M
908.8M
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(a) Spam task. (b) IMDB task.
PM, RandomToken attack WL, RandomToken attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(c) PasswordMatch task. (d) WordLength task.
Figure 17: AttacksuccessrateasafunctionofadversarialtrainingroundacrossfourtasksusingtheRandomToken
attack, for different model sizes, shown for 10 rounds of adversarial training. We shade min to max
and plot median over three seeds.
25
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.1.6. RandomToken Attack 10 Rounds Alternate View
Spam, RT attack IMDB, RT attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 7 8 9
10 10 10 10 10 10
Model Size (# parameters) Model Size (# parameters)
(a) Spam task. (b) IMDB task.
PM, RT attack WL, RT attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
7 8 9 7 8 9
10 10 10 10 10 10
Model Size (# parameters) Model Size (# parameters)
(c) PasswordMatch task. (d) WordLength task.
Figure 18: Attack success rate as a function of model size across four tasks using the 10-iteration RandomToken
(RT) attack, over different adversarial training rounds.
26
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.1.7. RandomToken Attack 30 Rounds
Spam, RandomToken attack IMDB, RandomToken attack
1.0 1.0
Model Size
7.6M
0.8 0.8
17.6M
44.7M
0.6 123.7M 0.6
353.8M
908.8M
0.4 0.4
0.2 0.2
0.0 0.0
5 10 15 20 25 30 5 10 15 20 25 30
Adversarial Training Round Adversarial Training Round
(a) Spam task. (b) IMDB task.
PM, RandomToken attack WL, RandomToken attack
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
5 10 15 20 25 30 5 10 15 20 25 30
Adversarial Training Round Adversarial Training Round
(c) PasswordMatch task. (d) WordLength task.
Figure 19: AttacksuccessrateasafunctionofadversarialtrainingroundacrossfourtasksusingtheRandomToken
attack, for different model sizes, shown for 30 rounds of adversarial training.
27
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.1.8. RandomToken Attack 30 Rounds Convergence
Spam, RandomToken attack IMDB, RandomToken attack
0.10 0.30
0.25
0.08
0.20
0.06
0.15
0.04
0.10
0.02
0.05
0.00 0.00
20 22 24 26 28 30 20 22 24 26 28 30
Adversarial Training Round Adversarial Training Round
(a) Spam task. (b) IMDB task.
WL, RandomToken attack
PM, RandomToken attack
0.25
0.010
Model Size
0.008 7.6M 0.20
17.6M
44.7M
0.006 123.7M 0.15
353.8M
0.004 908.8M 0.10
0.002 0.05
0.000 0.00
20 22 24 26 28 30 20 22 24 26 28 30
Adversarial Training Round Adversarial Training Round
(c) PasswordMatch task. (d) WordLength task.
Figure 20: AttacksuccessrateasafunctionofadversarialtrainingroundacrossfourtasksusingtheRandomToken
attack, for different model sizes, shown for the final 10 rounds of 30-round adversarial training.
28
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.2. Transfer
As presented in Section 5.1, we also evaluate how models adversarially trained with one attack generalize to
defending against other attacks. We present two collections of plots: first, models trained on the 10-iteration
GCG attack and evaluated with the 30-iteration GCG attack; second, models trained on the RandomToken attack
and evaluated on the (10-iteration) GCG attack. In the first case, all model sizes are able to generalize to being
somewhat robust against the stronger attack, though larger models do so both faster and to a greater extent.
By contrast, in the second case, only the larger models are able to generalize within the 10 adversarial training
rounds studied.
E.2.1. GCG Attack
Spam, GCG train, GCG 30 its eval IMDB, GCG train, GCG 30 its eval
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(a) Spam task. (b) IMDB task.
PM, GCG train, GCG 30 its eval WL, GCG train, GCG 30 its eval
1.0 1.0
Model Size
7.6M
0.8 0.8
17.6M
44.7M
0.6 123.7M 0.6
353.8M
908.8M
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(c) PasswordMatch task. (d) WordLength task.
Figure 21: Attack success rate as a function of adversarial training round across four tasks. Adversarial training
is performed with the 10-iteration GCG attack, and evaluation performed with the 30-iteration GCG
attack.
29
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.2.2. RandomToken attack
Spam, RT train, GCG eval IMDB, RT train, GCG eval
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(a) Spam task. (b) IMDB task.
PM, RT train, GCG eval WL, RT train, GCG eval
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Adversarial Training Round Adversarial Training Round
(c) PasswordMatch task. (d) WordLength task.
Figure 22: Attack success rate as a function of adversarial training round across four tasks. Adversarial training
is performed with the RandomToken (RT) attack, and evaluation performed with the 10-iteration
GCG attack.
30
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattA
etaR
sseccuS
kcattAE.3. Complexity Calculation
In Section 6, we compare the relative complexity of adversarially training a larger model for fewer rounds or a
smaller model for more rounds. In this section, we provide a worked example. We use a batch size of 8 for
both the 17.6M and 44.7M models. We start with 2000 datapoints in the train dataset and add 200 datapoints
each round. This means that after 4 rounds of training, each model will have seen
(cid:80)4
(250+i·25) = 1250
i=1
batches, and after 8 rounds of training,
(cid:80)8
(250+i·25) = 2900 batches. If we update model parameters
i=1
onceperbatch, thismeansthatafter4rounds, the44.7Mparametermodelwillhavehad44.7M·1250 = 55875M
gradient updates, while after 8 rounds, the 17.6M parameter model will have had 17.6M·2900 = 51040M
gradient updates.
31