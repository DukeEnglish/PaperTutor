Unlocking Tokens as Data Points for Generalization
Bounds on Larger Language Models
Sanae Lotfi1,∗ Yilun Kuang1,∗ Brandon Amos2,†
Micah Goldblum1 Marc Finzi3 Andrew Gordon Wilson1
1New York University 2Meta AI 3Carnegie Mellon University
Abstract
Large language models (LLMs) with billions of parameters excel at predicting the next
token in a sequence. Recent work computes non-vacuous compression-based generalization
bounds for LLMs, but these bounds are vacuous for large models at the billion-parameter
scale. Moreover, these bounds are obtained through restrictive compression techniques,
bounding compressed models that generate low-quality text. Additionally, the tightness of
these existing bounds depends on the number of IID documents in a training set rather
than the much larger number of non-IID constituent tokens, leaving untapped potential
for tighter bounds. In this work, we instead use properties of martingales to derive
generalization bounds that benefit from the vast number of tokens in LLM training sets.
Since a dataset contains far more tokens than documents, our generalization bounds not
only tolerate but actually benefit from far less restrictive compression schemes. With
Monarch matrices, Kronecker factorizations, and post-training quantization, we achieve
non-vacuous generalization bounds for LLMs as large as LLaMA2-70B. Unlike previous
approaches, our work achieves the first non-vacuous bounds for models that are deployed
in practice and generate high-quality text.
1 Introduction
Despite the impressive empirical performance of large language models (LLMs), our theoretical
understanding of their performance is lacking. PAC-Bayes and the related finite hypothesis
generalization bounds [5, 13, 17] offer a compelling framework for understanding this good
performance through the lens of compression. These bounds tell us that a model will provide
good generalization if it is capable of fitting its training data while simultaneously being
compressible relative to the size of its training set. The generalization bounds literature includes
many techniques for achieving tighter bounds on image classification problems, ranging from
improved bounds themselves to new compression methods [53, 14, 18, 38, 31].
Recent work presented the first non-vacuous generalization bounds for large language models,
considering training points to be independent and identically distributed (IID) documents [32].
The authors compute generalization bounds for the expected bits-per-dimension (BPD) loss,
defined for a document X composed of k tokens and a language model h as the average negative
log probability BPD(h,X) = −1 (cid:80)klog p (x |x ). These bounds are only non-vacuous for
k i 2 h i <i
compressed GPT2 variants [39] that output un-grammatical text. The term vacuous refers to
∗Equalcontribution,orderdecidedbycoinflip. Correspondenceto: SanaeLotfi<sl8160@nyu.edu>,
AndrewGordonWilson<andrewgw@cims.nyu.edu>.
†Meta-affiliatedauthorwasinvolvedonlyinanadvisoryrole. Allexperimentationanddataprocessingwere
conductedatNYU.
1
4202
luJ
52
]LM.tats[
1v85181.7042:viXra8
LLaMA1 LoRA
7
LLaMA2 3.0 Kronecker 7
LLaMA2-Chat Monarch
6
6 2.8
5
5
2.6
4
4
3 4 3 4 6 7
10 10 10 10 10 10
Compressed Model Size (MB) Compressed Model Size (MB) Trainable Parameters
Figure 1: Non-vacuous bounds for LLMs that scale up to 70B parameters. Left: Bits
per dimension (BPD) bounds on the Amber dataset [29] which contains 1.2 trillion tokens for
different LLMs from the LLaMA family ranging in scale from 7 billion to 70 billion parameters
[47]. All of these models are quantized to 2-bits, 3-bits and 4-bits per-weight using QuIP#
and are publicly available [48]. The different quantization precisions are accounted for in
the compressed model size. The trade-off between the empirical performance and the model
complexity in our bounds favors models with a smaller compressed size in general, though we
observe that across different architectures we can find larger models yielding better bounds.
Middle: The BPD training loss for different models from the LLaMA family—the legend is
sharedwiththefigureontheleft. Overall,weobservethatlargermodelsyieldalowerBPDwhile
having a higher compressed size. Right: Validation negative log-likelihood loss as a function of
the total number of trainable parameters for different nonlinear parametrization; namely low
rank adaptation (LoRA), the Kronecker decomposition of dense matrices and Monarch matrices.
The x-axis is in the log scale. As we vary the numer of trainable parameters, there are different
optimal compression techniques.
the random guess performance on next token prediction, which is log V for BPD where V is
2
the vocabulary size.
Compression-basedgeneralizationboundsatthedocumentlevelsufferfromthreeprimarylimita-
tions: (1) the number of documents in a training set is limited, and this small sample size leads
toloosebounds;(2)duetothesmallsamplesize,non-vacuousgeneralizationboundscanonlybe
achieved using compression techniques which significantly modify the LLM pretraining routine.
This limitation also applies to state-of-the-art generalization bounds for image classification,
which heavily alter the training procedure to optimize the bounds [53, 38, 31]; (3) as a result,
the models which produce non-vacuous bounds generate low-quality text, so it is unclear what
these bounds can tell us about more performant language models.
In this work, we address the above limitations and use our bounds to derive insights about the
generalization properties and limitations of LLMs. Namely, we make the following contributions:
• In Section 4, we present and derive a generalization bound that considers each sample to
be an individual token. Even though tokens within a document are not independent, we
use properties of martingales to obtain a valid bound that benefits from the number of
tokens in a language model’s pretraining dataset.
• In Sections 5 and 6, we explore several expressive model compression techniques such as
Monarch matrices, Kronecker factorizations, and post-training quantization and show that
bounding the performance at the token-level favors less restrictive compression strategies.
• Ourworkisthefirsttocomputenon-vacuousgeneralizationboundsformodelscompressed
only through post-training quantization and without altering the pretraining procedure
at all. Consequently, we obtain generalization bounds for massive pretrained LLMs like
LLaMA2-70B, as shown in Figure 1(Left) and Section 6, which generate high-quality text.
2
dnuoB
DPB
ssoL
DPB
niarT
ssoL
noitadilaV• Our experiments in Section 6 indicate that the chat versions of LLaMA have looser gener-
alization guarantees, demonstrating that fine-tuning these models for dialogue negatively
affects their performance on the next token prediction task.
• In Section 6.4, we demonstrate that GPT2 models that are restricted to only seeing k
tokens in their context for training and evaluation obtain significantly better bounds
than k-th order Markov chains for high values of k, reflecting the remarkable ability of
transformer-based models in capturing longer range correlations.
• WeshowinSection6.5thatamodel’sabilitytorecallmemorizedfactsfromitspretraining
data deteriorates faster than its ability to recognize structured patterns as we decrease the
size of the model through compression, distinguishing between compressible tasks where
generalization is possible and incompressible tasks that correspond to sheer memorization.
2 Related Work
Generalization bounds for neural networks. Deep neural networks are challenging to
understand using generalization theory due to their many parameters [51]. However, over the
pastyears,therehasbeensuccessinconstructingmeaningfulboundscoveringimageclassification
models [13], vision-language models [1], and tabular data [17], often through the methodology
of compression [53, 31]. Lotfi et al. [32] extend compression-based generalization bounds to
the LLM setting, and obtain non-vacuous bounds at the document level. Li et al. [27] explore
generalization in few-shot learning, establishing bounds based on in-context examples while
maintaining a fixed pretrained model. In contrast, we investigate pretraining generalization
bounds to understand why models do not overfit at training time, despite the increased dataset
complexity.
Non-IID Generalization bounds. While the vast majority of generalization bounds assume
that the different data points are drawn independently, there are a handful of works that aim to
relaxthisassumptionindifferentways. Ralaivolaetal.[41]analyzethedependencegraphofthe
random variables, deriving a bound based on the graph coloring number, fitting into a broader
line of work making use of properties of the dependence graph [52]. Unfortunately for text data,
the dependencies are unknown or assumed to follow the triangular autoregressive dependency
structure for all pairs in the sequence, which limits the applicability of such an approach. A
related line of work has been to explicitly estimate coefficients which quantify the extent that
random variables relate to each other [e.g., 33, 24]. However, it is unclear how best to apply
these methods to neural networks. Martingale tail bounds are sometimes used in online learning
and reinforcement learning, e.g., for establishing regret bounds [40]. Chugg et al. [7] present
a large collection of generalization bounds both in the IID and martingale settings, including
generalization bounds which could be used at the token level such as the one we derive. Their
results extend and generalize many existing bounds. We view our contribution as orthogonal
to these efforts since we focus on constructing the components necessary to generate practical
bounds for LLMs, rather than innovating on concentration inequalities themselves.
Large language models and compression. In recent years, language models went from
millions to billions of parameters, leading to consistent improvements across downstream
applications. A variety of compression techniques have been proposed to account for the
increased computational costs of LLMs while preserving their performance. Parameter-efficient
finetuningmethods,suchasLoRA[19],parametrizeweightmatricesasproductsoftwotrainable
low-rank matrices on top of frozen pretrained weights. QLoRA uses 4-bit NormalFloat (NF4)
and double quantization, enabling single-GPU finetuning for a 65B parameter LLM without
performance degradation [10, 11]. Post-training quantization approaches, such as GPTQ [16],
rely on second-order information and quantize each row of weight matrices independently. QuIP
uses adaptive rounding and incoherence processing of second-order Hessian matrices, enabling
2-bitquantizationofLLMs[6]. OthercompressiontechniquesforLLMsincludereplacingmostof
3the 16-bit operations with 8-bit matrix multiply [10], using data-free distillations [28], designing
custom kernels and sub-4-bit integer quantization [22, 36], and compressing embeddings as
low-rank matrix-product state [50].
3 Background
In this section, we review the different components of compression-based generalization bounds,
which we build upon with our method in Sections 4 and 5.
Finite hypothesis compression bounds. Let R(h,x)∈[a,a+∆] be a bounded risk and
h∈H be a hypothesis drawn from a finite hypothesis space with prior P(h). A classic finite
hypothesis generalization bound [43] states that for any δ >0 with probability 1−δ,
(cid:114)
log1/P(h)+log1/δ
R(h)≤Rˆ(h)+∆ (1)
2m
where the empirical risk is defined as Rˆ(h) := 1 (cid:80)m R(h,x ) with {x }m being IID and
m i=1 i i i=1
R(h) = E[Rˆ(h)]. The complexity term depends on the prior log probability log1/P(h). We
use the Solomonoff prior P(h)≤2−K(h) [45], where K(h) is the prefix Kolmogorov complexity
of h defined as the length of the shortest program that produces h for a fixed programming
language [23]. Consequently, our prior favors models h that have a small minimum compressed
length. While the Kolmogorov complexity is incomputable, it can be bounded as log1/P(h)≤
K(h)log2≤C(h)log2+2logC(h),whereC(h)isthecompressedsizeofthemodelaccordingto
a pre-specified compressor. Therefore, we can find the right trade-off between the empirical risk
and the compressed size of the model by tuning the extent of compression, hence the different
compression techniques we explore in this work.
Compression bounds for LLMs. When constructing document-level bounds for language,
the empirical risk is defined over an entire document X as R(h,X)=−log p (X)/L, where
2 h
p (X) is defined auto-regressively on the sequence of tokens X = [x ,x ,...x ] as p (X) =
h 1 2 L θ
(cid:81)L p (x |x ), where x denotes x ,x ,...,x .
i=1 h i <i <i 1 2 i−1
Prediction smoothing. Since the bound in Equation (1) only applies to a bounded risk, it is
not valid for the bits-per-dimension loss that is unbounded. In this case, one can introduce a
prediction smoothing probability α to the predictive model such that the generative probability
distribution becomes a mixture between the next token probability according to the auto-
regressive model f(θ) with parameters θ and a uniform distribution over the vocabulary of size
V as follows: p (x |x )=(1−α)p (x |x )+α/V. With this construction, R(h,X) can be
h i <i θ i <i
bounded in an interval of size ∆ = log (1+(1−α)V/α). The optimal hyperparameter α is
2
determined via a grid search in Lotfi et al. [32].
Compressing LLMs with SubLoRA. To achieve the extreme compression level necessary
to obtain non-vacuous document-level bounds, Lotfi et al. [32] propose SubLoRA, a non-linear
subspace parametrization of an LLM’s weights θ. Using SubLoRA, these weights can be written
as θ = θ +LoRA(Pw). Here θ ∈ RD are the model weights at random initialization and
0 0
LoRA(Pw) combines low-rank adaptation (LoRA) [19] with subspace training [31] via the
projector P ∈ RD×d. The LoRA decomposition parameterizes a dense matrix W ∈ Ra×b
as the product of two low-rank matrices A ∈ Ra×r,B ∈ Rr×b with a small rank r. As for
the linear subspace parametrization Pw, the projection matrix P is defined as a Kronecker
√ √ √
product P =Q ⊗Q produced by orthogonalizing Q ,Q ∼ N(0,1/ D) D× d via a QR
1 2 1 2
decomposition.
In practice, a selected subset of the dense matrices in an LLM are parameterized using LoRA’s
low rank matrices, then the concatenation of LoRA’s matrices is projected into the subspace
parameters w using P. The model is therefore effectively trained via the weights w ∈Rd. As a
result, the model can be coded via a random seed that reproduces the pre-fixed initialization θ
0
4and projection matrix P, and a coding of w which is performed using arithmetic coding [25].
The dimension d of w can be varied to achieve the best trade-off between empirical risk and
complexity, and these degrees of freedom are accounted for in the coding of the hypothesis h.
4 Token-Level Generalization Bounds
In order to unlock a deeper understanding of LLM generalization, it is not sufficient to consider
the training data at the level of entire documents. In fact, token-level performance is arguably
what we care about most when evaluating a model’s generalization on its next token prediction
pretraining task. Moreover, simplifying the bounds to meet the IID assumption over sampled
documents restricts our ability to capture the dependencies between individual tokens. In this
section, we derive novel bounds at the token level through a simple yet powerful application of
Azuma’s inequality that allows us to use the properties of martingales to go beyond the IID
setting. Then, we discuss the interpretation of our bounds and demonstrate their ability to
predict generalization on downstream tasks. Finally, we introduce a new optimization strategy
for tuning a prediction smoothing hyperparameter that further improves our bounds.
4.1 A Novel Non-IID Token-Level Generalization Bound
In deriving token-level bounds, one might consider applying Equation (1) to the finite dataset
D ={(x ,x )}M composed of input and output pairs. In this scenario, model training can be
<i i i=1
performed on a random subset S ⊂ D of m pairs, which differs from how training is usually
performed via contiguous sequences. Then, we could use the performance on S to bound the
average performance on D since S is constructed as an IID sample from D. While these bounds
are valid, they require fundamentally altering the training procedure, and they only pertain
to the held out pairs which must be collected in advance and separated from their naturally
occurring context.
Toavoidtheselimitations,weconstructanovelboundthatnaturallyaccommodatesthenon-IID
structure of the tokens as they occur in documents as follows:
Theorem 1. With probability at least 1 − δ over the randomness in a sampled sequence
{x ,x ,...,x },ifthenegativeloglikelihoodofamodelh∈Hcanbebounded−log p (·|x )∈
1 2 m 2 h <i
[a,a+∆ ], then the negative log likelihood of the data for model h satisfies
i
m (cid:114)
1 (cid:88) E[−log p (X |x )|x ]≤− 1 log p (x )+∆ˆ log1/P(h)+log1/δ , (2)
m 2 h i <i <i m 2 h ≤m 2m
i=1
(cid:113)
where ∆ˆ = 1 (cid:80)m ∆2, the expectation is taken over X ∼p(X |x ) from the data generating
m i=1 i i i <i
process, and P(h) is any normalized prior over a discrete hypothesis space H that does not
depend on {x }m .
i i=1
We provide a proof sketch as well as the full proof in Appendix A.1.
On the right-hand side of the bound is the conventional empirical risk: −1 log p (x ) =
−1 (cid:80) log p (x |x )onthemeasuredsequenceandacomplexitytermlog1/Pm (h).2 Wh ede≤ sm cribe
m i 2 h i <i
in detail how we sample sequence x and compute the empirical risk in Section 4.2. The
≤m
quantity which we are bounding on the left-hand side is the expected next token negative
log-likelihood under resampling from the data generating process, averaged over the different
contexts that have been encountered in the training set. The bound ensures generalization on
contexts seen at training when the next tokens are resampled, but not on data with contexts
that are different. However, given how diffuse the distribution over next tokens is, e.g., at the
beginning of a new sentence, our bounds remain predictive of generalization and achieving a
non-vacuous bound requires generalization. We further discuss the interpretation of our bounds
in Section 4.3.
530000 5.0
8 120 PPL ACC
100 BPD 4.8
6
20000
80 4.6
4
60 4.4
10000 2 40 4.2
0 20
4.0
0
2 4 6
0 2 4 8 16 128 256 512 768 1023 108 109
Entropy Token Index Trainable Parameters
Figure 2: Our bounds analyze a quantity that is meaningful and predictive of
generalization. Left: Using LLaMA2-7B, we compute the entropy of p(x |x ), where the
i <i
context x is fixed and sampled from the Amber training dataset. The distribution over next
<i
tokens given a fixed context from the training data is indeed diffuse and characterized by high
entropy values. Middle: Entropy of p(x |x ) as a function of the token index i shown on the
i <i
x-axis for a context length L=1024. The average entropy has a decreasing trend but remains
high overall; note that the average entropy for i = 768 is as high as the average entropy for
i=128. Right: Onthelefty-axis,weplottheaveragezero-shotaccuracy(ACC)andperplexity
(PPL) achieved by GPT2 models ranging in scale from 117M to 1.5B averaged over downstream
datasets, as reported in Radford et al. [39]. On the right y-axis, we plot an approximation of
the conditional BPD expectation that we bound in Equation (2) where we resample x from a
i
LLaMA2-7B given fixed training contexts x from the Amber dataset. The approximation of
<i
the BPD objective that we bound achieves 97.9% and 99.1% correlation with the accuracy and
perplexity, respectively.
4.2 Sampling and Empirical Risk Evaluation
In this section, we more precisely define the sequence x for which we compute the empirical
≤m
risk in Equation (2). We construct a sample x from the stochastic process p by first
≤m data
sampling independent and identically distributed documents, e.g., the documents that form
the OpenWebText dataset. Then, we concatenate these documents deterministically using end
of text (EOT) tokens. Consequently, the ground truth stochastic process has the following
property:
p (x |x )=p (x |x ,....,x ), (3)
data i <i data i k i−1
where x is the previous EOT token. This equality holds exactly due to how the stochastic
k
process is implemented.
On the other hand, it would not be guaranteed that a generative model p (x) satisfies the
h
property in Equation (3) apriori if the model were allowed to attend to tokens x , even when
<k
the data generating process has this property. However, we explicitly prohibit our generative
model h from attending to tokens x through the attention mask, as we have the flexibility to
<k
do so in defining our hypothesis class and model family. Therefore, our model p that we bound
h
also satisfies this property p (x |x )=p (x |x ,....,x ) exactly, and not approximately.
h i <i h i k i−1
In conclusion, the empirical risk for our generative model h and a sequence x sampled from
≤m
the stochastic process defined above can be written as follows:
1 1 (cid:88) 1 (cid:88)
− log p (x )=− log p (x |x )=− log p (x |x ,...x ),
m 2 h ≤m m 2 h i <i m 2 h i k i−1
i i
wherex isthenearestEOTtokenoccurringbeforex . GiventhelargesizeoftheOpenWebText
k i
andAmberdatasets,containing9billionsand1.2trilliontokensrespectively,weusesubsampling
for the evaluation of the empirical risk. More details can be found in Appendix A.2.
6
ycneuqerF yportnE
)%(
ccA
/
LPP
)DPB(
evitcejbO
dnuoB4.3 Token-level Bounds Are Predictive of Generalization
Token-level vs. document-level bounds. In contrast to document-level bounds, token-level
bounds increase the number of samples, driving down the size of the complexity term, and
do not require the IID assumption. Whereas the number of samples previously would be the
number of documents, it is now simply the number of tokens in the dataset, a far higher
number. As a consequence of decreasing the complexity term, the empirical risk will be a more
significantcontributortoourboundscomparedtodocument-levelbounds. Therefore,weachieve
non-vacuous bounds for much larger and more performant models that generate high-quality
text. This development brings our theoretical bounds much closer to aligning with empirical
generalization.
Interpretation of token-level bounds. It is important to note the difference between the
quantity that we bound 1 (cid:80)m E[−log p (X |x )|x ], which is conditioned on contexts
m i=1 2 h i <i <i
seen at training, and the expected risk E[−log p (X |x )] under resampling from the data
2 h i <i
generating process where new contexts can be sampled from this process. However, the
resampled next tokens x |x are not necessarily from the training set, and to the extent that
i <i
the distribution over next tokens is entropic, we are measuring a different quantity than the
empirical training performance of the hypothesis h. Moreover, we know that the distribution
over next tokens is often indeed diffuse; for instance, many words have common synonyms. The
distributionovernexttokensisespeciallydiffusewhenwestartanewsentence, forexample. We
demonstrate how diffuse the distribution p(x |x ) is for fixed contexts x from the publicly
i <i <i
available Amber training dataset [29] (see Appendix B.7) by sampling x |x using LLaMA2-7B
i <i
to approximate the generative process. Figure 2(Left) shows that, indeed, the distribution
p(x |x ) is characterized by a high entropy for a large number of tokens. In Figure 2(Middle),
i <i
we plot the entropy of p(x |x ) for each index i in a context of length 1024. This figure
i <i
confirms our intuition that the next token distribution is particularly diffuse at the beginning of
a sentence, while it decreases for later tokens but remains relatively high. Given how diffuse
the distribution is and the large number of possible sentences, it is broadly infeasible to make
predictions on new resampled tokens from the empirical distribution alone.
Our bounds are predictive of downstream performance. We compute an approximation
of the quantity that we bound in Equation (2) by sampling next tokens x using LLaMA2-7B
i
given fixed contexts x from the Amber dataset. We plot this quantity on the right y-axis of
<i
Figure 2(Right), and show on the left y-axis the performance of GPT2 models of varying sizes
on downstream datasets as reported in Radford et al. [39]; see Appendix B.4 for more details.
Not only does the approximation of the BPD objective show the same trend as the downstream
performance for different GPT2 variants, but it also achieves 97.9% and 99.1% correlation [4]
with downstream task accuracy and perplexity metrics, respectively.
In short, our bounds go significantly beyond the observation that the empirical distribution
converges to the true distribution, and are predictive of generalization on downstream tasks.
Achieving a non-vacuous token-level bound requires generalization. We provide further interpre-
tation of the bounds, including a protein application, in Section 6.
4.4 Token-Level Prediction Smoothing
Rather than using a single label smoothing α for all data points, we propose to use the
network itself to determine which tokens warrant more confidence and which ones require more
smoothing to limit their worst-case behavior. We perform token-level prediction smoothing
by adding a linear head to the LLM that outputs the probability α for each token, such that
p (x |x )=(cid:0) 1−α (x )(cid:1) p (x |x )+α (x )/V. The training objective corresponds to the
h i <i θ <i θ i <i θ <i
upperboundinEquation(2)ratherthantheempiricalriskalone,wheretheαparameterfactors
into the bound via the interval size ∆ =log (cid:0) 1+(1−α (x ))V/α (x )(cid:1). Therefore, the
i 2 θ <i θ <i
values of α (x ) are adjusted to achieve the best trade-off between the empirical risk and
θ <i
72000
10.5 8.8 Grid Search Token-Dependent
1500
9.5 8.4 1000
Before Optimization 500
After Optimization
8.5 106 107 8.0 104 103 102 101 0
0.05 0.10 0.15
Trainable Parameters Prediction smoothing ( ) Prediction smoothing ( )
Figure 3: Token-level prediction smoothing improves our bounds. Left: After training,
we optimize a conservative upper bound on the generalization bound that we would get from
Equation (2) with respect to the α head parameters. Doing so yields a noticeable reduction
in the value of the bound. Middle: BPD generalization bound as a function of a single
global parameter chosen from a discrete number of values vs. the generalization bound for the
token-dependent α after optimization. Right: Histogram of the values taken by α(x ) over
<i
different inputs.
the compressed model size. We perform this optimization post-training using a subset of the
training dataset.
We demonstrate in Figure 3(Left) that using this token-dependent α significantly improves the
value of the bounds. In Figure 3 (Middle), we compare to the setting where the optimal α
is obtained through a grid search, and in Figure 3(Right) we examine the distribution of α
produced by the model.
5 Compressing LLMs to Minimize Complexity
In shifting from document-level to token-level bounds, the number of data points m increases
considerably, and thus we can afford to pay significantly more bits in the complexity of the
compressed model. In this new regime, the SubLoRA compression technique, which consists of
training only a linear subspace of LoRA-parameterized weights in the attention layers, becomes
very restrictive. Therefore, we explore other less restrictive forms of compression.
5.1 Efficient Nonlinear Parametrizations
In addition to LoRA, we explore two expressive nonlinear parametrizations f(θ) that make
efficient use of the parameter space: Kronecker structures [15] and Monarch matrices [9]. We
can use these nonlinear parametrizations directly, or in conjunction with subspace compression,
parametrizing the full parameters as θ =θ +f(Pw) for a projection matrix P ∈RD×d. After
0
training, the parameters are quantized and coded using arithmetic coding. We describe these
structures below.
LoRA. With LoRA [19], the weight matrices of linear layers are parametrized via low rank
updates. EachweightmatrixW ∈Ra×b isparametrizedW =W +AB forA∈Ra×r,B ∈Rr×b
0
with a small rank r, where W is given by the initialization and A, B form the trainable
0
parameters in each layer. Rather than considering only self-attention layer weights [19, 32], we
extendSubLoRAtoalllinearlayersinthemodelandcompressthebiasesandlayernormweights
in the subspace projection. We define f(θ)=LoRA(θ) as the transformation that unpacks θ
into A and B, multiplies the low rank matrices and adding the initialization to form W and
reshaping them into the parameters in the usual way.
Kronecker Product. We can represent W as a Kronecker product W = A⊗B, where ⊗
is the Kronecker product, A∈Ra1×b1,B ∈Ra2×b2 and a 1a
2
=a, b 1b
2
=b, which reduces the
8
ssoL
dnuoB
dnuoB
DPB
noitubirtsiD
laciripmEparameters over the dense layer. This approach has been used in recent work for parameter-
efficient finetuning [15] and as an alternative structure for pretraining. Similarly to LoRA, we
define f(θ)=Kron(θ) as this parametrization for all linear layers.
Monarch Matrices. We also consider Monarch matrices [9], which employ two block diagonal
√ √ √
matricesA,andB typicallywithAandB formedby ablocksofsize a× bandareshapeor
permutation operation R: W =ARB. The matrix multiplication is implemented by reshaping
√ √
the input axis a into ( a, a), applying matrix A as a batched matrix multiply on one axis,
and then applying B to the other axis by permuting the axes. Monarch matrices have shown
considerable promise as an expressive and hardware-efficient replacement for linear layers. We
define f(θ)=Monarch(θ) for their application to each of the linear layers in the network.
In our experiments, we apply all three compression techniques, with and without subspace
compression, to all linear layers in the models.
5.2 QuIP 2-Bit Quantization of LLM
In addition to pretraining LLMs in efficient nonlinear subspaces, we explore recent post-training
quantization methods to reduce the model complexity. Quantization with Incoherence Process
(QuIP)compressesLLMweightstoasmallernumberofbitswhilepreservingmodelperformance
[6].
Adaptive Rounding. For a weight matrix W ∈Ra×b, QuIP minimizes the proxy quadratic
objective ℓ(Wˆ) = E[∥(Wˆ −W)x∥2] = tr((Wˆ −W)H(Wˆ −W)⊤), where Wˆ ∈ Ra×b are the
quantized weights, x ∈ Rb is a vector drawn randomly from a calibration set, and H is the
second moment matrix of these vectors used as a proxy Hessian [6].
Incoherence Processing. Based on the observation that incoherences between the weights W
and the proxy Hessian H benefit quantization, QuIP further applies incoherence post-processing
using Kronecker products of random orthogonal matrices U ∈ Ra×a,V ∈ Rb×b such that
H˜ ←VHV⊤,W˜ ←UWV⊤. Here U =U ⊗···⊗U and V =V ⊗···⊗V .
1 k 1 k
Subsequent work like QuIP# improves upon QuIP by using randomized Hadamard transform
and vector quantizations [48]. To compute the compressed size C(h) of QuIP-quantized models,
we use gzip [12] to compress the quantized model checkpoint and obtain the term C(h) as the
bits required for the storage afterwards.
6 Non-Vacuous Bounds for LLMs with Billions of Parame-
ters
As described in the previous section, we compute generalization bounds for: (i) models that are
trained through non-linear subspace compression in the form of LoRA, Kronecker product or
Monarch matrices on the OpenWebText dataset, then quantized using the same setup as Lotfi
et al. [32], or (ii) models that are pretrained on a dataset other than the OpenWebText dataset
– or on datasets that might have the OpenWebText as a subset– and made publicly available.
For the pretrained models, we either apply aggressive quantization, which is the case for GPT2,
or use QuIP 2-bit, 3-bit and 4-bit publicly-available quantized models, which is the case for
LLaMA. In the pretrained LLMs setting, we evaluate our bounds for both the OpenWebText
(9B tokens) and Amber (1.2T tokens) datasets. In both settings, we obtain highly compressed
models that lead to non-vacuous generalization bounds.
In addition to reporting generalization bounds on BPD, we also report the bounds that we
obtain on the Top-1, Top-10 and Top-100 error. The Top-k error refers to the 0-1 error in
predicting the next token among the top-k predictions of the model. For instance, the Top-1
error for token x is defined as 1[argmax p(x |x =x )=x ], where argmax operates over
i xj j <i <i i
tokens x across the vocabulary. We extend this definition to the Top-k error and define it
j
9Compression Approach BPD Bound Top-1 Error Top-10 Error Top-100 Error
SubLoRA [32] 10.49 90.44 71.33 49.77
Enhanced SubLoRA (Ours) 10.44 89.38 69.54 49.84
Enhanced LoRA (Ours) 7.85 78.15 52.48 31.64
Monarch Only (Ours) 7.65 75.87 47.47 28.34
Kronecker Only (Ours) 8.03 80.80 52.77 30.14
Kronecker + Subspace (Ours) 10.02 88.75 67.91 47.14
Random Guess 15.62 99.99 99.98 99.80
Table 1: Non-vacuous generalization bounds using different compression techniques
for GPT2 pretraining. We find that with the larger complexity budget afforded by the
token-levelbounds,subspacecompressionisnolongernecessaryorevenbeneficialforthebounds.
Of the structures we consider, the Monarch parametrization performs best.
as 1[x ∈argmax p(x |x =x )], where the argmax operator here selects the top-k tokens
i xj,k j <i <i
predicted by the model according to its next token probability distribution p(x |x = x ).
j <i <i
Our bound in Equation (2) applies not only to the log likelihood but to any bounded risk, and
therefore can be computed for the Top-k error since it is bounded between 0 and 1. We call a
Top-k error bound vacuous when the bound is larger than the random guess top-k error equal
to 1−k/V, where V is the vocabulary size.
We present our bounds in this section and contrast the quality of the text generated by our
best performing model in terms of the BPD bound to the text generated by Lotfi et al. [32]’s
best performing model. We also compute token-level generalization bounds for antibody design,
a task where conditioning on contexts from the training dataset arises naturally. Finally, we
investigate the effect of aggressive compression on memorization vs. reasoning in LLMs. We
provide all the experimental details in Appendix B.
6.1 Token-level Bounds via Nonlinear Parametrizations
As discussed in Section 5.1, we experiment with LoRA in addition to the Kronecker and
Monarch subspace parametrizations in order to train compressed versions of GPT2 small (124M
parameters). Compared to previous work, we enhance both LoRA and SubLoRA by not only
applying the low-rank decomposition to the attention layers and the linear head, but to all the
fully-connected layers in the LLM. Additionally, we train all the bias and layer normalization
parameters instead of keeping them fixed at their values at initialization. We also use rotary
position embeddings [46] to directly encode the positional information into the LLM. Combined
with our proposed token-level optimization of the label smoothing probability α, we significantly
improve upon the LoRA subspace compression, as shown in Table 1. It is worth noting the
LoRA alone led to vacuous BPD document-level bounds in Lotfi et al. [32] while our version is
non-vacuous.
AmongallsubspacecompressionstrategiesthatweexploreinTable1,Monarchwithoutsubspace
leadstothetightesttoken-levelbound. Infact,thesubstantialscaleofourdataset,comprising9
billion tokens, significantly changes the trade-off between the empirical risk and the compressed
model size compared to previous work, since the compressed size factor in the bound is divided
by the size of the dataset. Consequently, we have greater flexibility in selecting larger models
that achieve an improved empirical risk. In this setting, the Monarch parametrization achieves
the best trade-off between the empirical risk and the compressed size of the model as shown in
Table 1, followed by LoRA and Kronecker. Monarch and Kronecker also perform best in terms
of the validation loss, as shown in Figure 1(Right). The new trade-off between the empirical
risk and the compressed size of the model also explains why subspace compression is no longer
beneficial in obtaining tighter bounds compared to previous work, as further reducing the
10Model BPD Top-1 Error (%) Top-100 Error (%)
GPT2 (124M) 7.61 74.82 26.98
GPT2 (355M) 8.50 79.19 32.72
GPT2 (774M) 10.47 89.50 44.23
Random Guess 15.62 99.99 99.80
Table 2: Non-vacuous token-level generalization bounds for open-source pretrained
GPT2 models. PretrainedGPT2modelsachievenon-vacuousboundsfornexttokenprediction
on OpenWebText through post-training quantization only and without altering the pretraining.
Model BPD Top-1 Error (%) Top-100 Error (%)
LLaMA2-7B 4.28 47.50 12.56
LLaMA2-13B 4.51 47.85 14.44
LLaMA2-70B 6.39 58.26 25.04
Random Guess 14.97 99.99 99.68
Table 3: Non-vacuous token-level generalization bounds for open-source pretrained
LLaMA2 models. Pretrained LLaMA2 models achieve non-vacuous token-level bounds for
next token prediction on the Amber dataset via 2-bit post-training QuIP quantization only.
number of trainable parameters through linear subspace projection leads to a worse trade-off
between the empirical performance of the compressed model and its compressed size.
6.2 Non-vacuous Bounds for Pretrained LLMs: GPT2, LLaMA1 and
LLaMA2
Intensive quantization is another way we can achieve model compression, and therefore tighter
generalization bounds. We explore the setting where we only apply post-training quantization
to pretrained LLMs and compute the corresponding token-level generalization bounds.
Pretrained GPT2 models. We apply the post-training quantization [31] to the publicly
available GPT2 models [39] of sizes 124M (GPT2 small), 354M (GPT2 medium), and 773M
(GPT2 large) parameters that were pretrained on the WebText dataset and report the numbers
in Table 2. We find that GPT2 small not only yields non-vacuous bounds, but these bounds are
quite comparable to those obtained using aggressive compression techniques in Table 1. GPT2
medium and large also achieve non-vacuous bounds despite having almost a billion parameters.
Pretrained LLaMA models. In this set of experiments, we use pretrained and pre-quantized
publicly available LLaMA1, LLaMA2 and LLaMA2-Chat models and plug in their empirical
risk and compressed size directly into our token-level bounds. We report the bounds obtained
for 2-bit LLaMA2 in Table 3. The full set of results is reported in Table 8. The bounds
are computed for the next token prediction task on the Amber dataset, which contains 1.2T
tokens. We obtain non-vacuous bounds for these models despite their large scale, ranging from 7
billion to 70 billions parameters. Our experiments show that the LLaMA2-Chat models achieve
worse generalization bounds as reported in Table 8 and Figure 1(Left), demonstrating that
fine-tuning Chat models for dialogue use cases hurts their generalization performance on next
token prediction. Although we do not know what data was used to pretrain the LLaMA models,
our bounds remain valid since they do not require for the models to be trained on the same
data that the empirical risk is evaluated on.
High-quality text generation. Oneofthemajorbenefitstoachievingnon-vacuousgeneraliza-
tion bounds for the original model without aggressive subspace training is to be able to generate
high-quality text with the model that achieves the best bounds. In fact, a significant limitation
of document-level bounds is that the SubLoRA model achieving the best document-level bound
generates un-grammatical, low-quality text as demonstrated by Lotfi et al. [32] and shown in
11Compression Approach Bits Per Dimension Top-1 Error Top-10 Error Validation Loss
Mistral 377M 2.41 31.60 26.46 0.28
Mistral 212M 2.06 26.25 21.07 0.30
Mistral 94M 1.62 19.40 14.59 0.30
Random Guess 4.86 96.56 65.51 1.46
Table 4: Models pretrained on antibody sequences achieve non-vacuous token-level
generalization bounds. Language models based on the Mistral 7B architecture with scaled-
down sizes pretrained on antibody sequences achieves non-vacuous bounds for next token
prediction on a processed subset of Observed Antibody Sequences (OAS) through post-training
quantization only. The vocabulary size of an antibody LLM is 29.
Training Context Length 0 1 2 4 1024
GPT2-S-Quantized 13.9 11.1 9.0 7.9 7.6
Markov Chain 11.3 10.5 15.3 22.4 -
Table 5: GPT2 models achieve tighter bounds than Markov chains for large context
lengths. We evaluate the generalization bound BPD achieved when k =0,1,2,4,1024 previous
tokens are made visible to a GPT2 LLM during training when predicting the next token, and
we compare to a our bounds evaluated on a sparse k-th order Markov chain trained on the same
data; with 0th order being just the empirical token probabilities. Our LLM bounds provide a
much stronger statement than what would be explained by low order Markov models.
Table9. Moreover,document-levelboundsfortheoriginalmodelorformodelswithasufficiently
high number of trainable parameters to generate high-quality text are vacuous, as shown in
Table 1 and Figure 1 of Lotfi et al. [32]. In contrast, our top-performing model in terms of
token-level BPD bounds on the OpenWebText dataset, which is the quantized GPT2 small
model, generates high-quality text, ensuring a unique combination of practical usefulness and
tight guarantees on the population risk.
6.3 Token-Level Generalization Bounds on Antibody Sequences
Inadditiontonaturallanguages,ourtoken-levelgeneralizationboundsareparticularlydescriptive
of antibody design in biology. An antibody sequence is usually composed of 20 different amino
acid tokens to bind to a target of interest. In therapeutic antibody design, biologists propose
mutationstoexistingantibodysequencesbychangingtheaminoacidtokensatspecificpositions
in the sequence. Recent works have shown that LLMs pretrained on large antibody datasets
can be used to propose mutations conditioned on starting antibody sequences [44, 2]. Our
token-level generalization bounds match the settings by bounding the expected next amino acid
token negative log likelihood averaged over training contexts that serve as starting sequences
for iterative mutations. In Table 4, we show that language models based on the Mistral 7B
architecture pretrained on a processed subset of the Observed Antibody Sequences (OAS) from
scratch achieves non-vacuous token-level generalization bounds [20, 35, 2]. Details of these
experiments can be found in Appendix B.9
6.4 Contextualizing GPT2 Bounds Against Markov Chains
The best token-level bound that we achieve for BPD on the OpenWebText dataset is 7.6. But
what does this value exactly mean? One might consider the possibility that our bounds are
describing only the simplest components of fitting the data that exist in the model, such as the
predictions of a 0th or 1st order Markov chain [34].
12In Table 5 we show that this is not the case, by explicitly training a sparse k-th order Markov
chain on OpenWebText and computing our token-level bounds for the result. Sweeping over
different numbers of n-grams to use for the Markov chains, our bounds for these models cap
out at 10.5 BPD and rapidly degrade with higher order as more statistics need to be stored.
We also train and compress versions of GPT2 that are restricted to only seeing k tokens as
context, mirroring the restrictions of the Markov chains. We find that for the simple 0 and 1st
order Markov chains, our compression via the transformer is slightly worse. However, the LLM
performs much better for higher orders.
6.5 Memorization vs. Reasoning
Large language models are capable of memorizing facts from their pretraining data, but they
also can learn highly structured patterns. As we compress a model more and more, it must
lose its ability to recall memorized facts, but it may still remember patterns, since they are
compressible. In this section, we examine the difference between memorization and reasoning
by measuring the ability of LLMs to compress structured and unstructured sequence data.
To generate structured sequences, we first use
short binary expression trees to generate nu-
merical sequences of integers [17]. These se-
quences are highly compressible as they are 0.8
generated using short and deterministic pro-
0.6
grams. To generate unstructured sequences,
we collect the set of all unique integers from
0.4
the structured sequences and form random
sequences composed of IID samples from the 0.2 Structured Sequences
set of unique integers (see Appendix B.6 for Random Sequences
details). We train standard GPT2 models 0.0
from scratch on structured and random se- 25 50 75 100
Quantization Levels
quences separately. In Figure 4, we show the
integerpredictiontrainingaccuracywithvary-
Figure 4: As language models are com-
ing degrees of post-training quantization. We
pressed, they retain their understanding
observe that as models are quantized more
of patterns, but they forget highly ran-
aggressively, i.e. the number of quantization
dom and unstructured data rapidly. Exper-
levels decreases, they forget unstructured se-
imentsperformedonGPT2modelswithdatasets
quences far faster than structured sequences.
created as detailed in Section 6.5. Compression
These results parallel the findings of Jin et al. performed via post-training quantization where
[21] who show that smaller models can retain lower quantization levels reflect more aggressive
in-context learning capabilities but lose their compression.
ability to recall facts.
7 Conclusion
In this work, we introduced novel token-level generalization bounds for LLMs which are able
to accommodate the non-IID nature of the tokens within the training corpus. Combined with
different compression techniques, we achieve non-vacuous generalization bounds for LLMs with
up to 70 billion parameters. The compressed models for which we construct our bounds are
capable of producing high quality text, unlike those in prior work. While there is still have a
gap to close between the typical validation BPD and the constraint of our bounds, our bounds
are predictive of generalization and provide insights into model behaviour.
In future work, one could envision constructing new bounds that make use of the independence
structure between documents and then the non-independent structure within documents to
achieve the best of both. It would also be exciting to further explore the development of these
13
)%(
ycaruccAbounds for new downstream predictive tasks, in the vein of the antibody design task we briefly
consider here.
Acknowledgements
We thank Alan Amin for helpful discussions and anonymous reviewers for helpful feedback.
This work is supported by NSF CAREER IIS-2145492, NSF CDS&E-MSS 2134216, NSF
HDR-2118310, BigHat Biosciences, Capital One, and an Amazon Research Award.
References
[1] V. Akinwande, Y. Jiang, D. Sam, and J. Z. Kolter. Understanding prompt engineering may
not require rethinking generalization. arXiv preprint arXiv:2310.03957, 2023.
[2] Anonymous. Bayesianoptimizationofantibodiesinformedbyagenerativemodelofevolving
sequences. Manuscript, 2024.
[3] K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical
Journal, Second Series, 19(3):357–367, 1967.
[4] J. Benesty, J. Chen, Y. Huang, and I. Cohen. Pearson correlation coefficient. In Noise
reduction in speech processing, pages 37–40. Springer, 2009.
[5] O.Catoni.Pac-bayesiansupervisedclassification: thethermodynamicsofstatisticallearning.
arXiv preprint arXiv:0712.0248, 2007.
[6] J. Chee, Y. Cai, V. Kuleshov, and C. D. Sa. Quip: 2-bit quantization of large language
models with guarantees, 2024.
[7] B.Chugg,H.Wang,andA.Ramdas. Aunifiedrecipeforderiving(time-uniform)pac-bayes
bounds. Journal of Machine Learning Research, 24(372):1–61, 2023.
[8] T. Computer. Redpajama: an open dataset for training large language models, 2023. URL
https://github.com/togethercomputer/RedPajama-Data.
[9] T. Dao, B. Chen, N. S. Sohoni, A. Desai, M. Poli, J. Grogan, A. Liu, A. Rao, A. Rudra,
and C. Ré. Monarch: Expressive structured matrices for efficient and accurate training. In
International Conference on Machine Learning, pages 4690–4721. PMLR, 2022.
[10] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise
quantization, 2022.
[11] T. Dettmers, S. Shmitchell, A. Roberts, K. Lee, T. B. Brown, D. Song, and C. Raffel.
Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.
[12] P. Deutsch. Rfc1952: Gzip file format specification version 4.3, 1996.
[13] G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep
(stochastic)neuralnetworkswithmanymoreparametersthantrainingdata. arXiv preprint
arXiv:1703.11008, 2017.
[14] G. K. Dziugaite, K. Hsu, W. Gharbieh, G. Arpino, and D. Roy. On the role of data in
pac-bayes bounds. In International Conference on Artificial Intelligence and Statistics,
pages 604–612. PMLR, 2021.
[15] A. Edalati, M. Tahaei, I. Kobyzev, V. P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona:
Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022.
[16] Z. Frantal, A. Gruslys, and D. Kiela. Gptq: Accurate post-training quantization for
generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.
14[17] M. Goldblum, M. Finzi, K. Rowan, and A. G. Wilson. The no free lunch theorem,
kolmogorov complexity, and the role of inductive biases in machine learning. arXiv preprint
arXiv:2304.05366, 2023.
[18] S. Hayou, B. He, and G. K. Dziugaite. Probabilistic fine-tuning of pruning masks and
pac-bayes self-bounded learning. arXiv preprint arXiv:2110.11804, 2021.
[19] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:
Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas,
F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint
arXiv:2310.06825, 2023.
[21] T. Jin, N. Clement, X. Dong, V. Nagarajan, M. Carbin, J. Ragan-Kelley, and G. K.
Dziugaite. The cost of down-scaling language models: Fact recall deteriorates before
in-context learning. arXiv preprint arXiv:2310.04680, 2023.
[22] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, and D. Lee. Memory-efficient
fine-tuning of compressed large language models via sub-4-bit integer quantization. arXiv
preprint arXiv:2305.14152, 2023.
[23] A. N. Kolmogorov. On tables of random numbers. Sankhy¯a: The Indian Journal of
Statistics, Series A, pages 369–376, 1963.
[24] V. Kuznetsov and M. Mohri. Generalization bounds for non-stationary mixing processes.
Machine Learning, 106(1):93–117, 2017.
[25] G. G. Langdon. An introduction to arithmetic coding. IBM Journal of Research and
Development, 28(2):135–149, 1984.
[26] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki,
J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj,
J. Lamy-Poirier, J. Monteiro, O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee,
L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. Murthy, J. Stillerman, S. S.
Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu,
S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor,
J.Ding,C.Schlesinger,H.Schoelkopf,J.Ebert,T.Dao,M.Mishra,A.Gu,J.Robinson,C.J.
Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite,
C. M. Ferrandis, S. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries. Starcoder:
may the source be with you!, 2023.
[27] Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Gen-
eralization and stability in in-context learning. In International Conference on Machine
Learning, pages 19565–19594. PMLR, 2023.
[28] Y. Liu, Q. Xu, W. Xu, and J. Zhu. Llm-qat: Data-free quantization aware training for
large language models. arXiv preprint arXiv:2305.17888, 2023.
[29] Z. Liu, A. Qiao, W. Neiswanger, H. Wang, B. Tan, T. Tao, J. Li, Y. Wang, S. Sun,
O. Pangarkar, R. Fan, Y. Gu, V. Miller, Y. Zhuang, G. He, H. Li, F. Koto, L. Tang,
N. Ranjan, Z. Shen, X. Ren, R. Iriondo, C. Mu, Z. Hu, M. Schulze, P. Nakov, T. Baldwin,
and E. P. Xing. Llm360: Towards fully transparent open-source llms, 2023.
[30] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
[31] S. Lotfi, M. Finzi, S. Kapoor, A. Potapczynski, M. Goldblum, and A. G. Wilson. Pac-bayes
compression bounds so tight that they can explain generalization. Advances in Neural
Information Processing Systems, 35:31459–31473, 2022.
15[32] S. Lotfi, M. Finzi, Y. Kuang, T. G. Rudner, M. Goldblum, and A. G. Wilson. Non-vacuous
generalization bounds for large language models. arXiv preprint arXiv:2312.17173, 2023.
[33] M.MohriandA.Rostamizadeh. Stabilityboundsfornon-iidprocesses. Advances in Neural
Information Processing Systems, 20, 2007.
[34] J. R. Norris. Markov chains. Number 2. Cambridge university press, 1998.
[35] T. H. Olsen, F. Boyles, and C. M. Deane. Observed antibody space: A diverse database of
cleaned, annotated, and translated unpaired and paired antibody sequences. Protein Sci.,
31(1):141–146, Jan. 2022.
[36] G. Park, J. Kim, J. Kim, E. Choi, S. Kim, S. Kim, M. Lee, H. Shin, and J. Lee. Lut-gemm:
Quantizedmatrixmultiplicationbasedonlutsforefficientinferenceinlarge-scalegenerative
language model. arXiv preprint arXiv:2206.09557, 2022.
[37] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier,
E. Almazrouei, and J. Launay. The refinedweb dataset for falcon llm: Outperforming
curated corpora with web data, and web data only, 2023.
[38] M. Pérez-Ortiz, O. Rivasplata, J. Shawe-Taylor, and C. Szepesvári. Tighter risk certificates
for neural networks. Journal of Machine Learning Research, 22(227):1–40, 2021.
[39] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[40] A. Rakhlin and K. Sridharan. On equivalence of martingale tail bounds and deterministic
regret inequalities. In Conference on Learning Theory, pages 1704–1722. PMLR, 2017.
[41] L. Ralaivola, M. Szafranski, and G. Stempfel. Chromatic pac-bayes bounds for non-iid
data: Applications to ranking and stationary β-mixing processes. The Journal of Machine
Learning Research, 11:1927–1956, 2010.
[42] C. RelaxML. Quip#: Quip with lattice codebooks. https://github.com/Cornell-
RelaxML/quip-sharp, 2024.
[43] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
[44] R. W. Shuai, J. A. Ruffolo, and J. J. Gray. Generative language modeling for antibody
design. bioRxiv, pages 2021–12, 2021.
[45] R. J. Solomonoff. A formal theory of inductive inference. part i. Information and control, 7
(1):1–22, 1964.
[46] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer
with rotary position embedding. Neurocomputing, 568:127063, 2024.
[47] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,
P.Bhargava,S.Bhosale,D.Bikel,L.Blecher,C.C.Ferrer,M.Chen,G.Cucurull,D.Esiobu,
J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn,
S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev,
P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet,
T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta,
K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang,
R.Taylor,A.Williams,J.X.Kuan,P.Xu,Z.Yan,I.Zarov,Y.Zhang,A.Fan,M.Kambadur,
S.Narang,A.Rodriguez,R.Stojnic,S.Edunov,andT.Scialom. Llama2: Openfoundation
and fine-tuned chat models, 2023.
[48] A.Tseng,J.Chee,Q.Sun,V.Kuleshov,andC.DeSa. Quip#: Evenbetterllmquantization
with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024.
16[49] K.Wang,X.Hu,andJ.Zhang. Fastclonalfamilyinferencefromlarge-scaleBcellrepertoire
sequencing data. Cell Rep Methods, 3(10):100601, Oct. 2023.
[50] Q. Xu, W. Xu, and J. Zhu. Tensorgpt: Efficient compression of the embedding layer in
llms based on the tensor-train decomposition. arXiv preprint arXiv:2307.00526, 2023.
[51] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning
(still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.
[52] R.-R.ZhangandM.-R.Amini. Generalizationboundsforlearningundergraph-dependence:
A survey. arXiv preprint arXiv:2203.13534, 2022.
[53] W. Zhou, V. Veitch, M. Austern, R. P. Adams, and P. Orbanz. Non-vacuous generalization
bounds at the imagenet scale: a pac-bayesian compression approach. In International
Conference on Learning Representations, 2019.
17A Token-Level Martingale Bound
A.1 Proof of the Main Theorem
Theorem 2. With probability at least 1 − δ over the randomness in a sampled sequence
x ,x ,...,x , if the negative log likelihood of a model h∈H can be bounded −log p (·|x )∈
1 2 m 2 h <i
[a,a+∆ ] for some ∆ (possibly a function of h), then the negative log likelihood of the data of
i i
a given hypothesis h satisfies
m (cid:114)
1 (cid:88) E[−log p (X |x )|x ]≤− 1 log p (x )+∆ˆ log1/P(h)+log1/δ , (4)
m 2 h i <i <i m 2 h ≤m 2m
i=1
(cid:113)
where ∆ˆ = 1 (cid:80)m ∆2, the expectation is taken over X ∼p(X |x ) from the data generating
m i=1 i i i <i
process, and P(h) is any normalized prior over a discrete hypothesis space H that does not
depend on {x }m .
i i=1
Proof sketch. The proof of Theorem 1 is an application of Azuma’s inequality [3] and can be
broken down into the following steps:
• Construct a martingale difference sequence from the difference between the NLL on token
x , and its expectation given the tokens x . From the boundedness of NLL one can show
i <i
that the differences are bounded.
• Apply Azuma’s inequality for each hypothesis, choosing failure probability proportional to
the chosen prior P(h).
• Perform a union bound of the failure probabilities over all hypotheses. If all of the
hypotheses satisfy the bound simultaneously, then so does the data dependent hypothesis
h∗.
Proof. Given the autoregressive predictions R(h,x ,x ) := −log p (x |x ) where x :=
i <i 2 h i <i <i
{x ,x ,...,x }. Let{x }denotetheactualvaluesofthesequencethatwerefoundempirically,
1 2 i−1 i
and {X } be the random variables for these quantities.
i
The collection of random variables (indexed by i) Z = E[R(h,X ,x )|x ]−R(h,X ,x )
i i <i <i i <i
form a Martingale difference sequence with respect to x . Note here that the expectation is
<i
over the distribution X ∼p(X |x ). From the construction, E[Z |x ]=0 and the sequence
i i <i i <i
is bounded: A = E[R(h,X ,x )|x ]−a ≤ Z ≤ ∆ +E[R(h,X ,x )|x ]−a = B , with
i i <i <i i i i <i <i i
B −A =∆ .
i i i
∆ may depend on x but only through it’s dependence on the hypothesis h({x}m ). For a
i ≥i i=1
fixed h we may conclude that (cid:80)m Z is bounded difference Martingale sequence (with respect
i=1 i
to {x }m ), and we can apply Azuma’s inequality [3] to derive that for any t>0:
<i i=1
m m
P(cid:0)(cid:88)
Z
>mt(cid:1) ≤exp(cid:0) −2m2t2/(cid:88) ∆2(cid:1)
i i
i=1 i=1
m
P(cid:0) 1 (cid:88)
Z
>t(cid:1) ≤exp(cid:0) −2mt2/∆ˆ2(cid:1)
.
m i
i=1
Judiciously choosing
(cid:114)
log1/P(h)+log1/δ
t(h)=∆ˆ ,
2m
18we have that P(cid:0)1 (cid:80)m Z >t(h)(cid:1) =P(h)δ.
m i=1 i
Applying a union over the events (cid:83) (cid:2)1 (cid:80)m Z (h)>t(h)(cid:3), we have
h∈H m i=1 i
m
(cid:0) 1 (cid:88) (cid:1) (cid:88)
P Z >t(h) ≤ P(h)δ =δ,
m i
i=1 h
therefore P(cid:0)1 (cid:80)m Z ≤ t(h)(cid:1) > 1−δ. Unpacking the definition of Z , we have that with
m i=1 i i
probability at least 1−δ
m m (cid:114)
1 (cid:88) E[R(h,X ,x )|x ]≤ 1 (cid:88) R(h,x ,x )+∆ˆ log1/P(h)+log1/δ .
m i <i <i m i <i 2m
i=1 i=1
Expressed in terms of the log likelihood, we can write this as:
m (cid:114)
1 (cid:88) E[−log p (X |x )|x ]≤− 1 log p (x )+∆ˆ log1/P(h)+log1/δ
m 2 h i <i <i m 2 h ≤m 2m
i=1
A.2 Empirical Risk Subsampling
We evaluate our bounds for the OpenWebtext and Amber datasets which contain 9 billion and
1.2 trillion tokens, respectively. Computing the exact empirical risk for these datasets would be
prohibitively expensive. Therefore, we use subsampling for the evaluation of the empirical risk
to accelerate bound computation. In Equation (2), we use the following inequality which holds
with probability at least 1−δ :
2
n (cid:114)
− 1 log p (x )≤−1 (cid:88) log p (x |x )+∆ˆ log1/δ 2 (5)
m 2 h ≤m n 2 h σ(j) <σ(j) 2n
j=1
for a subsample of size n where σ is a random permutation. We choose δ in Equation (2) with
1
respecttoanewoverallfailureprobabilityδ tobeδ =δn/(n+m)andchooseδ =δm/(n+m)
1 2
so that the overall failure probability is still δ. The proof is simple and similar to that provided
in Lotfi et al. [32].
B Experimental Details
B.1 Pretraining with Nonlinear Parametrizations
Toachievethenecessarymodelcompressionlevelforcomputingnon-vacuousbounds,wepretrain
GPT2 Small with 124 million parameters on the OpenWebText1 dataset based on the nanoGPT
implementation2 [39]. We parametrize the linear layers of CausalSelfAttention, MLP, and the
LinearHead of the GPT2 models with our nonlinear compression techniques (LoRA, Kronecker,
Monarch),whereweuseabiasvectorexceptfortheLinearHeadlayer. ForLoRAandKronecker,
weuseweighttyingbetweenthetokenembeddingandthefinalLinearHeadlayerparameterized
by nonlinear compression techniques. We also train the layer norm parameters in addition to
all of the nonlinear projection parameters applied to the linear layers. For Monarch, we only
train the linear layers parameterized by Monarch matrices. We also combine the three nonlinear
parametrizations with linear subspace projection, where all the trainable parameters θ are
projected into a subspace of parameters w using a projection matrix P, such that θ =θ +Pw.
0
We vary the dimension of w as a hyperparameter in the bound evaluation.
1http://Skylion007.github.io/OpenWebTextCorpus
2https://github.com/karpathy/nanoGPT
19For all the pretraining experiments, we use a batch size of 8, a sequence length of 1024, and
a standard AdamW optimizer [30] with a learning rate of 0.0002. We perform a learning
rate warm-up for 500 iterations, and we apply rotary embedding [46] to all three nonlinear
parametrizations.
B.1.1 Hyperparameter Sweeps for LoRA
LoRA. We sweep over LoRA rank values r ∈{1,4,16,32,64,128,256}. We choose a learning
rate of 0.0002 with a LoRA dropout value of 0.1 and LoRA alpha value of 32.
SubLoRA. We report the rank r and the corresponding subspace dimension values that we
sweep over for SubLoRA in Table 6.
Rank r Subspace Dimension d
1 25000
4 50000
8 50000
16 50000
32 10000, 750000
64 25000, 2000000
128 7000000, 15000000
Table 6: Hyperparameter sweep for SubLoRA. For all the SubLoRA pretraining experiments,
we use a learning rate of 0.0002, a LoRA dropout value of 0.1, and a LoRA alpha value of 32.
B.1.2 Hyperparameter Sweeps for Kronecker
For the Kronecker factorization W = A ⊗ B, we choose the matrices A and B such that
A∈Ra1×b1,B ∈Ra2×b2 where a 1a
2
=a and b 1b
2
=b. We sweep over all possible combinations
of {a ,a } and {b ,b } by performing prime factorizations with multiplicity on the numbers
1 2 1 2
a,b and enumerating all possible combinations. All of our Kronecker pretraining experiments
use a learning rate of 0.0002.
B.1.3 Hyperparameter Sweeps for Monarch
For the Monarch parametrization, we relax the restriction for the number of blocks to be strictly
√
a and instead by a number divisible by a to sweep over different numbers of blocks. We
also perform experiments for Monarch where we are using absolute position encodings and
experiments where we are only applying the Monarch factorization to the attention layers and
the linear classification heads.
B.2 Quantization
Quantization Following Lotfi et al. [31], we apply post-training quantization of the trainable
weights that correspond to the subspace parameters and/or the LoRA, Kronecker, Monarch
parameters along with layer norm weights depending on the compression setup.
Experiments on QuIP-quantized Models. We compute token-level bounds on pretrained
LLaMA1 and LLaMA2 models [47] quantized with QuIP with publicly-available checkpoints
[42]. Although we do not know what data was used to pretrain these models, we can evaluate
the generalization bound on the Amber dataset and consider other tokens used in training as a
data-dependent prior.
20B.3 Bounds Evaluation
In the sequence of text, we use end of text tokens (EOT) which separate the documents. In
this way, we can consider concatenating many documents together to form one long sequence.
As a result of the EOT tokens and the structure of the text, the distribution p(x |x ) can
i <i
be simplified into p(x |x ,x ,...x ) where k is the index of the most recent EOT token
i k k+1 i−1
because the documents are sampled independently. In the evaluation of the LLM we likewise
have no dependence on tokens outside the given document in question.
To compute token-level bounds, we evaluate all of our generalization bounds with failure
probability δ =0.05 and subsample size of n=10,0000 tokens from the OpenWebText training
dataset of size m=9 billion tokens or the Amber dataset of size m=1.2 trillion tokens.
B.4 Correlation with Downstream Performance
We retrieve the downstream task performance of difference GPT2 variants ranging in scale
from 117M to 1.5B averaged over the downstream datasets as shown in Table 7. To obtain an
approximation of the conditional BPD expectation that we bound in Equation (2), we resample
x from a LLaMA2-7B given fixed training contexts x from the Amber dataset. We use a
i <i
sample size equal to 10,000 samples.
Model LAMBADA LAMBADA CBT-CN CBT-NE WikiText2 PTB WikiText103 1BW
Size (PPL) (ACC) (ACC) (ACC) (PPL) (PPL) (PPL) (PPL)
117M 35.13 45.99 87.65 83.4 29.41 65.85 37.50 75.20
345M 15.60 55.48 92.35 87.1 22.76 47.33 26.37 55.72
762M 10.87 60.12 93.45 88.0 19.93 40.31 22.05 44.575
1542M 8.63 63.24 93.30 89.05 18.34 35.76 17.48 42.16
Table 7: Zero-shot downstream task performance for GPT2 models with different model sizes as
reported in Radford et al. [39].
B.5 Markov Chain Comparison
FortrainingtheMarkovchains,wereusetheBytePairEncoding(BPE)tokenizationtoseparate
out the effect of the tokenizer. We apply prediction smoothing at level α=0.1 to the Markov
models to give them nonzero probability to ngrams that have not been seen in the training data
and limit the worst case NLL of a single token.
For constructing generalization bounds with the Markov chain, we upper bound the complexity
term log1/P(h) similarly to the large language models by performing quantization and com-
pression. We store and update the Markov chains sparsely, which becomes necessary when
considering the high order variants. In storing the model, we use a dictionary mapping each
prefix concatenated with the following token to a count. The counts can then be converted into
probabilities by normalizing by the count containing just the prefix. We quantize the counts
and store them in 16 bits, and we store the keys using a basic encoding. For training, we train
on a subsample of 106 tokens from the training corpus, sufficient for the performance of the
Markov chains to converge.
B.6 Memorization Experiment
Following Goldblum et al. [17], we select a complexity value of 4, which reflects the difficulty of
the task, and a sequence length of 30 and generate 984 sequences as the training dataset for
structured sequences. To build our baseline random sequences, we collect all unique integers in
the generated sequences into a set. We then sample integers IID from a uniform distribution
21over the set of unique integers from the structured sequences to build the baseline dataset.
Our vocabulary size is 12 as we only have integers, the beginning of text token, and an
additional delimiter token. The delimiter tokens are placed between distinct numbers during
our tokenization process. We use a GPT-2 Small model with 124M parameters and train it on
the structured and random sequences separately with a learning rate of 0.0001 for 1000 epochs.
Our quantization procedure is the same as described in Appendix B.2. We show the results for
this experiment in Figure 4.
B.7 Amber Dataset
We use a subset of the pretraining dataset for Amber 7B LLM [29] for our bound evaluations.
This dataset contains RedPajama V1 [8] (arxiv, C4, GitHub, StackExchange, Wikipedia),
StarCoder [26] (The Stack), RefinedWeb [37] (CommonCrawl) with around 1.2 trillion tokens.
We tokenize the entire dataset using a LLaMA tokenizer and then sample tokens from a uniform
distribution over the tokenized dataset.
B.8 Compute Budget
For all our pretraining experiments with the three proposed compression techniques, we run
each experiment for 5 days on 4 GPUs in parallel that are of type A100s or RTX8000. For
the bound computation experiments, we use a single GPU of any type and a subsample size of
10,000 samples. The running time varies between 1 to 8 hours depending on the model and the
dataset. All other experiments are performed on a single GPU of any type.
B.9 Bounds on Antibody Sequences
B.9.1 Datasets
An antibody consists of both the light chain and the heavy chain amino acid sequences. Among
these sequences, there are collections of sequences called the clonal family that our immune
systems developed to bind to targets. For our experiments, we select all human heavy chain
amino acid sequences from the Observed Antibody Space (OAS) and keep all the clonal families
with at least 25 sequences using the FastBCR filtering technique following [35, 49, 2]. The
processed dataset contains around 908 thousand heavy chain clonal families. A single example
in our dataset is thus a clonal family looking like [sequence 1,sequence 2,...,sequence N] where
N ≥25.
There are in total 29 different tokens with 20 of them corresponding to 20 different amino acids.
Let [ClSep] be a separator token between sequences in a clonal family. We process our input
example by forming the string “sequence 1 [ClSep] sequence 2 [ClSep] ... [ClSep] sequence N”
following [2]. This input example is tokenized and given to a language model using the next
token prediction training objective.
B.9.2 Language Models
WeuselanguagemodelarchitecturesthatarebasedontheMistral7Barchitecture[20]. Wescale
down the Mistral architecture using 24 layers with a varying hidden state size of (1024,768,512),
resulting in our Mistral 377M, Mistral 212M, and Mistral 94M models, respectively following
[2]. With a vocabulary size of 29 and a maximum context length of 2048, we train each of our
models using 4 NVIDIA A100s for 48 hours and perform post-training quantization following
Lotfi et al. [31].
22Bits per Top-1 Top-10 Top-100
Model
Dimension Error (%) Error (%) Error (%)
2 bits
LLaMA1-7B 4.29 48.08 22.82 12.83
LLaMA1-13B 4.60 48.87 24.23 14.59
LLaMA1-30B 5.37 52.91 28.06 19.14
LLaMA1-65B 6.10 56.63 32.29 24.14
LLaMA2-7B 4.28 47.55 22.48 12.56
LLaMA2-Chat-7B 4.54 49.10 24.18 13.50
LLaMA2-13B 4.52 47.85 23.54 14.44
LLaMA2-Chat-13B 4.77 49.82 24.95 15.10
LLaMA2-70B 6.14 56.24 32.61 24.32
LLaMA2-Chat-70B 6.40 58.26 34.16 25.04
3 bits
LLaMA1-7B 4.37 47.42 22.87 13.63
LLaMA1-13B 4.80 48.97 25.23 16.14
LLaMA1-30B 5.70 53.54 29.91 21.63
LLaMA1-65B 6.73 59.56 36.14 28.08
LLaMA2-7B 4.35 47.15 22.75 13.62
LLaMA2-Chat-7B 4.65 48.84 24.23 14.24
LLaMA2-13B 4.76 48.45 24.67 15.95
LLaMA2-Chat-13B 5.06 50.90 26.26 16.66
LLaMA2-70B 6.77 59.35 36.27 28.56
LLaMA2-Chat-70B 7.08 61.66 38.00 29.30
4 bits
LLaMA1-7B 4.50 47.52 23.53 14.52
LLaMA1-13B 5.02 49.96 26.46 17.47
LLaMA1-30B 6.05 55.55 32.09 23.93
LLaMA1-65B 7.27 62.56 39.38 31.54
LLaMA2-7B 4.49 47.64 23.64 14.53
LLaMA2-Chat-7B 4.83 49.49 25.15 15.12
LLaMA2-13B 4.96 49.46 25.67 17.21
LLaMA2-Chat-13B 5.27 51.61 27.23 18.12
LLaMA2-70B 7.33 62.53 39.89 32.11
LLaMA2-Chat-70B 7.68 65.32 41.59 32.87
Random Guess 14.97 99.99 99.96 99.68
Table 8: Non-vacuous token-level generalization bounds for open-source pretrained
LLM checkpoints on the Amber dataset. All of these models were quantized post-training
using QuIP# to different numbers of bits as shown above. All the bounds are non-vacuous
compared to random guess performance.
C Additional Results
C.1 LLaMA Bounds on Amber
In Table 8, we have the complete bounds computation results for LLaMA1, LLaMA2, LLaMA2
Chat with 2 bits, 3 bits, and 4 bits quantization. The best bound is achieved by a LLaMA2
model with 2 bits quantization.
C.2 Generated Text
In Table 9, we show the generated text by the model achieving the best bounds: the quantized
GPT2 model that achieves the best token-level bounds on the OpenWebText dataset in our
23Generated Text
GPT2 (124M)
Quantized The study, published in Proceedings of the National Academy of
(BPD Bound: 7.61) Sciences, examined the relationships between brain activity, gene
expression and inflammation in diseases including Alzheimer’s dis-
ease, dementia, Parkinson’s disease, glioblastoma and Alzheimer’s
disease. "Our study demonstrates that omega-3 fatty acids play a
role in the link between inflammation and brain function," said
lead author Dr Richard Collins, PhD, of Duke University’s Duke
Center for Bioethomics and Bioengineering. After controlling for.
GPT2 (124M)
SubLoRA th he the startedt at its„ the a more be power and- by. S and, of
[32] of -’s on. The UK I The, are the on the the under, but the then
the day,. The. The. It for the! a,. M an they first the the speak
have times. cover that ( illegal In the day where I The who when
and $ In We¨:[{¨: As she I WeP spirituality. The all And one which
a more says thought the other (ed 15: And P It as/ T - 2 But We
The The theah It who the full of that to was ’The they (It As We
A and each (. The It - We The M I“
Table9: Thebestnon-vacuoustoken-levelboundscorrespondtomodelsthatgenerate
high quality text. Examples of generated text from the GPT2 small quantized model that
achieves the best token-level bounds compared to the SubLoRA-pretrained GPT2 small model
in Lotfi et al. [32]. In contrast to the text generated by the best performing model in terms of
BPDboundsbyLotfietal.[32],ourquantizedGPT2smallgeneratessignificantlyhigher-quality
text while simultaneously achieving the best BPD and Top-1/10/100 error bounds.
work, and the GPT2 model trained with SubLoRA that achieves the best document-level
bounds in Lotfi et al. [32]. The text generated by the model achieving the best generalization
bounds in our work is visibly more coherent and grammatically correct. By switching from
document-level to token-level bounds, obtaining non-vacuous bounds requires less restrictive
compressiontechniquesandthereforecanbeachievedforhighlyperformantmodelsthatgenerate
high-quality text and can be deployed in practice.
24