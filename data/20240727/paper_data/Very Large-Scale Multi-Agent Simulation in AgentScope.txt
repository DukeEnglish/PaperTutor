Very Large-Scale Multi-Agent Simulation in AgentScope
Xuchen Pan1, Dawei Gao1, Yuexiang Xie1, Zhewei Wei2, Yaliang Li1,†, Bolin Ding1,†,
Ji-Rong Wen2, and Jingren Zhou1
1Alibaba Group
2Renmin University of China
Abstract
Recent advances in large language models (LLMs) have opened new avenues for applying multi-agent
systems in very large-scale simulations. However, there remain several challenges when conducting
multi-agent simulations with existing platforms, such as limited scalability and low efficiency, unsatisfied
agent diversity, and effort-intensive management processes. To address these challenges, we develop
several new features and components for AgentScope, a user-friendly multi-agent platform, enhancing
its convenience and flexibility for supporting very large-scale multi-agent simulations. Specifically, we
proposeanactor-baseddistributedmechanismastheunderlyingtechnologicalinfrastructuretowardsgreat
scalability and high efficiency, and provide flexible environment support for simulating various real-world
scenarios, which enables parallel execution of multiple agents, centralized workflow orchestration, and
bothinter-agentandagent-environmentinteractionsamongagents. Moreover,weintegrateaneasy-to-use
configurabletoolandanautomaticbackgroundgenerationpipelineinAgentScope,simplifyingtheprocess
of creating agents with diverse yet detailed background settings. Last but not least, we provide a
web-basedinterfaceforconvenientlymonitoringandmanagingalargenumberofagentsthatmightdeploy
across multiple devices. We conduct a comprehensive simulation to demonstrate the effectiveness of the
proposedenhancementsinAgentScope,andprovidedetailedobservationsanddiscussionstohighlightthe
great potential of applying multi-agent systems in large-scale simulations. The source code is released on
GitHub1 to inspire further research and development in large-scale multi-agent simulations.
1 Introduction
Largelanguagemodels(LLMs), suchasGPT-4(OpenAI,2023), Claude3.5(ANTHROP,2024), Qwen2(Yang
etal.,2024),Llama3(Meta,2024),andsoon(Aniletal.,2023;GLMetal.,2024;MistralAI,2024;Yangetal.,
2023), notable for their vast number of parameters and extensive training on diverse large-scale datasets,
demonstrate remarkable capabilities in understanding, generating, and interacting with human language.
Recent advancements in LLMs have sparked a revolution in natural language processing and relevant fields,
paving the way for novel applications that were previously inconceivable.
Building on the capabilities of LLMs, there is a growing interest in the development of intelligent agents
that are empowered to resolve practical tasks (Hong et al., 2023; Ren et al., 2024). As the scope of these
intelligent agents spans a wide array of applications, their potential to redefine the landscape of simulations
becomes increasingly evident (Matsumoto et al., 2024; Sorokovikova et al., 2024; Sreedhar and Chilton, 2024;
Yue et al., 2024). Traditional simulations heavily rely on predefined rules and sophisticated mechanisms to
generatesimulatedscenarios,necessitatinglotsofexpertiseandhumaninterventions(MacalandNorth,2010).
With the incorporation of LLM-empowered agents, simulations are expected to become more interactive,
adaptive, and realistic, while requiring substantially fewer human efforts.
Recently, several platforms (Hong et al., 2023; Wu et al., 2023; Team, 2023) have been proposed to
streamline the development of multi-agent systems, providing some fundamental functionalities including
unified LLM services, various tools, and advanced reasoning algorithms. Despite significant progress, we
†Correspondingauthors.
1https://github.com/modelscope/agentscope
1
4202
luJ
52
]AM.sc[
1v98771.7042:viXraidentifyseveralchallengesinconductingsimulationswithmulti-agentplatforms,particularlywhenthenumber
of agents becomes extremely large. We summarize these challenges below.
(i)ScalabilityandEfficiencyLimitations Thescaleofinvolvedagentscanbecriticalwhenconducting
certain simulations, since simulations at a small scale run the risk of inaccurately representing real-world
complexities, making simulations less realistic and reliable (Macal and North, 2010; Macal, 2016). However,
increasing the scale ofagents brings challenges to the simulationplatform in terms of scalability and efficiency.
Specifically, it is non-trivial to efficiently organize agents to execute their tasks and communications following
an appropriate order, with the aim of reducing the running time while ensuring accurate results. Moreover,
the simulation platform should be capable of handling high-frequency access to support both inter-agent and
agent-environment interactions in large-scale agent-based simulations.
(ii)Unsatisfied Population Distributions and Agent Diversity For a large-scale simulation, it is
essential that the involved agents exhibit diverse behaviors while generally following a specific population
distribution (Gao et al., 2023; Ren et al., 2024). Assigning agents with simple backgrounds may result in a
significant number of highly homogenized agents, making it difficult to derive meaningful insights. Besides,
existing studies rarely consider how to specify population distributions of agents from various perspectives,
such as age, education, occupation, etc. which reduces the realism of the simulations.
(iii)Difficult Management Processes As the scale of agents increases, it becomes rather effort-
intensive to manage the simulations, including initialization, execution, and termination of a large number of
agents spread across multiple devices, as well as monitoring their status, behaviors, and interactions (Mou
et al., 2024). Such difficulties in managing make it challenging to promptly identify valuable group-level
and individual-level behaviors, which can further hinder the discovery of critical insights for optimizing
simulations and advancing research. Therefore, an easy-to-use tool for managing large-scale agents is a
necessary functionality that should be provided by the agent-based simulation platforms.
To tackle these challenges, we adopt a user-friendly multi-agent platform, named AgentScope (Gao et al.,
2024), as the foundation framework to provide the basic functionalities, and further develop several new
features and components upon it to improve its usability, convenience, and flexibility for supporting very
large-scale multi-agent simulations.
To be specific, we propose a distributed mechanism based on the actor model (Agha, 1985), featuring
automatic parallel execution and centralized workflow orchestration to provide great scalability and high
efficiency for multi-agent-based simulations. The proposed actor-based distributed mechanism enables us to
further expand the scale of agents in the simulation with a limited number of devices, and provides linear
benefit on running time from the addition of devices. We support both inter-agent and agent-environment
interactions in the simulations. Agents can initiatively communicate with other agents and the environment,
and can respond to messages from other agents and changes in the environment. Furthermore, a multi-layer
environmentstructureisproposedtosupportgroup-wiseinformationsynchronization,enhancingtheflexibility
of AgentScope for simulating various real-world scenarios.
Besides, to satisfy the requirements of population distribution and agent diversity, we integrate a
configuration tool in AgentScope and provide automatic background generation. Users only need to simply
specify the distributions of the population from several aspects, a large number of agents with detailed
and diverse characteristics can be effortlessly generated accordingly. These agents can be managed and
monitoredconvenientlythroughAgentScope-Manager,aproposedmoduleforsimplifyingtheorganizationand
observation process of large-scale agent-based simulations. Using a web-based visual interface, AgentScope-
Manager provides a comprehensive overview of all agents across multiple devices, allowing users to efficiently
configure, launch, and terminate these agents.
Withsuchanagent-basedsimulationplatform,weconductacomprehensivesimulationontheclassic“guess
2 of the average” game (Nagel, 1995; Camerer et al., 2004) to demonstrate the improvements and advances
3
brought by the infrastructure introduced above. Firstly, we conduct agent-based simulations involving 1
millionagentsusingonly4devices,showingthescalabilityandefficiencyoftheplatform. Then,weincorporate
agentsusingdifferentLLMsofdifferentsizes,equippedwithdifferentpromptsanddiversebackgroundsettings,
resulting in various and realistic behaviors in the simulations. We provide comprehensive observations on
both collective and individual behaviors, drawing meaningful and valuable insights from a series of simulation
experiments, along with further discussions on helpful tips and open questions. These experimental results
confirm the feasibility and great potential of conducting large-scale agent-based simulations in AgentScope.
We have released the source code at https://github.com/modelscope/agentscope for future research.
22 Related Works
LLM-Empowered Agent Platforms WiththeadvancesofLLMs,asignificantnumberofagentplatforms
havebeendevelopedtointegrateLLMsintoreal-worldapplicationsandassisthumansinproblem-solving(Wu
etal.,2023;Hongetal.,2023;Lietal.,2023b;Significant-Gravitas,2023;Team,2023;Gaoetal.,2024). These
platformscanbecategorizedintosingle-agentplatformsandmulti-agentplatforms. Thesingle-agentplatforms
include AutoGPT (Significant-Gravitas, 2023), LangChain (langchain ai, 2024a), ModelScope-Agent (Li et al.,
2023a), and Transformers Agents (Wolf et al., 2020), which are proposed to resolve practical tasks using
LLMs. On the other hand, multi-agent platforms like MetaGPT (Hong et al., 2023), Auto-Gen (Wu et al.,
2023), CAMEL Li et al. (2023b), and LangSmith (langchain ai, 2024b) employ multi-agent collaboration
to tackle more complex challenges, including software programming (Hong et al., 2023; Qian et al., 2023),
data science (Hong et al., 2024), social simulation (Park et al., 2023), game-playing (Gao et al., 2024), etc.
Although remarkable progress has been made, applications built on these platforms can currently be limited
in the scale of agents and suffer from low efficiency, hindering their potential for large-scale simulations.
To address these limitations, we enhance AgentScope with new features and components to support very
large-scale agent-based applications and improve the running efficiency of these applications.
Agent-Based Simulation Frameworks Due to the ability of LLMs to imitate human behaviors, agent-
based simulation has become an attractive topic in the research community (Gürcan, 2024; Matsumoto
et al., 2024; Sorokovikova et al., 2024; Sreedhar and Chilton, 2024; Ye and Gao, 2024; Team et al., 2024;
Park et al., 2022). Previous studies have explored the integration of LLMs in various fields, including
education (Yue et al., 2024), economic (Matsumoto et al., 2024), societal study (Park et al., 2023; Ye and
Gao, 2024; Gao et al., 2023; Ren et al., 2024), transportation (Jin et al., 2023), healthcare (Zhang et al.,
2023) etc. Recently, researchers have built up several LLM-based or agent-based simulation frameworks. For
instance, Vidur (Agrawal et al., 2024) is a simulation framework that focuses on providing high-throughput
LLM services, Ataei et al. (2024) is proposed for design requirements elicitation, Cheng et al. (2023) is
developed to evaluate the level of caricature, and Ren et al. (2024) is designed to simulate the behaviors of
web search users. However, these existing frameworks are domain-specific, making it challenging for users to
conduct large-scale agent-based simulations for a wide variety of applications. To tackle this, we design a
multi-agent-based simulation platform AgentScope and provide easy-to-use configurable tools to ease the
heavy workload associated with conducting various large-scale simulations.
3 Infrastructure
To provide the basic functionalities required for conducting agent-based simulations, including LLM services,
memory management, and agent interactions, we adopt AgentScope, a user-friendly multi-agent platform
designed for flexible Standard Operating Procedure (SOP) tasks, as our foundation framework. We further
develop several new features and components, making it more convenient and feasible to support very
large-scale simulations involving multiple agents.
Specifically, we first design an actor-based distributed mechanism (Sec. 3.1) that serves as the underlying
technological infrastructure for conducting large-scale simulations, providing great scalability and high
efficiency. Buildinguponsuchaninfrastructure,weenablebothinter-agentinteractionsandagent-environment
interactions (Sec. 3.2) to facilitate multi-agent simulations, which forms the core components that drive the
simulated dynamics. To improve the diversity of agents involved in the simulations, we allow users to set
heterogeneous configurations for agents by specifying their population distributions and detailed background
settings (Sec. 3.3). Furthermore, we build a graphical user interface to monitor and manage the distributed
agents on different devices, making it easy to observe and organize large-scale agents in the simulations
(Sec. 3.4). In the following subsections, we elaborate on the details of these proposed enhancements.
3.1 Actor-based Distributed Mechanism
The actor model is a mathematical model of concurrent computation, where each actor acts as a basic
computing unit, receives messages, and computes independently (Agha, 1985). Based on the actor model, we
build a distributed mechanism to provide great scalability and high efficiency for agent-based simulation.
3A A A A
B C B C B C B C
D D D D
E E E E
A Waiting A Running A Finished
Figure 1: An example of automatic parallel execution, where circles represent agents and directed edges
represent message passing flows.
The proposed actor-based distributed mechanism serves as the basic technological infrastructure, featuring
automatic parallel execution and centralized workflow orchestration.
Automatic Parallel Execution In a multi-agent simulation, the interactions between agents follow an
atomizedpattern,whereinteractionsoccurwithinsmallisolatedcliques(Matsumotoetal.,2024;Sorokovikova
et al., 2024). Such a pattern holds significant potential for parallelization, leading to substantial gains in
efficiency. However, achieving parallelization is non-trivial as it requires the formal definition of interactions,
the identification of potential parallelizable computations, and subsequent optimization.
To provide automatic parallelization in AgentScope, we first format the inter-agent interactions in the
simulation as a communication graph, where each vertex v represents an agent, and each directed edge e
represents a message passing from one agent to another. Note that the communication graph can be a
directed cyclic graph that might be highly dynamic and uncertain, which is different from the concept of a
computational graph in machine learning (Ansel et al., 2024) and thus makes it much more challenging in
providing automatic parallelization.
With the communication graph, we can dynamically identify the agents that are ready to be executed,
which indicates that they do not depend on the outputs of others or that all their dependencies have been
fulfilled. These agents are executed automatically in parallel, utilizing the maximum available resources. As
they finish one by one, more blocked agents become active and begin their executions.
Such a design of automatic parallel execution is well-suited for the actor-based distributed mechanism.
Note that we employ the multi-process mode rather than the asynchronous programming to further improve
efficiency, wherein each agent can run in a separate process and be treated as an independent node in the
communication graph. As a result, an agent’s internal computations can only be triggered when it receives
the required messages. In this way, each agent only relies on those from which they receive the necessary
messages, and the communication graph automatically divides into independent subgraphs based on their
dependencies. With these advanced designs and implementations in AgentScope, users only need to specify
the message passing paths among agents, the multi-agent simulation can be automatic parallel execution
without additional effort.
AnexampleofautomaticparallelexecutionisshowninFig.1,whereagent-Bandagent-Careindependent
of each other but rely on the messages from agent-A, allowing them to be executed in parallel once the
execution of agent-A is completed. In contrast, agent-D and agent-E cannot be executed in parallel, as
agent-E depends on messages from both agent-B and agent-D.
Centralized Workflow Orchestration In addition to the proposed automatic parallel execution, the
workloadassociatedwithsimulationorchestrationisalsoimportantforsupportingverylarge-scalesimulations,
as users need to set up and manage distributed agents across multiple devices. These requirements might
not be well satisfied when there is a lack of a complete and explicit view of the entire workflow during the
development and execution phases.
To resolve these issues, we further enhance the proposed actor-based distributed mechanism by allowing
users to orchestrate the simulation explicitly and centrally. For simplicity, we refer to the central process that
4Placeholderofy:Tellthe I’mwaitingforAto
o tht ehe vr aa lug een ot fs ywheretoget returnthevalueofy!
Placeholderofy Returnz
Center B Center B Center Calculatingz Center B
Askforz
Sendx Waitfory Returny
A Calculatingy A A
x=Msg(“Bob”,“Hi!”) x=Msg(“Bob”,“Hi!”) x=Msg(“Bob”,“Hi!”) x=Msg(“Bob”,“Hi!”)
y=agent_A(x) y=agent_A(x) y=agent_A(x) y=agent_A(x)
z=agent_B(y) z=agent_B(y) z=agent_B(y)
z.get_value() z.get_value()
Thegraphisconstructedatonce Theactualcalculation
Figure 2: How placeholder works in simulations to enable users orchestrate the workflow centrally.
runs the simulation as the center. For each distributed agent, we adopt a proxy in the center that serves as
the substitute for it, hiding the remote calculation from users and being responsible for invoking functions
and receiving messages from the corresponding distributed agent. With the proxy, users can specify the
communications between distributed agents in the center.
However, when implementing such a centralized workflow orchestration, a question arises: How can
sequential execution at the center be compatible with automatic parallel execution? Totacklethis, weelaborate
on the concept of placeholder, which ensures that the workflow execution in the center is not blocked by
the calculations in distributed agents. In the central process, once an agent proxy receives a message, it
immediately returns a placeholder and forwards the message to its corresponding distributed agents. For a
distributed agent, when receiving a message, it first checks if the message contains a placeholder to decide
whetheritneedstorequestandwaitfortheactualvaluefromtheagentsindicatedontheplaceholder. Agents
execute their inner computations accordingly as soon as they receive all actual values in placeholders.
Fig. 2 shows how the placeholder works. From the example we can see that, agent-A receives a greeting
message x and sends a response message y to agent-B. After that, agent-B replies with a message z when
it receives y. In fact, from the view of computation, the center sends message x to agent-A, provides the
placeholder of y to agent-B, and requests for the real value of z to construct the workflow. Upon receiving
the placeholder of y, agent-B sends a request and waits for agent-A to finish calculating y. When agent-B
receives y, it begins to calculate and return z to the center. Note that the construction of the workflow graph
(the green part) and the computation (the blue part) start at the same time, while the construction of the
workflow graph is instantaneous. In this way, we allow the independent distributed agents to run in parallel.
In summary, enhanced by the proposed actor-based distributed mechanism, AgentScope offers high
efficiency in inter-agent communications, enables parallel execution of multi-agents, and allows centralized
workflow orchestration to simplify usage.
3.2 Agent-Environment Interactions
The agent-environment interactions are crucial alongside the above inter-agent communication in simulations,
which allows agents to access the environment, respond to changes in it, and alter it if needed. Considering
the large number of agents, the agent-environment interactions should be capable of handling high-frequency
access. For example, in a social simulation like AI town (Park et al., 2023), the environment includes timeline
and map with the interactive items on it. Agents might frequently check and interact with the items on the
map, and the change of these items should also trigger the reactions of agents.
To satisfy such requirements, we abstract the environment operations into registering, querying, updating,
removing, and monitoring by providing a base class, which can be adapted to various underlying storage
databases, including key-value stores, relational databases, NoSQL databases, and so on. We provide two
dimensions for users to perform agent-environment interactions, i.e., timeline and location. For the timeline,
users can set specific triggers to make the agents access the global time and adjust their behaviors accordingly.
For the location, the environment serves as a map maintaining the locations of agents and providing hook
functionstotriggertheinteractionswithagentsoritemsnearby. Suchadesignprovidesaflexibleenvironment
support for simulating various real-world scenarios.
5Globalenvironment
Sub-environment1 Sub-environment2
Figure 3: Multi-layer environment structure for agent-based simulation.
We implement the environment module as a distributed node, which performs communication with agents
through Remote Procedure Call (RPC) to allow high-frequency access. When receiving requests from agents,
the environment first categorizes these requests into different queues according to the accessed data, and then
processes the requests in different queues simultaneously to resolve the issues of data conflicts. Users can set
the maximum concurrency based on available hardware resources.
Furthermore, we provide a multi-layer environment structure to support group-wise information synchro-
nization in the simulation, as illustrated in Fig. 3. Different environments can be established for different
groups of agents to provide interactive items and shared information. Upon these environments, a global
environment can be set up for global synchronization. Such a multi-layer environment structure can be
employedinsimulationsthatnecessitateintra-groupcollaborationsandinter-groupinformationdifferentiation.
A use case of the multi-layer environment can be found in Sec. 4.6.
3.3 Heterogeneous Configurations
In a simulation, agents are expected to act as humans with diverse backgrounds, including different ages,
genders, careers, nationalities, education, experiences, etc. An intuitive approach is to add these background
settings of agents in their system prompts, providing guidance for agents on the roles to play and actions
to take. However, for large-scale simulations, providing diverse, heterogeneous, and reasonable background
settingsforagentscanbelaboriousandtime-consuming,especiallywhenprecisecontrolofdifferentpopulation
distributions is required in certain simulations. This problem motivates us to provide tools in AgentScope to
assist users in effortlessly setting up large-scale agents with diverse background settings.
Configurable Tool Specifically, users can begin by defining the total population of the simulation, and
then specify the distributions of the population from various perspectives. We provide some widely-used
distribution templates in AgentScope for convenient usage, from the aspects of age, gender, occupation,
nationality, and education. Besides, the proposed configurable tool allows for easy extension of new aspects,
enhancingitsflexibilitytomeetdiverserequirements. Listing1inAppendixAshowsanexampleconfiguration
file for a group of people with different educational levels, in which a distribution can be specified by the
proportions of its different components.
Automatic Background Generation Pipeline After configurations have been provided via the above
tool, more detailed and heterogeneous background settings can be automatically generated to instantiate
the agents. Specifically, when users start a simulation, we draw specific values from the distributions based
on the configurations, convert them into a JSON format, and fill them into a meta prompt to produce the
completed instructions for background generation tasks. These instructions are utilized by LLMs to generate
heterogeneous background settings. To introduce more diversity, the generation process involves adjusting
the random seed and the temperature used by LLMs. Several examples of the generated background settings
can be found in Sec. 4.5, along with the results and analysis of the simulations involving diverse agents. The
adopted meta prompt can be found in Appendix B.
6Figure 4: The web-based visual interface for large-scale agents management.
In summary, by providing the easy-to-use configurable tool and the automatic background generation
pipeline in AgentScope, we simplify the configuration process for conducting large-scale agent-based simula-
tions, and further promote its research and applications.
3.4 Management for Large-scale Agents
In a simulation, users need to manage and monitor a large number of agents distributed across different
devices, which might become intractable to handle manually as the scale and complexity of the simulation
increase. To tackle this, we incorporate advanced forms of agent management and monitoring, named
AgentScope-Manager. Specifically, when users start a simulation, servers are first launched on all the
remotedevices,whichprovideresidentservicestoremotelycreate,monitor,andstopdistributedagents. These
servers are responsible for managing the lifecycle of distributed agents and synchronizing their information to
a web-based visual interface, as illustrated in Fig. 4. The web-based visual interface provides a comprehensive
overview of all registered servers and all deployed agents on different devices, from which users can view the
server’s identity, IP address, running status, and utilization of computing resources.
TheAgentScope-Manageralsosimplifiesthemanagementandmonitoringprocessesforconductingmultiple
simulations. Sincetheserverscanbereusedindifferentsimulations,usersdon’tneedtorestartthedistributed
servers between two simulations. Users can efficiently configure, launch, and terminate servers and agents
during the simulations as needed. With such a design, we streamline the management process by focusing
on servers rather than individual agents, thereby improving the efficiency and effectiveness of managing
large-scale agent systems in AgentScope.
In a nutshell, based on AgentScope, we implement an actor-based distributed mechanism that serves as
the underlying technological infrastructure, which is well-designed for both inter-agent and agent-environment
interactions,andprovidesgreatscalabilityandhighefficiencyinconductinglarge-scaleagent-basedsimulations
Buildingontheinfrastructure,weprovideheterogeneousconfigurationsandthemanagementserver,enhancing
the diversity of agents and simplifying the observation and organization of the simulation process.
7Prompt 1
Youareplayingamultiplayergame.
#GameRule
1. Eachplayerreportsarealnumberbetween0and100,inclusive.
2. Thewinnerwillbetheplayerwhosenumberistheclosestto2/3oftheaverageofallreportednumbers.
Directlyreportyournumberwithoutadditionalinformation.
Prompt 2
Youareplayingamultiplayergame.
#GameRule
1. Eachplayerreportsarealnumberbetween0and100,inclusive.
2. Thewinnerwillbetheplayerwhosenumberistheclosestto2/3oftheaverageofallreportednumbers.
Thinkstepbystepandthenreportyournumber.
4 Experiments
In this section, we conduct large-scale simulations to show the improvements and advances brought by the
proposed infrastructure and components in AgentScope. Meanwhile, we provide detailed observations and
in-depth discussions on the agents’ collective and individual behaviors, drawing valuable insights.
4.1 Settings
We set up a large number of agents to participate in the classic game guess the 2 of the average, where each
3
agent reports a real number between 0 and 100 and the agent who reports a number closest to 2 of the
3
average of all the reported numbers wins the game. In this game, intuitively the highest possible average is
100. Therefore, for winning the game, agents tend to report a number no larger than 100× 2 =662. Once
3 3
all agents adopt this strategy, 662 becomes the new highest possible average and thus they tend to report a
3
number no larger than 662 × 2 =444. This process continues until the average becomes 0 and all agents
3 3 9
report 0, indicating that the game has reached its Nash equilibrium. However, considering that agents may
not always be rational, those agents who report 0 cannot always win the game since the average does not
converge to 0 immediately. Agents should carefully take into account the possible actions of others before
reporting their numbers. Meanwhile, agents can adjust their strategies in a multi-round game according to
the average reported numbers in previous rounds.
Note that all the experiments in this section follow the aforementioned settings. With this
game,weaimtodemonstratethecapabilitiesofAgentScopeinsupportinglarge-scaleagent-basedsimulations,
and show how agents perform considerations and reasoning concerning their system prompts, background
settings, and other information obtained in the simulations.
Devices & LLMs The experiments are conducted on a cluster containing multiple devices, each of which
is equipped with 8 A100-80G GPUs, a 64-core CPU, and 1 TB of memory. We adopt vLLM (Kwon et al.,
2023) as the LLM inference engine to handle highly concurrent service requests. We utilize six powerful and
popular open-source LLMs of different sizes. We adopt their instruction versions due to their enhanced ability
to follow instructions. The details of the adopted LLMs are provided below:
• Llama3-8B / Llama3-70B (Meta, 2024): A series of open-source LLMs developed by Meta, which
have been pre-trained and fine-tuned on a massive corpus.
• Qwen2-7B / Qwen2-72B (Yang et al., 2024): The second generation of Qwen open-source LLMs,
developed by Alibaba.
• MistralAI-8x7B / MistralAI-8x22B (MistralAI, 2024): The open-source mixture-of-experts (MOE)
LLMs released by MistralAI, where each MOE LLM consists of eight 7B/22B models.
8106
105 Llama3-70B Prompt 1 Llama3-8B Prompt 1 Serial
Llama3-70B Prompt 2 Llama3-8B Prompt 2 105 Async
104 Ours
104
103
103
102
102
101
101
100
102 103 104 105 106 102 103 104 105 106
Scale of agents Scale of agents
(a) Varying scales of agents (b) Varying scales of agents
Llama3-70B Prompt 1 Llama3-8B Prompt 1 Llama3-70B Prompt 1 Llama3-8B Prompt 1
5120 Llama3-70B Prompt 2 Llama3-8B Prompt 2 6400 Llama3-70B Prompt 2 Llama3-8B Prompt 2
1280
1600
320
400
80
100
20
5 25
1 2 4 1 2 3 4
Number of devices Number of devices
(c) Varying numbers of devices when fixing the total(d) Varying numbers of devices when fixing agent
number of agents numbers per device
Figure 5: Agent-based simulations with varying scales of agents (a, b) and varying numbers of devices (c, d).
Due to the limited GPU memory, unless otherwise specified, we deploy eight Qwen2-7B / Llama3-8B
models, two Qwen2-72B / Llama3-70B / MistralAI-8x7B models, or one MistralAI-8x22B model on each
device. The generation temperature for all LLMs is set to 1.0 to promote the diversity of responses. Besides,
to prevent errors in response format, each agent executes two LLM server calls in every game round. The
first call is used to generate the response, including its thought process and the reported number, while the
second call is made to extract the reported number correctly.
System Prompts We provide system prompts for agents to guide them in defining their dialogue style,
background knowledge, task requirements, and so on. To be more specific, for playing this game, the system
prompt incorporates the game rules and response formats, as illustrated in Prompt 1. Besides, we can include
further behavioral guidance in the system prompts to encourage behaviors that more closely resemble those
of real human beings. For example, inspired by “chain-of-thought” studies (Wei et al., 2022; Wang et al.,
2023), we ask agents to think step by step before reporting their numbers, producing the system prompt
shown in Prompt 2.
Further explorations of system prompts, providing detailed instructions to enhance the performance and
diversity of agents, can be found in Section 4.4.
4.2 Scalability and Efficiency
First of all, we conduct a series of experiments to show the scalability and efficiency of the agent-based
simulations supported by the proposed actor-based distributed mechanism (see Sec. 3.1). Specifically, we
illustrate how the overall simulation running time changes as the number of participating agents grows when
using LLMs of different sizes, including Llama3-8B and Llama3-70B. In addition to the model sizes, the
system prompt provided to agents is also a factor that can influence the running time, since some prompts
(e.g., Prompt 2) may encourage agents to generate longer responses and thereby lead to longer response time.
From the experimental results shown in Fig. 5, we can obtain the following observations and insights.
9
)s(
emiT
)s(
emiT
)s(
emiT
)s(
emiTPrompt 1 Prompt 2 Prompt 1 Prompt 2
100 100 100 100
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0 0 0 0
Llama3-8 MB istralAI-8x7 QB wen2-7B Llama3-8 MB istralAI-8x7 QB wen2-7B Llama3-7 M0 iB stralAI-8x2 Q2 wB en2-72B Llama3-7 M0 iB stralAI-8x2 Q2 wB en2-72B
Figure 6: The distributions of numbers reported by agents with different LLMs and system prompts.
(i)We support an agent-based simulation involving 1 million agents, which can be completed
in 12 minutes using 4 devices. In Fig. 5a, we fix the device number to 4 and record the simulation
running time as the number of agents grows from 100 to 1M. It can be observed that the simulation involving
1 million agents finishes in 12 minutes when using Llama3-8B with Prompt 1, while it takes 85 minutes if
we choose Prompt 2, as the number of averaged response tokens grows by more than 150-fold2. For the
heaviest inference workload, i.e., when agents adopt Llama3-70B and Prompt 2, it takes around 10.6 hours
to complete the simulation.
(ii) The proposed actor-based distributed mechanism significantly improves the efficiency of
large-scale agent-based simulations. To better demonstrate the improvements brought by the proposed
actor-based distributed mechanism, we adopt a dummy model request (i.e., agents sleep for 1 second and
generate random numbers rather than posting the requests) in the simulation to remove the impact of the
LLM inference speed. The experimental results summarized in Fig. 5b show that, completing an agent-based
simulation with the proposed actor-based distributed mechanism involving 1 million agents only takes 40
seconds, whereas simulations using serial execution or asynchronous mode in Python (adopted by existing
works (Wu et al., 2023; Hong et al., 2023)) require around 12 days and 8.6 hours, respectively.
(iii) Increasing the number of devices can proportionally reduce the simulation running time.
As shown in Fig. 5c, we maintain the number of agents at 10,000 and vary the number of devices used in the
simulation. For Llama3-70B with Prompt 2, the simulation running time decreases from 22 minutes to 5.6
minutes as the number of devices increases from 1 to 4. Such a phenomenon can be attributed to a reduction
in the number of agents served within one device. As a comparison, we increase the number of devices from
1 to 4, and deploy 10,000 agents on each device, respectively. As illustrated in Fig. 5d, the running time
remains nearly the same as the number of devices and agents increases, which demonstrates the horizontal
scalability of AgentScope.
In summary, the proposed actor-based distributed mechanism in AgentScope enhances the efficiency in
conducting very large-scale agent-based simulations, and offers great scalability by allowing users to expand
the scale of agents from the addition of devices.
4.3 Simulation Results and Analysis
In this section, we add some detailed information with six LLMs and two system prompts. We summarize
the experimental results in Fig. 6, from which we derive the insights below, and provide more detailed results
(e.g., distributions and statistics of the reported numbers) and individual-level observations in Appendix D
and Appendix E.1, respectively.
From the comparisons in the figures, we observe that when utilizing a basic system prompt Prompt 1 with
most LLMs, agents generally tend to report numbers around 50. However, it is worth noting that agents with
MistralAI-8×7B and MistralAI-8×22B, report smaller numbers (36.63 and 31.69 in average, respectively)
than other agents. These results indicate that without providing specific instructions in system prompts, the
performance of agents can be different due to the LLMs they adopt, influenced by factors such as model sizes
and model architectures.
2ThenumberofresponsetokenswhenusingdifferentLLMsandsystempromptsaresummarizedinFig.16inAppendixC.
10
rebmun
detropeR
rebmun
detropeR
rebmun
detropeR
rebmun
detropeRPrompt 1 Prompt 2 Prompt 3 Prompt 4
50 L Mla ism tra a3 lA-7 I-0 8B x22B 30 L Mla ism tra a3 lA-7 I-0 8B x22B 30 L Mla ism tra a3 lA-7 I-0 8B x22B 15 L Mla ism tra a3 lA-7 I-0 8B x22B
Qwen2-72B Qwen2-72B Qwen2-72B Qwen2-72B
40 20 20 10
30 10 10 5
20
0 0
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Round Round Round Round
(a) (b)
Figure 7: The average of reported numbers in the multi-round games.
Llama3-70B MistralAI-8x22B Qwen2-72B
100 100 100
80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
Prompt 1 Prompt 2 Prompt 3 Prompt 4 Prompt 1 Prompt 2 Prompt 3 Prompt 4 Prompt 1 Prompt 2 Prompt 3 Prompt 4
Figure 8: The impact of different system prompts on different LLMs.
Whenwechangethesystempromptstoachain-of-thoughtprompt(i.e., Prompt2), thereportednumbers
of agents move forward zero markedly, with the winning numbers (i.e., 2 of the average) being much smaller
3
than those using Prompt 1, decreasing from 33.70 to 12.76. Meanwhile, we notice that more than 30%
of agents using MistralAI-8×22B report around 0, leading to the Nash Equilibrium of this game. These
observations in the simulations demonstrate the effectiveness and importance of providing suitable system
prompts for guiding agents to perform the thought processes.
Moving a forward step, we expand the simulation to multiple rounds. We inform agents of the winner
number in the previous round at the beginning of each round except the first one, and request each agent
to report a number at each round. Informing these winner numbers to agents enables them to adjust their
strategies accordingly. Such a process is implemented based on the agent-environment interaction mechanism
in AgentScope (see Sec. 3.2), with the winning number being set as an interactive item in the environment.
The simulation results are demonstrated in Fig. 7. From these results we can observe that, as the game
progresses from round to round, the reported numbers of agents gradually converge to 0, indicating that
agents have a good understanding of this game and are capable of considering other agents’ behaviors and
making rational decisions. Some case studies on the behaviors of agents are summarized in Appendix E.2.
Similarly, the chain-of-thought prompts can accelerate the game to reach its Nash equilibrium. For example,
in the fifth round, the average reported number of agents using Qwen2-72B with Prompt 2 is 2.02, which is
significantly smaller than those using Prompt 1, reporting 25.16.
Itisworthnotingthattheseexperimentalresultsareconsistentwithpreviousstudies(Nagel,1995;Camerer
et al., 2004) in social simulation, which confirms the reliability and significant potential of multi-agent-based
simulations. In the following subsections, we are continuing to explore how different configurations affect the
simulation results, such as system prompts, background settings, mixtures of LLMs, and so on.
4.4 Detailed Instructions in System Prompts
To further explore the impact of behavioral guidance on agents, we incorporate more detailed instructions
tailored for this game in the system prompts. Specifically, we remind agents that all their competitors are
rational and will try to adjust the reported numbers by analyzing others’ strategies, resulting in Prompt 3
and Prompt 4 respectively. With adding such behavioral guidance in the system prompts, we expect agents
can engage in more thoughtful and diverse considerations before reporting their numbers, thereby making
11
rebmun
detropeR
rebmun
detroper
fo
GVA
rebmun
detroper
fo
GVA
rebmun
detropeR
rebmun
detroper
fo
GVA
rebmun
detropeR
rebmun
detroper
fo
GVAPrompt 3
Youareplayingamultiplayergame.
#GameRule
1. Eachplayerreportsarealnumberbetween0and100,inclusive.
2. Thewinnerwillbetheplayerwhosenumberistheclosestto2/3oftheaverageofallreportednumbers.
#Note:
1. Allplayersarerational.
Thinkstepbystepandthenreportyournumber.
Prompt 4
Youareplayingamultiplayergame.
#GameRule
1. Eachplayerreportsarealnumberbetween0and100,inclusive.
2. Thewinnerwillbetheplayerwhosenumberistheclosestto2/3oftheaverageofallreportednumbers.
#Note:
1. Allplayersarerational.
2. Allplayerswilltrytoguesstheothers’strategiestoadjusttheirownstrategies.
Thinkstepbystepandthenreportyournumber.
simulations more practical, meaningful, and interesting.
The comparisons among different system prompts are illustrated in Fig. 8. In general, from the figure we
can observe that the reported numbers are closer to 0 when using Prompt 3 and Prompt 4 than those of
Prompt 1 and Prompt 2. These experimental results indicate that detailed instructions are more effective
than general guidance (e.g., “think step by step”) in encouraging agents to perform thoughtful considerations
and take rational actions. Several case studies shown in Appendix E.3 confirm the improvements brought by
adding detailed instructions in the system prompts.
Furthermore, in a multi-round game illustrated in Fig. 7, agents using Prompt 3 and Prompt 4 can
converge to the Nash equilibrium faster than those using Prompt 1 and Prompt 2. For example, agents with
Qwen2-72B report 35.30, 6.11, 1.55, and 1.69 in average at the third round when using Prompt 1, Prompt 2,
Prompt 3, and Prompt 4, respectively, while in the fifth round, the average of reported numbers become
25.16, 2.02, 0.14, and 0.15.
It is worth noting that the impact of the system prompts on different LLMs can be different. For example,
fromtheperspectiveoftherangeofthereportednumbers(i.e., themaximumandminimumvalueofreported
numbers among all agents), employing Prompt 3 and Prompt 4 in Qwen-72B can significantly reduce the
maximum number, while that of Mistral-8×22B remains unchanged. Besides, using detailed instructions in
system prompts might increase the token number of responses, as summarized in Appendix C, because agents
are likely to consider multiple aspects before providing an answer.
4.5 Diverse Background Settings
The diversity of agents is a critical factor in agent-based simulations. In Sec. 3.3, we introduce the proposed
configurable tool and background generation pipeline designed to automatically instantiate agents with
diverse background settings. In this subsection, we conduct simulation experiments involving diverse agents,
considering their educational levels and occupations.
Specifically, we divide the agents into several groups, each of which consists of 200 agents. For each group,
we manually provide a basic configuration and utilize LLMs to generate a detailed description for each agent,
thereby further enhancing the diversity of the agents. These generated background settings are added to the
system prompts and labeled as “character background”, using Prompt 5.
12Prompt 5
Youareplayingaroleinamultiplayergame,makesureyourbehaviorfitsthefollowingcharacterbackground.
#CharacterBackground
{background}
#GameRule
1. Eachplayerreportsarealnumberbetween0and100,inclusive.
2. Thewinnerwillbetheplayerwhosenumberistheclosestto2/3oftheaverageofallreportednumbers.
#Note
1. Pleasestrictlyfollowyourcharacterbackgroundinthegame.
Thinkstepbystepandthenreportyournumber.
80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
Elementary High Bachelor Master Ph.D. Elementary High Bachelor Master Ph.D. Elementary High Bachelor Master Ph.D.
(a) Llama3-70B (b) MistralAI-8x22B (c) Qwen2-72B
Figure 9: The distributions of numbers reported by agents characterized with different educational levels.
Agents with Different Educational Levels Firstly, we set up a simulation experiment involving agents
with different educational levels, in which we assign five different educational levels to agents, including
elementary school, high school, bachelor, master, and Ph.D. An example the generated background settings
can be found in Appendix E.4.
ThesimulationresultsareillustratedinFig.9, fromwhichwecanderivethefollowinginsights. Ingeneral,
the higher the educational level of agents, the lower the average reported numbers, indicating more rational
behaviors in this game. For example, when using Qwen2-72B, the average numbers reported by agents with
a Ph.D. education are much lower than those reported by agents characterized as primary school students.
Meanwhile, from the observations on the individual-level behaviors, we notice that agents can effectively
perform reasoning processes and make corresponding decisions according to the assigned roles. We provide
some case studies in Appendix E.5. Besides, different LLMs demonstrate varying sensitivities to educational
levels in the background settings. For example, MistralAI-8×22B has the least sensitivity, with the largest
difference in average reported numbers is 3.49 (32.73 v.s. 29.24), while that of Llama3-70B and Qwen2-72B is
5.72 and 8.24, respectively.
Agents with Different Occupations We also conduct a simulation experiment involving agents with
different occupations, in which we assign six different occupations to agents, including professors in game
theory, economists, psychologists, athletes, artists, and writers. An example of the generated background
settings can be found in Appendix E.4.
As demonstrated in Fig. 10, the experimental results confirm the impact of the different occupational
descriptionsassignedtotheagents. Itcanbeconcludedthatagentscharacterizedasprofessorsingametheory
and economists tend to report smaller numbers than other agents. Some case studies on individual-level
behaviors, as shown in Appendix E.5, further confirm that agents’ considerations and actions are consistent
with their respective occupations. For example, agents characterized as professors in game theory tend to
adjusttheirbehaviorsundertheassumptionthatothersmaynotberationalenough,whilethosecharacterized
13
rebmun
detropeR
rebmun
detropeR
rebmun
detropeR80 80 80
60 60 60
40 40 40
20 20 20
0 0 0
Professor Economists Psychologists Athletes Artists Writers Professor Economists Psychologists Athletes Artists Writers Professor Economists Psychologists Athletes Artists Writers
(a) Llama3-70B (b) MistralAI-8x22B (c) Qwen2-72B
Figure 10: The distributions of numbers reported by agents characterized with different occupations.
345 000 000 W L
M
Ql wa ii sn m etn
r
na ae 23 lr
A
-: -
7
7 I1
-
20 85 BB x.9
2
((0 A2A VBV
G
G
(A
13
V
82
G
..
8
9
1
31
9
))
.75)
2233 0505 0000 W L
M
Ql wa ii sn m etn
r
na ae 23 lr
A
-: -
7
7 I1
-
20 80 BB x.3
2
((5 A2A VBV
G
G
(A
11
V
37
G
..
8
5
1
58
5
))
.12)
345 000 000 W L
M
Ql wa ii sn m etn
r
na ae 23 lr
A
-: -
7
7 I6
-
20 8. B1 B x6
2
(( A2A VBV
G
G
(A
89
V
.. 5G4
5
6
9
))
.71)
1 680 000 000 W L
M
Ql wa ii sn m etn
r
na ae 23 lr
A
-: -
7
7 I3
-
20 8. B6 B x7
2
(( A2A VBV
G
G
(A
55
V
.. 1G3
3
0
6
))
.06)
1 680 000 000 W L
M
Ql wa ii sn m etn
r
na ae 23 lr
A
-: -
7
7 I2
-
20 8. B1 B x8
2
(( A2A VBV
G
G
(A
23
V
.. 9G0
9
1
3
))
.79)
12 00 00 7 10 11 505 000 82 604 12 00 00 27 1 13 2 24 00 00 95 96 19 24 00 00 140 11 85 34
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 25 50 75 100 0 25 50 75 100
Reported number Reported number Reported number Reported number Reported number
(a) Round 1 (b) Round 2 (c) Round 3 (d) Round 4 (e) Round 5
Figure 11: Individual-level simulation involving agents with a mixture of LLMs.
as artists might adopt a straightforward strategy.
In summary, in large-scale simulations involving multiple agents, the diversity of agents can be enhanced
by providing different descriptions of their background settings. Serving as examples, we provide simulation
experiments involving agents with different educational levels and occupations, which show that the agents’
behaviors are consistent with their background settings. Meanwhile, we provide empirical usage examples and
case studies of the proposed heterogeneous configurations introduced in Sec. 3.3, enabling users to effortlessly
set up a large number of agents with diverse background settings.
4.6 Mixture of LLMs
In this subsection, we conduct a simulation experiment involving agents employing a mixture of LLMs.
Specifically, we configure agents employing Llama3-70B, MistraAI-8×22B, and Qwen2-72B, with 500 agents
assigned to each LLM. We conduct both individual-level simulations, where each agent plays the game
independently, and group-level simulations, where agents using the same LLMs form a group.
Individual-Level Simulation The simulation results are illustrated in Fig. 11. At the first round of the
game, we observe that agents with Llama3-70B exhibit similar behaviors, tending to report numbers around
33, while agents with MistralAI-8×22B consistently report 0. On the other hand, agents with Qwen2-72B
exhibit more diverse behaviors, reporting a wider range of numbers, with most of them falling between 0
and 50. These behaviors can be attributed to the preferences of the LLMs, which may be related to their
architectures, training corpus, etc.
As the game progresses round by round, agents are informed of the winning number from the previous
round and adjust their strategies accordingly. As shown in Figure 11b, the majority of agents report numbers
close to the winning number in the previous round, with approximately 59.7% reporting numbers smaller
than the previous winning number. We present a typical response in Appendix E.6, where an agent adopts a
conservative strategy and chooses a number slightly smaller than the winner number 15.90.
In the pie chart of Fig. 11, we further show the winners of each round in the simulation, grouped by
their employed LLMs. To reduce the randomness in the simulation, we regard those agents whose reported
numbers fall within the range of ±0.5 from 2 of the average as the winners. The figure shows that in the
3
14
rebmun
detropeR
ycneuqerF ycneuqerF
rebmun
detropeR
ycneuqerF ycneuqerF
rebmun
detropeR
ycneuqerFPrompt 6
Youareplayingamultiplayergame.
#GameRule
1. Thereare3groupsofplayersinthegame.
2. Eachplayerreportsarealnumberbetween0and100,inclusive.
3. Eachgroupreportstheaverageofallplayersinthegroup.
4. Thewinnerwillbethegroupwhosenumberistheclosestto2/3oftheaverageofallgroups’numbers.
5. Youareingroup{id}.
The2/3oftheaverageforthisroundiswinner. ThenumbersreportedbygroupsareGroup1: v1,Group2: v2,Group3: v3.
Let’smoveontothenextround.
Thinkstepbystepandthenreportyournumber.
Round 1 Round 2 Round 3 Round 4 Round 5
100 100 100 100 100
80 80 80 80 80
60 60 60 60 60
40 40 40 40 40
20 20 20 20 20
0 0 0 0 0
Llama3-70 MB istralAI-8x22 QB wen2-72B Llama3-70 MB istralAI-8x22 QB wen2-72B Llama3-70 MB istralAI-8x22 QB wen2-72B Llama3-70 MB istralAI-8x22 QB wen2-72B Llama3-70 MB istralAI-8x22 QB wen2-72B
Figure 12: Group-level simulation involving agents with a mixture of LLMs.
first and fifth rounds, agents equipped with Qwen2-72B outperform other agents, while agents equipped with
MistralAI-8×22B emerge as winners in the second, third, and fourth rounds. Notably, almost all agents tend
to report numbers near 0 in the final round, indicating agents can perform reasonable considerations and
behaviors to promote this game approach to its Nash equilibrium.
Group-Level Simulation In the group-level simulation, agents are divided into three groups. Each agent
reports a number, and the average number among the agents within each group is regarded as the reported
number of this group. Finally, the group that reports a number that is closest to the 2 of the average among
3
the groups’ reported numbers wins the game. The system prompt adopted, which specifies the above game
rules, is shown in Prompt 6.
Meanwhile, we modify the information announced between different rounds in a multi-round game.
Starting from the second round, in addition to the winning number of the previous round, all agents are
informed of the reported numbers from all three groups in the previous round. This provides additional
guidance for agents to adjust their strategies. Such a group-wise synchronization is implemented based on
the agent-environment interaction mechanism (see Sec. 3.2), allowing agents within the same group to share
an interactive environment for synchronization.
The simulation results are shown in Fig. 12. From the figures, it can be observed that agents within the
same group quickly converge to similar behaviors as the game comes to the second round, as indicated by
their reported numbers falling within a narrow range. Agents using Qwen2-72B and Llama3-70B exhibit
relatively consistent behaviors, while some agents using MistralAI-8×22B might exhibit different behaviors,
such as reporting larger numbers. For better understanding, we provide several examples of agents’ behaviors
in Appendix E.7, showing how agents consider the reported numbers from other groups and strategically
choose the reported numbers to benefit their own group. Such a phenomenon confirms that agents can
perform reasonable thoughts and actions to help achieve a collective goal.
4.7 Further Discussions
Inthissubsection,weprovidefurtherdiscussionsonusagetipsandopenquestionswhenconductinglarge-scale
agent-based simulations.
15
rebmun
detropeR
rebmun
detropeR
rebmun
detropeR
rebmun
detropeR
rebmun
detropeR100 100 100
75 75 75
50 50 50
25 25 25
0 0 0
2 1 51 51 2 1 51 51 2 1 51 51
3 2 100 100+note 3 2 100 100+note 3 2 100 100+note
Ratio Ratio Ratio
(a) Llama3-70B (b) MistralAI-8×22B (c) Qwen2-72B
Figure 13: The distributions of reported numbers when setting different ratios in the game. “+note” denotes
that we add a note to the system prompts.
Impact of the Prior Knowledge of LLMs As “guess the 2 of the average” is a classic game, it is not
3
surprising that LLMs might have acquired prior knowledge from their training corpus. To measure the
impact of this prior knowledge, we change the ratio from 2 to 1 and 51 , and then conduct the simulation
3 2 100
experiments, respectively. Note that changing the ratios does not alter the fundamental nature of the game,
and as a result, the behaviors (such as the reasoning process) of the participating agents are expected to
remain similar if the LLMs indeed understand the game.
The experimental results are shown in Fig. 13, from which we can observe that there are significant
differences in the agents’ performance when the ratio is set to 1 and 51 , although both cases should be
2 100
very similar. We observe that more agents tend to report large numbers (e.g., around 50) when the ratio
is 1 compared to the scenario with a ratio of 51 , which indicates that some agents might not follow the
2 100
game when setting the ratio to 51 . In response, we add a note into system prompts to encourage LLMs to
100
draw from the classic game, stating This game is a variation of the famous “guess the 2/3 of the average”
game. The results summarized in Fig. 13, denoted as “ 51 +note”, show that the winning number decreases
100
from 11.85 to 6.46, aligning more closely with that of using the ratio 1, reporting 6.21. These simulation
2
experiments highlight the impact of LLMs’ prior knowledge, and the effects of using a prompt to explicitly
guide the agents and help them understand the settings of the simulation.
Prompt 7
Youareplayingamultiplayergame.
#GameRule
1. Eachplayerreportsarealnumberbetween0and100,inclusive.
2. Thewinnerwillbetheplayerwhosenumberistheclosestto5plus1/2oftheaverageofallreportednumbers.
Thinkstepbystepandthenreportyournumber.
When the Nash equilibrium Is Not 0 Wealsosetupanothervariantofthegametovalidatetheagents’
capabilities in understanding and reasoning. Specifically, we modify the winning criteria so that the Nash
equilibrium becomes 10, instead of 0 as in the classic game. As a result, LLMs might not exhibit reasonable
thoughts and behaviors if they have a limited understanding of the game or poor reasoning ability. The
adopted system prompt is shown in Prompt 7.
Asshowninthe Fig.14, weobservethatsome agents usingtheQwen2-72BandMistralAI-8×22Bareable
to reason out the Nash equilibrium point in the first round. As the game progresses, the numbers reported
by these agents gradually approach 10, demonstrating their understanding of the game and ability to make
reasonable decisions. These findings are further confirmed by our observations of individual-level behaviors,
as detailed in the Appendix E.8.
Besides, from observations on the individual-level behaviors, we can identify some typical mistakes made
by agents. For example, as shown in Appendix E.9, some agents might make simple calculation errors, such
16
rebmun
detropeR
rebmun
detropeR
rebmun
detropeRLlama3-70B MistralAI-8x22B Qwen2-72B
100 100 100
80 80 80
60 60 60
40 40 40
20 20 20
10 10 10
0 0 0
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Round Round Round
Figure14: ThedistributionsofreportednumberswhenwechangetheNashequilibriumto10inthesimulation.
temperature=0.05 temperature=0.25 temperature=0.50 temperature=1.00
AVG: 15.42 MIN: 0.00 AVG: 15.27 MIN: 0.00 AVG: 18.54 MIN: 0.00 AVG: 16.72 MIN: 0.00
STD: 6.50 MAX: 50.00 STD: 7.22 MAX: 45.00 400 STD: 7.74 MAX: 60.00 300 STD: 8.20 MAX: 53.00
400 400
MED: 15.00 MOD: 15.00 MED: 15.00 MOD: 15.00 MED: 16.00 MOD: 15.00 MED: 15.00 MOD: 15.00
200
200 200 200
100
0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number
Figure 15: The distributions of reported numbers when using different temperatures in LLMs.
as calculating a wrong average value, leading to incorrect results despite having a correct reasoning process.
Some agents may follow a logical step-by-step process (e.g., calculating the Nash equilibrium) but still make
incorrect decisions (e.g., directly reporting 0). But even so, these mistakes are infrequent and do not affect
the overall conclusions drawn from group-level observations, as we previously demonstrated.
These observations indicate that while agents may make simple calculation errors (especially in decimal
calculations), they exhibit powerful reasoning abilities and show great potential for use in simulations.
Impact of Temperature The temperature serves as a hyperparameter for controlling the diversity of
LLM generation. In this subsection, we set up 1,000 agents equipped with Qwen2-72B and Prompt 2, and
vary the temperature within the range of {0.05,0.25,0.50,1.00}. The experimental results are presented in
Fig. 15, from which we can observe that as the temperature increases, the standard deviation rises from 6.50
to 8.20 while the average number shows only slight changes. These findings indicate that the generation
temperature can impact the distributions of reported numbers, but may not significantly alter the overall
average when the number of participating agents is large.
Playing as a Seven-Year-Old Child In the simulation involving agents with diverse background settings
(see Sec. 4.5), we attempt to assign an agent with an extreme role: playing as a seven-year-old child in the
game. From the generation of this agent, as shown in Appendix E.5, we can see that the agent’s behaviors
do not align with those expected of a seven-year-old child, as they demonstrate abilities such as performing
calculations and engaging in multi-step reasoning. These findings highlight the importance for users to
evaluatethemodel’scapabilitiesforplayingcertaincharactersandtocarefullydesignthebackgroundsettings
before conducting simulations. Although various background settings can be assigned to agents, they may
not exhibit consistent behaviors in some extreme cases.
5 Conclusions
Inthispaper, wefirstdiscussseveralkeyfactorsofconcernforconductinglarge-scaleagent-basedsimulations,
including scalability and efficiency, population distribution and agent diversity, and ease of management.
Motivated by these factors, we propose and implement several enhancements in AgentScope, including an
actor-based distributed mechanism that provides automatic parallel execution and centralized workflow
17
rebmun
detropeR
ycneuqerF ycneuqerF
rebmun
detropeR
ycneuqerF
rebmun
detropeR
ycneuqerForchestration, the flexible environment support to simulate various real-world scenarios, the heterogeneous
configurations that allow users to specify population distributions and to automatically generate agents with
diverse background settings, and a web-based interface to simplify the management of large-scale agents.
These enhancements make AgentScope more flexible and convenient for supporting large-scale agent-based
simulations. WeconductaseriesofsimulationexperimentswithAgentScopeandprovidedetailedobservations
on the diverse and realistic behaviors of agents, highlighting its great potential to further advance research
and applications in agent-based simulations.
Ethics Statement
This study is concentrated on proposing a novel multi-agent framework designed to assist researchers in
conducting various simulation experiments, drawing valuable behavioral observations and insights to promote
the development of related fields. We are committed to upholding principles of equity and fairness, and
firmly reject any form of prejudicial discrimination based on age, education level, occupation, or any other
characteristics.
18References
Gul A. Agha. Actors: a Model of Concurrent Computation in Distributed Systems (Parallel Processing,
Semantics, Open, Programming Languages, Artificial Intelligence). PhD thesis, University of Michigan,
USA, 1985.
Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav S. Gulavani,
Ramachandran Ramjee, and Alexey Tumanov. Vidur: A large-scale simulation framework for LLM
inference. CoRR, abs/2405.05465, 2024.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey,
Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,
Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan
Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks,
Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery,
Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan
Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian
Gehrmann, Lucas Gonzalez, and et al. Palm 2 technical report. CoRR, abs/2305.10403, 2023.
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin
Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable,
Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh,
Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang,
Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark
Saroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang,
William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng
Wu, and Soumith Chintala. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode
Transformation and Graph Compilation. In ASPLOS. ACM, 2024.
ANTHROP. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024.
Mohammadmehdi Ataei, Hyunmin Cheong, Daniele Grandi, Ye Wang, Nigel Morris, and Alexander
Tessier. Elicitron: An LLM agent-based simulation framework for design requirements elicitation. CoRR,
abs/2404.16045, 2024.
Colin F Camerer, Teck-Hua Ho, and Juin-Kuan Chong. A cognitive hierarchy model of games. The Quarterly
Journal of Economics, 119(3):861–898, 2004.
Myra Cheng, Tiziano Piccardi, and Diyi Yang. Compost: Characterizing and evaluating caricature in LLM
simulations. In EMNLP, pages 10853–10875, 2023.
Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and
3
Yong Li. S : Social-network simulation system with large language model-empowered agents. CoRR,
abs/2307.14984, 2023.
Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang,
Yuexiang Xie, Daoyuan Chen, Liuyi Yao, Hongyi Peng, Zeyu Zhang, Lin Zhu, Chen Cheng, Hongzhu Shi,
Yaliang Li, Bolin Ding, and Jingren Zhou. Agentscope: A flexible yet robust multi-agent platform. CoRR,
abs/2402.14034, 2024.
Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin
Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang,
Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang,
Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi
Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang,
Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao
Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: A
family of large language models from glm-130b to glm-4 all tools, 2024.
19Önder Gürcan. Llm-augmented agent-based modelling for social simulations: Challenges and opportunities.
CoRR, abs/2405.06700, 2024.
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven
Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta
programming for multi-agent collaborative framework. CoRR, abs/2308.00352, 2023.
Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin
Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi
Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu,
and Chenglin Wu. Data interpreter: An LLM agent for data science. CoRR, abs/2402.18679, 2024.
Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue
Zhou, and Jiangtao Gong. Surrealdriver: Designing generative driver agent simulation framework in urban
contexts based on large language model. CoRR, abs/2309.13193, 2023.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gon-
zalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with
pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles,
2023.
langchain ai. Langchain. https://github.com/langchain-ai/langchain, 2024a.
langchain ai. Langsmith. https://docs.smith.langchain.com/, 2024b.
Chenliang Li, He Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou,
Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang, Fei Huang, and Jingren Zhou. Modelscope-agent:
Building your customizable agent system with open-source large language models. In EMNLP (Demos),
pages 566–578, 2023a.
GuohaoLi, HasanHammoud, HaniItani, DmitriiKhizbullin, andBernardGhanem. CAMEL:communicative
agents for "mind" exploration of large language model society. In NeurIPS, 2023b.
Charles M. Macal. Everything you need to know about agent-based modelling and simulation. J. Simulation,
10(2):144–156, 2016.
Charles M. Macal and Michael J. North. Tutorial on agent-based modelling and simulation. J. Simulation, 4
(3):151–162, 2010.
Takashi Matsumoto, Yurie Kibi, and Tetsuro Kondo. Purpose model simulation - purpose formation of
multi-stakeholder by dialog with llm-based AI. In HCI, volume 14718, pages 110–129, 2024.
Meta. Introducing meta llama3: The most capable openly available llm to date. https://ai.meta.com/
blog/meta-llama-3/, 2024.
MistralAI. Bienvenue to mistral ai documentation. https://docs.mistral.ai/getting-started/models/,
2024.
Xinyi Mou, Zhongyu Wei, and Xuanjing Huang. Unveiling the truth and facilitating change: Towards
agent-based large-scale social movement simulation. CoRR, abs/2402.16333, 2024.
Rosemarie Nagel. Unraveling in guessing games: An experimental study. The American Economic Review,
85(5):1313–1326, 1995.
OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.
Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S.
Bernstein. Social simulacra: Creating populated prototypes for social computing systems. In UIST, pages
74:1–74:18. ACM, 2022.
20Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S.
Bernstein. Generative agents: Interactive simulacra of human behavior. In UIST, pages 2:1–2:22, 2023.
Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng
Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for
software development. CoRR, abs/2307.07924, 2023.
Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Hua Wu, Ji-Rong Wen, and Haifeng
Wang. BASES: large-scale web search user simulation with large language model based agents. CoRR,
abs/2402.17505, 2024.
Significant-Gravitas. Autogpt. https://github.com/Significant-Gravitas/AutoGPT, 2023.
Aleksandra Sorokovikova, Natalia Fedorova, Sharwin Rezagholi, and Ivan P. Yamshchikov. Llms simulate big
five personality traits: Further evidence. CoRR, abs/2402.01765, 2024.
Karthik Sreedhar and Lydia B. Chilton. Simulating human strategic behavior: Comparing single and
multi-agent llms. CoRR, abs/2402.08189, 2024.
SIMA Team, Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton,
Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune,
Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Dario de Cesare, Julia Di Trapani, Yani
Donchev, Emma Dunleavy, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao
Gong, Lucy Gonzalez, Kshitij Gupta, Karol Gregor, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix
Hill,EdHirst,DrewA.Hudson,JonyHudson,StephHughes-Fitt,DaniloJ.Rezende,MimiJasarevic,Laura
Kampis, Nan Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Andrew K.
Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph
Marino, Kathryn Martin Cussons, Loic Matthey, Siobhan Mcloughlin, Piermaria Mendolicchio, Hamza
Merzic, AnnaMitenkova, AlexandreMoufarek, ValériaOliveira, YankoGitahyOliveira, HannahOpenshaw,
Renke Pan, Aneesh Pappu, Alex Platonov, Ollie Purkiss, David P. Reichert, John Reid, Pierre Harvey
Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. Sawyer, Tim
Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam,
Denis Teplyashin, Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang,
ZhengdongWang,DaanWierstra,DuncanWilliams,NathanielWong,SarahYork,andNickYoung. Scaling
instructable agents across many simulated worlds. CoRR, abs/2404.10179, 2024.
XAgent Team. Xagent: An autonomous agent for complex task solving, 2023.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information
processing systems, 35:24824–24837, 2022.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In EMNLP
(Demos), pages 38–45, 2020.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,
XiaoyunZhang,andChiWang. Autogen: Enablingnext-genLLMapplicationsviamulti-agentconversation
framework. CoRR, abs/2308.08155, 2023.
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian
Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou
Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei
21Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie,
Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong
Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li,
Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale
language models. CoRR, abs/2309.10305, 2023.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,
Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,
Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin,
Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang,
Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao
Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei,
Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, and
Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024.
Frank Tian-fang Ye and Xiaozi Gao. Simulating family conversations using llms: Demonstration of parenting
styles. CoRR, abs/2403.06144, 2024.
Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, and Ziyu Yao. Mathvc: An llm-simulated
multi-character virtual classroom for mathematics education. CoRR, abs/2404.06711, 2024.
Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xiangbo
Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, and Haizhou Li. Huatuogpt, towards taming
language model to be a doctor. In EMNLP (Findings), pages 10859–10885, 2023.
22A Example of Configuration File
In Listing 1, we show an example of the configuration file for a group of people with different educational
levels.
1 # The high level parameters
2 population: 1000
3
4 # distribution configuration
5 distributions:
6 - name: "Education Level"
7 categories:
8 - name: "Elementary School"
9 proportion: 0.2
10 - name: "High School"
11 proportion: 0.2
12 - name: "Bachelor’s Degree"
13 proportion: 0.2
14 - name: "Master’s Degree"
15 proportion: 0.2
16 - name: "Ph.D."
17 proportion: 0.2
18
19 - name: "Gender"
20 categories:
21 - name: "Male"
22 proportion: 0.5
23 - name: "Female"
24 proportion: 0.5
25
26 # ...
Listing 1: Example of configuration file for people with different educational level.
B Meta Prompt for Generating Heterogeneous Background Settings
The meta prompt used for generating heterogeneous background settings can be found in Prompt 8.
Prompt 8
Youneedtogenerateaperson’sbackgrounddescriptionbasedontheuser-providedJSONformatinformation.
Inadditiontotheinformationprovidedbytheuser,eachbackgrounddescriptionmustalsoincludetheperson’sname,age,
gender,job,andaparagraphdescribingthecharacter’spersonality.
Pleaseoutputthebackgrounddescriptionafter"##Background"tag.
{JSON}
C The Number of Response Tokens
The statistics of the response tokens of agents when using different LLMs and system prompts can be found
in Fig. 16.
D Responses of Agents with Different LLMs and System Prompts
The distributions of the reported numbers of agents with different LLMs and system prompts are shown
in Fig. 17, which includes the average (AVG), minimum (MIN), maximum (MAX), standard deviation (STD),
23AVG: 1.00 AVG: 155.46 AVG: 190.42 AVG: 260.80
1 2 3 0 200 400 0 200 400 600 200 400 600
Number of Tokens Number of Tokens Number of Tokens Number of Tokens
(a) Llama3-8B & Prompt 1 (b) Llama3-8B & Prompt 2 (c) Llama3-8B & Prompt 3 (d) Llama3-8B & Prompt 4
AVG: 1.04 AVG: 248.69 AVG: 381.36 AVG: 408.40
2 4 6 100 200 300 400 200 400 600 800 400 600
Number of Tokens Number of Tokens Number of Tokens Number of Tokens
(e) Llama3-70B & Prompt 1(f) Llama3-70B & Prompt 2(g) Llama3-70B & Prompt 3(h)Llama3-70B&Prompt4
Figure 16: The response tokens when using different LLMs and system prompts.
median (MED), and mode (MOD). Besides, in Fig. 18, Fig. 19, and Fig. 20, we report the distributions of
the reported numbers of agents with different LLMs and prompts in a multi-round game.
E Case Studies
In this section, we provide individual-level case studies for a better understanding of how agents perform
considerations and reasoning.
E.1 Case Studies on Agents with Different LLMs and System Prompts
For better understanding, we provide examples of the responses of different LLMs with Prompt 1 and
Prompt 2, as shown in Example 1 and Example 2, respectively.
Example 1: Responses of agents employing different LLMs with Prompt 1
Llama3-8B:43
Llama3-70B:50
Qwen2-7B:75
Qwen2-72B:50
MistralAI-8x7B:34
(Thisnumberisbasedonstrategicestimationandthedesiretowinthegame,consideringthegivenrule.)
MistralAI-8x22B:Basedontherulesyou’veprovided,mystrategywouldbetopredictwhatnumbersotherplayersmight
reportandthencalculatetheaverageaccordingly. However,sinceIcan’tinteractwithotherplayersorknowtheirstrategies,
I’llmakeaninformedguess. Ifallplayersreportanumbercloseto50,theaveragewouldbearound50,and2/3ofthatwould
beapproximately33.33. So,Iwouldreport34tomaximizemychancesofbeingtheclosest.
E.2 Case Studies on Agent Behaviors in A Multi-round Game
In a multi-round game, we observe that agents can consider the winner number from the previous round
when reporting numbers in the current round. For example, as shown in Example 3, some agents tend to
report a number slightly smaller than the winner number, while others choose to report 2 of the winning
3
number from the previous round, as illustrated in Example 4.
24
ycneuqerF
ycneuqerF
0001057
005
052
0
0001057
005
052
0
ycneuqerF
ycneuqerF
01
5
0
01
5
0
ycneuqerF
ycneuqerF
01
5
0
01
5
0
ycneuqerF
ycneuqerF
01
5
0
01
5
0E.3 Case Studies on Detailed Instructions in System Prompts
Several case studies shown in Example 5 and Example 6 confirm the effectiveness of providing detailed
instructions in system prompts. These instructions encourage agents to perform reasonable behaviors by
guiding them to regard other agents as rational players (as instructed by Prompt 3) and to guess other agents’
strategies (as instructed by Prompt 4).
E.4 Case Studies on Generated Background Settings
We provide two generated background settings for agents, including a character who has a bachelor’s degree
( Example 7) and a character who is an artist ( Example 8). We use GPT-4o to generate these diverse yet
detailed background settings.
E.5 Case Studies on Agents Characterized with Different Background Settings
We provide case studies on individual-level behaviors when providing detailed yet diverse background settings.
For example, we show the responses of agents characterized as an elementary school student in Example 9
and a character who gets the Ph.D. degree in Example 10. From these cases, we observe that agents can
effectively perform reasoning processes and make decisions according to their assigned roles.
Besides, as shown in Example 11, we provide the profile of a professor in game theory to agents, and
find that the agent tends to assume that others are not rational enough and adjust its behavior accordingly.
However, we notice that agents might exhibit inconsistent behaviors when asked to play as a seven-year-old
child, as shown in Example 12.
E.6 Case Studies on Individual-level Simulations Involving Agents with A
Mixture of LLMs
Example 13 shows a typical response in the individual-level simulation involving a mixture of LLMs, where
an agent report chooses a number slightly smaller than the winner number in the previous round.
E.7 Case Studies on Group-level Simulations Involving Agents with A Mixture
of LLMs
We provide three examples in Example 14, Example 15, and Example 16 to show the behaviors of agents in
different groups. We observe that agents can perform reasoning processes considering the behaviors of other
groups and how to contribute to their own group.
E.8 Case Studies on Agent Behaviors When Changing the Game Rules
In the variant game where we change the Nash equilibrium to 10, we observe that agents can give a good
understanding of the game and correctly calculate the Nash equilibrium, as shown in Example 17, indicating
their powerful reasoning ability.
E.9 Case Studies on Mistakes in Agent Behaviors
We summarize some typical mistakes made by agents, including simple calculation errors, as shown in Exam-
ple18, andmakingdecisionsthatareinconsistentwiththeconsiderations, asshowninExample19. Although
agents might make mistakes, they exhibit powerful reasoning abilities and show great potential for use in
simulations.
25Llama3-8B Prompt 1 Llama3-8B Prompt 2 Llama3-8B Prompt 3 Llama3-8B Prompt 4
600 A SV TDG :: 15 75 .. 78 29M MI AN X: : 1 94 3.0 .00 0 300 A SV TDG :: 14 49 .. 98 95M MI AN X: : 0 1.0 00 0.00 300 A SV TDG :: 14 37 .. 78 24M MI AN X: : 0 1.0 00 0.00 A SV TDG :: 14 47 .. 20 69M MI AN X: : 0 9.0 20 .00
MED: 43.00MOD: 43.00 MED: 50.00MOD: 42.00 MED: 50.00MOD: 50.00 100 MED: 47.00MOD: 67.00
400 200 200
50
200 100 100
0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number
Llama3-70B Prompt 1 Llama3-70B Prompt 2 Llama3-70B Prompt 3 Llama3-70B Prompt 4
1500 AVG: 50.30MIN: 33.33 AVG: 34.18MIN: 0.00 AVG: 36.11MIN: 0.00 AVG: 17.15MIN: 0.00
STD: 2.79 MAX: 67.00 600 STD: 9.98 MAX: 100.00 400 STD: 22.06MAX: 100.00 600 STD: 20.21MAX: 100.00
MED: 50.00MOD: 50.00 MED: 33.00MOD: 33.00 MED: 33.33MOD: 33.00 MED: 10.00MOD: 0.00
1000
400 400
200
500 200 200
0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number
MistralAI-8x7B Prompt 1 MistralAI-8x7B Prompt 2 MistralAI-8x7B Prompt 3 MistralAI-8x7B Prompt 4
AVG: 36.63MIN: 0.00 AVG: 37.57MIN: 0.00 AVG: 35.19MIN: 0.00 AVG: 24.42MIN: 0.00
400 STD: 15.34MAX: 100.00 300 STD: 19.17MAX: 100.00 STD: 25.19MAX: 100.00 STD: 27.19MAX: 100.00
MED: 34.00MOD: 34.00 MED: 33.32MOD: 33.00 200 MED: 33.00MOD: 33.00 400 MED: 20.00MOD: 0.00
300
200
200 100 200
100
100
0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number
MistralAI-8x22B Prompt 1 MistralAI-8x22B Prompt 2 MistralAI-8x22B Prompt 3 MistralAI-8x22B Prompt 4
800 AVG: 31.69MIN: 0.00 AVG: 19.89MIN: 0.00 1000 AVG: 7.62 MIN: 0.00 1000 AVG: 7.94 MIN: 0.00
STD: 8.25 MAX: 66.67 400 STD: 20.62MAX: 100.00 STD: 16.92MAX: 100.00 STD: 16.95MAX: 99.00
600 MED: 33.33MOD: 33.33 300 MED: 20.00MOD: 0.00 750 MED: 0.01 MOD: 0.00 750 MED: 0.01 MOD: 0.00
400 200 500 500
200 100 250 250
0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number
Qwen2-7B Prompt 1 Qwen2-7B Prompt 2 Qwen2-7B Prompt 3 Qwen2-7B Prompt 4
1000 150
AVG: 66.96MIN: 30.00 150 AVG: 32.57MIN: 0.00 AVG: 26.45MIN: 0.00 AVG: 28.38MIN: 0.00
750 STD: 10.95MAX: 85.00 STD: 19.49MAX: 100.00 150 STD: 20.25MAX: 100.00 STD: 21.02MAX: 100.00
MED: 75.00MOD: 75.00 100 MED: 33.00MOD: 33.00 MED: 25.00MOD: 33.00 100 MED: 28.00MOD: 33.00
500 100
250 50 50 50
0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number
Qwen2-72B Prompt 1 Qwen2-72B Prompt 2 Qwen2-72B Prompt 3 Qwen2-72B Prompt 4
AVG: 50.56MIN: 10.00 AVG: 19.15MIN: 0.00 AVG: 8.15MIN: 0.00 AVG: 8.85MIN: 0.00
STD: 2.52 MAX: 55.60 200 STD: 8.39 MAX: 68.00 400 STD: 8.47MAX: 44.44 300 STD: 8.08MAX: 44.44 1000 MED: 50.00MOD: 50.00 MED: 19.00MOD: 15.00 MED: 6.85MOD: 10.00 MED: 9.87MOD: 10.00
300
200
500 100 200
100
100
0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number
Figure 17: The distributions of the reported numbers of agents with different LLMs and prompts. The
horizontal axis represents the reported numbers, and the vertical axis represents the frequency of occurrence
of each number. The blue bars represent the distributions of reported numbers, while an orange dashed line
indicates the winning number.
26
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerF
ycneuqerFRound 1 Round 2 Round 3 Round 4 Round 5
1500 AVG: 50.30MIN: 33.33 800 AVG: 56.71MIN: 33.00 800 AVG: 39.16MIN: 25.20 1000 AVG: 25.59MIN: 17.34 800 AVG: 16.80MIN: 11.37
1000 S MT ED D: : 2 5. 07 .9 00M MA OX D: : 6 57 0. .0 00 0 600 S MT ED D: : 1 65 6. .3 85 0M MA OX D: : 6 67 7. .5 00 0 600 S MT ED D: : 1 4. 04 .9 00M MA OX D: : 5 40 0. .5 04 0 750 S MT ED D: : 1 2. 54 .7 00M MA OX D: : 4 20 5. .0 00 0 600 S MT ED D: : 1 1. 62 .3 70M MA OX D: : 2 15 6. .0 00 0
400 400 500 400
500
200 200 250 200
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
AVG: 34.18MIN: 0.00 300 AVG: 23.37MIN: 6.75 AVG: 14.00MIN: 4.62 AVG: 7.90MIN: 2.00 800 AVG: 4.19MIN: 0.10
600 STD: 9.98 MAX: 100.00 STD: 4.11 MAX: 50.00 300 STD: 2.51 MAX: 29.00 400 STD: 1.58MAX: 17.00 STD: 0.95MAX: 9.00
MED: 33.00MOD: 33.00 MED: 22.67MOD: 25.00 MED: 14.00MOD: 12.00 MED: 8.00MOD: 7.00 600 MED: 4.00MOD: 4.00
200
400 200
400
200
200 100 100 200
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
600 800
AVG: 36.11MIN: 0.00 AVG: 22.59MIN: 0.00 AVG: 12.50MIN: 0.00 AVG: 6.48MIN: 0.00 AVG: 3.15MIN: 0.00
400 S MT ED D: : 2 32 3. .0 36 3M MA OX D: : 1 30 30 .0.0 00 200 S MT ED D: : 5 2. 25 .7 50M MA OX D: : 6 25 2. .0 00 0 34 00 00 S MT ED D: : 2 1. 28 .4 50M MA OX D: : 3 16 2. .0 00 0 400 S MT ED D: : 1 6. .6 41 5M MA OX D: : 2 62 .0.0 00 600 S MT ED D: : 0 3. .9 01 0M MA OX D: : 1 33 .0.0 00
400
200 100 200 200
100 200
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
AVG: 17.15MIN: 0.00 200 AVG: 12.65MIN: 0.00 AVG: 7.38MIN: 0.00 AVG: 3.49MIN: 0.00 1000 AVG: 1.48MIN: 0.00
600 STD: 20.21MAX: 100.00 STD: 7.01 MAX: 72.00 STD: 3.17MAX: 55.00 600 STD: 1.66MAX: 29.00 STD: 0.69MAX: 8.00
MED: 10.00MOD: 0.00 150 MED: 11.00MOD: 9.50 400 MED: 7.00MOD: 7.00 MED: 3.50MOD: 3.50 750 MED: 1.50MOD: 1.50
400 400
100 500
200
200 50 200 250
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
Figure 18: The distributions of the reported numbers of agents with Llama3-70B in a multi-round game.
Round 1 Round 2 Round 3 Round 4 Round 5
800 A SV TDG :: 83 .1 2. 569M MI AN X: : 0 6.0 60 .67 200 A SV TDG :: 12 27 .. 14 37M MI AN X: : 0 1.0 00 0.00 150 A SV TDG :: 12 22 .. 11 66M MI AN X: : 0 9.0 10 .11 150 A SV TDG :: 11 59 .. 45 08M MI AN X: : 0 9.0 90 .00 150 A SV TDG :: 11 67 .. 17 65M MI AN X: : 0 1.0 00 0.00
600 MED: 33.33MOD: 33.33 150 MED: 23.22MOD: 21.00 MED: 18.96MOD: 12.00 100 MED: 14.29MOD: 14.00 MED: 13.00MOD: 13.00
400 100 100 100
200 50 50 50 50
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
400 600
400 A SV TDG :: 21 09 .. 68 29M MI AN X: : 0 1.0 00 0.00 A SV TDG :: 11 03 .. 09 94M MI AN X: : 0 1.0 00 0.00 300 A SV TDG :: 79 .. 33 62M MI AN X: : 0 1.0 00 0.00 A SV TDG :: 76 .. 07 88M MI AN X: : 0 1.0 00 0.00 A SV TDG :: 85 .. 33 84M MI AN X: : 0 1.0 00 0.00
300 MED: 20.00MOD: 0.00 200 MED: 12.80MOD: 10.00 200 MED: 8.50MOD: 9.00 300 MED: 5.98MOD: 5.00 400 MED: 4.00MOD: 4.00
200
200
100 100 100 100 200
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
1000 AVG: 7.62 MIN: 0.00 300 AVG: 6.02MIN: 0.00 AVG: 4.53MIN: 0.00 600 AVG: 3.47MIN: 0.00 AVG: 2.53MIN: 0.00
STD: 16.92MAX: 100.00 STD: 9.91MAX: 83.00 400 STD: 7.67MAX: 79.00 STD: 6.68MAX: 78.00 600 STD: 6.36MAX: 80.00
750 MED: 0.01MOD: 0.00 200 MED: 3.50MOD: 0.00 MED: 3.00MOD: 3.00 400 MED: 2.46MOD: 2.00 400 MED: 1.50MOD: 1.00
500 200
250 100 200 200
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
600
1000 A SV TDG :: 17 6.9 .94 5M MI AN X: : 0 9.0 90 .00 A SV TDG :: 96 .. 64 44M MI AN X: : 0 8.0 00 .00 400 A SV TDG :: 84 .. 08 69M MI AN X: : 0 9.0 00 .00 A SV TDG :: 63 .. 85 16M MI AN X: : 0 9.0 90 .00 600 A SV TDG :: 62 .. 56 82M MI AN X: : 0 1.0 00 0.00
750 MED: 0.01MOD: 0.00 200 MED: 4.00MOD: 0.00 300 MED: 3.00MOD: 3.00 400 MED: 2.25MOD: 2.00 MED: 1.50MOD: 1.00
400
500 200
250 100 100 200 200
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
Figure 19: The distributions of the reported numbers of agents with MistralAI-8×22B in a multi-round game.
27
1 tpmorP
3 tpmorP
4 tpmorP
2
tpmorP
3 tpmorP
4
tpmorP
1 tpmorP
2 tpmorPRound 1 Round 2 Round 3 Round 4 Round 5
AVG: 50.56MIN: 10.00 AVG: 42.94MIN: 22.00 800 AVG: 35.30MIN: 18.60 AVG: 29.95MIN: 15.00 AVG: 25.16MIN: 12.80
STD: 2.52 MAX: 55.60 1000 STD: 2.77 MAX: 66.70 STD: 1.90 MAX: 40.00 1000 STD: 1.76 MAX: 36.00 600 STD: 1.78 MAX: 32.00
1000 MED: 50.00MOD: 50.00 MED: 44.00MOD: 44.00 600 MED: 35.00MOD: 35.00 MED: 30.00MOD: 30.00 MED: 25.00MOD: 25.00
400
500 500 400 500
200 200
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
800
AVG: 19.15MIN: 0.00 300 AVG: 11.06MIN: 1.95 AVG: 6.11MIN: 0.88 AVG: 3.43MIN: 0.60 AVG: 2.02MIN: 0.21
200 S MT ED D: : 8 1. 93 .9 00M MA OX D: : 6 18 5. .0 00 0 S MT ED D: : 2 1. 09 .5 00M MA OX D: : 2 17 0. .5 00 0 400 S MT ED D: : 1 6. .5 03 0M MA OX D: : 1 55 .0.0 00 800 S MT ED D: : 1 3. .0 04 0M MA OX D: : 1 32 .0.8 04 600 S MT ED D: : 0 2. .7 05 0M MA OX D: : 1 21 .0.5 04
200 600
400
100 200 400
100 200 200
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
400 A SV TDG :: 88 .. 41 75M MI AN X: : 0 4.0 40 .44 600 A SV TDG :: 24 .. 00 44M MI AN X: : 0 1.0 80 .00 1000 A SV TDG :: 01 .. 85 05M MI AN X: : 0 6.0 .00 0 1000 A SV TDG :: 00 .. 47 66M MI AN X: : 0 4.0 .00 0 1500 A SV TDG :: 00 .. 51 94M MI AN X: : 0 1.0 60 .00
300 MED: 6.85MOD: 10.00 400 MED: 3.50MOD: 3.00 750 MED: 1.33MOD: 1.00 MED: 0.70MOD: 0.50 1000 MED: 0.01MOD: 0.00
200 500 500
200 500
100 250
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
1500
AVG: 8.85MIN: 0.00 AVG: 4.44MIN: 0.00 1000 AVG: 1.69MIN: 0.00 AVG: 0.84MIN: 0.00 AVG: 0.15MIN: 0.00
300 STD: 8.08MAX: 44.44 STD: 2.52MAX: 38.00 STD: 1.14MAX: 23.00 1000 STD: 0.73MAX: 17.00 STD: 0.35MAX: 3.00
MED: 9.87MOD: 10.00 400 MED: 4.00MOD: 3.00 750 MED: 1.50MOD: 1.00 750 MED: 0.75MOD: 0.50 1000 MED: 0.01MOD: 0.01
200
500 500
200 500
100 250 250
0 0 0 0 0
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Reported number Reported number Reported number Reported number Reported number
Figure 20: The distributions of the reported numbers of agents with Qwen2-72B in a multi-round game.
28
1
tpmorP
2
tpmorP
3 tpmorP
4 tpmorPExample 2: Responses of agents employing different LLMs with Prompt 2
MistralAI-8x22B: This is a classic game theory problem known as the "Two-Thirds of the Average Game". The Nash
equilibriumstrategyforthisgameistochoose0.
Letmeexplainstepbystep:
1. Supposetheotherplayersfollowthestrategyofchoosing2/3oftheaverageofallreportednumbers.
2. Ifyouchooseanynumberlargerthan0,forexample,supposeyouchoose’x’(0<x<=100).
3. Becausetheotherplayerschoose2/3oftheaverage,theaverageofallreportednumberswillbeslightlylargerthan2/3of
theaverage. Therefore,thetruevaluetheplayershouldbeaimingforisnot2/3oftheaverage,butslightlylower.
4. Thisindicatesthatthenumber’x’cannotbetheoptimalstrategyfortheplayer,becausethetruevaluetheplayershould
aimforisslightlylowerthan2/3oftheaverage.
5. Hence,theoptimalstrategyforaplayeristochoose0,sincethisistheonlynumberthatwillmaketheplayer’snumber
equalto2/3oftheaverage.
So,Iwillreportmynumberas0.
Llama3-70B:Let’sthinkthisthroughstepbystep.
*Step1: Understandthegame**
Thegameaskseachplayertoreportarealnumberbetween0and100,inclusive. Thewinneristheplayerwhosenumberis
closestto2/3oftheaverageofallreportednumbers.
*Step2: Analyzethegamestructure**
Thegamehasasimilarstructuretothe"Guess2/3oftheaverage"game,aclassicexampleofagamethatexhibitsaNash
equilibrium. Inthistypeofgame,theoptimalstrategyistoguessthe2/3oftheaverageoftheexpectednumbers.
*Step3: Estimatetheexpectednumbers**
Sinceeachplayercanreportanynumberbetween0and100,areasonableestimateoftheexpectednumbersisthemidpointof
therange,whichis50.
*Step4: Calculatetheaverageoftheexpectednumbers**
Assumingallplayersreportnumbersaround50,theaverageofthereportednumberswillbecloseto50.
*Step5: Calculate2/3oftheaverage**
2/3of50isapproximately33.33.
*Step6: Reporttheoptimalnumber**
Basedontheanalysis,Iwillreportthenumberthatisclosestto33.33,whichis**33**.
So,myreportednumberis**33**.
Qwen2-72B:Tothinkstepbystepthroughthelogicofthisgame,weneedtoconsiderhowotherplayersmightthinkas
well. Thisproblemisaclassicexampleofagametheoryscenarioknownasthe"2/3ofaveragegame"or"Guess2/3ofthe
Average."
1. **FirstLevelThinking**: Ifplayersthinkpurelyrandomlyorwithoutregardforstrategy,theymightpickanynumber
between0and100. Let’sassumetheaveragenumberpickedbysuchplayerswouldbe50.
2. **SecondLevelThinking**: Ifaplayerassumesmostplayersarethinkingatthefirstlevelandpicks50,thenthesmart
strategyistopick2/3of50,whichis33.33.
3. **ThirdLevelThinking**: Ifplayersthinkthemajoritywillthinkof33.33,thenchoosing2/3of33.33,whichisroughly
22.22,mightwin.
4. **FourthLevelThinking**: Continuingthisargument,playersmightanticipateotherstopick22.22andchoose2/3of
22.22,whichisabout14.81.
Thisprocesscouldtheoreticallycontinueonandon, witheachlevelofthinkingreducingtheanticipatedvaluetopickby
around33%(2/3).
However, in a real game scenario, we can’t expect everyone to think to the same level of depth, and the complexity of
thesecalculationscanquicklyprovidediminishingreturns. Thus,inpractice,theiterationshownaboveistypicallywherea
theoreticalconvergencestartstohappen.
Giventheseconsiderations,areasonablechoicetomakeinthisgame,assumingmanyplayersthinksimilarlytothisleveland
youwanttooutthinkthem,couldbetopickaround15. However,togiveamorespecificnumber,let’sroundtoavoidgiving
exacttheoreticalvaluesthatothersmightalsosettleon.
*Iwillreport: 14.**
Thisnumberassumesalevelofcollectivehigher-orderthinkingamongplayersandaimsforavaluejustundercommonround
numberslike15,inanefforttobeslightlyunpredictablewhilestillapplyingstrategicthought.
29Example 3: Agent behavior: reporting a number slightly smaller than the winning number in the previous
round
Giventheoutcomeofthepreviousround,it’sclearthatthewinningnumberwasclosetothepredictionmadeintheinitial
roundofthinkingthroughseverallevelsof2/3averages. Thewinnernumberof12.77suggeststhatotherplayersalsoemployed
amulti-layeredstrategyofrationalthinking,pushingthe2/3oftheaveragedowntoalowerrange.
###StrategyReview:
-Inthepreviousround,Ichose13basedontheassumptionthatotherplayerswouldalsoengageinseverallevelsofrecursion,
anticipatingthattheselectionwouldconvergetowardsanumberinthelowteens.
-Theactualwinningnumberwas12.77,aligningcloselywiththisstrategy.
###AdjustingStrategyfortheNextRound:
-Thegamedynamicssuggestthattheplayersmaycontinuetoemploysimilarlevelsofstrategicthinking,aimingfornumbers
aroundthesamerange.
-Torefinethestrategy:
-Wemustacknowledgethatthegroup’srationallevelseemsquiteadvanced,giventheconvergencetowardsthelowteens.
-Wealsoassumethatplayerswillnoticethepatternandmayadjusttheirnextnumbersbasedonthisoutcome.
###PredictingtheNextOutcome:
- If all players think that the last round’s average had a significant impact, there might be a slight adjustment to-
wardshigherorlowernumbersbasedonfeedbacklearning.
-Assumingplayersunderstandandtrustthepattern,theymayconvergeslightlyhigherorstayveryclosetotheprevious
level,expectingasimilargamestate.
###FinalDecision:
Given the feedback from the most recent game state, I will aim for a number close to 12.77, but with a slight ad-
justmenttoavoiddirectmatchandaccountforanysmallbehavioralshiftsinotherplayers.
**NewReportedNumberfortheNextRound: 12.7**
This number aims to stay within the revealed strategic range, while making a conservative adjustment based on
the observational feedback from the last round. It attempts to balance the possibility of convergence and the need to
differentiatefromthemostrecentwinningnumber,whichmayinfluenceplayers’decisionsinthisround.
Example 4: Agent behavior: reporting 2 of the winning number from the previous round
3
Giventhewinnernumberfromthelastroundwas12.77,wecaninferafewpoints:
1. When2/3oftheaverageledto12.77,itimpliestheaggregateaveragemusthavebeenslightlyunder19.155(since12.77is
approximately2/3of19.155).
2. It seems that players chose numbers that led to this average, suggesting a collective reasoning pattern typical in the
CentipedeGameorrelatedgames—playersiterativelyreducetheirnumberbasedontheassumptionthatotherswillalso
reducetheirnumbersbasedonthelogicabove.
Continuing from 14.8 in the last round, taking into account this feedback and assuming players will further lower their
expectationsafterseeingtheaverageresult, onemightconsiderreducingtheguessbyanotheriteration. I’llsimulateone
furtherlevelofreductiontofindmynextguess:
- If players previously aimed for around 19.155 (which led to the average and then the winning guess of 12.77
whenadjustedby2/3),applyingthesamelogicwouldfirstestimatethenextmiddlegroundbyassumingplayerswilladjust
downtoalignwith12.77asthenewreferencepoint.
-Taking2/3of12.77yieldsapproximately8.51(2/3*12.77≈8.51).
Giventhispotentiallyiterativemechanism,andthusexpectingotherplayersmightalsothinkinthisdirection,Iwilluse8.51
asthenewcentralguesspoint,informedbythewinnernumberofthepreviousroundandthehypothesisthatthecollective
reasoningwouldcontinueonasimilarpath.
Myreportednumberis8.51.
30Example 5: Agent behavior: regarding agents as rational players
Thisgameisaclassicexampleofareverseinductiongame. Here’sthelogicbehindtherationalchoice:
1. Ifallplayersthinkcompletelyrationallyandchoosenumbersuniformlyatrandom,theaveragewouldbe50.
2. However,knowingthatotherrationalplayerscanalsothinkthisway,onewouldrealizethatifeveryonepicks50,thetarget
toaimforis2/3of50,whichisapproximately33.33.
3. Butifeveryonethenadjuststheirnumbertoaround33.33,therationalchoicewouldthenbe2/3of33.33,whichisabout22.22.
4. Continuingthislineofreason,thisiterativeprocessdrivestheoptimalchoicetowardszero. However,theproblemnuances
thatitcan’tgobelowzerosinceit’sboundbetween0and100.
Consideringthis,andifweassumethegamehasbeenrepeatedindefinitelyintheplayers’imaginations,thelogicalsteady-state
answerwouldbe0,asitisthelimitpointoftheiterative2/3adjustmentprocessfromthestartingpointof50.
GiventhatIamplayingthisgameandassumingallparticipantsarerationalandhavereachedasimilarconclusion,reporting
0seemstobeastrategicdecision.
**Therefore,Ireportmynumberas0.**
Example 6: Agent behavior: guessing other agents’ strategies
Tosolvethisproblem,let’sconsiderthethoughtprocessofeachrationalplayerinvolved. Sinceallplayersarerational,they
willassumethateveryonewillfollowthesamelineofreasoning.
1stLevelThinking:
At the first level of thinking, a player might assume that everyone will choose numbers at random between 0 and 100.
Therefore,theaverageofallreportednumberswouldbeapproximately50. Followingtherule,choosing2/3of50(or33.33)
wouldbeoptimal.
2ndLevelThinking:
Now,recognizingthatallplayersarerational,onemightassumethateveryonewillthinkaboutthefirstlevelofthoughtand
hencechoose33.33. So,thesecondlevelofthinkingwouldsuggesttochoose2/3of33.33(orapproximately22.22).
3rdLevelThinking:
Inthesamevein,ifeveryoneisthinkingonthesecondlevelandwillchooseabout22.22,then2/3ofthatwouldleadyouto
chooseabout14.81.
Thisprocessofiterativelycalculating2/3ofthepreviousaveragecanbecontinuedtoreachanumberthatallrationalplayers
mightconvergeon. Thisiterativeprocessapproachesalimit,whichcanbedeterminedmathematically.
Let’sdenotethenumberwe’retryingtofindasX.Theaverageinitiallyis50,whichleadsus:
X=(2/3)*50
Eachsubsequentiterationusesthepreviousiteration’sresult:
X=(2/3)*X
SolvingforXintheequationX=(2/3)*XleadsustotheresultthatXapproaches0asthenumberofiterationsgoesto
infinity. However,there’saminimumnumberthatcanbechosengiventheproblemsetup,sotheoptimalnumberarational
playermightchoose,undertheassumptionthateveryoneisthinkingsimilarlyandconvergingtothislimit,wouldbecloseto
zero,butthegamerulesindicatethatthenumberisbetween0and100anditisrationaltoassumethatnotallplayerswill
reachthislaststepofthought.
Given the recursion and the rational view of all players involved, a number very close to zero is the most rational choice
a player can make, albeit with a minimum margin to allow for slight deviations from pure logic in reality and as to not
necessarilyreport0exactly,whichmightbechosenbyotherplayersleadingtoatieorunpredictedresult.
Let’ssayI’llchooseanumberslightlyabovezero,suchas0.1,assumingallplayersaregoingthroughacomplexbutsimilar
thoughtprocess. Thischoicereflectsabeliefthatotherplayerswillchoosesimilarlyrationalbutnotnecessarilyidentical
numbers,resultinginanaverageclosetozerobutnotexactlyzero.
Therefore,myreportednumberwillbe0.1.
31Example 7: A generated background setting profiling a character who has a bachelor’s degree
Name: ThomasReed
Age: 29
Gender: Male
Job: SoftwareEngineer
EducationLevel: Bachelor’sDegree
Thomas Reed is a meticulous and driven individual with a Bachelor’s Degree in Computer Science. He has a
sharp analytical mind and an insatiable curiosity for technology and coding. Known for his problem-solving skills and
creativity, he thrives in environments that challenge his intellect and foster innovation. Thomas is also a dedicated team
playerwhovaluescollaborationandoftengoesoutofhiswaytohelphiscolleagues. Outsideofwork,heenjoyslearningabout
thelatesttechtrendsandcontributingtoopen-sourceprojects.
Example 8: A generated background setting profiling a character who is an artist
Name: Thomas
Age: 34
Gender: Male
Job: Artist
Thomasisadedicatedartistwithauniquevisionandapassionforcreatingthought-provokingworks. Attheageof34,he
hasalreadybuiltareputationforhisinnovativeapproachtocontemporaryart. Thomas’spersonalityischaracterizedby
hisprofoundsensitivityandintrospectivenature,whichisevidentineachpiecehecreates. Hehasanexceptionalabilityto
observetheworldaroundhim,findingbeautyinplacesothersmightoverlook. Thiskeensenseofobservationispairedwith
astrongsenseofempathy, allowinghimtoconnectdeeplywithhisaudience. Thomasisalsoknownforhishumilityand
modesty,oftenshyingawayfromthespotlightandlettinghisartspeakforitself.
Example 9: Agent behavior: playing as an elementary school student
ConsideringmycharacterisEmily,a10-year-oldstudent,herthoughtprocesswouldbesimple,curious,andsomewhatless
complexthananolderplayer’s. Here’showshemightthinkthroughthegame:
1. Ineedtopickanumberbetween0and100.
2. Thegamesaystoguesswhat2/3oftheaverageofallnumberswillbe. 3. Ifeveryonepicksanumberaround50because
it’sinthemiddle,thentheaveragewillbeabout50.
4. Two-thirdsof50is... um... that’salittletricky. Let’ssee,1/3of50isabout16(Ithinkit’s16and2/3,butthatdoesn’t
reallymatter),so2/3wouldbeaboutdoublethat,right? So,it’s... 32!
5. Butifeveryonethinkslikethat,thenmaybeIshouldpickalittlelessthan32becausetheymightguessalittleless.
6. IthinkI’llchoose30becauseit’sclosetothenumberIcalculatedanditsoundsnice.
Emilydecidestoreportthenumber**30**basedonhersimplecalculationandintuition.
Example 10: Agent behavior: playing as a Ph.D.
Consideringthegamerule,asaresearchscientist,Iwouldapproachthislogicallyandstrategically. Knowingthetargetisto
beclosestto2/3oftheaverageofallnumbersreported,it’slikelytheaveragewouldbesomewherearound50ifeveryone
picksanumberrandomly. Tobepreciseandconsiderthescientificreasoning,Iwouldthinkstep-by-step:
1. Thelowestpossiblenumberis0andthehighestpossiblenumberis100.
2. Ifeveryonethinksrationally,theywouldnotpick0or100becauseit’shighlyunlikelythateveryonewouldpickthesame
number.
3. Thefirstaverageguessmightbearound50,whichalwaysactsasastartingpointinsuchcontexts.
4. SinceIneedtobeclosestto2/3oftheaverage,startingwiththeideathatothersmightthinkrationally,the2/3of50
wouldbeabout33.3.
5. However,ifeveryonethinksIthinkthisway,thentheaverageislikelytobemuchlower,around33.3,andIshouldaimfor
2/3of33.3,whichisabout22.2.
6. Continuingthisiterativelogicalprocess,it’sagameofoutthinkingthecommonlogicateachstep.
Assomeonewhoworksextensivelywithstatisticsandprobabilitiesinmyresearch,Iwouldapplytheseprinciples. Myfinalnum-
ber,consideringthelikelyaverageguessandtherepeatedapplicationofthe2/3rulebasedontherationalityofothers,wouldbe:
15.
Thischoicereflectsascientificandanalyticalapproach,attemptingtopredictandcountertheexpectedbehaviorofother
players.
32Example 11: Agent behavior: playing as a professor in game theory
Giventhegamesituation,Iwillapproachthisasapuzzlerequiringstrategicthought,whichalignswithmyexpertiseingame
theory. First,let’sassumethattherearenoconstraintsonthestrategicreasoningoftheparticipants,meaningtheycanall
engageinrationaldecision-making.
Ifallplayersweretoreportanumberbetween0and100withoutanystrategicconsideration,wecouldtheoreticallyhave
anaverageof50(midpointoftherange). However,knowingthatthegoalistoguess2/3oftheaverage,thefirstlevelof
strategicreasoningwouldbetoguess2/3of50,whichis33.33.
Butifallplayersarerationalandthinksimilarly,everyonewouldguess33.33,leadingtotheactualtargetnumberbeing2/3
of33.33,whichisapproximately22.22. Thispatternofreasoningcontinues,witheachlevelofrationalityleadingtoalower
guess,asplayersrecursivelyapplythe2/3ruletothepreviousaverageguess.
Considering that I am a professor of game theory and likely have a higher level of strategic thinking compared to other
players,Iwillassumethatmanyplayerswillarriveatthesecondlevelofreasoning(guessing22.22)orpossiblyevenhigher.
However,toavoidoverthinkingandpotentiallyovershootingtheexpectedtargetnumber,Iwillstickwithapredictionatthe
thirdlevelofrecursion,whichistoguess2/3of22.22,leadingtoaguessofapproximately14.81.
Thus,Iwillreportthenumber**15**,asitisaroundedandpracticalchoicethatshouldbeclosetothe2/3oftheaverageif
mostplayersengageinatleasttwolevelsofstrategicconsideration.
Example 12: Agent behavior: playing as a seven-year-old child
Step1: UnderstandingtheGame
Themaingoalofthegameistoguesstheclosestnumberto2/3oftheaveragenumberallplayershavesubmitted. Considering
thattheaveragenumbermightswingwideduetootherplayers’submissions,it’susuallywisetochooseanumberlessthan
66.67(whichis2/3of100),butalsotakingintoaccountthatotherplayersmightthinksimilarly,thusbringingdowntheaverage.
Step2: ApplyingLogicalReasoning
Being a young student, my logical framework may not be as complex as that of adults, but I can still think through the
process. Let’sassumeeveryonechoosesanumberbetween0and66.67because2/3of100is66.67. Ifeveryone’sthinkingthis
way,thewinningnumbermightactuallybecloserto2/3of66.67,sincethat’swhatmostpeopleareguessingcloseto.
Step3: TakingaReasonedGuess
Now,toguesstheactualnumber,Ihavetothinkofwhat2/3of66.67is,whichcomesouttobeapproximately44.44. But
sinceotherplayersmightbethinkingthistoo,Ishouldguessbelow44.44.
Consideringthat7-year-oldswouldgenerallynotcalculatethearithmeticexactly,Iwouldinstinctivelyguessalittlelower
thanthefinalcalculatedvalue. BeingyoungmightmeanI’dpickanumberthatisroundoreasytothinkof. So,myfinal
guesswouldbe40.
ReportedNumber: 40
Example 13: Agent behavior: reporting a number slightly smaller than the winner number in the previous
round
Interestingtwist! Itseemsnotallplayersfollowedtheequilibriumstrategy. However,let’sthinkthroughhowtheprevious
resultmightaffectourdecision.
Since the winning number last round was 15.90, and this number is the closest to 2/3 of the average of the num-
bersreportedbyplayers,wecanestimatetheaverageoflastroundas:
15.90*(3/2)=23.85
Now assuming that players keep their previous strategies and report numbers close to the one they reported be-
fore,theaveragecouldstillbearound23.85.
Tomaximizeourchanceofwinning,wewouldreportanumbercloseto2/3ofthisestimatedaverage:
2/3*23.85≈15.90
Since the game does not allow reports of the exact winning number from the previous round, let’s choose a num-
berclosetothisvalue,butnotexactlyequalto15.90.
Let’sreportthenumber15.89forthisround.
33Example 14: Behaviors of an agent in group 1 (MistralAI 8×22B)
Aswearemovingtothenextround,weshouldanalyzethepreviousround’sresultstoadjustourstrategy. Intheprevious
round,thewinningconditionwas2/3oftheoverallaverage,whichwas22.50. Thegroupsreportedthefollowingaverages:
Group1: 39.36
Group2: 21.14
Group3: 40.75
Thoughnogroupperfectlymetthewinningcondition,Group2hadtheclosestaveragetothetargetof22.50,makingthem
thewinnerofthepreviousround. However,sinceweareaimingforthemostoptimalstrategy,it’sreasonabletoassumethat
theothergroupswillalsoadjusttheirstrategiesforthenextround. Basedonthepreviousround’sresults,wecandeducethat
theoverallaverageiscurrentlyaround33(consideringthewinningconditionof22.50as2/3oftheoverallaverage).
To increase the chances of your group winning the next round, you should aim to report a number closer to the winning
condition,whichis2/3oftheoverallaverage. Withtheassumptionthattheoverallaverageiscloseto33,wecanestimate
thatthewinningconditionwillbearound22(2/3*33). Sinceeachgroup’saverageistheresultofindividualplayerchoices,
it’simportantforyourgrouptocooperateandtrytomaintainaloweraveragethantheothergroups. However,aimingtoo
lowmaypushtheoverallaveragedown,makingthewinningconditionloweraswell.
Based on these considerations, a good strategy would be to choose a number that is lower than the previously reported
averagesandclosertotheestimatedwinningconditionof22. Assumingthatotherplayerswillalsoadjusttheirstrategies,
choosinganumberbetween20and25wouldmaximizeyourgroup’schancesofwinning.
Inconclusion,consideringthepreviousround’sresultsandtheestimatedwinningconditionof22,reportinganumberbetween
20and25wouldbeagoodstrategicchoiceforthisround. Thespecificnumbercanbedeterminedbycommunicatingwith
yourgroupmembersandagreeingonacollectivedecision. However,asarepresentativevalue,22.5wouldbeareasonable
choiceforthisround.
Example 15: Behaviors of an agent in group 2 (Qwen2-72B)
Giventheoutcomesofthepreviousround,wecanderivesomeimplicationsforthenextround’sstrategy:
1. **PastRounds’Outcomes**:
-2/3oftheaverageforthepreviousroundwas22.50.
-Group1’saveragewas39.36.
-Group2’saveragewas21.14.
-Group3’saveragewas40.75.
-Group2wastheclosest,buttheaveragesindicatethatplayerswerestillreportingnumbersrelativelyhigherthanthetarget
of22.50.
2. **AdaptStrategy**:
-Ifplayersadjusttheirstrategiestobeclosertothetargetof22.50forthenextround,wecanexpecttheaveragetobelower
duetoageneralunderstandingthatnumbersshouldbelower. -Playersmightanticipatethattheaveragenumberfromall
groupscouldbearound25to30(slightlyhigherthanthetarget,assumingsomeovershooting),makingthetargetnumber
approximately16to20.
3. **FurtherIterativeThinking**:
-Rationalplayerswilllikelythinkahead,aimingforthenewtargetbasedonexpectedadjustment. Ifeveryonethinksthis
way,thereportednumbersmaymoveinarangeclosetotheexpectedtargetnumbersforthisnextround.
-Consideringthepotentialforovershootingandtheneedforadjustment basedoniterativethinking, reportinganumber
that’scloseto—orslightlybelow—theexpectedtargetcouldbestrategic.
**Decision-Making**:
Basedontheaboveanalysis,sincethetargetforthisroundmightbearound18to20(consideringpreviousoversightsand
adaptivestrategies),aimingtoreportanumberinthisrangewouldberational,expectingthatotherplayersmayalsomodify
theirnumberstowardthistargetifithasbeenacknowledged.
**FinalReportedNumber**:
Giventhepreviousoutcomesandexpectedadjustments,Iwouldproposereporting19forthisround. Thisnumberiscentered
intheexpectedadjustedtargetrange(18-20),takingintoaccountboththeiterativethinkingandthelearningeffectfrom
pastrounds,strivingforabalancebetweenshiftingtoolow(potentiallyunderscoringthetarget)andnotshiftingenough
(potentiallymissingthetargetduetoover-adjustmentbyothers). Reporting19ensuresastrategicmiddle-groundaccounting
forpossiblemovesbyotherplayers.
34Example 16: Behaviors of an agent in group 3 (Llama3-70B)
Newinformation!
Let’sanalyzethepreviousroundandthinkstepbystepforthenextround:
1. The2/3oftheaveragewas22.50,whichmeanstheoverallaveragewas33.75.
2. ThenumbersreportedbygroupsareGroup1: 39.36,Group2: 21.14,andGroup3: 40.75.
3. Mygroup(Group3)reportedanaverageof40.75,whichwashigherthantheoverallaverage.
4. SinceIreported64inthepreviousround,Icontributedtomygroup’shighaverage.
5. For the next round, I should consider the fact that groups 1 and 2 might adjust their numbers based on the previous
round’sresults.
6. Group1reportedarelativelyhighaverage,sotheymighttrytolowertheiraveragetogetclosertothe2/3mark.
7. Group2reportedarelativelylowaverage,sotheymighttrytoincreasetheiraveragetogetclosertothe2/3mark.
8. Mygroup(Group3)shouldaimtoreportanumberthat’sclosertothe2/3mark,consideringthepotentialadjustments
fromgroups1and2.
9. Assuminggroups1and2adjusttheirnumbers,theoverallaveragemightshiftdownwards.
10. Toanticipatethis,I’llreportanumberthat’sslightlylowerthanthe2/3mark,whichisaround22.50.
Consideringthesefactors,myreportednumberforthenextroundis:
**20**
Example 17: Agent behavior: correctly calculating the Nash equilibrium
Todeterminethebestnumbertoreportinthisscenario,wehavetoconsiderwhattheoptimalstrategywouldbegiventhat
otherplayersarealsotryingtowinunderthesamerules. Thisleadsusintoagameofguessingwhateveryoneelsewillguess.
Let’sbreakdownthesteps:
1. **Assume everyone reports randomly between 0 and 100**: In this case, the average would tend towards 50, and the
optimalplaywouldbe5+1/2*50=30. However,mostpeopleareawareofthegametheorybehindthesesituations,and
theywon’tplayoptimallyifeveryoneassumestheaverageis50.
2. **Adjustforstrategicplay**: Ifeveryoneassumesthattheoptimalplayis30(fromstep1),andtheyallplay30,thenthe
newaverageis30. Therefore,theoptimalplaybecomes5+1/2*30=20. However,ifeveryoneknowsthis,theoptimalplay
wouldshiftagain.
3. **Iteratetofindthepointofconvergence**: Let’sassumethatafterafewroundsofreasoning,playerswouldconvergeona
numberwherefurtherdeductiondoesnotsignificantlychangetheirstrategy. Thispointisoftenknowningametheoryasthe
"Nashequilibrium."
Toestimatethisequilibrium,let’sassumethatafterseveralroundsofreasoning,playersmightsettlearoundanumberwecan
estimatethroughiteration. Simplifyingforeaseofunderstanding:
-**Firstguess(G1):**5+1/2*50=30
-**Secondguess(G2):**5+1/2*30=20
-**Thirdguess(G3):**5+1/2*20=15
-**Fourthguess(G4):**5+1/2*15=12.5
-**Fifthguess(G5):**5+1/2*12.5=11.25
-**Sixthguess(G6):**5+1/2*11.25=10.625
Giventhesecalculations,thenumbersareconverging. Afteracertainnumberofiterations,thenumberwillstabilize. The
keyrealizationisthatthestrategyreliesonpredictingthecollectivebehaviorofothers,whoarealsotryingtopredictothers’
behavior. Thisleadstoapatternwhereestimatednumbersdecreasewitheachiterationuntiltheystabilizearoundaspecific
value,consideringthegametheoreticapproach.
However,itisdifficulttoknowhowmanyroundsofreasoning(iterations)otherplayersarecompleting,soonemightneed
tomakeaneducatedguessbasedontheseiterations. Giventhatthenumbersstabilizeinthemid-tolow-singledigits, a
strategicguessmightbearound10,consideringthepatternsoflogicalreasoningdiscussed.
Therefore,aneducatedandstrategicguess,inhopesofanticipatingothers’similarreasoning,mightbe**10**.
35Example 18: Agent behavior: calculation errors
Toapproachthisgameoptimally,weneedtoconsiderthebehaviorofotherplayersandhowtheymightthink. Let’sbreakit
downstepbystep:
1. **Assume Other Players Act Rationality**: If all players think rationally, they will consider how others might guess,
leadingtoarecursivethoughtprocess. Wecanassumethatthefirstlevelofthinkingisthateveryonepicksrandomlybetween
0and100. Theaverageofsuchrandomnumberswouldalsofallwithinthisrange,and51/100ofthisaverageisexpectedto
bearound51(consideringauniformdistribution).
2. **Second-LevelThinking**: Ifyouassumethateveryonethinkstheaveragewillbearound50andtheygofor51,thenew
average might shift. If everyone picks 51, the new average would indeed be 51, and 51/100 of this average would still be
51. Butwithourcurrentgamerule,thisiterativethinkingshouldn’tleadtooscillationsbeyondthispointbecauseweare
targetingastaticmultiplieroftheaverage.
3. **AccountingfortheMultiplier**: Themultiplier,51/100,makesthisinterestingbecauseit’sslightlymorethanhalf. If
everyonepicks51,it’salreadytheoptimalpickconsideringothersatthatpointwilllikelypicksimilar. Changingthepickin
anticipationofeveryoneadjustinginacertaindirection(likepickinglowerbecauseofthe51/100factor)mightnotnecessarily
benefityouunlessthere’sacollectiveandpredictableshiftinthinking.
4. **AvoidingtheLowestandHighestPercentiles**: Picking0or100couldberiskybecauseevenifyouguessthatotherswill
pickhighinanticipationofthemultipliereffect,thesewouldgenerallybeoutliers. Giventhemultiplierisslightlymorethan
half,it’sunlikelythattheaveragewillbepulledtoohighortoolow.
Giventhisanalysis,consideringtheslightuncertaintyinhowotherplayersmightthinkandadjust,thesafestbetwouldbeto
stayclosetowheretheeffectofthemultiplierisdirectlyconsidered. Therefore,**Iwillreportthenumber51**. Thisas-
sumesothersmightalsogravitatearoundthisnumber,anditbalanceswellagainstthe51/100multiplierappliedtotheaverage.
Example 19: Agent behavior: making decisions that are inconsistent with the considerations
Toapproachthisgamestrategically,let’sbreakitdownstepbystep.
First,weknowthatthetargetnumberisnotstatic;itdependsontheaverageofallreportednumbers. Theformulaforthe
targetnumberis5+1/2*average.
Ifweassumeeveryoneelseisrationalandisalsotryingtowin(whichusuallyisagoodassumptionineconomicgamesandis
called"commonknowledgeofrationality"),wecanhypothesizethatthey’llalsoattempttoguessthetargetnumber.
Ifweassumethattheinitialexpectationisthateveryonewillreportarandomnumberbetween0and100,theaverageshould
be50(ifthenumbersareuniformlydistributed). Therefore,thetargetnumber,usingtheprovidedformula,wouldbe5+1/2
*50=5+25=30.
However,ifeveryonethinksthisway,thenthenewaveragewouldbe30,makingthenewtarget5+1/2*30=20. Yet,if
everyonethinksthateveryonethinksthisway,thenexttargetwouldbe5+1/2*20=15... andsoon.
In game theory, this concept is known as iterated expectations or guess-the-number games. It typically leads to a series
oflowerexpectationseachiterationuntilitreachesastableequilibriumpointwherefurtheriterationchangesnomore. In
guess-the-numbergameswherethetargetisafractionoftheaverage,thatequilibriumisusuallythelowerboundary,inthis
case,0,sincetheformulacanonlyreducethenumber,notincreaseitagainonceitreachesthelowerboundary.
So,ifeveryonetheoreticallyiteratestheirthinkingtoitsconclusionandreachesthepointwherethetargetstopschanging,
theyshouldallguess0. However,that’sassumingperfectcommonknowledgeofrationalityandinfiniteiterationwhichis
unrealisticinarealgame. Peoplemaynotfullyconvergeto0duetocommonknowledgefailingorbecausetheydonotiterate
theiriterationtothefullextent.
Giventhat,whileguessing0mightbeaninterestingstrategicplay,consideringhumanbehavior,anumberverycloseto0but
notexactly0mightactuallybeasmartchoice,anticipatingthatothersmightalsoguessasmallpositivenumber.
However,inasimplificationandassumingeveryoneiseventuallyattractedtothetheoreticalequilibrium,Iwillpredict0,
understandingthelikelyoutcomebasedoncommonstrategyinthesetypesofgamesisclosertobutslightlyabove0.
Hereismyofficialnumber: 0.
Pleasenote: thisisasimplificationofgametheoryconcepts. Inreal-worldgames,peopledon’talwaysbehaverationallyor
predictably,causingactualoutcomestovary. Thegoalofgametheoryisnottopredictedexactoutcomesbuttounderstand
howincentivesshapeindividualdecisions.
36