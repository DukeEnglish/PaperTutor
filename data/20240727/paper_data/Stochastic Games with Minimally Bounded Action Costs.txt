Stochastic Games with Minimally Bounded Action
Costs
David Mguni1
1Queen Mary University, London
July 26, 2024
Abstract
In many multi-player interactions, players incur strictly positive costs each
time they execute actions e.g. ‘menu costs’ or transaction costs in financial sys-
tems. Since acting at each available opportunity would accumulate prohibitively
large costs, the resulting decision problem is one in which players must make
strategic decisions about when to execute actions in addition to their choice of
action. This paper analyses adiscrete-time stochasticgame (SG) inwhich players
face minimally bounded positive costs for each action and influence the system
using impulse controls. We prove SGs of two-sided impulse control have a unique
value and characterise the saddle point equilibrium in which the players execute
actions atstrategically chosen times inaccordance with Markovian strategies. We
prove the game respects a dynamic programming principle and that the Markov
perfect equilibrium can be computed as a limit point of a sequence of Bellman
operations. We then introduce a new Q-learning variant which we show con-
verges almost surely to the value of the game enabling solutions to be extracted
in unknown settings. Lastly, we extend our results to settings with budgetory
constraints.
1 Introduction
The goal of successfully modelling financial behaviour is the cornerstone of financial
theory. Although this ambition holds the key to greater mastery over financial systems,
progress towards it is hindered by the fact that various economic intricacies remain ne-
glected in many of today’s financial models (Waggoner & Zha, 2012;Hansen & Sargent,
2001). Model misspecifications of this kind can result in vast disparities between the
predictions of financial models and the financial phenomena they attempt to model
(Simons et al.,1997;Krugman,2009). Correctly modellingeconomic behaviour requires
accurately specifying the costs and rewards associated with economic activity in a given
system.
1
4202
luJ
52
]AM.sc[
1v01081.7042:viXraStochastic games model strategic interactions between two competing agents that
take place over time (Shapley, 1953; Solan & Vieille, 2015). They are a standard mod-
ellingframeworkforcompetitive settingswithineconomicsandfinance(Prasad & Sethi,
2004; Browne, 2000). Nevertheless, classical stochastic games do not naturally induce
any restrictions on the cost magnitudes for taking actions. Therefore, in these mod-
els, costs can be made arbitrarily small given specific choices of action. Consequently,
agents can freely execute actions at each available opportunity without accumulating
large costs over time. In many financial settings, despite agents having the ability to
exercise control over the magnitude of their investments, the cost of agents’ investment
decisions cannot be made arbitrarily small (Cuypers et al., 2021). For example, the
amount of stock that an agent can purchase may be required to be at least some fixed
minimal amount e.g. an individual share unit or, agents may be required to pay trans-
action costs i.e., a payment to a broker to perform their investment decisions on their
1
behalf (Allen, 1991; Goldstein et al., 2009). In these settings, investment strategies
that prescribe performing actions at every opportunity can be vastly suboptimal since
acting in this way can accumulate massive costs (Azimzadeh, 2017). Consequently, in
settings where agents face minimally bounded costs for their actions, models derived
from classical stochastic games may fail to generate predictions that accord with ob-
served outcomes.
To resolve this issue in single-agent settings, a form of policy known as impulse con-
trol has emerged within optimal control theory (Øksendal & Sulem, 2005; Mguni et al.,
2023b; Davis et al., 2010). Impulse control models prescribe a set of optimal points to
performactionsinadditiontothesequenceofoptimalbest-responseactions(Azimzadeh,
2017; Jeanblanc-Picqué, 1993). Currently however, the theory of stochastic games in
discrete-time does not include a treatment that handles impulse control. As such, de-
vising algorithmic protocols for computing equilibrium strategies in stochastic games
with minimally bounded action costs is largely unfeasible. To address this issue, in this
paper, we tackle this problem by studying a stochastic game in which the players face
minimally bounded costs for each action. Our treatment includes an algorithmic learn-
ing framework for computing the minimax equilibrium strategies. To our knowledge,
this treatment is the first to tackle learning in dynamic strategic environments of this
kind.
Example: AdvertisingInvestment DuopolyCosts(Deal,1979;Prasad & Sethi,
2004). To demonstrate the ideas within a concrete setting, we consider a well-known
problem within economics, namely the problem of how a firm should invest in adver-
tising in a duopoly setting in which it competes for market share. In this setting, each
firm seeks to maximise its long-term profit and to this end, performs investment ad-
justments, the costs of which are bounded from below. At time t = 0,1,... each Firm
i 1,2 has a revenue stream Si = Si(ω) : R Ω R which is a stochastic process.
∈ { } t >0 × →
At any point, Firm i makes costly investments of size u where denotes the set
i i i
∈ U U
1Fixed costs of this kind are also prevalent in microeconomic settings where firms that make ad-
justments to their business strategies can be subject to adjustment costs or menu costs (Midrigan,
2011).
2of admissible investments for Firm i. Denote by M R the potential market size
>0
∈
and by bi ]0,1] the response rate to advertising for Firm i, then the revenue stream
∈
for Firm i is given by the following expression:
Si = Si +biui M Si Sj M−1 riSi +σi(B B ), (1)
t+1 t t − t − t − t t+1 − t
where i = j 1,2 and si (cid:2) Si R ar(cid:3) e the initial sales for Firm i. The constants
6 ∈ { } ≡ 0 ∈ >0
ri,σi R represent the rate at which Firm i abstracts market share and the volatility
∈
of the sales process for Firm i respectively. The term B is Brownian motion which
t
introduces randomness to the system. Each firm seeks to maximise its cumulative profit
which consists of its revenue due to sales h : R R minus its running advertising costs
→
c : R R R and lastly, a function of the firm’s terminal market share G : R R.
i >0
× → →
The profit function for Firm i, Πi is given by the following expression:
Πi(si;ui,uj) = E γt hi(Si) ci(ui) cj(uj) . (2)
t − t − t
" #
t≥0
X (cid:0) (cid:2) (cid:3)(cid:1)
Since the market is duopolistic, the payoff structure satisfies Π +Π = 0.
1 2
Therefore the firms engage in a zero-sum stochastic game (Deal, 1979). Models of
this kind have been used to analyse the strategic interaction within advertising duopoly
using a game-theoretic framework (Prasad & Sethi, 2004). Using this framework, the
behaviour of the firms in the advertising problem can be characterised by computing
the best-response strategies within the SG. Models such as the Vidale-Wolfe model
of advertising Sethi (1973) have been studied to analyse settings in which both firms
make continuous modifications to their investment positions models. These models
do not account for the fixed minimal costs that firms incur when executing investment
decisions. We refer the reader to (Erickson, 1995) for exhaustive discussions on duopoly
advertisinginvestment modelsandto(Prasad & Sethi,2004)forastochasticdifferential
game approach.
In this paper, we study a stochastic game that models the behaviour of firms (and
other economic agents) facing minimally-bounded fixed costs for each investment. This
feature prohibits firms from committing to strategies that involve performing (possibly
infinitesimal) adjustments over the horizon of the problem contrary to current strate-
gic investment models. To handle scenarios in which the game model may not be
known in advance, we develop a reinforcement learning framework (Sutton & Barto,
2018; Albrecht et al., 2024) that enables the equilibrium strategies to be computed us-
ing repeated interactions with the environment. In particular, we modify the standard
reinforcement learning paradigm in which agents solely learn to optimally select correct
actions at each time-step (Szepesvári, 2022) to handle strategic settings in which the
agents must also learn to determine when to perform actions. Our learning protocol
enables the equilibrium strategies for the game to be computed using a value-iterative
procedure. We then establish results that ensure convergence of a Q-learning variant to
the minimax equilibrium of the game. To do this, we prove a series of results namely:
3i) We prove the existence of a saddle point equilibrium of a (discrete-time) Markov
game in which players use impulse control and, therefore strategically select the set of
points to perform actions.
ii) We then establish a dynamic programming principle (DPP) for stochastic games of
two-sided impulse control and show that the minimax (saddle point) equilibrium can
be computed as a limit point of a sequence of Bellman operations (Theorem 2) which
lays the foundation for a learning methodology for computing minimax equilibrium
solutions to SGs of two-sided impulse control.
iii) We extend result ii) to a new variant of Q learning which enables the game to
be solved even when the game is unknown using a multi-agent reinforcement learning
method (Theorem 3).
iv) We then extend the result i) to (linear) function approximators enabling the value
function to be parameterised (Theorem 5).
v) In Sec. 5, we extend our analysis to include budgetary constraints so that each
action draws from a fixed budget which each player must stay within. Analogous to the
development of ii), we establish another DPP fromwhich we derive a Q-learning variant
for tackling impulse control with budgetary constraints (Theorem 7). A particular case
of a budget constraint is when the number of actions each player can take over the
horizon is capped.
2 Formulation
Stochastic Games. In a (two-player) stochastic game (SG) (Shapley, 1953), a pair
of players sequentially perform actions to maximise their individual expected returns.
The underlying problem is formalised by a tuple , , , ,P,R,γ where Rp is
hN S A B i S ⊂
the set of states, Rk and Rk are the set of actions for Player 1 and Player 2
A ⊂ B ⊂
respectively, P : [0,1] is a transition probability function describing
S ×A×B ×S →
the system’s dynamics, R : R is the reward function and the discount
S × A × B →
factor γ [0,1) specifies the degree to which each player’s rewards are discounted over
∈
time. At time t 0,1,..., the system is in state s and Player 1 and Player 2 must
t
∈ ∈ S
choose an action, a and b respectively which transitions the system to a new
t t
∈ A ∈ B
state s P( s ,a ,b ) and produces a reward R(s ,a ,b ). The goal of Player 1 is
t+1 t t t t t t
∼ ·|
to determine a strategy σ1 Σ1 that maximises its expected return given by the value
function: vσ1,σ2 (s) = E[ ∞∈ γtR(s ,a ,b ) a σ1( s ),b σ2( s ),s = s] whereas
t=0 t t t | t ∼ ·| t t ∼ ·| t 0
the goal of Player 2 is to minimise the same quantity using a strategy σ2 Σ2 where Σi
P ∈
is the policy set for player i 1,2 . The action value function is given by Q(s,a,b) =
∈ { }
E[ ∞ R(s ,a ,b ) a = a,b = b,s = s]. A Markov policy σ : [0,1] for
t=0 t t t | 0 0 0 S × A →
Player 1 is a probability distribution over state-action pairs where σ(a s) represents the
P |
probability of selecting action a in state s . We construct a Markov strategy
∈ A ∈ S
for Player 2 analogously. In this paper, we restrict our attention to Markov policies for
both players.
We consider a setting in which each player faces at least some minimal cost for
4each action it performs. Systems of this kind widely occur in economic and financial
systems. Common examples are duopolies in which firms face investment adjustment
costs. With this, the objective that Player 1 seeks to maximise is given by:
∞
vσ1,σ2 (s) = E γt (s ,a ,b ) (s ,a ,b ) s = s , (3)
t t t t t t 0
{R −C }
" #
Xt=0 (cid:12)
(cid:12)
(cid:12)
and the goal of Player 2 is to maximise v, where for any state s and any action
− ∈ S
a ,b , the function is given by (s,a,b) := R(s,a1 (a),b1 (b)), where 1 (y)
A B Y
∈ A ∈ B R R
is the indicator function which is 1 whenever y and 0 otherwise. For example, if
∈ Y
at time t only Player 1 takes an action aˆ1 while player 2 takes no action, the
t ∈ Ai
reward is R(s ,ˆa ,0). The cost function : R is given by (s,a,b) :=
t t
C S × A × B → C
c(s,a)1 + c(s,b)1 where c : ( ) R is a minimally bounded (cost)
a∈A b∈B >0
2 S × A∪B →
function that introduces a cost each time the player performs an action. Examples
of the cost function is a quasi-linear function of the form c(s ,y ) = κ+ f(y ) for any
t t t
y , where f : R and κ is a real-valued positive constant. In this
t >0
∈ A ∪B A∪B →
game, at each time step t, the system transitions according to the probability kernel
which is given by (s ,a ,b ,s ) := P(s ,a 1 (a ),b 1 (b ),s ). Since acting at
t+1 t t t t+1 t A t t B t t
P P
each time step would incur prohibitively high costs, the players must be selective about
when to act. Therefore, in this setting, the player’s problem is augmented to deciding
both a best-response strategy for their actions and when to apply their chosen action.
The question of when each player ought to act is one of the key questions addressed in
this paper.
The function c plays an important role in modulating the willingness of each player
to act. Larger costs for each action mean the players must be more selective about
executing actions, reserving their actions to a smaller number of states that induce
larger increases in their expected return (this is proven in Theorem 1). In the limit
c 0 we return to a classic SG framework where each player is willing to execute an
→
action at each state. In Sec. 5, as an alternative to using the function c as a way of
controlling the number of actions executed by the player. There we discuss imposing
a budget constraint on the number of actions each player can perform over a given
horizon.
Definition 1 Let us define by val+[v] := min max vσ1,σ2 the upper value function
σ2∈Σ2σ1∈Σ1
and by val−[v] := max min vσ1,σ2 , the lower value function. The upper (lower) value
σ1∈Σ1σ2∈Σ1
function represents the minimum payoff that Player 1 (Player 2) can guarantee itself
irrespective of the actions of the opponent. The value of the game exists if we can
commute the max and min operators:
val−[v] = max min vσ1,σ2 = min max vσ1,σ2 = val+[v]. (4)
σ1∈Σ1σ2∈Σ2 σ2∈Σ2σ1∈Σ1
2I.e. a function which is bounded below by a positive constant.
5We denote the value by v := val+[v] = val−[v] and denote by (σˆ1,σˆ2) Σ1 Σ2 the
∈ ×
pair that satisfies the equality equation 4.
In general, the functions val+[v] and val−[v] may not coincide. The value, should
it exist, is the minimum payoff each player can guarantee themselves under the equi-
librium strategy. If a value exists in Markov strategies, it constitutes a Markov perfect
equilibrium (Deng et al., 2021) of the game in which neither player can improve their
payoff by playing some other control — an analogous concept to a Nash equilibrium for
the case of two-player zero-sum games. Thus the central task to establish an equilib-
rium involves unambiguously assigning a value to the game, that is proving the equality
in equation 4.
3 Existence of a Value and Saddle Point Equilibrium
In this section, we prove the existence of a minimax equilibrium value of the game .
G
We then characterise the equilibrium conditions for executing actions and show that
such times are characterised by an ‘obstacle condition’ which can be evaluated online.
We perform some further studies on the properties of the equilibrium strategies. We
then show that the dynamic programming principle holds and prove the convergence
of a dynamic programming method to the minimax equilibrium of the game. We then
extend the result to allow for (linear) function approximators. Our first main result
is the existence of a unique minimiax equilibrium of the game in Markov strategies
G
(Theorem 1). Our second key result is that the game respects a dynamic programming
principle and that the value of the game can be obtained by computing the sequence of
Bellman operations acting on some function. With this, each player enacts a minimax
best response policy.
The results are achieved through several steps: Theorem 2 proves that Bellman
dynamic programming principle holds and that the Bellman operator for the game is a
contractionmapping. This paves the way for proving the convergence of the sequence of
repeatedapplicationofBellmanoperatorstothevalueofthegame. Prop. 1thenproves
that the equilibrium strategies derived in Theorem 2 consists of Markov strategies and
thus the equilibrium concept corresponds to a Markov perfect equilibrium. Thereafter
weprovethatthevalueofthegameisunique. Thereafter, wecharacterisetheconditions
under which each player takes an action when executing its best-response minimax
equilibrium strategy. Prop. 2 characterises the conditions under which a rational player
should execute anaction inresponse to its adversary anddoes so in terms of a condition
on the action-value function that can be evaluated online. The results of this section
lay the foundation for learning methods studied in Sec. 4 where we consider settings in
the game is unknown to the players who seek to determine their equilibrium strategies
through repeated interaction. The proof of the results in this section are deferred to
the Appendix.
Notation & Assumptions All results are built under Assumptions A.1 - A.5 which
are standard in RL and stochastic approximation theory (Bertsekas, 2012).
6We assume that is defined on a probability space (Ω, ,P) and any s is
S F ∈ S
measurable with respect to the Borel σ-algebra associated with Rp. We denote the
σ-algebra of events generated by s by . In what follows, we denote by
t t≥0 t
{ } F ⊂ F
( , ) any finite normed vector space and by the set of all measurable functions.
V kk H
The results of the paper are built under the following assumptions which are standard
within RL and stochastic approximation methods:
A.1. The stochastic process governing the system dynamics is ergodic, that is
the process is stationary and every invariant random variable of s is equal to a
t t≥0
{ }
constant with probability 1.
A.2. The function R is in L .
2
A.3. For any positive scalar c, there exists a scalar κ such that for all s and
c
∈ S
for any t N we have: E[1+ s c s = s] κ (1+ s c).
t 0 c
∈ k k | ≤ k k
A.4. There exists scalars C and c such that for any function v satisfying v(s)
1 1
| | ≤
C (1+ s c2)forsomescalarsc andC wehavethat: ∞ E[v(s ) s = s] E[v(s )]
2 k k 2 2 t=0| t | 0 − 0 | ≤
C C (1+ s c1c2).
1 2 0
k k P
A.5. There exists scalars c and C such that for any s we have that R(s, )
∈ S | · | ≤
C(1+ s c).
k k
Definition 2 Given a function Q : R, σi Σi and s ,s , we
τ ρ
S × A ∪ B → ∀ ∈ ∀ ∈ S
define the intervention operators M and M by
1 2
M Q(s ,a ,b) := max (s ,a ,b) c(s ,a )+γ P (s′;a ,b,s)v(s′) τ , b
1 τ τ
aτ∈A R
τ τ
−
τ τ s′∈S τ
| ∈ F ∀ ∈
, and
(cid:2) P (cid:3)
B
M Q(s ,a,b ) := min (s ,a,b )+c(s ,b )+γ P (s′;a,b ,s)v(s′) ρ , a
2 ρ ρ
bρ∈B R
ρ ρ ρ ρ s′∈S ρ
| ∈ F ∀ ∈
, where τ and ρ (cid:2) are Player 1 and Playe Pr 2 intervention times respectiv(cid:3)ely.
A ∈ F ∈ F
The interpretation of M is the following: suppose that Player 1 is using the policy
1
σ1 and at time t = τ the system is at a state s and the player performs an action
τ
a σ1( s ). A cost of c(s ,a ) is then incurred by the player, and the system
τ τ τ τ
tran∼ sitions·| to s′ P( ;a ,b,s ). Lastly, recall vσ1,σ2 is the value function under the
τ τ
policy pair (σ1,σ∼ 2). T· herefore, the quantity M Qσ1,σ2 measures the expected future
1
stream of rewards after an immediate action minus the cost of action. This object
plays a crucial role which as we later discuss and enables us to characterise the points
at which each player should perform an action.
For any a ,b , define Q (s,a) := Q(s,a,0) and Q (s,b) := Q(s,0,b), given
1 2
∈ A ∈ B
a function v : R, for any s , we define the Bellman operator T, by:
S → ∈ S
Tv(s) := min max M Q , (s,0)+γ (s′;0,s)v(s′) ,M Q . (5)
1 1 2 2
R P
" ( s′∈S ) #
X
The Bellman operator captures the nested sequential structure of the decision pro-
3
cess. In particular, the structure in equation 5 consists of an inner structure that
3Note that the Bellman operator in equation5 encodes an order of precedence for the players— if
7consists of two terms: the first term is the expected future return given an action is
taken at the current state under the policy σi. The second term is the expected future
return given no action is taken at the current state. Lastly, the outer structure is an
optimisation that compares the expected return of the two possibilities and selects the
maximum.
Theorem 1 The minimax value of the game exists and is unique, that is there exists
a function vˆ : R which respects the following equality:
S →
ˆv = min maxv(σˆ1,σˆ2) = max min v(σˆ1,σˆ2) (6)
σˆ1∈Σˆ1σˆ2∈Σˆ2 σˆ2∈Σˆ2σˆ1∈Σˆ1
Therefore, Theorem 1 confirms that the value vˆ for the game exists and constitutes
ˆ G
a saddle point equilibrium. Correspondingly, we denote by Q the corresponding action
value of the game associated with vˆ. The Theorem is proved in several steps, namely
G
by proving the convergence of a dynamic programming procedure to the solution of the
game (Theorem 2) and then proving the optimality of the solutions (Proposition 1).
An important consequence of Theorem 1 is that at the stable point, each player best
responds to the influence of the other, formally:
Definition 3 A joint strategy (σ1,σ2) = σ Σ is a Markov perfect equilibrium strategy
∈
if no player can improve the expected return by changing their current policy. The solu-
tion is a stable fixed point in which each player optimally responds to the policies of other
players in the system: Formally, i , σ′i Σi, we have v(s σ) v(s (σ′i,σ−i)) 0.
∀ ∈ N ∀ ∈ | − | ≤
We can now state a key result. The following theorem proves the convergence of a
value-iteration approach to the solution.
Theorem 2 Let v : R then the sequence of Bellman operators acting on v con-
S →
verges to the solution of the game, that is to say for any s the following holds:
∈ S
limTkv(s) = vˆ(s),
k→∞
Theorem2provesthesolutionto canbeobtainedbycomputingthelimitofadynamic
G
programming procedure. The proof of the Theorem is deferred to the Appendix.
Proposition 1 Let σˆ Σ be a strategy generated by the procedure outlined in Theorem
∈
2, then σˆ is a minimax Markov perfect equilibrium policy.
Proof 1 The proof is achieved using similar arguments as those presented in Theo-
rem 2 of (Shapley, 1953) with some modifications. Denote by the finite game k of
G
k < steps in which player i 1,2 maximise the following objectives
vσ1,σ2
(s) =
∞ ∈ { } k
both players decide to perform an action on the system at the same time, we only take into account
the action of Player 2. This introduces an obvious asymmetry in the game. Nevertheless, in infinite
horizon problems, such overlaps may be rare and hence the effect of this asymmetry is expectedly
small.
8E k γt (s ,a ,b ) (s ,a ,b ) s = s . Suppose in the game ˆk, Player 1 is
t=0 {R t t t −C t t t } 0 G
givhen a payoff of (s,a)+ a vσ1,σ2 (s(cid:12)′) givenia (a,b) (σ1,σ2) Σ1 Σ2 and for
P R Ps′s (cid:12) ≡ ∼ ∈ ×
any given s . Now the Markov stra(cid:12)tegy σ1(a s) Σ1 guarantees Player 1 a payoff
∈ S | ∈
of
vσ1,σ2
(s). Now in the game k, after n < steps and using the strategy σ1(a s)
k G ∞ |
gives Player 1 an expected payoff of at least
vσ1,σ2
(s) γn−1 max
(a,b)vσ1,σ2
(s′)
k − a∈A,b∈BPs′s k−1 ≤
vσ1,σ2
(s)
γn−1maxvσ1,σ2
(s′). Therefore, accounting for the n steps, the total payoff
k − s′∈S k−1
for Player 1 is at least
vσ1,σ2
(s)
γn−1maxvσ1,σ2
(s′)
n−1γtvσ1,σ2
(s′) =
vσ1,σ2
(s)
k − s′∈S k−1 − t=0 n−t k −
γn−1maxvσ1,σ2 (s′) γn1−γn v := v˜σ1,σ2 (s). This expresPsion holds for arbitrarily large
s′∈S k−1 − 1−γ k k k,n
values of n in particular lim
v˜σ1,σ2
(s) =
vσ1,σ2
(s) from which it follows that the strategy
k,n k
n→∞
σ1 is optimal for Player 1. After using analogous arguments for Player 2 we deduce the
result.
Having constructed a procedure to find each player’s best-response strategy, we now
seek to determine the conditions when an intervention should be performed. Let us
denote by τ ( ρ ) the points at which each Player 1 (Player 2) decides to act
k k≥0 r r≥0
{ } { }
or intervention times, so for example if Player 1 chooses to perform an action at state
s and again at state s , then τ = 6 and τ = 8. We say that the times τ and
6 8 1 2 k k≥0
{ }
ρ are best-response intervention times if executing actions at that sequence of
r r≥0
{ }
times supports an MPE strategy. The following result characterises the best-response
intervention times τ and ρ .
k k≥0 r r≥0
{ } { }
Proposition 2 The Player 1 and Player 2 best-response intervention times are given
by the following
ˆ ˆ
τ = inf τ > τ M Q = Q ,
k k−1 1
{ | }
ˆ ˆ
ρ = inf ρ > ρ M Q = Q .
r r−1 2
{ | }
ˆ
Therefore, given the function Q, the times τ , ρ can be determined by evaluating
k r
ˆ ˆ { } { }
if M Q = Q hold.
i
A key aspect of Prop. 2 is that it exploits the cost structure of the problem to
determine when each player should perform an intervention. In particular, the equality
ˆ ˆ
M Q = Q implies that performing an action and incurring a cost for doing so is
i
optimal. The following result characterises the action conditions:
Corollary 1 For any s , Player 1 performs an action whenever the following condi-
∈ S
tion is satisfied: 1 R
+
M 1Qˆ (s,a |·) −m a∈a AxQˆ (s,a,b) = 1, ∀b
∈
B, where 1 R
+
is the in-
dicatorfunction i.e. 1(cid:18) R (x) = 1 if x > 0 and 1 R (x)(cid:19) = 0 otherwise. Analogously, Player
+ +
2 performs an action whenever 1 R
+
m a∈i AnQˆ (s,a,b) −M 2Qˆ (s,a |·) = 1, ∀a
∈
A.
(cid:18) (cid:19)
The result provides characterisation of where each player should execute an action.
94 Learning in Unknown Environments
In this section, we study the problem while considering settings in which the game
G
is entirely unknown to the players. To devise a method that enables us to handle this
setting, we employ tools from reinforcement learning through which each player learns
its minimax equilibrium strategy by repeatedly playing the game. Within the standard
RL paradigm, RL agents decide on an action at each time step thus the standard
RL framework is not designed to handle the current setting (Sutton & Barto, 2018).
In what follows, we introduce an RL method that accommodates impulse controls in
the stochastic game setting enabling the players to learn their equilibrium strategies
entirely through repeated interaction. This, in turn, enables the players to learn when
to perform actions in addition to the best-response actions which together constitute a
best-response strategy to the play of their opponent.
Algorithm 1:
1: Input: Constant ǫ 0,
≥
2: Initialise: Q-function, Q
0
3: repeat
4: n 0
←
5: for t = 0,1,... do
6: Compute a argmaxQ (s ,0,b ),b argminQ (s ,a ,0)
t n t t t n t t
∈ ∈
7: if M Q Q then
1 n n
≥
8: Apply a so s P( a ,0,s ),
t t+1 t t
∼ ·|
9: Receive rewards r = (s ,a ,0)
t t t
R
10: else
11: if M Q Q then
2 n n
≤
12: Apply b so s P( 0,b ,s ), ‘
t t+1 t t
∼ ·|
13: Receive rewards r = (s ,0,b )
t t t
R
14: else
15: Apply no action so s P( 0,s ),
t+1 t
∼ ·|
16: Receive rewards r = (s ,0).
t t
R
17: end if
18: end if
19: end for
20: // Learn Qˆ
21: Update Q function according to the update rule equation 7
n
22: until Q (s,a) Q (s,a) ǫ, s
n n−1
| − | ≤ ∀ ∈ S
We initiate the study of learning the minimax solution to by presenting the main
G
result of the section, namely Theorem 3. The theorem proves that a variant of a Q-
ˆ
learning (Bertsekas, 2012) procedure converges almost surely to the action-value Q for
the game .
G
10Theorem 3 Consider the following Q learning variant:
Q (s ,a) = Q (s ,a)
t+1 t t t
+α (s ,a ) min max M Q (s ,a), (s ,0)+γQ (s ,0)
t t t 1 t t t t t+1
{ R }
h ‘ (7)
,M Q (s ,a) Q (s ,a) ,
2 t t t t
−
!
i
ˆ
then Q converges to Q with probability 1, where s ,s and a .
t t t+1
∈ S ∈ A×B
The proof of Theorem 3 requires several intermediate results using stochastic ap-
proximation theory (Bertsekas, 2012). We first make use of the following result:
Theorem 4 (Theorem 1, pg 4 in (Jaakkola et al., 1994)) Let Ξ (s) be a random
t
process that takes values in Rn and given by the following:
Ξ (s) = (1 α (s))Ξ (s)α (s)L (s), (8)
t+1 t t t t
−
then Ξ (s) converges to 0 with probability 1 under the following conditions:
t
i) 0 α 1, α = and α2 <
≤ t ≤ t t ∞ t t ∞
P P
ii) E[L ] γ Ξ , with γ < 1;
t t t
k |F k ≤ k k
iii) Var[L ] c(1+ Ξ 2) for some c > 0.
t t t
|F ≤ k k
Therefore, Theorem 4 serves as an important stepping stone to prove the convergence
of the procedure in equation 7 - in particular, we must show that for our construction
of the Bellman operator and the procedure outline in equation 7, the conditions oof
Theorem 4 hold. To prove the result, we show (i) - (iii) hold. Condition (i) holds by
choice of learning rate. It therefore remains to prove (ii) - (iii). We first prove (ii). For
this, we consider our variant of the Q-learning update rule:
Q (s ,a ,b ) = Q (s ,a ,b )
t+1 t t t t t t t
+α (s ,a ) min max M Q(s ,a ,b ), (s ,0)+γQ (s ,0)
t t t 1 t t t t t t+1
{ R }
h
,M Q(s ,a ,b ) Q (s ,a ,b ) ,
2 t t t t t t t
−
!
i
11ˆ
After subtracting Q(s ,a ,b ) from both sides and some manipulation we obtain that:
t t t
Ξ (s ,a ,b )
t+1 t t t
= (1 α (s ,a ,b ))Ξ (s ,a ,b )
t t t t t t t t
−
+α (s ,a ,b ) min max M Q (s ,a ,b ), (s ,0)+γQ (s ,0)
t t t t 1 t t t t t t t+1
{ R }
h
ˆ
,M Q(s ,a,b) Q(s ,a ,b ) ,
2 t t t t
−
!
i
ˆ
where Ξ (s ,a ,b ) := Q (s ,a ,b ) Q(s ,a ,b ).
r t t t r t t t t t t
−
Let us now define by
L (s ,a,b) := min(max M Q (s ,a ,b ), (s ,0)+γQ (s ,0) ,M Q(s ,a,b))
t τk
{
1 t t t t
R
t t t+1
}
2 t
ˆ
Q(s ,a,b).
t
−
Then
Ξ (s ,a ,b ) = (1 α (s ,a ,b ))Ξ (s ,a ,b )+α (s ,a ,b )L (s ,a,b). (9)
t+1 t t t
−
t t t t t t t t t t t t t τk
We now observe that
E[L (s ,a,b) ]
t τk |Ft
= P(s′;a,s )min max M Q (s ,a ,b ), (s ,0)+γQ (s ,0)
τk
{
1 t t t t
R
t t t+1
}
s X′∈S (cid:16)
ˆ
,M Q(s ,a,b) Q(s ,a)
2 t
−
τk
ˆ (cid:17)
= TQ (s,a) Q(s,a). (10)
t
−
ˆ ˆ
Now, using the fixed point property that implies Q = TQ, we find that
E[L (s ,a,b) ] = TQ (s,a) TQˆ (s,a)
t τk |Ft t
−
ˆ
TQ TQ
t
≤ −
(cid:13) ˆ (cid:13)
γ(cid:13) Q Q (cid:13) = γ Ξ . (11)
≤ (cid:13) t − ∞(cid:13) k t k∞
(cid:13) (cid:13)
(cid:13) (cid:13)
using the contraction property of T establi(cid:13)shed in P(cid:13)roposition 3. This proves (ii).
We now prove iii), that is
Var[L ] c(1+ Ξ 2). (12)
t t t
|F ≤ k k
12Now by equation 10 we have that
Var[L ]
t t
|F
= Var min(max M Q (s ,a ,b ), (s ,0)+γQ (s ,0) ,M Q(s ,a ,b ))
1 t t t t t t t+1 2 t t t
{ R }
h ˆ
Q(s ,a)
t
−
i
= E min(max M Q (s ,a ,b ), (s ,0)+γQ (s ,0) ,M Q(s ,a ,b ))
1 t t t t t t t+1 2 t t t
{ R }
"
2
ˆ ˆ
Q(s ,a) TQ (s,a) Q(s,a)
t t
− − −
! #
(cid:0) (cid:1)
= E min(max M Q (s ,a ,b ), (s ,0)+γQ (s ,0) ,M Q(s ,a ,b ))
1 t t t t t t t+1 2 t t t
{ R }
h(cid:16) 2
TQ (s,a)
t
−
= Var[min(max M Q (s ,a ,b ), (s ,0)+γQ (s ,0) ,M Q(s ,a ,(cid:17)bi))]
1 t t t t t t t+1 2 t t t
{ R }
c(1+ Ξ 2),
t
≤ k k
for some c > 0 where the last line follows due to the boundedness of Q (which follows
from Assumptions 2 and 4). This concludes the proof of the theorem.
4.1 Convergence using Linear Function Approximators
In reinforcement learning, an important consideration is the use of function approxima-
torsforthefunctionsbeinglearnedduringthelearningprocess. Functionapproximators
enable parameterisation of these key functions after which the parameters can be up-
dated according to an RL update rule (Sutton et al., 1999). Neural networks are an
importantcaseoffunctionapproximators. Inwhat follows, westudy ourlearning frame-
work with linear function approximators. Linear function approximators do not offer as
powerful approximation capabilities since the family of neural network function approx-
imators is dense in the function space (the universal approximation theorem (Lu & Lu,
2020)) (Xu et al., 2014). However, despite their deficiencies, linear function approxima-
tors are an important class due to their simplicity and convexity properties that do not
suffer from issues such as convergence to suboptimal stationary points (Jin et al., 2020;
Busoniu et al., 2017). Moreover, the following analysis serves as an important step for
proving analogous results with other function approximator classes.
Definition 4 For any test function ψ L , a projection operator Π acting ψ is defined
2
∈
by the following
Πψ := argmin ψ¯ ψ .
−
ψ¯∈{Φr|r∈Rp}
(cid:13) (cid:13)
(cid:13) (cid:13)
13Theorem 5 Algorithm 1 converges to the stable point of , moreover, given a set of
G
linearly independent basis functions Φ = φ ,...,φ with φ L , k. Algorithm 1
1 p k 2
{ } ∈ ∀
converges to a limit point rˆ Rp which is the unique solution to ΠF(Φrˆ) = Φrˆ where
∈
FΛ := +γP max[min M Λ,Λ ,M Λ]. Moreover, rˆ satisfies the following:
1 2
R { }
ˆ ˆ ˆ
Φrˆ Q (1 γ2)−1/2 ΠQ Q . (13)
− ≤ − −
(cid:13) (cid:13) (cid:13) (cid:13)
The theorem establishes(cid:13)the con(cid:13)vergence of Algo(cid:13)rithm 1 t(cid:13)o minimax equilibrium of
(cid:13) (cid:13) (cid:13) (cid:13)
with the use of linear function approximators. The second statement bounds the
G
proximity of the convergence point by the smallest approximation error that can be
achieved given the choice of basis functions.
The theorem is proven using a set of results that we now establish. First, we prove
the following bound holds:
Lemma 1 For any Q L we have that
2
∈
FQ FQ′ γ Q Q′ , (14)
k − k ≤ k − k
so that the operator F is a contraction.
Proof 2 Now, we first note that by result iv) in the proof of Proposition 3, we deduced
that for any Q,v L we have that
2
∈
M Q ( ,0)+γ 0 v′ γ v v′ , i 1,2 .
i
− R · P ≤ k − k ∀ ∈ { }
Atrivial mo(cid:13) dificationr(cid:2) evealsthatwe ca(cid:3) n(cid:13) deducethat foranyQ,Qˆ L : M Q Qˆ
(cid:13) (cid:13) 2 i
∈ − ≤
γ Q Qˆ . Using the contraction property of M and results i)-iv) of Pr(cid:13) oposition 3,(cid:13)
i (cid:13) (cid:13)
− (cid:13) (cid:13)
we(cid:13)readily(cid:13)deduce the following bound:
(cid:13) (cid:13)
(cid:13) (cid:13) ˆ ˆ ˆ
max M Q Q , M Q M Q γ Q Q , i,j 1,2 . (15)
i i j
− − ≤ − ∀ ∈ { }
n(cid:13) (cid:13) (cid:13) (cid:13)o (cid:13) (cid:13)
We now ob(cid:13)serve that F(cid:13)is(cid:13)a contraction. I(cid:13)ndeed, (cid:13)since for(cid:13)any Q,Q′ L we have
2
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ∈
that:
FQ FQ′
k − k
= +γP max[min M Q,Q ,M Q] ( +γP max[min M Q′,Q′ ,M Q′])
1 2 1 2
kR { } − R { } k
= γ P max[min M Q,Q ,M Q] P max[min M Q′,Q′ ,M Q′]
1 2 1 2
k { } − { } k
γ P max[min M Q,Q ,M Q] max[min M Q′,Q′ ,M Q′]
1 2 1 2
≤ k kk { } − { } k
γmax min M Q,Q min M Q′,Q′ , M Q min M Q′,Q′
1 1 2 1
≤ k { }− { }k k − { }k
(
, M Q′ min M Q,Q , M Q′ M Q
2 1 2 1
k − { }k k − k
)
γmax Q Q′ ,γ Q Q′
≤ {k − k k − k}
= γ Q Q′
k − k
14using the Cauchy-Schwarzinequality, equation15 andagainusing the non-expansiveness
of P.
We next show that the following two bounds hold:
Lemma 2 For any Q we have that
∈ V
i) ΠFQ ΠFQ¯ γ Q Q¯ ,
− ≤ −
(cid:13) ˆ (cid:13) (cid:13) ˆ ˆ(cid:13)
ii) (cid:13)Φrˆ Q (cid:13)1 (cid:13)ΠQ Q(cid:13) .
− ≤ √1−γ2 −
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
Proof 3 The fi(cid:13)rst result(cid:13)is straightfo(cid:13)rward sin(cid:13)ce as Π is a projection it is non-expansive
and hence:
ΠFQ ΠFQ¯ FQ FQ¯ γ Q Q¯ ,
− ≤ − ≤ −
using the
contractio(cid:13)
n property of
F(cid:13)
.
T(cid:13)
his
proves(cid:13)
i).
Fo(cid:13)
r ii),
w(cid:13)
e note that by the or-
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
ˆ ˆ
thogonality property of projections we have that Φrˆ ΠQ,Φrˆ ΠQ = 0, hence we
− −
observe that: D E
Φrˆ Qˆ 2 Φrˆ ΠQˆ 2 + Qˆ ΠQˆ 2
− ≤ − −
(cid:13) (cid:13) (cid:13) (cid:13) = (cid:13) (cid:13)ΠFΦrˆ Π(cid:13) (cid:13)Qˆ 2(cid:13) (cid:13)+ Qˆ Π(cid:13) (cid:13)Qˆ 2
(cid:13) (cid:13) (cid:13) − (cid:13) (cid:13) − (cid:13)
(cid:13)
(cid:13)FΦrˆ
Qˆ 2 +(cid:13)
(cid:13)
Qˆ(cid:13)
(cid:13)
ΠQˆ 2 (cid:13)
(cid:13)
≤ (cid:13) − (cid:13) (cid:13)− (cid:13)
=
(cid:13)
(cid:13)FΦrˆ
FQˆ(cid:13)
(cid:13)
2 +(cid:13)
(cid:13) Qˆ
ΠQˆ(cid:13)
(cid:13)
2
(cid:13) (cid:13) (cid:13) (cid:13)
− −
(cid:13)
(cid:13)γ2 Φrˆ
Qˆ(cid:13) (cid:13)2
+
(cid:13)
(cid:13)Qˆ
ΠQˆ(cid:13) (cid:13)2
,
≤ (cid:13) − (cid:13) (cid:13) − (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
after which we readily deduce the desir(cid:13)ed result(cid:13). (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
Lemma 3 Define the operator H by the following:
M Q (s,a), if M Q (s,a) > Φrˆ> M Q (s,b),
1 1 1 1 2 2
−
HQ(s,a,b) = M Q (s,a), if M Q (s,a) > Φrˆ> M Q (s,b),
 2 2 2 2 1 1
−
 Q(s,0), otherwise,
where we define F˜ by: F˜ Q := +γPHQ.
For any Q,Q¯ L we havR e that
2
∈
F˜Q F˜Q¯ γ Q Q¯ (16)
− ≤ −
(cid:13) (cid:13)
(cid:13) (cid:13)
and hence F˜ is a contraction m(cid:13) (cid:13)apping. (cid:13) (cid:13) (cid:13) (cid:13)
15Proof 4 Using equation 15, we now observe that
F˜Q F˜Q¯
−
=(cid:13) +γPH(cid:13)Q +γPHQ¯
(cid:13) (cid:13)
(cid:13)R (cid:13) − R
γ HQ HQ¯
(cid:13) (cid:0) (cid:1)(cid:13)
≤ (cid:13) − (cid:13)
γ (cid:13)max M Q(cid:13) M Q¯,M Q Q¯,M Q¯ Q,M Q M Q¯,Q Q¯
≤ (cid:13) 1 (cid:13)− 1 1 − 1 − 2 − 2 −
(cid:13) n
(cid:13) ,M Q Q¯,M Q¯ Q
(cid:13) 2 − 2 −
o(cid:13)
γmax M Q M Q¯ , M Q Q¯ , M Q¯ Q , M Q M(cid:13)Q¯ , Q Q¯
≤
1
−
1 1
−
1
−
2
−
2(cid:13)
−
n
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ,(cid:13) M(cid:13) Q Q¯ , M(cid:13) Q(cid:13)¯ Q (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13)2 (cid:13) (cid:13)
− −
γmax γ Q Q¯ , Q Q¯ (cid:13) (cid:13) (cid:13) (cid:13)o
≤ − − (cid:13) (cid:13) (cid:13) (cid:13)
= γ Q Q¯ ,
(cid:8) (cid:13) (cid:13) (cid:13) (cid:13)(cid:9)
− (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
again using equation 15 and the non-expansive property of P.
(cid:13) (cid:13)
Lemma 4 Define by
Q˜
:=
+γPvσ˜
where
R
vσ˜ (s) := min max M 1Qσ1,σ2 (s,a), (s,0)+γE s′∼P vσ1,σ2 (s′) ,M 2Qσ1,σ2 (s,a) ,
R
" #
n h io
(17)
then
Q˜
is a fixed point of
F˜Q˜
, that is
F˜Q˜
=
Q˜
.
Proof 5 We begin by observing that
HQ˜ (s,a) = H (s,a)+γ a vσ˜ (s′)
R
Pss′
M (cid:0) 1Q 1(s,a), if M 1Q(cid:1)1(s,a) > Φrˆ> M 2Q 2(s,b),
−
= M Q (s,a), if M Q (s,a) > Φrˆ> M Q (s,b),
 2 2 2 2 1 1
−
 Q(s,0), otherwise,
M Q (s,a), if M Q (s,a) > Φrˆ> M Q (s,b),
 1 1 1 1 2 2
−
= M Q (s,a), if M Q (s,a) > Φrˆ> M Q (s,b),
 2 2 2 2 1 1
−
  (s,0)+γPvσ˜, otherwise,
R
σ˜
= v (s).

Hence,
F˜Q˜
=
+γPHQ˜
=
+γPvσ˜
=
Q˜.
(18)
R R
which proves the result.
16Lemma 5 The following bound holds:
E vσˆ (s ) E vσ˜ (s ) 2 (1 γ) (1 γ2) −1 ΠQˆ Qˆ . (19)
0 0
− ≤ − − −
Proof 6 By d(cid:2)efinitio(cid:3)ns of(cid:2)vσˆ and(cid:3)vσ˜ (h c.f equatp ion 17) ani d u(cid:13) (cid:13)sing Jens(cid:13) (cid:13)en’s inequality
(cid:13) (cid:13)
and the stationarity property we have that,
E vσˆ (s ) E vσ˜ (s ) = E Pvσˆ (s ) E Pvσ˜ (s )
0 0 0 0
− −
E Pvσˆ (s ) E Pvσ˜ (s )
(cid:2) (cid:3) (cid:2) (cid:3) (cid:2) 0(cid:3) (cid:2) 0(cid:3)
≤ −
σˆ σ˜
Pv Pv . (20)
(cid:12) (cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
≤ (cid:12) − (cid:12)
Now recall that Q˜ := + γPvσ˜ and Qˆ (cid:13):= + γPv(cid:13)σ⋆ , using these expressions in
R (cid:13) R (cid:13)
equation 20 we find that
E vσˆ (s ) E vσ˜ (s ) 1 Q˜ Qˆ .
0 − 0 ≤ γ −
(cid:13) (cid:13)
(cid:2) (cid:3) (cid:2) (cid:3)
Moreover, by the triangle inequality and using the f(cid:13) act that(cid:13)F(Φrˆ) = F˜(Φrˆ) and that
(cid:13) (cid:13)
FQˆ
=
Qˆ
and
FQ˜
=
Q˜
(c.f. equation 19) we have that
Q˜ Qˆ Q˜ F(Φrˆ) + Qˆ F˜ (Φrˆ)
− ≤ − −
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)γ Q˜ Φrˆ (cid:13) (cid:13)+γ(cid:13) (cid:13)Qˆ Φrˆ (cid:13) (cid:13)
(cid:13) (cid:13) ≤ (cid:13) − (cid:13) (cid:13) − (cid:13)
2γ(cid:13) (cid:13) Q˜ Φ(cid:13) (cid:13)rˆ +γ(cid:13) (cid:13) Qˆ Q˜(cid:13) (cid:13) ,
≤ (cid:13) − (cid:13) (cid:13) − (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
which gives the following bound: (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
Q˜ Qˆ 2γ(1 γ)−1 Q˜ Φrˆ ,
− ≤ − −
(cid:13) (cid:13) (cid:13) (cid:13)
fromwhich, usingLemma(cid:13) (cid:13)2, wede(cid:13) (cid:13)duce that Q˜ Qˆ(cid:13) (cid:13) 2γ (cid:13) (cid:13)(1 γ) (1 γ2) −1 ΠQˆ Qˆ ,
− ≤ − − −
after which by equation 21, we finally obtain(cid:13) (cid:13) h p i (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
E vσˆ (s ) E vσ˜ (s ) 2 (1(cid:13) γ) ((cid:13) 1 γ2) −1 ΠQˆ Qˆ , (cid:13) (cid:13)
0 0
− ≤ − − −
as required. (cid:2) (cid:3) (cid:2) (cid:3) h p i (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
Let us rewrite the update in the following way:
r = r +γ Ξ(w ,r ),
t+1 t t t t
where the function Ξ : R2d Rp Rp is given by:
× →
Ξ(w,r) := φ(s)( (s, )+γmax min (Φr)(s′),M (Φr)(s′) ,M (Φr)(s′) (Φr)(s)),
1 2
R · { { } }−
for any w (s,s′) 2 and forany r Rp. Let us also define the function Ξ : Rp Rp
≡ ∈ S ∈ →
by the following:
Ξ(r) := E [Ξ(w ,r)];w := (s ,z ).
w0∼(P,P) 0 0 0 1
17Lemma 6 The following statements hold for all z 0,1 :
∈ { }×S
i) (r rˆ)Ξ (r) < 0, r = rˆ,
k
− ∀ 6
ii) Ξ (rˆ) = 0.
k
Proof 7 To prove the statement, we first note that each component of Ξ (r) admits a
k
representation as an inner product, indeed:
Ξ (r) = E[φ (s )( (s ,a )+γmax min (Φr)(s ),M (Φr)(s ) ,M (Φr)(s ) (Φr)(s )]
k k 0 0 0 1 1 1 2 1 0
R { { } }−
= E[φ (s )( (s ,a )+γE[max min (Φr)(s ),M (Φr)(s ) ,M (Φr)(s ) x ] (Φr)(s )]
k 0 0 0 1 1 1 2 1 0 0
R { { } }| −
= E[φ (s )( (s ,a )+γP max min (Φr),M (Φr) ,M (Φr) (s ) (Φr)(s )]
k 0 0 0 1 2 0 0
R { { } } −
= φ ,FΦr Φr ,
k
h − i
using the iterated law of expectations and the definitions of P and F.
We now are in a position to prove i). Indeed, we now observe the following:
(r rˆ)Ξ (r) = (r(l) rˆ(l)) φ ,FΦr Φr
k l
− − h − i
l=1
X
= Φr Φrˆ,FΦr Φr
h − − i
= Φr Φrˆ,(1 Π)FΦr +ΠFΦr Φr
h − − − i
= Φr Φrˆ,ΠFΦr Φr ,
h − − i
where in the last step we used the orthogonality of (1 Π). We now recall that ΠFΦrˆ=
−
Φrˆ since Φrˆ is a fixed point of ΠF. Additionally, using Lemma 2 we observe that
ΠFΦr Φrˆ γ Φr Φrˆ . With this, we now find that
k − k ≤ k − k
Φr Φrˆ,ΠFΦr Φr
h − − i
= Φr Φrˆ,(ΠFΦr Φrˆ)+Φrˆ Φr
h − − − i
Φr Φrˆ ΠFΦr Φrˆ Φrˆ Φr 2
≤ k − kk − k−k − k
(γ 1) Φrˆ Φr 2,
≤ − k − k
which is negative since γ < 1 which completes the proof of part i).
The proof of part ii) is straightforward since we readily observe that
Ξ (rˆ) = φ ,FΦrˆ Φr = φ ,ΠFΦrˆ Φr = 0,
k l l
h − i h − i
as required and from which we deduce the result.
To prove the theorem, we make use of a special case of the following result:
Theorem 6 (Th. 17, p. 239 in (Benveniste et al., 2012)) Consider a stochastic
process r : R Ω Rk which takes an initial value r and evolves according to
t 0
×{∞}× →
the following:
r = r +αΞ(s ,r ), (21)
t+1 t t t
for some function s : R2d Rk Rk and where the following statements hold:
× →
181. s t = 0,1,... is a stationary, ergodic Markov process taking values in R2d
t
{ | }
2. For any positive scalar q, there exists a scalar µ such that E[1+ s q s s ]
q t 0
k k | ≡ ≤
µ (1+ s q)
q
k k
3. The step size sequence satisfies the Robbins-Monro conditions, that is ∞ α =
t=0 t
and ∞ α2 <
∞ t=0 t ∞ P
P
4. There exists scalars d and q such that Ξ(w,r) d(1+ w q)(1+ r )
k k ≤ k k k k
5. There exists scalars d and q such that ∞ E[Ξ(w ,r) z z] E[Ξ(w ,r)]
t=0k t | 0 ≡ − 0 k ≤
d(1+ w q)(1+ r )
k k k k P
6. There exists a scalar d > 0 such that E[Ξ(w ,r)] E[Ξ(w ,r¯)] d r r¯
0 0
k − k ≤ k − k
7. Thereexists scalarsd > 0and q > 0such that ∞ E[Ξ(w ,r) w w] E[Ξ(w ,r¯)]
t=0k t | 0 ≡ − 0 k ≤
c r r¯ (1+ w q)
k − k k k P
8. There exists some rˆ Rk such that Ξ(r)(r rˆ) < 0 for all r = rˆ and s¯(rˆ) = 0.
∈ − 6
Then r converges to rˆ almost surely.
t
In order to apply the Theorem 6, we show that conditions 1 - 7 are satisfied.
Proof 8 Conditions 1-2 are true by assumption while condition 3 can be made true by
choice of the learning rates. Therefore it remains to verify conditions 4-7 are met.
To prove 4, we observe that
Ξ(w,r) = φ(s)( (s, )+γmax min (Φr)(s′),M (Φr)(s′) ,M (Φr)(s′) (Φr)(s))
1 2
k k k R · { { } }− k
φ(s) (s, )+γ( φ(s′) r +M Φ(s′)+M Φ(s′)) + φ(s) r
1 2
≤ k kkR · k kk k k k kk k
φ(s) ( (s, ) +γ M Φ(s′) +γ M Φ(s′) )+ φ(s) (γ φ(s′) + φ(s) ) r .
1 2
≤ k k kR · k k k k k k k k k k k k k
Now using the definition of M , we readily observe that M Φ(s′) c + +
i i ∞
γ σ Φ c + +γ Φ for i = 1,2 using the nok n-expansivk en≤ esk s k of P. kRk
kPs′st
k ≤ k
k∞
kRk k k
Hence, we lastly deduce that
Ξ(w,r) φ(s) ( (s, ) +γ M Φ(s′) +γ M Φ(s′) )+ φ(s) (γ φ(s′) + φ(s) ) r
1 2
k k ≤ k k kR · k k k k k k k k k k k k k
φ(s) ( (s, ) +2γ( c + + φ ))+ φ(s) (γ φ(s′) + φ(s) ) r ,
∞
≤ k k kR · k k k kRk | k k k k k k k k k
we then easily deduce the result using the boundedness of φ and .
R
19Now we observe the following Lipschitz condition on Ξ:
Ξ(w,r) Ξ(w,r¯)
k − k
= φ(s) γmax min (Φr)(s′),M Φ(s′) ,M Φ(s′) γmax min (Φr¯)(s′),M Φ(s′) ,
1 2 1
{ { } }− { }
(cid:13) (cid:16)
(cid:13) M Φ(s(cid:8)′) ((Φr)(s) Φr¯(s))
(cid:13) 2 − −
(cid:17) (cid:13)
γ φ(s) max min φ′(s′)r,M Φ′(s′) ,M Φ′(s′) (cid:9) (cid:13)
≤ k k { { 1 } 2 } (cid:13)
(cid:13)
(cid:13) max min (φ′(s′)r¯),M Φ′(s′) ,M Φ′(s′) + φ(s) φ′(s)r φ(s)r¯
(cid:13) − { { 1 } 2 } k kk − k
γ φ(s) min φ′(s′)r,M Φ′(s′) min (φ′(s′)r¯),M Φ′((cid:13)s′) + φ(s) φ′(s)r φ(s)r¯
1 1 (cid:13)
≤ k kk { }− { (cid:13) }k k kk − k
γ φ(s) φ′(s′)r φ′(s′)r¯ + φ(s) φ′(s)r φ′(s)r¯
≤ k kk − k k kk − k
φ(s) (γ φ′(s′) + φ′(s) ) r r¯
≤ k k k k k k k − k
const. r r¯ ,
≤ k − k
using Cauchy-Schwarz inequality and that for any scalars a,b,c we have that
max a,b max b,c a c and min a,b min b,c a c , which proves
| { }− { }| ≤ | − | | { }− { }| ≤ | − |
Part 6.
Using Assumptions 3 and 4, we therefore deduce that
∞
E[Ξ(w,r) Ξ(w,r¯) w = w] E[Ξ(w ,r) Ξ(w ,r¯) ] const. r r¯ (1+ w l).
0 0 0
k − | − − k ≤ k − k k k
t=0
X
(22)
which proves Part 7.
Part 2 is assured by Lemma 2 while Part 4 (and hence Part 5) is assured by Lemma
5 and lastly Part 8 is assured by Lemma 6. This result completes the proof of Theorem
2.
5 Intervention Budget Constraints
So far we have considered the case in which each player chooses a strategy that deter-
mines the conditions for the set of states in which the player will execute an action.
The action cost function plays a critical role in influencing the willingness of the player
to execute an action. In this section, we consider an alternative setting in which the
players face a budgetary constraint on the actions executed. A degenerate case of the
budgetary formalism is a constraint on the number of actions the player can execute
over thehorizon. Weshow that bytracking its remaining budget theAlgorithm2frame-
work is able to learn a policy that makes optimal usage of its budget while respecting
the budget constraint almost surely.
20The program for the new SG is given by the following for any s :
∈ S
∞
vσ1,σ2 (s) vσ′1,σ2 (s) s. t. n δt 0, σ′1 Σ1, σ2 Σ2 (23)
≥ 1 − τk ≥ ∀ ∈ ∀ ∈
t=0 k≥1
XX
∞
vσ1,σ2 (s) vσ1,σ′2 (s) s. t. n δt 0, σ1 Σ1, σ′2 Σ2 (24)
≥ 2 − ρr ≥ ∀ ∈ ∀ ∈
t=0 r≥1
XX
where for each i 1,2 , the quantity n N is a fixed value that represents the
i
∈ { } ∈
maximum number of allowed interventions and δt ( δt ) is equal to one if a
k≥1 τk r≥1 ρr
Player 1 (Player 2) action was applied at time t and zero if it was not.
P P
Hence, the value of the gamefor the new SG adheres to the following programs :
∈ S
v(s) = max min
vσ1,σ2
(s) = min max
vσ1,σ2
(s)
σ1∈Σ1σ2∈Σ2 σ2∈Σ2σ1∈Σ1
∞ ∞
s. t. n δt 0,n δt 0, (25)
1 − τk ≥ 2 − ρr ≥
t=0 k≥1 t=0 r≥1
XX XX
To do this, we combine the above impulse control technology with state augmentation
technique (Sootla et al., 2022). As in (Sootla et al., 2022; Mguni et al., 2023b), we
introducenewvariablesy andz thattracktheremaining number ofactionsforPlayer 1
t t
andPlayer 2respectively sothat: y := n δt andz := n δt
t 1 − t≥0 k≥1 τk t 2 − t≥0 r≥1 ρr
where the variables y and z are treated as new state variables which are components in
t t
an augmented state space X := N2. WeP introP duce the associated rewP ard fP unctions
S ×
: X A (D) and the probability transition function P : X A X [0,1]
R × → P × × →
whose state space input is now replaced by X for the game = , , , ,P˜, ˜,γ .
G hN S A B R i
Wee now prove Algorithm 2 ensures generates best-response meinimax strategies within
the constrained SG . e
G
s P( s ,a ), y = y δt , y = n , z = z δt , z = n .
t+1 ∼ ·| t t t+1 t − τk 0 1 t+1 t − τk 0 2
k≥1 k≥1
X X
x ( x ,a ), x (s ,y ,z ) X (26)
t+1 t t t t t t
∼ P ·| ≡ ∈
where the output is the sampled tuple [P( s ,a ),y δt ,z δt ].
P e ·| t t t − k≥1 τk t − k≥1 τk
To avoid violations, the reward function can be reshaped in the following manner
P P
e (s ,a ,b ) y ,z 0,
t t t t t
R ≥
y < 0,z > 0,
(s ,y ,z ,a ,b ) =  −∞ t t
t t t t t
R    ∞ y t > 0,z t < 0,
e 0 y t,z t < 0,
The value function i s now given by the following expression:


∞
vσ1,σ2 (x) = E γt (x ,a ,b ) a σ1( x ),b σ2( x ),x = (s ,y ,z ) , (27)
t t t t t t t t t t t
R | ∼ ·| ∼ ·|
" #
t=0
X
e e e e
21where the strategy for Player 1 and Player 2 now depends on the variable y and z
t t
respectively. Note that in Equation 26 is a Markov process and, the rewards are
P R
bounded. Therefore, we can apply directly the results for impulse control to this case
as well. We denote the aeugmented SG by = X, , ,R,γ . We have the followeing.
G h A P i
Theorem 7 Consider the budgeted problem , then:
e G e e
a) The Bellman equation holds, i.e. there exists a function v˜ such that for any
x X e
∈
v˜(x) = min max M 1Q(x,a,b), R(x,0)+γE x′∼Pe[v˜(x′)] ,M 2Q˜ (x,a,b) (28)
" #
n o
e e
where each player i has a best-response policy that takes the form σ( x);
G ·|
b) Given a function v : R, the stable point solution for is a given by
S ×Z → G
limT˜kvσ˜1,σ˜2
= max min
vσˆ1,σ˜2e
= min
maxvσˆ1,σˆ2 =e vˆ
, (29)
k→∞ e σ˜1∈Σ˜1σ˜2∈Σ˜2 σ˜2∈Σ˜2σ˜1∈Σ˜1 e
where
(σˆ˜1 ,σˆ˜2
) is an eequilibrium and
T˜
eis the Bellman opereator of e.
G G
Theresulthasseveralimportantimplications. Thefirstisthatwecanuseamodified
e e
version of Algorithm 1 to obtain the solution of the problem while guaranteeing con-
vergence (under standard assumptions). Secondly, the state augmentation procedure
admits a Markovian representation of the best response strategies.
Theorem 7 shows Algorithm 2 converges to the minimax equilibrium when the
players face an action-budget constraint.
6 Discussion on Subcases
As discussed earlier, stochastic games with minimally bounded action costs studied
in this paper have a number of applications within economics, financial systems and
computer science. In addition to direct application, there are a number of important
subcases that emerge as degenerate cases of our general framework.
Games of Impulse Control and Stopping (Mguni, 2018; Campi & De Santis,
2020)
In this setting, two players engage in a zero sum game in which one player’s actions
are subject to minimally bounded costs (thus the player executes their actions using
impulse controls) and the other player’s only action is to decide when to stop the game.
This induces a game of impulse control and stopping. This is a degenerate case of the
stochastic game with minimally bounded costs when one of the players faces an action
budget that caps the number of allowed actions to 1.
A general case of this includes a flow payoff which is a payment received by the
players after terminating the game. In particular, the game degenerates into this
G
case
22Algorithm 2:
1: Input: Constant ǫ 0,
≥
2: Initialise: Q-function, Q
0
3: repeat
4: n 0
←
5: for t = 0,1,... do
6: Compute a argmaxQ (x ,0,b ),b argminQ (x ,a ,0)
t n t t t n t t
∈ ∈
7: if M Q Q then
1 n n
≥
8: Apply a so x e( x ,a ,0), e
t t+1 t t
∼ P ·|
9: Receieve reweards r = (x ,a ,0)
t t t
R
10: else e
11: if M Q Q thene
2 n n
≤
12: Apply b so x ( x ,0,b ), ‘
t t+1 t t
∼ P ·|
13: Receieve reweards r = (s ,0,b )
t t t
R
14: else e
15: Apply no action so x e ( x ,0),
t+1 t
∼ P ·|
16: Receive rewards r = (x ,0).
t t
R
17: end if e
18: end if e
19: end for
20: // Learn
Qˆ
21: Update Q function according to the update rule equation 7
n
22: until Q (x,a) Q (x,a) ǫ
n n−1
| − | ≤
e
e e
Dynkin games (Optimal Stopping Games) (Ekström & Peskir, 2008; Martyr,
2016, 2015)
Inthis setting, two players engage in a zero sum gamein which each player’s only action
is to decide when to stop the game. When this action is executed by a player, the game
is arrested for both players at which point the players receive a terminal payoff thus the
players must choose a time to stop which maximises their individual expected payoff
contingent on the decision of the other player which gives rise to a strategic interaction.
This is a degenerate case of the stochastic game with minimally bounded costs. A
general case of this includes a flow payoff which is a payment received by the players
after terminating the game. In particular, the game degenerates into this case when
G
the number of actions for each player is capped at 1.
Optimal Stopping Problems. An obvious yet well-known degenerate case of this sub-
problem are optimal stopping problems (in discrete-time). This case arises when one
of the player’s strategies is fixed in the optimal stopping game.
23Impulse control (in discrete-time) (Bensoussan, 2008)
An obvious yet well-known degenerate case of this subproblem are impulse Control
problems (in discrete-time). This case arises when one of the player’s strategies is fixed
in the game .
G
7 Related Work
In continuous-time optimal control theory (Øksendal, 2003; Todorov, 2006), problems
in which the player faces a cost for each action are tackled with a form of policy known
as impulse control (Belak et al., 2017; Mguni, 2018). In impulse control frameworks,
the dynamics of the system are modified through a sequence of discrete actions or
bursts chosen at times that the player chooses to apply the control policy. This dis-
tinguishes impulse control models from classical decision methods in which a player
takes actions at each time step while being tasked with the decision of only which
action to take. Impulse control models represent appropriate modelling frameworks
for financial environments with transaction costs, liquidity risks and economic environ-
ments in which players face fixed adjustment costs (e.g. menu costs) (Midrigan, 2011).
However, as yet the literature on learning environments with impulse control remains
sparse (Mguni et al., 2023b) and extension to the multi-player settings with learning is
at present absent.
The current setting is intimately related to the optimal stopping problem which
widelyoccursinfinance,economics,andcomputerscience(Mguni,2019;Tsitsiklis & Van Roy,
1999). In the optimal stopping problem, the task is to determine a criterion that deter-
mines when to arrest the system and receive a terminal reward. In this case, standard
(MA)RL methods are unsuitable since they require an expensive sweep (through the
set of states) to determine the optimal point to arrest the system. The impulse con-
trol problem can be viewed as an augmented problem of optimal stopping since the
player must now determine both a sequence of points to perform an action or inter-
vene and their optimal magnitudes — only acting when the cost of action is justified
(Øksendal & Sulem, 2019). Adapting RL to tackle optimal stopping problems has been
widely studied (Tsitsiklis & Van Roy, 1999; Becker et al., 2018; Chen et al., 2020) and
appliedtoavarietyofreal-worldsettingswithinfinance(Fathan & Delage,2021),radia-
tion therapy (Ajdari et al., 2019) and network operating systems (Alcaraz et al., 2020).
Closely related to impulse control, switching control structures have been studied in
learning contexts to improve learning efficiency in reinforcement learning (Mguni et al.,
2022, 2023a,c). ROSA (Mguni et al., 2023a) and LIGS (Mguni et al., 2022) incorpo-
rate a dual switching method to activate a reward-shaping module to promote state
visitations and coordination between adaptive agents in an RL and MARL respectively.
LICRA (Mguni et al., 2023b) adds a trainable switch to decide whether to use a costly
execution-policy system to generate actions. Similarly, MANSA (Mguni et al., 2023c)
has an additional switch to decide whether to activate centralised training, a computa-
tionally expensive learning mode that facilitates coordination among adaptive agents.
248 Conclusion
We presented a novel method to tackle the problem of learning how to select when
to act in addition to learning which actions to execute. Our framework, which is a
generaltoolfortackling problems ofthiskindseamlessly adoptsRLalgorithmsenabling
them to efficiently tackle problems in which the player must be selective about when
it executes actions. This is of fundamental importance in practical settings where
performing many actions over the horizon can lead to costs and undermine the service
life of machinery. We demonstrated that our solution, our framework which at its core
has a sequential decision structure that first decides whether or not an action ought
to be taken under the action policy can solve tasks where the player faces costs with
extreme efficiency as compared to leading reinforcement learning methods. In some
tasks, we showed that our framework is able to solve problems that are unsolvable
using current reinforcement learning machinery. We envisage that this framework can
serve as the basis for extensions to different settings including adversarial training for
solving a variety of problems within RL.
References
AliAjdari, Maximilian Niyazi, Nils Henrik Nicolay, Christian Thieke, Robert Jeraj, and
Thomas Bortfeld. Towards optimal stopping in radiation therapy. Radiotherapy and
Oncology, 134:96–100, 2019.
Stefano V. Albrecht, Filippos Christianos, and Lukas Schäfer. Multi-Agent Reinforce-
ment Learning: Foundations and Modern Approaches. MIT Press, 2024. URL
https://www.marl-book.com.
JuanJAlcaraz, JoseAAyala-Romero,JavierVales-Alonso, andFernandoLosilla-López.
Online reinforcement learning for adaptive interference coordination. Transactions
on Emerging Telecommunications Technologies, 31(10):e4087, 2020.
Douglas W Allen. What are transaction costs? Rsch. in L. & Econ., 14:1, 1991.
Parsiad Azimzadeh. Impulse control in finance: Numerical methods and viscosity solu-
tions. arXiv preprint arXiv:1712.01647, 2017.
Sebastian Becker, Patrick Cheridito, and Arnulf Jentzen. Deep optimal stopping. arXiv
preprint arXiv:1804.05394, 2018.
Christoph Belak, Soren Christensen, and Frank Thomas Seifried. A general verifica-
tion result for stochastic impulse control problems. SIAM Journal on Control and
Optimization, 55(2):627–649, 2017.
Alain Bensoussan. Impulse control in discrete time. 2008.
25Albert Benveniste, Michel Métivier, and Pierre Priouret. Adaptive algorithms and
stochastic approximations, volume 22. Springer Science & Business Media, 2012.
Dimitri P Bertsekas. Approximate dynamic programming. Athena scientific Belmont,
2012.
Sid Browne. Stochastic differential portfolio games. Journal of Applied Probability, 37
(1):126–147, 2000.
Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst. Reinforcement
learning and dynamic programming using function approximators. CRC press, 2017.
Luciano Campi and Davide De Santis. Nonzero-sum stochastic differential games be-
tween an impulse controller and a stopper. Journal of Optimization Theory and
Applications, 186:688–724, 2020.
Shuhang Chen, Adithya M Devraj, Ana Bušić, and Sean Meyn. Zap q-learning for´
optimal stopping. In 2020 American Control Conference (ACC), pp. 3920–3925.
IEEE, 2020.
Ilya RPCuypers, Jean-FrançoisHennart, BrianSSilverman, andGokhanErtug. Trans-
action cost theory: Past progress, current challenges, and suggestions for the future.
Academy of Management Annals, 15(1):111–150, 2021.
Mark H.A Davis, Xin Guo, and Guoliang Wu. Impulse control of multidimensional
jump diffusions. SIAM Journal on Control and Optimization, 48(8):5276–5293, 2010.
Kenneth R. Deal. Optimizing advertising expenditures in a dynamic duopoly. Opera-
tions Research, 27(4):682–692, 1979.
Xiaotie Deng, Yuhao Li, David Henry Mguni, Jun Wang, and Yaodong Yang. On the
complexity of computing markov perfect equilibrium ingeneral-sumstochastic games.
arXiv preprint arXiv:2109.01795, 2021.
Erik Ekström and Goran Peskir. Optimal stopping games for markov processes. SIAM
Journal on Control and Optimization, 47(2):684–702, 2008.
Gary M. Erickson. Differential game models of advertising competition. European
Journal of Operational Research, 83(3):431–438, 1995.
AbderrahimFathanandErickDelage. Deepreinforcementlearningforoptimalstopping
with application in financial engineering. arXiv preprint arXiv:2105.08877, 2021.
Michael A Goldstein, Paul Irvine, Eugene Kandel, and Zvi Wiener. Brokerage com-
missions and institutional trading patterns. The Review of Financial Studies, 22(12):
5175–5212, 2009.
26Lars Peter Hansen and Thomas J Sargent. Acknowledging misspecification in macroe-
conomic theory. Review of Economic Dynamics, 4(3):519–535, 2001.
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. Convergence of stochas-
tic iterative dynamic programming algorithms. In Advances in neural information
processing systems, pp. 703–710, 1994.
Monique Jeanblanc-Picqué. Impulse Control Method and Ex-
change Rate. Mathematical Finance, 3(2):161–177, 1993. URL
https://ideas.repec.org/a/bla/mathfi/v3y1993i2p161-177.html.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient re-
inforcement learning with linear function approximation. In Conference on learning
theory, pp. 2137–2143. PMLR, 2020.
Paul Krugman. How did economists get it so wrong? New York Times, 2(9):2009,
2009.
YulongLuandJianfengLu. Auniversalapproximationtheoremofdeepneuralnetworks
for expressing probability distributions. Advances in neural information processing
systems, 33:3094–3105, 2020.
Randall Martyr. Optimal prediction games in local electricity markets. PhD thesis,
University of Manchester, 2015.
RandallMartyr. Solvingfinitetimehorizondynkingamesbyoptimalswitching. Journal
of Applied Probability, 53(4):957–973, 2016.
David Mguni. A viscosity approach to stochastic differential games of control and
stopping involving impulsive control. arXiv preprint arXiv:1803.11432, 2018.
David Mguni. Cutting your losses: Learning fault-tolerantcontrolandoptimalstopping
under adverse risk. arXiv preprint arXiv:1902.05045, 2019.
David Mguni, Taher Jafferjee, Jianhong Wang, Nicolas Perez-Nieves, Wenbin Song,
Feifei Tong, Matthew Taylor, Tianpei Yang, Zipeng Dai, Hui Chen, et al. Learning to
shape rewards using a game of two partners. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 37, pp. 11604–11612, 2023a.
David Mguni, Aivar Sootla, Juliusz Ziomek, Oliver Slumbers, Zipeng Dai, Kun Shao,
and Jun Wang. Timing is Everything: Learning to act selectively with costly actions
and budgetary constraints. In In International Conference on Learning Representa-
tions, 2023b.
David Henry Mguni, Taher Jafferjee, Jianhong Wang, Nicolas Perez-Nieves, Oliver
Slumbers, Feifei Tong, Yang Li, Jiangcheng Zhu, Yaodong Yang, and Jun Wang.
LIGS: Learnable intrinsic-reward generation selection for multi-agent learning. In
International Conference on Learning Representations, 2022.
27David Henry Mguni, Haojun Chen, Taher Jafferjee, Jianhong Wang, Longfei Yue, Xi-
dong Feng, Stephen Marcus Mcaleer, Feifei Tong, Jun Wang, and Yaodong Yang.
MANSA: Learning fast and slow in multi-agent systems. In Proceedings of the 40th
InternationalConferenceon Machine Learning,volume202ofProceedingsof Machine
Learning Research, pp. 24631–24658. PMLR, 23–29 Jul 2023c.
Virgiliu Midrigan. Menu costs, multiproduct firms, and aggregate fluctuations. Econo-
metrica, 79(4):1139–1180, 2011.
Bernt Øksendal. Stochastic differential equations. In Stochastic differential equations,
pp. 65–84. Springer, 2003.
Bernt Øksendal and Agnès Sulem. Approximating impulse control by iterated optimal
stopping. In Applied Stochastic Control of Jump Diffusions, pp. 255–272. Springer,
2019.
Bernt KarstenØksendal andAgnès Sulem. Applied stochastic control of jump diffusions,
volume 498. Springer, 2005.
Ashutosh Prasad and Suresh P. Sethi. Competitive advertising under uncertainty: A
stochastic differential game approach. Journal of Optimization Theory and Applica-
tions, 123(1):163–185, 2004.
Suresh P Sethi. Optimal control of the vidale-wolfe advertising model. Operations
research, 21(4):998–1013, 1973.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39
(10):1095–1100, 1953.
Katerina Simons et al. Model error. New England Economic Review, pp. 17–28, 1997.
Eilon Solan and Nicolas Vieille. Stochastic games. Proceedings of the National Academy
of Sciences, 112(45):13743–13746, 2015.
Aivar Sootla, Alexander I. Cowen-Rivers, Taher Jafferjee, Ziyan Wang, David Mguni,
Jun Wang, and Haitham Bou-Ammar. Sauté RL: Almost surely safe reinforcement
learning using state augmentation. arXiv preprint arXiv:2202.06558, 2022.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT
press, 2018.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy
gradient methods for reinforcement learning with function approximation. Advances
in neural information processing systems, 12, 1999.
Csaba Szepesvári. Algorithms for reinforcement learning. Springer nature, 2022.
Emanuel Todorov. Optimal control theory. 2006.
28John N Tsitsiklis and Benjamin Van Roy. Optimal stopping of markov processes:
Hilbert space theory, approximation algorithms, and an application to pricing high-
dimensional financial derivatives. IEEE Transactions on Automatic Control, 44(10):
1840–1851, 1999.
Daniel F Waggoner and Tao Zha. Confronting model misspecification in macroeco-
nomics. Journal of Econometrics, 171(2):167–184, 2012.
XinXu, LeiZuo, andZhenhua Huang. Reinforcement learning algorithmswithfunction
approximation: Recent advances and applications. Information sciences, 261:1–31,
2014.
9 Appendix
We begin the analysis with some preliminary lemmata and definitions that are useful
for proving the main results.
Definition 5 A.1 An operator T : is said to be a contraction w.r.t a norm
V → V
if there exists a constant c [0,1[ such that for any V ,V we have that:
1 2
k·k ∈ ∈ V
TV TV c V V . (30)
1 2 1 2
k − k ≤ k − k
Definition 6 A.2 An operator T : is non-expansive if v,V we have:
2
V → V ∀ ∈ V
Tv TV v V . (31)
2 2
k − k ≤ k − k
Lemma 7 For any f : R,g : R, we have that:
V → V →
maxf(a) maxg(a) max f(a) g(a) . (32)
a∈V − a∈V ≤ a∈V k − k
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
Proof 9 We restate(cid:13)the proof given in (Mg(cid:13)uni, 2019):
f(a) f(a) g(a) +g(a) (33)
≤ k − k
= maxf(a) max f(a) g(a) +g(a) max f(a) g(a) +max g(a). (34)
⇒ a∈V ≤ a∈V {k − k } ≤ a∈V k − k a∈V
Deducting max g(a) from both sides of (34) yields:
a∈V
maxf(a) maxg(a) max f(a) g(a) . (35)
a∈V − a∈V ≤ a∈V k − k
After reversing the roles of f and g and redoing steps (33) - (34), we deduce the desired
result since the RHS of (35) is unchanged.
29Lemma 8 Define val+[f] := min b∈Bmax a∈Af(a,b) and define
val−[f] := max a∈Amin b∈Bf(a,b), then for any b B we have that for any f,g L and
∈ ∈
for any c R :
>0
∈
maxf(a,b) maxg(a,b) c = val−[f] val−[g] c.
a∈A − a∈A ≤ ⇒ − ≤
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
Proof 10 (P(cid:12)roof of Lemma 8.) We (cid:12)begin by noting the following inequality for any
f : R,g : R such that f,g L we have that for all b :
V ×V → V ×V → ∈ ∈ V
maxf(a,b) maxg(a,b) max f(a,b) g(a,b) . (36)
a∈V − a∈V ≤ a∈V | − |
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
From (32) we can(cid:12) straightforwardly derive (cid:12)the fact that for any b :
∈ V
minf(a,b) ming(a,b) max f(a,b) g(a,b) , (37)
a∈V − a∈V ≤ a∈V | − |
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Assume that fo(cid:12)r any b the followin(cid:12)g inequality holds:
∈ V
max f(a,b) g(a,b) c (38)
a∈V | − | ≤
Since (37) holds for any b and, by (32), we have in particular that
∈ V
max minf(a,b) max ming(a,b)
b∈V a∈V − b∈V a∈V
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) max minf(a,b) ming(a,b) (cid:12)
(cid:12) (cid:12)
≤ b∈V a∈V − a∈V
(cid:12) (cid:12)
max(cid:12) max f(a,b) g(a,b) (cid:12)c, (39)
(cid:12) (cid:12)
≤ b∈V (cid:12)a∈V | − | ≤(cid:12)
whenever (38) holds which gives the required result.
Lemma 9 For any f,g,h L and for any c R we have that:
>0
∈ ∈
f g c = min f,h min g,h c.
k − k ≤ ⇒ k { }− { }k ≤
Lemma 10 Let the functions f,g,h L then
∈
max f,h max g,h f g . (40)
k { }− { }k ≤ k − k
Lemma 11 A.4 The probability transition kernel P is non-expansive, that is:
Pv PV v V . (41)
2 2
k − k ≤ k − k
30Proof 11 The result is well-known e.g. (Tsitsiklis & Van Roy, 1999). We give a proof
using the Tonelli-Fubini theorem and the iterated law of expectations, we have that:
PJ 2 = E (PJ)2[s ] = E [E[J[s ] s ])2 E E J2[s ] s = E J2[s ] = J 2,
0 1 0 1 0 1
k k | ≤ | k k
where we hav(cid:2)e used Jen(cid:3)sen’s (cid:0)inequality to g(cid:3)enerat(cid:2)e t(cid:2)he inequa(cid:3)l(cid:3)ity. T(cid:2)his com(cid:3)pletes the
proof.
The following result proves T is a contraction:
Proposition 3 The Bellman operator T is a contraction, in particular, the following
bound holds:
Tv Tv′ γ v v′ ,
k − k ≤ k − k
where v,v′ are elements of a finite normed vector space.
Proof of Theorem 2
Proof 12 (Proof of Proposition 3) Recall, the BellmanoperatorT acting ona func-
tion v : R is given by
S →
Tv(s) := min max M Q , (s,0)+γ P(s′;0,s)v(s′) ,M Q . (42)
1 1 2 2
R
" n s X′∈S o #
where Q (s,a) := Q(s,a,0) and Q (s,b) := Q(s,0,b).
1 2
In what follows and for the remainder of the script, we employ the following short-
hands:
a =: P(s′;a,s), Pσ =: σ(a s) a
Ps′s ss′
|
Pss′
s′∈S a∈A
X X
To prove that T is a contraction, we first note that by equation 37 we deduce that
Tv(s) Tv′(s) :
| − |
= min max M Q , (s,0)+γ P(s′;0,s)v(s′) ,M Q
1 1 2 2
R
(cid:12) (cid:12) " n s X′∈S o #
(cid:12)
(cid:12)
(cid:12) min max M Q′, (s,0)+γ P(s′;0,s)v′(s′) ,M Q′
− " 1 1 R 2 2 #(cid:12)
n s X′∈S o (cid:12)
(cid:12)
(cid:12)
max max max M Q , (s,0)+γ P(s′;0,s)v(s′) ,M Q (cid:12)
1 1 2 2
≤ (cid:12) ( R )
(cid:12) n s X′∈S o
(cid:12)
(cid:12)
(cid:12) max max M Q′, (s,0)+γ P(s′;0,s)v′(s′) ,M Q′
− 1 1 R 2 2
( n s X′∈S o )(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)
31We now consider the four cases produced by equation 42, that is to say we prove the
following statements:
i) (s ,a ,b )+γ 0 v(s′) (s ,a ,b )+γ 0 v′(s′) γ v v′
R
t t t Ps′st
− R
t t t Ps′st
≤ k − k
ii) M Q M Q′ γ v v′ , .
(cid:12) i i (cid:0) (cid:1)(cid:12)
(cid:12)k − k ≤ k − k (cid:12)
iii) M Q M Q′ γ v v′ ,
i j
k − k ≤ k − k
iv) M Q ( ,0)+γ 0 v′ γ v v′ .
i
− R ·
Ps′st
≤ k − k
We begin by proving i).
(cid:13) (cid:2) (cid:3)(cid:13)
(cid:13) (cid:13)
Indeed, for any a A and s , s′ we have that
t
∈ ∀ ∈ S ∀ ∈ S
(s ,0)+γ 0 v(s′) (s ,0)+γ 0 v′(s′)
R
t Ps′st
− R
t Ps′st
max γ a v(s′) γ a v′(s′)
(cid:12)
(cid:12)≤ a∈A
Ps′st
−
P(cid:2)s′st (cid:3)(cid:12)
(cid:12)
γ Pv(cid:12) Pv′ (cid:12)
≤ k (cid:12)− k (cid:12)
γ v v′ ,
≤ k − k
again using the fact that P is non-expansive and Lemma 7.
We now prove ii).
For any τ , define by τ′ = inf t > τ s ,τ . We do the case where
t I t
∈ F { | ∈ S ∈ F }
i = 1 with the case i = 2 being analogous. Now using the definition of M we have
1
that for any s
τ
∈ S
(M Q M Q′)(s ,a ,b)
1 1 τ τ
| − |
max (s ,a ,b) c(s ,a )+γ (aτ,b)v(s′) (s ,a ,b) c(s ,a )+γ (aτ,b)v′(s′)
≤ aτ∈A(cid:12)R
τ τ
−
τ τ Ps′sτ
− R
τ τ
−
τ τ Ps′sτ
(cid:12)
(cid:12) (cid:16) (cid:17)(cid:12)
(cid:12) (cid:12)
m(cid:12)ax γ (a,b)v(s′) (a,b)v′(s′) (cid:12)
≤ a∈A,b(cid:12) ∈B,s′∈S
Ps′sτ −Ps′sτ
(cid:12)
(cid:12) (cid:12)
γ Pv Pv′ (cid:12) (cid:12)
≤ k − k(cid:12) (cid:12)
γ v v′ ,
≤ k − k
using the fact that P is non-expansive.
Next we prove (iii). We break the proof into two cases:
Case 1:
max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
0
a′∈A R
τ
−
τ Ps′sτ
− b′∈B R
τ τ Ps′sτ
≤
(cid:16) (cid:17) (cid:16) (43) (cid:17)
32(M Q M Q′)(s ,a)
1 2 τ
| − |
= max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
a′∈A R
τ
−
τ Ps′sτ
− b′∈B R
τ τ Ps′sτ
(cid:12) (cid:12)
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:12) (cid:12)
max(cid:12)
(cid:12) max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
,min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v(s′) (cid:12)
(cid:12)
≤
(cid:12)
a′∈A R
τ
−
τ Ps′sτ
b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17)
(cid:12)
(cid:12)
(cid:12) min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
−b′∈B R
τ τ Ps′sτ
(cid:12)
(cid:16) (cid:17)(cid:12)
(cid:12)
max max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
,min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v(s′) (cid:12)
(cid:12)
≤
(cid:12)
a′∈A R
τ
−
τ Ps′sτ
b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17)
(cid:12)
m(cid:12) ax max (s ,a′,b) c(s ,a′)+γ (a′,b)v(s′) ,min (s ,a,b′)+c(s ,b′)+γ (a,b′)v′(s′)
−
(cid:12)
a′∈A R
τ
−
τ Ps′sτ
b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:16) (cid:17) (cid:16) (cid:17)
+max max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
,min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
a′∈A R
τ
−
τ Ps′sτ
b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:16) (cid:17) (cid:16) (cid:17)
min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
−b′∈B R
τ τ Ps′sτ
(cid:12)
(cid:16) (cid:17)(cid:12)
(cid:12)
min (s ,a,b′)+c(s ,b′)+γ (a,b′)v(s′) min (s ,a,b′)+c(s ,b′)+γ (a,b′)v′(s′) (cid:12)
≤ b′∈B R
τ τ Ps′sτ
− b′∈B R
τ τ Ps′sτ (cid:12)
(cid:12) (cid:12)
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:12) (cid:12)
+
m(cid:12)
(cid:12)ax max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′((cid:12)
(cid:12)s′) ,0
(cid:12) a′∈A R
τ
−
τ Ps′sτ
− b′∈B R
τ τ Ps′sτ
(cid:12)
(cid:26) (cid:27)
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17) (cid:12)
(cid:12)
(cid:12) γmaxmax
(a′,b′)v(s′) (a′,b′)v′(s′) (cid:12)
(cid:12)
(cid:12) ≤ a′∈A b′∈B
Ps′sτ −Ps′sτ
(cid:12)
γ v v′ .(cid:12) (cid:12) (cid:12) (cid:12)
≤ k − k (cid:12) (cid:12)
wherewe haveagainused the factthat foranyscalarsa,b,c we havethat max a,b max b,c
| { }− { }| ≤
a c using the non-expansiveness of .
| − | P
Case 2:
max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
a′∈A R
τ
−
τ Ps′sτ
(cid:16)
min (s ,a,b′)+c(s
(cid:17)
,b′)+γ
(a,b′)v′(s′)
> 0, a = (a,b) .
− b′∈B R
τ τ Ps′sτ
∀ ∈ A×B
(cid:16) (cid:17) (44)
Recall that the cost function is bounded in the following way 0 < c(s,y) < c for any
∞
k k
33y . Using this we have that c(s,b) > c(s,a) for any a ,b hence,
∈ A∪B − ∈ A ∈ B
(M Q M Q′)(s ,a)
1 2 τ
−
= max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
min (s ,a,b′)+c(s,b′)+γ
(a,b′)v′(s′)
a′∈A R
τ
−
τ Ps′sτ
− b′∈B R
τ Ps′sτ
max(cid:16)
(s ,a′,b)+c(s ,b)+γ
a′,bv(s′) (cid:17)
min
(cid:16)
(s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′) (cid:17)
≤ a′∈A R
τ τ Ps′sτ
− b′∈B R
τ τ Ps′sτ
(cid:16) (cid:17) (cid:16) (cid:17)
maxmax (s ,a′,b)+c(s,b′)+γ
(a′,b′)v(s)
≤ (cid:12)a′∈A b′∈B R
τ Ps′sτ
(cid:12) (cid:16) (cid:17)
(cid:12)
(cid:12)
(cid:12) minmin (s ,a′,b)+c(s,b′)+γ
(a′,b′)v′(s′)
−a′∈Ab′∈B R
τ Ps′sτ
(cid:12)
(cid:16) (cid:17)(cid:12)
(cid:12)
maxmax (s ,a′,b′)+c(s,b′)+γ
(a′,b′)v(s′) (cid:12)
(cid:12)
≤ (cid:12)a′∈A b′∈B R
τ Ps′sτ
(cid:12) (cid:16) (cid:17)
(cid:12)
(cid:12)
(cid:12) +maxmax (s ,a′,b′) c(s,b′) γ
(a′,b′)v′(s′)
a′∈A b′∈B −R
τ
− −
Ps′sτ
(cid:12)
(cid:16) (cid:17)(cid:12)
(cid:12)
maxmax γ (a′,b′)v(s′) γ (a′,b′)v′(s′) (cid:12)
≤ a′∈A b′∈B
Ps′sτ
−
Ps′sτ (cid:12)
(cid:12) (cid:12)
(cid:16) (cid:17)
(cid:12)
(cid:12)γmaxmax
(a′,b′)
(v v′)(s′)
(cid:12)
(cid:12)
≤ (cid:12) a′∈A b′∈B
Ps′sτ
− (cid:12)
γ v v′ (cid:12) (cid:12) (cid:12) (cid:12)
≤ k − k(cid:12) (cid:12)
where we have used the Cauchy-Schwarz inequality in the penultimate step.
34For the reverse inequality, we have
(M Q M Q )(s ,a)
2 1 1 τ
−
= min (s ,a,b′)+c(s,b′)+γ
(a,b′)v′(s′)
max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
b′∈B R
τ Ps′sτ
− a′∈A R
τ
−
τ Ps′sτ
(cid:16) (cid:17) (cid:16) (cid:17)
max max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
, min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v(s′)
≤ −a′∈A R
τ
−
τ Ps′sτ
−b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:16) (cid:17) (cid:16) (cid:17)
+min (s ,a,b′)+c(s,b′)+γ
(a,b′)v′(s′)
b′∈B R
τ Ps′sτ
(cid:16) (cid:17)
max max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
, min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v(s′)
≤ −a′∈A R
τ
−
τ Ps′sτ
−b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:16) (cid:17) (cid:16) (cid:17)
max max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
, min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
− −a′∈A R
τ
−
τ Ps′sτ
−b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:16) (cid:17) (cid:16) (cid:17)
+max max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
, min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
−a′∈A R
τ
−
τ Ps′sτ
−b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:16) (cid:17) (cid:16) (cid:17)
+min (s ,a,b′)+c(s,b′)+γ
(a,b′)v′(s′)
b′∈B R
τ Ps′sτ
(cid:16) (cid:17)
max max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
, min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v(s′)
≤ (cid:12) −a′∈A R
τ
−
τ Ps′sτ
−b′∈B R
τ τ Ps′sτ
(cid:26) (cid:27)
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17)
(cid:12)
m(cid:12)
(cid:12)ax max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
, min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
− −a′∈A R
τ
−
τ Ps′sτ
−b′∈B R
τ τ Ps′sτ
(cid:12)
(cid:26) (cid:27)
(cid:16) (cid:17) (cid:16) (cid:17) (cid:12)
(cid:12)
+ max max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
, min (s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′) (cid:12)
(cid:12)
(cid:12)
−a′∈A R
τ
−
τ Ps′sτ
−b′∈B R
τ τ Ps′sτ
(cid:12) (cid:26) (cid:16) (cid:17) (cid:16) (cid:17)(cid:27)
(cid:12)
(cid:12)
(cid:12) +min (s ,a,b′)+c(s,b′)+γ
(a,b′)v′(s′)
b′∈B R
τ Ps′sτ
(cid:12)
(cid:16) (cid:17)(cid:12)
(cid:12)
max (s ,a,b′)+c(s ,b′)+γ (a,b′)v(s′) (s ,a,b′)+c(s ,b′)+γ (a,b′)v′(s′) (cid:12)
≤ b′∈B R
τ τ Ps′sτ
− R
τ τ Ps′sτ (cid:12)
(cid:12) (cid:12)
h (cid:16) (cid:17)i
(cid:12) (cid:12)
+ max min
((cid:12)
(cid:12)s ,a,b′)+c(s ,b′)+γ
(a,b′)v′(s′)
max (s ,a′,b) c(s ,a′)+γ
(a′,b)v(s′)
,0
(cid:12)
(cid:12)
(cid:12)
b′∈B R
τ τ Ps′sτ
− a′∈A R
τ
−
τ Ps′sτ
(cid:26) (cid:27)
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17)
(cid:12)
(cid:12) γmaxmax
(a′,b′)
(v(s′) v′(s′))
(cid:12) ≤ a′∈A b′∈B
Ps′sτ
−
γ v v′ (cid:12) (cid:12)h i(cid:12) (cid:12)
≤ k − k (cid:12) (cid:12)
wherewe haveagainused the factthat foranyscalarsa,b,c we havethat max a,b max b,c
| { }− { }| ≤
a c using the non-expansiveness of .
| − | P
We now prove iv). We again split the proof of the statement into two cases:
Case 1:
M Q(s ,a) (s ,0)+γ 0 v′(s′) < 0. (45)
i τ
− R
τ Ps′sτ
(cid:0) (cid:1)
35We now observe the following:
M Q(s ,a) (s ,0)+γ 0 v′(s′)
i τ
− R
τ Ps′sτ
max (s ,0)+γ 0 v(s′),M Q(s ,a) (s ,0)+γ 0 v′(s′)
≤ R
τ (cid:0) Ps′sτ i (cid:1)τ
− R
τ Ps′sτ
max(cid:8) (s ,0)+γ 0 v(s′),M Q(s ,a(cid:9) ) (cid:0) max (s ,0)+γ 0 (cid:1) v′(s′),M Q(s ,a)
≤ (cid:12) R
τ Ps′sτ i τ
− R
τ Ps′sτ i τ
(cid:12)
(cid:8) (cid:9) (cid:8) (cid:9)
(cid:12)
(cid:12) (cid:12) +max (s ,0)+γ 0 v′(s′),M Q(s ,a) (s ,0)+γ 0 v′(s′)
R
τ Ps′sτ i τ
− R
τ Ps′sτ
(cid:12)
(cid:12)
(cid:8) (cid:9) (cid:0) (cid:1)
(cid:12)
(cid:12)
max (s ,0)+γ 0 v(s′),M Q(s ,a) max (s ,0)+γ 0 v′(s′),(cid:12)M Q(s ,a)
≤ (cid:12) R
τ Ps′sτ i τ
− R
τ Ps′sτ i τ
(cid:12)
(cid:12) (cid:12)
(cid:8) (cid:9) (cid:8) (cid:9)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) + max (s ,0)+γ 0 v′(s′),M Q(s ,a) (s ,0)+γ 0 v′(s′) (cid:12)
(cid:12) R
τ Ps′sτ i τ
− R
τ Ps′sτ
(cid:12)
(cid:12) (cid:12)
γma(cid:12) x a(cid:8) v(s′) a v′(s′) + max 0,M Q(cid:9) (s ,(cid:0) a) (s ,0)+γ 0 (cid:1) v(cid:12)′(s′)
≤
a∈A(cid:12)
(cid:12)
Ps′sτ −Ps′sτ i τ
− R
τ Ps′sτ (cid:12)
(cid:12)
max (cid:12) a v v′ (cid:12) (cid:12) (cid:8) (cid:0) (cid:1)(cid:9)(cid:12)
≤ a∈A P(cid:12)s′sτ k − k (cid:12) (cid:12) (cid:12)
γ v (cid:13)v′ ,(cid:13)
≤ k −(cid:13) k (cid:13)
wherewe haveagainused the factthat foranyscalarsa,b,c we havethat max a,b max b,c
| { }− { }| ≤
a c and the non-expansiveness of the operators.
| − | P
Case 2:
M Q(s ,a) (s ,0)+γ 0 v′(s′) 0.
i τ
− R
τ Ps′sτ
≥
For this case, first recall that fo(cid:0)r any τ and for(cid:1)any y we have
∈ F ∈ A ∪ B
c(s ,y) > λ for some λ > 0.
τ
M Q(s ,a) (s ,0)+γ 0 v′(s′)
i τ
− R
τ Ps′sτ
M Q(s ,a) max (s ,0)+γ 0 v′(s′) c(s ,a′)
≤
i τ (cid:0)
− a′∈A R
τ Ps(cid:1)′sτ
−
τ
max (s ,a) c(s(cid:0) ,a′)+γ a′ v(s′) (cid:1)
≤ a′∈A R
τ
−
τ Ps′sτ
h max (s ,0) ci (s ,a′)+γ 0 v′(s′)
− a′∈A R
τ
−
τ Ps′sτ
γmax a (v(s′) v′(s′)(cid:0)) (cid:1)
≤ a∈A
Ps′sτ
−
γ v(s′)(cid:13) v′(s′) (cid:12)
≤ | (cid:13)− | (cid:12)
γ v v′ ,
≤ k − k
again using the fact that P is non-expansive. Hence we have succeeded in showing that
for any v L we have that
2
∈
MQ ( ,0)+γ 0 v′ γ v v′ . (46)
− R · P ≤ k − k
Gathering the results o(cid:13)f the thre(cid:0)e cases gives the(cid:1)d(cid:13)esired result.
(cid:13) (cid:13)
36Proposition 4 The value of the game is unique.
G
Proof 13 By Proposition 3, we have that
Tk+1v Tkv γ Tkv Tk−1v γk Tv v . (47)
S S S S S S
k − k ≤ k − k ≤ ··· ≤ k − k
The result follows after considering the limit as k and using the boundedness of
→ ∞
Tv v , we deduce that Tkv ,Tk+1v ,..., is a Cauchy sequence which concludes
S S S S
k − k
the proof.
Proof 14 (Proof of Prop 4) Suppose there exist two values of the game , v′ and
G S
v . Then, since each is a solution, we have by Proposition 3 that each is a fixed point of
S
the Bellman operator and hence Tv = v and Tv′ = v′ . Hence, we have the following:
S S S S
v v′ = Tv Tv′ γ v v′ , (48)
k S − Sk k S − S| ≤ k S − Sk
whereafter, we immediately deduce that v = v′ .
S S
Summing up the above results we have succeeded in proving Theorem 1.
Proof of Proposition 2
Proof 15 We do the proof for Player 1 with the proof for Player 2 being analogous.
First let us recall that the intervention time τ is defined recursively τ = inf t >
k k
{
τ s A,τ where A = MQ (s ,a) = Q (s ,a) . The proof is given by
k−1 t k t 1 t 1 t
| ∈ ∈ F } }
establishing a contradiction. Therefore define by τ = inf τ > τ ;M Q (s ,a) =
k k−1 1 1 τ
{F ∋
Q (s ,a) and suppose that the intervention time τ′ > τ is an optimal intervention
1 τ } 1 1
time. Construct the strategy σ′ Σ1 and σ˜ Σ1 intervention times by (τ′,τ′,...,) and
∈ ∈ 0 1
(τ′,τ ,...) respectively. Define by m = sup t;t < τ′ . By construction, we have that
0 1 { 1}
σ′
v (s)
= E R(s 0,a 0)+E ...+γl−1E R(s τ1−1,a τ1−1)+...+γm−lE M 1Qσ′ (s
τ
1′,a,0)
=
Eh
(s ,a
)+Eh
...+γlE
Qhσ′
(s ,a,0)
h iiii
R
0 0 τ1
= Eh (s ,a )+Eh ...+γlEh M Qσ′ (s ,ai ,i 0i )
R
0 0 1 τ1
h h h iii
We now use the following observation
E M Qσ′ (s ,a,0) (49)
1 τ1
h i
max M Qσ′ (s ,a,0), max (s ,a ,0)+γ P(s′;a ,0,s )vσ (s′) .
≤ (
1 τ1
aτ1∈A "R
τ1 τ1
s′∈S
τ1 τ1
#)
X
(50)
37Using this we deduce that
vσ′ (s) E (s ,a )+E ...
0 0
≤ R
" "
+γl−1E (s ,a )+γmax M Qσ′ (s ,a,0), max (s ,a ,0)+γ P(s′;a ,s )vσ (s′)
"R
τ1−1 τ1−1
(
1 τ1
aτ1∈A "R
τ1 τ1
s′∈S
τ1 τ1
#)
X
= E (s ,a )+E ...+γl−1E (s ,a )+γ Tvσ˜ (s ) = vσ˜ (s),
R
0 0
R
τ1−1 τ1−1 τ1
where(cid:2)
the first
inequ(cid:2)
ality is
true(cid:2)
by assumption on
M(cid:2)
.
T(cid:3)
his
is(cid:3)(cid:3) a(cid:3)
contradiction since
σ˜ is a suboptimal policy for Player 1. Moreover, by invoking the same reasoning, we
can conclude that it must be the case that (τ ,τ ,...,τ ,τ ,τ ,...,) are the optimal
0 1 k−1 k k+1
intervention times.
10 Proof of Theorem 7
Proof 16 The proof of the Theoremis straightforward since by Theorem2, each player’s
budgeted problem can be solved using a dynamic programming principle extension of the
Bellman equation corresponding to equation 5. The result follows by application of
Theorem 2 in (Sootla et al., 2022) with minor modifications.
38