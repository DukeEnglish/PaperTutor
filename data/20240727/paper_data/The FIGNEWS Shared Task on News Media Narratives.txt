The FIGNEWS Shared Task on News Media Narratives
WajdiZaghouani,1 MustafaJarrar,2 NizarHabash,3 HoudaBouamor,4
ImedZitouni,5 MonaDiab,6 SamhaaR.El-Beltagy,7 MuhammedAbuOdeh3
1NorthwesternUniversityinQatar,EducationCity,Doha,Qatar 2BirzeitUniversity,Palestine
3NewYorkUniversityAbuDhabi,UAE 4CarnegieMellonUniversity,Qatar
5Google,USA 6CarnegieMellonUniversity,USA 7NewgizaUniversity,Egypt
wajdi.zaghouani@northwestern.edu, mjarrar@birzeit.edu, nizar.habash@nyu.edu
mdiab@cs.cmu.edu, samhaa@computer.org, mra9047@nyu.edu
Abstract with rich examples, while fostering a research-
oriented collaborative environment. By simulta-
We present an overview of the FIGNEWS
neouslyexaminingmultiplelanguages,comparing
shared task, organized as part of the Arabic-
andcontrastingvariousnarratives,FIGNEWSaims
NLP 2024 conference co-located with ACL
to unravel the layers of possible bias and propa-
2024. Thesharedtaskaddressesbiasandpro-
gandawithinnewsarticleswithalternativemedia
pagandaannotationinmultilingualnewsposts.
WefocusontheearlydaysoftheIsraelWaron narratives. Thisisespeciallycriticalattimesofany
Gazaasacasestudy.1 Thetaskaimstofoster war or conflict. The media’s portrayal of events
collaborationindevelopingannotationguide- duringsucheventshassignificantimplicationson
lines for subjective tasks by creating frame- publicperception,policy-making,andinternational
works for analyzing diverse narratives high-
relations. Byaddressingbiasandpropaganda,we
lighting potential bias and propaganda. In a
hopetoilluminatethevariedwaysinwhichnews
spirit of fostering and encouraging diversity,
can shape, and sometimes distort, public under-
we address the problem from a multilingual
perspective,namelywithinfivelanguages: En- standingofcomplexgeopoliticalevents. Thisinitia-
glish, French, Arabic, Hebrew, andHindi. A tiveseekstoexplorediverseperspectives,cultures,
total of 17 teams participated in two annota- andlanguages,therebyfosteringacomprehensive
tionsubtasks: bias(16teams)andpropaganda understandingofeventsthroughthelensofmajor
(6teams). Theteamscompetedinfourevalua-
newsoutletsacrosstheglobe.
tiontracks:guidelinesdevelopment,annotation
Developingguidelinesforcomplexdataisachal-
quality, annotation quantity, and consistency.
lengingtask. Theproblemisexacerbatedwhenthe
Collectively,theteamsproduced129,800data
dataiscontemporaneoushencetheannotatorsand
points. Keyfindingsandimplicationsforthe
fieldarediscussed. thetaskorganizersmighthaveastanceonthesub-
jectmatter. FIGNEWSisourattemptataddressing
1 Introduction the creation of annotation guidelines, addressing
whatrelevantbestpracticesshouldbe. Weusethe
The FIGNEWS 2024 shared task,2,3 henceforth
IsraelWaronGazaasausecasetohighlightsome
FIGNEWS,addressesthecriticalneedforanalyz-
of these aspects. To that end, we curate a shared
ingbiasandpropagandainmultilingualnewsdis-
corpusfor comprehensiveannotationacross vari-
course surrounding the Israel War on Gaza. This
ouslayers,craftingannotationguidelinesshapedby
taskalignswiththeNLPcommunity’sgrowingef-
thediverserangeofconflictingdiscoursesaround
fortstocreatedatasetsandguidelinesforcomplex
this sensitive topic. This endeavor facilitates the
opinionanalysisthroughcollaborativesharedtasks
developmentofrobustmethodologiesandmetrics
and datathons. Such a meta-task allows for the
fordetectingandanalyzingbiasandpropaganda,
exploration of various annotation frameworks in
which are crucial for ensuring fair and accurate
particularforcomplexandchallengingsubjective
media reporting. The curated corpus, along with
tasks. FIGNEWSfocusesonadiversemultilingual
meticulouslydevelopedguidelinesandannotations,
corpus,emphasizingthedevelopmentofguidelines
willserveasavaluableresourceforfutureresearch
1FIGNEWS:FramingtheIsraelWaronGazaNews. inNLPandrelatedfields.
2Website:https://sites.google.com/view/fignews
This initiative also seeks to bring to light both
3Code and Data: https://github.com/CAMeL-Lab/
challenges and commendable aspects within the
FIGNEWS-2024 & https://huggingface.co/datasets/
CAMeL-Lab/FIGNEWS-2024 data,fosteringacollaborativecommunitythatcan
4202
luJ
52
]LC.sc[
1v74181.7042:viXralearn from each other’s approaches and findings. datasets to detect news bias. Budak et al. (2016)
Webelievethatacollaborative,research-oriented used crowdsourced content analysis to quantify
environmentisessentialfortacklingtheintricate mediabias,whileBaumeretal.(2015)compared
taskofbiasandpropagandadetection. computationalapproachesfordetectingframingin
TheFIGNEWSsharedtaskthusrepresentsasig- politicalnews. ToolslikeBiasly(2017)helpusers
nificantstepforwardinthefieldofdataannotation, gauge news bias, quantifying liberal or conserva-
mediaanalysisandNLP.Asteptowardsananno- tiveleanings.
tationscience. Itnotonlyprovidesaplatformfor In the context of multilingual and multi-label
examining critical issues of bias and propaganda news framing analysis, Akyürek et al. (2020) ex-
but also promotes the development of best prac- plored the complexities of news framing across
ticesindataannotationandanalysis. Throughthis differentlanguages. Theirworkiscrucialforunder-
shared task, we aim to contribute to the broader standinghowbiasandframingmanifestinmultilin-
goal of improving media literacy and fostering a gualcontexts,aligningcloselywiththeobjectives
moreinformedandcriticallyengagedpublic. oftheFIGNEWSsharedtask. Similarly,thestudy
by(Heppelletal.,2023)offersavaluabledataset
2 RelatedWork and linguistic insights from two multilingual dis-
information websites, providing a foundation for
Propagandaandbiascanhavefar-reachingimpli- furtherexplorationoflanguage-specificdisinforma-
cations depending on the context and medium in tiontechniquesandtheirimpactonnewsframing.
which they are propagated. Polarization, conflict Recent advances in bias detection have also
andinjusticearebutsomeofthesideeffectsthey emphasized the importance of detailed and well-
can create in any context. In the context of poli- annotated datasets. Spinde et al. (2021a) intro-
tics, they can alter the outcomes of elections and ducedtheMBICdataset,whichincludesdetailed
changethefaceofnations(Gorenc,2020;Maweu, informationaboutannotatorcharacteristicsandpro-
2019; Solopova et al., 2024). News media fram- videslabelsforbiasidentificationatboththeword
ingandnarrativesaroundwarsandsocio-political andsentencelevels. Thisdatasetrepresentsasig-
events have been extensively studied, with a fo- nificant step forward in creating reliable ground-
cusonidentifyingbias,propaganda,offensivelan- truthdataforbiasdetection. Additionally,Spinde
guagedetection,anddiverseperspectives(Entman, etal.(2021b)developedTASSY,atextannotation
2007;Baumeretal.,2015;Fanetal.,2019;Morstat- surveysystemthatenhancesthequalitycontrolof
ter et al., 2018; Park et al., 2009; Martino et al., annotationprocessesinNLPtasks.
2020b;Aksenovetal.,2021;Yenkikaretal.,2022; Annotation of biased language and framing
Kameswari et al., 2020; Hong et al., 2023; Kim in news articles has been explored using tech-
etal.,2023;Sharmaetal.,2023;Maabetal.,2023; niqueslikeexpertannotation(Al-SarrajandLub-
Rodrigo-Ginés et al., 2024; Hamad et al., 2023; bad, 2018) and crowdsourcing (Lim et al., 2020,
Darwish et al., 2021). Several works have pro- 2018). Qualitycontrolandguidelinesforannota-
posed computational approaches to detect media tionprocessesinNLPtaskshavealsobeeninvesti-
biasthroughanalysisofwordchoice,labeling,and gated(Grosmanetal.,2020;Spindeetal.,2021b).
factual reporting (Hamborg et al., 2019; Budak Grosmanetal.(2020)developedERAS,asystem
et al., 2016; Vaagan et al., 2010; Varacheva and designedtoenhancequalitycontrolinNLPtasks,
Gherghina,2018). whichisparticularlyrelevantforensuringtherelia-
Entman(2007)definesframingastheprocessof bilityofannotationsinbiasandpropagandadetec-
selecting certain aspects of perceived reality and tion.
constructinganarrativethatemphasizestheircon- Further contributions to the detection of bias
nectionstopromoteaspecificinterpretation. This and propaganda include the work of Rashkin
foundational work highlights how news framing etal.(2017),whoanalyzedlanguageinfakenews
can influence public perception by emphasizing and political fact-checking, and Barrón-Cedeno
particularelementsoverothers. Entman(1993)fur- et al. (2019), who organized news based on their
therelaboratesonthisconcept,providingacompre- propaganda-pronecontent. Thesestudiesprovide
hensiveframeworkforunderstandinghowmedia foundationaltechniquesfortheidentificationand
framescanshapepoliticalandsocialrealities. analysis of biased and propaganda language in
Several studies have developed methods and newsmedia.The shared task on propaganda detection at velop improved guidelines and foster a collabo-
SemEval-2020,organizedbyMartinoetal.(2020a), rative exploration of media discourses through a
is directly relevant to the FIGNEWS task. This multilingual,multiculturallens.
taskfocusedondetectingpropagandatechniques
in news articles, providing valuable benchmarks 3 DataCollectionandSelection
andmethodologiesthatcanbeappliedtothedetec-
WeusedtheCrowdTangle4platformtocollectFace-
tionofbiasandpropagandainsocialmediaposts.
bookpostsrelatedtotheIsraelWaronGazainfive
InthecontextoftheIsrael-relatedwarsandcon-
languages: English,French,Arabic,Hebrew,and
flicts, Al-Sarraj and Lubbad (2018) conducted a
Hindi. Specifically,weretrievedpublicpostscon-
sentimentanalysisstudytodetectbiasinWestern
taining the keyword “Gaza”5 from verified blue-
mediacoverage. Thisworkhighlightsthespecific
checkaccounts,publiclypostedbetweenOctober
challengesofdetectingbiasinahighlypolarized
7,2023,andJanuary31,2024.
and sensitive geopolitical context. Additionally,
Foreachlanguage,wecollectedapproximately
Hamadetal.(2023)introducedanewdatasetde-
300,000posts. Tonarrowthisdownto3,000posts
signedfordetectingandclassifyingvariousforms
perlanguageandhaveabalanceddistribution,we
ofoffensivelanguageinHebrewonsocialmedia.
focusedontenkeymomentsinthefirstfewmonths
Furthermore, the WojoodNER Shared Task 2024
of the war: starting with the events of October 7,
offeredanewNERdatasetrelatedtotheIsraeliWar
onGazacalledWojoodGaza (Jarraretal.,2024). 2023andthedeclarationofwar,andincludingthe
bombings in Jabalia refugee camp, Al-Shifa hos-
Research on detecting political bias in social
pital, and St. Porphyrius Church, as well as the
mediaincludesZhouetal.(2020),whoidentified
ceasefire and hostage release, Gaza mass arrests,
biasinuser-generatedcontent,andLietal.(2019),
andthecoverageoftheInternationalCourtofJus-
whosurveyedsentimentanalysistechniques. The
ticecasetilltheendofJanuary2024.
Propittercorpus(Casavantesetal.,2024)wasde-
Foreachofthetenmoments,weselectedthetop
velopedusingdistantsupervisionwithrefinedan-
200postsperlanguage,rankedbytotalinteraction
notations. Hamborgetal.(2019)advancedmedia
count(likes,shares,comments,etc.). Asaresult,
biasdetectionbyfocusingonautomatedidentifica-
weselected15,000postswiththehighestinterac-
tion through word choice and labeling. Fan et al.
tions,coveringdifferentkeymomentsoftheWar,
(2019) analyzed factual reporting to reduce bias,
infivelanguages.
stressingtheneedforobjectivity.
Among the studies forming a strong basis for We divided the 15,000 posts into 15 batches
understanding bias and propaganda detection in ({B01, B02, ..., B015}) with 1,000 posts in each
media are the following. Baly et al. (2020) ex- batch. Eachbatchcontains200postsperlanguage.
amined news media profiling using text and so- Additionally,eachbatchincluded20postsforeach
cialmediaanalysis,showinghownewscontextaf- ofthe10keymomentsduringtheWar.
fectsbiasperception. AllcottandGentzkow(2017), Sincetheannotatorsmaynotspeakallfivelan-
Vosoughietal.(2018),andGrinbergetal.(2019) guages,weprovidedmachinetranslations(Google
studiedfakenewsspreadduringthe2016USelec- Translate)intoEnglishandArabictofacilitatean-
tion,highlightingtheneedforreliableinformation. notationacrossthemultilingualcorpus. Whilema-
Lazer et al. (2018) detailed fake news detection chinetranslationinevitablyintroducessomenoise,
methods, while Horne and Adali (2017) empha- weconsideredthistobereflectiveofthereal-world
sizedhigh-qualitydataforcombatingmisinforma- relianceonsuchtechnologies.
tion. Sharmaetal.(2019)surveyedfakenewsiden- To compare the Inter-Annotator Agreement
tification techniques. Zhou and Zafarani (2020) (IAA)amongallparticipatingteamsinafairman-
reviewed fake news detection theories and meth- ner,werandomlyselected20postsfromeachlan-
ods,pointingtofutureresearchopportunities. guage (i.e., 100 posts, which is 10%) from each
While prior work has made notable contribu- batch, totaling 1,500 posts. All participants re-
tions, our shared task focuses on comprehensive ceivedthedatasetinaGoogleSheetfilewithtwo
annotationofmedianarrativesandframingaround sheets: “Main”and“IAA”.The“Main”sheetcon-
a specific war to uncover new insights. We build
4https://crowdtangle.com
uponexistingannotationframeworks(Zaghouani
et al., 2016; Zaghouani and Charfi, 2018) to de- 5 ةﺰﻏ, הזע , गाजा, and Gaza.tains13,500posts,whilethe“IAA”sheetcontains suchasdefiningobjectives,describingthetask,es-
1,500posts. SeeAppendixBforascreenshotof tablishingcategories,providingdetailedguidelines
theinterface. with examples, outlining the annotation process,
Itisimportanttonotethatparticipantswerepro- settingqualitystandards,handlingambiguities,en-
videdonlywiththeoriginalpostsandtheirtrans- suringconsistency,andconsideringethicalaspects.
lations. Informationsuchasaccountowner,date, TheGuidelinesScoreusedtodeterminethewin-
andtotalinteractionswerenotgiventoanypartici- nersofthistrackistheaveragenormalizedDocu-
patingteam(Seetheethicalconsiderationsection). mentScoreandnormalizedIAAKappascore. The
Document Score is equal to the number of satis-
4 SubtasksandEvaluationTracks
fieddocumentchecklistitems,arangefrom0to8.
TheIAAKappascoreofateamistheaverageof
Thissectionpresentsthesharedtasksubtasks,eval-
allpairwiseIAAKappasoverteamannotatorsper
uation tracks andminimal requirement for teams
batch(sameasIAAQualityScorediscussednext).
toqualify. FurtherdetailsareinAppendixA.
Forbothsub-scores,normalizationisaccomplished
4.1 MinimalRequirementstoQualify bydividingbythemaximumvalueattainedbyany
qualifiedteam. FurtherdetailsareinAppendixA.
Toqualify,eachparticipatingteammustprovide
fullannotationguidelinesforeachsubtaskthey 4.3.2 IAAQualityTrack
choosetoworkon;andtheymustannotateatmin-
In the IAA Quality Track, teams compete based
imumBatch1andBatch2,i.e. (1,800posts)and
ontheirinternal(withinteam)IAAKappascores
theirdesignatedInter-annotatoragreementsubset
(Cohen,1960). InadditiontotheKappascore(our
(200posts)foratotalof2,000posts.
primary metric), we report on a number of other
usefulmetrics.
4.2 AnnotationSubtasks
The shared task consists of two subtasks: Bias • IAAKappa(PrimaryMetric)Ateam’sIAA
AnnotationandPropagandaAnnotation. Kappascoreiscalculatedastheaverageofall
pairwiseKappascoresbetweenteamannota-
4.2.1 BiasAnnotationSubtask
torsforeachrelevantIAAbatchabdsubtask.
The Bias Annotation subtask involves assigning
• Accuracy (Acc) The percentage of agreed
one of the following seven labels to each post:
upondatapoints(BiasandPropaganda)
(1) Unbiased, (2) Biased against Palestine, (3)
• Macro F1 Average The average F1 score
Biased against Israel, (4) Biased against both
over all the Bias or Propaganda subtask la-
Palestine and Israel, (5) Biased against others,
belsoverallpairsofannotatorsandrelevant
(6) Unclear, and (7) Not Applicable. Examples
IAAbatches, i.e. batcheswhichthepairsof
illustrating each label are provided in the shared
annotatorsannotated.
taskdescriptioninAppendixA.
• F1 Bias* The value of the average F1 score
4.2.2 PropagandaAnnotationSubtask ofallBiaslabelscollapsedasBiasvsother.
ThePropagandaAnnotationsubtaskrequirespar- • F1Prop*ThevalueoftheaverageF1score
ticipants to classify each post into one of the fol- ofPropagandalabelalone.
lowingfourcategories: (1)Propaganda,(2)Not
Propaganda,(3)Unclear,and(4)NotApplicable. 4.3.3 QuantityTrack
Examplesshowcasingeachcategoryareincluded In the Quantity Track, teams compete based on
inthesharedtaskdescriptioninAppendixA. the number of annotated data points. They must
completethebatchesinorderandfinishonebatch
4.3 EvaluationTracks
beforemovingtothenext.
Foreachsubtask,therearefourevaluationtracks.
4.3.4 ConsistencyTrack
4.3.1 GuidelinesTrack
IntheConsistencyTrack,teamscompetebasedon
Participantshavethefreedomtodesigntheirown thecentralityoftheirannotationchoicescompared
annotationguidelinesandapplythemtotheshared to all other teams. Centrality reflects the consis-
data. The organizers will evaluate the guidelines tency of a team’s annotations with those of other
basedonan8-pointchecklist,whichincludesitems teams. Wedefineateam’scentralityasMacroF1TeamName SystemDescription Bias Propaganda
BiasBluffBusters Paretietal.(2024) X X
BiasGanda AlWardietal.(2024) X
BSC-LANGTECH Ruiz-Fernándezetal.(2024) X
Ceasefire Sadiahetal.(2024) X
DRAGON Jafarietal.(2024) X
Eagles Eanetal.(2024) X
GroningenAnnotatesGaza Khatibetal.(2024) X
JusticeLeague Salehetal.(2024) X
NarrativeNavigators AlEmadietal.(2024) X X
NLPColab Raufetal.(2024) X X
SaharaPioneers Sollaetal.(2024) X
Sina Duaibesetal.(2024) X X
SQUad Al-MamariandAl-Farsi(2024) X
TheCyberEquityLab Helaletal.(2024) X X
TheGuidelineSpecialists BourahouatandAmer(2024) X
TheLexiconLadies El-Ghawietal.(2024) X
UoT1 Nwesrietal.(2024) X
Totals 16 6
Table1: Theparticipatingteams: names,papers,andsubtasks.
AverageofitsBiasorPropagandaannotations(as degree. Two thirds of the annotators come from
relevant)againstotherteams’annotations. Amore EngineeringandTechnologyareasofexpertise.
central team, with higher consistency, is one that
5.3 BiasSubtask
other teams agree with more on average. We re-
portonallthemetricsmentionedinSection4.3.2 Intotal,therewere85annotatorsacross16teams
except that we consider the annotations in Main andtheyannotatedtogether72Mainsetsand237
Batch1andBatch2forthistrack,andonlycom- IAAsets,foratotalof88,500datapoints. Table2
pareannotatorsindifferentteams(acrossteams). andTable3presenttheresultsontheBiasSubtask.
ThewinnersarepresentedinTable5.
5 Results
Guidelines Track The winners of the Bias
5.1 Teams GuidelinesTrackareNLPColab(1st),Eagles(2nd)
and Narrative Navigators (3rd). Details on their
Outofthe23teamsthatregistered,only17techni-
scoresandrankingareinTable2.
callyqualifiedpertherulesofthesharedtask,i.e.,
minimallyprovidedbatches1and 2inMainand
IAAQualityTrack ThewinnersoftheBiasIAA
IAA fully. Table 1 lists the qualifying teams and
QualityTrackareNLPColab(1st),JusticeLeague
thesubtaskstheyparticipatedin. Allofthequali-
(2nd)andSina(3rd). Detailsontheirscoresarein
fying teams submitted system description papers
Table3.
whichareincludedintheproceedings.
QuantityTrack ThewinnersoftheBiasQuan-
5.2 Annotators tity Track are DRAGON (1st), NLPColab (2nd)
andSina(3rd). Thesumoftheirannotationsequal
Intotal,85annotatorsfrom16teamsparticipated
to56%ofallBiassubtaskannotations. Detailson
intheBiassubtask,and51annotatorsfrom6teams
theirscoresareinTable3.
participated in the Propaganda subtask. Table 10
inAppendixChighlightsthediversityamongthe
ConsistencyTrack ThewinnersoftheBiasCon-
annotatorsbasedontheinformationtheyprovided
sistencyTrackareTheLexiconLadies(1st),NLP-
voluntarily. Onlyhalfoftheannotatorsarenative
Colab (2nd) and Narrative Navigators (3rd). De-
Arabic speakers and close to one-third are Urdu
tailsontheirscoresareinTable3.
speakers. Half are between the ages 18-24 and
closetoone-third25-34. Overthree-quartersiden- Observations Asanticipated,within-teamIAA
tifyasfemale. Theyclaimmanyregionsoforigin scoressignificantlysurpassacross-teamIAA,with
(SouthAsia33%,Levant27%,NorthAfrica13%, an average absolute increase of 22.6% (Kappa)
Western Europe 11%, among others). Almost all and 19.4% (Macro F1 Average). The F1 Bias*
arehighlyeducatewithclosetohalfwithMaster’s scores,indicatingbinarybiasdetermination,showGuidelines IAA Guidelines Final
Team Document
Kappa Score Rank
Score
Bias Bluff Busters Yes Yes Yes Yes Yes No Yes Yes 7 43.3 0.7120 4
BiasGanda Yes Yes Yes Yes Yes Yes Yes Yes 8 31.0 0.6964 6
BSC-LANGTECH Yes Yes Yes Yes Unclear Yes Yes Unclear 6 51.0 0.6982 5
Ceasefire Yes Yes Yes Yes Yes Yes Yes Yes 8 26.6 0.6685 8
DRAGON No No No Yes Yes No Yes No 3 35.7 0.4140 16
Eagles Yes Yes Yes Yes Yes Yes Yes Yes 8 55.5 0.8519 2
Groningen Annotates Gaza Yes Yes Yes No No No No No 3 43.5 0.4634 15
JusticeLeague Yes Yes Yes Unclear Yes Unclear No No 4 64.4 0.6585 10
Narrative Navigators Yes Yes Yes Yes Yes Yes Yes Yes 8 39.4 0.7497 3
NLPColab Yes Yes Yes Yes Yes Unclear Yes Yes 7 78.8 0.9375 1
Sina Yes Yes No No No No No Yes 3 61.4 0.5771 14
SQUad Yes Yes Yes Yes Yes Yes Yes Yes 8 23.4 0.6483 11
The CyberEquity Lab Yes Yes No Yes No No Yes Yes 5 48.1 0.6175 13
The Guideline Specialists Yes Yes Yes No Yes Yes Yes Yes 7 28.6 0.6192 12
The Lexicon Ladies Unclear Yes Yes Yes Yes Yes Yes Yes 7 37.2 0.6732 7
UoT1 Yes Yes Yes Yes Yes No Yes Unclear 6 44.9 0.6598 9
Bias Bluff Busters Yes Yes Yes Yes Yes No Yes Yes 7 31.5 0.6632 2
Narrative Navigators Yes Yes Yes Yes Yes Yes Yes Yes 8 12.8 0.5913 5
NLPColab Yes Yes Yes Yes Yes Unclear Yes Yes 7 69.9 0.9375 1
Sahara Pioneers Yes Yes Yes Yes Yes Yes Yes No 7 27.9 0.6373 4
Sina Yes Yes No No No No No Yes 3 65.3 0.6551 3
The CyberEquity Lab Yes Yes No Yes No No Yes Yes 5 33.5 0.5521 6
Table2: Resultsoftheguidelinesevaluationtrack
awiderangeofdisagreementswithinteams(aver- QuantityTrack ThewinnersofthePropaganda
age71.3%,stdev10.9%)andacrossteams(average QuantityTrackareNLPColab(1st),Sina(2nd)and
60.2%,stdev6.5%). Thisvariabilityreflectsthein- TheCyberEquityLab(3rd). Thesumoftheiran-
herentsubjectivityandcomplexityofbiaslabeling notationsequalto82.1%ofallPropagandasubtask
ingeneral. annotations. DetailsontheirscoresareinTable4.
Thebiaslabelingtaskischallenging,withhigh
IAA difficult to achieve both within and across
teams. However,thehighscoresoftop-performing Consistency Track The winners of the Propa-
teamshighlighttheneedformeticulousattention gandaConsistencyTrackareNLPColab(1st),Bias
todetailandcomprehensivetraining. BluffBusters(2nd)andSaharaPioneersandThe
CyberEquityLab(tied3rd). Detailsontheirscores
5.4 PropagandaSubtask areinTable4.
In total, there were 51 annotators across 6 teams
and they annotated together 36 Main sets and 89
Observations Asanticipated,within-teamIAA
IAAsets,foratotalof41,300datapoints. Table2
scores significantly surpass across-team IAA,
andTable4presenttheresultsonthePropaganda
with an average absolute increase of over 21.1%
Subtask. ThewinnersarepresentedinTable5.
(Kappa) and 18.9% (Macro F1 Average). The
F1 Prop* scores show a wide range of disagree-
Guidelines Track The winners of the Propa-
mentswithinteams(average66.5%,stdev15.4%)
gandaGuidelinesTrackareNLPColab(1st),Bias
and across teams (average 55.5%, stdev 6.1%).
BluffBusters(2nd)andSina(3rd). Detailsontheir
This variability reflects the inherent subjectivity
scoresandrankingareinTable2.
andcomplexityofthistask.
IAA Quality Track The winners of the Propa- LikeBiaslabeling,thePropagandalabelingtask
gandaIAAQualityTrackareNLPColab(1st),Sina isquitedemanding. Althoughithasasmallernum-
(2nd)andTheCyberEquityLab(3rd). Detailson beroflabels,weseecomparablepatternsinterms
theirscoresareinTable4. ofperformanceacrossanumberofmetrics.
saiB
adnagaporP
enifeD sevitcejbO hsilbatsE seirogetaC edivorP selpmaxE eniltuO ssecorP ytilauQ sdradnatS eldnaH seitiugibmA erusnE ycnetsisnoC & gniniarT troppuSQuantity Quality Centrality
Batches Total IAA Within Team Main B1+B2 Across Teams
Data Macro F1 Macro F1
Team Anno # Main IAA Points Kappa Acc F1 Avg Bias* Kappa Acc F1 Avg Bias*
Bias Bluff Busters 4 2 8 2,600 43.3 56.3 48.5 69.3 14.4 28.7 21.0 61.7
BiasGanda 4 2 4 2,200 31.0 51.5 31.5 66.4 26.0 43.6 29.5 64.2
BSC-LANGTECH 2 2 4 2,200 51.0 65.5 39.8 81.5 26.5 46.6 29.2 60.2
Ceasefire 3 2 6 2,400 26.6 46.3 29.3 61.2 24.2 42.0 27.2 66.0
DRAGON 4 15 60 19,500 35.7 75.5 41.0 43.2 19.7 41.1 21.9 59.7
Eagles 4 2 8 2,600 55.5 75.4 48.4 68.5 25.6 46.0 25.4 55.3
Groningen Annotates Gaza 7 2 14 3,200 43.5 56.8 39.8 69.9 25.3 28.9 25.7 56.4
JusticeLeague 3 2 6 2,400 64.4 83.7 63.8 73.6 19.9 43.3 19.6 46.5
Narrative Navigators 7 2 4 2,200 39.4 56.5 45.5 70.5 28.0 44.5 30.5 66.6
NLPColab 21 15 30 16,500 78.8 85.3 76.1 94.3 27.7 42.4 30.8 67.3
Sina 10 12 24 13,200 61.4 81.4 55.4 75.1 11.8 38.7 17.2 48.1
SQUad 2 4 8 4,400 23.4 40.8 27.2 66.8 -0.2 8.5 5.8 56.2
The CyberEquity Lab 5 3 15 4,200 48.1 71.6 39.5 70.5 25.0 46.5 24.1 58.0
The Guideline Specialists 2 2 30 4,800 28.6 51.3 34.9 84.4 21.0 36.8 24.5 66.2
The Lexicon Ladies 3 2 4 2,200 37.2 53.0 35.4 73.4 29.1 44.1 33.1 66.3
UoT1 4 3 12 3,900 44.9 58.2 48.7 71.5 26.8 42.5 29.0 64.6
Average 5.3 4.5 14.8 5,531 44.5 63.1 44.1 71.3 21.9 39.0 24.7 60.2
Total 85 72 237 88,500
Table3: ResultsoftheBiassubtask.
Quantity Quality Centrality
Batches Total IAA Within Team Main B1+B2 Across Teams
Annot Data Macro F1 Macro F1
Team # Main IAA Points Kappa Acc F1 Avg Prop* Kappa Acc F1 Avg Prop*
Bias Bluff Busters 4 2 8 2,600 31.5 54.3 47.3 62.6 20.2 51.1 37.6 54.2
Narrative Navigators 7 2 4 2,200 12.8 54.5 54.6 63.1 19.4 52.7 37.1 60.7
NLPColab 21 15 30 16,500 69.9 87.3 73.1 92.1 21.5 53.3 39.9 61.6
Sahara Pioneers 4 2 8 2,600 27.9 49.1 46.3 50.5 18.7 48.7 37.4 56.3
Sina 10 12 24 13,200 65.3 85.7 67.9 76.7 12.5 48.1 27.7 44.5
The CyberEquity Lab 5 3 15 4,200 33.5 65.3 41.1 54.2 22.1 55.0 37.4 55.7
Average 8.5 6.0 14.8 6,883 40.1 66.0 55.0 66.5 19.1 51.5 36.2 55.5
Total 51 36 89 41,300
Table4: ResultsofthePropagandasubtask.
6 Discussion PalestinealmostthreetimesthatofBiasedagainst
Israel. Biased against others appeared in about
Wenowconsiderthelabeldistributionpatternsfor
1/16th of the cases. All labels seem to be gener-
BiasandPropaganda,independentlyandtogether.
ally equally distributed across source languages
with the exception of Biased against Palestine
6.1 BiasLabelDistributions
andBiasedagainstIsraelstandingoutinHebrew-
Table 6 summarizes the Bias label distributions
sourced texts: Biased against Palestine is twice
overall and for different source languages: Ara-
theaverageofotherlanguages;andBiasedagainst
bic(Ar),English(En),French(Fr),Hebrew(He),
Israelisone-fifththeaverageofotherlanguages.
andHindi(Hi). Thereportedresultshereinclude
While this seems consistent with what would be
all annotated data points (from Main and IAA).
expectedofthenewsmediaofacountryatwar,it
Naturally, data points from Batches 1 and 2 are
ispossiblethatthereisanadditionalprimingbias
over-representedsincetheywereannotatedbyall
fromknowingthesourcelanguageofthetext. Per-
teams. Unbiased was the highest overall label
hapsinfutureeditions, wecouldcomparewitha
claiming over two-fifths of all data points. This
setting where all source language information is
isfollowedbyBiasedagainstPalestine(29.2%),
hiddenandonlytranslationsareprovided.
thenBiasedagainstIsrael; withBiasedagainstSubtask Track 1st Place 2nd Place 3rd Place
Bias Guidelines NLPColab Eagles Narrative Navigators
Bias IAA Quality NLPColab JusticeLeague Sina
Bias Quantity DRAGON NLPColab Sina
Bias Consistency The Lexicon Ladies NLPColab Narrative Navigators
Propaganda Guidelines NLPColab Bias Bluff Busters Sina
Propaganda IAA Quality NLPColab Sina The CyberEquity Lab
Propaganda Quantity NLPColab Sina The CyberEquity Lab
Propaganda Consistency NLPColab Bias Bluff Busters Sahara Pioneers/The CyberEquity Lab
Table5: Sharedtaskwinnersforeachsubtaskandtrack.
Source Language
Label Arabic English French Hebrew Hindi Total
Unbiased 9.5% 9.7% 8.6% 6.0% 9.0% 42.8%
Biased against Palestine 5.3% 4.6% 4.1% 10.4% 4.7% 29.2%
Biased against Israel 2.7% 2.2% 3.5% 0.5% 2.0% 10.9%
Biased against others 1.1% 1.4% 1.3% 1.1% 1.4% 6.4%
Unclear 1.0% 1.0% 1.4% 1.3% 1.7% 6.4%
Not Applicable 0.3% 0.7% 0.7% 0.6% 0.6% 2.8%
Biased against both Palestine and Israel 0.2% 0.4% 0.3% 0.1% 0.6% 1.6%
Total 20.0% 20.0% 20.0% 20.0% 20.0% 100.0%
Table6: BiasLabelDistributionsintotalandoversourcelanguage.
Source Language
Label Arabic English French Hebrew Hindi Total
Propaganda 9.2% 8.3% 8.5% 11.7% 8.4% 46.1%
Not Propaganda 9.6% 10.1% 9.2% 6.6% 9.8% 45.2%
Unclear 0.9% 0.9% 1.6% 1.1% 1.4% 6.0%
Not Applicable 0.3% 0.7% 0.7% 0.5% 0.4% 2.7%
Total 20.0% 20.0% 20.0% 20.0% 20.0% 100.0%
Table7: PropagandaLabelDistributionsintotalandoversourcelanguage.
Propaganda Labels
Bias Labels Propaganda Not Propaganda Not Applicable Unclear
Biased against Palestine 74.3% -61.2% -20.1% -21.7%
Biased against Israel -7.2% 15.2% -12.5% -6.4%
Biased against both Palestine and Israel -9.0% 13.7% -7.2% -3.2%
Biased against others 23.1% -20.0% -7.0% -4.0%
Unbiased -69.8% 78.7% -10.4% -2.3%
Not Applicable -27.9% -14.7% 85.9% 25.5%
Unclear -25.9% -9.7% 31.2% 57.6%
Table8: PearsoncorrelationcoefficientforallBiasvsPropagandalabelpairs.
6.2 PropagandaLabelDistributions seem to be generally equally distributed with the
exception of of Hebrew-sourced texts where pro-
Table 7 summarizes the Propaganda label distri-
paganda spikes: the ratio of Propaganda to Not
butionsoverallandfordifferentsourcelanguages.
Propagandais1.8asopposedtolessthan0.9for
Thereportedresultshereincludeallannotateddata
theothersourcelanguages. Thesameobservation
points(fromMainandIAA).PropagandaandNot
madeintheprevioussectionapplieshere.
Propagandasplitthedistributionalmostequally
withPropagandabeingslightlyhigher. AlllabelsUnbiased
Biased
>
Palestine
Biased
>
Israel
Propaganda
Not
Propaganda
Lang Text English
Hi इसराइल-हमास के बीच अ ाई यु  वराम, वापस लौट रहे Temporary ceasefire between Israel and Hamas, 94% 100%
लोग  ने  ा कहा what the returning people said
En Pro-Palestinian protesters marched in New York City on Sunday to call for a ceasefire between Israel 94% 100%
and Hamas amid New Year's Eve celebrations.
Ar ىﺮﺳأ ﻦﻋ جاﺮﻓﻹا ﺪﻌﺑ ﺔﯿﺑﺮﻐﻟا ﺔﻔﻀﻟاو سﺪﻘﻟﺎﺑ ةﺮﯿﺒﻛ ﺔﺣﺮﻓGreat joy in Jerusalem and the West Bank after the 88% 100%
.ﻞﯿﺋاﺮﺳإو سﺎﻤﺣ ﻦﯿﺑ لدﺎﺒﺘﻟا ﺔﻘﻔﺻ ﻦﻣ ﻰﻟوﻷا ﺔﻠﺣﺮﻤﻟاrelease of prisoners from the first phase of the
exchange deal between Hamas and Israel.
He .תדלומה תנגה לע םילפונהו םיחצרנה תא חכשנ אלו רוכזנWe will remember and not forget those who were 88% 100%
 💙 !יח לארשי םע .ודמשוי סאמח לש םיצאנהש דע םחלנmurdered and who fell in defense of the homeland.
Fight until the Hamas Nazis are destroyed. Israel
Lives! 💙 
Fr Le Hamas se cache délibérément parmi les civils, Hamas deliberately hides among civilians, making 88% 100%
faisant ainsi payer aux Gazaouis les conséquences Gazans pay for the consequences of Hamas's
des atrocités commises par le Hamas. Notre guerre atrocities. Our war is against Hamas, not against
est contre le Hamas, et non contre la population de the people of Gaza. We are taking important steps
Gaza. Nous prenons des mesures importantes pour to minimize harm to civilians while Hamas uses
minimiser les dommages causés aux civils, alors them as human shields.
que le Hamas les utilise comme boucliers humains.
Ar ﺔﯾﺪﯾﺪﺤﻟا_فﻮﯿﺴﻟا# ةﺰﻏ عﺎﻄﻗ ﻰﻠﻋ ﻢﯿﺤﺠﻟا باﻮﺑأ ﺖﺤﺘﻓ سﺎﻤﺣ Hamas opened the gates of hell on the Gaza Strip 88% 100%
#Iron_Swords
Fr Cette nuit à Gaza...le massacre des civils se Tonight in Gaza...the massacre of civilians 31% 13% 56% 83% 17%
poursuit #GazaUnderAttack #Palestine continues #GazaUnderAttack #Palestine
En Hamas has invited Elon Musk to witness in person the scope of the violence and devastation heaped 31% 63% 50% 50%
upon the Gaza Strip by Israel
Hi इसराइल के हमल  स ेअ ताल भी अछूते नह ... यु - वराम Even hospitals are not untouched by Israeli 25% 13% 56% 50% 50%
ख़  होने के बाद स ेइसराइल और हमास के बीच एक बार  फर attacks… Since the end of the ceasefire, war has
यु   छड़ चुका ह.ै इसराइल अब द  णी ग़ज़ा को भी  नशाना once again broken out between Israel and Hamas.
बना रहा ह,ै  जससे अ ताल भी अछूते नह  ह.  Israel is now targeting Southern Gaza as well,
from which even hospitals are not untouched.
Table9: Examplesofdifferenttextsandtheirmostrelevantannotationdistributions.
6.3 BiasvsPropagandaLabelCorrelations UnbiasedandNotPropaganda. Thesecondset
ofthreeexamplesweremarkedbythevastmajor-
Finally, Table 8 presents a cross-comparison of
ityagainasBiasedagainstPalestineandPropa-
BiasandPropagandalabelsusingMainBatches1
ganda. Thelastsetshowsexampleswithmajority
and2(1,800datapoints). Foreachtextinthisdata
BiasedagainstIsrael.
subset,wecalculateforeachlabelinBiasandPro-
pagandasubtasksthenumberofteamsthatselected 7 ConclusionandFutureWork
that label. We then report the Pearson product-
The FIGNEWS shared task successfully brought
momentcorrelationcoefficientofthe1,800counts
togetheradiversecommunitytoannotatebiasand
for every pair of Bias-Propaganda labels. Unsur-
propagandainmultilingualnewsposts. Thisinitia-
prisingly, Bias Not Applicable and Propaganda
tivebroughttogether17teamsproducing129,800
Not Applicable relate closely (r=85.9%). Simi-
data points. The shared task highlighted the cru-
larly r=78.7% for Unbiased is Not Propaganda.
cialroleofclearguidelines,examples,andcollab-
TheBiasedagainstPalestinelabelcorrelatesposi-
oration in advancing NLP research on complex,
tivelyhighly(74.3%)withPropaganda,andneg-
subjective, and sensitive, opinion analysis tasks.
atively strongly (-61.2%) with Not Propaganda.
Theresultingdatasetandinsightscontributevalu-
The patterns are reversed and much weaker for
ableresourcesanddirectionforfutureworkinthis
BiasedagainstIsrael. Theseareonlyhighlevel
important area. All data and code are publicly
initialobservationsthatopenexcitingpossibilities
available.3
forfurtherstudy.
Futureworkshouldfocusonexpandingtheanno-
tationeffortstoincludemorediverselanguagesand
6.4 SelectedExamples
topics,andrefiningannotationguidelinesbasedon
Table 9 presents a number of texts with their as- participant feedback. The created data should be
sociatedmostprominentlabelaveragesacrossthe leveragedtoadvanceNLPautomaticbiasandpro-
participatingteams. TheexamplesarefromMain pagandadetectiontechniques,aswellasfosterin-
Batches1and2. Therearenineexamples;thefirst terdisciplinarystudiestodeepenourunderstanding
threeweremarkedbythevastmajorityofteamsas ofbiasandpropagandainnewsmedia.Limitations Acknowledgments
We acknowledge the following limitations in the ThistaskispartiallysupportedbyNPRP14C-0916-
FIGNEWSsharedtaskdesignandimplementation. 210015 from the Qatar National Research Fund,
whichisapartofQatarResearchDevelopmentand
• Annotation Subjectivity The task involves
InnovationCouncil(QRDI).Thefindingsachieved
subjectivejudgmentsonbiasandpropaganda,
hereinaresolelytheresponsibilityoftheauthors.
whichvaryamongannotatorsandteams,im-
pactingannotationconsistencyandreliability.
Teams’self-selectionbasedonpreconceived
References
notionsaboutthetopicmayfurtherinfluence
Dmitrii Aksenov, Peter Bourgonje, Karolina Zaczyn-
thisvariability.
ska,MalteOstendorff,JuliánMoreno-Schneider,and
• LabelSelectionWeacknowledgethattheset
Georg Rehm. 2021. Fine-grained classification of
oflabelswespecifiedlimitthespaceofpossi- politicalbiasingermannews: Adatasetandinitial
bilitiesandmayoversimplifycomplexissues experiments. InWOAH2021-5thWorkshoponOn-
lineAbuseandHarms,ProceedingsoftheWorkshop.
imposing a binary perspective that does not
fullycapturenuancedviewpointsandbiases
AfraFeyzaAkyürek,LeiGuo,RandaElanwar,Prakash
withinthedataset. Ishwar,MargritBetke,andDerryTantiWijaya.2020.
• ScopeofTopicsandTextSelectionThefo- Multi-labelandmultilingualnewsframinganalysis.
InProceedingsofthe58thAnnualMeetingoftheAs-
cus on the early days of the Israel War on
sociationforComputationalLinguistics,pages8614–
Gazamaylimittheapplicabilityoffindingsto
8624.
othertypesofnewseventsorbroadermedia
MaryamAlEmadi,JanaElMesselmani,LynaBermak,
contexts. Thesizeofthecorpusisrelatively
Goumana Abdullah, Esraa Sherqawi, Anissa Jrad,
small,andmayincludesomesamplingbias.
ZiedZouabi,andWajdiZaghouani.2024. Narrative
• LimitedDiversitywhileweobservedarange NavigatorsatFIGNEWS2024sharedtask:Newfron-
ofbackgroundsamongtheannotators,weac- tiersinbiasandpropagandaannotationtechniques.
InTheSecondArabicNaturalLanguageProcessing
knowledgethatsomegroupswerehighlyover-
Conference (ArabicNLP 2024), Bangkok. Associa-
represented,whichpotentiallybiasestheover-
tionforComputationalLinguistics.
allconclusions.
AsmahanJumaAl-MamariandFatmaMohammedAl-
EthicalConsiderations Farsi.2024. SQUadatFIGNEWS2024sharedtask:
Unmaskingbiasinsocialmediathroughdataanal-
TheFIGNEWSsharedtaskdealswithsensitivetop- ysisandannotation. InTheSecondArabicNatural
icsandmedianarrativesrelatedtotheIsraelWaron LanguageProcessingConference(ArabicNLP2024),
Bangkok,Thailand.AssociationforComputational
Gaza. Theorganizersandparticipantshavetaken
Linguistics.
severalmeasurestoensureethicalconsiderations
areaddressed: Wael Al-Sarraj and Mohammed Lubbad. 2018. Bias
detectionofpalestinian/Israeliconflictinwesternme-
• Anonymization All posts have been dia: A sentiment analysis experimental study. In
anonymized,withnoidentifyinginformation ProceedingsoftheInternationalConferenceonCom-
munications,SignalProcessing,andtheirApplica-
abouttheaccountownersorusersprovidedto
tions(ICCSPA),pages98–103.
theparticipants.
• Public Posts Only publicly available posts Al Manar Al Wardi, Blqees Al Busaidi, Malath
from verified accounts were included in the Al Sibani, Hiba Al Siyabi, and Najma Al Zidjaly.
2024. BiasGanda at FIGNEWS 2024 shared task:
dataset,ensuringthatthecontentwasintended
Aquesttouncoverbiasedviewsinnewscoverage.
forpublicdissemination.
InTheSecondArabicNaturalLanguageProcessing
• BalancedRepresentationToensurefairrep- Conference (ArabicNLP 2024), Bangkok. Associa-
resentation, the dataset includes a balanced tionforComputationalLinguistics.
numberofpostsfromvariousviewpointsand
Hunt Allcott and Matthew Gentzkow. 2017. Social
narrativesduringthewar. mediaandfakenewsinthe2016election. InJournal
• ResponsibleUseParticipantswererequired of Economic Perspectives, volume 31, pages 211–
236.
toagreetousethedatasetsolelyforresearch
purposesandnotforanyunethicalorillegal
Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov,
activities. James Glass, and Preslav Nakov. 2020. What waswrittenvs.whoreadit: Newsmediaprofilingusing context-informed prescriptive approach to bias de-
textanalysisandsocialmediacontext. InProceed- tectionincontentiousnewsnarratives. InTheSec-
ingsofthe58thAnnualMeetingoftheAssociation ondArabicNaturalLanguageProcessingConference
forComputationalLinguistics,pages3363–3374. (ArabicNLP2024),Bangkok,Thailand.Association
forComputationalLinguistics.
Alberto Barrón-Cedeno, Israa Jaradat, Giovanni
DaSanMartino,andPreslavNakov.2019. Proppy: Yousra Alaa El-Ghawi, Abeer Ehab Marzouk, and
Organizingthenewsbasedontheirpropagandistic Aya Mohamed Khamis. 2024. Lexicon Ladies at
content. Information Processing & Management, FIGNEWS2024sharedtask: Identifyingkeywords
56(5):1849–1864. forbiasannotationguidelinesoffacebooknewshead-
linesontheIsrael-Palestine2023war. InTheSec-
EricBaumer,ElishaElovic,YingQin,FrancescaPol- ondArabicNaturalLanguageProcessingConference
letta, and Geri Gay. 2015. Testing and comparing (ArabicNLP2024),Bangkok,Thailand.Association
computational approaches for identifying the lan- forComputationalLinguistics.
guageofframinginpoliticalnews. InProceedings
RobertMEntman.1993. Framing: Towardclarification
ofthe2015ConferenceoftheNorthAmericanChap-
ofafracturedparadigm. JournalofCommunication,
teroftheAssociationforComputationalLinguistics:
43(4):51–58.
HumanLanguageTechnologies,pages1472–1482.
RobertMEntman.2007. Framingbias: Mediainthe
Biasly. 2017. Biasly. https://www.biasly.com/
distribution of power. Journal of Communication,
politician-ratings/.
57(1):163–173.
GhizlaneBourahouatandSamarM.Amer.2024. The Lisa Fan, Marshall White, Eva Sharma, Ruisi Su,
Guidelines Specialists at FIGNEWS 2024 shared Pratthana Kumar Choubey, Ruihong Huang, and
task: Anannotationguidelinetounravelbiasinnews LuWang.2019. Inplainsight: Mediabiasthrough
medianarrativesusingalinguisticapproach. InThe thelensoffactualreporting. InProceedingsofthe
SecondArabicNaturalLanguageProcessingConfer- 2019 Conference on Empirical Methods in Natu-
ence(ArabicNLP2024),Bangkok,Thailand.Associ- ralLanguageProcessingandthe9thInternational
ationforComputationalLinguistics. JointConferenceonNaturalLanguageProcessing
(EMNLP-IJCNLP),pages6343–6349.
CerenBudak,SharadGoel,andJustinRao.2016. Fair
andbalanced?quantifyingmediabiasthroughcrowd- NinaGorenc.2020. Politicalcommunicationinpost-
sourcedcontentanalysis. PublicOpinionQuarterly, truthsociety: Thecaseofthe2016uselection. Ars
80(2):250–271. etHumanitas,14.
MarcoCasavantes,ManuelMontesyGómez,LuisCar- NirGrinberg,KennethJoseph,LisaFriedland,Briony
los González, and Alberto Barróñ-Cedeno. 2024. Swire-Thompson,andDavidLazer.2019. Fakenews
Propitter: Atwittercorpusforcomputationalpropa- on twitter during the 2016 us presidential election.
gandadetection. LectureNotesinComputerScience Science,363(6425):374–378.
(includingsubseriesLectureNotesinArtificialIntel-
JonatasGrosman,PedroThompson,ArianeRodrigues,
ligenceandLectureNotesinBioinformatics),14392
GuilhermeSchardong,SimoneBarbosa,andHelio
LNAI.
Lopes.2020. Eras: Improvingthequalitycontrolin
theannotationprocessfornaturallanguageprocess-
Jacob Cohen. 1960. A coefficient of agreement for
ingtasks. InformationSystems,93:101553.
nominalscales. Educationalandpsychologicalmea-
surement,20(1):37–46.
NaghamHamad,MustafaJarrar,MohammedKhalilia,
andNadimNashif.2023. OffensiveHebrewCorpus
Kareem Darwish, Nizar Habash, Mourad Abbas,
and Detection using BERT. In Proceedings of the
Hend Al-Khalifa, Huseein T. Al-Natsheh, Houda
20th ACS/IEEE International Conference on Com-
Bouamor,KarimBouzoubaa,ViolettaCavalli-Sforza,
puterSystemsandApplications(AICCSA).IEEE.
SamhaaR.El-Beltagy,WassimEl-Hajj,MustafaJar-
rar,andHamdyMubarak.2021. APanoramicsurvey Felix Hamborg, Anastasia Zhukova, and Bela Gipp.
ofNaturalLanguageProcessingintheArabWorlds. 2019. Automated identification of media bias by
Commun.ACM,64(4):72–81. word choice and labeling in news articles. In Pro-
ceedingsofthe19thACM/IEEE-CSJointConference
LinaDuaibes,AreejJaber,MustafaJarrar,AhmadQadi, onDigitalLibraries(JCDL),pages1–10.
andMaisQandeel.2024. SinaatFIGNEWS2024:
Multilingual datasets annotated with bias and pro- Mohammed H.S. Helal, Radi Jarrar, Mohammad
paganda. InTheSecondArabicNaturalLanguage Alkhanafseh,AbdallahKarakra,andRubaAwadal-
ProcessingConference(ArabicNLP2024),Bangkok. lah.2024. TheCyberEquityLabatFIGNEWS2024
AssociationforComputationalLinguistics. sharedtask: Annotatingacorpusoffacebookposts
tolabelbiasandpropagandaingaza-Israelwarcover-
AmandaC.Ean,MaiA.Baddar,andSofienBâazaoui. ageinfivelanguages. InTheSecondArabicNatural
2024. Eagles at FIGNEWS 2024 shared task: A LanguageProcessingConference(ArabicNLP2024),Bangkok,Thailand.AssociationforComputational XiaodongLi,LidongBing,WaiLam,andBeiShi.2019.
Linguistics. Asurveyofsentimentanalysisinsocialmedia. In
Proceedingsofthe27thInternationalConferenceon
FreddyHeppell,KalinaBontcheva,andCarolinaScar- ComputationalLinguistics(COLING),pages3127–
ton.2023. Analysingstate-backedpropagandaweb- 3137.
sites: anewdatasetandlinguisticstudy. InEMNLP
2023 - 2023 Conference on Empirical Methods in SoraLim,AdamJatowt,MichaelFärber,andMasatoshi
NaturalLanguageProcessing,Proceedings. Yoshikawa.2020. Annotatingandanalyzingbiased
sentencesinnewsarticlesusingcrowdsourcing. In
JiwooHong,YejinCho,JaeminJung,JiyoungHan,and ProceedingsoftheTwelfthLanguageResourcesand
James Thorne. 2023. Disentangling structure and EvaluationConference,pages1478–1484.
style: Political bias detection in news by inducing
documenthierarchy. InFindingsoftheAssociation Sora Lim, Adam Jatowt, and Masatoshi Yoshikawa.
forComputationalLinguistics: EMNLP2023. 2018. Understandingcharacteristicsofbiasedsen-
tencesinnewsarticles. InProceedingsofthe2018
Benjamin D Horne and Sibel Adali. 2017. Just how ACMonConferenceonInformationandKnowledge
toxicisdata? assessingtheimpactofdatasources Management(CIKM),pages2482–2484.
onfakenewsdetection. InProceedingsofthe10th
InternationalConferenceonWebandSocialMedia IffatMaab,EdisonMarrese-Taylor,andYutakaMatsuo.
(ICWSM),pages259–268. 2023. Target-awarecontextualpoliticalbiasdetec-
tioninnews. InProceedingsofthe13thInternational
Sadegh Jafari, Mohsen Mahmoodzadeh, Vanooshe JointConferenceonNaturalLanguageProcessing
nazari, Razieh Bahmanyar, and Kathryn Burrows. andthe3rdConferenceoftheAsia-PacificChapterof
2024. DRAGON at FIGNEWS 2024 shared task: theAssociationforComputationalLinguistics(Vol-
A dedicated rag for october 7th conflict news. In ume 1: Long Papers), pages 782–792, Nusa Dua,
The Second Arabic Natural Language Processing Bali.AssociationforComputationalLinguistics.
Conference(ArabicNLP2024),Bangkok,Thailand.
Giovanni Da San Martino, Alberto Barrón-Cedeno,
AssociationforComputationalLinguistics.
HenningWachsmuth,RostislavPetrov,andPreslav
MustafaJarrar,NaghamHamad,MohammedKhalilia, Nakov. 2020a. Semeval-2020 task 11: Detection
BasharTalafha,AbdelRahimElmadany,andMuham- ofpropagandatechniquesinnewsarticles. InPro-
madAbdul-Mageed.2024. WojoodNER2024: The ceedings of the Fourteenth Workshop on Semantic
Second Arabic Named Entity Recognition Shared Evaluation,pages1377–1414.
Task. InProceedingsoftheSecondArabicNatural
Giovanni Da San Martino, Stefano Cresci, Alberto
LanguageProcessingConference(ArabicNLP2024),
Barrón-Cedeño,SeunghakYu,RobertoDiPietro,and
Bangkok,Thailand.AssociationforComputational
PreslavNakov.2020b. Asurveyoncomputational
Linguistics.
propagandadetection. IJCAIInternationalJointCon-
Lalitha Kameswari, Dama Sravani, and Radhika ferenceonArtificialIntelligence,2021-January.
Mamidi.2020. Enhancingbiasdetectioninpolitical
JacintaMwendeMaweu.2019. “fakeelections”? cyber
news using pragmatic presupposition. In Proceed-
propaganda, disinformation and the 2017 general
ingsoftheEighthInternationalWorkshoponNatural
electionsinkenya. AfricanJournalismStudies,40.
LanguageProcessingforSocialMedia,pages1–6,
Online.AssociationforComputationalLinguistics. FredMorstatter,LiangWu,UrazYavanoglu,Stephen
Corman,andHuanLiu.2018. Identifyingframing
KhalidAlKhatib,SaraGemelli,SaskiaJ.Heisterborg,
bias in online news. ACM Transactions on Social
GosseMinnemaPrithaMajumdar,AriannaMuti,and
Computing,1(2):1–20.
NoaVisserSolissa.2024. GroningenAnnotatesGaza
attheFIGNEWS2024sharedtask: Analyzingbias Abdusalam F. A. Nwesri, Mai Elbaabaa, Fatima
inconflictnarratives. InTheSecondArabicNatural Benlashihar, and Fatma Alalos. 2024. UoT1 at
LanguageProcessingConference(ArabicNLP2024), FIGNEWS 2024 shared task: Labeling news bias.
Bangkok,Thailand.AssociationforComputational InTheSecondArabicNaturalLanguageProcessing
Linguistics. Conference (ArabicNLP 2024), Bangkok. Associa-
tionforComputationalLinguistics.
KangMin Kim, MingyuLee, Hyun SikWon, MinJi
Kim,YeachanKim,andSangKeunLee.2023. Multi- SilviaPareti,JasminHeierli,SerenaPareti,andTatiana
stage prompt tuning for political perspective de- Lando.2024. BiasBluffBustersatFIGNEWS2024
tection in low-resource settings. Applied Sciences sharedtask: Developingguidelinestomakebiascon-
(Switzerland),13. scious. In The Second Arabic Natural Language
ProcessingConference(ArabicNLP2024),Bangkok,
David MJ Lazer, Matthew A Baum, Yochai Ben-
Thailand.AssociationforComputationalLinguistics.
kler,AdamJBerinsky,KellyMGreenhill,Filippo
Menczer, MiriamJMetzger, BrendanNyhan, Gor- SouneilPark,SeungwooKang,SangyoungChung,and
donPennycook,DavidRothschild,etal.2018. The JunehwaSong.2009. Newscube: Deliveringmulti-
scienceoffakenews. InScience,volume359,pages pleaspectsofnewstomitigatemediabias. InPro-
1094–1096. ceedings of the 27th International Conference onHumanFactorsinComputingSystems(CHI),pages Veronika Solopova, Viktoriia Herman, Christoph
443–452. Benzmüller,andTimLandgraf.2024. Checknewsin
oneclick: Nlp-empoweredpro-kremlinpropaganda
HannahRashkin,EunsolChoi,JinYeaJang,Svitlana
detection. InEACL2024-18thConferenceoftheEu-
Volkova, and Yejin Choi. 2017. Truth of varying
ropeanChapteroftheAssociationforComputational
shades: Analyzinglanguageinfakenewsandpoliti-
Linguistics,ProceedingsofSystemDemonstrations.
calfact-checking. InProceedingsofthe2017Con-
ferenceonEmpiricalMethodsinNaturalLanguage Timo Spinde, Jan-David Krieger, Manuel Plank, and
Processing,pages2931–2937. Bela Gipp. 2021a. Towards a reliable ground-
truthforbiasedlanguagedetection. arXivpreprint
SadafAbdulRauf,SaadiaIshtiaqNauman,HudaSar-
arXiv:2112.07421.
fraz, Hammad Afzal, Arooj Fatima, Sadaf Ziafat,
Momina Ishfaq, and Alishba Abdul Suboor. 2024.
TimoSpinde,KanishkaSinha,NormanMeuschke,and
NLPColabatFIGNEWS2024sharedtask: Biasand
BelaGipp.2021b. Tassy: Atextannotationsurvey
propagandaidentification. InTheSecondArabicNat- system. arXivpreprintarXiv:2112.07391.
uralLanguageProcessingConference(ArabicNLP
2024),Bangkok,Thailand.AssociationforComputa- RobertVaagan,FrøydisJohannessen,andMarieWalsøe.
tionalLinguistics. 2010. Tv news and “white voices”: Dagsrevyen’s
coverage of the gaza war. Javnost - The Public,
FranciscoJavierRodrigo-Ginés,JorgeCarrillodeAl-
17(3):39–56.
bornoz,andLauraPlaza.2024. Asystematicreview
onmediabiasdetection: Whatismediabias,howit TatianaVarachevaandSergiuGherghina.2018. Neutral
isexpressed,andhowtodetectit. or biased? the presentation of the kyrgyzstan and
egyptuprisingsbyrianovosti. Europe-AsiaStudies,
Valle Ruiz-Fernández, José Javier Saiz, and Aitor
70(1):1–23.
Gonzalez-Agirre. 2024. BSC-LANGTECH at
FIGNEWS 2024 shared task: Exploring semi-
Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018.
automaticbiasannotationusingframeanalysis. In
The spread of true and false news online. Science,
The Second Arabic Natural Language Processing
359(6380):1146–1151.
Conference(ArabicNLP2024),Bangkok,Thailand.
AssociationforComputationalLinguistics. Anuradha Yenkikar, C. Narendra Babu, and D. Jude
Hemanth.2022. Sentinet: adeepsentimentanalysis
Noor Sadiah, Sumaya Abdul Rahman, and Sara Al-
network for political media bias detection. Dyna
Emadi.2024. CeasefireatFIGNEWS2024shared
(Spain),97.
task: Automateddetectionandannotationofmedia
biasusinglargelanguagemodels. InTheSecondAra-
WajdiZaghouani,EmanAwad,andAnisCharfi.2016.
bicNaturalLanguageProcessingConference(Ara-
Errorannotationguidelinesforarabictexts. InPro-
bicNLP2024),Bangkok,Thailand.Associationfor
ceedings of the 10th International Conference on
ComputationalLinguistics.
LanguageResourcesandEvaluation(LREC2016),
pages3483–3488.
Amr Khaled Saleh, Huda Maher Mohamed, and
Hager Sayed Rashed. 2024. JusticeLeague at
Wajdi Zaghouani and Anis Charfi. 2018. Annotation
FIGNEWS 2024 shared task: Innovations in bias
guidelinesfortextanalyticsinsocialmedia. InQatar
annotation. InTheSecondArabicNaturalLanguage
Foundation Annual Research Conference Proceed-
ProcessingConference(ArabicNLP2024),Bangkok,
ings2018,pageSSHARP1191.
Thailand.AssociationforComputationalLinguistics.
Xiang Zhou and Reza Zafarani. 2020. A survey of
Karishma Sharma, Feng Qian, Haifeng Jiang, Natali
fakenews: Fundamentaltheories,detectionmethods,
Ruchansky, Minghui Zhang, and Yan Liu. 2019.
andopportunities. ACMComputingSurveys(CSUR),
Combating fake news: A survey on identification
53(5):1–40.
andmitigationtechniques. ACMTransactionsonIn-
telligentSystemsandTechnology(TIST),10(3):1–42.
YangZhou,PaulResnick,andQiaozhuMei.2020. De-
Manan Sharma, Krishanu Kashyap, Kushagra Gupta, tectingpoliticalbiasinsocialmediacontent. InPro-
and Shailender Kumar. 2023. Advancing political ceedingsofthe14thInternationalAAAIConference
bias detection: A novel high-accuracy model. In onWebandSocialMedia(ICWSM),pages299–308.
Proceedings of International Conference on Com-
putationalIntelligenceandSustainableEngineering
Solution,CISES2023.
MarwaSolla,HassanEbrahem,AlyaIssa,HarmainHar-
main,andAbdusalamNwesri.2024. SaharaPioneers
at FIGNEWS 2024 shared task: Data annotation
guidelinesforpropagandadetectioninnewsitems.
InTheSecondArabicNaturalLanguageProcessing
Conference(ArabicNLP2024),Bangkok,Thailand.
AssociationforComputationalLinguistics.A SharedTaskDetails
Weprovidebelowanupdatedversionofthesharedtaskdetailsreflectingthefinaldecisionsmadeinthe
effort,e.g.,weaddedafourthevaluationtrack(IAAQuality)thatwasnotmentionedintheoriginalcall
toparticipate.
A.1 SharedTaskObjectives
Thesharedtaskaimstoserveasacollaborativeplatformwhereparticipantsproposeguidelinesanddiverse
methodsforannotatingandanalyzingthedataset.
ProvidedData Theorganizerswillprovide15batchesofsocialmediaposts,1,000postperbatch. Each
1,000-postbatchwillcontain200postsfrom5languages: Arabic,English,French,Hebrew,andHindi,
togetherwiththeirmachinetranslatedversionsintoArabicandEnglish(asneeded). Participantsmust
specifywhethertheyannotatedtheoriginallanguageoritsmachinetranslatedversion. Soamonolingual
ArabicteamcanannotatethefullbatchinArabic(originalortranslated). Thebatcheswillbeprovidedto
theannotationteamswithclearinstructiononhowtosubmittheresults.
MinimalAnnotationstoQualify Toqualify,eachparticipatingteammustprovidefullannotation
guidelinesforeachsubtasktheychoosetoworkon;andtheymustannotateaminimumoftwobatches,
i.e. (1,800posts)andtheirdesignatedInter-annotatoragreementsubset(200posts)foratotalof2,000
posts(specificallyBatch1andBatch2). TheIAAsubsetmustbedonebyeveryannotatorontheteam;
buttherestcanbedividedamongthem.
A.2 SharedTaskSubtasksandTracks
Therewillbetwosubtasksoffocus. Foreachsubtask,therewillbefourevaluationtracksforwhich
winnerswillbecrowned.
A.2.1 SubtaskonBiasAnnotation
ThesubtaskisrestrictedtosevenpossibleLabels,presentedbelowwithillustrativeexamples.
1. Unbiased
Example: "IntheongoingIsrael-Palestineconflict,recenteventshaveescalatedtensions. Yesterday,
IsraeliforcesconductedoperationsinresponsetorocketattacksfromGaza. Bothsideshavereported
casualties. Internationalleadersarecallingforrestraintandareturntopeacetalks."
2. BiasedagainstPalestine
Example: "Once again, Palestinian aggression has disrupted peace in the region. Palestinian
extremists, ignoring efforts for peace, launched unprovoked attacks on innocent Israeli civilians.
Israel’sresponse,thoughportrayedasharshbysome,isajustifiedmeasuretoprotectitscitizens."
3. BiasedagainstIsrael
Example: "Inatypicaldisplayofexcessiveforce,IsraelitroopshaveyetagaintargetedPalestinian
areas, causing numerous civilian casualties. This aggression, under the guise of self-defense,
highlightstheongoingoppressivetacticsIsraelemploysagainstPalestinians."
4. BiasedagainstbothPalestineandIsrael
Example: "In the latest chapter of their endless and futile conflict, Israeli and Palestinian forces
haveonceagainengagedinsenselessviolence. Bothsidescontinuetocommitatrocities,showinga
completedisregardforpeaceorhumanlife."
5. Biasedagainstothers
Example: "In the shadow of the Israel-Palestine conflict, external actors, particularly Iran, are
exacerbatingtensions. Iran’scovertsupportforextremistgroupsshowsitsintenttodestabilizethe
region,disregardingthecatastrophicimpactonbothIsraeliandPalestiniancivilians."
6. Unclear
Example: "RecentdevelopmentsintheMiddleEasthaveseenanincreaseinhostilities. Thesituation
in the region is complex, with various factors contributing to the current state of affairs. The
internationalcommunityremainsdividedontheissue."7. NotApplicable
Example: "Inothernews,theannualtechnologyconferenceinTelAvivhasunveiledgroundbreaking
advancements in cybersecurity. Industry leaders from around the globe gathered to showcase
innovationsthatpromisetoshapethefutureofdigitalsecurity."
A.2.2 SubtaskonPropagandaAnnotation
ThesubtaskisrestrictedtofourpossibleLabels,presentedbelowwithillustrativeexamples.
1. Propaganda
Example: "Inadisplayofunmatchedheroism,ourtroopshaveonceagainsafeguardedournation
fromthebrinkofdestruction,heroicallyneutralizingthethreatfromGaza,whichaimstoundermine
ourveryexistence."
2. NotPropaganda
Example: "Yesterday,anescalationoccurredalongtheIsrael-Gazaborder,resultingincasualtieson
bothsides. IsraeliandPalestinianofficialsprovidedconflictingaccountsoftheeventsthatledtothe
confrontation."
3. Unclear
Example: "The situation in Gaza remains tense, with reports of civilian distress and military
movements. While some sources claim the military actions are defensive, others argue they are
provocative,leavingthetruenatureofthesituationopentointerpretation."
4. NotApplicable
Example: "AfeatureonGaza’sculturalscenehighlightstheresilienceofitsartcommunity,showcas-
inghowlocalartistsusetheircrafttoexpresshopeandenduranceamidchallengingcircumstances,
withoutdelvingintothepoliticalcontext."
A.2.3 GuidelinesEvaluationTrack
The teams have the freedom to design their own guidelines and apply them to the shared data. The
followingisthechecklistofallitemsthatwillbeevaluatedbytheorganizers.
Annotation Guidelines Detailed annotation guidelines including examples for all main and corner
cases. Considerthefollowingcomponentswhichwillbeusedintheevaluationoftheguidelines.
1. DefinetheObjectiveandDescribetheTask
OutlinethepurposeandspecificNLPtask. Provideadetailedtaskdescriptionwithcorrectexamples.
2. EstablishCategories
Listanddefineallannotationlabels/categories/tags.
3. IncludeDetailedCategoryGuidelineswithExamples
Explain application criteria for each category/tag, with examples. Offer examples for correct
applicationandcommonmistakes.
4. OutlinetheProcess
Describethestep-by-stepannotationprocessandtoolsused.
5. SetQualityStandards
Defineexpectationsforaccuracyandconsistency,alongwithqualitycheckprocedures.
6. HandleAmbiguitiesandDifficultCases
Provideguidanceonambiguouscasesandaprotocolforseekingclarification.
7. EnsureConsistency
Implementmeasuresforannotatorconsistencyandrecommendcalibrationsessions.
8. TrainingandSupport
Detail training procedures and support resources for annotators. Highlight unbiased annotation
practicesandhandlingofsensitivedata. Scheduleguidelinereviewsforupdatesbasedonfeedback
andnewinsights. Includeasystemforannotatorfeedbacktorefineguidelinesandprocesses.Theteamsmustprovidewell-documentedannotationguidelinesincludingexamples,andmustprovide
inter-annotatoragreement(IAA)numbersforatleast200posts(40fromeachlanguage)fromBatch1
andBatch2. WeexpecttheIAAtobecompetitive(e.g. CohenKappaof0.6+)inthetargetspace.
TheGuidelinesScoreusedtodeterminethewinnersofthistrackistheaveragenormalizedDocument
ScoreandIAAKappascore.
(cid:18) (cid:19)
DocumentScore IAAKappa
i i
GuidelinesScore = Average ,
i
DocumentScore IAAKappa
max max
TheDocumentScoreisequaltothenumberofsatisfieddocumentcomponentsmentionedabove, a
rangefrom0to8. TheIAAKappascoreofateamistheaverageofallpairwiseIAAKappasoverteam
annotatorsperbatch.
A.2.4 IAAQualityEvaluationTrack
We will also report the IAA Kappa scores per team and use them to determine the best performers,
independentoftheguidelinestrack.
A.2.5 QuantityEvaluationTrack
The teams can compete in the number of annotated data batches. They must finish them in order and
completeabatchbeforemovingtothenext. Theteamswiththehighestnumberofcompletedbatches
willbecrownedtheQuantitytrackwinnersforthesubtaskofchoice.
A.2.6 ConsistencyEvaluationTrack
Thevariousteamsinthesamesubtaskandsharedcompletedbatcheswillbecomparedforcorrelation
againsteachother. Theteamsthathavethehighestcorrelationagainstotherteams(centroidalchoices)
willbecrownedwinner. Thisneedsaminimumofthreeteamspersubtask.
A.3 Publication
All teams participating in the shared task are invited to submit short paper (4 pages) descriptions of
theirefforts. Thesepaperswillbeevaluatedbymultiplereviewerstobeselectedforpublicationinthe
ArabicNLP2024ConferenceProceedingsandindexedbytheACLAnthology.
A.4 CollaborativeCommitment
Participantsareencouragedtojointhesharedtaskwithacommitmenttocollaboration. Whetherworking
independently or within teams, every effort and insight contributed should be shared openly. This
collaborativeethosextendsbeyondindividualtasksandincludessharingmethodologies,findings,and
results.
A.5 OptionalDemographicDetails
Wewouldliketoinviteparticipantstoprovidesomedemographicdetailsvoluntarily. Thisinformation
includesaspectssuchasagerange,nativelanguage,educationalbackground,areaofstudyorexpertise,
gender,andregionoforigin. Pleasenotethatprovidingthisdemographicinformationisentirelyoptional
and will not influence the evaluation of your participation in any way. We respect your privacy and
understandifyouchoosenottosharethesedetails.B AnnotationInterface
Figure1: AscreenshotoftheGoogleSheetannotationsetupfortheMaindatasubset.C AnnotatorDemographics
Native Language Bias Propaganda Total Total (%) Region of Origin Bias Propaganda Total Total (%)
Arabic 43 24 67 49.3% South Asia 24 21 45 33.1%
Urdu 21 21 42 30.9% Levant 19 17 36 26.5%
Italian 4 2 6 4.4% North Africa 16 2 18 13.2%
Persian 4 0 4 2.9% Western Europe 11 4 15 11.0%
Dutch 3 0 3 2.2% GCC Countries 7 0 7 5.1%
English 2 1 3 2.2% Other Middle Eastern
4 0 4 2.9%
German 1 1 2 1.5% Countries
Hindi/Urdu 2 0 2 1.5% Southeast Asia 1 0 1 0.7%
Russian 1 1 2 1.5% Prefer not to say 3 7 10 7.4%
Spanish 2 0 2 1.5% Total 85 51 136 100.0%
Bengali 1 0 1 0.7%
Prefer not to say 1 1 2 1.5% Education Level Bias Propaganda Total Total (%)
Total 85 51 136 100.0% Master's degree 36 27 63 46.3%
Bachelor's degree 38 12 50 36.8%
Age Range Bias Propaganda Total Total (%) Doctoral degree 8 6 14 10.3%
18-24 48 17 65 47.8% Post-doctoral training 1 0 1 0.7%
25-34 20 19 39 28.7% Prefer not to say 2 6 8 5.9%
35-44 14 9 23 16.9% Total 85 51 136 100.0%
45-54 1 1 2 1.5%
55-64 1 0 1 0.7% Area of Expertise Bias Propaganda Total Total (%)
Prefer not to say 1 5 6 4.4% Engineering &
51 35 86 63.2%
Total 85 51 136 100.0% Technology
Arts and Humanities 18 4 22 16.2%
Gender Bias Propaganda Total Total (%) Social Sciences & Law 10 10 20 14.7%
Female 67 38 105 77.2% Education 3 0 3 2.2%
Male 16 8 24 17.6% Business & Economics 1 1 2 1.5%
Non-binary 1 0 1 0.7% Natural Sciences 1 0 1 0.7%
Prefer not to say 1 5 6 4.4% Prefer not to say 1 1 2 1.5%
Total 85 51 136 100.0% Total 85 51 136 100.0%
Table10: Annotatordemographics.