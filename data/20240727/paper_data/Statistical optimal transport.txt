Statistical Optimal Transport
Sinho Chewi Jonathan Niles-Weed Philippe Rigollet
Yale NYU MIT
Ecole d’Et´e de Probabilit´es de Saint-Flour XLIX
4202
luJ
52
]TS.htam[
1v36181.7042:viXraContents
Preface .................................................. 1
1 Optimal transport ................................... 7
1.1 The optimal transport problem ...................... 7
1.2 Wasserstein distances ............................... 13
1.3 Optimal transport in one dimension .................. 19
1.4 Brenier’s theorem .................................. 21
1.5 Kantorovich duality ................................ 27
1.6 Duality for p = 1................................... 37
1.7 Discussion......................................... 39
1.8 Exercises.......................................... 40
2 Estimation of Wasserstein distances.................. 45
2.1 The Wasserstein law of large numbers................. 45
2.2 The dyadic partitioning argument .................... 46
2.3 Dual chaining bounds............................... 52
2.4 A finer analysis for d = 2............................ 56
2.5 Applications ....................................... 61
2.6 Optimality ........................................ 64
2.7 Faster rates for smooth measures..................... 67
2.8 Regularization of Wasserstein distances ............... 72
2.9 Discussion......................................... 80
2.10 Exercises.......................................... 82
3 Estimation of transport maps ........................ 87
3.1 Problem formulation................................ 87
3.2 The semidual problem and its stability................ 89VIII Contents
3.3 A special case: affine transport maps.................. 92
3.4 Obtaining the slow rate ............................. 93
3.5 The fixed point argument ........................... 97
3.6 Obtaining the fast rate.............................. 98
3.7 Discussion......................................... 102
3.8 Exercises.......................................... 103
4 Entropic optimal transport........................... 107
4.1 Derivation of entropic optimal transport .............. 108
4.2 Duality ........................................... 112
4.3 Statistical rates for dual solutions .................... 115
4.4 Statistical rates for primal solutions .................. 121
4.5 Discussion......................................... 127
4.6 Exercises.......................................... 129
5 Wasserstein gradient flows: theory ................... 133
5.1 Metric derivative and the continuity equation .......... 134
5.2 Elements of Riemannian geometry.................... 138
5.3 The Riemannian structure of Wasserstein space ........ 140
5.4 Otto calculus ...................................... 143
5.5 Bures–Wasserstein ................................. 146
5.6 Gaussian mixtures.................................. 150
5.7 Wasserstein–Fisher–Rao............................. 152
5.8 Mean-field particle systems .......................... 156
5.9 Discussion......................................... 159
5.10 Exercises.......................................... 161
6 Wasserstein gradient flows: applications .............. 165
6.1 Variational inference................................ 165
6.2 Sampling.......................................... 175
6.3 Interacting particle systems.......................... 185
6.4 Non-parametric maximum likelihood.................. 189
6.5 Mean-field neural networks .......................... 193
6.6 Transformers ...................................... 195
6.7 Discussion......................................... 199
6.8 Exercises.......................................... 201
7 Metric geometry of the Wasserstein space............ 203
7.1 Geodesics ......................................... 203
7.2 Curvature ......................................... 209Contents IX
7.3 Tangent cones ..................................... 216
7.4 Discussion......................................... 225
7.5 Exercises.......................................... 226
8 Wasserstein barycenters.............................. 227
8.1 The Hilbert case ................................... 229
8.2 Barycenters on positively curved spaces ............... 231
8.3 Parametric rates for Wasserstein barycenters........... 239
8.4 Discussion......................................... 242
8.5 Exercises.......................................... 242
A Convex analysis...................................... 245
A.1 Convex functions, subdifferentials, and duality ......... 245
A.2 Strong convexity and smoothness..................... 248
A.3 Convex conjugate of a quadratic function.............. 251
B Probability........................................... 253
References............................................... 257
Index.................................................... 281Preface
The history of optimal transport begins in 1781 with a memoir by
GaspardMongethathesubmittedtotheAcad´emiedesSciences[Mon81].
Since then, it has grown into a mature mathematical field with many
important discoveries, such as Kantorovich’s duality theory, Brenier’s
theorem, Otto’s calculus, the JKO scheme, and the Lott–Sturm–Villani
definition of the Ricci curvature of geodesic spaces, to name a few. The
first comprehensive treatment of optimal transport dates back to the
seminal volumes of Rachev and Ru¨schendorf [RR98a, RR98b]. We also
referthereadertotheexcellenttextsofVillani[Vil03,Vil09b],Ambrosio,
Gigli, and Savar´e [AGS08], and more recently, Santambrogio [San15]
for a comprehensive treatment of this subject from the mathematical
perspective, and to the notes of Ambrosio and Gigli [AG13], Ambrosio,
Bru´e, and Semola [ABS21], and the short monograph of Figalli and
Glaudo [FG23] for quicker introductions. Even a quick inspection of
theirtablesofcontentsrevealsthatMonge’squestionwasthegatewayto
manymore,andthatthefieldofoptimaltransporthasmanyunexpected
connections, ranging from geometry to partial differential equations.
More recently, optimal transport has made a resounding entrance
into the field of machine learning under the impetus of Marco Cuturi,
who showed that Wasserstein distances could be computed efficiently
using the Sinkhorn algorithm [Cut13]. This initial spark was followed by
an extensive toolbox, built on optimal transport, that covered multiple
tasks across various areas of machine learning and graphics. The de-
velopment of this toolbox, now called computational optimal transport,
was led by Marco Cuturi and Gabriel Peyr´e and collected in their inspir-
ing manuscript [PC19b]. The far-reaching scope of optimal transport
across machine learning and data science rests on the fact that many2 Preface
objects such as point clouds, polygonal meshes, or even documents can
be encoded as probability measures. In turn, the Wasserstein metric
between these probability measures offers a semantically meaningful
notion of distance.
So what is statistical optimal transport? This modifier comes largely
as an echo to Peyr´e and Cuturi’s computational optimal transport.
It is an umbrella term that captures the remarkably diverse points
of contact between statistics and optimal transport. The aim of the
present monograph is to provide an introduction to a selection of topics
within this burgeoning field according to our tastes (see [PZ20] for a
complementary treatment).
Historically, Wasserstein distances have been employed in statistics
as a tool to quantify the rate of convergence of empirical probability
measures µ to their limit µ. This line of work was inaugurated in a
n
celebrated work of Dudley [Dud69], who provided bounds for W (µ ,µ).
1 n
Wasserstein distances are particularly well-suited for quantifying this
convergence for several reasons. First, unlike the total variation distance
or Kullback–Leibler divergence, the Wasserstein distance between a dis-
crete distribution µ and a potentially continuous one µ remains finite
n
and informative. Second, by definition, bounding the Wasserstein dis-
tanceamountstoexhibitingacouplingbetweenthetwomeasures.Third
and finally, thanks to Kantorovich duality, a bound on the Wasserstein
distance translates into a strong uniform bound on test functions. For
(cid:82) (cid:82)
example, when p = 1, W (µ ,µ) ≤ ε implies that | fdµ − fdµ| ≤ ε
1 n n
for all functions f that are 1-Lipschitz; in fact the two statements are
equivalent (see Section 1.6 for details).
Inthelastdecade,followingtheimpetusofmachinelearning,optimal
transport has percolated to many more aspects of statistics. One of the
mostexcitingdirectionsisanewavenueofresearchthathadlargelybeen
out of reach of classical methods in the past. In this class of problems,
the coupling of data is the main obstacle to statistical analysis. More
concretely, consider a classical statistical setup where one observes
independent copies of a pair of random variables (X,Y) where X is
thought of as input and Y output. Regression falls in this framework,
as does, more generally, all of supervised learning. In particular, the
observed X and Y are coupled. A more challenging model arises when
X and Y are observed in an uncoupled fashion: independent copies of
X and independent copies of Y. Such a setup arises naturally in single-
cell genomics where the destructive nature of the prevailing sequencingPreface 3
processdoesnotallowfortakingmultiplemeasurementsofthesamecell.
This conundrum is a key obstacle to cellular trajectory reconstruction
where one aims at reconstructing the time evolution of a cell in a
genetic landscape; see [SST+19, BMPKC22] for more details. These
problems and more raise fundamental questions about the estimation
of Wasserstein distances and the corresponding couplings, which are
taken up in the first part of this monograph.
The aforementioned applications make use of the role of optimal
transport in endowing the space of probability measures with an inter-
pretable and useful notion of distance. A deeper study of this space,
however, uncovers a rich underlying geometrical structure admitting
descriptions of curvature, geodesics, gradient flows, etc. This geometric
perspective, first advocated by Felix Otto in his seminal article [Ott01],
provides statisticians with powerful new tools for the design and anal-
ysis of algorithms for manipulating probability distributions. A key
development in this regard was the edifying interpretation, by Jordan,
Kinderlehrer, and Otto [JKO98], of the Langevin diffusion as a Wasser-
stein gradient flow of the KL divergence. Since the Langevin diffusion
is popularly employed as a sampling algorithm in Bayesian statistics,
this discovery has ushered in a decade of research linking sampling to
optimization over the Wasserstein space. More broadly, Wasserstein
gradient flows and their variants yield new algorithmic paradigms and
fresh perspectives for diverse problems including the nonparametric
MLE, the dynamics and training of neural networks, and variational
inference (see Chapter 6). Geometric considerations also lead to novel
applications, such as the geometric averaging of data which can be
cast as probability measures, e.g., images or speech. The subject of
Wasserstein geometry is studied in the second half of this monograph.
How to read this book.
This monograph aims to offer a concise introduction to optimal trans-
port, quickly transitioning to its applications in statistics and machine
learning. It is primarily tailored for students and researchers in these
fields, yet it remains accessible to a broader audience of applied mathe-
maticians and computer scientists.
Chapter 1 serves as the gateway to the subsequent chapters by
presenting the foundational concepts of optimal transport that will be
used throughout. The remaining chapters are largely independent, with
the exceptions of chapters 5 and 6, and chapters 7 and 8, which should4 Preface
be studied together. Figure 0.1 illustrates the dependencies between the
various chapters.
1
2 3 4 5 7
6 8
Fig. 0.1. Dependencies between chapters. Solid arrows show prerequisites; dotted
arrows indicate references.
Each chapter concludes with a series of exercises, allowing readers to
apply the concepts learned to questions not addressed in the main text.
Acknowledgments.
Sinho Chewi. Optimal transport was not what I originally planned to
study as a graduate student, but my research career has been infinitely
richer as a result. For this, I am grateful to my advisor and co-author
Philippe for fearlessly inducting me into this beautiful area, and my
academic sibling and co-author Jon for paving the way and constantly
acting as my source of inspiration.
I owe my understanding of optimal transport to many sources, be-
ginning with Villani’s expertly written monograph [Vil03] and includ-
ing countless hours of discussion with collaborators and members of
Philippe’s group. I am happy to have the opportunity to contribute to
the growth of the community through this pedagogical text.
I would like to acknowledge the Institute for Advanced Study for
the hospitable working environment and support through the Eric and
Wendy Schmidt Fund.
New Haven, CT, July 2024.
Jonathan Niles-Weed.Ihavebeenluckytolearnaboutoptimaltrans-
port from many brilliant collaborators; among them, Jason Altschuler,
Francis Bach, Sivaraman Balakrishnan, Quentin Berthet, Marco Cuturi,
Vincent Divol, Alberto Gonza´lez-Sanz, Tudor Manole, Aram-Alexandre
Pooladian, Austin Stromme, and Larry Wasserman have had an es-
pecially large impact on me. I am deeply grateful to them for theirPreface 5
knowledge and insight. It is an honor to be able to share some of their
work in this book.
I am also indebted to students at NYU who participated in a gradu-
ate seminar on portions of this material in 2021 and whose feedback
significantly improved the exposition.
Of course, I would be nowhere without Sinho Chewi and Philippe
Rigollet, my colleagues and friends. You both have inspired me beyond
measure. Collaborating with you has been one of the highlights of my
career, and I hope to have the privilege of many more collaborations
ahead.
My work on these notes was primarily supported by an National Sci-
ence Foundation CAREER award DMS-2339829 and several other NSF
grants (DMS-2015291, DMS-2210583), along with a Sloan Foundation
fellowship and gifts from Apple and Google.
New York, NY, July 2024.
Philippe Rigollet. These notes have grown significantly since I was
honored to give the 2019 St. Flour lecture series, which initially covered
roughly Chapters 1, 7, and 8. Much of the material presented here was
unfamiliar to me when I first delivered the lectures.
First and foremost, I extend my heartfelt thanks to the organizers
(espectially Christophe Bahadoran) and attendees of the 2019 St. Flour
Summer School. The valuable feedback I’ve received from the audience
has been instrumental in motivating the extensive expansion of topics
covered in these notes. My only regret is the delay in producing these
notes, which prevents them from being published concurrently with
those of my wonderful co-lecturers, Nicolas Curien and Elchanan Mossel.
Though we won’t be sharing a volume, we will forever be bonded as
Chevaliers du Taste-fourme.
I have learned most from others and would like to thank my collab-
orators who have taught me so much on this topic: Jason Altschuler,
Julien Clancy, Max Daniels, Aiden Forrow, Borjan Geshkovski, Florian
Gunsilius, Jan-Christian Hu¨tter, Cyril Letrouit, Jean-Michel Loubes,
Chen Lu, Tyler Maunu, Mor Nitzan, Quentin Paris, Geoff Schiebinger,
Justin Stromme, George Stepaniants, Felipe Suarez, William Torous,
Kaizheng Wang, Yuling Yan, Aleksandr Zimin. In particular, for enlight-
ening discussions on optimal transport and other topics, I would like to
specifically acknowledge S´ebastien Bubeck, Ramon van Handel, Thibaut
Le Gouic, Vianney Perchet, Yury Polyanskiy, Maxim Raginsky, and6 Preface
Justin Solomon. Many of the ideas in Chapters 5 and 6 were core discus-
sion topics during the program on Geometric Methods in Optimization
and Sampling at the Simons Institute in Berkeley during Fall 2021. I
extend my gratitude to all the participants, my co-organizers, and Peter
Bartlett, who made this program so wonderful and stimulating.
Since then, the community around statistical optimal transport has
grown significantly, and these notes have benefited from interactions
with many people. Starting with Marco Cuturi, who introduced me to
optimal transport when we were in Princeton together, I also learned a
lot of optimal transport from Guillaume Carlier, Victor Chernozhukov,
Lena¨ıc Chizat, Simone Di Marino, Augusto Gerolin, Promit Ghosal,
Marc Hallin, Zaid Harchaoui, Kengo Kato, Anna Korba, Alexei Krosh-
nin, Qin Li, Jan Maas, Axel Munk, Robert McCann, Dan Mikulincer,
Youssef Marzouk, Soumik Pal, Victor Panaretos, Gabriel Peyr´e, Filippo
Santambrogio, Bodhi Sen, Vladimir Spokoiny, Yair Shenfeld, and Jia-Jie
Zhu among others.
Part of this material has been taught at MIT in 2023 and 2024, at
Universit´e Paris Sorbonne in 2022, and two other summer schools in
2023: The Princeton Machine Learning Theory Summer School and
the CIME summer school in Cetraro, Italy. The audience has given
me excellent feedback, contributing to the improvement of these notes.
Special thanks to Th´eo Dumont, Max Daniels, and Giulia Bertagnolli
for typing up the respective material.
Some typos and errors were fixed by Michael Diao, Haruki Kono,
Aimee Maurais, Yaroslav Mukhin, Madhav Sankaranarayanan, Sabarish
Sainathan, Yucheng Shang, Vishwak Srinivasan, Panos Tsimpos, Oliver
Wang, and Julie Zhu.
Last but not least, I would like to thank my wonderful co-authors,
Sinho Chewi and Jonathan Niles-Weed. Thank you both for joining me
on this epic adventure. I’ve learned more from you than anyone else,
and you have been as much students as you have been teachers to me.
My work on these notes was primarily supported by NSF grant
CCF-1838071 and several other grants from the National Science Foun-
dationduringtheperiod2019-2024(DMS-1712596,CCF-1740751,DMS-
2022448, CCF-2106377). I am also thankful for a gift from Apple. A
first draft was conceived in Spring 2019, when I was supported by the
Eric and Wendy Schmidt Fund at the Institute for Advanced Study.
Cambridge, MA, July 2024.1
Optimal transport
1.1 The optimal transport problem
In his 1781 memoir [Mon81], Monge formulated the following problem:
how can one transport a given pile of sand to fill a given ditch so as
to minimize the cost of transporting the sand? This problem can be
abstracted into a problem involving probability distributions. Indeed,
note first that for this task to be solvable, the pile and the ditch must
occupy the same volume. Without loss of generality, let us normalize
this volume to be 1. We are therefore given two probability measures,
µ and ν over Rd (obviously d = 2 in the case of Monge, but it does not
cost much to consider the more general case). It is often convenient to
reason about two random variables, X ∼ µ, Y ∼ ν. This is our input
to a constrained optimization problem.
1.1.1 The Monge and Kantorovich problems
Back to our sand analogy, transporting the pile means finding a (mea-
surable) function, called a transport map T : Rd → Rd, which indicates
that the sand located at x ∈ Rd should be moved to T(x) ∈ Rd. For
the transport map to actually complete the job (filling the ditch), one
needs to ensure that T(X) ∼ ν whenever X ∼ µ. We say that T pushes
µ to ν or that ν is the pushforward measure of µ (through T) and write
T µ = ν. This is our constraint.
#
Turning now to our objective function, recall that Monge’s question
involved minimizing the cost of transporting the sand. There are many
ways to measure this cost (effort, fuel consumption, etc.) so to simplify
our exposition, we simply measure it in terms of the Euclidean distance8 1 Optimal transport
travelled by the sand. The sand at location x travels a distance of
∥T(x)−x∥. Therefore, the average distance travelled is
(cid:90)
∥T(x)−x∥µ(dx).
TheMongeformulationoftheoptimaltransportproblemistherefore
to minimize the above objective subject to the constraint that T pushes
µ to ν:
(cid:90)
inf ∥T(x)−x∥µ(dx).
T:T µ=ν
#
Note that many choices for the transport cost may be considered. In full
generality, it is customary to consider a general cost c(X,T(X)), where
c(x,y) measures the cost of transporting x ∈ Rd to y ∈ Rd. In this
general framework, we may even allow X and Y to be defined on two
different spaces, not necessarily Rd. In these notes, we focus primarily
on the cases where c(x,y) = ∥x − T(x)∥ or c(x,y) = ∥x − T(x)∥2,
which give rise to the Wasserstein distances. The space Rd may also be
replaced with more complex spaces such as Riemannian manifolds, but
this is generally beyond the scope of these lectures (with the exception
of Section 5.6).
WhiletheMongeproblemiseasytoformulate,weneedtoaskseveral
questions:
• Does there always exists such a valid transport map or, conversely,
is the constraint set empty?
• If there is a minimizer, is it unique? How to characterize it? Note
that our constraint is not convex, which makes finding an answer to
this question rather difficult.
A simple example gives an answer to the first question. Indeed, take
d = 1,assumethatµ = δ isapointmassat0,andthatν = 1 δ +1 δ
0 2 −1 2 1
is a mixture of two point masses. Whatever our choice of the transport
map T, the pushforward T µ is the point mass δ at T(0), so we
# T(0)
cannot achieve the transport at all, at least with a deterministic map.
Intuitively, we would like:
(cid:40)
−1 w.p. 1
T(0) = 2 and T(x) = x, ∀ x ̸= 0.
1 w.p. 1
2
Such a T is not a function but a Markov kernel: it assigns a probability
distribution to each point x ∈ R.1.1 The optimal transport problem 9
The second question remained without a satisfactory answer for
almost two centuries until the Soviet mathematician Leonid Kan-
torovich [Kan42] introduced a relaxation of the problem that exactly
allows for Markov kernels, as discussed in the example above, in a
groundbreaking two-pager. Equivalently, this formulation involves cou-
plings as opposed to maps.
Let µ,ν be two probability measures over Rd and let γ be a coupling
betweenthesetwodistributions,thatis,ajointdistributionoverRd×Rd
such that its first marginal is µ and its second marginal is ν: for any
Borel set A ∈ Rd, we have
γ(A×Rd) = µ(A) and γ(Rd×A) = ν(A).
The terminology coupling comes from the fact that while X ∼ µ and
Y ∼ ν wererandomvariablesthathadnothingtodowitheachother,the
coupling forces them to live on the same probability space by describing
their probabilistic dependence. Here and throughout these notes, we
use the notation Γ for the set of couplings of µ and ν.
µ,ν
Let c : Rd×Rd → [0,∞) be a measurable cost function. The general
Kantorovich formulation of the optimal transport problem consists of
the following optimization problem:
(cid:90)
inf c(x,y)γ(dx,dy). (KOT)
γ∈Γµ,ν
1.1.2 Couplings
To get a better understanding of the Kantorovich problem, it is infor-
mative to explore the set Γ .
µ,ν
Perhaps the simplest coupling is the independent coupling γ = µ⊗ν
where X ∼ µ and Y ∼ ν are simply assumed to be independent: for
any Borel sets A,B ⊆ Rd,
γ(A×B) = µ(A)·ν(B).
In Figure 1.1 (Left), we plot the independent coupling between two
mixtures of Gaussians.
The next proposition collects preliminary facts about Γ .
µ,ν
Proposition 1.1.Let µ,ν be two probability measures on Rd. The set
Γ of couplings between µ and ν is non-empty, convex, and compact
µ,ν
with respect to the topology of weak convergence.10 1 Optimal transport
Proof. Because the independent coupling always exists, we know that
Γ ̸= ∅.
µ,ν
To show that Γ is convex, consider two couplings γ ,γ ∈ Γ
µ,ν 0 1 µ,ν
and for any λ ∈ (0,1) define the mixture γ = (1−λ)γ +λγ . Observe
λ 0 1
that for any Borel set A ∈ Rd,
γ (A×Rd) = (1−λ)γ (A×Rd)+λγ (A×Rd)
λ 0 1
= (1−λ)µ(A)+λµ(A) = µ(A).
Hence the first marginal of γ is given by µ and by the same argument
λ
its second marginal is given by ν. Thus γ ∈ Γ for any λ ∈ (0,1),
λ µ,ν
whence Γ is convex.
µ,ν
To complete the proof of our proposition, we show that Γ is
µ,ν
compact. By Prokhorov’s theorem (Theorem B.3), it is sufficient to
show that Γ is closed and (uniformly) tight. To that end, recall that
µ,ν
from Prokhorov’s theorem, the constant sequences (µ) ,(ν) are both
n n
tight, so that for any ε > 0, there exists a compact set K ⊂ Rd such
that µ(Kc)+ν(Kc) < ε. Then the set K ×K is also compact and for
any γ ∈ Γ ,
µ,ν
γ((K ×K)c) ≤ γ(Rd×Kc)+γ(Kc×Rd) = µ(Kc)+ν(Kc) < ε.
Hence, Γ is tight. Moreover, since γ ∈ Γ is equivalent to
µ,ν µ,ν
(cid:90) (cid:90) (cid:90) (cid:90)
f(x)γ(dx,dy) = fdµ and f(y)γ(dx,dy) = fdν
for all bounded continuous f : Rd → R, by the definition of weak
convergence (Theorem B.4) it follows that Γ is closed. Therefore,
µ,ν
Prokhorov’s theorem yields that Γ is compact. ⊔⊓
µ,ν
A coupling γ ∈ Γ captures the dependence between two random
µ,ν
variables X ∼ µ and Y ∼ ν. At the opposite extreme of the independent
coupling, assume that X ∼ N(0,1) and Y ∼ χ2 and observe that Y
1
has the same distribution as X2. Then we can take the deterministic
coupling such that Y = X2:
γ(dx,dy) = µ(dx)δ (dy).
x2
We plot this coupling in Figure 1.1 (Right); observe that it is degenerate.
To continue or exploration of couplings, assume that X ∼ N(0,1)
and Y ∼ N(0,1), then we can take any coupling where1.1 The optimal transport problem 11
Fig. 1.1. (Left) Independent coupling of a mixture of two Gaussians. (Right)
Deterministic coupling of X ∼N(0,1) with Y ∼χ2.
1
(cid:18) (cid:19) (cid:18)(cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
X 0 1 ρ
∼ N , (1.1)
Y 0 ρ 1
and ρ ∈ [−1,1] is the correlation between X and Y. See Figure 1.2.
Fig. 1.2. The bivariate Gaussian coupling (1.1) for five different values of ρ.
The content of Brenier’s theorem later in this chapter is that under
mild regularity conditions, the solution of the Kantorovich problem with
quadratic cost is achieved by a deterministic coupling. These degenerate
couplings are extreme points of the set Γ . The fact that extreme
µ,ν
points are optimal couplings can be seen simply when µ and ν are
discrete measures, as we discuss next.
1.1.3 Discrete optimal transport
The case where µ and ν are two discrete distributions is of special
practical relevance. For example, µ,ν can be empirical measures on a
point cloud. Consider the case where12 1 Optimal transport
m n
(cid:88) (cid:88)
µ = p δ , and ν = q δ .
i xi j yj
i=1 j=1
In this case, a coupling γ of X ∼ µ and Y ∼ ν is characterized by a
non-negative matrix P ∈ Rm×n where P = γ(X = x ,Y = y ). The
i,j i j
marginal constraints on γ ∈ Γ readily translate into
µ,ν
(cid:88) (cid:88)
∀i ∈ [m], P = p , and ∀j ∈ [n], P = q .
i,j i i,j j
j∈[n] i∈[m]
Introducing1 ,1 fortheall-onesvectorsofsizesmandn,respectively,
m n
these constraints can be represented concisely as P1 = p, PT1 = q,
n m
where p = (p ,...,p )T and q = (q ,...,q )T.
1 n 1 m
Likethecoupling,thecostccanalsobecapturedbyanm×nmatrix
C where C = c(x ,y ) for i ∈ [m], j ∈ [n]. The Kantorovich optimal
i,j i j
transport problem (KOT) is therefore equivalent to
(cid:88)
min C P s.t. P1 = p, PT1 = q,
i,j i,j n m
P∈Rm×n
+ i,j∈[n]
which can also be written more concisely as
min ⟨C,P⟩ s.t. P1 = p, PT1 = q,
n m
P∈Rm×n
+
where ⟨C,P⟩ = tr(CTP) is the Frobenius inner product on the set of
m×n real matrices.
In particular, when m = n and all of the weights p , q are equal to
i j
1/n, the set of valid coupling matrices P is (a multiple of) the set of
doubly stochastic matrices, also known as the Birkhoff polytope:
Birk := {γ ∈ Rn×n : γ1 = 1 , 1Tγ = 1T}. (1.2)
+ n n n n
Then, (KOT) reduces to
min ⟨C,P⟩. (1.3)
P∈n−1Birk
The extreme points of the Birkhoff polytope are permutation matrices:
they are binary matrices π ∈ {0,1}n×n with exactly one non-zero
entry in each row and column. In particular, general principles of convex
geometry imply thatthe solution to any linear programof the form (1.3)
can be taken to be a matrix of the form n−1π. A transport plan of this
form is induced by a deterministic map (the permutation), and hence
in this case there is a solution to the Monge problem. As we shall see
in the subsequent sections, extreme points of Γ play a special role
µ,ν
more generally in the geometry of the optimal transport problem.1.2 Wasserstein distances 13
1.2 Wasserstein distances
The Kantorovich problem (KOT) makes sense for a wide variety of cost
functions, with different interpretations in each case. For instance, one
natural example comes from taking c(x,y) = 1 to be the trivial
x̸=y
metric. In this case, (KOT) gives:
inf γ(X ̸= Y)
γ∈Γµ,ν
which is a well-known formulation of the total variation distance. (See
Exercise 9.) Note, however, that the trivial distance 1 is unrelated
x̸=y
to the geometry of Rd. In particular, it does not say whether x and y
are far from each other but only if they are different. This limitation
manifests itself in the total variation. Indeed, if µ = δ and ν = δ , then
x y
the objective of (KOT) is equal to 1 as soon as x ̸= y.
To obtain a geometrically meaningful quantity from the Kantorovich
problem, we need to choose a cost that reflects the actual distance
between x and y. This idea gives rise to the Wasserstein distances.
For any p ≥ 1, let P (Rd) be the set of probability measures over Rd
p
equipped with the Euclidean norm ∥·∥ that have finite p-th moment:
(cid:90)
µ ∈ P (Rd) ⇔ ∥x∥pµ(dx) < ∞.
p
The p-Wasserstein distance between two probability measures µ,ν ∈
P (Rd) is defined by
p
(cid:18)(cid:90) (cid:19)1/p
W (µ,ν) = inf ∥x−y∥pγ(dx,dy) ,
p
γ∈Γµ,ν
where we recall that Γ is the set of couplings between µ and ν.
µ,ν
We first show that in fact the above infimum is attained. To that
end, define
(cid:90)
I(γ) := ∥x−y∥pγ(dx,dy)
and observe that by definition, there exists a sequence (γ ) in Γ
n n µ,ν
such that I(γ ) → Wp(µ,ν). Since Γ is compact (Proposition 1.1),
n p µ,ν
there is a subsequence of (γ ) which converges to some γ¯ ∈ Γ . By
n n µ,ν
definitionW (µ,ν) ≤ I(γ¯).Since(x,y) (cid:55)→ ∥x−y∥pisunbounded,I isnot
p
continuous,butitislowersemicontinuous,soI(γ¯) ≤ liminf I(γ ) =
n→∞ n
Wp(µ,ν) (part three of the portmanteau theorem, Theorem B.4). Hence
p14 1 Optimal transport
I(γ¯) = Wp(µ,ν). Note that the only property of the cost function
p
we used in this proof is lower semicontinuity so this argument readily
extends to more general costs.
We can therefore adopt the following definition of Wasserstein dis-
tances. Note that these distances should really be called Kantorovich–
Rubinstein distances but we stick to the modern trend of “Wassersteini-
fication”.
Definition 1.2.The p-Wasserstein distance between two probability
measures µ,ν ∈ P (Rd) is defined by
p
(cid:18)(cid:90) (cid:19)1/p
W (µ,ν) = min ∥x−y∥pγ(dx,dy) .
p
γ∈Γµ,ν
Proposition 1.3.The p-Wasserstein distance defines a metric over
P (Rd), that is for every µ,ν ∈ P (Rd), it holds
p p
1.W (µ,ν) ≥ 0
p
2.W (µ,ν) = W (ν,µ)
p p
3.W (µ,ν) = 0 iff µ = ν
p
4.W (µ,ν) ≤ W (µ,ρ)+W (ρ,ν) for any ρ ∈ P (Rd).
p p p p
Proof. Note first that 1. and 2. hold trivially.
Wenowturntotheproofof3.Ifµ = ν,thenthemeasureγ(dx,dy) =
µ(dx)δ (dy) is a valid coupling: γ ∈ Γ . Concretely, γ is the law of
x µ,ν
(X,X) for X ∼ µ. Therefore
(cid:90) (cid:90)
0 ≤ Wp(µ,µ) ≤ ∥x−y∥pγ(dx,dy) = ∥x−x∥pµ(dx) = 0.
p
To show the other direction of 3., observe that if W (µ,ν) = 0, there
p
exists γ¯ ∈ Γ such that (X,Y) ∼ γ¯, and X = Y almost surely; in
µ,ν
particular, they must have the same distribution: µ = ν.
To complete the proof, we check the triangle inequality 4. To that
end, we employ the gluing lemma (Lemma B.5) which ensures that
there exists X,Y,Z such that X ∼ µ, Y ∼ ν, Z ∼ ρ and such that
(X,Z) and (Z,Y) are optimally coupled.
Then
W (µ,ν) ≤
(cid:0)E∥X −Y∥p(cid:1)1/p
p
=
(cid:0)E∥X
−Z +Z
−Y∥p(cid:1)1/p
≤
(cid:0)E∥X −Z∥p(cid:1)1/p +(cid:0)E∥Z −Y∥p(cid:1)1/p1.2 Wasserstein distances 15
= W (µ,ρ)+W (ρ,ν),
p p
where in the first line we used the suboptimality of the coupling (X,Y),
in the third line we used the triangle inequality for Lp norms, and in the
last line, we used the optimality of the couplings (X,Z) and (Z,Y). ⊔⊓
Example 1.4(Wasserstein distances in simple cases).
1.Fix x,y ∈ Rd. Then
W (δ ,δ ) = ∥x−y∥.
p x y
Therefore, (Rd,∥·∥) is isometrically embedded in (P (Rd),W ) via
p p
x (cid:55)→ δ .
x
2.Fix x,y ∈ Rd and 0 ≤ λ,τ ≤ 1. Then
W (cid:0) λδ +(1−λ)δ , τ δ +(1−τ)δ (cid:1) = |λ−τ|1/p∥x−y∥.
p x y x y
Note that it follows from ordering of the Lp norms that W (µ,ν) ≤
p
W (µ,ν) whenever p ≤ q. In particular, the smallest of the Wasserstein
q
distances is W .
1
Wasserstein distances induce a useful topology on random variables:
they metrize weak convergence on compact spaces; see Appendix B for
background. More specifically, a sequence (µ ) satisfies W (µ ,µ) → 0
n n p n
if and only if it converges weakly to µ, denoted µ (cid:44)→ µ, and the
n
p-th moment converges: (cid:82) ∥·∥pdµ → (cid:82) ∥·∥pdµ. This “metrization”
n
property can be found in all of the main texts on optimal transport and
has often been employed as a justification for the use of Wasserstein
distance as opposed to other distances. This is hardly a discriminating
feature, however, and many other distances (L´evy–Prokhorov, Fortet–
Mourier, etc.) also have this property; see [Vil09b, Chapter 6]. In fact,
this folklore result does not do justice to the quantitative meaning of
W (µ,ν) ≤ ε for some ε.
p
For example, the following statement implies that if two random
variables with sufficiently light tails are close in p-Wasserstein distance
foranyp > 1,thenalloftheirmomentsmustalsobeclose.Weformalize
the assumption that the tails are light by considering sub-exponential
random variables, that is, random variables Z satisfying
Ee|Z| ≤ 2. (1.4)
For such random variables, we have the following bound.16 1 Optimal transport
Proposition 1.5.LetX ∼ µandY ∼ ν betwosub-exponentialrandom
variables. Then, for any p > 1, there exists a constant C > 0 such that
p
for any integer ℓ ≥ 1, it holds
(cid:12) (cid:12)E|X|ℓ−E|Y|ℓ(cid:12) (cid:12) ≤ (C pℓ)ℓW p(µ,ν).
Proof. By convexity of the function x (cid:55)→ |x|ℓ, ℓ ≥ 1, it holds
|X|ℓ−|Y|ℓ ≤ ℓ|X −Y|(|X|ℓ−1∨|Y|ℓ−1).
TakingexpectationonbothsidesandapplyingHo¨lder’sinequalityyields
for any coupling (X,Y),
(cid:12) (cid:12)E|X|ℓ−E|Y|ℓ(cid:12)
(cid:12) ≤
ℓ(cid:0)E|X−Y|p(cid:1)1/p(cid:0)E(|X|ℓ−1∨|Y|ℓ−1)q(cid:1)1/q
,
1 +1
= 1.
p q
Taking the optimal coupling between X and Y yields
(cid:12) (cid:12)E|X|ℓ−E|Y|ℓ(cid:12)
(cid:12) ≤ ℓW
p(µ,ν)(cid:0)E(|X|ℓ−1∨|Y|ℓ−1)q(cid:1)1/q
.
Toconclude,recallthatthatitisastandardpropertyofsub-exponential
randomvariables[Ver18]thatifZ issub-exponential,then(E[|Z|k])1/k ≤
k for all k ≥ 1. Thus
(cid:0)E(|X|ℓ−1∨|Y|ℓ−1)q(cid:1) (ℓ−1
1)q ≤ 2(ℓ−1)q ≤ 2
ℓp
.
p−1
⊔⊓
Theaboveresultisencouraging:obtainingboundsontheWasserstein
distance between two measures implies quantitative bounds on the
distance between their moments. It could be the case, though, that
the Wasserstein distance tends to be quite large compared to other
commonly used distances or divergences such as total variation or the
Kullback–Leibler divergence; see [Tsy09, Chapter 2] for a list of such
distances, their comparison, and relevance to statistical problems.
It turns out, however, that the Wasserstein distance can often be
controlled by other commonly used distances. For example, the next
result shows that on a bounded domain, the Wasserstein distance is
dominatedbythetotalvariationdistance(seeExercise9forbackground).
Moreover, its proof is our first illustration of how to bound Wasserstein
distances—it suffices to exhibit a (suboptimal) coupling γ such that
E ∥X −Y∥p is controlled appropriately.
γ1.2 Wasserstein distances 17
Theorem 1.6.Let µ,ν ∈ P (Rd) be two distributions with densities f
p
and g respectively. Then, for any p ≥ 1, it holds
(cid:90)
Wp(µ,ν) ≤ 2p−1 inf ∥x−x ∥p|f(x)−g(x)|dx.
p 0
x0∈Rd
In particular, if the supports of both µ and ν are included in the same
ball of diameter D, then
Wp(µ,ν) ≤ Dpd (µ,ν),
p TV
where d (µ,ν) is the total variation distance between µ and ν and is
TV
defined by
(cid:90)
1
d (µ,ν) = |f(x)−g(x)|dx.
TV
2
Proof. Assume that µ ̸= ν as otherwise the statement is trivial. As
mentioned before the statement of the theorem, we construct an explicit
coupling between µ and ν. To that end, consider the three positive
functions (f −g) , (f −g) , and f ∧g (see Figure 1.3 for reference)
+ −
and observe that:
(cid:82) (cid:82) (cid:82)
Fig. 1.3. The integrals (f −g) , (f −g) , and f ∧g correspond to the blue,
+ −
red, and orange regions, respectively.
(cid:90) (cid:90) (cid:90)
(cid:0) (cid:1)
(f −g) −(f −g) = f − g = 1−1 = 0
+ −
so that
(cid:90) (cid:90)
(f −g) = (f −g) =: t > 0
+ −
and
(cid:90) (cid:18)(cid:90) (cid:90) (cid:90) (cid:90) (cid:19)
1
f ∧g = f + g− (f −g) − (f −g) = 1−t.
+ −
218 1 Optimal transport
Next, we normalize these functions to obtain three densities
1 1 1
h = (f −g) , h = (f −g) , h = f ∧g.
+ + − − ∧
t t 1−t
We can rewrite f and g as mixtures of the above densities:
f = th +(1−t)h , g = th +(1−t)h .
+ ∧ − ∧
Next, let Z ,Z , and Z be three independent random variables with
+ − ∧
densities h ,h , and h respectively and let B be a Bernoulli random
+ − ∧
variable with parameter t ∈ (0,1], independent of Z ,Z , and Z .
+ − ∧
We are now in a position to define our coupling between µ and ν.
To that end, let (X,Y) be a random pair such that
X = BZ +(1−B)Z ,
+ ∧
Y = BZ +(1−B)Z ,
− ∧
and observe that the distribution γ of (X,Y) is indeed a valid coupling
betweenµandν.Usingthisfacttogetherwiththeinequality∥x−y∥p ≤
2p−1(∥x−x ∥p+∥y−x ∥p), we get
0 0
Wp(µ,ν) ≤ E ∥X −Y∥p
p γ
(cid:90)
= P(B = 0)·0+P(B = 1) ∥x−y∥ph (x)h (y)dxdy
+ −
(cid:18)(cid:90) (cid:90) (cid:19)
≤ t2p−1 ∥x−x ∥ph (x)dx+ ∥y−x ∥ph (y)dy
0 + 0 −
(cid:18)(cid:90) (cid:19)
= t2p−1 ∥x−x ∥p(h (x)+h (x))dx
0 + −
(cid:18)(cid:90) (cid:19)
= 2p−1 ∥x−x ∥p(cid:0) (f −g) (x)+(f −g) (x)(cid:1) dx
0 + −
(cid:18)(cid:90) (cid:19)
= 2p−1 ∥x−x ∥p|f(x)−g(x)|dx
0
and the result follows by minimizing the right-hand side with respect to
x . The second statement follows easily by taking x to be the center
0 0
of said ball. ⊔⊓
Theassumptionthatµandν areabsolutelycontinuousissuperfluous
and the exact same proof follows by manipulating measures rather
than densities, albeit with slightly more opaque notation; see [Vil09b,
Theorem 6.15].1.3 Optimal transport in one dimension 19
1.3 Optimal transport in one dimension
To gain a bit of insight into optimal transport, we look at the simpler
case where µ and ν are probability measures on the real line. In this
case, we may define their associated cumulative distribution functions.
Recall that the cumulative distribution function (CDF) of a random
variables Z is the function F : R → [0,1] defined by
F(t) := P(Z ≤ t), t ∈ R.
Since F is monotonically non-decreasing, we may define its pseudo-
inverse F† by
F†(u) = inf{t ∈ R : F(t) ≥ u}, u ∈ [0,1],
with the convention that inf∅ = ∞. While F† is not an inverse per se,
it does satisfy the following property:
F†(u) ≤ t ⇔ u ≤ F(t) (1.5)
This property is often used to simulate random variables. Let U ∼
Unif([0,1]) be a uniform random variable, then Z ∼ F†(U) has CDF F.
Indeed, for any t ∈ R,
P(Z ≤ t) = P(F†(U) ≤ t) = P(U ≤ F(t)) = F(t). (1.6)
The following theorem characterizes optimal transport in one dimen-
sion in terms of CDFs.
Theorem 1.7.Let µ,ν ∈ P (R) be two probability distributions with
1
CDFs F and F respectively. Let U ∼ Unif([0,1]) be a uniform random
µ ν
variable and denote by γ¯ the distribution of (F†(U),F†(U)). Then γ¯ ∈
µ ν
Γ is a valid coupling between µ and ν and it is optimal:
µ,ν
(cid:90) (cid:90)
W (µ,ν) = |x−y|γ¯(dx,dy) = min |x−y|γ(dx,dy).
1
γ∈Γµ,ν
Moreover,
(cid:90) ∞
(cid:12) (cid:12)
W 1(µ,ν) = (cid:12)F µ(t)−F ν(t)(cid:12)dt.
−∞20 1 Optimal transport
Proof. It follows from (1.6) that γ¯ ∈ Γ and it remains to check that
µ,ν
it is optimal. To that end, observe that for any γ ∈ Γ , it follows from
µ,ν
Fubini’s theorem that for (X,Y) ∼ γ,
(cid:90) (cid:90)(cid:90) ∞
(cid:0) (cid:1)
|x−y|γ(dx,dy) = 1I +1I dtγ(dx,dy)
x≤t<y y≤t<x
−∞
(cid:90) ∞
(cid:0) (cid:1)
= γ(X ≤ t < Y)+γ(Y ≤ t < X) dt
−∞
(cid:90) ∞
(cid:0) (cid:1)
= γ(X ≤ t)+γ(Y ≤ t)−2γ(X ≤ t,Y ≤ t) dt
−∞
(cid:90) ∞
(cid:0) (cid:1)
≥ F (t)+F (t)−2(F (t)∧F (t)) dt
µ ν µ ν
−∞
(cid:90) ∞
(cid:12) (cid:12)
= (cid:12)F µ(t)−F ν(t)(cid:12)dt.
−∞
To show that the above inequality becomes an equality when γ = γ¯,
observe that
γ¯(X ≤ t,Y ≤ t) = P(F†(U) ≤ t,F†(U) ≤ t)
µ ν
= P(U ≤ F (t),U ≤ F (t))
µ ν
= P(U ≤ F (t)∧F (t))
µ ν
= F (t)∧F (t).
µ ν
We have proved
(cid:90) (cid:90) ∞ (cid:90)
(cid:12) (cid:12)
|x−y|γ(dx,dy) ≥ (cid:12)F µ(t)−F ν(t)(cid:12)dt = |x−y|γ¯(dx,dy)
−∞
so that γ¯ is an optimal coupling. ⊔⊓
If Z admits a density, then its CDF F is actually a left inverse of F†,
i.e., F ◦F† = Id. If µ has a density, this fact implies that the optimal
coupling γ¯ takes the following special form. If X ∼ µ, then
(X,F†◦F (X)) ∼ γ¯. (1.7)
ν µ
In other words, the solution to the Monge problem and the Kantorovich
problem coincide since we have found a transport map T¯ = F−1◦F
ν µ
such that T¯ µ = ν and
#
(cid:90) (cid:90)
|x−T¯(x)|µ(dx) = min |x−y|γ(dx,dy)
γ∈Γµ,ν1.4 Brenier’s theorem 21
(cid:90)
= min |x−T(x)|µ(dx).
T:T µ=ν
#
Although we have focused on the W distance in this section, the
1
coupling γ¯ given in (1.7) turns out to be universally optimal, in the
sense that it is optimal for the Kantorovich problem for any strictly
convex cost (a cost of the form c(x,y) = h(x−y) where h : R → R is
strictly convex); this includes all W distances for p > 1. See Exercise 8.
p
Note that T¯ is a monotone increasing function as the composition of
two increasing functions. Continuous monotone functions in one dimen-
sion are derivatives of convex functions, suggesting that this property
may be generalized to higher dimension by considering gradients of
convex functions. Existence of such monotone transport maps in higher
dimensions is the content of the influential result of Brenier [Bre87],
which we explore next.
1.4 Brenier’s theorem
The p-Wasserstein distance is a natural object for any p ≥ 1. However,
the cases p = 1,2 possess remarkable special structure, and we focus on
them in much of what follows. We first explore the case p = 2, which is
notable for its close connection to convex analysis.
Recall that
(cid:90)
W2(µ,ν) = min ∥x−y∥2γ(dx,dy). (W2)
2 2
γ∈Γµ,ν
Theorem 1.8(Brenier). Let µ,ν ∈ P (Rd) be two probability mea-
2
sures such that µ has a density and let X ∼ µ. If γ¯ is an optimal
coupling for (W2),
2
(cid:90) (cid:90)
∥x−y∥2γ¯(dx,dy) = min ∥x−y∥2γ(dx,dy) = W2(µ,ν),
2
γ∈Γµ,ν
then there exists a µ-almost everywhere differentiable convex function
φ : Rd → R such that (X,∇φ(X)) ∼ γ¯ ∈ Γ .
µ,ν
Before turning to the proof, we first consider a first statistical im-
plication of Brenier’s theorem. Brenier’s theorem asserts that, as long
as µ has a density, for any ν ∈ P (Rd), there exists a convex function
2
ϕ so that ∇ϕ µ = ν. Since gradients of convex functions are natural
#22 1 Optimal transport
analogues of monotone functions in higher dimensions, this theorem
is therefore a significant generalization of the classical univariate fact
mentioned in Section 1.3, that if U ∼ Unif([0,1]), then F†(U) ∼ ν.
ν
In one dimension, the function F† is known as the quantile func-
ν
tion of ν, and is of fundamental statistical significance. Brenier’s the-
orem therefore can be used to define a multivariate notion of quan-
tiles [CGHH17, HdBCAM21]. This point of view has proven to be
extremely fruitful and has led to a wide range of statistical applica-
tions [Hal22]. (For more details, see the discussion section.)
Returning to the content of Brenier’s theorem, at this point it
is not obvious what optimal transport has to do with gradients of
convex functions. We therefore begin by studying such gradients to gain
intuition.
1.4.1 Gradients of convex functions
Note first that a continuous function f : R → R is such that f = φ′ for
some differentiable convex function φ if and only if f is non-decreasing.
Indeed, convexity of φ implies that for any x,y ∈ R:
φ(x)−φ(y) ≤ (x−y)φ′(x), (1.8)
φ(y)−φ(x) ≤ (y−x)φ′(y). (1.9)
Summing the above two inequalities yields
(x−y)(φ′(x)−φ′(y)) ≥ 0,
so that φ′ is non-decreasing.
Is there an analogue of this statement for functions on Rd? Of course
we immediately get that for any x,y ∈ Rd
⟨x−y,∇˜φ(x)−∇˜φ(y)⟩ ≥ 0,
where ∇˜φ(x) ∈ ∂φ(x) denotes a subgradient of φ at x (see Appendix A
for preliminaries on convex analysis). Unfortunately, while in dimension
1, the two-point inequalities (1.8)–(1.9) imply inequalities for any ar-
rangement of points, in higher dimension this is no longer the case and
we need to capture additional information.
In fact, convexity implies many such inequalities: for any integer
k ≥ 2, and any collection of points x ,...,x ∈ Rd, we have
1 k
φ(x )−φ(x ) ≤ ⟨x −x ,∇˜φ(x )⟩, i = 1,...,k−1,
i i+1 i i+1 i1.4 Brenier’s theorem 23
φ(x )−φ(x ) ≤ ⟨x −x ,∇˜φ(x )⟩.
k 1 k 1 k
Summing these inequalities yields:
k
(cid:88)
⟨x −x ,∇˜φ(x )⟩ ≥ 0, (1.10)
i i+1 i
i=1
with the convention that x = x .1
k+1 1
1.4.2 Cyclical monotonicity
Since there may exist several points in the subdifferential of φ at x, we
first describe the graph {(x,∇˜φ(x)) : x ∈ Rd} before thinking about
∇˜φ(·) as a map from Rd to Rd. We first define an important property
of such graphs.
Definition 1.9.A set A ⊂ Rd×Rd is said to be cyclically monotone
if for any integer k ≥ 2, and points (x ,y ) ∈ A, i = 1,...,k, it holds
i i
k
(cid:88)
⟨x −x ,y ⟩ ≥ 0, (1.11)
i i+1 i
i=1
with the convention that x = x .
k+1 1
In light of (1.10), the set ∂φ ⊂ Rd × Rd is cyclically monotone
whenever φ is convex. It turns out that all cyclically monotone subsets
of Rd×Rd are of this form.
Theorem 1.10(Rockafellar). A set A ⊂ Rd×Rd is cyclically mono-
toneifandonlyifthereexistsaclosedconvexfunctionφ : Rd → R∪{∞}
such that
A ⊆ ∂φ.
The proof of this classical theorem of convex analysis can be found
in Appendix A.
Note that condition (1.11) is equivalent to the requirement that
k k
(cid:88) (cid:88)
∥x −y ∥2 ≤ ∥x −y ∥2 (1.12)
i i i+1 i
i=1 i=1
1 With this convention, the points x →x →···→x →x form a “cycle.”
1 2 k 124 1 Optimal transport
for any points (x ,y ) ∈ A, i = 1,...,k. This formulation enables us to
i i
seetheconnectionwithoptimaltransport.Indeed,considerthefollowing
example for illustration purposes. Let
n n
1 (cid:88) 1 (cid:88)
µ = δ , ν = δ .
n
aj
n
bj
j=1 j=1
In this discrete case, the set of all couplings between µ and ν can be
identified with the Birkhoff polytope (see Section 1.1.3), whose extreme
points are rescaled permutation matrices. Since the discrete optimal
transport problem is a linear program, solutions can be taken to be ex-
treme points. We may therefore restrict our attention to couplings given
by permutations. Concretely, a permutation σ of {1,...,n} corresponds
to the coupling
n
1 (cid:88)
γ = δ .
n (aj,b σ(j))
j=1
Such a coupling is optimal if its cost is minimal among all permutations,
that is, if
n n
(cid:88) (cid:88)
∥a −b ∥2 ≤ ∥a −b ∥2, ∀τ . (1.13)
j σ(j) j τ(j)
j=1 j=1
This condition is precisely equivalent to the support of γ being
cyclically monotone. Indeed, by relabeling the atoms of ν, we may
assume without loss of generality that σ is the identity permutation.
Then the support of γ consists of the pairs (a ,b ), j ∈ {1,...,n}.
j j
Given any subset of k distinct points (x ,y ) = (a ,b ) ∈ supp(γ),
i i ji ji
i = 1,...,k, let τ be the cyclic permutation of {j ,...,j } that leaves
1 k
other indices unchanged

j if j = j , i ∈ {2,...,k}
 i−1 i

τ(j) = j if j = j
k 1

j otherwise.
Then (1.13) implies (1.12). In fact, since any permutation τ can be
decomposed into cycles, similar reasoning then shows that (1.12) is also
a sufficient condition for (1.13) to hold.
The preceding discussion indicates that, in the discrete case, the
support of an optimal coupling is cyclically monotone. A similar phe-
nomenonholdsinthegeneralcase;however,theargumentgivenaboveis1.4 Brenier’s theorem 25
not valid when γ does not assign positive mass to points in its support.
Nevertheless, the following result shows that a similar strategy can
be made to work by reasoning about small neighborhoods (e.g., balls)
rather than individual points. Some care is required to ensure that it
is possible to modify γ on such neighborhoods while maintaining the
constraint γ ∈ Γ .
µ,ν
Proposition 1.11.Let γ¯ ∈ Γ be an optimal coupling between µ and
µ,ν
ν in the sense that
(cid:90) (cid:90)
∥x−y∥2γ¯(dx,dy) = min ∥x−y∥2γ(dx,dy) = W2(µ,ν).
2
γ∈Γµ,ν
Then supp(γ¯) is cyclically monotone.
Proof. Suppose that S := supp(γ¯) is not cyclically monotone. Then
there exists k ≥ 2 and (x ,y ) ∈ S, i = 1,...,k such that
i i
k k
(cid:88) (cid:88)
∥x −y ∥2 > ∥x −y ∥2,
i i i+1 i
i=1 i=1
and by continuity of the Euclidean norm, there exist neighborhoods
U ,V of x ,y respectively for i = 1,...,k such that γ¯(U ×V ) > 0 and
i i i i i i
k k
(cid:88) (cid:88)
∥x˜ −y˜∥2 > ∥x˜′ −y˜′∥2, (1.14)
i i i+1 i
i=1 i=1
for all x˜ ,x˜′ ∈ U ,y˜,y˜′ ∈ V , i = 1,...,k.
i i i i i i
Now, let γ ,i = 1...,k be a family of (conditional) probability
i
distributions on Rd×Rd defined such that γ (A) = γ¯(A | U ×V ) for
i i i
any Borel set A ⊂ Rd×Rd. Next, let γ(1) and γ(2) denote the first and
i i
second marginal of γ respectively and define the mixture:
i
k
c (cid:88) (1) (2)
γ = γ¯+ (γ ⊗γ −γ ),
k i+1 i i
i=1
wherec > 0istobechosenlaterandwiththeconventionthatγ = γ .
k+1 1
Note that for any Borel set A ⊂ Rd×Rd, it holds
k
c (cid:88)
γ(A) ≥ γ¯(A)− γ (A)
i
k
i=126 1 Optimal transport
k
c (cid:88)
= γ¯(A)− γ¯(A | U ×V )
i i
k
i=1
k
c (cid:88) γ¯(A∩(U i×V i))
= γ¯(A)−
k γ¯(U ×V )
i i
i=1
k
cγ¯(A) (cid:88) 1
≥ γ¯(A)− .
k γ¯(U ×V )
i i
i=1
Thus γ(A) ≥ 0 if c < min γ¯(U ×V ). Moreover, γ(Rd×Rd) = 1 so
i∈[k] i i
that γ is indeed a probability distribution over Rd×Rd.
To check that γ ∈ Γ observe that for any Borel set B ⊂ Rd,
µ,ν
k
γ(B×Rd) = µ(B)+ c (cid:88) (γ(1) (B)−γ (B×Rd))
k i+1 i
i=1
k
c (cid:88) (1) (1)
= µ(B)+ (γ (B)−γ (B))
k i+1 i
i=1
c
(1) (1)
= µ(B)+ (γ (B)−γ (B)) = µ(B).
k k+1 1
Similarly
k
γ(Rd×B) = ν(B)+ c (cid:88) (γ(2) (B)−γ (Rd×B))
k i i
i=1
k
c (cid:88) (2) (2)
= ν(B)+ (γ (B)−γ (B)) = ν(B).
k i i
i=1
Next observe that
(cid:90) (cid:90)
∥x−y∥2γ(dx,dy)− ∥x−y∥2γ¯(dx,dy)
k (cid:18)(cid:90)
=
c (cid:88) ∥x−y∥2γ(1) (dx)γ(2)
(dy)
k i+1 i
i=1
Ui+1×Vi
(cid:90) (cid:19)
− ∥x−y∥2γ (dx,dy)
i
Ui×Vi
< 0,
by (1.14). This contradicts optimality of γ¯. ⊔⊓1.5 Kantorovich duality 27
1.4.3 Proof of Brenier’s theorem
We are now in a position to prove Brenier’s theorem.
Let γ¯ be an optimal coupling. In light of Proposition 1.11, supp(γ¯)
is cyclically monotone. By Rockafeller’s Theorem 1.10, this implies that
there exists a convex function φ : Rd → R such that γ¯(Y ∈ ∂φ(X)) =
1. But since φ is convex, it is almost everywhere differentiable with
respect to the Lebesgue measure by Rademacher’s theorem and since µ
has a density then φ is differentiable µ almost everywhere. Therefore,
γ¯(Y = ∇φ(X)) = 1 or in other words, if X ∼ µ, then (X,∇φ(X)) ∼ γ¯.
1.5 Kantorovich duality
Brenier’s theorem shows that an optimal coupling for (W2) if µ has a
2
density is a deterministic coupling given by the gradient of a convex
function φ. This result raises the question of whether it is possible
to solve an optimization problem to find φ directly, or whether it is
possible to certify that a convex function φ corresponds to an optimal
coupling. These questions can be answered by employing tools from
convex duality.
In the fully discrete setting (see Section 1.1.3), (W2) is a linear
2
program or LP (linear objective & linear constraints), which admits a
useful theory of duality. This intuition carries over to the general setting
(and the link can be made precise through approximation arguments,
see [Dud02, Chapter 11]). In fact, it was through optimal transport
that Kantorovich actually introduced LP duality, which has furnished
algorithmic advances continuously since its inception.
1.5.1 The dual Kantorovich problem
The dual problem to (W2) is a maximization problem. To find its
2
expression, encode the constraint γ ∈ Γ as
µ,ν
(cid:26)(cid:90) (cid:90) (cid:90) (cid:27)
(cid:0) (cid:1)
sup f(x)µ(dx)+ g(y)ν(dy)− f(x)+g(y) γ(dx,dy)
f,g∈C
b
(cid:40)
0, if γ ∈ Γ ,
µ,ν
=
∞, otherwise,
wherethesupremumistakenoverthesetC ofcontinuousandbounded
b
functions over Rd. Thus, (W2) is equivalent to
228 1 Optimal transport
(cid:90)
inf ∥x−y∥2γ(dx,dy)
γ∈M
+
(cid:90) (cid:90) (cid:90)
(cid:0) (cid:1)
+ sup f(x)µ(dx)+ g(y)ν(dy)− f(x)+g(y) γ(dx,dy)
f,g∈C
b
where the infimum is taken over the set M of all positive measures
+
on Rd×Rd (unrestricted). Note that for γ ∈/ Γ this new objective is
µ,ν
infinite so the problem is strictly equivalent.
Next, we switch the inf and sup to get the following lower bound on
the value of (W2):
2
(cid:26)(cid:90) (cid:90)
sup f(x)µ(dx)+ g(y)ν(dy)
f,g∈C
b
(cid:26)(cid:90) (cid:27)(cid:27)
+ inf
(cid:0) ∥x−y∥2−f(x)−g(y)(cid:1)
γ(dx,dy)
γ∈M
+
(1.15)
Next observe that since γ is a positive measure, it holds,
(cid:26)(cid:90) (cid:27)
inf
(cid:0) ∥x−y∥2−f(x)−g(y)(cid:1)
γ(dx,dy)
γ∈M
+
(cid:40)
0, if f(x)+g(y) ≤ ∥x−y∥2, ∀x,y ∈ Rd,
=
−∞, otherwise.
Indeed, if there exists a pair (x,y) that violates the above constraint
then we can take the sequence of measures γ = nδ and the integral
n (x,y)
would converge to −∞.
Hence we have shown that (W2) is bounded below by
2
(cid:26)(cid:90) (cid:90) (cid:27)
sup fdµ+ g(y)dν .
f,g∈C
b
f(x)+g(y)≤∥x−y∥2
It turns out that we can relax even further the condition that f,g ∈
C to a mere integrability condition and still get a lower bound on
b
W2(µ,ν).
2
Lemma 1.12.Let µ,ν ∈ P (Rd), then
2
(cid:90)
W2(µ,ν) = inf ∥x−y∥2γ(dx,dy)
2
γ∈Γµ,ν
(cid:26)(cid:90) (cid:90) (cid:27)
≥ sup fdµ+ gdν .
f∈L1(µ),g∈L1(ν)
f(x)+g(y)≤∥x−y∥21.5 Kantorovich duality 29
Proof. Let f ∈ L1(µ),g ∈ L1(ν) be such that f(x)+g(y) ≤ ∥x−y∥2
for µ-a.e. x, ν-a.e. y, and fix γ ∈ Γ . Then
µ,ν
(cid:90) (cid:90) (cid:90)
(cid:0) (cid:1)
f(x)µ(dx)+ g(y)ν(dy) = f(x)+g(y) γ(dx,dy)
(cid:90)
≤ ∥x−y∥2γ(dx,dy).
The proof follows by taking the supremum on the left-hand side and
the infimum on the right-hand side. ⊔⊓
The dual Kantorovich problem is given by
(cid:26)(cid:90) (cid:90) (cid:27)
sup fdµ+ gdν . (D-W2)
2
f∈L1(µ),g∈L1(ν)
f(x)+g(y)≤∥x−y∥2
It is the dual problem to the primal problem (W2).
2
In particular, Lemma 1.12 describes a phenomenon known as weak
duality, in which the dual is only shown to be a lower bound on the
primalproblem.Thisterminologyistobecontrastedwithstrong duality,
where the inequality becomes an equality so that the primal and dual
objectives take the same optimal value. While strong duality is, strictly
speaking, only a statement about objective values, it is often the case
that the solutions to the primal and dual problems are related to each
other; see [BV04, Chapter 5] for a treatment of duality in the context
of convex optimization.
We show in Subsection 1.5.3 that strong duality in fact holds and it
leads to important consequences for our problem of interest.
1.5.2 The semidual
Before moving to strong duality, we make a quick detour to define the
semidual problem,apartiallysolvedversionofthedualproblem(D-W2).
2
It plays an important role in the estimation of optimal transport maps
(Chapter 3).
Consider (D-W2) and suppose that we hold the first dual potential f
2
fixed; given this choice of f, what is the optimal choice of g? Since the
dual problem is a supremum, we want to make g as large as possible,
but we must respect the constraint f(x)+g(y) ≤ ∥x−y∥2. The optimal
function g is therefore given by30 1 Optimal transport
g(y) = inf {∥x−y∥2−f(x)}. (1.16)
x∈Rd
The function defined in (1.16) is called the c-conjugate or c-transform
of f, denoted fc, associated with the cost c(x,y) = ∥x − y∥2. This
reasoning shows that we can reformulate the dual as
(cid:26)(cid:90) (cid:90) (cid:27)
(D-W2) = sup fdµ+ fcdν . (1.17)
2
f∈L1(µ)
This is a version of the semidual problem, and it is applicable to optimal
transport for any cost function c provided that we replace ∥x−y∥2 with
c(x,y) in (1.16).
However, for the quadratic cost, we can go one step further and
explicitly link the semidual with convex analysis. In this case, the
semidual is given by
(cid:26)(cid:90) (cid:90) (cid:27)
sup ϕdµ+ ϕ∗dν (SD)
ϕ∈L1(µ)
where ϕ∗ denotes the convex conjugate of ϕ; see Appendix A.
Proposition 1.13.Let µ,ν ∈ P (Rd) be probability measures. Then,
2
the dual problem (D-W2) is equivalent to the semidual problem (SD) in
2
the following sense:
1.Objective values: Write S and D for the optimal objective values
of (SD) and (D-W2) respectively. Then
2
(cid:90) (cid:90)
D = ∥·∥2dµ+ ∥·∥2dν −2·S.
2.Solutions: A pair of functions (f,g) is optimal for (D-W2) if and
2
only if f = ∥ · ∥2 − 2φ and g = ∥ · ∥2 − 2φ∗ where φ is optimal
for (SD).
Proof. Let us write f = ∥·∥2−2ϕ and g = ∥·∥2−2ψ; this is simply a
reparametrization of the dual potentials. Then,
(cid:90) (cid:90) (cid:90) (cid:90) (cid:16)(cid:90) (cid:90) (cid:17)
fdµ+ gdν = ∥·∥2dµ+ ∥·∥2dν −2 ϕdµ+ ψdν .
The constraint f(x)+g(y) ≤ ∥x−y∥2 translates into
∥x∥2−2ϕ(x)+∥y∥2−2ψ(y) ≤ ∥x−y∥2 ⇔ ϕ(x)+ψ(y) ≥ ⟨x,y⟩.1.5 Kantorovich duality 31
Hence, (D-W2) is equivalent to
2
(cid:26)(cid:90) (cid:90) (cid:27)
inf ϕdµ+ ψdν .
ϕ∈L1(µ),ψ∈L1(ν)
ϕ(x)+ψ(y)≥⟨x,y⟩
Next, let us apply the same trick as described above: for fixed ϕ, the
optimal choice of ψ obeying the constraint is given by
ψ(y) = sup{⟨x,y⟩−ϕ(x)},
x∈Rd
which is precisely the definition of the convex conjugate ϕ∗. Substituting
this in yields the equivalence. ⊔⊓
In the preceding proof, we showed that for fixed ϕ , the optimal
0
choice of ψ is ψ = ϕ∗. Due to the symmetry of the problem, for fixed
0
ψ = ϕ∗, the optimal choice of ϕ is then ψ∗ = ϕ∗∗. One could imagine
0 0
iterating this process, obtaining better and better dual potentials, but
actually the process halts here. Since ϕ∗ is a closed convex function, it is
0
self-dual, so that ϕ∗∗∗ = ϕ∗; see Appendix A. In the end, this argument
0 0
shows that the optimal potential φ in (SD) can be taken to be a closed
convex function.
Thus far, we have seen two convex functions φ arise from the optimal
transport problem. From the primal standpoint, Brenier’s Theorem 1.8
showsthattheoptimaltransportplanissupportedonthesubdifferential
of a convex function. From the dual standpoint, a minimizer of (SD)
can be taken to be convex. In the next section, we show that these two
convex functions are one and the same.
1.5.3 The fundamental theorem of optimal transport
Recall from Brenier’s theorem that if a measure µ has a density then
any optimal coupling between µ and ν is supported on the graph
of the gradient of a convex function. It turns out that the converse
holds: any coupling γ ∈ Γ supported on the graph of the gradient
µ,ν
of a convex function has to be optimal. This equivalence follows from
the fundamental theorem of optimal transport stated below. In fact,
this theorem contains another fundamental result about strong duality
between the primal problem (W2) and its dual (D-W2) which is the key
2 2
to establishing this equivalence.32 1 Optimal transport
Theorem 1.14(Fundamental theorem of optimal transport).
Let µ,ν ∈ P (Rd) be two probability measures such that µ has a density
2
and let X ∼ µ. Then the following are equivalent:
(i)γ¯ ∈ Γ is an optimal coupling in the sense that:
µ,ν
(cid:90)
∥x−y∥2γ¯(dx,dy) = W2(µ,ν).
2
(ii)There exists a proper convex function φ such that (X,∇φ(X)) ∼
γ¯ ∈ Γ .
µ,ν
(iii)Strong duality holds between (W2) and (D-W2):
2 2
(cid:90) (cid:26)(cid:90) (cid:90) (cid:27)
∥x−y∥2γ¯(dx,dy) = sup fdµ+ gdν .
f∈L1(µ),g∈L1(ν)
f(x)+g(y)≤∥x−y∥2
Moreover, the above supremum is achieved for
f¯(x) := ∥x∥2−2φ(x) and g¯(y) := ∥y∥2−2φ∗(y).
Proof. We have already proved that (i) ⇒ (ii) in Subsection 1.4 so it
remains to prove that (ii) ⇒ (iii) and (iii) ⇒ (i).
We first prove that (ii) ⇒ (iii). To that end, observe that for µ
almost every x
∥x−∇φ(x)∥2 = ∥x∥2+∥∇φ(x)∥2−2⟨x,∇φ(x)⟩.
Moreover, the convex conjugate φ∗ of φ satisfies for µ almost every x,
φ(x)+φ∗(∇φ(x)) = ⟨∇φ(x),x⟩.
This is the optimality condition for the Fenchel–Young inequality (The-
orem A.6). The above two displays yield
∥x−∇φ(x)∥2 = ∥x∥2−2φ(x)+∥∇φ(x)∥2−2φ∗(∇φ(x)) .
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
f¯(x) g¯(∇φ(x))
Integrating with respect to µ yields
(cid:90) (cid:90)
∥x−y∥2γ¯(dx,dy) = ∥x−∇φ(x)∥2µ(dx)
(cid:90) (cid:90)
= f¯(x)µ(dx)+ g¯(∇φ(x))µ(dx)1.5 Kantorovich duality 33
(cid:90) (cid:90)
= f¯(x)µ(dx)+ g¯(y)ν(dy). (1.18)
We now check that the pair (f¯,g¯) satisfies the constraints of (D-W2). It
2
follows from the Fenchel–Young inequality (Theorem A.6) that for any
x,y ∈ Rd,
f¯(x)+g¯(y) = ∥x∥2+∥y∥2−2(cid:0) φ(x)+φ∗(y)(cid:1)
≤ ∥x∥2+∥y∥2−2⟨x,y⟩ = ∥x−y∥2. (1.19)
To check integrability, note that from the definition of convex conju-
gation, φ = φ∗∗ and φ∗ are both lower bounded by affine functions.
Therefore, since µ,ν ∈ P (Rd) we have that f ∈ L1(µ) and g ∈ L1(ν).
2 + +
(cid:82) (cid:82) (cid:82)
Moreover, (1.18) yields fdµ+ gdν ≥ 0 so that fdµ > −∞ and
(cid:82)
gdν > −∞. It yields
(cid:90) (cid:90) (cid:90)
|f|dµ = 2 f dµ− fdµ < ∞
+
so that f ∈ L1(µ) and similarly g ∈ L1(ν). This completes the proof of
(ii) ⇒ (iii).
We now turn to the proof of (iii) ⇒ (i). We have by (1.18) that for
any γ ∈ Γ
µ,ν
(cid:90) (cid:90) (cid:90)
∥x−y∥2γ¯(dx,dy) = f¯dµ+ g¯dν
(cid:90)
= (cid:0) f¯(x)+g¯(y))γ(dx,dy)
(cid:90)
≤ ∥x−y∥2γ(dx,dy),
where in the last inequality, we used (1.19). Therefore, γ¯ is an optimal
coupling and (i) follows. ⊔⊓
More general versions of this theorem do not require that µ have
a density. In particular, it can be shown using these tools that the
converse of Proposition 1.11 holds: for any two measures µ,ν ∈ P (Rd),
2
if γ ∈ Γ has a cyclically monotone support, then it must be an
µ,ν
optimal coupling (see [AG13] for example).
The optimal f and g arising in Theorem 1.14 play a central role in
the sequel.34 1 Optimal transport
Definition 1.15(Kantorovich potentials). The functions
f¯(x) = ∥x∥2−2φ(x) and g¯(y) = ∥y∥2−2φ∗(y)
that realize the optimum of the dual Kantorovich problem (D-W2) are
2
called Kantorovich potentials for the pair (µ,ν).
Even though a priori solutions to (D-W2) are only defined almost
2
everywhere, f¯and g¯ are bona fide functions defined everywhere on Rd.
Note that by symmetry of (D-W2) and the form of the Kantorovich
2
potentials, it is easy to check that if ν admits a density, then ∇φ∗ is an
optimal transport map from ν to µ.
1.5.4 An improved Brenier theorem
With the fundamental theorem, we can state an improved version of
Brenier’s theorem, which is often useful.
Theorem 1.16(Improved Brenier). Let µ,ν ∈ P (Rd) be two prob-
2
ability measures such that µ has a density and let X ∼ µ. Then there
exists a µ-almost everywhere differentiable convex function φ : Rd → R
such that (X,∇φ(X)) ∼ γ¯ ∈ Γ and γ¯ is an optimal coupling for (W2):
µ,ν 2
(cid:90) (cid:90)
∥x−y∥2γ¯(dx,dy) = min ∥x−y∥2γ(dx,dy) = W2(µ,ν).
2
γ∈Γµ,ν
Moreover, ∇φ is unique in the sense that if there exists a convex function
ψ such that ∇ψ(X) ∼ ν, then ∇ψ(X) = ∇φ(X), almost surely.
In particular, any valid coupling γ ∈ Γ of the form (X,∇ψ(X)) ∼
µ,ν
γ for some convex function ψ, must be the unique optimal coupling
between µ and ν.
Proof. We have already proved the existence of φ in the previous sub-
section and we need to prove uniqueness of ∇φ.
It turns out that as soon as every optimal coupling is induced by a
transport map, this transport map (and hence the optimal coupling)
must be unique.
To see this, let γ and γ be two optimal couplings induced by the
1 2
transport maps T and T respectively:
1 2
γ (Y = T (X)) = 1, and γ (Y = T (X)) = 1.
1 1 2 21.5 Kantorovich duality 35
Then consider the coupling γ¯ = (γ + γ )/2 ∈ Γ . Then γ¯ is also
1 2 µ,ν
optimal since
(cid:90)
∥x−y∥2γ¯(dx,dy)
(cid:90) (cid:90)
1 1
= ∥x−y∥2γ (dx,dy)+ ∥x−y∥2γ (dx,dy) = W2(µ,ν).
2 1 2 2 2
In particular, it follows from Brenier’s theorem that γ¯ is also induced
by a transport map T (which happens to be the gradient of a convex
function but we do not need this fact here). Therefore, if (X,Y) ∼ γ¯,
it must be the case that the conditional distribution of Y given X is
the Dirac δ . But by construction this conditional distribution is the
T(X)
mixture of two Diracs (δ +δ )/2 and the two may only be the
T1(X) T2(X)
same when T (X) = T (X) = T(X), almost surely.
1 2
Therefore, if there exists a convex function ψ such that ∇ψ(X) ∼ ν,
then by Theorem 1.14, it must be that γ such that (X,∇ψ(X)) ∼ γ ∈
Γ isanoptimalcouplingandthereforethat∇ψ(X) = ∇φ(X),almost
µ,ν
everywhere in light of the above discussion.
The last statement of the theorem follows by observing that the
equivalence (ii) ⇔ (i) in Theorem 1.14 implies that optimal couplings
are supported on the graph of the gradient of a convex function, which
has to be unique from the above argument. ⊔⊓
The improved Brenier theorem is very useful since it characterizes
optimality of a transport map: if a transport map is the gradient of a
convex function, then it is optimal and is unique! We call this map the
Brenier map. We can use Theorem 1.16 to characterize optimal trans-
port maps in two fundamental instantiations of the optimal transport
problem: the one-dimensional case and the Gaussian case.
Example 1.17(One-dimensional optimal transport). Recall that the cu-
mulative distribution function (CDF) F of a random variable X is given
by the map t (cid:55)→ P(X ≤ t).
Proposition 1.18.Let µ,ν ∈ P (R) be two univariate distributions
2
with CDFs, F and F respectively and such that µ admits a density.
µ ν
Then
(cid:90) 1
W2(µ,ν) = |F†(u)−F†(u)|2du
2 µ ν
0
and the optimal coupling between µ and ν is induced by the Brenier map
F†◦F .
ν µ36 1 Optimal transport
Proof.Since µ has a density, F ◦F† = id. Let U ∼ Unif([0,1]) be a
µ µ
uniform random variable and define X = F†(U),Y = F†(U) so that
µ ν
X ∼ µ and Y ∼ ν. Next observe that Y = F†◦F (X) and that F†◦F
ν µ ν µ
is an increasing function. In light of Theorem 1.16, this defines the
unique optimal coupling between µ and ν. ⊔⊓
The geometric consequence of this identity, as explored further
in Chapter 7, is that the metric space P (R) equipped with the 2-
2
Wasserstein distance is flat. Indeed the map µ (cid:55)→ F† is an isometric
µ
embedding of (P (R),W ) into the (flat) Hilbert space L2(R).
2 2
Example 1.19(Gaussian optimal transport). We can also used the im-
prove Brenier theorem to derive the optimal transport map between
two Gaussian measures. Let m ,m ∈ Rd and let Σ , Σ be positive
1 2 1 2
definite d×d matrices. Let µ = N(m ,Σ ) and µ = N(m ,Σ ).
1 1 1 2 2 2
Recall that affine maps preserve Gaussianity. Namely, if X ∼ µ
1 1
and T(x) = Ax+b, where A ∈ Rd×d and b ∈ Rd, then T(X ) is also
1
Gaussian. To calculate the distribution of T(X ), it suffices to compute
1
the mean and covariance, and we find that
ET(X ) = Am +b, covT(X ) = AΣ AT.
1 1 1 1
It is therefore a reasonable guess that the optimal transport map from
µ to µ is affine, and this can be verified using Brenier’s Theorem 1.16.
1 2
For this, we require that Am +b = m and AΣ AT = Σ , representing
1 2 1 2
the constraint that T µ = µ . We also require T to be the gradient of
# 1 2
a convex function. If we set
1
φ(x) = ⟨x,Ax⟩+⟨b,x⟩,
2
then ∇φ(x) = 1 (A+AT)x+b, and φ is convex provided A+AT ⪰ 0.
2
We conclude that T = ∇φ is the gradient of a convex function (and
therefore optimal) provided that A is symmetric and positive definite.
How do we choose A and b so that the pushforward constraints and
thePSDconstraintonAaresimultaneouslymet?Themostna¨ıvechoice
−1/2 1/2
for A, namely A = Σ Σ , works when Σ and Σ commute, but in
1 2 1 2
general this choice of A is not even symmetric. It takes a little ingenuity
to find a PSD choice for A that works, but it can be done as follows.
Starting with the idea that A is PSD and satisfies AΣ A = Σ , then by
1 2
1/2 1/2
squaring Σ AΣ we find that
1 11.6 Duality for p=1 37
1/2 1/2 2 1/2 1/2 1/2 1/2
(Σ AΣ ) = Σ AΣ AΣ = Σ Σ Σ .
1 1 1 1 1 1 2 1
Taking square roots and solving, we obtain
A =
Σ−1/2 (Σ1/2
Σ
Σ1/2 )1/2Σ−1/2
.
1 1 2 1 1
It is seen that this is the matrix A that we are looking for. By using
the other constraint Am +b = m , we find that
1 2
T(x) =
Σ−1/2 (Σ1/2
Σ
Σ1/2 )1/2Σ−1/2
(x−m )+m .
1 1 2 1 1 1 2
By Theorem 1.16, this is the unique optimal transport map from µ to
1
µ . Moreover, by substituting this into the definition of the Wasserstein
2
distance, we find that (exercise!)
W2(µ ,µ ) = ∥m −m ∥2+tr(cid:2) Σ +Σ −2(Σ1/2 Σ Σ1/2 )1/2]. (1.20)
2 1 2 1 2 1 2 1 2 1
1.6 Duality for p = 1
In the case of the 1-Wasserstein metric, the dual takes a remarkably
simple form. Most textbooks derive this result as a specific instanti-
ation of strong duality for optimal transport with a general cost c,
which requires tools to generalize Theorem 1.14 beyond the case of the
quadratic cost c(x,y) = ∥x−y∥2. The tools include a generalized notion
of cyclical monotonicity and of Legendre transform, and the reader is
invited to become familiar with them (see the discussion section for a
brief overview). When specialized to the p-Wasserstein distance, they
yield the following result; see, e.g., [San15, Section 3.1.1] for a proof.
Theorem 1.20.Fix p ≥ 1 and let µ,ν ∈ P (Rd) be two probability
p
measures. Then the following holds:
(cid:26)(cid:90) (cid:90) (cid:27)
Wp(µ,ν) = sup fdµ+ gdν . (1.21)
p
f∈L1(µ), g∈L1(ν)
f(x)+g(y)≤∥x−y∥p
In the case, where p = 1, this result can be simplified to eliminate
one of the dual functions.
Theorem 1.21.Let µ,ν ∈ P (Rd) be two probability measures. Then
1
the following holds:
(cid:26)(cid:90) (cid:90) (cid:27)
W (µ,ν) = sup fdµ− fdν , (1.22)
1
f∈Lip
1
where Lip is the set of 1-Lipschitz functions.
138 1 Optimal transport
Before proceeding to the proof of this theorem, let us see where
Lipschitz functions come from. Recall that the semidual in Section 1.5.2
was also removing one of the dual functions. In particular, when p = 1,
we can replace the function g in (1.21) with the c-transform
fc(y) = inf {∥x−y∥−f(x)}.
x∈Rd
The following lemma holds.
Lemma 1.22.For c(x,y) = ∥x−y∥, a function g : Rd → R is a c-
transform g = fc if and only it is 1-Lipschitz. Moreover, any 1-Lipschitz
function satisfies gc = −g.
Proof. Write
g(y) = fc(y) = inf {∥x−y∥−f(x)}.
x∈Rd
For any x, the function y (cid:55)→ ∥x−y∥−f(x) is clearly 1-Lipschitz by
the reverse triangle inequality. Since the set of 1-Lipschitz functions is
closed under taking infima, the function g is also 1-Lipschitz.
To prove the converse, let g be a 1-Lipschitz function so that for any
x,y ∈ Rd, it holds
g(y) ≤ ∥x−y∥+g(x).
Taking the infimum over x yields g ≤ (−g)c. Moreover,
(−g)c(y) = inf {∥x−y∥+g(x)} ≤ g(y),
x∈Rd
where we took y = x in the last inequality.
We have shown that (−g)c = g and in particular that g has to be a
c-transform (of −g). The second statement of the lemma follows from
the fact that if g is 1-Lipschitz, then so is −g. ⊔⊓
We are now in a position to prove Theorem 1.21.
Proof of Theorem 1.21. By the argument in Subsection 1.5.2, (1.21) is
equal to the following semidual
(cid:26)(cid:90) (cid:90) (cid:27)
sup fdµ+ fcdν .
f∈L1(µ)
We can continue optimizing the potentials further to obtain1.7 Discussion 39
(cid:26)(cid:90) (cid:90) (cid:27)
sup fccdµ+ (fcc)cdν .
f∈L1(µ)
It follows from Lemma 1.22 that
{fcc : f ∈ L1(µ)}
onlycontains1-Lipschitzfunctions.Moreover,sinceµ ∈ P (Rd),itholds
1
that any 1-Lipschitz function is integrable against µ:
(cid:90) (cid:90) (cid:90)
|f|dµ ≤ |f(x)−f(y)|µ(dx)+|f(y)| ≤ ∥x−y∥µ(dx)+|f(y)|
< ∞,
so that f ∈ L1(µ). Hence, we have shown that
(cid:26)(cid:90) (cid:90) (cid:27) (cid:26)(cid:90) (cid:90) (cid:27)
W (µ,ν) = sup fdµ+ fcdν = sup fdµ− fdν ,
1
f∈Lip f∈Lip
1 1
(1.23)
which concludes the proof of Theorem 1.21. ⊔⊓
1.7 Discussion
§1.1. For a historical bibliography, consult [Vil03].
§1.2. Proposition 1.5 is taken from the paper [RNW19], which also
provides a converse. Further comparisons between “information diver-
gences” (e.g., total variation, Kullback–Leibler, chi-squared divergence)
and optimal transport distances are implied by so-called transport in-
equalities, which also connects to a larger literature on concentration of
measure; see, e.g., [BV05] and [Vil09b, Chapter 22].
§1.3. As shown in Exercise 8, the monotone coupling in Theorem 1.7
and Proposition 1.18 is also optimal for any cost function which is a
strictly convex function of x−y; see [San15, Section 2.2].
§1.4. Brenier’s theorem was first used to define multivariate quantiles
for ν ∈ P (Rd) by [CGHH17], and this definition was extended to
2
ν which may not have a second moment by [Hal17]. In addition to
its mathematical elegance, this definition of multivarite quantiles has
important implications for nonparametric testing [GS22, DS23].
Rockafellar’s theorem is from [Roc66]. The proof of Proposition 1.11
is from [GM96].40 1 Optimal transport
§1.5. Although we deduced strong duality by explicitly exhibiting dual
potentials for which the duality gap is zero (namely, the ones obtained
from the characterization of optimal couplings as having cyclically
monotone support), it is also possible to prove strong duality directly
via appeal to an abstract min-max principle; see [Vil03, Section 1.1].
ManyoftheargumentsinSection1.5generalizetogeneralcontinuous
costs c : X×Y → R: the c-conjugate of a function f : X → R is given
by fc(y) := inf x∈X{c(x,y) − f(x)}, and likewise the c-conjugate of
g : Y → R is given by gc(x) := inf y∈Y{c(x,y)−g(y)}. We say that f
is c-concave if f = gc for some function g. The equality (1.17) still
holds and equals the optimal transport value (strong duality). Any
optimal transport plan is supported on a c-cyclically monotone set,
which is defined as in (1.12) but replacing the quadratic cost with c.
Then, c-cyclically monotone sets are characterized as c-subdifferentials,
where the c-subdifferential of a c-concave function f = gc is the set of
(x,y) pairs such that f(x)+g(y) = c(x,y). What does not generalize as
easily, however, is Brenier’s theorem, which requires further conditions
to ensure the single-valuedness of the c-subdifferential. For example if
c(x,y) = h(x−y) for some strictly convex function h, then a unique
optimal transport map exists but it need not be the gradient of a convex
function; see [San15, Theorem 1.17].
§1.6. The duality formula for W is classical and is closely related to
1
the bounded Lipschitz metric [Dud02], which can be extended to define
a norm over signed measures. As discussed in Subsection 2.8.1, this
formula expresses W as an integral probability metric.
1
1.8 Exercises
1. Let A ⊆ R×R be monotone:
∀(x,y),(x′,y′) ∈ A, x < x′ ⇒ y < y′.
For simplicity, assume that A is contained in the graph of a function.
Show that A is cyclically monotone.
2. Let X ∈ R be a random variable that admits a density supported on
the whole real line. Let f,g be two monotone increasing functions
such that f(X) has the same distribution as g(X). Then, f = g
almost everywhere.
3. Let m ,m ∈ Rd and let Σ , Σ be positive definite d×d matrices.
1 2 1 2
Let µ = N(m ,Σ ) and µ = N(m ,Σ ).
1 1 1 2 2 21.8 Exercises 41
a) Verify the equation (1.20).
b) Using a suboptimal coupling, prove the simple upper bound
W2(µ ,µ ) ≤ ∥m −m ∥2+∥Σ1/2 −Σ1/2 ∥2
2 1 2 1 2 1 2 HS
where ∥M −M ∥2 := tr((M −M )T(M −M )). Show that
1 2 HS 1 2 1 2
this is an equality when Σ and Σ commute.
1 2
c) Prove that if ν , ν are probability measures with means m , m
1 2 1 2
and covariance matrices Σ , Σ respectively, then
1 2
W (ν ,ν ) ≥ W (µ ,µ ).
2 1 2 2 1 2
Hint: What are the optimal dual potentials for the optimal
transport problem from µ to µ ?
1 2
4. a) Let X and Y be random vectors in Rd. Prove that
W2(cid:0)
law(X),
law(Y)(cid:1)
2
= ∥EX −EY∥2+W2(cid:0) law(X −EX), law(Y −EY)(cid:1) .
2
Thankstothisequality,oftenwhenweworkwiththeW distance,
2
it suffices to consider centered random variables.
b) Let X,Y ,...,Y be random vectors in Rd and λ ,...,λ ≥ 0.
1 k 1 k
Suppose that X and Y are optimally coupled for each i =
i
1,...,k. Show that X and
(cid:80)k
λ Y are optimally coupled.
i=1 i i
5. Letν admitadensityw.r.t.Lebesguemeasure.ShowthatW2(·,ν)is
2
strictly convex; that is, if µ ,µ ∈ P (Rd) are distinct and t ∈ (0,1),
0 1 2
then
W2(cid:0) (1−t)µ +tµ , ν(cid:1) < (1−t)W2(µ ,ν)+tW2(µ ,ν). (1.24)
2 0 1 2 0 2 1
Hint: Start by proving that (1.24) holds with ≤ instead of <. Next,
supposing that (1.24) fails, show that there is an optimal transport
plan between (1 − t)µ + tµ and ν which is not induced by a
0 1
transport map, contradicting Brenier’s theorem.
6. Consider the measures µ = 1 N(−m,1) + 1 N(+m,1) and ν =
2 2
1 N(−m,1)+ 3 N(+m,1), where m > 0. Prove that W (µ,ν) ≍ m.
4 4 2
Hint: The point of this question is that computing the optimal
transport map from µ to ν is painful, but it is not hard to obtain
good lower and upper bounds. For the lower bound, prove and use
the fact that for any 1-Lipschitz function f : R → R, it holds that
E f −E f ≤ W (µ,ν) ≤ W (µ,ν). For the upper bound, exhibit a
µ ν 1 2
coupling of µ and ν.42 1 Optimal transport
7. Recall that the chi-squared divergence between two µ and ν is
defined as
(cid:90) (cid:16)dµ (cid:17)2
χ2(µ∥ν) = −1 dν.
dν
Show that
(cid:112)
W (µ,ν) ≤ χ2(µ∥ν),
1
when ν has unit variance.
8. Let c : R → R be strictly convex and consider optimal transport
over R with the cost function (x,y) (cid:55)→ c(x−y). For example, when
c(z) = |z|p for p > 1, we obtain Wp. In this exercise, we show that
p
the coupling γ¯ given in Proposition 1.18 is universally optimal for
all costs of this form. See the discussion section for background.
a) Showthatforanya,b ∈ R,γ¯((−∞,a]×(−∞,b]) = F (a)∧F (b).
µ ν
b) Show that γ¯ is the unique coupling of µ and ν such that
∀(x,y),(x′,y′) ∈ suppγ¯, x < x′ ⇒ y ≤ y′. (1.25)
Hint: Consider the sets A = (−∞,a]×(b,∞) and B = (a,∞)×
(−∞,b]. Show that any coupling satisfying (1.25) must assign
one of these two sets measure zero. Use this to show that any
coupling γ which satisfies (1.25) must agree with γ¯.
c) Let (x,y),(x′,y′) ∈ suppγ, where γ is an optimal transport plan
between µ and ν with cost defined by c as above. By c-cyclical
monotonicity of suppγ,
c(x−y)+c(x′−y′) ≤ c(x−y′)+c(x′−y).
Show that if x < x′, then y ≤ y′, hence γ = γ¯.
Hint: Argue by contradiction. If the claim fails, then both u :=
x−y′ and v := x′−y lie between w := x−y and w′ := x′−y′.
Write u and v as convex combinations of w and w′, and apply
strict convexity of c.
d) Deduce that the optimal cost equals Ec(F†(U)−F†(U)) where
µ ν
U ∼ Unif([0,1]).
9. Given two probability measures µ, ν over the same space S, define
the total variation distance between µ and ν to be
d (µ,ν) = sup|µ(A)−ν(A)|,
TV
A⊆S
where the supremum ranges over all measurable subsets. Show that
thetotalvariationdistanceisalsoequaltoallofthefollowing.(Hint:
Consider the coupling in Theorem 1.6.)1.8 Exercises 43
a) If f and g denote the respective densities of µ and ν with
respect to some common dominating measure λ for µ and ν (e.g.,
λ = µ+ν), then
(cid:90) (cid:90)
1
d (µ,ν) = |f −g|dλ = 1− f ∧gdλ = µ(f ≥ g).
TV
2
b) d (µ,ν) = infP(X ̸= Y) where the infimum ranges over all
TV
couplings (X,Y) of µ and ν.
(cid:82)
c) d (µ,ν) = sup{ hd(µ−ν) | h : S → [0,1]}. Compare with W
TV 1
duality from Theorem 1.21.
10. Let µ,ν ∈ P (Rd) admit densities with respect to Lebesgue measure.
2
Show that if ∇φ is the optimal transport map from µ to ν, then
∇φ∗ is the optimal transport map from ν to µ. (See Appendix A.)
Apply this fact tothe optimal transport map between two Gaussians
and discover a non-trivial matrix identity.2
Estimation of Wasserstein distances
In applications of optimal transport in statistics, it is paramount to be
abletoobtaingoodupperandlowerboundsontheWassersteindistance
between probability measures. This chapter describes tools to bound
the Wasserstein distance. To do so, we heavily employ the primal and
dual formulations of optimal transport. As a primary application, we
consider a quantitative form of the Wasserstein law of large numbers,
which is the statement that if µ is an empirical measure consisting of
n
n i.i.d. samples from a probability measure µ, then EW (µ ,µ) → 0 as
p n
n → ∞.
2.1 The Wasserstein law of large numbers
i.i.d.
Suppose that X ,...,X ∼ µ, where µ is a probability measure on a
1 n
compact subset of Rd, which we assume for convenience is equal to the
unit cube [0,1]d. The empirical measure is defined to be the (random)
measure
n
1 (cid:88)
µ = δ .
n
n
Xi
i=1
The law of large numbers implies that µ (cid:44)→ µ and also (cid:82) ∥·∥pdµ →
n n
(cid:82) ∥·∥pdµ almost surely; therefore, the discussion in Chapter 1 implies
that W (µ ,µ) → 0. Moreover, since W (µ ,µ) is bounded almost
p n p n
surely, we also have convergence in mean:
EW (µ ,µ) → 0.
p n46 2 Estimation of Wasserstein distances
How fast does this convergence occur? In the context of the classic law
of large numbers for bounded random vectors X ,...,X in Rd, we of
1 n
course have
E(cid:13) (cid:13) (cid:13)1 (cid:88)n X i−EX(cid:13) (cid:13) (cid:13)2 ≲ 1 .
(cid:13)n (cid:13) n
i=1
Note that the rate of decay n−1 holds irrespective of the dimension, and
is true even in infinite-dimensional Hilbert spaces.
By contrast, the Wasserstein law of large numbers behaves quite
differently. In this chapter, we prove the following proposition.
Proposition 2.1.If the support of µ lies in [0,1]d, then

n−1/2 if d = 1,
√  
EW (µ ,µ) ≲ d· (logn/n)1/2 if d = 2, (2.1)
1 n

n−1/d if d ≥ 3,
and this rate is unimprovable in general.
In contrast to the standard law of large numbers, the convergence of
µ to µ in Wasserstein distance degrades exponentially as the dimension
n
grows, a phenomenon often known as the curse of dimensionality.
2.2 The dyadic partitioning argument
The fact that the Wasserstein distance is defined by a minimization over
couplings suggests a natural strategy for proving bounds: we can show
an upper bound on W by exhibiting a coupling with a small cost. In
1
this section, we build such a coupling, which, perhaps surprisingly, gives
rise to good bounds in many situations. The main idea is to attempt
to couple µ and ν by recursively constructing candidate couplings at
multiple scales.
Before stating the bound, let us describe the basic strategy. For
simplicity, let us consider proving an upper bound on W (µ,ν) for µ
1
and ν whose support lies in [0,1]d. We first make a trivial observation:
√
W (µ,ν) ≤ d. (2.2)
1
√
Indeed, the diameter of [0,1]d is d, so no coupling between µ and ν
can move mass a greater distance than this.2.2 The dyadic partitioning argument 47
Let us now imagine a slight sharpening of this bound. Let Q be the
collection of cubes of side length 1/2 whose corners lie at points of
the form 2−1(k ,...,k ) for k ,...,k ∈ {0,1,2}. These cubes form a
1 d 1 d
partition of [0,1]d into 2d pieces.1 Suppose for the sake of argument
that µ(Q) = ν(Q) for all Q ∈ Q, j = 1,...,2d, so that µ and ν assign
the same mass to each of the small cubes. Then, it would be possible to
couple µ and ν by only moving mass within each small cube. Since the
√
diameter of each small cube is d/2, any such coupling improves on
the bound in (2.2) by a factor of 2.
Even when µ and ν do not assign the same mass to each small cube,
we can use the above idea to construct a coupling between µ and ν in
two steps: first, we can match as much mass as possible between µ and
ν within each cube. This creates a partial coupling between a portion of
µ’smassandaportionofν’s.Sinceweonlymovemasswithineachsmall
√
cube, the total cost of this partial coupling is at most d/2. We then
need to extend this partial coupling to a full coupling, by transporting
µ’s extra mass on any cube Q for which µ(Q) > ν(Q) to ν’s extra mass
on some cube Q′ for which ν(Q′) > µ(Q′). The amount of extra mass
matched in this step is (cid:80) (µ(Q)−ν(Q)) = 1 (cid:80) |µ(Q)−ν(Q)|,
Q √∈Q + 2 Q∈Q
at a total cost of at most d (cid:80) |µ(Q)−ν(Q)|.
2 Q∈Q
Combining these bounds yields the refined estimate
√ √
d (cid:88) d
W (µ,ν) ≤ |µ(Q)−ν(Q)|+ . (2.3)
1
2 2
Q∈Q
This bound improves on (2.2) when µ and ν assign similar mass to each
cube.
The proof of the following bound is based on recursing the above
argument J times. At the j-th stage, we bound the discrepancy between
µ and ν on 2dj cubes of side length 2−j. To state this bound, let us
define the set Q , j ≥ 0, to consist of a set of 2dj cubes of side length
j
2−j which form a partition of [0,1]d.2
Theorem 2.2(Dyadic partitioning bound). Let µ,ν ∈ P([0,1]d).
For any J ≥ 0,
1 These cubes overlap at their boundaries, but we can easily modify these sets by
removing overlaps to obtain a bona fide partition.
2 As above, we assume that the elements of Q been modified at their boundary so
j
that Q is a partition and so that Q is a refinement of Q for all j ≥0.
j j+1 j48 2 Estimation of Wasserstein distances
√ J−1(cid:18) (cid:19) √
(cid:88) (cid:88)
W (µ,ν) ≤ d 2−j |µ(Q)−ν(Q)| + d2−J .
1
j=0 Q∈Q
j+1
Proof. We define a sequence of positive measures µ ,...,µ and
0 J
ν ,...,ν , which satisfy
(cid:80)J
µ = µ and
(cid:80)J
ν = ν and such that
0 J j=0 j j=0 j
µ (Q) = ν (Q) ∀Q ∈ Q , j = 0,...,J.
j j j
We write for simplicity Ω := [0,1]d. We first claim that
√ J
(cid:88)
W (µ,ν) ≤ d 2−jµ (Ω). (2.4)
1 j
j=0
This bound is nothing but an instantiation of the strategy described
above: since µ and ν assign the same mass to each element of Q , there
j j j
exists a coupling γ between µ and ν which only moves mass within
j j j
each element of Q ; for instance, we can take the piecewise independent
j
coupling
(cid:88) (µ j)| Q⊗(ν j)| Q
γ = .
j
µ (Q)
j
Q∈Q j:µj(Q)>0
The fact that γ ∈ Γ implies γ =
(cid:80)J
γ ∈ Γ , and
j µj,νj j=0 j µ,ν
(cid:90)
W (µ,ν) ≤ ∥x−y∥γ(dx,dy)
1
J (cid:90)
(cid:88)
= ∥x−y∥γ (dx,dy)
j
j=0
√ J
(cid:88)
≤ d 2−jµ (Ω),
j
j=0
where the last inequality follows from the fact if (x,y) ∈ supp(γ ), then
j
x and y lie in the same element Q ∈ Q , so that ∥x−y∥ ≤ diam(Q) =
√ j
d2−j.
We now exhibit the measures µ and ν which give rise to the final
j j
bound. Define the restriction of µ on each Q ∈ Q by setting
J J
µ(Q)∧ν(Q)
(µ )| = µ| ,
J Q Q
µ(Q)2.2 The dyadic partitioning argument 49
where by convention we let µ be zero on Q if µ(Q) = 0. Similarly, set
J
µ(Q)∧ν(Q)
(ν )| = ν| .
J Q Q
ν(Q)
For 1 ≤ j < J, let
(cid:88)
µ′ = µ− µ ,
j k
j<k≤J
(cid:88)
ν′ = ν − ν ,
j k
j<k≤J
and then, for each Q ∈ Q , define
j
µ′(Q)∧ν′(Q)
(µ )| = j j (µ′)| ,
j Q µ′(Q) j Q
j
µ′(Q)∧ν′(Q)
(ν )| = j j (ν′)| .
j Q ν′(Q) j Q
j
Finally, we set
J J
(cid:88) (cid:88)
µ = µ− µ and ν = ν − ν ,
0 j 0 j
j=1 j=1
so that
J J
(cid:88) (cid:88)
µ = µ and ν = ν.
j j
j=0 j=0
Itiseasytoseethatµ (Q) = ν (Q)forallQ ∈ Q andallj ∈ {0,...,J}.
j j j
To apply (2.4), we also need to check that µ ,ν ≥ 0.
j j
Lemma 2.3.The measures µ ,...,µ and ν ,...,ν are all positive.
0 J 0 J
Proof. By symmetry, it suffices to verify this fact for the sequence
µ ,...,µ .
0 J
We first show by backwards induction on j that
(cid:88)
µ ≥ 0 and 0 ≤ µ ≤ µ (A )
j+1 k j
j<k≤J
for all j = 0,...,J −1.50 2 Estimation of Wasserstein distances
For j = J −1, these bounds follow directly from the construction of
µ . Next assume that (A ) holds for some j, then
J j
(cid:88)
µ′ = µ− µ ≥ 0,
j k
j<k≤J
and therefore µ ≥ 0, since µ is obtained by reweighting µ′ on each
j j j
element of Q by a non-negative quantity. Note also that this non-
j
negative quantity is also bounded by one so that we also have µ ≤ µ′.
j j
Together these two facts yields 0 ≤ µ ≤ µ′ so that
j j
(cid:88) (cid:88) (cid:88)
0 ≤ µ = µ +µ ≤ µ +µ′ = µ.
k k j k j
j−1<k≤J j<k≤J j<k≤J
We have proved that (A ) holds. By induction, we obtain that
j−1
µ ,...,µ are all positive. Finally, since we have also shown that
1 J
(cid:88)
µ ≤ µ,
k
0<k≤J
we obtain µ ≥ 0 as well. ⊔⊓
0
In light of (2.4), it remains to bound µ (Ω) for j = 0,...,J. We first
j
claim that
|µ′(Q)−ν′(Q)| = |µ(Q)−ν(Q)| ∀Q ∈ Q , j = 1,...,J. (2.5)
j j j
This follows from the fact that
(cid:88)
µ′(Q)−ν′(Q) = µ(Q)−ν(Q)− (µ (Q)−ν (Q)),
j j k k
j<k≤J
since µ and ν assign the same mass to each element of Q and since
k k k
Q can be written as a disjoint union of elements of Q , so the sum
k
vanishes. We now claim that we can bound the mass that µ and ν
j j
assign to elements of Q in terms of the difference between µ and ν on
j
cubes in Q .
j+1
Lemma 2.4.If R ∈ Q for some 0 ≤ j < J, then
j
(cid:88)
µ (R) = ν (R) ≤ |µ(Q)−ν(Q)|.
j j
Q⊆R,Q∈Q
j+12.2 The dyadic partitioning argument 51
Proof. We have already shown that µ (R) = ν (R), so it suffices to
j j
show that expression holds for µ (R). For notational consistency, we
j
set µ′ = µ . Then, for any 0 ≤ j < J and any R ∈ Q ,
0 0 j
µ (R) ≤ µ′(R)
j j
(cid:88)
= µ′(Q)
j
Q⊆R,Q∈Q
j+1
(cid:88)
= (µ′ (Q)−µ (Q))
j+1 j+1
Q⊆R,Q∈Q
j+1
(cid:88)
= (µ′ (Q)−ν′ (Q))
j+1 j+1 +
Q⊆R,Q∈Q
j+1
(cid:88)
≤ |µ′ (Q)−ν′ (Q)|
j+1 j+1
Q⊆R,Q∈Q
j+1
(cid:88)
= |µ(Q)−ν(Q)|,
Q⊆R,Q∈Q
j+1
where the second equality comes from comparing the definitions of µ′
j
and µ′ , and the last equality follows from (2.5). ⊔⊓
j+1
Putting it all together, (2.4) implies
√ J
(cid:88)
W (µ,ν) ≤ d 2−jµ (Ω)
1 j
j=0
√ J−1 √
(cid:88)
= d 2−jµ (Ω)+ d2−Jµ (Ω)
j J
j=0
√ J−1(cid:18) (cid:19) √
(cid:88) (cid:88)
= d 2−j µ (R) + d2−Jµ (Ω)
j J
j=0 R∈Q
j
√ J−1(cid:18) (cid:19) √
(cid:88) (cid:88)
≤ d 2−j |µ(Q)−ν(Q)| + d2−J .
j=0 Q∈Q
j+1
This concludes the proof of Theorem 2.2. ⊔⊓
Applying Theorem 2.2 to µ and µ , we obtain the following bound.
n
Proposition 2.5.If the support of µ lies in [0,1]d, then52 2 Estimation of Wasserstein distances

n−1/2 if d = 1,
√  
EW (µ ,µ) ≲ d· (logn)/n−1/2 if d = 2,
1 n

n−1/d if d ≥ 3.
Proof. Theorem 2.2 implies that for any J ≥ 0,
√ J−1 √
(cid:88) (cid:88)
EW (µ ,µ) ≤ d 2−j E|µ (Q)−µ(Q)|+ d2−J
1 n n
j=0 Q∈Q
j+1
√ J−1 (cid:18) (cid:19)1/2
(cid:88) (cid:88)
≤ d 2−j2d(j+1)/2 E(µ (Q)−µ(Q))2
n
j=0 Q∈Q
j+1
√
+ d2−J
√ J−1 √
(cid:88)
≤ d 2−j2d(j+1)/2n−1/2+ d2−J
j=0

2(J+1)(d/2−1)n−1/2+2−J if d ≥ 3,
√  
≲ d· Jn−1/2+2−J if d = 2,

n−1/2+2−J if d = 1.
To balance these terms, we choose J such that 2J ≤ n1/2 < 2J+1 if
d ≤ 2, and J such that 2J+1 ≤ n1/d < 2J+2 if d ≥ 3. ⊔⊓
Note that bound of Proposition 2.5 is weaker than that of Proposi-
tion 2.1 when d = 2. Unfortunately, the dyadic partitioning argument
does not yield a sharp bound in two dimensions. We return to this
question in Section 2.4.
2.3 Dual chaining bounds
In this section, we present a superficially different proof of Proposi-
tion 2.5. Rather than constructing a coupling in the primal, we use the
dual representation of the 1-Wasserstein distance instead. The benefit
of this approach is that we can write
(cid:26)(cid:90) (cid:90) (cid:27)
W (µ ,µ) = sup fdµ − fdµ
1 n n
f∈Lip
1
n
1 (cid:88)
= sup {f(X )−Ef(X )}. (2.6)
i i
n
f∈Lip
1 i=12.3 Dual chaining bounds 53
The random process f (cid:55)→ 1 (cid:80)n {f(X ) − Ef(X )} is known as an
n i=1 i i
empirical process, and bounding the expected suprema of such processes
is a very common task in many areas of statistics.
To control this empirical process, we use a standard technique known
as chaining. Given a class F of real-valued functions on Ω ⊆ Rd, we call
a set F = {f ,...,f } an ε-cover of F if, for any f ∈ F, there exists
1 N
f ∈ F such that ∥f −f ∥ ≤ ε. The ε-covering number of F is
i i L∞(Ω)
N(ε,F) = min{|F| : F is an ε-cover of F}.
The chaining argument shows that the covering number of a class F
controls the supremum of an empirical process indexed by that set. We
use the following version:
Proposition 2.6([vH14, Theorem 5.31]). If F is a set of real-valued
functions on Ω such that ∥f∥ ≤ R for all f ∈ F, then
L∞(Ω)
1 (cid:88)n (cid:26) 1 (cid:90) R (cid:112) (cid:27)
Esup {f(X )−Ef(X )} ≲ inf τ + √ logN(ε,F)dε .
i i
f∈F n τ>0 n τ
i=1
Proposition 2.6 and (2.6) imply that we can obtain an upper bound
on EW (µ ,µ) as long as we can calculate the covering numbers of the
1 n
set of Lipschitz functions on [0,1]d. We also notice that we can assume
without loss of generality that the functions appearing in (2.6) take the
value 0 at (0,...,0). Indeed, a Lipschitz function on [0,1]d is bounded,
and since the value of 1 (cid:80)n {f(X )−Ef(X )} is unaffected if we shift
n i=1 i i
f by a constant, we may fix its value at (0,...,0) to be 0 without loss
of generality.
Lemma 2.7.Denote by Lip ([0,1]d) the set of 1-Lipschitz functions on
1
[0,1]d satisfying f(0) = 0. Then
√
logN(ε,Lip ([0,1]d)) ≲ (4 d/ε)d.
1
Proof. We bound the covering number by exhibiting an ε-cover of
Lip ([0,1]d) of the specified size. To do so, we again use the notion of
1
a dyadic partition of [0,1]d into a set Q of cubes of side length 2−j.
j
Each element of Q is of the form 2−j([k ,k +1]×...×[k ,k +1])
j 1 1 d d
for some integers k ,...,k ∈ [2j −1] := {0,...,2j −1}, and we denote
1 d
such an element by Q for ⃗k = (k ,...,k ).3
⃗k 1 d
3 This collection of cubes overlaps at the boundaries, but as above we may remove
overlaps to obtain a disjoint partition of [0,1]d.54 2 Estimation of Wasserstein distances
Fix an integer j ≥ 0 and positive δ > 0 to be specified. Consider the
set H of functions h satisfying the following requirements:
1. h is constant on each element of Q , i.e., there exist constants
j
(h ) such that h(x) = h for all x ∈ Q .
⃗k ⃗k∈[2j−1]d ⃗k ⃗k
2. h is an integer multiple of δ for all ⃗k ∈ [2j −1]d.
⃗k
3. h = 0.
(0,...,0) √
4. If ∥⃗k−⃗k′∥ ≤ 1, then |h −h | ≤ 2−j d+δ.
∞ ⃗k ⃗k′
√
WefirstclaimthatHconstitutesanε-coverofLip ([0,1]d)if2−j d+
1
δ ≤ ε. Given any f ∈ Lip ([0,1]d), denote by h the element of H given
1 f
by (h ) = δ⌊f(2−j(k ,...,k ))/δ⌋ for all ⃗k ∈ [2j −1]d. To see that
f ⃗k 1 d
h ∈ H, note that it immediately satisfies the first three requirements
f
by construction, and for the fourth, we have
|(h f) ⃗k −(h f) ⃗k′| = δ(cid:12) (cid:12)⌊f(2−j(k 1,...,k d))/δ⌋−⌊f(2−j(k 1′,...,k d′))/δ⌋(cid:12) (cid:12)
≤ |f(2−j(k ,...,k ))−f(2−j(k′,...,k′))|+δ
1 d 1 d
≤ 2−j∥⃗k−⃗k′∥ +δ,
2
where the last inequality follows from the fact that f is Lipschitz. Since
√
∥⃗k−⃗k′∥ ≤ d when ∥⃗k−⃗k′∥ = 1, the claim follows. Finally, for any
2 ∞
x ∈ Q , the fact that f is Lipschitz again implies
⃗k
|f(x)−(h f) ⃗k| = (cid:12) (cid:12)f(x)−δ⌊f(2−j(k 1,...,k d))/δ⌋(cid:12) (cid:12)
≤ |f(x)−f(2−j(k ,...,k ))|+δ
1 d
≤ diam(Q )+δ
√
⃗k
= 2−j d+δ.
√
Therefore ∥f −h ∥ ≤ 2−j d+δ.
f ∞
We have shown that for every f ∈ Lip ([0,1]d), there exists h ∈ H
√ 1 √ f
such that ∥f−h ∥ ≤ 2−j d+δ. Therefore, if 2−j d+δ ≤ ε, then H
f ∞ √
is an ε-cover of Lip ([0,1]d). We fix δ = 2−j d, so that this requirement
√ 1
reduces to 2−j d ≤ ε/2.
To bound |H|, note that if we fix the value of h for some ⃗k, then
⃗k
for any ⃗k′ such that ∥⃗k−⃗k′∥ = 1, there are at most 5 possible values
∞
of h . This follows from the fact that h must be an integer multiple
⃗k′ √ ⃗k′
of δ = 2−j d, and there are 5 integer multiples of δ in the interval
[h −2δ,h +2δ]. Therefore, if we consider specifying an element H
⃗k ⃗k
by specifying the values of h sequentially by setting h = 0 and
⃗k (0,...,0)2.3 Dual chaining bounds 55
proceeding in lexicographic order, then at each stage we have at most 5
choices for the next value of h . This implies that |H| ≤ 52dj−1.
√⃗k
For any j for which 2−j d ≤ ε/2, we have therefore obtained an
ε-cover H of F satisfying log|H| ≲ 2dj. Choosing 2j to be the smallest
√
power of two larger than 2 d/ε yields the claim. ⊔⊓
With the bound of Lemma 2.7 in hand, we can give another proof
of Proposition 2.5.
√
Proof of Proposition 2.5. Since ∥f∥ ≤ d for all f ∈ Lip ([0,1]d), by
∞ 1
Proposition 2.6 and (2.6), for any τ > 0,
√
1 (cid:90) d(cid:113)
EW (µ ,µ) ≲ τ + √ logN(ε,Lip ([0,1]d))dε.
1 n n 1
τ
Applying Lemma 2.7 yields
√
1 (cid:90) d √
EW (µ ,µ) ≲ τ + √ (4 d/ε)d/2dε.
1 n
n
τ
We now consider the bound separately for d = 1 and d > 1. If d = 1,
then we may take τ = 0 to obtain
1 (cid:90) 1
EW (µ ,µ) ≲ √ (4/ε)1/2dε ≲ n−1/2.
1 n
n
0
√
Ifd > 1,thenε−d/2isnolongerintegrableat0,sowetakeτ = 4 dn−1/d
to obtain
√
√ 1 (cid:90) d √
EW (µ ,µ) ≲ dn−1/d+ √ (4 d/ε)d/2dε.
1 n n √
4 dn−1/d
When d = 2, the integral is O(logn), and we obtain EW (µ ,µ) ≲
√ 1 n
(logn)/ n. When d > 2, the integral is O(n1/2−1/d), and we obtain
√
EW (µ ,µ) ≲ dn−1/d. ⊔⊓
1 n
Though these two proofs of Proposition 2.5 look quite different,
they are in fact very similar: in both cases, we employ a multi-scale
decomposition of [0,1]d. The dyadic partitioning argument uses this
decomposition to construct a coupling in the primal; the chaining
argument uses this decomposition to control the covering numbers of
Lipschitz functions in the dual.56 2 Estimation of Wasserstein distances
2.4 A finer analysis for d = 2
Both the dyadic partition argument presented in Section 2.2 and the
chaining argument presented in Section 2.3 suffer from the defect that
they fail to obtain the correct rate for the Wasserstein law of large
numbers in two dimensions. This fact is related to the fact that d = 2
is the “critical” case for the behavior EW (µ ,µ)—it can be shown
1 n
that in d = 1, the cost of the optimal transport between µ and µ
n
is dominated by “global” features and that when d ≥ 3, the cost of
optimal transport is dominated by “local” irregularities. In dimension
2, by contrast, irregularities at all scales contribute simultaneously, and
bounding the optimal cost requires more care.
The correct rate for d = 2 was first discovered by Ajtai, Komlo´s, and
Tusn´ady [AKT84] by a somewhat delicate argument. In this section,
we present an ingenious approach due to Bobkov and Ledoux [BL21]
that obtains the correct rate by simpler means. This proof is based on
Fourier analysis, and as a first step, we show that we can focus our
attention on periodic functions, to which the tools of Fourier analysis
cannaturallybeapplied.Thisgivesrisetothefollowingperiodicversion
of the Wasserstein distance: for probability measures µ and ν on Rd,
define
(cid:90)
W(cid:102)1(µ,ν) = sup f(dµ−dν), (2.7)
f∈L(cid:103)ip
where L(cid:103)ip denotes the set of 1-Lipschitz, 2π-periodic C∞ functions
on Rd. For measures on the cube, this definition actually agrees with
standard Wasserstein distance.
Lemma 2.8.If the supports of µ and ν lie in [0,1]d, then W(cid:102)1(µ,ν) =
W (µ,ν).
1
Proof. The point of this lemma is that, under the restriction on the
supportofµandν,wecanassumethattheLipschitzfunctionsappearing
in the dual representation of W are both periodic and smooth.
1
We first handle the former restriction. Define a metric d on Rd by
Td
d (x,y) = min∥x−y−2πz∥.
Td
z∈Zd
The notation d is used to emphasize that this is the metric that arises
Td
from identifying the opposite faces of [0,2π]d so that it becomes a flat
torus. Given any f ∈ Lip ([0,1]d), define the function f˜: Rd → R by
12.4 A finer analysis for d=2 57
f˜(y) = sup {f(x)−d (x,y)}. (2.8)
Td
x∈[0,1]d
Foreachx ∈ [0,1]d,thefunctiony (cid:55)→ f(x)−d (x,y)is2π-periodicand
Td
Lipschitz with respect to the Euclidean metric on Rd (since both facts
are true of d (x,y)). Both periodicity and Lipschitzness are preserved
Td
by taking pointwise suprema, so these properties are inherited by f˜
as well. We also note the crucial fact that f˜= f on [0,1]d: indeed, for
y ∈ [0,1]d, we clearly have f˜(y) ≥ f(y) by choosing x = y in (2.8). On
the other hand, since d (x,y) = ∥x−y∥ for any x,y ∈ [0,1]d, we also
Td
have
f(x)−d (x,y) = f(x)−∥x−y∥ ≤ f(y) ∀x ∈ [0,1]d,
Td
where the inequality follows from the fact that f is Lipschitz. Taking
suprema on both sides yields f˜(y) ≤ f(y).
Since the supports of µ and ν lie in [0,1]d, we therefore have, for any
f ∈ Lip ([0,1]d)
1
(cid:90) (cid:90)
f(dµ−dν) = f˜(dµ−dν),
where the function on the right side is Lipschitz and 2π-periodic. This
implies that we can always assume that the functions appearing in the
dual representation of W are periodic.
1
The restriction to smooth functions is routine: since any Lipschitz
function can be uniformly approximated by a smooth function, we can
always assume that the functions in question are C∞. ⊔⊓
Givenaprobabilitymeasureµ,wedenotebyϕ itsFouriertransform
µ
(or characteristic function):
(cid:90)
ϕ (m) = ei⟨m,z⟩µ(dz), m ∈ Zd. (2.9)
µ
The basis of the Bobkov–Ledoux argument is the following proposition.
Proposition 2.9.
(cid:88)
W(cid:102)1(µ,ν)2 ≤ ∥m∥−2|ϕ µ(m)−ϕ ν(m)|2, (2.10)
m̸=0
where the sum is over all nonzero m ∈ Zd and ∥m∥2 = m2+···+m2.
1 d58 2 Estimation of Wasserstein distances
Before giving the proof, we pause for a moment to compare Propo-
sition 2.9 to Theorem 2.2. Both results give a bound on W (µ,ν) by
1
comparing them at different scales: in the case of Theorem 2.2, this
is done by calculating how much they differ on smaller and smaller
cubes, in the case of Proposition 2.9, this is done by calculating how
much they differ at higher and higher frequencies. In both cases, each
term in the sum is weighted by the scale of the comparison (2−j in the
case of Theorem 2.2, ∥m∥−2 in the case of Proposition 2.9). The key
difference between these bounds is that Theorem 2.2 has an ℓ1 flavor,
whereas Proposition 2.9 has an ℓ2 flavor. This different turns out to be
√
the source of the logn savings in the rate for d = 2.
Proof of Proposition 2.9. Given a 2π-periodic C∞ function f, we can
expand it as a Fourier series:
(cid:88)
f(x) = fˆ ei⟨m,x⟩,
m
m∈Zd
where the coefficients fˆ tend to zero faster than any polynomial as
m
∥m∥ → ∞. We may therefore differentiate term-by-term and apply
Parseval’s identity to obtain
(cid:90)
1 (cid:88)
(∂ f(x))2dx = m2|fˆ(m)|2,
(2π)d i i
[0,2π]d
m∈Zd
and summing over the coordinates yields
(cid:90)
1 (cid:88)
∥∇f(x)∥2dx = ∥m∥2|fˆ(m)|2.
(2π)d
[0,2π]d
m∈Zd
If we assume that f is 1-Lipschitz, then ∥∇f(x)∥ ≤ 1 for all x ∈ [0,2π]d,
so
(cid:90)
(cid:88) 1
∥m∥2|fˆ(m)|2 = ∥∇f(x)∥2dx ≤ 1. (2.11)
(2π)d
m∈Zd
[0,2π]d
Fubini’s theorem therefore implies that for any 1-Lipschitz, 2π-
periodic C∞ function f,
(cid:90) (cid:90)
(cid:88)
f(dµ−dν) = fˆ ei⟨m,x⟩(µ(dx)−ν(dx))
m
m∈Zd2.4 A finer analysis for d=2 59
(cid:88)
= fˆ (ϕ (m)−ϕ (m))
m µ ν
m∈Zd
(cid:88)
= fˆ (ϕ (m)−ϕ (m)),
m µ ν
m∈Zd\{0}
wherethelastequalityfollowsfromthefactthatϕ (0) = ϕ (0) = 1.The
µ ν
result then follows from the Cauchy–Schwarz inequality and (2.11). ⊔⊓
Unfortunately, Proposition 2.9 is often vacuous—if µ is not abso-
lutely continuous, then ϕ is not integrable, and the sum in (2.10)
µ
can diverge. This problem is immediately apparent when attempting
to apply Proposition 2.9 to the singular empirical measure µ . The
n
solution to this issue is to inject additional regularity into the problem
by convolving with Gaussians. For any ε > 0, we denote by ν ⋆γ the
ε
convolution of ν with a N(0,εI) distribution; equivalently, ν ⋆γ is the
√ ε
law of X + εZ where X ∼ ν and Z ∼ N(0,I) are independent. We
first recall the effect that this smoothing has on the Fourier transform.
Lemma 2.10.For all ε > 0,
ϕ (m) = ϕ (m)e−ε∥m∥2/2 ∀m ∈ Zd.
ν⋆γε ν
Proof. This follows directly from the representation
(cid:90) √
ϕ (m) = ei⟨m,y⟩(ν ⋆γ )(dy) = Eei⟨m,X+ εZ⟩
ν⋆γε ε
for X ∼ ν and Z ∼ N(0,I) independent. ⊔⊓
The smoothing operation is useful because it immediately ensures
that the Fourier transform of the resulting measure is well-behaved.
Moreover, smoothing only changes W(cid:102)1 by a small amount.
Lemma 2.11.For all ε > 0,
√
W(cid:102)1(µ,ν) ≤ W(cid:102)1(µ,ν ⋆γ ε)+ dε.
Proof. First, the expression W(cid:102)1 satisfies the triangle inequality; this
follows directly from the definition in (2.7):
(cid:90) (cid:90)
W(cid:102)1(µ,ν′)+W(cid:102)1(ν′,ν) = sup f(dµ−dν′)+ sup f(dν′−dν)
f∈L(cid:103)ip f∈L(cid:103)ip60 2 Estimation of Wasserstein distances
(cid:26)(cid:90) (cid:90) (cid:27)
≥ sup f(dµ−dν′)+ f(dν′−dν)
f∈L(cid:103)ip
= W(cid:102)1(µ,ν).
Second, W(cid:102)1 is dominated by W 1, since the supremum in (2.7) is taken
over a strict subset of Lip. Combining these facts yields
W(cid:102)1(µ,ν) ≤ W(cid:102)1(µ,ν⋆γ ε)+W(cid:102)1(ν,ν⋆γ ε) ≤ W(cid:102)1(µ,ν⋆γ ε)+W 1(ν,ν⋆γ ε).
√
We now use the fact that (X,X + εZ) with X ∼ ν,Z ∼ N(0,I)
independent is a coupling between ν and ν ⋆γ , so that
ε
√ √ √
W (ν,ν ⋆γ ) ≤ E∥X −(X + εZ)∥ = εE∥Z∥ ≤ dε.
1 ε
This concludes the proof. ⊔⊓
Combining the preceding two lemmas yields the following corollary
to Proposition 2.9.
Corollary 2.12.For any ε > 0,
(cid:115) √
(cid:88)
W(cid:102)1(µ,ν) ≤ ∥m∥−2e−ε∥m∥2|ϕ µ(m)−ϕ ν(m)|2+2 dε.
m̸=0
We can now prove the desired bound.
Theorem 2.13.For any probability measure µ with support in [0,1]2,
(cid:112)
EW (µ ,µ) ≲ logn/n.
1 n
Proof. Since µ and µ have support lying in [0,1]2, we may equivalently
n
prove an upper bound on EW(cid:102)1(µ n,µ). Applying Corollary 2.12 and
Jensen’s inequality yields, for any ε > 0,
(cid:115) √
(cid:88)
EW(cid:102)1(µ n,µ) ≤ ∥m∥−2e−ε∥m∥2E|ϕ µn(m)−ϕ µ(m)|2+2 2ε.
m̸=0
Wecanwriteϕ µn(m)−ϕ µ(m) = n1 (cid:80)n i=1{ei⟨m,Xi⟩−Eei⟨m,Xi⟩},andsince
|ei⟨m,Xi⟩| = 1 almost surely we conclude that
E|ϕ (m)−ϕ (m)|2 ≤ n−1 ∀m ∈ Zd. (2.12)
µn µ
Continuing, we have for any ε > 0,2.5 Applications 61
(cid:115) √
(cid:88)
EW(cid:102)1(µ n,µ) ≤ n−1/2 ∥m∥−2e−ε∥m∥2 +2 2ε. (2.13)
m̸=0
Bycomparingthesumtotheintegral(cid:82) ∥x∥−2e−ε∥x∥2dx,weobtain
∥x∥≥1
that the sum is of order log(1/ε). Therefore, we obtain
(cid:112) √
EW(cid:102)1(µ n,µ) ≲ log(1/ε)/n+ ε.
Choosing ε = n−1 gives the claim. ⊔⊓
2.5 Applications
2.5.1 Estimation of Wasserstein distances
So far, we have focused on estimating a measure µ in Wasserstein
distance using the empirical measure µ . As the title of this chapter
n
indicates, we are often interested in the estimation of Wasserstein dis-
tances. Indeed, Wasserstein distances are central to many statistical
tasks. For example, one of the first applications of the 1-Wasserstein
distance (under the name “earth mover’s distance”) to machine learning
was in the context of information retrieval where it was used to measure
the distance between images [RTG00]. Other immediate examples in-
cludenearestneighbors[BDI+20,Pon23]andregression[GP22,CLM23]
for example.
ThegoalofestimationistoproduceanestimatorW(cid:99)ofW 1(µ,ν)using
i.i.d. data X ,...,X ∼ µ and Y ,...,Y ∼ ν. A natural candidate
1 m 1 n
is the plug-in estimator W(cid:99) := W 1(µ m,ν n) where µ
m
and ν
n
are the
empirical measures associated to the samples above. A performance
bound for this estimator can be readily obtained using the triangle
inequality and Proposition 2.1: for d ≥ 3,
E|W (µ ,ν )−W (µ,ν)| ≤ EW (µ ,µ)+EW (ν ,ν) ≲ (m∧n)−1/d.
1 m n 1 1 m 1 n
This coarse bound turns out to be sharp in general. In fact, using the
more general result (2.21) presented at the end of this Chapter, we can
get that for d > 2p.
E|W (µ ,ν )−W (µ,ν)| ≤ EW (µ ,µ)+EW (ν ,ν) ≲ (m∧n)−1/d.
p m n p p m p n
It turns out that when p > 1, this bound is only sharp when µ and ν
are sufficiently close. Indeed, better rates can be obtained when p > 1
and W (µ,ν) > c > 0. For example, when p = 2, [MNW24] show that
p62 2 Estimation of Wasserstein distances
E|W (µ ,ν )−W (µ,ν)| ≲ (m∧n)−2/d.
2 m n 2
Whilesignificant,thisimprovementshowsthatestimationofWasserstein
distances still suffers from the curse of dimensionality.
2.5.2 Hypothesis testing
These upper bounds can be readily applied to two classical non-
parametric hypothesis testing problems: goodness-of-fit and two-sample
(a.k.a. homogeneity) testing.
Consider first the goodness-of-fit test. Given observation X ,...,X
1 n
i.i.d. from some unknown distribution µ, and a fixed distribution µ0,
the goal is to test
H : µ = µ0 vs. H : µ ̸= µ0.
0 1
For example, µ0 can be taken to be a standard Gaussian distribution
on Rd or the uniform distribution on [0,1]d. There exist many goodness-
of-fit tests when d = 1, for example, the Kolmogorov–Smirnov test
for continuous distributions. For discrete distributions, the χ2-test is
another popular choice; see, e.g., [LR05, Chapter 14].
Note that the two hypotheses can be written equivalently as
H0 : W (µ,µ0) = 0 vs. H : W (µ,µ0) > 0.
1 1 1
Thiscallsforasimpletestwhichconsistsinrejectingthenullhypothesis
at level α ∈ (0,1) as soon as W (µ ,µ0) > Tα for some threshold Tα
1 n n n
such that
µ0[W (µ ,µ0) > Tα] = α.
1 n n
If we observe X = x , i = 1,...,n, a p-value for such a test may be
i i
computed as
µ0[W (µ ,µ0) > W (µobs,µ0)]
1 n 1 n
where
n
1 (cid:88)
µobs = δ .
n n xi
i=1
BoththecomputationofTα andofthep-valuerequireunderstanding
n
the actual distribution of W (µ ,µ0) under the null hypothesis. Our
1 n
results above are actually quite far from achieving this level of preci-
sion since we only know an upper bound on E[W (µ ,µ0)] when µ is
1 n 02.5 Applications 63
supported on the unit cube. Nevertheless, these bounds are sufficient to
paint a rather disappointing picture of the potential of the Wasserstein
distance in multivariate goodness-of-fit tests. Such a test would require
W (µ,µ ) ≳ n−1/d in order to detect a deviation of µ from µ with a
1 0 0
reasonable type II error. Even in moderate dimensions, this level of
separation is quite large. In fact, as the next section shows, this bound
is optimal and, as a result, the separation n−1/d is necessary.
An explanation for this phenomenon is that a test based on W tries
1
to be powerful against too many alternatives. Assume for the sake of
discussion that µ0 is the standard Gaussian distribution over Rd. The
1-Wasserstein distance does not discriminate between distributions that
are not µ0: Gaussian distributions, distributions with smooth densities,
those with discontinuous densities, or even discrete distributions...our
test statistic {W (µ ,µ0) > Tα} tries to detect all of them and spreads
1 n n
thin, resulting in low power against all alternatives. This behavior is
to be contrasted with a simple parametric test, for example the Wald
test {|X¯ | > τ }, which simply tries to detect if the mean of µ differs
n n
from that of µ0. This test is clearly unable to detect even if µ is a
Rademacher distribution, which is quite far from µ , but it focuses all
0
of its efforts on shifts in means: when these happen, it can detect them
very accurately.
The manifestation of the curse of dimensionality also extends to
two-sample testing where one observes two samples X ,...,X from µ
1 m
and Y ,...,Y from ν and the goal is to test
1 n
H : µ = ν vs. H : µ ̸= ν.
0 1
Denote the corresponding empirical distributions by
m n
1 (cid:88) 1 (cid:88)
µ = δ , ν = δ .
m
m
Xi n
n
Yj
i=1 j=1
In this context it is natural to reject the null hypothesis if W (µ ,ν )
1 m n
is large. Akin to the goodness-fit-test, such tests require a sample size
that is exponential in the dimension to achieve any reasonable power.
The conclusion of this section is that Wasserstein distances suf-
fer from the curse of dimensionality and are therefore unsuitable for
statistical applications of moderate dimension. In Section 2.8 we de-
scribe various regularizations of Wasserstein distances that escape the
curse of dimensionality and have been successfully applied in large-scale
statistical applications.64 2 Estimation of Wasserstein distances
2.6 Optimality
We have established upper bounds on the Wasserstein distance between
the empirical distribution µ and the data generating distribution µ
n
in two different ways: using the primal and using the dual formulation
of the problem. Omitting idiosyncrasies associated to low dimensions,
we found that µ estimates µ in W distance at a rate of order n−1/d.
n 1
While this result readily yields consistency, the rate is slow even in
moderate dimensions and is symptomatic of the curse of dimensionality
that plagues most non-parametric methods. One could wonder then
whether such rates can be improved.
Note that there are two ways to potentially improve these rates.
The most obvious one would be to provide a tighter analysis than the
one above and show that in fact, E[W (µ ,µ)] is much smaller than
1 n
n−1/d. Another possibility would be that while this rate is tight for the
empirical measures µ , there could be another estimator µ˜ of µ that
n n
enjoys much faster rates. In fact, the answer to both questions, while
different in nature, is negative, as illustrated by lower bounds.
While a negative answer to the second question implies a negative
answer to the first one—if no estimator can estimate µ faster than
n−1/d then certainly the empirical measure µ cannot—we also make
n
the negative answer to the first question explicit since it is, in some
sense stronger. Indeed, we show below that even in the case where µ
is the uniform measure on [0,1]d then, E[W (µ ,µ)] ≳ n−1/d. However,
1 n
in that case, there is clearly a better estimator than µ : simply take
n
µ˜ = µ itself! The answer to the second question relies on the theory
n
of minimax lower bounds as in [Tsy09, Chapter 2] and states that
for any estimator, i.e., any measurable function µ˜ = µ˜ (X ,...,X )
n n 1 n
of the data X ,...,X , there exists µ supported on [0,1]d such that
1 n
E[W (µ˜ ,µ)] ≳ n−1/d.Unlikethelowerboundfortheempiricalmeasure
1 n
µ , in the minimax lower bounds, the unfavorable distribution µ is not
n
explicit.
2.6.1 Lower bounds for the empirical measure µ
n
The goal of this section is to show that any distribution supported on n
points has to be far from the uniform measure on [0,1]d in W distance.
1
Theorem 2.14.Fix d ≥ 3 and let µ denote the uniform measure on
[0,1]d. Then for any measure µ˜ supported on n points x ,...,x ∈ Rd,
n 1 n
it holds2.6 Optimality 65
1
W (µ˜ ,µ) ≥ n−1/d.
1 n
108d
Proof. We employ the dual formulation of Theorem 1.21 since proving
a lower bound on W can be done by simply exhibiting a 1-Lipschitz
1
function, which we define as follows. Given x ∈ [0,1]d, let ξ(x) ∈
{x ,...,x } denote the closest point to x in {x ,...,x } (ties are
1 n 1 n
broken arbitrarily). Next, consider the function
f (x) = ∥x−ξ (x)∥,
n n
which is 1-Lipschitz thanks the the reverse triangle inequality. Moreover,
(cid:82)
for any i = 1,...,n, we have f (x ) = 0 so that fdµ˜ = 0. Hence
n i n
(cid:90) (cid:90)
W (µ˜ ,µ) ≥ f dµ = ∥x−ξ (x)∥µ(dx).
1 n n n
To bound this quantity from below, we show that µ places significant
massonpointsthatarefarfromany x .Tothatend,considerapartition
i
Q of [0,1]d into cubes of side length (2n)−1/d. Since |Q| = 2n, there
exist n such cubes Q ,...,Q that do not contain any of the x ’s. Let
1 n i
Q ∈ Q be one such cube with center q and consider its subcube Q′ ⊂ Q
also with center q but with a smaller side length than Q by a factor of
1−2/d. Using Minkowski sum notation, we can write this as:
Q′ = (cid:0) 1− 2(cid:1) (Q−{q})+{q}.
d
By construction, any x ∈ Q′ satisfies
1
∥x−ξ (x)∥ ≥ inf ∥x−y∥ = ·(2n)−1/d.
n
x∈Q′ d
y∈Qc
Hence
(cid:90) (cid:88)n (cid:90) (2n)−1/d (cid:88)n
∥x−ξ (x)∥µ(dx) ≥ ∥x−ξ (x)∥µ(dx) ≥ µ(Q′).
n n d i
Q′
i=1 i i=1
We conclude by observing that
µ(Q′) =
(cid:0)1−2/d(cid:1)d
≥
1
,
i (2n)1/d 54n
where we used the fact that d (cid:55)→ (1 − 2/d)d is increasing and that
d ≥ 3. ⊔⊓66 2 Estimation of Wasserstein distances
Theorem 2.14 shows that W (µ ,µ) is indeed of order n−1/d at least
1 n
for d ≥ 3. In fact the lower bound holds almost surely in X ,...,X
1 n
since it only exploits the fact that µ has a support of size at most n.
n
Note that the d dependence in Theorem 2.14 is off by some polynomial
√
factors in d. It can be shown that the d factor in Proposition 2.5
cannot be improved; see Exercise 1.
2.6.2 Minimax lower bounds
While it is hard to think of a better estimator for µ than µ in general
n
(in Section 2.7 we show that we can under additional assumptions on
µ) it could be the case that there exists another estimator µ˜ for which
n
E[W (µ˜ ,µ)] is smaller than E[W (µ ,µ)] uniformly over all measures
1 n 1 n
µ. This possibility is ruled out by the following minimax lower bound.
Theorem 2.15.Fix d ≥ 3,n ≥ 8 and let X ,...,X be n i.i.d. obser-
1 n
vations from a distribution µ on Rd. For any estimator µ˜ , i.e., any
n
measurable function of X ,...,X , there exists a measure µ supported
1 n
on [0,1]d such that
1
E [W (µ˜ ,µ)] ≥ (2n)−1/d.
µ 1 n
16
Proof. Ourproofreliesonclassicaltechniquesforminimaxlowerbounds.
Inparticular,weuseTheorem2.12in[Tsy09].Accordingtothistheorem,
if we can find 2m probability measures indexed by ω ∈ {−1,1}m each
supported on [0,1]d such that
(i) W (µ(ω),µ(ω′)) ≥ rn (cid:80)m |ω −ω′| for any ω,ω′ ∈ {−1,1}m,
1 2 j=1 j j
(ii) For any ω,∈ {−1,1}m differing in at most one coordinate,
1
KL(µ(ω)∥µ(ω′))
≤ ,
2n
then for any estimator µ˜ based on n i.i.d. observations, there exists
n
ω ∈ {−1,1}m such that
mr
E [W (µ˜ ,µ(ω))] ≥ n .
µ(ω) 1 n 4
In our construction, we take m = n and define the measures µ(ω) to
besupportedonadiscretesetasfollows.AsintheproofofTheorem2.14,
let Q denote a partition of [0,1]d into 2n cubes of side length (2n)−1/d2.7 Faster rates for smooth measures 67
and let q ,...,q denote their centers. Let µ(0) denote the uniform
1 2n
measure on {q ,...,q }:
1 2n
2n
1 (cid:88)
µ(0) = δ .
2n
qi
i=1
For ω ∈ {−1,1}n, let µ(ω) denote a perturbation of µ(0) defined as
n
α (cid:88)
µ(ω) = µ(0)+ ω (δ −δ ),
2n
i qi qn+i
i=1
where ω = (ω ,...,ω ) and α ∈ (0,1) is to be defined later. Note that
1 n
µ(ω) is a probability measure.
Since ∥q −q ∥ ≥ (2n)−1/d for j ̸= k for we have
j k
n n
W (µ(ω),µ(ω′)) ≥ α (2n)−1/d(cid:88) |ω −ω′| =: r n (cid:88) |ω −ω′|
1 2n j j 2 j j
j=1 j=1
for any ω,ω′ ∈ {0}n∪{−1,1}n.
It remains to show that (ii) holds for a suitable choice of α. To that
end, suppose that ω and ω′ differ on the jth coordinate. Observe that
KL(µ(ω)∥µ(ω′)) =
(cid:88)2n
µ(ω)(q
)log(cid:0)µ(ω)(q i)(cid:1)
i µ(ω′)(q )
i
i=1
(cid:26) (cid:27)
1 1+αω 1−αω
j j
= (1+αω )log +(1−αω )log
j j
2n 1−αω 1+αω
j j
α 1+α
= log ,
n 1−α
and this quantity is smaller than 1 if α = 1. With this choice of α, we
2n 4
obtain
1
r = (2n)−1/d, (2.14)
n
4n
which implies the desired bound. ⊔⊓
2.7 Faster rates for smooth measures
The preceding section indicates that no estimator can avoid the slow
n−1/d rate in general.68 2 Estimation of Wasserstein distances
There are multiple ways to alleviate this curse of dimensionality
and the rest of this chapter illustrates two main approaches. In this
section, we impose smoothness assumptions on the measure µ. Such
assumptions are classical in non-parametric statistics and known to
partially mitigate the curse of dimensionality. In the next section, we
describe how modifying/regularizing the Wasserstein distance into other
distances that are similar in nature can be used to bypass the curse of
dimensionality altogether.4
Thefactthatimposingsmoothnessconditionsonµcanleadtobetter
rates is natural in light of the lower bound presented in Theorem 2.15.
The measures used in the proof are mixtures of Dirac masses and are
therefore highly “irregular” in the sense that they do not even possess
densities with respect to the Lebesgue measure. By assuming that µ is
smooth, we rule out these pathological examples.
For mathematical convenience, we consider smooth densities de-
fined on the torus Td := Rd/(2πZ)d. Concretely, this can be viewed as
isomorphic to the set [0,2π)d, equipped with the metric d (x,y) :=
Td
min ∥x−y−2πz∥.Onthisspace,theWassersteindistancecoincides
z∈Zd
with W(cid:102)1 defined in Section 2.4.
We focus on the torus so that we can again use the tools of Fourier
analysis. A similar but slightly more technical argument can extend the
results of this section to standard Euclidean space. Note that the n−1/d
minimax lower bound proved in the previous section still holds on the
torus, so in moving to this setting we have not affected the fundamental
statistical difficulty of the problem.
We consider a probability measure µ on Td with a density, which
we also denote by µ. We make the assumption that the density of µ is
smooth, in the sense that it lies in a Sobolev space.
Definition 2.16.Given a positive integer s, the Sobolev space Hs con-
sists of all functions f : Td → R such that for every multi-index α with
|α| ≤ s, the derivative dαf lies in L2. Given f ∈ Hs, its Sobolev norm
is defined to be
(cid:90)
∥f∥2 = max ∥dαf∥2dx.
Hs
|α|≤s Td
The importance of the Sobolev spaces lies in their close connection
with Fourier series. If µ ∈ Hs, then its Fourier transform (2.9) satisfies
4 Since we are interested in improvements to the exponent in the rate of decay, in
the remainder of this chapter we ignore dimension-dependent constants in the
bounds for clarity.2.7 Faster rates for smooth measures 69
(cid:88)
(1+∥m∥2s)|ϕ (m)|2 < ∞.
µ
m∈Zd
Moreover, this expression in terms of Fourier coefficients actually gives
an equivalence of norms. Indeed, from the Fourier representation
(cid:88)
µ(x) ∝ ϕ (m)e−i⟨m,x⟩
µ
m∈Zd
we obtain, for any multi-index α,
(cid:88)
dαµ(x) ∝ m2αϕ (m)e−i⟨m,x⟩,
µ
m∈Zd
where mα = mα1···mα d. By Parseval’s identity,
1 d
(cid:90)
(cid:88)
∥dαf∥2dx ≍ m2α|ϕ (m)|2 (2.15)
µ
Td
m∈Zd
(cid:88)
≲ (1+∥m∥2s)|ϕ (m)|2. (2.16)
µ
m∈Zd
On the other hand, by the binomial theorem,
(cid:88)
∥m∥2s = m2α.
|α|=s
By (2.15), it holds that
(cid:88) (cid:88) (cid:88)
∥m∥2s|ϕ (m)|2 = m2α|ϕ (m)|2
µ µ
m∈Zd m∈Zd|α|=s
(cid:90)
(cid:88)
≲ ∥dαf∥2dx. (2.17)
Td
|α|=s
By (2.16) and (2.17) (applying the latter inequality also for s = 0), we
have shown that
(cid:88)
∥f∥2 ≍ (1+∥m∥2s)|ϕ (m)|2.
Hs µ
m∈Zd
We construct an estimator µ˜ obtained by estimating the Fourier
n
coefficients of µ for all m ∈ Zd satisfying ∥m∥ ≤ M. Concretely, we
define70 2 Estimation of Wasserstein distances
n
1 (cid:88)
ϕ(cid:99)µ(m) = ϕ µn(m) =
n
ei⟨m,Xj⟩,
j=1
then for any M ≥ 1 we set
1 (cid:88)
µ˜ n(x) =
(2π)d
ϕ(cid:99)µ(m)e−i⟨m,x⟩.
∥m∥≤M
Note that while µ˜ is always a real-valued function on Td integrating
n
to 1, it may not be positive everywhere; however, we ignore this issue
for now. Even when the density µ˜ takes negative values, the definition
n
of W(cid:102)1 in terms of its dual representation (2.7) still gives a sensible
interpretation of W(cid:102)1(µ,µ˜ n).
We have the following result.
Proposition 2.17.Assume µ ∈ Hs(Td) with ∥µ∥Hs ≲ 1. For any
M ≥ 1 and d ≥ 3, the estimator µ defined above satisfies
(cid:101)n
EW(cid:102)1(µ,µ˜ n) ≲ n−1/2Md/2−1+M−(s+1).
Proof. We first note that
(cid:88)
W(cid:102)1(µ,µ˜ n)2 ≲ ∥m∥−2|ϕ µ(m)−ϕ(cid:99)µ(m)|2
m̸=0,∥m∥≤M
(cid:88)
+ ∥m∥−2|ϕ (m)|2.
µ
∥m∥>M
This follows directly from Proposition 2.9, using the fact that the
(signed) measure µ˜
n
has Fourier coefficients ϕ(cid:99)µ(m) for ∥m∥ ≤ M and
zero otherwise. As in Section 2.4, we may use the fact that |ei⟨m,Xj⟩| ≤ 1
to conclude that E|ϕ µ(m)−ϕ(cid:99)µ(m)|2 ≤ n−1. Therefore,
(cid:115) (cid:115)
(cid:88) (cid:88)
EW(cid:102)1(µ,µ˜ n) ≤ n−1/2 ∥m∥−2+ ∥m∥−2|ϕ µ(m)|2.
m̸=0,∥m∥≤M ∥m∥>M
Before proceeding, we pause to compare this bound with (2.13).
There are two differences: first, the smooth cut-off e−ε∥m∥2 in (2.13) has
been replaced by the restriction ∥m∥ ≤ M. This change is inessential:
since e−ε∥m∥2 ≪ 1 when ∥m∥2 ≫ ε−1, the smooth cut off term mimics
a restriction to ∥m∥ ≲ ε−1/2. The second difference is that the term
√
2 2ε in (2.13) has been replaced by a term that depends on the higher2.7 Faster rates for smooth measures 71
Fourier coefficients of µ. This change is crucial, since, as we now show,
it implies that the second term automatically becomes smaller when µ
is smooth.
Since (cid:80) ∥m∥2s|ϕ (m)|2 ≲ 1, we may write
m∈Zd µ
(cid:88) (cid:88)
∥m∥−2|ϕ (m)|2 ≤ M−2(s+1) ∥m∥2s|ϕ (m)|2
µ µ
∥m∥>M ∥m∥>M
≲ M−2(s+1).
For the first term, we can compare the sum with the integral
((cid:82) ∥x∥−2dx)1/2, which is of order Md/2−1 when d ≥ 3. ⊔⊓
1≤∥x∥≤M
Tuning M appropriately, we arrive at the following theorem.
Theorem 2.18.If µ ∈ Hs(Td) with ∥µ∥Hs ≲ 1, then there exists an
estimator µ˜ such that
n
EW(cid:102)1(µ,µ˜ n) ≲ n− ds ++ 21 s .
Proof. Apply Proposition 2.17 with M ≍ n1/(d+2s). ⊔⊓
Theorem 2.18 shows that, when s > 0, the estimator µ˜ strictly
n
improves over the empirical measure µ . However, we note that the
n
estimator µ˜ is a signed measure, which may be viewed as undesirable.
n
This is a common phenomenon in non-parametric statistics; for instance,
in the design of kernel density estimators for very smooth densities, it
is necessary to employ higher-order kernels which take negative values.
If a positive estimator is desired, then it is possible to show that the
estimator µ¯ defined by
n
µ¯
n
:= argminW(cid:102)1(ν,µ˜ n)
ν∈P(Td)
also achieves the bound in Theorem 2.18: indeed, since µ ∈ P(Td),
W(cid:102)1(µ,µ¯ n) ≤ W(cid:102)1(µ,µ˜ n)+W(cid:102)1(µ¯ n,µ˜ n) ≤ 2W(cid:102)1(µ,µ˜ n),
so that µ¯ is worse than µ˜ by a factor of at most 2.
n n72 2 Estimation of Wasserstein distances
2.8 Regularization of Wasserstein distances
The curse of dimensionality that plagues statistical optimal transport
has been recognized since its early days. To overcome this limitation,
researchershaveproposedmultiplesolutionswhichcan,inretrospect,be
viewed as some kind of regularization of the original optimal transport
problem. In the rest of this section, we review three examples and
demonstrate how they escape the curse of dimensionality.
2.8.1 Integral probability metrics
Recall from the dual chaining argument of Section 2.3 that the rate
n−1/d came directly from the entropy number of the class of 1-Lipschitz
functions. Lemma 2.7 showed
√
logN(ε,Lip ([0,1]d)) ≲ (4 d/ε)d.
1
The polynomial scaling in 1/ε is characteristic of non-parametric classes,
as opposed to parametric classes where this scaling is logarithmic;
see e.g., [GN16]. This raises the question of potentially replacing the
class of 1-Lipschitz functions with a smaller, ideally parametric, class of
functions.
Take for example the class of linear functions on Rd:
(cid:110) (cid:111)
F := f(x) = ⟨θ,x⟩ : θ,x ∈ Rd, ∥θ∥ = 1 ,
lin
and consider the quantity
(cid:26)(cid:90) (cid:90) (cid:27)
δ(µ,ν) = sup fdµ− fdν
f∈F
lin
(cid:26)(cid:90) (cid:90) (cid:27)
= sup ⟨θ,x⟩µ(dx)− ⟨θ,y⟩ν(dy)
θ∈Rd,∥θ∥=1
= ∥E [X]−E [Y]∥.
µ ν
In particular, δ(µ,ν) = 0 if and only if µ and ν have the same mean.
This is of course not sufficient to say that the two measures are the same
so the above quantity does not define a distance between probability
measures like the Wasserstein distance. To do so, we need to find a
class of test functions F that is large enough to yield a distance but
not as massive as 1-Lipschitz functions so as to escape the curse of
dimensionality.2.8 Regularization of Wasserstein distances 73
Definition 2.19.A metric d(·,·) between two probability measures is
called an integral probability metric (IPM) if it satisfies the properties
of a metric and can be written in the form
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
d(µ,ν) = sup(cid:12) fdµ− fdν(cid:12) . (2.18)
f∈F(cid:12) (cid:12)
Note that both the 1-Wasserstein distance W and the quantity δ
1
above are of the form (2.18) with F = Lip and F = F respectively.
1 lin
Indeed, the absolute value in (2.18) is implicit when F is symmetric:
F = −F. However, while W is an IPM, the quantity δ is not because it
1
fails to satisfy the properties of a metric; here: definiteness.
Another example of a choice for F is the set of bounded Lipschitz
functions which indeed yields an IPM, but the size of this class is
the same as Lip for the matter at hand here. To improve the sample
1
complexity, we need much smoother functions.
2.8.2 Maximum mean discrepancy
ReproducingKernelHilbertSpaces(RKHS)formaflexibleandpractical
class of functions. To define this class of functions very briefly we
introduce some basic definitions and key properties. We refer the reader
to [MFSS17] for more details on kernel methods that are particularly
relevant to this section.
Consider a reproducing kernel Hilbert space H of functions Rd → R
associated to a bounded positive definite kernel k on Rd. Denote by
⟨·,·⟩H and ∥·∥H the inner product and norm on H respectively. The
reproducing property of the RKHS H ensures that for any f ∈ H,
⟨k(x,·),f⟩H = f(x).
In particular taking f = k(y,·) yields
⟨k(x,·),k(y,·)⟩H = k(x,y).
We are now in a position to define the Maximum Mean Discrepancy.
Definition 2.20.LetH beanRKHS.TheMaximumMeanDiscrepancy
(MMD) between two probability measures µ and ν on Rd is defined to
be the quantity
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
MMD(µ,ν) = sup (cid:12) fdµ− fdν(cid:12) .
f∈H (cid:12) (cid:12)
∥f∥H≤174 2 Estimation of Wasserstein distances
Without further assumptions on the RHKS, MMD need not define a
distance between probability measures. Indeed, observe that the set of
linear functions on Rd equipped with the Euclidean inner product is in
fact an RKHS associated to the linear kernel k(x,y) = ⟨x,y⟩. Moreover,
if f(x) = ⟨θ,x⟩, then
∥f∥2 = ∥⟨θ,·⟩∥2 = ∥k(θ,·)∥2 = k(θ,θ) = ∥θ∥2.
H H H
Hence, the unit ball of H is no other than F and we have shown that
lin
this class is not rich enough to define an IPM.
In fact, we have not addressed whether MMD is finite. To that end,
we use the following useful proposition.
Proposition 2.21.Let H be an RKHS. The Maximum Mean Discrep-
ancy (MMD) between two probability measures µ and ν on Rd can be
equivalently defined as
(cid:13)(cid:90) (cid:90) (cid:13)
(cid:13) (cid:13)
MMD(µ,ν) = (cid:13) k(x,·)µ(dx)− k(x,·)ν(dx)(cid:13) .
(cid:13) (cid:13)
H
Proof. For any f ∈ H it holds
(cid:90) (cid:90)
f(x)(µ−ν)(dx) = ⟨k(x,·),f⟩H(µ−ν)(dx)
(cid:68)(cid:90) (cid:69)
= k(x,·)(µ−ν)(dx),f .
H
Hence, the claim follows from Cauchy–Schwarz. ⊔⊓
As a corollary of Proposition 2.21, we get that
(cid:90)(cid:90) (cid:90)(cid:90)
MMD2(µ,ν) = k(x,y)µ(dx)µ(dy)+ k(x,y)ν(dx)ν(dy)
(cid:90)(cid:90)
−2 k(x,y)µ(dx)ν(dy) (2.19)
and
(cid:90) (cid:90)
MMD(µ,ν) ≤ ∥k(x,·)∥Hµ(dx)+ ∥k(x,·)∥Hν(dx)
(cid:112)
≤ 2 sup k(x,x) < ∞
x∈Rd
where we used the fact that k is bounded.2.8 Regularization of Wasserstein distances 75
The map µ (cid:55)→ (cid:82) k(x,·)µ(dx) which embeds µ onto the RKHS H
is called kernel mean embedding. It follows from Proposition 2.21 that
MMD is an IPM, meaning that it is indeed a metric, if and only
if the kernel mean embedding is injective. Kernels that ensure this
property are called characteristic and one such example is the Gaussian
kernel k(x,y) =
e−∥x 2− σy 2∥2
. To see this, observe that, up to normalizing
constants, the kernel mean embedding is a convolution of µ with a
Gaussian measure: for any y ∈ Rd, it holds
(cid:90)
k(x,y)µ(dx) =
(cid:90) e−∥x 2− σy 2∥2
µ(dx) =
(σ√
2π)d(µ⋆N(0,σ2I))(y).
Injectivity of the convolution with a Gaussian distribution can be seen
readily using characteristic functions. Indeed, the characteristic function
of µ⋆N(0,σ2I) is given by
ϕ µ⋆N(0,σ2I)(·) = ϕ µ(·)ϕ N(0,σ2I)(·) = ϕ
µ(·)e−σ2∥ 2·∥2
.
Hence, since the characteristic function
e−σ2∥ 2·∥2
of the Gaussian is
everywhere positive, we get that
µ⋆N(0,σ2I) = ν ⋆N(0,σ2I)
if and only if µ = ν.
Clearly, the above argument generalizes to translation-invariant
kernels that are of the form k(x,y) = K(x − y) for some bounded
positive definite function K : Rd → R and whose Fourier transform
is everywhere positive5. This includes for example the Laplace kernel
k(x,y) = e−∥x−y∥ as well as other examples; see [MFSS17, Table 3.1].
The representation of MMD given by (2.19) gives an easy way to
estimate MMD from data. For example, assume that X ,...,X are
1 m
i.i.d. from µ and Y ,...,Y are i.i.d. from µ. Denote by µ and ν the
1 n m n
corresponding empirical distributions. Then,
m n
1 (cid:88) 1 (cid:88)
MMD2(µ ,ν ) = k(X ,X )+ k(Y ,Y )
m n m2 i i′ n2 j j′
i,i′=1 j,j′=1
m n
2 (cid:88)(cid:88)
− k(X ,Y ).
i j
mn
i=1 j=1
5 Note that Bochner’s theorem implies that positive definite kernels have a non-
negative Fourier transform, so this is a stronger requirement.76 2 Estimation of Wasserstein distances
AnaturalquestioniswhetherthisgivesagoodestimatorofMMD2(µ,ν).
Using the triangle inequality, it is sufficient to control MMD(µ ,µ).
m
While MMD is an IPM, the closed-form representation of Proposi-
tion 2.21 allows us to bypass the use of empirical process theory to
control this quantity.
Theorem 2.22.Let k be a characteristic kernel such that k(x,x) ≤ 1
for any x ∈ Rd. Let X ,...,X be n i.i.d. observations from a distribu-
1 n
tion µ on Rd and define the empirical measure
n
1 (cid:88)
µ = δ .
n
n
Xi
i=1
Then
1
E [MMD(µ ,µ)] ≤ √ .
µ n
n
Proof. It follows from Proposition 2.21 that
n
(cid:13)1 (cid:88) (cid:13)2
E[MMD2(µ ,µ)] = E(cid:13) {k(X ,·)−Ek(X ,·)}(cid:13)
n (cid:13)n i i (cid:13)
H
i=1
1
= E∥k(X ,·)−Ek(X ,·)∥2
1 1 H
n
= 1 (cid:0)E∥k(X ,·)∥2 −∥Ek(X ,·)∥2 (cid:1)
1 H 1 H
n
1
≤ E∥k(X ,·)∥2 .
1 H
n
Next, observe that
E∥k(X ,·)∥2 = E[k(X ,X )] ≤ 1.
1 H 1 1
The claim follows from Jensen’s inequality. ⊔⊓
We see that unlike Wasserstein distances, MMD does not suffer from
the curse of dimensionality. This is certainly a desirable feature, but it
may also be interpreted from a more cautious perspective. Indeed, while
MMD does define a metric, it is less sensitive to deviations between
probability measures and tends to make them small. This is why µ ,
n
which according to the 1-Wasserstein distance is quite far from µ,
appears to be quite close to µ from the perspective of MMD.2.8 Regularization of Wasserstein distances 77
2.8.3 Smoothed Wasserstein distances
We see from Proposition 2.21 that when k is the Gaussian kernel,
MMD(µ,ν) is a Hilbert space norm involving the densities µ⋆N(0,σ2I )
d
and ν ⋆N(0,σ2I ). We could very well measure this distance between
d
probability measures using other distances, in particular, using Wasser-
stein distances.
Definition 2.23.Fix p ≥ 1. The smoothed p-Wasserstein distance
between two probability measures µ,ν ∈ P (Rd) is defined by
p
W(σ)(µ,ν) := W (µ⋆N(0,σ2I),ν ⋆N(0,σ2I)).
p p
It follows readily from this definition that the smoothed Wasserstein
distance is indeed a distance. Compared to MMD, which embeds dis-
tributions in a Hilbert space, the geometry induced on distributions
by the smoothed Wasserstein distance is much closer to the original
Wasserstein distance. Like MMD, however, smoothed Wasserstein dis-
tances enjoy faster statistical rates of convergence. For simplicity, we
focus here on the case p = 1, but parametric rates have been established
for p = 2 as well.
Theorem 2.24.Fix σ > 0. Let X ,...,X be n i.i.d. observations
1 n
from a distribution µ on [−1,1]d and define the empirical measure
n
1 (cid:88)
µ = δ .
n
n
Xi
i=1
Then
1
E [W(σ) (µ ,µ)] ≲ √ ,
µ 1 n n
where the implicit constant depends on both σ2 and d.
Before turning to the proof, we note that the constant factor in this
bound scales exponentially in the dimension. This poor scaling in d is,
in fact, unavoidable and reflects the fundamental statistical difficulty of
estimating the Wasserstein distance.
Proof. Denote by f the density of µ⋆N(0,σ2I) and by f the density of
n
µ ⋆N(0,σ2I). Write φ(z) := (2πσ2)−d/2exp(− 1 ∥z∥2) for the density
n 2σ2
of N(0,σ2I). Theorem 1.6 implies78 2 Estimation of Wasserstein distances
(cid:90)
EW(σ) (µ ,µ) ≤ E ∥z∥|f (z)−f(z)|dz
1 n n
(cid:90) (cid:12)1 (cid:88)n (cid:12)
= ∥z∥E(cid:12) φ(z−X )−Eφ(z−X )(cid:12)dz
(cid:12)n i i (cid:12)
i=1
(cid:90)
≤ √1 ∥z∥(cid:0)E(φ(z−X )−Eφ(z−X ))2(cid:1)1/2 dz
1 1
n
(cid:90)
1
≤ √ ∥z∥(Eφ(z−X )2)1/2dz.
1
n
It suffices to show that the integral is bounded. If z ∈ [−2,2]d, then we
can use the crude bound (Eφ(z−X )2)1/2 ≤ (2πσ2)−d/2. If z ∈/ [0,2]d,
1
then ∥z−X ∥ ≥ ∥z/2∥ almost surely, which yields (Eφ(z−X )2)1/2 ≤
1 1
φ(z/2). We obtain
(2πσ2)−d/2 (cid:90) 1 (cid:90)
EW(σ)
(µ ,µ) ≤ √ ∥z∥dz+ √ ∥z∥φ(z/2)dz
1 n n n
[−2,2]d
≤
(cid:0) (2πσ2)−d/2+σ(cid:1) 2d+1(cid:112)
d/n
≲ n−1/2,
as claimed. ⊔⊓
2.8.4 Sliced Wasserstein distances
Finally, we close this chapter with yet another method to avoid the
curse of dimensionality, this time based on considering the Wasserstein
distance between one-dimensional projections.
Formally, let Sd−1 denote the unit sphere in Rd and for θ ∈ Sd−1
let Πθ : Rd → R be the projection Πθ(x) := ⟨θ,x⟩. Define the sliced
Wasserstein distance between µ,ν ∈ P (Rd) to be the quantity
p
(cid:16)(cid:90) (cid:17)1/p
SW (µ,ν) := Wp(Πθµ,Πθν)σ(dθ) , (2.20)
p p # #
where σ is the uniform measure on Sd−1.
The idea of considering one-dimensional projections is rooted in
applications to imaging and tomography, for which various integral
transforms have been introduced. In particular, the Radon transform of
ameasureµisdefinedtobethecollectionofone-dimensionalprojections
(Πθµ) . It is a classical fact, known as the Cram´er–Wold theorem,
# θ∈Sd−1
that the Radon transform of µ completely characterizes µ, justifying2.8 Regularization of Wasserstein distances 79
its use in defining a metric over probability measures. Let us start by
checking that the axioms of a metric space are indeed satisfied.
Theorem 2.25.For every p ≥ 1, SW defines a metric over P (Rd).
p p
Proof. Symmetry and non-negativity follow from the corresponding
facts about the Wasserstein distance. For Θ ∼ σ, the triangle inequality
is verified via
SW (µ,ν) =
(cid:0)EWp(ΠΘµ,ΠΘν)(cid:1)1/p
p p # #
≤
(cid:0)EWp(ΠΘµ,ΠΘρ)(cid:1)1/p +(cid:0)EWp(ΠΘρ,ΠΘν)(cid:1)1/p
p # # p # #
= SW (µ,ρ)+SW (ρ,ν).
p p
Finally, we must check that SW (µ,ν) = 0 implies µ = ν. Certainly,
p
SW (µ,ν) = 0 implies that W (Πθµ,Πθν) = 0 for almost every θ ∈
p p # #
Sd−1, which implies Πθµ = Πθν. To finish, we would like to upgrade
# #
“almost every” to “every” to apply the Cram´er–Wold device.
To do so, we prove a Lipschitz continuity property of the mapping
θ (cid:55)→ Πθµ. For θ′ ∈ Sd−1 and X ∼ µ,
#
W (Πθµ,Πθ′ µ) ≤ (cid:0)E[|⟨θ−θ′,X⟩|p](cid:1)1/p ≤ (E∥X∥p)1/p∥θ−θ′∥.
p # #
Together with the W triangle inequality, it shows that
p
|W (Πθµ,Πθν)−W (Πθ′ µ,Πθ′ ν)|
p # # p # #
≤ W (Πθµ,Πθ′ µ)+W (Πθν,Πθ′ ν) ≲ ∥θ−θ′∥.
p # # p # #
Therefore, θ (cid:55)→ W (Πθµ,Πθν) is continuous, and SW (µ,ν) = 0 im-
p # # p
plies that this quantity vanishes for every θ ∈ Sd−1.6 ⊔⊓
We can now prove that the sliced Wasserstein distance can be
estimated at a parametric rate.
Proposition 2.26.Suppose that µ,ν ∈ P(B ), where B is the unit
1 1
ball in Rd, and let µ , ν denote the corresponding empirical measures
n n
formed from i.i.d. samples X ,...,X ∼ µ and Y ,...,Y ∼ ν. Then,
1 n 1 n
ESW (µ ,µ) ≲ n−1/2.
1 n
6 An alternative argument proceeds as follows: Πθµ = Πθν for almost every θ
# #
implies that the characteristic functions of µ, ν are equal almost everywhere. But
characteristic functions are uniformly continuous.80 2 Estimation of Wasserstein distances
Also,
E|SW (µ ,ν )−SW (µ,ν)| ≲ n−1/2.
1 n n 1
Proof. The second inequality follows from the first by the triangle
inequality. To establish the first, we can note that ⟨θ,X ⟩,...,⟨θ,X ⟩
1 n
is an i.i.d. sample from Πθµ, and Πθµ is the corresponding empirical
# # n
measure. Hence, from the one-dimensional rate in Proposition 2.5,
EW (Πθµ ,Πθµ) ≲ n−1/2. Then, average over θ. ⊔⊓
1 # n #
Although we have motivated the sliced Wasserstein distance for its
statistical benefits, fortuitously it also comes with substantial computa-
tional ones. Indeed, computation of SW boils down to one-dimensional
p
optimal transport, which for discrete measures can be solved via sorting;
see Exercise 5.
2.9 Discussion
§2.1. The Wasserstein law of large numbers is discussed in more detail
in [Dud02, Chapter 11]. The slow rate of convergence is a manifestation
of the fact that W convergence automatically implies convergence of
1
all Lipschitz text functions (and, for p > 1, convergence of all higher
moments, as Proposition 1.5 shows); it is therefore not surprising that
a large number of samples is needed to obtain such strong control.
§2.2. The usefulness of the dyadic partitioning argument for controlling
the Wasserstein distances was first highlighted by [BLG14]; see [FG15]
for an extension to the unbounded setting. A further discussion of the
history of this approach appears in [NWB19], from which this version
of the argument was taken. This argument can easily be extended to
show that the rate of convergence depends on the intrinsic dimension
of the measure µ rather than the ambient dimension.
The dyadic partitioning argument also applies to the p > 1 case, and
shows that

n−1/2p, if d < 2p,
√  
EW (µ ,µ) ≲ d· (logn)1/p/n1/2p, if d = 2p, (2.21)
p n p

n−1/d, if d > 2p.
This rate is essentially sharp, apart from the logarithmic factor in the
d = 2p case. On the other hand, the dual bounds we present in this2.9 Discussion 81
chapter do not easily extend to p > 1 since the dual formulation of
W (µ ,µ) does not give rise to an empirical process when p > 1.
p n
ThetriangleinequalityimpliesthatratesofconvergenceofW (µ ,ν)
p n
to W (µ,ν) can be derived from the corresponding rates of convergence
p
of W (µ ,µ); however, these rates can fail to be sharp. To give one
p n
example, [CRL+20] showed that

n−1/2, if d < 4,


E|W2(µ,ν )−W2(µ,ν)| ≲ (logn)/n1/2, if d = 4, (2.22)
2 n 2

n−2/d, if d > 4.
Note that when µ ̸= ν, this bound is stronger than what could be
deduced from (2.21). A similar phenomenon exists for other W dis-
p
tances [MNW24].
More strikingly, the rate at which W (µ ,ν) converges to W (µ,ν)
p n p
can be shown to depend on the smaller of the intrinsic dimensions of µ
and ν; see [HSM24]. In particular, if ν is supported on a finite number
of points (sometimes known as the semi-discrete optimal transport
problem), then W (µ ,ν) converges to W (µ,ν) at a rate that does not
p n p
suffer from the curse of dimensionality. This fact cannot be deduced
from bounds on W (µ ,µ) alone.
p n
§2.3.ChainingisanideathatgoesbackimplicitlytoKolmogorov.Inthe
formofProposition2.6,itisknownasDudley’sentropyintegral[Dud67].
For some of the many references on chaining and its applications,
see [vdVW96, Dud99, vH14, Ver18, Wai19, Tal21].
§2.4. As mentioned, the result of this section is due to [AKT84] and
the argument here is taken from [BL21]. More broadly, there is a large
literature on so-called matching problems, e.g., [Led17, LZ21, Tal21],
and recently techniques from partial differential equations have been
used to derive very sophisticated results when d = 2, see, e.g., [AST19].
§2.5. Applications of Wasserstein distances to testing can be found
in [dBCAMRR99, HMS21, GDGSCN23, NWKB23].
§2.6.ThelowerboundinTheorem2.14isdueto[Dud69].Theminimax
lowerboundinTheorem2.15wasfirstprovedby[SP18].Forexpositions
of minimax lower bound techniques, see [Tsy09, RH17, Wai19].
§2.7. Minimax estimation of smooth densities in the Wasserstein dis-
tance was studied in [SUL+18, Lia21] for W , and in [NWB22] for W ,
1 p
p > 1. The case of p > 1 evinces different behavior from the p = 1
case: [NWB22] showed that the rate in Theorem 2.18 is achievable for
p > 1 only under the additional assumption that the density of µ is82 2 Estimation of Wasserstein distances
bounded below; without this assumption, rates of estimation are strictly
worse. The results for the p > 1 case are confined to densities lying in
Besov classes; extending the arguments of [NWB22] to other classes of
densities is an open question. A version of this problem on manifolds
has been studied in [Div22].
Non-parametric density estimation is itself a classical topic in statis-
tics, albeit usually studied in other distance metrics [Tsy09].
§2.8. It is worth mentioning that IPMs have received significant atten-
tion in the context of Generative Adversarial Networks (GANs), and
in particular, Wasserstein GANs [ACB17] where F is chosen to be a
family of deep neural networks. For statistical analyses of IPMs and
GANs, see, e.g. [USP19, Lia21].
Maximum mean discrepancy was first developed in [BGR+06], and
is now the subject of a large literature, see, [MFSS17]. The rate given
in Theorem 2.22 is folklore. A notable special case of MMD is the class
of energy distances, for which we recommend [Ger24, Subsection 1.2.4]
for an introduction and references.
The favorable statistical properties of the smoothed Wasserstein
distances were first recorded in [GGNWP20]. The simple proof of The-
orem 2.24 is taken from [NW18].
Sliced Wasserstein distances were introduced in [RPDB12]. They
aroseasadevicetounderstandthe“iterativedistributiontransfer”algo-
rithm [PKD07]; see the PhD thesis of Bonnotte [Bon13] for history. For
further discussion, consult [San15, Section 5.5.4], and see [NDC+20] for
generalizations with similarmetricand statistical properties. Extensions
ofProposition2.26appearin[MBW22].Thesharpconditionforthefast
rate of estimation of sliced Wasserstein distances to hold was obtained
in [BL19]. Other results in this vein, including distributional limits, can
be found in [MBW22, NWR22, OI22, XNW22, XH22, PS23, GKRS24].
2.10 Exercises
√
1. This exercise shows that the d factor in Proposition 2.1 cannot be
improved.
a) Show that there exists a positive universal constant c such that
for any n ≥ 1, the Lebesgue measure of a ball in Rd with radius
√
c dn−1/d is at most (2n)−1. (Hint: recall that the unit ball in
Rd has Lebesgue measure πd/2/Γ(d +1).)
22.10 Exercises 83
b) Let µ be the uniform measure on [0,1]d, and let µ˜ be any mea-
n
sure supported on n points x ,...,x . If we denote by B(x ,ϵ)
1 n i
a ball of radius ϵ around x , show that µ((cid:83)n B(x ,ϵ)) ≤ 1 if
√ i i=1 i 2
ϵ = c dn−1/d, where c is the constant from part (a).
c) Conclude that if γ ∈ Γ(µ,µ˜ ), then (cid:82) ∥x − y∥dγ(x,y) ≥ 1 ·
√ n 2
c dn−1/d.
2. Adapt the proof of Proposition 2.5 to establish (2.21).
3. Recall the bounded differences inequality (e.g., [BLM13, Theorem
6.2]).UseittoproveaconcentrationinequalityforW (µ ,µ)around
1 n
its expectation, where X ,...,X are i.i.d. from a distribution µ
1 n
supported on a ball of radius R and µ is the empirical measure
n
µ = 1 (cid:80)n δ .
n n i=1 Xi
4. Prove that SW ≤ W for any p ≥ 1. Is this inequality tight?
p p
(σ)
Similarly, show that W ≤ W . Is this inequality tight?
1 1
5. We consider the computational aspects of the sliced Wasserstein
distance, defined in Subsection 2.8.4.
a) Show that if µ,ν ∈ P (R) (p ≥ 1), and µ and ν are each uni-
p
formly distributed on n points, then W (µ,ν) can be computed
p
in O(nlogn) time (where we treat arithmetic and comparison
operations as constant time). Assume that the measures µ, ν
are given as (unordered) lists of points.
b) Now suppose that µ,ν ∈ P (Rd) are each uniformly distributed
p
on n points, presented as lists of points in Rd. Argue that we
can compute W (Πθµ,Πθν) in O(dn+nlogn) time.
p # #
c) This is still insufficient for algorithmic purposes, since comput-
ing SW (µ,ν) exactly requires computing W (Πθµ,Πθν) for
1 1 # #
uncountably many values of θ and integrating. Argue instead
that if µ,ν ∈ P(B ) and we draw m i.i.d. points θ ,...,θ from
1 1 m
the uniform measure σ on Sd−1, then the Monte Carlo average
m
1 (cid:88)
S(cid:100)W 1(µ,ν) :=
m
W 1(Π #θiµ,Π #θiν)
i=1
approximates SW (µ,ν) to an additive error of size O(m−1/2).
1
6. Theorem 1.7 implies that if µ ∈ P (R), then
1
(cid:90) ∞
W (µ ,µ) = |F (t)−F (t)|dt,
1 n µn µ
−∞84 2 Estimation of Wasserstein distances
where F and F are the cumulative distribution functions of µ
µn µ n
and µ, respectively. Use this fact to show Proposition 2.1 for d = 1
directly. (Hint: E|F (t)−F (t)|2 = F (t)(1−F (t)).)
µn µ µ µ
7. The minimax lower bound proved in Theorem 2.15 is suboptimal
when d = 1. This exercise proves an optimal bound based on testing
between two hypotheses (Theorem 2.2 in [Tsy09]). To use this
approach, it suffices to construct two measures µ(0) and µ(1) with
support in [0,1] such that
W (µ(0),µ(1)) ≥ 2r ,
1 n
KL(µ(1)∥µ(0)) ≤ 1 .
2n
The existence of such measures implies that for any estimator µ˜
n
based on n i.i.d. observations, there exists j ∈ {0,1} such that
E [W (µ˜ ,µ(j))] ≥ rn.
µ(j) 1 n 4
a) Fix ε ∈ (0,1/2), and consider µ(0) = (1 +ε)δ +(1 −ε)δ and
2 0 2 1
µ(1) = (1 −ε)δ +(1 +ε)δ . Show that W (µ(0),µ(1)) = 2ε.
2 0 2 1 1
b) Show that this pair of measures satisfies KL(µ(1),µ(0)) ≤ 16ε2 .
1−4ε2
c) Conclude that there exists a positive universal constant c such
that for any estimator µ˜ there exists j ∈ {0,1} such that
n
E [W (µ˜ ,µ(j))] ≥ cn−1/2.
µ(j) 1 n
8. The regularization strategies discussed in Section 2.8 can be applied
more broadly. For example, given probability measures µ, ν, define
the following quantity and call it “smoothed L ”:
2
(cid:90)
d2(µ,ν) := (cid:0) µ⋆N(0,σ2I)−ν ⋆N(0,σ2I)(cid:1)2 .
σ
Show that it can be estimated at a parametric rate: if µ denotes
n
the empirical measure formed from n i.i.d. samples from µ, then
1
Ed2(µ ,µ) ≤ .
σ n (2πσ2)d/2n
9. For µ,ν ∈ P (Rd), the “max-sliced Wasserstein distance” [DHS+19,
p
KNS+19]7 between them is
MSW (µ,ν) := maxW (Πθµ,Πθν).
p p # #
θ∈S
The goal of this exercise is to show that MSW can be estimated at
1
the parametric rate.
7 Also known as the “low-dimensional Wasserstein distance” [NWR22] or the “sub-
space robust Wasserstein distance” [PC19a].2.10 Exercises 85
a) Let µ ∈ P(B ), and let µ be the empirical measure correspond-
1 n
i.i.d.
ing to X ,...,X ∼ µ. Show that
1 n
n
1 (cid:88)
MSW (µ ,µ) = sup {f(X )−Ef(X )},
1 n i i
f∈F n
i=1
where F is the class of functions of the form x (cid:55)→ h(θTx) where
θ ∈ Sd−1 and h is a 1-Lipschitz function on [−1,1] satisfying
h(0) = 0.
b) Prove that
logN(ε,F) ≲ 1/ε+dlog(1+1/ε).
Hint: Consult Exercise 2 in Chapter 3.
c) Using Proposition 2.6, conclude that
(cid:112)
EMSW (µ ,µ) ≲ d/n.
1 n3
Estimation of transport maps
Thus far, the statistical questions we have investigated center around
the estimation of optimal transport distances (and their variants), but
the gamut of diverse applications of optimal transport (to name but a
few: data fusion [CFTC16] adaptation/transfer learning [CFTR17], and
computational biology [SST+19, BSG+23]), it is the optimal transport
map which is the object of primary interest. In this chapter, we address
thequestionofestimatingthismaponthebasisoffinitelymanysamples.
3.1 Problem formulation
Recall from Brenier’s theorem that if µ has a density,
(cid:90)
W2(µ,ν) = min ∥x−y∥2γ(dx,dy)
2
γ∈Γµ,ν
(cid:90)
= min ∥x−T(x)∥2µ(dx).
T:T µ=ν
#
Moreover, the optimal transport map takes the form T = ∇φ, where φ
is convex. We can also write this as (X,∇φ(X)) ∼ γ, or γ(dx,dy) =
µ(dx)δ (dy).
T(x)
The statistical question under investigation is formulated as follows.
i.i.d. i.i.d.
Given samples X ,...,X ∼ µ and Y ,...,Y ∼ ν, how can we
1 n 1 n
estimate the optimal transport map T from µ to ν via an estimator T(cid:98)
constructed on the basis of the samples?
We take as our measure of performance the integrated error
(cid:90)
∥T(cid:98)(x)−T(x)∥2µ(dx).88 3 Estimation of transport maps
TheL2integratederrorisanaturalmeasureofdistancethatiscommonly
employed in non-parametric statistics. In this context, however, it takes
on an additional interpretation of controlling the Wasserstein distance
between the pushforwards of µ under the two maps. More precisely, by
definition we have ν = T #µ. If we define the measure ν
(cid:98)
via ν
(cid:98)
= T(cid:98)#µ,
then (T(cid:98)(X),T(X)) for X ∼ µ is a (suboptimal) coupling of ν
(cid:98)
and ν,
and hence
(cid:90)
W 22(ν (cid:98),ν) ≤ E∥T(cid:98)(X)−T(X)∥2 = ∥T(cid:98)(x)−T(x)∥2µ(dx).
See Figure 3.1 for an illustration.
ν (cid:98)=T(cid:98)#µ
T(cid:98)
T
µ ν
Fig.3.1.TheL2errorbetweenthetransportmapscontrolstheW2distancebetween
2
ν and ν.
(cid:98)
A first approach to estimation might be to compute the opti-
mal coupling between the empirical measures µ and ν , i.e., solve
n n
min (cid:82) ∥x−y∥2γ(dx,dy), but we rapidly recognize an untenable
γ∈Γµn,νn
hole in this na¨ıve plan. Namely, even if the optimal transport plan γ is
n
induced by a transport map T , so that γ (dx,dy) = µ (dx)δ (dy),
n n n Tn(x)
the mapping T is only well-defined on the sample {X ,...,X } and
n 1 n
it is not clear how to extend it in a principled manner to a mapping
over all of Rd (Figure 3.2). To remedy this, several approaches have
been proposed in the literature aimed at building an interpolation T(cid:98)n
of T
n
to out-of-sample points. For example, we can take T(cid:98)n(x) to equal
T (X ), where X is the closest sample point to x. This is a 1-nearest
n i i
neighbor estimator and it can be shown to be minimax optimal without
further smoothness assumptions [MBNWW21]; see Exercise 1 for the
one-dimensional case. Such an approach, however, cannot take advan-
tage of additional regularity of µ and ν and we do not pursue it any
further here.
Instead, in the next section, we devise an estimator based on the
semidual formulation of optimal transport. A benefit of this estima-
tion strategy is that it can be used to flexibly incorporate additional3.2 The semidual problem and its stability 89
?
x
Fig. 3.2. How do we interpolate the empirical optimal transport map at the out-of-
sample point x?
assumptions—e.g., smoothness—on the population-level transport map
T. Adopting sufficiently strong assumptions gives rise to map estimators
that avoid the curse of dimensionality.
3.2 The semidual problem and its stability
We recall the semidual problem: if ∇φ is the optimal transport map,
then φ solves
(cid:90) (cid:90)
minS(ϕ) := ϕdµ+ ϕ∗dν
ϕ
where ϕ∗(y) = sup {⟨x,y⟩−ϕ∗(x)} is the convex conjugate of ϕ.
x∈Rd
Crucially, the semidual problem readily lends itself to replacing the
population measures µ, ν with their empirical counterparts µ , ν ,
n n
leading to a natural estimator for φ: namely, we set
(cid:110)(cid:90) (cid:90) (cid:111)
φ = argminS (ϕ) := argmin ϕdµ + ϕ∗dν (3.1)
(cid:98) n n n
ϕ∈F ϕ∈F
where F is a suitable class of functions to be chosen later. We then
obtain an estimator for the optimal transport map by setting T(cid:98) = ∇φ (cid:98).
Through (3.1), we have placed the problem of transport map estimation
within the well-studied framework of empirical risk minimization (ERM)
whichisacornerstoneofmodernstatisticaltheory—see,e.g.,[Wai19]for
a modern overview of these techniques. Akin to many other estimators
definedviaERM,itisunclearwhethertheestimatorT(cid:98)canbecomputed
efficiently; however, our focus here is on the statistical, rather than
computational, aspects of transport map estimation.
Through the statistician’s lens, the uniqueness assertion in Brenier’s
theorem ensures that the optimal transport map T is identifiable, and90 3 Estimation of transport maps
henceourstatisticalquestioniswell-posed.Inotherwords,ifS(ϕ) = S(φ)
then ∇ϕ = ∇φ, µ-a.s. However, in order to obtain rates of estimation,
thisqualitativeassertionneedstobeupgradedintoastabilitystatement,
which is given as the following theorem.
Theorem 3.1.Assume that ϕ is strongly convex and smooth,
1
I ⪯ ∇2ϕ ⪯ 2I.
2
Then,
1
∥∇ϕ−∇φ∥2 ≤ S(ϕ)−S(φ) ≤ ∥∇ϕ−∇φ∥2 . (3.2)
4 L2(µ) L2(µ)
Before proving Theorem 3.1, we first describe how the stability result
feeds into the overall statistical analysis. The proof is prototypical of
analysis of ERM estimators. By definition, φ minimizes S . Applying
(cid:98) n
the machinery of empirical process theory, we control the fluctuations of
the random functional S from its mean S, thereby concluding that S(φ)
n (cid:98)
is small. The first inequality in (3.2) then implies that the estimation
error ∥T(cid:98)−T∥2 is small.
L2(µ)
Actually, to obtain faster rates of estimation, we improve upon
this argument by incorporating another ingredient: the fixed-point or
localization technique. Briefly, the estimation rates depend on a uniform
boundonthedeviationsofS fromSoverasetoffunctionsthatcontains
n
the estimator φ. Once we know through the stability inequality (3.2)
(cid:98)
that φ lies close to φ, we can repeat the argument but restricting to
(cid:98)
a smaller class of functions, thereby improving our estimation rates
further. Seeking the fixed point of this iterative process in which we
refine our bounds by localizing the estimator φ, we arrive at our final
(cid:98)
rates of estimation.
We now turn towards the proof of Theorem 3.1. We repeatedly use
the Fenchel–Young inequality (Theorem A.6), as well as the fact that
α-convexity of f is equivalent to α−1-smoothness of f∗ (Lemma A.9).
Proof of Theorem 3.1. Since (∇φ) µ = ν,
#
(cid:90) (cid:90) (cid:90)
S(ϕ) = ϕ(x)µ(dx)+ ϕ∗(y)ν(dy) = (cid:0) ϕ(x)+ϕ∗(∇φ(x))(cid:1) µ(dx).
By strong convexity of ϕ∗,3.2 The semidual problem and its stability 91
ϕ∗(∇φ(x)) ≥ ϕ∗(∇ϕ(x))+⟨∇ϕ∗(∇ϕ(x)),∇φ(x)−∇ϕ(x)⟩
(cid:124) (cid:123)(cid:122) (cid:125)
=x
1
+ ∥∇φ(x)−∇ϕ(x)∥2
4
hence
ϕ(x)+ϕ∗(∇φ(x)) ≥ ϕ(x)+ϕ∗(∇ϕ(x))+⟨x,∇φ(x)−∇ϕ(x)⟩
(cid:124) (cid:123)(cid:122) (cid:125)
=⟨x,∇ϕ(x)⟩
1
+ ∥∇φ(x)−∇ϕ(x)∥2
4
1
= ⟨x,∇φ(x)⟩+ ∥∇φ(x)−∇ϕ(x)∥2.
4
However,
(cid:90) (cid:90)
S(φ) = (cid:0) φ(x)+φ∗(∇φ(x))(cid:1) µ(dx) = ⟨x,∇φ(x)⟩µ(dx).
Therefore, we obtain
1
S(ϕ) ≥ S(φ)+ ∥∇φ−∇ϕ∥2 .
4 L2(µ)
Similarly, by smoothness,
ϕ∗(∇φ(x)) ≤ ϕ∗(∇ϕ(x))+⟨∇ϕ∗(∇ϕ(x)),∇φ(x)−∇ϕ(x)⟩
(cid:124) (cid:123)(cid:122) (cid:125)
=x
+∥∇φ(x)−∇ϕ(x)∥2
hence
ϕ(x)+ϕ∗(∇φ(x)) ≤ ϕ(x)+ϕ∗(∇ϕ(x))+⟨x,∇φ(x)⟩−⟨x,∇ϕ(x)⟩
+∥∇φ(x)−∇ϕ(x)∥2
= ⟨x,∇ϕ(x)⟩+φ(x)+φ∗(∇φ(x))−⟨x,∇ϕ(x)⟩
+∥∇φ(x)−∇ϕ(x)∥2
and the result follows from integration. ⊔⊓
Note that we have proved something even stronger: for
(cid:90)
S(ϕ) = (cid:0) ϕ(x)+ϕ∗(∇φ(x))(cid:1) µ(dx)
(cid:124) (cid:123)(cid:122) (cid:125)
s (x)
ϕ
we have the pointwise bounds
1
∥∇φ(x)−∇ϕ(x)∥2 ≤ s (x)−s (x) ≤ ∥∇φ(x)−∇ϕ(x)∥2.
ϕ φ
492 3 Estimation of transport maps
3.3 A special case: affine transport maps
As a sanity check, we first show that the semidual estimation technique
isreasonableforaverysimpleproblem.Considertheone-samplesetting,
where µ = N(0,I) is known, and we obtain samples from ν = (∇φ) µ
#
for some φ ∈ F, where F consists of all convex quadratic functions
x (cid:55)→ 1 xTAx+bTx with A ⪰ 0. If φ is of this form, then the transport
2
map ∇φ is the affine map Ax+b, and ν = N(b,A2).
In this setting, it is natural to estimate the transport map by first
computing the empirical mean m
(cid:98)
and covariance Σ(cid:98) of ν, and setting
T(cid:98)(x) = Σ(cid:98)1/2x+m
(cid:98)
. (3.3)
This estimator is studied in more generality in [FLF20] by leveraging
techniques to derive rates of estimation for covariance matrices.
ThenextresultshowsthatT(cid:98)definedin(3.3)ispreciselytheestimator
computed by minimizing the empirical semidual functional.
Proposition 3.2.Let F be the set of all convex quadratic functions on
Rd. Let µ = N(0,I), and write ν for an empirical measure consisting
n
of i.i.d. samples from a probability measure ν. If
(cid:26)(cid:90) (cid:90) (cid:27)
φ = argmin ϕdµ+ ϕ∗dν ,
(cid:98) n
ϕ∈F
then
∇φ (cid:98)(x) = Σ(cid:98)1/2x+m
(cid:98)
,
where m
(cid:98)
and Σ(cid:98) are the mean and covariance of ν n, respectively.
Proof. By definition, φ (cid:98)= 1
2
xTA(cid:98)x+(cid:98)bTx, where (A(cid:98),(cid:98)b) solve
(cid:20)(cid:90) (cid:16)1 (cid:17) (cid:90) (cid:16)1 (cid:17)∗ (cid:21)
min xTAx+bTx µ(dx)+ yTAy+bTy ν (dy) .
n
A⪰0,b∈Rd 2 2
By Lemma A.13, the convex conjugate of a quadratic is also a quadratic,
so the second integral only depends on moments of ν of order at most 2.
n
We can therefore replace the integration over ν with any other measure
n
that matches the first two moments, in particular N(m (cid:98),Σ(cid:98)). Then, since
thefunctionclasscontainsallKantorovichpotentialsbetweenGaussians
(see Example 1.19), it follows that the minimizer is the one which
corresponds to the optimal transport from N(0,I) to N(m (cid:98),Σ(cid:98)). ⊔⊓3.4 Obtaining the slow rate 93
InthesettingofProposition3.2,itiseasytoanalyzetheperformance
of T(cid:98) directly, since it is defined explicitly in terms of the sample mean
and covariance. However, for more general families F, we typically
cannot solve the semidual problem explicitly, and we need to use more
abstract arguments to analyze φ.
(cid:98)
3.4 Obtaining the slow rate
In this and the following section, we focus on estimating maps arising
from potentials that lie in a suitable class Φ whose covering numbers—
in the sense of Section 2.3—are suitably bounded. The size of these
covering numbers directly affects the quality of the estimator obtained
by minimizing the empirical semidual, as in (3.1).
To begin our analysis, we make several technical assumptions on the
measures µ and ν and the family Φ.
Assumption 3.3. There exists φ ∈ Φ such that ν = (∇φ) µ, where µ,
#
ν, and Φ satisfy:
• The supports of µ and ν lie in Ω = B (0).
1
• The set Φ is bounded in L∞ on Ω, i.e., sup ∥ϕ∥ ≲ 1.
ϕ∈Φ L∞(Ω)
• The potentials satisfy ϕ(0) = 0 and ϕ(x) = +∞ if x ∈/ Ω.
• Thepotentialsarelower-semicontinuous,smooth,andstronglyconvex
on Ω: 1I ⪯ ∇2ϕ(x) ⪯ 2I if ∥x∥ < 1.
2
The first and second of these assumptions can be weakened under
suitably strong moment assumptions, but we do not pursue this avenue
here.Thethirdiswithoutlossofgenerality:sincesubtractingaconstant
fromϕdoesnotaffectthesemidualobjectiveorthegradient∇ϕ,wecan
always assume that ϕ(0) = 0, and since the supports of µ and ν lie in
B (0),wemaydefineϕtobeinfinityoutsideofthissetwithoutaffecting
1
the semidual problem. The fact that ϕ = +∞ identically outside of Ω
simplifies several arguments involving the conjugate function, since it
implies that
ϕ∗(y) = sup {⟨x,y⟩−ϕ(x)} = sup{⟨x,y⟩−ϕ(x)}
x∈Rd x∈Ω
for all y ∈ Rd and ϕ ∈ Φ. The fourth assumption is the most impor-
tant, because it guarantees the stability of the semidual problem via
Theorem 3.1.
With these assumptions in hand, we can carry out the first step of
the analysis.94 3 Estimation of transport maps
Lemma 3.4.Adopt Assumption 3.3, and assume that ν = ∇φ µ for
#
some φ ∈ Φ. Let φ be given by
(cid:98)
φ = argminS (ϕ).
(cid:98) n
ϕ∈Φ
Then
∥∇φ−∇φ∥2 ≲ sup{|(µ −µ)(ϕ)|+|(ν −ν)(ϕ∗)|}. (3.4)
(cid:98) L2(µ) n n
ϕ∈Φ
Proof. The proof is an application of a standard argument in empirical
risk minimization. Theorem 3.1 implies
∥∇φ−∇φ∥2 ≲ S(φ)−S(φ)
(cid:98) L2(µ) (cid:98)
= S(φ)−S (φ)+S (φ)−S (φ)+S (φ)−S(φ)
(cid:98) n (cid:98) n (cid:98) n n
≤ 2sup|S (ϕ)−S(ϕ)|,
n
ϕ∈Φ
where the last inequality uses that S (φ)−S (φ) ≤ 0 by definition of
n (cid:98) n
φ. Expanding the definitions of S and S yields the claim. ⊔⊓
(cid:98) n
To bound the right side of (3.4), we employ the chaining technique
of Proposition 2.6. For simplicity, we focus on the case where the class
of functions is small enough that the covering numbers satisfy
logN(ε,Φ) ≲ ε−γlog(1+ε−1), γ ∈ [0,1) (3.5)
for all sufficiently small ε.
A paradigmatic exampleof such classes are parametric classes, where
the set Φ is finite dimensional, that is, where Φ = {ϕ } is indexed
θ θ∈Θ
by a parameter θ ∈ Θ ⊆ RM. Indeed, in this case, we have the following
bound.
Lemma 3.5.Assume that Φ = {ϕ } , where Θ ⊆ RM is bounded,
θ θ∈Θ
and the potentials satisfy ∥ϕ −ϕ ∥ ≲ ∥θ −θ′∥. Then there ex-
θ θ′ L∞(Ω)
ists a positive constant C such that the covering numbers of Φ satisfy
logN(ε,Φ) = 0 if ε ≥ C and
logN(ε,Φ) ≲ log(1+ε−1)
otherwise.3.4 Obtaining the slow rate 95
Proof. Byassumption,Θ ⊆ B (0)forsomeR > 0,so∥ϕ−ψ∥ ≲ R
R L∞(Ω)
for any ϕ,ψ ∈ Φ. Therefore, if ε is larger than a sufficiently large
constant, any element of Φ constitutes a one-element ε-cover of Φ.
For any δ > 0, Exercise 2 shows that there exists θ ,...,θ with
1 N
N ≤ (1+2Rδ−1)M such that (cid:83)N B (θ ) ⊇ Θ. Then ϕ ,...,ϕ is an
i=1 δ i θ1 θN
O(δ)-cover of Φ. Indeed, for any θ ∈ Θ, we may choose i ∈ [N] such
that ∥ϕ −ϕ ∥ ≲ ∥θ−θ ∥ ≤ δ. Taking δ = cε for a sufficiently
θ θi L∞(Ω) i
small positive constant c yields the claim. ⊔⊓
To give some examples of parametric classes, {ϕ } could be a set
θ θ∈Θ
of convex quadratic functions, as in Section 3.3, or it could consist of
linear combinations of a fixed dictionary {ϕ ,...,ϕ }, with
1 M
M
(cid:88)
ϕ = θ ϕ .
θ i i
i=1
Note that it is common in non-parametric statistics to choose the
dictionary carefully to balance approximation and estimation errors,
but we do not delve into such questions here in order to focus on the
core statistical content.
More generally, condition (3.5) allows for infinite-dimensional func-
tion classes which are nevertheless not “too large”. By contrast, it
excludes classes whose complexity grows with the ambient dimension,
such as the class of Lipschitz functions studied in Lemma 2.7.
What is the optimal rate of estimating T = ∇φ under this assump-
tion? If Φ is a parametric class, we expect the minimax rate to be
n−1—in particular, we expect that the map estimation problem avoids
the curse of dimensionality. Indeed, rates avoiding the curse of dimen-
sionality are achievable whenever a bound such as (3.5) is satisfied.
Lemma 3.4 involves both the potential ϕ and its conjugate ϕ∗. Unfor-
tunately, even if the set Φ has a simple form, the set Φ∗ = {ϕ∗ : ϕ ∈ Φ}
may defy easy description. However, we make the following crucial
observation: the covering numbers of Φ control those of Φ∗.
Lemma 3.6.For any ε > 0,
N(ε,Φ∗) ≤ N(ε,Φ).
Proof. The result follows from the fact that the conjugation operation
is a contraction in L∞. Indeed, given any pair of functions ϕ,ψ ∈ Φ,96 3 Estimation of transport maps
|ϕ∗(y)−ψ∗(y)| = |sup{⟨x,y⟩−ϕ(x)}− sup {⟨x′,y⟩−ψ(x′)}|
x∈Ω x′∈Ω
≤ sup|ϕ(x)−ψ(x)| = ∥ϕ−ψ∥ .
L∞(Ω)
x∈Ω
In particular, if ϕ ,...,ϕ is an ε-net for Φ, then ϕ∗,...,ϕ∗ is an ε-net
1 N 1 N
for Φ∗. ⊔⊓
We can now prove our first convergence rate for map estimation.
Theorem 3.7.Adopt Assumption 3.3 and assume (3.5) holds. The
semidual estimator φ satisfies the bound
(cid:98)
E∥∇φ−∇φ∥2 ≲ n−1/2.
(cid:98) L2(µ)
Proof. Lemma 3.4 implies
E∥∇φ−∇φ∥2 ≲ Esup|(µ −µ)(ϕ)|+Esup|(ν −ν)(ϕ∗)|.
(cid:98) L2(µ) n n
ϕ∈Φ ϕ∈Φ
By Assumption 3.3, there exists a positive constant R such that
∥ϕ∥ ≤ R and ∥ϕ∗∥ ≤ R for all ϕ ∈ Φ. Applying Proposition 2.6
L∞(Ω)
with τ = 0 yields
E∥∇φ−∇φ∥2 ≲ √1 (cid:90) R (cid:0)(cid:112) logN(ε,Φ)+(cid:112) logN(ε,Φ∗)(cid:1) dε.
(cid:98) L2(µ) n
0
Applying (3.5) and Lemma 3.6, we obtain
1 (cid:90) R (cid:112)
E∥∇φ−∇φ∥2 ≲ √ ε−γ/2 log(1+ε−1)dε ≲ n−1/2,
(cid:98) L2(µ) n
0
as desired. ⊔⊓
As anticipated, the parametric assumption on the class Φ translates
toarateofconvergencethatavoidsthecurseofdimensionality.However,
the “slow rate” n−1/2 is not quite what we hoped to prove. To obtain
the “fast rate” n−1, we need to localize and exploiting this localization
step requires imposing additional assumptions on µ.3.5 The fixed point argument 97
3.5 The fixed point argument
As mentioned above, the chaining argument in Theorem 3.7 fails to
give the correct rate of convergence because it is based on bounding the
deviations of S from S uniformly over the set Φ. However, Theorem 3.7
n
shows that φ is close to φ when n is large, which suggests that it is not
(cid:98)
necessary to bound the deviations of S from S uniformly over the set
n
Φ, but only over that subset near φ.
The argument below, due to van de Geer, formalizes this process in
the context of map estimation. The main idea is to apply the reasoning
of Lemma 3.4 not to φ but to a convex combination of φ and φ itself.
(cid:98) (cid:98)
For simplicity, to apply this argument, we make one more assumption
on Φ.
Assumption 3.8. The set Φ is convex, that is, if ϕ,ψ ∈ Φ, then
(1−λ)ϕ+λψ ∈ Φ for all λ ∈ [0,1].
Define
ε
φ = (1−λ)φ+λφ, λ = .
ε (cid:98)
ε+∥∇φ−∇φ∥
(cid:98) L2(µ)
Then,
∥∇φ −∇φ∥ = λ∥∇φ−∇φ∥
ε L2(µ) (cid:98) L2(µ)
∥∇φ−∇φ∥
(cid:0) (cid:98) L2(µ) (cid:1)
= ε ≤ ε.
ε+∥∇φ−∇φ∥
(cid:98) L2(µ)
Moreover:
ε ε∥∇φ (cid:98)−∇φ∥ L2(µ) ε
∥∇φ −∇φ∥ ≤ ⇐⇒ ≤
ε L2(µ)
2 ε+∥∇φ−∇φ∥ 2
(cid:98) L2(µ)
⇐⇒ ∥∇φ−∇φ∥ ≤ ε,
(cid:98) L2(µ)
so in considering ∥∇φ −∇φ∥ instead of ∥∇φ−∇φ∥ we only
ε L2(µ) (cid:98) L2(µ)
lose a factor of 2. Finally, Assumption 3.8 guarantees that φ ∈ Φ, since
ε
it is a convex combination of elements of Φ.
Also, note that for λ ∈ [0,1], pointwise we have ((1−λ)ϕ +λϕ )∗ ≤
0 1
(1−λ)ϕ∗+λϕ∗, from which it follows that S is a convex functional.
0 1 n
If we set S = S−S(φ) and S = S −S (φ), then by convexity,
n n n
S (φ ) ≤ (1−λ) S (φ)+λ S (φ) ≤ 0
n ε n n (cid:98)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=0 ≤098 3 Estimation of transport maps
by the definition of φ. Therefore, by Theorem 3.1, if φ is 1-strongly
(cid:98) ε 2
convex and 2-smooth,
1
∥∇φ −∇φ∥2 ≤ S(φ ) ≤ (S−S )(φ )
4 ε L2(µ) ε n ε
= (S−S )(φ )−(S−S )(φ)
n ε n
≤ sup (S−S )(ϕ)−(S−S )(φ)
n n
ϕ∈Φε
≤ sup {|(µ −µ)(ϕ−φ)|+|(ν −ν)(ϕ∗−φ∗)|},
n n
ϕ∈Φε
where Φ = {ϕ ∈ Φ : ∥∇ϕ−∇φ∥ ≤ ε}.
ε L2(µ)
If the right side of the above inequality is less than ε2/16, then
∥∇φ −∇φ∥ ≤ ε/2,whichinturnimpliesthat∥∇φ−∇φ∥ ≤ ε.
ε L2(µ) (cid:98) L2(µ)
We therefore can control the risk of φ if we can find an ε for which the
(cid:98)
supremum of the empirical process over F is of order ε2. We formalize
ε
the above considerations in the following proposition.
Proposition 3.9.Adopt Assumptions 3.3 and 3.8. For ε > 0, let
r(ε) = sup {|(µ −µ)(ϕ−φ)|+|(ν −ν)(ϕ∗−φ∗)|}.
n n
ϕ∈Φε
Then on the event {r(ε) ≤ ε2/16}, the semidual estimator φ satisfies
(cid:98)
∥∇φ−∇φ∥ ≤ ε.
(cid:98) L2(µ)
Inparticular,ifε isadeterministicquantitysuchthatr(ε ) ≤ ε2/16
n n n
with high probability, then the risk of φ is bounded by ε with high
(cid:98) n
probability.
Comparing Lemma 3.4 with Proposition 3.9 shows that we have
replaced the task of bounding the deviations of an empirical process
uniformly over Φ by the task of bounding them over the smaller set Φ .
ε
3.6 Obtaining the fast rate
In order to exploit the fact that we now seek to bound the empirical
process only over Φ , we need to formalize the notion that Φ is much
ε ε
smaller than Φ. A complicating factor is that the chaining technique
given in Proposition 2.6 measures the “size” of Φ by its ε-covering
numbers, which are defined in terms of L∞ covers. By contrast, the
restriction of Φ to Φ is based on the additional restriction on the L2
ε3.6 Obtaining the fast rate 99
norm of the gradients of ϕ. We therefore need a version of the chaining
bound which is able to exploit the size of a function class with respect
to both L∞ and L2.
The following modified chaining bound addresses this deficit.
Proposition 3.10([vdVW23, Theorem 2.14.21]). Let P be a prob-
ability measure on a set Ω ⊆ Rd. Let X ,...,X i. ∼i.d. P. If F is a set of
1 n
real-valued functions such that ∥f∥ ≤ σ and ∥f∥ ≤ R for all
L2(P) L∞(Ω)
f ∈ F, then
1 (cid:88)n 1 (cid:90) σ (cid:112)
Esup {f(X )−Ef(X )} ≲ √ logN(ε,F)dε
i i
f∈F n n 0
i=1
1 (cid:90) R
+ logN(ε,F)dε. (3.6)
n
0
Note that in the first term in (3.6), the upper limit of the integral
is σ rather than R. The second integral is the same term that appears
in Proposition 2.6, but with a prefactor of 1 rather than √1 . We may
n n
therefore hope that when n is large enough, the first term dominates.
If the L2 size of F is small, as captured by σ, then the first term
may be substantially smaller than the bound obtained by applying
Proposition 2.6 directly.
Proposition 3.10 also comes with a tail bound, showing that the
quantity on the right-hand side of (3.6) also bounds the empirical
process with high probability.
Proposition 3.11.Let J (F) denote the right side of (3.6). Under the
n
same assumptions as Proposition 3.10, there exists a positive universal
constant C such that for any t ≥ 0,
(cid:18) n (cid:114) (cid:19)
P sup 1 (cid:88) {f(X )−Ef(X )} ≥ C(cid:0) J (F)+σ t +R t(cid:1) ≤ exp(−t).
i i n
f∈F n n n
i=1
Proposition 3.10 requires us to control the L2 norm of the elements
of our function class; however, Φ is defined using the L2 norms of the
ε
gradients of the elements of Φ. We therefore adopt the final assumption
on µ, which allows us to move back and forth between these notions.
Definition 3.12.A measure P satisfies a Poincar´e inequality (with
constant C) if for all f ∈ L2(P),
(cid:90) (cid:16) (cid:90) (cid:17)2 (cid:90)
f − fdP dP ≤ C ∥∇f∥2dP .100 3 Estimation of transport maps
Assumption 3.13. The measure µ satisfies a Poincar´e inequality.
The Poincar´e inequality is a quantitative form of the statement that
the support of µ is connected. Indeed, a Poincar´e inequality holds for
any measure having a density bounded away from zero and infinity on
a bounded Lipschitz domain.
Under this new assumption, we obtain L2 bounds on Φ and Φ∗.
ε ε
Proposition 3.14.If Assumptions 3.3 and 3.13 hold, then
∥ϕ−φ−µ(ϕ−φ)∥ ≲ ε
L2(µ)
∥ϕ∗−φ∗−ν(ϕ∗−φ∗)∥ ≲ ε
L2(ν)
for all ϕ ∈ Φ .
ε
Proof. The first bound follows directly from the Poincar´e inequality: for
ϕ ∈ Φ ,
ε
∥ϕ−φ−µ(ϕ−φ)∥2 ≤ C∥∇(ϕ−φ)∥2 ≲ ε2.
L2(µ) L2(µ)
To prove the second bound, we first use the strong convexity of ϕ
and φ. Consider the functional T defined by
(cid:90) (cid:90)
T(ψ) := ψdν + ψ∗dµ.
That is, T is the semidual functional obtained by exchanging the roles
of µ and ν. Since (ϕ∗)∗ = ϕ for all convex and lower semicontinuous ϕ,
we have that S(ϕ) = T(ϕ∗) for all such ϕ. In particular, the minimizer
of T is φ∗, and Theorem 3.1 implies that
1
∥∇ϕ∗−∇φ∗∥2 ≤ T(ϕ∗)−T(φ∗) = S(ϕ)−S(φ) ≤ ∥∇ϕ−∇φ∥2 .
4 L2(ν) L2(µ)
Therefore ∥∇ϕ∗−∇φ∗∥2 ≲ ε2 for all ϕ ∈ Φ .
L2(ν) ε
To obtain the bound, all that is left is to show that ν also satisfies a
Poincar´e inequality, since we can then conclude as in the proof of the
first inequality. To see this, we use the fact that ν = (∇φ) µ. The fact
#
that φ is smooth means that for any f : Rd → Rd,
∥∇(f ◦∇φ)(x)∥ = ∥∇2φ(x)∇f(∇φ(x))∥ ≤ 2∥∇f(∇φ(x))∥.
The Poincar´e inequality for µ implies that for any f ∈ L2(ν),3.6 Obtaining the fast rate 101
(cid:90) (cid:16) (cid:90) (cid:17)2 (cid:90) (cid:16) (cid:90) (cid:17)2
f − fdν dν = f ◦∇φ− f ◦∇φdµ dµ
(cid:90)
≤ C ∥∇(f ◦∇φ)∥2dµ
(cid:90)
≤ 4C ∥∇f(∇φ(x))∥2dµ
(cid:90)
= 4C ∥∇f∥2dν.
Therefore ν also satisfies a Poincar´e inequality, with constant 4C. Hence
we may conclude as in the first case. ⊔⊓
We are finally ready to prove the desired rate. Note that in the
finite-dimensional setting, when Lemma 3.5 holds, this theorem shows
that the map can be estimated at nearly the parametric rate.
Theorem 3.15.Adopt Assumptions 3.3, 3.8, and 3.13. If (3.5) holds,
then the semidual estimator φ satisfies
(cid:98)
(cid:16)logn(cid:17) 2 t+1
∥∇φ−∇φ∥2 ≲ 2+γ + (3.7)
(cid:98) L2(µ) n n
with probability at least 1−e−t. In particular,
(cid:16)logn(cid:17) 2
E∥∇φ−∇φ∥2 ≲ 2+γ .
(cid:98) L2(µ) n
Proof. By Proposition 3.9, it suffices to show that r(ε ) ≤ ε2/16 with
n n
probability at least 1−e−t for ε ≍ (cid:0)logn(cid:1) 2+1 γ +(cid:113) t+1. Let us first
n n n
bound sup |(µ −µ)(ϕ−φ)|. Since (µ −µ)h = 0 if h is a constant
ϕ∈Φε n n
function, we have
sup |(µ −µ)(ϕ−φ)| = sup |(µ −µ)(ϕ−φ−µ(ϕ−φ))|.
n n
ϕ∈Φε ϕ∈Φε
We can apply Proposition 3.10, Proposition 3.11, and Proposition 3.14
along with the fact that ϕ−φ−µ(ϕ−φ) is bounded to obtain that
there exists a constant C such that
2
sup |(µ −µ)(ϕ−φ−µ(ϕ−φ))|
n
ϕ∈Φε
1 (cid:90) C2ε (cid:112)
≲ √ δ−γ/2 log(1+δ−1)dδ
n
0102 3 Estimation of transport maps
1 (cid:90) C2 (cid:114) t t
+ δ−γlog(1+δ−1)dδ+ε +
n n n
0
(cid:114) (cid:114)
log(1+ε−1) t t+1
≲ ε1−γ/2 +ε +
n n n
with probability at least 1−e−t. Therefore, taking
(cid:114)
(cid:16)(cid:16)logn(cid:17) 1 t+1(cid:17)
ε = C 2+γ + (3.8)
n
n n
for a sufficiently large constant C, we can ensure that
sup |(µ −µ)(ϕ−φ−µ(ϕ−φ))| ≤ ε2/32
n n
ϕ∈Φεn
with probability at least 1−e−t/2. An analogous argument yields
sup |(ν −ν)(ϕ∗−φ∗−ν(ϕ∗−φ∗))| ≤ ε2/32
n n
ϕ∈Φεn
with the same probability. By a union bound, we obtain that r(ε ) ≤
n
ε2/16 for ε as in (3.8) as claimed.
n n
The second bound following from integrating the tail. ⊔⊓
3.7 Discussion
§3.1. The empirical “plug-in” approach based on nearest neighbors
was developed as a simple alternative to the semidual approach
in [MBNWW21, DGS21, GS22]. Although the nearest neighbors es-
timator does not adapt to the smoothness of µ and ν, one recovers
minimax rates via the optimal transport map between density esti-
mators [MBNWW21], and even central limit theorems [MBNWW23].
However, compared to the plug-in approach, the semidual approach
developed here is overall more flexible and can be combined with other
tools such as kernel SoS [VMB+24].
§3.2. The semidual approach to map estimation was introduced in the
paper [HR21], which also proved the semidual stability estimates and
minimax lower bounds. That paper showed that, if the map between µ
and ν is assumed to be s-smooth (i.e., to possess s bounded derivatives),
then a suitable semidual estimator achieves the minimax-optimal rate
E∥∇φ (cid:98)−∇φ∥2
L2(µ)
≲ n− 2α−2α 2+d (logn)2+ n1 . (3.9)3.8 Exercises 103
This approach was then explored in great generality in [DNWP22],
and the arguments in that paper are closely related to those in this
chapter. However, the tools we describe here are not strong enough to
prove (3.9), since the class of s-smooth functions does not satisfy (3.5).
More information about how to obtain (3.9) along the lines of the
arguments we have presented in this chapter can be found in [DNWP22,
Section 4.4].
The alternative semidual stability estimates in Exercises 3 and 5 are
taken from [MBNWW21].
§3.3. Estimating the transport map between Gaussians was given as an
examplein[DNWP22]inwhichthesemidualapproachyieldsparametric
rates; see the paper for other function classes of interest.
§3.4. Standard references for empirical risk minimization (or M-
estimation) include [vdV98, Wai19]. The “slow rate” is characteristic of
M-estimation problems in the absence of strong convexity; the Poincar´e
inequality assumption adopted in §3.6 can be viewed as the appropriate
strong convexity condition for the semi-dual functional S.
§3.5. The one-shot localization we use is due to van de Geer [vdG87,
vdG02] and provides an alternative to the usual localization arguments
(e.g., [Wai19, Chapter 14]).
§3.6. The improved chaining bound of Proposition 3.10 is obtained by
the “generic chaining” technique developed by Talagrand [Tal96]. This
technique is at the heart of the study of Gaussian processes, see [Tal21].
The tail bound in Proposition 3.11 follows from a more general result
for generic chaining bounds [vdVW23, Theorem 2.2.19].
The Poincar´e inequality is a standard tool in high-dimensional prob-
ability, see [BLM13, BGL14, vH14]. It is an open question whether
the rates presented in this chapter are achievable without making this
assumption.
3.8 Exercises
1. Let µ,ν ∈ P([0,1]) and let X ,...,X ∼ µ, Y ,...,Y ∼ ν be
1 n 1 n
i.i.d. samples independent from each other. Assume that µ, ν have
differentiable CDFs F , F respectively, such that
µ ν
0 < c ≤ F′, F′ ≤ C < ∞ on [0,1].
µ ν
Thisisequivalenttoµ,ν havingdensitieson[0,1]whicharebounded
away from zero and infinity.104 3 Estimation of transport maps
Let us show that the following estimator T(cid:98)n obeys a parametric
rate of convergence. Let X < ··· < X , Y < ··· < Y denote
(1) (n) (1) (n)
the samples in sorted order, and given x ∈ [0,1] let X denote the
(i)
largest sample from µ such that X
(i)
≤ x. We then set T(cid:98)n(x) := Y
(i)
(if no such X
(i)
exists, then output T(cid:98)n(x) := 0). This estimator can
be viewed as a piecewise constant interpolation of the empirical
optimalcoupling,orasa1-nearestneighborestimator.Forsimplicity,
we fix x ∈ [0,1] and prove
E[|T(cid:98)n(x)−T(x)|2] ≲ 1/n
where T is the true optimal transport map F−1 ◦F from µ to ν,
ν µ
although it is a straightforward exercise to extend the results of this
problem to the integrated risk E(cid:82) |T(cid:98)n−T|2dµ.
a) Let N := (cid:80)n 1{X ≤ x} and N′ := (cid:80)n 1{Y ≤ y}. Argue
x i=1 i y i=1 i
that if N y′ < N x, then T(cid:98)n(x) ≥ y; if N y′ > N x, then T(cid:98)n(x) ≤ y.
b) Using the Dvoretzky–Kiefer–Wolfowitz inequality, argue that for
any δ ∈ (0,1), the following hold simultaneously with probability
at least 1−δ:
(cid:112)
|N −nF (x)| ≲ nlog(1/δ)
x µ
and
(cid:112)
|N′ −nF (y)| ≲ nlog(1/δ) for all y ∈ [0,1].
y ν
c) Use the previous two parts to conclude.
2. This exercise shows that the ball B (0) in Rd can be covered by
R
(1+2Rδ−1)d balls of radius δ.
a) Argue by rescaling that it suffices to consider the case R = 1.
b) Let X = {x ,...,x } be any set of elements of B (0) such that
1 N 1
∥x −x ∥ > δforalli ̸= j.Suchasetiscalledaδ-packingofB (0).
i j 1
Show that if X is a δ-packing of B 1(0), then the sets {B δ(x)} x∈X
2
are disjoint subsets of B (0). Conclude that |X| ≤ (1+2δ−1)d.
1+δ
2
c) Suppose that X is a maximal δ-packing of B (0), i.e., there does
R
not exist a strict superset of X which is also a δ-packing. Argue
(cid:83)
(via the contrapositive) that B (0) ⊆ B (x).
R x∈X δ
d) Use the previous two parts to conclude.
3. Let µ, ν be probability measures over Rd and let ∇φ denote the
optimal transport map from µ to ν. Assume that ∇φ is L-Lipschitz.3.8 Exercises 105
In this exercise, we establish the following estimate: for any other
convex function ϕ, if νˆ := (∇ϕ) µ, then
#
∥∇ϕ−∇φ∥2
L2(µ)
(cid:16) (cid:90) (cid:17)
≤ L W2(µ,νˆ)−W2(µ,ν)− (cid:0) ∥·∥2−2φ∗(cid:1) d(νˆ−ν) .
2 2
Hint: First, argue that by strong convexity of φ∗, it holds that
1
∥∇ϕ−∇φ∥2
2L L2(µ)
(cid:90) (cid:90)
≤ φ∗d(νˆ−ν)− ⟨x,∇ϕ(x)−∇φ(x)⟩µ(dx).
Then, expand out the quantity W2(µ,νˆ)−W2(µ,ν) and substitute
2 2
this into the above inequality.
4. We now use the stability estimate from the previous exercise to
deduce rates for map estimation in the one-sample setting. Let µ, ν
be probability measures with densities supported on the ball B (0)
1
of radius 1 and assume that the optimal transport map ∇φ from µ
to ν is Lipschitz. Assume that we have access to µ, and to n i.i.d.
samples from ν. Let ∇φ denote the optimal transport map from µ
(cid:98)
to the empirical measure ν . Using (2.22), prove that
n

n−1/2, d < 4,


E∥∇φ−∇φ∥2 ≲ n−1/2logn, d = 4,
(cid:98) L2(µ)

n−2/d, d > 4.
5. Starting with Exercise 3, assume additionally that φ is ℓ-strongly
convex. Let γ denote the optimal coupling between ν and νˆ, and let
(Y,Yˆ) ∼ γ. Then, (∇φ∗(Y),Yˆ) is a (suboptimal) coupling between
µ and νˆ, hence W2(µ,νˆ) ≤ E∥∇φ∗(Y)−Yˆ∥2. Expand this out and
2
use the smoothness of φ∗ to deduce the stronger stability estimate
(cid:112)
∥∇ϕ−∇φ∥ ≤ L/ℓW (ν,νˆ).
L2(µ) 24
Entropic optimal transport
Entropic regularization is one of the most active research areas in
modern optimal transport. As a regularization technique, it technically
falls under the scope of Section 2.8. Indeed, we show in this chapter
that it yields parametric rates, like many of the other regularization
approaches we have discussed. But entropic optimal transport is, in
fact, much more.
Since the groundbreaking work of Cuturi [Cut13], it has been pri-
marily used as a computational device that enables fast computation
of optimal transport distances using the Sinkhorn algorithm. However,
our focus remains statistical and we refer the reader to the excellent
manuscript [PC19b] of Gabriel Peyr´e and Marco Cuturi for more details
on the computational benefits of entropic regularization.
The basic principle of entropic optimal transport is to modify the
definition of optimal transport to include a penalization term based
on the entropy of the coupling, that is, to consider the optimization
problem
(cid:26)(cid:90) (cid:27)
inf ∥x−y∥2γ(dx,dy)−εEnt(γ) , (4.1)
γ∈Γµ,ν
where Ent(γ) denotes the differential entropy (cid:82) γ(x)log 1 dx for an
γ(x)
absolutely continuous probability measure γ. In fact, Cuturi originally
considered a discrete version of this problem, where µ and ν are finitely
supported and the coupling γ can therefore be identified with a matrix.
He considered the problem
(cid:26) (cid:27)
(cid:88)
inf ∥x −y ∥2γ −εH(γ) , (4.2)
i j i,j
γ∈Γµ,ν
i,j
where H(γ) denotes the Shannon entropy (cid:80) γ log 1 .
i,j i,j γi,j108 4 Entropic optimal transport
The role of the penalty terms in both (4.1) and (4.2) is to encourage
the coupling to be more spread out than the solution to the unregular-
ized optimal transport problem. Informally, the entropy of a measure
increases when its mass is more evenly spread. Indeed, Exercise 1 shows
that uniform distributions (over a subset of Rd in continuous case, or
over a finite set in the discrete case) have the maximum possible entropy.
The entropic penalty biases the solutions to (4.1) and (4.2) towards
that extreme.
To put (4.1) and (4.2) on a common footing, we introduce the KL
divergence between probability measures:
(cid:40)(cid:82)
dP log dP dQ if P ≪ Q,
KL(P ∥Q) = dQ dQ
+∞ otherwise.
Exercise 2 shows that both (4.1) and (4.2) are equivalent to
(cid:26)(cid:90) (cid:27)
inf ∥x−y∥2γ(dx,dy)+εKL(γ ∥µ⊗ν) , (4.3)
γ∈Γµ,ν
in the sense of yielding the same optimal γ, and we take (4.3) as the
main definition of entropic OT.
In the next section, we give a non-rigorous motivation for this regu-
larization approach from the perspective of convex duality. We analyze
the resulting dual problem in Section 4.2.
4.1 Derivation of entropic optimal transport
In this section, we attempt to motivate the definition of entropic OT
frombasicoptimizationanddualityprinciples.Forsimplicity,weassume
for now that we work on a compact set Ω ⊆ Rd. Given µ,ν ∈ P(Ω),
recall from Theorem 1.14 that W2(µ,ν) can be written
2
(cid:26)(cid:90) (cid:90) (cid:27)
W2(µ,ν) = sup fdµ+ gdν .
2
f,g∈C (Ω)
b
f(x)+g(y)≤∥x−y∥2
Formally,wecanrewritethisasanunconstrainedmaximizationproblem
by introducing a penalization term that enforces the constraint. Indeed,
if we define
(cid:40)
0 if f(x)+g(y) ≤ ∥x−y∥2 µ⊗ν-a.e.,
ι(f,g) =
+∞ otherwise,4.1 Derivation of entropic optimal transport 109
then we obtain
(cid:26)(cid:90) (cid:90) (cid:27)
W2(µ,ν) = sup fdµ+ gdν −ι(f,g) .
2
f,g∈C (Ω)
b
This is a concave maximization problem, so it is (in principle) benign;
however, from a computational and statistical perspective, the fact that
ι fails to be continuous, much less smooth, is a source of difficulty. To
remedy this, we can consider a relaxed version of this problem obtained
by replacing ι by a smoothed version.1 Define
(cid:90)(cid:90) (cid:16) (cid:17)
ιε(f,g) = ε e(f(x)+g(y)−∥x−y∥2)/ε−1 µ(dx)ν(dy).
Thefunctionιε isconvexandcontinuousonthespaceC (Ω)ofbounded,
b
continuous functions on Ω. Moreover, it is easy to see that as ε ↘ 0, we
recover the original hard constraint.
Lemma 4.1.For any measurable f,g,
limιε(f,g) = ι(f,g).
ε↘0
Proof. Suppose first that ι(f,g) = 0, so that f(x)+g(y) ≤ ∥x−y∥2
µ⊗ν-almost everywhere. Then the integral
(cid:90)(cid:90) (cid:16) (cid:17)
e(f(x)+g(y)−∥x−y∥2)/ε−1
µ(dx)ν(dy)
is bounded as ε ↘ 0, and hence ιε(f,g) → 0.
On the other hand, if ι(f,g) = +∞, then there exists δ > 0 and a
set U of positive µ⊗ν measure such that e(f(x)+g(y)−∥x−y∥2)/ε ≥ eδ/ε
for all (x,y) ∈ U. We obtain
(cid:90)(cid:90) (cid:16) (cid:17)
ε e(f(x)+g(y)−∥x−y∥2)/ε−1 µ(dx)ν(dy) ≥ εeδ/ε(µ⊗ν)(U)−ε
→ ∞.
This concludes the proof. ⊔⊓
1 The smoothing we employ is reminiscent of the “softmax” function in machine
learning.110 4 Entropic optimal transport
We are therefore led to consider the following “ε-smoothed” dual
version of the W2 distance:
2
(cid:26)(cid:90) (cid:90) (cid:27)
sup fdµ+ gdν −ιε(f,g) (ε-D-W2)
2
f,g∈C (Ω)
b
Now that we have derived a relaxation of the dual problem, we can
ask what this corresponds to in the primal problem. It turns out that
the relaxation we have proposed in the dual corresponds to an entropic
penalty in the primal problem.
To obtain this connection, let us define a version of the Kullback–
Leibler divergence over the space M (Ω) of all positive (not necessarily
+
probability) Borel measures on Ω:
(cid:40)(cid:82)(cid:16)
dP log dP − dP
+1(cid:17)
dQ if P ≪ Q,
KL(P ∥Q) = dQ dQ dQ
+∞ otherwise.
Note that the integrand is non-negative, so that the integral is always
well defined, and KL is always non-negative. When P and Q are proba-
bility measures, the terms −dP +1 cancel out and we obtain the usual
dQ
definition.
The importance of this definition is that the convex conjugate of
the functional KL(·∥Q) (see Appendix A.1) is precisely the exponential
term appearing in ιε. This fact is a variant of what is commonly known
as the Gibbs variational principle.
Proposition 4.2.For any bounded measurable function h,
(cid:110)(cid:90) (cid:111) (cid:90)
sup hdP −KL(P ∥Q) = (exph−1)dQ.
P∈M +(Ω)
Moreover the supremum is achieved at P satisfying dP h = exph.
h dQ
Proof. We show that, for any Borel measure P, the difference
(cid:90) (cid:90)
∆ := (exph−1)dQ− hdP +KL(P ∥Q)
is non-negative, and equals 0 when P = P . We may assume without
h
loss of generality that KL(P ∥Q) < +∞, since otherwise the claim is
vacuous. Expanding the definition of KL(P ∥Q), we obtain4.1 Derivation of entropic optimal transport 111
(cid:90) (cid:16) dP dP dP dP (cid:17)
∆ = eh−1−h + log − +1 dQ
dQ dQ dQ dQ
(cid:90) (cid:16)dP (cid:16) dP(cid:17) dP (cid:17)
= log e−h − +eh dQ. (4.4)
dQ dQ dQ
By change of measure, we have
dP dP dP dP
= h = eh .
dQ dQ dP dP
h h
Therefore (4.4) can be written
(cid:90) (cid:16) dP dP dP (cid:17)
∆ = eh log − +1 dQ = KL(P ∥P ).
h
dP dP dP
h h h
Since KL(P ∥P ) ≥ 0 and KL(P ∥P ) = 0, this proves the claim. ⊔⊓
h h h
We can therefore rewrite (ε-D-W2) as
2
(cid:110)(cid:90) (cid:90) (cid:111)
sup fdµ+ gdν
f,g∈C (Ω)
b
(cid:110)(cid:90) f(x)+g(y)−∥x−y∥2 (cid:111)
−ε sup γ(dx,dy)−KL(γ ∥µ⊗ν) ,
ε
γ∈M +(Ω)
and, rearranging,
(cid:110)(cid:90)
sup inf ∥x−y∥2γ(dx,dy)+εKL(γ ∥µ⊗ν)
f,g∈C b(Ω)γ∈M +(Ω)
(cid:90) (cid:90) (cid:90) (cid:111)
+ fdµ+ gdν − f ⊕gdγ ,
where we define (f ⊕g)(x,y) = f(x)+g(y).
As in Subsection 1.5.1, we can swap the inf and sup to get a lower
bound on the value of (ε-D-W2):
2
(cid:110)(cid:90)
inf ∥x−y∥2γ(dx,dy)+εKL(γ ∥µ⊗ν)
γ∈M(Ω)
(cid:110)(cid:90) (cid:90) (cid:90) (cid:111)(cid:111) (4.5)
+ sup fdµ+ gdν − f ⊕gdγ .
f,g∈C (Ω)
b
We have already seen that
(cid:40)
(cid:110)(cid:90) (cid:90) (cid:90) (cid:111) 0, if γ ∈ Γ ,
µ,ν
sup fdµ+ gdν − f ⊕gdγ =
∞, otherwise.
f,g∈C (Ω)
b112 4 Entropic optimal transport
Therefore, (4.5) is equivalent to
(cid:110)(cid:90) (cid:111)
inf ∥x−y∥2γ(dx,dy)+εKL(γ ∥µ⊗ν) . (ε-W2)
2
γ∈Γµ,ν
This is the primal version of the entropic OT problem, and it is the
version that is usually taken as the definition of entropic regularization.
This choice of regularization is usually justified a posteriori by the
existence of Sinkhorn’s algorithm (see Section 4.2), but as we have seen
it also arises naturally from a simple relaxation of the dual Kantorovich
problem. The argument in this section establishes a form of weak
duality, showing that the value of (ε-W2) is lower bounded by the value
2
of (ε-D-W2). The next section establishes a tight connection between
2
the primal and dual problems, both in terms of their optimal value and
their optimal solutions. This connection has been heavily exploited in
the statistical analysis of entropic OT.
4.2 Duality
In this section, we show that the values of the primal problem (ε-W2)
2
and dual problem (ε-D-W2) actually agree, and that an optimal solution
2
to one problem can be extracted from the optimal solution to the other.
Proposition 4.3.Let f⋆, g⋆ solve (ε-D-W2). Then
2
(cid:16)f⋆(x)+g⋆(y)−∥x−y∥2(cid:17)
γ⋆(dx,dy) = exp µ(dx)ν(dy) (4.6)
ε
is the unique solution to (ε-W2), and
2
(cid:90)
∥x−y∥2γ⋆(dx,dy)+εKL(γ⋆∥µ⊗ν)
(cid:90) (cid:90) (cid:90) (cid:90)
= f⋆dµ+ g⋆dν −ιε(f⋆,g⋆) = f⋆dµ+ g⋆dν.
Proof. Itsufficestoshowthatγ⋆isasolutionto(ε-W2),sinceuniqueness
2
follows immediately from the strict convexity of KL(γ ∥µ⊗ν).
We need to show that γ⋆ ∈ Γ . Clearly γ⋆ is positive, so it suffices
µ,ν
to show that it has the correct marginals. To that end, we need to verify
that for any Borel set A,
(cid:90) (cid:90) (cid:16)f⋆(x)+g⋆(y)−∥x−y∥2(cid:17)
ν(A) = exp µ(dx)ν(dy).
ε
A4.2 Duality 113
Equivalently, we need to show that
(cid:90) (cid:16)f⋆(x)+g⋆(y)−∥x−y∥2(cid:17)
exp µ(dx) = 1 ν-a.e. (4.7)
ε
Let us define the function
(cid:90) (cid:16)f⋆(x)−∥x−y∥2(cid:17)
g¯(y) = −εlog exp µ(dx). (4.8)
ε
We show that g¯= g⋆, ν-almost everywhere. Since
(cid:90) (cid:16)f⋆(x)+g¯(y)−∥x−y∥2(cid:17)
exp µ(dx) = 1 ∀y ∈ Rd (4.9)
ε
holds by definition, this establishs that (4.7) holds as well.
Let γ¯(dx,dy) = exp((f⋆(x)+g¯(y)−∥x−y∥2)/ε)µ(dx)ν(dy). Then,
(cid:90) (cid:16) dγ¯ dγ¯ dγ¯ (cid:17)
0 ≤ KL(γ¯∥γ⋆) = log − +1 dγ¯
dγ⋆ dγ⋆ dγ⋆
(cid:90) g¯(y)−g⋆(y) (cid:90) (cid:90)
= γ¯(dx,dy)− dγ¯+ dγ⋆
ε
1 (cid:16)(cid:90) (cid:17)
= (g¯−g⋆)dν −ιε(f⋆,g¯)+ιε(f⋆,g⋆) ,
ε
where the last step uses that the second marginal of γ¯ is ν, by (4.9).
We obtain that
(cid:90) (cid:90)
εKL(γ¯∥γ⋆) = g¯dν + f⋆dµ−ιε(f⋆,g¯)
(cid:18)(cid:90) (cid:90) (cid:19)
− g⋆dν + f⋆dµ−ιε(f⋆,g⋆)
≤ 0,
since (f⋆,g⋆) are optimal for (ε-D-W2).
2
ThereforeKL(γ¯∥γ⋆) = 0,soγ¯ = γ⋆,andg¯= g⋆,ν-almosteverywhere.
This establishes that the second marginal of γ⋆ is ν, and an analogous
argument shows that the first marginal of γ⋆ is µ. We obtain that γ⋆ is
feasible in (ε-W2).
2
To conclude, we compute the value that γ⋆ achieves in the primal
problem:
(cid:90)
∥x−y∥2γ⋆(dx,dy)+εKL(γ⋆∥µ⊗ν)114 4 Entropic optimal transport
(cid:90)
= ∥x−y∥2γ⋆(dx,dy)
(cid:90)
+ (f⋆(x)+g⋆(y)−∥x−y∥2)γ⋆(dx,dy)
(cid:90)
= (f⋆(x)+g⋆(y))γ⋆(dx,dy)
(cid:90) (cid:90)
= f⋆dµ+ g⋆dν −ιε(f⋆,g⋆),
where the last step uses that γ⋆ ∈ Γ and that ιε(f⋆,g⋆) = 0 since
µ,ν
γ⋆ is a probability measure. Therefore, the primal objective evaluated
at γ⋆ and the dual objective evaluated at (f⋆,g⋆) have the same value,
and weak duality (see Section 4.1) shows that γ⋆ and (f⋆,g⋆) are an
optimal pair of primal/dual solutions. ⊔⊓
Proposition 4.3 deserves several remarks. First, note that the hypoth-
esis of the proposition is the existence of optimal solutions to (ε-D-W2).
2
We do not justify the existence of such solutions here, but it can be
shown that that if µ and ν are compactly supported, then there exist
f⋆,g⋆ ∈ C (Ω). More generally, if µ and ν have finite second moments,
b
then optima exist in L1(µ) and L1(ν), respectively, and Proposition 4.3
continues to hold.
The proof of Proposition 4.3 actually shows that if f, g are such
that γ(dx,dy) = exp((f(x)+g(y)−∥x−y∥2)/ε)µ(dx)ν(dy) is a valid
coupling between µ and ν, then γ is optimal for (ε-W2) and f, g are
2
optimal for (ε-D-W2). This fact can be viewed as an entropic variant
2
of the complementary slackness condition f¯(x) + g¯(y) = ∥x − y∥2,
γ¯-almost everywhere, which holds for the optimal solutions of (W2)
2
and (D-W2). (See Theorem 1.14.) We can therefore conclude that f⋆
2
and g⋆ are optimal for (ε-D-W2) if and only if they satisfy the marginal
2
constraint (4.7) and the analogous constraint for the other marginal:
(cid:90) (cid:16)f⋆(x)+g⋆(y)−∥x−y∥2(cid:17)
exp ν(dy) = 1 µ-a.e. (4.10)
ε
The marginal constraints (4.7) and (4.10), sometimes known as the
Schro¨dinger system, are at the core of the theory of entropic OT. Even
though these equations a priori only hold ν- and µ-almost everywhere,
respectively, the construction in (4.8) shows that we can construct
canonical extensions of f⋆ and g⋆ so that the marginal constraints
hold everywhere in Rd. Moreover, the dominated convergence theorem4.3 Statistical rates for dual solutions 115
shows that if µ and ν are compactly supported, then these extensions
are continuous (even C∞) functions on Rd. In what follows, we may
therefore always assume that f⋆ and g⋆ are defined everywhere on Rd,
and that (4.7) and (4.10) hold for all y and x ∈ Rd, respectively.
Finally, we note that Proposition 4.3 is the basis for the celebrated
Sinkhorn algorithm for entropic optimal transport. This algorithm is
defined by initializing f ≡ 0, and for t ≥ 1 performing the updates
0
(cid:90) (cid:16)f (x)−∥x−y∥2(cid:17)
t−1
g (y) = −εlog exp µ(dx), (4.11)
t
ε
(cid:90) (cid:16)g (y)−∥x−y∥2(cid:17)
t
f (y) = −εlog exp ν(dy). (4.12)
t
ε
Proposition 4.3 shows that a fixed point of this algorithm yields an opti-
mal solution to (ε-D-W2), and therefore an optimal solution to (ε-W2).
2 2
4.3 Statistical rates for dual solutions
In this and the following section, we consider the statistical behavior
of empirical versions of the entropic OT problem. In contrast to the
results of Chapter 2, the rates of convergence (as a function of n) no
longer suffer from the curse of dimensionality. However, the price to pay
for this improvement is a steep dependence on 1/ε.
The strategy for proving statistical bounds is to analyze the dual
problem (ε-D-W2). We then transfer these bounds to the primal solu-
2
tion using the connection between primal and dual solutions given by
Proposition 4.3.
Let us denote by S(µ,ν) the value of the primal problem (ε-W2).
2
Given i.i.d. samples from µ and ν, we are chiefly interested in estimating
two quantities:
• The cost S(µ,ν),
• The entropic map or entropic regression function
b⋆(x) = E [Y | X = x].
(X,Y)∼γ⋆
Estimating the first quantity is the entropic analogue of the question we
considered in Chapter 2. Estimating the second quantity is the entropic
analogue of the map estimation task described in Chapter 3. Indeed, b⋆
is a projection of γ⋆, in the sense of L2, onto the space of maps; however,116 4 Entropic optimal transport
we stress that b⋆ is not a valid transport map between µ and ν, since
(b⋆) µ ̸= ν.
#
As in Chapter 2, we analyze plug-in estimators for these quantities:
S(µ ,ν ) for the cost, and ˆb(x) = E [Y | X = x], where γˆ is the
n n (X,Y)∼γˆ
optimal solution to the empirical entropic OT problem between µ and
n
ν .2
n
As emphasized above, our main tool to analyze these quantities is
the duality relationship established in Proposition 4.3. Denote by C⊕
b
the subspace of C (Ω×Ω) consisting of functions of the form f ⊕g
b
for f,g ∈ C (Ω). The dual problem (ε-D-W2) depends on f and g only
b 2
through their sum h = f ⊕g ∈ C⊕. In particular, if (f,g) is a dual
b
solution, then so is (f +λ,g−λ) for any λ ∈ R. Inspired by this fact,
let us define the dual functional Φ : C⊕ → R given by
b
(cid:90)
h (cid:55)→
(cid:0) h−ε(e(h−c)/ε−1)(cid:1)
d(µ⊗ν), (4.13)
wherec(x,y) = ∥x−y∥2 isthesquaredEuclideancost.Thedualproblem
can then be written succinctly as sup Φ(h).
h∈C⊕
b
Suppose we wish to compare S(µ,ν) to S(µ ,ν ), where, as in
n n
Chapter 2, µ and ν denote empirical measures corresponding to i.i.d.
n n
samples from µ and ν. We can define an empirical version of the dual
functional by
(cid:90)
Φ(cid:98)(h) =
(cid:0) h−ε(e(h−c)/ε−1)(cid:1)
d(µ n⊗ν n).
Then
S(µ n,ν n)−S(µ,ν) = sup Φ(cid:98)(h)− sup Φ(h) = Φ(cid:98)(hˆ)−Φ(h⋆),
h∈C⊕ h∈C⊕
b b
where hˆ and h⋆ are maximizers of Φ(cid:98) and Φ, respectively.
Exercise 7 sketches a direct approach to obtain an upper bound
on Φ(cid:98)(hˆ)−Φ(h⋆) based on empirical process theory, analogous to the
one developed in Section 2.3 for the unregularized optimal transport
problem. However, we pursue a different path, which leverages the
strong concavity of the dual functional.
2 Though the formulas for the entropic maps b⋆ and b define them as elements of
n
L1(µ) and L1(µ ), respectively, the canonical extensions described in Section 4.2
n
can be used to define continuous versions of b⋆ and b .
n4.3 Statistical rates for dual solutions 117
We begin with a non-rigorous sketch of the argument. Strong con-
cavity of the functional Φ(cid:98) should imply there exists δ > 0 such that
δ
Φ(cid:98)(hˆ) ≤ Φ(cid:98)(h⋆)+⟨∇Φ(cid:98)(h⋆),hˆ −h⋆⟩ L2(µn⊗νn)−
2
∥hˆ −h⋆∥2 L2(µn⊗νn).
While it is possible to define a suitable notion of gradient for ∇Φ(cid:98), it
is sufficient for our purposes to interpret the above inner product as a
directional (Gˆateaux) derivative. In contrast to the empirical process
theory approach, this inequality implies that we can obtain a bound by
controlling Φ(cid:98) and ∇Φ(cid:98) at the fixed function h⋆ rather than the random
function hˆ. In particular, there is no need to “sup-out” hˆ which allows
us to circumvent the use of empirical process theory.
We make the following assumption.
Assumption 4.4. The supports of µ and ν lie in Ω ⊆ B (0).
1/2
In particular, under Assumption 4.4, diam(Ω) ≤ 1. This assumption
implies simple a priori bounds on hˆ and h⋆.
Proposition 4.5.Under Assumption 4.4, it holds that
∥hˆ∥ ,∥h⋆∥ ≤ 2.
L∞ L∞
Proof. We first prove the claim for h⋆. Recall that h⋆ = f⋆ ⊕g⋆ for
f⋆,g⋆ ∈ C (Ω) and that thanks to canonical extensions, we may assume
b
that the marginal constraints (4.7) and (4.10) hold for all x,y ∈ Ω.
Since c(x,y) ≤ 1 for all x,y ∈ Ω, we get
(cid:90) (cid:90)
1 =
e(f⋆⊕g⋆−c)/εµ(dx)
≥
e(g⋆(y)−1)/ε ef⋆(x)/εµ(dx),
∀y ∈ Ω,
(cid:90) (cid:90)
1 =
e(f⋆⊕g⋆−c)/εν(dy)
≥
e(f⋆(x)−1)/ε eg⋆(y)/εν(dy),
∀x ∈ Ω.
Multiplying these two inequalities yields
(cid:90)
e(h⋆−2)/ε eh⋆/εd(µ⊗ν)
≤ 1.
Next note that by Jensen’s inequality, we get
(cid:90)
eh⋆/εd(µ⊗ν) ≥ eS(µ,ν)/ε ≥ 1,
where we used Proposition 4.3. From the above two displays, we get
that h⋆ ≤ 2 for all x,y ∈ Ω.118 4 Entropic optimal transport
Next, since c ≥ 0 on Ω×Ω, we get from the same argument that
(cid:90)
1 ≤
eh⋆/ε eh⋆/εd(µ⊗ν)
≤
e(h⋆+2)/ε,
where we used the bound h⋆ ≤ 2 that we just proved. Hence h⋆ ≥ −2
for all x,y ∈ Ω.
Since the only fact that was used about h⋆ is that it maximizes
the dual functional Φ corresponding to measures whose supports lie
in Ω, the claim also holds for hˆ when replacing (µ,ν) with (µ ,ν ) in
n n
Proposition 4.3. Again, canonical extensions play a crucial role here. ⊔⊓
We require some fundamental differentiability and concavity prop-
erties of the empirical dual functional. If we let µ = 1 (cid:80)n δ and
n n i=1 Xi
ν = 1 (cid:80)n δ , for X ,...,X i. ∼i.d. µ and Y ,...,Y i. ∼i.d. ν, then we
n n j=1 Yj 1 n 1 n
can write Φ(cid:98) explicitly as
n
Φ(cid:98)(h) =
n1
2
(cid:88)(cid:16)
h(X i,Y
j)−ε(cid:0) e(h(Xi,Yj)−∥Xi−Yj∥2)/ε−1(cid:1)(cid:17)
. (4.14)
i,j=1
Rather than appealing to functional analysis to study differentiability
of the functional Φ(cid:98), it is sufficient to study the function φ defined on
[0,1] by
φ(t) = Φˆ(h ), where h := (1−t)hˆ +th⋆. (4.15)
t t
In particular, it is twice differentiable with derivatives given by
n
φ′(t) = 1 (cid:88)(cid:16) (cid:0) h⋆(X ,Y )−hˆ(X ,Y )(cid:1)(cid:0) 1−e(ht(Xi,Yj)−∥Xi−Yj∥2)/ε(cid:1)(cid:17) ,
n2 i j i j
i,j=1
(4.16)
and
n
φ′′(t) = − 1 (cid:88)(cid:16) (cid:0) h⋆(X ,Y )−hˆ(X ,Y )(cid:1)2 e(ht(Xi,Yj)−∥Xi−Yj∥2)/ε(cid:17)
εn2 i j i j
i,j=1
e−3/ε
≤ − ∥hˆ −h⋆∥2 , (4.17)
ε L2(µn⊗νn)
where we used Proposition 4.5 and Assumption 4.4 in the above inequal-
ity. We readily get that φ is strongly concave on [0,1].
Theexpression(4.16)revealsthatthederivativeofφisanL2(µ ⊗ν )
n n
innerproductwithafunctioninthespaceC⊕.Thisinnerproductcanbe
b
well understood using the Hoeffding (a.k.a. Efron–Stein, a.k.a. ANOVA)
decomposition [Hoe48].4.3 Statistical rates for dual solutions 119
Definition 4.6.Let X,Y be two independent random variables with
distributions P and Q respectively. Given k ∈ L2(P ⊗Q), the Hoeffding
decomposition of k(X,Y) in L2(P ⊗Q) is given by
k(X,Y) = k¯ (X)+k¯ (Y)+k+r(X,Y)
1 2
where
k = E[k(X,Y)] ∈ R,
k¯ (x) = E[k(X,Y) | X = x]−k,
1
k¯ (y) = E[k(X,Y) | Y = y]−k,
2
and
r(X,Y) = k(X,Y)−k¯ (X)−k¯ (Y)−k.
1 2
It is easy to check (exercise!) that the Hoeffding decomposition is
orthogonal in L2(P ⊗ Q). In fact, the same calculations reveal the
relevance of this decomposition to our problem: for any h = f⊕g ∈ C⊕,
b
it holds
⟨h,k⟩ = ⟨f,k¯ ⟩ +⟨g,k¯ ⟩ +⟨h,k⟩
L2(P⊗Q) 1 L2(P) 2 L2(Q) L2(P⊗Q)
= ⟨h,k¯ +k¯ +k⟩ .
1 2 L2(P⊗Q)
Using Cauchy–Schwarz and orthogonality of the Hoeffding decomposi-
tion, we get
⟨h,k⟩2 ≤ ∥h∥2 ∥k¯ +k¯ +k∥2
L2(P⊗Q) L2(P⊗Q) 1 2 L2(P⊗Q)
= ∥h∥2 (cid:0) ∥k¯ ∥2 +∥k¯ ∥2 +k2(cid:1) .
L2(P⊗Q) 1 L2(P⊗Q) 2 L2(P⊗Q)
(4.18)
Using orthogonality again implies
E(cid:2)(cid:0)E[k(X,Y) | X](cid:1)2(cid:3) = ∥k¯ +k∥2
1 L2(P⊗Q)
2
= ∥k¯ ∥2 +k
1 L2(P⊗Q)
and similarly
E(cid:2)(cid:0)E[k(X,Y) | Y](cid:1)2(cid:3) = ∥k¯ ∥2 +k2 .
2 L2(P⊗Q)
These two identities together with (4.18) yield120 4 Entropic optimal transport
⟨h,k⟩2
L2(P⊗Q)
≤
E(cid:2)(cid:0)E[k(X,Y)
|
X](cid:1)2(cid:3) +E(cid:2)(cid:0)E[k(X,Y)
|
Y](cid:1)2(cid:3) −k2
.
∥h∥2
L2(P⊗Q)
Applying this result with P = µ , Q = ν we get the following lemma.
n n
Lemma 4.7.For any k ∈ L2(µ ⊗ν ) and any h ∈ C⊕, we have
n n b
⟨h,k⟩2 ≤ ∥h∥2 (∥ν (k)∥2 +∥µ (k)∥2 ),
L2(µn⊗νn) L2(µn⊗νn) n L2(µn) n L2(νn)
where
n n
1 (cid:88) 1 (cid:88)
µ (k)(y) = k(X ,y), ν (k)(x) = k(x,Y ).
n i n j
n n
i=1 j=1
We are now in a position to obtain an important lemma showing
that hˆ is a good estimator of h⋆. In turn, rates of convergence for the
cost and the entropic map follow from this lemma.
Lemma 4.8.Let Assumption 4.4 hold. Then
2ε2e10/ε
E∥hˆ −h⋆∥2 ≤ .
L2(µn⊗νn) n
Proof. Since φ is strongly concave, using respectively (4.17), the opti-
mality condition φ′(0) = 0, and (4.16), we get
e−3/ε
∥hˆ −h⋆∥2 ≤ φ′(0)−φ′(1) = −φ′(1)
ε L2(µn⊗νn)
n
= 1 (cid:88)(cid:16) (cid:0) h⋆(X ,Y )−hˆ(X ,Y )(cid:1)(cid:0) e(h⋆(Xi,Yj)−∥Xi−Yj∥2)/ε−1(cid:1)(cid:17)
n2 i j i j
i,j=1
= ⟨h⋆−hˆ,p⋆−1⟩ , (4.19)
L2(µn⊗νn)
where
p⋆(x,y) = e(h⋆(x,y)−∥x−y∥2)/ε.
Since h⋆−hˆ ∈ C⊕, Lemma 4.7 implies
b
⟨h⋆−hˆ,p⋆−1⟩ ≤ ∥h⋆−hˆ∥ δ ,
L2(µn⊗νn) L2(µn⊗νn) n
where
δ = (cid:0) ∥ν (p⋆−1)∥2 +∥µ (p⋆−1)∥2 (cid:1)1/2 .
n n L2(µn) n L2(νn)4.4 Statistical rates for primal solutions 121
Combining this with (4.19) yields
∥hˆ −h⋆∥2 ≤ ε2e6/εδ2. (4.20)
L2(µn⊗νn) n
Recall from (4.7) that
E[p⋆(X ,Y ) | Y ] = 1,
i j j
so that
n n
1 (cid:88)(cid:16)1 (cid:88) (cid:17)2
E∥µ (p⋆−1)∥2 = E p⋆(X ,Y )−E[p⋆(X ,Y ) | Y ]
n L2(νn) n n i j i j j
j=1 i=1
= 1 Evar(cid:0) p⋆(X 1,Y 1) (cid:12) (cid:12) Y 1(cid:1)
n
E[p⋆(X ,Y )2] e4/ε
1 1
≤ ≤ .
n n
An analogous bound holds for E∥ν (p⋆−1)∥2 , which implies that
n L2(µn)
2e4/ε
Eδ2 ≤ , (4.21)
n n
proving the claim. ⊔⊓
4.4 Statistical rates for primal solutions
Lemma 4.8 shows that solutions to the dual problem (ε-D-W2) converge
2
at the parametric rate. In this section, we use this result to give bounds
for the primal problem as well.
We now turn to our first quantity of interest, the cost S(µ,ν). The
following result shows that the mean squared error and bias of the
estimator S(µ ,ν ) are both of order n−1, albeit with constants that
n n
scale exponentially in 1/ε. Strikingly, the n−1 rate we have obtained for
thevarianceandbiasischaracteristicofparametric estimationproblems,
despite the non-parametric setting. It is of course to be contrasted with
the slow, non-parametric rates of Chapter 2.
Theorem 4.9.If µ and ν satisfy Assumption 4.4, then the mean
squared error and bias of S(µ ,ν ) satisfy
n n
1
E(S(µ ,ν )−S(µ,ν))2 ≲ ,
n n
n122 4 Entropic optimal transport
1
|ES(µ ,ν )−S(µ,ν)| ≲ , (4.22)
n n
n
where the implicit constants depends exponentially on 1/ε.
Proof. We begin with establishing (4.22) by studying the bias. Jensen’s
inequality implies
ES(µ n,ν n) = E sup Φ(cid:98)(h) ≥ sup Φ(h) = S(µ,ν).
h∈C⊕ h∈C⊕
b b
Hence
0 ≤ b := ES(µ ,ν )−S(µ,ν)
n n n
=
E(cid:2) Φ(cid:98)(hˆ)−Φ(cid:98)(h∗)(cid:3)
= E(cid:2) φ(0)−φ(1)(cid:3) ≤ −Eφ′(1),
where we recall that the concave function φ is defined in (4.15). It
follows from (4.16) that −φ′(1) is given by
n
1 (cid:88)(cid:16) (cid:0) h⋆(X ,Y )−hˆ(X ,Y )(cid:1)(cid:0) e(h⋆(Xi,Yj)−∥Xi−Yj∥2)/ε−1(cid:1)(cid:17)
n2 i j i j
i,j=1
≤ ∥hˆ −h⋆∥ δ ,
L2(µn⊗νn) n
where δ is defined as in the proof of Lemma 4.8 and we have applied
n
Lemma 4.7. By the Cauchy–Schwarz inequality,
(cid:113)
0 ≤ b ≤ E∥hˆ −h⋆∥2 Eδ2
n
√ √
L2(µn⊗νn) n
2εe5/ε 2e2/ε 2εe7/ε
≤ √ √ = ,
n n n
where we used Lemma 4.8 and (4.21).
To prove the bound on the mean squared error, we first use a bias–
variance decomposition to write
E(S(µ ,ν )−S(µ,ν))2 = var(S(µ ,ν ))+|ES(µ ,ν )−S(µ,ν)|2
n n n n n n
= var(S(µ ,ν ))+O(n−2).
n n
It therefore suffices to show that the variance of S(µ ,ν ) is O(n−1).
n n
For this purpose, we employ the Efron–Stein inequality. More pre-
cisely, [BLM13, Corollary 3.2] is sufficient for our purposes. It says that4.4 Statistical rates for primal solutions 123
if f = f(Z ,...,Z ) is a function of independent random variables that
1 m
satisfies the bounded differences inequality:
|f(z ,...,z )−f(z ,...,z ,z′,z ,...,z )| ≤ 2c (4.23)
1 m 1 i−1 i i+1 m
for all z ,...,z ,z′ and all i ∈ [m], then var(f) ≤ c2m.
1 m i
Let us view S(µ ,ν ) as a function of the m = 2n independent ran-
n n
dom variables X ,...,X ,Y ,...,Y . Fix (X ,...,X ) = (x ,...,x )
1 n 1 n 2 n 2 n
and (Y 1,...,Y n) = (y 1,...,y n), and view the dual functional Φ(cid:98) = Φ(cid:98)x1
as a function of the value of X = x alone. Then for any x ∈ Ω,
1 1 1
under Assumption 4.4, Proposition 4.5 implies that the maximizer of
the dual functional Φ(cid:98)x1 over C b⊕ is achieved at an h satisfying ∥h∥
∞
≤ 2.
Therefore
| hs ∈u Cp b⊕Φ(cid:98)x1(h)− hs ′∈u Cp b⊕Φ(cid:98)
x′
1(h′)| ≤
h∈C
b⊕s ,u ∥hp ∥∞≤2|Φ(cid:98)x1(h)−Φ(cid:98)
x′
1(h)|
2εe2/ε+4
≤ =: 2c,
n
where we have used the fact that each term in (4.14) is bounded.
Repeating this argument for X ,...,X ,Y ,...,Y , we obtain that
2 n 1 n
S(µ ,ν ) satisfies (4.23) with
n n
εe2/ε+2 2εe2/ε
c = ≤ ,
n n
since εe2/ε > 2 for all ε > 0.
Applying the Efron–Stein inequality, we obtain
8ε2e4/ε
var(S(µ ,ν )) ≤ 2c2n ≤ ,
n n
n
as claimed. ⊔⊓
We now conclude with an analogous sample complexity result for
the entropic map b⋆.
Recall from (4.6) that the density of γ⋆ with respect to µ⊗ν is
p⋆ = e(h⋆−c)/ε.
Similarly,letpˆ=
e(hˆ−c)/ε
denotethedensityofγˆ withrespecttoµ ⊗ν .
n n
Note that thanks to canonical extensions, these two functions may be
defined on the whole space.124 4 Entropic optimal transport
In the sequel, we use the fact that these densities are uniformly
bounded. Indeed, from Proposition 4.5 and Assumption 4.4,
∥pˆ∥ ,∥p⋆∥ ≤ e2/ε. (4.24)
L∞ L∞
With these definitions, we have the identities
(cid:90)
b⋆(x) = yp⋆(x,y)ν(dy),
(cid:90)
ˆb(x) = ypˆ(x,y)ν (dy).
n
The following bound holds.
Theorem 4.10.Adopt Assumption 4.4. The empirical entropic map
satisfies
1
E∥b⋆−ˆb∥2 ≲ ,
L2(µn) n
where the implicit constant depends exponentially on 1/ε.
Proof. Fix x ∈ Ω. Young’s inequality and Jensen’s inequality imply
∥b⋆(x)−ˆb(x)∥2
(cid:90) (cid:90)
≤ 2(cid:13) (cid:13) yp⋆(x,y)(ν −ν n)(dy)(cid:13) (cid:13)2 +2(cid:13) (cid:13) y(p⋆−pˆ)(x,y)ν n(dy)(cid:13) (cid:13)2
(cid:90) (cid:90)
≤ 2(cid:13) (cid:13) yp⋆(x,y)(ν −ν n)(dy)(cid:13) (cid:13)2 +2 ∥y∥2|(p⋆−pˆ)(x,y)|2ν n(dy)
(cid:90)
≤ 2(cid:13) (cid:13) yp⋆(x,y)(ν −ν n)(dy)(cid:13) (cid:13)2 +2∥p⋆(x,·)−pˆ(x,·)∥2 L2(νn),
where in the last inequality we use the fact that ∥y∥ ≤ 1 on the support
of ν , by Assumption 4.4. We therefore obtain
n
∥b⋆−ˆb∥2
L2(µn)
n (cid:90)
≤ n2 (cid:88)(cid:13) (cid:13) yp⋆(X i,y)(ν −ν n)(dy)(cid:13) (cid:13)2 +2∥p⋆−pˆ∥2 L2(µn⊗νn).
i=1
To control the first term, observe that
(cid:90)
E(cid:2)(cid:13) (cid:13) yp⋆(X i,y)(ν −ν n)(dy)(cid:13) (cid:13)2 (cid:12) (cid:12) X i(cid:3)
= 1 E(cid:2)(cid:13) (cid:13)Y 1p⋆(X i,Y 1)−E[Y 1p⋆(X i,Y 1)](cid:13) (cid:13)2 (cid:12) (cid:12) X i(cid:3) ≤ e4/ε ,
n n4.4 Statistical rates for primal solutions 125
where we used Assumption 4.4 and (4.24).
To control the second term, we use the fact that the exponential
function ex is eM-Lipschitz on (−∞,M] for any M. Hence, using As-
sumption 4.4 and Proposition 4.5, we get also that
|p⋆(x,y)−pˆ(x,y)| ≤ e2/ε|h⋆(x,y)−hˆ(x,y)| ∀x,y ∈ Ω.
Therefore
ε2e14/ε
E∥p⋆−pˆ∥2 ≤ e4/εE∥h⋆−hˆ∥2 ≤ ,
L2(µn⊗νn) L2(µn⊗νn) n
by Lemma 4.8. We have proved that
2e4/ε 2ε2e14/ε 1
E∥b⋆−ˆb∥2 ≤ + ≲ .
L2(µn) n n n
⊔⊓
The preceding theorem gives a bound on the empirical entropic map
in expected L2(µ ) norm. At the price of a larger constant factor, it is
n
also possible to obtain a bound in L2(µ).
Theorem 4.11.Adopt Assumption 4.4. The empirical entropic map
satisfies
1
E∥b⋆−ˆb∥2 ≲ ,
L2(µ) n
where the implicit constant depends exponentially on 1/ε.
Proof. As in the proof of Theorem 4.10, we have the pointwise bound
∥b⋆(x)−ˆb(x)∥2
(cid:13)(cid:90) (cid:13)2
≲ (cid:13) yp⋆(x,y)(ν −ν )(dy)(cid:13) +∥h⋆(x,·)−hˆ(x,·)∥2 .
(cid:13) n (cid:13) L2(νn)
Integrating with respect to µ and taking expectation, we obtain
(cid:90)
1
E∥b⋆−ˆb∥2 ≲ +E ∥h⋆(x,·)−hˆ(x,·)∥2 .
L2(µ) n L2(µ⊗νn)
It is thus sufficient to establish that
1
E∥h⋆−hˆ∥2 ≲ . (4.25)
L2(µ⊗νn) n126 4 Entropic optimal transport
To that end, we use the fact that the logarithm and exponential func-
tions are locally Lipschitz, with Lipschitz constant depending on the
magnitude of the arguments. In particular, we recall the elementary
inequalities
emin{a,b}|a−b| ≤ |ea−eb| ≤ emax{a,b}|a−b|,
|a−b|
which also imply the bound |loga−logb| ≤ for a,b > 0.
min{a,b}
Recall that
(cid:90)
hˆ(x,y) = fˆ(x)+gˆ(y) = −εlog e(gˆ(y)−∥x−y∥2)/εν (dy)+gˆ(y),
n
and
(cid:90)
h⋆(x,y) = f⋆(x)+g⋆(y) = −εlog e(g⋆(y)−∥x−y∥2)/εν(dy)+g⋆(y)
(cid:90)
= −εlog e(g⋆(y)−∥x−y∥2)/εν (dy)+g⋆(y)+∆(x),
n
where
(cid:90) (cid:90)
∆(x) = εlog
e(g⋆(y)−∥x−y∥2)/εν
(dy)−εlog
e(g⋆(y)−∥x−y∥2)/εν(dy).
n
Moreover, we may assume that (cid:82) gˆdν = (cid:82) g⋆dν without loss of
n n
generality since dual solutions are defined up to an additive constant.
UsingtheLipschitzpropertieslistedabovetogetherwithAssumption4.4
and Proposition 4.5, we get
(cid:90)
|hˆ(x,y)−h⋆(x,y)| ≲ |gˆ−g⋆|dν +|gˆ(y)−g⋆(y)|
n
(cid:12)(cid:90) (cid:12)
+(cid:12) e(g⋆(y)−∥x−y∥2)/ε(ν −ν)(dy)(cid:12).
(cid:12) n (cid:12)
Using Jensen’s inequality and a trivial variance bound for the average
of independent and bounded random variables, we finally obtain
1
E∥hˆ −h⋆∥2 ≲ E∥gˆ−g⋆∥2 + .
L2(µ⊗νn) L2(νn) n
Finally, note that
(cid:90)
∥hˆ −h⋆∥2 = (cid:2) (fˆ−f⋆)⊕(gˆ−g⋆)(cid:3)2 d(µ ⊗ν )
L2(µn⊗νn) n n4.5 Discussion 127
= ∥fˆ−f⋆∥2 +∥gˆ−g⋆∥2
L2(µn) L2(νn)
(cid:90) (cid:90)
+2 (fˆ−f⋆)dµ (gˆ−g⋆)dν
n n
≥ ∥gˆ−g⋆∥2 ,
L2(νn)
where in the last inequality we used the fact that (cid:82) gˆdν = (cid:82) g⋆dν .
n n
Hence we have proved that
1
E∥hˆ −h⋆∥2 ≲ E∥hˆ −h⋆∥2 + .
L2(µ⊗νn) L2(µn⊗νn) n
Together with Lemma 4.8, it completes the proof of (4.25), and hence
of the theorem. ⊔⊓
The conclusion of this section is quite striking: non-parameteric
quantities can be estimated at a parametric rate. An inspection of the
proofs of these results indicates that strong convexity is key to achieve
such a result. In retrospect it is not surprising that the empirical risk
minimizer of a strongly convex functional should enjoy such dimension-
free rates. Indeed, stochastic gradient descent on such an objective does
(see, e.g., [KNS16, Theorem 4]). This phenomenon is not new: it is
known for specific losses such as the ones employed in Chapter 8 and
was observed in [EHL18] for example.
4.5 Discussion
§4.1.Entropicoptimaltransportwasfirstpopularizedforcomputational
purposes in [Cut13]. See [PC19b] for an introduction to computational
optimal transport which nicely complements our treatment of statisti-
cal optimal transport. Entropic optimal transport between Gaussians
(Exercise 4) was computed in [JMPC20, MGM22].
Besides statistical applications, entropic optimal transport has also
been used to establish mathematical results for unregularized optimal
transport [FGP20, GLRT20, CP23].
§4.2. The convergence of Sinkhorn’s algorithm is discussed in many
places, see [KLRS08, ANWR17, DGK18, DBTHD21, L´eg21, AFKL22,
GN22, BB23, GNCD23, CDV24].
§4.3. The proofs in this section are based on [RS22]. Note that the
bounds obtained here are dimension-free, but scale exponentially w.r.t.128 4 Entropic optimal transport
1/ε. The sample complexity of entropic optimal transport was first es-
tablished by [GCB+19], who proved bounds that scaled as eO(1/ε)ε−O(d).
Later, [MNW19] observed that it was possible to slightly modify their
argument to remove the exponential factor. These bounds can be better
in low dimension, but provide poor control when the dimension is large.
Recent works have also focused on obtaining bounds which instead
scale as ε−O(d⋆), where the parameter d⋆ denotes the intrinsic dimen-
sionality of the measures (in fact, the minimum intrinsic dimension
among µ and ν). See Exercise 8 and [Str23] for an approach close to
the one taken here, and [GH23] for an empirical process argument.
The techniques used in this section can be used not only to prove
sample complexity bounds, but also to obtain distributional lim-
its [MNW19, dBSLNW23, GSLNW24]. Such bounds were originally
obtained in the discrete case by [BCP19a, KTM20].
Onenotablequirkaboutentropicoptimaltransportisthatingeneral,
S(µ,µ) > 0 due to the presence of the entropic term in the objective. In
light of this, Genevay et al. [GPC18] proposed the “debiased” quantity
D(µ,ν) := S(µ,ν)− 1 (S(µ,µ)+S(ν,ν)), called the Sinkhorn diver-
2
gence between µ and ν. It can be shown that the Sinkhorn divergence
is convex in each of its variables, non-negative, and vanishes if and only
if µ = ν [FSV+19]. Like entropic optimal transport, the Sinkhorn diver-
gence can be estimated at a parametric rate [GCB+19, dBSLNW23].
The Sinkhorn divergence has been advocated as tool for estimating
Wasserstein distances [CRL+20], although there are caveats when using
it for map estimation [PCNW22].
§4.4. Theorem 4.11 provides a rate for estimating the entropic map b⋆,
but combined with an approximation result quantifying the distance
between b⋆ and the true optimal transport map T, one can use ˆb as a
computationally efficient estimator for T (c.f. [PNW22]). Although it is
not minimax in general, it is in the semi-discrete case [PDNW23].
As the alternative nomenclature “entropic regression function” indi-
cates, the entropic map also solves a regression problem with respect to
the entropic coupling γ⋆; indeed, b⋆ = argmin E ∥Y −f(X)∥2.
f:Rd→Rd γ⋆
Analyzing this regression problem when the minimization is taken over
a smaller class of candidate regression functions rather than all maps
from Rd → Rd is an open problem.4.6 Exercises 129
4.6 Exercises
1. a) Let Ω be a compact subset of Rd with positive Lebesgue measure.
Show that the uniform measure on Ω has the largest differential
entropy of any probability measure on Ω.
b) Let m be a positive integer. Show that the uniform measure on
[m] has the largest Shannon entropy of any probability measure
on [m].
2. a) Show that if µ, ν, and γ are absolutely continuous, and γ ∈ Γ ,
µ,ν
then KL(γ ∥µ⊗ν) = Ent(µ)+Ent(ν)−Ent(γ). Conclude that
if µ and ν are absolutely continuous, then the optimization
problem (4.1) is equivalent to (4.3).
b) Show by an analogous calculation that if µ and ν are discrete,
then (4.2) is equivalent to (4.3).
3. Let γ denote the entropic optimal transport plan between µ and
ε
ν, with corresponding potentials f , g . Define φ := 1 ∥·∥2 −f .
ε ε ε 2 ε
Compute the derivatives of φ and conclude that
ε
∇φ (x) = E [Y | X = x], (4.26)
ε γε
∇2φ (x) = ε−1cov(Y | X = x). (4.27)
ε
γε
In particular, since we expect that φ converges to the unregularized
ε
Brenier potential φ as ε → 0 (proven rigorously in [NW22]), and
φ is convex by (4.27), this gives another explanation for Brenier’s
ε
Theorem 1.16.
4. Computetheentropicoptimaltransportsolution(i.e.,thepotentials,
theplan,thecost)betweentwoGaussians.Hint:asyoumightexpect,
the entropic potentials are quadratic functions.
5. In this exercise, we present another view on the Sinkhorn itera-
tions (4.11), (4.12). Consider the joint distributions
γ (dx,dy) ∝
exp(cid:0)
(f (x)+g
(y)−∥x−y∥2)/ε(cid:1)
µ(dx)ν(dy),
t−1 t−1 t
2
γ (dx,dy) ∝
exp(cid:0)
(f (x)+g
(y)−∥x−y∥2)/ε(cid:1)
µ(dx)ν(dy).
t t t
Here, we take γ (dx,dy) ∝ exp(−∥x − y∥2/ε)µ(dx)ν(dy). Show
0
that the Sinkhorn updates correspond to iteratively “fixing the
marginals”; i.e., γ is obtained from γ by keeping the condi-
t−1 t−1
2
tional distribution of X | Y fixed but setting the Y-marginal to ν,
and γ is obtained from γ by keeping the conditional distribution
t t−1
2
of Y | X fixed but setting the X-marginal to µ.130 4 Entropic optimal transport
6. In the discrete setting where µ, ν are finitely on {x ,...,x } and
1 m
{y ,...,y } respectively, Sinkhorn’s algorithm shows that there are
1 n
positive scalings κ ∈ Rm, λ ∈ Rn of the rows and columns of the
+ +
matrix M with entries M = exp(−∥x −y ∥2/ε), such that the
i,j i j
scaled matrix diag(κ)M diag(λ) has marginals µ and ν respectively;
see [PC19b] for details.
As a special case, suppose that µ, ν are uniformly distributed
and m = n. Then, the scaled matrix M˜ := diag(κ)M diag(λ) has
marginals µ and ν if and only if nM˜ is doubly stochastic, i.e.,
it belongs to the Birkhoff polytope (1.2). In this case, prove the
existence of these scalings for any matrix M with positive entries
by considering the KL minimization problem
n
(cid:88)(cid:0) γ i,j (cid:1)
minimize γ log −γ +M .
i,j i,j i,j
γ∈Birk M i,j
i,j=1
Namely, show that a solution to this problem exists, and using
Lagrange multipliers, show that γ is of the form diag(κ)M diag(λ)
for positive scalings κ, λ.
7. The strong convexity arguments in Section 4.3 are designed to avoid
the use of empirical process theory. However, this exercise shows
how to use empirical process theory to prove sample complexity
bounds using techniques analogous to those in Section 2.3. Unlike
the approach in Section 4.3, these bounds depend polynomially
on 1/ε, but with exponent scaling with d. For simplicity, we focus
on the one-sample problem, and prove bounds on the quantity
S(µ ,ν)−S(µ,ν).
n
a) Let f and g be solutions to (ε-D-W2) for any pair of measures
2
supported on Ω = B (0). Let s be a positive integer. Arguing
1/2
as in Exercise 3, show that there exists a positive constant C
s
such that for all multi-indices α = (α ,...,α ) with |α| = s, we
1 d
have the bound
sup|∂ f(x)| ≤ C ε1−s.
α s
x∈Ω
Arguethatwecanassumethatf(0) = 0withoutlossofgenerality,
and thereby obtain the bound sup |f(x)| ≤ C for some
x∈Ω 0
positive constant C .
0
b) For L > 0, define
s
(cid:110) (cid:88) (cid:88) (cid:111)
F (L) := f : Ω → Rd : ∥∂ f∥ ≤ L .
s α L∞(Ω)
k=0α:|α|=k4.6 Exercises 131
Fix a positive integer s. Argue that there exists a constant
C = C(s) such that for L = C(1+ε1−s), we have
(cid:12)(cid:90) (cid:90) (cid:12)
(cid:12) (cid:12)
|S(µ n,ν)−S(µ,ν)| ≤ sup (cid:12) fdµ n− fdµ(cid:12) .
f∈F s(L)(cid:12) (cid:12)
Hint: let f⋆ and g⋆ be solutions to (ε-D-W2) for µ and ν, and let
2
fˆand gˆ be the solutions for µ and ν. Argue that h⋆ = f⋆⊕g⋆
n
and hˆ = fˆ⊕gˆ satisfy
(cid:90) (cid:90)
S(µ n,ν)−S(µ,ν) = Φ(cid:98)(hˆ)−Φ(h⋆) ≤ fˆdµ n− fˆdµ,
and analogously
(cid:90) (cid:90)
S(µ,ν)−S(µ ,ν) ≤ f⋆dµ− f⋆dµ .
n n
Then apply part (a).
c) It can be shown that logN(δ,F (L)) ≲ (L/δ)d/s, and moreover
s
thatthisboundholdsalsoforfractionals,whereF isinterpreted
s
asasuitableHo¨lderspace.Takings = d/2+1,useProposition2.6
to conclude
E|S(µ ,ν)−S(µ,ν)| ≲ (1+ε−d/2)n−1/2.
n
8. The statistical results in Section 4.3 rely on pointwise bounds on
the density p⋆. Here, we show a different way to control ∥p⋆∥ ,
L2(µ⊗ν)
which provides an entry point into [Str23].
a) Argue that the dual potentials f⋆, g⋆ are O(1)-Lipschitz, and
that logp⋆ is O(ε−1)-Lipschitz. (Here, we are still working over
a bounded domain.)
b) Prove that for all δ > 0, (cid:82) ν(B(z,δ))−1ν(dz) ≤ N(δ/4,suppν),
where N(δ/4,suppν) is the covering number of suppν at scale
δ/4. (Let z ,...,z ∈ suppν be a δ/2-covering of suppν with
1 K
K ≤ N(δ/4,suppν). Bound the integral by summing over the
integals over B(z ,δ/2) for k = 1,...,K.)
k
c) Using the fact that
(cid:90) p⋆(x,y′) (cid:90) p⋆(x,y′)
1 = p⋆(x,y) ν(dy′) ≥ p⋆(x,y) ν(dy′)
p⋆(x,y) p⋆(x,y)
B(y,r)
and the log-Lipschitz property from (a), show that p⋆(x,y) ≲
ν(B(y,r))−1, where r ≍ ε.132 4 Entropic optimal transport
d) Combiningthiswiththeestimatein(b),provethat∥p⋆∥ ≲
L2(µ⊗ν)
(cid:112)
N(r′,suppν) where r′ ≍ ε. Explain why this implies that if
suppµ is d -dimensional and suppν is d -dimensional, then
µ ν
∥p⋆∥ ≲ ε−(dµ∧dν)/2.
L2(µ⊗ν)5
Wasserstein gradient flows: theory
We have seen in Proposition 1.3 that P (Rd), once endowed with the
2
W distance, has the structure of a metric space. It has in fact a much
2
richer geometric structure, as it resembles a Riemannian manifold.
Consequently, we can bring to bear the calculation rules of Riemannian
geometry, known in this context as Otto calculus, on the design and
interpretation of algorithms over the space of probability measures.
The identification of P (Rd) with a Riemannian manifold is purely
2
“formal” (that is, heuristic). For instance, P (Rd) is not locally homeo-
2
morphic to a Hilbert space. However, the Riemannian view of P (Rd) is
2
nevertheless a powerful tool for understanding the geometric properties
of the Wasserstein space.
To elucidate this Riemannian viewpoint, we work with absolutely
continuous measures in this chapter, for which the Riemannian formal-
ismcanbeputonamorerigorousfooting.Ourmaingoalinconstructing
thisformalismistodefineinterestingdynamicsontheWassersteinspace,
givenbygradient flows.Havingdefinedthesedynamics,weshallseethat
they often make sense even for non-absolutely continuous measures—in
particular, they give rise to well-defined dynamics for discrete measures,
viewedasparticlesystems.Oncederived,thesenon-trivialdynamicscan
be studied directly for discrete measures without the need for making
rigorous sense of the Riemannian calculations in the discrete case. The
reader interested in seeing a fully rigorous derivation of gradient flows
forgeneralmeasuresshouldconsulttheseminalmonographofAmbrosio,
Gigli, and Savar´e [AGS08], or [San17] for a quick overview.134 5 Wasserstein gradient flows: theory
5.1 Metric derivative and the continuity equation
The utility of optimal transport lies in its endowment of the space of
probability measures with a geometric structure which respects that of
the underlying space. For example, we saw that the mapping x (cid:55)→ δ
x
is an isometric embedding of (Rd,∥·∥) into (P (Rd),W ). Accordingly,
2 2
as we now seek to understand dynamics on P (Rd), our approach is to
2
“lift” the corresponding dynamics of particles on Rd.
The general way to prescribe dynamics on Rd using differential
calculus is via ordinary differential equations (ODEs). Namely, given a
time-dependent family of vector fields (v ) , consider the ODE
t t≥0
X˙ = v (X ). (5.1)
t t t
Under standard assumptions on (v ) ,1 there is a unique solution
t t≥0
to the ODE for any given initial condition X . Suppose now that X
0 0
is drawn randomly from a measure µ ∈ P (Rd), and similarly let µ
0 2 t
denote the law of X for all t ≥ 0. We think of the curve of measures
t
(µ ) as describing the evolution of a collection of particles. Then,
t t≥0
the dynamics of (µ ) is described by a partial differential equation
t t≥0
(PDE), known as the continuity equation.
Proposition 5.1(Continuity equation). Suppose that X ∼ µ , and
0 0
that (X ) evolves according to the dynamics (5.1), which we assume
t t≥0
is well-posed. Let µ denote the law of X for all t ≥ 0. Then, (µ )
t t t t≥0
satisfies the following equation in the weak sense,
∂ µ +div(µ v ) = 0, (5.2)
t t t t
i.e., for all compactly supported and smooth test functions φ : Rd → R,
it holds that
(cid:90) (cid:90)
∂ φdµ = ⟨∇φ,v ⟩dµ . (5.3)
t t t t
The equation (5.3), when written in probabilistic language, reads
∂ Eφ(X ) = E⟨∇φ(X ),v (X )⟩, and it simply follows from (5.1) and
t t t t t
the chain rule. The real content of the proposition actually lies in (5.2):
when µ admits a smooth density w.r.t. Lebesgue measure, which by an
t
1 Forexample,ifthevectorfieldsareLipschitzuniformlyintime,thenwell-posedness
follows from the Cauchy–Lipschitz theorem.5.1 Metric derivative and the continuity equation 135
abuse of notation we denote also by µ , then integration by parts yields
t
the equation
(cid:90) (cid:90) (cid:90) (cid:90)
φ∂ µ = ∂ φdµ = ⟨∇φ,v ⟩µ = − φdiv(µ v ).
t t t t t t t t
Since this equality is supposed to hold for all suitable test functions φ,
it follows that (5.2) holds pointwise. To summarize, we see that (µ )
t t≥0
solves the PDE (5.2), at least when µ admits a smooth density for all
t
t ≥ 0. In general, it is more convenient to make statements that hold
for curves (µ ) without knowing in advance the regularity of µ , in
t t≥0 t
which case (5.2) should be interpreted to hold in the weak sense (5.3).
However, for the sake of developing the framework of Otto calculus
unencumbered by technical distractions, from now on we ignore such
issues of regularity and pretend that we are working with curves of
smooth densities. See [AGS08] for a more rigorous treatment.
The equations (5.1) and (5.2) provide us with dual perspectives on
the same dynamics; in the field of fluid dynamics, these perspectives
are known as Lagrangian and Eulerian respectively. The Lagrangian
perspective describes the evolution of individual particle trajectories,
whereas the Eulerian perspective tracks the evolution of aggregate
quantities through the notions of mass density µ and velocity field v .
t t
To foreshadow the development of geometry over P (Rd), let us first
2
examine how to develop geometry over Rd; for now, we refer to concepts
from Riemannian geometry loosely, but we return to the subject in
Section 5.2. For a single particle trajectory t (cid:55)→ X evolving according
t
to (5.1), the kinetic energy at time t (assuming the particle has unit
mass) is ∥X˙ ∥2 = ∥v (X )∥2. The total energy of the curve over the time
t t t
interval [0,1] is (cid:82)1 ∥v (X )∥2dt, and if we minimize this energy over
0 t t
all curves (X ) with endpoints fixed at X and X , we obtain the
t t∈[0,1] 0 1
constant-speed geodesic t (cid:55)→ X := (1−t)X +tX . Geometrically, we
t 0 1
think of v (X ) as the tangent vector to the curve (X ) at time t,
t t t t∈[0,1]
and we measure its length using the Euclidean norm ∥·∥.
WenowtrytoliftthispicturetoP (Rd).Foracurve(µ ) evolving
2 t t≥0
according to (5.2), it is natural to think of the velocity vector field
v : Rd → Rd as an abstract “tangent vector” to (µ ) at time t, and
t t t≥0
to measure its squared “length” via the kinetic energy2
(cid:90)
∥v ∥2 := ∥v ∥2dµ . (5.4)
t µt t t
2 Sinceµ playstheroleofamassdensity,then∥v ∥2µ isthekineticenergydensity,
t t t
and integrating this over all of space yields the kinetic energy.136 5 Wasserstein gradient flows: theory
However, we immediately arrive at an obstacle in doing so: given a
curve (µ ) , there is not a unique choice of vector fields (v ) for
t t≥0 t t≥0
which (5.2) holds, and hence it is unclear what vector field v to choose
t
as our tangent vector. Indeed, if we start with any family of vector fields
(v ) for which (5.2) holds, and if w satisfies div(µ w ) = 0 for all
t t≥0 t t t
t ≥ 0, then (v +w ) is another family of vector fields for which (5.2)
t t t≥0
holds by linearity of the divergence operator. To see a concrete example
ofnon-uniqueness,supposethatµ isthestandardGaussiandistribution
t
on R2 for all t ≥ 0. Then, one natural choice of vector fields is to take
v = 0 for all t ≥ 0; however, due to the rotational invariance of the
t
standard Gaussian, another choice is to choose vector fields inducing a
rotation (see Figure 5.1).
Zero vector field Rotation vector field
Fig. 5.1. Two vector fields which preserve the standard Gaussian on R2.
WeseethatthevectorfieldontherightofFigure5.1inducesextrane-
ous motion for the particles and is therefore not the most parsimonious
explanation for the dynamics of (µ ) . To resolve the ambiguity in
t t≥0
the choice of vector fields, we can elect to declare as our tangent vector
the vector field which minimizes the kinetic energy (5.4) while still ex-
plaining the dynamics of (µ ) . As discussed below, the minimization
t t≥0
of kinetic energy falls naturally in line with the philosophy of “optimal”
transport of mass.
To further motivate this choice, we introduce the notion of the
metric derivative of a curve (x ) in a metric space (S,d). Although
t t≥0
in general we cannot make sense of the notion of a tangent vector (or
the “derivative”) of a curve in a general metric space, it turns out that
we can make sense of its speed.5.1 Metric derivative and the continuity equation 137
Definition 5.2(Metric derivative). Let (S,d) be a metric space and
let (x ) be a curve in S. Then, the metric derivative of the curve at
t t≥0
time t is given by
d(x ,x )
|x˙|(t) := lim s t ,
s→t,s̸=t |s−t|
provided that the limit exists.
The next theorem shows that our selection principle produces a
well-defined choice of tangent vector v which can be characterized in
t
any one of three ways: (1) as the vector field v with minimal kinetic
t
energy subject to the constraint (5.2); (2) as the unique choice of vector
field solving (5.2) with length ∥v ∥ equal to the metric derivative
t µt
|µ˙|(t); (3) as a limit of Brenier maps.
However,letusfirstintroducetheconceptoftheflow map associated
with the ODE (5.1) (or equivalently, with the family of vector fields
(v ) ). The map F : Rd → Rd is defined as the map which, given
t t≥0 0,t
X ∈ Rd, outputs the solution X to the ODE (5.1) at time t when
0 t
started at X . In an analogous manner, we can define the flow map
0
F : Rd → Rd for any pair of times 0 ≤ s ≤ t. The significance of this
s,t
definition is that it shifts our attention away from thinking about the
ODE as describing a single trajectory, and instead views the effect of
the ODE as a deformation of the entire space Rd.
Theorem 5.3.Let (µ ) be a regular3 curve of probability measures.
t t≥0
Then, for every family of vector fields (v ) for which (5.2) holds, we
t t≥0
have |µ˙|(t) ≤ ∥v ∥ for all t ≥ 0.
t µt
Conversely, there exists a unique family (v ) such that (5.2) holds
t t≥0
and for which |µ˙|(t) = ∥v ∥ for every t ≥ 0. This family is such that
t µt
T −id
v = lim µt→µ t+h in L2(µ ). (5.5)
t t
h↘0 h
Proof sketch. We start with the first statement. Let (X ) be the
t t≥0
curve of random variables with X ∼ µ solving X˙ = v (X ). Since
0 0 t t t
the continuity equation (5.2) holds by assumption, then X ∼ µ for
t t
3 Here,“regular”canbetakentomeanthatµ ∈P (Rd)andadmitsadensity,and
t 2
thatthemetricderivative|µ˙|(t)existsforallt≥0.Thequalifier“forallt≥0”in
theassumptionsandconclusionscanbereplacedby“foralmosteveryt≥0”,and
the assumed existence of a density can also be relaxed.138 5 Wasserstein gradient flows: theory
all t ≥ 0, and in particular, µ = (F ) µ . We can upper bound
t+h t,t+h # t
W (µ ,µ ) using this suboptimal coupling:
2 t t+h
W2(µ ,µ ) (cid:104)∥F (X )−X ∥2(cid:105)
2 t t+h ≤ E t,t+h t t .
h2 h2
Observe that F (X ) = X +hv (X )+o(h) so that letting h → 0,
t,t+h t t t t
we obtain |µ˙|(t) ≤
(cid:112)E[∥v
(X )∥2] = ∥v ∥ .
t t t µt
Note that the inequality above arises from the use of a suboptimal
coupling. Intuitively, if we take X and X to be optimally coupled
t t+h
and thereby define v according to (5.5), then we ought to obtain an
t
equality. This is in fact the case but we omit the proof.
Finally,bycombiningthetwostatements,wededucethattheoptimal
choice v satisfies
t
v = argmin ∥v +w ∥ s.t. div(µ w ) = 0.
t t t µt t t
vt+wt:Rd→Rd
Since this is a strictly convex problem, the minimizer is unique. ⊔⊓
In the above theorem, we used the fact that µ admits a density in
t
order to write (5.5), i.e., to assert that the optimal transport map exists.
In order to facilitate the discussion, we restrict to this class of measures
from now on, although we return to the subject of particle methods at
the end of the chapter.
Definition 5.4.P (Rd) is the class of probability measures over Rd
2,ac
with finite second moment and which are absolutely continuous (i.e.,
admit a density w.r.t. Lebesgue measure).
5.2 Elements of Riemannian geometry
Before proceeding further, we provide a brief and informal exposition
to the concepts from Riemannian geometry that we need.
A manifold M is a set which is locally homeomorphic to a Euclidean
space Rd. At each point p ∈ M, we can associate a tangent space T M,
p
whichis a d-dimensional vectorspaceand represents all possible velocity
vectors of curves passing through p. A Riemannian metric is a choice
of an inner product g on each tangent space T M, and once endowed
p p
with a Riemannian metric, the manifold is then called a Riemannian
manifold.ToemphasizetheHilbertianstructure,weusuallysimplywrite5.2 Elements of Riemannian geometry 139
⟨·,·⟩ for the metric g . Usually, one imposes additional smoothness
p p
assumptions for these objects in order to properly build up a theory of
differentialcalculus,butherewefocusonintroducingthebasiclanguage
without delving into details. In the case of the Wasserstein space, note
that we have already identified a natural norm for a “tangent vector”
(cid:112)
(velocity vector field) v at µ —the L2 norm, g (v ,v ) = ∥v ∥ —
t t µt t t t µt
indicating the possibility of identifying further Wasserstein analogues
of Riemannian theory.
The next important concept is that of a geodesic. For a curve
(p ) , let p˙ ∈ T M denote the tangent vector at time t. Given
t t∈[0,1] t pt
p ,p ∈ M, geodesics or shortest paths4 between p and p are obtained
0 1 0 1
by solving either of the following variational problems,
(cid:90) (cid:90)
min ∥p˙ ∥ dt or min ∥p˙ ∥2 dt
(pt)
t∈[0,1]
t pt
(pt)
t∈[0,1]
t pt
over curves (p ) joining p to p . In the first problem, the objective
t t∈[0,1] 0 1
functional is the arc length of the curve; in the second problem, the
objective functional is called the energy. The second variational problem
is technically more convenient; this is because the arc length is invariant
under reparametrization (i.e., replacing (p ) by (p ) for any
t t∈[0,1] f(t) t∈[0,1]
continuousandstrictlyincreasingfunctionf : [0,1] → [0,1]).Incontrast,
the second variational problem singles out a specific parametrization of
the optimal curves, namely, curves with constant speed (i.e., t (cid:55)→ ∥p˙ ∥
t pt
is constant). Henceforth, we only consider constant-speed geodesics and
therefore omit the adjective “constant-speed”.
The value of the variational problems are d(p ,p ) and d2(p ,p )
0 1 0 1
respectively, where d is a metric (in the sense of metric spaces) induced
by the Riemannian metric ⟨·,·⟩.
The exponential map5 is a mapping exp : T M → M which maps a
p p
tangent vector v to p , where (p ) is the constant-speed geodesic
1 t t∈[0,1]
such that p = p and p˙ = v. The inverse map is called the logarithmic
0 0
map log : M → T M, which maps q (cid:55)→ exp−1(q). Actually, in general,
p p p
the exponential map may not be defined over all of T M because a
p
4 In Riemannian geometry, it is more customary to define geodesics to only be
locally length-minimizing, but here we always use the word “geodesic” to refer to
shortest paths.
5 The name is motivated by a classical example of a manifold, the set of orthogonal
matrices, in which the tangent space at the identity matrix is the set of anti-
symmetric matrices and the exponential map exp (A)=exp(A) coincides with
I
the matrix exponential.140 5 Wasserstein gradient flows: theory
geodesic, once extended too far, may no longer remain a shortest path
between its endpoints; think, for instance, of extending the geodesic
from the north pole to the south pole of the sphere.
Withtheideaofageodesicinhand,wecanthendefinetheconceptsof
convexity, gradients, and gradient flows, which form the building blocks
of optimization over curved spaces. We say that a set C ⊆ Rd is convex
if for all p ,p ∈ C and all t ∈ [0,1], it holds that (1−t)p +tp ∈ C.
0 1 0 1
In this definition, t (cid:55)→ (1−t)p +tp is the Euclidean geodesic joining
0 1
p to p . We can generalize this definition to Riemannian manifolds: we
0 1
say that C ⊆ M is geodesically convex if for all p ,p ∈ C, the geodesic
0 1
joining p to p also lies in C.
0 1
We can also define convexity for functions: given α ∈ R, a function
f : M → R is called α-geodesically convex if
αt(1−t)
f(p ) ≤ (1−t)f(p )+tf(p )− d2(p ,p ) (5.6)
t 0 1 0 1
2
for all t ∈ [0,1] and all geodesics (p ) . Equivalently, we have the
t t∈[0,1]
first-order condition
α
f(q) ≥ f(p)+⟨∇f(p),log (q)⟩ + d2(p,q) ∀p,q ∈ M
p p 2
where ∇f, the Riemannian gradient, is defined so that for all curves
(p ) , ∇f(p ) ∈ T M satisfies ∂ f(p ) = ⟨∇f(p ),p˙ ⟩ . Equivalently,
t t≥0 t pt t t t t pt
we also have the second-order condition
∇2f(p)[v,v] ≥ α∥v∥2 ∀p ∈ M, ∀v ∈ T M,
p p
where ∇2f, the Riemannian Hessian, can be defined via ∇2f(p)[v,v] :=
∂2f(p )| , where (p ) is the geodesic with p = p and p˙ = v.
t t t=0 t t∈[0,1] 0 0
In the next section, we return to P (Rd) which, despite not being
2,ac
a bona fide Riemannian manifold, carries enough structure to apply
calculationrulesfromRiemanniangeometry(andindeed,theformidable
book [AGS08] is devoted to the task of placing this endeavor on rigorous
footing). It leads to a toolbox, known as Otto calculus after Felix Otto,
for the study of gradient flows over the space of probability measures.
5.3 The Riemannian structure of Wasserstein space
We are now in a position to define a formal Riemannian structure over
P (Rd). Recall from Brenier’s theorem (Theorem 1.16) that optimal
25.3 The Riemannian structure of Wasserstein space 141
transport maps for the quadratic cost are gradients of convex functions.
From (5.5), it follows that optimal velocity vector fields are gradients of
functions (which are not necessarily convex, since we have subtracted
the identity map).
Definition 5.5.Let µ ∈ P (Rd). We define the tangent space to
2,ac
P (Rd) at µ to be
2,ac
L2(µ)
T P (Rd) := {∇ψ | ψ : Rd → R compactly supported, smooth}
µ 2,ac
where
{·}L2(µ)
denotes the L2(µ) closure. We endow T P (Rd) with
µ 2,ac
the L2(µ) inner product,
(cid:90)
⟨∇ψ ,∇ψ ⟩ := ⟨∇ψ ,∇ψ ⟩dµ.
1 2 µ 1 2
One can show that requiring v to be the gradient of a function,
t
v = ∇ψ , in fact furnishes a fourth characterization of the optimal
t t
vector field v in Theorem 5.3, thus justifying Definition 5.5, but we do
t
not prove this here.
Remark 5.6.We pause to describe a common alternative convention:
instead of defining the tangent vector at µ to be the driving velocity
t
vector field v , we could take it to be the ordinary time derivative ∂ µ
t t t
which is given by the continuity equation: ∂ µ = −div(µ ∇ψ) =: χ.
t t t
In this case, the tangent space becomes the space of signed measures
with zero total mass, and the metric becomes ⟨χ,χ′⟩ = (cid:82) ⟨∇ψ,∇ψ′⟩dµ,
µ
where ψ, ψ′ solve the equations χ = −div(µ∇ψ), χ′ = −div(µ∇ψ′).
This just amounts to a change of notation: ∇ψ (cid:55)→ χ is an isometry
between our convention and the alternative convention.
Although our logical development thus far has strongly hinted at a
connectionbetweenthisRiemannianstructureandthetheoryofoptimal
transport, we have not yet stated any result to this effect. The following
theorem computes the constant-speed geodesics in the metric defined
above. In the language of Section 5.2, it asserts that the metric induced
by the Riemannian structure we defined over P (Rd) is indeed the
2,ac
Wasserstein distance.
Theorem 5.7(Benamou–Brenier). Let µ ,µ ∈ P (Rd). Then,
0 1 2,ac
(cid:110)(cid:90) 1 (cid:12) (cid:111)
W2(µ ,µ ) = inf ∥v ∥2 dt (cid:12) (µ ,v ) solves (5.2) . (5.7)
2 0 1 t µt (cid:12) t t t∈[0,1]
0142 5 Wasserstein gradient flows: theory
The optimal curve (µ ) is unique and is described by X ∼ µ ,
t t∈[0,1] t t
where X = (1−t)X +tX and (X ,X ) ∼ γ¯ ∈ Γ with γ¯ being
t 0 1 0 1 µ0,µ1
an optimal coupling.
Proof. Let (µ ,v ) solve (5.2), and let X˙ = v (X ) with X ∼ µ .
t t t∈[0,1] t t t 0 0
Then, it holds that
(cid:104)(cid:13)(cid:90) 1 (cid:13)2(cid:105) (cid:90) 1
W2(µ ,µ ) ≤ E[∥X −X ∥2] = E (cid:13) X˙ dt(cid:13) ≤ E[∥X˙ ∥2]dt
2 0 1 0 1 (cid:13) t (cid:13) t
0 0
(cid:90) 1
= ∥v ∥2 dt.
t µt
0
This proves (5.7). To study the equality case, note that in the above
calculations we employed two inequalities. The first inequality is an
equality if and only if (X ,X ) are optimally coupled. The second
0 1
inequality is an equality if and only if t (cid:55)→ X˙ is constant, which forces
t
X˙ = X −X for all t ∈ [0,1]. ⊔⊓
t 1 0
Note that we can also write µ = [(1−t)id+tT] µ , where T is the
t # 0
optimal transport map from µ to µ . Hence, we formulate the following
0 1
definition.
Definition 5.8.Let µ ,µ ∈ P (Rd) and let T denote the optimal
0 1 2,ac
transport map from µ to µ . The constant-speed geodesic joining µ
0 1 0
to µ is the curve (µ ) , where
1 t t∈[0,1]
µ = [(1−t)id+tT] µ . (5.8)
t # 0
This curve is known as the displacement interpolation, McCann’s inter-
polation, or simply the Wasserstein geodesic joining µ to µ .
0 1
From (5.8), we can identify log (ν) = T − id, and hence
µ µ→ν
exp (∇ψ) = (id+∇ψ) µ. Note that the exponential map is not well-
µ #
defined if ∇2ψ has an eigenvalue smaller than −1, since then id+∇ψ is
not the gradient of a convex function and thus not an optimal transport
map. This reflects our earlier discussion that the exponential map is not
necessarily defined on the full tangent space of a Riemannian manifold.6
6 However, the domain of the exponential map for a Riemannian manifold always
contains a neighborhood of the origin, whereas this is not true for the Wasser-
stein space. This is one of the reasons why the Wasserstein space is not truly a
Riemannian manifold, even an infinite-dimensional one.5.4 Otto calculus 143
5.4 Otto calculus
The next step is to identify the Wasserstein gradient, which, in turn,
allow us to define Wasserstein gradient flows. After obtaining criteria
for functionals to be geodesically convex, we can then obtain rates of
convergence thereof.
It turns out that the Wasserstein gradient can be expressed in terms
of the first variation.
Definition 5.9(First variation). Let F : P (Rd) → R be a func-
2,ac
tional. The first variation of F at µ, denoted δF(µ) : Rd → R, is the
function defined by
F(µ+εχ)−F(µ) (cid:90)
lim = δF(µ)dχ, (5.9)
ε↘0 ε
for all perturbations χ such that µ+εχ ∈ P (Rd) for all sufficiently
2,ac
small ε.
If (µ ) is a curve of densities, then we can write the linear ap-
t t≥0
proximation µ ≈ µ + ε∂ µ for ε small, where ∂ µ denotes the
t+ε t t t t t
usual time derivative. We can take χ = ∂ µ , in which case (5.9) reads
t t
∂ F(µ ) = (cid:82) δF(µ )∂ µ .Notealsothatthefirstvariationisonlydefined
t t t t t
up to an additive constant, since the perturbations χ always satisfy
(cid:82)
dχ = 0.
Proposition 5.10.Let F : P (Rd) → R be a functional with first
2,ac
variation δF. Then, the Wasserstein gradient of F is the vector field
∇∇F(µ) : Rd → Rd defined by
∇∇F(µ) = ∇δF(µ),
where ∇ on the right-hand side denotes the usual Euclidean gradient.
Proof. Let (µ ) be a curve of measures with tangent vectors (v ) .
t t≥0 t t≥0
By definition, the Wasserstein gradient ∇∇F(µ ) is the element of
t
T P (Rd) such that ∂ F(µ ) = ⟨∇∇F(µ ),v ⟩ . The fact that v
µt 2,ac t t t t µt t
is the tangent vector at time t means that it solves the continuity
equation (5.2). From the above discussion of the first variation,
(cid:90) (cid:90)
∂ F(µ ) = δF(µ )∂ µ = − δF(µ )div(µ v )
t t t t t t t t144 5 Wasserstein gradient flows: theory
(cid:90)
= ⟨∇δF(µ ),v ⟩dµ = ⟨∇δF(µ ),v ⟩ .
t t t t t µt
Moreover,since∇δF(µ )isthegradientofafunction,fromDefinition5.5
t
we have ∇δF(µ ) ∈ T P (Rd). From this, we conclude that ∇δF(µ )
t µt 2,ac t
is indeed the Wasserstein gradient of F at µ . ⊔⊓
t
To compute the Wasserstein gradient, we therefore have to compute
the first variation and then take its gradient. We illustrate this on three
canonical examples of functionals over P (Rd).
2,ac
Example 5.11(Potential energy). Let F(µ) := (cid:82) V dµ for some (po-
tential) function V : Rd → R. Then, ∂ F(µ ) = (cid:82) V ∂ µ and we can
t t t t
identify δF(µ) = V. Thus, ∇∇F(µ) = ∇V.
Example 5.12(Internal energy). Let F(µ) := (cid:82) U(µ(x))dx for some
function U : R → R. For example, U(x) = xlogx gives rise to the
+
entropy7 functional. Then, ∂ F(µ ) = (cid:82) U′(µ )∂ µ , so we can identify
t t t t t
δF(µ) = U′◦µ and therefore ∇∇F(µ) = ∇(U′◦µ). In the case of entropy,
δF(µ) = logµ+1, and ∇∇F(µ) = ∇logµ.
Example 5.13(Interaction energy). Take a symmetric kernel K : Rd →
R, i.e., K(−z) = K(z). Set F(µ) := 1 (cid:82)(cid:82) K(x − y)µ(dx)µ(dy). For
2
∥x∥2
example, we could consider a Gaussian kernel K(x) = exp(− ).
Then,∂ F(µ ) = (cid:82)(cid:82) K(x−y)µ (dy)∂ µ (dx),sowecanidentifyδF(µ2σ )2 =
t t t t t
(cid:82) K(·−y)µ(dy), and ∇∇F(µ) = (cid:82) ∇K(·−y)µ(dy).
We can now define the Wasserstein gradient flow of a functional F
overP (Rd).Thegradientflowisacurveofmeasures(µ ) suchthat
2,ac t t≥0
the tangent vector to the curve at time t equals −∇∇F(µ ). Recalling
t
that the tangent vectors governs the evolution of (µ ) through the
t t≥0
continuity equation (5.2), we arrive at the following definition.
Definition 5.14(Wasserstein gradient flow). Let F : P (Rd) →
2,ac
R be a functional. Then, (µ ) is called the Wasserstein gradient flow
t t≥0
of F if it solves the PDE
∂ µ = div(cid:0) µ ∇∇F(µ )(cid:1) .
t t t t
As is well-understood in optimization, gradient flows are natural
dynamicsforminimizingtheobjectivefunctionalF because,asdiscussed
7 This is the negative of the thermodynamic entropy.5.4 Otto calculus 145
shortly, they always reduce the value of the objective. Wasserstein
gradient flows therefore constitute a principled approach for designing
dynamics over the space of probability measures aimed at minimizing
some criterion, a task which is ubiquitous in applied mathematics,
statistics, and beyond; see Chapter 6.
A quick calculation using the definition of the Wasserstein gradient
flow (µ ) of F yields
t t≥0
∂ F(µ ) = ⟨∇∇F(µ ),v ⟩ = −∥∇∇F(µ )∥2 (5.10)
t t t t µt t µt
where v = −∇∇F(µ ) is the tangent vector at time t. This equality,
t t
which states that the objective functional is dissipated at a rate equal
to the squared norm of the gradient, is a generic fact about gradient
flows. In particular, if F is bounded below, it implies that any limit
point of the gradient flow must be a stationary point of F.
However, we can say more once we have a quantitative lower bound
on the rate of dissipation. The simplest such condition is the Polyak–
L(cid:32) ojasiewicz (PL(cid:32) ) inequality.
Definition 5.15(Polyak–L(cid:32) ojasiewicz (PL(cid:32) ) inequality). We say
that F : P (Rd) → R satisfies a PL(cid:32) inequality with constant α > 0 if
2,ac
for all µ ∈ P (Rd),
2,ac
∥∇F(µ)∥2 ≥ 2α(F(µ)−infF).
µ
The PL(cid:32) inequality over Rd is discussed in Appendix A.2; the above
definition adapts this concept to the present setting. From (5.10), the
PL(cid:32) inequality yields
∂ (F(µ )−infF) ≤ −2α(F(µ )−infF).
t t t
Let ϕ(t) := F(µ )−infF, so that ϕ˙(t) ≤ −2αϕ(t). If this inequality
t
were an equality, then we could solve the differential equation to obtain
ϕ(t) = ϕ(0)exp(−2αt).Ingeneral,whenwehaveadifferentialinequality,
we can bound ϕ by the solution to the differential equation; this is
formalized as Gr¨onwall’s inequality.
Lemma 5.16(Gro¨nwall’s inequality). Let c ∈ R. Let ϕ : [0,T] → R
be differentiable, satisfying ϕ˙(t) ≤ cϕ(t) for all t ∈ [0,T]. Then,
ϕ(t) ≤ ϕ(0)exp(ct) ∀t ∈ [0,T].146 5 Wasserstein gradient flows: theory
Proof. It holds that
∂ [exp(−ct)ϕ(t)] = exp(−ct)[−cϕ(t)+ϕ˙(t)] ≤ 0.
t
This implies exp(−ct)ϕ(t) ≤ exp(−c·0)ϕ(0) = ϕ(0). ⊔⊓
On the other hand, applying the same argument as in Lemma A.11,
one can show that F satisfies the PL(cid:32) inequality with constant α as soon
as F is α-geodesically convex. We deduce the following useful corollary.
Corollary 5.17.Let F : P (Rd) → R be α-geodesically convex. Then,
2,ac
along the Wasserstein gradient flow (µ ) for F, it holds
t t≥0
F(µ )−infF ≤ e−2αt(F(µ )−infF).
t 0
5.5 Bures–Wasserstein
It is illuminating to specialize the concepts in the previous section to
the submanifold of Wasserstein space consisting of Gaussian measures.
Definition 5.18.The Bures–Wassersteinspace BW(Rd) is the space of
non-degenerate Gaussians on Rd, equipped with the Wasserstein metric.
Concretely, since Gaussians are parameterized by the mean and
covariance matrix, we can think of BW(Rd) ∼ = Rd×Sd , where Sd is
++ ++
cone of symmetric positive definite d×d matrices. Recall from Exam-
ple 1.19 that for any µ ,µ ∈ BW(Rd), the optimal transport map T
0 1
from µ to µ is an affine map, and the Wasserstein geodesic joining µ
0 1 0
to µ is
1
µ = [(1−t)id+tT] µ , t ∈ [0,1].
t # 0
(cid:124) (cid:123)(cid:122) (cid:125)
affine
Since the pushforward of a non-degenerate Gaussian by a non-singular
affine map is also a non-degenerate Gaussian, the Wasserstein geodesic
from µ to µ lies entirely in BW(Rd), or in other words:
0 1
Proposition 5.19.BW(Rd) ⊆ P (Rd) is geodesically convex.
2,ac
Recall that a functional F on a Riemannian manifold M is α-
geodesicallyconvexifthemapping[0,1] → M,t (cid:55)→ F(p )isα-convexfor
t
all geodesics (p ) on M. The geodesic convexity of BW(Rd) means
t t∈[0,1]
that the intrinsic geodesics of BW(Rd) coincide with the Wasserstein
geodesics, which immediately furnishes the following corollary.5.5 Bures–Wasserstein 147
Corollary 5.20.Let F : P (Rd) → R be an α-geodesically convex
2,ac
functional. Then, F is also α-geodesically convex when viewed as a
functional over BW(Rd).
We make use of this fact in Subsection 6.1.2.
The Riemannian structure of P (Rd) descends to BW(Rd) and
2,ac
endows the Bures–Wasserstein space with the structure of a bona
fide finite-dimensional Riemannian manifold. The tangent space at
µ ∈ BW(Rd) is
T BW(Rd) = {λ(T −id) | λ > 0, ν ∈ BW(Rd)}
µ µ→ν
= {x (cid:55)→ Sx+a | a ∈ Rd, S ∈ Sd},
where Sd is the space of symmetric d × d matrices. Actually, it is
convenient to reparametrize the tangent space as
T BW(Rd) = {x (cid:55)→ S(x−m)+a | a ∈ Rd, S ∈ Sd},
µ
where m = E [X].
X∼µ
By definition, the Riemannian structure induced on BW(Rd) is
the restriction of the inner product on T P (Rd) to the subspace
µ 2,ac
T BW(Rd) ⊆ T P (Rd), i.e., the L2(µ) inner product. Using this, we
µ µ 2,ac
couldcomputetheBWgradientofafunctionalF fromscratch.However,
sincewehavealreadycomputedtheWassersteingradientofF atµtobe
the vector field ∇∇F(µ) = ∇δF(µ) (Proposition 5.10), a more expedient
approach is to now compute the orthogonal projection of ∇∇F(µ) onto
T BW(Rd).
µ
Theorem 5.21.Let F : P (Rd) → R be a functional with first
2,ac
variation δF(µ) at µ. Then, the Bures–Wasserstein gradient of F at
µ ∈ BW(Rd) is the affine mapping
(cid:90) (cid:90)
x (cid:55)→ (cid:0) ∇2δF(µ)dµ(cid:1) (x−m)+ ∇δF(µ)dµ,
(cid:82)
where m = xµ(dx) is the mean of µ.
Proof. Recall that ∇∇F(µ) = ∇δF(µ) (Proposition 5.10). The BW
gradient at µ is the orthogonal projection of ∇δF(µ) onto T BW(Rd);
µ
by definition, this is the element ∇ F(µ) ∈ T BW(Rd) which satisfies
BW µ
⟨∇ F(µ),v⟩ = ⟨∇δF(µ),v⟩ ∀v ∈ T BW(Rd). (5.11)
BW µ µ µ148 5 Wasserstein gradient flows: theory
We can write out this condition more explicitly. Since ∇ F(µ),v ∈
BW
T BW(Rd), they are of the form
µ
∇ F(µ) = S(·−m)+a,
BW
v = S˜(·−m)+a˜.
On one hand,
⟨∇ F(µ),v⟩ = E ⟨S(X −m)+a,S˜(X −m)+a˜⟩
BW µ X∼µ
= ⟨S,ΣS˜⟩+⟨a,a˜⟩, (5.12)
where Σ is the covariance matrix of µ. On the other hand,
⟨∇δF(µ),v⟩ = E ⟨[∇δF(µ)](X),S˜(X −m)+a˜⟩
µ X∼µ
= ⟨E [∇δF(µ)(X)(X −m)T],S˜⟩ (5.13)
X∼µ
+⟨E ∇δF(µ)(X),a˜⟩. (5.14)
X∼µ
Also, integration by parts yields
(cid:90)
E [∇δF(µ)(X)(X −m)T] = ∇δF(µ)(ΣΣ−1(·−m))T dµ
X∼µ
(cid:90)
= − ∇δF(µ)(∇logµ)TdµΣ
(cid:90)
= − ∇δF(µ)(∇µ)TΣ
(cid:90)
= ∇2δF(µ)dµΣ.
Hence,
(cid:68)(cid:90) (cid:69)
⟨E [∇δF(µ)(X)(X −m)T],S˜⟩ = ∇2δF(µ)dµΣ,S˜
X∼µ
(cid:68) (cid:90) (cid:69)
= Σ ∇2δF(µ)dµ,S˜
(cid:68)(cid:90) (cid:69)
= ∇2δF(µ)dµ,ΣS˜ . (5.15)
Since (5.11) is supposed to hold for all a˜ ∈ Rd and all S˜ ∈ Sd, by
comparing (5.12), (5.13), (5.14), and (5.15), we can identify
(cid:90) (cid:90)
S = ∇2δF(µ)dµ and a = ∇δF(µ)dµ.
This completes the derivation. ⊔⊓5.5 Bures–Wasserstein 149
Once we have identified the BW gradient, we can use the Lagrangian
interpretation of the continuity equation to implement the gradient flow
via the dynamics
X˙ = −∇ F(µ )(X )
t BW t t
(cid:90) (cid:90)
= −(cid:0) ∇2δF(µ )dµ (cid:1) (X −m )− ∇δF(µ )dµ ,
t t t t t t
where X ∼ µ and we denote the mean and covariance of µ by m
t t t t
and Σ respectively. However, since µ is a Gaussian for each t ≥ 0,
t t
it is expedient to instead track µ exactly through the mean m and
t t
covariance Σ . They follow the dynamics:
t
(cid:90)
m˙ = EX˙ = − ∇δF(µ )dµ ,
t t t t
and
Σ˙ = ∂ E[(X −m )(X −m )T]
t t t t t t
= E[X˙ (X −m )T]+E[(X −m )X˙T]
t t t t t t
(cid:104)(cid:16) (cid:90)
= −E (cid:0) ∇2δF(µ )dµ (cid:1) (X −m )
t t t t
(cid:90) (cid:17) (cid:105)
+ ∇δF(µ )dµ (X −m )T +···
t t t t
(cid:90)
= −(cid:0) ∇2δF(µ )dµ (cid:1)E[(X −m )(X −m )T]+···
t t t t t t
(cid:90) (cid:90)
= −(cid:0) ∇2δF(µ )dµ (cid:1) Σ −Σ (cid:0) ∇2δF(µ )dµ (cid:1) ,
t t t t t t
where above, A+··· is shorthand for the expression A+AT. Finally,
we have arrived at an explicit system of equations.
Theorem 5.22.The BW gradient flow of the functional F is the curve
(µ = N(m ,Σ )) , where
t t t t≥0
m˙ = −E∇δF(µ )(X ),
t t t
(5.16)
Σ˙ = −E∇2δF(µ )(X )Σ −Σ E∇2δF(µ )(X ),
t t t t t t t
and X ∼ µ .
t t150 5 Wasserstein gradient flows: theory
5.6 Gaussian mixtures
Building on top of the ideas introduced in Section 5.5, we now consider
gradient flows over the space of Gaussian mixtures, which is a far richer
space. In fact, as explained below, any measure over Rd can be viewed
as a Gaussian mixture when viewed through the right lens.
Before doing so, we first note that simply constraining the Wasser-
stein gradient flow to lie on the space of Gaussian mixtures, similarly
to how we proceeded in Section 5.5, does not work. The problem is
that we cannot explicitly compute the optimal transport map between
two Gaussian mixtures, even infinitesimally, and so we cannot identify
the tangent space—unless each Gaussian mixture has one component,
or one dimension. Nevertheless, following [CGT19, DD20], there is a
natural geometric structure we can consider: Wasserstein over Bures–
Wasserstein.
Gaussian mixtures are typically introduced as distributions of the
form (cid:80)K w N(m ,Σ ) for mixing weights w ≥ 0, (cid:80)K w = 1,
k=1 k k k k k=1 k
but we can define a Gaussian mixture more broadly as a measure
of the form (cid:82) N(m,Σ)ν(dm,dΣ). The finite Gaussian mixture above
corresponds to a discrete mixing measure ν: ν =
(cid:80)K
w δ .
k=1 k (m k,Σ k)
This new, broader definition of a Gaussian mixture, is nearly useless,
(cid:82)
since any measure µ can be represented thus: µ = δ µ(dx), where
x
δ = N(x,0) is a degenerate Gaussian. Also, the representation of a
x
Gaussian mixture by a mixing measure ν is “overparametrized”, i.e., ν
is highly non-unique: consider the equality N(0,I) = (cid:82) N(x,τI)ν(dx)
where ν = N(0,(1 − τ)I), valid for any τ ∈ [0,1]. Nevertheless, the
utility of this perspective is that it leads to a natural interpretation: a
mixing measure for a Gaussian mixture is simply a probability measure
over the Bures–Wasserstein space. Let us see how this leads to the
definition of a geometric structure.
The Bures–Wasserstein space is a Riemannian manifold. As noted
earlier,thespaceBW(Rd)isisometrictothemanifoldRd×Sd equipped
++
with a certain Riemannian metric. Hereafter we consider the metric
arising from Otto calculus but any metric, including the Euclidean one
could be used here.
We can consider the space of probability measures (with finite second
moment) over any metric space, and endow it with the 2-Wasserstein
distance. Indeed, recall from Section 1.1 that the optimal transport
problem can be defined with more general costs, so we can take the
squared distance function over the metric space as our cost.5.6 Gaussian mixtures 151
When the metric space in question is a Riemannian manifold, the
results from Sections 5.1–5.4 continue to hold with appropriate modifi-
cations. We do not justify this in detail here, but we invite the reader
to revisit these sections with a fresh perspective. For example, the
ODE (5.1) still makes sense, keeping in mind that a vector field v on a
manifold M is an assignment x (cid:55)→ v(x) of a tangent vector v(x) ∈ T M
x
at each point x ∈ M. The continuity equation still makes sense in its
weak form (5.3), where ∇ now refers to the Riemannian gradient, and
even (5.2) makes sense if we interpret µ as a density with respect to the
t
volume measure, etc. In fact, this geometric setting is the source of some
of the deepest developments in optimal transport theory; see [Vil09b].
Crucially, for our purposes, the formula for the Wasserstein gradient
given in Proposition 5.10 still holds, where we again interpret ∇ as the
Riemannian gradient.
Putting this discussion together, we can derive the Wasserstein
gradient flow which lives in the space (P (BW(Rd)),W ).
2 2
Theorem 5.23.Given ν ∈ P (BW(Rd)), let G denote the correspond-
2 ν
ing Gaussian mixture G = (cid:82) N(m,Σ)ν(dm,dΣ). Let F be a functional
ν
over P (Rd), and let G be the corresponding functional over P (BW(Rd))
2 2
given by ν (cid:55)→ F(G ). Then, the Wasserstein gradient flow of G is the
ν
curve (ν ) described as follows: ν = law(m ,Σ ), where
t t≥0 t t t
m˙ = −E∇δF(G )(X ),
t νt t
Σ˙ = −E∇2δF(G )(X )Σ −Σ E∇2δF(G )(X ),
t νt t t t νt t
and X ∼ N(m ,Σ ).
t t t
Proof. The first variation of G is computed as follows. If (ν ) is a
t t∈R
curve in P (BW(Rd)), then ∂ G = (cid:82) N(m,Σ)∂ ν (dm,dΣ). Hence,
2 t νt t t
(cid:90)
∂ G(ν ) = ∂ F(G ) = δF(G )∂ G
t t t νt νt t νt
(cid:90)(cid:90)
= δF(G )dN(m,Σ)∂ ν (dm,dΣ).
νt t t
This implies that the first variation is
(cid:90)
δG(ν) : (m,Σ) (cid:55)→ δF(G )dN(m,Σ).
ν152 5 Wasserstein gradient flows: theory
Based on our identification of BW(Rd) with Rd ×Sd , the Rieman-
++
nian gradient of δG(ν), evaluated at (m,Σ), is the same as the Bures–
Wasserstein gradient of µ (cid:55)→ (cid:82) δF(G )dµ, evaluated at µ = N(m,Σ),
ν
and we computed the latter in Theorem 5.21. Therefore, the result
follows from Theorem 5.22. ⊔⊓
5.7 Wasserstein–Fisher–Rao
We now describe a variation of the Wasserstein geometry that is often
useful in applications as illustrated in Chapter 6. This variation, known
as the Wasserstein–Fisher–Rao (WFR) or Hellinger–Kantorovich dis-
tance, was originally proposed and studied as a model of unbalanced
optimal transport, that is, optimal transport between positive measures
notnecessarilycontainingthesametotalmass.Anotableexampleisthe
cellular trajectory reconstruction application mentioned in the Preface,
in which measures are used to represent snapshots of the cell population
at different times, and for which the total mass indeed changes due to
the birth and death of individual cells.
From the static perspective, WFR defines a variant of the optimal
transport problem, and its various properties such as the cost, metric
properties, duality, etc. can all be investigated in a similar vein as we
did in Chapter 1. We refer to the references [LMS16, KMV16, CPSV18,
LMS18] for detailed investigations in this direction. In this section, we
follow [LCB+22, Appendix H] and focus on the Riemannian structure
of the resulting metric space for the purpose of deriving gradient flows.
Fisher–Rao
Before turning toward Wasserstein–Fisher–Rao, we first describe one of
its key components: the Fisher–Rao metric. This is a metric over the
space M (Rd) of positive measures over Rd, defined via
+
(cid:90)
√ √
d2 (µ ,µ ) := ( µ − µ )2,
FR 0 1 0 1
where as usual we identify measures with their Lebesgue densities,
assuming that they exist.8 When µ , µ are probability measures, then
0 1
d coincides with the statistician’s Hellinger distance.9
FR
8 When the densities do not exist, we can define the distance via d2 (µ ,µ ) :=
FR 0 1
(cid:113) (cid:113)
(cid:82) ( dµ0 − dµ1)2dλ with respect to any common dominating measure λ.
dλ dλ
9 This explains the competing naming conventions for the WFR metric; note that
{Wasserstein,Fisher–Rao}∼={Hellinger,Kantorovich}.5.7 Wasserstein–Fisher–Rao 153
Geometrically, d is the metric over (say) non-negative densities
FR √
obtained by demanding that µ (cid:55)→ µ be an isometry into the Hilbert
space L2(Rd). Therefore, the geometry of (M (Rd),d ) is flat, and we
+ FR
can obtain a Riemannian structure through the isometry. Namely, if µ˙
denotes the derivative in time of a curve of densities, then the derivative
√ √
of the square root is ˙µ = µ˙/(2 µ). If we measure the “length” of the
latter in the L2(Rd) norm, we arrive at the induced Riemannian metric
√ (cid:90) µ˙2
g (µ˙,µ˙) := ∥ ˙ µ∥2 =
µ L2(Rd) 4µ
over the tangent space T M (Rd) of functions µ˙ : Rd → R.
µ +
Thisgeometryissetoverthespaceofallpositivemeasures,including
measures with differing amounts of mass, and indeed this geometry
proves useful for modeling physical situations in which change of mass
naturally occurs. A canonical example is when µ represents the concen-
trationofachemicalsubstance,andtheconcentrationchangesovertime
due to chemical reactions. This is modelled by the reaction equation
∂ µ = α µ , where α : Rd → R dictates the rate of reaction at each
t t t t t √
point in space. Note that in this notation, α = µ˙/µ = 2 ˙µ, which
amounts to a reparametrization of the tangent space. In other words,
we can equivalently think of the tangent space as consisting of functions
α : Rd → R equipped with the metric
(cid:90)
1
g (α,α) := g (αµ,αµ) = α2dµ. (5.17)
(cid:101)µ µ
4
Going forward, we adopt this as our definition of the metric, and hence
we write ∥α∥2 := g (α,α).
µ (cid:101)µ
We can draw comparisons with the definition of the Wasserstein
geometry: at each measure µ, the “tangent space” T M (Rd) at µ is
µ +
now defined to be the space of all functions α : Rd → R, equipped
with the metric (5.17), and the continuity equation is replaced by the
reaction equation ∂ µ = α µ . Compared to the Wasserstein metric, the
t t t t
Fisher–Raometricisbasedonanentirelydifferentintuition:ratherthan
transportation of mass, the reaction equation now describes spontaneous
creation and destruction of mass.
Despite motivating the Fisher–Rao geometry for problems involving
change of mass, we may also wish to apply it to problems in which we
want to maintain a flow on the space of probability measures. To do so,
we consider the induced geometry over P(Rd). The equation ∂ µ = α µ
t t t t154 5 Wasserstein gradient flows: theory
(cid:82)
preserves the total mass if and only if α dµ = 0 for all t ≥ 0, so we
t t
restrict the tangent space to T P(Rd) = {α : Rd → R | (cid:82) αdµ = 0},
µ
equipped with the metric (5.17).10 The preservation of mass ensures
that any mass that is destroyed is also instantly created elsewhere.
To adhere to the lexicon of transport, this phenomenon is sometimes
referred to as teleportation; however, it should be noted that it merely
corresponds to a reweighting.
What about gradient flows? Given a functional F : M (Rd) →
+
R or F : P(Rd) → R, the gradient by definition satisfies ∂ F(µ ) =
t t
⟨∇ F(µ ),α ⟩ along every curve ∂ µ = α µ . By unpacking the
FR t t µt t t t t
definitions, one checks (see Exercise 14) that
(cid:90)
∇ F(µ) = δF(µ) or ∇ F(µ) = δF(µ)− δF(µ)dµ (5.18)
FR FR
depending on whether we are working over M (Rd) or P(Rd) respec-
+
tively; here, δF denotes the first variation of F (Definition 5.9). To
disambiguate the two cases and to emphasize the original motivation
of the WFR metric from unbalanced optimal transport, we refer to
the former case as unbalanced Fisher–Rao and the latter as simply
Fisher–Rao. The Fisher-Rao gradient flow of F follows the tangent
vector −∇ F(µ ) at time t and is given by
FR t
∂ µ = −∇ F(µ )µ . (5.19)
t t FR t t
Wasserstein–Fisher–Rao
We now combine both the Wasserstein and Fisher–Rao geometries
into a hybrid geometry that incorporates both mass transport and
creation/destruction (a.k.a. reweighting, a.k.a. teleportation). The idea
is to simply consider the continuity equation with reaction, ∂ µ +
t t
div(µ v ) = α µ .Thegoverningequationisparameterizedbyafunction
t t t t
α and a vector field v, which together form a tangent vector. It is then
natural to consider the metric11
(cid:90)
∥(α,v)∥2 = (α2+∥v∥2)dµ. (5.20)
µ
10 A discrete analogy: endow the space of probability measures on {1,...,d} (i.e.,
√ √
the simplex in Rd) with a geometry via an isometry p (cid:55)→ p, where p is an
element of the unit sphere Sd−1. See Exercise 13.
11 Strictly speaking, to add the Wasserstein and Fisher–Rao geometries, we should
addafactorof 1 infrontoftheα2 term,andthisisindeedtheconventionadopted
4
in some works. We omit this factor for parsimony.5.7 Wasserstein–Fisher–Rao 155
However,similarlytoourdiscussioninSection5.1,thisdoesnotuniquely
define a tangent vector because there is too much freedom to choose the
pair (α,v) while maintaining the same evolution of measures (µ ) . It
t t≥0
can be shown that the optimal pair (α,v), in the sense of minimizing
the norm (5.20) (c.f. the discussion in Section 5.1) can be characterized
as follows: α = ψ and v = ∇ψ for some function ψ : Rd → R. Hence,
we can define the WFR tangent space at µ to be
L2(µ)
T M (Rd) = {(ψ,∇ψ) | ψ : Rd → R compact supp., smooth}
µ +
equipped with the norm
(cid:90)
∥(ψ,∇ψ)∥2 := (ψ2+∥∇ψ∥2)dµ. (5.21)
µ
This has the pleasing interpretation of “completing” the Wasserstein
metric ∥∇ψ∥2 to the full Sobolev norm of ψ. Note also that the
L2(µ)
governing equation becomes
∂ µ +div(µ ∇ψ ) = ψ µ . (5.22)
t t t t t t
As before, we can also restrict to the space of probability measures
P(Rd), in which case we restrict to pairs (ψ,∇ψ) such that (cid:82) ψdµ = 0,
endowed with the same metric (5.21). We refer to WFR over the full
space M (Rd) as unbalanced WFR, henceforth reserving the use of
+
WFR for the restriction to P(Rd).
The following theorem computes the WFR gradient.
Theorem 5.24.Let F be a functional over M (Rd) or P(Rd). Then,
+
unbalanced WFR gradient of F, denoted ∇∇ F, is given by
FR
∇∇ F(µ) = (cid:0) δF(µ),∇δF(µ)(cid:1)
FR
and the WFR gradient by
(cid:16) (cid:90) (cid:17)
∇∇ F(µ) = δF(µ)− δF(µ)dµ,∇δF(µ) .
FR
Proof. Let (µ ) satisfy (5.22). Then, by integration by parts,
t t≥0
(cid:90) (cid:90)
∂ F(µ ) = ⟨∇δF(µ ),∇ψ ⟩dµ + δF(µ )ψ dµ .
t t t t t t t t156 5 Wasserstein gradient flows: theory
In the unbalanced case, we can identify this as
⟨(δF(µ),∇δF(µ)),(ψ ,∇ψ )⟩
t t µt
according to the definition of the metric (5.21). In the balanced case,
(cid:82)
we have ψ dµ = 0, and since the WFR gradient is by definition an
t t
element of the tangent space its first component must also have mean
zero, so the claim follows. ⊔⊓
The WFR gradient flow is therefore given by
(cid:16) (cid:90) (cid:17)
∂ µ = div(cid:0) µ ∇δF(µ )(cid:1) − δF(µ )− δF(µ )dµ µ . (5.23)
t t t t t t t t
5.8 Mean-field particle systems
We conclude this chapter by describing Wasserstein and WFR gradient
flows from a particle systems perspective. These arise naturally when
these gradient flows are initialized at finite measures. Indeed, a key
observation is that since both gradient flows can be implemented using
ordinary differential equations, if µ is a finite measure, µ remains a
0 t
finite measure at all times t along these gradient flows.
5.8.1 Particle Wasserstein gradient flow
To illustrate this point, let F be a function over P(Rd) and recall from
Definition 5.14 that the Wasserstein gradient flow of F is the continuity
equation associated with the ODE
X˙ = −∇∇F(µ )(X ), (5.24)
t t t
where µ denotes the law of X . In particular, we only need to describe
t t
these dynamics on the support of µ .
t
Assume now that the Wasserstein gradient flow is initialized at
N
1 (cid:88)
µ := δ ,
0
N
Xj
0
j=1
for a given collection of points X1,...,XN ∈ Rd. We get that
0 0
N
1 (cid:88)
µ := δ ,
t N X tj
j=15.8 Mean-field particle systems 157
where for i ∈ [N],
X˙i = −∇∇F(µ )(Xi). (5.25)
t t t
These dynamics describe an interacting particle system where particles
(X1,...,XN) are subject to dynamics of the form
t t
X˙i = Vi(X1,...,XN), i ∈ [N]. (5.26)
t t t t
Note that in the case of Wasserstein gradient flows, we further have
that:
(a) each particle Xi interacts with the others only through the effect of
t
their distribution µ , and
t
(b) these interactions have the same form for all the particles.
Slightly overloading notation, this means that the general dynamics
in (5.26) simplify to
X˙i = Vi(X1,...,XN) ( =a) Vi(Xi,µ ) ( =b) V (Xi,µ ), i ∈ [N].
t t t t t t t t t t
These two properties are precisely captured by (5.24): the first one
is obvious and the second one is manifest due to the absence of a
superscript i, which indicates that each particle is subject to the same
vector field. Such a system is said to exhibit mean-field interactions.
Both the Wasserstein and WFR gradient flows are of this form.
Mean-field interaction systems are convenient because it is strictly
equivalent to describe the dynamics of each particle and that of their
distribution. The latter takes the form of a PDE given by the continuity
equation (5.2).
Since the Wasserstein gradient flow only moves particles, the weights
in µ do not change over time: if the Wasserstein gradient flow is
0
initialized at
N
µ := (cid:88) wjδ , (5.27)
0 0 Xj
0
j=1
where wj ≥ 0, j ∈ [N] and (cid:80)N wj = 1, then
0 j=1 0
N
µ := (cid:88) wjδ ,
t 0 Xj
t
j=1
where X1,...,XN evolve according to (5.25). To also impose dynamics
t t
on the weights, we employ instead a WFR gradient flow.158 5 Wasserstein gradient flows: theory
5.8.2 Particle WFR gradient flow
Recall that tangent vector fields for the Wasserstein space are dis-
placement maps of the form ∇ψ. The Wasserstein–Fisher–Rao (WFR)
geometry reinterprets the tangent space by replacing the governing
continuity equation (5.2) with the reaction-transport equation (5.22).
In particular, it offer the possibility of traversing the space of probabil-
ity measures, say from initial distribution to target distribution, more
efficiently by reweighting particles rather than having to move them
across the space in a continuous fashion. When initialized at a finite
measure of the form (5.27), this effect manifests itself in the form of
time-varying weights:
N
µ := (cid:88) wjδ ,
t t Xj
t
j=1
where wj ≥ 0, j ∈ [N] and (cid:80)N wj = 1.
t j=1 t
TheparticleupdatesfollowtheWassersteingeometry,andtheweight
updates follow the Fisher–Rao geometry: for i ∈ [N],
X˙i = −∇δF(µ )(Xi),
t t t
(cid:16) (cid:90) (cid:17) (5.28)
w˙i = − δF(µ )(Xi)− δF(µ )dµ wi.
t t t t t t
5.8.3 Gaussian particles
Following Section 5.6, we can also take a finite Gaussian mixture with
mixing measure
K
1 (cid:88)
ν = δ
t
K
(mk,Σk)
k=1
thatevolvesaccordingtotheWassersteingradientflowforthefunctional
ν (cid:55)→ G(ν) = F(G ). By Theorem 5.23, this flow takes the following form:
ν
for each k ∈ [K],
m˙ k = −E∇δF(G )(Xk),
t νt t
(5.29)
Σ˙k = −E∇2δF(G )(Xk)Σk −ΣkE∇2δF(G )(Xk),
t νt t t t νt t
where Xk ∼ N(mk,Σk). Note that this is an interacting system of “par-
t t t
ticles” (mk,Σk), but each particle corresponds to a Gaussian component
t t5.9 Discussion 159
N(mk,Σk), and the collection thereof to the Gaussian mixture ν . We
t t t
therefore refer to N(mk,Σk) as a Gaussian particle.
t t
Weemphasizethatthesedynamicsdonot implementtheWasserstein
gradient flow for F. Nevertheless, these dynamics are perfectly valid for
minimizing F over the space of K-component Gaussian mixtures.
Recall that in Section 5.6, we equipped the space of probability
measures over BW(Rd)—i.e., the space of mixing measures—with the
Wasserstein geometry. But we could have equally well considered equip-
ping this space with the WFR geometry. The corresponding particle
dynamics evolves the finite Gaussian mixture
K
(cid:88)
ν = wkδ
t t (mk,Σk)
t t
k=1
with changing weights, governed by
m˙ k = −E∇δF(G )(Xk),
t νt t
Σ˙k = −E∇2δF(G )(Xk)Σk −ΣkE∇2δF(G )(Xk),
t νt t t t νt t
K
w˙k = −(cid:16) EδF(G )(Xk)− 1 (cid:88) EδF(G )(Xk′ )(cid:17) wk,
t νt t K νt t t
k′=1
where Xk ∼ N(mk,Σk).
t t t
5.8.4 Implementation strategies for gradient flows
For both the Wasserstein gradient flow and the WFR gradient flow, one
needs to compute the Wasserstein gradient ∇∇F(cid:0) µ (cid:1) = ∇δF(µ ) on the
t t
support of µ . When µ is a discrete measure, this quantity may not
t t
be well-defined. This the the case for example when F is the entropy
functional which is itself not defined on discrete measures, let alone its
Wasserstein gradient. (Note, however, that it may be well-defined when
we use Gaussian particles.)
In practice, the particle implementations discussed here typically
needs to be combined with other tricks (e.g., “kernelization” as in
Subsection 6.1.4). These implementation strategies are described in the
next chapter.
5.9 Discussion
§5.1. Detailed treatments of the metric derivative and the continuity
equation can be found in [AGS08, Vil09b, San15].160 5 Wasserstein gradient flows: theory
§5.2. There are many excellent textbooks covering Riemannian geome-
try, e.g., [dC92].
§5.3. The Benamou–Brenier formula is often called the “dynamical”
formulation of optimal transport (as opposed to Chapter 1, which
describes the “static” picture). There is also a dynamical version of
the dual problem, in which the dual potentials evolve according to
the Hamilton–Jacobi equation; see [Vil03, Section 8.1]. The dynamical
version of entropic optimal transport, introduced in Chapter 4, is closely
tied to the well-known Schr¨odinger bridge problem [L´eo14, CGP21].
§5.4.Theformalcalculationrulesdescribedinthissectionwerefirstlaid
outbyOtto[Ott01],althoughsomeoftheideaswerealreadyanticipated
in the earlier work of [Laf88].
The tangent space at a measure µ, together with its metric, can
be viewed as a linearization of the geometric structure at µ. This
gives rise to “linearized optimal transport” which has formed the basis
for numerous applications [WSB+13, BKR14, KR15, SC15, KTOR16,
BGKL17, PT18, CCCC20].
Besidesgradientflows,therehavealsobeenproposalsforadaptations
of other optimization algorithms to the Wasserstein space, e.g., [CLZ20,
WL20, WL22, Tan23, CLTW24].
§5.5. The Bures–Wasserstein geometry is named after Donald Bu-
res [Bur69], who introduced this metric over the PSD cone in his work
on quantum information theory. BW geometry is further explored
in [Mod17, BJL19, HMJG21, vO22]. The connection with the Burer–
Monteiro factorization, as described in Exercise 12, has been explored
in the context of low-rank matrix recovery [LGT22, MLGR23].
§5.6. As mentioned in the main text, the geometry described in this
section was first considered in [CGT19, DD20], although the gradient
flow equations were obtained in [LCB+22].
§5.7. The Fisher–Rao geometry is well-studied in information geome-
try [AN00, AJLS17].
§5.8.Insomesources,suchas[LMS16],WFRgradientflowsarewritten
√
in terms of the square root of the weights, i.e., in terms of r := w. This
convention is motivated by the fact that the FR distance corresponds
to the Euclidean distance between the square roots of the weights,
and the WFR distance can therefore be interpreted as a coupling
cost on the space of (r,x) pairs equipped with a “cone” metric (c.f.
Subsection 7.3.3). Since we do not cover this perspective here, we adopt
the more straightforward parametrization in terms of w.5.10 Exercises 161
The use of Gaussian particles was first advocated in [LCB+22].
5.10 Exercises
In the following exercises, you may use the following formula for the
Wasserstein gradient of the squared Wasserstein distance:
[∇∇W2(·,ν)](µ) = 2(id−T ). (5.30)
2 µ→ν
We do not give the full proof of (5.30) here, but it is straightforward to
establish the upper bound (Exercise 3).
1. Let X ∼ µ , where µ is the standard Gaussian over R2. For t ≥ 0,
0 0 0
let X = R X where R is a rotation by θ(t) radians. Compute the
t t 0 t
vector field v such that X˙ = v (X ) and show that div(µ v ) = 0
t t t t 0 t
for all t ≥ 0 (and hence that X ∼ µ for all t ≥ 0).
t 0
2. Show that the set of product measures over Rd is a geodesically
convex subset of P (Rd).
2
3. Suppose that (X ) follows the ODE X˙ = v (X ), so that
t t∈R t t t
µ = law(X ) evolves according to the continuity equation ∂ µ +
t t t t
div(µ v ) = 0, and suppose that µ = µ. Prove that
t t 0
W2(µ ,ν)−W2(µ ,ν)
limsup 2 h 2 0 ≤ 2⟨id−T ,v ⟩ .
µ→ν 0 µ
h
h↘0
Hint: Let X ∼ µ and Y ∼ ν be optimally coupled.
0
4. Let F : P (Rd) → R∪{∞} be a geodesically convex functional
2,ac
whichisminimizedatπ.Let(µ ) denotetheWassersteingradient
t t≥0
flow for F. By differentiating t (cid:55)→ 2tF(µ )+W2(µ ,π), prove
t 2 t
W2(µ ,π)
F(µ )−infF ≤ 2 0 .
t
2t
5. Let F : P → R∪{∞} be a functional. We saw that α-strong
2,ac
convexity of F implies the Polyak–L(cid:32) ojasiewicz (PL(cid:32) ) inequality
∥∇∇F(µ)∥2 ≥ 2α(F(µ)−infF), ∀µ ∈ P (Rd). (5.31)
µ 2,ac
a) Show that (5.31) implies the quadratic growth inequality
α
F(µ)−infF ≥ W2(µ,π), ∀µ ∈ P (Rd), (5.32)
2 2 2,ac162 5 Wasserstein gradient flows: theory
where π is the minimizer of F. This is known as the Otto–Villani
theorem after [OV00].
Hint: Differentiate t (cid:55)→ (cid:112)α W (µ ,µ )+(cid:112)F(µ )−infF along
2 2 t 0 t
the Wasserstein gradient flow of F. You may assume that the
gradient flow converges to π, which is a consequence of (5.32) if
F is uniquely minimized.
b) In general, (5.32) does not imply (5.31). However, prove that
when F is geodesically convex, then (5.32) implies (5.31) but
with α replaced by α/4.
6. Suppose that instead of the PL inequality (5.31), we instead have
the inequality
∥∇∇F(µ)∥p ≥ c(F(µ)−infF), ∀µ ∈ P (Rd),
µ 2,ac
for some power 0 < p < 2. Show that the Wasserstein gradient flow
dissipates F at a polynomial rate: F(µ t)−infF = O(1/tp2−1 ). What
happens in the case p > 2?
7. Consider F : P (Rd) → R which is the operator norm of the
2,ac
second moment matrix:
(cid:13)(cid:90) (cid:13)
F(µ) := (cid:13) xxTµ(dx)(cid:13) .
(cid:13) (cid:13)
op
Prove that F is geodesically convex.
8. a) Compute the Wasserstein gradient of the chi-squared divergence
χ2(·∥π) at µ. Recall that χ2(µ∥π) := (cid:82) dµ dµ−1. Also, write
dπ
down the equation for the Wasserstein gradient flow of the chi-
squared divergence.
b) Prove that when π is log-concave, then χ2(·∥π) is geodesically
convex.
9. We say that a functional F : P (Rd) → R ∪ {∞} is α-convex
2,ac
along generalized geodesics if for all triples µ ,µ ,ν ∈ P (Rd), if
0 1 2,ac
we define the generalized geodesic joining µ to µ with base ν via
0 1
µν := [(1−t)T +tT ] ν,
t µ0→ν µ1→ν #
then it holds:
αt(1−t)
F(µν) ≤ (1−t)F(µ )+tF(µ )− W2(µ ,µ ).
t 0 1 2 2 0 1
a) Explain why, if F is α-convex along generalized geodesics, then
it is α-strongly convex. Also, explain why being α-convex along
generalized geodesics is equivalent to the mapping F◦exp being
ν
α-strongly convex on the tangent space T P (Rd).
ν 2,ac5.10 Exercises 163
b) Show that for π ∝ exp(−V) where V is α-strongly convex, then
KL(·∥π)isα-convexalonggeneralizedgeodesics(thisstrengthens
the convexity result of Corollary 6.4).
c) Show that for any µ ,µ ,ν ∈ P (Rd), there exists at least one
0 1 2,ac
generalized geodesic joining µ to µ , along which 1 W2(·,ν) is
0 1 2 2
1-strongly convex.
Remark: Generalized geodesics play an important role in studying
the geometry of the Wasserstein space. The result of the third
part of this question was used to show existence of the minimizing
movements scheme, which in turn is used to rigorously construct
Wasserstein gradient flows; see [AGS08] for further reading.
10. Compute the Wasserstein geodesic joining two Gaussians.
11. Use (5.30) to show that if (µ ) is the Wasserstein gradient flow
t t≥0
of 1 W2(·,ν), then t (cid:55)→ µ is the constant-speed Wasserstein
2 2 1−exp(−t)
geodesic joining µ to ν.
0
12. Let F be a functional over P (Rd), and consider the functional
2
(m,U) (cid:55)→ F(m,U) := F(N(m,UUT)). Show that the Euclidean
gradient flow for F over Rd×Rd×d yields the same dynamics (up
to rescaling time) as the Bures–Wasserstein gradient flow (5.16)
where Σ = UUT. Similarly, show that the Euclidean gradient flow of
(m1,...,mK,U1,...,UK) (cid:55)→ F(1 (cid:80)K N(mk,Uk(Uk)T)) recovers
K k=1
the Gaussian mixture flow (5.29) (up to rescaling time).
Remark: The parametrization Σ = UUT is often referred to as
the Burer–Monteiro parametrization, especially when it is used to
constrain Σ to have low rank [BM03, BM05].
13. Let p be an element in the interior of the simplex, i.e., a strictly
positive probability distribution over the finite alphabet {1,...,d}.
√
Consider the isometry f : p (cid:55)→ p that maps p to an element of the
√
sphere: p ∈ Sd−1. Show that under this isometry, a tangent vector
√
p˙ on the simplex is mapped to v = p˙/(2 p) and conclude that p˙ is
(cid:80)
tangent to the simplex (i.e., p˙ = 0) if and only if v is tangent
√ i∈[d] i
to the sphere (i.e., p ⊥ v).
14. Verify the expressions (5.18) for the (unbalanced) Fisher–Rao gradi-
ent of a functional.
15. Compute the FR and WFR gradients of the functionals listed in
Examples 5.11, 5.12, and 5.13.
16. Show that the FR gradient flow (5.19) and the WFR gradient
flow (5.23) maintain the property that µ is a probability measure
t
for all t ≥ 0.164 5 Wasserstein gradient flows: theory
17. Show that (5.28) indeed follows the WFR gradient flow (5.23).6
Wasserstein gradient flows: applications
In the previous chapter, we developed a Riemannian structure on the
space (P (Rd),W ) in order to define Wasserstein and WFR gradient
2 2
flows. In this chapter, we use these gradient flows as optimization
algorithms over the space of probability measures for various tasks
arising in statistics and machine learning. Each task corresponds to
choosing a specific functional F over this space. In particular, akin to
the notion of convexity in classical optimization (see, e.g., [Bub15]),
the notion of geodesic convexity is instrumental in deriving rates of
convergence.
6.1 Variational inference
As our first application of gradient flow theory, we consider a rich source
ofoptimizationproblemsoverthespaceofmeasuresarisingfromthebur-
geoning field of variational inference (VI) [JGJS99, WJ08, BKM17]. In
VI, we posit access to a probability measure π over Rd via an expression
for its density, and our goal is to perform inference. A typical example
arises when π is the posterior distribution from a Bayesian inference
problem, in which case VI is also known as variational Bayes, and it
has gradually emerged as an appealing computational counterpoint to
traditional Markov chain Monte Carlo (MCMC) methods, which we
study in Section 6.2.
The idea of VI is to approximate π with an element of a simpler
class Q of probability measures by solving the optimization problem
q = argminKL(q∥π). (VI)
⋆
q∈Q166 6 Wasserstein gradient flows: applications
Although a plethora of variants have been proposed which replace
the KL divergence with other objectives1, the one we present here is
particularly popular in practice. This is because typically we do not
have access directly to π but rather to an unnormalized density π˜,
(cid:82)
and the unknown normalization constant Z = π˜ does not affect the
optimization objective in (VI).
This modelling choice also has fortuitous consequences for the con-
vexity of the VI problem over the Wasserstein space, leading to the
development of flow-based algorithms.
6.1.1 Convexity of the VI problem
In order to apply the gradient flow machinery to (VI), we are inexorably
led to our next undertaking: studying the geodesic convexity of the KL
divergence over the Wasserstein space.
Henceforth, we always assume that π admits a density of the form
π ∝ exp(−V), where V : Rd → R is called the potential function. The
first observation is that the KL divergence decomposes into a sum of
two functionals:
(cid:90) (cid:90) (cid:90)
µ
F(µ) := KL(µ∥π) = µlog = V dµ+ µlogµ+const. (6.1)
π
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:V(µ) =:H(µ)
where V is the potential energy, H is the entropy, and “const.” denotes
an additive constant that does not depend on µ (and hence is irrelevant
for studying properties of the gradient flow).
In Examples 5.11 and 5.12, we have already computed the Wasser-
stein gradients ∇∇V(µ) = ∇V and ∇∇H(µ) = ∇logµ. Adding these
together, we obtain ∇∇F(µ) = ∇logµ+∇V = ∇log(µ/π). From Defi-
nition 5.14, the Wasserstein gradient flow of F solves
(cid:0) µ(cid:1)
∂ µ = div µ ∇log . (6.2)
t t t
π
Leveraging (6.1), we can study the convexity of the two functionals
V and H separately. The potential energy is straightforward.
Theorem 6.1.Suppose that V : Rd → R is α-convex on Rd. Then, the
corresponding potential energy functional V defined by V(µ) := (cid:82) V dµ
is α-geodesically convex on P (Rd).
2,ac
1 IncludingtheKLdivergencewiththeorderofargumentsswapped,whichiscloser
to the statistician’s concept of maximum likelihood.6.1 Variational inference 167
Proof. We use the second-order condition from Section 5.2. Namely, let
X ∼ µ for t ∈ [0,1], where (µ ) is a Wasserstein geodesic; thus,
t t t t∈[0,1]
X = (1−t)X +tT(X ) where T is the optimal transport map from
t 0 0
µ to µ . We compute
0 1
∇∇2V(µ 0)[T −id,T −id] = ∂ t2V(µ t)(cid:12) (cid:12)
t=0
= ∂ t2EV(X t)(cid:12) (cid:12)
t=0
= E⟨T(X )−X ,∇2V(X )(T(X )−X )⟩
0 0 0 0 0
≥ αE[∥T(X )−X ∥2]
0 0
= α∥T −id∥2 ,
µ0
where we used the assumption ∇2V ⪰ αI. ⊔⊓
Next, we show that the entropy H is geodesically convex. For this,
we invoke the change of variables formula.
Lemma 6.2(Change of variables). Let µ be a density on Rd, let
T : Rd → Rd be a diffeomorphism, and let ν := T µ. Then, ν has
#
density given by
µ(x)
ν(T(x)) = .
|det∇T(x)|
Recall the mnemonic for memorizing this rule: under the change of
variables y = T(x), one has dy = |det∇T(x)|dx, since the Jacobian
determinant |det∇T(x)| measures the volume distortion of the map T.
(cid:82) (cid:82)
The pushforward satisfies, by definition, φ◦T dµ = φdν for all test
functions φ. We can write this as
(cid:90) (cid:90)
φ(T(x))µ(x)dx = φ(y)ν(y)dy
(cid:90)
= φ(T(x))ν(T(x))|det∇T(x)|dx
and Lemma 6.2 follows. In applications, we do not always know that
optimal transport maps are diffeomorphisms, but nevertheless a variant
of Lemma 6.2 still holds, and we refer to [Vil03, Theorem 4.8] for the
technical details.
The change of variables formula furnishes the quickest proof of
geodesic convexity of H.
Theorem 6.3.The entropy functional H, given by H(µ) := (cid:82) µlogµ,
is geodesically convex on P (Rd).
2,ac168 6 Wasserstein gradient flows: applications
Proof. Again let (µ ) be a Wasserstein geodesic and let T :=
t t∈[0,1] t
(1−t)id+tT, so that µ = (T ) µ . Then, by Lemma 6.2 (which we
t t # 0
apply blithely despite not knowing that T is a diffeomorphism),
t
(cid:90) (cid:90) (cid:90)
µ
H(µ ) = (logµ )dµ = log(µ ◦T )dµ = log 0 dµ
t t t t t 0 0
det∇T
t
(cid:90)
= H(µ )− logdet∇T dµ .
0 t 0
It is a standard exercise to show that −logdet is convex over the
positive definite cone, and t (cid:55)→ ∇T is affine; therefore, the composition
t
t (cid:55)→ −logdet∇T is convex. In turn, it shows that t (cid:55)→ H(µ ) is convex,
t t
which is what we wanted to show. ⊔⊓
Corollary 6.4.Let π ∝ exp(−V) be a density, where V : Rd → R is
α-convex. Then, the functional F := KL(·∥π) is α-geodesically convex
on P (Rd).
2,ac
Distributionsπ ∝ exp(−V)forwhichV isstronglyconvexareknown
as strongly log-concave distributions. Therefore, we have shown that
the KL divergence w.r.t. a (strongly) log-concave measure is (strongly)
geodesically convex.
For (VI), our goal is to minimize the KL divergence over a subset
Q ⊆ P (Rd). If Q is geodesically convex (see Section 5.2), then we
2,ac
immediately obtain the following corollary.
Corollary 6.5.Let π ∝ exp(−V) be a density on Rd, where V is α-
convex. Let Q ⊆ P (Rd) be geodesically convex. Then, KL(·∥π) is
2,ac
α-geodesically convex over Q.
In particular, the solution q to (VI) is unique.
⋆
Recall from Lemma A.11 and Section 5.4 that α-convexity implies a
PL inequality, which in turn implies rapid convergence for the gradient
flow:
Corollary 6.6.Let π ∝ exp(−V) be a density on Rd, where V is α-
convex. Let Q ⊆ P (Rd) be geodesically convex. Then, the Wasserstein
2,ac
gradient flow (q ) of KL(·∥π) constrained to lie in Q satisfies
t t≥0
KL(q ∥π)−KL(q ∥π) ≤ e−2αt{KL(q ∥π)−KL(q ∥π)}.
t ⋆ 0 ⋆
In the sequel, our aim is show how the constrained Wasserstein
gradient flow can be implemented in several important cases.6.1 Variational inference 169
6.1.2 Gaussian VI
In this section, we study the problem of Gaussian VI, in which the
variational family Q consists of all non-degenerate Gaussian measures
over Rd. This family is simple yet abundantly motivated: if we can
approximate π ∝ exp(−V) by a Gaussian N(m,Σ), then the parameters
(m,Σ) of the Gaussian are a reasonable guess for the mean and covari-
ance matrix of π, which already suffice to construct credible regions.
The Laplace approximation takes m = θ⋆ and Σ = [∇2V(θ⋆)]−1 where
θ⋆ = argminV is the mode of π. When π is a Bayesian posterior, the
validity of this approximation can be justified in the large-sample limit
by the Bernstein–von Mises theorem.
To go beyond the Laplace approximation, we can ask for the optimal
Gaussian approximation, which is formulated as the VI problem
q = argmin KL(q∥π) (GVI)
⋆
q∈BW(Rd)
where BW(Rd) is the Bures–Wasserstein space introduced in Section 5.5.
Note that if we had considered the KL divergence with the arguments
swapped, q (cid:55)→ KL(π ∥ q), then the optimal solution is the one that
matches the mean and covariance of π, which defeats the purpose of VI
since they are precisely the parameters we are trying to compute.
Following [LCB+22], our approach to solve (GVI) is to follow the
Wasserstein gradient flow constrained to lie in the Bures–Wasserstein
space,seeSection5.5.ByCorollary5.20,BW(Rd)isgeodesicallyconvex,
and hence the guarantee of Corollary 6.6 applies. It remains to derive
the form of the BW gradient flow using Theorem 5.21 in order to arrive
at an implementable algorithm.
For the KL divergence F = KL(·∥π), ∇δF(q) = ∇V +∇logq and
∇2δF(q) = ∇2V +∇2logq. Hence,
(cid:16)(cid:90) (cid:17)
∇ F(q)(x) = (∇2V +∇2logq)dq (x−m )
BW q
(cid:90) (cid:90)
+ ∇V dq+ ∇logqdq
(cid:124) (cid:123)(cid:122) (cid:125)
=0
(cid:16)(cid:90) (cid:17) (cid:90)
= ∇2V dq−Σ−1 (x−m )+ ∇V dq.
q q
By setting the BW gradient equal to zero, we also deduce the first-
order stationarity conditions, which are both necessary and sufficient
by convexity.170 6 Wasserstein gradient flows: applications
Proposition 6.7.Suppose that π ∝ exp(−V), where V is α-convex for
some α > 0. Then, the unique minimizer q in (GVI) is characterized
⋆
by the conditions
(cid:90) (cid:90)
∇V dq = 0 and ∇2V dq = Σ−1.
⋆ ⋆ q⋆
We can now write down the BW gradient flow using Theorem 5.22.
Note that there is a slight simplification since the covariance matrix Σ
t
cancels with the Hessian of the first variation of the entropy.
Theorem 6.8.The BW gradient flow of the functional KL(·∥π), where
π ∝ exp(−V), is the curve (q = N(m ,Σ )) , where
t t t t≥0
m˙ = −E∇V(X ),
t t
(6.3)
Σ˙ = −E∇2V(X )Σ −Σ E∇2V(X )+2I,
t t t t t
and X ∼ q .
t t
To implement the gradient flow, the system of ODEs (6.3) can be
discretized in time. At each iteration t, since we keep track of the
mean m and covariance Σ , the expectations E∇V(X ) and E∇2V(X )
t t t t
can be approximated via Monte Carlo averages by drawing samples
from q = N(m ,Σ ), or via quadrature rules. Furthermore, [DBCS23]
t t t
observed that the splitting (6.1) of the KL divergence naturally suggests
a proximal gradient method for (GVI).
We mention two appealing features of the gradient flow perspective.
First, it comes with principled guarantees: by mimicking optimization
proofsovertheBures–Wassersteinspace,thepapers[LCB+22,DBCS23]
translate Corollary 6.6 into non-asymptotic convergence rates for the
stochastic gradient-based implementations of (6.3). Second, it readily
leads to an extension to variational inference over the richer class
of mixtures of Gaussians by applying either of the Gaussian particle
methods from Subsection 5.8.3; see [LCB+22] for details.
6.1.3 Mean-field VI
Another important variational family is the class Q = P (R)⊗d of
2,ac
product measures over Rd, in which case the problem (VI) is known as
mean-field VI:
q = argmin KL(q∥π). (MFVI)
⋆
q∈P 2,ac(R)⊗d6.1 Variational inference 171
This form of VI has its roots in product measure approximations of
spin systems from statistical physics and can be motivated statistically
by the desire to compute integrals of separable test functions ϕ(x) =
(cid:80)d
ϕ (x ) against the posterior.
i=1 i i
Since the family of product measures is geodesically convex (Exer-
cise2inChapter5),Corollary6.6onceagainshowsthattheconstrained
Wasserstein gradient flow converges rapidly. One can also show that
for a functional F, the component of the Wasserstein gradient ∇∇F(µ)
which is tangential to the space of product measures takes the form
(cid:88)d (cid:16)(cid:90) (cid:17)
x (cid:55)→ ∇∇F(µ)(x)µ(dx ,...,dx ,dx ,...,dx ) e , (6.4)
1 i−1 i+1 d i
i=1
where e is the i-th standard basis vector; see Exercise 2.
i
Note that the particle approach of Subsection 5.8.1 does not apply
because the Wasserstein gradient of the KL divergence is not defined
for discrete measures. One way to circumvent this issue is via stochastic
dynamics, see Subsection 6.3. In this section, we instead describe the
approachof[JCP24]which,similarlytotheprevioussubsection,isbased
on finite-dimensional parameterization. Key to this approach is that for
mean-field VI, the measure q is not intrinsically high-dimensional due
⋆
to the product structure, which allows for efficient parameterization.
Theideaistoparameterizeanelementq ∈ P (R)⊗d viatheBrenier
2,ac
map T from the standard Gaussian measure ρ. Since both ρ and
ρ→q
q are product measures, one sees that the transport map is separable,
i.e., it is of the form T (x) = (T (x ),...,T (x )) for some univariate
ρ→q 1 1 d d
(and increasing, by Theorem 1.14) maps T : R → R. However, the
i
class of such maps is still infinite-dimensional and needs to be further
restricted for implementation purposes.
To do so, we take a finite family M of optimal transport maps—
called the dictionary—and take as our eventual family of maps the set
cone(M) of all conic combinations of elements of M:
cone(M) =
(cid:110)(cid:88)
λ T
(cid:12)
(cid:12) λ ∈
RM(cid:111)
.
T (cid:12) +
T∈M
We denote Tλ := (cid:80) λ T and ρλ := (Tλ) ρ. This set is now finite-
T∈M T #
dimensional, and in fact is parameterized by the positive orthant
RM
.
+
Therefore, we can now optimize the functional λ (cid:55)→ KL(ρλ∥π) over RM .
+
Note that we have replaced our original variational family P (R)⊗d
2,ac172 6 Wasserstein gradient flows: applications
with the smaller set cone(M) ρ, but the hope is that for an appropriate
#
choice of M, the family cone(M) ρ is expressive enough to approxi-
#
mately capture all of P (R)⊗d. In [JCP24], this is indeed shown to
2,ac
be the case, e.g., when M consists of increasing and piecewise linear
functions which act on a single coordinate, and that the total size of M
is polynomially bounded in the problem parameters.
We have the following lemma, whose proof we leave as Exercise 4.
Lemma 6.9.Assume that M consists of maps T which are separable,
in the sense that T(x) = (T (x ),...,T (x )) for increasing univariate
1 1 d d
maps T ,...,T : R → R. Then, cone(M) ρ is geodesically convex.
1 d #
Moreover, the map (RM ,∥·∥ ) → (cone(M) ρ,W ), λ (cid:55)→ ρλ is an
+ Q # 2
isometry, where ∥x∥2 := ⟨x,Qx⟩ and Q is the |M|×|M| matrix with
Q
entries
Q := ⟨T,T′⟩ , T,T′ ∈ M. (6.5)
T,T′ ρ
The first statement of Lemma 6.9 shows that under strong log-
concavity for π, the problem of minimizing KL(·∥π) over cone(M) ρ
#
is a strongly convex problem in the Wasserstein geometry, and in
particular, that Corollary 6.6 applies. The second statement shows that
implementing the Wasserstein gradient flow in this case amounts to
implementing a Euclidean gradient flow up to preconditioning by Q−1.
Indeed, the isometry implies that the Wasserstein gradient flow of the
KL divergence over cone(M) ρ is equivalent to the gradient flow of
#
λ (cid:55)→ KL(ρλ∥π) over (RM ,∥·∥ ), and the gradient operator in the ∥·∥
+ Q Q
norm is simply the Euclidean gradient operator premultiplied by the
matrix Q−1. In particular, we can write down the resulting algorithm
explicitly.
Theorem 6.10.The Wasserstein gradient flow of KL(·∥π) restricted
to cone(M) ρ is given by (ρλ(t)) , where
# t≥0
(cid:90)
λ˙ = − (cid:88) (Q−1) (cid:2) ⟨∇V ◦Tλ,T′⟩−⟨(∇Tλ)−1,∇T′⟩(cid:3) dρ
T T,T′
T′∈M
and the matrix Q is given in (6.5).
Proof. Lemma 6.9 implies that the Wasserstein gradient flow is given
by λ˙
t
= −Q−1∇ λKL(ρλt ∥π). We compute
(cid:90) (cid:90)
∂ V(ρλ) = ∂ V ◦Tλdρ = ⟨∇V ◦Tλ,T⟩dρ
λT λT6.1 Variational inference 173
and
(cid:90)
∂ H(ρλ) = ∂ ρλlogρλ
λT λT
(cid:90)
= −∂ logdet∇Tλdρ
λT
(cid:90)
= − ⟨(∇Tλ)−1,∇T⟩dρ,
where we used respectively Lemma 6.2 and a classical result of matrix
calculus to compute the gradient of the logdet function. ⊔⊓
As in the previous subsection, the expectations can be estimated
using Monte Carlo averages.
6.1.4 Stein variational gradient descent
We now ask whether we can minimize the KL divergence over the family
Q of empirical measures—measures of the form 1 (cid:80)N δ . Unlike the
N i=1 xi
precedingtwosubsections,thisclassisarbitrarilyexpressive:asN → ∞,
we can always find a sequence of empirical measures that converges
weaklytoπ,asaconsequenceofthelawoflargenumbers;seeChapter2.
Recall from Section 5.8 that the Wasserstein gradient flow of a
functional F over the full Wasserstein space can be represented via
X˙ = −∇∇F(µ )(X ), X ∼ µ , (6.6)
t t t t t
provided that ∇∇F(µ ) makes sense. Note also that unlike the previous
t
two subsections, we do not have to do anything special to ensure that
µ remains an empirical measure for all t ≥ 0: the gradient flow (6.6)
t
automatically preserves the space of empirical measures.
If we specialize this to F = KL(·∥π), then (6.2) leads to
µ
X˙ = −∇log t (X ), X ∼ µ . (WGF)
t t t t
π
Unfortunately, the expression ∇log(µ /π) does not make sense when
t
µ is an empirical measure.
t
The next idea that springs to mind is to replace µ in (WGF) with a
t
smoothed version via a kernel density estimator (KDE). More precisely,
let us initialize N particles X1,...,XN, and let k : Rd → R be a
0 0 +
(cid:82)
symmetric kernel with k = 1. At time t, we replace µ by the KDE
t
µ = 1 (cid:80)N k(·−Xi), whichleads to an interactingsystemof particles:
(cid:98)t N i=1 t174 6 Wasserstein gradient flows: applications
N
X˙i =
−∇V(Xi)−(cid:104)
∇log
1 (cid:88) k(·−Xj)(cid:105)
(Xi), i ∈ [N].
t t N t t
j=1
In the mean-field limit N → ∞, we expect that 1 (cid:80)N k(·−Xj) →
N j=1 t
(cid:82)
k(·−y)µ (dy) = k⋆µ , where µ = law(X ), leading to the dynamics
t t t t
X˙ = −∇V(X )−∇log(k⋆µ )(X )
t t t t
or equivalently
(cid:0) (cid:1)
∂ µ = div µ (∇V +∇log(k⋆µ )) .
t t t t
However, these dynamics do not necessarily converge to the target
π ∝ exp(−V). This issue can be fixed by introducing a bandwidth
parametertothekernelk whichtendstozeroasN → ∞,butthereisan
alternative which stems from the RKHS literature (recall the discussion
in Subsection 2.8.2) which we describe next. The Stein variational
gradient descent (SVGD) algorithm, due to [LW16], manages to use a
fixed kernel k but still admits π as a stationary solution.
We define the integral operator
(cid:90)
K : f (cid:55)→ f(y)k(·−y)µ(dy),
µ
and we follow the dynamics
(cid:16) µ (cid:17)
∂ µ = div µ K ∇log t , (SVGD)
t t t µt
π
where the integral operator acts on vector fields coordinate-wise. Clearly
these dynamics leave π stationary. Let us calculate the effect of the
integral operator above. First,
(cid:90)
1
K ∇log = K ∇V = ∇V(y)k(·−y)µ(dy). (6.7)
µ µ
π
For the other term, we use integration by parts:
(cid:90) (cid:90)
∇µ(y)
K ∇logµ = k(·−y)µ(dy) = ∇µ(y)k(·−y)dy
µ
µ(y)
(cid:90)
= ∇k(·−y)µ(dy). (6.8)6.2 Sampling 175
Both (6.7) and (6.8) are expectations w.r.t. µ, so they can be replaced
by empirical averages over N particles. This leads to the algorithm
N
X˙i = − 1 (cid:88) [k(Xi−Xj)∇V(Xj)+∇k(Xi−Xj)].
t N t t t t t
j=1
AlthoughSVGDhasbeenanactivesubjectofresearch,manytheoretical
questions regarding its convergence remain open.
Recall that we motivated SVGD with the idea of approximating π
by an empirical measure, noting that the class of empirical measures
over N atoms is arbitrarily expressive as N → ∞. But if our ultimate
goal is fidelity with respect to π, we may as well ask if we can directly
outputsamplesfromπ itself.Inthenextsection,wediscusstheproblem
of sampling via MCMC methods, which can be viewed as stochastic
implementations of the Wasserstein gradient flow.
6.2 Sampling
One of the most compelling applications of the theory of Wasserstein
gradient flows is to provide a geometric interpretation of the Langevin
diffusion, as put forth in the seminal work of Jordan, Kinderlehrer, and
Otto [JKO98]. As before, let V : Rd → R be a smooth potential with
(cid:82) exp(−V) < ∞ and let π denote the probability measure over Rd with
density π ∝ exp(−V).
Suppose that we wish to sample from the distribution π. In other
words, we want to design an algorithm for producing a random variable
whose law is close to π. For example, π could be the posterior distribu-
tion in a Bayesian inference problem, in which case basic downstream
tasks such as constructing credible regions or point estimates are often
intractable in non-conjugate models. Nevertheless, we can usually solve
these tasks approximately and efficiently, given a subroutine for drawing
approximate samples from the posterior. Beyond the application to
computational Bayesian statistics, sampling also plays an important
role in scientific computing through Monte Carlo integration and for
the design of randomized algorithms.
The predominant approach to this problem, dubbed Markov chain
Monte Carlo (MCMC), is to design a Markov chain whose unique
stationary distribution is, or at least is close to, the target π. When
π admits a positive and smooth density, as we assume in this section,176 6 Wasserstein gradient flows: applications
then we can write π ∝ exp(−V) without loss of generality (with V =
log(1/π)+const.).Inthiscase,acanonicalMCMCalgorithmisobtained
by discretizing the Langevin diffusion, which is the solution to the
stochastic differential equation (SDE)
√
dX = −∇V(X )dt+ 2dB ,
t t t
where (B ) is a standard Brownian motion. As soon as ∇V is, e.g.,
t t≥0
Lipschitz continuous, there is a unique strong solution to this SDE for
any prescribed initial condition, and its stationary distribution is π.
In the next section, we show that when we track the evolution of
the marginal law µ := law(X ) of the Langevin diffusion, then (µ )
t t t t≥0
follows the Wasserstein gradient flow of the KL divergence KL(·∥π).
More broadly, this story is the starting point of a fruitful literature
which has blossomed in recent years on an optimization perspective
(i.e., the application of optimization algorithms such as gradient flows)
on the problem of sampling.
6.2.1 The Langevin diffusion as a Wasserstein gradient flow
Tospoilthesurprise,thefundamentalreasonwhy(6.2)admitsastochas-
tic implementation is because we can rewrite
(cid:0) ∇µ t(cid:1)
div(µ ∇logµ ) = div µ = div(∇µ ) = ∆µ
t t t t t
µ
t
where ∆f = (cid:80)d ∂2f is the Laplacian of f. On the other hand,
i=1 i
second-order parabolic PDEs—the heat equation ∂ µ = ∆µ being the
t t t
most fundamental example—classically describe the evolution in law of
stochastic differential equations driven by Brownian motion. The rest
of this subsection aims to make this connection precise.
Using the computation above, we rewrite the Wasserstein gradient
flow of the KL divergence, given in (6.2), as
∂ µ = ∆µ +div(µ ∇V). (6.9)
t t t t
We next compute the marginal evolution of the Langevin diffusion
in order to compare with (6.9). The usual method for doing so is to
use Itoˆ’s formula from stochastic calculus, but we instead proceed more
informally. First, let us condition on X = x , and write the Langevin
0 0
diffusion in integral form.6.2 Sampling 177
(cid:90) t √
X = x − ∇V(X )ds+ 2B .
t 0 s t
0
Recall also that B
t
∼ N(0,tI). In particular, ∥(cid:82) 0t ∇V(X s)ds∥ = OP(t)
and∥B t∥ = OP(t1/2)forsmallt,sotheBrownianmotiontermdominates
and ∥X t−x 0∥ = OP(t1/2). By duality, to calculate the evolution of µ t, it
suffices to compute the evolution of the expectation of any test function
φ : Rd → R. A Taylor expansion yields
(cid:16) (cid:90) t √ (cid:17)
φ(X ) = φ x − ∇V(X )ds+ 2B
t 0 s t
0
(cid:68) (cid:90) t √ (cid:69)
= φ(x )+ ∇φ(x ),− ∇V(X )ds+ 2B
0 0 s t
0
1 (cid:68) (cid:16) (cid:90) t √ (cid:17)⊗2(cid:69)
+ ∇2φ(x 0), − ∇V(X s)ds+ 2B
t
+OP(t3/2).
2
0
The first-order term equals
√
−t⟨∇φ(x 0),∇V(x 0)⟩+ 2⟨∇φ(x 0),B t⟩+OP(t3/2).
The second-order term equals
⟨∇2φ(x 0)B t,B t⟩+OP(t3/2).
Therefore,
√
φ(X ) = φ(x )−t⟨∇φ(x ),∇V(x )⟩++ 2⟨∇φ(x ),B ⟩
t 0 0 0 0 t
+⟨∇2φ(x 0)B t,B t⟩+OP(t3/2).
Taking expectations and using E[B ] = 0, E[B BT] = tI,
t t t
Eφ(X t) = φ(x 0)+t(cid:0) tr∇2φ(x 0)−⟨∇φ(x 0),∇V(x 0)⟩(cid:1) +OP(t3/2)
= φ(x 0)+t(cid:0) ∆φ(x 0)−⟨∇φ(x 0),∇V(x 0)⟩(cid:1) +OP(t3/2).
Subtracting φ(x ), dividing by t, and letting t ↘ 0,
0
∂ tEφ(X t)(cid:12) (cid:12)
t=0
= ∆φ(x 0)−⟨∇φ(x 0),∇V(x 0)⟩. (6.10)
In the language of Markov semigroup theory, we have computed the
generator of the Langevin diffusion to be the second-order differential
operator L, defined by Lφ := ∆φ−⟨∇φ,∇V⟩.178 6 Wasserstein gradient flows: applications
More generally, by first conditioning on the value of X and using
t
the Markov property and (6.10), it holds that
∂ Eφ(X ) = ELφ(X ).
t t t
Expressed in terms of the marginal law µ , it reads
t
(cid:90) (cid:90)
φ∂ µ = (∆φ−⟨∇φ,∇V⟩)dµ .
t t t
In order to identify an equation for ∂ µ , we must compute the adjoint
t t
(w.r.t.Lebesguemeasure)ofL.Thisisaccomplishedthroughintegration
by parts, which shows that the right-hand side equals
(cid:90)
(cid:0) (cid:1)
φ ∆µ +div(µ ∇V) .
t t
We have established the following theorem.
Theorem 6.11(Fokker–Planck equation). The marginal law µ :=
t
law(X ) of the Langevin diffusion with potential V is given by the
t
solution to the Fokker–Planck equation
∂ µ = ∆µ +div(µ ∇V).
t t t t
Comparing with (6.9), it yields:
Corollary 6.12.The marginal law of the Langevin diffusion with po-
tential V is the Wasserstein gradient flow of KL(·∥π), where π has
density proportional to exp(−V).
As a special case when V = 0, we also obtain the following corollary.
Corollary 6.13.If (µ ) is the marginal law of a (rescaled) Brownian
√ t t≥0
motion ( 2B ) , then (µ ) solves the heat equation ∂ µ = ∆µ ,
t t≥0 t t≥0 t t t
and it is the Wasserstein gradient flow of the entropy functional H.
WeshowedinCorollary6.6thatthestronglog-concavityofπ implies
rapid convergence of the Wasserstein gradient flow of the KL divergence.
Therefore, we immediately obtain the following elegant convergence
result for the Langevin diffusion.
Corollary 6.14.Let π be an α-strongly log-concave measure, and let
(µ ) denote the marginal law of the Langevin diffusion with stationary
t t≥0
distribution π. Then,
KL(µ ∥π) ≤ e−2αtKL(µ ∥π).
t 06.2 Sampling 179
To conclude this section, we take stock of the situation at hand.
Recall that the Wasserstein gradient flow of the KL divergence can be
implemented via the deterministic evolution (WGF). On the other hand,
in this subsection, we started with the Langevin diffusion, which is a
stochastic evolution:
√
dX = −∇V(X )dt+ 2dB . (LD)
t t t
WeshowedinTheorem6.11thatthemarginallawµ := law(X )evolves
t t
according to the Fokker–Planck equation
∂ µ = ∆µ +div(µ ∇V), (FP)
t t t t
andmoreoverthatthisevolutioncoincideswiththeWassersteingradient
flowofKL(·∥π).Notethat(FP)isstrictlycoarserthan(LD)because(LD)
also includes information about correlations between different time
points of the stochastic process.
Ultimately, we have the equivalence (FP) ⇔ (LD) ⇔ (WGF) in the
sense that they correspond to the same curve on the space of probability
measures—clearly at the particle level, they are different—but these
three differing perspectives provide new avenues for algorithm design
and theoretical study.
6.2.2 Sampling as optimization
The gradient flow perspective on the Langevin diffusion is the starting
point of a flourishing literature on an optimization perspective on sam-
pling. In this subsection, we study the basic properties of KL divergence
minimization as an optimization problem, inspired by the treatment
in [Wib18]. Then, in the next subsection, we provide a glimpse of the
recent impact of this perspective on the theory of log-concave sampling.
We refer to the monograph [Che24] for a detailed exposition.
Let V : Rd → R be a potential which is α-convex, α > 0, and recall
that our goal is to output a sample from the target π ∝ exp(−V).
Corollary 6.14 then ensures that the continuous-time Langevin diffusion
converges rapidly to π, but in order to obtain algorithmic guarantees
we must discretize the process, and for this we also impose the dual
assumption of smoothness, ∇2V ⪯ βI, to ensure stability.
Error estimates for discretizations of the Langevin diffusion are
by now well-established. Under our assumptions of strong convexity180 6 Wasserstein gradient flows: applications
and smoothness of V, non-asymptotic convergence guarantees can be
established for the Euler–Maruyama scheme
X = X −h∇V(X )+N(0,2hI), k = 0,1,2,... , (6.11)
k+1 k k
which parallels the complexity theory for optimization [Nes18]. Unlike
thesituationinoptimization,however,thediscretization(6.11)isasymp-
totically biased: the stationary distribution of the Markov chain (6.11)
does not equal the target π. This leads to slower rates of convergence,
as the step size h must be chosen small to mitigate the bias. We now
explain how an optimization perspective sheds light on the source of
asymptotic bias and suggests a proximal scheme for removing it.
As in (6.1), we write the KL divergence as the sum of the potential
energy and the entropy,
(cid:90) (cid:90)
KL(µ∥π) = V dµ+ µlogµ+const. = V(µ)+H(µ)+const.
The problem of sampling from π is cast as the minimization of this
objective functional over P (Rd), and we have already begun studying
2,ac
its properties. Namely:
• The potential energy is strongly convex and smooth, α ≤ ∇∇2V ≤ β.
We proved the lower bound in Theorem 6.1, and the upper bound
follows by a similar computation.
• The entropy is convex, 0 ≤ ∇∇2H. We proved this as Theorem 6.3.
However, the entropy is non-smooth.2
The situation at hand is one that is commonly encountered in optimiza-
tion, known as composite optimization: minimize the sum f +g, where
f is (strongly) convex and smooth, and g is convex but non-smooth.
The prototypical example is the ℓ -penalized least squares objective (or
1
LASSO), θ (cid:55)→ ∥y−Xθ∥2+λ∥θ∥ . In optimization theory, the canonical
1
algorithm designed for such problems is the proximal gradient method
x = prox (x −h∇f(x )), (6.12)
k+1 hg k k
where
(cid:110) 1 (cid:111)
prox (y) := argmin hg(x)+ ∥y−x∥2 . (6.13)
hg 2
x∈Rd
2 One can in fact show that ∇∇2H(µ)[v,v] = (cid:82) ∥∇v −I∥2 dµ and there is no
HS
constant C >0 such that ∇∇2H(µ)[v,v]≤C∥v∥2 for all v∈T P (Rd).
µ µ 2,ac6.2 Sampling 181
When g has a simple structure, such as the ℓ norm, then the proximal
1
mapping sometimes admits a closed-form solution.
The proximal gradient algorithm is unbiased, meaning that its only
fixed points are minimizers of f +g. Moreover, one can show that the
iteration (6.12) converges at the same rate that gradient descent would
for a convex and smooth objective, despite the non-smoothness of g.
More broadly, we have introduced two discretization schemes: gradi-
ent descent, and the proximal step (6.13). Each has its relative merits.
Whereas gradient descent is cheaper to implement (especially for func-
tions which do not have a simple structure like ∥·∥ ), the proximal
1
schemeconvergesevenwithoutsmoothness.Therefore,wearemotivated
to apply one discretization method to f, and the other to g; this is
known as a splitting scheme. Not all splitting schemes are unbiased,
however, and the combination of gradient descent and the proximal
map is an especially auspicious match.3
With these principles from optimization in mind, let us now consider
the situation for sampling. The discretization (6.11) can be viewed as
the splitting scheme
X = X −h∇V(X ),
k+1/2 k k
X = X +N(0,2hI).
k+1 k+1/2
The two steps correspond, respectively, to gradient descent4 for V and
the gradient flow of H; the latter statement is Corollary 6.13. The
combination of gradient descent and gradient flow does not produce an
unbiased splitting scheme.
The intuition from optimization suggests to replace the gradient flow
for H with the proximal map for H. Generalizing the definition (6.13)
to the Wasserstein space, we arrive at
(cid:110) 1 (cid:111)
prox (µ) := argmin hH(ν)+ W2(µ,ν) . (6.14)
hH 2 2
ν∈P 2,ac(Rd)
(In fact, the proximal operator on the Wasserstein space was the device
through which [JKO98] first made precise the Wasserstein gradient flow
interpretation of the Langevin diffusion in Subsection 6.2.1.) The result-
ing proximal gradient algorithm on the Wasserstein space can indeed
3 In numerical analysis, these two discretizations are so-called “adjoints” to each
other, see [Wib18, Appendix B].
4 Check that if µ =law(X ), µ =law(X ), and h≤1/β, then µ =
k k k+1/2 k+1/2 k+1/2
exp (−h∇∇V(µ )), which is the Riemannian analogue of gradient descent.
µk k182 6 Wasserstein gradient flows: applications
be shown to converge rapidly to π [SKL20]. However, the proximal
map (6.14) is in general intractable, so the proximal gradient algorithm
is merely wishful thinking.
Actually, it is not just prox that is intractable; gradient descent
hH
on H is also impractical because it requires knowing the entire prob-
ability density (this is the motivation for our discussion of SVGD in
Section 6.1.4). It seems that the only operation we can reasonably
implement for minimizing H is the gradient flow, due to the fortuitous
link with Brownian motion. This can be considered both as a blessing
and curse. The blessing is that the routine we can implement—gradient
flow—succeeds despite the non-smoothness of H, which explains why
sampling is possible at all. The curse, however, is that the gradient
flow is “mismatched” with our discretization for V, and hence the
discretization (6.11) incurs asymptotic bias.
This is perhaps representative of the subject as a whole: although
optimization theory suggests a huge number of algorithmic paradigms
which we hope to port over to the world of sampling, the execution of
these ideas requires care. In the next subsection, we survey a number
of examples in which this philosophy has been successfully carried out,
including a surprising “proximal” algorithm for sampling.
6.2.3 Some recent developments
Algorithms
Open any modern book on convex optimization to find a formidable
arsenal of methods: coordinate descent, gradient descent, interior point,
mirror descent, Newton’s method, Nesterov’s fast gradient method,
proximal gradient, stochastic gradient descent, etc. In recent years, a
substantial research effort has been devoted to developing sampling
analogues of all of these methods and more, of which we describe only
a select few.
Our first example is the aforementioned proximal algorithm for
sampling. Since it is easier to motivate a posteriori, we begin by defining
the algorithm. Augment the target distribution π to form a distribution
π over Rd×Rd with density given by
(cid:16) 1 (cid:17)
π(x,y) ∝ exp −V(x)− ∥y−x∥2 .
2h
The proximal sampler, introduced in [LST21], applies Gibbs sampling
to the new target π. Explicitly, repeat the following steps:6.2 Sampling 183
1. Given X = x, resample Y ∼ πY|X=x = N(x,hI).
2. Given Y = y, resample X ∼ πX|Y=y, where the conditional distri-
bution πX|Y=y, called the restricted Gaussian oracle (RGO), has
density πX|Y=y(x) ∝ exp(−V(x)− 1 ∥y−x∥2).
2h
A few simple properties can be verified immediately. First, the X-
marginalofπ istheoriginaltargetπ ∝ exp(−V),soitsufficestosample
from the augmented target. Second, the proximal sampler is unbiased—
itsstationarydistributionisπ—becauseGibbssamplingisso.Asstated,
however, it is still an idealized algorithm, since it is unclear how to
implement step two. But notice the following analogy: if minimizing V
(optimization) corresponds to sampling from π ∝ exp(−V) (sampling),
then computing the proximal map (6.13) for V corresponds to sampling
from πX|Y=y; it is in this sense that the proximal sampler resembles a
“proximal” algorithm for sampling.
But perhaps the most convincing justification is based on the adage
“ifitlookslikeaduck...”:theconvergenceanalysesin[LST21,CCSW22]
show that the convergence rates for the proximal sampler exactly repli-
cate the rates for the proximal point method from convex optimization.
Through careful implementations of the RGO, the proximal sampler
has played an essential role in extending sampling guarantees to both
non-log-concave and non-log-smooth settings [FYC23, AC24].
Our next example is the adaptation of mirror descent. Recall that
the mirror descent algorithm for minimizing V, originally introduced
in [NY83] for optimization w.r.t. non-Euclidean norms, starts by choos-
ing a strictly convex function (the “mirror map”) ϕ : Rd → R∪{∞}
and iterating
∇ϕ(x ) = ∇ϕ(x )−h∇V(x ), k = 0,1,2,... .
k+1 k k
It turns out that the “mirror” analogue of (LD) is the so-called mirror
Langevin diffusion [ZPFP20, CLGL+20b]:
(cid:112)
Y := ∇ϕ(X ), dY = −∇V(X )dt+ 2∇2ϕ(X )dB .
t t t t t t
The mirror Langevin diffusion can also be suitably interpreted as a
Wasserstein “mirror” flow of the KL divergence.5 An important special
case is obtained when V is strictly convex and we take the mirror
5 Morespecifically,itisthegradientflowofKL(·∥π)withrespecttotheWasserstein
geometry induced by the Hessian metric induced by ϕ.184 6 Wasserstein gradient flows: applications
map ϕ = V, leading to the Newton–Langevin diffusion—the sampling
analogue of Newton’s method:
(cid:112)
Y := ∇V(X ), dY = −Y dt+ 2∇2V(X )dB .
t t t t t t
The Newton–Langevin diffusion inherits some appealing properties of
Newton’smethod,suchasitsaffineinvariance.However,discretizationof
the Newton–Langevin diffusion (or of the more general mirror Langevin
diffusion) is currently less well-understood than for (LD).
Finally, our last example is the adaptation of Nesterov’s accelerated
gradient descent [Nes83], which is an optimal first-order method for
convex smooth minimization. The corresponding SDE system, called
the underdamped (or kinetic) Langevin diffusion, dates back at least
to [Kol34] and is given by
(cid:112)
dX = P dt, dP = −∇V(X )dt−γP dt+ 2γdB .
t t t t t t
Here, P represents a momentum variable, and γ > 0 is the “friction”
parameter. This diffusion has already formed the basis for numerous
state-of-the-art guarantees for log-concave sampling. But in the world
of optimization, Nesterov’s method is best known for improving the
√
complexity of strongly convex and smooth minimization to O(cid:101)( κ),
where κ is the “condition number”. This remarkable result, which saves
√
a factor of κ over the basic rate for gradient descent, has been dubbed
theacceleration phenomenon,anditremainsanintriguingopenquestion
to establish such a phenomenon for sampling.
Complexity
Since the work of [NY83], a major goal of optimization research has
been to precisely characterize the minimax complexity of optimization
over various function classes and oracle models. With the advent of
this mindset to MCMC, it became natural to do the same for sampling,
starting with the non-asymptotic upper bounds in works such as [Dal17,
DM17,CB18,DMM19].Thesimilaritiesarestriking:bothfieldsconsider
similar choices for the function classes (e.g., strongly convex and smooth
functions) and for the oracle models. This connection also motivated
the search for oracle lower bounds which could certify the optimality of
our existing algorithms. This has proven to be a challenging problem,
with some modest progress made recently.
Another example of the transfer of ideas is the development of
a sampling analogue of “approximate first-order stationarity” which6.3 Interacting particle systems 185
provides an alternative approach to the quantitative study of sampling
in general non-log-concave settings [BCE+22].
Despite rapid progress in this direction, there are still many funda-
mental unresolved questions regarding the complexity of log-concave
sampling. We refer to the monograph [Che24] for an introduction to
this active field and for further references.
6.3 Interacting particle systems
The SDE systems we encountered in Section 6.2 all involve evolving
a single particle (possibly over an expanded state space) at a time.
More generally, we can consider an interacting system of particles,
either deterministic or stochastic. This is highly relevant because as
we discussed in Subsection 5.8.1, Wasserstein gradient flows initialized
at discrete measures can be implemented using mean-field interacting
particle systems. Indeed, recall that if we initialize the Wasserstein
gradient flow (6.6) at
N
1 (cid:88)
µN = δ ,
0 N Xj
0
j=1
then µ is given by the empirical measure
t
N
1 (cid:88)
µN = δ ,
t N X tj
j=1
where
N
X˙i = −∇∇F(cid:0) 1 (cid:88) δ (cid:1) , i ∈ [N]. (6.15)
t N X tj
j=1
This is true whenever the Wasserstein gradient is defined at the discrete
measure µN, and in this case, it is generally expected (and can be
t
rigorously established under assumptions on F) that as N → ∞ and
µN → µ ∈ P (Rd), the dynamics (6.15) converges to (6.6) initialized
0 0 2,ac
at µ . Using additional tools, one may also show that if µ ∈ P (Rd)
0 0 2,ac
then µ ∈ P (Rd) for all t ≥ 0.
t 2,ac186 6 Wasserstein gradient flows: applications
6.3.1 McKean–Vlasov equations
In this subsection, we consider other examples of interacting systems
arising from Wasserstein gradient flows. Recall that in Section 5.4, we
gave three fundamental examples of functionals over the Wasserstein
space: potential energy, internal energy, and interaction energy. What if
we consider the sum of the three?
(cid:90) (cid:90)(cid:90) σ2 (cid:90)
F(µ) := V dµ+ W(x−y)µ(dx)µ(dy)+ µlogµ, (6.16)
2
where W is even. By using the trick at the beginning of Subsection 6.2.1
and by computing Wasserstein gradients, convince yourself of the fol-
lowing theorem.
Theorem 6.15.The Wasserstein gradient flow (µ ) of (6.16) can
t t≥0
be described as follows: µ = law(X ), where (X ) solves the SDE
t t t t≥0
(cid:90)
dX = −∇V(X )dt− ∇W(X −y)µ (dy)dt+σdB . (6.17)
t t t t t
Note that the coefficients of the SDE system (6.17) depend on
the law of the process. Such systems are called McKean–Vlasov pro-
cesses [McK66].Toapproximate(6.17)byaninteractingparticlesystem,
we replace the integral over µ with an average over particles:
t
dXi = −∇V(Xi)dt− 1 (cid:88) ∇W(Xi−Xj)dt+σdBi, (6.18)
t t N −1 t t t
j∈[N]\i
where {Bi} is a collection of independent Brownian motions. A
i∈[N]
natural question that arises is to quantify how close the finite-particle
system (6.18) is to its mean-field limit (6.17). This problem is addressed
by the mathematical theory of propagation of chaos [Szn91].
More generally, suppose that we have the general entropically-
regularized functional
σ2 (cid:90)
F(µ) := F (µ)+ µlogµ. (6.19)
0
2
One can show that the Wasserstein gradient flow (µ ) of (6.19) can
t t≥0
be described as the marginal law µ = law(X ) of the SDE system
t t
dX = −∇∇F (µ )(X )dt+σdB .
t 0 t t t6.3 Interacting particle systems 187
This system is known as the mean-field (or interacting) Langevin dy-
namics [CRW23, SNW23]. The corresponding finite-particle system,
n
dXi = −∇∇F (cid:0) 1 (cid:88) δ (cid:1) (Xi)dt+σdBi, i ∈ [N],
t 0 N X tj t t
j=1
is actually the Langevin diffusion corresponding to the target
N
πˆ (x1,...,xN) ∝ exp(cid:16) −2N F (cid:0) 1 (cid:88) δ (cid:1)(cid:17) .
N σ2 0 N X tj
j=1
Thecomplexityofsamplingfromtheminimizersofthefunctionals(6.16)
and (6.19) was studied in [KZC+24].
We conclude this section with an application to the mean-field VI
problem introduced in Subsection 6.1.3. By writing down the stochastic
implementation of the Wasserstein gradient flow therein, [Lac23] arrived
at the McKean–Vlasov SDE
(cid:90) √
dX = − ∇ W(X ,y)µ (dy)dt+ 2dB ,
t 1 t t t
where µ = law(X ), ∇ denotes the gradient taken with respect to the
t t 1
first argument, and
d
(cid:88)
W(x,y) := V(y ,...,y ,x ,y ,...,y ).
1 i−1 i i+1 d
i=1
When this SDE is initialized at X ∼ µ which is a product measure,
0 0
µ remains a product measure for all t ≥ 0 so that (µ ) is indeed the
t t t≥0
constrained Wasserstein gradient flow.
The corresponding interacting particle system is given by
dXj = − 1 (cid:88) ∇ W(Xj,Xj′ )dt+√ 2dBj, j ∈ [N].
t N −1 1 t t t
j′∈[N]\i
OnethenexpectsthatifwetakeN sufficientlylargeandruntheparticle
system, then the law of (say) the first particle X1 will be close to the
t
mean-field minimizer q .
⋆188 6 Wasserstein gradient flows: applications
6.3.2 Birth-death sampling
WereturntothesamplingproblemfromSection6.2.Insteadoffollowing
the Wasserstein gradient flow of the KL divergence, what if we follow
the WFR gradient flow introduced in Section 5.7? The fundamental
difference between these approaches is that the Langevin diffusion is
a local algorithm and hence struggles to jump between well-separated
modes. This manifests itself in the convergence rate in Corollary 6.14,
which depends on the log-Sobolev constant of the target π. On the other
hand, the “teleportation” effect of the Fisher–Rao component gives rise
to a universal exponential convergence rate [DEP23].
The main challenge is to implement the flow. Recall that a particle
implementation for the WFR gradient flow was presented in Subsec-
tion 5.8.2, but it does not apply to the choice of functional F = KL(·∥π)
since it assumed there that the first variation δF can be evaluated at
an empirical measure. An implementation was provided in [LLN19b]
under the name of “birth-death” sampling, which we now describe.
The implementation is based on an interacting system of N particles,
which at time t are denoted X1,...,XN. The approach of Subsec-
t t
tion 5.8.2 would associate with each particle Xi a weight wi which
t t
evolves via w˙i = −α (Xi)wi, where α (x) := δF(µ )(x)−(cid:82) δF(µ )dµ .
t t t t t t t t
Here, α (x) represents an exponential rate of decay/growth (according
t
to α (x) > 0 or α (x) < 0) of the density at x. We instead replace the
t t
use of weights with a procedure that “kills” or “duplicates” the particle
after a random wait time. More precisely, associate with each particle
Xi an independent clock which rings in the next instantaneous time
t
interval [t,t+dt] with probability α (Xi)dt. Whenever one of these
t t
clock rings—corresponding to, say, Xi—we either remove Xi from the
t t
system (if α (Xi) > 0) or we duplicate Xi (if α (Xi) < 0). To keep the
t t t t t
total number of particles constant, in the former (resp. latter) case we
randomly duplicate (resp. kill) one of the other particles.
The birth-death process implements the Fisher–Rao component of
the WFR gradient flow. To implement the Wasserstein component,
we stipulate that each particle evolves independently according to a
Langevin diffusion between birth-death events.
The preceding discussion is ambiguous: what is the measure µ ?
t
Ideally it should be the marginal law of Xi (which is independent of
t
i due to exchangeability), but we do not have access to this marginal
law for implementation purposes. The methodology from the previous
subsection suggests to replace µ by the empirical measure µN :=
t t6.4 Non-parametric maximum likelihood 189
1 (cid:80)N δ over the particles, but for the KL divergence the first
N i=1 Xi
t
variation is not well-defined at such a measure. Therefore, as suggested
in [LLN19b], we resort to a kernel density estimator, replacing µ with
t
k⋆µN for an appropriate kernel function k : Rd → R. This leads to the
t
following algorithm.
1. Associate with each particle Xi an independent clock that rings
t
with instantaneous rate α (Xi), where
(cid:98)t t
α (x) := log
k⋆µN
t (x)−
1 (cid:88)N
log
k⋆µN
t (Xj).
(cid:98)t π N π t
j=1
Note that when π is given as an unnormalized density π ∝ exp(−V),
the computation of α does not require knowledge of the normaliza-
(cid:98)t
tion constant for π.
2. When one of the clock rings, kill or duplicate the corresponding par-
ticle Xi, and randomly duplicate or kill another particle, according
t
to α (Xi) > 0 or α (Xi) < 0.
(cid:98)t t (cid:98)t t
3. Between the rings of the clocks, each particle evolves according to a
Langevin diffusion:
√
dXi = −∇V(Xi)dt+ 2dBi, i ∈ [N],
t t t
where {Bi} are i.i.d. Brownian motions.
i∈[N]
As shown in [LLN19b], the marginal laws of this process converge to
the WFR gradient flow as N → ∞ and the bandwidth of the kernel
tends to zero appropriately.
6.4 Non-parametric maximum likelihood
We have just seen that sampling can be viewed as optimization over
the space of probability measures using the perspective originally put
forward in [JKO98] and [Wib18]. In this section we study a classical
statistical problem that is readily of this nature.
Consider a Gaussian mixture on Rd with density given by:
(cid:90)
G = ϕ(·−y)ρ(dy) = ϕ⋆ρ,
ρ
Rd
whereϕ(z) = (2π)−d/2exp(−∥z∥2/2)denotesthedensityofthestandard
isotropic Gaussian distribution N(0,I) and ρ is the mixing distribution190 6 Wasserstein gradient flows: applications
of interest. Note that in comparison with the general Gaussian mixtures
introduced in Section 5.6, we constrain all the Gaussian components to
have identity covariance matrix for simplicity.
Let ρ⋆ be an unknown mixing distribution on Rd. Given n indepen-
dent observations X ,...,X drawn from G , our goal is to estimate
1 n ρ⋆
ρ⋆. This is a Gaussian deconvolution problem, for which the rates of con-
vergenceareknowntobeveryslow[RNW19].Whenρ⋆ isassumedtobe
smooth, a classical approach that leads to optimal rates of convergence
uses kernel smoothing [Fan91]. In this section, we explore a different ap-
proach called non-parametric maximum likelihood estimation following
the paper [YWR24].
The negative log-likelihood for this problem is defined as:
n n
1 (cid:88) 1 (cid:88)
ℓ (ρ) := − logG (X ) = − logϕ⋆ρ(X ).
n ρ i i
n n
i=1 i=1
The non-parametric maximum likelihood estimator, or NPMLE, is
defined as any minimizer of ℓ :
n
ρˆ= argminℓ (ρ). (6.20)
n
ρ∈P(Rd)
Before turning to the computational aspects of this problem, we first
noteasurprisingconnectionwithentropicoptimaltransport,highlighted
in [RNW18]. Writing µ for the empirical measure 1 (cid:80)n δ , it turns
n n i=1 Xi
out that the NPMLE satisfies
ρˆ= argminS (µ ,ρ), (6.21)
2σ2 n
ρ∈P(Rd)
whereS (·,·)istheentropicoptimaltransportcostwithregularization
2σ2
parameter ε = 2σ2. In other words, the NPMLE precisely minimizes
the entropic OT cost to the data µ .
n
Thisconnectionarisesfromduality.AversionoftheGibbsvariational
principle (see Proposition 4.2) tailored to probability measures rather
than general positive measures implies that for suitable h : Rd → R, it
holds
(cid:90) (cid:110)(cid:90) (cid:111)
log exp(h)dQ = sup hdP −KL(P ∥Q) .
P∈P(Rd)
This result can be found for example as Proposition 1.4.2 in [DE97]. We
can use this expression to obtain a variational formulation of logG (X ):
ρ i6.4 Non-parametric maximum likelihood 191
(cid:90) (cid:16) ∥X −y∥2(cid:17)
−logG (X ) = (2π)d/2−log exp − i ρ(dy)
ρ i 2σ2
(cid:90)
1
= (2π)d/2+ inf ∥X −y∥2P (dy)+KL(P ∥ρ).
Pi∈P(Rd) 2σ2 i i i
This shows that the optimization problem in (6.20) is equivalent to
1 (cid:88)n (cid:104)(cid:90) (cid:105)
ρˆ= argmin inf ∥X −y∥2P (dy)+2σ2KL(P ∥ρ) .
i i i
ρ∈P(Rd) P1,...,Pn∈P(Rd) n
i=1
TheminimizationproblemoverP ,...,P canbeequivalentlyrewritten
1 n
as a minimization over measures of the form γ = 1 (cid:80)n δ ⊗P , which
n i=1 Xi i
are precisely joint measures whose first marginal is µ . It can be shown
n
that the second marginal can be taken to be ρ, which leads to the
representation in (6.21).
The convex infinite-dimensional optimization problem (6.20) has
primarily been studied in the case where d = 1 where it can be
shown that the solution is unique and supported on a small number of
atoms [Lin83, PW24]. Using these properties, various computational
schemes have been proposed by restricting the set of measures to ones
with a small support. In higher dimensions, much less is known.
The definition (6.20) of the NPMLE is precisely an optimization
over probability measures and in the rest of this section we describe
algorithms based on Wasserstein gradient flows to solve it.
We first compute the Wasserstein gradient. To that end, we need
the first variation of ℓ . Fix ε > 0 and ξ be a perturbation such that
n
ρ+εξ is a probability measure and observe that
n
1 (cid:88)
ℓ (ρ+εξ) = − log(ϕ⋆ρ+εϕ⋆ξ)(X )
n i
n
i=1
n n
=
−1 (cid:88)
log(ϕ⋆ρ)(X )−
ε (cid:88) ϕ⋆ξ(X i)
+O(ε2).
i
n n ϕ⋆ρ(X )
i
i=1 i=1
Hence
n n (cid:82)
ℓ n(ρ+εξ)−ℓ n(ρ) 1 (cid:88) ϕ⋆ξ(X i) 1 (cid:88) ϕ(·−X i)dξ
lim = − = − ,
ε↘0 ε n ϕ⋆ρ(X i) n ϕ⋆ρ(X i)
i=1 i=1
and we readily identify that the first variation is given by192 6 Wasserstein gradient flows: applications
n
1 (cid:88) ϕ(·−X i)
δℓ (ρ) = − .
n
n ϕ⋆ρ(X )
i
i=1
The Wasserstein gradient flow of ℓ correspond to the following ODE:
n
θ˙ = −∇∇ℓ (ρ )(θ )
t n t t
n
1 (cid:88) ∇ϕ(θ t−X i)
=
n ϕ⋆ρ (X )
t i
i=1
n
1 (cid:88) (θ t−X i)ϕ(θ t−X i)
= − , (6.22)
n ϕ⋆ρ (X )
t i
i=1
where ρ = law(θ ). In light of the continuity equation (5.2), we see that
t t
the Wasserstein gradient flow of ℓ is the curve described by the PDE:
n
n
1 (cid:88) (cid:16) (·−X i)ϕ(·−X i)(cid:17)
∂ ρ = div ρ .
t t t
n ϕ⋆ρ (X )
t i
i=1
Since the velocity field in (6.22) depends on ρ , we use a particle
t
implementation of this gradient flow: given N particles θ1,...,θN with
t t
marginal distribution ρ , we replace ρ with the empirical distribution
t t
N−1(cid:80)N
δ . It results in the system of coupled ODEs: for j ∈ [N],
j=1 θj
t
θ˙j =
−1 (cid:88)n (θ tj −X i)ϕ(θ tj −X i)
.
t n 1 (cid:80)N ϕ(θk −X )
i=1 N k=1 t i
Unfortunately, this Wasserstein gradient flow is difficult to analyze.
Moreover, time-discretizations of this Wasserstein gradient flow do
not perform well in practice. Instead, [YWR24] propose to study the
Wasserstein–Fisher–Raogradientflowofℓ .Inthiscontext,themeasure
n
ρ is approximated by
t
N
(cid:88) wjδ
t θj
t
j=1
where for any j ∈ [N], we use the following dynamics:
θ˙j =
−1 (cid:88)n (θ tj −X i)ϕ(θ tj −X i)
,
t n (cid:80)N wkϕ(θk −X )
i=1 k=1 t t i
w˙j =
(cid:104)1 (cid:88)n ϕ(θ tj −X i) −1(cid:105)
wj.
t n (cid:80)N wkϕ(θk −X ) t
i=1 k=1 t t i6.5 Mean-field neural networks 193
Under some conditions, the convergence guarantees of this system can
be established but they are entirely driven by the Fisher–Rao part
and the proof largely consists in finding conditions under which the
Wasserstein part does not get in the way of convergence.
6.5 Mean-field neural networks
A two-layer6 neural network is a parameterized function
m
1 (cid:88)
f(x;θ) = a σ(⟨w ,x⟩+b ), (6.23)
j j j
m
j=1
where θ := {(a ,w ,b ), j ∈ [m]} represents the parameters (or weights)
j j j
of the network, and σ(·) is a non-linearity, e.g., the common ReLU
activation σ(·) = (·) = max(0,·).
+
The first layer is the map ℓ : Rd → Rm defined by
1
(cid:0) (cid:1)T
ℓ (x) = σ(⟨w ,x⟩+b ),...,σ(⟨w ,x⟩+b ) =: σ(Wx+b). (6.24)
1 1 1 m m
It produces an internal representation of the vector x that is more
suitable for the subsequent task; e.g. classification or regression. This
representation is then passed on to the second and terminal layer
ℓ : Rm → R which collapses the representation z := ℓ (x) into a scalar
2 1
prediction using a linear projection:
m
1 (cid:88) 1
ℓ (z) = a z = ⟨a,z⟩.
2 j j
m m
j=1
This terminal layer is tailored to a regression task but it is also common
to employ terminal layers that are tailored to classification. For binary
classification for example, it is desirable to have the output of the
neural network lie in the interval [0,1] so further process the output
y = ℓ ◦ℓ (x) using
2 1
ey
logistic(y) = .
1+ey
The reader will recognize here the logistic function employed in gener-
alized linear models. In the rest of this section we focus on two-layer
6 In other words, the network has one hidden layer.194 6 Wasserstein gradient flows: applications
neural networks of the form ℓ ◦ℓ and leave the study of logistic◦ℓ ◦ℓ
2 1 2 1
as an exercise for the reader.
The parametrization (6.23) is called the mean-field parametrization,
and it lends itself to passing to the limit m → ∞.
Consider a simple regression task in which we have n data points
{(X ,Y ), i ∈ [n]} with X ∈ Rd and Y ∈ R. To train the neural
i i i i
network, we can minimize the squared error
n
(cid:88)(cid:0) (cid:1)2
L(θ) := Y −f(X ;θ) . (6.25)
i i
i=1
The training dynamics for minimizing the objective (6.25) are com-
plex because the parametrization θ (cid:55)→ f(·;θ) is non-linear and conse-
quently the loss L is non-convex. One approach to study these dynamics
istolifttheoptimizationproblemtoonesetoverthespaceofprobability
measures, with the hope that the lifted problem affords simplifications.
In doing so, we must preserve the connection with the originaldynamics,
and this leads naturally to Wasserstein gradient flows.
The lifting is carried out as follows. Let Ω = R×Rd×R denote the
space of (a,w,b) triples, and let µ be a measure over Ω. Set
(cid:90)
f(x;µ) = ρ(x;ω)µ(dω), ρ(x;ω) := aσ(⟨w,x⟩+b),
where ω = (a,w,b). One can check that if we encode parameters θ =
{(a ,w ,b )}m via the empirical measure µ := 1 (cid:80)m δ , then
i i i i=1 θ m i=1 (ai,wi,bi)
f(·;µ ) = f(·;θ), so this definition indeed generalizes (6.23). However,
θ
we can now formulate the problem of optimizing, over the space P (Ω)
2
of probability measures over Ω, the objective
n
(cid:88)(cid:0) (cid:1)2
L(µ) := Y −f(X ;µ) . (6.26)
i i
i=1
As discussed above, we require that the dynamics of minimizing
µ (cid:55)→ L(µ) over P (Ω) be compatible with the original dynamics of
2
minimizing θ (cid:55)→ L(θ) over Ωm. Herein lies the utility of the Wasserstein
geometry, as it was set up precisely to ensure that dynamics over the
base space Ω lift gracefully to dynamics over P (Ω). More precisely:
2
Proposition 6.16.The Wasserstein gradient flow (µ ) of (6.26),
t t≥0
when initialized at a measure of the form µ = µ , is such that µ = µ
0 θ t θt
for all t ≥ 0, where (θ ) is the (time-rescaled) Euclidean gradient
t t≥0
flow of (6.25) initialized at θ.6.6 Transformers 195
We can also rewrite the objective (6.26) as follows:
(cid:88)n (cid:16) (cid:90)
L(µ) = Y2−2Y ρ(X ;ω)µ(dω)
i i i
i=1
(cid:90)(cid:90) (cid:17)
+ ρ(X ;ω)ρ(X ;ω′)µ(dω)µ(dω′)
i i
(cid:90) n
(cid:88)
= const.−2 Y ρ(X ;ω)µ(dω)
i i
i=1
(cid:90)(cid:90) n
(cid:88)
+ ρ(X ;ω)ρ(X ;ω′)µ(dω)µ(dω′).
i i
i=1
We can recognize the second term as a potential energy (in the sense of
Example 5.11) and the third term as an interaction energy (in the sense
of Example 5.13), albeit a generalized version in which the interaction
is not of the form (x,y) (cid:55)→ K(x−y). As expected, the loss L is not in
general geodesically convex.
Since the Wasserstein perspective is essentially a reformulation of
the original neural network problem, a skeptic may ask what advan-
tages it brings. The answer is that we can now consider more general
initializations than empirical measures (measures of the form µ ), and
θ
in particular, a well-known result of Chizat and Bach [CB18] uses this
approachtoestablishglobalconvergenceinthemean-fieldregime,under
certain assumptions. Their result requires the initialization to be abso-
lutely continuous, and since such a measure can only be approximated
by empirical measures in the limit m → ∞, this corresponds in some
sense to “infinitely wide” neural networks. The error incurred for finite
m can be controlled and leads to insights for finite-width networks in
various settings [MMN18, ABAM22, ABAM23].
It has also been proposed to add an entropic regularization term to
the loss (6.26) and to train the network via the mean-field Langevin
dynamics from Subsection 6.3.1 [CRW23, SNW23, TR24].
6.6 Transformers
Since their introduction in 2017 in the paper “Attention is all you
need” [VSP+17], transformers have profoundly transformed practical
deep neural networks, most notably in natural language processing196 6 Wasserstein gradient flows: applications
(NLP), but also in computer vision and robotics. Central to this new ar-
chitecture is the so-called attention mechanism, a layer that is markedly
different from a perceptron (a.k.a. feed-forward) layer such as the one
in (6.23).
Unliketheneuralnetworksthatwehaveseenintheprevioussections
that are functions f : Rd → R, an attention layer is a sequence-to-
sequence map
g : (Rd)N → (Rd)N ,
(x1,...,xN) (cid:55)→ (cid:0) g1(x1,...,xN),...,gN(x1,...,xN)(cid:1) .
More specifically, a input (a sentence in NLP or an image in computer
vision) is broken into tokens x1,...,xN ∈ Rd and processed through an
attention layer g = (g1,...,gN) where for each i ∈ [N],
(cid:80)N xje⟨Qxi,Kxj⟩
gi(x1,...,xN) = xi+V j=1 ,
(cid:80)N e⟨Qxi,Kxj⟩
j=1
where K, Q, and V are three d×d matrices called key, query, and value
respectively.
While practical transformers combine perceptron layers with atten-
tion layers—and also normalization layers that are briefly discussed
below—we focus here on composing multiple attention layers with the
same matrices (K,Q,V). This composition results in a iterative scheme
where tokens are updated as:
xi = xi +V
(cid:80)N
j=1
xj te⟨Qxi t,Kxj t⟩
, i ∈ [N].
t+1 t (cid:80)N
j=1
e⟨Qxi t,Kxj t⟩
In turn, taking the same perspective as in neural ODEs [CRBD18], we
can view the above iterations as a time discretization of the following
dynamical system of interacting particles:
x˙i = V
(cid:80)N
j=1
xj te⟨Qxi t,Kxj t⟩
, i ∈ [N]. (6.27)
t (cid:80)N
j=1
e⟨Qxi t,Kxj t⟩
The above equation describes a system of N ordinary differential equa-
tions (ODEs), one for each token/particle, that are called self-attention
dynamics by [GLPR23, GLPR24]. The way these tokens interact is not
completely wild: each token evolves according to its own position and6.6 Transformers 197
the empirical distribution of all the tokens. Indeed, let µ denote this
t
empirical distribution at time t:
N
1 (cid:88)
µ = δ .
t
N
xi
t
i=1
We can rewrite (6.27) as
x˙i = V
(cid:82) ye⟨Qxi t,Ky⟩µ t(dy)
, i ∈ [N].
t (cid:82) e⟨Qxi t,Ky⟩µ t(dy)
It becomes now clear that the tokens all have the same mean-field
dynamics so we can drop the index i:
(cid:82) ye⟨Qxt,Ky⟩µ t(dy)
x˙ = V , (6.28)
t (cid:82)
e⟨Qxt,Ky⟩µ t(dy)
where µ = law(x ). Using the continuity equation, we get that µ
t t t
evolves according to the following PDE:
(cid:16) (cid:82) ye⟨Q·,Ky⟩µ (dy)(cid:17)
t
∂ µ +div µ V = 0.
t t t (cid:82)
e⟨Q·,Ky⟩µ (dy)
t
This perspective on transformers was first put forward in [SABP22]
which raised the question of whether this curve could be viewed as a
Wasserstein gradient flow. To investigate this question, assume that
K = Q = V = I so that (6.28) becomes:
x˙ =
(cid:82) ye⟨xt,y⟩µ t(dy)
=
∇(cid:104) log(cid:90)
e⟨·,y⟩µ
(dy)(cid:105)
(x ).
t (cid:82) t t
e⟨xt,y⟩µ t(dy)
The form of the velocity field is suggestive and readily begs the question
of whether there exists a functional F such that its first variation is
given by
(cid:90)
δF(µ) = log e⟨·,y⟩µ (dy).
t
Unfortunately,[SABP22]alsoshowthatthisisnotthecaseduetoalack
of symmetry. To overcome this limitation, one may consider instead the
unnormalized self-attention dynamics introduced in [GLPR24]. These
dynamics are of the form
(cid:90) (cid:104)(cid:90) (cid:105)
x˙ = ye⟨xt,y⟩µ (dy) = ∇ e⟨·,y⟩µ (dy) (x ).
t t t t198 6 Wasserstein gradient flows: applications
We readily get that these dynamics describe the Wasserstein gradient
flow of the interaction energy
(cid:90)(cid:90)
F(µ) := − e⟨x,y⟩µ(dx)µ(dy).
Unfortunately, it is easy to see that this functional does not admit a
global minimum over the space of probability measure, indeed, for any
Dirac delta µ = δ , we have F(δ ) = −e∥x∥2 → −∞ as x → ∞. In
x x
particular this suggests that tokens undergoing these dynamics will
simply diverge to infinity.
In practice however, tokens are restricted to live on the unit sphere
Sd−1 of Rd using a procedure know as layer normalization (or simply
“layernorm”).Withlayernorm,theunnormalizedself-attentiondynamics
then become
(cid:90) (cid:90)
x˙ = P ye⟨xt,y⟩µ (dy) = ∇ e⟨xt,y⟩µ (dy),
t xt t xt t
where for any x ∈ Sd−1, y ∈ Rd, we write P y := y −⟨x,y⟩x for the
x
projection of y onto the tangent space of the sphere Sd−1 at x and
∇ := P ∇ denotes the spherical (Riemannian) gradient at x ∈ Sd−1.
x x
Using the version Otto calculus on Riemannian manifolds alluded to in
Section 5.6, we get that these dynamics correspond to a Wasserstein
gradient flow of the interaction energy F now defined on the sphere. In
fact, since for x,y ∈ Sd−1, it holds that ∥x−y∥2 = 2(1−⟨x,y⟩), we
can write F as
(cid:90)(cid:90)
F(µ) := −e e−∥x− 2y∥2 µ(dx)µ(dy). (6.29)
Sd−1×Sd−1
It can be readily seen that the maximizers of this functional are
precisely Dirac deltas δ for any x ∈ Sd−1. Hence unnormalized self-
x
attention is a Wasserstein gradient flow of a functional minimized
at Dirac deltas, which correspond to states where all the tokens are
clustered. Unfortunately, this functional is not geodesically convex and
admits many stationary points where the Wasserstein gradient vanishes.
Using this framework [CRMB24, GLPR24] show that these points are
in fact saddle points, guaranteeing asymptotic convergence to a single
cluster when dynamics are initialized in a generic position.6.7 Discussion 199
6.7 Discussion
§6.1. Another natural geometric approach to VI is natural gradient
descent, which is motivated by the parametrization invariance of the
Fisher–Rao geometry [Ama98, AN00]. However, it has been challenging
to analyze this approach since the VI problem is often non-convex.
See [AR15] for an early work on algorithmic guarantees for VI.
Our discussion of Gaussian VI follows [LCB+22]. Algorithms for
Gaussian VI which are closely related to (6.3) have been proposed
and studied in works such as [AR20, Dom20, GFPO21]; here, our
emphasis is on the the derivation via Otto calculus. There have
been many subsequent works on Gaussian VI, both on the compu-
tational (e.g., [DBCS23, DGG23, KOW+23, BLB24]) and statistical
([KR24]) aspects, as well as applications to bandits [CHD24] and con-
trol [LBB23, LBB24].
The potential application of Wasserstein geometry to mean-field
VI was noticed by several authors [GLNZ22, Lac23, YY23]. The
works [GLNZ22, Lac23] also wrote down interacting SDE implemen-
tations of the gradient flow described in Subsection 6.3.1. The wider
literature on mean-field VI, which usually focuses on coordinate ascent
variational inference (CAVI), is vast and we do not survey it here, but
see [AL24, LZ24] for analyses leveraging Otto calculus.
SVGD was introduced in [LW16], and geometric interpretations of
SVGD are given in [Liu17, CLGL+20a, DNS23]. Convergence theory
remains underdeveloped, see, e.g., [LLN19a, KSA+20, SSR22, DN23,
SM23, PBS24].
Corollary 6.4 can be generalized to measures over Riemannian mani-
folds,inwhichcasethestrongconvexityparameteroftheKLdivergence
captures information about the Ricci curvature. The seminal work of
Lott and Villani [LV09] and Sturm [Stu06a, Stu06b] leverages this to
define a synthetic notion of Ricci curvature lower bounds for measured
geodesic spaces (see Chapter 7) that recover classical Ricci curvature
lower bounds when specialized to the Riemannian setting. These ideas
were later extended to discrete settings, e.g., [Oll10, OV12, Oll13].
Finally note that while the KL divergence plays a preponderant
role in variation inference, other distances between measures can be
considered for this task; see, e.g., [AKSG19] who use Maximum Mean
Discrepancy.
§6.2. As mentioned in the main text, the interpretation of the Langevin
diffusion as a Wasserstein gradient flow goes back to the seminal work200 6 Wasserstein gradient flows: applications
of[JKO98].Ottocalculuswasfirstappliedtoobtainquantitativeresults
for the Langevin diffusion, as in Exercise 6 below, in [OV00]; in this
context, Exercise 4 from Chapter 5 can be sharpened by a factor of
2 [BGL01, OV01]. See also [CE02] for proofs in this spirit which do
not require as much differential structure. For textbook treatments on
stochasticcalculus,see[Ste01]or[Le 16].TheWassersteinPLinequality
for the KL divergence functional is known as the log-Sobolev inequality
and it plays a key role in the study of high-dimensional probability
and Markov processes. Crucially, although we have presented strong
log-concavity as a sufficient condition for the validity of the log-Sobolev
inequality, it is not necessary. See [BGL14] for further detail and [OT11,
OT13, BB18] for generalizations.
The optimization perspective on sampling dates back to early works
suchas[DT12];ourdiscussionlargelyfollows[Wib18].Foranexposition
to the modern complexity theory of log-concave sampling and further
references, see [Che23, Che24]. Recent works also apply this perspective
for parameter estimation [ACG+23, CKPJ24].
Theproximalsamplerhasbeenappliedtostructuredlog-concavesam-
pling[LST21],tonon-Euclidean[GLL+23]andheavy-tailed[HMHBE24]
sampling, and to sampling from convex bodies [KVZ24].
As noted in [CLGL+20b], the Newton–Langevin diffusion converges
to any strictly log-concave target with a universal exponential rate as a
consequence of the Brascamp–Lieb inequality [BL76]. There is a sense
in which it is an optimal preconditioning of Langevin [CTZ24].
Convergence of the underdamped Langevin diffusion requires heavier
machinery than Wasserstein gradient flows and is based on the theory
of hypocoercivity, for which the standard reference is [Vil09a].
§6.3. Analysis of birth-death sampling was first carried out in [LLN19b]
and improved in [LSW23].
§6.4. The WFR gradient flow for NPMLE can be adapted to more
general mixtures. Usually, asymptotic convergence of the gradient flow
to the NPMLE is only established conditionally on convergence to a
limit point. In fact, it is shown in [YWR24] that the NPMLE is the
only stationary point of the gradient flow initialized at a measure that
is absolutely continuous.
§6.5. The study of training dynamics of two-layer neural networks from
the mean-field perspective was proposed in four independent papers
that were released within about a month period in 2018: [MMN18] on
April 18, [SS20] and [RVE22] both on May 2, and [CB18] on May 24. It6.8 Exercises 201
is worth noting that the normalization 1/m in (6.23) is critical for the
mean-field interpretation of the problem. Other works have proposed
√
to use the normalization 1/ m which results in the so called neural
tangent kernel (NTK) (a.k.a.lazy training)regime.Inthisregime,which
will remind the reader of the normalization employed in the central limit
theorem,itcanbeshownthattheparametersdonotmovefarawayfrom
a random initialization and the neural network can be studied using
linear approximation around initialization [JGH18]. For more details
on NTK and its relationship with the mean-field regime see [MM23].
§6.6. The functional F defined in (6.29) has appeared in the literature
on optimal configuration. In this line of work, the maximizers of this
functional are of interest. It is know that F is maximized by the uniform
distribution on the sphere [Tan17]. Finding maximizers subject to a
cardinality constraint on the support of µ is directly connected to
questions arising in sphere packing; see [CK07].
6.8 Exercises
1. Let K : Rd → R be a symmetric function on Rd, and consider
the corresponding interaction energy as defined in Example 5.13:
F(µ) := 1 (cid:82)(cid:82) K(x−y)µ(dx)µ(dy).
2
• Show that if K is convex, then F is geodesically convex on
P (Rd).Hint:letX = (1−t)X +tT(X ),sothat(µ ) :=
2,ac t 0 0 t t∈[0,1]
(law(X )) is a Wasserstein geodesic, then apply (5.6).
t t∈[0,1]
• Show that F is never α-geodesically convex for any α > 0.
Hint: Consider the geodesic (N(tv,I)) for a nonzero vector
t∈[0,1]
v ∈ Rd.
2. Show that the tangent space to the space of product measures at µ
is given by the space of separable vector fields:
T P (R)⊗d
µ 2,ac
= {x (cid:55)→ (ψ′(x ),...,ψ′(x )) | ψ ,...,ψ : R →
R}L2(µ)
,
1 1 d d 1 d
where ψ ,...,ψ are smooth and compactly supported. Then, show
1 d
that the Wasserstein gradient projected to this subspace takes the
form (6.4).
3. Compute the gradient of the KL divergence restricted to the space
of product measures. From the first-order optimality condition,
write down a fixed-point equation for the density of the solution q
⋆
to (MFVI).202 6 Wasserstein gradient flows: applications
4. Prove Lemma 6.9. Hint: Use the separability of the transport maps
to reduce to one-dimensional optimal transport, for which we can
apply the results of Section 1.3 and Proposition 1.18.
5. Compute the derivative of t (cid:55)→ KL(µ ∥ π) when (µ ) evolves
t t t≥0
according to (SVGD).
6. InterpretExercises4and5fromChapter5fortheLangevindiffusion.
7. Consider the Euler–Maruyama scheme (6.11) where the initial dis-
tribution is N(m,Σ) and the target distribution is π = N(0,I).
Compute the law of X for each k ≥ 0. Use this to compute the
k
stationary distribution πˆ of (6.11), and compute the KL divergence
KL(πˆ∥π). How small should we choose the step size h if we want to
ensure KL(πˆ ∥π) ≤ ε2?
8. Let µ = N(m,Σ) be a Gaussian measure with Σ ≻ 0. Evaluate
prox (µ),whereprox istheproximalmapfortheentropydefined
hH hH
in (6.14). This computation is used as the basis for the Gaussian VI
algorithm of [DBCS23].
9. For F := 1 W2(·,ν), compute the iterations µ = prox (µ ) of
2 2 n+1 hF n
the JKO scheme, where
(cid:110) 1 (cid:111)
prox (µ) := argmin hF(µ′)+ W2(µ,µ′) .
hF 2 2
µ′∈P 2(Rd)
Letting h ↘ 0 while nh → t, show that one recovers the gradient
flow from Exercise 11 from Chapter 5.
10. Consider the proximal sampler with initial distribution N(m,Σ) and
target distribution π = N(0,I). Compute the law of the k-th iterate
X for each k ≥ 0 and estimate the rate of convergence to π.
k
11. Let V(x) = 1 ⟨x,Ax⟩ and W(x) = λ ∥x∥2, where A ≻ 0 and λ ≥ 0.
2 2
Compute the stationary distributions π and πˆ of the McKean–
N
Vlasov SDE (6.17) and the finite-particle system (6.18) respectively.
12. Prove Proposition 6.16. Also, generalize to the case of a two-layer
neural network composed with a logistic function when the training
datasatisfiesY ∈ {0,1}foreachi ∈ [n]andweusethecross-entropy
i
loss: L(µ) =
−(cid:80)n
{(1−Y )log(1−f(X ;µ))+Y logf(X ;µ)}.
i=1 i i i i7
Metric geometry of the Wasserstein space
In the previous two chapters, we studied the space W = (P (Rd),W )
2 2 2
through the lens of Riemannian geometry. Although such an approach
yields considerable geometric insight and can even be treated rigorously
(see [AGS08]), it is important to keep in mind that W is not a bona
2
fide Riemannian manifold, and consequently technical issues abound.
Despite what its name suggests, metric geometry requires a bit more
structure than simply a metric space. Indeed, in the rest of this chapter,
we will talk about length/geodesic spaces which have a continuous
flavor. In particular it is possible to take derivatives of functions along
smooth curves. This primitive differential structure is often sufficient to
understand questions about curvature which we will employ to establish
rates of convergence for Wasserstein barycenters in the next chapter.
The goal of this chapter is to gather basic material from metric
geometry for a general metric space (S,d) following the classical book
[BBI01]; see also [AKP22] for a more advanced coverage. Main concepts
(curvature, tangent cone, logarithmic map, etc.) are instantiated to the
(2-)Wasserstein space.
7.1 Geodesics
We already appealed to an intuitive notion of geodesics in Chapter 5.
In this chapter we properly define these objects as length minimizing.
7.1.1 Length and geodesic spaces
Let (S,d) be a metric space. A path in S is a continuous map ω : I → S
where I ⊂ R is an interval. The length L(ω) ∈ R ∪ {∞} of a path204 7 Metric geometry of the Wasserstein space
ω : I → S is defined by
n−1
(cid:88)
L(ω) := sup d(ω(t ),ω(t )), (7.1)
i i+1
i=1
wherethesupremumistakenoveralln ≥ 1andalln-tuplest < ··· < t
1 n
in I.
A path is called rectifiable if it has finite length.
For any path ω and any interval J ⊂ R, we write ω to denote the
J
restriction of ω to I ∩J. The following lemma holds.
Lemma 7.1.For any rectifiable path ω : I → S, the function t (cid:55)→
ℓ(t) = L(ω ) is continuous on I.
(−∞,t]
Proof. We prove left continuity, i.e., that for any ε > 0, there exists
δ > 0 such if t−δ < t′ ≤ t, we have
ℓ(t)−ε ≤ ℓ(t′) ≤ ℓ(t).
Bycontinuityofω,thereexistsδ > 0suchthatt′ ∈ (t−δ ,t]implies
1 1
ε
d(ω(t′),ω(t)) ≤ . (7.2)
2
Next, let n and t < ··· < t = t be such that
1 n
n−1
ε (cid:88)
ℓ(t)− ≤ d(ω(t ),ω(t )) ≤ ℓ(t), (7.3)
i i+1
2
i=1
define δ = min |t −t | > 0, and let δ = min(δ ,δ ) > 0.
2 i=1,...,n−1 i+1 i 1 2
Observe that for any t′ such that t ≥ t′ > t−δ it holds t < t′ ≤ t so
n−1
that
n−2
(cid:88)
ℓ(t′) ≥ d(ω(t ),ω(t ))+d(ω(t ),ω(t′))
i i+1 n−1
i=1
n−1
(cid:88)
≥ d(ω(t ),ω(t ))−d(ω(t′),ω(t))
i i+1
i=1
≥ ℓ(t)−ε
where we used the triangle inequality in the second line and (7.2)–(7.3)
in the third. This completes the proof of left continuity. Right continuity
follows using the same argument. ⊔⊓7.1 Geodesics 205
Two paths ω : I → S and ω : I → S are equivalent if there exists
1 1 2 2
a continuous, non-decreasing, and surjective function φ : I → I such
1 2
that ω = ω ◦φ. In this case, ω is a reparametrization of ω (and
1 2 2 1
vice-versa) and it is easy to check that L(ω ) = L(ω ).
1 2
Finally a path ω : [a,b] → S is said to have constant speed if for all
a ≤ s ≤ t ≤ b,
t−s
L(ω ) = L(ω). (7.4)
[s,t] b−a
Proposition 7.2.Any rectifiable path ω : [a,b] → S has a constant-
speed reparametrization ω¯ : [0,1] → S.
Proof. Let us first reparametrize ω so that it is never locally constant
meaning that there exists no interval [c,d] ⊂ [a,b] such that ω is
[c,d]
constant. If such an interval exists, define π : R → R to be such that

t if t ≤ c,


π(t) = c if c < t ≤ d,

t−(d−c) if t > d.
Observe that π([a,b]) = [a,b − (d − c)] is an interval and that π is
continuous and non-decreasing on this interval. Then reparametrize ω
into ω′ : [a,b−(d−c)] → S such that ω = ω′◦π holds, which is possible
since ω is constant on [c,d].
By repeating this operation, we may assume that ω is never locally
constant and, in particular, that the map t (cid:55)→ φ(t) := L(ω )/L(ω) is
[a,t]
strictly increasing on [a,b] and continuous by Lemma 7.1 and therefore
invertible. In particular, φ−1 is also continuous, strictly increasing, and
defined over [0,1]. We define ω¯ : [0,1] → S by ω¯ = ω◦φ−1 which is a
constant-speed reparametrization of ω. ⊔⊓
Given x,y ∈ S, a path ω : [a,b] → S is said to connect (or join) x to
y if ω(a) = x and ω(b) = y. By construction of the length function L,
d(x,y) ≤ L(ω) for any path ω connecting x to y. The space S is called
a length space if for all x,y ∈ S,
d(x,y) = infL(ω), (7.5)
ω
where the infimum is taken over all paths ω connecting x to y. A length
space is said to be a geodesic space if for all x,y ∈ S, the infimum on
the right hand side of (7.5) is attained.206 7 Metric geometry of the Wasserstein space
Definition 7.3.Let (S,d) be a length space. A geodesic between x and
y is any path ω : [0,1] → S attaining the infimum in (7.5).
In other words, a geodesic is a shortest path between two points. It
follows from the minimizing property of a geodesic ω that
d(ω(s),ω(t)) = L(ω ),
[s,t]
for all 0 ≤ s ≤ t ≤ 1. Together with (7.4) it yields the following useful
characterization of constant-speed geodesics.
Proposition 7.4.A path ω : [0,1] → S is a constant-speed geodesic if
and only if
d(ω(s),ω(t)) = (t−s)d(ω(0),ω(1)),
for all 0 ≤ s ≤ t ≤ 1.
7.1.2 Midpoints
We now obtain a characterization of geodesic spaces in terms of mid-
points. For any two points x,y in a metric space, a midpoint of (x,y) is
any z ∈ S such that
1
d(x,z) = d(y,z) = d(x,y).
2
Proposition 7.5.Let (S,d) be a complete metric space. Then the fol-
lowing are equivalent:
(i)(S,d) is a geodesic space.
(ii)Any two points x,y ∈ M admit a midpoint.
Proof. We begin with the easy direction: (i) ⇒ (ii). Let ω be a geodesic
that connects x to y, then clearly ω(1/2) is a midpoint.
To prove (ii) ⇒ (i), we construct a path ω : [0,1] → S such that
ω(0) = x, ω(1) = y and L(ω) = d(x,y). To that end, we first define ω
on the set D of dyadic rationals of [0,1] defined by
D = {k/2m : m ≥ 1, k ≥ 0}∩[0,1].
We proceed in a recursive fashion. Let z be a midpoint of (x,y) =
(ω(0),ω(1)) and define ω(1/2) = z. Given H := {ω( k ), k ∈ [2m]},
m 2m
define H = {ω( k ), k ∈ [2m+1]} by setting ω( k ) = ω(k/2 ) ∈
m+1 2m+1 2m+1 2m7.1 Geodesics 207
H ifkisevenandlettingω( k )bethemidpointof(ω ,ω )
m 2m+1 (k−1)/2 (k+1)/2
when k is odd. The union of H , m ≥ 0 defines ω on
D.2m 2m
m
From our construction, for t,t′ ∈ D, it holds
d(ω(t),ω(t′)) = |t−t′|d(x,y) (7.6)
sothatω isd(x,y)-LipschitzonD.Wenowshowthatω canbeextended
to a continuous function on [0,1] that connects x to y. To that end,
fix t ∈ [0,1] and let (t ) ⊆ D be a sequence of dyadic integers that
n n≥0
converges to t. Observe that (ω(t )) forms a Cauchy sequence in
n n≥0
(S,d) since by (7.6) it holds
d(ω(t ),ω(t )) ≤ |t −t |d(x,y) → 0, n,m → ∞.
n m n m
Therefore since S is complete, (ω(t )) converges and we set ω(t)
n n≥0
to be its limit. To see that such an ω is continuous, note that for
any t,u ∈ [0,1], there exists sequences (t ) ,(u ) ⊆ D such that
n n≥0 n n≥0
t → t, u → u and
n n
d(ω(t),ω(u)) = lim d(ω(t ),ω(u )) ≤ lim |t −u |d(x,y)
n n n n
n→∞ n→∞
= |t−u|d(x,y)
where we used (7.6) in the equality. Therefore, we have constructed a
path that connects x to y.
To conclude the proof, it suffices to observe that (7.1) and (7.6)
imply that L(ω) = d(x,y) as desired. ⊔⊓
7.1.3 Geodesics in Wasserstein space
We are now in a position to place the Wasserstein space W within the
2
framework of metric geometry. Compare the following theorem with
Theorem 5.7.
Theorem 7.6.TheWassersteinspaceW isageodesicspace.Moreover,
2
let π (x,y) := (1−t)x+ty, t ∈ [0,1], and for any µ,ν ∈ W let γ ∈ Γ
t 2 µ,ν
be an optimal transport plan in the sense that
(cid:90)
∥x−y∥2γ(dx,dy) = W2(µ,ν).
2
Then the path ω given by ω(t) = (π ) γ is a constant-speed geodesic in
t #
W connecting ω(0) = µ to ω(1) = ν.
2208 7 Metric geometry of the Wasserstein space
Proof. For any 0 ≤ s ≤ t ≤ 1, define the coupling γ := (π ,π ) γ ∈
s,t s t #
Γ . Then
ω(s),ω(t)
(cid:90)
W2(ω(s),ω(t)) ≤ ∥x−y∥2γ (dx,dy)
2 s,t
(cid:90)
= ∥π (x,y)−π (x,y)∥2γ(dx,dy)
s t
(cid:90)
= ∥(1−s)x+sy−((1−t)x+ty)∥2γ(dx,dy)
(cid:90)
= (t−s)2 ∥x−y∥2γ(dx,dy)
= (t−s)2W2(ω(0),ω(1)).
2
We have proved that
W (ω(s),ω(t)) ≤ |t−s|W (ω(0),ω(1)).
2 2
To show that this inequality is in fact an equality, note that together
with the triangle inequality, it yields
W (ω(0),ω(1)) ≤ W (ω(0),ω(s))+W (ω(s),ω(t))+W (ω(t),ω(1))
2 2 2 2
≤ (s+|t−s|+|1−t|)W (ω(0),ω(1))
2
= W (ω(0),ω(1)).
2
Therefore, the above inequalities are equalities and in particular,
W (ω(0),ω(s))+W (ω(s),ω(t))+W (ω(t),ω(1))
2 2 2
= sW (ω(0),ω(1))+|t−s|W (ω(0),ω(1))+|1−t|W (ω(0),ω(1)).
2 2 2
Since each term on the left-hand side is smaller than its corresponding
part in the right-hand side, we have that
W (ω(s),ω(t)) = |t−s|W (ω(0),ω(1)),
2 2
and the conclusion follows from Proposition 7.4. This explicit construc-
tion of geodesics joining any pair µ,ν ∈ W readily implies that W is
2 2
indeed a geodesic space. ⊔⊓
For any constant-speed geodesic ω connecting two measures µ,ν ∈
W and any t ∈ [0,1], the measure ω(t) is often called displacement
2
interpolation after [McC97]. Crucially, if µ and ν have densities f and
µ7.2 Curvature 209
f , this interpolation differs from the usual interpolation given by the
ν
mixture with density (1−t)f +tf . This is a manifestation of the
µ ν
geometry of W .
2
Note that the proof of Theorem 7.6 above implies the following
interesting corollary.
Corollary 7.7.Let ω be any constant-speed geodesic in W and let γ be
2
an optimal coupling between ω(0) and ω(1). Then for any 0 ≤ s ≤ t ≤ 1,
thecouplingγ := (π ,π ) γ ∈ Γ ,whereπ (x,y) := (1−t)x+ty,
s,t s t # ω(s),ω(t) t
t ∈ [0,1], is optimal in the sense that
(cid:90)
∥x−y∥2γ (dx,dy) = W2(ω(s),ω(t)).
s,t 2
Finally, in the case where the geodesic emanates from a distribution
that admits a density, we get from Brenier’s Theorem 1.16 the following
useful corollary, which justifies Definition 5.8.
Corollary 7.8.Let µ,ν ∈ W be two probability measures such that µ
2
has a density and let T : Rd → Rd be the (unique) Brenier map such
that T µ = ν. Then, the constant-speed geodesic ω : [0,1] → W such
# 2
that ω(0) = µ and ω(1) = ν is unique and given by
(cid:0) (cid:1)
ω(t) = (1−t)id+tT µ, ∀t ∈ [0,1].
#
where id : Rd → Rd denotes the identity map.
In other words, if X ∼ µ, then (1−t)X +tT(X) ∼ ω(t).
7.2 Curvature
7.2.1 Alexandrov curvature
Given a real number κ ∈ R, a geodesic space of special interest is the
(complete and simply connected) 2-dimensional Riemannian manifold
with constant sectional curvature κ. For given κ ∈ R, this metric space
(M ,d ) is unique up to an isometry, and called a model space. For
κ κ
each κ ∈ R, we use the following representative of the equivalence class
generated by the group of isometries.
• If κ < 0, (M ,d ) is the hyperbolic plane of constant curvature
κ κ
κ < 0.210 7 Metric geometry of the Wasserstein space
• If κ = 0, (M ,d ) is the Euclidean plane R2 equipped with its
0 0
Euclidean metric.
• If κ > 0, (M ,d ) is the 2-dimensional Euclidean sphere of radius
√ κ κ
1/ κ equipped with the angular metric.
These model spaces play a central role in metric geometry. As described
below, curvature bounds in general metric spaces are formulated by
comparison arguments involving these model spaces as benchmarks.
The fundamental device allowing for this comparison is that of
comparison triangles. Given a metric space (S,d), we define a triangle
asanysetofthreedistinctpoints{p,x,y} ⊂ S.Forκ ∈ R,acomparison
triangle for {p,x,y} in M is an isometric embedding of {p,x,y} in M ,
κ κ
i.e., a set {p¯,x¯,y¯} ⊂ M such that
κ
d (p¯,x¯) = d(p,x), d (p¯,y¯) = d(p,y), and d (x¯,y¯) = d(x,y).
κ κ κ
When κ ≤ 0, such a comparison triangle always exists (and is unique
up to an isometry). When κ > 0, such a triangle exists (and is unique
up to an isometry) provided it fits on the sphere of radius κ−1/2. This
condition may be specified in terms of its perimeter:
2π
peri{p,x,y} := d(p,x)+d(p,y)+d(x,y) < √ . (7.7)
κ
For κ > 0 say that a triangle {p,x,y} that satisfies (7.7) is admissible.
When κ ≤ 0, all triangles are admissible.
We are now in a position to define curvature bounds for general
geodesic spaces.
Definition 7.9.Let κ ∈ R and (S,d) be a geodesic space.
•We say that curv(S) ≥ κ if for any admissible triangle {p,x,y} ⊂ S
and any comparison triangle {p¯,x¯,y¯} ⊂ M , the following holds.
κ
For any constant-speed geodesics ω : [0,1] → S and ω¯ : [0,1] → M
κ
joining x to y and x¯ to y¯ respectively, it holds
(cid:0) (cid:1) (cid:0) (cid:1)
d p,ω(t) ≥ d p¯,ω¯(t) , ∀t ∈ [0,1]. (7.8)
κ
•We say that curv(S) ≤ κ if for any admissible triangle {p,x,y} ⊂ S
and any comparison triangle {p¯,x¯,y¯} ⊂ M , the following holds.
κ
For any constant-speed geodesics ω : [0,1] → S and ω¯ : [0,1] → M
κ
joining x to y and x¯ to y¯ respectively, it holds
(cid:0) (cid:1) (cid:0) (cid:1)
d p,ω(t) ≤ d p¯,ω¯(t) , ∀t ∈ [0,1]. (7.9)
κ7.2 Curvature 211
The previous definition admits a natural geometric interpretation: if
curv(S) ≥ κ (resp. curv(S) ≤ κ), a triangle {p,x,y} looks thicker (resp.
thinner)thanacorrespondingcomparisontriangle{p¯,x¯,y¯}inthemodel
space M .
κ
The case κ = 0 is of special interest since the model space of
reference is flat. In that case, one compares our geometry to a familiar
Euclideanone.WesaythatS isaspaceofnon-positivecurvature(NPC)
when curv(S) ≤ 0, and a space of non-negative curvature (NNC) when
curv(S) ≥ 0.
In the flat case the following lemma holds.
Lemma 7.10.Let H be a Hilbert space equipped with inner product ⟨·,·⟩
and norm ∥·∥. Then, for any p,x,y ∈ H, the constant-speed geodesic
joining x to y is unique and given by ω(t) = (1−t)x+ty and for any
p ∈ H,
∥p−ω(t)∥2 = (1−t)∥p−x∥2+t∥p−y∥2−t(1−t)∥x−y∥2, ∀t ∈ [0,1].
In particular, this holds for the model space M = R2.
0
Proof. Itcanbeeasilycheckedthatωisindeedaconstant-speedgeodesic
joining x to y. Fix t ∈ [0,1]. To check the equality, observe that on the
one hand
∥p−ω(t)∥2 = ∥p−(1−t)x−ty∥2
= ∥(1−t)(p−x)+t(p−y)∥2
= (1−t)2∥p−x∥2+t2∥p−y∥2+2t(1−t)⟨p−x,p−y⟩.
On the other hand,
∥x−y∥2 = ∥x−p+p−y∥2 = ∥p−x∥2+∥p−y∥2−2⟨p−x,p−y⟩.
Putting the above two displays together yields
∥p−ω(t)∥2 = (1−t)2∥p−x∥2+t2∥p−y∥2
+t(1−t)(cid:2) ∥p−x∥2+∥p−y∥2−∥x−y∥2(cid:3)
= (1−t)∥p−x∥2+t∥p−y∥2−t(1−t)∥x−y∥2.
It remains to show that ω is unique. To that end, let ω′ by any constant-
speed geodesic joining x to y and fix t ∈ [0,1]. Apply the above identity
to p = ω′(t) to get212 7 Metric geometry of the Wasserstein space
∥ω′(t)−ω(t)∥2 = (1−t)∥ω′(t)−x∥2+t∥ω′(t)−y∥2−t(1−t)∥x−y∥2.
Since ω′ is a constant-speed geodesic joining x to y, we have by Propo-
sition 7.4 that ∥ω′(t)−x∥ = t∥x−y∥ and ∥ω′(t)−y∥ = (1−t)∥x−y∥.
Therefore
∥ω′(t)−ω(t)∥2 = (cid:0) (1−t)t2+t(1−t)2−t(1−t)(cid:1) ∥x−y∥2 = 0,
so that ω′ = ω. ⊔⊓
Lemma 7.10 involves only squared distances and can be directly
stated in geodesic spaces. It turns out that this generalization gives a
useful characterization of NNC or NPC spaces. Note that this charac-
terization does not extend to the cases where the reference space is not
flat (i.e., curvature bounded by a non-zero quantity).
Proposition 7.11.Let (S,d) be a geodesic space. Then curv(S) ≥ 0 if
and only if for triangle {p,x,y} ∈ S and any constant-speed geodesic ω
joining x to y, we have
d2(p,ω(t)) ≥ (1−t)d2(p,x)+td2(p,y)−t(1−t)d2(x,y) ∀t ∈ [0,1].
(7.10)
We have curv(S) ≤ 0 if and only if the same statement holds with the
opposite inequality.
Proof. Consider a triangle {p,x,y} together with a comparison triangle
{p¯,x¯,y¯} ∈ R2.
Assume that curv(S) ≥ 0 in the sense of Definition 7.9. Then for
any constant-speed geodesic ω that connects x to y and ω¯ the unique
constant-speed geodesic that connects x¯ to y¯, we have by Definition 7.9
and Lemma 7.10 respectively that
d2(p,ω(t)) ≥ ∥p¯−ω¯(t)∥2
= (1−t)∥p¯−x¯∥2+t∥p¯−y¯∥2−t(1−t)∥x¯−y¯∥2
= (1−t)d2(p,x)+td2(p,y)−t(1−t)d2(x,y).
To prove the converse, note that (7.10) yields
d2(p,ω(t)) ≥ (1−t)d2(p,x)+td2(p,y)−t(1−t)d2(x,y)
= (1−t)∥p¯−x¯∥2+t∥p¯−y¯∥2−t(1−t)∥x¯−y¯∥2
= ∥p¯−ω¯(t)∥2,
by Lemma 7.10 so that Definition 7.9 holds. ⊔⊓7.2 Curvature 213
Remark 7.12.Comparing with the definition of α-convexity in Ap-
pendixA,weseethat 1 ∥p−·∥2 is1-stronglyconvexinanyHilbertspace.
2
Similarly, (S,d) is an NPC space if and only if 1 d2(p,·) is 1-strongly
2
convex along the geodesics of (S,d). The notion of geodesic convexity
was also used in Section 5.2, but here we work in the more general
setting of geodesic spaces.
Ageodesicspace(S,d)withanycurvatureboundiscalledanAlexan-
drov space. If curv(S) ≤ κ for some κ ∈ R, then (S,d) is sometimes
called a CAT(κ) space in reference to E. Cartan, A. D. Alexandrov,
and V. A. Toponogov. As noted before, a CAT(0) space is also referred
to as an NPC (non-positively curved) or sometimes Hadamard space.
If curv(S) ≥ 0 we call the space non-negatively curved or NNC. It is
worth noting that the previous definitions are of global nature as they
require comparison inequalities to be valid for all triangles (that admit
a comparison triangle in the relevant model space). Some definitions
of curvature require the previous comparison inequalities to hold only
locally. The local validity of these comparison inequalities is known,
under suitable conditions depending on the value of κ, to imply their
global validity (globalization theorems).
We conclude this subsection by giving a third equivalent definition
of positive curvature.
Proposition 7.13.Let (S,d) be a geodesic space. Then curv(S) ≥ 0 if
andonlyifforanytriangle{p,x,y} ⊂ S,comparisontriangle{p¯,x¯,y¯} ⊂
M , and any constant-speed geodesics ω,ω′,ω¯,ω¯′ joining p to x, p to y,
0
p¯ to x¯, and p¯ to y¯ respectively, we have
d2(ω(s),ω′(t)) ≥ ∥ω¯(s)−ω¯′(t)∥2, ∀s,t ∈ [0,1]. (7.11)
We have curv(S) ≤ 0 if and only if the same statement holds with the
opposite inequality.
Proof. Assume first that curv(S) ≥ 0 and observe that by Proposi-
tion 7.11 and Definition 7.9 respectively, it holds
d2(ω(s),ω′(t)) ≥ (1−s)d2(p,ω′(t))+sd2(x,ω′(t))−s(1−s)d2(p,x)
≥ (1−s)∥p¯−ω¯′(t)∥2+s∥x¯−ω¯′(t)∥2−s(1−s)∥p¯−x¯∥2.
The right-hand side of the above inequality is precisely ∥ω¯(s)−ω¯′(t)∥2
by Lemma 7.10. We have proved (7.11).214 7 Metric geometry of the Wasserstein space
Conversely, let ω ,ω′ be constant-speed geodesics joining x to y and
x x
x to p, respectively, and let ω¯ ,ω¯′ be constant-speed geodesics joining
x¯ x¯
x¯ to y¯ and x¯ to p¯respectively. Then, taking s = 1 in (7.11), we get for
any t ∈ [0,1],
d2(p,ω (t)) = d2(ω′(1),ω (t))
x x x
≥ ∥p¯−ω¯ (t)∥2
x¯
= (1−t)∥p¯−x¯∥2+t∥p¯−y¯∥2−t(1−t)∥x¯−y¯∥2
= (1−t)d2(p,x)+td2(p,y)−t(1−t)d2(x,y),
which is the characterization of curv(S) ≥ 0 from Proposition 7.11.
The proof for curv(S) ≤ 0 follows using the same argument. ⊔⊓
7.2.2 Curvature of the Wasserstein space
Note that if d = 1, the space P (R) equipped with the Wasserstein
2
distance is actually flat.
Proposition 7.14.The space W (R) is flat in the sense that
2
curv(W (R)) ≤ 0 and curv(W (R)) ≥ 0
2 2
and it can be isometrically embedded into a Hilbert space.
Proof. Recall from Proposition 1.18 that for any µ,ν ∈ W , it holds
2,ac
(cid:90) 1
W2(µ,ν) = |F†(u)−F†(u)|2du = ∥F†−F†∥2,
2 µ ν µ ν
0
where ∥·∥ := ∥·∥ . In particular, the map µ (cid:55)→ F† is an isometry
L2(R) µ
from W to L2(R).
2,ac
Let now ω be a constant-speed geodesic that connects µ to ν and
recall from Proposition 1.18 and Theorem 7.6 that ω is uniquely char-
acterized by the fact that if V = (1 − t)F†(U) + tF†(U), where
µ ν
U ∼ Unif([0,1]), then W ∼ ω(t). It yields that for any v ∈ R,
P(V ≤ v) = P(cid:0) (1−t)F†(U)+tF†(U) ≤ v(cid:1) = (cid:0) (1−t)F†+tF†(cid:1)† (v).
µ ν µ ν
Hence,
F† = (1−t)F†+tF†.
ω(t) µ ν7.2 Curvature 215
Next, let ρ ∈ W and t ∈ [0,1]. Since L2(R) is a Hilbert space, we
2
get from Lemma 7.10 that
W2(ρ,ω(t)) = ∥F†−F† ∥2 = ∥F†−(1−t)F†−tF†∥2
2 ρ ω(t) ρ µ ν
= (1−t)∥F†−F†∥2+t∥F†−F†∥2−t(1−t)∥F†−F†∥2
ρ µ ρ ν µ ν
= (1−t)W2(ρ,µ)+tW2(ρ,ν)−t(1−t)W2(µ,ν).
2 2 2
This completes the proof that curv(W (R)) = 0. In turn, one can
2,ac
show that this implies curv(W (R)) = 0 as well. ⊔⊓
2
More generally, for any d ≥ 1, W (Rd) is positively curved as indi-
2
cated by the theorem below.
Theorem 7.15.The 2-Wasserstein space W is positively curved,
2
curv(W ) ≥ 0,
2
i.e., for any µ,ν,ρ ∈ W and any constant-speed geodesic ω that connects
2
µ to ν, it holds
W2(ρ,ω(t)) ≥ (1−t)W2(ρ,µ)+tW2(ρ,ν)−t(1−t)W2(µ,ν).
2 2 2 2
Proof. Let γ ∈ Γ be an optimal coupling and recall from Theorem 7.6
µ,ν
that for any t ∈ [0,1], ω(t) = (π ) γ where π (x,y) = (1−t)x+ty. In
t # t
particular, if (X,Y) ∼ γ, then V := (1−t)X +tY ∼ ω(t). Next, let
t
γ ∈ Γ be an optimal coupling between ω(t) and ρ. In particular, it
t ω(t),ρ
induces a conditional distribution on Z ∼ ρ given V . We have described
t
a joint distribution Υ for (X,Y,Z) that has marginals µ, ν, and ρ
respectively (the reader will have recognized a variant of the gluing
lemma, Lemma B.5).
With this notation, we have
(cid:90)
W2(ρ,ω(t)) = ∥z−v∥2γ (dx,dv)
2 t
(cid:90)
= ∥z−(1−t)x−ty∥2Υ(dx,dy,dz).
Next, observe that by Lemma 7.10 applied to H = Rd, we have
∥z−(1−t)x−ty∥2 = (1−t)∥z−x∥2+t∥z−y∥2−t(1−t)∥x−y∥2,
so that216 7 Metric geometry of the Wasserstein space
(cid:90)
W2(ρ,ω(t)) = (cid:2) (1−t)∥z−x∥2+t∥z−y∥2
2
−t(1−t)∥x−y∥2(cid:3)
Υ(dx,dy,dz)
≥ (1−t)W2(ρ,µ)+tW2(ρ,ν)
2 2
(cid:90)
−t(1−t) ∥x−y∥2Υ(dx,dy,dz)
= (1−t)W2(ρ,µ)+tW2(ρ,ν)−t(1−t)W2(µ,ν),
2 2 2
where in the inequality, we used the suboptimality of the first two
couplingsandinthelastequality,weusedtheoptimalityofthecoupling
between µ and ν induced by Υ. ⊔⊓
7.3 Tangent cones
A geodesic space has a priori no differentiable structure but a surrogate
for it may be built. It starts from the notion of angle which can be
defined on any metric space by analogy to the Hilbert case, akin to
our definition of curvature bounds. When applied to a geodesic space,
angles allow us to define the notion of direction, which can be thought
of as the initial velocity of a constant-speed geodesic. The collection of
such directions forms the tangent cone.
7.3.1 Angles
We first define angles on a metric space and show that they provide
alternative characterizations of curvature bounds for geodesic spaces.
Recall that for any three points p,x,y ∈ R2, the cosine of the angle
∡ (x,y), formed by vectors − p→ x and − p→ y is given by
p
⟨x−p,y−p⟩
cos∡ (x,y) = .
p
∥x−p∥∥y−p∥
Note that
−2⟨x−p,y−p⟩ = ∥(x−p)−(y−p)∥2−∥x−p∥2−∥y−p∥2
= ∥x−y∥2−∥x−p∥2−∥y−p∥2.
Therefore, we can rewrite this definition only in terms of squared
distances to obtain7.3 Tangent cones 217
∥x−p∥2+∥y−p∥2−∥x−y∥2
cos∡ (x,y) = .
p
2∥x−p∥∥y−p∥
This definition generalizes to any metric space.
Definition 7.16.Let (S,d) be a metric space and for any triangle
{p,x,y} in S, define the angle ∡ (x,y) ∈ [0,π] at p by
p
d2(p,x)+d2(p,y)−d2(x,y)
cos∡ (x,y) := .
p
2d(p,x)d(p,y)
Similar comparisons may be made with model spaces M for κ ̸= 0
κ
but are beyond the scope of these lectures.
Thenextresultpresentsacharacterizationofpositivelycurvedspaces
in terms of the angle monotonicity.
Proposition 7.17(Angle monotonicity). Let (S,d) be a geodesic
space. Then, curv(S) ≥ 0 in the sense of Definition 7.9, if and only if
for any triangle {p,x,y} in S and any geodesics ω and ω′ joining p to
x and p to y respectively, the function
(s,t) ∈ [0,1]2 (cid:55)→ ∡ (ω(s),ω′(t))
p
is non-increasing in each variable when the other is fixed.
Proof. Assume first that curv(S) ≥ 0 and consider a triangle {p,x,y}
in S and constant-speed geodesics ω and ω′ joining p to x and p to y
respectively. It is enough to prove that, for all (s,t) ∈ [0,1]2,
cos∡ (ω(s),ω′(t)) ≤ cos∡ (x,y).
p p
Let {p¯,x¯,y¯} be a comparison triangle for {p,x,y} in M = R2 and let
0
ω¯ and ω¯′ be constant-speed geodesics in M connecting p¯to x¯ and y¯
0
respectively. It holds
d2(p,ω(s))+d2(p,ω′(t))−d2(ω(s),ω′(t))
cos∡ (ω(s),ω′(t)) =
p 2d(p,ω(s))d(p,ω′(t))
s2d2(p,x)+t2d2(p,y)−d2(ω(s),ω′(t))
=
2std(p,x)d(p,y)
s2∥p¯−x¯∥2+t2∥p¯−y¯∥2−d2(ω(s),ω′(t))
=
2st∥p¯−x¯∥∥p¯−y¯∥
s2∥p¯−x¯∥2+t2∥p¯−y¯∥2−∥ω¯(s)−ω¯′(t)∥2
≤
2st∥p¯−x¯∥∥p¯−y¯∥218 7 Metric geometry of the Wasserstein space
∥p¯−ω¯(s)∥2+∥p¯−ω¯′(t)∥2−∥ω¯(s)−ω¯′(t)∥2
=
2∥p¯−ω¯(s)∥∥p¯−ω¯′(t)∥
= cos∡ (ω¯(s),ω¯′(t))
p¯
= cos∡ (x¯,y¯)
p¯
= cos∡ (x,y),
p
where in the inequality we used the fact that curv(S) ≥ 0 and Propo-
sition 7.13. This completes the proof that the curvature lower bound
implies angle monotonicity.
Conversely, assume that for any triangle {p,x,y} in S, any constant-
speed geodesics ω and ω′ connecting p to x and p to y respectively, and
all (s,t) ∈ [0,1]2, we have
cos∡ (ω(s),ω′(t)) ≤ cos∡ (x,y).
p p
Then, the first part of the proof implies that
d2(ω(s),ω′(t)) ≥ ∥ω¯(s)−ω¯′(t)∥2, ∀(s,t) ∈ [0,1]2
with the same notation as above, which is the characterization of
curv(S) ≥ 0 from Proposition 7.13. ⊔⊓
7.3.2 Directions
From the notion of angles between points, we can readily define an
angle between constant-speed geodesics.
Let (S,d) be a geodesic space such that curv(S) ≥ 0, p ∈ S, and ω,
ω′ two constant-speed geodesics connecting p to x and y respectively.
We define the angle between ω and ω′ as
∡(ω,ω′) := lim ∡ (ω(s),ω′(t)).
p
s,t↘0
It follows from Proposition 7.17 that this limit exists under the assump-
tion curv(S) ≥ 0. In fact, under the same assumption,
∡(ω,ω′) = lim∡ (ω(t),ω′(t)).
p
t↘0
Given a third constant-speed geodesic ω′′ : [0,1] → S such that
ω′′(0) = p and ω′′(1) = z, it can be shown (see Exercise 4) that we have
the triangular inequality7.3 Tangent cones 219
∡(ω,ω′) ≤ ∡(ω,ω′′)+∡(ω′′,ω′), (7.12)
so that ∡ is a pseudo-metric on the set G(p) of all constant-speed
geodesics emanating from p. Next, we define the equivalence relation ∼
on G(p) by
ω ∼ ω′ ⇔ ∡(ω,ω′) = 0.
We can turn ∡ into a proper metric (still denoted ∡) on the quotient
G(p)/∼.
Definition 7.18.The space of directions emanating from p is the com-
pletion (Σ ,∡) of (G(p)/∼,∡). An element of Σ is called a direction.
p p
7.3.3 Tangent cone
An analog of a tangent space for geodesic spaces is provided by the
notion of a tangent cone.
Definition 7.19(Tangent cone). Let (S,d) be a geodesic space with
positive curvature and fix p ∈ S. The tangent cone T S at p is the
p
Euclidean cone over the space of directions (Σ ,∡). In other words, T S
p p
is the metric space:
•whose underlying set consists in equivalence classes in Σ ×[0,+∞)
p
for the equivalence relation ∼ defined by
(cid:26)
s = t = 0
(ω,s) ∼ (ω′,t) ⇔
or ω = ω′ and s = t
•and whose metric d is defined
p
(cid:112)
d ((ω,s),(ω′,t)) := s2+t2−2stcos∡(ω,ω′).
p
For u = (ω,s) and v = (ω′,t) ∈ T S, we write ∥u−v∥ := d (u,v),
p p p
∥u∥ := d (o ,u), where o = (ω,0) ∈ T S is the tip of the cone and
p p p p p
⟨u,v⟩ := ∥u∥ ∥v∥ cos∡(ω,ω′) = 1 (cid:0) ∥u∥2+∥v∥2−∥u−v∥2(cid:1) .
p p p 2 p p p
The terminology cone and the notation ∥·∥ and ⟨·,·⟩ introduced
p p
above is justified by the fact that the cone T S possesses a Hilbert-like
p
structure described below. As often the case in metric geometry, the
definition of d comes from rewriting a Euclidean notion using only
p
notions that exist on a geodesic space, namely distances and angles in220 7 Metric geometry of the Wasserstein space
this case. Indeed, if ω,ω′ are points on the sphere and (ω,s) := s·ω,
(ω′,t) := t·ω′ then it follows from the law of cosines that their squared
distance is given by
∥s·ω−t·ω′∥2 = s2+t2−2stcos∡(ω,ω′). (7.13)
For a point u = (ω,t) and λ ≥ 0, we define λ·u := (ω,λt). Moreover,
it may be checked using the previous definitions that, for any u,v ∈ T S
p
and any λ ≥ 0, we get
∥λ·u∥ = λ∥u∥ and ⟨λ·u,v⟩ = ⟨u,λ·v⟩ = λ⟨u,v⟩ .
p p p p p
Note that the tangent cone may not be geodesic but in cases when
it is, the sum of points u,v ∈ T S is defined as the midpoint of 2·u
p
and 2·v as defined in Definition 7.5. In this case, T S is indeed a cone.
p
An example to keep in mind is when S is a filled-in square in the
plane R2. Then, the tangent cone at one of the corners of S is “missing”
some directions (the ones that would lead out of S) and is therefore not
a vector space, hence why we do not call it the tangent “space”.
The logarithmic map plays an important role in the sequel. For
all x ∈ S, we denote ⇑x ⊂ Σ the set of all equivalence classes of
p p
constant-speed geodesics connecting p to x in S. Then for every x ∈ S,
we arbitrarily choose one direction ↑x ∈ ⇑x.
p p
Definition 7.20(Logarithmic map). Let (S,d) be a positively curved
geodesic space. Then, having chosen ↑x ∈ ⇑x for every x ∈ S, the
p p
associated logarithmic map is defined by
log : x ∈ S (cid:55)→ (↑x,d(p,x)) ∈ T S.
p p p
At this level of generality, the definition of log (x) depends on the
p
choice of directions {↑x, x ∈ S}. This ambiguity may be removed by
p
restricting the log map to an appropriate subset, namely the set of
p
pointsx ∈ S forwhichthereisonlyoneequivalenceclassofdirectionsof
constant-speed geodesics connecting p to x. This set might be specified
even more accurately by observing that, in an NNC space S, if constant-
speed geodesics ω and ω′ from p to x satisfy ∡(ω,ω′) = 0, then ω = ω′.
In other words, the set of points x ∈ S for which there is more than
one equivalence class of constant-speed geodesics connecting p to x
is exactly the set of points x connected to p by at least two distinct
constant-speed geodesics. This set of points is denoted C(p) and called7.3 Tangent cones 221
the cut-locus of p. Then, for any x ∈ S\C(p), log (x) is defined without
p
ambiguity as
log (x) = (ω ,d(p,x)),
p x
where ω denotes the unique constant-speed geodesic from p to x there-
x
fore identified to its direction. With this notation, we get in particular,
for all t ∈ [0,1] and all x ∈ S \C(p), that
log ω (t) = tlog x.
p x p
More generally, if ω : [0,1] → S is a constant-speed geodesic in S and p
is such that p = ω(t) for some t ∈ [0,1], i.e., p is on the geodesic, then
log ω(s) = (1−s)log ω(0)+slog ω(1). (7.14)
p p p
In other words, the logarithmic maps turns geodesics into straight lines.
The following result shows that the log map is expanding in a space
p
of positive curvature.
Proposition 7.21.Let (S,d) be a geodesic space and p ∈ S be fixed. If
curv(S) ≥ 0, then the logarithmic map is expansive in the sense that
then for all x,y ∈ S,
d(x,y) ≤ ∥log (x)−log (y)∥ ,
p p p
with equality if x = p or y = p.
Proof. Let ω,ω′ be two constant-speed geodesics connecting p to x and
y respectively. Then if p ̸= x and p ≠ y, we have by definition of ∥·∥
p
and angle monotonicity that for all s,t ∈ [0,1], it holds
∥log (x)−log (y)∥2 = d2(log (x),log (y))
p p p p p p
= d2(p,x)+d2(p,y)−2d(p,x)d(p,y)cos∡(ω,ω′)
≥ d2(p,x)+d2(p,y)−2d(p,x)d(p,y)cos∡ (ω(s),ω′(t)).
p
Applying Definition 7.16, we get
∥log (x)−log (y)∥2
p p p
s2d2(p,x)+t2d2(p,y)−d2(ω(s),ω′(t))
≥ d2(p,x)+d2(p,y)− .
st
Letting s = t = 1 yields
∥log (x)−log (y)∥2 ≥ d2(x,y).
p p p
It is easy to check the equality cases from the definition of d . ⊔⊓
p222 7 Metric geometry of the Wasserstein space
7.3.4 Tangent cone of the Wasserstein space
Going to the very definition of the tangent cone, we can show that
it takes a very simple form in the Wasserstein case. To that end, let
µ,ν,ρ ∈ W be three probability distributions. Moreover, let ω and ω
2 ν ρ
be two geodesics in the 2-Wasserstein space W joining µ to ν and µ to
2
ρ respectively and recall that the tangent cone at µ is the metric space
of directions at µ equipped with distance d such that
µ
d2(cid:0)
(ω ,d(µ,ν)),(ω
,d(µ,ρ))(cid:1)
µ ν ρ
= W2(µ,ν)+W2(µ,ρ)−2W (µ,ν)W (µ,ρ)cos∡(ω ,ω ).
2 2 2 2 ν ρ
What is the angle ∡(ω ,ω ) between these two Wasserstein geodesics?
ν ρ
We can carry out a calculation assuming that µ has a density so that
Brenier’s theorem ensures the existence of two optimal transport maps
T and T so that
µ→ν µ→ρ
W2(µ,ω (t))+W2(µ,ω (t))−W2(ω (t),ω (t))
cos∡(ω ,ω ) = lim 2 ν 2 ρ 2 ν ρ
ν ρ
t↘0 2W 2(µ,ω ν(t))W 2(µ,ω ρ(t))
t2W2(µ,ν)+t2W2(µ,ρ)−W2(ω (t),ω (t))
= lim 2 2 2 ν ρ
t↘0 2t2W 2(µ,ν)W 2(µ,ρ)
W2(µ,ν)+W2(µ,ρ)−lim W 22(ων(t),ωρ(t))
= 2 2 t↘0 t2 .
2W (µ,ρ)W (µ,ν)
2 2
Lemma 7.22.Let µ ∈ P (Rd) and denote by T and T the
2,ac µ→ν µ→ρ
Brenier maps from µ to ν and µ to ρ respectively. Then
W2(ω (t),ω (t))
lim 2 ν ρ = ∥T −T ∥2 .
t↘0
t2 µ→ν µ→ρ L2(µ)
Proof. It is easy to show one of the required inequalities. Indeed, let
X ∼ µ and observe that
Xν := (1−t)X +tT (X) ∼ ω (t),
t µ→ν ν
Xρ := (1−t)X +tT (X) ∼ ω (t).
t µ→ρ ρ
Therefore,
W2(ω (t),ω (t)) ≤ E∥Xν −Xρ∥2
2 ν ρ t t
= t2E∥T (X)−T (X)∥2
µ→ν µ→ρ7.3 Tangent cones 223
= t2∥T −T ∥2 . (7.15)
µ→ν µ→ρ L2(µ)
To prove the converse, for any t ∈ [0,1], let Υ be the following
t
coupling between five random variables: (X,Y,Z,Y ,Z ) ∼ Υ if
t t t
1. X ∼ µ and Y = T (X) ∼ ν are optimally coupled,
µ→ν
2. Y = (1−t)X +tY ∼ ω (t),
t ν
3. Z ∼ ω (t) and Y ∼ ω (t) are optimally coupled,
t ρ t ν
4. Z ∼ ω (t) and Z ∼ ρ are are optimally coupled,
t ρ
5. Z ∼ ω (t) and X′ ∼ µ are are optimally coupled.
t ρ
Figure 7.1 indicates that this joint coupling can be realized using the
gluing lemma since the resulting graph is acyclic. Note in particular
that Z = (1−t)X′+tZ.
t
Y ∼ν Z ∼ρ
Y ∼ω (t) Z ∼ω (t)
t ν t ρ
X′ ∼µ
X ∼µ
Fig. 7.1. ThecouplingΥ between(µ,ν,ρ,ω (t),ω (t)).Solidlinesindicateoptimal
t ν ρ
couplings. Dashed lines and missing lines indicate potentially suboptimal ones.
We have
W2(ω (t),ω (t)) = E∥Y −Z ∥2 = E∥(1−t)X +tY −Z ∥2
2 ν ρ t t t
= (1−t)E∥X −Z ∥2+tE∥Y −Z ∥2−t(1−t)E∥X −Y∥2,
t t
(7.16)
where we used Lemma 7.10 for H = Rd. Now note that X and Z are
t
potentially coupled in a suboptimal way so that
(1−t)E∥X −Z ∥2 ≥ (1−t)W2(µ,ω (t)) = t2(1−t)W2(µ,ρ).
t 2 ρ 2
Moreover, using Lemma 7.10 again, we get
tE∥Y −Z ∥2 = tE∥Y −(1−t)X′+tZ∥2
t
= t(1−t)E∥Y −X′∥2+t2E∥Y −Z∥2−t2(1−t)E∥X′−Z∥2
≥ t(1−t)E∥Y −X∥2+t2E∥Y −Z∥2−t2(1−t)E∥X′−Z∥2,224 7 Metric geometry of the Wasserstein space
where in the above inequality, we used the fact that X and Y are
optimally coupled.
Plugging the above two displays in (7.16), we see that the terms in
E∥Y −X∥2 can cancel out. We get
W2(ω (t),ω (t))
2 ν ρ
≥ t2(1−t)W2(µ,ρ)+t2E∥Y −Z∥2−t2(1−t)E∥X′−Z∥2
2
= t2E∥Y −Z∥2.
Recall from (7.15) that W (ω (t),ω (t)) = O(t), so that E∥Y −Z ∥2 =
2 ν ρ t t
O(t2). Since
X = (Y −tY)/(1−t) and X′ = (Z −tZ)/(1−t),
t t
it implies E∥X−X′∥2 = O(t2) as well. Assuming (without justification)
that T is Lipschitz1
µ→ρ
E∥Y −Z∥2 = E∥T (X)−T (X′)∥2
µ→ν µ→ρ
= E∥T (X)−T (X)∥2−O(t).
µ→ν µ→ρ
This readily yields
W2(ω (t),ω (t))
lim 2 ν ρ ≥ ∥T −T ∥2 ,
t↘0
t2 µ→ν µ→ρ L2(µ)
which concludes the proof of our Lemma. ⊔⊓
It follows from Lemma 7.22 that
W2(µ,ν)+W2(µ,ρ)−∥T −T ∥2
cos∡(ω ,ω ) = 2 2 µ→ν µ→ρ L2(µ)
ν ρ
2W (µ,ν)W (µ,ρ)
2 2
∥T −id∥2 +∥T −id∥2 −∥T −T ∥2
µ→ν L2(µ) µ→ρ L2(µ) µ→ν µ→ρ L2(µ)
=
2∥T −id∥ ∥T −id∥
µ→ν L2(µ) µ→ρ L2(µ)
= cos∡(T −id,T −id), (7.17)
µ→ν µ→ρ
where the last cos is understood in the Hilbert space L2(µ).
In turn, the law of cosines (7.13) implies that the metric on the
tangent cone at µ is given by
1 This assumption be lifted via approximation arguments, at the cost of additional
technicalities.7.4 Discussion 225
d2(cid:0) (ω ,s),(ω ,t)(cid:1) = s2+t2−2stcos∡(ω ,ω )
µ ν ρ ν ρ
= s2+t2−2stcos∡(T −id,T −id)
µ→ν µ→ρ
= ∥s(T −id)−t(T −id)∥2 .
µ→ν µ→ρ L2(µ)
We have shown that the tangent cone equipped with the metric d is
µ
isometrictoaHilbertspace.Wehaveprovedthefollowingtheoremwhich
we had identified using the formalism of Otto calculus in Section 5.4.
Theorem 7.23.Let µ ∈ P (Rd). Then the tangent cone T W (Rd)
2,ac µ 2
at µ is a convex subset of L2(µ). Moreover, for any ν ∈ P (Rd), we have
2
log (ν) = T −id ∈ L2(µ),
µ µ→ν
where T is the Brenier map from µ to ν.
µ→ν
In cases such as the one above, where the tangent cone T W
µ 2
equipped with ⟨·,·⟩ from Definition 7.18 is, in fact, a convex subset of a
Hilbert space, we call it the tangent space at µ. It follows readily from
the definition of the logarithmic map that the inner product ⟨·,·⟩ is
µ
given for any ν,ρ ∈ P (Rd) by
2
⟨log (ν),log (ρ)⟩ = ⟨T −id,T −id⟩
µ µ µ µ→ν µ→ρ L2(µ)
(cid:90)
(cid:10) (cid:11)
= T (x)−x,T (x)−x µ(dx).
µ→ν µ→ρ
7.4 Discussion
§7.1. Wasserstein geodesics are discussed in detail in [Vil03, Chapter
5] and [AGS08, Chapter 7]. More generally, the Wasserstein space over
any length space is also a length space.
§7.2. The non-negative curvature of the Wasserstein space is proven
in [AGS08, Section 7.3]. More generally, the Wasserstein space over a
non-negatively curved Alexandrov space is also non-negatively curved.
The curve in Exercise 2 is called a generalized geodesic and it plays
an important role in the theory of Wasserstein gradient flows, as well
as occasionally in other applications of optimal transport.
As mentioned in the discussion notes for Section 6.1, there is a
notion of synthetic Ricci curvature lower bounds which makes sense
on geodesic spaces. It is a natural to ask whether this notion recovers
the non-negative Alexandrov curvature of the Wasserstein space when226 7 Metric geometry of the Wasserstein space
equipped with an appropriate measure. Unfortunately, [Cho12] shows
that does not yield any finite lower bound even for even for the (flat)
Wasserstein space on the real line.
§7.3. The tangent cone of the Wasserstein space and its relationship to
the tangent space is discussed in [AGS08, Section 12.4].
7.5 Exercises
1. Show that the space of probability measures endowed with MMD
(Definition 2.20) defines a flat geometry.
2. Let µ,ν,ρ ∈ P (Rd). Prove that there exists a curve ω : [0,1] →
2
P (Rd) with ω(0) = µ, ω(1) = ν such that the opposite inequality
2
to Theorem 7.15 holds, i.e.,
W2(ρ,ω(t)) ≤ (1−t)W2(ρ,µ)+tW2(ρ,ν)−t(1−t)W2(µ,ν).
2 2 2 2
Hint: for X ∼ µ, X ∼ ν, X ∼ ρ, optimally couple (X ,X ) and
µ ν ρ µ ρ
(X ,X ). Define ω(t) to be the law of a suitable interpolation of X
ν ρ µ
and X . Compare with Exercise 9 from Chapter 5.
ν
3. Generalize the proof of Theorem 7.6 to show that for any p ≥ 1,
the space P (Rd) of probability measures with finite p-th moment,
p
equipped with the p-Wasserstein metric W , is a geodesic space.
p
What are the geodesics?
4. Generalizing Definition 7.19, use the following outline to show that
if (S,d) is a metric space with diameter at most π, and cone(S) is
the set X ×[0,∞) with all points of the form (x,0) identified, then
(cid:112)
d((x,s),(y,t)) := s2+t2−2stcosd(x,y)
defines a metric on cone(S). To do so, let (x ,r ), (x ,r ), (x ,r )
1 1 2 2 3 3
be three points in the cone and construct three points y ,y ,y ∈ R2
1 2 3
so that their distances from the origin equal r , r , r respec-
1 2 3
tively, and so that the angles between y and y , and between
1 2
y and y , equal d(x ,x ) and d(x ,x ) respectively. Show that
2 3 1 2 2 3
∥y −y ∥ = d((x ,r ),(x ,r )) and ∥y −y ∥ = d((x ,r ),(x ,r )).
1 2 1 1 2 2 2 3 2 2 3 3
(Caution: ∥y −y ∥ does not necessarily equal d((x ,r ),(x ,r )).)
1 3 1 1 3 3
Now establish the triangle inequality for the cone metric, splitting
into two cases according to whether or not d(x ,x )+d(x ,x ) ≤ π.
1 2 2 38
Wasserstein barycenters
Averaging data is among the most fundamental of the statistician’s
tools, but its implementation on non-Euclidean spaces, capturing data
modalities that differ from the typical vector-valued covariates common
in traditional statistical literature, often requires care. Suppose, for
instance, that our dataset consists of images and we wish to define
a suitable notion of an average image which captures representative
aspects of the whole. This model problem arises in situations such as
the aggregation of information from repeated MRI scans.
We can represent a p-pixel image via its values in the R, G, B
channels for each pixel, thereby considering it as a vector taking values
in {0,1,...,255}3p. A na¨ıve approach to the averaging problem would
be to simply compute the usual average of these vector representations
of the image. Attempting this method on a few images, however, should
readily convince the reader that this notion of average is unsatisfactory,
see Figure 8.1 for a demonstration.
Fig. 8.1. Here we depict two images, a circle and a cross, and the ℓ average. Note
2
that the ℓ average only performs averaging at the level of pixel intensities, rather
2
than at the level of the “shapes” of the objects depicted within the image.228 8 Wasserstein barycenters
A closer inspection of the na¨ıve approach reveals the nature of the
problem: when we average the vector representations of the image, we
tacitly endow the space R3p with the Euclidean geometry, and there is
no reason to expect that this geometry should be compatible with our
embedding of images into R3p. Indeed, the representation of an image
as a vector in R3p is an engineering choice, not an intrinsic quality of
the image. A perhaps more principled approach would be to regard the
images as living in an abstract space S endowed with a metric d which
captures closeness with respect to the attributes we regard as important
for the data under consideration. Our task can then be formulated as
follows: given points x ,...,x inside a metric space (S,d), what is a
1 n
suitable notion for the average of x ,...,x ?
1 n
Fortunately there is a general and useful answer to this question,
which is motivated as follows. It is not hard to see that for x ,...,x
1 n
belonging to a Hilbert space H, the average 1 (cid:80)n x is characterized
n i=1 i
as the unique minimizer of the functional
n
1 (cid:88)
x (cid:55)→ ∥x −x∥2.
i
n
i=1
This formulation only involves squared distances and is amenable to
generalization to metric spaces.
Definition 8.1(Barycenter). Given any probability measure P over
a metric space (S,d), we say that b is a barycenter of P if it is a
minimizer of the functional
(cid:90)
b (cid:55)→ d2(b,x)P(dx).
In particular, if we take P to be an empirical measure 1 (cid:80)n δ ,
n i=1 xi
then a barycenter of P is an average of the points x ,...,x . Note that
1 n
at this level of generality, a barycenter may not exist, and even if one
exists, it may not be unique.
The case when (S,d) is the Wasserstein space is already of interest
and provides motivation for the theory we develop in this chapter. For
example, Wasserstein barycenters provide a geometrically meaningful
solution to the image averaging problem with which we opened, as well
as to many other problems such as curve registration; see [RPDB12,
CD14,GPC15,SdGP+15,BPC16,PZ16,SLD18,PC19b,LGLR20]and
the references therein. However, since the framework we develop fits8.1 The Hilbert case 229
naturally within metric geometry, as developed in Chapter 7, we work
in this setting and specialize later.
Here, we develop statistical theory to justify the use of geometric
averaging methods in practice. Namely, assume that we have i.i.d. data
X ,...,X drawn from a distribution P over (S,d), and let b⋆ denote
1 n
the barycenter of P. This population barycenter is our quantity of
interest and is unknown. In order for the statistical problem to be
well-posed, we always work under assumptions which guarantee that b⋆
exists and is unique.
There is a natural plug-in estimator for this problem: the barycenter
b of the empirical measure or the empirical barycenter, defined as the
n
minimizer of the functional
n
1 (cid:88)
b (cid:55)→ d2(b,X ).
i
n
i=1
Our goal is to quantify the error d(b ,b⋆) based on natural geometric
n
features of the space (S,d).
8.1 The Hilbert case
In the case where (S,d) is a Hilbert space, the empirical barycenter
converges at the so-called parametric rate. This is easy to see. To that
end, let H be a Hilbert space and recall that in this case
n (cid:90)
1 (cid:88)
b = X , b⋆ = EX = xP(dx),
n i
n
i=1
where we used a Pettis integral to define b⋆. We have
n
1 (cid:88)
E∥b −b⋆∥2 = E⟨X −EX,X −EX⟩
n n2 i j
i,j=1
1
= var(X), where var(X) = E∥X −EX∥2,
n
where we used the independence of the X ’s and bilinearity of the inner
i
product. Unfortunately, this proof, while concise and leading to an
equality (!) is not very instructive since it makes crucial use of the
closed form for the barycenter, as well as the inner product structure,
which do not extend beyond Hilbert spaces.230 8 Wasserstein barycenters
Remarkably, the same parametric rate of estimation for the barycen-
ter continue to hold in NPC spaces, see Exercise 2. Unfortunately, as we
sawinTheorem7.15,ourmainspaceofinterest,namelytheWasserstein
space, is an NNC space. Therefore, our goal is to develop statistical
theory for the more difficult setting of curv ≥ 0.
Returning to the Hilbert case for inspiration, we propose a second
proof which still leads to qualitatively the same result but is off by a
factor 4. By definition of b , we have
n
P ∥b − (cid:114) ∥2 ≤ P ∥b⋆− (cid:114) ∥2.
n n n
Here and in the sequel, we use the shorthand operator notation: for any
(cid:114) (cid:82)
integrable function f, Pf( ) = f(x)P(dx) and in particular
n n
(cid:114) 1 (cid:88) 1 (cid:88)
P f( ) = f(X ), where P = δ
n
n
i n
n
Xi
i=1 i=1
denotes the empirical distribution of the X ’s.
i
Next, note that
∥b − (cid:114) ∥2 = ∥b −b⋆∥2+∥b⋆− (cid:114) ∥2+2⟨b −b⋆,b⋆− (cid:114) ⟩.
n n n
Therefore, applying operator P , we get
n
∥b −b⋆∥2+P ∥b⋆− (cid:114) ∥2+2P ⟨b −b⋆,b⋆− (cid:114) ⟩ ≤ P ∥b⋆− (cid:114) ∥2,
n n n n n
so that
∥b −b⋆∥2 ≤ 2P ⟨b −b⋆, (cid:114) −b⋆⟩.
n n n
Now the above inequality simply says that ∥b −b⋆∥2 ≤ 2∥b −b⋆∥2,
n n
which is not very useful but we are going to keep going with it for the
sake of argument.
Note first that by linearity of the inner product
P⟨b −b⋆, (cid:114) −b⋆⟩ = 0.
n
Therefore, we have
∥b −b⋆∥2 ≤ 2(P −P)⟨b −b⋆, (cid:114) −b⋆⟩ = 2⟨b −b⋆,(P −P)((cid:114) −b⋆)⟩.
n n n n n
Dividing on both sides by ∥b −b⋆∥ applying Cauchy–Schwarz, we get
n
4
E∥b −b⋆∥2 ≤ 4E∥(P −P)((cid:114) −b⋆)∥2 = var(X). (8.1)
n n
n8.2 Barycenters on positively curved spaces 231
What did we learn in this proof? First we have only an inequality
and lost a factor 4. Our major gain was that we never used the closed
form for b . Instead, we only used the fact that
n
E∥(P −P)((cid:114) −b⋆)∥2 ≤ var(X)/n,
n
which applies more broadly. On the downside, we used the linearity
of the inner product and more generally the Hilbert structure quite
extensively. It turns out that this is quite necessary to obtain our results.
Therefore, we force the Hilbert structure in through the tangent space
of W and keep track of how much we lose.
2
8.2 Barycenters on positively curved spaces
Let P be a probability measure on a positively curved geodesic space
(S,d).Letb⋆beanybarycenterofP andletb beanempiricalbarycenter
n
built from n independent copies X ,...,X of X ∼ P:
1 n
(cid:90) n
(cid:88)
b⋆ ∈ argmin d2(b,x)P(dx), b ∈ argmin d2(b,X ).
n i
b∈S b∈S
i=1
Before we proceed, we discuss our overall approach. The argument
in the Hilbert case rests on the inequality P d2(b , (cid:114) ) ≤ P d2(b⋆, (cid:114) ),
n n n
which holds true by definition of b and highlights that the empirical
n
barycenter is an instance of the empirical risk minimization (ERM)
framework within the statistical estimation literature. Using ERM
techniques, we could hope to control the estimation error via measures
of the “complexity” of the space (S,d), and this approach has been
pursued in the literature (see [ACLGP20]). However, for our application
of interest in which (S,d) is taken to be the Wasserstein space, the
complexity is prohibitively large and it leads to non-parametric rates of
estimation; in particular, they suffer from the curse of dimensionality,
similarly to what we saw in Chapter 2.
However, we have just seen that the rates of estimation in a Hilbert
space escape the curse, despite the fact that Hilbert spaces can even
be infinite-dimensional. Our intuition therefore leads us to believe that,
even if we are working over a curved space (S,d), as long we can restrict
ourselves to sufficiently “flat” parts of S, then perhaps we could recover
theHilbertianrates.Inthesequel,weseeknaturalgeometricconditions—
morally, they encode curvature bounds—which enable fast, parametric
rates of estimation.232 8 Wasserstein barycenters
8.2.1 Master theorem
We begin with a general result that mimics the proof of Section 8.1 in
the Hilbert case.
Before stating it, we introduce a quantity that measures how much
the tangent space at the barycenter “hugs” the original space at the
barycenter b⋆.
Definition 8.2(Hugging). Let (S,d) be a geodesic space such that
curv(S) ≥ 0. For any b⋆,b ∈ S, let hb be the hugging function of S at
b⋆
b⋆ in direction b defined by
∥log (x)−log (b)∥2 −d2(x,b)
hb (x) = 1− b⋆ b⋆ b⋆ , x ∈ S. (8.2)
b⋆ d2(b,b⋆)
Note that it follows from Proposition 7.21 that hb (x) ≤ 1 for all
b⋆
x ∈ S. Moreover, if S is a Hilbert space, then ∥log (x)−log (b)∥2 =
b⋆ b⋆ b⋆
d2(x,b) and hb ≡ 1. In general, hb (x) may be negative when there
b⋆ b⋆
is a lot of curvature around b⋆ but it remains non-negative in average
when computed at barycenter b⋆. This result follows from the following
simple but important observation.
Theorem 8.3(Variance equality). Let (S,d) be a geodesic space
with curv(S) ≥ 0. Let Q ∈ P (S) be a probability distribution on S with
2
barycenter b⋆. Assume further that the tangent cone of S at b⋆ equipped
with ⟨·,·⟩ is a convex subset of a Hilbert space. Then, for all b ∈ S,
b⋆
(cid:90) (cid:90)
d2(b,b⋆) hb (x)Q(dx) = (d2(x,b)−d2(x,b⋆))Q(dx), (8.3)
b⋆
where hb is the hugging function defined in (8.2).
b⋆
Proof. By definition of hb , we have
b⋆
d2(b,b⋆)hb ((cid:114) ) = d2(b,b⋆)+d2((cid:114) ,b)−∥log b−log (cid:114) ∥2
b⋆ b⋆ b⋆ b⋆
=
d2(b,b⋆)+d2((cid:114)
,b)
−∥log b∥2 −∥log (cid:114) ∥2 +2⟨log (cid:114) ,log b⟩
b⋆ b⋆ b⋆ b⋆ b⋆ b⋆ b⋆
=
d2(b,b⋆)+d2((cid:114)
,b)
−d2(b,b⋆)−d2(b⋆, (cid:114) )+2⟨log (cid:114) ,log b⟩
b⋆ b⋆ b⋆
= d2((cid:114) ,b)−d2((cid:114) ,b⋆)+2⟨log (cid:114) ,log b⟩ .
b⋆ b⋆ b⋆8.2 Barycenters on positively curved spaces 233
Therefore applying the linear operator Q, we get
d2(b,b⋆)Qhb ((cid:114) ) = Qd2((cid:114) ,b)−Qd2((cid:114) ,b⋆)+2⟨log b⋆,log b⟩ ,
b⋆ b⋆ b⋆ b⋆
where we use the fact that Qlog (cid:114) = log b⋆ or, in other words, that
b⋆ b⋆
log b⋆ is the barycenter of (log ) Q. Indeed, we have by Proposi-
b⋆ b⋆ #
tion 7.21 that for all b ∈ S,
Q∥log (cid:114) −log b⋆∥2 = Qd2((cid:114) ,b⋆) ≤ Qd2((cid:114) ,b) ≤ Q∥log (cid:114) −log b∥2
b⋆ b⋆ b⋆ b⋆ b⋆ b⋆
with equality if b = b⋆ so that log b⋆ is a barycenter for (log ) Q and
b⋆ b⋆ #
therefore Qlog (cid:114) = log b⋆.
b⋆ b⋆
Finally since log b⋆ = o is the tip of the tangent cone, we have
b⋆ b⋆
∥log b⋆∥ = 0, which, in turn, yields ⟨log b⋆,log b⟩ = 0. This
b⋆ b⋆ b⋆ b⋆ b⋆
completes the proof. ⊔⊓
A direct consequence of the variance equality is that if (cid:82) hb dQ > 0,
b⋆
then b⋆ is the unique barycenter of Q. Moreover, since the right-hand
side of the variance equality is non-negative by definition of a barycenter
b⋆, we readily get the following corollary.
Corollary 8.4.Under the same assumptions as Theorem 8.3, we have
for any b ∈ S,
(cid:90)
0 ≤ hb (x)Q(dx) ≤ 1.
b⋆
Moreover, for any b,x ∈ S, we have hb (x) ≤ 1.
b⋆
It turns out the hugging function at b⋆ plays a key role in obtaining
parametric rates of convergence for empirical barycenters.
Theorem 8.5(Master theorem). Let P be a probability measure on
a NNC geodesic space (S,d) and denote by b⋆ and b a barycenter of
n
P and an empirical barycenter respectively. Assume further that the
tangent cone of S at b⋆ equipped with ⟨·,·⟩ is a convex subset of a
b⋆
Hilbert space. Then, the following holds: if for any b ∈ S,
hb ((cid:114) ) ≥ h > 0, (8.4)
b⋆ min
then b and b⋆ are both unique and
n
4σ2
Ed2(b ,b⋆) ≤ ,
n h2 n
min
where σ2 denotes the variance of P defined by
(cid:90)
σ2 = d2(b⋆,x)P(dx). (8.5)234 8 Wasserstein barycenters
Proof. Note first that uniqueness follows directly from the variance
equality and (8.4).
Next, we start as in the Hilbert case by observing that
P d2(b , (cid:114) ) ≤ P d2(b⋆, (cid:114) ).
n n n
It yields
P d2(b , (cid:114) )−P ∥log b −log (cid:114) ∥2
n n n b⋆ n b⋆ b⋆
(8.6)
+P ∥log b −log (cid:114) ∥2 −P d2(b⋆, (cid:114) ) ≤ 0.
n b⋆ n b⋆ b⋆ n
We now make use of the fact that the tangent cone has a Hilbert
structure so that
∥log b −log (cid:114) ∥2
b⋆ n b⋆ b⋆
= ∥log b ∥2 +∥log (cid:114) ∥2 −2⟨log b ,log (cid:114) ⟩
b⋆ n b⋆ b⋆ b⋆ b⋆ n b⋆ b⋆
= d2(b⋆,b )+d2(b⋆, (cid:114) )−2⟨log b ,log (cid:114) ⟩
n b⋆ n b⋆ b⋆
where in the second identity, we used twice the equality case in Propo-
sition 7.21. Plugging this into (8.6) yields
d2(b⋆,b ) ≤ P (cid:2) ∥log b −log (cid:114) ∥2 −d2(b , (cid:114) )(cid:3) +2P ⟨log b ,log (cid:114) ⟩ .
n n b⋆ n b⋆ b⋆ n n b⋆ n b⋆ b⋆
Next, by definition of the hugging function, we get
∥log b −log (cid:114) ∥2 −d2(b , (cid:114) ) = (1−hbn((cid:114) ))d2(b ,b⋆).
b⋆ n b⋆ b⋆ n b⋆ n
It yields
h d2(b ,b⋆) ≤ 2P ⟨log b ,log (cid:114) ⟩ .
min n n b⋆ n b⋆ b⋆
The right-hand side is simply an average in a Hilbert space so, dividing
by ∥log b ∥ = d(b ,b⋆) on both sides and applying Cauchy–Schwarz,
b⋆ n b⋆ n
we get
4σ2
h2 Ed2(b ,b⋆) ≤ ,
min n n
where
(cid:90) (cid:90)
σ2 = ∥log x∥2 P(dx) = d2(b⋆,x)P(dx)
b⋆ b⋆
as desired. ⊔⊓
It follows from inspecting the proof of the master theorem that
in order to obtain parametric rates of estimation for b⋆, it suffices to
have the weaker condition P
hbn((cid:114)
) ≥ h > 0. Since P is a random
n b⋆ min n
measure, we prefer not to impose conditions on it and focus instead on
the stronger condition (8.4). We are going to obtain such results using
the notion of extendable geodesics.8.2 Barycenters on positively curved spaces 235
8.2.2 Extendable geodesics
We now present a compelling synthetic geometric condition that implies
this lower bound in the context of NNC spaces: the extendability, by
a given factor, of all geodesics emanating from and arriving at the
barycenter b⋆.
Definition 8.6(Extendable geodesic). Consider a constant-speed
geodesic ω : [0,1] → S. For (λ ,λ ) ∈ [0,∞]2, we say that ω is
in out
(λ ,λ )-extendable if there exists a path ω+ : [−λ ,1+λ ] → S
in out in out
which agrees with ω on [0,1], called an extension of ω, which remains a
geodesic between its endpoints ω+(−λ ) and ω+(1+λ ).
in out
Before we state the main result of this subsection, we need the
following fact.
Theorem 8.7.Suppose that curv(S) ≥ 0. Let Q ∈ P (S) be a prob-
2
ability measure on S with a barycenter b⋆. Suppose that, for each
x ∈ supp(Q), there exists a constant-speed geodesic ω : [0,1] → S
x
connecting b⋆ to x which is (0,λ)-extendable for λ > 0. Suppose in
addition that b⋆ remains a barycenter of the distribution Q = (e ) Q
λ λ #
where e (x) = ω+(1+λ). Then for all b ∈ S,
λ x
λ
Qhb ((cid:114) ) ≥ . (8.7)
b⋆
1+λ
In particular, it implies that b⋆ is the unique barycenter of Q.
Proof. Fix y ∈ supp(Q) and define y = e (y). Let ω : [0,1] → S be a
λ λ
constant-speed geodesic connecting b⋆ to y . By definition, ω(τ) = y for
λ
τ = 1/(1+λ). Since curv(S) ≥ 0, we have for any b ∈ S,
d2(b,y) ≥ (1−τ)d2(b,b⋆)+τ d2(b,y )−τ (1−τ)d2(b⋆,y )
λ λ
λ 1 λ
= d2(b,b⋆)+ d2(b,y )− d2(b⋆,y ).
1+λ 1+λ λ (1+λ)2 λ
Next, observe that
d2(b⋆,y ) = (1+λ)2d2(b⋆,y)
λ
so that
λ 1
d2(b,b⋆) ≤ d2(b,y)+λd2(b⋆,y)− d2(b,y )
λ
1+λ 1+λ236 8 Wasserstein barycenters
= (cid:0) d2(b,y)−d2(b⋆,y)(cid:1) +(1+λ)d2(b⋆,y)− 1 d2(b,y ).
λ
1+λ
(8.8)
Moreover,
1
(1+λ)d2(b⋆,y)− d2(b,y )
λ
1+λ
= 1 (cid:0) (1+λ)2d2(b⋆,y)−d2(b,y )(cid:1)
λ
1+λ
= 1 (cid:0) d2(b⋆,y )−d2(b,y )(cid:1) .
λ λ
1+λ
Thus, writing Q := (e ) Q, we get
λ λ #
(cid:90) (cid:16) 1 (cid:17)
(1+λ)d2(b⋆,y)− d2(b,y ) Q(dy)
λ
1+λ
(cid:90)
=
1 (cid:0) d2(b⋆,y)−d2(b,y)(cid:1)
Q (dy) ≤ 0,
λ
1+λ
whereinthelastinequality,weusedthefactthatb⋆ remainsabarycenter
of Q . Together with (8.8) integrated with respect to Q, we get
λ
(cid:90)
λ d2(b,b⋆) ≤ (cid:0) d2(b,y)−d2(b⋆,y)(cid:1) Q(dy). (8.9)
1+λ
Combined with the variance equality (Theorem 8.3), this completes the
proof. ⊔⊓
The above notion of extendable geodesics gives a lower bound on
Phb ((cid:114) ) uniformly in b. While this is already an attractive feature that
b⋆
implies uniqueness of the barycenter, it suffers from two deficiencies.
First, we need to control P
hbn((cid:114)
) and b is data-dependent, and it is
n b⋆ n
unclearhowtocontrolthedeviation|P hbn−Phbn|inasuitablefashion.
n b⋆ b⋆
Second, the condition that P = (e ) P keeps the same barycenter is
λ λ #
difficult to check and appears to be restrictive.
To overcome both limitations, we allow for geodesics emanating from
b⋆ to be extendable in both directions.
Theorem 8.8.Suppose that curv(S) ≥ 0 and let x,b,b⋆ ∈ S. Suppose
that there exist λ ,λ > 0 and a geodesic connecting b⋆ to x which is
in out
(λ ,λ )-extendable. Then
in out
λ 1
hb (x) ≥ h = out − .
b⋆ min
1+λ λ
out in8.2 Barycenters on positively curved spaces 237
Proof. Let ω : [0,1] → S be a (λ ,λ )-extendable geodesic connect-
x in out
ing b⋆ to x and denote by ω+ : [−λ ,1+λ ] → S its extension. Let
x in out
z = ω+(−ξ) where ξ = λ /(1 + λ ) and consider the measure Q
x in out
defined by
ξ 1
Q := δ + δ .
x z
1+ξ 1+ξ
SinceQissupportedonω+ wecaneasilycomputeitsbarycenter.Indeed,
note that x = ω+(1) so the barycenter of Q is given by
x
ω+(cid:0) 1· ξ −ξ· 1 (cid:1) = ω+(0) = b⋆.
x 1+ξ 1+ξ
Now, we wish to apply Theorem 8.7 to Q. To that end, note that
the constant-speed geodesics ω connecting b⋆ to x and σ connecting b⋆
x
to z and defined by σ(t) = ω+(−tξ) are both (0,1+λ )-extendable
x out
by assumption and by construction respectively.
Finally, we check that b⋆ remains a barycenter of the probability
measure Q = (e ) Q where e (x) = ω+(1+λ ). Indeed, by
λout λout # λout x out
construction, Q is the two-point probability measure given by
λout
ξ 1
Q = δ + δ .
λout
1+ξ
ω+(1+λout)
1+ξ
ω+(−λin)
Therefore, the barycenter is given by
ω+(cid:0)
(1+λ )·
ξ
−λ ·
1 (cid:1)
out in
1+ξ 1+ξ
=
ω+(cid:0)(1+λ out)λ
in −
λ in(1+λ out)(cid:1)
= ω+(0) = b⋆.
1+λ +λ 1+λ +λ
in out in out
As a result, Theorem 8.7 implies that
λ ξ 1
out ≤ Qhb ((cid:114) ) = hb (x)+ hb (z)
1+λ
b⋆
1+ξ
b⋆
1+ξ
b⋆
out
ξ 1
≤ hb (x)+ ,
1+ξ
b⋆
1+ξ
where we used Corollary 8.4 to bound hb (z) ≤ 1 for all b,z ∈ S.
b⋆
Hence, we obtain
1+ξ (cid:16) λ 1 (cid:17)
hb (x) ≥ out −
b⋆
ξ 1+λ 1+ξ
out238 8 Wasserstein barycenters
1+ξ (cid:16)λ 1 (cid:17)
out
= ξ−
ξ λ 1+ξ
in
λ 1
out
= (1+ξ)−
λ ξ
in
λ λ λ 1+λ
out out in out
= + · −
λ λ 1+λ λ
in in out in
λ 1
out
= − ,
1+λ λ
out in
which completes the proof. ⊔⊓
Note that Theorem 8.8 gives a lower bound on hb (x) that is uniform
b⋆
in both b and x. It is of course possible to make this result depend on x
only and get a result of the form
λ (x) 1
hb (x) ≥ out − .
b⋆
1+λ (x) λ (x)
out in
If we assume that
(cid:114)
(cid:16) λ ( ) 1 (cid:17)
out
P − > 0,
(cid:114) (cid:114)
1+λ ( ) λ ( )
out in
then standard concentration tools ensure that P
hbn((cid:114)
) > 0 for n large
n b⋆
enough as desired.
Instead of going into these details, let us inspect the uniform bound
more closely. From the master theorem, and Theorem 8.8, we get the
following corollary.
Corollary 8.9.Let P be a probability measure on an NNC geodesic
space (S,d) and denote by b⋆ and b a barycenter of P and an empirical
n
barycenter respectively. Assume that the tangent cone of at b⋆ equipped
with ⟨·,·⟩ is a convex subset of a Hilbert space. Moreover, let λ ,λ ∈
b⋆ in out
[0,∞] be such that
λ 1
h := out − > 0
1+λ λ
out in
and assume further that for any x ∈ supp(P), there exists a geodesic
connecting b⋆ to x that is (λ ,λ )-extendable. Then b⋆ is unique and
in out
the empirical barycenter satisfies
E(cid:2) d2(b ,b⋆)(cid:3) ≤
4σ2
n
hn
where σ2 denotes the variance of P that is defined in (8.5).8.3 Parametric rates for Wasserstein barycenters 239
As a result, we get parametric rates when geodesics may be suffi-
ciently extended. In particular, if S is a Hilbert space, then all geodesics
are infinitely extendable. Therefore h = 1 and we recover (8.1).
8.3 Parametric rates for Wasserstein barycenters
Toconcludethesenotes,westudyWassersteinbarycentersasanexample.
Note that our result readily applies to this case. One may ask the
question: how does the condition of extendable geodesics translate in
terms of optimal transport? It turns out that it can be characterized in
terms of regularity conditions on the Brenier maps.
Theorem 8.10.Let µ,ν ∈ W be two probability measures such that
2
µ has a density and let φ : Rd → R be the convex function defined by
φ(x) = (∥x∥2 −f(x))/2, where f is the Kantorovich potential given
in Definition 1.15. In particular, ∇φ is defined µ-almost surely and
is the Brenier map. Recall that the unique constant-speed geodesic ω
connecting µ to ν is given by ω(t) = ((1−t)id+t∇φ) µ. Then, for
#
any λ > 0, ω is (0,λ)-extendable if and only if φ is λ -strongly convex.
1+λ
Proof. Assume first that ω is (0,1+λ)-extendable and let
ω+ : [0,1+λ] → W
2
denote its extension. Let Y ∼ ω+(1+λ) and observe that there exists
λ
a convex function φ defined µ-almost everywhere such that Y =
λ λ
∇φ (X), where X ∼ µ. Moreover, since ω+ is a geodesic and ∇φ(X) ∼
λ
ω+(1), we also have
λ 1
∇φ(X) = X + Y ,
λ
1+λ 1+λ
so that Y = ∇φ (X) = (1+λ)∇φ(X)−λX. In particular, it means
λ λ
that we can choose
1 λ
φ(x) = φ (x)+ ∥x∥2.
λ
1+λ 2(1+λ)
Since φ is convex, so is φ /(1+λ) and the above display implies that
λ λ
φ is λ -strongly convex.
1+λ
Conversely, assume that φ is λ -strongly convex and define Y =
1+λ λ
(1+λ)∇φ(X)−λX where X ∼ µ. We are going to show that Y and
λ
X are optimally coupled. To that end, note that Y = ∇φ (X) where
λ λ240 8 Wasserstein barycenters
λ
φ (x) = (1+λ)φ(X)− ∥x∥2.
λ
2
Since φ is λ -strongly convex, φ is convex and thus ∇φ is the
1+λ λ λ
Brenier map. It follows that Y and X are optimally coupled so that
λ
ω+ : [0,1+λ] → W defined by
2
(cid:16) t (cid:17)
ω+(t) = id+ (∇φ −id) µ
λ
1+λ #
is a geodesic connecting µ and the distribution of Y such that ω+(t) =
λ
ω(t) for t ∈ [0,1]. Therefore ω is (0,1+λ)-extendable. ⊔⊓
Recall that if µ and ν both have a density such that the Brenier map
from µ to ν is given by ∇φ, then the Brenier map from ν to µ is given
by ∇φ∗. Therefore, if φ is β-smooth in the sense that for any x,y ∈ Rd,
β
φ(x)−φ(y) ≤ ⟨∇φ(y),x−y⟩+ ∥x−y∥2,
2
then φ∗ is 1/β-strongly convex (see Lemma A.9), which, in turn implies
that the geodesic connecting ν to µ is (0, 1 )-extendable.
β+1
These facts yield the following theorem but we provide an alternate,
more direct, proof.
Theorem 8.11.Let P be a probability measure on W with a barycenter
2
b⋆ that admits a density. Assume further that for any µ ∈ supp(P) the
Breniermap from b⋆ toµisα-stronglyconvexand β-smoothwith β−α ∈
[0,1). Then b⋆ is unique and the empirical Wasserstein barycenter b
n
satisfies
E(cid:2) W2(b ,b⋆)(cid:3) ≤
4σ2
.
2 n (1−(β−α))2n
Proof. For any µ ∈ supp(P), let φ be such that ∇φ is the Brenier map
from b⋆ to µ. For any b,µ ∈ W , let X,X′ ∼ µ, Y,Y′ ∼ b and Z,Z′ ∼ b⋆.
2
In view of the gluing lemma, we may assume that (X,Z) and (Y,Z)
are optimally coupled whereas we assume that (X′,Y′) and (X′,Z′) are
optimally coupled.
Note that rearranging terms in the definition of the hugging function,
our goal is to prove that
E∥X −Y∥2 ≤ E∥X′−Y′∥2+(β−α)E∥Y −Z∥2. (8.10)
By assumption, for b⋆-almost all z ∈ Rd and any y ∈ Rd, we have8.3 Parametric rates for Wasserstein barycenters 241
α β
∥y−z∥2 ≤ φ(y)−φ(z)−⟨∇φ(z),y−z⟩ ≤ ∥y−z∥2. (8.11)
2 2
It holds
E∥X −Y∥2 = E∥X −Z∥2+E∥Y −Z∥2+2E⟨X −Z,Z −Y⟩
= E∥X −Z∥2+E∥Y −Z∥2−2E⟨Z,Z −Y⟩+2E⟨∇φ(Z),Z −Y⟩.
(8.12)
Next, note that on the one hand, it follows from (8.11) that
2E⟨∇φ(Z),Z −Y⟩ ≤ 2Eφ(Z)−2Eφ(Y)+βE∥Y −Z∥2
= 2Eφ(Z′)−2Eφ(Y′)+βE∥Y −Z∥2
≤ 2E⟨∇φ(Z′),Z′−Y′⟩−αE∥Y′−Z′∥2+βE∥Y −Z∥2
= 2E⟨X′,Z′−Y′⟩−αE∥Y′−Z′∥2+βE∥Y −Z∥2.
Since E∥Y′−Z′∥2 ≥ E∥Y −Z∥2, we get,
2E⟨∇φ(Z),Z −Y⟩ ≤ 2E⟨X′,Z′−Y′⟩+(β−α)E∥Y −Z∥2.
On the other hand,
E∥Y −Z∥2−2E⟨Z,Z −Y⟩ = E∥Y∥2−E∥Z∥2 = E∥Y′∥2−E∥Z′∥2.
Together, with (8.12), the above two displays yield
E∥X −Y∥2 ≤ E∥X′−Z′∥2+E∥Y′∥2−E∥Z′∥2
+2E⟨X′,Z′−Y′⟩+(β−α)E∥Y −Z∥2
= E∥X′−Y′∥2+(β −α)E∥Y −Z∥2,
which completes the proof of (8.10).
We have proved that hb (µ) ≥ 1−(β −α) which, together with the
b⋆
master theorem, completes the proof. ⊔⊓
Barycenters are the equivalent of averages on curved spaces. As
such they are the building block of many statistical tools including
regression [CLM23], analysis of variance [DM19], change-point detec-
tion [DM20], discriminant analysis [FCCR18], and principal component
analysis[BGKL17,CSB+18].Despiteinitialwork,manyquestionsabout
the statistical properties of these statistical objects remain to be under-
stood.242 8 Wasserstein barycenters
8.4 Discussion
§8.1. Beyond the setting of Hilbert spaces, quantitative laws of large
numbers are obtained over Banach spaces in relation to the theory of
type and cotype, see [LT91]. Also, see the excellent exposition [Stu03]
for barycenters over NPC spaces, from which Exercise 2 is taken.
§8.2. The material in this section is taken from [ACLGP20, LGPRS22].
§8.3. The basic theory of Wasserstein barycenters (existence, duality,
etc.) was developed in [AC11]; see Exercise 3. Statistical consistency
for Wasserstein barycenters was established in [LGL17]. The variance
inequality in Exercise 4 is from [CMRS20].
Substantial attention has also been devoted to the computation of
barycenters. For discrete distributions, the work of [ABA21, ABA22]
established polynomial-time tractability of Wasserstein barycenters
in fixed dimension, and NP-hardness in general dimension; see the
references therein for other approaches, such as parametrization via
neural networks and application of continuous optimization methods.
Another line of work, more closely related to Chapter 5, develops
algorithms for computing the barycenter via gradient methods in the
Wassersteinspace[AEdBCAM16,ZP19,CMRS20,ACGS21,BVFRT22,
KDLY22, BRT24]. The descent lemma in Exercise 5 is from [ZP19],
which interpreted the fixed-point approach of [AEdBCAM16] as Wasser-
stein gradient descent.
Barycenters for Gaussians were studied earlier than the general
case, dating back to [KS94, RU02]. Statistical estimation was studied
in [KSS21], and non-asymptotic computational guarantees for Wasser-
stein gradient descent were given in [CMRS20, ACGS21].
Similarly to Chapter 4, one can add entropic regularization to the
Wasserstein barycenter, at the level of the Wasserstein distance or the
barycenter objective or both; see [Kro18, BCP19b, LGYS20, CEK21,
Chi23, VC23].
8.5 Exercises
1. Let P be a distribution over P (R). Give a closed-form expression
2
for the W barycenter of P in terms of the CDFs of the measures in
2
suppP.
2. Suppose that P is a probability measure over an NPC space (S,d)
withbarycenterb⋆.Itturnsoutthatstatisticalestimationofbarycen-
ters over NPC spaces is far easier, as we demonstrate in this exercise.8.5 Exercises 243
a) Show that for any b ∈ S,
P[d2(b, (cid:114) )−d2(b⋆, (cid:114) )] ≥ d2(b,b⋆). (8.13)
b) Suppose that (X )n is an i.i.d. sequence drawn from P and
i i=1
form the following estimator b inductively: set b = X , and
n 1 1
for n ≥ 2 let b := ω (1/n) where ω : [0,1] → S is the
n bn−1,Xn x,y
constant-speed geodesic joining x to y. Prove by induction that
for all n ≥ 1,
σ2
Ed2(b ,b⋆) ≤ , where σ2 = Pd2(b⋆, (cid:114) ).
n
n
Hint: Apply the NPC inequality from Proposition 7.11 together
with the inequality (8.13).
3. Let µ ,...,µ ∈ P (Rd) and let Γ(µ ,...,µ ) denote the set of cou-
1 n 2 1 n
plings of µ ,...,µ . Consider the multi-marginal optimal transport
1 n
problem
(cid:90) n n
(cid:88)(cid:13) 1 (cid:88) (cid:13)2
min (cid:13)x i− x j(cid:13) γ(dx 1,...,dx n).
γ∈Γ(µ1,...,µn)
i=1
n
j=1
Let γ⋆ denote an optimal solution. Prove that if (X ,...,X ) ∼
1 n
γ⋆, then the law of 1 (cid:80)n X is the Wasserstein barycenter of
n i=1 i
µ ,...,µ .
1 n
4. Due to Theorems 8.7 and 8.10, in the case of the Wasserstein space
we know that as long as the transport maps from the barycenter b⋆
to elements in the support of P are obtained from α-strongly convex
potentials, and the barycenter of the extended distribution is still
b⋆, then Phb ((cid:114) ) ≥ α. It turns out that due to the structure of the
b⋆
Wasserstein space, the second condition is unnecessary.
To prove this, use the following dual characterization of the Wasser-
stein barycenter: for each µ ∈ supp(P), φ is such that (∇φ ) b⋆ =
µ µ #
(cid:82) ∥·∥2
µ,and ( −φ )P(dµ) = 0.Assumethateachφ isα(µ)-strongly
2 µ µ
convex. Use this to show that
α(µ)
φ∗(x)+φ (y) ≥ ⟨x,y⟩+ ∥y−∇φ∗(x)∥2.
µ µ 2 µ
By integrating this inequality, prove that (8.9) holds with λ
1+λ
(cid:82)
replaced by α(µ)P(dµ).244 8 Wasserstein barycenters
5. Let P be a probability measure over W and let F : P (Rd) → R
2 2
denote the barycenter functional F(b) := 1 PW2(b, (cid:114) ). Using (5.30),
2 2
the Wasserstein gradient of F is given by ∇∇F(b) = id−PT (cid:114), and
b→
a Wasserstein gradient descent step with step size h > 0 is given by
the iteration b+ := (id−h∇∇F(b)) b. Prove the descent lemma
#
F(b+)−F(b) ≤ −h(cid:0) 1− h(cid:1) ∥∇∇F(b)∥2,
2 b
which quantifies the progress made in one step of GD on F. Deduce
that h = 1 is a reasonable choice of step size and write out the form
of the GD updates in this case.
6. Specialize the GD updates (with step size h = 1) from the previous
exercisetothecasewhenP issupportedoncentered,non-degenerate
Gaussians. In particular, when initialized at a centered Gaussian,
show that all of the iterates are centered Gaussians, and write down
the update equations for the covariance matrix.A
Convex analysis
In this appendix, we provide a quick review of convex analysis. We refer
to the book [Roc97] for a comprehensive treatment.
A.1 Convex functions, subdifferentials, and duality
Definition A.1.A function f : Rd → R ∪ {∞} is convex if for all
x,y ∈ Rd and all t ∈ [0,1],
f((1−t)x+ty) ≤ (1−t)f(x)+tf(y).
Also, a set C ⊆ Rd is convex if for all x,y ∈ Rd and all t ∈ [0,1],
(1−t)x+ty ∈ C.
The domain of f, dom(f), is the set {f < ∞} of points where f
takes finite values. If f is convex, then dom(f) is a convex set.
We say that f is proper if it does not take the value −∞ (note that
this is already assumed in the definition of convexity given above) and
it is not identically +∞. We assume without further mention that the
convex functions we work with are proper. We say that f is closed or
lower semicontinuous if for any sequence x → x in Rd, it holds that
k
liminf f(x ) ≥ f(x); equivalently, all of the sublevel sets {f ≤ t}
k→∞ k
for t ∈ R are closed.
Suppose that f takes values in R. Then, convexity of f implies
that f is automatically continuous, and in fact locally Lipschitz, hence
differentiable almost everywhere by Rademacher’s theorem. If f is
continuously differentiable, then convexity of f is equivalent to f always
lying above its tangent line:246 A Convex analysis
f(y) ≥ f(x)+⟨∇f(x),y−x⟩, ∀x,y ∈ Rd. (A.1)
If f is twice continuously differentiable, then convexity of f is equivalent
to a condition on its Hessian:
∇2f(x) ⪰ 0, ∀x ∈ Rd.
In general, a convex function may not be differentiable. One reason
why differentiability can fail is simply because f takes on infinite values
(dom(f) ̸= Rd). However, as discussed above, f is always differentiable
almost everywhere on the interior of its domain. Moreover, we can find a
usefulsubstitutefordifferentiabilitythroughthenotionofasubgradient,
which is based on the “above tangent line” property encapsulated
in (A.1).
Definition A.2(Subdifferential). Let f : Rd → R∪{∞} be convex
and let x ∈ Rd. We say that g is a subgradient of f at x if
f(y) ≥ f(x)+⟨g,y−x⟩, ∀y ∈ Rd.
The set of all subgradients of f at x is called the subdifferential of f at
x, denoted ∂f(x). Also, ∂f := {(x,g) : x ∈ Rd, g ∈ ∂f(x)} is called the
subdifferential of f.
Importantly, the following lemma holds:
Lemma A.3.Iff : Rd → R∪{∞}isconvexandxliesintheinteriorof
dom(f), then ∂f(x) is non-empty. Also, if f is differentiable at x, then
the subdifferential at x is single-valued and satisfies ∂f(x) = {∇f(x)}.
We next turn towards the crucial concept of duality.
Definition A.4(Convex conjugate). For any function f : Rd →
R∪{∞}, we define its convex conjugate1 f∗ via
f∗(y) := sup {⟨x,y⟩−f(x)} , y ∈ Rd.
x∈Rd
Example A.5.Let A ≻ 0 be a positive definite matrix. Then, the con-
vex conjugate of x (cid:55)→ 1 ⟨x,Ax⟩ is the function y (cid:55)→ 1 ⟨y,A−1y⟩. See
2 2
Lemma A.13 for the proof. The reader is invited to write down other
examples of convex functions and to compute their conjugates.
1 TheconvexconjugateisalsoknownastheFenchel–Legendretransform,theFenchel
dual or variations of these terms.A.1 Convex functions, subdifferentials, and duality 247
As a supremum of affine functions, the convex conjugate of f is
always a closed convex function, even if f is not. Conversely, if f is
closed and convex, then f = f∗∗.
The inequality f(x)+f∗(y) ≥ ⟨x,y⟩ is trivial from the definition
of the convex conjugate. However, it is important enough to deserve a
name, and we need the equality case for later use.
Theorem A.6(Fenchel–Young inequality). For a convex function
f : Rd → R∪{∞} and any x,y ∈ Rd,
f(x)+f∗(y) ≥ ⟨x,y⟩.
Equality holds if and only if y ∈ ∂f(x).
Note that by symmetry, equality holds if and only if x ∈ ∂f∗(y). In
particular, when f and f∗ are differentiable, then the subdifferentials
are single-valued, so that the equality condition reads y = ∇f(x) and
x = ∇f∗(y). This says that the gradient mappings are inverse to each
other: ∇f∗ = (∇f)−1.
We conclude this section by proving Rockafellar’s theorem (Theo-
rem1.10),whichcharacterizessubdifferentialsofclosedconvexfunctions
as maximally monotone subsets of Rd×Rd. In Section 1.4.1, we show
that if φ : Rd → R is convex, then its subdifferential ∂φ is cyclically
monotone. Here, we prove the converse.
Proof of Theorem 1.10. Let A be cyclically monotone and fix (x ,y ) ∈
0 0
A. Define for any x ∈ Rd the function
(cid:8) (cid:9)
φ(x) = sup sup ⟨x −x ,y ⟩+⟨x −x ,y ⟩+···+⟨x−x ,y ⟩ .
1 0 0 2 1 1 k k
k≥0(xi,yi)∈A
i=1,...,k
Clearly φ is closed and convex as a supremum of affine functions.
Moreover, φ(x ) ≤ 0 by cyclical monotonicity2 and φ(x ) ≥ 0 (take
0 0
k = 1 and (x ,y ) = (x ,y )) so that φ(x ) = 0 and φ is a proper
1 1 0 0 0
convex function. Finally note that for any (x,y) = (x ,y ) ∈ A
k+1 k+1
and any z ∈ Rd, it holds
(cid:8)
φ(z) ≥ sup sup ⟨x −x ,y ⟩+⟨x −x ,y ⟩+···
1 0 0 2 1 1
k≥0(xi,yi)∈A,i=1,...,k
(cid:9)
+⟨x−x ,y ⟩+⟨z−x,y⟩
k k
= φ(x)+⟨z−x,y⟩.
Therefore, y ∈ ∂φ(x). ⊔⊓
2 This is in fact the only place we use cyclical monotonicity!248 A Convex analysis
A.2 Strong convexity and smoothness
Definition A.7.A function f : Rd → R∪{∞} is called α-convex (for
α ∈ R) if for all x,y ∈ Rd and all t ∈ [0,1],
αt(1−t)
f((1−t)x+ty) ≤ (1−t)f(x)+tf(y)− ∥y−x∥2.
2
The case when α = 0 corresponds to convexity, as in Definition A.1.
When α > 0, then f is called strongly convex, and the above inequality
strengthens the usual convexity inequality. When α < 0, then f is called
semi-convex.
When f is continuously differentiable, α-convexity is equivalent to
the first statement below. When f is twice continuously differentiable,
α-convexity is equivalent to both statements below.
1. f(y) ≥ f(x)+⟨∇f(x),y−x⟩+ α ∥x−y∥2, for all x,y ∈ Rd.
2
2. ∇2f(x) ⪰ αI for all x ∈ Rd.
Wealsoformulatethedualpropertyofanupperboundonthesecond
derivative.
Definition A.8.A continuously differentiable function f : Rd → R is
called β-smooth (β ≥ 0) if for all x,y ∈ Rd,
β
f(y) ≤ f(x)+⟨∇f(x),y−x⟩+ ∥y−x∥2.
2
When f is twice continuously differentiable, then this is equivalent
to ∇2f(x) ⪯ βI for all x ∈ Rd. If, in addition, f is convex, then
∇2f(x) ⪰ 0, so in particular the operator norm of ∇2f(x) is at most
β. In turn, this is equivalent to the β-Lipschitzness of the mapping
∇f : Rd → Rd (thus, convex and smooth functions are often referred to
as gradient Lipschitz).
The properties of strong convexity and smoothness are dual. For
simplicity, we state the following lemma assuming that f is continuously
differentiable, but the assumptions can be somewhat relaxed.
Lemma A.9.Let f : Rd → R be continuously differentiable, convex,
and ∥∇f(x)∥ → ∞ as ∥x∥ → ∞. Let α > 0. Then, f is α-strongly
convex if and only if its convex conjugate f∗ is 1-smooth.
αA.2 Strong convexity and smoothness 249
Proof. From classical results in convex analysis (see [Roc97, Theorem
25.5, Theorem 26.6, and Lemma 26.7]), under our assumptions, ∇f :
Rd → Rd is a diffeomorphism with inverse ∇f∗.
(⇒) By taking the first-order condition for strong convexity and
adding it to the inequality with x and y interchanged, we obtain
⟨∇f(x)−∇f(y),x−y⟩ ≥ α∥x−y∥2.
Let x = ∇f∗(x′) and y = ∇f∗(y′) and recall also that ∇f ◦∇f∗ = id.
The above inequality yields, for all x′,y′ ∈ Rd,
⟨x′−y′,∇f∗(x′)−∇f∗(y′)⟩ ≥ α∥∇f∗(x′)−∇f∗(y′)∥2.
Applying Cauchy–Schwarz to the left-hand side and rearranging, it
follows that ∇f∗ is 1-Lipschitz, which is equivalent to f∗ being 1-
α α
smooth, as discussed above.
(⇐) By smoothness of f∗,
f(y) = sup {⟨y,y′⟩−f∗(y′)}
y′∈Rd
(cid:110) 1 (cid:111)
≥ sup ⟨y,y′⟩−f∗(x′)−⟨∇f∗(x′),y′−x′⟩− ∥y′−x′∥2
2α
y′∈Rd
α
= −f∗(x′)+⟨y,x′⟩+ ∥y−∇f∗(x′)∥2.
2
Choosex′ = ∇f(x′)sothat∇f∗(x′) = xandrecallthatf(x)+f∗(x′) =
⟨x,x′⟩. It yields
α
f(y)−f(x)−⟨∇f(x),y−x⟩ ≥ ∥y−x∥2,
2
completing the proof. ⊔⊓
Note that if f, f∗ are twice continuously differentiable, then f
is α-strongly convex iff ∇2f ⪰ αI, and f∗ is 1-smooth iff ∇2f∗ =
α
(∇2f)−1◦∇f∗ ⪯ α−1I, which provides a more transparent proof.
Strong convexity also implies the following property, which can be
viewedasastrongquantitativeformoftheprinciplethatlocallyoptimal
points are globally optimal under convexity.
Definition A.10(Polyak–L(cid:32) ojasiewicz inequality). We say that a
continuously differentiable function f : Rd → R satisfies a Polyak–
L(cid:32) ojasiewicz (PL(cid:32) ) inequality with constant α > 0 if for all x ∈ Rd,
∥∇f(x)∥2 ≥ 2α(f(x)−inff).250 A Convex analysis
Lemma A.11.If f : Rd → R is continuously differentiable and α-
convex for α > 0, then it satisfies a PL(cid:32) inequality with constant α.
Proof. Let x denote the minimizer of f. Then,
⋆
α
inff = f(x ) ≥ f(x)+⟨∇f(x),x −x⟩+ ∥x −x∥2.
⋆ ⋆ ⋆
2
By Cauchy–Schwarz and Young’s inequality,
⟨∇f(x),x −x⟩ ≥ −∥∇f(x)∥∥x −x∥
⋆ ⋆
1 α
≥ − ∥∇f(x)∥2− ∥x −x∥2.
⋆
2α 2
Substituting and rearranging finishes the proof. ⊔⊓
We also note that strong convexity implies quadratic growth around
the minimizer.
Lemma A.12.If f : Rd → R∪{∞} is α-convex, and if x denotes the
⋆
minimizer of f, then for all x ∈ Rd,
α
f(x)−f(x ) ≥ ∥x−x ∥2. (A.2)
⋆ ⋆
2
Proof. The strong convexity inequality gives
αt(1−t)
f((1−t)x +tx) ≤ (1−t)f(x )+tf(x)− ∥x−x ∥2,
⋆ ⋆ ⋆
2
or
0 ≤ f((1−t)x +tx)−f(x )
⋆ ⋆
(cid:104) α(1−t) (cid:105)
≤ t f(x)−f(x )− ∥x−x ∥2 .
⋆ ⋆
2
Rearranging, dividing by t, and letting t ↘ 0 proves the result. ⊔⊓
Actually, in Exercise 5 in Chapter 5, we refine this statement to
show that the PL(cid:32) inequality itself implies the growth inequality (A.2).
We refer the reader to [KNS16, Appendix A] for a concise exposition of
the interplay between these inequalities.A.3 Convex conjugate of a quadratic function 251
A.3 Convex conjugate of a quadratic function
In this section we establish the following useful lemma which states that
the convex conjugate of a quadratic function is an explicit quadratic
function.
Lemma A.13.If f(x) = 1 xTAx+bTx for A ≻ 0, then
2
1
f∗(y) = (y−b)TA−1(y−b). (A.3)
2
If A is not invertible, the same expression holds if we interpret A−1(y−b)
as the solution to y = Ax+b if it exists, and f∗(y) = +∞ otherwise.
Proof. The definition of f∗(y) implies
(cid:110) 1 (cid:111)
f∗(y) = sup yTx− xTAx−bTx .
2
x∈Rd
Differentiating the objective yields that if a maximizer x⋆ exists, then
it satisfies
y = Ax⋆+b,
which yields x⋆ = A−1(y−b) if y−b ∈ span(A). In this case, we obtain
1 1
f∗(y) = yTx⋆− (x⋆)TAx⋆−bTx⋆ = (y−b)TA−1(y−b).
2 2
On the other hand, if y − b ̸∈ span(A), then we can find a vector
z ∈ ker(A) such that zT(y−b) ̸= 0. Considering x = λz for λ ∈ R, we
obtain
(cid:110) 1 (cid:111)
f∗(y) ≥ sup yT(λz)− (λz)TA(λz)−bT(λz)
λ∈R 2
= supλzT(y−b) = +∞,
λ∈R
as claimed. ⊔⊓B
Probability
In this appendix, we gather together some background material on
probability theory. See, e.g., the textbook [Bil99] for further discussion.
We begin with the notion of convergence of probability measures.
Definition B.1.A sequence of probability measures (µ ) on Rd is
n n
said to converge (weakly) to a probability measure µ if for all bounded
continuous functions f : Rd → R, it holds that
(cid:90) (cid:90)
fdµ → fdµ.
n
For the topology of Rd, we know exactly which subsets are compact:
namely, a set A is compact if and only if it is closed and bounded
(Heine–Borel theorem). This provides a useful criterion for when a
sequence (x ) in A converges, upon passing to a subsequence, to a
n n
point in A. The following definition and theorem characterize compact
sets of probability measures in the topology of weak convergence.
Definition B.2.A set A of probability measures on Rd is tight if for
all ε > 0, there is a compact set K such that µ(Kc) ≤ ε for all µ ∈ A.
Theorem B.3(Prokhorov’s theorem). Any weakly convergent se-
quence of probability measures is tight. Conversely, any tight sequence
of probability measures has a subsequential weak limit.
Equivalently, a set A of probability measures on Rd is compact if
and only if it is closed and tight.
We omit the proof, but the intuition can be gleaned via a simple
example: on R, let µ = δ for all n, where x → ∞; then, (µ )
n xn n n n254 B Probability
clearlyhasnoweaklyconvergentsubsequence.Theconditionoftightness
ensuresthatthemassdoesnotrunofftoinfinity,andoncethisisensured
then a weakly convergent subsequence is guaranteed.
The following theorem provides useful reformulations of weak con-
vergence of measures.
Theorem B.4(Portmanteau theorem). Let (µ ) be a sequence
n n
in P(Rd) and let µ ∈ P(Rd). The following are equivalent.
1.µ → µ weakly.
n
2.(cid:82) fdµ → (cid:82) fdµ for all bounded Lipschitz continuous f : Rd → R.
n
(cid:82) (cid:82)
3. fdµ ≤ liminf fdµ for all lower semicontinuous functions
n→∞ n
f : Rd → [0,∞].
(cid:82) (cid:82)
4. fdµ ≥ limsup fdµ for all upper semicontinuous functions
n→∞ n
f : Rd → [−∞,0].
5.µ(G) ≤ liminf µ (G) for all open G ⊆ Rd.
n→∞ n
6.µ(F) ≥ limsup µ (F) for all closed F ⊆ Rd.
n→∞ n
7.lim µ (A) = µ(A) for all Borel A ⊆ Rd such that µ(∂A) = 0.
n→∞ n
Proof. (1) ⇒ (2) is trivial. Also, it is easy to see that (3) is equivalent
to (4) by replacing f by −f, and that (5) is equivalent to (6) by taking
complements.
(2) ⇒ (3): It is known that one can approximate f from below by a
sequence (f ) of Lipschitz continuous functions Rd → [0,∞). For any
k k
k,n ∈ N, it holds that (cid:82) f dµ ≤ (cid:82) fdµ . Taking the limit n → ∞,
k n n
(cid:82) (cid:82)
we get f dµ ≤ liminf fdµ . Then, take k → ∞ using the
k n→∞ n
monotone convergence theorem.
(3) ⇒ (5): The indicator function 1 is lower semicontinuous. Sim-
G
ilarly, we obtain (4) ⇒ (6) since the indicator function 1 is upper
F
semicontinuous.
(5) and (6) ⇒ (7): The condition µ(∂A) = 0 means that µ(intA) =
µ(A) = µ(A). Applying (5) to the open set intA and (6) to the closed
set A proves (7).
(7) ⇒ (1): Let f be bounded and continuous; we may as well assume
f is non-negative. Observe that for t ∈ R, it holds that ∂f−1([t,∞)) ⊆
f−1({t}),andthissetcanhavepositiveµ-measureforatmostcountably
many values of t. Applying (7), we obtain
(cid:90) (cid:90) ∞ (cid:90) ∞ (cid:90)
fdµ = µ {f ≥ t}dt → µ{f ≥ t}dt = fdµ,
n n
0 0
where to justify the convergence we can use, e.g., bounded convergence
(since the integral can actually be taken over a finite interval). ⊔⊓B Probability 255
The following lemma, as the name suggests, allows us to “glue”
together couplings which share a marginal and is useful for some con-
structions in optimal transport (e.g., the proof of the triangle inequality
for Wasserstein distances in Proposition 1.3).
Lemma B.5(Gluing lemma). Let γ and γ′ be two measures on
Rd × Rd such that for any Borel set A ⊂ Rd, it holds γ(Rd × A) =
γ′(A×Rd),i.e.,thesecondmarginalofγ coincideswiththefirstmarginal
of γ′. Then there exists three random variables X,Y,Z ∈ Rd such that
(X,Z) ∼ γ and (Z,Y) ∼ γ′.
Proof. We are going to explicitly construct such a triplet (X,Y,Z). To
thatend,letZ bedistributedaccordingthesecondmarginalofγ (which
corresponds to the first marginal of γ′ by assumption). Then γ (resp.
γ′) determines the conditional distribution of X (resp. Y) given Z. For
example, we may draw X and Y to be conditionally independent given
Z. This gives a valid triplet (X,Y,Z). ⊔⊓
opt opt
X Z Y
Fig. B.1. The diagram above is a convenient way to represent couplings between
multiple random variables. An edge represent the constraint that the coupling needs
to be optimal. In general, any coupling described as a graph with no cycle can be
realized using the gluing lemma.References
ABAM22. E. Abbe, E. Boix-Adsera´, and T. Misiakiewicz, The merged-
staircase property: a necessary and nearly sufficient condition for
SGD learning of sparse functions on two-layer neural networks, in
Proceedings of Thirty Fifth Conference on Learning Theory (P.-L.
Loh and M. Raginsky, eds.), Proceedings of Machine Learning
Research 178, PMLR, 7 2022, pp. 4782–4887.
ABAM23. E. Abbe,E. Boix-Adsera`,andT. Misiakiewicz,SGDlearningon
neural networks: leap complexity and saddle-to-saddle dynamics, in
Proceedings of Thirty Sixth Conference on Learning Theory (G. Neu
and L. Rosasco, eds.), Proceedings of Machine Learning Research
195, PMLR, 7 2023, pp. 2552–2623.
AC11. M. Agueh and G. Carlier, Barycenters in the Wasserstein space,
SIAM Journal on Mathematical Analysis 43 (2011), 904–924.
ACLGP20. A. Ahidar-Coutrix, T. Le Gouic, and Q. Paris, Convergence
ratesforempiricalbarycentersinmetricspaces:curvature,convexity
andextendablegeodesics,Probab. Theory Related Fields 177(2020),
323–368.
AKT84. M. Ajtai, J. Komlo´s, and G. Tusna´dy, On optimal matchings,
Combinatorica 4 (1984), 259–264.
ACG+23. O.D.Akyildiz,F.R.Crucinio,M.Girolami,T.Johnston,and
S. Sabanis, Interacting particle Langevin algorithm for maximum
marginal likelihood estimation, arXiv preprint 2303.13429 (2023),
1–38.
AKP22. S. Alexander, V. Kapovitch, and A. Petrunin, Alexandrov
geometry: foundations, arXiv preprint 1903.08539 (2022), 1–301.
AR20. P. Alquier and J. Ridgway, Concentration of tempered posteriors
and of their variational approximations, Ann. Statist. 48 (2020),
1475–1497.
ACGS21. J.Altschuler,S.Chewi,P.Gerber,andA.J.Stromme,Averag-
ing on the Bures–Wasserstein manifold: dimension-free convergence
of gradient descent, in Advances in Neural Information Processing
Systems (M.Ranzato,A.Beygelzimer,K.Nguyen,P.S.Liang,
J. W. Vaughan, and Y. Dauphin, eds.), 34, Curran Associates,
Inc., 2021, pp. 22132–22145.258 References
ABA21. J. M. Altschuler and E. Boix-Adsera`, Wasserstein barycenters
can be computed in polynomial time in fixed dimension, Journal of
Machine Learning Research 22 (2021), 1–19.
ABA22. J. M. Altschuler and E. Boix-Adsera`, Wasserstein barycenters
areNP-hardtocompute,SIAMJ.Math.DataSci.4(2022),179–203.
AC24. J. M. AltschulerandS. Chewi,Fasterhigh-accuracylog-concave
sampling via algorithmic warm starts, J. ACM 71 (2024), 1–55.
ANWR17. J. M. Altschuler, J. Niles-Weed, and P. Rigollet, Near-linear
time approximation algorithms for optimal transport via Sinkhorn
iteration, in Advances in Neural Information Processing Systems 30,
2017, pp. 1961–1971.
Ama98. S.-i. Amari, Natural gradient works efficiently in learning, Neural
Computation 10 (1998), 251–276.
AN00. S.-i. Amari and H. Nagaoka, Methods of information geometry,
Translations of Mathematical Monographs 191,AmericanMathemat-
icalSociety,Providence,RI,2000,Translatedfromthe1993Japanese
original by Daishi Harada.
ABS21. L. Ambrosio,E. Brue´,andD. Semola,Lectures on optimal trans-
port, Unitext 130, Springer, 2021.
AG13. L. Ambrosio and N. Gigli, A user’s guide to optimal transport, in
Modelling and optimisation of flows on networks, Lecture Notes in
Math. 2062, Springer, Heidelberg, 2013, pp. 1–155.
AGS08. L. Ambrosio, N. Gigli, and G. Savare´, Gradient flows in metric
spaces and in the space of probability measures, second ed., Lectures
in Mathematics ETH Zu¨rich, Birkha¨user Verlag, Basel, 2008.
AST19. L. Ambrosio, F. Stra, and D. Trevisan, A PDE approach to a
2-dimensionalmatchingproblem,Probab. Theory Related Fields 173
(2019), 433–477.
AKSG19. M. Arbel, A. Korba, A. Salim, and A. Gretton, Maximum
mean discrepancy gradient flow, in Advances in Neural Information
Processing Systems (H. Wallach, H. Larochelle, A. Beygelz-
imer,F.d'Alche´-Buc,E.Fox,andR.Garnett,eds.),32,Curran
Associates, Inc., 2019.
ACB17. M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein genera-
tive adversarial networks, in International Conference on Machine
Learning, 2017, pp. 214–223.
AL24. M. Arnese and D. Lacker, Convergence of coordinate ascent
variational inference for log-concave measures via optimal transport,
arXiv preprint 2404.08792 (2024), 1–28.
AFKL22. P.-C. Aubin-Frankowski, A. Korba, and F. Le´ger, Mirror de-
scent with relative smoothness in measure spaces, with application
to Sinkhorn and EM, in Advances in Neural Information Process-
ing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-
grave,K.Cho,andA.Oh,eds.),35,CurranAssociates,Inc.,2022,
pp. 17263–17275.
AR15. P. Awasthi and A. Risteski, On some provably correct cases
of variational inference for topic models, in Advances in Neural
InformationProcessingSystems (C.Cortes,N.Lawrence,D.Lee,
M. Sugiyama, and R. Garnett, eds.), 28, Curran Associates, Inc.,
2015, pp. 1–9.References 259
AJLS17. N. Ay, J. Jost, H. V. Leˆ, and L. Schwachho¨fer, Information ge-
ometry, Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge.
A Series of Modern Surveys in Mathematics [Results in Mathemat-
ics and Related Areas. 3rd Series. A Series of Modern Surveys in
Mathematics] 64, Springer, Cham, 2017.
BVFRT22. J. Backhoff-Veraguas, J. Fontbona, G. Rios, and F. Tobar,
Bayesian learning with Wasserstein barycenters, ESAIM Probab.
Stat. 26 (2022), 436–472.
BDI+20. A. Backurs, Y. Dong, P. Indyk, I. Razenshteyn, and T. Wag-
ner, Scalable nearest neighbor search for optimal transport, in Pro-
ceedings of the 37th International Conference on Machine Learning
(H. D. III and A. Singh, eds.), Proceedings of Machine Learning
Research 119, PMLR, 7 2020, pp. 497–506.
BGL14. D. Bakry, I. Gentil, and M. Ledoux, Analysis and geometry
of Markov diffusion operators, Grundlehren der Mathematischen
Wissenschaften [Fundamental Principles of Mathematical Sciences]
348, Springer, Cham, 2014.
BCE+22. K. Balasubramanian, S. Chewi, M. A. Erdogdu, A. Salim,
and M. S. Zhang, Towards a theory of non-log-concave sampling:
first-order stationarity guarantees for Langevin Monte Carlo, in
Proceedings of Thirty Fifth Conference on Learning Theory (P.-L.
Loh and M. Raginsky, eds.), Proceedings of Machine Learning
Research 178, PMLR, 7 2022, pp. 2896–2923.
BB23. M. Ballu and Q. Berthet, Mirror Sinkhorn: fast online optimiza-
tion on transport polytopes, in Proceedings of the 40th Interna-
tional Conference on Machine Learning (A. Krause,E. Brunskill,
K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, eds.),
Proceedings of Machine Learning Research 202, PMLR, 7 2023,
pp. 1595–1613.
dBCAMRR99. E. del Barrio, J. A. Cuesta-Albertos, C. Matra´n, and J. M.
Rodr´ıguez-Rodr´ıguez, Tests of goodness of fit based on the L -
2
Wasserstein distance, Ann. Statist. 27 (1999), 1230–1239.
dBSLNW23. E. del Barrio, A. G. Sanz, J.-M. Loubes, and J. Niles-Weed,
An improved central limit theorem and fast convergence rates for
entropic transportation costs, SIAM J. Math. Data Sci. 5 (2023),
639–669.
BKR14. S. Basu, S. Kolouri, and G. K. Rohde, Detecting and visualizing
cell phenotype differences from microscopy images using transport-
basedmorphometry,ProceedingsoftheNationalAcademyofSciences
111 (2014), 3448–3453.
BJL19. R.Bhatia,T.Jain,andY.Lim,OntheBures–Wassersteindistance
between positive definite matrices, Expo. Math. 37 (2019), 165–191.
BCP19a. J. Bigot, E. Cazelles, and N. Papadakis, Central limit theo-
rems for entropy-regularized optimal transport on finite spaces and
statistical applications, Electron. J. Stat. 13 (2019), 5120–5150.
BCP19b. J.Bigot,E.Cazelles,andN.Papadakis,Penalizationofbarycen-
ters in the Wasserstein space, SIAM J. Math. Anal. 51 (2019),
2261–2285.260 References
BGKL17. J. Bigot, R. Gouet, T. Klein, and A. Lo´pez, Geodesic PCA in
the Wasserstein space by convex PCA, Ann. Inst. Henri Poincar´e
Probab. Stat. 53 (2017), 1–26.
Bil99. P. Billingsley, Convergence of probability measures, second ed.,
Wiley Series in Probability and Statistics: Probability and Statistics,
John Wiley & Sons Inc., New York, 1999, A Wiley-Interscience
Publication.
BB18. A. Blanchet and J. Bolte, A family of functional inequalities:
L(cid:32)ojasiewiczinequalitiesanddisplacementconvexfunctions,J.Funct.
Anal. 275 (2018), 1650–1673.
BKM17. D. M. Blei,A. Kucukelbir,andJ. D. McAuliffe,Variationalin-
ference:areviewforstatisticians,JournaloftheAmericanStatistical
Association 112 (2017), 859–877.
BL19. S. Bobkov and M. Ledoux, One-dimensional empirical measures,
order statistics, and Kantorovich transport distances, Mem. Amer.
Math. Soc. 261 (2019), v+126.
BGL01. S. G. Bobkov, I. Gentil, and M. Ledoux, Hypercontractivity
of Hamilton–Jacobi equations, J. Math. Pures Appl. (9) 80 (2001),
669–696.
BL21. S. G. Bobkov and M. Ledoux, A simple Fourier analytic proof of
the AKT optimal matching theorem, Ann. Appl. Probab. 31 (2021),
2567–2584.
BLG14. E. Boissard and T. Le Gouic, On the mean speed of convergence
of empirical and occupation measures in Wasserstein distance, Ann.
Inst. Henri Poincar´e Probab. Stat. 50 (2014), 539–563.
BV05. F. Bolley and C. Villani, Weighted Csisz´ar–Kullback–Pinsker
inequalitiesandapplicationstotransportationinequalities,Ann.Fac.
Sci. Toulouse Math. (6) 14 (2005), 331–352.
BLB24. S. Bonnabel, M. Lambert, and F. Bach, Low-rank plus diagonal
approximations for Riccati-like matrix differential equations, arXiv
preprint 2407.03373 (2024), 1–21.
BPC16. N. Bonneel, G. Peyre´, and M. Cuturi, Wasserstein barycentric
coordinates: histogram regression using optimal transport, ACM
Trans. Graph. 35 (2016), 10.
Bon13. N. Bonnotte, Unidimensional and evolution methods for optimal
transportation, Theses, Universit´e Paris Sud – Paris XI; Scuola nor-
male superiore (Pise, Italie), 12 2013.
BGR+06. K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel,
B. Scho¨lkopf, and A. J. Smola, Integrating structured biologi-
cal data by kernel maximum mean discrepancy, Bioinformatics 22
(2006), e49–e57.
BLM13. S. Boucheron, G. Lugosi, and P. Massart, Concentration in-
equalities, Oxford University Press, Oxford, 2013, A nonasymptotic
theory of independence, With a foreword by Michel Ledoux.
BV04. S. Boyd and L. Vandenberghe, Convex optimization, Cambridge
University Press, Cambridge, 2004.
BRT24. S. Brahmachari, R. Rubboli, and M. Tomamichel, A fixed-
pointalgorithmformatrixprojectionswithapplicationsinquantum
information, arXiv preprint 2312.14615 (2024), 1–17.References 261
BL76. H. J. Brascamp and E. H. Lieb, On extensions of the Brunn–
Minkowski and Pr´ekopa–Leindler theorems, including inequalities
for log concave functions, and with an application to the diffusion
equation, J. Functional Analysis 22 (1976), 366–389.
Bre87. Y. Brenier, Decomposition polaire et rearrangement monotone des
champsdevecteurs,C. R. Acad. Sci. Paris Ser. I Math.305(1987),
805–808.
Bub15. S. Bubeck, Convex optimization: algorithms and complexity, Now
Publishers Inc., 2015.
BMPKC22. C. Bunne, L. Meng-Papaxanthos, A. Krause, and M. Cuturi,
Proximal optimal transport modeling of population dynamics, in
International Conference on Artificial Intelligence and Statistics
(AISTATS), 2022.
BSG+23. C. Bunne, S. G. Stark, G. Gut, J. S. del Castillo,
M. Levesque, K.-V. Lehmann, L. Pelkmans, A. Krause, and
G. Ra¨tsch,Learningsingle-cellperturbationresponsesusingneural
optimal transport, Nature Methods 20 (2023), 1759–1768.
BBI01. D. Burago, Y. Burago, and S. Ivanov, A course in metric geom-
etry, Graduate Studies in Mathematics 33, American Mathematical
Society, Providence, RI, 2001.
BM03. S. Burer and R. D. C. Monteiro, A nonlinear programming
algorithmforsolvingsemidefiniteprogramsvialow-rankfactorization,
Math. Program.95(2003),329–357,Computationalsemidefiniteand
second order cone programming: the state of the art.
BM05. S. Burer and R. D. C. Monteiro, Local minima and convergence
in low-rank semidefinite programming, Math. Program. 103 (2005),
427–444.
Bur69. D. Bures, An extension of Kakutani’s theorem on infinite product
measures to the tensor product of semifinite w∗-algebras, Trans.
Amer. Math. Soc. 135 (1969), 199–212.
CCCC20. T. Cai, J. Cheng, N. Craig, and K. Craig, Linearized optimal
transport for collider events, Phys. Rev. D 102 (2020), 116019.
CKPJ24. R. Caprio, J. Kuntz, S. Power, and A. M. Johansen, Er-
ror bounds for particle gradient descent, and extensions of the
log-Sobolev and Talagrand inequalities, arXiv preprint 2403.02004
(2024), 1–33.
CEK21. G. Carlier, K. Eichinger, and A. Kroshnin, Entropic–
Wasserstein barycenters: PDE characterization, regularity, and CLT,
SIAM J. Math. Anal. 53 (2021), 5880–5914.
dC92. M.P.a.doCarmo,Riemanniangeometry,Mathematics:Theory&
Applications,Birkha¨userBoston,Inc.,Boston,MA,1992,Translated
from the second Portuguese edition by Francis Flaherty.
CSB+18. E. Cazelles,V. Seguy,J. Bigot,M. Cuturi,andN. Papadakis,
Geodesic PCA versus log-PCA of histograms in the Wasserstein
space, SIAM J. Sci. Comput. 40 (2018), B429–B456.
CRW23. F. Chen, Z. Ren, and S. Wang, Uniform-in-time propagation of
chaos for mean field Langevin dynamics, arXiv preprint 2212.03050
(2023), 1–66.262 References
CRBD18. R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Du-
venaud, Neural ordinary differential equations, Advances in Neural
Information Processing Systems 31 (2018), 13.
CLTW24. S. Chen, Q. Li, O. Tse, and S. J. Wright, Accelerating opti-
mization over the space of probability measures, arXiv preprint
2310.04006 (2024), 1–40.
CLM23. Y. Chen, Z. Lin, and H.-G. Mu¨ller, Wasserstein regression, Jour-
nal of the American Statistical Association 118 (2023), 869–882.
CCSW22. Y.Chen,S.Chewi,A.Salim,andA.Wibisono,Improvedanalysis
foraproximalalgorithmforsampling,inProceedings of Thirty Fifth
ConferenceonLearningTheory (P.-L.LohandM.Raginsky,eds.),
Proceedings of Machine Learning Research 178, PMLR, 7 2022,
pp. 2984–3014.
CGP21. Y. Chen, T. T. Georgiou, and M. Pavon, Stochastic control
liaisons: Richard Sinkhorn meets Gaspard Monge on a Schr¨odinger
bridge, SIAM Review 63 (2021), 249–313.
CGT19. Y.Chen,T.T.Georgiou,andA.Tannenbaum,Optimaltransport
for Gaussian mixture models, IEEE Access 7 (2019), 6269–6278.
CB18. X. Cheng and P. Bartlett, Convergence of Langevin MCMC
in KL-divergence, in Proceedings of Algorithmic Learning Theory
(F. Janoos, M. Mohri, and K. Sridharan, eds.), Proceedings of
Machine Learning Research 83, PMLR, 4 2018, pp. 186–211.
CGHH17. V. Chernozhukov, A. Galichon, M. Hallin, and M. Henry,
Monge–Kantorovich depth, quantiles, ranks and signs, Ann. Statist.
45 (2017), 223–256.
Che23. S. Chewi, An optimization perspective on log-concave sampling and
beyond, Ph.D. thesis, MIT, 2023.
Che24. S.Chewi,Log-concavesampling,Forthcoming,2024,Availableonline
at https://chewisinho.github.io/.
CLGL+20a. S. Chewi, T. Le Gouic, C. Lu, T. Maunu, and P. Rigollet,
SVGD as a kernelized Wasserstein gradient flow of the chi-squared
divergence, in Advances in Neural Information Processing Systems
(H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin, eds.), 33, Curran Associates, Inc., 2020, pp. 2098–2109.
CLGL+20b. S. Chewi, T. Le Gouic, C. Lu, T. Maunu, P. Rigollet, and
A. J. Stromme, Exponential ergodicity of mirror-Langevin dif-
fusions, in Advances in Neural Information Processing Systems
(H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, eds.), 33, Curran Associates, Inc., 2020, pp. 19573–19585.
CMRS20. S. Chewi, T. Maunu, P. Rigollet, and A. Stromme, Gradient
descentalgorithmsforBures–Wassersteinbarycenters,inProceedings
of Thirty Third Conference on Learning Theory (J. Abernethyand
S. Agarwal, eds.), Proceedings of Machine Learning Research 125,
PMLR, 7 2020, pp. 1276–1304.
CP23. S. Chewi and A.-A. Pooladian, An entropic generalization of
Caffarelli’s contraction theorem via covariance inequalities, Reports.
Mathematical 361 (2023), 1471–1482.
Chi23. L. Chizat, Doubly regularized entropic Wasserstein barycenters,
arXiv preprint 2303.11844 (2023), 1–27.References 263
CB18. L. Chizat and F. Bach, On the global convergence of gradient
descent for over-parameterized models using optimal transport, in
Advances in Neural Information Processing Systems (S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, eds.), 31, Curran Associates, Inc., 2018.
CDV24. L. Chizat, A. Delalande, and T. Vaˇskevicˇius, Sharper exponen-
tialconvergenceratesforSinkhorn’salgorithmincontinuoussettings,
arXiv preprint 2407.01202 (2024), 1–36.
CPSV18. L. Chizat, G. Peyre´, B. Schmitzer, and F.-X. Vialard, An
interpolating distance between optimal transport and Fisher–Rao
metrics, Found. Comput. Math. 18 (2018), 1–44.
CRL+20. L. Chizat,P. Roussillon,F. Le´ger,F. Vialard,andG. Peyre´,
Faster Wasserstein distance estimation with the Sinkhorn diver-
gence, in Advances in Neural Information Processing Systems 33
(H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, eds.), 2020.
Cho12. O. Chodosh, Optimal transport and Ricci curvature: Wasserstein
space over the interval, 2012, Cambridge Part III essay. Available at
arXiv:1105.2883.
CLZ20. S.-N. Chow, W. Li, and H. Zhou, Wasserstein Hamiltonian flows,
J. Differential Equations 268 (2020), 1205–1219.
CHD24. P. Clavier, T. Huix, and A. Durmus, VITS: variational infer-
ence Thompson sampling for contextual bandits, arXiv preprint
2307.10167 (2024), 1–43.
CK07. H. Cohn and A. Kumar, Universally optimal distribution of points
onspheres,JournaloftheAmericanMathematicalSociety 20(2007),
99–148.
CE02. D. Cordero-Erausquin, Some applications of mass transport to
Gaussian-type inequalities, Arch. Ration. Mech. Anal. 161 (2002),
257–269.
CFTC16. N. Courty, R. Flamary, D. Tuia, and T. Corpetti, Optimal
transport for data fusion in remote sensing, in 2016 IEEE Interna-
tional Geoscience and Remote Sensing Symposium (IGARSS), 2016,
pp. 3571–3574.
CFTR17. N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy,
Optimal transport for domain adaptation, IEEE Transactions on
Pattern Analysis and Machine Intelligence 39 (2017), 1853–1865.
CRMB24. C. Criscitiello, Q. Rebjock, A. D. McRae, and N. Boumal,
Synchronization on circles and spheres with nonlinear interactions,
2024.
CTZ24. T. Cui, X. Tong, and O. Zahm, Optimal Riemannian metric for
Poincar´e inequalities and how to ideally precondition Langevin dy-
manics, arXiv preprint 2404.02554 (2024), 1–28.
Cut13. M. Cuturi, Sinkhorn distances: lightspeed computation of optimal
transport,inAdvances in Neural Information Processing Systems 26
(C. J. C. Burges,L. Bottou,M. Welling,Z. Ghahramani,and
K. Q. Weinberger, eds.), Curran Associates, Inc., 2013, pp. 2292–
2300.264 References
CD14. M. Cuturi and A. Doucet, Fast computation of Wasserstein
barycenters, in Proceedings of the 31st International Conference
on Machine Learning (E. P. Xing and T. Jebara, eds.), 32 Pro-
ceedings of Machine Learning Research no. 2, PMLR, Bejing, China,
6 2014, pp. 685–693.
Dal17. A. S. Dalalyan, Theoretical guarantees for approximate sampling
from smooth and log-concave densities, Journal of the Royal Statis-
tical Society. Series B (Statistical Methodology) 79 (2017), 651–676.
DT12. A. S. Dalalyan and A. B. Tsybakov, Sparse regression learning
by aggregation and Langevin Monte-Carlo, J. Comput. System Sci.
78 (2012), 1423–1443.
DN23. A. Das and D. Nagaraj, Provably fast finite particle variants of
SVGD via virtual particle stochastic approximation, in Advances
in Neural Information Processing Systems (A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds.), 36,
Curran Associates, Inc., 2023, pp. 49748–49760.
DBTHD21. V. De Bortoli, J. Thornton, J. Heng, and A. Doucet, Diffu-
sion Schro¨dinger bridge with applications to score-based generative
modeling, in Advances in Neural Information Processing Systems
(M. Ranzato,A. Beygelzimer,Y. Dauphin,P. Liang,andJ. W.
Vaughan,eds.),34,CurranAssociates,Inc.,2021,pp.17695–17709.
DGS21. N. Deb, P. Ghosal, and B. Sen, Rates of estimation of optimal
transport maps using plug-in estimators via barycentric projections,
in Advances in Neural Information Processing Systems 34 (M. Ran-
zato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W.
Vaughan, eds.), 2021, pp. 29736–29753.
DS23. N. Deb and B. Sen, Multivariate rank-based distribution-free non-
parametric testing using measure transportation, J. Amer. Statist.
Assoc. 118 (2023), 192–207.
DD20. J. Delon and A. Desolneux, A Wasserstein-type distance in the
space of Gaussian mixture models, SIAM J. Imaging Sci. 13 (2020),
936–970.
DHS+19. I.Deshpande,Y.Hu,R.Sun,A.Pyrros,N.Siddiqui,S.Koyejo,
Z. Zhao, D. A. Forsyth, and A. G. Schwing, Max-sliced Wasser-
stein distance and its use for GANs, in IEEE Conference on Com-
puter Vision and Pattern Recognition, Computer Vision Foundation
/ IEEE, 2019, pp. 10648–10656.
DBCS23. M. Z. Diao, K. Balasubramanian, S. Chewi, and A. Salim,
Forward-backward Gaussian variational inference via JKO in the
Bures–Wasserstein space, in Proceedings of the 40th International
Conference on Machine Learning (A. Krause, E. Brunskill,
K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, eds.),
Proceedings of Machine Learning Research 202, PMLR, 7 2023,
pp. 7960–7991.
Div22. V. Divol, Measure estimation on manifolds: an optimal transport
approach, Probab. Theory Related Fields 183 (2022), 581–647.
DNWP22. V. Divol, J. Niles-Weed, and A.-A. Pooladian, Optimal trans-
port map estimation in general function spaces, arXiv preprint
2212.03722 (2022), 1–68.References 265
DEP23. C. Domingo-Enrich and A.-A. Pooladian, An explicit expansion
of the Kullback–Leibler divergence along its Fisher–Rao gradient
flow, Transactions on Machine Learning Research (2023), 1–16.
Dom20. J.Domke,Provablesmoothnessguaranteesforblack-boxvariational
inference, in Proceedings of the 37th International Conference on
Machine Learning (H. D. III and A. Singh, eds.), Proceedings of
Machine Learning Research 119, PMLR, 7 2020, pp. 2587–2596.
DGG23. J. Domke, R. Gower, and G. Garrigos, Provable convergence
guaranteesforblack-boxvariationalinference,inAdvances in Neural
Information Processing Systems (A. Oh, T. Neumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine, eds.), 36, Curran
Associates, Inc., 2023, pp. 66289–66327.
DM19. P. Dubey and H.-G. Mu¨ller, Fr´echet analysis of variance for
random objects, Biometrika 106 (2019), 803–821.
DM20. P. Dubey and H.-G. Mu¨ller, Fr´echet change-point detection, The
Annals of Statistics 48 (2020), 3312–3335.
Dud67. R. M. Dudley, The sizes of compact subsets of Hilbert space and
continuity of Gaussian processes, J. Functional Analysis 1 (1967),
290–330.
Dud69. R. M. Dudley, The speed of mean Glivenko–Cantelli convergence,
The Annals of Mathematical Statistics 40 (1969), 40–50.
Dud99. R.M.Dudley,Uniformcentrallimittheorems,CambridgeStudiesin
Advanced Mathematics 63, Cambridge University Press, Cambridge,
1999.
Dud02. R. M. Dudley, Real analysis and probability, Cambridge Studies in
Advanced Mathematics 74, Cambridge University Press, Cambridge,
2002, Revised reprint of the 1989 original.
DNS23. A. Duncan, N. Nuesken, and L. Szpruch, On the geometry of
Stein variational gradient descent, Journal of Machine Learning
Research 24 (2023), 1–39.
DE97. P. Dupuis and R. S. Ellis, A weak convergence approach to the
theory of large deviations, Wiley Series in Probability and Statistics:
Probability and Statistics, John Wiley & Sons Inc., New York, 1997,
A Wiley-Interscience Publication.
DMM19. A. Durmus, S. Majewski, and B. Miasojedow, Analysis of
Langevin Monte Carlo via convex optimization, J. Mach. Learn.
Res. 20 (2019), Paper No. 73, 46.
DM17. A. Durmus and E. Moulines, Nonasymptotic convergence analysis
fortheunadjustedLangevinalgorithm,Ann.Appl.Probab.27(2017),
1551–1587.
DGK18. P. Dvurechensky, A. Gasnikov, and A. Kroshnin, Computa-
tional optimal transport: complexity by accelerated gradient descent
isbetterthanbySinkhorn’salgorithm,inProceedings of the 35th In-
ternationalConferenceonMachineLearning (J.DyandA.Krause,
eds.), Proceedings of Machine Learning Research 80, PMLR, Stock-
holmsma¨ssan, Stockholm Sweden, 7 2018, pp. 1367–1376.
EHL18. W. E, J. Han, and Q. Li, A mean-field optimal control formulation
of deep learning, Research in the Mathematical Sciences 6 (2018),
1–41.266 References
AEdBCAM16.
P.C.A´lvarezEsteban,E.delBarrio,J.A.Cuesta-Albertos,
and C. Matra´n, A fixed-point approach to barycenters in Wasser-
stein space, J. Math. Anal. Appl. 441 (2016), 744–762.
Fan91. J. Fan, On the optimal rates of convergence for nonparametric
deconvolution problems, The Annals of Statistics 19 (1991), 1257–
1272.
FYC23. J. Fan, B. Yuan, and Y. Chen, Improved dimension dependence
of a proximal algorithm for sampling, in Proceedings of Thirty Sixth
Conference on Learning Theory (G. Neu and L. Rosasco, eds.),
Proceedings of Machine Learning Research 195, PMLR, 7 2023,
pp. 1473–1521.
FGP20. M. Fathi, N. Gozlan, and M. Prod’homme, A proof of the Caf-
farelli contraction theorem via entropic regularization, Calculus of
Variations and Partial Differential Equations 59 (2020), 96.
FSV+19. J. Feydy, T. Se´journe´, F.-X. Vialard, S.-i. Amari, A. Trouve,
and G. Peyre´, Interpolating between optimal transport and MMD
using Sinkhorn divergences, in Proceedings of the Twenty-Second
International Conference on Artificial Intelligence and Statistics
(K. Chaudhuri and M. Sugiyama, eds.), Proceedings of Machine
Learning Research 89, PMLR, 4 2019, pp. 2681–2690.
FG23. A. Figalli and F. Glaudo, An invitation to optimal transport,
Wassersteindistances,andgradientflows,seconded.,EMSTextbooks
in Mathematics, EMS Press, Berlin, 2023.
FCCR18. R. Flamary, M. Cuturi, N. Courty, and A. Rakotomamonjy,
Wasserstein discriminant analysis, Machine Learning 107 (2018),
1923–1945.
FLF20. R. Flamary, K. Lounici, and A. Ferrari, Concentration bounds
for linear Monge mapping estimation and optimal transport domain
adaptation, 2020.
FG15. N.FournierandA.Guillin,OntherateofconvergenceinWasser-
stein distance of the empirical measure, Probab. Theory Related
Fields 162 (2015), 707–738.
GFPO21. T. Galy-Fajou,V. Perrone,andM. Opper,Flexibleandefficient
inference with particles for the variational Gaussian approximation,
Entropy 23 (2021), Paper No. 990, 34.
GM96. W. Gangbo and R. J. McCann, The geometry of optimal trans-
portation, Acta Math. 177 (1996), 113–161.
vdG87. S. van de Geer, A new approach to least-squares estimation, with
applications, Ann. Statist. 15 (1987), 587–602.
vdG02. S. van de Geer, M-estimation using penalties or sieves, J. Statist.
Plann. Inference 108 (2002), 55–69, C. R. Rao 80th birthday felici-
tation volume, Part II.
GCB+19. A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyre´,
Sample complexity of Sinkhorn divergences, in The 22nd Interna-
tional Conference on Artificial Intelligence and Statistics, PMLR,
2019, pp. 1574–1583.
GPC18. A. Genevay, G. Peyre´, and M. Cuturi, Learning generative
models with Sinkhorn divergences, in International Conference on
Artificial Intelligence and Statistics, 2018, pp. 1608–1617.References 267
GLRT20. I. Gentil, C. Le´onard, L. Ripani, and L. Tamanini, An entropic
interpolation proof of the HWI inequality, Stoch. Process. Appl. 130
(2020), 907–923.
Ger24. P. R. Gerber,Likelihood-free hypothesis testing and applications of
the energy distance, Ph.D. thesis, MIT, 2024.
GLPR23. B. Geshkovski, C. Letrouit, Y. Polyanskiy, and P. Rigollet,
The emergence of clusters in self-attention dynamics, in Thirty-
seventh Conference on Neural Information Processing Systems, 2023.
GLPR24. B. Geshkovski, C. Letrouit, Y. Polyanskiy, and P. Rigollet,
A mathematical perspective on transformers, 2024.
GP22. L. Ghodrati and V. M. Panaretos, Distribution-on-distribution
regression via optimal transport maps, Biometrika 109 (2022), 957–
974.
GN22. P. Ghosal and M. Nutz, On the convergence rate of Sinkhorn’s
algorithm, arXiv preprint 2212.06000 (2022), 1–28.
GS22. P. Ghosal and B. Sen, Multivariate ranks and quantiles using
optimal transport: consistency, rates and nonparametric testing,
Ann. Statist. 50 (2022), 1012–1037.
GLNZ22. S. Ghosh, Y. Lu, T. Nowicki, and E. Zhang, On representations
ofmean-fieldvariationalinference,arXivpreprint2210.11385 (2022),
1–19.
GN16. E. Gine´ and R. Nickl, Mathematical foundations of infinite-
dimensional statistical models, Cambridge: Cambridge University
Press, 2016 (English).
GGNWP20. Z. Goldfeld, K. Greenewald, J. Niles-Weed, and Y. Polyan-
skiy,Convergenceofsmoothedempiricalmeasureswithapplications
toentropyestimation,IEEETrans.Inform.Theory 66(2020),4368–
4391.
GKRS24. Z. Goldfeld, K. Kato, G. Rioux, and R. Sadhu, Statistical
inferencewithregularizedoptimaltransport,Inf.Inference13(2024),
Paper No. 13, 68.
GDGSCN23. J. Gonza´lez-Delgado, A. Gonza´lez-Sanz, J. Corte´s, and
P.Neuvial,Two-samplegoodness-of-fittestsontheflattorusbased
on Wasserstein distance and their relevance to structural biology,
Electron. J. Stat. 17 (2023), 1547–1586.
GSLNW24. A. Gonzalez-Sanz, J.-M. Loubes, and J. Niles-Weed, Weak
limitsofentropyregularizedoptimaltransport;potentials,plansand
divergences, arXiv preprint 2207.07427 (2024), 1–24.
GLL+23. S. Gopi, Y. T. Lee, D. Liu, R. Shen, and K. Tian, Algorithmic
aspects of the log-Laplace transform and a non-Euclidean proximal
sampler, in Proceedings of Thirty Sixth Conference on Learning
Theory (G. Neu and L. Rosasco, eds.), Proceedings of Machine
Learning Research 195, PMLR, 7 2023, pp. 2399–2439.
GPC15. A. Gramfort, G. Peyre´, and M. Cuturi, Fast optimal transport
averagingofneuroimagingdata,inInformationProcessinginMedical
Imaging (S. Ourselin, D. C. Alexander, C.-F. Westin, and
M. J. Cardoso, eds.), Springer International Publishing, Cham,
2015, pp. 261–272.268 References
GNCD23. G. Greco, M. Noble, G. Conforti, and A. Durmus, Non-
asymptoticconvergenceboundsforSinkhorniteratesandtheirgradi-
ents:acouplingapproach.,inProceedings of Thirty Sixth Conference
on Learning Theory (G. Neu and L. Rosasco, eds.), Proceedings of
Machine Learning Research 195, PMLR, 7 2023, pp. 716–746.
GH23. M. Groppe and S. Hundrieser, Lower complexity adaptation for
empirical entropic optimal transport, arXiv preprint 2306.13580
(2023), 1–51.
Hal17. M. Hallin, On distribution and quantile functions, ranks and signs
inRd,WorkingPapersECARESECARES2017-34,ULB–Universite
Libre de Bruxelles, 9 2017.
Hal22. M. Hallin, Measure transportation and statistical decision theory,
Annu. Rev. Stat. Appl. 9 (2022), 401–424.
HdBCAM21. M.Hallin,E.delBarrio,J.Cuesta-Albertos,andC.Matra´n,
Distribution and quantile functions, ranks and signs in dimension d:
a measure transportation approach, Ann. Statist. 49 (2021), 1139–
1165.
HMS21. M. Hallin,G. Mordant,andJ. Segers,Multivariategoodness-of-
fit tests based on Wasserstein distance, Electron. J. Stat. 15 (2021),
1328–1371.
HMJG21. A. Han, B. Mishra, P. K. Jawanpuria, and J. Gao, On Rieman-
nian optimization over positive definite matrices with the Bures–
Wassersteingeometry,inAdvances in Neural Information Processing
Systems (M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang,
and J. W. Vaughan, eds.), 34, Curran Associates, Inc., 2021,
pp. 8940–8953.
vH14. R. van Handel, Probability in high dimension, Lecture Notes
(Princeton University), 2014.
HMHBE24. Y. He, A. Mousavi-Hosseini, K. Balasubramanian, and M. A.
Erdogdu,Aseparationinheavy-tailedsampling:Gaussianvs.stable
oracles for proximal samplers, arXiv preprint 2405.16736 (2024),
1–33.
Hoe48. W. Hoeffding, A class of statistics with asymptotically normal
distribution, Ann. Math. Statist. 19 (1948), 293–325.
HSM24. S. Hundrieser, T. Staudt, and A. Munk, Empirical optimal
transport between different measures adapts to lower complexity,
Ann. Inst. Henri Poincar´e Probab. Stat. 60 (2024), 824–846.
HR21. J.-C. Hu¨tter and P. Rigollet, Minimax estimation of smooth
optimal transport maps, Ann. Statist. 49 (2021), 1166–1194.
JGH18. A. Jacot, F. Gabriel, and C. Hongler, Neural tangent kernel:
convergence and generalization in neural networks, in Advances in
Neural Information Processing Systems (S. Bengio, H. Wallach,
H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,
eds.), 31, Curran Associates, Inc., 2018.
JMPC20. H. Janati, B. Muzellec, G. Peyre´, and M. Cuturi, Entropic
optimal transport between unbalanced Gaussian measures has a
closed form, in Advances in Neural Information Processing Systems
(H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and
H. Lin, eds.), 33, Curran Associates, Inc., 2020, pp. 10468–10479.References 269
JCP24. Y. Jiang, S. Chewi, and A.-A. Pooladian, Algorithms for mean-
field variational inference via polyhedral optimization in the Wasser-
steinspace,inProceedingsofThirtySeventhConferenceonLearning
Theory (S. Agrawal and A. Roth, eds.), Proceedings of Machine
Learning Research 247, PMLR, 7 2024, pp. 2720–2721.
JGJS99. M. I. Jordan,Z. Ghahramani,T. S. Jaakkola,andL. K. Saul,
An introduction to variational methods for graphical models, Mach.
Learn. 37 (1999), 183–233.
JKO98. R. Jordan, D. Kinderlehrer, and F. Otto, The variational
formulation of the Fokker–Planck equation, SIAM J. Math. Anal.
29 (1998), 1–17.
KLRS08. B. Kalantari, I. Lari, F. Ricca, and B. Simeone, On the com-
plexity of general matrix scaling and entropy minimization via the
RAS algorithm, Math. Program. 112 (2008), 371–401.
Kan42. L. V. Kantorovich, On the translocation of masses, Dokl. Akad.
Nauk SSSR 37 (1942), 227–229.
KNS16. H. Karimi, J. Nutini, and M. Schmidt, Linear convergence of gra-
dient and proximal-gradient methods under the Polyak–L(cid:32)ojasiewicz
condition, in Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, Springer, 2016, pp. 795–811.
KR24. A. KatsevichandP. Rigollet,Ontheapproximationaccuracyof
Gaussian variational inference, Ann. Statist. (to appear) (2024), 49.
KOW+23. K. Kim, J. Oh, K. Wu, Y. Ma, and J. Gardner, On the con-
vergence of black-box variational inference, in Advances in Neural
Information Processing Systems (A. Oh, T. Neumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine, eds.), 36, Curran
Associates, Inc., 2023, pp. 44615–44657.
KTM20. M. Klatt, C. Tameling, and A. Munk, Empirical regularized
optimaltransport:statisticaltheoryandapplications,SIAM Journal
on Mathematics of Data Science 2 (2020), 419–443.
KS94. M. Knott and C. S. Smith, On a generalization of cyclic mono-
tonicity and distances among random vectors, Linear Algebra Appl.
199 (1994), 363–371.
Kol34. A. N. Kolmogorov, Zuf¨allige Bewegungen (zur Theorie der Bro-
wnschen Bewegung), Ann. of Math. (2) 35 (1934), 116–117.
KNS+19. S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. K.
Rohde, Generalized sliced Wasserstein distances, in Advances
in Neural Information Processing Systems 32 (H. M. Wallach,
H. Larochelle, A. Beygelzimer, F. d’Alche´-Buc, E. B. Fox,
and R. Garnett, eds.), 2019, pp. 261–272.
KR15. S. Kolouri and G. K. Rohde, Transport-based single frame super
resolution of very low resolution face images, in Proceedings of the
IEEEConferenceonComputerVisionandPatternRecognition,2015,
pp. 4876–4884.
KTOR16. S. Kolouri, A. B. Tosun, J. A. Ozolek, and G. K. Rohde, A
continuous linear optimal transport approach for pattern analysis in
image datasets, Pattern Recognition 51 (2016), 453–462.
KMV16. S. Kondratyev, L. Monsaingeon, and D. Vorotnikov, A new
optimal transport distance on the space of finite Radon measures,
Advances in Differential Equations 21 (2016), 1117 – 1164.270 References
KVZ24. Y.Kook,S.S.Vempala,andM.S.Zhang,In-and-out:algorithmic
diffusion for sampling convex bodies, arXiv preprint 2405.01425
(2024), 1–32.
KZC+24. Y. Kook,M. S. Zhang,S. Chewi,M. A. Erdogdu,andM. B. Li,
Sampling from the mean-field stationary distribution, in Proceedings
of Thirty Seventh Conference on Learning Theory (S. Agrawal
andA. Roth,eds.),Proceedings of Machine Learning Research 247,
PMLR, 7 2024, pp. 3099–3136.
KSA+20. A. Korba, A. Salim, M. Arbel, G. Luise, and A. Gretton, A
non-asymptoticanalysisforSteinvariationalgradientdescent,inAd-
vances in Neural Information Processing Systems (H. Larochelle,
M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds.), 33,
Curran Associates, Inc., 2020, pp. 4672–4682.
Kro18. A.Kroshnin,Fr´echetbarycentersintheMonge–Kantorovichspaces,
J. Convex Anal. 25 (2018), 1371–1395.
KSS21. A. Kroshnin, V. Spokoiny, and A. Suvorikova, Statistical in-
ference for Bures–Wasserstein barycenters, Ann. Appl. Probab. 31
(2021), 1264–1298.
KDLY22. S. Kum, M. H. Duong, Y. Lim, and S. Yun, A GPM-based al-
gorithm for solving regularized Wasserstein barycenter problems in
some spaces of probability measures, Journal of Computational and
Applied Mathematics 416 (2022), 114588.
Lac23. D. Lacker, Independent projections of diffusions: gradient flows for
variational inference and optimal mean field approximations, arXiv
preprint 2309.13332 (2023), 1–27.
Laf88. J. D. Lafferty, The density manifold and configuration space
quantization, Trans. Amer. Math. Soc. 305 (1988), 699–741.
LBB24. M.Lambert,F.Bach,andS.Bonnabel,Variationaldynamicpro-
gramming for stochastic optimal control, arXiv preprint 2404.14806
(2024), 1–17.
LBB23. M. Lambert, S. Bonnabel, and F. Bach, Variational Gaussian
approximationoftheKushneroptimalfilter,inGeometric science of
information. Part I, Lecture Notes in Comput. Sci. 14071, Springer,
Cham, [2023] ©2023, pp. 395–404.
LCB+22. M.Lambert,S.Chewi,F.Bach,S.Bonnabel,andP.Rigollet,
Variational inference via Wasserstein gradient flows, in Advances in
Neural Information Processing Systems (A. H. Oh, A. Agarwal,
D. Belgrave, and K. Cho, eds.), 2022.
LZ24. H. Lavenant and G. Zanella, Convergence rate of random scan
coordinate ascent variational inference under log-concavity, arXiv
preprint 2406.07292 (2024), 1–12.
Le 16. J.-F. Le Gall, Brownian motion, martingales, and stochastic cal-
culus, French ed., Graduate Texts in Mathematics 274, Springer,
[Cham], 2016.
LGL17. T. Le Gouic and J.-M. Loubes, Existence and consistency of
Wasserstein barycenters, Probab. Theory Related Fields 168 (2017),
901–917.
LGLR20. T. Le Gouic, J.-M. Loubes, and P. Rigollet, Projection to
fairness in statistical learning, arXiv preprint 2005.11720 (2020),
1–14.References 271
LGPRS22. T. Le Gouic, Q. Paris, P. Rigollet, and A. J. Stromme, Fast
convergence of empirical barycenters in Alexandrov spaces and the
Wasserstein space, J. Eur. Math. Soc. (2022), 2229–2250.
Led17. M.Ledoux,OnoptimalmatchingofGaussiansamples,Zap.Nauchn.
Sem. S.-Peterburg. Otdel. Mat. Inst. Steklov. (POMI) 457 (2017),
226–264.
LT91. M. Ledoux and M. Talagrand, Probability in Banach spaces,
Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in
Mathematics and Related Areas (3)] 23, Springer-Verlag, Berlin,
1991, Isoperimetry and processes.
LZ21. M. Ledoux and J.-X. Zhu, On optimal matching of Gaussian
samples III, Probab. Math. Statist. 41 (2021), 237–265.
LST21. Y. T. Lee, R. Shen, and K. Tian, Structured logconcave sampling
with a restricted Gaussian oracle, in Proceedings of Thirty Fourth
Conference on Learning Theory (M. BelkinandS. Kpotufe,eds.),
Proceedings of Machine Learning Research 134, PMLR, 8 2021,
pp. 2993–3050.
L´eg21. F. Le´ger, A gradient descent perspective on Sinkhorn, Appl. Math.
Optim. 84 (2021), 1843–1855.
LR05. E. L. Lehmann and J. P. Romano, Testing statistical hypotheses,
third ed., Springer Texts in Statistics, Springer, New York, 2005.
L´eo14. C. Le´onard, A survey of the Schro¨dinger problem and some of its
connections with optimal transport, Discrete Contin. Dyn. Syst. 34
(2014), 1533–1574.
LGYS20. L. Li, A. Genevay, M. Yurochkin, and J. M. Solomon, Con-
tinuous regularized Wasserstein barycenters, in Advances in Neural
Information Processing Systems (H. Larochelle, M. Ranzato,
R. Hadsell,M. Balcan,andH. Lin,eds.),33,CurranAssociates,
Inc., 2020, pp. 17755–17765.
Lia21. T. Liang, How well generative adversarial networks learn distribu-
tions, J. Mach. Learn. Res. 22 (2021), Paper No. 228, 41.
LMS16. M.Liero,A.Mielke,andG.Savare´,Optimaltransportincompe-
titionwithreaction:theHellinger–Kantorovichdistanceandgeodesic
curves, SIAM J. Math. Anal. 48 (2016), 2869–2911.
LMS18. M. Liero, A. Mielke, and G. Savare´, Optimal entropy-transport
problemsandanewHellinger–Kantorovichdistancebetweenpositive
measures, Invent. Math. 211 (2018), 969–1117.
Lin83. B. G. Lindsay, The geometry of mixture likelihoods: a general
theory, The Annals of Statistics 11 (1983), 86–94.
Liu17. Q. Liu, Stein variational gradient descent as gradient flow, in Ad-
vances in Neural Information Processing Systems (I. Guyon, U. V.
Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,
and R. Garnett, eds.), 30, Curran Associates, Inc., 2017.
LW16. Q. Liu and D. Wang, Stein variational gradient descent: a general
purpose Bayesian inference algorithm, in Advances in Neural Infor-
mation Processing Systems (D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett, eds.), 29, Curran Associates, Inc.,
2016.
LV09. J. Lott and C. Villani, Ricci curvature for metric-measure spaces
via optimal transport, Ann. of Math. 169 (2009), 903–991.272 References
LLN19a. J. Lu, Y. Lu, and J. Nolen, Scaling limit of the Stein variational
gradient descent: the mean field regime, SIAM J. Math. Anal. 51
(2019), 648–671.
LLN19b. Y. Lu, J. Lu, and J. Nolen, Accelerating Langevin sampling with
birth-death, arXiv preprint 1905.09863, 2019.
LSW23. Y. Lu, D. Slepcˇev, and L. Wang, Birth-death dynamics for sam-
pling: global convergence, approximations and their asymptotics,
Nonlinearity 36 (2023), 5731–5772.
LGT22. Y. Luo and N. Garc´ıa Trillos, Nonconvex matrix factorization
is geodesically convex: global landscape analysis for fixed-rank ma-
trix optimization from a Riemannian perspective, arXiv preprint
2209.15130 (2022), 1–35.
MGM22. A. Mallasto, A. Gerolin, and H. Q. Minh, Entropy-regularized
2-Wasserstein distance between Gaussian measures, Inf. Geom. 5
(2022), 289–323.
MBNWW21. T. Manole, S. Balakrishnan, J. Niles-Weed, and L. Wasser-
man, Plugin estimation of smooth optimal transport maps, 2021.
MBNWW23. T. Manole, S. Balakrishnan, J. Niles-Weed, and L. Wasser-
man, Central limit theorems for smooth optimal transport maps,
arXiv preprint 2312.12407 (2023), 1–60.
MBW22. T. Manole, S. Balakrishnan, and L. Wasserman, Minimax
confidence intervals for the sliced Wasserstein distance, Electron. J.
Stat. 16 (2022), 2252–2345.
MNW24. T. Manole and J. Niles-Weed, Sharp convergence rates for em-
pirical optimal transport with smooth costs, The Annals of Applied
Probability 34 (2024), 1108–1135.
MLGR23. T. Maunu, T. Le Gouic, and P. Rigollet, Bures–Wasserstein
barycenters and low-rank matrix recovery, in Proceedings of the
26thInternationalConferenceonArtificialIntelligenceandStatistics
(F. Ruiz, J. Dy, and J.-W. van de Meent, eds.), Proceedings of
Machine Learning Research 206, PMLR, 4 2023, pp. 8183–8210.
McC97. R.J.McCann,Aconvexityprincipleforinteractinggases,Advances
in Mathematics 128 (1997), 153–179.
McK66. H. P. McKean, Jr., A class of Markov processes associated with
nonlinearparabolicequations,Proc.Nat.Acad.Sci.U.S.A.56(1966),
1907–1911.
MMN18. S. Mei, A. Montanari, and P.-M. Nguyen, A mean field view of
the landscape of two-layer neural networks, Proc. Natl. Acad. Sci.
USA 115 (2018), E7665–E7671.
MNW19. G. Mena and J. Niles-Weed, Statistical bounds for entropic opti-
mal transport: sample complexity and the central limit theorem, in
Advances in Neural Information Processing Systems (H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox, and
R. Garnett, eds.), 32, Curran Associates, Inc., 2019.
MM23. T. Misiakiewicz and A. Montanari, Six lectures on linearized
neural networks, 2023.
Mod17. K.Modin,Geometryofmatrixdecompositionsseenthroughoptimal
transport and information geometry, J. Geom. Mech. 9 (2017), 335–
390.References 273
Mon81. G. Monge,M´emoiresurlath´eoriedesd´eblaisetdesremblais,M´em.
de l’Ac. R. des Sc. (1781), 666–704.
MFSS17. K. Muandet, K. Fukumizu, B. Sriperumbudur, and
B. Scho¨lkopf, Kernel mean embedding of distributions: a
review and beyond, Foundations and Trends® in Machine Learning
10 (2017), 1–141.
NDC+20. K. Nadjahi, A. Durmus, L. Chizat, S. Kolouri, S. Shahram-
pour, and U. Simsekli, Statistical and topological properties of
sliced probability divergences, in Advances in Neural Information
ProcessingSystems33 (H.Larochelle,M.Ranzato,R.Hadsell,
M. Balcan, and H. Lin, eds.), 2020.
NY83. A. S. Nemirovsky and D. B. Yudin, Problem complexity and
method efficiency in optimization, A Wiley-Interscience Publication,
JohnWiley&SonsInc.,NewYork,1983,TranslatedfromtheRussian
and with a preface by E. R. Dawson, Wiley-Interscience Series in
Discrete Mathematics.
Nes83. Y. E. Nesterov, A method for solving the convex programming
problem with convergence rate O(1/k2), Dokl. Akad. Nauk SSSR
269 (1983), 543–547.
Nes18. Y. Nesterov, Lectures on convex optimization, Springer Optimiza-
tion and Its Applications 137, Springer, Cham, 2018.
NWKB23. M.Neykov,L.Wasserman,I.Kim,andS.Balakrishnan,Nearly
minimax optimal Wasserstein conditional independence testing,
arXiv preprint 2308.08672 (2023), 1–24.
NW18. J. Niles-Weed, Sharper rates for estimating differential entropy
under Gaussian convolutions, Massachusetts Institute of Technology
(MIT), Tech. Rep (2018), 1–2.
NWB19. J. Niles-Weed and F. Bach, Sharp asymptotic and finite-sample
rates of convergence of empirical measures in Wasserstein distance,
Bernoulli 25 (2019), 2620–2648.
NWB22. J. Niles-Weed and Q. Berthet, Minimax estimation of smooth
densitiesinWassersteindistance,Ann.Statist.50(2022),1519–1540.
NWR22. J. Niles-Weed and P. Rigollet, Estimation of Wasserstein dis-
tancesinthespikedtransportmodel,Bernoulli 28(2022),2663–2688.
NW22. M. Nutz and J. Wiesel, Entropic optimal transport: convergence
of potentials, Probab. Theory Related Fields 184 (2022), 401–424.
OT11. S.-i. Ohta and A. Takatsu, Displacement convexity of generalized
relative entropies, Adv. Math. 228 (2011), 1742–1787.
OT13. S.-I. Ohta and A. Takatsu, Displacement convexity of generalized
relative entropies. II, Comm. Anal. Geom. 21 (2013), 687–785.
OI22. R. Okano and M. Imaizumi, Inference for projection-based Wasser-
stein distances on finite spaces, arXiv preprint 2202.05495 (2022),
1–29.
Oll10. Y. Ollivier, A survey of Ricci curvature for metric spaces and
Markov chains, in Probabilistic approach to geometry, Adv. Stud.
Pure Math. 57, Math. Soc. Japan, Tokyo, 2010, pp. 343–381.
Oll13. Y. Ollivier, A visual introduction to Riemannian curvatures and
some discrete generalizations, in Analysis and geometry of metric
measure spaces, CRM Proc. Lecture Notes 56, Amer. Math. Soc.,
Providence, RI, 2013, pp. 197–220.274 References
OV12. Y.OllivierandC.Villani,AcurvedBrunn–Minkowskiinequality
on the discrete hypercube, or: what is the Ricci curvature of the
discrete hypercube?, SIAM J. Discrete Math. 26 (2012), 983–996.
vO22. J. van Oostrum, Bures–Wasserstein geometry for positive-definite
Hermitian matrices and their trace-one subset, Inf. Geom. 5 (2022),
405–425.
Ott01. F.Otto,Thegeometryofdissipativeevolutionequations:theporous
mediumequation.,CommunicationsinPartialDifferentialEquations
26 (2001), 101–174.
OV00. F. Otto and C. Villani, Generalization of an inequality by Tala-
grand and links with the logarithmic Sobolev inequality, J. Funct.
Anal. 173 (2000), 361–400.
OV01. F. Otto and C. Villani, Comment on: “Hypercontractivity of
Hamilton–Jacobi equations” [J. Math. Pures Appl. (9) 80 (2001),
no. 7, 669–696] by S. G. Bobkov, I. Gentil and M. Ledoux, J. Math.
Pures Appl. (9) 80 (2001), 697–700.
PZ16. V. M. Panaretos and Y. Zemel, Amplitude and phase variation
of point processes, Ann. Statist. 44 (2016), 771–812.
PZ20. V. M. Panaretos and Y. Zemel, An invitation to statistics in
Wasserstein space, Springer Nature, 2020.
PS23. S. Park and D. Slepcˇev, Geometry and analytic properties of the
sliced Wasserstein space, arXiv preprint 2311.05134 (2023), 1–49.
PT18. S. Park and M. Thorpe, Representing and learning high dimen-
sional data with the optimal transport map from a probabilistic
viewpoint, in 2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2018, pp. 7864–7872.
PC19a. F. Paty and M. Cuturi, Subspace robust Wasserstein distances,
in Proceedings of the 36th International Conference on Machine
Learning, ICML 2019 (K. Chaudhuri and R. Salakhutdinov,
eds.), Proceedings of Machine Learning Research 97, PMLR, 2019,
pp. 5072–5081.
PC19b. G. Peyre´andM. Cuturi,Computationaloptimaltransport,Foun-
dations and Trends® in Machine Learning 11 (2019), 355–607.
PKD07. F. Pitie´, A. C. Kokaram, and R. Dahyot, Automated colour
grading using colour distribution transfer, Computer Vision and
Image Understanding 107 (2007), 123–137, Special issue on color
image processing.
PW24. Y. Polyanskiy and Y. Wu, Information theory: from coding to
learning, Cambridge University Press, Cambridge, 2024.
Pon23. D.Ponnoprat,UniversalconsistencyofWassersteink-NNclassifier:
a negative and some positive results, Information and Inference: A
Journal of the IMA 12 (2023), 1997–2019.
PCNW22. A.-A. Pooladian, M. Cuturi, and J. Niles-Weed, Debiaser
beware: pitfalls of centering regularized transport maps, in Pro-
ceedings of the 39th International Conference on Machine Learning
(K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu,
and S. Sabato, eds.), Proceedings of Machine Learning Research
162, PMLR, 7 2022, pp. 17830–17847.References 275
PDNW23. A.-A. Pooladian, V. Divol, and J. Niles-Weed, Minimax esti-
mation of discontinuous optimal transport maps: the semi-discrete
case,inProceedingsofthe40thInternationalConferenceonMachine
Learning (A. Krause, E. Brunskill, K. Cho, B. Engelhardt,
S.Sabato,andJ.Scarlett,eds.),ProceedingsofMachineLearning
Research 202, PMLR, 7 2023, pp. 28128–28150.
PNW22. A.-A. Pooladian and J. Niles-Weed, Entropic estimation of
optimal transport maps, arXiv preprint 2109.12004 (2022), 1–37.
PBS24. V. Priser, P. Bianchi, and A. Salim, Long-time asymptotics of
noisySVGDoutsidethepopulationlimit,arXiv preprint 2406.11929
(2024), 1–28.
RPDB12. J. Rabin, G. Peyre´, J. Delon, and M. Bernot, Wasserstein
barycenter and its application to texture mixing, in Scale Space and
VariationalMethodsinComputerVision (A. M. Bruckstein,B. M.
ter Haar Romeny, A. M. Bronstein, and M. M. Bronstein,
eds.), Springer Berlin Heidelberg, Berlin, Heidelberg, 2012, pp. 435–
446.
RR98a. S. T. RachevandL. Ru¨schendorf,Mass transportation problems.
Vol. I, Probability and its Applications (New York), Springer-Verlag,
New York, 1998, Theory.
RR98b. S. T. RachevandL. Ru¨schendorf,Mass transportation problems.
Vol. II,Probability and its Applications (New York),Springer-Verlag,
New York, 1998, Applications.
RH17. P. Rigollet and J.-C. Hu¨tter, High-dimensional statistics, Lec-
ture notes, 2017.
RNW18. P. Rigollet and J. Niles-Weed, Entropic optimal transport is
maximum-likelihood deconvolution, Comptes Rendus Mathematique
356 (2018), 1228–1235.
RNW19. P. Rigollet and J. Niles-Weed, Uncoupled isotonic regression
via minimum Wasserstein deconvolution, Inf. Inference 8 (2019),
691–717.
RS22. P. Rigollet and A. J. Stromme, On the sample complexity of
entropic optimal transport, arXiv preprint 2206.13472 (2022), 1–28.
Roc66. R. T. Rockafellar, Characterization of the subdifferentials of
convex functions, Pacific J. Math. 17 (1966), 497–510.
Roc97. R. T. Rockafellar, Convex analysis, Princeton Landmarks in
Mathematics, Princeton University Press, Princeton, NJ, 1997,
Reprint of the 1970 original, Princeton Paperbacks.
RVE22. G. Rotskoff and E. Vanden-Eijnden, Trainability and accuracy
ofartificialneuralnetworks:aninteractingparticlesystemapproach,
Communications on Pure and Applied Mathematics 75(2022),1889–
1935.
RTG00. Y. Rubner, C. Tomasi, and L. J. Guibas, The earth mover’s
distance as a metric for image retrieval, Int. J. Comput. Vision 40
(2000), 99–121.
RU02. L. Ru¨schendorf and L. Uckelmann, On the n-coupling problem,
J. Multivariate Anal. 81 (2002), 242–258.
SKL20. A. Salim, A. Korba, and G. Luise, The Wasserstein proximal
gradient algorithm, in Advances in Neural Information Processing276 References
Systems (H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,
andH.Lin,eds.),33,CurranAssociates,Inc.,2020,pp.12356–12366.
SSR22. A. Salim, L. Sun, and P. Richtarik, A convergence theory for
SVGD in the population limit under Talagrand’s inequality T1,
in Proceedings of the 39th International Conference on Machine
Learning (K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,
G. Niu, and S. Sabato, eds.), Proceedings of Machine Learning
Research 162, PMLR, 7 2022, pp. 19139–19152.
SABP22. M.E.Sander,P.Ablin,M.Blondel,andG.Peyre´,Sinkformers:
transformers with doubly stochastic attention, in Proceedings of the
25thInternationalConferenceonArtificialIntelligenceandStatistics
(G. Camps-Valls,F. J. R. Ruiz,andI. Valera,eds.),Proceedings
of Machine Learning Research 151, PMLR, 3 2022, pp. 3515–3530.
San15. F. Santambrogio, Optimal transport for applied mathematicians,
Progress in Nonlinear Differential Equations and their Applications
87, Birkha¨user/Springer, Cham, 2015, Calculus of variations, PDEs,
and modeling.
San17. F. Santambrogio, {Euclidean, metric, and Wasserstein} gradient
flows: an overview, Bull. Math. Sci. 7 (2017), 87–154.
SST+19. G. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subra-
manian, A. Solomon, J. Gould, S. Liu, S. Lin, P. Berube,
L.Lee,J.Chen,J.Brumbaugh,P.Rigollet,K.Hochedlinger,
R.Jaenisch,A.Regev,andE.S.Lander,Optimal-transportanal-
ysisofsingle-cellgeneexpressionidentifiesdevelopmentaltrajectories
in reprogramming, Cell 176 (2019), 928–943.
SC15. V.SeguyandM.Cuturi,Principalgeodesicanalysisforprobability
measuresundertheoptimaltransportmetric,inAdvances in Neural
Information Processing Systems 28, 2015, pp. 3312–3320.
SM23. J. Shi and L. Mackey, A finite-particle convergence rate for Stein
variationalgradientdescent,inAdvances in Neural Information Pro-
cessing Systems (A. Oh,T. Naumann,A. Globerson,K. Saenko,
M. Hardt, and S. Levine, eds.), 36, Curran Associates, Inc., 2023,
pp. 26831–26844.
SP18. S.SinghandB.Po´czos,MinimaxdistributionestimationinWasser-
stein distance, arXiv preprint 1802.08855 (2018), 1–34.
SUL+18. S. Singh, A. Uppal, B. Li, C.-L. Li, M. Zaheer, and B. Poc-
zos, Nonparametric density estimation under adversarial losses, in
Advances in Neural Information Processing Systems (S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, eds.), 31, Curran Associates, Inc., 2018.
SS20. J. Sirignano and K. Spiliopoulos, Mean field analysis of neu-
ral networks: a law of large numbers, SIAM Journal on Applied
Mathematics 80 (2020), 725–752.
SdGP+15. J. Solomon, F. de Goes, G. Peyre´, M. Cuturi, A. Butscher,
A. Nguyen, T. Du, and L. Guibas, Convolutional Wasserstein
distances: efficient optimal transportation on geometric domains,
ACM Trans. Graph. 34 (2015), 66:1–66:11.
SLD18. S.Srivastava,C.Li,andD.B.Dunson,ScalableBayesviabarycen-
ter in Wasserstein space, Journal of Machine Learning Research 19
(2018), 1–35.References 277
Ste01. J. M. Steele, Stochastic calculus and financial applications, Appli-
cations of Mathematics (New York) 45, Springer-Verlag, New York,
2001.
Str23. A. J. Stromme, Minimum intrinsic dimension scaling for entropic
optimal transport, arXiv preprint 2306.03398 (2023), 1–53.
Stu03. K.-T. Sturm, Probability measures on metric spaces of nonpositive
curvature, in Heat kernels and analysis on manifolds, graphs, and
metricspaces(Paris,2002),Contemp.Math.338,Amer.Math.Soc.,
Providence, RI, 2003, pp. 357–390.
Stu06a. K.-T. Sturm, On the geometry of metric measure spaces. I, Acta
Math. 196 (2006), 65–131.
Stu06b. K.-T. Sturm, On the geometry of metric measure spaces. II, Acta
Math. 196 (2006), 133–177.
SNW23. T.Suzuki,A.Nitanda,andD.Wu,Uniform-in-timepropagationof
chaosforthemean-fieldgradientLangevindynamics,inTheEleventh
International Conference on Learning Representations, 2023.
Szn91. A.-S. Sznitman, Topics in propagation of chaos, in E´cole d’E´t´e de
Probabilit´esdeSaint-FlourXIX—1989,LectureNotesinMath.1464,
Springer, Berlin, 1991, pp. 165–251.
Tal96. M. Talagrand, Majorizing measures: the generic chaining, Ann.
Probab. 24 (1996), 1049–1103.
Tal21. M. Talagrand, Upper and lower bounds for stochastic processes—
decomposition theorems, second ed., Ergebnisse der Mathematik und
ihrer Grenzgebiete. 3. Folge. A Series of Modern Surveys in Math-
ematics [Results in Mathematics and Related Areas. 3rd Series. A
SeriesofModernSurveysinMathematics] 60,Springer,Cham,[2021]
©2021.
Tan17. Y. S. Tan, Energy optimization for distributions on the sphere and
improvement to the Welch bounds, Electronic Communications in
Probability 22 (2017), 1–12.
Tan23. K. Tanaka, Accelerated gradient descent method for functionals
of probability measures by new convexity and smoothness based on
transport maps, arXiv preprint 2305.05127 (2023), 1–31.
Tsy09. A.B.Tsybakov,Introductiontononparametricestimation,Springer
Series in Statistics,Springer,NewYork,2009,Revisedandextended
from the 2004 French original, Translated by Vladimir Zaiats.
TR24. B. Tzen and M. Raginsky, Function approximation by neural
nets in the mean-field regime: entropic regularization and controlled
McKean–Vlasov dynamics, arXiv preprint 2002.01987 (2024), 1–31.
USP19. A. Uppal, S. Singh, and B. Po´czos, Nonparametric density es-
timation & convergence rates for GANs under Besov IPM losses,
in Advances in Neural Information Processing Systems 32 (H. M.
Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche´-Buc,
E. B. Fox, and R. Garnett, eds.), 2019, pp. 9086–9097.
vdVW23. A. W. van der VaartandJ. A. Wellner,Weak convergence and
empirical processes—with applications to statistics, 2 ed., Springer
Series in Statistics, Springer, Cham, 2023.
vdV98. A. W. van der Vaart, Asymptotic statistics, Cambridge Series in
Statistical and Probabilistic Mathematics 3, Cambridge University
Press, Cambridge, 1998.278 References
vdVW96. A. W. van der VaartandJ. A. Wellner,Weak convergence and
empirical processes, Springer Series in Statistics, Springer-Verlag,
New York, 1996, With applications to statistics.
VMB+24. A. Vacher,B. Muzellec,F. Bach,F.-X. Vialard,andA. Rudi,
OptimalestimationofsmoothtransportmapswithkernelSoS,SIAM
J. Math. Data Sci. 6 (2024), 311–342.
VC23. T. Vaskevicius and L. Chizat, Computational guarantees for dou-
bly entropic Wasserstein barycenters, in Advances in Neural Infor-
mation Processing Systems (A. Oh, T. Naumann, A. Globerson,
K.Saenko,M.Hardt,andS.Levine,eds.),36,CurranAssociates,
Inc., 2023, pp. 12363–12388.
VSP+17. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. u. Kaiser, and I. Polosukhin, Attention is all
you need, in Advances in Neural Information Processing Systems
(I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, eds.), 30, Curran Associates,
Inc., 2017.
Ver18. R. Vershynin, High-dimensional probability, Cambridge Series in
Statistical and Probabilistic Mathematics 47, Cambridge University
Press, Cambridge, 2018, An introduction with applications in data
science, With a foreword by Sara van de Geer.
Vil03. C. Villani, Topics in optimal transportation, Graduate Studies in
Mathematics 58, American Mathematical Society, Providence, RI,
2003.
Vil09a. C. Villani, Hypocoercivity, Mem. Amer. Math. Soc. 202 (2009),
iv+141.
Vil09b. C. Villani, Optimal transport, Grundlehren der Mathematischen
Wissenschaften [Fundamental Principles of Mathematical Sciences]
338, Springer-Verlag, Berlin, 2009, Old and new.
Wai19. M. J. Wainwright, High-dimensional statistics: a non-asymptotic
viewpoint, Cambridge Series in Statistical and Probabilistic Mathe-
matics, Cambridge University Press, 2019.
WJ08. M. J. Wainwright and M. I. Jordan, Graphical models, exponen-
tial families, and variational inference, Foundations and Trends in
Machine Learning 1 (2008), 1–305.
WSB+13. W. Wang, D. Slepcˇev, S. Basu, J. A. Ozolek, and G. K. Ro-
hde, A linear optimal transportation framework for quantifying and
visualizing variations in sets of images, Int. J. Comput. Vis. 101
(2013), 254–269.
WL20. Y. Wang and W. Li, Information Newton’s flow: second-order
optimizationmethodinprobabilityspace,arXiv preprint 2001.04341
(2020), 1–62.
WL22. Y. Wang and W. Li, Accelerated information gradient flow, J. Sci.
Comput. 90 (2022), Paper No. 11, 47.
Wib18. A. Wibisono, Sampling as optimization in the space of measures:
the Langevin dynamics as a composite optimization problem, in
Conference on Learning Theory (S. Bubeck, V. Perchet, and
P. Rigollet, eds.), Proceedings of Machine Learning Research 75,
PMLR, 2018, pp. 2093–3027.References 279
XNW22. J. Xi and J. Niles-Weed, Distributional convergence of the sliced
Wasserstein process, in Advances in Neural Information Process-
ing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-
grave,K.Cho,andA.Oh,eds.),35,CurranAssociates,Inc.,2022,
pp. 13961–13973.
XH22. X. Xu and Z. Huang, Central limit theorem for the sliced 1-
Wasserstein distance and the max-sliced 1-Wasserstein distance,
arXiv preprint 2205.14624 (2022), 1–37.
YWR24. Y. Yan, K. Wang, and P. Rigollet, Learning Gaussian mixtures
using the Wasserstein–Fisher–Rao gradient flow, 2024.
YY23. R. Yao and Y. Yang, Mean-field variational inference via Wasser-
stein gradient flow, arXiv preprint 2207.08074 (2023), 1–120.
ZP19. Y. Zemel and V. M. Panaretos, Fr´echet means and Procrustes
analysis in Wasserstein space, Bernoulli 25 (2019), 932–976.
ZPFP20. K. S. Zhang, G. Peyre´, J. Fadili, and M. Pereyra, Wasserstein
control of mirror Langevin Monte Carlo, in Proceedings of Thirty
Third Conference on Learning Theory (J. AbernethyandS. Agar-
wal, eds.), Proceedings of Machine Learning Research 125, PMLR,
7 2020, pp. 3814–3841.280 ReferencesIndex
Alexandrov curvature, 209 empirical risk minimization
Alexandrov space, 213 (ERM), 231
Atjai–Koml´os–Tusn´ady entropic optimal transport, 107
theorem, 56
Fenchel–Young inequality, 247
attention, 196
first variation, 143
Fisher–Rao, 152
barycenter, 228
Fokker–Planck equation, 178
Benamou–Brenier formula, 141
fundamental theorem of
Birkhoff polytope, 12
optimal transport, 32
birth-death sampling, 188
Brenier’s theorem, 21
geodesic, 206
improved, 34
extendable, 235
Burer–Monteiro, 163
generalized, 225
Bures–Wasserstein, 146
geodesic space, 203
gradient, 147
Gibbs variational principle, 110
gluing lemma, 255
CAT(0), see NPC
goodness-of-fit testing, 62
chaining, 53
Gr¨onwall’s inequality, 145
change of variables, 167
continuity equation, 134 Hadamard, see NPC
convex duality, 246 Hellinger–Kantorovich, see
cyclical monotonicity, 23 Wasserstein–Fisher–Rao
Hoeffding’s decomposition, 118
dyadic partitioning, 47
hugging, 232
earth mover’s distance, 61 integral probability metric
empirical process, 53 (IPM), 73
281282 INDEX
interaction energy, 144 portmanteau theorem, 254
internal energy, 144 potential energy, 144
Prokhorov’s theorem, 253
Kantorovich potentials, 34
proximal sampler, 182
Kantorovich problem, 9
Kantorovich–Rubinstein
rectifiable path, 204
distance, see Wasserstein
Rockafellar’s theorem, 23
distance
semi-discrete optimal transport,
Langevin diffusion, 176
81, 128
mean-field, 187
semidual, 29
mirror Langevin, 183
Newton–Langevin, 184 stability, 89
underdamped Langevin, 184 Sinkhorn divergence, 128
length, 203 Sinkhorn’s algorithm, 115
logarithmic map, 220 Sobolev space, 68
logistic function, 193 Stein variational gradient
descent (SVGD), 173
maximum mean discrepancy
subdifferential, 246
(MMD), 74
McKean–Vlasov, 186 tangent cone, 216, 219
mean-field neural networks, 193 transformers, 195
metric derivative, 137
midpoint, 206 unbalanced optimal transport,
Monge problem, 8 152
neural network, 193
variance equality, 232
neural tangent kernel (NTK),
variational inference (VI), 165
201
Gaussian, 169
NNC, 213
mean-field, 170
non-parametric maximum
likelihood (NPMLE), 189
Wasserstein distance, 13, 14
NPC, 213
sliced, 78
path, 203 smoothed, 77
perceptron, 196 Wasserstein gradient, 143
plug-in estimator, 61 Wasserstein gradient flow, 144
Poincar´e inequality, 99 Wasserstein–Fisher–Rao
Polyak–L(cid:32) ojasiewicz (PL(cid:32) ) (WFR), 154, 188
inequality, 145, 249 weak convergence, 253