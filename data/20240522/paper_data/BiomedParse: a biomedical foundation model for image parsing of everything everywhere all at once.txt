BiomedParse: a biomedical foundation model for image parsing of
everything everywhere all at once
TheodoreZhao1∗†¶,YuGu1∗,JianweiYang1,NaotoUsuyama1,HoHinLee1,Tristan
Naumann1,JianfengGao1,AngelaCrabtree2,BrianPiening2,CarloBifulco2,MuWei1,‡,
HoifungPoon1,‡,§,ShengWang3,‡
1MicrosoftResearch,Redmond,WA,USA
2ProvidenceGenomics,Portland,OR,USA
3PaulG.AllenSchoolofComputerScienceandEngineering,UniversityofWashington,Seattle,
WA,USA
https://aka.ms/biomedparse-project
Abstract
Biomedical image analysis is fundamental for biomedical discovery in cell biology, pathology, radiol-
ogy, and many other biomedical domains. Holistic image analysis comprises interdependent subtasks
suchassegmentation,detection,andrecognitionofrelevantobjects. Traditionally,thesetasksaretack-
ledseparately. Forexample,therehavebeenalotofworksfocusingonsegmentationalone,completely
ignoringkeysemanticinformationindownstreamtasksofdetectionandrecognition. Incontrast,image
parsing is a unifying framework that jointly pursues these tasks by leveraging their interdependencies
such as the semantic label of a segmented object. Here, we propose BiomedParse, a biomedical foun-
dationmodelforimagingparsingthatcanjointlyconductsegmentation,detection,andrecognitionfor
82objecttypesacross9imagingmodalities. Throughjointlearning,wecanimproveaccuracyforindi-
vidualtasksandenablenovelapplicationssuchassegmentingallrelevantobjectsinanimagethrough
a text prompt, rather than requiring users to laboriously specify the bounding box for each object. In-
terestingly,wecantrainBiomedParseusingnomorethanstandardsegmentationdatasets. Thekeyisto
leveragereadilyavailablenatural-languagelabelsordescriptionsaccompanyingthosedatasetsanduse
GPT-4toharmonizethenoisy,unstructuredtextinformationwithestablishedbiomedicalobjectontolo-
gies. We created a large dataset comprising over six million triples of image, segmentation mask, and
textualdescription. Onimagesegmentation,weshowedthatBiomedParseisbroadlyapplicable,outper-
formingstate-of-the-artmethodson102,855testimage-mask-labeltriplesacross9imagingmodalities
(everything). BiomedParseisalsoabletoidentifyinvaliduserinputsdescribingobjectsthatdonotex-
ist in the image. On object detection, which aims to locate a specific object of interest, BiomedParse
againattainedstate-of-the-artperformance,especiallyonobjectswithirregularshapes(everywhere).On
objectrecognition,whichaimstoidentifyallobjectsinagivenimagealongwiththeirsemantictypes,
weshowedthatBiomedParsecansimultaneouslysegmentandlabelallbiomedicalobjectsinanimage
(all at once). In summary, BiomedParse is an all-in-one tool for biomedical image analysis by jointly
solvingsegmentation,detection,andrecognition. Itisbroadlyapplicabletoallmajorbiomedicalimage
modalities,pavingthepathforefficientandaccurateimage-basedbiomedicaldiscovery.
∗Jointfirstauthors
†Projectlead
¶Maintechnicalcontribution
‡ Corresponding authors: muhsin.wei@microsoft.com, swang@cs.washington.edu, hoi-
fung@microsoft.com
§Leadcontact
1
4202
yaM
12
]VC.sc[
1v17921.5042:viXra2
Introduction
Biomedical image analysis is critical to biomedical discovery because imaging is one of the most impor-
tant tools for studying physiology, anatomy, and function at multiple scales from the organelle level to the
organ level [1, 2]. Holistic image analysis comprises multiple subtasks, such as segmentation, detection,
and recognition of biomedical objects. Segmentation aims to divide an image into segments representing
different objects, often requiring the aid of a user-provided bounding box for each object of interest [3, 4].
Detection aims to identify the location of an object of interest in the image [5], whereas recognition aims
toidentifyallobjectswithinanimage[6]. Standardimageanalysismethodstypicallyapproachthesetasks
separately, using specialized tools for individual tasks [7]. Despite their encouraging performance, such a
disjoint approach misses significant opportunities for joint learning and reasoning across these interdepen-
denttasks.
For example, a lot of prior image analysis works focus on segmentation alone, thus ignoring key se-
manticinformationfromdownstreamtasksofobjectdetectionandrecognition. Thisresultsinsub-optimal
segmentation while imposing substantial burden on users, as many state-of-the-art segmentation tools re-
quire users to provide a tight bounding box indicating the location of an object of interest [8, 9]. The
bounding-boxrequirementleadstothreelimitations. First,usershavetomanuallydrawboundingboxesin
the image, which requires domain expertise to identify the locations and shapes of the target objects. Sec-
ond,boundingboxes,whichareoftenrectangular,fallshortofaccuratelyrepresentingobjectswithirregular
or complex shapes. Third, bounding box-based approaches are not scalable for images containing a large
number of objects, such as segmenting cells in a whole-slide pathology image, since users need to provide
aboundingboxforeachobject.
Inthispaper,weproposetoapproachbiomedicalimageanalysisasimageparsing,aunifyingframework
for joint learning and reasoning across segmentation, detection, and recognition [10–12]. Specifically, we
havedevelopedBiomedParse,abiomedicalfoundationmodelforimageparsingthatiscapableofcarrying
outallthreetasksbyleveragingtheirinterdependencies,thusaddressingkeylimitationsintraditionalmeth-
ods. In particular, joint learning of object detection and recognition eliminates the need for user-specified
boundingboxes,assegmentationcanbedoneusingsemanticlabelsfromtextpromptalone.
The major bottleneck for pretraining BiomedParse is data. While biomedical segmentation datasets
abound [13–15],therearerelativelyfewpriorworksonobjectdetectionandrecognitioninbiomedicine,let
alonedatasetscoveringallthreetasks. Toaddressthisproblem,weproposeanovelapproachforpretraining
BiomedParse using no more than standard segmentation datasets. The key insight is to leverage readily
availablenatural-languagelabelsordescriptionsaccompanyingthosedatasetsanduseGPT-4toharmonize
these noisy, unstructured texts with established biomedical object ontologies. This enables us to construct
BiomedParseData,abiomedicalimageparsingdatasetcomprising3.4milliontriplesofimage,segmentation
mask, and semantic label of the biomedical object and 6.8 million image-mask-description triples, from
over 1 million images. The semantic labels encompass 82 major biomedical object types across 9 imaging
modalities.
Unlike segmentation methods that focus on identifying salient segment boundary within a bounding
box,BiomedParselearnstomodeltypicalshapeofeachobjectclass,thusmimickinghowhumansperceive
objects in an image. BiomedParse can segment images using text prompts alone (e.g. “inflammatory cells
in breast pathology”), without requiring any user-specified localization such as bounding boxes. Conse-
quently,BiomedParsecanbetterrecognizeandsegmentobjectsofirregularandcomplexshapes,whichare
very challenging for traditional methods using rectangular bounding boxes. Moreover, BiomedParse can
recognizeallobjectsinanimage,withoutrequiringanyuserinputprompt.
We conduct a large-scale study to evaluate BiomedParse on 102,855 held-out image-mask-label triples
across 9 modalities for segmentation, detection, and recognition. On segmentation, BiomedParse estab-
lished new state-of-the-art results, outperforming prior best methods such as MedSAM [9] and SAM [8].3
Moreover, using text prompts alone, BiomedParse is much more scalable than these prior methods that re-
quire orders of magnitude more user operations in specifying object-specific bounding boxes to perform
competitively. We also demonstrated that BiomedParse can accurately detect invalid text prompts describ-
ingnon-existentobjectsintheimage. Ondetection,weshowthatBiomedParselearnsaccuratemodelingof
all object classes, including those with irregular shapes. This results in even larger improvement in image
analysisaccuracyforsuchobjects,attaininga0.857Dicescorethatis39.6%higherthanthebest-competing
method. On recognition, we show how BiomedParse can accurately segment and label all objects without
anyuser-specifiedtextprompt. Collectively,weintroduceabiomedicalfoundationmodelforimageparsing,
achievingsuperiorperformanceonsegmentation,detection,andrecognition,pavingthepathforlarge-scale
image-basedbiomedicaldiscovery.
Results
OverviewofBiomedParseandBiomedParseData
Todevelopamodelthatcanjointlyconductsegmentation,detection,andrecognition,weneedasupervision
datasetthatcoversallthreetasks. Toourbestknowledge, nosuchadatasetexists. Tothisend, wecreated
thefirstsuchdatasetBiomedParseDatabycombining45biomedicalimagesegmentationdatasetsandusing
GPT-4togeneratethecanonicalsemanticlabelforeachsegmentedobject.
Thekeyinsightisthatexistingsegmentationdatasetsoftencontainvaluablesemanticinformationabout
the segmented objects. However, such information typically resides in noisy and inconsistent natural-
languagetextdescriptionsthatdonotconformtostandardbiomedicalontologies. Toaddressthischallenge,
we use GPT-4 to create a unifying biomedical object ontology for image analysis and harmonize natural-
languagedescriptionswiththisontology(seeMethods). Thisontologyencompassesthreemaincategories
(histology,organ,abnormality),15meta-objecttypes,and82specificobjecttypes(Fig. 1a).
TheresultingBiomedParseDatacontains3.4milliondistinctimage-mask-labeltriples,spanning9imag-
ing modalities and 25 anatomic sites (Fig. 1b, Supplementary Figure 2), representing a large-scale and
diversedatasetforsemantic-basedbiomedicalimageanalysis.
To make BiomedParse better equipped in handling diverse text prompts not covered by the canonical
semanticlabels,wealsouseGPT-4tosynthesizesynonymoustextdescriptionsforeachsemanticlabeland
sample from them during training. This yielded a total of 6.8 million image-mask-description triples (see
Methods,SupplementaryFigure1and3).
While our method does not use bounding boxes, prior state-of-the-art methods such as MedSAM and
SAM generally require pre-specified bounding boxes. We consider two scenarios to provide the bounding
boxes: oracle bounding box (the minimum rectangular bounding box covering a segmented object) and
boundingboxcreatedbyGroundingDINO[16],astate-of-the-artobjectdetectionmethodthatcangenerate
boundingboxesfromtextpromptofanobjectlabel. (GroundingDINOdoesnotperformsegmentation.)
BiomedParseadoptsamodulardesignundertheSEEMarchitecture[56],comprisinganimageencoder
(for encoding the input image), a text encoder (for encoding the text prompt), a mask decoder (for out-
puting segmentation mask), and a meta-object classifier (for joint training of image encoder with object
semantics). SeeFig. 1c. Theimageandtextencoderswereinitializedusingstate-of-the-artFocal[17]and
PubMedBERT [18],respectively.
Before evaluating image analysis results, we first examine the quality of embeddings derived from
BiomedParse. Specifically, we compare the text embeddings from BiomedParse with those from PubMed-
BERT. We found that embeddings from BiomedParse can better distinguish fine-grained cell types, with a
Silhouettescoreof0.89thatismuchhigherthanusingtheembeddingsfromPubMedBERT(Fig. 1d,Sup-
plementaryFigure4). WealsocomparetheimageembeddingsfromBiomedParsewiththosefromFocal.
We observed that embeddings from BiomedParse are more predictive of tumor malignancy on a pathology4
dataset [19] (Fig. 1e). The superior performance of the text and image embeddings from BiomedParse
necessitatesthetrainingofBiomedParseusingBiomedParseData,raisingourconfidencethatBiomedParse
canbeaneffectiveapproachforbiomedicalimageanalysis.
Accurateandscalablesegmentationacrossninemodalities
WefirstevaluatedBiomedParseonbiomedicalimagesegmentationusingtheheld-outsetcomprising102,855
test instances (image-mask-label triples) across 9 imaging modalities (Fig. 2a, Supplementary Figure
4). We observed that BiomedParse achieved the best Dice score, even against the best-competing method
MedSAM with oracle bounding box as input (paired t-test p-value < 10−4). In the more realistic setting
when MedSAM or SAM is supplied with bounding boxes generated by Grounding DINO, the superiority
ofBiomedParseisevenmoreprominentinend-to-endbiomedicalobjectdetectionandsegmentation,espe-
ciallyinmorechallengingmodalitiessuchaspathologyandCTwhereirregular-shapedobjectsabound. By
training on domain-specific datasets, both BiomedParse and MedSAM outperform general-domain meth-
odssuchasSAM.WeshowedexamplescomparingBiomedParsesegmentationandthegroundtruthacross
multipleimagingmodalities,demonstratingthegeneralizabilityofBiomedParse(Fig. 2b). Wefurthercom-
paredBiomedParseonabenchmarkcreatedbyMedSAM[9]encompassing50tasksandagainobservedthe
best performance by BiomedParse, even against MedSAM with oracle bounding box (paired t-test p-value
< 10−2),furtherdemonstratingthesuperiorityofBiomedParse(SupplementaryFigure5).
In addition to being more accurate, BiomedParse is more scalable compared to bounding box-based
approaches, which stems from the generalizability of text prompts across images of the same modality or
anatomicalsite,thuseliminatingtheneedforlaborioususeroperationsinprovidingatightboundingboxfor
each object. To demonstrate this, we compared BiomedParse and prior state-of-the-art methods MedSAM
and SAM on a cell segmentation dataset with 42 colon pathology images (Fig. 2c). Using a single text
prompt“glandularstructureincolonpathologyimage”, BiomedParseachievesa0.942medianDicescore,
whereasneitherSAMnorMedSAMachievesamedianDicescorehigherthan0.75withouttightbounding
boxesasinput. Infact,toachievecompetitiveresultscomparabletoBiomedParsewithasingletextprompt,
MedSAM requires the users to supply a tight bounding box for each of the 430 cells in these images (Fig.
2c). Ingeneral,ourresultsrevealthattheboundingbox-basedapproachismuchlessaccurateonirregular-
shapedobjects,suchastumorsandabnormalcells(Fig. 2d,e). Incontrast,BiomedParsestillattainedhighly
accuratesegmentationforsuchobjects. ThescalabilityandaccuracyofBiomedParsebodewellforitsutility
inreal-worldapplications.
BiomedParse can also detect invalid text prompts (e.g., the request to identify a brain tissue in a chest
X-Ray image), by calculating a p-value using Kolmogorov–Smirnov (K-S) test (see Methods). From pre-
liminary experiments, we found that invalid text prompts have an average K-S test p-value smaller than
10−3 while the valid ones have an average K-S test p-value above 0.1 (Fig. 2f). Using 0.01 as the p-value
cutoff,BiomedParsecanachieveanestimatedperformanceof0.93precisionand1.00recallondetectingin-
validinput(Fig. 2g). BiomedParsesubstantiallyoutperformedGroundingDINOoninvalidinputdetection
(AUROC 0.99 vs 0.61). See Fig. 2h,i). This enables BiomedParse to perform recognition by enumerating
candidateobjecttypesintheontology,skippinginvalidtextpromptsandgeneratingsegmentationmasksfor
validobjectlabels.
Accuratedetectionofirregular-shapedobjects
Next,weevaluatedtheperformanceofBiomedParseonobjectdetection,wherethemodelisaskedtoiden-
tifyanobjectofinterestintheimage. BiomedParsecanresolvenatural-languagevariationsandaccepttext
prompts that do not exactly match any semantic label in the ontology. In the previous section, we already
showthatBiomedParseoutperformedbounding-box-basedmethodsingeneral. Additionally,sinceBiomed-5
Parse learns semantic representation for individual object types, We hypothesize that its superiority over
priormethodswillbeevenmorepronouncedindetectingirregular-shapedobjects. Toverifythis,weshow
theaggregateattentionmapofeachobjecttypelearnedbyBiomedParseontestimagesunseenduringtrain-
ingandobservedthattheyfaithfullyreflectobjectshapes,includingmanyirregular-shapedobjects(Fig. 3a).
Next, we define three metrics to assess the regularity of an object, including convex ratio (i.e., the ratio of
theobjectsizetothetightestconvexsize),boxratio(i.e.,theratiooftheobjectsizetothetightestrectangle
size),androtationalinertia(i.e.,thedifficultyinchangingtherotationalvelocity)(seeMethods). Wefound
thattheimprovementsofBiomedParseoverSAMandMedSAMarestronglycorrelatedwiththesemetrics
(average correlation 0.829), indicating that our method has a larger improvement on irregular-shaped ob-
jects (Fig. 3b-d, Supplementary Figure 6). Fig. 3e illustrates a few examples comparing BiomedParse
and MedSAM on detecting irregular-shaped objects. Furthermore, we show that BiomedParseData has
higheraverageobjectirregularitythanthedatasetsusedbyMedSAM(Fig. 3f,g,SupplementaryFigure7),
andtheimprovementofBiomedParseisalsolargeronBiomedParseData(Fig. 3h),highlightingthebenefit
fromjointlearningofobjectsemanticsindetectingthemorechallengingirregular-shapedobjects.
Objectrecognitionusingthesegmentationontology
In our final analysis, we explore BiomedParse’s capacity for object recognition, which aims to simultane-
ouslysegmentandlabeleveryobjectwithinanimage. Providedwithanimage,alongwithitsmodalityand
anatomicalsite,BiomedParseiterativelyperformsdetectionandsegmentationforallcandidateobjecttypes
withintheontologyofthatmodalityandanatomicalsite,andthesegmentedmasksareaggregatedtoensure
spatial cohesion among adjacent pixels (see Methods). This enables BiomedParse to accurately conduct
object recognition, as evidenced in Fig. 4a, where objects are accurately identified and segmented with an
averageDicescoreof0.94.
Grounding DINO [16] is the state-of-the-art general-domain object recognition system but it does not
perform segmentation, which makes Grounding DINO and BiomedParse not directly comparable. We cir-
cumventthisbycastingtheobjectrecognitiontaskasabinaryclassificationproblem: givenaninputimage
andacandidateobjecttype,themodeldetermineswhethertheimagecontainsatleastoneobjectofthegiven
type. Inthisclassificationformulation,weobservedthatBiomedParsesubstantiallyoutperformedGround-
ingDINOwitha25.0%,87.9%,74.5%improvementonprecision,recall,andF-1,respectively(Fig. 4b-d).
The improvement over Grounding DINO is even larger when more objects are present in the image (Fig.
4e).
Next, we evaluated the performance of BiomedParse on end-to-end object recognition using weighted
averageDicescore. ComparedwithMedSAMandSAMusingGroundingDINOforrecognitionandbound-
ing box generation, BiomedParse outperformed them by a large margin (Fig. 4f, Supplementary Figure
8). Similartoourobservationonobjectidentification,theimprovementovercomparisonapproachesiseven
largerwhenmoreobjectsarepresentintheimage(Fig. 4g). TheseresultsindicateBiomedParse’sabilityto
identifyallobjectsinanimage,offeringaneffectivetoolforholisticimageanalysis.
Finally,weevaluatedBiomedParseonreal-worlddatafromtheProvidenceHealthSystem(Fig. 5). We
performed object recognition here by asking BiomedParse to identify and segment all relevant cells in the
pathology slides. We found that the annotations by BiomedParse correctly identified regions of immune
cells and cancer cells, attaining high consistency with the pathologist annotations. While pathologists tend
tofocusonaspecificregionofcelltypeandprovidecoarse-grainedannotations,BiomedParsecanprecisely
labelallrelevantcellsasspecifiedintheontology,indicatingthepotentialforBiomedParsetohelpalleviate
clinicianburdensinreal-worldclinicalapplications.6
Discussion
WehavepresentedBiomedParse,abiomedicalfoundationmodelforimageanalysisbasedonimageparsing,
and a large-scale image parsing dataset BiomedParseData containing 3.4 million image-mask-label triples
and6.8millionimage-mask-descriptiontriples. Incontrasttoexistingbiomedicalfoundationalmodelsthat
requireuserstoprovideatightboundingboxforeachobjecttosegment,BiomedParseisboundingbox-free,
andcanperformholisticimageanalysiswithsegmentation,detection,andrecognitionallatonce. Wecon-
ductedalarge-scaleevaluationon102,855testimage-mask-labeltriplesacross9modalities. BiomedParse
attained new state-of-the-art results, substantially outperforming prior best methods such as MedSAM and
SAM, even when they were equipped with oracle bounding box as input. The improvement is even larger
when the objects have irregular shapes or when an image contains a large number of objects. We also val-
idated the accuracy and scalability of BiomedParse on previous unseen real-world data from Providence
HealthSystem. Collectively,BiomedParseoffersanaccurate,scalable,androbustbiomedicalimageanaly-
sistoolthatcanbebroadlyappliedtovariousmodalitiesandapplications,pavingthepathforimage-based
biomedicaldiscovery.
The image analysis field has witnessed rapid development in the past decade. Since its inception in
2015, the U-Net architecture has revolutionized the field of automatic pixel-wise prediction through su-
pervised training [20, 21]. This groundbreaking work laid the foundation for a diverse array of network
structures,rangingfromadvancedconvolution-networkdesignstovision-transformermodels[22–37]. Re-
centadvancesinimagedetectionandrecognition,suchasdevelopmentsinobjectdetectionframeworkslike
Faster R-CNN [38] and YOLOv4 [39], have significantly enhanced capabilities in identifying and localiz-
ing anatomical features with high precision. The introduction of SAM marked a significant milestone by
demonstrating the model’s ability to generalize segmentation to previously unseen classes, utilizing visual
promptssuchaspointsandboundingboxesasguides[8].
Despite the proliferation of advances in the general domain, research on adapting them for large-scale
biomedical image analysis across a wide range of organ or tissue classes remains relatively sparse [40].
MedSAM is a notable exception by adapting SAM to the medical realm through continued training on a
largenumberofbiomedicalsegmentationdatasets,establishingthestateofartinbiomedicalimageanalysis.
However,likeSAM,MedSAMfocusesonsegmentationalone,thusignoringvaluablesemanticinformation
from related tasks of detection and recognition. Consequently, both SAM and MedSAM require users to
provide labor-intensive input such as the tight bounding box for each object to segment, which is hard to
scaleandverychallengingforobjectswithirregularshapes [9].
We propose BiomedParse to overcome these challenges. By joint learning across segmentation, detec-
tion, and recognition in the unifying framework of image parsing, and by using GPT-4 to harmonize noisy
objectdescriptions,BiomedParsewasabletoacquirenovelcapabilitiessuchasidentifyingandsegmenting
objects of interest using text prompt alone, as well as recognizing all objects in an image by leveraging
the segmentation ontology. This represents an important step toward scaling holistic image analysis in
biomedicineandreal-worldclinicalapplications.
A particularly exciting area for biomedical image analysis is the application in cellular images such as
H&E staining and Multiplexed ImmunoFluorescence (MxIF) imaging. This could help elucidate the size,
shape, texture, and spatial relationships of individual cells, with potential ramifications in emerging appli-
cations such as modeling tumor microenvironments for precision immunotherapy [41–43]. The standard
approaches focus on instance segmentation by assigning unique identifiers to individual cells to facilitate
downstreamanalysis[44–46]. Hover-netrepresentsasignificantadvancementinaddressingthelimitations
of semantic breadth and cell categorization within segmentation tasks, by incorporating cell classification
into the segmentation process [47]. However, traditional methods typically rely on bounding box detec-
tion, and struggle with diverse cell morphologies and irregular shapes. Recent efforts aim to overcome
these challenges by adopting more refined representations and accommodating the multi-resolution nature7
of biological imaging [48–50]. Cell-ViT is a marquee example that leverages SAM’s encoder backbone to
improvehierarchicalrepresentation,particularlyfornucleussegmentation[51]. BiomedParsecancontribute
tothislonglineofexcitingresearchworkbyenablingcellsegmentationandidentificationinonefellswoop
andenhancinggeneralizabilitythroughjointtrainingonadiverserangeofimagemodalitiesandcelltypes.
WhileBiomedParsehasdemonstratedpromisingpotentialforunifyingbiomedicalimageanalysis,growth
areasabound. First,althoughBiomedParsehasdemonstratedhighaccuracy(e.g.,Dicescores)inidentifying
relevant pixels in an image for a given object type, by default it does not differentiate individual object in-
stancesandrequirespostprocessingtoseparatetheinstancemasks,whichisimportantinsomeapplications
such as cell counting. Second, while BiomedParse can already perform image analysis from text prompt
alone, itcurrentlydoesnotsupportinteractivedialoguewithusersinaconversationalstylelikeGPT-4. To
addressthis,weplantodevelopaconversationalsystemthatcanbettertailortocomplexuserneeds. Finally,
BiomedParse currently treats non-2D modalities such as CT and MRI by reducing them to 2D slices, thus
failingtoutilizethespatialandtemporalinformationintheoriginalmodalities. Infuturework,weneedto
extendBiomedParsebeyond2Dimageslicestofacilitate3Dsegmentation,detection,andrecognition.8
Methods
DetailsofBiomedParseData
We created the first large-scale biomedical image parsing dataset BiomedParseData, where each image is
associatedwithacollectionofobjects. Eachobjectisannotatedwiththesegmentationmaskandacanonical
semanticlabelspecifyingtheobjecttypefromabiomedicalobjectontology. Additionally,eachsemanticla-
belcomeswithasetofsynonymoustextualdescriptionsformodeltraining. BiomedParseDatawascreated
by synthesizing 45 publicly available biomedical segmentation datasets across 9 imaging modalities, com-
prising 1.1 million images, 3.4 million image-mask-label triples, and 6.8 million image-mask-description
triples(Fig. 1b). ToensurethequalityofBiomedParseData, weimposedstringentinclusioncriteria: each
image had to be manually or semi-manually segmented at the pixel level, and a name was available for
each segmented object from the dataset description. For 3D imaging modalities such as CT and MRI, we
pre-processedeachvolumeintoin-plane2Dslicestobeconsistentwithothermodalities.
For model training and evaluation, we randomly split each original dataset into 80% training and 20%
testing. Slicesfromeach3Dvolumealwaysappearinthesamesplittopreventinformationleakage.
To harmonize natural-language variations in noisy object descriptions, we use GPT-4 to create a three-
layer biomedical object ontology (Fig. 1a). The base layer comprises three broad semantic categories:
organ, abnormality, histology. The next layer comprises 15 meta-object types (e.g., heart in
organ and tumor in abnormality). The most fine-grained layer comprises 82 object types, such
asleft heart ventricleandehhancing tumor. Specifically,wefirstusedGPT-4togeneratea
preliminaryhierarchicalstructureforbiomedicalimageanalysisandproposecandidatenamesforindividual
objecttypes,drawingfromawiderangeoftasksandtextualdescriptionsacrossthesourcedatasets. Wethen
manually reviewed these candidates and mapped them to standardized OHDSI vocabularies using Athena
[52]. Weintroduceotherasacatch-allcategory. Forfutureexpansion,weexpectthatthefirsttwolayers
arerelativelystable,whileourframeworkcaneasilyincorporatenewobjecttypesinthefine-grainedlayers,
aswellasadditionaldatasetswithsegmentationandobjectlabels.
To enhance the robustness of BiomedParse in handling diverse text prompts, we also used GPT-4 to
generate synonymous textual descriptions for each semantic label, following other recent efforts in using
GPT-4 for synthetic data generation [53, 54]. Specifically, we adopt a templatic normalization for each
dataset,byformulatingtheunifyingimageanalysistaskasidentifying“[OBJECTTYPE]in[ANATOMIC
SITE] [MODALITY]”, such as “enhancing tumor in brain MRI” (Supplementary Figure 2). We then
introducedlinguisticdiversityintothesedescriptionsbyusingGPT-4togeneratevariationsinprofessional
language (Supplementary Figure 1), as well as introducing synonymous variations for each component
(Supplementary Figure 3). In each training epoch, we randomly sampled a description for each image-
maskpair,ensuringBiomedParsetounderstanddiversetextprompts.
DetailsofBiomedParse
Existing image analysis methods often focus on segmentation alone. They typically expect spatial input
promptssuchasboundingboxorscribblefortheobjecttosegment,andfocusonlearningspatialembedding
such as bounding box coordinates [8, 9, 55]. In contrast, BiomedParse follows SEEM [56] and focuses on
learning text prompt. Specifically, BiomedParse adopts a modular design, comprising an image encoder, a
text encoder, a mask decoder, and a meta-object classifier. See Fig. 1c. The image and text encoders were
initializedusingFocal[17]andPubMedBERT [18],respectively.
TheinputtoBiomedParseisanimageandatextprompt, whicharepassedalongtotheimageandtext
encoders,respectively. Thetextpromptspecifiestheobjecttypeforsegmentationanddetection. Themask
decoderoutputsasegmentationmaskthathasthesamesizeastheoriginalimage,withaprobabilitybetween9
0 and 1 for each pixel, indicating how likely the pixel belongs to the designated object in the text prompt.
Themeta-objectclassifierincludesinputfromtheimageandoutputsthemeta-objecttypetofacilitatejoint
trainingofimageencoderwithobjectsemantics.
Implementationofcompetingmethods
We compared BiomedParse to state-of-the-art segmentation models, SAM [8] and MedSAM [9]. We rec-
ognize the importance of precise bounding boxes as model input, so we evaluated competing methods in
two settings: (i) employing gold-standard bounding boxes, and (ii) utilizing bounding boxes predicted by
thestate-of-the-artobjectdetectionmodelGroundingDINO[16]toprovideboundingboxprompts. Forthe
firstsetting,wefollow[9]byderivingboundingboxesfromgold-standardmasks,ensuringeachboxtightly
encompassedthemaskwithauniformmarginof10pixels. Inthesecondsetting,weadheredtotheinference
pipeline of Grounding DINO where when presented with multiple bounding box predictions, we selected
theonewiththehighestconfidencescore. Thistext-to-box-to-segmentationschemefollowstheideain[57].
Tomaintainuniformityacrosscomparisons,allinputimageswereresizedto1024×1024pixels. Weusethe
sametestsplitofBiomedParseDataforevaluationacrosscompetingmethods,andperformancewasquanti-
fiedusingthemedianDicescoreoneachtask. Werecognizethatthetrain-testsplitsaredifferentacrossthe
originalevaluationsofthecompetingmethods,andtheBiomedParseDatatestsplitcouldcontainexamples
thatwereusedtotrainothermodels. WenotethattheimplementationsforMedSAM,SAM,andGrounding
DINO were used as-is for inference purposes without any fine-tuning. As for the task-specific nnU-Net
models[26]andtheDeepLabV3+models[58],duetotheunavailabilityofnumeroustask-finetunedmodels
and the lack of explicit training details in existing literature, we relied on performance metrics reported in
theMedSAMstudy[9].
Detectinginvalidtextualdescription
BiomedParse by design can input any image and text prompt. However, a text prompt may be invalid,
specifyinganobjectthatdoesn’texistinthegivenimage[54,59]. Forexample,therequesttoidentifyand
segment“leftheartventricle”inadermoscopyimageshouldberejectedbythemodelasinvalid. Itiscritical
todetectandrejectinvalidtextprompttopreempthallucinations[60].
In principle, the mask decoder should output low pixel probabilities for invalid text prompt. However,
giventhesheernumberofpixels,somemightgetarelativelyhighoutputprobabilitysimplybychance,thus
leading to erroneous object detection and segmentation results. To address this problem, we observe that
while individual pixels might get noisily high probabilities, collectively their distribution would be rather
different compared to pixels in valid objects. Consequently, we can estimate the distribution of its pixel
probabilities from training data, and then estimate how likely the pixel probabilities in a test image are
drawnfromthesamedistribution.
Specifically,afterBiomedParsewastrained,foreachobjecttype,wecomputedtheaverageobjectpixel
probability for each training image containing objects of the given type, and fit a Beta distribution for all
theseprobabilities. Attesttime,foragivenimage,wecomputedtheaverageobjectpixelprobabilityforthe
predictedobjectsegmentsofthegivenobjecttype,andcomputethep-valueusingone-sampleKolmogorov-
Smirnov (K-S) test [61]. Smaller p-value indicates that the predicted object segments are unlikely to be
correct. To increase the robustness, in addition to pixel probability, we also consider the RGB values. In
particular, for each color channel (R, G, B), we similarly fit a Beta distribution from the average value for
validobjectsintraining,andcomputethecorrespondingp-valueforthepredictedobjectsegmentsinatest
image. Overall,wetreatthesefourtestsasindependentandusetheirproductasthesummaryp-value.
Inthisway,wecanobtainasummaryp-valueforanygivenpairoftextpromptandimage. Toidentify
asummaryp-valuethresholdforseparatingvalidinputsfrominvalidones,wecreatedaninvaliddatasetby10
sampling invalid object types for each image. We plot the distribution for both valid text prompts (for a
given image) and invalid ones (Fig. 2f). For comparison against Grounding DINO, we use its confidence
scoregivenatextpromptandanimageforinvalidinputdetection.
Attentionmapconditionedonthetextualdescription
To visualize the shape of each segmentation object type, e.g. “hepatic vessel in CT”, we collected the
predicted pixel probabilities for each object type and aggregated probabilities from all images. The pixel-
level probability is derived from the top layer attention on the pixel. The attention map, reflecting the
shapeforatargett,isobtainedinafour-stepapproach. First,wecollectedallBiomedParse-predictedpixel
attention for target t as ρ ,··· ,ρ ∈ [0,1]H×W across n examples in the test set. Second, we initialized
1 n
shapedistributionfortargettasMt = ρ . Third, foriterationi = 1,··· ,n−1, wecomputed2-Dcross-
1 1
correlationbetweenρ andMt,andshiftedρ tobealignedwithMt athighestcross-correlation,and
i+1 i i+1 i
updated the ensemble distribution Mt = Mt + ρ˜ , where ρ˜ denotes the shifted attention matrix.
i+1 i i+1 i+1
Finally,theattentionmapfortargettisnormalizedasMt/n. For3DsegmentationtargetssuchasCTand
n
MRI, we first aggregated the predictions within one volume without shifting and then aligned the volume-
aggregatedmasksusingtheabovemethod.
Detailsofexperimentsonirregular-shapedobjectdetection
MedicalimagesegmentationmodelslikeMedSAMrequireaboundingboxasinput. Whentheshapeofthe
target is “irregular”, it is hard for the bounding box to precisely define the region of interest. To quantify
the“regularity”ofatargetmaskM,wedefinethefollowingthreemetrics: BoxRatiomeasuresthedegree
|M|
to which the target mask is similar to its tight bounding box: BoxRatio(M) = , where Box(M)
|Box(M)|
isthetightboundingboxaroundmaskM,and|·|denotestheareameasuredinnumberofpixels. Convex
|M|
Ratio measures how convex the target mask is and is defined as ConvexRatio(M) = ,
|ConvexHull(M)|
where ConvexHull(M) is the convex hull of mask M. Inverse rotational inertia (IRI) measures how
spreadouttheareaofthetargetmaskis. Tobeginwith,therotationalinertiaofM relativetoitscentroidc
M
isRI(M) = (cid:80) ∥x−c ∥2,wherexisthecoordinateofeachpixelinthemask,andc isthecoordinate
x∈M M 2 M
ofthecentroid. Tostandardizethemetrictobeindependentofthetotalmaskarea,wetaketheinverseofthe
rotationalinertiaandscalebythevalueofaround-shapedmaskwiththesamearea,representingthelowest
|M|2
rotationalinertiaachievablebyanymaskwiththesamearea: IRI(M) = . Underthisdefinition,
2π·RI(M)
anymaskhas0 < IRI ≤ 1,withanyround-shapedmaskhavingIRIequalto1.
Detailsofexperimentsonobjectrecognition
We built a hierarchical structure putting all supported targets under one modality at one anatomic site.
Given any image, e.g. abdominal CT, we traverse all the available targets t = 1,··· ,m under the branch
that are exclusive to each other, and prompt the BiomedParse model sequentially to get m prediction of
mask probabilities ρ1,··· ,ρm. It is possible that the predicted masks can overlap with each other. The
challenges then are how to select the right set of targets in the specific image and how to determine the
right mask regions for the selected targets to avoid overlapping. We used a two-stage approach for object
recognition, including a target selection stage and a mask aggregation stage. In the target selection stage,
we first calculate the original mask area for each target t as At. Then, we iterate through the pixels. For
eachpixel(i,j),werankthetargetsthathavepixelprobabilityρt > 0.5. Thetargetassignedtopixel(i,j)
ij
isT = argmaxρt′.Afterthisroundofpixelassigning,thefinalareaforeachtargettisA˜t = (cid:80) 1 .
ij ij i,j Tij=t
The targets with final area A˜t > λAt are the selected targets, with λ being the user-specified threshold. In11
themaskaggregationstage,wediscardallunselectedtargetmaskscompletely,andtheniteratethroughthe
pixelsagain. Foreachpixel,themostprobabletargettwithρt > 0.5isassigned. Thepixelswithpredicted
ij
probabilitiesρt ≤ 0.5forallselectedtargetsareleftblank.
ij
ForthebaselinemethodusingGroundingDINOwithSAMandMedSAM,wefirstpromptedGrounding
DINO with the set of targets to retrieve a collection of bounding boxes with confidence scores. Then we
implemented non-maximum suppression [62–64] to select a subset of identified targets in the scene, mini-
mizing the overlapping between the targets. To get the segmentation masks for these identified targets, we
furtherpromptedSAMandMedSAMwiththeboundingboxestoretrievethecorrespondingpredictions.
Data availability
We will provide access to BiomedParseData or scripts to reproduce BiomedParseData from the original
datasets,uponpublicationofthismanuscript.
Code availability
BiomedParsewillbemadefullyavailableuponpublication,includingthemodelweightsandrelevantsource
codeforpre-training,fine-tuning,andinference. Wewillalsoprovidedetailedmethodsandimplementation
stepstofacilitateindependentreplication.12
References
[1] Royer, L. A. The future of bioimage analysis: a dialog between mind and machine. Nature Methods
20,951–952(2023).
[2] Li,X.,Zhang,Y.,Wu,J.&Dai,Q.Challengesandopportunitiesinbioimageanalysis.NatureMethods
20,958–961(2023).
[3] Wang,R.etal. Medicalimagesegmentationusingdeeplearning: Asurvey. IETImageProcessing16,
1243–1267(2022).
[4] Salpea,N.,Tzouveli,P.&Kollias,D. Medicalimagesegmentation: Areviewofmodernarchitectures.
InEuropeanConferenceonComputerVision,691–708(Springer,2022).
[5] Ribli, D., Horva´th, A., Unger, Z., Pollner, P. & Csabai, I. Detecting and classifying lesions in mam-
mogramswithdeeplearning. Scientificreports8,4165(2018).
[6] Ma, W., Lu, J. & Wu, H. Cellcano: supervised cell type identification for single cell atac-seq data.
NatureCommunications14,1864(2023).
[7] Jiang, H. et al. A review of deep learning-based multiple-lesion recognition from medical images:
classification,detectionandsegmentation. ComputersinBiologyandMedicine157,106726(2023).
[8] Kirillov,A.etal. Segmentanything. arXivpreprintarXiv:2304.02643(2023).
[9] Ma,J.etal. Segmentanythinginmedicalimages. NatureCommunications15,654(2024).
[10] Tu, Z., Chen, X., Yuille, A. L. & Zhu, S.-C. Image parsing: Unifying segmentation, detection, and
recognition. InternationalJournalofcomputervision63,113–140(2005).
[11] Tighe,J.&Lazebnik,S. Superparsing: scalablenonparametricimageparsingwithsuperpixels. Inter-
nationalJournalofComputerVision101,329–349(2013).
[12] Zhou, S. K. Medical image recognition, segmentation and parsing: machine learning and multiple
objectapproaches(AcademicPress,2015).
[13] Gamper,J.etal. Pannukedatasetextension,insightsandbaselines. arXivpreprintarXiv:2003.10778
(2020).
[14] Ji, Y. et al. Amos: A large-scale abdominal multi-organ benchmark for versatile medical image seg-
mentation. arXivpreprintarXiv:2206.08023(2022).
[15] Bernard,O.etal. DeeplearningtechniquesforautomaticMRIcardiacmulti-structuressegmentation
anddiagnosis: Istheproblemsolved? IEEETransactionsonMedicalImaging37,2514–2525(2018).
[16] Liu,S.etal. GroundingDINO:MarryingDINOwithgroundedpre-trainingforopen-setobjectdetec-
tion(2023). 2303.05499.
[17] Yang, J., Li, C., Dai, X. & Gao, J. Focal modulation networks. Advances in Neural Information
ProcessingSystems35,4203–4217(2022).
[18] Gu,Y.etal. Domain-specificlanguagemodelpretrainingforbiomedicalnaturallanguageprocessing.
ACMTransactionsonComputingforHealthcare(HEALTH)3,1–23(2021).13
[19] Sirinukunwattana, K., Snead, D. R. J. & Rajpoot, N. M. A stochastic polygons model for glandular
structuresincolonhistologyimages. IEEETransactionsonMedicalImaging34,2366–2378(2015).
[20] Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image seg-
mentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th
International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, 234–241
(Springer,2015).
[21] C¸ic¸ek, O¨., Abdulkadir, A., Lienkamp, S. S., Brox, T. & Ronneberger, O. 3D U-Net: learning dense
volumetricsegmentationfromsparseannotation. InInternationalconferenceonmedicalimagecom-
putingandcomputer-assistedintervention,424–432(Springer,2016).
[22] Milletari, F., Navab, N. & Ahmadi, S.-A. V-Net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), 565–571
(IEEE,2016).
[23] Li,X.etal. H-DenseUNet: hybriddenselyconnectedunetforliverandtumorsegmentationfromCT
volumes. IEEEtransactionsonmedicalimaging37,2663–2674(2018).
[24] Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N. & Liang, J. UNet++: Redesigning skip connections to
exploit multiscale features in image segmentation. IEEE transactions on medical imaging 39, 1856–
1867(2019).
[25] Myronenko,A. 3DMRIbraintumorsegmentationusingautoencoderregularization. InInternational
MICCAIBrainlesionWorkshop,311–320(Springer,2018).
[26] Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J. & Maier-Hein, K. H. nnU-net: a self-configuring
methodfordeeplearning-basedbiomedicalimagesegmentation. Naturemethods18,203–211(2021).
[27] Lee,H.H.,Bao,S.,Huo,Y.&Landman,B.A. 3DUX-Net: Alargekernelvolumetricconvnetmod-
ernizing hierarchical transformer for medical image segmentation. arXiv preprint arXiv:2209.15076
(2022).
[28] Lee,H.H.etal. Scalingup3Dkernelswithbayesianfrequencyre-parameterizationformedicalimage
segmentation. arXivpreprintarXiv:2303.05785(2023).
[29] Lee,H.H.etal. DeformUX-Net: Exploringa3Dfoundationbackboneformedicalimagesegmenta-
tionwithdepthwisedeformableconvolution. arXivpreprintarXiv:2310.00199(2023).
[30] Chen,J.etal. TransUNet: Transformersmakestrongencodersformedicalimagesegmentation. arXiv
preprintarXiv:2102.04306(2021).
[31] Xu,G.,Zhang,X.,He,X.&Wu,X. LeViT-UNet: Makefasterencoderswithtransformerformedical
image segmentation. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV),
42–53(Springer,2023).
[32] Xie,Y.,Zhang,J.,Shen,C.&Xia,Y. Cotr: EfficientlybridgingCNNandtransformerfor3Dmedical
imagesegmentation. InInternationalconferenceonmedicalimagecomputingandcomputer-assisted
intervention,171–180(Springer,2021).
[33] Wang,W.etal. TransBTS:Multimodalbraintumorsegmentationusingtransformer. InInternational
Conference on Medical Image Computing and Computer-Assisted Intervention, 109–119 (Springer,
2021).14
[34] Hatamizadeh,A.etal. UNETR:Transformersfor3Dmedicalimagesegmentation. InProceedingsof
theIEEE/CVFWinterConferenceonApplicationsofComputerVision,574–584(2022).
[35] Hatamizadeh, A. et al. Swin UNETR: Swin transformers for semantic segmentation of brain tumors
inMRIimages. InInternationalMICCAIBrainlesionWorkshop,272–284(Springer,2022).
[36] Zhou, H.-Y. et al. nnFormer: Interleaved transformer for volumetric segmentation. arXiv preprint
arXiv:2109.03201(2021).
[37] Cao,H.etal. Swin-UNet: UNet-likepuretransformerformedicalimagesegmentation. arXivpreprint
arXiv:2105.05537 (2021).
[38] Ren, S., He, K., Girshick, R. & Sun, J. Faster r-cnn: Towards real-time object detection with region
proposalnetworks(2016). 1506.01497.
[39] Bochkovskiy, A., Wang, C.-Y. & Liao, H.-Y. M. Yolov4: Optimal speed and accuracy of object
detection(2020). 2004.10934.
[40] Litjens, G. et al. A survey on deep learning in medical image analysis. Medical Image Analysis 42,
60–88(2017). URLhttp://dx.doi.org/10.1016/j.media.2017.07.005.
[41] Stringer, C., Wang, T., Michaelos, M. & Pachitariu, M. Cellpose: a generalist algorithm for cellular
segmentation. Naturemethods18,100–106(2021).
[42] Greenwald,N.F.etal. Whole-cellsegmentationoftissueimageswithhuman-levelperformanceusing
large-scaledataannotationanddeeplearning. Naturebiotechnology40,555–565(2022).
[43] Ma,J.&Wang,B. Towardsfoundationmodelsofbiologicalimagesegmentation. NatureMethods20,
953–955(2023).
[44] Girshick, R. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision,
1440–1448(2015).
[45] He,K.,Gkioxari,G.,Dolla´r,P.&Girshick,R.MaskR-CNN.InProceedingsoftheIEEEinternational
conferenceoncomputervision,2961–2969(2017).
[46] Schmidt, U., Weigert, M., Broaddus, C. & Myers, G. Cell detection with star-convex polygons. In
Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st International
Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11, 265–273 (Springer,
2018).
[47] Graham, S. et al. Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue
histologyimages. Medicalimageanalysis58,101563(2019).
[48] Yang, H. et al. CircleNet: Anchor-free glomerulus detection with circle representation. In Medical
ImageComputingandComputerAssistedIntervention–MICCAI2020: 23rdInternationalConference,
Lima,Peru,October4–8,2020,Proceedings,PartIV23,35–44(Springer,2020).
[49] Nguyen, E. H. et al. CircleSnake: Instance segmentation with circle representation. In International
WorkshoponMachineLearninginMedicalImaging,298–306(Springer,2022).
[50] Ilyas,T.etal. Tsfd-net: Tissuespecificfeaturedistillationnetworkfornucleisegmentationandclassi-
fication. NeuralNetworks151,1–15(2022).15
[51] Ho¨rst, F. et al. Cellvit: Vision transformers for precise cell segmentation and classification. arXiv
preprintarXiv:2306.15350(2023).
[52] OHDSI. Athena standardized vocabularies. https://www.ohdsi.org/analytic-tools/
athena-standardized-vocabularies/(n.d.). Accessed: 2022-01-17.
[53] Gu, Y. et al. BiomedJourney: Counterfactual biomedical image generation by instruction-learning
frommultimodalpatientjourneys. arXivpreprintarXiv:2310.10765(2023).
[54] Li, C. et al. LLaVA-Med: Training a large language-and-vision assistant for biomedicine in one day.
AdvancesinNeuralInformationProcessingSystems36(2024).
[55] Wong, H. E., Rakic, M., Guttag, J. & Dalca, A. V. Scribbleprompt: Fast and flexible interactive
segmentationforanymedicalimage. arXivpreprintarXiv:2312.07381(2023).
[56] Zou,X.etal. Segmenteverythingeverywhereallatonce. AdvancesinNeuralInformationProcessing
Systems36(2024).
[57] Ren,T.etal. GroundedSAM:Assemblingopen-worldmodelsfordiversevisualtasks. arXivpreprint
arXiv:2401.14159(2024).
[58] Chen,L.-C.,Zhu,Y.,Papandreou,G.,Schroff,F.&Adam,H. Encoder-decoderwithatrousseparable
convolutionforsemanticimagesegmentation.InProceedingsoftheEuropeanconferenceoncomputer
vision(ECCV),801–818(2018).
[59] Lee,P.,Goldberg,C.&Kohane,I. TheAIrevolutioninmedicine: GPT-4andbeyond(Pearson,2023).
[60] Achiam,J.etal. GPT-4technicalreport. arXivpreprintarXiv:2303.08774(2023).
[61] MasseyJr,F.J. TheKolmogorov-Smirnovtestforgoodnessoffit. JournaloftheAmericanstatistical
Association46,68–78(1951).
[62] Canny, J. A computational approach to edge detection. IEEE Transactions on pattern analysis and
machineintelligence679–698(1986).
[63] Viola,P.&Jones,M.Rapidobjectdetectionusingaboostedcascadeofsimplefeatures.InProceedings
of the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR
2001,vol.1,I–I(Ieee,2001).
[64] Girshick,R.,Donahue,J.,Darrell,T.&Malik,J. Richfeaturehierarchiesforaccurateobjectdetection
and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern
recognition,580–587(2014).16
a b Number of image-mask-description
triples per modality in BiomedParseData
c
Image encoder
Mask
decoder
GPT-4
Text encoder Neoplas�c cells
Meta-object
“Pathological cells classifier Brain anatomies (0.5%)
of neoplas�c origin Histological structure (92%)
in the liver �ssue”
Fluid disturbance (0.2%)
d (cid:38)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:87)(cid:76)(cid:86)(cid:86)(cid:88)(cid:72)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86)(cid:3) (cid:44)(cid:81)(cid:73)(cid:79)(cid:68)(cid:80)(cid:80)(cid:68)(cid:87)(cid:82)(cid:85)(cid:92)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86)(cid:3) e
(cid:40)(cid:83)(cid:76)(cid:87)(cid:75)(cid:72)(cid:79)(cid:76)(cid:68)(cid:79)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86) (cid:49)(cid:72)(cid:82)(cid:83)(cid:79)(cid:68)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86) (cid:37)(cid:72)(cid:81)(cid:76)(cid:74)(cid:81) (cid:48)(cid:68)(cid:79)(cid:76)(cid:74)(cid:81)(cid:68)(cid:81)(cid:87)
(cid:37)(cid:76)(cid:82)(cid:80)(cid:72)(cid:71)Parse(cid:3) PubM(cid:72)(cid:71)BERT (cid:37)(cid:76)(cid:82)(cid:80)(cid:72)(cid:71)Parse (cid:41)(cid:82)(cid:70)(cid:68)(cid:79)
(cid:54)(cid:76)(cid:79)(cid:75)(cid:82)(cid:88)(cid:72)(cid:87)(cid:87)(cid:72)(cid:3)(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:29)(cid:3)(cid:3)(cid:19)(cid:17)(cid:27)(cid:27)(cid:27) (cid:54)(cid:76)(cid:79)(cid:75)(cid:82)(cid:88)(cid:72)(cid:87)(cid:87)(cid:72)(cid:3)(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:29)(cid:3)(cid:3)(cid:19)(cid:17)(cid:19)(cid:19)(cid:21) (cid:54)(cid:76)(cid:79)(cid:75)(cid:82)(cid:88)(cid:72)(cid:87)(cid:87)(cid:72)(cid:3)(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:29)(cid:3)(cid:3)(cid:19)(cid:17)(cid:23)(cid:24)(cid:27)(cid:3) (cid:54)(cid:76)(cid:79)(cid:75)(cid:82)(cid:88)(cid:72)(cid:87)(cid:87)(cid:72)(cid:3)(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:29)(cid:3)(cid:3)(cid:19)(cid:17)(cid:20)(cid:26)(cid:24)
(cid:56)(cid:48)(cid:36)(cid:51)(cid:3)(cid:20) (cid:56)(cid:48)(cid:36)(cid:51)(cid:3)(cid:20) (cid:56)(cid:48)(cid:36)(cid:51)(cid:3)(cid:20) (cid:56)(cid:48)(cid:36)(cid:51)(cid:3)(cid:20)
Figure1: OverviewofBiomedParseandBiomedParseData. a, TheGPT-4constructedontologyshowing
a hierarchy of object types that are used to unify semantic concepts across datasets. Bar plots showing the
numberofimagescontainingthatobjecttype. Linesbetweenbarsshowingtheobjecttypesimilarityinthe
textembeddingspace. b,Barplotshowingthenumberofimage-mask-descriptiontriplesforeachmodality
inBiomedParseData. c,FlowchatofBiomedParse. BiomedParsetakesanimageandatextpromptasinput
and then outputs the segmentation masks for the objects specified in the prompt. Image-specific manual
interactionsuchasboundingboxorclicksisnotrequiredinourframework. Tofacilitatesemanticlearning
fortheimageencoder,BiomedParsealsoincorporatesalearningobjectivetoclassifythemeta-objecttype.
Forevaluation,GPT-4isusedtoresolvetextpromptintoobjecttypesusingtheobjectontology,whichalso
uses the meta-object type output from BiomedParse to narrow down candidate semantic labels. d, UMAP
plots contrasting the text-embeddings for different cell types derived from BiomedParse text encoder (left)
andPubMedBERT(right). e,UMAPplotscontrastingtheimageembeddingsfordifferentcelltypesderived
fromBiomedParseimageencoder(left)andFocal(right).
(cid:21)(cid:3)(cid:51)(cid:36)(cid:48)(cid:56) (cid:21)(cid:51)(cid:36)(cid:48)(cid:56) (cid:21)(cid:51)(cid:36)(cid:48)(cid:56) (cid:21)(cid:51)(cid:36)(cid:48)(cid:56)17
a (cid:37)(cid:76)(cid:82)(cid:80)(cid:72)(cid:71)Parse(cid:3) (cid:48)(cid:72)(cid:71)(cid:54)(cid:36)(cid:48) (oracle box) MedSAM ((cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)(cid:12) c (cid:37) (cid:48)(cid:76) (cid:72)(cid:82) (cid:71)(cid:80) (cid:54)(cid:72) (cid:36)(cid:71) (cid:48)Parse(cid:3) (cid:54)(cid:36)(cid:48)
(cid:54)(cid:36)(cid:48) (oracle box) SAM ((cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)(cid:12)
(cid:13)(cid:13)(cid:13)(cid:13)
(cid:20)(cid:17)(cid:19) (cid:13)(cid:13)(cid:13)(cid:13) (cid:13)(cid:13) (cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)(cid:13)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:27)
(cid:19)(cid:17)(cid:28)
(cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:27)
(cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:26)
(cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:24)
(cid:36)(cid:79)(cid:79)(cid:3)(cid:11)(cid:81)(cid:32)(cid:20)(cid:19)(cid:21)(cid:27)(cid:24)(cid:24)(cid:12) (cid:38)(cid:55)(cid:3)(cid:11)(cid:81)(cid:32)(cid:23)(cid:24)(cid:22)(cid:19)(cid:25)(cid:12) (cid:48)(cid:53)(cid:44)(cid:3)(cid:11)(cid:81)(cid:32)(cid:22)(cid:19)(cid:28)(cid:28)(cid:19)(cid:12) (cid:59)(cid:16)(cid:53)(cid:68)(cid:92)(cid:3)(cid:11)(cid:81)(cid:32)(cid:20)(cid:22)(cid:27)(cid:23)(cid:19)(cid:12) (cid:51)(cid:68)(cid:87)(cid:75)(cid:82)(cid:79)(cid:82)(cid:74)(cid:92)(cid:3)(cid:11)(cid:81)(cid:32)(cid:28)(cid:26)(cid:26)(cid:12) (cid:56)(cid:79)(cid:87)(cid:85)(cid:68)(cid:86)(cid:82)(cid:88)(cid:81)(cid:71)(cid:3)(cid:11)(cid:81)(cid:32)(cid:20)(cid:19)(cid:20)(cid:27)(cid:23)(cid:12) (cid:41)(cid:88)(cid:81)(cid:71)(cid:88)(cid:86)(cid:3)(cid:11)(cid:81)(cid:32)(cid:27)(cid:19)(cid:19)(cid:12) (cid:40)(cid:81)(cid:71)(cid:82)(cid:86)(cid:70)(cid:82)(cid:83)(cid:72)(cid:3)(cid:11)(cid:81)(cid:32)(cid:23)(cid:20)(cid:19)(cid:12) (cid:39)(cid:72)(cid:85)(cid:80)(cid:82)(cid:86)(cid:70)(cid:82)(cid:83)(cid:92)(cid:3)(cid:11)(cid:81)(cid:32)(cid:25)(cid:24)(cid:12) (cid:50)(cid:38)(cid:55)(cid:3)(cid:11)(cid:81)(cid:32)(cid:21)(cid:27)(cid:22)(cid:12) (cid:50)(cid:81)(cid:72)(cid:3)(cid:83)(cid:72)(cid:85)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)(cid:86)(cid:72)(cid:87) (cid:50)(cid:81)(cid:72)(cid:3)(cid:83)(cid:72)(cid:85)(cid:3)(cid:76)(cid:80)(cid:68)(cid:74)(cid:72) (cid:50)(cid:81)(cid:72)(cid:3)(cid:83)(cid:72)(cid:85)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)
b
COVID-19 infection Lower-grade glioma COVID-19 infection Glandular structure Benign tumor in Optic disc in “Non-neoplastic polyp Melanoma in Cystoid macular
in chest CT in brain MRI in chest X-Ray in colon pathology breast ultrasound retinal fundus icolon endoscope skindermoscopy edema in OCT
Dice: 0.93 Dice: 0.97 Dice: 0.93 Dice: 0.97 Dice: 0.97 Dice: 0.97 Dice: 0.92 Dice: 0.98 Dice: 0.91
d Neoplastic cells Inflammatory cells Connective tissue Tumor core in Enhancing tumor e BiomedParse MedSAM MedSAM
in liver pathology in liver pathology cells in liver pathology brain MRI in brain MRI “Glandular structure” 1 box for all cells 22 boxes for ind. cells
Dice: 0.93 Dice: 0.88 Dice: 0.89 Dice: 0.97 Dice: 0.95 Dice: 0.96 Dice: 0.74 Dice: 0.95
Dice: 0.70 Dice: 0.11 Dice: 0.31 Dice: 0.88 Dice: 0.72 “Glandular structure” 1 box for all cells 5 boxes for ind. cells
Dice: 0.96 Dice: 0.53 Dice: 0.95
f g h i
(cid:51)(cid:85)(cid:72)(cid:70)(cid:76)(cid:86)(cid:76)(cid:82)(cid:81) (cid:53)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79) (cid:41)(cid:16)(cid:20)(cid:3)(cid:54)(cid:70)(cid:82)(cid:85)(cid:72) (cid:36)(cid:56)(cid:53)(cid:50)(cid:38)
(cid:20)(cid:19)(cid:19) (cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19)
(cid:20)(cid:20) (cid:19)(cid:19) (cid:23)(cid:21)
(cid:19)(cid:20) (cid:17)(cid:17) (cid:28)(cid:19)
(cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:27)
(cid:50)(cid:38)(cid:55)(cid:3)(cid:11)(cid:81)(cid:32)(cid:21)(cid:27)(cid:22)(cid:12)
(cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25)
(cid:20)(cid:19)(cid:25)
(cid:19)(cid:17)(cid:26)
(cid:20)(cid:19)(cid:27) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:23)
(cid:20)(cid:19)(cid:20)(cid:19) (cid:19)(cid:17)(cid:24) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:21)
(cid:20)(cid:19)(cid:20)(cid:21) (cid:44)(cid:81)(cid:89)(cid:68)(cid:79)(cid:76)(cid:71)(cid:3)(cid:76)(cid:81)(cid:83)(cid:88)(cid:87) (cid:57)(cid:68)(cid:79)(cid:76)(cid:71)(cid:3)(cid:76)(cid:81)(cid:83)(cid:88)(cid:87) (cid:19)(cid:17) (cid:20)(cid:23) (cid:19)(cid:25) (cid:20)(cid:19)(cid:24) (cid:20)(cid:19)(cid:23) (cid:20)(cid:19)(cid:22) (cid:20)(cid:19)(cid:21) (cid:20)(cid:19)(cid:20) (cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:27) (cid:20)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:27) (cid:20)(cid:17)(cid:19)
(cid:83)(cid:16)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:3)(cid:87)(cid:75)(cid:85)(cid:72)(cid:86)(cid:75)(cid:82)(cid:79)(cid:71) (cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50) (cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)
Figure 2: Comparison on large-scale biomedical image segmentation datasets. a, Bar plot comparing
the Dice score between our method and competing methods on 102,855 test instances (image-mask-label
triples)across9modalities. MedSAMandSAMrequireboundingboxasinput. Weconsidertwosettings:
oracle bounding box (minimum bounding box covering the gold mask); bounding boxes generated from
thetextprompt byGroundingDINO,a state-of-the-arttext-basedgroundingmodel. ndenotesthenumber
of test instances in the corresponding modality. ∗ indicates the significance level at which BiomedParse
outperformsthebest-competingmethod,withWilcoxontestp-value< 1×10−2 for**,p-value< 1×10−3
for***,p-value< 1×10−4for****. b,NineexamplescomparingthesegmentationresultsbyBiomedParse
and the ground truth, using just the text prompt at the top. c, Bar plot comparing the Dice score between
our method and competing methods on a cell segmentation test set with 42 images. BiomedParse requires
onlyasingleuseroperation(thetextprompt“Glandularstructureincolonpathology”). Bycontrast,toget
competitiveresults,MedSAM/SAMrequire430operations(oneboundingboxperanindividualcell).
esraPdemoiB
hturt
dnuorG
esraPdemoiB
MASdeM
(cid:72)(cid:88)(cid:79)(cid:68)(cid:89)(cid:16)(cid:83)(cid:3)(cid:87)(cid:86)et(cid:3)(cid:54)(cid:16)(cid:46)
(cid:72)(cid:85)(cid:82)(cid:70)(cid:54)(cid:3)(cid:72)(cid:70)(cid:76)(cid:39)
(cid:72)(cid:85)(cid:82)(cid:70)(cid:54)
esraP(cid:71)(cid:72)(cid:80)(cid:82)(cid:76)(cid:37)
(cid:72)(cid:85)(cid:82)(cid:70)(cid:86)(cid:3)(cid:72)(cid:70)(cid:76)(cid:39)
esraP(cid:71)(cid:72)(cid:80)(cid:82)(cid:76)(cid:37)18
d, Five examples contrasting the segmentation results by BiomedParse and MedSAM, along with text
prompts used by BiomedParse and bounding boxes used by MedSAM. e, Comparison between Biomed-
Parse and MedSAM on a benign tumor image (top) and a malignant tumor image (bottom). The improve-
mentofBiomedParseoverMedSAMisevenmorepronouncedonabnormalcellswithirregularshapes. f,
Bar plot comparing the K-S test p-values between valid text prompt and invalid text prompt. BiomedParse
learns to reject invalid text prompts describing object types not present in the image (small p-value). g,
Plot showing the precision and recall of our method on detecting invalid text prompts across different K-S
test p-value cutoff. h,i, Scatter plots comparing the AUROC (h) and F-1 (i) between BiomedParse and
GroundingDINOondetectinginvaliddescriptions.19
a Liverin abdomenCT Kidneysin abdomenCT Inflammatorycells inpathology Optic disc in retinal fundus Benigntumor in breastUS
LivervesselinabdomenCT Pancreasin abdomenCT Neoplasticcells inpathology Prostate transitional zone in MRI Malignanttumor in breastUS
b c d
(cid:38)(cid:82)(cid:81)(cid:89)(cid:72)(cid:91)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82) (cid:37)(cid:82)(cid:91)(cid:3)(cid:53)(cid:68)(cid:87)(cid:76)(cid:82) (cid:44)(cid:81)(cid:89)(cid:72)(cid:85)(cid:86)(cid:72)(cid:71)(cid:3)(cid:53)(cid:82)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:68)(cid:79)(cid:3)(cid:44)(cid:81)(cid:72)(cid:85)(cid:87)(cid:76)(cid:68)
e Left heart ventricle Prostate transitional zone Neoplastic cells Lung nodule Inflammatory cells Livervessel
Convexratio: 0.97 Convexratio: 0.70 Convexratio: 0.53 Convexratio: 0.13 Convexratio: 0.10 Convexratio: 0.07
Dice: 0.98 Dice: 0.93 Dice: 0.93 Dice: 0.86 Dice: 0.88 Dice: 0.85
Dice: 0.96 Dice: 0.80 Dice: 0.70 Dice: 0.02 Dice: 0.11 Dice: 0.08
f g h (cid:37)(cid:76)(cid:82)(cid:80)(cid:72)(cid:71)Parse (cid:48)(cid:72)(cid:71)(cid:54)(cid:36)(cid:48) (cid:54)(cid:36)(cid:48)
(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)(cid:13)
(cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:27)
(cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19)
(cid:48)(cid:72)(cid:71)(cid:54)(cid:36)(cid:48)(cid:3)(cid:69)(cid:72)(cid:81)(cid:70)(cid:75)(cid:80)(cid:68)(cid:85)(cid:78)(cid:3) BiomedParseData (cid:48)(cid:72)(cid:71)(cid:54)(cid:36)(cid:48)(cid:3)(cid:69)(cid:72)(cid:81)(cid:70)(cid:75)(cid:80)(cid:68)(cid:85)(cid:78)(cid:3) (cid:37)(cid:76)(cid:82)medParseData (cid:48)(cid:72)(cid:71)(cid:54)(cid:36)(cid:48)(cid:3)(cid:69)(cid:72)(cid:81)(cid:70)(cid:75)(cid:80)(cid:68)(cid:85)(cid:78)(cid:3) (cid:37)(cid:76)(cid:82)medParseData
Figure 3: Evaluation on detecting irregular-shaped objects. a, Attention maps of text prompts for
irregular-shaped objects, suggesting that BiomedParse learns rather faithful representation of their typi-
cal shapes. b-d, Scatter plots comparing the improvement in Dice score for BiomedParse over MedSAM
withshaperegularityintermsofconvexratio(b),boxratio(c),andinversedrotationalinertia(d). Smaller
number in x-axis means higher irregularity in average. Each dot is an object type. e, Six examples con-
trastingBiomedParseandMedSAMondetectingirregular-shapedobjects. Plotsareorderedfromtheleast
irregular one (left) to the most irregular one (right). f,g Comparison between BiomedParseData and the
benchmark dataset used by MedSAM in terms of convex ratio (f) and box ratio (g). BiomedParseData is
a more faithful representation of real-world challenges in terms of irregular-shaped objects. h, Bar plots
comparing BiomedParse and competing approaches on BiomedParseData and the benchmark dataset used
by MedSAM. BiomedParse has a larger improvement on BiomedParseData, which contains more diverse
imagesandmoreirregular-shapedobjects.
(cid:48)(cid:36)(cid:54)(cid:71)(cid:72)(cid:48)(cid:3)(cid:85)(cid:72)(cid:89)(cid:82)(cid:3)(cid:87)(cid:81)(cid:72)(cid:80)(cid:72)(cid:89)(cid:82)(cid:85)(cid:83)(cid:80)(cid:44)
esraPdemoiB
MASdeM
(cid:82)(cid:76)(cid:87)(cid:68)(cid:53)(cid:3)(cid:91)(cid:72)(cid:89)(cid:81)(cid:82)(cid:38)
(cid:48)(cid:36)(cid:54)(cid:71)(cid:72)(cid:48)(cid:3)(cid:85)(cid:72)(cid:89)(cid:82)(cid:3)(cid:87)(cid:81)(cid:72)(cid:80)(cid:72)(cid:89)(cid:82)(cid:85)(cid:83)(cid:80)(cid:44)
(cid:82)(cid:76)(cid:87)(cid:68)(cid:53)(cid:3)(cid:91)(cid:82)(cid:37)
(cid:48)(cid:36)(cid:54)(cid:71)(cid:72)(cid:48)(cid:3)(cid:85)(cid:72)(cid:89)(cid:82)(cid:3)(cid:87)(cid:81)(cid:72)(cid:80)(cid:72)(cid:89)(cid:82)(cid:85)(cid:83)(cid:80)(cid:44)
(cid:72)(cid:85)(cid:82)(cid:70)(cid:54)(cid:3)(cid:72)(cid:70)(cid:76)(cid:39)20
a
Abdomen CT (Dice: 0.98) Abdomen MRI (Dice: 0.96) Cardiac MRI (Dice: 0.96)
(cid:54)(cid:83)(cid:79)(cid:72)(cid:72)(cid:81) (cid:36)(cid:82)(cid:85)(cid:87)(cid:68)
(cid:53)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:78)(cid:76)(cid:71)(cid:81)(cid:72)(cid:92) (cid:54)(cid:83)(cid:79)(cid:72)(cid:72)(cid:81)
(cid:47)(cid:72)(cid:73)(cid:87)(cid:3)(cid:78)(cid:76)(cid:71)(cid:81)(cid:72)(cid:92) (cid:53)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:78)(cid:76)(cid:71)(cid:81)(cid:72)(cid:92)
(cid:47)(cid:76)(cid:89)(cid:72)(cid:85) (cid:47)(cid:72)(cid:73)(cid:87)(cid:3)(cid:78)(cid:76)(cid:71)(cid:81)(cid:72)(cid:92)
(cid:54)(cid:87)(cid:82)(cid:80)(cid:68)(cid:70)(cid:75)(cid:3) (cid:47)(cid:76)(cid:89)(cid:72)(cid:85) (cid:53)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:75)(cid:72)(cid:68)(cid:85)(cid:87)(cid:3)(cid:89)(cid:72)(cid:81)(cid:87)(cid:85)(cid:76)(cid:70)(cid:79)(cid:72)(cid:3)
(cid:51)(cid:82)(cid:86)(cid:87)(cid:70)(cid:68)(cid:89)(cid:68)(cid:3) (cid:54)(cid:87)(cid:82)(cid:80)(cid:68)(cid:70)(cid:75)(cid:3) (cid:48)(cid:92)(cid:82)(cid:70)(cid:68)(cid:85)(cid:71)(cid:76)(cid:88)(cid:80)
(cid:51)(cid:68)(cid:81)(cid:70)(cid:85)(cid:72)(cid:68)(cid:86) (cid:51)(cid:82)(cid:86)(cid:87)(cid:70)(cid:68)(cid:89)(cid:68)(cid:3) (cid:47)(cid:72)(cid:73)(cid:87)(cid:3)(cid:75)(cid:72)(cid:68)(cid:85)(cid:87)(cid:3)(cid:89)(cid:72)(cid:81)(cid:87)(cid:85)(cid:76)(cid:70)(cid:79)(cid:72)
(cid:53)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:68)(cid:71)(cid:85)(cid:72)(cid:81)(cid:68)(cid:79)(cid:3)(cid:74)(cid:79)(cid:68)(cid:81)(cid:71)(cid:3) (cid:51)(cid:68)(cid:81)(cid:70)(cid:85)(cid:72)(cid:68)(cid:86)
(cid:47)(cid:72)(cid:73)(cid:87)(cid:3)(cid:68)(cid:71)(cid:85)(cid:72)(cid:81)(cid:68)(cid:79)(cid:3)(cid:74)(cid:79)(cid:68)(cid:81)(cid:71)(cid:3) (cid:53)(cid:76)(cid:74)(cid:75)(cid:87)(cid:3)(cid:68)(cid:71)(cid:85)(cid:72)(cid:81)(cid:68)(cid:79)(cid:3)(cid:74)(cid:79)(cid:68)(cid:81)(cid:71)(cid:3)
(cid:36)(cid:82)(cid:85)(cid:87)(cid:68) (cid:47)(cid:72)(cid:73)(cid:87)(cid:3)(cid:68)(cid:71)(cid:85)(cid:72)(cid:81)(cid:68)(cid:79)(cid:3)(cid:74)(cid:79)(cid:68)(cid:81)(cid:71)
Pancreas Pathology (Dice: 0.91) Breast Pathology (Dice: 0.88) Kidney CT (Dice: 0.96)
(cid:49)(cid:72)(cid:82)(cid:83)(cid:79)(cid:68)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86)(cid:3) (cid:44)(cid:81)(cid:73)(cid:79)(cid:68)(cid:80)(cid:80)(cid:68)(cid:87)(cid:82)(cid:85)(cid:92)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86)(cid:3) (cid:55)(cid:88)(cid:80)(cid:82)(cid:85)(cid:3)
(cid:44)(cid:81)(cid:73)(cid:79)(cid:68)(cid:80)(cid:80)(cid:68)(cid:87)(cid:82)(cid:85)(cid:92)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86)(cid:3) (cid:38)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86)(cid:3) (cid:46)(cid:76)(cid:71)(cid:81)(cid:72)(cid:92)(cid:3)(cid:70)(cid:92)(cid:86)(cid:87)(cid:3)
(cid:38)(cid:82)(cid:81)(cid:81)(cid:72)(cid:70)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86) (cid:40)(cid:83)(cid:76)(cid:87)(cid:75)(cid:72)(cid:79)(cid:76)(cid:68)(cid:79)(cid:3)(cid:70)(cid:72)(cid:79)(cid:79)(cid:86) (cid:46)(cid:76)(cid:71)(cid:81)(cid:72)(cid:92)
b (cid:51)(cid:85)(cid:72)(cid:70)(cid:76)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) c (cid:53)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)(cid:3) d (cid:41)(cid:16)(cid:20)(cid:3)(cid:54)(cid:70)(cid:82)(cid:85)(cid:72)
(cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:27) (cid:19)(cid:17)(cid:27)
(cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:27) (cid:20)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:27) (cid:20)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:27) (cid:20)(cid:17)(cid:19)
(cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50) (cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50) (cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)
f e (cid:37)(cid:76)(cid:82)(cid:80)(cid:72)(cid:71)Parse (cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)
(cid:20)(cid:17)(cid:19)
(cid:37)(cid:76)(cid:82)(cid:80)(cid:72)(cid:71)Parse MedSAM ((cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)(cid:12)
(cid:19)(cid:17)(cid:27)
SAM ((cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)(cid:12)
(cid:19)(cid:17)(cid:25)
(cid:13)(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)(cid:13) (cid:13)(cid:13)(cid:13)(cid:13) (cid:13) (cid:19)(cid:17)(cid:23)
(cid:20)(cid:17)(cid:19)
(cid:13)(cid:13)(cid:13)(cid:13) (cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:27) (cid:21) (cid:23) (cid:25) (cid:27) (cid:20)(cid:19) (cid:20)(cid:21) (cid:20)(cid:23)
(cid:49)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)object types(cid:3)(cid:76)(cid:81)(cid:3)(cid:76)(cid:80)(cid:68)(cid:74)(cid:72)
g
(cid:37)(cid:76)(cid:82)(cid:80)(cid:72)(cid:71)Parse SAM ((cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)(cid:12)
(cid:19)(cid:17)(cid:25)
MedSAM ((cid:42)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:39)(cid:44)(cid:49)(cid:50)(cid:12)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:27)
(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:19) (cid:19)(cid:17)(cid:19)
(cid:21) (cid:23) (cid:25) (cid:27) (cid:20)(cid:19) (cid:20)(cid:21) (cid:20)(cid:23)
CT MRI X-Ray Pathology Ultrasound Fundus (cid:49)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)object types(cid:3)(cid:76)(cid:81)(cid:3)(cid:76)(cid:80)(cid:68)(cid:74)(cid:72)
Figure4: Evaluationonobjectrecognition. a,Sixexamplesshowingtheresultsofobjectrecognitionby
ourmethod. Objectrecognitionidentifiesandsegmentsallobjectsinanimagewithoutrequiringanyuser-
providedinputprompt. b-d, ScatterplotscomparingtheF1(b),Precision(c),andRecall(d)scoresbetween
BiomedParseandGroundingDINOonidentifyingobjectspresentedintheimage. e, Comparisonbetween
BiomedParse and Grounding DINO on object identification in terms of median F-1 score across different
numbersofobjectsintheimage. f, BarplotcomparingBiomedParseandMedSAM/SAM(usingbounding
boxesgeneratedbyGroundingDINO)onend-to-endobjectrecognition(includingsegmentation)inrelation
to various modalities. g, Comparison between BiomedParse and MedSAM/SAM (using bounding boxes
generated by Grounding DINO) on end-to-end object recognition (including segmentation) in relation to
numbersofdistinctobjectsintheimage.
(cid:72)(cid:85)(cid:82)(cid:70)(cid:54)(cid:3)(cid:72)(cid:70)(cid:76)(cid:39)
esraP(cid:71)(cid:72)(cid:80)(cid:82)(cid:76)(cid:37) esraP(cid:71)(cid:72)(cid:80)(cid:82)(cid:76)(cid:37)
(cid:72)(cid:85)(cid:82)(cid:70)(cid:54)(cid:3)(cid:20)(cid:16)(cid:41)
(cid:72)(cid:85)(cid:82)(cid:70)(cid:86)(cid:3)(cid:72)(cid:70)(cid:76)(cid:39)
esraP(cid:71)(cid:72)(cid:80)(cid:82)(cid:76)(cid:37)21
Pathologist annotation BiomedParse annotation
a b
Tumor region Stroma Neoplastic cells Inflammatory cells
Connective tissue cells
c d
Lymphocyte Neoplastic cells Inflammatory cells
Connective tissue cells
e f
Tumor region Neoplastic cells Inflammatory cells
Connective tissue cells
Figure 5: Evaluation of BiomedParse on real-world cell segmentation examples. a-f, De-identified
pathology images from Providence Health System are used to compare the pathologist annotations (a,c,e)
and the annotations from BiomedParse (b,d,f). We show the exact pathologist outputs, including ob-
ject names (e.g., lymphocyte, stroma) and object locations, as well as the exact outputs by BiomedParse.
BiomedParse does not need any user-provided inputs and can identify and segment cells of any types in-
cludedintheontology.22
System message
“You are a medical expert to help researcher in machine learning to
develop AI models. Your responses should be rigorous,
comprehensive, organized, truthful and logical.”
Prompt to GPT-4
“I want to generate a variety of text prompts to train a mul modal
model for medical image segmenta on tasks. Each text prompt is a
phrase describing the segmenta on target in an image of certain
modality at a specific site, e.g.
"[Object Type]in [Site] [Modality]([sub-modality])".
I want some diversity in my data by rephrasing the descrip on of the
segmenta on target. For each text prompt, please make sure that
the [Object Type]is always the subject of the phrase. Please
rephrase in a biomedical professional, precise, and accurate manner.
Can you help me generate mul ple different phrases of text prompt
similar to "[Object Type]in [Site] [Modality]([sub-modality])"?”
List of text prompts generated by GPT-4
Example input:
Object type: Inflammatory cells
Modality: Pathology
Site: Breast
Output of GPT-4:
"inflammatory cells in breast pathology"
"Pathological inflamma on cells within the breast  ssue"
"Inflammatory cellular presence in breast pathology"
"Breast  ssue exhibi ng inflammatory cells"
"Inflammatory cell infiltra on in breast pathology"
Supplementary Figure 1: GPT-4 prompt that is used to generate diverse descriptions for a given image
accordingtoitsobjecttype,imagemodality,andanatomicsite.23
Retinal Cardiac Brain
Colon
Other
OCT
Prostate
Abdomen
MRI
Retinal
FundS uki sn
DS
105
104
10310E
5
1n 021d 04o 1s 0c 3o 1p 02e 10 15 0 14
0 13
02
1 1
1
10 0
0
05 4
3
2
102103104105
Abdomen Liver
Pelvis
Lung
Chest X-ray 150 140 130
120
C T Chest
men
5 10 4 10 3 10 2 10
Pancreas
TransperineAbdo
al Cardiac
Breast
U
ltrasou
n d
105104103102
120 130 140 150
Breast
HeartColon
Colon
Liver
Lung
Testis Kidney
Cervix
Pathology
Prostate
Skin Bladder
Thyroid
Uterus
Pancreatic Esophagus
Ovarian
Stomach
Supplementary Figure 2: Number of images in each of the 25 anatomic sites from 9 modalities. One
anatomicsitecouldpresentinmultiplemodalities.24
Object
New BiomedicalTask type:
Infec on
Seed Prompt:
“Infec on in chest X-Ray”
Modality: Site:
Image: Label: X-Ray Chest
Chest X-Ray COVID-19
Rich clinical language
GPT-4
"Infec on detected in chest radiography"
Varia on
"Chest X-Ray revealing infec on",
"Infec on present in thoracic X-Ray",
"infected region in chest X-Ray“ Structured
“lesion in chest X-Ray" Varia on
Supplementary Figure 3: Generating textual description for each object in each image. Object type,
modality, and site are extracted from the metadata or the data description. We utilized both GPT-4 and
structuredbiomedicalconceptstogeneraterichvariationsofclinicallanguage,increasingtherobustnessof
BiomedParsetouser-providedtext.
BiomedParse BiomedParse-SAM BiomedParse-PubMedBERT
1.05 **** ** ***
*** **
0.90
0.75
0.60
0.45
All
(n=102855)
CT
(n=45306)
MRI
(n=30990)
X-Ray
(n=13840)
Pathology
(n=97 Ul7 t)
rasound
(n=10184 F)
undus
(n=800)
Endoscope
(n=410 D)
ermoscopy
(n=65)
OCT
(n=283)
Supplementary Figure 4: Ablation studies comparing the performance of BiomedParse and two variants.
BiomedParse-SAM stands for using SAM to initialize the image encoder. BiomedParse-PubmedBERT
stands for using the frozen PubmedBERT [18] as the text encoder. n denotes the number of images
in the corresponding modality. ∗ indicates the significance level at which BiomedParse outperforms the
best-competing method, with Wilcoxon test p-value< 1 × 10−2 for **, p-value< 1 × 10−3 for ***, p-
value< 1×10−4 for****.
erocS
eciD25
BiomedParse MedSAM (oracle box) nnU-Net (oracle box)
SAM (oracle box) DeepLabV3+ (oracle box)
** ** *
1.05
0.90
0.75
0.60
0.45
All
(n=58023)
CT
(n=29255)
MRI
(n=11701)
X-Ray
(n=5945)
Pathology
(n=42)
Ultrasound
(n=10056)
Fundus
(n=800)
Endoscope
(n=200)
Dermoscopy
(n=24)
Supplementary Figure 5: Comparison between BiomedParse and competing methods on the MedSAM
benchmark. We evaluated MedSAM and SAM using the ground truth bounding box for the segmentation.
For nnU-Net and DeepLabV3+, we reported the evaluation reported by MedSAM [9]. Results are shown
by imaging modality, with statistical significance comparison between BiomedParse and best-competing
methodMedSAM.∗indicatesthesignificancelevelatwhichBiomedParseoutperformsthebest-competing
method,withWilcoxontestp-value< 5×10−2 for*,p-value< 1×10−3 for**.
0.60 0.60 0.60
R2 = 0.76 R2 = 0.76 R2 = 0.68
p=7.1e 21 p=4.1e 21 p=5.6e 17
0.45 y= 0.80x+0.63 0.45 y= 0.57x+0.63 0.45 y= 0.57x+0.56
0.30 0.30 0.30
0.15 0.15 0.15
0.00 0.00 0.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Box Ratio Convex Ratio Inversed Rotational Inertia
SupplementaryFigure6: ScatterplotscomparingtheimprovementofBiomedParseoverSAMwithshape
irregularity in terms of box ratio (left), convex ratio (middle), and inversed rotational inertia (right). Each
dotrepresentsthemedianstatisticsoveroneobjecttypeinoursegmentationontology.
MAS
revo
tnemevorpmI
erocS
eciD
MAS
revo
tnemevorpmI
MAS
revo
tnemevorpmI26
1.0
0.8
0.6
0.4
0.2
0.0
MedSAM benchmark BiomedParseData
Supplementary Figure 7: Violin plot comparing the inversed rotational inertia between MedSAM bench-
markdataandBiomedParseData. Ahigherinversedrotationalinertiaindicateslessirregularity.
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
MedSAM (Grounding DINO) Dice SAM (Grounding DINO) Dice
SupplementaryFigure8: Evaluationonobjectrecognition.a,b,Densityplotscomparingtheperformance
onobjectrecognitionbetweenBiomedParseandMedSAM(GroundingDINO)a,andbetweenBiomedParse
andSAM(GroundingDINO)b.
eciD
esraPdemoiB
aitrenI
lanoitatoR
desrevnI
eciD
esraPdemoiB