1
Code-mixed Sentiment and Hate-speech Prediction
Anjali Yadav, Tanya Garg, Matej Klemen, Matej Ulcˇar,
Basant Agarwal, Marko Robnik-Sˇikonja
Abstract—Code-mixed discourse combines multiple languages text, and those pre-trained on informal texts, which are the
in a single text. It is commonly used in informal discourse most frequently used in code-mixed scenarios. While several
in countries with several official languages, but also in many
works applied LLMs to tasks with code-mixed languages (see
other countries in combination with English or neighboring
SectionIIforanoverview),theexistinganalyseswerelimited
languages. As recently large language models have dominated
most natural language processing tasks, we investigated their toindividualproblemsandpairsofcode-mixedlanguages.We
performance in code-mixed settings for relevant tasks. We first fill this gap and conduct an analysis, covering multiple types
created four new bilingual pre-trained masked language models of LLMs on five languages (French, Hindi, Russian, Slovene,
for English-Hindi and English-Slovene languages, specifically
and Tamil). We tackle two affective computing tasks where
aimed to support informal language. Then we performed an
code-mixing is especially prominent: sentiment analysis and
evaluation of monolingual, bilingual, few-lingual, and massively
multilingual models on several languages, using two tasks that hate speech detection.
frequently contain code-mixed text, in particular, sentiment Analyses of sentiment and hate speech detection in a code-
analysis and offensive language detection in social media texts. mixed setting are particularly relevant for affective comput-
Theresultsshowthatthemostsuccessfulclassifiersarefine-tuned ing due to increased cultural and linguistic diversity in a
bilingual models and multilingual models, specialized for social
globalized world. In this context, modern affective comput-
media texts, followed by non-specialized massively multilingual
ing systems must handle and interpret inputs from diverse
and monolingual models, while huge generative models are
not competitive. For our affective problems, the models mostly linguistic backgrounds, including those where code-mixing is
perform slightly better on code-mixed data compared to non- a natural part of communication. Emotions can be expressed
code-mixed data. differently across languages, and the nuances might change
significantly in code-mixed environments. Models trained on
monolingual data often fail to capture these subtleties. Adapt-
I. INTRODUCTION
ing them to handle code-mixed text might improve their
Code-mixing, the practice of combining multiple languages performance, robustness, user experience, and inclusiveness,
orvarietiesinasinglediscourse,iscommonintoday’sworld. especially in informal settings. Our study builds upon the
This phenomenon is observed in bilingual and multilingual growing interest in sentiment analysis of code-mixed text. A
communities, and influenced by informal settings, cultural recent study introduced robust transformer-based algorithms
identity, lack of vocabulary, media and pop culture, globaliza- to enhance sentiment prediction in code-mixed text, specifi-
tion, and the digital era [50]. In bilingual households, people callyfocusingonEnglishandRomanUrducombinations.[23]
often switch between languages based on context or personal The study employed state-of-the-art transformer-based mod-
preference.Ininformalsettings,code-mixingismorecommon els like Electra[13], code-mixed BERT (cm-BERT), and
when speaking with friends or expressing ideas or emotions Multilingual Bidirectional and Auto-Regressive Transformers
in a specific language [50]. It can also be a way to express (mBART).[18]
culturalidentityoradapttolinguisticdiversityinmulticultural Theaimofourworkistoinvestigatetheabilitiesofdifferent
environments. The digital era has introduced new avenues for largelanguagemodels,whichnowadaysrepresenttheessential
communication, making code-mixing more prevalent [2]. infrastructureforlanguageanalysis,intheareaofcode-mixed
Analyses of code-mixed data enable understanding of re- sentiment and hate speech.
alistic interactions in real-world communication, which is es- Our main contributions are as follows:
pecially important for understanding sentiment and emotions, • We created four new masked bilingual large language
expressed by people in multilingual environments. Successful models, focused on informal language.
analysis of code-mixed data also provides effective multilin- • We conducted an in-depth analysis of large language
gual insights and enhances decision-making in various do- modelsoncode-mixedlanguageusingtwotypesofrepre-
mains like marketing [27], customer support [46], and policy- sentativeclassificationproblemsinfivelanguages,where
making [47]. such problems are relevant and adequate large language
Recently, pre-trained large language models (LLMs) have models exist. The results show an advantage of bilingual
dominated the landscape of Natural Language Processing modelsandmodelsspecializedforsocialmediatextsover
(NLP).LLMsappearinvarioussizes,from100milliontosev- general massively multilingual and monolingual models.
eral hundred billion parameters, and cover different numbers • We investigated the detection of code-mixed language
of languages, from monolingual to massively multilingual, and observed weaknesses in existing commonly used
supporting a few hundred languages. In the context of code- language detectors.
mixing, two types of LLMs are interesting: those covering The remainder of our paper is structured as follows. In
all the languages that simultaneously appear in a code-mixed refsec:related, we review the related literature on code-mixing
4202
yaM
12
]LC.sc[
1v92921.5042:viXra2
patterns, language models, and code-mixed evaluation. In B. Evaluation in code-mixed settings
SectionIII,wedescribethebackgroundandessentialstatistics
Code-mixedLLMsaretypicallyevaluatedthroughextrinsic
ofdatasetsweuseinourcode-mixedevaluation.InSectionIV,
evaluation, i.e. by evaluating the system on a downstream
we present training of language models used in our study,
task that involves code-mixing. The tasks depend on the end
followed by the description and analysis of the experimental
goal and are either generative or discriminative. Examples of
results. We conclude in Section VI with an overview of the
generative tasks include code-switched text translation and
findings, a discussion on the limitations of the study, and
controlled code-switched text generation [48], [39], while
possible future directions for code-mixed natural language
examples of discriminative tasks include offensive language
processing.
identification, sentiment analysis, and natural language in-
ference [29], [11]. To provide a more general evaluation,
II. RELATEDWORK authors have also released benchmarks spanning multiple
In this section, we overview the related literature on code- tasks [30]. Likely due to its informal setting, the code-mixed
mixing.Initially,wepresentanoverviewofmultilingualLLMs datasets are commonly sourced from social networks (e.g.,
that are suitable for modeling code-mixed language in Sec- Twitter) or other online platforms with comment sections.
tion II-A. We describe the existing resources used to evaluate Despite the ubiquitous nature of code-mixing, the pool of
code-mixed NLP in Section II-B, and studies on code-mixed languages used in code-mixing evaluations is relatively small
evaluation of LLMs in Section II-C. Finally, in Section II-D, andtypicallyinvolvesalanguagecode-mixedwithEnglish,for
we present the existing works connecting affective computing example, one of the Indo-Aryan or Dravidian languages [12];
tasks and code-mixing. alternatively, a higher-resourced language such as Spanish [1]
can also be code mixed with English. However, code-mixing
does not necessarily occur paired with English, and some
A. Multilingual LLMs
resources have been collected for such settings: for example,
Initially, large language models (LLMs) were primarily
the Turkish-German dependency parsing dataset [10], or the
trained on well-resourced languages like English and Chinese
modernstandardArabicandEgyptiandialectalArabicdataset
due to the availability of ample resources. However, there
[17].
wasalimitationinunderstandingtextcontainingmultiplelan-
In our work, we focus on the evaluation of LLMs on two
guagessimultaneously.Manywidelyspokenlanguageslacked
affectivetasks:offensivelanguageidentificationandsentiment
sufficient resources for model training. Multilingual LLMs
analysis. In contrast to existing work, we consider a larger
such as the multilingual BERT [16] and XLM-RoBERTa
pool of languages (five language pairs), including languages
[14] were developed to address this issue. These models
for which code-mixed research is scarce.
have been trained on balanced datasets comprising about 100
languages, aiming to provide better support for multilingual
C. Comprehensive analyses of code-mixing
text understanding and processing.
Similarly, several few-lingual LLMs have been developed, Most works mentioned in Section II-B have focused on
each trained on a limited number of languages. Examples evaluating and optimizing LLMs for a specific code-mixed
include CroSloEngual BERT (supporting Croatian, Slovene, task or a small pool of tasks in one language. Several works
and English), FinEst BERT (trained on Finnish, Estonian, have instead focused on a larger-scale evaluations, which we
and English) [55], and MuRIL [28] (supporting 17 Indian focus on in our work as well.
languages) Winata et al. [57] study the effectiveness of multilingual
Forunderstandingcodemixing,modelsneedtocomprehend LLMs on code-mixed named entity recognition and part of
how different languages are interchangeably occurring within speechtaggingacrossthreelanguagepairsusingthreecriteria:
a single sentence, typically in informal discourse and often inference speed, performance, and number of parameters.
used in an affective context. While multilingual models are Zhang et al. [58] perform a similar analysis, comparing the
pre-trained on general language data (e.g., Wikipedia dumps), performance of few-hundred million parameters fine-tuned
code-mixing presents a specialized challenge. An alternative massively multilingual LLMs to the performance of multi-
anddirectwayofimprovingthecode-mixedperformanceisby billion parameters models in zero-shot and few-shot settings.
pre-trainingmodelsdirectlyoncode-mixedtexts.Forexample, They find that while large models are relatively successful,
Nayak and Joshi release multiple transformer-based models fine-tuned smaller models achieve the best results. Santy et
trainedonacarefullycuratedHindi-Englishcorpus[41].Such al. [45] study the effect of using different types of synthetic
modelsareaimedathandlingthespecificsofcode-mixingbut data to fine-tune multilingual LLMs for six tasks across one
are not available for many language pairs. or two language pairs. They find that including any type of
In this work, we extend the support for code-mixed lan- sythetic code-mixed data in the tuning process improves the
guage processing by introducing four new specialized LLMs: responsivity of attention heads to code-mixed data, indicating
two for the previously unsupported Slovene-English language the suboptimal support for code-mixing in multilingual mod-
pair, and two for Hindi-English. Additionally, we perform els.TanandJoty[49]andBirshertandArtemova[7]studythe
a comprehensive code-mixed evaluation using a selection of capabilityinanadversarialsetting,andshowthatsynthetically
LLMs, including few-lingual models previously untested in constructed code-mixed examples cause a significant drop in
such scenarios. the accuracy of multilingual LLMs.3
In our work, we aim to continue the line of comprehensive websites Babelio and Critiques Libres. The sentiment is de-
analyses on a pool of five languages, including multiple that rivedfromafive-starratingsystemusedinthereviewprocess:
are not commonly studied in the existing literature. We focus reviews with a rating ≤ 2.5 are considered negative, reviews
our analysis on two affective evaluation tasks and evaluate a with a rating ≥ 4.0 are considered positive, and the others are
large pool of monolingual, few-lingual, and massively multi- considered neutral. The dataset contains genuine code-mixing
lingual LLMs. examples in French reviews contributed by individuals.
2) FrenchOLID: The French offensive language identifi-
D. Affective Computing in Code-Mixed Language Modeling cation dataset [22] contains 5786 tweets posted during the
Affective computing, incorporating emotion and senti- COVID-19 pandemic. The authors consider a tweet offensive
ment comprehension in human language, is an important if the offense is directed towards somebody. Although the
area of NLP. aiming to decipher and understand emotional dataset is not specifically introduced as code-mixed, we use
nuances[44]. Yet, when it comes to code-mixed content, the it as Twitter commonly contains informal language and code-
landscape becomes more intricate. mixing.
A primary obstacle to better understanding affective hints
arises from the fact that emotions can be deeply intertwined B. Tamil datasets
with the choice of language for particular words or phrases.
The DravidianCodeMix dataset collection [11] contains
For example, embedding a term of endearment in Hindi
YouTube comments in three Dravidian languages (Tamil,
withinanEnglishsentencecancompletelyshiftthesentiment,
Kannada, and Malayalam) annotated for sentiment analysis
transforming a neutral statement into an affectionate one. For
andoffensivelanguageidentification.Inourwork,wedecided
example, consider the English neutral sentence ”I received a
to use only one Dravidian language, i.e. Tamil, as our LLMs
nice gift today” and the code-mixed sentence ”I received a
are covering it. The Tamil dataset contains around 44,000
nice gift today, meri pyaari maa”. In the original sentence,
examples.Thedatasetwasannotatedbybetweentwoandfive
”nice” conveys a positive but neutral sentiment about the
student annotators:
gift. However, in the code-mixed sentence, adding the Hindi
• In the sentiment analysis dataset, the data was annotated
phrase ”meri pyaari maa” completely transforms the tone.
as positive, neutral, negative, or mixed feelings. We
”meripyaarimaa”expressesaffectionandsuggeststhegifthas
decidedtodiscardtheexamplesannotatedas“mixedfeel-
a deeper meaning because it came from the mother. Another
ings” to maintain a unified three-label sentiment scheme
example with respect to hate speech is ”This politician is a
across all our tested languages.
complete bekaar” where bekaar is Hindi for ’useless’. The
• In the offensive language identification dataset, the data
English sentence criticizes the politician, but ”bekaar” adds a
was annotated as not offensive, untargeted offensive, or
stronger layer of insult specific to Hindi. Hate speech detec-
targeted offensive (three options). We consider any type
tion models trained only on English might miss the hateful
of offensiveness as the positive class, and the rest as the
connotation because they wouldn’t understand the severity of
negative class.
the Hindi word.
If the example did not contain the Tamil language, it was
Several works have ventured into understanding sentiment
labeled as “not-Tamil”; in our experiments, we discard such
analysis and emotion discernment in code-mixed contexts.
examples.WerefertothesentimentanalysisdatasetasTamil-
Balahur et al. [4] introduce a model that leverages cultural
CMSenti, and the offensive language identification dataset as
context and societal norms to enhance emotion detection
TamilCMOLID.
precision. Additionally, Bedi et al.[6] probe the intricacies of
spotting sarcasm in code-mixed exchanges, emphasizing the
importance of grasping the dynamics between languages. C. Hindi datasets
1) IIITH: The IIITH Hindi-English code-mixed sentiment
III. CODE-MIXEDAFFECTIVEDATASETS
dataset [25] contains 3879 user comments sourced from
Inthissection,wepresenttenaffectivecode-mixeddatasets popular Indian Facebook pages associated with influential
used to test the newly introduced and existing LLMs. For figures Salman Khan and Narendra Modi. The examples
each language, we select one sentiment analysis dataset and were annotated by two annotators using a three-level polarity
one offensive language detection dataset. Their summary is scale (positive, negative, or neutral); only the examples with
presented in Table I. We start by describing the datasets matching annotations were included in the final dataset.
grouped by the primary language in Sections III-A - III-E. 2) Hinglish Hate: The Hinglish Hate dataset [8] contains
Then, we focus on the label distribution (Section III-F) and Hindi-English code-mixed social media texts, consisting of
the degree of code-mixed content present in the datasets 4578 tweets. The authors annotated the dataset with the
(Section III-G). languageatthewordlevelandwiththeclassthetweetsbelong
to (Hate speech or Normal speech)
A. French Datasetss
1) FrenchBookReviews: The French book reviews dataset1
D. Slovene datasets
contains 9658 reviews by book readers made on the French
1) Sentiment15 : The Sentiment15 [40] is the Slovene
SL SL
1https://www.kaggle.com/datasets/abireltaief/books-reviews subset of the corpus of over 1.6 million tweets belonging4
TABLE I: Summary of the used datasets. The % of code-mixing is manually estimated on 100 random samples, except for
the Hindi and Russian dataset (marked with *), where we used the CodeSwitch and langdetect libraries; see Section III-G for
details.
Dataset DetectedLanguages Task Genre(s) #Inst. Code-mixing%
FrenchBookReviews French,English sentimentanalysis bookreviews 9658 17.00
FrenchOLID French,English offensivelanguage tweets 5786 11.00
TamilCMSenti Tamil,English sentimentanalysis YouTubecomments 36681 46.00
TamilCMOLID Tamil,English offensivelanguage YouTubecomments 41760 31.00
IIITH Hindi,English sentimentanalysis Facebookcomments 3879 *63.88
HinglishHate Hindi,English offensivelanguage tweets 4578 *86.57
Sentiment15SL Slovene,Croatian,English sentimentanalysis tweets 87392 36.00
IMSyPPSL Slovene,Croatian,English offensivelanguage tweets 47538 29.00
Sentiment15RU Russian,Bulgarian sentimentanalysis tweets 86948 *9.15
RussianOLID Russian,Bulgarian offensivelanguage socialmediacomments 14412 *3.91
TABLE II: Label distributions in the code-mixed datasets.
to 15 European languages. The 102392 Slovene tweets were
posted between April 2013 and February 2015, and collected (a) Sentiment analysis datasets.
using Twitter Search API by constraining the geolocation of
the tweet. They were annotated using a standard three-class Datasetname positive neutral negative
annotation scheme (positive, neutral, or negative) by seven IIITH 34.85% 50.45% 14.70%
annotators. FrenchBookReviews 69.06% 22.04% 8.90%
2) IMSyPP SL: IMSyPP SL [19]isaSlovenedatasetcontain- TamilCMSenti 67.11% 18.67% 14.20%
ing tweets posted between December 2017 and August 2020.
Sentiment15SL 27.12% 43.16% 29.72%
The tweets were manually annotated twice for hate speech
Sentiment15RU 27.92% 40.08% 32.00%
typeandtarget:weconsidertweetscontaininganytypeofhate
speechaspositiveandothersasnegative.Wepreprocessedthe
(b) Offensive language identification datasets.
data, keeping only tweets where both its annotations agree, in
total 47538 examples. Datasetname notoffensive offensive
FrenchOLID 77.51% 22.49%
TamilCMOLID 75.40% 24.60%
E. Russian datasets
HinglishHate 63.73% 36.27%
1) Sentiment15 RU: The Sentiment15
RU
is the Russian sub- IMSyPPSL 66.58% 33.42%
set of the Sentiment15 corpus of tweets. As the dataset was RussianOLID 66.51% 33.49%
collected as a whole, its collection is similar to the collection
of the Slovene Sentiment15 subset (see Section III-D1). In
SL
total, the dataset contains 87384 examples. G. Language proportions in code-mixed datasets
2) RussianOLID: The Russian language toxic comments Animportantaspectofcode-mixeddatasetsisthelanguage
dataset (RussianOLID) contains 14412 comments from Rus- diversity they contain. We used two methods for detecting
sian websites 2ch and Pikabu. The dataset was originally code-mixing. The first one tackles code-mixing in French,
published on Kaggle, with its annotation quality later being Russian, Tamil, and Slovene tweets using the langdetect2
independently validated by Smetanin [32]. The texts are an- library. This method first removes non-alphabetic characters
notated for toxicity using a binary annotation scheme (toxic from the text and then uses a character n-gram-based na¨ıve
or non-toxic). Bayeslanguagedetector.Weobtainalistofdetectedlanguages
for each text and consider them code-mixed if more than one
language is detected above the threshold of 5%. We selected
F. Label distribution in the code-mixed datasets this threshold by analyzing the accuracy of the langdetect
library on hand-picked samples, 50 actually code-mixed and
To provide insight into the used datasets, we quantify their
50not,fromeachoftheaforementioneddatasets.Weobserved
label distribution in Table II, separately for sentiment and
the peak accuracy for the 4-7% threshold range. Balancing
offensivelanguagedatasets.Asthenumbersshow,alldatasets
precision and recall, we chose 5% to reliably identify code-
are imbalanced to some degree. For offensive language iden-
mixed content while minimizing false positives and false
tification, all datasets contain a higher proportion of non-
negatives.
offensive than offensive examples. For sentiment analysis,
Duetoproblemswithcode-switchingidentificationinHindi
FrenchBookReviews and TamilCMSenti lean heavily towards
datasets, and the availability of a better alternative, we tried
positive sentiment, in IIITH the positive sentiment is domi-
theCodeSwitchlibrary3 forthislanguage.Thelibraryisbased
nant to a lesser degree, while datasets Sentiment15-SL and
Sentiment15-RU show a relatively balanced mix of positive 2https://pypi.org/project/langdetect/
and negative labels with the largest class being neutral. 3https://pypi.org/project/codeswitch/5
TABLE IV: Summary of used large language models. The
on multilingual BERT, which is known to effectively take the
models marked with * are newly introduced in this paper.
context of neighboring words into account. We used its mod-
Models marked with ▲ are declared as monolingual but have
ule,configuredspecificallyforHindiandEnglish(hin-eng),to
capabilities in multiple languages. M and B stand for millions
identify languages within each tweet. The approach identified
and billions of parameters, respectively.
Hindi, English, Nepali, and other languages. Upon manual
examination of a subset of labeled instances, we observed
Model Link Languages Parameters
that Hindi and Nepali were often identified interchangeably
Massivelymultilingualmodels:
due to their linguistic similarities and shared vocabulary. To
mBERTc-base link 104 178M
enhance the accuracy of our language identification process, XLM-R-base link 100 278M
we refined our function to categorize a tweet as code-mixed TwHIN-BERT-base link 100 279M
Few-lingualmodels:
only when it contained combinations of Hindi and English,
SlEng-BERT* link sl,en 117M
Nepali and English, or a trilingual mix of Hindi, Nepali, and SloBERTa-SlEng* link sl,en 117M
English. Tweets exhibiting a single language were labeled as MuRIL-en-hi-codemixed* link hi,en 117M
RoBERTa-en-hi-codemixed* link hi,en 117M
’not code-mixed’.
MuRILc-base link en+16in 238M
HingRoBERTa link hi,en 278M
TABLE III: Manual assessment of code-mix detection tools
CroSloEngualBERT link sl,hr,en 124M
precision in Hindi and Slovene. Monolingualmodels:
SloBERTa link sl 111M
Dataset Precision Tool CamemBERT-base link fr 111M
TamilBERT link ta 238M
IIITH 0.860 CodeSwitch
RuBERTc-base link ru 178M
HinglishHate 0.900 CodeSwitch
Monolingualgenerativemodels:
Sentiment15SL 0.326 langdetect GPT3▲ link en 175B
IMSyPPSL 0.201 langdetect Llama2-7B▲ link en 7B
Table III shows the manual evaluation of the precision
obtained by our code-mix detection tools. For Hindi and A. New code-mixed language models
Slovene datasets, we manually checked 50 random samples Wetrainedfournewlargelanguagemodelsonconsiderable
flagged as code-mixed and 50 random samples flagged as not amounts of non-standard language with the intention of using
code-mixedbyourtools.WhiletheCodeSwitchlibrary,which themforaffectivecomputingtasksandcode-mixedprocessing.
is based on contextual LLM, shows a promising performance, All the new models are masked LLMs utilizing transformer
the langdetect library often attributes words to an arbitrary architecture [56], with about 117 million parameters each.
languagewheretheyexistwithoutconsideringtheircontextual The models are bilingual, two being Hindi-English and two
usage. This limitation becomes evident when the same word Slovene-English. Two models were trained from scratch, and
appears in multiple languages within a sentence. For instance, two were further trained from existing models on new data.
consider the word ”brat”, which refers to a male sibling in Their names and classification are shown in Table V.
both Croatian and Slovene. The langdetect tool, using the
TABLE V: The names and properties of four newly trained
class-conditional independence assumption of na¨ıve Bayes,
bilingual masked language models.
struggles to accurately determine the appropriate language for
”brat” based on its surrounding context.
Languages Fromscratch Fromexisting
Acknowledging these challenges and the need for better
Hindi-English RoBERTa-en-hi-codemixed MuRIL-en-hi-codemixed
code-switchingdetectionaccuracy,wemanuallyannotated100
Slovene-English SlEng-BERT SloBERTa-SlEng
samples from each dataset, excluding Hindi and Russian lan-
guages.ThisexclusionwasbasedontheCodeSwitchlibrary’s
Each of the newly trained models has 12 transformer en-
promisingprecisionrateinHindiandthelackofcollaborators
coderlayers,equalinarchitectureandroughlyequalinnumber
to manually annotate Russian datasets. The results of this
ofparameterstothebase-sizedBERT[16]andRoBERTa[34]
manual annotation and code-mixing percentage are shown in
models. The models support a maximum sequence length of
therightmostcolumnofTableI.Thecode-mixingpercentages
512 tokens. The pre-training task was masked language mod-
in Table I for the two Russian datasets are calculated on the
eling, withno other tasks (e.g., next-sentenceprediction). The
whole dataset using the langdetect library.
models are publicly available on the HuggingFace repository
oftheCentreforLanguageResourcesandTechnologiesofthe
IV. PRE-TRAINEDLARGELANGUAGEMODELS
University of Ljubljana4. Their short description is provided
In this section, we describe the pre-trained LLMs which
below.
we use in our study. In Section IV-A, we first describe four
1) Slovene-English: The two new Slovene-English models
newly created LLMs trained on considerable amounts of non-
SlEng-BERT and SloBERTa-SlEng are trained on Slovene
standard language aimed at better processing Slovene-English
and English conversational, non-standard, and slang language
and Hindi-English code-mixed language. Then, we describe
corpora. Concretely, they are trained on English and Slovene
other (existing) LLMs used in our evaluation in Section IV-B.
tweets [20], Slovene part of the web crawl corpus MaCoCu
AllmodelsarelistedinTableIV,wherethenewlyintroduced
models are marked with an asterisk (*). 4https://huggingface.co/cjvt6
[5], the corpus of moderated web content Frenk [35], and a trained using a contrastive social loss, the goal of which is to
small subset of the English OSCAR corpus [43]. The size of learn if two tweets appeal to similar users. We use the base-
the English and Slovene corpora used is approximately equal. size version and refer to the model as TwHIN-BERT-base.
In total, the training data contains about 2.7 billion words, 2) Few-lingual models: As massively multilingual models
which were tokenized into 4.1 billion subword tokens prior cover a wide range of languages, their vocabulary and to-
to training. Both models share the same vocabulary and input kenization are not adapted to any specific language, which
embeddings, containing 40000 subword tokens. makes them inferior to LLMs incorporating fewer languages
Using the dataset, the models were trained using two dif- for many tasks [54]. We consider three few-lingual LLMs
ferenttrainingregimes:SlEng-BERTwastrainedfromscratch covering considerably fewer languages than massively mul-
for40epochswhileSloBERTa-SlEngwasinitializedusingthe tilingual LLMs.
SloBERTa[53]SlovenemonolingualmaskedLLMandfurther MuRIL [28] is a multilingual masked LLM based on the
pre-trained for two epochs. BERTarchitecture[16].ItwastrainedonlargeIndiancorpora
2) Hindi-English: The two new Hindi-English models consistingofEnglishand16Indianlanguages,includingHindi
RoBERTa-en-hi-codemixed and MuRIL-en-hi-codemixed are and Tamil which we consider in this work. We use the cased
trained on English, Hindi, and code-mixed English-Hindi base-sizeversionofthismodelandrefertoitasMuRILc-base.
corpora. The corpora used consist of primarily web-crawled HingRoBERTa [41] is a bilingual masked LLM based
data,includingcode-mixedtweets,focusingonconversational
on RoBERTa architecture [34]. It was fine-tuned on a large
language and the COVID-19 pandemic. The training corpora
corpus of Hindi-English tweets. HingRoBERTa has demon-
containabout2.6billionwords,whichweretokenizedinto3.4
strated competitive performance across various downstream
billion subword tokens prior to training. Both models share
taskscomparedtoothermodelstrainedonHindi-Englishcode-
thesamevocabularyandinputembeddings,containing40000
mixed datasets, as evidenced in research by Nayak et al. [42]
subword tokens.
CroSloEngual BERT [55] is a trilingual masked LLM
Similarly as for the Slovene-English models, the mod-
based on the BERT architecture [16]. It was trained on a
els were trained using two different training regimes. The
mixture of news articles, and web-crawled data in Croatian,
RoBERTa-en-hi-codemixed model was trained from scratch
Slovene, and English.
for 40 epochs while MuRIL-en-hi-codemixed was initialized
3) Monolingual models: While we hypothesize that mul-
using pre-trained MuRIL multilingual masked LLM [28] and
tilingual LLMs might be preferable for code-mixed affective
further pre-trained for two epochs.
tasks, we also test monolingual models. They might be com-
petitive and familiar with languages other than their main
B. Existing language models language,asasmallnumberofotherlanguagesislikelytobe
present in all monolingual training corpora due to their huge
In addition to four newly introduced bilingual LLMs, we
size and likely presence in the news, textbooks, and social
evaluate several existing massively multilingual, few-lingual,
media.WeconsiderfourmonolingualmaskedLLMscovering
and monolingual LLMs which we describe next. Our newly
specific languages.
introduced models are variants of the encoder-only BERT
CamemBERT[37] is a French monolingual masked LLM
and RoBERTa LLMs, and so are most of the other models,
based on the BERT architecture [16]. It was trained using a
but we also include two massive decoder-only LLMs due to
whole-word masking version of the masked language mod-
their strong general performance. In addition to the general
eling objective on web-crawled data. We use the base-size
domainmodels,wetestmultipletweetdomain-adaptedLLMs
version and refer to the model as CamemBERT .
as they are specialized for handling social media texts that base
commonlyincludecode-mixedlanguage.Below,wesplittheir SloBERTa [53] is a Slovene monolingual masked LLM
descriptions into three groups: massively multilingual mod- basedontheCamemBERTarchitecture[37].Itwastrainedon
els, few-lingual models, monolingual models, and generative aunionoffiveSlovenecorporacontainingnewsarticles,web-
monolingual models. crawled data, tweets, academic language, and parliamentary
1) Massively multilingual models: We consider three mas- data.
sively multilingual LLMs trained on 100 or more languages. TamilBERT [26] is a Tamil monolingual masked LLM
Multilingual BERT (mBERT) [16] is a multilingual basedontheBERTarchitecture[16].ItwastrainedonaTamil
masked LLM based on the BERT architecture [16]. It was monolingual corpus.
trainedonWikipediadumpsin104languageswiththelargest RuBERT [31] is a Russian monolingual masked LLM
Wikipedia size. We use the cased base-size version of this based on the BERT architecture [16]. It was trained on the
model and refer to it as mBERTc-base. Russian Wikipedia dump and news articles. We use the cased
XLM-RoBERTa [14] is a multilingual masked LLM based base-size version and refer to the model as RuBERTc base.
on the RoBERTa architecture [34], trained on Wikipedia 4) Monolingualgenerativemodels: Whileallothermodels
dumpsandwebcrawldatain100languages.Weusethebase- in our evaluation are masked LLMs using only the encoder
size version and refer to the model as XLM-R-base. stack of the transformer architecture, for comparison, we
TwHIN-BERT [59] is a multilingual masked LLM based also include two popular and considerably larger generative
ontheBERTarchitecture[16].Itwastrainedontweetsin100 LLMs that use only the decoder stack of the transformer
languages. In addition to masked language modeling, it was architecture.TheGPT3andLlama2modelswedescribebelow7
are primarily trained on English, although they have seen and of using a thorough hyperparameter optimization, and largely
are capable of processing other languages. follow existing practices for fine-tuning BERT-like models
GPT-3 [9] is a language model trained on a vast amount of [38] [52].
text and code. This vast training dataset allows it to perform 2) Fine-tuning GPT3 Model: The evaluation for genera-
a variety of tasks, including generating different creative text tive models necessarily differs from BERT-like classification
formats, translating languages, and answering questions in an models.Specifically,wefine-tunetheGPT3modeltogenerate
informative way. The specific inner workings of this model textual classes. In this process, GPT-3 is trained to map input
remain undisclosed. text to output text using our prompt structure. We fine-tune
Llama2 [51] is an English monolingual autoregressive the model for 800 prompt completion pairs for 2 epochs
LLM.Thedetailsaboutitstrainingdataarescarce:theauthors using the OpenAI API on randomly sampled training subsets.
state that it was trained on mostly English data from publicly During the generation phase, we set the temperature to 0.1
available sources. We use the version with 7B parameters and and the learning rate to 0.1 During fine-tuning, each example
refer to the model as Llama2-7B. is transformed into a prompt by appending the text to the
prefix ”Input:”. The model is optimized to produce text with
theprefix”Sentimentis:”(or”Labelis:”inoffensivelanguage
V. EVALUATION
identification), followed by the predicted class. The generated
In this section, we evaluate the described language models
output takes the form of ’Sentiment: class’ or ’Label: class’.
on multiple code-mixed datasets. We first describe the exper-
An example of a prompt and the output template are shown
imental setup in Section V-A and continue with the analysis
in Figure 1, for a code-mixed Hindi-English input.
of results in Section V-B. In Section V-C we further analyze
the results, specifically focusing on the effect of code-mixed
language.
A. Experimental setup
In our model evaluation process, we split the data into
training,validation,andtestingsetsrandomlyintheproportion
60%:20%:20%, using stratification across the class labels.
However,fortheGPT3andLamma2models,weimplemented
a nuanced approach to manage costs and streamline perfor-
Fig. 1: Example prompts and output templates for sentiment
mance.ForGPT,weselectedarandomsubsetof500samples
anaysis and offensive language detection tasks.
from the training dataset and 100 from the testing dataset.
Similarly, for Lamma2, we optimized output generation time Asshown,wefine-tunetheGPT3modeltogeneratetextual
by reducing the testing dataset to 300 samples. Subsequently, classes. Initially, we fine-tuned the Curie variant of GPT3
we reported every model’s macro F 1 score on the test set as using our prompt structure and hyperparameters as previously
a comprehensive metric for overall effectiveness. To enhance described. However, due to Curie’s deprecation, we transi-
the quality of the input data, we conducted preprocessing tioned to the davinci-002 variant for completing our tests on
steps that involved the removal of special characters, such the Hindi datasets. For fine-tuning the davinci-002 model, we
as ’@’, and trailing whitespaces from the text. To assure the employed a temperature setting of 0.1 and logit bias.
reproducibility of our results, we share our code online5. We 3) Fine-tuning the Llama2 models: Llama2 generative
provide descriptions of the finetuning process for different model was fine-tuned with parameter-efficient fine-tuning
model groups next: in Section V-A1 for BERT-like models, method QLoRA[15], utilizing 16-bit precision quantization
in Section V-A2 for the GPT3 model, and in Section V-A3 withalearningrateof2·10−4.Theideaofparameter-efficient
for the Llama2 model. fine-tuningtechniques[33]istoselectivelyfine-tunealimited
1) Fine-tuning BERT-like models: We evaluate the models setofadditionalmodelparameters,significantlyreducingboth
in a discriminative (classification) setting, meaning we fine- computational and storage expenses associated with the fine-
tune the models to discriminate between the two (in offensive tuning process.
languageidentification)orthree(insentimentanalysis)unique The same as the GPT3 model, we evaluate the Llama-2
classes. To enable batch processing, we truncate and pad all model in a generative setting, using prompts akin to those
inputsequencestoamaximumlengthof512subwordtokens. used in the GPT3 model (see Section V-A2).
WeoptimizethemodelsusingtheAdamW[36]optimizerwith
thelearningrate5·10−5 foruptofiveepochs,maximizingthe
B. Results on the full datasets
batch size based on the available GPU memory. On Slovene
Inthissection,wepresenttheresultsofdifferentmodelson
datasets, we fine-tune the models for up to ten epochs as we
affectivecode-mixedtasks,sentimentprediction,andoffensive
noticed some models did not converge after five epochs. Our
language detection. Table VI presents the results split by
fine-tuning settings are selected as reasonable defaults instead
language, i.e. each subtable corresponds to a specific code-
mixeddatasets:Slovene(TableVIa),Tamil(TableVIb),Hindi
5https://github.com/matejklemen/sentiment-hate-speech-with-code-mixed-
models (Table VIc), French (Table VId), and Russian (Table VIe).8
On Slovene datasets the best results are achieved by the
TABLE VI: Macro F scores on sentiment prediction and
1 newly introduced SlEng-BERT model, achieving F score
1
offensive language detection tasks achieved with massively
0.666 on Sentiment15 and 0.850 on IMSyPP . In general,
SL SL
multilingual,few-lingual,monolingual,andgenerativemodels
the results achieved by the Slovene monolingual (SloBERTa)
across five primary languages. The best F score for each
1 and few-lingual (SlEng-BERT, SloBERTa-SlEng, CroSloEn-
language is displayed in bold.
gual BERT) models are better than those achieved by the
(a) Results on Slovene datasets. The * marks the evaluation on a massively multilingual models and the significantly larger
sample of 1000 examples. generative English models (GPT3 and Llama2). GPT3 and
Llama2achievecomparablescoresonbothdatasets(F scores
1
Model Sentiment15SL IMSyPPSL
0.597 and 0.596 on Sentiment15 , and 0.825 and 0.821
SL
mBERTc-base 0.617 0.785 on IMSyPP ), performing worse than massively multilingual
XLM-R-base 0.645 0.811 SL
TwHIN-BERT-base 0.619 0.808 models on Sentiment15 SL and better than them on IMSyPP SL.
CroSloEngualBERT 0.660 0.834 On Tamil datasets, TwHIN-BERT-base emerges as the top-
SlEng-BERT 0.666 0.850 performing model, achieving the F score of 0.795 on the
SloBERTa-SlEng 0.659 0.845 1
Tamil sentiment dataset and 0.896 on the offensive language
SloBERTa 0.650 0.840
GPT3* 0.597 0.825 dataset. Following closely is mBERTc-base with an F 1 score
Llama2-7B 0.596 0.821 of 0.749 on the Tamil sentiment dataset and 0.832 on the
offensive language dataset. XLM-R-base shows a competitive
performance to mBERTc-base in sentiment analysis with an
(b) Results on Tamil datasets.
F1 score of 0.715, but falls short in hate speech detection
Model TamilCMSenti TamilCMOLID. tasks with an F1 score of 0.778. However, it should be noted
mBERTc-base 0.749 0.832 that XLM-R-base has shown a trend of lower scores in Tamil
XLM-R-base 0.715 0.778 in hate speech detection tasks in the research by Hossain et
TwHIN-BERT-base 0.795 0.896 al.[24] where it shows lower results in Tamil when compared
MuRILc-base 0.688 0.843
to other languages like English and Malayalam. The results
TamilBERT 0.539 0.772
GPT3 0.679 0.781 indicate that specialized models trained for specific languages
Llama2-7B 0.620 0.677 or tasks, such as TwHIN-BERT-base and mBERTc-base ,
tend to outperform more generalized and larger models like
GPT3 and Llama2-7B. GPT3, Llama2-7B, and TamilBERT
(c) Results on Hindi datasets.
show competitive but slightly lower performance across both
Model IIITH HinglishHate datasets.MuRILc-baseexhibitmixedperformanceacrossboth
mBERTc-base 0.749 0.711 the datasets. Their error analysis suggests that XLM-R-base
XLM-R-base 0.739 0.677 struggles with distinguishing between HS (hate speech) and
TwHIN-BERT-base 0.830 0.733
NHS (not hate speech) classes due to common code-mixed
MuRILc-base 0.698 0.748
words, potentially affecting its performance. Additionally, the
HingRoBERTa 0.854 0.833
RoBERTa-en-hi-codemixed 0.792 0.729 high class imbalance and biasness towards the not offensive
MuRIL-en-hi-codemixed 0.718 0.709 label in both the cases (table IIb) may cause misclassification
GPT3 0.660 0.551
of offensive as not offensive.
Llama2-7B 0.666 0.595
On Hindi sentiment and offensive language tasks, Hin-
gRoBERTa emerges as the top model with F scores of
1
(d) Results on French datasets. 0.854 and 0.833, showcasing the prowess of bilingual mod-
els. TwHIN-BERT-base and RoBERTa-en-hi-codemixed fol-
Model FrenchBookReviews FrenchOLID
lowclosely.ThemassivelymultilingualmodelsmBERTc-base
mBERTc-base 0.749 0.915
, MuRILc-base and XLM-R-base show comparable results.
XLM-R-base 0.644 0.902
TwHIN-BERT-base 0.759 0.936 The second newly introduced model MuRIL-en-hi-codemixed
CamemBERT 0.696 0.914 performs worse than these models but better than generative
GPT3 0.743 0.663 models GPT3 and LlaMa2-7B.
Llama2-7B 0.571 0.714
In French sentiment detection, massively multilingual
TwHIN-BERT-base leads the race in sentiment analysis
(e) Results on Russian datasets. task with a F 1 score of 0.759 followed by mBERTc-base,
GPT3, and monolingual CamemBERT. The XLM-R-base and
Model Sentiment15RU RussianOLID Llama2-7Baretrailingwithconsiderablegaps.Intheoffensive
mBERTc-base 0.871 0.915 speech detection task, several models have excelled in perfor-
XLM-R-base 0.846 0.902
mance with TwHIN-BERT-base in the lead with a score of
TwHIN-BERT-base 0.851 0.969
RuBERTc-base 0.856 0.971 F 1 score of 0.936, followed by mBERTc-base, CamemBERT
GPT3 0.734 0.665 and XLM-R-base. GPT3 performance is considerably lower
Llama2-7B 0.677 0.864 compared to other models.
InRussiantasks,themonolingualmodelRuBERTc leads
base9
in offensive speech detection with F score of 0.971. Mul-
1
tilingual models mBERTc-base, XLM-R-base, and TwHIN-
BERT-base perform competitively in sentiment analysis with
mBERTc-base leading with F score of 0.871. Massive gen-
1
erative models, GPT3 and Llama2-7B, lag behind.
Trying to draw more general conclusions, we observe two
key findings. The best results are achieved by either bilingual
models (Slovene and Hindi) or a model specialized for social
media content (TwHIN-BERT-base on Tamil and French); the
Russian language, without a bilingual model and with very
low proportion of code-mixed text, is an exception here, but
the model specialized for social media ((TwHIN-BERT-base)
is very competitive.
Theadvantageofbilingualmodelsistwo-fold.Firstly,their
targeted focus on a specific language pair allows them to
capture the intricacies of each language’s vocabulary and
grammarmoreeffectivelythanmassivelymultilingualmodels.
ThisisreflectedinthesuperiorperformanceofHingRoBERTa
in the Hindi-English datasets and SlEng-BERT in the Slovene a) Sentiment analysis.
code-mixed datasets compared to multilingual models, which
maystrugglewiththenuancesofcode-mixingpresentinsuch
data.Secondly,bilingualmodelstypicallyhavelowermemory
requirementscomparedtotheirmultilingualcounterparts.This
translates to greater practical efficiency, making them more
suitable for deployment in real-world applications, especially
when dealing with resource constraints.
Figure 2 presents a comparative analysis of various cat-
egories of models -— generative, monolingual, bilingual,
few-lingual, and massively multilingual -— in the context
of the two affective computing tasks. For this comparison,
we considered the model with the best performance within
each category. Figure 2a illustrates the results for sentiment
analysis. Generative models, while performing adequately,
lag behind other models. Monolingual models perform better
but still lag behind other types of models. Bilingual models
demonstrate superior performance where they exist, closely
followed by massively multilingual models. Interestingly, a
b) Hate speech detection.
dipinperformanceisobservedforfew-lingualmodels.Figure
2b showcases the performance of the models in the offensive Fig. 2: Comparing the performance of the best model in each
speech detection task. The trends show that generative mod- model category for the used tasks.
els lag behind others, massively multilingual models either
perform comparably to or fall below bilingual and fewlingual
models,withbilingualmodelsmostlymaintainingaprominent
manually annotated it for code-mixing. Table VII provides a
position.
comparison of model performance on code-mixed (CM) and
non-code-mixed (notCM) subsets.
On the code-mixed text, the overall best bilingual model
C. Results on the code-mixed subsets
SlEng-BERT, performs comparably or slightly worse to the
In this section, we analyze the performance of models best-performing monolingual SloBERTa model. During our
separately for code-mixed and non-code-mixed subsets of manual inspection of the code-mixing patterns, we find that
their respective datasets. We select Slovene and Hindi as the code-mixing is very rarely the dominant cause for the target
representative languages and curate the datasets to isolate label, therefore, it is unsurprising that the code-mixed models
code-mixed examples. For Hindi, we use the CodeSwitch do not perform significantly better than the general-purpose
librarytoseparatecode-mixedfromnon-code-mixedexamples ones on the code-mixed subset. These findings might indicate
in the test sets. For Slovene, where language detection tools alesserimpactofcode-mixinginaffectivetasksascommonly
perform poorly on code-switched text, we manually selected assumed. However, to confirm such findings. a future more
a subset of 1000 examples from the test set (identical to focused research of code-mixed text processing is necessary,
the sample used for GPT3 evaluation in Section V-B) and using better datasets with carefully curated texts (e.g., in the10
TABLE VII: Separate results for code-mixed (CM) and not
the newly introduced Slovene-English SlEng-BERT demon-
code-mixed data (notCM).
strated the best F scores in sentiment analysis and offensive
1
(a) Separate results on Slovene datasets. speech detection for Hindi and Slovene, respectively.
A separate analysis of code-mixed and non-code-mixed
Model Sentiment15SL IMSyPPSL data subsets showed slightly better performance of almost
CM notCM CM notCM
all models on code-mixed data compared to non-code-mixed
N =389 N =611 N =254 N =746
data for both Slovene-English and Hindi-English code-mixing
mBERTc-base 0.609 0.623 0.825 0.763
XLM-R-base 0.631 0.648 0.828 0.821 scenarios. While this might indicate that certain affective
TwHIN-BERT-base 0.589 0.633 0.811 0.822 role of code-mixing, more research is needed to confirm this
CroSloEngualBERT 0.653 0.671 0.848 0.853 hypothesis, especially as manual inspection showed relatively
SlEng-BERT 0.659 0.690 0.874 0.865
little impact of code-mixing in sentiment detection and offen-
SloBERTa-SlEng 0.649 0.661 0.862 0.852
SloBERTa 0.666 0.651 0.876 0.854 sive speech detection.
GPT3* 0.575 0.606 0.831 0.816 Our findings provide a foundation for future explorations.
Llama2-7B 0.595 0.598 0.803 0.840
While our focus has predominantly been on sentiment analy-
sis and offensive speech detection, future work shall extend
(b) Separate results on Hindi datasets. beyond these realms to encompass a broader spectrum of
affective tasks, including emotion and sarcasm detection. Di-
Model IIITH HinglishHate versifyingthelanguagepairsandrefiningfine-tuningstrategies
CM notCM CM notCM
forlow-resourcesettingsarecrucialstepsforward.Thetransi-
N =473 N =303 N =780 N =135
tion from research to practical applications involves testing
mBERTc-base 0.776 0.779 0.725 0.645
XLM-R-base 0.822 0.797 0.680 0.588 sentiment analysis models and offensive speech detection
TwHIN-BERT-base 0.828 0.832 0.740 0.685 models in real-world scenarios, further validating their utility.
MuRILc-base 0.707 0.650 0.673 0.564
HingRoBERTa 0.876 0.867 0.838 0.831
RoBERTa-en-hi-cm 0.850 0.787 0.692 0.564 VII. ACKNOWLEDGEMENT
MuRIL-en-hi-cm 0.723 0.669 0.704 0.625
TheworkwaspartiallysupportedbytheSlovenianResearch
GPT3 0.466 0.419 0.548 0.568
Llama2-7B 0.737 0.688 0.690 0.720 and Innovation Agency (ARIS) core research programme P6-
0411,youngresearchergrant,aswellasprojectsJ7-3159,CRP
V5-2297, L2-50070, BI-IN/22-24-015 and DST/ICD/lndo-
form of contrast sets [21]). Slovenia/2022/15(G). We sincerely thank Alice Baudhuin for
Results on Hindi datasets show several intriguing patterns. her expertise in language analysis, particularly in manually
The bilingual HingRoBERTa consistently demonstrates the detecting code-mixing within our French datasets.
best performance across all scenarios, showcasing its efficacy
incapturingthenuancesofbothsentimentanalysisandoffen- REFERENCES
sive speech detection in Hindi. The newly introduced mod-
[1] Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona Diab, Julia
els, RoBERTa-en-hi-codemixed and MuRIL-en-hi-codemixed, Hirschberg,andThamarSolorio. NamedEntityRecognitiononCode-
perform better in the code-mixed data subsets compared to Switched Data: Overview of the CALCS 2018 Shared Task. In
Proceedings of the Third Workshop on Computational Approaches to
the non-code-mixed subsets. This could be attributed to their
LinguisticCode-Switching,pages138–147,July2018.
traininginHinglishcode-mixedcorpora,givingthemacertain [2] JannisAndroutsopoulos.Code-switchingincomputer-mediatedcommu-
advantage in handling code-mixed text. In contrast, GPT3 nication. InHandbookofthePragmaticsofCMC.MoutondeGruyter,
December2011.
displays subpar results across both genres of tasks, indicating
[3] Ayme´ Arango, Jorge Pe´rez, and Barbara Poblete. Cross-lingual hate
potential limitations in its adaptability to the complexities speech detection based on multilingual domain-specific word embed-
of Hindi text for sentiment and hate speech analysis. In- dings. arXivpreprintarXiv:2104.14728,2021.
[4] Alexandra Balahur, Jesu´s M Hermida, Andre´s Montoyo, and Rafael
terestingly, when considering offensive language detection,
Munoz. EmotiNet: A Knowledge Base for Emotion Detection in Text
models mostly perform better in code-mixed scenarios. This BuiltontheAppraisalTheories. InNaturalLanguageProcessingand
phenomenon may be attributed to the possibility that multiple InformationSystems:16thInternationalConferenceonApplicationsof
NaturalLanguagetoInformationSystems,NLDB2011,Alicante,Spain,
languages share similar hate speech tendencies, as suggested
June28-30,2011.Proceedings16,pages27–39.Springer,2011.
by Arango et al.[3], who suggested the existence of common [5] MartaBan˜o´n,MiquelEspla`-Gomis,MikelL.Forcada,CristianGarc´ıa-
patterns in offensive speech across different languages. Romero, Taja Kuzman, Nikola Ljubesˇic´, Rik van Noord, Leopoldo
Pla Sempere, Gema Ram´ırez-Sa´nchez, Peter Rupnik, V´ıt Suchomel,
Antonio Toral, Tobias van der Werff, and Jaume Zaragoza. MaCoCu:
VI. CONCLUSION Massive collection and curation of monolingual and bilingual data:
focusonunder-resourcedlanguages. InProceedingsofthe23rdAnnual
Our research analyzed the performance of several types ConferenceoftheEuropeanAssociationforMachineTranslation,pages
of large language models on two affective computing tasks 303–304,2022.
[6] Manjot Bedi, Shivani Kumar, Md Shad Akhtar, and Tanmoy
in a code-mixed setting. A notable finding is the dominance
Chakraborty. Multi-modalSarcasmDetectionandHumorClassification
of bilingual and multilingual models, specialized for social inCode-mixedConversations. IEEETransactionsonAffectiveComput-
media texts, over general massively multilingual, few-lingual, ing,14(2):1363–1375,2021.
[7] AlexeyBirshertandEkaterinaArtemova. CallLarisaIvanovna:Code-
and monolingual models across diverse language pairs. For
SwitchingFoolsMultilingualNLUModels.InRecentTrendsinAnalysis
instance,thebilingualHIndi-EngishHingRoBERTamodeland ofImages,SocialNetworksandTexts,pages3–16,2022.11
[8] Aditya Bohra, Deepanshu Vijay, Vinay Singh, Syed Sarfaraz Akhtar, [24] EftekharHossain,OmarSharif,andMohammedMoshiulHoque. NLP-
andManishShrivastava.ADatasetofHindi-EnglishCode-MixedSocial CUET@LT-EDI-EACL2021: Multilingual Code-Mixed Hope Speech
Media Text for Hate Speech Detection. In Proceedings of the second Detection using Cross-lingual Representation Learner. arXiv preprint
workshoponcomputationalmodelingofpeople’sopinions,personality, arXiv:2103.00464,2021.
andemotionsinsocialmedia,pages36–41,2018. [25] AdityaJoshi,AmeyaPrabhu,ManishShrivastava,andVasudevaVarma.
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Towards Sub-Word Level Compositions for Sentiment Analysis of
Kaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,Girish Hindi-EnglishCodeMixedText. InProceedingsofCOLING2016,the
Sastry,AmandaAskell,etal. LanguageModelsareFew-ShotLearners. 26thInternationalConferenceonComputationalLinguistics:Technical
Advances in neural information processing systems, 33:1877–1901, Papers,2016.
2020. [26] Raviraj Joshi. L3Cube-HindBERT and DevBERT: Pre-Trained BERT
[10] O¨zlem C¸etinog˘lu and C¸ag˘rı C¸o¨ltekin. Two languages, one treebank: TransformermodelsforDevanagaribasedHindiandMarathiLanguages,
buildingaTurkish–Germancode-switchingtreebankanditschallenges. 2023.
LanguageResourcesandEvaluation,57(2):545–579,Jun2023. [27] SujataSKathpaliaandKENNETHKENGWEEONG.Theuseofcode-
[11] Bharathi Raja Chakravarthi, Ruba Priyadharshini, Vigneshwaran Mu- mixinginIndianbillboardadvertising.WorldEnglishes,34(4):557–575,
ralidaran, Navya Jose, Shardul Suryawanshi, Elizabeth Sherly, and 2015.
JohnP.McCrae. DravidianCodeMix:SentimentAnalysisandOffensive [28] Simran Khanuja, Diksha Bansal, Sarvesh Mehtani, Savya Khosla,
LanguageIdentificationDatasetforDravidianLanguagesinCode-Mixed Atreyee Dey, Balaji Gopalan, Dilip Kumar Margam, Pooja Aggarwal,
Text. LanguageResourcesandEvaluation,2022. Rajiv Teja Nagipogu, Shachi Dave, et al. MuRIL: Multilingual Rep-
[12] KhyathiChandu,EkaterinaLoginova,VishalGupta,JosefvanGenabith, resentations for Indian Languages. arXiv preprint arXiv:2103.10730,
Gu¨nterNeumann,ManojChinnakotla,EricNyberg,andAlanW.Black. 2021.
Code-MixedQuestionAnsweringChallenge:Crowd-sourcingDataand [29] Simran Khanuja, Sandipan Dandapat, Sunayana Sitaram, and Monojit
Techniques. In Proceedings of the Third Workshop on Computational Choudhury.ANewDatasetforNaturalLanguageInferencefromCode-
Approaches to Linguistic Code-Switching, pages 29–38, Melbourne, mixed Conversations. In Proceedings of the The 4th Workshop on
Australia,July2018. ComputationalApproachestoCodeSwitching,pages9–16,2020.
[13] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D [30] Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana
Manning. ELECTRA: Pre-training Text Encoders as Discriminators Sitaram,andMonojitChoudhury.GLUECoS:AnEvaluationBenchmark
RatherThanGenerators. arXivpreprintarXiv:2003.10555,2020. forCode-SwitchedNLP. arXivpreprintarXiv:2004.12376,2020.
[14] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaud- [31] YuriKuratovandMikhailArkhipov. AdaptationofDeepBidirectional
hary,GuillaumeWenzek,FranciscoGuzma´n,EdouardGrave,MyleOtt, MultilingualTransformersforRussianLanguage.InComputationalLin-
Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised Cross-lingual guisticsandIntellectualTechnologies:ProceedingsoftheInternational
Representation Learning at Scale. In Proceedings of the 58th Annual Conference“Dialogue2019”,2019.
MeetingoftheAssociationforComputationalLinguistics,pages8440– [32] Yuri Kuratov and Mikhail Arkhipov. Toxic comments detection in
8451,2020. Russian. In Computational Linguistics and Intellectual Technologies:
[15] TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer. ProceedingsoftheInternationalConference“Dialogue2020”,2020.
QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint [33] BrianLester,RamiAl-Rfou,andNoahConstant.ThePowerofScalefor
arXiv:2305.14314,2023. Parameter-Efficient Prompt Tuning. arXiv preprint arXiv:2104.08691,
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2021.
BERT: Pre-training of Deep Bidirectional Transformers for Language [34] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,Danqi
Understanding. In Proceedings of the 2019 Conference of the North Chen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.
American Chapter of the Association for Computational Linguistics: RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv
Human Language Technologies, Volume 1 (Long and Short Papers), preprintarXiv:1907.11692,2019.
pages4171–4186,2019. [35] Nikola Ljubesˇic´, Tomazˇ Erjavec, and Darja Fisˇer. Datasets of Slovene
[17] MonaDiab,MahmoudGhoneim,AbdelatiHawwari,FahadAlGhamdi, and Croatian Moderated News Comments. In Proceedings of the
Nada AlMarwani, and Mohamed Al-Badrashiny. Creating a Large 2nd Workshop on Abusive Language Online (ALW2), pages 124–131,
Multi-LayeredRepresentationalRepositoryofLinguisticCodeSwitched October2018.
ArabicData. InProceedingsoftheTenthInternationalConferenceon [36] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regular-
LanguageResourcesandEvaluation(LREC’16),pages4228–4235,May ization. In 7th International Conference on Learning Representations,
2016. 2019.
[18] PraveenDominic,NiranjanPurushothaman,AnishSkandaAnilKumar, [37] Louis Martin, Benjamin Muller, Pedro Javier Ortiz Sua´rez, Yoann
A Prabagaran, J Angelin Blessy, and A John. Multilingual Sentiment Dupont, Laurent Romary, E´ric Villemonte de La Clergerie, Djame´
AnalysisusingDeep-LearningArchitectures. In20235thInternational Seddah, and Benoˆıt Sagot. CamemBERT: a Tasty French Language
ConferenceonSmartSystemsandInventiveTechnology(ICSSIT),pages Model. arXivpreprintarXiv:1911.03894,2019.
1077–1083.IEEE,2023. [38] ChrisMcCormickandNickRyan.BERTFine-TuningTutorialwithPy-
[19] Bojan Evkoski, Andrazˇ Pelicon, Igor Mozeticˇ, Nikola Ljubesˇic´, and Torch. https://mccormickml.com/2019/07/22/BERT-fine-tuning/,2019.
Petra Kralj Novak. Retweet communities reveal the main sources of [39] Sneha Mondal, Ritika, Shreya Pathak, Preethi Jyothi, and Aravindan
hatespeech. PLOSONE,17(3):1–22,032022. Raghuveer. CoCoa:AnEncoder-DecoderModelforControllableCode-
[20] DarjaFisˇer,NikolaLjubesˇic´,andTomazˇErjavec.TheJanesproject:lan- switched Generation. In Proceedings of the 2022 Conference on
guageresourcesandtoolsforSloveneusergeneratedcontent.Language EmpiricalMethodsinNaturalLanguageProcessing,pages2466–2479,
ResourcesandEvaluation,54(1):223–246,Mar2020. December2022.
[21] Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben [40] IgorMozeticˇ,LuisTorgo,VitorCerqueira,andJasminaSmailovic´.How
Bogin,SihaoChen,PradeepDasigi,DheeruDua,YanaiElazar,Ananth to evaluate sentiment classifiers for Twitter time-ordered data? PLOS
Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, ONE,13(3):1–20,032018.
DanielKhashabi,KevinLin,JiangmingLiu,NelsonF.Liu,PhoebeMul- [41] RavindraNayakandRavirajJoshi.L3Cube-HingCorpusandHingBERT:
caire,QiangNing,SameerSingh,NoahA.Smith,SanjaySubramanian, A Code Mixed Hindi-English Dataset and BERT Language Models.
Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. Evaluating In Proceedings of the WILDRE-6 Workshop within the 13th Language
Models’LocalDecisionBoundariesviaContrastSets.InFindingsofthe ResourcesandEvaluationConference,June2022.
AssociationforComputationalLinguistics:EMNLP2020,pages1307– [42] RavindraNayakandRavirajJoshi.L3Cube-HingCorpusandHingBERT:
1323,2020. AcodemixedHindi-EnglishdatasetandBERTlanguagemodels. arXiv
[22] Yanzhu Guo, Virgile Rennard, Christos Xypolopoulos, and Michalis preprintarXiv:2204.08398,2022.
Vazirgiannis. BERTweetFR : Domain Adaptation of Pre-Trained Lan- [43] Pedro Javier Ortiz Sua´rez, Laurent Romary, and Benoˆıt Sagot. A
guage Models for French Tweets. In Proceedings of the Seventh Monolingual Approach to Contextualized Word Embeddings for Mid-
WorkshoponNoisyUser-generatedText(W-NUT2021),pages445–450, Resource Languages. In Proceedings of the 58th Annual Meeting of
November2021. the Association for Computational Linguistics, pages 1703–1714, July
[23] Ehtesham Hashmi, Sule Yildirim Yayilgan, and Sarang Shaikh. Aug- 2020.
menting sentiment prediction capabilities for code-mixed tweets with [44] RosalindWPicard. AffectiveComputing. MITpress,2000.
multilingual transformers. Social Network Analysis and Mining, [45] Sebastin Santy, Anirudh Srinivasan, and Monojit Choudhury. BERTo-
14(1):86,2024. logiCoMix: How does Code-Mixing interact with Multilingual BERT?12
InProceedingsoftheSecondWorkshoponDomainAdaptationforNLP,
pages111–121,2021.
[46] HopeJensenSchau,StephanieDellande,andMaryC.Gilly.Theimpact
ofcodeswitchingonserviceencounters.JournalofRetailing,83(1):65–
78,2007. ServiceExcellence.
[47] Dama Sravani, Lalitha Kameswari, and Radhika Mamidi. Political
DiscourseAnalysis:ACaseStudyofCodeMixingandCodeSwitching
in Political Speeches. In Proceedings of the Fifth Workshop on
Computational Approaches to Linguistic Code-Switching, pages 1–5,
2021.
[48] VivekSrivastavaandMayankSingh.PHINC:AParallelHinglishSocial
MediaCode-MixedCorpusforMachineTranslation. InProceedingsof
theSixthWorkshoponNoisyUser-generatedText(W-NUT2020),pages
41–49,November2020.
[49] Samson Tan and Shafiq Joty. Code-Mixing on Sesame Street: Dawn
of the Adversarial Polyglots. In Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 3596–3616, June
2021.
[50] S Thara and Prabaharan Poornachandran. Code-Mixing: A Brief
Survey. In 2018 International conference on advances in computing,
communicationsandinformatics(ICACCI),pages2382–2388,2018.
[51] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlma-
hairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal
Bhargava,ShrutiBhosale,etal. LLaMa2:OpenFoundationandFine-
TunedChatModels. arXivpreprintarXiv:2307.09288,2023.
[52] Chris K. Tran. BERT for Sentiment Analysis. https://chriskhanhtran.
github.io/posts/bert-for-sentiment-analysis/,2019.
[53] MatejUlcˇarandMarkoRobnik-Sˇikonja. SloBERTa:Slovenemonolin-
gual large pretrained masked language model. In Proceedings of the
24thInternationalMulticonference–IS2021(SiKDD),2021.
[54] MatejUlcˇarandMarkoRobnik-Sˇikonja.Trainingdatasetanddictionary
sizes matter in BERT models: the case of Baltic languages. In
International Conference on Analysis of Images, Social Networks and
Texts,pages162–172,2021.
[55] MatejUlcˇarandMarkoRobnik-Sˇikonja. FinEstBERTandCroSloEn-
gualBERT:LessIsMoreinMultilingualModels. InText,Speech,and
Dialogue:23rdInternationalConference,TSD2020,Proceedings,page
104–111,2020.
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones,AidanN.Gomez,ŁukaszKaiser,andIlliaPolosukhin. Attention
is All You Need. In Proceedings of the 31st International Conference
onNeuralInformationProcessingSystems,2017.
[57] Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin,
AndreaMadotto,andPascaleFung. AreMultilingualModelsEffective
in Code-Switching? In Proceedings of the Fifth Workshop on Compu-
tationalApproachestoLinguisticCode-Switching,pages142–153,June
2021.
[58] RuochenZhang,SamuelCahyawijaya,JanChristianBlaiseCruz,Genta
Winata, and Alham Aji. Multilingual Large Language Models Are
Not (Yet) Code-Switchers. In Proceedings of the 2023 Conference
onEmpiricalMethodsinNaturalLanguageProcessing,pages12567–
12582,Singapore,December2023.
[59] Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian
McWilliams, Jiawei Han, and Ahmed El-Kishky. TwHIN-BERT:
A Socially-Enriched Pre-Trained Language Model for Multilingual
Tweet Representations at Twitter. In Proceedings of the 29th ACM
SIGKDDConferenceonKnowledgeDiscoveryandDataMining,page
5597–5607,2023.