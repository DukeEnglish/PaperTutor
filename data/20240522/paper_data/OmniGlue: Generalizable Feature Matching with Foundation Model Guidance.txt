OmniGlue: Generalizable Feature Matching with Foundation Model Guidance
HanwenJiang∗ ArjunKarpur† BingyiCao† QixingHuang∗ AndréAraujo†
∗UniversityofTexasatAustin †GoogleResearch
(hwjiang,huangqx)@cs.utexas.edu (arjunkarpur,bingyi,andrearaujo)@google.com
Abstract DeepAerial
22.4
Theimagematchingfieldhasbeenwitnessingacontin-
NAVI- GSO-
MultiView Hard
uousemergenceofnovellearnablefeaturematchingtech-
niques,withever-improvingperformanceonconventional 13.2 16.4 8.6
benchmarks. However, our investigation shows that de- 6.2 6.8
spite these gains, their potential for real-world applica-
tionsisrestrictedbytheirlimitedgeneralizationcapabili- 4.2 15.0
ties to novel image domains. In this paper, we introduce N WA iV ldI- 12.4 25.8 31.3 ScanNet
OmniGlue, the first learnable image matcher that is de-
signedwithgeneralizationasacoreprinciple. OmniGlue 47.4 SIFT+MNN
SuperGlue
leveragesbroadknowledgefromavisionfoundationmodel MegaDepth
(in-domain) OmniGlue(ours)
to guide the feature matching process, boosting general-
Figure1. OmniGlueisageneralizablelearnablematcher. In-
ization to domains not seen at training time. Addition-
troducingfoundationmodelguidanceandanenhancedattention
ally, we propose a novel keypoint position-guided atten-
mechanism,OmniGluelearnseffectiveimagematchingthattrans-
tion mechanism which disentangles spatial and appear-
ferswelltoimagedomainsnotseenduringtraining.Wecompareit
ance information, leading to enhanced matching descrip-
againstreferencemethodsSIFT[30]andSuperGlue[42],withsub-
tors. We perform comprehensive experiments on a suite stantialimprovementsonasuiteofdiversedatasets:outdoorscenes
of7datasetswithvariedimagedomains,includingscene- (MegaDepth-1500[26]poseAUC@5°),indoorscenes(ScanNet
level,object-centricandaerialimages. OmniGlue’snovel [8]poseaccuracy@5°),aerialscenes(DeepAerial[36]PCK@1%)
components lead to relative gains on unseen domains of andobject-centricimages(GSO-Hard[12]andNAVI-MultiView/
20.9%withrespecttoadirectlycomparablereferencemodel, NAVI-Wild[19],poseaccuracy@5°).
whilealsooutperformingtherecentLightGluemethodby
9.5%relatively. Codeandmodelcanbefoundathttps: Despite substantial progress, these advancements over-
//hwjiang1510.github.io/OmniGlue. lookanessentialaspect: thegeneralizationcapabilityof
imagematchingmodels. Today,mostlocalfeaturematch-
ingresearch[13,28,45]focusesonspecificvisualdomains
1.Introduction
withabundanttrainingdata(e.g.,outdoorandindoorscenes),
Local image feature matching techniques provide fine- leadingtomodelsthatarehighlyspecializedforthetraining
grainedvisualcorrespondencesbetweentwoimages[31], domain. Unfortunately,weobservethattheperformanceof
whicharecriticalforachievingaccuratecameraposeestima- thesemethodsusuallydropsdramaticallyonout-of-domain
tion[40,42]and3Dreconstruction[4,16,20,43]. Thepast data(e.g.,object-centricoraerialcaptures),whichmaynot
decadehaswitnessedtheevolutionfromhand-crafted[3,30] even be significantly better than traditional approaches in
tolearning-basedimagefeatures[10,37,39,52,56]. More some cases. For this reason, traditional domain-agnostic
recently, novel learnable image matchers have been pro- techniques,suchasSIFT[30],arestillwidelyusedtoobtain
posed [13, 28, 42, 45, 48], demonstrating ever-improving posesfordownstreamapplications[2,25,32,49].Duetothe
performanceonconventionalbenchmarks[1,8,26]. costofcollectinghigh-qualitycorrespondenceannotations,
webelieveitisunrealistictoassumethatabundanttraining
datawouldbeavailableforeachimagedomain,likeinsome
∗ThisworkwascompletedwhileHanwenwasaninternatGoogle. othervisiontasks[9,27].Thus,thecommunityshouldfocus
1
4202
yaM
12
]VC.sc[
1v97921.5042:viXraondevelopingarchitecturalimprovementstomakelearnable ofdatafromthetargetdomain,weshowthatOmniGluecan
matchingmethodsgeneralize. beeasilyadaptedwithanimprovementupto8.1%(94.2%
Motivated by the above observations, we propose relatively).
OmniGlue, the first learnable image matcher that is de-
signed with generalization as a core principle. Building 2.RelatedWork
ontopofdomain-agnosticlocalfeatures[10],weintroduce
GeneralizableLocalFeatureMatching. Priortothedeep
noveltechniquesforimprovingthegeneralizabilityofmatch-
learningera,researchersfocusedondevelopinggeneraliz-
inglayers:foundationmodelguidanceandkeypoint-position
ablelocalfeaturemodels. Forexample,SIFT[30],SURF
attentionguidance. AsshowninFig.1,withtheintroduced
[3]andORB[41]havebeenwidelyusedforimagematch-
techniques, we enable OmniGlue to generalize better on
ingtasksacrossdiverseimagedomains. Stilltoday,many
out-of-distributiondomainswhilemaintainingqualityper-
computervisionsystemsignorerecentadvancesinlearnable
formanceonthesourcedomain.
localfeaturesandrelyonhand-craftedmethods,forexample,
Firstly,weincorporatebroadvisualknowledgeofafoun-
toobtainposesfordownstreamapplications[2,25,32,49].
dationmodel.Bytrainingonlarge-scaledata,thefoundation
One of the main reasons for such old hand-crafted meth-
model, DINOv2 [35], performs well in diverse image do-
ods to continue being adopted is that most of the recent
mains on a variety of tasks, including robust region-level
learning-basedmethods[14,33,34,39,50]arespecialized
matching[22,35,57].Eventhoughthegranularityofmatch-
to domains with abundant training data, such as outdoor
ingresultsyieldedfromfoundationalmodelsislimited,these
buildingscenes,anddonotgeneralizewelltootherdomains.
modelsprovidegeneralizableguidanceonpotentialmatch-
Recently,thecommunityshiftedthemainfocustodevelop
ingregionswhenaspecializedmatchercannothandlethe
learnableimagematchers,whichassociatelocalfeaturespro-
domainshift. Thus,weuseDINOtoguidetheinter-image
ducedbyoff-the-shelfmethods[10]orjointlylearnfeature
feature propagation process, downgrading irrelevant key-
description and association [45]. While they demonstrate
pointsandencouragingthemodeltofuseinformationfrom
betterperformancecomparedwithhand-craftedmatching
potentiallymatchableregions.
systems,theymaketheentireimagematchingpipelineeven
Secondly, we also guide the information propagation moredomain-specific. Ourexperimentsshowthatlearnable
process with keypoint position information. We discover matchersspecializestronglyinthetrainingdomain,withlim-
that previous positional encoding strategies [42] hurt per- itedgeneralization. OurproposedOmniGlueimprovesthe
formance when the model is applied to different domains generalizationcapabilityofexistinglearnablematchersby
– which motivates us to disentangle it from the matching introducingguidancefromfoundationmodelsandimproved
descriptorsusedtoestimatecorrespondence. Wepropose positionalencoding.
anovelkeypoint-positionguidedattentionmechanismde- Sparse Learnable Matching. Sparse learnable image
signedtoavoidspecializingtoostronglyinthetrainingdis- matchingmethods[6,28,42]associatesparsekeypoints,pro-
tributionofkeypointsandrelativeposetransformations. ducedbykeypointdetectors. Forexample,SuperGlue[42]
Experimentally, we assess OmniGlue’s generalization usesSuperPoint[10]forkeypointdetectionandleveragesthe
acrossdiversevisualdomains,spanningsyntheticandreal attentionmechanism[51]toperformintra-andinter-image
images,fromscene-leveltoobject-centricandaerialdatasets, keypointfeaturepropagation. However,SuperGluesshows
withsmall-baselineandwide-baselinecameras. Wedemon- limitedgeneralizationcapability. Onereasonisthatitentan-
stratesignificantimprovementscomparedtopreviouswork. glesthelocaldescriptorsandpositionalinformationofthe
Inmoredetail,ourcontributionsareasfollows. keypoints,makingthematchingprocessoverlydependent
Contributions. (1) We introduce foundation model guid- onlearnedpositionalpatterns. Ithindersthegeneralizability
ancetothelearnablefeaturematchingprocess,whichlever- todatawithdifferentposition-relatedmatchingpatterns. To
agesbroadvisualknowledgetoenhancecorrespondences solvethisproblem,OmniGlueproposestodisentanglethem
indomainsthatarenotobservedattrainingtime,boosting duringthefeaturepropagation,releasingtherelianceonpo-
poseestimationaccuracybyupto5.8%(14.4%relatively). sitionalpatternsandimprovingthegeneralizationcapability
(2) A new strategy for leveraging positional encoding of toimagesfromdiversedomains.
keypoints, which avoids an overly reliant dependence on (Semi-)DenseLearnableMatching.Denseimagematching
geometricpriorsfromthetrainingdomain,boostingcross- methodsjointlylearntheimagedescriptorsandthematching
domaintransferbyupto6.1%(14.9%relatively). (3)We module,performingpixel-wisematchingontheentireinput
performcomprehensiveexperimentson7datasetsfromvar- images [7, 13, 45, 47, 53]. They benefit from the end-to-
ieddomains,demonstratingthelimitedgeneralizabilityof endlearningpipelineanddemonstratebetterperformancein
existingmatchingmethodsandOmniGlue’sstrongimprove- thetrainingdomain. Forexample,thesemi-densemethod
ments,withrelativegainsof20.9%onaverageinallnovel LoFTRintroducesacoarse-to-finecorrespondencepredic-
domains.(4)Byfine-tuningOmniGlueusinglimitedamount tionparadigm[45]. Anotherlineofworkdirectlypredicts
2Keypoints& DINO Intra-Image Inter-Image Matching
Input Images Descriptors Features Graphs Graphs Result
𝐼𝐴 𝑨 𝑮𝐴 𝑮𝐵→𝐴 𝑮𝐴→𝐵 Information Propagation
w. Position Guidance
𝑨 Self- Cross-
Atten. Atten.
Encoders
{𝒅𝐴,𝒑𝐴} 𝒈𝐴 Build Graphs
w. 𝑮𝐴 w. 𝑮𝐵→𝐴
𝐼𝐵 S Pu op ine tr DINO 𝑩 Gw u. i dD aIN nO ce 𝑮𝑩 shared shared
𝑩 Self- Cross-
Atten. Atten.
w. 𝑮𝐵 w. 𝑮𝐴→𝐵
{𝒅𝑩,𝒑𝑩} 𝒈𝑩 xNblocks
Figure2. OmniGlueoverview. WeusefrozenDINOandSuperPointtodetectkeypointsandextractfeatures. Then,webuilddensely
connectedintra-imagekeypointgraphsandleverageDINOfeaturestobuildinter-imagegraphs.Werefinethekeypointfeaturesbasedonthe
constructedgraphs,performinginformationpropagation.Inthisprocess,weusekeypointpositionssolelyforguidance,disentanglingthem
fromthekeypointlocaldescriptors.Finally,thematchingresultsareproducedbasedontheupdatedkeypointlocaldescriptors.
the matching results as a 4D correlation volume [13, 47]. 3.1.ModelOverview
However, we notice that some of them generalize worse
Fig. 2 presents a high-level overview of our OmniGlue
on new domains compared with sparse methods. Thus,
method, with four main stages. First, image features are
OmniGluechoosestofocusonsparsemethods,whichcan
extractedusingtwocomplementarytypesofencoders: Su-
havebetter potentialto begeneralizable dueto theuse of
perPoint [10], focusing on generic fine-grained matching;
domain-agnosticlocaldescriptors.
and DINOv2 [35], an image foundation model which en-
MatchingwithAdditionalImageRepresentations. Lever-
codescoarsebutbroadvisualknowledge. Second,webuild
aging robust image representations is a promising avenue
keypointassociationgraphsusingthesefeatures,bothintra
toward generalizable image matching. One line of work
and inter-image. In contrast to previous work, our inter-
usesgeometricimagerepresentations,e.g.,depthmap[54]
imagegraphleveragesDINOv2guidance,whichprovides
andNOCSmap[24],toaugmenttheimagematchingpro-
acoarsesignalcapturinggeneralsimilaritybetweenSuper-
cess. However, they are dependent on a highly accurate
Pointkeypoints. Third,wepropagateinformationamongthe
monocular estimation of these geometric representations.
keypointsinbothimagesbasedonthebuiltgraphs,usingself
Differently,SFD2[55]usessemanticsegmentationresults
andcross-attentionlayersforintraandinter-imagecommuni-
torejectindistinguishablekeypointsinbackgroundregions.
cation,respectively. Crucially,wedisentanglepositionaland
Nevertheless, the semantic segmentation model has to be
appearancesignalsatthisstage,differentfromothermodels
trainedoneachspecifictargetdomain.Recently,largevision
that overlook this aspect. This design enables feature re-
models, e.g., self-supervisedvisionbackbones[5,17,35]
finementtobeguidedbybothkeypointspatialarrangement
andDiffusionmodels[18,46,57]demonstraterobustseman-
andtheirfeaturesimilarities,butwithoutcontaminatingthe
ticunderstandingproperties. Bytrainingonlargedata,these
finaldescriptorswithpositionalinformation,whichhinders
modelsshowcasestronggeneralizationcapabilityacrossdi-
generalizability. Finally, once the refined descriptors are
verse domains [21, 22, 29], which enables them to obtain
obtained,optimalmatchinglayersareappliedtoproducea
coarsepatch-levelmatchingresults. However,performing
mappingbetweenthekeypointsinthetwoimages. These
matching using image features extracted by these models
stagesaredescribedinmoredetailinthefollowingsection.
demonstrateslimitedperformanceonregions/keypointswith-
out strong semantic information and the accuracy is lim- 3.2.OmniGlueDetails
ited[23,57]. Insteadofdirectlyincorporatingthesecoarse
FeatureExtraction. Theinputsaretwoimageswithshared
signalsintothekeypointfeaturesandusingthemtoperform
content,denotedasI andI . WedenotetheSuperPoint
A B
matching,OmniGlueusesDINOv2featurestoidentifypo-
keypointsetsofthetwoimagesasA:={A ,...,A }and
1 N
tentiallyrelatedregionsandguidetheattention-basedfeature
B := {B ,...,B }. Note that N and M are the number
1 M
refinementprocess. Thankstothewidedomainknowledge
of identified keypoints of I and I , respectively. Each
A B
encodedinthismodel,OmniGluecanboostthegeneraliza-
keypointisassociatedwithitsSuperPointlocaldescriptor
tionabilityofourmethodtodiversedomains. d ∈ RC. Additionally, normalizedkeypointlocationsare
encodedwithpositionalembeddings,andwefurtherrefine
3.OmniGlue
themusingMLPlayers. Wedenotetheresultingpositional
Wefirstintroducetheoverviewandtechnicaldetailsofour featuresofakeypointasp∈RC. Furthermore,weextract
methodOmniGlue. ThenwecompareOmniGluewithSu- denseDINOv2featuremapsofthetwoimages. Weinter-
perGlueandLightGlueforclarifyingtheirdifferences. polate the feature maps using the location of SuperPoint
3Pairwise DINO similarity 𝑮 B→A𝑖 on the intra-image graphs, performing self-attention; The
1 𝑨 𝚫𝒅 𝑖 secondupdateskeypointsbasedontheinter-imagegraphs,
𝒊
performing cross-attention. In particular, this stage intro-
ducestwonovelelementscomparedtopreviouswork,which
Softmax weshowarecriticaltowardsgeneralizablematching: suit-
Prune ableguidancefromDINOv2andfromkeypointpositions.
𝑩 𝑩 First,DINOv2guidance: duringcross-attention,forkey-
pointA ,itonlyaggregatesinformationfromtheDINOv2-
W𝑞 W𝑘 𝑊𝑣 i
prunedpotentialmatchingsetselectedfromB,insteadof
𝒅 +𝒑 𝒅𝑆+𝒑𝑆 𝒅𝑆 allitskeypoints. Thisisparticularlyhelpfulforgeneralized
0 𝑖 𝑖
imagematching,whereDINO’sbroadknowledgemayguide
Figure3.(Left)Buildinginter-imagegraph.Weprunethedense thefeaturematchingprocessinadomainthatthemodelhas
pairwise graph based on the DINO feature similarity. (Right) notseenattrainingtime. Inthismanner,informationfrom
Position-guidedattention. Thekeypointpositionisinvolvedin irrelevantkeypointswillnotbefusedintothequerykeypoint
computingattentionweights,whiletheoutputattentionupdateis features. Thisprocessalsoencouragesthecross-attention
onlycomposedoflocaldescriptorcomponents. moduletofocusondistinguishingthematchingpointinthe
smallerpotentialmatchingset.Note,however,thatwedonot
keypointstoobtainDINOv2descriptorsforeachkeypoint, forciblylimitthematchingspacetothepotentialmatching
denotedasg∈RC′.Forclarity,wedenotethethreefeatures
sets,asDINOmayalsobeincorrectinsomecases.
of the ith keypoint in set A as dA, pA and gA. The fea- Second, we introduce refined keypoint guidance. We
i i i
turesofthekeypointsinsetBaredenotedaccordingly. The observethatpriormethodsentanglekeypointpositionalfea-
goalofourOmniGluemodelistoestimatecorrespondences turesandlocaldescriptorsduringfeaturepropagation[42],
betweenthetwokeypointsets. whichmakesthemodeloverlydependentonlearnedposition-
GraphBuildingLeveragingDINOv2. Webuildfourkey- relatedpriors–ourablationexperimentsinSection4high-
pointassociationgraphs: twointer-imagegraphsandtwo light this issue. The learned priors are vulnerable under
intra-imagegraphs. Thetwointer-imagegraphsrepresent image pairs with matching patterns that were not seen at
theconnectivitybetweenthekeypointsofthetwoimages, trainingtime,limitingthegeneralizationcapability. Todeal
fromI A toI B andviceversa. WedenotethemasG A→−B withthisissue,weproposeanovelposition-guidedattention,
andG B→−A,respectively. Thetwointer-imagegraphsare whichdisentanglesthekeypointpositionalfeaturespand
directed,whereinformationispropagatedfromthesource thelocaldescriptorsd. Thepositionalinformationisused
nodetothetargetnode. asspatialcontextinthismoduleandisnotincorporatedin
thefinallocaldescriptorrepresentationusedformatching.
We leverage DINOv2 features to guide the building of
Withthesenovelelements,ourattentionlayer,illustrated
theinter-imagegraphs. AsdepictedinFig.3(left),wetake
in Fig. 3 (right), is defined as follows, where we take the
G B→−Ai asanexample. ForeachkeypointA i inkeypoint
exampleofkeypointA :
setA,wecomputeitsDINOv2featuresimilaritieswithall i
keypointsinsetB. Notethatweperformchannel-wisenor- dA ←dA+MLP([dA|∆dA]),where (1)
malizationontheDINOv2featuresgAandgB beforecom- i i i i
i qA(kS)T
putingthesimilarities. Weselectthetophalfofkeypoints ∆dA =Softmax( i√ )·vS ∈RC,and (2)
i
in set B with the largest DINOv2 similarities to connect C
withA ,whichprunesthedensely-connectedpairwisegraph qA =Wq(dA+pA)+bq ∈RC, (3)
i i i i
betweenthekeypointsofthetwoimages. Weperformthe kS =Wk(dS +pS)+bk ∈RK×C, (4)
sameoperationonallkeypointsinAtoobtainG B→−A,and
thegraphG A→−B isbuiltinasimilarmanner. vS =Wv(dS)+bv ∈RK×C. (5)
Similarly, theintra-imagegraphsrepresenttheconnec- AsdescribedinEq.1,theattentionhasaresidualconnection,
tivitybetweenkeypointsbelongingtothesameimage. We whichintegratestheattentionupdatevalue∆dA. Thenota-
i
denote them as G A and G B, which are undirected – in- tion←istheupdatingoperationand[·|·]isthechannel-wise
formationispropagatedbi-directionallybetweenconnected concatenation. To compute the attention update value, as
keypoints. Eachkeypointisdenselyconnectedwithallother described in Eq. 2, we compute the feature similarity be-
keypointswithinthesameimage. tweenthekeypointA anditssourceconnectedkeypoints
i
InformationPropagationwithNovelGuidance. Weper- inagraph,whichisdenotedasS containingK keypoints.
forminformationpropagationbasedonthekeypointgraphs. Thequery,keyandvalueoftheattentionareqA,kS,and
i
Thismodulecontainsmultipleblocks,whereeachblockhas vS,respectively. Specifically,asshowninEq.3-5,thequery
twoattentionlayers. Thefirstoneupdateskeypointsbased andkeyarecomputedbyfusingbothlocaldescriptorsand
4positionalfeatures. Thevalue,however,istransformedfrom (1) (2) (3) (4) (5) (6) (7) (8)
Real Syn. Cam. Diff.
only the local descriptors. We note that the weights (W) Type Scene Mask Task
Img. Trans. Bl. Bg.
andbias(b),whichmapfeaturesintoquery,keyandvalue MegaDepth Scene Outdoor ✓ ✗ ✗ Large ✗ Corr.&PoseEst.
tokensinattention,arenotsharedacrossdifferentattention GSO-Hard Object None ✗ ✗ ✗ Large ✗ PoseEst.
GSO-Easy Object None ✗ ✗ ✗ Small ✗ PoseEst.
layers. Inself-attention(G A andG B),S iscomposedby NAVI-MV Object In&Outdoor ✓ ✗ ✓ Large ✗ PoseEst.
all keypoints; in cross-attention (G A→−B and G B→−A), S N SA cV aI n- NW eil td O Scb eje nc et In& InO dou otd roor ✓
✓
✗
✗
✓
✗
L La ar rg ge
e
✓
✗
PP oo ss ee EE ss tt ..
containsthekeypointsidentifiedbyDINO. SH Scene Outdoor ✓ ✓ ✗ Small ✗ Corr.Est.
Intuitively, the query and key compute the attention DeepAerial Scene Aerial ✓ ✓ ✗ N/A ✓ ImageReg.
weights,wherebothfeatureaffinityandspatialcorrelations Table1. Datasetandtaskcomparisonson: (1)Thegeneraltype;
areconsidered.However,theattentionupdatevalue,∆dA,is (2)Thebackgroundscenetype; (3)Useofreal(✓)orrendered
i (✗)images;(4)Whethertheposetransformationissynthetic;(5)
composedoflocaldescriptorcomponentsonly. Thisdesign
Whetherforegroundmasksareusedtofiltercorrespondencepredic-
allowsthemodeltoreasonaboutspatialcorrelationbetween
tions;(6)Thecamerabaselinetype;(7)Whethertwoinputimages
keypointsusingtheirpositionalfeatureswhileavoidingan
havedifferentbackgrounds;(8)Evaluatedtasks:Correspondence
over-relianceonit.
Estimation,PoseEstimationorImageRegistration.
MatchingLayerandLossFunction. Weusetherefined
keypoint representations to produce a pairwise similarity
4.1.ExperimentalSetup
matrixS∈RN×M,whereS =dA·(dB)T. Thenweuse
i,j i j
WelistthedatasetsandtasksusedforevaluatingOmniGlue
theSinkhornalgorithm[44]torefinethesimilarities,which
producesthematchingmatrixM∈[0,1]N×M,whereM inTable1. Weincludedetailsofdatasetsasfollows:
i,j
represents the matching probability between keypoint A • SyntheticHomography(SH)containsimagesfromthe
i
andB . TotrainOmniGlue,weminimizethenegativelog- OxfordandParisdataset[38]. Wegeneraterandomcrops
j
likelihoodofthematchingmatrixwithgroundtruth[42,45]. andhomographytransformationstosampleimagepatch
pairs,similarto[42]. Twosubsetsaregenerated,SH100
3.3.ComparisonAgainstSuperGlueandLightGlue andSH200, whereintheperturbationsoftheimagecor-
nersforhomographygenerationarewithin100and200
Itisimportanttohighlightdifferencesbetweenourmodel
pixels,respectively. Foreachsubset,wegenerateroughly
and reference sparse learnable feature matching methods,
9milliontrainingpairsand10Ktestpairs.
SuperGlue[42]andLightGlue[28]. Whileneitherofthese
• MegaDepth (MD) [26] is a large-scale outdoor image
is designed to target generalizability to multiple domains,
dataset. The ground-truth matches are computed using
therearecommon elementsinthemodel structure, so we
SfM[43].Wefollowthetrain/testsplitofpriorworks[45],
wouldliketoemphasizeournovelty.
withroughly625Ktrainingpairsand1500testpairs.
Bothworksuseattentionlayersforinformationpropaga-
• Google Scanned Objects (GSO) [12] comprises 1400
tion. Differently,OmniGlueleveragesafoundationmodel
daily object model scans of 17 categories. We render
toguidethisprocess,whichsignificantlyhelpswithtransfer-
syntheticimageswithlarge(60°-90°)rotation(Hardsub-
ringtoimagedomainsthatarenotobservedduringtraining.
set) and small (15°- 45°) rotation (Easy subset) camera
Intermsoflocaldescriptorrefinement,OmniGluedeparts baselines,intentionallydistinctfromthetrainingdistribu-
from SuperGlue to disentangle positional and appearance tion. Weproduce50imagepairsforeachobjectmodel,
features. Forreference,SuperGluerepresentskeypointwith resultinginaround140Ktestcases.
entanglingthetwofeaturesasd+p,wherepositionalfea- • NAVI[19]focusesonobjectsandencompassesavariety
tures are also used to produce matching results. Similar ofbothindoorandoutdoorimages. Itisdividedintotwo
to our design, LightGlue removes the dependency of the subsets: themultiviewsubset(25Kimagepairs),featur-
updateddescriptorsonthepositionalfeatures. However,it inginputimagescapturedinthesameenvironment;and
proposes a very specific positional encoding formulation, the wild subset (36K image pairs), where the two input
basedonrotaryencodings,onlyinself-attentionlayers. imagesaretakenindifferentenvironmentswithdistinct
Overall, SuperGlue is the closest model to OmniGlue, backgrounds,lightingconditionsandcameramodels.
servingasadirectlycomparablereferencewhereourcon- • ScanNet[8]collectsindoorimages. Wefollowthesplit
tributions can be clearly ablated. For this reason, in the ofpriorworks[45]with1500evaluationpairs.
followingsection,weuseSuperGlueasthemainreference • DeepAerialMatching[36]providesalignedpairsofsatel-
comparisonforexperimentalvalidation. liteimagesundervaryingconditions(i.e.differentseasons,
weather,time-of-day). Weintroducerandom2Drotations
4.Experiments andcrop520×520imagepatchestoproduceimagepairs
withknownaffinetransformations(500intotal).
Wefirstintroducetheexperimentsetupandthenpresentour
resultsaswellasablationstudies. Tasks and metrics. We assess the models across three
5Figure4.VisualizationofcorrespondencespredictedbyOmniGlueontheMegaDepth-1500benchmark.Wedistinguishthematchesby
differentcolors.Weshowresultsforscene"0022"and"0015"onthetopandbottomrows,respectively.
tasks: (1) Correspondence estimation, evaluated with Setting→ TestPerformance(in-domain)
correspondence-levelprecisionandrecall(forsparsemeth- SH100 SH200
odsonly). FollowingSuperGlue[42],weemploythresholds DINOv2[35]+SG[42] 87.6/88.4 79.8/80.2
of< 3pxand> 5pxtolabelacorrespondenceascorrect SP[10]+SG[42] 99.2/99.4 95.4/96.0
and incorrect, respectively. (2) Camera pose estimation, OmniGlue(ours) 99.2/99.5 96.4/98.0
evaluated with pose accuracy (% of correct poses within Setting→ TestGeneralization(src→trg)
{5◦,10◦,20◦}oferror)andAUC,withaccuracybeingused SH100→SH200 SH200→MD
bydefaultunlessotherwisespecified. Theposesarederived DINOv2[35]+SG[42] 72.6/77.3 19.2/18.8
fromtheestimatedcorrespondencesusingRANSAC[15], SP[10]+SG[42] 78.3/75.6 34.9/39.0
andweuseRodrigues’formulatocalculaterelativerotation OmniGlue(ours) 90.0/89.6 36.0/54.7
errorbetweenthepredicted/groundtruthrotationmatrices; relativegain(%) +14.9/18.5 +4.3/+40.3
(3)Aerialimageregistration,evaluatedwithpercentageof Table2.Resultsforin-domain(top)andzero-shotgeneralizationto
correct keypoints (PCK). We use RANSAC-based affine out-of-domaindatasets(bottom),formodelstrainedonSynthetic
estimationfromtheestimatedcorrespondences,andapply Homography(SH)datasets. Wemeasureprecision/recallatthe
thepredicted/groundtruthaffinetransformationsto20test correspondencelevel.
keypointswithfixedpositionstocalculatethePCKwithin
methods, weuse1024keypointsand256-dimensionalde-
τ ·max(h,w)pixelsoferror,forτ ∈{0.01,0.03,0.05}.
scriptors. Seemoretrainingdetailsinsupplementary.
Baselines. WecompareOmniGlueagainst:
4.2.Results
• SIFT[30]andSuperPoint[10]providedomain-agnostic
local visual descriptors for keypoints. We generate Following SuperGlue and LightGlue, we first initialize
matchingresultsusingbothnearestneighbor+ratiotest OmniGlue by training it on SH100. Then we further pre-
(NN/ratio)andmutualnearestneighbor(MNN),withthe trainOmniGlueonSH200,andfinallytrainOmniGlueon
bestoutcomesbeingreported. MegaDepth(MD).WeevaluateOmniGlueandallbaseline
• Sparsematchers: SuperGlue[42]employsattentionlay- methodsonthetestsplitsofeachtrainingdomain,andtest
ersforintra-andinter-imagekeypointinformationaggre- theirgeneralizationtobothsubsequenttrainingdatasetsor
gation,usingdescriptorsderivedfromSuperPoint[10]. It out-of-domain test datasets. Finally, we experiment with
istheclosestreferenceofOmniGlue. LightGlue[28]im- adaptingOmniGluetoout-of-domainimageswithlimited
provesSuperGlue[42]withbetterperformanceandspeed. targetdomaintrainingdata.
Besides,wealsotestwithDINOv2[35]+SuperGlue,by
FromSyntheticHomographytoMegaDepth. Asdepicted
substitutingSuperPointdescriptorswithDINOfeatures.
in Table 2, in comparison to the base method SuperGlue,
• (Semi-)Densematchers: LoFTR[45]andPDCNet[47]
OmniGlue not only exhibits superior performance on the
areusedasreferencedensematchingtechniques,tocon-
in-domaindatabutalsodemonstratesrobustgeneralization.
textualizeoursparsematchingperformancewithrespect
EvenwithaminimaldatadistributionshiftfromSH100to
toothertypesofapproaches.
SH200,SuperGlueexperiencessubstantialdropsinperfor-
Implementationdetails. InlinewithSuperGlue[42],we mancewitha20%reductioninprecisionandrecall. This
implement9contextualreasoningblocks,eachcomprising resultimpliesthatSuperGlueisoverlydependentonlearned
anintra-imageaggregationlayer(self-attention)andaninter- position-relatedpatternsandisunabletohandlefurtherim-
imageaggregationlayer(cross-attention).Thisconfiguration agewarpingdistortion. Incontrast, OmniGlueshowcases
resultsinatotalof18attentionallayers. Acrossallsparse stronggeneralizationcapability,surpassingSuperGluewith
6Figure5.Zero-shotgeneralizationtonoveldomains.ThetopandmiddlerowshowresultsonGSOandNAVI,thelastrowshowsresultson
ScanNetandDeepAerial.Wedrawthecorrectandincorrectestimatedcorrespondencesasgreenandred,respectively.
In-domain Out-of-domain(Zero-shotGeneralization)
GoogleScannedObject NAVI
MegaDepth-1500 ScanNet DeepAerial
Hard(60-90deg.) Easy(15-45deg.) Multiview Wild
Method AUC@5°/10°/20° Acc@5°/10°/20° Acc@5°/10°/20° Acc@5°/10°/20° Acc@5°/10°/20° Acc@5°/10°/20° PCK@1%/3%/5%
DENSEANDSEMI-DENSEMETHODS
PDCNet[47] 51.5/67.5/78.2 5.1/8.9/14.9 24.8/36.7/49.3 3.9/7.1/11.6 6.6/11.6/17.0 38.6/60.0/71.3 14.0/20.9/22.6
LoFTR[45] 52.8/69.2/81.2 7.6/14.0/22.9 38.2/54.1/67.5 12.5/22.7/34.2 9.8/18.4/29.8 36.2/56.1/68.6 17.8/23.7/25.0
DESCRIPTOR+HAND-CRAFTEDRULES
SIFT[30]+MNN 25.8/41.5/54.2 6.8/12.1/20.3 32.5/46.2/60.3 6.2/11.9/22.7 4.2/8.1/23.1 4.6/10.6/20.2 17.5/25.9/32.2
SuperPoint[10]+MNN 31.7/46.8/60.1 5.4/10.5/18.8 28.9/43.4/58.0 10.0/19.2/31.6 8.2/16.0/28.0 18.8/35.2/49.6 16.0/24.3/31.9
SPARSEMETHODS
DINOv2[35]+SG[42] 31.5/40.8/45.3 3.6/7.3/15.1 12.0/22.7/38.7 7.3/15.6/28.3 8.4/17.2/30.6 9.7/26.7/41.5 11.4/18.2/23.1
SuperGlue[42] 42.2/61.2/76.0 7.2/13.2/21.6 32.3/48.4/62.9 11.8/21.9/34.4 10.6/19.8/31.8 25.5/43.4/57.3 16.4/26.2/28.8
LightGlue[28] 47.6/64.8/77.9 7.5/13.8/21.7 36.4/53.2/66.9 13.2/24.0/34.8 9.7/17.6/25.9 36.7/59.4/71.6 18.1/25.8/27.3
OmniGlue(ours) 47.4/65.0/77.8 8.6/15.3/25.0 38.4/54.8/68.8 13.2/24.8/37.7 12.4/22.8/35.0 31.3/50.2/65.0 22.4/33.5/36.6
rel.gain(%)over[42] +12.3/+6.2/+2.4 +19.4/+15.9/+15.7 +18.9/+13.2/+9.4 +11.9/+13.4/+9.6 +16.7/+15.2/+10.1 +22.0/+15.7/+13.4 +36.6/+27.9/+27.0
Table3.Resultsforin-domain(left,measuredwithAUC)andzero-shotgeneralizationtoout-of-domaindatasets(right,measuredwithpose
accuracy/PCK),formodelstrainedontheMegaDepthdataset.Wehighlightthebestresultsonout-of-domaindataandshowourrelative
improvementagainstourbasemethodSuperGlue.Allsparsemethodsuse1024keypoints.
a12%improvementinprecisionanda14%boostinrecall. eralizeworse. Theirperformancesareclose,orevenworse,
Similarly, during the transfer from SH200 to Megadepth, toSuperGlue, whichhas10%lowerin-domainAUC@5°.
OmniGlueoutperformsSuperGluewithadrastic15%im- Weconjecturethismaybeduetothejointlearningofvisual
provementinrecall. descriptorsandthematchingmodule,makingthemeasierto
FromMegaDepthtootherDomains. AsshowninTable3, specializestronglytothetrainingdomain.
OmniGlue not only achieves comparable performance on Low-ShotFine-tuningonTargetDomain. Incertainreal-
MegaDepth-1500 with the state-of-the-art sparse matcher world scenarios, a limited set of target domain data may
LightGlue, butalsodemonstratesbettergeneralizationca- beavailableforfine-tuning. Totestthisscenario,wefine-
pabilityon5outof6noveldomains,whencomparedtoall tune OmniGlue on the target domain (object-centric GSO
othermethods. Indetail,onMegaDepth-1500,OmniGlue dataset), comparing its performance with the base model,
showcases 12.3% relative gain (pose AUC @5°) over the SuperGlue. We create small training subsets by utilizing
basemethodSuperGlue.Onthe6noveldomains,OmniGlue onlyafewdozenobjectscans. Notably,thesesmalltraining
shows20.9%and9.5%averagedrelativegains(forposeand sets consist of instances from the sneaker object category
registrationaccuracyatthetightestthresholds)overSuper- only, covering a significantly minor subset of the testing
Glue and LightGlue, respectively. Moreover, OmniGlue objectcategorydistribution.
demonstrateslargerperformancegainsonhardernoveldo- AsdepictedinTable4,OmniGlueismorereadilyadapted
mains against LightGlue, i.e., on GSO-Hard, NAVI-Wild, tothetargetdomain. Indetail,whenscalingfrom0to30
andDeepAerial. WeshowvisualizationinFig.5andFig4 instances for training, OmniGlue consistently exhibits en-
forzero-shotgeneralizationonnoveldomainsanditsperfor- hancedperformanceforbothtestsubsets. Withjust10in-
manceonthesourcedomain. stances for training, OmniGlue improves pose estimation
Notably,thereferencedensematchers,whichachievebet- accuracyby5.3%and4.0%onthetwosubsets. Expanding
terperformanceonthein-domainMegaDepthdataset,gen- thetrainingsetsbyincorporating10moreobjectsleadsto
7#Train Hard(60-90deg.) Easy(15-45deg.) In-domain Out-of-domain
Model
Inst. @5°/10°/20° @5°/10°/20° MegaDepth GoogleScannedObject
SG 7.2/13.2/21.6 32.3/48.4/62.9 Hard Easy
0 P/R @5°/10°/20° @5°/10°/20°
OG 8.6/15.3/25.0 38.4/54.8/68.8
(0) SuperGlue[42] 67.2/68.3 9.0/16.9/27.3 40.4/60.5/76.6
SG 11.6/20.8/31.7 38.9/55.7/68.6
(1) onlyDINO-guide 66.6/68.0 10.0/18.7/29.6 46.2/65.4/79.5
10 OG 13.9/24.6/36.8 42.4/60.1/74.0
onlynopos.emb.-all 60.5/58.1 9.1/17.2/27.7 43.5/63.2/78.2
rel.gain(%) +61.6/+60.8/+47.2 +10.4/+9.7/+7.6
(2) onlynopos.emb.-cross 63.3/62.1 9.3/17.0/28.0 44.8/64.1/79.4
SG 13.0/22.9/35.2 40.3/57.0/70.5
onlypos.guidance 69.2/73.9 9.8/18.0/28.6 46.4/66.6/80.2
20 OG 15.3/27.0/39.7 44.1/61.5/75.0 (2)+DINO-SP-merge 62.6/65.6 7.8/14.9/24.9 42.5/61.3/75.4
(3)
rel.gain(%) +77.9/+76.5/+58.8 +14.8/+12.2/+9.0 (2)+DINO-guide-intra+inter 66.4/72.2 10.5/19.4/30.5 47.1/66.8/80.8
SG 14.6/25.2/37.9 42.0/59.2/71.2 (2)+DINO-guide-0.3 66.8/73.3 10.3/19.3/30.8 47.3/67.1/81.0
30 OG 16.7/29.1/42.3 45.8/62.5/76.0 (4) (2)+DINO-guide-0.4 66.8/73.1 10.2/18.9/30.4 47.2/66.9/80.8
rel.gain(%) +94.2/+90.2/+69.2 +19.3/+14.1/+10.5 (2)+DINO-guide-0.6 66.7/74.1 10.2/19.1/30.3 47.7/67.4/81.1
(5) (2)+DINO-guide-0.5(full) 66.2/74.1 11.0/20.4/32.0 48.7/68.4/82.3
Table4.Fine-tuningresultsofSuperGlue[42](SG)andourmethod
OmniGlue(OG)onGoogleScannedObject(GSO)dataset.Weuse Table5.Ablationstudyon(1)onlywithDINOguidance,(2)only
dozensofsneakerobjectinstancestogeneratetrainingdataandtest withthedisentangledkeypointrepresentationvariants,(3)DINO
onall17GSOcategories.Wealsoshowarelativegaincompared guidancevariantsanalysis(basedon(2)withpositionguidance),(4)
withthezero-shotperformance. DINOguidancethresholdanalysis,and(5)fullmodelOmniGlue.
Theexperimentrevealsadeclineinperformance,suggesting
a further performance improvement of 2%. Furthermore,
thatthe twofeatures arenotcompatible, likelydueto the
OmniGlue consistently surpasses SuperGlue, achieving a
coarse granularity of DINO. The manner in which these
relativegainofapproximately10%acrossallexperiments.
featurescanbeeffectivelymergedremainsanopenproblem.
The results collectively demonstrate the applicability of
ThesecondmethodentailsapplyingDINOv2guidance
OmniGlue in real-world scenarios as a versatile and gen-
forconstructingbothintraandinter-imagegraphs,demon-
eralizablemethod.
stratingdiminishedperformancecomparedto(5). Wehy-
4.3.AblationStudyandInsights pothesize that the reason lies in the fact that intra-image
information propagation (self-attention) requires a global
We conduct a comprehensive ablation study on each pro-
context,particularlyfordistinguishingallkeypointsinthe
posedmodule,asdetailedinTable5. Pleasenotethatthe
feature space. Reducing connectivity on the intra-image
numbersreportedontheGSOdatasetarebasedonasubset,
graph adversely affects the global context, aligning with
encompassinghalfofalltestcases,forrapidevaluation.
findingsinthestudyofattentionspaninSuperGlue.
Theeffectivenessofeachproposedtechnique. Theresults
Detailsoffoundationmodelguidance.Weablatethehyper-
inTable5(1)highlighttheeffectivenessofourfoundation
parameterusedtodeterminethenumberofsourcekeypoint
model guidance, which enhances the generalization capa-
inagraph,aspresentedinTable5(4). Theresultsindicate
bility on out-of-domain data. Additionally, the third row
thatselectingthetophalfofkeypointsintheotherimagefor
ofTable5(2)illustratestheimpactoftheposition-guided
buildinginter-imagegraphsistheoptimalchoice.
attention,showcasingimprovementinbothin-domainand
out-of-domaindata. Furthermore,weconductablationswith
5.ConclusionsandFutureWork
different approaches to disentangling keypoint positional
features. The first two rows of Table 5 (2) demonstrate We propose OmniGlue, the first learnable image matcher
thatperformancedegradeswheneithernotusinganyposi- thatisdesignedwithgeneralizationasacoreprinciple. We
tionalfeaturesorapplyingtheposition-guidanceonlyonself- introducethebroadvisualknowledgeofafoundationmodel,
attention (without positional guidance on cross-attention). whichguidesthegraph-buildingprocess. Weidentifythe
This emphasizes the effectiveness of our position-guided limitationofthepreviousdescriptor-positionentangledrep-
attentioninfacilitatinginformationpropagationwithinboth resentation and present a novel attention module to deal
intra-andinter-imagecontexts. Besides,afterremovingthe withit. WedemonstratethatOmniGlueoutperformsprior
positionalembeddings,themodelshowsbettergeneraliza- work with better cross-domain generalization. Moreover,
tion even though the in-domain performance drops. This OmniGluecanalsobeeasilyadaptedtoatargetdomainwith
resultimpliesthattheinappropriatewaythatSuperGlueuses alimitedamountofdatacollectedforfine-tuning. Forfuture
positionalinformationlimitsitsgeneralization. work,itisalsoworthexploringhowtoleverageunannotated
ThewaysofincorporatingDINOfeatures. Asshownin dataintargetdomainstoimprovegeneralization. Bothof
Table5(3),weexploredifferentmethodsofincorporating better architectural designs and better data strategies can
DINOv2. The first involves merging DINO features and pavethewayforafoundationalmatchingmodel.
SuperPointlocaldescriptors. Thisintegrationisperformed Acknowledgements.WewouldliketoacknowledgesupportfromNSFIIS-2047677,
beforetheinformationpropagationmoduleusinganMLP. HDR-1934932,CCF-2019844,andtheIARPAWRIVAprogram.
8Appendix E.AdditionalQualitativeResults
A.AdditionalModelDetails WeadditionallypresentqualitativeresultsofOmniGluein
Figure7.Wecompareourmethod(lastcolumn)withtworef-
OmniGlueundergoestrainingwith750,000iterationsusing erencematchingmethods: mutualnearestneighbors(MNN,
a batch size of 48 on 8 NVIDIA Tesla V100 GPUs. The firstcolumn)andSuperGlue[42](secondcolumn).Weshow
initial learning rate is set at 3e−5, with a decay rate of MNN with SIFT [30] features for two domains, and with
0.999991 and a hinge step of 55000. For DINOv2 [35] SuperPoint[10]featuresforone.WeobservethatOmniGlue
featureextraction,weusetheimageswithamaximumreso- producesimprovedmatchesforimagepairswithsignificant
lution(longside)of630,maintainingtheaspectratioduring changesinviewingconditions,acrossarangeofdomains.
imageresizing,forreducethecomputation. TheDINOv2
backboneemployedViT-14-base[11]. Weusetheimproved References
positionalembeddingschemeproposedinLFM-3D[24].
[1] V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk.
Hpatches:ABenchmarkandEvaluationofHandcraftedand
B.TargetDomainVisualization LearnedLocalDescriptors. InProc.CVPR,2017. 1
[2] J.Barron,B.Mildenhall,D.Verbin,P.Srinivasan,andP.Hed-
Toillustratethetargetimagedomainsweconsiderinthis man. Zip-NeRF:Anti-AliasedGrid-BasedNeuralRadiance
work, Figure 6 presents example images pairs from each Fields. InProc.ICCV,2023. 1,2
[3] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf:
domain,namely: GoogleScannedObjects[12],NAVI[19],
Speeded up robust features. In European Conference on
ScanNet-1500 [8], and DeepAerial [36]. This shows that
ComputerVision,2006. 1,2
ourtargetdatasetscoverawiderangeofobjectandscene
[4] César Cadena, Luca Carlone, Henry Carrillo, Yasir Latif,
types,constitutingachallengingtaskforgeneralizableimage
Davide Scaramuzza, José Neira, Ian D. Reid, and John J.
matching. Leonard. Past,present,andfutureofsimultaneouslocaliza-
tionandmapping:Towardtherobust-perceptionage. IEEE
TransactionsonRobotics,32:1309–1332,2016. 1
C.AreaUnderCurve(AUC)PoseResults
[5] MathildeCaron,HugoTouvron,IshanMisra,Herv’eJ’egou,
JulienMairal,PiotrBojanowski,andArmandJoulin. Emerg-
WealsoreportposeAUCperformance,asshowninTable6.
ingpropertiesinself-supervisedvisiontransformers. 2021
Becausethelimitedperformanceonout-of-domaindata,we
IEEE/CVF International Conference on Computer Vision
reportposeaccuracyinthemainpaper.
(ICCV),pages9630–9640,2021. 3
[6] HongkaiChen,ZixinLuo,JiahuiZhang,LeiZhou,Xuyang
D.Latencyanalysis. Bai,ZeyuHu,Chiew-LanTai,andLongQuan. Learningto
matchfeatureswithseededgraphmatchingnetwork. 2021
IEEE/CVF International Conference on Computer Vision
WenotethatnovelOmniGluemodulesdonothurtlatencyas
(ICCV),pages6281–6290,2021. 2
comparedthebaselineSuperGluemodel. EventhoughDI-
[7] HongkaiChen,ZixinLuo,LeiZhou,YurunTian,Mingmin
NOv2introducesadditionalcomputation,weuseitsfeatures
Zhen,TianFang,DavidN.R.McKinnon,YanghaiTsin,and
toprunethegraphsandreducethecomputationaccordingly.
Long Quan. Aspanformer: Detector-free image matching
Theoretically,thecomputationthatDINOv2introducesis withadaptivespantransformer. InEuropeanConferenceon
O(n (hw)2),wheren =9(numberofDINOv2attention ComputerVision,2022. 2
1 1
layers), h = H and w = W (H and W are input resolu- [8] AngelaDai,AngelX.Chang,ManolisSavva,MaciejHalber,
14 14
ThomasFunkhouser,andMatthiasNießner. Scannet:Richly-
tion to DINOv2). The computation that pruning saves is
O(2n kk′), wheren = 9(numberofinformationpropa- annotated3dreconstructionsofindoorscenes. InProc.Com-
2 2 puterVisionandPatternRecognition(CVPR),IEEE,2017. 1,
gation blocks), k = 1024 (number of target keypoints in
5,9,10
one image), k′ = k (number of pruned keypoints in the
2 [9] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi
otherimage)andthecoefficient2ismultipliedbecausethere Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase.
are2inter-graphaggregationmodulesineachblock. Itis In 2009 IEEE conference on computer vision and pattern
simplifiedasO(n k2). WiththeresolutionW =630anda recognition,pages248–255.Ieee,2009. 1
2
typicalaspectratioof16:9,thehw ≈k =1024. Thus,the [10] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-
introducedandsavedcomputationarebalanced. novich. Superpoint:Self-supervisedinterestpointdetection
anddescription. InProceedingsoftheIEEEconferenceon
WereporttheempiricalspeedresultsinTable7,which
computervisionandpatternrecognitionworkshops,pages
shows that OmniGlue runs at a similar frame rate as the
224–236,2018. 1,2,3,6,7,9,10
baselineSuperGluemodel(nographpruning).Inferencewas [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
performedonanNVIDIAA40GPUwithFlashAttention. Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
TheresultisreproducedwithusingGlue-Factory. MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
9Out-of-domain
GoogleScannedObject[12] NAVI[19] ScanNet[8]
Hard(60-90degree) Easy(15-45degree) Multiview Wild
Method AUC@5°/10°/20° AUC@5°/10°/20° AUC@5°/10°/20° AUC@5°/10°/20° AUC@5°/10°/20°
PDCNet[47] 2.6/4.8/8.4 13.5/22.4/33.0 1.7/3.7/6.6 2.9/6.1/10.4 16.4/33.7/51.2
LoFTR[45] 3.6/7.3/13.0 20.7/33.9/47.9 5.7/11.8/20.4 4.5/9.4/17.0 16.9/33.6/50.6
SIFT[30]+MNN 3.4/6.5/11.5 16.7/30.1/40.8 3.3/6.9/12.8 2.8/5.9/11.7 1.7/4.8/10.3
SuperPoint[10]+MNN 2.5/5.3/10.0 15.2/26.1/38.8 4.5/9.7/17.8 3.7/8.0/15.1 7.7/17.8/30.6
DINOv2[35]+SG[42] 1.8/3.6/7.4 5.5/11.6/21.3 3.3/9.7/.155.6 3.8/8.4/16.3 3.3/10.0/22.0
SuperGlue[42] 3.4/6.9/12.2 17.5/30.1/42.6 5.1/11.2/19.9 4.8/10.2/18.3 10.4/22.9/37.2
LightGlue[28] 3.5/7.1/12.6 18.9/32.3/46.7 5.7/12.4/21.2 4.3/9.2/15.7 15.1/32.6/50.3
OmniGlue(ours) 4.1/8.2/14.3 20.7/34.1/48.4 5.8/12.6/22.2 5.6/11.8/20.7 14.0/28.9/44.3
Table6. Relativecameraposeestimationperformance(AUC)andzero-shotgeneralizationcapabilityofmodelstrainedonMegaDepth
dataset.
SuperGlue OmniGlue
Speed(FPS) 52 51
Table7. Latencyanalysis,comparingSuperGlueandourOmniGlue. Forbothmodels,weincludefeatureextraction(SuperPoint)and
featurematchinginferencetimes.Additionally,weincludeDINOv2inferencetimeinourmeasurementsforOmniGlue.
vainGelly, etal. Animageisworth16x16words: Trans- Vlasic,VittorioFerrari,AmeeshMakadia,CeLiu,Yuanzhen
formers for image recognition at scale. arXiv preprint Li,andHowardZhou. Navi:Category-agnosticimagecollec-
arXiv:2010.11929,2020. 9 tionswithhigh-quality3dshapeandposeannotations. 1,5,
[12] LauraDowns,AnthonyFrancis,NateKoenig,BrandonKin- 9,10
man,RyanMichaelHickman,KristaReymann,ThomasBar- [20] HanwenJiang,ZhenyuJiang,KristenGrauman,andYuke
lowMcHugh,andVincentVanhoucke. Googlescannedob- Zhu. Few-view object reconstruction with unknown cate-
jects:Ahigh-qualitydatasetof3dscannedhouseholditems. goriesandcameraposes. arXivpreprintarXiv:2212.04492,
2022InternationalConferenceonRoboticsandAutomation 2022. 1
(ICRA),pages2553–2560,2022. 1,5,9,10 [21] HanwenJiang,ZhenyuJiang,YueZhao,andQixingHuang.
[13] JohanEdstedt,IoannisAthanasiadis,MårtenWadenbäck,and Leap:Liberatesparse-view3dmodelingfromcameraposes.
MichaelFelsberg. Dkm:Densekernelizedfeaturematching arXivpreprintarXiv:2310.01410,2023. 3
forgeometryestimation. InProceedingsoftheIEEE/CVF [22] HanwenJiang,SanthoshKumarRamakrishnan,andKristen
Conference on Computer Vision and Pattern Recognition, Grauman.Single-stagevisualquerylocalizationinegocentric
pages17765–17775,2023. 1,2,3 videos. arXivpreprintarXiv:2306.09324,2023. 2,3
[14] Johan Edstedt, Georg Bökman, Mårten Wadenbäck, and [23] ZhenyuJiang,HanwenJiang,andYukeZhu. Doduo:Learn-
Michael Felsberg. Dedode: Detect, don’t describe - de- ingdensevisualcorrespondencefromunsupervisedsemantic-
scribe, don’t detect for local feature matching. ArXiv, awareflow. arXivpreprintarXiv:2309.15110,2023. 3
abs/2308.08479,2023. 2 [24] ArjunKarpur,GuilhermePerrotta,RicardoMartin-Brualla,
[15] MartinA.FischlerandRobertC.Bolles. Randomsample HowardZhou,andAndreAraujo. Lfm-3d:Learnablefeature
consensus:aparadigmformodelfittingwithapplicationsto matchingacrosswidebaselinesusing3dsignals. InProc.
imageanalysisandautomatedcartography. Commun.ACM, 3DV,2024. 3,9
24:381–395,1981. 6 [25] K. Li, M. Runz, M. Tang, L. Ma, C. Kong, T. Schmidt, I.
[16] MichaelGoesele,BrianCurless,andStevenMSeitz. Multi- Reid,L.Agapito,J.Straub,S.Lovegrove,andR.Newcombe.
viewstereorevisited.In2006IEEEComputerSocietyConfer- FroDO:FromDetectionsto3DObjects.InProc.CVPR,2020.
enceonComputerVisionandPatternRecognition(CVPR’06), 1,2
volume2,pages2402–2409.IEEE,2006. 1 [26] ZhengqiLiandNoahSnavely. Megadepth:Learningsingle-
[17] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRossB. viewdepthpredictionfrominternetphotos. 2018IEEE/CVF
Girshick. Momentumcontrastforunsupervisedvisualrepre- Conference on Computer Vision and Pattern Recognition,
sentationlearning. 2020IEEE/CVFConferenceonComputer pages2041–2050,2018. 1,5
VisionandPatternRecognition(CVPR),pages9726–9735, [27] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
2019. 3 PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
[18] EricHedlin,GopalSharma,ShwetaMahajan,HossamIsack, Zitnick. Microsoft coco: Common objects in context. In
Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. ComputerVision–ECCV2014: 13thEuropeanConference,
Unsupervisedsemanticcorrespondenceusingstablediffusion. Zurich,Switzerland,September6-12,2014,Proceedings,Part
2023. 3 V13,pages740–755.Springer,2014. 1
[19] VarunJampani,Kevis-KokitsiManinis,AndreasEngelhardt, [28] PhilippLindenberger,Paul-EdouardSarlin,andMarcPolle-
ArjunKarpur, KarenTruong, KyleSargent, StefanPopov, feys. LightGlue:LocalFeatureMatchingatLightSpeed. In
AndreAraujo,RicardoMartin-Brualla,KaushalPatel,Daniel Proc.ICCV,2023. 1,2,5,6,7,10
10Figure6.Targetdomainexamples.Wesharesomeexampleimagepairsfromeachofthetargetimagedatasets.Fromtoprowtobottom
row,thedomainsare:GoogleScannedObjects(Hard),NAVIWildSet,NAVIMultiview,ScanNet-1500,andDeepAerial.
[29] RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov, conferenceoncomputervision,pages3456–3465,2017. 2
SergeyZakharov,andCarlVondrick. Zero-1-to-3:Zero-shot [34] YukiOno, EduardTrulls, PascalV.Fua, andKwangMoo
oneimageto3dobject. InProceedingsoftheIEEE/CVF Yi. Lf-net:Learninglocalfeaturesfromimages. InNeural
InternationalConferenceonComputerVision,pages9298– InformationProcessingSystems,2018. 2
9309,2023. 3 [35] MaximeOquab,Timoth’eeDarcet,ThéoMoutakanni,HuyQ.
[30] David G. Lowe. Distinctive image features from scale- Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
invariantkeypoints.InternationalJournalofComputerVision, DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,Mah-
60:91–110,2004. 1,2,6,7,9,10 moud Assran, Nicolas Ballas, Wojciech Galuba, Russ
[31] Jiayi Ma, Xingyu Jiang, Aoxiang Fan, Junjun Jiang, and Howes,Po-Yao(Bernie)Huang,Shang-WenLi,IshanMisra,
JunchiYan. Imagematchingfromhandcraftedtodeepfea- MichaelG.Rabbat,VasuSharma,GabrielSynnaeve,Huijiao
tures: Asurvey. InternationalJournalofComputerVision, Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand
129:23–79,2020. 1 Joulin,andPiotrBojanowski.Dinov2:Learningrobustvisual
[32] B.Mildenhall, P.Srinivasan, M.Tancik, J.Barron, R.Ra- featureswithoutsupervision. ArXiv,abs/2304.07193,2023.
mamoorthi,andR.Ng.NeRF:RepresentingScenesasNeural 2,3,6,7,9,10
RadianceFieldsforViewSynthesis. InProc.ECCV,2020. 1, [36] Jae-HyunPark,Woo-JeoungNam,andSeong-WhanLee. A
2 two-streamsymmetricnetworkwithbidirectionalensemble
[33] HyeonwooNoh,AndreAraujo,JackSim,TobiasWeyand, foraerialimagematching. RemoteSensing,12(3):465,2020.
andBohyungHan. Large-scaleimageretrievalwithattentive 1,5,9
deeplocalfeatures. InProceedingsoftheIEEEinternational [37] G.Potje,F.Cadar,A.Araujo,R.Martins,andE.Nascimento.
11Figure7.Qualitativematchingcomparison.Wecomparethefollowingmethods:mutualnearestneighbor(MNN,left),SuperGlue(center)
andOmniGlue(right).Greenlinesdenotecorrectcorrespondences,whileredonesdenoteincorrectpredictions.Thefirsttworowspresent
resultsonGoogleScannedObjects(Hard),thefollowingtworowsontheNAVIWildSet,andthefinaltworowsonDeepAerial.TheMNN
resultsuseSuperPointfeaturesinthefirsttworows,andSIFTfeaturesintheothers.
EnhancingDeformableLocalFeaturesbyJointlyLearningto [39] Jérôme Revaud, Philippe Weinzaepfel, César Roberto de
DetectandDescribeKeypoints. InIEEEConf.Comput.Vis. Souza,No’ePion,GabrielaCsurka,YohannCabon,andM.
PatternRecog.,2023. 1 Humenberger. R2d2: Repeatableandreliabledetectorand
[38] Filip Radenovic´, Ahmet Iscen, Giorgos Tolias, Yannis descriptor. ArXiv,abs/1906.06195,2019. 1,2
Avrithis, and Ondˇrej Chum. Revisiting oxford and paris: [40] BarbaraRoessleandMatthiasNießner. End2endmulti-view
Large-scaleimageretrievalbenchmarking. InProceedingsof featurematchingwithdifferentiableposeoptimization. In
theIEEEconferenceoncomputervisionandpatternrecogni- ProceedingsoftheIEEE/CVFInternationalConferenceon
tion,pages5706–5715,2018. 5 ComputerVision,pages477–487,2023. 1
12[41] EthanRublee,VincentRabaud,KurtKonolige,andGaryR. Fua. LIFT:LearnedInvariantFeatureTransform. InProc.
Bradski. Orb: Anefficientalternativetosiftorsurf. 2011 ECCV,2016. 1
InternationalConferenceonComputerVision,pages2564– [57] JunyiZhang,CharlesHerrmann,JunhwaHur,LuisaPolania
2571,2011. 2 Cabrera, VarunJampani, DeqingSun, andMingYang. A
[42] Paul-EdouardSarlin,DanielDeTone,TomaszMalisiewicz, taleoftwofeatures:Stablediffusioncomplementsdinofor
andAndrewRabinovich. Superglue:Learningfeaturematch- zero-shotsemanticcorrespondence. ArXiv,abs/2305.15347,
ing with graph neural networks. In Proceedings of the 2023. 2,3
IEEE/CVFconferenceoncomputervisionandpatternrecog-
nition,pages4938–4947,2020. 1,2,4,5,6,7,8,9,10
[43] JohannesLSchonbergerandJan-MichaelFrahm. Structure-
from-motionrevisited. InProceedingsoftheIEEEconfer-
enceoncomputervisionandpatternrecognition,pages4104–
4113,2016. 1,5
[44] RichardSinkhornandPaulKnopp. Concerningnonnegative
matricesanddoublystochasticmatrices. PacificJournalof
Mathematics,21(2):343–348,1967. 5
[45] JiamingSun,ZehongShen,YuangWang,HujunBao,and
XiaoweiZhou. Loftr: Detector-freelocalfeaturematching
withtransformers. 2021IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pages8918–8927,
2021. 1,2,5,6,7,10
[46] LumingTang, MenglinJia, QianqianWang, ChengPerng
Phoo,andBharathHariharan.Emergentcorrespondencefrom
imagediffusion. arXivpreprintarXiv:2306.03881,2023. 3
[47] PruneTruong,MartinDanelljan,LucVanGool,andRadu
Timofte. Learningaccuratedensecorrespondencesandwhen
to trust them. 2021 IEEE/CVF Conference on Computer
VisionandPatternRecognition(CVPR),pages5710–5720,
2021. 2,3,6,7,10
[48] PruneTruong,MartinDanelljan,RaduTimofte,andLucVan
Gool. Pdc-net+: Enhanced probabilistic dense correspon-
dencenetwork. 2023. 1
[49] M. Tyszkiewicz, K.-K. Maninis, S. Popov, and V. Ferrari.
RayTran: 3Dposeestimationandshapereconstructionof
multipleobjectsfromvideoswithray-tracedtransformers. In
Proc.ECCV,2022. 1,2
[50] Michal J. Tyszkiewicz, P. Fua, and Eduard Trulls. Disk:
Learning local features with policy gradient. ArXiv,
abs/2006.13566,2020. 2
[51] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
andIlliaPolosukhin. Attentionisallyouneed. InNeural
InformationProcessingSystems,2017. 2
[52] Yannick Verdie, Kwang Moo Yi, Pascal Fua, and Vincent
Lepetit. TILDE:ATemporallyInvariantLearnedDetector.
InProc.CVPR,2015. 1
[53] QingWang,JiamingZhang,KailunYang,KunyuPeng,and
RainerStiefelhagen. Matchformer:Interleavingattentionin
transformersforfeaturematching. InAsianConferenceon
ComputerVision,2022. 2
[54] Shuzhe Wang, Juho Kannala, Marc Pollefeys, and Daniel
Barath.Guidinglocalfeaturematchingwithsurfacecurvature.
InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages17981–17991,2023. 3
[55] Fei Xue, Ignas Budvytis, and Roberto Cipolla. Sfd2:
Semantic-guidedfeaturedetectionanddescription. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages5206–5216,2023. 3
[56] KwangMooYi,EduardTrulls,VincentLepetit,andPascal
13