Can We Treat Noisy Labels as Accurate?
YuxiangZheng1∗ ZhongyiHan2∗† YilongYin3 XinGao2 TongliangLiu1†
1SydneyAICenter,TheUniversityofSydney;
2CEMSE,KingAbdullahUniversityofScienceandTechnology;
3SchoolofSoftware,ShandongUniversity
Abstract
Noisylabelssignificantlyhindertheaccuracyandgeneralizationofmachinelearn-
ingmodels,particularlyduetoambiguousinstancefeatures. Traditionaltechniques
thatattempttocorrectnoisylabelsdirectly,suchasthoseusingtransitionmatrices,
oftenfailtoaddresstheinherentcomplexitiesoftheproblemsufficiently. Inthis
paper,weintroduceEchoAlign,atransformativeparadigmshiftinlearningfrom
noisylabels. Insteadoffocusingonlabelcorrection,EchoAligntreatsnoisylabels
(Y˜)asaccurateandmodifiescorrespondinginstancefeatures(X)toachievebetter
alignment with Y˜. EchoAlign’s core components are (1) EchoMod: Employ-
ingcontrollablegenerativemodels,EchoModpreciselymodifiesinstanceswhile
maintainingtheirintrinsiccharacteristicsandensuringalignmentwiththenoisy
labels. (2)EchoSelect: Instancemodificationinevitablyintroducesdistribution
shiftsbetweentrainingandtestsets. EchoSelectmaintainsasignificantportionof
cleanoriginalinstancestomitigatetheseshifts. Itleveragesthedistinctfeature
similaritydistributionsbetweenoriginalandmodifiedinstancesasarobusttool
foraccuratesampleselection. Thisintegratedapproachyieldsremarkableresults.
Inenvironmentswith30%instance-dependentnoise, evenat99%selectionac-
curacy,EchoSelectretainsnearlytwicethenumberofsamplescomparedtothe
previousbestmethod. Notably,onthreedatasets,EchoAlignsurpassesprevious
state-of-the-arttechniqueswithasubstantialimprovement.
1 Introduction
Therapidadvancementofneuralnetworkshighlightstheimportanceoflearningfromnoisylabels
(LNL)[TanandLe,2019,Dosovitskiyetal.,2021,Stiennonetal.,2020,Chenetal.,2023a]. Though
cost-effectiveforlargedatasets,webcrawling,andcrowdsourcingcanintroducenoisylabelsthat
hindermachinelearningmodelgeneralization[Yuetal.,2018,Lietal.,2017,Welinderetal.,2010,
Zhang et al., 2017, Natarajan et al., 2013, Gu et al., 2023]. For example, recent studies indicate
thislabelnoiseinpretrainingdataimpactsout-of-distributiongeneralizationoffoundationmodels
on downstream tasks [Chen et al., 2023a, 2024]. Noisy labels are categorized as random, class-
dependent,orinstance-dependent,withthelattertwobeingparticularlychallengingduetoambiguous
instancefeatures. Thisambiguitycomplicatesthedistinctionbetweenmislabeledexamplesandthose
belongingtootherclasses[Menonetal.,2018,Xiaetal.,2020,Yaoetal.,2023a,Baietal.,2023].
PriorworkhaslargelyapproachedLNLfromeithernoise-modeling-freeornoise-modelingstand-
points. Thenoise-modeling-freemethods,suchasextractingexampleswithsmalllosses[Hanetal.,
2018,Yuetal.,2019,Wangetal.,2019],canonlydocleansampleselectionandcannotcorrectnoisy
labelstocleanlabels,whichwastemuchsupervisioninformation. Ontheotherhand,noise-modeling
approachesexplicitlyconsiderthelabel-noisegenerationprocess[Scottetal.,2013,Scott,2015,
∗Equalcontributions.
†Correspondingauthor.
Preprint.Underreview.
4202
yaM
12
]GL.sc[
1v96921.5042:viXraGoldbergerandBen-Reuven,2016]. Thesemethodsoperateontheprinciplethatthecleanclass
posteriorcanbededucedusingthenoisyclassposterior,derivedfromnoisydata,inconjunctionwith
atransitionmatrix[Berthonetal.,2021]. Theconceptisthat,givenatransitionmatrix,anoptimal
classifiermirroringcleandata’sbehaviorcanbetrainedsolelyusingsufficientnoisydata[Reedetal.,
2014,LiuandTao,2015]. Nonetheless,accuratelymodelingthegeneralprocess,specificallythe
transitionmatrix,isill-posedbyonlyexploitingnoisydata[Xiaetal.,2019,Chengetal.,2020].
Theadditionalassumptionsrequiredforthismodelareoftenhardtovalidateandmaynotholdin
real-worlddatasets,asindicatedbyrecentstudies[Xiaetal.,2020,Yaoetal.,2023b,Liuetal.,2023].
Thismismatchfrequentlyresultsinaninabilitytoeliminatelabelnoisethoroughly.
Inthispaper,wechallengetheconventionalap-
proachtolabelnoisewithinstancemodification.
Insteadofattemptingtocorrectnoisylabels,we
strategicallyadjustinstancestobetteralignwith
their labels, even if those labels are incorrect.
ThisgroundbreakingnewdirectionofLNLisvi-
suallydepictedinFigure1. Atitscore,instance
modificationtacklestheunderlyingcauseofla-
bel noise. Drawing on principles from causal
learning[Neuberg,2003,Petersetal.,2017,Yao
etal.,2021],weviewthegenerationofinstance-
dependentlabelnoisethroughacausallens,as
Figure1: Instancemodificationeffectivelyaligns
illustratedinthecausalgraphofFigure2. Here,
instanceswiththeirlabels,whilelabelcorrection
theobservednoisylabels(Y˜)emergefromthe
struggleswithambiguouscases.
instances(X),influencedbylatentvariables(Z).
Incrowdsourcingscenarios,thedifficultyoflabelingblurredorambiguousinstancesoftenleadsto
errors. Insteadoftryingtoguessthe‘true’labelinthesecases,modifyingtheinstanceitselftomake
itmoreeasilyclassifiablecanbemoreeffective. Therefore,instancemodificationoffersapractical
solution for particularly challenging instances that are difficult to label correctly – those that are
inherentlyambiguousforbothhumansandmachines.
While instance modification offers a breakthrough, it presents significant challenges at both the
instance and dataset levels (More discussions can be found in the Appendix E). At the instance
level,acriticalchallengeliesinmaintainingtheinstance’scorecharacteristics. Modifyinginstances,
particularlythosewithincorrectlabels,tostrictlyalignwithpotentiallyunrelatedlabelscanlead
tosubstantialcharacteristicshifts. Imaginemodifyingadogimagetoresembleanairplane–the
resultingimagemightbeunnaturalordistorted, compromisingthedog’sintrinsiccharacteristics
likeposture,contour,anddepthinformation. Meanwhile,it’salsocrucialtopreservetheseintrinsic
characteristicsduringmodificationforinstanceswithcorrectlabels. Atthedatasetlevel,instance
modificationcaninducecovariateshiftsbetweentrainingandtestsets[Hanetal.,2022a,b]. These
covariateshiftscanhinderamodel’sabilitytogeneralizeeffectivelytounseenreal-worlddata.
Totacklethesechallenges,weproposeasimpleyeteffectiveframeworkEchoAlign(§4). EchoAlign
hastwokeycomponents: EchoModandEchoSelect. EchoModmodifiesinstancesusingcontrollable
generativemodels toalign themwith noisylabels whilepreserving theirintrinsic characteristics.
By controllingthe image generationprocess using theoriginal image andnoisy label asguiding
information,EchoModensurestheoverallqualityandconsistencyofthetrainingdata. EchoSelect
addressestheissueofcovariateshiftbystrategicallyselectingoriginalinstanceswithcorrectlabelsto
balancethedistributionbetweenoriginalandmodifiedinstancesinthetrainingdata. Thisselection
leverages a novel insight: after instance modification, the Cosine feature similarity distribution
betweenoriginalandmodifiedimagesshowsacleardistinctionbetweencleanandnoisysamples.
EchoSelectemploysthisinsightthroughauniquesimilaritymetrictoselectthemostreliabletraining
instancestoreconstructthelearnablesetforfurthersupervisedorself-supervisedtraining.
Ourcontributionsandthekeyfindingsaresummarizedasfollows: (1)Weintroduceatransformative
changeinhownoisylabelsareapproached. Insteadoffocusingoncorrectingthem,weadvocate
treatingnoisylabelsasaccurateandmodifyinginstancestoalignwiththem. Thisparadigmshift
issupportedbyourtheoreticalanalysis(§3). (2)WepresentEchoAlign,aframeworkspecifically
designedtoaddressthechallengesofinstancemodification. ItemploysEchoModforcontrolled
instancemodificationandEchoSelectforstrategicsampleselection(§4). (3)Weempiricallyvalidate
the advantages of instance modification and demonstrate EchoAlign’s exceptional performance
2in noisy environments. Across three datasets with diverse noise types, EchoAlign consistently
outperformsstate-of-the-arttechniques,achievingsignificantaccuracygains(§5).
2 RelatedWork
LearningwithNoisyLabels Researchinthisdomainhaspredominantlyfollowedtwopaths: (1)
Noise-modeling-freemethods: Thesemethodsprimarilyrelyonthememorizationeffectsobserved
indeepneuralnetworks(DNNs). TheycapitalizeonthetendencyofDNNstoprioritizelearning
simpler (clean) examples before memorizing more complex (noisy) ones [Arpit et al., 2017, Wu
etal.,2020,Kimetal.,2021]. Techniquesincludeearlystopping[Hanetal.,2018,Nguyenetal.,
2020,Liuetal.,2020,Xiaetal.,2021,Luetal.,2022,Baietal.,2021],pseudo-labeling[Tanaka
etal.,2018], andleveragingGaussianMixtureModelsinasemi-supervisedlearningcontext[Li
etal.,2020]. (2)Noise-modelingmethods: Thiscategoryofapproachesfocusesonestimatinga
noisetransitionmatrix,whichmodelshowcleanlabelsmightbecorruptedintonoisyobservations.
However,accuratelymodelingthenoiseprocessischallengingwhenrelyingsolelyonnoisydata
[Xiaetal.,2019,Chengetal.,2020]. Existingstudiesrelyonassumptionsthatmightnotholdin
real-worlddatasets[Xiaetal.,2020,Yaoetal.,2023b,Liuetal.,2023]. Consequently,thesemethods
oftenstruggletofullyeliminatelabelnoise,especiallywhendealingwithstructureddependentnoise
patterns,suchassubclass-dominantlabelnoise[Baietal.,2023].
GenerativeModels Recentadvancesingenerativemodels,includingvariationalauto-encoders,
generative adversarial networks, and diffusion models, have transformed applications with their
exceptional sample generation capabilities [Du et al., 2023, Wang et al., 2023, Franceschi et al.,
2023]. Diffusionmodels,particularlyknownfortheirsuperioroutputcontrol,delicatelydenoise
signals[Zhangetal.,2023,Kingmaetal.,2021]. Whilethesemodelsholdpromisefornoisylabel
scenarios,existingapproacheslikeDynamics-EnhancedGenerativeModels(DyGen)[Zhuangetal.,
2023]andLabel-Retrieval-AugmentedDiffusionModels[Chenetal.,2023b]stillfocusonenhancing
predictionsorretrievinglatentcleanlabels. Ourworkintroducesafundamentallydifferentapproach.
Weadoptcontrollablegenerativemodels,treatingnoisylabelsascorrectandaligninginstancesto
them. This bypasses the challenges of traditional noise modeling and focuses on improving the
training data itself. Controllable generative models are a powerful subset of generative models
thatallowforprecisecontroloverthegeneratedoutputs,suchasControlNet[Zhangetal.,2023],
iPromptDiff[Chenetal.,2023c]. Unliketraditionalgenerativemodels,whichgenerateimagesbased
onrandomnoise,controllablegenerativemodelstakecontrolinformation(e.g.,textdescriptions,
classlabels,orreferenceimages)asinput[Boseetal.,2022]. Thiscontrolinformationguidesthe
generationprocess,ensuringthatthegeneratedoutputsalignwiththedesiredcharacteristics.
3 Analysis
ProblemDefinition Inaddressingthechallengesposedbylearningfromnoisylabels(LNL),we
formallydefinetheproblemandintroducetheconceptofinstancemodificationwithinamathematical
framework. LetX representtheinputspaceofinstancesandY thespaceoflabels. Inthetraditional
LNL setting, each instance X ∈ X is associated with a noisy label Y˜ ∈ Y, which may differ
from the true label Y ∈ Y. The goal is to learn a mapping f : X → Y that predicts the true
label Y as accurately as possible, despite the presence of noisy labels. Instance modification
diverges from the conventional approach of directly correcting noisy labels Y˜ to match the true
labelsY. Instead,itproposesadjustingeachinstanceX tobetteralignwithitsgivennoisylabelY˜.
Mathematically,thisinvolvestransformingeachinstanceX intoa
modifiedinstanceX′,suchthatf(X′)morecloselyapproximates
Y˜,leveragingtheinherentinformationcontainedwithinthenoisy
labelitself.
Theoretical Analysis According to the causal learning frame-
work [Liu et al., 2023, Yao et al., 2021], the noise can often be
Figure 2: A graphical causal
representedasafunctionofboththeinstancefeaturesandexternal
model, revealing a data gen-
factors,encapsulatedbylatentvariablesZ. Weassumethecausal
erativeprocesswithinstance-
relations(commonlyoccurringincrowdsourcingscenarios)arerep-
dependentlabelnoise.
resented in the causal graph as illustrated in Figure 2, where Z
3representslatentvariablesthataffectbothX andY˜ indirectlythroughX. Instancemodification
aims to transform X into X′ such that the modified instance X′ aligns better with Y˜ under the
assumptionthatY˜ containssomeinformationaboutthetruelabelY. Accordingly,wecandeduce
theeffectivenessofinstancemodificationasfollows.
Theorem3.1(EffectivenessofInstanceModification). Assumethatthenoisylabelsaregeneratedby
astochasticprocessinfluencedbylatentvariablesZ,whereY˜ =h(Y,Z)andY arethetruelabels.
LetT beatransformationsuchthatX′ =T(X,Y˜;θ),whereθischosentooptimizethealignment
ofX′ withY˜. Then,underthistransformation,thepredictiveperformanceofamodeltrainedon
(X′,Y˜)istheoreticallyimprovedcomparedtoamodeltrainedon(X,Y˜)intermsof:
1. Alignment: ThemutualinformationbetweenX′andY˜,I(X′;Y˜),ismaximizedrelativeto
I(X;Y˜),indicatingbetteralignmentofmodifiedinstanceswiththeirnoisylabels.
2. ErrorReduction: ComparedtoamodeltrainedontheoriginalinstancesX,theexpected
prediction error E [(Y −f(X′))2] is minimized, where f is the prediction function
X′,Y
trainedusingthemodifiedinstancesX′. ThisassumesthatthedistributionofX′doesnot
deviatesignificantlyfromthedistributionofX,ensuringthatthelearnedmodelgeneralizes
welltotheoriginaldistribution.
3. EstimationStability: Thevarianceoftheestimatorf isreducedwhenusingX′compared
toX,resultinginmorestablepredictions.
4. Generalization: Modifications in X′ lead to better generalization. By transforming the
originalinstancestobetteralignwiththeirnoisylabels,themodeltrainedonX′islessto
overfittothenoiseandmorecapableofcapturingthetrueunderlyingpatternsinthedata.
Thisimprovementiscontingentupontheassumptionthatthenoisemodelhandthetransformation
T areappropriatelydefinedandthatthelatentvariablemodeladequatelycapturestheunderlying
causalstructureofthedata. MoredetailsandproofscanbefoundintheAppendixA.
Thistheoreticalframework3.1suggeststhatinstancemodification,byaligningmorecloselywith
noisybutinformativelabelsY˜,canleveragetheinherentstructureandcausalityinthedatatoenhance
learning. Itdemonstratesthatinstancemodificationimprovesalignmentbetweeninstancesandnoisy
labels,reducesinformationloss,andultimatelyleadstobettergeneralization. Theseinsightsprovide
severalkeymotivationsforthedesignofourmethod.Firstly,theimprovementsinalignmenthighlight
theimportanceofmodifyinginstancestoembednoisylabelinformationdirectly. Thismotivatesthe
useofcontrollablegenerativemodelsinEchoAlign,whichcaneffectivelyincorporatelabelinforma-
tionintotheinstancefeatures. Secondly,ensuringaminimaldistributiondifferencebetweenX and
X′iscrucial. EchoModgeneratesX′withsmalldistributiondifferencesfromX,whileEchoSelect
retainscleansamplestocontroldistributiondifferences,ensuringbettergeneralizationontestdata.
Thirdly,theimprovementinestimationstability
indicatesthatusingmodifiedfeaturescanresult
in more consistent and reliable model predic-
tions,drivingthefocusonpreservingtheessen-
tialcharacteristicsofthedataduringtransforma-
tiontoreducevariabilityandenhancestability.
Analyzing Feature Similarity Distributions
In this study, we address the challenges of in-
stancemodificationwhichcaninducedistribu-
tion shifts between the training and test sets.
Preservingcleanoriginalinstancesiscrucialto
mitigatingtheseshifts. Existingsampleselec-
tionmethods(e.g.,smallloss[Hanetal.,2018])
often falter under complex label noise condi-
tions,suchasinstance-dependentnoise,necessi- Figure3: Thefeaturesimilaritybetweentheorigi-
tatingamorepreciseselectionstrategy. Tothis nalandmodifiedinstancesisavaluablemetricfor
end,wefindaninterestingphenomenon: Clean sampleselectionafterinstancemodification.
samplesgenerallyexhibithighersimilaritybe-
tweenfeaturesoforiginalandmodifiedimages,indicatingminimalsemanticandlabelchangesafter
4modification,whereasnoisysamplesdisplaylowersimilarityduetosignificantsemanticandlabel
adjustments. Utilizingthefeaturesimilaritydistributionsbetweenoriginalandmodifiedinstances
emergesasarobusttoolforenhancingsampleselectionaccuracy. Thesedistinctionsarevisuallyrep-
resentedinFigure3. ThesimilarityiscomputedusingtheCLIPViT-B-32featureextractor[Radford
etal.,2021]ontheCIFAR-10datasetwith30%instance-dependentnoise. WeuseControlNet[Zhang
etal.,2023]tomodifyinstances. Theblackdashedlineindicatesthesamplethresholdachievable
bythepreviousbestmethodat96%accuracy[Yangetal.,2022]. Incontrast,EchoSelect,at96%
accuracy,canretainthesamplesintheyellowsection. Inenvironmentswith30%instance-dependent
noise,EchoSelectretainsnearlytwicethenumberofsamplesat99%accuracy. Statisticalvalidation
usingtheKolmogorov-Smirnovtestconfirmedsignificantdifferencesinthedistributions(p-value<
0.001),demonstratingtheutilityoffeaturesimilarityasarobustmetricforidentifyingcleansamples
withinnoisydatasets.
4 EchoAlign
TheEchoAlignframeworktacklesthechallengeofnoisylabelsinsupervisedlearning. Itcomprises
two primary components: (1) EchoMod modifies data instances to align them better with their
potentiallyincorrectlabels. (2)EchoSelectselectsthemostreliablemodifiedinstances,creatinga
cleaner,moreconsistenttrainingdataset,whichisusedtotrainageneralneuralnetwork.
4.1 EchoMod: InstanceModification
Motivation Whenlabelsarenoisy,theydonotreflectthetruecharacteristicsofthecorresponding
datainstances. Thisdiscrepancycanhinderamodel’sabilitytolearnmeaningfulpatterns. EchoMod
addressesthisbytransformingdatainstancestobemoreconsistentwiththeirnoisylabels. This
controlledmodificationhelpsthemodelextractrelevantinformationwhenthelabelsarenoisy.
Mechanism EchoModleveragesapre-trainedcontrollablegenerativemodel(e.g.,acontrollable
diffusion-based model) to modify data instances. The primary goal is to enhance the alignment
betweeninstancesandtheirpotentiallynoisylabels. Thisalignmentisachievedbycarefullyguiding
thegenerativemodel’sprocess.Firstly,thecontrollablegenerativemodelhasundergonepriortraining
onalargedataset. Thispre-traininghasequippedthemodelwithadeepunderstandingofthepatterns
andstructuresinherentinthedatadomain. Secondly,EchoModprovidesboththeoriginalinstance
(X)andthenoisylabel(Y˜)asinputstothegenerativemodel. Thisdualconditioningshapesthe
output,encouragingthemodeltoproduceamodifiedinstance(X′)thatcloselyalignswiththenoisy
labelwhilestillretainingessentialcharacteristicsoftheoriginaldata. Strikingthisbalancebetween
labelalignmentandpreventingexcessivedistortioniscrucial.
Reasonableness,Effectiveness,andFairness Handlinglabelnoiseinnoisylabellearningisa
well-recognizedchallenge. Previousworkshaveprimarilyfocusedontheutilizationandoptimization
ofinternaldata. Ourapproachintroducesanovelperspectivebyintegratingexternalknowledgeto
enhancemodelrobustness. Thisintegrationdoesnotcompromisefairness,asourflexibleframework
accommodatesvariousgenerativemodels,andcanbefine-tunedforspecificnoisylabelproblems.
Bydoingso,weensurethatthemethoddoesnotoverlyrelyonanyparticularmodel. Thisapproach
isparticularlyadvantageouswhendealingwithambiguousdata. Theinherentambiguityinthedata
canleadtolowconfidenceindirectdiscrimination. Instead,bymodifyingtheinputdatathrough
controllablegenerativemodels,wecanbetterresolvediscrepanciesbetweeninstancesandlabels
while preserving the meaningful characteristics of the original data. This not only improves the
model’sdiscriminativecapabilitybutalsoenhancesoverallperformanceandreliability.
NoFine-TuningforGeneralizability Inmostcases,EchoModcanleverageapre-trainedcon-
trollable generative model without fine-tuning during the alignment process. This preserves the
model’sabilitytounderstandgeneraldatacharacteristics,promotingEchoAlign’sapplicabilityacross
variousdomains. WhileEchoModcanbeeffectivewithoutfine-tuning,additionalperformancegains
mightberealizedbytailoringthecontrollablegenerativemodeltohighlyspecializedtasksordata
distributions. Insuchcases,fine-tuningtheparameters(θ)couldleadtobetteralignmentbetween
instancesandnoisylabels. Examplesofthisspecializationincludemedicalimagingorscientificdata.
54.2 EchoSelect: InstanceSelection
Motivation WhileEchoModimprovesthealignmentbetweeninstancesandnoisylabels,some
modifiedinstancesmaystillexhibitinconsistencies. Additionally,theinstancemodificationprocess
can introduce distribution shifts between the modified training data and the true test distribution.
EchoSelectsafeguardsagainsttheseissuesbyidentifyingandretainingonlythemostreliablemodified
instances. Thisfiltering enhancesmodelrobustness, reducesthe impactofnoisydata, andhelps
mitigatethedistributionshiftscausedbyinstancemodification.
Mechanism EchoSelect employs a metric to assess the similarity between modified instances
andareferencerepresentationofcleandata. WeusetheCosinesimilaritybetweenfeaturevectors
extractedusingasuitablefeatureextractor(e.g.,theimageencoderofCLIP[Radfordetal.,2021]):
z(X′)·z(X)
S(X′,X)= , (1)
∥z(X′)∥∥z(X)∥
whereX′andX aremodifiedandoriginalinstances,andzdenotesthefeatureextractor.
SelectionProcess EchoSelectcalculatessimilarityforallmodifiedinstances,comparingthem
totheiroriginalcounterparts. Tomitigatedistributionshifts,priorityisgiventomaintainingclean
original instances as much as possible. The final training set consists of two parts: (1) Original
instanceswithsimilaritylargerthanadeterminedthresholdaredeemedsufficientlyalignedwithclean
datacharacteristicsandretained,and(2)modifiedinstanceswithsimilaritybelowthethresholdare
included.Theseinstancesarelikelythosewherethemodificationwasmostbeneficialinaligningthem
withthenoisylabels,whilealsoindicatingsomedegreeofdifferencefromtheoriginaldistribution.
Thisthresholdτ balancestheinclusionofmodifiedinstanceswiththepreservationoftheoriginal
datadistribution. Thisthresholdensuresthatonlyinstancesalignedwiththecharacteristicsofclean
dataareretained. Oursensitivityanalysis(§5.3)verifiedtherobustnessofτ acrosstypesofnoise.
4.3 EchoAlign: OptimizedCombination
The integration of EchoMod and
Algorithm1EchoAlignFramework
EchoSelectenablesthecreationofa
refinedtrainingdatasetthatisaligned Require: Pre-trained controllable generative model f ,
θ
withnoisylabelsandfilteredforqual- Noisydataset(X,Y˜),Thresholdτ,FeatureExtractor
ity. This optimized dataset is bet- Ensure: Refinedtrainingdataset
ter suited for robust learning in the 1: Generatemodifiedinstances: X′ ←f θ(X,Y˜)
presence of large label noise. Since 2: Computesimilarity: S(X′,X)usingEquation(1)
the refined training dataset can be
3: #Constructarefineddatasetwithtwoparts
further used to train a supervised
4: Part1: OriginalInstances
or self-supervised model for LNL, 5: SelectoriginalinstanceswhereS(X′,X)≥τ
EchoAligncanbecombinedwithad-
6: Part2: ModifiedInstances
vanced LNL methods to address the 7: SelectmodifiedinstanceswhereS(X′,X)<τ
impact of label noises further. The
8: CombinePart1andPart2toformtherefineddataset
integrationofEchoModandEchoSe-
9: Returntherefineddataset
lectisencapsulatedinthefollowing
Algorithm1,whichdetailsthestepsformodifyinginstancesandselectingtheoptimalsubset.
5 Experiments
5.1 ExperimentSetup
Dataset Ourexperimentsareconductedontwosyntheticdatasets: CIFAR-10,andCIFAR-100
[Krizhevskyetal.,2009],andareal-worlddataset: CIFAR-10N[Weietal.,2022]. CIFAR-10and
CIFAR-100eachcontain50,000trainingand10,000testingimages,withasizeof32×32,covering
10and100classesrespectively. CIFAR-10NutilizesthesametrainingimagesfromCIFAR-10but
withlabelsre-annotatedbyhumans. Followingpreviousresearchprotocols[Baietal.,2021,Xia
etal.,2019,2023b],wecorruptedthesesyntheticdatasetsbythreetypesoflabelnoise. Specifically,
symmetricnoiserandomlyaltersacertainproportionoflabelstodifferentclassestosimulaterandom
labelingerrors;pairflipnoisechangeslabelstospecificadjacentclasseswithacertainprobability;
instance-dependentnoisemodifieslabelsbasedontheimagefeaturestorelatedincorrectclasses. Due
6toresourceconstraints,weusealightandrepresentativehuman-annotatednoisydatasetCIFAR-10N
insteadofClothing1M[Xiaoetal.,2015].AdetailedruntimeanalysisisdiscussedintheAppendixD,
demonstratingthattheruntimeisreasonableforalldatasetsused. ForCIFAR-10N,weprovidefour
noisylabelsets:‘Randomi=1,2,3’,eachrepresentingthelabelprovidedbyoneofthreeindependent
annotators;and‘Worst’,whichselectsthenoisiestlabelwhenincorrectannotationsarepresent.
Baseline WecompareEchoAlignagainstvariousparadigmsofbaselinesforaddressinglabelnoise.
Under the robust loss function paradigm, we include APL [Ma et al., 2020], PCE [Menon et al.,
2019],AUL[Zhouetal.,2023],andCELC[Weietal.,2023]asbaselines;underthelosscorrection
paradigm,weutilizeT-Revision[Xiaetal.,2019]andIdentifiability[Liuetal.,2023];underthelabel
correctionparadigm,weselectJoint[Tanakaetal.,2018];andunderthesampleselectionparadigm,
weemployCo-teaching[Hanetal.,2018],SIGUA[Hanetal.,2020]andCo-Dis[Xiaetal.,2023a].
Wecomparethesemethodsagainstasimplecross-entropy(CE)lossbaseline. Followingthefair
baselinedesignusedbyXiaetal.[2023b],wedonotcomparewithsomemethodssuchasMixUp
[Zhangetal.,2018],DivideMix[Lietal.,2020],andM-correction[Arazoetal.,2019],asthese
involvesemi-supervisedlearning,makingsuchcomparisonsunfairduetoinconsistentsettings.
Implementation Details All experiments were conducted on an NVIDIA V100 GPU using
PyTorch. Themodelarchitecturesandparametersettingswerekeptconsistentwithpreviousstudies
[Baietal.,2021]. Theexperimentswereconfiguredwithalearningrateof0.1,usingtheStochastic
GradientDescent(SGD)optimizerwithamomentumof0.9,andweightdecaysetto1×10−4. We
applied30%and50%symmetricnoiseaswellas45%pairflipnoiseontheCIFAR-10andCIFAR-
100datasetstoassessmodelperformance. TheCIFAR-10datasetutilizedthestandardResNet-18
[Heetal.,2016]architecture, whileCIFAR-100usedResNet-34. Similarly, fortheCIFAR-10N
dataset,thesameResNet-18modelwasused. Priortothetrainingphase,ControlNetwasutilized
as our reference model in the controllable generation model module. This choice was strategic;
ControlNetwastheleasteffectivemodelidentifiedinprioranalyses[Chenetal.,2023c]. Employing
thismodelunderscorestherobustnessofourapproach,ensuringthattheefficacyofourmethodisnot
overlycontingentuponthecapabilitiesofanyspecificgenerativemodel. Thisdecisionhighlights
ourmethod’sadaptabilityandgeneralefficacyacrossvaryingscenarios. WeemployedtheCanny
edgedetectorasasimplepreprocessortoextractfeaturesfromtheinstances,usinglabelsastextual
controlswiththeprompt"aphotoof{label}". Thiswasexecutedwithoutanyadditionalornegative
prompts,andthesamplingprocesswaslimitedto20steps. Allexperimentswererepeatedthreetimes
underdifferentrandomseeds,andresultsarereportedasaverageswithstandarddeviations.
5.2 MainResults
WevalidatedtheeffectivenessofourmethodontheCIFAR-10andCIFAR-100syntheticdatasets.
Foreachdataset,weutilized90%ofthesampleswithnoisylabelsfortrainingandreserved10%
as validation samples, with testing conducted on clean samples. Part of the baseline results was
takenfrompreviouswork[Xiaetal.,2023b]. AsshowninTable1,ourresultsexhibitedstate-of-
the-artperformanceacrossnearlyallconditionsonbothdatasets. Notably,undermorechallenging
noiseconditions,50%instance-dependentnoiseonCIFAR-10and45%symmetricnoiseonCIFAR-
100,ourmethodsignificantlyoutperformedthecurrentstate-of-the-artbaselines,whichismainly
attributedtotherobustnessofourmethodtothenoiserate. Importantly,ourmethoddemonstrates
enhancedperformanceunderhighernoiserates,duetoEchoMod’snoise-independence.Thisattribute
allowsthemodeltolearnconsistentinstancefeaturesacrossdifferenttypesofnoiseandnoiserates.
Theprimarycauseofperformancevariationacrossdifferentsettingsistheinherentdifferenceinthe
numberofcleansampleswithinthedatasets. Ourapproachalsoexhibitedremarkableperformance
onthereal-worlddataset,CIFAR-10N,outperformingthestate-of-the-artexistingmethodsinall
realnoiseconditions. Furthermore,ourproposedmethoddemonstratedexceptionalrobustness,with
standarddeviationsinperformanceacrossallconditionswithinfloating-pointprecision.
5.3 In-DepthAnalyses
Ablation Analysis To assess the effectiveness of Instance Modification and EchoSelect, we
conductedablationstudiesbysystematicallydisablingthesecomponents. Specifically,weevaluated
twoconfigurations: InstanceModificationOnlyandEchoSelectOnly,andcomparedbothagainstthe
standardCross-EntropyLoss(CE)asabaseline. Theseexperimentswerecarriedoutunderseveral
settingswithhighnoiserates,whichpresentedsignificantchallengesforthemodel. Theexperimental
7Table1: Comparisonoftestaccuracy(%)withstate-of-the-artmethodsonsyntheticdatasetsCIFAR-
10andCIFAR-100. Thebestthreeresultsareboldedandthebestoneisunderlined.
Symmetric Pairflip Instance
Datasets Methods 30% 50% 45% 30% 50%
CE 73.17±1.13 52.59±0.70 51.49±0.42 71.56±0.19 49.20±0.42
APL 85.54±0.51 78.36±0.47 80.84±0.72 77.57±0.15 39.45±6.51
PCE 86.12±0.85 74.03±4.96 65.08±3.41 85.64±0.72 64.82±4.13
AUL 88.09±0.78 82.81±1.16 56.80±2.69 86.35±0.90 60.75±3.77
CELC 82.51±0.22 85.08±3.95 85.72±4.52 86.67±1.47 61.85±4.98
T-Revision 88.39±0.38 83.40±0.65 83.61±1.06 89.07±0.35 66.93±4.14
CIFAR-10 Identifiability 87.12±1.69 83.43±2.11 83.65±2.46 80.47±1.54 55.25±3.78
Joint 89.34±0.52 85.06±0.29 80.52±1.90 88.41±1.02 64.12±3.89
Co-teaching 88.93±0.56 74.02±0.04 84.19±0.68 87.07±0.35 60.09±3.31
SIGUA 83.19±1.26 77.92±3.11 70.39±1.94 82.90±2.00 30.95±9.70
Co-Dis 89.20±0.13 85.36±0.94 85.02±1.33 87.13±0.25 62.77±3.90
Ours 90.98±0.20 87.95±0.12 87.42±0.11 89.18±0.20 77.81±0.30
CE 50.99±1.29 34.5±0.96 37.03±0.41 50.33±2.14 34.70±1.45
APL 55.78±0.91 46.96±0.81 49.55±1.05 43.30±1.57 29.01±0.09
PCE 58.84±1.32 42.63±2.02 41.05±2.83 55.72±1.96 38.72±3.01
AUL 69.89±0.21 60.00±0.40 39.37±1.61 67.75±1.84 40.27±1.76
CELC 67.96±1.88 60.71±2.39 52.53±3.17 66.25±1.93 47.52±3.93
T-Revision 62.97±0.46 43.60±0.94 49.33±1.10 56.46±1.45 40.78±1.75
CIFAR-100 Identifiability 50.53±1.52 34.87±2.36 38.16±2.68 52.48±1.93 36.72±3.10
Joint 63.69±0.84 55.62±1.68 49.77±1.15 64.15±1.11 45.47±2.73
Co-teaching 59.49±0.36 52.19±1.42 47.53±1.39 56.71±1.26 42.09±1.73
SIGUA 54.22±0.90 50.64±3.92 39.92±2.33 53.19±2.64 38.50±1.69
Co-Dis 64.02±1.37 54.55±2.06 50.02±2.80 59.15±1.92 43.38±1.25
Ours 68.16±0.53 60.78±0.46 60.31±0.37 65.68±0.48 57.21±0.60
Table 2: Comparison of test accuracy (%) with state-of-the-art methods on real-world datasets
CIFAR-10N.Thebestthreeresultsareboldedandthebestoneisunderlined.
Datasets Methods Random1 Random2 Random3 Worst
CE 83.17±0.48 82.74±0.42 82.90±0.28 76.57±0.23
APL 84.40±0.26 84.45±0.50 84.35±0.43 78.16±0.17
PCE 63.06±0.37 62.26±0.36 35.47±0.36 33.80±0.33
AUL 76.26±0.28 75.24±0.20 75.48±0.40 63.61±1.62
CELC 83.11±0.14 83.09±0.22 82.60±0.04 73.49±0.50
T-Revision 80.99±0.26 78.99±1.59 78.80±1.87 78.37±0.96
CIFAR-10N Identifiability 82.52±0.87 81.97±0.85 82.09±0.73 71.62±1.16
Joint 88.20±0.29 87.54±0.33 87.67±0.22 84.29±0.40
Co-teaching 82.28±0.13 82.45±0.23 82.09±0.24 79.62±0.25
SIGUA 87.67±1.18 89.01±0.34 88.40±0.42 80.65±1.29
Co-Dis 80.81±0.23 80.36±0.20 80.76±0.13 78.12±0.25
Ours 89.42±0.12 89.31±0.06 89.80±0.25 84.35±0.09
Table3: Comparisonwithstate-of-the-artmethodsonCIFAR-10andCIFAR-100inaccuracy(%).
CIFAR-10Pairflip-45% CIFAR-10IDN-50% CIFAR-100Pairflip-45% CIFAR-100IDN-50%
CE 51.49 49.20 37.03 34.70
InstanceModificationOnly 42.77 44.98 15.69 16.36
EchoSelectOnly 79.46 65.77 44.24 41.24
Ours 87.42 77.81 60.31 57.21
8resultsinTable3revealedthatintheconfigurationusingonlyInstanceModification,themodel’s
accuracynotonlyfailedtoexceedthebaselineCEbutalsoshowedpoorerperformance. Thisdecline
primarilystemsfromthedatadistributionshiftcausedbysolelyusingmodifiedinstances,which
adverselyaffectsthemodel’sgeneralizationcapabilityandperformance. Incontrast,whileusing
onlytheEchoSelectcomponentdidimproveperformance,itstillfellshortcomparedtoEchoAlign.
ThisindicatesthatalthoughEchoSelectsignificantlyreducestheimpactofnoise,itseffectivenessis
limitedbytheavailablenumberofsamples.
Table4: ComparisonofsampleselectionqualityunderCIFAR-10instance-dependentnoise.
BLTM Ours
Noiserate
select.acc. #ofselectedexamples #ofselectedexamples
96% 17673/50000 26524/50000
IDN-30%
99% 10673/50000 19010/50000
94% 8029/50000 11660/50000
IDN-50%
98% 5098/50000 6090/50000
(a) (b)
Figure4: (a)Comparisonoftheeffectofthethreshold(τ)onaccuracyatdifferentsettingsof30%
noiserate. (b)Evaluationofthresholdingeffectsonthequalityandquantityofsampleselection
under30%instance-dependentnoiseonCIFAR-10.
SensitivityAnalysis TheperformanceofourproposedEchoSelectmethodisinfluencedbythe
choiceofthethresholdvalueτ,whichdecisivelyaffectsthenumberandqualityofsamplesselected
fromnoisydatasets. AccordingtotheassumptionsofEchoSelect, theoptimalthresholdvalueis
theoreticallysetaround0.5.Tovalidatetheefficacyofourmethod,weusedthestate-of-the-artBLTM
[Yangetal.,2022]approachasabaseline,withresultsdirectlycitedfromitsoriginalpublication. As
showninTable4,ourEchoSelectwasabletoselectsignificantlymoresamplesthanBLTMunder
the same accuracy conditions. Particularly, in environments with 30% instance-dependent noise,
whentheaccuracyreached99%,thenumberofsamplesselectedbyEchoSelectwasalmosttwice
thatofBLTM.Figure4aclearlydemonstratesthatthethresholdτ exhibitsrobustnessandstability
acrossdifferentclassesandtypesofnoise,predominantlyinfluencedbythenoiserate. Theslight
performancedisparitybetweenCIFAR-10andCIFAR-100asdepictedinthefigureisattributedto
CIFAR-100containing20superclasses, withhighsimilarityamongsubclassesthatincreasesthe
complexityofclassification. Furthermore,Figure4bdetailshowadjustmentstothethresholdvalueτ
affectthequantityandprecisionofsampleselection. Thesmoothtransitionsdisplayed,alongwitha
clearlydefinedoptimalequilibriumregion,furtheraffirmtheefficacyofourmethodinvariousnoise
environments. Toconclude,thethresholdτ isnotsensitiveandrobusttoset.
6 ConclusionandDiscussion
Conclusion Thisworkprovidedapositiveanswertowhetherwecaneffectivelytreatlabelnoiseas
accuratethroughinstancemodification. Theoreticalanalysissupportstheinstancemodificationthat
thisalignmentprocessallowsmodelstolearnmeaningfulpatternsdespitethepresenceoflabeling
9errors. Toovercomethechallengesofinstancemodification,weproposedtheEchoAlignframework.
Itintegratesacontrollablegenerativemodelwithstrategicsampleselectiontocreatearobusttraining
dataset. ExtensiveexperimentsondiversedatasetsdemonstratethesuperiorityofEchoAlignover
existing methods, particularly in scenarios with high levels of label noise. Future directions to
enhance EchoAlign’s impact include: investigating supervised or self-supervised extensions and
exploringitseffectivenessinbroaderapplicationslikemedicalimagingorreal-timesystems.
Potential Limitations and Social Impact While EchoAlign shows promising results, its suc-
cess partially depends on the capabilities of the controllable generative model. Additionally, the
computationalcostassociatedwithprocessinglargedatasetsthroughcontrollablegenerativemodels
couldbeabarrierinresource-limitedsettings. EchoAlignhasthepotentialtoenhancetherobustness
and fairness of AI systems in domains where noisy labels are prevalent, such as with real-world
datarangingfrommedicaldiagnosistosocialmediacontentanalysis. Byaddressinglabelnoise,
EchoAligncouldcontributetofairerAIsystems. However,it’scrucialtorecognizethatEchoAlignis
atool,andlikeanytool,itsimpactdependsonhowitisused. Carefulconsiderationisneededinthe
deploymentofAIsystemstoaddressethicalconcernsandensurepositivesocietaloutcomes.
10References
J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,D.Almeida,J.Altenschmidt,
S.Altman,S.Anadkat,etal. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
E.Arazo,D.Ortego,P.Albert,N.O’Connor,andK.McGuinness.Unsupervisedlabelnoisemodeling
andlosscorrection. InInternationalconferenceonmachinelearning,pages312–321.PMLR,
2019.
D.Arpit,S.Jastrzebski,N.Ballas,D.Krueger,E.Bengio,M.S.Kanwal,T.Maharaj,A.Fischer,
A.C.Courville,Y.Bengio,andS.Lacoste-Julien. Acloserlookatmemorizationindeepnetworks.
InICML,pages233–242,2017.
Y.Bai,E.Yang,B.Han,Y.Yang,J.Li,Y.Mao,G.Niu,andT.Liu. Understandingandimproving
earlystoppingforlearningwithnoisylabels. InNeurIPS,pages24392–24403,2021.
Y. Bai, Z. Han, E. Yang, J. Yu, B. Han, D. Wang, and T. Liu. Subclass-dominant label noise:
A counterexample for the success of early stopping. In Thirty-seventh Conference on Neural
InformationProcessingSystems,2023.
A.Berthon,B.Han,G.Niu,T.Liu,andM.Sugiyama. Confidencescoresmakeinstance-dependent
label-noiselearningpossible. InICML,ProceedingsofMachineLearningResearch,pages825–
836,2021.
J.Bose,R.P.Monti,andA.Grover. Controllablegenerativemodelingviacausalreasoning. Transac-
tionsonMachineLearningResearch,2022.
H.Chen,J.Wang,A.Shah,R.Tao,H.Wei,X.Xie,M.Sugiyama,andB.Raj. Understandingand
mitigatingthelabelnoiseinpre-trainingondownstreamtasks. arXivpreprintarXiv:2309.17002,
2023a.
H.Chen,B.Raj,X.Xie,andJ.Wang. Oncatastrophicinheritanceoflargefoundationmodels. arXiv
preprintarXiv:2402.01909,2024.
J.Chen,R.Zhang,T.Yu,R.Sharma,Z.Xu,T.Sun,andC.Chen.Label-retrieval-augmenteddiffusion
modelsforlearningfromnoisylabels. ArXiv,abs/2305.19518,2023b. doi: 10.48550/arXiv.2305.
19518.
T.Chen,Y.Liu,Z.Wang,J.Yuan,Q.You,H.Yang,andM.Zhou. Improvingin-contextlearning
indiffusionmodelswithvisualcontext-modulatedprompts. arXivpreprintarXiv:2312.01408,
2023c.
J. Cheng, T. Liu, K. Ramamohanarao, and D. Tao. Learning with bounded instance and label-
dependentlabelnoise. InInternationalconferenceonmachinelearning,pages1789–1799.PMLR,
2020.
A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M.Minderer,G.Heigold,S.Gelly,J.Uszkoreit,andN.Houlsby. Animageisworth16x16words:
Transformersforimagerecognitionatscale. InICLR,2021.
H.Du,H.Yuan,Z.Huang,P.Zhao,andX.Zhou. Sequentialrecommendationwithdiffusionmodels.
ArXiv,abs/2304.04541,2023. doi: 10.48550/arXiv.2304.04541.
J.-Y.Franceschi,M.Gartrell,L.D.Santos,T.Issenhuth,E.deB’ezenac,M.Chen,andA.Rako-
tomamonjy. Unifying gans and score-based diffusion as generative particle models. ArXiv,
abs/2305.16150,2023. doi: 10.48550/arXiv.2305.16150.
J.GoldbergerandE.Ben-Reuven. Trainingdeepneural-networksusinganoiseadaptationlayer. In
Internationalconferenceonlearningrepresentations,2016.
K. Gu, X. Masotto, V. Bachani, B. Lakshminarayanan, J. Nikodem, and D. Yin. An instance-
dependentsimulationframeworkforlearningwithlabelnoise. MachineLearning,112(6):1871–
1896,2023.
11B.Han,Q.Yao,X.Yu,G.Niu,M.Xu,W.Hu,I.Tsang,andM.Sugiyama. Co-teaching: Robust
trainingofdeepneuralnetworkswithextremelynoisylabels. InNeurIPS,pages8527–8537,2018.
B.Han,G.Niu,X.Yu,Q.Yao,M.Xu,I.Tsang,andM.Sugiyama. SIGUA:Forgettingmaymake
learningwithnoisylabelsmorerobust. InInternationalConferenceonMachineLearning,pages
4006–4016,2020.
Z.Han,X.-J.Gui,H.Sun,Y.Yin,andS.Li. Towardsaccurateandrobustdomainadaptationunder
multiplenoisyenvironments. IEEETransactionsonPatternAnalysisandMachineIntelligence,45
(5):6460–6479,2022a.
Z.Han,H.Sun,andY.Yin. Learningtransferableparametersforunsuperviseddomainadaptation.
IEEETransactionsonImageProcessing,31:6424–6439,2022b.
K.He,X.Zhang,S.Ren,andJ.Sun. Deepresiduallearningforimagerecognition. InCVPR,pages
770–778,2016.
T.Kim,J.Ko,S.Cho,J.Choi,andS.Yun. FINEsamplesforlearningwithnoisylabels. InNeurIPS,
pages24137–24149,2021.
D.Kingma, T.Salimans, B.Poole, andJ.Ho. Variationaldiffusionmodels. Advancesinneural
informationprocessingsystems,34:21696–21707,2021.
A.Krizhevsky,G.Hinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. Technical
report,2009.
J.Li,R.Socher,andS.C.H.Hoi. Dividemix:Learningwithnoisylabelsassemi-supervisedlearning.
InICLR,2020.
W.Li,L.Wang,W.Li,E.Agustsson,andL.VanGool. Webvisiondatabase: Visuallearningand
understandingfromwebdata. arXivpreprintarXiv:1708.02862,2017.
S.Liu,J.Niles-Weed,N.Razavian,andC.Fernandez-Granda. Early-learningregularizationprevents
memorizationofnoisylabels. InNeurIPS,pages20331–20342,2020.
T.LiuandD.Tao. Classificationwithnoisylabelsbyimportancereweighting. IEEETransactionson
patternanalysisandmachineintelligence,38(3):447–461,2015.
Y.Liu,H.Cheng,andK.Zhang. Identifiabilityoflabelnoisetransitionmatrix. InInternational
ConferenceonMachineLearning,pages21475–21496.PMLR,2023.
Y.Lu,Y.Bo,andW.He. Noiseattentionlearning: Enhancingnoiserobustnessbygradientscaling.
InNeurIPS,2022.
X.Ma,H.Huang,Y.Wang,S.Romano,S.Erfani,andJ.Bailey. Normalizedlossfunctionsfordeep
learningwithnoisylabels. InICML,2020.
A.K.Menon,B.VanRooyen,andN.Natarajan.Learningfrombinarylabelswithinstance-dependent
noise. MachineLearning,107:1561–1595,2018.
A.K.Menon,A.S.Rawat,S.J.Reddi,andS.Kumar. Cangradientclippingmitigatelabelnoise? In
InternationalConferenceonLearningRepresentations,2019.
N.Natarajan,I.S.Dhillon,P.K.Ravikumar,andA.Tewari. Learningwithnoisylabels. Advancesin
neuralinformationprocessingsystems,26,2013.
L.G.Neuberg. Causality: models,reasoning,andinference,byjudeapearl,cambridgeuniversity
press,2000. EconometricTheory,19(4):675–685,2003.
D.T.Nguyen,C.K.Mummadi,T.Ngo,T.H.P.Nguyen,L.Beggel,andT.Brox. SELF:learningto
filternoisylabelswithself-ensembling. InICLR,2020.
J.Peters,D.Janzing,andB.Schölkopf. Elementsofcausalinference: foundationsandlearning
algorithms. TheMITPress,2017.
12A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,
J. Clark, et al. Learning transferable visual models from natural language supervision. In
Internationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.
S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich. Training deep neural
networksonnoisylabelswithbootstrapping. arXivpreprintarXiv:1412.6596,2014.
C.Scott. Arateofconvergenceformixtureproportionestimation,withapplicationtolearningfrom
noisylabels. InArtificialIntelligenceandStatistics,pages838–846.PMLR,2015.
C.Scott,G.Blanchard,andG.Handy. Classificationwithasymmetriclabelnoise: Consistencyand
maximaldenoising. InConferenceonlearningtheory,pages489–511.PMLR,2013.
N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F.
Christiano. Learningtosummarizewithhumanfeedback. InNeurIPS,pages3008–3021,2020.
M.TanandQ.V.Le. Efficientnet: Rethinkingmodelscalingforconvolutionalneuralnetworks. In
ICML,pages6105–6114,2019.
D.Tanaka,D.Ikami,T.Yamasaki,andK.Aizawa. Jointoptimizationframeworkforlearningwith
noisylabels. InCVPR,pages5552–5560,2018.
G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.
Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805,2023.
X.Wang,S.Wang,J.Wang,H.Shi,andT.Mei. Co-mining: Deepfacerecognitionwithnoisylabels.
InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages9358–9367,
2019.
Y.Wang,Z.Liu,L.Yang,andP.S.Yu. Conditionaldenoisingdiffusionforsequentialrecommenda-
tion. ArXiv,abs/2304.11433,2023. doi: 10.48550/arXiv.2304.11433.
H.Wei,H.Zhuang,R.Xie,L.Feng,G.Niu,B.An,andY.Li. Mitigatingmemorizationofnoisy
labelsbyclippingthemodelprediction. InInternationalConferenceonMachineLearning.PMLR,
2023.
J.Wei,Z.Zhu,H.Cheng,T.Liu,G.Niu,andY.Liu. Learningwithnoisylabelsrevisited: Astudy
usingreal-worldhumanannotations. InInternationalConferenceonLearningRepresentations,
2022.
P.Welinder,S.Branson,S.J.Belongie,andP.Perona. Themultidimensionalwisdomofcrowds. In
NeurIPS,pages2424–2432,2010.
P.Wu,S.Zheng,M.Goswami,D.N.Metaxas,andC.Chen. Atopologicalfilterforlearningwith
labelnoise. InNeurIPS,pages21382–21393,2020.
X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, and M. Sugiyama. Are anchor points really
indispensableinlabel-noiselearning? InNeurIPS,pages6835–6846,2019.
X.Xia,T.Liu,B.Han,N.Wang,M.Gong,H.Liu,G.Niu,D.Tao,andM.Sugiyama. Part-dependent
labelnoise: Towardsinstance-dependentlabelnoise. AdvancesinNeuralInformationProcessing
Systems,33:7597–7610,2020.
X.Xia,T.Liu,B.Han,C.Gong,N.Wang,Z.Ge,andY.Chang. Robustearly-learning: Hindering
thememorizationofnoisylabels. InICLR,2021.
X.Xia,B.Han,Y.Zhan,J.Yu,M.Gong,C.Gong,andT.Liu. Combatingnoisylabelswithsample
selectionbymininghigh-discrepancyexamples. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages1833–1843,2023a.
X.Xia,P.Lu,C.Gong,B.Han,J.Yu,andT.Liu. Regularlytruncatedm-estimatorsforlearningwith
noisylabels. IEEETransactionsonPatternAnalysisandMachineIntelligence,2023b.
13T.Xiao,T.Xia,Y.Yang,C.Huang,andX.Wang. Learningfrommassivenoisylabeleddatafor
imageclassification. InCVPR,pages2691–2699,2015.
S. Yang, E. Yang, B. Han, Y. Liu, M. Xu, G. Niu, and T. Liu. Estimating instance-dependent
bayes-labeltransitionmatrixusingadeepneuralnetwork. InICML,pages25302–25312,2022.
Y.Yao,T.Liu,M.Gong,B.Han,G.Niu,andK.Zhang.Instance-dependentlabel-noiselearningunder
astructuralcausalmodel. AdvancesinNeuralInformationProcessingSystems,34:4409–4420,
2021.
Y.Yao, M.Gong, Y.Du, J.Yu, B.Han, K.Zhang, andT.Liu. Whichisbetterforlearningwith
noisylabels: thesemi-supervisedmethodormodelinglabelnoise? InInternationalConferenceon
MachineLearning,pages39660–39673.PMLR,2023a.
Y.Yao,T.Liu,M.Gong,B.Han,G.Niu,andK.Zhang. Causalityencouragestheidentifiability
ofinstance-dependentlabelnoise. InMachineLearningforCausalInference,pages247–264.
Springer,2023b.
X.Yu,T.Liu,M.Gong,andD.Tao. Learningwithbiasedcomplementarylabels. InECCV,pages
69–85,2018.
X.Yu,B.Han,J.Yao,G.Niu,I.Tsang,andM.Sugiyama.Howdoesdisagreementhelpgeneralization
againstlabelcorruption? InInternationalConferenceonMachineLearning,pages7164–7173.
PMLR,2019.
H.Zhang,M.Cissé,Y.N.Dauphin,andD.Lopez-Paz. mixup: Beyondempiricalriskminimization.
InICLR,2018.
J.Zhang,V.S.Sheng,T.Li,andX.Wu. Improvingcrowdsourcedlabelqualityusingnoisecorrection.
IEEEtransactionsonneuralnetworksandlearningsystems,29(5):1675–1688,2017.
L.Zhang,A.Rao,andM.Agrawala. Addingconditionalcontroltotext-to-imagediffusionmodels.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages3836–3847,
2023.
S.Zhao,D.Chen,Y.-C.Chen,J.Bao,S.Hao,L.Yuan,andK.-Y.K.Wong.Uni-controlnet:All-in-one
controltotext-to-imagediffusionmodels. AdvancesinNeuralInformationProcessingSystems,
2023.
X.Zhou,X.Liu,D.Zhai,J.Jiang,andX.Ji. Asymmetriclossfunctionsfornoise-tolerantlearning:
Theoryandapplications. IEEETransactionsonPatternAnalysisandMachineIntelligence,2023.
Y. Zhuang, Y. Yu, L. Kong, X. Chen, and C. Zhang. Dygen: Learning from noisy labels via
dynamics-enhancedgenerativemodeling. InProceedingsofthe29thACMSIGKDDConference
onKnowledgeDiscoveryandDataMining,2023. doi: 10.1145/3580305.3599318.
14A ProofoftheTheoremontheEffectivenessofInstanceModification
Toprovideacomprehensiveproofofthetheoremregardingtheeffectivenessofinstancemodification
inlearningfromnoisylabels,wewillassumethedefinitionsandsetupdescribedinthetheorem’s
statement. Wewilladdresseachcomponentofthetheorem,demonstratinghowtheinstancemodifica-
tionapproachtheoreticallyleadstoimprovementsinalignment,errorreduction,estimationstability,
andgeneralization.
Proof. WeproveeachcomponentofTheorem3.1regardingtheeffectivenessofinstancemodification
asfollows:
1. Alignment:
Claim: ThemutualinformationbetweenX′ andY˜, I(X′;Y˜), ismaximizedrelativetoI(X;Y˜),
indicatingbetteralignmentofmodifiedinstanceswiththeirnoisylabels.
Proof. Mutual information I(X;Y) measures the amount of information one random variable
containsaboutanother. BymodifyingX intoX′ usingthetransformationT,whichincorporates
Y˜, we embed information about Y˜ directly into X′. Since X′ is derived from both X and Y˜, it
inherentlycontainsalltheinformationX hasaboutY˜ andadditionalinformationfromthedirect
dependenceonY˜. Therefore,I(X′;Y˜)≥I(X;Y˜). Toprovideamorerigorousmathematicalproof
ofthisclaim,weproceedasfollows:
ThemutualinformationI(X;Y˜)isdefinedas:
I(X;Y˜)=H(X)−H(X |Y˜)
Similarly,themutualinformationI(X′;Y˜)is:
I(X′;Y˜)=H(X′)−H(X′ |Y˜)
SinceX′isgeneratedbythetransformationT whichtakesX andY˜ asinputs,i.e.,X′ =T(X,Y˜;θ),
wecananalyzethechangesinentropyandconditionalentropyasfollows:
H(X′|Y˜)≤H(X|Y˜)+H(T|Y˜,X)
BecauseT isadeterministictransformation,H(T|Y˜,X)=0. Therefore,
H(X′|Y˜)≤H(X|Y˜)
ConsideringthatX′incorporatesinformationfrombothX andY˜,theentropyofX′satisfies:
H(X′)≥H(X)
Usingthedefinitionsofmutualinformationandtheresultsfromtheentropyandconditionalentropy
comparisons,weget:
I(X′;Y˜)=H(X′)−H(X′|Y˜)≥H(X)−H(X|Y˜)=I(X;Y˜)
ThisinequalityshowsthatthemutualinformationbetweenX′andY˜ isgreaterthanorequaltothe
mutualinformationbetweenX andY˜.
2. ErrorReduction:
Claim: Compared to a model trained on the original instances X, the expected prediction error
E [(Y −f(X′))2]isminimized,wheref isthepredictionfunctiontrainedusingthemodified
X′,Y
instances X′. This assumes that the distribution of X′ does not deviate significantly from the
distributionofX,ensuringthatthelearnedmodelgeneralizeswelltotheoriginaldistribution.
Setup: Letf denotethepredictionmodeltrainedontheoriginalfeaturesX andf themodel
X X′
trainedonmodifiedfeaturesX′. Assumealinearrelationshipforsimplicity,thoughthemodelcanbe
generalizedtonon-linearrelationships.
Formulation:
15• The true model: Y = f¯(X)+ϵ, where ϵ represents random error with E[ϵ] = 0 and
Var(ϵ)=σ2.
• Modifiedfeatures: X′ =T(X,Y˜;θ),whereT aimstooptimizealignmentwithY˜.
Objective: ToprovethatE[(Y −f (X′))2]<E[(Y −f (X))2].
X′ X
Proof.
• ModelDefinitions:
– ModelforX: f (X)=βTX
X X
– ModelforX′: f (X′)=βT X′
X′ X′
• ErrorFormulations:
– ErrorformodelusingX: ϵ =Y −f (X)=Y −βTX
X X X
– ErrorformodelusingX′: ϵ =Y −f (X′)=Y −βT X′
X′ X′ X′
• ExpectedPredictionError:
ForX :E[ϵ2 ]=E[(Y −βTX)2]=E[Y2]−2E[Y(βTX)]+E[(βTX)2)]
X X X X
ForX′ :E[ϵ2 ]=E[(Y −βT X′)2]=E[Y2]−2E[Y(βT X′)]+E[(βT X′)2)]
X′ X′ X′ X′
SinceE[Y2]isthesameinbothcases, wemainlycompareE[YβTX]withE[YβT X′],
X X′
andE[(βTX)2]withE[(βT X′)2].
X X′
• ReductioninError:
– Weassumethatθ inT isconserved, β ischosensuchthatthevarianceofϵ is
X′ X′
minimized. Thisisachievedbyensuringβ alignswiththemodifiedfeaturespaceof
X′
X′tocapturemorerelevantinformationaboutY containedimplicitlyinY˜.
– TheadditionalalignmentinformationinX′comparedtoX reducesthecomponentof
theerrorduetonoiseorirrelevantvariabilityinX. Hence,E[ϵ2 ]isexpectedtobe
X′
lessthanE[ϵ2 ].
X
• MathematicalJustification:
Torigorouslyprovethis,weusecovarianceformulas. Assumeβ andβ aretheopti-
X X′
malregressioncoefficientsobtainedbyminimizingthemeansquarederroronX andX′
respectively:
Wewanttofindβ thatminimizesthesumofsquarederrors:
X
minE[(Y −βTX)2]=min(E[Y2]−2E[Y(βTX)]+E[(βTX)2)])
X X X
βX βX
Takingthederivativewithrespecttoβ andsettingittozero:
X
∂
(E[Y2]−2E[Y(βTX)]+E[(βTX)2)])=0
∂β X X
X
0−2E[YX]+2E[XXT]β =0
X
E[XXT]β =E[YX]
X
AssumingE[XXT]isinvertible,wegetβ =(E[XXT])−1E[YX].
X
ForX′,thederivationissimilar,wegetβ =(E[X′X′T])−1E[YX′].
X′
Since X′ contains more information about Y, we can assume E[YX′] ≥ E[YX]. This
directlyleadstoβT E[YX′]≥βTE[YX].
X′ X
Combiningthesewiththeexpectedpredictionerrorformulas:
E[(Y −βTX)2]=E[Y2]−2βTE[YX]+βTE[XXT]β
X X X X
E[(Y −βT X′)2]=E[Y2]−2βT E[YX′]+βT E[X′X′T]β
X′ X′ X′ X′
Because X′ aligns better with the information in Y˜, we can assume E[(βT X′)2] ≤
X′
E[(βTX)2].Sinceβ isobtainedbyminimizingE[(Y−βT X′)2],andE[YX′]≥E[YX],
X X′ X′
itfollowsthat:
E[(Y −βT X′)2]≤E[(Y −βTX)2]
X′ X
163. EstimationStability:
Claim: Thevarianceoftheestimatorf isreducedwhenusingX′comparedtoX,resultinginmore
stablepredictions.
Formulation: Assumethefollowinglinearregressionmodelsforsimplicity, thoughtheconcepts
generalizetonon-linearmodels:
• ModelusingX: f =βTX+ϵ ,whereϵ isthenoiseterm.
X X X X
• ModelusingX′: f =βT X′+ϵ ,whereϵ isthenoisetermforthemodifiedmodel.
X′ X′ X′ X′
Objective: Todemonstratethatthevarianceoftheestimatorf islowerthanthatoff .
X′ X
Proof.
• Model Definitions and Assumptions: Assume that both β and β are obtained by
X X′
ordinaryleastsquares(OLS),implyingthattheyminimizetherespectivemeansquarederrors.
ThevarianceoftheestimatorinOLSisinverselyproportionaltotheFisherinformationof
themodel,FisherinformationmatrixI(β)isrepresentedasXTX andX′TX′,reflecting
thevariabilityofinputfeatures.
• VarianceofEstimators: ThevarianceofeachestimatorunderOLScanbeexpressedas
follows:
Var(f )=σ2(XTX)−1
X
Var(f )=σ2(X′TX′)−1
X′
whereσ2isthevarianceoftheerrortermsϵ andϵ ,assumedequalforsimplicity.
X X′
• Comparative Analysis of Variance: Since X′ is designed to be more informative and
alignedwithY˜,itisreasonabletoassumethatX′ exhibitshighereffectivevariabilityin
thedimensionsthataremostrelevantforpredictingY. Thisincreasedeffectivevariability
inrelevantdimensionsimpliesthatthematrixX′TX′ ismorewell-conditioned(i.e.,has
largereigenvaluesandahigherdeterminant)thanXTX,leadingtoasmallervalueforits
inverse. Specifically, the increase in eigenvalues indicates an increase in information in
relevantdimensions,making(X′TX′)−1smallerthan(XTX)−1:
(X′TX′)−1 <(XTX)−1
Hence,Var(f )<Var(f ).
X′ X
• EstimationStability: Thereductioninvarianceimpliesthatf offersmorestableand
X′
reliablepredictionscomparedtof . Thisstabilityiscrucialwhenthemodelisappliedin
X
practice,particularlyinthepresenceofnoisydataconditions.
This detailed proof shows that by focusing on feature dimensions that are more predictive of Y,
instancemodificationviaX′notonlyimprovesthealignmentwiththenoisylabelsbutalsoenhances
thestabilityofthemodel’spredictions.
4. Generalization:
Claim: ModificationsinX′leadtobettergeneralization. Bytransformingtheoriginalinstancesto
betteralignwiththeirnoisylabels,themodeltrainedonX′islesstooverfittothenoiseandmore
capableofcapturingthetrueunderlyingpatternsinthedata.
Setup: Let
• X denotes the original feature space and X′ = T(X,Y˜,θ) denote the modified feature
space,whereT isatransformation,andθisfixed,thatoptimizessomeaspectofthedatato
betteralignwithnoisylabelsY˜.
• F betheclassoffunctionsf :X →Rconsideredbythelearningalgorithm,whereX is
eitherthespaceofX orX′.
17Rademacher Complexity: Rademacher complexity measures the ability of a function class to fit
random noise. The Rademacher complexity for the class of functions F applied to the original
featuresX andthemodifiedfeaturesX′aredefinedrespectivelyas:
(cid:34) n (cid:35)
1 (cid:88)
R (F )=E sup σ f(X )
n X σ,X n i i
f∈FX
i=1
(cid:34) n (cid:35)
1 (cid:88)
R (F )=E sup σ f(X′)
n X′ σ,X′ n i i
f∈F X′ i=1
GeneralizationBounds: Usingthesedefinitions,thegeneralizationboundsforbinaryclassification
underthe0-1losscanbeexpressedforbothfeaturesets. AssumingthesamehypothesisclassF,the
boundsare:
(cid:32) (cid:12) (cid:12) 1 (cid:88)n (cid:12) (cid:12) (cid:33) (cid:18) 2ϵ2n (cid:19)
Pr sup (cid:12)E[l(f(X),Y)]− l(f(X ),Y )(cid:12)>ϵ ≤2exp −
(cid:12) n i i (cid:12) R (F )2
f∈FX(cid:12)
i=1
(cid:12) n X
(cid:32) (cid:12) (cid:12) 1 (cid:88)n (cid:12) (cid:12) (cid:33) (cid:18) 2ϵ2n (cid:19)
Pr sup (cid:12)E[l(f(X′),Y)]− l(f(X′),Y )(cid:12)>ϵ ≤2exp −
(cid:12) n i i (cid:12) R (F )2
f∈F X′(cid:12)
i=1
(cid:12) n X′
Impact of Instance Modification on Feature Space: The transformation T is designed to adjust
features in X to more effectively align with Y˜, potentially reducing the variability of X that is
irrelevanttopredictingY. Thistransformationcan:
• Increasethesignal-to-noiseratioinX′comparedtoX.
• FocusthevariabilityinX′onaspectsthataremorepredictiveofY,basedontheinformation
containedinY˜.
Proof. ToshowthatR (F )<R (F ),weanalyzehowthetransformationT affectstheability
n X′ n X
offunctionclassF tofitrandomnoise:
• ReductioninEffectiveVariance: SinceT reducesthenon-predictivevariabilityofX,the
effectivevarianceofX′withinthecontextofthefunctionclassF islower. Lowervariance
inthefeaturesdirectlytranslatestoareducedabilitytofitarbitrarylabels(noise),effectively
decreasingtheRademachercomplexity.
• Alignment with Output: If T enhances the alignment of X′ with the output Y˜, the
predictions by F on X′ are less variable for random labeling. This reduction in fitting
capacitytorandomlabelsalsopointstoareducedRademachercomplexity.
ConsidertheexpectationofthesupremumofcorrelationwithRademachersequences:
(cid:34) n (cid:35) (cid:34) n (cid:35)
1 (cid:88) 1 (cid:88)
R (F )=E sup σ f(X′) <E sup σ f(X ) =R (F )
n X′ σ,X′ n i i σ,X n i i n X
f∈F f∈F
i=1 i=1
duetothefocusedandreducedvariabilityinX′relevanttotheoutputY˜.
TheinequalityderivedfromcomparingtheRademachercomplexitiesandthecorrespondinggen-
eralization bounds provides a theoretical basis for asserting that instance modification enhances
themodel’sabilitytogeneralize. Thisproofunderscorestheimportanceoffeaturealignmentand
relevanceinimprovingmachinelearningmodelperformanceinnoisysettings.
B MoreDetailsofTheoremandAnalysisofInstanceModification
To validate the correctness of our proposed theorem 3.1, we undertook specific experiments to
demonstrateitsefficacy. ThetheorempositsthatbyapplyinganappropriatetransformationT,the
18alignmentbetweentheinstancesX andthenoisylabelsY˜ canbeoptimized,therebyincreasingtheir
mutualinformation. OntheCIFAR-10dataset,wecalculatedthemutualinformationbetween50,000
imagesandtheirlabels. AsobservedinFigure5a,themutualinformationI(X′;Y˜)betweenthe
transformedinstancesX′andthenoisylabelY˜ issignificantlyhigherthanthemutualinformation
I(X;Y˜) between the original instances X and Y˜. Figure 5b also argues the third point of our
theorem,i.e.,theestimatortrainedonX’hasabettervariancethanthattrainedonX,whichillustrates
thehigherstabilityandrobustnessofourmethod. Furthermore,concerningpredictionerror,Figure6
displaysthetrainingandtestingresultsunderdifferentnoisetypes. Theresultsshowthatbothinthe
trainingandtestingsets,usingthemodifiedsamplesresultedinsignificantlylowererrors.
(a) (b)
Figure5: (a)illustratesthemutualinformationbetweenthelabelsof50,000originalsamplesand
theircorresponding50,000modifiedsamplesunder50%instance-dependentnoiseonCIFAR-10. (b)
showsthedistributionofthepredictiveprobabilityoftheestimatorf usingX′andX.
(a)50%symmetricnoise (b)45%pairflipnoise (c)50%instance-dependentnoise
Figure6: Figures(a),(b),and(c)respectivelyillustratethedifferencesintrainingandtestinglosses
betweenEchoAlignandtheCEmodelunder50%symmetricnoise,45%pairflipnoise,and50%
instance-dependentnoiseconditionsontheCIFAR-10. Thebrightpeachredanddeepburgundylines
representtheperformanceofCEandEchoAlignonthetestset,respectively,whilethelightpurple
andlightcoralpinklinesdenotetheirperformanceonthetrainingset.
C Datapreprocessingandhyperparametersettings
Data preprocessing For all datasets, including CIFAR-10, CIFAR-100, and CIFAR-10N, we
adoptedaunifiedandstraightforwarddataaugmentationmethod. Specifically, wefirstapplieda
4-pixelpaddingtotheimages,followedbyrandomcroppingto32×32pixels. Subsequently,we
appliedrandomhorizontalflippingandnormalization.
Hyperparametersettings FortheControlNetcontrollablegenerationmodel,weusedthesimplest
Cannypreprocessorwithboththelowthresholdandhighthresholdsetto75. Fortheprompt,we
usedthetemplate“aphotooflabel”withoutanyadditionalpromptsornegativeprompts. Thefeature
mapsoutputbythepreprocessorandthegeneratedimagesbothhaveamediumsizeof512×512
pixels. Thediffusionprocessuses20steps. FortheEchoSelectsection,thedefaultthresholdissetto
190.4forallcaseswith30%noise,and0.52forcaseswith45%and50%noise. Thehyperparameters
forthetrainingpartaredetailedinTable5.
Table5: TraininghyperparametersforCIFAR-10/CIFAR-10NandCIFAR-100.
CIFAR-10/CIFAR-10N CIFAR-100
architecture ResNet-18 ResNet-34
optimizer SGD SGD
lossfunction CE CE
learningrate(lr) 0.1 0.1
lrdecay 100thand150th 100thand150th
weightdecay 10−4 10−4
momentum 0.9 0.9
batchsize 128 128
trainingsamples 45,000 45,000
trainingepochs 200 200
D RuntimeAnalysis
Table6: ComparisonofruntimeatdifferentsettingsusinganNVIDIAV100-SXM2.
CIFAR-10 Clothing1M
Imageresolution
batchsize-1 batchsize-8 batchsize-16 batchsize-16
256×256 31.5 5.5 4.5 129.5
512×512 35.1 18.5 17.2 504.5
768×768 68.5 51.9 (cid:3) (cid:3)
Table7: Comparisonofruntimeatdifferentcomputingperformance.
GPU CIFAR-10 Clothing1M
V100-SXM2 18.5 504.5
RTX4090 8.5 338.2
TheefficiencyofEchoModissignificantlyinfluencedbyseveralfactors, includingthechoiceof
the controllable generative model, the GPU’s floating-point operations per second (FLOPS), the
resolution of generated images, batch size, GPU memory capacity, floating-point precision, and
thenumberofdiffusionstepsifadiffusionmodelisused. Inthisstudy, weemployanNVIDIA
V100-SXM2 with 32GB of VRAM, using ControlNet as the benchmark generative model, and
applymixedprecisiontoassesstheimpactsofimagesizeandbatchsizeonruntime. Runtimeis
measuredinGPUhours,whichquantifiesthetimeconsumedtoperformcomputationaltasksona
singleGPU.AsthenumberofGPUsincreases,weobserveasuper-lineardecreaseinruntime. Our
experimentsareconductedontheCIFAR-10dataset,andwealsoestimatetheruntimeforprocessing
theClothing1MdatasetonthesameGPUconfiguration. Table6demonstratesthatincreasingthe
batchsizeandreducingtheimageresolutionbothsignificantlyimpactruntime. Wedidnotconduct
testswithimageresolutionat768×768andabatchsizeof16duetosimilarVRAMconstraints.
Additionally, inTable7, wecomparetheeffectsofdifferentcomputingperformanceonruntime.
WeconductedtestsontwodifferentGPUswithanimageresolutionof512×512andabatchsize
of8. TheNVIDIAV100-SXM2-32GBoffersahalf-precisioncomputecapabilityof125Tensor
TFLOPSandasingle-precisioncapabilityof15.7TFLOPS.Incontrast,themorepowerfulNVIDIA
RTX 4090-24GB GPU provides 165.2 Tensor TFLOPS in half-precision and 82.58 TFLOPS in
single-precision.
20E ChallengesofInstanceModification
Instance modification presents several significant challenges that need to be addressed to ensure
theeffectivenessandreliabilityofthemodifiedinstances. Instancemodificationprimarilyrelieson
ControllableGenerativeModels(CGM),whichcangenerateinstancesthatmeetspecificrequirements
undercertainconditions.However,CharacteristicShiftisamajorchallenge,asillustratedinFigure7a.
Characteristic Shift occurs during instance modification when the fundamental attributes of the
instancesarealtered. Thesechangescanleadtoinconsistenciesanddistortionsintheoriginaldata,
theeffectmajorlyduetothegapbetweentheintrinsicfeaturesoftheoriginalinstancesandthenoise
labels. Forexample, iftheCGMssignificantlyalterkeyfeaturessuchasshapeortextureduring
modification, the models may fail to learn consistent representations. Therefore, addressing the
CharacteristicShiftbetweenoriginalinstancesandnoiselabelsiscrucial. Anothermajorchallenge
is Distribution Shift, as shown in Figures 7b and 7c. This challenge arises when the statistical
distributionofthemodifiedinstancesdeviatesfromtheoriginaldatadistribution. Suchshiftscanlead
tomodelsperformingwellonmodifieddatabutpoorlyonreal-worlddata. TheT-SNEvisualization
ofCIFAR-10instancerepresentationsshowsthechangesininstancedistributionbeforeandafter
usingEchoAlign,showcasingtheeffectivenessofourmethod.
(a)CharacteristicShift.
(b)T-SNEVisualizationofCIFAR-10instancerepre- (c)T-SNEVisualizationofCIFAR-10instancerepre-
sentationsbyusingX. sentationsbyusingX′
Figure7: (Top)Mainchallenge1: CharacteristicShift. (Bottom)Mainchallenge2: Distribution
Shift.
F MoreGenerationExamples
In this section, we present the results of several publicly available controllable (CGM) and non-
controllable(NGM)generativemodelsthatcaneasilygeneratedemos. Weused“aphotoofnoisy
label”asthepromptandtheoriginalinstanceasthecontrolcondition. AsshowninTable8, for
controllablegenerativemodels,wepresenttheresultsofControlNetandUniControl[Zhaoetal.,
2023]. Fornon-controllablegenerativemodels,weshowcasetheperformanceofGPT-4[Achiam
etal.,2023]andGemini[Teametal.,2023].
21Table8: ResultsofControllableandNon-ControllableGenerativeModels
CGM NGM
NoisyLabel OriginalInstance ControlNet UniControl GPT-4 Gemini
Cat
Magpie
T-shirt
Fabricbag
Dressshoes
G Framework
Figure8illustratestheframeworkofEchoAlign,whichcomprisestwomainmodules: EchoModand
EchoSelect. TheEchoModmoduleusesControllableGenerativeModels(CGM)tomodifynoisy
instances,generatinginstancesconsistentwiththeirlabels. Specifically,theCGMreceivesnoisy
instancesandgeneratesnewinstancesbasedontheimages’labelsandintrinsicfeatures. Through
theEchoModmodule,weobtainasetofmodifiedinstances. Subsequently,thesemodifiedinstances
entertheEchoSelectmodule. Inthismodule,thesystemusescosinesimilarityevaluationtoselectthe
instancesfromthenoisydatathataremostsimilartothemodifiedinstances.Thesimilarityevaluation
assumesthatthefeaturesofcleaninstancesarehighlyconsistentwiththoseofthemodifiedinstances,
therebyselectingsufficientlycleansamplestoenhancethemodel’sgeneralizationcapability. Finally,
theselectedcleaninstancesandthemodifiedinstancestogetherformthetraininginstancesetused
for subsequent model training. In this way, the EchoAlign framework effectively addresses the
issuesofcharacteristicshiftanddistributionshiftininstancemodificationthroughitsrobustinstance
modificationandselectionprocesses,ensuringthemodel’sgeneralizationabilityandrobustness.
22Figure8: TheframeworkofEchoAlign.
23