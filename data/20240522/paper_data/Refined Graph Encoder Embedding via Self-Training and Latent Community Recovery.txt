Refined Graph Encoder Embedding via Self-Training
and Latent Community Recovery
CenchengShen
DepartmentofAppliedEconomicsandStatistics
UniversityofDelaware
Newark,DE19711
shenc@udel.edu
JonathanLarson
MicrosoftResearch
Redmond,WA98052
jolarso@microsoft.com
HaTrinh
MicrosoftResearch
Redmond,WA98052
trinhha@microsoft.com
CareyE.Priebe
DepartmentofAppliedMathematicsandStatistics
JohnsHopkinsUniversity
Baltimore,MD21218
cep@jhu.edu
Abstract
This paper introduces a refined graph encoder embedding method, enhancing
theoriginalgraphencoderembeddingusinglineartransformation,self-training,
andhiddencommunityrecoverywithinobservedcommunities. Weprovidethe
theoreticalrationalefortherefinementprocedure,demonstratinghowandwhyour
proposedmethodcaneffectivelyidentifyusefulhiddencommunitiesviastochastic
blockmodels,andhowtherefinementmethodleadstoimprovedvertexembedding
andbetterdecisionboundariesforsubsequentvertexclassification. Theefficacyof
ourapproachisvalidatedthroughacollectionofsimulatedandreal-worldgraph
data.
1 Intro
Graph data has surged in popularity, serving as an ideal data structure for capturing interactions
acrossdiversedomains,includingsocialnetworks,citationsystems,communicationnetworks,and
physicalandbiologicalsystems(GirvanandNewman,2002;Newman,2003;BarabásiandOltvai,
2004;Boccalettietal.,2006;Varshneyetal.,2011;Uganderetal.,2011). Thisriseisdrivenby
theincreasingavailabilityofpublicgraphdatasets(LeskovecandKrevl,2014;RossiandAhmed,
2015;Huetal.,2020),coupledwithgrowinginterestingraphlearningtechniquessuchasgraph
convolutional networks (GCN) (Kipf and Welling, 2017; Wu et al., 2019; Xu et al., 2019; Wang
Preprint.Underreview.
4202
yaM
12
]IS.sc[
1v79721.5042:viXraandLeskovec,2022),aswellasaplethoraofliteratureexploringgraphdata,includingtheoretical
foundationsandmodernapplications.
Inthefieldofgraphlearningliterature,spectralembeddingstandsoutasafundamentalapproach
foranalyzinggraphdata. Itprojectsgraphdataintoad-dimensionalspaceusingeitherthegraph
adjacencyorgraphLaplacian(Roheetal.,2011;Sussmanetal.,2012;Priebeetal.,2019). While
manygraphlearningmethodsofferlow-dimensionalrepresentations,spectralembeddingstandsout
for its interpretability, as its vertex embedding converges to the underlying latent position under
popularrandomgraphmodels(Sussmanetal.,2014;Athreyaetal.,2018;Rubin-Delanchyetal.,
2022). Consequently,spectralembeddingprovidesaversatileandtheoreticallysoundgraphlearning
technique,withapplicationsincludingvertexclassification(Tangetal.,2013;Mehtaetal.,2021),
communitydetection(Muetal.,2022;Gallagheretal.,2023),vertexnomination(Zhengetal.,2022),
andtheanalysisofmultiplegraphsandtime-series(Arroyoetal.,2021;Gallagheretal.,2021).
However,thescalabilityofspectralembeddinghasbeenamajorbottleneckduetoitsuseofsingular
value decomposition (SVD), which can be time-consuming for moderate to large graphs. When
vertexlabelsareavailableforatleastpartofthevertexset,arecentmethodcalledone-hotgraph
encoderembedding(Shenetal.,2023), whichcanbeviewedasasupervisedversionofspectral
embedding,issignificantlyfaster. Itsvertexembeddingconvergestotheblockprobabilityvector
inthecaseofthestochasticblockmodel(Hollandetal.,1983),whichissimilartotheconvergence
ofspectralembedding. Duetotheuseofadditionallabelinput,thegraphencoderembeddingcan
outperformspectralembeddinginfinite-sampleperformanceacrossarangeofapplications,suchas
vertexclassification,clustering(Shenetal.,2023),multiple-graphinference(Shenetal.,2023),and
dynamic-graphanalysis(Shenetal.,2024). ThemostimportantadvantageofGEEisitssimplicity
andscalability: itinvolvesonlyonematrixmultiplicationandasinglepassovertheadjacencymatrix
(oredgelist),resultinginlinearcomputationalcomplexitywithrespecttothenumberofedgesand
vertices,withaconstantoverheadof1. Itisabletoprocess100millionedgeswithinseconds,making
itmuchfasterthanothergraphlearningmethodssuchasspectralembedding,GCN,andnode2vec
(GroverandLeskovec,2016).
Inthispaper,weproposeanenhancedversionofthegraphencoderembeddingthatincorporates
self-trainingandlatentcommunityrecovery. Therefinedgraphencoderembeddingcandetecthidden
sub-communitieswithinthegivenlabels. Ithasthecapabilitytodetectallsub-communitiesifdesired,
or to selectively detect latent communities that can benefit subsequent vertex classification and
preventover-refinement. Usingstochasticblockmodelswithvaryingparameterchoices,weprovide
illustrative examples to show what the refined method can detect, the meaning of the recovered
communitiesincontext,andwhydetectionshallberestrictedforthebenefitofvertexclassification.
Theoreticalrationaleisprovided,andtheproposedmethodisvalidatedthroughsimulationsandawide
rangeofrealgraphdata. Theappendixincludestheoremproofs,additionalnumericalevaluations
oncomputationalscalability,communityrefinementvisualizationsontworealgraphs,andmultiple-
graphexperiments.AllexperimentsarecarriedoutonalocaldesktopwithMATLAB2024a,Windows
10,anINTEL16-coreCPU,and64GBofmemory.
2 Review
Inthissection,webrieflyreviewthestochasticblockmodel(SBM),aclassicalrandomgraphmodel
(Hollandetal.,1983;SnijdersandNowicki,1997;KarrerandNewman,2011). Thisisfollowedby
anoverviewoftheadjacencyspectralembeddingandtheoriginalgraphencoderembedding.
2.1 GraphAdjacencyandStochasticBlockModels
Agraphconsistsofasetofverticesv ,i=1,...,nandasetofedgese ,j =1,...,s,whichcan
i j
besuccinctlyrepresentedbyann×nadjacencymatrixA. Inthismatrix,A(i,j)=0indicatesthe
absenceofanedgebetweenvertexiandj,whileA(i,j)=1indicatestheexistenceofanedge. The
adjacencymatrixcanalsobeweightedtoreflectaweightedgraphand,moregenerally,canrepresent
anysimilarityordissimilaritymatrix,suchaspairwisedistanceorkernelmatrices.
Under SBM, each vertex i is assigned a label Y(i) ∈ 1,...,K. The probability of an edge
betweenavertexfromclasskandavertexfromclasslisdeterminedbyablockprobabilitymatrix
2B=[B(k,l)]∈[0,1]K×K. Foranyi̸=j,itholdsthat
A(i,j)∼Bernoulli(B(Y(i),Y(j))).
Togenerateanundirectedgraph,simplysetA(j,i)=A(i,j)foralli<j.
Thedegree-correctedstochasticblockmodelisanextensionoftheSBMthataccountsforthesparsity
observedinrealgraphs(Zhaoetal.,2012). Itassignsadegreeparameterθ ∈[0,1]toeachvertexi.
i
Giventhedegrees,eachedgefromvertexitoanothervertexj isindependentlygeneratedasfollows:
A(i,j)∼Bernoulli(θ θ B(Y(i),Y(j))).
i j
2.2 SpectralEmbeddingandEncoderEmbedding
GivenanadjacencymatrixA∈Rn×n,let
USVT =A
bethesingularvaluedecomposition. LetS bethefirstd×dsubmatrixofS,andV bethefirst
d d
n×dsubmatrixofV,theadjacencyspectralembedding(ASE)is
ZASE =V S0.5 ∈Rn×d.
d d
TheLaplacianspectralembedding(LSE)hastheexactsameformulation,excepttheadjacencymatrix
AisreplacedbythecorrespondinggraphLaplacianL.
Theencoderembeddingrequiresanadditionalinput,alabelvectorY ∈[0,1,...,K]n,wherethere
areatotalofK classesandalabelof0meansunknownvertexlabel, thusallowingpartiallabel
vector. Itthencomputesthenumberofknownobservationsperclassas
n
(cid:88)
n = 1(Y(i)=k)
k
i=1
fork = 1,...,K. Thisisfollowedbyanormalizedone-hotencodingmatrixW ∈ [0,1]n×K as
follows: foreachvertexi=1,...,n,set
W(i,k)=1/n
k
ifandonlyifY(i) = k,and0otherwise. Theencoderembeddingisthencomputedbyasimple
matrixmultiplication:
Z=AW∈[0,1]n×K.
Fortheembedding,weuseZ(i,:)torepresenttherowvector,whichistheembeddingforvertexi.
Notethattheencoderembeddingcanbecomputedwithoutanyground-truthlabelsbyusingrandom
labelinitializationanditerativek-means(Shenetal.,2023). Additionally,anextranormalizationstep
pervertexcanbebeneficialinsparsegraphanalysis,i.e.,normalizingZ(i,:)byitsL2normforevery
i.
3 RefinedGraphEncoderEmbedding
3.1 LinearTransformationforSelf-Training
The refined GEE algorithm is shown in Algorithm 2 in pseudo-code, which relies on a linear
transformationoftheoriginalGEE,asdetailedinAlgorithm1.
ThepurposeofAlgorithm1istotransformtheoriginalGEEsuchthatthedimensionattainingthe
maximumvaluecanbeproperlyusedtodeterminetheclassassignmentforeachvertex. Whileone
couldapplyaneuralnetworkandusethesoftmaxoutputforthispurpose,theencoderembeddingis
approximatelynormallydistributed,asstatedinTheorem1. Therefore,lineardiscriminantanalysis
(LDA)isasuitableandfasterchoicetoestimatetheconditionalprobabilityandaligntheembedding.
Specifically,theLDAfunctioninAlgorithm1isasfollows. DenotetheoriginalGEEasZ. Letµ ∈
k
RK betheclass-conditionalmeanofZ(i,:)|Y(i) = k fork = 1,...,K,µ = [µ ,µ ,...,µ ] ∈
1 2 K
RK×K betheconcatenatedmeans,andΣ ∈ RK×K betheestimatedcommoncovariancematrix
3ofZ. LetΣ+ bethepseudo-inverse,⃗n bearowvectorwherethekthentryisn ,anddiagmean
K k
extractingthediagonaltermsasarowvector. Then
Z =ZΣ+µ−(diag(µ′Σ+µ)−log(⃗n /n)) (1)
1 K
isthelineartransformedGEEembedding,wheretheentiretermafterthefirstminussignshouldbe
understoodasarowvectorandsubtractedperrow. Itfollowsthat
Y (i)=arg max Z (i,k)
1 1
k=1,...,K
istheself-trainednewlabel. Onecanthencalculateamismatchedindexbetweentheinputlabel
vectorYandtheself-trainedlabelvectorY .
1
Algorithm1GEESelfTrainingviaLinearDiscriminantAnalysis(GEELDA)
Require: ThegraphadjacencymatrixA∈Rn×nandalabelvectorY ∈{0,1,...,K}n,where1
toK representknownlabels,and0isadummycategoryforverticeswithunknownlabels.
Ensure: A linear transformed encoder embedding Z ∈ Rn×K, self-trained new label Y ∈
1 1
{0,1,...,K}n,andalogicalvectoridx∈[0,1]nwhere1indicatesmismatchedlabels.
functionGEELDA(A,Y)
Z=GEE(A,Y); ▷originalone-hotgraphencoderembedding
Z =LDA(Z,Y); ▷transformtheencoderembeddingbyEquation1
1
[,Y ]=rowmax(Z ); ▷themaximumdimensionpervertex
1 1
ind=find(Y ==0);
Y (ind)=0; ▷omitverticeswithunknownlabels
1
idx=(Y ̸=Y );
1
endfunction
3.2 RefinedGEEviaSelf-TrainingandLatentCommunityRecovery
GiventhelineartransformationinAlgorithm1, Algorithm2refinesthelabelvectorviaiterative
self-trainingandlatentcommunityassignment. Theiterativeself-trainingpartisstandard,withtwo
parameters,ϵandϵ ,thatstopself-trainingwhenthemismatchbetweentraininglabelsandself-trained
n
labelsnolongerreduces. Fortheiterativecommunityassignment,wereassignmismatchedtraining
dataineachclassintoanewclasseachtimeandstoptherefinementwhenthemismatchbetween
traininglabelsandself-trainedlabelsnolongerreduces. Finally,theoriginalencoderembedding,
alongwithalltherefinedembeddingfromself-trainingandhiddencommunityassignmentpriorto
stopping,areconcatenated.
Notethattheparametersϵandϵ controlhowaggressivetherefinementis. Theusermayadjustthem
n
forcross-validationpurposesorothertasks. Forexample,ifthedownstreamtaskistovisualizeall
hiddencommunitiesordetectoutliers,onemaysetϵandϵ tosmallervalues,say0,andγ andγ
n K Y
tolargevalues(e.g.,100). Thisensuresthattherefinementwillcontinueaslongasthemismatched
indicescontinuetodecrease. Ourdefaultparameterchoiceisdesignedtobeslightlyconservative,
whichperformedwellthroughoutsimulationsandrealdataexperimentsforvertexclassification.
Finally,therefinedgraphencoderembeddingretainsthesamescalabilityadvantageastheoriginal
graphencoderembedding. SeeAppendixBandFigure3forarunningtimecomparison.
4 TheoreticalRationale
Inthissection,weprovidethetheoreticalrationalefortheself-trainingbylineartransformationand
thelatentcommunityassignment,explainingwhytheproposedmethodworksandwhencommunity
refinementhelpsimproveembeddingquality.
Theorem 1. The graph encoder embedding is asymptotically normally distributed under SBM.
Specifically,asnincreases,foragivenithvertexofclassy,itholdsthat
diag(⃗n )0.5·(Z(i,:)−µ )→n N(0,Σ ).
K y y
Theexpectationandcovarianceare: µ =B(y,:)andΣ (k,k)=B(y,k)(1−B(y,k)). Assuming
y y
Σ isthesameacrossally ∈[1,K],thetransformationinEquation1satisfies
y
n
Z (i,k)→Prob(Y =k|X =Z(i,:)).
1
4Algorithm2RefinedGraphEncoderEmbedding(R-GEE)
Require: ThegraphadjacencymatrixA∈Rn×nandalabelvectorY ∈{0,1,...,K}n;number
ofrefinementγ andγ ,setto5bydefault;stoppingcriterionϵ∈[0,1]andϵ ∈N,setto0.3
K Y n
and5bydefault.
Ensure: TherefinedgraphencoderembeddingZ∈Rn×d,aconcatenatedlabelmatrixY.
functionR-GEE(A,Y,γ ,γ ,ϵ,ϵ )
K Y n
[Z,Y ,idx ]=GEELDA(A,Y);Y =Y ;
1 1 1
fork =1,...,γ do
Y
[Z ,Y ,idx ]=GEELDA(A,Y );
2 2 2 1
ifsum(idx )−max(sum(idx )∗ϵ,ϵ )<sum(idx &idx )then
1 1 n 1 2
Break;
else
Z=[Z,Z ];Y =Y ;Y =[Y,Y ];
2 1 2 1
idx =idx &idx ;
1 1 2
endif
endfor
fork =1,...,γ do
K
[Z ,Y ,idx ]=GEELDA(A,Y +idx ∗K);
2 2 2 1 1
ifsum(idx )−max(sum(idx )∗ϵ,ϵ )<sum(idx &idx )then
1 1 n 1 2
Break;
else
Z=[Z,Z ];Y =Y ;Y =[Y,Y ];
2 1 2 1
idx =idx &idx ;
1 1 2
endif
endfor
endfunction
Theorem1showsthattheoriginalgraphencoderembeddingisasymptoticallynormallydistributed.
Asaresult,theproposedlineartransformationapproximatestheconditionalprobability,makingitan
appropriatechoiceforsubsequentself-training.
Theorem2. Supposethegraphisdistributedasthestochasticblockmodelwithblockprobability
B∈RK×K andobservedlabelvectorY ∈[1,...,K]. Thenforanytwoverticesi,j,theencoder
embeddingZusingobservedlabelssatisfies:
n
∥Z(i,:)−Z(j,:)∥ −∥B(Y(i),:)−B(Y(j),:)∥ →0
2 2
Suppose the same graph can be viewed as a realization of a latent stochastic block model with
B ∈RK′×K′ andalatentlabelvectorY ∈[1,...,K]whereK′. Theforthesametwovertices
0 0
i,j,theresultingencoderembeddingZ usingthelatentlabelssatisfies:
0
n
∥Z (i,:)−Z (j,:)∥ −∥B (Y (i),:)−B (Y (j),:)∥ →0
0 0 2 0 0 0 0 2
Theorem2suggeststhatwhencomparingtheencoderembeddingusingobservedlabelsversusthe
encoderembeddingusinglatentlabels,themarginofseparationfullydependsontheblockprobability
vectorbetweentheobservedmodelandthelatentmodel. Thismeansthat,fromamarginseparation
perspective,theencoderembeddingusinglatentcommunitiescouldperformbetterorworsethanthe
originalencoderembeddingusingobservedcommunities. Therefore,fortherefinedalgorithmto
improveovertheoriginalencoderembedding,itneedstoproperlydecidewhethertorefinethegiven
labelsornot. Moreover,itisimportanttoconcatenatetheembeddingineachrefinement,becausethe
concatenatedembeddingretainspreviousembeddinginformationandismorerobustagainstslight
over-refinement.
Underthestochasticblockmodel,thistheoremcanhelpverifywhetherthelatentcommunityleadsto
animprovementordeteriorationinthemarginofseparationovertheobservedcommunity. Notethat
thetheoremfocusesonasymptoticbehavior. Infinite-sampleperformance,theembeddingvariance
certainlyplaysaroleinthedecisionboundary. Inthispaper,weonlyconsideredthemeandifference
toillustratethekeyidea,forsimplicityofpresentationandtoavoidoverlycomplicatingmathematical
expressions. Thisisbecausethevarianceisgenerallysimilaracrossthegroupsandboundedabove
by0.25inSBM.
55 Simulations
Westartwiththreestochasticblockmodels,eachservingasarepresentativecase,anduseTheorem2
toverifywhetherthelatentcommunityleadstobetterembeddingseparationamonggroups. Wethen
useembeddingvisualization,vertexclassification,andprecision/recallmetricstoverifytheresults
andassesstheeffectivenessoftherefinedalgorithm.
5.1 ModelParameters
SimulatedGraph1
Foreachvertex,wesetthelatentcommunitiesasY =1,2,3,4withprobability0.25each,andset
0
thelatentblockprobabilitymatrixas
0.5,0.2,0.1,0.1
0.2,0.2,0.1,0.1
B = ,
0 0.1,0.1,0.2,0.2
0.1,0.1,0.2,0.5
i.i.d.
thensetthedegreeparametertobeθ ∼ Uniform(0.1,1).Next,wesettheobservedcommunities
i
asY =1ifY =1,2,andY =2ifY =3,4. Namely,thefirsttwolatentcommunitiesareobserved
0 0
asonegroup,whilethelasttwolatentcommunitiesareobservedasanothergroup. Therefore,the
observedblockprobabilitymatrixcanbecomputedas
(cid:20) (cid:21)
0.275,0.1
B= .
0.1,0.275
NowweuseTheorem2tocheckthemarginofseparation. Whenusingthelatentlabels,themargin
ofseparationbetweenclasses2and3equals∥(0.2,0.2,0.1,0.1)−(0.1,0.1,0.2,0.2)∥=0.2. When
usingtheobservedlabels,thedifferenceis∥(0.275,0.1)−(0.1,0.275)∥=0.25. Therefore,using
theobservedlabelsactuallyprovidesalargermarginofseparationbetweenthesevertices. Notethatif
weconsidertheseparationbetweenclasses1and4,thenthelatentcommunitiesarebetter;however,
thosetwolatentgroupsarelessimportantthantheseparationbetweenlatentclasses2and3.
SimulatedGraph2
The latent communities and block probability matrix are exactly the same as in simulated graph
1. However, theobservedcommunitiesaresetupasfollows: Y = 1ifY = 1,3, andY = 2if
0
Y =2,4. Asaresult,theobservedblockprobabilitymatrixcanbecomputedas
0
(cid:20) (cid:21)
0.225,0.15
B= .
0.15,0.225
Inthiscase,thelatentcommunitieshaveamarginof0.2betweenlatentclass2and3,whichbecomes
0.11whenusingobservedcommunities. Therefore,thissimulationprovidesanexamplewherelabel
refinementisnecessaryandsignificantlyimprovestheembeddingquality.
SimulatedGraph3
Inthissimulation,wesetthelatentcommunitiesasY =1,2,3,4.5withprobability0.2each,and
0
setthelatentblockprobabilitymatrixas
0.5,0.2,0.2,0.1,0.1
0.1,0.2,0.1,0.2,0.1
 
B =0.1,0.1,0.2,0.1,0.2.
0  
0.1,0.2,0.1,0.5,0.1
0.1,0.1,0.2,0.1,0.5
Theobservedcommunitiesare: Y =1ifY =1,2,3;Y =2ifY =4;andY =3ifY =5. Then
0 0 0
theobservedblockprobabilitymatrixcanbecomputedas
(cid:34)0.178,0.133,0.133(cid:35)
B= 0.133,0.500,0.100
0.133,0.100,0.500
6Simulated Graph 1
Latent Community Observed Community GEE-Refined Community
Simulated Graph2
Latent Community Observed Community GEE-Refined Community
Figure1: Thisfigurevisualizesthegraphusinglatentlabels(leftpanel), observedlabels(center
panel),andGEE-refinedlabelsusingonelabelrefinement(rightpanel).
This simulation is somewhat similar to simulated graph 1 but presents a more interesting mixed
situationwheresomedecisionboundariesareimprovedusinglatentgroups,whileothersareworse.
Forexample,thedifferencebetweenverticesinlatentgroup2and4is0.3usinglatentlabels,which
isenlargedto0.37usingobservedlabels. However,class1and4areseparatedby0.5745usinglatent
labels,whichisreducedto0.37usingobservedlabels,andsimilarlyforclass1versus5,orclass4
versus5.
5.2 LatentCommunityRecovery
Figure1showsthegraphconnectivityforsimulatedgraphs1and2,withverticescoloredbylatent
community(left),observedcommunity(center),andGEE-refinedcommunity(right)usingonelabel
refinement(γ =1andγ =0).
K Y
Sincethelatentcommunitiesdonotimprovetheembeddingseparationforsimulatedgraph1,we
expectGEErefinementtolargelyignorethelatentcommunities. Thisisindeedthecaseinthetop
rowofFigure1,wheretherefinementonlyhighlightsafewverticesinthemiddle,andmostvertices
remainintheirobservedgroups.
Thesituationisdifferentforsimulatedgraph2,wherethelatentcommunitiessignificantlyimprove
theembeddingqualityanddecisionboundary. Inthiscase,GEErefinementsuccessfullyidentifies
thelatentcommunities,assigningmostverticesinlatentcommunities2and3todifferentgroups,so
therightpanelcloselymatchestheleftpanelinthebottomrowofFigure1. Tomaintainaclearand
consistentvisualization,simulationgraph3isnotshownhere,asitmerelyrepresentsamixedcase
betweengraph1andgraph2.
75.3 VertexClassificationEvaluation
The top row of Figure 2 reports the 5-fold cross-validation for the simulated graphs using 30
replicates. Foreachreplicate,wegenerateasimulatedgraphofincreasingvertexsize,alongwiththe
correspondinglatentandobservedlabels.
GEE0computestheoriginalGEEusingthelatentcommunitylabelsY . GEEcomputestheoriginal
0
GEE using the observed community labels Y. R-GEE uses the proposed algorithm with default
parameters and the observed community labels Y as input. ASE stands for adjacency spectral
embeddingintod=20. EachmethodisthenevaluatedviaanLDAclassifierfortheobservedlabels
Y. NotethattheclassificationtaskisalwaysfortheobservedlabelsY,andthelatentlabelsY are
0
onlyused forembedding. Moreover, forallGEEmethods, the labelsoftestingobservationsare
assignedto0priortotheembedding.
Forsimulatedgraph1,GEE0usingthelatentlabelshastheworstclassificationerror,whileallother
methodsperformwellandsimilarlytoeachother. Thisresultmatchesthemodelsettingandour
previousverificationthatlatentcommunitiesyieldworseembeddingquality. Forsimulatedgraph2,
GEEusingtheobservedlabelsperformedtheworst,whileR-GEE,GEE0,andASEallperformed
verywell. Thisisareversalofsimulatedgraph1andalsomatchesthemodelsettingandprevious
verificationthatlatentcommunitiesimprovetheembeddingqualityinthiscase. Forsimulatedgraph
3, it is a mixed case where some refinement helps marginally, and GEE0, GEE, and R-GEE all
performedrelativelywellwithsomesmalldifferences.
ThebottomrowofFigure2showstheprecisionandrecallofR-GEE.Insimulatedgraphs1and
3,thevertexclassificationresultsindicatethatlatentcommunitiesarenotimportant,sowhilethe
precisionishigh(alldiscoverednewcommunitiesbelongtothetruelatentcommunities),therecall
isrelativelylow(manyverticesfromthelatentcommunitiesarenotdiscovered). Forsimulatedgraph
2,discoveringthelatentcommunitiesiscritical,andindeedbothprecisionandrecallareveryhigh,
showingthatR-GEEisperformingasintended.
Overall,thisfigureshowsthattheproposedalgorithmworksasdesigned,recoveringlatentcommuni-
tiesonlywhentheyareusefulforvertexclassification,andretainingexcellentembeddingqualitythat
isnotoverlyrefined,asevidencedbythegoodclassificationerrorthatconvergesto0ineverycase.
6 RealDataEvaluation
Wecollectedadiversesetofrealgraphswithassociatedlabelsfromvarioussources,includingthe
Network Repository1 (Rossi and Ahmed, 2015), Stanford network data2, and other public graph
data. Specifically,weexperimentedontheAdjNoungraph(Newman,2003),C.elegansneurondata,
whichprovidestwobinarygraphs(Pavlovicetal.,2014),theEUEmailNetwork(Yinetal.,2017),
thekarateclubgraph(Zachary,1977),theLastFMAsiasocialnetwork(RozemberczkiandSarkar,
2020),thelettergraph,thepoliticalblogsgraph(AdamicandGlance,2005),apoliticalretweetgraph,
thePubmedCitationnetwork,andaWikipediaarticlegraph(Shenetal.,2014)withfourgraphs.
Foramorecomprehensiveevaluationintherealdataexperiments,wecomparedGEE,refinedGEE
(R-GEE),adjacencyspectralembedding(ASE),Laplacianspectralembedding(LSE),andnode2vec
(GroverandLeskovec,2016). R-GEEusedthedefaultparameters;ASEandLSEprojectintod=20
dimensions;node2vecusesthegraspypackage(Chungetal.,2019)withdefaultparametersand128
dimensions. Foreachdatasetandeachmethod,wecarriedout10-foldvalidationandreportedthe
averageclassificationerrorusingLDA,alongwithonestandarddeviation,inTable1with30random
seeds. Anydirectedgraphwastransformedtoundirected,andanysingletonvertexwasremoved.
Notethatunlikethesimulatedgraphs,realgraphsdonotcomewithanyknownlatentcommunities.
Table1clearlyshowsthatrefinedGEEisabletopreserveorimprovetheclassificationerrorcompared
tooriginalGEE.Inafewcaseswhereitisworse,thedifferenceisonlymarginal. Moreover,GEE
andR-GEEareeitherthebestorveryclosetothebestintermsofclassificationerroracrossallreal
dataexperiments. Itshouldbenotedthatallmethodswithparameterscouldattainbetterperformance
ifwetunedtheparametersforeachrealdataset,butwechosetouseconsistentparameterchoices
1http://networkrepository.com/
2https://snap.stanford.edu/
8Simulated Graph 1 Simulated Graph 2 Simulated Graph 3
0.5 0.5 0.4
GEE0
r o GEE
r r 0.4 0.4 0.3 R-GEE
E ASE
n o 0.3 0.3
ita 0.2
c ifis 0.2 0.2
s a 0.1 0.1 0.1
lC
0 0 0
200 1000 2000 200 1000 2000 200 1000 2000
Number of Vertices
Simulated Graph 1 Simulated Graph 2 Simulated Graph 3
1 1 1
y
r
e
v0.8 0.8 0.8
o
c
s
iD0.6 0.6 0.6
y
R-GEE Precision
tin0.4
0.4 0.4
R-GEE Recall
u
m
m0.2 0.2 0.2
o
C
0 0 0
200 1000 2000 200 1000 2000 200 1000 2000
Number of Vertices
Figure2: Thefirstrowofthefigurereportsthe5-foldcross-validationerrorandstandarddeviation
forthethreesimulatedgraphs,using30replicates. Thebottomrowofthefigurereportstheprecision
andrecallforrefinedGEEinrecoveringthelatentcommunities.
throughouttheexperiments. Therefore,theresultsreportedhereshouldbeviewedasaconservative
illustrationoftheproposedmethod,notthebestpossibleerror.
Duetospacelimitations,additionalexperiments,suchasrefinedcommunityvisualizationfortwo
representativecasesandmultiple-graphsimulations,areprovidedinAppendixCandAppendixDto
furtherhighlighttheadvantagesofthemethod.
7 Conclusion
This paper introduces a refined graph encoder embedding, provides a theoretical rationale for
its usefulness, and explains when and how latent communities may improve subsequent vertex
classification. Themethodandtheoremsaresupportedbybothsimulationsandrealdataexperiments.
AcknowledgmentsandDisclosureofFunding
ThisworkwassupportedinpartbytheNationalScienceFoundationHDRTRIPODS1934979,the
NationalScienceFoundationDMS-2113099,andbyfundingfromMicrosoftResearch. Theauthors
declarenocompetinginterests.
References
Adamic,L.andN.Glance(2005). Thepoliticalblogosphereandthe2004uselection: Dividedthey
blog. InProceedingsofthe3rdInternationalWorkshoponLinkDiscovery,NewYork,pp.36–43.
ACMPress.
9(n,K) R-GEE GEE ASE LSE N2v
AdjNoun (112,2) 14.9±2.1 14.6±2.0 18.7±1.3 14.2±1.4 45.9±3.3
C-ElegansAc (253,3) 37.1±2.7 49.7±1.5 38.4±1.3 35.3±1.3 45.3±2.0
C-ElegansAg (253,3) 38.0±2.3 40.8±1.7 42.3±1.1 42.6±1.2 40.2±2.3
Coil-RAG (11687,100) 19.5±1.1 19.5±1.1 97.3±0.1 95.5±0.1 79.1±0.2
Email (1005,42) 29.4±1.1 33.1±0.5 44.4±0.4 88.5±0.4 28.9±0.4
Karate (34,2) 9.5±2.3 9.5±2.3 17.4±4.6 16.6±4.2 13.8±2.7
LastFM (7624,18) 17.5±0.5 18.1±0.2 48.9±0.1 23.1±0.1 14.7±0.1
Letter (10482,26) 3.2±0.2 3.2±0.2 89.9±0.2 88.9±0.2 74.7±0.2
PolBlogs (1224,2) 5.3±0.3 5.0±0.3 9.5±0.2 4.9±0.2 5.1±0.1
PolTweet (1847,2) 2.6±0.1 2.7±0.1 29.8±0.1 4.6±0.1 38.8±0.1
PubMed (19716,3) 20.3±0.1 20.4±0.1 37.4±0.1 34.0±0.1 58.8±0.2
WikiTE (1382,5) 16.0±0.5 20.4±0.3 26.2±0.2 26.6±0.2 n/a
WikiTF (1382,5) 15.8±0.5 20.9±0.3 27.7±0.3 27.7±0.3 n/a
WikiGE (1382,5) 33.5±0.9 41.2±0.5 46.4±0.4 53.8±0.4 40.2±0.5
WikiGF (1382,5) 42.9±0.8 50.6±0.8 47.1±0.3 56.7±0.5 46.6±0.4
Table 1: This table reports the 10-fold vertex classification error and standard deviation for real
graphs, using 30 random replicates. All numbers are in percentile. Note that there are two text
dissimilaritydatasets(WikiTEandWikiTF,whicharecosinedissimilarityoftheunderlyingarticles)
wherenode2vecisnotapplicable.
Arroyo,J.,A.Athreya,J.Cape,G.Chen,C.E.Priebe,andJ.T.Vogelstein(2021). Inferencefor
multipleheterogeneousnetworkswithacommoninvariantsubspace. JournalofMachineLearning
Research22(142),1–49.
Athreya,A.,D.E.Fishkind,M.Tang,C.E.Priebe,Y.Park,J.T.Vogelstein,K.Levin,V.Lyzinski,
Y.Qin,andD.L.Sussman(2018). Statisticalinferenceonrandomdotproductgraphs: asurvey.
JournalofMachineLearningResearch18(226),1–92.
Barabási, A.-L. and Z. N. Oltvai (2004). Network biology: Understanding the cell’s functional
organization. NatureReviewsGenetics5(2),101–113.
Boccaletti, S., V. Latora, Y. Moreno, M. Chavez, and D.-U. Hwang (2006). Complex networks:
Structureanddynamics. PhysicsReports424(4-5),175–308.
Chung, J., P. B. D., E. W. Bridgeford, B. K. Varjavand, H. S. Helm, and J. T. Vogelstein (2019).
Graspy: Graphstatisticsinpython. JournalofMachineLearningResearch20(158),1–7.
Devroye,L.,L.Gyorfi,andG.Lugosi(1996). AProbabilisticTheoryofPatternRecognition.
Gallagher,I.,A.Jones,A.Bertiger,C.E.Priebe,andP.Rubin-Delanchy(2023). Spectralclustering
ofweightedgraphs. JournaloftheAmericanStatisticalAssociation.
Gallagher,I.,A.Jones,andP.Rubin-Delanchy(2021). Spectralembeddingfordynamicnetworks
withstabilityguarantees. AdvancesinNeuralInformationProcessingSystems.
Girvan,M.andM.E.J.Newman(2002). Communitystructureinsocialandbiologicalnetworks.
ProceedingsofNationalAcademyofScience99(12),7821–7826.
Grover,A.andJ.Leskovec(2016). node2vec: Scalablefeaturelearningfornetworks. InProceedings
ofthe22ndACMSIGKDDinternationalconferenceonKnowledgediscoveryanddatamining,pp.
855–864.
Holland, P., K. Laskey, and S. Leinhardt (1983). Stochastic blockmodels: First steps. Social
Networks5(2),109–137.
Hu, W., M.Fey, M.Zitnik, Y.Dong, H.Ren, B.Liu, M.Catasta, andJ.Leskovec(2020). Open
graphbenchmark: Datasetsformachinelearningongraphs. InAdvancesinNeuralInformation
ProcessingSystems,Volume33,pp.22118–22133.
10Karrer,B.andM.E.J.Newman(2011).Stochasticblockmodelsandcommunitystructureinnetworks.
PhysicalReviewE83,016107.
Kipf,T.N.andM.Welling(2017). Semi-supervisedclassificationwithgraphconvolutionalnetworks.
InInternationalConferenceonLearningRepresentations.
Leskovec,J.andA.Krevl(2014,June). SNAPDatasets: Stanfordlargenetworkdatasetcollection.
http://snap.stanford.edu/data.
Mehta, K., R. F. Goldin, D. Marchette, J. T. Vogelstein, C. E. Priebe, and G. A. Ascoli (2021).
Neuronalclassificationfromnetworkconnectivityviaadjacencyspectralembedding. Network
Neuroscience5(3),689–710.
Mu,C.,A.Mele,L.Hao,J.Cape,A.Athreya,andC.E.Priebe(2022). Onspectralalgorithmsfor
communitydetectioninstochasticblockmodelgraphswithvertexcovariates. IEEETransactions
onNetworkScienceandEngineering9(5),3373–3384.
Newman,M.E.J.(2003). Thestructureandfunctionofcomplexnetworks. SIAMReview45(2),
167–256.
Pavlovic,D.,P.Vertes,E.Bullmore,W.Schafer,andT.Nicholas(2014). Stochasticblockmodeling
ofthemodulesandcoreofthecaenorhabditiselegansconnectome. PLoSONE9(9),e97584.
Priebe, C., Y. Parker, J. Vogelstein, J. Conroy, V. Lyzinskic, M. Tang, A. Athreya, J. Cape, and
E.Bridgeford(2019). Ona’twotruths’phenomenoninspectralgraphclustering. Proceedingsof
theNationalAcademyofSciences116(13),5995–5600.
Rohe,K.,S.Chatterjee,andB.Yu(2011). Spectralclusteringandthehigh-dimensionalstochastic
blockmodel. AnnalsofStatistics39(4),1878–1915.
Rossi,R.A.andN.K.Ahmed(2015). Thenetworkdatarepositorywithinteractivegraphanalytics
andvisualization. InAAAI.
Rozemberczki, B. and R. Sarkar (2020). Characteristic functions on graphs: Birds of a feather,
fromstatisticaldescriptorstoparametricmodels. InProceedingsofthe29thACMInternational
ConferenceonInformationandKnowledgeManagement(CIKM’20),pp.1325–1334.ACM.
Rubin-Delanchy,P.,J.Cape,M.Tang,andC.E.Priebe(2022). Astatisticalinterpretationofspectral
embedding: Thegeneralisedrandomdotproductgraph. JournaloftheRoyalStatisticalSociety
SeriesB:StatisticalMethodology84(4),1446–1473.
Shen,C.,J.Larson,H.Trinh,X.Qin,Y.Park,andC.E.Priebe(2024). Discoveringcommunication
patternshiftsinlarge-scalelabelednetworksusingencoderembeddingandvertexdynamics. 11(2),
2100–2109.
Shen, C., Y. Park, and C. E. Priebe (2023). Graph encoder ensemble for simultaneous vertex
embeddingandcommunitydetection. In20232ndInternationalConferenceonAlgorithms,Data
Mining,andInformationTechnology(ADMIT2023),Chengdu,China,September2023.ACM.
Shen, C., C. E. Priebe, J. Larson, and H. Trinh (2023). Synergistic graph fusion via encoder
embedding. https://arxiv.org/abs/2303.18051.
Shen,C.,M.Sun,M.Tang,andC.E.Priebe(2014). Generalizedcanonicalcorrelationanalysisfor
classification. JournalofMultivariateAnalysis130,310–322.
Shen,C.,Q.Wang,andC.E.Priebe(2023). One-hotgraphencoderembedding. IEEETransactions
onPatternAnalysisandMachineIntelligence45(6),7933–7938.
Snijders,T.andK.Nowicki(1997). Estimationandpredictionforstochasticblockmodelsforgraphs
withlatentblockstructure. JournalofClassification14(1),75–100.
Sussman,D.,M.Tang,D.Fishkind,andC.Priebe(2012). Aconsistentadjacencyspectralembedding
for stochastic blockmodel graphs. Journal of the American Statistical Association 107(499),
1119–1128.
11Sussman, D., M. Tang, and C. Priebe (2014). Consistent latent position estimation and vertex
classificationforrandomdotproductgraphs. IEEETransactionsonPatternAnalysisandMachine
Intelligence36(1),48–57.
Tang,M.,D.L.Sussman,andC.E.Priebe(2013). Universallyconsistentvertexclassificationfor
latentpositionsgraphs. AnnalsofStatistics41(3),1406–1430.
Ugander,J.,B.Karrer,L.Backstrom,andC.Marlow(2011). Theanatomyofthefacebooksocial
graph. arXivpreprintarXiv:1111.4503.
Varshney,L.,B.Chen,E.Paniagua,D.Hall,andD.Chklovskii(2011). Structuralpropertiesofthe
caenorhabditiselegansneuronalnetwork. PLoSComputationalBiology7(2),e1001066.
Wang, H. and J. Leskovec (2022). Combining graph convolutional neural networks and label
propagation. ACMTransactionsonInformationSystems40(4),1–27.
Wu,Z.,S.Pan,F.Chen,G.Long,C.Zhang,andP.S.Yu(2019). Acomprehensivesurveyongraph
neuralnetworks. IEEETransactionsonNeuralNetworksandLearningSystems32,4–24.
Xu,K.,W.Hu,J.Leskovec,andS.Jegelka(2019). Howpowerfularegraphneuralnetworks? In
Proc.ICLR,pp.1–17.
Yin,H.,A.R.Benson,J.Leskovec,andD.F.Gleich(2017). Localhigher-ordergraphclustering. In
Proceedingsofthe23rdACMSIGKDDInternationalConferenceonKnowledgeDiscoveryand
DataMining,pp.555–564.
Zachary,W.(1977). Aninformationflowmodelforconflictandfissioninsmallgroups. Journalof
AnthropologicalResearch33,452–473.
Zhao, Y., E. Levina, and J. Zhu (2012). Consistency of community detection in networks under
degree-correctedstochasticblockmodels. AnnalsofStatistics40(4),2266–2292.
Zheng, R., V. Lyzinski, C. E. Priebe, and M. Tang (2022). Vertex nomination between graphs
viaspectralembeddingandquadraticprogramming. JournalofComputationalandGraphical
Statistics31(4),1254–1268.
12Appendix
A TheoremProofs
Theorem 1. The graph encoder embedding is asymptotically normally distributed under SBM.
Specifically,asnincreases,foragivenithvertexofclassy,itholdsthat
diag(⃗n )0.5·(Z(i,:)−µ )→n N(0,Σ ).
K y y
Theexpectationandcovarianceare: µ =B(y,:)andΣ (k,k)=B(y,k)(1−B(y,k)). Assuming
y y
Σ isthesameacrossally ∈[1,K],thetransformationinEquation1satisfies
y
n
Z (i,k)→Prob(Y =k|X =Z(i,:)).
1
Proof. Thecentrallimittheoremfortheoriginalgraphencoderembeddingunderthestochasticblock
modelisprovedinShenetal.(2023)Theorem1. Soherewesimplyprovideabriefoverview.
First,anecessaryassumptionisthatasngoestoinfinity,sodoesn ;i.e.,asthenumberofvertices
k
goestoinfinity,thenumberofverticesperclassalsoincreasestoinfinity. Thisisastandardregularity
assumptioninpatternrecognitionbecause,withoutit,theclasswouldbecometrivialasnincreases.
UnderSBM,eachdimensionk =1,...,K ofthevertexembeddingsatisfies
Z(i,k)=A(i,:)W(:,k)
(cid:80)n
I(Y(j)=k)A(i,j)
= j=1
n
k
(cid:80)n
Bern(B(y,k))
j=1,j̸=i,Y(j)=k
= .
n
k
If k = y, the numerator is a summation of (n −1) i.i.d. Bernoulli random variables, since the
k
summationincludesadiagonalentryofA,whichisalways0. Otherwise,k ̸=yandthenumerator
isasummationofn i.i.d. Bernoullirandomvariables.
k
CheckingtheLyapunovconditionandapplyingthecentrallimittheorem,wehave
√
d
n (Z(i,k)−B(y,k))→N(0,B(y,k)(1−B(y,k))).
k
foreachdimensionk.
Note that Z(i,k) and Z(i,l) are always independent when k ̸= l. This is because every vertex
belongstoauniqueclass,sothesameBernoullirandomvariableneverappearsinanotherdimension.
Concatenatingeverydimensionyieldsthat
Diag(⃗n)0.5·(Z(i,:)−B(y,:))→d
N(0,Σ ).
y
Formoredetailedsteps,aswellascasesforotherrandomgraphmodels,pleaserefertoTheorem1in
Shenetal.(2023).
Now,givenZ(i,:)isnormallydistributedfornlarge,itfollowsimmediatelyfromclassicalpattern
recognition(Devroyeetal.,1996)thatunderthenormalityassumptionandacommonvarianceacross
allk,thelineartransformationinEquation1estimatestheconditionalprobability. Thisisbecausethe
LDAtransformationdirectlyestimatesProb(Y|X)whenX|Y isnormallydistributed. Specifically,
Z (i,k)=Z(i,k)Σ+µ −((µ′Σ+µ )−log(n /n))
1 k k k K
istheexactLDAtransformationforeachclassk =1,...,K. Writingitintoamatrixexpressionfor
allkleadstoEquation1.
Theorem2. Supposethegraphisdistributedasthestochasticblockmodelwithblockprobability
B∈RK×K andobservedlabelvectorY ∈[1,...,K]. Thenforanytwoverticesi,j,theencoder
embeddingZusingobservedlabelssatisfies:
n
∥Z(i,:)−Z(j,:)∥ −∥B(Y(i),:)−B(Y(j),:)∥ →0
2 2
13Suppose the same graph can be viewed as a realization of a latent stochastic block model with
B ∈RK′×K′ andalatentlabelvectorY ∈[1,...,K]whereK′. Theforthesametwovertices
0 0
i,j,theresultingencoderembeddingZ usingthelatentlabelssatisfies:
0
n
∥Z (i,:)−Z (j,:)∥ −∥B (Y (i),:)−B (Y (j),:)∥ →0
0 0 2 0 0 0 0 2
Proof. From Theorem 1, it is immediate that the encoder embedding satisfies the law of large
numbers,suchthat
n
∥Z(i,:)−B(Y(i),:)∥ →0.
2
Itfollowsthat
∥Z(i,:)−Z(j,:)∥ −∥(B(Y(i),:)−B(Y(j),:))∥
2 2
≤∥Z(i,:)−Z(j,:)−(B(Y(i),:)−B(Y(j),:))∥
2
=∥(Z(i,:)−B(Y(i),:))−(Z(j,:)−B(Y(j),:))∥
2
≤∥Z(i,:)−B(Y(i),:)∥ +∥Z(i,:)−B(Y(j),:)∥
2 2
→0.
Sincethegraphencoderembeddingisfullydependentonthegivenlabels,whenthelatentlabelsare
used,wealsohave
n
∥Z (i,:)−B(Y (i),:)∥ →0,
0 0 2
so the same derivation and convergence apply to the encoder embedding using latent labels as
well.
B RunningTimeAnalysis
TheoriginalGEEhasatimecomplexityofO(nK+s),wheresisthenumberofedges,makingit
linearwithrespecttothenumberofverticesandedges. LetK bethelargestpossiblenumberof
M
refinedclasses,therefinedGEEhasatimecomplexityofO(nK +nK2 +s),wherethequadratic
M M
term K2 comes from using linear discriminant. As K = γ K, or at most 5K in the default
M M K
parameter,themethodremainslinearwithrespecttothenumberofverticesandedges,thoughit
requiresmoreiterations.
Figure3showstherunningtimeusingsimulationmodel3withsparseadjacencymatrixinput,asn
increasesfrom3000to30000. Theaveragerunningtimeandonestandarddeviationarereported,
using10Monte-Carloreplicates. Itisclearthattherefinedmethod,althoughslowerthantheoriginal
encoderembedding,isstillvastlyfasterthansingularvaluedecomposition(SVD),whichisthemajor
computationalstepofspectralembedding. Atn=30000,thenumberofedgesisabout50million;a
singleSVDintod=20requiresabout200seconds,whilethegraphencoderembeddingtakes0.4
secondandtherefinedmethodtakes1.2second.
C RefinementVisualizationonRealData
Figure4illustratesthecommunityrefinementresultsfortworepresentativecases: thekarateclub
graphandthepoliticalblogsgraph.Theseexamplesclearlydemonstratehowtherefinementalgorithm
works. Itidentifiesverticesthataremisclassifiedintheencoderembeddingusingtheobservedlabels
and then assigns them to a new class. Visually, the method successfully detects useful hidden
communitieswithjustonerefinement.
Forthekarateclubgraph,thealgorithmidentifiesananomalyvertexthatalwaysconnectswiththe
othergroup,andanothervertexlocatedattheintersectionbetweenthetwoclasses. Forthepolitical
blogs,ouralgorithmidentifiesblogsthataredominantlyconnectedtotheotherparty. Whetherthese
are"swinger"blogsor"imposter"blogsisanissueofpracticalimportance.
D VertexClassificationonMultipleMatchedGraphs
Someoftherealdata,specificallytheC.elegansdataandtheWikipediadata,comewithmultiple
graphsofacommonvertexset. Thegraphencoderembeddingcanbedirectlyusedformultiple
14)
e 100
la
c
S
g 10
o
L
(
e 1
m
iT
g 0.1
n
in
n GEE
u0.01
R R-GEE
SVD (d=20)
0.5M 10M 50M
Approximate Number of Edges
Figure3: ThisfigureshowstherunningtimecomparisonbetweenGEE,RefinedGEE,andSVD.
TheX-axisrepresentstheapproximatenumberofedges,andtheY-axisrepresentstherunningtime
onalog-10scale.
R-GEE GEE U-ASE U-LSE N2v
C-ElegansAc+Ag 33.7±2.2 40.2±1.7 34.7±1.3 40.2±1.3 51.9±2.8
WikiTE+TF 14.6±0.5 18.0±0.3 20.7±0.3 21.1±0.3 n/a
WikiTE+GE 14.6±0.6 17.8±0.4 21.2±0.3 30.2±0.3 n/a
WikiTF+GF 15.7±0.5 18.7±0.3 21.1±0.3 31.3±0.4 n/a
WikiGE+GF 32.2±0.8 39.2±0.8 43.7±0.3 50.8±0.5 39.9±0.7
WikiTE+TF+GE+GF 13.3±0.5 16.1±0.3 18.0±0.3 27.9±0.4 n/a
Table2: Thistablereportsthevertexclassificationresultsformultiple-graphdatawithacommon
vertexset. Allnumbersareinpercentile.
graphinputs(Shenetal.,2023)byconcatenatingtheembeddings,ascantherefinedversion. LDA
classifiercanthenbeappliedtomeasurethequalityofthejointembeddingviavertexclassification.
Forthespectralembedding,weusetheunfoldedspectralembedding(Gallagheretal.,2021): Given
M graphsofmatchedcommonvertices,theunfoldedversionconcatenatesalladjacencymatrices
byrowsintoA ∈ Rn×Mn, andappliesSVDtoyieldZUASE = V S0.5 ∈ RMn×d, whereeach
d d
n×dmatrixistheembeddingforthecorrespondinggraph. Wethenreshapeandconcatenatethe
embeddingintoRn×Mdandcarryoutthevertexclassificationusinglineardiscriminantanalysis.
Fornode2vec,wesimplyapplynode2vectoeachgraph,concatenatetheirembeddings,andapply
LDA.EverythingelseisexactlythesameasinSection6, andTable2reportstheaveragevertex
classification error and the standard deviation for different combinations of the graph data. The
resultsareconsistentwiththoseinTable1,whererefinedGEEalwaysimprovesovertheoriginal
GEE,andisthebestperformerthroughoutallcombinations. Additionally,usingmultiplematched
graphsimprovesoversinglegraphresults.
15Karate Club Graph
Observed Community GEE-Refined Community
Political Blog Graph
Observed Community GEE-Refined Community
Figure4: Thisfigurevisualizestworealgraphs,KarateClubandPoliticalBlogs,usingobserved
labels(leftpanel)andGEE-refinedlabelswithonelabelrefinement(rightpanel).
16