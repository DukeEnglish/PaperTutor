[
    {
        "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
        "authors": "William BrandonMayank MishraAniruddha NrusimhaRameswar PandaJonathan Ragan Kelly",
        "links": "http://arxiv.org/abs/2405.12981v1",
        "entry_id": "http://arxiv.org/abs/2405.12981v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12981v1",
        "summary": "Key-value (KV) caching plays an essential role in accelerating decoding for\ntransformer-based autoregressive large language models (LLMs). However, the\namount of memory required to store the KV cache can become prohibitive at long\nsequence lengths and large batch sizes. Since the invention of the transformer,\ntwo of the most effective interventions discovered for reducing the size of the\nKV cache have been Multi-Query Attention (MQA) and its generalization,\nGrouped-Query Attention (GQA). MQA and GQA both modify the design of the\nattention block so that multiple query heads can share a single key/value head,\nreducing the number of distinct key/value heads by a large factor while only\nminimally degrading accuracy. In this paper, we show that it is possible to\ntake Multi-Query Attention a step further by also sharing key and value heads\nbetween adjacent layers, yielding a new attention design we call Cross-Layer\nAttention (CLA). With CLA, we find that it is possible to reduce the size of\nthe KV cache by another 2x while maintaining nearly the same accuracy as\nunmodified MQA. In experiments training 1B- and 3B-parameter models from\nscratch, we demonstrate that CLA provides a Pareto improvement over the\nmemory/accuracy tradeoffs which are possible with traditional MQA, enabling\ninference with longer sequence lengths and larger batch sizes than would\notherwise be possible",
        "updated": "2024-05-21 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12981v1"
    },
    {
        "title": "Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models",
        "authors": "Zhangyue YinQiushi SunQipeng GuoZhiyuan ZengXiaonan LiTianxiang SunCheng ChangQinyuan ChengDing WangXiaofeng MouXipeng QiuXuanJing Huang",
        "links": "http://arxiv.org/abs/2405.12939v1",
        "entry_id": "http://arxiv.org/abs/2405.12939v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12939v1",
        "summary": "Recent advancements in Chain-of-Thought prompting have facilitated\nsignificant breakthroughs for Large Language Models (LLMs) in complex reasoning\ntasks. Current research enhances the reasoning performance of LLMs by sampling\nmultiple reasoning chains and ensembling based on the answer frequency.\nHowever, this approach fails in scenarios where the correct answers are in the\nminority. We identify this as a primary factor constraining the reasoning\ncapabilities of LLMs, a limitation that cannot be resolved solely based on the\npredicted answers. To address this shortcoming, we introduce a hierarchical\nreasoning aggregation framework AoR (Aggregation of Reasoning), which selects\nanswers based on the evaluation of reasoning chains. Additionally, AoR\nincorporates dynamic sampling, adjusting the number of reasoning chains in\naccordance with the complexity of the task. Experimental results on a series of\ncomplex reasoning tasks show that AoR outperforms prominent ensemble methods.\nFurther analysis reveals that AoR not only adapts various LLMs but also\nachieves a superior performance ceiling when compared to current methods.",
        "updated": "2024-05-21 17:12:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12939v1"
    },
    {
        "title": "Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs",
        "authors": "Bilgehan SelPriya ShanmugasundaramMohammad KachueeKun ZhouRuoxi JiaMing Jin",
        "links": "http://arxiv.org/abs/2405.12933v1",
        "entry_id": "http://arxiv.org/abs/2405.12933v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12933v1",
        "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tasks such\nas summarization, arithmetic reasoning, and question answering. However, they\nencounter significant challenges in the domain of moral reasoning and ethical\ndecision-making, especially in complex scenarios with multiple stakeholders.\nThis paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing\nmoral reasoning in LLMs by exploring decisions' consequences from multiple\nstakeholder perspectives. Central to SKIG's mechanism is simulating\naccountability for actions, which, alongside empathy exercises and risk\nassessment, is pivotal to its effectiveness. We validate SKIG's performance\nacross various moral reasoning benchmarks with proprietary and opensource LLMs,\nand investigate its crucial components through extensive ablation analyses.",
        "updated": "2024-05-21 17:04:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12933v1"
    },
    {
        "title": "Code-mixed Sentiment and Hate-speech Prediction",
        "authors": "Anjali YadavTanya GargMatej KlemenMatej UlcarBasant AgarwalMarko Robnik Sikonja",
        "links": "http://arxiv.org/abs/2405.12929v1",
        "entry_id": "http://arxiv.org/abs/2405.12929v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12929v1",
        "summary": "Code-mixed discourse combines multiple languages in a single text. It is\ncommonly used in informal discourse in countries with several official\nlanguages, but also in many other countries in combination with English or\nneighboring languages. As recently large language models have dominated most\nnatural language processing tasks, we investigated their performance in\ncode-mixed settings for relevant tasks. We first created four new bilingual\npre-trained masked language models for English-Hindi and English-Slovene\nlanguages, specifically aimed to support informal language. Then we performed\nan evaluation of monolingual, bilingual, few-lingual, and massively\nmultilingual models on several languages, using two tasks that frequently\ncontain code-mixed text, in particular, sentiment analysis and offensive\nlanguage detection in social media texts. The results show that the most\nsuccessful classifiers are fine-tuned bilingual models and multilingual models,\nspecialized for social media texts, followed by non-specialized massively\nmultilingual and monolingual models, while huge generative models are not\ncompetitive. For our affective problems, the models mostly perform slightly\nbetter on code-mixed data compared to non-code-mixed data.",
        "updated": "2024-05-21 16:56:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12929v1"
    },
    {
        "title": "G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation",
        "authors": "Xingyuan PanLuyang HuangLiyan KangZhicheng LiuYu LuShanbo Cheng",
        "links": "http://arxiv.org/abs/2405.12915v1",
        "entry_id": "http://arxiv.org/abs/2405.12915v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12915v1",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities in\ngeneral scenarios. Instruction finetuning empowers them to align with humans in\nvarious tasks. Nevertheless, the Diversity and Quality of the instruction data\nremain two main challenges for instruction finetuning. With regard to this, in\nthis paper, we propose a novel gradient-based method to automatically select\nhigh-quality and diverse instruction finetuning data for machine translation.\nOur key innovation centers around analyzing how individual training examples\ninfluence the model during training. Specifically, we select training examples\nthat exert beneficial influences on the model as high-quality ones by means of\nInfluence Function plus a small high-quality seed dataset. Moreover, to enhance\nthe diversity of the training data we maximize the variety of influences they\nhave on the model by clustering on their gradients and resampling. Extensive\nexperiments on WMT22 and FLORES translation tasks demonstrate the superiority\nof our methods, and in-depth analysis further validates their effectiveness and\ngeneralization.",
        "updated": "2024-05-21 16:38:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12915v1"
    }
]