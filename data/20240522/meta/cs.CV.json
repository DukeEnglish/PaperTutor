[
    {
        "title": "OmniGlue: Generalizable Feature Matching with Foundation Model Guidance",
        "authors": "Hanwen JiangArjun KarpurBingyi CaoQixing HuangAndre Araujo",
        "links": "http://arxiv.org/abs/2405.12979v1",
        "entry_id": "http://arxiv.org/abs/2405.12979v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12979v1",
        "summary": "The image matching field has been witnessing a continuous emergence of novel\nlearnable feature matching techniques, with ever-improving performance on\nconventional benchmarks. However, our investigation shows that despite these\ngains, their potential for real-world applications is restricted by their\nlimited generalization capabilities to novel image domains. In this paper, we\nintroduce OmniGlue, the first learnable image matcher that is designed with\ngeneralization as a core principle. OmniGlue leverages broad knowledge from a\nvision foundation model to guide the feature matching process, boosting\ngeneralization to domains not seen at training time. Additionally, we propose a\nnovel keypoint position-guided attention mechanism which disentangles spatial\nand appearance information, leading to enhanced matching descriptors. We\nperform comprehensive experiments on a suite of $7$ datasets with varied image\ndomains, including scene-level, object-centric and aerial images. OmniGlue's\nnovel components lead to relative gains on unseen domains of $20.9\\%$ with\nrespect to a directly comparable reference model, while also outperforming the\nrecent LightGlue method by $9.5\\%$ relatively.Code and model can be found at\nhttps://hwjiang1510.github.io/OmniGlue",
        "updated": "2024-05-21 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12979v1"
    },
    {
        "title": "Personalized Residuals for Concept-Driven Text-to-Image Generation",
        "authors": "Cusuh HamMatthew FisherJames HaysNicholas KolkinYuchen LiuRichard ZhangTobias Hinz",
        "links": "http://arxiv.org/abs/2405.12978v1",
        "entry_id": "http://arxiv.org/abs/2405.12978v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12978v1",
        "summary": "We present personalized residuals and localized attention-guided sampling for\nefficient concept-driven generation using text-to-image diffusion models. Our\nmethod first represents concepts by freezing the weights of a pretrained\ntext-conditioned diffusion model and learning low-rank residuals for a small\nsubset of the model's layers. The residual-based approach then directly enables\napplication of our proposed sampling technique, which applies the learned\nresiduals only in areas where the concept is localized via cross-attention and\napplies the original diffusion weights in all other regions. Localized sampling\ntherefore combines the learned identity of the concept with the existing\ngenerative prior of the underlying diffusion model. We show that personalized\nresiduals effectively capture the identity of a concept in ~3 minutes on a\nsingle GPU without the use of regularization images and with fewer parameters\nthan previous models, and localized sampling allows using the original model as\nstrong prior for large parts of the image.",
        "updated": "2024-05-21 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12978v1"
    },
    {
        "title": "BiomedParse: a biomedical foundation model for image parsing of everything everywhere all at once",
        "authors": "Theodore ZhaoYu GuJianwei YangNaoto UsuyamaHo Hin LeeTristan NaumannJianfeng GaoAngela CrabtreeBrian PieningCarlo BifulcoMu WeiHoifung PoonSheng Wang",
        "links": "http://arxiv.org/abs/2405.12971v1",
        "entry_id": "http://arxiv.org/abs/2405.12971v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12971v1",
        "summary": "Biomedical image analysis is fundamental for biomedical discovery in cell\nbiology, pathology, radiology, and many other biomedical domains. Holistic\nimage analysis comprises interdependent subtasks such as segmentation,\ndetection, and recognition of relevant objects. Here, we propose BiomedParse, a\nbiomedical foundation model for imaging parsing that can jointly conduct\nsegmentation, detection, and recognition for 82 object types across 9 imaging\nmodalities. Through joint learning, we can improve accuracy for individual\ntasks and enable novel applications such as segmenting all relevant objects in\nan image through a text prompt, rather than requiring users to laboriously\nspecify the bounding box for each object. We leveraged readily available\nnatural-language labels or descriptions accompanying those datasets and use\nGPT-4 to harmonize the noisy, unstructured text information with established\nbiomedical object ontologies. We created a large dataset comprising over six\nmillion triples of image, segmentation mask, and textual description. On image\nsegmentation, we showed that BiomedParse is broadly applicable, outperforming\nstate-of-the-art methods on 102,855 test image-mask-label triples across 9\nimaging modalities (everything). On object detection, which aims to locate a\nspecific object of interest, BiomedParse again attained state-of-the-art\nperformance, especially on objects with irregular shapes (everywhere). On\nobject recognition, which aims to identify all objects in a given image along\nwith their semantic types, we showed that BiomedParse can simultaneously\nsegment and label all biomedical objects in an image (all at once). In summary,\nBiomedParse is an all-in-one tool for biomedical image analysis by jointly\nsolving segmentation, detection, and recognition for all major biomedical image\nmodalities, paving the path for efficient and accurate image-based biomedical\ndiscovery.",
        "updated": "2024-05-21 17:54:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12971v1"
    },
    {
        "title": "Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control",
        "authors": "Yue HanJunwei ZhuKeke HeXu ChenYanhao GeWei LiXiangtai LiJiangning ZhangChengjie WangYong Liu",
        "links": "http://arxiv.org/abs/2405.12970v1",
        "entry_id": "http://arxiv.org/abs/2405.12970v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12970v1",
        "summary": "Current face reenactment and swapping methods mainly rely on GAN frameworks,\nbut recent focus has shifted to pre-trained diffusion models for their superior\ngeneration capabilities. However, training these models is resource-intensive,\nand the results have not yet achieved satisfactory performance levels. To\naddress this issue, we introduce Face-Adapter, an efficient and effective\nadapter designed for high-precision and high-fidelity face editing for\npre-trained diffusion models. We observe that both face reenactment/swapping\ntasks essentially involve combinations of target structure, ID and attribute.\nWe aim to sufficiently decouple the control of these factors to achieve both\ntasks in one model. Specifically, our method contains: 1) A Spatial Condition\nGenerator that provides precise landmarks and background; 2) A Plug-and-play\nIdentity Encoder that transfers face embeddings to the text space by a\ntransformer decoder. 3) An Attribute Controller that integrates spatial\nconditions and detailed attributes. Face-Adapter achieves comparable or even\nsuperior performance in terms of motion control precision, ID retention\ncapability, and generation quality compared to fully fine-tuned face\nreenactment/swapping models. Additionally, Face-Adapter seamlessly integrates\nwith various StableDiffusion models.",
        "updated": "2024-05-21 17:50:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12970v1"
    },
    {
        "title": "Comprehensive Multimodal Deep Learning Survival Prediction Enabled by a Transformer Architecture: A Multicenter Study in Glioblastoma",
        "authors": "Ahmed GomaaYixing HuangAmr HagagCharlotte SchmitterDaniel HöflerThomas WeissmannKatharina BreiningerManuel SchmidtJenny StritzelbergerDaniel DelevRoland CorasArnd DörflerOliver SchnellBenjamin FreyUdo S. GaiplSabine SemrauChristoph BertRainer FietkauFlorian Putz",
        "links": "http://arxiv.org/abs/2405.12963v1",
        "entry_id": "http://arxiv.org/abs/2405.12963v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12963v1",
        "summary": "Background: This research aims to improve glioblastoma survival prediction by\nintegrating MR images, clinical and molecular-pathologic data in a\ntransformer-based deep learning model, addressing data heterogeneity and\nperformance generalizability. Method: We propose and evaluate a\ntransformer-based non-linear and non-proportional survival prediction model.\nThe model employs self-supervised learning techniques to effectively encode the\nhigh-dimensional MRI input for integration with non-imaging data using\ncross-attention. To demonstrate model generalizability, the model is assessed\nwith the time-dependent concordance index (Cdt) in two training setups using\nthree independent public test sets: UPenn-GBM, UCSF-PDGM, and RHUH-GBM, each\ncomprising 378, 366, and 36 cases, respectively. Results: The proposed\ntransformer model achieved promising performance for imaging as well as\nnon-imaging data, effectively integrating both modalities for enhanced\nperformance (UPenn-GBM test-set, imaging Cdt 0.645, multimodal Cdt 0.707) while\noutperforming state-of-the-art late-fusion 3D-CNN-based models. Consistent\nperformance was observed across the three independent multicenter test sets\nwith Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM,\nfirst external test set) and 0.618 (RHUH-GBM, second external test set). The\nmodel achieved significant discrimination between patients with favorable and\nunfavorable survival for all three datasets (logrank p 1.9\\times{10}^{-8},\n9.7\\times{10}^{-3}, and 1.2\\times{10}^{-2}). Conclusions: The proposed\ntransformer-based survival prediction model integrates complementary\ninformation from diverse input modalities, contributing to improved\nglioblastoma survival prediction compared to state-of-the-art methods.\nConsistent performance was observed across institutions supporting model\ngeneralizability.",
        "updated": "2024-05-21 17:44:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12963v1"
    }
]