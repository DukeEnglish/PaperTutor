[
    {
        "title": "Gender Representation and Bias in Indian Civil Service Mock Interviews",
        "authors": "Somonnoy BanerjeeSujan DuttaSoumyajit DattaAshiqur R. KhudaBukhsh",
        "links": "http://arxiv.org/abs/2409.12194v3",
        "entry_id": "http://arxiv.org/abs/2409.12194v3",
        "pdf_url": "http://arxiv.org/pdf/2409.12194v3",
        "summary": "This paper makes three key contributions. First, via a substantial corpus of\n51,278 interview questions sourced from 888 YouTube videos of mock interviews\nof Indian civil service candidates, we demonstrate stark gender bias in the\nbroad nature of questions asked to male and female candidates. Second, our\nexperiments with large language models show a strong presence of gender bias in\nexplanations provided by the LLMs on the gender inference task. Finally, we\npresent a novel dataset of 51,278 interview questions that can inform future\nsocial science studies.",
        "updated": "2024-09-20 04:38:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12194v3"
    },
    {
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
        "authors": "Peng WangShuai BaiSinan TanShijie WangZhihao FanJinze BaiKeqin ChenXuejing LiuJialin WangWenbin GeYang FanKai DangMengfei DuXuancheng RenRui MenDayiheng LiuChang ZhouJingren ZhouJunyang Lin",
        "links": "http://arxiv.org/abs/2409.12191v1",
        "entry_id": "http://arxiv.org/abs/2409.12191v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12191v1",
        "summary": "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL\nmodels that redefines the conventional predetermined-resolution approach in\nvisual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,\nwhich enables the model to dynamically process images of varying resolutions\ninto different numbers of visual tokens. This approach allows the model to\ngenerate more efficient and accurate visual representations, closely aligning\nwith human perceptual processes. The model also integrates Multimodal Rotary\nPosition Embedding (M-RoPE), facilitating the effective fusion of positional\ninformation across text, images, and videos. We employ a unified paradigm for\nprocessing both images and videos, enhancing the model's visual perception\ncapabilities. To explore the potential of large multimodal models, Qwen2-VL\ninvestigates the scaling laws for large vision-language models (LVLMs). By\nscaling both the model size-with versions at 2B, 8B, and 72B parameters-and the\namount of training data, the Qwen2-VL Series achieves highly competitive\nperformance. Notably, the Qwen2-VL-72B model achieves results comparable to\nleading models such as GPT-4o and Claude3.5-Sonnet across various multimodal\nbenchmarks, outperforming other generalist models. Code is available at\n\\url{https://github.com/QwenLM/Qwen2-VL}.",
        "updated": "2024-09-18 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12191v1"
    },
    {
        "title": "Qwen2.5-Coder Technical Report",
        "authors": "Binyuan HuiJian YangZeyu CuiJiaxi YangDayiheng LiuLei ZhangTianyu LiuJiajun ZhangBowen YuKai DangAn YangRui MenFei HuangXingzhang RenXuancheng RenJingren ZhouJunyang Lin",
        "links": "http://arxiv.org/abs/2409.12186v1",
        "entry_id": "http://arxiv.org/abs/2409.12186v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12186v1",
        "summary": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade\nfrom its predecessor, CodeQwen1.5. This series includes two models:\nQwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model,\nQwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained\non a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,\nscalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder\ndemonstrates impressive code generation capabilities while retaining general\nversatility. The model has been evaluated on a wide range of code-related\ntasks, achieving state-of-the-art (SOTA) performance across more than 10\nbenchmarks, including code generation, completion, reasoning, and repair,\nconsistently outperforming larger models of the same model size. We believe\nthat the release of the Qwen2.5-Coder series will not only push the boundaries\nof research in code intelligence but also, through its permissive licensing,\nencourage broader adoption by developers in real-world applications.",
        "updated": "2024-09-18 17:57:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12186v1"
    },
    {
        "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
        "authors": "Zayne SpragueFangcong YinJuan Diego RodriguezDongwei JiangManya WadhwaPrasann SinghalXinyu ZhaoXi YeKyle MahowaldGreg Durrett",
        "links": "http://arxiv.org/abs/2409.12183v1",
        "entry_id": "http://arxiv.org/abs/2409.12183v1",
        "pdf_url": "http://arxiv.org/pdf/2409.12183v1",
        "summary": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting\nreasoning capabilities from large language models (LLMs). But for what kinds of\ntasks is this extra ``thinking'' really helpful? To analyze this, we conducted\na quantitative meta-analysis covering over 100 papers using CoT and ran our own\nevaluations of 20 datasets across 14 models. Our results show that CoT gives\nstrong performance benefits primarily on tasks involving math or logic, with\nmuch smaller gains on other types of tasks. On MMLU, directly generating the\nanswer without CoT leads to almost identical accuracy as CoT unless the\nquestion or model's response contains an equals sign, indicating symbolic\noperations and reasoning. Following this finding, we analyze the behavior of\nCoT on these problems by separating planning and execution and comparing\nagainst tool-augmented LLMs. Much of CoT's gain comes from improving symbolic\nexecution, but it underperforms relative to using a symbolic solver. Our\nresults indicate that CoT can be applied selectively, maintaining performance\nwhile saving inference costs. Furthermore, they suggest a need to move beyond\nprompt-based CoT to new paradigms that better leverage intermediate computation\nacross the whole range of LLM applications.",
        "updated": "2024-09-18 17:55:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12183v1"
    },
    {
        "title": "A Controlled Study on Long Context Extension and Generalization in LLMs",
        "authors": "Yi LuJing Nathan YanSonglin YangJustin T. ChiuSiyu RenFei YuanWenting ZhaoZhiyong WuAlexander M. Rush",
        "links": "http://arxiv.org/abs/2409.12181v2",
        "entry_id": "http://arxiv.org/abs/2409.12181v2",
        "pdf_url": "http://arxiv.org/pdf/2409.12181v2",
        "summary": "Broad textual understanding and in-context learning require language models\nthat utilize full document contexts. Due to the implementation challenges\nassociated with directly training long-context models, many methods have been\nproposed for extending models to handle long contexts. However, owing to\ndifferences in data and model classes, it has been challenging to compare these\napproaches, leading to uncertainty as to how to evaluate long-context\nperformance and whether it differs from standard evaluation. We implement a\ncontrolled protocol for extension methods with a standardized evaluation,\nutilizing consistent base models and extension data. Our study yields several\ninsights into long-context behavior. First, we reaffirm the critical role of\nperplexity as a general-purpose performance indicator even in longer-context\ntasks. Second, we find that current approximate attention methods\nsystematically underperform across long-context tasks. Finally, we confirm that\nexact fine-tuning based methods are generally effective within the range of\ntheir extension, whereas extrapolation remains challenging. All codebases,\nmodels, and checkpoints will be made available open-source, promoting\ntransparency and facilitating further research in this critical area of AI\ndevelopment.",
        "updated": "2024-09-23 14:39:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.12181v2"
    }
]