1
Wav-KAN: Wavelet Kolmogorov-Arnold Networks
Zavareh Bozorgasl, Member, IEEE, Hao Chen, Member, IEEE
Abstract
In this paper 1, we introduce Wav-KAN, an innovative neural network architecture that leverages the Wavelet
Kolmogorov-Arnold Networks (Wav-KAN) framework to enhance interpretability and performance. Traditional
multilayer perceptrons (MLPs) and even recent advancements like Spl-KAN [1] face challenges related to
interpretability, training speed, robustness, computational efficiency, and performance. Wav-KAN addresses these
limitations by incorporating wavelet functions into the Kolmogorov-Arnold network structure, enabling the
network to capture both high-frequency and low-frequency components of the input data efficiently. Wavelet-
basedapproximationsemployorthogonalorsemi-orthogonalbasisandalsomaintainsabalancebetweenaccurately
representing the underlying data structure and avoiding overfitting to the noise. Analogous to how water conforms
to the shape of its container, Wav-KAN adapts to the data structure, resulting in enhanced accuracy, faster training
speeds,andincreasedrobustnesscomparedtoSpl-KANandMLPs.OurresultshighlightthepotentialofWav-KAN
as a powerful tool for developing interpretable and high-performance neural networks, with applications spanning
various fields. This work sets the stage for further exploration and implementation of Wav-KAN in frameworks
such as PyTorch, TensorFlow, and also it makes wavelet in KAN in wide-spread usage like nowadays activation
functions like ReLU, sigmoid in universal approximation theory (UAT).
Index Terms
Kolmogorov-Arnold Networks (KAN), Wavelet, Wav-KAN, Neural Networks.
I. INTRODUCTION
Advancements in artificial intelligence (AI) have led to the creation of highly proficient AI systems
that make decisions for reasons that are not clear to us. This has raised concerns about the widespread
deployment of untrustworthy AI systems in the economy and our daily lives, introducing several new
risks, including the potential for future AIs to deceive humans to achieve undesirable objectives [2], [3].
Z.Bozorgasl(zavarehbozorgasl@boisestate.edu)andH.Chen(haochen@boisestate.edu)arewiththeDepartmentofElectricalandComputer
Engineering, Boise State University, Boise, ID, 83712.
1Another title for the paper can be ”Wavelet for Everybody”.
4202
yaM
12
]GL.sc[
1v23821.5042:viXra2
The tendency to unravel the black-box behaviour of neural network has attracted a lot attention in recent
years.
Interpretability of neural networks is crucial as it influences trust in these systems and helps address
ethical concerns such as algorithmic discrimination. It is also essential for applying neural networks
in scientific fields like drug discovery and genomics, where understanding the model’s decisions is
necessary for validation and regulatory compliance [4], [5].
The multilayer feedforward perceptron (MLP) model is among the most widely used and practical neural
network models [6]. Despite their wide-spread usage, MLPs have serious drwabacks like consuming
almost all non-embedding parameters in transformers and also have less interpretability relative to
attention layers [7], [1], [8].
Model renovation approaches aim to enhance interpretability by incorporating more understandable
components into a network. These components can include neurons with specially designed activation
functions, additional layers with specific functionalities, modular architectures, and similar features [9].
Breaking down a neural network into individual neurons has a key challenge of polysemantic neurons,
i.e., each neuron activates for several unrelated types of feature [10]. [11] explores the reasons behind the
emergence of polysemanticity and propose that it occurs because of superposition, i.e., models learn more
distinct features than the available dimensions in a layer. Due to the limitation that a vector space can only
contain as many orthogonal vectors as it has dimensions, the network ends up learning an overcomplete
set of non-orthogonal features. For superposition to be beneficial, these features must activate sparsely;
otherwise, the interference between non-orthogonal features would negate any performance improvements
[8].
Kolmogorov-Arnold Networks (KANs) which stand on the Kolmogorov-Arnold representation theorem
[12] can bring a lot of advantages including more interpretability and accuracy [1]. KANs have univariate
learnable activation function on edges and nodes are summing those activation functions. The integration
of KAN, ensembles of probabilistic trees, and multivariate B-spline representations is presented in [13].
However, most of the previous works on KAN is for depth-2 representation; except, Liu et. al. [1] which
extends KANs to to arbitrary widths and depths. As [1] is based on B-Spline, we call it Spl-KAN.3
In this paper, we present an improved version of KAN, called Wav-KAN which uses wavelet in KAN
configuration. Figure 1 shows a Wav-KAN 2 with 2 input features, 3 hidden nodes, and 2 output nodes
(Wav-KAN[2,3,2]). In general, it can have an arbitrary number of layers. The structure is similar to MLPs
with weights replaced by wavelet functions, and nodes are doing summation of those Wavelet functions.
Fig. 1: Wav-KAN with arbitrary number of layers (here is Wav-KAN[2,3,2])
Wavelet has been extensively employed in multiresolution analysis [14]. There are some studies of
wavelet in neural networks in Universal Approximation Therory (UAT). For example, the complex Gabor
wavelet activation function is renowned for its optimal concentration in the space-frequency domain
and its exceptional ability to represent images accurately. Hence, [15] used Gabor wavelet in implicit
neural representation (INR) in application including image denoising, image inpainting, super-resolution,
computed tomography reconstruction, image overfitting, and novel view synthesis with neural radiance
fields. To the best of our knowledge, our proposed framework is the first work which uses wavelet in
Kolmogorov-Arnold representation theorem for an arbitrary widths and depth neural netwroks.
In comparison with the recent proposed Spl-KAN [1] and also MLPs, the proposed configuration is faster,
2Thesefancyplotsarejusttoshowthepotentialofwaveletforapproximation;theyarenotoutputofactivationfunctionsofarealnetwork;
hence, we see some of them are not differentiable.4
more accurate, and more robust, effectively addressing the existing issues and significantly enhancing its
performance.
Also, in statistical learning, we look for models which are more flexible and more interpretable [16]. Wav-
KANwillbeapowerfulmemberofafamilyofKANglass-boxmodelswhichfacilitatestheinterpretability
of neural networks. This work introduces Wav-KAN and we believe with the potential of Wav-KAN
which combines the potential of wavelet and KAN, it will be widely used in all the fields and it will be
implemented in Pytorch, Tensor-flow, R, to name but few.
Incorporating Wav-KAN in neural networks makes increasingly explainable components that can
achieve similar state-of-the-art performance across diverse tasks 3.
Section II discusses KAN and its generalization to multi-layers KAN with wavelet as function
approximation (i.e., activation function). Section III presents Continuous Wavelet Transform, especially the
definition and criteria for being a mother wavelet which can be used as basis for function approximation.
The comparison of Wav-KAN, Spl-KAN and MLPs is given in IV. Some experiment results will be given
in Section V. Finally, Section VI concludes the paper.
II. KOLMOGOROV-ARNOLD NETWORKS
Kolmogorov-Arnold Networks (KANs) represent a novel twist on neural network design that challenges
traditional concepts like the Multi-Layer Perceptron (MLP). At the heart of KANs is a beautiful and
somewhat abstract mathematical theorem from Kolmogorov and Arnold.
A. The Kolmogorov-Arnold Representation Theorem
Let’s start with the theorem that inspires KANs:
Theorem: Kolmogorov-Arnold Representation [12] For any continuous function f of n variables
defined on a cube [0,1]n, there exist 2n+1 functions ϕ and 2n+1×n functions ψ , all univariate and
q q,p
continuous, such that
(cid:32) (cid:33)
2n+1 n
(cid:88) (cid:88)
f(x ,...,x ) = Φ ϕ (x ) . (1)
1 n q q,p p
q=1 p=1
This theorem tells us that any multivariate function can essentially be decomposed into the sum of
functions of sums. The magic here is that these inner functions are univariate, meaning they each take a
3Some of the proofs and experiments will be presented in the final version of the paper5
single input. As we use mother wavelet as our basis, in Wav-KAn notation, we use ψ (x ) instead of
i,j j
ϕ (x ) and Ψ instead of Φ .
i,j j i i
B. From Theory to Networks
How do we translate this theorem into a neural network architecture? Imagine this: Instead of weights
and biases adjusting linear combinations of inputs at each node, KANs modify this process to work with
functions. One generalized version of KAN theorem that corresponds to deeper KANs is the recently
published work of Spl-KAN[1].
• In KANs, every ”weight” is actually a small function on its own. Each node in a KAN does not apply
a fixed non-linear activation function. Each learnable activation function in edges, gets the input and
gives an output.
Suppose we have an MLPs neural network with n input and m output in a fully connected layer
(between layer l and l+1). The equation in matrix form is given by:
x(l+1) = W x(l) +b(l+1) (2)
l+1,l
where:
• W l+1,l is the weight matrix connecting layer l and layer l+1,
• x(l) is the input vector,
• x(l+1) is the output vector,
• b(l+1) is the bias vector for layer l+1.
The weight matrix W is expanded as follows:
l+1,l
 
w w ··· w
1,1 1,2 1,n
 
 
w w ··· w 
2,1 2,2 2,n
W =   (3)
l+1,l 

. .
.
. .
.
... . .
.


 
 
w w ··· w
m,1 m,2 m,n
where w , i = 1,2,...,m and j = 1,2,...,n is the weight between i-th node in l+1-th layer, and j-th
i,j
node in l-th .The bias vector b(l+1) is:6
 
b(l+1)
1
 
 
b(l+1)

b(l+1) =  2  (4)
 . 
.
 . 
 
 
b(l+1)
m
Thus, the complete equation becomes:
      
x(l+1) w w ··· w x(l) b(l+1)
1 1,1 1,2 1,n 1 1
      
      
x(l+1)  w w ··· w x(l)  b(l+1) 
 2  =  2,1 2,2 2,n  2 + 2  (5)


. .
.




. .
.
. .
.
... . .
.
 

. .
.




. .
.


      
      
x(l+1) w w ··· w x(l) b(l+1)
m m,1 m,2 m,n n m
Now, suppose we have L layers, each of them having the structure described above. Let σ(·) be the
activation function. The compact formula for the whole network, f(x), where x is the input vector and
f(·) is the neural network, is given by:
f(x) = x(L) (6)
where
x(l+1) = σ(W x(l) +b(l+1)) (7)
l+1,l
for l = 0,1,2,...,L−1, and x(0) = x is the input vector.
f(x) = σ(W σ(W ···σ(W σ(W x+b )+b )···+b )+b ) (8)
L L−1 2 1 1 2 L−1 L
In KAN, the relationship between layers turns into: Let x(l) be a vector of size n. We transpose x(l)
and place it in a matrix X with m rows and n columns:
x(l) ∈ Rn ⇒ (x(l))T ∈ R1×n
We construct the matrix X as follows:
l7
 
(x(l))T
 
 
(x(l))T
X =   ∈ Rm×n
l  . 
.
 . 
 
 
(x(l))T
where each row of X is the transposed vector (x(l))T.
l
We define the operator T which acts on the matrix Ψ (X ). This operator sums the elements of
o l+1,l l
each row of the matrix and outputs the resulting vector v. The definition is as follows:
T (Ψ (X )) = v
o l+1,l l
where v is a vector with elements given by:
n
(cid:88) (cid:88)
v = [Ψ (X )] = ψ (x(l)), for i = 1,2,...,m
i l+1,l l ij i,j j
j j=1
In this expression, [Ψ (X )] represents the element in the i-th row and j-th column of the matrix
l+1,l l ij
Ψ (x(l)).
l+1,l
Thus, the operator T can be written as:
o
(cid:32) (cid:33)
(cid:88)
T (Ψ (X )) = [Ψ (X )]
o l+1,l l l+1,l l ij
j
i
In this definition, T takes the matrix Ψ (X ), sums the elements of each row, and outputs the
o l+1,l l
resulting vector v.
Indeed Ψ acts on the input vector x(l) and gives an output where each element of Ψ takes one
l+1,l l+1,l
corresponding element of x(l) as input, sums up the results, and produces one element of the output:
X = Ψ (X ) (9)
l+1 l+1,l l
where:8
 
ψ (x(l)) ψ (x(l)) ··· ψ (x(l))
1,1 1 1,2 2 1,n n
 
 
ψ (x(l)) ψ (x(l)) ··· ψ (x(l))
Ψ (X ) =  2,1 1 2,2 2 2,n n  (10)
l+1,l l 

. .
.
. .
.
... . .
.


 
 
ψ (x(l)) ψ (x(l)) ··· ψ (x(l))
m,1 1 m,2 2 m,n n
Here, Ψ represents the activation functions connecting layer l and layer l+1. Each element ψ (·)
l+1,l i,j
denotes the activation function that connects the j-th neuron in layer l to the i-th neuron in layer l +1.
Instead of multiplication, equation (9) computes a function of input with distinct learnable parameters.
Hence, if X be considered as input which just contains input vector as its rows, for the entire network,
0
the output after L layers is:
  
(T (Ψ (X )))T
o L−1,L−2 L−2
  
  
 (T (Ψ (X )))T
o L−1,L−2 L−2
f (X ) = x(L) = T (Ψ (X )) = T Ψ  
KAN 0 o L,L−1 L−1 o L,L−1 . 
.
  . 
  
  
(T (Ψ (X )))T
o L−1,L−2 L−2
  
(T (Ψ ···(T (Ψ (X ))))T
o L−1,L−2 o 1,0 0
  
  
 (T (Φ ···(T (Ψ (X ))))T
o L−1,L−2 o 1,0 0
= ··· = T Ψ   (11)
o L,L−1 . 
.
  . 
  
  
(T (Ψ ···(T (Ψ (X ))))T
o L−1,L−2 o 1,0 0
In summary, traditional MLPs use fixed nonlinear activation functions at each node and linear weights
(and biases) to transform inputs through layers. The output at each layer is computed by a linear
transformation followed by a fixed activation function. During backpropagation, gradients of the loss
function with respect to weights and biases are calculated to update the model parameters. In contrast,
KANs replace linear weights with learnable univariate functions placed on edges rather than nodes. In
the nodes, we just have a summation of some univariate functions from previous layers. Each function is
adaptable, allowing the network to learn both the activation and transformation of the inputs. This change
leads to improved accuracy and interpretability, as KANs can better approximate functions with fewer
parameters and avoid the curse of dimensionality. During backpropagation in KANs, the gradients are
computed with respect to these univariate functions, updating them to minimize the loss function. This9
results in more efficient learning for complex and high-dimensional functions.
C. Why Bother with KANs?
TheflexibilityofKANsallowsforamorenuancedunderstandingandadaptationtodata.Bylearningthe
functions directly involved in data relationships, KANs aim to provide a more accurate and interpretable
model:
• Accuracy: They can fit complex patterns in data more precisely with potentially fewer parameters.
• Interpretability: Since each function has a specific, understandable role, it’s easier to see what the
model is ”thinking.”
In summary, KANs leverage deep mathematical insights to offer a fresh perspective on how neural
networks can understand and interact with the world. By focusing on functions rather than mere weights,
they promise a richer and more intuitive form of machine learning.
III. CONTINUOUS WAVELET TRANSFORM
The Continuous Wavelet Transform (CWT) is a method mostly used in signal processing to analyze
the frequency content of a signal as it varies over time [14]. It acts like a microscope, zooming in on
different parts of a signal to determine its constituent frequencies and their variations.
CWT utilizes a base function known as a “mother wavelet,” which serves as a template that can be
scaled and shifted to match various parts of the signal. The shape of the mother wavelet is critical as it
dictates which features of the signal are highlighted.
Let ψ ∈ L2(R) be the mother wavelet and g(t) ∈ L2(R)4 be the function that we want to express in
the wavelet basis. Then, a mother wavelet must satisfy certain criteria [17], [18].
1) Zero Mean: The integral of the wavelet over its entire range must equal zero:
(cid:90) ∞
ψ(t)dt = 0 (12)
−∞
2) Admissibility Condition: The wavelet must have finite energy, which means the integral of the
square of the wavelet must be finite.
(cid:90) +∞ |ψˆ (ω)|2
C = dω < +∞ (13)
ψ
ω
0
4
(cid:26) (cid:90) (cid:27)
L2(R)= f(x)| |f(x)|2dx<∞
.10
ˆ
where ψ is the Fourier transform of the wavelet ψ(t).
The CWT of a signal/function is represented by wavelet coefficients, calculated as follows:
(cid:90) +∞ 1 (cid:18) t−τ(cid:19)
C(s,τ) = g(t)√ ψ dt (14)
s s
−∞
where:
• g(t) is the signal/function that we want to approximate by Wavelet basis.
• ψ(t) is the mother wavelet.
• s ∈ R+ is the scale factor which is greater than zero.
• τ ∈ R is the shift factor.
• C(s,τ) measures the match between the wavelet and the signal at scale s and shift τ.
A signal can be reconstructed from its wavelet coefficients using the inverse CWT:
1 (cid:90) +∞(cid:90) +∞ 1 (cid:18) t−τ(cid:19) dsdτ
g(t) = C(s,τ)√ ψ (15)
C s s s2
ψ −∞ 0
where C is a constant that depends on the wavelet, ensuring the reconstruction’s accuracy.
ψ
IV. WAV-KAN OR SPL-KAN OR MLPS?
Wavelets and B-splines are two prominent methods used for function approximation, each with distinct
advantages and limitations, particularly when applied in neural networks. B-splines provide smooth and
flexible function approximations through piecewise polynomial functions defined over control points.
They offer local control, meaning adjustments to a control point affect only a specific region, which
is advantageous for precise function tuning. This smoothness and local adaptability make B-splines
suitable for applications requiring continuous and refined approximations, such as in CAD and computer
graphics. However, the computational complexity increases significantly with higher dimensions, making
them less practical for high-dimensional data. Managing knot placement can also be intricate and affect
the overall shape and accuracy of the approximation. While B-splines can be used in neural networks to
approximate activation functions or smooth decision boundaries, their application is generally less suited
to feature extraction tasks compared to wavelets due to their limited ability to handle multi-resolution
analysis and sparse representations.11
Wavelets excel in multi-resolution analysis enabling different levels of detail to be represented
simultaneously which makes it a precious tool to decompose data into various frequency components.
This capability is highly beneficial for feature extraction in neural networks, as it enables capturing of
both high-frequency details and low-frequency trends. Additionally, wavelets offer sparse representations,
which can lead to more efficient neural network architectures and faster training times. They are also
well-suited for handling non-stationary signals and localized features, making them ideal for applications
such as image recognition and signal classification. However, the choice of the wavelet function is crucial
and can significantly impact performance, and edge effects can introduce artifacts that need special
handling.
In function approximation, wavelets excel by maintaining a delicate balance between accurately
representing the underlying data structure and avoiding overfitting to the noise. Unlike traditional methods
that may overly smooth the data or fit to noise, wavelets achieve this balance through their inherent ability
to capture both local and global features. By decomposing the data into different frequency components,
wavelets can isolate and retain significant patterns while discarding irrelevant noise. This multiresolution
analysis ensures that the approximation is robust and reliable, providing a more accurate and nuanced
representation of the original data without the pitfalls of noise overfitting. On the other side, while Spl-
KAN is powerful in capturing the changes in data, it also captures the noise in training data. Indeed, the
strength of Spl-KAN is its weakness, too.
The advantages of Spl-KAN which are mentioned in [1] including interpretability and/or accuracy with
respect to MLPs exist for Wav-KAN. More importantly Wav-KAN has solved the major disadvantage of
Spl-KANs which was slow training speed. In terms of number of parameter, we compared Wav-KAN
with Spl-KAN and MLPs for a hypothetical neural network which has N input nodes and N output node,
with L layers. As we see in Table I and by considering the value of G, Wav-KAN has less number of
parameters than Spl-KAN (k should be at least 2 to have good results in Spl-KAN, especially in complex
tasks).Thecoefficient3isbecuaseWav-KANhasalearnableweight,atranslationandascaling.Learnable
parameters in each neural network is in column of parameters. While the order of MLPs are less for the
hypothetical neural network, in practice, Wav-KAN needs less number of parameters to learn the same
task. Indeed, this originates because of capacity of Wavelet for capturing both low frequency and high
frequency functions.12
TABLE I: Comparison of MLPs, Spl-KAN and Wav-KAN
Neural network With L layers, each layer has N nodes
Neural Network Structure Order Parameters
MLPs O(N2L) or O(N2L+NL) weights and biases
Spl-KAN O(N2L(G+k +1)) ∼ O(N2LG) weights
Wav-KAN O(3N2L) weight, translation, scaling
Regarding the implementation, Spl-KAN requires a smooth function like b(x) in the equation (2.10)
[1] attempting to catch on some global features. Because of the inherent scaling property of Wavelet,
Wav-KAN does not need an additional term in its activation functions. This helps Wav-KAN to be faster.
For example, wavelets equal to the second derivative of a Gaussian which are called Mexican hats 5 and
first used in computer vision to detect multiscale edges [19], [14] has the following form
2 (cid:18) t2 (cid:19) (cid:18) −t2(cid:19)
ψ(t) = √ −1 exp . (16)
π1/4 3σ σ2 2σ2
Where σ shows the adjustable standard deviation of Gaussian. In our experiments, ψ (t)
exp
ψ (t) = wψ(t) (17)
exp
Indeed, w plays the role of CWT coefficients which is multiplied by the mother wavelet formula; as w
is a learnable parameter, it helps adapting the shape of the mother wavelet to the function that it tries to
approximate .
Moreover, Spl-KAN heavily depends on the grid spaces, and for better performance, it requires
increasing the number of grids 6, though, it brings two disadvantages. First it needs curvefitting which is
a cumbersome and computational expensive operation, and also while we increase the number of grids,
loss has some jumps 7. Fortunately, wavelet is safe from such computations and deficiencies.
Last but no least, we found that batch normalization [20] significantly improves accuracy and speeds
up training of both Wav-KAN and Spl-KAN; hence, we included batch normalization in both of these
methods 8.
5ThefollowingequationisnormalizedMexicanhatwavelet.Indeed,pywaveletshasaminussignbehinditwhichbecauseoftheweightwe
put behind the mother wavelet in our activation functinos, the sign doesn’t matter(https://pywavelets.readthedocs.io/en/latest/ref/cwt.html).
6Which is equivalent to decrease the spaces between the grids. This helps Spl-KAN accuracy to be improved
7Loss will be increased for some training, then, it works better. See Figure 2.3 [1] for better illustration.
8In [1], the authors did not mention and didn’t apply batch normalization.13
V. SIMULATION RESULTS
In this section, we present the results of our experiments conducted using the KAN (Kernel-based
Artificial Neural network) model with various wavelet transformations on the MNIST dataset, utilizing a
training set of 60,000 images and a test set of 10,000 images. It is important to note that our objective
was not to optimize the parameters to their best possible values, but rather to demonstrate that Wav-KAN
performs well in terms of overall performance. We have incorporated batch normalization into both
Spl-KAN and Wav-KAN, resulting in improved performance. The wavelet types considered in our study
include Mexican hat, Morlet, Derivative of Gaussian (DOG), and Shannon (see Table II). For each
wavelet type and also Spl-KAN, we performed five trials, training the model for 50 epochs per trial.
TABLE II: Mother Wavelet Formulas and Parameters
Wavelet Type Formula of Mother Wavelet Parameters
Mexican hat ψ(t) = √ 2 (t2 −1)e−t 22 τ, s
3π1/4
Morlet ψ(t) = cos(ω
t)e−t2
τ, s and scaling, ω = 5
0 2 0
(cid:16) (cid:17)
Derivative of Gaussian (DOG) ψ(t) = −d
e−t2
τ, s
2
dt
Shannon ψ(t) = sinc(t/π)·w(t) τ, s, and w(t): window function
The results were averaged across these trials to ensure the robustness and reliability of our findings.
Both Wav-KAN and Spl-KAN have the structure (number of nodes) of [28*28,32,10] 9. Although we
enhanced Spl-KAN by using spline order 3 and a grid size of 5, this approach is computationally much
more expensive compared to Wav-KAN. We employed AdamW optimizer [21], [22], with learning rate
of 0.001 with weight decay of 10−4. Loss is cross entropy.
Figures 2 and 3 show the result of training accuracy and test accuracy of Wav-KAN in comparison to
Spl-KAN. To not clutter up, we just show the result of Derivative of Gaussian (DOG) and Mexican hat
wavelet. These have been shown as a sample. By fine tuning, and using all the freedom of wavelet (like
frequency of sinusoid and variance of Gaussian), Wavelet shows significant superiority 10. While Spl-KAN
has better performance in training, which is because of overfitting to data, a lot of wavelet types have
shown superior performance with respect to Spl-KAN. Indeed, for this experiment we set the variance
of the Gaussian to be 1; though, we can find better variances by grid search or making it a learnable
parameter. Wavelet makes a balance by not fitting to the noise in the data.
9[first layer nodes, middle layer nodes, output nodes]
10We have done a lot of such experiments and we will publish them soon.14
Fig. 2: Training accuracy of Wav-KAN [28*28,32,10] versus Spl-KAN [28*28,32,10]
Weevaluatedtheperformanceofeachwavelettypeintermsoftrainingloss,trainingaccuracy,validation
loss, and validation accuracy. Figures 1 and 2 summarize the results, depicting the training and validation
metrics averaged over the five trials for each wavelet type.
ThesimulationresultsindicatethatthechoiceofwaveletsignificantlyimpactstheperformanceoftheKAN
model. This suggests that these wavelets are particularly effective at capturing the essential features of
the MNIST dataset while maintaining robustness against noise. On the other hand, wavelets like Shannon
and Bump did not perform as well, highlighting the importance of wavelet selection in designing neural
networks with wavelet transformations.
VI. CONCLUSION
In this paper, we have introduced Wav-KAN, a novel neural network architecture that integrates wavelet
functions within the Kolmogorov-Arnold Networks (KAN) framework to enhance both interpretability and
performance. By leveraging the multiresolution analysis capabilities of wavelets, Wav-KAN effectively
captures complex data patterns and provides a robust solution to the limitations faced by traditional
multilayer perceptrons (MLPs) and recently proposed Spl-KANs.15
Fig. 3: Test accuracy of Wav-KAN [28*28,32,10] versus Spl-KAN [28*28,32,10]
Our experimental results demonstrate that Wav-KAN not only achieves superior accuracy but also
benefits from faster training speeds compared to Spl-KAN. The unique structure of Wav-KAN, which
combines the strengths of wavelet transforms and the Kolmogorov-Arnold representation theorem, allows
for more efficient parameter usage and improved model interpretability.
Wav-KAN represents a significant advancement in the design of interpretable neural networks. Its ability
to handle high-dimensional data and provide clear insights into model behavior makes it a promising tool
for a wide range of applications, from scientific research to industrial deployment. Future work will focus
on further optimizing the Wav-KAN architecture, exploring its applicability to other datasets and tasks,
and implementing the framework in popular machine learning libraries such as PyTorch and TensorFlow.
Overall, Wav-KAN stands out as a powerful and versatile model, paving the way for the development
of more transparent and efficient neural network architectures. Its potential to combine high performance
with interpretability marks a crucial step forward in the field of artificial intelligence.
REFERENCES
[1] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljacˇic´, T. Y. Hou, and M. Tegmark, “Kan: Kolmogorov-arnold networks,”
arXiv preprint arXiv:2404.19756, 2024.16
[2] D. Hendrycks, M. Mazeika, and T. Woodside, “An overview of catastrophic ai risks,” arXiv preprint arXiv:2306.12001, 2023.
[3] R. Ngo, L. Chan, and S. Mindermann, “The alignment problem from a deep learning perspective,” arXiv preprint arXiv:2209.00626,
2022.
[4] Y. Zhang, P. Tinˇo, A. Leonardis, and K. Tang, “A survey on neural network interpretability,” IEEE Transactions on Emerging Topics
in Computational Intelligence, vol. 5, no. 5, pp. 726–742, 2021.
[5] F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable machine learning,” arXiv preprint arXiv:1702.08608, 2017.
[6] A. Pinkus, “Approximation theory of the mlp model in neural networks,” Acta numerica, vol. 8, pp. 143–195, 1999.
[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems, vol. 30, 2017.
[8] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey, “Sparse autoencoders find highly interpretable features in language
models,” arXiv preprint arXiv:2309.08600, 2023.
[9] F.-L.Fan,J.Xiong,M.Li,andG.Wang,“Oninterpretabilityofartificialneuralnetworks:Asurvey,”IEEETransactionsonRadiation
and Plasma Medical Sciences, vol. 5, no. 6, pp. 741–760, 2021.
[10] C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter, “Zoom in: An introduction to circuits,” Distill, vol. 5, no. 3,
pp. e00024–001, 2020.
[11] N. Elhage, R. Lasenby, and C. Olah, “Privileged bases in the transformer residual stream, 2023,” URL https://transformer-circuits.
pub/2023/privilegedbasis/index. html. Accessed, pp. 08–07, 2023.
[12] A. N. Kolmogorov, On the representation of continuous functions of several variables by superpositions of continuous functions of a
smaller number of variables. American Mathematical Society, 1961.
[13] D.Fakhoury,E.Fakhoury,andH.Speleers,“Exsplinet:Aninterpretableandexpressivespline-basedneuralnetwork,”NeuralNetworks,
vol. 152, pp. 332–346, 2022.
[14] S. Mallat, A wavelet tour of signal processing. Elsevier, 1999.
[15] V. Saragadam, D. LeJeune, J. Tan, G. Balakrishnan, A. Veeraraghavan, and R. G. Baraniuk, “Wire: Wavelet implicit neural
representations,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 18507–18516.
[16] G. James, D. Witten, T. Hastie, R. Tibshirani, and J. Taylor, An introduction to statistical learning: With applications in python.
Springer Nature, 2023.
[17] A. Caldero´n, “Intermediate spaces and interpolation, the complex method,” Studia Mathematica, vol. 24, no. 2, pp. 113–190, 1964.
[18] A. Grossmann and J. Morlet, “Decomposition of hardy functions into square integrable wavelets of constant shape,” SIAM journal on
mathematical analysis, vol. 15, no. 4, pp. 723–736, 1984.
[19] A. P. Witkin, “Scale-space filtering,” in Readings in computer vision. Elsevier, 1987, pp. 329–332.
[20] S.IoffeandC.Szegedy,“Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift,”inInternational
conference on machine learning. pmlr, 2015, pp. 448–456.
[21] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.
[22] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” arXiv preprint arXiv:1711.05101, 2017.