LLM Processes: Numerical Predictive Distributions
Conditioned on Natural Language
JamesRequeima∗ JohnBronskill∗ DamiChoi
UniversityofToronto UniversityofCambridge UniversityofToronto
requeima@cs.toronto.edu jfb54@cam.ac.uk choidami@cs.toronto.edu
RichardE.Turner DavidDuvenaud
UniversityofCambridge UniversityofToronto
ret26@cam.ac.uk duvenaud@cs.toronto.edu
Abstract
Machine learning practitioners often face significant challenges in formally in-
tegrating their prior knowledge and beliefs into predictive models, limiting the
potentialfornuancedandcontext-awareanalyses. Moreover,theexpertiseneeded
tointegratethispriorknowledgeintoprobabilisticmodelingtypicallylimitsthe
applicationofthesemodelstospecialists. Ourgoalistobuildaregressionmodel
that can process numerical data and make probabilistic predictions at arbitrary
locations,guidedbynaturallanguagetextwhichdescribesauser’spriorknowl-
edge. LargeLanguageModels(LLMs)provideausefulstartingpointfordesigning
suchatoolsincethey1)provideaninterfacewhereuserscanincorporateexpert
insightsinnaturallanguageand2)provideanopportunityforleveraginglatent
problem-relevantknowledgeencodedinLLMsthatusersmaynothavethemselves.
Westartbyexploringstrategiesforelicitingexplicit,coherentnumericalpredic-
tive distributions from LLMs. We examine these joint predictive distributions,
whichwecallLLMProcesses,overarbitrarily-manyquantitiesinsettingssuch
asforecasting,multi-dimensionalregression,black-boxoptimization,andimage
modeling. We investigate the practical details of prompting to elicit coherent
predictivedistributions,anddemonstratetheireffectivenessatregression. Finally,
wedemonstratetheabilitytousefullyincorporatetextintonumericalpredictions,
improvingpredictiveperformanceandgivingquantitativestructurethatreflects
qualitativedescriptions. Thisletsusbegintoexploretherich,groundedhypothesis
spacethatLLMsimplicitlyencode.
1 Introduction
Incorporatingpriorknowledgeintopredictivemodelsishighlychallengingwhichcanrestrictthe
scopefordetailed,context-sensitiveanalysis. Inaddition,theskillrequiredtoincorporatethisprior
knowledgeintoprobabilisticmodellingcanrestricttheuseofthesemodelstoexperts. Inthiswork,
ourobjectiveistodevelopaprobabilisticpredictionmodelthatfacilitatesuserinteractionthrough
straightforward, natural language. For this purpose, we explore strategies for eliciting explicit,
coherentnumericalpredictivedistributionsfromLLMs.
Whygotosomuchefforttoelicitpredictionsfromaslow,expensive,andsometimesinconsistent
modellikeanLLM?Weexpecttheirhypothesisclasstobebothrich,andgroundedinexactlythe
kinds of high-levelside information that we currently struggle to communicateto our numerical
models. Forinstance,knowingthatpricesrarelygobelowzero,thatcertainkindsofsensorscan
∗Equalcontribution.
4202
yaM
12
]LM.tats[
1v65821.5042:viXraData Text Information Data Text Information Data Text Information
+ + “…the companygoes +
“…a financial “…on day 30.”
out of business.”
time series.”
LLMP LLMP LLMP
Figure1: PredictivedistributionsfromanLLMPconditionedonbothdataandtextinformation. The
tenth-percentilesfrom50samplesarevisualizedinfadedblueandthemedianispresentedindark
bluewithfiverandomsamplesshowninvariouscolours.
saturateatparticularvalues,orthattrendsalmostalwayseventuallyleveloff,areeasytoexpress
in natural language, but not straightforward to incorporate into a model without getting lost in
difficult-to-specifydetailsaboutaspectsofthedomainthataren’twellunderstood. Tosummarize,
wewanttodevelopsuchamodelbecauseitwouldallowusersto1)provideprior,potentiallyexpert,
informationtothemodelabouttheproblemsettinginplain-languageratherthanattemptingtocapture
thisinformationinclosedformpriors(e.g.GaussianProcesskernels)and2)itwouldallowusersto
accessproblem-relevantlatentknowledgeencodedinLLMsthatusersmaynothavethemselves.
LLMshaverecentlybeenshowntobeabletoconditionontheparticulartaskbeingsolved,leveraging
contextualinformationtomakebetterpredictionsordecisions[1,2]. Theyhavealsobeenshownto
competitivelypredicttimeseriesbasedonlyonatexttokenizationofnumericaldata[3]. Inthiswork,
wefurtherpushinboththesedirections;1)usingLLMsfornumericalpredictiontasksgoingbeyond
one-dimensionaltimeseriesforecastingtomulti-dimensionalregressionanddensityestimationand
2)exploringtheabilityofthesemodelstoconditiononbothnumericaldataandrich,unstructured
texttoimprovethesepredictions. Inthispaperwemakethefollowingcontributions:
• WedefineLLMProcesses(LLMPs)usingmethodswedevelopforelicitingnumericalpre-
dictivedistributionsfromLLMs. LLMPsgobeyondone-dimensionaltimeseriesforecastingto
multi-dimensionalregressionanddensityestimation. Weproposetwoapproachesfordefiningthis
jointpredictivedistributionoveracollectionofquerypointsandevaluatetheircompatibilityin
principlewiththeconsistencyaxiomsnecessarytospecifyavalidstatisticalprocess.
• Wedevelopeffectivepromptingpracticesforelicitingjointnumericalpredictions. Weinves-
tigatevariousmethodsforconditioningLLMsonnumericaldata,includingpromptformatting,
ordering,andscaling. Wecharacterizewhichschemesperformbestonasetofsynthetictasks.
• WeshowthatLLMPsarecompetitiveandflexibleregressorsevenonmessydata. Throughan
extensivesetofsyntheticandrealworldexperiments,includingimagereconstructionandblack-box
functionoptimization,weevaluatethezero-shotregressionandforecastingperformanceofLLMPs.
WedemonstratethatLLMPshavewell-calibrateduncertaintyandarecompetitivewithGaussian
Processes(GPs),LLMTime[3],andOptuna[4]. WeshowthatLLMPsusein-contextlearningto
automaticallyleverageinformationfromrelateddatasets,caneasilyhandlemissingdatapoints,
performimagereconstruction,andoutputmultimodalpredictivedistributions.
• Lastly,wedemonstratetheabilitytousefullyincorporateproblem-relevantinformationpro-
videdthroughunstructuredtextintonumericalpredictions,visualizedinFigure1,resultingin
quantitativestructurethatreflectsqualitativedescriptions. Otheradditionssuchaslabellingfeatures
usingtextandspecifyingunitsallowLLMPstomakeuseofusually-ignoredsideinformation.
2 LLMProcesses: DefiningaStochasticProcessThatCanConditiononText
Our goal for this section is to use an LLM to elicit joint predictive distributions over arbitrary
sized target sets that we can guide and modify using natural language. Formally, given a set of
input and output observations D = {(x ,y )}M and some text, T, we would like to elicit
train i i i=1
2𝑦
𝑇: Problem Related Text
⟨𝑡⟩: Terminal Token 𝑁 samples at 𝑥 ,𝑦
𝑥 2,𝑦 2 Independent each target point 2 2
Marginal Prompts
𝑥 1,𝑦 1 𝑥 3,𝑦 3 “ “𝑇 𝑇⟨ ⟨𝑡 𝑡⟩ ⟩𝑥 𝑥1 1, ,𝑦 𝑦1 1⟨ ⟨𝑡 𝑡⟩ ⟩⋯ 𝑥𝑥 22 ,, 𝑦𝑦 22 ⟨⟨ 𝑡𝑡 ⟩⟩ 𝑥𝑥 33 ,, 𝑦𝑦 33 ⟨⟨ 𝑡𝑡 ⟩⟩ 𝑥𝑥 𝑛∗0∗ ”” LLM “ “𝑦 𝑦⋯0∗ 𝑛∗” ” 𝑥 1,𝑦 1 𝑥 3,𝑦 3
𝑥 Autoregressive
𝑥 0∗ ⋯ 𝑥 𝑛∗ Prompts Autoregressive: Median
Observed Point
“𝑇⟨𝑡⟩𝑥1,𝑦1⟨𝑡⟩𝑥2 ⋯,𝑦2⟨𝑡⟩𝑥3,𝑦3⟨𝑡⟩𝑥 0∗” oA up tp pe un td
t
op r pe rv oi mou ps
t Uncertainty
Target Locations “𝑇⟨𝑡⟩𝑥1,𝑦1⟨𝑡⟩𝑥2,𝑦2⟨𝑡⟩𝑥3,𝑦3⟨𝑡⟩𝑥 0∗,y 0∗⟨𝑡⟩… 𝑥𝑛∗𝑦𝑛∗⟨𝑡⟩”
Figure2: SamplingfromanLLMusingeitherindependentmarginalorautoregressivesampling.
the predictive distribution defined by an LLM at a collection of targets {(x∗,y∗)}N denoted
j j j=1
p (y∗,...,y∗ |x∗,...,x∗ ,D ,T).
LLM 1 N 1 N train
RejectionsamplingfromanLLMallowsustoaccesswhatwemayinterpretastheLLM’spredictive
distributionandgaininsightsintothemodel’sinductivebiases;samplingfromtheLLM’scategorical
distributionovertexttokenswhileignoringnon-numericaltokensyieldsnumericalsamplesfrom
theLLM.TheprocessofsamplingfromanLLMisdepictedinFigure2andAlgorithm1. Sample
promptsareinAppendixC.Sinceanaccuratesampling-basedempiricaldistributionincursahigh
computationalcost,nextwedefineanapproachtoelicitcontinuouslikelihoodsfromanLLM.
ContinuousMarginalLikelihoodsFromanLLM.Weapproximateacontinuousdensityoverour
targetvaluesbydiscretizingthespaceusingbinswitharbitrarilyfineprecision,similartothemethod
usedinGruveretal.[3]. Crucially,thishierarchicalapproachallowsustocomputetheprobability
of a bin with width 10−n. For example, if n = 1 then Pr{y ∈ [1.0,1.1)} = p(1)p(.|1)p(0|1.)
because‘1.0’isaprefixforally ∈[1.0,1.1). Wecanconvertprobabilitymasstoprobabilitydensity
by assuming a uniform distribution within each bin, and dividing the mass by the bin width. A
visualizationofthisconstructionisinFiguresH.2toH.4.
Unlike[3],wedonotrescalethevaluestoremovedecimalplaces. Wehypothesizethatsuchscaling
removespriorinformationcommunicatedtotheLLMviathescaleoftheproblem. Weexaminethe
effectofscalingvaluesinSection3. Wealsodifferfrom[3]byincludingaterminaltokenafterevery
valueinourprompt–forexample,givenaterminaltoken⟨t⟩,werepresent12as12⟨t⟩. Including
aterminaltokenpreventsnumbersofvaryingordersofmagnitudetosharethe same prefix–i.e.
p(1)p(2|1)p(⟨t⟩|12)nolongerincludestheprobabilityofnumbersin[120,130),[1200,1300),etc.
NotethatthisapproachdoesnotguaranteethatP(12⟨t⟩)yieldsthemassassignedbytheLLMto
valuesinthebin[12,13)butweempiricallyobservedthatourpredictivedistributioncloselymatches
thesamplingdistributiontooursatisfaction. SeeSectionH.1formoredetailsandcomparison.
Defining an LLM Process. Thus far we have established a procedure defining the predictive
distributionatasingletargetlocation,p (y∗ |x∗,D ,T). Wenowoutlinetwomethodswhich
LLM n n train
wecallindependentmarginal(I-LLMP)andautoregressive(A-LLMP)predictions,fordefiningthe
jointpredictivedistributionoveracollectionoftargetpoints:
N
(cid:89)
p (y∗,...,y∗ |x∗,...,x∗ ,D ,T)= p (y∗,|x∗,D ,T) (1)
I-LLMP 1 N 1 N train LLM n n train
n=1
N
(cid:89)
p (y∗,...,y∗ |x∗,...,x∗ ,D ,T)= p (y∗ |y∗,...,y∗ ,x∗,...,x∗,D ,T) (2)
A-LLMP 1 N 1 N train LLM n 1 n−1 1 n train
n=1
WenotethatEquation(1)satisfiestheKolmogorovExtensionTheorem[5]thereforedefiningvalid
stochasticprocess(seeAppendixA.3). However, itassumesconditionalindependencegiventhe
trainingsetandmodelweightsandthestochastistityrepresentedbythemodelisviaindependent
marginals. Equation(2)takesinspirationfromtheautoregressivestructureoftheLLMspredictive
distribution and should yield much richer predictive distributions as we are now able to model
dependenciesbetweenoutputvariables. However,thisdefinitionisnolongerguaranteedtogiveusa
validstochasticprocessasthepredictivedistributionisnowtargetorderdependentandwilllikely
failtheKolmogorovexchangabilitycondition. WeinvestigatebothofthesequestionsinSection3.
ConnectiontoNeuralprocessesNeuralProcesses(NPs)[6]areaclassofmeta-learningmodels
parametrizedbyneuralnetworksandtrainedtolearnamapfromtraining(context)setstopredictive
3distributions,p (y∗,...,y∗ |x∗,...,x∗ ,D ). ThedefinitionsinEquations1and2takeinspira-
θ 1 N 1 N train
tionfromthejointdistributionsdefinedbyConditionalNPs[6]asindependentmarginalsconditioned
onthetraining/contextsetandAutoregressiveNPs[7]utilizingthechainruleofprobability,respec-
tively. Throughthislens,LLMPscanbeviewedasexamplesofNPs. However,NPsaredirectly
trainedtooutputthispredictivedistributionwhereasLLMPsarerepurposingpretrainedLLMs.
Multi-dimensionalDensityEstimationandHandlingMissingData. Wehighlightthat,through
theflexibilityoftheLLMprompt,wedonothavetodrawadistinctionbetweenwhichvariables,or
variabledimensionsaretobemodelledorconditionedandcaneasilyhandlemissingvalues. Suppose
wehaveacollectionofvariables{x ,...,x }and{y ,...,y }(ormore),somesubsetofwhich
1 n 1 m
wewouldliketoregresson(includingxandy-values)andtheremainderwewishtoconditionon.
TodosousinganLLMP,wesimplyconstructthetrainingpromptsuchthatthevariableswewould
liketoregressonoccurattheendofthepromptandareblank(generated)whensamplingfromthe
LLMP.Ifanyvaluesaremissingtheycansimplyberemovedfromtheprompt.
3 LLMPConfiguration
Inalloftheexperiments,weusesixdifferentopensourceLLMs: Mixtral8×7B,Mixtral-8×7B-
Instruct[8],Llama-27B,Llama-270B[9],Llama-38B,andLlama-370B[10]. Ourprimarymetrics
arenegativelogprobabilitiesofthemodelevaluatedatthetruefunctionvaluesf(x∗)averagedover
thetargetlocations(NLL)andMeanAbsoluteError(MAE)betweenthepredictivemedianandthe
truefunctionvalue. Unlessotherwisestated,weuse50samplesfromtheLLMateachtargetlocation
x∗ andcomputethemedianandthe95%confidenceintervalofthesampledistribution. Sincethe
LLMsusedinourexperimentshaveundisclosedtrainingsets,weaddressthestepstakentomitigate
the issue of data-leakage in Appendix F. Additional implementation details are in Appendix G.
Prompt Engineering. We perform a set of experiments for determining the best LLMP prompt
Prompt Formatting Training Data Ordering Prompt y Scaling
0
0 5
1 1
0
0.10 0.2 0.050
0.05 0.1 0.025
0.00 0.0 0.000
Sigmoid-10 Quadratic-20 Linear+Cosine-75 Sigmoid-10 Quadratic-20 Linear+Cosine-75 Sigmoid-10 Quadratic-20 Linear+Cosine-75
_,_ _,_\n (_, _) distance sequential [0, 1] [0, 10]
x_y_ _, _\n x=_, y=_\n random [-1, 1] [-1000, 1000]
Figure3: NLLandMAEforvariouspromptformatsorderedfromthemosttoleasttokenefficient
(left),trainingdataorderings(middle),andprompty-scaling(right)usingtheMixtral-8×7BLLM.
Theheightofeachbaristhemeanof10randomseedsthatdeterminethetrainingpointlocations.
Theverticalblacklinesindicatethestandarderror. InthePromptFormattinglegend(left),thetwo‘_’
charactersindicatethepositionsofthexandyvaluesand\nrepresentsanewlineterminaltoken.
configuration. We use the Sigmoid, Quadratic, and Linear+Cosine functions with 10, 20 and 75
trainingpoints,respectively(seeAppendixE.1)withI-LLMPusingtheMixtral-8×7BLLM.
• PromptFormattingTwoseparatorsarerequiredtoachievethebestperformance. Onetoseparate
thexandyvalueswithinapairandanothertoseparatethex,ypairs. Figure3(left)demonstrates
that_,_\nisthebestoptionintermsofperformanceandtokenefficiency.
• PromptOrderingFigure3(middle)showsthatorderingthetrainingpointsbydistancetothecurrent
targetpointisbest,outperformingbothrandomandsequentialordering. Wepositthatordering
bydistanceprovidesahinttotheLLMtoweighthecontributionofclosertrainingpointstothe
currenttargetpointtoagreaterdegree.
• Prompty-ScalingFigure3(right)showsthatperformancedegradesastherangeoftheycompo-
nentsofthetrainingpointsincreasesandwhenincorporatingnegativevalues. Thisisduetothe
factthatwhentherangeiswider,theLLMmustaccuratelygeneratemorenumericaldigitsand
potentiallyanegativesignwhenpredictingf(x∗).
• top-pandTemperatureFigureH.9showsthatperformanceissurprisinglyinsensitivetovaryingthe
LLMnucleussamplingparametertop-p[11]andLLMsoftmaxtemperature.
AutoregressivevsIndependentMarginalPredictions. Inthisseriesofexperimentsweexamine
twoquestions: first,doestheautoregressivedefininitonofthejointpredictivelikelihood(A-LLMP)
4
LLN
EAM
LLN
EAM EAM
LLNA-LLMP vs I-LLMP Training Data Ordering Sigmoid-10 Quadratic-20 Lin+Cos-75
0 2.2 2.0 2.5
1 4 10 1
2.0 1.5 2.0 2
0.2 1.8 1.0 1.5 2 10 3
0.1 1.6 0.5 0 10 5
0.0
Sigmoid-10 Quadratic-20 Linear+Cosine-75
Random - I-LLMP Random - A-LLMP Independent Sorted Test 2 40 20 0 20 40
Distance - I-LLMP Distance - A-LLMP Ground Truth Random Test x
Figure 4: Autoregressive Experiments. Left: NLL and MAE for A-LLMP and I-LLMP using
differentpromptorderingsusingtheMixtral-8x7BLLM.Theheightofeachbaristhemeanof3
randomseedsthatdeterminethetrainingpointlocations. Theblacklinesindicatethestandarderror.
Center: Log-likelihoodresultsofusingvarioustestsetorderingswithLlama-2-7B,Llama-2-70Band
Mixtral-8x7BA-LLMP.TheorangeXindicatesI-LLMP,thepurplecirclesuseddistanceorderedtest
points,andthebluewhiskersarethemeanandstandarderrorof10randomlysampledtestorderings.
The red dashed line shows the log-likelihood of the test set under the generative process. Right:
HeatmapvisualizationoftheLlama-3-70BA-LLMPpredictivedistributionconditionedondatafrom
abimodalgenerativeprocess. Blackdotsaretrainingpoints.
inEquation(2)improveperformanceversustheindependentmarginaldefinitionofEquation(1)
(I-LLMP).Second,“howclose”isA-LLMPtoastochasticprocessintermsofperformancevariability
acrossqueryorderings.
Wefirstlookatlog-likelihoodsandMAEforA-LLMPandI-LLMPusingtherandomanddistance
trainingpointorderingsdiscussedearlier.ResultscanbeseeninFigure4(left).Similartoourfindings
earlier, ordering the training values according to distance to target has a large effect, improving
performanceforbothI-LLMPandA-LLMP.Unsurprisingly,thericherjointdistributiongivenby
A-LLMPgivesusbetterpredictiveperformance.
WenextexaminethevariabilityinperformanceofA-LLMPwhendifferentautoregressivetarget
orderingsareusedtogetasenseofhowfarourmethodisfromastochasticprocess(whichwould
be permutation invariant in the target points). The results of using ten sets of randomly ordered
targetpointscomparedtoI-LLMPandthegroundtruthlog-likelihoodofthetestsampleunderthe
generative distribution are presented in Figure 4 (center). Note that the training data is distance
sortedinallcases. Wealsopresenttheresultwhenorderingtargetpointsaccordingtodistancetothe
closesttrainingpoint,fromsmallesttolargest. Wemakethreekeyobservations: first,log-likelihood
performance of all A-LLMP orderings is better than I-LLMP. Second, the variance of random
orderingsissmallonthescaleofthelog-likelihoodofthegenerativemodel. Andthird,distance
orderingthetargetsgivesbetteroratleastcompetitiveperformancewitharandomordering. These
resultspresentpractitionersachoice: doyoucaremoreaboutusingavalidstatisticalprocessor
obtaininggoodpredictiveperformance? Ifitisthelatter,youwouldbebetterservedusingA-LLMP.
4 EvaluatingLLMPPerformance
Inthisseriesofexperiments,weevaluatetheperformanceofLLMPsinawidevarietyofsettings.
AdditionaldetailsandresultsforallexperimentsinthissectioncanbefoundinAppendixI
1DSyntheticDataExperiments. ToshowthatLLMPsareaviableregressionmodelwithwell-
calibrateduncertainties,webenchmarkinTable1ourA-LLMPmethodagainstaGPontheFunction
Dataset(AppendixE.1). TheGPusesanRBFkernelwithoptimizedlengthscaleandnoise. The
Mixtral-8×7BA-LLMPachievesthehighestlog-likelihoodsaveragedover7functionsizesand3
seedson10outof12ofthefunctionsandequalorbetterMAEon8ofthefunctions. Visualizations
ofthepredictivedistributionsandplotsofMAEandA-LLMPareshowninAppendixI.1.
Table1: MeanandstandarderrorofMAEandNLLaveragedoverovertheseventrainingsetsizes
and3seedsofeachfunctionforMixtral-8×7BA-LLMPandaGPwithanRBFkernel.
Function
Metric Beat Exp GauWave Linear Lin+Cos LinxSine Log Quadratic Sigmoid Sinc Sine XxSine
MAE↓ 0.33±0.01 0.32±0.12 0.20±0.02 0.11±0.04 0.16±0.02 0.12±0.03 0.09±0.03 0.07±0.01 0.37±0.05 0.08±0.02 0.22±0.02 12.79±1.07
GP NLL↓ 0.97±0.23 -1.03±0.31 -0.11±0.21 -1.45±0.22 -0.64±0.18 -1.38±0.22 -1.57±0.19 -0.40±0.29 0.03±0.21 -1.44±0.20 0.23±0.32 12.64±1.42
MAE↓ 0.31±0.01 0.08±0.01 0.24±0.01 0.05±0.00 0.19±0.01 0.05±0.00 0.04±0.00 0.07±0.01 0.51±0.04 0.08±0.02 0.27±0.02 12.45±1.37
LLMP NLL↓ -0.78±0.03 -1.56±0.04 -0.08±0.08 -2.38±0.08 -0.15±0.10 -1.90±0.02 -2.20±0.02 -1.35±0.03 -0.80±0.04 -1.96±0.03 0.14±0.11 3.30±0.23
5
LLN
EAM
)y(p
gol
gvA
B7L B07L xiM B7L B07L xiM B7L B07L xiM
y
stigoL
:)y(pTo verify that LLMPs are able to produce non-Gaussian, multimodal predictive distributions we
sampledtrainingdatafromsynthetic,multimodalgenerativedistribution(experimentaldetailsin
AppendixI.2). TheLlama-3-70BLLMPpredictivedistributionisvisualizedinFigure4(right).
ComparisontoLLMTime. Figure5demonstratesthatA-LLMPyieldssuperiorresultsinterms
ofMAEandNLLwhencomparedtoLLMTimeusingLlama-2-7Bonaforecastingtaskusingthe
weather dataset (described in Appendix E.2). Additional plots with missing training data are in
AppendixI.3. WepositthatA-LLMPbettersLLMTimeduetothefactthat1)A-LLMPnaturally
handlesirregularlyspacedxandydatawhereasLLMTimeusesonlyregularlyspacedyinformation
requiringimputationwithNaNvalueswheredataismissing;and2)A-LLMPperformsnoscalingon
yvaluesincontrasttoLLMTimethatscalesdatatoeliminatetheuseofdecimalsandnormalizethe
rangeofthedataandasaresultremovesinformationthattheLLMcanpotentiallyleverage.
True Function Training points A-LLMP Conf A-LLMP Median LLMTime Conf LLMTime Median LLMTime NLL LLMTime MAE
25 A-LLMP: Training Points=50 MAE=1.893 NLL=0.098 LLMTime: Training Points=50 MAE=1.916 NLL=1.429 3.25 A-LLMP NLL A-LLMP MAE 2.0
20 3.00 1.5 15 2.75
10 2.50 1.0
5 2.25 0.5
2.00
0 0 20 40 60 80 0 20 40 60 80 0% 20% 40% 60% 80%0.0
Days since December 12, 2023 Training data removed
Figure5: ComparisonofA-LLMPandLLMTimeontheweatherdataset. Left: Plotusingall50
trainingpoints. Right: PlotofMAEandNLLversustheamountoftrainingdataremoved. A-LLMP
haslowerMAEandNLLandthemarginoverLLMTimeincreasesasmoretrainingdataisremoved.
InthenextthreeexperimentsweshowcasetheabilityofLLMPstohandlemulti-dimensionaldata.
ImageReconstructionAsa2-dimensionalinputexperiment,Figure6showsreconstructionresults
fromimagesdrawnfromtheFashion-MNISTdataset[12]. Weconvertpixeldataintopromptdata
pointsbyformingaseriesof(row,column,pixelvalue)tuples. Additionalresultsanddetailsarein
AppendixI.4. Using20%trainpixels,thebasicformiscapturedandat50%,thereconstructionis
accuratedespitethesharppixelintensitytransitions.
True 20% Train 20% Result 50% Train 50% Result True 20% Train 20% Result 50% Train 50% Result
Figure6: Fashion-MNISTMixtralimagereconstructionresults. Thebluepixelsindicateunobserved.
Black-BoxFunctionOptimizationBlack-boxoptimizationinvolvesminimizingormaximizinga
functionwherethereisonlyaccesstotheoutputofafunctionforaspecifiedinput. Webenchmark
theabilityofLLMPstoperformmaximizationonsixcommonlyusedmulti-dimensionalfunctions.
WecompareourresultsusingLlama-2-7BtoOptuna[4],acommercialhyperparameteroptimization
framework. ResultsandimplementationdetailsareinAppendixI.5. Inallcases,LLMPsobtainas
goodorbetterapproximationtothetruemaximumvalueinafewernumberoftrials.
SimultaneousTemperature,Rainfall,andWindSpeedRegressionToexaminehowwellanLLMP
canmodelmulti-dimensionaloutputs,wecompareLLMPregressiontoamulti-outputGPonthe
weatherdatasetdescribedinAppendixE.2. Figure7showstheresultsfortheLlama-3-8BLLM(top)
anda3outputRBFkernelGPwithtrainedhyperparameters(bottom). TheLLMissimilartoandin
mostcasesbetterthantheGPintermsofMAEandNLL.
In-contextLearningUsingRelatedDataExamples. Inthisexperiment,weinvestigateLLMPs’
ability to learn from similar examples in-context to predict average monthly precipitation across
13 Canadian locations [13], one from each province and territory. For each location, we use the
Mixtral-8×7BA-LLMPtoforecast32monthsofaverageprecipitationvaluesgiventhepreviousfour
monthobservationstakenfromarandomhistoricalthree-yearperiodbetween1913-2017(conditional
ondataavailability). Itisthenprovidedwith1-12examplesofrandomthreeyearperiodsofhistorical
valuesfromthesamelocationin-context. ResultsshowninFigure8andexperimentaldetailsin
AppendixI.6. ConditioningtheLLMPonhistoricalexamplesimprovesperformancesaturatingafter
4years,anddegradingslightlythereafter. Generally,theLLMPisabletousetheexamplestopickup
6
)C°( erutarepmeT )C°( erutarepmeT
EAM LLNTrue Function LLM Confidence Median Training points GP Confidence Mean
Llama-3-7B MAE=1.253 NLL=5.205 Llama-3-7B MAE=1.188 NLL=5.205 Llama-3-7B MAE=4.626 NLL=5.205 20 20 60
40 10 10
20
0 0
0
GP MAE=1.424 NLL=8.135 GP MAE=1.622 NLL=8.135 GP MAE=4.513 NLL=8.135 20 20 60
40 10 10
20
0 0
0
0 20 40 60 80 0 20 40 60 80 0 20 40 60 80
Days since December 12, 2023
Figure7: Resultsforsimultaneouslypredictingtemperature,precipitation,andwindspeedusingthe
Llama-3-7BLLM(top)anda3outputRBFkernelGPwithtrainedhyperparameters(bottom).
MAE = 30.44531 NLL = 2.49844 MAE = 25.11906 NLL = 1.98546 MAE = 12.25406 NLL = 1.65454
3.0
100 100 100
2.5
50 50 50 2.0
1.5
0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 2 4 6 8 10 12
Months Months Months Number of Examples
(a)0examples (b)1example (c)4examples (d)ExamplesvsNLL
Figure8: (Leftthreeplots)VisualizationsofthepredictionsgivenbytheMixtral-8×7BLLMPfor
Ranfurly,Alberta. Blueandblackcirclesaretrainingandtestpoints,respectively. Redcirclesare
medianpredictionsandshadedareasindicatetenth-percentilesover30samples. (Right)NLLvs
numberofexamples. Errorbarsshowstandarderrorover13locations.
onseasonaltrendsfromhistory. Wenotethatsomelocationsdonothaveobviousorstrongseasonal
patternsbutexamplesstillhelpperformanceinthesecases(seeAppendixI.6).
5 ConditioningLLMPsonTextualInformation
OneofthemostexcitingdirectionsofLLMPsisthepotentialtoincorporatepriorinformationabout
problemsviatext. NowthatwecanexaminefunctionalpredictivedistributionsofLLMs,wecan
begintoexploretheirrichprioroverfunctionsbyconditioningonbothtextandnumericaldata.Inthis
sectionwepresenttwoexperimentswithdetailsandadditionalexperimentspresentedinAppendixJ.
Scenario-conditionalPredictions. Inthisexperiment,weexaminetheinfluenceoftextproviding
informationaboutvarioussyntheticproblemsettingsonthepredictivedistributionofanLLMPs. In
allofthefollowingexamples,weprovidethesametwosynthetictrainingpointstotheLLMPbut
changethepromptingtextthatcomesbeforethetrainingdata. WethenuseA-LLMPwithLlama-3-
70Btoforecasttrajectories50stepsahead. Webeginbyexaminingthepredictivedistributionwith
noprompt(Figure9a). WeprompttheLLMPtogeneratedailytemperaturemeasurementsindegrees
CelsiusfromMontrealinJanuary(Figure9b)andMay(Figure9c),andmonthlyprecipitationvalues
fromSanDiego,CA(Figure9d)andSingapore(Figure9e). Figure1Showstheresultsofprompting
theLLMPtogenerate(left)astockpricefinancialtimeseries(centre)foracompanythateventually
goesoutofbusinessand(right)foracompanywhosepricegoestozeroonday30.
Indeed,theLLMPmodifiesthepredictivedistributionaccordinglyrelativetothenopromptpredic-
tions. Wehighlightthefollowingobservations: first,forpromptsb)andc),themodelmovesabout
halfofitspredictivemassbelowzerofortemperaturesbeginninginJanuaryandabovezeroforthe
Maytemperatures. Second,theLLMPisabletorecallactualhistoricaltrendsforaveragemonthly
precipitationforSingaporeandSanDiegotoconditiononpromptsd)ande).Despitegettingthetrend
correct,wenotethatthemedianpredictionind)seemstobebiasedtowardthetrainingvaluesand
notreflectiveoftheactualmonthlymedian. Last,forstockpricesimulations,themodelplacesallof
itsdensityonpositivenumberssinceitismodellingprices. Itisabletoproducerealistictrajectories
anddecreasestheminexpectationwhenpromptedthatthecompanygoesoutofbusiness. Themodel
isabletoconditiononthefactthatthepricegoestozeroonday30whichcorrectlyinterpretsthe
meaningofthex-valuesasdaysstartingfrom0,thatthey-axisisthepriceandthephrase“pricegoes
tozero”correspondstoay-valueofzero. LabellingFeaturesUsingText. Inthefollowingexample,
weexaminetheperformanceofaMixtral-8x7BInstructI-LLMPonpredictingAmericanhousing
7
)C°(
erutarepmeT
)C°(
erutarepmeT
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP)mm(
noitatipicerP
)mm(
noitatipicerP
)h/mk(
deepS
dniW)h/mk(
deepS
dniW
LLN4 20 20
3 15 15
10 10
2 5 5
0 0
1
5 5
0 10 10
15 15
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
(a)Noprompt (b)Montrealdailytemp.inJan. (c)Montrealdailytemp.inMay
Average Monthly Precipitation: San Diego vs. Singapore
10 10 San Diego
12 Singapore
8 8 10
6 6 8
6
4 4
4
2 2 2
0
0 0 Jan FebMarAprMayJun Jul AugSepOctNovDec
0 10 20 30 40 50 0 10 20 30 40 50 Month
(d)Monthlyprecip.inSingapore (e)Monthlyprecip.inSanDiego (f)Actualmonthlyaverages
Figure9: a)-e)predictivedistributionsfromanA-LLMPusingLLama-3-70Bundervariousscenario
prompts. BlackpointsaretwotrainingpointsgiventotheLLMprocess,thesamevaluesforeach
scenario. The tenth-percentiles from 50 samples are visualized in faded blue and the median is
presentedindarkbluewithfiverandomsamplesshowninvariouscolours. Figuref)showstheactual
averagemonthlyrainfallforSingaporefrom1991-2020[14]andSanDiegofrom2000-2024[15].
1e6
2.5 True Price 4.50 4.5
12 .. 50 1 5) ) l aa lt l - fl eo an tg ures as text 44 .. 05 44 .. 02 05 3.0 4.0
01 .. 50 33 .. 05 333 ... 257 505 22 .. 05 3.5
0.0 2.5 3.00
3.0
0.5 2.0 2.75 1.5
1) lat-long 2) location, 3)+zip, 4)+population,5)+space, a) lat-long b) location, c) all features d) all features
1234567891011121314151617181920 as num lat-long income density bed, lat-long + text
House Example bath + text
Figure10:ResultsofaMixtral-8x7BInstructI-LLMPpredictingUShousingprices.Left:Predictions
for 10 randomly selected houses using index style 1) and 5). Xs are mean predictions using 30
samplesfromtheLLMPanderrorbarsindicate2standarddeviations. Centreandright: Average
MAE and NLL performance of the LLMP over 10 experiments with error bars representing the
standarderrorforexperimentsfromSection5.
prices. Thedataset[16]contains39980housingpricesandvariousvariablesaroundhousingand
demographicsforthetop50Americancitiesbypopulation. Notethatthisdatasetwasgeneratedon
12/09/2023,howeveritcontainsdatafromthe2020USCensusandthe2022AmericanCommunity
Survey(ACS)sowecannotguaranteethatmodelsdidnotseedatawithinthisdatasetduringtraining.
Foreachpredictiontask,weshowtheI-LLMP10randomlyselectedtrainingexamplesfromthe
datasetandpredicton20randomlyselectedtestexamples. Intheprompt,beforethenumericalvalue
(price)weprovideastringwhichencodesthedatapointindex/featuresthatthemodelcanuse. For
ourfirstexperimentweexaminethebehaviouroftheLLMPwhenmorefeaturesareaddedtothe
prompt. Weexperimentwithfivewaysofindexingthetrainingandtestpoints; Forcase(1), we
providelatitudeandlongitudeofthehouseasnumericalvalues(eg. 32.74831,-97.21828)converted
tostringssimilartoourmethodinpreviousexperiments. Fortheremaining4cases, weprovide
additionallabeledfeatures,addingmorefeaturesforeachcasewiththepromptforcase(5)containing
alllabelledfeatures,illustratedwiththefollowingexample:(2)Location:FortWorth,Texas,Latitude:
32.74831,Longitude: -97.21828,(3)ZipCode: 76112,MedianHouseholdIncome: 71452.0,(4)Zip
CodePopulation: 42404people,ZipCodeDensity: 1445.0peoplepersquaremile,(5)LivingSpace:
1620squarefeet,NumberofBedrooms: 3,NumberofBathrooms: 2.
This procedure is repeated 10 times to compute statistics. Results are presented in Figure 10
(left,centre). NotethattheLLMPisabletotakeadvantageoftheadditionalfeaturesprovidedto
improve predictive performance. To see examine the effect of adding text labels to the features,
we ran another set of experiments on 10 new random datasets providing the LLMP with either
labeledorunlabellednumericalfeatures. Thefollowingareexamplefeaturestrings: (i)“30.45738,
-97.75516”(ii)“Location: Austin,Texas,Latitude: 30.45738,Longitude: -97.75516”(iii)“30.45738,
8
)DSU(
ecirP
K001$
ni EAM LLN
)sehcni(
noitatipicerP
egarevA
K001$
ni EAM LLN-97.75516, 78729, 107830.0, 30907, 1216.1, 1349, 3” (iv) “Location: Austin, Texas, Latitude:
30.45738, Longitude: -97.75516, Zip Code: 78729, Median Household Income: 107830.0, Zip
CodePopulation: 30907people,ZipCodeDensity: 1216.1peoplepersquaremile,LivingSpace:
1349squarefeet,NumberofBedrooms: 3,NumberofBathrooms: 2”. Resultsofthisexperiment
arepresentedinFigure10(right). NotethattheLLMPisnotabletousetherawfeaturevaluesto
improveperformancefromonly10trainingexamples, butisabletodosowithlabelledfeatures
suggestingthatLLMisabletoutilizethelatentrelationshipbetweenthefeatureandthepriceonce
thefeatureisidentified. WefoundthattheMixtral-8×7BInstructmodelhadthebestperformanceon
thistaskandwasabletoutilizetextinformationbetter(resultsforothermodelsinAppendixJ.2).
6 RelatedWork
Inthissection,wediscussworkrelatedtoelicitingdistributionsfromLLMsincludingforecasting,
regression,andin-contextlearningamongothers. PleaseseeAppendixDforadditionalrelatedwork.
LLMForecastingThemostcloselyrelatedworktooursisLLMTime[3]. LLMTimeiscapableof
zero-shotextrapolationofone-dimensionaltimeseriesdataatalevelcomparabletotrainedpurpose-
builtapproaches. Inaddition,theydevelopamethodforelicitingmarginalprobabilitydistribution
functionsfromLLMposteriorsoverfunctions,whichwebuildon. Theyalsobegintoinvestigatethe
effectofconditioningontext. Incontrast,wefocuson(i)interpolationwithmulti-dimensionalinputs
andoutputs;(ii)elicitingjointdistributionsoverfunctions,notjustmarginals;and(iii)exploring
theabilityofmodelstoconditionsimultaneouslyonbothnumericaldataandtext. Morerecently,
TimesFM [17], a foundation model for one-dimensional zero-shot times series forecasting was
introduced. However,TimesFMdoesnotsupportinterpolationorhigherdimensionaldataanddoes
notconsiderdistributions. PromptCast[18]performszero-shottimeseriesforecastingbycombining
numericaldataandtextinaquestionanswerformat. Ourapproachforcombiningproblemspecific
text along with numerical data differs in that it handles both interpolation and extrapolation and
doesnotrelyonaquestion-answerformat. Hegselmannetal.[19]utilizeLLMstodozero-shotand
few-shotclassificationontabulardatathatcomparesfavorablytostandardMLapproaches.
LLMRegressionPesut[20]dosomeinitialinvestigationsintotheuseofLLMsasregressorson1D
syntheticfunctions. Ourworkgreatlyexpandsontheseearlyinvestigations. Vacareanuetal.[21]is
concurrentworkthatshowsthatLLMsarecapablelinearandnon-linearregressors. However,their
workdoesnotconditiononanytextualinformation,computelogprobabilities,comparetoGaussian
Processes,investigatethetheeffectofpromptformatting,oremployauto-regressivesampling.
In-contextlearning(ICL)inLLMsXieetal.[22]pointoutthatICLcanbeseenasbeingequivalent
toBayesianinferenceinalatentvariablemodel. Morerecently,[23]explainin-contextlearningin
LLMsaskernelregression. Gargetal.[24]traintransformerstodoin-contextlearningonvarious
functionclassesincludinglinear(upto50dimensions),decisiontrees,andtwo-layerReLUnetworks.
Coda-Forno et al. [25] demonstrate that LLMs are capable of meta-in-context learning and that
performanceon1-Dlinearregressionandtwo-armedbandittasksimproveswithmultipleexamples.
7 Discussion,Limitations,andSocietalImpact
Belowwediscussourfindings,thelimitationsandsocietalimpactoftheworkpresented. Further
discussionontheseissuescanbefoundinAppendixK.
DiscussionWedefinedLLMPsforelicitingnumericalpredictivedistributionsfromLLMsandwhen
usedasazero-shotmuti-dimensionalregressionmodelarecompetitivewithGPs. Excitingly,we
demonstratedtheabilitytoconditionontexttoimprovepredictionsandprobetheLLMs’hypothesis
space. Aninterestingextensionwouldbetoconditiononothermodalitiesinadditiontotext.
LimitationsAlongwiththeflexibilityofLLMs,LLMPsinherittheirdrawbacks. Maximumcontext
sizeslimitthesizeoftaskswecanapplythismethodtoandtheamountoftextualinformationwecan
conditionon. LLMPsarealsosignificantlymorecomputationallyexpensivecomparedtoGaussian
Processesandstandardregressionmethods. Allofexperimentswereperformedonreadilyavailable
opensourceLLMsthataresmallerandgenerallylesscapablecomparedtoproprietaryLLMs.
SocietalImpactOurworkhasdemonstratedanewandusefulzero-shotapproachforgenerating
probabilistic predictions using plain language to augment numerical data. It has the potential to
9allowpractitionersfromfieldssuchasmedicalresearchandclimatemodellingtomoreeasilyaccess
probabilisticmodellingandmachinelearning. Likeallmachinelearningtechnology,thereispotential
forabuse,andpossibleconsequencesfromincorrectpredictionsmadewithLLMPs. Also,wedonot
knowthebiasesintheunderlyingLLMsusedandwhateffecttheymayhaveonLLMPsoutput.
AcknowledgmentsandDisclosureofFunding
James Requeima and David Duvenaud acknowledge funding from the Data Sciences Institute at
theUniversityofTorontoandtheVectorInstitute. DamiChoiwassupportedbytheOpenPhilAI
Fellowship. RichardE.TurnerissupportedbygiftsfromGoogle,Amazon,ARM,Improbableand
EPSRCgrantEP/T005386/1.
WethankAnnaVaughanforhelpwiththeweatherdatasetsanddiscussions. WethankWillTebbutt
andMatthewAshmanforhelpfulcommentsandsuggestions.
References
[1] Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton
Greenside,andAndrewGordonWilson. Acceleratingbayesianoptimizationforbiologicalse-
quencedesignwithdenoisingautoencoders. InInternationalConferenceonMachineLearning,
pages20459–20478.PMLR,2022.
[2] Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. Lmpriors: Pre-trained
languagemodelsastask-specificpriors. arXivpreprintarXiv:2210.12530,2022.
[3] NateGruver,MarcFinzi,ShikaiQiu,andAndrewGordonWilson. Largelanguagemodelsare
zero-shottimeseriesforecasters. arXivpreprintarXiv:2310.07820,2023.
[4] TakuyaAkiba,ShotaroSano,ToshihikoYanase,TakeruOhta,andMasanoriKoyama. Optuna:
Anext-generationhyperparameteroptimizationframework. InProceedingsofthe25thACM
SIGKDDinternationalconferenceonknowledgediscovery&datamining,pages2623–2631,
2019.
[5] BerntOksendal. Stochasticdifferentialequations: anintroductionwithapplications. Springer
Science&BusinessMedia,2013.
[6] MartaGarnelo,DanRosenbaum,ChristopherMaddison,TiagoRamalho,DavidSaxton,Murray
Shanahan,YeeWhyeTeh,DaniloRezende,andSMAliEslami. Conditionalneuralprocesses.
InInternationalconferenceonmachinelearning,pages1704–1713.PMLR,2018.
[7] WesselPBruinsma,StratisMarkou,JamesRequiema,AndrewYKFoong,TomRAndersson,
AnnaVaughan,AnthonyBuonomo,JScottHosking,andRichardETurner. Autoregressive
conditionalneuralprocesses. arXivpreprintarXiv:2303.14468,2023.
[8] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,Chris
Bamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,
etal. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
[9] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[10] AI@Meta. Llama3modelcard. 2024. URLhttps://github.com/meta-llama/llama3/
blob/main/MODEL_CARD.md.
[11] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecuriouscaseofneural
textdegeneration. InInternationalConferenceonLearningRepresentations,2020.
[12] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarkingmachinelearningalgorithms,2017.
10[13] Environment and Climate Change Canada. Monthly total of
daily adjusted total precipitation. Online, 2024. URL https:
//www.canada.ca/en/environment-climate-change/services/
climate-change/science-research-data/climate-trends-variability/
adjusted-homogenized-canadian-data/precipitation-access.html. Accessed:
April2024,Lastupdated: 2017-08-09.
[14] University of East Anglia Climatic Research Unit. Observed Historical Cli-
mate Data for Singapore. World Bank Climate Knowledge Portal, 2024. URL
https://climateknowledgeportal.worldbank.org/country/singapore/
climate-data-historical. Accessed: 2024-05-06.
[15] Climate Data. National Weather Service, 2024. URL https://www.weather.gov/wrh/
Climate?wfo=sgx. Accessed: 2024-05-06.
[16] Jeremy Larcher. American house prices, 2023. URL https://www.kaggle.com/dsv/
7162651.
[17] AbhimanyuDas,WeihaoKong,RajatSen,andYichenZhou. Adecoder-onlyfoundationmodel
fortime-seriesforecasting. arXivpreprintarXiv:2310.10688,2023.
[18] HaoXueandFloraDSalim. Promptcast: Anewprompt-basedlearningparadigmfortime
seriesforecasting. IEEETransactionsonKnowledgeandDataEngineering,2023.
[19] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and
DavidSontag. Tabllm: Few-shotclassificationoftabulardatawithlargelanguagemodels. In
InternationalConferenceonArtificialIntelligenceandStatistics,pages5549–5581.PMLR,
2023.
[20] LovrePesut. Whomodelsthemodelsthatmodelmodels? anexplorationofgpt-3’sin-context
modelfittingability. URLhttps://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/who-
models-the-models-that-model-models-an-exploration-of,2022.
[21] Robert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, and Mihai Surdeanu. From words to
numbers: Your large language model is secretly a capable regressor when given in-context
examples. arXivpreprintarXiv:2404.07544,2024.
[22] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
in-contextlearningasimplicitbayesianinference. arXivpreprintarXiv:2111.02080,2021.
[23] ChiHan,ZiqiWang,HanZhao,andHengJi. Explainingemergentin-contextlearningaskernel
regression. arXivpreprintarXiv:2305.12766,2023.
[24] ShivamGarg,DimitrisTsipras,PercySLiang,andGregoryValiant. Whatcantransformers
learn in-context? a case study of simple function classes. Advances in Neural Information
ProcessingSystems,35:30583–30598,2022.
[25] JulianCoda-Forno,MarcelBinz,ZeynepAkata,MatthewBotvinick,JaneXWang,andEric
Schulz. Meta-in-contextlearninginlargelanguagemodels. arXivpreprintarXiv:2305.12907,
2023.
[26] MichaelZhang, NishkritDesai, JuhanBae, JonathanLorraine, andJimmyBa. Usinglarge
languagemodelsforhyperparameteroptimization. InNeurIPS2023FoundationModelsfor
DecisionMakingWorkshop,2023.
[27] TennisonLiu,NicolásAstorga,NabeelSeedat,andMihaelavanderSchaar. Largelanguage
modelstoenhancebayesianoptimization. arXivpreprintarXiv:2402.03921,2024.
[28] MarcelBinzandEricSchulz. Turninglargelanguagemodelsintocognitivemodels. arXiv
preprintarXiv:2306.03917,2023.
[29] BenjaminLipkin,LionelWong,GabrielGrand,andJoshuaBTenenbaum. Evaluatingstatistical
languagemodelsaspragmaticreasoners. arXivpreprintarXiv:2305.01020,2023.
11[30] EricSchulz, JoshuaBTenenbaum, DavidDuvenaud, MaartenSpeekenbrink, andSamuelJ
Gershman. Compositionalinductivebiasesinfunctionlearning. Cognitivepsychology, 99:
44–79,2017.
[31] BogdanGrigore,JaimePeters,ChristopherHyde,andKenStein. Methodstoelicitprobability
distributions from experts: a systematic review of reported practice in health technology
assessment. Pharmacoeconomics,31:991–1003,2013.
[32] HyunjikKim,AndriyMnih,JonathanSchwarz,MartaGarnelo,AliEslami,DanRosenbaum,
OriolVinyals,andYeeWhyeTeh. Attentiveneuralprocesses. arXivpreprintarXiv:1901.05761,
2019.
[33] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta
learningviasequencemodeling. arXivpreprintarXiv:2207.04179,2022.
[34] StratisMarkou,JamesRequeima,WesselBruinsma,andRichardTurner. Efficientgaussian
neuralprocessesforregression. arXivpreprintarXiv:2108.09676,2021.
[35] Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. Neural diffusion
processes. InInternationalConferenceonMachineLearning,pages8990–9012.PMLR,2023.
[36] OpenWeather. WeatherAPI,2024. URLhttps://openweathermap.org/api. Accessed:
2024-03-07.
[37] Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon
Wilson. Gpytorch: Blackboxmatrix-matrixgaussianprocessinferencewithgpuacceleration.
InAdvancesinNeuralInformationProcessingSystems,2018.
[38] WilliamRThompson. Onthelikelihoodthatoneunknownprobabilityexceedsanotherinview
oftheevidenceoftwosamples. Biometrika,25(3-4):285–294,1933.
[39] WilliamRThompson. Onthetheoryofapportionment. AmericanJournalofMathematics,57
(2):450–456,1935.
[40] MatthewW.HoffmanandBobakShahriari. benchfunk. https://github.com/mwhoffman/
benchfunk,2015.
[41] RobertBGramacyandHerbertKHLee.Casesforthenuggetinmodelingcomputerexperiments.
StatisticsandComputing,22:713–722,2012.
12A LLMProcesses: DefiningaStochasticProcessThatCanConditiononText
In this section we elaborate on the explanations and definitions in Section 2. Our goal is to use
anLLMtoelicitjointpredictivedistributionoverarbitrarysizedtargetsetsthatwecanguideand
modifyusingplainlanguage. Formally,givenasetofobservationsD ={(x ,y )}M andsome
train i i i=1
text, T, we would like to elicit the predictive distribution defined by an LLM at a collection of
targets {(x∗,y∗)}N denoted p (y∗,...,y∗ | x∗,...,x∗ ,D ,T). To achieve the goal, we
j j j=1 LLM 1 N 1 N train
can can keep in mind two interpretations of what we mean by a predictive distribution defined
byanLLM.First,wecaninterprettheLLMasmaintaininghavingapredictivedistributionover
numericalvalues,whichwecanprobebysamplingfromtheLLM.Thisinterpretationisbeneficial
ifwebelievethattheLLMhaslearnedusefulpriorinformationthatwewouldliketoaccessviaits
beliefsaboutthesenumericalvaluesandforourgoalofguidingthepredictivedistributionusing
text. Theotherinterpretationismoreempirical: wesimplyusetheLLMasatooltodefineavalid
predictivedistributionandevaluatehowwellthisdefinitionperformsontestcases. Ourapproachisa
combinationofthetwophilosophies–wewillproposeamethoddefiningapredictivedistribution
thatisvalidandperformswellontestcases,butcloselymatcheswhatwethinkofastheLLM’s
underlyingdistribution.
A.1 ContinuousMarginalLikelihoodsFromanLLM
AsdiscussedinSection2, weuseamethodsimilartotheoneproposedbyGruveretal.[3]; we
approximatethecontinuousdensitybydiscretizingthespaceusingbinswitharbitrarilyfineprecision.
Let’sassumeafixednumberofdecimalplacesn,andthatLLMsgenerateonedigitatatime2. The
key idea is that each new digit can be viewed as being generated from a categorical distribution
with the probabilities p given by a softmax over numerical tokens. Crucially, this hierarchical
approach allows us to compute the probability of a bin with width 10−n. For example, if n = 1
then Pr{y ∈ [1.0,1.1)} = p(1)p(.|1)p(0|1.) because ‘1.0’ is a prefix for all y ∈ [1.0,1.1) . We
canconvertprobabilitymasstoprobabilitydensitybyassumingauniformdistributionwithineach
bin,anddividingthemassbythebinwidth. Avisualizationofthisconstructioncanbeviewedin
AppendixH.1.
Themethodin[3]hastwomainshortcomingsforourpurposes: first,theauthorsproposetoscaleall
y ∈D toeliminatedecimalsfromtheirnumericalrepresentation. Forexample,foraprecisionof
train
2decimalplaces,thenumbers0.123,1.23,12.3,and123.0willbetransformedto12,123,1230,and
12300respectively. ScalingremovespriorinformationcommunicatedtotheLLMviathescaleofthe
problem. Forexample,itislikelythattheLLMhasencounteredfinancialdatawithdecimalplaces.
Potentially,italsomakesitmoredifficulttocommunicatepriorinformationabouttheproblemtothe
LLMviatext.
Second,probabilitiesofallsequencesofintegersgivenbyanLLMcontainthemassofallvalues
that also start with that sequence. We can think of this as the problem of not knowing when the
LLMintendstoterminateavalue. Forexample,ify = 12,Pr{y ∈ [12,13)} ̸= p(1)p(2|1)since
p(1)p(2|1)includestheprobabilityofallnumberswith‘12’asaprefix–thisincludes[12,13)but
also[120,130),[1200,1300)andsoon.
A.2 TheLLMProcessMethod
WefollowGruveretal.[3]anddiscretizethecontinuousspacewithbinsofwidth10−n,computing
theprobabilitiesforeachbinusingthehierarchicalsoftmaxapproach. However,differentfromtheir
approachwe1)keepvaluesattheiroriginalscale,and2)includeaterminaltokenaftereveryvalue
–forexample,givenaterminaltoken⟨t⟩,werepresent12as12⟨t⟩and120as120⟨t⟩. Includinga
terminaltokenpreventsnumbersofvaryingordersofmagnitudefromsharingthesameprefix–i.e.
p(1)p(2|1)p(⟨t⟩|12)nolongerincludestheprobabilityofnumbersin[120,130),[1200,1300),andso
on. Afterwecomputethemassofabinviahierarchicalsoftmax,wedividethemassbythebinwidth
10−ntogetanestimateofthedensityvalue. Thisproceduredefinesavalidpredictivedistribution
overy-values,andwecallthiselicitationmethod‘logit-based’sincewederiveprobabilitiesfromthe
logitsdirectlyinsteadofsampling. PseudocodecanbefoundinAlgorithm2.
2Themodelsweevaluatearetrainedwithtokenizationschemesthattokenizeeachdigitinanumberseparately.
Gruveretal.[3]includeaspacebetweeneachdigitfortokenizersthatdonottokenizeeachdigitseparately.
13ItmustbenotedthatthisapproachdoesnotguaranteethatP(12⟨t⟩)yieldsthemassassignedby
theLLMtovaluesinthebin[12,13). However,wenotethatourmethoddefinesavalidpredictive
distributionandweempiricallyobservedthatourpredictivedistributioncloselymatchesthesampling
distributiontooursatisfaction(seeAppendixH.1).
A.3 DefininganLLMProcess
So far we have established a procedure for defining the predictive distribution at a single target
location, p (y∗ | x∗,D ,T). Wenowdiscusshowtodefinethejointpredictivedistribution
LLM n n train
overacollectiontargetpoints. Inparticular,wewouldliketodefineastochasticprocessviaitsfinite-
dimensionalmarginaldistributionsρ definedoverlocationsx ,...,x . TheKolmogorov
x1,...,xN 1 N
ExtensionTheorem[5]statesthatsuchacollectiondefinesastochasticprocessifitsatisfies
1. Exchangeability: Givenanypermutationπoftheintegers{1,...,N}
ρ (y ,y )=ρ (y ,y )
x1,...,xN 1 N xπ(1),...,xπ(N) π(1) π(N)
2. Consistency: if1≤M ≤N then
(cid:90)
ρ (y ,...,y )= ρ (y ,y )dy ...dy
x1,...,xM 1 M xπ(1),...,xπ(N) π(1) π(N) M+1 N
InEquation(1)wedefineacollectionofjointdistributionsbydefiningafactorizeddistributionover
targetlocationsx∗,...,x∗ :
1 N
N
(cid:89)
p (y∗,...,y∗ |x∗,...,x∗ ,D ,T)= p (y∗,|x∗,D ,T)
I-LLMP 1 N 1 N train LLM n n train
n=1
wherep (y∗,|x∗,D ,T)isdefinedabove.
LLM n n train
This definition satisfies the Kolmogorov Extension Theorem and so it defines a valid stochastic
process. However,itassumesconditionalindependencegiventhetrainingsetandmodelweights
and,conditionalonthesevariables,thestochastistityrepresentedbythemodelisviaindependent
marginals. TakinginspirationfromtheautoregressivestructureoftheLLMspredictivedistribution,
wecanwritethejointdistributionaccordingtotheproductrule:
N
(cid:89)
p (y∗,...,y∗ |x∗,...,x∗ ,D ,T)= p (y∗ |y∗,...,y∗ ,x∗,...,x∗,D ,T)
A-LLMP 1 N 1 N train LLM n 1 n−1 1 n train
n=1
Where,theprevioustargetlocationisautoregressivelyaddedtotheconditioningdataviatheLLM
prompt. Thisshouldyieldmuchricherpredictivedistributionsaswearenowabletomodeldepen-
denciesbetweenoutputvariables. However,thisdefinitionisnolongerguaranteedtogiveusavalid
stochasticprocessasthepredictivedistributionisnowtargetorderdependentandmostlikelywill
failtheKolmogorovexchangabilitycondition. WeinvestigatethesequestionsinSection3.
14B LLMProcessesPseudocode
Algorithm1PseudocodeforsamplingnumbersfromanLLM
N ←Numberofdesiredsamples
samples←[]
whilelen(samples)<N do
out←model.generate(prompt)
ifoutisanumberthen
samples.append(out)
endif
endwhile
Algorithm2Pseudocodeforcomputingthelogpdfofy
n←numberofdigitsafterdecimalpoint
nonnum_idxs←tokens∈/ tokenize([‘0’,‘1’,...,‘9’,‘-’,‘.’,‘⟨t⟩’])
full_text←prompt+str(y)
y_idxs←indicesofthetokensthatcorrespondtoyinfull_text
logits←model(full_text)
y_logits←logits[y_idxs]
y_logits[nonnum_idxs]←-100
y_logpmf←CrossEntropy(logits=y_logits[:-1],targets=str(y)[1:]).sum() ▷Massofbinthat
includesy
y_logpdf←y_logpmf+nlog10 ▷Convertmasstocontinuouslikelihood
15C SamplePrompts
FigureC.1depictsthreeobservedtrainingpointsandfourtargetlocations. Belowaresampleprompts
forvariousconfigurationsdiscussedinthepaper. T referstoproblemrelatedtext.
𝑦
Observed Point
Target Locations
𝐵 ,B
𝑥 𝑦
𝐶 ,C
𝑥 𝑦
𝐴 ,A
𝑥 𝑦
𝑥
𝐷∗ 𝐸∗ 𝐹∗ 𝐺∗
𝑥 𝑥 𝑥 𝑥
FigureC.1: Threeobservedtrainingpointsandfourtargetlocationswhichserveasthebasisforthe
exampleprompts.
IndependentMarginalPrompts
Sequential:
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩D∗"
x y x y x y x
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩E∗"
x y x y x y x
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩F∗"
x y x y x y x
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩G∗"
x y x y x y x
Random:
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩D∗"
x y x y x y x
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩E∗"
x y x y x y x
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩F∗"
x y x y x y x
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩G∗"
x y x y x y x
Distance:
“T⟨t⟩C ,C ⟨t⟩B ,B ⟨t⟩A ,A ⟨t⟩D∗"
x y x y x y x
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩E∗"
x y x y x y x
“T⟨t⟩A ,A ⟨t⟩C ,C ⟨t⟩B ,B ⟨t⟩F∗"
x y x y x y x
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩G∗"
x y x y x y x
16AutoregressivePrompts
Sequential:
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩D∗"
x y x y x y x
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩D∗,D∗⟨t⟩E∗"
x y x y x y x y x
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩D∗,D∗⟨t⟩E∗,E∗⟨t⟩F∗"
x y x y x y x y x y x
“T⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩C ,C ⟨t⟩D∗,D∗⟨t⟩E∗,E∗⟨t⟩F∗,F∗⟨t⟩G∗"
x y x y x y x y x y x y x
Random:
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩D∗"
x y x y x y x
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩D∗,D∗⟨t⟩E∗"
x y x y x y x y x
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩D∗,D∗⟨t⟩E∗,E∗⟨t⟩F∗"
x y x y x y x y x y x
“T⟨t⟩C ,C ⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩D∗,D∗⟨t⟩E∗,E∗⟨t⟩F∗,F∗⟨t⟩G∗"
x y x y x y x y x y x y x
Distance:
“T⟨t⟩C ,C ⟨t⟩B ,B ⟨t⟩A ,A ⟨t⟩D∗"
x y x y x y x
“T⟨t⟩C ,C ⟨t⟩D∗,D∗⟨t⟩A ,A ⟨t⟩B ,B ⟨t⟩E∗"
x y x y x y x y x
“T⟨t⟩D∗,D∗⟨t⟩A ,A ⟨t⟩E∗,E∗⟨t⟩C ,C ⟨t⟩B ,B ⟨t⟩F∗"
x y x y x y x y x y x
“T⟨t⟩D∗,D∗⟨t⟩A ,A ⟨t⟩E∗,E∗⟨t⟩B ,B ⟨t⟩F∗,F∗⟨t⟩C ,C ⟨t⟩G∗"
x y x y x y x y x y x y x
17D AdditionalRelatedWork
InthissectionwecoveradditionalrelatedworkincludingLLMhyperparameteroptimization,eliciting
priorsfromLLMs,elicitingdistributionsfromhumans,andfinallyneuralprocesses.
LLMHyperparameterOptimizationZhangetal.[26]andLiuetal.[27]useLLMstoperform
hyperparameter optimization, showing that LLMs can condition on a mixture of textual data as
numericalobservationstoeffectivelyoptimizehyperparametersinmachinelearningmodels.
Eliciting priors from LLMs Binz and Schulz [28] fine-tune LLMs on data from psychological
experimentstoachieveaccuraterepresentationsofhumanbehavior. Choietal.[2]showhowusing
anLLMtoassesstheimportanceoffeaturesorthecausalrelationshipbetweenvariablesthatcan
improveperformanceontasks. Lipkinetal.[29]findthatLLMscanderivehuman-likedistributions
overtheinterpretationsofcomplexpragmaticutterances.
ElicitingdistributionsfromhumansSchulzetal.[30]lookatcompositionalinductivebiasesin
functionlearning,showinghumanshavecompositionalstructureintheirpriorsonfunctions. [31]
cataloguestandardstrategiesforelicitingdistributionsfromexperthumans.
Neural processes Neural Processes are a class of meta-learning models trained to learn a map
fromtraining(context)setstopredictivedistributions,p (y∗,...,y∗ | x∗,...,x∗ ,D ). These
θ 1 N 1 N train
modelsareparameterizedusinganeuralnetworkandtherehavebeenvariousproposalsfordifferent
architectures using attention [32], transformers [33], Gaussian Process output layers [34], and
diffusionmodels[35]. Thedefinitionsofthejointdistributionsinequations1and2takeinspiration
fromthejointdistributionsdefinedbyConditionalNeuralProcesses[6]asindependentmarginals
conditionedonthetraining/contextsetandAutoregressiveNeuralProcesses[7]utilizingthechain
ruleofprobability,respectively. Throughthislens,LLMPscanbeviewedasexamplesofNeural
Processes. LLMPs differ from standard NPs in two main ways: (i) Training objective: Neural
Processesaremeta-trainedusingmaximumlikelihoodtooptimizep(y∗|x∗,D )directly. LLMPs
train
haveaveryindirecttrainingprocedure–theyaretrainedtobelanguagemodelsi.e.autoregressive
tokenpredictors. Oneofthecontributionsofthispaperisthedemonstrationthat,despitethis,they
can perform zero-shot probabilistic regression. (ii) Architecture: NPs have an output layer that
parametrizesthepredictivedistributionovertargetsdirectly. SinceLLMPsarerepurposinglanguage
modelsforregression,weneedtodefinethemappingfromdistributionsoverlanguagetokensto
distributionsovertargetvariables. WenotethatLLMsthemselvescanbeviewedasAR-CNPS[7]
withafixed,predefinedtargetordering.
18E DatasetDetails
Thissectionprovidesdetailsonthevariousdatasetsusedintheexperiments
E.1 FunctionDataset
Weusethe12syntheticfunctiondatasets(Linear,Exponential,Sigmoid,Log,Sine,BeatInference,
Linear+Cosine,Linear×Sine,GaussianWave. Sinc,Quadratic,X×Sine)fromGruveretal.[3]
eachofwhichconsistsof200discretepoints. Weconstruct7datasetseachwith10randomseedsfor
eachfunctionwithasubsetof5,10,15,20,25,50,and75randomlytrainingpointssampledfrom
theoriginal200points. WeaddGaussiannoisewithµ=0andσ =0.05tothetrainingpointsand
thenroundthevaluesto2decimalplaces. Unlessotherwisestated,weuse40equallyspacedtarget
pointstosampleat.
E.2 WeatherDataset
ThedatasetwasqueriedfromOpenWeather[36]andconsistsofdailyhightemperature,precipitation,
andwindspeedreadingsfor86consecutivedayscommencingonDecember12,2023. Thedatawas
recordedafterthereleasedatesoftheLlama-2andMixtral-8x7BLLMreleasedatestoavoidany
dataleakageintotheLLMdatasets.
Forthe"ComparisontoLLMTime"experiment,Weusedthefirst50readingsofthetemperaturedata
fortrainingdataandaskLLMTimeandLLMPstopredict/forecastthefinal36values. Theauthors
ofLLMTimesuggestthemethodcanhandlemissingvaluesbyinputtingNaNvaluesintheirplace.
SinceLLMPscanworkwithirregularlyspacedandmissingdata,wealsocomparethemethodswith
areducednumberofrandomlyspacedtrainingpoints.
Forthe"SimultaneousTemperature,Rainfall,andWindSpeedRegression"experimentweused30
randomlychosentrainingpointswithinthefirst76points,leavingthelast10forextrapolation.
F DataLeakage
ItislikelythatLLMsusedinourexperimentshavebeenexposedduringtrainingtosomeofthe
real-world data that we use in our experiments which would give it an advantage against other
models. However, we feel confident that the LLMs tested were not simply recalling memorized
data–notethatinallcasestheLLMPsproducesafulldistributionandnotjustadeterministicvalue
–andwehavetakenstepsinourexperimentstomitigatethisissue. Whensyntheticfunctionsor
FashionMNISTdata[12]isused,wehavealteredtheoriginaldataviasubsampling,rescalingand
insomecasesaddingnoisetothedatapoints. Anydatausedfromtheinternetwasalteredfromits
originalformwhengiventothemodel. Somedatasets(inparticulartheWeatherDatasetdescribedin
AppendixE.2),wereexplicitlychosentoberecordedafterthereleasedatesoftheLLMsthatthey
wereevaluatedon.
19G AdditionalImplementationDetails
PyTorchisusedasthebasisforalloftheexperiments,withtheexceptionoftheGaussianProcesses
baselinesthatareimplementedusingtheGPyTorchpackage[37].
TheexperimentsusingtheMixtral8×7B,Mixtral-8×7B-Instruct[8],Llama-270B[9],andLlama-3
70B[10]LLMswererunontwoNVidiaA100GPUswith80GBofmemory. Theexperimentsusing
theLlama-27B[9]andLlama-38B[10]LLMswererunononeNVidia3090GPUwith24GBof
memory. Thetotalcomputeusedinthepaperexceeded600GPUhours.
NotrainingwasdoneinourLLMexperiments,wesimplyinputtheprompttotheLLMandranit
forwardtogetapredictionforaparticulartargetpoint.
Processingtimesvariedasafunctionof:
• TheGPUused.
• Thelengthoftheprompt.
• Thenumberoftargetpointsqueried.
• Thenumberoftokensrequiredtobegeneratedforaparticulartargetpoint.
• Thenumberofsamplestakenateachtargetpoint.
ExampleExperimentprocessingtimes:
1DSyntheticDataExperiments:
• LLM:Mixtral-8×-7B
• GPU:2×NvidiaA100,80GB
• Parameters: A-LLMP,40targetpoints,50samples,logprobabilities
• Tasks: 12functionsx3seedsx4sizes
• ApproximateTime: 19.6hours
BlackBoxOptimization:
• LLM:Llama-27B
• GPU:1×NvidiaA100,80GB
• Parameters: I-LLMP,500targetpoints,1sample
• Tasks: 6functions,100trials
• ApproximateTime: 20hours
FashionMNISTImageReconstruction:
• LLM:Mixtral-8×-7B
• GPU:2×NvidiaA100,80GB
• Parameters: I-LLMP,400targetpoints,50samples
• Tasks: 6imagesx2sizes
• ApproximateTime: 15hours
SimultaneousTemperature,Rainfall,andWindSpeedRegression
• LLM:Llama-38B
• GPU:1×Nvidia3090,24GB
• Parameters: A-LLMP,40targetpoints,50samples
• Tasks: 6functions,100trials
• ApproximateTime: 31minutes
20H AdditionalConfigurationResults
H.1 ComparingSamplingandLogitBasedDistributions
Wefirstinvestigatewhetherourlogit-basedmethodofelicitingdistributions(AppendixA.2)match
the sampling distribution of the LLM. In order to estimate the true distribution, we obtain 1000
samplesfromtheLLMateachtargetlocation,andfitahistogramusingthesamebinsasourlogit-
basedmethod. FiguresH.2toH.4showthatourmethodyieldsadistributionthatisvisuallysimilar
totheoneobtainedbysampling.
(a)Llama-7B (b)Mixtral8×7B
FigureH.2:Visualizationofthepredictivedensitiesestimatedviasampling(middle)andmodellogits
(bottom)fortheSigmoidfunctionwith10trainingpoints(showninwhite). Crosssectionhistograms
(top)arepresentedatx=50,100and150.
(a)Llama-7B (b)Mixtral8×7B
Figure H.3: Visualization of the predictive densities estimated via sampling (middle) and model
logits(bottom)fortheQuadraticfunctionwith20trainingpoints(showninwhite). Crosssection
histograms(top)arepresentedatx=50,100and150.
21(a)Llama-7B (b)Mixtral8×7B
FigureH.4:Visualizationofthepredictivedensitiesestimatedviasampling(middle)andmodellogits
(bottom)fortheLinear+Cosinefunctionwith75trainingpoints(showninwhite). Crosssection
histograms(top)arepresentedatx=50,100and150.
22H.2 AdditionalPromptFormatResults
FigureH.5showsNLLandMAEforvariouspromptformatsand3LLMs. TablesH.1andH.2show
thetabularversionsofpromptformattingresults.
Overall, LLMPs tested are robust to the prompt format. The results indicate that two separators
arerequiredtoachievethebestperformance. Onetoseparatethexandyvalueswithinapairand
anothertoseparatethex,ypairs. The_,_formatusesacommatoseparatewithinapairandnothing
toseparatethepairsandithastheworstresults. Thex_y_formatusesletterprefixestoseparate
valuesandpairswithimprovedmetrics. Tradingofftokenefficiencyandperformance,_,_\nisthe
bestoptionasitusesonlyonecommatodelimitxandyand\ntodelimitx,ypairs. However,given
thatsomeregionsuseacommaasadecimalplace,weuse_,_\npromptformatinourexperiments
asitcomparableperformanceandonlyusesoneadditionalspaceperpair. The(_,_)andx=_,y=_\n
formatsaremorehumanreadable,buttheextratokensdonotimproveperformance.
Sigmoid - 10 Observed Quadratic - 20 Observed Linear+Cosine - 75 Observed
0.0 0 0.0 Prompt Formats
0.5 _,_
1.0 1 0.5 x_y_
_,_\n
_, _\n
(_, _)
0.05 0.05 0.10 x=_, y=_\n
0.05
0.00 0.00 0.00
Llama-2-7B Llama-2-70BMixtral-8x7B Llama-2-7B Llama-2-70BMixtral-8x7B Llama-2-7B Llama-2-70BMixtral-8x7B
FigureH.5: NLLandMAEforvariouspromptformatsandeachLLM.Theheightofeachbaristhe
meanof10randomseedsthatdeterminethelocationsoftheobservedpoints. Thesmallblacklines
atthetopofeachbarindicatesthestandarderror. Thetwo’_’charactersinthelegendindicatethe
positionsthexandyvalues. \nindicatesthenewlinecharacter. Fromlefttoright,thepromptsare
orderedfromthemosttoleasttokenefficient.
TableH.1: NLLforvariouspromptformatsandeachLLM.Eachentryisthemeanandstandard
errorof10randomseedsthatdeterminethelocationsoftheobservedpoints. Fromlefttoright,the
promptsareorderedfromthemosttoleasttokenefficient. Thenumberbeloweachfunctionindicates
thenumberofobservedpoints.
Function LLM _,_ x_y_ _,_\n _,_\n (_,_) x=_,y=_\n
Sigmoid Llama-2-7B -0.963±0.056 -0.768±0.072 -1.140±0.051 -1.194±0.055 -1.192±0.048 -1.116±0.055
10 Llama-2-70B -0.956±0.053 -0.897±0.104 -1.335±0.053 -1.329±0.056 -1.231±0.054 -1.293±0.072
Mixtral-8x7B -0.861±0.067 -0.940±0.069 -1.135±0.057 -1.276±0.066 -1.348±0.062 -1.306±0.067
Quadratic Llama-2-7B -0.882±0.036 -0.824±0.039 -1.269±0.032 -1.266±0.032 -1.293±0.029 -1.263±0.023
20 Llama-2-70B -0.980±0.035 -1.207±0.042 -1.482±0.034 -1.489±0.037 -1.445±0.032 -1.540±0.032
Mixtral-8x7B -0.976±0.028 -1.179±0.040 -1.371±0.033 -1.401±0.038 -1.459±0.039 -1.459±0.039
Linear+ Llama-2-7B -0.362±0.012 -0.445±0.022 -0.645±0.029 -0.632±0.034 -0.613±0.028 -0.676±0.033
Cosine Llama-2-70B -0.386±0.012 -0.611±0.027 -0.679±0.021 -0.673±0.024 -0.718±0.029 -0.769±0.030
75 Mixtral-8x7B -0.368±0.013 -0.600±0.029 -0.785±0.038 -0.778±0.036 -0.723±0.031 -0.782±0.030
23
LLN
EAMTableH.2: MeanAverageError(MAE)forvariouspromptformatsandeachLLM.Eachentryis
themeanandstandarderrorof10randomseedsthatdeterminethelocationsoftheobservedpoints.
Fromlefttoright,thepromptsareorderedfromthemosttoleasttokenefficient. Thenumberbelow
eachfunctionindicatesthenumberofobservedpoints.
Function LLM _,_ x_y_ _,_\n _,_\n (_,_) x=_,y=_\n
Sigmoid Llama-2-7B 0.062±0.004 0.069±0.006 0.056±0.005 0.061±0.004 0.060±0.004 0.053±0.004
10 Llama-2-70B 0.070±0.008 0.060±0.006 0.047±0.005 0.049±0.004 0.054±0.005 0.047±0.005
Mixtral-8x7B 0.071±0.006 0.058±0.005 0.052±0.005 0.047±0.005 0.046±0.003 0.045±0.004
Quadratic Llama-2-7B 0.075±0.005 0.070±0.004 0.062±0.004 0.059±0.004 0.051±0.002 0.056±0.002
20 Llama-2-70B 0.066±0.003 0.055±0.003 0.044±0.002 0.046±0.003 0.050±0.003 0.040±0.002
Mixtral-8x7B 0.065±0.003 0.051±0.003 0.047±0.002 0.049±0.003 0.048±0.003 0.045±0.003
Linear+ Llama-2-7B 0.122±0.004 0.112±0.002 0.093±0.004 0.097±0.005 0.088±0.004 0.085±0.004
Cosine Llama-2-70B 0.110±0.003 0.087±0.004 0.074±0.002 0.074±0.003 0.074±0.003 0.074±0.004
75 Mixtral-8x7B 0.119±0.003 0.092±0.005 0.079±0.004 0.080±0.004 0.083±0.004 0.075±0.004
24H.3 AdditionalPromptOrderingResults
WeconsidertheeffectofthreedifferentorderingsofthetrainingdataD intheprompt:
train
• Sequential: (x ,y ),∈D areorderedsequentiallyfromsmallesttolargestx ,regardless
i i train i
ofthelocationofthetargetpoint.
• Random: (x ,y ),∈D arerandomlyordered.
i i train
• Distance: For the prediction at target point x∗, the training points (x ,y ),∈ D are
i i train
orderedfromlargesttosmallestdistancetothequerypointx∗i.e. |x∗ −x | suchthatthe
n i 2
trainingpointsclosertox∗appearlaterintheprompt.
FigureH.6showsNLLandMAEforvariouspromptorderingsandeachLLM.TableH.3showsthe
tabularversionoftheresults.
Distance ordering consistently yields the best results overall. We posit that distance ordering is
effectiveasitprovidesahinttotheLLMtoweighthecontributionofcloserpointstothecurrent
targetpointtoagreaterdegree. Unlessotherwisenoted,weusedistanceorderingforourexperiments.
Sigmoid - 10 Observed Quadratic - 20 Observed Linear+Cosine - 75 Observed
0.0 0 0.5 Prompt Orders
0.5 0.0 distance 1.0 1 0.5 r sa en qd uo em ntial
0.2
0.2
0.1
0.1 0.1
0.0 0.0 0.0
Llama-2-7B Llama-2-70B Mixtral-8x7B Llama-2-7B Llama-2-70B Mixtral-8x7B Llama-2-7B Llama-2-70B Mixtral-8x7B
FigureH.6: NLLandMAEforvariouspromptorderingsandeachLLM.Theheightofeachbaris
themeanof10randomseedsthatdeterminethelocationsoftheobservedpoints. Thesmallblack
linesatthetopofeachbarindicatesthestandarderror.
TableH.3: MeanAverageError(MAE)andNLLforvariouspromptorderingsandeachLLM.Each
entryisthemeanandstandarderrorof10randomseedsthatdeterminethelocationsoftheobserved
points. Thenumberbeloweachfunctionindicatesthenumberofobservedpoints.
Distance Random Sequential
Function LLM MAE↓ NLL↓ MAE↓ NLL↓ MAE↓ NLL↓
Sigmoid Llama-2-7B 0.060±0.004 -1.194±0.055 0.093±0.017 -0.977±0.063 0.150±0.016 -0.597±0.059
10 Llama-2-70B 0.049±0.004 -1.329±0.056 0.051±0.004 -1.307±0.066 0.086±0.016 -0.782±0.085
Mixtral-8x7B 0.050±0.005 -1.276±0.066 0.060±0.006 -1.240±0.077 0.073±0.016 -0.707±0.116
Quadratic Llama-2-7B 0.063±0.004 -1.266±0.032 0.146±0.007 -0.731±0.034 0.224±0.012 -0.147±0.019
20 Llama-2-70B 0.046±0.003 -1.490±0.037 0.099±0.009 -1.013±0.055 0.182±0.014 -0.368±0.035
Mixtral-8x7B 0.049±0.003 -1.401±0.038 0.095±0.011 -1.066±0.074 0.246±0.016 -0.117±0.053
Linear+ Llama-2-7B 0.092±0.003 -0.632±0.034 0.205±0.003 -0.086±0.015 0.213±0.004 0.445±0.022
Cosine Llama-2-70B 0.074±0.003 -0.673±0.024 0.189±0.008 -0.058±0.025 0.178±0.004 0.361±0.018
75 Mixtral-8x7B 0.080±0.004 -0.778±0.036 0.204±0.004 -0.114±0.027 0.154±0.006 0.410±0.034
25
LLN
EAMH.4 AdditionalPrompty-ScalingResults
Inthisexperiment,weexaminetheeffectofthemagnitudeandsignofthey-valuesofthetaskgiven
totheLLMwhennoothercontextualinformationisprovided. Wetakethesamethreesynthetic
examplesbutscalethey-valuestobeintheranges[0,1],[−1,1],[0,10]and[−1000,1000].
FigureH.7showsNLLandMAEforvariousprompty-scalingandeachLLM.TableH.4showsthe
tabularresults. TherawvaluesgiventotheLLMarescaledmeaningtheobservationnoiseisscaled
accordingly. WehavescaledthelikelihoodsandMAEvaluestocompensateforthedifferencein
range. Accordingtotheevaluationmetricsweobservethatperformancedegradeswithincreased
rangeandincorporatingnegativevaluesalsohurtsMAE.Thisisduetothefactthatwhentherangeis
wider,theLLMmustaccuratelygeneratemorenumericaldigitsandpotentiallyanegativesignwhen
predictingf(x∗).
Sigmoid - 10 Observed Quadratic - 20 Observed Linear+Cosine - 75 Observed
Prompt Scales
2 5.0 [0, 1]
5 1 2.5 [-1, 1]
[0, 10]
0 0 0.0 [-1000, 1000]
0.05 0.05 0.05
0.00 0.00 0.00
Llama-2-7B Llama-2-70B Mixtral-8x7B Llama-2-7B Llama-2-70B Mixtral-8x7B Llama-2-7B Llama-2-70B Mixtral-8x7B
FigureH.7: NLLandMAEforvariousprompty-scalingsandeachLLM.Theheightofeachbaris
themeanof10randomseedsthatdeterminethelocationsoftheobservedpoints. Thesmallblack
linesatthetopofeachbarindicatesthestandarderror.
TableH.4: MAEandNLLforvariousy-scalingrangesandthreeLLMs. Eachentryisthemeanand
standarderrorof10randomseedsthatdeterminethelocationsoftheobservedpoints. Thenumber
beloweachfunctionindicatesthenumberofobservedpoints.
[0,1] [-1,1] [0,10] [-1000,1000]
Function LLM MAE↓ NLL↓ MAE↓ NLL↓ MAE↓ NLL↓ MAE↓ NLL↓
Sigmoid Llama-2-7B 0.067+/-0.004 0.212+/-0.053 0.061+/-0.004 1.327+/-0.057 0.068+/-0.006 2.701+/-0.075 0.070+/-0.006 8.087+/-0.173
10 Llama-2-70B 0.049+/-0.004 0.086+/-0.049 0.054+/-0.005 1.246+/-0.066 0.050+/-0.005 2.565+/-0.062 0.070+/-0.008 8.036+/-0.210
Mixtral-8x7B 0.050+/-0.004 0.120+/-0.065 0.061+/-0.007 1.343+/-0.065 0.051+/-0.005 2.502+/-0.085 0.064+/-0.006 7.668+/-0.212
Quadratic Llama-2-7B 0.061+/-0.004 0.624+/-0.066 0.066+/-0.004 1.372+/-0.048 0.063+/-0.005 0.788+/-0.061 0.067+/-0.005 2.524+/-0.041
20 Llama-2-70B 0.047+/-0.003 0.324+/-0.049 0.054+/-0.003 1.176+/-0.047 0.052+/-0.003 0.669+/-0.063 0.054+/-0.003 1.874+/-0.052
Mixtral-8x7B 0.049+/-0.003 0.417+/-0.040 0.056+/-0.003 1.175+/-0.059 0.061+/-0.004 0.702+/-0.072 0.056+/-0.003 1.883+/-0.082
Linear+ Llama-2-7B 0.065+/-0.002 0.339+/-0.032 0.071+/-0.003 1.374+/-0.036 0.075+/-0.003 2.513+/-0.034 0.084+/-0.005 6.130+/-0.156
Cosine Llama-2-70B 0.053+/-0.003 0.276+/-0.039 0.056+/-0.003 1.453+/-0.033 0.057+/-0.002 2.245+/-0.041 0.061+/-0.003 5.709+/-0.163
75 Mixtral-8x7B 0.056+/-0.003 0.193+/-0.036 0.055+/-0.003 1.199+/-0.035 0.060+/-0.003 1.999+/-0.066 0.060+/-0.002 5.036+/-0.196
However,observingtheplotsinFigureH.8ofthepredictivedistributiononeachscale,themodel
givesreasonablepredictionsregardlessofscale.IfnoscenariocontextisprovidedviatexttotheLLM,
rescalingtaskvaluestobeapproximatelybetween0and1improvesperformanceinourexperiments.
However,ingeneralweuseunscaleddatasothatwecanexaminethepriorbeliefslearnedbythe
LLMabouttaskscommunicatedthroughtherawvalues.
26
EAM
LLNFigureH.8: PredictivedistributionsgivenbytheMixtral-8×7BLLMonscaledLinear+Coswith
75observations. Thisexampleexhibitedoneofthelargestvariationinmetricsasaresultofscaling.
Despitethis,allpredictivedistributionslookreasonable.
27H.5 top-pandtemperatureresults
FigureH.9showshowMAEvarieswithLLMtop-pandtemperature. TableH.5showsthetabular
versionoftheresults.
Surprisingly,allLLM’sareinsensitivetotemperatureandtop-pwithrespecttoMAE.
ThoughnotevidentfromtheseMAEresults,wesometimesobservedthatusingatop-pof1.0can
resultinsomeextremevaluesinsamples. However,weconsidertemperature=1.0,andtop-p=1.0
closesttothedefaultdistributiongivenbytheLLM.Sinceithadcompetitiveperformancewiththe
otheroptions,weusethesesettingstocomputelog-likelihoodsinourexperimentswhichallowsusto
examinethedefaultcharacteristicsoftheLLM’spredictivedistribution.
Sigmoid - 10 Observed Points Quadratic - 20 Observed Points Linear+Cosine - 75 Observed Points Top-p and Temperature
0.04 0.10 0.06 0 05 5, , 0 05 7 0 09 9, , 0 05 7
0.03 0.04 05, 09 09, 09
0.02 0.05 05, 10 09, 10
0.01 0.02 07, 05 10, 05
07, 07 10, 07
0.00 llama-7B llama-70B mixtral 0.00 llama-7B llama-70B mixtral 0.00 llama-7B llama-70B mixtral 0 07 7,
,
0 19
0
1 10 0,
,
0 19
0
FigureH.9: MAE(lowerisbetter)forvarioustemperatureandtop-psettingsandeachLLM.All
LLM’sarerelativelyinsensitivetotemperatureandtoppwithrespecttoMAE.
TableH.5: MAE(lowerisbetter)forvarioustop-pandtemperaturesettingsandallLLMs.
Temperature=0.5 Temperature=0.7 Temperature=0.9 Temperature=1.0
Function LLM p=0.5 p=0.7 p=0.9 p=1.0 p=0.5 p=0.7 p=0.9 p=1.0 p=0.5 p=0.7 p=0.9 p=1.0 p=0.5 p=0.7 p=0.9 p=1.0
Sigmoid L-7B 0.0329 0.033 0.0328 0.0329 0.0328 0.0331 0.0337 0.0351 0.0331 0.0322 0.0334 0.035 0.035 0.0345 0.0331 0.0339
Mix 0.0439 0.0436 0.0434 0.042 0.0441 0.0419 0.0427 0.0406 0.0404 0.0426 0.0412 0.0394 0.0414 0.0425 0.0426 0.0421
L-70B 0.045 0.0446 0.0439 0.0429 0.0459 0.0429 0.0417 0.0407 0.0459 0.0396 0.0409 0.0422 0.0452 0.0429 0.041 0.041
Square L-7B 0.089 0.0886 0.0918 0.0906 0.091 0.0931 0.0926 0.089 0.0955 0.0911 0.0899 0.0846 0.0951 0.09 0.0941 0.0888
Mix 0.094 0.0952 0.0961 0.0986 0.0914 0.0919 0.0945 0.094 0.0938 0.0951 0.0954 0.0982 0.092 0.0958 0.0941 0.0942
L-70B 0.1031 0.0991 0.1031 0.1077 0.1011 0.1015 0.1067 0.1052 0.1025 0.1066 0.1082 0.1066 0.1059 0.1071 0.1104 0.1152
Linear+ L-7B 0.0524 0.0554 0.056 0.0544 0.052 0.0546 0.0561 0.0551 0.0525 0.0561 0.0541 0.0583 0.0553 0.058 0.055 0.0544
Cosine Mix 0.0691 0.0686 0.0696 0.0674 0.0662 0.0674 0.0674 0.0689 0.0664 0.0671 0.07 0.0709 0.0671 0.0699 0.0648 0.0685
L-70B 0.0661 0.0645 0.0713 0.0701 0.0669 0.0681 0.0728 0.075 0.0662 0.0729 0.0781 0.0785 0.0709 0.0703 0.0826 0.0805
28
EAMH.6 AdditionalAutoregressiveSamplingResults
FigureH.10showsNLLandMAEofrandomanddistancetrainingpointorderingsforA-LLMPand
I-LLMPandeachLLM.TableH.6showsthetabularresults.
Sigmoid - 10 Observed Quadratic - 20 Observed Linear+Cosine - 75 Observed
0 0 0.0 Prompt Orders
Random - I-LLMP
1 1 0.5 Distance - I-LLMP
2 2 1.0 R Da isn td ao nm ce - - A A- -L LL LM MP P
0.2 0.2
0.1 0.1 0.1
0.0 0.0 0.0
Llama-2-7B Llama-2-70B Mixtral-8x7B Llama-2-7B Llama-2-70B Mixtral-8x7B Llama-2-7B Llama-2-70B Mixtral-8x7B
FigureH.10: NLLandMAEforvariousprompty-scalingsandeachLLM.Theheightofeachbar
isthemeanof3randomseedsthatdeterminethelocationsoftheobservedpoints. Thesmallblack
linesatthetopofeachbarindicatesthestandarderror.
TableH.6: MeanAverageError(MAE)andNegativeLogLikelihood(NLL)forautoregressiveand
marginalsamplingwithtwodifferentpromptorderingsandthreeLLMs.
RandomIND-LLMP DistanceIND-LLMP RandomAUTO-LLMP DistanceAUTO-LLMP
Function LLM MAE↓ NLL↓ MAE↓ NLL↓ MAE↓ NLL↓ MAE↓ NLL↓
Sigmoid Llama-2-7B 0.125±0.035 -0.829±0.061 0.070±0.005 -1.035±0.070 0.076±0.009 -1.843±0.052 0.067±0.016 -1.940±0.031
10 Llama-2-70B 0.061±0.008 -1.303±0.098 0.064±0.007 -1.257±0.016 0.060±0.006 -2.252±0.034 0.070±0.010 -2.162±0.019
Mixtral-8x7B 0.073±0.008 -1.082±0.040 0.070±0.005 -1.153±0.012 0.089±0.017 -2.196±0.023 0.065±0.009 -2.217±0.012
Quadratic Llama-2-7B 0.156±0.010 -0.769±0.044 0.062±0.006 -1.347±0.042 0.196±0.012 -1.184±0.030 0.064±0.007 -1.795±0.049
20 Llama-2-70B 0.081±0.004 -1.190±0.069 0.046±0.001 -1.634±0.018 0.068±0.004 -1.897±0.034 0.051±0.003 -1.924±0.018
Mixtral-8x7B 0.070±0.008 -1.261±0.103 0.053±0.005 -1.514±0.008 0.074±0.013 -1.900±0.054 0.049±0.005 -1.970±0.013
Linear+ Llama-2-7B 0.203±0.001 -0.076±0.030 0.093±0.001 -0.618±0.031 0.209±0.005 -0.116±0.031 0.102±0.003 -0.799±0.042
Cosine Llama-2-70B 0.172±0.015 -0.104±0.043 0.070±0.004 -0.685±0.031 0.173±0.011 -0.405±0.046 0.072±0.004 -0.968±0.058
75 Mixtral-8x7B 0.215±0.003 -0.030±0.020 0.081±0.007 -0.766±0.056 0.220±0.005 -0.111±0.059 0.080±0.006 -0.931±0.063
29
LLN
EAMH.7 AdditionalAutoregressiveProcessResults
Figure H.11 shows the MAE results for the autoregressive process experiments. Figures H.12
andH.13showtheAvglogp(y)andMAEfor10differentorderingsofthequerypoints.
Sigmoid-10 Quadratic-20 Lin+Cos-75
0.15
0.06
0.06
0.10
0.04
0.04
0.05
0.02
0.02
0.00 0.00 0.00
Independent Ground Truth
Sorted Test Random Test
FigureH.11: AutoregressiveprocessMAEresults.
Sigmoid - 10 Observed Quadratic - 20 Observed Linear+Cosine - 75 Observed
Random Seed
2.0 0.8 2.0 seed 1
seed 2
1.5 0.6 1.5 seed 3
seed 4
1.0 0.4 1.0 seed 5
seed 6
0.5 0.2 0.5 seed 7
seed 8
0.0 Llama-2-7B Llama-2-70B Mixtral-8x7B 0.0 Llama-2-7B Llama-2-70B Mixtral-8x7B 0.0 Llama-2-7B Llama-2-70B Mixtral-8x7B s se ee ed
d
9
1 0
FigureH.12: Avglogp(y)forthe10seedsforeachLLMfortheautoregressiveprocessexperiment.
Sigmoid - 10 Observed Quadratic - 20 Observed Linear+Cosine - 75 Observed
0.04 0.12 0.06 Random Seed
0.03 00 .. 01 80 00 .. 00 45 s s se e ee e ed d
d
1 2
3
0.02 0.06 0.03 seed 4
seed 5
0.04 0.02 seed 6
0.01 0.02 0.01 seed 7
seed 8
0.00 Llama-2-7B Llama-2-70B Mixtral-8x7B 0.00 Llama-2-7B Llama-2-70B Mixtral-8x7B 0.00 Llama-2-7B Llama-2-70B Mixtral-8x7B s se ee ed
d
9
1 0
FigureH.13: MAEforthe10seedsforeachLLMfortheautoregressiveprocessexperiment.
30
)y(p
gol
gvA
EAM
EAM
B7L B07L xiM B7L B07L xiM B7L B07L xiMI AdditionalLLMPPerformanceDetailsandResults
I.1 AdditionalComparisontoGaussianProcesses(GP)Results
FiguresI.14toI.25showsregressionresultsfromtheMixtral-8×7BLLMandanRBFkernelGPfor
the12differentsyntheticfunctions.
True Function LLM Confidence Median Training points GP Confidence Mean
Beat Inference 05 Mixtral MAE=0.321 NLL=-0.952 Beat Inference 05 GP MAE=0.423 NLL=1.003
2
0.5
1
0.0
0
0.5
1
Beat Inference 10 Mixtral MAE=0.346 NLL=-0.779 Beat Inference 10 GP MAE=0.430 NLL=5.382
1.5
1.0
1.0
0.5
0.5
0.0
0.0
0.5 0.5
1.0 1.0
Beat Inference 15 Mixtral MAE=0.332 NLL=-0.635 Beat Inference 15 GP MAE=0.359 NLL=0.668
1.0
1.0
0.5
0.5
0.0 0.0
0.5 0.5
Beat Inference 20 Mixtral MAE=0.304 NLL=-0.648 Beat Inference 20 GP MAE=0.301 NLL=0.613
1.0
0.5
0.5
0.0 0.0
0.5 0.5
1.0
Beat Inference 25 Mixtral MAE=0.302 NLL=-0.677 Beat Inference 25 GP MAE=0.289 NLL=0.603
1.0
0.5
0.5
0.0 0.0
0.5 0.5
1.0
Beat Inference 50 Mixtral MAE=0.295 NLL=-0.444 Beat Inference 50 GP MAE=0.304 NLL=0.628
1.0
0.5 0.5
0.0 0.0
0.5 0.5
Beat Inference 75 Mixtral MAE=0.296 NLL=-0.443 Beat Inference 75 GP MAE=0.280 NLL=0.569
1.0
1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.14: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheBeatfunction. TheGPusesanRBFkernel
withoptimizedlengthscaleandnoise.
31True Function LLM Confidence Median Training points GP Confidence Mean
Exponential 05 Mixtral MAE=0.198 NLL=-1.338 Exponential 05 GP MAE=1.462 NLL=2.025
8
6
6
4
4
2
2
0
0
Exponential 10 Mixtral MAE=0.101 NLL=-1.339 Exponential 10 GP MAE=0.120 NLL=-0.828
6 6
4 4
2 2
0
0
Exponential 15 Mixtral MAE=0.068 NLL=-1.400 Exponential 15 GP MAE=0.073 NLL=-1.579
6 6
4 4
2
2
0
0
Exponential 20 Mixtral MAE=0.059 NLL=-1.489 Exponential 20 GP MAE=0.056 NLL=-1.472
6 6
4 4
2 2
0
0
Exponential 25 Mixtral MAE=0.058 NLL=-1.413 Exponential 25 GP MAE=0.057 NLL=-1.572
6 6
4 4
2
2
0
0
Exponential 50 Mixtral MAE=0.049 NLL=-1.565 Exponential 50 GP MAE=0.042 NLL=-1.688
8
6
6
4 4
2 2
0 0
Exponential 75 Mixtral MAE=0.036 NLL=-1.665 Exponential 75 GP MAE=0.021 NLL=-1.846
6 6
4 4
2 2
0 0
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.15: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheExponentialfunction. TheGPusesanRBF
kernelwithoptimizedlengthscaleandnoise.
32True Function LLM Confidence Median Training points GP Confidence Mean
Gaussian Wave 05 Mixtral MAE=0.271 NLL=0.191 Gaussian Wave 05 GP MAE=0.273 NLL=0.615
1 1
0 0
1 1
Gaussian Wave 10 Mixtral MAE=0.282 NLL=0.240 Gaussian Wave 10 GP MAE=0.270 NLL=0.473
1.0
1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
Gaussian Wave 15 Mixtral MAE=0.266 NLL=0.101 Gaussian Wave 15 GP MAE=0.257 NLL=-0.013
1.5
1.0 1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
Gaussian Wave 20 Mixtral MAE=0.277 NLL=0.011 Gaussian Wave 20 GP MAE=0.236 NLL=-0.190
1.5
1 1.0
0.5
0 0.0
0.5
1 1.0
Gaussian Wave 25 Mixtral MAE=0.263 NLL=-0.152 Gaussian Wave 25 GP MAE=0.232 NLL=-0.360
1.5
1.0 1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
Gaussian Wave 50 Mixtral MAE=0.146 NLL=-0.545 Gaussian Wave 50 GP MAE=0.145 NLL=-0.950
1.0 1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
Gaussian Wave 75 Mixtral MAE=0.141 NLL=-0.735 Gaussian Wave 75 GP MAE=0.078 NLL=-1.365
1 1.0
0.5
0 0.0
0.5
1
1.0
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.16: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheGaussianWavefunction. TheGPusesan
RBFkernelwithoptimizedlengthscaleandnoise.
33True Function LLM Confidence Median Training points GP Confidence Mean
Linear 05 Mixtral MAE=0.060 NLL=-2.864 Linear 05 GP MAE=0.501 NLL=0.874
1 1.0
0 0.5
1 0.0
2 0.5
1.0
3
Linear 10 Mixtral MAE=0.056 NLL=-2.549 Linear 10 GP MAE=0.048 NLL=-1.272
1 1.0
0.5
0
0.0
1 0.5
1.0
2
Linear 15 Mixtral MAE=0.064 NLL=-2.302 Linear 15 GP MAE=0.041 NLL=-1.484
1.5
1 1.0
0 0.5
0.0
1 0.5
1.0
2
Linear 20 Mixtral MAE=0.051 NLL=-2.129 Linear 20 GP MAE=0.028 NLL=-1.613
1.5
1 1.0
0.5
0
0.0
0.5
1
1.0
Linear 25 Mixtral MAE=0.057 NLL=-2.004 Linear 25 GP MAE=0.029 NLL=-1.701
1 1
0
0
1
1
2
Linear 50 Mixtral MAE=0.033 NLL=-1.903 Linear 50 GP MAE=0.029 NLL=-1.788
1.0 1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
Linear 75 Mixtral MAE=0.033 NLL=-1.914 Linear 75 GP MAE=0.018 NLL=-1.902
1.0 1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.17: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusaGP
asafunctionofthenumberofobservedpointsfortheLinearfunction. TheGPusesanRBFkernel
withoptimizedlengthscaleandnoise.
34True Function LLM Confidence Median Training points GP Confidence Mean
Linear+Cosine 05 Mixtral MAE=0.255 NLL=0.260 Linear+Cosine 05 GP MAE=0.276 NLL=0.351
2.0 1.5
1.5 1.0
1.0
0.5
0.5
0.0
0.0
Linear+Cosine 10 Mixtral MAE=0.228 NLL=0.174 Linear+Cosine 10 GP MAE=0.237 NLL=-0.095
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
Linear+Cosine 15 Mixtral MAE=0.217 NLL=0.087 Linear+Cosine 15 GP MAE=0.192 NLL=-0.187
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
Linear+Cosine 20 Mixtral MAE=0.204 NLL=0.067 Linear+Cosine 20 GP MAE=0.157 NLL=-0.566
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
Linear+Cosine 25 Mixtral MAE=0.207 NLL=-0.118 Linear+Cosine 25 GP MAE=0.137 NLL=-0.846
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
Linear+Cosine 50 Mixtral MAE=0.131 NLL=-0.608 Linear+Cosine 50 GP MAE=0.073 NLL=-1.243
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
Linear+Cosine 75 Mixtral MAE=0.091 NLL=-0.794 Linear+Cosine 75 GP MAE=0.040 NLL=-1.635
1.5
1.5
1.0
1.0
0.5
0.5
0.0 0.0
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.18: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheLinear+Cosinefunction. TheGPusesan
RBFkernelwithoptimizedlengthscaleandnoise.
35True Function LLM Confidence Median Training points GP Confidence Mean
Log 05 Mixtral MAE=0.059 NLL=-2.134 Log 05 GP MAE=0.490 NLL=0.927
0
0
2
1
4
2
6
Log 10 Mixtral MAE=0.050 NLL=-1.898 Log 10 GP MAE=0.056 NLL=-1.194
0
0
1
2 1
3
2
4
Log 15 Mixtral MAE=0.050 NLL=-1.955 Log 15 GP MAE=0.057 NLL=-1.446
0
0
1
2 1
3 2
Log 20 Mixtral MAE=0.042 NLL=-1.894 Log 20 GP MAE=0.039 NLL=-1.579
0
0
1
1
2
2
Log 25 Mixtral MAE=0.041 NLL=-1.940 Log 25 GP MAE=0.039 NLL=-1.675
0
0
1
1
2
3 2
Log 50 Mixtral MAE=0.034 NLL=-1.853 Log 50 GP MAE=0.033 NLL=-1.735
0.0 0.0
0.5 0.5
1.0 1.0
1.5 1.5
2.0 2.0
2.5
Log 75 Mixtral MAE=0.030 NLL=-1.888 Log 75 GP MAE=0.020 NLL=-1.892
0.0 0.0
0.5 0.5
1.0 1.0
1.5 1.5
2.0 2.0
2.5
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.19: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheLogfunction. TheGPusesanRBFkernel
withoptimizedlengthscaleandnoise.
36True Function LLM Confidence Median Training points GP Confidence Mean
Quadratic 05 Mixtral MAE=0.224 NLL=-2.263 Quadratic 05 GP MAE=0.250 NLL=0.219
1.5 1.0
1.0
0.5 0.5
0.0
0.0
0.5
1.0
Quadratic 10 Mixtral MAE=0.087 NLL=-1.807 Quadratic 10 GP MAE=0.060 NLL=-1.400
1.0
1.0
0.5 0.5
0.0 0.0
Quadratic 15 Mixtral MAE=0.052 NLL=-1.941 Quadratic 15 GP MAE=0.092 NLL=0.050
1.5
1.0
1.0
0.5
0.5
0.0
0.0
Quadratic 20 Mixtral MAE=0.044 NLL=-2.000 Quadratic 20 GP MAE=0.039 NLL=-1.639
1.00
1.0
0.75
0.5 0.50
0.25
0.0 0.00
Quadratic 25 Mixtral MAE=0.041 NLL=-2.039 Quadratic 25 GP MAE=0.042 NLL=-1.726
2.0
1.00
1.5 0.75
1.0 0.50
0.5 0.25
0.0 0.00
Quadratic 50 Mixtral MAE=0.027 NLL=-2.118 Quadratic 50 GP MAE=0.021 NLL=-1.856
1.00
1.0 0.75
0.50
0.5
0.25
0.00
0.0
Quadratic 75 Mixtral MAE=0.028 NLL=-2.086 Quadratic 75 GP MAE=0.015 NLL=-1.949
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.20: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheQuadraticfunction. TheGPusesanRBF
kernelwithoptimizedlengthscaleandnoise.
37True Function LLM Confidence Median Training points GP Confidence Mean
Sigmoid 05 Mixtral MAE=0.023 NLL=-2.365 Sigmoid 05 GP MAE=0.360 NLL=0.472
1.5
1.0
1.0
0.5
0.5
0.0
0.0
Sigmoid 10 Mixtral MAE=0.046 NLL=-2.202 Sigmoid 10 GP MAE=0.037 NLL=-1.493
1.5
1.0 1.0
0.5 0.5
0.0 0.0
Sigmoid 15 Mixtral MAE=0.042 NLL=-2.215 Sigmoid 15 GP MAE=0.034 NLL=-1.657
1.5 1.5
1.0 1.0
0.5
0.5
0.0
0.0
0.5
Sigmoid 20 Mixtral MAE=0.042 NLL=-2.171 Sigmoid 20 GP MAE=0.026 NLL=-1.701
1.0 1.0
0.5 0.5
0.0 0.0
Sigmoid 25 Mixtral MAE=0.041 NLL=-2.248 Sigmoid 25 GP MAE=0.029 NLL=-1.781
1.5
1.0 1.0
0.5 0.5
0.0 0.0
Sigmoid 50 Mixtral MAE=0.024 NLL=-2.160 Sigmoid 50 GP MAE=0.024 NLL=-1.847
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
Sigmoid 75 Mixtral MAE=0.028 NLL=-2.092 Sigmoid 75 GP MAE=0.015 NLL=-1.940
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.21: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheSigmoidfunction. TheGPusesanRBF
kernelwithoptimizedlengthscaleandnoise.
38True Function LLM Confidence Median Training points GP Confidence Mean
Sinc 05 Mixtral MAE=0.094 NLL=-1.354 Sinc 05 GP MAE=0.098 NLL=1.876
1.0 1.00
0.5 0.75
0.50
0.0
0.25
0.5
0.00
1.0 0.25
Sinc 10 Mixtral MAE=0.087 NLL=-1.243 Sinc 10 GP MAE=0.089 NLL=0.030
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0.25
0.25
Sinc 15 Mixtral MAE=0.087 NLL=-1.206 Sinc 15 GP MAE=0.093 NLL=1.140
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0.25
0.25
Sinc 20 Mixtral MAE=0.087 NLL=-1.233 Sinc 20 GP MAE=0.079 NLL=0.496
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0.25 0.25
Sinc 25 Mixtral MAE=0.085 NLL=-1.256 Sinc 25 GP MAE=0.079 NLL=0.859
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
0.25 0.25
Sinc 50 Mixtral MAE=0.073 NLL=-1.402 Sinc 50 GP MAE=0.043 NLL=-1.314
1.00
2
0.75
1 0.50
0.25
0 0.00
0.25
1
Sinc 75 Mixtral MAE=0.027 NLL=-1.616 Sinc 75 GP MAE=0.022 NLL=-1.849
1.0 1.0
0.5 0.5
0.0 0.0
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.22: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheSincfunction. TheGPusesanRBFkernel
withoptimizedlengthscaleandnoise.
39True Function LLM Confidence Median Training points GP Confidence Mean
Sine 05 Mixtral MAE=0.656 NLL=-1.118 Sine 05 GP MAE=0.623 NLL=1.077
2
1
1
0
0
1 1
Sine 10 Mixtral MAE=0.641 NLL=-0.820 Sine 10 GP MAE=0.529 NLL=0.928
1 1
0 0
1
1
2
Sine 15 Mixtral MAE=0.596 NLL=-0.646 Sine 15 GP MAE=0.468 NLL=0.520
1 1
0 0
1
1
Sine 20 Mixtral MAE=0.621 NLL=-0.564 Sine 20 GP MAE=0.437 NLL=0.243
1.5
1.0 1
0.5
0.0 0
0.5 1
1.0
Sine 25 Mixtral MAE=0.572 NLL=-0.557 Sine 25 GP MAE=0.382 NLL=-0.106
1.0 1
0.5
0
0.0
0.5 1
1.0
Sine 50 Mixtral MAE=0.396 NLL=-0.720 Sine 50 GP MAE=0.210 NLL=-0.908
1.0 2
0.5 1
0.0 0
0.5
1
1.0
Sine 75 Mixtral MAE=0.241 NLL=-0.825 Sine 75 GP MAE=0.063 NLL=-1.468
1.0 1.0
0.5 0.5
0.0 0.0
0.5
0.5
1.0
1.0
1.5
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.23: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheSinefunction. TheGPusesanRBFkernel
withoptimizedlengthscaleandnoise.
40True Function LLM Confidence Median Training points GP Confidence Mean
X × Sine 05 Mixtral MAE=0.321 NLL=0.660 X × Sine 05 GP MAE=0.316 NLL=5.104
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
X × Sine 10 Mixtral MAE=0.321 NLL=0.567 X × Sine 10 GP MAE=0.308 NLL=0.530
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
X × Sine 15 Mixtral MAE=0.315 NLL=0.419 X × Sine 15 GP MAE=0.307 NLL=0.534
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
X × Sine 20 Mixtral MAE=0.314 NLL=0.343 X × Sine 20 GP MAE=0.274 NLL=0.394
1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0 1.0
X × Sine 25 Mixtral MAE=0.303 NLL=0.246 X × Sine 25 GP MAE=0.185 NLL=-0.006
1.0
0.5
0.5
0.0 0.0
0.5 0.5
1.0 1.0
X × Sine 50 Mixtral MAE=0.163 NLL=-0.423 X × Sine 50 GP MAE=0.121 NLL=-0.857
0.5 0.5
0.0
0.0
0.5
0.5
1.0
1.0
X × Sine 75 Mixtral MAE=0.111 NLL=-0.732 X × Sine 75 GP MAE=0.038 NLL=-1.408
1.0
0.5 0.5
0.0 0.0
0.5 0.5
1.0
1.0
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.24: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheX×Sinefunction. TheGPusesanRBF
kernelwithoptimizedlengthscaleandnoise.
41True Function LLM Confidence Median Training points GP Confidence Mean
Linear × Sine 05 Mixtral MAE=23.266 NLL=4.317 Linear × Sine 05 GP MAE=20.867 NLL=28.218
50 40
25 20
0 0
25 20
50 40
Linear × Sine 10 Mixtral MAE=18.735 NLL=4.149 Linear × Sine 10 GP MAE=17.924 NLL=18.019
50 40
25 20
0 0
25 20
40
50
Linear × Sine 15 Mixtral MAE=18.598 NLL=3.970 Linear × Sine 15 GP MAE=16.124 NLL=15.754
50 40
25 20
0 0
25 20
50 40
Linear × Sine 20 Mixtral MAE=16.527 NLL=3.773 Linear × Sine 20 GP MAE=14.315 NLL=13.041
40
40
20 20
0 0
20 20
40 40
Linear × Sine 25 Mixtral MAE=16.223 NLL=3.514 Linear × Sine 25 GP MAE=12.801 NLL=11.393
50 40
25 20
0 0
20
25
40
50
Linear × Sine 50 Mixtral MAE=6.799 NLL=2.259 Linear × Sine 50 GP MAE=9.061 NLL=8.357
60
40
40
20
20
0 0
20 20
40 40
Linear × Sine 75 Mixtral MAE=2.476 NLL=1.612 Linear × Sine 75 GP MAE=4.968 NLL=4.430
40 40
20 20
0 0
20 20
40 40
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Number of Observed Points
FigureI.25: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfortheLinear×Sinefunction. TheGPusesan
RBFkernelwithoptimizedlengthscaleandnoise.
42FigureI.26showsplotofNLLandMAEfortheMixtral-8×7BLLMandtheRBFkernelGPfor12
forthe12differentsyntheticfunctions.
GP MAE Mixtral Autoregressive MAE GP ANLL Mixtral Autoregressive ANLL
Sigmoid Quadratic Linear+Cosine
0.3
0.3 0 0.2 0 0.2 0 0.2 1 1
0.1 0.1 0.1 1
2
0.0 2
Linear Log Exponential
1 1.5 2
0.4 0 0.4
0 1.0
0
0.2 2 0.2 1 0.5
0.0 0.0 2 0.0 2
Sinc Linear × Sine Gaussian Wave
0.100 20 1
0.075 01 15 20 0.2 0
0.050 1 1 50 10 0.1 1
0.025 2
Sine X × Sine Beat Inference
0.6 1 0.3 2 0.40 2
0.4 0 0.2 0.35 1
0
0.2 1 0.1 0.30 0
1
10 20 30 40 50 60 70 10 20 30 40 50 60 70 10 20 30 40 50 60 70
Number of Observed Points
FigureI.26: MAE(lowerisbetter)andNLL(lowerisbetter)fortheMixtral-8×7BLLMversusa
GPasafunctionofthenumberofobservedpointsfor12differentsyntheticfunctions. Resultsare
averagedoverthreesetsofrandomsamplesfortheobservedpoints. TheGPusesanRBFkernelwith
optimizedlengthscaleandnoise.
43
EAM
EAM
EAM
EAM
LLN
LLN
LLN
LLN
EAM
EAM
EAM
EAM
LLN
LLN
LLN
LLN
EAM
EAM
EAM
EAM
LLN
LLN
LLN
LLNI.2 MultimodalPredictiveExperimentDetails
To verify that LLMPs are able to produce non-Gaussian, multimodal predictive distributions we
sampledtrainingdatafromthefollowingsynthetic,bimodalgenerativedistribution:
.05
y = +0.02x+ϵ (0.02x+0.08)+0.03ϵ (I.1)
1+exp−x 1 2
Whereϵ ∼ Bernoulli(p = 0.5)andϵ ∼ N(0,1). TheLlama-3-70BA-LLMPpredictivedistri-
1 2
bution using 100 training points is visualized in Figure 4 (right) and using 40 training points is
visualizedinFigureI.27.
4 10 1
10 2
2
10 3
0 10 4
10 5
2
40 20 0 20 40
x
FigureI.27: HeatmapvisualizationoftheLlama-3-70BA-LLMPpredictivedistributionconditioned
ondatafromabimodalgenerativeprocess. Blackdotsarethe40trainingpoints.
44
y
stigoL
:)y(pI.3 ComparisontoLLMTime
Figure I.28 compares A-LLMP in a temperature forecasting scenario to LLMTime. The dataset
consistsof86dailyhightemperaturereadings,obtainedafterthetrainingcut-offfortheLlama-2
LLMtoavoiddata-leakage. Weusethefirst50readingsfortrainingdataandaskthetwomethodsto
predict/forecastthefinal36values. TheauthorsofLLMTimesuggestthemethodcanhandlemissing
valuesbyinputtingNaNvaluesintheirplace. SinceLLMPscanworkwithirregularlyspacedand
missingdata,wealsocomparethemethodswithareducednumberofirregularlyspacedtraining
points. A-LLMPwinsoutoverLLMTime,asthelogprobabilitiesforA-LLMParesignificantly
better.
True Function Training points A-LLMP Conf A-LLMP Median LLMTime Conf LLMTime Median
AUTO-LLMP: Training Points=10 MAE=2.738 NLL=0.321 LLMTime: Training Points=10 MAE=3.189 NLL=1.946
30
25
20
15
10
5
0
AUTO-LLMP: Training Points=20 MAE=2.503 NLL=0.257 LLMTime: Training Points=20 MAE=3.028 NLL=1.652
17.5
15.0
12.5
10.0
7.5
5.0
2.5
0.0
AUTO-LLMP: Training Points=30 MAE=2.671 NLL=0.103 LLMTime: Training Points=30 MAE=2.532 NLL=1.537
20
15
10
5
0
AUTO-LLMP: Training Points=40 MAE=1.939 NLL=0.064 LLMTime: Training Points=40 MAE=2.082 NLL=1.610
25
20
15
10
5
AUTO-LLMP: Training Points=50 MAE=1.893 NLL=0.098 LLMTime: Training Points=50 MAE=1.916 NLL=1.429
25
20
15
10
5
0
0 20 40 60 80 0 20 40 60 80
Days since December 12, 2023
FigureI.28: MAE↓andNLL↓forA-LLMPversusaLLMTimeonadatasetofdailytemperatures
inLondon,UKrecordedafterthereleasedateoftheLLMwithavaryingnumberoftrainingpoints.
TheLLMisLlama-2-7Binbothcases.
45
)C°(
erutarepmeT
)C°(
erutarepmeT
)C°(
erutarepmeT
)C°(
erutarepmeT
)C°(
erutarepmeT
)C°(
erutarepmeT
)C°(
erutarepmeT
)C°(
erutarepmeT
)C°(
erutarepmeT
)C°(
erutarepmeTI.4 AdditionalImageReconstructionResultsandDetails
FigureI.29depictssiximagereconstructionresults,alldrawnfromtheFashion-MNISTdataset[12].
The28×28pixelimageswerefirstscaledto20×20, duetothecontextsizelimitationsofthe
open-sourceLLMsweusedinourexperiments. Thepixeldatawasthenconvertedintopromptdata
pointsbyformingaseriesof(row,column,pixelvalue)integertuples. Wethensampled80pixel
locations(20%)and200pixellocations(50%)asobservedpointsforthereconstruction. Eachpixel
location(400inall)wasusedasatargetpointlocationforindependentmarginalsamplingwiththe
Mixtral-8×7BLLM.
True 20% Observed 20% Reconstruct 50% Observed 50% Reconstruct
FigureI.29: ImagereconstructionresultsforsiximagesdrawnfromtheFashion-MNISTdataset[12].
1stcolumn: Trueimages.The2ndand4thcolumnsaretheobservedpixelsfortheregressiontaskand
aresampledat20%and50%fromthetrueimagepixels. Thebluepixelsindicateunobserved. The
3rdand5thcolumnsshowthereconstructionsusingtheMixtral-8×7BLLM.
46I.5 Black-boxOptimizationResultsandImplementationDetails
Blackboxoptimizationinvolvesminimizingormaximizingafunctionwherethereisonlyaccessto
theoutputofafunctionforaspecifiedinput. Itisoftenusedtooptimizefunctionsthatareexpensive
toevaluateandthegoalistofindtheminimumormaximumvaluewiththefewestnumberofcalls
tothefunction(oftenreferredtoastrials). Toacquirethelocationofthenextpointtoobserve,we
sampletheLLMusingThompsonsampling[38,39]. DetailsareinAlgorithm3. Webenchmarkthe
abilityofanLLMtoperformblackboxmaximizationonsixcommonlyusedfunctionsimplemented
in[40], includingGramacy[41], Branin, Bohachevsky, Goldstein, andHartmann3. Wecompare
ourresultsusingLlama-2-7BtoOptuna[4],acommercialhyperparameteroptimizationframework.
Werunbothmethodsfor100trialsandrecordthetrialatwhichthethebestapproximationtothe
maximum occurs. The results are shown in Table I.7. In all cases, we obtain as good or better
approximationtothetruemaximumvalueinafewernumberoftrials. NotethatOptunawillperform
100 trials in a few seconds while the LLM approach can take up to 2 Nvidia A100 GPU hours.
However, the results show that the log likelihood of LLMPs is capable of accurately portraying
regressionuncertainty.
TableI.7: Blackboxoptimizationresults. ThenumberintheFunctioncolumnindicatesthenumber
ofxdimensions. TheTrialcolumnindicatesthetrialatwhichtheBestestimateofthemaximumfor
eachmethodoccurred.
Optuna Llama-7B
Function TRUE Trial Best Trial Best
Sinusoidal(1) 1.879 70 1.879 23 1.879
Gramacy(1) 0.869 48 0.869 29 0.869
Branin(2) -0.040 85 -0.041 70 -0.040
Bohachevsky(2) 0.000 82 -5.539 49 -1.305
Goldstein(2) -3.000 35 -4.876 31 -3.101
Hartmann(3) 3.863 86 3.745 53 3.863
47Algorithm3PseudocodeforLLMblack-boxfunctionoptimization
Require: f(x): Functiontobemaximized
Require: x : Minimumboundonx
min
Require: x : Maximumboundonx
max
Require: T: Numberoftrials(default100)
Require: M: Numberoftargetpoints(default500)
Require: C: Numberofcoldstartpoints(default7)
observed ←[] ▷Listofobservedxvalues
x
observed ←[] ▷Listofobservedypoints
y
fortrial←1toC do
x←∼U(x ,x )
min max
observed .append(x)
x
observed .append(f(x))
y
endfor
fortrial←C +1toT do
targets←[] ▷Listoftargetxpoints
samples←[] ▷Listofsamplesattargetpoints
fori←1toM do
target ←∼U(x ,x )
x min max
targets.append(target )
x
prompt←construct_prompt(observed ,observed ,target ) ▷constructatextprompt
x y x
samples←Algorithm 1(N =1) ▷UseAlgorithm1toobtainasinglesampleatthe
targetpoint
endfor
new_observed ←targets[argmax(samples)] ▷Thompsonsampling
x
observed .append(new_observed )
x x
observed .append(f(new_observed ))
y x
endfor
max ←max(observed ) ▷Bestestimateofmaximumvalueoff
y y
max ←observed [argmax(observed )] ▷valueofxwherebestestimateofmaximumoff
x x y
occurs
48I.6 In-contextExperimentDetailsandAdditionalPlots
For the in-context learning experiment in Section 4 we investigate LLMPs’ ability to learn from
similarexamplesin-contexttopredictaveragemonthlyprecipitationacross13Canadianlocations
[13],onefromeachprovinceandterritory: Alert,NU,Charlottetown,PE,Comox,BC,Goose,NL,
Greenwood,NS,Keylake,SK,Montreal,QC,Ottawa,ON.Ranfurly,AB,SaintJohn,NB,Thompson,
MB,Whitehorse,YK,andYellowknife,NT.Foreachlocation,weusetheMixtral-8×7BA-LLMPto
forecast32monthsofaverageprecipitationvaluesgiventhepreviousfourmonthobservationstaken
fromarandomhistoricalthree-yearperiodbetween1913-2017(conditionalondataavailability).
Itisthenprovidedwith1-12examplesofrandomthreeyearperiodsofhistoricalvaluesfromthe
samelocationin-context. Anexamplepromptsfor0,1(1976-1978)and2(1976-1978,1949-1951)
examplesare:
1. “Monthlytotalofdailyadjustedrainfall,mm. \n1976-1978:\n”,
2. “Monthlytotalofdailyadjustedrainfall,mm. \n1967-1969:\n0,0.3\n1,0.6\n2,1.3\n
3,0.6\n4,31.7\n5,59.9\n6,135.4\n7,107.7\n8,78.3\n9,40.7\n10,37.3\n11,5.4\n12,1.0\n
13,41.4\n14,0.3\n15,29.2\n16,41.3\n17,67.8\n18,137.8\n19,139.9\n20,91.4\n21,143.1\n22,18.8
\n23,0.9\n24,0.6\n25,14.0\n26,4.0\n27,6.2\n28,45.1\n29,98.3\n30,97.0\n31,160.4\n32,116.3\n
33,22.4\n34,51.8\n35,38.1\n1976-1978:\n”,
3. “Monthlytotalofdailyadjustedrainfall,mm. \n1967-1969:\n0,0.3\n1,0.6\n2,1.3\n
3,0.6\n4,31.7\n5,59.9\n6,135.4\n7,107.7\n8,78.3\n9,40.7\n10,37.3\n11,5.4\n12,1.0\n
13,41.4\n14,0.3\n15,29.2\n16,41.3\n17,67.8\n18,137.8\n19,139.9\n20,91.4\n21,143.1\n22,18.8\n
23,0.9\n24,0.6\n25,14.0\n26,4.0\n27,6.2\n28,45.1\n29,98.3\n30,97.0\n31,160.4\n32,116.3\n
33,22.4\n34,51.8\n35,38.1\n
1949-1951:\n0,1.6\n1,0.0\n2,2.5\n3,2.1\n4,22.0\n5,51.7\n6,83.4\n7,113.3\n8,75.5\n9,34.7\n10,4.7\n
11,1.4\n12,1.1\n13,0.0\n14,0.8\n15,9.5\n16,33.3\n17,92.6\n18,118.5\n19,70.3\n20,34.6\n21,58.2\n
22,62.4\n23,8.5\n24,0.3\n25,7.4\n26,8.0\n27,30.6\n28,49.3\n29,40.0\n30,82.5\n31,97.1\n32,71.5\n
33,17.1\n34,32.1\n35,10.1\n1976-1978:\n”.
ResultsarepresentedinFigure8,FigureI.30andFigureI.31.
490examples 1example 4examples 12examples
Location: ALERT, NU Location: ALERT, NU Location: ALERT, NU Location: ALERT, NU
MAE = 3.84375 NLL = -0.95732 MAE = 3.84375 NLL = -1.88916 MAE = 3.79766 NLL = -2.03485 MAE = 3.38531 NLL = -1.94721
40 40 40 40
30 30 30 30
20 20 20 20
10 10 10 10
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: CHARLOTTETOWN, PE Location: CHARLOTTETOWN, PE Location: CHARLOTTETOWN, PE Location: CHARLOTTETOWN, PE
MAE = 65.70156 NLL = 3.66345 MAE = 43.01094 NLL = 3.25589 MAE = 36.81875 NLL = 3.04051 MAE = 28.90625 NLL = 3.09845
200 150 200 200 200
150
100 100 100 100
50 50
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: COMOX, BC Location: COMOX, BC Location: COMOX, BC Location: COMOX, BC
MAE = 80.05156 NLL = 4.43894 MAE = 70.75469 NLL = 3.76669 MAE = 68.45000 NLL = 3.67419 MAE = 54.88281 NLL = 3.64045
400 300 300 400
300 300
200 200
200 200
100 100 100 100
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: GOOSE, NL Location: GOOSE, NL Location: GOOSE, NL Location: GOOSE, NL
MAE = 49.91250 NLL = 3.10953 MAE = 33.20781 NLL = 2.59628 MAE = 26.35781 NLL = 2.25742 MAE = 19.72344 NLL = 2.53806
400 150 300 150 200
100 200 100
100
50 100 50
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: GREENWOOD NS Location: GREENWOOD NS Location: GREENWOOD NS Location: GREENWOOD NS
MAE = 47.44844 NLL = 3.75875 MAE = 32.94844 NLL = 3.08360 MAE = 27.85312 NLL = 2.97142 MAE = 28.67500 NLL = 3.22651
150 150
150 200
100 100 150 100
100
50 50 50 50
0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: KEYLAKE, SK Location: KEYLAKE, SK Location: KEYLAKE, SK Location: KEYLAKE, SK
MAE = 33.30469 NLL = 1.43380 MAE = 29.75312 NLL = 0.71411 MAE = 18.05625 NLL = 0.33982 MAE = 13.50000 NLL = 0.63969
150 150 150 150
100 100 100 100
50 50 50 50
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: MONTREAL, QC Location: MONTREAL, QC Location: MONTREAL, QC Location: MONTREAL, QC
MAE = 38.88437 NLL = 3.95602 MAE = 28.98281 NLL = 3.13462 MAE = 26.63594 NLL = 3.06370 MAE = 26.30625 NLL = 3.14295
150 200 150
100 150
100 100 100
50 50 50 50
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
FigureI.30: VisualizationsofthepredictionsgivenbytheMixtral-8×7BLLMPforsevenlocations
locationsaccrossCanada. Blueandblackcirclesaretrainingandtestpoints,respectively. Redcircles
aremedianpredictionsandshadedareasindicatetenth-percentilesover30samples.
50
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP0examples 1example 4examples 12examples
Location: OTTAWA, ON Location: OTTAWA, ON Location: OTTAWA, ON Location: OTTAWA, ON
MAE = 35.23438 NLL = 3.54943 MAE = 26.53750 NLL = 2.91439 MAE = 22.10781 NLL = 2.82099 MAE = 28.22812 NLL = 2.86180
200
150 200 200 150
100 100
100 100
50 50
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: RANFURLY, AB Location: RANFURLY, AB Location: RANFURLY, AB Location: RANFURLY, AB
MAE = 30.44531 NLL = 2.49844 MAE = 25.11906 NLL = 1.98546 MAE = 12.25406 NLL = 1.65454 MAE = 14.65312 NLL = 1.93917
200
1 70 50 150 200 150
100 100
50
100
25 50 50
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: SAINTJOHN, NB Location: SAINTJOHN, NB Location: SAINTJOHN, NB Location: SAINTJOHN, NB
MAE = 49.46094 NLL = 3.93776 MAE = 37.88750 NLL = 3.33256 MAE = 33.85938 NLL = 3.19853 MAE = 31.62188 NLL = 3.07337
200 200 200
200
150 150
100 100 100 100
50 50
0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: THOMPSON, MB Location: THOMPSON, MB Location: THOMPSON, MB Location: THOMPSON, MB
MAE = 40.65000 NLL = 2.51161 MAE = 32.69375 NLL = 1.69521 MAE = 22.52969 NLL = 1.33926 MAE = 22.35625 NLL = 1.40091
200 200 200 200
150 150 150 150
100 100 100 100
50 50 50 50
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: WHITEHORSE, YK Location: WHITEHORSE, YK Location: WHITEHORSE, YK Location: WHITEHORSE, YK
MAE = 17.93750 NLL = 1.88502 MAE = 17.55469 NLL = 1.05461 MAE = 10.79531 NLL = 0.79132 MAE = 9.61250 NLL = 1.20566
100 100 100 100
75 75 75 75
50 50 50 50
25 25 25 25
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
Location: YELLOWKNIFE, NT Location: YELLOWKNIFE, NT Location: YELLOWKNIFE, NT Location: YELLOWKNIFE, NT
MAE = 17.04219 NLL = 1.44717 MAE = 17.04219 NLL = 1.44717 MAE = 17.04219 NLL = 1.44717 MAE = 17.04219 NLL = 1.44717
100 100 100 100
50 50 50 50
0 0 0 0
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Months Months Months Months
FigureI.31: VisualizationsofthepredictionsgivenbytheMixtral-8×7BLLMPforsixlocations
locationsaccrossCanada. Blueandblackcirclesaretrainingandtestpoints,respectively. Redcircles
aremedianpredictionsandshadedareasindicatetenth-percentilesover30samples.
51
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerP
)mm(
noitatipicerPJ ConditioningonTextDetailsandAdditionalExperiments
J.1 Scenario-conditionalPredictionsDetailsandAdditionalExperiments
Forthescenario-conditionalpredictionsexperimentinSection5,weexaminetheinfluenceoftext
providinginformationaboutvarioussyntheticproblemsettingsonthepredictivedistributionofan
Llama-3-70BLLMP.Inallofthefollowingexamples,weprovidethesametwosynthetictraining
points,(1,2.53)and(2,2.21)totheLLMProcessbutchangethepromptingtextthatcomesbefore
thetrainingdata. WethenuseA-LLMPtoforecasttrajectoriesinteger50stepsahead. Promptswere
prependedtothestandarddataformattingschemeusedforLLMPs(seeAppendixC).
ThepromptsprovidedtotheLLMPvisualizedinFigure9are:
1. “”(i.e. notext);
2. ‘ThefollowingaredailytemperaturemeasurementsfromMontrealinJanuaryindegreesCelsius”
3. “ThefollowingaredailytemperaturemeasurementsfromMontrealinMayindegreesCelsius”
4. “Inthefollowingseries,thefirstnumberisthenumberofMonthsfromJanuaryandthesecondis
theMonthlyprecipitationmeasurementsininchesfromSanDiego,CA”
5. “Inthefollowingseries,thefirstnumberisthenumberofMonthsfromFebruaryandthesecondis
theMonthlyprecipitationmeasurementsininchesfromSingapore”
ThepromptsvisualizedinFigure1are:
1. “Thefollowingaredailystockpricesfromafinancialtimeseries”
2. “Thefollowingaredailystockpricesfromafinancialtimeseriesforacompanythateventually
goesoutofbusiness”
3. “Thefollowingaredailyaveragestockpricesfromafinancialtimeseriesforacompanywhose
stockpricegoestozeroonday30”
LynxHarePopulationForecasting: Similartothepreviousexperiment,thisexperimentexamines
towhatextentthepredictiveposteriorofanLLMProcessisinfluencedbytextualinformationabout
theproblemprovidedintheprompt. Weprefacethepromptwiththreedifferentstrings:
1. “”(i.e. notext);
2. “Thefollowingaresamplesfromlynx-harepopulations”
3. ‘’ThefollowingaresamplesfromthefamousCanadianHudsonBayLynx-Harepopulationdataset.
Whenhareincreases,lynxincreases. Thefirstnumberoftwoistheyear. Thesecondnumberis
thelynxpopulation. Itfollowsthepatternwhenlynxpopulationincreases,haredecreases”
FigureJ.32showsthepredictivedistributionoftheLLMwith10and50observedpoints. Asthe
specificityofthetextincreasesfromLtoR,theposteriorentropydecreases,andstructureofthe
sampleschangesdramatically.
No Prompt Text Short Prompt: Lynx-hare populations Long Prompt: Hudson Bay and Lynx-hare Description
100
80
60
40
20
0
100
80
60
40
20
0
1860 1880 1900 1920 1860 1880 1900 1920 1860 1880 1900 1920
Year
FigureJ.32: Resultsofconditiononbothtextandnumericaldatasimultaneously,ontheMixtral
model. Observedpointsareinpurple. Coloredlinesshowsampledtrajectories. Theblueshadingisa
visualizationofpercentilesbasedon50samples. Top: Conditioningon10observedpoints. Bottom:
Conditioningon50observedpoints. Thepredictivedistributionchangesasmoreinformationabout
theproblemisaddedtotheprompt.
52
noitalupoP
xnyL
noitalupoP
xnyL
noitalupoP
xnyL
noitalupoP
xnyL
noitalupoP
xnyL
noitalupoP
xnyLJ.2 LabellingFeaturesUsingTextDetailsandAdditionalPlots
In the experiments in section Section 5 we examine the performance of a Mixtral-8x7B Instruct
I-LLMPonpredictingAmericanhousingprices. Thedataset[16]contains39980housingpricesand
variousvariablesaroundhousinganddemographicsforthetop50Americancitiesbypopulation.
Thisdatasetwasgeneratedon12/09/2023,howeveritcontainsdatafromthe2020USCensusand
the2022AmericanCommunitySurvey(ACS).Itispossiblethatdatawithinthisdatasetwasused
totrainMixtral-8x7Bbutitisveryunlikelythatitwastrainedontheexactstringspresentedinthis
experiment.
Foreachpredictiontask,weshowtheI-LLMP10randomlyselectedtrainingexamplesfromthe
datasetandpredicton20randomlyselectedtestexamples. Intheprompt,beforethenumericalvalue
(price)weprovideastringwhichencodesthedatapointindex/featuresthatthemodelcanuse. Forour
firstexperimentweexaminethebehaviouroftheLLMPwhenmorefeaturesareaddedtotheprompt.
Weexperimentwithfivewaysofindexingthetrainingandtestpointsillustratedbythefollowing
trainingexamples;
1. “32.74831,-97.21828,Price: 224900.00”
2. “Location: FortWorth,Texas,Latitude: 32.74831,Longitude: -97.21828,Price: 224900.00”
3. “Location: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, Zip Code: 76112,
MedianHouseholdIncome: 71452.0,Price: 224900.00”
4. “Location: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, Zip Code: 76112,
MedianHouseholdIncome: 71452.0,ZipCodePopulation: 42404people,ZipCodeDensity:
1445.0peoplepersquaremile,Price: 224900.00”
5. “Location: Fort Worth, Texas, Latitude: 32.74831, Longitude: -97.21828, Zip Code: 76112,
MedianHouseholdIncome: 71452.0,ZipCodePopulation: 42404people,ZipCodeDensity:
1445.0peoplepersquaremile,LivingSpace: 1620squarefeet,NumberofBedrooms: 3,Number
ofBathrooms: 2,Price: 224900.00”
Thisprocedureisrepeated10timestocomputestatistics. Resultsfromthisexperimentarepresented
inFigure10(left,centre)andinJ.34. WealsoranthisexperimentusingMixtral-8x7Bandfound
thattheperformance,showninFigureJ.33,wasnotasgoodaswiththeinstructiontunedversionof
Mixtral-8×7B.
4.5 5.25
5.00
4.0
4.75
3.5 4.50
3.0 4.25
4.00
2.5
3.75
2.0
3.50
1) lat-long 2) location, 3)+zip, 4)+population,5)+space,
as num lat-long income density bed,
bath
FigureJ.33: AverageMAEandNLLperformanceoftheMixtral-8x7BLLMPover10experiments
witherrorbarsrepresentingthestandarderror.
AnadditionalexperimentispresentedinSection5toseeexaminetheeffectofaddingtextlabels
to the features. This experiment was run on 10 new random datasets providing the LLMP with
either labeled or unlabelled numerical features. Due to the results of the previous experiment, a
Mixtral-8x7B Instruct LLMP was used for this experiment. The following are example training
stringsforthefourcasesexamined:
a. “30.45738,-97.75516,Price: 385000.00”
53
K001$
ni
EAM
LLN1e6 1e7 1e7
45 T 1 5r ) )u l ae a
l
t lP - flr eoic ane tg
ures as text
345 T 1 5r ) )u l ae a
l
t lP - flr eoic ane tg
ures as text
111 ... 257 505 T 1 5r ) )u l ae a
l
t lP - flr eoic ane tg
ures as text
3 2 1.00
2 1 0.75
1 0 0.50
1 0.25
0 2 0.00
1234567891011121314151617181920 1234567891011121314151617181920 1234567891011121314151617181920
House Example House Example House Example
Run1 Run2 Run3
1e7 1e6 2.01e6
5 True Price 2.5 True Price True Price
4 1 5) ) l aa lt l - fl eo an tg ures as text 2.0 1 5) ) l aa lt l - fl eo an tg ures as text 1.5 1 5) ) l aa lt l - fl eo an tg ures as text 3 1.5 1.0
2 1.0
1 0.5 0.5
0 0.0 0.0
1 0.5
1234567891011121314151617181920 1234567891011121314151617181920 1234567891011121314151617181920
House Example House Example House Example
Run4 Run5 Run6
1e6 1e6 2.51e6
True Price True Price True Price
1.5 1) lat-long 4 1) lat-long 2.0 1) lat-long
5) all features as text 3 5) all features as text 5) all features as text
1.5
1.0
2
1.0
0.5 1
0.5
0
0.0 1 0.0
1234567891011121314151617181920 1234567891011121314151617181920 1234567891011121314151617181920
House Example House Example House Example
Run7 Run8 Run9
2.51e6
True Price
2.0 1) lat-long
5) all features as text 1.5
1.0
0.5
0.0
0.5
1234567891011121314151617181920
House Example
Run10
FigureJ.34: Resultsof10runsusingMixtral-8x7BInstructI-LLMPpredictingUShousingprices
for20randomhousesfrom[16]. Predictionsarevisualizedusingindexstyle1)and5). Xsaremean
predictionsusing30samplesfromtheLLMPanderrorbarsindicate2standarddeviations.
b. “Location: Austin,Texas,Latitude: 30.45738,Longitude: -97.75516,Price: 385000.00”
c. “30.45738,-97.75516,78729,107830.0,30907,1216.1,1349,3,Price: 385000.00”
d. “Location: Austin,Texas,Latitude: 30.45738,Longitude: -97.75516,ZipCode: 78729,Median
HouseholdIncome: 107830.0,ZipCodePopulation: 30907people,ZipCodeDensity: 1216.1
peoplepersquaremile,LivingSpace: 1349squarefeet,NumberofBedrooms: 3,Numberof
Bathrooms: 2,Price: 385000.00”.
ResultsofthisexperimentarepresentedinFigure10(right).
54
)DSU(
ecirP
)DSU(
ecirP
)DSU(
ecirP
)DSU(
ecirP
)DSU(
ecirP
)DSU(
ecirP
)DSU(
ecirP
)DSU(
ecirP
)DSU(
ecirP
)DSU(
ecirPK AdditionalCommentsonLimitationsandSocietalImpact
Limitations As mentioned in the main text along with the flexibility of LLMs, LLMPs inherit
theirdrawbacks. AnadditionaldrawbackofusingLLMsforprobabilisticregressionisthatresults
from LLMPs are inherently less interpretable than from methods like Gaussian processes where
we explicitly encode priors. As with other black-box methods, we must, at the moment, rely on
demonstratingempiricallythatitmakeswell-calibratedpredictions.
SocietalImpactOurworkhasdemonstratedanewandusefulzero-shotapproachforgenerating
probabilistic predictions using plain language to augment numerical data. It has the potential to
allowpractitionersfromfieldssuchasmedicalresearchandclimatemodellingtomoreeasilyaccess
probabilisticmodellingandmachinelearning. Wehopethatsuchanimpactwouldhelpresearchers
improvethelivesofallhumansbytacklingtheproblemsthathumanityfacestoday.
Likeallmachinelearningtechnology,thereispotentialforabuse,andpossibleconsequencesfrom
incorrectpredictionsmadewithLLMPs. Duetotheblack-boxnatureofthemethod,wedonotknow
thebiasesintheunderlyingLLMsusedandwhateffecttheymayhaveonLLMPsoutput. However,
LLMresearchersarestrivingtomakeLLMsmorefairandequitable. Anopenareaofresearchis
whetherLLMbiasespropagatetoLLMPpredictionsandwhetherde-biasingLLMshelpstofixsuch
anissue.
55