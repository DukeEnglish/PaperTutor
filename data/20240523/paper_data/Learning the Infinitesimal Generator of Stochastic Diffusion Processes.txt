Learning the Infinitesimal Generator
of Stochastic Diffusion Processes
Vladimir R. Kostic
Istituto Italiano di Tecnologia
University of Novi Sad
vladimir.kostic@iit.it
Karim Lounici
CMAP-Ecole Polytechnique
karim.lounici@polytechnique.edu
Helene Halconruy
Telecom Sud-Paris
helene.halconruy@telecom-sudparis.eu
Timothee Devergne
Istituto Italiano di Tecnologia
timothee.devergne@iit.it
Massimiliano Pontil
Istituto Italiano di Tecnologia
University College London
massimiliano.pontil@iit.it
May 2024
Abstract
Weaddressdata-drivenlearningoftheinfinitesimalgeneratorofstochasticdiffusionprocesses,
essentialforunderstandingnumericalsimulationsofnaturalandphysicalsystems. Theunbounded
nature of the generator poses significant challenges, rendering conventional analysis techniques
for Hilbert-Schmidt operators ineffective. To overcome this, we introduce a novel framework
based on the energy functional for these stochastic processes. Our approach integrates physical
priors through an energy-based risk metric in both full and partial knowledge settings. We
evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert
spaces (RKHS) in the partial knowledge setting. Notably, our approach provides learning
bounds independent of the state space dimension and ensures non-spurious spectral estimation.
Additionally, we elucidate how the distortion between the intrinsic energy-induced metric of the
stochastic diffusion and the RKHS metric used for generator estimation impacts the spectral
learning bounds.
1
4202
yaM
12
]LM.tats[
1v04921.5042:viXra1 Introduction
Continuous-time processes are often modeled using ordinary differential equations (ODEs), assuming
deterministic dynamics. However, real systems in science and engineering that are modeled by
ODEs are subject to unfeasible-to-model influences, necessitating the extension of deterministic
models through stochastic differential equations (SDEs), see [33, 35] and references therein. SDEs
are advantageous for modeling inherently random phenomena. For instance, in finance, they specify
the stochastic process governing asset behavior, a crucial step in constructing pricing models for
financial derivatives [34]. In atomistic simulations, where SDEs are used to model the evolution of
atomic systems subjected to thermal fluctuations via the Boltzmann distribution [30].
A diverse range of SDEs can be represented as dX = a(X )dt+b(X )dW , where X = x.
t t t t 0
Here, W denotes a (possibly multi-dimensional) Brownian motion, and the functions a and b are
commonly known as the drift and diffusion coefficients, respectively. Determining these coefficients
from one or more trajectories, whether discretized or continuous, has been a key pursuit in "diffusion
statistics" since the 1980s, as seen in works like [18, 24, 25]. However, uncovering the drift and
diffusioncoefficientsalonedoesnotrevealalltheintrinsicpropertiesofacomplexsystem, suchasthe
metastable states of Langevin dynamics in atomistic simulations [see e.g. 42, and references therein].
Consequently, there has been a shift and growing interest in the Infinitesimal Generator (IG) of an
SDE, as its spectral decomposition offers a more in-depth understanding of system dynamics and
behavior, thus providing a comprehensive picture beyond the mere identification of coefficients.
RecoveringthespectraldecompositionoftheIGcantheoreticallybeachievedbyexploitingthewell-
studiedTransferOperators(TO)[see21,andreferencestherein]. TOsrepresenttheaverageevolution
of state functions (observables) over time and, being linear and amenable to spectral decomposition
under certain conditions, they offer a valuable means to interpret and analyze nonlinear systems.
However, they require evenly sampled data at a high rate, which may be impractical. Additionally,
TO approaches are purely data-driven, complicating the incorporation of partial or full knowledge of
anSDEintothelearningprocess. Thus,thereisgrowinginterestinlearningtheIGdirectlyfromdata,
as it can handle uneven sampling and integrate SDE knowledge. The challenge lies in the IG being
an unbounded operator, unlike TO which are often well-approximated by Hilbert-Schmidt operators
with comprehensive statistical theory [22]. Unfortunately, the existing statistical theory collapses
when applied to unbounded operators, prompting the need to completely rethink the problem
Related work Extensive research has explored learning dynamical systems from data [see the
monographs 7, 26, and references therein]. Analytical models for dynamics are often unavailable,
motivating the need for data-driven methods. Two prominent approaches have emerged: deep
learning-based methods [5, 11, 29], effective for learning complex data representations but lacking
statisticalanalysis, andkernelmethods[2, 6,9,16,17,20–22,43], offeringsolidstatistical guarantees
for the estimation of the TO but requiring careful selection of the kernel function. A related question,
tackling the challenging problem of learning invariant subspaces of the TO, has recently led to the
development of several methodologies [17, 27, 31, 39, 45, 44], some of which are based on deep
canonical correlation analysis [3, 23]. In comparison, there have been significantly fewer works on
learning the IG and only in very specific settings. In [48] a deep learning approach is developed
for Langevin diffusion, while [19] presents an extended dynamical mode decomposition method for
learning the generator and clarifies its connection to Galerkin’s approximation. However, neither
of these two works provides any learning guarantees. In this respect the only previous work we
are aware of are [1], revising the Galerkin method for Laplacian diffusion, [15], presenting a kernel
approach for general diffusion SDE with full knowledge of drift and diffusion coefficients, and [36]
addressing Langevin diffusion. As highlighted in Appendix E.6, the bounds and analysis in these
works are either restricted to specific SDEs or are incomplete. Notably, none of these works proposed
2anadequateframeworktohandletheunboundednessoftheIG,resultinginanincompletetheoretical
analysis and suboptimal rates, sometimes explicitly depending on state space dimension. Moreover,
the estimators proposed in these works are prone to spurious eigenvalues and do not offer guarantees
on the accurate estimation of eigenvalues and eigenfunctions.
Contributions In summary, our main contributions are: 1) Underpinning a framework
for learning the generator of stochastic diffusion processes in reproducing kernel Hilbert space,
using a novel energy risk functional; 2) Proposing a reduced-rank estimator with dimension-free
learning bounds; 3) Establishing first-of-its-kind spectral learning bounds for generator learning; 4)
Demonstrating the practical advantages of our approach over previous methods.
2 Background and the problem
Our drive to estimate the eigenvalues of the infinitesimal generator for an SDE like (1) stems from
its crucial role in characterizing dynamics in physical systems. This operator’s closed form (3), relies
on the drift a and diffusion coefficient b, where we have partial knowledge: b is assumed to be known,
but a is not. To compensate for the lack of prior knowledge about a, we introduce the system’s
energy as an additional known quantity. Below, we detail the mathematical concepts framing the
problem (generator, spectrum, energy) exemplified through the Langevin and Cox-Ingersoll-Ross
processes. For detailed list of notation used throughout the paper we refer Table 7 in the appendix.
Transfer operator and infinitesimal generator A variety of physical, biological, and
financial systems evolve through stochastic processes X =(X ) , where X ∈X ⊂Rd denotes the
t t∈R+ t
system’s state at time t. A commonly employed model for such dynamics is captured by stochastic
differential equations (SDEs) of the form
dX =a(X )dt+b(X )dW and X =x, (1)
t t t t 0
where x ∈ X, W = (W1,...,Wp) is a Rp-dimensional (p ∈ N) standard Brownian motion,
t t t∈R+
the drift a : X → Rd, and the diffusion b : X → Rd×p are assumed to be globally Lipschitz and
sub-linear, so that the SDE (1) admits an unique solution X = (X t)⩾0 with values in (X,B(X)).
Processesakintoequationslike(1)arediverse,spanningmodelslikeLangevinandCox-Ingersoll-Ross
processes (see examples below), with broad applications in science and engineering. The process X is
a continuous-time Markov process with almost surely continuous sample paths whose dynamics is
described by a family of probability densities (p ) and transfer operators (A ) such that for
t t∈R+ t t∈R+
all t∈R , E ∈B(X), x∈X and measurable function f :X →R,
+
(cid:90) (cid:90)
P(X ∈E|X =x)= p (x,y)dy and A f = f(y)p (·,y)dy =E(cid:2) f(X )|X =·(cid:3) . (2)
t 0 t t t t 0
E X
Evaluating A f at x yields the expectation of f starting from x and evolving until time t, making
t
the transfer operator crucial for understanding X dynamics. The family (A ) satisfies the
t t∈R+
fundamental semigroup equation A = A ◦A , for s,t ∈ R . Here, we focus on the transfer
t+s t s +
operator’s effect on the set L2(X), a choice driven by the existence of an invariant measure π for A
π t
on (X,B(X)) which satisfies A∗π =π for all t∈R . Then, the process X is characterized by the
t +
infinitesimal generator L defined for every f ∈L2(X) such that the limit Lf =lim (A f −f)/t
π t→0+ t
exists in L2(X). The operator L is closed on its domain dom(L) which is equal to the Sobolev space
π
W1,2(X)={f ∈L2(X)|∥f∥2 =∥f∥ +∥∇f∥ <∞}.
π π W L2 π L2 π
3For SDE dynamics of the form (1), we can prove (see A.2 for details) that L is the second-order
differential operator given, for any f ∈L2(X), x∈X, by
π
Lf(x)=∇f(x)⊤a(x)+ 1 Tr(cid:2) b(x)⊤(∇2f(x))b(x)(cid:3) , (3)
2
where ∇2f =(∂2f) denotes the Hessian matrix of f.
ij i∈[d],j∈[p]
Spectral decomposition Knowing only the drift a and diffusion b is not enough to compute
(2) or to understand quantitative aspects of dynamical phase transitions, such as time scales and
metastable states. The eigenvalues and eigenvectors of the generator are crucial for capturing these
effects. To address the possible unbounded nature of L, one can turn to an auxiliary operator, the
resolvent, which, under certain conditions, shares the same eigenfunctions as L and becomes compact.
When it exists and is continuous for some µ∈C, the operator L =(µI−L)−1 is the resolvent of L
µ
and the corresponding resolvent set is defined by
ρ(L)=(cid:8) µ∈C|(µI−L)is bijective andL is continuous(cid:9)
µ
We assume L has a compact resolvent, meaning ρ(L) ̸= ∅ and there exists µ ∈ ρ(L) such that
0
(µ I−L)−1 is compact. Under this assumption, and given that (L,dom(L)) is self-adjoint, we can
0
prove the spectral decomposition of the generator (see A.1 for details) as follows:
(cid:88)
L= λ f ⊗f (4)
i i i
i∈N
where (λ )i∈N are the eigenvalues of L, and the corresponding eigenfunctions f ∈L2(X), forming
i i π
an orthonormal basis (f ) , are also eigenfunctions of the transfer operator A .
i i∈N t
Dirichletformsandenergy Tohandletheinitiallackofknowledgeaboutthedrifta,weassume
to have access to another quantity, called the energy, defined as E(f)=lim (cid:82) (f(f −A f))/tdπ
t→0 X t
for all functions f ∈L2(X) for which this limit exists, defining in the way the domain dom(E). The
π
associated Dirichlet form is the bilinear form defined by polarization for any f,g ∈dom(E) by
(cid:90) (cid:90)
E(f,g)=− f(Lg)dπ = (−Lf)gdπ. (5)
X X
Foreveryf ∈dom(E),wehaveE(f)=E(f,f). Asforeveryi∈N,0≤E(f ,f )=−(cid:82) f (λ f )dπ =
i i X i i i
−λ , we check that the eigenvalues of L are negative. To relate L to Dirichlet form, we assume there
i
exists a Dirichlet operator B =s⊤∇ where s=[s |...|s ]:x∈X (cid:55)→s(x)=[s (x)|...|s (x)]∈Rd×p
1 p 1 p
is a smooth function s.t. Lf =s(s⊤∇f)=s(Bf) and so that
(cid:90) (cid:90) (cid:90)
(−Lf)gdπ = (s(x)s(x)⊤∇f(x))⊤∇g(x)π(dx)= (Bf(x))⊤(Bg(x))π(dx).
X X X
We get that for any f ∈dom,(E)
(cid:90)
E(f)= ∥s⊤∇f∥2dπ =E [∥Bf(x)∥2], (6)
x∼π
X
which is reminiscent of the expected value of the kinetic energy in quantum mechanics [13]. To
illustrate this, we specify the generator and the associated Dirichlet gradient form for two processes:
Overdamped Langevin and Cox-Ingersoll-Ross.
4Example 1 (Langevin). Let k ,T ∈ R∗.The overdamped Langevin equation driven by a potential
b +
(cid:112)
V : Rd → R is given by dX = −∇V(X )dt + 2(k T)dW and X = x, where k and T
t t b t 0 b
respectively represent the coefficient of friction and the temperature of the system. Its infinitesimal
generator L is defined by Lf = −∇V⊤∇f +(k T)∆f, for f ∈ W1,2(X). Since (cid:82) (−Lf)gdπ =
b π
−(cid:82)
(cid:104) ∇(cid:16)
(k
T)∇f(x)e−(kbT)−1V(x)(cid:17)(cid:105)
g(x)dx=(k T)(cid:82) ∇f⊤∇gdπ =(cid:82) f(−Lg)dπ, generator L is self-
b Z b
adjoint and associated to a gradient Dirichlet form with s(x)=(k T)1/2(δ ) .
b ij i∈[d],j∈[p]
Example 2 (Cox-Ingersoll-Ross process). Let d = 1, a,b ∈ R, σ ∈ R∗. The Cox-Ingersoll-Ross
√ +
process is solution of the SDE dX = (a+bX )dt+σ X dW and X = x. Its infinitesimal
t t t t 0
generator L is defined for f ∈L2(X) by Lf =(a+bx)∇f + σ2x∆f. By integration by parts, we can
π 2
check that the generator L satisfies (cid:82) (−Lf)gdπ = (cid:82) σ2xf′(x)g′(x)π(dx) = (cid:82) f(−Lg)dπ, and it is
√2 √
associated to a gradient Dirichlet form with s(x)=σ x/ 2.
Learning in reproducing kernel Hilbert spaces (RKHSs) Throughout the paper we let
H be an RKHS and let k :X ×X →R be the associated kernel function. We let ϕ:X →H be a
feature map [38] such that k(x,x′)=⟨ϕ(x),ϕ(x′)⟩ for all x,x′ ∈X. We consider RKHSs satisfying
H⊂L2(X) [38, Chapter 4.3], so that one can approximate L:L2(X)→L2(X) with an operator
π π π
G:H→H. Notice that despite H⊂L2(X), the two spaces have different metric structures, that is
π
forallf,g ∈H, oneingeneralhas⟨f,g⟩ ̸=⟨f,g⟩ . Inordertohandlethisambiguity, weintroduce
H L2
π
the injection operator S : H → L2(X) such that for all f ∈ H, the object S f is the element of
π π π
L2(X) which is pointwise equal to f ∈H, but endowed with the appropriate L2 norm.
π π
Then, the infinitesimal generator restricted to H is simply LS which can be estimated by S G
π π
for some G ∈ HS(H). This approach is based on the embedding ℓϕ: X → H of the generator in
the RKHS that can be defined for kernels k ∈ C2(X ×X) whenever one knows drift and diffusion
coefficients, see B, so that the reproducing property ⟨ℓϕ(x),h⟩ =[LS h](x) holds. Based on
H π
this observation [15] developed empirical estimators of LS that essentially minimize the risk
π
∥LS −S G∥2 =E ∥ℓϕ(x)−G∗ϕ(x)∥2 . In scenarios where drift and diffusion coefficients
π π HS(H,L2) x∼π H
are not known, thenπ ℓϕ becomes non-computable. However if the process has the Dirichlet form (6),
one can still empirically estimate the Galerkin projection (S∗S )†S∗LS onto H, as considered in
π π π π
[1], which in fact minimizes the same risk. Yet this approach is problematic due to the unbounded
nature of the generator and the associated estimators typically suffer from a large number of spurious
eigenvalues around zero, making the estimation of physically most relevant eigenfunctions unreliable
evenforself-adjointgenerators. Conversely,classicalnumericalmethodscancomputetheleadingpart
of a spectrum without spuriousness issues, suggesting that data-driven approaches should achieve
similar reliability. In this paper, we address this problem by designing a novel notion of risk, leading
to principled estimators designed to surmount these challenges.
3 Novel statistical learning framework
In this section, we tackle the challenges in developing suitable generator estimators highlighted
earlier. To this end, we introduce a risk metric for resolvent estimation that can be efficiently
minimized empirically, leading to good spectral estimation. Since L and (µI−L)−1 share the
same eigenfunctions, the main idea is to learn the (compact) resolvent, which can be effectively
approximated by finite-rank operators, instead of learning the generator directly. However, this
approach is challenging due to the lack of closed analytical forms for the action of the resolvent.
First, given µ > 0, in order to approximate the action of the resolvent on the RKHS by
some operator G: H → H, we introduce its embedding χ : X → H via the reproducing property
µ
5⟨χ (x),h⟩ = [(µI−L)−1S h](x), formally given by χ (x) = (cid:82)∞ E[ϕ(X )e−µtdt|X = x], see B
µ H π µ 0 t 0
for details. Using this notation, we aim to estimate [(µI−L)−1S h](x)≈[Gh](x), h∈H, i.e. the
π
objective is to estimate χ (x)≈G∗ϕ(x) π-a.e.
µ
An obvious metric for the risk would be the mean square error (MSE) w.r.t. distribution π of the
data. However, this becomes intractable since in general χ is not computable in closed form when
µ
either full or partial knowledge of the process is at hand.
To mitigate this issue, we introduce a different ambient space in which we study the resolvent
Wµ(X)={f ∈dom(L)|∥f∥2 ≡E [f]=⟨f,(µI−L)f⟩ <∞},
π Wπµ(X) µ L2
π
where the norm now balances the energy of an observable f: X →R w.r.t. the invariant distribution
∥f∥2 and its energy w.r.t. the transient dynamics −⟨f,Lf⟩ . Indeed
L2 L2
π π
E [f]=E [µ|f(x)|2−f(x)[Lf](x)]=E [µ|f(x)|2+∥s(x)⊤∇f(x)∥2], (7)
µ x∼π x∼π
where the last equality holds for Dirichlet gradient form (6), in which case Wµ(X) is simply a
π
weighted Sobolev space. Importantly, this energy functional can be empirically estimated from data
sampled from π, whenever full knowledge, that is drift and diffusion coefficients of the SDE (1), or
partial knowledge, i.e. the diffusion coefficient and Dirichlet operator B in (6), is at hand. With that
in mind, instead of the standard MSE, we introduce the energy-based risk functional as
min R(G)=E [∥χ (·)−G∗ϕ(·)∥ ]. (8)
µ µ H
G:H→H
Denoting by Z : H→Wµ(X) the canonical injection, (8) can be equivalently written as
µ π
R(G)=∥(µI−L)−1Z −Z G∥2 =∥(µI−L)−1/2S −(µI−L)1/2S G∥2 , (9)
µ µ HS(H,W) π π HS(H,L2)
π
whereweabbreviatedW =Wµ(X)andusedZ∗ =S∗(µI−L),recallingthatHilbert-Schmidtandspec-
π µ π
tralnormsforoperatorsA: H→W,are∥A∥ =(cid:112)tr(A∗(µI−L)A)and∥A∥ =(cid:112) λ (A∗(µI−L)A)≥
HS(H,W) H→W 1
µ−1/2∥A∥ .
H→L2
Therefore,π(9) implies that the regularized energy norm, while dominating the classical L2 one,
π
exerts a balancing effect on the estimation of the resolvent. This leads us to the first general result
regarding the well-posedness of this framework.
Proposition 1. Given µ > 0, let H ⊆ Wµ(X) be the RKHS associated to kernel k ∈ C2(X×
π
X) such that Z ∈ HS(H,Wµ(X)), and let P be the orthogonal projector onto the closure of
µ π H
Im(Z ) ⊆ Wµ(X). Then for every ε > 0 there exists a finite rank operator G: H→H such that
µ π
R(G)≤∥(I−P )(µI−L)−1Z ∥2 +ε. Consequently, when k is universal, R(G)≤ε.
H µ HS(H,W)
Thepreviouspropositionrevealsthatwheneverthehypotheticaldomainisdenseinthetruedomain
and the injection operator is Hilbert-Schmidt, there is no irreducible risk and one can find arbitrarily
good finite rank approximations of the generator’s resolvent. Note that Z ∈ HS(H,Wµ(X)) is
µ π
equivalent to Z∗Z =S∗(µI−L)S being a trace class operator, which is assured for our Examples 1
µ µ π π
and 2, see the discussion in E.
Now,toaddresshowminimizationoftheriskimpactstheestimationofthespectraldecomposition,
let us define the operator norm error and the metric distortion functional, respectively, as
E(G)=∥(µI−L)−1Z −Z G∥ , G∈HS(H), and η(h)=∥h∥ /∥h∥ , h∈H. (10)
µ µ H→W H W
6Proposition 2. Let G(cid:98) =(cid:80) i∈[r](µ−λ(cid:98)i)−1(cid:98)h i⊗g
(cid:98)i
be the spectral decomposition of G(cid:98): H→H, where
λ(cid:98)i ≥λ(cid:98)i+1 and let f(cid:98)i =S π(cid:98)h i/∥S π(cid:98)h i∥ L2, for i∈[r]. Then for every µ>0 and i∈[r]
π
|µ−|λ
λi
i− ||µλ(cid:98) −i|
λ(cid:98)i|
≤E(G(cid:98))η((cid:98)h i) and ∥f(cid:98)i−f i∥2
L2
π
≤
µ[gap2 iE −(G(cid:98) E) (η G(cid:98)( )(cid:98)h
ηi
()
(cid:98)h i)]
+, (11)
where gap is the difference between i-th and (i+1)-th eigenvalue of (µI−L)−1.
i
Note that the estimation of the eigenfunctions is first obtained in the norm with respect to the
√
energy space, and then transformed to the L2-norm, as ∥f∥ ≥ µ∥f∥ , f∈W . Therefore, by
controlling the operator norm error and metπ ric distortion (rW efer to C),L w2 πe can guµ a,γ rantee accurate
spectral estimation. Consequently, this allows us to approximately solve the SDE (1) starting from
an initial condition.
E[h(X t)|X 0=x]=[eLtS πh](x)≈(cid:80) i∈[r]eλ(cid:98)it⟨g (cid:98)i,h⟩ H(cid:98)h i(x). (12)
4 Empirical risk minimization
Inthissectionweaddressempiricalriskminimization(ERM),derivingtwomainestimators. Thefirst
one minimizes empirical risk with Tikhonov regularization, while the second introduces additional
regularization in the form of rank constraints.
Topresenttheestimator,wedenotethecovarianceoperatorw.r.t. L2 byC =S∗S =E [ϕ(x)⊗
π π π x∼π
ϕ(x)],thecross-covarianceoperatorT =S∗LS ,capturingcorrelationsbetweeninputandtheoutputs
π π
of the generator, and the covariance operator W = Z∗Z = S∗(µI−L)S w.r.t. energy space W.
µ µ µ π π
All operators can be estimated from data, depending on the available prior knowledge. In this work
we focus on the case when
Dirichletgradientformisknown. ThenwecandefinetheembeddingoftheDirichletoperatorB =
s⊤∇: L2(X) → [L2(X)]p into RKHS dϕ: X → Hp via the reproducing property as ⟨dϕ(x),h⟩ =
π π H
[BS h](x)=s(x)⊤Dh(x)∈Rp, h∈H. More precisely, we have that dϕ(x) is a p-dimensional vector
π
with components d ϕ(x), where d ϕ: X → H is given via reproducing property ⟨d ϕ(x),h⟩ =
k k k H
s (x)⊤Dh(x), k ∈[p]. Hence, in this case we have that
k
T =−E [dϕ(x)⊗dϕ(x)]=−(cid:80) E [d ϕ(x)⊗d ϕ(x)]. (13)
x∼π k∈[p] x∼π k k
√
Moreover, defining wϕ: X →Hp+1 by wϕ(x)=[ µϕ(x),d ϕ(x),...d ϕ(x)]⊤ ∈Rp+1, we get
1 p
W =E [µϕ(x)⊗ϕ(x)+dϕ(x)⊗dϕ(x)]=E [wϕ(x)⊗wϕ(x)]. (14)
µ x∼π x∼π
Regularized risk Let us first introduce the regularized risk defined for some γ >0 by
R (G)=∥(µI−L)−1/2S −(µI−L)1/2S G∥2 +γµ∥G∥2 , G∈HS(H), (15)
γ π π HS(H,L2) HS(H)
π
which, after some algebra, can be written as
R (G) = tr(cid:2) G∗(µC−T+µγI)G−2CG+S∗(µI−L)−1S (cid:3)
γ π π
= ∥W−1/2C−W1/2G∥2 +E [χ (·)]−∥W−1/2C∥2
µ,γ µ,γ HS(H) µ µ µ,γ HS(H)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
varianceterm biasterm
where W =W +µγI is the regularized covariance w.r.t. W.
µ,γ µ
7Hence, assumingtheaccesstothedatasetD =(x ) madeofi.i.d. samplesfromtheinvariant
n i i∈[n]
distribution π, replacing the regularized energy E
µ
with its empirical estimate E(cid:98)µ leads to the
regularized empirical risk functional expressed as
R(cid:98)γ(G)=∥W(cid:99) µ− ,γ1/2C(cid:98)−W(cid:99) µ1 ,/ γ2G∥2 HS(H)+E(cid:98)µ[χ µ(·)]−∥W(cid:99) µ− ,γ1/2C(cid:98)∥2 HS(H), (16)
where W and C are estimated by their empirical counterparts W and C, respectively, via (13).
µ,γ µ,γ
Therefore, our regularized empirical risk minimization approach reduces to
min∥W(cid:99)−1/2C(cid:98)−W(cid:99)1/2G∥2 , (17)
µ,γ µ,γ HS(H)
G
and we analyze two different estimators, the first one G(cid:98)µ,γ is obtained by minimizing regularized
empirical risk (17) over all G∈HS(H), and, hence, the name Kernel Ridge Regression (KRR) of the
generators resolvent. The second one G(cid:98)r minimizes (17) subject to the (hard) constraint that G is
µ,γ
at most of (a priori fixed) rank r ∈N and, hence, is named Reduced Rank Regression (RRR) of the
generator’s resolvent. Notice that when r =n, the two estimators coincide. After some algebra, one
sees that both minimization problems have closed form solutions
G(cid:98)µ,γ =W(cid:99) µ− ,γ1C(cid:98) and G(cid:98)r
µ,γ
=W(cid:99) µ− ,γ1/2[[W(cid:99) µ− ,γ1/2C(cid:98)]] r, (18)
where [[·]] denotes the r-truncated SVD of a compact operator.
r
To conclude this section, we show how to compute the eigenvalue decomposition of (18). To this
end, we define the sampling operators S(cid:98): H→Rn and Z(cid:98)µ: H→R(1+p)n by
(S(cid:98)h)
i
= √1 nh(x i),i∈[n], and (Z(cid:98)µh)
kn+i
=(cid:40) √ √√ 1n nµ sh k( (x xi i) ),
⊤Dh(x i),
kk ∈= [p0 ], ,i i∈∈ [[ nn ]] .,
Further,letK=n−1[k(x ,x )] ∈Rn×n bekernelGrammatrix,andintroducetheGrammatrices
i j i,j∈[n]
N∈Rn×pn and M∈Rpn×pn whose elements, for k ∈[1+p],i,j ∈[n] are
N =n−1⟨ϕ(x ),d ϕ(x )⟩ and M =n−1⟨d ϕ(x ),d ϕ(x )⟩ . (19)
i,(k−1)n+j i k j H (k−1)n+i,(ℓ−1)n+j k i ℓ j H
We note that although we have introduced the above matrices via inner products in H, they can be
readily computed via the kernel and its gradients knowing the Dirichlet form, see D.
Theorem 1. Given µ>0 and γ >0, let J =K−N(M+γµI)−1N⊤+γI. Let (σ2,v ) be the
µ,γ (cid:98)i i i∈[r]
leading eigenpairs of the following generalized eigenvalue problem
µ−1(J −γI)Kv =σ2J v , v⊤Kv =δ , i,j ∈[r]. (20)
µ,γ i (cid:98)i µ,γ i i j ij
Denoting V =[v |...|v ]∈Rn×r and Σ =diag(σ ,...,σ ), if (ν ,wℓ,wr) are eigentriplets of
r 1 r r (cid:98)1 (cid:98)r i i i i∈[r]
matrix V⊤
r
V rΣ2
r
∈Rr×r, then the eigenvalue decomposition the RRR estimator G(cid:98)r
µ,γ
=Z(cid:98)µU rV⊤
t
S(cid:98) is
given by G(cid:98)r
µ,γ
= (cid:80) i∈[r](µ−λ(cid:98)i)−1(cid:98)h i⊗g (cid:98)i, where λ(cid:98)i = µ−1/ν i, g (cid:98)i=ν i−1/2S(cid:98)∗V rw iℓ and (cid:98)h i=Z(cid:98) µ∗U rw ir
for U =(µγ)−1[µ−1/2I|−N(M+γµI)−1]⊤(KV −µV Σ2)∈R(1+p)n×r.
r r r r
The main computational cost of our method, in view of (12), to solve SDE (1) lies in the implicit
inversion of J when solving (20). When computed with direct solvers this inversion is of the order
µ,γ
O(n3p3), however leveraging on the fact that µJ is Schur’s complement of the (1+p)n×(1+p)n
µ,γ
symmetricpositivedefinitematrixandusingclassicaliterativesolvers, likeLanczosorthegeneralized
Davidson method, when r ≪n this cost can significantly be reduced to O(rn2p2), c.f. [14].
85 Spectral learning bounds
Recalling Prop. 2, in order to obtain the bounds on eigenvalues and eigenfunctions of the generator,
itsufficestoanalyzethelearningratesfortheoperatornormerrorE andmetricdistortionη. Forthis
purpose, we analyze the operator norm error of empirical estimator G(cid:98)r using the decomposition
µ,γ
E(G(cid:98))≤∥(µI−L)−1Z µ−Z µG µ,γ∥ H→W+∥Z µ(G µ,γ−Gr µ,γ)∥ H→W+∥Z µ(Gr µ,γ−G(cid:98)r µ,γ)∥ H→W,
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
regularizationbias rankreductionbias estimator’svariance
where G =W−1C is the minimizer of the full (i.e. without rank constraint) Tikhonov regularized
µ,γ µ,γ
risk and Gr µ,γ=W µ− ,γ1/2[[W µ− ,γ1/2C]]
r
is the population version of the empirical estimator G(cid:98)r µ,γ.
Note that, while the last two terms in the above decomposition depend on the estimator, the first
term depends only on the choice of H and the regularity of L w.r.t. H. In this work we focus on
the classical kernel-based learning where one chooses a universal kernel [38, Chapter 4] and controls
the regularization bias with a regularity condition. For details see Rem. 2 of App. E.2. Let µ>0
be a prescribed parameter, we make following assumptions to quantify the difficulty of the learning
problem:
(BK) Boundedness. There exists c >0 such that esssup∥wϕ(x)∥2≤c , i.e. wϕ∈L∞(X,H1+p);
W W π
x∼π
(RC) Regularity. For some α∈(0,2] there exists c >0 such that C2 ⪯c2W1+α;
α α µ
(SD) Spectral Decay. There exists β∈(0,1] and c >0 s.t. λ (W )≤c j−1/β, for all j ∈J.
β j µ β
The above assumptions, discussed in more details in App E.1, are in the spirit of state-of-the-art
analysis of statistical learning theory of classical regression in RKHS spaces [12], recently extended
to regression of transfer operators [28, 22]. In our setting, instead of relying on the injection into L2
π
as in the case of transfer operators, the relevant object is the injection Z into the energy space W.
µ
The first assumption (BK) is the main limiting factor of our approach. Indeed, since ∥wϕ(x)∥2 =
µ∥ϕ(x)∥2+(cid:80) ∥d ϕ(x)∥2,apartfromneedingthekerneltobebounded,wealsoneedtheDirichlet
k∈[p] k
formembeddingtobebounded. Essentially, thismeansthattheDirichletcoefficientsarenotgrowing
too fast w.r.t. the kernel’s gradients decay.Having this, we assure that Z ∈HS(H,Wµ(X)) which
µ π
implies the operator norm error can be controlled.
Another key difference between generator and transfer operator regression is that the covariance
w.r.t. the domain of the operator becomes W =Z∗Z instead of C =S∗S . On the other hand, the
µ µ µ π π
"cross-covariance" that captures now RKHS cross-correlations of the resolvent w.r.t domain is simply
Z∗(µI−L)−1Z =S∗(µI−L)(µI−L)−1S =C. With this in mind, (RC) corresponds to the the
µ µ π π
regularity condition in [22] and it quantifies the relationship between the hypothesis class (bounded
operators in RKHS) and the object of interest (µI−L)−1. Indeed, if L has eigenfunctions that
belong to α-interpolation space between H and Wµ(X), (RC) holds true. In particular, if f ∈H
π i
for all i≥2 (constant eigenfunction excluded), one has that α≥1 (c.f. Proposition 7 in App. E.1.
Finally, (SD) quantifies the "size" of the hypothetical domain H within the true domain Wµ(X)
π
via the effective dimension tr(W−1W )≤c (µγ)−β, which, due to ((BK)), leads to another notion,
µ,γ µ β
known as the embedding property, quantifying the relationship between H and essentially bounded
functions in the domain of the operator
(KE) Kernel Embedding. There exists τ ∈[β,1] and such that
c =esssup(cid:80) σ2τ[µ|z (x)|2−z (x)[Lz ](x)]<+∞, (21)
τ j∈N j j j j
x∼π
where Z =(cid:80) σ z ⊗h is the SVD of the injection operator Z : H→Wµ(X).
µ j∈J j j j µ π
9Using the above assumptions, we prove a generalization bound for the RRR estimator of the
resolvent.
Theorem 2. Let (RC), (SD), and (KE) hold for some α ∈ (0,2], β ∈ (0,1] and τ ∈ [β,1],
respectively, and let cl(Im(S )) = L2(X). Denoting λ⋆ = λ (S∗(µI−L)−1S ), k ∈ N, and given
π π k k π π
δ ∈(0,1) and r ∈[n], if RRR estimator is build from the Dirichlet form and
if α≥τ, γ ≍n− α+1 β and ε⋆ n =n− 2(αα +β), or if α<τ, γ ≍n− β+1 τ and ε⋆ n =n− 2(βα +τ) (22)
then there exists a constant c>0, depending only on H and gap λ⋆−λ⋆ >0, such that for large
r r+1
enough n≥r with probability at least 1−δ in the i.i.d. draw of D from π it holds that
n
E(G(cid:98)r µ,γ)≤(σ
(cid:98)r+1
∧ (cid:112) λ⋆ r+1)+cε⋆
n
lnδ−1. (23)
Proof sketch. The regularization bias is bounded by c γα/2 by Prop. 9 of App. E.2, the rank
α
reduction bias is upper bounded by λ (S∗(µI−L)−1S ), while the bounds on the variance terms
r+1 π π
critically rely on the well-known perturbation result for spectral projectors reported in Prop. 4,
App. A. The latter is then chained to Pinelis-Sakhanenko’s inequality and Minsker’s inequality for
self-adjoint HS-operators, Props. 12 and 13 in App. E.3.1, respectively. Combining the bias and
variance terms, we obtain the balancing equations for the regularization parameter and then the next
result follows. □
Note that the learning rate (23) implies the L2-norm learning rate. Moreover, for α ≥ τ, it
π
matches information theoretic lower bounds for transfer operator learning upon replacing parameters
α, β and τ related to the space W with their L2 analogues [22], see App. E.6. This motivates the
π
development of the first mini-max optimality for the IG learning, for which our results an important
first step.
To conclude this section, we address the spectral learning bounds steaming from the Prop. 2.
The main task to do so is to control the metric distortions, which we show how in App. E.5. In
this context, an additional assumption α≥1 is needed, since otherwise the metric distortions can
blow-up due to eigenfunctions being out of the RKHS space. Importantly, our analysis reveals that
|λi−λ(cid:98)i| ≤(s ∧ 2(cid:112) λ⋆ /λ⋆)+cε⋆ lnδ−1, i∈[r], (24)
|µ−λi||µ−λ(cid:98)i| (cid:98)i r+1 r n
wheres =σ η istheempiricalspectralbiasthatinformshowgoodistheestimationoftheparticular
(cid:98)i (cid:98)i(cid:98)i
eigenpair is (see Fig. 1 a)), σ
(cid:98)i
being given in (20) and η (cid:98)i=∥(cid:98)h i∥ H/∥W(cid:99)µ1/2 (cid:98)h i∥ H.
6 Experiments
In this section, we showcase the key features of our method outlined in Table ??. We demonstrate
that our approach: (1) avoids the spurious effects noted in other IG methods [15, 1], (2) is more
effective than transfer operator methods [21], and (3) validates our bounds in a prediction task for a
model with a non-constant diffusion term. Further details are available in Appendix F.
One dimensional four well potential WefirstinvestigatetheoverdampedLangevindynamics
inapotentialthatpresentsfourdifferentwells,twoprincipalwellsandthenineachofthemtwosmaller
wells, given by V(x)=4(x8+0.8exp(−80(x2))+0.2exp(−80(x−0.5)2)+0.5exp(−40(x+0.5)2)). This
leads to three relevant eigenpairs: the slowest mode corresponds to the transition between the two
principal wells, while the others two capture transitions between the smaller wells. In Figure 1, we
show that the empirical bias s =σ η allows us to choose the hyperparameters of the model: higher
(cid:98)1 (cid:98)1(cid:98)1
10Figure 1: a) Empirical biases sˆ =σ η and estimation of the first (nontrivial) eigenfunction of the
1 (cid:98)1(cid:98)1
IG of a Langevin process under a four well potential. Ground truth is black, our method RRR is
red and blue for two different kernel lengthscales. b) Eigenvalue estimation for the same process
compared to the methods in [15, 1], for which eigenvalue histogram in blue shows spuriousness. c)
Estimation of the second eigenfunction of a Langevin process under Muller brown potential (white
level lines) by RRR, Transfer Operator (TO) in d) and ground truth in e). Observe that TO fails to
recover the metastable state. f) Prediction RMSE for the CIR model.
empirical bias leads to wrong operator. Compared to KRR [15, 1], there is no spuriousness in the
estimation of our eigenvalues, as shown in panel b).
Muller Brown potential WenextstudyLangevindynamicsundermorechallengingconditions:
the Muller brown potential. Figure 1(c-e) depicts the second eigenfunction obtained by our method
compared to the ground truth one and that found by the transfer operator approach, with the same
number of samples. Notably, our physics informed approach outperforms transfer operator learning
for this task. Note that with different lag times, we were able to recover this second eigenfunction.
CIR model Finally, we show with the CIR model that our method is not limited to Langevin
process with constant diffusion. For this process, the conditional expectation of the state X is
t
analyticallyknown. Wecanthuscomparethepredictionofourmodelwithrespecttothisexpectation
using root mean squared error (RMSE) and compute it for different number of samples to validate
our bounds. Conditional expectation were computed on 100 different simulations at t = ln(2)/a
which corresponds to the half life of the mean reversion. Results are shown in panel d) Figure 1.
7 Conclusion
We developed a novel energy-based framework for learning the Infinitesimal Generator of stochastic
diffusion SDEs using kernel methods. Our approach integrates physical priors, achieves fast error
rates, and provides the first spectral learning guarantees for generator learning. A limitation is its
computational complexity, scaling as n2d2. Future work will explore alternative methods to enhance
computational efficiency and investigate a broader suite of SDEs beyond stochastic diffusion.
11References
[1] A Cabannes, V. and Bach, F. (2024). The Galerkin method beats graph-based approaches for
spectralalgorithms. InProceedingsofThe27thInternationalConferenceonArtificialIntelligence
and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 451–459. PMLR.
[2] Alexander, R. and Giannakis, D. (2020). Operator-theoretic framework for forecasting nonlinear
time series with kernel analog techniques. Physica D: Nonlinear Phenomena, 409:132520.
[3] Andrew, G., Arora, R., Bilmes, J., and Livescu, K. (2013). Deep canonical correlation analysis.
In International conference on machine learning, pages 1247–1255. PMLR.
[4] Bakry, D., Gentil, I., and Ledoux, M. (2014). Analysis and Geometry of Markov Diffusion
Operators. Springer.
[5] Bevanda, P., Beier, M., Kerz, S., Lederer, A., Sosnowski, S., and Hirche, S. (2021).
KoopmanizingFlows: Diffeomorphically Learning Stable Koopman Operators. arXiv preprint
arXiv.2112.04085.
[6] Bouvrie, J. and Hamzi, B. (2017). Kernel Methods for the Approximation of Nonlinear Systems.
SIAM Journal on Control and Optimization, 55(4):2460–2492.
[7] Brunton, S. L., Budišić, M., Kaiser, E., and Kutz, J. N. (2022). Modern Koopman Theory for
Dynamical Systems. SIAM Review, 64(2):229–340.
[8] Caponnetto, A. and De Vito, E. (2007). Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331–368.
[9] Das,S.andGiannakis,D.(2020). KoopmanspectrainreproducingkernelHilbertspaces. Applied
and Computational Harmonic Analysis, 49(2):573–607.
[10] Davis, C. and Kahan, W. M. (1970). The rotation of eigenvectors by a perturbation. iii. SIAM
Journal on Numerical Analysis, 7(1):1–46.
[11] Fan, F., Yi, B., Rye, D., Shi, G., and Manchester, I. R. (2021). Learning Stable Koopman
Embeddings. arXiv preprint arXiv.2110.06509.
[12] Fischer, S. and Steinwart, I. (2020). Sobolev norm learning rates for regularized least-squares
algorithms. Journal of Machine Learning Research, 21(205):1–38.
[13] Haag,R.(2012). Localquantumphysics: Fields,particles,algebras. SpringerScience&Business
Media.
[14] Hogben, L., editor (2006). Handbook of Linear Algebra. CRC Press, Boca Raton, FL, USA.
[15] Hou, B., Sanjari, S., Dahlin, N., Bose, S., and Vaidya, U. (2023). Sparse learning of dynamical
systems in RKHS: An operator-theoretic approach. In Krause, A., Brunskill, E., Cho, K., Engel-
hardt,B.,Sabato,S.,andScarlett,J.,editors,Proceedingsofthe40thInternationalConferenceon
Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 13325–13352.
PMLR.
[16] Inzerilli, P., Kostic, V., Lounici, K., Novelli, P., and Pontil, M. (2023). Consistent long-term
forecasting of ergodic dynamical systems.
12[17] Kawahara, Y. (2016). Dynamic Mode Decomposition with Reproducing Kernels for Koopman
Spectral Analysis. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R., editors,
Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.
[18] Kessler, M., Lindner, A., and Sørensen, M. (2012). Statistical methods for stochastic differential
equations. Monographs on Statistics and Applied Probability, 124:7–12.
[19] Klus, S., Nüske, F., Peitz, S., Niemann, J.-H., Clementi, C., and Schütte, C. (2020). Data-driven
approximation of the koopman generator: Model reduction, system identification, and control.
Physica D: Nonlinear Phenomena, 406:132416.
[20] Klus, S., Schuster, I., and Muandet, K. (2019). Eigendecompositions of transfer operators in
reproducing kernel Hilbert spaces. Journal of Nonlinear Science, 30(1):283–315.
[21] Kostic, V., Novelli, P., Maurer, A., Ciliberto, C., Rosasco, L., and Pontil, M. (2022). Learning
dynamical systems via Koopman operator regression in reproducing kernel hilbert spaces. In
Advances in Neural Information Processing Systems.
[22] Kostic, V. R., Lounici, K., Novelli, P., and massimiliano pontil (2023a). Sharp spectral rates
for koopman operator learning. In Thirty-seventh Conference on Neural Information Processing
Systems.
[23] Kostic, V. R., Novelli, P., Grazzi, R., Lounici, K., et al. (2023b). Learning invariant representa-
tionsoftime-homogeneousstochasticdynamicalsystems. InTheTwelfthInternationalConference
on Learning Representations.
[24] Kutoyants, Y. A. (1984). Parameter estimation for stochastic processes. Berlin: Heldermann.
[25] Kutoyants, Y. A. (2013). Statistical inference for ergodic diffusion processes. Springer Science
& Business Media.
[26] Kutz, J. N., Brunton, S. L., Brunton, B. W., and Proctor, J. L. (2016). Dynamic Mode
Decomposition. Society for Industrial and Applied Mathematics.
[27] Li, Q., Dietrich, F., Bollt, E. M., and Kevrekidis, I. G. (2017). Extended dynamic mode
decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the
koopman operator. Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(10).
[28] Li, Z., Meunier, D., Mollenhauer, M., and Gretton, A. (2022). Optimal rates for regularized
conditional mean embedding learning. In Advances in Neural Information Processing Systems.
[29] Lusch, B., Kutz, J. N., and Brunton, S. L. (2018). Deep learning for universal linear embeddings
of nonlinear dynamics. Nature Communications, 9(1).
[30] Mackey, M. C. (2011). Time’s Arrow: The origins of thermodynamic behavior. Courier Corpo-
ration.
[31] Mardt, A., Pasquali, L., Wu, H., and Noé, F. (2018). VAMPnets for deep learning of molecular
kinetics. Nature Communications, 9(1).
[32] Minsker, S. (2017). On some extensions of Bernstein’s inequality for self-adjoint operators.
Statistics & Probability Letters, 127:111–119.
13[33] Oksendal, B. (2013). Stochastic differential equations: an introduction with applications.
Springer Science & Business Media.
[34] Pascucci, A. (2011). PDE and Martingale Methods in Option Pricing. Springer Milan.
[35] Pavliotis, G. A. (2014). Stochastic Processes and Applications. Springer New York.
[36] Pillaud-Vivien, L. and Bach, F. (2023). Kernelized diffusion maps. In The Thirty Sixth Annual
Conference on Learning Theory, pages 5236–5259. PMLR.
[37] Reed, M. and Simon, B. (1980). I: Functional Analysis. Academic Press.
[38] Steinwart, I. and Christmann, A. (2008). Support Vector Machines. Springer New York.
[39] Tian, W. and Wu, H. (2021). Kernel embedding based variational approach for low-dimensional
approximationofdynamicalsystems. ComputationalMethodsinAppliedMathematics,21(3):635–
659.
[40] Trefethen, L. N. and Embree, M. (2020). Spectra and Pseudospectra: The Behavior of
Nonnormal Matrices and Operators. Princeton University Press.
[41] Tropp, J. A. (2012). User-friendly tail bounds for sums of random matrices. Technical report.
[42] Tuckerman, M. E. (2023). Statistical mechanics: theory and molecular simulation. Oxford
university press.
[43] Williams, M. O., , Rowley, C. W., and Kevrekidis, I. G. (2015). A kernel-based method for
data-driven Koopman spectral analysis. Journal of Computational Dynamics, 2(2):247–265.
[44] Wu, H. and Noé, F. (2019). Variational Approach for Learning Markov Processes from Time
Series Data. Journal of Nonlinear Science, 30(1):23–66.
[45] Yeung, E., Kundu, S., and Hodas, N. (2019). Learning deep neural network representations
for koopman operators of nonlinear dynamical systems. In 2019 American Control Conference
(ACC), pages 4832–4839. IEEE.
[46] Yosida, K. (2012). Functional analysis, volume 123. Springer Science & Business Media.
[47] Zabczyk, J. (2020). Mathematical Control Theory: An Introduction. Systems & Control:
Foundations & Applications. Springer International Publishing.
[48] Zhang, W., Li, T., and Schütte, C. (2022). Solving eigenvalue pdes of metastable diffusion
processes using artificial neural networks. Journal of Computational Physics, 465:111377.
[49] Zhou, D.-X. (2008). Derivative reproducing properties for kernel methods in learning theory.
Journal of computational and Applied Mathematics, 220(1-2):456–463.
[50] Zwald, L. and Blanchard, G. (2005). On the Convergence of Eigenspaces in Kernel Principal
Component Analysis. In Advances in Neural Information Processing Systems.
14Supplementary Material
This appendix includes additional background on SDE and RKHS, proofs of the results omitted
in the main body and information about the numerical experiments.
A Background
A.1 Basics on operator theory for Markov processes.
We provide here some basics on operator theory for Markov processes. Let X ⊂ Rd (d ∈ N) and
(X ) be a X-valued time-homogeneous Markov process defined on a filtered probability space
t t∈R+
(Ω,F,(F ) ,P) where F =σ(X , s≤t) is the natural filtration of (X ) . The dynamics of
t t∈R+ t s t t∈R+
X can be described through of a family of probability densities (p ) such that for all t ∈ R ,
t t∈R+ +
E ∈B(X),
(cid:90)
P(X ∈E|X =x)= p (x,y)dy.
t 0 t
E
Let G be a set of real valued and measurable functions on X. For any t∈R the transfer operator
+
(TO) A :G →G maps any measurable function f ∈G to
t
(cid:90)
(A f)(x)= f(y)p (x,y)dy. (25)
t t
X
IntheoryofMarkovprocesses, thefamily(A ) isreferredtoastheMarkovsemigroupassociated
t t∈R+
to the process X.
Remark 1. A possible choice is G =L∞(X). Here, we are interested in another choice of G related
to the existence of an invariant measure for A i.e., a σ-finite measure π on (X,B(X)) such that
t
P∗π = π for any t ∈ R . In that case, we can choose G = L2(X) so that, for all f ∈ L2(X), P f
t + π π t
converges to f in L2(X) as t goes to 0. Note that P f =f and P f =(cid:82) fdX for a suitable integrable
π 0 ∞
function f :X →R.
Within the existence of this invariant measure π, the process X is then characterized by the
infinitesimal generator (IG) L:L2(X)→L2(X) of the family (A ) defined by
π π t t∈R+
A −I
L= lim t . (26)
t→0+ t
In other words, L characterizes the linear differential equation ∂/∂ A f = LA f satisfied by the
t t t
transfer operator. The domain of L denoted dom(L) coincides with the the Sobolev space W1,2(X)
π
W1,2(X)={f ∈L2(X)|∥f∥2 =∥f∥ +∥∇f∥ <∞}.
π π W L2 π L2 π
The spectrum of the IG can be difficult to capture due to the potential unboundedness of L. To
circumvent this problem, one can focus on an auxiliary operator, the resolvent, which, under certain
conditions, shares the same eigenfunctions as L and becomes compact. The following result can be
found in Yosida’s book ([46], Chap. IX) : For every µ>0, the operator (µI−L) admits an inverse
L =(µI−L)−1 that is a continuous operator on X and
µ
(cid:90) ∞
(µI−L)−1 = e−µtA dt.
t
0
15The operator L is the resolvent of L and the corresponding resolvent set of L is defined by
µ
ρ(L)=(cid:8) µ∈C|(µI−L)is bijective andL is continuous(cid:9) .
µ
In fact, ρ(L) contains all real positive numbers and (µI−L)−1 is bounded. Here, we assume that
L has compact resolvent, i.e. ρ(L)̸=∅ and there exists µ ∈ρ(L) such that L is compact. Note
0 µ0
that, through the resolvent identity, this implies the compactness of all resolvents L for µ∈ρ(L).
µ
Let then µ ∈ ρ(L). As (L,dom(L)) is assumed to be self-adjoint, so does L , so that L is both
µ µ
compact and self-adjoint. Then, its spectrum Sp(L )=C\ρ(L ) is purely discrete and consists of
µ µ
isolated eigenvalues (λµ) such that |λµ|→0 associated with an orthonormal basis (f ) (see
i i∈N i i i∈N
[46], chapter XI). In other words, the spectral decomposition of the resolvent writes
(cid:88)
L = λµf ⊗f
µ i i i
i∈N
where the functions f ∈L2(X) are also eigenfunctions of the operator L we get
i π
(cid:88)
L= λ f ⊗f .
i i i
i∈N
Example 3 (Langevin). Let (k T) ∈ R.The overdamped Langevin equation driven by a potential
b
(cid:112)
V : Rd → R is given by dX = −∇V(X )dt+ 2(k T)dW and X = x. Its invariant mea-
t t b t 0
sure of the solution process X is the Boltzman distribution π(dx) = z−1e−(kbT)−1V(x)dx where z
denotes a normalizing constant. Its infinitesimal generator L is defined by Lf = −∇V⊤∇f +
(k T)∆f, for f ∈W1,2(X), and if ∇V ∈L2(X) is positive and coercive, it has compact resolvent.
b π π
Finally, since (cid:82) (−Lf)gdπ = −(cid:82)
(cid:104) ∇(cid:16)
(k
T)∇f(x)e−(kbT)−1V(x)(cid:17)(cid:105)
g(x)dx = (k T)(cid:82) ∇f⊤∇gdπ =
b Z b
(cid:82)
f(−Lg)dπ, generator L is self-adjoint and associated to a gradient Dirichlet form with s(x) =
(k T)1/2(δ ) .
b ij i∈[d],j∈[p]
Example 4 (Cox-Ingersoll-Ross process). Let d = 1, a,b ∈ R, σ ∈ R∗. The Cox-Ingersoll-Ross
√ +
process is solution of the SDE dX =(a+bX )dt+σ X dW and X =x. Its invariant measure
t t t t 0
π is a Gamma distribution with shape parameter a/σ2 and rate parameter b/σ2. Its infinitesimal
generator L is defined for f ∈L2(X) by Lf =(a+bx)∇f+σ2x∆f. Note that by integration by parts,
π 2
we can check that the generator L satisfies
(cid:82)
(−Lf)gdπ
=(cid:82) σ2xf′(x)g′(x)π(dx)=(cid:82)
f(−Lg)dπ, and
√ 2√
it is associated to a gradient Dirichlet form with s(x)=σ x/ 2.
A.2 Infinitesimal generator for diffusion processes
After defining the infinitesimal generator for Markov processes (see A.1), we provide its explicit form
for solution processes of equations like(1). Given a smooth function f ∈C2(X,R), Itô’s formula (see
for instance [4], B, p.495) provides for t∈R ,
+
(cid:90) t d (cid:90) t d
(cid:88) (cid:88)
f(X )−f(X )= ∂ f(X )dXi+ 1 ∂2f(X )d⟨Xi,Xj⟩
t 0 i s s 2 ij s s
0 i=1 0 i,j=1
(cid:90) t (cid:90) t
= ∇f(X )⊤dX + 1 Tr(cid:2) X⊤(∇2f)(X )X (cid:3) ds.
s t 2 s s s
0 0
16Recalling (1), we get
(cid:90) t(cid:20) (cid:21)
f(X )=f(X )+ ∇f(X )⊤a(X )+ 1Tr(cid:2) b(X )⊤(∇2f(X ))b(X )(cid:3) ds
t 0 s s 2 s s s
0
(cid:90) t
+ ∇f(X )⊤b(X )dW . (27)
s s s
0
Provided f and b are smooth enough, the expectation of the last stochastic integral vanishes so that
we get
E[f(X )|X =x]=f(x)+(cid:90) t E(cid:104) ∇f(X )⊤a(X )+ 1Tr(cid:2) b(X )⊤(∇2f(X ))b(X )(cid:3)(cid:12) (cid:12)X =x(cid:105) ds
t 0 s s 2 s s s (cid:12) 0
0
Recalling that L= lim (A f −f)/t, we get for every x∈X,
t
t→0+
E[f(X )|X =x]−f(x)
Lf(x)= lim t 0
t→0 t
= lim 1(cid:20)(cid:90) t E(cid:104) ∇f(X )⊤a(X )+ 1Tr(cid:2) (X )⊤(∇2f(X ))b(X )(cid:3)(cid:105) ds(cid:12) (cid:12)X =x(cid:21)
t→0 t 0 s s 2 s s s (cid:12) 0
=∇f(x)⊤a(x)+ 1Tr(cid:2) b(x)⊤(∇2f(x))b(x)(cid:3) , (28)
2
which provides the closed formula for the IG associated with the solution process of (1).
A.3 Spectral perturbation theory
Recalling that for a bounded linear operator A on some Hilbert space H the resolvent set of the
operator A is defined as ρ(A)=(cid:8) λ∈C|A−λI is bijective and (A−λI)−1 is continuous(cid:9), and its
spectrumSp(A)=C\{ρ(A)},letλ⊆Sp(A)beisolatedpartofspectra,i.e. bothλandµ=Sp(A)\λ
are closed in Sp(A). Than, the Riesz spectral projector P : H→H is defined by
λ
1 (cid:90)
P = (zI−A)−1dz, (29)
λ 2π
Γ
where Γ is any contour in the resolvent set Res(A) with λ in its interior and separating λ from
µ. Indeed, we have that P2 = P and H = Im(P )⊕Ker(P ) where Im(P ) and Ker(P ) are
λ λ λ λ λ λ
both invariant under A and Sp(A ) = λ, Sp(A ) = µ. Moreover, P +P = I and
P P =P P =0.
|Im(Pλ) |Ker(Pλ) λ µ
λ µ µ λ
Finally if A is compact operator, then the Riesz-Schauder theorem, see e.g. [37], assures that
Sp(T) is a discrete set having no limit points except possibly λ = 0. Moreover, for any nonzero
λ∈Sp(T), then λ is an eigenvalue (i.e. it belongs to the point spectrum) of finite multiplicity, and,
hence, we can deduce the spectral decomposition in the form
(cid:88)
A= λP , (30)
λ
λ∈Sp(A)
where geometric multiplicity of λ, r =rank(P ), is bounded by the algebraic multiplicity of λ. If
λ λ
additionally A is normal operator, i.e. AA∗ =A∗A, then P =P∗ is orthogonal projector for each
λ λ
λ∈Sp(A) and P =(cid:80)rλ ψ ⊗ψ , where ψ are normalized eigenfunctions of A corresponding to λ
λ i=1 i i i
and r is both algebraic and geometric multiplicity of λ.
λ
We conclude this section with well-known perturbation bounds for eigenfunctions and spectral
projectors of self-adjoint compact operators.
17Proposition 3 ([10]). Let A be compact self-adjoint operator on a separable Hilbert space H. Given a
pair (λ(cid:98),f(cid:98))∈C×H such that ∥f(cid:98)∥=1, let λ be the eigenvalue of A that is closest to λ(cid:98) and let f be its
normalizedeigenfunction. Ifg (cid:98)=min{|λ(cid:98)−λ||λ∈Sp(A)\{λ}}>0, thensin(∢(f(cid:98),f))≤∥Af(cid:98)−λ(cid:98)f(cid:98)∥/g (cid:98).
Proposition 4 ([50]). Let A and A(cid:98) be two compact operators on a separable Hilbert space. For
nonempty index set J ⊂N let
gap (A)=min{|λ (A)−λ (A)||i∈N\J, j ∈J}
J i j
denote the spectral gap w.r.t J and let P
J
and P(cid:98)J be the corresponding spectral projectors of A and
A(cid:98), respectively. If A is self-adjoint and for some ∥A−A(cid:98)∥<gap (A), then
J
∥A−A(cid:98)∥
∥P
J
−P(cid:98)J∥≤
gap
(A).
J
B Representations in the RKHS
In section 2, we have defined the infinitesimal generator of a diffusion process and specified its form
when associated with Dirichlet forms. These operators act on L2(X) or specific subsets of it. To
π
develop our learning procedure, we need to understand these operators’ actions when embedding
into the RKHS, and define their versions for feature maps.
IG and Dirichlet operator in RKHS. As a reminder, we consider H be an RKHS and let
k : X × X → R be the associated kernel function. The canonical feature map is denoted by
ϕ(x) = k(x,·) for x ∈ X k(x,x′) = ⟨ϕ(x),ϕ(x′)⟩ for all x,x′ ∈ X. Assuming that k is square-
integrable with respect to the measure π, we define the injection operator S :H(cid:44)→L2(X) and its
π π
adjoint S∗ :L2(X)→H by S∗f =(cid:82) f(x)ϕ(x)π(dx). As a preliminary step, we need to define the
π π π X
reproducing partial derivatives in RKHS, which we introduce via Mercer kernels.
Definition 1 (Mercer kernel). A kernel function k : X ×X →R is called a Mercer kernel if it is
a continuous and symmetric function such that for any finite set of points {x ,...,x } ⊂ X, the
1 n
matrix (k(x ,x ))n is positive semi-definite.
i j i,j=1
Several standard kernels satisfy the Mercer property with s≥1, including the Gaussian kernel
which we will consider subsequently.
For s ∈ N and m ∈ N, we define the index set Im = {α ∈ Nm : |α| ≤ s} where |α| = (cid:80)s α ,
s j=1 j
for α = (α ,...,α ) ∈ Nm. For a function f : Rm → R and x = (x ,...,x ) ∈ Rm, we denote its
1 m 1 m
partial derivative dαf at point x (if it exists) as
Dαf(x)=
(cid:89)m
Dαjf(x)=
∂|α|
f(x).
j ∂α1x 1···∂αmx
m
j=1
For a function k ∈C2s(X ×X) with X ⊂Rd and α∈Id, we define
s
∂|α|
Dαk(x,y)=D(α,0)k(x,y)= k(x,y). (31)
∂α1x 1···∂αmx
m
and
∂|α|
D(0,α)k(x,y)= k(x,y).
∂α1y 1···∂αmy
m
18Theorem 3 (Theorem 1 in [49]). Let s ∈ N, X ⊆ Rm and k be a Mercer kernel such that
k ∈C2s(X ×X) with corresponding RKHS H. Then the following hold:
i. For any x∈X, α∈Im,
s
(Dαk) (y)=Dαk(x,y)∈H. (32)
x
ii. A partial derivative reproducing property holds for α∈Im
s
(Dαh)(x)=⟨(Dαk) ,h⟩ , ∀h∈H. (33)
x H
Theorem 3 allows us to introduce the first and second order operators D and D2 that act on any
feature map ϕ(x) as
Dϕ(x)=((Deik) ) and D2ϕ(x)=((Dei+ejk) )
x i∈[d] x i,j∈[d]
where the (Deik)
x
and (Dei+ejk)
x
can be defined via (32). Then, we define the operator d that
maps any feature map ϕ(x) to dϕ(x) = s(x)⊤Dϕ(x). Denote s⊤ = [s¯ |...|s¯ ] : x ∈ X (cid:55)→ s⊤(x) =
1 d
[s¯ (x)|...|s¯ (x)]∈Rp×d. We have
1 d
d d
(cid:88) (cid:88)
s(x)⊤Dh(x)= s¯(x)∂ h(x)= s¯(x)⟨Deiϕ(x),h⟩ =⟨dϕ(x),h⟩ ,
i i i H H
i=1 i=1
so that we can define the embedding of the Dirichlet operator B =s⊤∇: L2(X)→[L2(X)]p into
π π
RKHS dϕ: X →Hp via the reproducing property as ⟨dϕ(x),h⟩ =[BS h](x)=s(x)⊤Dh(x)∈Rp,
H π
h∈H. In fact, dϕ is a p-dimensional vector with components d ϕ: X →H given via ⟨d ϕ(x),h⟩ =
k k H
s (x)⊤Dh(x), k ∈[p]. Then, we can define
k
(cid:80)
T =−E [dϕ(x)⊗dϕ(x)]=− E [d ϕ(x)⊗d ϕ(x)].
x∼π k∈[p] x∼π k k
which captures correlations between input and the outputs of the generator in the RKHS. Defining
√
wϕ: X →Hp+1 by wϕ(x)=[ µϕ(x),d ϕ(x),...d ϕ(x)]⊤ ∈Rd+1, we can consider
1 p
W =Z∗Z =S∗(µI−L)S
µ µ µ π π
=E [µϕ(x)⊗ϕ(x)+dϕ(x)⊗dϕ(x)]=E [wϕ(x)⊗wϕ(x)]
x∼π x∼π
which corresponds to the RKHS covariance operator w.r.t. energy space W.
Examples. One way to ensure that the essential assumption ((KE)) holds is to show that wϕ
fulfils the boundedness condition ((BK)), i.e. wϕ∈L∞(X,H1+p). Let’s show that the property holds
π
true for the Damped Langevin (see Example 1) and the CIR process (see Example 2) if we consider
the Radial Basis Function (RBF) kernel k(x,y) = k (y) = exp(−κ∥x−y∥2) where κ > 0 is a free
x
parameter that sets the “spread” of the kernel. As a reminder, for every x∈X, we have
p
(cid:88)
∥wϕ(x)∥2 =µ∥ϕ(x)∥2+ ∥d ϕ(x)∥2 with d =s (x)⊤Dϕ(x).
k k k
k=1
Recalling that for the overdamped Langevin process s(x)=(k T)1/2(δ ) , ss⊤ is diagonal so
b ij i∈[d],j∈[p]
that ⟨s(x)⊤Dk (·),s(x)⊤Dk (·)⟩=0 for every x∈X. As ∥ϕ(x)∥2 =1, we get that for every x∈X,
x x
∥wϕ(x)∥2 ≤µ=:c .
H √ √
Consider now the CIR process. We have d=p=1 and s(x)=σ x/ 2 for any x∈X. For the very
same reasons, ⟨s(x)⊤Dk (·),s(x)⊤Dk (·)⟩=0 for every x∈X, so that ∥wϕ(x)∥2 ≤µ=:c .
x x H
In both Langevin and CIR cases, we have wϕ∈L∞(X,H1+p) when considering an RBF kernel.
π
19C Statistical learning framework
C.1 Spectral perturbation bounds
In this section, we prove key perturbation result and discuss the properties of the metric distortion.
We conclude this section with the approximation bound for arbitrary estimator G ∈ B (H) that
r
is the basis of the statistical bounds that follow. This result is a direct consequence of [21] and
Davis-Khan spectral perturbation result for compact self-adjoint operators, [10].
In the framework of Koopman operator learning [22], spectral bounds are expressed in terms
of a distortion metric between the RKHS H and L2(X), corresponding to the cost incurred from
π
observing the operator’s action on the H rather than on its domain L2(X). Aligned with the risk
π
definition (9), here we measure in a certain way the distortion between the H and Wµ(X) as given
π
in definitions in (10).
Proposition 2. Let G(cid:98) =(cid:80) i∈[r](µ−λ(cid:98)i)−1(cid:98)h i⊗g
(cid:98)i
be the spectral decomposition of G(cid:98): H→H, where
λ(cid:98)i ≥λ(cid:98)i+1 and let f(cid:98)i =S π(cid:98)h i/∥S π(cid:98)h i∥ L2, for i∈[r]. Then for every µ>0 and i∈[r]
π
|µ−|λ
λi
i− ||µλ(cid:98) −i|
λ(cid:98)i|
≤E(G(cid:98))η((cid:98)h i) and ∥f(cid:98)i−f i∥2
L2
π
≤
µ[gap2 iE −(G(cid:98) E) (η G(cid:98)( )(cid:98)h
ηi
()
(cid:98)h i)]
+, (11)
where gap is the difference between i-th and (i+1)-th eigenvalue of (µI−L)−1.
i
Proof. We first remark that
∥((µI−L)−1−λ(cid:98)iI
L2
π(X))−1∥− H1
→W
≤∥((µI−L)−1Z µ−Z µG(cid:98))(cid:98)h i∥ H→W/∥Z µ(cid:98)h i∥≤E(G(cid:98))η((cid:98)h i).
Then, from the first inequality, using that (µI−L)−1 is self-adjoint as operator Wµ(X)→Wµ(X),
π π
we obtain the first bound in (11).
So, observing that for every (µ−λ)−1∈Sp((µI−L)−1)\{(µ−λ )−1},
i
|(µ−λ(cid:98)i)−1−(µ−λ)−1|≥|(µ−λ i)−1−(µ−λ)−1|−|(µ−λ(cid:98)i)−1−(µ−λ i)−1|
we conclude that |(µ−λ(cid:98)i)−1−(µ−λ)−1|≥|(µ−λ i)−1−(µ−λ)−1|−E(G(cid:98))η((cid:98)h i), and
min{|(µ−λ(cid:98)i)−1−(µ−λ)−1||(µ−λ)−1 ∈Sp((µI−L)−1)\{(µ−λ i)−1}}≥gap i−E(G(cid:98))η((cid:98)h i).
So, applying Proposition 3, we obtain
sin(∢(f(cid:98)i,f i))≤ ∥(µI [− gaL p) i− −1f E(cid:98)i (− G(cid:98))(µ η(− (cid:98)hλ(cid:98) i)i ]) +−1f(cid:98)i∥ ≤ ∥((µI− [L ga)−
p
i1Z −µ E− (G(cid:98)Z )µ ηG(cid:98) ((cid:98)h)(cid:98)h i)i ]∥ +/∥Z µ(cid:98)h i∥
≤
E(G(cid:98))η((cid:98)h i)
.
[gap i−E(G(cid:98))η((cid:98)h i)]
+
Since, clearly ∥f(cid:98)i−f i∥2 ≤ 2(1−cos(∢(f(cid:98)i,f i)) ≤ 2sin(∢(f(cid:98)i,f i)), the proof of the second bound is
completed.
Next, we adapt the [22, Proposition 1] to our setting as follows.
Proposition 5. Let G(cid:98)∈B r(H). For all i∈[r] the metric distortion of (cid:98)h
i
w.r.t. energy space W πµ(X)
can be tightly bounded as
(cid:113)
1 / ∥W µ∥ ≤ η((cid:98)h i) ≤ ∥G(cid:98)∥/σ m+ in(Z µG(cid:98)). (34)
20D Empirical estimation
The Reduced Rank Regression (RRR) estimator is the exact minimizer of (17) under fixed rank
constraint. Specifically, RRR is the minimizer G(cid:98)r
µ,γ
of R(cid:98)γ(G) within the set of bounded operators
HS (H) on H that have rank at most r. The regularization term γ∥G∥2 is added to ensure stability.
r HS
The closed form solution of the empirical RRR estimator is
G(cid:98)r
µ,γ
=H(cid:98) γ−1/2[[H(cid:98) γ1/2C]] r, (35)
while its population counterpart is given by Gr =W−1/2[[W1/2C]]] .
µ,γ µ,γ µ,γ r
In order to prove Theorem 1, recall kernel matrices in (19) and define
(cid:20) µK √ µN(cid:21) (cid:20) µK √ µN (cid:21)
F = √ and F = √ γ . (36)
µ µN⊤ M µ,γ µN⊤ M+γµI
Now we provide the explicit forms of the matrices N and M in the case of Langevin (see Example
1)andCIR(seeExample2)processes,consideringanRBFkernelk(x,y)=k (y)=exp(−κ∥x−y∥2).
x
As a reminder (see (36)), N ∈ Rn×pn and M ∈ Rpn×pn are Gram matrices whose elements, for
k ∈[1+p],i,j ∈[n] are given by
N =n−1⟨ϕ(x ),d ϕ(x )⟩ and M =n−1⟨d ϕ(x ),d ϕ(x )⟩ ,
i,(k−1)n+j i k j H (k−1)n+i,(ℓ−1)n+j k i ℓ j H
where d ϕ(x ) = s (x )⊤Dϕ(x ) and Dϕ(x ) = Dk(x ,·) is defined by (31). For k ∈ [p],i,j ∈ [n],
k j k j j j j
we have
⟨ϕ(x ),d ϕ(x )⟩ =⟨k ,s (x )⊤Dk ⟩
i k j H xi k j xj H
(cid:68) (cid:16) k(·,x +he )−k(·,x )(cid:17) (cid:69)
= k ,s (x )⊤ lim j l j
xi k j h→0 h l∈[d]
(cid:68) (cid:16)k(·,x +he )−k(·,x )(cid:17) (cid:69)
= lim k ,s (x )⊤ j l j
h→0 xi k j h l∈[d]
(cid:16) k(x ,x +he )−k(x ,x )(cid:17)
=s (x )⊤ lim i j l i j
k j h→0 h l∈[d]
=s k(x j)⊤(cid:0) D(0,el)k(x i,x j)(cid:1)
l∈[d]
=2γs (x )⊤(cid:0) (x(l)−x(l))k(x ,x )(cid:1)
k j i j i j l∈[d]
where we have used the continuity of the inner product to get the third line and the reproducing
property to obtain the following one. Similarly, for k,ℓ∈[p],j ∈[n], we get
⟨d kϕ(x i),d ℓϕ(x j)⟩ H=(cid:68) s k(x i)⊤(cid:0) D(e l′,0)k(x i,·)(cid:1) l′∈[d],s ℓ(x j)⊤(cid:16) hli →m 0k(·,x j +he hl)−k(·,x j)(cid:17) l∈[d](cid:69)
= hli →m 0(cid:68) s k(x i)⊤(cid:0) D(e l′,0)k(x i,·)(cid:1) l′∈[d],s ℓ(x j)⊤(cid:16)k(·,x j +he hl)−k(·,x j)(cid:17) l∈[d](cid:69)
= hli →m 0(cid:68)(cid:0) D(e l′,0)k(x i,·)(cid:1) l′∈[d],s k(x j)s ℓ(x j)⊤(cid:16)k(·,x j +he hl)−k(·,x j)(cid:17) l∈[d](cid:69)
=s (x )s (x
)⊤(cid:16) limD(e l′,0)k(x i,x
j
+he l)−D(e l′,0)k(x i,x j)(cid:17)
k i ℓ j h→0 h l′,l∈[d]
=s (x )s (x )⊤(D k(x ,x )) ,
k i ℓ j l′,l i j l′,l∈[d]
21where we have used the partial derivative reproducing (32) property and where we define for l′ ̸=l,
D k(x ,x )=
limD(e l′,0)k(x i,x
j
+he l)−D(e l′,0)k(x i,x j)
l′,l i j h→0 h
(x(l′)−x(l′))k(x
,x +he
)−(x(l′)−x(l′))k(x
,x )
=−2γlim i j i j l i j i j
h→0 h
=−4γ2(x(l′)−x(l′))(x(l)−x(l))k(x ,x ), (37)
i j i j i j
and for l∈[d],
D k(x ,x )=
limD(el,0)k(x i,x
j
+he l)−D(el,0)k(x i,x j)
l,l i j h→0 h
(x(l)−x(l)−h)k(x ,x +he )−(x(l)−x(l))k(x ,x )
=−2γlim i j i j l i j i j
h→0 h
=(cid:2) 2γ −4γ2(x(l)−x(l))2(cid:3) k(x ,x )=2γ[1−2γ(x(l)−x(l))2]k(x ,x ). (38)
i j i j i j i j
For the overdamped Langevin (see Example 1), for k ≤d, s (x )=(k T)1/2e and s (x )s (x )⊤ =
k i b k k i ℓ j
(k T)I so that
b
N =n−1⟨ϕ(x ),d ϕ(x )⟩ =2γ(k T)1/2n−1(x(k)−x(k))k(x ,x )
i,(k−1)n+j i k j H b i j i j
and
M =n−1⟨d ϕ(x ),d ϕ(x )⟩ =(k T)n−1(D k(x ,x )) ,
(k−1)n+i,(ℓ−1)n+j k j ℓ j H b l′,l j j l′,l∈[d]
wheretheelementsofDaregivenby(37)and(38). FortheCIRprocess(seeExample2)indimension
√ √
d=1, we have s(x)=σ x/ 2. Then,
√ √
N =n−1⟨ϕ(x ),d ϕ(x )⟩ = 2σγn−1 x (x −x )exp(−γ|x −x |2)
i,(k−1)n+j i k j H j i j i j
and
√ √
M =n−1⟨d ϕ(x ),d ϕ(x )⟩ =σ2γn−1 x x exp(−γ|x −x |2).
(k−1)n+i,(ℓ−1)n+j k i ℓ j H i j i j
Based on the previous formulas, using Theorem 1, which we prove next, one can estimate
generator’s eigenpairs in practice.
Theorem 1. Given µ>0 and γ >0, let J =K−N(M+γµI)−1N⊤+γI. Let (σ2,v ) be the
µ,γ (cid:98)i i i∈[r]
leading eigenpairs of the following generalized eigenvalue problem
µ−1(J −γI)Kv =σ2J v , v⊤Kv =δ , i,j ∈[r]. (20)
µ,γ i (cid:98)i µ,γ i i j ij
Denoting V =[v |...|v ]∈Rn×r and Σ =diag(σ ,...,σ ), if (ν ,wℓ,wr) are eigentriplets of
r 1 r r (cid:98)1 (cid:98)r i i i i∈[r]
matrix V⊤
r
V rΣ2
r
∈Rr×r, then the eigenvalue decomposition the RRR estimator G(cid:98)r
µ,γ
=Z(cid:98)µU rV⊤
t
S(cid:98) is
given by G(cid:98)r
µ,γ
= (cid:80) i∈[r](µ−λ(cid:98)i)−1(cid:98)h i⊗g (cid:98)i, where λ(cid:98)i = µ−1/ν i, g (cid:98)i=ν i−1/2S(cid:98)∗V rw iℓ and (cid:98)h i=Z(cid:98) µ∗U rw ir
for U =(µγ)−1[µ−1/2I|−N(M+γµI)−1]⊤(KV −µV Σ2)∈R(1+p)n×r.
r r r r
Proof. First, note that µJ ≻ 0 is exactly Schurs’s complement w.r.t. second diagonal block of
µ,γ
F ≻0, and that, due to block inversion lemma [14], we have that
µ,γ
(cid:20) µ−1J µ−1/2J−1N(M+γµI)−1(cid:21)
F−1 = µ,γ µ,γ . (39)
µ,γ µ−1/(M+γµI)−1N⊤J−1 A
µ,γ
22where A is some np×np matrix. The first step in computing the RRR estimator lies in computing a
truncating SVD of W(cid:99)µ− ,γ1/2C(cid:98), that is C(cid:98)W(cid:99) µ− ,γ1C(cid:98)q
i
=σ (cid:98)i2q i, i∈[r]. Now, using the low-rank eigenvalue
problem formulation [14], we have that q
i
= S(cid:98)∗v
i
and S(cid:98)W(cid:99) µ− ,γ1C(cid:98)S(cid:98)∗v
i
= σ (cid:98)i2v i. Now, recalling that
S(cid:98)=[µ−1/2|0]Z(cid:98)µ we obtain and that Z(cid:98)µ(Z(cid:98) µ∗Z(cid:98)µ+µγI)−1=(Z(cid:98)µZ(cid:98) µ∗+µγI)−1Z(cid:98)µ, we obtain
[µ−1/2I |0]F−1F [µ−1/2I |0]⊤Kv =σ2v ,
µ,γ µ i (cid:98)i i
which after some algebra, using (39)
µ−1(I−γJ−1)Kv =σ2v ,
µ,γ i (cid:98)i i
i.e. µ−1(I−γJ−1)KV =V Σ2.
µ,γ r r r
By normalizing right singular value functions q
i
of W(cid:99)µ− ,γ1/2C(cid:98), that is by asking that ⟨q i,q i⟩
H
=
v i⊤Kv
i
=1, we obtain that [[W(cid:99)µ− ,γ1/2C(cid:98)]]
r
=W(cid:99)µ− ,γ1/2C(cid:98)Q rQ∗ r, for Q
r
=[q
1
|...|q r]. In other words, we
have
G(cid:98)µ,γ =W(cid:99) µ− ,γ1Z(cid:98) µ∗[µ−1/2I |0]⊤S(cid:98)S(cid:98)∗V rV⊤
r
S(cid:98)=Z(cid:98) µ∗F− µ,1 γ[µ−1/2I |0]⊤KV rV⊤
r
S(cid:98)=Z(cid:98) µ∗U rV⊤
r
S(cid:98),
where we used that J−1KV =γ−1[KV −µV Σ2]. Once with this form, we apply [21, Theorem 2]
µ,γ r r r r
to obtain the result.
Finally, next result provides the reasoning for using empirical metric distortion of Theorem 23.
Proposition 6. Under the assumptions of Theorem 1, for every i∈[r]
(cid:115)
η =
∥(cid:98)h i∥
=
(w ir)∗U r⊤F µU rw ir
, (40)
(cid:98)i ∥Z(cid:98)µ(cid:98)h i∥ ∥F µU rw ir∥2
and
(cid:12) (cid:12) (cid:16) (cid:17)
(cid:12) (cid:12)η (cid:98)i−η((cid:98)h i)(cid:12) (cid:12)≤ η((cid:98)h i) ∧η
(cid:98)i
η((cid:98)h i)η (cid:98)i∥W(cid:99)µ−W µ∥. (41)
Proof. First, note that (40) follows directly from Theorem 1. Next, since for every i∈[r],
(η (cid:98)i)−2−(η((cid:98)h i))−2 =
⟨(cid:98)h i,(W(cid:99)µ−W µ)(cid:98)h i)⟩
≤∥W(cid:99)µ−W µ∥,
∥(cid:98)h i∥2
we obtain
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)η (cid:98)i−2−(η((cid:98)h i))−2(cid:12) (cid:12) (cid:16) (cid:17)
(cid:12) (cid:12)η (cid:98)i−1−(η((cid:98)h i))−1(cid:12) (cid:12)≤
(η((cid:98)h i))−1 ∨ η (cid:98)i−1
≤ η((cid:98)h i) ∧η
(cid:98)i
∥W(cid:99)µ−W µ∥.
E Learning bounds
E.1 Main assumptions
Westartbyobservingthatdueto(BK),wehavethatoperatorZ ∈HS(H,Wµ(X)),whichaccording
µ π
to the spectral theorem for positive self-adjoint operators, has an SVD, i.e. there exists at most
countable positive sequence (σ ) , where J ={1,2,...,}⊆N, and ortho-normal systems (z )
j j∈J j j∈J
23and (h ) of cl(Im(Z )) and Ker(Z )⊥, respectively, such that Z h = σ z and Z∗z = σ h ,
j j∈J µ µ µ j j j µ j j j
j ∈J.
Now, given α≥0, let us define scaled injection operator Z : H→Wµ(X) as
µ,α π
(cid:88)
Z = σαz ⊗h . (42)
µ,α j j j
j∈J
Clearly, we have that Z =Z , while ImZ =cl(Im(Z )). Next, we equip Im(Z ) with a norm
µ µ,1 µ,0 µ µ,α
∥·∥ to build an interpolation space
H,α
 
[H] = f ∈Im(Z )|∥f∥2 =(cid:88) σ−2α⟨f,z ⟩2 <∞ ,
α µ,α H,α j j W
 
j∈J
noting that the inner product in Wµ(X) is given by bilinear energy functional
π
⟨f,g⟩ =µ⟨f,g⟩ −⟨f,Lg⟩ .
W L2 L2
π π
We remark that for α = 1 the space [H] is just an RKHS H seen as a subspace of Wµ(X).
α π
Moreover, we have the following injections
[H] (cid:44)→[H] (cid:44)→[H] (cid:44)→[H] =Wµ(X),
α1 1 α2 0 π
where α ≥1≥α ≥0.
1 2
In addition, from (BK) we also have that RKHS H can be embedded into
W πµ,∞(X)={f ∈W πµ(X)|∥f∥ Wπµ,∞ =esssup[|f(x)|2−f(x)[Lf](x)]<∞}
x∼π
that is, for some τ ∈(0,1]
[H] (cid:44)→[H] (cid:44)→Wµ,∞(X)(cid:44)→Wµ(X).
1 τ π π
Now, according to [12], if Z : [H] (cid:44)→L∞(X) denotes the injection operator, its boundedness
µ,τ,∞ τ π
implies the polynomial decay of the singular values of Z , i.e. σ2(Z ) ≲ j−1/τ, j ∈ J, and the
µ j µ
condition (KE) is assured.
Assumption (SD) allows one to quantify the effective dimension of H in ambient space Wµ(X),
π
while the kernel embedding property (KE) allows one to estimate the norms of whitened feature
maps, in our generator setting vector-valued since they define rank(1+p) operators on H,
ξ(x):=W−1/2wϕ(x)∈H1+p. (43)
µ,γ
This object plays a key role in deriving the learning rates for regression problems, [23] and the
following result is bounding it.
Lemma 1. Let (KE) hold for some τ ∈[β,1] and c ∈(0,∞). Then,
τ
(cid:40) cβ
β (µγ)−β ,β <1,
E ∥ξ(x)∥2 ≤ 1−β and ∥ξ∥2 =esssup∥ξ(x)∥2 ≤c (µγ)−τ. (44)
x∼π H1+p ∞ H1+p τ
c (µγ)−1 ,β =1. x∼π
τ
Proof. W.l.o.g. set µ=1, observing that the only change in the proof is in scaling γ >0. We first
observe that for every j ∈J from definition of wϕ and fact that h (x)=[Z h ](x) π-a.e., it holds
j µ j
that
(cid:88) ⟨w ϕ(x),h ⟩2=µ|h (x)|2−h (x)[LZ h ](x)=µ|[Z h ](x)|2−[Z h ](x)[LZ h ](x), π-a.e.,
i j j j µ j µ j µ j µ j
i∈[1+p]
24implying that (cid:80) ⟨w ϕ(x),h ⟩2 ≤σ µ|z (x)|2−z (x)[Lz ](x). So, for every τ >0
i∈[1+p] i j j j j j
∥ξ(x)∥2 =(cid:88) (cid:88) ⟨W−1/2w ϕ(x),h ⟩2 =(cid:88) (cid:88) 1 ⟨w ϕ(x),h ⟩2
H1+p µ,γ i j σ2+γ i j
j∈Ji∈[1+p] j∈Ji∈[1+p] j
=(cid:88) (cid:88) σ j2(1−τ) ⟨w iϕ(x),h j⟩2 σ2τ=γ−τ(cid:88) (cid:88) (σ j2γ−1)1−τ ⟨w iϕ(x),h j⟩2
σ2τ
σ2+γ σ2 j σ2γ−1+1 σ2 j
j∈Ji∈[1+p] j j j∈Ji∈[1+p] j j
≤γ−τ(cid:88)µ|h j(x)|2−h j(x)[LZ µh j](x) σ2τ=γ−τ(cid:88)
(µ|z (x)|2−z (x)[Lz ](x))σ2τ,
σ2 j j j j j
j∈J j j∈J
and, due to (21), we obtain ∥ξ∥2 ≤γ−τc . On the other hand, we also have that
∞ τ
tr(E [ξ(x)⊗ξ(x)])=tr(W−1/2W W−1/2)=tr(W−1W ),
x∼π µ,γ µ µ,γ µ,γ µ
which is an effective dimension of the RKHS H in Wµ(X). Therefore, following the proof of Fischer
π
andSteinwart[12, Lemma11]forclassicalcovariancesinL2, weshowthattheboundontheeffective
π
dimension is
tr(W−1W )=
(cid:88) σ j2 ≤(cid:40) 1c −β β βγ−β ,β <1,
(45)
µ,γ µ
j∈N
σ j2+γ c τγ−1 ,β =1.
For the case β =1, it suffices to see that
∥z j∥ W =E µ[z j]≤∥z j∥ Wπµ,∞ =esssup[|z j(x)|2−z j(x)[LZ µz j](x)],
x∼π
and, hence
(cid:88) (cid:88)
tr(W−1W )≤γ−1 σ2∥z ∥2 =γ−1 σ2E [z ]≤γ−1c .
µ,γ µ j j W j µ j τ
j∈N j∈N
For β <1 we can apply the same classical reasoning as in the proof of Proposition 3 of [8].
Proposition 7. If the eigenfunctions of L belong to [H] , then Condition (RC) is satisfied.
α
Proof. Note first that the resolvent (µI−L)−1 admits same eigenfunctions as the generator L,
meaning that Im((µI−L)−1Z ) ⊆ cl(Im(Z )). But according to [47, Theorem 2.2], this last
µ µ,α
condition is equivalent to Condition (RC).
Finally we prove here Proposition 8
Proposition 8. Given µ > 0, let H ⊆ Wµ(X) be the RKHS associated to kernel k ∈ C2(X×
π
X) such that Z ∈ HS(H,Wµ(X)), and let P be the orthogonal projector onto the closure of
µ π H
Im(Z ) ⊆ Wµ(X). Then for every ε > 0 there exists a finite rank operator G: H→H such that
µ π
R(G)≤∥(I−P )(µI−L)−1Z ∥2 +ε. Consequently, when k is universal, R(G)≤ε.
H µ HS(H,W)
Proof. Recall first that since Z ∈HS(H,Wµ(X)), according to the spectral theorem for positive
µ π
self-adjoint operators, Z admits an SVD. Its form is provided in (42) taking α=1.
µ
Now, recalling that [[·]] denotes the r-truncated SVD, i.e. [[Z ]] = (cid:80) σ z ⊗ h , since
r µ r j∈[r] j j j
∥Z −[[Z ]] ∥2 = (cid:80) σ2, for every δ > 0 there exists r ∈ N such that ∥Z −[[Z ]] ∥ < µδ/3.
µ µ r HS j>r j µ µ r HS
Consequently since all the eigenvalues of L are non-positive, ∥(µI−L)−1(Z −[[Z ]] )∥ ≤∥Z −
µ µ r HS µ
[[Z ]] ∥ /µ ≤ δ/3. Next since Im(P (µI−L)−1Z ) ⊆ cl(Im(Z )), for any j ∈ [r], there exists
µ r HS H µ µ
25g ∈ H s.t. ∥P (µI−L)−1z −Z g ∥ ≤ δ , and, denoting B := (cid:80) σ g ⊗h we conclude
j H j µ j 3r r j∈[r] j j j
∥P (µI−L)−1[[Z ]] −Z B ∥ ≤δ/3. Finally we recall that the set of non-defective matrices is
H µ r µ r HS
dense in the space of matrices [40], implying that the set of non-defective rank-r linear operators is
denseinthespaceofrank-rlinearoperatorsonaHilbertspace. Therefore,thereexistsanon-defective
G∈B (H) such that ∥G−B ∥ <δ/(3σ (Z )). So, we conclude
r r HS 1 µ
∥(µI−L)−1Z −Z G∥
µ µ HS
≤∥(I−P )(µI−L)−1Z ∥ +∥(µI−L)−1Z −[[(µI−L)−1Z ]] ∥
H µ HS µ µ r HS
+∥[[(µI−L)−1Z ]] −Z B ∥ +∥Z (G−B )∥
µ r µ r HS µ r HS
≤∥(I−P )(µI−L)−1Z ∥ +δ.
H µ HS
Example 5. These three conditions depend on the process X (through its generator L) as well as the
chosen RKHS. They are satisfied, for example, by choosing for k as a Gaussian kernel. Indeed, the
sub-linearity conditions on A and B required to ensure the existence and uniqueness of the solution
process of (1) ensure that A and B are sufficiently ’nice’ to fulfil, notably, condition ((BK)).
E.2 Bounding the Bias
Recalling the error decomposition and passing to the H and L2-norms, we have that
π
E(G(cid:98))≤∥(µI−L)−1Z µ−Z µG µ,γ∥ H→W+∥W µ1/2(G µ,γ−Gr µ,γ)∥+∥W µ1/2(Gr µ,γ−G(cid:98)r µ,γ)∥, (46)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
regularizationbias rankreductionbias estimator’svariance
and continue to prove the bound of the first term. Note that, while this proof technique is standard
for operator learning [23, 28], we present it here for the sake of completeness.
Proposition9. LetG =W−1C forγ >0,andP : Wµ(X)→Wµ(X)betheorthogonalprojector
µ,γ µ,γ H π π
onto cl(Im(Z )). If the assumptions (BK), (SD) and (RC) hold, then ∥G ∥ ≤ c c(α−1)/2 for
µ µ,γ α W
α∈[1,2], ∥G ∥≤c (µγ)(α−1)/2 for α∈(0,1], and
µ,γ α
∥(µI−L)−1Z µ−Z µG µ,γ∥ H→W ≤c α(µγ)α 2 +∥(I−P H)(µI−L)−1Z µ∥ H→W. (47)
Proof. Recalling that P := (cid:80) z ⊗ z , start by denoting the orthogonal projectors on the
H j∈J j j
subspaceofk leadingleftsingularfunctionsofZ asP :=(cid:80) z ⊗z , respectively. Next, observe
µ k j∈[k] j j
that C=Z∗(µI−L)−1Z and Z∗W−1=Z∗(Z∗Z +µγI)−1=(Z Z∗+µγI)−1Z . Therefore,
µ µ µ µ,γ µ µ µ µ µ µ
(µI−L)−1Z −Z G =(I−Z W−1Z∗)(µI−L)−1Z
µ µ µ,γ µ µ,γ µ µ
=(I−(Z Z∗+µγI)−1Z Z∗)(µI−L)−1Z
µ µ µ µ µ
=µγ(Z Z∗+µγI)−1(µI−L)−1Z
µ µ µ
26and, hence
 
(cid:88) µγ
(µI−L)−1Z µ−Z µG µ,γ= σ2+µγz
j
⊗z j(µI−L)−1Z
µ
j∈J j
 
(cid:88) µγ
=
(σ2+µγ)σ
z
j
⊗(Z µh j)(µI−L)−1Z
µ
j∈J j j
 
(cid:88) µγ
=
(σ2+µγ)σ
z
j
⊗h jC.
j∈J j j
Therefore, for every k ∈J, ∥P ((µI−L)−1Z −Z G )∥2 becomes
k µ µ µ,γ H→W
(cid:13)   (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)(cid:88) (σ2+µ µγ γ)σ z j ⊗h jC2 (cid:88) (σ2+γ µγ)σ h j ⊗z j(cid:13) (cid:13) (cid:13) ,
(cid:13) j∈[k] j j j∈[k] j j (cid:13)
H→W
which, due to (RC), implies that
(cid:13) (cid:13)
∥P k((µI−L)−1Z µ−Z µG µ,γ)∥ H→W≤c α (cid:13) (cid:13) (cid:13) (cid:13)(cid:88) σµ 2γ +σ µjα γz j ⊗z j(cid:13) (cid:13) (cid:13)
(cid:13)
.
(cid:13)j∈[k] j (cid:13)
W→W
On the other hand,
(cid:88) σµ 2γ +σ µjα γz j ⊗z j=γα 2 (cid:88) σ(σ 2(j2 µ(µ γγ )−)− 11 +)α 2 1z j ⊗z j ⪯(µγ)α 2 (cid:88) z j ⊗z j,
j∈[k] j j∈[k] j j∈[k]
where the inequality holds due to xs ≤ x+1 for all x ≥ 0 and s ∈ [0,1]. Since the norm of the
projector equals one, we get ∥P k((µI−L)−1Z µ−Z µG µ,γ)∥≤c α(µγ)α 2.
Next, observe that
(cid:13) (cid:13)
∥(P H−P k)((µI−L)−1Z µ−Z µG µ,γ)∥2 H→W=(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:88) (σ2µ +2γ µ2 γ)2(Z µ∗z j)⊗(Z µ∗z j)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)j∈J\[k] j (cid:13)
H→H
which is bounded by
(cid:88) µ2γ2
∥Z∗z ∥2 ≤
(cid:88) µ2γ2σ j2α
≤
(cid:88)
σ2α.
(σ2+µγ)2 µ j (σ2+µγ)2 j
j∈J\[k] j j∈J\[k] j j∈J\[k]
Using triangular inequality, for every k ∈ J we have that ∥P ((µI−L)−1Z −Z G )∥ is
H µ µ µ,γ H→W
bounded by
∥P ((µI−L)−1Z −Z G )∥ +∥(P −P )((µI−L)−1Z −Z G )∥
k µ µ µ,γ H→W H k µ µ µ,γ H→W
and, therefore,
∥P H((µI−L)−1Z µ−Z µG µ,γ)∥ H→W ≤c α(µγ)α 2 + (cid:88) (σ j2β)α β,
j∈J\[k]
27and, hence, letting k →∞ we obtain ∥P H(µI−L)−1Z µ−Z µG µ,γ∥≤c α(µγ)α 2. Hence, (47) follows
from triangular inequality.
To estimate the ∥G µ,γ∥, note that (RC) implies ∥G µ,γ∥≤ c α∥W µ− ,γ1W µ1+ 2α ∥ and considering two
cases. First, if (RC) holds for some α∈[1,2], then, clearly ∥G ∥≤c c(α−1)/2. On the other hand,
µ,γ α W
if α∈(0,1], then
σ1+α (cid:0) σ2(µγ)−1(cid:1)1+ 2α
j =(µγ)−1 j ≤(µγ)α− 21 ,
σ2+µγ σ2(µγ)−1+1
j j
and, thus, ∥G ∥≤c γ(α−1)/2.
µ,γ α
Remark 2. Inequality (47) says that the regularization bias is comprised of a term depending on the
choice of γ, and on a term depending on the “alignment” between H and Im((µI−L)−1Z ). The
µ
term ∥(I−P )(µI−L)−1Z ∥ can be set to zero by two different approaches. One is choose a kernel
H µ
which in some way minimizes ∥(I−P )(µI−L)−1Z ∥. Another is to choose a universal kernel [38,
H µ
Chapter 4], for which Im((µI−L)−1S )⊆cl(Im(S )). While we here develop theory for universal
π π
kernels, deep learning approaches that leverage on on our approach can be developed, which is the
direction to pursue in future.
In order to proceed with bounding the bias due to rank reduction for both considered estimators,
we first provide auxiliary result.
Proposition 10. Let B :=W−1/2C, let (RC) hold for some α∈(0,2], and for j ∈N denote
µ,γ
λ⋆ =σ2((µI−L)−1Z )=λ (S∗(µI−L)−1S ) (48)
j j µ j π π
Then for every j ∈N
λ⋆−c2 cα/2(µγ)α/2 ≤σ2(B)≤λ⋆. (49)
j α W j j
Proof. Start by observing that
B∗B =[(µI−L)−1Z ]∗Z W−1Z∗(µI−L)−1Z
µ µ µ,γ µ µ
=[(µI−L)−1Z ]∗(µI−L)−1Z −µγ[(µI−L)−1Z ]∗(Z Z∗+µγI)−1(µI−L)−1Z ,
µ µ µ µ µ µ
implies that, using [(µI−L)−1Z ]∗=S and [(µI−L)−1Z ]∗(µI−L)−1Z =S∗(µI−L)−1S ,
µ π µ µ π π
(cid:88) µγ
S∗(µI−L)−1S − (S∗z )⊗(S∗z )=B∗B ⪯Z∗Z .
π π σ2+µγ π j π j µ µ
j∈J j
Next, similarly to the above, for every k ∈J, we have
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)(cid:88) σ2µ +γ µγ(S π∗z j)⊗(S π∗z j)(cid:13) (cid:13) (cid:13) (cid:13)≤c2 α(cid:13) (cid:13) (cid:13) (cid:13)(cid:88) σ2(µγσ )j2 −α 1+1z j⊗z j(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)j∈[k] j (cid:13) (cid:13)j∈[k] j (cid:13)
(cid:13) (cid:13)
=c2
α(cid:13)
(cid:13) (cid:13)
(cid:13)(cid:88)(σ j2(µγ σ) 2− (µ1) γα )/ −2 1σ +jα( 1µγ)α/2
z j⊗z
j(cid:13)
(cid:13) (cid:13) (cid:13)≤c2 αγα/2∥W µ∥α/2,
(cid:13)j∈[k] j (cid:13)
28and
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:88) σ2+γ µγ(S π∗z j)⊗(S π∗z j)(cid:13) (cid:13) (cid:13)≤c2
α
(cid:88) σ2+γ µγσ j2α ≤c2
α
(cid:88) (σ j2β)α/β.
(cid:13)j∈J\[k] j (cid:13) j∈J\[k] j j∈J\[k]
So, as before, letting k →∞ we get the result.
As a consequence, we obtain the bound for the rank reduction bias of the RRR method.
Proposition 11 (RRR). Let (RC) hold for some α ∈ (0,2]. Then the bias of Gr due to rank
µ,γ
reduction, recalling (48) is bounded as
(cid:113) (cid:113)
λ⋆ −c cα/4(µγ)α/4−2c (µγ)(1∧α)/2 ≤∥Z (G −Gr )∥≤ λ⋆ . (50)
r+1 α W α µ µ,γ µ,γ r+1
Proof. Observe that
∥Z (G −Gr )∥≤∥W1/2(G −Gr )∥=∥B−[[B]] ∥=σ (B)≤σ ((µI−L)−1Z )
µ µ,γ µ,γ µ,γ µ,γ µ,γ r r+1 r+1 µ
while
∥Z (G −Gr )∥≥∥W1/2(G −Gr )∥−(µγ)1/2∥G −Gr ∥
µ µ,γ µ,γ µ,γ µ,γ µ,γ µ,γ µ,γ
≥σ ((µI−L)−1Z )−c ∥W ∥α/4(µγ)α/4−2c (µγ)(1∧α)/2.
r+1 µ α µ α
E.3 Bounding the Variance
E.3.1 Concentration Inequalities
All the statistical bounds we present will relay on two versions of Bernstein inequality. The first
one is Pinelis and Sakhanenko inequality for random variables in a separable Hilbert space, see [8,
Proposition 2].
Proposition12. LetA ,i∈[n]bei.i.dcopiesofarandomvariableAinaseparableHilbertspacewith
i
norm ∥·∥. If there exist constants Λ>0 and σ >0 such that for every m≥2 E∥A∥m ≤ 1m!Λm−2σ2,
2
then with probability at least 1−δ
(cid:13) (cid:13) (cid:13) (cid:13) √ (cid:114)
(cid:13) (cid:13) (cid:13)n1 (cid:88) A i−EA(cid:13) (cid:13) (cid:13)≤ 4 √ n2 log2
δ
σ2+ Λ n2 . (51)
(cid:13) i∈[n] (cid:13)
On the other hand, we recall that in [32], a dimension-free version of the non-commutative Bernstein
inequalityforfinite-dimensionalsymmetricmatricesisproposed(seealsoTheorem7.3.1in[41]foran
easier to read and slightly improved version) as well as an extension to self-adjoint Hilbert-Schmidt
operators on a separable Hilbert spaces.
Proposition 13. Let A , i ∈ [n] be i.i.d copies of a Hilbert-Schmidt operator A on the separable
i
Hilbert space. Let ∥A∥≤c almost surely, EA=0 and let E[A2]⪯V for some trace class operator V.
Then with probability at least 1−δ
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:114)
(cid:13) (cid:13) (cid:13)n1 (cid:88) A i(cid:13) (cid:13) (cid:13)≤ 32 nc L A(δ)+ 2∥ nV∥ L A(δ), (52)
(cid:13) i∈[n] (cid:13)
29where
4 tr(V)
L (δ)=log +log .
A δ ∥V∥
Proposition 14. Given δ >0, with probability in the i.i.d. draw of (x )n from π, it holds that
i i=1
P{∥W(cid:99)µ−W µ∥≤ε n(δ)}≥1−δ,
where
(cid:114)
2c 2∥W ∥ 4tr(W )
ε (δ)= WL(δ)+ µ L(δ) and L(δ)=log µ . (53)
n 3n n δ∥W ∥
µ
Proof. Proof follows directly from Proposition 13 applied to rank-(1+p) operators wϕ(x )⊗wϕ(x )
i i
using the fact that W =E[wϕ(x )⊗wϕ(x )].
µ i i
Proposition 15. Let (KE) hold for τ ∈[β,1]. Given δ >0, with probability in the i.i.d. draw of
(x )n from π, it holds that
i i=1
(cid:110) (cid:111)
P ∥W µ− ,γ1/2(W(cid:99)µ−W µ)W µ− ,γ1/2∥≤ε1 n(γ,δ) ≥1−δ, (54)
where
(cid:115)
2c µ−τ 2c µ−τ
ε1(γ,δ)= τ L1(γ,δ)+ τ L1(γ,δ), (55)
n 3nγτ nγτ
and
L1(γ,δ)=ln4 +lntr(W µ− ,γ1W µ)
.
δ ∥W−1W ∥
µ,γ µ
Moreover,
(cid:26) (cid:27)
1
P ∥W1/2W(cid:99)−1W1/2∥≤ ≥1−δ. (56)
µ,γ µ,γ µ,γ 1−ε1(γ,δ)
n
Proof. The idea is to apply Proposition 13 for operator ξ(x)⊗ξ(x), where ξ(x) is defined in (43).
Due to (1), we have that ∥A∥≤∥ξ∥2 ≤(µγ)−τc . On the other hand, we have that
∞ τ
E [ξ(x)⊗ξ(x)]2 ⪯∥ξ∥2 E [ξ(x)⊗ξ(x)]=∥ξ∥2 W−1/2W W−1/2,
x∼π ∞ x∼π ∞ µ,γ µ µ,γ
and, hence (54) follows. To complete the proof, observe that
∥I H−W µ− ,γ1/2W(cid:99)µ,γW µ− ,γ1/2∥=∥W µ− ,γ1/2(W µ−W(cid:99)µ)W µ− ,γ1/2∥≤ε1 n(γ,δ),
and, hence for ε1(γ,δ) smaller than one we obtain
n
1
∥W µ1 ,/ γ2W(cid:99) µ− ,γ1W µ1 ,/ γ2∥=∥(W µ− ,γ1/2W(cid:99)µ,γW µ− ,γ1/2)−1∥≤
1−∥I H−W µ− ,γ1/2W(cid:99)µ,γW µ−
,γ1/2∥.
Proposition 16. Let (RC), (SD) and (KE) hold for some α ∈ (0,2], β ∈ (0,1] and τ ∈ [β,1].
Given δ >0, with probability in the i.i.d. draw of (x )n from π, it holds
i i=1
(cid:110) (cid:111)
P ∥W µ− ,γ1/2(W(cid:99)µ−W µ)W µ− ,γ1C∥
HS
≤ε2 n(γ,δ) ≥1−δ,
where
(cid:115) (cid:40)
√ 2 c µ−β c µ−τ (µγ)−(τ−α)/2 ,α≤τ,
ε2(γ,δ)=4 2c c ln β + τ (57)
n α τ δ nγβ n2γτ c(α−τ)/2 ,α≥τ.
W
30Proof. Asbefore,w.l.o.g. setµ=1. First,recallthatHS(H)equippedwith∥·∥ isseparableHilbert
HS
space. Hence, we will apply Proposition 12 for A=ξ(x)⊗ψ(x), where ψ(x)=CW−1wϕ(x)∈H1+p.
µ,γ
To that end, observe that E∥A∥m =E[∥ξ(x)∥m ∥ψ(x)∥m ], and that
HS H1+p H1+p
1 (cid:16) √ (cid:17)m−2(cid:18)(cid:113) (cid:19)2
E[∥ξ(x)∥m ]≤ m! γ−τ/2 c tr(W−1W ) .
H1+p 2 τ µ,γ µ
Recalling Lemma 1, the task is to bound ∥ψ(x)∥2 =(cid:80) ∥CW−1w ϕ(x)∥2 . Using (RC), we
H1+p i∈[1+p] µ,γ i H
have that
∥ψ(x)∥2 ≤c (cid:88) ∥W(1+α)/2W−1w ϕ(x)∥2 =c (cid:88) (cid:88) ⟨W(1+α)/2W−1w ϕ(x),h ⟩2 .
H1+p α µ µ,γ i H α µ µ,γ i j H
i∈[1+p] j∈Ji∈[1+p]
But, since
σ1+α
⟨W(1+α)/2W−1w ϕ(x),h ⟩ = j ⟨w ϕ(x),h ⟩ ,
µ µ,γ i j H σ2+γ i j H
j
expanding as in proof of Lemma 1 and using (KE), we have that
∥ψ(x)∥2 ≤c
(cid:88) (cid:88)
(cid:34)
σ
j(2+α−τ)(cid:35)2
⟨w iϕ(x),h j⟩2
σ2τ
≤(cid:40)
γ−(τ−α) ,α≤τ,
H1+p α σ2+γ σ2 j c(α−τ) ,α≥τ.
j∈Ji∈[1+p] j j W
Therefore, we can set
(cid:40) (cid:40)
γ−(2τ−α) ,α≤τ, γ−(β+τ−α) ,α≤τ,
Λ2 =c c2 and σ2 =c c c
α τ c(α−τ)γ−τ ,α≥τ. α τ β c(α−τ)γ−β ,α≥τ.
W W
in Proposition 12 to obtain the bound
Proposition 17. Let (KE) hold for τ ∈[β,1]. Given δ >0, with probability in the i.i.d. draw of
(x )n from π, it holds
i i=1
(cid:110) (cid:111)
P ∥W µ− ,γ1/2(C(cid:98)−C)∥
HS
≤ε3 n(γ,δ) ≥1−δ,
where
√ (cid:115)
4 2c 2 c µ−β c µ−τ
ε3(γ,δ)= W ln β + τ . (58)
n µ δ nγβ n2γτ
Proof. First, note that since ∥ϕ(x)∥2 ≤ µ−1∥wϕ(x)∥2 and C ⪯ µ−1W , (KE) and (SD) for
H H1+p µ
W imply analogous assumptions for C. Hence, we can apply Proposition 13 from [23], which
µ
using the observation that ∥W−1/2C1/2∥2=∥C1/2(µC −T)−1C1/2∥≤µ−1∥C C−1∥=µ−1, completes
µ,γ γ γ γ γ γ γ
the proof.
Next, we develop concentration bounds of some key quantities used to build RRR empirical
estimator.
31E.3.2 Variance and Norm of KRR Estimator
Proposition 18. Let (RC), (SD) and (KE) hold for some α ∈ (0,2], β ∈ (0,1] and τ ∈ [β,1].
Given δ >0 if ε1(γ,δ)<1, then with probability at least 1−δ in the i.i.d. draw of (x ,y )n from ρ
n i i i=1
(cid:26) ε2(γ,δ/3)+ε3(γ,δ/3)(cid:27)
P ∥W µ1 ,/ γ2(G(cid:98)µ,γ −G µ,γ)∥≤ n 1−ε1(γ,n
δ/3)
≥1−δ.
n
Proof. Note that W µ1 ,/ γ2(G(cid:98)µ,γ −G µ,γ)=W µ1 ,/ γ2(W(cid:99) µ− ,γ1C(cid:98)−W µ− ,γ1C), and, hence,
W µ1 ,/ γ2(G(cid:98)µ,γ −G µ,γ)=W µ1 ,/ γ2W(cid:99) µ− ,γ1(C(cid:98)−W(cid:99)µ,γW µ− ,γ1C±C)
(cid:16) (cid:17)
=W µ1 ,/ γ2W(cid:99) µ− ,γ1W µ1 ,/ γ2 W µ− ,γ1/2(C(cid:98)−C)−W µ− ,γ1/2(W(cid:99)µ−W µ)W µ− ,γ1C . (59)
Thus, taking the norm and using ∥C−1T∥ ≤ c σα−1(S ) with the Propositions 16 and 15 we
γ α 1 π
prove the first bound.
E.3.3 Variance of Singular Values
In this section we prove concentration of singular values computed in Theorem 1, a necessary step to
derive learining rates for RRR estimator.
Proposition 19. Let (RC), (SD) and (KE) hold for some α∈(0,2], β ∈(0,1] and τ ∈[β,1]. Let
B =W µ− ,γ1/2C and B(cid:98) =W(cid:99)µ− ,γ1/2C(cid:98). Given δ >0 if ε1 n(γ,δ/5)<1, then with probability at least 1−δ
in the i.i.d. draw of (x )n from π
i i=1
|σ2(B(cid:98))−σ2(B)|≤∥B(cid:98)∗B(cid:98)−B∗B∥≤ε4(γ,δ/3), (60)
i i n
where
(cid:16)1 ε2(γ,δ/3)+ε3(γ,δ/3)(cid:17)
ε4(γ,δ/3)=(ε2(γ,δ/3)+ε3(γ,δ/3)) + n n . (61)
n n n µ 1−ε1(γ,δ/3)
n
Proof. We start from the Weyl’s inequalities for the square of singular values
|σ2(B(cid:98))−σ2(B)|≤∥B(cid:98)∗B(cid:98)−B∗B∥, i∈[n].
i i
But, since,
B(cid:98)∗B(cid:98)−B∗B =C(cid:98)∗W(cid:99)−1C(cid:98)−C∗W−1C =(C(cid:98)−C)∗W(cid:99)−1C(cid:98)+C∗W−1(C(cid:98)−C)+C∗(W(cid:99)−1−W−1)C(cid:98)
µ,γ µ,γ µ,γ µ,γ µ,γ µ,γ
denoting M =W µ− ,γ1/2(C(cid:98)−C), N =W µ− ,γ1/2(W(cid:99)µ−W µ) and R=W µ1 ,/ γ2(G(cid:98)µ,γ −G µ,γ), we have
B(cid:98)∗B(cid:98)−B∗B =B∗M +M∗C γ1/2G(cid:98)µ,γ −B∗NG(cid:98)µ,γ =B∗M +(M∗C γ1/2−B∗N)(G(cid:98)µ,γ ±G µ,γ)
=B∗M +M∗B−B∗NG +(M∗−B∗NW−1/2)R
µ,γ µ,γ
=(G µ,γ)∗(C(cid:98)−C)+(C(cid:98)−C)G µ,γ−(G µ,γ)∗(W(cid:99)µ−W µ)G µ,γ+(M∗+(G µ,γ)∗N∗)R.
Therefore, recalling that, due to (59), R=W µ1 ,/ γ2W(cid:99) µ− ,γ1W µ1 ,/ γ2(M −NG µ,γ), we conclude
B(cid:98)∗B(cid:98)−B∗B =(G µ,γ)∗(C(cid:98)−C)+(C(cid:98)−C)G µ,γ−(G µ,γ)∗(W(cid:99)µ−W µ)G
µ,γ
+(M −NG µ,γ)∗W µ1 ,/ γ2W(cid:99) µ− ,γ1W µ1 ,/ γ2(M −NG µ,γ). (62)
Next, observe that
32• ∥(C(cid:98)−C)G µ,γ∥≤∥(C(cid:98)−C)W µ1 ,/ γ2∥∥W µ1 ,/ γ2C∥≤µ−1∥W µ1 ,/ γ2(C(cid:98)−C)∥ is bounded by Proposition 17
• ∥M−NG µ,γ∥≤∥W µ− ,γ1/2(C(cid:98)−C)∥+∥W µ− ,γ1/2(W(cid:99)µ−W µ)W µ− ,γ1C∥ is bounded by Propositions 16
and 17,
• ∥G∗ µ,γ(W(cid:99)µ−W µ)G µ,γ∥≤µ−1∥W µ− ,γ1/2(W(cid:99)µ−W µ)W µ− ,γ1C∥ is bounded by Proposition 16.
Therefore, using additionally Proposition 15 result follows.
Remark that to bound singular values we can rely on the fact
|σ2(B(cid:98))−σ2(B)| |σ2(B(cid:98))−σ2(B)|
|σ i(B(cid:98))−σ i(B)|= i i ≤ i i .
σ i(B(cid:98))+σ i(B) σ i(B(cid:98))∨σ i(B)
E.3.4 Variance of RRR Estimator
RecallingthenotationB :=W µ− ,γ1/2C andB(cid:98) :=W(cid:99)µ− ,γ1/2C(cid:98), letdenoteP
r
andP(cid:98)r denotetheorthogonal
projector onto the subspace of leading r right singular vectors of B and B(cid:98), respectively. Then we
have [[B]]
r
=BP
r
and [[B(cid:98)]]
r
=B(cid:98)P(cid:98)r, and, hence Gr
µ,γ
=G µ,γP
r
and G(cid:98)r
µ,γ
=G(cid:98)µ,γP(cid:98)r.
Proposition 20. Let (RC), (SD) and (KE) hold for some α ∈ (0,2], β ∈ (0,1] and τ ∈ [β,1].
Given δ >0 and γ >0, if ε1(γ,δ)<1, then with probability at least 1−δ in the i.i.d. draw of (x )n
n i i=1
from π,
ε2(γ,δ/3)+ε3(γ,δ/3) σ (B)
∥Z µ(Gr
µ,γ
−G(cid:98)r µ,γ)∥≤ n 1−ε1(γ,n
δ/3)
+ σ2(B)−1
σ2
(B)ε4 n(γ,δ/3). (63)
n r r+1
Proof. Start by observing that ∥Z µ(Gr
µ,γ
−G(cid:98)r µ,γ)∥≤∥W µ1 ,/ γ2(Gr
µ,γ
−G(cid:98)r µ,γ)∥ and
W1/2(Gr −G(cid:98)r )=(W1/2W(cid:99)−1W1/2)·
µ,γ µ,γ µ,γ µ,γ µ,γ µ,γ
(cid:16) (cid:17)
W µ− ,γ1/2(W(cid:99)µ−W µ)G µ,γP r+W µ− ,γ1/2(C(cid:98)−C)P(cid:98)r+B(P(cid:98)r−P r) .
Using that the norm of orthogonal projector P(cid:98) is bounded by one and applying Propositions 15 and
16 together with Propositions 4 and 19 completes the proof.
E.4 Proof of Theorem 2
E.4.1 Operator Norm Error Bounds
Summarising previous sections, in order to prove Theorem 2, we just need to analyse the bounds ε1,
n
ε2 and ε3. Not that we fix the hyperparameter µ>0, which affects the constants, but need to chose
n n
the decay rate of Tikhonov regularization parameter γ >0 to obtain balancing of bias and variance
in the generalization bounds.
Let us first assume regime α ≥ τ, which covers "well-specified learning" when non-trivial
eigenfunctions of L are inside H. Since α≥τ and β ≤τ, we have that for large enough n one has
n−1/2 (cid:18) n−1/2 n−1(cid:19)
ε1(γ,δ)≲ lnδ−1 and εi(γ,δ)≲ ∨ lnδ−1, i=2,3,4. (64)
n γτ/2 n γβ/2 γτ/2
33But, since the bias term is ≲ γα/2 and the slow term from Proposition 20 is 1/(cid:112) nγβ, we can set
γ n =n− α+1 β and obtain
γα/2 =
n−1/2
=n− 2(αα +β), and
n−1/2
=n−α 2(+ αβ +− βτ ),
n γβ/2 γτ/2
n n
which, due to α≥τ, implies
lim ε1(γ ,δ/3)= lim ε2(γ ,δ/3)= lim ε3(γ,δ/3)= lim ε4(γ,δ/3)=0.
n n n n n n
n→∞ n→∞ n→∞ n→∞
Therefore, for this choice of regularization parameter Equation (46) with Propositions 9, 11 and
20 assure that
(cid:113)
E(G(cid:98)r µ,γ)−σ r+1(B)≤E(G(cid:98)r µ,γ)− λ⋆
r+1
≲n− 2(αα +β). (65)
Now, let us consider the more difficult to learn case α<τ when eignefunctions of the generator
have only weaker norms than the RKHS one. Then, for large enough n bounds in (64) hold for i=3,
but
(cid:18) n−1/2 n−1 (cid:19)
ε2(γ,δ)∨ε4(γ,δ)≲ ∨ lnδ−1. (66)
n n γ(β+τ−α)/2 γ(2τ−α)/2
Then, by balancing the slow terms with bias γα/2, we set γ n =n− τ+1 β and obtain
γα/2 =
n−1/2
=n− 2(τα +β), and
n−1/2
=n− 2(τβ +β),
n γ(τ+β−α)/2 γτ/2
n n
which, since τ ≥β, implies
lim ε1(γ ,δ/3)= lim ε2(γ ,δ/3)= lim ε3(γ,δ/3)= lim ε4(γ,δ/3)=0,
n n n n n n
n→∞ n→∞ n→∞ n→∞
and we obtain
E(G(cid:98)r µ,γ)−σ r+1(B)≤E(G(cid:98)r µ,γ)−λ⋆
r+1
≲n− 2(τα +β). (67)
Therefore, denoting
γ ≍(cid:40) n− τ+1 β ,α≤τ, and ε⋆ =(cid:40) n− 2(τα +β) ,α≤τ, (68)
n n− α+1 β ,α≥τ, n n− 2(αα +β) ,α≥τ.
as a consequence the operator norm error bound in Theorem 2 holds, and we have the following
result on the estimation of singular values of (µI−L)−1Z µ, that is λ⋆ is by singular values of B(cid:98), that
is σ s from Theorem 1.
(cid:98)i
Proposition 21. Let (RC), (SD) and (KE) hold for some α∈(0,2] and β ∈(0,1] and τ ∈[β,1],
and define (68). Then, there exists a constant c>0 such that for every given δ ∈(0,1), large enough
n>r and with probability at least 1−δ in the i.i.d. draw of (x )n from π for all i∈[r]
i i=1
|σ2−λ⋆|≲ε⋆ lnδ−1 (69)
(cid:98)i i n
Proof. The proof is direct consequence of Propositions 10 and 19 using (64)-(66).
34E.5 Spectral Learning Rates
Finally, we conclude the proof of Theorem 2 showing the concentration of eigenpairs. Recalling
Proposition 2, we need to combine the operator norm bound and metric distortion. Since, as
indicated in Proposition 9, the population KRR estimator can grow in the operator norm whenever
the regularity condition is violated α<1, leading to possibly unbounded metric distortions w.r.t.
increasing sample size, we restrict to the case α≥1.
First, combining (65)-(67) and Proposition 21, we have that
E(G(cid:98)r µ,γ)≤(σ (cid:98)r+1∧λ⋆ r+1)+cε⋆ nlnδ−1.
On the other hand, from Propositions 40 and 14, we have that with failure probability δ
(η (cid:98)i)2−(η((cid:98)h i))−2 ≤ε n(δ),
which, if we can prove that η
(cid:98)i
and η((cid:98)h i) are bounded, concludes the proof for empirical spectral
biases. To that end, recall that, c.f. Proposition 5, for RRR estimator we have
η((cid:98)h i)≤
∥G(cid:98)r µ,γ∥
≤
∥G(cid:98)µ,γ∥
≤
∥G
µ,
(cid:112)γ∥+∥G(cid:98)µ,γ −G µ,γ∥
σ r(Z µG(cid:98)r µ,γ) σ r((µI−L)−1Z µ)−E(G(cid:98)r µ,γ) λ⋆
r
−E(G(cid:98)r µ,γ)
≤
∥G µ,γ∥+(µγ)−1/2∥W µ1 ,/ γ2(G(cid:98)µ,γ −G µ,γ)∥
≲
1+n−1/2γ−(β+1)/2
=
1+n− αα +− β1
(cid:112) λ⋆ r −E(G(cid:98)r µ,γ) (cid:112) λ⋆ r −n−1/2γ−β/2 (cid:112) λ⋆ r −n− α+α β
where in the last inequality we have applied Propositions 9 and 18. Thus, using Proposition 40, the
proof of Theorem 2 is concluded.
E.6 Discussion of the learning rates
In this section, we discuss the learning rates reported in Table ??. Notice that although the papers
we compare to employ a different risk, our comparison remains meaningful because our energy-based
risk measure provides an upper bound on their risk measure. This ensures that any upper bound
derived with our risk also applies to theirs.
• The learning bound for IG obtained is [1] covers only pure diffusion processes (Laplacian
with constant weights). Their learning rate is non-parametric and depends on the state space
dimension d in a counter-intuitive way O(n− 2(dd +1)) in [1, Theorem 3], highlighting a potential
limitation of their approach. In comparison, when we specify our RRR method with an RBF
kernel (i.e. β =0), we achieve a much faster parametric learning rate O(n−1/2).
• The recent work of [36] covers Langevin processes via a kernel approach, but they derive a
sub-optimal learning bound for IG of order O(n−1/4) in [36, Theorem 4.4]. For Langevin
diffusions, our RRR method with an RBF kernel achieves a faster parametric rate O(n−1/2).
Moreover,thecomputationalcomplexityoftheirmethodisO(n3d3),whichlimitsitsapplication
in realistic molecular dynamics scenarios.
• As for [15, Theorem 7], although they considered general diffusions, they only derived a
suboptimal bound for the variance component of their risk, with an explicit dependence on the
dimension of the state space.
Finally, we note that the mentioned works lack learning guarantees for eigenfunctions and
eigenvalues. Notably, their methods are prone to the spurious eigenvalue phenomenon, requiring
expert manual review of each eigenpair to select plausible ones.
35Figure 2: Results of the RRR given by our method for two different length scales (blue and red)
compared with ground truth (black) for the Langevin dynamics driven by a four well one dimensional
potential.
F Experiments
Four well potential For this experiment, we used an in-house code to simulate the system. The
equations of motions were discretized using the Euler-Maruyama scheme with a timestep of 10−4.
RRR was fitted using 1000 points, µ=5 and γ =10−5. The length scales used were 0.05 and 0.5.
This experiment was reproduces 100 times leading to very small change in the estimation of the
eigenfunctions. Here we report the result of one of them.
Muller Brown For this experiment, we used an in-house code to simulate the system. The
equations of motions were discretized using the Euler-Maruyama scheme with a timestep of 10−3
and a temperature of 2 (arbitrary units). RRR was fitted using 2000 points, µ=1 and γ =10−5.
The length scale used was 0.6.
36Figure 3: Results of the RRR given by our method for two different length scales (blue and red)
compared with ground truth for the Langevin dynamics driven by a four well one dimensional
potential.
37notation meaning notation meaning
∧ minimum ∨ maximum
[[·]] r-truncated SVD of an operator I identity operator
r
HS(H,G) space of Hilbert-Schmidt operators H→G B (H) set of rank-r Hilbert-Schmidt operators on H
r
∥A∥ operator norm of an operator A ∥A∥ Hilbert-Schmidt norm of operator A
HS
σ (·) i-th singular value of an operator λ (·) i-th eigenvalue of an operator
i i
X state space of the Markov process (X ) time-homogeneous Markov process
t t≥0
p transition kernel of the Markov process π invariant measure of the Markov process
a drift of the Itô process b diffusion of the Itô process
L2(X) L2 space of functions on X w.r.t. measure π A Transfer operator on L2(X)
π π
W1,2(X) Sobolev space w.r.t. measure π on X L Generator of the semigroup on W1,2(X)
π π
s Dirichlet form diffusion B Dirichlet form operator on W1,2(X)
π
µ shift parameter Wµ(X) regularized energy space
π
k(x,y) kernel ϕ canonical feature map
H reproducing kernel Hilbert space S canonical injection H(cid:44)→L2(X)
π π
ℓϕ generator embedding Z canonical injection H(cid:44)→Wµ(X)
µ π
Dv v-directional derivative D derivative
Dvϕ embedding of the v-directional derivative Dϕ derivative embedding
w ϕ k-th component of Dirichlet operator embedding wϕ Dirichlet operator embedding
k
σ j-th singular value of Z J countable index set of singular values of Z
j µ µ
z j-th left singular function of Z h j-th right singular function of Z
j µ j µ
1 function in L2(X) with the constant output 1 γ regularization parameter
π
R true risk E operator norm error
E excess risk, i.e. HS norm error R irreducible risk
HS 0
D
n
dataset (x i)
i∈[n]
R(cid:98) empirical risk
S(cid:98) sampling operator w.r.t. L2 π(X) Z(cid:98)µ sampling operator w.r.t. W πµ(X)
C covariance operator w.r.t. L2(X) C(cid:98) empirical covariance operator w.r.t. L2(X)
π π
C
γ
regularized covariance operator w.r.t. L2 π(X) C(cid:98)γ regularized empirical covariance operator w.r.t. L2 π(X)
W
µ
covariance operator w.r.t. W πµ(X) W(cid:99)µ empirical covariance operator w.r.t. W πµ(X)
W
µ,γ
regularized covariance operator w.r.t. W πµ(X) W(cid:99)µ,γ regularized empirical covariance operator w.r.t. W πµ(X)
T derivative-covariance operator T(cid:98) empirical derivative-covariance operator
K kernel Gram matrix w.r.t. L2(X) K regularized kernel Gram matrix w.r.t. L2(X)
π γ π
F kernel Gram matrix w.r.t. Wµ(X) F regularized kernel Gram matrix w.r.t. Wµ(X)
µ π µ,γ π
M derivative-derivative kernel Gram matrix N feature-derivative kernel Gram matrix
G population estimator of (µI−L)−1 on H G(cid:98) empirical estimator of (µI−L)−1 on H
G
µ,γ
population KRR estimator G(cid:98)µ,γ empirical KRR estimator
Gr population RRR estimator G(cid:98)r empirical RRR estimator
µ,γ µ,γ
P spectral projector P(cid:98) empirical spectral projector
η metric distortion η empirical metric distortion
(cid:98)
λ generator eigenvalue λ(cid:98) eigenvalue of the empirical estimator
f generator eigenfunction in L2(X) f(cid:98) empirical eigenfunction in L2(X)
π π
(cid:98)h right empirical eigenfunction g
(cid:98)
left empirical eigenfunction
c boundness constant P orthogonal projector in Wµ(X) onto Im(Z )
W H π µ
α regularity parameter c regularity constant
α
β spectral decay parameter c spectral decay constant
β
τ embedding parameter c embedding constant
τ
38
Table 1: Summary of used notations.