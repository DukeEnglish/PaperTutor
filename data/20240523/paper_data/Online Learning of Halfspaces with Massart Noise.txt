Online Learning of Halfspaces with Massart Noise
Ilias Diakonikolas∗ Vasilis Kontonis†
UW Madison UT Austin
ilias@cs.wisc.edu vasilis@cs.utexas.edu
Christos Tzamos‡ Nikos Zarifis§
UW Madison & University of Athens UW Madison
tzamos@wisc.edu zarifis@wisc.edu
May 22, 2024
Abstract
We study the task of online learning in the presence of Massart noise. Instead of assuming that
the online adversary chooses an arbitrary sequence of labels, we assume that the context x is selected
adversarially but the label y presented to the learner disagrees with the ground-truth label of x with
unknown probability at most η. We study the fundamental class of γ-margin linear classifiers and
present a computationally efficient algorithm that achieves mistake bound ηT +o(T). Our mistake
boundisqualitativelytight forefficientalgorithms: itisknownthatevenintheofflinesettingachieving
classification error better than η requiressuper-polynomial time in theSQ model.
Weextendouronlinelearningmodeltoak-armcontextualbanditsettingwheretherewards—instead
ofsatisfyingcommonlyusedrealizabilityassumptions—areconsistent(inexpectation)withsomelinear
ranking function with weight vector w∗. Given a list of contexts x ,...x , if w∗ x > w∗ x , the
1 k i j
· ·
expected reward of action i must be larger than that of j by at least ∆. We use our Massart online
learnertodesignanefficientbanditalgorithmthatobtainsexpectedrewardatleast(1 1/k)∆T o(T)
− −
bigger than choosing a random action at every round.
∗Supported by NSF Medium Award CCF-2107079, NSF Award CCF-1652862 (CAREER), and a DARPA Learning with
LessLabels(LwLL)grant.
†Thisresearch wasdoneatUWMadison. Supported inpartbyNSFAwardCCF-2144298(CAREER).
‡Thisresearch wasdoneatUWMadison. Supported byNSFAwardCCF-2144298(CAREER).
§Supported byNSFMediumAwardCCF-2107079, andaDARPALearningwithLessLabels(LwLL)grant.
4202
yaM
12
]GL.sc[
1v85921.5042:viXra1 Introduction
Online prediction has a rich history dating back to the works of [Rob51, Han57, B+54]. In the online
scenario, the learner’s objective is to tackle a prediction task by acquiring a hypothesis from a series
of examples presented one at a time. The aim is to minimize the overall count of incorrect predictions,
known as the mistake bound, considering the knowledge of correct answers to previously encountered
examples [Lit88, Lit89, Blu90, LW94, MT94]. In the context of online linear classification, i.e., when
the presented labels can be realized by a linear threshold function, the seminal perceptron algorithm
[Ros58, Nov62], was thefirst online learning algorithm.
RealizableandAgnosticOnlineLearning In[Lit88]therealizableonlineclassificationsettingwas
defined,wheretheadversaryisallowedtoselectanarbitrarydatapointx(t) ateveryroundbutthelabel
y(t) mustbeconsistentwithanunderlyinggroundhypothesisf fromaclass . Thenumberofmistakes
H
in the realizable setting was shown to be characterized by theLittlestone dimension LD( ) of theclass
H
. Similarly to the agnostic PAC learning of [Hau92], in [BDPSS09], motivated by the fact that often
H
theobservedlabels arenoisy,thesettingofonline learningwith labelnoise wasintroduced. Inthemost
extreme case of agnostic (adversarial) label noise where no assumptions are placed on the labels, it was
shown that the regret over T rounds is O( T LD( )). Even though the regret in the agnostic setting
H
wasshowntobesublinearinT,thecorrespondingcomputationaltaskisfarfrom beingwell-understood.
p
In particular, even for simple classes suceh as linear classifiers efficient online algorithms with sublinear
regret would imply efficient algorithms in theoffline setting – a well known computationally intractable
problem [GR06].
OnlineLearningwithMassartNoise Investigatingregimesbeyondtheabove,worst-case,agnostic
settinghasbeenanimportantareaofresearchwiththegoaltogetimprovedregret(andmistake)bounds
but also to allow the design of efficient algorithms. Similar to the definition of Massart or Bounded
noise [MN06] for offline (PAC) learning, in [BDPSS09] a semi-random online classification setting was
introduced with a focus on improving over the agnostic regret bounds. In this model, while the online
adversaryisallowedtopickarbitrarylocationstopresenttothelearner,thelabelsthattheychoosemust
beconsistent with theground-truthwith probability strictly larger than 50%.
Definition 1.1 (Online Learning with Massart Noise [BDPSS09]). Fix a class of concepts over Rd
C
and a target concept c∗ . At round t=1,...,T:
∈C
1. The adversary selects a point x(t) Rd and generates a random label y(t) such that Pr[y(t) =
c∗(x(t)) x(t)] η. ∈ 6
| ≤
2. The learner observes x(t), predicts a label yˆ(t), and suffers loss 1 yˆ(t) =y(t) .
{ 6 }
Thegoal of the learner is tominimizethe total mistakes defined asM(T)=E T 1 y(t) =yˆ(t) .
t=1 { 6 }
Remark 1.2 (RegretvsMistakeBounds). InDefinition 1.1 often the regret overhTProunds is used, i.ei .,
the expected difference between the mistakes done by the learner and the minimum number of mistakes
that any hypothesis h in could achieve R(T,h)=M(T) E T 1 y(t) =h(x(t)) .
H − t=1 { 6 }
In[BDPSS09],analgorithmbasedonLittlestone’sStandardhOPptimalAlgorithmisgivienthatachieves
a mistake bound M(T) min T 1 h(x(t) =y(t)+LD( )logT/(1 2 η(1 η)). Therefore, as
≤ h∈H t=1 { 6 H − −
longasηisboundedawayfrom1/2,astrongseparationbetweentheagnosticregret(thatgrowsroughly
P p
asΩ(√T)) and theMassart regret was established. However,as discussed in [BDPSS09], thisand other
similarapproachesforestablishingregretboundsdonotyieldcomputationallyefficientalgorithmsasthey
rely on computing combinatorial quantities that are at least as hard as computing the VC-dimension
[FL98].
Linear Classification To investigate the computational aspects of online classification we focus on
the fundamental class of linear classifiers or halfspaces, i.e., functions of the form h(x) = sign(w x)
for some weight vector w Rd. In the offline setting, a long line of recent works [ABHU15, DGT· 19,
∈
DKTZ20, CKMY20, DKK+21a, DKK+21b] has successfully bypassed the computational hardness of
worst-caseagnosticlearningandhasprovidedefficientalgorithms forlinearclassification inthepresence
of Massart noise. However, no efficient algorithm that achieves any non-trivial mistake bound is known
for the more challenging online Massart setting of Definition 1.1. In this work, we aim to answer the
following fundamentalquestion.
1Are there computationally efficient algorithms for online linear classification with Massart noise?
Massart Bandits Goingbeyondfull-information onlinelearning, weconsiderthemulti-armedbandit
setting: a multi-disciplinary research area that was initiated by [Tho33] and has received enormous
attention in thepast 30 years (see [BF85, CBL06, S+19] and references therein). In this work, we focus
onthek-armcontextualbanditsetting,whereateveryroundtthelearnerreceiveskcontextsx(t),...x(t),
1 k
chooses an action α = 1,...,k, and receives the reward/loss of their chosen action (the rewards of the
other actions are not revealed). In this setting a common assumption known as realizability [FCGS10,
AYPS11,CLRS11,ADK+12,LLZ17,FAD+18,FR20]prescribesthattheexpectedrewardsareparametric
functions of the contexts, e.g., E[r x(t)] = w x(t) for some unknown weight vector w. Using this
i | · i
structuralassumption, those works typically reduce thebandit problem to an online regression problem
which can often be solved efficiently (for linear and generalized linear models). An orthogonal direction
[LZ07,DHK+11,AHK+14]makesminimaldistributionalassumptionsandreducesthecontextualbandit
problem to agnostic (offline) classification. Since agnostic classification is computationally intractable
(formost non-trivialclasses) suchapproaches donot provideend-to-endefficient algorithms andrely on
heuristicstosolvetheunderlyingclassificationtask. Therefore,existingalgorithmsforcontextualbandits
either(i)makestrongrealizabilityassumptionsandreducetheproblemto(linear)regressionor(ii)make
minimal assumptions but face the computational intractability of agnostic classification. Motivated by
the online Massart classification setting of [BDPSS09], in this work, we propose a semi-random and
semi-parametric Bandit model that lies between those two extreme settings and investigate the design
of “end-to-end” efficient algorithms.
Before we describe our semi-random noise model, we start with its “noiseless” version. Instead of
assuming that theexpected rewards are given by a linear function of thecontexts, we assume that they
are ranked according to their linear scores, i.e., if w∗ x >w∗ x then the reward of action i must be
i j
· ·
at least as that of action j.
Definition 1.3 (Contexts and Linearly Sorted Rewards). Let be the d-dimensional unit ball. We
B
define = (x ,...,x ):x ,...,x to be the context space. For simplicity, we view each context
1 k 1 k
as a dX k m{ atrix X. Fix M >0 and l∈ etB w}∗ Rd be some unit vector. Given a context (x ,...,x ), we
1 k
× ∈
say that a reward vector r [0,M]k is sorted according to the w∗ if
∈
for all i,j : (r r )(w∗ x w∗ x ) 0.
i j i j
− · − · ≥
Under the linear ranking setting of Definition 1.3 one can reduce the bandit problem to a noiseless
linearclassificationtaskanddesignefficientalgorithms(basedonperceptronorlinearprogramming)that
will eventually learn to select the action with the highest reward. Our semi-random model extends the
abovedefinition where we require that rewards are only sorted in expectation with some margin ∆>0.
Definition 1.4 (Monotone Reward Distributions). Let be the context space and =N be the set of
rounds. Fix some unit vector w∗ Rd, M >0, and ∆>X 0. Define a class of distribuT tions indexed by
rounds t and contexts X: = D∈(t,X) :t ,X . We assume that each reward distribD ution D(t,X)
D { ∈T ∈X}
is supported on [0,M]k. We say that the class has monotone rewards with respect to w∗ with margin
D
∆ if for all t , X , i,j [k] with i=j it holds
∈T ∈X ∈ 6
E [r r w∗ x >w∗ x ] ∆.
i j i j
r∼D(t,X) − | · · ≥
Remark1.5(MassartOnlineClassificationasa2-armBandit). ToobtaintheonlineMassartsettingwe
set the rewards r(t) =((1+y(t))/2,(1 y(t))/2) and the contexts X(t) =(x(t), x(t)). We observe that
− −
inthat case when y(t) satisfies the Massart noise condition of Definition 1.1, it holds that conditional on
w∗ x>0 it holds that E[(r(t) r(t))] 1 2η. Therefore, Definition 1.4 is satisfied with ∆=1 2η.
· 1 − 2 ≥ − −
We stress that the monotone reward distributions of Definition 1.4 are a semi-parametric model as
we do not assume that the expected rewards have some parametric form rather than only require that
they are sorted with respect to a linear sorting function. A similar semi-random linear sorting model
was used in [FKKT22] in the context of (offline) learning linear rankings with bounded noise under the
Gaussian distribution. Wenext defineourcontextual banditmodel.
Definition1.6 (ContextualBanditswithMonotoneRewards(MassartBandits)). Fixsome unitvector
w∗ Rd, γ,∆,M >0, and a class of monotone reward distributions (see Definition 1.4). At round t:
∈ D
1. The adversary picks context X(t) and draws a random reward vector r(t) from D(t,X(t)).
∈X
2. The learner observes the context X(t), picks action a(t) [K] and receives reward r .
∈
a(t)
2We also define the full-information setting the same way except from the last step where the learner
receives reward r and observes the full reward vector r(t).
a(t)
Remark 1.7. Inwhat follows, weshalloften simplify notationbywritingD(t) instead ofD(t,X) forthe
reward distribution at round t.
1.1 Our Results
Our first result answers our main question posed in Section 1 and gives an efficient online classification
algorithm that makes roughly ηT +o(T) mistakes in the Massart noise model of Definition 1.1. Our
algorithm only requires that the sequence of examples picked by the advesrary satisfies a standard γ-
marginassumption. Withoutthemarginassumptionitisknown[Lit88]that,eveninthenoiselesssetting,
it is information theoretically impossible for thelearner to do less than T mistakes.
Theorem 1.8. Consider the Online Massart Learning setting of Definition 1.1. Additionally, assume
that the examples picked by the adversary have at least γ-margin with respect to some target halfpsace,
i.e., for all t = 1,...,T, it holds that x(t) 1 and w∗ x(t) γ, for some unit vector w∗. There
2
k k ≤ | · | ≥
exists an algorithm that does M(T)=ηT +O(T3/4/γ) mistakes and runs in poly(d) time per round.
Our mistake bound matches (when viewed as an offline PAC learning result) the best known error
guarantees of the corresponding offline learners of Massart halfspaces with margin given in [DGT19,
CKMY20]. Moreover, the mistake-bound achieved by our algorithm is essentially best-possible when
considering computationlly efficient (statistical query)algorithms: in therecent works [DK20, DKRS22,
NT22]thatconsideroffline(PAC)learningwithMassartnoise,itisshownthat,evenwhentheunderlying
distribution has γ-margin, no polynomial-time algorithm can achieve classification error better than
η/polylog(1/(1 2η))in theStatistical Queryframework. By astandardonline toofflinereductionthis
−
readily implies a ηT lower bound (upto polylog(1/(1 2η)) factors).
−
Remark 1.9 (Information-Theoretic vs Computationally Efficient Online Learning). The ηT mistake
bound is not information-theoretically optimal: in particular, in [BDPSS09]) better, near-optimal, mis-
take bounds are given albeit with inefficient algorithms (i.e., with runtime exponential in the dimension
d). When considering computationally efficient algorithms, as we do here, there is strong evidence (see
the SQ lower bounds of [DK20, DKRS22]]) that the ηT mistake is essentially best possible. Before our
work no computationally efficient algorithm was known that could beat the random guessing benchmark
(that makes T/2 mistakes). Moreover, we remark that, even in the offline setting of linear classification
under Massart noise, the first computationally efficient that achieved classification error η (the offline
equivalent of ηT mistakes) was only given in the relatively (given the history of the problem) recent work
[DGT19].
Ouralgorithmisparticularlysimple: weperformonlinegradientdescentonasequenceofreweighted
Leaky-ReLUlossfunctions. TheLeaky-ReLUlosshasbeensuccessfullyusedinseveralworksonlearning
withRandomClassification andMassart noise[Byl98,DGT19,CKMY20]. Intheonlinesettinghowever
simply using (online) gradient descent on the Leaky-ReLU does not suffice: even though the adversary
is restricted to select examples with margin with respect to some ground-truth halfspace, they can still
select examples very close to the decision boundary of the current hypothesis which would cause online
gradient descent to get stuck or converge to sub-optimal solutions. To overcome this issue, we reweight
theLeaky-ReLUlossbythemarginofthecurrentexampleaccordingtothecurrenthypothesisvector,see
Algorithm 1. Weare thenableto showthat standard regret guarantees for OnlineConvexoptimization
can be translated to obtain mistake boundsin the presenceof Massart noise, see Lemma 2.2.
We next present our result on semi-random “Massart” k-arm setting presented in Definition 1.4. In
addition to the ∆ “reward-margin” assumption of Definition 1.4, similarly to our online classification
result of Theorem 1.8, we require a γ “geometric-margin” assumption for the contexts with respect to
some halfspace. We give an efficient bandit algorithm, see Algorithm 3, that is able to accumulate
expected reward roughly ∆T more than playinga random action at every round.
Theorem 1.10 (Monotone k-arm Contextual Bandits). Consider the monotone reward online setting
of Definition 1.4. Moreover, for some unit vector w∗ Rd, assume that for all t, it holds that for all i
X(t) 1 and for all i=j, w∗ X(t) w∗ X(t)) ∈ γ. There exists a bandit algorithm that runs in
k i k2 ≤ 6 | · i − · j |≥
poly(d) time per round and selects a sequence of arms α(1),...,α(T) [k] that obtain expected reward
∈
T T k
1 k 1
E r(t)(α(t)) E r(t) + − ∆T O(T5/6(k∆M2)1/3/γ).
" #≥ " k i # k −
t=1 t=1 i=1
X X X
3Our algorithm for the k-arm bandit setting relies on the observation that, assuming that we could
observe all rewards r(t), then one could treat the labeled pairs (x(t) x(t),r(t) r(t)) as real-valued
i i − j i − j
versions of online linear classification with Massart noise and provide it as input to our online learning
algorithm. As is common in bandit problems, to adapt this “full-information” approach to the bandit
setting, we pick a random action with small probability at every round that provides us with unbiased
estimates of the full reward vectors. For more details we refer to Section 3. Finally, we remark that for
thespecial case of 2-armed banditsour result implies thefollowing corollary.
Corollary 1.11 (Monotone 2-arm Contextual Bandits). Consider the monotone reward online setting
of Definition 1.4. In the bandit setting, Algorithm 3 produces a sequence of arm choices α(1),...,α(T)
∈
1,2 that obtain expected reward
{ }
T T r(t)+r(t) 1
E r(t)(α(t)) E 1 2 + ∆T O((M2∆)1/3T5/6/γ).
" #≥ " 2 # 2 −
t=1 t=1
X X
Corollary 1.11isageneralizationofouronlinelearningresultofTheorem 1.8: indeed,usingRemark 1.5,
weobtainthattheexpectedrewardisequaltoT M(T),whereM(T)aretheexpectedmistakes. Thus,
−
Corollary 1.11 implies that T M(T) T/2+(∆/2)T +o(T) and, using thefact that∆=1 2η and
− ≥ −
M = 1, we get that M(T) ηT +o(T). Given the hardness result for improving upon ηT regret for
≤
online Massart classification that we already discussed we conclude that our reduction from Bandit to
OnlineMassart is tight for 2-armed bandits.
1.2 Notation
For n Z , let [n] := 1,...,n . We use lowercase boldface characters for vectors. We use x y for
+
the inn∈ er product of x,{ y Rd.} For x Rd, x denotes the ℓ -norm of x. For a set of vect· ors k,
2 2
∈ ∈ k k
X = x ,...,x , X denotes the difference between the vectors x ,x , i.e., X = x x . We
1 k i−j i j i−j i j
use 1{ = 1 A t} o denote the characteristic function of the set A. We use the standard O()− ,Θ(),Ω()
A
{ } · · ·
asymptotic notation.
We use E [X] for the expectation of a random variable X according to the distribution and
X∼D
D
Pr[ ]fortheprobabilityof event . Forsimplicity ofnotation, weomitthedistributionwhenit isclear
E E
from thecontext.
2 Online Learning with Massart Noise
In this section, we provide an algorithm for online learning halfspaces with Massart noise. The learner
iteratively processes a sequence of covariates and associated labels (x,y) provided by the adversary,
chooses a label corresponding to the current decision vector w, and then updates (even if the label was
correct) the decision vector using a carefully chosen convex loss. Our loss is based on a modification of
theLeakyReLUloss,i.e.,LeakyReLU (t),(1 λ)t1 t>0 +λt1 t<0 . First,weshowanequivalent
λ − { } { }
expression for theLeaky-ReLUloss.
Fact 2.1. Let LeakyReLU (t) , (1 λ)t1 t > 0 +λt1 t < 0 . It can be equivalently expressed as
LeakyReLU (t),1/2((1 λ 2λ)t t)− . { } { }
λ − | |−
Proof. Note that it holds t1 t > 0 = (t+ t)/2 and t1 t < 0 = (t t)/2. Therefore by plugging
{ } | | { } −| |
theseidentities in thedefinition of LeakyReLU (t),we obtain theresult.
λ
In our algorithm, we use as loss the LeakyReLU ( ty) where t R and y 1 . For the sake of
λ − ∈ ∈ {± }
brevityin notation, we defineC (t;y)=LeakyReLU ( ty),i.e.,
∆ (1−∆)/2 −
1
C (t;y), (∆t yt).
∆ 2 | |−
We provide some intuition behind our choice of the Leaky-ReLU loss. Notice, that in Definition 1.1
we have the label consistency which corresponds to the condition E[y]sign(w∗ x) (1 2η) := ∆.
· ≥ −
This is clear, from the fact that y = sign(w∗ x) with probability at most η and that, E [y] =
y
E [sign(w∗ x)(1 y=sign(w∗ x) 61 y=sign(w· ∗ x) )]=sign(w∗ x)(1 2E[1 y=sign(w∗ x) ]),
y
· { · }− { 6 · } · − { 6 · }
therefore E[y]sign(w∗ x) ∆. The Leaky-ReLU loss has the property that E [C ( w∗ x;y)] 0
y ∆
· ≥ − · ≤
(see Claim 2.3), which we can later use to design a loss for our problem. However, optimizing this
loss function exclusively does not guarantee minimal regret. To this end, we define the loss function
4ℓu,τ,∆(w,y,x):= mC a∆ x(( |w u· ·x x; |y ,τ) ). Tosimplify thenotation,wedefineℓ(t)(w)= mC a∆ xe (( |w w· (x t)(t ·x); (y t( )t |) ,τ) ),whereτ,∆
are fixedand theother parameters are changing through the iterations of our algorithm.
Subsequently, we demonstrate that minimizing the regret associated with these reweighted choseen
loss functions concurrently yields substantiveguarantees for theregret in thecontext of Definition 1.1.
1. ∆←1−2η−ǫ and τ ←ǫγ/4
2. w(0) =e .
1
e
3. For t=1,...,T:
(a) Adversary selects point x(t) ∈Rd and generates label y(t).
(b) Learner observes x(t) and chooses label yˆ(t) =sign(w(t)·x(t))
(c) Learner gets label y(t).
(d) Set
ℓ(t)(w)=
C ∆e(w·x(t);y(t))
max(|w(t)·x(t)|,τ)
(e) Run Online Convex Optimization on ℓ(t)(·).
Algorithm 1:Online Learning Massart Halfspaces
Lemma 2.2. Assume that a sequence w(t) is produced by a (possibly randomized) online algorithm
A
with the guarantee that
T T
E ℓ(t)(w(t)) ℓ(t)(w∗) R¯(T),
" − #≤
t=1 t=1
X X
with ℓ(t)(w)= C ∆e(w·X( 1t −) 2;y(t)) . Then, it holds that
max(|w(t)·X( 1t −) 2|,τ)
E[(1 sign(w(t) X(t) )=y(t) )] Tη+R(T,ǫ,γ,τ),
{ · 1−2 6 } ≤
t=1
X
where R(T,ǫ,γ,τ)=T(ǫ + 8τ)+R¯(T)(1+ 8τ).
2 ǫγ ǫγ
Proof. Denote ǫ = ∆ ∆, with ǫ < ∆/2 and let τ ǫγ/2. First, we show that the optimal decision
vectorw∗ gets negativ− e loss on expectation. ≤
Claim 2.3. It holds E y[Ce ∆e(w∗ ·x;y)] ≤−(ǫ/2) |w∗ ·x
|
Proof. It holds that Ce(t;y)= 1(∆t yt),therefore, we havethat
∆ 2 | |−
1
E[Ce(we∗ x;y)]= (∆w∗ x E[y]w∗ x)
y ∆ · 2 | · |− ·
1
= (∆e E[y]sign(w∗ x))w∗ x
2 − · | · |
1
(∆e ∆)w∗ x ,
≤ 2 − | · |
where we used that E[y]sign(w∗ x) ∆. e
· ≥
Wefirst show that
T
E[ ℓ(t)(w∗))] Tγǫ/2.
≤−
t=1
X
Usingthe tower rule, we havethat
T T
E[ ℓ(t)(w∗))]= E[ℓ(t)(w∗))].
t=1 t=1
X X
5FromClaim 2.3, we havethat E y[C ∆e( −w∗ ·x(t);y)] ≤−(ǫ/2) |w∗ ·x(t) |. Hence,we have that
T T w∗ X(t)
E[ℓ(t)(w∗))] (ǫ/2) | · 1−2|
≤− max(w(t) X(t) ,τ)
Xt=1 Xt=1 | · 1−2|
T
1
(ǫγ/2) , (1)
≤− max(w(t) X(t) ,τ)
Xt=1 | · 1−2|
where we used that |w∗ ·X 1(t −)
2| ≥
γ from the assumptions. Note that it holds that C ∆e(w ·X( 1t −) 2;y) =
(1/2)(∆ ysign(w X(t) ))w X(t) . Letg(t)(y)=(1/2)(∆ ysign(w(t) X(t) )). Usingthetowerrule,
− · 1−2 | · 1−2| − · 1−2
we get
e e
T T
E ℓ(t)(w(t)) ℓ(t)(w∗)
" − #
t=1 t=1
X X
T w(t) X(t)
= g(t)(E[y(t)]) | · 1−2| E[ℓ(t)(w∗)] .
max(w(t) X(t) ,τ) − !
Xt=1 | · 1−2|
We define J to be the set of rounds where the w(t) X(t) is smaller than the threshold τ, i.e.,
| · 1−2|
J = t: w(t) X(t) τ . Wehavethat
{ | · 1−2|≤ }
w(t) X(t)
g(t)(E[y]) | · 1−2| = g(t)(E[y]) (T J )/2, (2)
max(w(t) X(t) ,τ) ≥− −| |
Xt6∈J | · 1−2| Xt6∈J
and that
w(t) X(t)
g(t)(E[y]) | · 1−2| J /2.
max(w(t) X(t) ,τ) ≥−| |
Xt∈J | · 1−2|
MoreovernotethatfromInequality (1),itholdsthat E[ℓ(t)(w∗)] ǫγ J andthat E[ℓ(t)(w∗)]
t∈J− ≥ 2τ| | t6∈J− ≥
0. Therefore, we get that
P P
T T
ǫγ 1
E[ℓ(t)(w(t))] E[ℓ(t)(w∗)] J J ,
− ≥| |2τ −| |2
Xt∈J Xt∈J
and by using our assumption that τ ǫγ/2, we get that T E[ℓ(t)(w(t))] T E[ℓ(t)(w∗)]
≤ t∈J − t∈J ≥
J ǫγ. Using the assumption for the regret guarantee, we have that E[ T ℓ(t)(w(t))] R¯(T) +
| |4τ P t=1 P ≤
E[ T ℓ(t)(w∗)], which is equivalentto
t=1 P
P E[ℓ(t)(w(t))] R¯(T) (E[ℓ(t)(w(t))] E[ℓ(t)(w∗)])+ E[ℓ(t)(w∗)] . (3)
≤ − −
Xi6∈J Xi∈J Xi6∈J
I1 I2 I3
Using that|I
1
≥({ |zJ |−T)}/2 from I|nequality(2), th{zat I
2
≤−|J |4ǫγ
τ
}from| abov{ezand t}hat I
3
≤0 from
Claim 2.3, we havethat
J (R¯(T)+T)4τ/(ǫγ). (4)
| |≤
Finally, from theregret guarantees, i.e., Inequality(3), we havethat E[ℓ(t)(w(t))] R¯(T). Recall
t6∈J ≤
that that g(t)(E[y])=(1/2)(∆ E[y]sign(w(t) X(t) )). By adding (∆ sign(w(t) X(t) )E[y(t)])
− · 1−2 Pt∈J − · 1−2
on both sides of Inequality (3) and using that g(t)(E[y]) 2 for all t [T], we have:
≤ P∈
e e
(∆ sign(w(t) X(t) )E[y(t)])+ (∆ sign(w(t) X(t) )E[y(t)]) 2R¯(T)+2J , (5)
− · 1−2 − · 1−2 ≤ | |
Xt∈J Xt6∈J
e e
where we have used that (∆ sign(w(t) X(t) )E[y(t)]) 2J and that I ,I 0 as discuseed
t∈J − · 1−2 ≤ | | 2 3 ≤
above. Equivalently, using Inequality(4), we obtain the following bound over all rounds t = 1,...,T:
P
(∆ sign(w(t) X(t) )E[y(te)]) 2R¯(T)+(R¯(T)+T)4τ/(ǫγ). Using that sign(w(t) X(t) )y(t) =
t=1 − · 1−2 ≤ · 1−2
1 21 sign(w(t) X(t) )=y(t) and ∆=1 2η ǫ, we get that
P− {e · 1−2 6 } − −
E[(1 sign(w(t) X(t) )=y(t) )] T(η+ǫ)+R¯(T)+16(R¯(T)+T)τ/(ǫγ).
{ ·
1−2e
6 } ≤
t=1
X
6Before proceedingwith theproof of Theorem 1.8,wemakeuseofthefollowing fact about theonline
gradient descent on convex functions to bound theR¯(T)of Lemma 2.2.
Fact 2.4 ([Haz16]). Let ℓ(t) be a convex function and let D = diam( ) and let G = max ℓ(t)() .
t 2
W k∇ · k
Online gradient descent with step size: η = D/(G√T) guarantees the following for all T 1: R(T) =
T ℓ(t)(w(t)) minw T ℓ(t)(w) GD/3√T. ≥
t=1 − t=1 ≤
PProof of Theorem 1.8. UPsingFact 2.4,wegetthatR¯(T)=O(√T/γ),thereforefromLemma 2.2,weget
that theexpected total regret is bound aboveby
E[ (1 sign(w(t) X(t) )=y(t) )] ηT +R(T,ǫ,γ,τ),
{ · 1−2 6 } ≤
t=1
X
whereR(T,ǫ,γ,τ)=(ǫ/2)T+R¯(T)+(R¯(T)+T)8τ/(ǫγ)Tominimizethisquantity,wesetτ =Θ(ǫ1+ζγ)
forany ζ >0 andǫ=Θ(T−1/(4+2ζ)/γ) gives R(T,ǫ,γ,τ) T3/4−O(ζ)/γ. By takingζ close to0and get
≤
theresult.
3 Contextual Bandits with Monotone Rewards
In this section, we describe an algorithm for thesetting of Definition 1.4. To this end,we usea general-
ization of the LeakyLeRU loss we used in Theorem 1.8. This loss is described in Algorithm 2. First we
show that our loss satisfies some properties required by our main algorithm to work. The proof can be
found at AppendixA.
1. Generate rewarddifferences and context for every j ∈{1,...,k}: y =r −r .
j α j
2. If u=0: Return the loss
ℓ(w)=G(w;X,v,r,α),−Λ w·X y
α−j j
j6=α
X
3. Otherwise:
(a) for every j ∈{1,...,k} set:
v
z =X +ρsign(X ·v)
j α−j α−j
kvk
2
(b) Return the loss
C (w·z ;y )
ℓ(w)=G(w;X,v,r,α), ∆ j j
|v·z |
j
j6=α
X
Algorithm 2:Compute Loss G(w;X,v,r,α), where w is the argument and X,v,r,α are parameters.
Claim 3.1. The loss ℓ(w)=G(w;X,v,r,α) computed by Algorithm 2 isconvex and 2Mkmax(Λ,1/ρ)-
Lipschitz.
Lemma 3.2. Let w(t) be the sequence produced by algorithm Algorithm 3 in the bandit setting of
Definition 1.4 with exploration probability q. It holds
T
E[ (G(w(t);X(t),r(t),α(t)) G(w∗;X(t),r(t),α(t)))]
−
t=1
X
O(kM√TΛ/q),
≤
where the expectation is over the randomness of Algorithm 3 and the randomness of the reward vectors
r(t) D(t).
∼
Proof. Wefirst showthatfor anyreward vectorr(t) theloss ℓ(t)(w)that weconstructat everyroundis
an unbiased estimate of the corresponding full-information loss function G(w;X(t),w(t),r(t),α(t)). We
show thefollowing claim.
71. w(0) =e .
1
2. For t=1,...,T:
(a) Adversarypicks context X(t) =(x(t),...,x(t)) and samples reward r(t) ∼D(t).
1 k
(b) If w(t) =0: Learner picks a uniformly random α(t).
(c) Otherwise: Learner picks α(t) =argmax w(t)·x(t).
i i
(d) Learner flips a coin c(t) with HEADS probability q.
(e) If HEADS:
i. Learner picks uniformly random action β(t).
ii. Learner gets reward r(t)(β(t)) and defines the fake reward vector r(t):
r(t)(i)= (k−1) r(t)(β(t)) if i=β(t) e
(M −r(t)(β(t)) if i6=β(t),
e
iii. Set ℓ(t)(w)= 1G(w;X(t),w(t),r(t),α(t)).
q
(f) If TAILS:
i. Learner gets reward r(t)(α(t)) aend sets ℓ(t)(w)=0.
(g) Learner performs Online Convex Optimization with loss ℓ(t)(·).
Algorithm 3:Bandits with Monotone Rewards
Claim 3.3 (Unbiased Loss Estimate). For any w Rd it holds
∈
E ℓ(t)(w) (t),r(t) =G(w;X(t),w(t),r(t),α(t)).
c(t),β(t) |F
h i
Proof. SinceateverystepofAlgorithm 3weconstructthelossG(w;X(t),w(t),r(t),α(t)),usingAlgorithm 2,
we denote by y(t) =r(t) r(t) the reward difference vector of the adapted reward vector r. Moreover,
α(t) − j
recall that we denote y(t) = r(t) r(t). Notice that y(t) is a random variaeble that depends on the
j α(t) − j
uniformly rande om ace tion β(te). The loss depends on whether the w(t) = 0 or not so, we ce onsider each
case separately. We start with the case where w(t) = 0eand we denote this event as A(t). Recall that
z(t) = X(t) +ρsign(X(t) w(t))w(t)/ w(t) 6 is the vector containing the context differences as
j α(t)−j α(t)−j · k k2
computed in Algorithm 2. Taking theexpectation with respect to therandom coin flip c(t) we obtain:
E ℓ(t)(w)1 A(t) (t),r(t)
c(t),β(t) { }|F
h i
1
=q E [G(w;X(t),w(t),r(t),α(t))1 A(t) (t),r(t)]
q β(t) { }|F
C (w z(t);ey(t))
= E ∆ · j j 1 A(t) (t),r(t)
β(t)
j6=Xα(t)
|w(t) ·z( jt)
e|
{ }
(cid:12) (cid:12)
F 
 E [C (w z(t);y(t)) (t),r(cid:12) (cid:12)(t)] 
= β(t) ∆ · j j |F 1 A(t) .
w(t) z(t) { }
j6=Xα(t) | ·ej |
where for the last equation we used the linearity of expectation and the fact that the action α(t), the
event A(t) and the weight vector at t-th iteration w(t) do not depend on β(t) conditional on (t). We
F
now observe that the loss C (t;y)=(1/2)(∆t yt) is linear in y. Therefore, using again the linearity
∆
| |−
of expectation, we havethat
E [C (w z(t);y(t)) (t),r(t)]
β(t) ∆ · j j |F
e=C ∆(w ·z( jt); βE (t)[y( jt) |F(t),r(t)]).
e
8Next, we consider the case where w(t) = 0, and we call this event (Ac)(t). We have that by taking the
expectation with respect to therandom coin flip c(t) we obtain:
E ℓ(t)(w)1 (Ac)(t) (t),r(t)
c(t),β(t) { }|F
h i
= E [G(w;X(t),w(t),r(t),α(t))1 (Ac)(t) (t),r(t)]
β(t) { }|F
= E [ w X ey(t)1 (Ac)(t) (t),r(t)]
β(t) −
j6=Xα(t)
· α−j j { }
(cid:12)
(cid:12)
F
e (cid:12)
= w X E [y(t) (t),r(t)(cid:12) ]1 (Ac)(t) ,
−
j6=Xα(t)
· α−j β(t) j
(cid:12)
(cid:12)
F { }
e (cid:12)
whereforthelastequationweusedthelinearityofexp(cid:12) ectationandthefactthattheactionα(t) andthe
eventA(t) do not depend on β(t) conditional on (t).
To finish the proof we have to show that theF adapted reward difference vector y(t) is an unbiased
estimate of the true reward difference vector y(t). Since we pick the action β(t) uniformly at random
from 1,...,k , we havethat the i-th coordinate of theadapted reward vectorr is eequal to
{ }
1 1
βE (t)[r i(t) |F(t),r(t)]= k(k −1)r( it)+
k
(M −r( st)).e
Xs6=i
e
Therefore, we havethat theexpected differencey(t) is equal to
j
1 1
βE (t)[y j(t) |F(t),r(t)]= k(ke −1)r( αt ()
t)
+
k
(M −r( st))
s6=Xα(t)
1e 1 1 1
(k 1)r(t) (M r(t))= (k 1)r(t) r(t)
− k − j − k − s k − α(t) − k j
Xs6=j
1 1
(k 1)r(t)+ r(t) =r(t) r(t) =y(t).
− k − j k α(t) α(t) − j j
Therefore, combining theabove equations, we conclude that
E [ℓ(t)(w) (t),r(t)]G(w;X(t),w(t),r(t),α(t)).
c(t),β(t) |F
This completes theproof of Claim 3.3.
Wehavethatthelossℓ(t)(w)=G(w;X(t),w(t),r(t),α(t))constructedateachsteptofAlgorithm 3is
convexsinceitistheconvexcombinationofthezerolossandtheconvexlossesG(w;X(t),w(t),r,α(t))(see
Claim 3.1)fordifferentrewardvectorvectorsr (eacehrealizationoftherandomvariableβ(t) corresponds
to a different reward vector r). From Fact 2.4 and Claim 3.1 we have that the sequence w(t) produced
byAlgorithm 3 achieves expected regret
T
E[ (G(w(t);X(t),r(t),α(t)) G(w∗;X(t),r(t),α(t)))]
−
t=1
X
O(kM√TΛ/q).
≤
Next, we prove a generalization of Lemma 2.2 for the k-arm setting. It shows that minimizing the
regret overtheloss functions, boundstheexpected reward of our setting.
Lemma 3.4. Let w(t) be a stochastic process in Rd adapted to the filtration ( (t)) such that
t∈T
F
T T
E[ G(w(t);X(t),w(t),r(t),α(t)) G(w∗;X(t),w(t),r(t),α(t))] R¯(T).
− ≤
t=1 t=1
X X
Then, it holds that
T T k
1
E[ r(t) )] E[ r(t)]+(k 1)/k∆T R(T,γ,∆,Λ,k),
α(t) ≥ k i − −
t=1 t=1 i=1
X X X
with R(T,γ,∆,Λ,k)=(R¯(T)/k)(1+1/(γΛ))+TM/Λ.
9Proof. We denote as A(t) the event that w(t) = 0 and ρ = γ/2. We first observe that by adding
6
ρsign(w X(t) )w to the difference X(t) we do not affect the choice of the the optimal weight
· α(t)−j α(t)−j
vector w∗ and our guess w(t). We observe that w∗ z(t)sign(w∗ X(t) ) (w∗ X(t) ρw∗
· j · α(t)−j ≥ | · α(t)−j|− ·
w(t)/ w(t) ) γ ρ 0, therefore sign(w∗ z(t)) = sign(w∗ X(t) ). For w(t) the similarly note
k k2 ≥ − ≥ · j · α(t)−j
that w z(t) =sign(w(t) X(t) )(w(t) X(t) +ρ w(t) ).
· j · α(t)−j | · α(t)−j| k k2
First,weshowthattheoptimaldecisionvectorw∗ getsnegativelossonexpectation(seeAppendixA
for theproof).
Claim 3.5. It holds that E [ T ℓ(t)(w∗))] kγ∆ T 1 (A(t))c .
y t=1 ≤− t=1 { }
Next,we bound thecontriPbution of theloss of thewP(t) (SeeAppendixA for theproof).
Claim 3.6. It holds that
T T
E[ ℓ(t)(w(t))]= (1/2)(∆ sign(w(t) X(t) )E[(r(t) r(t))])1 A(t) .
− · α(t)−j α(t) − j { }
Xt=1 Xt=1j6=Xα(t)
Let J = T 1 (A(t))c , we show that can bound J using our guarantees (See AppendixA for the
t=1 { }
proof).
P
Claim 3.7. It holds that J (R¯(T)+TM(k 1))/((k 1)γ∆Λ).
≤ − −
By plugging Inequality(7) and using Claim 3.5 in the assumption for the regret guarantee, we get
that
T
1 A(t) (∆ sign(w(t) X(t) )E[r(t) r(t)]) 2R¯(T). (6)
{ } − · α(t)−j α(t) − j ≤
Xt=1j6=Xα(t)
Finally, we need to bound from below the term E T r(t) . For this reason, we need to connect
t=1 α(t)
the reward of each round r(t) with the regret of thhePloss (6). Niote that if the learner chose the action
α(t)
α(t) that means that w(t) X(t) 0 for all j given that we are in the event A(t). Therefore we can
· α(t)−j ≥
decompose r(t) as follows
α(t)
k k
1 1
r(t) = r(t)+ sign(w(t) X(t) )(r(t) r(t)).
α(t) k i k · α(t)−j α(t) − j
Xi=1 j6=Xα(t)
Furthermore,notethatwhenweareintheevent(Ac)(t),thelearnerchoosesarandom action,therefore,
usingInequality (6) and theequality above, we havethat theexpected reward is
T T k
1 k 1
E[ r(t) ] E[ r(t)]+ − ∆(T J) (2R¯(T)/k).
α(t) ≥ k i k − −
t=1 t=1 i=1
X X X
UsingClaim 3.7andsettingR(T,γ,∆,Λ,k)=(R¯(T)/k)(1+1/(γΛ))+TM/Λ,wecompletetheproofof
Lemma 3.4.
The proof of Theorem 1.10 follows from Lemma 3.4 and Lemma 3.2. The proof can be found on
AppendixA.
4 Conclusions and Open Problems
In this work we considered online linear classification in the Massart or Bounded online classification
model of [BDPSS09]. Under a standard (and necessary) margin assumption, we gave the first efficient
algorithmforthisproblemachievingamistakeboundofηT+o(T). Thisboundisessentiallyoptimaldue
to known hardness results in the Statistical Query model [DKRS22]. We extended our online learning
setting to a k-arm Bandit model that lies between the commonly used regression-based, realizable and
the pessimistic agnostic classification contextual bandit models. In this model, we utilized our online
Massart learner to obtain an efficient bandit algorithm that obtains roughly (1 1/k)∆T more reward
−
than playing at random at every round. We observed that our reduction is tight for the case of 2 arms
10(given the aforementioned SQ hardness results for learning with Massart noise). However, for k > 2
armsitisunclearwhetherthisrewardboundisbestpossible,asthegapbetweenplayingatrandomand
playing the best arm at every round may be much larger than ∆. We leave this as an interesting open
problem for futurework.
References
[ABHU15] P.Awasthi,M.F.Balcan,N.Haghtalab,andR.Urner.Efficientlearningoflinearseparators
th
under bounded noise. In Proceedings of The 28 Conference on Learning Theory, COLT
2015, pages 167–190, 2015.
[ADK+12] Alekh Agarwal, Miroslav Dudík, Satyen Kale, John Langford, and Robert Schapire. Con-
textual bandit learning with predictable rewards. In Artificial Intelligence and Statistics,
pages 19–26. PMLR, 2012.
[AHK+14] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire.
Taming the monster: A fast and simple algorithm for contextual bandits. In International
Conference on Machine Learning, pages 1638–1646. PMLR, 2014.
[AYPS11] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear
stochastic bandits. Advances in neural information processing systems, 24, 2011.
[B+54] D. Blackwell et al. Controlled random walks. In Proceedings of the international congress
of mathematicians, volume 3, pages 336–338, 1954.
[BDPSS09] Shai Ben-David, Dávid Pál, and Shai Shalev-Shwartz. Agnostic online learning. In COLT,
volume3, page 1, 2009.
[BF85] Donald A Berry and Bert Fristedt. Bandit problems: sequential allocation of experiments
(monographsonstatisticsandappliedprobability). London: ChapmanandHall,5(71-87):7–
7, 1985.
[Blu90] A. Blum. Learning Boolean functions in an infinite attribute space. In Proceedings of the
Twenty-Second Annual Symposium on Theory of Computing, pages 64–72, 1990.
[Byl98] T. Bylander. Worst-case analysis of the Perceptron and exponentiated update algorithms.
Artificial Intelligence, 106, 1998.
[CBL06] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
[CKMY20] S. Chen, F. Koehler, A. Moitra, and M. Yau. Classification under misspecification: Half-
spaces, generalized linear models, and connections to evolvability. In Advances in Neural
Information Processing Systems, NeurIPS, 2020.
[CLRS11] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear
payoff functions. In Proceedings of the Fourteenth International Conference on Artificial
Intelligence and Statistics, pages 208–214. JMLR Workshop and Conference Proceedings,
2011.
[DGT19] I. Diakonikolas, T. Gouleakis, and C. Tzamos. Distribution-independent pac learning of
halfspaces with massart noise. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems
32, pages 4751–4762. Curran Associates, Inc., 2019.
[DHK+11] Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev
Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. arXiv preprint
arXiv:1106.2369, 2011.
[DK20] I.DiakonikolasandD.M.Kane. Hardnessoflearninghalfspaceswithmassartnoise. CoRR,
abs/2012.09720, 2020.
[DKK+21a] I. Diakonikolas, D. M. Kane, V. Kontonis, C. Tzamos, and N. Zarifis. Agnostic proper
learning of halfspaces undergaussian marginals. In Proceedings of The 34th Conference on
Learning Theory, COLT, 2021.
[DKK+21b] I. Diakonikolas, D. M. Kane, V. Kontonis, C. Tzamos, and N. Zarifis. Efficiently learning
halfspaces with tsybakov noise. STOC,2021.
11[DKRS22] IliasDiakonikolas,DanielKane,LishengRen,andYuxinSun. Sqlowerboundsforlearning
single neurons with massart noise. Advances in Neural Information Processing Systems,
35:24006–24018, 2022.
[DKTZ20] I.Diakonikolas,V.Kontonis,C.Tzamos,andN.Zarifis. Learninghalfspaceswith tsybakov
noise. arXiv, 2020.
[FAD+18] Dylan Foster, Alekh Agarwal, Miroslav Dudík, Haipeng Luo, and Robert Schapire. Prac-
tical contextual bandits with regression oracles. In International Conference on Machine
Learning, pages 1539–1548. PMLR, 2018.
[FCGS10] SarahFilippi,OlivierCappe,AurélienGarivier, andCsabaSzepesvári. Parametricbandits:
The generalized linear case. Advances in neural information processing systems, 23, 2010.
[FKKT22] Dimitris Fotakis, Alkis Kalavasis, Vasilis Kontonis, and Christos Tzamos. Linear label
rankingwithboundednoise. AdvancesinNeuralInformationProcessingSystems,35:15642–
15656, 2022.
[FL98] Moti Frances and Ami Litman. Optimal mistake bound learning is hard. Information and
Computation, 144(1):66–82, 1998.
[FR20] DylanFosterandAlexanderRakhlin. Beyonducb: Optimalandefficientcontextualbandits
withregressionoracles. InInternational Conference onMachineLearning,pages3199–3210.
PMLR, 2020.
[GR06] V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proc.
47th IEEE Symposium on Foundations of Computer Science (FOCS),pages543–552. IEEE
Computer Society,2006.
[Han57] J. Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of
Games, 3:97–139, 1957.
[Hau92] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other
learning applications. Information and Computation, 100:78–150, 1992.
[Haz16] E. Hazan. Introduction to online convex optimization. Foundations and Trends® in Opti-
mization, 2(3-4):157–325, 2016.
[Lit88] N.Littlestone. Learning quicklywhen irrelevant attributes abound: anew linear-threshold
algorithm. Machine Learning, 2(4):285–318, 1988.
[Lit89] N. Littlestone. Mistake bounds and logarithmic linear-threshold learning algorithms. PhD
thesis, University of California at SantaCruz, 1989.
[LLZ17] Lihong Li, YuLu, and DengyongZhou. Provably optimal algorithms for generalized linear
contextual bandits. In International Conference on Machine Learning, pages 2071–2080.
PMLR, 2017.
[LW94] N.Littlestone and M. Warmuth. The weighted majority algorithm. Information and Com-
putation, 108(2):212–261, February 1994.
[LZ07] JohnLangfordandTongZhang. Theepoch-greedyalgorithm formulti-armedbanditswith
side information. Advances in neural information processing systems, 20, 2007.
[MN06] P. Massart and E. Nedelec. Risk bounds for statistical learning. Ann. Statist., 34(5):2326–
2366, October 2006.
[MT94] W. Maass and G. Turan. How fast can a threshold gate learn? In S. Hanson, G. Drastal,
andR.Rivest,editors,ComputationalLearningTheoryandNaturalLearningSystems,pages
381–414. MIT Press, 1994.
[Nov62] A. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on
Mathematical Theory of Automata, volumeXII,pages 615–622, 1962.
[NT22] R. Nasser and S. Tiegel. Optimal SQ lower bounds for learning halfspaces with massart
noise. In Conference on Learning Theory, volume 178 of Proceedings of Machine Learning
Research, pages 1047–1074. PMLR, 2022.
[Rob51] H. Robbins. Asymptotically subminimax solutions of compound statistical decision prob-
lems. In Proceedings of the second Berkeley symposium on mathematical statistics and
probability, volume2, pages 131–149. Universityof California Press, 1951.
12[Ros58] F.Rosenblatt. ThePerceptron: aprobabilistic modelforinformation storage andorganiza-
tion in the brain. Psychological Review, 65:386–407, 1958.
[S+19] AleksandrsSlivkinset al. Introductiontomulti-armed bandits. Foundations and Trends®
in Machine Learning, 12(1-2):1–286, 2019.
[Tho33] William R Thompson. On the likelihood that one unknown probability exceeds another in
view of the evidenceof two samples. Biometrika, 25(3-4):285–294, 1933.
Appendix
A Omitted Proofs from Section 3
A.1 Proof of Claim 3.1
Weprovethefollowing:
ClaimA.1. Thelossℓ(w)=G(w;X,v,r,α)generated byAlgorithm 2isconvexand2Mkmax(Λ,1/ρ)-
Lipschitz.
Proof. First, ℓ() is convex because is a sum of convex functions. Moreover, note that the derivativeis
·
1∆sign(w z ) y
∇ℓ(w)= 2 v z· jj − z j ,
jX6=α | · |
andnotethat y max r and z 2+ρ since all thex havenorm at most 1. Hence, ℓ(w)
i i j 2 i 2
| |≤ | | k k ≤ k∇ k
is upperboundedby kmax r max (1/u z ) 2kmax r /ρ.
i i i i i i
| | | · | ≤ | |
A.2 Proof of Claim 3.5
Weprovideproof for thefollowing claim:
Claim A.2. It holds that E [ T ℓ(t)(w∗))] kγ∆ T 1 (A(t))c .
y t=1 ≤− t=1 { }
Proof. Using thetower rule, wPe havethat P
T T
E[ ℓ(t)(w∗))]= E[ℓ(t)(w∗))1 A(t) +1 (Ac)(t) ].
y y { } { }
t=1 t=1
X X
We first show that E [ℓ(t)(w∗))1 A(t) ] 0. Recall that C (t;y) = 1(∆t yt). By taking the
y { } ≤ ∆ 2 | |−
expectation over y in the E [C ( w∗ z(t);y(t))], we get that
y ∆ − · j j
E[C ( w∗ z(t);y(t))]
y ∆ − · j j
1
= (∆w∗ z(t) E r(t) r(t) w∗ z(t)).
2 | · j |− α(t) − j · j
h(cid:16) (cid:17)i
RecallthatbytheDefinition 1.4,wehavethatE[(r(t) r(t))]sign(w∗ X(t) ) ∆andaswediscussed
α(t)− j · α(t)−j ≥
abovesign(w∗ z(t))=sign(w∗ X(t) ). Therefore we havethat
· j · α(t)−j
E r(t) r(t) w∗ z(t) ∆w∗ z(t) ,
α(t) − j · j ≥ | · j |
h(cid:16) (cid:17)i
which gives that E [C ( w∗ z(t);y(t))] 0 and hence E [ℓ(t)(w∗))1 A(t) ] 0 . Next, with similar
y ∆ − · j j ≤ y { } ≤
argumentsasbeforewehavethatE [ℓ(t)(w∗))1 (Ac)(t) ] kΛ∆1 (Ac)(t) whichcompletestheproof.
y
{ } ≤− { }
13A.3 Proof of Claim 3.6
Weprovethefollowing:
Claim A.3. It holds that
T T
E[ ℓ(t)(w(t))]= (1/2) ∆
Xt=1 Xt=1j6=Xα(t)
(cid:0)
sign(w(t) X(t) )E[(r(t) r(t))] 1 A(t) .
− · α(t)−j α(t) − j { }
(cid:1)
Proof. Note that in the case that the event (Ac)(t) the loss of w(t) is zero. Recall that it holds that
C (w(t) z(t);y(t))=(1/2)(∆ y(t)sign(w(t) z(t)))w(t) z(t) . Tosimplify thenotation,letg(t)(y(t))=
∆ · j j − j · j | · j | j j
(1/2)(∆ y(t)sign(w(t) z(t))). Using thetower rule, we get
− j · j
T
E[ ℓ(t)(w(t))]
t=1
X
T g(t)(E[y(t)])w(t) z(t)
= j j | · j |1 A(t)
w(t) z(t) { }
Xt=1j6=Xα(t) | · j |
T
= g(t)(E[y(t)])1 A(t) . (7)
j j { }
Xt=1j6=Xα(t)
Moreover, since it holds that sign(w(t) z(t)) = sign(w(t) X(t) ), we have g(t)(E[y(t)]) = (1/2)(∆
· j · α(t)−j j j −
sign(w(t) X(t) )E[y(t)]). Hence,we have that
· α(t)−j j
g(t)(E[y(t)])
j j
=(1/2) ∆ sign(w(t) X(t) )E[(r(t) r(t))] .
− · α(t)−j α(t) − j
(cid:0) (cid:1)
A.4 Proof of Claim 3.7
Werestate and prove thefollowing claim.
Claim A.4. It holds that J (R¯(T)+TM(k 1))/((k 1)γ∆Λ).
≤ − −
Proof. From Inequality(7), we havethat
T T
E ℓ(t)(w(t)) = g(t)(E[y(t)])1 A(t)
" # j j { }
Xt=1 Xt=1j6=Xα(t)
M(k 1)(T J)/2.
≥− − −
Furthermore,sing the assumption for theregret guarantee, we have:
T T
E ℓ(t)(w(t)) ℓ(t)(w∗) R¯(T).
" − #≤
t=1 t=1
X X
Hence,using Claim 3.5, we get theresult.
A.5 Proof of Theorem 1.10
Werestate and prove Theorem 1.10.
Theorem A.5 (Monotonek-armContextualBandits). Consider the monotone reward onlinesetting of
Definition 1.4. Moreover, for some unit vector w∗ Rd, assume that for all t, it holds that for all i
∈
14X(t) 1 and for all i=j, w∗ X(t) w∗ X(t)) γ. There exists a bandit algorithm that runs in
k i k2 ≤ 6 | · i − · j |≥
poly(d) time per round and selects a sequence of arms α(1),...,α(T) [k] that obtain expected reward
∈
T T k
1
E r(t)(α(t)) E r(t)
" #≥ " k i #
t=1 t=1 i=1
X X X
k 1
+ − ∆T O(T5/6(k∆M2)1/3/γ).
k −
Proof of Theorem 1.10. Algorithm 3 in each iteration, either with probability q makes a random choice
or with probability 1 q chooses the best action according to the current decision weight vector (or a
−
random action if w(t) =0). Therefore we havethat
T T T k
q
E r(t) =(1 q) E r(t) + E r(t) .
c(t) − α(t) k " i #
Xt=1 h i Xt=1 h i Xt=1 Xi=1
UsingLemma 3.4,we get that
T T k
1
E r(t) E r(t)
≥ k " i #
Xt=1 h i Xt=1 Xi=1
k 1
+(1 q) − ∆T (1 q)R(T,γ,∆,Λ,k).
− k − −
where R(T,γ,∆,Λ,k) = (R¯(T)/k)(1 + 1/(γΛ)) + TM/Λ. From Lemma 3.2 we have that R¯(T) =
(k 1)M√T/qmax(1/γ,Λ). By maximizing it, we get Λ = 1/γT1/6(M/(k∆))1/3 and q = M/(γΛ∆).
−
Therefore we get that the expectedreward is
T T k
1
E r(t) E r(t)
≥ k " i #
Xt=1 h i Xt=1 Xi=1
k 1
+ − ∆T T5/6(k∆)1/3M2/3/γ .
k −
15