G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction
Data Selection for Machine Translation
XingyuanPan† LuyangHuang LiyanKang† ZhichengLiu
YuLu ShanboCheng‡
ByteDanceResearch
xypan00@gmail.com {huangluyang, chengshanbo}@bytedance.com
Abstract We argue that Diversity and Quality of the in-
struction data present a pair of challenges for in-
LargeLanguageModels(LLMs)havedemon-
structionfinetuning.Zhouetal.(2023)havedemon-
stratedremarkableabilitiesingeneralscenar-
ios. Instructionfinetuningempowersthemto strated that a model trained with a limited, care-
alignwithhumansinvarioustasks. Neverthe- fullycurateddatasetcomposedofhigh-qualityand
less,theDiversityandQualityoftheinstruction diverse examples outperforms models trained on
dataremaintwomainchallengesforinstruction larger,moreextensivedatasetsduringinstruction
finetuning. With regard to this, in this paper,
finetuning. Subsequently, various methods have
weproposeanovelgradient-basedmethodto
beenproposedtoautomaticallyselecthigh-quality
automatically select high-quality and diverse
and diverse training data from the large pool of
instructionfinetuningdataformachinetrans-
instructionfinetuningdatasets(Chenetal.,2023a;
lation. Ourkeyinnovationcentersaroundana-
lyzinghowindividualtrainingexamplesinflu- Cao et al., 2023). Yet, these methods often rely
ence the model during training. Specifically, onanothermodeltodecidequalityordiversity,ne-
weselecttrainingexamplesthatexertbenefi- glecting the inherent model behavior and strong
cial influences on the model as high-quality abilityoftheLLMitself.
ones by means of Influence Function plus a
To this end, we propose G-DIG, a novel
smallhigh-qualityseeddataset. Moreover,to
Gradient-based method to automatically select
enhance the diversity of the training data we
Diverse and hiGh-quality instruction finetuning
maximizethevarietyofinfluencestheyhaveon
themodelbyclusteringontheirgradientsand data for machine translation. We use influence
resampling.ExtensiveexperimentsonWMT22 function(KohandLiang,2017),agradient-based
andFLOREStranslationtasksdemonstratethe methodthatquantifiestheimpactmadebyindivid-
superiorityofourmethods,andin-depthanal- ualtrainingsamples. Concretely,we(1)measure
ysis further validates their effectiveness and
theresponsequalityofeachtrainingsamplewith
generalization.1
the influence score of the training sample on test
1 Introduction instancesand(2)enhancethediversityofthetrain-
ing data by maximizing the variety of influences
Large Language Models (LLM) have revolution-
theyhaveonthemodel.
izedthefieldofNaturalLanguageProcessingwith
Specifically, we hypothesize that high-quality
their strong abilities in general language under-
datashouldhavepositiveinfluencesonhigh-quality
standingandgeneration(OpenAI,2023;Achiam
test samples. Hence, we first manually create a
etal.,2023). Toenablethisstrongability,instruc-
smallsetofhigh-qualityseeddataandthenauto-
tion finetuning has been proposed to better align
maticallyselecthigh-qualitydatathathavepositive
language models (Wei et al., 2021; Chung et al.,
influences on seed data. Moreover, we utilize K-
2022; Ouyang et al., 2022). Significant progress
meansclusteringalgorithmstoclustertrainingdata
hasbeenmadeoncollectingextensiveinstruction
withsimilarinfluences,usinggradientsrepresent-
finetuningdataforbetteraligningLLMstoproduce
ingtheirinfluencesonthemodel. Unlikeexisting
helpfulresponses(Chungetal.,2022).
methods that introduce an external model to de-
†The work was done when the author was an intern at cidequalityanddiversity,ourmethodsdirectlyuse
ByteDance.
modelgradients,whichcapturethemodelbehav-
‡Correspondingauthor.
ior through learning algorithms and back to the
1Code is available at https://github.com/xypan0/
G-DIG trainingdata.
4202
yaM
12
]LC.sc[
1v51921.5042:viXraWe conduct experiments on Zh En and De diversityoftranslationdata.
⇒
Entranslationtasks. Specially,Wecollectlarge
⇒ Traning Data Quality and Diversity. Various
candidatepoolsandmanuallyconstructtwosmall
studies evident that the quality and diversity of
setsofseeddata. WethenfinetunedifferentLLM
instruction finetuning data predominate the per-
backbonesonvarioussizesofselectedsubsetsand
formance of LLMs (Zhou et al., 2023; Touvron
compare their performances with different selec-
et al., 2023; Li et al., 2023). For example, Zhou
tivemethods and existing SOTA LLMs. Under a
etal.(2023)manuallycuratedasmall,high-quality
thorough comparison in a range from 1k to 64k
instruction set to elevate the model’s instruction
selected samples, our proposed method not only
followingpower. Althoughthemethodsin(Zhou
surpasses the baseline selective methods but also
etal.,2023)relyheavilyonhumaneffort,theymo-
achieves competitive performance against SOTA
tivateresearchaimingtoautomaticallyobtainhigh
models. Extensiveexperimentsandin-depthanal-
quality instructions. Cao et al. (2023) propose to
ysis emphasize the need for data selection and
scorethequalityofeachinstructionbycombining
demonstratetheeffectivenessandgeneralizationof
several language indicators using a linear model.
ourproposedmethods.
However,alltheseindicatorsrelyonexternalmod-
els. Besides,Duetal.(2023)presentacomprehen-
2 RelatedWork
siveapproachforselectinghighqualityanddiverse
instruction based on reward model score and se-
LLM for Machine Translation. Due to
manticdiversity. Still,theirmethodsrelyheavily
their strong in-context learning and instruction-
onexternalmodelsandoverlookthedirectimpact
following abilities, powerful LLMs like GPT-4
thatfinetuningdatahasonthemodel. Remarkably,
have achieved remarkable progress in machine
Li et al. (2023) propose a self-guided approach
translation, with comparable performance to the
to select difficult instructions with the guidance
top systems on the WMT translation task (Zhu
of the LLM itself. Admittedly, their methods are
etal.,2023;Heetal.,2023;Raunaketal.,2023).
freeofanyexternalmodels. However,theyselect
TofullyleverageLLMs’translationability,various
trainingexamplesmoreassociatedwithnecessity
methodshavebeenproposed,includingin-context
andcomplexitythanquality,andtheyoverlookthe
translationexemplarselection(Garciaetal.,2023;
importance of finetuning data diversity. Despite
Linetal.,2022;Zhangetal.,2023a;Agrawaletal.,
the efforts these methods have made to automate
2022),promptoptimization(Jiaoetal.,2023)and
instructionselection,theyeitheroverlooktheim-
decodingstrategies(Zengetal.,2023a).
portance of the quality and diversity of training
Theaforementionedstudiesallfocusonthein-
data or rely on external models for judging. Sig-
ference stage optimization, while another line of
nificantly different from them, our methods take
work focuses on instruction tuning the LLMs for
both quality and diversity into consideration and
bettertranslationperformance. Xuetal.(2023)pro-
arefreeofexternalmodels.
poses to first finetune the model on monolingual
dataandthenonhigh-qualityparalleldata. Lietal. Gradient-basedDataSelection. Dataselection
(2024b)trainsthemodeltoelicittranslationability withinfluencefunctionhasbeenwidelystudiedin
bymultilingualinstructiontuning. Lietal.(2024a) NLP. Lam et al. (2022) propose to identify erro-
proposestocreatehigh-qualityinstruction-tuning neoustrainingdatabyusingsyntheticnoisydata,
datafromlargermodelsbyapatchingmechanism. showing that vanilla influence functions are not
Chenetal.(2023b)improvesthemodelinstruction sufficientforgood retrievalperformance. On the
understandingbyaddingaglobalinstructionrepre- contrary, we select high-quality and diverse fine-
sentationandimprovesmodelfaithfulnessbycom- tuning data with the aid of gradient information.
paringover-translationandmisstranslationresults Also, we show that our use of influence function
withthecorrecttranslation. Zengetal.(2023b)pro- iscapableofselectinghigh-qualitydata.Akyurek
posesanovelframeworkusingexamplesincom- et al. (2022) demonstrate the potential of using
parisontoteachLLMstolearntranslation. How- gradientinformationtotracefactualknowledgein
ever,allthesemethodsneglecttheimportanceof languagemodelsbacktothetrainingdata. Never-
instructionfinetuningdataqualityanddiversityin theless, theirpracticalapplicationsremainunder-
machine translation. And in this paper, we pro- studied. Methods for scaling up influence func-
poseanovelapproachtoenhancethequalityand tion (Schioppa et al., 2022) and explaining black1.Quality 2.Enhance
Candidate
 Selection High-Quality Diversity Final SFT

Pool Data Data
step 1
 step 2
 step 1
 step 2
 step 3

Calculate Select with
 Clustering on Sample Uniformly Collect and Get
Influence IF Score Gradients from Each Cluster Final Result
Seed Data EFFECT
Candidate Pool
Diverse

TRAINING EFFECT
: Positive Influence : Select
: Negative Influence : Discard
Figure1: Overviewofourproposedmethod. Ouroverallmethodconsistsoftwocomponents: (1)high-quality
dataselectionand(2)enhancingtheirdiversity. Inhigh-qualitydataselection,wecalculatethepair-wiseinfluence
(dashedarrows)ofthecandidatesonseeddata. Thenweselectthosewithallpositiveinfluences(asmarkedgreen).
Afterwards,weclusterontheselectedhigh-qualitydatatodistinguishdissimilarinfluences(asmarkedindotswith
differentcolors)andresampletofurtherobtainhigh-qualityanddiversefinetuningdata.
box predictions of NLP models have been pro- 3.1 High-qualityDataSelection
posed (Han et al., 2020). Remarkably, the con-
Inthissection,wedetailourapproachforselecting
currentwork(Xiaetal.,2024)proposesasimilar
high-qualitytrainingdataformachinetranslation.
methodtoselecttask-specificLLMfinetuningdata
Intuitively,ifatrainingexamplesignificantlybene-
byestimatinginfluencesfortrainingdata. Differ-
fitsthemodeltogeneratehigh-qualityoutputs, it
ent from them, we select high-quality finetuning
islikelytobeofhighqualityitself. Consequently,
data for machine translation. And we show that
wefirstmanuallycurateasmallsetofhigh-quality
ourmethodiscapableofcapturingahigher-level
translationdatathatwerefertoastheseeddatato
concept(i.e.,thequalityoftrainingdata).
formacriteriaforevaluatingtrainingdataquality.
3 Methods Thenweselecttrainingdatathataidsthemodelin
generatinghigh-qualityseeddata.
In this section, we describe our gradient-based
Concretely,weemployInfluenceFunction(IF)
method to select high-quality and diverse train-
(KohandLiang,2017)toquantifyhowatraining
ing data for instruction finetuning, as displayed
examplez influencesthemodel’sbehaviorona
m
in Figure 1. Our methods consist of two parts:
testexamplez . Inourinfluencefunctionsetting,
t
1) high-quality data selection with the influence
westartwiththefollowingfinetuningobjective:
function(§3.1)and2)diversedataselectionwith
gradient-basedclustering(§3.2). Forhigh-quality
n
1 (cid:88)
dataselection,importantly,weutilizetheinfluence θ∗ := argmin L(z θ), (1)
i
n |
functiontoquantifytheimpactofindividualtrain- θ i=1
ingonthetestsample. Fordiversedataselection,
weusegradientdistancetoassesstheoveralldiver- whereθisthemodelparameter,z = (x ,y )isthe
i i i
sityoftheinstructiontrainingdata. i-thprompt-responsepairandthelossLissimplythelanguagemodelinglossoftheresponsesolely: whichwecollectpubliclyavailableWMT22’spar-
allel texts. Throughout our implementations, we
L(x,y) = logP(y x) usetheprompttemplate: Translatethefollowing
− |
T textinto{trg_lang}.\n\nText:\n“{src_text}”,where
(cid:89)
= log p(yj x,y<j), (2) {trg_lang}aretargetlanguagessuchas"English"
− |
j=1 and{src_text}arethesourcetext. Andtheresponse
issimplythetargettext. Wedetailourdataprepa-
wherexj denotesthej-thtokenofx. Theninflu-
rationinSection4.
encefunctioncalculatestheinfluenceofz onz
m t
by: 3.2 DiverseDataSelection
(z ,z ) = L(z )⊤H−1 L(z ), Afterobtaininghigh-qualitytranslationpairs,we
I
m t −∇θ|θ∗ t θ∗ ∇θ|θ∗ m
furtherensurethediversityofthetrainingdata. To
(3)
where θ∗ is the optimal model parameter of fine- ensure coverage of different translation patterns,
t iu sn ti hn eg Hin ess(1 ia) na on fd thH eθ t∗ rai= nin∇ g2 θ on b1 j(cid:80) ectn i i= v1 eL a( tz θi |θ =∗) w die vep rr so itp yo .s Se pt eo cu ifise cag llr ya ,d wie ent cs oi nm sii dla er rit ty heto ga rass de iess ntth oe f
θ∗. Thoughthecalculationoftheinfluencefunc- theresponselossinequation(2)withrespecttothe
final MLP layer and average them out on all the
tion is complex, the most significant aspect for
readers is that if the scalar (z ,z ) is negative, tokens. We utilize the Euclidean distance as the
m t
training the model on z I reduces the model’s similaritymeasure.
m
loss on z . In this case, z is considered to To maximize the diversity of training data in-
t m
be helpful for the model to generate z . In our fluences, we cluster on the gradients of training
t
implementation of IF, we use the gradients of examples to obtain different patterns. Then we
model’sMultilayerPerceptron(MLP)parameters sampleuniformlyfromtheclusteringresulttoen-
in 3,6,9,12,15,18,21,24,27,30 -th layers to sure the diversity of training data. Moreover, we
spe{ ed up calculation. And we av} erage the gra- employ K-means as the clustering algorithm due
dients of each token to form a vector. Also, we to its ability to process large-scale datasets. Fur-
use Kronecker-Factored Approximate Curvature thermore,tospeedupandreducememory,weuse
(KFAC) (Martens and Grosse, 2015) to approxi-
RandomProjectionasthedimensionalityreduction
mateHessianforreducingmemoryconsumption. methodtoreducethedimensionofthegradientsto
WedetailthederivationofIFandourmodification 400(BinghamandMannila,2001). Inpractice,we
forfittingitintoLLMfiningtuninginAppendixA. clusterthetrainingdatainto512clusters.
Correspondingly, as depicted in Figure 1, our
4 ExperimentSetup
proposedhigh-qualitydataselectionmethodcon-
sistsoftwosteps: (1)calculatinginfluencesand(2)
Datasets. WeconductexperimentsonZh En
selection. Weselecttrainingexamplesinourcan- ⇒
andDe Entasks. Wecollectseparatecandidate
didatepool thatexertbeneficialimpactonall ⇒
raw pools for different translation directions. Specifi-
D
samplesintheseeddata,i.e.,examplez
m raw cally, our Zh En pool is composed of 1.9 mil-
∈ D
meets: ⇒
lionexamplessampledfromWMT22’sParaCrawl
v9, News Commentary v16, UN Parallel Corpus
z , (z ,z ) < 0. (4)
t seed m t
∀ ∈ D I V1.0, WikiMatrix and Back-translated news and
our De En pool contains 256k examples sam-
Thus,weselecttrainingdatathatcontributestothe
⇒
pledfromWMT22’sEuroparlv10,ParaCrawlv9,
model’shigh-qualitygeneration.
NewsCommentaryv16andWikiMatrix. Wesplit
In practice, we find that the seed instruction
512examplestovalidationsetsforevaluation. We
dataset of size 256 suffices for selecting
seed
D test our methods on the latest WMT22 test sets
high quality instructions. Hence, we set the size
fromthenewstranslationtrackofWMT22compe-
of to 256 in our implementation. As the
seed
D tition2 andFlores-101dev-testsplit(Goyaletal.,
focus of this paper is to select high-quality fine-
2022). TheWMT22Zh EnandDe Entest
tuning data for translation task, we randomly se-
⇒ ⇒
setscontain1875and1984samples,respectively.
lect256paralleltextsfromthevalidationsetand
have them revised by human translators. We se-
2https://github.com/wmt-conference/wmt22-news-
lecthigh-qualitydatafromourcandidatepool,for systemsZh En De En
⇒ ⇒
WMT FLORES WMT FLORES
83
78 88.5
86
82
76 88.0
85
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
40
20 29
26 18 28
38
16 24 27
14 26 36
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
66
71 79.0
75
64 78.5
70
74 78.0
62 69
73 77.5
60
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
TrainingDatasetSize TrainingDatasetSize TrainingDatasetSize TrainingDatasetSize
G-DIG G-DIGw/oDiversity Random
Figure2: ThecomparisonresultsofmodeltrainedonvariousamountsofdataselectedbyourG-DIG,G-DIGw/o
DiversityandRandomselectiononZh EnandDe Entranslations. WeplottheresultsonZh EnandDe
⇒ ⇒ ⇒ ⇒
Entranslationsintheleftandrighttwocolumnsrespectively.
And the Flores-101 dev-test split contains 1012 BaselinesandComparisons. Inordertodemon-
samplesforeachZh EnandDe En. stratethesuperiorityandeffectivenessofourmeth-
⇒ ⇒
ods,wecompareourG-DIGtovalidatetheperfor-
ImplementationDetails. WeuseBaichuan2-7B
mance of our overall approach. Also, we assess
(Baichuan, 2023) for Zh En and Llama2-7B
its variant G-DIG w/o Diversity (G-DIG without
⇒
forDe En. Forallfinetuningexperiments,we
enhancingthedatadiversity)tosolelyinvestigate
⇒
adopt the same setting. The finetuning process
ourhigh-qualitydataselectionmodule. Forcom-
lasts for 3 epochs with an initial learning rate of
parisons,weconsiderthefollowingbaselines:
1e 5andaglobalbatchsizeof64. Theinstruc-
−
tion template we use is Translate the following • Bayling-13B(Zhangetal.,2023b),anEnglish
textinto{trg_lang}.\n\nText:\n“{src_text}”,where /ChineseLLMbasedonLlamawithsuperior
{trg_lang}aretargetlanguagessuchas"English" translationcapabilities.
and{src_text}arethesourcetext. Themodelsare
• BigTranslate-13B(Yangetal.,2023),amulti-
evaluated every 10 steps. We use the checkpoint
lingualLLMbasedonLlamawiththecapabil-
withthesmallestlossonthevalidsetforthefinal
ityoftranslatingover100naturallanguages.
test. Duringtheinference,weusebeamsearchas
thedecodingstrategywithabeamsizeof4. The • TIM (Zeng et al., 2023b). We present the
trainingandinferenceof7Bsizemodelsarecon- results of BLOOMZ-7b-mt and Llama2-7b
ducted on 16 NVIDIA A100 80GB GPUs with trainedwithTIM-Full-LM-basedBadOutput
DeepSpeedZeRO-3Offload. for WMT22 test sets and FLORES respec-
tively.
Evaluation. Forautomaticevaluation,weusethe
widelyusedmetricsBLEU(Papinenietal.,2002), • Modeltrainedontherandomsubset. Toem-
BLEURT(Sellametal.,2020),andCOMET(Rei phasizetheneedforinstructionselection,we
et al., 2020). We use ScareBLEU3, BLEURT-20 choosek randominstructionstoformafine-
(Puetal.,2021)andUnbabel/wmt22-comet-da4 in tuningsubset. WefinetunetheLLMonthese-
theevaluationimplementations. lectedsubsetsandreporttheirperformances.
3https://github.com/mjpost/sacrebleu • Model trained on the reward subset. We
4https://huggingface.co/Unbabel/wmt22-comet-da also compare our method with the existing
TEMOC
UELB
TRUELBZh En De En
⇒ ⇒
Model WMT22 FLORES WMT22 FLORES
COMET BLEU BLEURT COMET BLEU BLEURT COMET BLEU BLEURT COMET BLEU BLEURT
SOTAModels
Bayling-13B 77.72 20.12 - - - - 83.02 27.34 - - - -
BigTranslate-13B 74.26 14.16 - - - - 80.68 23.35 - - - -
TIM-7B 79.33 23.81 - 85.81 26.25 78.19 25.43 - 88.82 41.96 -
NLLB-54B 70.70 16.56 - - - - 78.94 26.89 - - - -
BaselineModels
Random 75.55 14.31 61.35 85.63 24.25 73.84 82.58 27.60 70.57 87.80 36.89 77.52
Reward 77.90 16.29 64.64 86.16 24.48 74.73 83.28 27.52 71.51 88.37 38.69 78.62
Ours
G-DIG 78.29 20.63 64.86 86.18 25.04 74.75 83.28 28.99 71.25 88.59 39.97 78.79
G-DIGw/oDiversity 78.43 20.35 65.08 86.45 25.81 75.19 82.93 28.71 70.96 88.68 39.97 78.83
Table1: Inthistable,wepresentthecomparisonresultsofourmethodswithvariousbaselinesinaccordancewith
Section4. Wedirectlyadopttheresultsfromtheoriginalpaperandomitthemissingmetrics. Wereporttheresults
ofourG-DIGandG-DIGw/oDiversity. TheBestresultineachgroupisinbold. TheBestresultineachcolumnis
in red.
selection method. We use the commonly
Model Score Win↑ Tie Lose↓
used reward model-based method for select-
inghigh-qualitytrainingdata(Duetal.,2023; Zh⇒En
Cao et al., 2023). Specifically, we follow
w/Random 3.59 — — —
(Du et al., 2023) to use the reward-model-
debertav3-large-v25 toscoreeachinstruction w/Ours 3.92 35.0% 53.0% 12.0%
andselectthetopkinstructionswiththehigh-
De⇒En
estscoretoformthefinetuningsubset.
w/Random 3.92 — — —
5 ExperimentalResults
w/Ours 4.21 34.0% 53.0% 13.0%
5.1 MainResults
Table2: Humanevaluationresultsonrandomlysam-
Inthissection,wepresentourmainexperimental
pledsets. “Win”/“Tie”/“Lose”standsforthepercentage
findings. We start with comparing our methods
oftranslationswhereoursisbetterthan, tiedwith, or
withbaselinesonvariousamountsoftrainingdata
worsethantherandomsubset.
inFigure2. Thenwecompareourbestresultswith
SOTA models and baselines in Table 1. Further-
more, we conduct human evaluation and present baseline in almost all cases. Statistical analysis
theresultsinTable2. Weshowthatourapproach resultsinAppendixCfurthersuggestourmethods
issuperiorintermsofeffectivenessandrobustness. excel the random baseline. These results demon-
stratetheefficacyandrobustnessofourmethodsin
Our Methods Improve the Translation Perfor-
termsoftheamountofselecteddata. Meanwhile,
manceAcrossVariousAmountofTrainingData.
we also observe that the quality and diversity of
In order to demonstrate the scalability of our
finetuningdatadominatetheperformanceofLLM.
methodintermsoftheamountofselectedtraining
Specifically,forbothZh EnandDe Entrans-
data, we present the results as a function of the ⇒ ⇒
lations we can see that models trained on 1000
amount of training data from 1000 to 64,000 in
examples selected by our G-DIG outperform the
Figure 2 for Zh En and De En translation.
⇒ ⇒ modelstrainedwith64krandomlyselectedexam-
Notably,ourG-DIGmodelconsistentlysurpasses
ples.
the random model across all metrics and dataset
sizes for Zh En translation. Also, for De Our Methods Surpass Baselines and Achieve
⇒ ⇒
Entranslation,ourG-DIGoutperformstherandom Comparable Results with SOTA Models. To
seehowourmethodperformscomparedwithbase-
5https://huggingface.co/OpenAssistant/reward-model-
deberta-v3-large-v2 linesandSOTAmodels,inTable1wepresenttheZh En De En
⇒ ⇒
WMT FLORES WMT FLORES
78
83
88.5
77 86
82
76 88.0
75 85
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
40
20
26 28 18
38
16 24 26
14 36
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
79.0
64 75 71
78.5
70
62 74 78.0
69
73 77.5
60
1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k 1k 2k 4k 8k 16k 32k 64k
TrainingDatasetSize TrainingDatasetSize TrainingDatasetSize TrainingDatasetSize
G-DIGw/oDiversity Reward Random
Figure3: ThecomparisonresultsofmodeltrainedonvariousamountsofdataselectedbyourG-DIGw/oDiversity,
RewardmodelselectionandRandomselectiononZh EnandDe Entranslations. WeplottheresultsonZh
⇒ ⇒ ⇒
EnandDe Entranslationsintheleftandrighttwocolumnsrespectively.
⇒
resultsofSOTAmodels,ourbestandcorrespond- indicatingourmethodsbetteralignthemodelthan
ingbaselines. Specifically,ourG-DIGachievesits therandomselectionmethod.
bestresultsat4000trainingexamplesforZh En
⇒
translationand64kforDe Entranslation. For
⇒
baselinescomparisons,weobservethatourmeth-
ods surpass baselines in almost all cases, demon-
strating the effectiveness of our approach. For TheFewertheInstructions,theGreatertheSig-
SOTAmodelscomparisons,our7Bmodelsachieve nificance of Diversity. To see the role that di-
comparable results with TIM-7B and even better versity plays during finetuning, we compare the
resultscomparedwithBayling-13B,BigTranslate- resultsofG-DIGandG-DIGw/oDiversityonZh
13BandNLLB-54B. En and De En translation in Figure 2. Re-
⇒ ⇒
markably, our diversity enhancement makes sig-
OurMethodsAligntheModelBetterCompared nificantadvancementsinenhancingthetranslation
withRandomBaseline. Wefurtherconducthu- performancewhenthereareonlyfewtrainingdata
man evaluation to analyze the translation quality. provided (k 2000). For Zh En translation
≤ ⇒
Werespectivelyrandomlypick100sentencesfrom with 1000 training examples, our G-DIG further
Zh EnandDe Entestsets,andrecruitthree improvesG-DIGw/oDiversityby1.7intermsof
⇒ ⇒
human judges for evaluation. For each sentence, COMETonWMTandby2.17intermsofBLEU
thejudgesreadthesourcesentenceandtwocandi- onFLORES.Inaddition,forDe Entranslation
⇒
datetranslations,whicharefromtherandomsubset with1000trainingexamples,ourG-DIGimproves
model and G-DIG subset model. The judges are theBLEUby2.11and1.24onWMTandFLORES
requiredtorateeachcandidateonascaleof1to5 respectivelycomparedwithG-DIGw/oDiversity.
andpickthebetterone. However,astheamountofinstructionsincreases,
From Table 2, we can see our methods enable thiseffectfadesaway,sincelargeinstructionsets
themodeltotranslatebetterwithahigheraverage arealreadycoupledwithhighdiversity. Asshown
scoreinbothZh EnandDe Entranslations. inFigure2,theG-DIGcurvesalmostcoincidewith
⇒ ⇒
Also,ourG-DIGsubsetmodelismorefrequently theG-DIGw/oDiversitycurvesinallmetricswhen
ratedasbettertranslationthantherandomsubset theamountofinstructionsgoesbeyond4000and
modelonbothZh EnandDe Entranslations, 8000forWMTandFLORES,respectively.
⇒ ⇒
TEMOC
UELB
TRUELBZh⇒En De⇒En
SourceText TargetText SourceText TargetText
Ours
例如，如果在大部分时间里 Forexample,ifthepriceisbe- Der ganze MDF Italia Tense TheentireMDFItaliaTenseTa-
价格在1,200美元到1,800美元 tween$1,200and$1,800most Tischwirdmiteiner3mmdün- bleiscoatedwitha3mmthin
之间，这就是常见值域。 ofthetime,thisisthecommon nen Acrylharzfolie und Stein- acrylicresinfilmandstonemin-
range. mineralieninWeißverkleidet. eralsinwhite.
根据武器法第6条，若无司 AccordingtotheWeaponsAct, X850Infrarot-Induktions-Funk- X850InfraredInductionWire-
法部长颁发许可证，禁止出 Section6,itisprohibitedwith- Türklingelw/blaueAnzeige- lessDoorbellw/BlueIndicator
口(任何种类)武器和武器设 outalicensefromtheMinister Weiß(3xAAA+3xAA) -White(3xAAA+3xAA)
备。 ofJusticetoexportweapons(of
anykind)andwarequipment.
Random
在对她的行为进行调查之 The West Bradford MP was Ebenso wichtig ist das In this regard, sociology, an-
前，西拉德福德议员被剥夺 stripped of the parliamentary Engagement in interdiszi- thropology, history, education,
了议会的鞭打并被禁止参加 whipandbarredfrompartyac- plinären und internationalen andphilosophyareconsidered
政党活动。 tivitypendinganinvestigation Forschungszweigen wie der asimportantasworkinresearch
ofherbehaviour-whichDavid Critical Psychology, den Cul- fields like critical psychology,
Cameronbrandedracist. turalundPostcolonialStudies, cultural and postcolonial stud-
den Gender wie auch den ies,genderstudies,andreligious
ReligiousStudies. studies.
我们提供2134个酒店在西安 HotelsinXi’an(Shaanxi) Hin-undRückflügevonLagos Return flights from Lagos to
nachMadrid Madrid
Table3: Examplesofoursandrandomlyselectedtrainingdataintheformofparalleltexts.
5.2 Analysis WMT FLORES
86.5
Inthissection,weconductablationstudiesonour
78.5
86.0
78.0
high-qualitydataselectionmoduleandourdiver-
85.5
77.5
sityenhancementmodule. Specifically,toseethe
85.0
superiorityofourhigh-qualitydataselectionmod- 1k 2k 4k 1k 2k 4k
ule, we compare G-DIG w/o Diversity with the
20.5
rewardmodel-basedbaselineinFigure3. Inorder 25 20.0
todemonstratetheadvantageofourgradient-based 24
19.5
diverse data selection module, we compare our
19.0 23
G-DIGwithitsembedding-basedcounterpartsin 1k 2k 4k 1k 2k 4k
Figure4. 75
65
Our High-quality Data Selection Module Sur-
74
64
passes the Reward Model-based Method In
Figure 3, we show that our high-quality data se- 1k 2k 4k 1k 2k 4k
TrainingDatasetSize TrainingDatasetSize
lectionmoduleG-DIGw/oDiversityachievessu-
G-DIG Embedding-based G-DIGw/oDiversity
perior results compared with the existing reward
model-basedmethodinFigure3. Specifically,for Figure4: ThecomparisonsbetweenourG-DIG,G-DIG
Zh En translation our G-DIG achieves over- w/oDiversityandembedding-basedmethodsonvarious
⇒
allbetterresultscomparedwiththerewardmodel- amountsoftrainingdataonZh Entranslation.
⇒
based method, falling behind in only few cases.
ForDe Entranslation,itishardtodistinguish
⇒ withitsembedding-basedcounterpart. Specifically,
betweenoursandrewardmodels-basedbaselines
embedding-basedmethodsmeasurethesimilarity
in COMET and BLEURT. However, our G-DIG
oftrainingexamplesbasedonembeddings. Their
w/oDiversitysignificantlyoutperformsthereward
objectiveistomaximizethesemanticrichnessof
model in terms of BLEU on both WMT22 and
the training data. We follow the model used in
FLORES.
(Duetal.,2023)fordiversifyingthetrainingdata.
OurDiversityEnhancementModuleImproves WeuseBERT(Devlinetal.,2018)toextractem-
theTrainingDataDiversity. Inthispart,wesub- beddings. Then we run Kmeans to cluster these
stantiatethesuperiorityofourgradient-baseddiver- embeddings and use our sampling procedure to
sityenhancementmodule. WecompareourG-DIG enhancethetrainingdatadiversity. Asillustrated
TEMOC
UELB
TRUELBinFigure4,ourproposedmethodoutperformsits clusteringontheirgradientsandresampling. Exten-
embedding-basedcounterpartinalmostallcases, siveexperimentsprovethatourmethodsimprove
whichvalidatestheadvantageofourgradient-based theLLMintermsoftranslationability. Also,hu-
method. Forinstance,with1000instructionspro- manevaluationresultsdemonstratethatourmeth-
vided,ourmethodssurpasstheembedding-based odsbetteraligntheLLMcomparedwiththebase-
method by 2.11 of BLEURT and 1.91 of BLEU line model. We hope this work facilitates the re-
on WMT and FLORES respectively. Moreover, searchonLLMfinetuningdataselection.
in almost all scenarios, the curves of embedding-
Limitations
basedmethodcoincidewithG-DIGw/oDiversity
curves with trivial improvements, indicating that
ComputationalCostforInfluenceFunction. In
theexistingmethodsthatenhancesemanticcover-
this paper, we utilize Influence Function to mea-
agecontributelittletothefinetuningdatadiversity.
sure the influence of a training sample on a test
OurMethodsSelectHighlyParallelTexts. In sample’sprediction. However,thecomputational
this part, we showcase our selected high-quality costforcalculatingthisinfluencecanbelargefor
data and randomly selected data in the form of LLMs. Theasymptoticcomputationalcomplexity
paralleltextsinTable3. Remarkably, notonlyis for calculating Hessian is (P2) where P is the
O
ourselecteddataaccurateandcoherentinthetarget number of model parameters. And the computa-
textspace,butitisalsonaturalandofthecorrect tional complexity for calculating pair-wise influ-
formatandgrammarintermsofthesourcetext. encebetweentheseeddataandcandidatepoolis
(MNP)whereM andN denotethenumberof
O
5.3 DiscussiononHyperparameters seeddataandcandidates,respectively. Howtore-
There are two primary hyperparameters in our ducethecomputationalcostofinfluencefunction
methods: thenumberofseeddata inhigh- stillremainsachallenge. Weleavethisforfuture
seed
|D |
qualitydataselectionandthenumberofclustersfor work.
K-meansindiversedataselection. Intuitively,the
EthicalConsiderations
moreseeddataweuse,themorestrictthecriteria
in (4) would be. Therefore, increasing the num-
Allthedatasourcesarepubliclyavailableanddo
ber of seed data improves the quality of selected
not involve privacy issues. For all human evalu-
data. Andusinglessseeddatacanberegardedas
ations mentioned in the paper, we hire full-time
a relaxation to the criteria in (4). We experiment
professionaltranslatorsandpaythemwithmarket
with seed dataset sizes of 128, 256, and 512 and
wage. Allofourtranslatorsaregraduates.
note that further enlarging the seed dataset con-
tributeslittletoimprovingthequalityofselected
seeddata. Consideringthehighcostofobtaining References
human-annotatedhigh-qualitydata,weultimately
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
use256seeddataafterbalancingannotationcosts
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
andmodelperformancebenefits. DiogoAlmeida,JankoAltenschmidt,SamAltman,
Thereasonablechoiceofthenumberofclusters ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774.
indiversedataselectioniscrucialforourmethods.
Sinceourcandidatepoolisquitelarge,wecluster Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke
thedatainto512clusterstoensurethevarietyof Zettlemoyer, andMarjanGhazvininejad.2022. In-
trainingdata. contextexamplesselectionformachinetranslation.
arXivpreprintarXiv:2212.02437.
6 Conclusion
Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Bin-
binXiong,IanTenney,JacobAndreas,andKelvin
Inthispaper,weproposeG-DIG,anovelgradient-
Guu.2022. Towardstracingknowledgeinlanguage
based method for selecting high-quality and di- modelsbacktothetrainingdata. InFindingsofthe
verseLLMfinetuningdataformachinetranslation. AssociationforComputationalLinguistics: EMNLP
Specifically, usingInfluenceFunctionandaseed 2022, pages 2429–2446, Abu Dhabi, United Arab
Emirates.AssociationforComputationalLinguistics.
dataset we select high-quality training data that
have beneficial influence on the model. Further-
Baichuan. 2023. Baichuan 2: Open large-scale lan-
more, we enhance the training data diversity by guagemodels. arXivpreprintarXiv:2309.10305.EllaBinghamandHeikkiMannila.2001. Randompro- Wenxiang Jiao, Wenxuan Wang, JT Huang, Xing
jectionindimensionalityreduction: applicationsto Wang, and ZP Tu. 2023. Is chatgpt a good trans-
imageandtextdata. InProceedingsoftheseventh lator? yeswithgpt-4astheengine. arXivpreprint
ACMSIGKDDinternationalconferenceonKnowl- arXiv:2301.08745.
edgediscoveryanddatamining,pages245–250.
PangWeiKohandPercyLiang.2017. Understanding
YihanCao,YanbinKang,ChiWang,andLichaoSun. black-box predictions via influence functions. In
2023. Instructionmining: Whendataminingmeets Internationalconferenceonmachinelearning,pages
large language model finetuning. arXiv preprint 1885–1894.PMLR.
arXiv:2311.15653.
StevenGeorgeKrantzandHaroldRParks.2002. The
LichangChen,ShiyangLi,JunYan,HaiWang,Kalpa
implicitfunctiontheorem: history,theory,andappli-
Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-
cations. SpringerScience&BusinessMedia.
vasan,TianyiZhou,HengHuang,andHongxiaJin.
2023a. Alpagasus: Training a better alpaca with TszKinLam,EvaHasler,andFelixHieber.2022. An-
fewerdata. alyzing the use of influence functions for instance-
specificdatafilteringinneuralmachinetranslation.
Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen,
In Proceedings of the Seventh Conference on Ma-
JinanXu,andJieZhou.2023b. Improvingtranslation
chineTranslation(WMT),pages295–309.Associa-
faithfulnessoflargelanguagemodelsviaaugmenting
tionforComputationalLinguistics.
instructions. arXivpreprintarXiv:2308.12674.
HyungWonChung,LeHou,ShayneLongpre,Barret Jiahuan Li, Shanbo Cheng, Shujian Huang, and Jia-
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi jun Chen. 2024a. Mt-patcher: Selective and ex-
Wang,MostafaDehghani,SiddharthaBrahma,etal. tendableknowledgedistillationfromlargelanguage
2022. Scalinginstruction-finetunedlanguagemodels. models for machine translation. arXiv preprint
arXivpreprintarXiv:2210.11416. arXiv:2403.09522.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and JiahuanLi,HaoZhou,ShujianHuang,ShanboCheng,
KristinaToutanova.2018. Bert: Pre-trainingofdeep andJiajunChen.2024b. Elicitingthetranslationabil-
bidirectionaltransformersforlanguageunderstand- ityoflargelanguagemodelsviamultilingualfinetun-
ing. arXivpreprintarXiv:1810.04805. ingwithtranslationinstructions. Transactionsofthe
AssociationforComputationalLinguistics,12:576–
QianlongDu,ChengqingZong,andJiajunZhang.2023. 592.
Mods: Model-orienteddataselectionforinstruction
tuning. arXivpreprintarXiv:2311.15653. MingLi,YongZhang,ZhitaoLi,JiuhaiChen,Lichang
Chen,NingCheng,JianzongWang,TianyiZhou,and
Xavier Garcia, Yamini Bansal, Colin Cherry, George
JingXiao.2023. Fromquantitytoquality: Boosting
Foster,MaximKrikun,MelvinJohnson,andOrhan
llmperformancewithself-guideddataselectionfor
Firat.2023. Theunreasonableeffectivenessoffew-
instructiontuning. arXivpreprintarXiv:2308.12032.
shot learning for machine translation. In Inter-
national Conference on Machine Learning, pages XiVictoriaLin,TodorMihaylov,MikelArtetxe,Tianlu
10867–10878.PMLR. Wang,ShuohuiChen,DanielSimig,MyleOtt,Na-
manGoyal,ShrutiBhosale,JingfeiDu,etal.2022.
IanGoodfellow,YoshuaBengio,andAaronCourville.
Few-shotlearningwithmultilingualgenerativelan-
2016. Deeplearning. MITpress.
guagemodels. InProceedingsofthe2022Confer-
NamanGoyal,CynthiaGao,VishravChaudhary,Peng- ence on Empirical Methods in Natural Language
Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr- Processing,pages9019–9052.
ishnan,Marc’AurelioRanzato,FranciscoGuzmán,
and Angela Fan. 2022. The flores-101 evaluation James Martens and Roger Grosse. 2015. Optimizing
benchmark for low-resource and multilingual ma- neural networks with kronecker-factored approxi-
chinetranslation. TransactionsoftheAssociationfor matecurvature. InInternationalconferenceonma-
ComputationalLinguistics,10:522–538. chinelearning,pages2408–2417.PMLR.
XiaochuangHan,ByronC.Wallace,andYuliaTsvetkov. OpenAI.2023. Chatgpt: Optimizinglanguagemodels
2020. Explainingblackboxpredictionsandunveil- fordialogue.
ing data artifacts through influence functions. In
Proceedingsofthe58thAnnualMeetingoftheAsso- LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
ciationforComputationalLinguistics,pages5553– CarrollWainwright,PamelaMishkin,ChongZhang,
5563, Online. Association for Computational Lin- SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
guistics. 2022. Training languagemodelsto followinstruc-
tions with human feedback. Advances in Neural
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng InformationProcessingSystems,35:27730–27744.
Zhang,YujiuYang,RuiWang,ZhaopengTu,Shum-
ing Shi, and Xing Wang. 2023. Exploring human- KishorePapineni,SalimRoukos,ToddWard,andWei-
liketranslationstrategywithlargelanguagemodels. JingZhu.2002. Bleu: amethodforautomaticevalu-
arXivpreprintarXiv:2305.04118. ationofmachinetranslation. InProceedingsofthe40thannualmeetingoftheAssociationforComputa- Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie
tionalLinguistics,pages311–318. Zhou. 2023b. Tim: Teaching large language mod-
els to translate with comparison. arXiv preprint
AmyPu,HyungWonChung,AnkurPParikh,Sebastian arXiv:2307.04408.
Gehrmann, and Thibault Sellam. 2021. Learning
compactmetricsformt. InProceedingsofEMNLP. Biao Zhang, Barry Haddow, and Alexandra Birch.
2023a. Prompting large language model for ma-
Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah, chine translation: A case study. arXiv preprint
and Arul Menezes. 2023. Leveraging gpt-4 for arXiv:2301.07069.
automatic translation post-editing. arXiv preprint
arXiv:2305.14878. ShaoleiZhang,QingkaiFang,ZhuochengZhang,Zhen-
grui Ma, Yan Zhou, Langlin Huang, Mengyu Bu,
ShangtongGui,YunjiChen,XilinChen,andYang
RicardoRei,CraigStewart,AnaCFarinha,andAlon
Feng.2023b. Bayling: Bridgingcross-lingualalign-
Lavie. 2020. Comet: A neural framework for mt
evaluation. arXivpreprintarXiv:2009.09025. ment and instruction following through interactive
translationforlargelanguagemodels. arXivpreprint
arXiv:2306.10968.
AndreaSchioppa,PolinaZablotskaia,DavidVilar,and
ArtemSokolov.2022. Scalingupinfluencefunctions.
ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,Jiao
InProceedingsoftheAAAIConferenceonArtificial
Sun,YuningMao,XuezheMa,AviaEfrat,PingYu,
Intelligence,volume36,pages8179–8186.
LiliYu,etal.2023. Lima:Lessismoreforalignment.
arXivpreprintarXiv:2305.11206.
Thibault Sellam, Dipanjan Das, and Ankur P Parikh.
2020. Bleurt: Learningrobustmetricsfortextgener-
WenhaoZhu,HongyiLiu,QingxiuDong,JingjingXu,
ation. arXivpreprintarXiv:2004.04696.
Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian
Huang.2023. Multilingualmachinetranslationwith
StefanoTeso,AndreaBontempelli,FaustoGiunchiglia,
largelanguagemodels: Empiricalresultsandanaly-
andAndreaPasserini.2021. Interactivelabelclean- sis. arXivpreprintarXiv:2304.04675.
ingwithexample-basedexplanations. Advancesin
NeuralInformationProcessingSystems,34:12966–
A InfluenceFunction.
12977.
Influence Function (IF) was introduced to deep
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
learningby(KohandLiang,2017). Intheclassical
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti influencefunctionsetting,wearegiventhetraining
Bhosale, et al. 2023. Llama 2: Open founda- dataset = (x ,y ) n , where x and y are
tion and fine-tuned chat models. arXiv preprint D { i i }i=1 i i
the input and label of the i-th training example.
arXiv:2307.09288.
And the model parameter θ∗ is obtained through
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin empiricalriskminimization:
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drewMDai,andQuocVLe.2021. Finetunedlan- θ∗ : = argmin ( θ) (5)
guagemodelsarezero-shotlearners. arXivpreprint θ L D|
arXiv:2109.01652. n
1 (cid:88)
= argmin L(z θ), (6)
i
MengzhouXia,SadhikaMalladi,SuchinGururangan, θ n |
i=1
Sanjeev Arora, and Danqi Chen. 2024. Less: Se-
lectinginfluentialdatafortargetedinstructiontuning. where z = (x ,y ) denotes the i-th input-label
i i i
arXivpreprintarXiv:2402.04333.
pairandListhelossfunction,e.g.,crossentropy
loss. Influencefunctionmeasurestheimpactofa
Haoran Xu, Young Jin Kim, Amr Sharaf, and
Hany Hassan Awadalla. 2023. A paradigm shift training example z m to the response function by
inmachinetranslation: Boostingtranslationperfor- up-weightingz byεinthetrainingobjective:
m
mance of large language models. arXiv preprint
arXiv:2309.11674. n
1 (cid:88)
θ∗(ε) = argmin L(z θ)+εL(z ). (7)
i m
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing θ n |
i=1
Zong.2023. Bigtrans: Augmentinglargelanguage
modelswithmultilingualtranslationcapabilityover The influence of the training example z on the
m
100languages. arXivpreprintarXiv:2305.18098. response function θ∗ is the derivative of θ∗ with
respecttoεatε = 0:
JialiZeng,FandongMeng,YongjingYin,andJieZhou.
2023a. Improvingmachinetranslationwithlargelan-
guagemodels: Apreliminarystudywithcooperative
dθ∗(ε)(cid:12)
(cid:12)
decoding. arXivpreprintarXiv:2311.02851.
Iθ∗(z m) =
dε
(cid:12)
(cid:12)
. (8)
ε=0Zh En De En
⇒ ⇒
Metric WMT22 FLORES WMT22 FLORES
COMET BLEU BLEURT COMET BLEU BLEURT COMET BLEU BLEURT COMET BLEU BLEURT
pvalue 1.6e-7 1.6e-8 2.0e-6 7.1e-5 3.4e-3 1.6e-5 3.5e-2 1.6e-6 2.1e-1 1.7e-4 1.7e-3 1.8e-3
Table4: Inthistable,wepresentthestaticalanalysisoutcomeofourexperimentalresultsinFigure2. Weconduct
t-testonourG-DIGcomparedwithrandombaselineandpresentthep-value.
UsingtheImplicitFunctionTheorem(Krantzand NotethatcalculatingtheHessianH accurately
θT
Parks,2002)andfirst-orderTaylorapproximation, is time-consuming. Therefore, we follow (Teso
wedefinetheinfluence etal.,2021)toapproximatetheHessianusingem-
piricalFisherInformationMatrix(eFIM).Also,we
Iθ∗(z m) := −H− θ∗1 ∇θ|θ∗L(z m), (9)
furtherusetheKronecker-FactoredApproximate
w trah ie nr ie ngH oθ b∗ je= cti∇ ve2 θL at( D θ|θ =∗) θi ∗s .th Ie nH the isss pia an peo rf
,
t whe
e
C reu dr uv ca etu tr he e( mKF eA mC or) y(M usa ar gte en .s Wan ed imG pro les mse e, n2 t0 t1 h5 e) It Fo
calculationbasedonnngeometry6.
areinterestedintheinfluenceofthetrainingexam-
plez onthemodelbehavioratthetestexample
m B AnnotationGuidelines
z ,i.e.,L(z θ∗). Usingchainrule,wedefinethe
t t
|
influenceofz m onthemodellossatz t: Wehirefull-timetranslatorswhoarefluentinboth
dL(z t
θ∗(ε))(cid:12)
(cid:12)
ChineseandEnglishandtranslatorswhoarefluent
(z m,z t) : = | (cid:12) (10) in both German and English. They are recruited
I dε (cid:12)
ε=0 toconducthumanevaluations. Thetranslatorsare
= L(z )⊤H−1 L(z ).
−∇θ|θ∗ t θ∗ ∇θ|θ∗ m shownthesourcesentenceandtwocandidatetrans-
(11) lations,whicharefromtherandomsubsetmodel
andG-DIGsubsetmodel. Thenthetranslatorsare
Sinceinfinetuningthemodelistrainedwithsmall
requiredtorateonthetranslationqualityfrom1to
learningrateandfewsteps,themodelparameterat
5with1theworstand5thebestandpickthebetter
theendoftrainingisnottheminimizeroftheob-
one. AllourtranslatorsareChinese.
jectivein(5)andtheHessianmightnotbepositive
definitethusisnotinvertible. Thefollowingequiv-
C StatisticalAnalysisofExperimental
alence between non-converged model parameter
Results
andregularizedempiricalriskminimizerprovides
amethodformodifyingIF: Weconductstatisticalanalysisonourexperimental
results as shown in Table 4. Specifically, we use
Lemma1 (Goodfellowetal.,2016)Assumingthat
t-test to analyze the results of our G-DIG across
themodelistrainedviaT stepsofStochasticGradi-
variousdatasetsizesfromFigure2comparedwith
entDescent(SGD)andthelearningrateη isfixed
random baseline. Our experimental results have
duringtraining. Thenthemodelparameteratthe
demonstratedstatisticalsignificance,asevidenced
endoftrainingθ isequaltothemodelparameter
T
bythep-valueslessthan0.05inalmostallcases.
obtainedthroughregularizedempiricalriskmini-
mizationθˆ= argmin ( θ)+λ θ 2whenthe
θ L D| 2∥ ∥2
followingtwoconditionsaresatisfied:
I ηΛ I, (12)
| − | ⪯
(I ηΛ)T = (Λ+λI)−1λ, (13)
−
whereΛisthediagonalmatrixintheeigendecom-
positionof H = QΛQ⊤.
θ∗
HenceweapproximateoftheHessianoffinetuning
asH +λI. Forfinetuning,wehave:
θT
(z ,z ) = L(z )⊤(H +λI)−1 L(z ).
I
m t −∇θ t θT ∇θ m
(14) 6https://github.com/tfjgeorge/nngeometry