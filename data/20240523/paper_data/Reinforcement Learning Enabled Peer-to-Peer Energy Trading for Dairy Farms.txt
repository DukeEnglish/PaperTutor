Reinforcement Learning Enabled Peer-to-Peer
Energy Trading for Dairy Farms
Mian Ibad Ali Shah, Enda Barrett, and Karl Mason
School of Computer Science, University of Galway; Galway, Ireland, H91 FYH2
{m.shah7,enda.barrett,karl.mason}@universityofgalway.ie
Abstract. Farmbusinessesareincreasinglyadoptingrenewablestoen-
hance energy efficiency and reduce reliance on fossil fuels and the grid.
This shift aims to decrease dairy farms’ dependence on traditional elec-
tricitygridsbyenablingthesaleofsurplusrenewableenergyinPeer-to-
Peer markets. However, the dynamic nature of farm communities poses
challenges, requiring specialized algorithms for P2P energy trading. To
address this, the Multi-Agent Peer-to-Peer Dairy Farm Energy Simula-
tor(MAPDES)hasbeendeveloped,providingaplatformtoexperiment
with Reinforcement Learning techniques. The simulations demonstrate
significantcostsavings,includinga43%reductioninelectricityexpenses,
a 42% decrease in peak demand, and a 1.91% increase in energy sales
compared to baseline scenarios lacking peer-to-peer energy trading or
renewable energy sources.
Keywords: Peer-to-peerEnergyTrading·Multi-agentSystems·Dairy
Farming · Reinforcement Learning · Sustainability
1 Introduction
In accordance with the European climate law, EU member states are mandated
to reduce greenhouse gas emissions by a minimum of 55% by 2030, with the
ultimate aim of achieving climate neutrality within the EU by 2050 [3]. How-
ever,whenconsideringdairyfarming,particularlyincountrieslikeIrelandwhere
a substantial portion of the economy relies heavily on dairy product exports,
milk production involves significant energy consumption, giving rise to concerns
regarding carbon dioxide (CO ) emissions. In Ireland, dairy cows consume an
2
averageof350kilowatt-hours(kWh)ofelectricityannually[25].Recentincreases
inenergypriceshaveraisedelectricityexpensesforfarms,especiallyduringpeak
utility grid hours [24]. Addressing this challenge requires the development of an
artificial intelligence (AI) system to ensure the future sustainability of the dairy
industryandaimedatreducingcarbonemissionsandpeakgriddemand,offering
both environmental and financial benefits for dairy farms.
Proc. of the Main Track of 22nd International Conference on Practical Applications
ofAgentsandMulti-AgentSystems,26th-28thJune,2024,https://www.paams.net/.
4202
yaM
12
]IA.sc[
1v61721.5042:viXra2 Shah et al.
Multi-agent systems (MAS) consist of autonomous agents collaborating to
achieve common goals, making decisions with limited information and commu-
nicatingwithimmediateneighbours[1].MASshowspromiseinaddressingmicro-
gridchallengesbyprovidingscalable,decentralizedcontrol[5].Performancemod-
els optimize profit for prosumers and meet end-user needs in energy-sharing re-
gions (ESRs). Peer-to-peer (P2P) energy trading involves sharing energy within
microgrids before trading with retailers [31].
P2P energy trading is still in its early stages of development but researchers
have explored different approaches using centralized, decentralized, and dis-
tributed trading markets. Centralized markets face computational burdens and
privacy concerns, while decentralized models offer direct P2P transactions [23].
Distributed approaches combine the benefits of both, and often use auction-
based mechanisms like the Double Auction (DA) market to clear the energy
trading market [17].
1.1 Centralized Trading
Incentralizedtrading,Qiuetal.proposedthePS-MADDPGalgorithm,address-
ing prosumer heterogeneity to enhance energy trading by allowing agents in the
same cluster to share experiences and learning policies [19]. Zhang et al. devel-
oped a MAS involving energy suppliers, employing non-cooperative games to
achieve dynamic pricing and reduce peak demand [29]. Yang et al. introduced a
multi-energysharingstrategyoptimizingenergytradingforbetterfairnessusing
linear programming [27].
1.2 Decentralized Trading
Some researchers have pursued decentralized approaches. Guimaraes et al. uti-
lized an ABM model to examine the economic benefits of P2P trading [9]. Zhou
etal.proposedadecentralizedmulti-agentmodelemployingQlearning,Maulti-
agent Reinforcement Learning (MARL), and Mid-Market Rate (MMR) to max-
imize revenue for each participant agent [30]. Elkazaz et al. presented a hierar-
chical, decentralized approach considering factors like battery storage and PV
sizetoexaminetheireffectonthefinancialbenefitforthecommunity[6].Lianyi
Zhu investigated market-based and price-based optimal trading mechanisms us-
ing blockchain for maximized profits in a microgrid [32]. Khorasany et al. pro-
poseddecentralizedP2Penergytradingsystemsprioritizingdataprotectionand
financial benefits for participants [12].
1.3 Distributed Trading
Ourfocusliesinthedistributedapproach.Distributedmarketsrequirelessinfor-
mation from peers and offer scalability and greater autonomy to customers [11].
Shah et al. discussed the features of distributed P2P energy-sharing markets,
blending centralized and decentralized aspects, enhancing customer privacy andRL Enabled P2P Energy Trading for Dairy Farms 3
autonomy [21]. Various studies categorized under distributed systems aimed at
achievingfinancialbenefit,scalability,andtheprivacyofprosumers.Studieslike
those by Long et al. [14] and Okwuibe et al. Qiu et al. [17] and Charbonnier et
al. [2] proposed scalable solutions for P2P energy trading systems. Studies by
Qiuetal.[18]andLinetal.[13]addressedprivacyandsecurityconcernsinP2P
energy trading.
1.4 Our Approach
The proposed models have shown promise in improving efficiency, data privacy,
scalability,andcostsavingsinP2Penergysystems.Ourresearchthoroughlyex-
amined the literature on P2P energy trading, identifying crucial success factors.
Notably, work in this domain-specific to dairy farms has not yet been explored,
underscoringthenoveltyandimportanceofourstudy.Ourinvestigationfocuses
on the unique characteristics of dairy farms, necessitating a specialized MAS-
basedapproach.Oursystemiscustomizedtooptimizeenergytradingconsidering
dairyfarm-specificpatterns.Byintegratingfinancialbenefits,dataprivacy,scal-
ability, and transparent auction mechanisms, our simulation provides a tailored
solution for dairy farms.
This study builds upon our previously proposed simulator outlined in [20],
which primarily relied on a rule-based multi-agent system. However, this paper
introduces Q-learning into the simulator as a singular agent, alongside the rule-
basedagents,toevaluatewhethertheintegrationofreinforcementlearning(RL),
particularly Q-learning, enhances the outcomes. While newer RL algorithms of-
fer advancements, Q-learning remains robust and well-established, making it
ideal for our research due to its simplicity, interpretability, and effectiveness in
addressing our specific problem domain. The results section reveals that the
inclusion of a Q-learning trained agent alongside rule-based agents has yielded
improvements in key metrics, including reductions in electricity procurement
and peak demand, as well as an increase in surplus electricity sales, compared
tothesolelyrule-basedsimulator.Thisresearchcontributestotheadvancement
of:
– Development of a tailored Q-learning agent for P2P energy trading in dairy
farming.
– Examinationofincreasedrevenue,decreasedpeakdemand,andreducedelec-
tricity costs achieved through integrating an RL agent within a rule-based
agent environment.
2 Background
2.1 Rule-based Systems
Theessenceofarule-basedsystemliesinitscollectionofIF-THENrules,which
encapsulates domain knowledge provided by experts or gleaned from historical4 Shah et al.
data, alongside an inference engine enabling the system to generate outputs
based on these rules [28].
A proficient rule-based system holds promise for achieving a favourable bal-
ancebetweenaccuracy,efficiency,andexplainability.Despitewidespreadutiliza-
tion of off-the-shelf rule-based systems across diverse domains, challenges per-
sist due to opaque rule relations, efficiency issues in inference, ineffective search
strategies, and constraints in knowledge acquisition, limiting their market via-
bility and posing ongoing challenges [8].
2.2 Reinforcement Learning
Reinforcement learning is a branch of machine learning focused on training
agents to make decisions based on rewards and penalties. In RL, agents interact
withanenvironment,receivefeedback,andadjusttheiractionstoachievespecific
objectives.Thegoalistomaximizecumulativerewardsbylearningapolicythat
dictates actions based on observed states. RL algorithms can be categorized as
either value-based like Q-learning algorithm, aiming to learn the optimal value
function, or policy-based like REINFORCE and Actor-Critic, aiming to learn
theoptimalpolicydirectly.RLisinstrumentalindevelopingautonomousagents
capable of learning from experience, refining decision-making processes, and en-
hancing their behaviour over time [4].
2.3 Q-learning
Q-learning, a widely adopted reinforcement learning algorithm, is renowned for
its simplicity and efficacy in solving Markov decision processes (MDPs) without
requiring a model of the environment [26]. In the realm of P2P energy trading,
Q-learning offers a promising avenue for optimizing energy transactions and re-
sourceallocationamongdistributedparticipants.Byleveragingpastexperiences
and rewards, agents can learn optimal strategies for buying, selling, and storing
energy, thereby enhancing the efficient utilization of renewable energy resources
andgridinfrastructure[21].Throughexplorationandexploitation,agentsadapt
todynamicmarketconditions,maximizingenergytradingprofitswhileminimiz-
ingrelianceoncentralizedutilitygrids.AdvancedvariantsofQ-learning,suchas
Deep Q-Networks (DQN) and Prioritized Experience Replay, further bolster its
effectiveness and scalability in P2P energy trading scenarios [15]. By harnessing
Q-learning and its derivatives, stakeholders in the energy sector can develop in-
telligent systems that optimize energy management, reduce costs, and promote
sustainability in decentralized energy markets.
In Q-learning, action-value function Q(s,a), represents the expected cumu-
lativerewardfortakingactionsaiterativelyinstatesandfollowingtheoptimal
policy thereafter. The update equation is as follows:
(cid:16) (cid:17)
Q(s,a)←(1−α)·Q(s,a)+α· r+γ·maxQ(s′,a′)
a′RL Enabled P2P Energy Trading for Dairy Farms 5
Here, α denotes the learning rate, r signifies the immediate reward, γ rep-
resents the discount factor, and s′ denotes the next state. The policy for action
selection, often ϵ-greedy, balances exploration and exploitation.
3 System Design
The research aims to present a MAS algorithm incorporating both rule-based
and Q-learning RL agents to facilitate distributed P2P energy trading among
dairy farms. The Q-learning agent is trained independently in its environment.
Once the Q-table is developed, it is integrated into the pre-built rule-based sim-
ulator. By employing MAS, the proposed approach enables dairy farms to trade
surplus renewable energy resources with neighbouring farms, thereby reducing
their dependence on the utility grid and fostering energy self-sufficiency. Figure
1 depicts the process flow of the simulation model, which is elaborated upon in
subsequent sections outlining each step in detail.
Fig.1. Process Flow for MAPDES P2P Energy Trading Simulator
3.1 Dataset and Infrastructure
ThestudypresentsaPython-basedsimulationmodelforcomputingenergygen-
eration, consumption, and storage on farms using RE like solar and wind. The
model allows for battery storage and enables energy trading within a P2P net-
workorwiththegrid.ThefarmloaddataoriginatesfromKhaleghyetal.’sdairy
farm load modelling algorithm [10]. PV and wind generation data, varying with
farm size, are sourced from SAM for County Dublin, Ireland, and jgmill et al.
[16],[7]. PV system capacities range from 10 kW to 20 kW, with a fixed 10 kW
turbine capacity for wind power. Tesla PowerWall features are considered for
farms with battery storage [22].
Theinfrastructuresupportsscalablefarmnumbers,eachwithPV,windsys-
tems, and battery storage. All farms act as prosumers, connected to the grid6 Shah et al.
for energy exchange based on Internal Selling Price (ISP) and Internal Buying
Price(IBP),determinedbySupplyDemandRatio(SDR)hourly.Thesimulator
encompasses load and battery management, ISP/IBP calculation, and market
clearing through auction and energy trading, adaptable for various durations
[10].
Acentralauctioneer/advisorsetsISPandIBPusingSDRhourly,facilitating
fair energy exchange within the P2P network. The infrastructure operates on a
distributedpeer-to-peermodel,withmarketclearancecontrolledbyDoubleAuc-
tion (DA) algorithm. Participants independently manage their load, generation,
andbatteryusagewithoutdisclosingdata,ensuringefficientenergydistribution.
Agents include farms, batteries, renewable energy sources, auctioneers, and en-
ergy traders, collaborating for effective energy management. Load and battery
management is divided into rule-based and Q-learning-based approaches.
3.2 Rule-Based Training
The rule-based load and battery management, along with all the related algo-
rithmsandequations,havealreadybeendiscussedindetailinourpreviouswork
[20]. However, a summarized discussion has been included in this paper.
Input parameters include energy consumption, renewable source capacity,
and battery storage. Energy prices are determined based on time of day, renew-
able source generation, and initial battery storage. The model calculates farm
energy consumption and checks if renewable resources can meet the demand. If
not, the farm may purchase energy from the grid or other farms via a P2P net-
work.Themodelsimulatesvariousvariablessuchastotalenergygeneration,RE
resourceavailability,batterystoragestatus,andenergypricestomanageenergy
generation, consumption, and storage effectively. It also considers scenarios like
RE resource availability and battery storage state to determine energy trading
with the grid or other farms. The electricity price for purchasing from the grid
is categorized into night, day, and peak rates, depending on the location of the
farms. The model includes equations for battery management, energy buy/sell
decisions, and electricity pricing based on the Supply Demand Ratio (SDR).
MarketclearingandenergytradingarefacilitatedusingaDoubleAuction(DA)
algorithm.TheDAmarketefficientlymatchesmultiplebuyersandsellersforen-
ergytrading.Itisawidelyusedmechanismfortradingvariouscommodities,such
asstocksandelectricity.TheDAmarketoperatesduringafixedauctionperiod,
withthisresearchusinganhourlyresolution.Traderssubmittheirbids/offersat
the start of the auction period based on the ISP and IBP, in a controlled man-
ner. The auctioneer then clears the market and publishes the outcomes (trading
pricesandquantities)attheendofeachperiod[17].Thisensuresefficientenergy
distribution and fair market outcomes.
3.3 Q-learning based Training
TheQ-learningalgorithmemployedinthisstudyunderwenttrainingover300,000
episodeswithspecificparameters:alearningrateof0.1,adiscountfactorof0.99,RL Enabled P2P Energy Trading for Dairy Farms 7
an exploration rate starting at 1.0 and decaying by 0.99. The observation space
encompasses four features: the farm’s load profile, renewable energy generation
profile, battery percentage, and the current hour. The action space comprised
nineactions:buy,sell,self-consumptiononly,batterychargeandsell,chargeand
buy, battery discharge and sell, discharge and buy, self-utilization and charge,
andself-utilizationanddischarge.Rewardsweredeterminedbasedonelectricity
cost from grid tariff, RE generation, farm load, and peak hours. Correct ac-
tions yielded positive rewards corresponding to the amount of electricity sold or
bought, while incorrect actions led to negative rewards. The rewards converged
at around the 60000th episode, as seen in Figure 2. The convergence line shown
in this figure is an average value line of rewards per episode with a window size
of 200 (used to smooth the learning curve by averaging recent experiences).
Fig.2. Q-learning Algorithm Convergence
4 Results and Discussions
Weevaluatedourapproachbysimulating10dairyfarmsoveroneyear,compris-
ing one Q-learning-based agent and nine rule-based agents. Using hourly time
steps, we assessed the simulation results based on four key metrics:
(i) Comparison of energy purchased by farms without RE resources and P2P
vs with RE resources and P2P.
(ii) Electricity purchase among RE-equipped farms (P2P vs Non-P2P).
(iii) Electricity sold by RE-equipped farms (P2P vs non-P2P).
(iv) Peak hours grid demand by RE-equipped farms (P2P vs non-P2P).8 Shah et al.
Fig.3. Total Electricity Cost: P2P & RE vs Non-P2P & No RE
Figure 3 illustrates the contrast in total energy procurement between farm
communities employing RE resources and engaging in P2P energy trading, ver-
sus those without RE resources and P2P trading. Notably, there’s a 70.44%
reduction in electricity purchases for a farm community when RE resources and
P2P trading are utilized.
Fig.4. Total Electricity Cost: P2P vs Non-P2P (RE-equipped)RL Enabled P2P Energy Trading for Dairy Farms 9
Furthermore, Figure 4 compares total energy purchases between farm com-
munities, all using RE resources, but with and without P2P energy trading.
Here, we observe a 4.13% decrease in electricity procurement with P2P trading,
despite both scenarios having RE resources. While this percentage reduction
may seem modest, it’s essential to consider the significant impact during peak
hours,asdepictedinFigure5.P2Penergytradingnotablyreducesgridreliance
during peak hours by a remarkable 87.84%.
Moreover, the adoption of P2P energy trading leads to a 1.91% increase in
revenue generated from electricity sales, as demonstrated in Figure 6.
Fig.5. Electricity demand of Farm Community from the grid in peak hours (5 pm to
7 pm): P2P vs Non-P2P
When comparing the performance metrics between our approach, which in-
tegrates Q-learning-based agents with rule-based agents, and the results from
a purely rule-based approach, our method shows promising outcomes. Specifi-
cally,ourapproachyieldslowerelectricitypurchasecosts,reducedpeakdemand,
and enhanced revenue from electricity sales, as illustrated in Table 1. This in-
dicates that Q-learning outperforms the rule-based approach in dairy farming
P2P energy trading.
5 Conclusion
This paper introduces a comprehensive framework designed to optimize energy
management within dairy farming communities by integrating Q-learning-based
agents with rule-based systems. Through extensive simulations and analysis, we
have showcased the effectiveness of our approach, achieving remarkable reduc-
tionsinelectricitypurchasecosts(by70.44%),substantialminimizationofpeak10 Shah et al.
Fig.6. Total Daily Revenue from Excess Electricity Sold using P2P vs non-P2P
Table 1. MAPDES Results for Different Locations: Finland vs Ireland
Rule-based Combined
(Q-learning
& Ruled)
Electricity Cost with no RE (€) 51710 51710
Electricity Cost with RE no P2P (€) 21066 15284
Electricity Cost with P2P and RE (€) 20497 14653
ElectricityCost%reductionwithP2PandREvs59.26 70.44
w/o RE (%)
ElectricityCost%reductionwithP2PandREvs2.69 4.13
with RE only (%)
Electricity Revenue without P2P (€) 46032 46022
Electricity Revenue with P2P (€) 46612 46904
Electricity Revenue % increase P2P vs no P2P1.25 1.91
(%)
Peak Demand without P2P (kW) 27837 27837
Peak Demand with P2P (kW) 8618 3382
peak Demand % reduction using P2P vs no P2P69.03 87.84
(%)
demand(by87.84%),andnotablemaximizationofrevenuefromelectricitysales
(by 1.91%). By combining reinforcement learning with traditional rule-based
strategies, our method presents a versatile and efficient solution for sustainable
energy management in agricultural contexts. Furthermore, our findings under-
score the significant advantages of peer-to-peer energy trading, highlighting its
potential to revolutionize the dairy farming sector. Looking ahead, future re-
search avenues may explore enhancements to the algorithm, including the uti-RL Enabled P2P Energy Trading for Dairy Farms 11
lizationofapurelyreinforcementlearning-basedmulti-agentsystem,andextend
itsapplicationtootheragriculturaldomains,therebyfosteringmoreresilientand
eco-friendly farming practices.
Acknowledgments. Thispublicationhasemanatedfromresearchconductedwiththe
financialsupportofScienceFoundationIrelandunderGrantnumber[21/FFP-A/9040].
Disclosure of Interests. AuthorhasreceivedaresearchgrantfromScienceFounda-
tion Ireland.
References
1. Adjerid,H.,Maouche,A.R.:Multi-agentsystem-baseddecentralizedstateestima-
tionmethodforactivedistributionnetworks.Computers&ElectricalEngineering
86, 106652 (2020)
2. Charbonnier, F., Morstyn, T., McCulloch, M.D.: Scalable multi-agent reinforce-
ment learning for distributed control of residential energy flexibility. Applied En-
ergy 314, 118825 (2022)
3. Commission, E.: Climate change: what the eu is doing. https://www.consilium.
europa.eu/en/policies/climate-change/#2050/(2024),[AccessedonMarch15,
2024]
4. Dadman, S., Bremdal, B.A.: Multi-agent reinforcement learning for structured
symbolicmusicgeneration.In:InternationalConferenceonPracticalApplications
of Agents and Multi-Agent Systems. pp. 52–63. Springer (2023)
5. Elena,D.O.,Florin,D.,Valentin,G.,Marius,P.,Octavian,D.,Catalin,D.:Multi-
agent system for smart grids with produced energy from photovoltaic energy
sources. In: 2022 14th International Conference on Electronics, Computers and
Artificial Intelligence (ECAI). pp. 1–6. IEEE (2022)
6. Elkazaz, M., Sumner, M., Thomas, D.: A hierarchical and decentralized energy
managementsystemforpeer-to-peerenergytrading.AppliedEnergy 291,116766
(2021)
7. Fraukewiese,sjpfenninger,jgmill:Load,windandsolar,pricesinhourlyresolution.
https://data.open-power-system-data.org/time_series/latest/ (2020), ac-
cessed on January 15, 2024
8. Fürnkranz, J., Kliegr, T., Paulheim, H.: On cognitive preferences and the plausi-
bility of rule-based models. Machine Learning 109(4), 853–898 (2020)
9. Guimarães,D.V.,Gough,M.B.,Santos,S.F.,Reis,I.F.,Home-Ortiz,J.M.,Catalão,
J.P.: Agent-based modeling of peer-to-peer energy trading in a smart grid envi-
ronment. In: 2021 IEEE International Conference on Environment and Electrical
Engineering and 2021 IEEE Industrial and Commercial Power Systems Europe
(EEEIC/I&CPS Europe). pp. 1–6. IEEE (2021)
10. Khaleghy, H., Wahid, A., Clifford, E., Mason, K.: Modelling electricity con-
sumption in irish dairy farms using agent-based modelling. arXiv preprint
arXiv:2308.09488 (2023)
11. Khorasany,M.,Mishra,Y.,Ledwich,G.:Marketframeworkforlocalenergytrad-
ing:Areviewofpotentialdesignsandmarketclearingapproaches.IETGeneration,
Transmission & Distribution 12(22), 5899–5908 (2018)12 Shah et al.
12. Khorasany, M., Mishra, Y., Ledwich, G.: A decentralized bilateral energy trad-
ing system for peer-to-peer electricity markets. IEEE Transactions on industrial
Electronics 67(6), 4646–4657 (2019)
13. Lin, W.T., Chen, G., Zhou, X.: Distributed carbon-aware energy trading of vir-
tual power plant under denial of service attacks: A passivity-based neurodynamic
approach. Energy 257, 124751 (2022)
14. Long, C., Wu, J., Zhou, Y., Jenkins, N.: Peer-to-peer energy sharing through a
two-stage aggregated battery control in a community microgrid. Applied energy
226, 261–276 (2018)
15. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., et al.: Human-level
control through deep reinforcement learning. nature 518(7540), 529–533 (2015)
16. National Renewable Energy Lab (NREL): System advisor model (sam). https:
//sam.nrel.gov (2017), version 2017.9.5
17. Qiu, D., Wang, J., Dong, Z., Wang, Y., Strbac, G.: Mean-field multi-agent rein-
forcement learning for peer-to-peer multi-energy trading. IEEE Transactions on
Power Systems (2022)
18. Qiu, D., Wang, J., Wang, J., Strbac, G.: Multi-agent reinforcement learning for
automated peer-to-peer energy trading in double-side auction market. In: IJCAI.
pp. 2913–2920 (2021)
19. Qiu,D.,Ye,Y.,Papadaskalopoulos,D.,Strbac,G.:Scalablecoordinatedmanage-
ment of peer-to-peer energy trading: A multi-cluster deep reinforcement learning
approach. Applied Energy 292, 116940 (2021)
20. Shah,M.I.A.,Wahid,A.,Barrett,E.,Mason,K.:Amulti-agentsystemsapproach
forpeer-to-peerenergytradingindairyfarming.arXivpreprint:2310.05932(2023)
21. Shah, M.I.A., Wahid, A., Barrett, E., Mason, K.: Multi-agent systems in peer-to-
peerenergytrading:Acomprehensivesurvey.EngineeringApplicationsofArtificial
Intelligence 132, 107847 (2024)
22. Tesla: How powerwall works. https://www.tesla.com/support/energy/
powerwall/learn/how-powerwall-works (2023), [Online; Accessed March
20, 2023]
23. Umer,K.,Huang,Q.,Khorasany,M.,Afzal,M.,Amin,W.:Anovelcommunication
efficient peer-to-peer energy trading scheme for enhanced privacy in microgrids.
Applied Energy 296, 117075 (2021)
24. Upton, J., Murphy, M., De Boer, I., Koerkamp, P.G., Berentsen, P., Shalloo, L.:
Investmentappraisaloftechnologyinnovationsondairyfarmelectricityconsump-
tion. Journal of dairy science 98(2), 898–909 (2015)
25. Upton, J., Humphreys, J., Koerkamp, P.G., French, P., Dillon, P., De Boer, I.J.:
Energy demand on dairy farms in ireland. Journal of dairy science 96(10), 6489–
6498 (2013)
26. Watkins, C.J., Dayan, P.: Q-learning. Machine learning 8, 279–292 (1992)
27. Yang,J.,Xu,W.,Ma,K.,Li,C.:Athree-stagemulti-energytradingstrategybased
on p2p trading mode. IEEE Transactions on Sustainable Energy 14(1), 233–241
(2022)
28. Yang, L.H., Liu, J., Ye, F.F., Wang, Y.M., Nugent, C., Wang, H., Martínez, L.:
Highly explainable cumulative belief rule-based system with effective rule-base
modeling and inference scheme. Knowledge-Based Systems 240, 107805 (2022)
29. Zhang, M., Eliassen, F., Taherkordi, A., Jacobsen, H.A., Chung, H.M., Zhang,
Y.:Energytradingwithdemandresponseinacommunity-basedp2penergymar-
ket. In: 2019 IEEE International Conference on Communications, Control, andRL Enabled P2P Energy Trading for Dairy Farms 13
Computing Technologies for Smart Grids (SmartGridComm). pp. 1–6 (2019).
https://doi.org/10.1109/SmartGridComm.2019.8909798
30. Zhou,H.,Erol-Kantarci,M.:Decentralizedmicrogridenergymanagement:Amulti-
agent correlated q-learning approach. In: 2020 IEEE International Conference on
Communications, Control, and Computing Technologies for Smart Grids (Smart-
GridComm). pp. 1–7. IEEE (2020)
31. Zhou,Y.,Wu,J.,Long,C.:Evaluationofpeer-to-peerenergysharingmechanisms
basedonamultiagentsimulationframework.Appliedenergy222,993–1022(2018)
32. Zhu, L.: Market-based versus price-based optimal trading mechanism design in
microgrid. Computers and Electrical Engineering 100, 107904 (2022)