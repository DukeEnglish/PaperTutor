Personalized Residuals for Concept-Driven Text-to-Image Generation
CusuhHam* MatthewFisher JamesHays
GeorgiaInstituteofTechnology AdobeResearch GeorgiaInstituteofTechnology
cusuh@gatech.edu matfishe@adobe.com hays@gatech.edu
NicholasKolkin YuchenLiu RichardZhang TobiasHinz
AdobeResearch AdobeResearch AdobeResearch AdobeResearch
kolkin@adobe.com yuliu@adobe.com rizhang@adobe.com thinz@adobe.com
Abstract personalizationapproachesdifferinwhichparametersthey
trainandwhethertheyarespecifictoasingleconcept(i.e.,
We present personalized residuals and localized theyneedtobeseparatelytrainedforeachnewconcept)or
attention-guided sampling for efficient concept-driven can generalize to new concepts without retraining. To en-
generation using text-to-image diffusion models. Our ablepersonalizationofarbitraryconcepts,onecanfinetune
method first represents concepts by freezing the weights of the model’s parameters [24] or its inputs [7] directly such
apretrainedtext-conditioneddiffusionmodelandlearning that it can reconstruct the training data. These approaches
low-rankresidualsforasmallsubsetofthemodel’slayers. can be applied to any kind of concepts, but the finetun-
The residual-based approach then directly enables appli- ing needs to be done on a per-concept basis and different
cation of our proposed sampling technique, which applies parameters need to be stored for each. Other approaches
the learned residuals only in areas where the concept trainanencoderspecifictoaparticulardomain(e.g.,faces)
is localized via cross-attention and applies the original andfinetunethediffusionmodeloncetousetheencoder’s
diffusion weights in all other regions. Localized sampling embeddingstoreconstructspecificconceptswithinthatdo-
thereforecombinesthelearnedidentityoftheconceptwith main [8, 25, 33]. The advantage of the latter approach is
the existing generative prior of the underlying diffusion thatitdoesnotrequireretrainingforeveryconceptandcan
model. We show that personalized residuals effectively insteadbeusedtoinstantlygeneratenewconceptsfromthe
capturetheidentityofaconceptin∼3minutesonasingle givendomain. However,thisapproachislimitedtoasingle
GPU without the use of regularization images and with domainandrequiresalargedatasettotraintheencoder.
fewer parameters than previous models, and localized
Ourapproachfollowstheformersetting,i.e.,itfinetunes
sampling allows using the original model as strong prior
the model’s parameters for each concept so that there are
forlargepartsoftheimage.
no constraints on the domain (see Figure 1 for examples
usingourproposedmethod). Themainchallengesofopen-
domainapproachesistheneedforregularizationtomitigate
1.Introduction forgettingofconceptslearnedinthemodel’soriginaltrain-
ing,andthecomputationaloverheadinfinetuninganewset
Large-scale text-to-image diffusion models have demon-
ofparametersforeachconcept. Themostcommonregular-
strated the ability to generate high-quality images that fol-
izationapproachistouseimagesfromthesamedomainas
low the constraints of the input text [21, 22, 26]. How-
thetargetconceptwiththereferenceimagesduringthefine-
ever, these models do not inherently encode any informa-
tuning of parameters. The choice of regularization images
tion about the identity of a specific concept, thus limiting
affectsthequalityofthefinaloutputsand,assuch,isusually
the control over specifying a particular instance to appear
model-, training-, and sometimes even concept-dependent.
inthegeneratedimage. Toaddressthis,recentapproaches
Finally,toaddressthelargeoverheadoffinetuningawhole
propose techniques to personalize these models such that
new model for each concept, many approaches only fine-
they can generate specific concepts in novel environments
tune a subset of parameters (e.g., attention layers weights
andstyles.
[16])ortheinputtothetext-to-imagemodel(e.g., thetext
Given a set of images depicting the desired concept,
embeddingrepresentingaspecificconcept[7]).
*WorkperformedduringaninternshipatAdobeResearch. Our approach further reduces the number of learnable
4202
yaM
12
]VC.sc[
1v87921.5042:viXraConcept “A rusty V* toy gnome in a Concept “V* plushie oil painting Concept “V* cat wearing sunglasses”
post-apocalyptic landscape” Ghibli inspired”
Concept “V* action figure riding a Concept “The V* lighthouse Concept “A V* car resting beneath the
motorcycle” surrounded by a tranquil lake” cherry blossoms in full bloom”
Figure1.(Top)Givenasetofreferenceimages,welearnpersonalizedresidualsforasubsetofapretraineddiffusionmodel’sweightsfor
efficientconcept-driventext-to-imagegeneration. (Bottom)Theresidualscanbecombinedwithourproposedlocalizedattention-guided
(LAG)sampling,whichleveragesthecross-attentionmapsfromthediffusionmodelstolocalizetheapplicationoftheresidualsanduses
theoriginal,unchanged,diffusionmodelforgeneratingeverythingelse.
parameters and does not rely on regularization images. the target concept and the rest of the image, respectively.
While most approaches focus on finetuning the key and To achieve this, we use the attention maps from the cross-
valueweightsofthecross-attentionlayers,weinsteadpre- attention layers of the diffusion model at each timestep to
dict a low-rank residual [14] to the weights of the output predict the location of the concept in the generated image
projectionconvlayeraftereachcross-attentionlayer. This and then apply the features, produced using the personal-
allowsustofinetuneevenfewerparameters(about∼0.1% izedresiduals,onlyinthepredictedregionsuchthattherest
ofthebasemodel)thanpreviousapproaches. Furthermore, oftheimage(e.g.,backgroundandotherobjects)isgener-
wefindthatthisapproachdoesnotrequireanyregulariza- ated by the original model. Thus, we ensure that we do
tionimageswhichmakesourapproachbothsimpler,since notlosethecapabilityofgeneratingspecificbackgroundsor
wedonotneedtofindappropriatestrategiestoobtainreg- unrelatedobjectsduetooverfitting. Furthermore,thissam-
ularization images, and faster, since we do not need addi- pling approach does not require any additional training or
tionaltrainingiterationsforlearningfromtheregularization data, anddoesnotincreasesamplingtimeasnoadditional
images. We also show that the choice of macro class for modelevaluationsareneeded.
personalizing a given image affects the performance, e.g.,
using“car”insteadof“Lamborghini”asthemacroclassin We evaluate our approach and sampling technique on
Figure1affectsthequalityoftheoutcome(seesupplemen- theCustomConcept101dataset[16],whichwasspecifically
tary). Based on this, removing the need for regularization designed to evaluate personalization approaches. We use
images removes an additional dependency and decreases CLIP and DINO scores to evaluate the text-image align-
theneedformanualselections. ment (i.e., how well the personalized model can generate
theconceptinnovelscenesandenvironments)andidentity
Additionally, many personalization approaches struggle preservationofthepersonalizedmodel(i.e.,howwellitcan
torenderspecificbackgroundsoraddnewobjectsoftendue generatethedesiredconcept). Wealsoperformauserstudy
to some degree of overfitting to the target concept. For toevaluatehumanpreferencefortext-imagealignmentand
these scenarios, we propose a novel localized attention- identitypreservation. Ourresultsshowthatourmodelper-
guided (LAG) sampling scheme, which allows us to use forms on par or better compared to current state-of-the-art
the finetuned residuals with the original model to generate baselineswhileusingsignificantlyfewerparameters,notre-
slaudiseR
dezilanosreP
slaudiseR
dezilanosreP
gnilpmaS
GAL
+lyingonregularizationimages,andbeingfastertotrain. ages by manipulating the cross-attention map correspond-
To summarize, our key contributions are a novel and ingtotheeditingtarget.Similarly,[2]performseditsonex-
more efficient low-rank personalization approach for text- istingimagesalbeitthroughinstructionsandmodifications
to-imagediffusionmodelsthatworksforarbitrarydomains withinself-attentionlayers.
and concepts, uses fewer parameters than previous ap-
3.Approach
proaches, does not rely on regularization images and is,
therefore, faster and simpler to train. We also introduce a
Our method consists of two components: 1) Personal-
novellocalizedattention-guided(LAG)samplingapproach
ized residuals, which encode the identity of a given con-
that allows us to flexibly combine the original pretrained
cept through a set of learned offsets applied to a subset of
and the finetuned model on the fly to generate different
weights within a pretrained text-to-image diffusion model,
parts of the image, without increasing the sampling time
and 2) Localized attention-guided (LAG) sampling, which
and without requiring additional training or user inputs.
leveragesattentionmapstolocalizewheretheresidualsare
Our user study and quantitative evaluations show that our
applied,essentiallyallowingasingleimagetobeefficiently
methodperformscomparablyorbetterthanotherbaselines,
generatedbyleveragingboththebasediffusionmodeland
andourproposedsamplingapproachcanaddresschallenges
thepersonalizedresiduals.
withcertaintypesofrecontextualizationscenarios,suchas
backgroundchanges. 3.1.Preliminaries
Diffusionmodels. Diffusionmodels[13]consistofafixed
2.RelatedWork
forward noising process that gradually adds noise to an
2.1.Personalizationoftext-to-imagemodels image, and a learned denoising process that iteratively re-
movesnoisetoproduceavalidimage. Thedenoisingpro-
Thetaskoftext-to-imagepersonalizationwasproposedby cessislearnedthroughaU-Net[23]ϵ ,parameterizedbyθ,
θ
[7], where a few example images of the given concept are andisconditionedonanimagex noisedtotimestept,and
t
used to finetune a “personalized” token embedding while t itself. Text guidance can be incorporated through condi-
allotherparametersofthemodelfrozen. Insteadoftrying tioningonembeddingsc=τ(y)ofinputpromptsyfroma
to find an embedding within the existing text conditioning textencoderτ,suchasCLIP[20].
space to represent a concept, DreamBooth [24] finetunes In this work, we leverage Stable Diffusion, a text-
thediffusionmodel’sparameterstodirectlyinjectthecon- conditioned latent diffusion model (LDM) [22]. An LDM
cept into the learned prior, leading to better performance. is a variant of a diffusion model that operates in the latent
Custom Diffusion [16] only finetunes the cross-attention spaceofavariationalautoencoder[15]. TheencoderE em-
weightsinadditiontothetokenembeddingtoachievemore bedsaninputimagexintoalatentrepresentationz =E(x)
efficient personalization compared to DreamBooth. Based andadecoderD mapsz backintopixelspacex′ = D(z).
ontheseworks,otheraimtoimprovetheperformanceand ThediffusionportionofLDMoperatesonz andistrained
efficiency of personalizing text-to-image models through usingthefollowingobjective:
approaches such as, but not limited to, learning multiple
personalized tokens [5, 12], imposing constraints on the L =E (cid:104) ∥ϵ−ϵ (cid:0) z ,t,τ(y)(cid:1) ∥2(cid:105) . (1)
trainable parameters (e.g., key-locking [30], orthogonality LDM z∼E(x),y,ϵ∼N(0,1),t θ t 2
[19], low-rank [28], singular values only [9]), training hy-
Low rank adaptation (LoRA). Low rank adaptation
pernetworks and domain-specific encoders [8, 17, 25, 33],
(LoRA)[14]isanefficientmethodoriginallyproposedfor
andinjectingofvisualfeatures[10,32,33].
updating large language models through learned residuals
insteadofdirectlyfinetuningtheirparameters. Foragiven
2.2.Attention-guidedtext-to-imagesynthesis
layer of the pretrained model with weight matrix W ∈
0
Attention layers [31] have been shown to play an impor- Rm×n,LoRAlearnstwomatricesAandB whoseproduct
tantroleinthesuccessoftext-conditionedimagesynthesis formsaresidual∆W = AB ∈ Rm×n,whereA ∈ Rm×r,
using diffusion models. Recent works propose to manipu- B ∈ Rr×n, and r ≪ min(m,n) isthe rank. Theupdated
late attention maps from these layers for guided synthesis weight matrix is then defined as W′ = W +∆W. With
0
andediting. [4]modifiescross-attentionvaluestoguidethe smallvaluesofr,LoRAhasbeenshowntosignificantlyre-
generationprocesssothatthesubjectsspecifiedinaninput ducethenumberoflearnableparameterswhileretainingor
prompt appear and the attributes are associated to its cor- evenimprovingperformance.
responding subject. [1, 11] enable conditioning on a user-
3.2.Learningresidualsforcapturingidentity
provided layout by guiding the localization of objects via
cross-attentionmanipulation.Givenanexistingimageanda Thegoalofpersonalizingtext-to-imagemodelsistofaith-
promptthatdescribestheimage,[6,12]synthesize/editim- fullycapturetheidentityofatargetconceptwhilesimulta-neously avoiding overfitting so that the concept can be re- samplingmethodtobettercombineanewlylearnedconcept
contextualized into new settings and configurations. Since with the original generative prior of the diffusion model.
concepts are often learned using only a few reference im- As shown in Figure 2, within every transformer block of
ages,directlyfinetuningtheweightsofaverylargegenera- thediffusionmodelisacross-attentionlayer,whichaimsto
tivemodelcaneasilyleadtooverfittingand/oroverwriting learnthecorrespondencebetweentexttokensandimagere-
unnecessarypartsofthelearnedlanguageprior. Insteadwe gions. Eachcross-attentionlayercomputesattentionmaps
propose to use a LoRA-based approach to learn low-rank A for each token y in the prompt, indicating where the
yi i
offsets for a small subset of the diffusion model weights token will affect the generated image. The attention maps
which will represent the target concept. Thus, we are able areproducedusingthefollowingequation:
torecoverthefullgenerativecapacityoftheoriginalmodel
bysimplynotapplyingthelearnedresidualsatinference. (cid:16)QK⊤(cid:17)
A(Q,K)=softmax √ , (2)
The diffusion model contains multiple transformer d
k
blocks,whichconsistofself-andcross-attentionlayers[31]
where Q = WQx is the query, K = WKy is the key,
with a 1×1 conv projection layer on either end (see Fig-
andd isthedimensionofthequeryandkey.
ure2). Whileseveralapproachesprimarilytargetthecross- k
Given the indices C of the unique identifier and macro
attention layers due to their learning of relationships be-
classtokensspecifyingtheconcept(e.g.,“V*”and“dog”),
tween text and images, we choose to learn offsets for the
we sum the values of the corresponding attention maps
outputprojectionconvlayersbecausetheselocalizedoper-
(cid:80)
A = A intransformerblocki,andthenbinarize
ationscancapturefinerdetailsthantheglobaloperationsof i,C j∈C j
usingitsmedianvaluetogetM =binarize(A ). Finally,
cross-attention. i i,C
wecomputetheoutputfeaturefˆ ofeachtransformerblock
Weillustratetheprocessoflearningpersonalizedresid- i
ias:
uals in Figure 2. Given a pretrained text-to-image dif-
fusion model containing L transformer blocks, we learn
∆W i = A iB i ∈ Rmi×mi for the output projection layer fˆ i =(1−M i)⊗f i+M i⊗f i′, (3)
l
projout,i
with weight matrix W
i
∈ Rmi×mi×1 within each
wheref =W xisthefeatureproducedusingtheorigi-
transformerblocki,whereA
i
∈Rmi×ri andB
i
∈Rri×mi. nalconvwei ightWi
,andf′ =W′xisthefeatureproduced
We reshape the residual such that ∆W
i
∈ Rmi×mi×1 and
using the
updatedi weighti
from
i
the personalized residual
addtotheoriginalweightsW toproduceW′ =W +∆W .
i i i i W′ = W +∆W . Thus, theidentityrepresentedthrough
The ∆W ’sareupdatedusing theoriginaldiffusionobjec- i i i
i the personalized residuals is only being applied in the re-
tiveinEquation(1).
gions corresponding to the target concept, and the remain-
Similartootherworks, weassociatetheconceptwitha
ing regions are generated by the original diffusion model.
unique identifier token (e.g., V*), which is initialized us-
TheproposedLAGsamplingtechniqueisvisualizedinFig-
ing a rarely occurring token embedding. During training,
ure4.
weusetheuniquetokenandmacroclassoftheconceptina
While there exist personalization works using attention
fixedtemplateforthepromptassociatedwitheachreference
guidance (e.g., [10, 33]), they often rely on object masks
image (e.g., “a photo of a V* macro class”). Person-
and/oradditionallossesattraintimetofocusontherelevant
alization approaches that involve direct updates to the dif-
objectlocationinthereferenceimages,whereasmanually-
fusionmodel’sweightsaresusceptibletooverwritingparts
provided object masks or specific training are not needed
of the existing generative prior with the new concept and
to enable LAG. Additionally, LAG sampling explicitly
thusexplicitlyrequire“priorpreservation”throughregular-
merges the features of two layers (personalized/finetuned
ization images during training [16, 24]. Since our method
and original/non-finetuned) on-the-fly based on the cross-
doesnotdirectlyupdatethediffusionmodel,weavoidthis
attentionmapsobtainedduringinferenceandhasnegligible
issue entirely and eliminate the burden on the user to de-
impact on the sampling speed. In contrast, other synthe-
termine an effective set of regularization images, which is
sis/editing works (see Section 2.2) use cross-attention val-
notalwaysstraightforward. Additionally,thelow-rankcon-
ues to up- or down-weight the influence of specific tokens
straintontheresidualsreducesthenumberoftrainablepa-
atspecificimagelocations.
rameters, making our method a simpler and more efficient
LAG sampling can be beneficial in scenarios where the
approachforpersonalization.
learned residuals overfit to the reference images and have
not effectively disentangled the target concept from the
3.3.Localizedattention-guidedsampling
background, which can occur as a consequence of ambi-
Withourresidual-basedpersonalizationapproach,wehave guities of the target concept given the reference images or
additionalflexibilityinhowtheoffsetsareappliedatinfer- model biases (e.g., furniture often photographed indoors).
ence.Weintroduceanewlocalizedattention-guided(LAG) ByleveragingtheattentionmapsfromthetokensdenotingModified for customization
Transformer Block𝑖 1−𝑀 (cid:3036), Background
map
Q
Input Conv 𝑓(cid:4632) Output
K V feature map 1x1 (cid:3036) feature map
𝑊
(cid:3036)
Conv
1x1
“photo of a
V*penguin 𝑇 𝑊’
plushie” 𝐴(cid:3036),(cid:3004)
Binarize
𝑀(cid:3036)
Ori wgi en ia gl
h
c tsonv 𝑊
(cid:3036)
(cid:3036)
𝑀 (cid:3036), F mor ae pground
Text encoder
(1) Trainable
personalized ∆𝑊 (cid:3036) (2) Localized Attention-
residuals Guided Sampling
Figure2. Overviewofourproposedwork. (1)Personalizedresiduals: Welearnlow-rankresidualsfortheoutputprojectionlayerwithin
eachtransformerblockinthediffusionmodel. Theresidualscontainrelativelyfewparameters,arefasttotrain,anddonotrequireany
regularizationimagesduringtraining. (2)Localizedattention-guidedsampling: Weoptionallyapplythepersonalizedresidualsonlyin
theareasthatthecross-attentionlayershavelocalizedtheconceptviapredictedattentionmaps. Thus,wecancombinethenewlylearned
conceptwiththeoriginalgenerativepriorofthebasediffusionmodelwithinasingleimage.
the concept, we can localize the residuals so that they do model frozen. DreamBooth finetunes the entire diffusion
not affect the background, which can instead be generated model using the reference images and a set of regulariza-
usingthebasemodel. tionimages,whicharegeneratedwithinthesamedomainas
thetargetconceptusingtheoriginalmodel. WhileDream-
4.Experiments
Booth was originally proposed using Imagen [26], we use
anopen-sourceversionbuiltonStableDiffusion1. Custom
Inthissection,wedescribeourexperimentalsetupandeval-
Diffusion finetunes only the key and value weights of the
uationprotocols,andvisualizeexamplesusingtheproposed
cross-attentionlayersinadditiontotheidentifiertokenem-
personalizedresidualswithandwithoutlocalizedattention-
bedding, and uses a set of real regularization images sam-
guidedsampling.
pledfromLAION-400M[27].
4.1.Trainingdetails Weusetherecommendedsettingsdescribedbyeachpa-
per. For Textual Inversion and ViCo, which initialize the
We build upon Stable Diffusion v1.4 [22]. For each trans-
identifier token embedding to a single word that best rep-
former block i, we compute the rank r for its output
i resents the concept, we use our best discretion to pick a
projection convolution layer with weight matrix W ∈
i wordmostsimilartothemacroclassgivenbyCustomCon-
Rmi×mi×1 as r
i
= 0.05m i, totalling 1.2M trainable pa-
cept101.
rameters(∼0.1%ofStableDiffusion).Eachofthelow-rank
matricesarerandomlyinitialized. Wetrainourmethodfor 4.3.Evaluationmetrics
150 iterations with a batch size of 4 and learning rate of
Following the protocol described in [16], we leverage the
1.0e-3on1A100GPU(∼3minutes)acrossallexperiments.
CustomConcept101 dataset, consisting of 101 concepts
4.2.Baselines
across 16 broader categories. For every concept we gen-
erate 50 samples for each of the 20 prompts given by the
Wefocusoncomparisonstoopen-domain(i.e.,doesnotre-
dataset. We use DDIM sampling [29] with N = 50 steps,
quireencoderslimitedtoasinglegivendomain)approaches
η = 0.0, and a guidance scale of 6.0 for all methods. We
with publicly available code. Specifically, we compare
setthesamerandomseedforsamplingacrosseachmethod
our method against four baselines: Textual Inversion [7],
so that the “choice” of starting noise does not impact the
DreamBooth [24], Custom Diffusion [16], and ViCo [10].
results. ResultsofourmethodwithLAGsamplingareex-
TextualInversionfreezestheentirediffusionmodelandop-
timizesonlytheuniqueidentifiertokenV*foreachconcept. plicitlylabeledassuch.
ViCo optimizes V* as well as newly added cross-attention We evaluate each method for text alignment and image
layerstothediffusionmodeltoincorporatevisualinforma- 1https://github.com/XavierXiao/Dreambooth-
tionfromthereferenceimageswhilekeepingtherestofthe Stable-Diffusion
jorP
ni -fleS
noitnettA
-ssorC
noitnettA
jorP
tuoTable 1. Quantitative evaluations for text and image alignment ingregularizationimages.
using the similarity of CLIP and DINO features. We report the
Wecompareexamplesusingourproposedpersonalized
numberofparametersforeachmethodinadditiontoscoresfrom
residualswithandwithoutlocalizedattention-guidedsam-
thebaseStableDiffusionmodel,whichisnottrainedforperson-
plinginFigure4. WeillustratehowLAGsamplingaffects
alization,forreference.
theoutputimagebyusingthesamestartingnoisemapz to
T
Method #params CLIPtext CLIPimage DINOimage sampleeachpairof{w/oLAG,w/LAG}images. Wehigh-
TextualInversion 768 0.6150 0.7259 0.4700 light scenarios where LAG sampling performs better than
ViCo 51.3M 0.7403 0.7111 0.4678
normalsamplinginFigure4aandviceversainFigure4b.
DreamBooth 983M 0.7536 0.7424 0.5212
CustomDiffusion 19M 0.7664 0.7074 0.4669 Quantitativeevaluationsfortextandimagealignmentus-
Ours 1.2M 0.7193 0.7594 0.5671
Oursw/LAGsampling 1.2M 0.7220 0.7424 0.5411 ingCLIPandDINOareshowninTable1. Weincludere-
StableDiffusion 983M 0.8126 0.6207 0.2920 sults using the original Stable Diffusion model, which has
no notion of any of the concepts, for reference. We show
Table2. Humanpreferenceevaluationsfortextandimagealign-
that our method performs similarly with and without LAG
ment through Amazon Mechanical Turk. We perform bootstrap
samplingaveragedacrossthewholedataset,demonstrating
resamplingoverthe1250responsescollectedforeachtask.
higher image alignment and slightly lower text alignment
thanthemorecomputationally-heavybaselines.
Textual Custom Oursw/
Oursvs. ViCo DreamBooth
Inversion Diffusion LAG However, as seen by the results of 1250 responses col-
Text 81.85 37.40 41.34 50.99 58.57 lected through AMT user studies for both text and image
±4.15% ±7.46% ±5.08% ±5.46% ±6.34%
alignmentinTable2,weshowthattheCLIPtextalignment
61.96 62.11 51.33 63.27 26.26
Image scoresdonotnecessarilycorrelatetohumanpreference.We
±4.76% ±5.80% ±4.65% ±5.59% ±4.91%
observethatourmethodperformssimilarlytoCustomDif-
fusion for text alignment, which was assigned the highest
alignment. Textalignmentismeasuredasthesimilaritybe- CLIP text score, and outperforms all baselines for image
tweentheCLIP[20]textfeatureoftheinputpromptandthe alignment.Again,wenotethatourmethodachievessimilar
CLIPimagefeatureoftheresultinggeneratedimage.Image performancetothebetterperformingbaselineswhilebeing
alignmentismeasuredasthesimilaritybetweenimagefea- significantlymorecomputationallyefficient. Wealsocom-
turesfromeitherCLIPorDINO[3]ofthereferenceimages pareourmethodwithandwithoutLAGsamplingintheuser
andcorrespondinggeneratedimages. studiesandshowthatLAGispreferredforimagealignment
Additionally,weevaluatebothtextandimagealignment butnottextalignment. Furtheranalysiscomparingthetwo
using human evaluations through user studies on Amazon samplingapproachescanbefoundinthesupplementary.
MechanicalTurk(AMT).Foreachtextalignmentcase,we WealsotrainandevaluateourmethodusingCLIPsimi-
display a text prompt and a pair of corresponding gener- laritytoselectthe“mostrepresentative”macroclassamong
atedimages,andaskusers“Whichimageismoreconsistent the 117k nouns in WordNet [18] for each concept. In Ta-
with the given text prompt?”. For each image alignment ble4,weshowthatusingtheWordNetmacroclassleadsto
case,wedisplay3referenceimagesforaconceptandapair furtherimprovementsinimagealignmentwhiledecreasing
ofcorrespondinggeneratedimages,andask“Whichimage text alignment, the latter of which may not necessarily re-
better preserves the identity of the subject in the provided flecthumanpreferenceaspreviouslydemonstrated. Seethe
reference images?”. For both studies, each pair of images supplementaryforadditionaldiscussions.
containsonefrom{TextualInversion, ViCo, DreamBooth, Ablation studies. We perform ablation studies on
Custom Diffusion, Ours w/ LAG sampling} and one from changingthetargetsforwheretheresidualsareapplied,re-
ours with normal DDIMsampling. Users can select either moving the macro class from the prompt, including regu-
imageorneither(“Notsure”). larization images (sampled from LAION) during training,
updating the concept identifier token embedding V*, and
4.4.Results
varyingtherankoftheresiduals. ResultsareshowninTa-
We visualize samples generated by each method for vari- ble3(seeTable5forresultsonchangingtherank).
ous types of prompts in Figure 3. Textual Inversion fails Weshowthatchangingwheretheresidualsareappliedto
toreliablycapturetheconcept’sidentityand/ortheprompt eitherthekeyandvalueweightsofthecross-attentionlayers
whereasallothermethods,includingours,areabletobetter (like Custom Diffusion) or the input projection conv layer
preserve the concept’s identity while also adhering to the (ratherthantheoutput)slightlydecreasesthescoresacross
prompt. We highlight that our method is able to achieve all three metrics compared to our proposed approach. We
theseresultswhilehavingsignificantlyfewerlearnablepa- hypothesize that the output projection layer achieves no-
rametersandrequiringlesstrainingtimecomparedtoViCo, ticeably higher identity preservation because it refines the
DreamBooth,andCustomDiffusion,aswellasnotleverag- featuremapattheendofeachblock. Additionally,learningConcept Ours Textual Inversion ViCo DreamBooth Custom Diffusion
“V* penguin plushie in Grand Canyon”
“V* backpack on a café table with a steaming cup of coffee nearby”
“A pink V* chair”
“Georgia O’Keeffe style V* dog painting”
“An origami art of V* houseplant”
“V* barn in snowy ice”
“Gold colored V* shoes”
“Marigold flowers in the V* vase”
Figure3.Qualitativecomparisonofourproposedapproachwiththebaselines.Concept “V* action figure on a Concept “C-3PO reading a book in Concept “A watercolor painting of Concept “A V* dish formed from
sandy beach, with waves the light of the V* lamp” V* car, cruising down a chocolate”
in the background” countryside road”
Concept “A watercolor painting of Concept “A V* ring on a mossy Concept “A red color V* chair” Concept “A koala in the style of V*
V* flower and a teapot on forest floor” teddybear”
the table”
(a)ExampleswhereLAGproducesresultsthatarebetteralignedwiththe (b)Exampleswherenormalsamplingproducesresultsthatarebetteraligned
conceptandprompt. withtheconceptandprompt.
Figure4.ComparisonofimagegeneratedwithandwithoutLAGsampling.Weusethesamestartingnoisemaptogeneratecorresponding
pairsofimagestodirectlyvisualizehowLAGsamplingaffectstheoutputimage.
Table 3. We evaluate our method using two different targets for 5.Conclusion
theresidualsandalteringvarioustrainingsettings.
Weintroducepersonalizedresiduals,amethodforconcept-
Method CLIPtext CLIPimage DINOimage
KVweights 0.7172 0.7508 0.5353 drivensynthesisusingtext-to-imagediffusionmodels. Pre-
l projinweights 0.7136 0.7460 0.5333 viousapproachestopersonalizationareoftenslowtotrain,
KV+l projoutweights 0.7049 0.7733 0.5868 have high computational demands, require regularization
KV+l projin+l projoutweights 0.6739 0.7870 0.6040
images, and/or have difficulty recontextualizing the target
w/omacroclass 0.6605 0.6521 0.3798
w/regimages 0.7204 0.6771 0.3830 concept. ThroughourproposedLoRA-basedapproachthat
Updatetokenembedding 0.6673 0.8000 0.6194 learnsasmallsetofresidualstorepresenttheidentityofa
Ours 0.7193 0.7594 0.5671
concept,wereducethenumberoflearnableparametersand
trainingtimeandremovetherelianceondomainregulariza-
tionwhilemaintainingflexibilitywithediting. Wealsoin-
troduce localized attention-guided sampling which applies
residualsformultiplelayerssimultaneouslyleadstooverfit-
thepersonalizedresidualsonlyinregionswheretheconcept
tingtothereferenceimagesasdemonstratedbythehigher
islocalizedviathecross-attentionmechanism. Weevaluate
imagealignmentscoresandlowertextalignment.
ourmethodacrossseveralmetricstoshowthatweareable
toefficientlyenablepersonalization.
Omitting the macro class leads to significant drops
across all metrics, demonstrating that the additional infor- Limitations and future work. We show that localized
mation is useful to our method for knowing what within sampling is not always the best choice (e.g., changing the
the reference images is important to model. Similar to the color of a concept) and relies on the cross-attention layers
effect of using regularization images for DreamBooth and toproducehigh-qualityattentionmaps,whichisnotalways
CustomDiffusion, regularizationimagesslightlyimproves the case. Our approach can be sensitive to the choice of
textalignmentbutdecreasesimagealignment. Ontheother macro class and inherits the pretrained model’s biases and
hand, updating the token embedding for V* leads to over- limitations, suchasmixinguptherelationshipbetweenat-
fittingasshownbytheincreaseinimagealignmentandde- tributesintheprompt. Finally,weleavemulti-conceptgen-
creaseintextalignment. erationthroughLAGsamplingasfuturework.
w/o
LAG
w/
LAG
w/o
LAG
w/
LAG
w/o
LAG
w/
LAG
w/o
LAG
w/
LAGReferences Lora: Low-rankadaptationoflargelanguagemodels. arXiv
preprintarXiv:2106.09685,2021. 2,3
[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
[15] DiederikPKingmaandMaxWelling. Auto-encodingvari-
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
ationalbayes. InternationalConferenceonLearningRepre-
SamuliLaine,BryanCatanzaro,etal. ediffi: Text-to-image
sentations(ICLR),2014. 3
diffusionmodelswithanensembleofexpertdenoisers.arXiv
[16] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
preprintarXiv:2211.01324,2022. 3
Shechtman,andJun-YanZhu. Multi-conceptcustomization
[2] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-
oftext-to-imagediffusion. InProceedingsoftheIEEE/CVF
structpix2pix:Learningtofollowimageeditinginstructions.
Conference on Computer Vision and Pattern Recognition,
In Proceedings of the IEEE/CVF Conference on Computer
pages1931–1941,2023. 1,2,3,4,5
VisionandPatternRecognition,pages18392–18402,2023.
[17] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-
3
diffusion: Pre-trained subject representation for control-
[3] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou,
lable text-to-image generation and editing. arXiv preprint
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg-
arXiv:2305.14720,2023. 3
ingpropertiesinself-supervisedvisiontransformers.InPro-
[18] George A Miller. Wordnet: a lexical database for english.
ceedingsoftheIEEE/CVFinternationalconferenceoncom-
CommunicationsoftheACM,38(11):39–41,1995. 6
putervision,pages9650–9660,2021. 6
[19] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao
[4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and
Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard
Daniel Cohen-Or. Attend-and-excite: Attention-based se-
Scho¨lkopf. Controllingtext-to-imagediffusionbyorthogo-
mantic guidance for text-to-image diffusion models. ACM
nalfinetuning. arXivpreprintarXiv:2306.07280,2023. 3
TransactionsonGraphics(TOG),42(4):1–10,2023. 3
[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[5] Ziyi Dong, Pengxu Wei, and Liang Lin. Drea-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
martist: Towards controllable one-shot text-to-image gen-
AmandaAskell,PamelaMishkin,JackClark,etal.Learning
eration via contrastive prompt-tuning. arXiv preprint
transferable visual models from natural language supervi-
arXiv:2211.11337,2022. 3
sion.InInternationalconferenceonmachinelearning,pages
[6] DaveEpstein, AllanJabri, BenPoole, AlexeiAEfros, and 8748–8763.PMLR,2021. 3,6
Aleksander Holynski. Diffusion self-guidance for control-
[21] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
lable image generation. arXiv preprint arXiv:2306.00986,
andMarkChen. Hierarchicaltext-conditionalimagegener-
2023. 3
ationwithcliplatents. arXivpreprintarXiv:2204.06125,1
[7] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash- (2):3,2022. 1
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Or. An image is worth one word: Personalizing text-to- Patrick Esser, and Bjo¨rn Ommer. High-resolution image
image generation using textual inversion. arXiv preprint synthesis with latent diffusion models. In Proceedings of
arXiv:2208.01618,2022. 1,3,5 the IEEE/CVF conference on computer vision and pattern
[8] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, recognition,pages10684–10695,2022. 1,3,5
GalChechik,andDanielCohen-Or. Encoder-baseddomain [23] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
tuningforfastpersonalizationoftext-to-imagemodels.ACM net: Convolutionalnetworksforbiomedicalimagesegmen-
TransactionsonGraphics(TOG),42(4):1–13,2023. 1,3 tation.InMedicalImageComputingandComputer-Assisted
[9] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Intervention–MICCAI2015:18thInternationalConference,
Dimitris Metaxas, and Feng Yang. Svdiff: Compact pa- Munich,Germany,October5-9,2015,Proceedings,PartIII
rameter space for diffusion fine-tuning. arXiv preprint 18,pages234–241.Springer,2015. 3
arXiv:2303.11305,2023. 3 [24] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
[10] Shaozhe Hao, Kai Han, Shihao Zhao, and Kwan-Yee K MichaelRubinstein,andKfirAberman. Dreambooth: Fine
Wong. Vico: Detail-preserving visual condition for tuning text-to-image diffusion models for subject-driven
personalized text-to-image generation. arXiv preprint generation. In Proceedings of the IEEE/CVF Conference
arXiv:2306.00971,2023. 3,4,5 onComputerVisionandPatternRecognition,pages22500–
[11] Yutong He, Ruslan Salakhutdinov, and J Zico Kolter. Lo- 22510,2023. 1,3,4,5
calizedtext-to-imagegenerationforfreeviacrossattention [25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,
control. arXivpreprintarXiv:2306.14636,2023. 3 TingboHou,YaelPritch,NealWadhwa,MichaelRubinstein,
[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, and Kfir Aberman. Hyperdreambooth: Hypernetworks for
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im- fastpersonalizationoftext-to-imagemodels. arXivpreprint
age editing with cross attention control. arXiv preprint arXiv:2307.06949,2023. 1,3
arXiv:2208.01626,2022. 3 [26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
[13] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif- Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
fusionprobabilisticmodels. Advancesinneuralinformation RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
processingsystems,33:6840–6851,2020. 3 etal.Photorealistictext-to-imagediffusionmodelswithdeep
[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- language understanding. Advances in Neural Information
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen. ProcessingSystems,35:36479–36494,2022. 1,5[27] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes,JeniaJitsev,andAranKomatsuzaki.Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
arXivpreprintarXiv:2111.02114,2021. 5
[28] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting
Hua,ZsoltKira,YilinShen,andHongxiaJin.Continualdif-
fusion: Continual customization of text-to-image diffusion
withc-lora. arXivpreprintarXiv:2304.06027,2023. 3
[29] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502,2020. 5
[30] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.
Key-locked rank one editing for text-to-image personaliza-
tion. In ACM SIGGRAPH 2023 Conference Proceedings,
pages1–11,2023. 3,1
[31] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. Advancesinneural
informationprocessingsystems,30,2017. 3,4
[32] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei
Zhang, and Wangmeng Zuo. Elite: Encoding visual con-
ceptsintotextualembeddingsforcustomizedtext-to-image
generation. arXivpreprintarXiv:2302.13848,2023. 3
[33] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fre´do
Durand, andSongHan. Fastcomposer: Tuning-freemulti-
subject image generation with localized attention. arXiv
preprintarXiv:2305.10431,2023. 1,3,4Personalized Residuals for Concept-Driven Text-to-Image Generation
Supplementary Material
Table4. Wecomputethenearestneighbor(NN)inCLIPembed-
dingspaceforeachconceptamongallWordNetnouns. Wecom-
pare our method using different combinations of macro classes
duringtrainingandsampling.
Macroclasschoice
CLIPtext CLIPimage DINOimage
Training Sampling
CustomConcept101 0.7193 0.7594 0.5671
CustomConcept101
WordNetNN 0.7155 0.7594 0.5671
CustomConcept101 0.6626 0.7798 0.5904
WordNetNN
WordNetNN 0.6869 0.7798 0.5904
calculate the cosine similarity against the CLIP text em-
Figure5.AMTtextalignmentscoresperprompttype.
bedding for each of the 117k nouns within WordNet. We
6.Additionalexperimentalresults train our method and/or sample using the WordNet noun
withthehighestsimilarityandcomparewithusingthepro-
WeexplorethedifferenceinnormalandLAGsamplingby vided macro class from CustomConcept101 during train-
using ChatGPT to categorize each prompt into {add ob- ing and/or sampling in Table 4. We observe that using the
ject(s),artisticstyle,changeattribute,changebackground, WordNetnearestneighborasthemacroclassleadstohigher
identity,objectinstyleofV*}. Wenotethatapromptmay imagealignmentandlowertextalignmentcomparedtothe
fall into multiple categories, but we only use one as deter- CustomConcept101-providedmacroclass.
minedbyChatGPT.WesplittheAMTevaluationsfortext Selecting the “best” macro class for concepts can be
alignment by category in Figure 5. We observe that LAG challengingandgiventhatitcanleadtonoticeablechanges
sampling performs best for identity, change background, inalignmentmetrics,anautomaticheuristicforchoosinga
andaddobject(s),whicharetasksinwhichthetargetobject suitablemacroclasswouldbehelpfultousers.Weleavethe
is somewhat independent of the rest of the image. Tasks designingofsuchaheuristicasfuturework.
that require modifying the target (artistic style, change at-
tribute, object in style of V*) perform better with normal 8.Ablationstudy: rankvalue
DDIMsampling.
In Figures 6 to 11 we directly compare examples from
Table 5. Quantitative evaluations for varying the rank of the
each of the six prompt categories using the two sampling
learnedresiduals. m isthedimensionoftheweightofthepro-
i
methodsbygeneratingcorrespondingpairsusingthesame
jectionlayerintransformerblocki.
startingnoisemaps. Additionalqualitativesamplescanbe
foundinFigures12and13. Rank CLIPtext CLIPimage DINOimage
We plot CLIP/DINO image alignment scores against 1 0.7398 0.6809 0.4148
CLIP text scores, averaged across concepts within the the 8 0.7054 0.7402 0.5239
16categoriesofCustomConcept101,foreachmethodfrom 16 0.6926 0.7573 0.5513
Section4. 32 0.6832 0.7701 0.5713
64 0.6704 0.7798 0.5865
Additionally, we compare our method to an unofficial
128 0.6544 0.7938 0.6053
implementation2ofPerfusion[30](anofficialversionisnot
0.025m 0.6889 0.7622 0.5595
publiclyavailable).Wefollowedtheexperimentalsetupand i
Ours(0.05m ) 0.7193 0.7594 0.5671
hyperparameter values described by the original authors, i
butnotethatwewereunabletoreproducethequalityofthe We evaluate different values for the rank of the learned
resultsshowninthepaper: CLIPtext0.6879, CLIPimage residuals in Table 5 and observe that text alignment is in-
0.5669,DINOimage0.2228. versely proportional to the rank and image alignment is
directly proportional. Since the dimensions of the conv
7.Effectofmacroclasschoice weight matrix varies across the transformer blocks within
theU-Net,webelievethatcalculatingtherankwithrespect
For each concept in CustomConcept101, we compute the
tothedimensionsisthebetterapproachoversettingafixed
mean CLIP image embedding of its reference images and
valueacrossalllayers,whichisempiricallyvalidatedbythe
2https://github.com/ChenDarYen/Key-Locked-Rank- resultswithourproposedformulaachievingabetterbalance
One-Editing-for-Text-to-Image-Personalization/ ofimageandtextalignment.Concept Ours Ours w/ LAG sampling
“V* cat is wearing sunglasses”
“Bouquet of V* flower and roses”
Figure6. Samplesforaddobject(s)promptsusingpersonalizedresidualswithandwithoutLAGsamplingwherecorrespondingpairsare
generatedusingthesameinputnoisemap.
Concept Ours Ours w/ LAG sampling
”A graffiti mural of V* bottle on a brick wall in a city alley”
“V* person painted with thick, dramatic brush strokes, in the style
of Edvard Munch”
Figure7. SamplesforartisticstylepromptsusingpersonalizedresidualswithandwithoutLAGsamplingwherecorrespondingpairsare
generatedusingthesameinputnoisemap.Concept Ours Ours w/ LAG sampling
“An orange V* sofa”
“A transparent V* cup filled with steaming hot cocoa”
Figure8. SamplesforchangeattributepromptsusingpersonalizedresidualswithandwithoutLAGsamplingwherecorrespondingpairs
aregeneratedusingthesameinputnoisemap.
Concept Ours Ours w/ LAG sampling
“V* sculpture in the middle of highway road”
“A V* headphone on a table with mountains and sunset in
the background”
Figure9. Samplesforchangebackground promptsusingpersonalizedresidualswithandwithoutLAGsamplingwherecorresponding
pairsaregeneratedusingthesameinputnoisemap.Concept Ours Ours w/ LAG sampling
“Photo of a V* unicorn plushie”
“Photo of a V* sofa”
Figure 10. Samples for identity prompts using personalized residuals with and without LAG sampling where corresponding pairs are
generatedusingthesameinputnoisemap.
Concept Ours Ours w/ LAG sampling
“A pair of design shoes in the style of V* earrings”
“An egg chair in the style of V* chair”
Figure11. SamplesforobjectinstyleofV*promptsusingpersonalizedresidualswithandwithoutLAGsamplingwherecorresponding
pairsaregeneratedusingthesameinputnoisemap.Concept Ours Ours w/ LAG sampling
“Print of V* houseplant on a sweater”
“V* bear oil painting Ghibli inspired”
“A teapot in the style of V* vase”
“Japanese ukiyo-e style depiction of the V* waterfall”
“A purple colored V* purse”
Figure12.SamplesgeneratedusingpersonalizedresidualswithandwithoutLAGsampling.Concept Ours Ours w/ LAG sampling
“Rose flowers in V* wooden pot on a table”
“A funky Picasso-style cubist painting of V* violin”
“V* plushie sitting at the beach with a view of the sea”
“V* canal scene painting by artist Claude Monet”
“A V* car painted with vibrant graffiti, parked in an urban alley”
Figure13.SamplesgeneratedusingpersonalizedresidualswithandwithoutLAGsampling.(a)PlotofCLIPimagealignmentvs.CLIPtextalignment. (b)PlotofDINOimagealignmentvs.CLIPtextalignment.
Figure14.Foreachmethod,weplottheeitherCLIPorDINOimagealignmentscoresagainstCLIPtextalignmentscoresaveragedacross
theconceptswithineachofthe16categoriesofCustomConcept101.