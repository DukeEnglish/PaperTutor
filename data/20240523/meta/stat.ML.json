[
    {
        "title": "Online Learning of Halfspaces with Massart Noise",
        "authors": "Ilias DiakonikolasVasilis KontonisChristos TzamosNikos Zarifis",
        "links": "http://arxiv.org/abs/2405.12958v1",
        "entry_id": "http://arxiv.org/abs/2405.12958v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12958v1",
        "summary": "We study the task of online learning in the presence of Massart noise.\nInstead of assuming that the online adversary chooses an arbitrary sequence of\nlabels, we assume that the context $\\mathbf{x}$ is selected adversarially but\nthe label $y$ presented to the learner disagrees with the ground-truth label of\n$\\mathbf{x}$ with unknown probability at most $\\eta$. We study the fundamental\nclass of $\\gamma$-margin linear classifiers and present a computationally\nefficient algorithm that achieves mistake bound $\\eta T + o(T)$. Our mistake\nbound is qualitatively tight for efficient algorithms: it is known that even in\nthe offline setting achieving classification error better than $\\eta$ requires\nsuper-polynomial time in the SQ model.\n  We extend our online learning model to a $k$-arm contextual bandit setting\nwhere the rewards -- instead of satisfying commonly used realizability\nassumptions -- are consistent (in expectation) with some linear ranking\nfunction with weight vector $\\mathbf{w}^\\ast$. Given a list of contexts\n$\\mathbf{x}_1,\\ldots \\mathbf{x}_k$, if $\\mathbf{w}^*\\cdot \\mathbf{x}_i >\n\\mathbf{w}^* \\cdot \\mathbf{x}_j$, the expected reward of action $i$ must be\nlarger than that of $j$ by at least $\\Delta$. We use our Massart online learner\nto design an efficient bandit algorithm that obtains expected reward at least\n$(1-1/k)~ \\Delta T - o(T)$ bigger than choosing a random action at every round.",
        "updated": "2024-05-21 17:31:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12958v1"
    },
    {
        "title": "Learning the Infinitesimal Generator of Stochastic Diffusion Processes",
        "authors": "Vladimir R. KosticKarim LouniciHelene HalconruyTimothee DevergneMassimiliano Pontil",
        "links": "http://arxiv.org/abs/2405.12940v1",
        "entry_id": "http://arxiv.org/abs/2405.12940v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12940v1",
        "summary": "We address data-driven learning of the infinitesimal generator of stochastic\ndiffusion processes, essential for understanding numerical simulations of\nnatural and physical systems. The unbounded nature of the generator poses\nsignificant challenges, rendering conventional analysis techniques for\nHilbert-Schmidt operators ineffective. To overcome this, we introduce a novel\nframework based on the energy functional for these stochastic processes. Our\napproach integrates physical priors through an energy-based risk metric in both\nfull and partial knowledge settings. We evaluate the statistical performance of\na reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the\npartial knowledge setting. Notably, our approach provides learning bounds\nindependent of the state space dimension and ensures non-spurious spectral\nestimation. Additionally, we elucidate how the distortion between the intrinsic\nenergy-induced metric of the stochastic diffusion and the RKHS metric used for\ngenerator estimation impacts the spectral learning bounds.",
        "updated": "2024-05-21 17:13:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12940v1"
    },
    {
        "title": "LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language",
        "authors": "James RequeimaJohn BronskillDami ChoiRichard E. TurnerDavid Duvenaud",
        "links": "http://arxiv.org/abs/2405.12856v1",
        "entry_id": "http://arxiv.org/abs/2405.12856v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12856v1",
        "summary": "Machine learning practitioners often face significant challenges in formally\nintegrating their prior knowledge and beliefs into predictive models, limiting\nthe potential for nuanced and context-aware analyses. Moreover, the expertise\nneeded to integrate this prior knowledge into probabilistic modeling typically\nlimits the application of these models to specialists. Our goal is to build a\nregression model that can process numerical data and make probabilistic\npredictions at arbitrary locations, guided by natural language text which\ndescribes a user's prior knowledge. Large Language Models (LLMs) provide a\nuseful starting point for designing such a tool since they 1) provide an\ninterface where users can incorporate expert insights in natural language and\n2) provide an opportunity for leveraging latent problem-relevant knowledge\nencoded in LLMs that users may not have themselves. We start by exploring\nstrategies for eliciting explicit, coherent numerical predictive distributions\nfrom LLMs. We examine these joint predictive distributions, which we call LLM\nProcesses, over arbitrarily-many quantities in settings such as forecasting,\nmulti-dimensional regression, black-box optimization, and image modeling. We\ninvestigate the practical details of prompting to elicit coherent predictive\ndistributions, and demonstrate their effectiveness at regression. Finally, we\ndemonstrate the ability to usefully incorporate text into numerical\npredictions, improving predictive performance and giving quantitative structure\nthat reflects qualitative descriptions. This lets us begin to explore the rich,\ngrounded hypothesis space that LLMs implicitly encode.",
        "updated": "2024-05-21 15:13:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12856v1"
    },
    {
        "title": "Wav-KAN: Wavelet Kolmogorov-Arnold Networks",
        "authors": "Zavareh BozorgaslHao Chen",
        "links": "http://arxiv.org/abs/2405.12832v1",
        "entry_id": "http://arxiv.org/abs/2405.12832v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12832v1",
        "summary": "In this paper , we introduce Wav-KAN, an innovative neural network\narchitecture that leverages the Wavelet Kolmogorov-Arnold Networks (Wav-KAN)\nframework to enhance interpretability and performance. Traditional multilayer\nperceptrons (MLPs) and even recent advancements like Spl-KAN face challenges\nrelated to interpretability, training speed, robustness, computational\nefficiency, and performance. Wav-KAN addresses these limitations by\nincorporating wavelet functions into the Kolmogorov-Arnold network structure,\nenabling the network to capture both high-frequency and low-frequency\ncomponents of the input data efficiently. Wavelet-based approximations employ\northogonal or semi-orthogonal basis and also maintains a balance between\naccurately representing the underlying data structure and avoiding overfitting\nto the noise. Analogous to how water conforms to the shape of its container,\nWav-KAN adapts to the data structure, resulting in enhanced accuracy, faster\ntraining speeds, and increased robustness compared to Spl-KAN and MLPs. Our\nresults highlight the potential of Wav-KAN as a powerful tool for developing\ninterpretable and high-performance neural networks, with applications spanning\nvarious fields. This work sets the stage for further exploration and\nimplementation of Wav-KAN in frameworks such as PyTorch, TensorFlow, and also\nit makes wavelet in KAN in wide-spread usage like nowadays activation functions\nlike ReLU, sigmoid in universal approximation theory (UAT).",
        "updated": "2024-05-21 14:36:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12832v1"
    },
    {
        "title": "Refined Graph Encoder Embedding via Self-Training and Latent Community Recovery",
        "authors": "Cencheng ShenJonathan LarsonHa TrinhCarey E. Priebe",
        "links": "http://arxiv.org/abs/2405.12797v1",
        "entry_id": "http://arxiv.org/abs/2405.12797v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12797v1",
        "summary": "This paper introduces a refined graph encoder embedding method, enhancing the\noriginal graph encoder embedding using linear transformation, self-training,\nand hidden community recovery within observed communities. We provide the\ntheoretical rationale for the refinement procedure, demonstrating how and why\nour proposed method can effectively identify useful hidden communities via\nstochastic block models, and how the refinement method leads to improved vertex\nembedding and better decision boundaries for subsequent vertex classification.\nThe efficacy of our approach is validated through a collection of simulated and\nreal-world graph data.",
        "updated": "2024-05-21 13:48:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12797v1"
    }
]