[
    {
        "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
        "authors": "William BrandonMayank MishraAniruddha NrusimhaRameswar PandaJonathan Ragan Kelly",
        "links": "http://arxiv.org/abs/2405.12981v1",
        "entry_id": "http://arxiv.org/abs/2405.12981v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12981v1",
        "summary": "Key-value (KV) caching plays an essential role in accelerating decoding for\ntransformer-based autoregressive large language models (LLMs). However, the\namount of memory required to store the KV cache can become prohibitive at long\nsequence lengths and large batch sizes. Since the invention of the transformer,\ntwo of the most effective interventions discovered for reducing the size of the\nKV cache have been Multi-Query Attention (MQA) and its generalization,\nGrouped-Query Attention (GQA). MQA and GQA both modify the design of the\nattention block so that multiple query heads can share a single key/value head,\nreducing the number of distinct key/value heads by a large factor while only\nminimally degrading accuracy. In this paper, we show that it is possible to\ntake Multi-Query Attention a step further by also sharing key and value heads\nbetween adjacent layers, yielding a new attention design we call Cross-Layer\nAttention (CLA). With CLA, we find that it is possible to reduce the size of\nthe KV cache by another 2x while maintaining nearly the same accuracy as\nunmodified MQA. In experiments training 1B- and 3B-parameter models from\nscratch, we demonstrate that CLA provides a Pareto improvement over the\nmemory/accuracy tradeoffs which are possible with traditional MQA, enabling\ninference with longer sequence lengths and larger batch sizes than would\notherwise be possible",
        "updated": "2024-05-21 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12981v1"
    },
    {
        "title": "Can We Treat Noisy Labels as Accurate?",
        "authors": "Yuxiang ZhengZhongyi HanYilong YinXin GaoTongliang Liu",
        "links": "http://arxiv.org/abs/2405.12969v1",
        "entry_id": "http://arxiv.org/abs/2405.12969v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12969v1",
        "summary": "Noisy labels significantly hinder the accuracy and generalization of machine\nlearning models, particularly due to ambiguous instance features. Traditional\ntechniques that attempt to correct noisy labels directly, such as those using\ntransition matrices, often fail to address the inherent complexities of the\nproblem sufficiently. In this paper, we introduce EchoAlign, a transformative\nparadigm shift in learning from noisy labels. Instead of focusing on label\ncorrection, EchoAlign treats noisy labels ($\\tilde{Y}$) as accurate and\nmodifies corresponding instance features ($X$) to achieve better alignment with\n$\\tilde{Y}$. EchoAlign's core components are (1) EchoMod: Employing\ncontrollable generative models, EchoMod precisely modifies instances while\nmaintaining their intrinsic characteristics and ensuring alignment with the\nnoisy labels. (2) EchoSelect: Instance modification inevitably introduces\ndistribution shifts between training and test sets. EchoSelect maintains a\nsignificant portion of clean original instances to mitigate these shifts. It\nleverages the distinct feature similarity distributions between original and\nmodified instances as a robust tool for accurate sample selection. This\nintegrated approach yields remarkable results. In environments with 30%\ninstance-dependent noise, even at 99% selection accuracy, EchoSelect retains\nnearly twice the number of samples compared to the previous best method.\nNotably, on three datasets, EchoAlign surpasses previous state-of-the-art\ntechniques with a substantial improvement.",
        "updated": "2024-05-21 17:49:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12969v1"
    },
    {
        "title": "The future of cosmological likelihood-based inference: accelerated high-dimensional parameter estimation and model comparison",
        "authors": "Davide PirasAlicja PolanskaAlessio Spurio ManciniMatthew A. PriceJason D. McEwen",
        "links": "http://arxiv.org/abs/2405.12965v1",
        "entry_id": "http://arxiv.org/abs/2405.12965v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12965v1",
        "summary": "We advocate for a new paradigm of cosmological likelihood-based inference,\nleveraging recent developments in machine learning and its underlying\ntechnology, to accelerate Bayesian inference in high-dimensional settings.\nSpecifically, we combine (i) emulation, where a machine learning model is\ntrained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii)\ndifferentiable and probabilistic programming, e.g. JAX and NumPyro,\nrespectively; (iii) scalable Markov chain Monte Carlo (MCMC) sampling\ntechniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv)\ndecoupled and scalable Bayesian model selection techniques that compute the\nBayesian evidence purely from posterior samples, e.g. the learned harmonic mean\nimplemented in harmonic. This paradigm allows us to carry out a complete\nBayesian analysis, including both parameter estimation and model selection, in\na fraction of the time of traditional approaches. First, we demonstrate the\napplication of this paradigm on a simulated cosmic shear analysis for a Stage\nIV survey in 37- and 39-dimensional parameter spaces, comparing $\\Lambda$CDM\nand a dynamical dark energy model ($w_0w_a$CDM). We recover posterior contours\nand evidence estimates that are in excellent agreement with those computed by\nthe traditional nested sampling approach while reducing the computational cost\nfrom 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a joint\nanalysis between three simulated next-generation surveys, each performing a\n3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces.\nStandard nested sampling techniques are simply not feasible in this\nhigh-dimensional setting, requiring a projected 12 years of compute time on 48\nCPU cores; on the other hand, the proposed approach only requires 8 days of\ncompute time on 24 GPUs. All packages used in our analyses are publicly\navailable.",
        "updated": "2024-05-21 17:45:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12965v1"
    },
    {
        "title": "Comprehensive Multimodal Deep Learning Survival Prediction Enabled by a Transformer Architecture: A Multicenter Study in Glioblastoma",
        "authors": "Ahmed GomaaYixing HuangAmr HagagCharlotte SchmitterDaniel HöflerThomas WeissmannKatharina BreiningerManuel SchmidtJenny StritzelbergerDaniel DelevRoland CorasArnd DörflerOliver SchnellBenjamin FreyUdo S. GaiplSabine SemrauChristoph BertRainer FietkauFlorian Putz",
        "links": "http://arxiv.org/abs/2405.12963v1",
        "entry_id": "http://arxiv.org/abs/2405.12963v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12963v1",
        "summary": "Background: This research aims to improve glioblastoma survival prediction by\nintegrating MR images, clinical and molecular-pathologic data in a\ntransformer-based deep learning model, addressing data heterogeneity and\nperformance generalizability. Method: We propose and evaluate a\ntransformer-based non-linear and non-proportional survival prediction model.\nThe model employs self-supervised learning techniques to effectively encode the\nhigh-dimensional MRI input for integration with non-imaging data using\ncross-attention. To demonstrate model generalizability, the model is assessed\nwith the time-dependent concordance index (Cdt) in two training setups using\nthree independent public test sets: UPenn-GBM, UCSF-PDGM, and RHUH-GBM, each\ncomprising 378, 366, and 36 cases, respectively. Results: The proposed\ntransformer model achieved promising performance for imaging as well as\nnon-imaging data, effectively integrating both modalities for enhanced\nperformance (UPenn-GBM test-set, imaging Cdt 0.645, multimodal Cdt 0.707) while\noutperforming state-of-the-art late-fusion 3D-CNN-based models. Consistent\nperformance was observed across the three independent multicenter test sets\nwith Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM,\nfirst external test set) and 0.618 (RHUH-GBM, second external test set). The\nmodel achieved significant discrimination between patients with favorable and\nunfavorable survival for all three datasets (logrank p 1.9\\times{10}^{-8},\n9.7\\times{10}^{-3}, and 1.2\\times{10}^{-2}). Conclusions: The proposed\ntransformer-based survival prediction model integrates complementary\ninformation from diverse input modalities, contributing to improved\nglioblastoma survival prediction compared to state-of-the-art methods.\nConsistent performance was observed across institutions supporting model\ngeneralizability.",
        "updated": "2024-05-21 17:44:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12963v1"
    },
    {
        "title": "Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale",
        "authors": "Shriram ChennakesavaluFrank HuSebastian IbarraranGrant M. Rotskoff",
        "links": "http://arxiv.org/abs/2405.12961v1",
        "entry_id": "http://arxiv.org/abs/2405.12961v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12961v1",
        "summary": "Searching through chemical space is an exceptionally challenging problem\nbecause the number of possible molecules grows combinatorially with the number\nof atoms. Large, autoregressive models trained on databases of chemical\ncompounds have yielded powerful generators, but we still lack robust strategies\nfor generating molecules with desired properties. This molecular search problem\nclosely resembles the \"alignment\" problem for large language models, though for\nmany chemical tasks we have a specific and easily evaluable reward function.\nHere, we introduce an algorithm called energy rank alignment (ERA) that\nleverages an explicit reward function to produce a gradient-based objective\nthat we use to optimize autoregressive policies. We show theoretically that\nthis algorithm is closely related to proximal policy optimization (PPO) and\ndirect preference optimization (DPO), but has a minimizer that converges to an\nideal Gibbs-Boltzmann distribution with the reward playing the role of an\nenergy function. Furthermore, this algorithm is highly scalable, does not\nrequire reinforcement learning, and performs well relative to DPO when the\nnumber of preference observations per pairing is small. We deploy this approach\nto align molecular transformers to generate molecules with externally specified\nproperties and find that it does so robustly, searching through diverse parts\nof chemical space. While our focus here is on chemical search, we also obtain\nexcellent results on an AI supervised task for LLM alignment, showing that the\nmethod is scalable and general.",
        "updated": "2024-05-21 17:35:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12961v1"
    }
]