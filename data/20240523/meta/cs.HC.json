[
    {
        "title": "Tutorly: Turning Programming Videos Into Apprenticeship Learning Environments with LLMs",
        "authors": "Wengxi LiRoy PeaNick HaberHari Subramonyam",
        "links": "http://arxiv.org/abs/2405.12946v1",
        "entry_id": "http://arxiv.org/abs/2405.12946v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12946v1",
        "summary": "Online programming videos, including tutorials and streamcasts, are widely\npopular and contain a wealth of expert knowledge. However, effectively\nutilizing these resources to achieve targeted learning goals can be\nchallenging. Unlike direct tutoring, video content lacks tailored guidance\nbased on individual learning paces, personalized feedback, and interactive\nengagement necessary for support and monitoring. Our work transforms\nprogramming videos into one-on-one tutoring experiences using the cognitive\napprenticeship framework. Tutorly, developed as a JupyterLab Plugin, allows\nlearners to (1) set personalized learning goals, (2) engage in\nlearning-by-doing through a conversational LLM-based mentor agent, (3) receive\nguidance and feedback based on a student model that steers the mentor moves. In\na within-subject study with 16 participants learning exploratory data analysis\nfrom a streamcast, Tutorly significantly improved their performance from 61.9%\nto 76.6% based on a post-test questionnaire. Tutorly demonstrates the potential\nfor enhancing programming video learning experiences with LLM and learner\nmodeling.",
        "updated": "2024-05-21 17:17:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12946v1"
    },
    {
        "title": "Enabling Additive Manufacturing Part Inspection of Digital Twins via Collaborative Virtual Reality",
        "authors": "Vuthea ChheangSaurabh NarainGarrett HootenRobert CerdaBrian AuBrian WestonBrian GieraPeer-Timo BremerHaichao Miao",
        "links": "http://arxiv.org/abs/2405.12931v1",
        "entry_id": "http://arxiv.org/abs/2405.12931v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12931v1",
        "summary": "Digital twins (DTs) are an emerging capability in additive manufacturing\n(AM), set to revolutionize design optimization, inspection, in situ monitoring,\nand root cause analysis. AM DTs typically incorporate multimodal data streams,\nranging from machine toolpaths and in-process imaging to X-ray CT scans and\nperformance metrics. Despite the evolution of DT platforms, challenges remain\nin effectively inspecting them for actionable insights, either individually or\nin a multidisciplinary team setting. Quality assurance, manufacturing\ndepartments, pilot labs, and plant operations must collaborate closely to\nreliably produce parts at scale. This is particularly crucial in AM where\ncomplex structures require a collaborative and multidisciplinary approach.\nAdditionally, the large-scale data originating from different modalities and\ntheir inherent 3D nature pose significant hurdles for traditional 2D\ndesktop-based inspection methods. To address these challenges and increase the\nvalue proposition of DTs, we introduce a novel virtual reality (VR) framework\nto facilitate collaborative and real-time inspection of DTs in AM. This\nframework includes advanced features for intuitive alignment and visualization\nof multimodal data, visual occlusion management, streaming large-scale\nvolumetric data, and collaborative tools, substantially improving the\ninspection of AM components and processes to fully exploit the potential of DTs\nin AM.",
        "updated": "2024-05-21 16:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12931v1"
    },
    {
        "title": "Panmodal Information Interaction",
        "authors": "Chirag ShahRyen W. White",
        "links": "http://arxiv.org/abs/2405.12923v1",
        "entry_id": "http://arxiv.org/abs/2405.12923v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12923v1",
        "summary": "The emergence of generative artificial intelligence (GenAI) is transforming\ninformation interaction. For decades, search engines such as Google and Bing\nhave been the primary means of locating relevant information for the general\npopulation. They have provided search results in the same standard format (the\nso-called \"10 blue links\"). The recent ability to chat via natural language\nwith AI-based agents and have GenAI automatically synthesize answers in\nreal-time (grounded in top-ranked results) is changing how people interact with\nand consume information at massive scale. These two information interaction\nmodalities (traditional search and AI-powered chat) coexist in current search\nengines, either loosely coupled (e.g., as separate options/tabs) or tightly\ncoupled (e.g., integrated as a chat answer embedded directly within a\ntraditional search result page). We believe that the existence of these two\ndifferent modalities, and potentially many others, is creating an opportunity\nto re-imagine the search experience, capitalize on the strengths of many\nmodalities, and develop systems and strategies to support seamless flow between\nthem. We refer to these as panmodal experiences. Unlike monomodal experiences,\nwhere only one modality is available and/or used for the task at hand, panmodal\nexperiences make multiple modalities available to users (multimodal), directly\nsupport transitions between modalities (crossmodal), and seamlessly combine\nmodalities to tailor task assistance (transmodal). While our focus is search\nand chat, with learnings from insights from a survey of over 100 individuals\nwho have recently performed common tasks on these two modalities, we also\npresent a more general vision for the future of information interaction using\nmultiple modalities and the emergent capabilities of GenAI.",
        "updated": "2024-05-21 16:49:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12923v1"
    },
    {
        "title": "From Human-to-Human to Human-to-Bot Conversations in Software Engineering",
        "authors": "Ranim KhojahFrancisco Gomes de Oliveira NetoPhilipp Leitner",
        "links": "http://arxiv.org/abs/2405.12712v1",
        "entry_id": "http://arxiv.org/abs/2405.12712v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12712v1",
        "summary": "Software developers use natural language to interact not only with other\nhumans, but increasingly also with chatbots. These interactions have different\nproperties and flow differently based on what goal the developer wants to\nachieve and who they interact with. In this paper, we aim to understand the\ndynamics of conversations that occur during modern software development after\nthe integration of AI and chatbots, enabling a deeper recognition of the\nadvantages and disadvantages of including chatbot interactions in addition to\nhuman conversations in collaborative work. We compile existing conversation\nattributes with humans and NLU-based chatbots and adapt them to the context of\nsoftware development. Then, we extend the comparison to include LLM-powered\nchatbots based on an observational study. We present similarities and\ndifferences between human-to-human and human-to-bot conversations, also\ndistinguishing between NLU- and LLM-based chatbots. Furthermore, we discuss how\nunderstanding the differences among the conversation styles guides the\ndeveloper on how to shape their expectations from a conversation and\nconsequently support the communication within a software team. We conclude that\nthe recent conversation styles that we observe with LLM-chatbots can not\nreplace conversations with humans due to certain attributes regarding social\naspects despite their ability to support productivity and decrease the\ndevelopers' mental load.",
        "updated": "2024-05-21 12:04:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12712v1"
    },
    {
        "title": "GeckoGraph: A Visual Language for Polymorphic Types",
        "authors": "Shuai FuTim DwyerPeter J. Stuckey",
        "links": "http://arxiv.org/abs/2405.12699v1",
        "entry_id": "http://arxiv.org/abs/2405.12699v1",
        "pdf_url": "http://arxiv.org/pdf/2405.12699v1",
        "summary": "Polymorphic types are an important feature in most strongly typed programming\nlanguages. They allow functions to be written in a way that can be used with\ndifferent data types, while still enforcing the relationship and constraints\nbetween the values. However, programmers often find polymorphic types difficult\nto use and understand and tend to reason using concrete types. We propose\nGeckoGraph, a graphical notation for types. GeckoGraph aims to accompany\ntraditional text-based type notation and to make reading, understanding, and\ncomparing types easier. We conducted a large-scale human study using GeckoGraph\ncompared to text-based type notation. To our knowledge, this is the largest\ncontrolled user study on functional programming ever conducted. The results of\nthe study show that GeckoGraph helps improve programmers' ability to succeed in\nthe programming tasks we designed, especially for novice programmers.",
        "updated": "2024-05-21 11:46:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.12699v1"
    }
]