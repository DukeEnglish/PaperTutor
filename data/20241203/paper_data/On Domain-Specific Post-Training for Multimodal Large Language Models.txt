On Domain-Specific Post-Training for Multimodal Large Language Models
DaixuanCheng1 ShaohanHuang2 ZiyuZhu1,3 XintongZhang1,4 WayneXinZhao5
ZhongzhiLuan2 BoDai1(cid:66) ZhenliangZhang1(cid:66)
1StateKeyLaboratoryofGeneralArtificialIntelligence,BIGAI 2BeihangUniversity
3TsinghuaUniversity 4BeijingInstituteofTechnology 5RenminUniversityofChina
https://huggingface.co/AdaptLLM
Abstract VQA-RAD OPEN VQA-RAD OPEN
Fo 1o 0d 3Seg
42.0
59.8
V CQ LA O- SR EA DD Fo 1o 0d 3Se 2g
3.9
58.0 76.1V CQ LA O- SR EA DD
81.3 Food
R ere ac len mt uy le tia mrs odh aa lve law rgit ene lass ne gd uath ge er map oi dd ed lsev (e Mlo Lp Lm Me sn )t .of Hg oe wn-
-
F 1o 0o 1d
65.3
43 78 .9.9
45.9 56.3
49.258.0S OL PA EK NE 10173.9
72.0
19.9 37.0
46.750.0 60.2
S OL PA EK NE
ever, adapting general MLLMs to specific domains, such Nut 5r Kition 36.1 29. 16 8.6 47.715.262.373.3 CS LL OA SK EE D Nut 5r Kition 41.236.4 18.2 52.4 75.0 CS LL OA SK EE D
asscientificfieldsandindustrialapplications,remainsless 36.5 38.7 17.8
e ax dp al po tr ae td io. nT oh fis Mp La Lp Mer
s
s thy rs ote um ghati pc oa sl tl -y tri an iv ne is nt gig ,a ft oe cs usd io nm ga oi nn Re 1c Mipe 24.8
47.9 78.6
22.9Pa Oth PV ENQA Re 1c Mipe 24.0 45 4.8
6.5 53.6
20.6 Pa Oth PV ENQA
PMC-VQA PathVQA PMC-VQA PathVQA
datasynthesis, trainingpipelines, andtaskevaluation. (1) CLOSED CLOSED
Data Synthesis: Using open-source models, we develop a LLaVA-v1.6-8B Qwen2-VL-2B
visual instruction synthesizer that effectively generates di- AdaMLLM-8B AdaMLLM-2B
verse visual instruction tasks from domain-specific image-
Figure 1. Domain-Specific Performance of AdaMLLM and
caption pairs. Our synthetic tasks surpass those gener-
GeneralMLLM.Foreachofthetwodomains—biomedicineand
ated by manual rules, GPT-4, and GPT-4V in enhancing
food—weconductpost-trainingtoadaptthegeneralMLLMtothe
the domain-specific performance of MLLMs. (2) Training
targetdomainandevaluatemodelperformanceonvariousdomain-
Pipeline: Whilethetwo-stagetraining—initiallyonimage-
specifictasks. Biomedicineandfoodtasksarecoloredgrayand
captionpairsfollowedbyvisualinstructiontasks—iscom- orange,respectively.
monly adopted for developing general MLLMs, we apply
a single-stage training pipeline to enhance task diversity pertiseplummetsinspecializeddomainsduetoinsufficient
fordomain-specificpost-training. (3)TaskEvaluation: We domain-specifictraining[11]. Forinstance,scientificfields
conductexperimentsintwodomains,biomedicineandfood, require learning from specialized images and terminology
by post-training MLLMs of different sources and scales notcommonlyfoundingeneralscenarios[31,39,40,56];
(e.g.,Qwen2-VL-2B,LLaVA-v1.6-8B,Llama-3.2-11B),and andindustrialapplicationsfaceprivacyconstraintsthatlimit
then evaluating MLLM performance on various domain- dataaccessfortraininggeneralMLLMs[3,50].
specifictasks.TosupportfurtherresearchinMLLMdomain Domain-specific training for MLLMs requires diverse
adaptation,wewillopen-sourceourimplementations. visual instruction tasks infused with domain knowl-
edge [21]. Recent efforts in this area [7, 22, 32] typically
followthedatasynthesisandtrainingpipelineusedforgen-
1.Introduction eral training. For data synthesis, manual rules or strong
models are employed to generate domain-specific tasks.
Progress toward building general-purpose agents [20, 58]
However, rule-basedmethodscanlimittaskdiversity[10],
relies critically on the ability to handle multimodal tasks.
closed-source models [1, 34] may pose privacy concerns,
Therecentsuccessoflargelanguagemodels(LLMs)[5,12,
andopen-sourcemodelscanbeconstrainedbyinsufficient
44] has accelerated the development of multimodal large
domainexpertise.Fortraining,manyworksuseatwo-stage
language models (MLLMs) [2, 16, 23, 24, 28, 35, 43, 52,
pipeline—firsttrainingonimage-captionpairs, thenonvi-
54]. Byaligningvisualencoders[37]withLLMs,MLLMs
sual instruction tasks [28]. However, tasks in specialized
have shown impressive capabilities in addressing vision-
domainsareoftenlimited,andsplittingthemintotwostages
language tasks in general scenarios. However, their ex-
canfurtherreducetaskdiversitywithineachstage.
(cid:66)CorrespondingAuthor. In this paper, we systematically investigate domain-
1
4202
voN
92
]LC.sc[
1v03991.1142:viXraspecific data synthesis and training pipeline for MLLM research has shifted to synthesizing domain-specific vi-
post-training. While domain-specific image-caption pairs sual instruction data, following approaches for general
arerelativelyaccessible,domain-specificvisualinstruction MLLMs [21, 59]. These approaches fall into two cate-
tasksaremuchhardertoobtain[22]. Tobridgethisgap,we gories: (1) transforming existing datasets into visual in-
developavisualinstructionsynthesizertoextracttasksfrom struction formats: LLaVA-Chef [32] and FoodLMM [53]
image-caption pairs. Fine-tuned on a seed data collection convert domain-specfic datasets [38] using manual rules
encompassing various domains and tasks, our synthesizer or GPT-4; (2) prompting closed-source models to gener-
can effectively leverage domain knowledge in the image- atevisualinstructiontasksfromimages/annotations: Med-
caption source to generate diverse instruction-response VInT [57], LLaVA-Med [22] and PubMedVision [7] em-
pairs1. Tochecktheaccuracyofsyntheticresponses,rather ployGPT-3.5,GPT-4,andGPT-4V,respectively. Ourwork
than directly verifying each response against the instruc- alignswiththesecondcategoryinsynthesizingdatabased
tion—which requires extensive expertise—we propose se- onimage-captionpairs, butweutilizeopen-sourcemodels
lecting tasks with inherently consistent responses. This andeffectivelysynthesizetasksofgreaterdiversity.
improves accuracy while reducing the need for expert an-
notation. Although generated from open-source models, Domain-Specific Training One type of domain-specific
our synthetic tasks improve model performance more ef- trainingbeginswithanunalignedLLMandvisualencoder.
fectively than those generated by manual rules [32], GPT- SimilartothedevelopmentofgeneralMLLMs,thismethod
4[22],orGPT-4V[7].Usingoursyntheticdata,wepropose usesdomain-specificdatasets,possiblymixedwithgeneral
asingle-stagetrainingpipelinethatcombinesthesynthetic datasets,toaligntheLLMwiththeencoder[7,57].Another
task with the image-caption pair in each training exam- typeispost-training, whichstartswithawell-alignedgen-
ple, avoiding the limitations from two-stage training. This eralMLLM[7,22,32]. Comparedtotrainingfromanun-
simpleapproachenrichestaskdiversityduringtrainingand alignedLLMandvisionencoder,post-trainingismoreeffi-
leadstobetterperformanceinmostexperiments. cient in terms of data and computation, making it our pre-
We conduct experiments in two domains: biomedicine ferredmethod. Indomain-specificpost-training,manypre-
and food, on general MLLMs of different sources and viousworks[7,22,32]adoptthetwo-stagetrainingpipeline
scales, such as Qwen2-VL-2B [45], LLaVA-v1.6-8B [27], originally proposed for general MLLMs: first on image-
andLlama-3.2-VL-11B[14]. AsshowninFigure1,ourre- captionpairs,thenonvisualinstructiontasks. Wesimplify
sultingmodel,AdaMLLM(forAdaptedMultimodalLarge thisintoasingle-stagetrainingpipelinetoenhancetaskdi-
Language Model), consistently outperforms the general versitywithinthetrainingphase.
MLLMsinvariousdomain-specifictasks.
Insummary,ourcontributionsinclude: 3.Method
• Weinvestigatedomain-specificpost-trainingforMLLMs
WeadaptMLLMstodomainsviapost-trainingondomain-
andopen-sourceourimplementations.
specificdatasets.AsshowninFigure2,webeginbysynthe-
• Wedevelopavisualinstructionsynthesisapproachbased
sizingdomain-specifictasksusingaunifiedvisualinstruc-
on open-source models, capable of generating diverse
tion synthesizer, followed by a consistency-based data fil-
tasksfromdomain-specificimage-captionpairswhilere-
ter. These synthetic tasks are then combined with image-
ducingtheneedfordomainexpertise.
captioningtasksintoasinglestageforpost-training.
• We propose a single-stage post-training method to en-
hancetaskdiversityduringthetrainingphase.
3.1.Domain-SpecificVisualInstructionSynthesis
• Our experiments demonstrate the effectiveness of our
methodinimprovingMLLMperformanceacrossvarious Theeffectivenessofvisualinstructiontasksdependsondi-
tasksintwodomains: biomedicineandfood. versityandaccuracy,withdomainknowledgebeingessen-
tialfordomainadaptation[21].Tomeettheserequirements,
2.RelatedWork weproposeadatasynthesisapproachcomprisingtwomain
components: avisualinstructionsynthesizerthatgenerates
We discuss related work on data collection and training diversetasksinfusedwithdomain-specificknowledge,and
pipelinesfordevelopingdomain-specificMLLMs. aconsistency-basedfiltertoenhanceaccuracy.
3.1.1.VisualInstructionSynthesizer
Domain-Specific Data Initially, Med-Flamingo [33] uti-
While domain-specific image-caption pairs are widely ac-
lizes multimodal paired and interleaved data. With the
cessible, domain-specificvisualinstructiontasksaremuch
rise of visual instruction tuning for general MLLMs [28],
harder to obtain [22]. Inspired by Instruction Pre-
1Weuse“task”and“instruction-responsepairs”interchangeably,with
Training[10]whichdevelopsamodeltoextracttasksfrom
“instruction”astaskinputand“response”astaskoutput. texts, we fine-tune an MLLM to generate diverse tasks
2(A)Fine-TuneaVisualInstructionSynthesizeracrossDomainsandTasks
Art & Culture Software/Digital Content Textual Content Animal Medical
…
A watercolor painting featuring Adialog box which is a part A financial document, a section An elderly dog with soulful, It displays the cross-sectional
a group of lorikeets gathered of the language settings in of an annual report detailing cloudy eyes suggesting visual anatomy of a human abdomen
around and on a birdbath … Windows XP… fixed assets of a company … impairment in older dogs … at the level of the kidneys …
Visual Instruction Synthesizer
Instruction
Classify the image based on its Thegoal is to … Is the next What is the computer software Identifythe breed of the dog Was thisa contrast CT or a
domain. or the previous step? as of31/3/2012? in the image. non-contrast CT?
Informative Response
…
The domain ofthis image is Theimage appears to show The “Computer Software” Thedog appears to be a Acontrast-enhanced CT
“painting”. This classification … the step where a user … lineunder the Intangible … Cocker Spaniel, which is … scan. The evidence is …
Precise Response
painting next 510.08 cocker spaniel ContrastCT
Domain-Classification Step-by-Step Guidance Text Detection and OCR Object Recognition Task-Oriented Recognition
(B)SynthesizeDomain-SpecificTaskstoPost-TrainGeneralMLLMs
Domain-Specific Data Format for MLLM Training
Image-Caption Pair Instruction
In which lobe do you see an abnormal Reserve User: <Image>{Instruction}
mass in the above images? as CoT: Assistant:{Informative Response}
Therefore, the answer is {Precise Response}
Visual ✓ Yes
SIn ys nt tr hu ec st ii zo en r P Rr ige hc tis ue p R pee rs p loo bn ese A cre o nre sis sp teo nn ts ?es ✗ No/Open U As sse ir s: taD ne ts :c {ri Cb ae p t th ioe n i }mage.
Informative Response
A black and white The abnormal mass appears to be Dump: User: <Image>Describe the image.
radiograph, known as an located in the right upper lobe of Assistant: {Caption}
X-ray of a human chest ... the lung. This is based on the ...
Figure2.MethodOverview.(A)Wefine-tuneaunifiedvisualinstructionsynthesizerthatgeneratesdiversetasksbasedonimage-caption
pairsacrossvariousdomains.(B)Usingthissynthesizer,wesynthesizetasksbasedondomain-specificimage-captionpairsandthenapply
a consistency-based data filter. The filtered synthetic tasks, combined with the original image captioning tasks, are employed to train
generalMLLMsthroughasingle-stagepost-trainingprocess,MLLMtraininglossiscomputedonlyonthepartcoloredinorange.
based on image-caption pairs across various domains, de- User:<Image>Describetheimage.
veloping a visual instruction synthesizer. Instead of gen- Assistant:{Caption}
erating domain-specific tasks from scratch, which requires User:Answerwithapreciseresponse.{Instruction}
significantexpertise,oursynthesizerextractstasksfromex- Assistant:{PreciseResponse}
isting data, thus reducing the relianceon domain expertise User:Answerwithaninformativeresponse.{Instruction}
fromhumanexpertsorstrongmodels[1,34,41]. Further- Assistant:{InformativeResponse}
more, we incorporate specific designs to handle situations
wherethesynthesizerstrugglestocomprehendtheimage. Table 1. Data Format for Synthesizer Tuning. The prefixes
“User”and“Assistant”aredeterminedbysynthesizer’schattem-
plate.Tuninglossiscomputedonlyonthepartcoloredinorange.
Seed Data across Domains and Tasks We convert a
mativeresponsecontainsmanymoredetails,suchaschain-
combinationofVisionFLAN[51]andALLaVA[6]intothe
of-thoughtreasoning[48]. Theseeddataencompassawide
seeddataforfine-tuningourvisualinstructionsynthesizer.
rangeofimagedomains(e.g.,art,software)andtasktypes
As shown in part (A) of Figure 2, each seed data exam-
(e.g.,objectrecognition,domainclassification). Detailson
ple includes an image-caption pair as input and a related
dataconstructionanddistributionareinAppendixA.
task triplet—comprising an instruction, an informative re-
sponse,andapreciseresponse—asoutput. Specifically,for
each image, both the instruction and precise response are Generalization-Enhanced Multitask Tuning Using the
annotated by human experts, while the caption and infor- seed data, we conduct multitask fine-tuning on an open-
mativeresponsearegeneratedbyGPT-4V.Comparedtothe source MLLM to generate task triplets based on the cor-
precise response which is often a single phrase, the infor- respondingimage-captionpairs. AsshowninTable1,each
3seed data example is converted into a multi-turn conversa- tasks [28]. This pipeline has also been applied in domain-
tiontofittheMLLM’sconversationalformat. Thefirstturn specificpost-training[7,22,32]. However,taskdiversityin
presentstheimage-captionpair. Thenthetasktripletisdi- domain-specifictrainingisoftenmorelimitedthaningen-
videdintotwoconversationalturns. Eachturnstartswitha eraltraining,andsplittingthetrainingintotwostagesmay
promptrequestingeitherapreciseresponseoraninforma- further reduce diversity within each stage, negatively im-
tiveresponse,followedbytheinstruction. Theorderofthe pacting the task generalization of the trained models [47].
preciseandinformativeresponseturnsisshuffledrandomly. Moreover, we empirically find that two-stage training can
Wecalculatethetuninglossonlyontheturnsrelatedtothe leadthemodeltocatastrophicallyforgettheknowledge/task
tasktriplet,ensuringthesynthesizerfocusesonthem. learnedfromthefirststagewhentransitioningtothesecond
Furthermore, since generalizing to unseen image types stage, resulting in worse final performance [30]. To mit-
issubstantiallymorechallengingthantounseentexttypes, igate this, we propose combining the training data into a
we replace 10% of the images with a blank image during singlestage.Asshowninpart(B)ofFigure2,eachtraining
tuning.Thissimulatescaseswherethesynthesizerstruggles examplecanincludetwotasks:
to interpret the image, encouraging it to rely more on the • Image Captioning Task: The image-caption pair is con-
captionwhenfacingdifficultiesinimagecomprehension. verted into an image captioning task [28]. A question
promptingtheMLLMtodescribetheimageisrandomly
chosenfromapoolin[6]asthetaskinstruction,withthe
TaskSynthesisforTargetDomain Aftertuning,weuse
originalcaptionastheground-truthresponse.
thesynthesizertogeneratetasktripletsfromimage-pairsin
• SyntheticVisualInstructionTask:Foreachimage-caption
thetargetdomain. Foreachimage-captionpair,weinputit
pair with a synthetic task after filtering, we combine it
into the synthesizer using the conversational format in Ta-
with the image captioning task in a multi-turn format,
ble1andextractthetasktripletfromtheoutputaccordingly.
with the task order randomized. If no synthetic task re-
3.1.2.Consistency-BasedFilter mains,onlytheimagecaptioningtaskisused.
Developed from an open-source model without sufficient Following[28],wetrainonthedatausingthenext-token
domainexpertise,oursynthesizerinevitablyproducessome prediction objective [36], computing loss only on the re-
inaccurateresponses,necessitatingdatafiltering. Toreduce sponsepartofeachinstruction-responsepair.
relianceonexpertsforvalidation,weproposefilteringtasks
4.ExperimentSettings
basedoninherentconsistency. Thisisinspiredbyensemble
methods [13] and self-consistency decoding [46], enhanc-
Given current data availability, we conduct experiments in
ingaccuracywhilereducingtheneedforexpertannotation.
twodomains: biomedicineandfood. Foreachdomain,we
As shown in part (B) in Table 2, we prompt an open-
performpost-trainingtoadaptgeneralMLLMsandevaluate
sourcelanguagemodeltoclassifyeach“instruction-precise
modelperformanceonvariousdomain-specifictasks.
response-informativeresponse”tripletintooneofthreecat-
egories: consistent, inconsistent, or open. The consistent
Image-Caption Data Source For biomedicine, we use
andinconsistentcategoriesindicatewhetherthepreciseand
twosourcesfromPubMedCentral2:(1)PMCRaw[55]from
informative responses align, while the open category indi-
LLavaMed [22], comprising 470K available images with
cates tasks that request open-ended responses (e.g., image
human-annotatedcaptions,and(2)PMCRefined fromPub-
captionsorbackgroundinformation). Theprompttemplate
MedVision [7], featuring 510K image-caption pairs where
isinFigure6inAppendix. Wediscardtripletsclassifiedas
thecaptionshavebeenrefinedbyGPT-4V.Forfooddomain,
inconsistent, astheyshowlowsynthesizerconfidence, and
weconvertRecipe1M[38]datasetintoimage-captionpairs.
thoseclassifiedasopenduetotheirambiguity.
Eachrecipeismatchedwithoneimageandconvertedtoa
For consistent triplets, we combine the informative and
captionusingtemplatesfrom[32]. Recipeswithoutimages
preciseresponsesintoachain-of-thought(CoT)formatus-
areremoved,resultingin130Kimage-captionpairs.
ingdiversetemplatesfrom[29]. Theinformativeresponse
serves as the reasoning process, and the precise response
serves as the final conclusion. This ensures both informa- VisualInstructionSynthesis Ourvisioninstructionsyn-
tivenessandaccuracy,balancingtheconciseresponsestyp- thesizer is fine-tuned from the open-source version [8] of
icalofacademicdatasetswiththemoredetailedresponses LLaVA-v1.6-Llama3-8B [27]. For the consistency-based
preferredbyhumans[26,51]. filter, we prompt Llama-3-8B [14] to evaluate the consis-
tency of each synthesized task triplet. On average, about
3.2.Domain-SpecificSingle-StagePost-Training 30%ofthetasktripletsarereservedafterfiltering. Details
onimplementationsandsyntheticdataareinAppendixB.
GeneralMLLMtrainingtypicallycontainsmultiplestages:
first on image-caption pairs, then on visual instruction 2https://www.ncbi.nlm.nih.gov/pmc/
4SLAKE PathVQA VQA-RAD
Biomedicine PMC-VQA
OPEN CLOSED OPEN CLOSED OPEN CLOSED
GPT-4o 59.1 71.6 24.1 76.0 51.6 64.0 56.7
LLaVA-v1.6-8B 49.2 62.3 15.2 47.7 45.9 56.3 36.5
LLaVA-Med-8B 43.4 50.2 10.1 59.2 35.0 62.5 37.1
PubMedVision-8B 50.0 68.3 17.0 67.5 43.3 67.3 40.4
AdaMLLM-8BfromPMCRaw 56.8 76.4 19.7 79.3 51.0 80.5 44.3
AdaMLLM-8BfromPMCRefined 58.0 73.3 22.9 78.6 59.8 81.3 47.9
Qwen2-VL-2B 50.0 52.4 17.8 38.7 37.0 46.7 45.8
LLaVA-Med-2B 43.4 55.5 11.8 60.1 37.1 58.8 41.2
PubMedVision-2B 45.2 63.2 18.2 64.7 41.3 67.3 43.2
AdaMLLM-2BfromPMCRaw 53.2 75.2 20.1 63.8 49.8 74.6 43.5
AdaMLLM-2BfromPMCRefined 60.2 75.0 20.6 53.6 58.0 76.1 46.5
Llama-3.2-11B 56.2 63.9 22.7 72.1 46.9 63.6 51.9
LLaVA-Med-11B 47.6 58.7 14.6 69.5 38.0 69.1 47.5
PubMedVision-11B 49.1 74.3 19.3 70.9 46.2 73.9 47.1
AdaMLLM-11BfromPMCRaw 56.7 77.6 22.2 87.3 55.0 76.1 49.9
AdaMLLM-11BfromPMCRefined 59.5 76.4 24.3 84.9 57.4 79.8 51.9
Table2.BiomedicineTaskPerformanceofgeneralMLLMsandMLLMsafterdomain-adaptivetraining.Wemarkthebestperformance
boldandthesecond-bestunderlined. Theimage-captionsourcesfor AdaMLLMfromPMCRaw and AdaMLLMfromPMCRefined
arePMCRawandPMCRefined,respectively.
Post-Training Using synthetic data from the LLaVA- Food Recipe Nutrition Food101 FoodSeg
v1.6-Llama3-8B-based synthesizer, we conduct domain-
GPT-4o 26.1 46.6 89.4 61.9
specific post-training on LLaVA-v1.6-Llama3-8B itself.
LLaVA-v1.6-8B 18.6 29.6 47.9 38.9
Besides, we use the same synthetic data to post-train
LLaVA-Chef-8B 23.1 29.1 46.8 14.5
Qwen2-VL-2B-Instruct [45] and Llama-3.2-11B-Vision-
AdaMLLM-8B 24.8 36.1 65.3 42.0
Instruct [14] to assess effectiveness across different mod-
Qwen2-VL-2B 18.2 36.4 73.9 19.9
els and scales. For simplicity, we refer to these models as
LLaVA-Chef-2B 24.1 24.5 68.8 7.7
LLaVA-v1.6-8B, Qwen2-VL-2B, and Llama-3.2-11B, re-
AdaMLLM-2B 24.0 41.2 72.0 23.9
spectively. TrainingdetailsareinAppendixC.
Llama-3.2-11B 23.7 40.0 80.8 47.6
LLaVA-Chef-11B 25.7 26.2 82.1 16.7
Task Evaluation We evaluate MLLMs on domain- AdaMLLM-11B 26.1 41.0 82.2 42.0
specifictaskswithoutfurtherfine-tuning. Forbiomedicine,
we evaluate on SLAKE [25], PathVQA [15], VQA- Table 3. Food Task Performance of general MLLMs and
RAD[19],andPMC-VQA[57]. Forfooddomain,weeval- MLLMs after domain-adaptive training. We mark the best per-
formanceboldandthesecond-bestunderlined.
uateonRecipe1M[38],FoodSeg103[49],Food101[4],and
Nutrition5k[42]. DetailsareinAppendixE.
domains, using GPT-4o [17] as a reference. Our method
consistentlyenhancesMLLMperformance, outperforming
Baseline Forbiomedicinedomain,wecomparewithtwo baseline models across various domain-specific tasks. Al-
baselines: (1)LLaVA-Med[22]whichusestext-onlyGPT- though our synthesizer is based on LLava-v1.6-8B, we
4 to synthesize tasks from PMCRaw, and (2) PubMed- observe consistent improvements on Qwen2-VL-2B and
Vision [7] which uses GPT-4V to synthesize tasks from Llama-3.2-11B, demonstrating its effectiveness across dif-
PMCRefined. Forfooddomain,wecomparewithLLaVA- ferent models and scales. Among the evaluated tasks,
Chef [32] which uses manual rules to transform image- VQA-RADandRecipe1Mcanberegardedaspartiallyseen
recipe pairs from Recipe1M into multiple tasks. All base- tasks, with VQA-RAD included in our seed data for fine-
linemethodsemploytwo-stagepost-training. tuningthesynthesizerandRecipe1Mincludedintheimage-
caption source3. Nevertheless, AdaMLLM shows consis-
5.MainResults tentgainsonotherunseentasks,demonstratingitstaskgen-
eralizationcapabilitiesinthetargetdomain.
OverallPerformance Tables2and3compareourmod-
els (AdaMLLM) with others in the biomedicine and food 3Test/validationsetsofVQA-RADandRecipe1Marenotincluded.
5Image-Caption Recipe1M PMCRaw PMCRefined
TrainPipeline Two-stage Single-stage Two-stage Single-stage Two-stage Single-stage
Instruction Rule Ours Rule Ours GPT-4 Ours GPT-4 Ours GPT-4V Ours GPT-4V Ours
LLaVA-v1.6-8B 28.4 29.0 34.1 42.0↑ 42.5 55.6 46.1 58.3↑ 50.5 58.6 55.5 60.3↑
Qwen2-VL-2B 31.3 38.2 31.9 40.3↑ 44.0 55.5 41.3 54.3↓ 49.0 59.5 51.6 55.7↓
Llama-3.2-11B 37.7 40.9 36.6 47.8↑ 49.3 59.2 48.8 60.7↑ 54.4 60.3 53.7 62.0↑
Table4. Domain-SpecificTaskPerformanceofMLLMsafterPost-Trainingwithdifferentsyntheticdataandtrainingpipelines. We
reporttheaverageperformanceineachdomain,withdetailedresultsinTable14inAppendix.Whentheimage-captionsourceandtraining
pipelinearefixed,syntheticdataofbetterperformancearemarkedinbold.Whentheimage-captionsourceisfixedandoursyntheticdata
areused,numbersmarkedwith↑indicatethatsingle-stagetrainingoutperformstwo-stagetraining,while↓indicatestheopposite.
w/o w/oConsistencyFilter w/o w/o General GeneralTask+
Ours
BlankImage SyntheticTask ImageCaption Task DomainCaption
Precise Informative
BioMed. 58.3 55.8 31.2 44.4 26.7 54.2 49.8 55.3
Food 42.0 35.9 37.9 37.6 25.6 36.8 36.0 38.6
Table 5. Ablation Results. “w/o Blank Image” fine-tunes the synthesizer without replacing 10% of images with blank ones. “w/o
Consistency Filter” removes the consistency-based filter and trains with either precise or informative responses. “w/o Synthetic Task”
removessynthetictask,and“w/oImageCaption”removesimagecaptioningtask. “GeneralTask”trainsonseeddataprocessedintoour
taskformat,“GeneralTask+DomainCaption”mixestheprocessedseeddatawithdomain-specificimage-captionpairs.
Comparison of Synthetic Task and Training Pipeline Toimproveresponseaccuracy,wedesignaconsistency-
In addition to the overall comparison, we assess the effec- based filter to select tasks with inherent consistency and
tivenessofoursyntheticvisualinstructiontasksandsingle- combine the informative and precise responses into a CoT
stagetrainingseparatelybyvaryingonefactoratatime. As format. As shown in Table 5, removing this filter results
shown in Table 4, we conduct both two-stage and single- indecreasedmodelperformance,regardlessofwhetherthe
stage post-training with synthetic tasks generated by dif- responsecontainsonlypreciseorinformativecontent.
ferent methods: manual rules in LLaVA-Chef, GPT-4 in
LLaVA-Med,andGPT-4VinPubMedVision.Oursynthetic
Single-Stage Post-Training Our motivation for combin-
tasks consistently outperform others across both training
ingthesynthetictaskwiththeimagecaptioningtaskintoa
pipelines. Furthermore, with our synthetic tasks, single-
single stage is to enhance training task diversity. This ef-
stage training surpasses two-stage training in most of the
ficacy is evident in the ablation results in Table 5, where
experiments. WediscussinTable14inAppendix,inferring
removing either the synthetic task or the image captioning
thattwo-stagetrainingleadsthemodeltoforgettheknowl-
taskfromthetrainingdataharmsthemodelperformance.
edge/tasklearnedinthefirststagewhentransitioningtothe
secondstage,resultinginworseperformanceacrosstasks.
DomainKnowledge Ourdomain-specificvisualinstruc-
tion tasks incorporate both domain knowledge and visual
6.Ablations
instruction task format. To evaluate the contributions of
To evaluate the effectiveness of each component in our domainknowledge,weconductacomparisonwithgeneral
method,weconductablationstopost-trainLLaVA-v1.6-8B visual instruction tasks. Our seed data for fine-tuning the
with different settings. We report the average task perfor- synthesizer,whichcontainsdiversetasksacrossvariousdo-
mance within each domain for the trained models in Ta- mains, serves as the source for general tasks. To ensure
ble 5. The image-caption sources for the biomedicine and alignment in visual instruction task format, we apply our
fooddomainsarePMCRaw andRecipe1M,respectively. consistency-based filter to process and reformat the seed
dataintoourformat. AsshowninTable5,ourmethodout-
performsbothsettingsthatutilizegeneraltasks,underscor-
Visual Instruction Synthesis To simulate scenarios
ingtheeffectivenessofincorporatingdomainknowledge.
wherethemodelstrugglestointerprettheimage,wereplace
10%oftheimageswithblankimagesduringthefine-tuning
7.Analysis
ofvisualinstructionsynthesizer.Theimpactofthisstrategy
isdemonstratedinTable5,whereremovingthisdesignre- Weconductadetailedanalysisofourapproachfordomain-
sultsinadeclineinmodelperformance. specific visual instruction synthesis and the synthesized
6FinetuneInput - Image Caption Image+Caption Image-Caption Recipe1M PMCRaw PMCRefined
BlankImage - - - ✗ ✓ Instruction Rule Ours GPT-4 Ours GPT-4V Ours
Diversity 52.5 68.0 75.2 81.0 85.5 Diversity 23.5 52.9 47.1 58.8 64.7 76.5
Knowledge 72.5 95.0 93.8 97.5 98.1 Knowledge 20.9 21.9 44.9 58.9 67.7 63.2
Complexity 43.8 77.9 75.3 80.0 83.2 Complexity 38.4 69.9 41.7 83.2 49.6 80.5
Accuracy 63.8 60.0 65.6 66.3 71.3 Accuracy 98.7 84.3 84.4 75.1 87.5 79.6
Table 6. Quality of Synthetic Tasks by Different Visual In- Table8.QualityofSyntheticTasksbyourmethod,manualrules,
struction Synthesizers, assessed in terms of task diversity, do- GPT-4, and GPT-4V, assessed in terms of task diversity, domain
mainknowledgeutilization, taskcomplexity, andresponseaccu- knowledgeutilization,taskcomplexity,andresponseaccuracy.
racy. Column 1 presents results from the MLLM without fine-
tuning. Columns 2-5 show results after fine-tuning the MLLM Task-Oriented Image Recognition
usingourseeddatatosynthesizetasksbasedondifferentinputs. Attribute and Context Recognition
Besides,Column5replaces10%oftheimageswithblankimages. Object Recognition
Data Representation
Step-by-Step Guidance
w/oFilter w/Filter
Anomaly Detection
Consist. PreciseAcc Info.Acc Consist. Acc Image-Text Matching
Caption Generation
BioMed. 30.3 64.3 61.0 92.2 75.1
Text Detection and OCR
Food 35.7 77.2 75.5 97.1 84.3
Scene Classification
Domain Classification
Table7.QualityofResponseswith/withoutUsingConsistency-
Sentiment Analysis
Based Filter, assessed in terms of consistency between pre-
Pose and Activity Recognition
ciseandinformativeresponses(Consist.),accuracyofprecisere-
Face and Expression Classification
sponses (Precise Acc), accuracy of informative responses (Info.
Utility and Affordance Recognition
Acc),andaccuracyofcombinedresponses(Acc).
Figure3.TaskTypeDistributionofalloursynthetictasksbased
datatounderstandtheirimpactonmodelperformance. onthreeimage-captionsources.
7.1.Domain-SpecificVisualInstructionSynthesis
blankonesachievesthehighestqualityacrossallmetrics.
VisualInstructionSynthesizer Wefine-tuneanMLLM
to synthesize tasks from image-caption pairs and incorpo-
Consistency-BasedFilter Ourconsistency-basedfilteris
rate a design where 10% of the images are replaced with
designed to select tasks with inherent consistency, thereby
blank images. To evaluate the effectiveness, we compare
increasing data accuracy. As shown in Table 7, using the
tasksgeneratedbysynthesizerswithdifferentdesignsusing
filtersignificantlyincreasestheconsistencybetweenprecise
avalidationsetfromourseeddata.Specifically,weconduct
andinformativeresponses,makingthecombinationofthem
human evaluation of data quality in the following aspects
inthechain-of-thoughtformatreasonable. Asaresult, the
(detailedscoringcriteriaareprovidedinAppendixD):
filtersuccessfullyincreaseresponseaccuracyoftheselected
• Task Diversity: Classifies each instruction-response pair
databyabout10%inbothdomains.
as one of the common visual instruction task types and
reports the number of distinct types normalized by the
7.2.Domain-SpecificSyntheticData
totalnumberofcommontasktypes.
• Domain Knowledge Utilization: Measures the extent to Quantitative Analysis Table 8 presents the data quality
whichdomain-specificknowledgefromtheimageisuti- scores for synthetic tasks generated by different methods.
lizedtocompletethetask. Our tasks are diverse and complex, demonstrating a high
• Task Complexity: Assesses task complexity, with higher utilization of domain knowledge. The distribution of task
scores for tasks requiring reasoning and instruction- types for all our instruction-response pairs is displayed in
followingabilities. Figure 3. This explains the effectiveness of our method
• Response Accuracy: Evaluates how accurately the re- in enhancing MLLM performance across domain-specific
sponseaddressestheinstruction. tasks. However, our method underperforms the baselines
The results in Table 6 indicate that fine-tuning for task in terms of response accuracy, with manual rules achiev-
synthesis using either image [59] or caption [10] inputs ingnearly100%accuracyandGPT-4andGPT-4Vreaching
yields improvements in most aspects. Our design, which around 85%. This may because of the increased complex-
employsbothimageandcaptioninputs,leadstoevenhigher ity of our synthesized tasks, which make generating accu-
performance. Besides, replacing 10% of the images with rateresponsesmorechallenging. Theseresultsindicatethe
7(A) Caption (B)
The dish in this image is called Serrano Creamed Spinach Stuffed
Tomatoes.Step-by-step instructions:
1. Cook spinach according to package directions then drain and squeeze
out water. (…)
5. Stuff tomatoes with spinach and sprinkle with more cheese. (…)
Rule-Based Instruction-Response Pair
Instruction: Detail the steps for cooking Serrano Creamed Spinach Stuffed Tomatoes.
Response: 1. Cook spinach according to package directions then drain and squeeze out (…)
5. Stuff tomatoes with spinach and sprinkle with more Parmesan cheese(…)
Our Instruction-Response Pair
Instruction: What is the step?
Response: The step depicted in the image is the preparation of a stuffed tomato, (…) The tomato
has been hollowed out and is being filled with a mixture that includes cooked spinach, cream
cheese, Parmesan cheese, salt, and garlic powder. The answer is stuffingthe tomato.
(C) Caption
The image displays a
collection of medical
imaging and graphical Caption
data. Image (a) and (b) are
MRI scans in the coronal Algorithm of the treatment approach used in our institution. BSC best supportive
section showing the pelvic care, CHT chemotherapy, CT computed tomography, EPD extended
area, depicting a thin pleurectomy/decortication, EPP extrapleural pneumonectomy, MPM malignant
white line (thin arrow) pleural mesothelioma, PET positron emission tomography Scan, RT radiotherapy.
passing near a denser GPT-4 Instruction-Response Pair
bundle (thick arrow),
assumed to be the Instruction: What is the purpose of the algorithm?
obturator bundle in the Response: The purpose of the algorithm is to outline the treatment approach
right obturator fossa. used in the institution for managing a specific medical condition. It provides a
Image (c) shows an axial step-by-step guide for healthcare professionals to follow, helping them make
section of an MRI scan decisions about the most appropriate course of action for each patient based on
presenting detailed their individual circumstances, such as their response to neoadjuvant
internal anatomy of the chemotherapy, suitability for surgery, and the feasibility of radiotherapy.
pelvic.(…) Our Instruction-Response Pair
GPT-4V Instruction-Response Pair Instruction: Context: Our institution has a standardized treatment approach for
Instruction: I noticed a thin white line in the ultrasound image (e); what does that represent? malignant pleural mesothelioma (MPM). The algorithm is based on the
Response: In the ultrasound image (e), the thin white line you see represents a type of internal histological subtype and the clinical stage.
structure, possibly a medical device or tape, used in procedures related to the pelvic region. (…) Can the context support "Surgery is not recommended for mesothelioma"?
Our Instruction-Response Pair Options: (a) no (b) yes (c) not sure
Response: The context provided does not support the statement “Surgery is not
Instruction: The given image contains 6 image choices numbered A through F. Select the least
recommended for mesothelioma”.The algorithm presented in the image outlines
similar image among the group.
a specific treatment approach for malignant pleural mesothelioma (MPM) that
Response: The task is to identify the least similar image among the group. Images (a), (b), and (c)
includes surgical options, such as extra pleural pneumonectomy (EPP) and
are MRI scans of the pelvic region, (…)Image (f) is a graph representing electromyography data,
extended pleurectomy/decortication (EPD). (…)The correct answer to the
which is significantly different from the other images as it does not depict anatomical structures
question is: (a) no
but rather physiological data. Therefore, the least similar image among the group is image (f).
Figure4.CasesofInstruction-ResponsePairssynthesizedbyourmethod,manualrules,GPT-4,andGPT-4V,theimage-captionsources
forcases(A),(B),(C)areRecipe1M,PMCRawandPMCRefined,respectively.Certainportionsareomittedandarerepresentedas(...).
need for further improvements to enhance response accu- providedinFigure8inAppendix.
racy,evenwithhighlycomplextasks.
8.Conclusion
QualitativeAnalysis Figure4presentscasesofsynthetic This paper investigates adapting general MLLMs to
tasks by different methods when given the same image- specific domains via post-training. To synthesize domain-
caption pair. In case (A), the rule-based task is a simple specific visual instruction tasks, we develop a unified
transformationoftherecipecaption,ignoringtheimagein- visual instruction synthesizer that generates instruction-
formation. Incontrast,ourtaskconductsadetailedanalysis response pairs based on domain-specific image-caption
of the food’s state in the image and accurately matches it data, and then apply a consistency-based filter to improve
withthecookingstepinthecaption,demonstratingahigher data accuracy. This enables us to effectively synthesize
diverse tasks with high domain knowledge utilization.
levelofdomainknowledgeutilization.Incase(B),bothour
For the post-training pipeline, we propose combining the
task and the GPT-4 synthesized task focus on interpreting
synthetic tasks with image-captioning tasks into a single
intent. WhiletheGPT-4taskstraightforwardlyasksforthe
trainingstagetoenhancetaskdiversity. Intwodomains—
intent,ourtaskincreasestaskcomplexitybyrequiringinfer-
biomedicine and food—our resulting model, AdaMLLM,
ence from the context to make a “yes/no/not sure” choice. consistently outperforms general MLLMs across various
In case (C) with multiple sub-images, our task type is dis- domain-specific tasks. We hope our work can inspire
tinctinrequiringtheidentificationoftheleastsimilarimage further exploration into MLLM domain adaptation, em-
amongthegroup,showcasingtaskdiversity.Morecasesare poweringmodelsfordownstreamtasksinspecializedareas.
8pathways. JournalofMachineLearningResearch,24(240):
1–113,2023. 1
References [13] ThomasGDietterich. Ensemblemethodsinmachinelearn-
ing.InInternationalworkshoponmultipleclassifiersystems,
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah- pages1–15.Springer,2000. 4
mad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida, [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-
JankoAltenschmidt, SamAltman, ShyamalAnadkat, etal. hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil
Gpt-4 technical report. arXiv preprint arXiv:2303.08774, Mathur,AlanSchelten,AmyYang,AngelaFan,etal. The
2023. 1,3 llama 3herd ofmodels. arXiv preprint arXiv:2407.21783,
[2] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine 2024. 2,4,5
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men- [15] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and
sch,KatherineMillican,MalcolmReynolds,etal.Flamingo: PengtaoXie. Pathvqa: 30000+questionsformedicalvisual
a visual language model for few-shot learning. Advances questionanswering.arXivpreprintarXiv:2003.10286,2020.
inneuralinformationprocessingsystems,35:23716–23736, 5,14
2022. 1 [16] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
[3] GaganBhatia,ElMoatezBillahNagoudi,HasanCavusoglu, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
and Muhammad Abdul-Mageed. Fintral: A family of gpt- Owais Khan Mohammed, Barun Patra, et al. Language is
4 level multimodal financial large language models. arXiv not all you need: Aligning perception with language mod-
preprintarXiv:2402.10986,2024. 1 els. Advances in Neural Information Processing Systems,
[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 36:72096–72109,2023. 1
Food-101–mining discriminative components with random [17] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-
forests. In Computer vision–ECCV 2014: 13th European man,AdityaRamesh,AidanClark,AJOstrow,AkilaWeli-
conference,zurich,Switzerland,September6-12,2014,pro- hinda,AlanHayes,AlecRadford,etal. Gpt-4osystemcard.
ceedings,partVI13,pages446–461.Springer,2014. 5,14 arXivpreprintarXiv:2410.21276,2024. 5
[18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng,
[5] Tom B Brown. Language models are few-shot learners.
arXivpreprintarXiv:2005.14165,2020. 1 Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao
Zhang, and Ion Stoica. Efficient memory management for
[6] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Juny-
largelanguagemodelservingwithpagedattention. InPro-
ingChen, XiangboWu, ZhiyiZhang, ZhihongChen, Jian-
ceedingsoftheACMSIGOPS29thSymposiumonOperating
quanLi, XiangWan, andBenyouWang. Allava: Harness-
SystemsPrinciples,2023. 12
inggpt4v-synthesizeddataforalitevision-languagemodel.
[19] JasonJLau, SoumyaGayen, AsmaBenAbacha, andDina
arXivpreprintarXiv:2402.11684,2024. 3,4,12
Demner-Fushman. A dataset of clinically generated visual
[7] Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian
questions and answers about radiology images. Scientific
Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang,
data,5(1):1–10,2018. 5,14
ZhenyangCai,KeJi,GuangjunYu,etal. Huatuogpt-vision,
[20] YannLeCun. Apathtowardsautonomousmachineintelli-
towardsinjectingmedicalvisualknowledgeintomultimodal
gence. OpenReview,2022. 1
llmsatscale. arXivpreprintarXiv:2406.19280,2024. 1,2,
[21] Chen Li, Yixiao Ge, Dian Li, and Ying Shan. Vision-
4,5
language instruction tuning: A review and analysis. arXiv
[8] Lin Chen and Long Xing. Open-llava-next: An open-
preprintarXiv:2311.08172,2023. 1,2
sourceimplementationofllava-nextseriesforfacilitatingthe
[22] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,
largemulti-modalmodelcommunity.https://github.
HaotianLiu,JianweiYang,TristanNaumann,HoifungPoon,
com/xiaoachen98/Open-LLaVA-NeXT,2024. 4,13
and Jianfeng Gao. Llava-med: Training a large language-
[9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui
and-visionassistantforbiomedicineinoneday.Advancesin
He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:
NeuralInformationProcessingSystems,36,2024. 1,2,4,5,
Improving large multi-modal models with better captions.
14
arXivpreprintarXiv:2311.12793,2023. 13
[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
[10] DaixuanCheng,YuxianGu,ShaohanHuang,JunyuBi,Min- Blip-2: Bootstrapping language-image pre-training with
lieHuang,andFuruWei.Instructionpre-training:Language frozen image encoders and large language models. In In-
models are supervised multitask learners. arXiv preprint ternational conference on machine learning, pages 19730–
arXiv:2406.14491,2024. 1,2,7 19742.PMLR,2023. 1
[11] DaixuanCheng, ShaohanHuang, andFuruWei. Adapting [24] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Moham-
large language models via reading comprehension. In The madShoeybi, andSongHan. Vila: Onpre-trainingforvi-
Twelfth International Conference on Learning Representa- suallanguagemodels.InProceedingsoftheIEEE/CVFCon-
tions,2024. 1 ferenceonComputerVisionandPatternRecognition,pages
[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, 26689–26699,2024. 1
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul [25] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and
Barham, Hyung Won Chung, Charles Sutton, Sebastian Xiao-MingWu. Slake: Asemantically-labeledknowledge-
Gehrmann, et al. Palm: Scaling language modeling with enhanceddatasetformedicalvisualquestionanswering. In
92021 IEEE 18th International Symposium on Biomedical [39] WenhaoShi,ZhiqiangHu,YiBin,JunhuaLiu,YangYang,
Imaging(ISBI),pages1650–1654.IEEE,2021. 5,14 See-KiongNg, LidongBing, andRoyKa-WeiLee. Math-
[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. llava:Bootstrappingmathematicalreasoningformultimodal
Improved baselines with visual instruction tuning. In Pro- large language models. arXiv preprint arXiv:2406.17294,
ceedingsoftheIEEE/CVFConferenceonComputerVision 2024. 1
andPatternRecognition,pages26296–26306,2024. 4 [40] Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, and
[27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhiqiang Tao. Stllava-med: Self-training large lan-
Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im- guage and vision assistant for medical. arXiv preprint
provedreasoning,ocr,andworldknowledge,2024. 2,4 arXiv:2406.19973,2024. 1
[41] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-
[28] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
BaptisteAlayrac,JiahuiYu,RaduSoricut,JohanSchalkwyk,
Visual instruction tuning. Advances in neural information
AndrewMDai,AnjaHauth,KatieMillican,etal.Gemini:a
processingsystems,36,2024. 1,2,4,13
familyofhighlycapablemultimodalmodels. arXivpreprint
[29] Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
arXiv:2312.11805,2023. 3
HyungWonChung,YiTay,DennyZhou,QuocVLe,Bar-
[42] Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia,
ret Zoph, Jason Wei, et al. The flan collection: Designing
LiviuPanait,TobiasWeyand,andJackSim.Nutrition5k:To-
dataandmethodsforeffectiveinstructiontuning. InInter-
wards automatic nutritional understanding of generic food.
national Conference on Machine Learning, pages 22631–
In Proceedings of the IEEE/CVF Conference on Computer
22648.PMLR,2023. 4
VisionandPatternRecognition,pages8903–8911,2021. 5,
[30] YunLuo,ZhenYang,FandongMeng,YafuLi,JieZhou,and
14
YueZhang. Anempiricalstudyofcatastrophicforgettingin
[43] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun
large language models during continual fine-tuning. arXiv
Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,
preprintarXiv:2308.08747,2023. 4,17
ShushengYang,AdithyaIyer,XichenPan,etal. Cambrian-
[31] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai
1: A fully open, vision-centric exploration of multimodal
Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open mul-
llms. arXivpreprintarXiv:2406.16860,2024. 1
timodalgenerativepre-trainedtransformerforbiomedicine.
[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
arXivpreprintarXiv:2308.09442,2023. 1
Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste
[32] FnuMohbatandMohammedJZaki. Llava-chef: Amulti-
Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
modalgenerativemodelforfoodrecipes. InProceedingsof
Llama: Open and efficient foundation language models.
the33rdACMInternationalConferenceonInformationand
arXivpreprintarXiv:2302.13971,2023. 1
KnowledgeManagement,pages1711–1721,2024. 1,2,4,5
[45] PengWang,ShuaiBai,SinanTan,ShijieWang,ZhihaoFan,
[33] Michael Moor, Qian Huang, Shirley Wu, Michihiro Ya-
Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
sunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Ed-
Ge, et al. Qwen2-vl: Enhancing vision-language model’s
uardoPontesReis,andPranavRajpurkar. Med-flamingo: a perception of the world at any resolution. arXiv preprint
multimodalmedicalfew-shotlearner. InMachineLearning arXiv:2409.12191,2024. 2,5
forHealth(ML4H),pages353–367.PMLR,2023. 2
[46] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed
[34] OpenAI. Gpt-4v(ision)systemcard. 2023. 1,3 Chi, Sharan Narang, Aakanksha Chowdhery, and Denny
[35] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan Zhou. Self-consistency improves chain of thought reason-
Huang, ShumingMa, QixiangYe, andFuruWei. Ground- ing inlanguage models. arXiv preprintarXiv:2203.11171,
ingmultimodallargelanguagemodelstotheworld. InThe 2022. 4
Twelfth International Conference on Learning Representa- [47] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,
tions,2024. 1 AdamsWeiYu,BrianLester,NanDu,AndrewMDai,and
[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya QuocVLe. Finetunedlanguagemodelsarezero-shotlearn-
Sutskever,etal. Improvinglanguageunderstandingbygen- ers. arXivpreprintarXiv:2109.01652,2021. 4
erativepre-training. 2018. 4 [48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Chain-of-thought prompting elicits reasoning in large lan-
AmandaAskell,PamelaMishkin,JackClark,etal.Learning guage models. Advances in neural information processing
transferable visual models from natural language supervi- systems,35:24824–24837,2022. 3
sion.InInternationalconferenceonmachinelearning,pages [49] XiongweiWu,XinFu,YingLiu,Ee-PengLim,StevenCH
8748–8763.PMLR,2021. 1 Hoi,andQianruSun. Alarge-scalebenchmarkforfoodim-
[38] AmaiaSalvador,NicholasHynes,YusufAytar,JavierMarin, agesegmentation. InProceedingsofthe29thACMinterna-
FerdaOfli,IngmarWeber,andAntonioTorralba. Learning tionalconferenceonmultimedia, pages506–515, 2021. 5,
cross-modal embeddings for cooking recipes and food im- 14
ages. In Proceedings of the IEEE conference on computer [50] QianqianXie, DongLi, MengxiXiao, ZihaoJiang, Ruoyu
visionandpatternrecognition,pages3020–3028,2017. 2, Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang
4,5,14 Han, Yuzhe Yang, et al. Open-finllms: Open multimodal
10large language models for financial applications. arXiv
preprintarXiv:2408.11878,2024. 1
[51] Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying
Shen, Di Jin, Yu Cheng, Qifan Wang, and Lifu Huang.
Vision-flan: Scaling human-labeled tasks in visual instruc-
tion tuning. arXiv preprint arXiv:2402.11690, 2024. 3, 4,
12
[52] YuanYao,TianyuYu,AoZhang,ChongyiWang,JunboCui,
HongjiZhu,TianchiCai,HaoyuLi,WeilinZhao,ZhihuiHe,
etal.Minicpm-v:Agpt-4vlevelmllmonyourphone.arXiv
preprintarXiv:2408.01800,2024. 1
[53] YuehaoYin, HuiyanQi, BinZhu, JingjingChen, Yu-Gang
Jiang, and Chong-Wah Ngo. Foodlmm: A versatile food
assistant using large multi-modal model. arXiv preprint
arXiv:2312.14991,2023. 2
[54] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao,
Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan,
Songyang Zhang, Shuangrui Ding, et al. Internlm-
xcomposer: A vision-language large model for advanced
text-imagecomprehensionandcomposition. arXivpreprint
arXiv:2309.15112,2023. 1
[55] ShengZhang, YanboXu, NaotoUsuyama, JaspreetBagga,
Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen
Valluri,CliffWong,etal. Large-scaledomain-specificpre-
training for biomedical vision-language processing. arXiv
preprintarXiv:2303.00915,2(3):6,2023. 4
[56] Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, and
XueruiMao. Earthgpt: Auniversalmulti-modallargelan-
guage model for multi-sensor image comprehension in re-
motesensingdomain.IEEETransactionsonGeoscienceand
RemoteSensing,2024. 1
[57] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin,
Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Vi-
sual instruction tuning for medical visual question answer-
ing. arXivpreprintarXiv:2305.10415,2023. 2,5,14
[58] Zhenliang Zhang, Zeyu Zhang, Ziyuan Jiao, Yao Su,
HangxinLiu,WeiWang,andSong-ChunZhu. Ontheemer-
gence of symmetrical reality. In Proceedings of the IEEE
Conference Virtual Reality and 3D User Interfaces (VR),
pages639–649.IEEE,2024. 1
[59] Henry Hengyuan Zhao, Pan Zhou, and Mike Zheng Shou.
Genixer:Empoweringmultimodallargelanguagemodelsas
apowerfuldatagenerator.arXivpreprintarXiv:2312.06731,
2023. 2,7
[60] YaoweiZheng,RichongZhang,JunhaoZhang,YanhanYe,
ZheyanLuo,ZhangchiFeng,andYongqiangMa. Llamafac-
tory: Unified efficient fine-tuning of 100+ language mod-
els. InProceedingsofthe62ndAnnualMeetingoftheAs-
sociationforComputationalLinguistics(Volume3: System
Demonstrations),Bangkok,Thailand,2024.Associationfor
ComputationalLinguistics. 13
11Appendix B. Implementation Details of Visual Instruc-
tionSynthesis
A.SeedDataConstructionandDistribution
Table9presentsthehyper-parametersusedforsynthesizer
tuning. WeemploythevLLMinferenceframework[18]to
We convert the combination of VisionFLAN [51] and
speeduptasksynthesisandconsistencychecks.Onasingle
ALLaVA [6] into our required format. Each seed data
A100-80GB GPU, it takes approximately 10 hours to syn-
example consists of an image-caption pair as the input
thesizetasktripletsandanadditional2.5hourstoperform
and a related task triplet as the output, which includes
consistency-based filtering for every 100K image-caption
an instruction, an informative response, and a precise re-
pairs. After applying consistency-based filtering, we col-
sponse. VisionFLAN is a human-annotated visual instruc-
lect 150K, 144K, and 32K instruction-response pairs for
tion task dataset containing 191 tasks, each with 1K ex-
PMCRaw,PMCRefined,andRecipe1M,respectively.
amples. ALLaVA builds on VisionFLAN by using GPT-
4V to generate a caption for each image and to regener-
ate a response for each instruction. In our format, the im- Hyper-Parameter Assignment
age, instruction, and human-annotated response from Vi-
BaseModel LLaVA-v1.6-8B
sionFLAN are used as the image, instruction, and precise Trainable FullModel
response, respectively, while the caption and GPT-4V re- Epoch 2
generated response from ALLaVA are used as the caption BatchSize 128
andinformativeresponse. Benefitingfromthediversityof MaxSeqLength 6144
existingdatasets,ourseeddataencompassawiderangeof LR projector&LLM 2e-5
imagedomainsandtasktypes,asshowninFigure5. LR visualencoder 2e-6
LRScheduler Cosine
WeightDecay 0
Warm-UpRatio 0.03
Image Domain
ComputingInfrastructure 8A100-80GBGPUs
Objects/General Items
Animal TrainingTime 13Hours
Celebrity/Human
Nature
Art & Culture Table9.Hyper-ParametersforSynthesizerTuning
Vehicles
Abstract/Diagrammatic
Landmark/Environment
Food
Sketch/Drawing
Emotion/Sentiment MLLM LLaVA-v1.6 Qwen2-VL Llama-3.2
Textual Content
Educational/Instructional
Software/Digital Content Trainable FullModel FullModel FullModel
Historical Epoch 1 1 1
Transportation
Architecture BatchSize 128 128 128
Medical MaxSeqLength 6144 6144 6144
Sports
Fashion/Clothing LR projector&LLM 2e-5 1e-5 5e-6
LR 2e-6 1e-5 5e-6
Task Type visualencoder
LRScheduler Cosine Cosine Cosine
Image-Text Matching
Object Recognition WeightDecay 0 0.1 0.1
Caption Generation Warm-UpRatio 0.03 0.1 0.1
Domain Classification
Style Classification
Scene Classification Table 10. Hyper-Parameters for MLLM Single-Stage Post-
Face and Expression Classification Training.
Data Representation
Attribute and Context Recognition
Task-Oriented Image Recognition
Step-by-Step Guidance
Anomaly Detection Image-Caption PMCRaw PMCRefined Recipe1M
Pose and Activity Recognition
Text Detection and OCR LLaVA-v1.6-8B 21 23 6
Sentiment Analysis
Qwen2-VL-2B 3.5 4 1
Logo Detection
Utility and Affordance Recognition Llama-3.2-11B 29 31 9
Figure 5. Distribution of Image Domains and Task Types in
Table11.TrainingTime(Hours)forMLLMSingle-StagePost-
SeedData.
Trainingon8A100-80GBGPUs.
12C.MLLMPost-TrainingSettingsandCosts • Attribute and Context Recognition: Detecting image at-
tributesandcontexts,suchasobjectpresenceandtempo-
Tables 10 and 11 present the hyper-parameters and train-
ralclassification.
ing time for the single-stage post-training of MLLMs. In
• Task-Oriented Image Recognition: Recognizing objects
the two-stage training experiments, we employ two com-
instructuredcontexts,likeweedspeciesandquick-draw
mon approaches for the first stage on image-caption pairs:
sketches.
(1) unfreezing only the vision-language projector [28] for
• Step-by-Step Guidance: Recognizing steps in instruc-
LLaVA-v1.6-8B, and (2) unfreezing the full model [9]
tionalcontent,likewikihowprocedures.
for Qwen2-VL-2B and Llama-3.2-11B. During the second
• Data Representation and Visualization: Visual QA for
stageonvisualinstructiontasks,weunfreezethefullmodel
chartsandchartcaptioning.
across all setups. The hyperparameters for both stages on
• UtilityandAffordanceRecognition:Detectingobjectutil-
Qwen2-VL-2B and Llama-3.2-11B are the same as those
ityoraffordanceinimages.
listed in the table. For LLaVA-v1.6-8B, the first stage
• VisualGrounding: Linkingimagepartstocorresponding
differs only in the trainable module, which is the vision-
wordsorphrases.
languageprojector,usingalearningrateof2e-3.Thehyper-
• Segmentation: Dividing images into meaningful seg-
parametersforthesecondstagearethesameasthoselisted
ments,identifyingobjectsorregions.
inthetable. Weusethecodeimplementationsfrom[8]for
• VisualStorytelling: Creatingnarrativesbasedonaseries
experiments on LLaVA-v1.6-8B and from [60] for experi-
ofimages.
mentsonQwen2-VL-2BandLlama-3.2-11B.
D.ScoringCriteriaforDataQuality Domain Knowledge Utilization For each instruction-
response pair, the annotator evaluates the extent to which
For each synthetic dataset, we sample 200 examples and
domain-specific knowledge from the image is utilized to
usethefollowingscoringcriteriatoevaluatedataqualityin
complete the task. The scoring follows the criteria below,
eachaspect. Thefinalscoresarerescaledtoa0-100range
andwereporttheaveragescoreacrossallsamples.
forpresentationuniformity.
• 1: Thetaskistotallyirrelevanttotheimage.
• 2: The task is relevant, but the question is mundane and
TaskDiversity Foreachinstruction-responsepair,thean-
answerablewithoutreviewingtheimage.
notatorselectsthemostappropriatecategoryfromthecom-
• 3:Thetaskrequiresreviewingtheimage,butthequestion
monvisioninstructiontasktypeslistedbelow.Oncealldata
isvague,suchasaskingforageneralcaption.
samplesareannotated,wereportthenumberofdistincttask
• 4: Thetaskisclear,butthequestionfocusesononlyone
typesnormalizedbythetotalnumberofcommontasktypes.
detailintheimage.
• DomainClassification: Classifyingimagesintodomains • 5: Thetaskishighlyrelevanttoboththedetailsandover-
likerace,animalcategories,andenvironmenttypes. allcontextoftheimage.
• ObjectRecognition:Recognizingdetailedobjectslikean-
imalspecies,carbrands,andspecificobjecttypes.
TaskComplexity Foreachinstruction-responsepair,the
• Pose and Activity Recognition: Identifying specific hu-
annotator assesses task complexity, with higher scores for
manposesandactivities.
tasks requiring reasoning and instruction-following abili-
• LogoDetection: Detectingandrecognizingbrandlogos.
ties, using the criteria below. We report the average score
• FaceandExpressionClassification:Classifyingfacialat-
acrossallsamples.
tributesbyage,gender,anddetectingexpressions.
• 1:Thetaskcanbeeasilycompletedbymimickingpartof
• Scene Classification: Categorizing images into scene
thecaption.
typeslikebeaches,forests,andcities.
• 2: Thetaskcanbeeasilycompletedbyreviewingtheim-
• SentimentAnalysis: Detectingsentimentinimages.
age,suchasidentifyinganobviousobject.
• CaptionGeneration: Generatingcaptionsforimages,in-
• 3: Thetaskrequiresconsiderationofthedetails.
cludinggeneralandcontextualdescriptions.
• 4: The task requires complex reasoning on details and
• TextDetectionandOCR:Recognizingtextinimagesand
overview.
structuredtextdetection.
• 5: The task requires complex reasoning and instruction-
• Image-Text Matching: Assessing image-text similarity
following abilities, such as returning the answer in a re-
andcoherenceformultimodalcontent.
quiredformat.
• AnomalyDetection:Identifyinganomaliesinsettingslike
industrialandroadscenes.
• Style Classification: Classifying images by artistic style Response Accuracy For each instruction-response pair,
andquality. the annotator assesses whether the response correctly ad-
13dresses the task based on the context, using the following datasets,withquestionsrangingfromidentifyingmodal-
criteria. Wereporttheaveragescoreacrossallsamples. ities and organs to complex questions requiring special-
• 1:Theresponseistotallyirrelevanttothetaskinstruction. izedknowledge. Allquestionsaremultiple-choice.
• 2: The response attempts to address the instruction, but Forthefooddomain,thetaskdescriptionsareasfollows:
boththereasoningandconclusionareincorrect. • Recipe1M [38]containsrecipeinformation,includingti-
• 3: The reasoning is correct, but the conclusion is incor- tles, ingredients, and cooking instructions. We evalu-
rect. ate models by taking an image and asking for the recipe
• 4: The conclusion is correct, but the reasoning is incor- name,ingredients,andsteps.
rect. • Nutrition5K [42] comprises real-world food dishes with
• 5: Boththereasoningandconclusionarecorrect. RGBimagesandnutritionalcontentannotations. Weuse
theingredientinformationtocreateaningredientpredic-
E.TaskEvaluationDetails tiontask,wherethemodelgeneratesingredientsfroman
image.
Tables12and13presentthespecificationsandprompttem- • Food101[4]featuresimagesacross101foodcategories.
platesforevaluatedtasksineachdomain.Weconductzero- We ask the model to classify each image into one of the
shotpromptingevaluationsonthesetasks. 101categories.
For biomedicine, we follow the evaluation approach • FoodSeg103 [49] includes 103 food categories with im-
of [22] for SLAKE, PathVQA, and VQA-RAD, and the ages and pixel-wise ingredient annotations. We ask the
methodof[57]forPMC-VQA. modeltoselectoneormultiplecategoriesfromaprovided
• SLAKE [25] is a semantically-labeled, knowledge- list.
enhanced medical VQA dataset with radiology images
and diverse QA pairs annotated by physicians. The
datasetincludessemanticsegmentationmasks,objectde-
tection bounding boxes, and covers various body parts.
“CLOSED”answersareyes/notype, while“OPEN”an-
swers are one-word or short phrases. We use only the
Englishsubset.
• PathVQA [15] consists of pathology images with QA
pairs covering aspects like location, shape, and color.
Questions are categorized as “OPEN” (open-ended) or
“CLOSED”(closed-ended).
• VQA-RAD[19]includesclinician-generatedQApairsand
radiologyimagesspanningthehead,chest,andabdomen.
Questionsarecategorizedinto11types,withanswersas
either“OPEN”(shorttext)or“CLOSED”(yes/no).
• PMC-VQA [57] is larger and more diverse MedVQA
Task Description Metric TestNum
BioMed.
SLAKEOPEN[25] Medicalquestionanswering Recall 645
SLAKECLOSED[25] Medicalbinaryclassification Accuracy 416
PathVQAOPEN[15] Medicalquestionanswering Recall 3357
PathVQACLOSED[15] Medicalbinaryclassification Accuracy 3362
VQA-RADOPEN[19] Medicalquestionanswering Recall 179
VQA-RADCLOSED[19] Medicalbinaryclassification Accuracy 272
PMC-VQA[57] Medicalmulti-chioiceQA Accuracy 2000
Food
Recipe1M[38] Recipegeneration Rouge-L 1000
Nutrition5K[42] Ingredientprediction Recall 507
Food101[4] Foodcategoryclassification Accuracy 25250
FoodSeg103[49] Foodmulti-labelclassification F1 2135
Table12.SpecificationsoftheEvaluatedDomain-SpecificTaskDatasets.
14Task Instruction Response
BioMed.
SLAKE {question} {answer}
PathVQA {question} {answer}
VQA-RAD {question} {answer}
Question: {question}
PMC-VQA {option}
The choices are: {options}
Food
Recipe1M {question} {recipe}
Nutrition5K What ingredients are used to make the dish in the image? {ingredients}
What type of food is shown in this image?
Food101 Choose one type from the following options: {food type}
{food type options}
Identify the food categories present in the image.
The available categories are: {options}
FoodSeg103 Please return a list of the selected food categories, {categories}
formatted as a list of names like
[candy, egg tart, french fries, chocolate].
Table13.PromptTemplatesoftheEvaluatedDomain-SpecificTaskDatasets.
I have an image-related question for which I have received two answers: one is informative, and the
other is precise. Please help me assess whether the precise answer is consistent with the informative
answer.
Below are some examples. For each example, write your judgement as follows:
- Yes: Consistent, meaning the precise answer can be inferred from the informative answer.
- No: Inconsistent, meaning the precise answer cannot be inferred from the informative answer.
- Open: Indicates that the question invites multiple interpretations or that answers can be expressed
through open-ended phrasing, acknowledging complexity, or encouraging personal perspectives. It is also
applicable to questions that require image description, captions, or background knowledge.
## Question: Given an image of something from the kitchen, identify what it could be. The image could be
of cooking tools or items that are used for eating. It could also be used for serving food or storing
it.\nOptions: (a) This image contains a cup (b) This image contains a bottle (c) This image contains a
knife (d) This image contains a wine glass (e) This image contains a spoon (f) This image contains a
bowl (g) This image contains a fork
## Informative Answer: The options provided relate to identifying kitchenware or items related to food
service. From the given list, the most clearly identifiable object in the image is a plastic bottle with
a blue label, which is visible in the background behind the dishes. No cups, knives, wine glasses,
spoons, bowls (in the sense of a single, empty bowl), or forks are distinctly visible. Therefore, the
correct identification from the provided options is:\n(b) This image contains a bottle.
## Precise Answer: (a) This image contains a cup
## Consistent: No
## Question: You are provided with an image which contains two pictures side by side. Your task is to
identify the differences between the two pictures. Separate the differences with a comma.
## Informative Answer: Upon close inspection, the differences between the two images are subtle but
distinct. The shadows cast by the trees and lamp posts differ in length and direction, suggesting a
slight variation in the time or position of the sun when the photos were taken. The arrangement and
presence of cars vary\u2014some are present in one image but not in the other. Architectural details
such as windows, roof lines, and the design on the building exteriors also show discrepancies. The color
of the roof on the small central structure is different in each picture, with one appearing darker than
the other. Lastly, the patterns on the sidewalks, especially the crosswalk lines, are not identical,
indicating a variation in either the perspective or actual changes in the environment.
## Precise Answer: the redish brown car moves slightly.
## Consistent: Open
Figure6.PromptTemplateforConsistency-BasedFilter(Part1),continuedinPart2.
15## Question: Given an image of an animal, identify the kind of animal in the image. The picture could
be of more popular animals that are visible around zoos or are sometimes domesticated at home. They
could also sometimes be found in the wild.\nOptions: (a) This image contains a zebra (b) This image
contains a cow (c) This image contains a horse (d) This image contains an elephant (e) This image
contains a giraffe (f) This image contains a sheep (g) This image contains a cat (h) This image
contains a bear (i) This image contains a bird (j) This image contains a dog
## Informative Answer: The animal in the image can be identified as a dog, specifically a golden
retriever. This determination is based on several visual cues such as the golden coat, the size and
shape of the ears, the facial structure, and the overall body size, which are characteristic of this
breed. Golden retrievers are a popular domesticated breed known for their friendly and intelligent
demeanor. They are commonly kept as pets and are not typically found in zoos or in the wild. The image
does not contain any of the other animals listed in the options (a) through (i).
## Precise Answer: (j) This image contains a dog
## Consistent: Yes
## Question: Question: Is the picture taken outdoors?\nPlease answer the question by analyzing the
scene in the provided image.
## Informative Answer: Analyzing the elements within the image, there are several indicators confirming
that the picture is taken outdoors. The presence of natural elements like grass, trees, and the open
sky, along with architectural features that are not enclosed, such as the adobe-style building with
exterior walls and windows, support this observation. The lighting and shadows cast by the trees are
consistent with natural sunlight, further corroborating the outdoor setting. Moreover, the materials
used in the path and bench are typical of outdoor furnishings, designed to withstand weather conditions.
All these aspects contribute to the conclusion that the photograph was taken in an exterior space,
likely a garden or park associated with the building in the background.
## Precise Answer: yes
## Consistent: Yes
## Question: This task tests your ability to count number of objects. Here is the question \"How many
chairs are in the photo?\"
## Informative Answer: Upon careful observation of the photo, each individual is seated in a distinct
chair. The positioning of the subjects and their engagement with the camera suggests they are occupying
all the chairs present at their table. There are no other chairs discernible in the immediate vicinity
of the table or in the background. Therefore, it can be concluded that there are exactly four chairs in
the photo, corresponding to the four people seen around the table.
## Precise Answer: two
## Consistent: No
## Question: Can you give me a caption and some background knowledge about this image?
## Informative Answer: A detailed caption for this image could read: \"A moment of reflection and
accomplishment, as a retired NBA All-Star sits with the Championship Trophy, surrounded by peers and
the spirit of the game.\" The image is probably taken at an NBA event dedicated to honoring the
achievements of past basketball legends. The trophy next to the man symbolizes the pinnacle of success
in the league, suggesting that he may be one of the accomplished players who has reached this level of
success. The attire of the individuals indicates a formal celebration of basketball history, and the
relaxed atmosphere hints at a session of storytelling or interviews about their experiences in the
sport.\"
## Precise Answer: Moses Malone was selected by the New Orleans Jazz with the first pick. On December 9,
1975, the NBA planned to host a supplementary draft to settle negotiating rights to five ABA players
who had never been eligible for the NBA draft because their college classes had not graduated and they
had not apply for hardship. The teams selected in reverse order of their winloss record in the previous
season. The team that made a selection must withdraw their equivalent selection in the 1976 Draft. The
teams were allowed to not exercise their rights on this hardship draft and thus retained their full
selection in the 1976 Draft. The draft itself attracted strong opposition from the ABA who accuse the
NBA trying to reduce confidence in the stability of their league. Despite the initial postponement of
the draft, the draft was finally held on December 30, 1975.
## Consistent: Open
## Question: {Instruction}
## Informative Answer: {Informative Response}
## Precise Answer: {Precise Response}
## Consistent:
Figure7.PromptTemplateforConsistency-BasedFilter(Part2).
16Recipe1M TrainPipeline Instruction Recipe Nutrition Food101 FoodSeg AVERAGE
Rule 23.1 29.1 46.8 14.5 28.4
Two-Stage
Ours 16.2 28.3 43.5 28.0 29.0
LLaVA-v1.6-8B
Rule 21.8 36.7 63.9 13.9 34.1
Single-Stage
Ours 24.8 36.1 65.3 42.0 42.0
Rule 24.1 24.5 68.8 7.7 31.3
Two-Stage
Ours 16.5 43.0 69.5 23.9 38.2
Qwen2-VL-2B
Rule 19.3 37.1 64.7 6.6 31.9
Single-Stage
Ours 24.0 41.2 72.0 23.9 40.3
Rule 25.7 26.2 82.1 16.7 37.7
Two-Stage
Ours 17.8 38.0 74.6 33.2 40.9
Llama-3.2-11B
Rule 21.4 32.2 75.8 16.9 36.6
Single-Stage
Ours 26.1 41.0 82.2 42.0 47.8
SLAKE PathVQA VQA-RAD
PMCRaw TrainPipeline Instruction PMC-VQA AVERAGE
OPEN CLOSED OPEN CLOSED OPEN CLOSED
GPT-4 43.4 50.2 10.1 59.2 35.0 62.5 37.1 42.5
Two-Stage
Ours 56.2 71.4 17.2 74.5 50.6 79.0 40.4 55.6
LLaVA-v1.6-8B
GPT-4 44.2 59.1 11.6 62.2 38.5 67.3 39.9 46.1
Single-Stage
Ours 56.8 76.4 19.7 79.3 51.0 80.5 44.3 58.3
GPT-4 43.4 55.5 11.8 60.1 37.1 58.8 41.2 44.0
Two-Stage
Ours 55.2 74.5 18.4 68.4 48.8 79.8 43.8 55.5
Qwen2-VL-2B
GPT-4 43.6 59.6 13.2 47.4 37.3 57.0 31.2 41.3
Single-Stage
Ours 53.2 75.2 20.1 63.8 49.8 74.6 43.5 54.3
GPT-4 47.6 58.7 14.6 69.5 38.0 69.1 47.5 49.3
Two-Stage
Ours 60.0 75.7 22.1 76.8 51.4 80.5 47.9 59.2
Llama-3.2-11B
GPT-4 46.8 56.5 16.0 69.9 41.9 65.4 45.3 48.8
Single-Stage
Ours 56.7 77.6 22.2 87.3 55.0 76.1 49.9 60.7
SLAKE PathVQA VQA-RAD
PMCRefined TrainPipeline Instruction PMC-VQA AVERAGE
OPEN CLOSED OPEN CLOSED OPEN CLOSED
GPT-4V 50.0 68.3 17.0 67.5 43.3 67.3 40.4 50.5
Two-Stage
Ours 54.8 73.1 19.3 79.7 55.6 82.7 45.1 58.6
LLaVA-v1.6-8B
GPT-4V 52.3 76.2 20.1 73.3 47.0 76.5 43.1 55.5
Single-Stage
Ours 58.0 73.3 22.9 78.6 59.8 81.3 47.9 60.3
GPT-4V 45.2 63.2 18.2 64.7 41.3 67.3 43.2 49.0
Two-Stage
Ours 60.8 76.9 21.4 75.0 55.0 82.7 44.7 59.5
Qwen2-VL-2B
GPT-4V 51.4 66.1 18.9 61.4 45.1 73.2 45.1 51.6
Single-Stage
Ours 60.2 75.0 20.6 53.6 58.0 76.1 46.5 55.7
GPT-4V 49.1 74.3 19.3 70.9 46.2 73.9 47.1 54.4
Two-Stage
Ours 58.5 76.4 27.0 73.2 58.3 77.6 51.3 60.3
Llama-3.2-11B
GPT-4V 47.1 72.6 19.5 70.7 45.9 73.9 46.5 53.7
Single-Stage
Ours 59.5 76.4 24.3 84.9 57.4 79.8 51.9 62.0
Table14. Domain-SpecificTaskPerformanceofMLLMsafterPost-Trainingwithdifferentsyntheticdataandtrainingpipelines. The
image-captionsourcesareRecipe1M,PMCRawandPMCRefined,respectively.Inmostcasesusingoursyntheticdata,wefindthatsingle-
stagetrainingoutperformstwo-stagetrainingondomain-specifictasks,particularlyevidentintheRecipegenerationresultsforthefood
domain. Recall that in the two-stage training approach for the food domain, the model first trains on recipe captions and then on our
synthetictasks. WeexaminethetaskperformanceofLLaVA-v1.6-8BonRecipegenerationandobservethatthemodelachievesascore
of25.3afterthefirststageonrecipecaptions.However,thisscoredrasticallydecreasesto16.2afterthesecondstage.Fromthis,weinfer
thatthetwo-stageapproachcausesthemodeltocatastrophicallyforgetthetask/knowledgelearnedinthefirststagewhentransitioningto
thesecondstage[30],leadingtopoorerperformanceaftercompletingthesecond-stagetraining.
17Caption
To make Evie'SRanch Nuggets, you'll need these ingredients: (1). 2 pounds skinless, boneless chicken
breasts, cut into 2 inch pieces (2). 1/2 cup sour cream (3). 1/2 cup all-purpose flour (4). 4 ounces Ranch-
flavored tortilla chips (5). vegetable cooking spray
Follow these steps to cook it:
[+] Preheat the oven to 400 degrees F (200 degrees C). Line a baking sheet with aluminum foil.
[+] Place tortilla chips and flour into a food processor or blender, and process into crumbs. Pour into a bowl.
Place chicken pieces into a sealable plastic bag with sour cream. Massage the bag to make sure chicken
pieces are coated with sour cream. Let sit for 5 minutes.
[+] Dip coated chicken pieces into the chip crumbs, and place on the prepared baking sheet. Spray lightly
with vegetable cooking spray.
[+] Bake for 30 minutes in the preheated oven, until chicken is cooked through and juices run clear.
Rule-Based Instruction-Response Pair
Instruction: I'm curious! What's the name, ingredients, and cooking method for this mouth-watering creation?
Response: To make Evie'SRanch Nuggets, you'll need these ingredients: (1). 2 pounds skinless, boneless chicken breasts, cut into 2 inch
pieces (2). 1/2 cup sour cream (3). 1/2 cup all-purpose flour (4). 4 ounces Ranch-flavored tortilla chips (5). vegetable cookingspray
Follow these steps to cook it:
[+] Preheat the oven to 400 degrees F (200 degrees C). Line a baking sheet with aluminum foil. (…)
Our Instruction-Response Pair
Instruction: Texture is defined as the feel, appearance or consistency of a surface or substance from a human's perspective. Detect the
primary texture represented in the image.
Response: The primary texture represented in the image is that of a crispy, crunchy exterior typical of fried foods. The uneven, rough
surface of the chicken nuggets suggests a firm texture that would be audible upon bite. The golden-brown color indicates a Maillard
reaction, which is a chemical reaction between amino acids and reducing sugars that gives browned food its distinctive flavor. This
reaction also contributes to the crunchy texture, as it creates a rigid outer layer. The visible crumbs and the rough, unevensurface
further emphasize the crunchy texture that one would expect from such a food item.
Caption
CT scan of the orbits revealed right eye enophthalmos, focal osseous
defect of lamina papyracea (red arrow), deficient orbital fat (white
arrow), fibrosis of soft tissue (red star) along medial wall of right orbit.
GPT-4 Instruction-Response Pair
Instruction: What is the red arrow pointing to?
Response: The red arrow in the CT scan is pointing to a focal osseous defect of the lamina papyracea.
Our Instruction-Response Pair
Instruction: What is the location of the lesion?
Response: The lesion is located in the right orbit. On the CT scan, it appears as an area of increased density (hyperdense) relative tothe
surrounding brain tissue. The lesion is situated medial to the eyeball, which is the most prominent structure within the orbit. It is
important to note that the interpretation of radiological images should be performed by a qualified radiologist, as it requires a detailed
understanding of human anatomy and the ability to correlate findings with clinical symptoms for an accurate diagnosis. The answer is
right orbit.
Figure8.CasesofInstruction-ResponsePairs(Part1)synthesizedbyourmethod,manualrules,GPT-4,andGPT-4V,theimage-caption
sourcesforthecasesareRecipe1M,PMCRaw andPMCRefined,respectively. ContinuedinPart2. Inthefirstcase,therule-basedtask
simplytransformstherecipecaption,ignoringtheimagecontent.Incontrast,ourtaskinvolvesanalyzingthefood’sstateintheimageand
applyingfood-relatedknowledgetoinferitstexture,demonstratingahigherlevelofdomainknowledgeutilization. Inthesecondcase,
theGPT-4generatedtaskstraightforwardlyasksaboutthepointingoftheredarrow,whileoursrequiresadetailedanalysisandinference,
showinggreatertaskcomplexity.
18Caption
The image displays three distinct panels, each labeled with a different phase of bone
development under distraction osteogenesis. The top panel, labeled 'Latency phase',
shows an X-ray image of a bone with a surgical device composed of two parallel metal
plates attached by screws on either side of a bone segment. The middle panel, labeled
'Distraction phase', depicts the same bone but with a noticeable increase in the gap
between the bone ends, suggesting bone lengthening. The bottom panel, labeled
'Consolidation phase', illustrates further progress of bone growth within the gap,
indicating new bone formation and consolidation. Each panel uses the same angle and
lighting conditions to emphasize the changes in the bone structure across different
phases.
GPT-4V Instruction-Response Pair
Instruction: Can you identify the type of surgical hardware used in these phases based on the image, and explain its role in the
manipulation of bone segments during distraction osteogenesis?
Response: The surgical hardware visualized in the image appears to be a type of external fixator, consisting of two parallel metal plates
connected by screws that penetrate through the bone segments. This hardware plays a pivotal role in distraction osteogenesis by
stabilizing the bone while also allowing for gradual bone lengthening. The screws can be adjusted to manipulate the distance between
the divided bone segments, facilitating new bone growth in the process required by the distraction phase.
Our Instruction-Response Pair
Instruction: What is the gap between the bone ends called?
The choices are: (A) Consolidation; (B) Distraction; (C) Latency; (D) None of the above.
Response: The gap between the bone ends, as depicted in the middle panel of the image, is referred to as the 'Distraction phase'. This
phase is characterized by the lengthening of the bone due to the application of a distraction device, which separates the bone ends
and allows for new bone growth to fill the gap. Therefore, the correct answer to the question is:
(B) Distraction.
Figure9.CasesofInstruction-ResponsePairs(Part2)synthesizedbyourmethod,manualrules,GPT-4,andGPT-4V,theimage-caption
sourcesforthecasesareRecipe1M,PMCRawandPMCRefined,respectively.Inthiscase,ourtaskstandsoutasamultiple-choicequestion,
showcasingtaskdiversity.
19