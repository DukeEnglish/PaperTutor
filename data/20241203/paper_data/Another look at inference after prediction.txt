Another look at inference after prediction
Another look at inference after prediction
Jessica Gronsbell1 j.gronsbell@utoronto.ca
Jianhui Gao1 jianhui.gao@mail.utoronto.ca
Yaqi Shi1 yaqi.shi@mail.utoronto.ca
Zachary R. McCaw2 zmccaw@alumni.harvard.edu
David Cheng3 dcheng@mgh.harvard.edu
1Department of Statistical Sciences, University of Toronto, Toronto, ON
2Department of Biostatistics, UNC Chapel Hill, Chapel Hill, NC
3Biostatistics Center, Massachusetts General Hospital, Boston, MA
Abstract
Prediction-based (PB) inference is increasingly used in applications where the out-
comeofinterestisdifficulttoobtain, butitspredictorsarereadilyavailable. Unlike
traditional inference, PB inference performs statistical inference using a partially
observed outcome and a set of covariates by leveraging a prediction of the outcome
generated from a machine learning (ML) model. Motwani and Witten (2023) re-
cently revisited two innovative PB inference approaches for ordinary least squares.
They found that the method proposed by Wang et al. (2020) yields a consistent
estimator for the association of interest when the ML model perfectly captures the
underlying regression function. Conversely, the prediction-powered inference (PPI)
method proposed by Angelopoulos et al. (2023a) yields valid inference regardless
of the model’s accuracy. In this paper, we study the statistical efficiency of the
PPI estimator. Our analysis reveals that a more efficient estimator, proposed 25
years ago by Chen and Chen (2000), can be obtained by simply adding a weight
to the PPI estimator. We also contextualize PB inference with methods from the
economicsandstatisticsliteraturedatingbacktothe1960s. Ourextensivetheoret-
ical and numerical analyses indicate that the Chen and Chen (CC) estimator offers
a balance between robustness to ML model specification and statistical efficiency,
making it the preferred choice for use in practice.
Keywords: Missing data, prediction-based inference, semi-parametric inference
1
4202
voN
92
]LM.tats[
1v80991.1142:viXraGronsbell et al.
1 Introduction
In the rapidly evolving landscape of data science, accurately predicting an outcome
is often just an initial step in scientific inquiry. Researchers are increasingly turn-
ing to machine learning (ML) models to obtain predictions for outcomes that are
prohibitively time-consuming or expensive to directly measure, and then drawing
statistical inferences from the ML model’s predictions (Hoffman et al., 2024; Fan
et al., 2024; Gao et al., 2024; Miao and Lu, 2024). This prediction-based (PB) in-
ference scenario was introduced by Wang et al in the context of global health and
genomicsapplications, andiscomprisedoftwostepsoutlinedinFigure1(Wangetal.,
2020). First, the outcome of interest is imputed using a prediction from a pre-trained
model. Then, the predictions are used to draw statistical inference on the parameter
of interest, such as the association between an outcome and a set of covariates.
Figure 1: Prediction-based (PB) inference. Y is the outcome of interest, which
isonlypartiallyobserved, ZisasetofpredictorsofY, Y(cid:98) isthepredictionof
Y based on Z, and X is a set of covariates. The focus of scientific inquiry
is on the association between X and Y. Traditional statistical inference
relies solely on the data on X and Y while PB inference incorporates Y(cid:98)
to maximize the use of all available data. Dark red represents labeled
observations while light red represents unlabeled observations.
2Another look at inference after prediction
In the last several years, applications of PB inference have arisen in diverse fields,
spanning genetics, biology, and medicine. For example, population biobanks, such as
the UK Biobank (N ≈ 500K individuals), provide an unprecedented data source for
high-throughput genetic research. However, many traits and disease outcomes are
only partially observed due to the time and cost of measurement. ML models are
commonly used to predict missing outcomes based on fully observed baseline data
and the predictions are utilized for genetic discovery (Tong et al., 2020; Alipanahi
et al., 2021; Cosentino et al., 2023; Dahl et al., 2023; An et al., 2023; McCaw et al.,
2024; Liu et al., 2024; Miao et al., 2024). Similarly, in the field of structural biology,
AlphaFold provides highly accurate structural predictions without the need for ex-
pensive and labor-intensive experiments for proteomic studies (Jumper et al., 2021).
In the analysis of electronic health records data, large language models can rapidly
ascertain disease status at an accuracy competitive with manual chart review by ex-
perts, enabling downstream epidemiological and clinical studies (Yang et al., 2022;
Zhou et al., 2022; Alsentzer et al., 2023).
GiventherelevanceofPBinference, MotwaniandWitten(2023)recentlyrevisited
two innovative methods in this area. In the context of least squares estimation,
they showed that the approach proposed by Wang et al. (2020) provides a consistent
estimator for the association between an outcome and a set of covariates only in
restrictive scenarios. For example, when the pre-trained model perfectly captures the
regression function. This is a significant practical limitation as it is essential that
PB inference methods yield valid inference in the likely scenario that the prediction
model is imperfect. In contrast, the prediction powered inference (PPI) approach
proposed by Angelopoulos et al. (2023a) possesses this necessary property and yields
valid inference regardless of the quality of the model.
Here we build on the work on Motwani and Witten to study the statistical ef-
ficiency of the PPI method. We highlight that a more efficient estimator can be
obtained by simply adding a weight to the PPI estimator, an approach that was pro-
posed by Chen & Chen (CC) more than 25 years ago in the context of double sampling
designs (Chen and Chen, 2000). We begin by formally introducing the PB inference
problem and reviewing the contributions of Motwani and Witten in Section 2. We
then study the asymptotic properties of the PPI estimator and introduce the CC esti-
mator in Section 3. Our key finding is that the CC estimator is optimal with respect to
statistical efficiency in a class of estimators to which the PPI estimator belongs. We
also point to earlier literature from the statistics and economics literature dating back
to the 1960s that provides important historical context for the PB inference problem
in Section 4. In Section 5, we evaluate our findings with extensive simulation studies.
Our results show that the CC estimator offers a balance between robustness to ML
model specification and statistical efficiency, making it the preferred choice for use in
practice. We close with additional discussion in Section 6.
3Gronsbell et al.
2 Problem setting & background
2.1 Notation and Assumptions
We let Y denote the outcome of interest and Z a set of predictors. Y is only partially
observed and a pre-trained ML model is available to predict it on the basis of Z. The
ˆ
ML model, f : Z → Y, maps from the predictor space, Z, to the outcome space,
ˆ
Y. The predictions of Y are denoted as Y(cid:98) = f(Z). We let X = (1,X ,...,X )T be
1 p
a set of covariates that are potentially associated with Y and that are the focus of
scientific inquiry. X may also be predictive of Y and these covariates may or may not
be included in Z. We assume that X and Z are both fully observed. The available
data therefore consists of a labeled dataset and an independent unlabeled dataset,
respectively denoted as
L = {(Y ,ZT,XT)T | i = 1,...,n } and U = {(ZT,XT)T | i = n +1,...,n},
i i i lab i i lab
where n = n +n , n is the number of unlabeled observations, and n /n → π
lab unlab unlab lab
as n → ∞ for π ∈ (0,1).
We focus on the case in which L and U are independent and identically dis-
tributed samples from the population of interest. More complex scenarios have been
the focus of recent work (Angelopoulos et al., 2023a; Zrnic and Cand`es, 2024a). Ad-
ditionally, fˆ is assumed to be pre-trained with a dataset that is independent of L
and U and is therefore treated as “fixed” (i.e., the variation in estimating fˆ is not
ˆ
of interest in downstream inference). In fact, f may be a black box function in that
its training data and specification are not be available to the users. Approaches that
ˆ
address uncertainty in the estimation of f have recently been proposed in Zrnic and
Cand`es (2024b).
2.2 Parameter of interest
Following Motwani and Witten (2023), we consider least squares estimation and aim
to make inference on the population parameter
β∗ := argminE[(Y −XTβ)2] = [E(XXT)]−1E(XY),
β
which quantifies the association between Y and X. Throughout, we use superscripts
to denote different estimators of β∗. For example, the simplest unbiased estimator of
β∗, denoted as βˆ lab, uses only L for estimation and is the solution to the estimating
equation
n n
(cid:88)lab (cid:88)lab
ϕ(Y ,X ;β) := X (Y −XTβ) = 0.
i i i i i
i=1 i=1
4Another look at inference after prediction
2.3 Motivation for PB inference
The goal of PB inference is to utilize the abundant realizations of Y(cid:98), which can
be computed in the unlabeled data, to provide valid and efficient inference on the
association between Y and X. Motwani and Witten consider the question of validity
and we study efficiency in Section 3. With respect to validity, PB inference should
enable users to draw correct conclusions regarding the magnitude and direction of β∗
and to perform reliable uncertainty quantification. Incorporating information from
Y(cid:98) should not change the parameter being estimated and the credibility of inferences
ˆ
should not depend on the quality of predictions from f.
The need for PB inference is highlighted by limitations of “naive” inference, in
which inference on Y(cid:98) is performed in lieu of inference on Y (Wang et al., 2020). The
parameter targeted by naive inference is
γ∗ := argminE[(Y(cid:98) −XTγ)2] = [E(XXT)]−1E(XY(cid:98)).
γ
A least squares estimator for γ∗, denoted as γˆunlab, is the solution to
n n
(cid:88) (cid:88)
ϕ(Y(cid:98),X ;γ) := X (Y(cid:98) −XTγ) = 0.
i i i i i
i=n +1 i=n +1
lab lab
Critically, γ∗ bears no necessary correspondence with β∗ except under highly re-
strictive scenarios, such as when Z contains X and the ML model captures the true
regression function. The fundamental problem is that (Y(cid:98),XT)T has a different distri-
bution than (Y,XT)T. To overcome these limitations, PB inference methods utilize
L together with U to essentially balance the relative contributions of Y and Y(cid:98) in
making inference on β∗.
2.4 PB inference methods
Motwani and Witten revisited two innovative strategies for PB inference. The first
is the proposal of Wang et al. (2020), which is a two-step procedure to incorporate
Y(cid:98) into estimation. First, L is used to model the relationship between Y and Y(cid:98) to
address potential inaccuracies in the predictions. Then, this model is used together
with U to make inference on β∗, with analytical or bootstrap approaches described.
In their illuminating work, Motwani and Witten show that the estimator of Wang
et al is consistent for β∗ under the strong and unrealistic assumption that the ML
model captures the true regression function. In other scenarios, the Wang et al
estimator may not be consistent for β∗ even while its standard error converges to
0 as (n ,n ) → ∞. Consequently, the familiar Wald statistic will not converge
lab unlab
in distribution under the null, resulting in type I error that is not controlled at the
nominal level and confidence intervals with inadequate coverage.
5Gronsbell et al.
Motwani and Witten also study the PPI method, which has the crucial property
that inference remains valid regardless of the quality of the predictions. Stated oth-
ˆ
erwise, the PPI method does not require that f capture the true regression function
for the estimator to maintain consistency or to yield reliable hypothesis testing and
interval estimation. The proposed estimator utilizes a common statistical technique
involving the augmentation of an initial consistent estimator for the parameter of
interest with an additional term that converges in probability to 0. This approach
dates back to at least Robins et al. (1994), and has been applied in multiple settings
to improve estimation efficiency outside of the PB inference problem (e.g., Chen and
Chen (2000); Tsiatis (2006); Tian et al. (2012); Wang and Wang (2015)). We return
to this point in Section 4.
The PPI estimator is
βˆ PPI = βˆ lab −(γˆlab −γˆunlab),
where γˆlab is the solution to (cid:80)n labϕ(Y(cid:98),X ;γ) = 01. In contrast to the approach of
i=1 i i
Wang et al, βˆ PPI is consistent for β∗ and n1(βˆ PPI −β∗) converges weakly to a zero-
2
mean multivariate normal distribution regardless of the quality of the prediction. The
proposed method therefore asymptotically controls the type I error for Wald-based
hypothesis testing and provides confidence intervals with the nominal coverage level.
3 Efficiency analysis
As PB inference methods suppose the availability of at least a small labeled data set,
ˆ
a natural baseline estimator for comparison is βlab, which simply regresses Y on X
within L. For a PB inference method to be preferred over this baseline, it should
ˆ
not only yield valid inference, but also improve statistical efficiency relative to βlab
when the prediction is informative of Y, and perform no worse than estimation based
on L when Y(cid:98) is uninformative of Y. For the remainder of this paper, we study the
ˆ
latter property and analyze an estimator very similar to βPPI given by
βˆ PPIa = βˆ lab −(γˆlab −γˆall),
where γˆall is the solution to
(cid:80)n
ϕ(Y(cid:98),X ;γ) = 0. The only difference between
i=1 i i
ˆ ˆ
βPPI and βPPIa is that all of the data, rather than the unlabeled data, is used to
estimate γ∗ in the augmentation term (i.e., γˆunlab is replaced by γˆall). When π ≈ 0,
γˆall ≈ γˆunlab so that βˆ PPIa ≈ βˆ PPI. In general, it can be shown that βˆ PPIa gains
ˆ ˆ ˆ ˆ
efficiency over βlab whenever βPPI gains over βlab, and βPPIa can be more efficient
ˆ
than βPPI when Y(cid:98) is informative of Y (see Appendix B, specifically Section B.3, for
(cid:16) (cid:17) (cid:16) (cid:17)
1. Angelopoulosetal.(2023a)expressβˆPPI asγˆunlab− γˆlab−βˆlab where γˆlab−βˆlab isreferred
to as a “rectifier.” The rectifier serves the purpose of correcting potential bias in γˆunlab due to
inaccuracies in Y(cid:98) for Y. We reframe the PPI method to study its statistical efficiency.
6Another look at inference after prediction
a detailed analysis). Additionally, estimators of this form are commonly used in the
semi-parametric inference literature, which we draw upon in the subsequent section
in our efficiency analysis.
3.1 Asymptotic analysis of the PPI estimator
ˆ
To study the asymptotic variance of βPPIa, Theorem 1 provides the influence function
ˆ ˆ
expansion of βPPIa. All of the asymptotic results for βPPIa are detailed in Appendix
A.
ˆ
Theorem 1 (Influence function expansion of βPPIa) Let R be the indicator of
i
whether the ith observation is labeled. Then,
n
(cid:16) (cid:17) (cid:88)
n1
2
βˆ PPIa −β∗ = n− 21 IFPPIa(Y i,Y(cid:98) i,X i;β∗,γ∗)+o p(1),
i=1
(cid:110) (cid:104) (cid:105) (cid:111)
where IFPPIa(Y ,Y(cid:98),X ;β∗,γ∗) = A [ ϕ(Y ,X ;β∗)] Ri + ϕ(Y(cid:98),X ;γ∗) π−Ri and
i i i i i π i i π
(cid:16) (cid:17)
A = [E(XXT)]−1. Therefore, n 21 βˆ PPIa −β∗ converges weakly to a zero-mean mul-
tivariate normal distribution with variance-covariance
(cid:104) (cid:105)
E IFPPIa(Y ,Y(cid:98),X ;β∗,γ∗)IFPPIa(Y ,Y(cid:98),X ;β∗,γ∗)T .
i i i i i i
ˆ
We use Theorem 1 to compare the asymptotic variance-covariance of βPPIa with that
ˆ ˆ ˆ
of βlab. Corollary 1.1 shows that the benefit of βPPIa over βlab scales directly with the
predictiveness of Y(cid:98) for Y and inversely with the proportion of labeled data, π.
(cid:16) (cid:17)
Corollary 1.1 (Comparison of βˆ PPIa with βˆ lab) LetΣ = E ϵ2 XXT , Σ =
Y(cid:98),Y(cid:98) Y(cid:98) Y,Y(cid:98)
E(cid:0)
ϵ ϵ
XXT(cid:1)
, ϵ = Y −XTβ∗ and ϵ = Y(cid:98) −XTγ∗. The difference in the asymptotic
Y Y(cid:98) Y Y(cid:98)
ˆ ˆ
variance-covariance (aVar) of βlab and βPPIa is
(cid:18) (cid:19)
(cid:104) (cid:105) 1 (cid:16) (cid:17)
ˆ ˆ ˆ ˆ
∆ (βlab,βPPIa) = n aVar(βlab)−aVar(βPPIa) = −1 A 2Σ −Σ A.
aVar π Y,Y(cid:98) Y(cid:98),Y(cid:98)
ˆ ˆ
βPPIa is therefore more efficient than βlab if and only if 2Σ ≻ Σ . A sufficient
Y,Y(cid:98) Y(cid:98),Y(cid:98)
ˆ ˆ
condition for βPPIa to have lower variance than βlab is that
(cid:104) (cid:105)
E (cid:0) ϵ −ϵ (cid:1)2 | X < E(cid:0) ϵ2 | X(cid:1) .
Y(cid:98) Y Y
More specifically, the sufficient condition in Corollary 1.1 coincides with the intuition
that incorporating unlabeled data into estimation can improve efficiency when Y(cid:98) has
additional predictive power for Y beyond what is already explained by the regression
model of Y on X. This condition is similar to the result for mean estimation provided
7Gronsbell et al.
in the Supplementary Materials of Angelopoulos et al. (2023a) and can be used to
ˆ
describe several scenarios when βPPIa is guaranteed to have lower asymptotic variance
ˆ
than βlab when the ML model captures the true regression function (see Appendix
ˆ
A.4). Here we consider a simple example to demonstrate when βPPIa gains efficiency
ˆ
over βlab under homoskedasticity.
ˆ
Example 1 (Efficiency gain of βPPIa under homoskedasticity) Inthehomoskedas-
tic setting where E(ϵ2 | X) = σ2, E(ϵ2 | X) = σ2, and E(ϵ ϵ | X) = σ ,
Y Y Y(cid:98) Y(cid:98) Y Y(cid:98) Y,Y(cid:98)
(cid:18) (cid:19)
1 (cid:104) (cid:105)
∆ (βˆ lab,βˆ PPIa) = −1 2σ −σ2 A,
aVar π Y,Y(cid:98) Y(cid:98)
ˆ ˆ
and βPPIa is more efficient than βlab if and only if
1σ
ρ > Y(cid:98),
Y,Y(cid:98) 2σ
Y
where ρ = σ /σ σ is the correlation of residuals for Y and Y(cid:98) given X.
Y,Y(cid:98) Y,Y(cid:98) Y Y(cid:98)
ˆ ˆ
The improvement of βPPIa over βlab is therefore stronger with increasing correla-
tion of residuals between Y and Y(cid:98) and decreasing variability in the residuals of Y(cid:98).
Therefore, Y(cid:98) must be a strong predictor for Y after adjusting for X, though it does
not necessarily need to be derived from a correctly specified ML model. In particular,
ˆ
βPPIa gains efficiency if and only if the correlation of residuals is at least half of the
ratio of residual standard deviations of Y(cid:98) to that of Y.
3.2 Asymptotic analysis of the CC estimator
ˆ
To further analyze the efficiency of βPPIa we draw on a landmark paper that intro-
duced a class of augmented inverse probability weighted (AIPW) estimators which
includes all regular asymptotic linear estimators (Robins et al., 1994). In the current
setting, where the goal is to estimate linear regression coefficients β∗ with Y partially
observed, but Y(cid:98) fully observed, these estimators have influence functions of the form
(cid:20) (cid:21)
R π −R
IF(Y,Y(cid:98),X;ω) = A ϕ(Y,X;β∗) +ω(Y(cid:98),X) ,
π π
with ω(Y(cid:98),X) being a function of Y(cid:98) and X. The optimal choice of ω(Y(cid:98),X) with
respect to asymptotic variance is
ωopt(Y(cid:98),X) = E[ϕ(Y,X;β∗) | Y(cid:98),X].
ˆ
By Theorem 1, βPPIa is an AIPW estimator and can be seen as an approximation
to the optimal estimator with ω(Y(cid:98),X) = ϕ(Y(cid:98),X;γ∗). The PPI estimator is therefore
8Another look at inference after prediction
not the most efficient estimator in its class and, from Corollary 1.1, can even be less
ˆ
efficient than βlab when Σ ≻ 2Σ . A PB inference strategy should enhance
Y(cid:98),Y(cid:98) Y,Y(cid:98)
ˆ
statistical efficiency relative to βlab whenever possible and, at the very least, should
not be negatively impacted by incorporating unlabeled data into estimation. This
raises concerns about the practical utility of the PPI estimator.
ˆ
ToimproveuponβPPIa,oneapproachistoassumeaparametricmodelforωopt(Y(cid:98),X)
that can be estimated from L for analysis. However, it is often difficult to specify
an appropriate parametric model for ωopt(Y(cid:98),X), particularly when the relationship
between Y and Y(cid:98) is unknown. Moreover, when the model for ωopt(Y(cid:98),X) is misspec-
ified, an efficiency gain is not guaranteed. To overcome these difficulties, Chen and
Chen (2000) proposed a particular choice of ω(Y(cid:98),X) that is guaranteed to be at least
ˆ
as efficient as βlab and that does not require modeling of ωopt(Y(cid:98),X). Their approach
is motivated by the double-sampling design which, apart from differences in termi-
nology and notation (i.e., they regard Y(cid:98) as a mismeasured version of Y), is identical
to the partially labeled data setting considered here. The CC estimator approximates
ωopt(Y(cid:98),X) with
ωCC(Y(cid:98),X) = Cov[ϕ(Y,X;β∗),ϕ(Y(cid:98),X;γ∗)]{Var[ϕ(Y(cid:98),X;γ∗)]}−1ϕ(Y(cid:98),X;γ∗). (1)
The estimator with this influence function can be written as
βˆ CC = βˆ lab −W(cid:99)CC(γˆlab −γˆall),
where W(cid:99)CC is a consistent estimator of
WCC = ACov[ϕ(Y,X;β∗),ϕ(Y(cid:98),X;γ∗)]{Var[ϕ(Y(cid:98),X;γ∗)]}−1A−1
obtained with L. Therefore, a simple way to ensure that an augmented estimator
ˆ
likethePPIestimatorhasasymptoticvarianceatleastaslowasβlab istoaddasimple
weight function in front of the augmentation term. In fact, the framework of Robins
et al. (1994) implies that all regular asymptotically linear estimators for β∗ augment
ˆ
βlab with an augmentation term that incorporates information from Y(cid:98). We present
ˆ
the influence function expansion for βCC in Theorem 2 to further study its asymptotic
ˆ ˆ
variance and compare it with βPPIa. All of the asymptotic results provided for βCC
are detailed in Appendix C.
9Gronsbell et al.
ˆ
Theorem 2 (Influence function expansion of βCC) Following Chen and Chen
(2000),
n
(cid:16) (cid:17) (cid:88)
n1
2
βˆ CC −β∗ = n−1
2
IFCC(Y i,Y(cid:98) i,X i;β∗,γ∗)+o p(1),
i=1
(cid:104) (cid:105)
whereIFCC(Y ,Y(cid:98),X ;β∗,γ∗) = A ϕ(Y ,X ;β∗)Ri +ωCC(Y(cid:98),X )π−Ri andωCC(Y(cid:98),X)
i i i i i π i i π
(cid:16) (cid:17)
is defined in (1). Therefore, n1 βˆ CC −β∗ converges weakly to a zero-mean multi-
2
variate normal distribution with variance-covariance
(cid:104) (cid:105)
E IFCC(Y ,Y(cid:98),X ;β∗,γ∗)IFCC(Y ,Y(cid:98),X ;β∗,γ∗)T .
i i i i i i
ˆ
The subsequent corollaries apply Theorem 2 to show our two key results: (1) βCC
ˆ ˆ
is at least as efficient as βlab and (2) βCC has the highest asymptotic efficiency within
the class of estimators with a weighted linear augmentation term. Together these
results illustrate that the CC method is both robust to ML model specification and
optimizes statistical efficiency, making it preferable to the PPI method in practice.
ˆ ˆ
Corollary 2.1 (βCC is at least as efficient as βlab) The difference in the asymp-
ˆ ˆ
totic variance-covariance of βlab and βCC is
(cid:18) (cid:19)
1
∆ (βˆ lab,βˆ CC) = −1 AΣ Σ−1 Σ A ⪰ 0.
aVar π Y,Y(cid:98) Y(cid:98),Y(cid:98) Y,Y(cid:98)
ˆ ˆ
Therefore, βCC is guaranteed to be at least as efficient as βlab.
ˆ
Corollary 2.2 (βCC is the most efficient estimator in its class) Let the mean
squared error (MSE) of an estimator βˆ for β∗ be defined as E[(βˆ − β∗)T(βˆ − β∗)].
Analogously to the well-known result for the best linear predictor (e.g., Bickel and
Doksum (2015), page 40), W(cid:99)CC is the weight, W, that minimizes the asymptotic
MSE of estimators of the form βˆ lab −W(γˆlab −γˆall) for estimating β∗.
ˆ ˆ
From Corollary 2.1, we note that in contrast to ∆ (βlab,βPPIa), the covariance
aVar
ˆ ˆ
term in ∆ (βlab,βCC) is normalized by the residual variance of the Y(cid:98) to scale the
aVar
ˆ
covariance according to the precision of Y(cid:98). This scaling essentially enables βCC to
ˆ
always be as least as efficient as βlab. Coming back to the example from the previous
ˆ
section, we can gain more intuition as to why βCC is guaranteed to be at least as
ˆ ˆ
efficient as βlab while βPPIa does not possess this property.
ˆ
Example 1 continued (Efficiency gain of βCC under homoskedasticity) We
revisit the case of homoskedasticity and recall that
(cid:18) (cid:19)
1 (cid:16) (cid:17)
∆ (βˆ lab,βˆ PPIa) = −1 2σ −σ2 A
aVar π Y,Y(cid:98) Y(cid:98)
10Another look at inference after prediction
(cid:18) (cid:19)(cid:20) (cid:21)
1 (cid:16) (cid:17)2
= −1 − σ ρ −σ +σ2ρ2 A.
π Y Y,Y(cid:98) Y(cid:98) Y Y,Y(cid:98)
Theorem 2 can be used to show that
(cid:18) (cid:19)(cid:32) σ2 (cid:33) (cid:18) (cid:19)
1 1
∆ (βˆ lab,βˆ CC) = −1 Y,Y(cid:98) A = −1 σ2ρ2 A.
aVar π σ2 π Y Y,Y(cid:98)
Y(cid:98),Y(cid:98)
ˆ ˆ
βCC is more efficient than βlab provided that ρ > 0 and the gain in efficiency scales
Y,Y(cid:98)
directly with ρ and σ2. In contrast, βˆ PPIa will only be more efficient than βˆ lab
Y,Y(cid:98) Y
provided that ρ >
1σ
Y(cid:98) and the gain in efficiency will only scale in this manner
Y,Y(cid:98) 2σY
σ
with the additional condition that ρ = Y(cid:98).
Y,Y(cid:98) σY
4 Related literature
Existing PB inference literature has established connections with semi-supervised in-
ference, semi-parametric inference, missing data, and measurement error (Angelopou-
losetal.,2023a,b;GanandLiang,2023;Miaoetal.,2023;ZrnicandCand`es,2024a,b).
Here we draw connections between PB inference methods and methods from the eco-
nomics and statistics literature dating back to the 1960s. These connections reveal
that aspects of the PB inference problem have been explored in various areas, albeit
with differences in motivation, terminology, and notation. Given the close relation-
ship between the PPI and CC methods, future research in PB inference could benefit
from a more comprehensive review of these and other related approaches than what
is provided here.
4.1 Seemingly unrelated regression & its use in statistical genetics
Seemingly unrelated regression (SUR) was introduced by economist Arnold Zellner
in 1962 for simultaneous estimation of multiple related regression models (Zellner,
1962; Swamy and Mehta, 1975; Schmidt, 1977). The regression models have different
outcome variables, but are assumed to have correlated errors. SUR leverages the
correlation of the error terms to produce more efficient estimators compared to esti-
mating each regression independently. In the current setting, the SUR problem can
be stated formally as
Y = XTβ∗ +ϵ and Y(cid:98) = XTγ∗ +ϵ′
where E(ϵ | X) = 0 and E(ϵ′ | X) = 0. An estimator for β∗ that is based on
ˆ
augmenting βlab was proposed by Conniffe (1985) and is given by:
(cid:80)n lab(Y −XTβˆ lab)(Y(cid:98) −XTγˆlab)
βˆ SUR = βˆ lab −W(cid:99)SUR(γˆlab −γˆall) where W(cid:99)SUR = i=1 i i i i .
(cid:80)n
lab(Y(cid:98) −XTγˆlab)2
i=1 i i
11Gronsbell et al.
Note that W(cid:99)SUR is an estimator of the scalar WSUR =
E[Cov(ϵ,ϵ′|X)].
Similar to the PPI
E[Var(ϵ′|X)]
and CC methods, the contribution of the augmentation term to estimation increases
with the predictiveness of Y(cid:98) for Y. In the extreme case where Y(cid:98) is a perfect predictor
for Y such that Y(cid:98) = Y, W(cid:99)SUR = 1 and βˆ SUR is simply γˆall. In the homoskedastic case
when Var(ϵ | X) and Cov(ϵ,ϵ′ | X) are constant and do not depend on X, βˆ CC
ˆ
is asymptotically equivalent to βSUR (Chen and Chen, 2000). Moreover, when the
conditional distribution of (ϵ,ϵ′) given X is bivariate normal with mean 0 and has
ˆ ˆ
constant variance-covariance, βSUR (and thus βCC as well) is the maximum likelihood
estimator of β∗ and fully efficient under the assumed parametric model (Conniffe,
ˆ ˆ
1985). In other scenarios, βCC achieves a lower asymptotic variance than βSUR.
The SUR estimator was recently applied for a PB inference problem inspired by
genome-wide association studies conducted with population biobanks. These stud-
ies often rely on pre-trained models to impute outcomes of interest (i.e., phenotypes
or traits) as they are partially missing due to the time and expense of ascertain-
ment (McCaw et al., 2024). The proposed method, “SynSurr”, assumes the true and
predicted outcomes follow a bivariate normal distribution. The SynSurr estimator
ˆ ˆ
is equivalent to βSUR and asymptotically equivalent to βCC when Var(ϵ | X) and
Cov(ϵ,ϵ′ | X) are constant. Building on the SynSurr estimator, Miao et al. (2024)
proposed the “POP-GWAS” estimator for a variety of regression models utilizing the
augmentation technique. The POP-GWAS estimator is
n
βˆ POP = βˆ lab −W(cid:99)POP(γˆlab −γˆunlab) where W(cid:99)POP = unlabC(cid:100)orr(Y,Y(cid:98) | X).
n
IncontrasttotheSURestimator, βˆ POP usesγˆunlab insteadofγˆall fortheaugmentation,
and the weight, W(cid:99)POP is not generally a consistent estimator of WSUR. βˆ POP is
ˆ
thereforelessefficientthanβSUR whentheresidualsarebivariatenormalwithconstant
ˆ
variance-covariance and less efficient than βCC in general.
4.2 Augmented estimation for quantifying a treatment effect
Outside of statistical genetics, the augmented estimation approach used by the PPI
and CC methods has been widely used for treatment effect estimation in the statistics
and biostatistics literature (e.g., Leon et al. (2003); Lu and Tsiatis (2008); Tsiatis
et al. (2008); Gilbert et al. (2009); Zhang and Gilbert (2010); Tian et al. (2012);
Yang and Ding (2020)). Many of these approaches build on methods from the survey
sampling literature (e.g., Cassel et al. (1976); Sa¨rndal et al. (2003)).
For example, Tsiatis et al. (2008) consider covariate adjusted analyses for com-
paring two treatments in randomized clinical trials. Letting X be an indicator for
randomization to the treated or control arms, the focus of inference is on β in the
model
E(Y | X) = α+βX
12Another look at inference after prediction
in order to estimate the treatment effect ∆ = E(Y | X = 1) − E(Y | X = 0). A
standard estimator of ∆ is ∆(cid:98) =
Y1
−
Y0
, where
Y1
=
n−1(cid:80)n
Y X and
Y0
=
1 i=1 i i
n−1(cid:80)n
Y (1−X ) are the empirical means of Y in the treated and control groups of
0 i=1 i i
sizes n and n , respectively. To utilize information in baseline covariates, Z, Tsiatis
1 0
et al. (2008) propose an augmented estimator of the treatment effect given by
n
(cid:88)
∆(cid:98) − (X −X¯ )[n−1h(0)(Z )+n−1h(1)(Z )], (2)
i 0 i 1 i
i=1
where X¯ = n−1(cid:80)n X and h(k)(Z) for k = 0,1 are arbitrary scalar functions of
i=1 i
Z. As X ⊥ Z are independent due to randomization, the augmentation term in 2
converges in probability to 0, analogously to the augmentation terms in the PPI and
CC estimators. In the context of PB inference, pre-trained models could be used for
predicting Y on the basis of Z for h(k)(Z) for k = 0,1.
4.3 Improving precision of Monte Carlo simulations with control variates
The augmentation technique has also been utilized to improve the efficiency of Monte
Carlo estimation. Specifically, Rothery (1982) considers using a control variate, a
statistic whose properties are known, to improve precision when estimating power
via simulation. For exposition, suppose interest lies in estimating the power of a test
T of H : θ = 0 for some scalar parameter θ. Let S denote an existing test of the
0
same hypothesis whose power µ = E (Z) is known. Define Y as an indicator that
Z θ
test T rejects and Z as an indicator that test S rejects. For a set of n simulation
replicates, the baseline estimator of power for T is simply Y =
n−1(cid:80)n
Y while a
i=1 i
control variate augmented estimator is given by:
n
1 (cid:88)
[Y −λ{Z −µ }]. (3)
i i Z
n
i=1
The variance of the augmented estimator is minimized at λ∗ = Cov(Y,Z) Rothery
Var(Z)
(1982); Davidson and MacKinnon (1992). The augmentation term reduces the vari-
ance of the unadjusted power estimate Y in proportion to the correlation between Y
and Z. Building on work from the survey sampling literature, a similar idea was later
used for mean estimation in the semi-supervised setting by Zhang et al. (2019).
5 Numerical studies
5.1 Data generation
We perform several simulation studies to compare the performance of the PPI and CC
methods. In all settings, we generate two datasets: (i) a training dataset with realiza-
tions of (Y,ZT,X )T to train the ML model and (ii) an independent analysis dataset
1
13Gronsbell et al.
forevaluatingthePBinferencemethodscontainingalabeleddatasetwithrealizations
of (Y,ZT,X )T and an unlabeled dataset with realizations of (ZT,X )T. We use 10,000
1 1
ˆ
replications of the analysis dataset for each scenario and a single training set as f is
assumed to be fixed. The size of the training set is fixed at 10,000 and the size of the
analysis dataset is also set to n = 10,000. We let n = 1000,2000,3000,4000,5000
lab
and n = n−n . For both the training and analysis datasets, (ZT,X )T are gener-
unlab lab 1
ated from a multivariate normal distribution. Each element of (ZT,X )T has a mean
1
of 0, variance of 1, and covariance of 0.4 with the remaining variables. The goal is
to conduct inference on the marginal association of Y and X in a linear model. The
1
parameter of interest is therefore β∗ defined as
1
(β∗,β∗) = argminE[(Y −(β +β X ))2].
0 1 0 1 1
β
We consider 2 primary scenarios for evaluating the PB inference methods. In both
scenarios, the outcome is generated as
Y = X +θ X W +WTθ +ϵ
1 INT 1 1
where WT = (W ,...,W ) and the error term ϵ is simulated independently from a
1 q
standard normal distribution. We varied the coefficients of θ and θ as well as q
INT
and the composition of Z.
Scenario 1 We consider two sub-scenarios to illustrate Corollary 1.1, that is, that
ˆ ˆ
βPPIa is more efficient than βlab when the sufficient condition holds. In both
sub-scenarios, we let p = 10, θ = (1,1,0.5,0.5,0.25,0 )T, and θ = 0.
1×5 INT
In Scenario 1a, we obtain a strong prediction model by regressing Y on Z =
(X ,W ,...,W )T while in Scenario 1b we obtain a weaker prediction model
1 1 10
by regressing Y on Z = (X ,W ,X W )T.
1 5 1 5
Scenario 2 We consider two sub-scenarios to illustrate the results in Corollary 1.1
ˆ ˆ
and Corollary 2.1, which show that βPPIa can be less efficient than βlab with
ˆ ˆ
a weak prediction model while βCC is at least as efficient βlab. In both sce-
narios, we mimic a common scenario where the prediction model does not
contain X (e.g., in a genome-wide association study). In Scenario 2a, we
1
let p = 9, θ = (0.2,0.1 × 1 )T, and θ = 0. We obtain the prediction
1×8 INT
model by regressing Y on Z = (W ,...,W )T. In Scenario 2b, we let p = 3,
1 9
θ = (0.25,0.25,0.25)T, and θ = 0.25. We obtain the prediction model by
INT
regressing Y on Z = (W ,W ,W ,W W ,W W ,W W )T. Scenario 2b uses a
1 2 3 2 1 3 1 3 2
weaker prediction model than Scenario 2a.
ˆ ˆ ˆ
We evaluate the performance of the PPI and CC methods: βPPI, βPPIa, and βCC.
We also include two comparison methods: βˆ lab, serving as the baseline, and γˆall,
highlighting the limitations of naive inference. We present the percent bias for each
14Another look at inference after prediction
estimator. For the PPI and CC methods, we present the confidence interval (CI)
length ratio, computed as the mean of the ratio of the length of the PB inference
ˆ
estimator’s normal-based CI relative to the length of the normal-based CI for βlab
(which is simply the ratio of the asymptotic standard errors). The CI length ratio
has a baseline value of 1, which indicates that the CI of the PB inference method is
ˆ
the same length as the CI for βlab. A CI length ratio less than 1 indicates that the
ˆ
PB inference estimator has lower standard error than βlab.
5.2 Results
Table 1 presents the percent bias of the various estimators across the different scenar-
ios and labeled dataset sizes. With the exception of γˆall, the methods exhibit minimal
bias as they are all consistent estimators for β∗. The limitations of naive inference
1
are particularly apparent in Scenarios 2a and 2b, which do not include X in ML
1
model training. γˆall exhibits an average percent bias of -47% in Scenario 2a and -64%
in Scenario 2b.
Figure2displaystheCIlengthratioacrossthedifferentsettings. InScenario1, we
ˆ ˆ
observe that βPPIa and βCC perform similarly, with CI lengths much less 1, indicating
ˆ
that the PB inference methods have reduced variance relative to βlab. Comparing
Scenario 1a and 1b, we note a significant decrease in the gains in statistical efficiency
of the PB inference methods due to the weaker prediction model used in Scenario
ˆ
1b. However, all methods are at least as efficient as βlab, as the sufficient condition
in Corollary 1.1 is satisfied. Additionally, as the labeled set size increases and hence
ˆ
π = n /n increases, the efficiency gain decreases as expected. βPPIa is always more
lab
ˆ
efficient than βPPI, particularly when the labeled dataset size is larger, due to the
ˆ
difference in the augmentation terms of these two estimators. In this scenario, βPPIa
ˆ
is asymptotically equivalent to βCC.
ˆ ˆ
Scenario 2 highlights the differences in performance between βPPIa and βCC, which
underscores our findings in Corollary 1.1 and Corollary 2.1. In both sub-scenarios,
ˆ ˆ ˆ ˆ
βCC consistently outperforms both βPPI and βPPIa and is at least as efficient as βlab.
ˆ
In Scenario 2a, βCC achieves an average CI length ratio of 0.9 across the various
ˆ ˆ
sample sizes, indicating a gain in efficiency relative to βlab. In Scenario 2b, βCC is
ˆ ˆ ˆ
comparable to βlab for the larger labeled dataset sizes. In contrast, βPPIa and βPPI are
ˆ ˆ
outperformed by βlab and have confidence intervals up to 30% longer than βlab for the
larger labeled dataset sizes. This behavior is exacerbated in Scenario 2b, where the
ˆ ˆ ˆ
CI of βPPIa is up to 50% longer than the CI for βlab and up to 72% longer for βPPI.
15Gronsbell et al.
Labeled set size
Method 1000 2000 3000 4000 5000
Scenario 1a
γˆall -0.181 0.124 0.650 0.430 0.165
ˆ
βlab 0.035 -0.042 0.015 0.005 -0.005
ˆ
βPPI 0.004 0.010 -0.004 -0.018 -0.001
ˆ
βPPIa 0.007 0.000 0.002 -0.009 -0.003
ˆ
βCC 0.007 0.000 0.002 -0.009 -0.003
Scenario 1b
γˆall -1.070 0.181 0.426 -0.423 -1.205
ˆ
βlab 0.008 0.002 -0.016 -0.001 0.004
ˆ
βPPI 0.007 -0.001 -0.003 -0.008 -0.010
ˆ
βPPIa 0.007 0.000 -0.007 -0.006 -0.003
ˆ
βCC 0.006 0.000 -0.007 -0.006 -0.003
Scenario 2a
γˆall -46.980 -47.523 -45.946 -46.725 -47.695
ˆ
βlab -0.033 -0.023 0.027 0.004 0.005
ˆ
βPPI -0.023 -0.009 -0.003 -0.032 -0.003
ˆ
βPPIa -0.024 -0.012 0.006 -0.018 0.001
ˆ
βCC -0.030 -0.016 0.016 -0.007 0.003
Scenario 2b
γˆall -64.272 -64.115 -63.307 -63.759 -64.627
ˆ
βlab -0.012 -0.003 0.002 0.012 0.013
ˆ
βPPI 0.000 0.001 -0.006 0.012 0.009
ˆ
βPPIa -0.001 0.000 -0.004 0.012 0.010
ˆ
βCC -0.014 -0.006 -0.003 0.011 0.010
Table 1: Percent biases of the various estimators. Negative values indicate underesti-
mation, while positive values indicate overestimation of the true parameter.
16Another look at inference after prediction
Figure 2: Confidence interval length ratio of the PB inference methods compared to
ˆ
βlab. A ratio less than 1 indicates that the PB inference method has a
ˆ
confidence interval that is shorter than the confidence interval for βlab.
17Gronsbell et al.
6 Discussion
In this paper, we showed that adding a simple weight to the PPI estimator provides
valid inference and is guaranteed to be at least as efficient as estimation using only
labeled data. The weight is drawn from the work of Chen and Chen (2000) in the con-
text of double sampling. In contrast, the original PPI estimator can be less efficient
thanestimationusingonlythelabeleddata,apropertythatcanlimititspracticalutil-
ity. By incorporating the weight derived from Chen and Chen (2000), the augmented
estimation approach principally balances the contribution of labeled and unlabeled
data and extends the applicability of the PPI method to scenarios where leveraging
all available data is essential for making valid and efficient inference. In addition to
theoretical results that support these findings, our simulation studies demonstrate
that the PPI method can result in substantial efficiency loss when the ML model is
inaccurate. Additionally, we provide connections to several existing approaches from
the economics and statistics literature that provide some interesting context for the
PB inference problem. While we focused on least squares estimation, these results
extend to other settings. The work of Chen and Chen (2000) and Angelopoulos et al.
(2023a)applytoawidevarietyofmodelsbeyondlinearregression, suchasparameters
defining generalized linear models or parameters derived from estimating equations.
The work of Angelopoulos et al. (2023a) was pivotal in advancing the growing
body of literature in PB inference. Recent work has considered settings where the
ML model is not black box and where active learning is utilized to obtain the labeled
data Zrnic and Cand`es (2024b,b). Additionally, two concurrent pre-prints have noted
potential losses in efficiency of PB inference approaches. The authors of the PPI
paper introduced a new method, PPI++, which aims to address this limitation of
the PPI estimator (Angelopoulos et al., 2023b). The PPI++ method is designed to
adaptively minimize the trace of the asymptotic variance-covariance of the proposed
estimator to ensure that it yields an improvement over an approach based solely
on labeled data. In contrast, the approach of Chen and Chen (2000) minimizes
the mean square error among a class of estimators with a linear augmentation term.
Simultaneously, Miao et al. (2023) introduced the PoSt-Prediction Adaptive inference
(PSPA) estimator that is both robust to the accuracy of the ML model and guaranteed
to be at least as efficient as estimation using only labeled data. Unlike the PPI++
estimator, the PSPA estimator is designed to achieve element-wise variance reduction
and was shown to outperform the PPI++ approach. The PSPA approach is based
on augmenting estimating equations with information from Y(cid:98), while the CC method
provides a simple and natural extension to the PPI method by adding a weight to the
augmentation term.
18Another look at inference after prediction
Code availability
Ouranalysescanbereplicatedwiththecodeat: https://github.com/SelinaS37/PBInference.
Acknowledgments
TheauthorsgratefullyacknowledgefundingfromanNSERCDiscoveryGrant(RGPIN-
2021-03734), the Connaught Fund, the University of Toronto Data Science Institute,
and the McLaughlin Center for Genetic Research.
ˆ
Appendix A. Asymptotic properties of βPPIa
A.1 Preliminaries
Notation. We denote the outer product of a vector v as v⊗2 = vvT. We denote that
a matrix, M, is positive semi-definite as M ⪰ 0, and positive definite as M ≻ 0. For
ˆ ˆ
any estimator β, we let aVar(β) represent its asymptotic variance-covariance matrix.
We define the difference in asymptotic variance-covariance matrices between two es-
ˆ ˆ ˆ ˆ ˆ ˆ
timators β and β as ∆ (β ,β ) = n[aVar(β )−aVar(β )].
1 2 aVar 1 2 1 2
Basic assumptions. We make the standard assumption that Var(X) ≻ 0. We also
assume that X has compact support and that (Y ,Y(cid:98),XT)T have finite first and second
i i
moments.
A.2 Proof of Theorem 1
ˆ
Theorem 1 (Influence function expansion of βPPIa) Let R be the indicator of
i
whether the ith observation is labeled. Then,
n
(cid:16) (cid:17) (cid:88)
n 21 βˆ PPIa −β∗ = n−1
2
IFPPIa(Y i,Y(cid:98) i,X i;β∗,γ∗)+o p(1)
i=1
(cid:110) (cid:104) (cid:105) (cid:111)
where IFPPIa(Y ,Y(cid:98),X ;β∗,γ∗) = A [ ϕ(Y ,X ;β∗)] Ri + ϕ(Y(cid:98),X ;γ∗) π−Ri and
i i i i i π i i π
(cid:16) (cid:17)
A = [E(XXT)]−1. Therefore, n 21 βˆ PPIa −β∗ converges weakly to a zero-mean mul-
tivariate normal distribution with variance-covariance
E[IFPPIa(Y ,Y(cid:98),X ;β∗,γ∗)IFPPIa(Y ,Y(cid:98),X ;β∗,γ∗)T].
i i i i i i
(cid:16) (cid:17)
Proof First, note that n1
2
βˆ PPIa −β∗ may be decomposed as
(cid:16) (cid:17) (cid:104)(cid:16) (cid:17) (cid:105)
n1
2
βˆ PPIa −β∗ = n 21 βˆ lab −β∗ −(γˆlab −γ∗)+(γˆall −γ∗) .
19Gronsbell et al.
We consider the asymptotic linear expansions of βˆ lab, γˆlab, and γˆall. Since βˆ lab is the
familiar least squares estimator,
(cid:32)
n
(cid:33)−1
n
n21 (cid:16) βˆ lab −β∗(cid:17) = n−1(cid:88)lab X XT n−1 2 (cid:88)lab X (Y −XTβ∗). (4)
lab lab i i lab i i i
i=1 i=1
By Slutsky’s theorem and the assumption that lim n /n = π, equation (4) can
n→∞ lab
be rewritten as
n (cid:26) (cid:27)
(cid:16) (cid:17) (cid:88) R
n1 2 βˆ lab −β∗ = n−1 2 A [X i(Y i −XT iβ∗)] πi +o p(1). (5)
i=1
Similar arguments can also be used to show that
n (cid:26) (cid:27)
(cid:88) (cid:104) (cid:16) (cid:17)(cid:105) R
n 21 (γˆlab −γ∗) = n−1
2
A X
i
Y(cid:98)
i
−XT iγ∗ πi +o p(1)
i=1
and
n
(cid:88) (cid:110)(cid:104) (cid:16) (cid:17)(cid:105)(cid:111)
n 21 (γˆall −γ∗) = n− 21 A X
i
Y(cid:98)
i
−XT iγ∗ +o p(1).
i=1
Consequently,
n (cid:26)(cid:20) (cid:21)(cid:27)
(cid:16) (cid:17) (cid:88) R π −R
n 21 βˆ PPIa −β∗ = n− 21 A ϕ(Y i,X i;β∗) i +ϕ(Y(cid:98) i,X i,γ∗) i +o p(1)
π π
i=1
n
(cid:88)
= n− 21 IFPPIa(Y i,Y(cid:98) i,X i;β∗,γ∗)+o p(1).
i=1
Itfollowsfromthestandardcentrallimittheorem(CLT)thatn1 2(βˆ PPIa−β∗)converges
weakly to a zero mean multivariate Gaussian distribution with variance-covariance
(cid:104) (cid:105)
given by E IFPPIa(Y ,Y(cid:98),X ;β∗,γ∗)IFPPIa(Y ,Y(cid:98),X ;β∗,γ∗)T .
i i i i i i
A.3 Proof of Corollary 1.1
ˆ ˆ
Corollary 1.1 (Comparison of βPPIa with βlab) The difference in the asymptotic
ˆ ˆ
variance-covariance of βlab and βPPIa is
(cid:18) (cid:19)
1 (cid:16) (cid:17)
ˆ ˆ
∆ (βlab,βPPIa) = −1 A 2Σ −Σ A.
aVar π Y,Y(cid:98) Y(cid:98),Y(cid:98)
20Another look at inference after prediction
ˆ ˆ
βPPIa is therefore more efficient than βlab if and only if 2Σ ≻ Σ . A sufficient
Y,Y(cid:98) Y(cid:98),Y(cid:98)
ˆ ˆ
condition for βPPIa to have lower variance-covariance than βlab is that
(cid:104) (cid:105)
E (cid:0) ϵ −ϵ (cid:1)2 | X < E(cid:0) ϵ2 | X(cid:1) .
Y(cid:98) Y Y
Proof Equation (5) in the proof of Theorem 1 gives the asymptotic expansion of
n1(βˆ
lab−β). The standard CLT implies
n1(βˆ
lab−β) converges weakly to a zero-mean
2 2
multivariate Gaussian distribution with variance-covariance given by
(cid:26) R2(cid:27)
1
E [Aϕ(Y,X;β∗)]⊗2 = AΣ A.
π2 π Y,Y
Similarly, by Theorem 1
(cid:26) R2(cid:27) (cid:26)
(cid:104) (cid:105)⊗2 (π
−R)2(cid:27)
n[aVar(βˆ PPIa)] =E [Aϕ(Y,X;β∗)]⊗2 +E Aϕ(Y(cid:98),X;γ∗)
π2 π2
(cid:26) (cid:27)
(cid:104) (cid:105)T R(π −R)
+2E [Aϕ(Y,X;β∗)] Aϕ(Y(cid:98),X;γ∗)
π2
1 (cid:104) (cid:105)
= A Σ +(1−π)Σ −2(1−π)Σ A.
π Y,Y Y(cid:98),Y(cid:98) Y,Y(cid:98)
Thus,
(cid:18) (cid:19)
1 (cid:16) (cid:17)
ˆ ˆ
∆ (βlab,βPPIa) = −1 A 2Σ −Σ A.
aVar π Y,Y(cid:98) Y(cid:98),Y(cid:98)
ˆ ˆ
βPPIa is therefore more efficient than βlab if and only if 2Σ ≻ Σ . Additionally,
Y,Y(cid:98) Y(cid:98),Y(cid:98)
(cid:16) (cid:17) (cid:104) (cid:105)
if 2E(cid:0) ϵ ϵ | X(cid:1) > E ϵ2 | X or equivalently if E (cid:0) ϵ −ϵ (cid:1)2 | X < E(ϵ2 | X), the
Y Y(cid:98) Y(cid:98) Y(cid:98) Y Y
ˆ ˆ ˆ
diagonal entries of ∆ (βlab,βPPIa) are positive and βPPIa will have a lower variance
aVar
ˆ
than βlab.
A.4 Examples for Corollary 1.1
We provide several scenarios where the sufficient condition from Corollary 1.1 is satis-
ˆ ˆ
fied and βPPIa is more efficient than βlab. These scenarios characterize settings where
the predictions contain more information about Y beyond what is explained by the
linear regression model based on X (i.e., the focus of statistical inference.)
Examples of Corollary 1.1. Let Z = (XT,WT)T where W are predictors of Y
ˆ ˆ ˆ
beyond X. βPPIa is more efficient than βlab in the following scenarios where f perfectly
estimates the true regression function:
1. fˆ (X) = E(Y | X) and E(Y | X) ̸= XTβ∗
21Gronsbell et al.
2. fˆ (Z) = E(Y | Z), W ⊥ Y | X, and E(Y | X) ̸= XTβ∗
3. fˆ (Z) = E(Y | Z) and W ̸⊥ Y | X
4. fˆ (W) = E(Y | W) and X ⊥ Y | W
Proof For scenario 1, assume that fˆ (X) = E(Y | X). It follows that
γ∗ = E(XXT)E[XE(Y | X)] = β∗
so that
(cid:104) (cid:105)
E (cid:0) ϵ −ϵ (cid:1)2 | X = E(cid:8) [E(Y | X)−Y]2 | X(cid:9) < E(cid:0) ϵ2 | X(cid:1) = E(cid:2) (Y −XTβ∗)2 | X(cid:3) .
Y(cid:98) Y Y
This inequality is strict provided that E(Y | X) ̸= XTβ∗. If the regression model is
correctly specified (i.e., E(Y | X) = XTβ∗), then βˆ lab and βˆ PPIa are asymptotically
equivalent.
For scenario 2, assume that fˆ (Z) = E(Y | Z) where Z = (XT,WT)T and W ⊥ Y |
X. Similar to scenario 1,
γ∗ = E(XXT)E[XE(Y | X,W)] = E(XXT)E[XE(Y | X)] = β∗.
Therefore,
(cid:104) (cid:105)
E (cid:0) ϵ −ϵ (cid:1)2 | X = E(cid:8) [E(Y | Z)−Y]2 | X(cid:9)
Y(cid:98) Y
= E(cid:8) [E(Y | X)−Y]2 | X(cid:9) = Var(Y | X) ≤ E(cid:0) ϵ2 | X(cid:1) .
Y
The inequality is strict when E(Y | X) ̸= XTβ∗ and is an equality if the regression
model is correctly specified.
For scenario 3, again assume that fˆ (Z) = E(Y | Z) so that γ∗ = β∗. It then
follows that
(cid:104) (cid:105)
E (cid:0) ϵ −ϵ (cid:1)2 | X = E(cid:8) [E(Y | Z)−Y]2 | X(cid:9)
Y(cid:98) Y
= Var(Y | X)−Var[E(Y | X,W) | X]
< Var(Y | X) ≤
E(cid:0)
ϵ2 |
X(cid:1)
.
Y
The equality is strict as we assume W ̸⊥ Y | X.
ˆ
Lastly, for scenario 4, assume that f(W) = E(Y | W) and X ⊥ Y | W. Then,
γ∗ = E(XXT)E[XE(Y | W)] = E(XXT)E[XE(Y | W,X)] = β∗.
Analogously to the argument for scenario 3,
(cid:104) (cid:105)
E (cid:0) ϵ −ϵ (cid:1)2 | X = E(cid:8) [E(Y | W)−Y]2 | X(cid:9)
Y(cid:98) Y
22Another look at inference after prediction
= Var(Y | X)−Var[E(Y | W) | X]
< Var(Y | X) ≤
E(cid:0)
ϵ2 |
X(cid:1)
.
Y
ˆ
Appendix B. Asymptotic properties of βPPI
ˆ
B.1 Asymptotic expansion of βPPI
ˆ
Theorem S1 (Influence function expansion of βPPI) Let R be the indicator of
i
whether the ith observation is labeled. Then,
n
(cid:16) (cid:17) (cid:88)
n1
2
βˆ PPI −β∗ = n−1
2
IFPPI(Y i,Y(cid:98) i,X i;β∗,γ∗)+o p(1)
i=1
(cid:110)(cid:104) (cid:105)(cid:111)
where IFPPI(Y ,Y(cid:98),X ;β∗,γ∗) = A ϕ(Y ,X ;β∗)Ri +ωPPI(Y(cid:98),X ,γ∗)π−Ri , and
i i i i i π i i π
(cid:16) (cid:17)
ωPPI(Y(cid:98) i,X i,γ∗) = ϕ(Y(cid:98)i 1, −X πi;γ∗). Therefore, n1
2
βˆ PPI −β∗ converges weakly to a zero-
mean multivariate normal distribution with variance-covariance
(cid:104) (cid:105)
E IFPPI(Y ,Y(cid:98),X ;β∗,γ∗)IFPPI(Y ,Y(cid:98),X ;β∗,γ∗)T .
i i i i i i
ˆ
Proof The only difference in studying the asymptotic properties of βPPI compared
to βˆ PPIa is that we rely on the asymptotic linear expansion of γˆunlab rather than γˆall.
Using the same argument as in the proof of Theorem 1,
n (cid:26) (cid:27)
(cid:88) (cid:104) (cid:16) (cid:17)(cid:105) 1−R
n1
2
(γˆunlab −γ∗) = n− 21 A X
i
Y(cid:98)
i
−XT iγ∗ 1−πi +o p(1).
i=1
Consequently,
(cid:40)(cid:34) (cid:35)(cid:41)
n1
(cid:16)
βˆ PPI
−β∗(cid:17)
= n−1
(cid:88)n
A ϕ(Y ,X
;β∗)R
i +
ϕ(Y(cid:98) i,X i;γ∗)π −R
i +o (1)
2 2 i i p
π 1−π π
i=1
n
(cid:88)
= n− 21 IFPPI(Y i,Y(cid:98) i,X i;β∗,γ∗)+o p(1).
i=1
(cid:16) (cid:17)
ThestandardCLTimpliesn1 βˆ PPI −β∗ convergesweaklytoazero-meanmultivari-
2
(cid:104) (cid:105)
atenormalwithvariance-covarianceE IFPPI(Y ,Y(cid:98),X ;β∗,γ∗)IFPPI(Y ,Y(cid:98),X ;β∗,γ∗)T .
i i i i i i
23Gronsbell et al.
ˆ ˆ
B.2 Comparison of βPPI & βlab
ˆ ˆ
Corollary S1.1 (Comparison of βPPI with βlab) Thedifferenceintheasymptotic
ˆ ˆ
variance-covariance of βlab and βPPI is
(cid:18) (cid:19)
1 1
ˆ ˆ
∆ (βlab,βPPI) = A 2Σ − Σ A.
aVar π Y,Y(cid:98) 1−π Y(cid:98),Y(cid:98)
βˆ PPI is therefore more efficient than βˆ lab if and only if 2Σ ≻ 1 Σ . A sufficient
Y,Y(cid:98) 1−π Y(cid:98),Y(cid:98)
ˆ ˆ
condition for βPPI to have lower variance than βlab is that
(cid:18) (cid:19)
(cid:104) (cid:105) 1
E (cid:0) ϵ −ϵ (cid:1)2 | X < E(cid:0) ϵ2 | X(cid:1) + 1− E(cid:0) ϵ2 | X(cid:1) .
Y(cid:98) Y Y 1−π Y(cid:98)
Proof From the proof for Corollary 1.1, n[aVar(βˆ lab)] = 1AΣ A. By Theorem S1,
π Y,Y
(cid:26) R2(cid:27) (cid:26) (cid:104) (cid:105)⊗2 (π −R)2 (cid:27)
n[aVar(βˆ PPI)] =E [Aϕ(Y,X;β∗)]⊗2 +E Aϕ(Y(cid:98),X;γ∗)
π2 π2(1−π)2
(cid:26) (cid:27)
(cid:104) (cid:105)T R(π −R)
+2E [Aϕ(Y,X;β∗)] Aϕ(Y(cid:98),X;γ∗)
π2(1−π)
1 1 2
= AΣ A+ AΣ A− AΣ A
π Y,Y π(1−π) Y(cid:98),Y(cid:98) π Y,Y(cid:98)
(cid:20) (cid:21)
1 1
= A Σ + Σ −2Σ A.
π Y,Y 1−π Y(cid:98),Y(cid:98) Y,Y(cid:98)
Thus,
(cid:20) (cid:21)
1 1
ˆ ˆ
∆ (βlab,βPPI) = A 2Σ − Σ A.
aVar π Y,Y(cid:98) 1−π Y(cid:98),Y(cid:98)
βˆ PPI is therefore more efficient than βˆ lab if and only if 2Σ ≻ 1 Σ . Note that if
Y,Y(cid:98) 1−π Y(cid:98),Y(cid:98)
(cid:16) (cid:17) (cid:104) (cid:105)
2E(cid:0) ϵ ϵ | X(cid:1) > 1 E ϵ2 | X or equivalently if E (cid:0) ϵ −ϵ (cid:1)2 | X < E(ϵ2 | X)+
Y Y(cid:98) 1−π Y(cid:98) Y(cid:98) Y Y
(cid:16) (cid:17)
(cid:0) 1− 1 (cid:1)E ϵ2 | X , the diagonal entries of ∆ (βˆ lab,βˆ PPI) are positive and βˆ PPI
1−π Y(cid:98) aVar
ˆ
will have a lower variance than βlab.
ˆ ˆ
B.3 Comparison of βPPI & βPPIa
Recall from Theorem 1 and S1 that
n (cid:26)(cid:20) (cid:21)(cid:27)
(cid:16) (cid:17) (cid:88) R π −R
n1
2
βˆ PPIa −β∗ = n− 21 A ϕ(Y i,X i;β∗) i +ϕ(Y(cid:98) i,X i,γ∗) i +o p(1) and
π π
i=1
24Another look at inference after prediction
n (cid:26)(cid:20) (cid:21)(cid:27)
(cid:16) (cid:17) (cid:88) R π −R
n1
2
βˆ PPI −β∗ = n− 21 A ϕ(Y i,X i;β∗) i +ωPPI(Y(cid:98) i,X i,γ∗) i +o p(1).
π π
i=1
ˆ ˆ
From Corollary 1.1 and Corollary S1.1, βPPI is more efficient than βlab if and only
if
1
2Σ ≻ Σ
Y,Y(cid:98) 1−π Y(cid:98),Y(cid:98)
ˆ ˆ
whereas βPPIa is more efficient than βlab if and only if
2Σ ≻ Σ .
Y,Y(cid:98) Y(cid:98),Y(cid:98)
ˆ ˆ ˆ ˆ
Therefore, βPPIa is more efficient than βlab when βPPI is more efficient than βlab.
Moreover, the difference between asymptotic variance-covariance can be derived from
Corollary 1.1 and S1.1 as
ˆ ˆ ˆ ˆ ˆ ˆ
∆ (βPPI,βPPIa) = ∆ (βlab,βPPIa)−∆ (βlab,βPPI)
aVar aVar aVar
(cid:18) (cid:19) (cid:18) (cid:19)
1 (cid:16) (cid:17) 1 1
= −1 A 2Σ −Σ A− A 2Σ − Σ A
π Y,Y(cid:98) Y(cid:98),Y(cid:98) π Y,Y(cid:98) 1−π Y(cid:98),Y(cid:98)
(cid:18) (cid:19)
2−π
= A Σ −2Σ A.
1−π Y(cid:98),Y(cid:98) Y,Y(cid:98)
Note that if βˆ PPI is less efficient than βˆ lab then 1 Σ ≻ 2Σ , implying
1−π Y(cid:98),Y(cid:98) Y,Y(cid:98)
2−π
Σ ≻ 2Σ .
1−π Y(cid:98),Y(cid:98) Y,Y(cid:98)
ˆ ˆ
This confirms that when βPPI is less efficient than βlab, it will also be less efficient
ˆ ˆ ˆ
than βPPIa. A sufficient condition for βPPIa to be more efficient than βPPI is
1
E[ϵ2 | X] < E[(ϵ −ϵ )2 | X]+ E[ϵ2 | X].
Y Y(cid:98) Y 1−π Y(cid:98)
ˆ
Appendix C. Asymptotic properties of βCC
C.1 Proof of Theorem 2
ˆ
Theorem 2 (Influence function expansion of βCC) Following Chen and Chen
(2000),
n
(cid:16) (cid:17) (cid:88)
n1
2
βˆ CC −β∗ = n−1
2
IFCC(Y i,Y(cid:98) i,X i;β∗,γ∗)+o p(1)
i=1
25Gronsbell et al.
(cid:104) (cid:105)
whereIFCC(Y ,Y(cid:98),X ;β∗,γ∗) = A ϕ(Y ,X ;β∗)Ri +ωCC(Y(cid:98),X )π−Ri andωCC(Y(cid:98),X)
i i i i i π i i π
(cid:16) (cid:17)
is defined in (1). Therefore, n1 βˆ CC −β∗ converges weakly to a zero-mean multi-
2
variate normal distribution with variance-covariance
(cid:104) (cid:105)
E IFCC(Y ,Y(cid:98),X ;β∗,γ∗)IFCC(Y ,Y(cid:98),X ;β∗,γ∗)T .
i i i i i i
Proof The proof follows from Chen and Chen (2000), but is detailed for complete-
ness. Recall that
(cid:110) (cid:104) (cid:105)(cid:111)
W(cid:99)CC = A(cid:98) C(cid:100)ov ϕ(Y,X;β∗),ϕ(Y(cid:98),X;γ∗) {V(cid:100)ar[ϕ(Y(cid:98),X;γ∗)]}−1A(cid:98)−1
−1
= A(cid:98)Σ(cid:98) Σ(cid:98) A(cid:98)−1
Y,Y(cid:98) Y(cid:98),Y(cid:98)
where
(cid:32)
n
(cid:33)−1
(cid:88)lab
A(cid:98) = n X XT ,
lab i i
i=1
n
(cid:88)lab (cid:16) (cid:17)(cid:16) (cid:17)
Σ(cid:98) = n−1 Y −XTβˆ lab Yˆ −XTγˆlab X XT,
Y,Y(cid:98) lab i i i i i i
i=1
n
(cid:88)lab (cid:16) (cid:17)2
and Σ(cid:98) = n−1 Yˆ −XTγˆlab X XT.
Y(cid:98),Y(cid:98) lab i i i i
i=1
It follows that
(cid:40) (cid:41)
n
(cid:104) (cid:105) (cid:88)(cid:104) (cid:16) (cid:17)(cid:105) R
n1
2
W(cid:99)CC(γˆlab −γ∗) = AΣ Y,Y(cid:98)Σ− Y(cid:98)1
,Y(cid:98)
n−1
2
X
i
Y(cid:98)
i
−XT iγ∗ πi +o p(1)
i=1
and
(cid:40) (cid:41)
n
(cid:104) (cid:105) (cid:88)(cid:104) (cid:16) (cid:17)(cid:105)
n1
2
W(cid:99)CC(γˆall −γ∗) = AΣ Y,Y(cid:98)Σ− Y(cid:98)1
,Y(cid:98)
n− 21 X
i
Y(cid:98)
i
−XT iγ∗ +o p(1).
i=1
Consequently,
n (cid:20) (cid:21)
(cid:16) (cid:17) (cid:88) R π −R
n 21 βˆ CC −β∗ =n− 21 A ϕ(Y i,X i;β∗) i +ωCC(Y(cid:98) i,X i) i +o p(1).
π π
i=1
n
(cid:88)
=n− 21 IFCC(Y i,Y(cid:98) i,X i;β∗,γ∗)+o p(1).
i=1
26Another look at inference after prediction
(cid:16) (cid:17)
The standard CLT implies n1 βˆ CC −β∗ converges weakly to a zero-mean multi-
2
variate normal distribution with variance-covariance
(cid:104) (cid:105)
E IFCC(Y ,Y(cid:98),X ;β∗,γ∗)IFCC(Y ,Y(cid:98),X ;β∗,γ∗)T .
i i i i i i
C.2 Proof of Corollary 2.1
ˆ ˆ
Corollary 2.1 (βCC is at least as efficient as βlab) The difference in the asymp-
ˆ ˆ
totic covariance of βlab and βCC is
(cid:18) (cid:19)
1
∆ (βˆ lab,βˆ CC) = −1 AΣ Σ−1 Σ A ⪰ 0.
aVar π Y,Y(cid:98) Y(cid:98),Y(cid:98) Y,Y(cid:98)
ˆ ˆ
Therefore, βCC is guaranteed to be at least as efficient as βlab.
Proof From the proof for Corollary 1.1, n[aVar(βˆ lab)] = 1AΣ A. By Theorem
π Y,Y
ˆ
2.1, n[aVar(βCC)] is given by
(cid:26) R2(cid:27) (cid:26)
(cid:104) (cid:105)⊗2 (π
−R)2(cid:27)
E [Aϕ(Y,X;β∗)]⊗2 +E AΣ Σ−1 ϕ(Y(cid:98),X;γ∗)
π2 Y,Y(cid:98) Y(cid:98),Y(cid:98) π2
(cid:26) (cid:27)
(cid:104) (cid:105)T R(π −R)
+2E [Aϕ(Y,X;β∗)] AΣ Σ−1 ϕ(Y(cid:98),X;γ∗)
Y,Y(cid:98) Y(cid:98),Y(cid:98) π2
1 1−π 1−π
= AΣ A+ AΣ Σ−1 Σ A−2 AΣ Σ−1 Σ A
π Y,Y π Y,Y(cid:98) Y(cid:98),Y(cid:98) Y,Y(cid:98) π Y,Y(cid:98) Y(cid:98),Y(cid:98) Y,Y(cid:98)
1 1−π
= AΣ A− AΣ Σ−1 Σ A.
π Y,Y π Y,Y(cid:98) Y(cid:98),Y(cid:98) Y,Y(cid:98)
Hence, ∆ (βˆ lab,βˆ CC) = 1−πAΣ Σ−1 Σ A ⪰ 0.
aVar π Y,Y(cid:98) Y(cid:98),Y(cid:98) Y,Y(cid:98)
C.3 Proof of Corollary 2.2
ˆ
Corollary 2.2 (βCC is the most efficient estimator in its class) Let the mean
squared error (MSE) of βˆ for estimating β∗ be defined as E[(βˆ − β∗)T(βˆ − β∗)].
Analogously to the well-known result for the best linear predictor (e.g., Bickel and
Doksum (2015), page 40), W(cid:99)CC is the weight, W, that minimizes the asymptotic
MSE of estimators of the form βˆ lab −W(γˆlab −γˆall) for estimating β∗.
27Gronsbell et al.
Proof Letting βˆ = βˆ lab −W(γˆlab −γˆall), the MSE of estimating βˆ is
W W
(cid:26) (cid:27)
(cid:104) (cid:105)⊗2
Q(W) = E βˆ lab −W(γˆlab −γˆall)−β∗ .
Thus, the optimal choice of W that minimizes the MSE is
W∗ = E[(βˆ lab −β∗)(γˆlab −γˆall)](cid:8)E(cid:2) (γˆlab −γˆall)⊗2(cid:3)(cid:9)−1 .
Note that the first term is the covariance between βˆ lab −β∗ and γˆlab −γˆall, and the
second term is the inverse variance-covariance of γˆlab −γˆall. Since
(cid:34) (cid:35)
n
(cid:88) R −π
n1
2
(γˆlab −γˆall) = A n− 21 X i(Y(cid:98)
i
−XT iγ∗) i
π
+o p(1),
i=1
we have Cov(βˆ lab−β∗,γˆlab−γˆall) = 1−πAΣ A and Var(γˆlab−γˆall) = 1−πAΣ A.
π Y,Y(cid:98) π Y(cid:98),Y(cid:98)
Thus,
1−π
(cid:20)
1−π
(cid:21)−1
W∗ = AΣ A AΣ A = AΣ Σ−1 A−1 = WCC.
π Y,Y(cid:98) π Y(cid:98),Y(cid:98) Y,Y(cid:98) Y(cid:98),Y(cid:98)
28Another look at inference after prediction
References
B Alipanahi, F Hormozdiari, B Behsaz, et al. Large-scale machine-learning-based
phenotyping significantly improves genomic discovery for optic nerve head mor-
phology. American Journal of Human Genetics, 108(7):1217 – 1230, 2021. doi:
10.1016/j.ajhg.2021.05.004.
Emily Alsentzer, Matthew J Rasmussen, Romy Fontoura, Alexis L Cull, Brett
Beaulieu-Jones, Kathryn J Gray, David W Bates, and Vesela P Kovacheva. Zero-
shot interpretable phenotyping of postpartum hemorrhage using large language
models. NPJ Digital Medicine, 6(1):212, 2023.
U An, A Pazokitoroudi, M Alvarez, et al. Deep learning-based phenotype imputation
on population-scale biobank data increases genetic discoveries. Nature Genetics,
55(12):2269 – 2276, 2023. doi: 10.1038/s41588-023-01558-w.
Anastasios N Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I Jordan, and
Tijana Zrnic. Prediction-powered inference. Science, 382(6671):669–674, 2023a.
Anastasios N Angelopoulos, John C Duchi, and Tijana Zrnic. Ppi++: Efficient
prediction-powered inference. arXiv preprint arXiv:2311.01453, 2023b.
Peter J Bickel and Kjell A Doksum. Mathematical statistics: basic ideas and selected
topics, volumes I-II package. Chapman and Hall/CRC, 2015.
Claes M Cassel, Carl E Sa¨rndal, and Jan H Wretman. Some results on generalized
difference estimation and generalized regression estimation for finite populations.
Biometrika, 63(3):615–620, 1976.
Yi-HauChenandHungChen. Aunifiedapproachtoregressionanalysisunderdouble-
sampling designs. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 62(3):449–460, 2000.
Denis Conniffe. Estimating regression equations with common explanatory variables
but unequal numbers of observations. Journal of Econometrics, 27(2):179–196,
1985.
J Cosentino, B Behsaz, B Alipanahi, et al. Inference of chronic obstructive pul-
monary disease with deep learning on raw spirograms identifies new genetic
loci and improves risk models. Nature Genetics, 55(12):787 – 795, 2023. doi:
10.1038/s41588-023-01372-4.
Andrew Dahl, Michael Thompson, Ulzee An, Morten Krebs, Vivek Appadurai,
Richard Border, Silviu-Alin Bacanu, Thomas Werge, Jonathan Flint, Andrew J
Schork, et al. Phenotype integration improves power and preserves specificity in
biobank-based genetic studies of major depressive disorder. Nature Genetics, 55
(12):2082–2093, 2023.
29Gronsbell et al.
R Davidson and JG MacKinnon. Regression-based methods for using control variates
in monte carlo experiments. Journal of Econometrics, 54(1):203–222, 1992. doi:
10.1016/0304-4076(92)90106-2.
Shuxian Fan, Adam Visokay, Kentaro Hoffman, Stephen Salerno, Li Liu, Jeffrey T
Leek, and Tyler H McCormick. From narratives to numbers: Valid inference us-
ing language model predictions from verbal autopsy narratives. arXiv preprint
arXiv:2404.02438, 2024.
Feng Gan and Wanfeng Liang. Prediction de-correlated inference. arXiv preprint
arXiv:2312.06478, 2023.
JianhuiGao,Clara-LeaBonzel,ChuanHong,PaulVarghese,KarimZakir,andJessica
Gronsbell. Semi-supervised roc analysis for reliable and streamlined evaluation of
phenotyping algorithms. Journal of the American Medical Informatics Association,
31(3):640–650, 2024.
Peter B Gilbert, Alicia Sato, Xiao Sun, and Devan V Mehrotra. Efficient and robust
method for comparing the immunogenicity of candidate vaccines in randomized
clinical trials. Vaccine, 27(3):396–401, 2009.
Kentaro Hoffman, Stephen Salerno, Awan Afiaz, Jeffrey T Leek, and Tyler H Mc-
Cormick. Do we really even need data? arXiv preprint arXiv:2401.08702, 2024.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
ˇ
Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Z´ıdek, Anna
Potapenko, et al. Highly accurate protein structure prediction with alphafold. Na-
ture, 596(7873):583–589, 2021.
Selene Leon, Anastasios A Tsiatis, and Marie Davidian. Semiparametric estimation
of treatment effect in a pretest-posttest study. Biometrics, 59(4):1046–1055, 2003.
Molei Liu, Xinyi Wang, and Chuan Hong. A semiparametric approach for robust and
efficient learning with biobank data. arXiv preprint arXiv:2404.01191, 2024.
Xiaomin Lu and Anastasios A Tsiatis. Improving the efficiency of the log-rank test
using auxiliary covariates. Biometrika, 95(3):679–694, 2008.
Zachary R McCaw, Jianhui Gao, Xihong Lin, and Jessica Gronsbell. Synthetic sur-
rogates improve power for genome-wide association studies of partially missing
phenotypes in population biobanks. Nature Genetics, pages 1–10, 2024.
JMiao, YWu, ZSun, etal. Validinferenceformachinelearning-assistedgenome-wide
association studies. Nature Genetics, 2024. doi: 10.1038/s41588-024-01934-0.
Jiacheng Miao and Qiongshi Lu. Task-agnostic machine learning-assisted inference.
arXiv preprint arXiv:2405.20039, 2024.
30Another look at inference after prediction
Jiacheng Miao, Xinran Miao, Yixuan Wu, Jiwei Zhao, and Qiongshi Lu. Assumption-
lean and data-adaptive post-prediction inference. arXiv preprint arXiv:2311.14220,
2023.
Keshav Motwani and Daniela Witten. Revisiting inference after prediction. Journal
of Machine Learning Research, 24(394):1–18, 2023.
James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression
coefficients when some regressors are not always observed. Journal of the American
statistical Association, 89(427):846–866, 1994.
P Rothery. The use of control variates in monte carlo estimation of power. JRSSC,
31(2):125 – 129, 1982. https://www.jstor.org/stable/2347974.
Carl-Erik Sa¨rndal, Bengt Swensson, and Jan Wretman. Model assisted survey sam-
pling. Springer Science & Business Media, 2003.
Peter Schmidt. Estimation of seemingly unrelated regressions with unequal numbers
of observations. Journal of Econometrics, 5(3):365–377, 1977.
Paravaster AVB Swamy and Jatinder S Mehta. Bayesian and non-bayesian analysis
of switching regressions and of random coefficient regression models. Journal of the
American Statistical Association, 70(351a):593–602, 1975.
Lu Tian, Tianxi Cai, Lihui Zhao, and Lee-Jen Wei. On the covariate-adjusted estima-
tion for an overall treatment difference with data from a randomized comparative
clinical trial. Biostatistics, 13(2):256–273, 2012.
Jiayi Tong, Jing Huang, Jessica Chubak, Xuan Wang, Jason H Moore, Rebecca A
Hubbard, and Yong Chen. An augmented estimation procedure for ehr-based asso-
ciation studiesaccounting fordifferential misclassification. Journal of the American
Medical Informatics Association, 27(2):244–253, 2020.
AA Tsiatis. Semiparametric Theory and Missing Data. Springer New York, NY, 1
edition, 2006. doi: 10.1007/0-387-37345-4.
Anastasios A Tsiatis, Marie Davidian, Min Zhang, and Xiaomin Lu. Covariate ad-
justment for two-sample treatment comparisons in randomized clinical trials: a
principled yet flexible approach. Statistics in medicine, 27(23):4658–4677, 2008.
Siruo Wang, Tyler H McCormick, and Jeffrey T Leek. Methods for correcting infer-
ence based on outcomes predicted by machine learning. Proceedings of the National
Academy of Sciences, 117(48):30266–30275, 2020.
Xuan Wang and Qihua Wang. Semiparametric linear transformation model with
differential measurement error and validation sampling. Journal of Multivariate
Analysis, 141:67–80, 2015.
31Gronsbell et al.
Shu Yang and Peng Ding. Combining multiple observational data sources to estimate
causal effects. Journal of the American Statistical Association, 2020.
XiYang, AokunChen, NimaPourNejatian, HooChangShin, KalebESmith, Christo-
pher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G Flores,
et al. A large language model for electronic health records. NPJ digital medicine,
5(1):194, 2022.
Arnold Zellner. An efficient method of estimating seemingly unrelated regressions
and tests for aggregation bias. Journal of the American statistical Association, 57
(298):348–368, 1962.
AnruZhang,LawrenceDBrown,andTTonyCai. Semi-supervisedinference: General
theory and estimation of means. 2019.
Min Zhang and Peter B Gilbert. Increasing the efficiency of prevention trials by
incorporating baseline covariates. Statistical communications in infectious diseases,
2(1), 2010.
Sicheng Zhou, Nan Wang, Liwei Wang, Hongfang Liu, and Rui Zhang. Cancerbert:
a cancer domain-specific language model for extracting breast cancer phenotypes
from electronic health records. Journal of the American Medical Informatics As-
sociation, 29(7):1208–1216, 2022.
Tijana Zrnic and Emmanuel J Cand`es. Active statistical inference. arXiv preprint
arXiv:2403.03208, 2024a.
Tijana Zrnic and Emmanuel J Cand`es. Cross-prediction-powered inference. Proceed-
ings of the National Academy of Sciences, 121(15):e2322083121, 2024b.
32