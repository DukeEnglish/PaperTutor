Highlights
RMIO:AModel-BasedMARLFrameworkforScenarioswithObservationLossinSomeAgents
ShiZifeng,LiuMeiqin,ZhangSenlin,ZhengRonghao,DongShanling
• Weproposeanovelmodel-basedMARLmethodcapableofensuringstabledecision-makingevenwhensome
agentsarecompletelyunabletoobtainanyobservationalinformation.
• Our approach are the first work to utilize the world model to reconstruct missing observations in multi-agent
environment and effectively reduces prediction errors of the world mode by integrating information across
agents.
• Our approach follows the CTDE paradigm in standard (without observation loss) settings and incorporates
limitedcommunicationthroughworldmodeltoassistdecision-makingwhencertainagentscannotaccessany
observations.
• Byadoptingrewardsmoothingandamorereasonabletrainingstructure,ourmethodachievessuperiorexperi-
mentalresultsinbothstandardandscenariosinvolvingobservationloss.
4202
voN
92
]AM.sc[
1v93691.1142:viXraRMIO: A Model-Based MARL Framework for Scenarios with Observation Loss
in Some Agents
ShiZifeng,LiuMeiqin,ZhangSenlin,ZhengRonghao,DongShanling
ZhejiangUniversity,No.38ZhejiangRoad,XihuDistrict,Hangzhou,310058,Zhejiang,China
Abstract
Inrecentyears,model-basedreinforcementlearning(MBRL)hasemergedasasolutiontoaddresssamplecomplexity
in multi-agent reinforcement learning (MARL) by modeling agent-environment dynamics to improve sample effi-
ciency. However, most MBRL methods assume complete and continuous observations from each agent during the
inferencestage,whichcanbeoverlyidealisticinpracticalapplications. Anovelmodel-basedMARLapproachcalled
RMIOisintroducedtoaddressthislimitation, specificallydesignedforscenarioswhereobservationislostinsome
agent. RMIOleveragestheworldmodeltoreconstructmissingobservations, andfurtherreducesreconstructioner-
rorsthroughinter-agentinformationintegrationtoensurestablemulti-agentdecision-making.Secondly,unlikeCTCE
methodssuchasMAMBA,RMIOadoptstheCTDEparadigminstandardenvironment,andenablinglimitedcommu-
nicationonlywhenagentslackobservationdata,therebyreducingrelianceoncommunication. Additionally,RMIO
improvesasymptoticperformancethroughstrategiessuchasrewardsmoothing,adual-layerexperiencereplaybuffer,
andanRNN-augmentedpolicymodel,surpassingpreviouswork. OurexperimentsconductedinboththeSMACand
MaMuJoCoenvironmentsdemonstratethatRMIOoutperformscurrentstate-of-the-artapproachesintermsofasymp-
totic convergence performance and policy robustness, both in standard mission settings and in scenarios involving
observationloss.
Keywords: WorldModel;MARL;MBRL;ObservationLoss;StateEstimation;
1. Introduction
Multi-AgentReinforcementLearning(MARL)providesapowerfulandgeneraldecision-makingframeworkfor
multi-agenttasks. Byeffectivelycoordinatingtheinteractionsbetweenagents, MARLhasbeenextensivelyapplied
in tasks that involve both cooperation and competition among agents, such as multi-agent cluster control [1, 2],
autonomousdriving[3,4],andmulti-agentgames[5,6,7]. Intheseapplications,thepartialobservabilityofinputs,
thedynamicnatureoftheenvironmentandpoliciesoftennecessitatealargenumberofinteractiontrajectoriestotrain
MARLsystems[8],whichcanresultinhighsamplingcosts.
Model-based reinforcement learning methods typically simulate real interaction data by constructing an envi-
ronment interaction dynamics model. This model generates pseudo interaction trajectories, thereby increasing the
quantity of sample data and enhancing sample efficiency. This has been confirmed in single-agent reinforcement
learning environments [9, 10, 11]. Recently, world model techniques based on latent variables have gradually been
appliedtoMARL[12,13,14]. However, theaccuracyconstraintsoftheworldmodelincapturingthedynamicsof
environmentalinteractionssignificantlyimpactthereliabilityofsampletrajectorygeneration. Thishindersthediver-
sifiedexplorationoftherealtrajectorysamplespace,makingtheeffectivepredictionspaceoftheworldmodelnarrow
andinaccurate[14].
IntherealmofMARL,thepredominantmethodologiescurrentlydefinetheinteractionwiththeenvironmentasa
DecentralizedpartiallyobservableMarkovprocess(Dec-POMDP)[15],undertheassumptionthateachagentreceives
reliable, albeitlimited, observableinformationateachinteractionstep. However, inthedynamicandcomplexreal-
world scenarios, the observational information obtained by each agent at any moment can be subject to noise [16],
time delays [17], communication limits [18] or even complete loss [19], often due to communication issues, sensor
limitations,orenvironmentalinterference. Infact,sincetheworldmodelisessentiallyatemporalpredictionmodel,
PreprintsubmittedtoElsevier December2,2024MBRL can utilize it to predict missing or noisy information, making it more suitable for multi-agent tasks under
incomplete observation in terms of mechanism. However, much of the current work has primarily focused on us-
ing world models to generate data rather than leveraging them to further assist decision-making under incomplete
observationconditions.
Inthiswork,weproposearesilientmulti-agentoptimizationframeworkdesignedforenvironmentswithincom-
plete observations, called RMIO, stands for Robust decision-making system, Model-based prediction of missing
information,andhandlingofIncompleteObservationchallenges. RMIOoffersthreekeycontributions:
1. World Model for Completing Missing Observations. RMIO is the first model-based method to utilize the world
model to reconstruct missing observations, ensuring stable decision-making even in environments with observation
loss. Furthermore,byintegratinginformationacrossagents,RMIOeffectivelyreducespredictionerrorsoftheworld
model,furtherenhancingitsabilitytohandleincompleteobservations.
2.CTDEParadigm. Unlikepreviousmodel-basedCTCEapproachessuchasMAMBA[13],RMIOadoptstheCTDE
paradigminthenormalenvironment,whileinthecaseofsomeagentsareunabletoobtainanyobservations,decision-
makingisassistedthroughlimitedcommunication.
3. AsymptoticPerformanceEnhancement. Byincorporatingstrategiessuchasrewardsmoothing,adual-layerexpe-
riencereplaybuffer,andaddinganRNNnetworktothepolicymodel,RMIOachievessuperiorexperimentalperfor-
manceintheCTDEparadigmcomparedtothepreviousCTCEapproach.
Experimental results on various tasks in the StarCraftII [20] and MaMuJoCo [21] benchmark show that RMIO
consistentlyoutperformsexistingmethodsinbothstandard(withoutobservationloss)andobservation-lossenviron-
ment.
2. RelatedWorkes
In this section, we discuss the background and related work of RMIO, including the definition of the environ-
ment in MARL with probabilistic missing observation information, as well as the technical approaches and recent
developmentsinmodel-basedreinforcementlearning(MBRL).
2.1. MARL
In most MARL, the problem is typically defined as a Dec-POMDP [15]. The process is defined by the tuple
⟨N,S,A,P,R,γ,Ω,O⟩, where N represents the number of agents, S denotes the global state space of agents, and
A = (cid:81) Ai describesthejointactionspaceofagents. P(s t+1|s t,a t)representsthestatetransitionprobabilityfunction,
whileR(s,a)istherewardfunctionthatreflectstheteamrewardfollowingthejointactiona ={a1,...,an|ai ∈ Ai,i∈
t t t
{1,...,n}}inthestate ∈S. γ∈(0,1]isthediscountfactorwhichdeterminesthedegreeofimportancegiventofuture
rewards. Ω(s)istheobservationspaceofagents,andO(si)isthemappingfunctionfromstatespaceS toobservation
spaceΩ,whichmeansagentigetsthepartialobservationoi ofsi. IntheprocessofDec-POMDP,theagentichooses
t t
an action ai at time t according to the policy π(ai|τi) , which is conditioned by the action-observation history τi.
t i t t t
Subsequently, the environment will return the team reward r = R(s,a) of the joint action a of agents at time t.
t t t t
Then the environment will undergoes a transition, with the global state st transitioning according to the transition
probabilityfunction P(s t+1|s t,a t). MARLstepsthroughthisprocesswiththegoalofmaximizingthereturnofjoint
policyπ = J(π1,...,πn) := E π[(cid:80)∞ t′=0γ t′r t+t′|s t,a t]. Inmodel-freeMARL, PandRaregenerallynotmodeled. Sothe
agentsneedtotraintheirjointpolicythroughfrequentlyinteractingwiththeenvironmenttoreachthetarget.
2.2. MBRL
In order to cope with the high sampling cost problem, MBRL employs self-supervised learning to construct an
interactive dynamics model, called world model, to estimate the state transition probability distribution P and the
rewardfunctionR.Itisprovedthatexpandingsamplewiththeworldmodelcanimprovesampleefficiency[10,22,23].
Recently, consideringtheintricatenatureofdynamicinteractionsinhigh-dimensionalenvironments, latentvariable
world models have been proposed to represent the state transition process in complex scenarios. For instance, the
Dreamerseries[9,24,25]andrelatedstudiesleveragetheRecurrentStateSpaceModel(RSSM)torepresentthestate
2transitionprocess.Conversely,IRIS[26],Storm[27],TWM[28]andotherrelatedworksemploytheTransformer[29]
to update their latent state. These methods map the current state information to a latent space, and then perform
temporalrecursiontoestimatethelatentstateforthenexttimestep.Finally,thestateinformationatthenexttimestep
isreconstructedfromthelatentspacebacktotheoriginallow-dimensionalspace. Thistemporalprocessenablesthe
simulationofagent-environmentinteractionsandthegenerationofpseudotrajectories,effectivelyimprovingsample
efficiency.
While the world model has found extensive application in single-agent tasks, such as Atari games [30], its uti-
lization in multi-agent environments remains limited. MAMBA [13], drawing inspiration from DreamerV2 [24],
standsoutasapioneeringeffortincraftingaworldmodelspecificallytailoredformulti-agentenvironments. Based
onMAMBA,MAG[14]addressestheissueoflocalmodelpredictionerrorspropagatingthroughmulti-steprollouts
by treating local models as decision-making agents, significantly improving the accuracy of predictions in complex
multi-agent environments. Although MAMBA and MAG demonstrate notable improvements in sample efficiency
overmodel-freemethods, as CTCEparadigm, theirapplicabilityisconstrained, and thereremainsconsiderablepo-
tentialforfurtherenhancementintheirasymptoticconvergenceperformance. Moreover,thesemethodsprimarilyuse
worldmodelsfordatageneration,failingtofullyexploittheirtemporalpredictioncapabilities.
2.3. IncompleteObservations
Currently, mostmodel-basedMARLapproachesassumethateachagentcanaccuratelyobtainlocalobservation
informationateachtimestep. However,thisassumptionoverlookspracticallimitationsincommunicationandobser-
vationsystems,suchasnoisyormissingobservations,communicationnetworkpacketloss,anddelays. Inreal-world
interactiveenvironments,agentsoftencannotacquirecompletelocalobservationinformationintimeandmayevenbe
entirelydeprivedofanyobservationinformationduetointerference. Whilemanystudieshaveaddressedthesechal-
lengestosomeextentinmodel-freeMARLmethods[16,17,19,31], thereisanotablelackoftargetedresearchin
themodel-basedMARLdomain. Therefore,thisstudyfocusesontheissueofincompleteobservationinformationto
ensurethatoptimalasymptoticperformanceismaintainedundersuchconditions. Specifically,weconsiderscenarios
whereagentshaveaprobabilityofreceivingnoobservationinformationatall,therebycreatingextremeobservation
environments.
3. ProblemReformulation
Toformalizetheenvironmentalchallengesunderharshandincompleteinformationconditions,adetaileddefini-
tionofthisspecializedPOMDPisprovided,asshowninDefinition1.
Completion Correction
… … …
…
… …
Figure1: Ateachsteparandomsubsetofagentshasaprobabilityofexperiencingobservationloss. Theprocessofcompletingandcorrecting
observationsbyRMIO(onlymagentsgetobservations).
3Definition1. Observationloss. Ateachtimestept,agentsreceiveobservationsoi accordingtoO(s ,pi),where
t t t

O(s
,pi)=oi t, pi t,
(1)
t t ∅, 1−pi.
t
In order to maintain the stability of the policy under the condition of observations loss, the goal of this work
is to ensure the stable performance of both world models and policies in that special environment. As illustrated
in Figure 1, our method firstly utilizes the world model to fill in missing observation data. Subsequently, it adjusts
the {oˆi}N based on the reliable observations {oˆi}m , aligning it more closely with the actual values, getting the
t i=m+1 t i=1
correctionvalueoˆ. Thereby,wehavegeneralizedthisspecialcaseintoastandardPOMDPproblem.
t
4. Methodology
In this section, we will introduce the composition of the algorithm and the underlying principles. Next, the
improvement in asymptotic performance achieved by RMIO will be discussed. Additionally, the implementation
processofRMIOwillbedeliberatedindetail.
4.1. Architecture
Thearchitectureofworldmodel,asshowninFormulas(2)and(3),whichisimprovedonthebasisofMAMBA,
includesRSSMmodelsandpredictors.
RSSM  R
P Po
re isc otu rer
r
mr ie
o
on
r
dt
m
em
lo
:o dd ee l:l: h
z zˆi
t
ii t
∼
∼=
p
pf r pe oc s( th
(
(zi t zˆ−
i
t
i1 |, |he
hi
ti t i,− )o1)
i t)
(2)

C Co or mre mct ui no in cab tl eoc bk lo:
ck:
o
eˆt
i
t
== ff
cp oir ro
(
(r
oˆ zi t,
,t
o atj)
)t
t com t t
Predictors 

R
DO
e
ib sws ce
a
or
r
uv
d
na :t ti :on: ro
γˆ
ˆˆ tii t i∼∼ ∼pp pro eb ws (((
γr
ˆo ˆˆ
ti
ii t
|
||
h
hh
i
t
ii t
,
,,
z
zz
i
t
ii t
)
))
(3)
t dis t t t
4.1.1. RSSMStructureandReconstructPredictors
Recurrent Model. The recurrent model adopts a GRU [32] structure to accurately learn environmental dynamics in
partiallyobservablemulti-agentenvironments. Itcaptureshistoricalandcurrentstateinformationthroughdetermin-
istic embeddings h and stochastic embeddings z, respectively. During the communication process, the stochastic
t t
embeddings z andactions a interactacrossagents,resultingine,whichservesastheinputfortherecurrentmodel
t t t
toupdatethehistoricalstateembeddings.
Posterior Model. The posterior model (representation model) aims to predict z when the observation o is known.
t t
Thistaskispotentiallyeasiertolearnbyminimizingtheevidencelowerbound[33]. InRMIO,theposteriormodelis
additionallydesignedtoupdate z whenthereisnoobservationloss.
t
Prior Model. In model-based methods, the goal of the prior model (transition model) is to predict zi as accurately
t
aspossiblewithoutpriorinformationoi. ItistrainedbyminimizingtheKullback-Leibler(KL)divergencebetween
t
zˆi andzi toapproximatetheposteriormodel. Thus,theworldmodelcanforecastfuturetrajectorieswithoutthetrue
t t
observationinformationandgeneratesamplesfortrainingthepolicymodel. Unliketraditionalmethods, RMIOnot
onlyutilizesthepriormodeltocreatepseudotrajectories,butalsotopredictziofagentswhoseobservationislost.And
t
itthenemploystheobservationpredictortoreconstructthemissingobservationoˆifromzi. Theprocessofcompleting
t t
andpredictingmissingobservationsinincompleteenvironmentswillbediscussedindetailinSection4.3.1.
4CommunicationBlock. ComposedofTransformers[29],thecommunicationblockfacilitatespromotingcrossagent
learningofglobalinformationinworldmodelsbyintegratingthestatesandactionsofdifferentagents,obtainingthe
encodedembeddingse.
t
Reconstructpredictors. Observation,rewardanddiscountpredictorsareemployedtoreconstructo t,r t+1,andγ tfrom
h and z. Thesepredictorsaretrainedviasupervisedloss.
t t
The world model joint loss includes temporal prediction KL divergence loss and predictor reconstruction loss.
Minimizethejointlossfunctionthroughgradientdescenttoupdatetheworldmodel.
(cid:88)T
L(θ M)= −lnp(cid:0) (cid:98)o
t
| h t,z t(cid:1) −lnp(rˆ
t
| h t,z t)
(4)
t=1
−lnp(cid:0)
(cid:98)γ
t
| h t,z
t(cid:1)+βKL(cid:2)
z t||(cid:98)z
t(cid:3)
4.1.2. CorrectionBlock
…
…
… …
Figure2: Networkcompositionandinferenceprocessofcorrectionblock. Thescenariointhefigureassumesthattherearemagentsobtaining
accurateobservations,andn−magentscannotobtainanyobservationvalues.
Whenobservationlossoccurstoseveralagents(assumingthenumberism),RMIOleveragesthepriormodeland
observationpredictortoreconstructthemissingobservationandobtaintheimputedobservation{oˆi}n . However,
t i=m+1
errorsareinevitableinthisprocess. Tominimizetheimpactofpriorpredictionerrors,RMIOproposesacorrection
blocktorefinetheobservationestimatesobtainedfromthepriormodel,asshowninFigure2. Thiscorrectionblock
takesaccurateobservations{oi}m andimputedobservations{oˆi}n asinputs. Itextractsaccurateinformationfrom
t i=1 t i=m+1
{oi}m toaidincorrecting{oˆi}n ,ultimatelygeneratingthecorrectedvalues{oˆˆi}n . Thecorrectedestimatesare
t i=1 t i=m+1 t i=m+1
thenconcatenatedwiththetruevaluestoformoˆ,asillustratedinFormula5.
t
oˆ ={f ({oˆi}n ,{oi}m ),{oi}m }
t cor t i=m+1 t i=1 t i=1
={{oˆˆi}n ,{oi}m } (5)
t i=m+1 t i=1
5The correction block first uses encoders to encode the original real observation {oi}m (real values) and recon-
t i=1
structedobservation{oˆi}n (estimatedvalues)fromdifferentagents,andthenextractsfeatureinformationthrough
t i=m+1
theMLPlayerandselfattentionlayer[29]. Finally,thecorrectedobservation{oˆˆi}n isdecodedandmergedwith
t i=m+1
theoriginalrealobservationdatatoobtainthe oˆ. Duringthetrainingprocess,RMIOmaskthecompleterealobser-
t
vationo ontheagentdimension,getting{oi}m ,andusetheworldmodeltoreconstructmissingobservations,getting
t t i=1
{oˆi}n . Thecorrectionblocktakesthesetwoitemsasinputsandcalculatesoˆ. Toaccuratelyfitoˆ tothetrueobser-
t i=m+1 t t
vations o,weusetheMSElossfunctionasshowninFormula(6). RMIOtrainsthecorrectionblockbyminimizing
t
theobjectivelossfunctionL .
cor
L (θ )=
1 (cid:88)n (cid:16) oi−oˆˆi(cid:17)2
(6)
cor C n−m t t
i=m+1
4.1.3. Policy
RMIOadoptstheMAPPOmethod[34]asthepolicymodelπ,leveraginganActor-Criticarchitecture. TheActor
(policy)modelπistrainedbyoptimizingthefollowingobjectivefunction:
(cid:104) (cid:16) (cid:17)(cid:105)
L (θ )=E min ρ(π)Aˆ,clip(ρ(π),1−ϵ,1+ϵ)Aˆ ,
policy π t t t t t
whereρ(π)istheimportancesamplingratioofthecurrentandoldpolicies,definedas:
t
π(a|o)
ρ(θ )= t t , (7)
t π π (a|o)
old t t
andAˆ istheadvantagefunction,computedusingGeneralizedAdvantageEstimation(GAE):
t
Aˆ
t
=δ t+(γλ)δ t+1+(γλ)2δ t+2+...
(8)
δ
t
=r t+γV(s t+1)−V(s t),
whereγisthediscountfactor,andλistheGAEparameter.
TheCritic(value)modelV istrainedbyminimizingthefollowingvaluelossfunction:
L (ϕ )=
1 (cid:88)N (cid:16)
V(s)−Rˆ
(cid:17)2
, (9)
value V N t t
t=1
whereRˆ isthetargetreturnfortimestept,computedas:
t
Rˆ
t
=r t+γr t+1+γ2r t+2+···+γT−tr
T
(10)
=r t+γV(s t+1),
wherebootstrappingisusedtoincorporatethevalueestimateifthetrajectoryhasnotterminatedatt+1.
Previousmodel-basedmethods,suchasMAMBAandMAG,relyoncentralizedfeaturerepresentations h,z of
t t
theworldmodelastheinputofpolicymodelduringbothtrainingandexecution. Instead,thepolicymodelπinour
approachdirectlyutilizesdistributed,localenvironmentobservationsoi(wherei∈{1,...,N})foreachagentiasinput
t
in both training and execution process. In addition, RMIO adds GRU units to the policy model to make better use
ofhistoricalinformation. Thisstructureofdecouplingtheworldmodelfromthepolicymodelmakesourmethoda
CTDE(CentralizedTrainingwithDecentralizedExecution)approachinthestandardsettings. Thepolicyisthenused
to compute the agent’s action distribution ai ∼ πi(ai|oi). Notably, these local observations are reconstructed during
t t t
trainingusingthecentralizedworldmodel,whileduringexecutiontheyaredirectlyobtainedbytheagentsinteracting
withtheenvironment.
4.2. AsymptoticPerformanceImprovement
Inordertoimprovetheasymptoticperformance,RMIOadoptsvariousstrategiesasfollows.
64.2.1. RewardSmooth
MBRL trains policies by generating hypothetical trajectories with predicted rewards. However, precise reward
modelingisdifficulttoachieveduetothedynamiccomplexityoftheenvironmentinteraction. Incorrectrewardswill
seriouslyaffecttheiterativeconvergenceprocessofthepolicyπ. Inspiredbythehumanintuition,DreamSmooth[35]
replaces precise reward prediction with rough estimates of rewards in high complexity and sparse reward environ-
ments,includingCrafter[36],RoboDesk[37]andShadowHand[38]. SinceMARLenvironmentshavesimilarchar-
acteristics,RMIOalsoadoptstemporalsmoothtotheteamrewardineachepisodeandensurethatthetotalrewards
remainconsistent:
(cid:88)H
rˆ t ← f(r t−H:t+H)= f i·r clip(t+i,0,T) (11)
i=−H
whereT and H denotethehorizonsofepisodeandsmoothing,andthesmoothingfunction f satisfies(cid:80)H f = 1.
i i=−H i
This method aims to smooth the reward data in each episode, and then use the processed reward data to train the
reward model, so that the reward model fits onto the smoothed reward distribution. In our experiments, Gaussian
smoothingischosentoprocesstherewardfunctioninthetimeseries,asshowninFormula12. Theuseofsmoothed
rewards in MARL also does not change the optimality of the strategy, and the proof process can be referred to the
Appendix.
(cid:16) (cid:17)
exp − i2
f i = (cid:80)H exp2 (cid:16)σ −2 i2 (cid:17) (12)
i=−H 2σ2
4.2.2. DoubleExperienceReplayBuffer
Due to factors such as overfitting and deviations between the distributions of samples from different batches,
theworldmodelmayexhibitabnormaliterationperiodswherethepredicteddistributionsignificantlydeviatesfrom
the true trajectory distribution. These abnormal pseudo trajectories generated during this period, especially the re-
wardsamples[31],cancausethepolicynetworktooptimizeinconflictingdirections,therebydisruptingthenormal
convergenceoptimizationprocess.
Sample
Train
Interaction
Interaction
Sample
Train
Figure3:DoubleExperienceReplayBufferstructure.
Insuchunstablescenarios,adualexperiencereplaybufferstructureisdesignedinRMIOtomitigatethisissue.As
showninFigure3, anadditionalpseudotrajectoryexperiencereplaybufferisintroducedalongsidetheoriginalreal
experiencereplaybufferfortruetrajectories. Thisdesignreducesthecorrelationbetweensamplesandsmoothsout
changesinthetargetdistributionduringtraining.Inaddition,comparedtotrainingdirectlyonsamplesgeneratedfrom
a single trajectory fragment, the additional replay buffer contains samples generated from fragments across several
different trajectories. This diversity helps prevent the policy from overfitting to the data generated by the single
trajectoryandenhancesitsgeneralizationability. Theproofoftheeffectivenessofthedoublereplaybufferstructure
canbefoundintheAppendix.
4.3. OverallAlgorithmProcess
Inthissection,weprovideadetailedexplanationofhowRMIOmanagesenvironmentswithobservedinformation
losses.
74.3.1. TrainingProcessofRMIO
Algorithm1ThetrainingprocessofRMIO
1: Initializejointpolicyπ,worldmodelM,correctionblockC,realtrajectoryreplaybufferB randpseudotrajectory
replaybufferB .
p
2: forN episodesdo
3: Collectanepisodeofreal-environmenttrajectoryandaddittoB r;
4: forE wmepochesdo
5: Initialize z t andh t.
6: Sampleτ r =< o t,a t,r t,γ t,o t+1 >fromB r.
7: UseMforone-steptemporalpredictionandreconstructiononτ r;
8: Calculatethejointone-steploss:L M(θ M)=L rec+βL KL;
9: MinimizeL M(θ M)bygradientdescentandupdateM;
10: endfor
11: forE πepochesdo
12: Initialize z t andh t.
13: Sampleo t fromB r astheinitialdata.
14: forkrolloutstepsdo
15: Agentstakeactiona t accordingtoπ(a t|o t)andcommunicatetogete t = f com(z t,a t).
16: Mpredicts{o t+1,r t+1,γ t+1},andstorethemtoB p;
17: Leto t+1 = o t,t=t+1;
18: endfor
19: forE sampleepochesdo
20: Sampleτ p =< o t,a t,r t,γ t >fromB p;
21: Compute A t andreturnsonτ pandcomputeL π(θ π),L value(ϕ V);
22: MinimizeL π bygradientdescentandsoftupdateπ;
23: endfor
24: endfor
25: forE cepochesdo
26: Samplecontinuousτ c =< o t,a t >fromB r. Initialize z t andh t.
27: forlrolloutstepsdo
28: Maskpartialagents’observationandestimatepriorstates ˆz t = p prior(h t);
29: Reconstruct{oˆi t}n i=m+1 = p obs({hi t,zˆi t}n i=m+1);
30: UseCtocorrectoˆ t = f cor({oi t}m i=1,{oˆi t}n i=m+1);
31: CalculatecorrectionlossL cor(θ C)=MSE(oˆ t,o t);
32: Predictposteriorstate z t = p post(h t,o t)andupdateh t+1 = f rec(f com(z t,a t),h t);
33: endfor
34: MinimizeL cor(θ C)bygradientdescentandupdateC.
35: endfor
36: endfor
AsshowninAlgorithm1,thetrainingprocesscanbedividedintothreeparts: trainingworldmodelM(asshown
in lines 3-10 in Algorithm 1), training policy π (as shown in lines 11-23 in Algorithm 1) and training correction
blockC(asshowninlines25-35inAlgorithm1). InthetrainingprocessofM,RMIOsamplesfromtherealsample
experiencereplaybufferB andupdatesMbyminimizingthejointlossfunctionofsinglesteptemporalpredictionand
r
statereconstructionthroughgradientdescent,asshowninFormula4;Duringthetrainingprocessofthepolicymodel
π,RMIOfirstusesMtogeneratepseudosampletrajectoriesandputsthemintotheexperiencereplaybufferB .Then
p
RMIO samples trajectories in B , calculate the policy advantage function and cumulative return on the trajectories
p
accordingtoFormula8and10,andsoftupdateπ. SincetheexplorationspaceoftheworldmodelMisdetermined
bytheactiondecisionsgeneratedbythepolicymodelπ,andthequalityofthetrainingsamplesforπdependsonthe
predictionaccuracyofM,thetrainingprocessesoftheMandπprocesscomplementeachother. Toensurethestable
8Decentralized execution States synchronization Reconstruction&Execution
Correction block
Posterior
Model
Observation
predictor
Communication
block
Prior Model
Recurrent Model …
Noloss Observation loss
Figure4:ThewholereasoningprocessofRMIOfacingobservationloss(onlymagentsgetaccurate{oi}m )attimestept.RMIOfirstcommunicate
t i=1
tosynchronizethehistorical{ot−i,at−i}1
i=t
amongagents,gettinghistoricalstatusinformationht. Basedonht,thepriormodelandobservation
predictorcanreconstructthemissing{oi}n ,gettingoˆi. Atlast,thecorrectionblockusethepartialaccurate{oi}m tocorrecttheestimated
t i=n−m+1 t t i=1
observation{oˆi t}n i=m+1},gettingoˆt={{oi t}m i=1,{oˆˆi t}n i=m+1}.Thus,theagentscantakeactionbasedontheestimatedoˆt.
convergence of both models, model-based methods usually employ an alternating training approach to jointly train
theworldmodelandthepolicymodel. ThetrainingprocessofConlyneedstomasktherealobservationsofsome
agents to simulate the situation of observation loss, as shown in the line 28 in Algorithm 1. Then the MSE loss of
thecorrectionprocessiscalculatedaccordingtoFormula6andtheweightsofCareupdated,asshowninline34in
Algorithm1.
4.3.2. ReasoningProcessofRMIO
Afterfinishingthecentralizedtrainingprocess,RMIOcanadapttothescenariosinvolvingobservationloss,which
is shown in Algorithm 2 and Figure 4. At each time step, all agents are expected to receive feedback from the en-
vironment and utilize the discriminator D to ascertain if any agents encounter observation loss (as shown in line
3 of Algorithm 2). It is presumed that at time step t, n−m agents encounter observation loss, implying that only
m(m < n)observations{oi}n . However, in the normal step-by-step process, the joint policy model requires com-
t i=m+1
pleteobservationo asinputstomakeactiondecisions.Soitisnecessarytoestimatethemissingobservations{oi}n .
t t i=m+1
Asshowninlines5-9ofAlgorithm2,RMIOfirstusessingle-stepcommunicationamongagentstosynchronizeo,a
i i
from the l steps prior to time t and applies temporal recursion through the posterior model to obtain deterministic
historical state h. Based on this, the prior model is used to estimate the stochastic latent state variable ˆz, and the
t t
observationpredictorreconstructsthemissingobservations{oˆi}n .Finally,thecorrectionmoduleutilizestheobser-
t i=m+1
vationsofotheragentstorefinetheestimatedvaluesofthemissingobservations,thusgeneratingajointobservation
oˆ thatincludesallagents’observationinformation. Fortimestepswithoutobservationloss,RMIOdirectlyusesthe
t
posterior model to obtain the stochastic latent state variable z. Unlike traditional model-based MARL, RMIO can
t
maintainastableactionpolicyregardlessofwhetheragentsreceivecompleteobservationinformation.
AsoutlinedinSection4.1.3,followingtheCTDEparadigm,RMIOeliminatestheneedforinter-agentcommuni-
cationinstandardenvironments. Communicationisonlyrequiredincasesofobservationloss,whereagentsperform
a single communication step to synchronize historical states, unlike MAMBA and MAG, which rely on continuous
communicationateverystep. Consequently,thecommunicationoverheadinRMIOissolelydeterminedbythefre-
quencyofobservationloss. However,whentwoinstancesofobservationlossoccurin"close"succession,thesecond
instancedoesnotrequirere-synchronizationofhistoricalstateinformation. Thisisbecauseouragentsaredesigned
to share the same set of network parameters, allowing local policy models to estimate the policies of other agents.
Foragenti,giventheobservationinformationofotheragentsattimeT −1,andknowingthattheactionprobability
distributionofallagentsdependssolelyontheirobservedstates, itispossibletodirectlyestimatetheactionproba-
bilitydistributionofotheragentsasai ∼ π(ai |oi ). Withtheobservation o andaction a attimeT −1,
T−1 T−1 T−1 T−1 T−1
9Algorithm2ThereasoningprocessofRMIO
1: Loadtheweightsθ wofM,weightsθ πofπ;
2: fort=t 0;t<T done;t=t+1do
3: Agentsgetpartialobservation: {oi t}m i=1 =D(o t);
4: if m<nthen
5: Agentscommunicatetosynchronize{o h,a h}t h=t−l;
6: Initializeh t−land z t−l;
7:
forh=t−l;h<t;h=h+1do
8: h h+1 = f rec(f com(z h,a h),h h);
9: endfor
10: Predictstochasticstate ˆz t : ˆz t = p prior(h t);
11: Reconstruct{oˆi t}n i=m+1 = p obs({hi t,zˆi t}n i=m+1);
12: Correctoˆ t = f cor({oi t}m i=1,{oˆi t}n i=m+1);
13: Updatestochasticstate z t = p post(oˆ t,h t);
14: Agentstakeactions: ai ∼πi(ai|oˆi);
t t t
15: else
16: Agentstakeactions: ai ∼πi(ai|oi);
t t t
17: endif
18: endfor
the world model can be used to perform a prior estimation of o at time T, and ultimately refine oi . In that case,
T T
asshowninFigure5,themissinglocalobservationscanbecompletedlocallywithoutadditionalcommunicationby
combining the prior predictions from the world model with policy estimates from the policy model. This approach
further reduces communication overhead, ensuring that communication frequency is smaller than the frequency of
observationloss. Inexperiment,thestandardsof"close"aredynamicallyadjustedaccordingtothecomplexityofthe
experimentalenvironment.
②estimate other ④reconstruct & correct
②communication ④takeactions agents action own lost observation
… …
① observation loss ①observation ③reconstruct other ⑤take action
③reconstruct&correct loss again agents observation
Reconstructed Recnstructed& corrected Estimated
Lost observation Local observation Agent action
observation observation agent action
Figure5: Takingagent3asanexample,thelightcoloredpartsinthefigurerepresentthestatusinformationofotheragentsthatagent3cannot
access. AttimeT −2,eachagentexecutesinadistributedmanner,andagent3canonlyaccesslocalstateinformation. AttimeT −1,agent3
experiencesobservationloss. Atthistime,themissingobservationinformationissupplementedandcorrectedthroughcommunicationwiththe
worldmodeltosupportagent3’sdecision-making. Throughcommunication,agent3canaccesssomestateinformationofotheragents. Attime
T,agent3experiencesobservationlossagain,anditcanbeconsideredthatthetwoobservationlossesareina"close"proximitystate. Agent3
doesnotneedtocommunicateatthistime,butdirectlyusesthestateinformationofotheragentsobtainedafterT−1communicationtomakeprior
estimatesoftheobservedstateattimeT andtheactionsofotheragents,therebycompletingandcorrectingthemissingobservedstateattimeT.
Thisprocessdoesnotrequirefurthercommunicationandiscompletedlocallybyagent3.
105. Experiments
Inthissection,wewillintroduceRMIO’sempiricalresearchonthechallengingStarCratIIbenchmark(SMAC).
Inthefirstpart,severalbaselines(withoutobservationloss)willbecomparedwithRMIOinanormalenvironment.
Subsequently,aquantitativecomparativeexperimentswillbeconductedtocomparetheperformanceretentionlevels
ofRMIOandotherbaselinesunderdifferentobservationlossconditions.
5.1. Environments
TheStarcraftMultiAgentChallenge(SMAC)[20]isamulti-agentdiscreteandcollaborativecontrolbenchmark
basedonStarcraftII.Eachtaskcontainsascenariowheretherearetwoopposingteams,onecontrolledbythegame
robot and the other controlled by our algorithm. The goal is to defeat all the enemy agents. Our method and other
baselines are tested on 8 maps of SMAC from easy to super hard, including 2s_vs_1sc, 3s_vs_3z, 2s3z, 3s_vs_4z,
3s_vs_5z,1c3s5z,8m,corridor.
The Multi-Agent MuJoCo (MaMuJoCo) [21] is a multi-agent continuous and collaborative control benchmark
based on the MuJoCo physics simulator. Each task involves a collaborative scenario where multiple agents, rep-
resented as different parts of a robot, must work together to complete a specific objective, such as locomotion or
manipulation. Thegoalistomaximizethecollectiverewardbyachievingefficientandcoordinatedcontrolamongthe
agents. OurmethodandotherbaselinesareevaluatedonseveraltasksfromtheMAMuJoCobenchmarkatdifferent
levelsofdifficulty,includingHalfCheetah(2agent),HalfCheetah(6agent),Swimmer(2agent),Swimmer(10agent).
5.2. RewardModelingExperiment
Ablation experiments on reward smoothing were conducted to demonstrate the superiority of RMIO in reward
modeling. The experimental results showed that the loss function of RMIO in reward modeling was significantly
smallerthanthatofitsablationbaselinewithoutrewardsmoothing.Forinstance,Figure6ashowstherewardfunction
valuesofRMIO(withrewardsmoothing)andRMIO*(withoutrewardsmoothing)onthe3s_vs_3zmap,whileFig-
ure6bpresentsthecorrespondinglosscurves. TheresultsindicatethatthelosscurvewithEMArewardsmoothing
isapproximatelyone-tenthofthatintheablationbaseline,anditsdistributionisalsomorestable. Thisdemonstrates
that reward smoothing can effectively enhance the performance of reward modeling. A detailed discussion of the
asymptoticperformanceofRMIOwithrewardsmoothingwillbeprovidedinSection5.3.
Reward Reward loss
 RMIO  RMIO*  RMIO  RMIO* 
0.03
6
0.025
5
0.02
4
0.015
3
0.01
2
1 0.005
0 0 Step
Real environment Step 100k 200k 300k 400k
(a)Thevaluesoftherawrewards(RMIO*)andsmoothedrewards(RMIO) (b)ComparisonofrewardmodelinglosscurvesbetweenRMIOandRMIO*
ateachtimestepwithinasinglecompletetrajectoryonthe3s_vs_3zmap. onthe3s_vs_3zmap.ThecurveissmoothedwithEMAandtheparameter
issetto0.5.
5.3. ExperimentsofConvergencePerformanceinStandardEnvironment
5.3.1. Baselines
WecompareRMIOwithmodel-basedandmodel-freebaselinemethodstoassesstheconvergenceperformanceof
ourapproachunderfullyobservedconditions. Themodel-basedmethodsinclude1)MAMBA,amulti-agentadapta-
tionofDreamerV2[24],whichimprovesthesampleefficiencyofMARLbyanorderofmagnitudeforthefirsttime;2)
112s vs 1sc 8m 3s vs 3z 2s3z
1 1 1 1
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0 0 0 0
10k 20k 30k 40k 20k 40k 60k 20k 40k 60k 20k 60k 100k 140k
3s vs 4z 1c3s5z 3s vs 5z corridor
1 1 1 1
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0 0 0 0
50k 100k 150k 50k 100k 150k 100k 200k 300k 400k 100k 200k 300k 400k
4 agent swimmer 10 agent swimmer 2 agent HalfCheetah 6 agent HalfCheetah
100 2500
80 100 2000 2000
60 50 1500 1500
40 1000 1000
0
20 500 500
0 -50 0 0
-20
0 50k 100k 150k 50k 100k 150k 200k 400k 600k 800k 200k 400k 600k 800k
RMIO(ours) MAMBA MAG MAPPO QMIX FACMAC
Figure7: Comparisonswithotherbaselines. Thesolidlinerepresentstherunningaverageof3differentrandomseeds, andtheshadedarea
correspondstothestandarddeviationbetweentheseruns.TheX-axisrepresentsthenumberofstepstakenintherealenvironment,andtheY-axis
representsthewinrate(SMAC)orepisodereward(MAMuJoCo).
SMAC
Maps MES REIS RMIO MAG MAMBA MAPPO QMIX
2s_vs_1sc 50k 94(5) 90(8) 91(15) 9(12) 0(0)
8m 75k 94(3) 87(9) 77(10) 32(15) 63(9)
3s_vs_3z 75k 96(3) 93(6) 90(10) 12(16) 0(0)
2s3z 150k 98(1) 81(11) 78(13) 46(23) 23(17)
300
3s_vs_4z 200k 96(1) 71(13) 63(37) 0(0) 0(0)
1c3s5z 200k 95(3) 78(8) 70(9) 52(17) 46(19)
3s_vs_5z 500k 95(2) 65(15) 65(8) 58(21) 0(0)
corridor 500k 66(19) 55(25) 46(16) 0(0) 0(0)
MaMuJoCo
Scenarios MES REIS RMIO MAG MAMBA MAPPO FACMAC
4agentSwimmer 200k 98(3) 94(4) 91(6) 52(7) 32(6)
10agentSwimmer 200k 102(2) 96(3) 67(29) -20(7) -38(6)
300
2agentHalfCheetah 1m 2042(48) 1814(58) 1759(64) 823(98) 531(102)
6agentHalfCheetah 1m 2098(42) 1957(47) 1836(58) 946(104) 620(94)
Table1:Duringthetrainingprocess,themaximumepisodesteps(MES)isfixedforeachmapandscene.Aftercompletingtrainingforaspecified
numberofrealenvironmentinteractionsteps(REIS)indifferentenvironments,themodelweightsaresaved,andtheaveragewinrate(inSMAC)or
episodereward(inMaMuJoCo),alongwiththeirstandarddeviations,areindependentlyevaluatedover1000testepisodes.Boldnumbershighlight
thehighestaverageperformanceamongallCTDEmethods.RMIOconsistentlyachievesthebestperformanceacrossalltests.Duetothefactthat
QMIXisonlyapplicabletodiscreteenvironmentssuchasSMAC,theFACMACmethodwasselectedasthemodel-freemethodforcomparisonin
theMaMuJoCoenvironment.
12MAG[14],whichisbasedonMAMBAandtakesintoaccountthelong-termjointeffectsoflocalpredictionsateach
steptogeneratetrajectorieswithlowercumulativeerrors,therebyimprovingthestabilityofasymptoticconvergence
performance. Theadvancedmodel-freemethodsinclude1)MAPPO[34],2)QMIX[5]and3)FACMAC[21].
5.3.2. ResultsandAnalysis
The comprehensive experimental results illustrate the superiority of our approach over both model-based and
model-freemethodsinalltestmapsorscenarioswithinaconstrainednumberofiterations,asshowninTable1and
Figure 7. Compared to other CTCE baselines, RMIO, as a CTDE method, achieves a significantly higher win rate
onvariousmapsinSMACenvironmentsandachieveshigherepisoderewardsinMaMuJoCo, allwhilemaintaining
stableperformanceacrossdifferentrandomseedsandexhibitingmarkedlybetterstabilityinpolicyconvergence.
Thisadvantageprimarilystemsfromthefollowingmechanisms: duringthetrainingoftheworldmodel,dynamic
fluctuations in the distribution of real data samples (e.g., due to sampling bias or exploration during training) may
leadtonon-stationarityintheworldmodel’sperformance. Duringsuchperiods,theoutputoftheworldmodelmay
deviatesignificantlyfromthetrainingdatadistribution,enteringanunacceptableperformanceregime.CTCEmethods
(such as MAMBA and MAG) rely on feature vectors generated by the world model as inputs to the policy model.
Consequently,whentheworldmodel’sperformancebecomesnon-stationary,theperformanceofpolicymodelwhich
isstronglycoupledwiththeworldmodelismoresusceptibletodegradation,suchasthetrainingresultofMAMBAin
10agentswimmerinFigure7. Incontrast,RMIOintroducesmechanismssuchasrewardsmoothingandadual-layer
experiencereplaybuffertoeffectivelymitigatetheimpactoftheworldmodel’snon-stationaryfluctuationsonpolicy
training.Furthermore,RMIOreconstructsobservationdatatoserveasinputstothepolicymodel,adesignthatreduces
theentropyoftheinputfeaturesandthusminimizestheadverseeffectsoftheworldmodel’sperformancefluctuations
onthepolicymodel.Experimentalresultsindicatethat,owingtothesedesigns,RMIOsignificantlyoutperformsother
methodsintermsofstabilityintheMaMuJoCoenvironment.
5.4. ExperimentsofPerformancePreservationinObservation-lossEnvironment
5.4.1. Baselines
Thelackofworldmodelstofillinmissingobservationrendersmodel-freemethodsineffectiveinscenarioswhere
observations are incomplete. Although there are some attempts to use traditional prediction methods like Kalman
filters andGaussian predictions inmulti-agent environments [39,40], they struggleto handle thehighly non-linear,
decentralized, and partially observable complexities, along with the high interaction dynamics, typical of scenarios
like SMAC. Hence, only model-based MARL methods are considered for comparison. In the context of ablation
experiments,wedenotetheRMIOmethodthatexcludescorrectionblocksasRMIO∗forcomparativeanalysis. Toen-
surefairnessinthecomparisonwithMAMBAandMAG,theirpriormodelsandobservationpredictorsareemployed
forpredictingandfillinginmissingobservations,aligningwithRMIO∗.
5.4.2. Observation-lossEnvironmentSetting
To assess the performance stability of RMIO in environments affected by observation loss, we introduce an ob-
servation loss mechanism based on SMAC and MaMuJoCo. At each time step, there exists a probability p that
loss
resultsinarandomsubsetofagentslosingobservationinformation.Notably,toenhancetheexperimentalcomplexity,
we extend the duration of observation loss, that means after the initial occurrence of observation loss, subsequent
losses will persist for the following several steps. In the experiment, this duration was set to 10 steps. This setting
is also closer to the realistic environmental conditions where the observation environment is harsh (there are strong
interferenceandotherfactorslastingforacertainperiodoftime). What’smore,tointensifythechallengeposedby
observation loss in easy maps, the number of agents experiencing observation loss in the four maps of 2s_vs_1sc,
2s3z,3s_vs_3z,3s_vs_4zissettoafixednumbern−1insteadofarandomnumber.
5.4.3. ResultsandAnalysis
Withinthisenvironment, thecorrectionblockcanextractpertinentinformationfrominter-agentcommunication
duringtraining,therebysignificantlydecreasingobservationlosspost-calibration.Forinstance,thetraininglosscurve
inthe3s_vs_3zofSMACisprovidedinFigure8. Uponcompletionoftrainingwith400kreal-timesteps(equivalent
to 2.5M generated time steps), the MSE loss value before correction is 0.007212 and reduced to 0.0009924 after
132s_vs_1sc 8m 3s_vs_3z 2s3z
0.01 0.03
0.014 0.014
0.008 0.012 0.025 0.012
0.01 0.02 0.01
0.006
0.008 0.015 0.008
0.004 0.006 0.006
0.01
0.004 0.004
0.002 0.002 0.005 0.002
0 0 0 0
80k 100k 120k 140k 160k 180k 100k 150k 200k 250k 300k 100k 150k 200k 250k 300k 350k 400k 100k 150k 200k 250k 300k 350k
3s_vs_4z 1c3s5z 3s_vs_5z corridor
0.03 0.03
0.014 0.014
0.012 0.025 0.012 0.025
0.01 0.02 0.01 0.02
0.008 0.015 0.008 0.015
0.006 0.006
0.01 0.01
0.004 0.004
0.002 0.005 0.002 0.005
0 0 0 0
100k 200k 300k 400k 500k 1M 1.5M 2M 2.5M 3M 500k 1M 1.5M 2M 2.5M 500k 1M 1.5M 2M 2.5M 3M
Before correction Before correction(Max) After correction After correction(Max)
Figure8: Comparisonoflossfunctionvaluesbeforeandaftercorrectingtheworldmodelpredictions. Before/aftercorrection(MAX)isthe
maximumvalueofthecorrectionloss.
2s_vs_1sc 3s_vs_3z 2s3z 3s_vs_4z
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
8m 1c3s5z 3s_vs_5z corridor
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.20 0.40 0.60 0.80 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.20 0.40 0.60 0.80 1.00 0.00 0.20 0.40 0.60 0.80 1.00
4 agentSwimmer 10agentSwimmer 2 agentHalfCheetah 6 agentHalfCheetah
120 120 2.5 2.5
100 100 2.0 2.0
80 80
1.5 1.5
60 60
1.0 1.0
40 40
20 20 0.5 0.5
0 0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
RMIO RMIO* MAG MAMBA
Figure9:TheperformanceofRMIOmethodanditsablationcontrolgroup,aswellasothermodel-basedmethods,in1000randommatchesunder
differentobservationlossprobabilitiesplossondifferentmapsinSMACanddifferentscenariosinMAMUJOCO.InSMAC,thestatisticalmeasure
istheaveragepercentagewinrateandstandarddeviationof3randomseeds,whileinMaMuJoCo,thestatisticalmeasureistheaverageepisode
rewardandstandarddeviationof3randomseeds.
14correction. Thisresultshowsthatthecorrectionblockcaneffectivelyreducethepredictionerrorandachievenearly
anorderofmagnitudereduction.
Aftercompletingthetrainingprocess,RMIOistestedintheobservation-lossSMACandMaMuJoCoenvironment.
AllexperimentaloutcomesarepresentedinFigure9,Table??andTable??asillustrativeexamples. Theexperiment
resultsindicatethattheRMIOapproachconsistentlyachievesbothnotablyhigherwinratesacrossallmapsinSMAC
andnotablyhigherepisoderewardsinallscenariosinMaMuJoCocomparedtoothermethods,includingtheRMIO*
ablationcomparisonmethod,undervaryingobservationlossprobabilitiesp .Moreover,RMIOdemonstratesrobust
loss
performance as the observation loss probability p increases, showing minimal susceptibility to observation loss
loss
effectsacrossdiversemaps. Remarkably,evenwhen p = 1,RMIOmaintainsarelativelyhighwinrateinSMAC,
loss
particularly in moderately challenging maps. These results suggest that the completion and correction block can
effectivelyestimatemissingobservationsbyutilizingtemporalrecursiverelationshipsandintegratingagent-specific
information,eveninscenarioswhereobservationlossoccursateachtimestep.
However, it is also observed that while RMIO demonstrates high efficiency in low-difficulty tasks, its efficacy
diminishesinhigh-difficultytasks(e.g., corridor). Thisdeclineinperformancecanbeattributedtovariousfactors.
Firstly, as the number of agents increases, each agent faces difficulties in observing the states of all other agents,
thus weakening the effectiveness of the correction block’s fusion of agent-related information. Secondly, in high-
difficultytasks,theaccuracyofthepriormodel’spredictionsalsodecreases. Thisdeclineinaccuracymaystemfrom
theincreasedcomplexityofpredictionsduetohigherfeaturedimensionsandtheheightenedcomplexityofthejoint
policyspaceresultingfromalargernumberofopponentagents.
6. Conclusion
Inthiswork,RMIOfirsteffectivelyaddressesthechallengeoflostobservationindynamicenvironmentsbylever-
agingpriorpredictionmodelsandobservationreconstructionpredictorstomanagemissingobservationdata. Based
on this, a correction block further refines the observation estimates by incorporating correlation information among
agents. Moreover, By decoupling the world model and the policy model, RMIO achieves the CTDE paradigm in
standardenvironmentsettings.Inthecaseofobservationloss,italsoonlyrequireslimitedone-stepcommunicationto
assistdecision-making,whileensuringthatthecommunicationfrequencyislowerthanthefrequencyofobservation
loss. Additionally, RMIO enhances asymptoticconvergence performance through reward smoothing, double replay
bufferstructuredesign,andtheintegrationofanadditionalRNNnetworkinthepolicymodel. Empirically,weshow
that RMIO outperforms both model-based and model-free baselines on several challenging tasks in the SMAC and
MaMuJoCobenchmarks,especiallywhenfacedwithincompleteobservations. Futureresearchwillfocusonenhanc-
ingitsrobustnessbytacklingnon-stationaryperformancechallengescausedbyobservationlossinmorecomplexand
dynamic scenarios. Additionally, plans are underway to extend its applicability to collaborative competition tasks,
withtheultimategoalofdevelopingacomprehensiveandadaptiveMARLframeworkthatcanaddressawiderange
ofreal-worldapplicationswithvaryinglevelsofobservationcompletenessandenvironmentalcomplexity.
References
[1] L. Matignon, L. Jeanpierre, A.-I. Mouaddib, Coordinated multi-robot exploration under communication constraints using decentralized
markovdecisionprocesses,ProceedingsoftheAAAIConferenceonArtificialIntelligence(2022)2017–2023.
[2] S.-M.Hung,S.N.Givigi,Aq-learningapproachtoflockingwithuavsinastochasticenvironment,IEEETransactionsonCybernetics(2017)
186–197.
[3] C.You,J.Lu,D.Filev,P.Tsiotras,Advancedplanningforautonomousvehiclesusingreinforcementlearninganddeepinversereinforcement
learning,RoboticsandAutonomousSystems(2019)1–18.
[4] S. Shalev-Shwartz, S. Shammah, A. Shashua, Safe, multi-agent, reinforcement learning for autonomous driving, arXiv preprint
arXiv:1610.03295(Oct2016).
[5] T.Rashid,M.Samvelyan,C.S.DeWitt,G.Farquhar,J.Foerster,S.Whiteson,Monotonicvaluefunctionfactorisationfordeepmulti-agent
reinforcementlearning,JournalofMachineLearningResearch21(178)(2020)1–51.
[6] B. Baker, I. Kanitscheider, T. Markov, Y. Wu, G. Powell, B. McGrew, I. Mordatch, Emergent tool use from multi-agent autocurricula,
InternationalConferenceonLearningRepresentations(2020).
[7] D.Ye,Z.Liu,M.Sun,B.Shi,P.Zhao,H.Wu,H.Yu,S.Yang,X.Wu,Q.Guo,Q.Chen,Y.Yin,H.Zhang,T.Shi,L.Wang,Q.Fu,W.Yang,
L.Huang,Masteringcomplexcontrolinmobagameswithdeepreinforcementlearning,ProceedingsoftheAAAIConferenceonArtificial
Intelligence34(04)(2020)6672–6679.
[8] S.Gronauer,K.Diepold,Multi-agentdeepreinforcementlearning:asurvey,ArtificialIntelligenceReview(2022)895–943.
15[9] D.Hafner,T.Lillicrap,J.Ba,M.Norouzi,Dreamtocontrol:learningbehaviorsbylatentimagination,InternationalConferenceonLearning
Representations(2020).
[10] M. Jänner, J. Fu, M. Zhang, S. Levine, When to trust your model: model-based policy optimization, Advances in Neural Information
ProcessingSystems(Jun2019).
[11] T.M.Moerland,J.Broekens,A.Plaat,C.M.Jonker,etal.,Model-basedreinforcementlearning: asurvey,FoundationsandTrends®in
MachineLearning16(1)(2023)1–118.
[12] O.Krupnik,I.Mordatch,A.Tamar,Multi-agentreinforcementlearningwithmulti-stepgenerativemodels,ConferenceonRobotLearning
(2020)776–790.
[13] V.Egorov, A.Shpilman, Scalablemulti-agentmodel-basedreinforcementlearning, ProceedingsoftheInternationalConferenceonAu-
tonomousAgentsandMultiagentSystems(2022)381–390.
[14] Z.Wu, C.Yu, C.Chen, J.Hao, H.H.Zhuo, Modelsasagents: optimizingmulti-steppredictionsofinteractivelocalmodelsinmodel-
basedmulti-agentreinforcementlearning,ProceedingsoftheThirty-SeventhAAAIConferenceonArtificialIntelligenceandThirty-Fifth
ConferenceonInnovativeApplicationsofArtificialIntelligenceandThirteenthSymposiumonEducationalAdvancesinArtificialIntelligence
(2023).
[15] F.A.Oliehoek,C.Amato,Aconciseintroductiontodecentralizedpomdps,SpringerBriefsinIntelligentSystems(Jan2016).
[16] Y.Sun,W.Li,D.Zhao,Convergencetimeandspeedofmulti-agentsystemsinnoisyenvironments,Chaos:AnInterdisciplinaryJournalof
NonlinearScience22(4)(Dec2012).
[17] B.Chen,M.Xu,Z.Liu,L.Li,D.Zhao,Delay-awaremulti-agentreinforcementlearningforcooperativeandcompetitiveenvironments,arXiv
preprintarXiv:2005.05441(2020).
[18] L. Matignon, L. Jeanpierre, A.-I. Mouaddib, Coordinated multi-robot exploration under communication constraints using decentralized
markovdecisionprocesses,ProceedingsoftheAAAIConferenceonArtificialIntelligence(2012)2017–2023.
[19] J.Gao,S.Wang,X.Wang,Y.Zhang,X.Yang,Reinforcementlearningformulti-agentwithasynchronousmissinginformationfusionmethod,
InternationalJournalofMachineLearningandCybernetics(2024)1–17.
[20] M.Samvelyan,T.Rashid,C.SchroederdeWitt,G.Farquhar,N.Nardelli,T.G.Rudner,C.-M.Hung,P.H.Torr,J.Foerster,S.Whiteson,
Thestarcraftmulti-agentchallenge,ProceedingsoftheInternationalConferenceonAutonomousAgentsandMultiAgentSystems(2019)
2186–2188.
[21] B.Peng,T.Rashid,C.SchroederdeWitt,P.-A.Kamienny,P.Torr,W.Böhmer,S.Whiteson,Facmac:Factoredmulti-agentcentralisedpolicy
gradients,AdvancesinNeuralInformationProcessingSystems34(2021)12208–12221.
[22] V.Feinberg,A.Wan,I.Stoica,M.I.Jordan,J.E.Gonzalez,S.Levine,Model-basedvalueestimationforefficientmodel-freereinforcement
learning,arXivpreprintarXiv:1803.00101(2018).
[23] A.Ayoub,Z.Jia,C.Szepesvari,M.Wang,L.Yang,Model-basedreinforcementlearningwithvalue-targetedregression,InternationalCon-
ferenceonMachineLearning(2020)463–474.
[24] D.Hafner,T.Lillicrap,M.Norouzi,J.Ba,Masteringatariwithdiscreteworldmodels,InternationalConferenceonLearningRepresentations
(2021).
[25] D.Hafner,J.Pasukonis,J.Ba,T.Lillicrap,Masteringdiversedomainsthroughworldmodels,arXivpreprintarXiv:2301.04104(2023).
[26] V.Micheli,E.Alonso,F.Fleuret,Transformersaresample-efficientworldmodels,InternationalConferenceonLearningRepresentations
(2023).
[27] W.Zhang,G.Wang,J.Sun,Y.Yuan,G.Huang,Storm: Efficientstochastictransformerbasedworldmodelsforreinforcementlearning,
AdvancesinNeuralInformationProcessingSystems36(2024).
[28] J.Robine,M.Höftmann,T.Uelwer,S.Harmeling,Transformer-basedworldmodelsarehappywith100kinteractions,DeepReinforcement
LearningWorkshopNeurIPS2022(2022).
[29] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.Gomez,L.Kaiser,I.Polosukhin,Attentionisallyouneed,AdvancesinNeural
InformationProcessingSystems(Jun2017).
[30] M.G.Bellemare,Y.Naddaf,J.Veness,M.Bowling,Thearcadelearningenvironment:anevaluationplatformforgeneralagents,Journalof
ArtificialIntelligenceResearch(2018)253–279.
[31] J.Wang,Y.Liu,B.Li,ProceedingsoftheAAAIConferenceonArtificialIntelligence(04)(2020)6202–6209.
[32] K.Cho,B.vanMerrienboer,C.Gulcehre,D.Bahdanau,F.Bougares,H.Schwenk,Y.Bengio,Learningphraserepresentationsusingrnn
encoder-decoderforstatisticalmachinetranslation,ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(Jan2014).
[33] D.P.Kingma,M.Welling,Auto-encodingvariationalbayes,CoRRabs/1312.6114(2013).
[34] C.Yu, A.Velu, E.Vinitsky, J.Gao, Y.Wang, A.Bayen, Y.Wu, Thesurprisingeffectivenessofppoincooperativemulti-agentgames,
AdvancesinNeuralInformationProcessingSystems35(2022)24611–24624.
[35] V.Lee,P.Abbeel,Y.Lee,Dreamsmooth:Improvingmodel-basedreinforcementlearningviarewardsmoothing,InternationalConferenceon
LearningRepresentations(2024).
[36] D.Hafner,Benchmarkingthespectrumofagentcapabilities,DeepReinforcementLearningWorkshopNeurIPS2021(2021).
[37] H. Kannan, D. Hafner, C. Finn, D. Erhan, Robodesk: A multi-task reinforcement learning benchmark, https://github.com/
google-research/robodesk(2021).
[38] M.Plappert,M.Andrychowicz,A.Ray,B.McGrew,B.Baker,G.Powell,J.Schneider,J.Tobin,M.Chociej,P.Welinder,etal.,Multi-goal
reinforcementlearning:Challengingroboticsenvironmentsandrequestforresearch,arXivpreprintarXiv:1802.09464(2018).
[39] S.Wang,X.Zhan,Y.Zhai,J.Shen,H.Wang,Performanceestimationforkalmanfilterbasedmulti-agentcooperativenavigationbyemploying
graphtheory,AerospaceScienceandTechnology112(2021)106628.
[40] J.Li,S.Tang,J.Guo,Event-triggereddistributedcooperativeextendedkalmanfilterbasedonformationestimation,AerospaceScienceand
Technology138(2023)108326.
16