NONCOMMUTATIVE MODEL SELECTION FOR DATA CLUSTERING AND
DIMENSION REDUCTION USING RELATIVE VON NEUMANN ENTROPY
ARACELIGUZMA´N-TRISTA´NANDANTONIORIESER∗
Abstract. We propose a pair of completely data-driven algorithms for unsupervised classifica-
tionanddimensionreduction, and weempiricallystudytheirperformanceonanumberofdata
sets,bothsimulateddatainthree-dimensionsandimagesfromtheCOIL-20dataset. Thealgo-
rithms take as input a set of points sampled from a uniform distribution supported on a metric
space,thelatterembeddedinanambientmetricspace,andtheyoutputaclusteringorreduction
of dimension of the data. They work by constructing a natural family of graphs from the data
andselectingthegraphwhichmaximizestherelativevonNeumannentropyofcertainnormalized
heat operators constructed from the graphs. Once the appropriate graph is selected, the eigen-
vectors of the graph Laplacian may be used to reduce the dimension of the data, and clusters
in the data may be identified with the kernel of the associated graph Laplacian. Notably, these
algorithms do not require information about the size of a neighborhood or the desired number
of clusters as input, in contrast to popular algorithms such as k-means, and even more modern
spectralmethodssuchasLaplacianeigenmaps,amongothers.
In our computational experiments, our clustering algorithm outperforms k-means clustering
on data sets with non-trivial geometry and topology, in particular data whose clusters are not
concentrated around a specific point, and our dimension reduction algorithm is shown to work
wellinseveralsimpleexamples.
1. Introduction
Unsupervised clustering and dimension reduction are two of the most important and difficult
problems in modern data analysis, as well as one of the most ubiquitous, appearing in nearly every
area of data science, including image processing, bioinformatics, and natural language processing,
among others. Most popular unsupervised clustering algorithms, including classical methods such
as k-means ([11], Section 14.3.6), in addition to many more recently proposed techniques [3, 16,
5, 1, 9, 18, 20, 7, 2, 17], depend on additional data which must be chosen by the user in order to
run, and which keeps these methods from being completely data-driven. In this paper, we propose
∗Correspondingauthor.
AraceliGuzm´an-Trista´nwassupportedbytheCONAHCYTprogram”EstanciasPosdoctoralesporM´exicopara
la Formaci´on y Consolidaci´on de las y los Investigadores por M´exico”. Antonio Rieser was supported by the US
National Science Foundation under grants No. DMS-1929284 and DMS-1928930, the first while in residence at the
Institute for Computational and Experimental Research in Mathematics in Providence, RI, during the ”Math +
Neuroscience: Strengthening the Interplay Between Theory and Mathematics” program, and the second while in
residence at the Simons-Laufer Mathematical Sciences Research Institute in the spring of 2024 and in a program
supportedbytheMathematicalSciencesResearchInstituteinthesummerof2022,heldinpartnershipwiththethe
UniversidadNacionalAuto´nomadeM´exico. AntonioRieserwasalsosupportedbytheCONAHCYTInvestigadoras
yInvestigadoresporM´exicoProject#1076andbythegrantN62909-19-1-2134fromtheUSOfficeofNavalResearch
Global and the Southern Office of Aerospace Research and Development of the US Air Force Office of Scientific
Research.
1
4202
voN
92
]LM.tats[
1v20991.1142:viXraNONCOMMUTATIVE MODEL SELECTION 2
new spectral clustering and dimension reduction algorithms in which their free parameters may be
chosen by considering the noncommutative information-theoretic aspects of these problems.
Following [13], we interpret the clustering problem to be the problem of assigning each point
in a data set S to the closest connected component of the support X of a probability distribution
µ from which the data was sampled (perhaps with additional noise). Our algorithm works by
constructing a family of weighted graphs G for each r > 0, where the vertices of G are the data
r r
points and the points are connected if they are within a distance r of one another, with weights
given by the ambient distance between two points. If one considers the heat semigroup on each
graph as a heat flow on the vertices, then the resulting steady state of the flow on G is constant
r
on the connected components of each G . We also adopt the model-selection heuristic used in [13]:
r
that the optimal graph in the family of graphs should be the one for which the diffusion process
generated by the graph Laplacian is relatively local initially, but relatively global at the steady
state. We measure this by calculating the relative von Neumann entropy between the (normalized)
heat operator at t = 1 and the (normalized) heat operator at a second time t′ ≫ 1, the latter of
which we interpret as an estimate of limit of the relative von Neumann entropy between the two
operatorsast′ →∞. ChoosingthegraphwherethisrelativevonNeumannentropyismaximalthen
produces a graph which is balanced between vertices being very connected at short distances but
not particularly connected at long distances. Once the graph is chosen, the number of connected
components of the graph is the dimension of the kernel of its Laplacian matrix, and each point
can then be algorithmically assigned to its connected component as in [13] by analyzing the null
space of the graph Laplacian. Alternately, one may use the chosen graph to reduce the dimension
of the data set by using the eigenvectors of the graph Laplacian to construct a map from the data
set S to Rk for some small integer k as is typically done in Laplacian Eigenmaps [3] and Diffusion
Maps [7]. In our experiments, the algorithm improved on the average relative entropy method in
[13]whenappliedtographswhoseedgesweredefinedusingEuclideanballsaroundeachpoint, and
outperformed k-means on several examples using simulated data as well as the unprocessed images
from the COIL-20 image database.
1.1. Contributions and Related Work. A difficult problem inherent to many spectral ap-
proaches to dimension reduction and clustering, including local linear embedding [14], Laplacian
eigenmaps [3], Hessian eigenmaps [9], isomap [18], local tangent space alignment [20], diffusion
maps [7], and vector diffusion maps [17], is how to systematically choose the free parameter nec-
essary to run the algorithms. Indeed, most current practitioners simply choose these parameters
by hand on an ad hoc basis after some trial and error. To the best of our knowledge, there are
two earlier works which have proposed methods for resolving this issue. In [13], the second author
of the present article proposed two new methods for automatic clustering, each of which consisted
of a heuristic for selecting a graph from a family of graphs by examining the action of the heat
semigroup on a basis for the functions from the data set to R, and which then analyzed the kernel
of the relevant graph Laplacian to identify the connected components of the chosen graph. In [15],
a heuristic based on the semigroup property of the operators constructed in diffusion maps was
developed for selecting the free parameter in that method. We remark that these algorithms are
not interchangeable. The semigroup technique from [15] is not expected to work in the setting of
the current article (or that in [13]), because the free parameter r >0 in [13] and the present article
is no longer a priori coupled to the semigroup parameter. Conversely, we do not expect that either
theAverageLocalVolumeMethodortheAverageRelativeEntropyMethodfrom[13]canbemake
to work with diffusion maps, since they both depend on the fact the the graphs in question areNONCOMMUTATIVE MODEL SELECTION 3
disconnected for sufficiently small radii, and all the graphs in diffusion maps are connected (and,
indeed, they are even complete graphs on the vertices).
In this article, we make two main contributions to the literature on spectral clustering and
dimension reduction. First, we introduce a new spectral method which uses the ambient distances
between points (up to some distance r) for the weights in our graphs, instead of the similarity
kernels which are more typically used, and we demonstrate empirically that the spectral properties
oftheLaplaciansandheatsemigroupsofsuchgraphsmayalsobeusedforclusteringanddimension
reduction. Thissimplifiestheconstructionofthegraphsrelativetothestandardconstructions,and
it also clarifies that graph approximations of a metric space can be taken to be geometric models
of the underlying space, and, furthermore, that the diffusion processes analyzed in the subsequent
clustering and dimension reduction techniques may be built on top of the geometry encoded in
the edge weights of the graphs, instead of guessed a priori. Second, and most importantly, we
propose a method for selecting a geometric graph model of a metric space from a one-parameter
family of such models. We do so by maximizing the relative von Neumann entropy between an
operatorintheheatsemigroupforagraphs{G } atsomefinitetime(inthiscaset=1)andan
r r>0
approximation of the operator representing the steady state of the diffusion process. This builds
andimprovesonthemethodsintroducedin[13],anditintroducesgenuinelynoncommutativetools
into the model selection process for data clustering and dimension reduction. We give a number
of experimental demonstrations that the choices of edge weights and model selection techniques
introduced here produce good results for the data clustering and dimension reduction problems,
using both simulated and real data sets.
2. Graph models and model selection
2.1. Graphs, Laplacians, and Heat Semigroups. As in [13], we assume that our data set S
has been sampled from a disconnected metric measure space (X,d ,µ), possibly with noise, and
X
we wish to assign each point x ∈ S to the closest connected component X ⊂ X. The number of
i
connectedcomponentsofX isatopologicalinvariant, andsooneofouraimswillbetoincorporate
topological techniques into the solution to this problem, following the general ideas in [13].
We begin with some basic preliminaries on graph Laplacians and the semigroups they generate.
Foranyfinite,weighted,undirectedgraphG=(V,E,w),whereGcontainsnoloops,i.e. (x,x)̸∈E
for all x ∈ V, and w : E → (0,∞) is a function which assigns a positive weight to every edge, we
define the Laplacian matrix of the graph G, L , by
G

−w(x ,x ) if (x ,x )∈E,
 i j i j
(1) (L ) = 0 if (x ,x )̸∈E and i̸=j
G (i,j) i j
 (cid:80) w(x ,x ) if i=j.
(xi,xk)∈E i k
We also define the corresponding heat operators e−tLG for t ∈ [0,∞). Note that e−tLGe−sLG =
e−(t+s)LG,so{e−tLG}
t∈[0,∞)
formsasemigroupundermatrixmultiplication,whichwecalltheheat
semigroup of G.
TofindtheconnectedcomponentsofagraphG,wewillappealtothefollowingwell-knownfacts
(see, for instance, Lemma 1.7(iv) in [6]):
Theorem 1. The number of connected components of a graph G is equal to the dimension of the
kernel of L .
G
Theorem 2. Each eigenfunction f ∈kerL is constant on each connected component of G.
GNONCOMMUTATIVE MODEL SELECTION 4
Note that the operators L
G
and e−tLG act on the space of functions {f : V
G
→ R}. The above
theorems imply that the clustering problem can be solved if we can find a basis for kerL such
G
that each basis function is non-zero and constant on one of the clusters and zero otherwise.
For the dimension reduction problem, we wish to embed the data set S in to Rk in a way which
preserves the local structure of S as much as possible. The spectral methods for doing so all are
loosely based on the following classical theorem by B´erard, Besson, and Gallot [4] on embeddings
of a Riemannian manifold into ℓ2. We begin with a preliminary definition.
Definition 1. Let M be an n-dimensional closed manifold and let a:={ϕ } be an orthonormal
j j≥0
basis of the Laplacian of M. Define the family of maps ψa :M →ℓ2, t>0 by
t
√ (cid:110) (cid:111)
ψa(x):= 2(4π)n/4t(n+2)/4 e−λjt/2ϕa(x)
t j
j≥1
Theorem 3. Let (M,g) be a closed Riemannian manifold and let a={ϕa} be an orthonormal
j j≥0
basis of its Laplacian. Let g denote the usual Euclidean scalar product on ℓ2. then
E
(1) For all positive t, the map ψa is an embedding of M into ℓ2.
t
(2) the pull-back metric (ψa)∗g is asymptotic to the metric g of M when t→0 .
t can +
Forthepurposesofdimensionreduction,thistheoremimpliesthat,forsomefixedbutsufficiently
small t, the map S →Rk given by
x(cid:55)→{ϕa(x)}
t 1≤j≤k
may be seen as an approximation of the map ψa, up to a multiplicative constant which depends on
t
theintrinsicdimensionofthemanifold,anditthereforeapproximatelypreservesthelocalgeometry
encoded in the Riemannian metric (again, up to some constant coefficients).
2.2. The Clustering and Dimension Reduction Problems. Our approach to both the data
clustering and dimension reduction problems center around choosing a weighted graph which best
estimates the intrinsic geometry of the data from a family of possibile graphs.
Wenowdefinethefamilyofweightedgraphswewillconsider. LetS ⊂(X,d )beafinitesubset
X
of a metric space (X,d ) which itself has been embedded in another metric space (Z,d ). We
X Z
further suppose that, for every x∈X,
d (x,x′)
lim Z =1.
X∋x′→xd X(x,x′)
This guarantees that d (x,x′) and d (x,x′) are close for d (x,x′) (or d (x,x′)) sufficiently small.
Z X X Z
Foreachrealnumberr >0,letG =(V ,E ,w)beaweightedgraph,whereV ,E ,andw :E →R
r r r r r r
are defined by
V =S,
r
E ={(x,x′)∈V ×V |d(x,x′)≤r}
r r r
w(x,x′)=d (x,x′)
Z
That is, the vertices of G are the points in the data set, and two vertices are connected by an
r
edge if they lie in a ball of radius r centered at one of them, and the weight of each edge is the
ambient distance in Z between the vertices. Note that these weights are quite different from those
used in in Laplacian Eigenmaps [3] or Diffusion Maps [7], where the weights use a heat kernel
K : Rn ×Rn → R defined on the ambient space Rn, which is meant to provide a local estimate
r
of a diffusion kernel on X evaluated at the vertices of each edge. Also the graphs G are not fully
r
connected in general, unlike in Laplacian eigenmaps and diffusion maps. No edge is added betweenNONCOMMUTATIVE MODEL SELECTION 5
any pair of vertices x,x′ with d (x,x′)>r. In fact, the graphs G will be completely disconnected
Z r
for sufficiently small r >0.
Once we have the graphs G , to solve the clustering problem, we must choose a scale rˆ > 0
r
so that the connected components of the graph G at this scale best approximate the connected
rˆ
componentsofX. Forthedimensionreductionproblem,ontheotherhand,wewouldliketochoose
a scale rˆ>0 and an embedding Φ:S →Rk, k <n, of the form
Φ(x):=(ϕˆ (x),ϕˆ (x),...,ϕˆ (x)),
0 1 k
where the ϕˆ are the eigenfunctions of L , so that the local geometry of (X,d ) restricted to S is
i Grˆ X
as well-preserved as possible in the image of S ⊂Rk.
2.3. Relative von Neumann Entropy. Quantum information theory has provided many new
toolsandinsightswithwhichtostudylinearoperatorsandoperatoralgebras,motivatedbytheneed
to provide a solid theoretical foundation for quantum computation and quantum communication.
While the computational setting of the present work is unequivocally classical, we are nonetheless
confronted with families of noncommutative Hermitian operators, a setting in which many of the
constructions of quantum information theory are quite natural. In the algorithms and experiments
whichfollow, wewill seethatthesequantumconstructionsnot onlyapplyinthiscontext, butthey
also reveal important information about the collection of graphs {G } and they are essential to
r r>0
our model selection algorithms. In this section, we collect the basic definitions and results from
quantum information theory that we will require.
Definition 2. Let H be a Hilbert space. A positive operator A on H is defined to be an operator
such that for any vector v ∈ H, the inner product ⟨v,Av⟩ is a real, non-negative number. If, in
H
addition, ⟨v,Av⟩ >0 for all v ̸=0, then we say that the operator A is positive definite.
H
Remark 1. We recall that any positive operator has non-negative eigenvalues, and any positive
definite operator has strictly positive eigenvalues. See [8] for this and other properties of positive
operators on Hilbert spaces.
Following physics terminology, we define a density operator as follows.
Definition 3. A positive operator ρ is called a density operator iff Tr(ρ)=1.
We now define the relative von Neumann entropy, also known as the relative quantum entropy.
Definition 4. Let the support of an operator ρ on a Hilbert space H be the set
supp ρ:={v ∈H |ρ(v)̸=0}.
Suppose that ρ is a density operator, and let σ be a positive operator. We define the relative von
Neumann entropy or relative quantum entropy H(ρ||σ) of ρ and σ by
(cid:26)
Tr(ρlog(ρ)−ρlog(σ)) if supp ρ⊆supp σ
H(ρ||σ):=
+∞ Otherwise.
Although the relative von Neumann entropy is neither symmetric nor satisfies the triangle in-
equality, it nonetheless provides a useful way to compare positive operators with trace between 0
and 1. In particular, we have
Proposition 1. [19], Theorem 11.8.2 If ρ is a density operator and 0 ≤ Tr(σ) ≤ 1, then H(ρ||σ)
is non-negative. In addition, under these conditions, H(ρ||σ)=0 iff σ =ρ.NONCOMMUTATIVE MODEL SELECTION 6
Insomespecialcases,therelativevonNeumannentropyreducestotherelativeShannonentropy
of the eigenvalues of the operators, viewed as distributions on a finite space. We will use following
case in our algorithms.
Proposition2. LetρbeadensityoperatoronafinitedimensionalvectorspaceV. Ifσ isapositive
semi-definite operator such that supp ρ ⊆ supp σ and ρ and σ are simultaneously diagonalizable,
then
(cid:88)
H(ρ||σ)= λρlogλρ−λρlogλσ,
i i i i
i
where the λρ are the eigenvalues of ρ and the λσ are the eigenvalues of σ.
i i
Proof. If the matrices ρ and σ are simultaneously diagonalizable, then the expression above is the
sum of the eigenvalues of ρlogρ−ρlogσ, which is equal to the trace. □
2.4. SelectingtheModel. Wenowdescribeourmodelselectionprocedureforboththeclustering
and dimension reduction problems, which may be seen as a genuinely noncommutative version of
the Average Maximum Relative Entropy Method in [13]. Let S be a collection of n points in a
metric space (X,d), and for each 0 < r < diam(S), let G be the graph defined in Section 2.1.
r
Let L
r
denote the graph Laplacian of G
r
and let {e−tLr}∞
t=0
be the resulting heat semigroup. The
heuristicbehindourmodelselectionalgorithmisthattheoperatorse−tLr forlowvaluesoftreflect
the local combinatorial and geometric structure of the graph G . That is, for low values of t,
r
given a function f : V → V which takes the value 1 at a vertex v and is 0 elsewhere, the function
e−tLrf is supported close to v. On the other hand, when t→∞ the heat semigroup converges to a
steady state, erasing the local geometry, but giving the connected components of G . We wish to
r
choose an r >0 such that the local geometry of G
r
reflected in e−Lr (with t=1) is as different as
possible from the steady state lim t→∞e−tLr. We might like to measure this difference by the limit
of the relative entropies as t→∞, however, since the trace of the heat operators Tr(e−tLr) may be
larger than 1 a priori, we first normalize the operators before calculating the relative entropy. Our
experiments confirm that this is a useful metric. Our selected scale rˆwill therefore be
(cid:18) (cid:18) (cid:12)(cid:12) (cid:19)(cid:19)
(2) rˆ:=argmax lim H 1 e−Lr(cid:12) (cid:12)(cid:12) (cid:12) 1 e−tLr ,
t→∞ Tr(e−Lr) (cid:12)(cid:12)Tr(e−tLr)
In practice, it is sufficient to use t ≫ 1, large enough so that the eigenvalues of e−tLr are close to
either0or1. Oncewehavecomputedrˆ,weconstructamapΨ:V →RdimkerLrˆ asin[13](seealso
Algorithm2below)whichsendstheverticesinthei-thconnectedcomponentofG tothestandard
rˆ
basis vector e
i
∈RdimkerLrˆ. To identify the clusters, we then take the inverse image of each of the
e
i
∈RdimkerLrˆ.
3. The Algorithms
We present three algorithms below: the clustering algorithm, a modified Gaussian elimination
algorithm used in the clustering algorithm to identify the clusters from the kernel of a graph
Laplacian, and the dimension reduction algorithm. In both the clustering and dimension reduction
algorithms,theinputisafinitecollectionofpointsS inametricspaces,andforeveryr <Diam(S),
weconstructthegraphG
r
usingtheEuclideandistancebetweenpoints,andweconstructL r,e−Lr,
and e−t∗Lr as in Section 2.1, where t∗ ≫ 0 is sufficiently large that all the eigenvalues of e−t∗Lr
are either close to 0 or close to 1. The target scale in both cases is chosen according to Equation
(2). In the clustering algorithm, we calculate the kernel of the Laplacian at the target scale, and
then use the modified Gaussian elimination algorithm (Algorithm 2) to identify the clusters. InNONCOMMUTATIVE MODEL SELECTION 7
the dimension reduction algorithm (Algorithm 3 below), once we have identified the target scale,
we use the first k eigenvectors, k < n of the Laplacian L to define a map Ψ : S → Rk by
rˆ
Ψ(x)=(ϕ (x),...,ϕ (x)). The dimension k of the dimension reduction is chosen by the user.
1 k
Algorithm 1 Clustering Algorithm
1: for r <Diam(S) do
2: ComputeG r,L r,exp(−L r)andestimatel´ım t→∞exp(−tL r)byexp(−t∗L r)forsomet∗large.
3: Compute the relative von Neumann entropy S r(ρ||σ) where ρ = tr(exp1 (−Lr))exp(−L r) and
σ = 1 exp(−1000L )
tr(exp(−1000Lr)) r
4: end for
5: Define rˆ:=argmaxS r.
6: Compute a basis for thee kernel of L rˆ, i.e. ϕ i for i∈1,...,k.
7: Using Algorithm 2 and the ϕ i, create the map Ψ : z m → Ψ(z m) =
(ψ (z ),ψ (z ),...,ψ (z ))∈Rk.
1 m 2 m k m
8: Compute the distances d i(z m):=||Ψ(z m)−e i|| for each point z m in the sample.
9: Assign the vertex m to the i-th cluster if d i(z m)<d j(z m) for all j ̸=i.
The modified Gaussian elimination algorithm (Algorithm 2 below) takes a matrix whose rows
span the kernel of a graph Laplacian - and so each row is constant on connected components of
the graph - and outputs a matrix whose entries are either (very close to) 1 or (very close to) 0,
and where each row is supported (up to a small error) on exactly one connected component of the
graph. The clusters are then identified as the support of each row. This algorithm was also used
in the clustering methods in [13]
Algorithm 2 Modified Gaussian elimination Ψ
1: for i=1 to k do
2: Reorder columns i through n of Ψ so that |Ψ (i,i)| is the maximum of |Ψ (i,j)| in row i.
3: Divide row i by Ψ (i,i)
4: Using elementary row operations, make Ψ (k,i) =0 for k ̸=i.
5: end for
6: Redefine ψ i := Ψ i,∗, and (abusing notation) using the new ψ i, redefine the map Ψ(z m) :=
(ψ (z ),ψ (z ),...,ψ (z )).
1 m 2 m k m
4. Experimental Results
4.1. Clustering. We now present the results of the numerical experiments we ran to test the data
clustering and dimension reduction algorithms, as well as comparisons to k-means clustering. We
tested the data clustering algorithm on both synthetic data and the unprocessed COIL-20 image
database [12], the latter of which consists of 72 images of each of five objects, where each object
is rotated five degrees around a vertical axis between one image and the next. The dimension
reduction algorithm was tested on a number of shapes in R3 and then reduced to shapes in R2 in
order to facilitate the visualization of the results.NONCOMMUTATIVE MODEL SELECTION 8
Algorithm 3 Dimension Reduction Algorithm
1: for r <Diam(S) do
2: ComputeG r,L r,exp(−L r)andestimatel´ım t→∞exp(−tL r)byexp(−t∗L r)forsomet∗large.
3: Compute the relative von Neumann entropy S r(ρ||σ) where ρ = tr(exp1 (−Lr))exp(−L r) and
σ = 1 exp(−1000L )
tr(exp(−1000Lr)) r
4: end for
5: Define rˆ:=argmaxS r.
6: Compute the eigenvalues of L rˆ and sort them in ascending order λ 0 =0<λ 1 <...<λ n <1.
7: Discard the 0 eigenvalue and take the corresponding eigenvectors Ψ 1,Ψ 2,...,Ψ n.
 
ϕ (x)
1
ϕ 2(x)
8: Letk <nbethetargetdimension,thentheembeddingmapfromX toRk isΨ(x)=  . .  .
 . 
ϕ (x)
k
4.2. Relative Entropy Clustering Results. Tables 1 and 3 summarize the results (the number
β of clusters) produced by the clustering algorithm on data sets of 500 and 1000 points sampled
0
from three interlinked circles embedded in R3 with a small amount of Gaussian noise (standard
deviation SD). Images of the samples, colored according to the results of the clustering algorithm,
are shown in Figures 1-8. The horizontal circle has radius 1 and center (0,0,0), and the other
haveradii0.5and0.4andcenters(0,−1,0)and(0,1,0), respectively. Fortheclusteringalgorithm,
the graphs G were obtained the Euclidean distance between points as the edge weights and the
r
calculation of the relative entropy used t∗ =1000. We ran five different trials, where the Gaussian
noise had standard deviations of 0.01, 0.02, 0.03, 0.04, and 0.05. For each standard deviation
value, we repeated the experiment 150 times. In each trial, the relative von Neumann entropy was
computed for 200 values of r.
Table 1. Relative von Neumann entropy Method, 500 sample points
SD|β 1 2 3 >4
0
0.01 0 6.667 93.334 0
0.02 0 64.667 35.334 0
0.03 2.667 92 5.334 0
0.04 31.334 68.667 0 0
0.05 77.334 22.667 0 0
Table 2. Eachrowinthetablecontainstheresultsofthetrialforpointssamples
at the corresponding noise level (SD in the table). The number in each cell is the
percent of the experiments for that noise level whose output (β ) was the value at
0
the top of the column.
Comparing these results with those obtained in the model selection by average relative entropy
(the ARE method) in [13], we see that this method represents a significant improvement. For
example, in the case of 500 points and standard deviation of noise equal to 0.01, the ARE method
in [13] returns the correct number of clusters for 64% of the trials, compared with the 93.334% inNONCOMMUTATIVE MODEL SELECTION 9
Table 3. Relative von Neumann entropy Method, 1000 sample points
SD|β 1 2 3 >4
0
0.01 0 0 100 0
0.02 0 0 100 0
0.03 0 1.334 98.667 0
0.04 0 51.334 48.667 0
0.05 0 96.667 3.334 0
Table 4. Eachrowinthetablecontainstheresultsofthetrialforpointssamples
at the corresponding noise level (SD in the table). The number in each cell is the
percent of the experiments for that noise level whose output (β ) was the value at
0
the top of the column.
this method. In the case of 1000 points and standard deviation of noise equal to 0.01, the ARE
method in [13] returns the correct number of clusters for 62.67% of the trials, compared with the
100% in this method.
In Figures 1-8, we give several clustering results of the relative von Neumann entropy algorithm
with different amounts of noise (as the ones in the table) and its respective entropy vs scale graph
fora500samplepoints. Thefirstoftheseisanexamplewherethealgorithmcorrectlyclassifiesthe
clusters, and the second is where the algorithm fails. These were typical results for these trials, i.e.
whenthealgorithmreportedthecorrectnumberofclusters,theresultingclusteringwasalsocorrect,
andwhenitreportedanincorrectnumberofclusters,thealgorithmtypicallycombinedtwoormore
clustersintoone. Weseefromthefiguresthat, evenwhenourmethodreportsanincorrectnumber
ofclusters,thereportedclustersarewell-separatedfromtheothers. Theresultingclassificationwill
still likely useful to an end user in such cases, and typically reflects the existence of a genuine gap
between the clusters. Higher amounts of noise reduce the performance of the algorithm.
4.3. Comparison with other methods. We also compared our clustering algorithm with the
k-means algorithm. For a 1000 points sample in the three interlinked circles we ran the three algo-
rithms, in different amount of Gaussian noise. In addition to the sample of points, for the k-means
algorithm we took as input the correct number of clusters k =3. The results of these experiments
are shown in the Figures 9-14. We see from the figures that the k-means algorithms incorrectly
identified the clusters. Nonetheless, we consider this unsurprising, as these two algorithms are
known to perform poorly on data in which the clusters have interesting geometry and which are
not concentrated at a point.
4.4. Test on Image Data. In order to test our algorithm on image data, we used the unpro-
cessed images from Columbia University Image Library (COIL-20) database. These images form a
collection of 448×416-pixel gray scale images from 5 objects, each of which is photographed at 72
different rotation angles [12].
Regarding each image as a vector of pixel intensities in R448×416 yields a set X ⊂ R448×416
with 360 points; this set becomes a finite metric space when endowed with the ambient Euclidean
distance. The result of applying our clustering algorithm to this set is the correct classification
of all images in five clusters. We also calculated the results of applying the k-means and the k-
NN algorithms to the same set. In the first case, we gave the data k = 5 as input and got anNONCOMMUTATIVE MODEL SELECTION 10
Figure 1. A typical example showing the correct classification of clusters. Noise
SD=0.01. Left: Output of the algorithm. Right: Graph of relative von Neumann
entropy (y-axis) vs. scale (x-axis).
Figure 2. Incorrect classification of clusters. Noise SD=0.01. Left: Output of
the algorithm. Right: Graph of Entropy (y-axis) vs. Scale (x-axis).
incorrect classification of 13 images. In the second case, we also took k =5, obtaining an incorrect
classification of 47 images. The results of each of these experiments are summarized in Figures
15-16.
4.5. Dimension Reduction. In order to test our dimension reduction algorithm, we tried the
algorithm on several figures in three dimensions and reduced them to two dimensions. The results
are found in Figures 17-20, where we see that the local geometry of the circular figures was largely
preserved,andforthetwo-dimensionalsurfaces,pointswhichwerecloseinthree-dimensionsmostly
stayed close in two-dimensions.NONCOMMUTATIVE MODEL SELECTION 11
Figure 3. Correct classification of clusters. Noise SD=0.02. Left: Output of the
algorithm. Right: Graph of Entropy (y-axis) vs. Scale.
Figure 4. Incorrect classification of clusters. Noise SD=0.02. Left: Output of
the algorithm. Right: Graph of Entropy (y-axis) vs. Scale (x-axis).
5. Discussion and future work
In this article, we have presented new clustering and dimension reduction algorithms for a data
set S sampled from a uniform distribution on a metric measure space X, possibly corrupted by
Gaussian noise, where X has been embedded in a larger metric space Z, and such that the metrics
on X and Z are similar at small enough scales. The algorithms work by constructing a family of
graphs indexed by the positive real numbers r > 0 which, roughly speaking, indicate the size of
a local neighborhood around each point in the sample. We then identify an optimal scale rˆ > 0
by maximizing the relative von Neumann entropy of specially constructed heat operators based
on the graphs. For clustering, we identify the clusters as the components of the associated graphNONCOMMUTATIVE MODEL SELECTION 12
Figure 5. Correct classification of clusters. Noise SD=0.03. Left: Output of the
algorithm. Right: Graph of Entropy (y-axis) vs. Scale (x-axis).
Figure 6. Incorrect classification of clusters. Noise SD=0.03. Left: Output of
the algorithm. Right: Graph of Entropy (y-axis) vs. Scale (x-axis).
best approximate the same connected components as the space X, and for dimension reduction,
we use the eigenvectors of the Laplacian L to construct a map Ψ:S →Rk. We have shown that
rˆ
the clustering algorithm represents a significant improvement over the Average Relative Entropy
Method of [13], in addition to outperforming k-means clustering on the examples which we have
shown here. A particularly interesting aspect of our construction is that the weights in our graphs
are simply chosen to be the pairwise distance in the ambient space, in contrast to other spectral
methods such as Laplacian Eigenmaps [3] and Diffusion Maps [7], where an ambient heat kernel is
used.
There are a number of benefits to considering relative von Neumann entropy instead of an
average of classical relative entropy as in [13], or even the semigroup-based heuristic of [15]. InNONCOMMUTATIVE MODEL SELECTION 13
Figure 7. Incorrect classification of clusters. Noise SD=0.04. Left: Output of
the algorithm. Right: Graph of Entropy (y-axis) vs. Scale (x-axis).
Figure 8. Incorrect classification of clusters. Noise SD=0.05. Left: Output of
the algorithm. Right: Graph of Entropy (y-axis) vs. Scale (x-axis).
particular, von Neumann entropy is a natural noncommutative construction on the (normalized)
heatoperators, andwebelievethatthiswillmakeitsrigoroustheoreticaltreatmentmoretractable
than the methods in either of [13] or [15], in addition to providing motivation and a setting for
studying more noncommutative techniques in statistics.
Wenotethreeissueswiththismethodwhichwehopetoaddressinfuturework. First,thesuccess
of the clustering algorithm presented here depends strongly on the assumption that the sampling
distribution is well-spread-out on its support - in this case, we used a uniform distribution - which
unfortunately is not fulfilled in many interesting real-world examples. Extending this method to
non-uniformdistributions,inadditiontodealingwithawiderrangeofnoisemodels,isanimportant
avenue for future research. We also note that the method proposed here is, from a certain point ofNONCOMMUTATIVE MODEL SELECTION 14
Figure 9. Comparison. No added noise. K-means: 244 mistakes
Figure 10. Comparison, SD=0.01. K-means: 233 mistakes
Figure 11. Comparison, SD=0.02. K-means: 255 mistakes
view, a refinement of single-linkage clustering (see [10], Section 4.2), and as such, it shares many
of its shortcomings, in particular that it will produce ‘chaining’ artifacts that may occur in single-
linkage clustering. Nonetheless, we also expect that the solutions which have been found for these
issuesinthecaseofsingle-linkageclusteringwillalsoworkherewiththeappropriatemodifications.NONCOMMUTATIVE MODEL SELECTION 15
Figure 12. Comparison, SD=0.03. K-means: 237 mistakes
Figure 13. Comparison, SD=0.04. K-means: 247 mistakes
Figure 14. Comparison, SD=0.05. K-means: 248 mistakes
Afurtherissuewiththemethodwhichiscurrentlyunresolvedisthatcomputingtheeigenvalues
of large, dense matrices is computationally expensive. However, since the maxima of the relative
von Neumann entropy appear to occur for relatively small values of the scale parameter r >0, we
are optimistic that future investigation will eliminate the need to consider matrices which are not
sparse. Indeed, one may simply impose sparseness of the graphs as a constraint of the algorithm,NONCOMMUTATIVE MODEL SELECTION 16
Figure 15. Performance of our algorithm for the COIL-20 unprocessed image
data. Left: the Entropy graph, right: the Confusion Matrix.
Figure 16. Confusion Matrix for the k-means algorithm applied to the COIL-20
unprocessed image data.
and then explore the effectiveness of this modification, but we believe that theoretical results that
justify restricting consideration to sparse graphs may also be possible to find. Given the results
we observed in our current experiments, however, we do not expect that restricting the algorithm
to only consider sparse graphs would significantly affect the accuracy of the algorithms, but we do
expect that it would significantly improve their speed.
Finally, the empirical success of the techniques introduced here introduces many interesting,
difficult questions of how to establish performance guarantees for these methods.
Conflict of Interest Statement
The authors certify that they have no conflict of interest in the subject matter or materials
discussed in this article.REFERENCES 17
Figure 17. The corona and its two-dimensional reduction.
Figure 18. A trefoil knot and its two-dimensional reduction
References
[1] MuhammadSirajoAbdullahi,PoomKumam,andP.ChristopherStaecker.“DigitalLefschetz
numbers and related fixed point theorems”. In: Rev. R. Acad. Cienc. Exactas F´ıs. Nat. Ser.
AMat.RACSAM 116.4(2022),PaperNo.173,23.issn:1578-7303,1579-1505.doi:10.1007/
s13398-022-01318-1. url: https://doi-org.biblio.cimat.remotexs.co/10.1007/
s13398-022-01318-1.
[2] Ralph Abraham and Joel Robbin. Transversal Mappings and Flows. W.A. Benjamin, Inc.,
1967.REFERENCES 18
Figure 19. A torus knot and its two-dimensional reduction
Figure 20. The Swiss roll.
[3] Mikhail Belkin and Partha Niyogi. “Laplacian Eigenmaps for Dimensionality Reduction and
Data Representation”. In: Neural Comput. 15.6 (June 2003), pp. 1373–1396. issn: 0899-7667.
doi:10.1162/089976603321780317.url:http://dx.doi.org/10.1162/089976603321780317.
[4] P.B´erard,G.Besson,andS.Gallot.“EmbeddingRiemannianmanifoldsbytheirheatkernel”.
In:Geom. Funct. Anal.4.4(1994),pp.373–398.issn:1016-443X.doi:10.1007/BF01896401.
url: https://doi.org/10.1007/BF01896401.REFERENCES 19
[5] Fr´ed´eric Chazal et al. “Persistence-Based Clustering in Riemannian Manifolds”. In: J. ACM
60.6(Nov.2013).issn:0004-5411.doi:10.1145/2535927.url:https://doi.org/10.1145/
2535927.
[6] Fan R. K. Chung. Spectral graph theory. Vol. 92. CBMS Regional Conference Series in Math-
ematics.PublishedfortheConferenceBoardoftheMathematicalSciences,Washington,DC;
bytheAmericanMathematicalSociety,Providence,RI,1997,pp.xii+207.isbn:0-8218-0315-
8.
[7] RonaldR.CoifmanandSt´ephaneLafon.“Diffusionmaps”.In:Appl.Comput.Harmon.Anal.
21.1 (2006), pp. 5–30. issn: 1063-5203. doi: 10.1016/j.acha.2006.04.006. url: http:
//dx.doi.org/10.1016/j.acha.2006.04.006.
[8] John B. Conway. A course in functional analysis. Second. Vol. 96. Graduate Texts in Math-
ematics. Springer-Verlag, New York, 1990, pp. xvi+399. isbn: 0-387-97245-5.
[9] DavidL.DonohoandCarrieGrimes.“Hessianeigenmaps:locallylinearembeddingtechniques
forhigh-dimensionaldata”.In:Proc.Natl.Acad.Sci.USA100.10(2003),pp.5591–5596.issn:
0027-8424,1091-6490. doi: 10.1073/pnas.1031596100. url: https://doi.org/10.1073/
pnas.1031596100.
[10] BrianS.Everittetal.Cluster analysis.Fifth.WileySeriesinProbabilityandStatistics.John
Wiley & Sons, Ltd., Chichester, 2011, pp. xvi+330. isbn: 978-0-470-74991-3. doi: 10.1002/
9780470977811. url: https://doi.org/10.1002/9780470977811.
[11] TrevorHastie,RobertTibshirani,andJeromeFriedman.TheElementsofStatisticalLearning.
Second. Springer Series in Statistics. Data mining, inference, and prediction. Springer, New
York, 2009, pp. xxii+745. isbn: 978-0-387-84857-0. doi: 10.1007/978-0-387-84858-7. url:
https://doi.org/10.1007/978-0-387-84858-7.
[12] S.A. Nene, S.K. Nayar, and H. Murase. Columbia Object Image Library (COIL-20),”. Tech.
rep. Columbia University, 1996.
[13] Antonio Rieser. “A topological approach to spectral clustering”. In: Foundations of Data
Science 3.1 (2021), pp. 49–66.
[14] SamT.RoweisandLawrenceK.Saul.“NonlinearDimensionalityReductionbyLocallyLinear
Embedding”.In:Science 290.5500(2000),pp.2323–2326.doi:10.1126/science.290.5500.
2323.eprint:https://www.science.org/doi/pdf/10.1126/science.290.5500.2323.url:
https://www.science.org/doi/abs/10.1126/science.290.5500.2323.
[15] Shan Shan and Ingrid Daubechies. Diffusion Maps : Using the Semigroup Property for Pa-
rameter Tuning. 2022. arXiv: 2203.02867 [stat.ML]. url: https://arxiv.org/abs/2203.
02867.
[16] Wen-Jun Shen et al. “Introduction to the Peptide Binding Problem of Computational Im-
munology:NewResults”.English.In:FoundationsofComputationalMathematics 14.5(2014),
pp. 951–984. issn: 1615-3375. doi: 10.1007/s10208-013-9173-9. url: http://dx.doi.
org/10.1007/s10208-013-9173-9.
[17] A. Singer and H.-T. Wu. “Vector diffusion maps and the connection Laplacian”. In: Comm.
Pure Appl. Math. 65.8 (2012), pp. 1067–1144. issn: 0010-3640. doi: 10.1002/cpa.21395.
url: http://dx.doi.org/10.1002/cpa.21395.
[18] JoshuaB.Tenenbaum,VindeSilva,andJohnC.Langford.“AGlobalGeometricFramework
for Nonlinear Dimensionality Reduction”. In: Science 290.5500 (2000), pp. 2319–2323. doi:
10.1126/science.290.5500.2319. eprint: https://www.science.org/doi/pdf/10.1126/
science.290.5500.2319. url: https://www.science.org/doi/abs/10.1126/science.
290.5500.2319.REFERENCES 20
[19] Mark M. Wilde. Quantum information theory. Second. Cambridge University Press, Cam-
bridge, 2017, pp. xvii+757. isbn: 978-1-107-17616-4. doi: 10.1017/9781316809976. url:
https://doi.org/10.1017/9781316809976.
[20] Zhen-yue Zhang and Hong-yuan Zha. “Principal manifolds and nonlinear dimensionality re-
duction via tangent space alignment”. In: Journal of Shanghai University (English Edition)
8.4 (2004), pp. 406–424. doi: 10.1007/s11741-004-0051-1. url: https://doi.org/10.
1007/s11741-004-0051-1.
Centro de Investigacio´n en Matema´ticas, A.C., Calle Jalisco S/N, Colonia Valenciana, Guanajuato
C.P. 36023, Guanajuato, Me´xico
Email address: araceli.guzman@cimat.mx,antonio.rieser@cimat.mx