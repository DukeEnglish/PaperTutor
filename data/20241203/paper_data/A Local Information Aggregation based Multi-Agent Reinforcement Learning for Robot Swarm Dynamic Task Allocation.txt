IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 1
A Local Information Aggregation based
Multi-Agent Reinforcement Learning for Robot
Swarm Dynamic Task Allocation
Yang Lv, Jinlong Lei and Peng Yi
Abstract—In this paper, we explore how to optimize task these swarms for large-scale, complex tasks presents consid-
allocation for robot swarms in dynamic environments, empha- erable challenges. A primary hurdle is the task allocation
sizing the necessity of formulating robust, flexible, and scalable
problem,whichinvolvesintelligentlydistributingtasksamong
strategiesforrobotcooperation.Weintroduceanovelframework
therobotstooptimizeperformance[5].Thisissueiscrucialin
using a decentralized partially observable Markov decision pro-
cess (Dec POMDP), specifically designed for distributed robot robotics and has broader implications for fields like industrial
swarm networks. At the core of our methodology is the Local automation [6], emergency rescue [7], and environmental
Information Aggregation Multi-Agent Deep Deterministic Policy monitoring[8].Thus,thestudyoftaskallocationinlarge-scale
Gradient (LIA MADDPG) algorithm, which merges centralized
robotswarmshasbecomeafocalpointforbothacademicand
training with distributed execution (CTDE). During the central-
industrial communities. izedtrainingphase,alocalinformationaggregation(LIA)module
ismeticulouslydesignedtogathercriticaldatafromneighboring Task allocation in dynamic environments poses significant
robots, enhancing decision-making efficiency. In the distributed challenges in robotics. Existing research primarily focuses on
execution phase, a strategy improvement method is proposed to scenarios involving unexpected events, such as the sudden
dynamically adjust task allocation based on changing and par-
additionorremovalofrobotsortasks.Thesechangestypically
tiallyobservableenvironmentalconditions.Ourempiricalevalua-
occur in contexts where they are manageable and infrequent,
tionsshowthattheLIAmodulecanbeseamlesslyintegratedinto
various CTDE-based MARL methods, significantly enhancing which allows for only periodic adjustments [9], [10]. How-
their performance. Additionally, by comparing LIA MADDPG ever, some real-world situations may demand a wider variety
with six conventional reinforcement learning algorithms and a of tasks and more frequent changes, necessitating continual
heuristicalgorithm,wedemonstrateitssuperiorscalability,rapid
adaptation [11]. Tasks may range from simple data collection,
adaptation to environmental changes, and ability to maintain
requiring only linear movement, to complex environmental
both stability and convergence speed. These results underscore
LIA MADDPG’s outstanding performance and its potential to monitoring that necessitates intricate pathways and variable
significantly improve dynamic task allocation in robot swarms speedsforeffectivecoverage.Theinherentvariabilityofthese
through enhanced local collaboration and adaptive strategy tasks creates a strongly dynamic environment that requires
execution.
consistent reallocation efforts by the robots. Furthermore,
Index Terms—robot swarm, multi-robot systems, networked as the size of the swarms increases, the complexity of the
robots, dynamic task allocation. task allocation process also escalates due to the expanded
search space [12], posing challenges for timely responses.
This study aims to address these challenges by proposing
I. INTRODUCTION
newcommunicationprotocolsandcoordinationmechanismsto
WITH the continuous advancement of modern tech-
effectively manage task allocation in large-scale dynamically
nology, robot swarms have emerged as a significant
changing environments.
research area, adept at handling complex tasks such as UAV
To address the challenges of task allocation, typical plan-
swarms[1],adhocnetworkrelay[2],andcooperativetracking
ning methods are categorized into centralized and distributed
control[3].Theseswarms,comprisingnumeroussmallrobots,
approaches. Centralized methods rely on a central planning
excel in cooperative collaboration, underscoring the potential
system that collects all task information and uses various
ofcollectiveintelligence[4].However,efficientlycoordinating
algorithms to devise task assignment strategies for each robot
[13], [14], [15]. However, the dynamic nature of real-world
Manuscript created November, 2023; The paper was sponsored by the
National Key Research and Development Program of China under No tasksmakesaone-time,globaltaskallocationimpractical[16].
2022YFA1004701,theNationalNaturalScience FoundationofChinaunder Consequently, researchers have shifted towards dynamic task
GrantNo.72271187andNo.62373283,andpartiallybyShanghaiMunicipal
allocation methods that involve periodic re-planning to adapt
ScienceandTechnologyMajorProjectNo.2021SHZDZX0100.
Yang Lv is with Shanghai Research Institute for Intelligent Au- tochangingconditionsandenvironmentaldynamics[17],[18].
tonomous Systems, Tongji University Shanghai 200092, China, (email: Nevertheless, the real-time execution of centralized planning
726564418@qq.com).
algorithms can be time-consuming and complex, especially
Jinlong Lei and Peng Yi are with the Department of Control Sci-
ence and Engineering, Tongii University, Shanghai, 201804, China; The with a large number of robots.
Shanghai Research Institute for Intelligent Autonomous Systems,Shanghai, Distributed task allocation methods generally offer higher
201804, China;Shanghai Institute of Intelligent Science and Technol-
computationalefficiency[19],[20],[21].Thesemethodsadapt
ogy, Tongji University 200092, China, (email: leijinlong@tongji.edu.cn;
pengyi@tongji.edu.cn). theirobjectivesbasedonthecommunicationdynamicsamong
4202
voN
92
]IA.sc[
1v62591.1142:viXraIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 2
agents. Prominent algorithms used include the auction algo- data from other agents. This information overload can hinder
rithm[22]andthecontractnetworkalgorithm[23],alongwith agents from focusing on data pertinent to their own decision-
theirvariousderivativealgorithms[24],[25],[26],whichhave making, thereby reducing the overall efficiency of decision-
found extensive application in robot team task assignments. making.
Although these algorithms are effective for the collabora- To address the challenges of task allocation in large-
tion of multiple robots, their efficiency decreases in large- scale robot swarms within strongly dynamic environments,
scale operations. To overcome this, a distributed autonomous our framework adopts a local information aggregation-based
decision framework based on game theory was introduced, Multi-Agent Reinforcement Learning (MARL) strategy. We
whichassumesastablecommunicationtopologyamongrobots treat each robot as an intelligent agent and transform the
andintegratessocialinhibitionmechanismstoensureefficient problem into a multi-robot cooperative system. By utilizing
convergence to a Nash equilibrium allocation within polyno- MARL, we develop an end-to-end solution that translates raw
mial time [27]. However, in a strongly dynamic multi-robot statedataintotaskallocationstrategieswithoutrelyingontra-
systemswherecommunicationtopologiescanchange,efficient ditional plan-based methods. The MARL agents continuously
robot communication and collaboration become crucial. For refine their strategies by engaging with the environment and
example, in emergency rescue scenarios [28], the status infor- interactingwithotherrobots.Thisdynamicinteractionenables
mation of robots and target tasks may evolve, necessitating real-time and efficient collaboration among robots, allowing
efficient real-time dynamic task allocation. However, existing for swift adaptation to changing environmental conditions.
distributed algorithms often struggle to manage these com- Ourmethodfocusesonaggregatingessentialinformationfrom
plexities effectively. a subset of locally relevant agents rather than all agents,
Recent studies have showcased that multi-agent reinforce- substantially reducing the dimensionality of the input space.
ment learning (MARL) methods such as MAAC [29], QMIX Building on this approach, we introduce a novel distributed
[30], and MAPPO [31] are potent tools for tackling dynamic MARL framework called LIA MADDPG, which emphasizes
task allocation challenges in complex scenarios. Originating distributed cooperation. This framework includes a Local
with[32],afoundationaltabular-basedmulti-agentQ-learning InformationAggregation(LIA)module,enablingeachrobotto
framework was developed to manage dynamic tasks in unpre- manage task-related information and actively engage in infor-
dictable environments. This approach was further advanced mation exchange and collaboration with neighboring robots.
by [33], [34] through the integration of deep neural networks, The main contributions of this paper are as follows:
enhancingthesystem’sadaptability.However,challengessuch • We investigate a new task allocation problem for robot
as environmental non-stationarity persisted. To resolve it, [35] swarms in dynamic task environments, and set up a
introduced a multi-agent actor-critic (MAAC) method, which distributed perception and communication model within
significantly expedited the convergence of task allocation pro- the robot swarm network. Subsequently, we reformulate
cesses in manufacturing systems via expert-guided strategies. theproblemasDec POMDP,thatenablesrobotstomake
TofurtheradvancetherobustnessofMARL,[36]introduceda informed and dynamic task allocation decisions under
parallel training mechanism employing MADDPG with asyn- conditions of limited information exchange from nearby
chronous updates to better manage uncertainties in dynamic robots.
environments, marking a pivotal advancement towards more • Wethenproposeanovelmulti-agentreinforcementlearn-
robust and scalable MARL applications. Building on this ing algorithm, called LIA MADDPG, that employs a
robust training foundation, [37] tailored an enhanced MAPPO combination of centralized training and distributed ex-
method specifically for dynamic multi-objective task alloca- ecution. During the centralized training phase, a local
tion in manufacturing settings, thereby broadening MARL’s information aggregation module is incorporated to en-
practical applicability. Concurrently, [38] tackled resource couragerobotstofocusmoreoninformationbeneficialto
contention—a frequent challenge in dynamic settings—by themselves during the training process. In the distributed
employingtheQMIXalgorithm,ensuringthatagentsnotonly execution phase, robots must adapt to constantly chang-
met individual objectives but also contributed to achieving ing conditions and make decisions based on incomplete
collective goals. information in dynamic environments. Thus, we have
Despite these advancements, previous studies have primar- developed a optimization method to improve the quality
ily focused on the dynamics of unexpected events without of allocation policy. The method involves the analysis of
fully addressing scenarios where task locations continuously observable information from each robot, allowing them
change. Furthermore, while CTDE-based frameworks have to enhance allocation strategies through self-exploration
utilizedadvancedglobalevaluationstrategiestoenhanceagent and mutual collaboration.
coordination and policy learning, they confront a significant • Finally,weconductednumericalexperimentstocompare
scalability challenge known as dimensionality explosion. This the proposed method with six reinforcement learning
issue becomes particularly severe in large-scale tasks like algorithmsandaheuristicalgorithm.Inaddition,wehave
robotswarmtaskallocation,wherethestateandactionspaces incorporated LIA module with two other MARL meth-
growexponentiallyasmoreagentsareadded.Thisexponential ods (e.g.,MAAC and MAPPO). The numerical results
growth significantly increases computational complexity and demonstrate the broad applicability of the LIA module
memory demands. Moreover, these frameworks require each and highlight the superiority of our method in terms of
agenttoprocessasubstantialamountofinformation,including convergencespeed,stability,andscalability,especiallyinIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 3
tasks involving a larger number of robots. and the current environment, robot i ∈ N can dynamically
The paper is organized as follows: Section II offers a choose target tasks, and subsequently, moves at a constant
comprehensive investigation of the robot swarm dynamic velocity vr towards the chosen task. Fig.1 depicts a example
i
task allocation problem, along with its reformulation as a inwhichsevenrobotsareinterconnected,formingadistributed
Dec POMDP model. Section III provides an in-depth de- perceptionandcommunicationnetwork.Itshouldbenotedthat
scription of the proposed method LIA MADDPG. Section the interaction between the robot and the task occurs through
IV presents simulation results and the associated discussions. dataexchange.Thisnetworkenablestherobotstoobserveand
Lastly, some concluding remarks are given in Section V. collaborate with neighbors by sharing observation data, task
status, and decision outcomes.
II. PROBLEMDESCRIPTIONS
In this section, we begin by delving into the details of
modellingtherobotswarmtaskallocationproblemindynamic
taskenvironments.Subsequently,wereformulatethisproblem
as a Dec POMDP.
A. Robot Swarm Task Allocation Problem
The robotic swarm task allocation problem presents a chal-
lenging scenario where a swarm of available robots needs
to be allocated to mobile tasks in dynamic environments.
The primary objective is to efficiently assign robots to tasks
according to some specific performance metric. This problem
comprises three essential components. The first part focuses
onthemobiletasksetandtheircorrespondingmotionmodels.
The second part addresses the robot swarm network and
provides a detailed description of the associated distributed
communication model. The last part designs an appropriate
performance metric and introduces the optimization model.
a) Movable Task Set: Define M={1,2,...,M} as the Fig. 1: Distributed perception and communication in Robot
setofmovabletaskswithinthetimeseriesT :={0,1,...,T}. Swarm Network
Letvmrepresentthespeedoftaskj ∈M,andθm,tdenotethe
j j
movement angle of task j at time t. Let Pm,t =(xm,t,ym,t) We define gt ∈ M as the target task of robot i ∈ N at
j j j i
represent the two-dimensional spatial coordinates of task j at time t∈T, and dt ∈R as the Euclidean distance between
i,gt
time t. The motion state of the tasks is determined by their robot i and its targei t task gt at the t-th decision moment.
i
speed and movement angle and can be updated using (1). Let d >0 denote the predefined association distance, and
bind
(cid:26) xm,t+vmτcos(θm,t) T i ∈T bethefinishtimeofroboti.Ifatsometimet,roboti’s
Pm,t+1 = j j j (1) distance from its target task satisfies dt ≤d , then robot
j ym,t+vmτsin(θm,t) i,gt bind
j j j i ∈ N becomes bound to this task dt i ∈ M, and T ≜ t is
where τ represents the decision time step.
set as the binding time. At time T
,i, tg hit
e robot is
coni
sidered
i
Each task in M requires a substantial allocation of robots
to have completed the task assignment and it cannot change
while maintaining a moderate demand, which means it has itstargettaskanymore.LetPr,t+1 =(cid:0) xr,t,yr,t(cid:1) representthe
i i i
twocharacteristics:(1)eachtasknecessitatestheallocationof ym,t−yr,t
multiple robots for execution, and (2) there is a limitation on coordinates of robot i ∈ N, and λt+1 = git i represent
the number of robots required per task. Therefore, we define
the direction vector of robot i ∈
Ni, .g it Thenxm
g eit
a,t c− hx rr
i
o,t
bot moves
h¯ as the maximum number of robots that task j ∈ M can
j towards the chosen target task and can update their motion
accommodate.
state according (2). It should be emphasized that once the
b) Robot Swarm Network: Define the robot swarm
robot is bound, its motion state will consistently synchronize
networkasG ={N,E},whereN ={1,2,...,N}represents
with its target task.
the node set of robots, and E ⊂ N × N represents the
perceptionrelationshipsamongrobots.Eachrobotcorresponds  xr,t+vrτcos(cid:16) λt+1(cid:17)
toanodeV inthenetwork.Foranedge(i,i′)∈E,itsignifies Pr,t+1 = i i (cid:16) i,g it (cid:17), (2)
that robot i can observe information from robot i′. Therefore, i yr,t+vrτsin λt+1
i i i,gt
we can denote the set of all neighboring robots of robot i i
as N ≜ {i′ ∈ N : (i,i′) ∈ E}. We assume that each c) Utility Function: For each robot i ∈ N, let u
i i
robot i ∈ N is a rational agent with limited perception and representtheutilityfunctionthatroboti∈N canobtainwhen
communication capabilities, which can instantly access the assigned to some task j ∈ M. We design it to comprise the
status of all tasks and information from neighboring robots task feedback rewards u1 and movement cost reward u2. In
i i
i′∈N . Based on the observed and received information the following, we will separately introduce them.
iIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 4
The first item u1 represents the reward that the robot 3). Let x (t) ∈ {0,1} be a binary decision variable
i i,j
receives upon completing a task assignment, and the assigned indicating whether robot i ∈ N chooses task j ∈ M at time
task is denoted as j = gTi for convenience. Define ht as the t, where x (t) = 1 signifies that task j ∈ M is chosen by
i j i,j
number of robots already bounded to task j ∈ M at time robot i ∈ N at time t. Each robot can only select one target
(cid:80)
t ∈ T, and r as the reward that robot i ∈ N can obtain task at each time t, hence x (t)=1,∀i∈N,∀t∈T.
i,j i,j
from task j ∈ M. In fact, only when the number of robots j∈M
Based on the above discussions, the robot swarm task
bounded to task j is less than h¯ (the maximum number of
j allocationproblemindynamictasksettingscanbeformulated
robots that task j ∈ M can accommodate), robot i ∈ N can
as the following optimization problem.
receive rewards from task j ∈ M. Therefore, the expression
for u1
i
is as follo uw
1
is =.
(cid:40) r 0i ,,j, i of thh eT j rwi i< seh¯ .j (3)
 m πa ix
T gi
t(cid:80)
∈ = =N
|u
gTi
T(
|
iπ ,=i)
∀m i t∈ ∈a Nx [TT i ,, Td ]t i ;,g iTi ≤d bind;
The second component is the cost incurred by robot i∈N (cid:88)i i i (5)
w
(cid:80)
deh nTtTi
=
ol
i
he
t1
e
ervm
deir
fo
aτ
o
sv
.
ri
e
jn ,g =fot go rw Tiea ,ar icd
th
ss ut rh
to
ie
lb io
tt ytar fig ue n∈t ctt ia
N
os nk.
cw
aI it nthi bs etd
h
de
e
en sao cst re
s ii
bd
g
ena des bdu elt2
i
oa
ws=
k
.
s.t.
xj∈ i,M
j(tx
)i
=j(t (cid:26))
1
0=
, ,j
j1
=
̸=,∀i
g
gi
it
t∈N,∀t∈T;
i

r
i,j
−(cid:88)Ti
v irτ, if hT ji <h¯
j
B M. arR ke of vor Dm eu cl ia st ii oo nn Pa rs oca essDecentralized Partially Observable
u =u1−u2 = t=1 (4)
i i i −(cid:88)Ti
v irτ, otherwise. r oo
nbIn
o lt
ist mhe
ia tr
ea
e
dfo rr oee
s
bm
p so
ee rnn vst ai io
b
tin
l
oee nd
sf
.oro
r
Wb so eet les tcw
ht
eia
n
nr gm
rt
et ha
fe
osk
i rr
ma
t
al al uo
rg
tc eea tt ti ho
ta
en skp psr ro obb bal le
s
em
e
md,
t=1
d) Optimization Problem Formulation: Ourobjectiveis as a Dec POMDP [39], which comprises the tuple H =
{S,A,P,Z,O,R}. Specifically, given the current state st ∈
to dynamically select target task for each robot based on the
S, each robot i∈N can obtain its local observations ot ∈O
states of neighboring robots and target tasks, with the aim of i
through the observation function Z. Subsequently, it takes
maximizingthecumulativeutilityofallrobots.Toachievethis,
actions at ∈ A according to a predetermined policy and
we formulate the aforementioned problem as a collaborative i
receives corresponding rewards rt ∈ R. The environment
optimization problem consisting of three key components: i
then transitions to the next state based on the state transition
decision variables, an objective function, and constraints.
function P. Next, we will provide a detailed explanation of
the Dec POMDP model.
a) State Space: The state information of robot i ∈ N
at time t is denoted as sr,t = {xr,t,yr,t,vr,λt,ct}. Here,
i i i i i i
ct ∈ {0,1} represents the working status of robot i, with
i
ct =0indicatingthattherobotisalreadyboundtoatask,and
i
ct = 1 indicating that the robot can actively choose a task.
i
The state information of task j ∈ M is defined as sm,t =
j
{xm,t,ym,t,vm,θm,t,hm,t}. These symbols are consistent
j j j j j
with the previous text. Thus, the state space S consists of two
components: the robots state space Sr = {sr,t,sr,t,...,sr,t}
1 2 N
Fig. 2: A simple diagram of decision variable based on the and the tasks state space Sm ={sm,t,sm,t,...,sm,t}.
1 2 M
case in Fig.1 b) ActionSpace: Theactionofrobotiinvolvesselecting
itsdesiredtargettaskgt ateachdecisionmoment.Weutilizea
i
Wedefinethedecisionvariableforroboti∈N asπ i,which M-dimensional one-hot encoded vector as robot i’s action at.
i
represents the target task selected by the robot throughout the If robot i selects task j =gt at time t, then the j-th element
i
time series T, i.e., π i ={g i1,g i2,...,g iT}. The utility function of action at
i
is set to 1, while all other elements are set to 0.
(4) is employed as the objective function to assess the value Additionally, if the control information of robot i ct =0, i.e.,
i
of the current decision variable π i. A simple diagram for the boundtothetargettask,thenat+1 =at.Thus,thejointaction
i i
caseinFig.1isshowninFig.2.Additionally,wemustadhere space of all robots is defined as A={at,at,...,at }.
1 2 N
to the following constraints: c) State Transition Function: P(S,A): S×A→S′ is
1). The length of the time series T is restricted to the employed to describe the state transition of the environment
maximum time required for the last robot to reach its target afterrobotsexecutejointactions.Similarly,thestatetransition
task, i.e., T =|T|=max i∈N T i, where dt i,gTi ≤d bind. function encompasses both the task state transition function
2). Once robot i∈N is bound to its targeit task at time T , Pm and the robot state transition function Pr .
i i i
it cannot switch to another target task, i.e., gt = gTi for any The task state transition function primarily involves updat-
i i
t∈[T ,T]. ing the task’s position according to (1). Furthermore, based
iIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 5
on the actions taken by the robots, it is necessary to update This is because when rewards are not observable, the policy
the number of robots assigned to the task j ∈ M, i.e., gradient becomes zero, making it impossible to improve the
ht+1 =ht+num , where num signifies the count of robots policy. To tackle this issue, one approach is to leverage prior
j j j j
assignedtotaskj attimet.Therefore,thetaskstatetransition knowledge of the problem and propose a non-sparse reward
function Pm for task j ∈M is as follows: function that can provide more informative feedback to guide
j
the learning process.
s jm,t+1 ={xm j ,t+v jmτcos(θ jm,t),y jm,t+v jmτsin(θ jm,t), Based on the optimization model in (5) and the limited
(6)
vm,θm,t+1,ht+1} demand constraints of tasks, we will design a reward function
j j j
consisting of three components as follows.
Therobotstatetransitionfunctionisalsoacomponentofthe
environmentalstatetransition.Sinceeachrobotisindependent, r ti =r ti,dis+r ti,step+r ti,final, (8)
the state transition function Pr for robot i depends solely on
i where the first two items are designed to be non-sparse, and
the its current state st and the action at. It can be categorized
i i the last term represents the ultimate reward obtained when a
into three scenarios.
robot reaches a target task.
1). ct i =0, i.e., the robot has been bound to the target task, In details, r ti,dis =−φ 2 is a constant negative reward given
and then synchronized their motion state with its target tasks.
to robot i∈N at each time step until reaching its target task.
2). ct = 1 →at i ct+1 = 1, i.e., the robot remains unbound to ri,step is designed based on the utility function u1 and aims
i i t i
any task after executing the action. Then the state transition at guiding robots to satisfy the limited demand constraints of
involves updating the robot’s position according to (2). tasks. It is defined as:
3). ct = 1 →at i ct+1 = 0, i.e., the robot becomes bound to ri,step =φ (cid:0)ℏ¯ −ℏt(cid:1) . (9)
i i t 3 j j
a target task after executing the action. In this case, besides
The above reward mechanism requires robots to consider the
updating the robot’s position, it is vital to update the robot’s
task’sdemandwhenchoosingatarget.Ifthenumberofrobots
control information ct as well.
i bound to task j ∈ M exceeds the task’s maximum demand
Hence, the transition function Pr for robot i ∈ N after
i at time t ∈ T, these additional robots will be penalized.
executing action at can be expressed as follows:
i Moreover, as the number of robots exceeding the maximum

{xm,t+1,ym,t+1,vm,θm,t+1,0},if ct =0, demand increases, the penalty strength will also increase to
 g it g it g it g it i
prevent excessive concentration of robots on a single target.
sr i,t+1 = {xr i,t+1,y ir,t+1,v ir,λt i+1,1},if ct i =1,ct i+1 =1, The final term ri,final represents the ultimate reward ob-
{xr,t+1,yr,t+1,vr,λt+1,0},if
ct =1,ct+1 =0. tained when a
robot
t reaches a target task and is defined by
i i i i i i
(7) (cid:40) φ r , if ht <h¯ ,
d) Observation Space: In the above-mentioned model, ri,final = 1 i,j j j (10)
each robot has limited perception and communication ca- t 0, otherwise.
pabilities that can obtain information with each task and
Additionally,itiscrucialtoaddressanexceptionalscenario
neighboring robots. We define Nt = {nat,nat,...,nat }
i 1 2 αi where robots frequently switch target tasks without making
as the set of neighboring robots of robot i ∈ N at time
progresstowardscompletion.Therefore,itisnecessarytotake
t ∈ T, where α represents the number of neighboring
i a balance between φ and φ to ensure that robots are still
1 2
robots, depending on the perception capacity of robot i. influenced by the penalty term ri,dis when frequent target
Therefore, each robot i ∈ N can only observe a portion t
switching occurs.
of the state, and its local observation information primarily
includes three parts. 1). Self-state information ot = sr,t.
Self i III. ANOVELMARLALGORITHMWITHLOCAL
2). The relative state information between robot i and all
INFORMATIONAGGREGATION
tasks ot = {∆xt ,∆yt ,∆vt ,∆θt ,ht,κ ,∀j ∈ M},
Task i,j i,j i,j i,j j i,j In this section, we systematically introduce our novel
where ∆xt , ∆yt , ∆vt , and ∆θt represent the horizontal
i,j i,j i,j i,j MARL algorithm with local information aggregation, includ-
and vertical coordinates, relative velocity and relative motion
ing the main components of the algorithm and the design
direction between robot i and task j, respectively, ht denotes
j inspiration behind them.
the number of robots already bound to task j ∈ M at time
t∈T,andκ representsthenormalizedweightofthereward
i,j
that robot can obtain from target task. 3) The relative state A. Key Modules and Mechanisms of LIA MADDPG
information between robot i ∈ N and its neighboring ones MADDPG is a classic multi-agent deep reinforcement
ot = {∆xt ,∆yt ,∆vt ,∆θt ,gt−1,∀i′ ∈ Nt}. learning algorithm to address multi-agent problems in mixed
Neighbor i,i′ i,i′ i,i′ i,i′ i′ i
Therefore, the local observation value for robot i∈N can be cooperative-competitive environments [40]. However, when
defined as oi ={ot ,ot ,ot }. appliedtolarge-scaleproblemsliketherobotswarmtaskallo-
t Self Task Neighbor
e) Reward Function: Rewards play a crucial role in cationproblemconsideredinthiswork,MADDPGencounters
reflectingtheenvironment’sresponsetochangesintheagents’ difficulties in coordinating learning due to scalability issues.
statescausedbytheiractions.However,inthecaseofdynamic To overcome this challenge, we introduce a novel distributed
task allocation problems with sparse rewards, designing ef- method called LIA MADDPG, which incorporates the Local
fective learning strategies becomes complex and challenging. Information Aggregation (LIA) module.IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 6
In a typical MADDPG setup with N agents, it is necessary task), their interactions due to spatial proximity can influence
for all robots to maintain a set of policy networks {µ }N each other’s decisions. Similarly, robots that choose the same
i i=1
and their corresponding target policy networks {µ′}N , as taskwillalsoaffecteachotherbecauseofthelimitedcapacity
i i=1
wellas asetofvalue networks{Q }N andtheir targetvalue of each task.
i i=1
networks{Q′}N .However,forthelarge-scalehomogeneous To accurately capture this phenomenon, we define locally
i i=1
robots in the robot swarm task allocation problem, a policy- related robots(Gt) notonly through spatialproximity but also
i
sharing approach can be employed during training to simplify through action synchronization. Specifically, Gt = {Nt,Lt},
i i i
the network structure. In this approach, all robots share a where Nt = {nat,nat,...,na } includes robots that are
i 1 2 αt
common policy network µ¯ with parameters θa and a shared near to robot i, and Lt ≜ {i′ ∈i N : at = at } includes
i i i′
value network Q¯ with parameters θq. Additionally, the shared robots executing the same action as robot i. The influence
policy network µ¯ also has a corresponding target policy of these robots on i’s decision-making is significant because
network µ¯′ with parameters θ′a, and the shared target value nearby robots can create local environmental conditions that
networkQ¯′ withparametersθ′q.Thispolicy-sharingapproach directly affect robot i’s operational context. Simultaneously,
reducesthenumberofnetworksrequired,resultinginasimpler robots performing the same actions may lead to competition
algorithm with lower computational complexity. or cooperation for resources, further impacting i’s decisions.
a) SharedPolicyNetwork: Thesharedpolicynetworkµ¯ This dual consideration integrates collective behavior more
utilizes fully connected layers to handle partial observability effectively into the training process, enabling our model to
and sequential information from the environment. It incorpo- accurately predict and adapt to group dynamics, thereby en-
ratesresidualconnections[41]andbatchnormalization[42]to hancingeachrobot’sdecision-makingprocess.Byrecognizing
enhancetheperformanceandtrainingefficiencyofdeepneural how the same actions performed by nearby robots amplify
networks. For each robot i ∈ N, µ¯ takes the current time- environmentalimpacts,ourmodelnotonlycapturesdynamics
step observations ot as input and generates the corresponding more precisely but also reduces computational load by focus-
i
action at for the next time-step. ing on a manageable subset of influential robots, simplifying
i
the training process. Thus, the local information needed for
at =µ¯(ot;θa). (11)
i i training can be defined as follows:
Similarly, the action output based on the target network can Local Information Set: Define the local information for
be defined as: robot i as the collection of observations and actions from its
at i+1 =µ¯′(ot i+1;θ′a). (12) locally related robots, represented as L i = (ot k,at k)|k ∈G it,
where ot and at represent the observation vector and the
k k
b) Local Information Aggregation (LIA): The value action vector of robot k at time t.
network of the traditional MADDPG algorithm takes the It’s evident that the dimensionality of robot i’s local in-
joint observations ot = (ot 1,ot 2,...,ot N) and joint actions formation set vector is related to the number of its locally
at = (at 1,at 2,...,at N) from all robots to compute the Q-value related robots. However, the set of locally related robots
Q¯(ot,at,θq). While this approach mitigates the impact of varies among different robots, and even for the same robot,
non-stationary environments by considering all agents’ states, this set may change over time. In such cases, concatenating
meanwhile it faces exponential growth in input dimensions robot i’s state information with its local information for input
as the number of agents increases, making state evaluation into the value network results in inconsistent input vector
difficult. In our problem, a robot i only needs to consider dimensions. To address this issue, inspired by the mean field
the states of certain robots, referred to as “locally related RL approach from [43], we developed a local information
robots.” Therefore, during centralized training, we focus only aggregationfunction,denotedasφ .Thefunctionφ takesthe
i i
oninformationfromtheselocallyrelatedrobots.Thisselective localobservationsandactionsoftherobotsinGt asinputsand
i
focus helps mitigate the challenges of dimensionality but generates a fixed-dimensional aggregated information vector
introduces another issue: the number of locally related robots suitable for input into the value network. The LIA function
isuncertain,leadingtovariableinputdimensionsforthevalue φ is expressed as follows:
i
network. To address these challenges, we propose a Local
(cid:16) (cid:17) (cid:88)
Information Aggregation (LIA) module, which aims to solve φ i ot k = w it ,kot k, (13)
twokeyproblems:(1)definingandselectingthesetoflocally k∈Gt
i
related robots, and (2) handling the variability in the number
of these related robots, which leads to issues with the input φ i(cid:0) at k(cid:1) = (cid:88) w it ,kat k, (14)
dimensions of the value network. Next, we will introduce the k∈Gt
i
detailed definition of locally related robots.
where wt is distance-dependent weight coefficients defined
Inourstudy,weassumethatrobotsperformingthesameac- i,k
basedonthedistancedt betweenroboti∈N anditslocally
tionincloseproximity(referredtoas“locallyrelatedrobots”) i,k
relevant robot k ∈Gt. These weight coefficients are assigned
collectively influence their environment. This assumption is i
to each locally relevant robot k ∈Gt .
crucial for understanding group dynamics. The synchronized i
(cid:16) (cid:17)
efforts of these robots significantly amplify their impact. For exp βln(dt )
i,k
example, when multiple robots gather at a specific location wt = . (15)
or perform the same action (such as selecting the same target i,k (cid:80) exp(cid:16) βln(dt )(cid:17)
k∈Gt i,k
iIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 7
Remark: The weighting method defined in (15) and the process,changesinotheragents’strategiescanintroducenon-
attention mechanism [44] represent two distinct methods of stationarity in the state transition probability p(ssst+1 |ssst,at).
i
aggregatingagentinformationinmulti-agentsystems.Eq.(15) Consequently, relying solely on a temporal-difference (TD)
calculates weight coefficients wt based on the physical based approach for iteratively learning the Q function may
i,k
distance dt between robots, using an exponential function result in suboptimal learning outcomes.
i,k
scaled by a factor β. This deterministic approach prioritizes In our study, this problem can be effectively addressed by
agents based on proximity, ideal for applications such as introducing the extended Q-function G, which evaluates the
robotic swarms where spatial relationships are crucial. In expected total reward that robot i ∈ N can obtain based on
contrast, attention mechanisms in neural networks learn to its own local observation o , action a and the aggregated
i i
assign weights adaptively from data, allowing the model to informationfromlocallyconnectedrobots.Wethenderivethe
dynamically assess the relevance of each agent based on the Bellman optimality equation for the extended Q-function of
task and contextual interactions. While (15) provides a fixed, robot i’s policy.
proximity-basedweightingidealforenvironmentswhereagent
G∗(ot,at,φ (ot),φ (at))=
distance is a dominant factor, attention mechanisms offer i i i k i k
(cid:88)
greater flexibility by adapting to complex data relationships p(ooot+1 |ooot,at)[rt+1+
i i
(18)
beyond physical proximity. This adaptability makes them ooot+1
suitable for a wider range of applications where contextual γmaxG(ot+1,at+1,φt+1(o ),φt+1(a ))].
i i i k i k
nuances of agent interactions are critical. at+1
i
By aggregating the local information from locally related
The state transition p(ooot+1 |ooot,at) can be decomposed as
robots,φ enablesrobotitoeffectivelyconsiderthecollective i
i
(cid:88)
knowledge and actions of its nearby peers. This aggregation p(ooot+1 |ooot,at)= p(at |ot )p(ooot+1 |ot,at,at ), (19)
i −i −i i i −i
function not only ensures consistency in the input dimensions
at
−i
for the value network but also captures the intricate dynamics
oftheenvironmentmoreprecisely.Byintegratingobservations where p(at −i | ot −i) = (cid:81) j̸=ip(at j | ot j). For robot i ∈ N, the
and actions from locally related robots, φ allows robot i to source of environmental instability arises from the strategy
make more informed decisions, reflecting bi oth the immediate distributions p(cid:0) at −i|ot −i(cid:1) of other robots related to i. In this
environmental conditions and the synchronized actions of article, these relevant robots can be approximated as the local
its peers. Consequently, this approach enhances the model’s relevant robot set G it of robot i. Therefore, the state transition
ability to predict and adapt to group dynamics, ultimately in (19) can be redefined as
improvingtheoveralldecision-makingprocessandoperational p(ooot+1 |ooot,at)=
i i
efficiency of the robot.
(cid:88)
c) SharedValueNetwork: Weintroduceanovelcentral- p(at k |ot k)p(ooot+1 |ot i,at i,at k) k∈Gt,k̸=i, (20)
i
ized value function, denoted as G. The corresponding shared at
k
valuenetworkG¯,takesitsownlocalobservationot andaction
i where at and ot represent the local agent actions and ob-
at, as well as the aggregated information φ (ot) and φ (at) k k
i i k i k servations incorporated into the extended Q-function through
as input. G¯ then computes the evaluation value qt of robot i
i the aggregation function φ . When we know the actions
at time t. i
taken by the relevant robots of robot i, even if the policies
qt =G¯[ot,at,φ (ot),φ (at);θg],k ∈Gt,k ̸=i. (16) change, the environment can be considered static. This is
i i i i k i k i
because for any policies π ̸= π′,π ̸= π′, we have
sL ti rk ue cw tui rs ee .,thetargetsharedvaluenetworkG¯′ followsthesame p(cid:0) ooot+1 |ooot,at i,at k,π i,π k(cid:1) k∈Gi
t·k̸=i
=i p(cid:16)k ot+1 (cid:12) (cid:12) ok t,at i,at k(cid:17) =
p(cid:0) ot+1 |ot,at,at,π′,π′(cid:1) i . As a result, we can learn
q i′(t+1) =G¯′[ot i+1,at i+1,φ i(ot k+1),φ i(at k+1);θ′g],k ∈G i,k ̸=i.
the extended
Qi -fk uncti ionk Gk∈ bG ait s, ek d̸=i
on the above equation in a
(17) stable environment.
d) Network Updating: In multi-agent reinforcement Therefore, the shared value network G¯ can be trained to
learning,acriticalchallengeduringthenetworkupdateprocess approximate extended Q-function-function G by minimizing
is the environmental non-stationarity arising from the policy
the squared TD error. The loss function is given as follows:
changes of other agents during training. Take Ind DDPG [45]
as an example. It learns independently for each agent by
(cid:26)L(θg)=E[(y−q it)2],
(21)
treating other agents as components of the environment and y =rt+γq′(t+1),
i i
evaluates the Q function according to the following.
where rt is the reward obtained by the robot i at time t, and
i
Q∗(ssst,at)=
γ is the discount factor.
i
(cid:88) p(ssst+1 |ssst,at)[rt+1+γmaxQ∗(ssst+1,at+1)], The shared policy network µ¯ of robot i ∈ N is updated
i i i i
at+1 through gradient ascent, aiming to maximize the agent’s ex-
ssst+1 i
pected return. The gradient update is performed according to
where p(ssst+1 | ssst,at) represents the probability of agent i
i the following equation:
transitioning from state ssst to the next state ssst+1 when taking
action at. However, it is worth noting that during the learning ∇ J(θa)=E[∇ µ¯(a |o )∇ qt]. (22)
i θa θa i i ai iIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 8
It is crucial to note that, due to the utilization of a shared Algorithm 1: Training Process of LIA MADDPG
network approach, in the update process, each robot is re- Input : Max episode length T , batch size δ,
e
quired to update the network parameters sequentially based discount factor γ, soft target update rate η
on the sampled data. The subsequent section will present a Output: Trained actor network µ¯∗
comprehensive explanation of the specific update process. 1 Initialize: Expand Q network G¯ and policy network µ¯
with random weights θq, θa, target networks G¯′ and
µ¯′ with weights θ′g ←θg,θ′a ←θa, and replay buffer
B. Overview of the LIA MADDPG Framework
D
The LIA MADDPG framework consists of two distinct
2 while Training is not terminated do
phases: the off-line centralized training phase and the on-line
3 Initialize a random process H for action
distributed execution phase. To enhance the learning process,
exploration;
the framework employs four types of neural networks: an
4 Receive initial state o=(o 1,o 2,...,o N);
actornetworkforgeneratingtheactionpolicy;acriticnetwork
5 for t = 1 to T e do
for evaluating the action policy; a target actor network; and 6 for each robot i, do a i =µ¯(o i;θa)+H t;
a target critic network for stabilizing the learning process.
7 Execute actions a=(a 1,a 2,...,a N) and
Additionally, a Local Information Aggregation (LIA) module observe reward r and new state o′;
is integrated to accelerate the learning process. The overall
8 Obtain the locally related robot set
structureoftheLIA MADDPGalgorithmisillustratedinFig.
G=(G ,G ,...,G ) of each robot;
1 2 N
3, and the pseudocode is summarized in Algorithm 1. In the
9 Calculate ϕ(o)=
following sections, we will provide a detailed explanation of (cid:12) (cid:12) (cid:12)
(φ (o )(cid:12) ,φ (o )(cid:12) ,...,φ (o )(cid:12) )
these two phases. 1 k (cid:12) 2 j (cid:12) N k (cid:12)
k∈G1 k∈G2 k∈GN
a) Off-line Centralized Training Phase: The off-line according to (13);
centralizedtrainingprocesscanbedividedintothegeneration 10 Calculate ϕ(a)=
(cid:12) (cid:12) (cid:12)
of empirical data and network training, which are executed (φ (a )(cid:12) ,φ (a )(cid:12) ,...,φ (a )(cid:12) )
1 k (cid:12) 2 k (cid:12) N k (cid:12)
alternately. k∈G1 k∈G2 k∈GN
according to (14);
Empirical Data Generation: As depicted in the leftmost
11 Store (o,a,ϕ(o),ϕ(a),r,o′) in replay buffer
part of Fig. 3, the empirical data generation phase involves
D;
each robot continuously interacting with the environment to
12
o←o′
collect relevant experiential data. During this process, each
13 for I = 1 to N do
robot determines its actions a based on its current local
i 14 Sample a random minibatch of δ samples
observation o using a shared policy network. Moreover, each
i (oj,aj,ϕ(o)j,ϕ(a)j,rj,o′j) from D;
robot identifies its set of related robots Gt and its local infor-
i 15 Update critic by minimizing the loss
mation set L according to the previously described method.
i according to (21);
The LIA module then aggregates this information, applying
16 Update actor using the sampled policy
distance-dependent weight coefficients to emphasize the in-
gradient according to (22);
fluence of closer neighbors. This aggregation process yields
17 end
a comprehensive dataset that encapsulates both individual and
18 Update the target networks by soft update
collectivebehaviorwithintheenvironment.Thecollecteddata
manner:
(o,a,ϕ(o),ϕ(a),r,o′)aresubsequentlystoredintheexperi-
ence replay buffer D , which serves as a critical resource for θ′g ←ηθg+(1−η)θ′g
optimizing the policy and value networks during the network θ′a ←ηθa+(1−η)θ′a
training phase. This process of generating empirical data can
be found in lines 6-12 of Algorithm 1. 19 end
Network Training: After the experience data is generated, 20 end
the data set is extracted from the buffer D for training based
on priority experience replay [46]. At this stage, we need to
update G¯ and µ¯ according to (21) and (22). First, use the
target G¯′ to calculate the extended Q function qt+1 of the enablesittoquicklyproducesuitabletaskallocationsolutions
nextstate,andusethetemporaldifferencemethodtooptimize without complex heuristic rules. However, it struggles in
the parameters of the G¯ network based on the value of the dynamic environments with partial observability, as robots
extendedQfunction.Then,useG¯ tocalculatetheextendedQ must adapt to variable conditions and make decisions based
value qt of the current state-action, and use the calculated Q on incomplete information. Hence, during the distributed
valueandtheactionat+1 outputbyµ¯toupdatetheparameters execution phase, strategy optimization methods are required
of the µ¯ network (Algorithm 1 Lines 13-17). to enhance each robot’s decisions based on the shared policy
b) On-line Distributed Execution Phase: network µ¯. Therefore, there are two process during the
LIA MADDPG’s strength lies in its ability to autonomously distributed execution phase: policy output (Algorithm 2,
develop strategies for problem-solving through extensive Lines 2-4) and policy improvement (Algorithm 2, Lines 5-8),
instance sampling and learning. This end-to-end approach which alternate to facilitate decision-making by each robot.IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 9
Fig. 3: Structure of the proposed LIA MADDPG, which consists of two parts: Off-line centralized training and on-line
distributed execution. The LIA MADDPG employs four types of neural networks: 1) an actor network for generating the
action policy; 2) a critic network for evaluating the action policy; 3) a target actor network; 4) a target critic network for
stabilizing the learning process. Furthermore, a local information aggregation (LIA) module is employed to accelerate the
learning process.
In the policy output stage, robot feeds the current obser- Algorithm 2: Distributed Policy Improvements and
vation ot
i
into a pretrained neural network µ¯(·| θa) to obtain Execution
the corresponding action at. This stage maps observations of Input : Robot i’s policy network µ¯∗ , state s action
i i
the environment to actions with the shared policy, generating space a
i
initial decisions using the pretrained network. Output: Optimized allocation strategy µ¯
finl
In the policy improvement stage, we design a deviation 1 for t = 1 to T e do
probability δ depending on two factors to assess the prob- // Policy execution
i,gt
ability of a robi ot deviating from the current policy or target 2 if ct i =1 then
task. Firstly, we consider the correlation between the number 3 obtain observation ot i of robot i;
of robots already assigned to the current selected target task 4 g it ←at i =µ¯∗(ot i| θa);
ht and the highest number of robots can be assigned to the // Policy improvement
gt
tasi k h¯ . If ht is closer to h¯ , there is a higher probability 5 Calculate divergence probability δ i,gt and
of
devg iait
tion.
Tg it
his is because
wg it
e encourage robots to explore
ξ=random(0,1); i
tasks with fewer assigned robots. 6 if ξ <δ i,gt then
i
7 at i ←g it =argmax j∈Mφ 1r i,j −d i,j;
(cid:40) h¯ −h , if h¯ >ht 8 end
h¯
g it
⊗ht
g it
= 0,g it g it otheg rwit
ise
g it (23) 109 endst i+1 ←P i(st i,at i);
11 end
Secondly, the correlation between the number of neighboring
robotsadoptingthesamestrategy.Letα representthenumber
i
of neighbor agents that can be observed by robot i, and β i C. Interpretability of LIA MADDPG
represent the number of robots i′ that adopt the same strategy
In robotic physics systems, the interpretability of MARL
asroboti,i.e.,gt =gt,i′ ∈Nt.Bycalculatingthedifference
i′ i i is crucial for ensuring that robot teams can execute tasks
α − β , we can assess the level of potential conflict with
i i
safelyandefficientlywithinphysicalconstraints.Robotsmust
neighboring robots. In order to avoid tasks that may conflict
interactnotonlywithcomplex,dynamicenvironmentsbutalso
with neighboring robots, we encourage robots to prioritize
coordinate with one another, significantly increasing the com-
tasks where the difference between α and β is smaller.
i i
plexity of group decision-making. Therefore, understanding
This ensures that robot i selects a task that is less likely to
and explaining the mechanisms behind these decisions is vital
cause conflicts or overlap with the tasks of its neighboring
for optimizing the system performance and ensuring that their
robots. Finally, based on the above discussions, we design the
(cid:18) (cid:19) behavior adheres to physical laws.
deviation probability as δ
i,gt
=e− h¯ git⊗ht
git
(αi−βi)
. Early research on the interpretability of RL primarily
iIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 10
focused on single-agent models, employing techniques like designed to rigorously test the algorithm under diverse and
feature importance analysis and policy-level explanations to controlled conditions, ensuring a robust evaluation of its
clarify why an agent takes a particular action in a given state. potential in real-world multi-robot applications.
These methods highlight the most critical features driving
agent decisions. For example, Shapley Additive Explanations A. Experimental Setup
(SHAP)[47] have been widely used to quantify each feature’s
Simulation Environment Design: The simulation environ-
contribution to the decision-making process. In addition, vi-
ment for the training process is designed in a 1000*1000m2
sualization techniques have also played an important role in
obstacle-free 2D space. At the beginning of each training
enhancing RL model interpretability. Tools such as interactive
episode, we randomly generate the initial positions for N
saliency maps and visual analytics provide deep insights into
robots and M target tasks, where the coordinates of all
agentlearningstrategies.Forinstance,[48]usedsaliencymaps
positions are normalized to the range [0, 1]. During the
to analyze agent strategies in Atari games, while [49] per-
generationprocess,thevelocityofeachrobotissampledfrom
formed multi-level analyses on the DQN algorithm, offering
a uniform distribution [2m/s, 5m/s]. The velocity of each task
detailed insights into agent behavior across different learn-
is generated from a uniform distribution [0.5m/s, 1m/s], and
ing stages. These techniques not only help in understanding
its turning angle varies randomly within the range of [-π, π].
how agents learn over time but also make complex decision
Thismeansthattherobotscanchoosetheirtargettasksinany
processes more intuitive and easier to interpret. However,
direction. The association distance between tasks and agents
MARL introduces additional complexity. In multi-agent sys-
is set to d = 30m. The maximum demand of agents for
tems, agents must learn from both environmental interactions bind
each task, h¯ is set to ⌈N/M⌉, and the maximum number of
and coordination or competition with other agents, making j
neighbors with which an agent can perceive simultaneously is
their behavior harder to explain. Traditional methods struggle
α =10.
to clarify how individual behaviors evolve into collective i
Training Process Configuration: The maximum step
strategies, posing significant challenges for the design and
length per episode is capped at 150 time steps. The reward
validation of MARL systems.
parametersaredefinedasφ =10,φ =−0.001,andφ =1,
To address these challenges, this paper explores the inter- 1 2 3
with r values sampled uniformly between 0 and 1. For the
pretability of LIA MADDPG in large-scale robot task allo- neurali, nj etworks, both the shared value network G¯ and the
cation, examining the algorithm’s logic and reward function
shared policy network µ¯ consist of two hidden layers, each
design. Additionally, we explore visual analytics as a promis-
containing 128 neurons. Network updates are managed using
ing research direction. In the last part of the experiment,
the Adam optimizer with learning rates of 0.001 and 0.002,
we have developed a high-fidelity physics engine simulation
respectively. An experience replay buffer is maintained with
system and employed visualization techniques to illustrate the
a capacity of 5000 to facilitate efficient learning, and batch
interactionsandbehavioralevolutionofagentsunderdifferent
sizes for stochastic gradient descent (SGD) are set at 64, with
physical conditions. These visual tools not only enhance the
a learning rate η of 0.01 for the target network.
understanding of MARL decision-making processes but also
Performance Metrics: Performance comparison among
reveal the interaction mechanisms and behavioral dynamics
differentalgorithmsisbasedonthreekeymetrics:Normalized
of agents. By tracing the decision rationale of each agent
Average Total Utility (NATU), which evaluates the effective-
in complex physical environments, these methods provide
ness of an algorithm in task allocation by assessing overall
strong theoretical and practical support for further research
rewards; Normalized Average Time Cost (NATC), measur-
on interpretability in physical systems, paving the way for
ing task completion efficiency within a set timeframe; and
the development of more transparent and reliable multi-agent
Dominance Rate (DR), indicating the frequency at which one
systems.
algorithm outperforms others in terms of total rewards over a
series of sample processes.
IV. EXPERIMENTS
B. Training Results and Comparisons
Inthissection,wewillconductacomprehensiveevaluation
of the proposed LIA MADDPG algorithm to assess its effi- Tosystematicallyevaluatetheeffectivenessofourproposed
ciency and scalability in a simulated multi-robot system. Our LIA MADDPG algorithm, we conduct experiments involv-
experiments include: benchmarking LIA MADDPG against ing different numbers of robots (N = 30, 60, and 90) to
established reinforcement learning methods for dynamic task test the pretraining strategies. Our objective is to assess the
allocation and agent coordination; conducting ablation stud- performance of LIA MADDPG in terms of efficiency and
ies to highlight the contributions of the Local Information scalability, comparing it against six existing reinforcement
Aggregation (LIA) module and policy improvement method; learning-based algorithms: Ind DQN [50], Ind DDDPG [45],
integrating the LIA module with other MARL methods to MAAC [29], QMIX [30], LINDA QMIX [51], and MAPPO
showcase its wide applicability; performing scalability tests [31]. These methods are chosen due to their effectiveness
across various robot system sizes to analyze the algorithm’s in the relevant fields: Ind DQN and Ind DDDPG demon-
performance consistency; and utilizing a high-fidelity physics strate strong performance in single-agent environments, while
enginesimulationtobridgethegapbetweentheoreticalmodels MAAC, QMIX, and MAPPO are recognized for their multi-
and practical applicability. Each component is meticulously agentcoordinationcapabilities.Additionally,wehaveincludedIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 11
TABLE I: STATISTICALCOMPARISONOFTHETRAINEDPOLICIESWITHLIA MADDPG(OURS),IND DQN,
IND DDPG,MAAC,QMIX,LINDA QMIX,ANDMAPPO
Metrics N Ind DDPG Ind DQN MAAC QMIX LINDA QMIX MAPPO LIA MADDPG
30 0.458±0.153 0.454±0.185 0.742±0.087 0.769±0.080 0.778±0.081 0.775±0.075 0.793±0.079
NATU 60 0.414±0.145 0.405±0.187 0.770±0.072 0.806±0.067 0.801±0.064 0.828±0.057 0.864±0.059
90 0.282±0.127 0.295±0.145 0.666±0.079 0.730±0.073 0.705±0.062 0.775±0.060 0.863±0.052
30 0.395±0.179 0.387±0.187 0.372±0.187 0.357±0.187 0.331±0.188 0.348±0.185 0.325±0.123
NATC 60 0.529±0.182 0.527±0.187 0.460±0.173 0.431±0.172 0.451±0.171 0.418±0.170 0.364±0.154
90 0.447±0.171 0.450±0.174 0.397±0.165 0.362±0.160 0.378±0.164 0.309±0.142 0.260±0.138
30 2% 3% 12% 10% 15% 25% 33%
DR 60 1% 2% 6% 14% 9% 23% 45%
90 0% 0% 2% 7% 3% 10% 78%
the LINDA QMIX algorithm that incorporates the LINDA of individualistic algorithms like Ind DQN and Ind DDPG at
module, a local information decomposition mechanism. The N=90.
LINDA module decomposes local information by leveraging RegardingtheNormalizedAverageTimeCost(NATC),our
agents’ historical trajectories, enhancing team awareness and method demonstrates the lowest time consumption across all
allowingagentstoeffectivelyestimatetheglobalstateevenin scales, underscoring its superior efficiency compared to both
partially observable scenarios. LINDA is particularly useful single-agentandothermulti-agentstrategies.Thisemphasizes
in small-scale heterogeneous environments, such as StarCraft, therobustnessandconsistentperformanceofouralgorithm.In
where agents can infer the state of entities beyond their terms of the Dominance Ratio (DR), our method slightly out-
observable range. To ensure a fair comparison, all algorithms performs other multi-agent reinforcement learning algorithms
employed a unified network architecture and identical hyper- atsmallerscalesandshowsasignificantadvantageasthescale
parameters. increases. At N = 90, our method achieves a DR of nearly
80%, demonstrating its effectiveness and scalability. In stark
The convergence curves depicted in Fig.4 show that
contrast, methods like MAPPO experience a notable decline
LIA MADDPGconsistentlyachievesnear-optimalnormalized
in performance as N increases, with DR values dropping
average episode rewards across all tested robot counts, high-
significantly—MAPPO’s DR falls to just 10% at N = 90.
lighting its robustness in managing the increasing complexity
LINDA-enhanced QMIX (LINDA QMIX) faces similar is-
ofstateandactionspaces.ThoughotherMARLmethodsreach
sues, and in fact, the problem is even more pronounced.
comparable performance at N = 30, they suffer from slower
This further supports the idea that the LINDA module is
convergence, greater reward variability, and lower average
particularly useful in small-scale heterogeneous environments
rewards overall. In particular, LINDA QMIX, despite quickly
like StarCraft. However, in our problem setting, agents only
attaining 78% of the optimal normalized reward at N = 90,
need to focus on nearby relevant agents when selecting target
shows no further reward improvement during training, with
tasks,makingglobalstateawarenessunnecessary.Theexperi-
significant fluctuations in its convergence curve. This is due
mentalresultsdemonstratethatourmethodnotonlymaintains
to the large number of robots and the relatively homogenous
high efficiency in task completion across varying scales but
state information in our scenario, which hampers LINDA’s
also significantly improves performance metrics compared to
abilitytoeffectivelydecomposelocalinformationanddevelop
existing algorithms, affirming its superiority across diverse
individual teammate awareness. On the other hand, Ind DQN
operational contexts.
and Ind DDPG, which ignore inter-agent interactions, deliver
thepoorestperformance,underscoringthecrucialroleofsuch
dynamics in ensuring effective robot coordination. Our find- C. Performance Analysis of Key Components
ings reaffirm LIA MADDPG’s superior convergence speed,
In this subsection, we investigate the effects of the Local
stability, and overall performance, especially in large-scale
Information Aggregation (LIA) module and the policy im-
environments.
provementtechniquesasdetailedinSectionIII.Toassesstheir
Comparisonwithexistingmethods:Followingthetraining contributions, we conducted two sets of ablation experiments.
phase,wecarryoutanextensiveevaluationofourpolicy’sper- We first compare the performance of LIA MADDPG with
formance through 100 distinct initial test scenarios, character- the traditional MADDPG algorithm, which does not include
ized by randomly assigned task locations and robot departure the LIA module. Additionally, we integrate the LIA module
points.Thesetestsspanthreedifferentrobotpopulationssizes into two other MARL algorithms, MAAC and MAPPO, to
(N=30, 60, and 90). As detailed in Table I, our method con- evaluate its impact across different frameworks. The results,
sistentlyoutperformstraditionalMARLmethods,especiallyin illustrated in Fig.5 and Table II, clearly demonstrate the
theNormalizedAverageTotalUtility(NATU)metric.Though superior performance of the LIA-enhanced algorithms across
the other methods are competitive at a smaller scale (N=30), various metrics. Specifically, the LIA module significantly
our approach demonstrates significant superiority at larger accelerates the convergence process of the base algorithms,
scales, achieving NATU values up to twice as high as those as evidenced by the rapid increase in normalized averageIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 12
Fig. 4: Normalized average episode utility by LIA MADDPG and six comparison RL-based methods in different robot scales.
The solid and dashed lines show the mean and standard deviation of the results over ten runs, respectively.
TABLE II: STATISTICALCOMPARISONOFTHEPOLICIESLEARNEDWITHTHREEENHANCEDMARLALGORITHMS
(LIA MADDPG,LIA MAPPO,ANDLIA MAAC)ANDTHERETRADITIONALVISIONS(MADDPG,MAPPO,AND
MAAC)
Metrics N MAAC LIA MAAC MAPPO LIA MAPPO MADDPG LIA MADDPG
30 0.742±0.086 0.784±0.117 0.774±0.075 0.803±0.067 0.749±0.108 0.793±0.078
NATU 60 0.770±0.072 0.835±0.062 0.828±0.057 0.859±0.063 0.782±0.105 0.864±0.059
90 0.666±0.079 0.755±0.065 0.774±0.060 0.884±0.065 0.693±0.141 0.863±0.052
30 0.372±0.187 0.332±0.136 0.348±0.185 0.303±0.112 0.356±0.127 0.325±0.123
NATC 60 0.460±0.173 0.382±0.127 0.418±0.170 0.336±0.112 0.464±0.166 0.364±0.154
90 0.397±0.165 0.296±0.124 0.309±0.142 0.273±0.110 0.356±0.161 0.260±0.138
30 42% 58% 38% 62% 35% 65%
DR 60 31% 69% 26% 74% 20% 80%
90 25% 75% 12% 88% 10% 90%
total utility (NATU) for LIA MADDPG, LIA MAPPO, and
LIA MAAC during the initial training phases, as shown in
Fig.5. Moreover, the LIA module improves training stability,
as reflected in the reduced fluctuations in the utility curves
of the LIA-enhanced algorithms compared to their traditional
counterparts. We have also conducted a statistical evaluation
of these algorithm pairs using 100 sets of randomly generated
testcases.ThestatisticalresultsinTableIIfurthercorroborate
thesefindings.TheLIA-enhancedalgorithmsconsistentlyout-
perform their traditional versions across all evaluated metrics,
including NATU, NATC, and DR. Notably, as the problem
scale increases, the advantages of the LIA module become
evenmorepronounced.Theseresultsunderscoretheeffective-
ness of the LIA module in optimizing multi-agent reinforce-
ment learning algorithms, suggesting its broader applicability
in complex, dynamic environments.
Subsequently, to verify the effectiveness of the policy
improvement method during the online distributed execution
Fig. 5: NORMALIZEDAVERAGEEPISODEUTILITYBY
phase, we conduct a comparative experiment. We compare
THREEENHANCEDMARLALGORITHMS(LIA MADDPG,
the performance of a pretraining policy with LIA MADDPG,
LIA MAPPO,ANDLIA MAAC)ANDTHEIR
named P , against the same policy after incorporating
original TRADITIONALVISIONS(MADDPG,MAPPO,AND
thepolicyimprovementmethod,namedP .Thiscom-
improved MAAC)IN60ROBOTSCALES.
parisonisimplementedthroughtestingbothalgorithmsacross
100 randomly generated scenarios, with the outcomes docu-
mented in Fig.6 and Table III. The empirical evidence shows
thatP attainshigherNormalizedAverageTaskUtility Time (NATC) compared to P . This difference high-
improved original
(NATU) and reduced Normalized Average Task Completion lights the significant role of the policy improvement methodIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 13
TABLE III: COMPARISONBETWEENLIA MADDPGAND TABLE IV: COMPARISONBETWEENLIA MADDPGAND
LIA MADDPGWITHOUTPOLICYIMPROVEMENT THEGREEDYCHOICEMETHODINDIFFERENTSCALE
SCENARIOS
Metrics N P P
original improved
Metrics Scale Greedy choice LIA MADDPG
30 0.513±0.207 0.538±0.198
Small 0.498±0.207 0.525±0.198
NATU 60 0.484±0.064 0.534±0.219
NATU Medium 0.420±0.064 0.534±0.219
90 0.443±0.045 0.519±0.226 Large 0.310±0.045 0.509±0.226
30 0.314±0.168 0.293±0.136 Small 0.338±0.168 0.283±0.136
NATC 60 0.4591±0.154 0.364±0.146 NATC Medium 0.331±0.154 0.238±0.146
90 0.422±0.199 0.306±0.189 Large 0.461±0.199 0.336±0.189
Small 42% 58%
30 31% 69%
DR Medium 32% 68%
DR 60 21% 79% Large 18% 72%
90 8% 92%
robots and M=10 tasks. Subsequently, we test this trained
model on three different scales: a small-scale system (N=30,
M=5), a medium-scale system (N=100, M=8), and a large-
scalesystem(N=300,M=10).Eachofthesescalesissubjected
to 100 varied initial test scenarios to ensure comprehensive
comparative analysis.
For our baseline comparison, we selected the well-
established greedy choice method [52]. This heuristic method
processesdecision-makingatanindividualrobotlevel,aiming
to maximize personal rewards based on real-time task status,
without considering the interaction and cooperation among
robots. In contrast, our LIA MADDPG method integrates
these interactions, which is crucial for coordinated tasks in
multi-robotsystems.Toensureafairevaluation,bothmethods
are implemented with the same utility function and identical
(a) (b)
initial conditions for each test case.
Fig. 6: The statistics of NATU and NATC between our The effectiveness and performance metrics, specifically the
LIA MADDPG and LIA MADDPG without policy improve- task completion rate (DR), normalized average task util-
ment across different numbers of robots. ity (NATU), and normalized average task completion time
(NATC), are carefully analyzed. The results are presented in
Table IV, and the DR results are displayed in a bar chart in
in encouraging robots to actively explore and adjust their
Fig. 7.
strategiestooptimizeutility.Additionally,thetaskcompletion
Regarding NATU and NATC, it can be seen from Table IV
rate (DR), detailed in the third column of Table III, indicates
that our method consistently outperforms the greedy choice
thattheversionlackingthestrategyimprovementmethodper-
method across all scale scenarios. Notably, in the small
formsadequatelyonlyinsmallersettings(N =30).However,
and medium scale scenarios, our method exhibits nearly a
as the number of robots increases, its performance notably
15% improvement in NATU and a 20% reduction in NATC
deteriorates, underscoring the policy improvement method’s
compared to the greedy choice method. Even more, in the
essential role in boosting algorithm performance under more
large-scalescenario,ourmethodshowcasesremarkableperfor-
complex and challenging conditions. These findings robustly
mance, achieving a remarkable 64.2% improvement in NATU
demonstratethecriticalimportanceofthepolicyimprovement
and a substantial 36.9% reduction in NATC when compared
method, particularly in effectively scaling up the system to
to the greedy choice method. These results underscore the
handle larger scenarios. This underlines that integrating such
exceptional scalability of our proposed method, particularly
methodsiscrucialforenhancingtheadaptabilityandefficiency
in large-scale scenarios. From the perspective of DR, our
of the system in dynamic and unpredictable environments.
method outperforms the greedy choice method in 51 out of
100trainingsetsinthesmall-scalescenario.Asthenumberof
D. Scalability Performance of LIA MADDPG
robot increases, our method’s DR gradually expands over the
In order to rigorously test the scalability of our greedy choice method. In large-scale problems, our method
LIA MADDPG method across various sizes of robotic sys- even achieves an impressive DR as high as 71%, further
tems, we employ a structured experimental approach. We emphasizing the substantial advantages of our approach in
initiallytrainourmodelonamedium-scalesystemwithN=60 addressing large-scale problems.IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 14
(a) Small-scale scenario
(b) Medium-scale scenario
Fig. 8: Map of the robot swarm task allocation problem in a
1000 × 1000 area with small and medium scales.
complexity of the scenario. Robots are modeled as collision-
free spheres, also with dynamic state updates, enhancing the
realism of their interactions.
Fig. 7: The bar plot shows the DR statistics comparing our
In our subsequent experiments, we compare the perfor-
method (LIA MADDPG) with the greedy choice method
manceofouralgorithmwiththatofthegreedychoicemethod
across different scales (small, medium, large). Each subfigure
[52], which serves as a baseline. This comparative analysis is
reflectsperformanceover100experiments,wherethebarcolor
designed to validate our algorithm’s performance in simulated
indicates the better-performing algorithm
scenarios that closely mimic real-world conditions. To aid
in visual tracking and understanding of the task allocation
process, robots change color to match their selected tasks,
E. Physics Enginebased Simulation Experiment
and this color shifts whenever they switch targets. This visual
Torigorouslyevaluateouralgorithm’spracticalapplicability mechanism not only makes the simulation more intuitive but
in real robot swarms, we have implemented a high-fidelity also provides clear, real-time feedback on the algorithm’s
physics engine that features an accurate system model. This decision-making process.
model is crucial for facilitating the ’sim-to-real’ transition, a) Local Situational Planning: Fig.9 provides insights
allowing our algorithm to move smoothly from simulation intotheinitialallocationstrategiesofbothalgorithmsinsmall-
environments to real-world applications. Our aim with these scale and medium-scale scenarios. Upon close examination
experimentsistothoroughlyassessthefeasibilityandefficacy of the figure, a notable distinction emerges: the strategy
of the algorithm under realistic conditions. employed by the greedy method prioritizes the maximization
The simulation of the robot swarm task allocation problem oftaskrewardsbuttendstooverlooktheactualenvironmental
iscarriedoutusingthePyBulletphysicsengine.Thedynamics conditions. For instance, as depicted in Fig.9a, Robot 1 opts
of the environment are depicted in Fig.8, where each task for the purple target task due to the potential higher return,
is visually represented by a cube, and the state of these while it neglects the task’s remote location that might entail
cubes updates dynamically. The spatial relationship between a risk of surpassing the agent’s capacity limit. Hence, it may
robots and tasks is indicated by transparent circles that define result in no rewards for Robot 1 while incurring significant
the association distance. Our simulation environment supports costs. In contrast, Fig.9b and Fig.9d reveal that the initial
various task motion patterns, including horizontal, vertical, allocation facilitated by LIA MADDPG leads to a more
circular, and random movements, which contribute to the clustered arrangement of robots around their respective targetIEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 15
(a) The greedy choice method in (b) The LIA MADDPG method (a) The greedy choice method (b) LIA MADDPG method(ours)
small-scale (ours) in small-scale
Fig.10:Comparetherobotmotiontrajectoriesinasmall-scale
scenario after the same time step.
solely reliant on the greedy method.
c) CollaborativeSynergyEnhancement: InFig.10a,the
white large circle illustrates instances where multiple robots
simultaneously select and endeavor to complete the same
blue target task simultaneously. This concurrent pursuit can
result in task exceeding the maximum robot load capacity,
as indicated by it turning gray. Consequently, this may result
in the robots missing out on their deserved rewards. In
(c) The greedy choice method in (d) The LIA MADDPG method contrast,LIA MADDPGstandsoutasaneffectiveapproachin
medium-scale (ours) in medium-scale coordinatingactionsamongrobots,promotingahigherlevelof
Fig. 9: A schematic diagram comparing the initial allocation collaboration in the task allocation process. This coordinated
strategiesofourmethodandthegreedychoicemethodinsmall effort ensures that tasks are allocated efficiently and effec-
and medium-scale scenarios. tively, reducing the likelihood of task overload and enabling
all participating robots to receive the deserves rewards.
task locations. This allocation strategy effectively mitigates V. CONCLUDINGREMARKS
potential risks associated with prolonged dynamic processes. This research has addressed a challenging problem known
Consequently, when juxtaposed with local strategies reliant as the robot swarm task allocation problem in dynamic task
on greedy selection methods, LIA MADDPG demonstrates environment. We have modeled this problem as Dec POMDP
a heightened ability to consider the specific context within andproposedanovelmulti-agentdeepreinforcementlearning
whichagentsoperate.Itexcelsinadaptingandmakingflexible approach, called LIA MADDPG. In the centralized training
decisions at the local level, effectively striking a balance phase, we introduce a module for local information ag-
between long-term returns and immediate rewards. gregation among robots, encouraging them to focus more
b) Forward-Thinking Capability : Fig.10 visualizes the on information beneficial to themselves during the training
trajectories and task completion status of two different al- process. In the distributed execution phase, we design strat-
gorithms in small and medium-scale scenarios over time, egy improvement methods to further enhance the quality of
where the red small circles highlight robots that have not allocation solutions. Finally, through extensive experiments,
yet completed their assigned tasks. It can be seen from the we have validated the effectiveness and superiority of this
figurethatrobotsfollowingthegreedymethodtendtotraverse method in terms of convergence speed and agent cooperation
moreconvolutedpathswhencomparedtothoseemployingthe performance.
LIA MADDPG approach, which leads to higher path costs. It is worth pointing out that the current design of
This divergence in path complexity underscores the efficiency LIA MADDPG is most effective in large-scale homogeneous
of LIA MADDPG in guiding robots to more streamlined agent scenarios. It may not be well-suited for environments
routes. In addition, after a period of execution,the statistical involving heterogeneous agents, where agents have different
resultsshowthatover90%oftherobotsusingLIA MADDPG capabilities, tasks, and information processing requirements.
have successfully completed their task assignments, while Additionally,itisofintereststoextendourresearchtoincorpo-
nearlythirtypercentoftherobotsadheringtothegreedyalgo- ratecollisionavoidancestrategieswithintherobotswarmtask
rithm still remains unfinished. This discrepancy demonstrates allocation framework. Therefore, a potential future direction
therobustnessofLIA MADDPGintermsoftaskcompletion. will involve developing a multi-agent reinforcement learning
In essence, these findings suggest that LIA MADDPG approach that comprehensively considers both heterogeneous
exhibits a higher level of foresight and is more effective agent scenarios and collision avoidance strategies, ensuring
in yielding superior long-term results compared to strategies robustperformanceacrossdiverseanddynamicenvironments.IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 16
REFERENCES [22] D.P.Bertsekas,“Anewalgorithmfortheassignmentproblem,”Math-
ematicalProgramming,vol.21,no.1,pp.152–171,1981.
[1] X.Dong,Y.Li,C.Lu,G.Hu,Q.Li,andZ.Ren,“Time-varyingforma- [23] R. G. Smith, “The contract net protocol: High-level communication
tiontrackingforuavswarmsystemswithswitchingdirectedtopologies,” and control in a distributed problem solver,” IEEE Transactions on
IEEETransactionsonNeuralNetworksandLearningSystems,vol.30, computers,vol.29,no.12,pp.1104–1113,1980.
no.12,pp.3674–3685,2018. [24] L. Luo, N. Chakraborty, and K. Sycara, “Distributed algorithms for
[2] O. T. Abdulhae, J. S. Mandeep, and M. Islam, “Cluster-based routing multirobottaskassignmentwithtaskdeadlineconstraints,”IEEETrans-
protocols for flying ad hoc networks (fanets),” IEEE Access, vol. 10, actionsonAutomationScienceandEngineering,vol.12,no.3,pp.876–
pp.32981–33004,2022. 888,2015.
[3] G. Chen and Y.-D. Song, “Cooperative tracking control of nonlinear [25] D.-H. Lee, S. A. Zaheer, and J.-H. Kim, “A resource-oriented, de-
multiagentsystemsusingself-structuringneuralnetworks,”IEEETrans- centralized auction algorithm for multirobot task allocation,” IEEE
actions on Neural Networks and Learning Systems, vol. 25, no. 8, Transactions on Automation Science and Engineering, vol. 12, no. 4,
pp.1496–1507,2013. pp.1469–1481,2014.
[4] M.Campion,P.Ranganathan,andS.Faruque,“Uavswarmcommunica- [26] J. Zhang, G. Wang, and Y. Song, “Task assignment of the improved
tionandcontrolarchitectures:areview,”JournalofUnmannedVehicle contractnetprotocolunderamulti-agentsystem,”Algorithms,vol.12,
Systems,vol.7,no.2,pp.93–106,2018. no.4,p.70,2019.
[5] S.PoudelandS.Moh,“Taskassignmentalgorithmsforunmannedaerial [27] I.Jang,H.-S.Shin,andA.Tsourdos,“Anonymoushedonicgamefortask
vehiclenetworks:Acomprehensivesurvey,”VehicularCommunications, allocationinalarge-scalemultipleagentsystem,”IEEETransactionson
vol.35,p.100469,2022. Robotics,vol.34,no.6,pp.1534–1548,2018.
[6] X. Wang, M. Peng, H. Lin, Y. Wu, and X. Fan, “A privacy-enhanced [28] S. A. Akgun, M. Ghafurian, M. Crowley, and K. Dautenhahn, “Using
multiareataskallocationstrategyforhealthcare4.0,”IEEETransactions affectasacommunicationmodalitytoimprovehuman-robotcommuni-
onIndustrialInformatics,vol.19,no.3,pp.2740–2748,2022. cationinrobot-assistedsearchandrescuescenarios,”IEEETransactions
[7] T. Chen, Y. Cai, L. Chen, and X. Xu, “Trajectory and velocity plan- onAffectiveComputing,2022.
ning method of emergency rescue vehicle based on segmented three- [29] S.IqbalandF.Sha,“Actor-attention-criticformulti-agentreinforcement
dimensional quartic bezier curve,” IEEE Transactions on Intelligent learning,” in International conference on machine learning, pp. 2961–
TransportationSystems,vol.24,no.3,pp.3461–3475,2022. 2970,PMLR,2019.
[8] H.Gao,J.Feng,Y.Xiao,B.Zhang,andW.Wang,“Auav-assistedmulti- [30] Q.Chen,“Nqmix:Non-monotonicvaluefunctionfactorizationfordeep
task allocation method for mobile crowd sensing,” IEEE Transactions multi-agent reinforcement learning,” arXiv preprint arXiv:2104.01939,
onMobileComputing,2022.
2021.
[9] K.LiuandY.Zhang,“Distributeddynamictaskallocationformoving
[31] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu,
targettrackingofnetworkedmobilerobotsusingk-wtanetwork,”IEEE
“Thesurprisingeffectivenessofppoincooperativemulti-agentgames,”
TransactionsonNeuralNetworksandLearningSystems,2024.
AdvancesinNeuralInformationProcessingSystems,vol.35,pp.24611–
[10] Q.Zhang,L.Gui,F.Hou,J.Chen,S.Zhu,andF.Tian,“Dynamictask
24624,2022.
offloadingandresourceallocationformobile-edgecomputingindense
[32] T.GabelandM.Riedmiller,“Adaptivereactivejob-shopschedulingwith
cloud ran,” IEEE Internet of Things Journal, vol. 7, no. 4, pp. 3282–
reinforcement learning agents,” International Journal of Information
3299,2020.
TechnologyandIntelligentComputing,vol.24,no.4,pp.14–18,2008.
[11] Q. Peng, H. Wu, and R. Xue, “Review of dynamic task allocation
[33] B. Waschneck, A. Reichstaller, L. Belzner, T. Altenmu¨ller, T. Bauern-
methods for uav swarms oriented to ground targets,” Complex System
hansl,A.Knapp,andA.Kyek,“Deepreinforcementlearningforsemi-
ModelingandSimulation,vol.1,no.3,pp.163–175,2021.
conductorproductionscheduling,”in201829thannualSEMIadvanced
[12] A.Khamis,A.Hussein,andA.Elmogy,“Multi-robottaskallocation:A
semiconductormanufacturingconference(ASMC),pp.301–306,IEEE,
reviewofthestate-of-the-art,”CooperativeRobotsandSensorNetworks
2018.
2015,pp.31–51,2015.
[34] B. Waschneck, A. Reichstaller, L. Belzner, T. Altenmu¨ller, T. Bauern-
[13] L. Wang, J. Liu, J. Hu, Q. Zhuge, and E. H.-M. Sha, “Optimal
hansl, A. Knapp, and A. Kyek, “Optimization of global production
assignment for tree-structure task graph on heterogeneous multicore
scheduling with deep reinforcement learning,” Procedia Cirp, vol. 72,
systems considering time constraint,” in 2012 IEEE 6th International
pp.1264–1269,2018.
SymposiumonEmbeddedMulticoreSoCs,pp.121–127,IEEE,2012.
[35] S.Qu,J.Wang,andJ.Jasperneite,“Dynamicschedulinginmodernpro-
[14] S.AnandH.J.Kim,“Simultaneoustaskassignmentandpathplanning
cessingsystemsusingexpert-guideddistributedreinforcementlearning,”
usingmixed-integerlinearprogrammingandpotentialfieldmethod,”in
in201924thIEEEInternationalConferenceonEmergingTechnologies
201313thInternationalConferenceonControl,AutomationandSystems
andFactoryAutomation(ETFA),pp.459–466,IEEE,2019.
(ICCAS2013),pp.1845–1848,IEEE,2013.
[36] C.-L. Liu, C.-C. Chang, and C.-J. Tseng, “Actor-critic deep reinforce-
[15] T.Issac,S.Silas,andE.B.Rajsingh,“Investigationsonpsobasedtask
mentlearningforsolvingjobshopschedulingproblems,”IeeeAccess,
assignment algorithms for heterogeneous wireless sensor network,” in
vol.8,pp.71752–71762,2020.
20192ndInternationalConferenceonSignalProcessingandCommu-
nication(ICSPC),pp.89–93,IEEE,2019. [37] S.Luo,L.Zhang,andY.Fan,“Real-timeschedulingfordynamicpartial-
no-waitmultiobjectiveflexiblejobshopbydeepreinforcementlearning,”
[16] W.C.StirlingandM.A.Goodrich,“Conditionalpreferencesforsocial
systems,”in2001IEEEInternationalConferenceonSystems,Manand IEEE Transactions on Automation Science and Engineering, vol. 19,
Cybernetics.e-Systemsande-ManforCyberneticsinCyberspace(Cat. no.4,pp.3020–3038,2021.
No.01CH37236),vol.2,pp.995–1000,IEEE,2001. [38] X.Wang,L.Zhang,T.Lin,C.Zhao,K.Wang,andZ.Chen,“Solving
[17] V. G. Lopez, F. L. Lewis, Y. Wan, E. N. Sanchez, and L. Fan, “Solu- job scheduling problems in a resource preemption environment with
tions for multiagent pursuit-evasion games on communication graphs: multi-agentreinforcementlearning,”RoboticsandComputer-Integrated
Finite-time capture and asymptotic behaviors,” IEEE Transactions on Manufacturing,vol.77,p.102324,2022.
AutomaticControl,vol.65,no.5,pp.1911–1923,2019. [39] F.A.Oliehoek,C.Amato,etal.,AConciseIntroductiontoDecentralized
[18] J.C.Amorim,V.Alves,andE.P.deFreitas,“Assessingaswarm-gap POMDPs,vol.1. Springer,2016.
based solution for the task allocation problem in dynamic scenarios,” [40] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mordatch,
ExpertSystemswithApplications,vol.152,p.113437,2020. “Multi-agent actor-critic for mixed cooperative-competitive environ-
[19] M. M. Zavlanos and G. J. Pappas, “Dynamic assignment in dis- ments,” Advances in Neural Information Processing Systems, vol. 30,
tributed motion planning with local coordination,” IEEE Transactions 2017.
onRobotics,vol.24,no.1,pp.232–242,2008. [41] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage
[20] D.Panagou,M.Turpin,andV.Kumar,“Decentralizedgoalassignment recognition,” in Proceedings of The IEEE Conference on Computer
and safe trajectory generation in multirobot networks via multiple VisionandPatternRecognition,pp.770–778,2016.
lyapunovfunctions,”IEEETransactionsonAutomaticControl,vol.65, [42] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
no.8,pp.3365–3380,2019. network training by reducing internal covariate shift,” in International
[21] J. Yu, S.-J. Chung, and P. G. Voulgaris, “Target assignment in robotic ConferenceonMachineLearning,pp.448–456,pmlr,2015.
networks: Distance optimality guarantees and hierarchical strategies,” [43] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean
IEEETransactionsonAutomaticControl,vol.60,no.2,pp.327–341, field multi-agent reinforcement learning,” in International conference
2014. onmachinelearning,pp.5571–5580,PMLR,2018.IEEETRANSACTIONSONNEURALNETWORKSANDLEARNINGSYSTEMS,VOL.,NO., 17
[44] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neuralinformationprocessingsystems,vol.30,2017.
[45] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D.Silver,andD.Wierstra,“Continuouscontrolwithdeepreinforcement
learning,”arXivpreprintarXiv:1509.02971,2015.
[46] T.Schaul,J.Quan,I.Antonoglou,andD.Silver,“Prioritizedexperience
replay,”arXivpreprintarXiv:1511.05952,2015.
[47] S. Lundberg, “A unified approach to interpreting model predictions,”
arXivpreprintarXiv:1705.07874,2017.
[48] J. Wang, W. Zhang, H. Yang, C.-C. M. Yeh, and L. Wang, “Visual
analyticsforrnn-baseddeepreinforcementlearning,”IEEETransactions
on Visualization and Computer Graphics, vol. 28, no. 12, pp. 4141–
4155,2021.
[49] J. Wang, L. Gou, H.-W. Shen, and H. Yang, “Dqnviz: A visual ana-
lytics approach to understand deep q-networks,” IEEE transactions on
visualizationandcomputergraphics,vol.25,no.1,pp.288–298,2018.
[50] V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,”arXivpreprintarXiv:1312.5602,2013.
[51] J.Cao,L.Yuan,J.Wang,S.Zhang,C.Zhang,Y.Yu,andD.-C.Zhan,
“Linda: Multi-agent local information decomposition for awareness
of teammates,” Science China Information Sciences, vol. 66, no. 8,
p.182101,2023.
[52] A. Vince, “A framework for the greedy algorithm,” Discrete Applied
Mathematics,vol.121,no.1-3,pp.247–260,2002.