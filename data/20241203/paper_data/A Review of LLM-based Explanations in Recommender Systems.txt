A Review of LLM-based Explanations in
Recommender Systems
Alan Said
University of Gothenburg, Gothenburg, Sweden
Correspondence*:
Corresponding Author
alan@gu.se
ABSTRACT
The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has opened new
opportunities for enhancing recommender systems through improved explainability. This paper
provides a systematic literature review focused on leveraging LLMs to generate explanations for
recommendations — a critical aspect for fostering transparency and user trust. We conducted a
comprehensive search within the ACM Guide to Computing Literature, covering publications from
the launch of ChatGPT (November 2022) to the present (November 2024). Our search yielded
232articles, butafterapplyinginclusioncriteria, onlysixwereidentifiedasdirectlyaddressingthe
useofLLMsinexplainingrecommendations.Thisscarcityhighlightsthat,despitetheriseofLLMs,
their application in explainable recommender systems is still in an early stage. We analyze these
select studies to understand current methodologies, identify challenges, and suggest directions
for future research. Our findings underscore the potential of LLMs improving explanations of
recommender systems and encourage the development of more transparent and user-centric
recommendation explanation solutions.
Keywords:recommendersystems,explainablerecommendation,largelanguagemodels,llms,explanations
1 INTRODUCTION
Recommendersystemshavebecomeintegraltovariousdigitalplatforms,assistingusersinnavigatingvast
amountsofinformationbysuggestingproducts, services, orcontenttailoredtotheirpreferences. From
e-commercesitesrecommendingproductstostreamingservicessuggestingmovies,thesesystemsaimto
enhanceuserengagementandsatisfaction.However,asrecommendersystemsgrowincomplexity,users
oftenremainunawareoftheunderlyingmechanismsthatgeneratethesepersonalizedsuggestions.Eslami
et al. (2019) and Ge et al. (2024) identified that this opacity can lead to mistrust, reduced satisfaction,
reducedengagement,andreluctancetoadoptnewrecommendations.Engagementinthiscontextrefersto
theusers’willingnesstocontinueinteractingwith,orusing,thesystem.Explanationswithinrecommender
systems serve as a bridge between complex algorithms and user understanding. By providing insights
into why a particular item is recommended, explanations empower users to make informed decisions,
enhancing their overall experience. They help users assess the relevance of recommendations, increase
transparency,andbuildtrustinthesystem(Luetal.,2023).Forinstance,anexplanationthathighlights
howarecommendedbookalignswithauser’spastreadinghabitscanmakethesuggestionmorepersuasive
andacceptable.
Moreover, explanations can alleviate concerns about privacy and data usage by clarifying how user
informationisusedwhengeneratingrecommendations(AbdollahiandNasraoui,2018).Theycontributeto
asenseofcontrol,allowinguserstoadjusttheirpreferencesandinteractmoreeffectivelywiththesystem.
1Said LLM-basedExplanationsinRecommenderSystems
In scenarios where recommendations might seem unexpected or irrelevant, explanations can mitigate
confusionandpreventuserdisengagement(TintarevandMasthoff,2022).
There are two primary categories of explainable AI methods commonly used in recommender
systems: local and global explanations. Local explanations focus on providing insights into a specific
recommendation,explainingwhyaparticularitemwassuggestedtoaparticularuser.Incontrast,global
explanationsaimtoprovideaholisticunderstandingoftheoverallbehavioroftherecommendermodel,
offering insights into how the model functions generally across all users. Additionally, methods for
explainability can be categorized as model-specific or model-agnostic. Model-specific methods are
designedforspecifictypesofalgorithms,whilemodel-agnosticmethodscanbeappliedtoanymachine
learning model. Common frameworks like LIME (Local Interpretable Model-agnostic Explanations)
by Ribeiro et al. (2016) and SHAP (SHapley Additive exPlanations) by Lundberg and Lee (2017) are
widelyusedtogeneratesuchexplanations.Theseframeworksprovidemechanismstoexplainindividual
predictions (local explanations) or give a broader view of the model’s decision-making process (global
explanations). Explanations based on these methods provide a sense of technical transparency, as they
generateexplanationsbasedonthefeaturesofaspecificitemoralgorithnicmodel.
WhileexplanationsgeneratedbyLLMssuchas,e.g.,OpenAI’sChatGPT(Brownetal.,2020)orMeta’s
LlaMA(Touvronetal.,2023)offernewopportunitiesforgeneratingrich,naturallanguagedescriptionsof
whyanitemisrecommended,theydonotnecessarilyconformtotraditionallocalorglobalexplanation
frameworks.UnlikeLIMEandSHAP,LLMsfocusonproducingcontextuallyrelevant,human-readable
interpretations of recommendations. These interpretations tend to justify why a recommendation might
makesensefromauser’sperspectiveratherthanexplainbyanalyzingthealgorithm’sinternalmechanics.In
thissense,LLMsthusofferaccessiblejustificationsratherthanprecise,analyticexplanations(asdiscussed
bySilvaetal.(2024)).
Thisliteraturereviewexplorestheintersectionofrecommendersystems,explanations,andlargelanguage
models.ByexaminingrecentstudiesthatleverageLLMstogenerateexplanationsforrecommendations,
weaimtounderstandthecurrentlevelofintegratingLLM-basedexplanations(orjustifications),identify
challenges,andhighlightopportunitiesforfutureresearch.
2 METHODOLOGY
Fordatacollection,weadheredtothesystematicliteraturereviewprocedureoutlinedintheguidelinesby
Kitchenhametal.(2007).Todevelopaneffectivesearchstrategy,weconductedascopingreviewofrelevant
published literature. The scoping review procedure is outlined in Fig. 1. In line with findings by Bauer
etal.(2024),thisscopingreviewrevealedthatthekeywordsrecommendationsystemsandrecommender
systemsareusedinterchangeably,withthelatterbeingmoreprevalentintheresearchcommunitycentered
aroundtheACMConferenceonRecommenderSystems(RecSys)1,whilebothtermsarewidelyusedin
otherresearchoutlets.
2.1 Literature Search
Withouraimtocoverresearchrevolvingaroundexplainablerecommendersystems,weidentifiedthat
searchesusingthekeywordsexplainabilityandexplanationssignificantlyoverlap.Toensuretheinclusion
ofcomprehensiveandsubstantialstudies,welimitedoursearchtoworkslabeledasResearchArticleor
ShortPaper,therebyexcludingabstracts,reports,andsimilarpublications.
1 https://recsys.acm.org/
2Said LLM-basedExplanationsinRecommenderSystems
Scoping review
232 papers Query
Data Cleaning and Paper Selection
Control criteria for
6 papers inclusion/exclusion
Selection Analysis
Figure1. Papersearchandselectionprocedureforthesurveyedpapers.
Oursearchstrategytoidentifyeligiblepapersconsistedofseveralconsecutivestages.TheACMDigital
Library2 notonlycontainspaperspublishedbyACMbutalsobyotherpublishers,allowingustosearch
for papers in the main established conferences and journals where research on recommender system
evaluation is published. Besides the main conference on recommender systems—ACM RecSys—this
includesconferencessuchasSIGIR,CIKM,UMAP,andKDD.Relevantjournalsinclude, forinstance,
TORS,TOIS,TIST,andTIIS.
Accordingly, we sampled papers from the ACM Digital Library (The ACM Guide to Computing
Literature), which is described as “the most comprehensive bibliographic database in existence today
focusedexclusivelyonthefieldofcomputing.”3
We considered papers within an encapsulated time frame from November 1, 2022 (the launch of
ChatGPT),tothepresent(November1,2024atthetimeofwriting),assumingthattheemployeddatabases
andsearchengineshavealreadycompletedindexingpapersfromconferencesandjournals.
Asourliteraturereviewfocusesonresearchregardingtheexplanationofrecommendersystemsusinglarge
languagemodels,wesearchedforpapersindexedwiththetermsrecommendationsystem*orrecommender
system*(coveringbothcommonlyusedterms),andlargelanguagemodel*,LLM*,ChatGPT,orChat-GPT
(toencompassrelatedLLMterms),aswellasthetermexpla*(tocoverexplainabilityandexplanation(s)).
Altogether,thisresultedinthefollowingquery:
2 https://dl.acm.org
3 https://libraries.acm.org/digital-library/acm-guide-to-computing-literature
3Said LLM-basedExplanationsinRecommenderSystems
"query":
{
Fulltext:(
"recommender system*"
OR
"recommendation system*"
)
AND
Fulltext:(
"large language model*"
OR
"llm*"
OR
"chatgpt"
OR
"chat-gpt"
)
AND
Fulltext:(expla*)
}
"filter": {
Article Type: Research Article OR Short Paper,
E-Publication Date: (11/01/2022 TO 11/01/2024)
}
Thisquery4 returnedatotalof232hitsasofNovember5,2024.
2.2 Literature Selection
Apaperwasincludedifitfulfilledallofthefollowingcriteria(pre-establishedinclusioncriteria):
(i) Thepaperdealswiththeexplanationofrecommendations.
(ii) Thepaperusesalargelanguagemodelforgeneratingexplanations.
(iii) ThepaperisaResearchArticleoraShortPaper.
(iv) The paper was published within the time range from November 1, 2022, to November 1, 2024,
inclusive.
Apaperwasexcludedifitmetanyofthefollowingcriteria(pre-establishedexclusioncriteria):
(a) Thepaperisnotaresearcharticleorshortpaper.
(b) Thepaperisanabstract,demopaper,tutorialpaper,orreport.
(c) Thepaperisaliteraturereview.
4 https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&expand=all&AfterMonth=
11&AfterYear=2022&BeforeMonth=10&BeforeYear=2024&AllField=Fulltext%3A%28%22recommender+system*%22+
OR+%22recommendation+system*%22%29+AND+Fulltext%3A%28%22large+language+model*%22+OR+%22llm*%22+OR+
%22chatgpt%22+OR+%22chat-gpt%22%29+AND+Fulltext%3A%28expla*%29&startPage=&ContentItemType=short-paper&
ContentItemType=research-articleNB:DuetohowACMindexespapersfromotherpublishers,thenumberofretrievedpapersmaydifferon
differentdates.
4Said LLM-basedExplanationsinRecommenderSystems
(d) Thepaperisnotconcernedwithrecommendersystems.
(e) Thepaperdoesnotcontributetoexplanationsofrecommendations.
Thisdatacleaningandselectionprocedureledtotheexclusionof226papers.Theremainingsixpapers
constitute our final sample resulting from the query. Table 1 provides an overview of the papers in the
sample,includingvenuewhereitwaspublished,papertype,andreasonsforinclusion.
Table1. Overviewoftheincludedpapers,orderedbyyearandvenue.
Paper Venue Type Reasonforinclusion
Parketal.(2023) EICS Late-breakingResults Conversationalexplanations
Guoetal.(2023) SIGIR ResourcePaper Evaluationsofexplanations
Silvaetal.(2024) IUI ResearchPaper LLM-basedexplanations
LLM-basedcross-domain
Petruzzellietal.(2024) RecSys ResearchPaper
recommendationandexplanations
Hendrawanetal.(2024) ExUM@UMAP Workshoppaper LLM-basedexplanations
Lubosetal.(2024) ExUM@UMAP Workshoppaper LLM-basedexplanations
2.3 Literature Analysis
The systematic search identified only six papers focusing on recommendation explanations using
LLMs.Weanalyzesthesepapersfocusingonuse-cases,datasets,methodologies,andresearchquestions
investigatedinthecontextoftheseworksinordertogetanunderstandingofthecurrentstateoftheart
ofleveragingLLMsforrecommendationexplanations. Themajorityoftheexcludedpapersmentioned
recommendationandexplanationinthecontextofLLMs,albeitwithoutapplyingLLMsforthepurpose
ofgeneratingexplanations.Thebulkoftheretrieved232paperswerepublishedin2024(186),notably,
only two out of the retrieved papers were published in 2022 pointing to the recent rapid growth of the
LLM-relatedliterature.
3 RESULTS
Afterapplyingthespecificinclusionandexclusioncriteriaforthisliteraturereview(cf.Section2),onlysix
paperswereselectedfordetailedanalysis.Giventhatlessthantwoyearshavepassedsincethelaunchof
ChatGPT,thepublicationandpeerreviewofthesesixpapersatconferencesandworkshopssignalarapidly
emergingfield.TheselectedpaperswerepublishedatEICS,SIGIR(both2023),andIUI,ExUM@UMAP,
andRecSys(all2024)reflectingtheincreasingfocusthepotentialofLLMsforgeneratingexplanationsin
recommendersystems.
3.1 Papers
Below,weprovideadetailedoverviewofeachofthesixincludedpapers.
3.1.1 A User Preference and Intent Extraction Framework for Explainable Conversational
Recommender Systems (Park et al., 2023)
The paper focuses on improving conversational recommender systems (CRS) by addressing two
primary challenges, i.e., preference extraction from users during conversations and transparency in
recommendations. Traditional CRSs have limitations in extracting accurate and context-aware user
preferences and often function as black-box models, leading to a lack of transparency in their
5Said LLM-basedExplanationsinRecommenderSystems
recommendations. To overcome these issues, the authors propose a novel framework that combines
entity-baseduserpreferenceextractionwithcontext-awareintentextraction,generatingrecommendations
that are both interpretable and aligned with user preferences. The framework uses item feature entity
detectionandsentimentanalysistoextractuserpreferencesandassignsratingstoeachfeaturementioned
duringtheconversation.Itthenusesgraph-basedrepresentationsofuserutterancestocapturecontextual
user intent, enhancing the accuracy of recommendations. The final recommendation phase involves
candidateselectionandrankingwithouttheuseofblack-boxmodels,ensuringthesystemisexplainable.
Forgeneratingexplanations,thepaperusesGPT-2(Radfordetal.,2019),whichprovidesuserswithnatural
languageexplanationsbasedonitemfeaturesandestimateduserpreferencesfortherecommendations.
The framework was evaluated using two datasets, INSPIRED introduced by Hayati et al. (2020) and
ReDial by Li et al. (2018), demonstrating improvements in preference extraction and recommendation
accuracycomparedtoexistingmethods.Thepaperconcludesbyhighlightingthepotentialofthisapproach
tomakeCRSsmoretransparentanduser-friendly,whilealsooutliningfutureresearchdirections,suchas
conductinguserstudiestoevaluatesatisfactionwiththeexplanations.
3.1.2 Towards Explainable Conversational Recommender Systems (Guo et al., 2023)
This paper focuses on improving explainable CRSs by addressing the challenges associated with
generating multiple contextualized explanations in real-time. It proposes a novel set of ten evaluation
perspectivestomeasurethequalityofexplanationsinCRS,suchaseffectiveness,persuasiveness,andtrust.
Toaddressthelackofhigh-qualityexplanationsinexistingdatasets,theauthorsdevelopE-ReDial,anew
conversationalrecommendationdatasetwithover2,000manuallyrewrittenhigh-qualityexplanationsfor
movies.
ThepaperhighlightstwobaselineapproachesforgeneratingexplanationsinCRS,i.e.,training-based
models,suchasT5(Raffeletal.,2020),andprompt-basedmodels,suchasGPT-3(Brownetal.,2020),
evaluatedontheirabilitytogenerateexplanationsusingthenewlyconstructeddataset.Theresultsshow
thatmodelstrainedonE-ReDialoutperformexistingmethodsinproducingexplanationsthatareclearer,
morepersuasive,andcontextuallyappropriate.Additionally,integratingexternalknowledge,suchasmovie
reviewsanddescriptions,furtherimprovesperformance,especiallyintermsofexplanationquality.
The results demonstrate the effectiveness of E-ReDial in improving CRS explainability, while also
pointing out future directions for improving explainable CRS, including the need of developing more
realisticdatasetsandadvancingautomaticexplanationgenerationtechniques.
3.1.3 Leveraging ChatGPT for Automated Human-centered Explanations in Recommender
Systems (Silva et al., 2024)
ThispaperinvestigatestheuseofChatGPTtogeneratepersonalized,human-centeredexplanationsin
recommender systems. The paper aims to address the need for transparency and interpretability in RS,
whichthepaperstatesarecrucialforbuildingusertrust.Currentsystemsoftenlackscalableandmeaningful
waystoexplaintheirrecommendations.Tofillthisgap,thepaperproposesleveragingChatGPTtogenerate
personalizedexplanationsformovierecommendations,withafocusonuserexperience.
The paper presents a user study evaluating ChatGPT’s ability to generate explanations of
recommendations,involving94participants.Eachparticipantprovidedmoviepreferences,andChatGPT
wastaskedwithgeneratingbothpersonalizedrecommendationsanddisrecommendations(itemstoavoid).
ThestudyassessedwhetheruserspreferredChatGPT-generatedrecommendationsandexplanationsover
random(butpopular)recommendations.
6Said LLM-basedExplanationsinRecommenderSystems
Thepaper’skeyfindingsstatethatuserspreferredChatGPT-generatedrecommendationsoverrandom
selectionsofpopularmovies,thatpersonalizedexplanationsbasedonuserpreferenceswerenotperceived
assignificantlymoreeffectiveorpersuasivethangenericonesunlesstherecommendationswererandom,
andthatdisrecommendationsalsobenefitedfromChatGPT’sexplanations,althoughgenericexplanations
oftenoutperformeduser-basedonesinthesecases.
ThepaperconcludesthatwhileChatGPTcaneffectivelygeneratenaturallanguageexplanations(referred
toasjustifications),moreworkisneededtofullyunderstandhowusersperceivepersonalizationandwhat
makesexplanationstrulypersuasive.Thepaperalsoexploreshowexplanationgoals,suchaseffectiveness
andpersuasiveness,relatetooneanotherthroughapathmodelanalysis,revealingthatpersuasivenessplays
akeyroleinhowusersjudgeexplanationquality.
3.1.4 Explanations in Open User Models for Personalized Information Exploration (Hendrawan
et al., 2024)
Thispaperexploreshowopenusermodelscanenhancetransparencyandusercontrolinpersonalized
informationexplorationsystems.Thefocusisonprovidinguserswithcontrolovertherecommendation
process by allowing them to adjust their profile through adding or removing topics and receiving
explanations for these choices. The study is conducted within the Grapevine system by Rahdari et al.
(2020),anexploratorysearchplatformdesignedtohelpstudentsfindfacultyadvisorsforresearchprojects.
ThepaperextendsthesystembyincorporatingLLMstogenerateexplanations,introducingtwotypesof
explanations:individualkeyphraseexplanationsandrelationshipexplanationsbetweenkeyphrases.These
explanationsareintendedtoassistusersinunderstandingthesignificanceofthetopicsintheirprofileand
howdifferenttopicsareinterconnected.
The study features an observational experiment with 23 participants, who used the Grapevine system
to select research topics and faculty members. The results show that users who accessed explanations,
especiallythosethatexploredrelationshipsbetweentopics,exhibitedhigherengagementwiththesystem
and made more refined adjustments to their user profile. However, it was noted that while individual
explanationsimprovedusers’confidence,relationshipexplanationswereusedlessfrequentlyandledto
mixedsatisfactionlevels.
The paper highlights the potential benefits of using LLMs to generate contextual explanations in
personalizedsystemsbutalsopointsoutchallenges,suchastheneedforclearerexplanationsandmore
intuitiveinterfacestoimproveusersatisfaction.
3.1.5 LLM-generated Explanations for Recommender Systems (Lubos et al., 2024)
ThepaperexplorestheuseofLLMs,suchasLlaMA2,togeneratepersonalizedexplanationsforthree
typesofrecommendersystems:feature-based,item-basedcollaborativefiltering,andknowledge-based
recommendations.ItexaminesthepotentialofLLMstoimprovethequalityofexplanationsthroughnatural
languagegenerationandevaluateshowusersperceivetheseexplanationscomparedtotraditionalmethods.
To gather insights, the authors conducted an online user study with 97 participants, who interacted
with LLM-generated explanations across different recommendation types. The study aimed to explore
user preferences, assess the quality of LLM-generated explanations, and identify characteristics that
users found appealing. Results indicated a clear preference for explanations generated by LLMs over
baseline approaches, with participants appreciating the detail, creativity, and informative nature of the
explanations.UsersfavoredLLM-generatedexplanationsfortheirabilitytoprovidecontextualinformation
andbackgroundknowledge,whichenhancedtrusttoandsatisfactionwiththerecommendations.
7Said LLM-basedExplanationsinRecommenderSystems
However, the study also identified challenges, particularly in maintaining clarity for more complex
explanations,suchasthosebasedonknowledge-basedrecommendations.Thepaperhighlightstheneed
forfurtherresearchonLLMuseandintegratiionofexternalinformationintoLLM-generatedexplanations
tohandlemorespecializeddomainseffectively.
3.1.6 Instructing and Prompting Large Language Models for Explainable Cross-domain
Recommendations (Petruzzelli et al., 2024)
The paper explores using LLMs to generate explainable recommendations in cross-domain
recommendation tasks, focusing on both improving recommendation accuracy and producing natural
languageexplanations.Throughinstruction-tuningandpersonalizedprompting,LLMslikeGPT,LLaMa,
andMistral(Jiangetal.,2023)generatecontextualexplanationsalongsiderecommendeditems,offering
personalizedjustificationsbasedonusers’preferencesinasourcedomain.ThisstrategyallowstheLLM
to deliver coherent and relevant explanations, addressing transparency issues that commonly arise in
cross-domainsettings.
ExperimentalresultsshowthatLLM-producedexplanationsarenotonlyreadablebutalsocontextually
rich,revealinghowpastpreferencesinfluencenewrecommendations.Thestudyhighlightsthepotentialof
in-contextlearningtofurtherenhancetherelevanceandqualityofexplanations,demonstratingthatLLMs
canprovideintuitive,user-friendlyjustificationsthatbuildusertrustandunderstandingincross-domain
recommendations.
4 DISCUSSION
The reviewed papers collectively study how LLMs can be used to enhance the explainability of
recommender systems, revealing several insights into LLM-generated explanations’ impact on user
experience. Across the studies, there is a trend toward shifting from traditional, rigid explanation
methods—suchasfeature-basedorcollaborativefilteringexplanations—tomoreflexibleandpersonalized
naturallanguageexplanationsofferedbyLLMs.Silvaetal.(2024),forexample,showedthatChatGPT
improveduserengagementthroughhuman-centered,contextuallyrelevantexplanations.Similarly,Lubos
etal.(2024)foundthatusersgenerallypreferredLLM-generatedexplanationsfortheircreativityanddepth,
whichenhancedtrustandsatisfactioninrecommendations.
AcommonchallengeidentifiedwithLLM-generatedexplanationsisbalancingdetailwithclarity.While
detailed explanations can enrich user experience, as observed in Hendrawan et al. (2024), they can
sometimesoverwhelmusers,particularlyinspecializedorknowledge-intensivedomains.Thisunderscores
theimportanceofavoidingunnecessarycomplexityandmaintainingexplanationclarity.
ThepapersalsoexploretheuniquecapabilitiesofLLMsingeneratingcross-domainrecommendations.
Forexample,Petruzzellietal.(2024)demonstratedhowpersonalizedpromptingcanbeusedtogenerate
relevantexplanationsthatconnectuserpreferencesfromasourcedomaintorecommendationsinatarget
domain. This cross-domain adaptability highlights the potential of in-context learning to improve the
relevanceandqualityofexplanationsacrossdifferentrecommendationsettings,emphasizingtheversatility
ofLLMsinhandlingdiverserecommendationchallenges.
Animportantdistinctionmadeinseveralstudiesisbetweengeneratingexplanationsandjustifications.
LLMs, as noted by Park et al. (2023) and Guo et al. (2023), bypass traditional model-explainability
frameworks, andinsteadonproduceaccessiblejustificationsthatimproveperceivedtransparency. This
8Said LLM-basedExplanationsinRecommenderSystems
approachemphasizesperceived ratherthantechnicaltransparency,whichalignswiththegoalofenhancing
userexperiencebutmaylimitthedepthofintrospectionprovidedintothemodel’sworkings.
Lastly, the studies identify a need for evaluation methods and datasets tailored to LLM-generated
explanations(Guoetal.,2023).ThecontinuedintegrationofLLMsintorecommendersystemswilllikely
benefitfromsystematicassessmentmethodsofexplanationqualityandeffectivenessacrossvarieddomains
anduserneeds.
5 CONCLUSIONS
This review demonstrates the potential of LLMs to deliver user-friendly explanations in recommender
systems, offering a level of personalization and contextual richness that traditional approaches lack.
While these models excel at generating natural language justifications, they shift focus from technical
transparency to enhancing perceived transparency for users. As a result, LLMs contribute to a more
engagingandaccessibleexplainability,thoughtheydonotprovidethealgorithmicintrospectionthatformal
explanationframeworksoffer. Leveragingtheirlinguisticcapabilities,LLMsopennewpossibilitiesfor
personalizingandtailoringexplanations,notonlyintermsofwhichinsightstoincludebutalsoinhowthese
insightsareconveyedtoenhanceusersatisfaction.Furthermore,LLMsallowforincorporatingexternal
information—suchassustainabilityfactors—broadeningthescopeofexplanationstobetteralignwithuser
valuesandpreferences.
Lookingforward,onepossibledirectionistorefineLLM-generatedexplanationstoensuretheyremain
clearandconcise,especiallyincomplexrecommendationcontexts.Developingapproachesthatfilterand
summarizeinformationwithoutlosingrelevanceiscentralforavoidinguseroverload.Anotherpromising
research direction is combining LLMs with traditional explanation methods methods to provide hybrid
explanations that blend intuitive, human-readable justifications with local or global insights. This dual
approachcouldsupportbothcasualusersseekingquickunderstandingandmoretechnicallyinclinedusers
wantingadeeperexplanationoftherecommendationprocess.
REFERENCES
Abdollahi,B.andNasraoui,O.(2018). TransparencyinFairMachineLearning:theCaseofExplainable
Recommender Systems. In Human and Machine Learning: Visible, Explainable, Trustworthy and
Transparent,eds.J.ZhouandF.Chen(Cham:SpringerInternationalPublishing).21–35. doi:10.1007/
978-3-319-90403-0 2
Bauer, C., Zangerle, E., and Said, A. (2024). Exploring the Landscape of Recommender Systems
Evaluation: Practices and Perspectives. ACM Transactions on Recommender Systems 2, 11:1–11:31.
doi:10.1145/3629170
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,P.,etal.(2020). LanguageModels
are Few-Shot Learners. In Advances in Neural Information Processing Systems (Curran Associates,
Inc.),vol.33,1877–1901
Eslami, M., Vaccaro, K., Lee, M. K., Elazari Bar On, A., Gilbert, E., and Karahalios, K. (2019). User
AttitudestowardsAlgorithmicOpacityandTransparencyinOnlineReviewingPlatforms. InProceedings
ofthe2019CHIConferenceonHumanFactorsinComputingSystems(NewYork,NY,USA:Association
forComputingMachinery),CHI’19,1–14. doi:10.1145/3290605.3300724
Ge, Y., Liu, S., Fu, Z., Tan, J., Li, Z., Xu, S., et al. (2024). A Survey on Trustworthy Recommender
Systems. ACMTrans.Recomm.Syst.doi:10.1145/3652891. JustAccepted
9Said LLM-basedExplanationsinRecommenderSystems
Guo,S.,Zhang,S.,Sun,W.,Ren,P.,Chen,Z.,andRen,Z.(2023). TowardsExplainableConversational
RecommenderSystems. InProceedingsofthe46thInternationalACMSIGIRConferenceonResearch
andDevelopmentinInformationRetrieval(NewYork,NY,USA:AssociationforComputingMachinery),
SIGIR’23,2786–2795. doi:10.1145/3539618.3591884
Hayati,S.A.,Kang,D.,Zhu,Q.,Shi,W.,andYu,Z.(2020).INSPIRED:TowardSociableRecommendation
DialogSystems. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP),eds.B.Webber,T.Cohn,Y.He,andY.Liu(Online:AssociationforComputational
Linguistics),8142–8152. doi:10.18653/v1/2020.emnlp-main.654
Hendrawan,R.A.,Brusilovsky,P.,LekshmiNarayanan,A.B.,andBarria-Pineda,J.(2024). Explanations
in OpenUser Models for Personalized Information Exploration. InAdjunct Proceedings of the 32nd
ACMConferenceonUserModeling,AdaptationandPersonalization(NewYork,NY,USA:Association
forComputingMachinery),UMAPAdjunct’24,256–263. doi:10.1145/3631700.3665188
[Dataset]Jiang,A.Q.,Sablayrolles,A.,Mensch,A.,Bamford,C.,Chaplot,D.S.,delasCasas,D.,etal.
(2023). Mistral7b
Kitchenham,B.,Charters,S.,Budgen,D.,Brereton,P.,Turner,M.,Linkman,S.,etal.(2007). Guidelines
forperformingsystematicliteraturereviewsinsoftwareengineering. EBSETechnicalReportEBSE-
2007-01,version2.3,KeeleUniversityandUniversityofDurham
Li,R.,Kahou,S.,Schulz,H.,Michalski,V.,Charlin,L.,andPal,C.(2018). Towardsdeepconversational
recommendations. In Proceedings of the 32nd International Conference on Neural Information
ProcessingSystems(RedHook,NY,USA:CurranAssociatesInc.),NIPS’18,9748–9758
Lu,H.,Ma,W.,Wang,Y.,Zhang,M.,Wang,X.,Liu,Y.,etal.(2023). UserPerceptionofRecommendation
Explanation:AreYourExplanationsWhatUsersNeed? ACMTransactionsonInformationSystems41,
48:1–48:31. doi:10.1145/3565480
Lubos, S., Tran, T. N. T., Felfernig, A., Polat Erdeniz, S., and Le, V.-M. (2024). LLM-generated
Explanations for Recommender Systems. In Adjunct Proceedings of the 32nd ACM Conference on
User Modeling, Adaptation and Personalization (New York, NY, USA: Association for Computing
Machinery),UMAPAdjunct’24,276–285. doi:10.1145/3631700.3665185
Lundberg,S.M.andLee,S.-I.(2017). Aunifiedapproachtointerpretingmodelpredictions. InProceedings
ofthe31stInternationalConferenceonNeuralInformationProcessingSystems(RedHook,NY,USA:
CurranAssociatesInc.),NIPS’17,4768–4777
Park,J.,Kim,S.,andLee,S.(2023). AUserPreferenceandIntentExtractionFrameworkforExplainable
Conversational Recommender Systems. In Companion Proceedings of the 2023 ACM SIGCHI
Symposium on Engineering Interactive Computing Systems (New York, NY, USA: Association for
ComputingMachinery),EICS’23Companion,16–23. doi:10.1145/3596454.3597178
Petruzzelli,A.,Musto,C.,Laraspata,L.,Rinaldi,I.,deGemmis,M.,Lops,P.,etal.(2024). Instructingand
PromptingLargeLanguageModelsforExplainableCross-domainRecommendations. InProceedingsof
the18thACMConferenceonRecommenderSystems(NewYork,NY,USA:AssociationforComputing
Machinery),RecSys’24,298–308. doi:10.1145/3640457.3688137
Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.(2019). Languagemodelsare
unsupervisedmultitasklearners. OpenAIblog1,9
Raffel,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,etal.(2020). Exploringthelimitsof
transferlearningwithaunifiedtext-to-texttransformer. J.Mach.Learn.Res.21,140:5485–140:5551
Rahdari, B., Brusilovsky, P., and Babichenko, D. (2020). Personalizing Information Exploration with
an Open User Model. In Proceedings of the 31st ACM Conference on Hypertext and Social Media
10Said LLM-basedExplanationsinRecommenderSystems
(NewYork,NY,USA:AssociationforComputingMachinery),HT’20,167–176. doi:10.1145/3372923.
3404797
Ribeiro,M.T.,Singh,S.,andGuestrin,C.(2016). ”WhyShouldITrustYou?”:ExplainingthePredictions
ofAnyClassifier. InProceedingsofthe22ndACMSIGKDDInternationalConferenceonKnowledge
DiscoveryandDataMining(NewYork,NY,USA:AssociationforComputingMachinery),KDD’16,
1135–1144. doi:10.1145/2939672.2939778
Silva, I., Marinho, L. B., Said, A., and Willemsen, M. (2024). Leveraging ChatGPT for Automated
Human-centeredExplanationsinRecommenderSystems. InProceedingsofthe29thACMConference
onIntelligentUserInterfaces.(NewYork,NY,USA:ACM),IUI’24
Tintarev,N.andMasthoff,J.(2022). BeyondExplainingSingleItemRecommendations. InRecommender
SystemsHandbook,eds.F.Ricci,L.Rokach,andB.Shapira(NewYork,NY:SpringerUS).711–756.
doi:10.1007/978-1-0716-2197-4 19
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., et al. (2023). LLaMA:
OpenandEfficientFoundationLanguageModelsdoi:10.48550/arXiv.2302.13971
11