Unified PAC-Bayesian Study of Pessimism for Offline Policy Learning with
Regularized Importance Sampling
ImadAouali1,2 Victor-EmmanuelBrunel1 DavidRohde2 AnnaKorba1
1CREST,ENSAE,IPParis,France
2CriteoAILab,Paris,France
Abstract
In OPE, a significant portion of research has focused on
the inverse propensity scoring (IPS) estimator of the risk
[Horvitz and Thompson, 1952, Dudík et al., 2011]. IPS
Off-policylearning(OPL)ofteninvolvesminimiz- employsimportanceweights(IWs),whicharetheratiosbe-
ingariskestimatorbasedonimportanceweighting tweenthetargetpolicyandtheloggingpolicyusedtocollect
tocorrectbiasfromtheloggingpolicyusedtocol- data,toestimatetheriskofthetargetpolicy.AlthoughIPS
lectdata.However,thismethodcanproduceanes- isunbiasedundermildassumptions,itcansufferfromhigh
timatorwithahighvariance.Acommonsolutionis variance, especially when the target and logging policies
toregularizetheimportanceweightsandlearnthe differsignificantly[Swaminathanetal.,2017].Toaddress
policybyminimizinganestimatorwithpenalties thisissue,variousmethodshavebeendevelopedtoregular-
derivedfromgeneralizationboundsspecifictothe izeIPS,primarilybytransformingtheIWs[Bottouetal.,
estimator.Thisapproach,knownaspessimism,has 2013,SwaminathanandJoachims,2015a,Suetal.,2020,
gainedrecentattentionbutlacksaunifiedframe- Metellietal.,2021,Aoualietal.,2023a,Gabbianellietal.,
work for analysis. To address this gap, we intro- 2024].Whiletheseregularizationsintroducesomebias,they
duceacomprehensivePAC-Bayesianframework aim to reduce the estimator’s variance. Most of these IW
toexaminepessimismwithregularizedimportance regularizationshavebeenproposedandinvestigatedinthe
weighting. We derive a tractable PAC-Bayesian contextofOPE,wheretheprimarygoalistoenhancethe
generalization bound that universally applies to estimator’saccuracy,typicallymeasuredbymeansquared
common importance weight regularizations, en- error(MSE).Incontrast,off-policylearning(OPL)aimsto
ablingtheircomparisonwithinasingleframework. findapolicywithminimalrisk.Therefore,itiscrucialto
Our empirical results challenge common under- determinewhethertheseIWregularizationsleadtobetter
standing,demonstratingtheeffectivenessofstan- performanceinOPL.
dardIWregularizationtechniques.
AcommonapproachinOPListolearnthepolicythrough
pessimisticlearningprinciples[Jinetal.,2021],wherethe
estimatedriskisoptimizedalongwithapenaltytermoften
1 INTRODUCTION
derivedfromgeneralizationbounds.Consequently,previ-
ousstudiesonOPLwithregularizedIPSestimatorshave
Offlinecontextualbandits[Dudíketal.,2011]havegained
adoptedthisapproachbutfocusedonspecificIWregular-
significantinterestasaneffectiveframeworkforoptimizing
izations.Forexample,SwaminathanandJoachims[2015a]
decision-makingusingofflinedata.Inthisframework,an
studiedtheIPSestimatorwithclippedIWsandproposed
agentobservesacontext,takesanactionbasedonapolicy,
learningapolicybyminimizingtheestimatedriskpenalized
i.e., a probability distribution over a set of actions, and
withanempiricalvarianceterm.Similarly,LondonandSan-
receives a cost that depends on both the action and the
dler[2019]suggestedanalternativeregularizationforthe
context.Thesesequentialinteractions,recordedaslogged sameestimator,incorporatinganL distancetothelogging
2
data, serve two purposes in offline scenarios. The first is
policy. Additionally, Sakhi et al. [2023] derived tractable
off-policy evaluation (OPE), which aims to estimate the
generalizationboundsforasimplifieddoublyrobustversion
expectedcost(risk)ofafixedtargetpolicyusingthelogged
oftheIPSestimatorwithclippedIWs,usingthesebounds
data.Thesecondisoff-policylearning(OPL),whosegoal
for their pessimistic learning principle. Similarly, Aouali
istofindapolicythatminimizestherisk.Ingeneral,OPL
et al. [2023a] derived a tractable bound for an estimator
reliesonOPE’sriskestimator.
4202
nuJ
5
]GL.sc[
1v43430.6042:viXrathatexponentiallysmoothstheIWsinsteadofclippingthem aincontextx.Theexpectedcostofactionaincontextxis
andproposedtwolearningprinciples:onewherethebound givenbythecostfunctionc(x,a) = E [c].Using
c∼p(·|x,a)
is optimized and another heuristic inspired by it. Finally, analternativeterminology,costscanbedefinedasthenega-
Gabbianelli et al. [2024] introduced implicit exploration tiveofrewards:forany(x,a)∈X×A,c(x,a)=−r(x,a),
regularization,whereaconstantisaddedtothedenominator wherer :X×A→[0,1]istherewardfunction.Theseinter-
oftheIWsandusedalearningprinciplethatdirectlymini- actionsresultinann-sizedloggeddataS =(x ,a ,c ) ,
i i i i∈[n]
mizesthecorrespondingestimatorsincetheirgeneralization where(x ,a ,c )arei.i.dfromµ ,thejointdistributionof
i i i π
upperbounddidnotdependonthetargetpolicy. (x,a,c) defined as µ (x,a,c) = ν(x)π(a|x)p(c|x,a) for
π
any(x,a,c)∈X ×A×[−1,0].
A limitation of these studies is that their guarantees and
learningprinciplesarespecifictotheparticularIWregular- Agentsarerepresentedbystochasticpoliciesπ ∈Π,where
izationtheyconsiderandarenottransferabletootherIW Π denotes the space of policies. Specifically, for a given
regularizations. Consequently, in their OPL experiments, context x ∈ X, π(·|x) defines a probability distribution
IW regularizations are compared using different learning overtheactionspaceA.Then,theperformanceofapolicy
principles,makingitdifficulttodetermineifbetterperfor- π ∈Πismeasuredbytherisk,definedas
manceisduetotheenhancedpropertiesoftheproposedIW
R(π)=E [c(x,a)] . (1)
regularizerormerelyanartifactoftheproposedlearning x∼ν,a∼π(·|x)
principle.Asaresult,itremainsunclearwhetheraparticular
Givenapolicyπ ∈ΠandloggeddataS,thegoalofOPEis
IWregularizationyieldsbetterperformanceinOPL.This todesignanestimatorRˆ(π,S)forthetrueriskR(π)such
highlightsagapintheliterature:thereisnounifiedstudy thatRˆ(π,S)≈R(π).Leveragingthisestimator,OPLaims
providingboundsontheriskofpolicieslearnedusingpes-
tofindapolicyπˆ ∈ΠsuchthatR(πˆ )≈min R(π).
simisticlearningprinciplestailoredtovariousregularized n n π∈Π
We focus on the IPS estimator [Horvitz and Thompson,
IWestimatorsoftherisk.Ourworkaimstobridgethisgap.
1952],whichestimatestheriskR(π)byre-weightingsam-
Specifically,weprovideageneric,practicalgeneralization
plesusingtheratiobetweenπandπ
boundandanassociatedlearningprinciplethatapplyuni- 0
n
versallytoalargefamilyofIWregularizations,enablinga Rˆ (π,S)= 1 (cid:88) w(x ,a )c , (2)
faircomparisoninpracticeonOPLtasks. IPS n i i i
i=1
Thispaperisorganizedasfollows.Section2providesthe
whereforany(x,a)∈X ×A,w(x,a)=π(a|x)/π (a|x)
0
necessarybackgroundonIPSestimatorsandIWregulariza-
aretheimportanceweights(IWs).
tions.Section3reviewsrelatedwork,focusingontheguar-
anteesandlearningprinciplesfoundintheOPLliterature.
Section4presentsourPAC-Bayesiangeneralizationbounds 2.2 REGULARIZEDIMPORTANCEWEIGHTING
forregularizedIPSandintroducesourlearningprinciples
derived from these bounds. Finally, Section 5 compares The IPS estimator in (2) is unbiased when π 0(a|x) = 0
differentIWregularizationsonreal-worlddatasets. implies that π(a|x) = 0 for all (x,a) ∈ X × A. How-
ever,itsvariancescaleslinearlywiththeIWs[Swaminathan
etal.,2017]andcanbelargeifthetargetpolicyπ differs
2 BACKGROUND significantly from the logging policy π . To mitigate this
0
effect,itiscommontotransformtheIWsusingaregulariza-
2.1 OFFLINECONTEXTUALBANDITS tionfunctionthatintroducessomebiastoreducevariance.
Specifically,aregularizedIPSestimatorisdefinedas
An agent interacts with a contextual bandit environment
n
overnrounds.Inroundi∈[n],theagentobservesacontext Rˆ(π,S)= 1 (cid:88) wˆ(x ,a )c , (3)
x ∼ ν,whereν isadistributionwithsupportX ⊆ Rd,a n i i i
i i=1
d-dimensionalcompactcontext space.Theagentthense-
wherewˆ(x,a)aretheregularizedIWs.Examplesofwˆin-
lectsanactiona fromafiniteactionspaceA=[K].This
i clude clipping (Clip) [London and Sandler, 2019], ex-
actionissampledasa ∼π (·|x ),whereπ isthelogging
i 0 i 0 ponential smoothing (ES) [Aouali et al., 2023a], implicit
policyusedtocollectdata.Specifically,foragivencontext
exploration(IX)[Gabbianellietal.,2024],andharmonic
x, π (a|x) represents the probability that the agent takes
0 (Har)[Metellietal.,2021],definedas
actionaunderitscurrent(logging)policy.Finally,theagent
receives a stochastic cost1 c ∈ [−1,0] that depends on Clip: wˆ(x,a)= π(a|x) , τ ∈[0,1], (4)
i max(π0(a|x),τ)
theobservedcontextx i andtheactiona i.Precisely,c i ∼ ES: wˆ(x,a)= π(a|x) , α∈[0,1],
p(·|x i,a i),wherep(·|x,a)isthecostdistributionofaction π0(a|x)α
IX: wˆ(x,a)= π(a|x) , γ ∈[0,1],
π0(a|x)+γ
1Forsimplicity,weassumethatcostsc∈[−1,0],thoughthis
Har: wˆ(x,a)= w(x,a) , λ∈[0,1].
canbeeasilyextendedtoc∈[−C,0]forC >0. (1−λ)w(x,a)+λTheseregularizationsarelinearinπexceptHar.Othernon- Thisisthenusedtodefinethelearnedpolicyπˆ ∈Πas
n
linear regularizations have been proposed [Swaminathan πˆ =argminRˆ(π,S)+f(δ,Π,π,π ,n). (7)
n 0
and Joachims, 2015a, Su et al., 2020], but we will focus
π∈Π
ontheaboveexamplesbecausetheirhyperparametersfall
The issue with (6) is that it is a one-sided inequality that
withinthesamerange[0,1],facilitatingtheircomparison. doesnotattesttothequalityoftheestimatorRˆ.Toillustrate,
considerthatwithprobability1,R(π) ≤ RˆPOOR(π),using
apoorestimatoroftherisk,RˆPOOR(π) = 0foranyπ ∈ Π.
3 PESSIMISTICLEARNINGPRINCIPLES
Thisholdsbecause,bydefinition,R(π)∈[−1,0].However,
RˆPOOR isnotinformativeaboutR,makingitsminimization
Wenowdiscussthetheoreticalguaranteesandpessimistic
irrelevant.Thusweneedtocontrolthequalityoftheupper
learningprinciplespreviouslyderivedintheliterature.An
boundonR,whichisachievedbytwo-sidedinequalities
extendedrelatedworksectioncanbefoundinAppendixA.
P(cid:0) ∀π ∈Π,|R(π)−Rˆ(π,S)|≤f(δ,Π,π,π ,n)(cid:1) (8)
LetRˆbeanestimatoroftheriskR.InOPL,thegoalisto 0
≥1−δ.
minimizetheunknownriskRusingtheestimatorRˆ.Pes-
simisticlearningprinciplestypicallypenalizeRˆ,aimingto Here,thepessimisticlearningprinciplein(7)usesthefunc-
findπˆ =argmin Rˆ(π,S)+pen(π,S),withtheexpec- tion f from the two-sided inequality in (8). In particu-
n π∈Π
tationthatR(πˆ )≈min R(π).Thepenalizationterm lar, this allows us to derive high-probability inequalities
n π∈Π
pen(·,S)isderivedusingoneofthefollowingmethods. on the suboptimality (SO) gap of πˆ n, which is the dif-
ference R(πˆ ) − R(π ). Specifically, we can show that
n ∗
TheUseofEvaluationBounds.Metellietal.[2021]de-
R(πˆ ) − R(π ) ≤ 2f(δ,Π,π ,π ,n), where πˆ is the
rived evaluation bounds for the Har regularization in (4) n ∗ ∗ 0 n
learnedpolicyfrom(7)(withf obtainedfromthetwo-sided
andusedthemtoformulateapessimisticOPLlearningprin-
inequalityin(8))andπ =argmin R(π)istheoptimal
ciple.Specifically,theyshowedthatthefollowinginequality ∗ π∈Π
policy.Thisdemonstrateswhypessimismisappealingin
holdsforafixedtargetpolicyπ ∈Πandδ ∈(0,1)
OPL:thesuboptimalitygapofthelearnedpolicyπˆ ,i.e.,
n
P(cid:0)(cid:12) (cid:12)R(π)−Rˆ(π,S)(cid:12)
(cid:12)≤f(δ,π,π
0,n)(cid:1)
≥1−δ, (5)
R f( isπˆ n ev) a− luR at( eπ d∗ a) t, ti hs eb oo pu tn id me ad lb py ol2 icf y(δ π,Π ., Cπ o∗ n, sπ e0 q, un e) n, tlw y,h te hr ee
∗
risk estimator Rˆ needs to be precise only for the optimal
forsomefunctionf.Essentially,(5)indicatesthatforafixed
policy,ratherthanforallpolicieswithintheclassΠ.
policyπ ∈Π,theevent|R(π)−Rˆ(π,S)|≤f(δ,π,π ,n)
0
holdswithhighprobability.However,thiseventdependson TheUseofHeuristics.Manystudieshaveproposedspe-
thetargetpolicyπ.Thus(5)isusefulforevaluatingasingle cificheuristicswhereasimplifiedfunctiongisusedinstead
targetpolicywhenhavingaccesstomultipleloggeddata of the theoretical function f in (7). For example, Swami-
setsS.ThisposesaproblemforOPL,whereweoptimize nathanandJoachims[2015a]minimizedtheestimatedrisk
overapotentiallyinfinitespaceofpoliciesusingasingle while penalizing the empirical variance of the estimator.
loggeddatasetS.Thisisthefundamentaltheoreticallimita- Thisapproachwasinspiredbyageneralizationboundwith
tionofusingevaluationboundssimilarto(5)inOPL.While afunctionf thatincludesavariancetermbutdiscardsmore
itispossibletotransform(5)intoageneralizationbound complicated terms from the bound, such as the covering
thatsimultaneouslyholdsforanypolicyπ ∈ Πbyapply- numberofthepolicyspaceΠ.Similarly,LondonandSan-
ingaunionbound,thisapproachmayresultinintractable dler [2019] parameterized policies by a mean parameter
complexitytermsand,consequently,intractablepessimistic andproposedpenalizingtheestimatedriskbytheL 2 dis-
learningprinciples. tancebetweenthemeansoftheloggingandtargetpolicies,
discardingallothertermsfromtheirgeneralizationbound.
TheUseofOne-SidedGeneralizationBounds.Alterna-
Whiletheseheuristicsleadtotractableandcomputationally
tively,generalizationbounds[SwaminathanandJoachims,
attractive objectives, they often lack theoretical justifica-
2015a,LondonandSandler,2019,Sakhietal.,2023]ad-
tion and guarantees. We note that pessimistic principles
dress the limitations of evaluation bounds. These bounds
havebeenusedinadifferentcontextthanregularizedIPS
generallytakethefollowingform:forδ ∈(0,1),
estimators. For example, Wang et al. [2024] proposed a
heuristicapproachwherethestandard(non-regularized)IPS
P(cid:0) ∀π ∈Π,R(π)≤Rˆ(π,S)+f(δ,Π,π,π 0,n)(cid:1) (6) estimatorRˆ (π,S)in(2)ispenalizedwithapseudo-loss
IPS
≥1−δ, PL(π,S) = 1 (cid:80) (cid:80) π(a|xi) . Precisely, they de-
n i∈[n] a∈A π0(a|xi)
finedπˆ = argmin Rˆ (π,S)+βPL(π,S),whereβ
wherethefunctionf nowdependsonthespaceofpolicies n π∈Π IPS
isahyperparameter.Theyupperboundedthesuboptimality
Π.Thekeydifferencebetween(5)and(6)isthatherethe
gapoftheirπˆ foraspecifictheoreticalchoiceofβ.
eventR(π)≤Rˆ(π,S )+f(δ,Π,π,π ,n)holdswithhigh n
Π 0
probability for all target policies π. Since this is a high- TheUseofImplicitPessimism.Recently,Gabbianellietal.
probabilityevent,weassumeitholdsforourloggeddataS. [2024]proposedtheuseoftheIX-estimatorin(4)inOPLand demonstrated that, with careful analysis, they could 4 THEORETICALANALYSIS
obtaintightbounds.TheyobservedthattheIX-estimator
exhibitsasymmetryandthusdidnotuseasingletwo-sided We derive our PAC-Bayes generalization bound for the
inequality to derive their bound. Instead, they analyzed regularized IPS estimator Rˆ(π) in (3) under the assump-
each side individually using distinct methods and com- tion that wˆ(x,a) = g(π(a|x),π (a|x)) for any (x,a) ∈
0
binedtheresultstoobtainthedesiredtwo-sidedinequality. X ×A,whereg : [0,1]×[0,1] → R+.Thisassumption
Inparticular,thisallowedthemtoderiveanupperbound is broadly applicable and aligns with known IW regular-
function f that depends only on the policy space Π, con- izations. We make it to explicitly clarify the dependence
fidence level δ, and the number of samples n, such that on π(a|x) and purposefully exclude self-normalized IW,
(cid:80)
f(δ,Π,π,π ,n)=f(δ,Π,n).Thisledthemtodefine where wˆ(x ,a ) = nw(x ,a )/ w(x ,a ). In self-
0 i i i i j∈[n] j j
normalizedIW,theregularizationdependsnotonlyonthe
specificpairx ,a butalsoonallotherpairsx ,a ,which
i i j j
πˆ =argminRˆ(π,S)+f(δ,Π,π,π ,n), (9) isnotsupportedbyourtheory.
n 0
π∈Π
=argminRˆ(π,S)+f(δ,Π,n)=argminRˆ(π,S),
4.1 INTRODUCTIONTOPAC-BAYESTHEORY
π∈Π π∈Π
Considerlearningproblemsspecifiedbyaninstancespace
where the principle of pessimism becomes equivalent to denotedasZ,ahypothesisspaceHconsistingofpredictors
directlyminimizingtheestimatorsincef doesnotdepend h,andalossfunctionL:H×Z →R.Assumeaccesstoa
on π. This approach is appealing as it avoids computing datasetS =(z ) ,wherez ,...,z arei.i.d.fromanun-
i i∈[n] 1 n
potentially heavy statistics of the upper bound while still knowndistributionD.Theriskofahypothesishisdefined
enjoyingthebenefitsofpessimism.However,itrequiresa asR(h) = E z∼D[L(h,z)],whileitsempiricalcounterpart
carefulanalysisofthespecificIWregularization,whereas isdenotedasRˆ(h,S)= 1 (cid:80)n L(h,z ).
n i=1 i
weprovideagenericboundthatholdsforanyIWregular-
InPAC-Bayes,ourprimaryfocusistoexaminetheaverage
ization.FollowingAoualietal.[2023a],wedirectlyderive
generalizationcapabilitiesunderadistributionQonHby
two-sidedboundsforregularizedIPS,whichmightbeloose
controllingthedifferencebetweentheexpectedriskunder
dependingontheloggingpolicy(AppendixC.5)butstill
leadtogoodempiricalperformance(Section5).Investigat-
Q(expressedasE h∼Q[R(h)])andtheexpectedempirical
ingsimilarasymmetricanalysisingeneralregularizedIPS
riskunderQ(expressedasE h∼Q[Rˆ(h,S)]).
isaninterestingavenueforfuturework. AnexampleofPAC-Bayesgeneralizationboundsoriginally
proposedbyMcAllester[1998]isasfollows.Assumethat
OurApproach.Wederiveatwo-sidedgeneralizationbound
the values of L(h,z) ∈ [0,1] for any (h,z) ∈ H × Z,
that holds simultaneously for any policy π ∈ Π, as out-
and that we have a fixed prior distribution P on H and a
lined in (8). We examine two pessimistic learning princi-
parameterδthatfallswithin(0,1).Then,withaprobability
ples: directly optimizing the bound or optimizing a sim-
ofatleast1−δ overthesamplesetS drawnfromDn,it
plified penalty inspired by it (heuristic). Both principles
holdssimultaneouslyforanydistributionQonHthat
applytoanyIWregularization,includingthestandard,non-
r Be ag yu ela sir aiz ned boI uP nS d. sO inur Ath oe uo ar liy eb tu ail ld .s [2o 0n 23th a]e ,p er xo teo nf do inf gPa itc s- E h∼Q[R(h)]≤E h∼Q[Rˆ(h,S)]+(cid:113) DKL(Q∥P 2) n+log2√ δn ,
scopebeyondtheESregularizationin(4)toincludeother whereD denotestheKullback-Leiblerdivergence.The
KL
IWregularizations.Alimitationofthepreviousworkwas reader may refer to Alquier [2021] for a comprehensive
theempiricalcomparisonofdifferentpessimisticlearning introductiontoPAC-Bayestheory.
principles,eachemployingadifferentIWregularizationfor
IPS.Forexample,Aoualietal.[2023a]comparedoptimiz-
4.2 GENERALIZATIONBOUNDSFOROPL
ing ES-IPS penalized by their generalization bound with
optimizing Clip-IPS penalized by existing bounds (e.g.,
[Sakhietal.,2023,LondonandSandler,2019]).Although Let d′ be a positive integer, and let Θ ⊂ Rd′ be a d′-
they demonstrated significant improvements in OPL per- dimensional parameter space. We parametrize our learn-
formance with ES, they did not determine whether these ing policies as π θ, defining our space of policies as Π =
improvementswereduetothenewIWregularizationtech- {π θ;θ ∈ Θ}. An example of this is the softmax policy,
nique(ESvs.Clip)orthenewgeneralizationbound(their parameterizedasfollows
boundsvs.thoseinSakhietal.[2023],LondonandSandler exp(ϕ(x)⊤θ )
[2019]). This ambiguity motivates our development of a π θSOF(a|x)= (cid:80) exp(ϕ(x)a ⊤θ ), (10)
genericgeneralizationboundthatappliesuniversallytoany a′∈A a′
IWregularizationandalsoservesasthebasisforageneric where θ ∈ Rd and consequently θ = (θ ) ∈ RdK,
a a a∈A
heuristicinspiredbyit. with d′ = dK. Moreover, let Q be a distribution on theparameterspaceΘ.ThenPAC-Bayestheoryallowsusto isalsolinearinπ ,yielding
θ
(cid:12) (cid:12)
controlthequantity(cid:12) (cid:12)E θ∼Q[R(π θ)−Rˆ(π θ,S)](cid:12) (cid:12),where (cid:12)
(cid:12) (cid:12)E θ∼Q[R(π θ)−Rˆ(π
θ,S)](cid:12)
(cid:12)
(cid:12)=(cid:12)
(cid:12)
(cid:12)R(πQ)−Rˆ(πQ,S)(cid:12)
(cid:12)
(cid:12)
,
R(π )=E [c(x,a)],
θ x∼ν,a∼πθ(·|x) wherewedefine
n
Rˆ(π θ,S)= n1 (cid:88) wˆ θ(x i,a i)c i, πQ =E θ∼Q[π θ]. (12)
i=1 Thistechniqueiswidelyusedintheliterature[Londonand
withwˆ (x,a)=g(π (a|x),π (a|x)).Wealsoassumethat Sandler,2019,Sakhietal.,2023,Aoualietal.,2023a]be-
θ θ 0
thecostsaredeterministicforeaseofexposition.Thesame causeitallowstranslatingtheboundinTheorem1,which
(cid:12) (cid:12)
resultholdsforstochasticcosts.Theproofisprovidedin controls(cid:12) (cid:12)E θ∼Q[R(π θ)−Rˆ(π θ,S)](cid:12) (cid:12),intoaboundthatcon-
AppendixB. trols|R(πQ)−Rˆ(πQ,S)|,thequantityofinterestinOPL.
Theorem 1. Let λ > 0, n ≥ 1, δ ∈ (0,1), and let P ThemainrequirementistofindlinearIWregularizations
beafixedprioronΘ.Thefollowinginequalityholdswith and policies that satisfy (12). Fortunately, many IW reg-
probabilityatleast1−δforanydistributionQonΘ: ularizations, such as Clip, IX, and ES in (4), are linear
in π, and several practical policies adhere to the formu-
(cid:12) (cid:12)
(cid:12) (cid:12)E θ∼Q[R(π θ)−Rˆ(π θ,S)](cid:12)
(cid:12)
(11) lation in (12); refer to Aouali et al. [2023a, Section 4.2]
foranin-depthexplanationofsuchpolicies,includingsoft-
(cid:114)
≤
KL1(Q)
+
KL2(Q)
+B (Q)+
λ
V¯ (Q),
max,mixed-logit,andGaussianpolicies.Infact,Sakhietal.
2n nλ n 2 n [2023]demonstratedthatanypolicycanbewrittenas(12).
√
w Dh Ke Lr (e QK ∥L P1 )( +Q) log= 4 δ,aD nK dL(Q∥P) + log4 δn, KL2(Q) = I ren gC ulo ar ro izl ala tir oy n2 s, ow fte hesp foec rmial wi ˆz θe (xT ,h ae )or =em h(π1 πθ 0(u (a an | |xd x)e ))r ,l ain sse ua mrI inW g
h(π (a|x)) ≥ π (a|x)forany(x,a) ∈ X ×A.Addition-
0 0
V¯ n(Q)= n1 (cid:88)n E θ∼Q(cid:2)E a∼π0(·|xi)[wˆ θ(x i,a)2] {al 0l ,y 1, }w fe oa rs as nu ym (e xt ,h aa )t ∈π θ Xis ×bi Ana .ry In, m ote ha en rin wg orπ dθ s( ,a
π
θ| ix s) d∈
e-
i=1 terministic,allowingustouseπ (a|x)2 =π (a|x)forany
+wˆ θ(x i,a i)2c2 i(cid:3) , (x,a)∈X ×A.Essentially,theθ policiesπQdθ efinedin(12)
and B n(Q)= n1 (cid:88)n (cid:88) E θ∼Q(cid:2) |π θ(a|x i) c Qa .n Nb oe tv eie thw ae td tha is sa asm suix mtu pr te ioo nfd oe nte πr θm bin ei is nt gic bp io nl ai rc yie is su mnd ile dr .
i=1a∈A For instance, policies πQ that can be written as mixtures
(cid:3)
−π 0(a|x i)wˆ θ(x i,a)| . ofbinarypi
θ
includesoftmax,mixed-logit,andGaussian
policies [Aouali et al., 2023a, Section 4.2]. Under these
Generally,theboundistractableduetotheconditioningon assumptions,Theorem1yieldsthefollowingresult.
the contexts (x i) i∈[n], allowing us to bypass the need for Corollary2. AssumetheregularizedIWscanbewritten
computing the unknown expectation E x∼ν[·]. Recall that as wˆ (x,a) = πθ(a|x) with h : [0,1] → R+ verifies
the prior P is any fixed distribution over Θ. In particular, θ h(π0(a|x))
h(p)≥pforanyp∈[0,1].Moreover,foranydistribution
ifaθ existssuchthattheloggingpolicyπ = π ,then
P
can0
be specified as Gaussian with
mean0
θ
anθ d0
some
Q in the parameter space Θ, we define πQ = E θ∼Q[π θ]
0 whereπ isbinary.Then,letλ>0,n≥1,δ ∈(0,1),and
covariance.ThetermsKL1(Q)andKL2(Q)containthedi- letPbeθ
afixedprioronΘ,Thefollowinginequalityholds
vergence D (Q∥P), which penalizes posteriors Q that
KL withprobabilityatleast1−δforanydistributionQonΘ
deviatesignificantlyfromthepriorP.Thelattercanbecom-
p Mu ote rd eoi vn ec r,lo Bsed (- Qfo )r rm epi rf eb so enth tsP t, hQ eba ir ae sG ina tu ross di ua cn ef do br yin ts ht ean Ic We. (cid:12) (cid:12) (cid:12)R(πQ)−Rˆ(πQ,S)(cid:12) (cid:12)
(cid:12)
(13)
n (cid:114)
r wˆe θg (u xla ,r ai )za =tio wn (, xg ,iv ae )n (nc oon IWtex rt es g( ux lai) rii z∈ a[n ti] o; nB )n an(Q d) B= n(Q0w )>hen
0
≤ KL 21 n(Q) +B n(πQ)+ KL n2 λ(Q) + λ 2V¯ n(πQ),
otherwise.ThefirstterminV¯ (Q)resemblesthetheoretical
secondmomentofthereguln
arizedIWswˆ (x,a)(without
whereKL1(Q)andKL2(Q)aredefinedinTheorem1,and
θ
t th ere mco rs et s) ew mh be ln esvi te hw ee ed ma ps ir ra icn ad lo sm ecv oa nr dia mbl oes m,w enh til oe ft wh ˆe θ(s xec ,o an )d
c
V¯ n(πQ)= n1 (cid:88)n E a∼π0(·|xi)(cid:20) h(π πQ (( aa || xx i )) )2(cid:21)
(withthecost).IfV¯ (Q)isbounded(whichisthecasefor i=1 0 i
n
allIWr √egularizationsinSection √2.2exceptES),wecanset + πQ(a i|x i) c2,
λ=1/ n,resultinginaO(1/ n+B n(Q))bound. h(π 0(a i|x i))2 i
n
L linin eae rar inv πs. θ(N xo ,n a- )li (n i.e ea .,r gI lW ineR are ig nu il ta sr fiiz ra stti vo an r. iaI bf lewˆ )( ,x th, ea n) Ri ˆs B n(πQ)=1− n1 (cid:88) i=1a(cid:88) ∈Aπ 0(a|x i) h(π πQ 0( (a a| |x xi i) )).Thetermsintheaboveboundhavesimilarinterpretationsto pectationunderQ.Fortunately,thereparameterizationtrick
thoseinTheorem1.ThemainbenefitofCorollary2isthat [Kingmaet al.,2015] canbe usedinthis case.This trick
iteliminatestheneedfortheexpectationE θ∼Q[·],which allowsustoexpressagradientofanexpectationasanexpec-
isnowembeddedinthedefinitionofpoliciesin(12).For tationofagradient,whichcanthenbeestimatedusingthe
example,Corollary2allowsustorecoverthemainresult empiricalmean(MonteCarloapproximation).Inourcase,
ofESinAoualietal.[2023a]whenh(p)=pα,α∈[0,1]. we use the local reparameterization trick [Kingma et al.,
Similarly,wecanapplyittoIX[Gabbianellietal.,2024] 2015],knownforreducingthevarianceofstochasticgradi-
bysettingh(p)=p+γ,γ ≥0,andtoClip[Londonand ents.Specifically,weconsidersoftmaxpoliciesπSOF(a|x)
Sandler,2019]bysettingh(p)=max(p,τ),τ ∈[0,1]. in (10) and set Q = N (cid:0) µ,σ2I (cid:1) where µ ∈ Rθ dK and
dK
σ >0arelearnableparameters.Then,alltermsin(14)are
Finally,ifwˆ (x,a)isnotlinearinπ ,thenthistechnique
θ θ of the form E [f(πSOF(a|x))] for some func-
cannot be used, and the original expectation E θ∼Q[·] in tionf.Thesetθ e∼ rmN s(µ c, aσ n2I bdK e) rewritθ
tenas
Theorem1mustberetained.
Limitations. This bound has two main limitations. 1) E θ∼N(µ,σ2IdK)[f(π θSOF(a|x))]
Despite its broad applicability, directly applying Theo- =E (cid:104) f(cid:16) exp(ϕ(x)⊤µa+σϵa) (cid:17)(cid:105) .
rem 1 to bound the suboptimality gap of the learned ϵ∼N(0,∥ϕ(x)∥2 2IK) (cid:80) a′∈Aexp(ϕ(x)⊤µ a′+σϵ a′)
policy-specifically,toboundR(πˆ )−R(π ),whereπ =
n ∗ ∗
Thisexpectationisapproximatedbygeneratingi.i.d.sam-
argmin R(π) is the optimal policy and πˆ is learned
π∈Π n plesϵ ∼N(0,∥ϕ(x)∥2I )andcomputingthecorrespond-
by optimizing the bound-is not straightforward. To illus- i 2 K
ingempiricalmean.Thegradientsareapproximatedsimi-
trate,considerthelinearIWregularizationcaseinCorol-
larly(AppendixC.2).Unfortunately,thesetechniquescan
wla hry er2 ea Qn ∗ds =up ap ro gs met ih na Qt Rπ ∗ (πc Qa )n ib se thex ep ore ps tis med alas diπ s∗ tri= buπ tiQ o∗ n,
.
inducehighvariancewhenthenumberofactionsK islarge.
This can be mitigated by considering linear IW regular-
Inthisscenario,thesuboptimalitygapwouldbebounded
izations and optimizing the bound in Corollary 2. How-
by the upper bound in Corollary 2, evaluated at the op-
timal distribution Q = Q . However, the scaling of this ever,thistechniqueonlyworksforlinearIWregularizations.
∗
Therefore,weproposeanotherpracticallearningprinciple
suboptimalityboundwithnisnotimmediatelyevidentfor
inspiredbyourboundinTheorem1,whichenhancesper-
generalIWregularizersandrequiresindividualexamination
formanceatthecostofadditionalhyperparameters.
foreachIWregularization.Thisisbecausetheboundcon-
tainsnumerousempirical(data-dependent)termssuchas HeuristicOptimization.Thefollowingheuristicavoidsthe
B n(πQ ∗)andV¯ n(πQ ∗)thatarenoteasilytra √nsformedinto obstaclesofdirectlyoptimizingthebound,atthecostofin-
data-independent terms that scale as O(1/ n). Nonethe- troducingsomehyperparameters,whilestillbeinginspired
less,theversatility,tractability,andprovenempiricalbene- byTheorem1.Thisapproachinvolvesminimizingtheesti-
fitsofourbound(Section5)makeitappealing.2)Ithasbeen matedriskRˆ,penalizedbyitsassociatedbiasandvariance
notedthatdirectlyderivingtwo-sidedboundsforIWesti- termsfromTheorem1,alongwithaproximitytermtothe
matorsmightbeloosebecausetheytreatbothtailssimilarly, loggingpolicyπ =π suchas
0 θ0
whereaspriorwork[Gabbianellietal.,2024]indicateses-
sentialdifferencesbetweentheloweranduppertails,asseen Rˆ(π ,S)+λ ∥θ−θ ∥2+λ V˜ (π )+λ B˜ (π ), (15)
θ 1 0 2 n θ 3 n θ
in the IX-estimator [Gabbianelli et al., 2024]. This work
directlyderivestwo-sidedboundsforgeneralregularized whereV˜ (π )andB˜ (π )arethetermsinsidetheexpecta-
n θ n θ
IPS.Investigatingwhetherboundingeachsideindividually tionsinV¯ (Q)andB (Q),respectively,θ istheparameter
n n 0
couldleadtotermsthatareeasiertointerpretandsolvethe ofπ ,andλ ,λ ,λ aretunablehyperparameters.
0 1 2 3
aboveproblemisaninterestingdirectionforfutureresearch.
Both learning principles in (14) and (15) are suitable for
stochasticgradientdescent.Theyarealsogeneric,enabling
4.3 PESSIMISTICLEARNINGPRINCIPLES the comparison of different IW regularization techniques
given a fixed, observed logged data S. In Section 5, we
Theorem1yieldstwopessimisticlearningprinciples. willempiricallycomparethesetwolearningprinciplesand
evaluatetheeffectofdifferentIWregularizationtechniques.
BoundOptimization.First,onecandirectlylearnaπˆ that
n
optimizestheboundofTheorem1asfollows
4.4 SKETCHOFPROOFFORTHEMAINRESULT
(cid:114)
arg QmaxE θ∼Q(cid:104) Rˆ(π θ,S)(cid:105) + KL 21 n(Q) +B n(Q)
Our goal is to bound E
θ∼Q(cid:104)
R(π θ)−Rˆ(π
θ,S)(cid:105)
. To
+KL2(Q)
+
λ
V¯ (Q), (14) achievethis,wedecomposeitintothreetermsasfollows
nλ 2 n
Here,themainchallengeisthattheobjectiveinvolvesanex-
E θ∼Q[R(π θ)−Rˆ(π θ,S)]=I 1+I 2+I 3,Next,weexplainthetermsI ,I ,andI andtherationale 5.1 SETTING
1 2 3
fortheirintroduction.
First, I
1
= E θ∼Q(cid:2) R(π θ)− n1 (cid:80)n i=1R(π θ|x i)(cid:3) , where WeadoptasimilarsettingtoSakhietal.[2023].Webegin
R(π |x )=E [c(x ,a)],representstheriskgiven withasupervisedtrainingsetSTR andconvertitintologged
θ i a∼πθ(·|xi) i
contextx .Thistermcapturestheestimationerrorofthe banditdataS usingthestandardsupervised-to-banditcon-
i
empiricalmeanoftheriskusingni.i.d.contexts(x ) . versionmethod[Agarwaletal.,2014].Inthisconversion,
i i∈[n]
It is introduced to avoid the intractable expectation over thelabelsetAservesastheactionspace,whiletheinput
x ∼ ν, thereby leading to a tractable bound that can be spaceservesasthecontextspaceX.WethenuseS totrain
directlyusedinourpessimisticlearningprinciple. ourpolicies.Aftertraining,weevaluatetherewardofthe
(cid:104) (cid:105) learnedpoliciesonthesupervisedtestsetSTS.Theresulting
Second,I 2 = n1 (cid:80)n i=1E θ∼Q R(π θ|x i)−Rˆ(π θ|x i) ,with rewardmeasurestheabilityofthelearnedpolicytopredict
Rˆ(π |x ) = E [wˆ (x ,a)c(x ,a)],representsthe the true labels of the inputs in the test set and serves as
θ i a∼π0(·|xi) θ i i
expectationoftheriskestimatorgivenx .Thistermisabias ourperformancemetric.Weusetwoimageclassification
i
termconditionedonthecontexts(x ) ,anditsabsolute datasetsforthispurpose:MNIST[LeCunetal.,1998]and
i i∈[n]
valuecanbeboundedbytractableterms. FashionMNIST[Xiaoetal.,2017].Althoughwealsoex-
ploredtheEMNISTdataset,itledtosimilarconclusions,so
(cid:104) (cid:105)
Finally,I 3 = n1 (cid:80)n i=1E θ∼Q Rˆ(π θ|x i)−Rˆ(π θ,S) repre- wedidnotincludeittoreduceclutter.
sents the estimation error of the risk conditioned on the
Wedefinetheloggingpolicyasπ =πSOF asin(10),
contexts(x ) .Thisconditioningallowsustoavoidthe 0 η0·µ0
i i∈[n]
unknown expectation over x ∼ ν, making it possible to
bound|I |bytractableterms.
3
exp(η ϕ(x)⊤µ )
Thesetermsareboundedasfollows.1)Alquier[2021,The- πSOF (a|x)= 0 0,a , (16)
orem 3.3] allows bounding I
1
with high probability as
η0·µ0 (cid:80) a′∈Aexp(η 0ϕ(x)⊤µ 0,a′)
(cid:113)
|I | ≤
KL1(Q).
2) Using the fact that |c(x,a)| ≤ 1 for
1 2n
any(x,a)∈X ×A,|I |canbeboundedas|I |≤B (Q).
2 2 n
3)Bounding|I 3|ismorechallenging.Wemanagethisby where µ 0 = (µ 0,a) a∈A ∈ RdK are learned using 5%
expressingthetermusingmartingaledifferencesequences of the training set STR. The parameter η 0 ∈ R is an
andadapting[HaddoucheandGuedj,2022,Theorem2.1]. inverse-temperatureparameterthatcontrolsthequalityof
Let(F i) i∈{0}∪[n]beafiltrationadaptedto(S i) i∈[n]where theloggingpolicyπ 0.Highervaluesofη 0leadtoabetter-
S =(a ) foranyi∈[n].Thendefine performing logging policy, while lower values lead to a
i ℓ ℓ∈[i]
poorer-performingloggingpolicy.Inparticular,η =0cor-
0
f (a ,π )=E [wˆ (x ,a)c(x ,a)] respondstoauniformloggingpolicy.Wesettheprioras
i i θ a∼π0(·|xi) θ i i
−wˆ (x ,a )c(x ,a ).
P=N(η 0µ 0,I dK)inallPAC-Bayesianlearningprinciples
θ i i i i
consideredintheseexperiments,includingours.Wetrain
Weshowthatforanyθ ∈Θ,(f (a ,π )) formsamar- policiesontheremaining95%ofSTR usingAdam[Kingma
i i θ i∈[n]
tingaledifferencesequence,whichyields andBa,2014]withalearningrateof0.1for20epochs.The
training objective for learning the policy varies based on
|E θ∼Q[M n(θ)]|≤ KL2 λ(Q) +nλ 2V¯ n(Q), t oh ue rc ph roo ps oen sem de ht eh uo rd is: tiw ce inu (s 1e 5o ),u or rth oe tho er ret pi ec sa sl ib mo iu smnd lein ar( n1 in4 g),
principlesfoundintheliterature.
with high probability. Recognizing that E θ∼Q[M n(θ)] =
nI ,wederivethedesiredinequality Weconsidertwomainexperiments.InSection5.2,wefo-
3
cusonacommonIWregularizationtechnique,specifically
|I |≤ KL2(Q) + λ V¯ (Q). Clip in (4). We then apply PAC-Bayesian learning prin-
3 nλ 2 n ciples from the literature that were specifically designed
for Clip and compare them with ours applied to Clip.
Ourfinalboundisobtainedbycombiningthepreviousin-
Thegoalistodemonstratethatourlearningprinciplenot
equalitieson|I |,|I |,and|I |.
1 2 3
onlyhasbroaderapplicabilitybutalsooutperformsexist-
ingones.Aftervalidatingtheimprovedperformanceofour
5 EXPERIMENTS PAC-Bayesianlearningprinciple,weproceedinSection5.3
tocompareexistingIWregularizationsbytrainingpolicies
Wepresentourcoreexperimentsinthissection.Detailsand usingourlearningprinciplesappliedtothem.Thegoalof
additionalexperimentsareprovidedinAppendixC,where these experiments is to determine whether there is a par-
wealsodiscussthetightnessofourboundinAppendixC.5. ticular IW regularization technique that yields improved
OurcodeispubliclyavailableonGitHub. performanceinOPL.5.2 COMPARINGLEARNINGPRINCIPLES
MNIST, K=10, d=784 FashionMNIST, K=10, d=784
UNDERACOMMONIWREGULARIZATION 0.9 0.8
0.8 0.7
0.7 0.6
0.6
0.5
0.5
0.4
0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
inverse-temperature parameter ´0 inverse-temperature parameter ´0
Ours Sakhi et al. 1 Logging
Here,wefocusontheimpactofdifferentpessimisticlearn-
London et al. Sakhi et al. 2
ing principles on the performance of the learned policy
givenafixedIWregularizationmethod,specificallyClip Figure1:Performanceofthelearnedpolicywithdifferent
as defined in (4). Recall that Clip regularizes the IW as PAC-Bayespessimisticlearningprinciples(ourCorollary2
√
wˆ(x,a) = π(a|x) , with τ set to 1/4n following andthoseinLondonandSandler[2019],Sakhietal.[2023])
thesuggestiom nax in(π I0 o(a n| ix d) e,τ s) [2008].Toensureafaircompari- usingtheClipIPSriskestimatorin(4).
son,weconsiderPAC-Bayesianlearningprinciplesfromthe
literaturewherethetheoreticalboundwasoptimized.Specif-
ically,weincludetwoPAC-Bayesianboundsproposedprior 5.3 COMPARINGIWREGULARIZATIONS
to our work for Clip, from London and Sandler [2019] UNDERACOMMONLEARNINGPRINCIPLE
andSakhietal.[2023].WelabelthebaselinesasLondon
etal.foroptimizingtheboundfromLondonandSandler Afterdemonstratingthefavorableperformanceofourap-
[2019,Theorem1],andforSakhietal.[2023],wedistin- proachintheprevioussection,wenowevaluateitsperfor-
guishtheirtwoboundsasSakhietal.1(fromSakhietal. mancewithdifferentIWregularizationtechniques.Specif-
[2023,Proposition1],basedonCatoni[2007])andSakhiet
ically,weconsiderClip,Har,IX,andESasdefinedin
al.2(fromSakhietal.[2023,Proposition3],aBernstein- (4).Weemploybothlearningprinciples:onethatdirectly
type bound). Since both London and Sandler [2019] and optimizesthetheoreticalboundandanotherthatoptimizes
Sakhietal.[2023]usedthelinearIWregularizationtrick the heuristic derived from it. For the bound optimization,
describedinSection4,wecomparetheirmethodswithopti- we cannot use Corollary 2 since it includes a non-linear
mizingourboundin(2),adirectconsequenceofTheorem1
IWregularization(Har).Instead,weoptimizethebound
whentheIWregularizationislinearinπ.Sinceweuselin- in Theorem 1 as explained in the Bound Optimization
earIWregularizations,weoptimizeoverGaussianpolicies paragraphinSection4.3.Fortheheuristicoptimization,we
asdescribedinAoualietal.[2023a]andbrieflydiscussedin usethemethoddescribedintheHeuristicOptimization
AppendixC.3,astheseareknowntoperformbetterinthese paragraphinSection4.3.Inthiscontext,weoptimizeover
scenarios[Sakhietal.,2023,Aoualietal.,2023a].Finally, softmaxpoliciesdefinedas
wealsoincludetheloggingpolicyasabaseline.
exp(ϕ(x)⊤θ )
πSOF(a|x)= a , (17)
In Figure 1, the reward achieved by the learned policy is θ (cid:80) exp(ϕ(x)⊤θ )
a′∈A a′
plottedasafunctionofthequality(i.e.,performance)ofthe
loggingpolicy,η ∈ [0,1].Thiscomparisonisconducted where parameters θ are learned using either Bound Op-
0
for learned policies that were optimized using one of the timization (14) or Heuristic Optimization (15). Bound
pessimisticlearningprinciplesabove.Theresultsdemon- Optimizationinvolvesasinglehyperparameter,λ,asde-
stratethatoursoutperformsallbaselinesacrossawiderange finedinTheorem1.Wesetλtoitsoptimalvalue,λ ,which
∗
ofloggingpolicies.Thus,inadditiontobeinggenericand minimizes the bound with respect to λ. Our theory does
applicabletoalargefamilyofIWregularizers,ourapproach not support this approach since Theorem 1 requires λ to
provestobemoreeffectivethanobjectivestailoredforspe- be fixed in advance, whereas λ is data-dependent. How-
∗
cificIWregularizations.Theenhancedperformanceofour ever,wefoundthismethodtoyieldgoodempiricalperfor-
method holds when η is not very close to zero, a more mance.Ontheotherhand,HeuristicOptimizationrelies
0
realisticscenarioinpracticalsettingswheretheloggingpol- onthreehyperparameters,λ ,λ ,andλ ,whichwesetto
1 2 3
icy typically outperforms a uniform policy. Additionally, λ =λ =λ =10−5.
1 2 3
notethattheperformanceofthelearnedpolicyusingany
InFigures2and3,wepresenttherewardsofthelearned
method(includingours)improvesupontheperformanceof
policiesusingdifferentIWregularizationsasafunctionof
theloggingpolicy(indicatedbydashedblacklines).
thequalityoftheloggingpolicyπ ,basedonthetwopro-
0
Finally, we also conducted an experiment comparing our posedlearningprinciples:BoundOptimizationinFigure2
learning principle, Heuristic Optimization, with the L andHeuristicOptimizationinFigure3.Inbothfigures,the
2
heuristicfromLondonandSandler[2019].Wefoundthat firstandsecondrowscorrespondtoresultsonMNISTand
bothheuristicshadidenticalperformance(AppendixC.4). FashionMNIST, respectively. In the first four columns,
ycilop
denrael
eht
fo
drawerMNIST, K=10, d=784 MNIST, K=10, d=784 MNIST, K=10, d=784 MNIST, K=10, d=784 MNIST, K=10, d=784
0.9 0.8 0.8 0.8 0.9
0.8 Logging 0.7 Logging 0.7 Logging 0.7 Logging 0.8
0.7 Clip 0.6 Har 0.6 IX 0.6 ES 0.7
0.6 0.5 0.5 0.5 0.6 00 .. 45 0.4 0.4 0.4 00 .. 45
0.3 0.3 0.3 0.3 0.3
0.2 0.2 0.2 0.2 0.2
0.1 0.1 0.1 0.1 0.1
0.0 0.0 0.0 0.0 0.0
inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0
FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784
0.8 0.7 0.7 0.7 0.7
0.7 Logging 0.6 Logging 0.6 Logging 0.6 Logging 0.6
0.6 Clip 0.5 Har 0.5 IX 0.5 ES 0.5
0.5 0.4 0.4 0.4 0.4
0.4 0.3 0.3 0.3 0.3 0.3
0.2 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.1
0.0 0.0 0.0 0.0 0.0
−0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6
inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0
Figure2:PerformanceofthepolicylearnedbyBoundOptimization(14)fordifferentIWregularizations.Thex-axis
reflectsthequalityoftheloggingpolicyη ∈[−0.5,0.5].Inthefirstfourcolumns,weplottherewardofthelearnedpolicy
0
usingafixedIWregularizationtechnique(Clip,Har,IX,orESasdefinedin(4))forvariousvaluesofitshyperparameter
within[0,1].Inthelastcolumn,wereportthemeanrewardacrossthesehyperparametervalues.
MNIST, K=10, d=784 MNIST, K=10, d=784 MNIST, K=10, d=784 MNIST, K=10, d=784 MNIST, K=10, d=784
0.9 0.9 0.9 0.9 0.9
0.8 Logging 0.8 Logging 0.8 Logging 0.8 Logging 0.8
0.7 Clip 0.7 Har 0.7 IX 0.7 ES 0.7
0.6 0.6 0.6 0.6 0.6 0.5 0.5 0.5 0.5 0.5
0.4 0.4 0.4 0.4 0.4
0.3 0.3 0.3 0.3 0.3
0.2 0.2 0.2 0.2 0.2
0.1 0.1 0.1 0.1 0.1
0.0 0.0 0.0 0.0 0.0
inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0
FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784
0.8 0.8 0.8 0.8 0.8
0.7 Logging 0.7 Logging 0.7 Logging 0.7 Logging 0.7
0.6 Clip 0.6 Har 0.6 IX 0.6 ES 0.6
0.5 0.5 0.5 0.5 0.5
0.4 0.4 0.4 0.4 0.4
0.3 0.3 0.3 0.3 0.3
0.2 0.2 0.2 0.2 0.2
0.1 0.1 0.1 0.1 0.1
0.0 0.0 0.0 0.0 0.0
−0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6
inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0
Figure3:PerformanceofthepolicylearnedbyHeuristicOptimization(15)fordifferentIWregularizations.Thex-axis
reflectsthequalityoftheloggingpolicyη ∈[−0.5,0.5].Inthefirstfourcolumns,weplottherewardofthelearnedpolicy
0
usingafixedIWregularizationtechnique(Clip,Har,IX,orESasdefinedin(4))forvariousvaluesofitshyperparameter
within[0,1].Inthelastcolumn,wereportthemeanrewardacrossthesehyperparametervalues.
weplottherewardofthelearnedpolicyusingafixedIW provement.Overall,Clip,IX,andESachievecomparable
regularizationtechnique(Clip,Har,IX,orESasdefined performances,assummarizedinthefar-rightcolumn,de-
in(4))forvariousvaluesofitshyperparameterwithin[0,1]. spiteregularizingIWsinverydifferentways.Ontheone
Inthelastcolumn,wereportthemeanrewardacrossthese hand,theseresultsalignwiththegeneralityofourbound,
hyperparameter values to assess the sensitivity of the IW whichappliestoalltheseIWregularizations.Ontheother
regularizationtechniquetoitshyperparameter.Thex-axis hand,theysuggestthatonecanchooseanyIWregulariza-
representsη ,whichcontrolsthequalityoftheloggingpol- tion method when learning the policy by optimizing the
0
icy;highervaluesindicatebetterperformanceofthelogging theoreticalboundwithoutriskingunderperformance.
policy.Wevaryη ∈ [−0.5,0.5]toconsiderloggingpoli-
0 Theseresultsandconclusionsarefurtherconfirmedbythe
cies that perform worse than the uniform one (i.e., when
rewardsreportedinFigure3,wherethepoliciesarelearned
η < 0),tohighlightsettingsthatmightrequiremoreIW
0 throughHeuristicOptimization(15).Theperformances
regularization,althoughsuchscenariosmaynotberealistic.
are even better than those obtained when optimizing the
Ourresultsleadtothefollowingconclusions.InFigure2, theoreticalbound.AsdiscussedinSection4.3,thismaybe
weobservethatallregularizationsresultinimprovedper- duetothepracticaloptimizationofthetheoreticalbound,
formanceovertheloggingpolicy(i.e.,alllinesareabove whereweusedMonteCarlotoestimatetheexpectations,
the dashed line representing the performance of the log- whichperformspoorlyinhigh-dimensionalproblems.How-
gingpolicy),withtheHarregularizationshowinglessim- ever,optimizingtheheuristicortheoreticalboundleadsto
drawer
drawer
drawer
drawer
sretemaraprepyh
.t.r.w
drawer
.vAsretemaraprepyh
.t.r.w
drawer
.vA
sretemaraprepyh
.t.r.w
drawer
.vAsretemaraprepyh
.t.r.w
drawer
.vAsimilar performance when the IW regularizers are linear References
(AppendixC.3)since,inthatcase,theMonteCarloestima-
tionwasimproved.Moreover,assummarizedinthefar-right AlekhAgarwal,DanielHsu,SatyenKale,JohnLangford,
columnofFigure3,theaverageperformancesforallregu- Lihong Li, and Robert Schapire. Taming the monster:
larizationsarecomparable,exceptforHar,whichisbelow A fast and simple algorithm for contextual bandits. In
theothers,andES,whichperformsslightlybetter.Notably, International Conference on Machine Learning, pages
forallregularizations,thereisatleastonechoiceofregular- 1638–1646.PMLR,2014.
izationhyperparameterthatachievesoptimalperformance.
This finding diverges from Aouali et al. [2023a], who at- Pierre Alquier. User-friendly introduction to PAC-Bayes
tributedsignificantperformanceimprovementstoESina bounds. arXivpreprintarXiv:2110.11216,2021.
similarsettingtoours.Ourresultsclarifythatthesegains
ImadAouali.Diffusionmodelsmeetcontextualbanditswith
maybeduetotheirnewlyintroducedpessimisticlearning
large action spaces. arXiv preprint arXiv:2402.10028,
principleratherthantheirsmoothIWregularization(ES).
2024.
ImadAouali,AmineBenhalloum,MartinBompaire,Achraf
AitSidiHammou,SergeyIvanov,BenjaminHeymann,
DavidRohde,OtmaneSakhi,FlavianVasile,andMaxime
Vono. Rewardoptimizingrecommendationusingdeep
6 CONCLUSION learning and fast maximum inner product search. In
proceedings of the 28th ACM SIGKDD conference on
knowledgediscoveryanddatamining,pages4772–4773,
Inthispaper,wepresentthefirstcomprehensiveanalysis 2022a.
ofestimatorsthatrelyonIWregularizationtechniques,an
increasinglypopularapproachinOPEandOPL.Ourresults ImadAouali,AchrafAitSidiHammou,SergeyIvanov,Ot-
hold for a broad spectrum of IW regularization methods mane Sakhi, David Rohde, and Flavian Vasile. Proba-
andarealsoapplicabletothestandardIPSwithoutanyIW bilisticrankandreward:Ascalablemodelforslaterec-
regularization.Fromourtheoreticalfindings,wederivetwo ommendation. arXivpreprintarXiv:2208.06263,2022b.
learningprinciplesthatapplyacrossvariousIWregulariza-
ImadAouali,Victor-EmmanuelBrunel,DavidRohde,and
tions. Our results suggest that despite the numerous pro-
AnnaKorba. Exponentialsmoothingforoff-policylearn-
posedIWregularizationtechniquesforOPE,conventional
ing. InProceedingsofthe40thInternationalConference
methodslikeclipping,stillperformverywellinOPL.
onMachineLearning,pages984–1017.PMLR,2023a.
Nevertheless,ourworkhasthreeprimarylimitations.First,
ourboundincludesempiricalbiasandvarianceterms,mak- Imad Aouali, Branislav Kveton, and Sumeet Katariya.
ingitchallengingtoderivedata-independentsuboptimality Mixed-effectThompsonsampling. InInternationalCon-
gaps,asdiscussedinSection4.2.Additionally,two-sided ference on Artificial Intelligence and Statistics, pages
boundsforregularizedIPScanbeloosebecausetheytreat 2087–2115.PMLR,2023b.
both tails similarly, whereas some studies indicate differ-
ImadAouali,Victor-EmmanuelBrunel,DavidRohde,and
encesbetweentheloweranduppertailsofsuchestimators.
AnnaKorba. Bayesianoff-policyevaluationandlearning
Investigatingmethodstoprovegenericboundsbytreating
forlargeactionspaces. arXivpreprintarXiv:2402.14664,
eachsideoftheinequalityindividually,asinGabbianelli
2024.
et al. [2024], could address this issue and is left for fu-
tureresearch.Second,optimizingourboundreliesonthe
PeterAuer,NicoloCesa-Bianchi,andPaulFischer. Finite-
reparametrizationtrickandMonteCarloestimation,which
timeanalysisofthemultiarmedbanditproblem. Machine
mayhavelimitationsinhigh-dimensionalproblems.Also,
Learning,47:235–256,2002.
ourreparametrizationtrickwasonlyappliedtosimplelinear-
softmaxpoliciesdefinedin(10).Therefore,exploringmore Heejung Bang and James M Robins. Doubly robust es-
advancedtechniquesforoptimizingourtheoreticalbound timation in missing data and causal inference models.
presentsanintriguingdirectionforfutureresearch.While Biometrics,61(4):962–973,2005.
this limitation can be mitigated by considering linear IW
regularizationtechniques,asdiscussedinCorollary2,there Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela,
ispotentialforbetterpracticaloptimizationoftheboundfor Denis X Charles, D Max Chickering, Elon Portugaly,
non-linearIWregularizations.Finally,extendingourexper- DipankarRay,PatriceSimard,andEdSnelson. Counter-
imentstomorecomplexpoliciesandchallengingsettings, factualreasoningandlearningsystems:Theexampleof
such as recommender systems with large action spaces, computationaladvertising. JournalofMachineLearning
couldfurtherhighlighttheimpactofIWregularization. Research,14(11),2013.O Catoni. PAC-Bayesian supervised classification: The EdwardLIonides. Truncatedimportancesampling. Journal
thermodynamicsofstatisticallearning.instituteofmath- of Computational and Graphical Statistics, 17(2):295–
ematicalstatisticslecturenotes—monographseries56. 311,2008.
IMS,Beachwood,OH.MR2483528,5544465,2007.
OlivierJeunenandBartGoethals. Pessimisticrewardmod-
WeiChu,LihongLi,LevReyzin,andRobertSchapire.Con- els for off-policy learning in recommendation. In Fif-
textualbanditswithlinearpayofffunctions. InProceed- teenthACMConferenceonRecommenderSystems,pages
ings of the 14th International Conference on Artificial 63–74,2021.
IntelligenceandStatistics,pages208–214,2011.
YingJin,ZhuoranYang,andZhaoranWang. Ispessimism
MiroslavDudík,JohnLangford,andLihongLi. Doublyro-
provablyefficientforofflineRL? InInternationalCon-
bustpolicyevaluationandlearning. InProceedingsofthe
ferenceonMachineLearning,pages5084–5096.PMLR,
28thInternationalConferenceonInternationalConfer-
2021.
enceonMachineLearning,ICML’11,page1097–1104,
2011. NathanKallus,YutaSaito,andMasatoshiUehara. Optimal
off-policyevaluationfrommultipleloggingpolicies. In
Miroslav Dudík, Dumitru Erhan, John Langford, and Li-
International Conference on Machine Learning, pages
hongLi. Sample-efficientnonstationarypolicyevalua-
5247–5256.PMLR,2021.
tionforcontextualbandits. InProceedingsoftheTwenty-
Eighth Conference on Uncertainty in Artificial Intelli-
DiederikPKingmaandJimmyBa. Adam:Amethodfor
gence,UAI’12,page247–254,Arlington,Virginia,USA,
stochasticoptimization. arXivpreprintarXiv:1412.6980,
2012.AUAIPress.
2014.
MiroslavDudik,DumitruErhan,JohnLangford,andLihong
Durk P Kingma, Tim Salimans, and Max Welling. Vari-
Li. Doubly robust policy evaluation and optimization.
ational dropout and the local reparameterization trick.
StatisticalScience,29(4):485–511,2014.
Advancesinneuralinformationprocessingsystems,28,
Mehrdad Farajtabar, Yinlam Chow, and Mohammad 2015.
Ghavamzadeh.Morerobustdoublyrobustoff-policyeval-
uation.InInternationalConferenceonMachineLearning, IljaKuzborskij,ClaireVernade,AndrasGyorgy,andCsaba
pages1447–1456.PMLR,2018. Szepesvári. Confident off-policy evaluation and selec-
tionthroughself-normalizedimportanceweighting. In
Louis Faury, Ugo Tanielian, Elvis Dohmatob, Elena InternationalConferenceonArtificialIntelligenceand
Smirnova, and Flavian Vasile. Distributionally robust Statistics,pages640–648.PMLR,2021.
counterfactualriskminimization. InProceedingsofthe
AAAIConferenceonArtificialIntelligence,pages3850– Tor Lattimore and Csaba Szepesvari. Bandit Algorithms.
3857,2020. CambridgeUniversityPress,2019.
Germano Gabbianelli, Gergely Neu, and Matteo Papini. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Importance-weightedofflinelearningdoneright. InIn- Haffner. Gradient-based learning applied to document
ternationalConferenceonAlgorithmicLearningTheory, recognition.ProceedingsoftheIEEE,86(11):2278–2324,
pages614–634.PMLR,2024. 1998.
AlexandreGilotte,ClémentCalauzènes,ThomasNedelec,
LihongLi,WeiChu,JohnLangford,andRobertSchapire.A
AlexandreAbraham,andSimonDollé. OfflineA/Btest-
contextual-banditapproachtopersonalizednewsarticle
ing for recommender systems. In Proceedings of the
recommendation. In Proceedings of the 19th Interna-
EleventhACMInternationalConferenceonWebSearch
tionalConferenceonWorldWideWeb,2010.
andDataMining,pages198–206,2018.
BenLondonandTedSandler. Bayesiancounterfactualrisk
Maxime Haddouche and Benjamin Guedj. PAC-Bayes
minimization. InInternationalConferenceonMachine
withunboundedlossesthroughsupermartingales. arXiv
Learning,pages4125–4133.PMLR,2019.
preprintarXiv:2210.00928,2022.
Joey Hong, Branislav Kveton, Sumeet Katariya, Manzil David A McAllester. Some PAC-Bayesian theorems. In
Zaheer,andMohammadGhavamzadeh. Deephierarchy ProceedingsoftheeleventhannualconferenceonCom-
inbandits. arXivpreprintarXiv:2202.01454,2022. putationallearningtheory,pages230–234,1998.
Daniel G Horvitz and Donovan J Thompson. A general- AlbertoMariaMetelli,AlessioRusso,andMarcelloRestelli.
ization of sampling without replacement from a finite Subgaussiananddifferentiableimportancesamplingfor
universe. JournaloftheAmericanstatisticalAssociation, off-policyevaluationandlearning. AdvancesinNeural
47(260):663–685,1952. InformationProcessingSystems,34:8119–8132,2021.James M Robins and Andrea Rotnitzky. Semiparametric WilliamR.Thompson. Onthelikelihoodthatoneunknown
efficiencyinmultivariateregressionmodelswithmissing probability exceeds another in view of the evidence of
data. JournaloftheAmericanStatisticalAssociation,90 twosamples. Biometrika,25(3-4):285–294,1933.
(429):122–129,1995.
Lequn Wang, Akshay Krishnamurthy, and Alex Slivkins.
DanielRusso,BenjaminVan Roy,AbbasKazerouni,Ian Oracle-efficientpessimism:Offlinepolicyoptimization
Osband,andZhengWen. AtutorialonThompsonsam- incontextualbandits. InInternationalConferenceonAr-
pling. FoundationsandTrendsinMachineLearning,11 tificialIntelligenceandStatistics,pages766–774.PMLR,
(1):1–96,2018. 2024.
NoveenSachdeva,YiSu,andThorstenJoachims.Off-policy
Yu-XiangWang,AlekhAgarwal,andMiroslavDudık. Op-
banditswithdeficientsupport. InProceedingsofthe26th
timal and adaptive off-policy evaluation in contextual
ACMSIGKDDInternationalConferenceonKnowledge
bandits. InInternationalConferenceonMachineLearn-
Discovery&DataMining,pages965–975,2020.
ing,pages3589–3597.PMLR,2017.
YutaSaitoandThorstenJoachims. Off-policyevaluation
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-
forlargeactionspacesviaembeddings. arXivpreprint
MNIST:anovelimagedatasetforbenchmarkingmachine
arXiv:2202.06317,2022.
learningalgorithms. arXivpreprintarXiv:1708.07747,
Yuta Saito, Qingyang Ren, and Thorsten Joachims. Off- 2017.
policy evaluation for large action spaces via conjunct
Houssam Zenati, Alberto Bietti, Matthieu Martin, Eu-
effectmodeling. IninternationalconferenceonMachine
stacheDiemert,andJulienMairal. Counterfactuallearn-
learning,pages29734–29759.PMLR,2023.
ing of continuous stochastic policies. arXiv preprint
OtmaneSakhi,StephenBonner,DavidRohde,andFlavian arXiv:2004.11722,2020.
Vasile.BLOB:Aprobabilisticmodelforrecommendation
Yinglun Zhu, Dylan J Foster, John Langford, and Paul
thatcombinesorganicandbanditsignals. InProceedings
Mineiro. Contextual bandits with large action spaces:
ofthe26thACMSIGKDDInternationalConferenceon
Madepractical. InInternationalConferenceonMachine
KnowledgeDiscovery&DataMining,pages783–793,
Learning,pages27428–27453.PMLR,2022.
2020.
OtmaneSakhi,PierreAlquier,andNicolasChopin. PAC- ShiZong,HaoNi,KennySung,NanRosemaryKe,Zheng
Bayesianofflinecontextualbanditswithguarantees. In Wen, and Branislav Kveton. Cascading bandits for
International Conference on Machine Learning, pages large-scale recommendation problems. arXiv preprint
29777–29799.PMLR,2023. arXiv:1603.05359,2016.
YiSu,LequnWang,MicheleSantacatterina,andThorsten
Joachims. CAB:Continuousadaptiveblendingforpolicy
evaluationandlearning. InInternationalConferenceon
MachineLearning,pages6005–6014.PMLR,2019.
YiSu,MariaDimakopoulou,AkshayKrishnamurthy,and
Miroslav Dudík. Doubly robust off-policy evaluation
withshrinkage. InInternationalConferenceonMachine
Learning,pages9167–9176.PMLR,2020.
AdithSwaminathanandThorstenJoachims. Batchlearn-
ingfromloggedbanditfeedbackthroughcounterfactual
risk minimization. The Journal of Machine Learning
Research,16(1):1731–1755,2015a.
Adith Swaminathan and Thorsten Joachims. The self-
normalized estimator for counterfactual learning. ad-
vances in neural information processing systems, 28,
2015b.
AdithSwaminathan,AkshayKrishnamurthy,AlekhAgar-
wal,MiroDudik,JohnLangford,DamienJose,andImed
Zitouni. Off-policyevaluationforslaterecommendation.
AdvancesinNeuralInformationProcessingSystems,30,
2017.Unified PAC-Bayesian Study of Pessimism for Offline Policy Learning with
Regularized Importance Sampling
(Supplementary Material)
ImadAouali1,2 Victor-EmmanuelBrunel1 DavidRohde2 AnnaKorba1
1CREST,ENSAE,IPParis,France
2CriteoAILab,Paris,France
Thesupplementarymaterialisorganizedasfollows.
• AppendixAincludesanextendedrelatedworkdiscussion.
• AppendixBoutlinestheproofsofourmainresults,aswellasadditionaldiscussions.
• AppendixCpresentsourexperimentalsetupforreproducibility,alongwithsupplementaryexperiments.
A EXTENDEDRELATEDWORK
Theframeworkofcontextualbanditsisawidelyadoptedmodelforaddressingonlinelearninginuncertainenvironments
[LattimoreandSzepesvari,2019,Aueretal.,2002,Thompson,1933,Russoetal.,2018,Lietal.,2010,Chuetal.,2011].
Thisframeworknaturallyalignswiththeonlinelearningparadigm,whichseekstoadaptinreal-time.However,inpractical
scenarios,challengesarisewhendealingwithalargeactionspace.Whilenumerousonlinealgorithmshaveemergedto
efficientlynavigatethelargeactionspacesincontextualbandits[Zongetal.,2016,Hongetal.,2022,Zhuetal.,2022,Aouali
etal.,2023b,Aouali,2024],anotableneedremainsforofflinemethodsthatenabletheoptimizationofdecision-making
basedonhistoricaldata.Fortunately,weoftenpossesslargesamplesetssummarizinghistoricalinteractionswithcontextual
banditenvironments.Leveragingthis,agentscanenhancetheirpoliciesoffline[SwaminathanandJoachims,2015a,London
andSandler,2019,Sakhietal.,2023,Aoualietal.,2023a].Thisstudyisprimarilydedicatedtoexploringthisofflinemode
ofcontextualbandits,oftenreferredtoastheoff-policyformulation[Dudíketal.,2011,2012,Dudiketal.,2014,Wangetal.,
2017,Farajtabaretal.,2018].Off-policycontextualbanditsentailtwoprimarytasks.Thefirsttask,knownasoff-policy
evaluation(OPE),revolvesaroundestimatingpolicyperformanceusinghistoricaldata.Thisestimationreplicateshow
evaluationswouldunfoldasifthepolicyisengagingwiththeenvironmentinreal-time.Subsequently,thederivedestimator
isoptimizedtofindtheoptimalpolicy,andthisiscalledoff-policylearning(OPL)[SwaminathanandJoachims,2015a].
Next,wereviewbothOPEandOPL.
A.1 OFF-POLICYEVALUATION
Inrecentyears,OPEhasexperiencedanoticeablesurgeofinterest,withnumeroussignificantcontributions[Dudíketal.,
2011,2012,Dudiketal.,2014,Wangetal.,2017,Farajtabaretal.,2018,Suetal.,2019,2020,Kallusetal.,2021,Metelli
etal.,2021,Kuzborskijetal.,2021,SaitoandJoachims,2022,Sakhietal.,2020,JeunenandGoethals,2021,Saitoetal.,
2023].TheliteratureonOPEcanbebroadlyclassifiedintothreeprimaryapproaches.Thefirst,referredtoasthedirect
method(DM)[JeunenandGoethals,2021,Aoualietal.,2024],involvesthedevelopmentofamodeldesignedtoapproximate
expectedcostsforanycontext-actionpair.Thismodelissubsequentlyemployedtoestimatetheperformanceofthepolicies.
Thisapproachisoftenusedinlarge-scalerecommendersystems[Sakhietal.,2020,JeunenandGoethals,2021,Aouali
etal.,2022b,a].Thesecondapproach,knownasinversepropensityscoring(IPS)[HorvitzandThompson,1952,Dudíketal.,
2012],aimstoestimatethecostsassociatedwiththeevaluatedpoliciesbycorrectingfortheinherentpreferencebiasofthe
loggingpolicywithinthesampledataset.WhileIPSmaintainsitsunbiasednaturewhenoperatingundertheassumption
thattheevaluationpolicyisabsolutelycontinuousconcerningtheloggingpolicy,itcanbesusceptibletohighvarianceandsubstantialbiaswhenthisassumptionisviolated[Sachdevaetal.,2020].Inresponsetothevarianceissue,various
techniqueshavebeenintroduced,includingtheclippingofimportanceweights[Ionides,2008,SwaminathanandJoachims,
2015a],theirsmoothing[Aoualietal.,2023a],andself-normalization[SwaminathanandJoachims,2015b],amongothers
[Gilotteetal.,2018].Thethirdapproach,knownasdoublyrobust(DR)[RobinsandRotnitzky,1995,BangandRobins,
2005,Dudíketal.,2011,Dudiketal.,2014,Farajtabaretal.,2018],combineselementsfromboththedirectmethod(DM)
andinversepropensityscoring(IPS).Thisamalgamationservestoreducevarianceintheestimationprocess.Typically,the
accuracyofanOPEestimatorRˆ(π,S),isassessedusingthemeansquarederror(MSE).It’sworthmentioningthatMetelli
etal.[2021]advocateforthepreferenceofhigh-probabilityconcentrationratesasthefavoredmetricforevaluatingOPE
estimators.ThisworkfocusesprimarilyonOPLandhencewedidnotevaluatetheregularizedIPSonOPE.
A.2 OFF-POLICYLEARNING
PriorOPLresearchhasprimarilyfocusedonthederivationoflearningprinciplesrootedingeneralizationboundsunder
theclippedIPSestimator.First,SwaminathanandJoachims[2015a]designedalearningprinciplethatfavorspoliciesthat
simultaneously demonstrate low estimated cost and empirical variance. Furthermore, Faury et al. [2020] extended this
conceptbyincorporatingdistributionalrobustnessoptimization,whileZenatietal.[2020]adaptedittocontinuousaction
spaces.Thelatteralsoproposedasofterimportanceweightregularizationthatisdifferentfromclipping.Additionally,
LondonandSandler[2019]haselegantlyestablishedaconnectionbetweenPAC-BayestheoryandOPL.Thisconnection
ledtothederivationofanovelPAC-BayesgeneralizationboundfortheclippedIPS.Onceagain,thisboundservedasthe
foundationforthecreationofanovellearningprinciplethatpromotespolicieswithlowestimatedcostandparametersthat
areincloseproximitytothoseoftheloggingpolicyintermsofL distance.Additionally,Sakhietal.[2023]introduced
2
newgeneralizationboundstailoredtotheclippedIPS.Anotablefeatureoftheirapproachisthedirectoptimizationofthe
theoreticalbound,renderingtheuseoflearningprinciplesunnecessary.Expandingupontheseadvancements,Aoualietal.
[2023a]presentedageneralizedbounddesignedforIPSwithexponentialsmoothing.Whatdistinguishesthisparticular
boundfrompreviousonesisitsapplicabilitytostandardIPSwithouttheprerequisiteassumptionthatimportanceweights
arebounded.TheyalsodemonstratedthatoptimizingthisboundforIPSwithexponentialsmoothingresultsinsuperior
performance compared to optimizing existing bounds for clipped IPS. However, a significant question lingers: is the
performance improvement primarily attributed to the enhanced regularization offered by exponential smoothing over
clipping,orisittheconsequenceofthenovelbounditself?Thisuncertaintyarisesfromthefactthattheboundsemployedfor
clippingandexponentialsmoothingdiffer.Inlightoftheseconsiderations,ourworkintroducesaunifiedsetofgeneralization
bounds,allowingformeaningfulcomparisonsthataddressthisquestionandcontributetoadeeperunderstandingofthe
performancedynamics.
B MISSINGPROOFSANDRESULTS
Inthissection,weproveTheorem1.
Theorem 3 (Theorem 1 Restated). Let λ > 0, n ≥ 1, δ ∈ (0,1), and let P be a fixed prior on Θ. Then the following
inequalityholdswithprobabilityatleast1−δforanydistributionQonΘ
(cid:114)
(cid:12) (cid:12) (cid:12)E θ∼Q(cid:104) R(π θ)−Rˆ(π θ,S)(cid:105)(cid:12) (cid:12) (cid:12)≤ KL 21 n(Q) +B n(Q)+ KL n2 λ(Q) + λ 2V¯ n(Q), (18)
√
whereKL1(Q)=D KL(Q∥P)+log4 δn,KL2(Q)=D KL(Q∥P)+log(4/δ),and
n
V¯ n(Q)= n1 (cid:88) E θ∼Q[E a∼π0(·|xi)(cid:2) wˆ θ(x i,a)2(cid:3) +wˆ θ(x i,a i)2c2 i]
i=1
n
1 (cid:88)(cid:88)
B n(Q)=
n
E θ∼Q[|π θ(a|x i)−π 0(a|x i)wˆ θ(x i,a)|]
i=1a∈A(cid:104) (cid:105)
Proof. First,wedecomposethedifferenceE θ∼Q R(π θ)−Rˆ(π θ,S) as
(cid:34) n (cid:35) n (cid:34) n (cid:35)
E θ∼Q(cid:104) R(π θ)−Rˆ(π θ,S)(cid:105) =E θ∼Q R(π θ)− n1 (cid:88) R(π θ|x i) + n1 (cid:88) E θ∼Q R(π θ|x i)− n1 (cid:88) Rˆ(π θ|x i)
i=1 i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I1 I2
n
+ n1 (cid:88) E θ∼Q(cid:104) Rˆ(π θ|x i)(cid:105) −E θ∼Q(cid:104) Rˆ(π θ,S)(cid:105) ,
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
I3
where
R(π )=E [c(x,a)] ,
θ x∼ν,a∼πθ(·|x)
R(π |x )=E [c(x ,a)] ,
θ i a∼πθ(·|xi) i
Rˆ(π |x )=E [wˆ (x ,a)c(x ,a)] ,
θ i a∼π0(·|xi) θ i i
n
Rˆ(π ,S)= 1 (cid:88) wˆ (x ,a )c ,
θ n θ i i i
i=1
(cid:104) (cid:105)
wherewˆ θ(x,a)=g(π θ(a|x),π 0(a|x))forsomenon-negativefunctiong.Ourgoalistobound|E θ∼Q R(π θ)−Rˆ(π θ,S) |
andthusweneedtobound|I |+|I |+|I |.Westartwith|I |,Alquier[2021,Theorem3.3]yieldsthatfollowinginequality
1 2 3 1
holdswithprobabilityatleast1−δ/2foranydistributionQonΘ
(cid:115) √
D (Q∥P)+log4 n
|I |≤ KL δ ,
1 2n
(cid:114)
=
KL1(Q)
. (19)
2n
Moreover,|I |canbeboundedbydecomposingitas
2
(cid:12) (cid:12) (cid:34) 1 (cid:88)n 1 (cid:88)n (cid:35)(cid:12) (cid:12)
|I 2|=(cid:12) (cid:12)E θ∼Q n E a∼πθ(·|xi)[c(x i,a)]− n E a∼π0(·|xi)[wˆ θ(x i,a)c(x i,a)] (cid:12) (cid:12) ,
(cid:12) (cid:12)
i=1 i=1
(cid:12) (cid:12)
(cid:12)1 (cid:88)n (cid:88) (cid:12)
=(cid:12)
(cid:12)n
E θ∼Q[π θ(a|x i)c(x i,a)−π 0(a|x i)wˆ θ(x i,a)c(x i,a)](cid:12)
(cid:12)
,
(cid:12) (cid:12)
i=1a∈A
n
1 (cid:88)(cid:88)
≤
n
E θ∼Q[|π θ(a|x i)−π 0(a|x i)wˆ θ(x i,a)||c(x i,a)|] .
i=1a∈A
But|c(x,a)|≤1foranya∈Aandx∈X.Thus
n
1 (cid:88)(cid:88)
|I 2|≤
n
E θ∼Q[|π θ(a|x i)−π 0(a|x i)wˆ θ(x i,a)|] ,
i=1a∈A
=B (Q) (20)
n
Finally,weneedtoboundthemainterm|I |.Toachievethis,weborrowandadaptthestatementofthefollowingtechnical
3
lemma[HaddoucheandGuedj,2022,Theorem2.1]tooursetting.
Lemma4. LetZ beaninstancespaceandletS =(z ) beann-sizeddatasetforsomen≥1.Let(F ) bea
n i i∈[n] i i∈{0}∪[n]
filtrationadaptedtoS .Also,letΘbeaparameterspaceandπ forθ ∈Θarethecorrespondingpolicies.Nowassume
n θ
that(f (S ,π )) isamartingaledifferencesequenceforanyθ ∈Θ,thatisforanyi∈[n],andθ ∈Θ,wehavethat
i i θ i∈[n]
E[f (S ,π )|F ]=0.Moreover,foranyθ ∈Θ,letM (θ)=(cid:80)n f (S ,π ).Thenforanyfixedprior,P,onΘ,any
i i θ i−1 n i=1 i i θ
λ>0,thefollowingholdswithprobability1−δoverthesampleS ,simultaneouslyforanyQ,onΘ
n
D (Q∥P)+log(2/δ) λ
|E θ∼Q[M n(θ)]|≤ KL
λ
+
2
(E θ∼Q[⟨M⟩ n(θ)+[M] n(θ)]) ,
(cid:104) (cid:105)
where⟨M⟩ (θ)=(cid:80)n E f (S ,π )2|F and[M] (θ)=(cid:80)n f (S ,π )2.
n i=1 i i θ i−1 n i=1 i i θRecallthatwˆ (x,a) = g(π (a|x),π (a|x))forsomenon-negativefunctiong.ToapplyLemma4,weneedtoconstruct
θ θ 0
anadequatemartingaledifferencesequence(f (S ,π )) forθ ∈Θthatallowsustoretrieve|I |.Toachievethis,we
i i θ i∈[n] 3
defineS =(a ) asthesetofntakenactions.Also,welet(F ) beafiltrationadaptedtoS .Forθ ∈Θ,we
n i i∈[n] i i∈{0}∪[n] n
definef (S ,π )as
i i θ
f (S ,π )=f (a ,π )=E [g(π (a|x ),π (a|x ))c(x ,a)]−g(π (a |x ),π (a |x ))c(x ,a ).
i i θ i i θ a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
We stress that f (S ,π ) only depends on the last action in S , a , and the policy π . For this reason, we denote it by
i i θ i i θ
f (a ,π ).Thefunctionf isindexedbyisinceitdependsonthefixedi-thcontext,x .Thecontextx isfixedandthus
i i θ i i i
randomnessonlycomesfroma ∼π (·|x ).Itfollowsthattheexpectationsareundera ∼π (·|x ).First,wehavethat
i 0 i i 0 i
E[f (a ,π )|F ]=0foranyi∈[n],andθ ∈Θ.Thisfollowsfrom
i i θ i−1
(cid:104) (cid:12) (cid:105)
E[f (a ,π )|F ]=E f (a ,π )(cid:12)a ,...,a ,
i i θ i−1 ai∼π0(·|xi) i i θ (cid:12) 1 i−1
(cid:104) (cid:12) (cid:105)
=E E [g(π (a|x ),π (a|x ))c(x ,a)]−g(π (a |x ),π (a |x ))c(x ,a )(cid:12)a ,...,a ,
ai∼π0(·|xi) a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i (cid:12) 1 i−1
( =i)E [g(π (a|x ),π (a|x ))c(x ,a)]−E (cid:104) g(π (a |x ),π (a |x ))c(x ,a )(cid:12) (cid:12)a ,...,a (cid:105) .
a∼π0(·|xi) θ i 0 i i ai∼π0(·|xi) θ i i 0 i i i i (cid:12) 1 i−1
In(i)weusethefactthatgivenx ,E [g(π (a|x ),π (a|x ))c(x ,a)]isdeterministic.Nowa doesnotdependon
i a∼π0(·|xi) θ i 0 i i i
a ,...,a sinceloggeddataisi.d.d.Hence
1 i−1
(cid:104) (cid:12) (cid:105)
E g(π (a |x ),π (a |x ))c(x ,a )(cid:12)a ,...,a =E [g(π (a |x ),π (a |x ))c(x ,a )] ,
ai∼π0(·|xi) θ i i 0 i i i i (cid:12) 1 i−1 ai∼π0(·|xi) θ i i 0 i i i i
=E [g(π (a|x ),π (a|x ))c(x ,a)] .
a∼π0(·|xi) θ i 0 i i
Itfollowsthat
E[f (a ,π )|F ]
i i θ i−1
(cid:104) (cid:12) (cid:105)
=E [g(π (a|x ),π (a|x ))c(x ,a)]−E g(π (a |x ),π (a |x ))c(x ,a )(cid:12)a ,...,a ,
a∼π0(·|xi) θ i 0 i i ai∼π0(·|xi) θ i i 0 i i i i (cid:12) 1 i−1
=E [g(π (a|x ),π (a|x ))c(x ,a)]−E [g(π (a|x ),π (a|x ))c(x ,a)] ,
a∼π0(·|xi) θ i 0 i i a∼π0(·|xi) θ i 0 i i
=0.
Therefore,foranyθ ∈Θ,(f (a ,π )) isamartingaledifferencesequence.HenceweapplyLemma4andobtainthat
i i θ i∈[n]
thefollowinginequalityholdswithprobabilityatleast1−δ/2foranyQonΘ
D (Q∥P)+log(4/δ) λ
|E θ∼Q[M n(θ)]|≤ KL
λ
+
2
(E θ∼Q[⟨M⟩ n(θ)+[M] n(θ)]) ,
=
KL2 λ(Q)
+
λ
2
(E θ∼Q[⟨M⟩ n(θ)+[M] n(θ)]) , (21)
where
n
(cid:88)
M (θ)= f (a ,π ) ,
n i i θ
i=1
n
⟨M⟩ (θ)=(cid:88) E(cid:104) f (a ,π )2|F (cid:105) ,
n i i θ i−1
i=1
n
[M] (θ)=(cid:88) f (a ,π )2 .
n i i θ
i=1Nowthesetermscanbedecomposedas
n
(cid:88)
E θ∼Q[M n(θ)]= E θ∼Q[f i(a i,π θ)] ,
i=1
n
=(cid:88) E θ∼Q(cid:2)E a∼π0(·|xi)[g(π θ(a|x i),π 0(a|x i))c(x i,a)]−g(π θ(a i|x i),π 0(a i|x i))c(x i,a i)(cid:3) ,
i=1
n
( =i)(cid:88) E θ∼Q(cid:104) Rˆ(π θ|x i)(cid:105) −nE θ∼Q(cid:104) Rˆ(π θ,S)(cid:105) ,
i=1
=nI , (22)
3
whereweusedthefactthatc =c(a ,x )foranyi∈[n]in(i).
i i i
Nowwefocusontheterms⟨M⟩ (θ)and[M] (θ).First,wehavethat
n n
(cid:16) (cid:17)2
f (a ,π )2 = E [g(π (a|x ),π (a|x ))c(x ,a)]−g(π (a |x ),π (a |x ))c(x ,a ) , (23)
i i θ a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
(cid:16) (cid:17)2
=E [g(π (a|x ),π (a|x ))c(x ,a)]2+ g(π (a |x ),π (a |x ))c(x ,a )
a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
−2E [g(π (a|x ),π (a|x ))c(x ,a)]g(π (a |x ),π (a |x ))c(x ,a ),
a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
=E [g(π (a|x ),π (a|x ))c(x ,a)]2+g(π (a |x ),π (a |x ))2c(x ,a )2
a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
−2E [g(π (a|x ),π (a|x ))c(x ,a)]g(π (a |x ),π (a |x ))c(x ,a ).
a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
Moreover,f (a ,π )2doesnotdependona ,...,a .Thus
i i θ 1 i−1
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
E f (a ,π )2|F =E f (a ,π )2|F =E f (a ,π )2 =E f (a,h)2 .
i i θ i−1 ai∼π0(·|xi) i i θ i−1 ai∼π0(·|xi) i i θ a∼π0(·|xi) i
(cid:104) (cid:105)
ComputingE f (a,h)2 usingthedecompositionin(23)yields
a∼π0(·|xi) i
(cid:104) (cid:105)
E[f (a ,π )2|F ]=E f (a,h)2 ,
i i θ i−1 a∼π0(·|xi) i
=−E [g(π (a|x ),π (a|x ))c(x ,a)]2+E (cid:2) g(π (a|x ),π (a|x ))2c(x ,a)2(cid:3) (24)
a∼π0(·|xi) θ i 0 i i a∼π0(·|xi) θ i 0 i i
Combining(23)and(24)leadsto
E[f (a ,π )2|F ]+f (a ,π )2 =E (cid:2) g(π (a|x ),π (a|x ))2c(x ,a)2(cid:3) +g(π (a |x ),π (a |x ))2c(x ,a )2
i i θ i−1 i i θ a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
−2E [g(π (a|x ),π (a|x ))c(x ,a)]g(π (a |x ),π (a |x ))c(x ,a ),
a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
( ≤i) E (cid:2) g(π (a|x ),π (a|x ))2c(x ,a)2(cid:3) +g(π (a |x ),π (a |x ))2c(x ,a )2. (25)
a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
The inequality in (i) holds because −2E [g(π (a|x ),π (a|x ))c(x ,a)]g(π (a |x ),π (a |x ))c(x ,a ) ≤ 0
a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
sincegisnon-negative.Therefore,wehavethat
n
⟨M⟩ (θ)+[M] (θ)≤(cid:88) E (cid:2) g(π (a|x ),π (a|x ))2c(x ,a)2(cid:3) +g(π (a |x ),π (a |x ))2c(x ,a )2.
n n a∼π0(·|xi) θ i 0 i i θ i i 0 i i i i
i=1
Itfollowsthat
E θ∼Q[⟨M⟩ n(θ)+[M] n(θ)]
n
≤(cid:88) E θ∼Q(cid:2)E a∼π0(·|xi)(cid:2) g(π θ(a|x i),π 0(a|x i))2c(x i,a)2(cid:3)(cid:3) +E θ∼Q(cid:2) g(π θ(a i|x i),π 0(a i|x i))2c(x i,a i)2(cid:3) . (26)
i=1Combining(21)and(26)yields
n
n|I 3|=|(cid:88) E θ∼Q(cid:104) Rˆ(π θ|x i)(cid:105) −nE θ∼Q(cid:104) Rˆ(π θ,S)(cid:105) |
i=1
≤ KL2 λ(Q) + λ
2
(cid:88)n E θ∼Q(cid:2)E a∼π0(·|xi)(cid:2) g(π θ(a|x i),π 0(a|x i))2c(x i,a)2(cid:3)(cid:3)
i=1
+E θ∼Q(cid:2) g(π θ(a i|x i),π 0(a i|x i))2c(x i,a i)2(cid:3) . (27)
Thismeansthatthefollowinginequalityholdswithprobabilityatleast1−δ/2foranydistributionQonΘ
|I 3|≤ KL n2 λ(Q) + 2λ n(cid:88)n E θ∼Q(cid:2)E a∼π0(·|xi)(cid:2) g(π θ(a|x i),π 0(a|x i))2c(x i,a)2(cid:3)(cid:3)
i=1
n
+ 2λ n(cid:88) E θ∼Q(cid:2) g(π θ(a i|x i),π 0(a i|x i))2c(x i,a i)2(cid:3) . (28)
i=1
Howeverweknowthatc(x,a)2 ≤1foranyx∈X anda∈Aandthatc(x ,a )=c foranyi∈[n].Thusthefollowing
i i i
inequalityholdswithprobabilityatleast1−δ/2foranydistributionQonΘ
|I 3|≤ KL n2 λ(Q) + 2λ n(cid:88)n E θ∼Q(cid:2)E a∼π0(·|xi)(cid:2) g(π θ(a|x i),π 0(a|x i))2(cid:3)(cid:3) + 2λ n(cid:88)n E θ∼Q(cid:2) g(π θ(a i|x i),π 0(a i|x i))2c2 i(cid:3) ,
i=1 i=1
=
KL2(Q)
+
λ
V¯ (Q), (29)
nλ 2 n
whereweusethatg(π (a|x),π (a|x)) = wˆ (x,a).Theunionboundof(19)and(29)combinedwiththedeterministic
θ 0 θ
resultin(20)yieldsthatthefollowinginequalityholdswithprobabilityatleast1−δforanydistributionQonΘ
(cid:114)
|E θ∼Q(cid:104) R(π θ)−Rˆ(π θ,S)(cid:105) |≤ KL 21 n(Q) +B n(Q)+ KL n2 λ(Q) + λ 2V¯ n(Q). (30)
C ADDITIONALEXPERIMENTS
C.1 DETAILEDSETUP
Webeginbyemployingthewell-establishedsupervised-to-banditconversionmethodasdescribedinAgarwaletal.[2014].
Specifically,weworkwithtwosetsfromaclassificationdataset:thetrainingsetdenotedasSTR andthetestingsetasSTS.
Thefirststepinvolvestransformingthetrainingset,STR,intoabanditloggeddatadenotedasS,followingtheprocedure
outlined in Algorithm 1. This newly created logged data, S, is subsequently employed to train our policies. The next
phaseassessestheeffectivenessofthelearnedpoliciesonthetestingset,STS,asoutlinedinAlgorithm2.Wemeasure
theperformanceofthesepoliciesusingtherewardobtainedbyrunningAlgorithm2.Higherrewardsindicatesuperior
performance.Inourexperimentalevaluations,wemakeuseofvariousimageclassificationdatasets,specifically:MNIST
[LeCunetal.,1998]andFashionMNIST[Xiaoetal.,2017].
TheinputtoAlgorithm1isaloggingpolicydenotedasπ ,definedasfollows
0
exp(η ϕ(x)⊤µ )
π (a|x)= 0 0,a , ∀(x,a)∈X ×A. (31)
0 (cid:80) exp(η ϕ(x)⊤µ )
a′∈A 0 0,a′
Here, ϕ(x) ∈ Rd represents the feature transformation function, which writes ϕ(x) = x . The parameters µ =
∥x∥ 0
(µ ) ∈ RdK arelearnedusingafraction(5%)ofthetrainingsetSTR,withthecross-entropyloss.Optimizationis
0,a a∈A
carriedoutusingtheAdamoptimizer[KingmaandBa,2014].Theinverse-temperatureparameterη isacriticalfactor
0
affectingtheperformanceoftheloggingpolicy.Ahighpositivevalueofη indicatesawell-performingloggingpolicy,
0
whileanegativevalueleadstoalower-performingone.Inallexperiments,thepriorPissetasP=N(η µ ,I ).
0 0 dKAlgorithm1Supervised-to-bandit:creatingtheloggeddata
Input.ClassificationtrainingdatasetSTR ={(x ,y )}n ,loggingpolicyπ .
i i i=1 0
Output.loggeddataS =(x ,a ,c ) .
i i i i∈[n]
InitializeS ={}
fori=1,...,ndo
a ∼π (·|x )
i 0 i
c =−I
i {ai=yi}
S ←S∪{(x ,a ,c )}.
i i i
Algorithm2Supervised-to-bandit:testingpolicies
Input:ClassificationtestdatasetSTS ={(x i,y i)}n i=TS 1,learnedpolicyπˆ n.
Output:Testrewardr.
fori=1,...,n do
TS
a ∼πˆ (·|x )
i n i
r =I
i {ai=yi}
r = 1 (cid:80)n TS r .
n i=1 i
TS
C.2 BOUNDOPTIMIZATION
Inthissection,thelearnedpolicyπˆ isobtainedbyoptimizingthefollowingobjective
n
(cid:114)
arg QmaxE θ∼Q(cid:104) Rˆ(π θ,S)(cid:105) + KL 21 n(Q) +B n(Q)+ KL n2 λ(Q) + λ 2V¯ n(Q), (32)
wherethequantitiesaredefinedinTheorem1andthelearningpoliciesπ aredefinedassoftmaxpoliciesas
θ
exp(ϕ(x)⊤θ )
πSOF(a|x)= a , (33)
θ (cid:80) exp(ϕ(x)⊤θ )
a′∈A a′
Tooptimize(32),weemploythelocalreparameterizationtrick[Kingmaetal.,2015].Precisely,wesetQ=N (cid:0) µ,σ2I (cid:1)
dK
(cid:104) (cid:105)
where µ ∈ RdK and σ > 0 are learnable parameters. Then roughly speaking, the terms E θ∼Q Rˆ(π θ,S) , B n(Q) and
V¯ (Q)in(32)areoftheformE [f(πSOF(a|x))]forsomefunctionf andtheycanberewrittenas
n θ∼N(µ,σ2IdK) θ
(cid:20) (cid:16) exp(ϕ(x)⊤θ ) (cid:17)(cid:21)
E [f(πSOF(a|x))]=E f a ,
θ∼N(µ,σ2IdK) θ θ∼N(µ,σ2IdK) (cid:80) exp(ϕ(x)⊤θ )
a′∈A a′
(cid:20) (cid:16) exp(ϕ(x)⊤µ +σϵ ) (cid:17)(cid:21)
=E f a a ,
ϵ∼N(0,IK) (cid:80) exp(ϕ(x)⊤µ +σϵ )
a′∈A a′ a′
whereweuseinthesecondequalitythefactthat∥ϕ(x)∥ =1inourexperimentssincewenormalizedfeatures.Thenthe
2
aboveexpectationcanbeapproximatedas
E [f(πSOF(a|x))]≈
1 (cid:88) f(cid:16) exp(ϕ(x)⊤µ a+σϵ i,a) (cid:17)
, ϵ ∼N(0,I ),∀i∈[S].
θ∼N(µ,σ2IdK) θ S (cid:80) exp(ϕ(x)⊤µ +σϵ ) i K
i∈[S] a′∈A a′ i,a′
forsomeS ≥1.Similarly,thegradientsareapproximatedas
∇ E [f(πSOF(a|x))]≈
1 (cid:88)
∇
f(cid:16) exp(ϕ(x)⊤µ a+σϵ i,a) (cid:17)
, ϵ ∼N(0,I ),∀i∈[S].
µ,σ θ∼N(µ,σ2IdK) θ S µ,σ (cid:80) exp(ϕ(x)⊤µ +σϵ ) i K
i∈[S] a′∈A a′ i,a′
C.3 BOUNDOPTIMIZATIONWHENIWREGULIZATIONISLINEAR
Inthissection,thelearnedpolicyπˆ isobtainedbyoptimizingthefollowingobjectivederivefromtheboundinCorollary2
n
(cid:114)
arg QmaxRˆ(πQ,S)+ KL 21 n(Q) +B n(πQ)+ KL n2 λ(Q) + λ 2V¯ n(πQ), (34)√
whereKL1(Q)=D KL(Q∥P)+log4 δn,KL2(Q)=D KL(Q∥P)+log(4/δ),and
n (cid:20) (cid:21)
V¯ n(πQ)= n1 (cid:88) E
a∼π0(·|xi)
h(π πQ (( aa || xx i ))
)2
+ h(π πQ (( aa i || xx i )) )2c2
i
0 i 0 i i
i=1
n
B n(πQ)=1−
n1 (cid:88)(cid:88)
π 0(a|x i)
h(π πQ( (a a| |x xi)
)).
0 i
i=1a∈A
WeoptimizethisobjectiveoverlearningpoliciesπQthataredefinedasGaussianpoliciesoftheform
πGAUS(a|x)=E [π (a|x)] , where π (a|x)=1 . (35)
µ,σ θ∼N(µ,σ2Id) θ θ {argmax a′∈Aϕ(x)⊤θ a′=a}
Note that these Gaussian policies satisfy the form πQ(a|x) = E θ∼Q[π θ(a|x)] where π
θ
is binary, which required by
Corollary2.ThereisnoexpectationintheaboveobjectiveandhencethemethoddescribedinAppendixC.2isnolonger
needed.AllweneedistobeabletocomputethepropensitiesπQ(a|x)andgradientsoftheaboveobjectivewithrespectto
Q,whichboilsdowntocomputingthegradientofπQwithrespecttoQsincetheobjectiveaboveislinearinπQ.Computing
propensitiesandgradientsisdoneasfollows.First,Sakhietal.[2023]showedthat(35)canbewrittenas
πGAUS(a|x)=E (cid:104) (cid:89) Φ(cid:0) ϵ+ ϕ(x)⊤(µ a−µ a′)(cid:1)(cid:105) ,
µ,σ ϵ∼N(0,1) σ∥ϕ(x)∥
a′̸=a
whereΦisthecumulativedistributionfunctionofastandardnormalvariable.Then,thepropensitiesareapproximatedas
πGAUS(a|x)≈
1 (cid:88) (cid:89) Φ(cid:0)
ϵ +
ϕ(x)⊤(µ a−µ a′)(cid:1)
, ϵ ∼N(0,1),∀i∈[S]. (36)
µ,σ S i σ∥ϕ(x)∥ i
i∈[S]a′̸=a
Similarly,thegradientreads
∇ πGAUS(a|x)=E (cid:104) ∇ (cid:89) Φ(cid:0) ϵ+ ϕ(x)⊤(µ a−µ a′)(cid:1)(cid:105) ,
µ,σ µ,σ ϵ∼N(0,1) µ,σ σ∥ϕ(x)∥
a′̸=a
whichcanbeapproximatedas
∇ πGAUS(a|x)=
1 (cid:88)
∇
(cid:89) Φ(cid:0)
ϵ +
ϕ(x)⊤(µ a−µ a′)(cid:1)
,
µ,σ µ,σ S µ,σ i σ∥ϕ(x)∥
i∈[S] a′̸=a
The results are presented in Figure 4 and are generally consistent with the conclusions drawn in Section 5. The main
distinctionisthatwhenutilizinglinearIWregularizationsandtheapproachdetailedinCorollary2,allmethodsexhibit
betterperformancecomparedtowhentheyareoptimizedusingthetheoreticalboundinTheorem1,whichisapplicableto
potentiallynon-linearIWregularizations.Thisimprovementisattributedtothereductionofvarianceachievedbyremoving
theexpectationE θ∼Q[·]fromtheboundandemployingGaussianpolicies.MNIST, K=10, d=784 MNIST, K=10, d=784 MNIST, K=10, d=784 MNIST, K=10, d=784
0.9 0.9 0.9 0.9
0.8 Logging 0.8 Logging 0.8 Logging 0.8
0.7 Clipping 0.7 Implicit Exploration 0.7 Exponential Smoothing 0.7
0.6 0.6 0.6 0.6
0.5 0.5 0.5 0.5
0.4 0.4 0.4 0.4
0.3 0.3 0.3 0.3
0.2 0.2 0.2 0.2
0.1 0.1 0.1 0.1
0.0 0.0 0.0 0.0
inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0
FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784 FashionMNIST, K=10, d=784
0.8 0.8 0.8 0.8
0.7 Logging 0.7 Logging 0.7 Logging 0.7
Clipping Implicit Exploration Exponential Smoothing
0.6 0.6 0.6 0.6
0.5 0.5 0.5 0.5
0.4 0.4 0.4 0.4
0.3 0.3 0.3 0.3
0.2 0.2 0.2 0.2
0.1 0.1 0.1 0.1
0.0 0.0 0.0 0.0
−0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6 −0.6−0.4−0.2 0.0 0.2 0.4 0.6
inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0 inverse-temperature parameter ´0
Figure4:PerformanceofthepolicylearnedbyoptimizingtheboundinCorollary2fordifferentIWregularizations.The
x-axisreflectsthequalityoftheloggingpolicyη ∈[−0.5,0.5].Inthefirstthreecolumns,weplottherewardofthelearned
0
policyusingafixedIWregularizationtechnique(Clip,IX,orESasdefinedin(4))forvariousvaluesofitshyperparameter
within[0,1].Inthelastcolumn,wereportthemeanrewardacrossthesehyperparametervalues.
C.4 COMPARINGHEURISTICS
WealsocomparedourHeuristicOptimization(15)withtheL heuristicfromLondonandSandler[2019]andfoundthat
2
bothheuristicsexhibitidenticalperformance(redandbluecolorsoverlapinthisplot).
MNIST, K=10, d=784 FashionMNIST, K=10, d=784
0.9 0.8
0.8 0.7
0.7
0.6
0.6
0.5
0.5
0.4
0.4
Logging Logging
0.3
0.3 Our Heuristic Our Heuristic
0.2 L2 Heuristic 0.2 L2 Heuristic
0.1 0.1
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
inverse-temperature parameter ´0 inverse-temperature parameter ´0
Figure5:Performanceofthelearnedpolicywithtwolearningprinciples(ourHeuristicOptimization(15)andtheL
2
heuristicinLondonandSandler[2019]withvaryingvaluesoftheirhyperparametersinagridwithin[10−5,10−3])using
√
theClipIPSriskestimatorin(4)withfixedτ =1/4n.
C.5 TIGHTNESSOFTHEBOUND
WeassessthetightnessofourboundforafixedIWregularizationontheMNISTdataset.Specifically,weconsidertheClip
methodasdefinedin(4),whichregularizestheIWaswˆ(x,a)= π(a|x) .WeapplyCorollary2tothisestimatorby
max(π0(a|x),τ)
settingh(p)=max(p,τ)andevaluatetheboundatthelearnedpolicyfordifferentvaluesofτ.Theresultsareplottedin
Figure6.Generally,theboundisloosewhentheloggingpolicyperformspoorly,i.e.,whenη <0.2,andittightensasthe
0
performanceoftheloggingpolicyimproves,i.e.,asη increases.Thevalueofτ affectstheboundtightness,buttheimpact
0
isnotverysignificantinthesensethatthereisnochoiceofτ thatleadstoaconsistentlyloosebound,irrespectiveofthe
loggingpolicy.
ycilop
denrael
eht
fo
drawer
ycilop
denrael
eht
fo
drawer
ycilop
denrael
eht
fo
drawer
sretemaraprepyh
.t.r.w
drawer
.vA
sretemaraprepyh
.t.r.w
drawer
.vAMNIST, K=10, d=784
0.0
−0.2
−0.4
−0.6
−0.8
−1.0
0.0 0.2 0.4 0.6 0.8 1.0
inverse-temperature parameter ´
0
Figure6:TightnessoftheboundinCorollary2appliedtoClip-IPSin(4)withvaryingvaluesofhyperparameterτ.
dnuob
reppU