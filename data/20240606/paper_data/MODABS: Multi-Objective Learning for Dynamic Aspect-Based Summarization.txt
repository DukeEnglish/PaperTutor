MODABS: Multi-Objective Learning for Dynamic Aspect-Based
Summarization
XiaoboGuo and SoroushVosoughi
DepartmentofComputerScience
DartmouthCollege
Hanover,NewHampshire
{xiaobo.guo.gr,soroush.vosoughi}@dartmouth.edu
Abstract
The rapid proliferation of online content ne-
cessitates effective summarization methods,
among which dynamic aspect-based summa-
Figure1: Diagramillustratingaspect-basedsummariza-
rizationstandsout. Unlikeitstraditionalcoun-
tion,withdistinctcolorsrepresentingdifferentaspects.
terpart, which assumes a fixed set of known
"[SENi]"indicatesthei-thsentenceinthesourcearti-
aspects,thisapproachadaptstothevariedas-
cle.
pectsoftheinputtext. Weintroduceanovel
multi-objectivelearningframeworkemploying
a Longformer-Encoder-Decoder for this task.
recently proposed task (Tan et al., 2020; Mad-
Theframeworkoptimizesaspectnumberpre-
dela et al., 2022), requires models to automati-
diction,minimizesdisparitybetweengenerated
andreferencesummariesforeachaspect,and cally identify and extract relevant aspects from a
maximizesdissimilarityacrossaspect-specific giventext,removingtheneedforpriorknowledge
summaries. Extensive experiments show our aboutaspectnumbersorcontent. Priorresearchon
methodsignificantlyoutperformsbaselineson
DABStypicallyadoptsatwo-phasemethodology,
three diverse datasets, largely due to the ef-
initially focusing on generating aspect-based ele-
fective alignment of generated and reference
ments—suchasidentifyingkeysentences(Hayashi
aspectcountswithoutsacrificingsingle-aspect
etal.,2021;Amaretal.,2023)orkeywords(Souza
summarizationquality.
andManzato,2022;Yangetal.,2023b;Zhuetal.,
1 Introduction
2021) relevant to each aspect. This step is fol-
Withtherapidincreaseindigitalcontent,theneed lowedbyemployingaconventionalsummarization
for automated text summarization systems has model that integrates the identified aspect infor-
grown significantly, as summaries efficiently dis- mation with the main article. An alternative ap-
tillkeyinformationfromextensivetexts. Among proach utilizes the capabilities of large language
various approaches, query-focused summariza- models(LLMs),suchasGPT-3.5-turbo,foraspect-
tion (Wang et al., 2022; Zhong et al., 2021; based summarization without predetermined as-
Zhu et al., 2022) and aspect-based summariza- pects(Yangetal.,2023a;GuoandVosoughi,2024).
tion (Hayashi et al., 2021; Ahuja et al., 2022) However,despiteLLMs’potential,theireffective-
areparticularlyprominentforgeneratingcontent- ness in DABS is often lower than the two-step
specificsummaries. Thesemethods,referencedin approach,mainlyduetotheirinsufficientexplicit
recentstudies,catertodiverseinformationneeds aspectknowledgeandpriorinformation(Guoand
byemphasizingparticularaspectsorquerieswithin Vosoughi,2024).
the summaries. Aspect-based summarization, il- To solve this task, we propose a novel multi-
lustrated in Figure1, specifically focuses on ex- objective dynamic aspect-based summarization
tractingandsummarizinginformationrelevantto framework, which we call MODABS. Our frame-
predefinedaspects,offeringatargetedapproachto workdynamicallyidentifiesandsummarizestheas-
understandinglargedatasets. pectswithoutneedingpriorknowledge. Ourmulti-
However, both methodologies encounter sub- objective model not only works towards the con-
stantial constraints with the requirement of pre- ventionalsummarizationobjectivebutalsopredicts
specified aspects, thereby limiting their applica- thenumberofaspectsandworkstowardsmaximiz-
bility. Dynamic aspect-based summarization, a ing the divergence (i.e., minimizing the overlap)
4202
nuJ
5
]LC.sc[
1v97430.6042:viXrabetween the summaries of different aspects. Em- Nevertheless, a significant limitation persists
pirical evaluations on three datasets for this task across these datasets: they all have pre-defined
showthatourframeworkoutperformstheexisting aspects. Consequently, the aspects related to a
models in predicting the number of aspects and domain’s samples remain static, simplifying the
generating high-quality aspect-based summaries. task considerably. Therefore, dynamic aspect-
Furthermore,ourablationanalysisrevealsthecru- based summarization is proposed where aspects
cialroleofmulti-objectivelearninginenhancing areneitherlimitednorpredefined. Thisapproach
performance. encompasses datasets that identify entities (e.g.,
Thecontributions1 ofthisworkareasfollows: AnyAspect(Tanetal.,2020),ENTSUMV2(Mad-
delaetal.,2022))orconceptuallabels/sub-topics
• We propose a novel multi-objective frame-
withinthetext(e.g.,OASUM(Yangetal.,2023b),
work,calledMODABS,thatenablesthegen-
OpenAsp(Amaretal.,2023)). Furthermore,exten-
erationofmultipleaspect-specificsummaries
sions like disordered-DABS (Guo and Vosoughi,
from a single input without requiring prior
2024) add complexity by introducing disordered
knowledgeoftheaspects. Alongsidethestan-
aspectinformation,wheresentencesrelatedtothe
dardsummarizationobjective,weincorporate
sameaspectmaynotappearinsequence.
thedivergenceofdifferentaspect-basedsum-
mariesandthepredictionoftheaspectnum- 2.2 MethodsinDynamicAspect-Based
bertoenhancetheperformanceofourmethod. Summarization
Theversatilityofourframeworkallowsitto
Existingmethodsfordynamicaspect-basedsum-
beincorporatedwithanyencoder-decoderor
marization (DABS) have primarily evolved from
decoder-onlymodel.
traditionalaspect-basedsummarizationtechniques,
wheremodelsleverageautomaticallygeneratedas-
• Through our ablation analysis, we demon-
pectinformationtoproducesummaries. Theland-
stratetheeffectivenessoftheseadditionalob-
scape of DABS is predominantly shaped by two
jectives in improving dynamic aspect-based
frameworks: one that segments the task into sen-
summarization. By controlling the distance
tence grouping and summarization, and another
betweenthegeneratedsummariesandthepre-
thatgeneratessummariesbasedonpredetermined
cision of the aspect numbers, our approach
aspectkeywords.
achievesenhancedperformance.
Thefirstapproachusesatwo-stepprocedureto
2 RelatedWork deliveraspect-specificsummaries(Liuetal.,2018).
AngelidisandLapata(AngelidisandLapata,2018)
2.1 DatasetsforAspect-BasedSummarization
proposedaweaklysupervisedmethodforthistask,
Numerousstudieshaveexploredaspect-basedsum- leveragingatopicextractivecomponentandasum-
marization, predominantly in the context of re- marizer. These techniques typically group sen-
viewssuchasthoseforproductsorrestaurants(Lu tencesusingdomain-specificinformation. Forim-
et al., 2009; Wang and Ling, 2016; Yang et al., proved model generalization, recent studies have
2018;Angelidisetal.,2021;Amplayoetal.,2021; favoredsupervisedmethodsforsentencegrouping,
ChuandLiu,2019). Recently,theresearchscope utilizing trained classifiers (Hayashi et al., 2021;
expanded to include news articles, which facili- Amplayoetal.,2021),orsentenceembeddingssim-
tatedthecreationofartificiallyengineereddatasets ilartoidentifiedtopicsentences(Angelidisetal.,
with pre-defined aspects (Frermann and Klemen- 2021).
tiev,2019;Ahujaetal.,2022). Encyclopediadata, An alternative method for aspect-based sum-
another substantial information source, has been marization is the controllable generation, com-
leveragedforcraftingaspect-basedsummarization monly applied across various domains such as
datasets(Hayashietal.,2021). Thisdiversification politeness (Sennrich et al., 2016), content (Fan
teststhegeneralizabilityofmodelsacrossdifferent etal.,2018),andstyle(FiclerandGoldberg,2017).
datasets. Theentity-basedsummarizationdataset Based on the idea of controllable generation He
proposedbyMaddelaetal.(2022)offersanother etal.(2022)usekeywordsasprompts,whichwas
angleonaspect-basedsummarization. further enhanced by Ahuja et al. (2022), who de-
visedasupervisedmethodtoselectaspect-oriented
1The code and data for this paper are available here:
https://github.com/xiaobo-guo/MODABS keywords. NotethatwhenapplyingthesemethodstotheDABS,thesupervisedmethodforsentence tiondatasetcontainingoverthreemillionsamples.
clusteringandkeywordgenerationshiftstoanun- UnlikeDisordered-DABS,aspectsinOASUMmay
supervisedapproach,likeBERTopic(Grootendorst, exhibit hierarchical structures, where one aspect
2022),duetothelackofgroundtruthintheinitial maybenestedwithinanother,e.g.,“History”and
phaseofaspect-basedinformationextraction(Yang “History: Japanese invasion”. To ensure a fair
et al., 2023b; Guo and Vosoughi, 2024; Maddela comparison with other baselines, we preprocess
etal.,2022). OASUMtoretainonlyaspectswiththenarrowest
Beyond these conventional methods, an alter- scope. SimilartoDisordered-DABSandFrermann
nativeone-stepapproachutilizespromptingtech- andKlementiev(2019),sentenceshufflingisintro-
niquesalongsideLLMs,suchasGPT-3.5-turboor ducedtocreatedisorderlyscenarios.
GPT-4(Goyaletal.,2022;Yangetal.,2023a;Guo Both DDABS and OASUM datasets include
andVosoughi,2024). Thismethodreliesoncrafted sampleswithverylonginputs,single-aspectsum-
promptstodirectLLMsingeneratingaspect-based maries, or multiple aspects. Following Guo and
summariesfromthegiveninputs. GiventhatLLMs Vosoughi(2024)and Yangetal.(2023b),wepre-
requirenopriordomain-specificknowledge,they process these datasets by truncating source arti-
aretheoreticallywell-suitedforaddressingthechal- cles and summaries and limiting the number of
lengesofdynamicaspectsettings. Despitethis,the aspectsbasedontheiraveragelengthandvariabil-
Prompting method has shown limited success in ity, as detailed in Appendix C.3. Given that the
suchcontexts,asidentifiedby(GuoandVosoughi, originalOASUMdatasetincludesabouthalfofits
2024),primarilyduetotheinsufficiencyofaspect samples with only one aspect, we exclude these
information and prior knowledge, which signifi- single-aspectsamplesfromourexperiments.
cantlyhindersthemodel’sabilitytoaccuratelyful-
fillthetask. 4 Methodology
Toovercomethesechallenges, ourstudyintro-
Ourfine-tuningprocess,illustratedinFigure2,di-
ducesanovelmulti-objectiveapproachthatlever-
vergesfromstandardsummarizationfine-tuningin
ages weak-supervised techniques. This method
two key areas: First, we modify the decoder em-
allows language models to implicitly understand
beddings with the intent to account for multiple
aspect-basedinformation,aidinginthegeneration
aspects,afeaturenotcommonlyconsideredintra-
ofcoherentaspect-basedsummaries. Byintegrat-
ditionalsummarizationtasks. Bytransformingthe
ingweaksupervision,ourapproachaddressesthe
embeddings,ourmodelgeneratesmultipleaspect-
complexitiesofDABSmoreeffectively,sidestep-
specificsummariesconcurrently,offeringamore
ping the need for explicit aspect annotations or
comprehensivesummaryoftheinputtext. Second,
extensivepreprocessing.
weincorporateamulti-objectivelearningscheme
intoourframework.
3 Datasets
These objectives include not only the standard
In our study, we focus on three datasets de- text summarization task but also the maximiza-
signed for dynamic aspect-based summarization: tionofthedistancebetweendifferentaspect-based
Disordered-DABS(GuoandVosoughi,2024)and summarizationrepresentations. Furthermore,our
OASUM(Amaretal.,2023). modelisdesignedtopredictthenumberofaspects
Disordered-DABSiscraftedfordisordereddy- based on the decoding representation. In effect,
namicaspect-basedsummarization,encompassing themulti-objectivelearningapproachencourages
twodatasets: D-CnnDM,adaptedfromCNN/Daily ourmodeltocreatemoredistinctandspecificsum-
Mail(Seeetal.,2017),andD-WikiHowfromWik- mariesperaspect,thusenhancingthegranularity
iHow(KoupaeeandWang,2018). Thesedatasets andprecisionofthesummarization.
utilizeexistingsummarizationdatasetstogenerate
4.1 DecoderEmbeddings
aspect-basedsummariesbymergingmultiplesam-
ples(D-CnnDM)orsegmentingtopicsintodetailed In traditional encoder-decoder or decoder-only
steps(D-WikiHow),introducingshuffledinputsen- models,thedecodergeneratesembeddings(E)of
tencestomimicthedisorderoftenfoundinonline shape B ×L×D, where B is the batch size, L
discussions. is the summary length, and D is the dimensions
OASUMisadynamicaspect-basedsummariza- of all heads. This embedding is then passed toFigure 2: Diagram of our framework. Colored sentences represent different aspects. “[SEN i]” indicates the
i-thsentenceintheinput. Aspects, tokens, andtheirgeneratedsummariesaredenotedasAsp i, T i, andS(cid:100)um i,
respectively. Thepredictednumberofaspectsis#(cid:100)Asp,whileground-truthsummariesandaspectnumbersare
Sum and #Asp, respectively. Cross-entropy loss and KL divergence loss are indicated by “CE” and “KLD”.
i
Weightsfordifferentlossesareλ .
1/2/3
a language model (LM) head to generate the to- λ ,λ ,andλ areweightsassignedtoeachobjec-
1 2 3
ken distribution. Our model, aiming to generate tive. Thelimitfunction,L,canbeeitheraSigmoid
multipleaspect-specificsummariessimultaneously, orTanh. Moredetailsontheweightingoftheloss
reshapes E into B × L × N × D , where N is andthelimitfunctionaregiveninAppendixA.1.
n
the predefined maximum number of aspects and Theintuitionbehindthemulti-objectivesettingis
D = N × D . To produce N aspect-specific expandedoninAppendixA.
n
summaries (S(cid:100)um 1,S(cid:100)um 2,···Su(cid:100)m N), we apply
an LM head to these aspect-based embeddings. 5 Experiments2
Followingstandardconvention,themodelpredicts
5.1 BaselinesandMetrics
thenumberofaspectsusingtheembeddingsofall
aspect-basedsummaries’firsttokens(E[:,:,0,:]). Wecompareourmodelagainstthreebaselinescon-
Thistransformedembeddingisthensenttoaclas- sistent with those used in the Disordered-DABS
sifierheadtoestimatetheaspectnumber(#(cid:100)Asp). and OASUM datasets: Cluster, Keywords, and
Prompting.
4.2 Multi-ObjectiveLearning
For Disordered-DABS (specifically, the D-
Ourmodelincorporatestwoadditionalobjectives CnnDMandD-WikiHowsub-datasets),ourbench-
beyondstandardtextsummarization: maximizing marksalignwiththosereportedbyGuoetal.(Guo
thedistancebetweenaspect-basedsummarization andVosoughi,2024)forboththeClusterandKey-
representationsandpredictingtheaspectcountus- words strategies. For OASUM, we adhere to the
ingthedecodingrepresentation. Ourlossfunction, aspect-based summarization methodology as de-
therefore, comprises three components: (1) The tailed by Guo et al.(Guo and Vosoughi, 2024).
cross-entropy(CE)lossbetweengenerated(S(cid:100)um i) Notably, for the Keywords method, we opt for a
andreference(Sum )summaries(Eq.1);(2)The hyper-parameters-tunedBERTopicmodelforkey-
i
KL divergence (KLD) between different aspect- wordgeneration,divergingfromtheutilizationof
specific summaries, bounded by a limit function Wikipedia subtitles to mitigate potential perfor-
(L) (Eq. 2); (3) The cross-entropy loss between manceinflationduetoleakedaspectinformation.
predicted (#(cid:100)Asp) and reference (#Asp) aspect WeextendourexplorationofthePromptingstrat-
numbers(Eq.3). Thefinallossforamodelwitha egytoencompassbothzero-shotlearningoutcomes
maximumaspectcountofN isthencomputedas: and few-shot fine-tuning results, as reported by
Guo et al. (Guo and Vosoughi, 2024), leveraging
(cid:80)
Loss =λ ∗
iCE(S(cid:100)um i,Sum i)
(1) thelatestOpenAIAPIcapabilitiesforfine-tuning
1
N
the GPT-3.5-turbo model to accept up to 16k in-
(cid:80)
i,jL(KLD(S(cid:100)um i,S(cid:100)um j)) put tokens. Detailed insights into GPT-3.5-turbo
+λ ∗−
2 N areprovidedinAppendixC.2. Thepromptsarede-
(2) signedasfollows: Wegeneratedfiveinitialprompts
+λ 3∗CE(#(cid:100)Asp,#Asp) (3)
2ThecomputinginfrastructureisdetailedinAppendixC.1Dataset Model #AbsAspDiff BERTScore Rouge-1 Rouge-2 Rouge-L
D-CnnDM Keywords 1.3(0.0)* 14.2(0.1)* 24.8(0.0)* 8.9(0.2)* 17.2(0.1)*
Cluster 1.3(0.0)* 15.1(0.2)* 25.4(0.1) 9.1(0.1)* 17.1(0.1)*
GPT-3.5-Zero 6.2 9.1 12.4 4.1 9.1
GPT-3.5-Tuned 3.5 12.9 18.4 6.7 11.9
MODABS 1.0(0.1) 18.2(0.4) 26.4(0.5) 9.7(0.2) 18.3(0.4)
D-WikiHow Keywords 2.7(0.1)* 30.5(0.1)* 14.6(0.2)* 5.0(0.1)* 14.2(0.2)*
Cluster 2.7(0.1)* 31.3(0.0)* 19.1(0.1)* 7.8(0.1)* 18.5(0.1)*
GPT-3.5-Zero 5.5 17.7 11.4 3.8 10.3
GPT-3.5-Tuned 3.8 20.7 13.4 5.2 12.6
MODABS 1.5(0.1) 40.6(0.6) 23.0(1.0) 9.5(0.6) 22.4(0.9)
OASUM Keywords 1.5(0.0)* 21.8(0.8)* 22.3(0.1)* 11.0(0.0)* 19.4(0.0)*
Cluster 1.5(0.0)* 20.7(0.1)* 21.2(0.1)* 10.0(0.0)* 18.6(0.00)
GPT-3.5-Zero 4.2 9.9 11.9 4.4 9.0
GPT-3.5-Tuned 1.0 14.2 19.8 8.9 15.8
MODABS 0.5(0.0) 28.5(0.1) 29.9(0.1) 14.9(0.1) 26.0(0.1)
Table 1: The performance of our models and baselines across all three datasets. Mean scores are reported,
accompanied by standard deviations in brackets. An asterisk (*) indicates a statistically significant difference
(p<0.05)betweenthebaselinemodelsandourMODABSimplementation. Duetobudgetaryconstraints,theresults
forGPT-3.5-ZeroandGPT-3.5-Tunedarederivedfromasingleexperimentalrun;consequently,standarddeviations
andp-valuescouldnotbecomputedforthismodel.
tailoredtothesummarizationtaskandselectedthe 5.2 AutomaticEvaluationResults
most effective one based on performance. This
The performance of our MODABS model, along-
promptwasthenrefinedthroughmultipleiterations,
side baseline methods, is summarized in Table 1.
toensureadherencetoformat,aspect,andlength
These results leverage the Longformer-Encoder-
requirements. Thefinalizedpromptemphasizesthe
Decoder (Beltagy et al., 2020) used by both
generationofconcise,aspect-specificsummaries.
datasets for its ability to take more than 10k in-
puttokens.
Ourevaluationemploysasuiteofautomaticmet-
As shown in Table 1, MODABS excels in all
rics: BERTScore, Rouge-1, Rouge-2, Rouge-L,
evaluation metrics across the three datasets. The
and#AbsAspDiff. Thesemetrics,assessingsum-
performanceimprovementsoverbaselinemodels
maryqualitythroughprecision,recall,andlinguis-
are statistically significant (p<0.05). The small-
tic quality, are complemented by #AbsAspDiff,
estperformancegainsareseenontheD-CnnDM
which specifically quantifies the discrepancy in
dataset,whereasOASUMexhibitsthemostmarked
aspect numbers between reference and gener-
improvements.
ated summaries, employing the aspect alignment
methodologyfromGuoetal.(GuoandVosoughi, Thissuggeststhatourmodel’sefficacymaybe
2024)foraconsistentcomparisonframework. tiedtotheintricacyofdistinguishingbetweenas-
pects. Given that D-CnnDM aggregates multiple
Inadditiontoautomaticmetrics,weincorporate newsarticles,itinherentlypresentsfewercomplex-
human annotation scores, evaluating summaries ities, accounting for the smaller gains observed.
on“Coherence”,“Consistency”,“Fluency”,“Rele- GPT-3.5-Zero performs sub-optimally due to its
vance”,“AspectQuality”,and“OverallRank”,as deployment as an off-the-shelf model with zero-
informed by previous work (Fabbri et al., 2021; shot inference. In line with the findings reported
GuoandVosoughi,2024;Amaretal.,2023;Yang byGuoandVosoughi(2024),ourstudyobserves
etal.,2023b). Thesehuman-centricscoresensure thattheperformanceofGPT-3.5-Tuned,whenap-
a comprehensive assessment of summary quality pliedtotheDABStask,significantlyexceedsthat
from both technical and perceptual standpoints. ofzero-shotlearningapproaches.
The criteria and details pertaining to human an- Thisenhancementunderscoresthevalueoffine-
notatorsareelucidatedinAppendixB. tuningintailoringmodelresponsestothespecificnuancesofDABStasks. Despitethisimprovement, datasetsandbaselinemodelsinthecontextofthe
itisnoteworthythatGPT-3.5-Tuneddoesnotreach source article. These evaluations concentrate on
the performance benchmarks set by the two-step twoprimaryaspects: theoverallqualityofthesum-
baselinemodels or ourproposedMODABS. This marizationandthequalityoftheaspectscaptured.
underscorestheadvancedcapabilityofMODABS Following Fabbri et al. (2021), we assess sum-
indynamicallynavigatingthechallengesofaspect- mariesusingthefollowingcriteria: (1)Coherence,
based summarization, further reinforcing the ne- assessing the structural integrity and logical flow
cessityforspecializedapproachesinhandlingthe ofthesummary;(2)Consistency,ensuringthesum-
intricaciesofsuchtasks. mary’sfactualcontentalignswiththesource,par-
Analyzing the absolute aspect count differ- ticularlybyavoidingtheinclusionofinformation
encesbetweenreferenceandgeneratedsummaries, not present in the source articles; (3) Relevance,
MODABSexhibitsconsistentperformanceacross highlightingtheinclusionofcriticalcontentfrom
datasets,unlikethevariableoutcomesfromother thesource;and(4)Fluency,scrutinizingthegram-
baselines. Thisstabilityimpliesthat,inadditionto matical and linguistic quality of the summary to
aspect-basedsummarization,ourmodeleffectively guaranteereadability.
predicts aspect numbers. We validate this aspect Inadditiontothesecriteria,theuniquerequire-
number prediction against two baseline methods: mentsofDABSnecessitateanextrametric,Aspect-
(1) a fine-tuned classifier and (2) the BERTopic Quality. This metric is crucial for evaluating the
modelwithadjustedhyperparameters. effectivenesswithwhichaspect-basedsummaries
maintaindistinctivenessandfocus. Itensuresthat
D-CnnDM D-WikiHow OASUM eachaspectisnotonlydelineatedbutalsoremains
Classifier 1.69(0.34) 1.89(0.02) 0.48(0.00) thefocalpointwithinitsdesignatedsummary(An-
Cluster 1.28(0.00) 2.69(0.06) 1.48(0.00) gelidisetal.,2021;Amplayoetal.,2021;Guoand
MODABS 1.04(0.01) 1.48(0.07) 0.51(0.01)
Vosoughi,2024).
Thehumanevaluationswereconductedonsam-
Table 2: Absolute aspect number difference between
modelpredictionsandground-truthsummaries. “Classi- ples from the D-CnnDM dataset, following the
fier”and“Cluster”correspondtopredictingthenumber methodologyandinsightsfromGuoandVosoughi
ofaspectsusingaclassifierandBERTopic,respectively. (2024). Annotatorscomparedsummariesfromour
method against baselines using the five criteria
Table2demonstratesMODABSsuperiorityover aboveandrankedthemtodetermineoverallquality.
the“Cluster”methodacrossalldatasets. Compared Details about the annotators and their compensa-
withthe“Classifier”method,MODABSperforms tioncanbefoundinAppendixB.
betteronD-CnnDMandD-WikiHowandalmost Theresults,detailedinTable3,showourmethod
thesameonOASUM.Thisunexpectedresultun- outperformingbaselinesacrossallcriteria. Wedi-
derscores the benefit of incorporating the aspect videdevaluationcriteriaintosingle-aspectquality
numberpredictionobjectiveintoourmodel’sloss (Coherence and Fluency) and multi-aspect qual-
function. ity(Consistency,Relevance,andAspectQuality),
with our method showing marked improvements,
5.3 HumanEvaluations
especially in multi-aspect quality. This suggests
Humanevaluationmetricsprovideareference-free ourmulti-objectivelearningframeworkeffectively
perspective, essential for assessing the quality of enhancesthemodel’sabilitytoidentifyandsynthe-
Model Coherence Consistency Fluency Relevance AspectQuality Rank
GPT-3.5-Zero 3.21(1.28) 2.71(0.75) 4.29(1.12) 3.12(0.80) 4.25(1.07) 2.04(0.95)
Keywords 3.17(0.76) 2.79(0.93) 3.38(0.97) 3.25(0.68) 3.00(1.14) 3.12(0.99)
Cluster 3.50(0.72) 3.21(0.78) 3.50(0.72) 3.46(0.88) 4.04(1.00) 2.62(0.71)
MODABS 3.83(0.48) 3.79(0.66) 4.54(0.78) 4.38(0.77) 4.67(0.64) 1.21(0.59)
Table3: Average(std)humanevaluationratings(1–5scale)onthefivequalitycriteriaandtheranking(1–4scale),
determinedby30instancesfromthetestsamples. TheresultsofGPT-3.5-Zero,Keywords,andClusterexceptfor
theRankisfromGuoandVosoughi(2024)Figure3: Anexampleofthesourcearticle,reference,andthegeneratedSummaries. Emptyquotes(“”)indicate
thatnogeneratedsummariescorrespondtothisreferencesummary.
sizerelevantaspect-basedinformation,particularly tified aspects but introduces an additional aspect
inhandlingcomplexsummarizationtasks. not present in the source article, demonstrating a
tendencytowardshallucination. Thisissueiscriti-
6 AblationAnalysis
calinsummarizationtasks,asitcancompromise
theaccuracyandreliabilityofthegeneratedsum-
Inthissection,wepresentexamplesofsummaries
maries.
generated using different methods. We also con-
ductablationanalysestoinvestigate(1)Theinflu-
6.2 InfluenceoftheMultipleObjectives
ence of the multi-objective settings and (2) The
influence of discrepancies in the aspect numbers Toassesstheimpactofvariousmulti-objectivecon-
betweenthereferenceandgeneratedsummaries. figurationsonmodelperformance,weanalyzeda
20%sampleofthedataset. Detailedsettingsofthe
6.1 ExamplesofSummaries lossweightsfortheseexperimentsareprovidedin
Figure3offersanillustrativeexamplethatincludes Appendix A.1. The outcomes of these configura-
thesourcearticle,referencesummaries,andsum- tions are summarized in Table 4, showcasing the
mariesgeneratedbyboththebaselinemodelsand effectsofdifferentobjectivecombinationsacross
MODABS.Thesourcearticlediscussestheproce- allthreedatasets.
dure for resetting a phone to its factory settings. TheadditionofAspectnumberpredictionloss
For easier comparison, we have aligned the ref- (+Asp)directlyenhancesthemodel’sproficiencyin
erences and generated summaries. Instances of accuratelypredictingthenumberofaspectswithin
empty quotes (“”) indicate the absence of a cor- summaries,asevidencedbyimprovementsinthe
responding generated summary for a given refer- #AbsAspDiff metric. This direct targeting, how-
encesummary. Notably,MODABSproducestwo ever, does not extend to improving the diversity
aspect-basedsummaries,highlightingitscapability among aspect-based summaries. Consequently,
toaddressmultipleaspectssimultaneously. Incon- while#AbsAspDiffbenefitsfromthisfocus,other
trast,theKeywordsandClustermethodsgenerate performancemetricsaretheworstamongallthree
summariesfocusedonasingleaspectonly. modelarchitectures
RegardingthePromptingmethod,thesummary In contrast, the integration of KL Divergence
generated by GPT-3.5-Zero is notably as long or loss (+KLD) demonstrates a broader impact on
evenlongerthanthesourcearticleitself,indicating performancemetrics,withnotableexceptions. Al-
inefficiencyincondensinginformation. GPT-3.5- thoughKLDgenerallyfostersimprovementsacross
Tuned shows an attempt to cover all three iden- mostmetricsbyenhancingdifferentiationbetweenDataset Model #AbsAspDiff BERTScore Rouge-1 Rouge-2 Rouge-L
+Asp 1.27 10.39 20.38 6.17 14.51
D-CnnDM +KLD 1.30 12.71 21.82 7.23 15.37
All 0.93 14.89 24.05 8.02 17.00
+Asp 1.89 33.99 15.34 4.38 14.93
D-WikiHow +KLD 2.14 34.28 16.19 5.05 15.75
All 1.63 38.37 21.91 8.57 21.21
+Asp 0.54 23.74 25.37 10.78 21.90
OASUM +KLD 0.55 24.49 26.51 11.95 22.95
All 0.52 24.80 26.73 12.03 23.19
Table4: Optimalperformancemetricsformodelsemployingvariousobjectives. Theterms“+Asp”and“+KLD”
indicatetheinclusionofadditionallossmetricsalongsidethestandardsummarycross-entropyloss,where“+Asp”
referstothelossassociatedwithpredictingthenumberofaspects,and“+KLD”referstotheKLDivergenceloss
calculatedbetweendifferentaspect-specificsummaries. Thedesignation“All”representstheaggregationofall
consideredlossmetrics.
aspect-based summaries, it does not benefit the at predicting the number of aspects compared to
#AbsAspDiffmetric. Thisdiscrepancystemsfrom thebaselines,whichalignswithexpectationssince
KLD’spropensitytopenalizemismatchesinaspect- MODABS’slearningobjectiveincludesanaspect
based summary distributions, which can inadver- numberpredictioncomponent. Thisimprovement
tentlyraisetheKLDivergenceincaseswherean is manifested in two distinct ways. Firstly, the
aspect-basedsummaryisinaccuratelyidentifiedas pronouncedpeakata“0”differenceindicatesthat
empty. Itisimportanttonotethatthepenalization MODABSmorefrequentlyalignswiththereference
forincorrectaspectnumberpredictionsinherently regardingaspectcount,underscoringitsprecision
affectsthesescores,suggestingthattheactualper- inaspectprediction. Secondly,unlikeothermod-
formanceimpactofKLDcouldbemorefavorable els,particularlyGPT-3.5-Zero,MODABSexhibits
thaninitiallyapparentwhensolelyconsideringits azero-centricdistribution. Thispatternsuggestsa
effectonaspectnumberaccuracy. morerefinedcomprehensionofthedataset-specific
Thetruepotentialofourapproachisunlocked aspectdefinitionsbyMODABSandhighlightsits
through the combination of both Aspect number methodologicaladvantageinunderstandingthenu-
prediction and KL Divergence losses. This com- ancedstructureofaspect-basedinformation.
bination not only rectifies the individual limita- The distributions associated with Prompting
tions posed by each loss when applied in isola- methods, representedbyGPT-3.5-ZeroandGPT-
tion but also capitalizes on their strengths to fos- 3.5-Tuned, furtherilluminatethedynamicsofas-
ter a comprehensive enhancement of model per- pect prediction. GPT-3.5-Zero tends to overesti-
formance. By accurately predicting aspect num- matethenumberofaspects,asevidencedbyaright-
bersandpromotingdiversityinaspect-basedsum- shifted curve, whereas fine-tuning through GPT-
maries,themodelachievesasignificant,balanced 3.5-Tuned corrects this bias, resulting in a distri-
improvementacrossallevaluatedmetrics. Thissyn- butionthatisslightlyleft-shifted,indicatingfewer
ergy demonstrates the effectiveness of our multi- aspects predicted than the reference. This shift
objectiveframework. underscores the adaptability of LLMs to capture
the definitional nuances of aspects through mini-
6.3 AnalysisofAspectNumberDiscrepancies
mal tuning, with less than 200 samples required
The difference in aspect numbers between refer- foreffectivelearning. Interestingly,theBERTopic
enceandgeneratedsummariesplaysasignificant method,evenwithouthyperparameteroptimization,
roleintheperformanceofthemodelsoftheDABS demonstratesanabilitytosurpasstheperformance
task. Hence, we examine the distribution of the of LLMs in this context. This observation may
aspect number discrepancies between references pointtolimitationswithinLLMs’capacitytograsp
andgeneratedsummariesfortheMODABSandall and replicate aspect-based summaries’ complex,
baselinesinFigure4. multifaceted nature without extensive customiza-
AsseeninFigure4,MODABSismoreaccurate tionordomain-specifictuning.35
35 60
30
30 25 50
25
20 40
20
15 15 30
10 10 20
5 5 10
0 0 0
10 5 0 5 10 15 10 5 0 5 10 15 6 4 2 0 2 4 6 8
#Aspectpred - #Aspectref #Aspectpred - #Aspectref #Aspectpred - #Aspectref
Keywords MODABS GPT-3.5-Tuned Keywords MODABS GPT-3.5-Tuned Keywords MODABS GPT-3.5-Tuned
Cluster GPT-3.5-Zero Cluster GPT-3.5-Zero Cluster GPT-3.5-Zero
(a)D-CnnDM (b)D-WikiHow (c)OASUM
Figure 4: Distribution of aspect number differences between reference and generated summaries for all three
datasets.
7 ConclusionandFutureDirections could involve treating these weights as learnable
parameters.
This study introduces a robust multi-objective
Model Compatibility: In our research, we
frameworktailoredfordynamicaspect-basedsum-
restrictedourexperimentationtotheLongformer-
marization, addressing the complexities of disor-
Encoder-Decoderduetolengthconstraints. Even
deredinformationprevalentintoday’sdigitalland-
thoughourframeworkisdesignedtobecompati-
scape. Byinnovativelymergingaspectdiscovery
blewithallencoder-decoder/decoder-onlymodels,
with aspect-based summarization, our approach
additionalverificationisrequired.
surpasses traditional methods in accurately pre-
Memory Requirements: The multi-objective
dicting aspect counts and significantly enhances
learningprocessofDASdemandsmoreGPUmem-
summary quality. The empirical evidence drawn
orythanthebaselinemodels. Mitigatingthiscon-
fromextensiveexperimentsacrossvariousdatasets
straintisachallenge,butpotentialsolutionscould
underscorestheefficacyofourframework.
involvetheuseofmodelslikeRWKV(Pengetal.,
Looking ahead, the integration of advanced at-
2023), or Hyena (Poli et al., 2023), which have
tentionmechanismsandadaptivelayerspromises
linearornon-dependentmemoryrequirementsrel-
to further refine our model, paving the way for
ativetotheinput/outputlength.
evenmorepreciseandefficientsummarizationtech-
LanguageConstraints: Ourexperimentshave
niques.
beenexclusivelyconductedinEnglish. Although
ourmethodistheoreticallylanguage-independent,
8 Limitations&EthialConsiderations
it is still uncertain whether our framework will
Weforeseenosignificantethicalissuesrelatedto effectively generalize to other languages. More
ourwork. However,weemploythreepubliclyac- testsareneededtoaffirmthis.
cessibledatasetsthatmaycontainsensitiveoroffen-
sivematerial. Researchersareadvisedtoapproach Acknowledgments
thesedatasetswithcaution.
This work is supported in part by NSF Award
Whenapplyingthefindingsandthemethodpro-
2242072 and a generous grant by the Templeton
posedinourstudytosummarizationtasks,thefol-
Foundation.
lowing potential limitations should be taken into
account:
Dependence on Loss Weights: Our multi- References
objectiveframework’sperformancedependsonthe
Ojas Ahuja, Jiacheng Xu, Akshay Gupta, Kevin
weightsassignedtovariousobjectives,whichmay
Horecka, and Greg Durrett. 2022. Aspectnews:
differ from one dataset to another. Currently, we Aspect-orientedsummarizationofnewsdocuments.
determinetheoptimalweightcombinationthrough In Proceedings of the 60th Annual Meeting of the
AssociationforComputationalLinguistics(Volume
small-scaledatasetexperiments, aprocessthatis
1: LongPapers),pages6494–6506.
bothtime-consumingandnotnecessarilysuccess-
ful in finding the best combination. Future work Shmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori
egatnecreP egatnecreP egatnecrePShapira,andIdoDagan.2023. OpenAsp: Abench- XiaoboGuoandSoroushVosoughi.2024. Disordered-
markformulti-documentopenaspect-basedsumma- dabs: Abenchmarkfordynamicaspect-basedsum-
rization. InProceedingsofthe2023Conferenceon marization in disordered texts. arXiv preprint
EmpiricalMethodsinNaturalLanguageProcessing, arXiv:2402.10554.
pages1967–1991,Singapore.AssociationforCom-
putationalLinguistics. HiroakiHayashi,PrashantBudania,PengWang,Chris
Ackerson, Raj Neervannan, and Graham Neubig.
ReinaldKimAmplayo,StefanosAngelidis,andMirella 2021. Wikiasp: Adatasetformulti-domainaspect-
Lapata.2021. Aspect-controllableopinionsumma- basedsummarization. TransactionsoftheAssocia-
rization. InProceedingsofthe2021Conferenceon tionforComputationalLinguistics,9:211–225.
EmpiricalMethodsinNaturalLanguageProcessing,
Junxian He, Wojciech Kryscinski, Bryan McCann,
pages6578–6593.
NazneenRajani,andCaimingXiong.2022. CTRL-
sum: Towardsgenericcontrollabletextsummariza-
StefanosAngelidis,ReinaldKimAmplayo,Yoshihiko
tion. InProceedingsofthe2022ConferenceonEm-
Suhara, Xiaolan Wang, and Mirella Lapata. 2021.
pirical Methods in Natural Language Processing,
Extractiveopinionsummarizationinquantizedtrans-
pages5879–5915,AbuDhabi,UnitedArabEmirates.
formerspaces. TransactionsoftheAssociationfor
AssociationforComputationalLinguistics.
ComputationalLinguistics,9:277–293.
MahnazKoupaeeandWilliamYangWang.2018. Wiki-
Stefanos Angelidis and Mirella Lapata. 2018. Sum-
how: Alargescaletextsummarizationdataset. arXiv
marizing opinions: Aspect extraction meets senti-
preprintarXiv:1810.09305.
mentpredictionandtheyarebothweaklysupervised.
In Proceedings of the 2018 Conference on Empiri- Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
calMethodsinNaturalLanguageProcessing,pages Goodrich,RyanSepassi,LukaszKaiser,andNoam
3675–3686. Shazeer.2018. Generatingwikipediabysummariz-
inglongsequences. InInternationalConferenceon
IzBeltagy,MatthewEPeters,andArmanCohan.2020. LearningRepresentations.
Longformer: Thelong-documenttransformer. arXiv
preprintarXiv:2004.05150. YueLu,ChengXiangZhai,andNeelSundaresan.2009.
Ratedaspectsummarizationofshortcomments. In
Eric Chu and Peter Liu. 2019. Meansum: A neural Proceedingsofthe18thinternationalconferenceon
modelforunsupervisedmulti-documentabstractive Worldwideweb,pages131–140.
summarization. InInternationalConferenceonMa-
chineLearning,pages1223–1232.PMLR. Mounica Maddela, Mayank Kulkarni, and Daniel
Preo¸tiuc-Pietro.2022. Entsum: Adatasetforentity-
AlexanderRFabbri,WojciechKrys´cin´ski,BryanMc- centric extractive summarization. In Proceedings
Cann,CaimingXiong,RichardSocher,andDragomir of the 60th Annual Meeting of the Association for
Radev.2021. Summeval: Re-evaluatingsummariza- ComputationalLinguistics(Volume1: LongPapers),
tionevaluation. TransactionsoftheAssociationfor pages3355–3366.
ComputationalLinguistics,9:391–409.
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Al-
balak,SamuelArcadinho,HuanqiCao,XinCheng,
AngelaFan,DavidGrangier,andMichaelAuli.2018.
Michael Chung, Matteo Grella, Kranthi Kiran GV,
Controllableabstractivesummarization. InProceed-
et al. 2023. Rwkv: Reinventing rnns for the trans-
ingsofthe2ndWorkshoponNeuralMachineTrans-
formerera. arXivpreprintarXiv:2305.13048.
lationandGeneration,pages45–54.
MichaelPoli,StefanoMassaroli,EricNguyen,DanielY
Jessica Ficler and Yoav Goldberg. 2017. Controlling
Fu, TriDao, StephenBaccus, YoshuaBengio, Ste-
linguisticstyleaspectsinneurallanguagegeneration.
fanoErmon,andChristopherRé.2023. Hyenahierar-
InProceedingsoftheWorkshoponStylisticVariation,
chy: Towardslargerconvolutionallanguagemodels.
pages94–104.
arXivpreprintarXiv:2302.10866.
LeaFrermannandAlexandreKlementiev.2019. Induc- AbigailSee,PeterJLiu,andChristopherDManning.
ingdocumentstructureforaspect-basedsummariza- 2017. Gettothepoint: Summarizationwithpointer-
tion. InProceedingsofthe57thAnnualMeetingof generatornetworks. InProceedingsofthe55thAn-
theAssociationforComputationalLinguistics,pages nualMeetingoftheAssociationforComputational
6263–6273. Linguistics (Volume 1: Long Papers), pages 1073–
1083.
TanyaGoyal, JunyiJessyLi, andGregDurrett.2022.
News summarization and evaluation in the era of Rico Sennrich, Barry Haddow, and Alexandra Birch.
gpt-3. arXivpreprintarXiv:2209.12356. 2016. Controllingpolitenessinneuralmachinetrans-
lation via side constraints. In Proceedings of the
Maarten Grootendorst. 2022. Bertopic: Neural topic 2016ConferenceoftheNorthAmericanChapterof
modelingwithaclass-basedtf-idfprocedure. arXiv theAssociationforComputationalLinguistics: Hu-
preprintarXiv:2203.05794. manLanguageTechnologies,pages35–40.Luan Soares de Souza and Marcelo Garcia Manzato.
2022. Aspect-based summarization: an approach
withdifferentlevelsofdetailstoexplainrecommen-
dations. InProceedingsoftheBrazilianSymposium
onMultimediaandtheWeb,pages202–210.
Bowen Tan, Lianhui Qin, Eric Xing, and Zhiting
Hu. 2020. Summarizing text on any aspects: A
knowledge-informed weakly-supervised approach.
InProceedingsofthe2020ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(EMNLP),
pages6301–6309.
Alex Wang, Richard Yuanzhe Pang, Angelica Chen,
JasonPhang,andSamuelBowman.2022. Squality:
Buildingalong-documentsummarizationdatasetthe
hardway. InProceedingsofthe2022Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,
pages1139–1156.
LuWangandWangLing.2016. Neuralnetwork-based
abstractgenerationforopinionsandarguments. In
ProceedingsofNAACL-HLT,pages47–57.
MinYang,QiangQu,YingShen,QiaoLiu,WeiZhao,
andJiaZhu.2018. Aspectandsentimentawareab-
stractivereviewsummarization. InProceedingsof
the27thinternationalconferenceoncomputational
linguistics,pages1110–1120.
XianjunYang,YanLi,XinluZhang,HaifengChen,and
WeiCheng.2023a. Exploringthelimitsofchatgpt
forqueryoraspect-basedtextsummarization. arXiv
preprintarXiv:2302.08081.
XianjunYang,KaiqiangSong,SangwooCho,Xiaoyang
Wang, XiaomanPan, LindaPetzold, andDongYu.
2023b. OASum: Large-scale open domain aspect-
basedsummarization. InFindingsoftheAssociation
for Computational Linguistics: ACL 2023, pages
4381–4401,Toronto,Canada.AssociationforCom-
putationalLinguistics.
MingZhong,DaYin,TaoYu,AhmadZaidi,Mutethia
Mutuma,RahulJha,AhmedHassan,AsliCelikyil-
maz,YangLiu,XipengQiu,etal.2021. Qmsum: A
newbenchmarkforquery-basedmulti-domainmeet-
ingsummarization. InProceedingsofthe2021Con-
ferenceoftheNorthAmericanChapteroftheAsso-
ciationforComputationalLinguistics: HumanLan-
guageTechnologies,pages5905–5921.
FangweiZhu,ShangqingTu,JiaxinShi,JuanziLi,Lei
Hou, and Tong Cui. 2021. Twag: A topic-guided
wikipediaabstractgenerator. InProceedingsofthe
59thAnnualMeetingoftheAssociationforCompu-
tationalLinguisticsandthe11thInternationalJoint
Conference on Natural Language Processing (Vol-
ume1: LongPapers),pages4623–4635.
HaichaoZhu,LiDong,FuruWei,BingQin,andTing
Liu.2022. Transformingwikipediaintoaugmented
dataforquery-focusedsummarization. IEEE/ACM
TransactionsonAudio,Speech,andLanguagePro-
cessing,30:2357–2367.A Multi-objectiveLoss TheKLdivergence,whichmeasuresthedisparity
betweentwoprobabilitydistributions,couldpoten-
The parameters for our model’s multi-objective
tially become very large or even infinite, destabi-
frameworkarebasedonthefollowingobservations
lizingthefine-tuningprocess. Topreventthis,we
andconsiderations:
employed the limit function to cap the KL diver-
Cross-Entropy Loss: For the summarization
gence to bound the limits (using either the Tanh
task, it is vital to calculate the cross-entropy loss
orSigmoidfunctions). However,ourexperiments
between the generated and reference summaries.
indicatedthatthislimitmightreduceourmodel’s
Thisapproachdirectsthemodeltowardproducing
performanceincaseswheretherewasnonumerical
moreaccuratesummaries.
stabilityissue.
KLDivergenceLoss: Ourexperimentsreveal
thatmodelsoftengeneratesummariesapplicable B HumanAnnotators
toseveralaspects,likelytoreducetheaverageloss
Forthistask,eachsampleisevaluatedwiththree
acrossaspects. Thispropensitycouldresultinthe
evaluators. The evaluators are paid about 10–15
creationofgeneric,non-aspect-specificsummaries,
dollars per hour for this task. We look for evalu-
conflictingwithourgoalofdynamicaspect-based
ators with a bachelor’s degree or higher living in
summarization. Tocounterthis,weincorporateKL
theUS.Wealsorequirethattheevaluatorshavea
divergencelossbetweensingle-aspectsummaries.
goodcommandofEnglish. Theevaluatorsaretold
Aspect Number Prediction: Our trials show
thatthiswillbeusedforacademicusage.
thatamismatchinaspectnumbersbetweentheref-
We follow the instructions used by Guo and
erenceandgeneratedsummariescansignificantly
Vosoughi(2024)forthehumanevaluationprocess.
affectthemodel’sperformance. Tominimizethis
Weprovidethescoresofbaselinestoalignthescore
difference,weincorporateanobjectivetopredict
ofthebaselinesandourMODABS.
thenumberofaspects.
Byintegratingtheseobjectives,weaimtobuild
C ExperimentalDetails
amulti-objectiveframeworkcapableofproducing
aspect-basedsummariesthatarebothaccurateand C.1 ComputingInfrastructure
uniquetoeachaspectwhileaccuratelypredicting
Forourexperiments,weusedaLambdamachine
thenumberofaspects.
equipped with 250 GB of memory, 4 RTX 8000
GPUs, and 32 CPU cores. The machine runs on
A.1 LossWeightandLimitFunction
Ubuntu20.04,andourexperimentswereconducted
Selectingtheweights(λ ,λ ,andλ )foreachob-
1 2 3 using Python 3.8.10. The CUDA version is 11.9,
jectiveinthemulti-objectivelearningframework
andtheGPUDriverVersionis520.61. Themain
iscrucialtobalanceeachtask’scontributiontothe
packagesweutilizeincludebertopic(0.14.1),cuml-
model’s final performance. In our method, λ is
1 cu11 (23.4.1), deepspeed (0.8.0), torch (1.13.1),
alwayssetto1,whilethevaluesforλ andλ are
2 3 scikit-learn(1.1.2),sentence-transformers(2.2.2),
determined via grid search. For this process, we
scipy (1.9.1), transformers (4.22.1), and numpy
set aside a fraction (20%) of the original dataset
(1.23.3). The complete list of packages will be
andfine-tunedthemodelusingdifferentcombina-
providedinthecoderelease.
tionsofweights. Wechoosetheweightvaluesof
λ andλ from0(whichdisablesthecorrespond- C.2 GPT-3.5-TurboDetails
2 3
ing objective), 0.1, 0.5, or 1. The combination
Inourexperiments,weleveragedmultipleversions
thatyieldedthebestperformanceonthevalidation
of GPT-3.5-Turbo, adapting to the evolving Ope-
data was chosen as the optimal one for the main
nAIAPIsandmodelupdates. Thespecificsofour
experiments.
modelusageareasfollows:
Analternativeapproach,whichcouldpotentially
improvethemodel’sperformance,involvesadding • ForD-CnnDMandD-WikiHow,thedataset
a layer to the model to learn the optimal weight paper by Guo and Vosoughi (2024) reports
settings during the fine-tuning process using the zero-shot learning results using GPT-3.5-
completedataset. Turbo-16k-0613. Our few-shot fine-tuning
Regardingthelimitfunction(L),itspurposeis experiments were conducted with GPT-3.5-
to ensure numerical stability during fine-tuning. Turbo-1106.• In the case of OASUM, we employed GPT- single-aspect summaries is guided by statistical
3.5-Turbo-1106 for both zero-shot and few- measures, specifically the mean and standard de-
shotlearningexperiments. viation of their lengths, setting the threshold to
approximatelythemeanplustwicethestandardde-
Thedeterminationoftheoptimalnumberofsam-
viation(≈mean+2×std). Thiscriterionensuresa
ples for fine-tuning was a pivotal aspect of our
balancedrepresentationofcontentwhilemanaging
methodology. Consistentwiththeinsightsprovided
outlierlengthseffectively.
byGuoandVosoughi(2024),ourobservationscon-
Indeterminingthemaximumnumberofaspects
firmed that increasing the number of fine-tuning
(#Asp)forourdatasets,weadopteddataset-specific
samples does not necessarily lead to a linear im-
thresholdstoreflecttheinherentvariabilityinas-
provement in model performance. Consequently,
pect counts. For D-CnnDM, the threshold was
we relied on evaluating the loss values from vali-
setbasedonthemaximumaspectcountobserved
dationsetsobtainedduringthefine-tuningprocess
withinthedataset,whichstandsat11. Fortheother
toidentifythemosteffectivemodelconfiguration
datasets,recognizingtheirlong-taildistributionin
fortheD-CnnDMandOASUMdatasets. Forthe
aspectcounts,weestablishedthresholdsof16for
D-WikiHow dataset, we adopted the fine-tuning
D-WikiHowand8forOASUM.Thesethresholds
settings as recommended by Guo and Vosoughi
approximatethemeanplustwicethestandarddevi-
(2024). Table C1 presents the observed loss val-
ation(≈mean+2×std),enablingustoencompass
uesfordifferentfine-tuningsamplesizesforboth
abroadspectrumofaspectdistributionseffectively.
D-CnnDMandOASUM.Theprocessofselecting
Accordingly, samples that exceeded these thresh-
thesamplenumberwashaltedonceweidentifieda
olds were omitted from our study. Additionally,
pointofdiminishingreturns,whereadditionalsam-
weexcludedsamplescomprisingsolelyofsingle
ples no longer contributed to performance gains.
aspectsfromD-WikiHowandOASUMtoprevent
Themodelconfigurationthatachievedthehighest
single-aspectsamplesfrompredominatingourex-
levelofperformancewassubsequentlychosenfor
periments,aligningwiththeapproachsuggestedby
detailedpresentationinthemainpaper.
GuoandVosoughi(2024). Thisdecisionensures
a balanced representation across varying aspect
DCnnDM OASUM
counts,enhancingtherobustnessofouranalysis.
50 1.9651 1.4669
100 1.0328 1.2620
Arc. Leng Sum. Length #Asp
200 0.2759 1.3198
400 1.5383 N/A D-CnnDM 11,264 76 12
D-WikiHow 2,040 20 16
TableC1: Thelossofbothdatasetsonthesampledvali- OASUM 8192 128 8
dationsetswithvariousnumbersoffine-tuningsamples.
Theoneswiththebestperformancearebolded. TableC2: Thethresholdsforthesourcearticlelength,
single-aspectsummarylength,andthemaximumaspect
numberforalldatasets.
C.3 DataProcessing
Table C2 outlines the established thresholds for
the length of source articles, the length of single-
C.4 BERTopicHyperparameterTuning
aspect summaries, and the maximum number of
aspectsconsideredfortruncationduringourexper- Forthebaselines,weperformedagridsearchfor
iments. Specifically, the threshold for summary tuningtheBERTopicmodelhyperparameters. The
lengthpertainstothelengthallocatedforasingle hyperparameterstunedfortheBERTopicinclude
aspect, with the aggregate length of multi-aspect “n_neighbours”, “n_component”, and “min_dist”
summariesbeingtheproductofthisthresholdand which control the cluster size and the samples
the number of aspects. Our data processing ap- within a cluster. We performed BERTopic clus-
proachalignswiththemethodologiesemployedby tering on each sample of the validation data and
GuoandVosoughi(2024)andYangetal.(2023b) selectedthecombinationofhyperparametersthat
forbaselinecomparisons. minimizedtheabsolutedifferencebetweenthegen-
Foralldatasets,truncationofsourcearticlesand eratedandreferenceaspectnumbers.C.5 HyperparametersandRandomSeeds
Forourexperiments,weusedthreerandomseeds
(0, 10, and 42) for the complete dataset experi-
ments. For the ablation analysis and loss weight
search, we used only one random seed (42). We
used the Hugging Face implementation for fine-
tuning the language model with a batch size of
4 due to GPU memory limitations. The training
epochwassetto10withanearlystop,andallother
training process hyperparameters were set to the
default values provided by the package. We also
usedDeepSpeedtoreducememoryrequirements
duringthefine-tuningprocess. ThespecificDeep-
Speed configuration will be provided along with
ourcode.