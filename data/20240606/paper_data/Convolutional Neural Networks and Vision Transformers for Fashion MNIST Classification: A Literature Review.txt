1
4202
nuJ
5
]VC.sc[
1v87430.6042:viXraConvolutional Neural Networks and Vision
Transformers for Fashion MNIST Classification:
A Literature Review
Sonia BOUZIDI1[0009−0004−7876−5211], Ghazala HCINI1[0000−0003−3571−417X],
Imen JDEY1[0000−0001−7937−941X], and Fadoua DRIRA1[0000−0001−6706−4218]
ReGIM-Lab. REsearch Groups in Intelligent Machines (LR11ES48)
Abstract. OurreviewexploresthecomparativeanalysisbetweenCon-
volutional Neural Networks (CNNs) and Vision Transformers (ViTs) in
the domain of image classification, with a particular focus on clothing
classificationwithinthee-commercesector.UtilizingtheFashionMNIST
dataset, we delve into the unique attributes of CNNs and ViTs. While
CNNshavelongbeenthecornerstoneofimageclassification,ViTsintro-
duceaninnovativeself-attentionmechanismenablingnuancedweighting
of different input data components. Historically, transformers have pri-
marily been associated with Natural Language Processing (NLP) tasks.
Through a comprehensive examination of existing literature, our aim
is to unveil the distinctions between ViTs and CNNs in the context of
image classification. Our analysis meticulously scrutinizes state-of-the-
art methodologies employing both architectures, striving to identify the
factors influencing their performance. These factors encompass dataset
characteristics,imagedimensions,thenumberoftargetclasses,hardware
infrastructure, and the specific architectures along with their respective
top results.
Ourkeygoalistodeterminethemostappropriatearchitecturebetween
ViT and CNN for classifying images in the Fashion MNIST dataset
withinthee-commerceindustry,whiletakingintoaccountspecificcondi-
tionsandneeds.Wehighlighttheimportanceofcombiningthesetwoar-
chitectureswithdifferentformstoenhanceoverallperformance.Byunit-
ingthesearchitectures,wecantakeadvantageoftheiruniquestrengths,
which may lead to more precise and reliable models for e-commerce ap-
plications.CNNsareskilledatrecognizinglocalpatterns,whileViTsare
effectiveatgraspingoverallcontext,makingtheircombinationapromis-
ing strategy for boosting image classification performance.
Keywords: Clothingclassification·Deeplearning·ViT·CNN·Hybrid
Model· Fashion MNIST.
1 Introduction
Presently, there is a notable surge in e-commerce sales, with the fashion sec-
tor emerging as a frontrunner, particularly in the aftermath of the COVID-19CNN and VIT for Fashion MNIST Classification : A Litterature review 3
pandemic [1]. The evolution of online sales turnover is vividly depicted in fig-
ure 1, showcasing a substantial rise from 1336 in 2014 to 5908 in 2023, with a
promising projection of 6388 for 2024 [58]. Fashion is positioned as the most
Fig.1. The evolution of online sales turnover since 2014 and predictions for 2024.
lucrative sector of e-commerce, holding the largest market share in 2022 among
all e-commerce sectors, and now estimated at a whopping 871.2 billion in global
online sales [59] as shown in figure 2. This financial dominance underlines the
considerable impact of the fashion industry in the digital economy, highlighted
by continued growth and optimistic prospects for the future.
The surge in online shopping has posed a significant hurdle for the fashion
industry: the struggle to precisely distinguish garments amidst the vast array
of images accessible on the internet. Despite the plethora of images offering
consumersanarrayofoptions,thisoftenleadstouncertaintyregardingthetrue
attributes of products. Consequently, this uncertainty triggers a worrying surge
inproductreturns,impactingnotonlytheprofitabilityofonlineretailbusinesses
butalsodisruptingthecustomershoppingexperience,therebyerodinglong-term
brand trust and loyalty.
Employingdeeplearningtechniquesforclothingdetectioninonlineshopping
endeavors to offer a pragmatic remedy to enhance consumers’ online shopping
encounters [2][3]. With the flourishing e-commerce landscape in the fashion in-
dustry, this technology presents a novel avenue to cater to evolving consumer
demands,demonstratingitscapabilitytopreciselydiscernclothingitemswithin
images.4 Bouzidi et al.
Fig.2. Estimated annual spending in each consumer goods e-commerce category in
2022.
The growing interest in Vision Transformers (ViTs) as an innovative archi-
tecture for image recognition tasks, alongside the proven track record of Convo-
lutional Neural Networks (CNNs) in image classification, motivates this review
to scrutinize and compare these two methodologies. CNNs have traditionally
been lauded for their efficacy in capturing spatial hierarchies of features, serv-
ing as a cornerstone in image classification. However, the emergence of ViTs,
harnessingself-attentionmechanisms,hassparkedconsiderableattentiondueto
their capability to model long-range dependencies and adapt to diverse input
sizes.
WhileViTspresentcompellingadvantages,includingtheircapacitytohandle
long-range dependencies and adapt to variable input sizes, they also encounter
challenges such as computational complexity, large model sizes, scalability to
extensive datasets, interpretability, resilience to adversarial attacks, and gener-
alization performance. These challenges highlight the necessity of juxtaposing
ViTs with the established CNN models to gain insights into their respective
strengths and weaknesses.
Inthisreview,weoutlinethebackgroundinSection2,examinerelatedworks
in Section 3, provide a discussion in Section 4, and conclude in Section 5. An
overview of each section is as follows:
– Section 2: Background explores the performance of deep learning architec-
tures CNN and ViT in image classification. We discuss the importance of
merging these two architectures in different ways and introduce commonly
used datasets and metrics crucial for classification tasks.CNN and VIT for Fashion MNIST Classification : A Litterature review 5
– Section 3: Related Works presents an analysis of recent studies on image
classification with CNN, ViT, and their hybridization, focusing on the use
of the Fashion MNIST dataset.
– Section 4: Discussion delves into the significance of choosing the right archi-
tecture (CNN, ViT), the value of combining these approaches, the impor-
tance of hyperparameter tuning, the specific characteristics of the fashion
industry, as well as the associated limitations and challenges.
– Section 5: Conclusion wraps up the main points of our review and offers
suggestions for future research.
2 Background
In response to the exponential growth of data, the field of machine learning
hasembracedacutting-edgeapproachknownas”deeplearning”[4].Recognized
for its exceptional performance with large datasets, deep learning has become
the forefront of machine learning techniques [5]. In recent times, its application
has expanded notably within computer vision tasks, particularly in the domain
of fashion image classification [6]. Deep learning models, notably Convolutional
Neural Networks (CNNs) and vision transformers (ViTs), have demonstrated
promising results in tasks such as image classification, recognition within the
fashion image [7]. Our focus remains on image classification, a crucial aspect
of image processing. Similar to its application in fashion image classification,
deeplearningtechniquesareincreasinglyadoptedinfashionimageclassification
tasks. These techniques, including CNN, VIT and their different architectures
as detailed in figure 3, aim to automatically extract features from input images,
enabling tasks like detection, segmentation, and classification without the need
for manual feature engineering [8].
Fig.3. Different architectures of ViT and CNN.6 Bouzidi et al.
Recent progress in the field of deep learning has seen an increase in research
efforts focused on architectures designed for image processing. CNNs and ViTs
are among the most notable models and have drawn significant interest. These
architecturesoperateusingsuperviseddeeplearningframeworks.Inrecenttimes,
VisionTransformer(ViT)hasgainedprominenceasanalternativetotraditional
Convolutional Neural Networks (CNNs) for image classification tasks. Research
on ViTs is anticipated to grow in the future as more researchers explore this
promising approach.
To determine which model is more dominant in image classification tests,
the emphasis is on the number of research publications in recent years. This
method enables a comparison of different approaches based on the volume of
papers published on each. The analysis follows the approach of Keele et al.
[102] and includes manual searches in digital libraries such as IEEE Xplore,
SpringerLink, ACM Digital Library, Wiley Online Library, and Science Direct.
As shown in Figure 4, transformers have gained prominence in recent years
compared to various CNN methods, such as ResNet, VGG, DenseNet, VGG16,
and MobileNetV2.
Fig.4. Number of papers using Vision Transformer and different CNN architectures
in image classification tasks between 2018 and 2024.CNN and VIT for Fashion MNIST Classification : A Litterature review 7
Drawing from the competition between my vision transformers and convolu-
tional neural networks (CNNs) in image classification, we aim to delineate the
fundamental principles of each model.
2.1 Convolutional Neural Networks
CNNstracetheiroriginsbacktothe1980s[8],yettheirwidespreadadoptionfor
image classification surged following the breakthrough achievement of AlexNet
in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [9].
The underlying principle of CNNs lies in their layered architecture, with each
layerplayingapivotalroleintheimageprocessingpipeline.Convolutionallayers
serveasthecornerstone,employingfilterstodiscernspecificpatternsandstruc-
tures within input images, such as edges or textures[10]. Subsequently, pooling
layerstypicallysucceedconvolutionallayers,facilitatingthereductionofdimen-
sionality in resultant activation maps while retaining salient features. Integrat-
ing normalization layers, like Batch Normalization, is commonplace to stabilize
learningdynamicsandexpeditemodelconvergence.Extractedfeaturesthentra-
verse through fully connected layers, amalgamating spatial and structural cues
to accomplish targeted objectives, such as image classification. Finally, an out-
put layer, often manifested as a softmax layer, allocates probabilities to each
conceivableclass,thusenablingtheclassificationofimagesbasedonthegleaned
features [11] [103] , as shown in figure 5. The primary steps involved in CNN-
Fig.5. The standard CNN architecture.
based image classification encompass:
1. Convolution and Pooling: Initially, the input image with dimensions
(H×W×C) undergoes convolution with multiple filters to extract essential8 Bouzidi et al.
features. This process detects various visual patterns like edges, shapes, and
textures. Following convolution, pooling operations are employed to down-
size the image, making the detected features robust against minor spatial
variations.
2. Flattening:Subsequent to convolution and pooling, the extracted features
areflattenedintoaone-dimensionalvector.Thistransformationensuresthat
the image representation is compatible with the input requirements of fully
connected layers.
3. FullyConnected(FC)Layer:Flattenedfeaturesarethenpassedthrough
oneormorefullyconnectedlayers.Intheselayers,eachneuronisintricately
connected to all neurons in the preceding layer, allowing for the extraction
of intricate feature combinations.
4. Activation Function:Aftereachfullyconnectedlayer,anon-linearactiva-
tion function is applied to introduce non-linearity into the model. Typically,
Rectified Linear Unit (ReLU) activation is utilized, enabling the network to
capture complex feature relationships.
5. Classification Layer: Finally, a classification layer is incorporated, typi-
callycomprisingafullyconnectedlayerfollowedbyasoftmaxfunction.This
layer takes the extracted features and produces probabilities corresponding
to each class in the classification task.
The most common CNN architectures are LeNet, AlexNet, VGGNet, ResNet,
InceptionNet and DenseNet[87]:
– LeNet: is a convolutional neural network (CNN) architecture introduced
by LeCun et al. in 1998 [88]. This model played a significant role in the
early development of deep learning. The most recognized version, LeNet-5,
comprises seven layers: an input layer, two convolutional layers, two pool-
ing layers, and two fully connected layers. The convolutional layers use 5x5
kernels,whilethepoolinglayersutilize2x2kernels.Thefullyconnectedlay-
ers contain 84 and 10 neurons, respectively. Initially, LeNet was created for
recognizing handwritten digits, but it has since been applied to other image
classification tasks.
– AlexNet:IsrenownedasapioneeringConvolutionalNeuralNetwork(CNN)
architecture extensively employed for image recognition tasks. Unveiled in
2012,itaimedatresolvingintricatelarge-scaleimageclassificationchallenges
[12].Throughitssequenceofeightconvolutionallayersfollowedbythreefully
connectedlayers,AlexNetshowcasedthecapabilityofdeepCNNstodiscern
discriminative features from raw data. Its operational principle lies in the
hierarchical extraction of features across multiple layers, where lower layers
capture low-level features such as edges and textures, while higher layers
learn complex patterns and semantic information [12].
– VGGNet:IsdistinguishedasadeepConvolutionalNeuralNetwork(CNN)
architecture recognized for its depth and uniformity. Introduced in 2014,
VGGNet was engineered to extract intricate features from images utilizing
a series of convolutional and pooling layers [13]. With its fixed-size con-
volutional layers (3x3) and pooling layers (2x2), VGGNet established newCNN and VIT for Fashion MNIST Classification : A Litterature review 9
standards in image classification accuracy. Its operational principle involves
the systematic extraction of features through multiple convolutional layers,
gradually learning hierarchical representations of input data with increasing
depth [14].
– ResNet: ResNet emerges as a groundbreaking CNN architecture that in-
troduced the concept of residual connections in 2015 [15]. Addressing the
challenge of training increasingly deep networks, ResNet innovation lies in
residual connections that allow information to flow directly through certain
layers, mitigating the vanishing gradient problem. By enabling the train-
ing of much deeper networks (over 100 layers), ResNet revolutionized image
classification performance [15].
– InceptionNet:InceptionNet,alsoreferredtoasGoogLeNet,isadeepCNN
architecture developed by Szegedy et al. in 2014 [16] [17]. The design is
based on incorporating several convolutional filters of different sizes within
the same layer, allowing the network to extract features at various scales.
InceptionNetconsistsofmultipleinceptionmodules,eachcontainingvarious
convolutionalfiltersofdifferentsizes.Theoutputfromeachinceptionmodule
is combined and passed to the next layer. InceptionNet has been utilized for
numerous image classification tasks and has achieved top performance on
multiplebenchmarkdatasets.Itconsistsof22layers,includingnineinception
modules.
– DenseNet: DenseNet represents a CNN architecture distinguished by its
dense connectivity between layers. Introduced in 2016, DenseNet offers an
innovative approach where each layer is connected to all subsequent lay-
ers.Thisdenseconnectivityfostersinformationsharingamongdifferentlay-
ers, enhancing feature reuse and facilitating the learning of richer and more
discriminative representations of input data [100]. Its operational principle
involves the exploitation of dense connectivity patterns, promoting feature
reuseandfacilitatingtheflowofinformationthroughoutthenetwork,which
leads to improved learning efficiency and performance [101].
Table 1. Number of hyperparameters for various CNN architectures [104].
Architecture Number of Hyperparameters (M)
LeNet 0.06
AlexNet 62.38
VGGNet 138.36
ResNet 25.6
InceptionNet 11.4
DenseNet 10-2010 Bouzidi et al.
2.2 Vision Transformers
ViT is a recent breakthrough in the area of computer vision [18].The history of
the ViT dates back to 2017, when it was initially designed for natural language
processing (NLP) [36]. However, in 2020, its application expanded to computer
vision tasks, marking the advent of the ”vision transformer” [19]. In 2021, the
ViT surpassed the Convolutional Neural Networks (CNNs) in terms of perfor-
mance and efficiency, especially in image classification[19]. The ViT stands out
for its ability to capture complex patterns in images thanks to its attention-
basedapproach,thusofferinganeffectivealternativetotraditionalarchitectures
based on convolutions [20]. These advances have positioned ViT as a promis-
ing method for image classification, opening up new perspectives in the field of
deep learning applied to vision [20]. The vision transformer works by dividing
an image into fixed-size patches, embedding each one linearly, adding position
embeddings, and then feeding the assembled vectors to a conventional Trans-
former encoder to create a sequence of vectors [21]. To perform classification,
the traditional method involves adding a learnable ”classification token” in the
sequence [22] as shown in figure 6.
Fig.6. ViT architecture.
The main Vision Transformer steps [60] [61] for image clasification are :
1. Image Patching:Theimageofsize(H×W×C)isdividedintoNpatchesof
size (P×P×C), where H is the height, W is the width, and C is the number
of channels of the image. P is the resolution of the image patch.
2. Linear Transformation of Patches to Vectors: The patches are flat-
tenedsothatthesizeoftheflattenedpatchhasavectorizedformof(1×P²*C).
Eachpatchistransformedintoa1Dvectortobeusedasinputforthemodel.
3. Adding Position Tokens: Position embeddings are added to the patch
embeddings to retain positional information. A special classification token
(CLS) is added to the positional encoding. The positional coded patch vec-
tors are then fed as input to the Transformer Encoder.CNN and VIT for Fashion MNIST Classification : A Litterature review 11
4. Encoder Layer: The transformer encoder consists of alternating layers of
multi-head self-attention (MSA) and MLP (Multilayer Perceptron) blocks.
Alayernormalizationisappliedbeforetheself-attentionblockandtheMLP
block.Residualconnectionsareappliedaftereachblocktofacilitatelearning.
5. Classification Layer: A classification head is implemented using an MLP
with a hidden layer for feature extraction and a single linear layer for fine-
tuningimageclassification.Thislayertakesasinputthefinalrepresentation
of the image produced by the encoder and generates classification scores for
each possible class.
The most common pretrained ViT architectures a DeiT, T2T-ViT, CrossViT,
CaiT, DINO, TNT and Swin Transformer [89][94]:
– DeiT (Data-efficient image Transformers): DeiT is a Vision Trans-
former that was introduced in a research paper published at ICML 2021
[90]. It is a data-efficient model that achieves comparable performance to
state-of-the-art CNNs on image classification tasks [91]. DeiT uses a dis-
tillation token to transfer knowledge from a teacher model to the student
model. It consists of 12 transformer layers, with a hidden size of 768 and 12
attention heads [92].
– T2T-ViT (Token-to-Token Vision Transformer):T2T-ViTisaVision
TransformerthatwasintroducedinaresearchpaperpublishedatICLR2021
[93]. It is a hierarchical model that uses a token-to-token structure to model
images at different scales. T2T-ViT consists of 12 transformer layers, with a
hiddensizeof768and12attentionheads.Italsoincludesaspatialreduction
module to reduce the spatial dimensions of the input image.
– CrossViT: CrossViT is a Vision Transformer that was introduced in a re-
search paper published at CVPR 2021 [95]. It is a model that uses a cross-
modal embedding space to learn from both local and global features of the
inputimage.CrossViTconsistsof12transformerlayers,withahiddensizeof
768 and 12 attention heads. It also includes a convolutional stem to extract
local features from the input image.
– CaiT (Co-scale Attention in Vision Transformers): CaiT is a Vision
Transformer that was introduced in a research paper published at ICCV
2021. It is a model that uses a co-scale attention mechanism to learn from
both local and global features of the input image [96]. CaiT consists of 24
transformerlayers,withahiddensizeof1024and16attentionheads.Italso
includes a hierarchical structure to model images at different scales.
– DINO (Self-supervised learning of deep features for image recog-
nition):DINOisaself-supervisedlearningmethodforVisionTransformers
thatwasintroducedinaresearchpaperpublishedatICLR2022[97].Ituses
a teacher-student framework to learn deep features from unlabeled images.
DINO consists of 12 transformer layers, with a hidden size of 768 and 12
attention heads.
– TNT (Training Normalization-free Transformers): TNT is a Vision
Transformer that was introduced in a research paper published at NeurIPS
2021 [98]. It is a normalization-free model that uses a hierarchical structure12 Bouzidi et al.
to model images at different scales. TNT consists of 24 transformer layers,
with a hidden size of 1024 and 16 attention heads.
– Swin Transformer: Swin Transformer is a Vision Transformer that was
introducedinaresearchpaperpublishedatNeurIPS2021[99].Itisamodel
that uses a shifted windowing scheme to learn from the input image. Swin
Transformerconsistsof24transformerlayers,withahiddensizeof1024and
16 attention heads. It also includes a hierarchical structure to model images
at different scales.
Table 2. Number of hyperparameters for various ViT architectures [105].
Method Number of Hyperparameters( (M)
DeiT 22.04
T2T-ViT 22
CrossViT 30.6
CaiT 27.35
DINO 86
TNT 23.8
Swin Transformer 28.3
CNN and ViT each bring unique strengths and weaknesses in terms of per-
formance and complexity. Integrating these two approaches may optimize their
respective benefits and reduce their drawbacks, potentially enhancing the effi-
ciency of image classification tasks.
2.3 Hybrid Models
In recent times, hybrid architectures have surfaced as a compelling strategy to
overcome the limitation of both CNNs and ViTs such as long-range dependency
capture, inductive bias and rigid input size requiremen for CNN and weak lo-
cal feature extraction, sensitivity to noise and High memory cos for ViT. By
combining their respective advantages, resulting in enhanced performance for
image classification tasks [23][24]. Various forms of integration, including par-
allel and sequential approaches, are actively being researched to optimize the
synergybetweenthesearchitectures.Byunderstandingthesestrengthsandlim-
itations(Table3and4),researchershavedevisedstrategiestoalleviatethemand
engineer hybrid architectures.CNN and VIT for Fashion MNIST Classification : A Litterature review 13
Table 3. Limitations and Strengths of CNN
Strengths Limitations
Local Feature Extraction:Long-Range Dependency Capture: CNNs,
CNNs demonstrate remarkablelimited by local receptive fields, struggle to cap-
proficiency in capturing and pro-tureextensivepixelrelationships,impactingtasks
cessing intricate local features,likeobjectdetectionandimagesegmentationthat
rendering them highly adept atrequire understanding distant contexts [65] [66].
tasks necessitating meticulous
attention to detail, such as image
recognition [62].
Inductive Biases: This robustInductiveBias:CNNsdemonstrateasignificant
biasarisesfromthelocalconnectiv-inductivebias,resultinginoverfittingandlimited
ity and convolution operations, en-generalization. This arises as CNNs learn specific
hancinggeneralizationperformancelocalpatternsfromtrainingdata,potentiallyhin-
and decreasing sensitivity to noisydering their ability to generalize to new, unseen
data [64]. data [63].
Rigid Input Size Requirement: CNNs of-
ten necessitate a fixed input size, reducing their
adaptability to images of diverse resolutions [67].
Table 4. Limitations and Strengths of ViT
Strengths Limitations
Global Context Modeling ViTsWeak local feature extraction: ViTs may
employ the self-attention mecha-struggle with tasks that require strong local fea-
nism, enabling them to captureture extraction, such as image recognition tasks
long-rangedependenciesandglobalwithfine-graineddetails.ThisisbecauseViTsfo-
contextual relationships betweencus on capturing global context rather than ex-
pixels,whichiscrucialfortasksliketracting fine-grained local features [70].
objectdetectionandimagesegmen-
tation [68].
Scalability: ViTs image tokeniza-Sensitivity to noise: ViTs can be sensitive to
tion allows adaptability to var-noisy data, as they rely on global context. Noisy
ied image sizes, offering versatilitydata can poorly affect the self-attention mecha-
within model-defined token limitsnism and lead to poor performance [71].
[69].
High memory cost: ViTs often involve high
computational costs, especially with larger image
sizes or more tokens, due to the quadratic com-
plexity of self-attention mechanisms [72]
Parallel Hybridization: In parallel hybridization, both CNN and ViT net-
works are employed simultaneously on the same input data [23]. This strategy
harnessesthespatialrepresentationcapabilitiesofCNNsalongwithViTs’ability
to capture long-range relationships within the data (Fig. 7). By amalgamating
these distinct representations, parallel hybrid models aim to elevate both the
precision and resilience of classification outcomes.14 Bouzidi et al.
Fig.7. Parallel Hybridization.
Sequential Hybridization: The sequential integration of CNNs and ViTs
entails a step-by-step progression of data, where one architecture sequentially
handles the input and transfers its results to the next architecture (Fig. 8).
Usually, a CNN initiates by extracting local features, which are subsequently
analyzed by the ViT to identify long-range relationships. Ensuring alignment of
representations between these architectures is vital, often requiring adjustments
ortransformations,likeresizing,reshaping,oradaptingtheoutputfeaturesfrom
one architecture to conform to the input format demanded by the subsequent
architecture during sequential processing [25].
Fig.8. Sequential Hybridization.
2.4 Hierarchical Hybridization
Hierarchical integration involves the fusion of Convolutional Neural Networks
(CNNs) and Vision Transformers (ViTs) in a layered manner, leveraging their
respective strengths at various stages of data processing(Fig.9). CNNs excel in
extracting local features, capturing intricate spatial details, while ViTs special-
ize in understanding global context and long-range dependencies [56]. Ensuring
coherence and alignment between the representations of these architectures is
essential in hierarchical integration. Techniques such as cross-layer connections
orhierarchicalfusionfacilitateasmoothcombinationoffeaturesacrossdifferent
levels.CNN and VIT for Fashion MNIST Classification : A Litterature review 15
Fig.9. Hierarchical Hybridization.
Evaluating the effectiveness of CNN, ViT, and their hybrid counterparts in
imageclassificationtasksrequirescarefulconsiderationofthedatasetused.The
performance of these models is significantly influenced by the chosen dataset,
making dataset selection a vital aspect of the evaluation process. As a result,
the choice of dataset becomes a critical factor in determining the accuracy and
reliability of these models in image classification applications.
2.5 Dataset Description
As e-commerce continues to expand, the significance of technologies such as
clothing recognition, retrieval engines, and automated product recommendation
systems grows for fashion-related enterprises. However, effectively categorizing
clothingpresentschallengesduetothediversecharacteristicsofgarmentsandthe
complexities of classification. This difficulty extends to distinguishing between
similar classes, complicating the task of multiple-class clothing classification.
Consequently, the adoption of algorithms tailored to handle extensive volumes
of fashion-specific data is becoming increasingly prevalent. Within the fashion
industry, various datasets are utilized for image recognition as shown in table 5,
includingwidelyknownonessuchas”FashionMNIST,””DeepFashion”,”Watch
and Buy” ”Fashion IQ,” ”FGVCx Fashion”, ”iMaterialist”, ”ModaNet”, ”Chic-
topia10K”,and”DeepFashion3D”[26][27][84][28][29][30][31][85][86].Tofind
the leading dataset, we use the number of research publications over time as an
importantmetric,enablingcomparisonsinimageclassificationtasks.Inlinewith
establishedpractices,ouranalysisincludesmanualsearchesacrossmajordigital
libraries as outlined in section 2. Our results, shown in Figure 10, show that
”Fashion-MNIST” is the most popular dataset, representing around 52.9% of
researchefforts.Fashion-MNISTisfrequentlyemployedasabenchmarktoeval-
uate the efficacy of image classification and computer vision algorithms. Image
classificationservesasafundamentalaspectofcomputervision[32],extensively
utilized within the fashion industry to create detailed product descriptions and
streamline product tagging processes. This automated approach becomes in-
dispensable, especially when managing vast collections, a common occurrence
across various e-commerce platforms [33].16 Bouzidi et al.
Table 5. Various Fashion Image Datasets.
Dataset Number of imagesDataset Link
Fashion MNIST 70 000 images https://github.com/zalandoresearch/
fashion-mnist
DeepFashion 800 000 images https://www.kaggle.com/datasets/
vishalbsadanand/deepfashion-1
Watch and Buy 1,042,178images https://tianchi.aliyun.com/competition/
entrance/531893/information
Fashion IQ 77 683 images https://github.com/XiaoxiaoGuo/
fashion-iq
FGVCx Fashion 55 000 images https://sites.google.com/view/fgvc7/
home
iMaterialist 50 000 images https://www.kaggle.com/c/
imaterialist-fashion-2019-FGVC6/
overview
ModaNet 55 000 images https://github.com/modanet/ModaNet
Chictopia10K 17,706 images https://files.is.tue.mpg.de/classner/
gp/
DeepFashion3D 2000 images https://drive.google.com/drive/folder/
1JWkrjoJk7ATBhtanNm6aUOhFswRYD1WP
Fig.10. Number of papers using Different database of the fashion industry between
2020 and 2024 in the image classification task.
FashionMNIST,curatedbyZalando[73],consistsof70,000grayscaleimages
representing various fashion products across 10 categories. The dataset includes
a training set of 60,000 images and a test set of 10,000 images as detailed in
table 6. Each image is a 28x28 pixel representation, associated with a label
corresponding to one of the 10 clothing classes [34].CNN and VIT for Fashion MNIST Classification : A Litterature review 17
Table 6. Data distribution of Fashion Mnist.
Label Description Samples Examples
Training Test
0 Top 6.000 1.000
1 Trouser 6.000 1.000
2 Pullover 6.000 1.000
3 Dress 6.000 1.000
4 Coat 6.000 1.000
5 Sandal 6.000 1.000
6 Shirt 6.000 1.000
7 Sneaker 6.000 1.000
8 Bag 6.000 1.000
9 Boot 6.000 1.000
Fashion MNIST is evaluated using several metrics in image classification
tasks.Arangeofmeasuresistypicallyappliedtoassessmodelperformancewith
this dataset.
2.6 Performance Metrics
In the realm of image classification, multiple evaluation metrics are utilized to
assess classification models notably CNN and ViT, ensuring a thorough under-
standing of their performance. Commonly employed metrics include accuracy,
precision, recall, F1 score, and specificity, as delineated in Table 7, where true
posi- tive (TP), false positive (FP), false negative (FN), and true negative (TN)
represent the values. This array of metrics allows for the incorporation of di-
verseperspectivesonmodelperformance,facilitatingamorecomprehensiveand
accurate evaluation of its classification abilities.18 Bouzidi et al.
Table 7. Overview of Performance Evaluation Metrics.
Metrics Description Formula
Accuracy Indicatestheratioofaccuratepredictionsrela-
tivetothetotalnumberofpredictions,provid-
TP +TN
ing insight into the model’s precision in fore-
TP +TN +FN +FP
casting outcomes.
(1)
Precision Indicatestheproportionofcorrectlypredicted
TP
positive instances among all instances pre- (2)
dicted as positive, illustrating the model’s ef- TP +FP
fectiveness in accurately identifying relevant
outcomes.
Recall Denotes the fraction of samples with positive
TP
labels that were correctly identified as posi- (3)
tive,reflectingthemodel’scapacitytocapture TP +FN
all relevant instances without omission.
F1 score Quantifiestheharmonicmeanofprecisionand
recall,offeringacomprehensiveevaluationofa
Precision∗Recall
model’s ability to balance accurate identifica- 2∗( )
Precision+Recall
tionofrelevantinstancesandminimizingfalse
(4)
positives.
specificityQuantifies the fraction of true negative in-
TN
stances correctly identified as negative, pro- (5)
viding an indication of the model’s capability TN +FP
to accurately discern negative outcomes and
minimize false positives
Afteroutliningthebackgroundandfoundationalconcepts,wewillnowreview
recentdevelopmentsinimageclassificationresearch.Thenextsectionfocuseson
keyworksthatutilizeCNN,ViT,andhybridmethodswiththeFashionMNIST
dataset.
3 Related Works
Thefollowingsearchtermswereusedtofindpapersonartificialintelligenceand
Fashion MNIST dataset, we applied the following two research methods :
C1: ”Deep learning”, ”Vision Transformer (ViT)”, and ”Fashion MNIST clas-
sification”.
C2: ”Deep Learning” and ”Convolutional Neural Networks (CNN)” and ”Fash-
ion MNIST Classification”.
C3: ”Deep Learning” and ”Hybridizing Vision Transformer(ViT) and Convo-
lutional Neural Networks(CNN)” and ”Fashion MNIST Classification”(Figure
11).CNN and VIT for Fashion MNIST Classification : A Litterature review 19
Fig.11.NumberofpapersusingCNNs,ViTsandhybridizingCNNandViTforFash-
ion MNIST Classification.
3.1 CNNs for Fashion MNIST Classification
Leithardtetal.[35]developedgarmentrecognitionalgorithmstomeettherising
demands of the online fashion market. Their study utilized Convolutional Neu-
ral Network (CNN) models to classify fashion items using the Fashion-MNIST
dataset. They observed a significant improvement in accuracy, from 89.7% to
99.1%, achieved with a novel CNN model termed cnn-dropout-3. This research
highlights the potential of CNN models in accurately classifying fashion items,
thereby enhancing sales strategies in the online fashion market.
Khanday et al. [36] investigated the impact of filter size hyperparameters
on Convolutional Neural Networks (CNNs) for image classification, specifically
focusing on the FashionMNIST dataset. They explored three filter sizes (3×3,
5×5, and 7×7) while maintaining a constant number of filters (32). The study
found that smaller filter sizes led to higher classification accuracy, with the 3×3
filter achieving an accuracy of 93.68% on the FashionMNIST dataset. However,
it was noted that smaller filter sizes resulted in longer training times due to
increased computational costs. These findings emphasize the significance of fil-
ter size selection in optimizing CNN performance for image classification tasks,
particularly in datasets like Fashion MNIST.
Nocentini et al. [37]explored the use of neural network models to improve
apparel image classification accuracy on the Fashion-MNIST dataset, particu-
larly in the context of service robotics for elderly care. Their study focused on
addressing the challenge of clothing manipulation, crucial for assisting elderly
individuals in household settings. By proposing four different neural network
modelsandtestingthemonvariousdatasets,includingFashion-MNISTandcus-
tomized datasets, Nocentini et al. achieved significant advancements. Notably,
their model, the Multiple Convolutional Neural Network with 15 convolutional
layers (MCNN15), outperformed previous literature with a classification accu-
racyof94.04%ontheFashion-MNISTdataset.Thisresearchunderscorestheim-
portance of advanced neural network models in effectively addressing practical20 Bouzidi et al.
challenges, such as object-based manipulation tasks in household environments,
thus contributing to the development of service robotics for eldercare.
Erko¸c et al.[38]introduced a unique training algorithm for convolutional lay-
ers in CNNs, operating without supervision or backpropagation. This method
facilitates feature extraction from unlabeled data, automatically determining
the optimal number of filters post-filter extraction and eliminating the need
for weight initialization mechanisms. The approach simplifies CNN training, re-
ducing reliance on labeled data and hyperparameter tuning. Impressively, on
datasets like MNIST, EMNIST-Digits, Kuzushiji-MNIST, and Fashion-MNIST,
the method achieved high test performances of 99.19%, 99.39%, 95.03%, and
90.11%, respectively, without using backpropagation or preprocessed data.
Swain et al. [39] introduced a computer vision solution employing Convolu-
tional Neural Networks (CNN) and the LeNet model to assist visually impaired
individuals in recognizing clothing items. They utilized the Fashion-MNIST
dataset for training and validation. The CNN model attained an accuracy of
93.7%, while the LeNet model demonstrated superior performance with an ac-
curacyof98.4%.Theseresultsunderscoretheeffectivenessofdeeplearningtech-
niques in facilitating clothing classification tasks for individuals with visual im-
pairments.
Yu et al. [40] introduced a novel method for clothing classification utilizing
a convolutional neural network (CNN) architecture ”FFENet( frequency-spatial
feature enhancement network for clothing classification). Their approach incor-
porates a Discrete Cosine Transform (DCT) frequency domain enhancement
module to extract spatial information from various frequency bands of the im-
age. These spatial feature maps are then merged to create a comprehensive fea-
ture map, which undergoes further processing via a clothing feature extraction
module.Themethodconcludeswithasequenceofoperations,includinga1×1
convolution,globalaveragepooling,andtwofullyconnectedlayers,culminating
in the final clothing classification outcomes. Notably, the method achieved an
accuracyof94.62%ontheFashionMNISTdatasetanddemonstratedpromising
performance on the Clothing-8 dataset.
Wan et al. [41] introduce LMFRNet, a lightweight convolutional neural net-
work (CNN) model tailored for resource-constrained environments. The innova-
tionofLMFRNetliesinitsmulti-featureblockdesign,whicheffectivelyreduces
model complexity and computational load while maintaining high performance.
Remarkably, achieving an accuracy of 94.6% on the CIFAR-10 dataset, LMFR-
Net demonstrates remarkable efficiency in image analysis tasks. Furthermore,
the authors validate the model’s performance across various datasets, includ-
ing CIFAR-100, MNIST, and Fashion-MNIST, showcasing its robustness and
adaptability. Particularly noteworthy is LMFRNet’s accuracy of 94.11% on the
Fashion-MNIST dataset, highlighting its suitability for real-world applications
with limited computational resources.
Sun et al. [42] advanced MADPL-net, an innovative end-to-end model in-
tegrating CNN learning, deep encoder learning, and attention dictionary pair
learning (ADicL) in a unified framework. MADPL-net addresses limitations ofCNN and VIT for Fashion MNIST Classification : A Litterature review 21
previous methods by jointly updating dictionary learning and classification, en-
hancingclassificationperformance.TheincorporationofADicLenablesselection
of image-attentive atoms, improving classification capability. Empirical results
demonstrate MADPL-net’s superiority over existing methods, achieving an im-
pressive 91.24% accuracy.
The approach devised by Shin et al. [43]for the classification of clothing
imagesutilizesanenhancedCNNarchitecture,integratingrefinedconvolutional
andpoolinglayersactivatedbyReLU,alongwithprudentmanagementofdropout
layers to mitigate overfitting risks. They introduced a novel dynamic learning
rate approximation method, facilitating automatic adjustment during training,
consequently reducing training duration and enhancing convergence. Perfor-
mance evaluation conducted on the Fashion-MNIST dataset yielded a notable
classification accuracy of 93%, marking a 4.6% advancement compared to the
baselineCNNmodel.Thisinnovativemethodologyoffersapromisingavenuefor
clothing image recognition tasks
Intheirresearch,METLEKetal.[44]introduceCNNTuner,anovelmodelde-
signedforclothingimageclassification.TheyutilizetheFashionMNISTdataset
as the basis for their evaluation. The CNNTuner architecture consists of 15 lay-
ers, with hyperparameter optimization achieved through the integration of the
Keras Tuner tool. Important parameters like the number of filters in convolu-
tionallayers,windowsizes,andactivationfunctionsareautomaticallyselectedto
ensure seamless compatibility across different layers. Through experimentation,
theauthorsachieveremarkableperformancemetrics,includingF1scores,recall,
precision,specificity,andaccuracy,allexceeding0.99or99.18%.Thisautomated
approach represents a significant advancement in clothing image classification,
with potential applications in tasks such as clothing search and product recom-
mendation.
The table 8 summarizes the works mentioned above
3.2 ViTs for Fashion MNIST Classification
Mukherjeeetal.[45]developedagroundbreakingdeeplearningframework,OC-
Former, based on Vision Transformers (ViT), specifically designed for one-class
classification tasks. Unlike previous methods that employed multiple loss func-
tions,OCFormersimplifiestheprocessbyutilizingzero-centeredGaussiannoise
as a surrogate pseudo-negative class in the latent space representation, opti-
mizing the training process with a specialized loss function. Through extensive
experimentation on various datasets such as CIFAR-10, CIFAR-100, Fashion-
MNIST, and CelebA eyeglasses datasets, the effectiveness of OCFormer was
demonstrated. It outperformed conventional CNN-based one-class classification
techniques, achieving an impressive accuracy of 92.71%.
Chhabra et al. [46] offered PatchRot, a self-supervised technique tailored for
vision transformers. PatchRot involves rotating images and image patches and
training the network to predict these rotation angles, enabling the network to
extract both global and local features effectively. Through extensive experimen-
tation on various datasets, PatchRot training was found to learn rich features22 Bouzidi et al.
Table 8. Existing CNNs Approaches in Literature
Years References Methods Performances
2021 [35] CNN-dropout-3 Accuracy: 99.1%
2021 [36] CNN Accuracy: 93.68%
2022 [37] MCNN15 Accuracy: 94.04%
2023 [38] Unsupervised Filter Learning Accuracy: 90.11%
2023 [39]
– CNN – Accuracy: 93.7%
– LeNet – Accuracy: 98.4%
2023 [40] FFENet Accuracy: 94.62%
2023 [41] LMFRNet Accuracy: 94.11%
2023 [42] MADPL-net Accuracy: 91.24%
2023 [43] CNN Accuracy: 93%
2023 [44] CNNTuner
– Accuracy: 99.18%
– Precision: 99%
– F1 score: 99%
– Recall: 99%
– Specificity: 99%
– F2 score: 99%
surpassing those achieved by supervised learning and benchmarking. This ap-
proach yielded an accuracy of 92.6%.
Chhabra et al. [47] proposed a regularization technique called PatchSwap,
where patches are exchanged between two images, creating a fresh input for
transformerregularization.Throughcomprehensiveexperimentation,itwasfound
that PatchSwap outperforms existing advanced methods. Notably, its straight-
forwardimplementationfacilitatesasmoothtransitiontosemi-supervisedlearn-
ing environments with minimal complexity. Impressively, utilizing PatchSwap
achieved an accuracy of 92.6%, highlighting its effectiveness in improving model
performance.
Bacochinaetal.[48]introducedanewattentionmechanismcalled”Element-
WiseAttentionLayers,”whichaimstooptimizeDot-ProductAttentionbytran-
sitioning from matrix to element-wise operations. This approach was evaluated
on Fashion MNIST and CIFAR10 datasets using models with VGG-like archi-
tectures.Resultsdemonstrateasignificantimprovementinparameterefficiency,
withtheproposedmechanismachieving92%accuracyonFashionMNISTwhile
reducing parameter count by 97%. For CIFAR10, accuracy remains competitive
at 60% of the VGG-like counterpart while utilizing 50% fewer parameters. This
highlights the effectiveness of the element-wise attention approach in enhancing
model performance and efficiency.CNN and VIT for Fashion MNIST Classification : A Litterature review 23
Abd Alaziz et al. [49] conducted a study with the aim of refining fashion
image classification techniques. Their research introduced a novel methodology
employing Vision Transformer (ViT) architecture, which integrates transformer
blocks comprising self-attention and multilayer perceptron layers. The authors
meticulously fine-tuned the hyperparameters of the ViT model to maximize its
efficacy. Comparative assessments were conducted with other models, encom-
passing various CNN architectures such as VGG16, DenseNet-121, Mobilenet,
and ResNet50, across two distinct fashion image datasets. Evaluation metrics,
includingaccuracy,precision,recall,F1-score,andAUC,wereemployedtogauge
model performance. The findings underscored ViT’s superiority over alternative
models, illustrating its effectiveness in fashion image classification tasks. For in-
stance, on the Fashion-MNIST dataset, ViT achieved notable results with an
accuracy of 95.25%, precision of 95.20%, recall of 95.25%, and an F1-score of
95.20%. This extensive evaluation highlights ViT’s significant potential in ad-
vancing fashion image classification methodologies.
Rodriguezetal.[50]proposedamethodtoenhanceprivacyinimageclassifi-
cationbyemployingtwodifferentimagetransformationtechniques:oneutilizing
convolutional autoencoder (CAE) latent representation and the other utilizing
visiontransformer(ViT)embeddings.Theseapproachesaredesignedtodevelop
classification models while concealing sensitive image information and improv-
ing resistance against reconstruction attacks. We assessed their performance
across multiple datasets including Fashion MNIST, CIFAR-10, and Chest X-
ray. The CAE-based method achieved a classification accuracy of 91.43% on
FashionMNIST,whiletheViT-basedapproachattained87.32%.Thesefindings
underscoretheeffectivenessofbothstrategiesinmaintainingmodelutilitywhile
bolstering privacy measures.
Shahetal.[51]introducedanovelmethodcalledSWEKP-basedViT(Stochas-
tic Weighted Composition of Contrastive Embeddings & Divergent Knowledge
DispersionforHeterogeneousPatchEncoding),aimedatenhancingVisionTrans-
formers (ViT) for image recognition tasks. This approach modifies the patch
encoding module to create heterogeneous embeddings, incorporating traditional
linear projection and positional embeddings along with three additional embed-
dings:SpatialGated,FourierTokenMixing,andMulti-layerperceptronMixture
embedding.Additionally,aDivergentKnowledgeDispersion(DKD)mechanism
isproposedtoefficientlypropagatelatentinformationacrossthetransformernet-
work.Theperformanceofthemodelwasevaluatedonfourbenchmarkdatasets:
MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100. Notably, on the Fashion-
MNIST dataset, the proposed method achieved an accuracy of 93.57%, repre-
senting an improvement over conventional ViT models.
Xu et al. [52] introduce a groundbreaking method named Optronic Vision
Transformer (OPViT), which integrates spatial light modulators (SLMs) and
lens groups to facilitate matrix multiplication, heralding the introduction of
Transformer into optical neural networks. This innovative approach maximizes
theadvantagesofphotoniccomputing,leadingtoasignificantreductionincom-
putational costs. OPViT exhibits reconfigurability, scalability, and low space24 Bouzidi et al.
complexity. Remarkably, with just one layer of Transformer and two or three
layers of optical convolution, OPViT achieves remarkable classification results.
The authors evaluate this approach on both the MNIST and Fashion-MNIST
datasets, achieving a test accuracy of 88.93% on the Fashion-MNIST dataset,
surpassing existing optical convolutional neural networks and rivalling previous
optical architectures connected to electronic networks.
The Table 9, provides a concise overview of the previously discussed works.
Table 9. Existing CNNs Approaches in Literature
Years References Methods Performances
2022 [45] OCFormer Accuracy: 92.71%
2022 [46] PatchRot Accuracy: 92.6%
2022 [47] PatchSwap Accuracy: 92.6%
2023 [48] Element-Wise Attention Layers Accuracy: 92.87%
2023 [49] ViT
– Accuracy: 95.25%
– Precision: 95.20%
– Recall: 95.25%
– F1-score: 95.20%
2023 [50] ViT Accuracy: 87.32%
2023 [51] SWEKP-ViT Accuracy: 93.57%
2023 [52] OPViT Accuracy: 88.93%
3.3 Hybrid Models for Fashion MNIST Classification
Shao et al. [53] have introduced a groundbreaking visual transformer architec-
turecalledTSD,whichblendsconvolutionalneuralnetworks(CNNs)andtrans-
formers using novel blocks such as CPSA and LFFN in an parallel Integration.
This innovative design captures both local and long-range image details effec-
tively. Experiments demonstrated that training the TSD model from scratch
on small datasets resulted in exceptional performance while keeping the model
lightweightandefficient.TheTSDmodeloutperformedexistingCNN-basedand
transformer-based models, reaching a top-1 accuracy of 96.56% on the Fashion
MNISTdatasetwithfewercomputationsandparameters.Theresearchersfound
a way to eliminate the need for positional token encoding without sacrificing
model performance, simplifying the design and allowing for greater flexibility in
adjustinginputimageresolutions.Thisadaptabilityiscrucialforvariousvision-
based applications. Overall, the TSD model achieved an outstanding accuracy
of 96.56% on the Fashion MNIST dataset.
Coolsetal.[54]introducedapioneeringhybridarchitectureknownasCAReNet
(ConvolutionalAttentionResidualNetwork),whichintegratesconvolutionalneu-
ral networks (CNN) with attention mechanisms in an Hierarchical integrationCNN and VIT for Fashion MNIST Classification : A Litterature review 25
for image classification tasks. This architecture was developed from scratch
to mitigate architectural disparities often encountered with pre-trained mod-
els. CAReNet comprises an initial convolutional block for fundamental feature
extraction, followed by parallel blocks incorporating bottleneck structures and
residual units to enhance feature extraction and information flow. These fea-
tures are merged through element-wise addition, followed by max pooling for
spatialhierarchyrefinement.Thismodularapproachisiteratedthricewithvary-
ing channel dimensions to capture multi-scale features. The architecture also
includes a CareNet block featuring bottleneck structures and attention mecha-
nisms operating in grid and window patterns, followed by feature averaging for
consolidation and consistent propagation through an additional residual unit.
CAReNet demonstrates competitive performance across benchmark datasets,
achieving notable accuracy, including an impressive 95% top1 validation accu-
racy on the Fashion-MNIST dataset, showcasing its efficacy in diverse image
classification tasks.
Meng et al. [55] have presented an innovative method called MixMobileNet
that blends convolutional neural networks (CNNs) with vision transformers
(ViTs) in an architectural hybrid to enhance both local and global image fea-
ture extraction. This technique resulted in a high accuracy of 95.37% on the
Fashion MNIST dataset. MixMobileNet introduces MMb (MixMobile) blocks,
which encompass an LFAE (Local-Feature Aggregation Encoder) for extract-
ing local features and a GFAE (Global-Feature Aggregation Encoder) for cap-
turing global features. The GFAE encoder optimizes computation by reducing
the channel dimensions and applying feature attention calculations per channel.
Meanwhile,theLFAEencoderusesadaptivePConv(Partial-Conv)convolutions
to capture local features at different scales within an image. MixMobileNet has
demonstratedversatilityandeffectivenessinimageclassificationacrossmultiple
datasets.
Xu et al. [57] have introduced a novel method called HSViT (Horizontally
Scalable Vision Transformer) to address key obstacles in computer vision. By
merging the strengths of convolutional neural networks (CNNs) with Vision
Transformer architectures (ViT) in an Hierarchical Integration, HSViT resolves
the lack of pre-training on large datasets and limitations of mobile computing
resources. This approach uses image-level embeddings to allow ViT to leverage
the inductive bias of CNNs. The horizontally scalable architecture minimizes
layers and parameters and enables collaborative training across multiple nodes.
Experiments, particularly with Fashion-MNIST, show HSViT achieving up to
10% higher top-1 accuracy compared to other methods, making it a promising
solution for efficient learning and inference across different platforms.
The table 10 summarizes the works mentioned above
4 Discussion
In this section, we delve into various essential aspects, including the significance
ofarchitecturalchoicesbycomparingtheadvantagesanddrawbacksofCNNand26 Bouzidi et al.
Table 10. Existing Hybrid ViT with CNN Approaches in Literature.
Years References Methods Types of hybridiza-Performances
tion
2022 [53] TSD Parallel Integration Accuracy: 96.56%
2023 [54] CAReNet Hierarchical Integration Top1 validation accu-
racy: 95%
2024 [55] MixMobileNetSequential Integration accuracy: 95.37%
2024 [57] HSViT Hierarchical Integration Top-1 Accuracy: 95.92%
ViTmodels.We considerfusionmethodsthatleveragethestrengthsofbothar-
chitecturesandtheroleofhyperparametertuningforoptimizingbothCNNand
ViT models performance. Additionally, we examine the practical uses of these
models in the fashion industry, such as clothing classification, recommendation
systems, virtual try-on technologies, and trend analysis. Lastly, we discuss ex-
isting limitations and challenges, such as data scarcity, domain adaptation, and
model interpretability, offering insights into possible future research directions
and areas for improvement.
4.1 Importance of Architecture Choice (CNN, ViT)
When considering image classification with the Fashion MNIST dataset, the
selection of architecture, CNNs or ViTs is pivotal to achieving optimal model
performance. Both architectures have shown their strengths and weaknesses in
various approaches.
CNN-basedmethods,suchasCNN-dropout-3andCNNTuner,haveachieved
impressive accuracy rates of 99.1% and 99.18% respectively. Their success is
largely due to their ability to effectively capture localized features in images
andadaptwelltoimageclassificationtasks.However,otherCNNstrategies,like
the standardCNN methods (93.68% and94.04% accuracy), have shown slightly
lower outcomes.
ViTs also demonstrate robust results in image classification. Strategies such
as SWEKP-ViT and ViT have yielded accuracies of 93.57% and 95.25% respec-
tively.ViTsleverageattentionmechanismstoidentifyglobalpatternsinimages,
which may lead to stronger generalization and precision.
Thatsaid,theperformanceofViTmethodscanvaryacrossdifferentmodels,
as evidenced by the range of accuracies observed (87.32% to 95.25%). These
variationshighlighttheimportanceofchoosingamethodtailoredtothespecific
task and dataset.
Overall, both CNNs and ViTs have distinct advantages in classifying images
in the Fashion MNIST dataset. CNNs are a reliable option due to their strong
trackrecordandadaptability,whileViTsbringfreshperspectiveswiththeiruse
of attention mechanisms. There is potential in combining CNNs and ViTs to
create hybrid models that can capitalize on the strengths of both architectures
usingCNNsforeffectivelocalfeatureextractionandViTsforbroadercontextual
understanding.CNN and VIT for Fashion MNIST Classification : A Litterature review 27
4.2 Fusion Approaches
HybridmodelsthatintegrateCNNsandViTshavedemonstratedpromisingpo-
tential in image classification tasks. These approaches merge the local feature
extraction capabilities of CNNs with the broader context understanding offered
byViTsattentionmechanisms.Forinstanceasshownintable10,TSDachieved
an accuracy of 96.56%, showcasing the benefits of combining the two architec-
tures. Similarly, CAReNet and MixMobileNet reached accuracies of 95% and
95.37%respectively,andHSViTachievedaTop-1accuracyof95.92%highlight-
ing the reliability of these models.
Fusing CNNs and ViTs can create models that are more robust and adapt-
able for various image classification tasks. As research in this area progresses,
thesehybridapproachescouldbecomecrucialforenhancingimageclassification
performance, offering new and promising directions for visual AI. However, it’s
important to acknowledge that while this fusion is indeed effective, it brings
along its own set of challenges. One such challenge is the increased complexity
and computational costs associated with integrating two distinct architectures.
CNNs and ViTs, both inherently complex models, when combined, substan-
tiallyescalatethecomputationalrequirements.Thisheightenedcomplexitymay
pose significant hurdles, particularly in resource-constrained settings, necessi-
tating more powerful hardware and potentially elongating the training process.
Furthermore, the amalgamation of these architectures could potentially obscure
model interpretability and lengthen development cycles, as ensuring seamless
integration and optimization may demand additional time and effort.
4.3 Hyperparameters Selection
Indeeplearning,parametersandhyperparametersplayadistinctroleinshaping
and training models [80]. Parameters are the variables that are learned and
adjusted by the model during training, such as weights and biases in neural
networks [81]. These values change as the model is trained to minimize the loss
function value and improve its performance on the task at hand.
Hyperparameters, on the other hand, are the settings that define the archi-
tectureandtrainingprocessofthemodel[82][83].Theyincludeaspectssuchas
learning rate, batch size, the number of layers, and architecture-specific options
like patch size in ViTs or filter size in CNNs. Unlike parameters, hyperparam-
eters are not learned during training; instead, they are preset before training
begins. They play a crucial role in the overall behavior and efficiency of the
model, impacting its learning speed, accuracy, and ability to generalize to new
data.Choosingappropriatehyperparametersiskeytoachievingthebestpossible
performance from a model.
CNN Hyperparameters: CNNs rely heavily on hyperparameters to shape
their architecture and tune performance in image classification tasks. These hy-
perparametersinfluencevariousaspectsofthemodel,affectingthemodel’sabil-28 Bouzidi et al.
ity to learn and generalize effectively. The following are some instances of how
changing hyperparameters might improve model performance:
– Number of layers: The intricacy and capacity of a neural network can be
influencedbythecountofhiddenlayerswithinthemodel.Morehiddenlayers
typicallyallowthenetworktolearnmorecomplexpatternsandrelationships
in the data.
– Activation functions: are crucial for transforming the input each neuron re-
ceives,enablingthemodeltolearnbetterrepresentations.Forexample,Rec-
tifiedLinearUnit(ReLU)andthesoftmaxfunctionarefrequentlyemployed
in deep learning due to their effectiveness and their ability to address the
vanishing gradient issue.
– Learning rate: determines the step size at which the model updates its
weights during training. It is important to choose an appropriate learning
rate; a high rate can cause the model to converge too quickly or overshoot
the optimal solution, while a low rate may lead to slow convergence.
– Optimizers: such as Stochastic Gradient Descent (SGD), Adam, Adagrad,
and RMSprop adjust the model’s weights and biases based on the gradient
of the loss function. Each optimizer has its own set of hyperparameters that
can be fine-tuned to improve model performance.
– Number of Epochs: refers to the number of times the model goes through
theentiretrainingdataset.Increasingthenumberofepochscanenhancethe
model’s performance, but excessive training can result in overfitting, where
themodelperformswellonthetrainingdatabutstruggleswithnew,unseen
data.
– Dropout Rate: helps prevent overfitting by randomly deactivating connec-
tions during training, promoting better generalization.
– Kernel Size: affects the receptive field of convolutional layers, impacting the
model’s ability to capture features from the input data. Larger kernels can
capture broader patterns, while smaller kernels focus on finer details. The
choice of kernel size should align with the dataset’s characteristics and the
features of interest.
– BatchSize:Referstothenumberofdatasamplesthemodelprocessesatonce
during training. It affects the speed of learning and the amount of memory
needed. Choosing the right batch size helps balance training efficiency and
model stability.
Fine-tuning hyperparameters is crucial for optimizing the performance and ef-
ficiency of CNN architectures. Selecting the best hyperparameter values can
significantly impact how effectively a model learns from data, its convergence
speed, and its ability to generalize to new data. By adjusting these settings,
one can customize a model’s behavior to suit a specific task or dataset, leading
to improved accuracy and robustness. Table 11 provides a detailed look at the
hyperparameters used in various CNN architectures with the Fashion MNIST
dataset. This analysis enables researchers and practitioners to make thoughtful
decisions when building models, helping to maximize performance and achieve
optimal results.CNN and VIT for Fashion MNIST Classification : A Litterature review 29
Table 11. Hyperparameters of CNN architectures in various studies
Hyperparameters Values References
Batch size 123, 8, 128 [35], [40], [43]
Number of layers 2, 13, 5, 3, 15 [35] [39], [36], [37][38], [41], [44]
Activation functions SoftMax, ReLU and[35], [36], [37] [38][39] [43] [44], [41]
SoftMax, ReLU
Learning rate 0.001, 0.01, 0.1 [36] [37][39], [40], [41] [43]
Optimizers Adadelta, Adam,[35][38],[36][37][39][44],[40],[41]
SGD, Adam and
SGD
Number of Epochs 12, 50, 20, 100 [35], [38], [44], [37]
Dropout Rate 0.5 [38]
ViT Hyperparameters: Vision Transformers (ViT) use specific hyperparam-
eters to shape their architecture and tune performance in image classification
tasks. These hyperparameters impact the model’s learning process and its abil-
ity to generalize effectively to new data. Below are key hyperparameters that
can be adjusted to enhance the performance of ViT models:
– Number of layers: This parameter controls the depth of the model and its
capacity to understand complex relationships within the image data. More
layers can lead to a greater ability to capture intricate patterns.
– Embedding size: The size of the embedding space affects how the model
representsinputdata.Largerembeddingsizesprovidemoreinformationbut
also increase model complexity.
– Patch size: In ViT, images are split into fixed-size patches. Patch size deter-
mines the granularity of the visual data, influencing the model’s accuracy
and learning process.
– Attention head size: The attention mechanism helps the model focus on
relevant parts of the image. The head size influences how the model assigns
attention across different areas, affecting performance.
– Dropout rate: Dropout is used to reduce overfitting by randomly dropping
connections during training, thus encouraging better generalization.
– Activation function: Activation functions, such as ReLU or GELU, are used
to process data in each layer. The choice of activation function can impact
model performance.
– Optimizer: Optimizers like Adam, Adagrad, or RMSprop manage weight
adjustments during training. The choice of optimizer and its associated pa-
rameters significantly affect model performance and efficiency.
– Number of attention heads: Controls how many parallel attention layers are
used. Each head independently examines the input data. This hyperparam-
eter plays a key role in how the model captures and interprets different
features.
Fine-tuning hyperparameters is essential for achieving optimal performance
and efficiency in ViT architectures. Adjusting these parameters can influence30 Bouzidi et al.
how effectively a model learns, its convergence rate, and its ability to generalize
tonewdata.Customizingthemodel’sbehaviorforspecifictasksordatasetscan
lead to improved accuracy and robustness.
Table12outlinesthehyperparametersusedinvariousViTarchitectureswith
the Fashion MNIST dataset. This overview supports researchers and practition-
ers in making informed decisions when designing models, ensuring choices made
enhance model performance and contribute to achieving superior outcomes.
Table 12. Hyperparameters of ViT architectures in various studies
Hyperparameters Values References
Epochs 15, 10, 300, 30, 60 [45], [46], [47], [49], [51]
Embedding size 768 and 102, 256 [45], [46]
Encoder block 12 and 24, 6 [45], [46] [47]
Number of attention4 [47]
heads
Batch size 128, 256, 32 [46] [47],[49], [51]
Weight decay 3∗102, 0.03 [46], [47]
Patch size 64, 4 and 8, 4 [45], [46], [47]
Activation function GELU, Softmax [45], [45] [49]
Learning rate 1e4,5∗104,5∗104,0.1,0.001[45], [47], [49], [51]
Dropout rate 0.1 [47]
Optimizer Adam [45] [46] [49] [51]
4.4 Fashion Sector Particularities
Theincorporationofadvancedimageclassificationandobjectdetectionmethods
inthefashionindustryhasrevolutionizedtheshoppingexperienceforconsumers
while offering brands invaluable insights. Personalized recommendation systems
leverage these technologies to provide individualized outfit suggestions based on
a customer’s tastes and preferences, enhancing their shopping experience and
boosting sales by aligning recommendations with their style [74] [75]. Virtual
try-on applications enable customers to visualize how clothing and accessories
willlookontheminreal-time,usingobjectdetectionandclassificationtocreate
an engaging and interactive online shopping experience [76] [77][78]. This inno-
vation increases customer confidence in purchases and can lead to lower return
rates. Trend analysis tools utilize image recognition and classification to help
fashionbrandsmonitorandpredictemergingtrendsbyanalyzinglargedatasets
of fashion images. These insights support strategic decisions in product design
and marketing by highlighting popular styles, colors, and patterns [79].
Research in the field highlights the benefits of these applications, such as
high-accuracy techniques for categorizing fashion items like clothes and acces-
sories. This assists with inventory management and precise advertising. ObjectCNN and VIT for Fashion MNIST Classification : A Litterature review 31
detection algorithms can also classify fashion items in images, aiding in trend
forecasting and brand positioning.
The fashion industry has emerged as a pivotal area for image classifica-
tion tasks within the realm of computer vision, outpacing other industries in
terms of significance and complexity. This is partly due to the expanding size of
datasets,witheachnewdatasetfeaturinganincreasingnumberofclasses.These
classesencompassavarietyofsubcategories,suchasdifferentsizes,genders,col-
ors, brands, and other attributes. Furthermore, the dynamic nature of fashion
trendsmeansthatclasseswithindatasetsevolveperiodically.Thesecharacteris-
ticspresentbothchallengesandopportunitiesforimprovingimageclassification
tasks.Classificationmodelsmustadapttotherapidlychangingstylesandtrends
within the fashion industry, which helps enhance their accuracy and efficiency
over time. By staying current with the ever-shifting landscape of fashion, image
classification systems can achieve greater success in this sector.
4.5 Limitations and Challenges
When comparing ViTs and CNNs for Fashion MNIST image classification, sev-
eral limitations and challenges should be taken into account to evaluate the
performance and suitability of each architecture.
Data scarcity can pose a challenge for training both ViTs and CNNs, as di-
verse and high-quality datasets are essential for building models that generalize
effectively across various fashion items. Data augmentation or transfer learning
may be needed when data is limited. Another concern is domain adaptation,
as models trained on Fashion MNIST may not perform well on other fashion
datasets due to differences in styles and trends. This is particularly relevant for
ViTs,whichrelyonhigh-qualityinputdataforeffectiveattention-basedprocess-
ing.Modelinterpretabilityisacrucialconsideration,especiallygiventheopaque
natureofdeeplearningmodels.Understandinghowamodelarrivesatitsclassi-
fications can build trust and offer insights into how each architecture processes
fashion images, which is important in comparing ViTs and CNNs. Biases in
data or models can also lead to incorrect classifications and the reinforcement
of stereotypes. Minimizing biases and ensuring fair, accurate classifications are
essentialgoalsforbothViTsandCNNs.Managingcomputationalcomplexityis
another key challenge, particularly when balancing performance with resource
usage.ViTscanbemorecomputationallyintensivethanCNNs,whichmaylimit
their practicality for certain applications.
Despite these limitations and challenges, ongoing research and development
in ViTs and CNNs for Fashion MNIST classification continue to advance the
field, offering opportunities for more efficient and reliable models in the future.
5 Conclusion
Afterthoroughlyexaminingtheapplicationofdeeplearningtechniques,particu-
larly CNNs and ViTs, in Fashion MNIST image classification tasks, it’s evident32 Bouzidi et al.
that these methods have significantly propelled the field forward. The stud-
ies reviewed showcase the effectiveness of both CNNs and ViTs in accurately
categorizing fashion items, achieving notable milestones in classification accu-
racy and model efficiency. However, despite their successes, several challenges
persist, including computational complexity, sensitivity to hyperparameters, in-
terpretability issues, and reliance on labeled data.
ThediscussionsectiondelvesintothestrengthsandweaknessesofCNNsand
ViTs,emphasizingthenecessityforongoingresearchtotackletheselimitations.
Additionally, it explores the potential of hybrid approaches that combine CNNs
and ViTs, suggesting promising avenues for future exploration. Furthermore,
the discussion underscores the importance of enhancing model interpretability,
robustness, and the utilization of unlabeled data to bolster model performance
and versatility.
Lookingahead,it’scrucialtoconcentrateonoptimizingmodelarchitectures,
improving interpretability, harnessing unlabeled data, fortifying models against
adversarialattacks,andinvestigatingnovelhybridapproaches.Thesefuturetra-
jectories hold significant potential for advancing Fashion MNIST classification
and deep learning methodologies overall, facilitating the development of more
effective, interpretable, and resilient models.
References
1. Cunha, Maria Nascimento, et al. ”Digital Lens: Exploring Customer Perceptions
in the World of E-Commerce and E-Marketplaces.” (2024): 150-167.
2. Micu, Adrian, et al. ”Assessing an on-site customer profiling and hyper-
personalization system prototype based on a deep learning approach.” Techno-
logical Forecasting and Social Change 174 (2022): 121289.
3. Heni, Ashref, Imen jdey, and Hela Ltifi. ”Blood Cells Classification Using Deep
Learning With Customized Data Augmentation and Using Deep Learning With
Custmized Data Augmentation And EK-Means Segmentarion.” Journal of Theo-
retical and Applied Information Technology 101.3 (2023): 1-12.
4. Deldjoo,Yashar,etal.”Areviewofmodernfashionrecommendersystems.”ACM
Computing Surveys 56.4 (2023): 1-37.
5. Dargan, Shaveta, et al. ”A survey of deep learning and its applications: a new
paradigm to machine learning.” Archives of Computational Methods in Engineer-
ing 27 (2020): 1071-1092.
6. Vijayaraj,A.,etal.”Deeplearningimageclassificationforfashiondesign.”Wireless
Communications and Mobile Computing 2022 (2022).
7. Maur´ıcio,Jos´e,InˆesDomingues,andJorgeBernardino.”ComparingVisionTrans-
formersandConvolutionalNeuralNetworksforImageClassification:ALiterature
Review.” Applied Sciences 13.9 (2023): 5521.
8. Lee, Chin Poo, et al. ”Plant-CNN-ViT: plant classification with ensemble of con-
volutional neural networks and vision transformer.” Plants 12.14 (2023): 2642.
9. Cong, Shuang, and Yang Zhou. ”A review of convolutional neural network ar-
chitectures and their optimizations.” Artificial Intelligence Review 56.3 (2023):
1905-1969.CNN and VIT for Fashion MNIST Classification : A Litterature review 33
10. Slimani, Nawel, Imen Jdey, and Monji Kherallah. ”Performance comparison of
machinelearningmethodsbasedonCNNforsatelliteimageryclassification.”2023
9th International Conference on Control, Decision and Information Technologies
(CoDIT). IEEE, 2023.
11. Hcini, G. H. A. Z. A. L. A., et al. ”Hyperparameter optimization in customized
convolutional neural network for blood cells classification.” J. Theor. Appl. Inf.
Technol 99 (2021): 5425-5435.
12. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ”ImageNet classifica-
tionwithdeepconvolutionalneuralnetworks.”CommunicationsoftheACM60.6
(2017): 84-90.
13. Muhammad, Usman, et al. ”Pre-trained VGGNet architecture for remote-sensing
imagesceneclassification.”201824thInternationalConferenceonPatternRecog-
nition (ICPR). IEEE, 2018.
14. Haripriya, P., G. Parthiban, and R. Porkodi. ”A STUDY ON CNN ARCHITEC-
TUREOFVGGNETANDRESNETFORDICOMIMAGECLASSIFICATION.”
NeuroQuantology 20.16 (2022): 2027.
15. He,Kaiming,etal.”Deepresiduallearningforimagerecognition.”Proceedingsof
the IEEE conference on computer vision and pattern recognition. 2016.
16. Qomariah, Dinial Utami Nurul, Handayani Tjandrasa, and Chastine Fatichah.
”Classification of diabetic retinopathy and normal retinal images using CNN and
SVM.” 2019 12th International Conference on Information & Communication
Technology and System (ICTS). IEEE, 2019.
17. Iswanto, Irene Anindaputri, Amadeus Suryo Winoto, and Michael Kristianus.
”FruitsRecognitionusingDeepConvolutionalNeuralNetworkforLowComputing
Device.”Engineering,MAthematicsandComputerScienceJournal(EMACS)5.2
(2023): 85-91.
18. Jamil, Sonain, Md Jalil Piran, and Oh-Jin Kwon. ”A comprehensive survey of
transformers for computer vision.” Drones 7.5 (2023): 287.
19. Dosovitskiy,Alexey,etal.”Animageisworth16x16words:Transformersforimage
recognition at scale.” arXiv preprint arXiv:2010.11929 (2020): 1-22.
20. Khan,Salman,etal.”Transformersinvision:Asurvey.”ACMcomputingsurveys
(CSUR) 54.10s (2022): 1-41.
21. Bouzidi, Sonia, Imen Jdey, and Adel Alimi. ”A Vision Transformer Approach
withL2RegularizationforSustainableFashionClassification.”AvailableatSSRN
4686032.
22. Aladhadh, Suliman, et al. ”An effective skin cancer classification mechanism via
medical vision transformer.” Sensors 22.11 (2022): 4008.
23. Peng, Zhiliang, et al. ”Conformer: Local features coupling global representations
forvisualrecognition.”ProceedingsoftheIEEE/CVFinternationalconferenceon
computer vision. 2021.
24. Dai,Zihang,etal.”Coatnet:Marryingconvolutionandattentionforalldatasizes.”
Advances in neural information processing systems 34 (2021): 3965-3977.
25. Yunusa,Haruna,etal.”ExploringtheSynergiesofHybridCNNsandViTsArchi-
tecturesforComputerVision:Asurvey.”arXivpreprintarXiv:2402.02941(2024).
26. Xin, Jia, et al. ”Convolutional Neural Network for Fashion Images Classification
(Fashion-MNIST).”JournalofAppliedTechnologyandInnovation(e-ISSN:2600-
7304) 7.4 (2023): 11.
27. An,Hyosun,etal.”Conceptualframeworkofhybridstyleinfashionimagedatasets
for machine learning.” Fashion and Textiles 10.1 (2023): 1-18.34 Bouzidi et al.
28. Wu, Hui, et al. ”Fashion iq: A new dataset towards retrieving images by natural
languagefeedback.”ProceedingsoftheIEEE/CVFConferenceoncomputervision
and pattern recognition. 2021.
29. Hu, Shell Xu, et al. ”Pushing the limits of simple pipelines for few-shot learning:
External data and fine-tuning make a difference.” Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 2022.
30. Treneska,Sandra,andSonjaGievska.”Objectdetectionandinstancesegmentation
of fashion images.” Conference for Informatics and Information Technology. 2020.
31. Wang, Xinhui. ”Towards color compatibility in fashion using machine learning.”
(2019).
32. Bhatt, Dulari, et al. ”CNN variants for computer vision: History, architecture,
application, challenges and future scope.” Electronics 10.20 (2021): 2470.
33. Rathore, Bharati. ”Cloaked in Code: AI & Machine Learning Advancements in
Fashion Marketing.” Eduzone: International Peer Reviewed/Refereed Multidisci-
plinary Journal 6.2 (2017): 25-31.
34. Xiao, Han, Kashif Rasul, and Roland Vollgraf. ”Fashion-mnist: a novel im-
age dataset for benchmarking machine learning algorithms.” arXiv preprint
arXiv:1708.07747 (2017).
35. LEITHARDT, VALDERI. ”Classifying garments from fashion-MNIST dataset
throughCNNs.” Advances inScience,Technologyand Engineering SystemsJour-
nal 6.1 (2021): 989-994.
36. Khanday, Owais Mujtaba, Samad Dadvandipour, and Mohd Aaqib Lone. ”Effect
offiltersizesonimageclassificationinCNN:AcasestudyonCFIR10andfashion-
MNISTdatasets.”IAESInternationalJournalofArtificialIntelligence10.4(2021):
872.
37. Nocentini, Olivia, et al. ”Image classification using multiple convolutional neural
networks on the fashion-MNIST dataset.” Sensors 22.23 (2022): 9544.
38. Erkoc¸, Tug˘ba, and Mustafa Taner Eskıl. ”A Novel Similarity Based Unsupervised
Technique for Training Convolutional Filters.” IEEE Access (2023).
39. Swain,Debabrata,etal.”AnIntelligentFashionObjectClassificationUsingCNN.”
EAI Endorsed Transactions on Industrial Networks and Intelligent Systems 10.4
(2023).
40. Yu,Feng,etal.”FFENet:frequency-spatialfeatureenhancementnetworkforcloth-
ing classification.” PeerJ Computer Science 9 (2023): e1555.
41. Wan,Guangquan,andLanYao.”LMFRNet:ALightweightConvolutionalNeural
Network Model for Image Analysis.” Electronics 13.1 (2023): 129.
42. Sun,Yulin,etal.”MADPL-net:Multi-layerattentiondictionarypairlearningnet-
work for image classification.” Journal of Visual Communication and Image Rep-
resentation 90 (2023): 103728.
43. Shin, Seong-Yoon, Gwanghyun Jo, and Guangxing Wang. ”A novel method for
fashion clothing image classification based on deep learning.” Journal of Informa-
tion and Communication Technology 22.1 (2023): 127-148.
44. METLEK, Sedat, and Halit C¸ETI˙NER. ”CNNTuner: Image Classification with
A Novel CNN Model Optimized Hyperparameters.” Bitlis Eren U¨niversitesi Fen
Bilimleri Dergisi 12.3: 746-763.
45. Mukherjee, Prerana, Chandan Kumar Roy, and Swalpa Kumar Roy. ”OC-
Former:One-ClassTransformerNetworkforImageClassification.”arXivpreprint
arXiv:2204.11449 (2022).
46. Chhabra,Sachin,etal.”PatchRot:ASelf-SupervisedTechniqueforTrainingVision
Transformers.” arXiv preprint arXiv:2210.15722 (2022).CNN and VIT for Fashion MNIST Classification : A Litterature review 35
47. Chhabra, Sachin, Hemanth Venkateswara, and Baoxin Li. ”PatchSwap: A Regu-
larization Technique for Vision Transformers.” (2022).
48. Bacochina, Giovanni Araujo, and Rodrigo Clemente Thom de Souza.
”Element-Wise Attention Layers: an option for optimization.” arXiv preprint
arXiv:2302.05488 (2023): 1-26.
49. Abd Alaziz, Hadeer M., et al. ”Enhancing Fashion Classification with Vision
Transformer (ViT) and Developing Recommendation Fashion Systems Using DI-
NOVA2.” Electronics 12.20 (2023): 4263.
50. Rodriguez,David,andRamKrishnan.”LearnableImageTransformationsforPri-
vacy Enhanced Deep Neural Networks.” 2023 5th IEEE International Conference
onTrust,PrivacyandSecurityinIntelligentSystemsandApplications(TPS-ISA).
IEEE Computer Society, 2023.
51. Shah,S.MuhammadAhmedHassan,etal.”AHybridNeuro-FuzzyApproachfor
Heterogeneous Patch Encoding in ViTs Using Contrastive Embeddings & Deep
Knowledge Dispersion.” IEEE Access (2023).
52. Xu,Chen,etal.”Transformerinoptronicneuralnetworksforimageclassification.”
Optics & Laser Technology 165 (2023): 109627.
53. Shao,Ran,andXiao-JunBi.”Transformersmeetsmalldatasets.”IEEEAccess10
(2022): 118454-118464.
54. Cools, Aur´elie, Sidi Ahmed Mahmoudi, and Mohammed Amin Belarbi.
”CARENET: A NOVEL ARCHITECTURE FOR LOW DATA REGIME MIX-
ING CONVOLUTIONS AND ATTENTION.” (2023).
55. Meng, Yanju, et al. ”MixMobileNet: A Mixed Mobile Network for Edge Vision
Applications.” Electronics 13.3 (2024): 519.
56. Hassani, Ali, and Humphrey Shi. ”Dilated neighborhood attention transformer.”
arXiv preprint arXiv:2209.15001 (2022).
57. Xu, Chenhao, et al. ”HSViT: Horizontally Scalable Vision Transformer.” arXiv
preprint arXiv:2404.05196 (2024).
58. Statista: https://www.wizishop.fr/blog/lancer-ecommerce.html.
59. Data Report: https://dash.app/blog/ecommerce-statistics.
60. Bazi,Yakoub,etal.”Visiontransformersforremotesensingimageclassification.”
Remote Sensing 13.3 (2021): 516.
61. Vilas, Martina G., Timothy Schaumlo¨ffel, and Gemma Roig. ”Analyzing Vision
Transformers for Image Classification in Class Embedding Space.” Advances in
Neural Information Processing Systems 36 (2024).
62. Strudel, Robin, et al. ”Segmenter: Transformer for semantic segmentation.” Pro-
ceedings of the IEEE/CVF international conference on computer vision. 2021.
63. d’Ascoli,St´ephane,etal.”Convit:Improvingvisiontransformerswithsoftconvo-
lutional inductive biases.” International conference on machine learning. PMLR,
2021.
64. Zhou,Tianfei,etal.”Rethinkingsemanticsegmentation:Aprototypeview.”Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion. 2022.
65. Linsley, Drew, et al. ”Learning long-range spatial dependencies with horizontal
gated recurrent units.” Advances in neural information processing systems 31
(2018).
66. Yang, Jianwei, et al. ”Focal attention for long-range interactions in vision trans-
formers.” Advances in Neural Information Processing Systems 34 (2021): 30008-
30022.
67. Simonyan, Karen, and Andrew Zisserman. ”Very deep convolutional networks for
large-scale image recognition.” arXiv preprint arXiv:1409.1556 (2014).36 Bouzidi et al.
68. Zhong,Zilong,etal.”Squeeze-and-attentionnetworksforsemanticsegmentation.”
Proceedings of the IEEE/CVF conference on computer vision and pattern recog-
nition. 2020.
69. Dosovitskiy,Alexey,etal.”Animageisworth16x16words:Transformersforimage
recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).
70. Deininger,Luca,etal.”Acomparativestudybetweenvisiontransformersandcnns
in digital pathology.” arXiv preprint arXiv:2206.00389 (2022).
71. Bello, Irwan. ”Lambdanetworks: Modeling long-range interactions without atten-
tion.” arXiv preprint arXiv:2102.08602 (2021).
72. Zhai,Xiaohua,etal.”Scalingvisiontransformers.”ProceedingsoftheIEEE/CVF
conference on computer vision and pattern recognition. 2022.
73. Zalando is the Europe’s largest online fashion platform: https://www.zalando.
com/
74. Chakraborty, Samit, et al. ”Fashion recommendation systems, models and meth-
ods: A review.” Informatics. Vol. 8. No. 3. MDPI, 2021.
75. Deldjoo,Yashar,etal.”Areviewofmodernfashionrecommendersystems.”ACM
Computing Surveys 56.4 (2023): 1-37.
76. Yuan, Miaolong, et al. ”A mixed reality virtual clothes try-on system.” IEEE
Transactions on Multimedia 15.8 (2013): 1958-1968.
77. ALZAMZAMI, Ohoud, et al. ”Smart Fitting: An Augmented Reality mobile ap-
plication for Virtual Try-On.” Romanian Journal of Information Technology and
Automatic Control 33.2 (2023): 103-118.
78. Kakade,Parul,etal.”DigitalApparelTry-Ons:ATechnologicalOdysseyintothe
World of Virtual Dressing Rooms and Consumer Engagement.” International Re-
searchJournalonAdvancedEngineeringandManagement(IRJAEM)2.02(2024):
12-19.
79. Rudniy,Alex,OlenaRudna,andArimPark.”Trendtrackingtoolsforthefashion
industry:theimpactofsocialmedia.”JournalofFashionMarketingandManage-
ment: An International Journal (2023).
80. Jdey, Imen, Ghazala Hcini, and Hela Ltifi. ”Deep learning and machine learning
forMalariadetection:Overview,challengesandfuturedirections.”arXivpreprint
arXiv:2209.13292 (2022).
81. Suo, Leiming, et al. ”Wind speed prediction by a swarm intelligence based deep
learning model via signal decomposition and parameter optimization using im-
proved chimp optimization algorithm.” Energy 276 (2023): 127526.
82. Ali, Yasser A., et al. ”Hyperparameter search for machine learning algorithms for
optimizing the computational complexity.” Processes 11.2 (2023): 349.
83. Bischl,Bernd,etal.”Hyperparameteroptimization:Foundations,algorithms,best
practices,andopenchallenges.”WileyInterdisciplinaryReviews:DataMiningand
Knowledge Discovery 13.2 (2023): e1484.
84. Rao,Jun,etal.”WatchandBuy:APracticalSolutionforReal-timeFashionProd-
uctIdentificationinLiveStream.”Proceedingsofthe1stWorkshoponMultimodal
Product Identification in Livestreaming and WAB Challenge. 2021.
85. Pang,Shanchen,etal.”Anefficientstylevirtualtryonnetworkforclothingbusi-
ness industry.” arXiv preprint arXiv:2105.13183 (2021).
86. Meng,Xiaoxu,WeikaiChen,andBoYang.”Neat:Learningneuralimplicitsurfaces
witharbitrarytopologiesfrommulti-viewimages.”ProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition. 2023.
87. Krichen,Moez.”Convolutionalneuralnetworks:Asurvey.”Computers12.8(2023):
151.CNN and VIT for Fashion MNIST Classification : A Litterature review 37
88. Patel, Sanskruti. ”A comprehensive analysis of Convolutional Neural Network
models.” International Journal of Advanced Science and Technology 29.4 (2020):
771-777.
89. Liu, Yang, et al. ”A survey of visual transformers.” IEEE Transactions on Neural
Networks and Learning Systems (2023).
90. Yin, Hongxu, et al. ”Adavit: Adaptive tokens for efficient vision transformer.”
arXiv preprint arXiv:2112.07658 (2021).
91. Touvron, Hugo, Matthieu Cord, and Herv´e J´egou. ”Deit iii: Revenge of the vit.”
European conference on computer vision. Cham: Springer Nature Switzerland,
2022.
92. Tang, Yehui, et al. ”A Survey on Transformer Compression.” arXiv preprint
arXiv:2402.05964 (2024).
93. Yuan,Li,etal.”Tokens-to-tokenvit:Trainingvisiontransformersfromscratchon
imagenet.” Proceedings of the IEEE/CVF international conference on computer
vision. 2021.
94. Gong, Chengyue, and Dilin Wang. ”NASViT: Neural architecture search for ef-
ficient vision transformers with gradient conflict-aware supernet training.” ICLR
Proceedings 2022 (2022).
95. Chen, Chun-Fu Richard, Quanfu Fan, and Rameswar Panda. ”Crossvit: Cross-
attention multi-scale vision transformer for image classification.” Proceedings of
the IEEE/CVF international conference on computer vision. 2021.
96. Touvron,Hugo,etal.”Goingdeeperwithimagetransformers.”Proceedingsofthe
IEEE/CVF international conference on computer vision. 2021.
97. Caron, Mathilde, et al. ”Emerging properties in self-supervised vision transform-
ers.” Proceedings of the IEEE/CVF international conference on computer vision.
2021.
98. Han, Kai, et al. ”Transformer in transformer.” Advances in neural information
processing systems 34 (2021): 15908-15919.
99. Liu, Ze, et al. ”Swin transformer: Hierarchical vision transformer using shifted
windows.” Proceedings of the IEEE/CVF international conference on computer
vision. 2021.
100. Rajaraman,Sivaramakrishnan,etal.”Pre-trainedconvolutionalneuralnetworks
as feature extractors toward improved malaria parasite detection in thin blood
smear images.” PeerJ 6 (2018): e4568.
101. Strisciuglio, Nicola, Manuel Lopez-Antequera, and Nicolai Petkov. ”Enhanced
robustness of convolutional networks with a push–pull inhibition layer.” Neural
Computing and Applications 32 (2020): 17957-17971.
102. Keele,Staffs.”Guidelinesforperformingsystematicliteraturereviewsinsoftware
engineering.” (2007).
103. Brahmi, Walid, and Imen Jdey. ”Automatic tooth instance segmentation and
identification from panoramic X-Ray images using deep CNN.” Multimedia Tools
and Applications (2023): 1-21.
104. https://keras.io/api/applications/
105. Liu,Zhenhua,etal.”Post-trainingquantizationforvisiontransformer.”Advances
in Neural Information Processing Systems 34 (2021): 28092-28103.