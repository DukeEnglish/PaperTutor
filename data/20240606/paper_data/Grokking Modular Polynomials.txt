ICLR2024WorkshopBGPT
GROKKING MODULAR POLYNOMIALS
DarshilDoshia,b,† TianyuHea,b AritraDasb AndreyGromova,b
{ddoshi, tianyuh, aritrad, andrey}@umd.edu
ABSTRACT
Neuralnetworksreadilylearnasubsetofthemodulararithmetictasks,whilefail-
ing to generalize on the rest. This limitation remains unmoved by the choice of
architectureandtrainingstrategies. Ontheotherhand,ananalyticalsolutionfor
theweightsofMulti-layerPerceptron(MLP)networksthatgeneralizeonthemod-
ularadditiontaskisknownintheliterature. Inthiswork,we(i)extendtheclass
ofanalyticalsolutionstoincludemodularmultiplicationaswellasmodularaddi-
tionwithmanyterms. Additionally,weshowthatrealnetworkstrainedonthese
datasetslearnsimilarsolutionsupongeneralization(grokking). (ii)Wecombine
these“expert”solutionstoconstructnetworksthatgeneralizeonarbitrarymodu-
larpolynomials. (iii)Wehypothesizeaclassificationofmodularpolynomialsinto
learnableandnon-learnablevianeuralnetworkstraining;andprovideexperimen-
talevidencesupportingourclaims.
1 INTRODUCTION
Modular arithmetic finds applications in many important fields including cryptography (Regev,
2009; Wenger et al., 2023), computer algebra (Fortin et al., 2020), number theory, error detection
inserialnumbersandmusic(A.AsKew&Klima,2018). Recently,modulararithmetichaspeaked
theinterestofthemachinelearningcommunityduetothephenomenonofGrokking(Poweretal.,
2022) – delayed and sudden generalization that occurs long after memorization. Since it was first
observed (Power et al., 2022), many works have attempted to understand grokking dynamics (Liu
et al., 2022; 2023; Thilak et al., 2022; Notsawo et al., 2023; Kumar et al., 2023; Lyu et al., 2023;
Daviesetal.,2022;Minegishietal.,2023). Anotherlineofworkattemptstounravelthegrokking
behaviorsusingsolvablemodelsandmechanisticinterpretability(Nandaetal.,2023;Gromov,2023;
Zhong et al., 2023; Barak et al., 2022; Merrill et al., 2023; Doshi et al., 2023; Varma et al., 2023;
Rubinetal.,2023). ItwasnotedinPoweretal.(2022)thatneuralnetworksreadilygrokasubsetof
modulararithmeticpolynomialswhilefailingtogeneralizeontherest. Thislimitationhasremained
unwaveringtovariationsinarchitecturesandtrainingstrategies.
Notably, Gromov (2023) presented an “analytical solution” for the weights of a 2-layer MLP net-
work that has a 100% accuracy on modular addition dataset (n +n modp). They showed that
1 2
realnetworkstrainedonmodularadditiondatafindssimilarsolutionsupongrokking. Inthiswork,
weextendtheclassofanalyticalsolutionstotoincludemodularmultiplication(e.g. n n modp)
1 2
andmodularadditionwithmanyterms(e.g. n +n +···n modp).1 Indeed,weshowthattrain-
1 2 S
ing real networks leads to similar network weights upon grokking for both of these tasks. Using
theseanalyticalsolutionsas“experts”,weconstructnetworksthatoffersgeneralizationonarbitrary
modularpolynomials,includingtheonesthataredeemedun-learnableinliterature. Thisformula-
tionopensupthepossibilityoftrainingMixture-of-Experts(Jordan&Jacobs,1993;Shazeeretal.,
2017;Lepikhinetal.,2021;Fedusetal.,2022)modelsthatcanbetrainedtolearnarbitrarymodular
arithmetic tasks. Based on our analysis, we hypothesize a classification of modular polynomials
intolearnableandnon-learnableby2-layerMLPs(andpossiblymoregeneralarchitecturessuchas
TransformersanddeeperMLPs).
aCondensedMatterTheoryCenter,UniversityofMaryland,CollegePark
bDepartmentofPhysics,UniversityofMaryland,CollegePark
†Correspondingauthor
1The solution for modular multiplication can be also readily extended to many terms. We present the
solutionfortwotermsjustforsimplicity.
1
4202
nuJ
5
]GL.sc[
1v59430.6042:viXraICLR2024WorkshopBGPT
2 MODULAR ADDITION WITH MANY TERMS
Consider the modular addition task with many terms with arbitrary coefficients:
(c n +c n +···+c n ) modp; where c ∈ Z \{0} are the nonzero coefficients of the
1 1 2 2 S S s p
modular variables n ∈ Z . Note that this is a generalization of the modular addition tasks
s p
generally considered in literature: (n + n )modp (Power et al., 2022; Gromov, 2023). We
1 2
considera2-layerMLP(ofsufficientwidth)forthistask.
(cid:16) (cid:17)
f (e ⊕···⊕e )=Wϕ(U(e ⊕···⊕e ))=Wϕ U(1)e +···+U(S)e ,
addS n1 nS n1 nS n1 nS
(1)
wheree ,...,e ∈ Rp areone hotencodednumbersn ,...,n . “⊕”denotesconcatenation
ofvectorn s1 (e ⊕n ·S ··⊕e ∈ RSp). U ∈ RN×Sp andW ∈1 Rp×NS arethefirstandsecondlayer
n1 nS
weightmatrices,respectively. ϕ(x)=xS istheelement-wiseactivationfunction. U isdecomposed
intoSblocksofN×P: U =U(1)⊕···⊕U(S). U(1),...,U(S)serveasembeddingmatricesfor
n ,...,n .f(e ⊕···⊕e )∈Rpisthenetwork-outputononeexampledatapoint(n ,...,n ).
1 S n1 nS 1 S
Thetargetsareone hotencodedanswerse .
(c1n1+···+cSnS)modp
2.1 ANALYTICALSOLUTION
Forasufficientlywidenetwork,itispossibletowriteasolutionfortheweightsofthenetworkthat
generalizeswith100%accuracy.
(cid:20) (cid:21)
2π
U(s) =A cos σ(k)c i+ψ(s) (∀s∈[0,S])
ki p s k
(cid:34) S (cid:35) (2)
W =A cos −2π σ(k)q−(cid:88) ψ(s) ,
qk p k
s=1
whereσ(k)denotesarandompermutationofkinS –reflectingthepermutationsymmetryofthe
N
hiddenneurons. Thephasesψ(s) uniformlyi.i.d. sampledbetween(−π,π]. Aisthenormalization
k
factortoensurecorrectmagnitudeoftheoutputlogits,givenbyA = (cid:0) 2S/(N ·S!)(cid:1)1/(S+1) . Note
thatthissolutionisageneralizationofthatpresentedinGromov(2023).
The rows (columns) of U(1),...,U(S),W are periodic with frequencies
2πσ(k)c ,··· ,2πσ(k)c ,−2πσ(k). We can use Inverse Participation Ratio (IPR) (Gromov,
p 1 p S p
2023;Doshietal.,2023)toquantifythesimilaritybetweentheanalyticalsolutionandrealtrained
networks. We denote discrete Fourier transforms of the row- (column-) vectors U(s),W by
k· ·k
F(cid:0) U(s)k·(cid:1) ,F(W ·k).2
IPR(cid:16) U k(s ·)(cid:17) := (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)F F(cid:16) (cid:16)U Uk k( (s s· ·) )(cid:17) (cid:17)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)4 4 ; IPR(W ·k):=(cid:18) ∥ ∥F F( (W W· ·k k) )∥ ∥4 2(cid:19)4 ; (3)
2
where∥·∥ denotestheLP-normofthevector.Subscripts“k·”and“·k”denotekthrowandcolumn
P
vectorsrespectively. Wealsodefinetheper-neuronIPRas
1 (cid:16) (cid:16) (cid:17) (cid:16) (cid:17) (cid:17)
IPR := IPR U(1) +···+IPR U(S) +IPR(W ) , (4)
k S+1 k· k· ·k
andtheaverageIPRofthenetworkbyaveragingitoverallneuronsasIPR:=E [IPR ].
k k
InFigure1,weshowthattheanalyticalsolutionequation2indeedhas100%forsufficientnetwork-
width. WeobserveanexponentialincreaseintherequiredwidthN duetoanexponentialincrease
inthenumberofcross-termsuponexpansion;whichneedstobesuppressedbythefactor1/N. We
referthereadertoAppendixAforadetaileddiscussion.
2IngeneralIPRforavectoruisdefinedas(∥u∥ /∥u∥ )2r.Wesetr=2,whichisacommonchoice.
2r 2
2ICLR2024WorkshopBGPT
Accuracy (%)
100
2 25.5 64.1 84.5 99.4 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
80
3 5.5 9.2 19.4 35.4 66.5 93.8 99.9 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
60
4 4.5 4.9 6.2 9.2 15.3 25.1 44.2 74.9 94.6 100.0 100.0 100.0 100.0 100.0 100.0
40
5 4.3 3.9 4.6 4.6 5.9 7.8 10.4 18.7 32.6 51.0 78.8 96.2 99.9 100.0 100.0
20
6 4.7 4.7 4.9 4.6 4.4 4.5 5.8 7.0 9.2 13.6 21.3 34.1 55.2 81.5 97.1
23 24 25 26 27 28 29 210 211 212 213 214 215 216 217
N
Figure 1: Modular addition with many terms – analytical solution applied to real 2-layer MLP
networks (p = 23). The solution equation 2 works for sufficiently wide networks. Note that the
x-axisisscaledlogarithmically;whichsuggestsanexponentialincreaseintherequiredwidthupon
adding more terms (as expected). The accuracies shown are calculated over a randomly chosen
subset of the entire dataset, consisting of 10k examples. The results shown are the best out of 10
randomseeds.
2.2 COMPARISONWITHTRAINEDNETWORKS
Wecomparetheabovesolutiontonetworkstrainedonmodularadditiondata. Weseethat2-layer
MLPwithsufficientwidthcanindeedgrokonmodulararithmeticdata. Moreover,thetrainednet-
work learns qualitatively similar weights to the analytical solution equation 2. The network gen-
eralizesbylearningtheperiodicweights, asevidencedbytheinitialandfinalIPRdistributionsin
Figure2.
(a) (b) (c)
Figure 2: Training on modular addition with many terms ((n + n + n + n )modp). p =
1 2 3 4
11;N = 5000; Adam optimizer; learning rate = 0.005; weight decay = 5.0; 50% of the dataset
used for training. (a) MSE loss on train and test dataset. (b) Accuracy on train and test dataset
as well as average IPR of the network IPR. The training curves show the well-known grokking
phenomenon; whileIPRmonotonicallyincreases. (c)InitialandfinalIPRdistributions, evidently
showingperiodicneuronsinthegrokkednetwork,confirmingthesimilaritytoequation2. Notethat
theIPRfortheanalyticalsolution(equation2)is1.
3 MODULAR MULTIPLICATION
Now consider the modular multiplication task in two variables3: (nanb)modp (where a,b ∈
1 2
Z \{0}; p is a prime number). It can be learned by a 2-layer MLP, in a similar fashion as mod-
p
3Theanalysiscanbereadilyextendedtomodularmultiplicationinmorethantwovariables;insimilarvein
asthemodularadditionanalysispresentintheprevioussection.
3
smret
fo
rebmuNICLR2024WorkshopBGPT
ularaddition(Gromov,2023;Doshietal.,2023).
(cid:16) (cid:17)
f (e ⊕e )=Qϕ(P(e ⊕e ))=Qϕ P(1)e +P(2)e , (5)
mul2 n1 n2 n1 n2 n1 n2
wherewehaveusedidenticalnotationtoequation1.
3.1 EXPONENTIALANDLOGARITHMICMAPSOVERFINITEFIELDS
Note that both modular addition and multiplication are defined over finite field of prime order p,
denotedbyGF(p). However,thereisasubtleyetcrucialdifferenceinthetwooperations. Addition
has a cycle of length p; i.e. (p+p+···(ntimes)) modp = 0 ∀ n ∈ Z ). On the other hand,
p
multiplication has a cycle of length p − 1; i.e. (g·g····(n−1times)) mod p = 1, where g
is a primitive root of GF(p).4 Note that multiplying any number by 0 gives 0; which warrants
a separate treatment for 0. The remaining elements r ∈ Z \{0} can be used to define the map
p
gr : Z \{0} → Z \{0}, where g is a primitive root. This exponential is bijective (invertible).
p p
Invertingthismapdefinesthelogarithmicmapwithbaseg. Weemphasisethatoverthefinitefield
GF(p), theexponential(gr)andlogarithmic(log r)arebasicallyareshufflingoftheelementsof
g
Z \{0}.Reshufflingthenumbersaccordingtothelogarithmicmapturnsthemultiplicationtaskinto
p
theadditiontask.Thisistheequivalentoftheresultlog (n n )=log n +log n onfinitefields.
g 1 2 g 1 g 2
Thus, barring the application of this map, the solution for multiplication is similar to equation 6.
Notethattheelement0needstobetreatedseparatelywhileconstructingtheanalyticalsolutionfor
thenetworkweights.
3.2 ANALYTICALSOLUTION
Using the above insights, we now construct the analytical solution for the modular multiplication
task. Notethatinthefollowingequation,i̸=0;j ̸=0;k ̸=0,q ̸=0.
P(1) =1, P(2) =1, Q =1,
00 00 00
P(1) =P(1) =0, P(2) =P(2) =0, Q =Q =0,
0i k0 0j k0 q0 k0
P(1)
=(cid:18)
2
(cid:19)−1
3
cos(cid:20)
2π σ(k)(alog
i)+ψ(1)(cid:21)
,
ki N −1 p−1 g k
(6)
P(2)
=(cid:18)
2
(cid:19)−1
3
cos(cid:20)
2π σ(k)(blog
j)+ψ(2)(cid:21)
,
kj N −1 p−1 g k
Q
=(cid:18)
2
(cid:19)−1
3
cos(cid:20)
−
2π
σ(k)(log
q)−ψ(1)−ψ(2)(cid:21)
,
qk N −1 p−1 g k k
whereσ(k)denotesarandompermutationofkinS –reflectingthepermutationsymmetryofthe
N
hiddenneurons. Thephasesψ(1)andψ(2)areuniformlyi.i.d. sampledbetween(−π,π].
k k
The rows (columns) of P(1),P(2),Q are periodic with frequencies
2π aσ(k), 2π bσ(k),− 2π σ(k), upon performing exponential map on their column (row)
p−1 p−1 p−1
indicesi,j,q (Notethatthisexcludesthe0th entryineachrow(column). Thoseentriesdealwith
theelement0). Thismappingisequivalenttoreshufflingthecolumns(rows)ofP(1),P(2),Q. Let
(1) (2)
usdenotethesenewre-shuffled,0-excludedweightmatricesasP ,P ,Q.
P(1) =P(1) , P(2) =P(2) , Q =Q (i̸=0,j ̸=0,q ̸=0,k ̸=0), (7)
ki k,gi kj k,gj qk gq,k
where we denote the shuffled indices (i,j,q) by gi,gj,gq in the subscript. Again, we can use
IPRquantifytheperiodicity. WedenotediscreteFouriertransformsoftherow-(column-)vectors
4Thisisawell-knownresultcalledFermat’slittletheorem.
4ICLR2024WorkshopBGPT
(1) (2) (cid:16) (1)(cid:17) (cid:16) (2)(cid:17) (cid:0) (cid:1)
P ,P ,Q byF P ,F P ,F Q .
k· k· ·k k· k· ·k
(cid:13) (cid:16) (cid:17)(cid:13) 2
IPR(cid:16) P( kt ·)(cid:17) := (cid:13) (cid:13) (cid:13)(cid:13) (cid:13) FF (cid:16)P P( k ( kt t· ·) )(cid:17)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)4  ; IPR(cid:0) Q ·k(cid:1) :=(cid:32) (cid:13) (cid:13)(cid:13) (cid:13) FF (cid:0)(cid:0) Q Q· ·k k(cid:1) (cid:1)(cid:13) (cid:13) (cid:13) (cid:13)4 2(cid:33)4 , (8)
4
wheret ∈ {1,2}. Per-neuronIPRaswellasaverageIPRofthenetworkcanbedefinedasbefore;
withIPR :=1/3(cid:16) IPR(cid:16) P(1)(cid:17) +IPR(cid:16) P(2)(cid:17) +IPR(cid:0) Q (cid:1)(cid:17) andIPR:=E [IPR ].
k k· k· k· k k
3.3 COMPARISONWITHTRAINEDNETWORKS
Now, weshowthattherealnetworkstrainedonmodularmultiplicationdatalearnsimilarfeatures
asEquation6.
(a) (b) (c)
Figure3: Trainingonmodularmultiplication(n n modp). p = 97;N = 500; Adamoptimizer;
1 2
learningrate= 0.005; weightdecay= 5.0; 50%ofthedatasetusedfortraining. (a)MSElosson
train and test datset. (b) Accuracy on train and test dataset as well as average IPR of the network
IPR. The training curves show the well-known grokking phenomenon; while IPR monotonically
increases. (c)InitialandfinalIPRdistributions,evidentlyshowingperiodicneuronsinthegrokked
network,confirmingthesimilaritytoequation6. NotethattheIPRfortheanalyticalsolution(equa-
tion6)is1.
4 ARBITRARY MODULAR POLYNOMIALS
Considerageneralmodularpolynomialintwovariables(n ,n )containingS terms:
1 2
(cid:16) (cid:17)
c na1nb1 +c na2nb2 +···+c naSnbS modp. (9)
1 1 2 2 1 2 S 1 2
Weutilizethesolutionspresentedinprevioussectionstoconstructasimplenetworkthatgeneralizes
onthistask. Eachterm(withoutthecoefficientsc )canbesolvedbya2-layerMLPexpert(equa-
s
tions5,6)thatmultipliestheappropriatepowersofn andn . TheoutputoftheseS termscanbe
1 2
added,alongwiththecoefficients,usinganotherexpertnetwork(equations1. Theexpertnetworks
are designed to perform best when their inputs are close to one hot vectors; so we apply (low
temperature)Softmaxfunctiontotheoutputsoftheeachterm. Asbefore,theinputtothenetwork
arestacked,one hotrepresentednumbersn ,n : (e ⊕e )
1 2 n1 n2
(cid:16) (cid:17)
t(s) =f(s) (e ⊕e )=Q(s)ϕ P(s,1)e +P(s,2)e , (s∈[1,S]) (10)
mul2 n1 n2 n1 n2
(cid:16) (cid:17)
u(s) =softmax t(s) , (11)
β
(cid:16) (cid:17) (cid:16) (cid:17)
z =f u(1)⊕···⊕u(S) =Wϕ U(1)u(1)+···+U(S)u(S) (12)
addS
The weights P(s,1),P(s,2),Q(s) are given by equation 6, with appropriate powers a ,b for each
s s
term. Similarly, theweightsU(s),W aregivenbyequation2, withcoefficientsc . Wehaveused
s
5ICLR2024WorkshopBGPT
thetemperature-scaledSoftmaxfunction
softmax
(cid:16) t(s)(cid:17)
:=
eβt( is)
. (13)
β i (cid:80)peβt( js)
j
Weselectahighvaluefortheinversetemperatureβ ∼ 100sothattheintermediateoutputsuget
closetoone hotvectors. Thisresultsinahigheraccuracyinthesummationofmonomialstobe
performedinthesubsequentlayers.
In Appendix A, we show the performance of real networks, with the solutions equation 10, on
general modular polynomials. We show that such a construction is able to learn polynomials that
areun-learnableviatrainingstandardnetworkarchitectures.
Instead of using the analytical weights, one can train the “experts”: f(s) ,f separately, on
addS addS
multiplicationandadditiondatasets,andthencombinethemintoaMixture-of-Expertsmodel.
5 DISCUSSION
Wehavepresentedtheanalyticalsolutionsfortheweightsof2-layerMLPnetworksonmodularmul-
tiplicationusingthebijectiveexponentialandlogarithmicmapsonGF(p). Wehavealsopresented
solutionsformodularmultiplicationwithmanytermsandarbitrarycoefficients. Inbothcases,We
haveshownthatrealnetworkstrainedonthesetasksfindsimilarsolutions.
Usingthese“expert”solutions,wehaveconstructedanetworkthatgeneralizesonarbitrarymodu-
larpolynomials,givensufficientwidth. ThisnetworkisreminiscentofMixture-of-Expertmodels.
Therefore, our construction opens the possibility of building such network architectures, that can
learnmodulararithmetictasksthatSOTAmodelshavebeenunabletolearn. Wedelegatetheexplo-
rationofthisavenueforfuturework.
On the other hand, our general formulation of analytical solutions for 2-layer MLP points to a
potentialclassificationofmodularpolynomialsintolearnableandnon-learnableones.
Hypothesis 5.1 (Weak generalization). Consider training a 2-layer MLP network trained on a
datasetconsistingofmodularpolynomialonGF(p)intwovariables(n ,n ),withcommonlyused
1 2
optimizers (SGD, Adam etc.), loss functions (MSE, CrossEntropy etc.) and regularization meth-
ods(weightdecay,dropout,BatchNorm). Thenetworkachieves100%testaccuracyifthemodular
polynomialisofthefollowingform:5
h(g (n )+g (n ))modp, (14)
1 1 2 2
whereg ,g andharefunctionsonGF(p).
1 2
• Notethatequation14alsoincludesmodularmultiplication:takingg ,g tobelog (·)and
1 2 g
htobeexp (·),wheregisaprimitiverootofGF(p).
g
• Ifthefunctionhisinvertible,thenitisalsopossibletoconstructananalyticalsolutionfor
thistaskinasimilarfashionasequations2and6.6
In Appendix C, we provide experimental evidence supporting this claim. We show the results of
trainingexperimentsonvariousmodularpolynomialsbelongingtobothcategories. Weseeaclear
differenceintestperformanceontasksthatfallundertheformequation14andthosethatdonot.
Itispossibletogeneralizethisclaimtoothernetworkarchitectures. Generalarchitecturessuchas
transformersanddeeperMLPshavealsobeenunabletosolvegeneralmodulararithmetictasksthat
donotfallunderequation14(Poweretal.,2022).Moreover,Doshietal.(2023)hypothesizedagen-
eralframeworkforgeneralizationonmodulararithmetictasksinthesearchitectures. Consequently,
weconjecturethatHypothesis5.1canbeextendedtogeneralarchitectures.
Hypothesis5.2(Stronggeneralization). Considertrainingastandardneuralnetwork(MLP,Trans-
formeretc.;notpre-trained)onadatasetconsistingofmodularpolynomialonGF(p)intwovari-
5Thenetworksarenaturallytrainedon<100%oftheentiredataset;andtestedontherest.
6ThisgeneralconstructionofanalyticalsolutionwasalsohypothesizedinGromov(2023).
6ICLR2024WorkshopBGPT
ables (n ,n ), with commonly used optimizers (SGD, Adam etc.), loss functions (MSE, CrossEn-
1 2
tropyetc.) andregularizationmethods(weightdecay,dropout,BatchNorm). Thenetworkachieves
100%testaccuracyifthemodularpolynomialisofthefromofequation14.
Whileexperimentsvalidatethesehypotheses,acomprehensiveproofoftheclaimremainsanopen
challenge. Formulatingsuchaproofwouldrequirearobustclassificationofmodularpolynomials;
as well as the understanding biases of neural network training on these tasks. We defer such an
analysistofutureworks.
ACKNOWLEDGMENTS
A.G.’sworkattheUniversityofMarylandwassupportedinpartbyNSFCAREERAwardDMR-
2045181,SloanFoundationandtheLaboratoryforPhysicalSciencesthroughtheCondensedMatter
TheoryCenter.
REFERENCES
K.KennedyA.AsKewandV.Klima. Modulararithmeticandmicrotonalmusictheory. PRIMUS,
28(5):458–471,2018. doi: 10.1080/10511970.2017.1388314.
BoazBarak,BenjaminL.Edelman,SurbhiGoel,ShamM.Kakade,eranmalach,andCyrilZhang.
Hidden progress in deep learning: SGD learns parities near the computational limit. In Al-
ice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neu-
ralInformationProcessingSystems,2022. URLhttps://openreview.net/forum?id=
8XWP2ewX-im.
Xander Davies, Lauro Langosco, and David Krueger. Unifying grokking and double descent.
In NeurIPS ML Safety Workshop, 2022. URL https://openreview.net/forum?id=
JqtHMZtqWm.
DarshilDoshi,AritraDas,TianyuHe,andAndreyGromov. Togrokornottogrok: Disentangling
generalizationandmemorizationoncorruptedalgorithmicdatasets,2023.
WilliamFedus,BarretZoph,andNoamShazeer. Switchtransformers: Scalingtotrillionparameter
modelswithsimpleandefficientsparsity,2022.
PierreFortin,AmbroiseFleury,Franc¸oisLemaire,andMichaelMonagan. Highperformancesimd
modulararithmeticforpolynomialevaluation,2020.
Andrey Gromov. Grokking modular arithmetic, 2023. URL https://arxiv.org/abs/
2301.02679.
M.I.JordanandR.A.Jacobs.Hierarchicalmixturesofexpertsandtheemalgorithm.InProceedings
of1993InternationalConferenceonNeuralNetworks(IJCNN-93-Nagoya,Japan),volume2,pp.
1339–1344vol.2,1993. doi: 10.1109/IJCNN.1993.716791.
Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, and Cengiz Pehlevan. Grokking as the
transitionfromlazytorichtrainingdynamics,2023.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
MaximKrikun,NoamShazeer,andZhifengChen. {GS}hard: Scalinggiantmodelswithcondi-
tionalcomputationandautomaticsharding. InInternationalConferenceonLearningRepresen-
tations,2021. URLhttps://openreview.net/forum?id=qrwe7XHTmYb.
ZimingLiu,OuailKitouni,NiklasSNolte,EricMichaud,MaxTegmark,andMikeWilliams. To-
wards understanding grokking: An effective theory of representation learning. In S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neu-
ral Information Processing Systems, volume 35, pp. 34651–34663. Curran Associates, Inc.,
2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/
file/dfc310e81992d2e4cedc09ac47eff13e-Paper-Conference.pdf.
7ICLR2024WorkshopBGPT
Ziming Liu, Eric J Michaud, and Max Tegmark. Omnigrok: Grokking beyond algorithmic data.
In The Eleventh International Conference on Learning Representations, 2023. URL https:
//openreview.net/forum?id=zDiHoIWa0q1.
KaifengLyu, JikaiJin, ZhiyuanLi, SimonS.Du, JasonD.Lee, andWeiHu. Dichotomyofearly
andlatephaseimplicitbiasescanprovablyinducegrokking,2023.
WilliamMerrill,NikolaosTsilivis,andAmanShukla. Ataleoftwocircuits: Grokkingascompeti-
tionofsparseanddensesubnetworks,2023.
GoukiMinegishi,YusukeIwasawa,andYutakaMatsuo.Grokkingtickets:Lotteryticketsaccelerate
grokking,2023.
NeelNanda,LawrenceChan,TomLieberum,JessSmith,andJacobSteinhardt. Progressmeasures
forgrokkingviamechanisticinterpretability,2023.
PascalJr.TikengNotsawo, HattieZhou, MohammadPezeshki, IrinaRish, andGuillaumeDumas.
Predictinggrokkinglongbeforeithappens:Alookintothelosslandscapeofmodelswhichgrok,
2023.
AletheaPower,YuriBurda,HarriEdwards,IgorBabuschkin,andVedantMisra. Grokking: Gener-
alizationbeyondoverfittingonsmallalgorithmicdatasets,2022.
OdedRegev. Onlattices,learningwitherrors,randomlinearcodes,andcryptography. J.ACM,56
(6),sep2009. ISSN0004-5411. doi: 10.1145/1568318.1568324. URLhttps://doi.org/
10.1145/1568318.1568324.
NoaRubin,InbarSeroussi,andZoharRingel. Dropletsofgoodrepresentations: Grokkingasafirst
orderphasetransitionintwolayernetworks,2023.
Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hin-
ton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-
experts layer. In International Conference on Learning Representations, 2017. URL https:
//openreview.net/forum?id=B1ckMDqlg.
Vimal Thilak, Etai Littwin, Shuangfei Zhai, Omid Saremi, Roni Paiss, and Joshua Susskind. The
slingshotmechanism: Anempiricalstudyofadaptiveoptimizersandthegrokkingphenomenon,
2022.
Vikrant Varma, Rohin Shah, Zachary Kenton, Ja´nos Krama´r, and Ramana Kumar. Explaining
grokkingthroughcircuitefficiency,2023.
EmilyWenger,MingjieChen,Franc¸oisCharton,andKristinLauter. Salsa: Attackinglatticecryp-
tographywithtransformers,2023.
ZiqianZhong,ZimingLiu,MaxTegmark,andJacobAndreas. Theclockandthepizza:Twostories
inmechanisticexplanationofneuralnetworks,2023.
8ICLR2024WorkshopBGPT
A ANALYTICAL SOLUTIONS
A.1 MODULARMULTIPLICATION
Hereweshowthatthattheanalyticalsolutionpresentedinequation6solvesthemodularmultipli-
cation task. Let us first calculate the network output for n ̸= 0,n ̸= 0. We will treat the cases
1 2
withn =0and/orn =0separately.
1 2
f (e ⊕e )
mul2 n1 n2
(cid:16) (cid:17)
=Qϕ P(1)e +P(2)e
n1 n2
N
=(cid:88) Q (P(1) +P(2))2
qk kn1 kn2
k=1
N−1 (cid:18) (cid:19)
= 2 (cid:88) cos − 2π σ(k)log q−(ψ(1)+ψ(2)) ·
N −1 p−1 g k k
k=1
(cid:18) (cid:18)
2π
(cid:19) (cid:18)
2π
(cid:19)(cid:19)2
· cos σ(k)alog n +ψ(1) +cos σ(k)blog n +ψ(2)
p−1 g 1 k p−1 g 2 k
N−1(cid:26) (cid:18) (cid:19)
= 2 (cid:88) 1 cos 2π σ(k)(2alog n −log q)+ψ(1)−ψ(2)
N −1 4 p g 1 g k k
k=1
(cid:18) (cid:19)
1 2π
+ cos σ(k)(2alog n +log q)+3ψ(1)+ϕ(2)
4 p−1 g 1 g k k
(cid:18) (cid:19)
1 2π
+ cos σ(k)(2blog n −log q)−ψ(1)+ψ(2)
4 p−1 g 2 g k k
(cid:18) (cid:19)
1 2π
+ cos σ(k)(2blog n +log q)+ψ(1)+3ψ(2)
4 p−1 g 2 g k k
(cid:18) (cid:19)
1 2π
+ cos σ(k)(alog n +blog n −log q)
2 p−1 g 1 g 2 g
(cid:18) (cid:19)
1 2π
+ cos σ(k)(alog n +blog n +log q)+2ψ(1)+2ψ(2)
2 p−1 g 1 g 2 g k k
(cid:18) (cid:19)
1 2π
+ cos σ(k)(alog n −blog n −log q)−2ψ(1)
2 p−1 g 1 g 2 g k
(cid:18) (cid:19)
1 2π
+ cos σ(k)(alog n −blog n +log q)+2ψ(1)
2 p−1 g 1 g 2 g k
(cid:18) (cid:19)(cid:27)
2π
+cos − σ(k)log q−ψ(1)−ψ(2) . (15)
p−1 g k k
Wehavehighlightedthetermthatwillgiveusthedesiredoutputwithabox. Notethatthedesired
term is the only one that does not have additive phases in the argument. Recall that the phases
ψ(1) andψ(2) arerandomlychosen–uniformlyiidsampledbetween(−π,π]. Consequently,asN
becomeslarge,allothertermswillvanishduetorandomphaseapproximation. Theonlysurviving
termwill betheboxed term. Wecanwrite the boxedtermin amoresuggestive formtomake the
analyticalsolutionapparent.
9ICLR2024WorkshopBGPT
N−1 (cid:18) (cid:19)
1 (cid:88) 2π
f (e ⊕e )= cos σ(k)(alog n +blog n −log q)
mul2 n1 n2 N −1 p−1 g 1 g 2 g
k=1
N−1 (cid:18) (cid:19)
1 (cid:88) 2π
= cos σ(k)(log na+log nb −log q)
N −1 p−1 g 1 g 2 g
k=1
∼δp(cid:0) nanb −q(cid:1) , (16)
1 2
werewehavedefinedthemodularKroneckerDeltafunctionδ(·)asKroneckerDeltafunctionupto
integermultipliesofthemodularbasep.
(cid:26) 1 x=rp (r ∈Z)
δp(x)= , (17)
0 otherwise
whereZdenotesthesetofallintegers. Notethatδp(nanb −q)arethedesiredone hotencoded
1 2
labels for the modular multiplication task, by definition. Thus our network output with periodic
weightsisindeedasolution.
Note that if either n = 0 or n = 0 (or both), the 0th output logit will be the largest (= 1).
1 2
Consequently,thenetworkwillcorrectlypredict0outputforsuchcases.
Hence,thesolutionpresentedinequation6gives100%accuracy.
A.2 MODULARADDITIONWITHMANYTERMS
Next, we show that that the analytical solution presented in equation 2 solves the task of modular
additionwithmanyterms.
f (e ⊕···⊕e )
addS n1 nS
(cid:16) (cid:17)
=Wϕ U(1)e +···+U(S)e
n1 nS
N
=(cid:88) W (U(1) +···P(S))S
qk kn1 knS
k=1
=
2S (cid:88)N cos(cid:32) −2π σ(k)q−(cid:88)S ψ(s)(cid:33)
·
N ·S! p k
k=1 s=1
(cid:18) (cid:18)
2π
(cid:19) (cid:18)
2π
(cid:19)(cid:19)S
· cos σ(k)c n +ψ(1) +···+cos σ(k)c n +ψ(S)
p 1 1 k p 2 2 k
2S (cid:88)N (cid:40) S! (cid:18) 2π (cid:19) (cid:41)
= ···+ cos σ(k)(c n +···+c n −q) +··· (18)
N ·S! 2S p 1 1 S S
k=1
Here, we have omitted all the additional terms that drop out due to random phase approximation;
showingonlythedesiredsurvivingterm. Notethatthenumberoftheseadditionaltermsincreases
exponentially in S; and they are suppressed by a factor of 1/N. This provides an explanation for
anexponentialbehaviourintherequirednetworkwidthN withincreasingnumberoftermsinthe
additiontaskFigure1.
N (cid:18) (cid:19)
1 (cid:88) 2π
f (e ⊕···⊕e )= cos σ(k)(c n +···+c n −q)
addS n1 nS N p 1 1 S S
k=1
∼δp(c n +···+c n −q) .
1 1 S S
Notethatδp(c n +···+c n −q)isindeedthedesiredoutputforthemodularadditiontask.
1 1 S S
10ICLR2024WorkshopBGPT
B PERFORMANCE ON ARBITRARY MODULAR POLYNOMIALS
Inthisappendix,wedocumenttheperformanceofthenetworkconstructedinequation10ongeneral
modular polynomials. Note that none of the polynomials listed in Appendix B are learnable by
training 2-layer MLP or other network architectures (such as Transformers). The MSE loss and
accuracyarecomputedontheentiredataset(p2examples). Theexpert-widthsaretakentobeN =
1
500andN =2000.SinceS =3inalltheexamples,weusecubic(ϕ(x)=x3)activationfunction.
2
WehavetruncatedtheMSElossat6digitsafterthedecimalpoint.
Modularpolynomial MSEloss Accuracy
(cid:0) 2n4n +n2n2+3n n3(cid:1) mod97 0.007674 100%
1 2 1 2 1 2
(cid:0) n5n3+4n2n +5n2n3(cid:1) mod97 0.007660 100%
1 2 1 2 1 2
(cid:0) 7n4n4+2n3n2+4n2n5(cid:1)
mod97 0.007683 100%
1 2 1 2 1 2
(cid:0) 2n4n +n2n2+3n n3(cid:1) mod23 0.009758 100%
1 2 1 2 1 2
(cid:0) n5n3+4n2n +5n2n3(cid:1) mod23 0.009757 100%
1 2 1 2 1 2
(cid:0) 7n4n4+2n3n2+4n2n5(cid:1)
mod23 0.010201 100%
1 2 1 2 1 2
C TRAINING ON LEARNABLE AND NON-LEARNABLE MODULAR
POLYNOMIALS
Herewepresentthetrainingresultsonvariousmodularpolynomials. 2-layerMLPnetworkswith
quadratic activations and width N = 5000 are trained Adam optimizer, MSE loss, learning rate
=0.005,weightdecay=5.0,on50%ofthetotaldataset.
Weobservethattrainingthenetworkonpolynomialsoftheformh(g (n )+g (n ))modpresults
1 1 2 2
ingeneralization. Whereas, makingaslightchangetothesepolynomialsresultsininabilityofthe
networkstogeneralize. ThisservesasevidenceforHypothesis5.1.
Notethatthefollowingresultsremainqualitativelyunchangeduponchanging/tuninghyperparame-
ters.
Modularpolynomial Trainloss Testloss Trainacc Testacc
(cid:0)
4n
+n2(cid:1)3
mod 97 0.000344 0.000569 100% 100%
1 2
(cid:0)
4n
+n2(cid:1)3
+n n mod 97 0.001963 0.011216 100% 2.27%
1 2 1 2
(2n +3n )4 mod 97 0.000139 0.000172 100% 100%
1 2
(2n +3n )4−n2 mod 97 0.001916 0.011097 100% 3.93%
1 2 1
(cid:0) 5n3+2n4(cid:1)2
mod 97 0.000146 0.000161 100% 100%
1 2
(cid:0) 5n3+2n4(cid:1)2
−n mod 97 0.001523 0.006108 100% 72.32%
1 2 2
(cid:0)
4n
+n2(cid:1)3
mod 23 0.000143 0.006274 100% 100%
1 2
(cid:0)
4n
+n2(cid:1)3
+n n mod 23 0.000147 0.051866 100% 1.89%
1 2 1 2
(2n +3n )4 mod 23 0.000093 0.001010 100% 100%
1 2
(2n +3n )4−n2 mod 23 0.000132 0.049618 100% 7.17%
1 2 1
(cid:0) 5n3+2n4(cid:1)2
mod 23 0.000056 0.001004 100% 100%
1 2
(cid:0) 5n3+2n4(cid:1)2
−n mod 23 0.000150 0.052047 100% 2.64%
1 2 2
11