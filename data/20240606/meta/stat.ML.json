[
    {
        "title": "Grokking Modular Polynomials",
        "authors": "Darshil DoshiTianyu HeAritra DasAndrey Gromov",
        "links": "http://arxiv.org/abs/2406.03495v1",
        "entry_id": "http://arxiv.org/abs/2406.03495v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03495v1",
        "summary": "Neural networks readily learn a subset of the modular arithmetic tasks, while\nfailing to generalize on the rest. This limitation remains unmoved by the\nchoice of architecture and training strategies. On the other hand, an\nanalytical solution for the weights of Multi-layer Perceptron (MLP) networks\nthat generalize on the modular addition task is known in the literature. In\nthis work, we (i) extend the class of analytical solutions to include modular\nmultiplication as well as modular addition with many terms. Additionally, we\nshow that real networks trained on these datasets learn similar solutions upon\ngeneralization (grokking). (ii) We combine these \"expert\" solutions to\nconstruct networks that generalize on arbitrary modular polynomials. (iii) We\nhypothesize a classification of modular polynomials into learnable and\nnon-learnable via neural networks training; and provide experimental evidence\nsupporting our claims.",
        "updated": "2024-06-05 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03495v1"
    },
    {
        "title": "Solving Poisson Equations using Neural Walk-on-Spheres",
        "authors": "Hong Chul NamJulius BernerAnima Anandkumar",
        "links": "http://arxiv.org/abs/2406.03494v1",
        "entry_id": "http://arxiv.org/abs/2406.03494v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03494v1",
        "summary": "We propose Neural Walk-on-Spheres (NWoS), a novel neural PDE solver for the\nefficient solution of high-dimensional Poisson equations. Leveraging stochastic\nrepresentations and Walk-on-Spheres methods, we develop novel losses for neural\nnetworks based on the recursive solution of Poisson equations on spheres inside\nthe domain. The resulting method is highly parallelizable and does not require\nspatial gradients for the loss. We provide a comprehensive comparison against\ncompeting methods based on PINNs, the Deep Ritz method, and (backward)\nstochastic differential equations. In several challenging, high-dimensional\nnumerical examples, we demonstrate the superiority of NWoS in accuracy, speed,\nand computational costs. Compared to commonly used PINNs, our approach can\nreduce memory usage and errors by orders of magnitude. Furthermore, we apply\nNWoS to problems in PDE-constrained optimization and molecular dynamics to show\nits efficiency in practical applications.",
        "updated": "2024-06-05 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03494v1"
    },
    {
        "title": "Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles",
        "authors": "Joseph FeldmanJerome P. ReiterDaniel R. Kowal",
        "links": "http://arxiv.org/abs/2406.03463v1",
        "entry_id": "http://arxiv.org/abs/2406.03463v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03463v1",
        "summary": "We present an approach for modeling and imputation of nonignorable missing\ndata under Gaussian copulas. The analyst posits a set of quantiles of the\nmarginal distributions of the study variables, for example, reflecting\ninformation from external data sources or elicited expert opinion. When these\nquantiles are accurately specified, we prove it is possible to consistently\nestimate the copula correlation and perform multiple imputation in the presence\nof nonignorable missing data. We develop algorithms for estimation and\nimputation that are computationally efficient, which we evaluate in simulation\nstudies of multiple imputation inferences. We apply the model to analyze\nassociations between lead exposure levels and end-of-grade test scores for\n170,000 students in North Carolina. These measurements are not missing at\nrandom, as children deemed at-risk for high lead exposure are more likely to be\nmeasured. We construct plausible marginal quantiles for lead exposure using\nnational statistics provided by the Centers for Disease Control and Prevention.\nComplete cases and missing at random analyses appear to underestimate the\nrelationships between certain variables and end-of-grade test scores, while\nmultiple imputation inferences under our model support stronger adverse\nassociations between lead exposure and educational outcomes.",
        "updated": "2024-06-05 17:11:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03463v1"
    },
    {
        "title": "Unified PAC-Bayesian Study of Pessimism for Offline Policy Learning with Regularized Importance Sampling",
        "authors": "Imad AoualiVictor-Emmanuel BrunelDavid RohdeAnna Korba",
        "links": "http://arxiv.org/abs/2406.03434v1",
        "entry_id": "http://arxiv.org/abs/2406.03434v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03434v1",
        "summary": "Off-policy learning (OPL) often involves minimizing a risk estimator based on\nimportance weighting to correct bias from the logging policy used to collect\ndata. However, this method can produce an estimator with a high variance. A\ncommon solution is to regularize the importance weights and learn the policy by\nminimizing an estimator with penalties derived from generalization bounds\nspecific to the estimator. This approach, known as pessimism, has gained recent\nattention but lacks a unified framework for analysis. To address this gap, we\nintroduce a comprehensive PAC-Bayesian framework to examine pessimism with\nregularized importance weighting. We derive a tractable PAC-Bayesian\ngeneralization bound that universally applies to common importance weight\nregularizations, enabling their comparison within a single framework. Our\nempirical results challenge common understanding, demonstrating the\neffectiveness of standard IW regularization techniques.",
        "updated": "2024-06-05 16:32:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03434v1"
    },
    {
        "title": "Noisy Data Visualization using Functional Data Analysis",
        "authors": "Haozhe ChenAndres Felipe Duque CorreaGuy WolfKevin R. Moon",
        "links": "http://arxiv.org/abs/2406.03396v1",
        "entry_id": "http://arxiv.org/abs/2406.03396v1",
        "pdf_url": "http://arxiv.org/pdf/2406.03396v1",
        "summary": "Data visualization via dimensionality reduction is an important tool in\nexploratory data analysis. However, when the data are noisy, many existing\nmethods fail to capture the underlying structure of the data. The method called\nEmpirical Intrinsic Geometry (EIG) was previously proposed for performing\ndimensionality reduction on high dimensional dynamical processes while\ntheoretically eliminating all noise. However, implementing EIG in practice\nrequires the construction of high-dimensional histograms, which suffer from the\ncurse of dimensionality. Here we propose a new data visualization method called\nFunctional Information Geometry (FIG) for dynamical processes that adapts the\nEIG framework while using approaches from functional data analysis to mitigate\nthe curse of dimensionality. We experimentally demonstrate that the resulting\nmethod outperforms a variant of EIG designed for visualization in terms of\ncapturing the true structure, hyperparameter robustness, and computational\nspeed. We then use our method to visualize EEG brain measurements of sleep\nactivity.",
        "updated": "2024-06-05 15:53:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.03396v1"
    }
]