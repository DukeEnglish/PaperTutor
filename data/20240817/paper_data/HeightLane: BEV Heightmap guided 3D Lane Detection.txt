HeightLane: BEV Heightmap guided 3D Lane Detection
ChaesongPark2 EunbinSeo2 JongwooLim1,2
ME1 &IPAI2,SeoulNationalUniversity
{chase121, rlocong339, jongwoo.lim }@snu.ac.kr
Abstract
Accurate 3D lane detection from monocular images
presentssignificantchallengesduetodepthambiguityand
imperfect ground modeling. Previous attempts to model
the ground have often used a planar ground assumption
withlimiteddegreesoffreedom,makingthemunsuitablefor
complexroadenvironmentswithvaryingslopes. Ourstudy
introducesHeightLane,aninnovativemethodthatpredicts
a height map from monocular images by creating anchors
basedonamulti-slopeassumption. Thisapproachprovides
adetailedandaccuraterepresentationoftheground.
HeightLaneemploysthepredictedheightmapalongwith
a deformable attention-based spatial feature transform
frameworktoefficientlyconvert2Dimagefeaturesinto3D
bird’s eye view (BEV) features, enhancing spatial under-
standing and lane structure recognition. Additionally, the
heightmap is used for the positional encoding of BEV fea-
tures, further improving their spatial accuracy. This ex-
plicit view transformation bridges the gap between front- Figure 1. (a) Assuming the ground is a flat plane, 2D images
view perceptions and spatially accurate BEV representa- orfeaturescanbetransformedintoBEVfeaturesusingIPM.(b)
tions,significantlyimprovingdetectionperformance. Modeling the ground as a plane with 2 degrees of freedom (2-
DoF), such as pitch and height, provides more generality and is
Toaddressthelackofthenecessarygroundtruthheight
usedbyLATRforpositionalencodinginthetransformer. (c)Our
map in the original OpenLane dataset, we leverage the
methodpredictsadenseheightmaptospatiallytransform2Dim-
Waymo dataset and accumulate its LiDAR data to gener-
agefeaturesontoapredefinedBEVfeaturegrid. Boldindicates
ate a height map for the drivable area of each scene. The
howeachmethodrepresentstheground.
GT heightmaps are used to train the heightmap extraction
modulefrommonocularimages. Extensiveexperimentson
costs, a superior perception range compared to LiDAR,
theOpenLanevalidationsetshowthatHeightLaneachieves
and the ability to capture high-resolution images with de-
state-of-the-artperformanceintermsofF-score,highlight-
tailed textures, which are essential for identifying narrow
ingitspotentialinreal-worldapplications.
andelongatedlanemarkings. Furthermore,thestrongper-
formance of deep learning-based 2D lane detection across
variousbenchmarkshasdrivenactiveresearchinthisarea,
1.Introduction
highlighting the potential for similar breakthroughs in 3D
Monocular 3D lane detection, which involves estimat- lane detection [11,13,19,27,29]. However, the lack of
ing the 3D coordinates of lane markings from a single depthinformationin2Dimagesmakesthistaskparticularly
image, is a fundamental task in autonomous driving sys- challenging. Thus,accuratelyderiving3Dlaneinformation
tems. WhileLiDAR-based methodshaveachieved signifi- from 2D images remains a significant research and devel-
cantprogressinmany3Dperceptiontasks,monocularcam- opmentfocus.
eras are increasingly favored for 3D lane detection due to Recently, with the increasing focus on birds-eye view
several key advantages. These advantages include lower (BEV) representation [6,9,10], there has been a surge in
1
4202
guA
51
]VC.sc[
1v07280.8042:viXraresearchonBEVlanedetectionand3Dlanedetection. To featuresandBEVfeatures.
addressthechallengesposedbythelackofdepthinforma-
• We validate HeightLane’s performance on the Open-
tion,severalstudieshaveattemptedtomodelthegroundon
Lane dataset [2], one of the most promising bench-
whichthelanesarelocated.Someapproaches,suchasPers-
marksfor3Dlanedetection. HeightLaneachievedthe
Former[2–4,12],haveappliedinverseperspectivetransfor-
highestF-scoreonOpenLane’svalidationset,surpass-
mation (IPM) to 2D images or features extracted from 2D
ing previous state-of-the-art models by a significant
images,achievingspatialtransformationandcreatingBEV
margininmultiplescenarios.
featuresfor3DlanedetectionasshowninFig.1(a).
However, in real-world scenarios, the ground has vary-
ing slopes and elevations, making these methods, which
2.RelatedWorks
assume a flat ground, prone to misalignment between the
2DfeaturesandthetransformedBEVfeatures. Toaddress 2.1.3Dlanedetection
this, models like LATR applying transformers to 3D lane
3D lane detection has become essential for accurate lo-
detection[17],asillustratedinFig.1(b),haveincorporated
calizationinrealisticdrivingscenarios. While2Dlanede-
groundinformationthroughpositionalencoding,aimingto
tection has been extensively studied, fewer works address
providemoreaccuratespatialcontextforthefeatures. De-
the challenges of 3D lane modeling. Traditional methods
spitethis, predictingthegroundusingonlythepitchangle
[2–4,8]oftenutilizeInversePerspectiveMapping(IPM)to
andheighteffectivelytreatsitasa2-degree-of-freedom(2-
convert2Dfeaturesintoa3Dspace,operatingundertheflat
DoF)problem,whichstillencountersmisalignmentissues,
roadassumption. Thisassumptionfailsonuneventerrains,
particularlyinscenarioswherethegroundslopeisinconsis-
suchasinclinesordeclines,leadingtodistortedrepresenta-
tent,suchastransitionsfromflatareastoinclinedones.
tionsandreducedreliability.
To resolve the misalignment issues that arise from sim-
SALAD [24] tackles 3D lane detection by combining
plistic ground modeling, we propose HeightLane, a di-
front-view image segmentation with depth estimation, but
rect approach to ground modeling as shown in Fig. 1 (c).
it relies on dense depth annotations and precise depth pre-
HeightLane creates a predefined BEV grid for the ground
dictions. Additionally, distant lanes appear smaller, mak-
andgeneratesmultipleheightmapanchorsonthisgrid, as-
ingeachpixelcoverabroaderdepthrange. M2-3DLaneNet
suming various slopes. These heightmap anchors are then
[16]enhancesmonocular3DdetectionbyincorporatingLi-
projected back onto the image to sample front-view fea-
DARdata,liftingimagefeaturesinto3Dspace,andfusing
tures,allowingthemodeltoefficientlypredictaheightmap.
multi-modal data in BEV space, which increases data col-
TobetteraligneachBEVgridpixelwiththe2Dfront-view
lection complexity and cost. Similarly, DV-3DLane [15]
features, height information from the predicted heightmap
uses both LiDAR and camera inputs for 3D lane detection
isaddedtothepositionalencodingoftheBEVgridqueries.
butgenerateslanequeriesfrombothsourcestouseastrans-
Using the predicted heightmap along with deformable at-
formerqueries,ratherthanliftingimagefeatures.
tentionmechanisms,HeightLaneexplicitlyperformsspatial
Meanwhile, BEVLaneDet [22] uses a View Relation
transformationsofimagefeaturesontotheBEVgrid. This
Module [18] to learn the mapping between image features
methodsignificantlyreducesthemisalignmentbetweenthe
and BEV features. For this purpose, the relationship be-
imageandBEVfeatures,ensuringmoreaccuraterepresen-
tweenimagefeaturesandBEVfeaturesmustbefixed. The
tationandprocessing. Byleveragingtheheightmapforpre-
paper introduces a Virtual Coordinate to always warp the
cise ground modeling, HeightLane effectively transforms
imageusingaspecificextrinsicmatrixandintrinsicmatrix.
front-view features into BEV features, thereby improving
Additionally, insteadofusinganchorsforBEVfeatures, it
theaccuracyandrobustnessof3Dlanedetection.
proposes a key-point representation on the BEV to predict
Ourmaincontributionscanbesummarizedasfollows:
lanesdirectly.
• WedefineaBEVgridforthegroundwherelanesare LATR [17] and Anchor3DLane [7] represent recent ad-
detected and explicitly predict the height information vancements in 3D lane detection by assuming the ground
forthisgridfromimages. Unlikepreviousstudiesthat as a plane with 2 degrees of freedom (2-DoF). LATR uses
predictedtheheightofobjects,ourapproachisthefirst ground modeling as positional encoding by predicting the
to explicitly predict the ground height for use in 3D pitch and height of the ground, while Anchor3DLane uses
lanedetection. groundmodelingwithpitchandyawfor2Dfeatureextrac-
tionusinganchors.
• We propose a framework that utilizes the heightmap Buildingontheseapproaches,ourmethod,HeightLane,
toperformeffectivespatialtransformationbetween2D utilizesLiDARonlyduringthecreationofthegroundtruth
imagefeaturesandBEVfeatures. Theheightmapsig- heightmaptomodelthegroundinBEVspace. UnlikeM2-
nificantlyreducesthemisalignmentbetween2Dimage 3DlaneNet [16], which requires both LiDAR and camera
2Figure2. OverallArchitectureofHeightLane. HeightLanetakesa2Dimageasinputandextractsmulti-scalefront-viewfeaturesthrough
aCNNbackbone. Usingpredefinedmulti-slopeheightmapanchors,theextrinsicmatrixT,andtheintrinsicmatrixK,the2Dfront-view
featuresaresampledontoaBEVgridtoobtainBEVheightfeature. BEVheightfeatureisthenprocessedthroughaCNNlayertopredict
the heightmap. The predicted heightmap is used in spatial feature transformation, where the initial BEV feature query and heightmap
determinethereferencepixelsthatthequeryshouldrefertointhefront-viewfeatures. Thefront-viewfeaturesserveaskeysandvalues,
whiletheBEVfeaturesactasqueries.Thisprocess,throughdeformableattention,producesenhancedBEVfeaturequeries.
data during inference, HeightLane simplifies the inference itlyspatiallytransformstheimagefeaturesintoapredefined
processbyrelyingsolelyoncameradata. Insteadofmodel- BEVgridcorrespondingtotheground. Thisapproachsim-
ingthegroundwith2-DoF,ourmethodpredictstheheight plifiesthetaskandaimstoimprovetheaccuracyofspatial
for every point in a predefined BEV grid, creating a dense transformationin3Dobjectdetection.
heightmap. By sampling spatial features focused on the
ground, we generate BEV features that allow accurate 3D 3.Methods
lane prediction using a keypoint-based representation, ef-
The overall architecture of the proposed HeightLane is
fectively bridging 2D image data and 3D lane geometry.
illustrated and described in Fig. 2. Given an RGB front-
This method optimizes the processing of spatial features,
view image I ∈ RH×W×3, where H and W denote the
maintaininghighaccuracywhileenhancingefficiency.
heightandwidthoftheinputimage,aResNet-50[5]CNN
2.2.BEVheightmodeling backboneisutilizedtoextractfront-viewfeaturesF FV. A
predefined BEV grid B ∈ RH′×W′, where H′ and W′
BEVHeight [25] introduced a novel method by adapt- denote the longitudinal and lateral ranges relative to the
ingthedepthbinningtechniqueusedindepthestimationto ego vehicle , representing the ground, is then used in con-
the concept of height. This approach classifies the height junctionwithaHeightExtractionModuletoextractheight
binsofobjectsthroughimages,proposingforthefirsttime information from the front-view features, resulting in a
a regression method to determine the height between ob- heightmap.
jects and the ground in 3D object detection. However, ex- Building upon the insights from previous research with
periments were conducted using roadside camera datasets PersFormer[2],weproposeaheightmap-guidedspatialfea-
[26,28], limiting the scope of the study. BEVHeight’s ture transform framework. This framework is based on
methodaimedtoprovidemoreprecise3Dpositionalinfor- the observation in PersFormer [2] that 2D front-view fea-
mationbyleveragingtheheightinformationofobjects. turescanactasthekeyandvalue,whileBEVfeaturescan
On the other hand, HeightFormer [23] experimented act as the query in deformable cross-attention [30]. The
with the regression of the height between objects and the originalPersFormer[2]researchassumesaflatgroundand
groundusingtheNuscenes[1]autonomousdrivingdataset. usesIPMtotransformfront-viewfeaturesintoBEVfeature
HeightFormer incorporated the predicted height informa- queries.Incontrast,ourapproachusesaheightmapthatpre-
tionintothetransformer’sdecoder,achievingimprovedper- dicts theheight within apredefined BEVgrid B, allowing
formance compared to depth-based approaches. This en- us to match each BEV feature query with the correspond-
hancementdemonstratedthepotentialofutilizingheightin- ing front-view feature without relying on the flat ground
formationformoreaccurate3Dobjectdetection. assumption. This enables more efficient execution of de-
Our proposed method, HeightLane, leverages the fact formableattention.ThesetransformedBEVfeaturesF
BEV
thatlanesarealwaysattachedtotheground. Bypredicting are subsequently processed through a lane detection head,
only the height relative to the ground, HeightLane explic- whichfollowsthekeypoint-basedrepresentationof[22],ul-
3F isasfollows:
FV
F [x,y,:]=concat(F (uθ,vθ)) (3)
Height FV θ∈Θ
where Θ denotes multiple slopes. If the actual road in
theimagehasaslope,usingasingleslopeanchordoesnot
ensurealignmentbetweentheimagefeaturesandtheBEV
grid. Toaddressthis,weusemulti-slopeheightanchorsfor
sampling, thenconcatenatethesefeaturestoformthefinal
BEVheightfeatureF .
Height
Figure3.LiDARaccumulationresultsfortheUp&Downscenario WithF ,heightmapHcanbepredictedas:
Height
intheOpenLane[2]validationset. Thecolorbarontheleftrep-
resentscolorvaluescorrespondingtotheroadheight. H=ψ(F ) (4)
Height
whereH∈RH′×W′,F ∈RH′×W′×C andψiscom-
Height
timatelyproducingthe3Dlaneoutput. posedofseveralconvolutionlayers.
3.1.HeightExtractionModule
3.1.2 HeightSupervision
3.1.1 HeightPrediction
Duetothelackofpointcloudsorlabelsforthegroundinthe
The heightmap, H ∈ RH′×W′ with a resolution of 0.5 OpenLanedataset[2], existingstudieshavefocusedsolely
meters per pixel, represents height information for an area on the areas where lanes are present for data creation and
extending H′ meters forward and W′ meters to each side supervision. LATR [17] applied loss only to the regions
2 2
from the vehicle’s position, where the height is zero. Un- withlanestoestimatetheground’spitchangleandheight.
likeotherresearch[8,17]thatdirectlypredictsroadsurface Similarly, LaneCPP [20] simulated the ground by interpo-
fromfront-viewfeatures, wefirstdefineadenseBEVgrid lating the results in the areas where lanes are present. To
B and then predict the heightmap H for all corresponding provide dense heightmap ground truth, this paper utilizes
heightswithinthisgrid. Thisapproachnecessitatesthecre- theLiDARpointcloudfromWaymo[21],thebasedataset
ation of BEV features, which are derived from 2D front- ofOpenLane. ByaccumulatingtheLiDARpointcloudsof
viewfeatures,toaccuratelycapturetheheightinformation. drivableareasintheWaymodataforeachsceneasFig.3,
For instance, a heightmap with a slope of 0, meaning all adensegroundpointcloudisobtainedforeachscene. This
heightsarezero,isgeneratedandusedasheightmapanchor densegroundpointcloudisthensampledontoapredefined
H˜0 to obtain the 3D coordinates of the BEV grid B. This BEV grid B ∈ RH′×W′, and used as supervision for the
heightmap anchor is then projected onto the image using heightmapH.
intrinsic and extrinsic parameters to sample the front-view
3.2.HeightguidedSpatialTransformFramework
features corresponding to the BEV points. The process of
projecting the x,y grid of the heightmap anchor H˜θ with In this section, we propose a spatial transform frame-
slopeθontotheimageisasfollows: workutilizingtheheightmappredictedinSec.3.1asillus-
tratedinFig.4. TheBEVinitialqueryisflattenedandun-
 uθ  x  dergoes self-attention. During self-attention, BEV queries
 y  interact with each other, and positional encoding is added
v dθθ =KT v→c H˜θ x

(1) toeachBEVquerytoprovidepositionalinformation. The
1 positional encoding is a learnable parameter. While stud-
ies performing attention on 2D FV features [14,17] con-
Here, K and T denote the camera intrinsic matrix and catenate3Draycoordinateswithimagefeaturequeries,our
the transformation matrix from ego vehicle coordinates to methodusesBEVgridcoordinatesandheightembeddings
thecamera,respectively,andH˜θ xisformulatedasEq.(2).It for each BEV query. After the self-attention module, the
shouldbenotedthatwhengeneratingtheheightmapanchor, output query of the self-attention module Ql in the lth
SA
onlythelongitudinalslopeisconsidered,sotheheightvalue layerisrepresentedasfollows:
isdefinedbyθandxvalues.
H˜θ =xtan(θ) (2) Ql SA =SelfAttention(Ql−1,Ql−1+PE(x,y,H x,y))
x
(5)
Alongwiththeprojecteduθ,vθ,theprocessofsampling wherelisthelayerindexandx,yarethegridvaluesofthe
theheightmapfeatureF fromthefront-viewfeature correspondingquery.
Height
4representations. A BEV query is generated for each res-
olution, and the final BEV feature F is obtained by
BEV
concatenatingthequeriesfromeachscale.
3.3.Training
The F generated through the spatial transform
BEV
frameworkpassesthroughseveralconvolutionallayersand
predicts the confidence, offset, and embedding of the
BEV grid following the key-point representation of BEV-
LaneDet [22]. The dense heightmap H predicted by
heightmap extraction module is used as a 3D lane repre-
sentationalongwithconfidence,offset,andembedding.
The loss corresponding to confidence p is the same as
Eq. (8). Here, BCE denotes the binary cross-entropy loss,
andIoUrepresentsthelossfortheintersectionoverunion.
Figure 4. Structure of the Height-Guided Spatial Transform
Framework using deformable attention [2,30]. Flattened BEV
(cid:88)H′ (cid:88)W′
queries receive height positional encoding during self-attention, L c = (BCE(p ij,pˆ ij))+IoU(p,pˆ) (8)
andincross-attention,theheightmapmapsBEVqueriestoimage i=1j=1
pixels.Deformableattentionthenlearnsoffsetstogeneratemulti-
Additionally, the predicted offset loss in the x-direction
plereferencepoints.
ofthelaneisasfollows. σdenotesthesigmoidfunction.
The BEV queries Ql that have undergone self- (cid:88)H′ (cid:88)W′
SA L = BCE(x ,σ(xˆ )) (9)
attention perform deformable cross-attention with the 2D offset ij ij
front-view features. Deformable attention defines a refer- i=1j=1
encepointu,v foreachqueryandlearnsoffsetstothesur- In [22], the embedding of each grid cell is predicted to
rounding areas from this reference point. These learnable distinguishthelaneidentityofeachpixelintheconfidence
offsetsdeterminethefinalreferencepoints,andthefeatures branch. This paper adopts the same embedding loss, as
corresponding to these final reference points in the front- shown in Eq. (10), where L represents the pull loss that
var
view feature Fref act as values in the cross-attention with minimizesthevariancewithinaclusterandL represents
FV dist
the BEV queries. Since we have the BEV heightmap H thepushlossthatmaximizesthedistancebetweendifferent
correspondingtotheBEVgrid,asexplainedinSec.3.1,we clusters.
effectively know the 3D coordinates of the BEV queries.
Therefore, similar to Eq. (1), we can precisely determine L =λ ·L +λ ·L (10)
e var var dist dist
thereferencepointu,vinthefront-viewfeatureontowhich
eachBEVgridpixelwillbeprojectedasfollows: The loss between the predicted heightmap H and the
groundtruthheightmapHGT iscalculatedusingL1loss.
 
  x
u
v d=KT
v→c

Hy x,y
  (6) L h
=(cid:88)H′ (cid:88)W′
(cid:12) (cid:12)H iG jT −H ij(cid:12) (cid:12) (11)
1 i=1j=1
Furthermore, the query Ql that has undergone cross- Finally,toensurethe2Dfeatureeffectivelycaptureslane
CA
attentioninthelthlayerisexpressedasfollows: features, we added a 2D lane detection head and incorpo-
ratedanauxiliarylossfor2Dlanedetectionasfollows:
Ql =CrossAttention(Ql ,Fref) (7)
CA SA FV L =IoU(lane ,laˆne ) (12)
2D 2D 2D
ThespatialtransforminHeightLaneconsistsofmultiple
layers,eachcontainingaself-attentionandacross-attention The total loss is defined as follows, where λ represents
module. Inourexperiments,wesetthenumberoflayersto theweightappliedtoeachlosscomponent:
N = 2. TheBEVquerythathaspassedthroughallN lay-
ersbecomestheBEVfeatureusedastheinputforthelane
detectionhead. Furthermore,tocapturefront-viewfeatures L=λ L +λ L +λ L +λ L +λ L (13)
c c offset offset e e h h 2D 2D
atvariousresolutions,weemployedmulti-scalefront-view
5Figure5. QualitativeevaluationontheOpenLane’svalidationset. Comparedwiththeexistingbestperformingmodel,LATR[17]. First
row:inputimage.Secondrow:3Dlanedetectionresults-Groundtruth(red),HeightLane(green),LATR(blue).Thirdrow:groundtruth
andHeightLaneinY-Zplane.Fourthrow:GroundtruthandLATRinY-Zplane.Zoomintoseedetails.
data, we found that it is densely accumulated in the mid-
dle of each segment and becomes sparse towards the end
frames. For example, Fig. 3 illustrates a scene where the
carstarts,goesuphill,turnsright,andcontinuesonanother
slope. Atthestartingpoint(greenregion),theLiDARdata
is sparse, so bilinear interpolation was used to fill gaps in
theheightmaps,ensuringconsistencyoftheheightmap.The
evaluationcoversdiversescenarios,includingUp&Down,
Curve,ExtremeWeather,Night,Intersection,andMerge&
Split conditions. The evaluation metrics, as proposed by
PersFormer [2], include the F-score, X-error, and Z-error
forbothnearandfarregions.
Figure 6. Visualization of the Heightmap Extraction Module.
4.2.ImplementationDetails
Fromlefttoright: inputimage,predictedheightmap,andground
truthheightmap. We adopted ResNet-50 [5] as the 2D backbone for ex-
tractingimagefeaturesandsettheimagesizeto576x1024.
4.Experiment
To obtain multi-scale image features, we added additional
CNN layers to produce image features at 1/16 and 1/32 of
4.1.Dataset
the input image size, with each feature having 1024 chan-
We evaluated our method using the OpenLane dataset nels.TheBEVgridsizefortheheightmapandBEVfeature
[2], which encompasses a variety of road conditions, wassetto200x48,witharesolutionof0.5metersperpixel.
weather conditions, and lighting scenarios. OpenLane is For the multi-slope heightmap anchors used in the
built on the Waymo dataset [21], utilizing 150,000 images heightmap extraction module, we set the slopes Θ to -5°,
for training and 40,000 images for testing. The OpenLane 0°,and5°. Withaslopeof5°,theheightmapcanrepresent
dataset consists of 798 scenes for training and 202 scenes heightsuptoapproximately8.75meters.
for validation, with each scene comprising approximately IntheHeight-guidedSpatialFeatureTransform,weused
200images.AlthoughOpenLanedoesnotcontaintheinfor- deformableattention[30]with2attentionheadsand4sam-
mationrequiredtocreateheightmaps,itisbasedonWaymo, pling points. The positional encoding was derived by em-
whichallowsustoextractthenecessaryLiDARdatafrom bedding the BEV grid’s X and Y position along with the
WaymoforeachOpenLanescene. WhenextractingLiDAR correspondingpredictedheight.
6Method All Up&Down Curve ExtremeWeather Night Intersection Merge&Split
3DLaneNet[3] 44.1 40.8 46.5 47.5 41.5 32.1 41.7
PersFormer[2] 50.5 42.4 55.6 48.6 46.6 40.0 50.7
Anchor3DLane[7] 53.1 45.5 56.2 51.9 47.2 44.2 50.5
Anchor3DLane+[7] 54.3 47.2 58.0 52.7 48.7 45.8 51.7
BEV-LaneDet[22] 58.4 48.7 63.1 53.4 53.4 50.3 53.7
LaneCPP[20] 60.3 53.6 64.4 56.7 54.9 52.0 58.7
LATR[17] 61.9 55.2 68.2 57.1 55.4 52.3 61.5
HeightLane(Ours) 62.5 51.9 67.0 58.4 56.6 53.4 60.2
Table1. QuantitativeresultscomparisonbyscenarioontheOpenLanevalidationsetusingF-score. Thebestresultsforeachscenario
are highlighted in bold and second-best results are underlined. Anchor3DLane+ is the version of [7] that uses temporal multi-frame
information.
Method F-score(%) X-error(near) X-error(far) Z-error(near) Z-error(far)
3DLaneNet[3] 44.1 0.479 0.572 0.367 0.443
PersFormer[2] 50.5 0.485 0.553 0.364 0.431
Anchor3DLane[7] 53.1 0.300 0.311 0.103 0.139
Anchor3DLane+[7] 54.3 0.275 0.310 0.105 0.135
BEV-LaneDet[22] 58.4 0.309 0.659 0.244 0.631
LaneCPP[20] 60.3 0.264 0.310 0.077 0.117
LATR[17] 61.9 0.219 0.259 0.075 0.104
HeightLane(Ours) 62.5 0.246 0.294 0.111 0.176
Table2. QuantitativeresultscomparisonwithothermodelsontheOpenLanevalidationset. EvaluationmetricsfollowPersFormer[2]:
F-score(higherisbetter),X-error,andZ-error(bothlowerarebetter). Thebestresultsarehighlightedinboldandsecond-bestresultsare
underlined.
4.3.EvaluationonOpenLane maintainscontinuouslanedetectionevenoncurves,show-
casingimpressiveperformance.
4.3.1 QualitativeResult
Fig. 6 visualizes the heightmap predicted by the height
Fig. 5 shows a qualitative evaluation on the validation set extraction module, displaying the input image, predicted
ofOpenLane. ThepredictionsoftheproposedHeightLane, heightmap, and ground truth heightmap from left to right.
theexistingSOTAmodelLATR[17],andthegroundtruth The scenarios depicted from top to bottom are uphill, flat
arevisualized.Thegroundtruthisvisualizedinred,Height- ground, and downhill. Additional visualizations can be
Lane in green, and LATR in blue. The first row of Fig. 5 foundinthesupplementarymaterials.
displays the input images to the model, while the second
rowvisualizesHeightLane, LATR,andthegroundtruthin
4.3.2 QuantitativeResult
3D space. The third and fourth rows present the 3D lanes
viewedfromtheY-Zplane,wheretheY-axisrepresentsthe Tab. 1 presents the quantitative evaluation of HeightLane.
longitudinaldirectionandtheZ-axisrepresentsheight. The HeightLane achieved an overall F-score of 62.5% on the
third row shows HeightLane and the ground truth, and the OpenLanevalidationset, outperformingallexistingSOTA
fourthrowshowsLATRandthegroundtruth. models. Specifically, HeightLane showed significant im-
Notably, HeightLane accurately detects lanes even in provementinExtremeWeather,Night,andIntersectionsce-
scenarioswherethelanesareinterruptedandresume,such narios, achieving the best scores in these challenging con-
as at intersections or over speed bumps. This is particu- ditions. Additionally,HeightLanedemonstratedstrongper-
larly evident in columns 1, 2, 4, 5, and 6 of the Fig. 5. In formance in Curve and Merge & Split scenarios, securing
column1,despitetheocclusionfromacarandpartiallane thesecond-bestperformanceinthesecategories. Although
markings,HeightLanecontinuestodeliverpreciselanepre- HeightLanedidnotperformaswellintheUp&Downsce-
dictions,demonstratingitsrobustnessinhandlingcomplex nario,thisisbecausea2-DoFplaneassumptionsufficesfor
scenes with occlusions and incomplete information. Ad- consistentuphillordownhillslopes. However,HeightLane
ditionally, thanks to the use of the heightmap, HeightLane excelsinscenarioswheretheslopechangesascolumn3in
effectively models changes in slope, as seen in column 3, Fig. 5, demonstrating its capability to fit and predict more
wheretheroadtransitionsfromflattosloped. Incolumns2 freelyinvaryinggradientconditions.
and5,whichdepictcurvedroadsandpartiallyvisiblelanes, Tab. 2 shows the F-score, X-error, and Z-error on the
HeightLanedemonstratessuperiorpredictionaccuracyand Openlane validation set. HeightLane achieved SoTA in
7HeightExtractionMethod F-score(%) Method M F-score X-near X-far Z-near Z-far
ViewRelationModule[22] 57.8 Ours C 62.5 0.25 0.29 0.11 0.18
Single-slopeHeightmapAnchor 56.2 M2-3D[16] C+L 55.5 0.28 0.26 0.08 0.11
Multi-slopeHeightmapAnchor 62.5 DV-3D[15] C+L 66.8 0.12 0.13 0.03 0.05
Ours(GT) C 64.2 0.22 0.29 0.05 0.09
Table3.ComparisonofF-scoresbasedondifferentheightextrac-
tionmethods.Theconfigurationinboldrepresentsthefinalchoice Table 5. Comparison with multi-modal models on the Open-
inthepaper. Lane validation set. Ours (GT) means that we use ground truth
heightmapforspatialfeaturetransformframeworkduringthein-
HeightmapAnchorDesign
F-score(%) ferencestep.Mdenotestheinputmodalitiesused,whereMstands
0° ±3° ±5°
forcameraandLstandsforLiDAR.
✓ 56.2
✓ ✓ 60.7
✓ ✓ 62.5
nels in the final BEV height feature, which in turn raises
✓ ✓ ✓ 62.6
thecomputationalcost. Tomaintainabalancebetweenper-
Table4.ComparisonofF-scoresbasedondifferentheightmapan- formanceandcomputationalefficiency,weultimatelychose
chordesigns.Theconfigurationinboldrepresentsthefinalchoice theconfigurationwith0°and±5°heightmapanchorsforthe
inthepaper. finalmethodinthepaper.
Comparison with Multi-modal Methods Tab. 5 com-
terms of F-score, outperforming all other models with a paresourmethodwithvariousmulti-modal3Dlanedetec-
score of 62.5%. Although it did not match the best- tors. Inthistable,Ours(GT)representstheresultsobtained
performingandsecond-bestperformingmodelsinZ-error, by using the ground truth heightmap instead of the height
itstilldemonstratedcompetitiveresults.IntermsofX-error, extraction module. This substitution aims to observe the
HeightLane achieved the second-best performance, show- performanceofthespatialfeaturetransformframework,as-
casingitsrobustnessinestimatinglanepositionsaccurately sumingthatthepredictedheightmapfromtheheightextrac-
inthelateraldirection. tionmoduleishighlyaccurate.ByusingtheGTheightmap,
which is derived from LiDAR data, we can make a fair
4.4.AblationStudy
comparison with detectors that utilize LiDAR input. The
DifferentHeightExtractionMethodsTab.3showsthe results suggest that when the heightmap is accurately pre-
F-score corresponding to different height extraction meth- dicted, our HeightLane method can achieve performance
ods. The view relation module, initially proposed in [18], that is comparable to or even surpasses models employing
is an MLP module used for transforming BEV features in bothLiDARandcamerainputs. Thisdemonstratesthepo-
[22]. Thesingle-slopeheightmapanchormethodprojectsa tential of our approach to effectively utilize precise height
zero-heightplaneontotheimageandusesthesampledim- information, highlighting the robustness and capability of
agefeaturesfromthisplaneastheBEVfeatures. However, our method in transforming between front-view features
thisapproachassumesaflatplaneandsamplesonlythe2D andBEVfeatures.
image features at this height, resulting in incomplete fea-
ture representation. Consequently, features corresponding
5.Conclusion
toinclinedroadsarenotsampled,eventhoughtheyarepart
of the actual road. In contrast, the multi-slope heightmap
In conclusion, this work resolves key challenges in 3D
anchorproposedinthispaperprojectsmultipleplaneswith
lanedetectionfrommonocularimagesbyimprovingdepth
various slopes onto the image, samples the image features
ambiguityandgroundmodelingwithanovelheightmapap-
fromeachplane,andfusesthemtoformtheBEVfeatures.
proach. OurmaincontributionsincludeestablishingaBEV
Thismulti-anchorapproachachievedthehighestF-score.
gridfordirectheightmappredictionwithmulti-slopeheight
Heightmap Anchor Design Tab. 4 shows the F-scores
anchor, introducing a heightmap-guided spatial transform
forvariousheightmapanchordesigns. Thefirstrowcorre-
framework, and empirically demonstrating the robust per-
sponds to the single-slope heightmap anchor from Tab. 3.
formanceofourHeightLanemodelincomplexscenarios.
Whenusing0°alongwith±3°,theperformanceimproved
by 4.5% compared to using only 0°. Similarly, using 0° The proposed method enhances spatial understanding
along with ±5° resulted in a 6.3% increase. Although and lane recognition, significantly advancing autonomous
the configuration with 0°, ±3°, and ±5° achieved the best vehicle systems through precise 3D transformations en-
performance, the difference was marginal compared to us- abled by the heightmap. Our extensive experiments vali-
ing just 0° and ±5°. However, increasing the number of date the model’s effectiveness, marking a significant step
heightmap anchors directly increases the number of chan- forwardinreal-worldapplications.
8References posefromasingleimageviageometryconstraints. InPro-
ceedings of the AAAI Conference on Artificial Intelligence,
[1] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,
volume36,pages1765–1772,2022. 2
VeniceErinLiong,QiangXu,AnushKrishnan,YuPan,Gi-
[13] RuijinLiu,ZejianYuan,TieLiu,andZhiliangXiong. End-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
to-endlaneshapepredictionwithtransformers. InProceed-
modal dataset for autonomous driving. In Proceedings of
ingsoftheIEEE/CVFWinterConferenceonApplicationsof
the IEEE/CVF conference on computer vision and pattern
ComputerVision,pages3694–3702,2021. 1
recognition,pages11621–11631,2020. 3
[14] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
[2] LiChen,ChonghaoSima,YangLi,ZehanZheng,JiajieXu,
Petr: Positionembeddingtransformationformulti-view3d
Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi,
objectdetection. InEuropeanConferenceonComputerVi-
YuQiao,andJunchiYan. Persformer: 3dlanedetectionvia
sion,pages531–548.Springer,2022. 4
perspectivetransformerandtheopenlanebenchmark.InEu-
[15] YueruLuo,ShuguangCui,andZhenLi. DV-3DLane: End-
ropeanConferenceonComputerVision(ECCV),2022. 2,3,
to-endmulti-modal3dlanedetectionwithdual-viewrepre-
4,5,6,7
sentation.InTheTwelfthInternationalConferenceonLearn-
[3] NoaGarnett,RafiCohen,TomerPe’er,RoeeLahav,andDan
ingRepresentations,2024. 2,8
Levi. 3d-lanenet: end-to-end3dmultiplelanedetection. In
[16] Yueru Luo, Xu Yan, Chaoda Zheng, Chao Zheng, Shuqi
ProceedingsoftheIEEE/CVFInternationalConferenceon
Mei, Tang Kun, Shuguang Cui, and Zhen Li. M²-
ComputerVision,pages2921–2930,2019. 2,7
3dlanenet: Multi-modal 3d lane detection. arXiv preprint
[4] YuliangGuo,GuangChen,PeitaoZhao,WeideZhang,Jing-
arXiv:2209.05996,2022. 2,8
haoMiao,JingaoWang,andTaeEunChoe. Gen-lanenet:A
generalizedandscalableapproachfor3dlanedetection. In [17] YueruLuo,ChaodaZheng,XuYan,TangKun,ChaoZheng,
EuropeanConferenceonComputerVision,pages666–681. Shuguang Cui, and Zhen Li. Latr: 3d lane detection from
Springer,2020. 2 monocularimageswithtransformer. InProceedingsofthe
IEEE/CVF International Conference on Computer Vision
[5] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
(ICCV),pages7941–7952,October2023. 2,4,6,7
Deep residual learning for image recognition. In Proceed-
ingsoftheIEEEConferenceonComputerVisionandPattern [18] Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Ando-
Recognition(CVPR),2016. 3,6 nian, and Bolei Zhou. Cross-view semantic segmentation
for sensing surroundings. IEEE Robotics and Automation
[6] JunjieHuang,GuanHuang,ZhengZhu,YeYun,andDalong
Letters,5(3):4867–4873,2020. 2,8
Du. Bevdet: High-performancemulti-camera3dobjectde-
tectioninbird-eye-view. arXivpreprintarXiv:2112.11790, [19] XingangPan,JianpingShi,PingLuo,XiaogangWang,and
2021. 1 XiaoouTang. Spatialasdeep: Spatialcnnfortrafficscene
[7] ShaofeiHuang,ZhenweiShen,ZehaoHuang,ZiHanDing, understanding. In Proceedings of the AAAI Conference on
Jiao Dai, Jizhong Han, Naiyan Wang, and Si Liu. An- ArtificialIntelligence,volume32,2018. 1
chor3dlane: Learning to regress 3d anchors for monocular [20] MaximilianPittner,JoelJanai,andAlexandruPCondurache.
3d lane detection. In Proceedings of the IEEE Conference Lanecpp: Continuous 3d lane detection using physical pri-
onComputerVisionandPatternRecognition(CVPR),2023. ors. InProceedingsoftheIEEE/CVFConferenceonCom-
2,7 puterVisionandPatternRecognition, pages10639–10648,
[8] Chenguang Li, Jia Shi, Ya Wang, and Guangliang Cheng. 2024. 4,7
Reconstruct from top view: A 3d lane detection approach [21] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
based on geometry structure prior. In Proceedings of the Chouard,VijaysaiPatnaik,PaulTsui,JamesGuo,YinZhou,
IEEE/CVF Conference on Computer Vision and Pattern YuningChai,BenjaminCaine,etal.Scalabilityinperception
Recognition,pages4370–4379,2022. 2,4 forautonomousdriving: Waymoopendataset. InProceed-
[9] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran ingsoftheIEEE/CVFConferenceonComputerVisionand
Wang,YukangShi,JianjianSun,andZemingLi. Bevdepth: PatternRecognition,pages2446–2454,2020. 4,6
Acquisitionofreliabledepthformulti-view3dobjectdetec- [22] RuihaoWang,JianQin,KaiyingLi,YaochenLi,DongCao,
tion. arXivpreprintarXiv:2206.10092,2022. 1 andJintaoXu. Bev-lanedet: Anefficient3dlanedetection
[10] ZhiqiLi,WenhaiWang,HongyangLi,EnzeXie,Chonghao basedonvirtualcameraviakey-points.InProceedingsofthe
Sima,TongLu,YuQiao,andJifengDai.Bevformer:Learn- IEEEConferenceonComputerVisionandPatternRecogni-
ing bird’s-eye-view representation from multi-camera im- tion(CVPR),2023. 2,3,5,7,8
agesviaspatiotemporaltransformers. InProceedingsofthe [23] YimingWu,RuixiangLi,ZequnQin,XinhaiZhao,andXi
EuropeanConferenceonComputerVision(ECCV),2022. 1 Li. Heightformer: Explicit height modeling without extra
[11] LizheLiu, XiaohaoChen, SiyuZhu, andPingTan. Cond- dataforcamera-only3dobjectdetectioninbird’seyeview.
lanenet: a top-to-down lane detection framework based on arXivpreprintarXiv:2307.13510,2023. 3
conditional convolution. In Proceedings of the IEEE/CVF [24] Fan Yan, Ming Nie, Xinyue Cai, Jianhua Han, Hang Xu,
international conference on computer vision, pages 3773– ZhenYang,ChaoqiangYe,YanweiFu,MichaelBiMi,and
3782,2021. 1 LiZhang.Once-3dlanes:Buildingmonocular3dlanedetec-
[12] RuijinLiu,DapengChen,TieLiu,ZhiliangXiong,andZe- tion. InProceedingsoftheIEEEConferenceonComputer
jian Yuan. Learning to predict 3d lane shape and camera VisionandPatternRecognition(CVPR),2022. 2
9[25] Lei Yang, Kaicheng Yu, Tao Tang, Jun Li, Kun Yuan, Li
Wang,XinyuZhang,andPengChen. Bevheight: Arobust
frameworkforvision-basedroadside3dobjectdetection. In
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages21611–21620,2023. 3
[26] Xiaoqing Ye, Mao Shu, Hanyu Li, Yifeng Shi, Yingying
Li, Guangjie Wang, Xiao Tan, and Errui Ding. Rope3d:
Theroadsideperceptiondatasetforautonomousdrivingand
monocular 3d object detection task. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages21341–21350,2022. 3
[27] SeungwooYoo, HeeSeokLee, HeesooMyeong, Sungrack
Yun,HyoungwooPark,JanghoonCho,andDuckHoonKim.
End-to-end lane marker detection via row-wise classifica-
tion. InProceedingsoftheIEEE/CVFconferenceoncom-
putervisionandpatternrecognitionworkshops,pages1006–
1007,2020. 1
[28] HaibaoYu,YizhenLuo,MaoShu,YiyiHuo,ZebangYang,
Yifeng Shi, Zhenglong Guo, Hanyu Li, Xing Hu, Jirui
Yuan, et al. Dair-v2x: A large-scale dataset for vehicle-
infrastructurecooperative 3dobjectdetection. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages21361–21370,2022. 3
[29] Tu Zheng, Yifei Huang, Yang Liu, Wenjian Tang, Zheng
Yang, Deng Cai, and Xiaofei He. Clrnet: Cross layer re-
finement network for lane detection. In Proceedings of
theIEEE/CVFinternationalconferenceoncomputervision,
page898–907,2022. 1
[30] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang,andJifengDai. Deformabledetr: Deformabletrans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159,2020. 3,5,6
10