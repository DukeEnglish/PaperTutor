HELP: Hierarchical Embeddings-based Log Parsing
AndyXu ArnoGau
IudexAI IudexAI
HarveyMuddCollege SanFransisco,California,USA
Claremont,California,USA arno@iudex.ai
andxu@hmc.edu
ABSTRACT convertedintostructuredlogevents[17].Logmessagestypically
Logsareafirst-handsourceofinformationforsoftwaremainte- consistof1)logtemplates-fixedtextoftenwrittenbydevelopers
nance and failure diagnosis. Log parsing, which converts semi- thatdescribethemainportionofloggedeventsand2)logparam-
structuredlogmessagesintostructuredtemplates,isaprerequisite eters - dynamic values of program variables that carry runtime
forautomatedloganalysistaskssuchasanomalydetection,trou- informationfromdifferentexecutions[14].Forexample,thelog
bleshooting,androotcauseanalysis.However,existinglogparsers
"start processing 2 alerts for org org_bff943b3ca"would
failinreal-worldsystemsforthreemainreasons.First,traditional
beparsedintothelogtemplate"start processing <*> alerts
heuristics-basedparsersrequirehandcraftedfeaturesanddomain for org <*>",wherethelogparameters"2"and"org_bff943b3ca"
knowledge,whicharedifficulttogeneralizeatscale.Second,ex- representthenumberofalertsandtheorgidrespectively.
istinglargelanguagemodel-basedparsersrelyonperiodicoffline Astraightforwardapproachtologparsingwouldbetomatch
processing,limitingtheireffectivenessinreal-timeusecases.Third, logswiththeircorrespondingsourcecode.However,inpractice,
existingonlineparsingalgorithmsaresusceptibletologdrift,where sourcecodeisoftenunavailable,particularlyforcommercialsoft-
slightlogchangescreatefalsepositivesthatdrownoutrealanom- wareandthird-partylibraries.Analternativeistoperformmanual
alies.Toaddressthesechallenges,weproposeHELP,aHierarchical log parsing, but due to the large volume of logs generated—up
Embeddings-basedLogParser.HELPisthefirstonlinesemantic- tohundredsofmillionsoflogsperhour—thisiserror-proneand
basedparsertoleverageLLMsforperformantandcost-effectivelog impractical[11,42].Therefore,numerousautomatedlogparsers
parsing.Weachievethisthroughanovelhierarchicalembeddings haveemergedinrecentyears.Theseparserscanbecategorized
module,whichfine-tunesatextembeddingmodeltoclusterlogs intotraditionalheuristics-basedparsers[5,7,10]anddeeplearning
beforeparsing,reducingqueryingcostsbymultipleordersofmag- semantic-basedparsers[14,20,22,45].
nitude.Tocombatlogdrift,wealsodevelopaniterativerebalancing Althoughmanyoftheaforementionedlogparsingmethodshave
module,whichperiodicallyupdatesexistingloggroupings.Weeval- shownstrongperformanceinpreviousbenchmarks,recentstud-
uateHELPextensivelyon14publiclarge-scaledatasets,showing ies[15,17,34]haveshownthatexistinglogparsersfailinhigh
thatHELPachievessignificantlyhigherF1-weightedgroupingand throughputrealworldsystems,particularlythosewithalargeand
parsingaccuracythancurrentstate-of-the-artonlinelogparsers. diversesetoflogtemplates.Thisisbecauseheuristics-basedparsers
We also implement HELP into Iudex’s production observability relyonhand-craftedrulesandspecificdomainknowledge[5,7,10]
platform,confirmingHELP’spracticalityinaproductionenviron- whilesemantic-basedlogparsersoftenrequirelabeleddatawhich
ment. Our results show that HELP is effective and efficient for preventsthealgorithmfromworkingeffectivelywhenappliedto
high-throughputreal-worldlogparsing. newdatasources[20,22].
Inaddition,existingstate-of-the-artlogparsersareoffline,rather
KEYWORDS thanonlinealgorithms[11,14,45].Offlinelogparsersrequireall
logmessagesaprioriandbatchparselogmessagesperiodically.
LogParsing,EmbeddingModels,LargeLanguageModels,Fine-
Incontrast,onlinelogparsersparselogmessagesinastreaming
Tuning
manner.Thismakesonlinelogparsingpreferablefordownstream
applications,asmanyoftheseapplicationsbenefitfromreal-time
1 INTRODUCTION parsingthatcanflagandresolveerrorsquicklytoavoidsystem
downtime.However,thisalsomakesonlineparsingmoredifficult
Modernsoftwaresystemsconsistofamultitudeofinterdependent
duetotheirincompleteaccesstologcontext[38].
servicesandcomponentsthathavebecomeincreasinglycomplex.
Therefore,weproposeHELP,aHierarchicalEmbeddings-based
Whenthesesystemsexperiencedowntime,thiscancausesignif-
LogParser.HELPisthefirstframeworktoutilizesemanticem-
icantrevenueloss,especiallyforlarge-scaledistributedsystems
beddingsandvectordatabasesforlogparsing.Tocreatedomain-
[11],withhourlycostsofserviceoutagesrangingfromhundreds
specificembeddings,wefine-tunedauniversalembeddingmodel
ofthousandstomillionsofU.S.dollars[35,36].
onover300,000logs.Byseparatinglogclusteringandlogpars-
Softwarelogshavebeenwidelydeployedacrossvariousrelia-
ing,HELPmakesonlineLLM-basedparsingperformantandcost-
bilityassurancesystems[11]astheyareoftentheonlydataavail-
effective.
ableforavarietyofdownstreamtaskssuchasanomalydetection
HELPconsistsofthreemaincomponents.Thefirstcomponent
[18,47,48],failuretroubleshooting[3,46],androotcauseanalysis
istheonlinehierarchicalembeddingmodule,whichtakesinraw
[4,30].Therefore,loganalysisplaysacriticalroleinmaintaining
logsandwordfrequencystatistics,generatesvectorembeddingsfor
thestabilityandsecurityofsoftwaresystems.Thefirststepinlog
logsfromtheirsemanticandstructuralmeaning,andclusterslogs
analysisislogparsing,wherebysemi-structuredlogmessagesare
4202
guA
51
]ES.sc[
1v00380.8042:viXraXuetal.
intotheircorrespondingpatterns.Thesecondcomponentisthe (2) Clustering.OfflinelogparserslikeLogCluster[40]andLog-
context-awareparsingmodule,whichleveragesfew-shotandChain Mine[9],andonlineparserslikeLenMa[38]groupsimilar
ofThought(CoT)promptinginLargeLanguageModels(LLMs)to logstogetherviatheirtokenfrequenciesorsimilarity.They
extractlogtemplates.Thefinalcomponentistheiterativerebalanc- considerlogsclusteredtogetherasbelongingtothesame
ingmodule,whichperiodicallymergeslogclustersandreorients template.
vectorembeddingstoimprovegroupingaccuracyandpreventtem- (3) Heuristics. Offline log parsers like AEL [16], and online
platedrift.Byutilizingcosinesimilarity,languagemodels,andword parserslikeSpell[7]andDrain[10]employheuristicsbased
frequencystatistics,HELPcombinestheadvantagesofclustering, onassumptionsaboutlogstructureandtokencountsto
semantic,andheuristics-basedparsers. extracttemplatesefficiently.
WeevaluatedtheperformanceofHELPagainstotherstate-of- (4) Semantic.RecentofflinelogparserslikeLogPPT[20]and
the-artonlineandofflinelogparserson14large-scalelogparsing LILAC[14]utilizethebroadpre-trainedknowledgeofLarge
datasetsinLoghub-2.0[15]fromtheLogPAIteam[51].Againstthe LanguageModels(LLMs),sometimescombinedwithaddi-
state-of-the-artonlineparsers,DrainandUniParser,HELPachieves tionalfine-tuning,toeffectivelyparselogtemplates.
significantly higher accuracy across all metrics with 62.9% and
79.2%higherF1weightedgroupingaccuracy(FGA),and169.0% 2.2 LargeLanguageModels
and189.6%higherweightedtemplateaccuracy(FTA)respectively.
LargeLanguageModels(LLMs)haveshownremarkableperfor-
Againstthestate-of-the-artparser,LILAC,HELPachievescompara-
manceforvariousnaturallanguageprocessingtasks.Thesemodels
bleperformancedespitebeinganonlinealgorithm.Moreover,HELP
adopttheTransformerarchitecture[41]andarepre-trainedona
ishighlyparallelizableduetoit’srebalancingmodule,withtheabil-
vastquantityoftextcorporausingself-supervisedobjectives[2].
itytobatchingestthousandsoflogswithlittletonoperformance
RecentstudieshavehighlightedthatLLM-basedparsers[14,20,45]
degradation.Duetoit’sefficiencyandaccuracy,wesuccessfullyin-
cansignificantlyoutperformotherlogparsers[15,17,34].Most
tegratedHELPintoIudex’sproductionobservabilityplatform.Our
ofthesemethods(e.g.DivLog[45]andLILAC[14])utilizethein-
evaluationresultsconfirmthatHELPcanprocesslogsinreal-time,
contextlearning(ICL)paradigmoflanguagemodels,whereinLLMs
achievinga1.5secondP95end-to-endlatencyfromlogemission
canbeappliedtodownstreamtaskswithouttheresourceintensive
topatternassignment.
taskoffine-tuningmodelparameters[6,21].Theyfirstgrouplogs
Tosummarize,ourmaincontributionsareasfollows:
basedonlogcharacteristicslikecosinesimilarity[45]ortop-Kfre-
• WeproposeHELP,thefirstsemanticembeddings-basedlog quenttokens[14],thensamplecandidatestomaximizesimilarity
parserforperformantandcost-effectiveonlinelogcluster- ordiversity.
ingandparsing. However,despitepromisingresults,allLLM-basedparsersfail
• WeevaluatedHELPon14publiclogdatasets.Ourexperi- tosupportonlineparsing.SinceLLMshavebillionsofweightsand
mentalresultsshowthatHELPoutperformsallotherstate- requireextensiveGPUcomputeforinference,theyareoftende-
of-the-artlogparsersinloggroupingandparsing,andcan ployedonhigh-performanceserversandarequeriedthroughAPIs.
easilybeconvertedintoaparallelizablebatchprocessing ThismakessequentialqueryingofLLMs[19,45]impracticalinpro-
frameworkwithlittleperformancedegradation. ductiondeploymentforitslatencyandcost[15,27].Forexample,
• WeconfirmedthepracticalityandefficiencyofHELPbyde- asshowninFigure1,azeroshotquerysimilartotheoneusedby
ployingHELPinaproductionenvironment.HELPachieves LILACwouldrequired227tokensfortheprompttemplateand60
real-timepatternassignmentandavoidstemplatedriftvia tokensforthelogitselfperOpenAI’stokenizer[32].Thisgenerates
itsperiodicrebalancingmodule. anoutputforthelogtemplatethatrequires14tokens[32].This
meansthatforeverymillionlogs,thecostofqueryingOpenAI’s
2 BACKGROUNDANDMOTIVATION cheapestmodelGPT-4ominiwouldbe (227+60)∗1,000,000∗
(0.15/1,000,000)+(14)∗1,000,000∗(0.6/1,000,000)=$51.45per
2.1 LogParsing
1Mlogs,assumingthepriceofGPT-4ominiis$0.15per1Minput
Log parsing is the initial and most critical step in log analysis tokensand$0.60per1Moutputtokens[31].Giventhatsystemscan
[11,51],wherebyparsersconvertsemi-structuredlogmessages generateuptohundredsofmillionsoflogsperhour[11,42],this
intostructureddata.Thisisdonebyextractingtheconstant(log cancosthundredsofthousandsofdollarsaday,makingLLM-based
templates)anddynamic(logparameters)partsfromlogmessages. onlineparsinginfeasibleinpractice.
Although some studies have extracted log templates from log-
gingstatementsinsourcecode[33,37],thisisoftenimpractical 2.3 EmbeddingModels
whensourcecodeisinaccessible,suchasincommercialsoftware
AlthoughsequentialqueryingofLLMsremainsexpensiveandin-
andthird-partylibraries[51].Consequently,manyautomatedlog
feasibleatscale,embeddingmodelsrepresentanalternativeforlog
parsershaveemergedinrecentyears.Thesedata-drivenlogparsing
clustering.Embeddingmodelsenableasimilarhigh-levelsemantic
approachescanbedividedintofourmaingroups:
understandingoflogswhilerequiringsignificantlyfewercompu-
(1) Frequentpatternmining.OfflinelogparserslikeSLCT[39], tationalresources,reducinglatencyandcost.Embeddingmodels
LFA[28],andLogram[5]findfrequentpatternsthatemerge canbecategorizedintotwogroups:wordandtextembeddings.
acrosstheentiredataset,leveragingtokenpositionorn- Wordembeddingsrepresenteachindividualwordasitsownvector,
graminformationtoextractlogtemplates. whiletextembeddingscompressvariablelengthtextsequencesintoHELP:HierarchicalEmbeddings-basedLogParsing
Figure1:ExamplezeroshotLLMquerywithtokenizervisu- Figure2:OverviewofHELPcomponentswheninsertinga
alization. newlogtemplate
asinglevectorrepresentation.Althoughpreviousmethodshave reflectonitsexistingloggroupings,combiningsimilarclustersand
appliedwordembeddingsforlogclusteringandanomalydetec- avoidingtemplatedrift.
tion[25,26,40,44],textembeddingsareadvantageousduetotheir Figure2isanoverviewofHELP’sonlineingestionpipeline.
contextualunderstandingacrosstheentiretyofalogandtheirin- HELPfirsttakesinrawlogsandwordfrequencystatistics,and
variancetologlength.OnesuchtextembeddingmodelisOpenAI’s generatesacustomvectorembedding.Then,HELPperformsan
text-embedding-3-small,whichisinitializedfrompre-trainedGPT approximatenearestneighbor(ANN)searchofthevectordatabase
modelsandrefinedviaunsupervisedcontrastivetrainingforse- containingexistinglogtemplates.Ifthecosinesimilarityisbelow𝑇 𝑐,
manticsimilarity[29].Comparedtothecheapestcompletionmodel, HELPwillinsertthenewvectorintothevectordatabase.Otherwise,
OpenAI’stextembeddingmodelisanorderofmagnitudecheaper HELPwillmergethetwovectorstogether,andtheexistinglog
bytoken[31].Sincenopromptoroutputisrequired,muchfewer templateofthegroupwillbeassignedtothevector.Inaddition,
tokensarerequired,furtherreducingthecostofembeddingmodels. theparsingmodulewillbecalled,whichwillparseandcreatea
Thismakesembeddingmodelsuniquelysuitedtosupportonline logtemplatebasedontheloginthenewcluster.Fortheoffline
logclustering,enablingdownstreamcluster-basedlogparsing. rebalancingprocessinvokedevery𝑁 logs,therebalancingmodule
willsequentiallysearchfortheclosestneighborforeveryvector
3 METHODOLOGY andmergethemiftheircosinesimilarityisabove𝑇 𝑐.
3.1 Overview
3.2 HierarchicalEmbeddingsModule
Inthissection,weintroduceHELP,aHierarchicalEmbeddings-
DirectlyqueryinganLLMforonlinelogparsingisexpensiveand
basedLogParser.HELPconsistsofthreemaincomponents:the
inefficientduetothelargevolumeoflogdata.Therefore,HELP
onlinehierarchicalembeddingmodule,thecontext-awareparsing
firstembedslogsintovectors,andthenclustersvectorsbasedon
module,andtheiterativerebalancingmodule.TospecializeHELP
theircosinesimilarity.
forlogdata,wetrainaneuralnetworkencoderonover300,000
logswhichwestackontopoftheuniversalOpenAIembedding 3.2.1 Embedding. Whilethesemanticmeaningoflogsiscrucialfor
model.Wefurtherutilizethein-contextlearningparadigmofthe clustering,relyingsolelyonsemanticembeddingscancreateoverly
LLMbyprovidingfew-shotexamplesofparsedlogscoupledwith specificgroups(e.g.groupsbasedonusersorfilepaths)thatdonot
chainofthoughtreasoningtoexplainparsingrationale.Toenable generalizewelltologtemplates.Torefinetheembeddingsforlog
onlineparsing,HELPintroducesanovelefficientandcost-effective templateclustering,wemaketwokeymodificationstoOpenAI’s
embeddings-basedclusteringpipeline.HELPisbuiltonthepremise universalembeddings:
that there exist many orders of magnitude fewer log templates 1)Wordcount:Previousworkhaveshownthecorrelationbe-
thanlogsinreal-worldsystems[15,42].Forexample,thedatasets tweenthewordcountofalogmessageanditslogtemplate[38].
inLoghub-2.0containover50millionlogs,butthetotalnumber However,LLMsandembeddingmodelsstruggletoaccuratelydeter-
oftemplatesislessthan3,500[15].Therefore,byfirstclustering minewordcountduetotheirbyte-pairencoding,whereindividual
logsandqueryinganLLM oncepercluster,HELPsignificantly tokenscanconsistofpartsofwordsormultiplewords[29].Em-
increasesparsingefficiency,enablingthefirstLLM-basedonline pirically,LogPAI[15]showedthatsemantic-basedlogparsersare
parsingmethod.Viaitsiterativerebalancingmodule,HELPcan superioratlogparsing,butareweakeratgroupingbecauseoftheirXuetal.
Algorithm1HierarchicalEmbeddingsModule
Require: Setoflogs𝐿={𝑥 𝑖}𝑛 𝑖=1,similaritythreshold𝑇 𝑐,OpenAI
embedding𝜖(·),nnencoder𝜙(·)
1: InitializesetofexistingvectorsV =∅
2:
for𝑥
𝑖
∈𝐿do
3: 𝑣 𝑖′ =𝜖(𝑥 𝑖)
4:
𝑣
𝑖
=𝜙(𝑣 𝑖′)
5: 𝑣 𝑖 = ∥𝑣 𝑣𝑖 𝑖∥
6:
(𝑣 𝑗,𝑐)=simSearch(𝑣 𝑖,V)
7: if𝑐 <𝑇 𝑐 then
8: V =V∪{𝑣 𝑖}
9: else
1 10 1:
:
𝑤 𝑣 𝑗𝑗 == 𝑣g 𝑗e +tW 𝑣 𝑤𝑖e − 𝑗i +g 𝑣 1h 𝑗t(𝑣 𝑗) F exig au mre pl3 e: .Context-awareparsingmoduletemplategeneration
12: 𝑣 𝑗 = ∥𝑣 𝑣𝑗 𝑗∥
13: Update𝑣 𝑗 inV
Instead,HELPemploysanincrementalvectorupdatethatapproxi-
14: endif
matesthecentroidofeachclusterviaamovingaverage.
15: endfor
ThedetailedalgorithmisshowninAlgorithm1.Foreachlog𝑥
𝑖
inthesetoflogs𝐿,HELPfirstencodesthelogusingtheOpenAI
embedding𝜖(·)toobtainanintermediatevector𝑣 𝑖′.Thisvectoris
lackofglobalinformation.Therefore,HELPembedsthewordcount passedthroughtheneuralnetworkencoder𝜙(·)andnormalized,
ofeachloginadditiontoitslogcontent.Thisimprovesclustering resultinginthefinalvector𝑣 𝑖.HELPthenperformsasimilarity
accuracyasthewordcountofthecurrentlogiscomparedwith searchsimSearch(𝑣 𝑖,V)acrosstheHierarchicalNavigableSmall
previousingestedlogs.Thus,theingestionprocessconsidersboth Worlds(HNSW)[23]graphV,comparing𝑣 𝑖 againsttheexisting
macro-levelsemanticandstatisticalfeaturesoflogmessages. vectorsinVandyieldingtheclosestmatch𝑣 𝑗 andcorresponding
2) Custom Embedding: To fine-tune OpenAI’s embedding similarityscore𝑐.NotethattheuseofHNSWallowsfor𝑂(𝑙𝑜𝑔(𝑁))
modelforlogtemplateclustering,westackatwolayerlinearneural search [23], which scales better for large datasets than a naive
networkencoderontopoftheOpenAIembeddingoutput.This k-NearestNeighbor(k-NN)𝑂(𝑁) search.Ifthesimilarityscore
allowsHELPtofine-tuneOpenAI’sembeddingmodelwithoutneed- 𝑐 isbelowthethreshold𝑇 𝑐,thevector𝑣 𝑖 isaddedtothesetV.
ingaccesstotheweightsoftheblackboxmodel,makingthepro- Otherwise,theexistingvector𝑣 𝑗 isupdatedinV suchthat𝑣 𝑗 =
c ises trs ac inh ee dap ve ir a, 1e 4as fi oe lr d,a cn rod sl se vss ald ida ata ti- oin nt ,e wns hi ev re e.T thh ee mn oeu dera ll isn te rt aw ino er dk 𝑣 cl𝑗 u+ ste𝑣 𝑛𝑖 r− + c𝑣 1 o𝑗 r, rw esh pe or ne d𝑛 in= gg toet 𝑣W 𝑗.eight(𝑣 𝑗)isthenumberoflogsinthe
andvalidatedon13ofthedatasets,andtestedonthelastdataset.
Thetwolayerlinearneuralnetworkarchitecturewaschosendue 3.3 Context-AwareParsingModule
toitssimplicity,whichpreservestherichsemanticinformation
WhenHELPcreatesanewlogcluster,thecontext-awareparsing
presentintheuniversalOpenAIembeddingsandreducestheriskof
module parses the log via the state-of-the-art LLM, Claude-3.5-
overfitting.EachdatasetconsistsofOpenAIembeddingslogpairs,
Sonnet[1],wheretheresultantlogtemplatewillbeassignedto
labeledassimilar(1)ordissimilar(0)basedontheirlogpatterns.
allsubsequentlogsinthecluster.Theparserleveragesthestrong
Eachembeddingispassedthroughthesameneuralnetwork,and
textualandsemanticcomprehensionofLLMs,whichhaveshown
themodelminimizestheMeanSquaredError(MSE)lossbetween
remarkablezero-shotandfew-shotperformanceacrossmanynatu-
thepredictedcosinesimilarityoftheneuralnetworkencoderem-
rallanguageprocessingtasks.Thus,althoughwedonotprovide
beddingsandthegroundtruthlabels.TheMSElossfunctionis
logsamplesfromthesamelogsourceincontext,webelievethat
definedasfollows:
byelicitingstrongreasoningcapabilitiesandprovidingin-domain
𝑁
1 ∑︁ knowledge,theLLMcanstilleffectivelyparselogtemplates.
MSE= (𝑦 𝑖 −𝑦ˆ𝑖)2 (1)
𝑁
𝑖=1 3.3.1 PromptDesign. Followingpreviouswork[12],wedesign
thepromptformattoquerytheLLMandgeneratethelogtem-
where𝑦 𝑖 arethetargetvalues,𝑦ˆ𝑖 arethepredictedvalues,and𝑁 is
plateforindividuallogmessages.Ourpromptconsistsoffiveparts:
thenumberofpairs.Thecompletelistofhyperparametersusedfor
taskinstructions,parameterexamples,outputconstraints,parsing
trainingandtestingaredescribedinsubsection4.4.
demonstrations,andthequeriedlog.Figure3showsanexample.
3.2.2 VectorUpdate. Asmentionedinsubsection3.1,thereexist (1) TaskInstructions.ToprovidetheLLMwithtask-specific
ordersofmagnitudefewerlogtemplatesthanlogs.Therefore,in loginformation,weutilizeaspecialsystemprompttogive
ordertoreducestorageandcomputationalcosts,HELPstoresone an overview of log parsing and align the model for log
vectorpercluster.Thismeans,however,thattraditionalclustering analysis.Wealsoprovideinstructionsthatincludebasic
algorithmslikeK-Means[13]andDBSCAN[8]cannotbeapplied. requirements,whichspecifythetasktoextractdynamicHELP:HierarchicalEmbeddings-basedLogParsing
variablesfromlogs,andadviceonparameters,whichgive Algorithm2IterativeRebalancingModule
descriptorsforcommondynamicandstaticelements. Require: SetofvectorsV ={𝑣 𝑖}𝑚 𝑖=1,similaritythreshold𝑇 𝑐
(2) ParameterExamples.Toprovidedomainknowledgeto 1: Initializeindex𝑖 =0
themodel,weprovidespecificexamplesofcommondy- 2: while𝑖 < |V|do
namicparameterssuchasanip"192.168.0.1:8008"or
3:
(𝑣 𝑗,𝑐)=simsearch(𝑣 𝑖,V)
boolean"true".Theuseofdomainspecificplaceholders 4: ifcosSim≥𝑇 𝑐 then
ratherthanuniversal{variables}allowstheLLMtobet- 5: 𝑤 𝑖 =getWeight(𝑣 𝑖)
tercontextualizeandrecognizedynamicvalues. 6: 𝑤 𝑗 =getWeight(𝑣 𝑗)
(3) OutputConstraintsToensurethemodelcreatesaconsis- 7: V =V\{𝑣 𝑖,𝑣 𝑗}
t
e
lie
a
mn ct
h
it, ep
t
dea mr bs ypib
l
bale
ate
co kiu
s
tt ipp ckru
e
st
p
., ew ne ds ep de bc yify "Lt oh ge Td ee msi pr le ad teo [u idtp xu ]"t, aw ndhe dr ee
-
98 :: 𝑣𝑣 == 𝑤
∥𝑣
𝑣𝑖· ∥𝑤𝑣𝑖 𝑖+ +𝑤 𝑤𝑗 𝑗·𝑣𝑗
(4) ParsingDemonstrations.Few-shotdemonstrationsof 10: V =V∪{𝑣}
logparsinghavebeenshowntogreatlyimproveperfor- 11: else
mance on downstream tasks [49]. In addition, Chain of 12:
𝑖 =𝑖+1
Thoughthasbeenshowntosignificantlyimprovetheabil- 13: endif
ityofLLMstoperformcomplexreasoning[43].Therefore, 14: endwhile
HELPprovidesfourexamplesoflogs,theirreasoning,and 15: returnV
theircorrespondinglogtemplates.Tokeepthereasoning
briefandself-contained,reasoningisdenotedbythepair
oftags<Inner Monologue>and</Inner Monologue>. 3.5 Parallelization
(5) QueriedLog.Finally,thelogmessage,prependedby"Log[idx]"
Inprevioussections,wehavediscussedhowHELPcanbedeployed
isfedintothemodel.
inasequentialfashion.However,inpractice,multipleservicescan
emitlogsconcurrently,requiringmultiplelogstobeprocessedat
3.3.2 Template Extraction. After receiving a response from the once.Therefore,wealsodeployHELPinanonline,batchedfashion,
LLM,weuseregexpost-processingtoextractvalidtemplatesdes- wherebythehierarchicalembeddingsmodulecanembed,search,
ignatedbybacktickedstringswitha"LogTemplate"prefixsimilar anduploadvectorsintothevectordatabaseinparallel.
toULog[12].Then,wereplaceallbracketedparametersbythe Akeyedge-casewiththeproposedparallelizationariseswhen
placeholder<*>,andmergecommonplaceholders(e.g.<*><*> → multiplelogswiththesameunseenlogpatternareprocessedsimul-
<*>followingpreviouswork[14,20,22,45]. taneously.Sincelogsareprocessedinparallelandcannot"see"each
otherinthesimilaritysearch,eachlogwillindependentlyreturna
similarityscorelessthanthethreshold.Thiscausesduplicatesof
3.4 IterativeRebalancingModule thesamepatterntobeinserted.Theiterativerebalancingmodule
Becausethehierarchicalembeddingsmoduleperformsavector resolvesthisissuebycombiningalllogsthatshouldbelongtothe
updatewhichreorientscentroidsovertime,vectorsfromdifferent samecluster.ToreducequeryingcostswhendeployingHELPin
clusterscandriftclosertogether,indicatingthattheyshouldbe parallel,thecontext-awareparsingmoduleprocessesnewpatterns
mergedintoasinglecluster.Thisisaninherentdisadvantageto aftertheiterativerebalancingmodulehasmergedexistingvectors,
onlineclusteringmethods,wherenotallinformationisavailableat ratherthanatinsertionintothevectordatabase.
inference.Toaddressthis,HELPincorporatesanIterativeRebal-
4 EXPERIMENTALDESIGN
ancingModule,detailedinAlgorithm2anddepictedinFigure2,to
periodicallyreflectandmergeexistingclusters. 4.1 ResearchQuestions
Foreachvector𝑣 𝑖inthesetofvectorsV,thealgorithmperforms
Weevaluateourapproachbyansweringthefollowingresearch
asimilaritysearchsimsearch(𝑣 𝑖,V) tofindtheclosestmatch𝑣
𝑗
questions(RQs):
andthecorrespondingsimilarityscore𝑐.Ifthesimilarityscore𝑐
isgreaterthanorequaltothethreshold𝑇 𝑐,thealgorithmmerges • RQ1: HoweffectiveisHELP?
thetwovectors.Thenumberoflogsineachcluster𝑤 𝑖 and𝑤 𝑗 of • RQ2: HowdoeseachcomponentcontributetoHELP?
thevectors𝑣 𝑖 and𝑣 𝑗 areretrievedusingthefunctiongetWeight(·). • RQ3: HowpracticalisHELPinaproductionenvironment?
Thevectors𝑣 𝑖 and𝑣 𝑗 arethenremovedfromthesetV,andanew
vector𝑣 iscomputedastheweightedaverageof𝑣 𝑖 and𝑣 𝑗,given 4.2 Datasetsandbaselines
by𝑣 = 𝑤𝑖· 𝑤𝑣𝑖 𝑖+ +𝑤 𝑤𝑗 𝑗·𝑣𝑗 andnormalized.Theupdatedvector𝑣 isthen WeevaluateHELPonLoghub-2.0[15,50],acollectionoflarge-
addedbackintothesetV.Ifthesimilarityscore𝑐islessthanthe scalebenchmarkdatasetsforlogparsingfromtheLogPAIteam
threshold𝑇 𝑐,thealgorithmincrementstheindex𝑖andcontinues [51].Loghub-2.0containsground-truthtemplatesfor14diverse
tothenextvector. logdatasets,coveringdistributedsystems,supercomputersystems,
Sincetheindex𝑖isonlyincrementedif𝑐 <𝑇 𝑐,thisallowsfora andserver-sideapplications.Onaverage,eachdatasetinLoghub-
singlevectortobesuccessivelymergedwithmultipleothervectors. 2.0contains3.6millionlogmessages,andthereareapproximately
ThisprocessrepeatsuntilallvectorsinVhavebeenprocessed. 3,500uniquelogtemplatesintotal.Xuetal.
WecompareHELPagainstsixopen-sourcedstate-of-the-artlog clustersthanoverlygenerallogclusters,asthispreventsthewrong
parsers,includingthreeonlineandthreeofflineparsers.Fortheon- templatefrombeingassignedtologsandcombiningmultiplelog
lineparsers,weimplementtwosyntax-basedmethods,LenMa[38], clusterswiththesametemplateiscomparativelyeasier.Thecode
andDrain[10],andonesemantic-basedmethod,UniParser[22]. isimplementedinPyTorch2.3.Duringtraining,weusetheAdam
LenMaandDrainwerechosenfortheirsuperiorperformancecom- optimizerandchoseaninitiallearningrateof0.0005.Wesetthe
paredtoothersyntax-basedonlineparsers[15,17,51].UniParser batchsizeto2048andtrainfor50epochs.Wealsouseasimilarity
isthestate-of-the-artsemantic-basedonlineparserandtrainsa threshold𝑇 𝑐 forlogclusteringof0.9.
bidirectionallong-shorttermmemory(BiLSTM)modelonlabeled
logdataforlogparsing.Fortheofflineparsers,weselectonesyntax- 5 EVALUATIONRESULTS
basedmethodAEL[16]andtwosemantic-basedmethodsLogPPT
5.1 RQ1:HoweffectiveisHELP?
[20]andLILAC[14].LogPPTuseslabeledlogdataforprompt-based
fine-tuningofRoBERTawhileLILACutilizestheICLparadigmof TodeterminetheefficacyofHELPcomparedtootherstate-of-the-
LLMstoparselogsbysamplingfrom𝑘similarlogs. art log parsers, we evaluate all log parsers on the four metrics
mentioned in subsection 4.3. The results are shown in Table 1,
4.3 Metrics wherethebestresultforeachmetricismarkedinbold,whilethe
secondbestresultisunderlined.Followingpreviousworks[15,17],
Followingpreviousstudies[15,17,51],weevaluateallmethods
ifaparsercannotfinishparsinginareasonabletime-frame(12
usingthefollowingfourmetrics:
hours),itsscoreisdenotedas"—".
• GroupingAccuracy(GA)isalog-levelmetricthatiscom- OurresultsshowthatHELPsignificantlyoutperformsallother
putedastheratioofcorrectlygroupedlogmessagesover onlinelogparsers,andachievesthesecondbestresultsoverall,
alllogmessages.Alogmessageisconsideredgroupedcor- onlyperformingslightlyworsethanthebestofflineparserLILAC.
rectlyifandonlyifitspredictedtemplatehavethesame Additionally,HELPachievesconsistentlyhighaccuracyacrossall
groupoflogmessagesasthegroundtruth. 14Loghub2.0datasets,achievingthebestFTAon14of14datasets
• F1 score of Grouping Accuracy (FGA) is a template-level andthebestFGAon10of14datasetscomparedtootheronlinelog
metricthatmeasurestheratioofcorrectlygroupedtem- parsers.ThisindicatesHELP’srobustnessinlogparsingdatafrom
plates.Let𝑁 𝑔 bethecorrectnumberoftemplatesinthe adiversesetoflogsources.
groundtruthdatasetand𝑁 𝑝 bethenumberoftemplates Intermsofgroup-relatedmetrics(i.e.GAandFGA),Drain,the
generatedbythelogparser.Suppose𝑁 𝑐 isthenumberof bestonlinebaselineachievesaGAof0.840andFGAof0.550.HELP
templatescorrectlyparsedbythelogparser.Then,Precision achieves4.8%and62.9%higherGAandFGAcomparedtoDrain,
ofGroupingAccuracy𝑃𝐺𝐴= 𝑁𝑁 𝑝𝑐 andRecallofGrouping withaGAof0.880andFGAof0.896.NotethattheFGAofDrain
A 𝐹𝐺cc 𝐴ur =ac 2y ×𝑅 𝑃𝐺 𝐺𝐴𝐴 ×= 𝑅𝐺𝑁𝑁 𝐴𝑔𝑐.FGAistheirharmonicmean,where i rs elm yu oc nh hl ao nw der crt ah fa ten dH feE aL tP u. reT sh ti hs ai ts sb te ruca gu gs lee ts oyn ht aa nx d-b leas ae ld arp ga ers ae nr ds
𝑃𝐺𝐴+𝑅𝐺𝐴 diversesetoftemplates.Ontheotherhand,HELP’suseofsemantic
• ParsingAccuracy(PA)isalog-levelmetricthatmeasures
embeddingsallowsittobettercatchedgecasesandachievesuperior
thecorrectnessofextractedtemplates.PAiscomputedas
performanceongroupingaccuracy.
theproportionofcorrectlyparsedlogmessagestothetotal
Intermsofparsing-relatedmetrics(i.e.PAandFTA),UniParser
numberoflogmessages.Alogmessageiscorrectlyparsed
achieved the highest PA of 0.680 and the highest FTA of 0.260
ifandonlyifalltokensareidenticalwiththegroundtruth.
amongallonlinebaselines.HELPachieves22.0%higherPAand
• F1scoreofTemplateAccuracy(FTA)isatemplate-levelmet-
189.6%higherFTAcomparedtoUniParser,withaPAof0.830and
ricthatcomputestheharmonicmeanofprecisionandrecall
aFTAof0.753.Thisillustrateshowleveragingtheextensivepre-
oftemplateaccuracy.Atemplateisconsideredcorrectifand
trainedknowledgeofuniversalLLMsispreferabletotraininga
onlyiflogmessagesoftheparsedtemplatehavethesame
domain-specificmodelfromscratchforlogparsing.
groupastheground-truthtemplatelogsandalltokensin
Comparedwithexistingofflinelogparsers,HELPoutperforms
thetemplatearethesameastheground-truth.
AELandLogPPTbyanaverageof88.0%and43.0%acrossallfour
metricsandachievescomparableperformancetotheexistingstate-
4.4 EnvironmentandImplementation
of-the-artparserLILAC.Specifically,ondatasetswithalargenum-
WeconductallexperimentsonanM3MacbookAirwith24GBof beroftemplateslikeBGL(320),Hadoop(236),HealthApp(156),
RAM.Weusethetext-embedding-3-smallmodelforembeddings Linux(338),Mac(626),Spark(236),andThunderbird(1,241),HELP
duetoitscheapinferencecostsandstrongperformance[31].Weuse outperformsLILACby4.0%inFGA,7.0%inPA,and3.0%inFTA.
Anthropic’sClaude-3.5-SonnetmodelfromJuly2024[1]becauseof Thisillustrateshowfine-tunedsemanticembeddingsoutperform
itssuperiorperformancecomparedtoOpenAI’sGPTmodels,and LLMandtokenbasedmethodsonsystemswithlargeanddiverse
setitstemperatureto0sothesameoutputwouldbegeneratedper logtemplates.BecauseHELPistunedtobemoresensitivetocreate
querytoensurereproducibilityandconsistency. overlyspecifictemplates,HELPperformsworseondatasetswith
Fortheneuralnetworkencoderusedinthehierarchicalembed- fewertemplates.Thisincreasedspecificityhelpsreducefalseneg-
dingsmodule,24,000pairsofembeddingsareselectedatrandom atives,whicharemoreconsequentialinlogparsingcomparedto
fromineachdatasetina1:5ratioofsimilartodissimilarpairs.We falsepositives.Thisisbecausefalsenegativescreatesilenterrors
chosethisratiosinceitispreferabletocreateoverlyspecificlog thatcancausesystemoutageswhilefalsepositivesareshownasHELP:HierarchicalEmbeddings-basedLogParsing
Table1:AccuracyofHELPcomparedtostate-of-the-artbaselinesonpublicdatasets.
Method Metric Apache BGL Hadoop HDFS HealthApp HPC Linux Mac OpenSSH OpenStack Proxifier Spark Thunderbird Zookeeper Average
OfflineLogParsers
GA 1 0.915 0.823 0.999 0.725 0.748 0.916 0.797 0.705 0.743 0.974 — 0.786 0.996 0.860
FGA 1 0.587 0.117 0.764 0.008 0.201 0.806 0.793 0.689 0.682 0.667 — 0.116 0.788 0.560
AEL
PA 0.727 0.406 0.535 0.621 0.311 0.741 0.082 0.245 0.364 0.029 0.677 — 0.163 0.842 0.440
FTA 0.517 0.165 0.058 0.562 0.003 0.136 0.217 0.205 0.333 0.165 0.417 — 0.035 0.465 0.250
GA 0.786 0.311 0.533 0.694 0.839 0.782 0.200 0.536 0.278 0.534 0.51 0.450 0.416 0.973 0.560
FGA 0.484 0.519 0.570 0.385 0.863 0.812 0.686 0.459 0.115 0.929 0.750 0.383 0.334 0.903 0.590
LogPPT
PA 0.952 0.855 0.725 0.897 0.987 0.997 0.621 0.41 0.713 0.408 0.993 0.954 0.283 0.843 0.760
FTA 0.352 0.509 0.462 0.308 0.787 0.762 0.399 0.275 0.147 0.788 0.917 0.331 0.131 0.753 0.490
GA 1 0.894 0.872 1 1 0.869 0.971 0.876 0.690 1 1 1 0.806 1 0.927
FGA 1 0.859 0.962 0.968 0.967 0.907 0.931 0.825 0.838 1 1 0.901 0.793 0.967 0.924
LILAC
PA 0.996 0.958 0.832 0.999 0.687 0.705 0.765 0.638 0.941 1 1 0.973 0.559 0.687 0.842
FTA 0.862 0.746 0.779 0.946 0.868 0.800 0.740 0.553 0.865 0.979 1 0.759 0.572 0.868 0.810
OnlineLogParsers
GA 0.993 — 0.796 0.999 — 0.793 0.806 0.701 0.748 0.851 0.504 — — 0.857 0.800
FGA 0.900 — 0.059 0.831 — 0.035 0.616 0.107 0.800 0.395 0.189 — — 0.661 0.46
LenMa
PA 0.031 — 0.052 0.137 — 0.642 0.035 0.121 0.142 0.019 0.495 — — 0.683 0.24
FTA 0.267 — 0.009 0.404 — 0.015 0.17 0.021 0.200 0.06 0.151 — — 0.263 0.16
GA 1 0.919 0.921 0.999 0.862 0.793 0.686 0.761 0.707 0.752 0.692 0.888 0.831 0.994 0.840
FGA 1 0.624 0.785 0.935 0.01 0.309 0.778 0.229 0.872 0.007 0.206 0.861 0.237 0.904 0.550
Drain
PA 0.727 0.407 0.541 0.621 0.312 0.721 0.111 0.357 0.586 0.029 0.688 0.394 0.216 0.843 0.470
FTA 0.517 0.193 0.384 0.609 0.004 0.152 0.259 0.069 0.487 0.002 0.176 0.412 0.071 0.614 0.280
GA 0.287 0.549 0.718 1 0.447 0.794 0.263 0.886 0.495 1 0.51 0.853 0.440 0.996 0.660
FGA 0.256 0.641 0.490 0.936 0.536 0.670 0.444 0.743 0.017 1 0.171 0.001 0.398 0.678 0.500
UniParser
PA 0.456 0.949 0.835 0.948 0.814 0.949 0.171 0.685 0.474 0.516 0.634 0.775 0.331 0.989 0.680
FTA 0.093 0.223 0.319 0.574 0.303 0.371 0.255 0.281 0.011 0.292 0.229 0 0.176 0.500 0.260
Ourproposedmethod
GA 0.993 0.999 0.923 0.999 0.991 0.854 0.774 0.817 0.625 0.940 0.847 0.800 0.762 0.999 0.880
FGA 0.931 0.961 0.928 0.857 0.968 0.944 0.923 0.869 0.722 0.896 0.727 0.898 0.939 0.978 0.896
HELP
PA 0.969 0.996 0.908 0.690 0.975 0.990 0.756 0.577 0.622 0.946 0.939 0.934 0.644 0.669 0.830
FTA 0.754 0.918 0.792 0.630 0.842 0.908 0.625 0.565 0.686 0.825 0.696 0.786 0.638 0.878 0.753
Table2:AblationstudyofHELPcomponents.
Method GA FGA
HELP 0.878 0.931
w/oLLM 0.620(↓29.4%) 0.891(↓4.35)
w/oprev.&NN 0.581(↓33.76%) 0.855(↓8.16%)
w/oprev.&WC 0.535(↓39.05%) 0.817(↓12.28%)
w/oprev.&IRM 0.499(↓43.13%) 0.804(↓13.71%)
alertsthatcanbemanuallyignoredbydevelopers.Additionally,
Figure4:UMAPvisualizationof1000HealthApplogembed-
whileLILACisanofflineparserthatrequirestop-Kfrequenttokens
dingswith20neighbors.
for clusteringand retrievalof similar templatesduring parsing,
HELP achieves comparable performance while being an online
parserwherelogsmustbeprocessedinreal-timeandtheinforma-
theirlargetemplatecount(>100templates).Specifically,weincre-
tionavailableforparsingisincomplete.ThishighlightsHELP’s
mentallyremovefourcomponentsandcomparetheirGAandFGA
advantageinproductionenvironments,wherereal-timeparsingis
totheunmodifiedHELP.Thefourcomponentsweremoveare1)
criticalfordownstreamapplications.
HELPw/oLLM:HELPremovingthecontext-awareparsingmodule,
2)HELPw/oprev.&NN:HELPremovingthecontext-awareparsing
5.2 RQ2:Howdoeseachcomponentcontribute
moduleandreplacingthefine-tunedneuralnetworkembeddings
toHELP?
withtheunmodifiedOpenAIembeddings3)HELPw/oprev.&WC:
5.2.1 Ablations. Todeterminetherelativeimportanceofeachcom- HELPremovingallaformentionedpartsandtheembeddingofthe
ponentofHELPinitsoverallperformance,weconductaseriesof logwordcountdata,4)HELPw/oprev.&IRM:HELPremoving
ablationstudiesonsixoftheLoghub2.0datasets(BGL,Hadoop, allpreviouslymentionedcomponentsandtheiterativerebalancing
HealthApp,Linux,Mac,andThunderbird),whicharechosenfor module.Xuetal.
6 DISCUSSION
6.1 PracticalityofLILAC
HELPisdesignedtobeauniversalmodelforlogparsing.Although
HELPrequiresaninitialofflinepre-trainingsteponlabeledlog
data,afterpre-training,HELPcanbeappliedtonewservicesfor
onlineinferencewithoutanyadditionalsuperviseddataortraining.
WeprovidetheneuralnetworkmodelweightsforHELPtrained
onall14datasetsforotherorganizationsinterestedinutilizing
Figure5:LatencystatisticsofIudexlogwriteservice.
HELP’sfine-tunedembeddings,bypassingtheneedfortheirown
pre-training.AswehaveshownbysuccessfullydeployingHELP
intoIudex,HELP’suseofcosteffectivetext-embeddingsmakes
TheevaluationresultsareshowninTable2.Theresultsmake
real-timelogparsingpracticalforproductiondeployment.
theimportanceofeachdesignchoiceofHELPclear.Forinstance,
removingthecontext-awareparsingmodulesignificantlyimpacts
GAwhilehavingasmallereffectonFGA.Thisdifferencearises 6.2 ThreatstoValidity
becausethehierarchicalembeddingmoduletendstocreateoverly DataLeakage.BecauseLLMsaretrainedextensivelyonavast
specific groups, which has a greater impact on GA, a log-level corporaoftextdata,thereexistsapossibilityofdataleakagewhere
metric, compared to FGA, a template-level metric. The parsing theembeddingorlanguagemodelshavepreviouslyseentheopen
modulemitigatesthisissuebymergingtheseoverlyspecificgroups sourcelogdatasets. Thiscouldresult inthemodelmemorizing
byassigningthemtothesametemplate,improvingoverallaccuracy. ground-truthtemplatesduringinference.However,oursuperior
Ourhypothesesregardingtheimportanceofdomain-specificlog performanceagainstmethodssuchasLogPPTwhichsolelyuse
information,thelimitationsofsemanticmodels,andthedetrimental LLMs,ourablationstudieswhichremoveindividualcomponentsof
effectsoflogdriftinonlinealgorithmsarefurthervalidatedby HELP,andtherobustperformanceofHELPinprivateproduction
thenoticeableperformancedeclineswhenthefine-tunedneural dataindicatealowprobabilityofrotememorization.
networkembeddings,wordcount,anditerativerebalancingmodule Randomness.RandomnesscanaffecttheperformanceofHELP
areremoved. andotherbaselinemethods.Tocombatthis,HELPminimizesthe
randomnessofitscontext-awareparsingmodulebysettingthe
5.2.2 Visualization. Oneofthemaincontributionsofourpaperis
temperatureofthemodelto0,ensuringconsistentoutputsforthe
demonstratingthatembeddingmodelsprovideacost-effectiveand
sameinputtext.HELPalsoruneachexperimentthreetimesand
efficientalternativetosequentialLLMquerying.Tovalidatethe
averagestheresultstoreducevariance.
capabilityofembeddingmodelsinaccuratelycreatingloggroup-
ings,andtodemonstratetheefficacyoffine-tunedneuralnetwork
embeddingsforthistask,wevisualize1000embeddingsfromthe 7 CONCLUSION
HealthAppdatasetusingUnifoldManifoldApproximationandPro- Logparsingisthefoundationforloganalysistasks,wherebysemi-
jection(UMAP)[24],adimensionreductiontechnique.Theresults structuredlogdataistransformedintostructuredlogtemplates.
areshowninFigure4.TheleftpanelshowstheUMAPvisualization Inthispaper,weproposeanovelhierarchicalembeddings-based
ofnaiveOpenAIembeddings,whiletherightpaneldisplaysthe logparser,HELP.HELPisthefirstonlinesemantic-basedparserto
fine-tunedneuralnetworkembeddings.Althoughbothareable leverageLLMsforeffectiveandefficientlogparsing.HELPachieves
tocapturetemplategroupings,themoredenselygroupedclusters thisviaitshierarchicalembeddingsmodule,whichfine-tunesatext
andthegreaterseparationbetweenclustersofdifferenttemplates embeddingmodeltoclusterlogsbeforeparsing,reducinginference
from the fine-tuned embeddings highlights the effectiveness of costsbymultipleordersofmagnitude.Tocombatlogdriftwhich
fine-tuninginimprovingloggroupings. cancreatefalsepositivealertsandtoenabledeploymentinparallel,
HELPimplementsaniterativerebalancingmoduletoperiodically
5.3 RQ3:HowpracticalisHELPinaproduction updateexistingloggroupings.WeevaluateHELPextensivelyon14
environment? largescalepublicdatasets,whereHELPsignificantlyoutperformed
otheronlineparsersongroupingandparsingaccuracy.Finally,
HELPhasbeensuccessfullydeployedintoIudex’sproductionob-
HELPisdeployedintoIudex’sproductionobservabilityplatform,
servabilityplatformasanonlinealgorithm.Becausemultipleser-
demonstratingHELP’spracticalityinhigh-throughoutreal-world
vicesemitlogsconcurrently,HELPisimplementedasabatched
logparsing.
algorithm,wherethousandsoflogscanbeembeddedandgrouped
inparallel.InternaltestinghasfoundthatHELP’siterativerebal-
ancingmoduleallowsHELPtoachievelittletonoperformance REFERENCES
degredationasabatchedalgorithm,withanaveragedecreaseof [1] Anthropic.2024.Claude3.5Sonnet.https://www.anthropic.com/news/claude-3-
1-2%ingroupingaccuracywhileprovidingsignificantlylowerla- 5-sonnet.
[2] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,
tency.Figure5showstheend-to-endP95latencyfromlogemission PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda
tologstorageandgroupingoveraneighthourtime-frame.We Askell,etal.2020.LanguageModelsAreFew-ShotLearners.Advancesinneural
informationprocessingsystems33(2020),1877–1901.
achieveanaverageP95latencyofapproximately1.5seconds,mak-
[3] AnRanChen,Tse-HsunChen,andShaoweiWang.2022.Pathidea:Improving
ingHELPefficientandeffectiveforreal-timeloganalysistasks. InformationRetrieval-BasedBugLocalizationbyRe-ConstructingExecutionHELP:HierarchicalEmbeddings-basedLogParsing
PathsUsingLogs.IEEETransactionsonSoftwareEngineering48,8(2022),2905– [24] LelandMcInnes,JohnHealy,andJamesMelville.2020.UMAP:UniformManifold
2919. https://doi.org/10.1109/TSE.2021.3071473 ApproximationandProjectionforDimensionReduction. https://doi.org/10.
[4] EdwardChuah,Shyh-haoKuo,PaulHiew,William-ChandraTjhi,GaryLee, 48550/arXiv.1802.03426arXiv:1802.03426[cs,stat]
JohnHammond,MarekT.Michalewicz,TerenceHung,andJamesC.Browne. [25] WeibinMeng,YingLiu,YuhengHuang,ShenglinZhang,FedericoZaiter,Bingjin
2010. DiagnosingtheRoot-CausesofFailuresfromClusterLogFiles.In2010 Chen,andDanPei.2020. ASemantic-AwareRepresentationFrameworkfor
InternationalConferenceonHighPerformanceComputing.1–10. https://doi.org/ OnlineLogAnalysis.In202029thInternationalConferenceonComputerCommu-
10.1109/HIPC.2010.5713159 nicationsandNetworks(ICCCN).1–7. https://doi.org/10.1109/ICCCN49398.2020.
[5] HetongDai,HengLi,Che-ShaoChen,WeiyiShang,andTse-HsunChen.2022. 9209707
Logram:EfficientLogParsingUsingNn-GramDictionaries.IEEETransactions [26] WeibinMeng,YingLiu,YichenZhu,ShenglinZhang,DanPei,YuqingLiu,Yihao
onSoftwareEngineering48,3(2022),879–892. https://doi.org/10.1109/TSE.2020. Chen,RuizhiZhang,ShiminTao,PeiSun,etal.2019.Loganomaly:Unsupervised
3007554 DetectionofSequentialandQuantitativeAnomaliesinUnstructuredLogs..In
[6] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,Xu IJCAI,Vol.19.4739–4745.
Sun,JingjingXu,andZhifangSui.2022.ASurveyonIn-ContextLearning.arXiv [27] PriyankaMudgalandRitaWouhaybi.2023.AnAssessmentofChatGPTonLog
preprintarXiv:2301.00234(2022).arXiv:2301.00234 Data.InInternationalConferenceonAI-generatedContent.Springer,148–169.
[7] MinDuandFeifeiLi.2016.Spell:StreamingParsingofSystemEventLogs.In [28] MeiyappanNagappanandMladenAVouk.2010. AbstractingLogLinesto
2016IEEE16thInternationalConferenceonDataMining(ICDM).IEEE,859–864. LogEventTypesforMiningSoftwareSystemLogs.In20107thIEEEWorking
[8] MartinEster,Hans-PeterKriegel,JörgSander,XiaoweiXu,etal.1996.ADensity- ConferenceonMiningSoftwareRepositories(MSR2010).IEEE,114–117.
BasedAlgorithmforDiscoveringClustersinLargeSpatialDatabaseswithNoise. [29] ArvindNeelakantan,TaoXu,RaulPuri,AlecRadford,JesseMichaelHan,Jerry
InKdd,Vol.96.226–231. Tworek,QimingYuan,NikolasTezak,JongWookKim,ChrisHallacy,Johannes
[9] HosseinHamooni,BiplobDebnath,JianwuXu,HuiZhang,GuofeiJiang,and Heidecke,PranavShyam,BorisPower,TynaEloundouNekoul,GirishSastry,
AbdullahMueen.2016. Logmine:FastPatternRecognitionforLogAnalytics. GretchenKrueger,DavidSchnurr,FelipePetroskiSuch,KennyHsu,Madeleine
InProceedingsofthe25thACMInternationalonConferenceonInformationand Thompson,TabarakKhan,TokiSherbakov,JoanneJang,PeterWelinder,and
KnowledgeManagement.1573–1582. LilianWeng.2022. TextandCodeEmbeddingsbyContrastivePre-Training.
[10] PinjiaHe,JiemingZhu,ZibinZheng,andMichaelRLyu.2017. Drain:An arXiv:2201.10005[cs]
OnlineLogParsingApproachwithFixedDepthTree.In2017IEEEInternational [30] PaoloNotaro,SoroushHaeri,JorgeCardoso,andMichaelGerndt.2023.LogRule:
ConferenceonWebServices(ICWS).IEEE,33–40. EfficientStructuredLogMiningforRootCauseAnalysis.IEEETransactionson
[11] ShilinHe,PinjiaHe,ZhuangbinChen,TianyiYang,YuxinSu,andMichaelR. NetworkandServiceManagement20,4(2023),4231–4243. https://doi.org/10.
Lyu.2021. ASurveyonAutomatedLogAnalysisforReliabilityEngineering. 1109/TNSM.2023.3282270
AcmComputingSurveys54,6(July2021),130:1–130:37. https://doi.org/10.1145/ [31] OpenAI.[n.d.].Pricing.https://openai.com/api/pricing/.
3460345 [32] OpenAI.[n.d.].Tokenizer.
[12] JunjieHuang,ZhihanJiang,ZhuangbinChen,andMichaelRLyu.2024.ULog: [33] AntonioPecchia,MarcelloCinque,GabriellaCarrozza,andDomenicoCotroneo.
UnsupervisedLogParsingwithLargeLanguageModelsthroughLogContrastive 2015.IndustryPracticesandEventLogging:AssessmentofaCriticalSoftware
Units.arXivpreprintarXiv:2406.07174(2024).arXiv:2406.07174 DevelopmentProcess.In2015IEEE/ACM37thIEEEInternationalConferenceon
[13] AbiodunM.Ikotun,AbsalomE.Ezugwu,LaithAbualigah,BelalAbuhaija,and SoftwareEngineering,Vol.2.169–178. https://doi.org/10.1109/ICSE.2015.145
JiaHeming.2023.K-MeansClusteringAlgorithms:AComprehensiveReview, [34] StefanPetrescu,FlorisDenHengst,AlexandruUta,andJanSRellermeyer.2023.
VariantsAnalysis,andAdvancesintheEraofBigData.InformationSciences622 LogParsingEvaluationintheEraofModernSoftwareSystems.In2023IEEE
(April2023). https://doi.org/10.1016/j.ins.2022.11.139 34thInternationalSymposiumonSoftwareReliabilityEngineering(ISSRE).IEEE,
[14] ZhihanJiang,JinyangLiu,ZhuangbinChen,YichenLi,JunjieHuang,Yintong 379–390.
Huo,PinjiaHe,JiazhenGu,andMichaelR.Lyu.2024.LILAC:LogParsingUsing [35] Ponemon.2016.CostofDataCenterOutages.(Jan.2016).
LLMswithAdaptiveParsingCache.Proc.ACMSoftw.Eng.1,FSE(July2024), [36] JimRapoza.2014.PreventingVirtualApplicationDowntime.AberdeenGroup
7:137–7:160. https://doi.org/10.1145/3643733 (2014).
[15] ZhihanJiang,JinyangLiu,JunjieHuang,YichenLi,YintongHuo,JiazhenGu, [37] DaanSchipper,MaurícioAniche,andArievanDeursen.2019. TracingBack
ZhuangbinChen,JiemingZhu,andMichaelRLyu.2023.ALarge-scaleBench- LogDatatoItsLogStatement:FromResearchtoPractice.In2019IEEE/ACM
markforLogParsing.arXivpreprintarXiv:2308.10828(2023).arXiv:2308.10828 16thInternationalConferenceonMiningSoftwareRepositories(MSR).545–549.
[16] ZhenMingJiang,AhmedEHassan,ParminderFlora,andGilbertHamann.2008. https://doi.org/10.1109/MSR.2019.00081
AbstractingExecutionLogstoExecutionEventsforEnterpriseApplications [38] KeiichiShima.2016. LengthMatters:ClusteringSystemLogMessagesUsing
(ShortPaper).In2008theEighthInternationalConferenceonQualitySoftware. LengthofWords.arXivpreprintarXiv:1611.03213(2016).arXiv:1611.03213
IEEE,181–186. [39] R.Vaarandi.2003.ADataClusteringAlgorithmforMiningPatternsfromEvent
[17] ZanisAliKhan,DonghwanShin,DomenicoBianculli,andLionelBriand.2022. Logs.InProceedingsofthe3rdIEEEWorkshoponIPOperations&Management
GuidelinesforAssessingtheAccuracyofLogMessageTemplateIdentification (IPOM2003)(IEEECat.No.03EX764).119–126. https://doi.org/10.1109/IPOM.
Techniques.InProceedingsofthe44thInternationalConferenceonSoftwareEngi- 2003.1251233
neering(ICSE’22).AssociationforComputingMachinery,NewYork,NY,USA, [40] RistoVaarandiandMaunoPihelgas.2015. Logcluster-aDataClusteringand
1095–1106. https://doi.org/10.1145/3510003.3510101 PatternMiningAlgorithmforEventLogs.In201511thInternationalConference
[18] Van-HoangLeandHongyuZhang.2022.Log-BasedAnomalyDetectionwith onNetworkandServiceManagement(CNSM).IEEE,1–7.
DeepLearning:HowFarAreWe?.InProceedingsofthe44thInternationalConfer- [41] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
enceonSoftwareEngineering(ICSE’22).AssociationforComputingMachinery, AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. AttentionIsAll
NewYork,NY,USA,1356–1367. https://doi.org/10.1145/3510003.3510155 YouNeed.Advancesinneuralinformationprocessingsystems30(2017).
[19] Van-HoangLeandHongyuZhang.2023.LogParsing:HowFarCanChatGPT [42] XuhengWang,XuZhang,LiqunLi,ShilinHe,HongyuZhang,YudongLiu,
Go?.In202338thIEEE/ACMInternationalConferenceonAutomatedSoftware LinglingZheng,YuKang,QingweiLin,YingnongDang,etal.2022.SPINE:A
Engineering(ASE).1699–1704. https://doi.org/10.1109/ASE56229.2023.00206 ScalableLogParserwithFeedbackGuidance.InProceedingsofthe30thACMJoint
[20] Van-HoangLeandHongyuZhang.2023.LogParsingwithPrompt-BasedFew- EuropeanSoftwareEngineeringConferenceandSymposiumontheFoundationsof
ShotLearning.InProceedingsofthe45thInternationalConferenceonSoftware SoftwareEngineering.1198–1208.
Engineering(ICSE’23).IEEEPress,Melbourne,Victoria,Australia,2438–2449. [43] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,
https://doi.org/10.1109/ICSE48619.2023.00204 QuocVLe,DennyZhou,etal.2022.Chain-of-ThoughtPromptingElicitsRea-
[21] PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,and soninginLargeLanguageModels. Advancesinneuralinformationprocessing
GrahamNeubig.2023.Pre-Train,Prompt,andPredict:ASystematicSurveyof systems35(2022),24824–24837.
PromptingMethodsinNaturalLanguageProcessing. Comput.Surveys55,9 [44] YiXiao,Van-HoangLe,andHongyuZhang.2024. Stronger,Cheaperand
(2023),1–35. Demonstration-FreeLogParsingwithLLMs. https://doi.org/10.48550/arXiv.
[22] YudongLiu,XuZhang,ShilinHe,HongyuZhang,LiqunLi,YuKang,YongXu, 2406.06156arXiv:2406.06156[cs]
MinghuaMa,QingweiLin,YingnongDang,SaravanRajmohan,andDongmei [45] JunjielongXu,RuichunYang,YintongHuo,ChengyuZhang,andPinjiaHe.
Zhang.2022. UniParser:AUnifiedLogParserforHeterogeneousLogData. 2024. DivLog:LogParsingwithPromptEnhancedIn-ContextLearning.In
InProceedingsoftheACMWebConference2022(WWW’22).Associationfor ProceedingsoftheIEEE/ACM46thInternationalConferenceonSoftwareEngineering
ComputingMachinery,NewYork,NY,USA,1893–1901. https://doi.org/10.1145/ (ICSE’24).AssociationforComputingMachinery,NewYork,NY,USA,1–12.
3485447.3511993 https://doi.org/10.1145/3597503.3639155
[23] YuAMalkovandDmitryAYashunin.2018.EfficientandRobustApproximate [46] WeiXu,LingHuang,ArmandoFox,DavidPatterson,andMichaelJordan.2009.
NearestNeighborSearchUsingHierarchicalNavigableSmallWorldGraphs.IEEE LargescaleSystemProblemDetectionbyMiningConsoleLogs.InProceedingsof
transactionsonpatternanalysisandmachineintelligence42,4(2018),824–836. SOSP,Vol.9.Citeseer,1–17.Xuetal.
[47] LinYang,JunjieChen,ZanWang,WeijingWang,JiajunJiang,XuyuanDong, https://doi.org/10.1145/3338906.3338931
andWenbinZhang.2021.Semi-SupervisedLog-BasedAnomalyDetectionvia [49] ZihaoZhao,EricWallace,ShiFeng,DanKlein,andSameerSingh.2021.Calibrate
ProbabilisticLabelEstimation.In2021IEEE/ACM43rdInternationalConference BeforeUse:ImprovingFew-shotPerformanceofLanguageModels.InProceedings
onSoftwareEngineering(ICSE).1448–1460. https://doi.org/10.1109/ICSE43902. ofthe38thInternationalConferenceonMachineLearning.PMLR,12697–12706.
2021.00130 [50] JiemingZhu,ShilinHe,PinjiaHe,JinyangLiu,andMichaelR.Lyu.2023.Loghub:
[48] XuZhang,YongXu,QingweiLin,BoQiao,HongyuZhang,YingnongDang, ALargeCollectionofSystemLogDatasetsforAI-drivenLogAnalytics.In2023
ChunyuXie,XinshengYang,QianCheng,ZeLi,JunjieChen,XiaotingHe, IEEE34thInternationalSymposiumonSoftwareReliabilityEngineering(ISSRE).
RandolphYao,Jian-GuangLou,MuraliChintalapati,FuraoShen,andDongmei 355–366. https://doi.org/10.1109/ISSRE59848.2023.00071
Zhang.2019.RobustLog-BasedAnomalyDetectiononUnstableLogData.In [51] JiemingZhu,ShilinHe,JinyangLiu,PinjiaHe,QiXie,ZibinZheng,andMichaelR
Proceedingsofthe201927thACMJointMeetingonEuropeanSoftwareEngineering Lyu.2019.ToolsandBenchmarksforAutomatedLogParsing.In2019IEEE/ACM
ConferenceandSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE 41stInternationalConferenceonSoftwareEngineering:SoftwareEngineeringin
2019).AssociationforComputingMachinery,NewYork,NY,USA,807–817. Practice(ICSE-SEIP).IEEE,121–130.