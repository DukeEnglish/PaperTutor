Towards Flexible Visual Relationship Segmentation
FangruiZhu1 JianweiYang2 HuaizuJiang1
1NortheasternUniversity 2MicrosoftResearch
https://neu-vi.github.io/FleVRS
Standard HOI Segmenta,on Promptable HOI Segmenta,on
<person, hold, horse>
<person, straddle, horse>
<person, walk, horse>
<person, ride, horse>
Prompt: <?, straddle, ?> Prompt: <?, hold, horse> Prompt: <person, ride, horse>
<person, horse> <person, horse> <person, horse>
Promptable Panop,c SGG
Standard Panop,c SGG
<bench, in front of , tree>
<bench, on , pavement>
<person, sit on , bench>
<person, look at , person> Prompt: <?, look at, ?> Prompt: <?, sit on, bench> Prompt: <bench,?, pavement>
<person, person> <person, bench> <bench, on , pavement>
<person, bench> <bench, on , pavement>
Figure1: FleVRSisasinglemodeltrainedtosupportstandard,promptableandopen-vocabulary
fine-grained visual relationship segmentation (<subject mask, relationship categories, object
mask>). Itcantakeimagesonlyorimageswithstructuredpromptsasinputs,andsegmentallexisting
relationshipsortheonessubjecttothetextprompts.
Abstract
Visual relationship understanding has been studied separately in human-object
interaction(HOI)detection,scenegraphgeneration(SGG),andreferringrelation-
ships(RR)tasks. Giventhecomplexityandinterconnectednessofthesetasks,itis
crucialtohaveaflexibleframeworkthatcaneffectivelyaddressthesetasksina
cohesivemanner. Inthiswork,weproposeFleVRS,asinglemodelthatseamlessly
integratestheabovethreeaspectsinstandardandpromptablevisualrelationship
segmentation,andfurtherpossessesthecapabilityforopen-vocabularysegmenta-
tiontoadapttonovelscenarios. FleVRSleveragesthesynergybetweentextand
imagemodalities,togroundvarioustypesofrelationshipsfromimagesanduse
textualfeaturesfromvision-languagemodelstovisualconceptualunderstanding.
Empiricalvalidationacrossvariousdatasetsdemonstratesthatourframeworkout-
performsexistingmodelsinstandard,promptable,andopen-vocabularytasks,e.g.,
+1.9mAP onHICO-DET,+11.4AcconVRD,+4.7mAP onunseenHICO-DET.
OurFleVRSrepresentsasignificantsteptowardsamoreintuitive,comprehensive,
andscalableunderstandingofvisualrelationships.
1 Introduction
An image is not merely a collection of objects. Understanding the visual relationships between
differententitiesatpixel-levelthroughsegmentationisafundamentaltaskincomputervision,which
hasbroadapplicationsinautonomousdriving[28,58],behavioranalysis[65,67],navigation[10,15,
22,27],etc. Furthermore,segmentingrelationalobjectsextendsbeyondmeredetection,playinga
Preprint.Underreview.
4202
guA
51
]VC.sc[
1v50380.8042:viXra
…Standard
Method Promptable Open-vocabulary One/Two-stageModel
HOI SGG
RLIPv2[92] ✓ ✓ ✗ ✓ Two
UniVRD[101] ✓ ✓ ✗ ✗ Two
SSAS[38] ✗ ✗ ✓ ✗ One
GEN-VLKT[47] ✓ ✗ ✗ ✓ One
FleVRS(Ours) ✓ ✓ ✓ ✓ One
Table1: Comparisonswithpreviousrepresentativemethodsinthreeaspectsofmodelcapabili-
ties. Tothebestofourknowledge,ourFleVRSisthefirstone-stagemodelcapableofperforming
standard,promptable,andopen-vocabularyvisualrelationshipsegmentationallatonce.
crucialroleinimprovingvisualunderstandingandprovidingamorecomprehensiveabstractionon
thevisualcontentsandinteractionsamongthem.
Ideally,avisualrelationshipsegmentation(VRS)modelshoulddemonstrateflexibilityacrossthree
key dimensions. 1) Capability of segmenting various types of relationships, including both
human-centricandgenericones. Theserelationshipsaredefinedastripletsintheformof<subject,
predicate, object>. Human-object interaction (HOI) detection [4, 18], which we adapt into
HOIsegmentationinourwork,exemplifiesthiscapability,suchas<person, ride, horse>in
Fig.1. Panopticscenegraphgeneration(SGG)[55,81,85],capturesgenericspatialorsemantic
relationshipsamongpairsofobjectsinanimage,e.g.,bench on pavementinFig.1. Aunified
modelthatcanhandlethesetasksconcurrentlyisessential, asiteliminatestheneedforseparate
designsandmodificationsforeachspecifictask. 2)Groundingofrelationalsubject-objectpairs
withdifferentprompts. Givenvarioustextualprompts,themodelshouldoutputthedesiredentities
and relationships, facilitating a more natural and intuitive user interface. For instance, it should
be able to detect just the person in an image or all possible interactions between a person and
a horse, as illustrated in Fig. 1. 3) Open-vocabulary recognition of visual relationships. In
realisticopen-worldapplications,themodelshouldgeneralizetonewscenarioswithoutrequiring
annotations for new concepts not seen during training. This capability includes detecting novel
objects,relationships,andtheircombinations.
Existingmodelsinvisualrelationshipsegmentation(VRS)havetargetedaspectsofthedesiredcapabil-
itiesbutfallshortofprovidingacomprehensivesolution,asdetailedinTab.1. Modelshavetypically
focusedontaskslikehuman-objectinteraction(HOI)detection[23,35,47,57,72,95,97,108]and
panopticSGG[50,71,81,85,93,104]. Althoughmodelssuchas[92,101]haveattemptedtounify
VRSunderasingleframework,theyneedadditionalpretrainingonHOIdatasets(Tab.1)andlack
featuressuchaspromptablesegmentation,whichallowsfordynamicentityandrelationshipgenera-
tionbasedontextualprompts,aswellascapabilitiesforopen-vocabularypromptablesegmentation.
Effortstodetectinstancesreferredtobytextualpromptshavebeenmade[21,38,75,107],butthese
modelsfailtocapturealldesiredentitiesorrelationshipscomprehensivelyandstrugglewithclassi-
fyingmulti-labelinteractionsbetweenthepairs,limitingtheireffectivenessincomplexscenarios.
Althoughrecentvision-languagegroundingmodelslike[29,52,83]andmultimodallargelanguage
modelssuchas[2,5,76,78,88]exhibitenhancedcapabilitiesingroundinginstancesspecifiedby
free-form text and show strong generalization over novel concepts, they still do not generate the
requiredpairsintheformatofsegmentationmasks. Furthermore,thesemodelsrequiresignificant
computationalresourcesandadditionalvisionmodelsforpreciselocalizations. Foropen-vocabulary
VRS, existing works [47, 91, 92] leverage textual embeddings to transfer knowledge. However,
models[91,92]fallshortingroundingdiverseprompts,while[47]isexclusivelydesignedforHOI
detection,notgenericVRS.
Toaddressthelimitationsinexistingmodels,weintroduceFleVRS,aflexibleone-stageframework
capableofperformingstandard,promptable,andopen-vocabularyvisualrelationshipsegmentation
simultaneously. Our approach integrates human-centric (HOI segmentation) and generic VRS
(PanopticSGG)byadoptingSAM[36]tounifydifferenttypesofannotationsintosegmentation
masksandusingaquery-basedTransformerarchitecturethatoutputstripletsintheformat<subject,
predicate, object>. Themodelenhancesitsinteractivecapabilitiesbyacceptingtextualprompts
as inputs. These prompts are converted into textual queries that assist the decoder in accurately
identifyingandlocalizingobjectswithintherelationships. Additionally,weunifythelabelsfrom
differentdatasetsintoasharedtextualspace,transformingclassificationintoaprocessofmatching
with a set of textual features. Leveraging textual features from the CLIP model [64], we enable
the effective matching of visual features with textual knowledge of novel concepts. This design
2Figure2: ExamplesofconvertingHOIdetectionboxestomasks. Wefilteroutlow-qualitymasks
duringtrainingbycomputingIoUbetweenthemaskandbox.
inherentlysupportsopen-vocabularyandpromptablerelationshipsegmentationwithoutpre-defining
thenumberofobjectorpredicatecategories,facilitatingdynamicandextensiveadaptability.
OurFleVRSproposesaunifiedframeworkthatintegratesstandard,promptable,andopen-vocabulary
VRS tasks into a single system, as detailed in Tab. 1, providing greater flexibility compared to
existing methods. It employs a mask-based approach to effectively manage various VRS tasks,
enablingadaptationtodifferenttypesofannotations,includingHOIdetectionandpanopticSGG.
Our architecture incorporates dynamic prompt handling, which supports both prompt-based and
open-vocabularysettings,allowingourmodeltocombinepromptablequerieswithopen-vocabulary
capabilitiestogroundnovelrelationalobjects.
WeevaluateourFleVRSonstandard,promptable,andopen-vocabularyVRStasks,i.e.,HOIsegmen-
tation[4,18]andpanopticSGG[85]. Crucially,wedemonstratecompetitiveperformancefromthree
perspectives–standard(40.5vs. 39.1mAP onHICO-DET[4]),promptable(56.8vs. 33.5sIoUon
VRD[55]),andopen-vocabulary(31.7vs. 25.6mAP for“unseenobject”onHICO-DET[4])visual
relationshipsegmentation.
Insummary,ourmaincontributionsareasfollows: 1)Weintroduceaflexibleone-stageframework
capableofsegmentingbothhuman-centricandgenericvisualrelationshipsacrossvariousdatasets.
2)Wepresentapromptablevisualrelationshiplearningframeworkthateffectivelyutilizesdiverse
textualpromptstogroundrelationships. 3)Wedemonstratecompetitiveperformanceinbothstandard
close-setandopen-vocabularyscenarios,showcasingthemodel’sstronggeneralizationcapabilities.
2 RelatedWork
Visual Relationship Detection (VRD) is split into two lines of works, including human-object
interaction (HOI) detection [4, 18] and panoptic scene graph generation (SGG) [37, 85]. They
aredefinedas detectingtripletsintheform of<subject, predicate, object> triplet, where
subjectorobjectincludesobjectboxandcategory. HOIdetectionaimstodetecthuman-centric
visualrelationships,whilePSGfocusesongenericobjectpairs’relationships. Previousworks[1,
13,16,32,35,41,41,47,57,81,89,95,96,97,100,104,105,106]usuallytrainspecialistmodels
on a single data source and tackle them separately. Departing from this traditional bifurcation,
UniVRD[101]initiatedthedevelopmentofaunifiedmodelforVRD,withsubsequenteffortslike[91,
92]advancingrelationalunderstandingthroughlarge-scalelanguage-imagepre-training. Unlikethe
two-stageapproachof[92],whichperformsobjectdetectionbeforedecodingvisualrelationships,
ourmethodemploysaone-stagedesignthatdecodesobjectsandtheirrelationshipssimultaneously.
Crucially,ourmodelextendsbeyondstandardVRDcapabilitiestosupportpromptableandopen-
vocabularyvisualrelationshipsegmentation,enhancingdetailedscenecomprehension.
Referringrelationshipandvisualgrounding. Themostrelevantworktooursisreferringvisual
relationshipintroducedin[38],wherethemodeldetectsthesubjectandobjectdepictingthestructured
relationship<subject, predicate, object>. One-stage[38,75],two-stage[63,107]andthree-
stage[21]methodsareproposedtolocalizethetwoentities’boxesiterativelybasedonthegiven
structuredprompt<subject, predicate, object>. Unlikethesemethods,ourapproachallows
formoreflexibletextualpromptswithoutrequiringthecompletespecificationofthetriplet. Asshown
inFig.1,ourmodelcanhandlequeriesthatincludeasingleitem(e.g.,predicate)oracombination
oftwo(e.g.,predicateandobject). Additionally,ourmethodiscapableofperformingstandard
andopen-vocabularyVRS.Visualgroundingrepresentsanotherrelatedarea,wheremodelsoutput
boundingboxes[7,8,20,29,40,52,83]orobjectmasks[9,17,44,45,51,56,79,87,99]inresponse
totextualinputs. Thisprocessrequiresreasoningovertheentitiesmentionedinthetexttoidentify
thecorrespondingobjectsinthevisualspace. However,ourtaskfundamentallydiffersfromthis.
3Pixel Decoder
Image ×
Encoder
Input Image
< , , >
𝐐𝐯 𝐐𝐯 Output Triplet
Latent queries Textual queries
Subjectmask Object mask
<subject, object> ET ne cx otu da el r 𝐐𝐭 Relationship 𝐐𝐭 Subjectclass Object class Decoder
PromptableVRS Predicateclass
Figure3: OverviewofFleVRS.InstandardVRS,withouttextualqueries,thelatentqueriesperform
self- and cross-attention within the relationship decoder to output a triplet for each query. For
promptableVRS,thedecoderadditionallyincorporatestextualqueriesQt,concatenatedwithQv.
This setup similarly predicts triplets, each based on Qv outputs aligned with features from the
optionaltextualpromptQt.
InourFleVRS,thepromptableVRStaskgoesbeyondmereidentification; itinvolvesoutputting
segmentationmasksforbothsubjectandobjectpairsalongwithcategorizingtheirrelationships. This
capabilityisessentialforunderstandingandinterpretingcomplexrelationaldynamics.
VisionandlanguagemodelsRecentadvancementsinlarge-scalepre-trainedvision-languagemod-
els(VLM)[39,64,68,69,82,94]andmultimodallargelanguagemodels(MLLM)[2,5,76,78,88]
havedemonstratedimpressiveperformanceandgeneralizationcapabilitiesacrossavarietyofvision
andmultimodaltasks[36,109,110]. However,thesemodelsprimarilyfocusonentity-levelgeneral-
ization,withopen-vocabularyVRSreceivinglessattention. Whilerecenteffortsinzero-shotHOI
detection[47,61,80,92]oftenutilizeCLIP[64]forcategoryclassification,theiropen-vocabulary
capabilitieslacktheflexibilityneededforprompt-driveninput. AlthoughcurrentVLMsandMLLMs
areadeptatgroundingnovelconceptsfromtext,theyrequiresignificantcomputationalresourcesand
additionalvisualmodels,andcannotdirectlygeneratecomprehensivesegmentationmasksforsubject
andobjectpairs. Incontrast,ourFleVRSprovidesalightweightsolutionthateffectivelysupports
varioustypesofopen-vocabularyVRS,enablingcategoryclassificationandtheintegrationofnovel
conceptsfromprompts.
3 Method
3.1 Overview
Standard VRS. Given an image I, the goal of standard visual relationship segmentation (VRS)
istodetectallthevisualrelationshipsofinterest,eitherhuman-centric(i.e.,HOIdetection)orthe
genericones(SGG),intermsoftripletsintheformof<subject, predicate, object>(masks
and object categories of subject and object, and the predicate category). The subject is
alwayshumaninHOIdetection,whereasitcanbeanytypeofobjectinSGG(mayormaynotbe
human). Weconsiderthepanopticsetting[85]ofSGG,whereamodelneedstogenerateamore
comprehensivescenegraphrepresentationbasedonpanopticsegmentationratherthanrigidbounding
boxes,providingaclearandprecisegroundingofobjects. Toproducefine-grainedmasks,weconvert
existing bounding box annotations from HOI detection datasets [4, 18] into segmentation masks
usingthefoundationmodelSAM[36],asillustratedinFig.2. Weemployafilteringapproachbased
onIntersectionoverUnion(IoU)tofilteroutinaccuratemasks. DetailsareinAppendix.
PromptableVRS.OurFleVRSoptionallyacceptstextualpromptsasinputs,enablinguserstospecify
visualrelationshipsforpromptableVRS.Itaccommodatesthreetypesofstructuredtextprompts: a
singleelement(e.g.,<?, predicate, ?>),anytwoelements(e.g.,<subject, predicate, ?>,
<subject, ?, object>),orallthreeelements. Consequently,themodeloutputsonlythetriplets
thatmatchthespecifiedelementsintheprompt,asdepictedintherightcolumnofFig.1. Without
textualprompts,itfunctionsasastandardVRSmodel,exhaustivelygeneratingallpossibletriplets,
illustratedintheleftpartofFig.1.
4
…
…
…
…Open-vocabularyVRS.Inpractice,it’sessentialforaVRSmodeltoadapttonewconcepts,including
newcategoriesofentities(i.e.,subjectandobject),predicates,andtheirvariouscombinations.
Expandingtheseconceptvocabulariestoencompassawiderrangeisparticularlychallengingdueto
thevastpotentialcombinationsandthelong-taildistributionofthesecategories. Thus,ourgoalis
toequipthemodeltooperateinanopen-vocabularysetting,whereitcaneffectivelyhandlethese
diversities. Ititimportanttonotethattheabovethreecapabilitiesarecomplementary;forexample,
thetextpromptsinpromptableVRScanincludenovelobjectorpredicatecategories.
Tothisend,weproposeintegratingtheabovethreeaspectsintoasingleunifiedframework. Since
thesesettingsarecomplementary,ageneral-purposemodelshouldbecapableofperformingvarious
combinationsofthesethreefunctions. Additionally,theirinherentsimilaritiesmakeitmoreintuitive
toconsolidatethemwithinaflexible,unifiedapproach.
3.2 ModelArchitecture
InspiredbythesuccessofTransformer-basedsegmentationmodels[6,109],wedesignadual-query
systemforourVRSmodel,illustratedinFig.3. Latentqueries,asetoflearnedembeddings,generate
triplets(whichmaybeempty)toformulateoutputmasksandrelationshipcategories. Forpromptable
VRS,textualqueriesderivedfrominputpromptsareincorporated. Weemployanimageencoder
andapixeldecodertoextractvisualfeatures, coupledwitharelationshipdecoderthatprocesses
<subject, object>pairsandtheirinterrelations. Foropen-vocabularyVRS,ourapproachshifts
fromtraditionalclassificationtoamatchingstrategythatalignsvisualandtextualfeaturesforboth
objectandpredicatecategories,enhancingthemodel’sadaptabilitytonewconcepts. Eachcomponent
ofthisarchitectureiselaboratedfurtherbelow.
ImageEncoder. Specifically,giventheimageI∈RH×W×3,itisfirstfedintotheimageencoder
(cid:110) (cid:111)
Enc I toobtainmulti-scalelow-resolutionfeaturesF= F s ∈RCF×H s×W s ,wherethestrideof
thefeaturemaps∈{4,8,16,32},andC isthenumberofchannels.
F
Pixel Decoder. A Transformer-based pixel decoder Dec is used to upsample F and gradually
p
generatehigh-resolutionper-pixelembeddingsP. PisthenpassedtotherelationshipdecoderDec
R
tocomputecross-attentionwithqueryfeatures.
TextualEncoder. WhenatextpromptisprovidedforpromptableVRS,weusethetextualencoder
Enc TtoencodeitintoasetoftextualqueriesQt ∈RNt×Cq,whereN tisthenumberoftokensin
thetextualqueries,andC denotesthechannelnumberofqueryfeatures. Inpractice,weusethe
q
textualencoderfromCLIP[64]asEnc . Theformatofthetextpromptcanbeasingleitem(e.g.
T
“<p>predicate</p>”),twoofthem(“<s>subject</s><p>predicate</p>”),orallthreeofthem,
where“predicate”and“subject”denotecategorynamesofpredicateandsubject,respectively.
“<s>”,“<o>”,“<p>”areusedasseparatetokensbetweensubject,predicateandobjectin
thetextprompt. Wecouldusenaturallanguageasthetextualpromptinsteadofusingastructured
format. However,collectingthetextualVRDdataisnottrivial,andweleaveitasanextensionofour
modelinfuturework.
RelationshipDecoder. TherelationshipdecoderDec ,basedonaTransformerdecoderdesign,
R
processespixeldecoderoutputsPandlatentqueriesQv togenerateallpossibletripletsforstandard
VRS.Inside,maskedattention[6]utilizesmasksfromearlierlayersforforegroundinformation. Each
Qv outputfeedsintofiveparallelheads: twomaskheadsforsubjectandobjectmasks(M ,M ),
s o
twoclassheadsfortheircategories(C ,C ),andanotherclassheadforrelationshipprediction(C )
s o p
Duringtraining,Hungarianmatchingalignspredictedtripletswithgroundtruth. ForstandardVRS
inference, tripletsaboveaconfidencethresholdareconsideredfinalpredictions. Forpromptable
VRS,Enc transformstextpromptsintotextualqueriesQtthatareconcatenatedwithQvandinput
T
intoDec . Thisprocess,whichusesself-andcross-attentionmechanisms,generates<subject,
R
predicate, object>triplets,similartostandardVRS.Anadditionalmatchinglossduringtraining
ensuresthemodelpredictstripletsasspecifiedbythetextprompt. Duringinference,wecalculate
similarityscoresbetweenthetextualqueryfeature(lasttoken’sfeatureofQt)andthelatentquery
outputs. We then select entities and relationships specified in the textual prompt from the top k
tripletsforthefinaloutputs.
Matching with textual features. To enable open-vocabulary VRS, our FleVRS uses the CLIP
textualencoder[64]tomatchvisualfeatureswithcandidatetextualfeaturesforobjectandpredicate
5categories. Weconvertthesecategoriesintotextualfeaturesusingprompttemplates,suchas“Aphoto
of[predicate-ing]”forHOIsegmentationand“Aphotoofsomething[predicate-ing](something)”
forpanopticSGG.1Themodelcomputesmatchingscoresbetweenpredictedclassembeddingsand
thesetextualfeatures, allowingclassificationbeyondthefixedvocabularyofthetrainingsetand
facilitatingopen-vocabularyVRS.Textualpromptsaresimilarlyencoded,andtheirfeaturesareused
tocalculatesimilarityscoresforpromptableVRSinference.
3.3 Lossfunctions
WeuseHungarianmatchingduringtrainingtofindthematchedtripletswithgroundtruthones. For
standard VRS, we compute focal losses Ls, Lo and dice losses Ls, Lo on subject and object
b b d d
maskpredictions,cross-entropylossesLs,Lo,Lponsubject,object,andpredicatecategory
c c c
classifications,whichcanbewrittenas
(cid:88) (cid:88) (cid:88)
L=λ Li +λ Lj + λkLk, (1)
b b d d c c
i∈{s,o} j∈{s,o} k∈{s,o,p}
whereλ , λ , andλ arehyper-parametersforadjustingtheweightsofeachloss. λs, λo, λp are
b d c c c c
differentclassificationlossweightsforsubject,object,andpredicate. ForpromptableVRS,
weadoptanadditionalmatchinglossL betweenthematchedtripletclassembeddingandthetextual
g
queryfeature(thelasttokenfeatureofQt),whichisintheformofcross-entropyloss. Thefinal
traininglossiswrittenas
(cid:88) (cid:88) (cid:88)
L=λ Li +λ Li + λkLk+λ L , (2)
b b d d c c g g
i∈{s,o} j∈{s,o} k∈{s,o,p}
whereλ controlstheweightofL . L dependsonthetextprompt. Forexample,given<subject,
g g c
predicate>, there will not have Ls and Lp terms in Eq. (2), with subject and predicate
c c
categoriesbeinggiven. Seetheappendixfortheconcretevaluesoflossweights.
4 Experiments
4.1 ExperimentalSettings
DatasetsForHOIsegmentation,weutilizetwopublicbenchmarks:HICO-DET[4]andV-COCO[18].
To fit Our FleVRS, we use SAM [36] to transform box annotations into masks and apply Non-
Maximum Suppression (NMS) to remove overlapping masks with an IoU threshold greater than
0.1. Weomitno_interactionannotationsfromHICO-DETduetoincompleteannotation,leaving
44,329 images (35,801 training, 8,528 testing) with 520 HOI classes from 80 objects and 116
actions.2 V-COCO is built from COCO [49], comprising 10,396 images (5,400 training, 4,964
testing),featuring80objectsand29actions,andincludes263HOIclasses. Bothdatasetsalignwith
COCO’sobjectcategories. ForpanopticSGG,weusethePSGdataset[85],sourcedfromCOCOand
VG[37]intersections,containing48,749images(46,572training,2,177testing)with133objects
and56predicates.
DataStructureforopen-vocabularyHOIsegmentationFollowingpriorstudies[3,23],weevaluate
HICO-DETunderthreescenarios:(1)UnseenComposition(UC),wheresomeHOIclassesareabsent
despiteallobjectandverbcategoriesbeingpresent;(2)UnseenObject(UO),wherecertainobject
classesandtheircorrespondingHOItripletsareexcludedfromtraining;and(3)UnseenVerb(UV),
where specific verb classes and their associated triplets are similarly omitted. In UC, the Rare
First (RF-UC) approach targets tail HOI classes, while Non-rare First (NF-UC) focuses on head
categories. Originally,UCincluded120/480/600categoriesforunseen/seen/fullsets,whichreduces
to115/405/520afterremovingno_interactionannotations. ForUO,weselect12unseenobjects
from80,resultingin88/432unseen/seenHOIcategories.
EvaluationMetricForstandardHOIsegmentation,weconvertthepredictedmaskstobounding
boxes to compare with current methods, and follow the setting in [4] to use the mean Average
Precision(mAP)forevaluation. Wealsoturntheoutputsofothermethodsintomasksandreport
1Omit“something”forspatialrelationships.
2interchangeablewith“verb”,“predicate”.
6(a) Text prompt: <?, object, predicate> (b) Text prompt: <subject, predicate, ?>
<?, flip, skateboard> <?, wash, bus> <person, hold, ?> <person, pull, ?>
<?, ride, boat> <?, hold, baseball glove> <person, hold, ?> <person, straddle, ?>
(c) Text prompt: <subject, ?, object>
<person, ?, motorcycle> <person, ?, laptop> <person, ?, kite> <person, ?, skis>
[ride, sit on] [hold, read] [hold, carry] [stand on, wear]
Figure4: QualitativeresultsofpromptableVRSonHICO-DET[4]testset. Weshowvisual-
izationsofsubjectandobjectmasksandrelationshipcategoryoutputs,giventhreetypesoftext
prompts. In(c),weshowthepredictedpredicatesinboldcharacters. Unseenobjectsandpredicates
aredenotedinredcharacters.
maskmAP forthoroughcomparison. AnHOItripletpredictionisatruepositiveif(1)bothpredicted
humanandobjectboundingboxes/maskshaveIoUlargerthan0.5w.r.t. GTboxes/masks;(2)Both
thepredictedobjectandverbcategoriesarecorrect. ForHICO-DET,weevaluatethethreedifferent
categorysets: all520HOIcategories(Full),112HOIcategories(lessthan10traininginstances)
(Rare),andtheother408HOIcategories(Non-Rare). ForVCOCO,wereporttherolemAPsintwo
scenarios: (1)S1: 29actionsincluding4bodymotions;(2)S2: 25actionswithouttheno-objectHOI
categories. ForstandardpanopticSGG,following[85],weuseR@K andmR@K metrics,which
calculatethetripletrecallandmeanrecallforeverypredicatecategory,giventhetopKtripletsfrom
themodel. Asuccessfulrecallrequiresbothsubjectandobjecttohavemask-basedIoUlargerthan
0.5comparedtotheirGTmasks,withthecorrectpredicateclassificationinthetriplet.
ImplementationDetailsFollowing[6,109],weuse100latentqueriesand9decoderlayersinthe
relationshipdecoder. WeadoptFocal-T/L[84]fortheImageEncoderandDaViT-B/Lforthepixel
decoder. WeusethetextualencoderfromCLIPtoencodeinputtextpromptandsubject,object,and
predicatecategories. Duringtraining,wesettheinputimagetobe640×640,withbatchsizeof
64. WeoptimizeournetworkwithAdamW[54]withaweightdecayof10−4. Wetrainallmodels
for 30 epochs with an initial learning rate of 10−4 decreased by 10 times at the 20th epoch. To
improvetrainingefficiency,weinitializeOurFleVRSusingthepre-trainedweightsfrom[109]. For
allexperiments,theparametersofthetextualencoderarefrozenexceptitslogitscales. Theloss
weightsλ , λ , λ andλ (superscriptomitted)aresetto1,1,2, and2. Moredetailsareinthe
b d c grd
appendix.
4.2 StandardVRS
Weevaluateourmethodonthreebenchmarks,i.e. HICO-DET[4],VCOCO[18]forHOIsegmenta-
tion,andPSG[85]forthepanopticSGG.
HOIsegmentationSinceOurFleVRSleveragesmasksupervision,eitherconvertingmaskresultsinto
boundingboxesortransformingboundingboxesfrompreviousmethods’outputintomasksdoesnot
facilitateacompletelyequitablecomparison. Fortheutmostfairnessincomparison,wereportboth
7Default(%)
Model Backbone
box/maskmAP box/maskmAP box/maskmAP
F R N
Bottom-upmethods
SCG[96] ResNet-50 31.3/31.3 24.7/25.0 33.3/35.5
UPT[97] ResNet-101 32.6/34.9 28.6/29.4 33.8/36.1
STIP[100] ResNet-50 32.2/30.8 28.2/28.6 33.4/32.5
ViPLO [60] ViT-B 37.2/39.1 35.5/37.8 37.8/39.7
Additionaltrainingwithobjectdetectiondata
UniVRD[101] ViT-L 37.4/- 28.9/- 39.9/-
PViC[98] Swin-L 44.3/- 44.6/- 44.2/-
RLIPv2[92] Swin-L 45.1/48.6 45.6/44.3 43.2/49.8
Single-stagemethods
HOTR[33] ResNet-50 25.1/26.5 17.3/18.5 27.4/29.0
QPIC[70] ResNet-101 29.9/30.5 23.0/23.1 31.7/33.1
CDN[95] ResNet-101 32.1/33.9 27.2/28.9 33.5/36.0
RLIP[91](VG+COCO) ResNet-50 32.8/34.4 26.9/27.7 34.6/36.5
GEN-VLKT[47] ResNet-101 35.0/35.6 31.2/32.6 36.1/37.8
ERNet[48] EfficientNetV2-XL 35.9/- 30.1/- 38.3/-
MUREN[35] ResNet-50 32.9/35.4 28.7/30.1 34.1/37.6
Ours Focal-L 38.1/40.5 33.0/34.9 39.5/42.4
Table2:QuantitativeresultsontheHICO-DETtestset. WereportbothboxandmaskmAP under
theDefaultsetting[4]containingtheFull(F),Rare(R),andNon-Rare(N)sets. no_interaction
classisremovedinmaskmAP.Thebestscoreishighlightedinbold,andthesecond-bestscoreis
underscored. ’-’meansthemodeldidnotreleaseweightsandwecannotgetthemaskmAP. Dueto
spacelimit,weshowthecompletetablewithmoremodelsintheappendix.
Model Backbone APS#1 APS#2
role role
Bottom-upmethods
VSGNet[72] ResNet-152 51.8/- 57.0/-
ACP[34] ResNet-152 53.2/- -/-
IDN[43] ResNet-50 53.3/- 60.3/-
STIP[100] ResNet-50 66.0/66.2 70.7/70.5
Additionaltrainingwithobjectdetectiondata
UniVRD[101] ViT-L 65.1/- 66.3/-
PViC[98] Swin-L 64.1/- 70.2/-
RLIPv2[92] Swin-L 72.1/71.7 74.1/73.5
Single-stagemethods
HOTR[33] ResNet-50 55.2/55.0 64.4/64.1
DIRV[11] EfficientDet-d3 56.1/- -/-
CDN[95] ResNet-101 63.9/61.3 65.8/63.2
RLIP[91] ResNet-50 61.9/61.3 64.2/64.0
GEN-VLKT[47] ResNet-101 63.6/61.8 65.9/64.0
ERNet[48] EfficientNetV2-XL 64.2/- -/-
Ours Focal-L 65.2/66.5 66.5/67.9
Table3: QuantitativeresultsonV-COCO.WereportbothboxandmaskmAP.Thebestscoreis
highlightedinbold,andthesecond-bestscoreisunderscored. ’-’meansthemodeldidnotrelease
weightsandwecannotgetthemaskmAP. Duetospacelimit,weshowthecompletetablewith
moremodelsintheappendix.
boxmAP andmaskmAP fromtheaboveways. AsshowninTable2,OurFleVRSshowssuperior
performanceovercurrentsingle-stagemethodsintermsofboxandmaskmAP onHICO-DET.We
alsoachievecompetitiveperformanceonVCOCO[18],asshowninTable3. TheadvantagesofOur
FleVRScomefrom: (1)one-stageTransformer-baseddesignwithfine-grainedtrainingsupervision
forVRS.Withsubjectandobjectmasks,themodelhasmoreaccuratesupervision,comparedwithbox
annotationsthatcontainredundancy[85]. (2)goodlanguage-visualalignmentwiththelarge-scale
pretrained model [64]. Our FleVRS achieves competitive results without additional training on
large-scaledetectiondatasets[101]. Amongone-stageHOImethods,ourapproachissimplerand
abletotackledifferentdatasetswithoutmodificationstothestructure.
PanopticSGGFromTable4,OurFleVRScanachievecompetitiveresultsintermsofR@50and
R@100withoutelaborateddesignsforPSG,comparedwithmostofpreviouswork. OurFleVRSis
8Method Backbone R/mR@20 R/mR@50 R/mR@100
AdaptedfromSGGmethods
IMP[81] VGG-16 17.9/7.35 19.5/7.88 20.1/8.02
MOTIFS[93] VGG-16 20.9/9.60 22.5/10.1 23.1/10.3
VCTree[71] VGG-16 21.7/9.68 23.3/10.2 23.7/10.3
GPSNet[50] VGG-16 18.4/6.52 20.0/6.97 20.6/7.17
One-stagePSGmethods
PSGTR[85] ResNet-101 28.2/15.4 32.1/20.3 35.3/21.5
PSGFormer[85] ResNet-101 18.0/14.2 20.1/18.3 21.0/19.8
Trainingwithadditionaldata
HiLo[104] Swin-L 40.6/29.7 48.7/37.6 51.4/40.9
Ours Focal-L 27.0/15.4 31.0/18.3 31.7/18.8
Table4: QuantitativeresultsonPSG.Thebestscoreishighlightedinbold,andthesecond-best
scoreisunderscored.
notsuperiortoHiLo[104],whichismainlyduetothelong-taildistributionofthedatasetandthe
limitationofusingCLIPtoencodeabstractrelationships(e.g.,entering,exiting). Themodeltendsto
predicthigh-frequencyrelationshipsandishardtounderstandandpredictlow-frequencyones.
Ablationstudy. WeablateOurFleVRSbytestingdifferentencodingstrategiesforrelationshipsvia
thetextualencoderin7. Specifically,wecompareencodingobjectandpredicatecategoriesas
<person, predicate, object>tripletsorseparately, associatingtheresultswitheithertriplet
cross-entropy (CE) loss or disentangled CE loss. Results reveal that while HICO-DET benefits
fromthedisentangledCEloss,allowingbettergeneralizationtonovelconcepts,VCOCOperforms
betterwithtripletCElossduetothechallengeofdistinguishingverbswithoutcorrespondingobjects
invariouscontexts(e.g.,differentiating“eat”in“apersoneatinganapple”vs“apersoneating”).
Furtherexperimentswithvariousbackbonesdemonstrateperformanceenhancementswithlarger
models. Additionally,incorporatingaboxheadforsupervisionalongsidemasksupervisionenhances
performance,whichisattributedtothemaskedattentionmechanisminspiredby[6]. Exploringthe
potentialsynergiesoftrainingacrossmultipledatasets,wefindthatwhileunifiedtrainingimproves
VCOCO’s performance due to its smaller size, HICO-DET and PSG show limited gains. This
disparityislikelyduetothedifferentpredicatecategoriesusedinPSGcomparedtoHICO-DETand
VCOCO.
Comparison with previous works. UniVRD uses a two-stage approach, where the model
first detects independent objects and then decodes relationships between them, retrieving boxes
from the initial detection stage. In contrast, our method employs a one-stage approach,
where each query directly corresponds to a <subject, object, predicate> triplet. This tran-
sition improves time efficiency from O(MxN) to O(K), where M is the number of subject
boxes, N is the number of object boxes, and K is the number of interactive pairs. Our ap-
proach also provides greater flexibility by learning a unified representation that encompasses
object detection, subject-object association, and relationship classification in a single model.
In terms of training data, we use much fewer train-
ing data (x50 less, without using VG [37] and Ob-
Nosubject Noobject Onlypredicate
jects365[66])andOurFleVRSwiththeFocal-L[84] S-IoU O-IoU S-IoU O-IoU
backboneismuchsmallerthanUniVRD[101](164M Conv-basedmethods
vs640M)withLiT(ViT-H/14),weachievecomparable VRD[55] 0.208 0.008 0.024 0.026
SSAS[38] 0.335 0.363 0.334 0.365
results(37.4vs38.1onHICO-DET).Whileourmethod
Ours 0.568 0.364 0.556 0.366
does not match RLIPv2 [92] in performance, this is
duetodifferentdesignphilosophiesandgoals. RLIPv2 Table 5: Comparison of promptable
isatwo-stageapproachoptimizedforlarge-scalepre- VRD results with the baseline on VRD
trainingandreliesonseparatelytraineddetectors. Our dataset[55].
FleVRS,however,isnotdesignedforpretrainingand
doesnotincludeaseparatelytraineddetector. OurfocusisonenhancingtheflexibilityoftheVRS
modelwithoutdirectlytrainingonextensivecurateddata(x50more,VGandObjects365). Thus,the
differencesinperformanceareattributedtothescaleanddesignobjectives. Wefurtherdiscussthe
FLOPsandthenumberparametersofthebackbonecomparedtopreviousworksintheAppendix.
94.3 PromptableVRS
WeevaluatetheabilityofpromptableVRSontheVRDdataset[55],tocomparewith[38]. Asin
Table5,OurFleVRScanlocateentitiesgivenflexibletextqueryinputsandperformsbetterlocalizing
subjectsandobjects. OurFleVRSgetsparticularbetterresultsonlocalizingsubjects(0.568vs0.335,
0.556 vs 0.334), which is mainly because there are fewer categories in subjects compared with
objectsandlotsofsubjectsarehumans,makingiteasiertosegmentsubjects. Wefurtherevaluateour
promptableVRSapproachonHICO-DETandPSG,astheycontainrichrelationshiplabels. Since
therearenopreviousbaselines,weshowqualitativeresultsinFig.4. Wevisualizethesubjectand
objectmaskswiththehighestmatchingscoreforeachexample. Wecanseethatthemodelisable
tolocalizesubjectandobjectmasksandpredicttheirrelationshipsgiventhestructuredtextual
prompt. WefurtherperformpostprocessingwaytosearchtripletsfromstandardVRSoutput,which
servesasanotherbaselinetoshowtheeffectivenessofourmethod. PleaserefertosectionEinthe
appendixforresultsanddiscussionsoffaircomparison.
Difference with standard REC tasks The referring expression comprehension (REC) tasks on
benchmarks like RefCOCO [30], RefCOCO+ [59], and RefCOCOg [90] are designed to detect
objectsbasedonfree-formtextualphrases,suchas"aballandacat"or"Twopandaslieonaclimber."
Incontrast,thepromptableVRStaskinourworkfocusesondetectingsubject-objectpairswithina
structuredpromptformat,suchas<?, sit_on, bench>or<person, ?, horse>,asillustrated
inFig.1ofthemainpaper. OurFleVRSisdesignedtoencodeandcomputesimilarityscoresfor
eachoftheseelementsseparately. Ourprimaryfocusisonrelationalobjectsegmentationbasedona
singlestructuredquery,whichdifferssignificantlyfromtheobjectivesofRECbenchmarks.
4.4 Open-vocabularyVRS
Weconductopen-vocabularyexperimentsfollowingthedefinedzero-shotHOIdetectionsetting[23,
25, 26, 47] on HICO-DET. As shown in Table 6, Our FleVRS surpasses previous single-dataset
methods across all settings, with its open-vocabulary capabilities stemming from the knowledge
transferred from CLIP [64]. GEN-VLKT [47] also leverages CLIP to facilitate open-vocabulary
capabilitiesbyencoding<person, predicate, object>asatripletandusingitforHOIcategory
classification.Incontrast,ourapproachseparatestheencodingofpredicateandobject,enhancing
themodel’sgeneralizationabilityovernovelconcepts.
Method Unseen Seen Full HICO-DET VCOCO PSG
R Va Cr LeF [2ir 4s ]tUnseenComposition 10.06 24.28 21.43 Variant maskmAPF maskAPS ro# l1 e R/mR@20
ATL[25] 9.18 24.67 21.57 Differentlosses
FCL[26] 13.16 24.23 22.01 DisentangledCEloss 40.5 62.1 27.0/15.4
GEN-VLKT[47] 21.36 32.91 30.56 TripletCEloss 36.8 66.5 25.5/14.6
DisentangledCEloss+TripletCEloss 39.0 64.5 26.5/14.8
Ours 26.06 39.61 36.60
Non-rareFirstUnseenComposition Differentvisualbackbones
VCL[24] 16.22 18.52 18.06 FocalTiny 34.2 59.8 25.8/15.0
ATL[25] 18.25 18.78 18.67 FocalLarge 40.5 66.5 27.0/15.4
FCL[26] 18.66 19.55 19.37 Differentdesignchoices
GEN-VLKT[47] 25.05 23.38 23.71 Boxheadonly 33.0 62.0 -
Ours 26.62 31.17 30.17 Maskheadonly 40.5 66.5 27.0/15.4
UnseenObject Maskandboxhead 41.2 67.0 -
FCL[26] 0.00 13.71 11.43
Differenttrainingdatasets
ATL[25] 5.05 14.69 13.08 Singlesource 40.5 66.5 27.0
GEN-VLKT[47] 10.51 28.92 25.63 HICO-DET+VCOCO 40.3 66.9 -
Ours 14.48 35.28 31.71 HICO-DET+VCOCO+PSG 40.0 66.4 27.6
UnseenVerb
GEN-VLKT[47] 20.96 30.23 28.74 Table7: Ablationsofdifferentlosstypes,backbones,
Ours 21.50 35.63 33.09
designchoicesandtrainingsets. WeadopttheFocal-
Table6: Resultsofopen-vocabularyHOI Lbackbonebydefault.
detectiononHICO-DET.
5 LimitationsandFutureWork
Deployingourmodelinreal-worldscenariosrequiresspecializedpretrainingdataforrelationshipun-
derstanding,whichisnotablyscarce. Thelackofautomatedannotationpipelinesanddependenceon
theCLIPmodelposescalabilitychallengesduetospecificresourcerequirements. Ideally,weaimfor
asinglegeneral-purposeframeworkthatcanbetrainedonmultipledatasetsandenhanceperformance
acrossvarioustasksandbenchmarks. However,achievingthisremainsachallengewithourcurrent
10model. Weleavetheexplorationofhowtosynergizedifferentdatasetsanddevelopeffectivetraining
strategiestofuturework. Whileintegratingfree-formtextinputsismorenaturalaslargelanguage
modelsevolve,itnecessitatesadditionalpreprocessingtoalignwithourframework. Furthermore,the
absenceofcomparablemethodsforpromptableVRSmakescompletefairbenchmarkingdifficult.
6 Conclusion
Inthiswork,wepresentanovelapproachforvisualrelationshipsegmentationthatintegratesthethree
criticalaspectsofaflexibleVRSmodel: standardVRS,promptablequerying,andopen-vocabulary
capabilities. OurFleVRSdemonstratestheabilitytonotonlysupportHOIsegmentationandpanoptic
SGGbutalsotodosoinresponsetovarioustextualpromptsandacrossaspectrumofpreviously
unseenobjectsandinteractions. Byharnessingthesynergisticpotentialoftextualandvisualfeatures,
ourmodeldeliverspromisingexperimentalresultsonexistingbenchmarkdatasets. Wehopeour
workcanserveasasolidsteppingstoneforpursuingmoreflexiblevisualrelationshipsegmentation
models.
References
[1] Abdelkarim, S., Agarwal, A., Achlioptas, P., Chen, J., Huang, J., Li, B., Church, K., and
Elhoseiny,M.(2021). Exploringlongtailvisualrelationshiprecognitionwithlargevocabulary. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.
[2] Bai,J.,Bai, S.,Yang,S.,Wang,S.,Tan,S.,Wang,P., Lin,J.,Zhou,C., andZhou,J.(2023).
Qwen-vl: Aversatilevision-languagemodelforunderstanding,localization,textreading,and
beyond. arXiv.
[3] Bansal,A.,Rambhatla,S.S.,Shrivastava,A.,andChellappa,R.(2020). Detectinghuman-object
interactionsviafunctionalgeneralization. InAAAI.
[4] Chao,Y.-W.,Liu,Y.,Liu,X.,Zeng,H.,andDeng,J.(2018). Learningtodetecthuman-object
interactions. InWACV.
[5] Chen,K.,Zhang,Z.,Zeng,W.,Zhang,R.,Zhu,F.,andZhao,R.(2023). Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic. arXivpreprintarXiv:2306.15195.
[6] Cheng,B.,Misra,I.,Schwing,A.G.,Kirillov,A.,andGirdhar,R.(2022). Masked-attention
masktransformerforuniversalimagesegmentation. InCVPR.
[7] Dai,X.,Chen,Y.,Xiao,B.,Chen,D.,Liu,M.,Yuan,L.,andZhang,L.(2021). Dynamichead:
Unifyingobjectdetectionheadswithattentions. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages7373–7382.
[8] Deng,J.,Yang,Z.,Chen,T.,Zhou,W.,andLi,H.(2021). Transvg: End-to-endvisualgrounding
withtransformers. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages1769–1779.
[9] Ding,Z.,Wang,J.,andTu,Z.(2022). Open-vocabularypanopticsegmentationwithmaskclip.
arXivpreprintarXiv:2208.08984.
[10] Du,H.,Yu,X.,andZheng,L.(2020). Learningobjectrelationgraphandtentativepolicyfor
visualnavigation. InComputerVision–ECCV2020: 16thEuropeanConference,Glasgow,UK,
August23–28,2020,Proceedings,PartVII16,pages19–34.Springer.
[11] Fang, H.-S., Xie, Y., Shao, D., and Lu, C. (2021). Dirv: Dense interaction region voting
forend-to-endhuman-objectinteractiondetection. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence.
[12] Gao,C.,Zou,Y.,andHuang,J.-B.(2018). ican: Instance-centricattentionnetworkforhuman-
objectinteractiondetection. InBMVC.
[13] Gao,C.,Xu,J.,Zou,Y.,andHuang,J.-B.(2020). DRG:Dualrelationgraphforhuman-object
interactiondetection. InECCV.
11[14] Gkioxari,G.,Girshick,R.,Dollár,P.,andHe,K.(2018). Detectingandrecognizinghuman-
objectinteractions. InCVPR.
[15] Gu,J.,Stefani,E.,Wu,Q.,Thomason,J.,andWang,X.(2022).Vision-and-languagenavigation:
Asurveyoftasks,methods,andfuturedirections. InProceedingsofthe60thAnnualMeetingof
theAssociationforComputationalLinguistics(Volume1: LongPapers),pages7606–7623.
[16] Gu,J.,Wang,Y.,Zhao,N.,Xiong,W.,Liu,Q.,Zhang,Z.,Zhang,H.,Zhang,J.,Jung,H.,and
Wang,X.E.(2024a). Swapanything: Enablingarbitraryobjectswappinginpersonalizedvisual
editing. arXivpreprintarXiv:2404.05717.
[17] Gu,J.,Fang,Y.,Skorokhodov,I.,Wonka,P.,Du,X.,Tulyakov,S.,andWang,X.E.(2024b).
Via: A spatiotemporal video adaptation framework for global and local video editing. arXiv
preprintarXiv:2406.12831.
[18] Gupta,S.andMalik,J.(2015). Visualsemanticrolelabeling. arXivpreprintarXiv:1505.04474.
[19] Gupta,T.,Schwing,A.,andHoiem,D.(2019). No-frillshuman-objectinteractiondetection:
Factorization,layoutencodings,andtrainingtechniques. InICCV.
[20] Han,Z.,Zhu,F.,Lao,Q.,andJiang,H.(2024). Zero-shotreferringexpressioncomprehension
viastructuralsimilaritybetweenimagesandcaptions.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages14364–14374.
[21] He,C.,Zhu,H.,Gao,J.,Chen,K.,andNevatia,R.(2020). Cparr: Category-basedproposal
analysisforreferringrelationships. InCVPRworkshops.
[22] Hong,Y.,Rodriguez,C.,Qi,Y.,Wu,Q.,andGould,S.(2020). Languageandvisualentity
relationshipgraphforagentnavigation. AdvancesinNeuralInformationProcessingSystems,33,
7685–7696.
[23] Hou,Z.,Peng,X.,Qiao,Y.,andTao,D.(2020a). Visualcompositionallearningforhuman-
objectinteractiondetection. InECCV.
[24] Hou,Z.,Peng,X.,Qiao,Y.,andTao,D.(2020b). Visualcompositionallearningforhuman-
object interaction detection. In Computer Vision–ECCV 2020: 16th European Conference,
Glasgow,UK,August23–28,2020,Proceedings,PartXV16,pages584–600.Springer.
[25] Hou, Z., Yu, B., Qiao, Y., Peng, X., and Tao, D. (2021a). Affordance transfer learning for
human-objectinteractiondetection. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages495–504.
[26] Hou,Z.,Yu,B.,Qiao,Y.,Peng,X.,andTao,D.(2021b). Detectinghuman-objectinteraction
viafabricatedcompositionallearning. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages14646–14655.
[27] Hu,X.,Lin,Y.,Wang,S.,Wu,Z.,andLv,K.(2023). Agent-centricrelationgraphforobject
visualnavigation. IEEETransactionsonCircuitsandSystemsforVideoTechnology.
[28] Huang,Z.,Mo,X.,andLv,C.(2022). Multi-modalmotionpredictionwithtransformer-based
neural network for autonomous driving. In 2022 International Conference on Robotics and
Automation(ICRA),pages2605–2611.IEEE.
[29] Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., and Carion, N. (2021). Mdetr-
modulateddetectionforend-to-endmulti-modalunderstanding. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages1780–1790.
[30] Kazemzadeh, S., Ordonez, V., Matten, M., and Berg, T.(2014). Referitgame: Referring to
objectsinphotographsofnaturalscenes. InProceedingsofthe2014conferenceonempirical
methodsinnaturallanguageprocessing(EMNLP),pages787–798.
[31] Kim,B.,Choi,T.,Kang,J.,andKim,H.J.(2020a). Uniondet: Union-leveldetectortowards
real-timehuman-objectinteractiondetection. InComputerVision–ECCV2020: 16thEuropean
Conference,Glasgow,UK,August23–28,2020,Proceedings,PartXV16,pages498–514.Springer.
12[32] Kim,B.,Lee,J.,Kang,J.,Kim,E.-S.,andKim,H.J.(2021a).HOTR:End-to-endhuman-object
interactiondetectionwithtransformers. InCVPR.
[33] Kim,B.,Lee,J.,Kang,J.,Kim,E.-S.,andKim,H.J.(2021b). Hotr: End-to-endhuman-object
interactiondetectionwithtransformers. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages74–83.
[34] Kim, D.-J., Sun, X., Choi, J., Lin, S., and Kweon, I. S. (2020b). Detecting human-object
interactionswithactionco-occurrencepriors. InECCV.
[35] Kim,S.,Jung,D.,andCho,M.(2023). Relationalcontextlearningforhuman-objectinteraction
detection. InCVPR.
[36] Kirillov,A.,Mintun,E.,Ravi,N.,Mao,H.,Rolland,C.,Gustafson,L.,Xiao,T.,Whitehead,S.,
Berg,A.C.,Lo,W.-Y.,etal.(2023). Segmentanything. arXivpreprintarXiv:2304.02643.
[37] Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y.,
Li,L.-J.,Shamma,D.A.,etal.(2017). Visualgenome: Connectinglanguageandvisionusing
crowdsourceddenseimageannotations. Internationaljournalofcomputervision,123,32–73.
[38] Krishna,R.,Chami,I.,Bernstein,M.,andFei-Fei,L.(2018). Referringrelationships. InCVPR.
[39] Li,J.,Li,D.,Xiong,C.,andHoi,S.(2022a). Blip: Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration. InInternationalconferenceonmachine
learning,pages12888–12900.PMLR.
[40] Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L.,
Hwang, J.-N., et al. (2022b). Grounded language-image pre-training. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages10965–10975.
[41] Li,R.,Zhang,S.,Wan,B.,andHe,X.(2021). Bipartitegraphnetworkwithadaptivemessage
passingforunbiasedscenegraphgeneration. InCVPR.
[42] Li, Y.-L., Zhou, S., Huang, X., Xu, L., Ma, Z., Fang, H.-S., Wang, Y., and Lu, C. (2019).
Transferableinteractivenessknowledgeforhuman-objectinteractiondetection. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages3585–3594.
[43] Li,Y.-L.,Liu,X.,Wu,X.,Li,Y.,andLu,C.(2020). Hoianalysis: Integratinganddecomposing
human-objectinteraction. AdvancesinNeuralInformationProcessingSystems,33,5011–5022.
[44] Liang,C.,Wu,Y.,Zhou,T.,Wang,W.,Yang,Z.,Wei,Y.,andYang,Y.(2021). Rethinking
cross-modal interaction from a top-down perspective for referring video object segmentation.
arXivpreprintarXiv:2106.01061.
[45] Liang,F.,Wu,B.,Dai,X.,Li,K.,Zhao,Y.,Zhang,H.,Zhang,P.,Vajda,P.,andMarculescu,D.
(2023). Open-vocabularysemanticsegmentationwithmask-adaptedclip. InCVPR.
[46] Liao, Y., Liu, S., Wang, F., Chen, Y., Qian, C., and Feng, J. (2020). Ppdm: Parallel point
detectionandmatchingforreal-timehuman-objectinteractiondetection. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages482–490.
[47] Liao,Y.,Zhang,A.,Lu,M.,Wang,Y.,Li,X.,andLiu,S.(2022).Gen-vlkt:Simplifyassociation
andenhanceinteractionunderstandingforhoidetection. InCVPR.
[48] Lim,J.,Baskaran,V.M.,Lim,J.M.-Y.,Wong,K.,See,J.,andTistarelli,M.(2023). Ernet: An
efficientandreliablehuman-objectinteractiondetectionnetwork. IEEETransactionsonImage
Processing.
[49] Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ramanan,D.,Dollár,P.,andZitnick,
C.L.(2014). Microsoftcoco: Commonobjectsincontext. InECCV.
[50] Lin,X.,Ding,C.,Zeng,J.,andTao,D.(2020). Gps-net: Graphpropertysensingnetworkfor
scenegraphgeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages3746–3753.
13[51] Liu,C.,Ding,H.,andJiang,X.(2023a). Gres: Generalizedreferringexpressionsegmentation.
InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
23592–23601.
[52] Liu,S.,Zeng,Z.,Ren,T.,Li,F.,Zhang,H.,Yang,J.,Li,C.,Yang,J.,Su,H.,Zhu,J.,etal.
(2023b). Groundingdino: Marryingdinowithgroundedpre-trainingforopen-setobjectdetection.
arXivpreprintarXiv:2303.05499.
[53] Liu,Y.,Chen,Q.,andZisserman,A.(2020). Amplifyingkeycuesforhuman-object-interaction
detection. InECCV.
[54] Loshchilov,I.andHutter,F.(2017). Decoupledweightdecayregularization. arXivpreprint
arXiv:1711.05101.
[55] Lu,C.,Krishna,R.,Bernstein,M.,andFei-Fei,L.(2016). Visualrelationshipdetectionwith
languagepriors. InECCV.
[56] Lüddecke, T.andEcker, A.(2022). Imagesegmentationusingtextandimageprompts. In
CVPR.
[57] Ma,S.,Wang,Y.,Wang,S.,andWei,Y.(2023). Fgahoi:Fine-grainedanchorsforhuman-object
interactiondetection. arXivpreprintarXiv:2301.04019.
[58] Ma,X.,Li,J.,Kochenderfer,M.J.,Isele,D.,andFujimura,K.(2021). Reinforcementlearning
forautonomousdrivingwithlatentstateinferenceandspatial-temporalrelationships. In2021
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages6064–6071.IEEE.
[59] Mao,J.,Huang,J.,Toshev,A.,Camburu,O.,Yuille,A.L.,andMurphy,K.(2016). Generation
andcomprehensionofunambiguousobjectdescriptions. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pages11–20.
[60] Park,J.,Park,J.-W.,andLee,J.-S.(2023). Viplo: Visiontransformerbasedpose-conditioned
self-loopgraphforhuman-objectinteractiondetection. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition,pages17152–17162.
[61] Peyre,J.,Laptev,I.,Schmid,C.,andSivic,J.(2019). Detectingunseenvisualrelationsusing
analogies. InICCV.
[62] Qi,S.,Wang,W.,Jia,B.,Shen,J.,andZhu,S.-C.(2018). Learninghuman-objectinteractions
bygraphparsingneuralnetworks. InProceedingsoftheEuropeanconferenceoncomputervision
(ECCV),pages401–417.
[63] Raboh,M.,Herzig,R.,Berant,J.,Chechik,G.,andGloberson,A.(2020). Differentiablescene
graphs. InWACV.
[64] Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,Askell,A.,
Mishkin,P.,Clark,J.,etal.(2021). Learningtransferablevisualmodelsfromnaturallanguage
supervision. InICML.
[65] Shadlen,M.N.,Britten,K.H.,Newsome,W.T.,andMovshon,J.A.(1996). Acomputational
analysisoftherelationshipbetweenneuronalandbehavioralresponsestovisualmotion. Journal
ofNeuroscience,16(4),1486–1510.
[66] Shao,S.,Li,Z.,Zhang,T.,Peng,C.,Yu,G.,Zhang,X.,Li,J.,andSun,J.(2019). Objects365:
Alarge-scale,high-qualitydatasetforobjectdetection. InICCV.
[67] Shic,F.andScassellati,B.(2007). Abehavioralanalysisofcomputationalmodelsofvisual
attention. Internationaljournalofcomputervision,73,159–177.
[68] Singh,A.,Hu,R.,Goswami,V.,Couairon,G.,Galuba,W.,Rohrbach,M.,andKiela,D.(2022).
Flava: Afoundationallanguageandvisionalignmentmodel. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages15638–15650.
[69] Su,W.,Zhu,X.,Cao,Y.,Li,B.,Lu,L.,Wei,F.,andDai,J.(2019). Vl-bert: Pre-trainingof
genericvisual-linguisticrepresentations. arXivpreprintarXiv:1908.08530.
14[70] Tamura,M.,Ohashi,H.,andYoshinaga,T.(2021). Qpic: Query-basedpairwisehuman-object
interactiondetectionwithimage-widecontextualinformation. InCVPR.
[71] Tang,K.,Zhang,H.,Wu,B.,Luo,W.,andLiu,W.(2019). Learningtocomposedynamictree
structuresforvisualcontexts. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages6619–6628.
[72] Ulutan,O.,Iftekhar,A.,andManjunath,B.S.(2020). VSGNet: Spatialattentionnetworkfor
detectinghumanobjectinteractionsusinggraphconvolutions. InCVPR.
[73] Wan,B.,Zhou,D.,Liu,Y.,Li,R.,andHe,X.(2019). Pose-awaremulti-levelfeaturenetwork
forhumanobjectinteractiondetection. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages9469–9478.
[74] Wang,H.,Zheng,W.-s.,andYingbiao,L.(2020a). Contextualheterogeneousgraphnetworkfor
human-objectinteractiondetection. InComputerVision–ECCV2020: 16thEuropeanConference,
Glasgow,UK,August23–28,2020,Proceedings,PartXVII16,pages248–264.Springer.
[75] Wang,H.,Du,Y.,Zhang,Y.,Li,S.,andZhang,L.(2022a). One-stagevisualrelationshiprefer-
ringwithtransformersandadaptivemessagepassing. IEEETransactionsonImageProcessing.
[76] Wang,P.,Yang,A.,Men,R.,Lin,J.,Bai,S.,Li,Z.,Ma,J.,Zhou,C.,Zhou,J.,andYang,H.
(2022b).Ofa:Unifyingarchitectures,tasks,andmodalitiesthroughasimplesequence-to-sequence
learning framework. In International Conference on Machine Learning, pages 23318–23340.
PMLR.
[77] Wang, T., Yang, T., Danelljan, M., Khan, F. S., Zhang, X., and Sun, J. (2020b). Learning
human-objectinteractiondetectionusinginteractionpoints. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages4116–4125.
[78] Wang,W.,Chen,Z.,Chen,X.,Wu,J.,Zhu,X.,Zeng,G.,Luo,P.,Lu,T.,Zhou,J.,Qiao,Y.,
etal.(2024). Visionllm: Largelanguagemodelisalsoanopen-endeddecoderforvision-centric
tasks. AdvancesinNeuralInformationProcessingSystems,36.
[79] Wu, J., Jiang, Y., Sun, P., Yuan, Z., and Luo, P. (2022). Language as queries for referring
videoobjectsegmentation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages4974–4984.
[80] Xu,B.,Wong,Y.,Li,J.,Zhao,Q.,andKankanhalli,M.S.(2019). Learningtodetecthuman-
objectinteractionswithknowledge. InCVPR.
[81] Xu,D.,Zhu,Y.,Choy,C.B.,andFei-Fei,L.(2017).Scenegraphgenerationbyiterativemessage
passing. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
5410–5419.
[82] Xu,J.,DeMello,S.,Liu,S.,Byeon,W.,Breuel,T.,Kautz,J.,andWang,X.(2022). Groupvit:
Semanticsegmentationemergesfromtextsupervision. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition,pages18134–18144.
[83] Yan,B.,Jiang,Y.,Wu,J.,Wang,D.,Luo,P.,Yuan,Z.,andLu,H.(2023). Universalinstance
perception as object discovery and retrieval. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pages15325–15336.
[84] Yang,J.,Li,C.,Dai,X.,andGao,J.(2022a). Focalmodulationnetworks. NeurIPS.
[85] Yang,J.,Ang,Y.Z.,Guo,Z.,Zhou,K.,Zhang,W.,andLiu,Z.(2022b). Panopticscenegraph
generation. InECCV.
[86] Yang,J.,Peng,W.,Li,X.,Guo,Z.,Chen,L.,Li,B.,Ma,Z.,Zhou,K.,Zhang,W.,Loy,C.C.,
etal.(2023). Panopticvideoscenegraphgeneration. InCVPR.
[87] Yi, M., Cui, Q., Wu, H., Yang, C., Yoshie, O., andLu, H.(2023). Asimpleframeworkfor
text-supervisedsemanticsegmentation. InCVPR.
15[88] You,H.,Zhang,H.,Gan,Z.,Du,X.,Zhang,B.,Wang,Z.,Cao,L.,Chang,S.-F.,andYang,
Y. (2023). Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint
arXiv:2310.07704.
[89] Yu,J.,Chai,Y.,Wang,Y.,Hu,Y.,andWu,Q.(2021). Cogtree: Cognitiontreelossforunbiased
scenegraphgeneration. InIJCAI.
[90] Yu,L.,Poirson,P.,Yang,S.,Berg,A.C.,andBerg,T.L.(2016). Modelingcontextinreferring
expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The
Netherlands,October11-14,2016,Proceedings,PartII14,pages69–85.Springer.
[91] Yuan, H., Jiang, J., Albanie, S., Feng, T., Huang, Z., Ni, D., and Tang, M. (2022). Rlip:
Relationallanguage-imagepre-trainingforhuman-objectinteractiondetection. InAdvancesin
NeuralInformationProcessingSystems.
[92] Yuan,H.,Zhang,S.,Wang,X.,Albanie,S.,Pan,Y.,Feng,T.,Jiang,J.,Ni,D.,Zhang,Y.,and
Zhao,D.(2023). Rlipv2: Fastscalingofrelationallanguage-imagepre-training. InICCV.
[93] Zellers,R.,Yatskar,M.,Thomson,S.,andChoi,Y.(2018). Neuralmotifs: Scenegraphparsing
with global context. In Proceedings of the IEEE conference on computer vision and pattern
recognition,pages5831–5840.
[94] Zhai,X.,Wang,X.,Mustafa,B.,Steiner,A.,Keysers,D.,Kolesnikov,A.,andBeyer,L.(2022).
Lit:Zero-shottransferwithlocked-imagetexttuning.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages18123–18133.
[95] Zhang,A.,Liao,Y.,Liu,S.,Lu,M.,Wang,Y.,Gao,C.,andLi,X.(2021a). Miningthebenefits
oftwo-stageandone-stagehoidetection. InNeurIPS.
[96] Zhang,F.Z.,Campbell,D.,andGould,S.(2021b). Spatiallyconditionedgraphsfordetecting
human-objectinteractions. InICCV.
[97] Zhang,F.Z.,Campbell,D.,andGould,S.(2022a).Efficienttwo-stagedetectionofhuman-object
interactionswithanovelunary-pairwisetransformer. InCVPR.
[98] Zhang,F.Z.,Yuan,Y.,Campbell,D.,Zhong,Z.,andGould,S.(2023a). Exploringpredicate
visual context in detecting of human-object interactions. In Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision.
[99] Zhang,H.,Li,F.,Zou,X.,Liu,S.,Li,C.,Yang,J.,andZhang,L.(2023b). Asimpleframework
foropen-vocabularysegmentationanddetection. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages1020–1031.
[100] Zhang,Y.,Pan,Y.,Yao,T.,Huang,R.,Mei,T.,andChen,C.-W.(2022b). Exploringstructure-
awaretransformeroverinteractionproposalsforhuman-objectinteractiondetection. InCVPR.
[101] Zhao, L., Yuan, L., Gong, B., Cui, Y., Schroff, F., Yang, M.-H., Adam, H., and Liu, T.
(2023). Unifiedvisualrelationshipdetectionwithvisionandlanguagemodels. arXivpreprint
arXiv:2303.08998.
[102] Zhong,X.,Ding,C.,Qu,X.,andTao,D.(2020). Polysemydecipheringnetworkforhuman-
objectinteractiondetection. InECCV.
[103] Zhong,X.,Qu,X.,Ding,C.,andTao,D.(2021). Glanceandgaze: Inferringaction-aware
pointsforone-stagehuman-objectinteractiondetection. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition,pages13234–13243.
[104] Zhou,Z.,Shi,M.,andCaesar,H.(2023a). Hilo: Exploitinghighlowfrequencyrelationsfor
unbiasedpanopticscenegraphgeneration. InICCV.
[105] Zhou,Z.,Shi,M.,andCaesar,H.(2023b). Vlprompt: Vision-languagepromptingforpanoptic
scenegraphgeneration. arXiv:2311.16492.
[106] Zhu,F.,Xie,Y.,Xie,W.,andJiang,H.(2023). Diagnosinghuman-objectinteractiondetectors.
arXivpreprintarXiv:2308.08529.
16[107] Zhu,J.andWang,H.(2021). Multiscaleconditionalrelationshipgraphnetworkforreferring
relationshipsinimages. IEEETransactionsonCognitiveandDevelopmentalSystems.
[108] Zou,C.,Wang,B.,Hu,Y.,Liu,J.,Wu,Q.,Zhao,Y.,Li,B.,Zhang,C.,Zhang,C.,Wei,Y.,
etal.(2021). End-to-endhumanobjectinteractiondetectionwithhoitransformer. InCVPR.
[109] Zou,X.,Dou,Z.-Y.,Yang,J.,Gan,Z.,Li,L.,Li,C.,Dai,X.,Behl,H.,Wang,J.,Yuan,L.,etal.
(2023a). Generalizeddecodingforpixel,image,andlanguage. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages15116–15127.
[110] Zou,X.,Yang,J.,Zhang,H.,Li,F.,Li,L.,Gao,J.,andLee,Y.J.(2023b). Segmenteverything
everywhereallatonce. arXivpreprintarXiv:2304.06718.
17A ModelStructureDetails
WeuseFocalT/L[84]networkastheimageencoderEnc . GiventheimageI∈RH×W×3,wepass
I
ittoEnc andobtainmulti-scalefeaturesofdifferentstridesandchannalsF={F |s=4,8,16,32},
I s
wheresisthestride.
Then,thepixeldecoderDec graduallyupsampleFtogeneratehigh-resolutionper-pixelembeddings
p
P={P |i=1,2,3,4},whereiisthelayernumberanddifferentP shavethesamechannelnumber
i i
butdifferentresolutions. PwilltheninputtoDec .
R
UnderstandardVRS,Dec takeslatentqueriesQv andPasinputs. UnderpromptableVRS,we
R
use the textual encoder Enc to encode the textual prompt into a set of textual queries Qt and
T
concatenate Qt with the latent queries Qv, and input them to Dec . Inside Dec , cross- and
R R
self-attentionarecomputedamongqueriesandper-pixelembeddings,wheremaskedattention[6]is
adoptedtoenhancetheforegroundregionsofpredictedmasks.
OntopoflatentqueriesoutputQv
o
∈RNv×Cq (N
v
isthenumberoflatentqueries,C
q
isthechannel
number),therearefiveheads,producingpredictionsinparallel. Theyaretwomaskheadsf (·),
Ms
f (·) forpredicting subjectandobject masks(M ,M ), andtwo classheads g (·), g (·) for
Mo s o Cs Co
predictingtheirobjectcategories(C ,C ). Anotherclassheadg isusedtopredictrelationshipsC
s o Cp p
forthis<subject, object>pair. Detailedoperationscanbewrittenas
M =Up[P ·f (Qv)], (3)
s 4 Ms o
M =Up[P ·f (Qv)], (4)
o 4 Mo o
C =T ·g (Qv), (5)
s s Cs o
C =T ·g (Qv), (6)
o o Co o
C =T ·g (Qv), (7)
p p Cp o
whereUp[·]denotestheupsamplingoperation, T , T andT denotecandidatetextualfeatures
s o p
ofsubject,objectandpredicatecategoriesthatareencodedbyCLIP[64]. Themaskembeddings
f (Qv) and f (Qv) compute the dot products with the last layer’s per-pixel embedding P ,
Ms o Mo o 4
respectively,andupsampletotheoriginalresolutionasfinalmaskpredictions.
Training. WeemployHungarianmatchingtoalignpredictedtripletswithgroundtruth,calculating
mask and category classification losses on these matches. For promptable VRS, we introduce a
matchinglosstoassessthesimilaritybetweenthematchedtripletembeddingandthetextualprompt’s
feature,formulatedasacross-entropyloss. Thetripletembeddingcombinesclassembeddingsfrom
classheads. Forthetextualprompt’sfeature,weusethelasttoken’sfeaturefromtextualqueries
Qt. Forinstance,withatextualpromptlike<subject, predicate, ?>,wherethesubjectand
predicatearespecified,thesimilaritymeasurementutilizesthesummationoftheirclassembeddings
g (Qv)+g (Qv).
Cs o Cp o
Inference. Under standard VRS, we compute the confidence score of each triplet, which comes
fromtheproductofsubject,object,andpredicateclassificationscores. Wetaketopk (k =100)
s s
tripletstocomputemeanaverageprecisionforHOIsegmentationandmeanrecallforpanopticSGG.
UnderpromptableVRS,wecomputesimilaritiesbetweenthetextualprompt’sfeatureandtriplet
embeddingsandchoosek (k =10)asthefinalpredictedtriplets.
f f
B Implementationdetails
We set the input image size to be 640 × 640, with batch size as 64. The model is optimized
with AdamW [54] with a weight decay of 10−4. We set N = 100 and N = 200 for Focal-T
v v
and Focal-L backbones, respectively, and C = 512. The structure of pixel decoder Dec is a
q p
Transformerencoderwith6encoderlayersand8heads. ThestructureofrelationshipdecoderDec
R
isaTransformerdecoderwith9decoderlayers.
StandardHOIsegmentationThemodelonlytakestheimageasinputwithouttextualprompts.
Sincethesubjectclassisalways“person"inHOIsegmentation,weomitthesubjectclasshead. The
modelistrainedwith30epochs,withaninitiallearningrateof10−4(10−5fortheimageencoder)
decreasedby10timesatthe20thepoch. Thelossweightsλ ,λ ,λoandλparesettobe2,1,1,2.
b d c c
18(a) Text prompt: <?, object, predicate> (b) Text prompt: <subject, predicate, ?>
<?, walk, bicycle> <?, load, bus> <person, load, ?> <person, sit on, ?>
<?, sit on, coach> <?, jump, skateboard> <person, lie on, ?> <person, straddle, ?>
(c) Text prompt: <subject, ?, object>
<person, ?, dining table> <person, ?, skateboard> <person, ?, hotdog> <person, ?, knife>
[eat_at, sit_at] [jump, ride] [eat, hold] [hold, wield]
Figure5: Qualitativeresultsofpromptableandopen-vocabularyVRSonHICO-DET[4]test
set. Weshowvisualizationsofthepredictedtripletwiththehighestmatchingscore,includingsubject,
objectmasks,andpredictedpredicatecategories. Therearethreetypesoftextualpromptsshown
in(a),(b),and(c),withunseenconceptsintherightmostcolumns. In(c),weshowthepredicted
predicatesinboldcharacters. Unseenobjectsandpredicatesaredenotedinredcharacters. Notethat
thesubjectisalways“person”inHICO-DET.
PromptableHOIsegmentationToenablepromptableHOIsegmentation,webuildthreetypesoftex-
tualprompts: 1)“<s>person</s><p>predicate</p>”;2)“<p>predicate</p><o>object</o>”;
3)“<s>person</s><o>object</o>”. WeevaluatethemodelonHICO-DET[4]sinceitcontains
richerhuman-objectinteractionsthanVCOCO[18]. Duringtraining,werandomlysamplevarious
types of text prompts and simultaneously train different objectives using distinct loss terms. To
preventthemodelfromlearningshortcuts,weselectonegroundtruthtripletpertrainingimageand
pairitwitharandomlychosentextualprompttype. Thisapproachensuresabalanceddistributionof
labeledtrainingdataforpromptableVRSacrossdifferentprompttypes.
StandardpanopticSGGWeusethesubjectclassheadtopredictthesubjectcategoryandthemodel
doesnothavetextualpromptsasinputs. Themodelistrainedwith60epochs,withaninitiallearning
rateof10−4(10−5fortheimageencoder)decreasedby10timesatthe40thepoch. Thelossweights
λ ,λ ,λs,λoandλparesettobe2,1,1,1,2.
b d c c c
PromptablepanopticSGGSimilartopromptableHOIsegmentation,therearethreetypesoftextual
prompts: 1)“<s>subject</s><p>predicate</p>”;2)“<p>predicate</p><o>
object</o>”; 3) “<s>subject</s><o>object</o>”. Similarly, during training, we randomly
sample different types of text prompts, and different objectives are trained simultaneously with
differentlossterms. WealsokeepabalanceddistributionoflabeledtrainingdataforpanopticSGG.
Wesettheweightofgroundinglossλ tobe2,whileotherweightsarethesameasstandardpanoptic
g
SGG.
Open-vocabularypromptableVRSWeadoptthezero-shotsettingfrom[47]foropen-vocabulary
HOIsegmentation. Tostreamlineourapproach,weintegratetheopen-vocabularypromptablesetting
within the broader context of open-vocabulary VRS. In this setting, ’open-vocabulary’ refers to
handlingbothseenandunseencategoriesintheinputtextualprompts. Werandomlyexcludeobject
andpredicatecategoriesduringtrainingandassessourmodelontheseaswellasonseencategories.
Giventheabsenceofexistingbenchmarksforthisspecificchallenge,wepresentqualitativeresults
demonstratingourmodel’sproficiencyinopen-vocabularypromptableVRS.
19Default(%)
Model Backbone
box/maskmAP box/maskmAP box/maskmAP
F R N
Bottom-upmethods
InteractNet[14] ResNet-50 9.9/- 7.2/- 10.8/-
iCAN[12] ResNet-50 14.8/- 10.5/- 16.2/-
No-Frills[19] ResNet-152 17.2/- 12.2/- 18.7/-
DRG[13] ResNet-50 24.5/- 19.5/- 26.0/-
VSGNet[72] ResNet-152 19.8/- 16.1/- 20.9/-
FCMNet[53] ResNet-50 20.4/- 17.3/- 21.6/-
IDN[43] ResNet-50 23.4/- 22.5/- 23.6/-
ATL[25] ResNet-101 23.8/- 17.4/- 25.7/-
SCG[96] ResNet-50 31.3/31.3 24.7/25.0 33.3/35.5
UPT[97] ResNet-101 32.6/34.9 28.6/29.4 33.8/36.1
STIP[100] ResNet-50 32.2/30.8 28.2/28.6 33.4/32.5
ViPLO [60] ViT-B 37.2/39.1 35.5/37.8 37.8/39.7
Additionaltrainingwithobjectdetectiondata
UniVRD[101] ViT-L 37.4/- 28.9/- 39.9/-
PViC[98] Swin-L 44.3/- 44.6/- 44.2/-
RLIPv2[92] Swin-L 45.1/48.6 45.6/44.3 43.2/49.8
Single-stagemethods
DIRV[11] EfficientDet-d3 21.8/- 16.4/- 23.4/-
PPDM-Hourglass[46] DLA-34 21.9/- 13.9/- 24.3/-
HOI-Transformer[108] ResNet-101 26.6/- 19.2/- 28.8/-
GGNet[103] Hourglass-104 29.2 22.1/- 30.8/-
HOTR[33] ResNet-50 25.1/26.5 17.3/18.5 27.4/29.0
QPIC[70] ResNet-101 29.9/30.5 23.0/23.1 31.7/33.1
CDN[95] ResNet-101 32.1/33.9 27.2/28.9 33.5/36.0
RLIP[91](VG+COCO) ResNet-50 32.8/34.4 26.9/27.7 34.6/36.5
GEN-VLKT[47] ResNet-101 35.0/35.6 31.2/32.6 36.1/37.8
ERNet[48] EfficientNetV2-XL 35.9/- 30.1/- 38.3/-
MUREN[35] ResNet-50 32.9/35.4 28.7/30.1 34.1/37.6
Ours Focal-L 38.1/40.5 33.0/34.9 39.5/42.4
Table8:QuantitativeresultsontheHICO-DETtestset. WereportbothboxandmaskmAP under
theDefaultsetting[4]containingtheFull(F),Rare(R),andNon-Rare(N)sets. no_interaction
classisremovedinmaskmAP.Thebestscoreishighlightedinbold,andthesecond-bestscoreis
underscored. ’-’meansthemodeldidnotreleaseweightsandwecannotgetthemaskmAP.
C Qualitativeresultsofpromptableandopen-vocabularyVRS
We show qualitative results of promptable and open-vocabulary VRS on HOI segmentation and
panopticSGGbygivingthemodeldifferenttypesofstructuredtextualprompts. Forsimplicity,we
showexamplesofomittingonlyonecomponentofthetriplet. Wecanseethatourmodelisableto
localizethecorrectsubjectandobjectandcomplementthemissingelementcorrespondingtothe
giventextualprompt,e.g. <person,?,dining_table>inFig.5(c)and<truck,on,?>inFig.6(b). The
modelcanalsopredictmultipleinteractionsforthesamesubject-objectpair,asshowninFig.5(c)
andFig.6(c). Wefurthertrainedtwoversionsbyremovingunseenobjectsandunseenpredicates,
respectively. Weshowthatourmodelcandetectnovelobjectsandpredicatesbyfeedingunseen
conceptsintextualprompts,asintherightmostcolumnsofFig.5andFig.6. Fig.6(b)showsthe
modeloutputsmultipleinstancesinonesubjectmaskduetosimilarpatternsoccurringinthetraining
set. NotethattheflexibleVRDtaskismoredifficultonthePSG[85]datasetduetoitscomplexity
ofscenes,whilewemakethefirstattemptandourmodelisstillabletoshowpromisinggrounding
results.
D QuantitativeresultsofstandardVRS
DuetothelargenumberofworksonHOIdetection,weshowthecompletecomparisonwithprevious
methodsinFig.8andFig.9. Ourmodelachievescompetitiveresultsonbothdatasets,especially
compared with other single-stage methods. From Table 9, MUREN [35] gets the best result on
VCOCO(68.8vs. 65.2,68.2vs. 66.5),butcannotachieveasimilarlystrongresultonHICO-DET
(32.9vs. 38.1,35.4vs. 40.5),wheretheverbcategoriesaremorecomplicated.
20Model Backbone APS#1 APS#2
role role
Bottom-upmethods
InteractNet[14] ResNet-50 40.0/- -/-
GPNN[62] ResNet-50 44.0/- -/-
iCAN[12] ResNet-50 45.3/- 52.4/-
TIN[42] ResNet-50 47.8/- 54.2/-
DRG[13] ResNet-50 51.0/- -/-
IP-Net[77] ResNet-50 51.0/- -/-
VSGNet[72] ResNet-152 51.8/- 57.0/-
PMFNet[73] ResNet-50 52.0/- -/-
PD-Net[102] ResNet-50 52.6/- -/-
CHGNet[74] ResNet-50 52.7/- -/-
FCMNet[53] ResNet-50 53.1/- -/-
ACP[34] ResNet-152 53.2/- -/-
IDN[43] ResNet-50 53.3/- 60.3/-
STIP[100] ResNet-50 66.0/66.2 70.7/70.5
Additionaltrainingwithobjectdetectiondata
VCL[24] ResNet-101 48.3/- -/-
SCG[96] ResNet-50 54.2/49.2 60.9/53.4
UPT[97] ResNet-101 61.3/60.3 67.1/65.6
UniVRD[101] ViT-L 65.1/- 66.3/-
PViC[98] Swin-L 64.1/- 70.2/-
RLIPv2[92] Swin-L 72.1/71.7 74.1/73.5
Single-stagemethods
UnionDet[31] ResNet-50 47.5/- 56.2/-
HOI-Transformer[108] ResNet-101 52.9/- -/-
GGNet[103] Hourglass-104 54.7/- -/-
HOTR[33] ResNet-50 55.2/55.0 64.4/64.1
DIRV[11] EfficientDet-d3 56.1/- -/-
QPIC[70] ResNet-101 58.3/- 60.7/-
CDN[95] ResNet-101 63.9/61.3 65.8/63.2
RLIP[91] ResNet-50 61.9/61.3 64.2/64.0
GEN-VLKT[47] ResNet-101 63.6/61.8 65.9/64.0
ERNet[48] EfficientNetV2-XL 64.2/- -/-
MUREN[35] ResNet-50 68.8/68.2 71.0/70.2
Ours Focal-L 65.2/66.5 66.5/67.9
Table9: QuantitativeresultsonV-COCO.WereportbothboxandmaskmAP.Thebestscoreis
highlightedinbold,andthesecond-bestscoreisunderscored. ’-’meansthemodeldidnotrelease
weightsandwecannotgetthemaskmAP.
FairComparison. SinceexistingmodelsuseboundingboxannotationstotrainandevaluatemAP,
weensurefaircomparisonsbyconvertingourmodel’soutputmasksintoboundingboxestocompute
boxmAP. Additionally,weapplyreleasedweightsfrompreviousmethods,transformtheiroutput
boxesintosegmentationmasksusingSAM[36],andreportmaskmAP. Inbothmetrics,ourmodel
demonstratessuperiorperformance.
WefurthertraintheexistingHOIdetectorsCDN[95],STIP,GEN-VLKTthesameSAMgenerated
datausedinourpaper,whichleadstoworseaccuracyonHICO-DET,asinTab.10. Thus,themajor
performanceimprovementsofourworkareduetoboththeSAM-labeleddataandourarchitectural
design.
Model Trainedwithoriginalboxes TrainedwithSAMmasks
CDN 31.4 28.5
STIP 32.2 29.7
GEN-VLKT 35.6 32.1
Table10: ResultsofboxmAP onHICO-DETtestset. WetrainexistingHOIdetectorswitha
maskhead,byusingthemaskswegeneratedthroughSAM.
Atthesametime,wealsotrainourmodelwithboundingboxesonly,wherewegetdecreasedaccuracy
(mAP of30.7vs36.3). WeattributeittothenetworkarchitecturederivedfromMask2Former[6],
whichismainlydesignedforpixel-wisesegmentationtasks.
FLOPsandthenumberparametersofthebackbonecomparedtopreviousworks. AsinTable
2and3ofthemainpaper,wehavedoneextensivecomparisonswithpreviousmethods,including
backbonesonResNet-50/101/152,EfficientNet,Hourglass,SwinTransformers,andLiTarchitectures.
21(a) Text prompt: <?, object, predicate> (b) Text prompt: <subject, predicate, ?>
<?, standing on, road> <?, on, floor> <truck, on, ?> <person, hold, ?>
<?, driving on, road> <?, over, pavement> <person, ride, ?> <car, driving on, ?>
(c) Text prompt: <subject, ?, object>
<bo-le, ?, cabinet> <coach, ?, floor> <person, ?, sur6oard> <teddy bear, ?, chair>
[on] [over] [in, lie on, carry, touch] [over, beside, on]
Figure6: Qualitativeresultsofpromptableandopen-vocabularyVRSonPSG[85]testset. We
showvisualizationsofthepredictedtripletwiththehighestmatchingscore,includingsubject,object
masks,andpredictedpredicatecategories. Therearethreetypesoftextualpromptsshownin(a),(b),
and(c),withunseenconceptsintherightmostcolumns. In(c),weshowthepredictedpredicatesin
boldcharacters. Unseenobjectsandpredicatesaredenotedinredcharacters.
For previous methods that utilize ResNet backbone for HOI detection and PSG, our comparison
includes VSGNet, ACP, No-Frills, which use ResNet-152. To the best of our knowledge, larger
ResNet,suchasResNet-200,andResNet-269,arenotusedinpreviousmethodsonrelatedtasks.
ResNet,wehaveincludedthelargestmodelResNet-152,whichhas65Mparametersand15GFLOPs.
OtherbaselinesarenotusingtheResNetbackbone,forexample,UniVRDisusingLiT(ViT-H/14)
backbone. Ithas632Mparametersand162GFLOPs,alotmorethanour198Mparametersand15.6
GFLOPs,butstillperformsworsethanourmodel.
E FaircomparisonofpromptableVRS
PostprocessingofstandardVRSoutputs. Sincenoexistingmodelsshare the samesettingsas
promptable VRS, we create a baseline for fair comparison. Typically, promptable VRS can be
addressedbyfilteringstandardVRSoutputs. Wepost-processoutputsfromourstandardVRSmodel
to extract the desired triplets and compared their mAP with those from promptable VRS. The
post-processedresultsyieldalowermAP (15.7vs. 26.8),primarilybecausetheselectedtriplets
oftenhavelowerconfidencescores. Additionally,thepost-processingapproachisslower,taking8
secondscomparedto5secondsfordirectlypromptingthemodeltoretrievethedesiredtriplet.
Groundingabilitycomparedwithprompt-basedvision-languagemodels. Althoughpromptable
VRSissimilartovision-languagemodelslikeGLIP[40]andMDETR[29]ingroundingcapabilities,
ithasdistinctobjectives. Unlikethesemodels,whichfocusonentities,promptableVRSoutputs
triplets, making direct comparisons infeasible. Previous models are not equipped to handle the
promptablerelationshipunderstandingtaskdirectly. Toexplorethis,wemodifyourstructuraldesign
toincorporatemultipletextpromptsasinputs,whichareindividuallyprocessedwiththeirmatching
scoresaggregatedforclassification.Thisexperimentalsetup,however,resultsinreducedperformance,
increasedinferencetime(26svs. 5s),andhigherGPUmemoryusage(5Gvs. 3G).Thus,weargue
thattheproposedstructureissuitablefortacklingpromptableVRS.
22F MasksgeneratedbySAM
ClarificationsofchoosingsegmentationmasksWefirstlyillustratetheimportanceofchoosing
segmentationmasksoverboxesinFig.7. Traditionalboundingboxesoftenincludeoverlappingand
ambiguousinformation,leadingtoredundancy. Segmentationmasks,byaccuratelydelineatingobject
boundaries,provideamorepreciseandclearrepresentation,reducingsuchredundancy,whichisalso
illustratedin[85]and[86]. Besides,segmentationmasksprovideenhancedvisualunderstandingand
comprehensivecontextualanalysis. Additionally,objectdetectionmodelsoftenstruggletoprecisely
extractforegroundobjects,whichiswhytheyaretypicallycombinedwithsegmentationmodelslike
SAMforfine-grainedimagetasks. Ourmodel,however,presentsaunifiedmodelthatcanlocalize
bothsubjectsandobjects,alongwiththeircorrespondingsegmentationmasks.
Figure7: Illustrationoftheimportanceofusingmasksinsteadofboundingboxes. Weshow
exampleswhereoneobjectisoccludedbyotherobjects. Weshowbothboundingboxannotations
andmasksgeneratedwithSAM,whereonlythemaskscancorrectlylocatethepureobject.
NoisehandlinginusingmasksgeneratedbySAM.Toaddresspotentialnoiseandinaccuraciesin
masksgeneratedbySAM,weemployafilteringapproachbasedonIntersectionoverUnion(IoU).We
computetheIoUbetweenthegeneratedmasksandtheoriginalboxannotations. MaskswithanIoU
scorebelowathresholdof0.2areconsideredtohavesignificantdeviationsfromthegroundtruthand
arefilteredout. Thisthresholdischosentobalancethetrade-offbetweenincludingsufficientmask
dataandexcludingthosewithsubstantialinaccuracies. ThechosenIoUthresholdhelpsensurethat
onlymaskswithareasonableoverlapwiththegroundtruthannotationsareretained. Thisthreshold
issetbasedonempiricalevaluationandaimstominimizetheimpactofmasksthataretoonoisyor
incorrect,whilestillretainingasmuchusefuldataaspossible. Afterusingthisstrategy,weconduct
analysison200samples. Wetestedvariousthresholdsandfoundthisgetsthebestbalancebetween
denoisinganddataretaining(95%validdataretraining).
More visualizations of generated masks. We have included additional visualizations in Fig. 8
toillustratethefine-grainedmasksgeneratedfromtheboundingboxannotationsofexistingHOI
detectiondatasets. Thesevisualizationsindicatethatconvertingtomaskssignificantlyreducesthe
redundancyintheboxannotations. Additionally,asshowninFig.8(d),filteringwithIoUhelps
eliminatelow-qualitymasks.
(a) (b) (c) (d)
Figure8: Samplesoffine-grainedmasksgeneratedbyconvertingexistingboundingboxannota-
tionswithSAM.SamplesarechosenfromtheHICO-DETdataset. Greenboxesareoriginalbox
annotations. Duplicatedboxesaresuppressedafterconvertingtothemask,asshownin(a). Thereare
alsofailurecaseswherenomasksaregeneratedwiththegivenboxannotations,asin(d).
23