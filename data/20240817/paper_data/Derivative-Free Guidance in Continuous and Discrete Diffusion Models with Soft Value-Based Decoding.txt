Derivative-Free Guidance in Continuous and Discrete
Diffusion Models with Soft Value-Based Decoding
XinerLi1 YulaiZhao2 ChenyuWang3 GabrieleScalia4 GokcenEraslan4
SuragNair4 TommasoBiancalani4 AvivRegev4∗ SergeyLevine5∗ MasatoshiUehara4∗
1TexasA&MUniversity 2PrincetonUniversity 3MIT 4Genentech5UCBerkeley
Abstract
Diffusionmodelsexcelatcapturingthenaturaldesignspacesofimages,molecules,
DNA, RNA, and protein sequences. However, rather than merely -generating
designsthatarenatural,weoftenaimtooptimizedownstreamrewardfunctions
while preserving the naturalness of these design spaces. Existing methods for
achieving this goal often require “differentiable” proxy models (e.g., classifier
guidanceorDPS)orinvolvecomputationallyexpensivefine-tuningofdiffusion
models (e.g., classifier-free guidance, RL-based fine-tuning). In our work, we
proposeanewmethodtoaddressthesechallenges. Ouralgorithmisaniterative
samplingmethodthatintegratessoftvaluefunctions,whichlooksaheadtohow
intermediate noisy states lead to high rewards in the future, into the standard
inferenceprocedureofpre-traineddiffusionmodels. Notably,ourapproachavoids
fine-tuninggenerativemodelsandeliminatestheneedtoconstructdifferentiable
models. Thisenablesusto(1)directlyutilizenon-differentiablefeatures/reward
feedback,commonlyusedinmanyscientificdomains,and(2)applyourmethodto
recentdiscretediffusionmodelsinaprincipledway. Finally,wedemonstratethe
effectivenessofouralgorithmacrossseveraldomains,includingimagegeneration,
moleculegeneration,andDNA/RNAsequencegeneration. Thecodeisavailableat
https://github.com/masa-ue/SVDD.
1 Introduction
Diffusionmodelshavegainedpopularityaspowerfulgenerativemodels. Theirapplicationsextend
beyond image generation to include natural language generation (Sahoo et al., 2024; Shi et al.,
2024;Louetal.,2023),moleculegeneration(Joetal.,2022;Vignacetal.,2022),andbiological
(DNA,RNA,protein)sequencegeneration(Avdeyevetal.,2023;Starketal.,2024),andineachof
thesedomainsdiffusionmodelshavebeenshowntobeveryeffectiveatcapturingcomplexnatural
distributions. However, inpractice, wemightnotonlywanttogenerate realisticsamples, butto
producesamplesthatoptimizespecificdownstreamrewardfunctionswhilepreservingnaturalnessby
leveragingpre-trainedmodels. Forexample,incomputervision,wemightaimtogeneratenatural
imageswithhighaestheticandalignmentscores. Indrugdiscovery,wemayseektogeneratevalid
moleculeswithhighQED/SA/dockingscores(Leeetal.,2023;Jinetal.,2018)ornaturalRNAs(such
asmRNAvaccines(Chengetal.,2023))withhightranslationalefficiencyandstability(Castillo-Hair
andSeelig,2021;Asranietal.,2018),andnaturalDNAswithhighcell-specificity(Gosaietal.,2023;
Taskiranetal.,2024;Laletal.,2024).
The optimization of downstream reward functions using pre-trained diffusion models has been
approachedinvariousways. Inourwork,wefocusonnon-fine-tuning-basedmethodsbecausefine-
tuninggenerativemodels(e.g.,whenusingclassifier-freeguidance(Hoetal.,2020)orRL-basedfine-
∗regev.aviv@gene.com,svlevine@eecs.berkeley.edu ,uehara.masatoshi@gene.com
Preprint.Underreview.
4202
guA
51
]GL.sc[
1v25280.8042:viXraTable1: Acomparisonofourmethodwithanumberofpriorapproaches. “Non-differentiable”refers
tothemethod’scapabilitytooperatewithoutrequiringdifferentiableproxymodels. “NoTraining”
meansthatnoadditionaltrainingofthediffusionmodelisrequiredaslongaswehaveaccesstothe
rewardfeedback.
Nofine-tuning Non-differentiable NoTraining
Classifierguidance(DhariwalandNichol,2021) ✓
DPS(Chungetal.,2022) ✓ ✓
Classifier-free(HoandSalimans,2022) ✓
RLfine-tuning ✓
SVDD-MC(Ours) ✓ ✓
SVDD-PM(Ours) ✓ ✓ ✓
tuning(Blacketal.,2023;Fanetal.,2023;Ueharaetal.,2024;Clarketal.,2023;Prabhudesaietal.,
2023))oftenbecomescomputationallyintensive,especiallyaspre-trainedgenerativemodelsgrow
largerintheeraof“foundationmodels”. Althoughclassifierguidanceanditsvariants(e.g.,Dhariwal
andNichol(2021);Songetal.(2020);Chungetal.(2022);Bansaletal.(2023);Hoetal.(2022))have
shownsuccessasnon-fine-tuningmethodsinthesesettings,theyfacesignificantchallenges. Firstly,
astheywouldrequireconstructingdifferentiableproxymodels,theycannotdirectlyincorporateuseful
domain-specificnon-differentiablefeatures(e.g.,molecular/proteindescriptors(vanWestenetal.,
2013;Ghiringhellietal.,2015;EmontsandBuyel,2023))ornon-differentiablerewardfeedback(e.g.,
physics-basedsimulationssuchasVinaandRosetta(TrottandOlson,2010;Alhossaryetal.,2015;
Alfordetal.,2017)).Thislimitationalsohinderstheapplicationofcurrentclassifierguidancemethods
torecentlydevelopeddiscretediffusionmodels(Austinetal.,2021;Campbelletal.,2022;Sahoo
etal.,2024;Shietal.,2024;Louetal.,2023)inaprincipledmanner(i.e.,withouttransformingthe
discretespaceintotheEuclideanspace).Additionally,gradientcomputationcanbememory-intensive
withlargeproxymodels.
Weproposeanovelmethod,SVDD(SoftValue-
based Decoding in Diffusion models), for op-
timizing downstream reward functions in diffu- x T M M M M M A M M G M
sionmodels(summarizedinFigure1)toaddress
the aforementioned challenges. Inspired by re- x t A M M G M A U M G M A M C G M A M M G M
2.4 4.2 3.5
cent literature on RL-based fine-tuning (Uehara
x
et al., 2024), we first introduce soft value func- t−1 A M C G M v(xt−1):=E[r(x 0)|xt−1]
tionsthatserveaslook-aheadfunctions,indicat-
inghowintermediatenoisysamplesleadtohigh x 0 A C C G U r(x 0)
rewards in the future of the diffusion denoising RNA Translational efficiency
process. Afterlearning(orapproximating)these
valuefunctions,wepresentanewinference-time Figure 1: Summary of SVDD. The notation v
technique,SVDD,whichobtainsmultiplenoisy denotesvaluefunctionsthatpredictrewardr(x )
0
statesfromthepolicy(i.e.,denoisingmap)ofpre- (at time 0) from states at time t − 1. SVDD
trained diffusion models and selects the sample involvestwosteps: (1)generatingmultiplenoisy
withthehighestvaluefunctionateachtimestep. statesfrompre-trainedmodelsand(2)selecting
Specifically,weintroducetwoalgorithms(SVDD- thestatewiththehighestvalueaccordingtothe
MC and SVDD-PM) depending on how we es- valuefunction.
timatevaluefunctions. Notably,theSVDD-PM
approach,leveragingthecharacteristicsofdiffu-
sionmodels (e.g.,utilizingtheforwardprocessindiffusionmodelstodirectlymaptto0interms
ofexpectationinFigure1),requiresnoadditionaltrainingaslongaswehaveaccesstothereward
feedback.
Ourcontributionsaresummarizedasfollows(alsoseeTable1). Weproposeanoveltechniquefor
optimizingdownstreamrewardfunctionsinpre-traineddiffusionmodelsthateliminatestheneedto
constructdifferentiableproxymodelsandavoidstheneedtofine-tunethegenerativemodelitself.
Thefirstpropertyallowsfortheuseofnon-differentiablerewardfeedback,whichiscommoninmany
scientificfields,andmakesourmethodapplicabletorecentdiscretediffusionmodelswithoutany
modification. Thesecondpropertyaddressesthehighcomputationalcostassociatedwithfine-tuning
diffusionmodels. Wedemonstratetheeffectivenessofourmethodsacrossvariousdomains,including
imagegeneration,moleculegeneration,andDNA/RNAgeneration.
22 RelatedWorks
Here,wesummarizerelatedwork. Wefirstoutlinemethodsrelevanttoourgoal,categorizingthem
based on whether they involve fine-tuning. We then discuss related directions, such as discrete
diffusionmodels,whereourmethodexcels,anddecodinginautoregressivemodels.
Non-fine-tuningmethods. Thereareprimarilytwomethodsforoptimizingdownstreamfunctions
indiffusionmodelswithoutfine-tuning.
• Classifierguidance(DhariwalandNichol,2021;Songetal.,2020): Ithasbeenwidelyused
to condition pre-trained diffusion models without fine-tuning. Although these methods do not
originally focus on optimizing reward functions, they can be applied for this purpose (Uehara
etal.,2024,Section6.2). Inthisapproach,anadditionalderivativeofacertainvaluefunctionis
incorporatedintothedriftterm(mean)ofpre-traineddiffusionmodelsduringinference.Subsequent
variants(e.g.,Chungetal.(2022);Hoetal.(2022);Bansaletal.(2023);Guoetal.(2024);Wang
et al. (2022); Yu et al. (2023)) have been proposed to simplify the learning of value functions.
However,thesemethodsrequireconstructingdifferentiablemodels,whichlimitstheirapplicability
tonon-differentiablefeatures/rewardfeedbackscommonlyencounteredinscientificdomainsas
mentionedinSection1.Additionally,thisapproachcannotbedirectlyextendedtodiscretediffusion
modelsinaprincipleway2. Ourapproachaimstoaddressthesechallenges.
• Best-of-N:Thenaivewayistogeneratemultiplesamplesandselectthetopsamplesbasedon
therewardfunctions, knownasbest-of-Kintheliteratureon(autoregressive)LLMs(Stiennon
et al., 2020; Nakano et al., 2021; Touvron et al., 2023; Beirami et al., 2024; Gao et al., 2023).
This approach is significantly less efficient than ours, as our method uses soft-value functions
thatpredicthowintermediatenoisysamplesleadtohighrewardsinthefuture. Wevalidatethis
experimentallyinSection6.
Fine-tuning of diffusion models. Several methods exist for fine-tuning generative models to
optimizedownstreamrewardfunctions,suchasclassifier-freeguidance(HoandSalimans,2022)and
RL-basedfine-tuning(Fanetal.,2023;Blacketal.,2023)/itsvariants(Dongetal.,2023). However,
theseapproachesoftencomewithcaveats,includinghighcomputationalcostsandtheriskofeasily
forgettingpre-trainedmodels. Inourwork,weproposeaninference-timetechniquethateliminates
theneedforfine-tuninggenerativemodels,similartoclassifierguidance.
Discretediffusionmodels. BasedonseminalworksAustinetal.(2021);Campbelletal.(2022),
recent work on masked diffusion models (Lou et al., 2023; Shi et al., 2024; Sahoo et al., 2024)
hasdemonstratedtheirstrongperformanceinnaturallanguagegeneration. Additionally,theyhave
been applied to biological sequence generation (e.g., DNA, protein sequences in Campbell et al.
(2024);Sarkaretal.(2024)). Inthesecases,theuseofdiffusionmodelsoverautoregressivemodels
isparticularlyapt,giventhatmanybiologicalsequencesultimatelyadoptcomplexthree-dimensional
structures. WealsonotethatESM3(Hayesetal.,2024),awidelyrecognizedfoundationalmodelin
proteinsequencegeneration,bearssimilaritiestomaskeddiffusionmodels.
Despiteitssignificance,itcannotbeintegratedwithstandardclassifierguidancebecauseaddinga
continuousgradienttoadiscreteobjectiveisnotinherentlyvalid. Unlikestandardclassifierguidance,
ouralgorithmcanbeseamlesslyappliedtodiscretediffusionmodels.
Decodinginautoregressivemodelswithrewards. Thedecodingstrategy,whichdictateshow
sentencesaregeneratedfromthemodel,isacriticalcomponentoftextgenerationinautoregressive
languagemodels(Wuetal.,2016;ChorowskiandJaitly,2016;Leblondetal.,2021). Recentstudies
haveexploredinference-timetechniquesforoptimizingdownstreamrewardfunctionsDathathrietal.
(2019);YangandKlein(2021);Qinetal.(2022);Mudgaletal.(2023);Zhaoetal.(2024);Hanetal.
(2024). Whiletherearesimilaritiesbetweentheseworksandours, tothebestofourknowledge,
nopriorworkhasextendedsuchmethodologiestodiffusionmodels. Furthermore, ourapproach
leveragescharacteristicsuniquetodiffusionmodelsthatarenotpresentinautoregressivemodels
suchasSVDD-PM.
2Wenotethatanotableexceptionhasbeenrecentlyproposed(Nisonoffetal.,2024).However,ourmethod
canbeappliedtobothcontinuousanddiscretediffusionmodelsinaunifiedmanner.
33 PreliminariesandGoal
Inthissection,wedescribethestandardmethodfortrainingdiffusionmodelsandoutlinetheobjective
ofourwork: optimizingdownstreamrewardfunctionsgivenpre-traineddiffusionmodels.
3.1 DiffusionModels
In diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020), our goal is
tolearn asampler p(x|c) ∈ [C → ∆(X)] givendata consistingof (x,c) ∈ X ×C. For instance,
intext-to-imagediffusionmodels,cisatext,andxisanimage. Inproteininversefolding,cisa
backboneproteinstructure,andxisaproteinsequence.
Thetrainingprocessforastandarddiffusionmodelissummarizedasfollows. First,weintroducea
(fixed)forwardprocessq :X →∆(X). Now,afterintroducingtheforwardprocess,weaimtolearn
t
abackwardprocess: {p }whereeachp isX ×C →∆(X)sothattheinduceddistributionsinduced
t t
bytheforwardprocessandbackwardprocessmatchmarginally. Forthispurpose,byparametrizing
thebackwardprocesseswithθ ∈Rd,wetypicallyusethefollowinglossfunction:
(cid:34) T (cid:35)
(cid:88)
E −logp (x |x )+ KL(q (·|x )∥p (·|x ,c;θ))+KL(q (·)∥p (·)) ,
x1,···,xT∼q(·|x0) 0 0 1 t t−1 t t+1 T T
t=1
whichisderivedfromthevariationallowerboundofthenegativelikelihood(i.e.,ELBO).
Herearetwoexamplesofconcreteparameterizations. Letα ∈Rbeanoiseschedule.
t
Example1(Continuousspace). WhenX isEuclideanspace,wetypicallyusetheGaussiandistribu-
√
tionq (·|x)=N( α x,(1−α )). Then,thebackwardprocessisparameterizedas
t t t
√ √
(cid:18) α (1−α¯ )x + α (1−α )xˆ (x ,c;θ) (1−α )(1−α¯ )(cid:19)
N t t+1 t t−1 t 0 t , t t−1 ,
1−α¯ 1−α¯
t t
where α¯ =
(cid:81)t
α . Here, xˆ (x ,c;θ) is a neural network that predicts x from x (i.e.,
t i=1 i 0 t 0 t
E [x |x ,c]).
q 0 t
Example2(DiscretespaceinSahooetal.(2024);Shietal.(2024)). LetX beaspaceofone-hot
columnvectors{x∈{0,1}K :(cid:80)K x =1},andCat(π)bethecategoricaldistributionoverK
i=1 i
classeswithprobabilitiesgivenbyπ ∈∆K where∆K denotestheK-simplex. Atypicalchoiceis
q (· | x) = Cat(α x+(1−α )m)wherem = [0,··· ,0,Mask]. Then,thebackwardprocessis
t t t
parameterizedas
(cid:40)
Cat(x ), ifx ̸=m
t t
(cid:16) (cid:17)
Cat (1−αt−1)m+(αt−1−αt)xˆ0(xt,c;θ) , ifx =m.
1−αt t
Here,xˆ (x ,c;θ)isaneuralnetworkthatpredictsx fromx .Notethatwhenconsideringasequence
0 t 0 t
ofLtokens(x1:L),weusethedirectproduct: p (x1:L|x1:L,c)=(cid:81)L p (xl|x1:L,c).
t t t+1 l=1 t t t+1
Afterlearningthebackwardprocess,wecansamplefromadistributionthatemulatestrainingdata
distribution(i.e.,p(x|c))bysequentiallysampling{p }0 fromt=T tot=0.
t t=T
Notation. Thenotationδ denotesaDiracdeltadistributioncenteredata. Thenotation∝indicates
a
thatthedistributionisequaluptoanormalizingconstant. Withslightabuseofnotation,weoften
denotep (·|·,·)byp (·).
T T
3.2 Objective: GeneratingSampleswithHighRewardsWhilePreservingNaturalness
We consider a scenario where we have a pre-trained diffusion model, which is trained using the
loss function explained in Section 3.1. These pre-trained models are typically designed to excel
atcharacterizingthenaturaldesignspace(e.g.,imagespace,biologicalspace,orchemicalspace)
by emulating the extensive training dataset. Our work focuses on obtaining samples that also
optimizedownstreamrewardfunctionsr :X →R(e.g.,QEDandSAinmoleculegeneration)while
maintainingthenaturalnessbyleveragingpre-traineddiffusionmodels. Weformalizethisgoalas
follows.
4Given a pre-trained model {ppre}0 , we denote the induced distribution by ppre(x|c) (i.e.,
t t=T
(cid:82) {(cid:81)1 ppre(x |x ,c)}dx ). Weaimtosamplefromthefollowingdistribution:
t=T+1 t−1 t−1 t 1:T
p(α)(x|c):= argmax E [r(x)]−αKL(p(·|c)∥ppre(·|c))
x∼p(·|c)
p∈[C→∆(X)](cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
term(a) term(b)
∝exp(r(x)/α)ppre(x|c).
Here,theterm(a)isintroducedtooptimizerewardfunction,whiletheterm(b)isusedtomaintain
thenaturalnessofthegeneratedsamples.
Existing methods. Several existing approaches target this goal (or its variant), as discussed in
Section2,includingclassifierguidance,fine-tuning(RL-basedorclassifier-free),andBest-of-N.In
ourwork,wefocusonnon-fine-tuning-basedmethods,suchasclassifierguidanceandBest-of-N.
Specifically,weaimtoaddressthelimitationsofthesemethods: therequirementfordifferentiable
proxymodelsinclassifierguidanceandtheinefficiencyofBest-of-N.
4 SoftValue-BasedDecodinginDiffusionModels
In this section, we first present the motivation behind developing our new algorithm. We then
introduceouralgorithm,whichsatisfiesdesiredproperties,i.e.,thelackofneedforfine-tuningor
constructingdifferentiablemodels.
4.1 KeyObservation
Weintroduceseveralkeyconcepts. First,fort∈[T +1,··· ,1],wedefinethesoftvaluefunction:
(cid:20) (cid:18) (cid:19) (cid:21)
r(x )
v (·):=αlogE exp 0 |x =· ,
t−1 x0∼ppre(x0|xt−1) α t−1
where E [·] is induced by {ppre(·|x ,c)}0 . This value function represents the expected
{ppre} t t+1 t=T
future reward at t = 0 from the intermediate noisy state at t−1. Next, we define the following
soft optimal policy (denoising process) p⋆,α : X × C → ∆(X) weighted by value functions
t−1
v :X →R:
t−1
ppre(·|x ,c)exp(v (·)/α)
p⋆ t−,α 1(·|x t,c)= (cid:82) ppt r− e1 (x|xt ,c)exp(vt−1 (x)/α)dx.
t−1 t t−1
Here,werefertov assoftvaluefunctions,p⋆,αassoftoptimalpolicies,respectively,becausethey
t t
literally correspond to soft value functions and soft optimal policies where we embed diffusion
modelsintoentropy-regularizedMDPs,asdemonstratedinUeharaetal.(2024).
Withthispreparationinmind,weutilizethefollowingkeyobservation:
Theorem 1 (From Theorem 1 in Uehara et al. (2024)). The distribution induced by
{p⋆,α(·|x ,c)}0 isthetargetdistributionp(α)(x|c),i.e.,
t t+1 t=T
(cid:110) (cid:111)
p(α)(x |c)=(cid:82) (cid:81)1 p⋆,α(x |x ,c) dx .
0 t=T+1 t−1 t−1 t 1:T
WhileUeharaetal.(2024)presentsthistheorem,theyuseitprimarilytointerprettheirRL-based
fine-tuningintroducedinFanetal.(2023);Blacketal.(2023). Incontrast,ourworkexploreshowto
convertthisintoanewfine-tuning-freeoptimizationalgorithm.
Ourmotivationforanewalgorithm. Theorem1statesthatifwecanhypotheticallysamplefrom
{p⋆,α}0 ,wecansamplefromthetargetdistributionp(α). However,therearetwochallengesin
t t=T
samplingfromeachp⋆,α:
t−1
1. Thesoft-valuefunctionv inp⋆,α isunknown.
t−1 t−1
2. Itisunnormalized(i.e.,calculatingthenormalizingconstantisoftenhard).
5We address the first challenge later in Section 4.3. Assuming the first challenge is resolved, we
considerhowtotacklethesecondchallenge. Anaturalapproachistouseimportancesampling(IS):
p⋆,α(·|x ,c)≈
(cid:88)M w t⟨ −m 1⟩
δ , {x⟨m⟩}M ∼ppre(·|x ,c),
t−1 t (cid:80) w⟨j⟩ x⟨ t−m 1⟩ t−1 m=1 t−1 t
m=1 j t−1
where w⟨m⟩ := exp(v (x⟨m⟩)/α). Thus, we can approximately sample from p⋆,α(·|x ,c) by
t−1 t t−1 t−1 t
obtaining multiple (M) samples from pre-trained diffusion models and selecting the sample
basedonanindex,whichisdeterminedbysamplingfromthecategoricaldistributionwithmean
{w⟨m⟩/(cid:80) w⟨j⟩ }⟨M⟩.
t−1 j t−1 m=1
DifferenceoverBest-of-N. Best-of-N,whichgeneratesmultiplesamplesandselectsthehighest
rewardsample,istechnicallyconsideredISwheretheproposaldistributionistheentireppre(x |c)=
0
(cid:82) (cid:81) {ppre(x |x ,c)}dx . However,theuseofimportancesamplinginouralgorithmdiffers
t t t−1 t 1:T
significantly,asweapplyitateachtimesteptoapproximateeachsoft-optimalpolicy.
4.2 Inference-TimeAlgorithm
Algorithm1SVDD(SoftValue-BasedDecodinginDiffusionModels)
1: Require: Estimated soft value function {vˆ t}0
t=T
(refer to Algorithm 2 or Algorithm 3), pre-
traineddiffusionmodels{ppre}0 ,hyperparameterα∈R
t t=T
2: fort∈[T +1,··· ,1]do
3: GetM samplesfrompre-trainedpolices{x⟨ t−m 1⟩}M
m=1
∼pp t−re 1(·|x t),andforeachm,calculate
w⟨m⟩ :=exp(vˆ (x⟨m⟩)/α)
t−1 t−1 t−1
(cid:32)(cid:26) (cid:27)M (cid:33)
4: x t−1 ←x t⟨ −ζt 1−1⟩ afterselectinganindex: ζ t−1 ∼Cat (cid:80)Mw t⟨ −m w1⟩ ⟨j⟩ ,
j=1 t−1 m=1
5: endfor
6: Output: x 0
Now,weintroduceouralgorithm,whichleveragestheobservation,inAlgorithm1. Ouralgorithmis
aniterativesamplingmethodthatintegratessoftvaluefunctionsintothestandardinferenceprocedure
ofpre-traineddiffusionmodels.Eachstepisdesignedtoapproximatelysamplefromavalue-weighted
policy{p⋆,α}0 .
t t=T
Wehaveseveralimportantthingstonote.
• Whenα=0,Line4correspondstoζ =argmax vˆ (x⟨m⟩).Inpractice,weoften
t−1 m∈[1,···,M] t−1 t−1
recommendthischoice.
• AtypicalchoicewerecommendisM =[5,··· ,20]. TheperformancewithvaryingM valueswill
bediscussedinSection6.
• Line3canbecomputedinparallelattheexpenseofadditionalmemory.
• Whenthenormalizingconstantcanbecalculatedrelativelyeasily(e.g.,indiscretediffusionwith
smallK,L),wecandirectlysamplefrom{p⋆,α}0 .
t t=T
Theremainingquestionishowtoobtainthesoftvaluefunction,whichweaddressinthenextsection.
4.3 LearningSoftValueFunctions
Next, we describe how to learn soft value functions v (x) in practice. We propose two main
t
approaches: theMonteCarloregressionapproachandtheposteriormeanapproximationapproach.
MonteCarloregression. Here,weusethefollowingapproximationv′ asv where
t t
v′(·):=E [r(x )|x =·].
t x0∼ppre(x0|xt) 0 t
6Thisisbasedon
v (x ):=αlogE [exp(r(x )/α)|x ]≈log(exp(E [r(x )|x ]))=v′(x ).
t t x0∼ppre(x0|xt) 0 t x0∼ppre(x0|xt) 0 t t t
(1)
Byregressingr(x )ontox ,wecanlearnv′ asinAlgorithm2. CombiningthiswithAlgorithm1,
0 t t
werefertotheentireoptimizationapproachasSVDD-MC.
Algorithm2ValueFunctionEstimationUsingMonteCarloRegression
1: Require: Pre-traineddiffusionmodels,rewardr :X →R,functionclassΦ:X ×[0,T]→R.
2: Collectdatasets{x(s),··· ,x(s)}S byrolling-out{ppre}0 fromt=T tot=0.
T 0 s=1 t t=T
3: vˆ′ =argmin (cid:80)T (cid:80)S {r(x(s))−f(x(s),t)}2.
f∈Φ t=0 s=1 0 t
4: Output: vˆ′
Notethattechnically, withoutapproximationintroducedin(1), wecanestimatev byregressing
t
exp(r(x )/α) onto x based on the original definition. This approach may work in many cases.
0 t
However,whenαisverysmall,thescalingofexp(r(·)/α)tendstobeexcessivelylarge. Duetothis
concern,wegenerallyrecommendusingusingAlgorithm2.
Remark1(Anotherwayofconstructingdatasets). Wehaveusedthebackwardprocessinpre-trained
diffusionmodelstocollecttrainingdatasetsinAlgorithm2. Giventhatthebackwardandforward
processesaredesignedtomatchmarginallyduringthetrainingofdiffusionmodelsasdiscussedin
Section3.1,wecanalsousetheforwardprocessinpre-traineddiffusionmodelstoconstructdatasets.
Remark2(Anotherwayoflearningvaluefunctions). Technically,anothermethodforlearningvalue
functionsisavailablesuchassoft-Q-learning(SectionB),byleveragingsoft-Bellmanequationsin
diffusionmodelsUeharaetal.(2024,Section3)(asknowninstandardRLcontexts(Levine,2018;
Haarnojaetal.,2017;Geistetal.,2019)). However,sincewefindMonteCarloapproachestobe
morestable,werecommendthemoversoft-Q-learning.
Posterior mean approximation. Here, recalling we use xˆ (x ) (approximation of
0 t
E [x |x ]) when training pre-trained diffusion models in Section 3.1, we perform the
x0∼ppre(xt) 0 t
followingapproximation:
v (x):=αlogE [exp(r(x )/α)|x ]≈αlog(exp(r(xˆ (x ))/α)=r(xˆ (x )).
t x0∼ppre(x0|xt) 0 t 0 t 0 t
Then,wecanuser(xˆ (x ))astheestimatedvaluefunction.
0 t
Theadvantageofthisapproachisthatnoadditionaltrainingisrequiredaslongaswehaver. When
combinedwithAlgorithm1,werefertotheentireapproachasSVDD-PM.
Algorithm3ValueFunctionEstimationusingPosteriorMeanApproximation
1: Require: Pre-traineddiffusionmodels,rewardr :X →R
2: Setvˆ⋄(·,t):=r(xˆ 0(x t =·),t)
3: Output: vˆ⋄
Remark3(RelationwithDPS). Inthecontextofclassifierguidance,similarapproximationshave
beenemployed(e.g.,DPSinChungetal.(2022)). However,thefinalinference-timealgorithmsdiffer
significantly,asthesemethodscomputegradientsattheend.
5 Advantages,Limitations,andExtensionsofSVDD
Sofar,wehavedetailedouralgorithm,SVDD.Inthissection,wediscussitsadvantages,limitations,
andextensions.
5.1 Advantages
Nofine-tuning(ornotraininginSVDD-PM). Unlikeclassifier-freeguidanceorRL-basedfine-
tuning,SVDD doesnotrequireanyfine-tuningofthegenerativemodels. Inparticular,whenusing
SVDD-PM,noadditionaltrainingisneededaslongaswehaver.
7No need for constructing differentiable models. Unlike classifier guidance, SVDD does not
requiredifferentiableproxymodels,asthereisnoneedforderivativecomputations. Forexample,ifr
isnon-differentiablefeedback(e.g.,physically-basedsimulations,QED,SAinmoleculegeneration),
ourmethodSVDD-PMcandirectlyutilizesuchfeedbackwithoutconstructingdifferentiableproxy
models. Incaseswherenon-differentiablefeedbackiscostlytoobtain,proxyrewardmodelsmay
stillberequired,buttheydonotneedtobedifferentiable;thus,non-differentiablefeaturesornon-
differentiable models based on scientific knowledge (e.g., molecule fingerprints, GNNs) can be
leveraged. Similarly,whenusingSVDD-MC,whileavaluefunctionmodelisrequired,itdoesnot
needtobedifferentiable,unlikeclassifierguidance.
Additionally,comparedtoapproachesthatinvolvederivatives(likeclassifierguidanceorDPS),our
algorithmmaybemorememoryefficientatinferencetime,particularlywhenM ismoderate,and
canbedirectlyappliedtodiscretediffusionmodelsmentionedinExample2.
Proximity to pre-trained models (robust to reward over-optimization). Since samples are
consistentlygeneratedfrompre-traineddiffusionpoliciesateachstep,theyareensuredtoremain
withinthenaturalspacedefinedbypre-traineddiffusionmodels.
ThisisespeciallyadvantageouswhenrewardstobeoptimizedbySVDDarelearnedfromofflinedata.
Insuchcases,learnedrewardfunctionsmaybeinaccurateforout-of-distributionsamples(Uehara
etal.,2024). Consequently,conventionalfine-tuningmethodsoftensufferfromover-optimization
(rewardhacking)byexploitingtheseout-of-distributionregions(Fanetal.,2023;Clarketal.,2023).
Giventhatnon-naturaldesignspacestypicallyencompassasignificantportionofout-of-distribution
regions of the offline data, maintaining proximity to pre-trained models acts as a regularization
mechanismagainsttheseregions.
5.2 PotentialLimitations
Memoryandcomputationalcomplexityininferencetime. Ourapproachrequiresmorecompu-
tationalresources(ifnotparallelized)ormemory(ifparallelized),specificallyM timesmorethan
standardinferencemethods.
Proximitytopre-trainedmodels. Theproximitytopre-trainedmodelsmightbeadisadvantageif
significantchangestothepre-trainedmodelsaredesired. WeacknowledgethatRL-basedfine-tuning
couldbemoreeffectiveforthispurposethanouralgorithm.
5.3 Extensions
Usingalikelihood/classifierasareward. Whileweprimarilyconsiderscenarioswherereward
models are regression models, by adopting a similar strategy in (Zhao et al., 2024), they can be
readilyreplacedwithclassifiersorlikelihoodfunctionsinthecontextofsolvinginverseproblems
(Chungetal.,2022;Bansaletal.,2023).
CombinationwithsequentialMonteCarlo. SVDDiteratesISandresamplingateachteamstep
locally,butouralgorithmcanbecombinedwiththemoresophisticatedglobalresamplingmethod
knownasparticlefiltermethod(i.e.,sequentialMonteCarlo)(DelMoralandDoucet,2014;Kitagawa,
1993; Doucet et al., 2009). Although we do not recommend this approach in practice due to its
difficultyinparallelization,wediscussitsextensioninSectionA.
Applicationtofine-tuning. OurSVDD,canalsobenaturallyextendedtofine-tuningbygenerating
sampleswithSVDD,andthenusingthesesamplesforsupervisedfine-tuning.
6 Experiments
We conduct experiments to assess the performance of our algorithm relative to baselines and its
sensitivity to various hyperparameters. We start by outlining the experimental setup, including
baselinesandmodels,andthenpresenttheresults.
86.1 Settings
Methodstocompare. Wecomparethefollowingmethods.
• Pre-trainedmodels: Wegeneratesamplesusingpre-trainedmodels.
• Best-of-N(Nakanoetal.,2021): Wegeneratesamplesfrompre-trainedmodelsandselectthe
top1/N samples. Thisselectionismadetoensurethatthecomputationaltimeduringinferenceis
approximatelyequivalenttothatofourproposal.
• DPS(Chungetal.,2022): DPSisawidelyusedenhancementofclassifierguidance. Althoughthe
originalworkwasnotdesignedfordiscretediffusion,weemployspecificheuristicstoadaptitfor
thispurpose(SectionC).
• SVDD(Ours): WeimplementSVDD-MCandSVDD-PM.WegenerallysetM =20forimages
andM =10forotherdomains,additionallywesetα=0.
Datasets and reward models. We provide details on the pre-trained diffusion models and
downstreamrewardfunctionsused. Forfurtherinformation,refertoSectionC.
• Images:WeuseStableDiffusionv1.5asthepre-traineddiffusionmodel(T =50).Fordownstream
rewardfunctions,weusecompressibilityandaestheticscores(LAIONAestheticPredictorV2in
Schuhmann(2022)),asemployedinBlacketal.(2023);Fanetal.(2023). Notethatcompressibility
isnon-differentiablerewardfeedback.
• Molecules: WeuseGDSS(Joetal.,2022),trainedonZINC-250k(IrwinandShoichet,2005),
asthepre-traineddiffusionmodel(T =1000). Fordownstreamrewardfunctions,weuseQED
andSAcalculatedbyRDKit,whicharenon-differentiablefeedback. Here,werenormalizeSAto
(10−SA)/9sothatahighervalueindicatesbetterperformance.
• DNAs(Enhancers): Weusethediscretediffusionmodel(Sahooetal.,2024),trainedondatasets
fromGosaietal.(2023),asourpre-traineddiffusionmodel(T =128). Forthedownstreamreward
function,weuseanEnformermodel(Avsecetal.,2021)topredictactivityintheHepG2cellline.
• RNAs (5’UTRs): We use the discrete diffusion model (Sahoo et al., 2024) as the pre-trained
diffusionmodel(T = 128). Fordownstreamrewardfunctions,weemployarewardmodelthat
predictsthemeanribosomalload(MRL)measuredbypolysomeprofiling(trainedondatasetsfrom
Sampleetal.(2019))andstability(trainedondatasetsfrom(AgarwalandKelley,2022)).
6.2 Results
(a) Images: compress-(b) Images: aesthetic (c)Molecules:QED (d)Molecules:SA
ibility score
(e)Enhancers (f)5’UTRs:MRL (g)5’UTRs:stability
Figure2: Weshowthehistogramofgeneratedsamplesintermsofrewardfunctions.
Wecomparethebaselineswithourtwoproposals. TheperformanceisshowninFigure2,andthe
generatedsamplesarepresentedinFigure3.
9(a)Images:compressibility (b)Images:aestheticscores
(c)Molecules:QEDscores (d) Molecules: SA scores (Normalized as (10 −
SA)/9)
Figure3: Weshowgeneratedsamplesfromourproposal. Formoresamples,refertoSectionC.2.
Overall,ourproposaloutperformsthebaselinemethods(Best-of-NandDPS),asevidencedbyhigher
rewardsforsamplesgeneratedinlargequantities. Moreformally,inSectionC,wecomparethetop
10and20quantilesfromeachalgorithmandconfirmthatSVDDalwaysoutperformsthebaselines.
Thisindicatesthatouralgorithmcangeneratehigh-rewardsamplesthatBest-of-NandDPScannot.
• ComparedtoBest-of-N,althoughtherewardsforgeneratingsamplesinsmallerquantities
couldbelowerwithouralgorithm,thisisexpectedbecauseouralgorithmgeneratessamples
with high likelihood ppre(x|c) in pre-trained diffusion models, but with possibly lower
rewards.
• ComparedtoDPS,ouralgorithmconsistentlyoutperforms.Notably,inmoleculargeneration,
DPSishighlyineffectiveduetothenon-differentiablenatureoftheoriginalfeedback.
Thesuperiorityofourtwoproposals(SVDD-MCorSVDD-PM)appearstobedomain-dependent.
Generally,SVDD-PMmaybemorerobustsinceitdoesnotrequireadditionallearning(i.e.,itdirectly
utilizesrewardfeedback). TheperformanceofSVDD-MCdependsonthesuccessofvaluefunction
learning,whichisdiscussedinSectionC.
AblationstudiesintermsofM. WeplottheperformanceasM
varies. TheperformancegraduallyreachesaplateauasM increases.
Thistendencyisseeninallotherdomains.
7 Conclusion
Weproposeanovelinference-timealgorithm,SVDD,foroptimizing
downstreamrewardfunctionsinpre-traineddiffusionmodelsthat
eliminatetheneedtoconstructdifferentiableproxymodels.Infuture
work, we plan to conduct experiments in other domains, such as
proteinsequenceoptimization(Gruveretal.,2023;Alamdarietal., Figure4: Performanceofour
2023;Watsonetal.,2023)orcontrollable3Dmoleculegeneration algorithmSVDDasM varies,
(Xuetal.,2023). forimagegenerationwhileop-
timizingtheaestheticscore.
10References
Agarwal,V.andD.R.Kelley(2022). Thegeneticandbiochemicaldeterminantsofmrnadegradation
ratesinmammals. Genomebiology23(1),245.
Alamdari,S.,N.Thakkar,R.vandenBerg,A.X.Lu,N.Fusi,A.P.Amini,andK.K.Yang(2023).
Proteingenerationwithevolutionarydiffusion: sequenceisallyouneed. bioRxiv,2023–09.
Alford,R.F.,A.Leaver-Fay,J.R.Jeliazkov,M.J.O’Meara,F.P.DiMaio,H.Park,M.V.Shapovalov,
P.D.Renfrew, V.K.Mulligan, K.Kappel, etal.(2017). Therosettaall-atomenergyfunction
formacromolecularmodelinganddesign. Journalofchemicaltheoryandcomputation13(6),
3031–3048.
Alhossary,A.,S.D.Handoko,Y.Mu,andC.-K.Kwoh(2015). Fast,accurate,andreliablemolecular
dockingwithquickvina2. Bioinformatics31(13),2214–2216.
Asrani,K.H.,J.D.Farelli,M.R.Stahley,R.L.Miller,C.J.Cheng,R.R.Subramanian,andJ.M.
Brown(2018). Optimizationofmrnauntranslatedregionsforimprovedexpressionoftherapeutic
mrna. RNAbiology15(6),756–762.
Austin, J., D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg (2021). Structured denoising
diffusionmodelsindiscretestate-spaces. AdvancesinNeuralInformationProcessingSystems34,
17981–17993.
Avdeyev, P., C.Shi, Y.Tan, K.Dudnyk, andJ.Zhou(2023). Dirichletdiffusionscoremodelfor
biologicalsequencegeneration. arXivpreprintarXiv:2305.10699.
Avsec,Zˇ.,V.Agarwal,D.Visentin,J.R.Ledsam,A.Grabska-Barwinska,K.R.Taylor,Y.Assael,
J.Jumper,P.Kohli,andD.R.Kelley(2021). Effectivegeneexpressionpredictionfromsequence
byintegratinglong-rangeinteractions. Naturemethods18(10),1196–1203.
Bansal,A.,H.-M.Chu,A.Schwarzschild,S.Sengupta,M.Goldblum,J.Geiping,andT.Goldstein
(2023). Universalguidancefordiffusionmodels. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.843–852.
Beirami,A.,A.Agarwal,J.Berant,A.D’Amour,J.Eisenstein,C.Nagpal,andA.T.Suresh(2024).
Theoreticalguaranteesonthebest-of-nalignmentpolicy. arXivpreprintarXiv:2401.01879.
Black, K., M.Janner, Y.Du, I.Kostrikov, andS.Levine(2023). Trainingdiffusionmodelswith
reinforcementlearning. arXivpreprintarXiv:2305.13301.
Campbell, A., J. Benton, V. De Bortoli, T. Rainforth, G. Deligiannidis, and A. Doucet (2022).
Acontinuoustimeframeworkfordiscretedenoisingmodels. AdvancesinNeuralInformation
ProcessingSystems35,28266–28279.
Campbell,A.,J.Yim,R.Barzilay,T.Rainforth,andT.Jaakkola(2024). Generativeflowsondiscrete
state-spaces: Enablingmultimodalflowswithapplicationstoproteinco-design. arXivpreprint
arXiv:2402.04997.
Castillo-Hair,S.M.andG.Seelig(2021). Machinelearningfordesigningnext-generationmrna
therapeutics. AccountsofChemicalResearch55(1),24–34.
Cheng,F.,Y.Wang,Y.Bai,Z.Liang,Q.Mao,D.Liu,X.Wu,andM.Xu(2023). Researchadvances
onthestabilityofmrnavaccines. Viruses15(3),668.
Chorowski, J. and N. Jaitly (2016). Towards better decoding and language model integration in
sequencetosequencemodels. arXivpreprintarXiv:1612.02695.
Chung,H.,J.Kim,M.T.Mccann,M.L.Klasky,andJ.C.Ye(2022). Diffusionposteriorsampling
forgeneralnoisyinverseproblems. arXivpreprintarXiv:2209.14687.
Clark,K.,P.Vicol,K.Swersky,andD.J.Fleet(2023). Directlyfine-tuningdiffusionmodelson
differentiablerewards. arXivpreprintarXiv:2309.17400.
11Dathathri, S., A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu (2019).
Plugandplaylanguagemodels: Asimpleapproachtocontrolledtextgeneration. arXivpreprint
arXiv:1912.02164.
DelMoral,P.andA.Doucet(2014). Particlemethods: Anintroductionwithapplications. InESAIM:
proceedings,Volume44,pp.1–46.EDPSciences.
Dey,R.andF.M.Salem(2017). Gate-variantsofgatedrecurrentunit(gru)neuralnetworks. In2017
IEEE60thinternationalmidwestsymposiumoncircuitsandsystems(MWSCAS),pp.1597–1600.
IEEE.
Dhariwal,P.andA.Nichol(2021). Diffusionmodelsbeatgansonimagesynthesis. Advancesin
neuralinformationprocessingsystems34,8780–8794.
Dong,H.,W.Xiong,D.Goyal,R.Pan,S.Diao,J.Zhang,K.Shum,andT.Zhang(2023).Raft:Reward
rankedfinetuningforgenerativefoundationmodelalignment. arXivpreprintarXiv:2304.06767.
Doucet,A.,A.M.Johansen,etal.(2009). Atutorialonparticlefilteringandsmoothing: Fifteen
yearslater. Handbookofnonlinearfiltering12(656-704),3.
Emonts,J.andJ.F.Buyel(2023). Anoverviewofdescriptorstocaptureproteinproperties–tools
andperspectivesinthecontextofqsarmodeling. ComputationalandStructuralBiotechnology
Journal21,3234–3247.
Fan,Y.,O.Watkins,Y.Du,H.Liu,M.Ryu,C.Boutilier,P.Abbeel,M.Ghavamzadeh,K.Lee,and
K.Lee(2023). DPOK:Reinforcementlearningforfine-tuningtext-to-imagediffusionmodels.
arXivpreprintarXiv:2305.16381.
Gao,L.,J.Schulman,andJ.Hilton(2023). Scalinglawsforrewardmodeloveroptimization. In
InternationalConferenceonMachineLearning,pp.10835–10866.PMLR.
Geist,M.,B.Scherrer,andO.Pietquin(2019). Atheoryofregularizedmarkovdecisionprocesses.
InInternationalConferenceonMachineLearning,pp.2160–2169.PMLR.
Ghiringhelli, L.M., J.Vybiral, S.V.Levchenko, C.Draxl, andM.Scheffler(2015). Bigdataof
materialsscience: criticalroleofthedescriptor. Physicalreviewletters114(10),105503.
Gosai,S.J.,R.I.Castro,N.Fuentes,J.C.Butts,S.Kales,R.R.Noche,K.Mouri,P.C.Sabeti,S.K.
Reilly,andR.Tewhey(2023). Machine-guideddesignofsyntheticcelltype-specificcis-regulatory
elements. bioRxiv.
Gruver, N., S. Stanton, N. C. Frey, T. G. Rudner, I. Hotzel, J. Lafrance-Vanasse, A. Rajpal,
K.Cho,andA.G.Wilson(2023). Proteindesignwithguideddiscretediffusion. arXivpreprint
arXiv:2305.20009.
Guo,Y.,H.Yuan,Y.Yang,M.Chen,andM.Wang(2024). Gradientguidancefordiffusionmodels:
Anoptimizationperspective. arXivpreprintarXiv:2404.14743.
Haarnoja,T.,H.Tang,P.Abbeel,andS.Levine(2017). Reinforcementlearningwithdeepenergy-
basedpolicies. InInternationalconferenceonmachinelearning,pp.1352–1361.PMLR.
Han,S.,I.Shenfeld,A.Srivastava,Y.Kim,andP.Agrawal(2024). Valueaugmentedsamplingfor
languagemodelalignmentandpersonalization. arXivpreprintarXiv:2405.06639.
Hayes,T.,R.Rao,H.Akin,N.J.Sofroniew,D.Oktay,Z.Lin,R.Verkuil,V.Q.Tran,J.Deaton,
M. Wiggert, et al. (2024). Simulating 500 million years of evolution with a language model.
bioRxiv,2024–07.
Ho,J.,A.Jain,andP.Abbeel(2020). Denoisingdiffusionprobabilisticmodels. Advancesinneural
informationprocessingsystems33,6840–6851.
Ho,J.andT.Salimans(2022). Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598.
Ho,J.,T.Salimans,A.Gritsenko,W.Chan,M.Norouzi,andD.J.Fleet(2022). Videodiffusion
models. AdvancesinNeuralInformationProcessingSystems35,8633–8646.
12Irwin,J.J.andB.K.Shoichet(2005). ZINC-afreedatabaseofcommerciallyavailablecompounds
forvirtualscreening. Journalofchemicalinformationandmodeling45(1),177–182.
Jin,W.,R.Barzilay,andT.Jaakkola(2018). Junctiontreevariationalautoencoderformolecular
graphgeneration. InInternationalconferenceonmachinelearning,pp.2323–2332.PMLR.
Jo,J.,S.Lee,andS.J.Hwang(2022). Score-basedgenerativemodelingofgraphsviathesystemof
stochasticdifferentialequations. InInternationalConferenceonMachineLearning,pp.10362–
10383.PMLR.
Kitagawa,G.(1993). Amontecarlofilteringandsmoothingmethodfornon-gaussiannonlinearstate
spacemodels. InProceedingsofthe2ndUS-Japanjointseminaronstatisticaltimeseriesanalysis,
Volume110.
Lal,A.,D.Garfield,T.Biancalani,andG.Eraslan(2024). reglm: Designingrealisticregulatorydna
withautoregressivelanguagemodels. bioRxiv,2024–02.
Leblond, R., J.-B. Alayrac, L. Sifre, M. Pislar, J.-B. Lespiau, I. Antonoglou, K. Simonyan,
and O. Vinyals (2021). Machine translation decoding beyond beam search. arXiv preprint
arXiv:2104.05336.
Lee,S.,J.Jo,andS.J.Hwang(2023). Exploringchemicalspacewithscore-basedout-of-distribution
generation. InInternationalConferenceonMachineLearning,pp.18872–18892.PMLR.
Levine,S.(2018). Reinforcementlearningandcontrolasprobabilisticinference: Tutorialandreview.
arXivpreprintarXiv:1805.00909.
Lew,A.K.,T.Zhi-Xuan,G.Grand,andV.K.Mansinghka(2023). Sequentialmontecarlosteering
oflargelanguagemodelsusingprobabilisticprograms. arXivpreprintarXiv:2306.03081.
Lou,A.,C.Meng,andS.Ermon(2023). Discretediffusionlanguagemodelingbyestimatingthe
ratiosofthedatadistribution. arXivpreprintarXiv:2310.16834.
Mudgal, S., J. Lee, H. Ganapathy, Y. Li, T. Wang, Y. Huang, Z. Chen, H.-T. Cheng, M. Collins,
T. Strohman, et al. (2023). Controlled decoding from language models. arXiv preprint
arXiv:2310.17022.
Nakano,R.,J.Hilton,S.Balaji,J.Wu,L.Ouyang,C.Kim,C.Hesse,S.Jain,V.Kosaraju,W.Saunders,
etal.(2021). Webgpt: Browser-assistedquestion-answeringwithhumanfeedback. arXivpreprint
arXiv:2112.09332.
Nisonoff,H.,J.Xiong,S.Allenspach,andJ.Listgarten(2024). Unlockingguidancefordiscrete
state-spacediffusionandflowmodels. arXivpreprintarXiv:2406.01572.
Prabhudesai,M.,A.Goyal,D.Pathak,andK.Fragkiadaki(2023). Aligningtext-to-imagediffusion
modelswithrewardbackpropagation. arXivpreprintarXiv:2310.03739.
Qin,L.,S.Welleck,D.Khashabi,andY.Choi(2022). Colddecoding: Energy-basedconstrained
textgenerationwithlangevindynamics. AdvancesinNeuralInformationProcessingSystems35,
9538–9551.
Sahoo,S.S.,M.Arriola,Y.Schiff,A.Gokaslan,E.Marroquin,J.T.Chiu,A.Rush,andV.Kuleshov
(2024). Simpleandeffectivemaskeddiffusionlanguagemodels. arXivpreprintarXiv:2406.07524.
Sample, P. J., B. Wang, D. W. Reid, V. Presnyak, I. J. McFadyen, D. R. Morris, and G. Seelig
(2019). Human5’utrdesignandvarianteffectpredictionfromamassivelyparalleltranslation
assay. Naturebiotechnology37(7),803–809.
Sarkar,A.,Z.Tang,C.Zhao,andP.Koo(2024). Designingdnawithtunableregulatoryactivityusing
discretediffusion. bioRxiv,2024–05.
Schuhmann,C.(2022,Aug). LAIONaesthetics.
Shi,J.,K.Han,Z.Wang,A.Doucet,andM.K.Titsias(2024). Simplifiedandgeneralizedmasked
diffusionfordiscretedata. arXivpreprintarXiv:2406.04329.
13Shi,Y.,V.DeBortoli,A.Campbell,andA.Doucet(2024). Diffusionschro¨dingerbridgematching.
AdvancesinNeuralInformationProcessingSystems36.
Sohl-Dickstein, J., E. Weiss, N. Maheswaranathan, and S. Ganguli (2015). Deep unsupervised
learningusingnonequilibriumthermodynamics. InInternationalconferenceonmachinelearning,
pp.2256–2265.PMLR.
Song, J., C. Meng, and S. Ermon (2020). Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502.
Song,Y.,J.Sohl-Dickstein,D.P.Kingma,A.Kumar,S.Ermon,andB.Poole(2020). Score-based
generativemodelingthroughstochasticdifferentialequations. arXivpreprintarXiv:2011.13456.
Stark,H.,B.Jing,C.Wang,G.Corso,B.Berger,R.Barzilay,andT.Jaakkola(2024). Dirichletflow
matchingwithapplicationstodnasequencedesign. arXivpreprintarXiv:2402.05841.
Stiennon,N.,L.Ouyang,J.Wu,D.Ziegler,R.Lowe,C.Voss, A.Radford,D.Amodei, andP.F.
Christiano(2020). Learningtosummarizewithhumanfeedback. AdvancesinNeuralInformation
ProcessingSystems33,3008–3021.
Taskiran,I.I.,K.I.Spanier,H.Dickma¨nken,N.Kempynck,A.Pancˇ´ıkova´,E.C.Eks¸i,G.Hulselmans,
J. N. Ismail, K. Theunis, R. Vandepoel, et al. (2024). Cell-type-directed design of synthetic
enhancers. Nature626(7997),212–220.
Touvron, H., L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,
P.Bhargava, S.Bhosale, etal.(2023). Llama2: Openfoundationandfine-tunedchatmodels.
arXivpreprintarXiv:2307.09288.
Trott, O. and A. J. Olson (2010). Autodock vina: improving the speed and accuracy of docking
withanewscoringfunction,efficientoptimization,andmultithreading. Journalofcomputational
chemistry31(2),455–461.
Uehara,M.,Y.Zhao,T.Biancalani,andS.Levine(2024). Understandingreinforcementlearning-
basedfine-tuningofdiffusionmodels: Atutorialandreview. arXivpreprintarXiv:2407.13734.
Uehara,M.,Y.Zhao,K.Black,E.Hajiramezanali,G.Scalia,N.L.Diamant,A.M.Tseng,T.Bian-
calani, and S. Levine (2024). Fine-tuning of continuous-time diffusion models as entropy-
regularizedcontrol. arXivpreprintarXiv:2402.15194.
Uehara,M.,Y.Zhao,E.Hajiramezanali,G.Scalia,G.Eraslan,A.Lal,S.Levine,andT.Biancalani
(2024). Bridgingmodel-basedoptimizationandgenerativemodelingviaconservativefine-tuning
ofdiffusionmodels. arXivpreprintarXiv:2405.19673.
vanWesten,G.J.,R.F.Swier,I.Cortes-Ciriano,J.K.Wegner,J.P.Overington,A.P.IJzerman,H.W.
vanVlijmen,andA.Bender(2013). Benchmarkingofproteindescriptorsetsinproteochemo-
metric modeling (part 2): modeling performance of 13 amino acid descriptor sets. Journal of
cheminformatics5,1–20.
Vignac,C.,I.Krawczuk,A.Siraudin,B.Wang,V.Cevher,andP.Frossard(2022). Digress: Discrete
denoisingdiffusionforgraphgeneration. arXivpreprintarXiv:2209.14734.
Wang,Y.,J.Yu,andJ.Zhang(2022).Zero-shotimagerestorationusingdenoisingdiffusionnull-space
model. arXivpreprintarXiv:2212.00490.
Watson,J.L.,D.Juergens,N.R.Bennett,B.L.Trippe,J.Yim,H.E.Eisenach,W.Ahern,A.J.Borst,
R.J.Ragotte,L.F.Milles,etal.(2023). Denovodesignofproteinstructureandfunctionwith
rfdiffusion. Nature620(7976),1089–1100.
Wu, Y., M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K.Macherey,etal.(2016). Google’sneuralmachinetranslationsystem: Bridgingthegapbetween
humanandmachinetranslation. arXivpreprintarXiv:1609.08144.
Xu,K.,W.Hu,J.Leskovec,andS.Jegelka(2018). Howpowerfularegraphneuralnetworks? arXiv
preprintarXiv:1810.00826.
14Xu,M.,A.S.Powers,R.O.Dror,S.Ermon,andJ.Leskovec(2023). Geometriclatentdiffusion
modelsfor3dmoleculegeneration. InInternationalConferenceonMachineLearning,pp.38592–
38610.PMLR.
Yang,K.andD.Klein(2021). Fudge: Controlledtextgenerationwithfuturediscriminators. arXiv
preprintarXiv:2104.05218.
Yu,J.,Y.Wang,C.Zhao,B.Ghanem,andJ.Zhang(2023). Freedom: Training-freeenergy-guided
conditional diffusion model. In Proceedings of the IEEE/CVF International Conference on
ComputerVision,pp.23174–23184.
Zhao,S.,R.Brekelmans,A.Makhzani,andR.Grosse(2024). Probabilisticinferenceinlanguage
modelsviatwistedsequentialmontecarlo. arXivpreprintarXiv:2404.17546.
Zhao,Y.,M.Uehara,G.Scalia,T.Biancalani,S.Levine,andE.Hajiramezanali(2024). Addingcon-
ditionalcontroltodiffusionmodelswithreinforcementlearning. arXivpreprintarXiv:2406.12120.
15Algorithm4DecodingviaSequentialMonteCarloinDiffusionModels
1: Require: Estimatedvaluefunctions{vˆ t(x)}0 t=T,pre-traineddiffusionmodels{pp tre}0 t=T,hyper-
parameterα∈R,BatchsizeN
2: fort∈[T +1,··· ,0]do
3: ISstep:
4:
exp(vˆ (x[i] )/α)
i∈[1,··· ,N];x[i] ∼ppre(·|x[i]),w[i] := t−1 t−1
t−1 t−1 t t−1 exp(vˆ(x[i])/α)
t t
5: Selectionstep: selectnewindiceswithreplacement
(cid:32)(cid:26) (cid:27)N (cid:33)
6: {x[i] }N ←{xζ t[i −] 1}N , {ζ[i] }N ∼Cat w t[i −] 1
t−1 i=1 t−1 i=1 t−1 i=1 (cid:80)N w[j]
j=1 t−1 i=1
7: endfor
8: Output: x 0
A SoftValue-BasedDecodingwithParticleFilter
Inthissection,weexplainsoft-valuedecodingincorporatingparticlefiltering(Doucetetal.,2009).
However,wegenerallydonotrecommendthisapproachinpracticeduetoitsdifficultyinparalleliza-
tion.
ThecompletealgorithmissummarizedinAlgorithm4. Here,weprovideabriefoverview. Itconsists
oftwosteps. Sinceouralgorithmisiterative,attimepointt,considerwehaveN samples(particles)
{x[i]}i=1N.
t
ISstep(line3). Wegenerateasetofsamples{x[i] }N followingapolicyfromapre-trained
t−1 i=1
modelppre(·|·). Inotherwords,
t−1
∀i∈[1,··· ,N];x[i] ∼ppre(·|x[i]).
t−1 t−1 t
Now, wedenotetheimportanceweightforthenextparticlex giventhecurrentparticlex as
t−1 t
w(x ,x ),expressedas
t−1 t
exp(v (x )/α) exp(v (x )/α)
w(x t−1,x t):= (cid:82)
exp(v (x
)t− /α1 )pt p− re1
(x |x )dx
= expt (− v1 (xt− )/1
α)
,
t−1 t−1 t−1 t−1 t t−1 t t
anddefine
∀i∈[1,··· ,N]; w[i] :=w(x[i] ,x[i]).
t−1 t−1 t
NoteherewehaveusedthesoftBellmanequation:
(cid:90)
exp(v (x )/α)= exp(v (x )/α)ppre(x |x )dx .
t t t−1 t−1 t−1 t−1 t t−1
Hence,bydenotingthetargetmarginaldistributionatt−1,wehavethefollowingapproximation:
(cid:88)N w[i]
ptar ≈ t−1 δ .
t−1 (cid:124)(cid:123)(cid:122)(cid:125) (cid:80)N w[j] x[ ti −] 1
IS i=1 j=1 t−1
Selection step (line 5). Finally, we consider a resampling step. The resampling indices are
determinedbythefollowing:
(cid:40) w[i] (cid:41)N 
{ζ t[i −] 1}N
i=1
∼Cat
(cid:80)N
t− w1
[j]
.
j=1 t−1 i=1
Tosummarize,weconduct
(cid:88)N w[i]
1
(cid:88)N
ptar ≈ t−1 δ ≈ δ .
t−1 (cid:124)(cid:123) IS(cid:122)(cid:125)
i=1
(cid:80)N j=1w t[j −]
1
x[ ti −] 1 Re(cid:124) sa(cid:123) m(cid:122) p(cid:125) lingN
i=1
xζ t−t[i − 1] 1
Remark4(Worksinautoregressivemodels). Wenotethatinthecontextofautoregressive(language)
models,Zhaoetal.(2024);Lewetal.(2023)proposedasimilaralgorithm.
16Algorithm5ValueFunctionEstimationUsingSoftQ-learning
1: Require: Pre-traineddiffusionmodels{ppre}0 ,valuefunctionmodelv(x;θ)
t t=T
2: Collectdatasets{x(s),··· ,x(s)}S byrolling-out{ppre}0 fromt=T tot=0.
T 0 s=1 t t=T
3: forj ∈[0,··· ,J]do
4: Updateθbyrunningregression:
T S
θ′
←argmin(cid:88)(cid:88)(cid:110)
v(x(s);θ)−v(x(s) ;θ′
)(cid:111)2
.
j t t−1 j−1
θ
t=0s=1
5: endfor
6: Output: v(x;θ′)
J
B SoftQ-learning
Inthissection,weexplainsoftvalueiterationtoestimatesoftvaluefunctions,whichservesasan
alternativetoMonteCarloregression.
SoftBellmanequation. Here,weusethesoftBellmanequation:
(cid:90)
exp(v (x )/α)= exp(v (x )/α)ppre(x |x )dx ,
t t t−1 t−1 t−1 t−1 t t−1
asprovedinSection4.1in(Ueharaetal.,2024). Inotherwords,
v (x )=αlog{E [exp(v (x )/α)|x ]}.
t t xt−1∼ppre(·|xt) t−1 t−1 t
Algorithm. Basedontheabove,wecanestimatesoftvaluefunctionsrecursivelybyregressing
v (x ) onto x . This approach is often referred to as soft Q-learning in the reinforcement
t−1 t−1 t
learningliterature(Haarnojaetal.,2017;Levine,2018).
Inourcontext,duetotheconcernofscalingofα,aswehavedoneinAlgorithm2,wehadbetteruse
v (x )=E [v (x )|x ].
t t xt−1∼ppre(·|xt) t−1 t−1 t
Withtheaboverecursiveequation,wecanestimatesoftvaluefunctionsasinAlgorithm5.
C AdditionalExperimentalDetails
Wefurtheraddadditionalexperimentaldetails.
C.1 AdditionalSetupsforExperiments
Images.
• DPS:Werequiredifferentiablemodelsthatmapimagestocompressibility. Forthistask,we
haveusedastandardCNN.
Molecules.
• DPS:FollowingtheimplementationinLeeetal.(2023),weusethesameGNNmodelasthe
rewardmodel. Notethatthismodelcannotcomputederivativeswithrespecttoadjacency
matrices.
• SVDD-MC: We use a Graph Isomorphism Network (GIN) model (Xu et al., 2018) as a
valuefunctionmodel.
Enhancers.
• DPS: Although DPS was originally proposed in the continuous space, we have adapted
itforourusebyincorporatingthegradientofthevaluefunctionmodelateachstepand
representingeachsequenceasaone-hotencodingvector.
• SVDD-MC:WehaveusedtheEnformermodelasthefunctionmodel.
175’UTRs. WehaveusedConvGRUastherewardmodel(DeyandSalem,2017).
• DPS: Although DPS was originally proposed in the continuous space, we have adapted
itforourusebyincorporatingthegradientofthevaluefunctionmodelateachstepand
representingeachsequenceasaone-hotencodingvector.
• SVDD-MC:WeemployedConvGRUasthevaluefunctionmodel.
C.2 AdditionalResults
We evaluate the performance of the generated samples using histograms in Section 6. Here, we
presentthetop10and20quantilesofthegeneratedsamplesinTable2.
Table2: Top10and20quantilesofthegeneratedsamplesforeachalgorithm. Higherisbetter.
Domain Quantile Pre-Train Best-N DPS SVDD-MC SVDD-PM
Imagescompress 20% -86.2 -63.2 -67.1 - -43.7
10% -78.6 -57.3 -61.2 - -38.8
Imagesaesthetic 20% 5.875 6.246 5.868 - 6.356
10% 5.984 6.343 5.997 - 6.472
MoleculesQED 20% 0.771 0.881 0.802 0.912 0.916
10% 0.812 0.902 0.843 0.925 0.928
MoleculesSA 20% 0.750 0.916 0.802 1.0 1.0
10% 0.803 0.941 0.849 1.0 1.0
Enhancers 20% 0.74 3.00 2.68 5.53 6.44
10% 1.41 3.52 3.85 5.75 7.02
5’UTRMRL 20% 0.78 0.97 0.90 1.09 1.33
10% 0.86 1.021 0.93 1.12 1.38
5’UTRStability 20% -0.63 -0.59 -0.62 -0.52 -0.56
10% -0.61 -0.58 -0.60 -0.51 -0.55
Performanceofvaluefunctiontraining. Wereporttheperformanceofvaluefunctionlearning
usingMonteCarloregressionasfollows. WeplotthePearsoncorrelationonthetestdataset.
(a)Images: compressibil-(b)Images:aestheticscore (c)Molecules:QED
ity
Figure5: Trainingcurve
Moregeneratedsamples. WehaveprovidedadditionalgeneratedsamplesinFigure6,Figure7,
Figure8andFigure9.
18Pre-trained
-103 -112 -155 -97 -103 -130
Best-N
-79 -74 -81 -61 -68 -73
SVDD-PM
-37 -39 -20 -39 -31 -43
Figure6: Additionalgeneratedsamples(Domain: images,Reward: Compressibility)
Pre-trained
5.7 5.5 5.5 5.6 5.3 5.6
Best-N
6.1 6.0 6.1 5.8 5.7 5.9
SVDD-PM
6.5 6.5 6.5 6.6 6.9 6.5
Figure7: Additionalgeneratedsamples(Domain: Images,Reward: Aestheticscore)
19Figure8: Additionalgeneratedsamples(Domain: Molecules,Reward: QEDscore)
Figure 9: Additional generated samples (Domain: Molecules, Reward: SA score, normalized as
(10−SA)/9)
20Figure10: AdditionalgeneratedsamplesfromSVDD(Domain: Molecules,Reward: SAscore=1.0
(normalizedas(10−SA)/9))
21