HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals
Through Contrastive Learning
Hongyu Li†,‡, Snehal Dikhale†, Jinda Cui†, Soshi Iba†, and Nawid Jamali†
Abstract—To achieve dexterity comparable to that of hu-
mans, robots must intelligently process tactile sensor data.
Taxel-based tactile signals often have low spatial-resolution,
with non-standardized representations. In this paper, we
propose a novel framework, HyperTaxel, for learning a
geometrically-informed representation of taxel-based tactile
signals to address challenges associated with their spatial
resolution.Weusethisrepresentationandacontrastivelearning (a) Master Chef Can (b) Sugar Box
objectivetoencodeandmapsparselow-resolutiontaxelsignals
to high-resolution contact surfaces. To address the uncertainty
inherent in these signals, we leverage joint probability distri-
butions across multiple simultaneous contacts to improve taxel
hyper-resolution.Weevaluateourrepresentationbycomparing
itwithtwobaselinesandpresentresultsthatsuggestourrepre-
sentation outperforms the baselines. Furthermore, we present
qualitative results that demonstrate the learned representation (c) Mustard Bottle (d) Power Drill
captures the geometric features of the contact surface, such as
Fig. 1: A visualization of tactile embeddings for different
flatness, curvature, and edges, and generalizes across different
objectsandsensorconfigurations.Moreover,wepresentresults YCB [19] objects. The embeddings capture the geometric
that suggest our representation improves the performance of features of the contact surface, such as flatness, curvature,
various downstream tasks, such as surface classification, 6D and edges, and are consistent across different objects.
in-hand pose estimation, and sim-to-real transfer.
I. INTRODUCTION
perception, such as in-hand 6D pose estimation [16–18].
Tactile sensing is a critical modality for humans to in- As suggested by Dahiya et al. [15], one promising line of
teract with everyday objects [1]. Tactile sensors can be research is the use of super-resolution algorithms.
divided into two broad categories [2]: vision-based [3, 4], In this paper, we present a two-stage solution to address
and taxel-based [5–7]. Recently, vision-based tactile sensors theaforementionedchallenges.Inthefirststage,wepropose
have gained popularity, partly due to their pixel-based rep- a method for learning a representation of the tactile signals
resentation, which makes them amenable to deep learning in an embedding space using contrastive learning. This
approaches [8–10]. However, their size limits full coverage approach generalizes across various taxel layouts, different
on multi-fingered hands [11, 12]. In contrast, taxel-based objects, and multiple tasks. Our key intuition is that by
sensors remain underexplored because they present many exploiting the correspondence between the taxel signals
challengestodeeplearningapproaches,includinglowspatial and their contact surface, we can learn a geometrically-
resolution and a lack of consensus on how to represent informativerepresentation.Tothisend,weproposegraphsto
and process taxel-based sensors. However, they continue to represent the tactile signals, with a novel graph construction
remainofinteresttotheroboticmanipulationcommunitydue strategy and convolution kernel for tactile processing.
to their unique ability to directly respond to the underlying In the second stage, we map low-resolution taxel signals
phenomena measured, thereby offering valuable opportuni- toahigh-resolutionthree-dimensionalsurfaceusingamulti-
ties for enhancing robotic manipulation [13, 14]. contact strategy to reduce uncertainty in taxel signals, a
The encoding and processing of taxel signals is still an process we term hyper-resolution. Unlike super-resolution,
open research question. Taxel-based sensors present unique which focuses on upscaling data within the same domain,
challenges before they can be used in downstream tasks. hyper-resolution extends beyond mere upscaling within the
These challenges include 1) the development of effective same domain across different domains and modalities. For
representationsfortactilesensordata,and2)theirinherently example, in image processing, super-resolution produces
low resolution [2, 12, 15], which has been a long-standing the same image with a higher resolution. However, hyper-
barrier hindering tactile dexterous manipulation [15] and resolution maps low-resolution taxel signals to capture vari-
ous object properties, such as the three-dimensional surface
†HondaResearchInstituteUSA.{snehalsubhashdikhale,jindacui,siba,
of the object, surface texture, etc. This distinction allows
njamali}@honda-ri.com.
hyper-resolution to provide informative data beneficial for
‡ BrownUniversity.ThisworkwasdoneduringhisinternshipatHonda
ResearchInstituteUSA.hongyu@brown.edu. tasks such as 6D pose estimation.
4202
guA
51
]OR.sc[
1v21380.8042:viXraOur contributions can be summarized as follows: 1) We
propose a novel taxel encoder. 2) We introduce a novel
representationlearningapproachforsignalsfromtaxel-based ...
tactile sensors. 3) We propose a novel hyper-resolution
algorithmfortaxel-basedtactilesensors,whichleveragesthe
proposed taxel encoder.
We present results of qualitative analysis that suggest
our tactile representation captures geometric features of the
contact surface, such as flatness, curvature, and edges, and
generalizes across different objects and sensor configura-
tions. Furthermore, we benchmark our proposed framework
against two seminal taxel encoders and present quantitative
Graph
results that demonstrate the effectiveness of our approach. Construction
We also perform a comparative analysis across different
representations and confirm the effectiveness of the graph
representation. We also assess the quality of our hyper-
resolution using 6D object pose estimation and show that
using our hyper-resolved data improves performance. In the
end, we verify the sim-to-real transferability of our learned Fig. 2: Overview of our proposed tactile representation
tactilerepresentationonthesurfaceclassificationtaskonthe learning framework. The tactile signal, left blue box, is
real robot. representedasagraphandencodedusingtactileencoderE ,
t
and the corresponding contact surface patch, top red box, is
II. RELATEDWORK
encoded using surface encoder E .
s
Representation learning is the process of encoding infor-
mativefeaturesfromrawdatatomakeitsuitableformachine
learning tasks. Most of the earlier works use supervised method. Recently, there has been a shift from probabilistic-
learning methods [20]. Recently, there has been a growing based approaches to learning-based ones. Wu et al. [2]
interest in self-supervised learning [21–23] and multi-modal propose a method wherein the taxel-based tactile signal is
learning [24]. While most of the prior studies focus on interpreted as a 2D image, subsequently enhanced using
domains such as vision [21–23] and language [24–26], in SRGAN [37]. This process results in a higher-resolution
this paper, we are interested in whether the same paradigm representation of the contact surface between the sensor and
can be applied in the tactile domain. the object. However, interpreting tactile signals as a 2D
Severalpriorworkshaveinvestigatedtactilerepresentation image limits their application to 2D arrangements; the 3D
learning.Intheimage-basedsensordomain,Villalongaetal. arrangement found in curved fingers cannot be accurately
[27] leverage the contrastive framework MoCo [22], and represented. Moreover, they do not utilize the geometric
Caddeo et al. [28] utilize an autoencoder to learn a repre- information of the object in contact. In this paper, we
sentation.However,theirtransferfromimage-basedtotaxel- present a geometrically-informed hyper-resolution algorithm
baseddataisnon-trivial.Guzeyetal.[14]learnarepresenta- invariant to sensor arrangements.
tionfortaxel-basedsensorsusingBYOL[21].However,their
primary focus is on dexterous manipulation, and they do not
III. METHODOLOGY
explore various downstream tasks or representation learning
approaches.Therefore,themosteffectiveparadigmfortaxel- Given a sparse taxel-based tactile signal, our goal is to
based signals remains a topic for further exploration. obtain a high-resolution depiction of the contact surface
Contact localization has been utilized to achieve tactile between the sensor and the object of interest. To achieve
super-resolution. In contact localization, the goal is to es- this, we propose a two-stage solution. The first stage, rep-
timate the contact location from a given tactile observa- resentation learning, involves using a graph neural network
tion. Early works use probabilistic approaches to estimate and contrastive learning to learn a geometrically-informed
the probability distribution of the contact location [29–32]. representationofthetactilesignals.Thesecondstage,hyper-
Piacenza et al. [12] adopt a data-driven approach for contact resolution, uses the learned representation to map low-
localizationin3Dspace.Withtheadvancementsincomputer resolutiontaxelsignalsintoahigh-resolutioncontactsurface
vision,recentworkshaveincreasinglyexploredvision-based using multi-contact localization.
tactile sensors [27, 28, 30, 33–35], which lend themselves
well to deep learning techniques due to their pixel-based
A. Representation Learning
output.However,thefusionoftaxel-basedsensorswithdeep
learning methodologies remains relatively underexplored. Figure 2 shows an overview of the proposed tactile repre-
In the seminal work Lepora et al. [36] propose taxel- sentation learning framework. In this section, we detail the
based tactile super-resolution using the Bayesian perception different components of our framework.
...1) TaxelRepresentaion: Tactiledatacanberepresentedas height and width are set to the sensor’s dimensions, and its
point clouds, images, or graphs. The point cloud represen- depth,δ ,representsthepenetrationintotheobject’ssurface.
p
tation, however, fails to encode the absence of contact, and Inourexperiments,wesetδ =0.8cm,aslowervaluesofδ
p p
the image representation struggles to capture the 3D spatial increasedtheriskofmeshcollisionduringinitialization.The
arrangement of tactile sensors. In our research, we opted for intersectionbetweentheobjectOandthiscubeisreferredto
the graph representation because it encodes both the spatial asacontactsurfacepatch.Thecontactsurfacepatchcaptures
arrangement of the taxel signals and the absence of contact. thelocalgeometryoftheobjectatthecontactpoint,andcan
WeuseanundirectedspatialgraphG =(V,E)torepresent beusedtolearnacorrespondencebetweenthelow-resolution
the taxel data, where V and E are the vertices and edges of tactile signal and the high-resolution object surface shape.
the graph, respectively. Each vertex corresponds to a taxel We view the contact surface patch as the hyper-resolution
of the tactile sensor, and each edge represents the spatial space of that respective tactile sensor.
proximity between two taxels. The vertices have two types
4) Learning the Tactile Representation: We propose the
of features: the 3D coordinates of each taxel, X ∈ Rt×3,
contrastive learning framework shown in Fig. 2 to learn the
and their corresponding signals, K ∈ Rt, where t is the
tactile representation that leverages the inherent relationship
number of taxels in the tactile sensors. For taxel signals
between the contact surface and the tactile signals. For
with three axes, K is considered the Euclidean norm of the
example,whenthesensorispressedagainstaflatsurface,the
signals from all three axes. We combine X and K into a
taxel signals should demonstrate the flatness feature. On the
matrix V = [X|K] of dimension Rt×4. To improve sim-
contrary,whenthesensorispressedagainstacurvedsurface,
to-real transfer, we simplify the taxel signal into a boolean
the signals should demonstrate the curvature feature and
activation state [38, 39].
distinguish itself from the flatness feature. Our key insight
EdgesE connectthetaxelverticesV inthetaxelgraphand
for learning an effective representation of tactile signals is
construct the graph. Since we are considering tactile sensors
to exploit this correspondence.
with arbitrary spatial arrangements, we propose that tactile
Inspired by the vision-language learning framework
message passing should be relative to the spatial distance.
CLIP[24],wedrawN randompairsoftactilesensorsignals
We use a radius graph to construct E. In this graph, an edge
(the blue box) and corresponding contact surfaces (the red
connects two vertices if their distance is within a certain
box) for each data sample. While the CLIP framework is
radius. This approach ensures that the interaction between
typically used for visual-language tasks, we adapt it for
sensors is stronger when they are closer to each other.
tactile representation learning. This adaptation requires two
2) Tactile Encoder E : We process the constructed taxel
t
significant modifications to the original formulation. 1) Due
graph using a graph neural network (GNN). Specifically, the
to the lack of an existing dataset for taxel-based tactile
taxelgraphpassesthroughthreemessagepassinglayers[40],
sensors,weneedtocollecttherequiredpaireddata.2)Since
a pooling layer, and a non-linear layer output head. The
theoriginalencoders(visionandlanguage)areincompatible
message-passing layer is defined as
withtaxelsignals,weneedtodesignaneuralnetworkmodel
(cid:77)
n′ =γ(n , ϕ(n ,n ,e )), (1) to encode taxel signals.
i i i j j,i
j∈N(i) The tactile graph described in Section III-A.1 is passed to
where n i is the vertex feature, and e j,i is the edge feature thetactileencoder E t (Sec.III-A.2)and encodedintotactile
between vertices i and j. (cid:76) is a differentiable ag- embeddingT ∈Rn,wherenrepresentstheembeddingsize.
j∈N(i)
gregation function, such as maximum or summation, and γ Second, the contact surface data (red box), represented as
and ϕ are two differentiable functions such as MLPs. Our a point cloud, is encoded through the surface encoder E s
observation is that the taxel signals rely on relative features into surface embedding S ∈Rn, which has the same size as
with respect to their neighbors instead of absolute features. the tactile embedding Rn. We choose PointNet [42] as our
For example, a 4×4 taxel pad with evenly high activation surface encoder. The embedding size n is empirically set as
signalsandevenlylowactivationsignalsshouldrepresentthe 128.
samecontactsurface(flatsurface).Therefore,weproposeto Thefinalstepinvolveslearningtactilerepresentation.This
usetheEdgeConvoperator[41],whichleveragestherelative is achieved by computing the dot product of the tactile
features between vertices n and n embedding and surface embedding T · S⊺, resulting in a
i j
(cid:88) N × N matrix as depicted in the bottom right corner of
n′ = (ϕ(x ,h ,x −x ,h −h )), (2)
i i i j i j i Fig. 2. This matrix contains N positive pairs and N2 −N
j∈N(i) negative pairs. The dot product operation measures the
where h and h are the hidden features of node i and j. cosine similarity between the tactile embedding and surface
i j
3) Sensor-Object Contact Surface Representation: To embedding. We optimize both encoders using a symmetric
map the low-resolution tactile signals to the high-resolution cross-entropy loss [24] such that the N ×N matrix turns
surface shape, we need to represent the contact surface into an identity matrix IN×N. By doing so, we learn a
between the sensor and the object. To this end, we represent representation that brings matching pairs closer together and
thesensor-objectcontactsurfaceasacubeencapsulatingthe pushes non-matching pairs farther apart in the embedding
object’ssurfacethatisincontactwiththesensor.Thecube’s space.Fig. 4: Illustration of data collection using NVIDIA Isaac
Sim simulator. The figure shows the curved fingertip sensor
(left) and the 4×4 flat pad sensor (right), respectively.
a distribution of contact locations Ω ⊆P [28].
i c
Foranytwosensorsincontactj ,j ∈J,weobtaintheir
a b
Fig.3:Avisualillustrationofmulti-contactlocalization.The respectivedistributionΩ a,Ω bforcontactlocationcandidates.
first column shows the contact locations. The second and We filter the pair-wise Euclidean distance between each
third columns show the likelihood map of contact location candidate location p c,a ∈ Ω a,p c,b ∈ Ω b using the actual
using single-, and multi-contact reasoning, respectively. sensor poses P a,P b
Ω ={p |∥p −p ∥−∥P −P ∥≤δ }
d,a c,a c,a c,b a b n
(3)
Ω ={p |∥p −p ∥−∥P −P ∥≤δ },
B. Multi-Contact Localization for Hyper-Resolution d,b c,b c,a c,b a b n
Wedevelopaninnovativeapproachthatusedmulti-contact resulting in two distance filtered sets Ω d,a and Ω d,b. δ n
localization to transform sparse touch data into detailed is a threshold to offset noises, such as in calibration
object surface geometry, thereby achieving hyper-resolution and forward kinematics. Repeating this operation on all
from tactile sensors with limited spatial resolution. Consider N c sensors in contact, we obtain N c distance filter sets
acollectionoftactilepatternsandtheircorrespondingobject Ω d,1,Ω d,2,··· ,Ω d,Nc.
surface details, analogous to a library of tactile experiences. We desire to find an optimal solution Ψ∈Ω d,1×Ω d,2×
Given the spatial sparsity inherent in these tactile patterns, ··· × Ω d,Nc that maximizes the similarities between taxel
confidentlyassociatingatactilepatternwithitsobjectsurface embeddings T and the surface embeddings S l. To achieve
isanon-trivialtask.Toaddressthischallenge,wereasonover this, we build a multipartite graph K with N c partites. Each
multiple simultaneous contacts with the object to increase candidateleftinΩ d isaddedasanode,andedgesareadded
confidence in our estimates. ifthedistanceconstraint(Eqn.3)issatisfied.WeusePaton’s
Having a set of objects O, we first collect a contact algorithm [43] to find the cycle Ψ with the largest joint
database B for each object o∈O offline, which consists of probability.
o
the contact surface patches S , the corresponding contact Figure 3 illustrates the process of Hyper-Resolution.
o,c
signalsC ,andthe6DposesofthesensorP inacommon
o o,c
frame of reference F . We omit the subscript o in the
IV. DATASETS
c
following paragraph for simplicity and note that these data We collected two datasets using NVIDIA Isaac Sim for
are object o specific. For each sample b in the database B , a subset of YCB objects [19]. We used an Allegro Hand
i o
b = (s ∈ S ,c ∈ C ,p ∈ P ). The 6D pose p ∈ R7 equipped with XELA tactile sensors, resembling our real-
i c,i c i o c,i c c,i
is represented as the concatenation of 3D translation R3 world setup. The Allegro Hand has eleven 4 × 4 flat pad
and 3D rotation in quaternion form R4. To ease online sensors, three 4 × 6 flat pad sensors, and four curved tip
computation, we preprocess the contact surface patches S sensors (each has 30 taxels). The first dataset, Section IV-
c
and encode them into embeddings S using the pretrained A, is a comprehensive database of tactile sensors interacting
l
surface encoder E . withtheobjects.Thisdatasetservesasourtactileexperience
s
During deployment, we assume there is N number of libraryandisusedtoevaluatetactilerepresentationlearning.
c
sensors that are in contact with the object o. We denote J The second dataset, Section IV-B, consists of the Allegro
asthesetofthesesensorsandtheactualposeofeachsensor Hand holding an object and executing random trajectories.
j ∈ J as P ∈ R7. The robot’s forward kinematics is This dataset is used to evaluate the performance of our
u,j
used to transform the sensor poses to a common frame of methods on a downstream task, namely the in-hand 6D pose
reference.Eachsensorj hasasensorreadingd represented estimation task.
j
as the taxel graph (Sec. III-A.1). We encode the collection
A. Contact Database
of sensor readings D = {d | i ∈ 1,2,··· ,N } into taxel
i c
embeddingsT usingtactileencoderE suchthatT ={T = Wefirstconstructadatasetthatcapturestactileexperiences
t i
E (d ) | ∀d ∈ D}. We measure the similarity between T across the entire surface of an object at various points. The
t i i
and the surface embeddings S stored in the database and tactile sensor is simulated using the Contact Sensor
l
rank the candidate poses P accordingly. We take top δ provided by Isaac Sim. We sample 2048 points on the
c C
candidatestoreducethecomputationinlaterstepsandobtain object mesh using Poisson disk. Each point corresponds toTABLE I: Comparison of Hyper-Resolution Performance
using Different Tactile Representations.
Method Rank↓ CD↓
Image-based(CNN) 104.40 0.82
Pointcloud-based(PointNet) 150.31 0.85
Graph-based(Ours) 84.05 0.74
A. Qualitative Analysis of Learned Tactile Representation
To evaluate the quality of our learned tactile representa-
tion,weperformedaqualitativeanalysisusingvisualizations
(a) (b) ofthetactileembeddings.Weusedthecontactdatabase(Sec-
tionIV-A)togeneratethetactileembeddingsforeachcontact
Fig. 5: Allegro Hand equipped with XELA tactile sensors:
using our learned tactile encoder. We then applied principal
a) taxel distribution, b) grasping an object
component analysis (PCA) to reduce the dimensionality of
the embeddings to 3, and used the resulting values as RGB
a 3D position R3 and its respective surface normal R3. Like colors for visualization.
previous works [27, 28, 34], we randomly chose a subset The results, as depicted in Fig. 1, reveal that the tactile
of points to collect our tactile experience. For each selected embeddings effectively capture the geometric features of the
point, we align the tactile sensor’s z-axis with the surface contact surface, such as flatness, curvature, and edges. For
normal and conduct eight contact trials. In each trial, the example, the flat surfaces on the Master Chef Can, Sugar
sensor is rotated 45◦. We start by positioning the sensor 2.5 Box, and Mustard Bottle are all represented in shades of
cm away from the point and then gradually push it towards purple and blue, while the curved surfaces are depicted in
the surface along the normal. Once the sensor is in contact yellow and green. The edges of the Master Chef Can are
with the object, we collect the tactile observations and highlightedinlimegreen,indicatingastarkcontrastbetween
their corresponding poses. This process is repeated for each the neighboring points. Notably, the tactile embeddings
type of taxel sensor on the Allegro Hand, which includes demonstrate consistency across different objects and sensor
4×4,4×6, and curved tips, to compile a comprehensive types,demonstratingthegeneralizationofourrepresentation.
contact database.
B. Hyper-Resolution Performance Evaluation
B. In-hand Object Dataset
In this section, we evaluate the performance of our
To evaluate our framework on downstream tasks such as proposed hyper-resolution algorithm, which maps the low-
6D in-hand pose estimation, we also collected a simulated resolution taxel signals to high-resolution contact surface
dataset, which consists of the Allegro Hand holding an patches using a contact database. We compare our method
object and executing random trajectories. For each object, with two baselines: image-based approaches (CNN) [2, 14],
we collect 16,000 samples for the training set and 4,000 and point-cloud-based approaches (PointNet) [17, 18].
samples for the validation set. The dataset is collected using Weusetwometricstomeasuretheaccuracyofourhyper-
the following procedures: resolution: Chamfer distance (CD) and rank. Chamfer dis-
1) The hand is initialized to face upwards. tance computes the average minimum distance between two
2) The object is dropped from a height of 2cm above the point sets, and reflects the geometric similarity between the
hand, with its pose randomly initialized. estimated and ground truth contact surfaces. Rank measures
3) The hand performs the grasping action. the precision of identifying the correct surface based on the
4) Ifthegraspfails(e.g.,theobjectfalls),returntostep1. similarity between the tactile embeddings and the surface
embeddings. The rank of an algorithm is then the average
V. EXPERIMENTS
rank it assigns to the ground truth surface across all tactile
WeutilizetheAdamW[44]optimizerwithalearningrate contactpointsinthedatabaseinSectionIV-A.Alowerrank
of 0.001. We pre-train the tactile encoder for 100 epochs means a better performance. Table I shows the results of
using all objects and optimize the pose estimation model for these experiments. We observe that our method outperforms
each object for 500 epochs. both of the baselines on both metrics, demonstrating the
We begin by qualitatively assessing the tactile represen- effectiveness of our hyper-resolution algorithm.
tation learned through our approach (Section V-A). Next,
C. Comparison of Graph Operators and Constructors
we examine the effectiveness of our method in the hyper-
resolution task (Section V-B). We then ablate the chosen Weablatetheimpactofdifferentgraphoperatorsandcon-
graph operators and constructors (Section V-C) and study structors on the quality of the learned tactile representation.
the impact of multi-contact localization on hyper-resolution We compared our proposed EdgeConv operator with two
(Section V-D). In addition, we test our approach on two seminal works: TacGNN [45], GCN [46]. We also examined
downstreamtasks:in-handobjectposeestimation(SectionV- differentgraphconstructors,suchasKNNandradiusgraphs,
E) and surface classification (Section V-F). withdifferentparameters.TableIIshowstheresultsoftheseTABLE II: Performance of Different Graph Operators and
Constructors on the Hyper-Resolution Task.
GraphOperator GraphConstructor Rank↓ CD↓
TacGNN KNN(n=1) 111.03 0.99 RGB Depth
TacGNN[45] KNN(n=3) 87.11 0.79 6D Pose
TacGNN KNN(n=5) 114.10 1.09 Hyper-
Resolution
TacGNN Radius(r=0.005) 201.06 2.47 VisuoTactile
TacGNN Radius(r=0.01) 115.71 1.12 Pose Estimator
TacGNN Radius(r=0.015) 117.57 1.10 Sparse Point Cloud HyperTaxel Point Cloud
GCN[46] KNN(n=3) 111.77 0.89
EdgeConv KNN(n=1) 92.31 0.81 Fig. 7: An illustration of the modified ViTa algorithm with
EdgeConv KNN(n=3) 84.44 0.75 our hyper-resolution module. The tactile data is enhanced
EdgeConv KNN(n=5) 84.85 0.75
by our module to produce a high-resolution representation
EdgeConv Radius(r=0.005) 168.37 2.25
EdgeConv(our) Radius(r=0.01) 84.05 0.74 of the object surface. This representation is then fed into the
EdgeConv Radius(r=0.015) 85.21 0.76 ViTa algorithm as its tactile input.
TABLE III: Comparative Analysis of the Effect of Hyper-
540 Taxel on In-Hand 6D Pose Estimation.
530
Method AngularError PositionError ADD
520
DenseFusion 10.52±0.12 0.46±0.00 0.87±0.01
510 ViTa 8.85±0.10 0.43±0.00 0.77±0.01
ViTa+HyperTaxel 8.56±0.10 0.40±0.00 0.74±0.01
500
490
1 2 3
Number of contacts
column shows the refined likelihood map after applying
Fig. 6: Effect of Multi-Contact Localization on Hyper- multi-contact reasoning. After multi-contact reasoning, the
Resolution. true contact areas are brightly colored while all other areas
are dark, accurately pinpointing the potential origin of the
tactile input on the object.
experiments. We observe that our EdgeConv operator com-
bined with the radius graph constructor (r = 0.01) achieves E. Effect of HyperTaxel on In-Hand 6D Pose Estimation
the best performance in terms of rank and CD metrics. This
In this section, we evaluate the effectiveness of our ap-
indicatesthatouroperatorcaneffectivelycapturetherelative
proach by integrating it with ViTa [17], an existing vi-
features between taxels and that the radius graph can better
suotactile model for 6D pose estimation. ViTa uses visual
reflect the spatial distance between taxels.
and tactile data to represent the object’s surface, but low-
resolution tactile data can affect its performance. Fig. 7
D. EffectofMulti-ContactLocalizationonHyper-Resolution
shows the modified pipeline, which includes our hyper-
In this section, we evaluate how the number of contacts resolution method to map the sparse low-resolution tactile
affects our hyper-resolution algorithm. Taxel-based sensors data to a high-resolution object surface representation. This
perceive coarser geometry features, making it challenging to representation is then fed into the ViTa algorithm without
estimate the corresponding surface from a single observa- any further changes. Following prior works [16, 17], we
tion. Previous studies [27, 28] have confirmed performance evaluate the performance using three metrics: position error
gains by incorporating multi-contacts on vision-based tactile (cm), angular error (deg), and ADD (cm). Position error is
sensors. In this study, we extend this concept to taxel-based the L2 norm of the difference between the estimated and
sensors. ground truth translation vectors, ||t−tˆ|| . Angular error is
2
Figure 6 shows the results of the quantitative analysis the inverse cosine of the inner product of the estimated and
of this experiment. We observe that the CD decreases as ground truth quaternions, cos−1(2⟨R,Rˆ⟩2 − 1), and ADD
the number of contacts increases, indicating that the hyper- measuresthepairwisedistancesbetweenthe3Dmodelpoints
resolution quality improves with more contacts. This is transformed using estimated and ground truth 6D poses,
because more contacts provide more information and con- 1 (cid:80) ||(Rx+T)−(Rˆx+Tˆ)||, where x is the 3D point,
m x∈o
straints about the object surface, reducing the ambiguity and and m is the number of 3D points on the object model o.
uncertainty in the hyper-resolution. We verify the performance of our proposed hyper-
We provide a visualization sample in Fig. 3. The second resolution algorithm in the synthetic pose estimation data
and third column shows the likelihood map of a single collected in Sec. IV-B. Table III shows the results. We first
contactandmulti-contactwiththeobjectineachrow,respec- compare the vision-only baseline DenseFusion [47] with the
tively.Brightercolorsindicateahigherlikelihood.Wenotice visuotactile baseline ViTa [17]. By adding tactile informa-
the curved surfaces are accurately depicted with brighter tion,ViTahasa1.67degreeslowerangularerrorand0.03cm
colors,suggestingthatthealgorithmcorrectlyidentifiesthese lower position error. ViTa+HyperTaxel outperforms all of
contacts as originating from a curved surface. The third them. An object-wise analysis, depicted in Fig. 8, reveals
)%(
ecnatsiD
refmahCTABLE IV: Surface Classification Performance in Real-
Robot Experiments.
Method RI↑ ARI↑ Acc↑
BYOL 0.546 0.091 83.5
AE 0.502 0.004 83.5
Raw 0.520 0.038 72.8
Ours 0.586 0.171 85.4
cluster tactile signals based on the geometric features of the
contact surfaces, such as flatness and curvature. To conduct
this experiment, we use a real-world object that has both
flat and curved surfaces: the Master Chef can. As shown in
Fig. 9, we press the tactile sensors (both the curved tip and
the 4×4 flat pad) on different parts of the can. We collect
the tactile signals from multiple contacts on each surface,
covering the flat and curved areas as evenly as possible.
A video demonstration of this process is available in our
supplementary material.
We then encode the tactile signals using three representa-
tions:BYOL,AE,andours.Wealsocompareagainstdirectly
using the raw data (Raw). We then apply the K-means
clusteringalgorithmtoclassifythemintotwoclasses:flatand
curved.TableIVshowstheresultsofthisexperiment,where
we measure the performance of our representation using
Fig. 8: Pose estimation performance on YCB objects. three metrics: random index (RI), adjusted random index
(ARI)[48],andaccuracy.RImeasuresthesimilaritybetween
theestimatedclusteringandtheground-truthclustering.ARI
takes into account the expectedvalue of the RI, which is the
randomguessingprobability.Weincludetheaccuracymetric,
following the linear classification protocol used to evaluate
representation quality in self-supervised learning [21–23] by
freezing the learned representation and adding a linear layer
to predict the surface class. We observe that our method
outperforms the baselines on all metrics, indicating that our
Fig. 9: Demonstration of data collection process. We collect representationcaneffectivelyclusterthetactilesignalsbased
a real-world dataset by pressing the tactile sensor-equipped on the geometric features of the contact surfaces.
fingertip on flat surfaces (left) and curved surfaces (right).
VI. CONCLUSIONS
In this paper, we presented a novel framework, Hyper-
enhancements for most objects. Notably, our approach faces Taxel, for learning a geometrically-informed representation
challengesonpowerdrillobjectswhichrevealsonepotential of taxel-based tactile signals to achieve hyper-resolution of
limitation. Since our offline collected database relies on contact surfaces between the sensor and the object. We
random sampling on the object model, our current choice introducedagraph-basedrepresentationoftactilesignalsand
of sample number might not capture the complex geometry acontrastivelearningobjectivetolearnacorrespondencebe-
of the power drill accurately. In the future, this limitation tweenthelow-resolutiontaxelsignalsandthehigh-resolution
might be lifted by scaling up the samples on the object. contact surfaces. We proposed a multi-contact localization
algorithm to reduce the uncertainty and ambiguity in the
F. Real Robot Results
taxel signals and map them to the object surface geome-
We deployed our model, trained on synthetic data on a try. We conducted extensive experiments on synthetic and
multi-fingered gripper (Allegro Hand equipped with XELA also presented real-world experiments and showed that our
tactilesensors)affixedtoaSawyerrobot.Thetactilesensors framework outperforms the baselines. We demonstrated that
capturesurfacecontactpointsontheobject.WeuserealYCB thelearnedrepresentationcancapturethegeometricfeatures
objects to evaluate the performance of our framework in a of the contact surface and generalize across different objects
real-world robot environment. and taxel arrangements. We also showed that the hyper-
Agoodrepresentationshoulddemonstrateabilitytodistin- resolution algorithm can improve the performance of the
guish different surface types. We evaluate our representation visuotactile pose estimation model and enable robust sim-
onthesurfaceclassificationtasktodemonstrateitsabilityto to-real transfer.Our framework opens up new possibilities for leverag- [24] A.Radfordetal.,“LearningTransferableVisualModelsFromNatural
ing taxel-based tactile sensors for dexterous manipulation Language Supervision,” in Proceedings of the 38th International
ConferenceonMachineLearning. PMLR,Jul.2021,pp.8748–8763,
and perception. Some future directions for improving the
iSSN:2640-3498.
framework include incorporation of temporal information, [25] T. B. Brown et al., “Language Models are Few-Shot Learners,” Jul.
expandingthecontactdatabase,andapplyingtheframework 2020,arXiv:2005.14165[cs].
[26] F. Yang et al., “Binding Touch to Everything: Learning Unified
to other modalities.
Multimodal Tactile Representations,” Jan. 2024, arXiv:2401.18084
REFERENCES null.
[27] M.B.Villalongaetal.,“TactileObjectPoseEstimationfromtheFirst
[1] R.S.Dahiyaetal.,“TactileSensing—FromHumanstoHumanoids,” TouchwithGeometricContactRendering,”inProceedingsofthe2020
IEEETransactionsonRobotics,vol.26,no.1,pp.1–20,Feb.2010. ConferenceonRobotLearning. PMLR,Oct.2021,pp.1015–1029,
[2] B. Wu et al., “Tactile Pattern Super Resolution with Taxel-based iSSN:2640-3498.
Sensors,” in 2022 IEEE/RSJ International Conference on Intelligent [28] G. M. Caddeo et al., “Collision-aware In-hand 6D Object Pose
RobotsandSystems(IROS),Oct.2022,pp.3644–3650. EstimationusingMultipleVision-basedTactileSensors,”in2023IEEE
[3] W.Yuanetal.,“GelSight:High-ResolutionRobotTactileSensorsfor International Conference on Robotics and Automation (ICRA), May
Estimating Geometry and Force,” Sensors, vol. 17, no. 12, p. 2762, 2023,pp.719–725.
Dec.2017. [29] C.CorcoranandR.Platt,“Ameasurementmodelfortrackinghand-
[4] E.Donlonetal.,“GelSlim:AHigh-Resolution,Compact,Robust,and object state during dexterous manipulation,” in 2010 IEEE Interna-
CalibratedTactile-sensingFinger,”May2018,arXiv:1803.00628[cs]. tionalConferenceonRoboticsandAutomation,May2010,pp.4302–
[5] T. P. Tomo et al., “A New Silicone Structure for uSkin—A Soft, 4308,iSSN:1050-4729.
Distributed, Digital 3-Axis Skin Sensor and Its Integration on the [30] S.Luoetal.,“LocalizingtheObjectContactthroughMatchingTactile
HumanoidRobotiCub,”IEEERoboticsandAutomationLetters,vol.3, Features with Visual Map,” in 2015 IEEE International Conference
no.3,pp.2584–2591,Jul.2018. on Robotics and Automation (ICRA), May 2015, pp. 3903–3908,
[6] N. Wettels et al., “Biomimetic Tactile Sensor Array,” Advanced arXiv:1708.04441[cs].
Robotics,vol.22,no.8,pp.829–849,Jan.2008. [31] J.Bimboetal.,“In-HandObjectPoseEstimationUsingCovariance-
[7] Z.Dingetal.,“Sim-to-RealTransferforRoboticManipulationwith BasedTactileToGeometryMatching,”IEEERoboticsandAutomation
Tactile Sensory,” in 2021 IEEE/RSJ International Conference on Letters,vol.1,no.1,pp.570–577,Jan.2016.
IntelligentRobotsandSystems(IROS),Sep.2021,pp.6778–6785. [32] M. C. Koval et al., “The manifold particle filter for state estimation
[8] S. Li et al., “Visual–Tactile Fusion for Transparent Object Grasping onhigh-dimensionalimplicitmanifolds,”in2017IEEEInternational
inComplexBackgrounds,”IEEETransactionsonRobotics,pp.1–19, ConferenceonRoboticsandAutomation(ICRA),May2017,pp.4673–
2023. 4680.
[9] Q. K. Luu et al., “Simulation, Learning, and Application of Vision- [33] R.Gaoetal.,“TheObjectFolderBenchmark:MultisensoryLearning
BasedTactileSensingatLargeScale,”IEEETransactionsonRobotics,
withNeuralandRealObjects,”Jun.2023,arXiv:2306.00956[cs].
vol.39,no.3,pp.2003–2019,Jun.2023. [34] S. Suresh et al., “MidasTouch: Monte-Carlo inference over distribu-
[10] F.Yangetal.,“TouchandGo:LearningfromHuman-CollectedVision tionsacrossslidingtouch,”inProceedingsofThe6thConferenceon
andTouch,”Jun.2022. RobotLearning. PMLR,Mar.2023,pp.319–331,iSSN:2640-3498.
[11] S.Lietal.,“TaTa:AUniversalJammingGripperwithHigh-Quality [35] F. Yang et al., “Generating Visual Scenes from Touch,” 2023, pp.
Tactile Perception and Its Application to Underwater Manipulation,” 22070–22080.
in2022InternationalConferenceonRoboticsandAutomation(ICRA), [36] N. F. Lepora et al., “Tactile Superresolution and Biomimetic Hyper-
May2022,pp.6151–6157. acuity,”IEEETransactionsonRobotics,vol.31,no.3,pp.605–618,
[12] P.Piacenzaetal.,“Data-DrivenSuper-ResolutiononaTactileDome,” Jun.2015.
IEEERoboticsandAutomationLetters,vol.3,no.3,pp.1434–1441, [37] C.Ledigetal.,“Photo-RealisticSingleImageSuper-ResolutionUsing
Jul.2018. aGenerativeAdversarialNetwork,”2017,pp.4681–4690.
[13] G. Bu¨scher et al., “Augmenting curved robot surfaces with soft [38] Z.-H.Yinetal.,“RotatingwithoutSeeing:TowardsIn-handDexterity
tactileskin,”in2015IEEE/RSJInternationalConferenceonIntelligent
throughTouch,”vol.19,Jul.2023.
RobotsandSystems(IROS),Sep.2015,pp.1514–1519. [39] Z. Xue et al., “ArrayBot: Reinforcement Learning for Gen-
[14] I.Guzeyetal.,“DexterityfromTouch:Self-SupervisedPre-Training eralizable Distributed Manipulation through Touch,” Jun. 2023,
ofTactileRepresentationswithRoboticPlay,”inProceedingsofThe
arXiv:2306.16857[cs].
7th Conference on Robot Learning. PMLR, Dec. 2023, pp. 3142– [40] J. Gilmer et al., “Neural Message Passing for Quantum Chemistry,”
3166,iSSN:2640-3498. in Proceedings of the 34th International Conference on Machine
[15] R.S.Dahiyaetal.,“DirectionsTowardEffectiveUtilizationofTactile Learning. PMLR,Jul.2017,pp.1263–1272,iSSN:2640-3498.
Skin: A Review,” IEEE Sensors Journal, vol. 13, no. 11, pp. 4121– [41] Y.Wangetal.,“DynamicGraphCNNforLearningonPointClouds,”
4138,Nov.2013. Jun.2019,arXiv:1801.07829[cs].
[16] H.Lietal.,“ViHOPE:VisuotactileIn-HandObject6DPoseEstima- [42] C.R.Qietal.,“PointNet:DeepLearningonPointSetsfor3DClas-
tionWithShapeCompletion,”IEEERoboticsandAutomationLetters, sificationandSegmentation,”inProceedingsoftheIEEEConference
vol.8,no.11,pp.6963–6970,Nov.2023. onComputerVisionandPatternRecognition(CVPR),Jul.2017.
[17] S. Dikhale et al., “VisuoTactile 6D Pose Estimation of an In-Hand [43] K. Paton, “An algorithm for finding a fundamental set of cycles of
Object Using Vision and Tactile Sensor Data,” IEEE Robotics and a graph,” Communications of the ACM, vol. 12, no. 9, pp. 514–518,
AutomationLetters,vol.7,no.2,pp.2148–2155,Apr.2022.
Sep.1969.
[18] A. Rezazadeh et al., “Hierarchical Graph Neural Networks for Pro- [44] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regulariza-
prioceptive 6D Pose Estimation of In-hand Objects,” in 2023 IEEE tion,”Sep.2018.
International Conference on Robotics and Automation (ICRA), May [45] L.Yangetal.,“TacGNN:LearningTactile-BasedIn-HandManipula-
2023,pp.2884–2890. tionWithaBlindRobotUsingHierarchicalGraphNeuralNetwork,”
[19] B. Calli et al., “The YCB object and Model set: Towards common IEEERoboticsandAutomationLetters,vol.8,no.6,pp.3605–3612,
benchmarksformanipulationresearch,”in2015InternationalConfer-
Jun.2023.
enceonAdvancedRobotics(ICAR),Jul.2015,pp.510–517. [46] A.Garcia-Garciaetal.,“TactileGCN:AGraphConvolutionalNetwork
[20] K.Heetal.,“DeepResidualLearningforImageRecognition,”2016, forPredictingGraspStabilitywithTactileSensors,”in2019Interna-
pp.770–778. tional JointConference on NeuralNetworks (IJCNN), Jul.2019, pp.
[21] J.-B. Grill et al., “Bootstrap Your Own Latent - A New Approach 1–8.
to Self-Supervised Learning,” in Advances in Neural Information [47] C.Wangetal.,“DenseFusion:6DObjectPoseEstimationbyIterative
ProcessingSystems,2020.
DenseFusion,”2019,pp.3343–3352.
[22] K. He et al., “Momentum Contrast for Unsupervised Visual Repre- [48] L.HubertandP.Arabie,“Comparingpartitions,”JournalofClassifi-
sentationLearning,”2020,pp.9729–9738. cation,vol.2,no.1,pp.193–218,Dec.1985.
[23] ——, “Masked Autoencoders Are Scalable Vision Learners,” Dec.
2021,arXiv:2111.06377[cs].