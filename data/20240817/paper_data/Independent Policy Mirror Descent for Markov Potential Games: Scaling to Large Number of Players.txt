Independent Policy Mirror Descent for Markov Potential Games:
Scaling to Large Number of Players
∗ ∗
Pragnya Alatur Anas Barakat Niao He
Abstract—Markov Potential Games (MPGs) form an impor- for modeling marketplaces for distributed energy resources
tant sub-class of Markov games, which are a common frame- such as electricity markets and pollution tax [15], [9].
work to model multi-agent reinforcement learning problems.
A common objective in MARL problems is to reach
In particular, MPGs include as a special case the identical-
a Nash equilibrium. It is well-known that even in (static)
interest setting where all the agents share the same reward
function.ScalingtheperformanceofNashequilibriumlearning normal form games, computing a Nash equilibrium is com-
algorithmstoalargenumberofagentsiscrucialformulti-agent putationally hard in general[4]1. For MPGs, however,Nash
systems. To address this important challenge, we focus on the equilibria are computationally tractable [11], [19]. Recently,
independentlearningsettingwhereagentscanonlyhaveaccess
in the static (stateless) setting, it has been shown in [3]
to their local information to update their own policy. In prior
that the dependence on the number of players of the Nash
work on MPGs, the iteration complexity for obtaining ε-Nash
regretscaleslinearlywiththenumberofagentsN.Inthiswork, regret guarantees can be improved to be sublinear using an
weinvestigatetheiterationcomplexityofanindependentpolicy independentnaturalpolicygradientalgorithmwithadditional
mirrordescent(PMD)algorithmforMPGs.WeshowthatPMD entropic regularization.
with KL regularization, also known as natural policy gradient,
In the present work, we investigate the performance of
enjoys a better √N dependence on the number of agents,
improving over PMD with Euclidean regularization and prior an independent policy mirror descent algorithm for finding
work.Furthermore,theiterationcomplexityisalsoindependent approximateNashequilibriainMPGs.Inparticular,wefocus
of the sizes of the agents’ action spaces. on improving the dependence on the number of players of
the Nash regretincurredbythe algorithm.Ourcontributions
I. INTRODUCTION
are summarized as follows:
Introduced by the seminal work of [18], Markov games WeintroduceaPolicyMirrorDescent(PMD)algorithm
(also called stochastic games) offer a convenientmathemati- • whichcanbeimplementedindependentlybytheagents,
cal framework to model multi-agent reinforcement learning inspired by prior work in the single-agent setting [10],
(MARL) problems where several agents interact in a shared [21]. We show that this algorithm can be seen as a
space environment to make strategic decisions. A major mirror descent algorithm with a dynamically weighted
challenge in MARL is to deal with the so-called curse of Bregman divergence regularization. This observation
multi-agents in which the size of the joint action space allows to draw a direct connection between PMD and
scales exponentially with the number of agents. Given the thestandardindependentpolicygradientalgorithm.This
ever-growing scale of real-world multi-agent systems such algorithmunifiestheQ-ascentalgorithmobtainedwhen
as transportationnetworks, social networksand autonomous using Euclidean regularizationand the celebrated Natu-
vehicles,scalabilityofMARLalgorithmstotheselarge-scale ral gradient algorithm which follows from using a KL
systems involving a large number of agents poses a funda- regularization.
mental challenge. A useful approach to address this scaling We establish Nash regret bounds for PMD with either
issue in the literature consists in adopting the independent • Euclideanor KL regularizationforMPGs. In particular,
learning protocol in which each agent independently selects we show that the dependence w.r.t. the number of
their policy and observes only local information including players in the iteration complexity to reach an ε-Nash
theirownactionsandrewards,alongwithasharedstate(see regret2 improves from N to √N when using the KL
e.g. [16] for a recent survey). regularization instead of the Euclidean regularization.
In this work, we focus on designing scalable and inde- Notably, the Nash regret bound is also independent
pendentalgorithmsfor a specific subclass ofMarkovgames: of the size of the agents’ action spaces. To achieve
MarkovPotentialGames(MPGs).Inspiredbythecelebrated these improvements over prior work, the use of the
(static) potential games in normal form pioneered by the geometry induced by entropic regularization appearsto
celebrated work [14], MPGs can be seen as an extension be crucial. Our guarantees hold in the full information
of this class of games to the dynamic setting in which settingwherewesupposeaccesstotheaverageQ-values
the environment is subject to state transitions. This class of w.r.t. the policies of the opponents.
MPGshasrecentlyattractedattentioninalineofworks[15],
[11],[24],[23], [5],[9].Inparticular,MPGshavebeenused
1Moreprecisely,[4]showedthatcomputingaNashequilibriumisinthe
complexity classPPAD.
Authors are affiliated with the department of Computer Science, ETH 2The iteration complexity to obtain ε-Nash regret gives us the iteration
Zurich,Switzerland. ∗standsforfirst-authorcontribution. complexity required toobtainanε-approximate Nashequilibrium.
4202
guA
51
]GL.sc[
1v57080.8042:viXraTABLEI
II. RELATEDWORK
ITERATIONCOMPLEXITYFOROBTAININGε-NASHREGRETINMPGS.
For static games, a large body of work in the literature
Algorithm ε-NashRegret
( inth tea rt esw tee dc inan en so tat bh lio shp ie ngto gug ai rv ae nteju es sti fc oe
r
lt eo arh ne inre g) alh ga os ritb he men
s Independent PG1 [11],[24] O
φmaxκ2 ρSN im ∈a Nx |Ai|
(1 γ)4ε2
− !
in potential games for concave potential functions. Be-
yond this particular concavity structure, a few works have Q-Ascent1[5] O
Φmaxmin {κ˜ρ,S }4N im ∈a Nx |Ai|
(1 γ)6ε2
addressed the question of the convergence of some no- − !
regret algorithms such as the celebrated Hedge algorithm GD(softmax) [23] O φmaxM2N im ∈a Nx |Ai|
(1 γ)4c2ε2
to Nash equilibria including in the bandit feedback set- − !
ting [8]. Our work has been partially inspired by [3] which Natural GD(softmax) [23] O φ2 maxMN
(1 γ)3cε2
providesaO˜(min(√N,φ )φ /ε4)iterationcomplexity (cid:18) − (cid:19)
(where φ
max
is the maxm imax umma ox f the potential function) PMD (Euclidean reg.) O φ2 max (κ˜ 1ρP γ)N i 4= ε21|Ai|
(cid:18) − (cid:19)
for reaching an ε-approximate Nash equilibrium using an PMD (KL reg.)2 O φ2 maxκ˜ρ√N
independentnaturalpolicygradientalgorithmwithadditional (1 γ)4cε2
(cid:18) − (cid:19)
entropy regularization. While this result does not require We highlight the dependence on the number of agents N
stationary policies to be isolated (see Assumption V.4) and and focus only on prior work with non-asymptotic guarantees.
improves over prior work in terms of dependence on the Notation: ρ ∆( ) is the initial distribution, Φ max ,
max
Φπ(ρ∈ ),cisS
aconstantdependentontheinitialpolicy
numberofagents,itonlyholdsforstaticpotentialgamesand and dπ e∈fiΠ n| ed in T| heorem V.6, M , sup max 1 > 0
achievesa worseiterationcomplexityintermsofaccuracyε π s ∈S dπ ρ(s)
under Assumption V.1. With our definition of MPGs, Φ
compared to the existing O(ε −2) results for MPGs. φmax . For other notations see Preliminaries section IIIm ba ex lo≤ w.
Initiated by works such as [13] proposing state-based 11 U−sγ
eamoregeneraldefinitionofMPGs,seeRemarkIII.1below
potential games, a line of research has focused on MPGs as foradiscussion.2Wealsoobtaina φ2 maxM√N regretbound
a natural generalization of potential games to the dynamic
for thisalgorithmunder minor
moO difi(cid:16) c( a1 t−ioγ n) s3c iε n2 o(cid:17)
ur proof. This
stateful setting as well as (single-agent) Markov Decision shows a strict improvement in terms of N w.r.t. the result for
Processes to the multi-agent setting. [11], [24] proposed Natural GD [23] in the table.
an independent policy gradient method for MPGs with
a (ε 2) iteration complexity to reach an ε-approximate , 1, ,N with N 2 is the set of players3,
− • N { ··· } ≥
O is a finite set of states4,
Nashequilibrium.Later,[5]proposedtheQ-ascentalgorithm
• S
for MPGs with an O(ε −2) iteration complexity improving
• bA
yi is ,a finite set o tf heac st eio tn os ff jo or ine ta ac ch tii
on∈
s,N. We denote
over the dependence on the state space size. A few other i i
A ×∈NA
worksfocusedonasymptoticconvergenceresults[12],[6]or P : ∆( ) is the Markov transition kernel,
• S ×A → S
regretguaranteesforpolicygradientalgorithmswithsoftmax where we use throughout this paper the notation ∆( )
S
for the set of probability distributions over the set ,
parametrization [6], [23]. To the best of our knowledge,
all the aforementioned finite-time guarantees in prior works r i : R is the reward functionof agenti S ,
• S×A→ ∈N
provide iteration complexities scaling at least linearly with ρ ∆( ) is an initial distribution over states,
• ∈ S
the number of agents. The main focus of our work is to γ (0,1) is a discount factor.
• ∈
reduce such a dependence. We refer the reader to Table I Policies. For each i , denote by π : ∆( )
i i
∈ N S → A
for a brief comparison of our results to the closest related the policy of agent i and Πi the set of all possible
works.Morerecently,[20]establishedanasymptotic (ε 1) Markov stationary policies for agent i . We
−
O ∈ N
iterationcomplexityresultfortheindependentnaturalpolicy denote by π = (π ) the joint policy. We use the
i i
gradientundersome specific additionalassumptionsthat we notationΠ, Πi fo∈rNthe set of jointMarkovstationary
i
×∈N
do notrequire in our work (see their Assumption 3.2), i.e. a policies.
bound that is valid only after a certain unknown number of
iterations. In contrast, our guarantees hold globally for any In a Markov game, the agents’ interaction with the
iteration. environment unfolds as follows: At each time step t 0,
≥
Sample-based approaches for MPGs have been proposed the agents observe a shared state s and choose a joint
t
∈S
and analyzed in [11], [24], [5] for example. action a = (a ) according to their joint policy π(t),
t t,i i
Finally, a few works proposed algorithms for extensions i.e. for every i ∈ ,N a is sampled according to π(t)( s ).
∈ N t,i i ·| t
of MPGs to networked settings [1], [2], [25] or to a more Each agent i receives a reward r (s ,a ). Then the
i t t
∈ N
general class of games relaxing the definition of MPGs [7]. game proceeds by transitioning to a state s drawn from
t+1
the distribution P(s ,a ).
t t
III. PRELIMINARIES
Markov Game.We considera discountedinfinite time hori- 3Weuseagentandplayerinterchangeably throughout thiswork.
4Extensiontoinfinitecountableorevencontinuousstatespaceispossible
zon Markov Game Γ = , ,( ) ,P,(r ) ,ρ,γ
hN S Ai i ∈N i i ∈N i underadditional technical assumptions suchasboundedness ofthereward
[18] where: andpotential functions.Occupancy measures and distribution mismatch IV. INDEPENDENTPOLICY MIRROR DESCENT
coefficients. For any joint policy π Π, we define Inspired by a line of works [10], [21], [22] for the single-
∈
bth ye ds π ρta (t se )oc =cup (a 1nc −y γm )eas ∞ tu =re 0γd tπ ρ P ρf ,πo (r s teve =ry sst )a .te Ss imi∈ larlS y a ag lge on rt its he mttin tog, tw hee mex ute ltn i-d agth ee ntP so el ti tc iy ngM fi orr ror MD Pe Gs sc .en Tt h( eP nM wD e)
to prior work [5], we introduce distribution mismatch
discuss two instantiations of PMD with Euclidean and KL
P
coefficients to quantify the difficulty for learning agents to
regularizations respectively.
explore in a Markov game. In particular, we define for any We start by defining a central quantity for independent
distributionρ ∆( )andanypolicyπ Π,thedistribution learning in MPGs: The averaged Q-value Q¯π : R
m tm hi eis nm i dma ivatc ix sh iovc neo re s i∈ if sofi nc toie κ˜nS bρt eκ , uρ ni d, n ef rν ss ∈tu o∆p o(π dS∈ )Π cs ouk mpd ρ∈ ππ ρ p∈ok nΠ∞ ek na d tν ws π ρ k iw s∞eel , il nwas bh oei trt hes f Q¯o π ir (a sn ,y a ia )g ,ent Ei a−∈ i∼N π−a in ( ·d |s)p [Qol π iic (y s,π a i∈ ,aΠ −id )e ],fi ∀ni sed ∈S a Ss× ,f aoA illi ∈o→ w As: i.
cases. Notice that κ˜ min(κ , ). For each agent i and each state s , starting
ρ ≤ ρ |S| froman initial policy∈ π(1N) ∆( ) , independ∈ enS tPMD for
i ∈ Ai S
Value functions. For each joint policy π Π, and each i MPGs unfolds iteratively as follows:
N, define the state-action value
function∈
Qπ i : S ×A →
R∈
π(t+1) argmax Q¯π(t) ,π 1 D (π ,π(t)) ,
for each s ,a by i,s ∈ h i,s i,s i− η ψ i,s i,s
∈S ∈A πi,s∈∆( Ai)(cid:26) (P(cid:27)
MD)
Qπ i(s,a),E π
"
Xt∞ =0γtr i(s t,a t) |s 0 =s,a 0 =a
#
. w anh der πe i,sw =e πus i(e ·|sth ),e ws hh eo rr eth ηan id
s
ano pt oa st ii to ivn es sπ tei( , pt s) si=
ze,
π Q¯i(
π
it ,)
(
s( t)·|s ∈)
We also define the value function Vπ : R for R |Ai| is a vector containing average state-action values for
each s by Vπ(s) , π(asi )Qπ(sS ,a)→ . For any π(t) atstates,andD ψ istheBregmandivergenceinducedby
∈ S i a | i a mirror map ψ : domψ R such that ∆( ) domψ,
initial state distribution ρ and a∈nyApolicy π Π, we use the → Ai ⊂
notation V iπ(ρ),E
s
ρ[V iπP (s)]. ∈ i.e. for any p,q ∈∆( Ai),
∼
D (p,q)=ψ(p) ψ(q) ψ(q),p q ,
ψ
− −h∇ − i
Nash equilibria. For any ε 0, a joint policy π =
≥ wherewesupposethroughoutthispaperthatthefunctionψis
(π ) Π is an ε-approximate Nash equilibrium for
thei i g∈aNme∈
Γ if for every i and every π Πi,
of Legendretype, i.e. strictly convexand essentially smooth
Vπi,π−i(ρ) Vπ i′,π−i(ρ) ε. W∈ hN en ε=0, such
ai′
po∈ licy π in the relative interior of domψ (see [17], section 26). We
i ≥ i − emphasize that the updatesare performedsimultaneously by
from which no agent has an incentive to deviate unilaterally,
all the agents.
is called a Nash equilibrium policy.
Remark IV.1. A standard mirror descent algorithm as in the
optimization literature would feature the gradients Vπ(ρ)
Markov Potential Game. A Markov game Γ is a Markov ∇ i
of the value functions in the inner product in the (PMD)
PotentialGame(MPG)ifthereexistsafunctionφ:
R s.t. for any i , and any policies (π ,π ),πS× ΠA , → update rule above. Recall that the policy gradients are given
V iπ i′,π−i(ρ) −∈ V iπN i,π−i(ρ)=Φπ i′,π−i(ρ) −i′ Φ− πi i,π−i∈ (ρ), (1) b sA iy ni g∂ l(∂ eπ eV -.i agi (π a g.( i eρ |s ns) e) te s= e[2 tt1 i1 n]−) g1 .γ ,S (d Piπ ρ m M(s i Dl) aQ )r¯ l cyπ i o( t rs o r, ea sti h p) e onf Po dMr ste D ove aar l sy g taos nri dt∈ h arm dS m, ina ii rt rh o∈ e
r
w toh tae lre poΦ teπ n( tρ ia) l, funE cs ti0 o∼ nρ, iπ nd[ uc∞ t e= d0 bγ yt tφ h( es ft u,a nct) ti] oi ns φth .e Ws eo- dc ea nll oe td e wde is thce ant da yl ng ao mrit ih cam llyen wh ea in gc he td edw dit ivh era gd ea np ct eiv :e Ep ar ce hco sn tad ti etio Bn ri en gg -
by φ , max P φ(s,a) . The identical-interest man divergence is weighted by the discounted visitation
max s ,a
case is an important∈ pS art∈ icA ul| ar case| of this definition. In this measure dπ ρ(s). We refer the reader to section 4, p.15-16
case in which all the reward functions are identical, it can in [21] for a more precise exposition of this observation in
be easily seen that the total potential function is the value the single-agent setting.
function of any of the players. Beyond this important case, The mirrormapchoice generatesa large class of PMD al-
conditions to obtain MPGs with non-identical rewards were gorithms.Inthiswork,we focusontwo concretealgorithms
identified in [15], [11], [24]. corresponding to the choice of two prominent mirror maps:
RemarkIII.1. ThedefinitionofMPGsweconsidercoincides 1) Projected Q-ascent. When the mirror map ψ is the
with the definitions used in [24], [23]. Compared to other squared ℓ 2-norm, the corresponding Bregman diver-
works [11], [5], our definition requires the total potential gence is the squared Euclidean distance and the re-
function to have a cumulative discounted sum form with a sulting algorithm is playerwise projected Q-ascent:
functionφ.SuchasumstructurenaturallyconnectsMPGsto π(t+1) =Proj (π(t)+ηQ¯π(t) ), (2)
their historical parent, the (stateless) potential game. Notice i,s ∆( Ai) i,s i,s
that this definition also capturesthe identical-interestsetting foreveryi ,s andProj istheprojection
in the same way. We will also comment later on about the operator
on∈ thN
e
sim∈ plS
ex ∆(
)∆
.
( Ai)
i
A
consequences of this definition in our results.2) Natural Policy Gradient. When the mirror map ψ A. Analysis of PMD with Euclidean Regularization
isthenegativeentropy,theBregmandivergenceD ψ is We makethe followingstandardassumptionensuringthat
theKullback-Leibler(KL)divergenceand(PMD)boils
all the states are visited.
down to independent Natural Policy Gradient which
updates for every i ,(s,a ) as follows, Assumption V.1. For every joint policy π Π and every
i i ∈
∈N ∈S×A state s ,dπ(s)>0.
π(t)(a s)exp(ηQ¯π(t)(s,a )) ∈S ρ
π(t+1)(a s)= i i | i i , (3) Since dπ(s) (1 γ)ρ(s) for every π Π,s ,
i i | Zi,s ρ ≥ − ∈ ∈ S
t Assumption V.1 is automatically satisfied when the initial
where Zi,s , π(t)(a s)exp(ηQ¯π(t)(s,a )), distribution ρ has full support.
and
thet
initial
joa ii n∈tAi poi
licy
i π|
(0) is
choi
sen in
i
the
The key step of our analysis consists in quantifying the
P potential function improvement between two consecutive
interior of ∆( ) . Algorithm (3) is also referred
|S|
A time steps of Algorithm (2). Since agents are updating their
to as a multiplicative weights update algorithm. We
policy simultaneously, the challenge is to establish a joint
remark that the averaged Q-function in (3) (including
in Zi,s) can be replaced by the averaged advantage policyimprovement.Thefollowinglemmacharacterizesthis
functt ionA¯π(t) definedforeverys ,i ,a improvement.
i ∈S ∈N i ∈Ai
by Proposition V.2 (Potential Improvement- Euclidean PMD).
A¯π(t) (s,a ),Q¯π(t) (s,a ) Vπ(t) (s). (4) Under Assumption V.1, for any µ ∆( ),t 1, we have
i i i i − i ∈ S ≥
Information setting. Throughoutthis work, we assume that 1 φ N
Φπ(t+1)
(µ)
Φπ(t)
(µ)
max i=1|Ai
|
e fua nch ctioag ne Qn ¯t π(i
t) ∈ at eN
achha its era ac tic oe nss tt oo
f
tt hh eeir algav oe rir ta hg me .Q B- ev ya ol nu de − ≥ 2η(1 −γ) − (1 P−γ)2 !·
i N
thisoracle-basedfeedbacksetting,theaveragedQ-valuefunc- dπ(t+1) (s) π(t+1) π(t) 2.
tionscanalsobeestimatedindependently.Indeed,eachagent µ k i,s − i,sk
s i=1
can resort to payoff-based methods to estimate their own X∈S X
Proof sketch (of PropositionV.2). Thefirst step consistsin
average Q-value function relying uniquely on their received
using the performance difference lemma (Lemma IX.4) to
rewards and without observing the policies or the actions of
relate the potential function change to the policy change:
the other agents. The extension of our guarantees to this
stochastic setting requires further investigation, especially Φπ(t+1) (µ) Φπ(t) (µ)
−
when using the KL regularization. We leave it for future dπ(t+1)(s)
work. = µ (π(t+1)(as) π(t)(as))Qπ(t) (s,a),
1 γ | − | φ
V. NASH REGRET ANALYSIS s ∈SX,a ∈A −
In this section, we provide Nash regret guarantees for where Qπ(t) is the Q-function induced by the reward func-
φ
the PMD algorithm we introduced in the previous section tion φ and the policy π(t). The second step consists in
when implemented either with Euclidean (2) or KL (3) connecting the joint policy change to the individual policy
regularization,withafocusonthedependenceonthenumber deviation to use the update rule of the algorithm. For this,
ofplayers.Followingpriorwork(seee.g.[5],[23]),tomake we recall that the joint policies are product policies across
ouranalysisprecise,wefirstdefinethenotionofNashregret agents and use the following decomposition:
for every time horizon T 1 as follows:
≥
π(t+1)(as) π(t)(as)
| − |
T
Nash-regret(T), T1 Xt=1m
i
∈a Nx πm
i′
∈a Πx iV iπ i′,π −(t i) (ρ) −V iπ(t) (ρ), = Xi=N 1(π i(t+1)(a
i
|s) −π i(t)(a
i
|s))π˜ −(t i)(a
−i
|s),
w oat vh ete i rr me te hπ es(t t se) tp a= tet( ∈π spi( {t a) 1 c, e,π ·−( ·t i)
·
.) , ITi ts
}
ft oh a le ln oj d woi ρ snt fi rsp oo t mhli ec ty hin io sif ti dath ele fid nN is itt ir op ib nla uy t ti he o ar ns
t
w wh ite hre thπ˜ e−(t ci) o(a n− vei | ns t) ion, Q0 j=i j− = 11 1 zπ jj(t =+1) 1(a aj n|s d) QN j N j= =i N+1 +π 1j( zt j)(a =j |s 1)
S for any sequence of nonnegative reals (z j). The rest of the
Nash-regret(T) 0 for every T 1. Furthermore, when Q Q
≥ ≥ prooffollowsfrompluggingthisdecompositionintothefirst
Nash-regret(T) ε for some accuracy ε > 0, there ex-
istst 1,
≤
,T
suchthatπ(t∗)
isanε-Nashequilibrium.
expression of the potential change above, using the update
∗ ∈{ ··· } rule of the algorithm to obtain a potential improvementand
At each time step t and for every player i , the
∈ N control the remaining error terms.
joint policy π(t) is compared to the policy where player i
The complete proof can be found in Appendix VIII.
unilaterally deviates to its best response to policy π(t). The
i Connecting the above policy improvement to the Nash
difference in value functions quantifies player i’s N−ash gap.
regret(see proofinAppendixVIII), weobtainthefollowing
TheNashregretcomputestheaverageovertheworstplayer’s
Nash regret guarantee.
Nash gap induced by the joint policy π(t) over the time
horizonT .Thecompleteproofsoftheresultsinthissection Theorem V.3 (PMD with Euclidean Regularization). Let
are deferred to the appendix. Assumption V.1 hold. Set η = 4φmax1 P−N iγ =1|Ai|. Then theindividualpolicies(πt) obtainedfromrunning B. Analysis of PMD with KL Regularization
i i ,t 1, ,T
PMD with Euclidean re∈gNula∈r{iza··t·ion}(2) and constant step
It has been shown in [6] that the iterates of independent
size η for T 1 iterations satisfy the following,
NaturalPolicyGradientwithsoftmaxpolicyparametrization
≥
convergetofixedpointpoliciesofthemultiplicativeweights
2φ2 κ˜ N algorithm (namely (3)). This asymptotic result required an
Nash-regret(T) 12 max ρ i=1|Ai |. (5)
≤ s (1 γ)4T assumptiononthesefixedpoints.Beforestatingthisassump-
−P
tion which we will also make to guarantee convergence,we
Hence, the number of iterations to achieve ε-Nash regret is: recall that the fixed points of (3) are the policies π Π
satisfyingeitherπ (a s)=0orA¯π(s,a )=0forevery∈ s
i i | i i ∈
T
288φ2 maxκ˜ ρ N i=1|Ai
|.
S,i ∈N,a i ∈Ai as can be readily seen in (3).
≥ (1 −γ P)4ε2 Assumption V.4. The fixed points of (3) are isolated.
Comparison to Prior Work. We provide a few comments NoticethatAssumptionV.4hasbeenmadeinseveralprior
regardingTheorem V.3 in the light of the existing literature: works [6], [23], [20].
Similarly to the previous section, we start by quantifying
Ourresultimprovesovertheconvergenceratesprovided
• the potential improvementfor our algorithm.
in [5] in terms of the distribution mismatch coeffi-
cientκ˜ whichcanscalewiththestatespacesize in PropositionV.5(PotentialImprovement-KLPMD). Under
ρ
|S|
the worst case, reducing the dependencefrom κ˜4 to κ˜ Assumption V.1, for any µ ∆( ),t 1, we have
ρ ρ ∈ S ≥
(see Table I).
M boo ur ne dov pe rr o, vth idis edrat fe orm ta ht ech se ps et ch ie ficO i(cid:18)deκ˜ nρ ( tN 1 ic−m i a∈ γ la N )x 4 i| nεA 2 tei| r(cid:19)estre cg ar se et Φπ(t+1) (µ) −Φπ(t) (µ) ≥ η1 − φ (1ma −x√ γ)N 2 !·
in [5] (Theorem2)whereφ max =1 since theirrewards dπ(t+1) (s)KL(π(t+1) π(t))+1 dπ(t+1) (s) N logZi,s.
are bounded in [0,1]. This closes the gap between the µ s || s η µ t
purelyidenticalinterestcaseandthemoregeneralMPG Xs ∈S s X∈S Xi=1
setting. However, this result leverages the fact that the Moreover, if η (1 −γ)2 , then
potentialfunctionis an expecteddiscountedcumulative ≤ φmax√N
sumof‘state-wise’potentialfunctions.Whiletheresult N
1
in [5] applies to a more general definition of MPGs Φπ(t+1) (µ) Φπ(t) (µ) dπ(t+1) (s) logZi,s. (6)
− ≥ η µ t
which does not require the cumulative sum form of the s i=1
X∈S X
totalpotentialfunction,weimproveoverthisresultwith
Note that the potential improvement bound now features
this additional structure. Our result for the Euclidean a dependence on √N instead of N in comparison to Propo-
setting mainly serves as a comparison point for our
sition V.2. Crucially, this improvementallows to take larger
upcoming result for PMD under KL regularization.
step sizes for PMD with KL regularization which leads to
Overall, we follow a proof strategy similar to [5], [23].
our improved iteration complexity. Using Proposition V.5,
•
However,wehighlightthatcomparedto[5],werelyon
we connect the right-hand side of (6) to the Nash regret
a different decomposition of the policy improvement.
using Lemma 21 in [23] which is adapted and reported in
The refined decomposition we use was considered in a
Lemma IX.3 in the appendix.We obtain the following main
similar way in [3] in the context of stateless potential
result whose complete proof is deferred to Appendix IX.
games when analyzing an independent natural gradient
method with entropy regularization and in [23] when Theorem V.6 (PMD with KL Regularization). Let Assump-
analyzingsoftmaxgradientplay.Inthepresentwork,we tions V.1 and V.4 hold. Set η = 1 −γ . Then the
2φmax√N
addressthecase ofMPGsbeyondtheparticularcase of individualpolicies(πt) obtainedfromrunning
i i ,t 1, ,T
(stateless) potential games and we do not consider any Algorithm (3) for T 1∈Niter∈a{tio·n··s w}ith constant step size η
≥
additionalentropy regularization.Importantly,our goal satisfy the following regret guarantee,
is to learn Nash equilibria for an unregularized game
and our iteration complexity is of the order O(ε −2)
Nash-regret(T)
12φ2 maxκ˜ ρ√N
, (7)
compared to (ε −4) for the algorithm proposed in [3]. ≤s (1 γ)4cT
O −
NoticethattheiterationcomplexityinTheoremV.3scales
where c , mininf min π(t)(a s) > 0.
wlin ee aa nrl ay lyw zeith theth Ne an su hm reb ge rr eo tf inp cl ua ry re er ds bN y. PMIn Dth ue sin ne gxt Ks Lec rt ei go un -, i ∈Nt ∈Ns ∈Sa∗ i∈a ar ig ∈m Aa iPxQ¯π i(t)(s,ai) i ∗i|
larizationinsteadofEuclideanregularization.Inthiscase,we The number of iterations to achieve ε-Nash regret is:
show that the dependence on the number of players N can
12φ2 κ˜ √N
beimprovedto scale with√N. TheresultingNaturalPolicy T max ρ .
≥ (1 γ)4cε2
Gradient algorithm requires a different treatment, which we −
undertake in the following section. Some comments are in order about Theorem V.6:First and foremost, observe the improved √N depen- Extending the result to the stochastic setting where
• •
dence on the number players in the theorem instead average Q-functions are estimated from the immediate
of N in Theorem V.3 which uses Euclidean regulariza- rewards observed by the agents, i.e. designing payoff-
tion. based methods is a fruitful avenue for future work.
It is also worth noting that there is no dependence on While the extension for the Euclidean case is straight-
•
the size of the action space anymore. Furthermore, the forward, obtaining the result for the Natural Policy
dependence on the state space size is only indirect via Gradient seems more delicate as it is not immediate
the minimax distribution mismatch coefficient κ˜ . This to show that the constant c is indeed positive in the
ρ
offers the possibility of further extension of the result stochastic setting.
to the continuous state space setting. While our focus in this paper was on improving the
•
The positivity of the constant c has been previously dependence on the number of agents, scaling the algo-
•
shown in [23] using Assumption V.4. Relaxing this rithmtolargestatespacesisalsoanimportantchallenge.
assumption and removing the dependence on the con- Extending our results in this direction using function
stant c (which can be very small) is an interesting approximationmeritsfurtherinvestigation.Someresults
question left for future work. wererecentlydiscussedalongtheselinesforPMDwith
The proof of this result uses similar arguments to Euclidean regularization [5].
•
the proof of Theorem 5 in [23]. Our improved result
stems from a tighter policy improvement lemma than REFERENCES
Lemma 20 in [23]. More precisely, we control an error
[1] S. Aydın and C. Eksin. Networked policy gradient play in markov
term differently using similar techniques to the proofs potential games. In IEEE International Conference on Acoustics,
in [3] which is concerned with stateless (i.e. static) SpeechandSignalProcessing(ICASSP),pages 1–5,2023.
[2] S. Aydin and C. Eksin. Policy gradient play over time-varying
potential games enhanced with entropy regularization.
networksinmarkovpotential games. In202362ndIEEEConference
However,noticeherethat(a)we considerMPGswhich onDecisionandControl(CDC),pages1997–2002, 2023.
have stateless potential games as a particular case and [3] S. Cen, F. Chen, and Y. Chi. Independent natural policy gradient
methods for potential games: Finite-time global convergence with
(b) we are interested in natural policy gradient (PMD
entropy regularization. In 2022 IEEE 61st Conference on Decision
with KL regularization) without the additional entropic andControl(CDC),pages2833–2838, 2022.
regularization considered in [3]. [4] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The
complexity of computing a nash equilibrium. SIAM Journal on
In contrast to the single-agent setting where a unified
• Computing, 39(1):195–259, 2009.
analysis of PMD for different mirror maps has been [5] D. Ding, C.-Y. Wei, K. Zhang, and M. Jovanovic. Independent
developed [21], our analysis is tailored to the choice Policy Gradient for Large-Scale Markov Potential Games: Sharper
Rates, Function Approximation, and Game-Agnostic Convergence.
of the mirror map. Whether a unified analysis can be
In Proceedings of the 39th International Conference on Machine
performed remains an open question. Learning, pages5166–5220. PMLR,June2022. ISSN:2640-3498.
[6] R. Fox, S. M. Mcaleer, W. Overman, and I. Panageas. Independent
natural policy gradient always converges in markov potential games.
VI. CONCLUSION AND FUTURE WORK
In Proceedings of The 25th International Conference on Artificial
Intelligence andStatistics, 2022.
In this work, we introduced an independent PMD algo-
[7] X.Guo,X.Li,C.Maheshwari,S.Sastry,andM.Wu. Markovalpha-
rithm for MPGs which subsumes as particular cases the potentialgames:Equilibriumapproximationandregretanalysis.arXiv
projected Q-ascent and the celebrated Natural Policy Gra- preprintarXiv:2305.12553, 2023.
[8] A.Heliou,J.Cohen,andP.Mertikopoulos. Learningwithbanditfeed-
dient algorithms, corresponding to PMD with Euclidean
backinpotential games. Advances inNeuralInformation Processing
and entropic mirror maps respectively. We provided Nash Systems,30,2017.
regret guarantees for both algorithms showing an improved [9] P.Jordan,A.Barakat,andN.He. Independentlearninginconstrained
Markov potential games. In Proceedings of The 27th International
dependence on the number of agents when using Natural
Conference onArtificialIntelligence andStatistics, 2024.
Policy Gradient. Notably, the Nash regret guarantee for the [10] G. Lan. Policy mirror descent for reinforcement learning: Linear
latter algorithm is independent of the size of the action convergence, new sampling complexity, and generalized problem
classes. Mathematical programming,198(1):1059–1106, 2023.
spaceincontrastwiththeiterationcomplexityforPMDwith
[11] S. Leonardos, W. Overman, I. Panageas, and G. Piliouras. Global
Euclidean regularization. convergenceofmulti-agentpolicygradientinmarkovpotentialgames.
Thereareseveralinterestingdirectionsforfutureresearch: InInternational ConferenceonLearningRepresentations, 2022.
[12] C. Maheshwari, M. Wu, D. Pai, and S. Sastry. Independent and
Relaxing the assumption of isolated stationary policies decentralized learning in markov potential games. arXiv preprint
• isaninterestingquestionforfuturework.Regularization arXiv:2205.14590, 2022.
[13] J.R.Marden. Statebasedpotentialgames. Automatica,48(12):3075–
mightofferawaytogetridofthisassumptionaswellas
3088,2012.
the constant c as pursued in [23]. Achieving this while [14] D.MondererandL.S.Shapley.Potentialgames.Gamesandeconomic
alsoobtainingourimproveddependenceonthenumber behavior, 14(1):124–143, 1996.
[15] D. Narasimha, K. Lee, D. Kalathil, and S. Shakkottai. Multi-agent
of players is an interesting open question.
learning via markov potential games in marketplaces for distributed
Obtainingguaranteesonthelast-iterateofthealgorithm energy resources. In 2022 IEEE 61st Conference on Decision and
• instead of our average Nash regret result is an interest- Control(CDC),pages6350–6357. IEEE,2022.
[16] A. Ozdaglar, M. O. Sayin, and K. Zhang. Independent learning in
ingquestionto addressto ensurethatimplementingthe
stochastic games. Invited chapter for the International Congress of
last-iterate policies indeed leads to Nash equilibria. Mathematicians 2022(ICM2022),2022.[17] R. T. Rockafellar. Convex Analysis. Princeton University Press,
Princeton, 1970.
[18] L.S.Shapley.Stochasticgames.Proceedingsofthenationalacademy
ofsciences, 39(10):1095–1100, 1953.
[19] Z. Song, S. Mei, and Y. Bai. When can we learn general-sum
markovgames withalarge numberofplayers sample-efficiently? In
International Conference onLearningRepresentations, 2022.
[20] Y. Sun, T. Liu, R. Zhou, P. Kumar, and S. Shahrampour. Provably
fast convergence of independent natural policy gradient for markov
potential games. InThirty-seventh ConferenceonNeuralInformation
ProcessingSystems,2023.
[21] L.Xiao.Ontheconvergenceratesofpolicygradientmethods.Journal
ofMachine LearningResearch,23(282):1–36, 2022.
[22] W.Zhan, S.Cen, B. Huang, Y. Chen, J.D. Lee, and Y. Chi. Policy
mirror descent for regularized reinforcement learning: A generalized
framework with linear convergence. SIAM Journal on Optimization,
33(2):1061–1091, 2023.
[23] R.Zhang, J.Mei, B.Dai, D.Schuurmans, andN.Li. Onthe global
convergence rates of decentralized softmax gradient play in markov
potentialgames. AdvancesinNeuralInformationProcessingSystems,
35:1923–1935, 2022.
[24] R. C. Zhang, Z. Ren, and N. Li. Gradient play in stochastic games:
Stationarypointsandlocalgeometry. IFAC-PapersOnLine,55(30):73–
78,2022. 25thInternational Symposium onMathematical Theoryof
Networks andSystemsMTNS2022.
[25] Z. Zhou, Z. Chen, Y. Lin, and A. Wierman. Convergence rates for
localized actor-critic in networked Markov potential games. In Pro-
ceedings of the Thirty-Ninth Conference on Uncertainty in Artificial
Intelligence, pages2563–2573, 2023.
VII. ACKNOWLEDGMENTS
We thank the reviewers for their useful comments. This
workwassupportedbyanETHFoundationsofDataScience
(ETH-FDS) postdoctoral fellowship.VIII. PROOF OF THEOREMV.3 (EUCLIDEAN REGULARIZATION)
We divide the proof into two main steps:
1) The first step consists in estimating the policy improvement in terms of the potential function, i.e., lower bounding
the quantity Φπ(t+1)(µ) Φπ(t)(µ) where π(t+1) is the joint policy of the agents at time t+1 after one step of PMD
−
from the joint policy π(t).
2) In the second step, we relate the Nash regret to the policy improvement controlled in the first step and conclude.
Let µ be any initial distribution over the state space . Recall the definition of the total potential function Φ:Π R:
S →
Φπ(µ),E ∞ γtφ(s ,a ) . (8)
µ,π t t
" #
t=0
X
Define also the Q-functioninduced by the potential function φ and a given policy π over the joint state and action space
for every state-action pair (s,a) as follows:
∈S×A
Qπ(s,a),E ∞ γtφ(s ,a )s =s,a =a . (9)
φ π " t t | 0 0 #
t=0
X
Proposition VIII.1. For any initial distribution µ ∆( ),
∈ S
1 2φ N N
Φπ(t+1) (µ) Φπ(t) (µ) max i=1|Ai | dπ(t+1) (s) π(t+1)( s) π(t)( s) 2.
− ≥ 2η(1 γ) − (1 γ)3 µ k i ·| − i ·| k
− P− ! s i=1
X∈S X
Proof. Using the performance difference lemma (Lemma IX.4), we can first write
1
Φπ(t+1) (µ) Φπ(t) (µ)= dπ(t+1) (s)(π(t+1)(as) π(t)(as))Qπ(t) (s,a). (10)
− 1 γ µ | − | φ
− s ,a
∈SX∈A
Remark VIII.2. Notice here that our first step consists in using the performance difference lemma between the joint
policies π(t) and π(t+1) rather than the multi-agent form of the performance difference lemma for each agent i where
the policy of all the agents but the i-th one is fixed to π(t) (see Lemma 1 in [5]).
i
−
We then use the following decomposition of the policy increment:
N i N i 1 N
π(t+1)(as) π(t)(as)= π(t+1)(a s) π(t)(a s) − π(t+1)(a s) π(t)(a s) , (11)
| − |  j j | j j | − j j | j j | 
i=1 j=1 j=i+1 j=1 j=i
X Y Y Y Y
 
which can be verified by noticing that the policy π(t)(as) has the product form: π(t)(as) = N π(t)(a s) for every
integert,wherea=(a ) andstandardtelescoping.N| oticethatweusetheconvention | 0 z =i= 11 ani d i N| z =1
i 1 ≤i ≤N j=1 j Q j=N+1 j
for any sequence of nonnegative reals (z ) .
j j 0
≥ Q Q
Before proceeding with this decomposition, we introduce a few additional useful notations for every integer t 1,
≥
i ,s and a=(a ) ,
i i
∈N ∈S ∈N ∈A
i 1 N
π˜(t)(a s), − π(t+1)(a s) π(t)(a s), (12)
−i −i | j j | j j |
j=1 j=i+1
Y Y
Q˜π(t) (s,a ), π˜(t)(a s)Qπ(t) (s,a), (13)
φ,i i −i −i | φ
a−Xi∈A−i
Q¯π(t) (s,a ), π(t)(a s)Qπ(t) (s,a). (14)
φ,i i −i −i | φ
a−Xi∈A−i
Intherestoftheproof,wewillusetheshorthandnotationsQ˜(t)(s,a ),Q˜π(t)(s,a ),Q(t)(s,a),Qπ(t)(s,a),Q¯(t)(s,a ),
φ,i i φ,i i φ φ i i
Q¯π(t)(s,a ),d(t+1) ,dπ(t+1).
i i µ µPlugging (11) into (10), we obtain
N
1
Φπ(t+1) (µ) Φπ(t) (µ)= d(t+1)(s) (π(t+1)(a s) π(t)(a s))Q˜(t)(s,a )
− 1 γ µ i i | − i i | φ,i i
− s X∈S Xi=1a Xi∈Ai
N
1
= d(t+1)(s) (π(t+1)(a s) π(t)(a s))Q¯(t)(s,a )
1 γ µ i i | − i i | i i
− s X∈S Xi=1a Xi∈Ai
TermA
N
+ | 1 d(t+1)(s) (π({t+z1)(a s) π(t)(a s))(Q˜(t)(s,a} ) Q¯(t)(s,a )) . (15)
1 γ µ i i | − i i | φ,i i − i i
− s X∈S Xi=1a Xi∈Ai
TermB
We now control each one of|the terms above successively. {z }
1) Term A. This term brings policy improvement as a consequence of the update rule (2). This step is the same as in
the proof of Theorem 1 in [5]. The optimality condition for π(t+1) in the update rule (2) yields:
1
π(t+1)( s) π(t)( s),Q¯π(t) (s, ) πt+1( s) π(t)( s)) 2. (16)
h i ·| − i ·| i · iAi ≥ 2ηk i ·| − i ·| k
As a consequence, we have
N
1
Term A d(t+1)(s) π(t+1)( s) π(t)( s) 2. (17)
≥ 2η(1 γ) µ k i ·| − i ·| k
− s i=1
X∈S X
2) Term B. For this term we start by observing that for every joint policy π Π, every state s and every
∈ ∈ S
action a ,i ,
i i
∈A ∈N
Q¯π(s,a )=Q¯π (s,a ). (18)
i i φ,i i
This is a key observation for our proof which follows from the properties of a Markov Potential Game. Indeed, first,
it follows from the MPG definition that for every joint policy π and every s ,i ,a , we have
i i
∈S ∈N ∈A
∂Vπ(µ) ∂Φπ(µ)
i = . (19)
∂π (a s) ∂π (a s)
i i i i
| |
Then the policy gradient theorem [21, Eq. (18)] yields an expression for each one of the quantities in the above
identity:
∂Vπ(µ) 1 ∂Φπ(µ) 1
i = dπ(s)Q¯π(s,a ); = dπ(s)Q¯π (s,a ). (20)
∂π (a s) 1 γ µ i i ∂π (a s) 1 γ µ φ,i i
i i i i
| − | −
As a consequence,we obtain (18) under the assumption that dπ(s)>0 for every joint policy π and every s . We
µ ∈S
now turn to controlling the term B. For this purpose, we first control the difference between the two Q-functions in
term B as follows:
Q˜(t)(s,a ) Q¯(t)(s,a ) ( =a) Q˜(t)(s,a ) Q¯(t)(s,a )
| φ,i i − i i | | φ,i i − φ,i i |
( =b) (π˜(t)(a s) π(t)(a s))Qπ(t) (s,a)
(cid:12)
(cid:12) (cid:12)a−Xi∈A−i
−i −i | − −i −i | φ (cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) i 1 i 1 (cid:12) N
( =c)(cid:12) − π(t+1)(a s) − π(t)(a s) (cid:12) π(t)(a s)Qπ(t) (s,a)
(cid:12)  j j | − j j |  j j | φ (cid:12)
(cid:12) (cid:12)a−Xi∈A−i j Y=1 j Y=1 j= Yi+1 (cid:12)
(cid:12)
(cid:12) N  (cid:12)
(d)(cid:12)φ (cid:12)
(cid:12) max π(t+1)( s) π(t)( s) , (cid:12) (21)
≤ 1 γ k j ·| − j ·| k1
− j=1
X
where (a) follows from (18), (b) stems from the definitions (13)-(14), (c) from (12) and (d) can be proved by
decomposing the difference of products into a sum of differences to obtain the sum of the differences betweenindividual policies at the successive times t and t+1. Using the bound (21), we immediately obtain
2
N
φ
Term B max d(t+1)(s) π(t+1)( s) π(t)( s)
| |≤ (1 γ)2 µ k i ·| − i ·| k1 !
− s i=1
X∈S X
2
N
φ
max d(t+1)(s) π(t+1)( s) π(t)( s)
≤ (1 γ)2 µ |Ai |·k i ·| − i ·| k!
− s i=1
X∈S Xp
φ N N
max i=1|Ai | d(t+1)(s) π(t+1)( s) π(t)( s) 2, (22)
≤ (1 γ)2 µ k i ·| − i ·| k
P− s i=1
X∈S X
where the last inequality follows from the Cauchy-Schwartz inequality.
Combining (17) and (22) in (15), we obtain
1 φ N N
Φπ(t+1) (µ) Φπ(t) (µ) max i=1|Ai | d(t+1)(s) π(t+1)( s) π(t)( s) 2.
− ≥ 2η(1 −γ) − (1 P−γ)2 !
s
µ i=1k i ·| − i ·| k
X∈S X
This concludes the proof of the lemma.
The rest of the proof of the theorem follows the same lines as the proof of Theorem 1 in [5] upon noticing that we use
a sharper result for policy improvement, namely Lemma VIII.1. We provide a proof in the following for completeness.
WenowproveatechnicallemmathatwillhelpconnectingtheNashregretwiththepolicyimprovementthatwequantified
in Lemma VIII.1.
Lemma VIII.3. For every i ,π Π , and every integer t 1, we have for every s ,
∈N
i′
∈
i
≥ ∈S
2 max
hπ i′( ·|s) −π i(t)( ·|s),Q¯( it)(s, ·)
iAi ≤ η
+
p
1
−i ∈N
γ
|Ai | !kπ i(t+1)( ·|s) −π i(t)( ·|s) k. (23)
Moreover, if η 1 −γ , then
≤ √maxi∈N
|Ai|
3
π ( s) π(t)( s),Q¯(t)(s, ) π(t+1)( s) π(t)( s) . (24)
h i′ ·| − i ·| i · iAi ≤ ηk i ·| − i ·| k
Proof. Let t 1,i ,π Π . Then, we have
≥ ∈N
i′
∈
i
π ( s) π(t)( s),Q¯(t)(s, ) = π ( s) π(t+1)( s),Q¯(t)(s, ) + π(t+1)( s) π(t)( s),Q¯(t)(s, )
h i′ ·| − i ·| i · i h i′ ·| − i ·| i · i h i ·| − i ·| i · i
1
π ( s) π(t+1)( s),π(t+1)( s) π(t)( s)
≤ ηh i′ ·| − i ·| i ·| − i ·| i
+ π(t+1) ( s) π(t) ( s),Q¯(t) (s, ) , (25)
h i ·| − i ·| i · i
where the last step follows from using the first-order optimality condition for the optimization problem defining the update
rule (2). Then we use Cauchy-Schwarz inequality and notice that π ( s) π(t+1)( s) π ( s) π(t+1)( s) 2
k i′ ·| − i ·| k ≤ k i′ ·| − i ·| k1 ≤
and Q¯(t)(s, ) max /(1 γ) to conclude.
k i · k≤ i ∈N |Ai | −
p
We are now ready to bound the Nash regret. We start with the following series of inequalities, using i for the index of
the maximum over ,
NT
maxmaxVπ i′,π −(t i)
(ρ)
Vπ(t)
(ρ)
i π′ i − i
t=1 ∈N i
X
T
( =a)
1
1
γ
m πa ′x d ρπ i′,π −(t i) (s)(π i′(a
i
|s) −π i(t)(a
i
|s))Q¯( it)(s,a i)
− Xt=1 i s ∈SX,ai∈Ai
T
(b) 3 dπ i′,π −(t i) (s) π(t+1)( s) π(t)( s)
≤ η(1 γ) ρ k i ·| − i ·| k
− t=1s
XX∈S
= 3
T
d
ρπ i′,π −(t i)
(s) dπ(t+1)(s) dπ i′,π −(t i) (s) π(t+1)( s) π(t)( s)
η(1 γ) v udπ(t+1)(s) ν r ρ k i ·| − i ·| k
− Xt=1 Xs ∈Su
t
ν q
dπ
( ≤c) 3 rs ηu (p 1π ∈Π γ)(cid:13) (cid:13)3/νρ 2(cid:13) (cid:13)∞
v
T d ρπ i′,π −(t i) (s)
v
T dπ ν(t+1)(s) N kπ j(t+1)( ·|s) −π j(t)( ·|s) k2
− (cid:13) (cid:13) ut=1s ut=1s j=1
uXX∈S uXX∈S X
t t
dπ
3 sup ρ T
(d) π Π ν
r ∈ (cid:13) (cid:13)∞ 4η(1 γ)(Φπ(T+1)(ν) Φπ(1)(ν))
≤ η(1 γ(cid:13))3/2(cid:13) − −
− (cid:13) (cid:13) q
dπ
2sup ρ φ T
(e) π Π ν max
≤
6v
u
∈
η(1(cid:13) γ(cid:13))∞3
u
t
(cid:13) (cid:13)−(cid:13)
(cid:13)
(f) 2κ˜ φ T
6 ρ max , (26)
≤ sη(1 γ)3
−
where(a)followsfromthemulti-agentperformancedifferencelemma,(b)usesLemmaVIII.3,(c)holdsforanydistributionν
with full support on the state space , uses the inequality dπ(t+1)(s) (1 γ)ν(s) together with the Cauchy-Schwarz
S ν ≥ −
inequality and upperbounds the i-th term by the sum over the N players, (d) uses Lemma VIII.1, (e) follows from the
boundedness of the potential function φ and finally (f) stems from the definition of the constant κ˜ .
ρ
Finally, setting η = 4(φmax−φm1 − inγ )PN i=1|Ai|, we obtain:
2φ2 κ˜ N
Nash-regret(T) 12 max ρ i=1|Ai |. (27)
≤ s (1 γ)4T
−P
Hence, the iteration complexity to achieve ε-Nash regret is up to numerical constants:
φ2 κ˜ N
max ρ i=1|Ai | .
O (1 −Pγ)4ε2 !
Remark VIII.4. Notice that
κ˜ min(κ , ), (28)
ρ ρ
≤ |S|
whereκ ,sup dπ ρ .Observeforthisthatsettingν tobetheuniformdistributionoverthestatespace givesκ˜
ρ π ∈Π ρ S ρ ≤
sup dπ ρ (cid:13) wh(cid:13)e∞ reas ν =ρ yields κ˜ κ .
π ∈Π ν ≤|S(cid:13) (cid:13)| (cid:13) (cid:13) ρ ≤ ρ
(cid:13) (cid:13)∞
(cid:13) (cid:13)
(cid:13) (cid:13)IX. PROOF OF THEOREMV.6(KL REGULARIZATION)
Recall first from (3) that the update rule of our PMD algorithm with KL regularization can also be written as follows:
π(t)(a s)exp( η A¯π(t)(s,a ))
π i(t+1)(a i |s)= i i | Z1 i− ,sγ i i , (29)
t
with Zi,s , π(t) (a s)exp( η A¯π(t)(s,a )).
t ai∈Ai i i | 1 −γ i i
Remark IX.1. Notice that we introduceda (1 γ) factor here for convenience.With a slight abuse of notation, we also use
P −
in the rest of this section η as the stepsize, which is up to the constant 1 γ the same as the stepsize η in (3). Note that
we also use with a slight abuse of notation the same Zi,s notation even wh− en using the advantage function.
t
The first lemma establishes a policy improvement result for the natural policy gradient algorithm for MPGs. Crucially,
compared to Lemma 20 in [23], our result allows to take a larger step size. In particular, the dependence of the step size
on the number of agents is of the order of √N instead of N.
Proposition IX.2. Let Assumption V.1 hold. For any initial distribution µ ∆( ), the iterates of PMD with KL
∈ S
regularization (3) satisfy for every time step t 1,
≥
1 φ √N 1 N
Φπ(t+1) (µ) Φπ(t) (µ) max dπ(t+1) (s)KL(π(t+1) π(t)) + dπ(t+1) (s) logZi,s, (30)
− ≥ η − (1 γ)2 ! µ s || s η µ t
− s s i=1
X∈S X∈S X
where we recallthatZi,s = π(t)(a s)exp ηA¯( it)(s,ai) forevery i ,s andeveryintegert 1. Moreover,
t ai∈Ai i i |
(cid:18)
1 −γ
(cid:19)
∈N ∈S ≥
if η (1 −γ)2 , then P
≤ φmax√N
N
1
Φπ(t+1) (µ) Φπ(t) (µ) dπ(t+1) (s) logZi,s. (31)
− ≥ η µ t
s i=1
X∈S X
Proof. We first introduce a few additional notations for every s ,a=(a ) and i :
i i
∈S ∈N ∈A ∈N
Vπ(t) (s),E ∞ γtφ(s ,a )s =s , (32)
φ µ,π " t t | 0 #
t=0
X
Aπ(t) (s,a),Qπ(t)
(s,a)
Vπ(t)
(s), (33)
φ φ − φ
A˜π(t) (s,a ), π˜(t)(a s)Aπ(t) (s,a), (34)
φ,i i −i −i | φ
a−Xi∈A−i
A¯π(t) (s,a ), π(t) (a s)Aπ(t) (s,a). (35)
φ,i i −i −i | φ
a−Xi∈A−i
We will also use again the shorthand notation d(t+1) =dπ(t+1).
µ µ
Similarly to the proof of Lemma VIII.1 (and Lemma 20 in [23]), the performance difference lemma yields:
N
1
Φπ(t+1) (µ) Φπ(t) (µ)= d(t+1)(s) (π(t+1)(a s) π(t)(a s))A˜(t)(s,a )
− 1 γ µ i i | − i i | φ,i i
− Xs ∈S Xi=1a Xi∈Ai
N
1
= d(t+1)(s) (π(t+1)(a s) π(t)(a s))A¯(t)(s,a )
1 γ µ i i | − i i | i i
− Xs ∈S Xi=1a Xi∈Ai
TermA
N
+ | 1 d(t+1)(s) (π({t+z1)(a s) π(t)(a s))(A˜(t)(s,a} ) A¯(t)(s,a )), (36)
1 γ µ i i | − i i | φ,i i − i i
− s X∈S Xi=1a Xi∈Ai
TermB
where we used the shorthand n|otations A˜(t)(s,a )=A˜π(t)(s,a ),A¯(t)({sz,a )=A¯π(t)(s,a ). }
φ,i i φ,i i i i i i
We now control each one of the terms separately in what follows.
1) TermA.Forthisterm,ourtreatmentisthesameasin[23].Weprovideaproofhereforcompletenessandwealsoadd
a crucial precision in the proof that seemed to be used in [23] as a claim. This is the fact that A¯π(s,a )=A¯π (s,a )
i i φ,i ifor every π Π,s ,i ,a , i.e. the averaged advantage function A¯π coincides with the averaged
advantagefun∈ ctionin∈ ducS edb∈ y thN e poi te∈ ntiA ali functionA¯π (see (33)-(35) fora definitioni ).Thisresultis a consequence
φ,i
of the equality between the partial derivatives of the individual value functions and the partial derivative of the
potential function w.r.t. the individual policies entries, as implied by the definition of MPGs (see (19) and (20)).
Before proceeding,we provide a brief proofof this fact that will be usefullater on. Since Q¯π(s,a )=Q¯π(s,a ) (see
i i φ i
(18)) and Vπ(s) = π (a s)Q¯π(s,a ), we also have the desired equality A¯π(s,a ) = A¯π (s,a ) using the
definition
ofφ
the
averaga e′ id∈Aadi vai nta′i g|
e
fuφ nction′i
.
i i φ,i i
P
It follows from our update rule (29) that:
1 γ π(t+1)(a s)
A¯(t)(s,a )= − log i i | +logZi,s . (37)
i i η π(t) (a s) ! t !
i i |
Plugging (37) into the Term A immediately implies that
N
1
Term A= d(t+1)(s) (π(t+1)(a s) π(t)(a s))A¯(t)(s,a )
1 γ µ i i | − i i | i i
− s X∈S Xi=1a Xi∈Ai
N
( =a) 1 d(t+1)(s) π(t+1)(a s)A¯(t)(s,a )
1 γ µ i i | i i
− Xs ∈S Xi=1a Xi∈Ai
1 N π(t+1)(a s)
= d(t+1)(s) π(t+1)(a s) log i i | +logZi,s
η s X∈S µ Xi=1a Xi∈Ai i i | π i(t)(a i |s) ! t !
N N
1 1
= d(t+1)(s) KL(π(t+1)( s) π(t)( s))+ d(t+1)(s) logZi,s
η µ i ·| || i ·| η µ t
s i=1 s i=1
X∈S X X∈S X
N
1 1
= d(t+1)(s)KL(π(t+1)( s) π(t)( s))+ d(t+1)(s) logZi,s, (38)
η µ ·| || ·| η µ t
s s i=1
X∈S X∈S X
where(a)followsfromobservingthat π(t)(a s)A¯(t)(s,a )=0andthelastidentityfollowsfromtheadditivity
of the KL divergence for product
distribuai ti∈oAni
s.
i i | i i
P
2) TermB.Ourimprovementw.r.t.Theorem5in[23]comesfromthewaywecontrolthisterm.Thefollowingderivations
aresimilartopartsoftheproofsin[3],namelyAppendixAandtheendofAppendixB.1.Recallthoughthatthework
[3] which inspired our analysis only deals with (stateless) potential games and considers a different natural policy
gradient algorithm enhanced with entropy regularization. First of all, we have that
A˜(t)(s,a ) A¯(t)(s,a ) ( =a) A˜(t)(s,a ) A¯(t)(s,a )
| φ,i i − i i | | φ,i i − φ,i i |
( =b) E [A(t)(s,a ,a )] E [A(t)(s,a ,a )]
a−i∼π˜ −(t i) φ i −i − a−i∼π −(t i) φ i −i
( =c)(cid:12) (cid:12)E [Q(t)(s,a ,a )] E [Q(t)(s,a ,a )](cid:12) (cid:12)
(cid:12) a−i∼π˜ −(t i) φ i −i − a−i∼π −(t i) φ i −i (cid:12)
(d)(cid:12) (cid:12)
≤
(cid:12) (cid:12) kQ( φt)(s,a i, ·) k∞d TV(π˜ −(t i)( ·|s),π −(t i)( ·|s)) (cid:12) (cid:12)
(e) φ
maxd (π˜(t)( s),π(t)( s)), (39)
≤ 1 γ TV −i ·| −i ·|
−
where d (, ) is the total variation distance, (a) uses the identity A¯(t)(s,a ) = A¯(t)(s,a ), (b) stems from the
TV · · i i φ,i i
definitions (34) and (35), (c) is a consequence of the definition of the advantage function, (d) follows from the
standard inequality fdµ fdν f d (µ,ν) for any bounded measurable function f : Ω R and any
probability measures µΩ and ν− anΩ d finall≤ y (k e)k s∞ temT sV from observing that Q(t) (s,a , ) φ /(1 → γ).
Plugging the
above(cid:12) (cid:12)iR
nequality
iR
nto
Ter(cid:12)
(cid:12)m B, we obtain
k φ i · k∞ ≤ max −
N
1
Term B d(t+1)(s) π(t+1)(a s) π(t)(a s) A˜(t)(s,a ) A¯(t)(s,a )
| |≤ 1 γ µ | i i | − i i | |·| φ,i i − i i |
− Xs ∈S Xi=1a Xi∈Ai
N
φ
max d(t+1)(s) d (π˜(t)( s),π(t)( s)) π(t+1)( s) π(t)( s) . (40)
≤ (1 γ)2 µ TV −i ·| −i ·| ·k i ·| − i ·| k1
− s i=1
X∈S XThen we observe that
N
d (π˜(t)( s),π(t)( s)) π(t+1)( s) π(t)( s)
TV −i ·| −i ·| ·k i ·| − i ·| k1
i=1
X
(a) N 1 √N
d (π˜(t)( s),π(t)( s))2+ π(t+1)( s) π(t)( s) 2
≤ 2√N TV −i ·| −i ·| 2 k i ·| − i ·| k1 !
i=1
X
(b) N 1 √N
KL(π˜(t)( s) π(t)( s))+ KL(π(t+1)( s) π(t)( s))
≤ 2√N −i ·| || −i ·| 2 i ·| || i ·| !
i=1
X
(c) N 1 √N
KL(π(t+1)( s) π(t)( s))+ KL(π(t+1)( s) π(t)( s))
≤ 2√N ·| || ·| 2 i ·| || i ·| !
i=1
X
√N √N N
= KL(π(t+1)( s) π(t)( s))+ KL(π(t+1)( s) π(t)( s))
2 ·| || ·| 2 i ·| || i ·|
i=1
X
( =d)√NKL(π(t+1)( s) π(t)( s)), (41)
·| || ·|
where (a) stems from Young’s inequality ( xy x2 + √Ny2 for any x,y R), (b) is a consequence of Pinsker’s
≤ 2√N 2 ∈
inequality, (c) follows from observing that
N
KL(π˜(t)( s) π(t)( s))= KL(π(t+1)( s) π(t)( s)) KL(π(t+1)( s) π(t)( s)),
−i ·| || −i ·| j ·| || j ·| ≤ j ·| || j ·|
j<i j=1
X X
whereweusedthedefinitionofπ˜(t) in(12).Then,(d)usestheadditivityoftheKLdivergenceforproductdistributions.
i
Overall, plugging (41) into (40),−we obtain
φ √N
Term B max d(t+1)(s)KL(π(t+1)( s) π(t)( s)). (42)
| |≤ (1 γ)2 µ ·| || ·|
− s
X∈S
Combining (38) and (42) yields the desired inequality (30) and concludes the proof.
We now introduce an additional convenient notation for the Nash gap induced by a given policy π Π as follows:
∈
Nash-gap (π), maxVπ i′,π−i(ρ) Vπi,π−i(ρ), i , (43)
i π′ Πi i − i ∀ ∈N
i∈
Nash-gap(π),maxNash-gap (π). (44)
i
i
∈N
The next lemma connects the second term in the policy improvement (see Lemma IX.2) with the Nash equilibrium gap.
This lemma is a slightly refined version of Lemma 21 in [23].
Lemma IX.3. If η (1 γ)2, then for any initial distribution ν ∆( ),
≤ − ∈ S
N
3κ˜
Nash-gap(π(t))2 ρ dπ(t+1) (s)logZi,s, (45)
≤ cη2(1 γ) ν t
− i=1s
XX∈S
where we recall that c,mininf min π(t) (a s)>0.
i ∈Nt ∈Ns ∈Sa∗ i∈a ar ig ∈m Aa iPxQ¯π i(t)(s,ai) i ∗i|
Proof. We start by upper-bounding the Nash gap of every policy π(t) for every iteration t. As a first step, we use theperformance difference lemma for policy π(t), for any i and any policy π Πi to write:
∈N
i′
∈
Vπ i′,π −(t i)
(ρ)
Vπ(t)
(ρ)=
1 dπ i′,π −(t i)
(s)π (a
s)A¯π(t)
(s,a )
i − i 1 γ ρ i′ i | i i
− s ∈SX,ai∈Ai
1 dπ i′,π −(t i) (s)maxA¯π(t)
(s,a )
≤ 1 −γ
s
ρ ai∈Ai i i
X∈S
π′,π(t)
= 1 d ρi −i(s) dπ(t+1)(s) dπ i′,π −(t i) (s)maxA¯π(t) (s,a )
1 −γ
s
X∈Sv u
u
tdπ ν(t+1)(s)
q
ν r ρ ai∈Ai i i
κ˜ ρ dπ(t+1)(s) dπ i′,π −(t i) (s)maxA¯π(t) (s,a )
≤ (1 −pγ)3/2
s q
ν r ρ ai∈Ai i i
X∈S
2 1/2
1/2 ηmaxA¯π(t)(s,a )
≤
( η1 (1− −γ)
γ
p)3κ˜ /2ρ
s
d ρπ i′,π −(t i) (s)
! 
s
dπ ν(t+1) (s)

ai∈Ai
1
−i
γ
i
 
X∈S X∈S
  
  2 1/2  
κ˜ N ηmaxA¯π i(t)(s,a i)
≤ η√1
ρ
γ 
dπ ν(t+1) (s)

ai∈Ai
1 γ  
. (46)
p− i=1s −
XX∈S
  
   
Hence, we have shown so far that
2
κ˜
N ηmaxA¯π i(t)(s,a i)
Nash-gap(π(t))2
≤
η2(1ρ
γ)
dπ ν(t+1) (s)

ai∈Ai
1 γ 
. (47)
− i=1s −
XX∈S
 
 
To conclude the proof, we now use an inequality that appeared in the proof of Lemma 21 in [23] in p. 26 (in the arxiv
version), namely the second to last inequality in the proof:
2
N ηmaxA¯π i(t)(s,a i)
3
N
dπ ν(t+1) (s)

ai∈Ai
1 γ  ≤ c
dπ ν(t+1) (s)logZ ti,s. (48)
i=1s − i=1s
XX∈S
 
XX∈S
 
Combining (47) and (48) leads to the desired inequality of Lemma IX.3.
The last part of the proof follows the same lines as [23] (see p. 29 therein). Combining Lemma IX.2 with Lemma IX.3,
we obtain:
3κ˜
Nash-gap(π(t))2 ρ (Φπ(t+1) (ν) Φπ(t) (ν)). (49)
≤ cη(1 γ) −
−
As a consequence, we have
T
1 3κ˜ 6κ˜ φ
NE-gap(π(t))2 ρ (Φπ(T+1) (ν) Φπ(1) (ν)) ρ max (50)
T ≤ cη(1 γ)T − ≤ (1 γ)2cηT
t=1 − −
X
We conclude by using Jensen’s inequality and setting the step size η to its largest possible value, i.e., η = (1 −γ)2 :
2φmax√N
1/2
1 T 12φ2 κ˜ √N
Nash-regret(T) NE-gap(π(t))2 max ρ . (51)
≤ T ! ≤s (1 γ)4cT
t=1 −
X
A. Auxiliary Lemmas
The following lemma is standard and well-known in the literature (see e.g. [21]).
Lemma IX.4 (Performance Difference Lemma). Consider a Markov Decision Process ( , ,P,r,γ,µ). Then for any
S A
policies π,π Π,
′
∈ Vπ′ (µ) Vπ(µ)= 1 dπ′ (s)(π (as) π(as))Qπ(s,a),
− 1 γ µ ′ | − |
− s a
X∈S X∈Awhere for every policy π Π,Vπ(µ) and Qπ : R are the state-value function with initial distribution µ and
∈ S ×A →
state-action value function of policy π respectively.
Proof. See Lemma 1 in [21].
The nextlemma is a multi-agentversionof the aboveperformancedifferencelemmawhich wasused for examplein [11].
Lemma IX.5 (Multi-Agent Performance Difference Lemma (Single-Agent Deviation)). Consider a Markov game Γ =
, ,( ) ,P,(r ) ,γ with initial state distribution µ ∆( ). Then, for any agent i , any two policies
i i i i
h πN ,πS A Πi a∈nNd any join∈tNpolii cy π Π i of the other agents, w∈ e haS ve: ∈ N
i′ i
∈
−i
∈
−
V iπ i′,π−i(µ) −V iπi,π−i(µ)=
1
1
γ
d µπ i′,π−i(s)(π i′(a
i
|s) −π i(a
i
|s))Q¯ iπi,π−i(s,a i),
− s X∈Sa Xi∈Ai
where we use the same notations as in section III.
Proof. We provide a proof for completeness. Using Lemma IX.4, we have for every i ,π ,π Πi,π Π i,
∈N
i′ i
∈
−i
∈
−
V iπ i′,π−i(µ) −V iπi,π−i(µ)=
1
1
γ
d µπ i′,π−i(s)(π i′(a
i
|s)π −i(a
−i
|s) −π i(a
i
|s)π −i(a
−i
|s))Qπ i(s,a)
− s X∈Sa=(aiX,a−i)
∈A
=
1
1
γ
d µπ i′,π−i(s)(π i′(a
i
|s) −π i(a
i
|s)) π −i(a
−i
|s)Qπ i(s,(a i,a −i))
− s X∈Sa Xi∈Ai a−Xi∈A−i
=Q¯π i(s,ai)
=
1
1
γ
d µπ i′,π−i(s)(π i′(a
i
|s) −π i(a
i
|s))Q¯|π i(s,a i). {z }
− s X∈Sa Xi∈Ai