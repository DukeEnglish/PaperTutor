[
    {
        "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
        "authors": "Zeju QiuWeiyang LiuHaiwen FengZhen LiuTim Z. XiaoKatherine M. CollinsJoshua B. TenenbaumAdrian WellerMichael J. BlackBernhard Schölkopf",
        "links": "http://arxiv.org/abs/2408.08313v1",
        "entry_id": "http://arxiv.org/abs/2408.08313v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08313v1",
        "summary": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.",
        "updated": "2024-08-15 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08313v1"
    },
    {
        "title": "Understanding the Local Geometry of Generative Model Manifolds",
        "authors": "Ahmed Imtiaz HumayunIbtihel AmaraCandice SchumannGolnoosh FarnadiNegar RostamzadehMohammad Havaei",
        "links": "http://arxiv.org/abs/2408.08307v1",
        "entry_id": "http://arxiv.org/abs/2408.08307v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08307v1",
        "summary": "Deep generative models learn continuous representations of complex data\nmanifolds using a finite number of samples during training. For a pre-trained\ngenerative model, the common way to evaluate the quality of the manifold\nrepresentation learned, is by computing global metrics like Fr\\'echet Inception\nDistance using a large number of generated and real samples. However,\ngenerative model performance is not uniform across the learned manifold, e.g.,\nfor \\textit{foundation models} like Stable Diffusion generation performance can\nvary significantly based on the conditioning or initial noise vector being\ndenoised. In this paper we study the relationship between the \\textit{local\ngeometry of the learned manifold} and downstream generation. Based on the\ntheory of continuous piecewise-linear (CPWL) generators, we use three geometric\ndescriptors - scaling ($\\psi$), rank ($\\nu$), and complexity ($\\delta$) - to\ncharacterize a pre-trained generative model manifold locally. We provide\nquantitative and qualitative evidence showing that for a given latent, the\nlocal descriptors are correlated with generation aesthetics, artifacts,\nuncertainty, and even memorization. Finally we demonstrate that training a\n\\textit{reward model} on the local geometry can allow controlling the\nlikelihood of a generated sample under the learned distribution.",
        "updated": "2024-08-15 17:59:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08307v1"
    },
    {
        "title": "Towards Flexible Visual Relationship Segmentation",
        "authors": "Fangrui ZhuJianwei YangHuaizu Jiang",
        "links": "http://arxiv.org/abs/2408.08305v1",
        "entry_id": "http://arxiv.org/abs/2408.08305v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08305v1",
        "summary": "Visual relationship understanding has been studied separately in human-object\ninteraction(HOI) detection, scene graph generation(SGG), and referring\nrelationships(RR) tasks. Given the complexity and interconnectedness of these\ntasks, it is crucial to have a flexible framework that can effectively address\nthese tasks in a cohesive manner. In this work, we propose FleVRS, a single\nmodel that seamlessly integrates the above three aspects in standard and\npromptable visual relationship segmentation, and further possesses the\ncapability for open-vocabulary segmentation to adapt to novel scenarios. FleVRS\nleverages the synergy between text and image modalities, to ground various\ntypes of relationships from images and use textual features from\nvision-language models to visual conceptual understanding. Empirical validation\nacross various datasets demonstrates that our framework outperforms existing\nmodels in standard, promptable, and open-vocabulary tasks, e.g., +1.9 $mAP$ on\nHICO-DET, +11.4 $Acc$ on VRD, +4.7 $mAP$ on unseen HICO-DET. Our FleVRS\nrepresents a significant step towards a more intuitive, comprehensive, and\nscalable understanding of visual relationships.",
        "updated": "2024-08-15 17:57:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08305v1"
    },
    {
        "title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training",
        "authors": "Gengwei ZhangLiyuan WangGuoliang KangLing ChenYunchao Wei",
        "links": "http://arxiv.org/abs/2408.08295v1",
        "entry_id": "http://arxiv.org/abs/2408.08295v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08295v1",
        "summary": "In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA.",
        "updated": "2024-08-15 17:50:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08295v1"
    },
    {
        "title": "HeightLane: BEV Heightmap guided 3D Lane Detection",
        "authors": "Chaesong ParkEunbin SeoJongwoo Lim",
        "links": "http://arxiv.org/abs/2408.08270v1",
        "entry_id": "http://arxiv.org/abs/2408.08270v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08270v1",
        "summary": "Accurate 3D lane detection from monocular images presents significant\nchallenges due to depth ambiguity and imperfect ground modeling. Previous\nattempts to model the ground have often used a planar ground assumption with\nlimited degrees of freedom, making them unsuitable for complex road\nenvironments with varying slopes. Our study introduces HeightLane, an\ninnovative method that predicts a height map from monocular images by creating\nanchors based on a multi-slope assumption. This approach provides a detailed\nand accurate representation of the ground. HeightLane employs the predicted\nheightmap along with a deformable attention-based spatial feature transform\nframework to efficiently convert 2D image features into 3D bird's eye view\n(BEV) features, enhancing spatial understanding and lane structure recognition.\nAdditionally, the heightmap is used for the positional encoding of BEV\nfeatures, further improving their spatial accuracy. This explicit view\ntransformation bridges the gap between front-view perceptions and spatially\naccurate BEV representations, significantly improving detection performance. To\naddress the lack of the necessary ground truth (GT) height map in the original\nOpenLane dataset, we leverage the Waymo dataset and accumulate its LiDAR data\nto generate a height map for the drivable area of each scene. The GT heightmaps\nare used to train the heightmap extraction module from monocular images.\nExtensive experiments on the OpenLane validation set show that HeightLane\nachieves state-of-the-art performance in terms of F-score, highlighting its\npotential in real-world applications.",
        "updated": "2024-08-15 17:14:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08270v1"
    }
]