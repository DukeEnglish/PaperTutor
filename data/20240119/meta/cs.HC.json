[
    {
        "title": "Through the Looking-Glass: Transparency Implications and Challenges in Enterprise AI Knowledge Systems",
        "authors": "Karina Cortiñas-LorenzoSiân LindleyIda Larsen-LedetBhaskar Mitra",
        "links": "http://arxiv.org/abs/2401.09410v1",
        "entry_id": "http://arxiv.org/abs/2401.09410v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09410v1",
        "summary": "Knowledge can't be disentangled from people. As AI knowledge systems mine\nvast volumes of work-related data, the knowledge that's being extracted and\nsurfaced is intrinsically linked to the people who create and use it. When\nthese systems get embedded in organizational settings, the information that is\nbrought to the foreground and the information that's pushed to the periphery\ncan influence how individuals see each other and how they see themselves at\nwork. In this paper, we present the looking-glass metaphor and use it to\nconceptualize AI knowledge systems as systems that reflect and distort,\nexpanding our view on transparency requirements, implications and challenges.\nWe formulate transparency as a key mediator in shaping different ways of\nseeing, including seeing into the system, which unveils its capabilities,\nlimitations and behavior, and seeing through the system, which shapes workers'\nperceptions of their own contributions and others within the organization.\nRecognizing the sociotechnical nature of these systems, we identify three\ntransparency dimensions necessary to realize the value of AI knowledge systems,\nnamely system transparency, procedural transparency and transparency of\noutcomes. We discuss key challenges hindering the implementation of these forms\nof transparency, bringing to light the wider sociotechnical gap and\nhighlighting directions for future Computer-supported Cooperative Work (CSCW)\nresearch.",
        "updated": "2024-01-17 18:47:30 UTC",
        "id": 1
    },
    {
        "title": "Establishing Awareness through Pointing Gestures during Collaborative Decision-Making in a Wall-Display Environment",
        "authors": "Valérie MaquilDimitra AnastasiouHoorieh AfkariAdrien CoppensJohannes HermenLou Schwartz",
        "links": "http://dx.doi.org/10.1145/3544549.3585830",
        "entry_id": "http://arxiv.org/abs/2401.09324v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09324v1",
        "summary": "Sharing a physical environment, such as that of a wall-display, facilitates\ngaining awareness of others' actions and intentions, thereby bringing benefits\nfor collaboration. Previous studies have provided first insights on awareness\nin the context of tabletops or smaller vertical displays. This paper seeks to\nadvance the current understanding on how users share awareness information in\nwall-display environments and focusses on mid-air pointing gestures as a\nfoundational part of communication. We present a scenario dealing with the\norganization of medical supply chains in crisis situations, and report on the\nresults of a user study with 24 users, split into 6 groups of 4, performing\nseveral tasks. We investigate pointing gestures and identify three subtypes\nused as awareness cues during face-to-face collaboration: narrative pointing,\nloose pointing, and sharp pointing. Our observations show that reliance on\ngesture subtypes varies across participants and groups, and that sometimes\nvague pointing is sufficient to support verbal negotiations.",
        "updated": "2024-01-17 16:47:46 UTC",
        "id": 2
    },
    {
        "title": "Same Data, Diverging Perspectives: The Power of Visualizations to Elicit Competing Interpretations",
        "authors": "Cindy Xiong BearfieldLisanne van WeeldenAdam WaytzSteven Franconeri",
        "links": "http://arxiv.org/abs/2401.09289v1",
        "entry_id": "http://arxiv.org/abs/2401.09289v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09289v1",
        "summary": "People routinely rely on data to make decisions, but the process can be\nriddled with biases. We show that patterns in data might be noticed first or\nmore strongly, depending on how the data is visually represented or what the\nviewer finds salient. We also demonstrate that viewer interpretation of data is\nsimilar to that of 'ambiguous figures' such that two people looking at the same\ndata can come to different decisions. In our studies, participants read\nvisualizations depicting competitions between two entities, where one has a\nhistorical lead (A) but the other has been gaining momentum (B) and predicted a\nwinner, across two chart types and three annotation approaches. They either saw\nthe historical lead as salient and predicted that A would win, or saw the\nincreasing momentum as salient and predicted B to win. These results suggest\nthat decisions can be influenced by both how data are presented and what\npatterns people find visually salient.",
        "updated": "2024-01-17 15:43:12 UTC",
        "id": 3
    },
    {
        "title": "Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues",
        "authors": "Dominic PetrakThy Thy TranIryna Gurevych",
        "links": "http://arxiv.org/abs/2401.09248v1",
        "entry_id": "http://arxiv.org/abs/2401.09248v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09248v1",
        "summary": "The success of task-oriented and document-grounded dialogue systems depends\non users accepting and enjoying using them. To achieve this, recently published\nwork in the field of Human-Computer Interaction suggests that the combination\nof considering demographic information, user emotions and learning from the\nimplicit feedback in their utterances, is particularly important. However,\nthese findings have not yet been transferred to the field of Natural Language\nProcessing, where these data are primarily studied separately. Accordingly, no\nsufficiently annotated dataset is available. To address this gap, we introduce\nFEDI, the first English dialogue dataset for task-oriented document-grounded\ndialogues annotated with demographic information, user emotions and implicit\nfeedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data\nhave the potential to improve task completion and the factual consistency of\nthe generated responses and user acceptance.",
        "updated": "2024-01-17 14:52:26 UTC",
        "id": 4
    },
    {
        "title": "BibSonomy Meets ChatLLMs for Publication Management: From Chat to Publication Management: Organizing your related work using BibSonomy & LLMs",
        "authors": "Tom VölkerJan PfisterTobias KoopmannAndreas Hotho",
        "links": "http://dx.doi.org/10.1145/3627508.3638298",
        "entry_id": "http://arxiv.org/abs/2401.09092v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09092v1",
        "summary": "The ever-growing corpus of scientific literature presents significant\nchallenges for researchers with respect to discovery, management, and\nannotation of relevant publications. Traditional platforms like Semantic\nScholar, BibSonomy, and Zotero offer tools for literature management, but\nlargely require manual laborious and error-prone input of tags and metadata.\nHere, we introduce a novel retrieval augmented generation system that leverages\nchat-based large language models (LLMs) to streamline and enhance the process\nof publication management. It provides a unified chat-based interface, enabling\nintuitive interactions with various backends, including Semantic Scholar,\nBibSonomy, and the Zotero Webscraper. It supports two main use-cases: (1)\nExplorative Search & Retrieval - leveraging LLMs to search for and retrieve\nboth specific and general scientific publications, while addressing the\nchallenges of content hallucination and data obsolescence; and (2) Cataloguing\n& Management - aiding in the organization of personal publication libraries, in\nthis case BibSonomy, by automating the addition of metadata and tags, while\nfacilitating manual edits and updates. We compare our system to different LLM\nmodels in three different settings, including a user study, and we can show its\nadvantages in different metrics.",
        "updated": "2024-01-17 09:53:50 UTC",
        "id": 5
    }
]