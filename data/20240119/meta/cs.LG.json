[
    {
        "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
        "authors": "Lianghui ZhuBencheng LiaoQian ZhangXinlong WangWenyu LiuXinggang Wang",
        "links": "http://arxiv.org/abs/2401.09417v1",
        "entry_id": "http://arxiv.org/abs/2401.09417v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09417v1",
        "summary": "Recently the state space models (SSMs) with efficient hardware-aware designs,\ni.e., Mamba, have shown great potential for long sequence modeling. Building\nefficient and generic vision backbones purely upon SSMs is an appealing\ndirection. However, representing visual data is challenging for SSMs due to the\nposition-sensitivity of visual data and the requirement of global context for\nvisual understanding. In this paper, we show that the reliance of visual\nrepresentation learning on self-attention is not necessary and propose a new\ngeneric vision backbone with bidirectional Mamba blocks (Vim), which marks the\nimage sequences with position embeddings and compresses the visual\nrepresentation with bidirectional state space models. On ImageNet\nclassification, COCO object detection, and ADE20k semantic segmentation tasks,\nVim achieves higher performance compared to well-established vision\ntransformers like DeiT, while also demonstrating significantly improved\ncomputation & memory efficiency. For example, Vim is 2.8$\\times$ faster than\nDeiT and saves 86.8% GPU memory when performing batch inference to extract\nfeatures on images with a resolution of 1248$\\times$1248. The results\ndemonstrate that Vim is capable of overcoming the computation & memory\nconstraints on performing Transformer-style understanding for high-resolution\nimages and it has great potential to become the next-generation backbone for\nvision foundation models. Code is available at https://github.com/hustvl/Vim.",
        "updated": "2024-01-17 18:56:18 UTC",
        "id": 1
    },
    {
        "title": "Vlogger: Make Your Dream A Vlog",
        "authors": "Shaobin ZhuangKunchang LiXinyuan ChenYaohui WangZiwei LiuYu QiaoYali Wang",
        "links": "http://arxiv.org/abs/2401.09414v1",
        "entry_id": "http://arxiv.org/abs/2401.09414v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09414v1",
        "summary": "In this work, we present Vlogger, a generic AI system for generating a\nminute-level video blog (i.e., vlog) of user descriptions. Different from short\nvideos with a few seconds, vlog often contains a complex storyline with\ndiversified scenes, which is challenging for most existing video generation\napproaches. To break through this bottleneck, our Vlogger smartly leverages\nLarge Language Model (LLM) as Director and decomposes a long video generation\ntask of vlog into four key stages, where we invoke various foundation models to\nplay the critical roles of vlog professionals, including (1) Script, (2) Actor,\n(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,\nour Vlogger can generate vlogs through explainable cooperation of top-down\nplanning and bottom-up shooting. Moreover, we introduce a novel video diffusion\nmodel, ShowMaker, which serves as a videographer in our Vlogger for generating\nthe video snippet of each shooting scene. By incorporating Script and Actor\nattentively as textual and visual prompts, it can effectively enhance\nspatial-temporal coherence in the snippet. Besides, we design a concise mixed\ntraining paradigm for ShowMaker, boosting its capacity for both T2V generation\nand prediction. Finally, the extensive experiments show that our method\nachieves state-of-the-art performance on zero-shot T2V generation and\nprediction tasks. More importantly, Vlogger can generate over 5-minute vlogs\nfrom open-world descriptions, without loss of video coherence on script and\nactor. The code and model is all available at\nhttps://github.com/zhuangshaobin/Vlogger.",
        "updated": "2024-01-17 18:55:12 UTC",
        "id": 2
    },
    {
        "title": "Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text",
        "authors": "Mazal BethanyBrandon WherryEmet BethanyNishant VishwamitraPeyman Najafirad",
        "links": "http://arxiv.org/abs/2401.09407v1",
        "entry_id": "http://arxiv.org/abs/2401.09407v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09407v1",
        "summary": "With the recent proliferation of Large Language Models (LLMs), there has been\nan increasing demand for tools to detect machine-generated text. The effective\ndetection of machine-generated text face two pertinent problems: First, they\nare severely limited in generalizing against real-world scenarios, where\nmachine-generated text is produced by a variety of generators, including but\nnot limited to GPT-4 and Dolly, and spans diverse domains, ranging from\nacademic manuscripts to social media posts. Second, existing detection\nmethodologies treat texts produced by LLMs through a restrictive binary\nclassification lens, neglecting the nuanced diversity of artifacts generated by\ndifferent LLMs. In this work, we undertake a systematic study on the detection\nof machine-generated text in real-world scenarios. We first study the\neffectiveness of state-of-the-art approaches and find that they are severely\nlimited against text produced by diverse generators and domains in the real\nworld. Furthermore, t-SNE visualizations of the embeddings from a pretrained\nLLM's encoder show that they cannot reliably distinguish between human and\nmachine-generated text. Based on our findings, we introduce a novel system,\nT5LLMCipher, for detecting machine-generated text using a pretrained T5 encoder\ncombined with LLM embedding sub-clustering to address the text produced by\ndiverse generators and domains in the real world. We evaluate our approach\nacross 9 machine-generated text systems and 9 domains and find that our\napproach provides state-of-the-art generalization ability, with an average\nincrease in F1 score on machine-generated text of 19.6\\% on unseen generators\nand domains compared to the top performing existing approaches and correctly\nattributes the generator of text with an accuracy of 93.6\\%.",
        "updated": "2024-01-17 18:45:13 UTC",
        "id": 3
    },
    {
        "title": "Élivágar: Efficient Quantum Circuit Search for Classification",
        "authors": "Sashwat AnagolumNarges AlavisamaniPoulami DasMoinuddin QureshiEric KesslerYunong Shi",
        "links": "http://arxiv.org/abs/2401.09393v1",
        "entry_id": "http://arxiv.org/abs/2401.09393v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09393v1",
        "summary": "Designing performant and noise-robust circuits for Quantum Machine Learning\n(QML) is challenging -- the design space scales exponentially with circuit\nsize, and there are few well-supported guiding principles for QML circuit\ndesign. Although recent Quantum Circuit Search (QCS) methods attempt to search\nfor performant QML circuits that are also robust to hardware noise, they\ndirectly adopt designs from classical Neural Architecture Search (NAS) that are\nmisaligned with the unique constraints of quantum hardware, resulting in high\nsearch overheads and severe performance bottlenecks.\n  We present \\'Eliv\\'agar, a novel resource-efficient, noise-guided QCS\nframework. \\'Eliv\\'agar innovates in all three major aspects of QCS -- search\nspace, search algorithm and candidate evaluation strategy -- to address the\ndesign flaws in current classically-inspired QCS methods. \\'Eliv\\'agar achieves\nhardware-efficiency and avoids an expensive circuit-mapping co-search via\nnoise- and device topology-aware candidate generation. By introducing two\ncheap-to-compute predictors, Clifford noise resilience and Representational\ncapacity, \\'Eliv\\'agar decouples the evaluation of noise robustness and\nperformance, enabling early rejection of low-fidelity circuits and reducing\ncircuit evaluation costs. Due to its resource-efficiency, \\'Eliv\\'agar can\nfurther search for data embeddings, significantly improving performance.\n  Based on a comprehensive evaluation of \\'Eliv\\'agar on 12 real quantum\ndevices and 9 QML applications, \\'Eliv\\'agar achieves 5.3% higher accuracy and\na 271$\\times$ speedup compared to state-of-the-art QCS methods.",
        "updated": "2024-01-17 18:09:26 UTC",
        "id": 4
    },
    {
        "title": "Diverse Part Synthesis for 3D Shape Creation",
        "authors": "Yanran GuanOliver van Kaick",
        "links": "http://arxiv.org/abs/2401.09384v1",
        "entry_id": "http://arxiv.org/abs/2401.09384v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09384v1",
        "summary": "Methods that use neural networks for synthesizing 3D shapes in the form of a\npart-based representation have been introduced over the last few years. These\nmethods represent shapes as a graph or hierarchy of parts and enable a variety\nof applications such as shape sampling and reconstruction. However, current\nmethods do not allow easily regenerating individual shape parts according to\nuser preferences. In this paper, we investigate techniques that allow the user\nto generate multiple, diverse suggestions for individual parts. Specifically,\nwe experiment with multimodal deep generative models that allow sampling\ndiverse suggestions for shape parts and focus on models which have not been\nconsidered in previous work on shape synthesis. To provide a comparative study\nof these techniques, we introduce a method for synthesizing 3D shapes in a\npart-based representation and evaluate all the part suggestion techniques\nwithin this synthesis method. In our method, which is inspired by previous\nwork, shapes are represented as a set of parts in the form of implicit\nfunctions which are then positioned in space to form the final shape. Synthesis\nin this representation is enabled by a neural network architecture based on an\nimplicit decoder and a spatial transformer. We compare the various multimodal\ngenerative models by evaluating their performance in generating part\nsuggestions. Our contribution is to show with qualitative and quantitative\nevaluations which of the new techniques for multimodal part generation perform\nthe best and that a synthesis method based on the top-performing techniques\nallows the user to more finely control the parts that are generated in the 3D\nshapes while maintaining high shape fidelity when reconstructing shapes.",
        "updated": "2024-01-17 17:55:06 UTC",
        "id": 5
    }
]