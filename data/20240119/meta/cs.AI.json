[
    {
        "title": "Vlogger: Make Your Dream A Vlog",
        "authors": "Shaobin ZhuangKunchang LiXinyuan ChenYaohui WangZiwei LiuYu QiaoYali Wang",
        "links": "http://arxiv.org/abs/2401.09414v1",
        "entry_id": "http://arxiv.org/abs/2401.09414v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09414v1",
        "summary": "In this work, we present Vlogger, a generic AI system for generating a\nminute-level video blog (i.e., vlog) of user descriptions. Different from short\nvideos with a few seconds, vlog often contains a complex storyline with\ndiversified scenes, which is challenging for most existing video generation\napproaches. To break through this bottleneck, our Vlogger smartly leverages\nLarge Language Model (LLM) as Director and decomposes a long video generation\ntask of vlog into four key stages, where we invoke various foundation models to\nplay the critical roles of vlog professionals, including (1) Script, (2) Actor,\n(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,\nour Vlogger can generate vlogs through explainable cooperation of top-down\nplanning and bottom-up shooting. Moreover, we introduce a novel video diffusion\nmodel, ShowMaker, which serves as a videographer in our Vlogger for generating\nthe video snippet of each shooting scene. By incorporating Script and Actor\nattentively as textual and visual prompts, it can effectively enhance\nspatial-temporal coherence in the snippet. Besides, we design a concise mixed\ntraining paradigm for ShowMaker, boosting its capacity for both T2V generation\nand prediction. Finally, the extensive experiments show that our method\nachieves state-of-the-art performance on zero-shot T2V generation and\nprediction tasks. More importantly, Vlogger can generate over 5-minute vlogs\nfrom open-world descriptions, without loss of video coherence on script and\nactor. The code and model is all available at\nhttps://github.com/zhuangshaobin/Vlogger.",
        "updated": "2024-01-17 18:55:12 UTC",
        "id": 1
    },
    {
        "title": "Through the Looking-Glass: Transparency Implications and Challenges in Enterprise AI Knowledge Systems",
        "authors": "Karina Cortiñas-LorenzoSiân LindleyIda Larsen-LedetBhaskar Mitra",
        "links": "http://arxiv.org/abs/2401.09410v1",
        "entry_id": "http://arxiv.org/abs/2401.09410v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09410v1",
        "summary": "Knowledge can't be disentangled from people. As AI knowledge systems mine\nvast volumes of work-related data, the knowledge that's being extracted and\nsurfaced is intrinsically linked to the people who create and use it. When\nthese systems get embedded in organizational settings, the information that is\nbrought to the foreground and the information that's pushed to the periphery\ncan influence how individuals see each other and how they see themselves at\nwork. In this paper, we present the looking-glass metaphor and use it to\nconceptualize AI knowledge systems as systems that reflect and distort,\nexpanding our view on transparency requirements, implications and challenges.\nWe formulate transparency as a key mediator in shaping different ways of\nseeing, including seeing into the system, which unveils its capabilities,\nlimitations and behavior, and seeing through the system, which shapes workers'\nperceptions of their own contributions and others within the organization.\nRecognizing the sociotechnical nature of these systems, we identify three\ntransparency dimensions necessary to realize the value of AI knowledge systems,\nnamely system transparency, procedural transparency and transparency of\noutcomes. We discuss key challenges hindering the implementation of these forms\nof transparency, bringing to light the wider sociotechnical gap and\nhighlighting directions for future Computer-supported Cooperative Work (CSCW)\nresearch.",
        "updated": "2024-01-17 18:47:30 UTC",
        "id": 2
    },
    {
        "title": "Neural Contractive Dynamical Systems",
        "authors": "Hadi Beik-MohammadiSøren HaubergGeorgios ArvanitidisNadia FigueroaGerhard NeumannLeonel Rozo",
        "links": "http://arxiv.org/abs/2401.09352v1",
        "entry_id": "http://arxiv.org/abs/2401.09352v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09352v1",
        "summary": "Stability guarantees are crucial when ensuring a fully autonomous robot does\nnot take undesirable or potentially harmful actions. Unfortunately, global\nstability guarantees are hard to provide in dynamical systems learned from\ndata, especially when the learned dynamics are governed by neural networks. We\npropose a novel methodology to learn neural contractive dynamical systems,\nwhere our neural architecture ensures contraction, and hence, global stability.\nTo efficiently scale the method to high-dimensional dynamical systems, we\ndevelop a variant of the variational autoencoder that learns dynamics in a\nlow-dimensional latent representation space while retaining contractive\nstability after decoding. We further extend our approach to learning\ncontractive systems on the Lie group of rotations to account for full-pose\nend-effector dynamic motions. The result is the first highly flexible learning\narchitecture that provides contractive stability guarantees with capability to\nperform obstacle avoidance. Empirically, we demonstrate that our approach\nencodes the desired dynamics more accurately than the current state-of-the-art,\nwhich provides less strong stability guarantees.",
        "updated": "2024-01-17 17:18:21 UTC",
        "id": 3
    },
    {
        "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
        "authors": "Baoxiong JiaYixin ChenHuangyue YuYan WangXuesong NiuTengyu LiuQing LiSiyuan Huang",
        "links": "http://arxiv.org/abs/2401.09340v1",
        "entry_id": "http://arxiv.org/abs/2401.09340v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09340v1",
        "summary": "3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io .",
        "updated": "2024-01-17 17:04:35 UTC",
        "id": 4
    },
    {
        "title": "Large Language Models Are Neurosymbolic Reasoners",
        "authors": "Meng FangShilong DengYudi ZhangZijing ShiLing ChenMykola PechenizkiyJun Wang",
        "links": "http://arxiv.org/abs/2401.09334v1",
        "entry_id": "http://arxiv.org/abs/2401.09334v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09334v1",
        "summary": "A wide range of real-world applications is characterized by their symbolic\nnature, necessitating a strong capability for symbolic reasoning. This paper\ninvestigates the potential application of Large Language Models (LLMs) as\nsymbolic reasoners. We focus on text-based games, significant benchmarks for\nagents with natural language capabilities, particularly in symbolic tasks like\nmath, map reading, sorting, and applying common sense in text-based worlds. To\nfacilitate these agents, we propose an LLM agent designed to tackle symbolic\nchallenges and achieve in-game objectives. We begin by initializing the LLM\nagent and informing it of its role. The agent then receives observations and a\nset of valid actions from the text-based games, along with a specific symbolic\nmodule. With these inputs, the LLM agent chooses an action and interacts with\nthe game environments. Our experimental results demonstrate that our method\nsignificantly enhances the capability of LLMs as automated agents for symbolic\nreasoning, and our LLM agent is effective in text-based games involving\nsymbolic tasks, achieving an average performance of 88% across all tasks.",
        "updated": "2024-01-17 16:57:19 UTC",
        "id": 5
    }
]