[
    {
        "title": "Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text",
        "authors": "Mazal BethanyBrandon WherryEmet BethanyNishant VishwamitraPeyman Najafirad",
        "links": "http://arxiv.org/abs/2401.09407v1",
        "entry_id": "http://arxiv.org/abs/2401.09407v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09407v1",
        "summary": "With the recent proliferation of Large Language Models (LLMs), there has been\nan increasing demand for tools to detect machine-generated text. The effective\ndetection of machine-generated text face two pertinent problems: First, they\nare severely limited in generalizing against real-world scenarios, where\nmachine-generated text is produced by a variety of generators, including but\nnot limited to GPT-4 and Dolly, and spans diverse domains, ranging from\nacademic manuscripts to social media posts. Second, existing detection\nmethodologies treat texts produced by LLMs through a restrictive binary\nclassification lens, neglecting the nuanced diversity of artifacts generated by\ndifferent LLMs. In this work, we undertake a systematic study on the detection\nof machine-generated text in real-world scenarios. We first study the\neffectiveness of state-of-the-art approaches and find that they are severely\nlimited against text produced by diverse generators and domains in the real\nworld. Furthermore, t-SNE visualizations of the embeddings from a pretrained\nLLM's encoder show that they cannot reliably distinguish between human and\nmachine-generated text. Based on our findings, we introduce a novel system,\nT5LLMCipher, for detecting machine-generated text using a pretrained T5 encoder\ncombined with LLM embedding sub-clustering to address the text produced by\ndiverse generators and domains in the real world. We evaluate our approach\nacross 9 machine-generated text systems and 9 domains and find that our\napproach provides state-of-the-art generalization ability, with an average\nincrease in F1 score on machine-generated text of 19.6\\% on unseen generators\nand domains compared to the top performing existing approaches and correctly\nattributes the generator of text with an accuracy of 93.6\\%.",
        "updated": "2024-01-17 18:45:13 UTC",
        "id": 1
    },
    {
        "title": "Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating LLMs' Mathematical Competency through Ontology-guided Perturbations",
        "authors": "Pengfei HongDeepanway GhosalNavonil MajumderSomak AdityaRada MihalceaSoujanya Poria",
        "links": "http://arxiv.org/abs/2401.09395v1",
        "entry_id": "http://arxiv.org/abs/2401.09395v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09395v1",
        "summary": "Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness, in mathematical reasoning tasks, remains an open question. In\nresponse, we develop (i) an ontology of perturbations of maths questions, (ii)\na semi-automatic method of perturbation, and (iii) a dataset of perturbed maths\nquestions to probe the limits of LLM capabilities in mathematical reasoning\ntasks. These controlled perturbations span across multiple fine dimensions of\nthe structural and representational aspects of maths questions. Using GPT-4, we\ngenerated the MORE dataset by perturbing randomly selected five seed questions\nfrom GSM8K. This process was guided by our ontology and involved a thorough\nautomatic and manual filtering process, yielding a set of 216 maths problems.\nWe conducted comprehensive evaluation of both closed-source and open-source\nLLMs on MORE. The results show a significant performance drop across all the\nmodels against the perturbed questions. This strongly suggests that current\nLLMs lack robust mathematical skills and deep reasoning abilities. This\nresearch not only identifies multiple gaps in the capabilities of current\nmodels, but also highlights multiple potential directions for future\ndevelopment. Our dataset will be made publicly available at\nhttps://huggingface.co/datasets/declare-lab/GSM8k_MORE.",
        "updated": "2024-01-17 18:13:07 UTC",
        "id": 2
    },
    {
        "title": "Efficient slot labelling",
        "authors": "Vladimir Vlasov",
        "links": "http://arxiv.org/abs/2401.09343v1",
        "entry_id": "http://arxiv.org/abs/2401.09343v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09343v1",
        "summary": "Slot labelling is an essential component of any dialogue system, aiming to\nfind important arguments in every user turn. Common approaches involve large\npre-trained language models (PLMs) like BERT or RoBERTa, but they face\nchallenges such as high computational requirements and dependence on\npre-training data. In this work, we propose a lightweight method which performs\non par or better than the state-of-the-art PLM-based methods, while having\nalmost 10x less trainable parameters. This makes it especially applicable for\nreal-life industry scenarios.",
        "updated": "2024-01-17 17:08:36 UTC",
        "id": 3
    },
    {
        "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
        "authors": "Baoxiong JiaYixin ChenHuangyue YuYan WangXuesong NiuTengyu LiuQing LiSiyuan Huang",
        "links": "http://arxiv.org/abs/2401.09340v1",
        "entry_id": "http://arxiv.org/abs/2401.09340v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09340v1",
        "summary": "3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io .",
        "updated": "2024-01-17 17:04:35 UTC",
        "id": 4
    },
    {
        "title": "Large Language Models Are Neurosymbolic Reasoners",
        "authors": "Meng FangShilong DengYudi ZhangZijing ShiLing ChenMykola PechenizkiyJun Wang",
        "links": "http://arxiv.org/abs/2401.09334v1",
        "entry_id": "http://arxiv.org/abs/2401.09334v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09334v1",
        "summary": "A wide range of real-world applications is characterized by their symbolic\nnature, necessitating a strong capability for symbolic reasoning. This paper\ninvestigates the potential application of Large Language Models (LLMs) as\nsymbolic reasoners. We focus on text-based games, significant benchmarks for\nagents with natural language capabilities, particularly in symbolic tasks like\nmath, map reading, sorting, and applying common sense in text-based worlds. To\nfacilitate these agents, we propose an LLM agent designed to tackle symbolic\nchallenges and achieve in-game objectives. We begin by initializing the LLM\nagent and informing it of its role. The agent then receives observations and a\nset of valid actions from the text-based games, along with a specific symbolic\nmodule. With these inputs, the LLM agent chooses an action and interacts with\nthe game environments. Our experimental results demonstrate that our method\nsignificantly enhances the capability of LLMs as automated agents for symbolic\nreasoning, and our LLM agent is effective in text-based games involving\nsymbolic tasks, achieving an average performance of 88% across all tasks.",
        "updated": "2024-01-17 16:57:19 UTC",
        "id": 5
    }
]