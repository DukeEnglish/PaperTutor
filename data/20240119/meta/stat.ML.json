[
    {
        "title": "Randomized Kaczmarz with geometrically smoothed momentum",
        "authors": "Seth J. AldermanRoan W. LuikartNicholas F. Marshall",
        "links": "http://arxiv.org/abs/2401.09415v1",
        "entry_id": "http://arxiv.org/abs/2401.09415v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09415v1",
        "summary": "This paper studies the effect of adding geometrically smoothed momentum to\nthe randomized Kaczmarz algorithm, which is an instance of stochastic gradient\ndescent on a linear least squares loss function. We prove a result about the\nexpected error in the direction of singular vectors of the matrix defining the\nleast squares loss. We present several numerical examples illustrating the\nutility of our result and pose several questions.",
        "updated": "2024-01-17 18:55:24 UTC",
        "id": 1
    },
    {
        "title": "Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter Paradigm for Performance Estimation in Online and Static Settings",
        "authors": "Kevin SloteElaine Lee",
        "links": "http://arxiv.org/abs/2401.09376v1",
        "entry_id": "http://arxiv.org/abs/2401.09376v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09376v1",
        "summary": "In the realm of machine learning and statistical modeling, practitioners\noften work under the assumption of accessible, static, labeled data for\nevaluation and training. However, this assumption often deviates from reality\nwhere data may be private, encrypted, difficult- to-measure, or unlabeled. In\nthis paper, we bridge this gap by adapting the Hui-Walter paradigm, a method\ntraditionally applied in epidemiology and medicine, to the field of machine\nlearning. This approach enables us to estimate key performance metrics such as\nfalse positive rate, false negative rate, and priors in scenarios where no\nground truth is available. We further extend this paradigm for handling online\ndata, opening up new possibilities for dynamic data environments. Our\nmethodology involves partitioning data into latent classes to simulate multiple\ndata populations (if natural populations are unavailable) and independently\ntraining models to replicate multiple tests. By cross-tabulating binary\noutcomes across ensemble categorizers and multiple populations, we are able to\nestimate unknown parameters through Gibbs sampling, eliminating the need for\nground-truth or labeled data. This paper showcases the potential of our\nmethodology to transform machine learning practices by allowing for accurate\nmodel assessment under dynamic and uncertain data conditions.",
        "updated": "2024-01-17 17:46:10 UTC",
        "id": 2
    },
    {
        "title": "High Confidence Level Inference is Almost Free using Parallel Stochastic Optimization",
        "authors": "Wanrong ZhuZhipeng LouZiyang WeiWei Biao Wu",
        "links": "http://arxiv.org/abs/2401.09346v1",
        "entry_id": "http://arxiv.org/abs/2401.09346v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09346v1",
        "summary": "Uncertainty quantification for estimation through stochastic optimization\nsolutions in an online setting has gained popularity recently. This paper\nintroduces a novel inference method focused on constructing confidence\nintervals with efficient computation and fast convergence to the nominal level.\nSpecifically, we propose to use a small number of independent multi-runs to\nacquire distribution information and construct a t-based confidence interval.\nOur method requires minimal additional computation and memory beyond the\nstandard updating of estimates, making the inference process almost cost-free.\nWe provide a rigorous theoretical guarantee for the confidence interval,\ndemonstrating that the coverage is approximately exact with an explicit\nconvergence rate and allowing for high confidence level inference. In\nparticular, a new Gaussian approximation result is developed for the online\nestimators to characterize the coverage properties of our confidence intervals\nin terms of relative errors. Additionally, our method also allows for\nleveraging parallel computing to further accelerate calculations using multiple\ncores. It is easy to implement and can be integrated with existing stochastic\nalgorithms without the need for complicated modifications.",
        "updated": "2024-01-17 17:11:45 UTC",
        "id": 3
    },
    {
        "title": "Central Limit Theorem for Two-Timescale Stochastic Approximation with Markovian Noise: Theory and Applications",
        "authors": "Jie HuVishwaraj DoshiDo Young Eun",
        "links": "http://arxiv.org/abs/2401.09339v1",
        "entry_id": "http://arxiv.org/abs/2401.09339v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09339v1",
        "summary": "Two-timescale stochastic approximation (TTSA) is among the most general\nframeworks for iterative stochastic algorithms. This includes well-known\nstochastic optimization methods such as SGD variants and those designed for\nbilevel or minimax problems, as well as reinforcement learning like the family\nof gradient-based temporal difference (GTD) algorithms. In this paper, we\nconduct an in-depth asymptotic analysis of TTSA under controlled Markovian\nnoise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA\ninfluenced by the underlying Markov chain, which has not been addressed by\nprevious CLT results of TTSA only with Martingale difference noise. Building\nupon our CLT, we expand its application horizon of efficient sampling\nstrategies from vanilla SGD to a wider TTSA context in distributed learning,\nthus broadening the scope of Hu et al. (2022). In addition, we leverage our CLT\nresult to deduce the statistical properties of GTD algorithms with nonlinear\nfunction approximation using Markovian samples and show their identical\nasymptotic performance, a perspective not evident from current finite-time\nbounds.",
        "updated": "2024-01-17 17:01:08 UTC",
        "id": 4
    },
    {
        "title": "Mitigating distribution shift in machine learning-augmented hybrid simulation",
        "authors": "Jiaxi ZhaoQianxiao Li",
        "links": "http://arxiv.org/abs/2401.09259v1",
        "entry_id": "http://arxiv.org/abs/2401.09259v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09259v1",
        "summary": "We study the problem of distribution shift generally arising in\nmachine-learning augmented hybrid simulation, where parts of simulation\nalgorithms are replaced by data-driven surrogates. We first establish a\nmathematical framework to understand the structure of machine-learning\naugmented hybrid simulation problems, and the cause and effect of the\nassociated distribution shift. We show correlations between distribution shift\nand simulation error both numerically and theoretically. Then, we propose a\nsimple methodology based on tangent-space regularized estimator to control the\ndistribution shift, thereby improving the long-term accuracy of the simulation\nresults. In the linear dynamics case, we provide a thorough theoretical\nanalysis to quantify the effectiveness of the proposed method. Moreover, we\nconduct several numerical experiments, including simulating a partially known\nreaction-diffusion equation and solving Navier-Stokes equations using the\nprojection method with a data-driven pressure solver. In all cases, we observe\nmarked improvements in simulation accuracy under the proposed method,\nespecially for systems with high degrees of distribution shift, such as those\nwith relatively strong non-linear reaction mechanisms, or flows at large\nReynolds numbers.",
        "updated": "2024-01-17 15:05:39 UTC",
        "id": 5
    }
]