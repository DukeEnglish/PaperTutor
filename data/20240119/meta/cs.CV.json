[
    {
        "title": "GARField: Group Anything with Radiance Fields",
        "authors": "Chung Min KimMingxuan WuJustin KerrKen GoldbergMatthew TancikAngjoo Kanazawa",
        "links": "http://arxiv.org/abs/2401.09419v1",
        "entry_id": "http://arxiv.org/abs/2401.09419v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09419v1",
        "summary": "Grouping is inherently ambiguous due to the multiple levels of granularity in\nwhich one can decompose a scene -- should the wheels of an excavator be\nconsidered separate or part of the whole? We present Group Anything with\nRadiance Fields (GARField), an approach for decomposing 3D scenes into a\nhierarchy of semantically meaningful groups from posed image inputs. To do this\nwe embrace group ambiguity through physical scale: by optimizing a\nscale-conditioned 3D affinity feature field, a point in the world can belong to\ndifferent groups of different sizes. We optimize this field from a set of 2D\nmasks provided by Segment Anything (SAM) in a way that respects coarse-to-fine\nhierarchy, using scale to consistently fuse conflicting masks from different\nviewpoints. From this field we can derive a hierarchy of possible groupings via\nautomatic tree construction or user interaction. We evaluate GARField on a\nvariety of in-the-wild scenes and find it effectively extracts groups at many\nlevels: clusters of objects, objects, and various subparts. GARField inherently\nrepresents multi-view consistent groupings and produces higher fidelity groups\nthan the input SAM masks. GARField's hierarchical grouping could have exciting\ndownstream applications such as 3D asset extraction or dynamic scene\nunderstanding. See the project website at https://www.garfield.studio/",
        "updated": "2024-01-17 18:57:53 UTC",
        "id": 1
    },
    {
        "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
        "authors": "Lianghui ZhuBencheng LiaoQian ZhangXinlong WangWenyu LiuXinggang Wang",
        "links": "http://arxiv.org/abs/2401.09417v1",
        "entry_id": "http://arxiv.org/abs/2401.09417v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09417v1",
        "summary": "Recently the state space models (SSMs) with efficient hardware-aware designs,\ni.e., Mamba, have shown great potential for long sequence modeling. Building\nefficient and generic vision backbones purely upon SSMs is an appealing\ndirection. However, representing visual data is challenging for SSMs due to the\nposition-sensitivity of visual data and the requirement of global context for\nvisual understanding. In this paper, we show that the reliance of visual\nrepresentation learning on self-attention is not necessary and propose a new\ngeneric vision backbone with bidirectional Mamba blocks (Vim), which marks the\nimage sequences with position embeddings and compresses the visual\nrepresentation with bidirectional state space models. On ImageNet\nclassification, COCO object detection, and ADE20k semantic segmentation tasks,\nVim achieves higher performance compared to well-established vision\ntransformers like DeiT, while also demonstrating significantly improved\ncomputation & memory efficiency. For example, Vim is 2.8$\\times$ faster than\nDeiT and saves 86.8% GPU memory when performing batch inference to extract\nfeatures on images with a resolution of 1248$\\times$1248. The results\ndemonstrate that Vim is capable of overcoming the computation & memory\nconstraints on performing Transformer-style understanding for high-resolution\nimages and it has great potential to become the next-generation backbone for\nvision foundation models. Code is available at https://github.com/hustvl/Vim.",
        "updated": "2024-01-17 18:56:18 UTC",
        "id": 2
    },
    {
        "title": "TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion",
        "authors": "Yu-Ying YehJia-Bin HuangChangil KimLei XiaoThu Nguyen-PhuocNumair KhanCheng ZhangManmohan ChandrakerCarl S MarshallZhao DongZhengqin Li",
        "links": "http://arxiv.org/abs/2401.09416v1",
        "entry_id": "http://arxiv.org/abs/2401.09416v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09416v1",
        "summary": "We present TextureDreamer, a novel image-guided texture synthesis method to\ntransfer relightable textures from a small number of input images (3 to 5) to\ntarget 3D shapes across arbitrary categories. Texture creation is a pivotal\nchallenge in vision and graphics. Industrial companies hire experienced artists\nto manually craft textures for 3D assets. Classical methods require densely\nsampled views and accurately aligned geometry, while learning-based methods are\nconfined to category-specific shapes within the dataset. In contrast,\nTextureDreamer can transfer highly detailed, intricate textures from real-world\nenvironments to arbitrary objects with only a few casually captured images,\npotentially significantly democratizing texture creation. Our core idea,\npersonalized geometry-aware score distillation (PGSD), draws inspiration from\nrecent advancements in diffuse models, including personalized modeling for\ntexture information extraction, variational score distillation for detailed\nappearance synthesis, and explicit geometry guidance with ControlNet. Our\nintegration and several essential modifications substantially improve the\ntexture quality. Experiments on real images spanning different categories show\nthat TextureDreamer can successfully transfer highly realistic, semantic\nmeaningful texture to arbitrary objects, surpassing the visual quality of\nprevious state-of-the-art.",
        "updated": "2024-01-17 18:55:49 UTC",
        "id": 3
    },
    {
        "title": "Vlogger: Make Your Dream A Vlog",
        "authors": "Shaobin ZhuangKunchang LiXinyuan ChenYaohui WangZiwei LiuYu QiaoYali Wang",
        "links": "http://arxiv.org/abs/2401.09414v1",
        "entry_id": "http://arxiv.org/abs/2401.09414v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09414v1",
        "summary": "In this work, we present Vlogger, a generic AI system for generating a\nminute-level video blog (i.e., vlog) of user descriptions. Different from short\nvideos with a few seconds, vlog often contains a complex storyline with\ndiversified scenes, which is challenging for most existing video generation\napproaches. To break through this bottleneck, our Vlogger smartly leverages\nLarge Language Model (LLM) as Director and decomposes a long video generation\ntask of vlog into four key stages, where we invoke various foundation models to\nplay the critical roles of vlog professionals, including (1) Script, (2) Actor,\n(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,\nour Vlogger can generate vlogs through explainable cooperation of top-down\nplanning and bottom-up shooting. Moreover, we introduce a novel video diffusion\nmodel, ShowMaker, which serves as a videographer in our Vlogger for generating\nthe video snippet of each shooting scene. By incorporating Script and Actor\nattentively as textual and visual prompts, it can effectively enhance\nspatial-temporal coherence in the snippet. Besides, we design a concise mixed\ntraining paradigm for ShowMaker, boosting its capacity for both T2V generation\nand prediction. Finally, the extensive experiments show that our method\nachieves state-of-the-art performance on zero-shot T2V generation and\nprediction tasks. More importantly, Vlogger can generate over 5-minute vlogs\nfrom open-world descriptions, without loss of video coherence on script and\nactor. The code and model is all available at\nhttps://github.com/zhuangshaobin/Vlogger.",
        "updated": "2024-01-17 18:55:12 UTC",
        "id": 4
    },
    {
        "title": "POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images",
        "authors": "Antonin VobeckyOriane SiméoniDavid HurychSpyros GidarisAndrei BursucPatrick PérezJosef Sivic",
        "links": "http://arxiv.org/abs/2401.09413v1",
        "entry_id": "http://arxiv.org/abs/2401.09413v1",
        "pdf_url": "http://arxiv.org/pdf/2401.09413v1",
        "summary": "We describe an approach to predict open-vocabulary 3D semantic voxel\noccupancy map from input 2D images with the objective of enabling 3D grounding,\nsegmentation and retrieval of free-form language queries. This is a challenging\nproblem because of the 2D-3D ambiguity and the open-vocabulary nature of the\ntarget tasks, where obtaining annotated training data in 3D is difficult. The\ncontributions of this work are three-fold. First, we design a new model\narchitecture for open-vocabulary 3D semantic occupancy prediction. The\narchitecture consists of a 2D-3D encoder together with occupancy prediction and\n3D-language heads. The output is a dense voxel map of 3D grounded language\nembeddings enabling a range of open-vocabulary tasks. Second, we develop a\ntri-modal self-supervised learning algorithm that leverages three modalities:\n(i) images, (ii) language and (iii) LiDAR point clouds, and enables training\nthe proposed architecture using a strong pre-trained vision-language model\nwithout the need for any 3D manual language annotations. Finally, we\ndemonstrate quantitatively the strengths of the proposed model on several\nopen-vocabulary tasks: Zero-shot 3D semantic segmentation using existing\ndatasets; 3D grounding and retrieval of free-form language queries, using a\nsmall dataset that we propose as an extension of nuScenes. You can find the\nproject page here https://vobecant.github.io/POP3D.",
        "updated": "2024-01-17 18:51:53 UTC",
        "id": 5
    }
]