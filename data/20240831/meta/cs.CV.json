[
    {
        "title": "3D Whole-body Grasp Synthesis with Directional Controllability",
        "authors": "Georgios PaschalidisRomana WilschutDimitrije AntićOmid TaheriDimitrios Tzionas",
        "links": "http://arxiv.org/abs/2408.16770v1",
        "entry_id": "http://arxiv.org/abs/2408.16770v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16770v1",
        "summary": "Synthesizing 3D whole-bodies that realistically grasp objects is useful for\nanimation, mixed reality, and robotics. This is challenging, because the hands\nand body need to look natural w.r.t. each other, the grasped object, as well as\nthe local scene (i.e., a receptacle supporting the object). Only recent work\ntackles this, with a divide-and-conquer approach; it first generates a\n\"guiding\" right-hand grasp, and then searches for bodies that match this.\nHowever, the guiding-hand synthesis lacks controllability and receptacle\nawareness, so it likely has an implausible direction (i.e., a body can't match\nthis without penetrating the receptacle) and needs corrections through major\npost-processing. Moreover, the body search needs exhaustive sampling and is\nexpensive. These are strong limitations. We tackle these with a novel method\ncalled CWGrasp. Our key idea is that performing geometry-based reasoning \"early\non,\" instead of \"too late,\" provides rich \"control\" signals for inference. To\nthis end, CWGrasp first samples a plausible reaching-direction vector (used\nlater for both the arm and hand) from a probabilistic model built via\nraycasting from the object and collision checking. Then, it generates a\nreaching body with a desired arm direction, as well as a \"guiding\" grasping\nhand with a desired palm direction that complies with the arm's one.\nEventually, CWGrasp refines the body to match the \"guiding\" hand, while\nplausibly contacting the scene. Notably, generating already-compatible \"parts\"\ngreatly simplifies the \"whole.\" Moreover, CWGrasp uniquely tackles both right-\nand left-hand grasps. We evaluate on the GRAB and ReplicaGrasp datasets.\nCWGrasp outperforms baselines, at lower runtime and budget, while all\ncomponents help performance. Code and models will be released.",
        "updated": "2024-08-29 17:59:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16770v1"
    },
    {
        "title": "SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners",
        "authors": "Ziyu GuoRenrui ZhangXiangyang ZhuChengzhuo TongPeng GaoChunyuan LiPheng-Ann Heng",
        "links": "http://arxiv.org/abs/2408.16768v1",
        "entry_id": "http://arxiv.org/abs/2408.16768v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16768v1",
        "summary": "We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .",
        "updated": "2024-08-29 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16768v1"
    },
    {
        "title": "PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning",
        "authors": "Noor HusseinFahad ShamshadMuzammal NaseerKarthik Nandakumar",
        "links": "http://arxiv.org/abs/2408.16769v1",
        "entry_id": "http://arxiv.org/abs/2408.16769v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16769v1",
        "summary": "Medical vision-language models (Med-VLMs) trained on large datasets of\nmedical image-text pairs and later fine-tuned for specific tasks have emerged\nas a mainstream paradigm in medical image analysis. However, recent studies\nhave highlighted the susceptibility of these Med-VLMs to adversarial attacks,\nraising concerns about their safety and robustness. Randomized smoothing is a\nwell-known technique for turning any classifier into a model that is\ncertifiably robust to adversarial perturbations. However, this approach\nrequires retraining the Med-VLM-based classifier so that it classifies well\nunder Gaussian noise, which is often infeasible in practice. In this paper, we\npropose a novel framework called PromptSmooth to achieve efficient certified\nrobustness of Med-VLMs by leveraging the concept of prompt learning. Given any\npre-trained Med-VLM, PromptSmooth adapts it to handle Gaussian noise by\nlearning textual prompts in a zero-shot or few-shot manner, achieving a\ndelicate balance between accuracy and robustness, while minimizing the\ncomputational overhead. Moreover, PromptSmooth requires only a single model to\nhandle multiple noise levels, which substantially reduces the computational\ncost compared to traditional methods that rely on training a separate model for\neach noise level. Comprehensive experiments based on three Med-VLMs and across\nsix downstream datasets of various imaging modalities demonstrate the efficacy\nof PromptSmooth. Our code and models are available at\nhttps://github.com/nhussein/promptsmooth.",
        "updated": "2024-08-29 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16769v1"
    },
    {
        "title": "ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model",
        "authors": "Fangfu LiuWenqiang SunHanyang WangYikai WangHaowen SunJunliang YeJun ZhangYueqi Duan",
        "links": "http://arxiv.org/abs/2408.16767v1",
        "entry_id": "http://arxiv.org/abs/2408.16767v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16767v1",
        "summary": "Advancements in 3D scene reconstruction have transformed 2D images from the\nreal world into 3D models, producing realistic 3D results from hundreds of\ninput photos. Despite great success in dense-view reconstruction scenarios,\nrendering a detailed scene from insufficient captured views is still an\nill-posed optimization problem, often resulting in artifacts and distortions in\nunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction\nparadigm that reframes the ambiguous reconstruction challenge as a temporal\ngeneration task. The key insight is to unleash the strong generative prior of\nlarge pre-trained video diffusion models for sparse-view reconstruction.\nHowever, 3D view consistency struggles to be accurately preserved in directly\ngenerated video frames from pre-trained models. To address this, given limited\ninput views, the proposed ReconX first constructs a global point cloud and\nencodes it into a contextual space as the 3D structure condition. Guided by the\ncondition, the video diffusion model then synthesizes video frames that are\nboth detail-preserved and exhibit a high degree of 3D consistency, ensuring the\ncoherence of the scene from various perspectives. Finally, we recover the 3D\nscene from the generated video through a confidence-aware 3D Gaussian Splatting\noptimization scheme. Extensive experiments on various real-world datasets show\nthe superiority of our ReconX over state-of-the-art methods in terms of quality\nand generalizability.",
        "updated": "2024-08-29 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16767v1"
    },
    {
        "title": "CSGO: Content-Style Composition in Text-to-Image Generation",
        "authors": "Peng XingHaofan WangYanpeng SunQixun WangXu BaiHao AiRenyuan HuangZechao Li",
        "links": "http://arxiv.org/abs/2408.16766v1",
        "entry_id": "http://arxiv.org/abs/2408.16766v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16766v1",
        "summary": "The diffusion model has shown exceptional capabilities in controlled image\ngeneration, which has further fueled interest in image style transfer. Existing\nworks mainly focus on training free-based methods (e.g., image inversion) due\nto the scarcity of specific data. In this study, we present a data construction\npipeline for content-style-stylized image triplets that generates and\nautomatically cleanses stylized data triplets. Based on this pipeline, we\nconstruct a dataset IMAGStyle, the first large-scale style transfer dataset\ncontaining 210k image triplets, available for the community to explore and\nresearch. Equipped with IMAGStyle, we propose CSGO, a style transfer model\nbased on end-to-end training, which explicitly decouples content and style\nfeatures employing independent feature injection. The unified CSGO implements\nimage-driven style transfer, text-driven stylized synthesis, and text\nediting-driven stylized synthesis. Extensive experiments demonstrate the\neffectiveness of our approach in enhancing style control capabilities in image\ngeneration. Additional visualization and access to the source code can be\nlocated on the project page: \\url{https://csgo-gen.github.io/}.",
        "updated": "2024-08-29 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16766v1"
    }
]