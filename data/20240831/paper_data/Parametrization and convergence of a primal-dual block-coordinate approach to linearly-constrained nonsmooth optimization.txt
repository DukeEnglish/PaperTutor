PARAMETRIZATION AND CONVERGENCE OF A PRIMAL-DUAL
BLOCK-COORDINATE APPROACH TO LINEARLY-CONSTRAINED
NONSMOOTH OPTIMIZATION
OLIVIER BILENNE
Abstract. This note is concerned with the problem of minimizing a separable, convex,
composite (smooth and nonsmooth) function subject to linear constraints. We study a
randomized block-coordinate interpretation of the Chambolle-Pock primal-dual algorithm,
based on inexact proximal gradient steps. A specificity of the considered algorithm is its
robustness,asitconvergeseveninthe absenceofstrongduality orwhenthe linearprogram
is inconsistent. Using matrix preconditiong, we derive tight sublinear convergence rates
with and without duality assumptions and for both the convex and the strongly convex
settings. Our developments are extensions and particularizations of original algorithms
proposed by Malitsky (2019) and Luke and Malitsky (2018). Numerical experiments are
provided for an optimal transport problem of service pricing.
1. Introduction
In an p-agent network, we consider the problem
min g(x)+h(x) subject to Ax = b, (P)
x Rn
∈
where A Rm n, b Rm, and x = (x ,...,x ) Rn contains p decision variables x
× 1 p 1
Rn1,...,x∈ Rnp, eac∈ h locally assigned to one of th∈ e agents (n = p n ). The functions∈ h
p ∈ i=1 i
and g are assumed to be additively separable for the local variables, i.e.,
P
p p
h(x) = h (x ), g(x) = g (x ),
i i i i
i=1 i=1
X X
where localfunctions h ,...,h aresmoothanddifferentiable, andg ,...,g areconvex lower
1 p 1 p
semi-continuous (possibly non-smooth) functions with easily computable resolvents. We call
G(x) = p G (x )thetotalobjectivefunction, whereG (x ) = g (x )+h (x ),andwegiveA
i=1 i i i i i i i i
the block matrix structure A = (A A A ), with A Rm n1,...,A Rm np, so that
1 2 p 1 × p ×
the equaP lity constraint in (P) rewrites a· s·· p A x = b.∈ Problem (P) fin∈ ds application in
i=1 i i
numerous fields, including linear programming, optimal transport, composite minimization,
P
distributed optimization, and inverse problems.
(Olivier Bilenne) Laboratoire Informatique d’Avignon, Avignon, France
E-mail address: olivier.bilenne@univ-avignon.fr.
2020 Mathematics Subject Classification. 49M29,65Y20, 90C25, 49Q22.
Keywords. Saddle-point problems Primal-dual algorithms Proximal gradient methods Randomized
· · ·
coordinate methods Optimal transport.
·
This research was supported by the Gaspard Monge Program for optimization, operations research and
their interactions with data sciences (PGMO). It was done in part at the Department of Data Science and
Knowledge Engineering, Maastricht University, Netherlands, and in part at CERMICS, E´cole des Ponts
ParisTech,France. The author is now with Laboratoire Informatique d’Avignon.
1
4202
guA
92
]CO.htam[
1v42461.8042:viXraTo solve (P) we consider a semi-distributed algorithm in the spirit of the primal-dual
algorithm(PDA)ofChambolleandPock(2011). Ourdevelopments buildontheconvergence
studyissuedinMalitsky(2019)fortheprimal-dualmethodasappliedtoequality-constrained
convex optimization. In the original analysis, the problem is reformulated as an instance of
the more general constrained optimization problem
min h(x)+g(x) subject to x argminf(x˜), (P’)
x Rn ∈ x˜
∈
where the residual function f is given the quadratic form
1
f(x) = Ax b 2. (1)
2k − k
The primal-dual algorithm is then regarded as a variant of the accelerated proximal gradient
methodofTseng(2008). Anadvantageofthisviewpoint, ascomparedtheclassicprimal-dual
analysis, is that convergence can be guaranteed in the absence of a constraint qualification
for strong duality, or even when the constraint Ax = b is inconsistent and (P) admits no
solution, in which case (P’) remains feasible. We refer to Luke and Malitsky (2018) for a
thorough discussion on the benefits of this particular approach to solving (P).
Inthepresent workwerevisit thecoordinate-descent implementation ofthePDAproposed
in Luke and Malitsky (2018), extending the latter to random coordinate block selection, and
to composite objective functions with smooth and nonsmooth parts by implementing inexact
proximal gradient steps. The demand for proximal gradient methods, in particular, is high
in the practical appplications where proximal steps cannot be implemented exactly.
For the case of strongly convex objectives, we derive a specific stepsize sequence that leads
to accelerated convergence rates, which we obtain using suitable matrix preconditioners. To
our knowledge, these faster rates were, to date, only available for the centralized implemen-
tation of the algorithm. Numerical experiments are eventually reported for a simple problem
in optimal transport.
Notation. In the real space Rn, the identity matrix is denoted by I , the vector of ones
n
by 1 = (1,1,...,1), and diag(a ,...,a ) is the diagonal matrix with a ,...,a as diagonal
n 1 n 1 n
entries. For x,z Rn, we let x,z = n x z = x z denote the Euclidean inner product
∈ h i i=1 i i ⊤
of x and z, and x = x,x 1/2 the Euclidean norm of x. Given a symmetric and positive
definite matrix Λk k Rph p, wi e write xP = Λx,x 1/2 for x Rn. We call the set of
× Λ
∈ k k h i ∈ S
solutions of (P’) and call G its optimal value, so that G(x ) = G for all x .
∗ ∗ ∗ ∗
∈ S
2. Algorithm and main results
Before discussing the algorithm, we formulate the assumptions that are made in the intro-
duction on the convexity of g and the smoothness of h.
Assumption 2.1. For i = 1,...,p:
(i) The effective domain dom(g ) Rni is nonempty, closed, and convex. There exists a
i
symmetric, positive semi-defini⊆ te matrix Υ Rni ni such that g ( ) 1 2 is convex.
i ∈ × i · − 2k·kΥi
(ii) Function h is convex differentiable, and there exists a symmetric, positive semi-definite
i
matrix Λ Rni ni such that, for every x ,x˜ dom(r ),
i × i i i
∈ ∈
1
0 6 h (x˜ ) h (x ) h (x ),x˜ x 6 x˜ x 2 . (2)
i i − i i −h∇ i i i − i i 2k i − i kΛi
2We write Υ = diag(Υ ,...,Υ ), and Λ = diag(Λ ,...,Λ ).
1 p 1 p
We note that in the general convex problem the matrices Υ and Λ are positive semi-definite
(Υ,Λ < 0). Strong convexity of G implies as well the existence of a positive definite Υ 0.
≻
Under these assumptions, we consider the following iterative algorithm (Algorithm 1), in
which we use
1
proxM f (z) = (I +M −1∂f) −1z = argmin x ∈Rn f(x)+ 2kx −z k2 M
(cid:26) (cid:27)
to denote the (scaled) proximal operator associated with a function f and a symmetric and
positive definite scaling matrix M.
Algorithm 1: Primal-dual block coordinate descent
Parameters :P, Tk, σk
Initialization:x0 Rn, y0 = σ0(Ax0 b), u0 = Ax0 b
∈ − −
Output :xk
for k = 0,1,2,... do
1 select random block Bk(ω) 1,...,p
⊂ { }
for i = 1,...,p do
2 if i Bk(ω) then xk+1 = proxPiT ik xk (P Tk) 1( h (xk)+A yk)
∈ i gi i − i i − ∇ i i ⊤i
3 else xk+1 = xk
i i (cid:0) (cid:1)
4 uk+1 = uk +A(xk+1 xk)
−
5 yk+1 = yk +σkAP(xk+1 xk)+σk+1uk+1
−
At each step, Algorithm 1 proceeds on Line 2 to individual proximal gradient steps for the
primal vectors xk,...,xk along a randomly selected subset of coordinate direction blocks.
1 p
The full dual vector yk is then upated on Line 5, using the dual residual uk+1 = Axk+1 b
−
(recursively computed on Line 4) and a linear extrapolation term derived from the last two
primal iterates. The random block selection routine on Line 1 consists in drawing successive
subsets of coordinate block indices; this is done by simulating a Bernoulli process B : Ω
(2 1,...,p )N with underlying probability space (Ω, ,P), where the probability measure7→P
{ }
satisfies P(Bk = ) = 0 and P(i Bk B k) = P(i F Bk) = π > 0 at each step k and for any
− i
∅ ∈ | ∈
block index i 1,...,p . In the algorithm, a weighting matrix P = diag(P ,...,P ) 0 is
1 p
∈ { } ≻
combined with a sequence of block diagonal scaling matrices Tk = diag(Tk,...,Tk) where,
1 p
for i = 1,...,p, P := (1/π )I and Tk Rni ni is positive definite. The products P Tk are
i i ni i ∈ × i i
then used as a sequence of preconditioners for the proximal gradient steps.
It can be seen that the global implementation of Algorithm 1 obtained by setting p = 1
with P = I and σ σ yields the dual update rule yk+1 = yk+σ[A(2xk+1 xk) b], in which
n
≡ − −
case the algorithm then reduces to a simple application the PDA with parameter θ = 1. The
main differences between Algorithm 1 and the method studied in Luke and Malitsky (2018)
are (i) the proximal steps are not computed exactly, (ii) the primal vectors are updated
simultaneously in random subsets, and (iii) there is flexibility in the choice of the step size
sequences Tk and σk. Also, we consider two distinct implementations of Algorithm 1, with
either constant or decreasing stepsizes.
3Constant stepsize. With fixed parameters Tk T and σk σ, Algorithm 1 is a mere ex-
≡ ≡
tension of Luke and Malitsky (2018) to composite objective functions with updates involving
random blocks of coordinates. A suitable choice for the preconditioner T, then, is to set
I
T =
ni
+π Λ +σA A i = 1,...,p (3a)
i
τ
i i ⊤i i
i
for some nonegative parameters (τ ,...,τ ) and σ satisfying
1 p
1 I 1 I
diag
n1
+σA A ,...,
np
+σA A σΞ 0, (3b)
π τ
⊤1 1
π τ
⊤p p
− ≻
(cid:18) 1 1 p p (cid:19)
(cid:16) (cid:17) (cid:16) (cid:17)
where we define Ξ = (Ξ ), with Ξ = π A A /(π π ) and π = P( ω Ω i,j ω ).
ij ij i,j ⊤i j i j i,j
{ ∈ | ∈ }
We note that the matrix Ξ features nondiagonal terms only when the the coordinates are
updated by blocks, i.e., if π = 0 for some i = j.
i,j
6 6
Theorem 2.2 characterizes the convergence of Algorithm 1 when stepsize (3) is used. Its
statement involves a particular sequence sk = 1 k xl or, equivalently,
k l=1
k −1σlxl
P
k −1σlxl+1
sk = (I P) l=0 +P l=0 , (4)
n −
P
lk
=−
01σl
! P
lk
=−
01σl
!
which has the quality of a weighted time average of the primal iterates.
P P
Theorem 2.2 (Constant stepsize). Let sequence (xk) be issued by Algorithm 1 with parame-
k
ters Tk T,σk σ satisfying (3), and consider the sequence (sk) such that sk = 1 k xl.
≡ ≡ k k l=1
(i) If there exists a Lagrange multiplier for Problem (P’), then (xk) and (sk) almost surely
P
converge to a solution of (P’) and, almost surely, f(xk) f = o(1/k), f(sk) f =
∗ ∗
− −
O(1/k2).
(ii) If is a bounded set and h + g is bounded from below, then all limit points of (sk)
almS ost surely belong to with f(sk) f = O(1/k), and E[f(sk) f ] = o(1/k).
∗ ∗
S − −
Decreasing stepsizes. Accelerated convergence rates can be obtained when the objective
function is strongly convex, i.e., if Υ = diag(Υ ,...,Υ ) 0. In the strongly convex case, we
1 p
≻
set Tk = T/τk, and
α
σk = β, (5)
τk −
where α = σ 1 (ΞΥ 1P) and β = σ (ΛΥ 1P)α, in which σ ( ) denotes the spectral
−max − max − max
·
radius. Define
β
κ = = σ (ΛΥ 1P).
max −
α
Given an initial τ0 < 1/κ, the stepsize τk+1 is recursively computed from τk using
2
1 1 1 κ (τk)2 + 1+ 1 1 κ τk 1 2 1+2κ (τk)2 τk
2 πi − − 2 πi − − 4 πi −
τk+1 = max  r ,
i 1,...,p  (cid:0) (cid:1) 1(cid:16) +( 1 (cid:0) κ)τk (cid:1) κ(cid:17) (τk)2 (cid:0) (cid:1) 
∈{ }  πi − −  
(6)
 
 
 
The primal stepsize sequence (6) is shown in Section 4 to have the asymptotic behavior
(τk)2
τk+1 τk + = O (τk)3 ,
− 2
(cid:0) (cid:1)
4so that τk is O(1/k) decreasing with exact local convergence rate 2/k, and the stepsize σk
for the dual iterates increases as O(k). Alternatively, one can use a stepsize sequence given
by the polynomial computation rule
(τk)2 17 3 1 1
τk+1 = τk +γ δ2 + δ + +κ (τk)3 +γ 2δ + κ(τk)4, (7)
− 2 8 4 8 2
(cid:18) (cid:19) (cid:18) (cid:19)
where (γ 1) > 0 and δ = max 1 κ , which decreases as
− i ∈{1,...,p }{|πi − |}
1 ǫ
τk+1 6 τk − (τk)2 (8)
− 2
(cid:18) (cid:19)
on condition that τ0 is taken small enough. An actual upper bound on the initial stepsize
can be obtained by combining the conditions (C1), (C2) and (C3) derived in Section 4.
The following accelerated convergence rates for Algorithm 1 were derived by adjusting to
our distributed setting the discussions of (Malitsky, 2019, Section 4.2).
Theorem 2.3 (Strong convexity). Let Υ 0, and denote by x the solution of (P’).
∗
≻
Let (xk) be a sequence issued by Algorithm 1 with parameter σk given by (5) and with Tk =
k
P 2Υ/τk, where τk satisfies either (6) or (7). Consider the sequence (sk) such that sk =
− k
1 k xl.
k l=1
(i) If there exists a Lagrange multiplier for Problem (P’), then (xk) almost surely converges
P
to x ∗ with rate xk x ∗ P2T = O(1/k) and, almost surely, f(xk) f ∗ = o(1/k3),
k − k −
f(zk) f = O(1/k4), f(sk) f = O(1/k4).
∗ ∗
(ii) Otherw− ise, (sk) almost surely− converges to x with f(sk) f = O(1/k2), and E[f(sk)
∗ ∗
− −
f ] = o(1/k2).
∗
3. Convergence analysis
3.1. Interpretation as a proximal gradient algorithm. The proofs for Theorems 2.2
and 2.3 rest on rewriting Algorithm 1 as the (primal-only) proximal gradient algorithm given
as Algorithm 2, which shares similarities with the accelerated proximal gradient algorithm
of Tseng (2008), now used with stepsize θk σk/Sk, where Sk = k σl.
≡ l=0
Algorithm 2: Expression of Algorithm 1 as an accelerated proximal gradient
P
Parameters :P, Tk, σk
Initialization:x0 = s0 Rn, S 1 = 0, S0 = σ0
−
∈
Output :xk, sk
for k = 0,1,2,... do
1 zk = 1/Sk Sk 1sk +σkxk
−
2 select random block Bk(ω) 1,...,p
(cid:0) (cid:1)(cid:0) (cid:1) ⊂ { }
for i = 1,...,p do
3 if i Bk(ω) then xk+1 = proxPiT ik xk (P Tk) 1( h (xk)+Sk f(zk))
∈ i gi i − i i − ∇ i i ∇i
4 else xk+1 = xk
i i (cid:0) (cid:1)
5 sk+1 = zk + σk/Sk P xk+1 xk
−
6 Sk+1 = Sk +σk+1
(cid:0) (cid:1) (cid:0) (cid:1)
5The equivalence between Algorithms 1 and 2 can be shown by introducing a dual variable
yk Sk(Azk b). The dual variable allows us to write, on Line 3 of Algorithm 2,
≡ −
Sk f(zk) = A Sk(Azk b) = A yk. (9)
∇i ⊤i
−
⊤i
Line 5 also gives us
Sk(Ask+1 b) = yk +σkAP(xk+1 xk). (10)
− −
Algorithm 2-Line 1 then yields the dual update rule
yk+1 = Sk+1(Azk+1 b)
−
= A(Sksk+1 +σk+1xk+1) Sk+1b
− (11)
= Sk(Ask+1 b)+σk+1(Axk+1 b)
− −
(10)
= yk +σkAP(xk+1 xk)+σk+1uk+1,
−
where a second dual variable uk (Axk b) has been introduced, which satisfies
≡ −
uk+1 = uk +A(xk+1 xk). (12)
−
Algorithm1caneventuallyberecoveredafterinclusionof (9),(11),and(12)intoAlgorithm2.
Straightforward computations also lead to the expression previously given in (4) for the
sequence sk, which can be derived by induction on k. We note that the sequences sk and zk
are bounded whenever the algorithm produces a bounded sequence xk.
In Sections 3.2 to 3.5 we study the convergence of the sequences produced by Algorithm 2.
For analysis purposes, we consider the auxiliary sequence defined by
xˆk+1 = proxPTk xk (PTk) 1( h(xk)+Sk f(zk)) , (13)
g − − ∇ ∇
which would coincide with iterate (cid:0) xˆk+1 if all coordinates were to be ud(cid:1) pated at step k (i.e.,
in the event Bk = 1,...,p ).
{ }
3.2. ESO for block coordinate sampling. Let k := σ(x0,s0,z0,...,xk,sk,zk) denote
F
thesigmaalgebraoftheprocesshistoryuptostepk. TheupcomingpropertyforAlgorithm2
relates to the concept of expected separable overapproximation (ESO), as seen in Richt´arik
and Tak´aˇc (2014, 2016); Fercoq and Richt´arik (2015).
Lemma 3.1 (Proximal step). In Algorithm 2, x Rn,
∀ ∈
1
g(xˆk+1) g(x) 6 ζ(xˆk+1),x xˆk+1 6 ζ(x) ζ(xˆk+1) x xˆk+1 2 , (14)
− h∇ − i − − 2k − kPTk+Υ
where
ζ(x) = h(xk)+ h(xk),x xk +Sk f(zk),x zk + 1 x xk 2 . (15)
h∇ − i h∇ − i 2k − kPTk
Proof. Equation (13) rewrites as xˆk+1 = argmin g(x˜) + ζ(x˜) . Hence, 0 ∂g(xˆk+1) +
x˜
{ } ∈
ζ(xˆk+1), which yields thefirst inequality. The second inequality follows by strong convexity
∇
of g +ζ with modulus PTk +Υ. (cid:3)
63.3. Extrapolation. The next lemma characterizes the sequence (sk) as a convex combina-
tion of past primal iterates. It is an extension of Lemma 2 in Fercoq and Richt´arik (2015).
Lemma 3.2. In Algorithm 2, we have
k
sk = Γk,lxl, k > 1, (16)
l=0
X
where (Γk,l) is a sequence of diagonal matrices such that Γ1,0 = I P, Γ1,1 = P and,
n
−
for k > 1,
Sk 1Γk,l for l=0,...,k 1,
−
−
SkΓk+1,l= σk 1P σk(P I ) if l=k, (17)
− n
 − −
σkP if l=k+1.

Moreover, Γk+1,k = [Sk 1Γk,k σk(P I )]/Sk and k Γk,l = I .
− − − n l=0 n
Proof. We proceed by induction. By combining LinesP1 and 5 in Algorithm 2, we find
1
sk+1 = Sk 1sk +σkPxk+1 σk(P I )xk , (18)
Sk − − − n
which yields s1 = (I P)x0+P(cid:0) x1, and the values of Γ1,0 and Γ1,1.(cid:1) Suppose now that (16)
n
−
holds for k > 1, then we infer from (18) that
Sksk+1 = k Sk 1Γk,lxl +σkPxk+1 σk(P I )xk
l=0 − − − n
= Plk =− 01Sk −1Γk,lxl +Sk −1Γk,kxk +σkPxk+1 −σk(P −I n)xk (19)
= Plk =− 01Sk −1Γk,lxl +[Sk −1Γk,k −σk(P −I n)] xk +σkPxk+1.
Equation (17) followsPby inspection of (16) and (19). Hence, (16) holds for all k.
It can be checked that sk is a convex combination of x0,...,xk. Again, we do this by
induction. We already know that 1 Γ1,l = (I P)+P = I and, by (17), 2 Γ2,l =
l=0 n − n l=0
1/S1 S0Γ1,0 + σ0P σ1(P I ) + σ1P = 1/S1 S0 + σ1 I = I . Suppose now that
n n n
− − P P
k Γk,l = I is true for some k > 1, then
(cid:0) l=0 (cid:1)(cid:2) n (cid:3) (cid:0) (cid:1)(cid:2) (cid:3)
P k+1 Γk+1,l ( =17) 1/Sk Sk 1 k −1 Γk,l +σk 1P σk(P I )+σkP
− − n
l=0 l=0 − −
X (cid:0) (cid:1)h X = 1/Sk Sk −1(I n Γk,k)+iσk −1P +σkI n
−
( =17) 1/Sk Sk 1 I σk 1/Sk 1 P +σk 1P +σkI = 1/Sk Sk 1+σk I = I ,
− n − − (cid:0)− (cid:1)(cid:2) n − n (cid:3)n
−
and the(cid:0)last i(cid:1)d(cid:2)entity(cid:0)holds(cid:0)by inducti(cid:1)on(cid:1)on k. Observe th(cid:3)at t(cid:0)his la(cid:1)st(cid:2) result cou(cid:3)ld also be
obtained immediately by simple inspection of (4). (cid:3)
Now, define = (G ,...,G ) and Gˆk = ˆk,1 , where
1 p p
G hG i
k
ˆk = Γk,l (xl), k > 1. (20)
G G
l=0
X
By convexity, it follows from (16) and (20) that ˆk > (sk) and Gˆk > G(sk). We show the
G G
following result.
7Lemma 3.3. In Algorithm 2 and for any positive definite M = diag(M ,...,M ),
1 p
E[ xk+1 x 2 k] = xˆk+1 x 2 + xk x 2 , (21)
k − ∗ kPM|F k − ∗ kM k − ∗ k(P −In)M
E[Gˆk+1 k] = 1 [Sk 1Gˆk +σkG(xˆk+1)], (22)
|F Sk −
where xˆk+1 and Gˆk are defined as in (13) and (20).
Proof. For i = 1,...,p, We find,
E[ xk+1 x k]=π xˆk+1 x 2 +(1 π ) xk x 2
k i − ∗ikPiMi|F i k i − ∗ikPiMi − i k i − ∗ikPiMi (23)
= xˆk+1 x 2 +(π 1 1) xk x 2 .
k i − ∗ikMi i− − k i − ∗ikMi
Summing up the above for i = 1,...,p gives (21). Next, observe that E[G (xk+1) k] =
i
|F
π G (xˆk+1)+(1 π )G (xk) for i 1,...,p , which in matrix form rewrites as
i i i i
− ∈ { }
E[ (xk+1) k] = P 1 (xˆk+1)+(I P 1) (xk). (24)
− n −
G |F G − G
It follows that
E[ˆk+1 k]( =20) k −1Γk+1,l (xl)+Γk+1,k (xk)+Γk+1,k+1E[ (xk+1) k]
G |F l=0 G G G |F
( =17) S Sk− k1 Plk
=−
01Γk,l G(xl)+Γk+1,k G(xk)+ Sσk
k
PE[ G(xk+1) |Fk]
(25)
( =24) (cid:0)S Sk− k1 (cid:1)Plk =− 01Γk,l G(xl)+[Γk+1,k +( Sσk k)( (cid:0)P −(cid:1)I n)] G(xk)+ Sσk
k
G(xˆk+1)
( =20) Sk−1 ) ˆk + σk (xˆk+1),
(cid:0) Sk (cid:1)P G Sk G (cid:0) (cid:1)
which yields (22(cid:0)) sinc(cid:1)e G =(cid:0) ,(cid:1)1 and Gˆk = ˆk,1 . (cid:3)
p p
hG i hG i
3.4. Properties of f. In this section we report useful properties related to the penalty
function f. First notice that, for all x,x˜,x¯ Rq,
∈
f(x),x˜ x¯ = f(x),x˜ x f(x),x¯ x
h∇ − i h∇ − i−h∇ − i
=f(x˜) f(x) 1 A(x˜ x) 2 [f(x¯) f(x) 1 A(x¯ x) 2] (26)
− − 2k − k − − − 2k − k
=f(x˜) f(x¯) 1 A(x˜ x) 2 + 1 A(x¯ x) 2.
− − 2k − k 2k − k
For all x,x˜ Rq, and α [0,1], we also have
∈ ∈
α(1 α)
f(αx+(1 α)x˜) = αf(x)+(1 α)f(x˜) − A(x x˜) 2. (27)
− − − 2 k − k
For any x argmin f(x), we have f(x ) = 0, and f satisfies
∗ x ∗
∈ ∇
1 1
f(x) f(x ) = A A(x x ),x x = A(x x ) 2, x Rq. (28)
∗ ⊤ ∗ ∗ ∗
− 2h − − i 2k − k ∀ ∈
Besides, we know from Line 1 in Algorithm 2 that
f(zk) = f((Sk 1sk +σkxk)/Sk) = A [A(Sk 1sk +σkxk)/Sk b]
∇ ∇ − ⊤ − − (29)
= 1 [Sk 1A (Ask b)+σkA (Axk b)] = 1 (Sk 1 f(sk)+σk f(xk)),
Sk − ⊤ − ⊤ − Sk − ∇ ∇
and it follows from Lines 1 and 5 in Algorithm 2 that
f(sk+1) = f Sk−1 sk + σk [xk +P(xk+1 xk)]
Sk Sk −
( =27) Sk−1 f((cid:16) s(cid:0)k)+ (cid:1)σk f((cid:0)xk(cid:1)+P(xk+1 xk)) 1(cid:17)σkSk−1 A(xk +P(xk+1 xk) sk) 2.
Sk Sk − − 2 (Sk)2 k − − k
(30)
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
The last result of this section requires new notions to be introduced. For i 1,...,p let
U be the n n block matrix defined by U = diag(0,...,I ,0,...,0). Clearl∈ y { p U =} I ,
i × i ni i=1 i n
8 Pand U x = (0,...,x ,...,0) for any x = (x ,...,x ) Rn. Then, for any random block
i i 1 p
B , we define the n n matrix U(B) = U .∈ We find E[U(B)P] = I and Ξ =
∈ I × i B i n
E[U(B)PA APU(B)], where the expectations ar∈e taken with respect to the probabbility
⊤
measure P. It follows that P
1 1
A(xˆk+1 sk) 2 = E[A(xk +P(xk+1 xk) sk) k] 2
2k − k 2k − − |F k
1 1
= E[ A(xk+P(xk+1 xk) sk) 2 k] E[ A(PU(B) I)(xˆk+1 xk) 2 k]
2 k − − k |F − 2 k − − k |F
1 1
= E[ A(xk +P(xk+1 xk) sk) 2 k] xˆk+1 xk 2
2 k − − k |F − 2k − kΞ A⊤A
−
Sk (Sk)2 Sk
( =30) f(sk) E[f(sk+1) k]+ E[f(xk +P(xk+1 xk)) k]
σk − σkSk 1 |F Sk 1 − |F
− −
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) 1
xˆk+1 xk 2
−2k − kΞ A⊤A
−
Sk (Sk)2 1
( =1) f(sk) E[f(sk+1) k] xˆk+1 xk 2
σk − σkSk 1 |F − 2k − kΞ A⊤A
− −
(cid:16) (cid:17) (cid:16) 1 (cid:17) Sk
+ E[ [Axˆk+1 b]+A[xk +P(xk+1 xk) xˆk+1] 2 k]
2 Sk 1 k − − − k |F
−
Sk (Sk)2 (cid:16) (cid:17) 1
= f(sk) E[f(sk+1) k] xˆk+1 xk 2
σk − σkSk 1 |F − 2k − kΞ A⊤A
− −
(cid:16) (cid:17) (cid:16) (cid:17) Sk 1
+ f(xˆk+1)+ E[ A(PU(B) I)(xˆk+1 xk) 2 k]
Sk 1 2 k − − k |F
−
Sk (Sk)2 (cid:16) (cid:17) (cid:2) 1 σk Sk (cid:3)
= f(sk) E[f(sk+1) k]+ xˆk+1 xk 2 + f(xˆk+1),
σk − σkSk 1 |F 2 Sk 1 k − kΞ A⊤A Sk 1
− − − −
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17) (31)
where we have used E[A[xk +P(xk+1 xk) xˆk+1] k] = 0.
− − |F
3.5. Main descent argument. Wenow derive the mainargument for thetheorems. Let x
∗
be a solution of (P’). It follows from Lemma 3.1 that
G(xˆk+1) G
∗
−
(2) 1
6 g(xˆk+1)+h(xk) G + h(xk),xˆk+1 xk + xˆk+1 xk 2
− ∗ h∇ − i 2k − kΛ
1
( =15) [g(xˆk+1)+ζ(xˆk+1)] G Sk f(zk),xˆk+1 zk xˆk+1 xk 2
− ∗ − h∇ − i− 2k − kPTk Λ
−
(14) 1 1
6 [g(x )+ζ(x ) xˆk+1 x 2 ] G Sk f(zk),xˆk+1 zk xˆk+1 xk 2
∗ ∗ − 2k − ∗ kPTk+Υ − ∗ − h∇ − i− 2k − kPTk Λ
−
1
( =15) h(xk) h(x )+ h(xk),x xk +Sk f(zk),x xˆk+1 + xk x 2
− ∗ h∇ ∗ − i h∇ ∗ − i 2k − ∗ kPTk
1 1
xˆk+1 x 2 xˆk+1 xk 2
−2k − ∗ kPTk+Υ − 2k − kPTk Λ
−
(2) 1 1 1
6 Sk f(zk),x xˆk+1 + xk x 2 xˆk+1 x 2 xˆk+1 xk 2
h∇ ∗ − i 2k − ∗ kPTk − 2k − ∗ kPTk+Υ − 2k − kPTk Λ
−
91
( =29) Sk 1 f(sk),x xˆk+1 +σk f(xk),x xˆk+1 + xk x 2
− h∇ ∗ − i h∇ ∗ − i 2k − ∗ kPTk
1 1
xˆk+1 x 2 xˆk+1 xk 2
−2k − ∗ kPTk+Υ − 2k − kPTkΛ
Sk 1 Sk 1 σk
( =26) Sk(f(xˆk+1) f ) − A(x sk) 2 + − A(xˆk+1 sk) 2 A(x xk) 2
∗ ∗ ∗
− − − 2 k − k 2 k − k − 2 k − k
σk 1 1 1
+ A(xˆk+1 xk) 2 + xk x 2 xˆk+1 x 2 xˆk+1 xk 2
2 k − k 2k − ∗ kPTk − 2k − ∗ kPTk+Υ − 2k − kPTk Λ
−
Sk 1
( =1) [Skf(xˆk+1)+Sk 1f(sk)+σkf(xk) 2Skf ]+ − A(xˆk+1 sk) 2
− ∗
− − 2 k − k
1 1 1
+ xk x 2 xˆk+1 x 2 xˆk+1 xk 2
2k − ∗ kPTk − 2k − ∗ kPTk+Υ − 2k − kPTk σkA⊤A Λ
− −
1 (Sk 1)2 1
( =31) xk x 2 + − (f(sk) f ) xˆk+1 x 2
2k − ∗ kPTk σk − ∗ − 2k − ∗ kPTk+Υ
(Sk)(cid:0)2 (cid:1) 1
E[f(sk+1) f k] xˆk+1 xk 2 σk(f(xk) f ).
− σk − ∗ |F − 2k − kPTk σkΞ Λ − − ∗
− −
Using (21) and (22(cid:0) ), sett(cid:1) ing Tk = T/τk, and multiplying by σk, we find
E σk xk+1 x 2 +SkF k 6 σk xk x 2 +Sk 1F
2τkk − ∗ kP2T+τkPΥ k+1 |F 2τkk − ∗ kP2T+τk(P In)Υ − k (32)
−
h σk xˆk+i1 xk 2 (σk)2(f(xk) f ),
−2τkk − kPT τk(σkΞ+Λ) − − ∗
−
where we define F = Gˆk G +Sk 1(f(sk) f ). For the convergence of the above sequence
k ∗ − ∗
− −
we require, at every k,
τk(σkΞ +Λ) PT 4 0, (33a)
−
σk+1 σk
(P2T +τk+1(P I )Υ) (P2T +τkPΥ) 4 0. (33b)
τk+1 − n − τk
After introducing
σk
Vk(x) = xk x 2 +Sk 1F ,
2τkk − kP2T+τkPΥ − k
we can rewrite (32) as the inequality
σk
E Vk+1(x ) k 6 Vk(x ) xˆk+1 xk 2 (σk)2(f(xk) f ), (34)
∗ |F ∗ − 2τkk − kPT τk(σkΞ+Λ) − − ∗
−
which (cid:2)becomes the(cid:3)focal point in our convergence analysis. Moreover, taking the total
expectation in (34) and iterating on k gives
k 1
σl
E Vk(x )+ − xˆk xl 2 6 C. (35)
∗ l=0 2τlk − kPT −τl(σlΞ+Λ)
(cid:20) (cid:21)
X
where C := σ0 x0 x 2 . It follows from Gˆk > G(sk) and the definitions
2τ0 k − ∗ kP2T+τ0(P In)Υ
of Vk(x ) and F , that −
∗ k
σk C
E xk x 2 +Sk 1E[f(sk) f ]+E[G(sk) G ] 6 . (36)
2τkSk 1k −
∗ kP2T+τkPΥ −
−
∗
−
∗
Sk 1
(cid:20) − (cid:21) −
10Besides, if G is bounded from below by a finite constant G , then we also find
min
C G G
E[f(sk) f ] 6 + ∗ − min . (37)
∗
− (Sk 1)2 Sk 1
− −
Recall the sequence (34). Convergence of (Vk(x)) follows almost surely from (34) on
condition that Vk(x) is bounded from below, in which case Doob’s supermartingale theorem
applies. As we see below, such a lower bound exists in the case when strong duality holds
in (P’) with respect to the constraint f(x) = A (Ax b) = 0 and an optimal multiplier
⊤
∇ −
can be found (Section 3.6). In the negative, it is necessary to step back and to consider the
convergence of a downscaled version of the sequence Vk(x) (Section 3.7).
3.6. Duality and existence of a Lagrange multiplier. Rewrite the constraint in (P’)
as f(x) = 0, i.e., A Ax A b = 0. First assume that (P’) admits an optimal primal-dual
⊤ ⊤
∇ −
pair (x ,v ) satisfying 0 ∂G(x )+A Av , or
∗ ∗ ∗ ⊤ ∗
∈
G(sk) G = G(sk) G(x ) > A Av ,sk x = Av ,A(sk x )
∗ ∗ ⊤ ∗ ∗ ∗ ∗
− − h− − i −h − i (38)
(28)
> Av A(sk x ) = Av 2(f(sk) f ),
∗ ∗ ∗ ∗
−k kk − k −k k −
where we have used A (Ax b) = 0. Since Gˆk > G(sk), we find
⊤ ∗ p
−
Sk 1F = Sk 1[Gˆk G +Sk 1(f(sk) f )] > Sk 1[G(sk) G +Sk 1(f(sk) f )]
− k − ∗ − ∗ − ∗ − ∗
− − − −
(38)
> (Sk 1)2(f(sk) f ) √2 Av (Sk 1)2(f(sk) f )
− ∗ ∗ − ∗
− − k k −
2
= 21 (Sk −1)2(f(sk) −f ∗)+ 1
2
(Sk −1)2(f(sk) −f ∗)p −kAv
∗ k
−kAv
∗
k2
1
(cid:16)q > (Sk 1)2(f(sk) f ) Av (cid:17)2 > Av 2.
− ∗ ∗ ∗
2 − −k k −k k
(39)
It follows that for all x the sequence Vk(x) > Av 2 is bounded from below, and the
∗
−k k
supermartingale theorem applies in (34), so that the sequence (Vk(x )) converges almost
∗
surely to a random quantity not smaller than Av 2, and
∗
−k k
∞
σk
xˆk+1 xk 2 +(σk)2(f(xk) f ) < a.s.. (40)
2τkk − kPT τk(σkΞ+Λ) − ∗ ∞
−
k=0(cid:20) (cid:21)
X
The convergence analysis of the sequences xk, sk and zk is deferred to the proofs of Theo-
rems 2.2(i) and 2.3(i) in the Appendix.
3.7. Without a Lagrange multiplier. Suppose now that no such optimal primal-dual
pair exists. If G is bounded from below by a finite constant G , then Gˆk > G(sk) > G ,
min min
and we find
F = Gˆk G +Sk 1(f(sk) f ) > G G +Sk 1(f(sk) f ) > G G . (41)
k ∗ − ∗ min ∗ − ∗ min ∗
− − − − −
The supermartingale theorem can now be applied to the sequence defined by W0(x ) =
∗
V0(x ) = σ0 x0 x 2 and Wk(x ) = Vk(x )/Sk 1 for k > 1. This downscaled
∗ 2τ0 k − ∗ kP2T+τ0(P In)Υ ∗ ∗ −
sequence is bounded from belo−w by G G and satisfies
min ∗
−
1
E Wk+1(x ) k 6 E Vk+1(x ) k
∗ ∗
|F Sk 1 |F
− (42)
(34) σk (σk)2
(cid:2) 6 Wk(x )(cid:3) (cid:2) xˆk+1 xk 2 (cid:3) (f(xk) f ),
∗ − 2τkSk 1k − kPT τk(σkΞ+Λ) − Sk 1 − ∗
− − −
11where we have used Sk > Sk 1. We find that Wk(x ) converges almost surely to a random
− ∗
quantity Z(ω) > G G , and
min ∗
−
∞
σk (σk)2
xˆk+1 xk 2 + (f(xk) f ) < a.s.. (43)
2τkSk 1k − kPT τk(σkΞ+Λ) Sk 1 − ∗ ∞
k=0(cid:20) − − − (cid:21)
X
See the proofs of Theorems 2.2(ii) and 2.3(ii) in the Appendix for a derivation of the conver-
gence rates of the individual sequences. We note that the orders of convergence lost in the
absence of a Lagrange multiplier are due to the scaling factor 1/Sk 1 in the sequence Wk(x ).
− ∗
4. Decreasing stepsizes for the strongly convex case
In this section we derive the decreasing stepsize sequence (6) for the strongly convex case
(Υ 0). First, weensure(33a)holdsbysetting σk = α/τk β, withα = σ 1 (Ξ(PT) 1)and
β =≻σ (Λ(PT) 1)α. Indeed, wefind1/τk = σk/α+σ − (Λ(PT) 1) = σ−m ka σx (Ξ(P− T) 1)+
max − max − max −
σ (Λ(PT) 1). Consequently,
max −
I
τk(σkΞ +Λ) PT = τk (σkΞ +Λ)(PT) 1 n PT
−
− − τk
(cid:18) (cid:19)
= τk (σkΞ +Λ)(PT) 1 [σkσ (Ξ(PT) 1)+σ (Λ(PT) 1)]I PT 4 0,
− max − max − n
−
and (33a) is tru(cid:0) e. Then, solving (33) for τk yields a lower bound for the sequ(cid:1) ence τk, which
must satisfy
[(α βτk)(P2T +τkPΥ) β(τk)2(I P)Υ](τk+1)2 +(τk)2(βP2T +α(I P)Υ)τk+1
n n
− − − −
α(τk)2P2T < 0.
−
A close inspection (omitted in this study) of the above condition yields an upper bound for
the sequence (τk), which decreases with asymptotic convergence rate O(2σ (Υ 1P2T)/k).
max −
In view of this conjecture, we can maximize the sequence by balancing the eigenspectrum of
the matrix Υ 1P2T. We do so by choosing T = QΥ, where Q = diag(q I ,...,q I ), and
− 1 n1 p np
the condition becomes
[(α βτk)(P2Q+τkP) β(τk)2(I P)](τk+1)2 +(τk)2(βP2Q+α(I P))τk+1
n n
− − − −
α(τk)2P2Q Υ < 0.
(cid:2) −
(44)
where α = σ 1 (Ξ(PQΥ) 1) and β = σ (Λ(PQΥ) 1)α. The stepsize seque(cid:3) nce (6) is
−max − max −
obtained by letting Q = P 2 (i.e., T = P 2Υ) and by recursively taking for τk+1 the positive
− −
solution of (44). As we proceed to show next, the sequence (6) is decreasing with asymptotic
rate 2/k.
FixT = QΥ, whereQ = diag(q I ,...,q I ), andensurethat(33a)holdsbysettingσk =
1 n1 p np
α/τk β, where α = σ 1 (Ξ(PQΥ) 1) and β = σ (Λ(PQΥ) 1)α. Then, Condition (33b)
−
−max − max −
reduces to (44), which rewrites as
[(α βτk)(q +π τk)+βπ (1 π )(τk)2](τk+1)2
− i i i − i (45)
+(βq απ (1 π ))(τk)2τk+1 αq (τk)2 > 0, i = 1,...,p.
i i i i
− − −
12The discriminant of (45), given by
π β π β 2 π (2 π ) 2β π2 (τk)2
∆ = 4α2 1+ i τk + i i − i + i q2(τk)2
q − α q − α − q α q 4 i
" (cid:18) i (cid:19) " (cid:18) i (cid:19) (cid:18) i (cid:19) i # #
(46)
π β τk 2 π (2 π ) 2β π2(τk)2
= 4α2 1+ i i − i + i q2(τk)2,
q − α 2 − q α 4q i
" (cid:18) (cid:18) i (cid:19) (cid:19) (cid:18) i (cid:19) i #
is nonnegative for
2
τk 6 . (C1)
max 0, β πi πi πi(2 −πi) + 2β, πi πi(2 −πi) + 2β πi β
α − qi − √qi qi α √qi qi α − qi − α
(cid:16) q q (cid:17)
Under (C1), Condit(cid:0)ion (45)(cid:1)reduces to (cid:0) (cid:1)
(βq απ (1 π ))(τk)2 +√∆
τk+1 > − i − i − i , (47)
2[(α βτk)(q +π τk)+βπ (1 π )(τk)2]
i i i i
− −
¯
holding for i = 1,...,p. Now, define δ, δ and δ as
¯
π β π β
i ¯ i ¯
δ = min , δ = max , δ = max( δ , δ ). (48)
¯ i ∈{1,...,p }(cid:26)q i − α (cid:27) i ∈{1,...,p }(cid:26)q i − α (cid:27) |¯| | |
Let γ > 1, and assume that
γ 1 βπ2 π β γ 1
− 6 i (τk)2 i τk 6 − , (49)
− γ αq − q − α γ
(cid:18) i (cid:19) (cid:18) i (cid:19)
or, equivalently,
αq βπ2 γ 1
τk 6 i δ + δ2 +4 i − ,
(cid:18)2βπ i2 (cid:19)"¯ s¯ (cid:18)αq
i (cid:19)(cid:18)
γ
(cid:19)#
(C2)
αq βπ2 γ 1
τk 6 i δ¯ δ¯2 4 i − if δ¯ > 0.
(cid:18)2βπ i2
(cid:19)"
−s − (cid:18)αq
i (cid:19)(cid:18)
γ
(cid:19)#
Using the identities √1+x 6 1+(x/2), and (1 x) 1 6 1+x+γx2 for x 6 1 1/γ, we
−
− −
find
1 (βq απ (1 π ))(τk)2 +√∆
i i i
− − −
τk 2[(α βτk)(q +π τk)+βπ (1 π )(τk)2]
i i i i !
− −
1+(πi β π i2 )τk + πi β 2 πi(2 −πi) + 2β π i2 (τk)2
(46) qi − α − 2qi qi − α − qi α qi 8
6 (cid:20) (cid:21)
1 (βπ
i2(cid:16)
)(τk)2
(cid:17)
(πi
(cid:16)
β)τk
(cid:17)
− αqi − qi − α
6 1+ πi β π i2 τ(cid:2) k + πi β 2 πi(2 −(cid:3)πi) + 2β π i2 (τk)2
qi − α − 2qi qi − α − qi α qi 8
(cid:20) (cid:20) (cid:21) (cid:21)
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
1+ βπ i2 (τk)2 πi β τk +γ πi β τk βπ i2 (τk)2 2
× αqi − qi − α qi − α − αqi
(cid:20) (cid:21)
h(cid:16) (cid:17) (cid:16) (cid:17) i h(cid:16) (cid:17) (cid:16) (cid:17) i
= 1 π i2 τk + βπ i2 (τk)2 +γ βπ i2 (τk)2 πi β τk 2
− 2qi αqi αqi − qi − α
(cid:16) (cid:17) (cid:16) (cid:17) h(cid:16) (cid:17) (cid:16) (cid:17) i
13+ πi β 2 πi(2 −πi) + 2β π i2 (τk)2
qi − α − qi α qi 8
(cid:20) (cid:21)
(cid:16) (cid:17) (cid:16) (cid:17)
+ βπ i2 (τk)2 πi β τk +γ πi β τk βπ i2 (τk)2 2 πi β π i2 τk
αqi − qi − α qi − α − αqi qi − α − 2qi
(cid:20) (cid:21)
h(cid:16) (cid:17) (cid:16) (cid:17) i h(cid:16) (cid:17) (cid:16) (cid:17) i (cid:16) (cid:17)
+ βπ i2 (τk)2 πi β τk +γ πi β τk βπ i2 (τk)2 2
αqi − qi − α qi − α − αqi
(cid:20) (cid:21)
h(cid:16) (cid:17) (cid:16) (cid:17) i h(cid:16) (cid:17) (cid:16) (cid:17) i
πi β 2 πi(2 −πi) + 2β π i2 (τk)2
× qi − α − qi α qi 8
(cid:20) (cid:21)
(cid:16) (cid:17) (cid:16) (cid:17)
(48 6),(49) 1 π i2 τk +γ 17δ2 + δ + β π i2 + 1+2δ π i2 2 (τk)2 +γ 2δ + π i2 βπ i2 (τk)3
− 2qi 8 2 α qi 8 qi 2qi αqi
(cid:20) (cid:21)
(cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)(cid:16) (cid:17)
π2 (cid:0) (cid:1)
= 1 i τk +u(τk), (50)
− 2q
(cid:18) i(cid:19)
where u(ǫ) = O(ǫ2). The above result can be balanced by setting q = π2 for i = 1,...,p. In
i i
particular, it can be seen from (47) and (50) that τk+1 6 τk (1 ǫ)(τk)2/2 whenever
− −
(17δ2 +6δ +1)α+8β 16αβ(4δ+1)ǫ
τk 6 1+ 1 . (C3)
8(4δ +1)β "s [(17δ2 +6δ +1)α+8β]2γ − #
(cid:20) (cid:21)
It follows that the stepsize sequence (7) satisfies (33) and decreases with complexity τk+1 6
τk (1 ǫ)(τk)2/2 on condition that τ0 satisfies (C1), (C2) and (C3).
− −
Now we show that the tight stepsize rule (6) vanishes with asymptotic behavior τk+1 =
τk (τk)2/2+O((τk)3). To see this, observe that (45) has two fixed points, 0 and α, where 0
− β
is the unique attracting fixed point. Indeed, the positive solution of (45) with q = π2 is
i i
given by (6), which reduces for τk = α ǫ to
β −
βπ +α
τk+1 = τk i ǫ+O(ǫ2). (51)
− βπ +(1 π )α
i i
−
Hence, stepsize sequence (6) will run away from α/β and decrease towards 0 provided that
τ0 < α. Moreover, we will have σk > 0 for all k in (5).
β
To find the asymptotic rate of τk under q = π2 for i = 1,...,p, we apply √1+x =
i i
1 + (x/2) + O(x2) and (1 x) 1 6 1 + x + O(x2) to (6). Proceeding as in (50), we find
−
−
τk+1 = τk (τk)2/2 + O((τk)3). We infer that τk 2/k = o(1/k), and it follows from (5)
− −
that σk (αk/2 β) = o(1), Sk [αk(k +1)/4 (k +1)β] = o(k).1
− − − −
5. Numerical experiments for an optimal transport problem
We consider an optimal transport problem where a number of customers are willing to
buy a service available at various sites 1,...,p. The customers are identical within each of m
distinctclassesindexedby1,...,m. Sitej ischaracterizedbyacostvectorc = (c ,...,c ),
j 1j mj
where c denotes the cost charged to any customer from class i for buying the service at
ij
site j (i = 1,...,m; j = 1,...,p). We use the following notation: for i = 1,...,m and
j = 1,...,p,
µ R : mass of customers from class i,
i >0
• ∈
1If0<a[1]<2,thesolutionofthedifferenceequationa[k+1]=a[k] 1 a[k]2 isa[k]= 2 +o(1/k)fork>1.
2 k
−
14ν R : capacity at site j,
j >0
• x ∈ R : mass of customers from class i receiving service at site j,
ij >0
• x =∈ (x ,...,x ) Rm : service schedule at site j,
• xj
= (x
1 ,j
...,x
)mj R∈mp:> g0
lobal service schedule,
•
1 p
∈
>0
c = (c ,...,c ): cost vector associated with site j,
j 1j mj
•
c : cost charged to a customer from class i when receiving service at site j.
ij
•
One would like to the maximize the overall attractiveness of the service by solving
minimize p [ c ,x +d (x )]
x j=1 h j j i j j
subject to p x = µ (i = 1,...,m)
P j=1 ij i (OT)
m x 6 ν (j = 1,...,p)
Pi=1 ij j
x > 0 (i = 1,...,m, j = 1,...,p)
ij
P
where d : Rm R is a smooth, convex function such that d (x ) models an additional
j >0
7→
>0 j j
congestion cost around site j under service schedule x .
j
Problem (OT) rewrites as as an instance of (P) in which n m for j = 1,...,p, the
j
≡
equality constraint is specified by b = µ = (µ ,...,µ ) and A = (A A ) with A = I ,
1 m 1
···
p j nj
the feasible sets are given for j = 1,...,p by
X = x Rm : ν x > 0 , (52)
j
{
j
∈
>0 j
−||
j ||1
}
and the objective functions by
1 1
h (x ) = c ,x +d (x ) x 2 , g (x ) = I (x )+ x 2 , (53)
j j h j j i j j − 2k j kΥj j j Xj j 2k j kΥj
where h is smooth convex and g is proper convex lower semi-continuous, and I denotes
j j C
the indicator function of the set C. The stationarity condition for (OT) implies that every
primal-dual solution (x,y) satisfies
c + d (x )+y > 0 = x = 0, (54)
ij j j j i ij
∇ ⇒
x > 0 = c + d (x )+δ +y = 0, (55)
ij ij j j j j i
⇒ ∇
for some δ > 0, where δ denotes the dual variable relative to the side inequality constraint
j j
m x ν 6 0, and y relates to the equality constraint p x µ = 0.
i=1 ij − j i j=1 ij − i
CPustomer guidance through pricing. TheinterpretationPofthesedualvariablesasprices
allows the operator to maximize the service quality by assigning unit prices p ,...,p to the
1 p
service at each site, under the assumption that all customers are willing to be served at
lowest cost, i.e., a customer from class i is going to purchase the service at a site j that
minimizes their prospective total cost c +p . Indeed, implementing the price profile
ij j
p = d (x )+δ a, (56)
j j j j j
∇ −
where a is an arbitrary constant parameter, yields
x > 0 = c +p +y +a = 0, (57)
ij ij j i
⇒
c +p +y +a > 0 = x = 0 (58)
ij j i ij
⇒
or, equivalently,
x ij > 0 = j argmin j′ c ij′ +p j′ . (59)
⇒ ∈ { }
which is the desired result. Any indecisions in (59) may be dealt with in practice using a
booking system.
15(m,p) : (10,10) (20,20) (50,50) (100,100) (10,40) (10,250) (10,1000)
Constant (σ = 1): 261 519 1091 1927 914 3929 9350
Constant (f.t. σ): 221 99 44 56 41 96 278
Accelerated: 130 128 152 122 107 92 62
(a) Epochs until Ax b < 10 6
−
k − k∞
(m,p) : (10,10) (20,20) (50,50) (100,100) (10,40) (10,250) (10,1000)
Constant (σ = 1): 409 816 1988 3861 1655 10762 /
Constant (f.t. σ): 221 99 53 64 45 141 472
Accelerated: 1589 1288 2333 1094 1092 1771 1773
(b) Epochs until dist (∂ , ∂ )L(xk,yk),0 < 10 6 where L(x,y) = h(x)+g(x)+ y,Ax b
x y −
∞ − h − i
Table 1. Appl(cid:0) ication of Algorithm(cid:1) 1 to (OT) with d (x ) = 1 x 2. Con-
j j 2k j k
stant (3) with σ = 1 and with finely-tuned σ, and accelerated (6) scaling.
Experiments. In our numerical experiments we use the basic congestion model d (x ) =
j j
1 x 2 for j = 1,...,p, where Υ < 0, which implies convexity in the sense of Assump-
2k j kΥj j
tion 2.1(i) with characteristic Υ = diag(Υ ,...,Υ ). In particular, if we set Υ = M I for
1 p j j nj
some M > 0, the primal step on Line 2 of Algorithm 1 reduces to
j
xk j+1 = prox
I Xj
M
j
+λk
j
−1 λk jxk
j −
c
j
+yk , (60)
(cid:16) (cid:17)
where we have used P = (1/π )I and(cid:0)either th(cid:1)e co(cid:0)nstant p(cid:0)recondit(cid:1)i(cid:1)ong Tk = T with T =
j j nj j j j
(1/τ +σ)I derived from (3), or the decreasing sequence Tk = T /τk with T = (M /π2)I
j nj j j j j j nj
and τk satisfying (6), which respectively give λk = (1/π )(1/τ + σ) and λk = (M π /τk).
j j j j j j
The proximal operation in (60) thus involves a (scaled) gradient descent and an Euclidean
projection on X , which are straightforward to compute.
j
In our tests, we set M = = M = 1 for j = 1,...,p. All the entries of c ,...,c ,
1j mj 1 p
···
µ ,...,µ , and ν ,..., ν are picked randomly, independently and uniformly in [0,1]. The
1 m 1 p
populations µ ,...,µ are then normalized so that the total load over total capacity satu-
1 m
ration factor is 0.8 i.e., m µ = 0.8 p µ . Algorithm 1 is then implemented with the
i=1 i j=1 j
following block selection policy: at every step, each agent updates their primal variables inde-
P P
pendently with probability 1/p. Ignoring all update-free steps, which occur with probability
π = (1 1/p)p, wefindπ = = π = [p(1 π )] 1 andπ = [p2(1 π )] 1 ifi = j. Forthe
0 1 p 0 − i,j 0 −
constan− t stepsize policy, we fi· x·· σ and we set− τ = = τ = (1/2σ)p− (1 π )(σ 6 (Ξ) 1) 1
1 p 0 max −
··· − −
in accordance with (3). For the accelerated implementation, the decreasing stepsize se-
quence τk is computed as in (6) with τ0 = 1. Since in this problem Λ = 0, we have
β = κ = 0.
Table 1 displays the number of updates per agent (epochs) that were needed to approach
optimality. As stopping criteria we consider the feasiblity residual Ax b and the KKT
residual dist (∂ , ∂ )L(xk,yk),0 , where L(x,y) =
h(x)+g(x)+k y,− Axk∞
b denotes the
x y
∞ − h − i
Lagrangian of (P). In our simulations, the constant scaling implementation proved quite
(cid:0) (cid:1)
sensitive to the chosen parameters. Setting σ = 1 was fine with smaller networks, though
performance deteriorated as the size of the problem increased, and in particular for a larger
number p of agents. Speed of convergence can be considerably increased by fine tuning the
16parameter values, as can be seen on the second lines of Table 1a and Table 1b, where we
used σ = 0.1 for (m,p) = (10,10) and σ = 0.01 for the larger networks. The accelerated
scaling, on the other hand, offered good and consistent (near dimension-free) performance
even in the larger networks with tens of thousands of variables and many agents.
Appendix A. Proofs of Theorems 2.2 and 2.3
A.1. Convex case (Υ = 0). IfΥ = 0, thenwetaketheconstantstepsizes τk = 1andσk = σ
all k, so that (33a) reduces to a condition on σ,
σΞ +Λ 4 PT. (61)
Besides, in Algorithm 2 we get Sk = (k + 1)σ, while F = Gˆk G + σk(f(sk) f ) and
k ∗ ∗
− −
Vk(x) = σ xk x 2 +kσF .
2k − kP2T k
We are now in a position to show Theorem 2.2. Part (i) in the proof, in particular, follows
the lines of (Luke and Malitsky, 2018, proof of Theorem 1, part (i)), with a few changes
required by the presence of the smooth component h. All the developments are detailed
below for completeness.
Proof of Theorem 2.2. (i) First suppose there is a Lagrange multiplier. Then, (39) becomes
σk2 Av 2 Av 2
kF > (f(sk) f ) k ∗ k > k ∗ k , (62)
k ∗
2 − − σ − σ
and we have Vk(x) > Av 2. The supermartingale theorem applies, and (43) reduces to
∗
−k k
∞ 1
xˆk+1 xk 2 +σ(f(xk) f ) < a.s., (63)
2k − kPT (σΞ+Λ) − ∗ ∞
−
k=0(cid:20) (cid:21)
X
which almost surely yields f(xk) f = o(1/k) and xˆk+1 xk 2 0 (hence,
− ∗ k − kPT (σΞ+Λ) →
by (28), f(xˆk+1) f = 1 xˆk+1 x 2 0), and the sequence Vk(− x ) converges almost
− ∗ 2k − ∗ kA⊤A → ∗
surely to a random quantity not smaller than Av 2. It follows that Vk(x ) is almost
∗ ∗
−k k
surely bounded from above by a constant C(ω), so that
σ (62) σ
C(ω)+ Av 2>Vk(x )+ Av 2= xk x 2 +σkF + Av 2 > xk x 2 >0a.s.,
k
∗
k
∗
k
∗
k 2k −
∗ kP2T k
k
∗
k 2k −
∗ kP2T
(64)
which shows that the sequence xk is pointwise almost surely bounded, and so are the se-
quences sk and zk as a consequence of (4). Since almost surely kσF = Vk(x ) σ xk
k ∗ − 2k −
x 2 6 Vk(x ) is bounded from above, we infer from (62) that f(sk) f = O(1/k2)
∗ kP2T ∗
−
∗
almost surely. Since we also have f(xk) f = o(1/k) almost surely, Sk 1/Sk = O(1), and
∗ −
−
σk/Sk = O(1/k) as k , it follows from Line 1 of Algorithm 2 and from the convexity
→ ∞
of f that f(zk) f = O(1/k2) almost surely. Then, using Ax = b, we find
∗ ∗
−
(1)
f(zk),x xˆk+1 = A (Azk b),x xˆk+1
∗ ⊤ ∗
h∇ − i h − − i
= A A(zk x ),x xˆk+1 6 A(zk x ) A(x xˆk+1) (65)
⊤ ∗ ∗ ∗ ∗
h − − i k − kk − k
(28)
= 2 f(zk) f f(xˆk+1) f = o(1/k),
∗ ∗
− −
p p
17where we have used f(xˆk+1) f 0. By convexity and smoothness of h, we find
∗
− →
h(xk),x xˆk+1 = h(xk),x xk h(xk),xˆk+1 xk
∗ ∗
h∇ − i h∇ − i−h∇ − i
(2)
6 [h(x ) h(xk)] [h(xˆk+1) h(xk) 1 xˆk+1 xk 2] (66)
∗ − − − − 2k − kΛ
= h(x ) h(xˆk+1)+ 1 xˆk+1 xk 2.
∗ − 2k − kΛ
Consider a bounded sequence (xk) (this event happens with probability one), and one of
its convergent subsequence (xkl) with accumulation point x˜, which is feasible since f(x˜) =
lim
l
f(xkl) = f ∗. Now, applying Lemma 3.1 at point x
∗
gives
→∞
(14)
g(xˆkl+1) g(x ∗) 6 h(xkl)+Skl f(zkl)+PTkl(xˆkl+1 xkl),x
∗
xˆkl+1
− h∇ ∇ − − i
(66)
6 h(x ∗) −h(xˆkl+1)+ 21 kxˆkl+1 −xkl k2
Λ
+ Skl f(zkl)+PTkl(xˆkl+1 xkl),x
∗
xˆkl+1 ,
h ∇ − − i
which rewrites as
G(xˆkl+1) −G(x ∗) 6 21 kxˆkl+1 −xkl k2
Λ
+Skl h∇f(zkl),x
∗
−xˆkl+1
i (67)
+ PTkl(xˆkl+1 xkl),x
∗
xˆkl+1 .
h − − i
Since xˆkl+1 xkl 2 0, the sequence (xˆkl+1) is bounded and, by taking the limit of
k − kPT (σΞ+Λ) →
the last equation forl− , we find, using (65), Skl = k σ andthe lower-semicontinuity of G:
l
→ ∞
G(x˜) 6 lim
l
G(xˆkl+1) 6 G(x ∗), and x
∗
. To show that the bounded sequence (xk)
has a
unique→∞
accumulation point, we
consi∈ deS
r a second convergent subsequence
xk¯
l x¯.
→
Recall that Vk(x) = σ xk x 2 + kσF and that Vk(x) is convergent for our bounded
2k − kP2T k
sequence (xk), thus admitting the same limit for subsequences (xkl) and (xk¯ l). Computing
these limits at x = x˜ yields lim Vkl(x˜) = lim k σF and lim Vk¯ l(x˜) = σ x¯
x˜ 2 +lim k¯ σF , hence liml →∞ k σF = σ x¯l → x∞ ˜ 2l +kl lim k¯l → σF∞ . Repeatin2 gk th− is
opkP er2 aT tion atl → x∞ = l x¯ gik¯ vl es σ x˜ x¯ l 2→∞ +l limkl k2k σF− =kP li2 mT k¯l σ→ F∞ .l Thk¯ ul s x˜ x¯ 2 = 0
2k − kP2T l →∞ l kl l →∞ l k¯ l k − kP2T
and x˜ = x¯, which shows that almost surely (xk) converges pointwise to a point of . Let x
∗
S
be that limit point. From Lemma 3.2, we know that k Γk,l = k Γk,l = I . Moreover,
l=0| | l=0 n
(17) gives Γk+1,l = (Sl/Sk)Γl+1,l = O(1/k) and thus, for all l, Γk+1,l 0 as k . It
P P → → ∞
followsfrom(16)andtheToeplitzTheorem, Toeplitz(1911),thatsk = k Γk,lxl x
l=0 → ∗ ∈ S
almost surely, which completes the proof of (i).
P
(ii) We know from Section 3.7 that almost surely Wk(x ) converges to Z(ω) > G G
∗ min ∗
−
and is thus bounded from above by a constant C(ω), so that C(ω) > Wk(x ) > F . It
∗ k
follows from (41) that f(sk) f 6 [F + G G ]/(kσ) 6 [C(ω) + G G ]/(kσ)
∗ k ∗ min ∗ min
− − −
and, consequently, we find f(sk) f = O(1/k) almost surely. From (43) we also have
∗
−
(f(xk) f ) = o(1) almost surely. It follows from Line 1 in Algorithm 2 and from the
∗
−
convexity of f that almost surely f(zk) f = O(1/k).
∗
−
Now, consider the sublevel sets defined by
(c) = x : max(f(x) f ,G(x) G ) 6 c , c > 0. (68)
∗ ∗
L { − − }
Since (0) = is bounded and both f and G are convex functions, the sets (c) are
L S L
bounded for all c > 0. From Gˆk > G(sk) and the definitions of Wk(x ), we find Wk(x) >
∗
G(sk) G +Sk 1(f(sk) f ), which yields
∗ − ∗
− −
1
f(sk) f 6 (Wk(x )+G G ) and G(sk) G 6 Wk(x ), (69)
− ∗ Sk 1 ∗ ∗ − min − ∗ ∗
−
18in which Wk(x ) Z(ω) for any outcome ω Ω, where Ω is an event of probability 1.
∗
→ ∈
It follows that both sequences (f(sk(ω)) f ) and (G(sk(ω)) G ) are bounded by a con-
∗ ∗
− −
stant C (ω). Hence, the sequence sk(ω) belongs to (C (ω)), which is a bounded set. Thus,
′ ′
L
the sequence sk(ω) is bounded if ω Ω.
∈
Since Sk 1 = σk, (69) also yields f(sk(ω)) f = O(1/k) if ω Ω. Hence f(sk(ω)) f =
− ∗ ∗
− ∈ −
O(1/k) almost surely, as seen previously. Besides, all the accumulation points of (sk(ω)) are
feasible if ω Ω. It follows that liminf G(sk) > G if ω Ω, hence with probability
k ∗
∈ →∞ ∈
one. Now, observe that (36) reduces in the convex case to
1 C
E xk x 2 +σkE[f(sk) f ]+E[G(sk) G ] 6 , (70)
2kk −
∗ kP2T+PΥ
−
∗
−
∗
σk
(cid:20) (cid:21)
andsupposethatthereisaneventΩ˜ withnonzeroprobabilitysuchthatlimsup G(sk(ω)) >
k
G . Then, takingthelimitsuperiorin(70)leadstoacontradiction. Hence, limsu→p∞ G(sk) 6
∗ k
G with probability one. Thus, lim G(sk) = G with probability one, and alm→os∞t surely
∗ k ∗
the accumulation points of (sk) belo→ ng∞ to .
S
It remains to derive the convergence rate of the sequence (f(sk(ω))). We know that
G(sk(ω)) G = o(1) for every ω Ω. Consider the nonnegative sequence uk(ω) =
∗
max(0,G − G(sk(ω))). For all k and∈ for all ω Ω, we have E[G G(sk(ω)) ω Ω] 6
∗ ∗
E[uk(ω) ω − Ω] where(uk)converges pointwise tow∈ ards 0onΩ. Since (− uk)satisfies| u∈ k(ω) 6
G G| ∈ for all k, with E[ G G ω Ω] = G G < , Lebesgu| e’s do| mi-
∗ min ∗ min ∗ min
nate−
d convergence theorem
ap| plie−
s, and
| w|
e
∈
find lim
−E[uk(ω)
ω
∞
Ω] = 0. It follows
k
that lim E[G G(sk(ω))] 6 Prob(ω Ω) lim →∞E[uk(ω) ω| ∈ Ω] + (1 Prob(ω
k ∗ k
Ω)) lim →∞E[uk(ω− ) ω /Ω] 6 0, where we h∈ ave used P→ ro∞ b(ω Ω)| =∈ 1 and uk 6− G G ∈ .
k ∗ min
→∞ | ∈ ∈ −
Combining this result with (70) gives
C E[G G(sk)]
E[f(sk) f ] 6 + ∗ − = o(1/k),
∗
− (σk)2 σk
which completes the proof of Theorem 2.2. (cid:3)
A.2. Strongly convex case. The proof of 2.3 under strong convexity is similar to that of
Theorem 2.2. Recall from Section 4, however, that in Algorithm 2 we now have τk 2/k =
−
o(1/k), σk (αk/2 β) = o(1), and Sk [αk(k +1)/4 (k +1)β] = o(k).
− − − −
Proof of Theorem 2.3. (i) We begin as in the proof of Theorem 2.3(i) by showing that kF
k
is bounded from below. If (P’) admits an optimal primal-dual pair (x ,v ) satisfying (38),
∗ ∗
then, it follows from (39) that the sequence Vk(x) > Av 2 is bounded from below, and
∗
−k k
the supermartingale theorem applies in (34) to the nonnegative sequence Vk(x) + Av 2,
∗
k k
so that
∞
σk
xˆk+1 xk 2 +(σk)2(f(xk) f ) < a.s., (71)
2τkk − kPT τk(σkΞ+Λ) − ∗ ∞
−
k=0(cid:20) (cid:21)
X
where PT τk(σkΞ+Λ) PT αΞ, which almost surely yields f(xk) f = o(1/k3) and
∗
k2 xˆk+1
−
xk 2 0
(hen→
ce,
by−
(28), k2[f(xˆk+1) f ] =
k2
xˆk+1 x
−
2 0), and the
k − kPT → − ∗ 2 k − ∗ kA⊤A →
sequence Vk(x ) converges almost surely to a random quantity not smaller than Av 2.
∗ ∗
−k k
It follows that Vk(x ) is almost surely bounded from above by a constant C, so that
∗
(39) σk
C + Av 2 > Vk(x )+ Av 2 > xk x 2 > 0 a.s., (72)
k
∗
k
∗
k
∗
k 2τkk −
∗ kP2T
19which shows that xk x ∗ P2T = O(1/k) and, since P2T has full rank, the sequence (xk)
k − k
converges pointwise almost surely to the solution x . Since (17) in Lemma 3.2 gives Γk+1,l =
∗
(Sl/Sk)Γl+1,l = O(1/k2) for all l, so do sequences (sk) and (zk) as a consequence of (16),
Line 1 in Algorithm 2, and the Toeplitz Theorem. Since almost surely Sk 1F = Vk(x )
− k ∗
−
σk xk x 2 6 Vk(x ) is bounded from above, we infer from (39) that f(sk) f =
2τkk − ∗ kP2T+τkPΥ ∗ − ∗
O(1/k4) almost surely. Since we also have f(xk) f = o(1/k3) almost surely, Sk 1/Sk =
∗ −
−
O(1), and σk/Sk = O(1/k) as k , it follows from Line 1 of Algorithm 2 and from the
→ ∞
convexity of f that f(zk) f = O(1/k4) almost surely.
∗
−
(ii) In the strongly convex case, we know from Section 3.7 that almost surely Wk(x )
∗
converges to Z(ω) > G G and is thus bounded from above by a constant C(ω), so
min ∗
−
that C(ω) > Wk(x ) > F . It follows from (41) that f(sk) f 6 [F +G G ]/Sk 1 6
∗ k ∗ k ∗ min −
− −
[C(ω) + G G ]/Sk 1 and, since Sk = O(k2), we find f(sk) f = O(1/k2) almost
∗ min − ∗
− −
surely. From (43) we also have (f(xk) f ) = o(1/k) almost surely. It follows from Line 1
∗
−
in Algorithm 2 and from the convexity of h that f(zk) f = O(1/k2) almost surely.
∗
−
Since(69)stillholds, wefindthatthesequencesk(ω)isboundedforω Ω,withf(sk(ω))
∈ −
f = O(1/k2).
∗
All the accumulation points of (sk(ω)) are thus feasible if ω Ω. As a consequence, we
∈
find liminf G(sk) > G almost surely. Under strong convexity, (36) now reduces to
k ∗
→∞
1 C
E xk x 2 +Sk 1E[f(sk) f ]+E[G(sk) G ] 6 . (73)
2Sk 1k − ∗ kΨk − − ∗ − ∗ Sk 1
(cid:20) − (cid:21) −
Reasoning as we did for (70), we also find limsup G(sk) 6 G with probability one. It
k ∗
followthat,almostsurely, lim G(sk) = G andx→i∞stheuniqueaccumulationpointof(sk).
k ∗ ∗
Asfortheconvergence rateoft→ h∞ esequence (f(sk(ω))), itstill holdsthatG(sk(ω)) G = o(1)
∗
almost surely, and that lim E[G G(sk(ω))] 6 0. Applying this last resul to− (70) yields
k ∗
→∞ −
C E[G G(sk)]
E[f(sk) f ] 6 + ∗ − = o(1/k2),
∗
− (Sk 1)2 Sk 1
− −
which completes the proof of Theorem 2.3. (cid:3)
References
Chambolle, A., Pock, T., 2011. A first-order primal-dual algorithm for convex problems with
applications to imaging. Journal of Mathematical Imaging and Vision 40, 120–145. URL:
https://doi.org/10.1007/s10851-010-0251-1, doi:10.1007/s10851-010-0251-1.
Fercoq, O., Richt´arik, P., 2015. Accelerated, parallel, andproximalcoordinatedescent. SIAM
Journal on Optimization 25, 1997–2023. URL: https://doi.org/10.1137/130949993,
doi:10.1137/130949993, arXiv:https://doi.org/10.1137/130949993.
Luke, D.R., Malitsky, Y., 2018. Block-coordinate primal-dual method for nonsmooth min-
imization over linear constraints, in: Giselsson, P., Rantzer, A. (Eds.), Large-Scale and
Distributed Optimization. Springer International Publishing, Cham, pp. 121–147. URL:
https://doi.org/10.1007/978-3-319-97478-1 6, doi:10.1007/978-3-319-97478-1 6.
Malitsky, Y., 2019. The primal-dual hybrid gradient method reduces to a primal method for
linearly constrained optimization problems. URL: https://arxiv.org/abs/1706.02602,
arXiv:1706.02602.
20Richt´arik, P., Tak´aˇc, M., 2014. Iteration complexity of randomized block-coordinate descent
methods for minimizing a composite function. Mathematical Programming 144, 1–38.
URL: https://doi.org/10.1007/s10107-012-0614-z.
Richt´arik, P., Tak´aˇc, M., 2016. Parallel coordinate descent methods for
big data optimization. Mathematical Programming 156, 433–484. URL:
https://doi.org/10.1007/s10107-015-0901-6.
Toeplitz, O., 1911. U¨ber allgemeine lineare mittelbildungen. Prace Matematyczno-Fizyczne
22, 113–119. URL: http://eudml.org/doc/215310.
Tseng, P., 2008. On accelerated proximal gradient methods for convex-concave optimization.
Submitted to SIAM J. Optim.
21