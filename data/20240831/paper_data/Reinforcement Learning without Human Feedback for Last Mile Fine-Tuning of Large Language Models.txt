Reinforcement Learning without Human Feedback
for Last Mile Fine-Tuning of Large Language Models
AlecSolway
TwoSixTechnologies
Abstract
Reinforcementlearningisusedtoalignlanguagemodelswithhumanpreference
signalsafterfirstpre-trainingthemodeltopredictthenexttokenoftextwithina
largecorpususinglikelihoodmaximization. Beforebeingdeployedinaspecific
domain, models are often further fine-tuned on task specific data. Since human
preferencesareoftenunavailableforthelaststep,itisperformedusinglikelihood
maximizationasthatisthetypicaldefaultmethod.However,reinforcementlearn-
inghasotheradvantagesbesidesfacilitatingalignmenttoahumanderivedreward
function.Forone,whereaslikelihoodmaximizationisaformofimitationlearning
inwhichthemodelistrainedonwhattodounderidealconditions,reinforcement
learningis notlimited to demonstratingactionsjust foroptimallyreachedstates
andtrainsamodelwhattodounderarangeofscenariosasitexploresthepolicy
space. Inaddition,italsotrainsamodelwhatnottodo,suppressingcompetitive
butpooractions. Thisworkdevelopsaframeworkforlast-milefine-tuningusing
reinforcementlearning and tests whether it garnersperformancegains. The ex-
perimentscenteronabstractivesummarization,buttheframeworkisgeneraland
broadlyapplicable.Useoftheprocedureproducedsignificantlybetterresultsthan
likelihoodmaximizationwhen comparingraw predictions. For the specific data
tested, the gapcouldbe bridgedbyemployingpost-processingof the maximum
likelihood outputs. Nonetheless, the framework offers a new avenue for model
optimization in situations where post-processing may be less straightforward or
effective,anditcanbeextendedtoincludemorecomplexclassesofundesirable
outputstopenalizeandtrainagainst,suchashallucinations.
1 Introduction
Reinforcementlearningfromhumanfeedback(RLHF)isastandardkeyingredientforfine-tuning
largelanguagemodelstofollowinstructionsandproducefluentdialogue[1,2,3,4]. Inthetypical
setup,afoundationmodelisfirsttrainedviamaximumlikelihood(equivalently,crossentropymin-
imization)topredictthenexttokenoftextconditionalonawindowofpriortokens.Asimilarsetup
is then used in a first fine-tuningstep to train the modelon a targetset of tasks based on ground-
truthinput-outputpairs.Followingthefirststageoffine-tuning,themodel’soutputisstochastically
sampledinresponsetoaseriesoffurtherprompts,andtheoutputsarerankedbyhumanannotators.
The rankingsare used as the basis for a secondstage of fine-tuningusing reinforcementlearning.
Thisstagemaybeiteratedseveraltimestofurtherimprovethemodel’sperformance.
Reinforcementlearning has a number of standard components. In this context, reward is derived
fromhumanrankingseitherbytrainingamodelontherankingdataorbyreferencingthemdirectly,
thestatespaceconsistsoftheinput/outputtokensproducedsofar,theactionsarethetokensthem-
selves,andthestatetransitiondynamicsaretriviallydefinedastheresultofappendinganadditional
token.Withtheseingredientsinplace,apolicynetworkistrainedusingeitherproximalpolicyopti-
mization(PPO)[5]ifarewardmodelislearnedordirectlyusingdirectpreferenceoptimization[6].
4202
guA
92
]LC.sc[
1v35761.8042:viXraAftertrainingageneralversionofthemodeltoperformaswellaspossibleacrossarangeoftasks,
themodelmayyetfurtherbefine-tunedtospecializeinaparticulartaskforaspecificusecase.This
finalfine-tuningistypicallydoneviamaximumlikelihood,notreinforcementlearning,asinthevery
firststepdescribedabove. RLHFproducesstate-of-the-artresultsforseveralreasons. First,human
rankingsprovidedirectfeedbackonhowwellamodelaccomplishesaparticularobjective,suchas
engaginginfluidandusefulconversation.Second,trainingexamplesincludenotonlywhatoutputs
aregoodbutalsowhatoutputsarebad,thatis, lowerrankedornotchosen. Third,althoughdirect
preferenceoptimizationhasotheradvantages,whenPPOisemployed,ahybridmodel-based/model-
free RL scheme is used to simulate a range of additionaltraining exampleswhich likely help the
modellearnwhattodowhenreachingsuboptimalstates. Incontrast,likelihoodmaximizationonly
tells a model what should be done under optimal conditions, not what it should not do or what
secondbestperformancewouldlooklikeifperfectrecallisnotpossible.
Whilehumanrankingsofoutputsareclearlyvaluableforguidingmodeltraining,theycanbecostly
and time consumingto obtain and are not always available, which is why the last step of domain
specificfine-tuningoftendoesnotincorporatethem.However,thesecondandthirdadvantagesmo-
tivatethequestionofwhetherincorporatingreinforcementlearningintolast-milefine-tuningbased
on data without human rankings can garner performancegains. Testing this formed the focus of
the currentwork. The same tools used for traditionalRLHF were importedwith minimalmodifi-
cation, withPPO-basedoptimizationusedbecausethe datawerenotrank-based. A rewardmodel
wastrainedonacombinationoftheground-truthoutputsandsimpleexamplesofnegativeoutputs.
Similar to RLHF, a reward model of this type in essence represents a similarity metric between
the outputpresentedto it andthe bestoutputto produceas determinedby generalizingacross the
ground-truthdata.PPOwasusedincombinationwithahybridmodel-based/model-freeRLscheme,
alsosimilartoRLHF, tosimulatedifferenttrainingexamples. Besidestherewardmodel,whichis
trainedonstaticdata,thestatetransitiondynamicsarenaturallyandtriviallydefinedasnotedabove.
PPO-basedoptimizationrunsaseriesofsimulationsusingthestatetransitionandrewardmodelsto-
getherwiththecurrentpolicytoproducearangeofexampleswhichinturnareusedtofurtherrefine
thepolicynetworktoperformwellunderdifferentscenarios(states). Theexperimentscenteredon
abstractivesummarization,butthegeneralframeworkisbroadlyapplicabletoanytask.
2 Related work
Reinforcementlearninghad been appliedto the topic of summarizationpriorto the modernLLM
era. A numberof papersapproachedthe problemby directlymaximizingROUGE ora variantas
partofthecriterion(reward)beingoptimized. Forexample,PasunuruandBansal[7]usedacom-
bination of standard ROUGE, a modified ROUGE augmentedto upweightsalient words detected
withasaliencyclassifier,andanentailmentmeasureoutputbyaseparateentailmentmodel. These
criteriawereoptimizedtogetherbya policynetworkwithan LSTMarchitecture. Paulusetal.[8]
combinedstandardnext-tokenlikelihoodandROUGE andanLSTMarchitecturewith specificat-
tentionmechanismsaimedatreducingrepetitionin theoutput. Lietal.[9] usedagatedrecurrent
unit(GRU)architecturewithrewardbasedonacombinationofthenext-tokenlikelihoodandclas-
sifieroutputdistinguishingground-truthandgeneratedsummaries.ChenandBansal[10]employed
convolutionatthesentencelevelcombinedwithanLSTMarchitectureacrosssentencesinamodel
which first extracts salient sentencesand then summarizesthem, with ROUGE-L playingthe role
of reward. Wu andHu [11] used a combinedconvolution/GRUarchitectureanda combinationof
ROUGEandcoherenceforreward,withthelatterlearnedbyasubnetworktrainedonpositive(adja-
cent)andnegative(random)sentencepairsinthetrainingdata. Keneshlooetal.[12]usedROUGE
andan LSTM architecturewith a specific focusontransfer learning,trainingthe modelon mixed
samples from more than one dataset. Combining RL-based approaches with the richer semantic
representationsofapre-trainedlanguagemodel,Baeetal.[13]andWangetal.[14]operatedover
BERTembeddingsinsteadofstartingwithrawinputswhilestillusingROUGEforreward.Moving
awayfromROUGE,Lietal.[15]usedBERTScoretorewardsemanticsimilaritybetweenthetarget
and output instead of training the model to capture the target exactly. The use of reinforcement
learninginthesecontextswasoftenmotivatedbyitsabilitytooptimizeanon-differentiablemetric
likeROUGE.
Thereisofcoursealargeliteratureonsummarizationwithoutappealingtoreinforcementlearning.
The present focus is specifically on the ability to leverage pre-trained large language models and
2notonspecializedarchitectures. LiuandLapata[16]usedsentenceanddocumentlevelrepresenta-
tionsbuiltonBERTwithasummarynetworktrainedusingmaximumlikelihood. PEGASUS[17]
isalargerTransformerbasedlanguagemodelpre-trainedwithsummarizationspecificallyinmind.
Duringtraining,importantsentencesratherthanrandomwordsweremaskedandrecovered,paral-
lelingextractivesummarization. T5isaTransformerbasedencoder-decodermodelbuiltbasedon
theresultsofasystematicevaluationofhowdifferentchoicesduringthemodelingprocessimpact
performance. Itachievedstate-of-the-artperformanceatthetimeonacollectionoftaskswhichin-
cludedsummarization[18].BothPEGASUSandT5weretrainedusingalikelihood-basedobjective.
AsnotedintheIntroduction,manymodernmodelsutilizeahybridmaximumlikelihood/RLtraining
procedure,usingreinforcementlearningtoalignthemodelwithhumanpreferences.Stiennonetal.
[19]isaprominentearlyexampleofthisideainwhichtheauthorsfine-tunedGPT3forsummariza-
tionspecifically.Llama2,GeminiandmodernversionsofGPTallusethisapproach[1,2,3,4].
Thehybridmodel-based/model-freeRLschemeemployedhereandinPPO-basedversionsofRLHF
isaformofdataaugmentation.Dataaugmentationiscommoninthevisiondomain,andwhileless
frequent,has also beenappliedto textdata [20, 21, 22]. It haslargelybeen used to make models
robust to noise and variability in the input, especially for small datasets. Here, augmentation is
instead of the output, aimed at helping the model both avoid poor outputs and perform well in a
rangeofdifferentstates.
3 Models
3.1 Generalmodelingprocedures
A 4bit GPTQ quantized version of Llama2-7B-Chat was used as the base model for all others,
includingthe reward, value, and policy networks, and the maximumlikelihood fine-tunedcontrol
model. Training was performedusing low rank adaptation[23] with r = 16 and α = 16 for the
rewardnetworkandr = 32andα = 16fortheothermodels. Therewardandvaluenetworkshad
a different final linear layer that projected the model’s state to a scalar instead of to token logits
asinthelanguagemodel. OptimizationofallmodelswasperformedusingAdamWwithacosine
learningrateschedulestartingat1e-5; trainingconsistedofoneepoch. Thebatchsize was14for
trainingtherewardmodelsand7forPPOandmaximumlikelihoodoptimization.
3.2 Rewardmodeling
Trainingarewardmodelrequiresnotjustpositiveexamplesbutalsonegativeexamplesthatrepresent
undesirableoutputs. Inthecurrentwork,fivebasiccategoriesofnegativeexamplesweregenerated
basedonthepositiveexamplesintheoriginaldataset:
1. Completelyrandomtokensfortheoutputspairedwithexistinginputs.
2. Existinginputandoutputsequencesrandomlyre-paired,representingcoherentbutirrele-
vantoutputs.
3. Words (for simplicity, whitespace delineated entities) from an existing output sequence
randomly shuffled and paired with the same original input. The output is a bag of the
correctwordswithoutcoherence.
4. Outputsthatbegincorrectlybutendwithrepetitivesequences,whichisapatterncommonly
producedbymodelsfitviamaximumlikelihood.
5. Outputswhichrepeattheinputsequence,alsoacommonfailuremodeformaximumlike-
lihoodoptimizedmodels.
Additionalcategoriesrepresentingmorecomplexerrors,suchashallucinationsorpoorsummariza-
tions, can also be added as noted in the Discussion. The size of each class was equal to the size
oftheoriginaldataset. Toaccountfortheclassimbalancewhentrainingtherewardnetwork,each
negative datum was weighted by a factor of 1/5. Each token in the outputof a positive example
wasassignedreward+1andeachtokenintheoutputofanegativeexamplewasassignedreward0.
Thislefta trailofbreadcrumbsforthe agenttofollowduringpolicyoptimization. Morecomplex
categoriesofnegativeexamplesmayrequirerewardatthesequenceratherthantokenlevel,which
maybehardertooptimize. Therewardmodelwastrainedusingasquaredlossfunctiontopredict
3ascalarconditionalontheinputtokensandallprecedingoutputtokens.Duringpolicylearning,the
reward predictedby the modelwas additionallycombinedwith a length penaltyof −2.5 for each
tokenproducedbeyondtheground-truthlength.
3.3 Policyandvaluemodels
In the currentwork one epoch of standard proximalpolicy optimization [5] was applied for each
outer training batch with all trajectories combined in a single mini-batch (i.e. there was a single
gradientupdateforeachouterbatch),althoughthisrepresentsanavenueforfurtherrefinement.For
half of the outer loop batches, outputs were randomly sampled from the policy network, and the
remaininghalfusedground-truthoutputs.Forcompleteness,thepolicylosswas:
π (a |s )
θ t t
w (θ)= , (1)
t
π (a |s )
θold t t
T−1
A
t
=
X(γλ)t′−t(r
t′+1+γV(s t′+1)−V(s t′)), (2)
t′=t
L =E [min(w (θ)A ,clip(w (θ),1−ǫ,1+ǫ)A )], (3)
PPO t t t t t
wherer istherewardfromtherewardmodelplusthelengthpenalty,T isthelengthoftheoutput
t
sequence, λ = 0.95 and γ = 0.99999. Note that with a single update the clipping is redundant.
For simplicity, the loss did not include an entropy term or the KL divergence with the original
model. However,theuseofLoRAactsasanimplicitprior,similartoincorporatinganexplicitKL
divergenceterm. Asquaredlosswasusedforvaluewiththetargetdefinedas:
T−1
V(s t)target = X(γ)t′−tr t′+1 ∀t∈[0,T −1] (4)
t′=t
andV(s )target =0.
T
3.4 Controlmodels
Twoclassesofmodelswereusedascontrolsfortheexperiments: thebasemodelwithoutanyfine-
tuningandthebasemodelfine-tunedusingmaximumlikelihood.
4 Experiments
4.1 Metrics
ROUGE and BLEURT [24] were used to evaluate the models. ROUGE is a simple well-known
metricthatcapturesn-gramoverlap,whileBLEURTisamodel-basedmeasurebuiltonBERTem-
beddingsto predicthumanevaluations. ROUGE is confoundedwhen the lengthsof the predicted
andreferencesummariesaredifferent: alongerpredictedsummaryhasalargerchanceofincreas-
ingthenumeratorforROUGE whilethedenominatorisfixed. Likewise, alongerreference
recall
summaryhasalargerchanceofincreasingthenumeratorforROUGE . Toaccountforthis,
precision
lengthadjusted(la-)versionsofROUGE werecomputedinadditionto thestandardversions. For
recall,thestandardmeasurewasmultipliedbyng/npwhennp>ngandby1otherwise,whereng
isthenumberofwhitespacedelineatedentitiesintheground-truthsummaryandnpisthenumberof
suchentitiesintheprediction. Forprecision,thestandardmeasurewasmultipliedbynp/ngwhen
np<ngandby1otherwise.
4.2 Datasets
Performance was tested using two datasets: samsum [25], consisting of short conversations, and
xsum[26],consistingofnewsarticlesandsinglesentencesummaries. Inordertofitthemodelsin
areasonableamountoftimeontheavailablehardware,onlydatumwithamaximumof400input
tokens were included. This removed at most 13.4%of the data for each of the train/test splits of
samsum. Thexsumdatasetwassignificantlylargerandwasfurtherreducedto15000trainingand
3000test examples. DuringPPO and at test for all models, the outputwas limited to 100tokens,
4which is more than the maximumlength of the ground-truthdata in the training and test splits of
bothdatasets.
4.3 Results
Table 1, focusing first on the first three columns, shows the average excess length of the output
producedby the differentmodelsrelativeto the groundtruth. Lengthis defined as the numberof
whitespacedelineatedentities. Thebaselinenon-fine-tunedmodelwasthemostverbose.Themaxi-
mumlikelihoodtrainedmodelproducedshorteroutputs,however,fine-tuningwasmoreeffectivefor
samsumthanxsuminthisregard. TheRLmodelproducedoutputlengthsveryclosetotheground
truthforbothdatasetsandoutperformedtheothermodelsbyalargemargin.
Table 2 displaysBLEURT andlength-adjustedROUGE scoresas describedabove, as well as tra-
ditionalROUGE scoresforreference. Maximumlikelihoodfine-tuningimprovedperformanceon
bothBLEURTandlengthadjustedROUGEmeasures,andtheRLmodelinturnoutperformedthe
maximumlikelihoodoptimizedmodel. Qualitatively,theoutputoftheRLmodelwasconsistently
clean and concise. In contrast, the outputof the maximum likelihood model contained repetition
of sectionsof the inputand prioroutputas well asoccasionalrandomtokens. These effectswere
presentforbothdatasetsbutwere morepronouncedforsamsum. Incontrast, the outputforxsum
appearedtobesignificantlyverboseevenwhenaccountingfortheextratokensduetorepetition.
Asanadditionaltest, performanceofthebaselineandmaximumlikelihoodmodelswasevaluated
afterlightpost-processing.Thebaselinemodelsoftenstartedtheirresponsewithastatementsimilar
to “Sure, here is a summary...” followed by a newline character even when instructed to output
the summary only. For the cleaned version, the text leading up to the first newline character was
removed if the response contained a newline and started with “Sure,”. The maximum likelihood
models required two types of cleanup. First, repetition of the input had to be removed. Second,
forxsum,theoutputwassignificantlyverboserelativetothegroundtruthandhadtobeshortened.
Thereisnotaprincipledwaytoachievethelatter,butareasonableattemptgiventhenatureofthe
data(extremesummarization)istothrowawayeverythingafterthefirstnewline.Thisassumesthat
thebeginningofthesummarycontainshighlyrelevantinformation—whetherornotthisassumption
is correct is an empirical question. For both datasets, truncatingthe outputafter the first newline
character was also a reasonable heuristic to remove repetition1. Thus, this simple transformation
was uniformly applied to both datasets to achieve the dual aims. The results are displayed in the
lasttwocolumnsofTables1and2. Post-processingtheoutputsofthemaximumlikelihoodsmodels
bridgedtheirperformancegapwiththeRLmodels,withthecleanedupversionscomingoutslightly
aheadonsomemeasures.
Table1:Extralengthrelativetogroundtruth
Base MaxLL RL Basecleaned MaxLLcleaned
samsum 39.2 12.7 2.9 28.9 5.8
xsum 44.7 42.0 -1.1 39.6 -1.8
5 Discussion
Reinforcementlearning-basedfine-tuningproducedcleaner outputsandbeatmaximumlikelihood
tunedmodelsonsemanticandn-grambasedevaluationmetrics. However,lightpost-processingof
themaximumlikelihoodoutputswasabletobridgetheobservedperformancegapforthedatasets
analyzedhere.Theresultsofthisworkcanbeinterpretedfromtwoperspectives.First,fromaprac-
ticalperspective,the immediateutilityofthe methodmayinitially appeardiminishedbythegood
performanceof maximum likelihood with post-processing. However, the post-processing applied
to the xsum dataset was to arbitrarily truncate the outputs, which is a procedure whose utility is
idiosyncratictospecifictypesofdata. Thus,itwouldstillbeusefultoattemptthemethodacrossa
rangeofotherchallengingfine-tuningscenarios.
1This is in contrast to separately attempting to remove repetition using a more complex decoding
scheme[e.g.27],whichmaybenecessarywithotherdata.
5Table2:Evaluationmetrics
samsum xsum
Base Max RL Base Max Base Max RL Base Max
LL cleaned LL LL cleaned LL
cleaned cleaned
bleurt 0.51 0.57 0.58 0.54 0.58 0.32 0.37 0.48 0.33 0.49
la-rouge1-F1 0.21 0.39 0.43 0.25 0.42 0.14 0.18 0.35 0.15 0.36
la-rouge1-precision 0.21 0.39 0.44 0.25 0.43 0.14 0.18 0.35 0.15 0.36
la-rouge1-recall 0.21 0.39 0.43 0.25 0.42 0.14 0.18 0.35 0.15 0.36
la-rouge2-F1 0.08 0.20 0.21 0.10 0.22 0.03 0.07 0.15 0.03 0.15
la-rouge2-precision 0.08 0.20 0.21 0.10 0.22 0.03 0.06 0.15 0.03 0.15
la-rouge2-recall 0.08 0.20 0.21 0.10 0.22 0.03 0.07 0.15 0.03 0.15
la-rougeL-F1 0.16 0.32 0.35 0.19 0.35 0.09 0.13 0.28 0.10 0.29
la-rougeL-precision 0.16 0.32 0.36 0.20 0.35 0.09 0.13 0.28 0.10 0.29
la-rougeL-recall 0.16 0.31 0.35 0.19 0.34 0.09 0.13 0.28 0.10 0.29
rouge1-F1 0.31 0.46 0.50 0.35 0.50 0.21 0.26 0.39 0.22 0.40
rouge1-precision 0.21 0.43 0.50 0.26 0.49 0.14 0.18 0.41 0.15 0.43
rouge1-recall 0.65 0.60 0.56 0.63 0.58 0.45 0.54 0.39 0.44 0.39
rouge2-F1 0.11 0.24 0.24 0.13 0.25 0.05 0.10 0.16 0.05 0.17
rouge2-precision 0.08 0.22 0.25 0.10 0.25 0.03 0.07 0.17 0.03 0.18
rouge2-recall 0.25 0.30 0.27 0.25 0.30 0.10 0.21 0.16 0.10 0.16
rougeL-F1 0.23 0.38 0.41 0.27 0.40 0.14 0.19 0.31 0.15 0.33
rougeL-precision 0.16 0.35 0.41 0.20 0.40 0.09 0.13 0.33 0.10 0.35
rougeL-recall 0.50 0.49 0.46 0.49 0.48 0.31 0.40 0.31 0.30 0.32
Second,thecurrentworkprovidesageneralframeworkthatcanfurtherbebuiltupon.Anadvantage
of the RL-based approach is the ability to intuitively encode undesirable outputs via the reward
model and to optimize the policy for each state to be cognizant of long-term cumulative reward.
Thiswasdemonstratedwithbasicclassesofoutputs,butmoresignificantgainsmayberealizedby
encodingmorecomplexclasses. Forexample,hallucinationsandvalidbutpoorsummariescanbe
included, which could possibly be produced in an automated fashion. In order to train a reward
model for these kinds of outputs, it may be necessary to modify the reward function from being
token-based to being sequence-based, as in such cases it only makes sense to mark an output as
“bad”afteratleastpartofthesequenceoreventheentiresequenceisproduced.Thiswouldposea
greaterchallengeforpolicyoptimizationbecausetherewardsignalwouldbesparseanddelayedin
comparisontousingtoken-basedrewardsandfittingthemodelmayrequiregreatercare.
Adisadvantageoftheapproachpresentedisthesignificantadditionalcomputationalburdenoffitting
allofthereinforcementlearningrelatedmodels,especiallyrunningtheLLMforwardduringpolicy
optimization. While the focusherehasbeenonlast-mile fine-tuningto a specific dataset, if some
additionalgeneralizationerroristolerable,itmaybepossibletoincurthesecostsoncebytraining
oncombinedrepresentativedata.
TheuseofRLforlast-milefine-tuningwasdemonstratedforsummarization,butthesameapproach
canbe usedforanytask. Finally,the goalof thecurrentworkwas to contrasttheperformanceof
RL with maximumlikelihood within an as identical as possible experimentalsetup rather than to
producestate-of-the-artresults. However,theprocedurepresentedisindependentofthesizeofthe
model, whether the model is quantized, and whether all of the model’s parametersare fine-tuned
or LoRA is employed, which all impact absolute performance. In addition, althoughthe framing
andfocushasbeenonLLMs,theprocedurecanlikelyeasilybemodifiedtoworkwithspecialized
state-of-the-art task-dependent architectures, and serve as a more general additional option when
selectingamongoptimizationalgorithmsforfine-tuning.
6References
[1] Google. Google Gemini AI, 2024. URL https://blog.google/technology/ai/
google-gemini-ai/.
[2] OpenAI. GPT-4TechnicalReport. arXivpreprintarXiv:2303.08774,2023.
[3] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, SandhiniAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Chris-
tiano,JanLeike,andRyanLowe.Traininglanguagemodelstofollowinstructionswithhuman
feedback. AdvancesinNeuralInformationProcessingSystems,36:27730–27744,2022.
[4] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra, PrajjwalBhargava,ShrutiBhosale,andothers. Llama2:
Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[5] JohnSchulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,and OlegKlimov. Proximal
policyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
[6] RafaelRafailov, ArchitSharma, EricMitchell, ChristopherD Manning,Stefano Ermon,and
ChelseaFinn.Directpreferenceoptimization:Yourlanguagemodelissecretlyarewardmodel.
AdvancesinNeuralInformationProcessingSystems,37:53728–53741,2023.
[7] Ramakanth Pasunuru and Mohit Bansal. Multi-Reward Reinforced Summarization with
SaliencyandEntailment. Proceedingsofthe2018ConferenceoftheNorthAmericanChapter
ofthe AssociationforComputationalLinguistics: HumanLanguageTechnologies, Volume2
(ShortPapers),pages646–653,2018.
[8] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive
summarization. InternationalConferenceonLearningRepresentations,2018.
[9] Piji Li, Lidong Bing, and Wai Lam. Actor-critic based training framework for abstractive
summarization. arXivpreprintarXiv:1803.11070,2018.
[10] Yen-ChunChenandMohitBansal. FastAbstractiveSummarizationwithReinforce-Selected
SentenceRewriting. Proceedingsofthe56thAnnualMeetingoftheAssociationforComputa-
tionalLinguistics(Volume1: LongPapers),pages675–686,2018.
[11] YuxiangWuandBaotianHu.LearningtoExtractCoherentSummaryviaDeepReinforcement
Learning.ProceedingsoftheAAAIConferenceonArtificialIntelligence,32:5602–5609,2018.
[12] YaserKeneshloo,NarenRamakrishnan,andChandanK.Reddy. Deeptransferreinforcement
learningfortextsummarization. Proceedingsofthe2019SIAMInternationalConferenceon
DataMining(SDM),pages675–683,2019.
[13] Sanghwan Bae, Taeuk Kim, Jihoon Kim, and Sang-goo Lee. Summary Level Training of
SentenceRewritingforAbstractiveSummarization. Proceedingsofthe2ndWorkshoponNew
FrontiersinSummarization,pages10–20,2019.
[14] Qicai Wang, Peiyu Liu, Zhenfang Zhu, Hongxia Yin, Qiuyue Zhang, and Lindong Zhang.
A Text Abstraction Summary Model Based on BERT Word Embedding and Reinforcement
Learning. AppliedSciences,9(21):4701,2019.
[15] Siyao Li, Deren Lei, Pengda Qin, and William Yang Wang. Deep ReinforcementLearning
with Distributional Semantic Rewards for Abstractive Summarization. Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-
nationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages6038–
6044,2019.
[16] Yang Liu and Mirella Lapata. Text Summarization with Pretrained Encoders. Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
3730–3740,2019.
7[17] JingqingZhang, Yao Zhao, MohammadSaleh, and Peter Liu. PEGASUS: Pre-trainingwith
extractedgap-sentencesforabstractivesummarization. Proceedingsofthe37thInternational
ConferenceonMachineLearning,pages11328–11339,2020.
[18] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. JournalofMachineLearningResearch,21(1):1–67,2020.
[19] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec
Radford,DarioAmodei,andPaulFChristiano. Learningtosummarizewithhumanfeedback.
AdvancesinNeuralInformationProcessingSystems,34:3008–3021,2020.
[20] MarkusBayer,Marc-Andre´Kaufhold,andChristianReuter. ASurveyonDataAugmentation
forTextClassification. ACMComputingSurveys,55(7):1–39,2023.
[21] StevenFeng,VarunGangal,JasonWei,SarathChandar,SoroushVosoughi,TerukoMitamura,
and Eduard Hovy. A Survey of Data Augmentation Approaches for NLP. Findings of the
AssociationforComputationalLinguistics:ACL-IJCNLP2021,pages968–988,2021.
[22] JasonWeiandKaiZou.EDA:EasyDataAugmentationTechniquesforBoostingPerformance
on TextClassification Tasks. Proceedingsof the 2019 Conference on Empirical Methodsin
NaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguage
Processing(EMNLP-IJCNLP),pages6382–6388,2019.
[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In-
ternationalConferenceonLearningRepresentations,2022.
[24] AmyPu,HyungWonChung,AnkurParikh,SebastianGehrmann,andThibaultSellam.Learn-
ingCompactMetricsforMT. Proceedingsofthe2021ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,pages751–762,2021.
[25] BogdanGliwa,IwonaMochol,MaciejBiesek,andAleksanderWawer. SAMSumCorpus: A
Human-annotatedDialogue Dataset for Abstractive Summarization. Proceedingsof the 2nd
WorkshoponNewFrontiersinSummarization,pages70–79,2019.
[26] ShashiNarayan,ShayB.Cohen,andMirellaLapata.Don’tGiveMetheDetails,JusttheSum-
mary!Topic-AwareConvolutionalNeuralNetworksforExtremeSummarization.Proceedings
ofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1797–
1807,2018.
[27] YixuanSu,TianLan,YanWang,DaniYogatama,LingpengKong,andNigelCollier. Acon-
trastive framework for neural text generation. Advances in Neural Information Processing
Systems,36:21548–21561,2022.
8