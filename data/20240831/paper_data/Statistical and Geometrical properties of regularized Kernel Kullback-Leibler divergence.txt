Statistical and Geometrical properties of regularized
Kernel Kullback-Leibler divergence
ClémentineChazal AnnaKorba
CREST,ENSAE CREST,ENSAE
IPParis IPParis
clementine.chazal@ensae.fr anna.korba@ensae.fr
FrancisBach
INRIA-EcoleNormaleSupérieure
PSLResearchuniversity
francis.bach@inria.fr
Abstract
Inthispaper,westudythestatisticalandgeometricalpropertiesoftheKullback-
Leiblerdivergencewithkernelcovarianceoperators(KKL)introducedbyBach
[2022].UnliketheclassicalKullback-Leibler(KL)divergencethatinvolvesdensity
ratios,theKKLcomparesprobabilitydistributionsthroughcovarianceoperators
(embeddings)inareproduciblekernelHilbertspace(RKHS),andcomputethe
Kullback-Leiblerquantumdivergence. Thisnoveldivergencehencesharesparallel
butdifferentaspectswithboththestandardKullback-Leiblerbetweenprobability
distributionsandkernelembeddingsmetricssuchasthemaximummeandiscrep-
ancy. AlimitationfacedwiththeoriginalKKLdivergenceisitsinabilitytobe
definedfordistributionswithdisjointsupports. Tosolvethisproblem,wepropose
inthispaperaregularisedvariantthatguaranteesthatthedivergenceiswelldefined
foralldistributions. Wederiveboundsthatquantifythedeviationoftheregularised
KKLtotheoriginalone,aswellasfinite-samplebounds. Inaddition,weprovide
aclosed-formexpressionfortheregularisedKKL,specificallyapplicablewhen
thedistributionsconsistoffinitesetsofpoints,whichmakesitimplementable. Fur-
thermore,wederiveaWassersteingradientdescentschemeoftheKKLdivergence
inthecaseofdiscretedistributions,andstudyempiricallyitspropertiestotransport
asetofpointstoatargetdistribution.
1 Introduction
Afundamentaltaskinmachinelearningistoapproximateatargetdistributionq. Forexample,in
Bayesianinference[Gelmanetal.,1995],itisofinteresttoapproximateposteriordistributionsofthe
parametersofastatisticalmodelforpredictiveinference. Thishasledtothevastdevelopmentof
parametricmethodsfromvariationalinference[Bleietal.,2017],ornon-parametriconessuchas
MarkovChainMonteCarlo(MCMC)[RobertsandRosenthal,2004],andmorerecentlyparticle-
basedoptimization[LiuandWang,2016,Korbaetal.,2021]. Ingenerativemodelling[Brocketal.,
2019,Hoetal.,2020,Songetal.,2020,Franceschietal.,2023]onlysamplesfromqareavailableand
thegoalistogeneratedatawhosedistributionissimilartothetrainingsetdistribution. Generally,this
problemcanbecastasanoptimizationproblemoverP(Rd),thespaceofprobabilitydistributions
overRd,wheretheoptimizationobjectiveischosenasadissimilarityfunctionD(·|q)(adistance
ordivergence)betweenprobabilitydistributions, thatonlyvanishesatq. Startingfromaninitial
distributionp ,adescentschemecanthenbeappliedsuchthatthetrajectory(p ) approachesq.
0 t t≥0
Preprint.Underreview.
4202
guA
92
]LM.tats[
1v34561.8042:viXraInparticular,onthespaceofprobabilitydistributionswithboundedsecondmomentP (Rd),one
2
canconsidertheWassersteingradientflowofthefunctionalF(p) = D(p|q). Thelatterdefinesa
pathofdistributions,ruledbyavelocityfield,thatisofsteepestdescentforF withrespecttothe
Wasserstein-2distancefromoptimaltransport.
Thisapproachhasledtoalargevarietyofalgorithmsbasedonthechoiceofaspecificdissimilarity
functionalF,oftendeterminedbytheinformationavailableonthetargetq. Forexample,inBayesian
orvariationalinference,wherethetarget’sdensityisknownuptoanintractablenormalizingconstant,
acommonchoiceforthecostistheKullback-Leibler(KL)divergence,whoseoptimizationistractable
inthatsetting[Wibisono,2018,Ranganathetal.,2014]. Whenonlysamplesofq areavailable,it
is not convenient to choose the optimization cost as the KL, as it is only defined for probability
distributionspthatareabsolutelycontinuouswithrespecttoq. Incontrast,itismoreconvenient
tochooseanF thatcanbewrittenasintegralsagainstq,forinstance,maximummeandiscrepancy
(MMD)[Arbeletal.,2019,Hertrichetal.,2023],sliced-Wassersteindistance[Liutkusetal.,2019]
andSinkhorndivergence[Genevayetal.,2018]. However,sliced-Wassersteindistances,thataverage
optimaltransportdistancesof1-dimensionalprojectionsofprobabilitydistributions(slices)overan
infinitenumberofdirections,havetobeapproximatedbyafinitenumberofdirectionsinpractice
[Tanguyetal.,2023];andSinkhorndivergencesinvolvesolvingarelaxedoptimaltransportproblem.
Incontrast, MMDcanbewritteninclosed-formfordiscretemeasuresthankstothereproducing
propertyofpositivedefinitekernels. TheMMDrepresentsprobabilitydistributionsthroughtheir
kernelmeanembeddingsinareproducingkernelHilbertspace(RKHS),andcomputetheRKHS
normofthedifferenceofembeddings(namely,thewitnessfunction). Moreover,theMMDflowwith
asmoothkernel(e.g.,Gaussian)asinArbeletal.[2019]iseasytoimplement,asthevelocityfieldis
expressedasthegradientofthewitnessfunction,andpreservediscretemeasures. However,duetothe
non-convexityoftheMMDintheunderlyingWassersteingeometry[Arbeletal.,2019],itsgradient
flow is often stuck in local minimas in practice even for simple target as Gaussian q, calling for
adaptiveschemestuningthelevelofnoiseorkernelhyperparameters[Xuetal.,2022,Galashovetal.,
2024]. MMDwithnon-smoothkernels,e.g.,basedonnegativedistances[Sejdinovicetal.,2013],
have also attracted attention recently, as their gradient flow enjoys better empirical convergence
propertiesthanthepreviousones[Hertrichetal.,2024,2023]. However,theirgradientflowdoes
notpreservediscretemeasures;andtheirpracticalsimulationrelyonimplicittimediscretizations
[Hertrichetal.,2024]orslicing[Hertrichetal.,2023].
BycontrastwiththeMMDwithsmoothkernels,theKLdivergenceisdisplacementconvex[Villani,
2009,Definition16.5]whenthetargetislog-concave(i.e.,qhasadensityq ∝e−V withV convex),
anditsgradientflowenjoysfastconvergencewhenqsatisfiesalog-Sobolevinequality[Bakryetal.,
2014].
Inthisregard,itenjoysbettergeometricalpropertiesthantheMMD.Moreover,theKLdivergenceis
equaltoinfinityforsingularpandq,whichmakesitsgradientflowextremelysensitivetomismatch
ofsupport,sothattheflowenforcestheconcentrationonthesupportofqasdesired.Onthedownside,
while the Wasserstein gradient flow of KL divergences is well-defined [Chewi et al., 2020], its
associatedparticle-baseddiscretizationisdifficulttosimulatewhenonlysamplesofqareavailable,
andasurrogateoptimizationproblemusuallyneedstobeintroduced[Gaoetal.,2019,Ansarietal.,
2020,Simonsetal.,2022,Birrelletal.,2022a,Liuetal.,2022]. However,itisunclearwhetherthis
surrogateoptimizationproblempreservesthegeometryoftheKLflow.
Recently,Bach[2022]introducedalternativedivergencesbasedonquantumdivergencesevaluated
throughkernelcovarianceoperators,thatwecallhereakernelKullback-Leibler(KKL)divergence.
The latter can be seen as second-order embeddings of probability distributions, in contrast with
first-orderkernelmeanembeddings(asusedinMMD).InBach[2022],itwasshownthattheKKL
enjoysnicepropertiessuchasseparationofmeasures,andthatitisframedbetweenastandardKL
divergence(fromabove)andasmoothedKLdivergence(frombelow),i.e.,aKLdivergencebetween
smoothedversionsofthemeasureswithrespecttoaspecificsmoothingkernel. Hence,itcannot
directlybeidentifiedtoaKLdivergenceandcorrespondstoanovelanddistinctdivergence. However,
manyofitspropertiesremainedunexplored,includingacompleteanalysisoftheKKLforempirical
measures,atractableclosed-formexpressionanditsoptimizationproperties. Inthispaper,wetackle
thepreviousquestions. WeproposearegularizedversionoftheKKLthatiswell-definedforany
discretemeasures,incontrastwiththeoriginalKKL. Weestablishupperboundsthatquantifythe
deviationoftheregularisedKKLtoitsunregularizedcounterpart,andconvergenceforempirical
distributions. Moreover,wederiveatractableclosed-formfortheregularisedKKLanditsderivatives
2thatwriteswithrespecttokernelGrammatrices,leadingtoapracticaloptimizationalgorithm.Finally,
weinvestigateempiricallythestatisticalpropertiesoftheregularisedKKL,aswellasitsgeometrical
propertieswhenusingitasanobjectivetotargetaprobabilitydistributionq.
Thispaperisorganizedasfollows. Section2introducesthenecessarybackgroundandtheregularised
KKL. Section3presentsourtheoreticalresultsonthedeviationandfinite-samplepropertiesofthe
latter. Section4providestheclosed-formofregularisedKKLfordiscretemeasuresaswellasthe
practicaloptimizationschemebasedonanexplicittime-discretisationofitsWassersteingradientflow.
Section5discussescloselyrelatedworkincludingdistancesordivergencesbetweendistributions
basedonreproducingkernels. Finally,Section6illustratesthestatisticalandoptimizationproperties
oftheKKLonavarietyofexperiments.
2 RegularizedkernelKullback-Leibler(KKL)divergence
Inthissection,westateournotationsandpreviousresultsonthe(original)kernelKullback-Leibler
(KKL)divergenceintroducedbyBach[2022],beforeintroducingourproposedregularisedversion.
Notations. LetP(Rd)thesetofprobabilitymeasuresonRd. LetP (Rd)thesetofprobability
2
measures on Rd with finite second moment, which becomes a metric space when equipped with
Wasserstein-2(W )distance[Villani,2009].
2
Forp∈P(Rd),wedenotethatpisabsolutelycontinuousw.r.t.qusingp≪q,andweusedp/dq
todenotetheRadon-Nikodymderivative. WerecallthestandarddefinitionoftheKullback-Leibler
(cid:82)
divergence,KL(p||q)= log(dp/dq)dpifp≪q,+∞else.
Ifg :Rd →Rr isdifferentiable,wedenotebyJg :Rd →Rr×ditsJacobian. Ifr =1,wedenoteby
∇gthegradientofgandHgitsHessian.
Ifr =d,∇·gdenotesthedivergenceofg,i.e.,thetraceoftheJacobian. Wealsodenoteby∆gthe
Laplacianofg,where∆g =∇·∇g. WealsodenoteI theidentitymatrixoroperator.
Forapositivesemi-definitekernelk : Rd ×Rd → R, itsRKHSH isaHilbertspacewithinner
product ⟨·,·⟩ and norm ∥·∥ . For q ∈ P (Rd) such that (cid:82) k(x,x)dq(x) < ∞, the inclusion
H H 2
operatorι :H→L2(q), f (cid:55)→f isaboundedoperatorwithitsadjointbeingι∗ :L2(q)→H, f (cid:55)→
q q
(cid:82)
k(x,·)f(x)dq(x) [Steinwart and Christmann, 2008, Theorem 4.26 and 4.27]. The covariance
operatorw.r.t.qisdefinedasΣ =(cid:82) k(·,x)⊗k(·,x)dq(x)=ι∗ι ,where(a⊗b)c=⟨b,c⟩ afor
q q q H
a,b,c∈H. ItcanalsobewrittenΣ =(cid:82) φ(x)φ(x)∗dq(x)where∗denotesthetranspositioninH
q Rd
(recallthatforu∈H,uu∗ :H→Hdenotestheoperatoruu∗(f)=⟨f,u⟩ uforanyf ∈H).
H
Kernel Kullback-Leibler divergence (KKL). For p,q ∈ P(Rd), the kernel Kullback-Leibler
divergence(KKL)isdefinedinBach[2022]as:
(cid:18) (cid:19)
(cid:88) λ
KKL(p||q):=Tr(Σ logΣ )−Tr(Σ logΣ )= λlog ⟨f ,g ⟩2 . (1)
p p p q γ λ γ H
(λ,γ)∈Λp×Λq
whereΛ andΛ arethesetofeigenvaluesofthecovarianceoperatorsΣ andΣ ,withassociated
p q p q
eigenvectors (f ) and (g ) . The KKL (1) evaluates the Kullback-Leibler divergence
λ λ∈Λp γ γ∈Λq
between operators on Hilbert Spaces, that is well-defined for any couple of positive Hermitian
operatorswithfinitetrace,attheoperatorsΣ andΣ . FromBach[2022,Proposition4],ifpand
p q
q are supported on compact subset of Rd, and if k is a continuous positive definite kernel with
k(x,x)=1forallx∈Rd,andifk2isuniversal[SteinwartandChristmann,2008,Definition4.52],
then KKL(p||q) = 0 if and only if p = q. In Bach [2022], it also was proven that the KKL is
upperboundedbythe(standard)KL-divergencebetweenprobabilitydistributions(seeProposition4
therein)andlowerboundedbythesameKLbutevaluatedatsmoothedversionsofthedistributions,
wherethesmoothingisaconvolutionwithrespecttoaspecifickernel(seeSection4therein). Thus,
theKKLdefinesanoveldivergencebetweenprobabilitymeasures. Itdefinesthenaninteresting
candidateastocompareprobabilitydistributions,forinstancewhenusedasanoptimizationobjective
overP(Rd),inordertoapproximateatargetdistributionq.
DefinitionoftheregularizedKKL. AmajorissuethattheKKLshareswiththestandardKullback-
Leibler divergence between probability distributions, is that it diverges if the support of p is not
3includedintheoneofq(1). Indeed,fortheKKL(p||q)tobefinite,weneedKer(Σ )⊂Ker(Σ ).
q p
This condition is satisfied when the support of p is included in the support of q. Indeed, if f ∈
Ker(Σ ),then⟨f,Σ f⟩ = (cid:82) f(x)2dq(x) = 0,andsof iszeroonthesupportofq,thenalsoon
q q Rd
thesupportofp. Hence,theKKLisnotaconvenientdiscrepancywhenqisadiscretemeasure(in
particular,ifpisalsodiscretewithdifferentsupportthanq). Asimplefixthatweproposeinthis
paperistoconsideraregularizedversionofKKLwhichis,forα∈]0,1[,
KKL (p||q):=KKL(p||(1−α)q+αp)=Tr(Σ logΣ )−Tr(Σ log((1−α)Σ +αΣ )). (2)
α p p p q p
The advantage of this definition is that KKL is finite for any distribution p,q. It smoothes the
α
distributionqbymixingitwiththedistributionp,toadegreedeterminedbytheparameterα. Such
divergenceisanapproximationoftheoriginalKKLdivergencethatdoesnotrequirethedistribution
ptobeabsolutelycontinuouswithrespecttoq. Moreover,foranyα∈]0,1[,KKL (p||q)=0ifand
α
onlyifp=q. Asα→0,werecovertheoriginalKKL(1),andasα→1,thisquantityconverges
pointwisetozero.
Remark 1. The regularization we consider in (2) has also been considered for the standard KL
divergence[Lee,2000]. Theseobjects,aswellastheirsymmetrizedversion,werealsoreferredto
intheliteratureasskeweddivergences[KimuraandHino,2021]. ThemostfamousoneisJensen-
Shannondivergence,recoveredasasymmetrizedskewedKLdivergenceforα= 1,thatisdefinedas
2
JS(p,q)=KL(p||1p+ 1q)+KL(q||1p+ 1q).
2 2 2 2
3 SkewnessandconcentrationoftheregularizedKKL
InthissectionwestudytheskewnessoftheregularisedKKLduetotheintroductionoftheparameter
α,aswellasitsconcentrationpropertiesforempiricalmeasures.
Skewness. WewillfirstanalyzehowtheregularisedKKLbehaveswithrespecttotheregularization
parameterα. First,weshowitismonotonewithrespecttoαinthefollowingProposition.
Proposition2. Letp≪q. Thefunctionα(cid:55)→KKL (p||q)isdecreasingon[0,1].
α
Proposition2showsthattheregularisedKKLsharesasimilarmonotonybehaviorthantheregularised,
orskewed,(standard)KLbetweenprobabilitydistributions,asrecalledinAppendixA.1. Theproof
ofProposition2canbefoundinAppendixB.1. ItreliesonthepositivityoftheKKLdivergence,and
theuseoftheidentity[Ando,1979]
(cid:90) +∞
Tr(Σ (logΣ −logΣ ))=
Tr(cid:0)
Σ (Σ
+βI)−1(cid:1) −Tr(cid:0)
Σ (Σ
+βI)−1(cid:1)
dβ, (3)
p p q p p q q
0
whereI istheidentityoperator,thatisusedinallourproofs. Wenowfixα ∈]0,1[andprovidea
quantitativeresultaboutthedeviationoftheregularised(orskewed)KKLtoitsoriginalcounterpart.
Proposition3. Letp,q ∈P(Rd). Assumethatp≪qandthat dp ⩽ 1 forsomeµ>0. Then,
dq µ
(cid:18) (cid:18) 1(cid:19) α2 (cid:18) 1 (cid:19)(cid:19)
|KKL (p||q)−KKL(p||q)|⩽ α 1+ + 1+ |Tr(Σ logΣ )|. (4)
α µ 1−α µ2 p q
Proposition3recoversasimilarquantitativeboundthantheonewecanobtainforthestandardKL
betweenprobabilitydistributions,seeAppendixA.2;andstatethattheskewnessoftheregularized
KKLcanbecontrolledbytheregularizationparameterα. However,thetoolsusedtoderivethis
inequalityarecompletelydifferentbynature. ItscompleteproofcanbefoundinAppendixB.2,but
weprovidehereasketch.
Sketchofproof. LetN =αΣ +(1−α)Σ .WewriteKKL(p||q) −KKL(p||q)=TrΣ logΣ −
p q α p q
TrΣ logΓthatwewriteas(3). Inordertoupperboundthisintegralweusetheoperatorequalities,
p
for two operators A and B, A−1 −B−1 = A−1(B −A)B−1 = A−1(B −A)A−1 −A−1(B −
A)B−1(B−A)A−1whichweapplytoA=Γ+βIandB =Σ +βI.Then,weusetheassumption
q
µΣ ≼Σ andcarefulupperboundingsonpositivesemidefiniteoperatorsusingtheresultsonmatrix
p q
inequalitiesgiveninAppendixA.3toconcludetheproof.
4Statisticalproperties. WenowfocusontheregularisedKKLforempiricalmeasuresandderive
finite-sampleguarantees.
Proposition 4. Let p,q ∈ P(Rd). Assume that p ≪ q and that dp ⩽ 1 for some µ > 0.
dq µ
We remind that φ(x) is the feature map of x ∈ Rd in the RKHS H. Assume also that c =
(cid:82)+∞ sup ⟨φ(x),(Σ +βI)−1φ(x)⟩2dβisfinite. Letpˆ,qˆsupportedonn, mi.i.d. samplesfrom
0 x∈Rd p
pandqrespectively. Wehave:
35 1 √
E|KKL (pˆ||qˆ)−KKL (p||q)|⩽ √ (2 c+logn)
α α m∧nαµ
1 (cid:18) 1 c(24logn)2 n (cid:19)
+ 1+ + (1+ ) . (5)
m∧n µ αµ2 m∧m
Remark 5. It is possible to calculate a similar bound for the above proposition which does not
requirethep≪qcondition. Thisbound,whichcanbefoundinAppendixB.3.3,scalesworstwhen
αgoesto0becauseitscalesinO( 1 )insteadofO(1)here.
α2 α
ThelatterpropositionextendssignificantlyBach[2022,Proposition7]thatprovidedanupperbound
ontheentropytermonly,i.e.,thefirsttermin(1):
1+c(8logn)2 17 √
E[|Tr(Σ logΣ )−Tr(Σ logΣ )|]⩽ + √ (2 c+logn). (6)
pˆ pˆ p p n n
Ourbound(5)isexplicitinthenumberofsamplesn,mforpˆ,qˆ,andforn=mwerecoversimilar
termsas(6). Ourcontributionistoupperboundthecrossterm,i.e.,thesecondtermin(1),involving
bothpandq. Wedosobycloselyfollowtheproofof[Bach,2022,Proposition7]inordertobound
thecrosstermsdifference. Inconsequence,ourproofinvolvestechnicalintermediateresults,among
whichconcentrationofsumsofrandomself-adjointoperators,andestimationofdegreesoffreedom.
TheproofofProposition4canbefoundinAppendixB.3,butweprovidehereasketch.
Sketchofproof. WedenoteΓˆ = αΣ +(1−α)Σ andN itspopulationcounterpart. Inorderto
pˆ qˆ
boundthecrosstermwewriteTrΣ logΓˆ−TrΣ logN using(3). Wesplittheintegralsinthree
pˆ p
terms,withrespecttotwoparameters0<β <β thatweintroduce: (a)oneforβ between0and
0 1
β ,(b)oneforβ betweenβ andinfinityand(c)anintermediateone. Theβ quantityischosento
0 1 0
bedependentofmandn, sothatitconvergetozeroasnandmgotoinfinity. Thisway, for(a)
theintegralbetween0andβ wesimplyhavetoboundTrΣ (Γ+βI)−1andTrΣ (Γˆ+βI)−1by
0 p pˆ
constantorintegrablequantitiescloseto0. Then,for(b),β ischosensothatitgoestoinfinitywhen
1
nandmgotoinfinityand(b)isboundedby1/β . Finallyweupperboundfinelyenough(c)to
1
compensateforthefactthattheboundsoftheintegralstendtowards0andinfinity.
4 Time-discretizedregularizedKKLgradientflow
Inthissection,weshowthattheregularizedKKLcanbeimplementedinclosed-formfordiscrete
measures,aswellasitsWassersteingradient,makingitsoptimizationtractable.
RegularisedKKLclosed-form. WefirstdescribehowtocomputetheregularizedKKLfor(any,
notnecessarilyempirical)discretemeasuresinpractice. Thiswillbeusefulforthepracticalimple-
mentationofregularisedKKLoptimizationcomingnext. Weprovideaclosed-formforthelatter,
involvingkernelGrammatricesbetweensupportsofthediscretemeasures.
Proposition6. Letpˆ= 1 (cid:80)n δ andqˆ= 1 (cid:80)m δ twodiscretedistributions. DefineK =
n i=1 xi m j=1 yj pˆ
(k(x ,x ))n ∈Rn×n,K =(k(y ,y ))m ∈Rm×m,K =(k(x ,y ))n,m ∈Rn×m. Then,
i j i,j=1 qˆ i j i,j=1 pˆ,qˆ i j i,j=1
foranyα∈]0,1[,wehave:
(cid:18) (cid:19)
1 1
KKL (pˆ||qˆ)=Tr K log K −Tr(I Klog(K)),
α n pˆ n pˆ α
 (cid:113) 
whereI
α
=(cid:18) α1 0I 00(cid:19) andK =(cid:113) α(1α n −K α)pˆ
K
α 1( n −1− m αα K)K pˆ,qˆ . (7)
nm qˆ,pˆ m qˆ
5Proposition 6 extends non-trivially the result of Bach [2022, Proposition 6] that only provided a
closed-formfortheentropytermTr(Σ log(Σ )),thatcorrespondstoourfirstterminEquation(7).
pˆ pˆ
ItscompleteproofcanbefoundinAppendixB.4butweprovidehereasketch.
Sketchofproof. Our goal there is to derive a closed-form for the cross-term in pˆ,qˆof the KKL,
that is Tr(Σ log(αΣ +(1−α)Σ )). It is based on the observation that if we define ϕ =
pˆ pˆ qˆ x
(cid:113)
(φ(x ),...,φ(x ))∗,ϕ =(φ(y ),...,φ(y ))∗andψtheconcatenationof(cid:112)αϕ and 1−αϕ ,
1 n y 1 m n x m y
thenthecovarianceoperatorswriteΣ =ψTI ψandαΣ +(1−α)Σ =ψTψ. Then,thematrices
pˆ α pˆ qˆ
K andK writeK =ψI ψT andψψT =K. Finally,weapplyanintermediateresult(Lemma12)
pˆ pˆ α
toobtaintheexpressionoflog(αΣ +(1−α)Σ )asafunctionoflogK.
pˆ qˆ
Gradient flow and closed-form for the derivatives. We now discuss how to optimize p (cid:55)→
KKL (p||q)foragiventargetdistributionq.ForagivenfunctionalF :P (Rd)→R+,aWasserstein
α 2
gradientflowofF canbethoughtasananalogobjecttoaEuclideangradientflowinthemetricspace
(P (Rd),W ) [Santambrogio, 2017], which defines a trajectory (p ) in P (Rd) following the
2 2 t t≥0 2
steepestdescentforF withrespecttotheW distance. Itischaracterizedbyacontinuityequation:
2
∂ p +∇·(p ∇F′(p ))=0, (8)
t t t t
whereF′(p) : Rd → RisthefirstvariationofF atp ∈ P (Rd). Werecallthatthefirstvariation
2
atp ∈ P (Rd)asdefinedinAmbrosioetal.[2005, Lemma10.4.1]isdefined, ifitexists, asthe
2
functionF′ :Rd →Rsuchthat
1 (cid:90)
lim F(p+ϵξ)−F(p)= F′(p)(x)dξ(x), (9)
ϵ→0 ϵ
foranyξ = q−p,q ∈ P (Rd). TooptimizeKKL ,itisthennaturaltoconsideritsWasserstein
2 α
gradientflowanddiscretizeitintimeandspace. SinceKKL iswell-definedfordiscretemeasures
α
pˆ,qˆ,wedirectlyderiveitsfirstvariationforthissetting. Ournextresultyieldsaclosed-formforthe
firstvariationoftheregularisedKKL.
Proposition7. Considerpˆ,qˆandthematricesK , K asdefinedinProposition6. Letg(x)= logx.
pˆ x
Then,thefirstvariationofF =KKL (·|qˆ)atpˆis,foranyx∈Rd:
α
F′(pˆ)(x)=1+S(x)Tg(K )S(x)−T(x)Tg(K)T(x)−T(x)TAT(x), (10)
pˆ
where
(cid:18) (cid:19) (cid:32)(cid:114) (cid:114) (cid:33)
1 1 α 1−α
S(x)= √ k(x,x ),...,√ k(x,x ) , T(x)= k(x,x ),...; k(x,y ),... ,
n 1 n n n 1 m 1
andA=n (cid:88)+m ∥a j∥2
c cT
+(cid:88)logη
j
−logη
k⟨a ,a ⟩c cT,
η j j η −η j k j k
j j k
j=1 j̸=k
where(c ) aretheeigenvectorsofK,and(a ) thevectorsoffirstntermsof(c ) .
j j j j j j
TheproofofProposition7canbefoundinAppendixB.5,weprovideasketchhere.
Sketchofproof. Ourproofdealsseparatelywiththeentropyandthecrossterm,writingF =F +F .
1 2
Starting from the definition (9), we note ∆ = εΣ . For F , we write F (pˆ+εξ)−F (pˆ) =
ξ 1 1 1
(cid:80)n
f(λ (Σ +∆))−f(λ (Σ )). Towritethisterm,weusetheresidualformula,whichcanbe
i=1 i pˆ i pˆ
use to differentiate eigenvalues of functions. Indeed, we can write, for an operator A with finite
numberofpositiveeigenvalues,(cid:80) f(λ)=(cid:72) f(z)Tr(cid:0) (zI−A)−1(cid:1)
dzwhereγ isaloopin
λ∈Λ(A) γ
CsurroundingallthepositiveeigenvaluesofA. Applyingthistoourcase,ifwechoseγ suchthatit
surroundsboththeeigenvaluesofΣ andofΣ
+∆,weobtain(cid:80)n
f(λ (Σ +∆))−f(λ (Σ ))=
pˆ pˆ i=1 i pˆ i pˆ
1 (cid:72) f(z)Tr(cid:0) (zI−Σ −∆)−1(cid:1) −f(z)Tr(cid:0) (zI−Σ )−1(cid:1) dz. UsingtheidentityA−1−B−1 =
2iπ γ pˆ pˆ
A−1(B−A)A−1+o(B−A),thepreviousquantitybecomes(cid:80)n
f(λ (Σ +∆))−f(λ (Σ ))=
i=1 i pˆ i pˆ
1 (cid:72) (cid:80)n f(z) dzTr(f∗f ∆)+o(ε). Undertheintegralwerecogniseaholomorphicfunction
2iπ γ k=1 (z−λk)2 k k
withisolatedsingularitiesandwecanthereforeapplytheresidueformulaagain. ConcerningF ,we
2
proceedinthesameway,withthedifferencethataswehaveacrossterm,eigenvectorswillappearin
thecalculationandinthefinalresult.
6LeveragingtheanalyticalformforthefirstvariationgivenbyProposition7,theWassersteingradient
ofF =KKL (·|qˆ)atpisgivenby∇F′(p):Rd →Rdbytakingthegradientwithrespecttoxin
α
Equation(10). Noticethatthelatteronlyinvolvesderivativeswithrespecttothekernelk,andcan
becomputedinO((n+m)3)duetothesingularvaluedecompositionofthematrixK definedin
Proposition6.
Startingfromsomeinitialdistributionp ,andforsomegivenstep-sizeγ >0,aforward(orexplicit)
0
time-discretization of (8) corresponds to the Wasserstein gradient descent algorithm, and can be
writtenateachdiscretetimeiterationl≥1as:
p =(Id−γ∇F′(p )) p (11)
l+1 l # l
where Id is the identity map in L2(p ) and # denotes the pushforward operation. For discrete
l
measuresµ
n
=1/n(cid:80)n i=1δ xi,wecandefineF(Xn):=F(p n)whereXn =(x1,...,xn),sincethe
functionalF iswelldefinedfordiscretemeasures. TheWassersteingradientflowofF (8)becomes
the standard euclidean gradient flow of the particle based function F. Furthermore, Wasserstein
gradientdescent(11)writesaseuclideangradientdescentonthepositionoftheparticles.
5 Relatedwork
Divergencesbasedonkernelsembeddings. Kernelshavebeenusedextensivelytodesignuseful
distancesordivergencesbetweenprobabilitydistributions,astheyprovideseveralwaystorepresent
probabilitydistributions,e.g.,throughtheirkernelmeanorcovarianceembeddings. TheMaximum
Mean Discrepancy (MMD) [Gretton et al., 2012] is maybe the most famous one. It is defined
(cid:82)
as the RKHS norm of the difference between the mean embeddings m := k(x,·)dp(x) and
p
(cid:82)
m := k(x,·)dq(x),i.e.,MMD(p∥q)=∥m −m ∥ .Whenkischaracteristic,MMD(p∥q)=0
q p q H
ifandonlyifµ=π[Sriperumbuduretal.,2010]. MMDbelongstothefamilyofintegralprobability
metrics[Müller,1997]asitcanbewrittenasMMD(p∥q)=sup E [f(X)]−E [f(X)].
f∈H,∥f∥H≤1 p q
Alternatively,itcanbeseenastheL2-distancebetweenkerneldensityestimators. Itbecamepopular
instatisticsandmachinelearningthroughitsapplicationsintwo-sampletest[Grettonetal.,2012],or
morerecentlyingenerativemodeling[Bin´kowskietal.,2018].
However, kernel mean embeddings are not the only way (and maybe not the most powerful) to
represent probability distributions. For instance, MMD may be not discriminative enough when
the distributions differ only in their higher-order moments but have the same mean embedding.
Forthisreason,severalworkshaveresortedtoteststatisticsthatincorporatethekernelcovariance
operator of the probability distributions. For instance, Harchaoui et al. [2007] construct a test
statistic that resembles the MMD(p∥q) but incorporating covariance operators (more precisely,
∥(Σ +βI)−1(m −m )∥ )yieldinginsomesenseachi-squaredivergencebetweenthetwo
p+q p q H
2
distributions. This work has been recently generalized in Hagrass et al. [2022] to more general
spectralregularizations. AsimilarregularizedMMDstatisticisemployedbyBalasubramanianetal.
[2021],Hagrassetal.[2023]inthecontextofthegoodness-of-fittest.
KernelvariationalapproximationoftheKL. Analternativeuseofkernelstocomputeprobability
divergencesisthroughapproximationofvariationalformulationsoff-divergences[Nguyenetal.,
2010,Birrelletal.,2022b]ofwhichKL-divergenceisanexample.Indeed,theKLdivergencebetween
pandqwritessup (cid:82) gdp−(cid:82) egdqwhereM denotesthesetofallboundedmeasurablefunctions
g∈Mb b
onRd. Forinstance,Glaseretal.[2021]consideravariationalformulationoftheKLdivergence
restrictedtoRKHSfunctions,namelytheKALEdivergence:
(cid:90) (cid:90) λ
KALE(p||q)=(1+λ)max gdp− egdq− ∥g∥2 (12)
g∈H 2 H
Recently,Neumayeretal.[2024]extendedthelatterworkandstudiedkernelizedvariationalformula-
tionofgeneralf-divergences,referredtoasMoreauenvelopesoff-divergencesinRKHS,including
theKALEasaparticularcase. Theyprovethatthesefunctionalsarelowersemi-continuous,and
thattheirWassersteingradientflowsarewelldefinedforsmoothkernels(i.e.,thefunctionalsare
λ-convex, andthesubdifferentialcontainsasingleelement). However, theKALEdoesnothave
a closed form expression. For discrete distributions p and q supported on n samples, the KALE
divergencecanbewrittenastronglyconvexn-dimensionalproblem,andcanbesolvedusingstandard
7euclideanoptimizationmethods. Still,thismakesthesimulationofKALEWassersteingradientflow
(e.g.,gradientdescentonthepositionsofparticles)computationallydemanding,asitrequiressolving
aninneroptimizationproblemateachiteration. Thisinneroptimizationproblemissolvedcalling
anotheroptimizationalgorithm. Glaseretal.[2021]resorttovariousonesamongtheirexperiments,
amongwhichNewton’smethod(thatscalesasO(n3)duetothematrixinversion),orcheaperones
suchasgradientdescent(GD)orcoordinatedescent. Forlargevaluesoftheregularizationparameter
λ,usingplainGDworksreasonable,butforsmallλ,theproblembecomesquiteill-conditionedand
GDhastoberanforsmallstep-sizes. Moreover,asKALE(anditsgradient)arenotavailablein
closed-form,theycannotbeusedwithfastandhyperparameter-freemethods,suchasL-BFGS[Liu
andNocedal,1989]whichrequiresexactgradients. ThiscontrastswithourregularisedKKLdiver-
genceanditsgradient,whichareavailableinclosed-form. Inourexperiments,wewillinvestigate
furthertherelativeperformanceofKALEandKKL.
6 Experiments
Inthissection,weillustratethevalidityofourtheoreticalresultsandtheperformanceofgradient
descentfortheregularisedKKL. Inallourexperiments,weconsiderGaussiankernelsk(x,y) =
(cid:16) (cid:17)
exp
−∥x−y∥2
where σ denotes the bandwith. Our code is available on the github repository
σ2
https://github.com/cclementine25/KKL-divergence-gradient-flows.git.
Illustrations of skewness and concentration of
theKKL. WefirstillustrateourresultsofProposi-
tion3andProposition4,i.e. theskewnessandcon- = 1e-06
centrationpropertiesofKKL α.Weinvestigatethese 8 = 1e-05
= 0.0001
propertiesforvarioussettingsofp,qtwofixedprob-
= 0.001
abilitydistributionsonRd,varyingthechoiceofα, 6 = 0.01
dimensiond,anddistributionsp,q. Weconsiderem- = 0.1
4 = 0.5
piricalmeasurespˆ,qˆsupportedonni.i.d. samples
ofp,q(inthissectionwetakethesamenumberof
2
samples for both distributions, i.e., n = m in the
notationsofSection3),andweobservetheconcen- 0
trationofKKL (pˆ,qˆ)arounditspopulationlimitas 0 250 500 750 1000
α
thenumberofsamplesn(particles)gotoinfinity. number of particules
Figure1: ConcentrationofempiricalKKL
Eachtime,weplottheresultsobtainedover50runs, α
ford=10,σ =10,p,qGaussians.
randomizingthesamplesdrawnfromeachdistribu-
tion. Thicklinesrepresenttheaveragevalueover
thesemultipleruns. Werepresentthedependence
inαandnindimension10inFigure1, forp,q anisotropicGaussiandistributionswithdifferent
means and variances. Alternative settings and additional results are deferred to the Appendix C,
suchasdifferentdistributions(e.g. aGaussianpversusanExponentialq),aswellasthedimension
dependenceforafixedα. WecanclearlyseeinFigure1themonotonyofKKL withrespectto
α
α(asstatedinProposition2)andtheconcentrationoftheempiricalKKL arounditspopulation
α
version,whichhappensfasterforalargervalueofα,aspredictedbyourfinite-sampleboundsin
Proposition4.
Sampling with KKL gradient descent. Finally, we study the performance of KKL gradient
descentinpractice,asdescribedinSection4. WeconsidertwosettingsalreadyusedbyGlaseretal.
[2021]forKALEgradientflow,reflectingdifferenttopologicalpropertiesforthesource-targetpair: a
pairwithatargetsupportedonahypersurface(zerovolumesupport)andapairwithdisjointsupports
ofpositivevolume. Alternativesettings,e.g. GaussianssourceandmixtureofGaussianstargetthat
arepairsofdistributionswithapositivedensitysupportedonRd,aredeferredtoAppendixC.We
alsoreportthereadditionalplotsrelatedtotheexperimentsofthissection.
Wehavetreatedαasahyperparameterhere,andinthissectionforsimplicityofnotationswereferto
KKLastheobjectivefunctional. WewillconsiderbothautomaticoptimizationviaL-BFGS[Liu
andNocedal,1989]scheme,orconstantstep-sizes. AsbothKKLanditsgradientcanbeexplicitly
computed,onecanimplementdescenteitherusingaconstantstep-size,orthroughaquasi-Newton
algorithmsuchasL-BFGS[LiuandNocedal,1989]. Thelatterisoftenfasterandmorerobustthan
8
)q||p(
LKKtheconventionalgradientdescentanddoesnotrequirechoosingcriticalhyper-parameters,suchas
alearningrate,sinceL-BFGSperformsaline-searchtofindsuitablestep-sizes. Itonlyrequiresa
toleranceparameteronthenormofthegradient,whichisinpracticesettomachineprecision. In
contrast,theKALEanditsgradientisnotavailableinclosed-form.
The first example is a target distribution q supported (and uniformly distributed) on a lower-
dimensional surface that defines three non-overlapping rings, see Figure 2. The initial source
isaGaussiandistributionwithameaninthevicinityofthetargetq.WecompareWassersteingradient
descentofKKL,MaximumMeanDiscrepancy[Arbeletal.,2019]andKALE[Glaseretal.,2021],
usingthecodeprovidedinthesereferences. Foreachmethod,wechooseabandwithσ =0.1,and
weoptimizethestepsizeforeachmethod,andsamplen=100pointsfromthesourceandtarget
distribution. Ourmethodwasrobusttothechoiceofαandgenerallyperformsverywellonthis
example,asshowninFigure2. WecannoticethatsinceMMDisnotsensitivetothedifferenceof
supportbetweenpandq,theparticlesmayleavetherings;whilefortheregularisedKKLflow,asfor
KALEflow,theparticlesfollowcloselythesupportofthetargetdistribution.
Thesecondexampleconsistsofasourceandtargetpairp,qthataresupportedondisjointsubsets,
eachwithafinite,positivevolume,incontrastwiththepreviousexample. Thesourceandthetarget
areuniformsupportedonaheartandaspiralrespectively. WeagainrunMMD,KALEandKKL
gradientdescent. Inthisexample,bothKKLandKALErecoverthespiralshape,muchbeforethe
MMDflowtrajectory;butbothhaveahardertimerecoveringoutliers,disconnectedfromthemain
supportofthespiral.
T=0 T=2 T=30 T=60 T=0 T=10 T=30 T=60 T=99
KALE kale_0001
target
MMD kale_10000
KKL KKL
Figure2: MMD,KALEandKKLflowfor3ringstarget. Figure3: Shapetransfer
7 Conclusion
Inthiswork,weinvestigatedthepropertiesoftherecentlyintroducedKernelKullback-Leibler(KKL)
divergenceasatoolforcomparingprobabilitydistributions. Weprovidedseveraltheoreticalresults,
amongwhichquantitativeboundsonthedeviationfromtheregularisedKKLtotheoriginalone,and
finite-sampleguaranteesforempiricalmeasures,thatarevalidatedbyournumericalexperiments.
Wealsoderivedaclosed-formandcomputableexpressionfortheregularizedKKLaswellasits
derivatives,enablingtoimplement(Wasserstein)gradientdescentforthisobjective. Ourexperiments
validate the use of KKL as a tool to compare discrete measures, as its gradient flow is much
betterbehavedthantheoneofMaximumMeanDiscrepancywhichreliesonlyon"firstmoments"
embeddingsofprobabilitydistributions. Itcanalsobecomputedinclosed-form,incontrasttothe
KALEdivergenceintroducedrecently,andcanbenefitfromfastandhyperparameter-freemethods
suchasL-BFGS.
WhileourstudyhasadvancedourunderstandingoftheKKLdivergence,severallimitationsmust
be acknowledged. Firstly, theoretical guarantees for the convergence of the KKL flow remain
unestablished. Futureworkcouldexplorethisbyeitherdemonstratingthatthefunctionalissmooth
and convex in the Wasserstein sense or by showing that functional inequalities hold for certain
targetdistributions. Secondly,reducingthecomputationalcostiscrucialforpracticalapplications.
Investigatingtheuseofrandomfeaturespresentsapromisingavenueformakingthecomputations
moreefficient.
9References
LuigiAmbrosio,NicolaGigli,andGiuseppeSavaré. GradientFlows: InMetricSpacesandinthe
SpaceofProbabilityMeasures. SpringerScience&BusinessMedia,2005.
TsuyoshiAndo. Concavityofcertainmapsonpositivedefinitematricesandapplicationstohadamard
products. LinearAlgebraanditsApplications,26:203–241,1979.
AbdulFatirAnsari,MingLiangAng,andHaroldSoh. Refiningdeepgenerativemodelsviadiscrimi-
natorgradientflow. arXivpreprintarXiv:2012.00780,2020.
MichaelArbel,AnnaKorba,AdilSalim,andArthurGretton. Maximummeandiscrepancygradient
flow. AdvancesinNeuralInformationProcessingSystems,32,2019.
FrancisBach. Informationtheorywithkernelmethods. IEEETransactionsonInformationTheory,
69(2):752–775,2022.
DominiqueBakry,IvanGentil,andMichelLedoux. AnalysisandGeometryofMarkovDiffusion
Operators,volume103. Springer,2014.
KrishnakumarBalasubramanian,TongLi,andMingYuan. Ontheoptimalityofkernel-embedding
basedgoodness-of-fittests. JournalofMachineLearningResearch,22(1):1–45,2021.
RajendraBhatia. PositiveDefiniteMatrices. PrincetonUniversityPress,2009.
MikołajBin´kowski,DanicaJSutherland,MichaelArbel,andArthurGretton. Demystifyingmmd
gans. InternationalConferenceonLearningRepresentations(ICLR),2018.
Jeremiah Birrell, Paul Dupuis, Markos A Katsoulakis, Yannis Pantazis, and Luc Rey-Bellet.
(f,gamma)-divergences: Interpolating between f-divergences and integral probability metrics.
JournalofMachineLearningResearch,23(39):1–70,2022a.
JeremiahBirrell,MarkosAKatsoulakis,andYannisPantazis. Optimizingvariationalrepresentations
ofdivergencesandacceleratingtheirstatisticalestimation. IEEETransactionsonInformation
Theory,68(7):4553–4572,2022b.
DavidMBlei,AlpKucukelbir,andJonDMcAuliffe. Variationalinference:Areviewforstatisticians.
JournaloftheAmericanstatisticalAssociation,112(518):859–877,2017.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
naturalimagesynthesis. InInternationalConferenceonLearningRepresentations,2019.
SinhoChewi,ThibautLeGouic,ChenLu,TylerMaunu,andPhilippeRigollet. SVGDasakernel-
izedWassersteingradientflowofthechi-squareddivergence. AdvancesinNeuralInformation
ProcessingSystems,33:2098–2109,2020.
Jean-YvesFranceschi,MikeGartrell,LudovicDosSantos,ThibautIssenhuth,EmmanueldeBézenac,
MickaëlChen,andAlainRakotomamonjy.UnifyingGANsandscore-baseddiffusionasgenerative
particlemodels. arXivpreprintarXiv:2305.16150,2023.
AlexandreGalashov,ValentindeBortoli,andArthurGretton. DeepMMDgradientflowwithout
adversarialtraining. arXivpreprintarXiv:2405.06780,2024.
YuanGao,YulingJiao,YangWang,YaoWang,CanYang,andShunkangZhang. Deepgenerative
learningviavariationalgradientflow. InInternationalConferenceonMachineLearning,pages
2093–2101.PMLR,2019.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. Bayesian data analysis.
ChapmanandHall/CRC,1995.
Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with Sinkhorn
divergences.InInternationalConferenceonArtificialIntelligenceandStatistics,pages1608–1617,
2018.
10Pierre Glaser, Michael Arbel, and Arthur Gretton. KALE flow: A relaxed KL gradient flow for
probabilities with disjoint support. Advances in Neural Information Processing Systems, 34:
8018–8031,2021.
ArthurGretton,KarstenMBorgwardt,MalteJRasch,BernhardSchölkopf,andAlexanderSmola. A
kerneltwo-sampletest. TheJournalofMachineLearningResearch,13(1):723–773,2012.
OmarHagrass,BharathK.Sriperumbudur,andBingLi. Spectralregularizedkerneltwo-sampletests.
arXivpreprintarXiv:2212.09201,2022.
OmarHagrass,BharathK.Sriperumbudur,andBingLi. Spectralregularizedkernelgoodness-of-fit
tests,2023.
Zaïd Harchaoui, Francis Bach, and Eric Moulines. Testing for homogeneity with kernel Fisher
discriminantanalysis. AdvancesinNeuralInformationProcessingSystems,20,2007.
JohannesHertrich,ChristianWald,FabianAltekrüger,andPaulHagemann. Generativeslicedmmd
flowswithrieszkernels. arXivpreprintarXiv:2305.11463,2023.
JohannesHertrich,ManuelGräf,RobertBeinert,andGabrieleSteidl. Wassersteinsteepestdescent
flowsofdiscrepancieswithRieszkernels. JournalofMathematicalAnalysisandApplications,
531(1):127829,2024.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InAdvancesin
NeuralInformationProcessingSystems,volume33,pages6840–6851,2020.
MasanariKimuraandHideitsuHino. α-geodesicalskewdivergence. Entropy,23(5):528,2021.
AnnaKorba,Pierre-CyrilAubin-Frankowski,SzymonMajewski,andPierreAblin. KernelStein
discrepancydescent. InInternationalConferenceonMachineLearning,pages5719–5730,2021.
LillianLee. Measuresofdistributionalsimilarity. arXivpreprintcs/0001012,2000.
DongCLiuandJorgeNocedal. OnthelimitedmemoryBFGSmethodforlargescaleoptimization.
Mathematicalprogramming,45(1-3):503–528,1989.
QiangLiuandDilinWang. Steinvariationalgradientdescent: AgeneralpurposeBayesianinference
algorithm. AdvancesinNeuralInformationProcessingSystems,29,2016.
SongLiu,JiahaoYu,JackSimons,MingxuanYi,andMarkBeaumont. Minimizingf-divergencesby
interpolatingvelocityfields. arXivpreprintarXiv:2305.15577,2022.
Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert Stöter.
Sliced-Wassersteinflows: Nonparametricgenerativemodelingviaoptimaltransportanddiffusions.
InInternationalConferenceonMachineLearning,pages4104–4113,2019.
AlfredMüller. Integralprobabilitymetricsandtheirgeneratingclassesoffunctions. Advancesin
AppliedProbability,29(2):429–443,1997.
Sebastian Neumayer, Viktor Stein, and Gabriele Steidl. Wasserstein gradient flows for Moreau
envelopesoff-divergencesinreproducingkernelhilbertspaces. arXivpreprintarXiv:2402.04613,
2024.
XuanLongNguyen,MartinJWainwright,andMichaelIJordan. Estimatingdivergencefunctionals
andthelikelihoodratiobyconvexriskminimization. IEEETransactionsonInformationTheory,
56(11):5847–5861,2010.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial
IntelligenceandStatistics,pages814–822.PMLR,2014.
GarethO.RobertsandJeffreySRosenthal.GeneralstatespaceMarkovchainsandMCMCalgorithms.
ProbabilitySurveys,1:20–71,2004.
AlessandroRudiandLorenzoRosasco. Generalizationpropertiesoflearningwithrandomfeatures.
AdvancesinNeuralInformationProcessingSystems,30,2017.
11FilippoSantambrogio. {Euclidean,metric,andWasserstein}gradientflows: Anoverview. Bulletin
ofMathematicalSciences,7:87–154,2017.
Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of
distance-basedandRKHS-basedstatisticsinhypothesistesting. TheAnnalsofStatistics,41(5):
2263–2291,2013.
JackSimons,SongLiu,andMarkBeaumont. Variationallikelihood-freegradientdescent. InFourth
SymposiumonAdvancesinApproximateBayesianInference,2022.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020.
Bharath K Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Schölkopf, and Gert RG
Lanckriet. Hilbertspaceembeddingsandmetricsonprobabilitymeasures. TheJournalofMachine
LearningResearch,11:1517–1561,2010.
IngoSteinwartandAndreasChristmann. SupportVectorMachines. SpringerScience&Business
Media,2008.
EloiTanguy,RémiFlamary,andJulieDelon. Propertiesofdiscreteslicedwassersteinlosses. arXiv
preprintarXiv:2307.10352,2023.
CédricVillani. OptimalTransport: OldandNew,volume338. Springer,2009.
AndreWibisono. Samplingasoptimizationinthespaceofmeasures: TheLangevindynamicsasa
compositeoptimizationproblem. InConferenceonLearningTheory,pages2093–3027,2018.
Lantian Xu, AnnaKorba, and Dejan Slepcev. Accurate quantization ofmeasures via interacting
particle-basedoptimization. InInternationalConferenceonMachineLearning, pages24576–
24595,2022.
12A AdditionalBackground
A.1 MonotonicityoftheKL divergence
α
Proposition8. Thefunctionα→KL(p||αp+(1−α)q)isdecreasing.
Proof. Let0<α′ <α,
KL(p||αp+(1−α)q)−KL(p||α′p+(1−α′)q)
(cid:90)
= (log(q−α′(p−q))−log(q−α(p−q)))dp
(cid:90)
= (log(q−α′(p−q))−log(q−α(p−q)))(αdp+(1−α)dq)
(cid:90)
+(1−α) (log(q−α′(p−q))−log(q−α(p−q)))(dp−dq)
We have (cid:82) (log(q−α′(p−q)) − log(q−α(p−q)))(αdp + (1 − α)dq) = −KL(αp + (1 −
α)q)||α′p+(1−α′)q) ⩽ 0. For the second term, let’s note that because of the increasing na-
tureofthelog,forthepointsforwhichp−q >0,log(q−α′(p−q))⩽log(q−α(p−q))andvice
versa. So
(cid:90)
(1−α) (log(q−α′(p−q))−log(q−α(p−q)))(dp−dq)⩽0.
Thisconcludestheproof.
A.2 SkewnessoftheKL divergence
α
WeremindtheidentitywhichistheequivalentforrealvalueofEquation(3). Letx,y >0,
(cid:90) ∞(cid:18) 1 1 (cid:19)
logx−logy = − dβ. (13)
y+β x+β
0
Proposition9. Supposethatdp/dq ⩽ 1,
µ
(cid:18) (cid:18) 1(cid:19) α2 (cid:18) 1 (cid:19)(cid:19)(cid:90)
|KL(p||αp+(1−α)q)−KL(p||q)|⩽ α 1+ + 1+ logqdp
µ 1−α µ2
Proof.
(cid:90) p (cid:90) p
|KL(p||αp+(1−α)q)−KL(p||q)|= log dp− log dp
q+α(p−q) q
(cid:90)
= (logq−log(q+α(p−q)))dp
UsingidentityEquation(13),
(cid:90) +∞(cid:18) 1 1 (cid:19)
logq−log(q+α(p−q))= − dβ
q+β+α(p−q) q+β
0
(cid:90) +∞ p−q (cid:90) +∞(cid:18) p−q(cid:19)2 1
=α dβ−α2 dβ.
(q+β)2 q+β q+β+α(p−q)
0 0
Theterm|α(cid:82)+∞ p−q dβ|=α(cid:82)+∞ p dβ+α(cid:82)+∞ q dβ. Usingtheinequalityq ⩾µp,
0 (q+β)2 0 (q+β)2 0 (q+β)2
we have (cid:82)+∞ p dβ ⩽ 1 (cid:82)+∞ 1 dβ = 1 logq. We also have directly (cid:82)+∞ q dβ ⩽
0 (q+β)2 µ 0 (q+β) µ 0 (q+β)2
logq. The second term is positive so we only have to bound
(cid:82)+∞(cid:16) p−q(cid:17)2
1 dβ ⩽
0 q+β q+β+α(p−q)
(cid:16) (cid:17)
(cid:82)+∞ p2+q2 1 dβ ⩽ 1 1+ 1 logq. Finally,
0 (q+β)2q+β+α(p−q) 1−α µ2
(cid:18) (cid:18) 1(cid:19) α2 (cid:18) 1 (cid:19)(cid:19)(cid:90)
|KL(p||αp+(1−α)q)−KL(p||q)|⩽ α 1+ + 1+ logqdp.
µ 1−α µ2
13A.3 Backgroundoperatormonotony
Werecallhereresultsaboutmatrix/operatormonotony,thatweextensivelyuseinallourproofs.These
aresetoutintheblogposthttps://francisbach.com/matrix-monotony-and-convexity/,
see[Bhatia,2009]formoredetails. For2operatorsAandBinH,wedenoteA≼Btheoperators
inequalityinthesense: ∀x∈H,x∗Ax⩽x∗Bx. LetS beingthesetofsymmetricoperatorsandS+
thesetofsymmetricpositiveoperators. Wehave
i) IfA,B ∈S,X anotheroperatorinH,A≼B ⇒X∗AX ≼X∗BX.
ii) IfA,B ∈S,M ≽0,A≼B ⇒Tr(AM)⩽Tr(BM).
iii) IfBisinvertible,A≼B ⇒B−1/2AB−1/2 ≼I.
iv) IfB ∈S,B∗B ≼I ⇒BB∗ ≼I.
v) IfA,B ∈S+,A≼B ⇒A1/2 ≼B1/2.
vi) IfA,B ∈S+andareinvertible,A≼B ⇒B−1 ≼A−1.
B Proofs
B.1 ProofofProposition2
Let0<α′ <α,
KKL (p||q)−KKL (p||q)
α α′
=TrΣ log(Σ +α′(Σ −Σ ))−TrΣ log(Σ +α(Σ −Σ ))
p q p q p q p q
=Tr(αΣ +(1−α)Σ )log(Σ +α′(Σ −Σ ))−Tr(αΣ +(1−α)Σ )log(Σ +α(Σ −Σ ))
p q q p q p q q p q
+(1−α)Tr(Σ −Σ )[log(Σ +α′(Σ −Σ ))−log(Σ +α(Σ −Σ ))].
p q q p q q p q
Werecognize
Tr(αΣ +(1−α)Σ )log(Σ +α′(Σ −Σ ))−Tr(αΣ +(1−α)Σ )log(Σ +α(Σ −Σ ))
p q q p q p q q p q
=−KKL(αp+(1−α)q||α′p+(1−α′)q)⩽0. (14)
Wethen
have
(1−α)Tr(Σ −Σ )[log(Σ +α′(Σ −Σ ))(cid:57)log(Σ +α(Σ −Σ ))]
p q q p q q p q
(cid:90) +∞
=(1−α) Tr(Σ −Σ )(cid:0) (Σ +βI+α(Σ −Σ ))−1−(Σ +βI+α′(Σ −Σ ))−1(cid:1) dβ
p q q p q q p q
0
=(1−α)(α′−α)×
(cid:90) +∞
Tr(Σ −Σ )(Σ +βI+α(Σ −Σ ))−1(Σ −Σ )(Σ +βI+α′(Σ Σ ))−1dβ.
p q q p q p q q p q
0
ThetermundertheintegralwritesasXTAX,
whereA=(Σ +βI+α(Σ −Σ ))−1 ≽0,and(1−α)(α′−α)⩽0,so
q p q
(1−α)Tr(Σ −Σ )(log(Σ +α′(Σ −Σ ))−log(Σ +α(Σ −Σ )))⩽0.
p q q p q q p q
Finally,
KKL (p||q)−KKL (p||q)⩽0.
α α′
B.2 ProofofProposition3
Thisproofmakesrepeateduseoftheresultsaboutmatrixmonotony,someofwhichwerecallin
Appendix A.3. We denote Γ = αΣ +(1−α)Σ = Σ +α(Σ −Σ ). We write using direct
p q q p q
integration[Ando,1979]
14KKL(p||q)−KKL (p||q)=TrΣ logΣ −TrΣ logΓ
α p q p
(cid:90) +∞
= (cid:0) TrΣ (Γ+βI)−1−TrΣ (Σ +βI)−1(cid:1) dβ
p p q
0
(cid:90) +∞
=α TrΣ (Σ +βI)−1(Σ −Σ )(Σ +βI)−1dβ
p q q p q
0
(cid:90) +∞
−α2 TrΣ (Σ +βI)−1(Σ −Σ )(Γ+βI)−1(Σ −Σ )(Σ +βI)−1dβ
p q q p q p q
0
:=(a)−(b).
Wewillfirstupperbound(a)inabsolutevalue,sinceitisnotnecessarilypositive. Usingthematrix
inequalitiesΣ ≼ 1Σ weget
p µ q
1
(Σ +βI)−1Σ (Σ +βI)−1 ≼(Σ +βI)−1and(Σ +βI)−1Σ (Σ +βI)−1 ≼ (Σ +βI)−1,
q q q q q p q µ q
(15)
andsowehave
|TrΣ p(Σ q+βI)−1(Σ q−Σ p)(Σ q+βI)−1|=|TrΣ p1 2(Σ q+βI)−1(Σ q−Σ p)(Σ q+βI)−1Σ p21 |
⩽TrΣ p1 2(Σ q+βI)−1Σ q(Σ q+βI)−1Σ p1 2 +TrΣ p1 2(Σ q+βI)−1Σ p(Σ q+βI)−1Σ p1 2
⩽TrΣ p1 2(Σ q+βI)−1Σ p1 2 + µ1 TrΣ p1 2(Σ q+βI)−1Σ p1 2
(cid:18) (cid:19)
1
= 1+ TrΣ (Σ +βI)−1.
µ p q
Hencewecanupperbound|(a)|as:
(cid:12)
(cid:12) (cid:12)
(cid:12)α(cid:90) +∞
TrΣ p(Σ q+βI)−1(Σ q−Σ p)(Σ
q+βI)−1dβ(cid:12)
(cid:12) (cid:12)
(cid:12)⩽α(cid:18)
1+
µ1(cid:19)
|TrΣ plogΣ q|. (16)
0
We now turn to (b) which we can upper bound without absolute value since it is a positive term.
SinceΓ≽(1−α)Σ ,α∈[0,1]andwearedealingwithp.s.d. operators,wecanboundtheinverse
q
as(Γ+βI)−1 ≼ 1 (Σ + β I)−1 ≼ 1 (Σ +βI)−1andso,usingTr(AM)≤Tr(BM)for
1−α q 1−α 1−α q
A≼BandM ≽0,wehave:
TrΣ (Σ +βI)−1(Σ −Σ )(Γ+βI)−1(Σ −Σ )(Σ +βI)−1
p q q p q p q
1
⩽ TrΣ (Σ +βI)−1(Σ −Σ )(Σ +βI)−1(Σ −Σ )(Σ +βI)−1. (17)
1−α p q q p q q p q
Wewillsplitther.h.s. ofthepreviousinequalityinfourterms,involvingtwiceΣ ,twocrossterms
q
(boundedsimilarly)withΣ ,Σ andtwiceΣ . Wehaveforthefirstone:
p q p
TrΣ p1 2(Σ q+βI)−1Σ q(Σ q+βI)−1Σ q(Σ q+βI)−1Σ p1 2 ⩽TrΣ p21 (Σ q+βI)−1Σ q(Σ q+βI)−1Σ p1 2
⩽TrΣ (Σ +βI)−1.
p q
Forthecross-termwehave:
−TrΣ (Σ +βI)−1Σ (Σ +βI)−1Σ (Σ +βI)−1 ⩽0.
p q p q q q
andforthelasttermwehave:
1
TrΣ (Σ +βI)−1Σ (Σ +βI)−1Σ (Σ +βI)−1 ⩽ TrΣ (Σ +βI)−1.
p q p q p q µ2 p q
Finally,combiningourboundsforthetermsin(b)andintegratingwithrespecttoβ,weget
(cid:90) +∞
α2 TrΣ (Σ +βI)−1(Σ −Σ )(Γ+βI)−1(Σ −Σ )(Σ +βI)−1dβ
p q q p q p q
0
(cid:18) (cid:19)
1 1
⩽ 1+ |TrΣ logΣ |.
1−α µ2 p q
15B.3 ProofofProposition4
B.3.1 Intermediateresult1: Concentrationofsumsofrandomself-adjointoperators
Lemma10. LetD =(Γ+βI)− 21(Γ−Γˆ)(Γ+βI)− 21 and0<u<1. Then,
2 (cid:32) 48 (cid:18) C(β) u(cid:19)2(cid:33) (cid:32) (m∧n)u2 (cid:33)
P(λ (D)>u)⩽ C(β) 1+ + exp −
max µ u4(m∧n) µ 6 8(C(β) + u)
µ 6
whereλ (D)isthemaximaleigenvalueof(D2)1/2.
max
Proof. Let’s note X = (Γ + βI)−1/2φ(x )φ(x )∗(Γ + βI)−1/2 and Y = (Γ +
i i i j
βI)−1/2φ(y )φ(y )∗(Γ + βI)−1/2. We have Σ = E[X], Σ = E[Y] and so Γ = αE[X] +
j j p q
(1−α)E[Y]. Wecanwrite
n m
α(cid:88) 1−α(cid:88)
D = (X −E[X])+ (Y −E[Y]).
n i m j
i=1 j=1
However, for two operators A and B we have ∥A+B∥ ⩽ ∥A∥ +∥B∥ which means that
op op op
λ (A+B)⩽λ (A)+λ (B). Then,
max max max
(cid:32) n (cid:33)  m 
1 (cid:88) 1 (cid:88)
λ max(D)⩽λ max n αX i−E[αX] +λ max m (1−α)Y j −E[(1−α)Y]
i=1 j=1
whichisequivalentto
(cid:16) (cid:17)
λ max(D)⩽λ
max
α(Γ+βI)− 21(Σ pˆ−Σ p)(Γ+βI)− 21
(cid:16) (cid:17)
+λ max (1−α)(Γ+βI)− 21(Σ qˆ−Σ q)(Γ+βI)−1 2 .
(cid:16) (cid:17)
The quantities λ max(D p) := λ
max
α(Γ+βI)−1 2(Σ pˆ−Σ p)(Γ+βI)− 21 and λ max(D q) :=
(cid:16) (cid:17)
λ max (1−α)(Γ+βI)−1 2(Σ qˆ−Σ q)(Γ+βI)−1 2 arepositiveso
(cid:16) u(cid:17) (cid:16) u(cid:17)
P(λ (D)>u)⩽P λ (D )> +P λ (D )> .
max max p 2 max q 2
√
Then Bach [2022, Lemma 2] can be applied twice to φ˜(x) = α(Γ + βI)−1/2φ(x) and to
φ˜˜(y)=(cid:112)
(1−α)(Γ+βI)−1/2φ(y).
√
For the term with p, ∥φ(˜ x)∥2 = ∥ α(Γ + βI)−1/2φ(x)∥2 = α⟨φ(x),(Γ + βI)−1φ(x)⟩ ⩽
H
⟨φ(x),(Σ + βI)−1φ(x)⟩ ⩽ C(β) where we use Γ ≽ αΣ . Using the same inequality, we
p p
alsoobtainTr(Γ+βI)−1/2αΣ (Γ+βI)−1/2 ⩽Tr(Σ +βI)−1/2Σ (Σ +βI)−1/2 ⩽C(β)and
p p p p
λ ((Γ+βI)−1/2αΣ (Γ+βI)−1/2)=∥(Γ+βI)−1/2αΣ (Γ+βI)−1/2∥ ⩽1. Then,
max p p op
(cid:16) u(cid:17) (cid:18) 48 u (cid:19) (cid:18) nu2 (cid:19)
P λ (D )> ⩽C(β) 1+ (C(β)+ )2 exp − .
max p 2 u4n2 6 8(C(β)+ u)
6
For the term with q, using the matrix inequalities Γ ≽ (1−α)Σ and Σ ≽ µΣ , with similar
q q p
computationsweget∥φ˜˜(y)∥2 ⩽ 1C(β),Tr(Γ+βI)−1/2(1−α)Σ (Γ+βI)−1/2 ⩽ 1C(β)and
H µ q µ
λ (cid:0) (Γ+βI)−1/2(1−α)Σ (Γ+βI)−1/2(cid:1)⩽1. Then,
max q
(cid:16) u(cid:17) C(β)(cid:18) 48 C(β) u (cid:19) (cid:32) mu2 (cid:33)
P λ (D )> ⩽ 1+ ( + )2 exp − .
max q 2 µ u4m µ 6 8(C(β) + u)
µ 6
Wecancombinebothresultstoget
2 (cid:32) 48 (cid:18) C(β) u(cid:19)2(cid:33) (cid:32) (m∧n)u2 (cid:33)
P(λ (D)>u)⩽ C(β) 1+ + exp − . (18)
max µ u4(m∧n)2 µ 6 8(C(β) + u)
µ 6
16B.3.2 Intermediateresult2: Degreesoffreedomestimation
ThePropositionbelowadaptstheproofofBach[2022,Proposition15]toboundthecrossterms
betweenempiricalcovarianceoperatorsofpandαp+(1−α)q.
Proposition11(Estimationofskeweddegreesoffreedom). Assumethatp≪qandthat dp ⩽ 1 for
dq µ
some0<µ<1. Assumealsothatc=(cid:82)+∞ sup ⟨φ(x),(Σ +βI)−1φ(x)⟩2dβ isfinite. Given
0 x∈Rd p
x ,...,x i.i.d. samplesfrompandy ,...,y i.i.d. samplesfromqwehave
1 n 1 m
(cid:12) (cid:104) (cid:105)(cid:12)
(cid:12)E TrΣ (Γ+βI)−1−TrΣ (Γˆ+βI)−1 (cid:12)
(cid:12) p pˆ (cid:12)
(cid:32) (cid:18) (cid:19) (cid:115) (cid:18) (cid:19)(cid:33) (cid:18) (cid:19)
12 m∧n 6 1 1 28 1 1
⩽ nexp − + + C(β)+ + C(β)2. (19)
αµ 16C(β) µ n m αµ2 n m
whereC(β)=sup ⟨φ(x),(Σ +βI)−1φ(x)⟩issupposedtobeinferiorto µ(m∧n).
x∈Rd p 24
Proof. Wewilldenote
A:=TrΣ (Γ+βI)−1−TrΣ (Γˆ+βI)−1
p pˆ
=Tr(Σ −Σ )(Γ+βI)−1+TrΣ (Γ+βI)−1(N −Γˆ)(Γˆ+βI)−1.
p pˆ pˆ
Inexpectation,thefirsttermcanbeupper-boundedasfollows,usingthatTr(cid:0)
(Σ −Σ
)(Γ+βI)−1(cid:1)
p pˆ
isthesumofzero-meanrandomvariables:
(cid:113)
|ETr(Σ −Σ )(Γ+βI)−1|⩽ E[(Tr(Σ −Σ )(Γ+βI)−1)2]
p pˆ p pˆ
(cid:114)
1
= Var[⟨φ(x),(Γ+βI)−1φ(x)⟩]
n
(cid:114)
1
⩽ E[⟨φ(x),(Γ+βI)−1φ(x)⟩2]
n
(cid:115)
1 1 β
⩽ E[⟨φ(x),(Σ + I)−1φ(x)⟩2]
µ(1−α) n p µ(1−α)
(cid:114) (cid:18) (cid:19)
1 1 β
⩽ C (20)
nµ(1−α) µ(1−α)
(cid:114)
1 2
⩽ C(β), (21)
nµ
wherethepenultimateinequalityusesΓ≽(1−α)Σ ≽µ(1−α)Σ .Thelastinequalityisduetothe
q p
factsthatα⩾ 1 andso 1 ⩽2andalsothatµ⩽1so β ⩾βandbecauseC isnonincreasing,
2 1−α µ(1−α)
(cid:16) (cid:17)
C β ⩽C(β). Thesesimplificationsareusemanytimesinthisproof.
µ(1−α)
ThesecondtermcanwriteusingtheidentityD(I−D)−1 =D(I−D)−1(I−D+D),following
theproofof[RudiandRosasco,2017];
B :=TrΣ (Γ+βI)−1(Γ−Γˆ)(Γˆ+βI)−1
pˆ
=TrΣ pˆ(Γ+βI)−1 2D(I−D)−1(Γ+βI)− 21
=TrΣ pˆ(Γ+βI)−1 2D(I−D)−1D(Γ+βI)− 21 +TrΣ pˆ(Γ+βI)− 21D(Γ+βI)− 21
whereD =(Γ+βI)−1 2(Γ−Γˆ)(Γ+βI)− 21 issupposedtobelessthenI.
Wehave
TrΣ pˆ(Γ+βI)−1 2D(Γ+βI)− 21 =TrD1 2(Γ+βI)− 21Σ pˆ(Γ+βI)− 21D1 2
1
⩽ TrD21(Γ+βI)− 21Γˆ(Γ+βI)− 21D1 2,
α
17and,bydefinitionofDabove,
−λ max(D)I ≼(Γ+βI)−1 2(Γ−Γˆ)(Γ+βI)−1 2 ≼λ max(D)I
whereλ max(D)istheabsolutevalueofthemaximaleigenvalueof(D2)1 2. So
(Γ+βI)−1 2Γˆ(Γ+βI)− 21 ≼(λ max(D)+1)I (22)
assoonasλ (D)islessthan1. Inthiscase,
max
1 1+λ (D)
TrD1 2(Γ+βI)−1 2Γˆ(Γ+βI)− 21D1 2 ⩽ max TrD. (23)
α α
Stillconsideringthatλ (D)<1wealsohave
max
TrΣ pˆ(Γ+βI)−1 2D(I−D)−1D(Γ+βI)− 21
⩽∥(Γ+βI)− 21Σ pˆ(Γ+βI)− 21∥ opTr(cid:0) D2(I−D)−1(cid:1)
1+λ (D)
⩽ max ∥(I−D)−1∥ TrD2
α op
1+λ (D)
= max TrD2, (24)
α(1−λ (D))
max
whereinthesecondinequalityweusedΣ ≼ 1Γˆ and(22). Let0 < u < 1. Combining(21),(23)
pˆ α
and(24),wehave:
|A|=1 λmax(D)>u|A|+1 λmax(D)⩽u|A| (25)
(cid:32) (cid:114) (cid:33)
1+u 1+u 1 2
⩽1 λmax(D)>u|A|+1 λmax(D)⩽u α(1−u)TrD2+
α
TrD+ nµC(β) (26)
(cid:114)
1+u 1+u 1 2
⩽1 |A|+ TrD2+ TrD+ C(β). (27)
λmax(D)>u α(1−u) α nµ
(cid:16) (cid:17)
UsingΓ≽µ(1−α)Σ ,thetermTrΣ (Γ+βI)−1isboundedby 1 C β ⩽ 2C(β)⩽
p p µ(1−α) µ(1−α) µ
n∧m by hypothesis. And with Γˆ ≽ 1Σ , we have both TrΣ (Γˆ + βI)−1 ⩽ 1 TrΣ (Σ +
12 α pˆ pˆ α pˆ pˆ
βI)−1 ⩽ n and TrΣ (Γ + βI)−1 ⩽ 1 C(β) ⩽ 2C(β) ⩽ n∧m. Then, almost surely,
α α p (1−α)µ µ 12
|A|⩽max(cid:8)n,n∧m(cid:9)⩽ 2n. Then,
α 12 α
(cid:20) (cid:21) (cid:114)
2n 1+u 1+u 1 2
E|A| ⩽ P(λ (D) > u) + E TrD2+ TrD + C(β). (28)
max α α(1−u) α nµ
WithLemma10wehave
2 (cid:32) 48 (cid:18) C(β) u(cid:19)2(cid:33) (cid:32) (m∧n)u2 (cid:33)
P(λ (D)>u)⩽ C(β) 1+ + exp − . (29)
max µ u4(m∧n)2 µ 6 8(C(β) + u)
µ 6
UsingthehypothesisthatC(β)⩽ µ(n∧m),
24
4n (cid:18) 48 (cid:16)m∧n u(cid:17)2(cid:19) (cid:32) (m∧n)u2 (cid:33)
E|A|⩽ C(β) 1+ + exp −
µα u4(m∧n)2 24 6 8(C(β) + u)
µ 6
(cid:20) (cid:21) (cid:114)
1+u 1+u 1 2
+E TrD2+ TrD + C(β).
α(1−u) α nµ
Wehave
(cid:113)
E[TrD]⩽ E[(Tr(Γ+βI)− 21 (Γ−Γˆ)(Γ+βI)−1 2)2]
18anddenotingX
i
= Tr(Γ+βI)−1 2(Γ−φ(x i)φ(x i)∗)(Γ+βI)− 21 andY
j
= Tr(Γ+βI)− 21 (Γ−
φ(y j)φ(y j)∗)(Γ+βI)− 21,
E[(Tr(Γ+βI)−1 2(Γ−Γˆ)(Γ+βI)−1 2)2]=
α n22 (cid:88)n
E[(E[X]−X i)×(E[X]−X k)]
i,k=1
(1−α)2 (cid:88)m α(1−α)(cid:88)
+ E[E[Y]−Y )(E[Y]−Y )]+ E[(E[X]−X )(E[Y]−Y ).
m2 j l nm i j
j,l=1 i,j
ThevariablesX ,...,X ,Y ,...,Y areindependantsoweget
1 n 1 m
E[(Tr(Γ+βI)−1 2(Γ−Γˆ)(Γ+βI)− 21)2]
α2 (cid:88)n (1−α)2 (cid:88)n α(1−α) (cid:88)n
= E[X X ]+ E[Y Y ]+ E[X Y ]
n2 i k m2 j k nm i j
i,k=1 j,l=1 i,j=1
α2 α2(n−1) (1−α)2 (1−α)2(m−1)
= E[X2]+ E[X]2+ E[Y2]+ E[Y]2
n n m m
+2α(1−α)E[X]E[Y]
α2 (1−α)2
= (E[X2]−E[X]2)+ (E[Y2]−E[Y]2)+(αE[X]+(1−α)E[Y])2
n m
α2 (1−α)2
= Var[X]+ Var[Y].
n m
ThelastequalityisduetoE[αφ(x)φ(x)∗+(1−α)φ(y)φ(y)∗]=0. Wehave
Var[X]=Var[Tr(Γ+βI)− 21 (Γ−φ(x)φ(x)∗)(Γ+βI)− 21]
=Var[Tr(Γ+βI)− 21 φ(x)φ(x)∗(Γ+βI)− 21]
⩽E[(Tr(Γ+βI)− 21 φ(x)φ(x)∗(Γ+βI)− 21)2]
=E[(⟨φ(x),(Γ+βI)−1φ(x)⟩2]
andtheequivalentinequalityisalsoverifiedforY. Then,
E[(Tr(Γ+βI)− 21 (Γ−Γˆ)(Γ+βI)−1 2)2]
α2 (1−α)2
⩽ E[⟨φ(x),(Γ+βI)−1φ(x)⟩2]+ E[⟨φ(y),(Γ+βI)−1φ(y)⟩2]
n m
1 α2 (1−α)2 (cid:18) β (cid:19)2 4 (cid:18) 1 1 (cid:19)
⩽ ( + )C ⩽ + C(β)2.
µ2(1−α)2 n m µ(1−α) µ2 n m
Then,
(cid:115)
(cid:18) (cid:19)
2 1 1
E[TrD]⩽ + C(β). (30)
µ n m
Usingsimilarcalculations,weobtain
(cid:18) (cid:19)
4 1 1
E[TrD2]⩽ + C(β)2. (31)
µ2 n m
Finally,
4 (cid:18) 48 (cid:16)m∧n u(cid:17)2(cid:19) (cid:32) (m∧n)u2 (cid:33)
E|A|⩽ n 1+ + exp − C(β)
αµ u4(m∧n)2 24 6 8(C(β) + u)
µ 6
(cid:18) (cid:19) (cid:32) (cid:115) (cid:18) (cid:19) (cid:114) (cid:33)
1+u 4 1 1 1+u 1 1 1 2
+ + C(β)2+ + + C(β).
α(1−u)µ2 n m α n m n µ
19Takingu= 3 wegetthefinalbound
4
4
(cid:32)
160
(cid:18)
m∧n
1(cid:19)2(cid:33) (cid:32)
9(m∧n)
(cid:33)
E|A|⩽ n 1+ + exp − C(β)
αµ (m∧n)2 24 8 16(8C(β) +1)
µ
(cid:115)
(cid:18) (cid:19) (cid:18) (cid:19)
28 1 1 11 1 1
+ + C(β)2+ + C(β)
αµ2 n m 2µ n m
(cid:32) (cid:18) (cid:19) (cid:115) (cid:18) (cid:19)(cid:33) (cid:18) (cid:19)
12 m∧n 6 1 1 28 1 1
⩽ nexp −µ + + C(β)+ + C(β)2.
αµ 16C(β) µ n m αµ2 n m
B.3.3 FinalproofofProposition4
From[Bach,2022,Proposition7]wealreadyhaveaboundontheentropyterm:
1+c(8logn)2 17 √
E[|Tr(Σ logΣ )−Tr(Σ logΣ )|]⩽ + √ (2 c+logn). (32)
pˆ pˆ p p n n
Inthefollowingwewillcloselyfollowtheproofof[Bach,2022,Proposition7]inordertoboundthe
crosstermsdifference. WecanwritewiththeintegralrepresentationinBach[2022](Eq5),
(cid:90) +∞
TrΣ logΓˆ−TrΣ logΓ= TrΣ (Γ+βI)−1−TrΣ (Γˆ+βI)−1dβ.
pˆ p p pˆ
0
We will treat separately the part close to infinity, the one close to zero and the central part. Let
β >β >0. Fromβ toinfinitywehave
1 0 1
(cid:90) +∞ (cid:16) (cid:17)
TrΣ (Γ+βI)−1−TrΣ (Γˆ+βI)−1dβ =TrΣ log Γˆ+β I −Σ log(Γ+β I)
p pˆ pˆ 1 p 1
β1
⩽log(1+β )−logβ
1 1
1
⩽ .
β
1
From0toβ wehave
0
(cid:90) β0 (cid:90) β0
TrΣ (Γ+βI)−1dβ ⩽ TrΣ ((1−α)µΣ +βI)−1dβ
p p p
0 0
1 (cid:90) β0 β
= TrΣ (Σ + I)−1dβ
µ(1−α) p p µ(1−α)
0
1 (cid:90) β0 β 2 (cid:90) β0
⩽ sup⟨φ(x),(Σ + I)−1φ(x)⟩dβ ⩽ C(β)dβ,
µ(1−α) p µ(1−α) µ
0 x∈Rd 0
whereC(β)=sup ⟨φ(x),(Σ +βI)−1φ(x)⟩. Wealsohave
x∈Rd p
(cid:90) β0 (cid:90) β0 1
TrΣ (Γˆ+βI)−1dβ ⩽ TrΣ (αΣ +βI)−1dβ ⩽ nβ .
pˆ pˆ pˆ α 0
0 0
WithProposition11wehave
E|TrΣ logΓˆ−TrΣ logΓ|
pˆ p
1 1 (cid:90) β0 (cid:90) β1
⩽ + nµβ + C(β)dβ+ E|TrΣ logΓˆ−TrΣ logΓ|dβ
µβ α 0 pˆ p
1 0 β0
1 1 (cid:90) β0 (cid:32) 12 (cid:18) m∧n (cid:19) 6(cid:115) (cid:18) 1 1 (cid:19)(cid:33) (cid:90) β1
⩽ + nµβ + C(β)dβ+ nexp −µ + + C(β)dβ
µβ α 0 αµ 16C(β ) µ n m
1 0 0 β0
28 (cid:18) 1 1 (cid:19)(cid:90) β1
+ + C(β)2dβ.
αµ2 n m
β0
20Let’stakeβ suchthatC(β )=µ n∧m . ThefunctionC beingnonincreasing,theconditionof
0 0 24log(n)
Proposition11,whichisC(β)⩽ µ(m∧n),iswellsatisfiedbetweenβ andβ forthischoiceofβ .
24 0 1 0
Wehavethen
(cid:18) (cid:19) (cid:18) (cid:19)
12 m∧n 12 24
nexp −µ ⩽ nexp − log(n)
αµ 16C(β ) αµ 16
0
(cid:114)
12 1 12 1 1
⩽ √ ⩽ + ,
αµ n αµ n m
andalso,(cid:82)β1C(β)2dβ ⩽c. Then,
β0
1 1 (cid:90) β0
E|TrΣ logΓˆ−TrΣ logΓ|⩽ + nµβ + C(β)dβ
pˆ p µβ α 0
1 0
(cid:115)
18 (cid:18) 1 1 (cid:19)(cid:90) β1 28c (cid:18) 1 1 (cid:19)
+ + C(β)dβ+ + .
αµ n m αµ2 n m
β0
ThefunctionC isdecreasingso,C(β)2β ⩽ (cid:82)β C(β′)2dβ′ ⩽ candso,C(β) ⩽ (cid:113) 2c. Wealso
2 β/2 β
deducefromthat
(cid:18) 24log(n)(cid:19)2
β ⩽2c (33)
0 µ(m∧n)
Hence
1 (cid:90) β0 2c (cid:18) 24log(n)(cid:19)2 96clog(n)
nµβ + C(β)dβ ⩽ n +
α 0 α µ(m∧n) µ(n∧m)
0
Let’stakeβ =β +n. Wehave
1 0
(cid:90) β1
C(β)dβ
⩽(cid:90) β0+1 C(β)dβ+logβ 0+n ⩽logn+2(cid:112)
c(1+β ).
β +1 0
β0 0 0
Then,pluggingtheboundonβ givenbyEquation(33),
0
1 2c×n(24logn)2 96clog(n)
E|TrΣ logΓˆ−TrΣ logΓ|⩽ + +
pˆ p µn α µ(m∧n)2 µ(n∧m)
(cid:115) (cid:115)
18 (cid:18) 1 1 (cid:19) 2c(24logn)2 28c (cid:18) 1 1 (cid:19)
+ + (logn+2 c(1+ ))+ + . (34)
αµ n m µ2 (m∧n)2 αµ2 n m
Concatenatingwith(32),weget
1+ 1 +c(8logn)2 2c n(24logn)2 17 √
E|KKL (pˆ||qˆ)−KKL (p||q)|⩽ µ + + √ (2 c+logn)
α α n µα (m∧n)2 n
(cid:115) (cid:115)
28c (cid:18) 1 1 (cid:19) 96clog(n) 18 (cid:18) 1 1 (cid:19) 2c(24logn)2
+ + + + + (logn+2 c(1+ )). (35)
αµ2 n m µ(n∧m) αµ n m µ2 (m∧n)2
Usingincrementssuchthat1 ⩽ logn ⩽ (logn)2, 1 ⩽ 1 ⩽ √ 1 ⩽ 1andα,µ ⩽ 1,wecan
n n∧m m∧n
upperbound(35)byasimplerbound,whilestillretainingthemainconvergenceratesin √logn and
m∧n
in
(logn)2
. Thisboundis
m∧n
35 1 √
E|KKL (pˆ||qˆ)−KKL (p||q)|⩽ √ (2 c+logn)
α α m∧nαµ
(cid:18) (cid:19)
1 1 c n
+ 1+ +(24logn)2 (1+ ) . (36)
m∧n µ αµ2 m∧m
21AsmentionedinRemark5,itispossibletore-writethisproofwithoutconsideringtheassumption
thatp≪qand dp ≤ 1 andtoderiveaboundsimilartothisonebutwhichscalesinO( 1 )instead
dq µ α2
ofO(1). Thisboundis
α
32 √
E|KKL (pˆ||qˆ)−KKL (p||q)|⩽ √ (2 c+logn)
α α
α m∧n
2 (cid:18) 1 c(26logn)2 n (cid:19)
+ + (1+ ) . (37)
m∧n α α2 m∧m
Togetthisnewupperbound,theoperatorinequalityΓ≽(1−α)Σ ≽(1−α)µΣ mustbereplaced,
q p
eachtimeitisused,byΓ≽αΣ.Thisway,theoperatorinequality(Γ+βI)−1 ≼ 1 (Σ +βI)−1
µ(1−α) p
becomes(Γ+βI)−1 ≼ 1(Σ +βI)−1whichexplainstheadditionalfactor 1 inthefinalbound.
α p α
B.4 ProofofProposition6
According to [Bach, 2022, Proposition 6] we have that the eigenvalues of Σ (resp Σ ) are the
pˆ qˆ
samethantheonesof1/nK pˆ;andwealsohavethatforaneigenvalueλof1/nK pˆwithassociated
eigenvector α, the function f =
(cid:80)n
α φ(x ) is an eigenvector of Σ associated to the same
i=1 i i pˆ
eigenvalue. Hence, denoting(λ i)n
i=1
theeigenvaluesof1/nK pˆandas theassociatednormalized
eigenvectors,thefirsttermin(2)writes:
(cid:18) (cid:19) n
1 1 (cid:88)
Tr(Σ logΣ )=Tr K log K = λ log(λ ).
pˆ pˆ n pˆ n pˆ i i
i=1
Wenowturntothesecondtermin(2). Letϕ =(φ(x ),...,φ(x ))∗,ϕ =(φ(y ),...,φ(y ))∗
x 1 n y 1 m
andψ
=(cid:32) (cid:113)(cid:112)α nϕ x (cid:33)
. WehaveΣ =ψT
(cid:18) α1I 0(cid:19)
ψandαΣ +(1−α)Σ =ψTψ. Wealso
1−αϕ pˆ 0 0 pˆ qˆ
m y
remarkthatψψT =K. KnowingthattheoperatorψTψandthematrixψψT havethesamespectrum,
wetrytoreplaceαΣ +(1−α)Σ byK intheexpressionofKKL ,whichwecandowiththe
pˆ qˆ α
followinglemma.
Lemma12. Ifψ ∈Rd×r withd>r,g :R+ →RandψψT isinvertible,then
g(ψTψ)=ψT(ψψT)− 21g(ψψT)(ψψT)− 21ψ
Proof. Letψ =UDiag(S)VT theSVDofψwithU ∈Rr×r andV ∈Rd×dorthonormalmatrices.
Wehave: ψψT =UDiag(S2)UT andψTψ =VDiag(S2)VT,so,g(ψψT)=UDiag(g(S2))UT
and g(ψTψ) = VDiag(g(S2))VT. And so, g(ψTψ) = VUTg(ψψT)UVT. Remarking that
(ψψT)−1 2ψ =UVT weconcludetheproof.
Withthelemmaabovewecanwritethesecondtermin(2)as:
(cid:18) (cid:18) (cid:19) (cid:19)
Tr(Σ pˆlog(αΣ pˆ+(1−α)Σ qˆ))=Tr ψT α1 0I 00 ψψT(ψψT)− 21 log(cid:0) ψψT(cid:1) (ψψT)− 21ψ
(cid:18)(cid:18) (cid:19) (cid:19)
=Tr α1I 0 K1 2 log(K)K1 2
0 0
(cid:18)(cid:18) (cid:19) (cid:19)
1I 0
=Tr α Klog(K) ,
0 0
wherethelastequalityresultsfromthefactthatK1 2 andlogK commutebecausetheyhavethesame
eigenbasis.
B.5 ProofofProposition7
WewriteF =F +F andderivethefirstvariationofeachfunctionalinthenexttwolemmas. Then,
1 2
weconcludeonthefirstvariationofF.
22Lemma13. LetpˆasdefinedinProposition6. ThefirstvariationofF :pˆ→Tr(Σ logΣ )is,for
1 pˆ pˆ
x∈supp(pˆ):
F′(pˆ)(x)=Tr(φ(x)φ(x)∗(I+logΣ )),
1 pˆ
and+∞else.
Proof. Inthisproofweuseresidualformulawhichisusefultoderivespectralfunctions1. Indeed,
we can write Tr(Σ logΣ ) = (cid:80)n f(λ (Σ )) with f : C\R− → C,z → zlogz. Consider a
pˆ pˆ i=1 i pˆ
perturbationξ ∈P(Rd),ε>0andlet∆=εΣ . Letz ∈C. Usingthelinearityofp(cid:55)→Σ ,wehave
ξ p
Tr((Σ )log(Σ ))−Tr(Σ logΣ )=Tr((Σ +∆)log(Σ +∆))−Tr(Σ logΣ ) (38)
pˆ+ϵξ pˆ+ϵξ pˆ pˆ pˆ pˆ pˆ pˆ
n
(cid:88)
= f(λ (Σ +∆))−f(λ (Σ )). (39)
i pˆ i pˆ
i=1
Letγ beacloseddirectedcontourinC\R−whichsurroundsallthepositiveeigenvaluesofΣ and
pˆ
Σ +∆. Bysomemanipulationwithresidualformula,
pˆ
(cid:88)n
f(λ (Σ +∆))−f(λ (Σ ))=
1 (cid:73) f(z)Tr(cid:0)
(zI−Σ
−∆)−1(cid:1) −f(z)Tr(cid:0)
(zI−Σ
)−1(cid:1)
dz
i pˆ i pˆ 2iπ pˆ pˆ
i=1 γ
= 1 (cid:73) f(z)Tr(cid:0) (zI−Σ −∆)−1−(zI−Σ )−1(cid:1) dz.
2iπ pˆ pˆ
γ
However,
(zI−Σ −∆)−1−(zI−Σ )−1 =(zI−Σ )−1∆(zI−Σ )−1+o(∥∆∥ ).
pˆ pˆ pˆ pˆ op
So,notingΣ =(cid:80)n λ f f∗theSVDofΣ ,
pˆ i=1 i i i pˆ
Tr((Σ +∆)log(Σ +∆))−TrΣ logΣ (40)
pˆ pˆ pˆ pˆ
= 1 (cid:73) f(z)Tr(cid:0) (zI−Σ )−1∆(zI−Σ )−1(cid:1) dz+o(ε) (41)
2iπ pˆ pˆ
γ
=
1 (cid:73) f(z)Tr(cid:32) (cid:88)n (cid:88)n f if i∗∆f kf k∗ (cid:33)
dz+o(ε) (42)
2iπ (z−λ )(z−λ )
γ i=1k=1 i k
1 (cid:88)n (cid:73) f(z)
= dzTr(f∗f ∆)+o(ε). (43)
2iπ (z−λ )2 k k
k=1 γ k
Theresidueofh(z)= f(z) = zlogz atλ is2 Res(h,λ )=1+logλ . Applyingagainthe
(z−λk)2 (z−λk)2 k k k
residueformulawehave
n
(cid:88)
Tr((Σ +∆)log(Σ +∆))−Tr(Σ logΣ )= (1+logλ )Tr(f f∗∆)+o(ε)
pˆ pˆ pˆ pˆ k k k
k=1
=Tr((I+logΣ )∆)+o(ϵ)
pˆ
=1+Tr(log(Σ )∆)+o(ϵ)
pˆ
Thisconcludestheproofbydividingthelaterquantitybyϵandtakingthelimitasϵ→0.
Lemma 14. Let pˆ,qˆ as defined in Proposition 6. The first variation of F : pˆ →
2
Tr(Σ log(αΣ +(1−α)Σ ))is,foranyx∈supp(pˆ):
pˆ pˆ qˆ
F′(pˆ)(x)=Tr(log(αΣ +(1−α)Σ )φ(x)φ(x)∗)
2 pˆ qˆ
  
+αTrn (cid:88)+mh jh∗ jΣ ηpˆh jh∗
j
+(cid:88)log ηη
j
−− ηlogη
kh jh∗ jΣ pˆh kh∗ kφ(x)φ(x)∗ , (44)
j j k
j=1 j̸=k
where(η ,h )n+maretheeigenvaluesandeigenvectorsofαΣ +(1−α)Σ .
j j i=1 pˆ qˆ
1Seehttps://francisbach.com/cauchy-residue-formula/.
2Usingthatifh(z)= f(z) ,thenRes(h,λ)=f′′(z)wheref(z)=zlog(z).RecallthatRes(h,λ)=
(z−λ)2
1 (cid:72) h(z)dzwhereγisacontourcirclingstrictlyλ.
2iπ γ
23Proof. TosimplifynotationsletΓˆ =(1−α)Σ +αΣ . Asinthepreviouspart,let∆=εΣ . We
qˆ pˆ ξ
have:
(cid:16) (cid:17)
Tr(Σ log(αΣ +(1−α)Σ ))=Tr(Σ +∆)log Γˆ+α∆ −Tr(Σ )logΓˆ
pˆ+∆ pˆ+∆ qˆ pˆ pˆ
(cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
=Tr (Σ +∆)log Γˆ+α∆ −Tr (Σ +∆)logΓˆ +Tr (Σ +∆)logΓˆ −Tr Σ logΓˆ
pˆ pˆ pˆ pˆ
(cid:16) (cid:16) (cid:17) (cid:17) (cid:16) (cid:17)
=Tr Σ (log Γˆ+α∆ −logΓˆ) +Tr ∆logΓˆ .
pˆ
Thesecondtermonther.h.s. isalreadylinearin∆asdesired,wenowfocusonthefirstone. Using
anSVDdecompositionofΓˆ+α∆andΓˆ wewrite: :
(cid:16) (cid:16) (cid:17) (cid:17)
Tr Σ (log Γˆ+α∆ −logΓˆ) =
pˆ
     
n+m n+m
TrΣ pˆ(cid:88) logη j(Γˆ+α∆)h′ jh′ j∗ −TrΣ pˆ(cid:88) logη j(Γˆ)h jh∗ j,
j=1 j=1
where(h′) aretheeigenvectorsofpositiveeigenvaluesofΓˆ+α∆. Ifγisaloopsurroundingallthe
j j
eigenvaluesη (Γˆ+α∆)andη (Γˆ),then,
j j
n (cid:88)+m
logη (Γˆ+α∆)h′h′∗ =
1 (cid:73)
log(z)(zI−Γˆ−α∆)−1dz
j j j 2iπ
j=1 γ
and
n (cid:88)+m
logη (Γˆ)h h∗ =
1 (cid:73)
log(z)(zI−Γˆ)−1dz.
j j j 2iπ
j=1 γ
Moreover,wehave(zI−Γˆ−α∆)−1−(zI−Γˆ)−1 =(zI−Γˆ)−1α∆(zI−Γˆ)−1+o(ε). Hence,
(cid:16) (cid:16) (cid:17) (cid:17) 1 (cid:73) (cid:16) (cid:17)
Tr Σ (log Γˆ+α∆ −logΓˆ) = Tr Σ log(z)(zI−Γˆ)−1α∆(zI−Γˆ)−1 dz+o(ε)
pˆ 2iπ pˆ
γ
  
= 2α
iπ
(cid:73) log(z)TrΣ pˆn (cid:88)+m (zh −jh η∗ j∆ )(zh k −h∗ k
η
)dz
γ j,k=1 j k
 
= 2α
iπ
n (cid:88)+m(cid:73)
(z−ηlog )(( zz)
−η
)dzTr(cid:0) Σ pˆh jh∗ j∆h kh∗ k(cid:1) .
j,k=1 γ j k
(cid:16) (cid:17)
With the residue theorem, for j ̸= k, (cid:72) log(z) dz = 2iπ logηj + logηk =
γ (z−ηj)(z−ηk) (ηj−ηk) (ηk−ηj)
2iπlogηj−logηk,andfork =j,(cid:72) log(z) dz = 2iπ. Wethenhave,
ηj−ηk γ (z−ηj)2 ηj
n+m
Tr(cid:16) Σ (log(cid:16) Γˆ+α∆(cid:17) −logΓˆ)(cid:17) =α (cid:88) 1 Tr(cid:0) h h∗Σ h h∗∆(cid:1)
pˆ η j j pˆ j j
j
j=1
n+m
+α (cid:88) logη j −logη k Tr(cid:0) h h∗Σ h h∗∆(cid:1) +o(ε). (45)
η −η k k pˆ j j
j k
j̸=k
WecanremarkthatifΣ anΣ werediagonalizableinthesameeigenbasis,thenthepreviousquantity
pˆ qˆ
would be equal to α(cid:80)n+m λj Tr(cid:0) h h∗∆(cid:1) = TrΣ Γ†∆. We conclude again dividing the latter
j=1 ηj j j p
quantitybyϵandconsideringitslimitasϵ→0.
WecannowwritethematrixexpressionforthefirstvariationofF usinfLemma12. Weremind
thatϕ =
(cid:32)φ(x ..1)∗(cid:33)
(respϕ )andψ =
(cid:32) (cid:113)(cid:112)α
nϕ
x
(cid:33)
,andthatψTψ = αΣ +(1−α)Σ and
x φ(x n)∗ y 1− mαϕ y. pˆ qˆ
24ψψT =K. WeremarkthatΣ pˆ= √1 nϕT x√1 nϕ xandϕ xϕT
x
= n1K pˆ. ByLemma12,wehave
(cid:32)
1
(cid:18)
1
(cid:19)− 21 (cid:18)
1
(cid:19)(cid:18)
1
(cid:19)− 21 (cid:33)
Tr(log(Σ )φ(x)φ(x)∗)=Tr ϕT K log K K ϕ φ(x)φ(x)∗
pˆ n x n pˆ n pˆ n pˆ x
=S(x)Tg(K )S(x)
pˆ
since S(x) = ϕ φ(x). We show the same way that Trlog(αΣ +(1−α)Σ )φ(x)φ(x)∗ =
x pˆ qˆ
T(x)Tg(K)T(x).
ForthethirdterminthefirstvariationofF = F +F ,i.e. thesecondoneinEquation(44),our
goalistorewriteTr(cid:0) h h∗Σ h h∗∆(cid:1) intermsof1 matric2 es. Wehaveh =ψTc /∥ψTc ∥(idemj)
k k pˆ j j k k k
wherec isaneigenvectorofK ofeigenvalueη ,and∥ψTc ∥2 =Kc =η . Hence,
k k k k k
(cid:18) (cid:18) (cid:19) (cid:19)
Tr(cid:0) h h∗Σ h h∗∆(cid:1) =Tr ψTc cTψψT α1I 0 ψψTc cTψ∆ /(η η )
k k pˆ j j j j 0 0 k k k j
(cid:18) (cid:18) (cid:19) (cid:19)
1I 0
=Tr c cTK α Kc cTψ∆ψT /(η η )
j j 0 0 k k k j
(cid:18) (cid:18) (cid:19) (cid:19)
1I 0
=Tr c cT α c cTψ∆ψT
j j 0 0 k k
(cid:90) (cid:18) (cid:18) 1I 0(cid:19) (cid:19)
=ε Tr c cT α c cTψφ(x)φ(x)∗ψT dξ(x).
j j 0 0 k k
We have ψφ(x)φ(x)∗ψT = V(x)V(x)T where V(x) = (cid:0)αk(x,x ),...,1−αk(x,y )(cid:1)T , and if
n 1 m 1
wenotec =(a ,b )T:
j j j
(cid:18) (cid:19)
c cT
α1I 0
c cT =
⟨a j,a k⟩
c cT.
j j 0 0 k k α j k
Finally,
(cid:16) (cid:16) (cid:17) (cid:17)
Tr Σ (log Γˆ+α∆ −logΓˆ) =
pˆ
 
ε(cid:90) Trn (cid:88)+m ∥a ηj∥2
c jcT
j
+(cid:88)logη
ηj
− −l ηogη
k⟨a j,a k⟩c jcT kV(x)V(x)T. (46)
j j k
j=1 j̸=k
C AdditionalExperiments
SkewnessandconcentrationoftheKKL. Intheseexamples,weplotKKL (pˆ||qˆ)asthenumber
α
ofn=msamplesoftwodistributionsp,qincreases. InFigure4weplottheKKLfortwoGaussians
byvaryingthedimensiond. Itcanbeseenthatthelargerdis,thelessKKLoscillates. Wecanalso
remarkthatthevalueofKKLincreaseswiththedimension,reflectingtheeffectoftheconstantsof
Proposition4. Figure5isthesameexperimentasinthemaintext,exceptthatthedimensionis2.
WecanalsoseeherethatKKL ismonotoneinα. Wecanalsoseethatconvergencetothevalue
α
ofKKLinpopulationisfasterinthiscasethanford=10. ThethirdexperimentinFigure6isin
dimension1andthedistributionofqisanexponentialdistributionwithparameterλ=1,whilepis
aGaussiandistribution. Wecannoticeafewpoints,suchasforexamplethatthevaluestakenby
KKLaresmallerandthatitvarieslesswithαthaninthecaseofFigure5.
SamplingwithKKLgradientdescentonGaussiansandmixturesofGaussians. Weareinter-
estedinsamplingonGaussiansormixturesof2Gaussiansbyvaryingthedimension. Figure7and
Figure8showtheevolutionoftheKKLvalueduringthegradientdescentofdifferentdimensions
d,startingwithaGaussianpandtakingqtobeamixtureof2GaussiansforFigure7andpandq
GaussiansdistributionsforFigure8.Foreachd,wereporttheaverageanderrorbarsofourresultsfor
20runs,varyingthesamplesdrawnfromtheinitializationandthethicklinesrepresentstheaverage
254.0 d d d d d d= = = = = = 2 3 4 5 1 20 0 10 7. .0 5 = = = = = = 1 1 0 0 0 0e e . . . .0 0 0 1- - 0 0 10 0 0 16 5 1 12 .. 50 = = = = 0 0 0 0. . . .0 0 0 10 0 10 11
= 0.5
3.5 5.0 1.0
3.0 2.5 0.5
0.0 0.0
0 500 1000 0 500 1000 0 500 1000
number of particules number of particules number of particules
Figure 4: α = 0.01, p,q Gaus- Figure5: α=0.1,σ =2. Figure 6: p ∼ N(1,1), q ∼
sians,σisthesquareofthemean E(1),α=0.1,σ =2.
ofdistancesbetweenpˆandqˆ.
value. Wecanseethatinbothcasestheconvergenceisfasterforsmallvaluesofd. OnFigure9we
observetheevolutionofW (pˆ||qˆ),the2Wassersteindistance,duringgradientdescentindimension
2
d=10forvariousparametersα. ThedistributionpandqarerespectivelyaGaussianandamixture
of2Gaussians. ThevaluesofW ateachiterationtiscomputedasthemeanofW (pˆ,qˆ)on10runs
2 2
ofthegradientdescentwhereforeachthemeanofpisdrawnatrandom. Wecanseethatiftheα
valueistoohigh,thenconvergencein2-Wassersteinisslower,whereasifitistoosmall,convergence
isfasteratthebeginning,butdoesnotleadtoanoptimalvalueinWassersteindistanceattheendof
thealgorithm.
1.0 d = 2 d = 2 10 = = 0.0001
d = 3 d = 3 = = 0.001
0.8 d = 5 0.6 d = 5 8 = = 0.01
d = 10 d = 10 = = 0.1
0.6 d = 20 d = 20 6 = = 0.5
0.4
0.4 4
0.2
0.2 2
0.0 0.0 0
0 25 50 75 100 0 25 50 75 100 0 100 200 300
iterations iterations iterations
Figure 7: α = 0.01, p is a Figure8: α=0.01,pandqare Figure 9: σ = 10, h = 5, p is
Gaussian distribution and q a Gaussians. Bandwidthσ isthe Gaussianandqisamixtureof2
mixture of 2 Gaussians, σ is mean of the square distances Gaussians.
the square of the mean of dis- betweenpˆandqˆpoints,andh=
tancesbetweenpˆandqˆandh=
1
(cid:16)
(cid:80) ∥x −y
∥2(cid:17)1/2
n−1/(d+4).
1 (cid:16) (cid:80) ∥x −y ∥2(cid:17)1/2 n−1/(d+4n ). i,j i j
n i,j i j
3rings. AppendixCcomparestheevolutionofthegradientflowsofMMD,KaleandKKL in
α
termsofWassertseindistanceandEnergydistanceinthecasewhereoptimisationofKKLisdone
withL-BFGSlinesearchinFigure2. WeobservethatbothKaleandKKLseemtoconvergetowards
0intermsofenergydistanceandWassersteindistancebutKKL isfastertoconverge,intermof
α
numberofiterationsthanKale. TheMMDflowdecreasestheenergydistancebutdoesnotconverge
to0in2-Wassersteindistance,unlikeKaleandKKL,reflectingthefactthatsomeparticlesarenot
supportedonthetargetsupport. Thebandwidthofkisfixedatσ =0.1forKaleandMMDandat
σ =0.3forKKL. InFigure13thistimewerepeattheexperimentbutforasimplegradientdescent
forKKLwithconstantsteph=0.01. Weseethatinthiscasethespeedofconvergenceintermsof
iterationsforKKLisslowerthanforKale(thereareonlyabout100iterationsnecessaryforKaleand
MMDand300forKKL)butitendsupobtaining(seeFigure12),intermsofWassertseindistance,a
similarlimit. Ontheotherhand,theexecutiontimeofthegradientdescentfor300iterationsofKKL
isaboutthesameasforKaleandMMDfor100iterations.
26
)q||p(
LKK
)q||Tp(LKK
)q||p(
LKK
)q||Tp(LKK
)q||p(
LKK
)q||Tp(2WKKL 0.8 KKL 0.8 KKL
MMD MMD MMD
0.4 KALE 0.6 KALE 0.6 KALE
0.4 0.4
0.2
0.2 0.2
0.0 0.0 0.0
0 25 50 75 100 0 25 50 75 100 0 100 200 300
iterations iterations iterations
Figure 10: L-BFGS, σ = 0.3, Figure 11: L-BFGS, σ = 0.3, Figure 12: Constant step size
α=0.01 α=0.01 h=0.01,σ =0.3,α=0.001
T=0 T=2 T=99 T=300
KALE
MMD
KKL
Figure13
27
ecnatsid
ygrene
ecnatsid
2W
ecnatsid
2W