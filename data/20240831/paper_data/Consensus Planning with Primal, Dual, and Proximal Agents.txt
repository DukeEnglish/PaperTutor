Consensus Planning with Primal, Dual, and Proximal Agents
Alvaro Maggiar
Amazon Supply Chain Optimization Technologies
maggiara@amazon.com
Lee Dicker
Amazon Supply Chain Optimization Technologies
leehd@amazon.com
Michael W. Mahoney
Amazon Supply Chain Optimization Technologies
zmahmich@amazon.com
August 29, 2024
Abstract
Consensus planning is a method for coordinating decision making across complex systems and
organizations, including complex supply chain optimization pipelines. It arises when large interde-
pendent distributed agents (systems) share common resources and must act in order to achieve
a joint goal. In this paper, we introduce a generic Consensus Planning Protocol (CPP) to solve
such problems. Our protocol allows for different agents to interact with the coordinating algorithm
in different ways (e.g., as a primal or dual or proximal agent). In prior consensus planning work,
all agents have been assumed to have the same interaction pattern (e.g., all dual agents or all
primal agents or all proximal agents), most commonly using the Alternating Direction Method
of Multipliers (ADMM) as proximal agents. However, this is often not a valid assumption in
practice, where agents consist of large complex systems, and where we might not have the luxury
ofmodifyingtheselargecomplexsystemsatwill. OurgenericCPPallowsforanymixofagentsby
combining ADMM-like updates for the proximal agents, dual ascent updates for the dual agents,
and linearized ADMM updates for the primal agents. We prove convergence results for the generic
CPP, namely a sublinear O(1/k) convergence rate under mild assumptions, and two-step linear
convergence under stronger assumptions. We also discuss enhancements to the basic method and
provide illustrative empirical results.
1 Introduction
Consensus Planning. Consensus planning refers to a coordination mechanism to align different
distributed agents (or systems) who share common resources and who must act in order to achieve
a joint goal. For example, in the context of a large retailer, such a distribution of tasks is necessary,
given the scale of the supply chain problem, with separate systems focusing on (say) buying, removals,
placement, capacity control, fulfillment, or transfers, to name just a few. These individual components
act semi-independently, but some form of communication is required, given their interdependence, to
ensure that they work in unison. This alignment is often times built through ad-hoc solutions, or
proxiesof connectedsystems; andthus thereis aneed forgenericConsensus PlanningProtocols(CPPs)
to solve such problems. A CPP starts with the agents, and it introduces a coordinator. Information is
exchanged between the agents through the coordinator in a structured manner; and the coordinator
reconciles the various favored plans of the individual agents in a principled way to eventually reach a
consensus. Associated with such mechanisms are agent-level plans, costs that characterize their favored
plans, and costs of deviating from them.
Problem. Consensusplanningisoftenviewedasavariantofdistributedoptimization,wherethereare
relatively few optimization agents and querying information from each agent is comparatively costly. A
1
4202
guA
92
]CO.htam[
1v26461.8042:viXraCPP relies on the iterative exchange of information between agents and a coordinator. The mechanism
through which the coordinator updates and aligns the plans and prices hinges upon the nature of the
agents. A theoretically-popular approach centers around the use of proximal interfaces, which are
motivated by the theoretically-attractive properties of the Alternative Direction Method of Multipliers
(ADMM) [BPC+11]. In this proximal framework, the agents and the coordinator exchange two types
of information: 1) a favored plan (primal variable); and 2) a price (or cost of deviation) (dual variable).
These two pieces of information are iteratively exchanged and updated by the agents and coordinator
until convergence. Implicit in this approach of using ADMM as a coordination mechanism is the
assumption that all agents are amenable to interacting with the coordinator via a proximal interface.
In many practical industrial settings, however, this assumption is not satisfied. The reason is
that large complex systems already in place cannot be readily modified to fit this framework without
incurring large engineering costs and development time. One example of such a hurdle would be in
the presence of systems designed as large linear or mixed integer programs that would not allow for
the addition of a proximal (nonlinear) term in their objective function. Another example is when a
system that knows its utility function and can output the value and the gradient of that function (given
a proposed plan) interacts with an agent that takes prices as input and outputs a plan but can not
output the value of its utility function.
More generally, we can categorize agents into three broad classes, depending on the interface that
they support: primal, dual, and proximal. Primal agents can be probed given a tentative plan, and
they return a corresponding cost of deviation; dual agents can be called with a price, and they return a
corresponding favored plan; and proximal agent can consume both a tentative plan and tentative cost,
andtheyreturnupdatedvaluesthereof. MostCPPalgorithmsarebuiltwithonetypeofagentsinmind,
requiring that all agents belong to the same class. The practical difficulty that we address theoretically
in this paper is to devise methodologies that allow for a “mix-and-match” of agents, that are compatible
with all types of agent interfaces, so as not to require the preliminary burden of modifying them to fit
the same type.
Main contributions. The purpose of this paper is to propose a generic CPP algorithm that is
compatible with all three type of agents, without needing that any agent be modified. Our main
algorithm combines ADMM-like updates for the proximal agents, dual ascent updates for the dual
agents, and linearized ADMM updates for the primal agents. We prove several convergence results,
including sublinear O(cid:0)1(cid:1) convergence rate under mild conditions, and two-step linear convergence
k
under stronger conditions; and we provide numerical examples that illustrate the behavior of the
algorithm under different combinations of agent types, as well as the impact of acceleration.
2 Background
2.1 Consensus Problem
The consensus problem is one that involves a set of agents M that each have their own cost function
g (x ),i∈M. Thegoalistooptimizethesumofthesefunctionsforacommonplanz: min (cid:80) g (z).
i i z i∈M i
In order to leverage the separability of the agents, the problem is re-written to endow each agent with
their own plan variable, subject to the constraint that they must all be equal:
(cid:88)
min g (x )
i i
xi,z
i∈M
s.t. x =z, ∀i∈M.
i
Note that each agent’s objective function could itself be the result of an optimization problem of the
form g (x):=min g (x,y), where y corresponds to a set of variables that are “private” to the agent.
i y i
This formulation can be interpreted as the agents working towards agreeing on a common plan z,
and optimizing their own objective function, given the consensus plan. Our objective is to define a
generic Consensus Planning Protocol (CPP) to solve this general consensus problem. What algorithm
we use depends in part on the interface the agents offer, in other words, what information they can
consume, and what information they return. In practical systems, there are many ways agents can
output information and interact with other agents and/or a coordinating algorithm. We detail in the
2next section the three main types of agents considered in our consensus planning problem: primal, dual,
and proximal agents.
2.2 Agents
In our consensus planning problem, there exist three types of agent interfaces, primal, dual, and
proximal, which are the natural interfaces for the three main approaches to the problem, gradient
descent, dual ascent, and the augmented Lagrangian method of multipliers, respectively.
Primal Agents: A primal interface is the one that would be required in a first-order primal method
such as gradient descent, and corresponds to the basic formulation of the consensus problem as:
(cid:88)
min g (z).
i
z
i∈M
In a simple gradient descent scheme, we would in each iteration k query the agents with the
current tentative plan zk, and they would return the gradients ∇g (zk) at that point. We would
i
then update the plan as zk+1 =zk−ρ(cid:80) ∇g (zk), for some step size ρ>0.
i∈M i
Dual Agents: A dual interface arises from a dualization of the constraint, and then solving of the
dual problem as:
(cid:40) (cid:41)
(cid:88)
max min g (x )−λT(z−x ) .
i i i i
λi xi
i∈M
We then require from the agents that they be able to consume the dual variables λ and return
i
the solutions to the subproblems in bracket as x =argmin g (x)−λT(z−x).
i x i i
Proximal Agents: A proximal interface results from the augmented Lagrangian formulation of the
consensus problem, as:
(cid:40) (cid:41)
(cid:88) ρ
max min g (x )−λT(z−x )+ ∥z−x ∥2 .
λi xi
i∈M
i i i i 2 i
In this case, the agents need to be able to consume three pieces of information, prices λ , as in
i
the dual agent, but also a consensus plan z, and a regularizing parameter ρ. They then return
the solution to the subproblem: x =argmin g (x)−λT(z−x)+ ρ∥z−x∥2.
i x i i 2
One of the motivations of the CPP is the ADMM algorithm and its favorable properties, which
make the proximal interface a popular one. However, as a practical matter, in many realistic systems
consisting of large complex agents, those agents cannot be readily “turned into” proximal agents.
Depending on their implementation, agents may naturally exist as primal or dual agents. This requires
that generic CPP algorithms be able to work across interface types.
2.3 Previous Work
Consensus planning is often viewed as a variant of distributed optimization, where there are relatively
few optimization agents and querying information from each agent is comparatively costly. This variant
is in contrast with much of the distributed optimization work where the central concern is that of scale
across many agents [RY22, ch.11]. Most of the distributed optimization algorithms assume the same
interface for all agents: for example, a primal interface in the case of distributed (proximal) gradient
descent; a dual interface in the case of dual decomposition; and a proximal interface in the case of
distributed ADMM. By contrast, this paper allows for any mix of primal, dual, and proximal interfaces.
2.4 Motivating Examples
We present in Table 1 several applications where consensus planning is relevant for a large online
retailer. Note that, in many cases, the interface of a given agent is fixed by the existing implementation
and design thereof, and thus must be accommodated by a generic CPP. These examples involve a few
agents with varying types of interfaces, often times with mixed interfaces. Part of the motivation of
this work is to offer a single, unifying CPP algorithm to handle any such use case, in which different
agents have different interfaces.
3Application Description
Fullness Optimization Objective: coordinate inventory buying with physical network capacity.
Agents: buying agent and fullness (capacity) agent.
Interfaces: dual for the buying agent; primal for the capacity agent.
Throughputcoordination Objective: coordinate inventory flows throughout different regions (e.g.,
inbound, transfer, outbound), given network labor constraints.
Agents: each region.
Interfaces: proximal interface for all agents.
Transportation optimiza- Objective: coordinate transportation capacity across delivery stations
tion and third party carriers.
Agents: different stations, which cover overlapping geographical areas.
Interfaces: proximal interface for all agents.
Arrivals and throughput Objective: Like in throughput coordination, coordinate inventory flows,
coordination but this differs because there is only one region and only one inventory
flow (inbound arrivals) is considered.
Agents: buying agent (similar to fullness optimization problem) and
throughput agent (analogous to throughput coordination problem).
Interfaces: buying agent has a dual interface; throughput agent may
use a primal or dual interface.
Table 1: Examples of consensus problems at a large online retailer.
3 Algorithm
3.1 Derivation
We consider a consensus optimization problem involving a mixed set of agents i∈M that can be either
primal, dual, or proximal. We let P be the set of primal agents, D be the set of dual agents, and X be
the set of proximal agents. Each agent’s objective function is given by g , where i denotes their index,
i
and we assume that the functions g are convex, and in particular µ -strictly convex for i ∈ D, and
i i
have β -Lipschitz continuous gradients for i∈X. The consensus problem to be solved is:
i
(cid:88)
min g (x ) (1)
i i
x,z
i∈M
s.t. Az =x
where:
   
I x
1
. .
A=. ., x= . . .
   
I x
M
We first make the different treatments of the agents explicit by separating agents based on their
type. Applying partial duality (e.g., [LY21, ch. 14]) to the dual and proximal agents, and augmenting
thelatterwiththeaugmentedLagrangianproximaltermassociatedwiththeircorrespondingconstraint,
yields the following equivalent formulation:
max min (cid:88) g (x )+(cid:88) g (x )+λT(z−x )+(cid:88) g (x )+λT(z−x )+ ρ i∥z−x ∥2, (2)
λi,i∈D∪X xi,z
i∈P
i i
i∈D
i i i i
i∈X
i i i i 2 i
s.t. x =z ∀i∈P
i
where we explicitly separate the three types of agents, and ρ is the augmented Lagrangian parameter
i
associated with agent i∈X.
Formulation (2) already suggests a direction for the solution of the problem, owing in particular to
the similarity in structure between ADMM [BPC+11] and traditional dual ascent methods [LY21, ch.
14] used to solve dualized consensus problems. Both of these approaches involve iterations comprised of
the same 3 steps:
41. x update: update of the agents’ primal variables,
i
2. z update: update of the consensus plan through averaging,
3. λ update: update of the dual variables.
i
Thesimilaritybetweendualandproximalagentscanalsobeexplainedbythefactthatoneinterpretation
of the augmented Lagrangian relaxation is that it is simply the regular Lagrangian relaxation applied
to a penalized version of the objective function (penalized by the addition of the quadratic term). That
these two type of updates can be performed jointly is still to be proved, but it provides us with a
framework within which to operate. We then need to further incorporate the primal agents, which are
queried with a tentative plan, x , and return gradient information at that point. We detail in the next
i
paragraph how we treat primal agents as approximate proximal agents, and we integrate them in the
framework outlined above.
Primal agents as approximate proximal agents. A natural idea is to recast the primal agent
as a proximal agent, where its function g is replaced by some linearized approximation thereof using
i
the gradient information returned by the primal agent. Fortunately, a substantive body of work
has demonstrated that the ADMM updates do not need to be exact for the method to converge
[HLHY02, EB92, YZ11]. In particular, the terms of the ADMM updates can be linearized, be it
the augmented quadratic term [LLS11, YY13], or the agent function [OHTG13, Suz13], or both
through a more general use of a Bregman divergence term to replace the quadratic penalty, and an
additionalBregmandivergenceterm[WB14]. Leveragingtheselatterresults,asimpleBregman-ADMM
formulation in the context of the consensus problem has updates of the form:
xk+1 =argming (x)−λkT x+ ρ i∥z−x∥2+D (x,xk),
i x i i 2 ϕi i
where D is the Bregman distance associated with the convex function ϕ (see Appendix B.1). Letting
ϕi i
ϕ be defined as:
i
L
ϕ (x)= i∥x∥2−g (x),
i 2 i
where L ≥β is a constant greater than the Lipschitz constant of ∇g , the update reads:
i i i
xk+1 =argming (xk)+∇g (xk)Tx+ L i∥x−xk∥2−λkT x+ ρ i∥zk−x∥2. (3)
i x i i i i 2 i i 2
We can observe that the function g has been linearized and replaced by a quadratic upper bound. The
i
solution to this subproblem is readily obtained as:
xk+1 = L ixk i +ρ izk − 1 (cid:0) ∇g (xk)−λk(cid:1) .
i L +ρ L +ρ i i i
i i i i
WewillseeinSection3.3thatwhenallagentsareprimal,theapplicationofthislinearizedADMMyields
updates that bear a strong similarity to the ones performed by distributed gradient descent [NO09].
3.2 Formulation
We detail in this section the steps of the algorithm following the high level derivation presented in
Section 3.1. The agents can be either primal (i ∈ P), dual (i ∈ D), or proximal (i ∈ X). We recall
that we consider µ -strongly convex functions g ,i ∈ D for dual agents, and β -Lipschitz continuous
i i i
gradientsforg ,i∈P. Weallowforµ =0andL =+∞forthoseotherfunctionsthatarenotstrongly
i i i
convex or do not have Lipschitz continuous gradients, for notational simplicity. Each agent is allowed
to have their own regularizing parameter/learning rate ρ , although in practice we often use the same
i
one for agents of the same type. We will return to a discussion on the choice of these hyperparameters
in Section 5, but since the dual agents essentially perform dual ascent, and as we will confirm in the
proof of the convergence results, we impose that ρ <µ , ∈D. On the other hand, we recall that we
i i
require L >β for i∈P, and place no restriction on ρ >0 for i∈X. We further assume throughout
i i i
the paper that the initial prices λ0 are such that (cid:80) λ =0.
i i∈M i
In each iteration of the algorithm, we alternate between agent updates, consensus update, and price
updates as follows:
5Agent Updates: Update the agents i∈M in parallel, with the following updates depending on the
agent type.
Primal Agents (i∈P):
xk+1 =argming (xk)+∇g (xk)Tx+ L i∥x−xk∥2−λkT x+ ρ i∥zk−x∥2,
i x i i i i 2 i i 2
= L ixk i +ρzk − 1 (cid:0) ∇g (xk)−λk(cid:1) . (4)
L +ρ L +ρ i i i
i i i i
Dual Agents (i∈D):
xk+1 =argming (x)−λkT x. (5)
i i i
x
Proximal Agents (i∈X):
xk+1 =argming (x)−λkT x+ ρ i∥zk−x∥2. (6)
i x i i 2
Consensus Update:
zk+1 =argmin (cid:88) λkT z+ (cid:88) ρ i∥z−xk+1∥2.
z i 2 i
i∈P∪D∪X i∈P∪D∪X
The update takes the form of a weighted average of the agents’ plans (using the fact that the
sum of the price variables is null):
1 (cid:88)
zk+1 = ρ xk+1. (7)
(cid:80) ρ i i
i∈M i i∈M
Price Updates:
λk+1 =λk+ρ (cid:0) zk+1−xk+1(cid:1) i∈P ∪D∪X. (8)
i i i i
Remark 3.1. All the agents’ problems can be expressed through the same equation as:
xk+1 =argming (x)−λkT x+ ρ˜ i∥zk−x∥2+D (x,xk), (9)
i x i i 2 ϕi i
by letting:
(cid:40) (cid:40)
0, i∈X ∪D ρ , i∈P ∪X
ϕ (x)= , ρ˜ = i
i L 2i∥x∥2−g i(x), i∈P i 0, i∈D.
Note that, technically, the update of the primal agents is performed by the coordinator. The primal
agentsarequeriedwiththetentativeplanxk andreturn∇g (xk), whichisthenusedbythecoordinator
i i i
to yield the updated primal agent plan through (4). Additionally, the consensus update should have a
term comprised of a multiple of the sum of the prices, but summing up the price updates, (8) we obtain
the optimality condition of the consensus update problem, meaning that the sum of the prices is null
after the first iteration (see e.g. [BPC+11]). Making the additional assumption that the sum of the
initial prices is also null further simplifies the notation. The algorithm is summarized in Algorithm 1,
which we name 3-Agent Consensus Planning (3ACP).
3.3 Comments
We can glean some insight into the generic CPP algorithm by considering its behavior when all agents
are of the same type.
6Algorithm 1 Vanilla 3-Agent CP (3ACP)
Let ρ >0, ∀i∈M, and λ0 such that (cid:80) λ0 =0.
i i i∈M i
while convergence criterion not met do
Update the primal, dual, and proximal agents’ plans xk+1 using (4), (5), and (6), respectively.
i
Update the consensus plan zk+1 using (7).
Update the agents’ prices λk+1 using (8).
i
end while
Primal agents only. Consider the case when all the agents are primal agents, and assume for
simplicity that they use common values L = L ,∀i and ρ = ρ ,∀i. The updates take the form of
i i
Bregman ADMM [WB14]:
xk+1 = Lxk i +ρzk − 1 (cid:0) ∇g (xk)−λk(cid:1) ,
i L+ρ L+ρ i i i
yielding consensus updates of the form:
1 (cid:88)
zk+1 = xk+1
|P| i
i∈P
1 (cid:88)
=zk− ∇g (xk),
L+ρ i i
i∈P
by using the fact that the sum of the λ is null.
i
These updates bear strong similarities with a gradient descent type of update. In fact, letting:
xk =(cid:2) xk,xk,...,xk (cid:3)T ,
1 2 M
∇g(xk)=(cid:2) ∇g (xk),∇g (xk),...,∇g (xk )(cid:3)T ,
1 1 2 2 M M
λk =(cid:2) λk,λk,...,λk (cid:3)T
1 2 M
and
 L+ ρ ρ ... ρ 
M M M
1  ρ L+ ρ ... ρ 
W = L+ρ  M. . . M ... M. . .  ,
 
ρ ρ ... L+ ρ
M M M
the updates in the case of all primal agents can be rewritten as:
xk+1 =Wxk−α(cid:0) ∇g(xk)−λk(cid:1) ,
with α= 1 .
L+ρ
Given that the matrix W is symmetric and doubly stochastic, these updates are almost those of
the decentralized gradient descent [NO09], except for the presence of dual variables λk in the update.
Decentralized gradient descent requires that the step sizes be decreasing in order to converge to the
optimal, otherwise it only converges to within a neighborhood of the optimal [NO09, YLY16]. When
the functions are strongly convex, the consensus plan converges linearly to the optimal until it reaches
said neighborhood [YLY16]. By working on both primal and dual variables through the coordinator,
the linearized (Bregman) ADMM overcomes the need for decreasing step sizes and converges linearly to
the optimal solution.
Dual agents only. When all agents are dual and use common values L=L ,∀i, and ρ=ρ ,∀i, the
i i
algorithm yields updates of the form:
xk+1 =argming(x )−λkT x ,
i i i i
xi
71 (cid:88)
zk+1 = xk+1,
|D| i
i∈D
λk+1 =λk+ρ(zk+1−xk+1),
i i i
which are simply the updates resulting from a dual ascent procedure.
Proximal agents only. When all the agents are proximal, the algorithm reduces to the traditional
consensus ADMM [BPC+11].
4 Convergence
4.1 Overview of Results
We prove in this section different forms of convergence of Algorithm 1 (3ACP). We saw in Section 3.3
that when the agents are exclusively primal, dual, or proximal, the algorithm reduces to Bregman
ADMM, dual ascent, and regular ADMM, respectively. Individually, each of these algorithms is
known to converge sublinearly when the functions are merely assumed to be generally convex, and
strongly convex in the case of dual ascent. Under stricter conditions, namely strong-convexity and
Lipschitz continuous gradients, they reach linear convergence rates (see [DY16, HL17] for ADMM and
[WB14, LLF22] for Bregman ADMM).
Our main convergence result in Section 4.4 below shows that when all three types of agents are
combined, and with similar general convexity assumptions, strong convexity for the dual agents,
and Lipschitz-continuous gradients for primal agents, we preserve the sublinear convergence rate.
Furthermore, the additional strong convexity and Lipschitz continuity of the gradients for all agents
allow us to establish two-step linear convergence, which we prove in Section 4.5.
We divide the convergence proofs in three parts: we first establish the plain convergence of the
theorem in Section 4.3; we prove its sublinear convergence rate in Section 4.4; and we consider linear
convergence in Section 4.5.
4.2 Preliminaries and Assumptions
We detail here the main assumptions and some additional notation used in the proofs.
Assumption 4.1. Let M=P∪D∪X be a set of primal (P), dual (D), and proximal agents (X), with
objective functions g , i∈M. For all i∈M, the functions g are convex, and in particular µ -strongly
i i i
convex for i∈D, and with β -Lipschitz continuous gradients for i∈P.
i
These assumptions are the weakest we can require, and the ones we will use to prove the O(cid:0)1(cid:1)
k
convergence of the algorithm. The strong convexity of the dual agents is necessary for the convergence
of dual ascent, which is a special case of 3ACP, while the Lipschitz-continuity of the primal agents is
necessary in order to bound them above by a quadratic function.
The second assumption has to do with the existence of a solution to the problem in the form of a
saddle point for its (regular) Lagrangian, defined as:
(cid:88)
L(x,z,λ):= g (x )+λT(z−x ).
i i i i
i∈M
Assumption 4.2. The (regular) Lagrangian associated with the consensus problem (1) has a saddle
point (x∗,z∗,λ∗):
L(x∗,z∗,λ)≤L(x∗,z∗,λ∗)≤L(x,z,λ∗), ∀ x,z,λ. (10)
The final assumption concerns the learning rate of the dual agents, which we require to be less than
the strong convexity parameter µ , and is the usual assumption for dual ascent.
i
Assumption 4.3. For i∈D, the learning rate ρ used in Algorithm 1 is less than the strong-convexity
i
bound of g : ρ ≤µ .
i i i
8To prove linear convergence results, as well as to tighten the convergence bounds even in the
sublinear case, we will assume that all agents have strongly convex objective functions with Lipschitz
continuous gradients.
Assumption 4.4. Let M be a set of primal, dual, and proximal agents, with objective functions
g , i∈M. Foralli∈M, thefunctionsg areµ -stronglyconvexwithβ -Lipschitzcontinuousgradients.
i i i i
We additionally define some terms and functions that will be useful in the proofs:
Vk := (cid:88) 21
ρ
i∥λk
i
−λ∗ i∥2+ (cid:88) ρ 2i∥zk−z∗∥2+(cid:88) D ϕi(x∗ i,xk i)+(cid:88) D
−g
i∗(λk i,λ∗ i),
i∈M i∈P∪X i∈P i∈D
rk :=(cid:88) 21
ρ
i∥λk
i
−λk i−1∥2−(cid:88) D
−g
i∗(λk i,λk i−1)+ (cid:88) ρ 2i∥zk−1−xk i∥2+(cid:88) D ϕi(xk i,xk i−1),
i∈D i∈D i∈P∪X i∈P
whereg∗ standsfortheconvexconjugateofg (seeAppendixC).ThefunctionVk measuresinsomeway
i i
the distance of the variables, both primal and dual, to their optimal values. Enforcing Assumption 4.3
makes rk non-negative, and it is similar to the notion of residual in ADMM-related proofs, since rk
being null would make the optimality conditions satisfied.
Remark 4.5. • When D =∅, rk can also be expressed as:
rk =
(cid:88) 1
∥λk−λk−1∥2+
(cid:88) ρ i∥zk−zk−1∥2+(cid:88)
D (xk,xk−1),
2ρ i i 2 ϕi i i
i
i∈M i∈M i∈P
owing to the fact that (cid:80) ∥zk−1−xk∥2 =(cid:80) ∥zk−1−zk+zk−xk∥2 =(cid:80) ∥zk−zk−1∥2+
i∈M i∈M i∈M
1 ∥λk−λk−1∥2+ 2(zk−1−zk)T(λk−λk−1)=(cid:80) ∥zk−zk−1∥2+ 1 ∥λk−λk−1∥2, where we
uρ s2
i ed λk−λk−1
=ρρi
(zk−xk), and (cid:80) λk =0.
i∈M ρ2
i
i i i i i∈M i
• When M=D, we have:
(cid:88)
Vk = D (λk,λ∗),
ψi i i
i∈D
(cid:88)
rk = D (λk,λk−1),
ψi i i
i∈D
where ψ (λ):=−g∗(λ)+ 1 ∥λ∥2.
i i 2ρi
4.3 Plain Convergence
We consider in this section the plain convergence of the algorithm. The first step is to leverage the
existence of a saddle point (x∗,z∗,λ∗) and the optimality conditions of the agents’ subproblems to
establish inequalities that will serve as the basis for the various convergence proofs.
Proposition 4.6. Consider the consensus problem (1) and suppose Assumptions 4.1 and 4.2 hold.
Then, the iterates generated by Algorithm 1 satisfy the following inequality for all k ≥0:
0≤ (cid:88) g (xk+1)−g (x∗)+λ∗T(zk+1−xk+1)≤Vk−Vk+1−rk+1, (11)
i i i i i i
i∈M
leading to:
(cid:88) µ i∥xk+1−x∗∥2 ≤Vk−Vk+1−rk+1, and (cid:88) µ i ∥λk−λ∗∥2 ≤Vk−Vk+1−rk+1. (12)
2 i i 2L2 i i
i∈M i∈D i
Proof. See Appendix A.1.
The following proposition justifies the need for Assumption 4.3, as it corresponds to the condition
under which rk+1 is non-negative.
Proposition 4.7. Under Assumption 4.3, we have:
rk+1 ≥0, ∀k ≥0.
9Proof. See Appendix A.2
Remark 4.8. An important observation in Proposition 4.6 is that under Assumption 4.3 we have
in particular Vk+1 ≤Vk, and thus that the (non-negative) sequence {Vk} is non-increasing and thus
bounded, and can serve as a Lyapunov function.
The next theorem concerns the convergence of the algorithm, its asymptotic primal feasibility, as
well as convergence of the dual variables for the dual agents. When the set of dual agents is not null,
Algorithm 1 also implies convergence of the primal and dual variables of the dual agents, as well as
convergence of the consensus variable.
Theorem 4.9 (Convergence of Algorithm 1). Consider the consensus problem (1) and suppose
Assumptions 4.1, 4.2 , and 4.3 hold. Then, for the iterates generated by Algorithm 1 we have:
(cid:88) (cid:88)
g (xk+1)− g (x∗)→0,
i i i i
i∈M i∈M
zk+1−xk+1 →0, ∀i∈M.
i
Additionally, if D ≠ ∅, we have:
λk →λ∗, ∀i∈D,
i i
xk →x∗, ∀i∈M,
i i
zk →z∗.
Proof. See Appendix A.3.
4.4 Sublinear Convergence
We prove in this section the O(cid:0)1(cid:1) ergodic convergence rate of the vanilla 3ACP algorithm (1) under
k
Assumptions 4.1, 4.2, and 4.3. By ergodic, we mean the convergence of the running average of the
iterates. While Theorem 4.9 showed the plain convergence of Algorithm 1, we show here that at any
iteration K, both the distance between the value of the the running average up to K, as well as its
violation of the feasibility constraint decreases at a rate of 1/K.
Theorem 4.10 (Ergodic Sublinear Convergence). Consider the consensus problem (1) and let As-
sumptions 4.1, 4.2, and 4.3 be satisfied. Let:
C :=2V0, ρ¯:=max{ρ }, ρ:=min{ρ },
i i
i i
and define the following running average iterates:
K K
1 (cid:88) 1 (cid:88)
xˆK+1 = xk+1, zˆK+1 = zk+1.
(K+1) (K+1)
k=0 k=0
Then, after K iterations of Algorithm 1, we have:
(cid:12) (cid:12) √
(cid:12)(cid:88) (cid:88) (cid:12) C 2 ρ¯C∥λ∗∥
(cid:12) g (xˆK+1)− g (x∗)(cid:12)≤ + ,
(cid:12) i i i i (cid:12) 2(K+1) ρ(K+1)
(cid:12) (cid:12)
i∈M i∈M
√
(cid:13) (cid:13)Azˆk+1−xˆk+1(cid:13)
(cid:13)≤
2 ρ¯C
.
ρ(K+1)
Proof. See Appendix A.4.
104.5 Linear Convergence
We prove in this section the two-step linear convergence of the 3ACP Algorithm under the stronger
assumptions that all agents’ functions be strongly convex with Lipschitz continuous gradients (Assump-
tion 4.4) rather than the milder assumptions that only the dual agents be strongly convex, and only the
primal agents have Lipschitz continuous gradients. We first define a few terms to simplify the notation,
letting S ⊂M be any subset of agents:
ρ :=min{ρ }, α :=max{β +L +ρ }, L :=β −µ ,
S i∈S i S i∈S i ϕi i ϕi i i
ρ¯ :=max{ρ }, µ :=min{µ }, L :=max{L }.
S i∈S i S i∈S i ϕ i∈P ϕi
Theorem 4.11 (Linear Convergence of 3ACP). Consider the consensus problem (1) and let Assump-
tions 4.4, 4.2, and 4.3 be satisfied. Then, the iterates generated by Algorithm 1 satisfy the following
two-step linear convergence rate:
(cid:18) 1 (cid:26)ρ µ µ µ (cid:27)(cid:19)−1
Vk+2 ≤ 1+ min M, D, P∪X, P Vk.
2 α α ρ L
M D P∪X ϕ
Proof. See Appendix A.5.
Note that if all the agents are of the same type, we recover known results about the (one-step)
linear convergence of the algorithm, corresponding to the linear convergence of ADMM in the case of
all proximal agents (see [LLF22, Thm 3.4]), of Bregman ADMM in the case of all primal agents (see
[LLF22, Thm 3.8]), and of dual ascent in the case of all dual agents. These results are summarized
below.
Corollary 4.12 (LinearConvergenceof3ACPinthecaseofasingleinterface). Consider the consensus
problem (1) and let Assumptions 4.4, 4.2, and 4.3 be satisfied. Then, the iterates generated by
Algorithm 1 satisfy the following linear convergence rates when the agents are all of the same type:
All Primal Agents:
(cid:18) 1 (cid:26)ρ µ µ (cid:27)(cid:19)−1
Vk+1 ≤ 1+ min P, P, P Vk.
3 α ρ L
P P ϕ
All Dual Agents:
(cid:18) 1 (cid:26)ρ µ (cid:27)(cid:19)−1
Vk+1 ≤ 1+ min D, D Vk.
2 α α
D D
All Proximal Agents:
(cid:18) 1 (cid:26)ρ µ (cid:27)(cid:19)−1
Vk+1 ≤ 1+ min X ,, X, Vk.
2 α ρ
X X
Proof. See Appendix A.6.
5 Practical Considerations
Algorithm1presentedthemostbasicversionofthealgorithm,anditcanbeimproveduponinanumber
of different ways, either by relaxing some assumptions, or by incorporating algorithmic improvements.
We consider in this section a few such extensions and practical considerations, some of which could
represent future research directions.
115.1 Acceleration
The algorithms considered so far are essentially first-order methods, only making use of gradient
information in some form or another. Such algorithms can often be accelerated through schemes that
use previous iterates in the updates. One important such acceleration scheme is Nesterov’s accelerated
gradient descent [Nes83], which has been successfully ported to many first order algorithms, including
ADMM. This is especially interesting in our context since the primal and dual agents are handled
as approximations of proximal agents, and are thus likely to lose some performance with respect to
the latter.
Given that ADMM works both on the primal (individual plans and consensus) and dual (prices)
variables, acceleration techniques may target either of those sets of variables, or both. For example,
[GOSB14] presents an accelerated ADMM where the second set of primal variable and the prices are
accelerated, while [KCSB15] only modifies the dual variable but requires an additional update of the
primal variables. [OHTG13] considers accelerating the linearized ADMM algorithm used for the primal
agents and is thus particularly relevant to us. Additionally, accelerated methods are known to not be
monotone in the objective value, and they usually display rippling effects. Adaptive restart strategies
to alleviate these issue were proposed in [OC15], and they can be adapted to the ADMM acceleration
schemes, as was the case in [GOSB14].
One difficulty is combining these different schemes. We can readily apply the accelerated scheme of
[GOSB14] when the agents are dual and/or proximal, while we can also directly apply the algorithm of
[OHTG13] when the agents are all primal, and some results resulting from those implementations are
presented in Section 6. Combining these in order to allow for the acceleration of the algorithm for any
combination of agents will be an interesting research topic.
5.2 Tighter Quadratic Bounds for Primal Agents
Ourapproachtoprimalagentsinvolvedtheuseofaquadraticapproximationoftheobjectivefunctionsg
i
oftheformx(cid:55)→g i(xk i)+∇g i(xk i)T(x−xk i)+L 2i∥x∥2. Thisnonethelessrestrictsthetypeoflinearization
to spherical quadratic functions. We can tighten the bounds by considering not just spherical quadratic
functions but more general quadratic functions so long as they dominate g . Letting H be a symmetric
i i
positive definite matrix such that H ⪰ ∇2g (x), ∀x, we may then use the following function ϕ in
i i i
the definition of the Bregman divergence D : ϕ (x)= 1xTH x−g (x)= 1∥x∥2 −g (x), instead of
ϕi i 2 i i 2 Hi i
L 2ixTx−g i(x).
5.3 Second Order Information
Related to Section 5.2 above, we might actually have access to second order information. This is
especially useful for the primal and dual agents, since in the former case we could make use of a (close
to) second-order linearization, while in the latter case, the price update could result from a Newton (or
quasi-Newton) update as opposed to a simple first-order dual ascent.
For example, for primal agents, we could consider Bregman divergence D letting ϕ (x) =
ϕi i
1xT(∇2g (x)+(β −∥∇2g (x)∥)I)x−g (x) or ϕ (x)= 1xT(∇2g (x)+(β −µ )I)x−g (x).
2 i i i i i 2 i i i i
Fordualagents,wecouldconsiderapriceupdateoftheformλk+1 =λk+ρ ∇2g (xk+1)(zk+1−xk+1),
i i i i i i
or alternatively λk+1 = λk +ρ (∇2g (xk+1)+ϵI)(zk+1−xk+1), for some ϵ > 0, where ρ and ϵ are
i i i i i i i
appropriately chosen to guarantee convergence. In that case, however, the consensus update would
need to be modified to:
(cid:32) (cid:33)−1(cid:32) (cid:33)
(cid:88) (cid:88)
zk+1 = Hk+1 Hk+1xk+1 ,
i i i
i∈M i∈M
where:
(cid:40)
ρ I, for i∈P ∪X,
Hk+1 := i .
i ρ ∇2g (xk+1), for i∈D
i i i
125.4 Choice of Hyperparameters
One critical aspect in the implementation of Algorithm 1, similarly to ADMM algorithms, is the choice
of the hyperparameters ρ ,i∈M. In ADMM, it is traditional to use a single parameter across agents,
i
although there is a benefit to allowing for individual values, especially because different agents might
have different scales of variables for which a single regularizing parameter might not be appropriate.
Convergence bounds and in particular bounds characterizing the linear convergence of ADMM are
often optimized for choices of ρ given by the geometric mean of the strong convexity and gradient
√ i
Lipschitz continuity: ρ = µ β , although this requires both that all the functions be strongly convex
i i i
with Lipschitz continuous gradients, and that we have (approximate) knowledge thereof. On the other
hand, dual ascent, which is essentially what the dual agents are performing, is optimized for ρ =µ .
i i
As a result, even if we aim at using as few parameters as possible, it might be beneficial to set different
values for dual agents than for primal/proximal ones.
In the absence of knowledge of µ and/or β , we can conceive of evaluating them “on the fly” based
i i
on accumulated information, or alternatively, dynamically adjusting them, as suggested by [HYW00].
In particular, the self-adaptive parameter tuning can be applied at an agent level by considering their
respective primal and and dual residuals.
We also note that the consensus update (7) is performed as a weighted sum of the agents’ preferred
plans, where the weights are precisely given by the learning rates ρ . Differences in the learning
i
rates across agents imply differences in the relative importance of each agent’s plan in computing the
consensusplan. Theinteractionbetweenprogressmadebytheindividualagents’plansandtheirweight
in the consensus update is not obvious.
5.5 Regularized Dual Updates
One scheme we considered, but discarded for now as it didn’t seem to have much effect on numerical
examples, was to update the dual agents through a two step procedure, as follows:
x˜k+1 =argming (x)−λkT x,
i i i
x
L x˜k+1+ρ zk
xk+1 = i i i ,
i L +ρ
i i
forsomeL >0,althoughwedonotrequirethatL ≥β unlikeintheprimalcase. xk+1 isthuswritten
i i i i
as a weighted average of x˜k+1 (which would have been the regular update in dual ascent) and z . The
i k
plan is thus regularized towards the consensus. This nonetheless put restrictions on the admissible
values of (ρ ,L ). This procedure can be motivated through several observations:
i i
• The optimality condition for the problem yielding x˜k+1 is ∇g (x˜k+1)=λk. As a result, xk+1 can
i i i i i
be expressed as:
xk+1 =argming (x˜k+1)+∇g (x˜k+1)Tx+ L i∥x−x˜k+1∥2−λkT x+ ρ i∥zk−x∥2.
i x i i i i 2 i i 2
This is an update of the same form as the linearized ADMM update (3) for the primal agent,
where the linearization is here performed at x˜k+1, but because we do not require that L ≥β ,
i i i
the linearization does not necessarily yield an upper bound of g .
i
• Suppose that all the agents are dual agents and we use ρ as a learning rate for the price update
and a common value L=L ,∀i. Performing dual ascent using this two step approach yields the
i
following steps:
x˜k+1 =argming(x )−λkT x ,
i i i i
xi
Lx˜k+1+ρ zk
xk+1 = i i ,
i L+ρ
i
zk+1 =
1 (cid:88)
xk+1 =
Lz˜k+1+ρ izk
,
|D| i L+ρ
i
13λk+1 =λk+ρ (zk+1−xk+1)=λk+ ρ iL (cid:0) z˜k+1−x˜k+1(cid:1) .
i i i i i ρ +L i
i
We observe that this would result in updates that are the ones of a dual ascent procedure with
rate α = ρL , which also imposes the condition ρ(L−µ) ≤ µL since the dual ascent update
ρ+L
requires that α≤µ.
• Suppose the function g is quadratic of the form g (x) = 1LxTx+bTx. Solving the problem
i i 2
as a proximal agent problem yields xk+1 = λk i−b+ρzk. On the other hand, using the above two
i L+ρ
step procedure leads to x˜k+1 = λk i−b, and then xk+1 = Lx˜k i+1+ρzk = λk i−b+ρzk, which is the same
i L i L+ρ L+ρ
updateastheonefortheproximalagent. Asaresult,inthecaseofasphericalquadraticfunction,
any dual agent can be turned into a proximal agent through this regularization. Solving the dual
agent problem and then regularizing is equivalent to directly solving the proximal agent problem.
6 Example: Mixed Quadratic Agents
To illustrate the basic properties of our generic CPP, we consider in this section a synthetic example
involving mixed quadratic agents. The advantage of such a setting, albeit simple, is that quadratic
functions yield closed-form solutions that allow one to get more insight into how different agents are
handled.
6.1 Agents
Consider agents whose objective functions are given by the following, where Q are symmetric definite
i
positive matrices:
1
g (x)= xTQ x+bTx.
i 2 i i
Primal Agents: When i ∈ P, the agent receives xk and returns the gradient ∇g (xk) = Q xk +b .
i i i i i i
Using (4), we obtain an update of the form:
(L I −Q )xk+ρ zk b −λk
xk+1 = i n i i i − i i .
i L +ρ L +ρ
i i i i
Dual Agents: When i∈D, the agent receives λk and returns xk, the solution to the conjugate dual
i i
problem of g at λk:
i i
xk+1 =Q−1(λk−b ).
i i i i
Proximal Agents: When i∈X, the agent receives both λk and zk and returns the solution to (6):
i
xk+1 =(Q +ρ I )−1(ρ zk+λk−b ).
i i i n i i i
We rewrite these updates in a way that allows for a more direct comparison of their respective
mechanisms. Let
xˆk+1 :=argming (x)−λkT xk =Q−1(λk−b )
i i i i i i i
x
be the solution to the dual problem, which in the case of a dual agent is simply the definition of xk+1,
i
but which we extend to the other types of agents using a different notation to avoid confusion. We
then have:
(L I −Q )
primal update xk+1 = (L I +ρ I )−1ρ zk+ (L I +ρ I )−1L xˆk+1+ i n i (xk−xˆk+1)
i i n i n i i n i n i i L +ρ i i
i i
dual update xk+1 = xˆk+1
i i
prox. update xk+1 = (Q +ρ I )−1ρ zk+ (Q +ρ I )−1Q xˆk+1.
i i i n i i i n i i
Formulated in this manner, the update equations yield a number of comments:
14• Alltheupdatesshareacommonstructureandinterpretationasresultingfromaweightedaverage
of the current consensus plan zk and the tentative plan that would have resulted from the current
price λk in the absence of regularization, i.e., the solution resulting from the direct dualization of
i
the constraint.
• In the case of a spherical quadratic function of the form LixTx+bTx, the updates of the primal
2 i
and proximal agents would be exactly the same and would have the form:
ρ zk+L xˆk+1
xk+1 = i i i .
i L +ρ
i i
This would also be the exact same form of the updates for dual agents, were we to use the
regularized dual update suggested in Section 5.5.
• TheexpressionshighlightthefactthatL I isasubstitutefortheHessianoftheagents’functions.
i n
This also emphasizes the potential benefits of having tight quadratic bounds, as suggested in
Section 5.2.
• Compared to the proximal update, the primal update directly replaces the Hessian by L I . As a
i n
result, theweighingbetweenthecurrentconsensuszk andxˆk+1 isthesameinalldirections, while
i
in the proximal update, the effect of the Hessian Q is to weigh different components differently
i
towards one or the other vector.
• Thedualupdatedoesn’tjusttakeaweightedaverageofzk andxˆk+1,italsocontainsanadditional
i
term that drags it towards the previous plan xk. The larger the gap between the upper quadratic
i
bound and the function, the larger this effect.
• The interpretation of the update as a weighted average also provides some guidance as to how to
set ρ. It is reasonable that we would want the weights to be of the same order of magnitude; and,
as a result, we should set ρ to have the same order of magnitude as the eigenvalues of the Q , for
√ i
example, as the geometric mean of the smallest and largest eigenvalues µβ.
6.2 Data
We generate 30 random quadratic functions by generating symmetric definite matrices Q as Q =
i i
αI +ATA , where α>0 guarantees that the matrix is definite, and A =r (2U −1), where r >0
n i i i 1i i 1i
and U is a n×n matrix of random uniform numbers over [0,1]. We use α = 1 and r = U˜ , which
i 1 i
yields matrices with condition numbers ranging from a little over 1 to 60. We then generate 30 random
vectors b as b =r u , where u is a random uniform vector over [0,1]. We used r =1e4.
i i 2 i i 2
The quadratic functions were then assigned to be: 1) all primal; 2) all dual; 3) all proximal; 4) one
third primal, one third dual, and one third proximal; 5) one half primal and one half dual; 6) one half
primal and one half proximal; 7) one half dual and one half proximal.
6.3 Algorithm
We solved all the configurations above using a version of the algorithm that allows for different regular-
izing parameters ρ ,ρ ,ρ , and also applied acceleration with adaptive restart in the configurations
p d x
that allowed it (all primal, all dual, all proximal, and part dual/proximal).
6.4 Results
The results of the algorithm on the different configurations are presented below for different levels of
the regularizing parameters. We tried the following settings:
• ρ =ρ =ρ =0.1 (Figure 1 top left),
p d x
• ρ =ρ =ρ =1 (Figure 1 top right),
p d x
• ρ =ρ =10 and ρ =1 (Figure 1 bottom left),
p x d
• ρ =ρ =50 and ρ =1 (Figure 1 bottom right).
p x d
15Here, ρ is the learning rate used for primal agents, ρ the learning rate used for dual agents, and ρ
p d x
the learning rate used for proximal agents.
The plots show the relative error of the objective function f(z) = (cid:80) g (z) for the consensus
i∈M i
plan iterates z : f(zk)−f(z∗).
k |f(z∗)|
all primal
all primal accelerated
all dual
100 100 a al ll
l
d pu roa xl imac ac lelerated
all proximal accelerated
1/3 primal, 1/3 dual, 1/3 proximal
1/2 primal, 1/2 dual
102 102 1/2 primal, 1/2 proximal
1/2 dual, 1/2 proximal
1/2 dual, 1/2 proximal, accelerated
104 104
all primal
all primal accelerated
106 all dual 106
all dual accelerated
all proximal
all proximal accelerated
108 1/3 primal, 1/3 dual, 1/3 proximal 108
1/2 primal, 1/2 dual
1/2 primal, 1/2 proximal
1/2 dual, 1/2 proximal
1/2 dual, 1/2 proximal, accelerated
1010 1010
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Iterate Iterate
all primal all primal
all primal accelerated all primal accelerated
all dual all dual
100 a al ll l d pu roa xl imac ac lelerated 100 a al ll l d pu roa xl imac ac lelerated
all proximal accelerated all proximal accelerated
1/3 primal, 1/3 dual, 1/3 proximal 1/3 primal, 1/3 dual, 1/3 proximal
1/2 primal, 1/2 dual 1/2 primal, 1/2 dual
102 1/2 primal, 1/2 proximal 102 1/2 primal, 1/2 proximal
1/2 dual, 1/2 proximal 1/2 dual, 1/2 proximal
1/2 dual, 1/2 proximal, accelerated 1/2 dual, 1/2 proximal, accelerated
104 104
106 106
108 108
1010 1010
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Iterate Iterate
Figure 1: Relative error of the consensus plan iterates for different configurations of the mix of agents
and learning rates: ρ =ρ =ρ =0.1 (top left), ρ =ρ =ρ =1 (top right), ρ =ρ =10, ρ =1
p d x p d x p x d
(bottom left), ρ =ρ =50, ρ =1 (bottom right).
p x d
The results in Figure 1 highlight several points:
• Regardless of the mix of agents (which in many cases is constrained by the application and is not
able to be adjusted), the general CPP achieves good convergence. In each case, the convergence
speed of the algorithm is impacted by the choice of the learning rate, which should thus be
addressed in any practical implementation.
• Overall, there is (if possible) a preference for proximal agents because of their superior theoretical
properties and guarantees is reflected in the results.
• The choice between dual and primal agents (when such a choice is possible) is not straightforward.
Dual agents require knowledge or a good guess of the strong convexity constant, while proximal
agents require the use of a Lipschitz constant. Nonetheless, dual agents require a single value to
be set (the learning rate), while primal agents call for two (learning rate and upper bound on the
Lipschitz constant), which could make the dual interface an easier one to implement.
• Acceleration usually yields improvements, but also requires that some parameters be set.
7 Conclusion
We presented in this paper a general Consensus Planning Protocol that doesn’t place any restriction
on agent type, and thus allows for any combination of primal, dual, and proximal agents. We proved
16
rorre
evitaleR
rorre
evitaleR
rorre
evitaleR
rorre
evitaleRthe convergence of the algorithm, and its sublinear O(1) convergence rate, as well as two-step linear
k
convergence under strong convexity and Lipschitz continuous gradients for all functions. Numerical
examples illustrate the behavior of the algorithm under different combinations of agent types, and the
impact of acceleration. While we touched on some practical considerations, several of them will be
explored in future work, in particular as relates to acceleration and asynchronous updates.
References
[BPC+11] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed
optimization and statistical learning via the alternating direction method of multipliers.
Foundations and Trends® in Machine learning, 3(1):1–122, 2011.
[DY16] WeiDengandWotaoYin.Ontheglobalandlinearconvergenceofthegeneralizedalternating
direction method of multipliers. Journal of Scientific Computing, 66:889–916, 2016.
[EB92] JonathanEcksteinandDimitriPBertsekas.OntheDouglas—Rachfordsplittingmethodand
the proximal point algorithm for maximal monotone operators. Mathematical programming,
55:293–318, 1992.
[GOSB14] TomGoldstein,BrendanO’Donoghue,SimonSetzer,andRichardBaraniuk. Fastalternating
direction optimization methods. SIAM Journal on Imaging Sciences, 7(3):1588–1623, 2014.
[HL17] Mingyi Hong and Zhi-Quan Luo. On the linear convergence of the alternating direction
method of multipliers. Mathematical Programming, 162(1-2):165–199, 2017.
[HLHY02] Bingsheng He, Li-Zhi Liao, Deren Han, and Hai Yang. A new inexact alternating directions
method for monotone variational inequalities. Mathematical Programming, 92:103–118,
2002.
[HYW00] Bing-Sheng He, Hai Yang, and SL Wang. Alternating direction method with self-adaptive
penalty parameters for monotone variational inequalities. Journal of Optimization Theory
and applications, 106:337–356, 2000.
[KCSB15] MojtabaKadkhodaie,KonstantinaChristakopoulou,MaziarSanjabi,andArindamBanerjee.
Accelerated alternating direction method of multipliers. In Proceedings of the 21th ACM
SIGKDD international conference on knowledge discovery and data mining, pages 497–506,
2015.
[LLF22] Zhouchen Lin, Huan Li, and Cong Fang. Alternating Direction Method of Multipliers for
Machine Learning. Springer, 2022.
[LLS11] Zhouchen Lin, Risheng Liu, and Zhixun Su. Linearized alternating direction method with
adaptive penalty for low-rank representation. Advances in neural information processing
systems, 24, 2011.
[LY21] David G Luenberger and Yinyu Ye. Linear and Nonlinear Programming. International
series in operations research & management science. Springer Nature, Cham, Switzerland,
5 edition, November 2021.
[Nes83] Yurii Evgen’evich Nesterov. A method of solving a convex programming problem with
convergence rate o(1/k2). In Doklady Akademii Nauk, volume 269, pages 543–547. Russian
Academy of Sciences, 1983.
[NO09] Angelia Nedic and Asuman Ozdaglar. Distributed Subgradient Methods for Multi-Agent
Optimization. IEEE Transactions on Automatic Control, 54(1):48–61, 2009.
[OC15] Brendan O’donoghue and Emmanuel Candes. Adaptive restart for accelerated gradient
schemes. Foundations of computational mathematics, 15:715–732, 2015.
[OHTG13] Hua Ouyang, Niao He, Long Tran, and Alexander Gray. Stochastic alternating direction
methodofmultipliers. InInternationalconferenceonmachinelearning,pages80–88.PMLR,
2013.
17[RY22] Ernest K Ryu and Wotao Yin. Large-Scale Convex Optimization: Algorithms & Analyses
via Monotone Operators. Cambridge University Press, 2022.
[Suz13] Taiji Suzuki. Dual averaging and proximal gradient descent for online alternating direction
multiplier method. In International Conference on Machine Learning, pages 392–400.
PMLR, 2013.
[WB14] HuahuaWangandArindamBanerjee. BregmanAlternatingDirectionMethodofMultipliers.
Advances in Neural Information Processing Systems, 27, 2014.
[YLY16] KunYuan,QingLing,andWotaoYin. Ontheconvergenceofdecentralizedgradientdescent.
SIAM Journal on Optimization, 26(3):1835–1854, 2016.
[YY13] Junfeng Yang and Xiaoming Yuan. Linearized augmented Lagrangian and alternating
directionmethodsfornuclearnormminimization. Mathematicsofcomputation,82(281):301–
329, 2013.
[YZ11] Junfeng Yang and Yin Zhang. Alternating direction algorithms for l -problems in compres-
1
sive sensing. SIAM journal on scientific computing, 33(1):250–278, 2011.
A Proofs
A.1 Proof of Proposition 4.6
The proof of the inequality makes use of the optimality conditions of both the agents’ subproblems,
and the optimality condition of the main problem through the saddle point inequality (10). We thus
start by stating the subproblems’ optimality conditions, which we will subsequently use to transform
the saddle point inequality and express it through relations that involve distances of the iterates to
their optimal values. We will also use the following definition:
V˜k := (cid:88) 1 ∥λk−λ∗∥2+ (cid:88) ρ i∥zk−z∗∥2+(cid:88) D (x∗,xk).
2ρ i i 2 ϕi i i
i
i∈M i∈P∪X i∈P
Subproblems optimality conditions: Algorithm 1 carries out two main types of subproblems, the
agent updates, and the consensus update. The agents’ subproblems correspond to Equations (4),
(5), and (6). Their optimality conditions read:
∇g (xk+1)=λk+ρ (zk−xk+1)−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1) , i∈P (13)
i i i i i i i i i
∇g (xk+1)=λk+ρ (zk−xk+1), i∈X, (14)
i i i i i
∇g (xk+1)=λk, i∈D. (15)
i i i
Saddle-point inequality: We now apply the right-hand side of the saddle-point inequality (10) to
the iterate (xk+1,zk+1,λk+1), thus yielding:
0≤L(xk+1,zk+1,λ∗)−L(x∗,z∗,λ∗)
= (cid:88) g (xk+1)−g (x∗)+λ∗T(zk+1−xk+1),
i i i i i i
i∈M
≤ (cid:88) ∇g (xk+1)T(xk+1−x∗)+λ∗T(zk+1−xk+1), (16)
i i i i i i
i∈M
using the convexity of the functions g . We then apply the optimality conditions (13), (14), and
i
(15) to replace ∇g (xk+1) in the inequality above. For ease of exposition, we consider each term
i i
in turn.
• For the primal agents, we have:
(cid:88) ∇g (xk+1)T(xk+1−x∗)+λ∗T(zk+1−xk+1)
i i i i i i
i∈P
18=(cid:88)(cid:0) λk+ρ (zk−xk+1)−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)(cid:1)T (cid:0) xk+1−x∗(cid:1)
i i i i i i i i i
i∈P
+λ∗T(zk+1−xk+1)
i i
=(cid:88)(cid:0) λk+ρ (zk−zk+1)+ρ (zk+1−xk+1)−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)(cid:1)T (cid:0) xk+1−x∗(cid:1)
i i i i i i i i i i
i∈P
+λ∗T(zk+1−xk+1)
i i
=(cid:88) λk+1T (xk+1−zk+1)+λk+1T (zk+1−z∗)+ρ (zk−zk+1)T(xk+1−x∗)
i i i i i i
i∈P
+λ∗T(zk+1−xk+1)−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (cid:0) xk+1−x∗(cid:1)
i i i i i i i i
(cid:88)
= −(λk+1−λ∗)T(xk+1−zk+1)+ρ (zk−zk+1)T(xk+1−x∗)
i i i i i i
i∈P
−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (cid:0) xk+1−x∗(cid:1) +λk+1T (zk+1−z∗)
i i i i i i i
(cid:88) 1
= − (λk+1−λ∗)T(λk+1−λk)+ρ (zk−zk+1)T(xk+1−zk+1)
ρ i i i i i i
i
i∈X
+ρ (zk−zk+1)T(zk+1−z∗)−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (cid:0) xk+1−x∗(cid:1)
i i i i i i i
+λk+1T (zk+1−z∗)
i
(cid:88) 1
= − (λk+1−λ∗)T(λk+1−λk)−ρ (zk+1−zk)T(zk+1−z∗)
ρ i i i i i
i
i∈P
+(zk+1−zk)T(λk+1−λk)
i i
−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (cid:0) xk+1−x∗(cid:1) +λk+1T (zk+1−z∗)
i i i i i i i
(cid:88) 1 1 1
= − ∥λk+1−λ∗∥2+ ∥λk−λ∗∥2− ∥λk+1−λk∥2
2ρ i i 2ρ i i 2ρ i i
i i i
i∈P
ρ ρ ρ
− i∥zk+1−z∗∥2+ i∥zk−z∗∥2− i∥zk+1−zk∥2
2 2 2
1 ρ ρ
+ ∥λk+1−λk∥2+ i∥zk+1−zk∥2− i∥zk−xk+1∥2
2ρ i i 2 2 i
i
−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (cid:0) xk+1−x∗(cid:1) +λk+1T (zk+1−z∗)
i i i i i i i
=(cid:88) − 1 ∥λk+1−λ∗∥2+ 1 ∥λk−λ∗∥2− ρ i∥zk+1−z∗∥2+ ρ i∥zk−z∗∥2
2ρ i i 2ρ i i 2 2
i i
i∈P
ρ
− i∥zk−xk+1∥2
2 i
−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (cid:0) xk+1−x∗(cid:1) +λk+1T (zk+1−z∗)
i i i i i i i
Using Lemma B.2, we find:
(cid:88) ∇g (xk+1)T(xk+1−x∗)+λ∗T(zk+1−xk+1)
i i i i i i
i∈P
≤(cid:88) − 1 ∥λk+1−λ∗∥2+ 1 ∥λk−λ∗∥2− ρ i∥zk+1−z∗∥2+ ρ i∥zk−z∗∥2
2ρ i i 2ρ i i 2 2
i i
i∈P
ρ
− i∥zk−xk+1∥2
2 i
−D (x∗,xk+1)+D (x∗,xk)−D (xk+1,xk)
ϕi i i ϕi i i ϕi i i
+λk+1T (zk+1−z∗). (17)
i
• The sum over proximal agents is carried out identically to the primal agents, only with
ϕ =0, yielding:
i
(cid:88) ∇g (xk+1)T(xk+1−x∗)+λ∗T(zk+1−xk+1)
i i i i i i
i∈X
19≤ (cid:88) − 1 ∥λk+1−λ∗∥2+ 1 ∥λk−λ∗∥2− ρ i∥zk+1−z∗∥2+ ρ i∥zk−z∗∥2
2ρ i i 2ρ i i 2 2
i i
i∈X
− ρ i∥zk−xk+1∥2+λk+1T (zk+1−z∗). (18)
2 i i
• For the dual agents, we have:
(cid:88) ∇g (xk+1)T(xk+1−x∗)+λ∗T(zk+1−xk+1)
i i i i i i
i∈D
=(cid:88) λkT (xk+1−x∗)+λ∗T(zk+1−xk+1)
i i i i i
i∈D
=(cid:88) λkT (xk+1−zk+1)+λk(zk+1−z∗)+λ∗T(zk+1−xk+1)
i i i i i
i∈D
=(cid:88) −1 (λk−λ∗)T(λk+1−λk)+λkT (zk+1−z∗)
ρ i i i i i
i
i∈D
(cid:88) 1 1 1
= − ∥λk+1−λ∗∥2+ ∥λk−λ∗∥2+ ∥λk+1−λk∥2
2ρ i i 2ρ i i 2ρ i i
i i i
i∈D
+λkT (zk+1−z∗) (19)
i
Combining (17), (18), (19) into (16), we find:
0≤L(xk+1,zk+1,λ∗)−L(x∗,z∗,λ∗)
=V˜k−V˜k+1
−(cid:88)
D (xk+1,xk)−
(cid:88) ρ i∥zk−xk+1∥2+(cid:88) 1
∥λk+1−λk∥2
ϕi i i 2 i 2ρ i i
i
i∈P i∈P∪X i∈D
+ (cid:88) λk+1T (zk+1−z∗)+(cid:88) λkT (zk+1−z∗).
i i
i∈P∪X i∈D
We then recall that (cid:80) λk =0, so that (cid:80) λk =−(cid:80) λk, leading to:
i∈M i i∈P∪X i i∈D i
0≤L(xk+1,zk+1,λ∗)−L(x∗,z∗,λ∗)
≤V˜k−V˜k+1
−(cid:88)
D (xk+1,xk)−
(cid:88) ρ i∥zk−xk+1∥2+(cid:88) 1
∥λk+1−λk∥2
ϕi i i 2 i 2ρ i i
i
i∈P i∈P∪X i∈D
−(cid:88)(cid:0) λk+1−λk(cid:1)T (zk+1−z∗)
i i
i∈D
=V˜k−V˜k+1
−(cid:88)
D (xk+1,xk)−
(cid:88) ρ i∥zk−xk+1∥2+(cid:88) 1
∥λk+1−λk∥2
ϕi i i 2 i 2ρ i i
i
i∈P i∈P∪X i∈D
−(cid:88)(cid:0) λk+1−λk(cid:1)T (zk+1−xk+1)−(cid:88)(cid:0) λk+1−λk(cid:1)T (xk+1−x∗)
i i i i i i i
i∈D i∈D
=V˜k−V˜k+1
−(cid:88)
D (xk+1,xk)−
(cid:88) ρ i∥zk−xk+1∥2+(cid:88) 1
∥λk+1−λk∥2
ϕi i i 2 i 2ρ i i
i
i∈P i∈P∪X i∈D
−(cid:88) 1 ∥λk+1−λk∥2−(cid:88)(cid:0) λk+1−λk(cid:1)T (xk+1−x∗)
ρ i i i i i i
i
i∈D i∈D
=V˜k−V˜k+1
−(cid:88)
D (xk+1,xk)−
(cid:88) ρ i∥zk−xk+1∥2−(cid:88) 1
∥λk+1−λk∥2
ϕi i i 2 i 2ρ i i
i
i∈P i∈P∪X i∈D
20−(cid:88)(cid:0) λk+1−λk(cid:1)T (xk+1−x∗). (20)
i i i i
i∈D
We focus on the last term in the sum, and recall that for i ∈ D, xk+1 = −∇g∗(λk) (see
i i i
Proposition C.2), whence:
(cid:88) −(cid:0) λk+1−λk(cid:1)T (xk+1−x∗)
i i i i
i∈D
=(cid:88) −(cid:0) λk+1−λk(cid:1)T (∇−g∗(λk)−∇−g∗(λ∗))
i i i i i i
i∈D
(cid:88)
= D (λk,λ∗)−D (λk+1,λ∗)+D (λk+1,λk),
−g i∗ i i −g i∗ i i −g i∗ i i
i∈D
using Lemma B.2. Substituting this last equality in (20), we get:
0≤L(xk+1,zk+1,λ∗)−L(x∗,z∗,λ∗)≤Vk−Vk+1−rk+1,
which is the first inequality in Proposition 4.6.
Toobtainthesecondinequality,werecallthatL(xk+1,zk+1,λ∗)−L(x∗,z∗,λ∗)=(cid:80) g (xk+1)−
i∈M i i
g (x∗)+λ∗(zk+1−xk+1). Using the convexity of the functions g , and in particular their strong
i i i i i
convexity for i∈D (letting µ =0 for those functions in P∪X that are not strongly convex), we
i
then have:
(cid:88)
g (xk+1)−g (x∗)+λ∗(zk+1−xk+1)
i i i i i i
i∈M
(cid:88)
= g (xk+1)−g (x∗)−λ∗(xk+1−x∗)+λ∗(zk+1−z∗)
i i i i i i i i
i∈M
≥ (cid:88) µ i∥xk+1−x∗∥2+ (cid:88) λ∗(zk+1−z∗)
2 i i i
i∈M i∈M
= (cid:88) µ i∥xk+1−x∗∥2,
2 i i
i∈M
using the fact that (cid:80) λk =0. This yields the second inequality in Proposition 4.6.
i∈M i
The third one is then obtained by recalling that xk+1 = −∇g∗(λk) for i ∈ D, and thus that
i i i
∥xk+1−x∗∥2 =∥∇g∗(λk)−∇g∗(λk)∥2. Additionally,sincethefunctionsg ,i∈DhaveL -Lipschitz
i i i i i i i i
continuous gradients, their convex conjugates g∗ are 1 -strongly convex, and so ∥∇g∗(λk)−
i Li i i
∇g∗(λk)∥2 ≥ 1 ∥λk−λ∗∥2, which gives the desired inequality.
i i L2 i i
i
A.2 Proof of Proposition 4.7
To prove that rk+1 ≥0, we need only prove that (cid:80) 1 ∥λk−λk−1∥2−(cid:80) D (λk,λk−1)≥0.
i∈D 2ρi i i i∈D −g i∗ i i
Since the functions g are µ -strongly convex, the negative of their conjugates are convex with 1 -
Lipschitz continuous
gi radienti
s. Using Lemma B.3, we have:
µi
(cid:88) 1 (cid:88)
∥λk−λk−1∥2− D (λk,λk−1)
2ρ
i
i i −g i∗ i i
i∈D i∈D
(cid:18) (cid:19)
≥(cid:88) 1 1− ρ i ∥λk−λk−1∥2
2ρ µ i i
i i
i∈D
≥0,
since by assumption we have ρi ≤1.
µi
21A.3 Proof of Theorem 4.9
From Proposition 4.6, we have:
rk+1 ≤Vk−Vk+1.
Summing this inequality over k from 0 to ∞, and using the telescoping sum, we get:
∞
(cid:88)
rk+1 ≤V0.
k=0
Since the sequence {rk} is non-negative and its series sum is bounded above, it implies that rk →0.
We also noted that the sequence Vk is bounded, whence the sequences ∥λk −λ∗∥2, ∥zk −z∗∥, and
i i
D (x∗,xk) are also bounded.
ϕi i i
We then consider the cases where D =∅ and D ≠ ∅ separately.
D =∅: When D =∅, and as shown in Section 4.2, we have:
rk =
(cid:88) 1
∥λk−λk−1∥2+
(cid:88) ρ i∥zk−zk−1∥2+(cid:88)
D (xk,xk−1).
2ρ i i 2 ϕi i i
i
i∈M i∈M i∈P
Itfollowsfromtheconvergenceof{rk}to0that∥λk−λk−1∥2,∥zk−zk−1∥2,andD (xk,xk−1)also
i i ϕi i i
convergeto0foralli∈M. Then,writeλk+1−λk =ρ (zk+1−xk+1)=ρ (zk+1−z∗)−ρ (xk+1−x∗).
i i i i i i i i
We established that λk+1−λk →0, which implies that zk+1−xk+1 →0 as well. We also wrote
i i i
the bounded (and convergent) sequence λk+1−λk as the sum of two sequences, one of which is
i i
bounded, implying that the other one, xk−x∗, is bounded as well.
i i
We now turn to the convergence of the objective function, and show that the difference
(cid:80) g (xk+1)−(cid:80) g (x∗) can be bounded below and above by sequences that converge to 0.
i∈M i i i∈M i i
• We first use the convexity of the functions to bound the difference above:
(cid:88) (cid:88) (cid:88)
g (xk+1)− g (x∗)≤ ∇g (xk+1)T(xk+1−x∗).
i i i i i i i i
i∈M i∈M i∈M
The expressions for the gradients ∇g (xk+1) were given in (13), (13), and (15), yielding:
i i
(cid:88) (cid:88)
g (xk+1)− g (x∗)
i i i i
i∈M i∈M
≤ (cid:88) (cid:0) λk+ρ (zk−xk+1)(cid:1)T (xk+1−x∗)
i i i i i
i∈M
+(cid:88)(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (xk+1−x∗).
i i i i i i
i∈P
The first sum can be transformed to:
(cid:88) (cid:0) λk+ρ (zk−xk+1)(cid:1)T (xk+1−x∗)
i i i i i
i∈M
= (cid:88) (cid:0) λk+1+ρ (zk−zk+1)(cid:1)T (xk+1−x∗)
i i i i
i∈M
= (cid:88) λk+1T (xk+1−x∗)+ρ (zk−zk+1)T(xk+1−x∗)
i i i i i i
i∈M
= (cid:88) −1 λk+1T (λk+1−λk)+λk+1T (zk+1−z∗)+ρ (zk−zk+1)T(xk+1−x∗)
ρ i i i i i i i
i
i∈M
= (cid:88) −1 λk+1T (λk+1−λk)+ρ (zk−zk+1)T(xk+1−x∗).
ρ i i i i i i
i
i∈M
22We thus have:
(cid:88) (cid:88)
g (xk+1)− g (x∗)
i i i i
i∈M i∈M
≤ (cid:88) −1 λk+1T (λk+1−λk)+ρ (zk−zk+1)T(xk+1−x∗)
ρ i i i i i i
i
i∈M
+(cid:88)(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (xk+1−x∗)
i i i i i i
i∈P
(cid:88) 1
≤ ∥λk+1∥∥λk+1−λk∥+ρ ∥zk−zk+1∥∥xk+1−x∗∥
ρ i i i i i i
i
i∈M
(cid:88)
+ ∥∇ϕ (xk+1)−∇ϕ (xk)∥∥xk+1−x∗∥. (21)
i i i i i i
i∈P
Wethenrecallthatthesequence∥λk∥isboundedwhile∥λk+1−λk∥→0;that∥zk+1−zk∥→0
i i i
while ∥xk+1−x∗∥ is bounded; and that since D (xk+1,xk)→0, we have xk−xk+1 →0,
i i ϕi i i i i
and as a result ∥∇ϕ (xk+1)−∇ϕ (xk)∥ → 0 by continuity. Put all together, this implies
i i i i
that (21) converges to 0.
• On the other hand, the first inequality from Proposition 4.6 gives:
(cid:88) g (xk+1)− (cid:88) g (x∗)≥ (cid:88) −λ∗T (cid:0) zk+1−xk+1(cid:1)
i i i i i i
i∈M i∈M i∈M
(cid:88)
≥ −∥λ∗∥∥zk+1−xk+1∥
i i
i∈M
→0,
since we showed that zk+1−xk+1 →0.
i
D ̸=∅: When D ≠ ∅, we have:
rk :=(cid:88) 1 ∥λk−λk−1∥2−(cid:88) D (λk,λk−1)+ (cid:88) ρ i∥zk−1−xk∥2
2ρ
i
i i −g i∗ i i 2 i
i∈D i∈D i∈P∪X
(cid:88)
+ D (xk,xk−1).
ϕi i i
i∈P
It follows from the convergence of {rk} to 0 that ∥λk−λk−1∥2 converges to 0 for all i∈D, as
i i
well as D (xk+1,xk) for all i∈P, and zk−1−xk →0 for all i∈P ∪X. Using Proposition 4.6,
ϕi i i i
and summing the second and third inequalities from 0 to ∞ we also have:
∞
(cid:88)(cid:88)µ
i∥xk+1−x∗∥2 ≤V0,
2 i i
k=0i∈D
∞
(cid:88)(cid:88) µ i ∥λk−λ∗∥2 ≤V0,
2L2 i i
k=0i∈D i
whence ∥xk+1−x∗∥2 →0 and ∥λk−λ∗∥2 →0 for i∈D, i.e. xk →x∗, and λk →λ∗.
i i i i i i i i
Then,sinceρ (zk+1−z∗)=λk+1−λk+ρ (xk+1−x∗)andbothλk+1−λk →0andxk+1−x∗ →0
i i i i i i i i i i
for i∈D, we also have zk+1−z∗ →0. It follows immediately from the convergence of xk and zk
i
to x∗ =z∗ for i∈D that we also have zk+1−xk+1 →0 for all i∈D. For i∈P ∪X, since we
i i
have both that zk →z∗ and zk−1−xk →0, it follows that x →z∗ =x∗.
i i i
For the proof of the convergence of the objective function, we proceed just as in the case where
D = ∅ by bounding the difference (cid:80) g (xk+1)−(cid:80) g (x∗) by sequences that converge
i∈M i i i∈M i i
to 0.
• Using the convexity of the functions g , we have:
i
(cid:88) (cid:88) (cid:88)
g (xk+1)− g (x∗)≤ ∇g (xk+1)T(xk+1−x∗).
i i i i i i i i
i∈M i∈M i∈M
23Using transformations similar to the ones performed above, we find:
(cid:88) (cid:88)
g (xk+1)− g (x∗)
i i i i
i∈M i∈M
≤
(cid:88)
ρ
(cid:0) zk−zk+1(cid:1)T (cid:0) xk+1−x∗(cid:1)
i i i
i∈P∪X
+(cid:88)(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1)T (xk+1−x∗)
i i i i i i
i∈P
+(cid:88)(cid:0) λk−λk+1(cid:1)T (xk+1−x∗).
i i i i
i∈D
Thesameargumentsusingtheboundednessofthesequence∥xk+1−x∗∥,andtheconvergences
i i
to 0 of ∥zk+1−zk∥, ∥xk+1−xk∥ for i∈P, and ∥λk+1−λk∥ for i∈D.
i i i i
• The convergence of the lower bound to 0 is identical to the case where D =∅.
A.4 Proof of Theorem 4.10
Consider the first inequality in Proposition 4.6, and average it over k =0,...,K:
K (cid:40) (cid:41)
1 (cid:88) (cid:88) g (xk+1)− (cid:88) g (x∗) +λ∗T(AzˆK+1−xˆK+1)≤ C .
K+1 i i i i 2(K+1)
k=0 i∈M i∈M
Using the convexity of the functions g , this yields:
i
(cid:88) g (xˆK+1)− (cid:88) g (x∗)+λ∗T(AzˆK+1−xˆK+1)≤ C ,
i i i i 2(K+1)
i∈M i∈M
so that
(cid:12) (cid:12) (cid:13) (cid:13)
(cid:12)(cid:88) (cid:88) (cid:12) C ∥λ∗∥ (cid:13)(cid:88)K (cid:13)
(cid:12) g (xˆK+1)− g (x∗)(cid:12)≤ + (cid:13) (Azk+1−xk+1)(cid:13).
(cid:12) i i i i (cid:12) 2(K+1) (K+1)(cid:13) (cid:13)
(cid:12) (cid:12) (cid:13) (cid:13)
i∈M i∈M k=0
Next, letting P be a diagonal matrix of appropriate size with elements equal to the ρ :
i
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)(cid:88)K (Azk+1−xk+1)(cid:13) (cid:13)=(cid:13) (cid:13)(cid:88)K
P−1(cid:0)
λk+1−λk(cid:1)(cid:13)
(cid:13),
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
k=0 k=0
=(cid:13) (cid:13)P−1(cid:0) λK+1−λ0(cid:1)(cid:13)
(cid:13),
≤∥P−1∥(cid:0)(cid:13) (cid:13)λK+1−λ∗(cid:13) (cid:13)+(cid:13) (cid:13)λ0−λ∗(cid:13) (cid:13)(cid:1)
.
Additionally, as noted in Remark 4.8, Proposition C.2 also implies that Vk ≤ V0 for any k, and in
particular that:
1(cid:13)√ −1 (cid:13)2 C
(cid:13) P (λk+1−λ∗)(cid:13) ≤ ,
2(cid:13) (cid:13) 2
whence it follows that:
(cid:13) (cid:13)λk+1−λ∗(cid:13) (cid:13)≤(cid:112)
ρ¯C,
and thus that:
√
(cid:13) (cid:13)Azˆk+1−xˆk+1(cid:13)
(cid:13)≤
2 ρ¯C
,
ρ(K+1)
which concludes the proof.
24A.5 Proof of Theorem 4.11
To prove the result, we will bound each term in Vk by a term involving Vk −Vk+1. In order to
achieve that, we first derive the inequalities resulting from making use of the strong-convexity and
Lipschitz-continuity of the gradients of all the functions.
Strong convexity bound: The bound resulting from the application of the functions’ µ -strong
i
convexity was already established in Equation (12) of Proposition 4.6 as:
(cid:88) µ i∥xk+1−x∗∥2 ≤Vk−Vk+1−rk+1,
2 i i
i∈M
and in particular:
(cid:88) µ i∥xk+1−x∗∥2 ≤Vk−Vk+1, (22)
2 i i
i∈M
Gradient Lipschitz-continuity bound: The application of gradient Lipschitz continuity to Equa-
tion (11) would have added a term − 1 ∥∇g (xk+1)−∇g (x∗)∥2 to the right of Equation (16) in
the proof of Proposition 4.6, and yiel2 dL eid: i i i i
(cid:88) 1
∥∇g (xk+1)−∇g (x∗)∥2 ≤Vk−Vk+1−rk+1. (23)
2L i i i i
i
i∈M
We can carry out a unified treatment of the terms 1 ∥∇g (xk+1)−∇g (x∗)∥2 by recalling that
the optimality conditions (13), (15), and (14) can a2 lL libe wri itti en as: i i
∇g (xk+1)=λk+ρ˜(zk−xk+1)−(cid:0) ∇ϕ (xk+1)−∇ϕ (xk)(cid:1) ,
i i i i i i i i i
with:
(cid:40)
ρ i∈P ∪X,
ρ˜ = i , ϕ (x)=0, i∈D∪X.
i i
0 i∈D
We then have:
1
∥∇g (xk+1)−∇g (x∗)∥2
2L i i i i
i
= 2L1 (cid:13) (cid:13)λk
i
−λ∗
i
+ρ˜ i(zk−xk i+1)−(cid:0) ∇ϕ i(xk i+1)−∇ϕ i(xk i)(cid:1)(cid:13) (cid:13)2
i
≥ (1 2− Lγ i)(cid:13) (cid:13)λk
i
−λ∗
i
+ρ˜ i(zk−xk i+1)(cid:13) (cid:13)2 − ( γ1 i 2L−1) (cid:13) (cid:13)∇ϕ i(xk i+1)−∇ϕ i(xk i)(cid:13) (cid:13)2
i i
=
2(L
+1
L
)(cid:13) (cid:13)λk
i
−λ∗
i
+ρ˜ i(zk−xk i+1)(cid:13) (cid:13)2 − 2L1 (cid:13) (cid:13)∇ϕ i(xk i+1)−∇ϕ i(xk i)(cid:13) (cid:13)2
i ϕi ϕ
≥
2(L
+1
L
)(cid:13) (cid:13)λk
i
−λ∗
i
+ρ˜ i(zk−xk i+1)(cid:13) (cid:13)2 −D ϕi(xk i+1,xk i).
i ϕi
(cid:16) (cid:17)
We here used the fact that ∥u+v∥2 ≥ (1−γ)∥u∥2 − 1 −1 ∥v∥2 (see Lemma D.1) and set
γ
γ i = LiL +ϕ Li ϕi. This value was set so as to cancel the Bregman divergence term between x ik+1
and xk with the one present in rk+1. We then proceed similarly to isolate the term involving
i
∥λk−λ∗∥2:
i i
1
∥∇g (xk+1)−∇g (x∗)∥2
2L i i i i
i
≥
2(L
+1
L
)(cid:13) (cid:13)λk
i
−λ∗
i
+ρ˜ i(zk−xk i+1)(cid:13) (cid:13)2 −D ϕi(xk i+1,xk i)
i ϕi
≥ 2(( L1− +ν Li) )(cid:13) (cid:13)λk
i
−λ∗ i(cid:13) (cid:13)2 − 2(( Lν1 i +− L1) )ρ˜2
i
(cid:13) (cid:13)(zk−xk i+1)(cid:13) (cid:13)2 −D ϕi(xk i+1,xk i)
i ϕi i ϕi
25=
2(L
+L1
+ρ
)(cid:13) (cid:13)λk
i
−λ∗ i(cid:13) (cid:13)2 − ρ˜ 2i (cid:13) (cid:13)(zk−xk i+1)(cid:13) (cid:13)2 −D ϕi(xk i+1,xk i),
i ϕi i
where we again used Lemma D.1 and set ν = ρ˜i in order to cancel the ∥zk−xk+1∥2 with
i Li+Lϕi+ρi i
the one in rk+1. Plugging this back into Equation (23) yields:
(cid:88)
2(L
+L1 +ρ˜)(cid:13) (cid:13)λk
i
−λ∗ i(cid:13) (cid:13)2 − ρ˜ 2i (cid:13) (cid:13)(zk−xk i+1)(cid:13) (cid:13)2 −D ϕi(xk i+1,xk i)
i∈M
i ϕi i
(cid:88) 1
≤ ∥∇g (xk+1)−∇g (x∗)∥2
2L i i i i
i
i∈M
≤Vk−Vk+1−rk+1
which implies
(cid:88) 1 (cid:13) (cid:13)λk−λ∗(cid:13) (cid:13)2 ≤Vk−Vk+1. (24)
2(L +L +ρ˜) i i
i∈M
i ϕi i
We now bound each term in Vk+1.
• Using (24), we have:
ρ M (cid:88) 1 ∥λk+1−λ∗∥2 ≤ (cid:88) 1 (cid:13) (cid:13)λk+1−λ∗(cid:13) (cid:13)2 ≤Vk+1−Vk+2.
α 2ρ i i 2(L +L +ρ˜) i i
M
i∈M
i
i∈M
i ϕi i
• Recall that since −g∗ has 1 -Lipschitz continuous gradients, we have according to Lemma B.3
i µi
that D (λk+1,λ∗)≤ 1 ∥λk+1−λ∗∥2, and so, using (24) once again:
−g i∗ i i 2µi i i
µ (cid:88)
D D (λk+1,λ∗)≤Vk+1−Vk+2.
α
D
−g i∗ i i
i∈D
• Using Jensen’s inequality and (22), we have:
µ P∪X (cid:88) ρ i∥zk+1−z∗∥2 ≤ µ P∪X (cid:88) ρ i∥zk+1−z∗∥2
ρ 2 ρ 2
P∪X i∈P∪X P∪X i∈M
≤ µ P∪X (cid:88) ρ i∥xk+1−x∗∥2
ρ 2 i i
P∪X i∈M
≤ (cid:88) µ i∥xk+1−x∗∥2
2 i i
i∈M
≤Vk−Vk+1.
• Finally, using Lemma B.3, we have D ϕi(x∗ i,xk i+1)≤ L 2ϕi∥xk i+1−x∗ i∥2, which combined with (22)
gives:
µ P (cid:88) D (x∗,xk+1)≤ µ P (cid:88)L ϕi∥x∗−xk+1∥2
L ϕi i i L 2 i i
ϕ i∈P ϕ i∈P
≤(cid:88)µ
i∥x∗−xk+1∥2
2 i i
i∈P
≤ (cid:88) µ i∥x∗−xk+1∥2
2 i i
i∈M
≤Vk−Vk+1.
26Grouping the four derived inequalities, we have:
ρ (cid:88) 1
M ∥λk+1−λ∗∥2 ≤Vk+1−Vk+2,
α 2ρ i i
M i
i∈M
µ (cid:88)
D D (λk+1,λ∗)≤Vk+1−Vk+2
α
D
−g i∗ i i
i∈D
µ P∪X (cid:88) ρ i∥zk+1−z∗∥2 ≤Vk−Vk+1
ρ 2
P∪X i∈P∪X
µ (cid:88)
P D (x∗,xk+1)≤Vk−Vk+1.
L ϕi i i
ϕ i∈P
Summing these four inequalities yields:
1 (cid:26)ρ µ µ µ (cid:27)
min M, D, P∪X, P Vk+1 ≤Vk−Vk+2.
2 α α ρ L
M D P∪X ϕ
The monotonicity of the sequence {Vk} (see Remark 4.8) further implies that Vk+2 ≤ Vk+1, and
thus that:
1 (cid:26)ρ µ µ µ (cid:27)
min M, D, P∪X, P Vk+2 ≤Vk−Vk+2,
2 α α ρ L
M D P∪X ϕ
whence:
(cid:18) 1 (cid:26)ρ µ µ µ (cid:27)(cid:19)−1
Vk+2 ≤ 1+ min M, D, P∪X, P Vk.
2 α α ρ L
M D P∪X ϕ
A.6 Proof of Theorem 4.12
All Dual Agents: Ifalltheagentsaredual,thenVk =(cid:80) 1 ∥λk−λ∗∥2+D (λk,λ∗). Summing
up only the first two inequalities yields:
i∈D 2ρi i i −g i∗ i i
1 (cid:26)ρ µ (cid:27)
min D, D Vk ≤Vk−Vk+1.
2 α α
D D
The monotonicity of the sequence {Vk} (see Remark 4.8) further implies that Vk ≤Vk+1, and
thus that:
1 (cid:26)ρ µ (cid:27)
min M, D Vk+1 ≤Vk−Vk+1,
2 α α
M D
whence:
(cid:18) 1 (cid:26)ρ µ (cid:27)(cid:19)−1
Vk+1 ≤ 1+ min M, D Vk.
2 α α
M D
All Primal Agents: When all the agents are primal (or proximal), we recall from Remark 4.5 that
rk reads:
rk =
(cid:88) 1
∥λk−λk−1∥2+
(cid:88) ρ i∥zk−zk−1∥2+(cid:88)
D (xk,xk−1).
2ρ i i 2 ϕi i i
i
i∈M i∈M i∈P
As a result, we may change the bounds used in the Gradient Lipshitz-continuity bound in proof
A.5 as follows:
1
∥∇g (xk+1)−∇g (x∗)∥2
2L i i i i
i
≥
2(L
+1
L
)(cid:13) (cid:13)λk i+1−λ∗
i
+ρ˜ i(zk−zk+1)(cid:13) (cid:13)2 −D ϕi(xk i+1,xk i)
i ϕi
27≥ 2(( L1− +ν Li) )(cid:13) (cid:13)λk i+1−λ∗ i(cid:13) (cid:13)2 − 2(( Lν1 i +− L1) )ρ˜2
i
(cid:13) (cid:13)(zk−zk+1)(cid:13) (cid:13)2 −D ϕi(xk i+1,xk i)
i ϕi i ϕi
=
2(L
+L1
+ρ
)(cid:13) (cid:13)λk i+1−λ∗ i(cid:13) (cid:13)2 − ρ˜ 2i (cid:13) (cid:13)(zk−zk+1)(cid:13) (cid:13)2 −D ϕi(xk i+1,xk i),
i ϕi i
where we used Lemma D.1 and set ν = ρ˜i in order to cancel the ∥zk−zk+1∥2 with the
i Li+Lϕi+ρi
one in rk+1. As a result, we may replace (24) with:
(cid:88) 1 (cid:13) (cid:13)λk+1−λ∗(cid:13) (cid:13)2 ≤Vk−Vk+1.
2(L +L +ρ˜) i i
i∈P
i ϕi i
Proceeding exactly as in A.5, we obtain the three following inequalities:
ρ (cid:88) 1
P ∥λk+1−λ∗∥2 ≤Vk−Vk+1,
α 2ρ i i
P i
i∈P
µ
P
(cid:88)ρ
i∥zk+1−z∗∥2 ≤Vk−Vk+1
ρ 2
P i∈P
µ (cid:88)
P D (x∗,xk+1)≤Vk−Vk+1.
L ϕi i i
ϕ i∈P
Summing up these three inequalities yields the desired result.
All Proximal Agents: The proof is identical to the one for all proximal agents, except that there is
one fewer inequality due to the absence of the Bregman terms.
B Bregman Divergence
Definition B.1 (Bregman Divergence). Given a differentiable convex function ϕ, the associated
Bregman divergence is defined as:
D (y,x)=ϕ(y)−ϕ(x)−∇ϕ(x)T(y−x).
ϕ
Lemma B.2. Let ϕ be a convex function and D be the associated Bregman divergence. Then:
ϕ
(∇ϕ(u)−∇ϕ(v))T (w−u)=D (w,v)−D (w,u)−D (u,v), ∀u,v,w.
ϕ ϕ ϕ
Lemma B.3. Let ϕ be a convex function with L-Lipschitz continuous gradients, and D be the
ϕ
associated Bregman divergence. Then:
L
D (x,y)≤ ∥x−y∥2.
ϕ 2
C Convex Conjugate
Definition C.1 (Convex Conjugate). Let f : Rn → R be a convex function. Its convex conjugate
f∗ :Rn →R is defined as:
f∗(y)= sup (cid:8) xTy−f(x)(cid:9) .
x∈Rn
Proposition C.2. Let f be a convex and differentiable function, and f∗ be its convex conjugate. Then
the following statements are equivalent:
y =∇f(x)⇔x=∇f∗(y)⇔xTy =f(x)+f∗(y).
28D Inequalities
Lemma D.1. For any two vectors x,y ∈Rn, and ν >0, we have:
(cid:18) (cid:19)
1
∥x−y∥2 ≥(1−ν)∥x∥2− −1 ∥y∥2.
ν
Proof. We have:
∥x+y∥2 =∥x∥2+∥y∥2+2xTy
√ (cid:18) 1 (cid:19)
=∥x∥2+∥y∥2+2( νx)T √ y
ν
=∥x∥2+∥y∥2+(cid:13) (cid:13) (cid:13)√ νx+ √1 y(cid:13) (cid:13) (cid:13)2 −ν∥x∥2− 1 ∥y∥2
(cid:13) ν (cid:13) ν
(cid:18) (cid:19)
1
≥(1−ν)∥x∥2− −1 ∥y∥2.
ν
29