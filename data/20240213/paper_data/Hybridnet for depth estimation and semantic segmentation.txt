Sanchez-Escobedo, Dalila, et al. "Hybridnet for depth estimation and semantic segmentation." 2018 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.
DOI: 10.1109/ICASSP.2018.8462433 Copyright IEEE
HYBRIDNET FOR DEPTH ESTIMATION AND SEMANTIC SEGMENTATION
Dalila Sa´nchez-Escobedo, Xiao Lin ∗, Josep R. Casas, Montse Parda`s
Universitat Polite`cnica de Catalunya. BARCELONATECH
Image and Video Processing Group
C. Jordi Girona 31, 08034 Barcelona, Spain.
ABSTRACT
Depth Map
Semantic segmentation and depth estimation are two impor-
Color Image
Gl Nob ea twl D ore kp th
Depth Refining
Network
tant tasks in the area of image processing. Traditionally, these
two tasks are addressed in an independent manner. However,
Features Network
Segmentation
Upsampling
for those applications where geometric and semantic informa-
Network
tion is required, such as robotics or autonomous navigation,
Depth Estimation Network Semantic Segmentation Network
depth or semantic segmentation alone are not sufficient. In Fig. 1: HybridNet. Overview of the proposed hybrid convolutional
this paper, depth estimation and semantic segmentation are framework, consisting of two main parts: ”Depth estimation net-
addressed together from a single input image through a hy- work”(blue) and ”Semantic segmentation network” (green). Notice
brid convolutional network. Different from the state of the art both networks are linked in the features network block (blue&green).
methods where features are extracted by a sole feature extrac-
tion network for both tasks, the proposed HybridNet improves
the features extraction by separating the relevant features for a hybrid convolutional network that integrates depth estima-
one task from those which are relevant for both. Experimen- tion and semantic segmentation into a unified framework. The
tal results demonstrate that HybridNet results are comparable idea of integrating the two tasks into a sole structure is mo-
with the state of the art methods, as well as the single task tivated by the fact that segmentation information and depth
methods that HybridNet is based on. maps represent geometrical information of a scene. In this pa-
per we propose to build a model where the features extracted
Index Terms— Semantic segmentation, Depth estima-
are suitable for both tasks, thus leading to an improved ac-
tion, Hybrid convolutional network.
curacy in the estimated information. One of the main advan-
tages of the proposed approach is the straightforward manner
1. INTRODUCTION semantic segmentation and depth map are estimated from a
single image, providing a feasible solution to these problems.
Semantic segmentation and depth information are intrin-
sically related and both pieces of information need to be
2. RELATED WORK
considered in an integrated manner to succeed in challenging
applications, such as robotics [1] or autonomous navigation
Deep learning techniques address depth estimation and se-
[2]. In robotics, performing tasks in interactive environments
mantic segmentation problems with efficient and accurate re-
requires to identify objects as well as their distance from
sults. One of the most influential approaches on semantic seg-
the camera. Likewise, autonomous navigation applications
mentation is the DeepLab model presented in [3]. This model
need a 3D reconstruction of the scene as well as semantic
integrates a VGG structure [4] for the features extraction and
information, to ensure that the agent device has enough in-
upsamples the feature maps with atrous convolution layers to
formation available to carry out the navigation in a safe and
obtain the pixel-level semantic labelling. The VGG structure
independent manner. Although RGB-D sensors are currently
proposed to increase the receptive field of the CNN by stack-
being used in many applications, most systems only provide
ing as many convolutional layers as needed but keeping the
RGB information. This is why addressing depth estimation
size of the filters 3 × 3. It is proved that a significant improve-
and semantic segmentation under a unified framework is of
ment can be achieved with this deeper network.
special interest.
Ghiasi et al. [5] present a Laplacian pyramid for seman-
In the last years, deep learning techniques have shown
tic segmentation refinement incorporating, into the decoding
extraordinary success for both tasks. This paper introduces
step, the spatial information contained in the high-resolution
∗corresponding author feature maps to keep the spatial information destroyed after
concatenationpooling. Thus, a better dense pixel-accurate labeling is ob- these tasks under a feature extraction block whose output be-
tained. On the other hand, [6] introduces ”DeepLabv3” model comes the input of a group of decoders designed to carry out
which solved the spatial accuracy problem using atrous con- each task. We propose a new approach that, additionally to
volution to capture multi-scale context by adopting multiple the common features extraction, uses s a global depth esti-
atrous rates. Although this approach captured contextual in- mation network to estimate separately the global layout of a
formation effectively and fit objects at multiple scales, it still scene from the input image, as shown in figure 1. The main
delivers feature maps with low resolution which generate un- motivation to incorporate this extra step is based on the idea
clear boundaries. that the features network will focus better on extracting com-
For the depth estimation task from monocular images, one mon features working for both tasks by separating the global
of the first efforts was made by Eigen et al in [7]. This ap- information extraction only needed in the depth estimation
proach estimates a low resolution depth map from an input task during the training process. The modularized features
image as a first step, then finer details are incorporated by a extraction process helps on producing better features, which
fine-scale network that locally refines the low resolution depth leads to an improved refined depth map and segmentation.
map using the input image as a reference. Additionally, the The experiments presented in this paper demonstrate that in-
authors introduced a scale-invariant error function that mea- corporating this additional step improves the results obtained
sures depth relations instead of scale. Ivanecky´ [8] presents by those where features are extracted by a sole feature extrac-
an approach inspired in [7], incorporating estimated gradient tion network for both tasks [13].
information to improve the fine tuning stage. Additionally, in
this work a normalized loss function is applied leading to an
3. HYBRIDNET
improvement in depth estimation.
On the other hand, there are some approaches that have Our HybridNet model, presented in figure 1, is divided in two
addressed depth estimation and semantic segmentation into main components: The Depth Estimation Network (blue) and
multiple tasks frameworks. In [9] a unified framework was The Semantic Segmentation Network (green). The first one
proposed that incorporates global and local prediction un- initially estimates a depth map of the scene at a global level
der an architecture that learns the consistency between depth from the input image via a global depth network. Meanwhile,
and semantic segmentation through a joint training process. from the input image, robust feature information is extracted
Another unified framework is presented in [10] where depth by the features network, which is the component shared be-
map, surface normals and semantic labeling are estimated. tween the depth network and the semantic segmentation net-
The results obtained by [10] outperformed the ones presented work. The features network in our hybrid model is based on
in [7] proving how the integration of multiple tasks into a VGG-net [4]. Finally, with the estimated feature information
common framework may lead to a better performance of the and the input image, the global depth map is locally refined in
tasks. the refining depth network obtaining the final depth map. The
A more recent multi-task approach is introduced in [11]. Depth Estimation Network is based on DepthNet [8]. How-
The methodology proposed in this work makes initial estima- ever, DepthNet is formed by global depth estimation, gradient
tions for depth and semantic label at a pixel level through a estimation and depth refinement networks trained separately,
joint network. Later, depth estimation is used to solve possi- whilst Depth Estimation Network is trained end to end skip-
ble confusions between similar semantic categories and thus ping gradient estimation.
to obtain the final semantic segmentation. Another multi- On the other hand, in the Semantic Segmentation Network
task approach by Teichmann et al. [12] presents a network robust feature information is obtained by the features network
architecture named MultiNet that can perform classifica- from the input image, as in the depth estimation network. Af-
tion, semantic segmentation and detection simultaneously. terwards, the Upsampling Network estimates a class score
They incorporate these three tasks into a unified encoder- map where the number of channels is equal to the number
decoder network where the encoder stage is shared among all of labels. The upsampling network is based on Atrous Spa-
tasks and specific decoders for each task producing outputs tial Pyramid Pooling (ASPP) proposed in [3]. We denote the
in real-time. This work efforts were focused on improv- semantic segmentation network as DeepLab-ASPP following
ing the computational efficiency for real-time applications [3].
as autonomous driving. A similar approach is Pixel Level In our proposed system the multi-task work is concen-
enconding and Depth Layering (PLEDL) [13], this work ex- trated in the features network, which is common to both
tended a well known fully convolutional network [14] with tasks, and is based, in our current implementation, in VGG-
three output channels jointly trained to obtain pixel-level se- net. During the training process, the parameters in the fea-
mantic labeling, instance-level segmentation and 3D depth tures network are learned in such a way that the extracted
estimation. features convey both depth and semantic information. As
Multi-task approaches seek to extract features suitable to these two pieces of information are complementary for scene
perform diverse tasks at a time. However, most of them unify understanding, our hybrid model, besides solving the twotasks at a time, can outperform the independent solving of the G C IoUclass
two tasks, leading to a mutual benefit in terms of accuracy. HybridNet 93.26 79.47 66.61
Our approach seeks to analyze the common attributes be- PLEDL [13] - - 64.3
tween tasks as well as their distinctions in order to clarify DeepLab-ASPP [3] 90.99 74.88 58.02
how these two tasks may help each other. The motivation FCN [14] - - 65.3
of building a hybrid architecture where each component has SegNet[17] - - 57.0
a specific function relies on the idea that each part can be GoogLeNetFCN[18] - - 63.0
replaceable for stronger architectures with more parameters
working for the same purpose, in order to obtain expected Table 1: Evaluation of HybridNet against Multi-task and single task
improvements when more computing resources are available. approaches.
For example, replacing VGG-net for RES-Net101 [15] in the
features network. It is important to remember that the main
goal of our hybrid model is to solve more than one task at 5. EXPERIMENTS
a time, but above all to find a way to outperform the results
of both tasks when addressed separately. This paper consid- In this section we present the evaluation of the proposed hy-
ers that sharing parameters between tasks during the training brid model. We aim to determine if the features obtained in
process may lead to mutual benefit in terms of accuracy. Ex- the shared part of the HybridNet solving the two tasks simul-
perimental results illustrate the promising performance of our taneously provide better results than the ones that we would
approach compared with similar state of the art methods. obtain using two identical networks trained separately. This
is why in addition to the results of our HybridNet and for
comparison purposes, we present the results obtained by the
4. TRAINING PARAMETERS AND INITIALIZATION
models that solve these two tasks separately.
The models used to perform semantic segmentation and
The database used for training and experimental evaluation
depth estimation independently are DeepLab-ASPP [3] and
of our model is the Cityscapes dataset [16] which contains
Depth Net [8], respectively. We trained these two models
5000 RGB images manually selected from 27 different cities.
using the code provided by the authors and the Cityscapes
The 5000 images of the dataset are split into 2975 training
images, 500 images for test validation and, for benchmarking dataset in order to compare results. Likewise, to match the
size of the input image with the size that our hybrid model
purposes, 1525 images.
supports, the images in the evaluation set were manually
We fist train DepthNet [8] with Cityscapes dataset for ini-
cropped into 18 different images. Once the semantic segmen-
tialization during 100K iteration. After that we took the pa-
tation and depth estimation were performed, those 18 images
rameters of the global depth network and depth refinement
were rearranged into the original image size.
network to initialize those blocks in our hybrid model. Fea-
tures network and upsampling network are initialized with the Figure 2 provides 4 examples from the evaluation set for
model provided by DeepLab [3] which was pre-trained for visual comparison between the results obtained by our hy-
classification purposes on ImageNet. Once we have a good brid model and ground truth as well as those obtained by
initialization for each block of our hybrid model the whole DeepLab-ASPP. The purpose of this figure is to depict the
network is trained ent-to-end using Cityscapes dataset. differences between a single task and a multi-task approach.
The loss function used in the semantic segmentation net- Figure 2 shows how the segmentation performed by Hybrid-
work L is the sum of the cross-entropy terms for each spa- Net retains with a greater detail the geometrical characteris-
S
tial position in the output class score map, being our targets tics of the objects contained in the scene. Like, for instance,
the ground truth labels. All positions and labels of the out- in the 3rd row where the shapes of a pedestrian and a car can
put class score map are equally weighted in the overall loss be better distinguished in the estimation obtained by the pro-
function with the exception of those unlabeled pixels which posed hybrid model than the obtained by DeepLab-ASPP.
are ignored. The loss function utilized for the depth estima- For depth estimation evaluation, in figure 2 we present a
tion network is composed of two Euclidean losses. L DL com- visual comparison of the results obtained by our hybrid model
putes Euclidean distance between the ground truth and the es- as well as those obtained by the single task approach Depth
timated depth map in linear space, while the L DN computes Net presented in [8] against the ground truth. Note how the
the Euclidean distance between the normalized ground truth results obtained by our HybridNet seem more consistent with
and the estimated map, both normalized by the mean vari- the ground truth than those obtained by Depth Net in terms of
ance normalization. The hybrid loss function L H is defined the depth layering.
as L H = αL S + (L DL + L DN ), where α is the term used to In addition to qualitative results, table 1 presents in the
balance the loss functions of depth estimation and semantic
first section a comparison between HybridNet and the Pixel
segmentation tasks. For training our hybrid model we defined
Level enconding and Depth Layering (PLEDL) approach pro-
α = 1000.
posed in [13], while the second section presents a compari-Input Image DepthNet HybridNet Ground Truth Deeplab-ASPP HybridNet Ground Truth
Fig. 2: Qualitative results. Input image is presented in column 1. Depth estimation results are presented in columns 2 and 3 while column
4 depicts depth ground truth. On the other hand, columns 5 and 6 depict semantic segmentation results while semantic segmentation ground
truth is presented in column 7.
HybridNet DepthNet ages and configuration used for training the HybridNet. The
[8] hybrid architecture outperforms in 4 out of the 8 measures,
γ < 1.25 0.5968 0.6048 higher which proves that training the feature extraction network for
γ < 1.252 0.8221 0.8187 is the tasks of semantic segmentation and depth estimation si-
γ < 1.253 0.9194 0.9152 better multaneously improves also the depth estimation results. Al-
ARD 0.24 0.23
though the quantitative results presented for depth estimation
SRD 4.27 4.43 lower
in PLEDL [13] are similar than ours, they are not comparable
RMSE-linear 12.09 12.35 is
since PLEDL only extracts depth for the detected instances,
RMSE-log 0.4343 0.4340 better
and not the whole scene.
SIE 0.26 0.25
Table 2: Depth estimation. Quantitative evaluation: ratio threshold, 6. CONCLUSIONS
ARD, SRD, RMSE-linear, RMSE-log and SIE.
This article has introduced a methodology that unifies under
a single convolutional framework depth estimation and se-
son between HybridNet and single-task approaches [14, 17, mantic segmentation tasks using as an input a single image.
3, 13]. Three commonly measures for segmentation perfor- The main goal of the proposed method is to seek for a bet-
mance are used: the global accuracy (G) that counts the per- ter hybrid architecture of convolutional neural networks that
centage of pixels which are correctly labeled with respect modularises the features extraction process by separating it
to the ground truth labelling, the class average accuracy (C) into distinct features extraction for a specific task and com-
that determines the mean of the pixel accuracy in each class mon features extraction for both tasks. In this manner, both
and mean intersection over union (IoUclass) that measures tasks can benefit from the extracted common features with-
the average Jaccard scores over all classes. This table shows out being affected by those features only relevant to one task,
that HybridNet outperforms the results obtained by DeepLab- which leads to a better performance. We also prove that solv-
ASPP which is the model HybridNet is based on, as well as ing correlated tasks like semantic segmentation and depth es-
presenting competitive results related to the state of the art. timation together can improve the performance of methods
The evaluation is performed on validation set and test set of tackling the tasks separately. The qualitative and quantitative
cityscapes. results shown in section 5 illustrate that our hybrid model out-
performs the state of the art multi-task approach proposed in
On the other hand, for depth estimation evaluation we em-
[13], as well as the single task approaches it is based on.
ploy 6 commonly used measures: ratio threshold γ, Abso-
lute Relative Difference (ARD), Square Relative Difference
(SRD), Linear Root Mean Square Error (RMSE-linear), Log 7. ACKNOWLEDGMENT
Root Mean Square error (RMSE-Log) and Scale Invariant Er-
ror (SIE) [7]. Table 2 shows the results of our method and This work has been developed in the framework of project
those obtained by DepthNet with the different metrics intro- TEC2016-75976-R and TEC2013-43935-R, financed by the
duced above. Depth Net is identical to the depth estimation Spanish Ministerio de Economia y Competitividad and the
part of HybridNet. It is trained with the same training im- European Regional Development Fund (ERDF).
noitamitsE
htpeD
noitatnemgeS
citnameS8. REFERENCES [12] Marvin Teichmann, Michael Weber, Marius Zoellner,
Roberto Cipolla, and Raquel Urtasun, “Multinet: Real-
[1] David Ball, Patrick Ross, Andrew English, Peter Milani, time joint semantic reasoning for autonomous driving,”
Daniel Richards, Andrew Bate, Ben Upcroft, Gordon arXiv preprint arXiv:1612.07695, 2016.
Wyeth, and Peter Corke, “Farm workers of the future:
Vision-based robotics for broad-acre agriculture,” IEEE [13] Jonas Uhrig, Marius Cordts, Uwe Franke, and Thomas
Robotics & Automation Magazine, 2017. Brox, “Pixel-level encoding and depth layering for
instance-level semantic labeling,” in German Confer-
[2] Utsav Shah, Rishabh Khawad, and K Madhava Krishna, ence on Pattern Recognition. Springer, 2016, pp. 14–25.
“Deepfly: towards complete autonomous navigation of
mavs with monocular camera,” in Proceedings of the [14] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
Tenth Indian Conference on Computer Vision, Graphics “Fully convolutional networks for semantic segmenta-
and Image Processing. ACM, 2016, p. 59. tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
[3] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-
3440.
nos, Kevin Murphy, and Alan L Yuille, “Deeplab:
Semantic image segmentation with deep convolutional [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
nets, atrous convolution, and fully connected crfs,” Sun, “Deep residual learning for image recognition,” in
arXiv preprint arXiv:1606.00915, 2016. Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[4] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni- [16] Marius Cordts, Mohamed Omran, Sebastian Ramos,
tion,” arXiv preprint arXiv:1409.1556, 2014. Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele, “The
[5] Golnaz Ghiasi and Charless C Fowlkes, “Laplacian
cityscapes dataset for semantic urban scene understand-
pyramid reconstruction and refinement for semantic seg-
ing,” in Proceedings of the IEEE Conference on Com-
mentation,” in European Conference on Computer Vi-
puter Vision and Pattern Recognition, 2016, pp. 3213–
sion. Springer, 2016, pp. 519–534.
3223.
[6] Liang-Chieh Chen, George Papandreou, Florian
[17] Vijay Badrinarayanan, Alex Kendall, and Roberto
Schroff, and Hartwig Adam, “Rethinking atrous
Cipolla, “Segnet: A deep convolutional encoder-
convolution for semantic image segmentation,” arXiv
decoder architecture for image segmentation,” arXiv
preprint arXiv:1706.05587, 2017.
preprint arXiv:1511.00561, 2015.
[7] David Eigen, Christian Puhrsch, and Rob Fergus,
“Depth map prediction from a single image using a [18] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
multi-scale deep network,” in Advances in neural in- manet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
formation processing systems, 2014, pp. 2366–2374. Vincent Vanhoucke, and Andrew Rabinovich, “Going
deeper with convolutions,” in Proceedings of the IEEE
[8] Bc JA´ N Ivanecky´, “Depth estimation by convolutional conference on computer vision and pattern recognition,
neural networks,” 2016. 2015, pp. 1–9.
[9] Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian
Price, and Alan L Yuille, “Towards unified depth and
semantic prediction from a single image,” in Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2015, pp. 2800–2809.
[10] David Eigen and Rob Fergus, “Predicting depth, surface
normals and semantic labels with a common multi-scale
convolutional architecture,” in Proceedings of the IEEE
International Conference on Computer Vision, 2015, pp.
2650–2658.
[11] Arsalan Mousavian, Hamed Pirsiavash, and Jana
Kosˇecka´, “Joint semantic segmentation and depth esti-
mation with deep convolutional networks,” in 3D Vision
(3DV), 2016 Fourth International Conference on. IEEE,
2016, pp. 611–619.