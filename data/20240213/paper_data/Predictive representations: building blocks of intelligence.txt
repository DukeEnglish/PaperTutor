Predictive representations:
building blocks of intelligence
Wilka Carvalho,1∗ Momchil S. Tomov,2,3∗ William de Cothi,4∗
Caswell Barry,4 Samuel J. Gershman1,2,5
1Kempner Institute for the Study of Intelligence, Harvard University
2Department of Psychology and Center for Brain Science, Harvard University
3Motional AD LLC
4Department of Cell and Developmental Biology, University College London
5Center for Brains, Minds, and Machines, MIT
∗Equal contribution
February 12, 2024
Abstract
Adaptive behavior often requires predicting future events. The theory of reinforce-
ment learning prescribes what kinds of predictive representations are useful and how
to compute them. This paper integrates these theoretical ideas with work on cogni-
tion and neuroscience. We pay special attention to the successor representation (SR)
and its generalizations, which have been widely applied both as engineering tools and
models of brain function. This convergence suggests that particular kinds of predictive
representations may function as versatile building blocks of intelligence.
1
4202
beF
9
]IA.sc[
1v09560.2042:viXraContents
1 Introduction 3
2 Theory 4
2.1 The reinforcement learning problem . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Classical solution methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 The successor representation . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.4 Successor models: a probabilistic perspective on the SR . . . . . . . . . . . . 11
2.5 Successor features: a feature-based generalization of the SR . . . . . . . . . . 13
2.5.1 Generalized policy improvement: adaptively combining behaviors . . 15
2.5.2 Option Keyboard: chaining together behaviors . . . . . . . . . . . . . 16
2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3 Practical learning algorithms
and associated challenges 17
3.1 Learning successor features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1.1 Discovering cumulants . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1.2 Estimating successor features . . . . . . . . . . . . . . . . . . . . . . 17
3.2 Learning successor models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2.1 Learning successor models that one can sample from . . . . . . . . . 19
3.2.2 Learning successor models that one can evaluate . . . . . . . . . . . . 20
4 Artificial intelligence applications 22
4.1 Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.1.1 Pure exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.1.2 Balancing exploration and exploitation . . . . . . . . . . . . . . . . . 24
4.2 Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.2.1 Transferring behaviors between tasks . . . . . . . . . . . . . . . . . . 25
4.2.2 Learning about non-task goals . . . . . . . . . . . . . . . . . . . . . . 26
4.2.3 Other advances in transfer . . . . . . . . . . . . . . . . . . . . . . . . 26
4.3 Hierarchical reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . 27
4.3.1 Discovering options to efficiently explore the environment . . . . . . . 27
4.3.2 Transferring options with the SR . . . . . . . . . . . . . . . . . . . . 28
4.4 Model-based reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . 29
4.5 Multi-agent reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . 29
4.6 Other artificial intelligence applications . . . . . . . . . . . . . . . . . . . . . 30
5 Cognitive science applications 31
5.1 Revaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
5.2 Multi-task learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
5.3 Associative learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
5.3.1 Bayesian RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
25.3.2 Context-dependent learning . . . . . . . . . . . . . . . . . . . . . . . 38
5.3.3 Bayesian learning of context-dependent predictive maps . . . . . . . . 39
5.4 Spatial navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
5.5 Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
5.5.1 The temporal context model . . . . . . . . . . . . . . . . . . . . . . . 45
5.5.2 TCM as an estimator for the SR . . . . . . . . . . . . . . . . . . . . 45
5.5.3 Combining TCM and the SR for decision making . . . . . . . . . . . 47
6 Neuroscience applications 48
6.1 A brief introduction to the medial temporal lobe . . . . . . . . . . . . . . . . 49
6.2 The hippocampus as a predictive map . . . . . . . . . . . . . . . . . . . . . 49
6.3 Learning a biologically plausible SR . . . . . . . . . . . . . . . . . . . . . . . 53
6.4 Replay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
6.5 Dopamine and generalized prediction errors . . . . . . . . . . . . . . . . . . 56
7 Conclusions 57
1 Introduction
The ability to make predictions has been hailed as a general feature of both biological and
artificial intelligence, cutting across disparate perspectives on what constitutes intelligence
(e.g., Ciria et al., 2021; Clark, 2013; Friston and Kiebel, 2009; Ha and Schmidhuber, 2018;
Hawkins and Blakeslee, 2004; Littman and Sutton, 2001; Lotter et al., 2016). Despite this
general agreement, attempts to formulate the idea more precisely raise many questions:
Predict what, and over what timescale? How should predictions be represented? How
should they be used, evaluated, and improved? These normative “should” questions have
corresponding empirical questions about the nature of prediction in biological intelligence.
Our goal is to provide systematic answers to these questions. We will develop a small set of
principles that have broad explanatory power.
Our perspective is based on an important distinction between predictive models and pre-
dictive representations. A predictive model is a probability distribution over the dynamics of
a system’s state. A model can be “run forward” to generate predictions about the system’s
future trajectory. This offers a significant degree of flexibility: an agent with a predictive
model can, given enough computation time, answer virtually any query about the proba-
bilities of future events. However, the “given enough computation time” proviso places a
critical constraint on what can be done with a predictive model in practice. An agent that
needs to act quickly under stringent computational constraints may not have the luxury of
posing arbitrarily complex queries to its predictive model. Predictive representations, on
the other hand, cache the answers to certain queries, making them accessible with limited
computational cost. The price paid for this efficiency gain is a loss of flexibility: only certain
queries can be accurately answered.
3Caching is a general solution to ubiquitous flexibility-efficiency trade-offs facing intelli-
gent systems (Dasgupta and Gershman, 2021). Key to the success of this strategy is caching
representations that make task-relevant information directly accessible to computation. We
will formalize the notion of task-relevant information, as well as what kinds of computations
access and manipulate this information, in the framework of reinforcement learning (RL)
theory (Sutton and Barto, 2018). In particular, we will show how one family of predictive
representation, the successor representation (SR) and its generalizations, distills information
thatisusefulforefficientcomputationacrossawidevarietyofRLtasks. Thesepredictiverep-
resentations facilitate exploration, transfer, temporal abstraction, unsupervised pre-training,
multi-agent coordination, creativity, and episodic control. On the basis of such versatility,
we argue that these predictive representations can serve as fundamental building blocks of
intelligence.
Converging support for this argument comes fromcognitive scienceand neuroscience. We
review a body of data indicating that the brain uses predictive representations for a range
of tasks, including decision making, navigation, and memory. We then discuss biologically
plausible algorithms for learning and computing with predictive representations. This con-
vergence of biological and artificial intelligence is unlikely to be a coincidence: predictive
representations may be an inevitable tool for intelligent systems.
2 Theory
In this section, we introduce the general problem setup and a classification of solution tech-
niques. We then formalize the SR and discuss how it fits into the classification scheme.
Finally, we describe two key extensions of the SR that make it much more powerful: the
successor model and successor features. Due to space constraints, we omit some more exotic
variants such as the first-occupancy representation (Moskovitz et al., 2021) or the forward-
backward representation (Touati et al., 2022).
2.1 The reinforcement learning problem
We consider an agent situated in a Markov decision process (MDP) defined by the tuple
M = (γ,S,A,T,R), where γ ∈ [0,1) is a discount factor, S is a set of states (the state
space), A is a set of actions (the action space), T(s′|s,a) is the probability of transitioning
from state s to state s′ after taking action a, and R(s) is the expected reward in state s.1
Following Sutton and Barto (2018), we consider settings where the MDP and an agent give
rise to a trajectory of experience
s ,a ,r ,s ,a ,... (1)
0 0 1 1 1
where state s and action a lead to reward r and state s . The agent chooses actions
t t t+1 t+1
probabilistically according to a state-dependent policy π(a|s).
1For notational convenience, we will assume that the state and action spaces are both discrete, but this
assumption is not essential.
4We consider settings where the agent prioritizes immediate reward over future reward so
we focus on discounted returns. The value of a policy is the expected2 discounted return:
" H (cid:12) #
Vπ(s) = E X γtR(s t+1)(cid:12) (cid:12) (cid:12)s 0 = s , (2)
(cid:12)
t=0
One can also define a state-action value (i.e., the expected discounted return conditional on
action a in state s) by:
" H (cid:12) #
Qπ(s,a) = E X γtR(s t+1)(cid:12) (cid:12) (cid:12)s 0 = s,a 0 = a (3)
(cid:12)
t=0
The optimal policy π∗ is then defined by:
π∗(·|s) = argmaxVπ(s) = argmaxX π(a|s)Qπ(s,a), (4)
π(·|s) π(·|s) a
TheoptimalpolicyforanMDPisalwaysdeterministic(choosethevalue-maximizingaction):
π∗(a|s) = I[a = argmaxQ∗(s,a˜)], (5)
a˜
where I[·] = 1 if its argument is true, 0 otherwise. This assumes that the agent can compute
the optimal values Q∗(s,a) = max Qπ(s,a) exactly. In most practical settings, values must
π
be approximated. In these cases, stochastic policies are useful (e.g., for exploration), as
discussed later.
The Markov property for MDPs refers to the conditional independence of the past and
future given the current state and action. This property allows us to write the value function
in a recursive form known as the Bellman equation (Bellman, 1957):
Vπ(s) = X π(a|s)X T(s′|s,a)[R(s′)+γVπ(s′)]
a s′
= E[R(s′)+γVπ(s′)]. (6)
Similarly, the state-action value function obeys a Bellman equation:
Qπ(s,a) = E[R(s′)+γQπ(s′,a′)]. (7)
These Bellman equations lie at the heart of many efficient RL algorithms, as we discuss next.
2.2 Classical solution methods
We say that an algorithm “solves” an MDP if it outputs an optimal policy (or an approxi-
mation thereof). Broadly speaking, algorithms can be divided into two classes:
2To simplify notation, we will sometimes leave implicit the distributions over which the expectation is
being taken.
5• Model-based algorithms use an internal model of the MDP to compute the optimal
policy (Figure 1, left).
• Model-free algorithms compute the optimal policy by interacting with the MDP
(Figure 1, middle).
These definitions allow us to be precise about what we mean by predictive model and predic-
ˆ
tive representation. ApredictivemodelcorrespondstoT, theinternalmodelofthetransition
ˆ ˆ
distribution, and R, an internal model of the reward function. An agent equipped with T
can simulate state trajectories and answer arbitrary queries about the future. Of principal
relevance to solving MDPs is policy evaluation—an answer to the query, “How much reward
do I expect to earn in the future under my current policy?” A simple (but inefficient) way
to do this, known as Monte Carlo policy evaluation, is by running many simulations from
each state (roll-outs) and then averaging the discounted return.3 The basic problem with
this approach stems from the curse of dimensionality (Bellman, 1957): the trajectory space
is enormous, requiring a number of roll-outs that is exponential in the trajectory length.
A better model-based approach exploits the Bellman equation. For example, the value
ˆ
iteration algorithm starts with an initial estimate of the value function, Vπ, then simultane-
ously improves this estimate and the policy by applying the following update (known as a
Bellman backup) to each state:
Vˆ π(s) ← maxX Tˆ (s′|s,a)h Rˆ (s′)+γVˆ π(s′)i . (8)
a
s′
This is a form of dynamic programming, guaranteed to converge to the optimal solution,
ˆ ˆ
V∗(s) = max Vπ(s), when the agent’s internal model is accurate (T = T,R = R). After
π
convergence, the optimal policy for state s is given by:
π∗(a|s) = I[a = a∗(s)], (9)
ˆ
a∗(s) = argmaxQ∗(s,a) ≈ argmaxQπ(s,a). (10)
a a
The approximation becomes an equality when the agent’s internal model is accurate.
Value iteration is powerful, but still too cumbersome for large state spaces, since each
iteration requires O(|A||S|2) steps. The basic problem is that algorithms like value iteration
attempt to compute the optimal policy for every state, but in an online setting an agent only
needs to worry about what action to take in its current state. This problem is addressed
by tree search algorithms, which rely on roll-outs (as in Monte Carlo policy evaluation), but
only from the current state. When combined with heuristics for determining which roll-outs
to perform (e.g., learned value functions; see below), this approach can be highly effective
(Silver et al., 2016).
3Technically,thisapproachwon’tconvergetothecorrectanswerintheinfinitehorizonsettingwithfinite-
lengthroll-outs,thoughthediscountfactorensuresthatrewardsinthedistantfuturehaveavanishingeffect
on the discounted return.
6Model-based Model-free Predictive
representations
s s s
s
4 5 6
a 5
a a a 2
2 2 2 a a
a a 1 s 3 s π s
s 1 s 3 s 1 4 2 6
2 1 3 π π
1 3
s
1
T(s ,a ) = s Qπ(s ,a ) =
1 1 2 1 1
T(s ,a ) = s Qπ(s ,a ) =
T(s1 1,a2 3) = s5
3
Qπ(s1 1,a2 3) = MMπ π1 2( (s s1, ,s s4)
)
=
=
1
1
1 5
R(s 4) = π(s 1,a 1) = 0.4 Mπ3(s 1,s 6) = 1
R(s ) = π(s ,a ) = 0.4
R(s5 6) = π(s1 1,a2 3) = 0.2 ψψπ π1 2( (s s1)
)
=
=
1
ϕ(s ) = ψπ3(s ) = 4 1
ϕ(s ) =
5
ϕ(s ) = R(s ) =
6 4
R(s ) =
5
R(s ) =
6
Figure 1: Algorithmic solutions to the RL problem. An agent solving a three-
armed maze (bottom) can adopt different classes of strategies (top). Model-based strategies
(left) learn an internal model of the environment, including the transition function (T), the
reward function (R), and (optionally) the features (φ). At decision time, the agent can
run forward simulations to predict the outcomes of different actions. Model-free strategies
(middle) learn action values (Q) and/or a policy (π). At decision time, the agent can consult
the cached action values and/or policy in the current state. Strategies relying on predictive
representations (right) learn the successor representation (SR) matrix (M) mapping states
to future states and/or the successor features (ψ) mapping states to future features, as well
as the reward function (R). At decision time, the agent can consult the cached predictions
and cross-reference them with its task (specified by the reward function) to choose an action.
7
snoitisnarT
sdraweR
serutaeF
noitcA
yciloP
seulav
rosseccuS
rosseccuS
noitatneserper
serutaef
sdraweRDespite their effectiveness for certain problems (e.g., games like Go and chess), model-
based algorithms have had only limited success in a wider range of problems (e.g., video
games) due to the difficulty of learning a good model and planning in complex (possibly
infinite/continuous) state spaces.4 For this reason, much of the work in modern RL has
focused on model-free algorithms.
ˆ ˆ
A model-free agent by definition has no access to T (and sometimes no access to R),
but nonetheless can still answer certain queries about the future if it has cached a predictive
representation. For example, an agent could cache an estimate of the state-action value func-
ˆ
tion, Qπ(s,a) ≈ Qπ(s,a). This predictive representation does not afford the same flexibility
as a model of the MDP, but it has the advantage of caching, in a computationally convenient
form, exactly the information about the future that an agent needs to act optimally.
ˆ
Importantly, Qπ(s,a) can be learned purely from interacting with the environment, with-
outaccesstoamodel. Forexample,temporal difference (TD)learningmethodsusestochastic
approximation of the Bellman backup. Q-learning is the canonical algorithm of this kind:
ˆ ˆ
Qπ(s,a) ← Qπ(s,a)+ηδ (11)
ˆ ˆ
δ = R(s′)+γmaxQπ(s′,a′)−Qπ(s,a), (12)
a′
where η ∈ [0,1] is a learning rate and s′ is sampled from T(s′|s,a). When the estimate is
ˆ
exact, E[δ] = 0 and Qπ = Qπ. Moreover, these updates converge to Q∗(s,a) with probability
1 (Watkins and Dayan, 1992).
We have briefly discussed the dichotomy of model-based versus model-free algorithms for
learning an optimal policy. Model-based algorithms are more flexible—capable of generating
predictions about future trajectories—while model-free algorithms are more computationally
efficient—capable of rapidly computing the approximate value of an action. The flexibility
of model-based algorithms is important for transfer: when the environment changes locally
(e.g., a route is blocked, or the value of a state is altered), an agent’s model will typically
also change locally, allowing it to transfer much of its previously learned knowledge without
extensive new learning. In contrast, a cached value function approximation (due to its long-
term dependencies) will change non-locally, necessitating more extensive learning to update
all the affected cached values.
One of the questions we aim to address is how to get some aspects of model-based
flexibility without learning and computing with a predictive model. This leads us to another
class of predictive representations: the SR. In this section, we describe the SR (§2.3), its
probabilistic variant (the successor model; §2.4), and an important generalization (successor
features; §2.5). Applications of these concepts will be covered in §4.
2.3 The successor representation
The SR, denoted Mπ, was introduced to address the transfer problem described in the pre-
vious section (Dayan, 1993; Gershman, 2018). In particular, the SR is well-suited for solving
4Recent work on applying model-based approaches to video games has seen some success (Tsividis et al.,
2021),butprogresstowardsscalableandgenerallyapplicableversionsofsuchalgorithmsisstillinitsinfancy.
8Goal
s<latexit sha1_base64="3fl7AMR82nporqgYiPa0q5VI1tY=">AAAB8HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqseiF48V7Ie0oWw203bpbhJ2N0IJ/RVePCji1Z/jzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSNsGm4EdhKFVAYC28H4dua3n1BpHkcPZpKgL+kw4gPOqLHSY89wEWKmp/1yxa26c5BV4uWkAjka/fJXL4xZKjEyTFCtu56bGD+jynAmcFrqpRoTysZ0iF1LIypR+9n84Ck5s0pIBrGyFRkyV39PZFRqPZGB7ZTUjPSyNxP/87qpGVz7GY+S1GDEFosGqSAmJrPvScgVMiMmllCmuL2VsBFVlBmbUcmG4C2/vEpaF1WvVq3dX1bqN3kcRTiBUzgHD66gDnfQgCYwkPAMr/DmKOfFeXc+Fq0FJ585hj9wPn8AP1aQuA==</latexit>˜
s
<latexit sha1_base64="uWioic9Sc3+uT7pr32g9YUpvtBk=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZq6H6p7FbcOcgq8XJShhz1fumrN4hZGqE0TFCtu56bGD+jynAmcFrspRoTysZ0iF1LJY1Q+9n80Ck5t8qAhLGyJQ2Zq78nMhppPYkC2xlRM9LL3kz8z+umJrzxMy6T1KBki0VhKoiJyexrMuAKmRETSyhT3N5K2IgqyozNpmhD8JZfXiWty4pXrVQbV+XabR5HAU7hDC7Ag2uowT3UoQkMEJ7hFd6cR+fFeXc+Fq1rTj5zAn/gfP4A4jeNAg==</latexit>
Agent
s
<latexit sha1_base64="uWioic9Sc3+uT7pr32g9YUpvtBk=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KolI9Vj04rEF+wFtKJvtpF272YTdjVBCf4EXD4p49Sd589+4bXPQ1gcDj/dmmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJssFrHqBFSj4BKbhhuBnUQhjQKB7WB8N/PbT6g0j+WDmSToR3QoecgZNVZq6H6p7FbcOcgq8XJShhz1fumrN4hZGqE0TFCtu56bGD+jynAmcFrspRoTysZ0iF1LJY1Q+9n80Ck5t8qAhLGyJQ2Zq78nMhppPYkC2xlRM9LL3kz8z+umJrzxMy6T1KBki0VhKoiJyexrMuAKmRETSyhT3N5K2IgqyozNpmhD8JZfXiWty4pXrVQbV+XabR5HAU7hDC7Ag2uowT3UoQkMEJ7hFd6cR+fFeXc+Fq1rTj5zAn/gfP4A4jeNAg==</latexit>
Figure 2: The successor representation (SR). We present (a) a schematic of an envi-
ronment where the agent is a red box at state s and the goal is a green box at state s . In
13 5
general, an SR Mπ(s,s˜) (Eq. 13) describes the discounted state occupancy for state s˜when
beginning at state s and following policy π. In panels (b-c), we showcase Mπ(s ,s˜) for a
13
random policy and an optimal policy. In (b), we can see that the SR under a random policy
measures high state occupancy near the agent’s current state (e.g. Mπ(s ,s ) = 5.97) and
13 14
low state occupancy at points further away from the agent (e.g. Mπ(s ,s ) = 0.16). In
13 12
(c), we see the SR under the optimal policy has highest state occupancy along the shortest
path to the goal (e.g. here Mπ(s ,s ) = .66), fading as we get further from the current
13 12
state. In contrast to a random policy, states not along that path have 0 occupancy (e.g.
Mπ(s ,s ) = 0.0). Once we know a reward function, we can efficiently evaluate both poli-
13 19
cies (Eq. 19). (d) We show an example reward function that has a cost of −0.1 for each state
except the goal state where reward is 1. The SR allows us to efficiently compute (e) the
value function under a random policy and (f) the value function under the optimal policy.
9sets of tasks that share the same transition structure but vary in their reward structure; we
will delve more into this problem setting later when we discuss applications.
Like the value function, the SR is a cumulant—a quantity of interest summed (accumu-
lated) across time. The value function is a cumulant of discounted reward, whereas the SR
is a cumulant of discounted occupancy:
" H (cid:12) #
Mπ(s,s˜) = E X γt I[s t+1 = s˜](cid:12) (cid:12) (cid:12)s 0 = s (13)
(cid:12)
t=0
Here s˜denotes a future state, and Mπ(s,s˜) is the expected discounted future occupancy of s˜
starting in state s under policy π. An illustration of the SR and comparison with the value
function is shown in Figure 2.
The SR can be derived analytically from the transition function when H = ∞:
∞
Mπ = X γt[Tπ]t −I = (I−γTπ)−1 −I, (14)
t=0
where we have subtracted the identity matrix I because we are not counting the current
state, and the marginal transition matrix Tπ is given by:
Tπ(s,s′) = X π(a|s)T(s′|s,a). (15)
a
Eq. 14 makes explicit the sense in which the SR (a predictive representation) is a compilation
of the transition matrix (a predictive model). The SR discards information about individ-
ual transitions, replacing them with their cumulants, analogous to how the value function
replaces individual reward sequences with their cumulants.
Like the value function, the SR obeys a Bellman equation:
Mπ(s,s˜) = X π(a|s)X T(s′|s,a)h I[s′ = s˜]+γMπ(s′,s˜)i
a s′
" #
= E I[s′ = s˜]+γMπ(s′,s˜) . (16)
This means that it is possible to learn the SR using TD updates similar to the ones applied
to value learning:
ˆ ˆ
Mπ(s,s˜) ← Mπ(s,s˜)+ηδ (s˜), (17)
M
where
ˆ ˆ
δ M(s˜) = I[s′ = s˜]+γMπ(s′,s˜)−Mπ(s,s˜) (18)
is the TD error. Notice that, unlike in TD learning for value, the error is now vector-valued
(one error for each state). Once the SR is learned, the value function for a particular reward
function under the policy π can be efficiently computed as a linear function of the SR:
Vπ(s) = X Mπ(s,s˜)R(s˜) (19)
s˜
10Intuitively, Eq. 19 expresses a decomposition of future reward into immediate reward in each
state and the frequency with which those states are visited in the near future.
Just as one can condition a value function on actions to obtain a state-action value
function (Eq. 3), we can condition the SR on actions as well5:
" H (cid:12) #
Mπ(s,a,s˜) = E X γt I[s t+1 = s˜](cid:12) (cid:12) (cid:12)s 0 = s,A 0 = a (20)
(cid:12)
t=0
" #
= E I[s′ = s˜]+γMπ(s′,a′,s˜) . (21)
Given an action-conditioned SR, the action value function for a particular reward function
can be computed as a linear function of the SR with
Qπ(s,a) = X Mπ(s,a,s˜)R(s˜). (22)
s˜
Having established some mathematical properties of the SR, we can now explain why it is
useful. First, the SR, unlike model-based algorithms, obviates the need to simulate roll-outs
or iterate over dynamic programming updates, because it has already compiled transition
information into a convenient form: state values can be computed by simply taking the
inner product between the SR and the immediate reward vector. Thus, SR-based value
computation enjoys efficiency comparable to model-free algorithms.
Second, the SR can, like model-based algorithms, adapt quickly to certain kinds of envi-
ronmental changes. In particular, local changes to an environment’s reward structure induce
local changes in the reward function, which immediately propagate to the value estimates
when combined with the SR.6 Thus, SR-based value computation enjoys flexibility compara-
ble to model-based algorithms, at least for changes to the reward structure. Changes to the
transition structure, on the other hand, require more substantial non-local changes to the
SR due to the fact that an internal model of the detailed transition structure is not available.
Our discussion has already indicated several limitations of the SR. First, the policy-
dependence of its predictions limits its generalization ability. Second, the SR assumes a
finite, discrete state space. Third, it does not generalize to new environment dynamics.
When the transition structure changes, the Eq. 14 no longer holds. We will discuss how the
first and second challenges can be addressed in §3 and some attempts to address the third
challenge in §4.2.3.
2.4 Successor models: a probabilistic perspective on the SR
As we’ve discussed, the SR buys efficiency by caching transition structure, while maintaining
some model-based flexibility. One thing that is lost, however, is the ability to simulate
5Note that we overload Mπ to also accept actions to reduce the amount of new notation. In general,
Mπ(s,s˜)=P π(a|s)Mπ(s,a,s˜).
a
6The attentive reader will note that, at least initially, these are not exactly the correct value estimates,
because the SR is policy-dependent, and the policy itself requires updating, which may not happen instan-
taneously (depending on how the agent is optimizing its policy). Nonetheless, these value estimates will
typically be an improvement—a good first guess. As we will see, human learning exhibits similar behavior.
11Single-step model Successor model
T<latexit sha1_base64="iMJud+d9EyNfHNqLxni9MLR6T7c=">AAAB8XicbVDLSgNBEOyNrxhfUY9ehgQxooRdD9Fj0IvHCHlhsoTZyWwyODu7zMwKy5q/yMWDIl79G2/5GyePgyYWNBRV3XR3eRFnStv2xMqsrW9sbmW3czu7e/sH+cOjpgpjSWiDhDyUbQ8rypmgDc00p+1IUhx4nLa8p7up33qmUrFQ1HUSUTfAA8F8RrA20mO9pM5e1CXC57180S7bM6BV4ixIsVroXown1aTWy393+yGJAyo04VipjmNH2k2x1IxwOsp1Y0UjTJ7wgHYMFTigyk1nF4/QqVH6yA+lKaHRTP09keJAqSTwTGeA9VAte1PxP68Ta//GTZmIYk0FmS/yY450iKbvoz6TlGieGIKJZOZWRIZYYqJNSDkTgrP88ippXpWdSrnyYNK4hTmycAIFKIED11CFe6hBAwgIGMMbvFvKerU+rM95a8ZazBzDH1hfP/y3ks8=</latexit> (s s,a) µ<latexit sha1_base64="GorRaJlrnpaPANY/DKowrKgW4SI=">AAACDnicbVC7TsMwFHXKq5RXgJElalWpCFQlDIWxgoWxSPQhNaFyHKe16jiR7SBFIV/AwsCPsDCAECszW/8Gp+0ALUeyfHTOvbr3HjeiREjTnGiFldW19Y3iZmlre2d3T98/6Igw5gi3UUhD3nOhwJQw3JZEUtyLOIaBS3HXHV/lfvcec0FCdiuTCDsBHDLiEwSlkgZ61XZD6okkUF9qB3F2l9oRyWq2JNTDqcgexCk8HugVs25OYSwTa04qzbJ98jxpJq2B/m17IYoDzCSiUIi+ZUbSSSGXBFGclexY4AiiMRzivqIMBlg46fSczKgqxTP8kKvHpDFVf3ekMBD5xqoygHIkFr1c/M/rx9K/cFLColhihmaD/JgaMjTybAyPcIwkTRSBiBO1q4FGkEMkVYIlFYK1ePIy6ZzVrUa9caPSuAQzFMERKIMasMA5aIJr0AJtgMAjeAFv4F170l61D+1zVlrQ5j2H4A+0rx/HGaAt</latexit> ⇡(s˜s,a)
0
| |
Current state Current state
Figure 3: The Successor Model. A cartoon schematic of a robot leg that can hop
forward. Left: with a single-step model, we can only compute likelihoods for states at the
next time-step. Right: with successor models, we can compute likelihoods for states over
some horizon into the future. Adapted with permission from (Janner et al., 2020).
trajectories through the state space. In this section, we introduce a generalization of the
SR—the successor model (SM; Janner et al., 2020; Eysenbach et al., 2020)—that defines an
explicit model over temporally abstract trajectories (Figure 3). Temporal abstraction here
means a conditional distribution over future states within some time horizon, rather than
only the next time-step captured by the transition model.
The SM uses a k-step conditional distribution (i.e., the distribution over state occupancy
after k steps when starting in a particular state) as its cumulant:
∞
µπ(s˜|s) = (1−γ)X γt P(s
k
= s˜|s
0
= s,π,T). (23)
k=1
where γ determines the horizon of the prediction and (1−γ) ensures that µπ integrates to
1.The SM is essentially a normalized SR, since
µπ(s˜|s) = (1−γ)Mπ(s,s˜). (24)
This relationship becomes apparent when we note that the expectation of an indicator func-
tion is the likelihood of the event, i.e. E[ I[X = x]] = P(X = x).
SincetheSMintegratesto1, akeydifferencetotheSRisthatitdefinesavalidprobability
distribution. As we will discuss in §3.2, this allows us to estimate it with density estimation
techniques such as generative adversarial learning Janner et al. (2020), variational infer-
ence (Thakoor et al., 2022), and contrastive learning (Eysenbach et al., 2020; Zheng et al.,
2023).
Intuitively, the SM describes a probabilistic discounted occupancy measure. SMs are
interesting because they are a different kind of environment model. Rather than defining
transition probabilities over next states, they describe the probability of reaching s˜within a
horizon determined by γ when following policy π. While we don’t know exactly when s˜will
12be reached, we can answer queries about whether it will be reached within some relatively
long time horizon with less computation compared to rolling out the base transition model.
Additionally, depending on how the SM is learned, we can also sample from it. This can be
useful for policy evaluation (Thakoor et al., 2022) and model-based control (Janner et al.,
2020). We will return to this topic when we consider applications.
Like the original SR, the SM obeys a Bellman-like recursion:
µπ(s˜|s) = E[(1−γ)T(s˜|s,a)+γµπ(s˜|s′)], (25)
where the next-state probability s˜ in the first term resembles one-step reward in Eq. 7 and
the second term resembles the expected value function at the next time-step. As with Eq. 19,
we can use the SM to perform policy evaluation by computing:
1
Vπ(s) = 1−γEs˜∼µπ(·|s)[R(s˜)]. (26)
Additionally, we can introduce an action-conditioned variant of the SM
∞
µπ(s˜|s,a) = (1−γ)X γt P(s
t+1
= s˜|s
0
= s,a
0
= a,π,T)
t=0
= (1−γ)T(s˜|s,a)+γE[µπ(s˜|s,a)]. (27)
We can leverage this to compute an action value:
1
Qπ(s,a) = 1−γEs˜∼µπ(·|s,a)[R(s˜)]. (28)
2.5 Successor features: a feature-based generalization of the SR
When the state space is large or unknown, the SR can be challenging to compute or learn
becauseitrequiresmaintainingoccupancyexpectationsoverallstates. Thiscanalsobechal-
lenging when dealing with learned state representations, as is common in practical learning
algorithms (see §3.1). In these settings, rather than maintain occupancy measure for states,
we can maintain occupancy measure over cumulants that are features shared across states,
φ(s). This generalization of the SR is known as successor features (Barreto et al., 2017).
The power of this representation is particularly apparent when reward can be decomposed
into a dot-product of these successor features and a vector w describing feature “preferences”
for the current task:
R(s,w) = φ(s)⊤w. (29)
Successor features are then predictions of how much of features φ the agent can expect to
obtain when following a policy π:
"∞ #
ψπ(s) = E X γtφ(s t+1) | s 0 = s (30)
t=0
13(a) Successor Features (b) Generalized Policy Improvement
<latexit sha1_base64="Waw5YcCTllGahBnnmSg7wjYGcIk=">AAACUXicbVFBaxQxFH47am23Wtd67CW4CIKwzGylehFKRfBYwW0Lm+mQyWR2wiaTIXkjLMP8RQ968n/00kOLmdlFtPVBeN/7vvdI3pe0UtJhGP4aBA8ePtp6vL0z3H3ydO/Z6Pn+mTO15WLGjTL2ImVOKFmKGUpU4qKygulUifN0+bHTz78J66Qpv+KqErFmi1LmkjP0VDIqaGpU5lbap4ZWTraXPsmWfCBUMyzStPnUJj1FlchxTqtCJhF5Q+iCac1IX0//1JfTNXPYMSoz6KiViwLjZDQOJ2Ef5D6INmAMmzhNRj9oZnitRYlcMefmUVhh3DCLkivRDmntRMX4ki3E3MOSaeHipnekJa88k5HcWH9KJD3790TDtOuW9p3dlu6u1pH/0+Y15u/jRpZVjaLk64vyWhE0pLOXZNIKjmrlAeNW+rcSXjDLOPpPGHoTorsr3wdn00l0NDn68nZ8fLKxYxsO4CW8hgjewTF8hlOYAYfvcAU3cDv4ObgOIAjWrcFgM/MC/olg9zfKb7Or</latexit> ⇡ = E⇡   1 +   2 + 2  3 +... ⇡<latexit sha1_base64="yWN7FMIcSQGJYSArI0BBEESubl0=">AAACRnicbVBNT9wwEJ0s/aDbry099mJ1VWmRqlWCKsoRwaXHrdRdkDZp5HgdsHBsy55AV8G/jgtnbv0JvXCgqnqts+TQQkey/PzePHnmFUYKh3H8PeqtPXj46PH6k/7TZ89fvBy82pg5XVvGp0xLbQ8L6rgUik9RoOSHxnJaFZIfFCf7rX5wyq0TWn3BpeFZRY+UKAWjGKh8kKVGjOi52ySpsdqgJmlFv+VNoHPh0yYttFy4ZRWuwDnhv3bSyL2nm+GB2njSBBMeF2Vz5oMVkSh+5n3q88EwHserIvdB0oEhdDXJB1fpQrO64gqZpM7Nk9hg1lCLgknu+2ntuKHshB7xeYCKVtxlzSoGT94FZkFKbcNRSFbs346GVq5dJXS247q7Wkv+T5vXWO5kjVCmRq7Y7UdlLUlIq82ULITlDOUyAMqsCLMSdkwtZRiS74cQkrsr3wezrXGyPd7+/GG4u9fFsQ5v4C2MIIGPsAufYAJTYHABP+AGfkaX0XX0K/p929qLOs9r+Kd68Aef/rRL</latexit> (a s) max ⇡ i(s,a) >w new
| / ⇡ { }
i
⇥ ⇤
Open Fridge New task: Get milk
<latexit sha1_base64="R9osOw/dYDAjZREfWK3W+rGCyaM=">AAACAHicbVDLSsNAFJ3UV62vqAsXbgaL4KokUtRl0Y3LCvYBTQyTyaQdOnkwcyOUkI2/4saFIm79DHf+jdM2C209cOFwzr3ce4+fCq7Asr6Nysrq2vpGdbO2tb2zu2fuH3RVkknKOjQRiez7RDHBY9YBDoL1U8lI5AvW88c3U7/3yKTiSXwPk5S5ERnGPOSUgJY888hJFX/InZR7uQOAQ8mDISsKz6xbDWsGvEzsktRRibZnfjlBQrOIxUAFUWpgWym4OZHAqWBFzckUSwkdkyEbaBqTiCk3nz1Q4FOtBDhMpK4Y8Ez9PZGTSKlJ5OvOiMBILXpT8T9vkEF45eY8TjNgMZ0vCjOBIcHTNHDAJaMgJpoQKrm+FdMRkYSCzqymQ7AXX14m3fOGfdFo3jXrresyjio6RifoDNnoErXQLWqjDqKoQM/oFb0ZT8aL8W58zFsrRjlziP7A+PwBdM6W+Q==</latexit>
⇡
fridge a ⇡
<latexit sha1_base64="93wdZ10L3t3S7aEV393Ipgnd7W8=">AAACBHicbVDLSsNAFJ3UV62vqMtuBovgqiTia1l047KCfUATwmQyaYdOHszcCCVk4cZfceNCEbd+hDv/xmmbhbYeuHA4517uvcdPBVdgWd9GZWV1bX2julnb2t7Z3TP3D7oqySRlHZqIRPZ9opjgMesAB8H6qWQk8gXr+eObqd97YFLxJL6HScrciAxjHnJKQEueWSdeDgV2FI+wk3IvdwBwKHkwZIVnNqymNQNeJnZJGqhE2zO/nCChWcRioIIoNbCtFNycSOBUsKLmZIqlhI7JkA00jUnElJvPnijwsVYCHCZSVwx4pv6eyEmk1CTydWdEYKQWvan4nzfIILxycx6nGbCYzheFmcCQ4GkiOOCSURATTQiVXN+K6YhIQkHnVtMh2IsvL5PuadO+aJ7fnTVa12UcVVRHR+gE2egStdAtaqMOougRPaNX9GY8GS/Gu/Exb60Y5cwh+gPj8wfKlpg1</latexit> t fridge
⇠
. . . <latexit sha1_base64="R9osOw/dYDAjZREfWK3W+rGCyaM=">AAACAHicbVDLSsNAFJ3UV62vqAsXbgaL4KokUtRl0Y3LCvYBTQyTyaQdOnkwcyOUkI2/4saFIm79DHf+jdM2C209cOFwzr3ce4+fCq7Asr6Nysrq2vpGdbO2tb2zu2fuH3RVkknKOjQRiez7RDHBY9YBDoL1U8lI5AvW88c3U7/3yKTiSXwPk5S5ERnGPOSUgJY888hJFX/InZR7uQOAQ8mDISsKz6xbDWsGvEzsktRRibZnfjlBQrOIxUAFUWpgWym4OZHAqWBFzckUSwkdkyEbaBqTiCk3nz1Q4FOtBDhMpK4Y8Ez9PZGTSKlJ5OvOiMBILXpT8T9vkEF45eY8TjNgMZ0vCjOBIcHTNHDAJaMgJpoQKrm+FdMRkYSCzqymQ7AXX14m3fOGfdFo3jXrresyjio6RifoDNnoErXQLWqjDqKoQM/oFb0ZT8aL8W58zFsrRjlziP7A+PwBdM6W+Q==</latexit> ⇡fridge Q<latexit sha1_base64="q0/rPJF2cF/fpjDRGwxwnNC+7Cg=">AAACDHicbVDLSsNAFJ3UV62vqEs3g0FwVRIp6rLoxmUL9gFtDJPppB06eTBzo5SQD3Djr7hxoYhbP8Cdf+O0zUJbD1w4c869zL3HTwRXYNvfRmlldW19o7xZ2dre2d0z9w/aKk4lZS0ai1h2faKY4BFrAQfBuolkJPQF6/jj66nfuWdS8Ti6hUnC3JAMIx5wSkBLnmk177J+wr2sD4ADyQdDlude9jAXQi7G+mladtWeAS8TpyAWKtDwzK/+IKZpyCKggijVc+wE3IxI4FSwvNJPFUsIHZMh62kakZApN5sdk+MTrQxwEEtdEeCZ+nsiI6FSk9DXnSGBkVr0puJ/Xi+F4NLNeJSkwCI6/yhIBYYYT5PBAy4ZBTHRhFDJ9a6YjogkFHR+FR2Cs3jyMmmfVZ3zaq1Zs+pXRRxldISO0Sly0AWqoxvUQC1E0SN6Rq/ozXgyXox342PeWjKKmUP0B8bnDx3AnFA=</latexit> ⇡fridge
wmilk
=
Legend <latexit sha1_base64="kQkDxPnVv0Mgc/tsfqEuJYOLy9w=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69BIvgqSRS1GPRi8cK9gPaUDabTbt2sxt2J0Ip/Q9ePCji1f/jzX/jts1BWx8MPN6bYWZemApu0PO+ncLa+sbmVnG7tLO7t39QPjxqGZVpyppUCaU7ITFMcMmayFGwTqoZSULB2uHodua3n5g2XMkHHKcsSMhA8phTglZq9USk0PTLFa/qzeGuEj8nFcjR6Je/epGiWcIkUkGM6fpeisGEaORUsGmplxmWEjoiA9a1VJKEmWAyv3bqnlklcmOlbUl05+rviQlJjBknoe1MCA7NsjcT//O6GcbXwYTLNEMm6WJRnAkXlTt73Y24ZhTF2BJCNbe3unRINKFoAyrZEPzll1dJ66LqX1Zr97VK/SaPowgncArn4MMV1OEOGtAECo/wDK/w5ijnxXl3PhatBSefOYY/cD5/AL8Nj0E=</latexit> <latexit sha1_base64="2M9Uun3jhzFBOkbVOCuyBd1NlPo=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6rHoxWMF0xbaUDabTbt0sxt2J0Ip/Q1ePCji1R/kzX/jts1BWx8MPN6bYWZelAlu0PO+ndLa+sbmVnm7srO7t39QPTxqGZVrygKqhNKdiBgmuGQBchSsk2lG0kiwdjS6m/ntJ6YNV/IRxxkLUzKQPOGUoJWCnooV9qs1r+7N4a4SvyA1KNDsV796saJ5yiRSQYzp+l6G4YRo5FSwaaWXG5YROiID1rVUkpSZcDI/duqeWSV2E6VtSXTn6u+JCUmNGaeR7UwJDs2yNxP/87o5JjfhhMssRybpYlGSCxeVO/vcjblmFMXYEkI1t7e6dEg0oWjzqdgQ/OWXV0nrou5f1S8fLmuN2yKOMpzAKZyDD9fQgHtoQgAUODzDK7w50nlx3p2PRWvJKWaO4Q+czx/uTI7H</latexit> <latexit sha1_base64="8LVrqJucQl9xNwsjgr7UF8xAfX0=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKUC9C0IvHBMwDkiXMTnqTMbOzy8ysEEK+wIsHRbz6Sd78GyfJHjSxoKGo6qa7K0gE18Z1v53c2vrG5lZ+u7Czu7d/UDw8auo4VQwbLBaxagdUo+ASG4Ybge1EIY0Cga1gdDfzW0+oNI/lgxkn6Ed0IHnIGTVWqt/0iiW37M5BVomXkRJkqPWKX91+zNIIpWGCat3x3MT4E6oMZwKnhW6qMaFsRAfYsVTSCLU/mR86JWdW6ZMwVrakIXP198SERlqPo8B2RtQM9bI3E//zOqkJr/0Jl0lqULLFojAVxMRk9jXpc4XMiLEllClubyVsSBVlxmZTsCF4yy+vkuZF2bssV+qVUvU2iyMPJ3AK5+DBFVThHmrQAAYIz/AKb86j8+K8Ox+L1pyTzRzDHzifP4+7jMo=</latexit>
 
w
 <latexit sha1_base64="hpnySaJh+/CAhHti5mNvHw101yA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI9FLx4r2A9oQ9lsN83S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJV8epIrRDYh6rfoA15UzSjmGG036iKBYBp71gepf7vSeqNIvlo5kl1Bd4IlnICDa5NEwiNqrW3Lq7AFonXkFqUKA9qn4NxzFJBZWGcKz1wHMT42dYGUY4nVeGqaYJJlM8oQNLJRZU+9ni1jm6sMoYhbGyJQ1aqL8nMiy0nonAdgpsIr3q5eJ/3iA14Y2fMZmkhkqyXBSmHJkY5Y+jMVOUGD6zBBPF7K2IRFhhYmw8FRuCt/ryOule1b1mvfHQqLVuizjKcAbncAkeXEML7qENHSAQwTO8wpsjnBfn3flYtpacYuYU/sD5/AEWdo5I</latexit> <latexit sha1_base64="dwqnoHlLqdsg8BuVgD6mRKkkLpQ=">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Ac0oWy223bpbhJ2J0oJ/RtePCji1T/jzX/jts1BWx8MPN6bYWZemEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCanhUkS8iQIl7ySaUxVK3g7HtzO//ci1EXH0gJOEB4oOIzEQjKKV/Kde5iMSJeR42itX3Ko7B1klXk4qkKPRK3/5/ZilikfIJDWm67kJBhnVKJjk05KfGp5QNqZD3rU0ooqbIJvfPCVnVumTQaxtRUjm6u+JjCpjJiq0nYriyCx7M/E/r5vi4DrIRJSkyCO2WDRIJcGYzAIgfaE5QzmxhDIt7K2EjaimDG1MJRuCt/zyKmldVL3Lau2+Vqnf5HEU4QRO4Rw8uII63EEDmsAggWd4hTcndV6cd+dj0Vpw8plj+APn8wdGwJHa</latexit> milk
State features
.
Apple .
Open Drawer
.<latexit sha1_base64="qDf6pzroMMJvMcIkHWdR/tYPv+o=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69BIvgqSRS1GPRi8cKthbaUDabTbt2sxt2J4VS+h+8eFDEq//Hm//GbZuDtj4YeLw3w8y8MBXcoOd9O4W19Y3NreJ2aWd3b/+gfHjUMirTlDWpEkq3Q2KY4JI1kaNg7VQzkoSCPYbD25n/OGLacCUfcJyyICF9yWNOCVqp1R1FCk2vXPGq3hzuKvFzUoEcjV75qxspmiVMIhXEmI7vpRhMiEZOBZuWuplhKaFD0mcdSyVJmAkm82un7plVIjdW2pZEd67+npiQxJhxEtrOhODALHsz8T+vk2F8HUy4TDNkki4WxZlwUbmz192Ia0ZRjC0hVHN7q0sHRBOKNqCSDcFffnmVtC6q/mW1dl+r1G/yOIpwAqdwDj5cQR3uoAFNoPAEz/AKb45yXpx352PRWnDymWP4A+fzB85dj0s=</latexit>
Max
⇡
<latexit sha1_base64="uAph9Fync0syabW4IbKWtkcTpLo=">AAAB9XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cK9gPaWDbbTbt0Nwm7E6WE/g8vHhTx6n/x5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsslrFuB9RwKSLeQIGStxPNqQokbwWjm6nfeuTaiDi6x3HCfUUHkQgFo2ilh24ielkXkSghR5NeueJW3RnIMvFyUoEc9V75q9uPWap4hExSYzqem6CfUY2CST4pdVPDE8pGdMA7lkZUceNns6sn5MQqfRLG2laEZKb+nsioMmasAtupKA7NojcV//M6KYZXfiaiJEUesfmiMJUEYzKNgPSF5gzl2BLKtLC3EjakmjK0QZVsCN7iy8ukeVb1Lqrnd+eV2nUeRxGO4BhOwYNLqMEt1KEBDDQ8wyu8OU/Oi/PufMxbC04+cwh/4Hz+ALoPkqw=</latexit> milk
Milk <latexit sha1_base64="YoX6KhxbTerZB8O3mp8Q8Gf5Oeg=">AAACAHicbVDLSsNAFJ34rPUVdeHCzWARXJVEirosunFZwT6giWEymbRDJw9mbpQSsvFX3LhQxK2f4c6/cdpmoa0HLhzOuZd77/FTwRVY1rextLyyurZe2ahubm3v7Jp7+x2VZJKyNk1EIns+UUzwmLWBg2C9VDIS+YJ1/dH1xO8+MKl4Et/BOGVuRAYxDzkloCXPPHRSxe9zJ+Ve7gDgQJJHJovCM2tW3ZoCLxK7JDVUouWZX06Q0CxiMVBBlOrbVgpuTiRwKlhRdTLFUkJHZMD6msYkYsrNpw8U+EQrAQ4TqSsGPFV/T+QkUmoc+bozIjBU895E/M/rZxBeujmP0wxYTGeLwkxgSPAkDRxwySiIsSaESq5vxXRIJKGgM6vqEOz5lxdJ56xun9cbt41a86qMo4KO0DE6RTa6QE10g1qojSgq0DN6RW/Gk/FivBsfs9Ylo5w5QH9gfP4Ak0iXDQ==</latexit>
⇡
drawer a ⇡
<latexit sha1_base64="aaob0i0A55J+HffMxei18XxR810=">AAACBHicbVDLSsNAFJ34rPUVddnNYBFclUR8LYtuXFawD2hCmEwm7dDJg5kbpYQs3Pgrblwo4taPcOffOG2z0NYDFw7n3Mu99/ip4Aos69tYWl5ZXVuvbFQ3t7Z3ds29/Y5KMklZmyYikT2fKCZ4zNrAQbBeKhmJfMG6/uh64nfvmVQ8ie9gnDI3IoOYh5wS0JJn1oiXQ4EdxSPspNzLHQAcSPLAZOGZdathTYEXiV2SOirR8swvJ0hoFrEYqCBK9W0rBTcnEjgVrKg6mWIpoSMyYH1NYxIx5ebTJwp8pJUAh4nUFQOeqr8nchIpNY583RkRGKp5byL+5/UzCC/dnMdpBiyms0VhJjAkeJIIDrhkFMRYE0Il17diOiSSUNC5VXUI9vzLi6Rz0rDPG2e3p/XmVRlHBdXQITpGNrpATXSDWqiNKHpEz+gVvRlPxovxbnzMWpeMcuYA/YHx+QPo/JhJ</latexit> t drawer
⇠
Fork <latexit sha1_base64="YoX6KhxbTerZB8O3mp8Q8Gf5Oeg=">AAACAHicbVDLSsNAFJ34rPUVdeHCzWARXJVEirosunFZwT6giWEymbRDJw9mbpQSsvFX3LhQxK2f4c6/cdpmoa0HLhzOuZd77/FTwRVY1rextLyyurZe2ahubm3v7Jp7+x2VZJKyNk1EIns+UUzwmLWBg2C9VDIS+YJ1/dH1xO8+MKl4Et/BOGVuRAYxDzkloCXPPHRSxe9zJ+Ve7gDgQJJHJovCM2tW3ZoCLxK7JDVUouWZX06Q0CxiMVBBlOrbVgpuTiRwKlhRdTLFUkJHZMD6msYkYsrNpw8U+EQrAQ4TqSsGPFV/T+QkUmoc+bozIjBU895E/M/rZxBeujmP0wxYTGeLwkxgSPAkDRxwySiIsSaESq5vxXRIJKGgM6vqEOz5lxdJ56xun9cbt41a86qMo4KO0DE6RTa6QE10g1qojSgq0DN6RW/Gk/FivBsfs9Ylo5w5QH9gfP4Ak0iXDQ==</latexit> ⇡drawer
. . .
Knife
=
<latexit sha1_base64="kQkDxPnVv0Mgc/tsfqEuJYOLy9w=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69BIvgqSRS1GPRi8cK9gPaUDabTbt2sxt2J0Ip/Q9ePCji1f/jzX/jts1BWx8MPN6bYWZemApu0PO+ncLa+sbmVnG7tLO7t39QPjxqGZVpyppUCaU7ITFMcMmayFGwTqoZSULB2uHodua3n5g2XMkHHKcsSMhA8phTglZq9USk0PTLFa/qzeGuEj8nFcjR6Je/epGiWcIkUkGM6fpeisGEaORUsGmplxmWEjoiA9a1VJKEmWAyv3bqnlklcmOlbUl05+rviQlJjBknoe1MCA7NsjcT//O6GcbXwYTLNEMm6WJRnAkXlTt73Y24ZhTF2BJCNbe3unRINKFoAyrZEPzll1dJ66LqX1Zr97VK/SaPowgncArn4MMV1OEOGtAECo/wDK/w5ijnxXl3PhatBSefOYY/cD5/AL8Nj0E=</latexit> <latexit sha1_base64="2M9Uun3jhzFBOkbVOCuyBd1NlPo=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6rHoxWMF0xbaUDabTbt0sxt2J0Ip/Q1ePCji1R/kzX/jts1BWx8MPN6bYWZelAlu0PO+ndLa+sbmVnm7srO7t39QPTxqGZVrygKqhNKdiBgmuGQBchSsk2lG0kiwdjS6m/ntJ6YNV/IRxxkLUzKQPOGUoJWCnooV9qs1r+7N4a4SvyA1KNDsV796saJ5yiRSQYzp+l6G4YRo5FSwaaWXG5YROiID1rVUkpSZcDI/duqeWSV2E6VtSXTn6u+JCUmNGaeR7UwJDs2yNxP/87o5JjfhhMssRybpYlGSCxeVO/vcjblmFMXYEkI1t7e6dEg0oWjzqdgQ/OWXV0nrou5f1S8fLmuN2yKOMpzAKZyDD9fQgHtoQgAUODzDK7w50nlx3p2PRWvJKWaO4Q+czx/uTI7H</latexit> <latexit sha1_base64="8LVrqJucQl9xNwsjgr7UF8xAfX0=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKexKUC9C0IvHBMwDkiXMTnqTMbOzy8ysEEK+wIsHRbz6Sd78GyfJHjSxoKGo6qa7K0gE18Z1v53c2vrG5lZ+u7Czu7d/UDw8auo4VQwbLBaxagdUo+ASG4Ybge1EIY0Cga1gdDfzW0+oNI/lgxkn6Ed0IHnIGTVWqt/0iiW37M5BVomXkRJkqPWKX91+zNIIpWGCat3x3MT4E6oMZwKnhW6qMaFsRAfYsVTSCLU/mR86JWdW6ZMwVrakIXP198SERlqPo8B2RtQM9bI3E//zOqkJr/0Jl0lqULLFojAVxMRk9jXpc4XMiLEllClubyVsSBVlxmZTsCF4yy+vkuZF2bssV+qVUvU2iyMPJ3AK5+DBFVThHmrQAAYIz/AKb86j8+K8Ox+L1pyTzRzDHzifP4+7jMo=</latexit>
 
Task w <latexit sha1_base64="G4NrL4+FchmdNAyoq1HYtt9oFXI=">AAAB6HicbVDLTgJBEOzFF+IL9ehlIjHxRHYNUY9ELx4hkUcCGzI79MLI7OxmZlZDCF/gxYPGePWTvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLR7cxvPaLSPJb3ZpygH9GB5CFn1Fip/tQrltyyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmhdl77JcqVdK1ZssjjycwCmcgwdXUIU7qEEDGCA8wyu8OQ/Oi/PufCxac042cwx/4Hz+AOejjQQ=</latexit> Q<latexit sha1_base64="AyR5teNAat/Susz4Jr45Wl5qJQ8=">AAACDHicbVDLSsNAFJ3UV62vqEs3g0FwVRIRdVl047IF+4Cmhslk0g6dPJi5sZTQD3Djr7hxoYhbP8Cdf+O0zUJbD1w4c869zL3HTwVXYNvfRmlldW19o7xZ2dre2d0z9w9aKskkZU2aiER2fKKY4DFrAgfBOqlkJPIFa/vDm6nffmBS8SS+g3HKehHpxzzklICWPNNq3Oduyr3cBcCBJCMmJxMvH82FiIuhfpqWXbVnwMvEKYiFCtQ988sNEppFLAYqiFJdx06hlxMJnAo2qbiZYimhQ9JnXU1jEjHVy2fHTPCJVgIcJlJXDHim/p7ISaTUOPJ1Z0RgoBa9qfif180gvOrlPE4zYDGdfxRmAkOCp8nggEtGQYw1IVRyvSumAyIJBZ1fRYfgLJ68TFpnVeeiet44t2rXRRxldISO0Sly0CWqoVtUR01E0SN6Rq/ozXgyXox342PeWjKKmUP0B8bnDz1mnGQ=</latexit> ⇡drawer
w
<latexit sha1_base64="dwqnoHlLqdsg8BuVgD6mRKkkLpQ=">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Ac0oWy223bpbhJ2J0oJ/RtePCji1T/jzX/jts1BWx8MPN6bYWZemEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCanhUkS8iQIl7ySaUxVK3g7HtzO//ci1EXH0gJOEB4oOIzEQjKKV/Kde5iMSJeR42itX3Ko7B1klXk4qkKPRK3/5/ZilikfIJDWm67kJBhnVKJjk05KfGp5QNqZD3rU0ooqbIJvfPCVnVumTQaxtRUjm6u+JjCpjJiq0nYriyCx7M/E/r5vi4DrIRJSkyCO2WDRIJcGYzAIgfaE5QzmxhDIt7K2EjaimDG1MJRuCt/zyKmldVL3Lau2+Vqnf5HEU4QRO4Rw8uII63EEDmsAggWd4hTcndV6cd+dj0Vpw8plj+APn8wdGwJHa</latexit> milk
wmilk
Task Policies
⇡<latexit sha1_base64="Kg2YPF3194EvR9fJmGppeZNxF/U=">AAACDnicbVA9SwNBEN3z2/gVtbRZDAGrcCeiNkJQCzsVzAfkjrC3mSSLe3vH7pwajvwCG/+KjYUittZ2/hs3yRV+PRh4vDfDzLwwkcKg6346U9Mzs3PzC4uFpeWV1bXi+kbdxKnmUOOxjHUzZAakUFBDgRKaiQYWhRIa4fXJyG/cgDYiVlc4SCCIWE+JruAMrdQulv1EtDMfkXY0uwU9pEfUR7jD7DwBRU8nYrtYcivuGPQv8XJSIjku2sUPvxPzNAKFXDJjWp6bYJAxjYJLGBb81EDC+DXrQctSxSIwQTZ+Z0jLVunQbqxtKaRj9ftExiJjBlFoOyOGffPbG4n/ea0Uu4dBJlSSIig+WdRNJcWYjrKhHaGBoxxYwrgW9lbK+0wzjjbBgg3B+/3yX1LfrXj7lb3LvVL1OI9jgWyRbbJDPHJAquSMXJAa4eSePJJn8uI8OE/Oq/M2aZ1y8plN8gPO+xdNP5xI</latexit> drawer=OpenDrawer  <latexit sha1_base64="atwBTWaewmruY30kSfX/vOndlS0=">AAAB7XicbVBNSwMxEJ3Ur1q/qh69BIvgqexKUY9FLx4r2A9ol5JNs21sNlmSrFCW/gcvHhTx6v/x5r8xbfegrQ8GHu/NMDMvTAQ31vO+UWFtfWNzq7hd2tnd2z8oHx61jEo1ZU2qhNKdkBgmuGRNy61gnUQzEoeCtcPx7cxvPzFtuJIPdpKwICZDySNOiXVSq5eMeN/vlyte1ZsDrxI/JxXI0eiXv3oDRdOYSUsFMabre4kNMqItp4JNS73UsITQMRmyrqOSxMwE2fzaKT5zygBHSruSFs/V3xMZiY2ZxKHrjIkdmWVvJv7ndVMbXQcZl0lqmaSLRVEqsFV49joecM2oFRNHCNXc3YrpiGhCrQuo5ELwl19eJa2Lqn9Zrd3XKvWbPI4inMApnIMPV1CHO2hAEyg8wjO8whtS6AW9o49FawHlM8fwB+jzBz40juw=</latexit> 1  <latexit sha1_base64="qQ8W6PPGg+MLPiwWXFeQFQUGATw=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8cK/YI2lM120i7dbOLuRiihf8KLB0W8+ne8+W/ctjlo64OBx3szzMwLEsG1cd1vp7C2vrG5Vdwu7ezu7R+UD49aOk4VwyaLRaw6AdUouMSm4UZgJ1FIo0BgOxjfzfz2EyrNY9kwkwT9iA4lDzmjxkqdXjLi/awx7ZcrbtWdg6wSLycVyFHvl796g5ilEUrDBNW667mJ8TOqDGcCp6VeqjGhbEyH2LVU0gi1n83vnZIzqwxIGCtb0pC5+nsio5HWkyiwnRE1I73szcT/vG5qwhs/4zJJDUq2WBSmgpiYzJ4nA66QGTGxhDLF7a2EjaiizNiISjYEb/nlVdK6qHpX1cuHy0rtNo+jCCdwCufgwTXU4B7q0AQGAp7hFd6cR+fFeXc+Fq0FJ585hj9wPn8AOl6QGw==</latexit> T
⇡<latexit sha1_base64="pPTfju0Z4qLrEuyMsfS7lSSD+dg=">AAACDnicbVDLSgNBEJyNrxhfUY9eBoPgKeyKqBchKIg3I5gHZEOYnfQmg7Ozy0yvGJZ8gRd/xYsHRbx69ubfOHkcNFrQUFR1090VJFIYdN0vJzc3v7C4lF8urKyurW8UN7fqJk41hxqPZaybATMghYIaCpTQTDSwKJDQCG7PR37jDrQRsbrBQQLtiPWUCAVnaKVOcc9PRCfzEWmoRbcHQ3pKfYR7zK4SUPRiInaKJbfsjkH/Em9KSmSKaqf46XdjnkagkEtmTMtzE2xnTKPgEoYFPzWQMH7LetCyVLEITDsbvzOke1bp0jDWthTSsfpzImORMYMosJ0Rw76Z9Ubif14rxfCknQmVpAiKTxaFqaQY01E2tCs0cJQDSxjXwt5KeZ9pxtEmWLAheLMv/yX1g7J3VD68PixVzqZx5MkO2SX7xCPHpEIuSZXUCCcP5Im8kFfn0Xl23pz3SWvOmc5sk19wPr4BDs+cIA==</latexit> fridge=OpenFridge t<latexit sha1_base64="4mSRiAOC1HPbUsbyd7QN48TyFAA=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsN+3azSbsToQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNnGqGW+xWMa6E1DDpVC8hQIl7ySa0yiQ/CEY3878hyeujYjVPU4S7kd0qEQoGEUrNbFfrrhVdw6ySrycVCBHo1/+6g1ilkZcIZPUmK7nJuhnVKNgkk9LvdTwhLIxHfKupYpG3PjZ/NApObPKgISxtqWQzNXfExmNjJlEge2MKI7MsjcT//O6KYbXfiZUkiJXbLEoTCXBmMy+JgOhOUM5sYQyLeythI2opgxtNiUbgrf88ippX1S9y2qtWavUb/I4inACp3AOHlxBHe6gAS1gwOEZXuHNeXRenHfnY9FacPKZY/gD5/MH4xeNAQ==</latexit>
State features agent experiences over time
Figure 4: A schematic of Successor features (SFs; §2.5) and Generalized policy
improvement (GPI; §2.5.1). Note that we use the shorthand φ = φ(s ) to represents
t t
“state-features” that describe what is visible to the agent at time t. π corresponds to policies
that the agent knows how to perform. (a) Examples of SFs (Eq. 30) for the “open drawer”
and “open fridge” policies. In this hypothetical scenario, the state-features that agent holds
describe whether an apple, milk, fork, or knife are present. Beginning for the first time-step,
the SFs for these behaviors encode predictions for which of these features will be present
when the policies are executed—predicted to be present for apple and milk for the open
fridge policy, and for the fork and knife when the open drawer policy is executed. (b) The
agent can re-use these known behaviors with GPI (Eq. 35). When given a new task, say
“get milk”, it is able to leverage the SFs for the policies to knows to decide which behavior
will enable it to get milk. In this example, the policy for opening the fridge will also lead to
milk. Concretely, The agent selects actions with GPI by computing Q-values for each known
behavior as the dot-product between the current task and each known SF. The highest Q-
value is then used to select actions. If the agent wants to execute the option keyboard (OK;
§2.5.2)), they can so by having w be adaptively set based on the current state. For example,
at some states the agent may want to pursue getting milk, while in others they may want to
pursue getting a fork. Adapted from (Carvalho et al., 2023b) with permission.
14and obey the following Bellman equation:
ψπ(s) = E[φ(s′)+γψ(s′)]. (31)
Under the assumption of Eq. 29, a task-dependent value Vπ(s,a,w) is equivalent to:
Qπ(s,a,w) = E[r(s 1)+γr(s 2)+... | s
0
= s,a
0
= a]
h i
= E φ(s 1)⊤w+γφ(s 2)⊤w+... | s 0 = s,a 0 = a
"∞ #⊤
= E X γtφ(s t+1) | s 0 = s,a 0 = a w
t=0
= ψπ(s)⊤w. (32)
As with the SR and SM, we can introduce an action-conditioned variant of SFs
"∞ #
ψπ(s,a) = E X γtφ(s t+1) | s 0 = s,a 0 = a
t=0
= E[φ(s′)+γψ(s′,a′)]. (33)
This provides an avenue to re-use these behavioral predictions for new tasks. Once ψπ(s,a)
hasbeenlearned,thebehaviorπ canbereusedtopursueanovelrewardfunctionR(s,w ) =
new
φ(s)⊤w by simply changing the corresponding task encoding w :
new new
Qπ(s,a,w ) = ψπ(s,a)⊤w (34)
new new
As we will see in the next section and is exploited heavily in AI applications (§4), this
becomes even more powerful when combined with generalized policy improvement.
2.5.1 Generalized policy improvement: adaptively combining behaviors
One limitation of Eq. 19 and Eq. 26 is that they only enable us to re-compute the value of
states for a new reward function under a known policy. However, we may want to synthesize
a new policy from the other policies we have learned so far. We can accomplish this with
SFs by combining them with Generalized Policy improvement (GPI; (Barreto et al., 2017)),
shown in Figure 4. Assume we have learned (potentially optimal) policies {π }ntrain and their
i i=1
corresponding SFs {ψπi(s,a)}ntrain for n training tasks {w }ntrain. When presented with
i=1 train i i=1
a new task w , we can obtain a new policy with GPI in two steps: (1) compute Q-values
new
using the training task SFs; (2) select actions using the highest Q-value. This operation is
summarized as follows:
n o
π(a|s,w ) ∝ max ψπi(s,a)⊤w
new new
i∈{1,...,ntrain}
= max {Qπi(s,a,w )} (35)
new
i∈{1,...,ntrain}
If w
new
is in the span of the training tasks (i.e., w
new
= P iα iw i, where α
i
∈ R), the GPI
theorem states that π will perform at least as well as any of the training policies—i.e.,
Qπ(s,a,w ) ≥ max Qπi(s,a,w )∀(s,a) ∈ (S ×A).
new i new
152.5.2 Option Keyboard: chaining together behaviors
One benefit of Eq. 35 is that it enables transfer to linear combinations of training task
encodings. However, it has two limitations. First, the task feature “preferences” w are
new
fixed across time. When w is a complex task (e.g., avoiding an object at some time-points
new
but going towards it at others), we may want preferences that are state-dependent. The
“Option Keyboard” (OK; (Barreto et al., 2019, 2020)) addresses this by having a state-
dependent preference vector w that is produced by a policy g(·) which takes in the current
s
state s and task w , i.e. w = g(s,w ). Given w , we can then select actions with GPI
new s new s
as follows:
n o
π (a|s,w ) ∝ max ψπi(s,a)⊤w (36)
ws new
i∈1,...,ntrain
s
Notethatthispolicyisadaptive—whichknownpolicyπ ischosenatatime-stepisdependent
i
on which one maximizes the successor features at that time-step. This is why it’s called the
option “keyboard”: w will induce a set of behaviors to be active for a period of time,
s
somewhat like chord on a piano.
2.6 Summary
The predictive representations introduced above can be concisely organized in terms of par-
ticular cumulants, as summarized in Table 1. These cumulants have different strengths and
weaknesses. Value functions (reward cumulants) directly represent the key quantity for RL
tasks, but they suffer from poor flexibility. The SR (state occupancy cumulant) and its vari-
ations (feature occupancy and state probability cumulants) can be used to compute values,
but also retain useful information for generalization to new tasks (e.g., using generalized
policy improvement).
Predictive representation Cumulant δ: TD update when a′ ∼ π and s′ ∼ T
Qπ(s,a) (§2.2) r(s) r(s′)+γQπ(s′,a′)−Qπ(s,a)
Mπ(s,s˜) (SR; §2.3) I(s = s˜) I(s′ = s˜)+γMπ(s′,s˜)−Mπ(s,s˜)
ψπ(s,a) (SFs; §2.5) φ(s) φ(s′)+γψπ(s′,a′)−ψπ(s,a)
µπ(s˜|s,a) (SM; §2.4) P(s
t+1
= s˜|s 0,a 0) (1−γ)T(s′|s,a)+γµπ(s˜|s′,a′)−µπ(s˜|s,a)
Table 1: Summary of predictive representations that we focus on. For each, we also
describe the “cumulant” that this predictive representation forms predictions over, along
with a corresponding on-policy Bellman update one can use to learn the representation for a
policy π. Qπ(s,a) is the action-value function which forms predictions about future reward.
Mπ(s,s˜) is the successor representation (SR) which forms predictions about how much a
state s˜ will be visited. ψπ(s,a) are successor features (SFs) which form predictions about
how much state-features φ(s) will be experienced. µπ(s˜|s,a) is the successor model (SM)
which predicts the likelihood of experiencing s˜ in the future.
163 Practical learning algorithms
and associated challenges
Of the predictive representations discussed in §2, only successor features and successor mod-
els have been successfully scaled to environments with large, high-dimensional state spaces
(including continuous state spaces). Thus, these will be our focus of discussion. We will first
discuss learning SFs in §3.1 and then learning successor models in §3.2.
3.1 Learning successor features
In this section we discuss practical considerations for learning successor features, including
learning a function φ that produces cumulants (§3.1.1) and estimating SFs ψ (§3.1.2).
θ θ
3.1.1 Discovering cumulants
One central challenge to learning SFs is that they require cumulants that are useful for adap-
tive agent behavior; these are not always easy to define a priori. In most work, these cumu-
lants have been hand-designed, but potentially better ones can be learned from experience.
Some general methods for discovering cumulants include leveraging meta-gradients (Veeriah
et al., 2019), discovering features that enable reconstruction (Kulkarni et al., 2016; Machado
et al., 2017), and maximizing the mutual information between task encodings and the cu-
mulants that an agent experiences when pursuing that task (Hansen et al., 2019). However,
these methods don’t necessarily guarantee that the learned cumulants respect a linear rela-
tionship with reward (Eq. 29). To satisfy this, methods typically enforce this by minimizing
the L2-norm of their difference (Barreto et al., 2017):
L = ||r−φ (s)⊤w||2. (37)
r θ 2
WhenlearningcumulantsthatsupporttransferwithGPI,onestrategythatcanbolsterEq.37
is to learn an n-dimensional φ vector for n tasks such that each dimension predicts one task
reward (Barreto et al., 2018). Another strategy is to enforce that cumulants describe inde-
pendent features (Alver and Precup, 2021)—e.g., by leveraging a modular architecture with
separate parameters for every cumulant dimension (Carvalho et al., 2023a) or by enforcing
sparsity in the cumulant dimensions (Filos et al., 2021).
3.1.2 Estimating successor features
Learning an estimator that can generalize across tasks The first challenge for learn-
ing SFs is that they are defined for a particular policy π. We can mitigate this by learning
Universal Successor Feature Approximators (USFAs; Borsa et al., 2018), which approximate
SFs for a policy π that is maximizing reward for task w with a function that takes the
w
corresponding preference vector as input ψ (s,a,w):
θ
ψπw(s,a) ≈ ψ (s,a,w). (38)
θ
17This has several benefits. First, one can share the estimator parameters across tasks, which
can improve learning. Second, ψ now accepts policy encodings z ∈ Z , which represent
θ π π
a particular policy in the space of preference vectors. In other words, z is the preference
π
vector thatinduces policy π. This is useful because it allows SFs to generalize across different
policies. Leveraging a USFA, the GPI operation in Eq. 35 can be adapted to perform a max
operation over Z :
π
n o
π(s;w ) = argmax max ψ (s,a,z )⊤w (39)
new θ π new
a∈A zπ∈Zπ
Several algorithms exploit this property (see §4.1.1, §4.2.2, §4.5). If the policy encoding
space is equivalent to the n training tasks, Z = {w ,...,w }, this recovers the original
π 1 n
GPI operation (Eq. 35).
Learning successor features while learning a policy Often SFs need to be learned
simultaneously with policy optimization. This requires the SFs to be updated along with
the policy. One strategy is to simultaneously learn an action-value function Q (s,a,w) =
θ
ψ (s,a,w)⊤w that is used to select actions. One can accomplish this with Q-learning over
θ
values defined by the SFs and task encoding. Q-values are updated towards a target y
Q
defined as the sum of the reward R(s′,w) and the best next Q-value. We define the learning
objective L as follows:
Q
y = R(s′,w)+γψ (s′,a∗,w)⊤w L = ||ψ (s,a,w)⊤w−y || (40)
Q θ Q θ Q
where a∗ = argmax ψ (s′,a′,w)⊤w is the action which maximizes features determined by
a′ θ
w at the next time-step. To ensure that these Q-values follow the structure imposed by SFs
(Eq. 32), we additionally update SFs at a state with a target y defined as the sum of the
ψ
cumulant φ(s′) and the SFs associated with best Q-value at the next state:
y = φ(s′)+γψ (s′,a∗,w) L = ||ψ (s,a,w)−y ||. (41)
ψ θ ψ θ ψ
In an online setting, it is important to learn SFs with data collected by a policy which
chooses actions with high Q-values. This is especially important if the true value is lower
than the estimated Q-value. Because Q-learning leverages the maximum Q-value when doing
backups, it has a bias for over-estimating value. This can destabilize learning, particularly
in regions of the state space that have been less explored (Ostrovski et al., 2021).
Another strategy to stabilize SF learning is to learn individual SF dimensions with sep-
arate modules (Carvalho et al., 2023a,b). Beyond stabilizing learning, this modularity also
enables approximating SFs that generalize better to novel environment configurations (i.e.,
which are more robust to novel environment dynamics).
Estimating successor features with changing cumulants Insomecases,thecumulant
itselfwillchangeovertime(e.g.,whentheenvironmentisnon-stationary). Thisischallenging
for SF learning because the prediction target is non-stationary (Barreto et al., 2018). This
18is an issue even when the environment is stationary but the policy is changing over time:
different policies induce different trajectories and different state-features induce different
descriptions of those trajectories.
One technique that has been proposed to facilitate modelling a non-stationary cumulant
trajectories is to learn an SF as a probability mass function (pmf) p(ψ(k)|s,a,w) defined over
some set of possible values B = {b 1,...,b M}. Specifically, we can estimate an n-dimensional
SF ψπw(s,a) ∈ Rn with ψ (s,a,w) and represent the k-th SF dimension as ψk(s,a,w) =
θ θ
PM p(ψ(k) = b |s,a,w)b . WecanthenlearnSFswithanegativelog-likelihoodlosswhere
m=1 m m
we construct categorical target labels y from the return associated with the optimal Q-
ψ(k)
value (Carvalho et al., 2023b):
y = φk(s′)+γψk(s′,a∗,w) (42)
ψ(k) θ θ
n
L = −X f (y )⊤logp(ψ(k)|s,a,w). (43)
ψ label ψ(k)
k=1
Prior work has found that the two-hot representation is a good method for defining f (·)
label
(Carvalho et al., 2023b; Schrittwieser et al., 2020). In general, estimating predictive repre-
sentations such as SFs with distributional losses such as Eq. 42 has been shown to reduce
the variance in learning updates (Imani and White, 2018). This is particularly important
when cumulants are being learned as this can lead to high variance in y .
ψ(k)
3.2 Learning successor models
In this section, we focus on estimating µπ(s˜|s,a) with µ (s˜|s,a). In a tabular setting, one
θ
can leverage TD-learning with the Bellman equation in Eq. 25. However, for very large state
spaces (such as with infinite size continuous state spaces), this is intractable or impractical.
Depending on one’s use-case different options exist for learning this object. First, we discuss
the setting where one wants to learn a successor model they can sample from (§3.2.1).
Afterwards, we discuss the setting where one only wants to evaluate a successor model for
different actions given a target state (§3.2.2).
3.2.1 Learning successor models that one can sample from
Learning a successor model that one can sample from can be useful for evaluating a policy
(Eq. 26), evaluating a sequence of policies (Thakoor et al., 2022), and in model-based con-
trol (Janner et al., 2020). There are two ways to learn successor models you can sample
from: adversarial learning and density estimation (Janner et al., 2020). Adversarial learning
has been found to be unstable, so we focus on density estimation, where the objective is to
find parameters θ that maximize the log-likelihood of states sampled from µπ:
L
µ
= E(s,a)∼p(s,a)[logµ θ(s˜| s,a)] (44)
s˜∼µπ(·|s,a)
19We can optimize this objective as follows. When sampling targets, we need to sample s˜
in proportion to the discount factor γ—we can accomplish this by first sampling a time-
step from a geometric distribution, k ∼ Geom(1−γ), and then selecting the state at that
time-step,s , as the target s˜.
t+k
While this is a simple strategy, it has several challenges. For values of γ close to 1, this
becomes a challenging learning problem requiring predictions over very long time horizons.
Another challenge is that you are using s obtained under policy π. In practice, we may
t+k
want to leverage data collected under a different policy. This happens when, for example,
we want to learn from a collection of different datasets, or we are updating our policy over
the course of learning. Learning from such off-policy data can lead to high bias, or a high
variance learning update from off-policy corrections (Precup, 2000).
We can circumvent these challenges as follows. First let’s define a Bellman operator Tπ:
(Tπµπ)(s˜|s,a) = (1−γ)T(s′|s,a)+γX T(s′|s,a)X π(a′|s′)µ(s˜|s′,a′). (45)
s′ a′
With this we can define a “cross-entropy temporal-difference” loss (Janner et al., 2020):
L µ = E (s,a)∼p(s,a) [logµ θ(s˜| s,a)] (46)
s˜∼(Tπµπ)(·|s,a)
Intuitively, (Tπµπ)(·|s,a) defines a random variable obtained as follows. First sample s′ ∼
T(·|s,a). Terminate and emit s′ with probability (1 − γ). Otherwise, sample a′ ∼ π(a′|s′)
and them sample s˜∼ µπ(·|s′,a′).
The most recent promising method for learning Eq. 46 has been to leverage a varia-
tional autoencoder (VAE; Thakoor et al., 2022). Specifically, we can define an approximate
posterior q (z|s,a,s˜) and then optimize the following evidence lower-bound:
ψ
" " µ (s˜| s,a,z)##
L µ = E (s,a)∼p(s,a) Ez∼q ψ(·|s,a,s˜) log qθ (z|s,a,s˜) (47)
s˜∼(Tπµπ)(·|s,a) ψ
See Thakoor et al. (2022) for more details. While they were able to scale their experiments
to slightly more complex domains than Janner et al. (2020), their focus was on composing
policies via Geometric Policy Composition (discussed more in §4.2.3), so it is unclear how
well their method does in more complex domains. The key challenge for this line of work is
in sampling from µπ(· | s,a), where s˜ can come from a variable next time-step after (s,a).
In the next section, we discuss methods which address this challenge.
3.2.2 Learning successor models that one can evaluate
Sometimes we may not need to learn a successor model that we can sample from, only one
that we can evaluate. This can be used, for example, to generate and improve a policy that
achieves some target state (Eysenbach et al., 2020; Zheng et al., 2023). One strategy is to
learn a classifier pµ that, given (s,a), computes how likely s˜ is compared to some set of N
θ
random (negative) states {s−}N the agent has experienced:
i i=1
exp(f (s,a,s˜))
pµ(s˜| s,a,s− ) = θ (48)
θ 1:N exp(f (s,a,s˜))+PN exp(f (s,a,s−))
θ i=1 θ i
20One option for learning f is to find a value that maximizes this classification across random
θ
states and actions in the agent’s experience, (s,a) ∼ p(s,a), target states s˜ drawn from
the empirical successor model distribution, s˜ ∼ µπ(s˜| s,a), and negatives drawn from the
state-marginal, s− ∼ p(s),
1:N
h i
L µ = E(s,a)∼p(s,a) logpµ θ(s˜| s,a,s− 1:N) . (49)
s˜∼µπ(s˜|s,a)
s− ∼p(s)
1:N
If we can find such an f , then the classification it defines pµ is approximately equal to the
θ θ
density ratio between µπ(s˜ | s,a) and the state marginal p(s˜) (Poole et al., 2019; Zheng
et al., 2023):
µπ(s˜| s,a)
≈ (N +1)·pµ(s˜| s,a,s− ). (50)
p(s˜) θ 1:N
Optimizing Eq. 49 is challenging because it requires sampling from µπ. We can circumvent
this by instead learning the following TD-like objective where we replace sampling from µπ
with sampling from the state-marginal and reuse pµ as an importance weight:
θ
h i
L µ = E(s,a)∼p(s,a) (1−γ)logpµ θ(s′|s,a,s− 1:N)+γ(N +1)pµ θ(s˜|s′,a′,s− 1:N)logpµ θ(s˜|s,a,s− 1:N)
s′∼T(·|s,a)
a′∼π(a|s′)
s− ∼p(s)
1:N (51)
Zheng et al. (2023) show that (under some assumptions) optimizing Eq. 51 leads to the
following Bellman-like update
h i
pµ θ(s˜|s,a,s− 1:N) ← (1−γ)T(s′ = s˜|s,a)+γEs′∼T(·|s,a) pµ θ(s˜|s′,a′,s− 1:N) (52)
a′∼π(·|s′)
which resembles the original successor model Bellman equation (Eq. 25). However, one key
difference to Eq. 25 is that we parameterize pµ(s˜|s,a,s− ) with N + 1 random samples
θ 1:N
(e.g., from a replay mechanism), which is a form of contrastive learning. This provides an
interesting, amortized algorithm for learning successor models. With the SR, one performs
the TD update for all states (Eq. 18); here, one performs this update using a random sample
of N +1 states.
We can define f as the dot product between a predictive representation ϕ (·,·) and
θ θ
label representation φ (·), f (s,a,s˜) = ϕ (s,a)⊤φ (s˜). φ (s) can then be thought of as
θ θ θ θ θ
state-features analogous to SFs (§2.5). ϕ (s,a) is then a “prediction” of these future features
θ
similar to SFs, ψ (s,a), with labels coming from future states; however, it doesn’t necessarily
θ
have the same semantics as a discounted sum (i.e., Eq. 32). We use similar notation because
of their conceptual similarity. We can then understand Eq. 51 as doing the following. The
first term in this objective pushes the prediction ϕ (s,a) towards the features at the next-
θ
timestep φ (s′), and the second term pushes ϕ (s,a) towards the features at arbitrary state-
θ θ
features φ (s˜). Both terms repel ϕ (s,a) from arbitrary “negative” state-features φ (s˜−).
θ θ θ i
This provides an interesting contrastive-learning based mechanism for how brains may learn
µπ in a non-tabular setting.
214 Artificial intelligence applications
In this section, we discuss how the SR and its generalizations have enabled advances in
artificial agents that learn and transfer behaviors from experience.
4.1 Exploration
4.1.1 Pure exploration
Learning to explore and act in the environment before exposure to reward This
is perhaps closest to the original inspiration for the SR (Dayan, 1993). Here, the agent can
explore the environment without exposure to reward or punishment for some period of time,
and tries to learn a policy that can transfer to an unknown task w .
new
One useful property of SFs is that they encode predictions about what features one can
expect when following a policy. Before reward is provided, this can be used to reach different
parts of the state space with different policies. One strategy is to associate different parts
of the state space with different parts of a high-dimensional task embedding space (Hansen
et al., 2019). At the beginning of each episode, an agent samples a “goal” encoding w from
a high-entropy task distribution p(w). During the episode, the agent selects actions that
maximize the features described by w (e.g., with Eq. 32). As it does this, it learns to predict
w from the states it encounters. If we parameterize this p(w) as a von Mises distribution,
then we can learn this prediction by simply maximizing the dot-product between the state-
features φ(s) and goal encoding w:
L = logp (w|s) = φ (s)⊤w. (53)
φ θ θ
This is equivalent to maximizing the mutual information between φ (s) and w. As the agent
θ
learns cumulants, it learns SFs as usual (e.g., with Eq. 41). Thus, one can essentially use a
standard SF learning algorithm and simply replace the cumulant discovery loss (e.g., Eq. 37)
with Eq. 53. Once the agent is exposed to task reward w , it can then freeze φ and solve
new θ
for w (e.g., with Eq. 37). The agent can then use GPI to find the best policy for achieving
new
w by searching over a Gaussian ball defined around w . This is equivalent to setting
new new
Z = {z ∼ N(w ,σ)}n for Eq. 39, where σ defines the standard deviation of the Gaussian
i new i=1
ball.
Hansen et al. (2019) leveraged this strategy to develop agents which could explore Atari
games without any reward for 250 million time-steps and then have 100 thousand time-steps
toearnreward. Theyshowedthatthisstrategywasabletoachievesuperhumanperformance
across most Atari games, despite not observing any rewards for most of its experience. Liu
and Abbeel (2021) improved on this algorithm by adding an intrinsic reward function that
favorsexploringpartsofthestatespacethatare“surprising”(i.e.,whichinducehighentropy)
given a memory of the agent’s experience. This dramatically improved sample efficiency for
many Atari games.
22Exploring the environment by building a map. Agents need to explore large state
space systematically. One strategy is to build a map of the environment defined over land-
marksintheenvironment. Withsuchamap, anagentcansystematicallyexplorebyplanning
paths towards the frontier of its knowledge (Ramesh et al., 2019; Hoang et al., 2021). How-
ever, numerous questions arise in this process. How do we define good landmarks? How
do we define policies that traverse between landmarks? How does an agent identify that it
has made progress between landmarks after it has set a plan, or course-correct if it finds
that it accidentally deviated? Hoang et al. (2021) developed an elegant solution to all of
these problems with the successor feature similarity (SFS) metric. This similarity metric
defines “closeness” by how likely two states are to visit the same parts of the environment.
Intuitively, if starting from two states s and s , an agent visits very different parts of the
1 2
environment, this metric measures these states as being far apart; if instead the two states
lead to the same parts of the environment, the metric measures them as being close together.
Concretely:
S ψ(s 1,a,s 2) = ψπ¯(s 1,a)⊤ EA∼π¯[ψπ¯(s 2,A)] (54)
S ψ(s 1,s 2) = EA∼π¯[ψπ¯(s 1,a)⊤] EA∼π¯[ψπ¯(s 2,A)], (55)
where π¯ is a uniform policy.7
Through only learning of successor features over pretrained cumulants φ, Hoang et al.
(2021) are able to address all the needs above by exploiting SFS. Let L ⊂ S be the set of
landmarks discovered so far. The algorithm works as follows. Each landmark L ∈ L has
associated with it a count N(L) for how often its been visited. A “frontier” landmark L is
F
sampled in proportion to the inverse of this count. The agent makes a shortest-path plan
of subgoals (s∗,...,s∗) towards this landmark where s = L . In order to navigate to the
1 n n F
next subgoal s∗, it defines a policy with the action of the current successor features that
i
are most aligned with the goal’s successor features, a∗(s,s∗) = argmax S (s,a,s∗). As it
i a ψ i
traverses towards the landmark, it localizes itself by comparing the current state s to known
landmarks f (s,L) = argmax S (s,L). When s∗ = f (s,L), it has reached the next
loc L∈L ψ i loc
landmark and it moves on to the next subgoal. Once a frontier landmark is found, the agent
explores with random actions. The states it experiences are added to its replay buffer and
used in 2 ways. First, they update the agent’s current SR. Second, if a state is sufficiently
different from known landmarks, it is added. In summary, the 3 key functions that one can
compute without additional learning are:
π(s,s∗) = argmaxS (s,a,s∗) goal-conditioned policy
i ψ i
a
f (s,L) = argmaxS (s,L) localization function
loc ψ
L∈L
f (s,L) = L ← L∪s if (S (s,L) < ϵ )∀L ∈ L landmark addition function
add ψ add
The process then repeats. This exploration strategy was effective in exploring both min-
igrid (Chevalier-Boisvert et al., 2023) and the partially observable 3D VizDoom environ-
ment (Kempka et al., 2016).
7Note that to avoid excessive notation, we’ve overloaded the definition of S .
ψ
234.1.2 Balancing exploration and exploitation
Cheap uncertainty computations. Balancing exploration and exploitation is a central
challenge in RL (Sutton and Barto, 2018; Kaelbling et al., 1996). One method that pro-
vides close to optimal performance in tabular domains is posterior sampling (also known as
Thompson sampling, after Thompson, 1933), where an agent updates a posterior distribu-
tion over Q-values and then chooses the value-maximizing action for a random sample from
this distribution (see Russo et al., 2018, for a review). The main difficulties for implement-
ing posterior sampling are associated with representing, updating, and sampling from the
posterior when the state space is large. Janz et al. (2019) showed that SFs enable cheap
method for posterior sampling. They assume the following prior, likelihood, and posterior
for the environment reward:
p(w) = N(0,I) p(r|w) = N(φ(s)⊤w,β) p(w|r,s) = N(µ ,Σ ) (56)
w w
where β is the variance of reward around a mean that is linear in the state feature φ(s); µ
w
and Σ are (known) analytical solutions for the mean and variance of the posterior given the
w
Gaussian prior/likelihood assumptions and a set of observations. The posterior distribution
for the Q-values is then given by:
Qˆ
π ∼ N
(cid:16)
Ψπµ ,ΨπΣ
(Ψπ)⊤(cid:17)
(57)
SU w w
where Ψπ = [ψπ(s,a)]⊤ is a matrix where each row is an SF for a state-action
(s,a)∈S×A
pair. Successor features, cumulants, and task encodings are learned with standard losses
(e.g., Eq. 37 and Eq. 41).
Count-based exploration. Another method that provides (near) optimal exploration in
q
tabular settings is count-based exploration with a bonus of 1/ N(s) (Auer, 2002). Here
N(s) represents the number of times a state s has been visited. However, when the state
space is large it can be challenging to matain this count. Machado et al. (2020) showed that
that the L1-norm of SFs is proportional to visitation count and can be used as an exploration
bonus:
1
Rint(s) = . (58)
||ψπ(s)||
1
With this exploration bonus, they were able to improve exploration in sparse-reward Atari
games such as Montezuma’s revenge. Recent work has built on this idea: Yu et al. (2023)
combinedSFswithpredecessor representations,whichencoderetrospectiveinformationabout
the agent’s trajectory. This was shown to more efficiently target exploration towards bottle-
neck states (i.e., access points between large regions of the state space).
4.2 Transfer
We’ve already introduced the idea of cross-task transfer in our discussion of GPI. We now
review the broader range of ways in which the challenges of transfer have been addressed
using predictive representations.
244.2.1 Transferring behaviors between tasks
We first consider transfer across tasks that are defined by different reward functions. In the
following two sections, we consider other forms of transfer.
Few-shot transfer between pairs of tasks. The SR can enable transferring behaviors
from one reward function R to another function R by exploiting Eq. 34. One can ac-
new
complish this by learning cumulant φ and SFs ψ for a source task. At transfer time, one
θ θ
freezes each set of parameters and solves for w (e.g., with Eq. 37). Kulkarni et al. (2016)
new
showed that this enabled an RL agent that learned to play an Atari game to transfer to a
new version of that game where the reward function was scaled. Later, Zhu et al. (2017)
showed that this enabled transfer to new “go to” tasks in a photorealistic 3D household
environment.
Continual learning across a set of tasks. Beyond transferring across task pairs, an
agent may want to continually transfer its knowledge across a set of tasks. Barreto et al.
(2017) showed that SFs and GPI provide a natural framework to do this. Consider sequen-
tially learning n tasks. As the agent learns new tasks, they maintain a growing library of
successor features {ψπi}m where m < n is the number of tasks learned so far. When the
i=1
agent is learning the m-th task, they can select actions with GPI according to Eq. 35 using
w as the current transfer task. The agent learns successor features for the current task
m
according to Eq. 41. Zhang et al. (2017) extended this approach to enable continual learn-
ing when the environment state space and dynamics were changing across tasks but still
relatively similar. Transferring SFs to an environment with a different state space requires
leveraging new state-features. Their solution involved re-using old state-features by mapping
them to the new state space with a linear projection. By exploiting linearity in the Q-value
decomposition (see Eq. 34), this allowed reusing SFs for new environments.
Zero-shot transfer to task combinations. AnotherbenefitofSFsandGPIarethatthey
facilitate transfer to task conjunctions when they are defined as weighted sums of training
tasks, w = Pn α w . A clear example is combining “go to” tasks (Barreto et al., 2018;
new i=1 i i
Borsa et al., 2018; Barreto et al., 2020; Carvalho et al., 2023a). For example, consider 4
tasks {w ,...,w } defining by collecting different object types; w = (−1)·w +2·w +1·
1 4 new 1 2
w +0·w defines a new task that tries to avoid collecting objects of type 1, while trying to
3 4
collect objects of type 2 twice as much as objects of type 3. This approach has been extended
to combining policies with continuous state and action spaces (Hunt et al., 2019), though
it has so far been limited to combining only two policies. Another important limitation of
this general approach is that it can only specify which tasks to prioritize, but cannot specify
an ordering for these tasks. For example, there is no way to specify collecting object type 1
before object type 2. One can address this limitation by learning a state-dependent transfer
task encoding as with the option keyboard (§2.5.2).
254.2.2 Learning about non-task goals
As an agent is learning a task (defined by w), they may also want to simultaneously do well
on another task (defined by w˜). This can be helpful for hindsight experience replay, where
an agent re-labels failed experiences at reaching some task goal to successful experiences in
reaching some non-task goal (Andrychowicz et al., 2017). This is particularly useful when
tasks have sparse rewards. When learning a policy with SMs (§2.4), hindsight experience
replay naturally arises as part of the learning objective. It has been shown to improve
sample efficiency in sparse-reward virtual robotic manipulation domains and long-horizon
navigation tasks (Eysenbach et al., 2020, 2022; Zheng et al., 2023). Despite their potential,
learning and exploiting SMs is still in its infancy, whereas SFs have been more thoroughly
studied. Recently, Schramm et al. (2023) developed an asymptotically unbiased importance
sampling algorithm that leverages SFs to remove bias when estimating value functions with
hindsightexperiencereplay. Thisenabledlearningforbothsimulationandreal-worldrobotic
manipulation tasks in environments with large state and action spaces.
Learning to reach non-task goals can also be useful for off-task learning. In this setting,
as an agent learns a control policy for w, they also learn a control policy for w˜. Borsa
et al. (2018) showed that one can learn control policies for tasks w˜ that are not too far
from w in the task encoding space by leverage universal SFs. In particular, nearby off-
task goals can be sampled from a Gaussian ball around w with standard deviation σ: Z =
{w˜ ∼ N(w ,σ)}n . Then a successor feature loss following Eq. 41 would be applied
i new i=1
for each non-task goal w˜ . Key to this is that the optimal action for each w˜ would be
i i
the action that maximized the features determined by w˜ at the next time-step, e.g. a∗ =
i
argmax′ ψ (s′,a′,w˜ )⊤w˜ . This enabled an agent to learn a policy for not only w but also
a θ i i
for non-task goals w˜ with no direct experience on those tasks in a simple 3D navigation
i
environment.
4.2.3 Other advances in transfer
Generalization to new environment dynamics. One limitation of SFs is that they’re
tied to the environment dynamics T with which they were learned. Lehnert and Littman
(2020) and Han and Tschiatschek (2021) both attempt to address this limitation by learning
SFs over state abstractions which respect bisimulation relations (Li et al., 2006). Abdolshah
et al. (2021) attempt to address this by integrating SFs with Gaussian processes such that
they can be quickly adapted to new dynamics given a small amount of experience in the new
environment.
Synthesizing new predictions from sets of SFs. While GPI enables combining a set
of SFs to produce a novel policy, it does not generate a novel prediction of what features will
be experienced from a combination of policies. Some methods attempt to address this by
combining SFs via their convex combination (Brantley et al., 2021; Alegre et al., 2022).
26Alternatives to generalized policy improvement. Madarasz and Behrens (2019)
develop the Gaussian SF, which learns a set of reward maps for different environments
that can be adaptively combined to adjudicate between different policies. While they had
promising results over GPI, these were in toy domains; it is currently unclear if their method
scales to more complex settings as gracefully as GPI. A more promising alternative to GPI
is Geometric Policy Composition (GPC; Thakoor et al., 2022) which enables estimating Q-
values when one follows a ordered sequence of n policies (π ,...,π ). Whereas GPI evaluates
1 n
how much reward will be obtained by the best of a set of policies, GPC is a form of model-
based control where we evaluate the path obtained from following a sequence of policies. We
discuss this in more detail in §4.4.
4.3 Hierarchical reinforcement learning
Manytaskshavemultipletime-scales,whichcanbeexploitedusingahierarchicalarchitecture
in which the agent learns and acts at multiple levels of temporal abstraction. The classic
exampleofthisistheoptions framework(Suttonetal.,1999). An“option”oisatemporally-
extended behavior defined by (a) an initiation function I that determines when it can be
o
activated, (b) a behavior policy π , and (c) a termination function β that determines when
o o
the option should terminate: o = ⟨I ,π ,β ⟩. In this section, we discuss how predictive
o o o
representations can be used to discover useful options and transfer them across tasks.
4.3.1 Discovering options to efficiently explore the environment
One key property of the SR is that it captures long-range structure of an agent’s paths
through the environment. This has been useful in discovering options. Machado et al. (2017,
2023) showed that if one performs an eigendecomposition on a learned SR, the eigenvectors
corresponding to the highest eigenvalues could be used to identify states with high diffusion
(i.e., states that tend to be visited frequently under a random walk policy). In particular,
for eigenvector e , they defined the following intrinsic reward function:
i
Rint(s,s′) = e⊤(φ(s′)−φ(s)) (59)
i i
which rewards the agent for exploring more diffuse parts of the state space. Note that φ is
either a one-hot vector in the case of the SR or can be state-features in the case of successor
features. Here, we focus on the SR. With this reward function, the agent can iteratively
build up a set of options O that can be leveraged to improve exploration of the environment.
The algorithm proceeds as follows:
1. Collect samples with a random policy which selects between between primitive ac-
tions and options. The set of options is initially empty (O = {∅}).
2. Learn successor representation Mπ from the gathered samples.
3. Get new exploration option o = ⟨I ,π ,β ⟩. π is a policy that maximizes the
o o o o
intrinsic reward function in Eq. 59 using the current SR. The initiation function I is
o
271 for all states. The termination function β is 1 when the intrinsic reward becomes
o
negative (i.e., when agent begins to go towards more frequent states). This option is
added to the overall set of options, O ← O∪{o}.
Agents endowed with this strategy were able to discover meaningful options and improve
sample efficiency in the four-rooms domain, as well as Atari games such as Montezuma’s
revenge.
4.3.2 Transferring options with the SR
Instant synthesis of combinations of options. One of the benefits of leveraging SFs is
thattheyenabletransfertotasksthatarelinearcombinationsofknowntasks(see§2.5.1). At
transfer time, an agent can exploit this when transferring options by defining subgoals using
this space of tasks (see §2.5.2). In continuous control settings where an agent has learned
to move in a set of directions (e.g., up, down, left, right), this has enabled generalization to
combinations of these policies (Barreto et al., 2019). For example, the agent could instantly
move in novel angles (e.g. up-right, down-left, etc.) as needed to complete a task.
Transferring options to new dynamics. One limitation of both options and SFs is that
they are defined for a particular environment and don’t naturally transfer to new environ-
ments with different transition functions T′. Han and Tschiatschek (2021) addressed this
limitation by learning abstract options with SFs defined over abstract state features that
respect bisimulation relations (Li et al., 2006). This method assumes that transfer from a
set of N ψ-MDPs {M }n where M = ⟨S ,O ,T ,ψ,γ⟩. It also assumes the availability
i i=1 i i i i
of MDP-dependent state-feature functions φ (s,a) that map individual MDP state-action
Mi
pairs to a shared feature space. Assume an agent has learned a (possibly) distinct set of
options {o } for each source MDP. The SFs for each option’s policy are defined as usual
k
(Eq. 30). When we are transferring to a new MDP M′, we can map each option to this
environment by mapping its SFs to a policy which produces similar features using an inverse
reinforcement learning algorithm (Ng et al., 2000). Once the agent has transferred options
to a new environment, it can plan using these options by constructing an abstract MDP
over which to plan. Han and Tschiatschek (2021) implement this using successor homomor-
phisms, which define criteria for aggregating states to form an abstract MDP. In particular,
pairs of states (s ,s ) and options (o ,o ) will map to the same abstract MDP if they follow
1 2 1 2
bisimulation relations (Li et al., 2006) and if their SFs are similar:
||ψo1(s ,a)−ψo2(s ,a)|| ≤ ϵ , (60)
1 2 ψ
where ϵ is a similarity threshold. With this method, agents were able to discover state
ψ
abstractions that partitioned large grid-worlds into intuitive segments and successfully plan
in novel MDPs with a small number of learning updates.
284.4 Model-based reinforcement learning
The successor model (SM) is interesting because it offers a novel way to do model-based
reinforcement learning. Traditionally, one simulates trajectories with a single-step model.
While this is very flexible, it is expensive. SMs offer an alternative, where we can instead
sample and evaluate likely (potentially distal) states that will be encountered when following
some policy π. Thakoor et al. (2022) leverage this property to develop Generalized Policy
Composition (GPC), a novel algorithm that enables a jumpy form of model-based reinforce-
ment learning. In this setting, rather than simulate trajectories defined over next states,
we simulate trajectories by using SMs to jump between states using a given set of policies.
While this is not as flexible as simulating trajectories with a single-step model, it is much
more efficient.
In RL, one typically uses a large discount factor (i.e., γ ≈ 1). When learning an SM,
this is useful because you can learn likelihoods over potentially very distal states. However,
this makes learning an SM more challenging. GPC mitigates this challenge by composing a
shorter horizon SM µπ with a longer horizon SM µπ, where β < γ. Composing two separate
β γ
SMs with different horizons has the following benefits. An SM with a shorter horizon µπ is
β
easier to learn but cannot sample futures as distal as µπ; on the other hand, µπ is harder to
γ γ
learn but can do very long horizon predictions and better avoids compounding errors. By
combining the two, Thakoor et al. (2022) show that you can trade off between these two
errors.
At an intuitive level, GPC with SMs works as follows. Given a starting state-action
pair (s ,a ) and policies (π ,...,π ), we sample a sequence of n − 1 next states with our
0 0 1 n
shorter horizon SM µπi and π , i.e. s ∼ µπ1(·|s ,a ), a ∼ π (·|s ), s ∼ µπ1(·|s ,a ), and
β i 1 β 0 0 1 1 1 2 β 1 1
so on. We then sample a (potentially more distal) state s from our longer horizon SM,
n
µπ, s ∼ µπn(·|s ,a ). The reward estimates for the sampled state-action pairs can
γ n β n−1 n−1
be combined as a weighted sum to compute Qπ(s ,a ) analogously to Eq. 28 (see Thakoor
0 0
et al., 2022, for technical details). Leveraging GPC enabled convergence with an order
of magnitude fewer samples in the four-rooms domain and in a continuous-control domain
where one had to control an ant through a maze.
4.5 Multi-agent reinforcement learning
As we’ve noted earlier, the SR is conditioned on a policy π. In a single-agent setting, the
SR provides predictions about what that agent can expect to experience when executing
the policy. However, in multi-agent settings, one can instead parametrize this prediction
with another agent’s policy to form predictions about what one can expect to see in the
environment when other agent’s follow their own policies. This is the basis for numerous
algorithms that aim to learn about, from, and with other agents (Rabinowitz et al., 2018;
Kim et al., 2022; Filos et al., 2021; Gupta et al., 2021; Lee et al., 2019).
Learning about other agents. Rabinowitz et al. (2018) showed that an AI agent could
learn aspects of theory of mind (including passing a false belief test) by meta-learning SFs
29that described other agents. While Rabinowitz did not explicitly compare against humans
(and was not trying to directly model human cognition), this remains exciting as a direction
for exploring scalable algorithms for human-like theory of mind.
Learning from other agents. One nice property of SFs is that they can be learned with
TD-learning using off-policy data (i.e., data collected from a policy different from the one
currently being executed). This can be leveraged to learn SFs for the policies of other agents
just as an agent learns SFs for their own policy. Filos et al. (2021) exploited this to design an
agent that simultaneously learned SFs for both its own policy and for multiple other agents.
They were then able to generalize effectively to new tasks via a combination of all of their
policies by exploiting GPI (Eq. 35).
Learning with other agents. One of the key benefits of leveraging universal SFs with
GPI (Eq. 39) is that you can systematically change the Q-value used for action selection by
(1) shifting the policy encoding z defining the SF or (2) shifting the feature preferences w
π
that are being maximized. Gupta et al. (2021) exploit this for cooperative multi-agent RL.
Let s(i) refer to the state of the i-th agent and a(i) ∈ A be the action they take. Now let
i
(cid:16) (cid:17)
ψ s(i),a,z refer to that agent’s successor features for behavioral encoding z , let C
θ π π tasks
denote the set of possible feature preferences that agents can follow, and let C denote the
π
set of possible policy encodings over which agents can make predictions. Gupta et al. (2021)
studied the cooperative setting where the overall Q-value is simply the sum of individual
Q-values. If C denotes policies for separate but related tasks, acting in accordance to
π
(cid:26) (cid:16) (cid:17)⊤ (cid:27)
a(i) = argmax max max ψ s(i),a,z w (61)
θ π
a∈Ai w∈Ctaskszπ∈Cπ
enablesacollectionofagentstotakeactionswhichsystematicallyexploretheenvironmentto
quickly do well on tasks specified by C . They were able to improve exploration and zero-
tasks
shot generalization in a variety of multi-agent environments including Starcraft (Samvelyan
et al., 2019).
4.6 Other artificial intelligence applications
The SR and its generalizations have been broadly applied within other areas of AI. For
example, it has been used to define an improved similarity metric in episodic control set-
tings (Emukpere et al., 2021). By leveraging SFs, one can incorporate information from prior
experienced states with similar future dynamics to the current state. The SR has also been
applied towards improving importance sampling in off-policy learning. If we learn a den-
sity ratio similar to Eq. 49, this can enable more simple marginalized importance sampling
algorithms (Liu et al., 2018) that improve off-policy evaluation (Fujimoto et al., 2021). In
addition to these example, we highlight the following applications of the SR.
30Representation learning. LearningSMscanobviatetheneedforseparaterepresentation
losses. In many applications, the reward signal is not enough to drive learning of useful
representations. Some strategies to address this challenge include data augmentation and
learningofauxiliarytasks. LearningtheSMhasbeenshowntoenablerepresentationlearning
with superior sample-efficiency without using these additions (Eysenbach et al., 2020, 2022;
Zheng et al., 2023).
Learning auxiliary tasks is also a natural use-case for predictive representations. A
simple and natural example comes from inspecting the loss for learning SFs (Eq. 41). In
standard Q-learning, we only learn about achieving task-specific reward. When learning
SFs, we also learn representations that enable achieving state-features that are potentially
not relevant for the current task (i.e., we are—by default—leaning auxiliary tasks). This
important ability is even possible in a continual learning setting where the distribution of
state-features is non-stationary (McLeod et al., 2021). Another interesting example comes
from proto-value networks (Farebrother et al., 2023). The authors show that if one learns
a successor measure (a set-inclusion based generalization of the SR) over random sets, this
can enable the discovery of predictive representations that enable very fast learning in the
Atari learning environment (5 million vs. 200 million samples).
Learning diverse policies. A final application of the SR has been in learning diverse
policies. In the mujoco environment, Zahavy et al. (2021) showed that SFs enabled discov-
ering a set of diverse behaviors for controlling a simulated dog avatar. Their approach used
SFs to prospectively summarize future trajectories. A set of policies was than incrementally
learned so that each new policy would be different in its expected features from all policies
learned so far. This ideas was then generalized to enable diverse chess playing (Zahavy et al.,
2023). Similar to before, different strategies were discovered such that their expected future
features would be maximally different.
5 Cognitive science applications
A rich body of work dating back over a century has linked RL algorithms to reward-based
learning processes in the brains of humans and nonhuman animals (Niv, 2009). As explained
further below, model-based control is thought to underlie reflective, goal-directed behaviors,
while model-free control is thought to underlie reflexive, habitual behaviors. The existence of
both systems in the brain and their synergistic operation has received extensive support by
a wide range of behavioral and neural studies across a number of species and experimental
paradigms (see Dolan and Dayan, 2013, for a detailed review).
The general success of RL in modeling brain function, and the success of the SR in
solving problems faced by artificial agents, raises the question of whether evolution has
arrived at similar predictive representations for tackling problems faced by biological agents.
In particular, the SR can confer ecological advantages when flexibility and efficiency are
both desirable, which is the case in most real-world decision making scenarios. In the field of
31cognitive science, several lines of research suggest that human learning and generalization is
indeed consistent with the SR. In this section, we examine studies showing that patterns of
responding to changes in the environment (Momennejad et al., 2017), transfer of knowledge
across tasks (Tomov et al., 2021), planning in spatial domains (Geerts et al., 2023), and
contextual memory and generalization (Gershman et al., 2012; Smith et al., 2013; Zhou
et al., 2023) exhibit signature characteristics of the SR that cannot be captured by pure
model-based or model-free strategies.
A Revaluation B Multi-task learning C Associative learning
s 1 s 3 s 5 $10 z 1 z 2 z 3 z t z ~ sCRP
P Trh aa ins ie n g1: s 2 s 4 s 6 $1 M M M ... M M ~ LDS
1 2 3 t
Reward Transition
Phase 2: s 3 s 5 $1 s 3 s 5 $10
Revaluation
Phase 3: s 4 s 6 $10 s 4 s 6 $1 Geerts et al. (2023) Psych. Review
Test s 1 or s 2 ? s 1 or s 2 ? E Memory
Model-free s 1 s 1 Temporal Episodic
M SRodel-based s s2
2
s s2
1
coding
World s t
context
c t
nem
w
e mm emor oy
ry
En s 1 s 2 s 3 ... s t
Momennejad et al. (2017) Nat. Hum. Behav.
al Temporal Episodic
D Spatial navigation etriev context c
t
memory
p(s)
R
s i ~ p(s) s 1s 2 s 3 ... s t
ng s 1 M : c → p(s)
ki
ma s
2
s
5
s
6
s
7
n
ciso s
3
ß → 0: i.i.d. samples from SR
e
D s
4
de Cothi et al. (2022) Curr. Biol. Tomov et al. (2021) Nat. Hum. Behav. ß → 1: Monte Carlo rollout Zhou et al. (2023) PsyArXiv
Figure 5: Predictive representations in cognitive science. (A) Revaluation paradigm
and predictions from Momennejad et al. (2017, §5.1). (B) Multi-task learning paradigm from
Tomov et al. (2021, §5.2). Reused with permission from the authors. A person is trained on
tasks where they are either hungry (w1 = [1,0,0]) or groggy (w2 = [0,1,0]), and then
train train
tested on a task in which they are hungry, groggy, and looking to have fun (w = [1,1,1]).
new
(C) Context-dependent Bayesian SR from Geerts et al. (2023, §5.3). z, context. M, SR
matrix associated with each context. sCRP, sticky Chinese restaurant process. LDS, linear-
Gaussian dynamical system. (D) Spatial navigation paradigm from de Cothi et al. (2022,
§5.4). (E) TCM-SR: Using the temporal context model (TCM) to learn the SR for decision
making (Zhou et al., 2023, §5.5). M, SR matrix. c, context vector. s, state and/or item.
SR, successor representation.
325.1 Revaluation
Some of the key findings pointing to a balance between a goal-directed system and a habitual
system in the brain came from studies of reinforcer revaluation (Adams and Dickinson, 1981;
Adams, 1982; Dickinson, 1985; Holland, 2004). In a typical revaluation paradigm, an animal
(e.g., a rat) is trained to associate a neutral action (e.g., a lever press) with an appetitive
outcome (e.g., food). The value of that outcome is subsequently reduced (e.g., the rat is
satiated, so food is less desirable) and the experimenter measures whether the animal keeps
taking the action in the absence of reinforcement. Goal-directed control predicts that the
animal would not take the action, since the outcome is no longer valuable, while habitual
control predicts that the animal would keep taking the action, since the action itself was
not devalued. Experimenters found that under some conditions—such as moderate training,
complex tasks, or disruptions to dopamine inputs to striatum—behavior appears to be goal-
directed (e.g., lever pressing is reduced), while under other conditions—such as extensive
training, or disruptions to prefrontal cortex—behavior appears to be habitual (e.g., the rat
keeps pressing the lever).
A modeling study by Daw et al. (2005) interpreted these findings through the lens of RL.
The authors formalized the goal-directed system as a model-based controller putatively im-
plemented in prefrontal cortex and the habitual system as a model-free controller putatively
implemented in dorsolateral stratum. They proposed that the brain arbitrates dynamically
between the two controllers based on the uncertainty of their value estimates, preferring
the more certain (and hence likely more accurate) estimate for a given action. Under this
account, moderate training or complex tasks would favor the model-based estimates, since
the model-free estimates may take longer to converge and hence be less reliable. On the
other hand, extensive training would favor the model-free estimates, since they will likely
have converged and hence be more reliable than the noisy model-based simulations.
One limitation of this account is that it explains sensitivity to outcome revaluation in
terms of a predictive model, but it does not rule out the possibility that the animals may
instead be relying on a predictive representation. A hallmark feature of predictive repre-
sentations is that they allow an agent to adapt quickly to changes in the environment that
keep its cached predictions relevant, but not to changes that require updating them. In
particular, an agent equipped with the SR should adapt quickly to changes in the reward
structure (R) of the environment, but not to changes in the transition structure (T). Since
the earlier studies on outcome revaluation effectively only manipulated reward structure,
both model-based control and the SR could account for them, leaving open the question of
whether outcome revaluation effects could be fully explained by the SR instead.
This question was addressed in a study by Momennejad et al. (2017) which examined
how changes in either the reward structure or the transition structure experienced by human
participants affect their subsequent choices. The authors used a two-step task consisting
of three phases: a learning phase, a relearning (or revaluation) phase, and a test phase
(Figure 5A). During the learning phase, participants were presented with two distinct two-
step sequences of stimuli and rewards corresponding to two distinct trajectories through
state space. The first trajectory terminated with high reward (s → s → s $10), while the
1 3 5
33second trajectory terminated with a low reward (s → s → s $1), leading participants to
2 4 6
prefer the first one over the second one.
During the revaluation phase, participants had to relearn the second half of each tra-
jectory. Importantly, the structure of the trajectories changed differently depending on the
experimental condition. In the reward revaluation condition, the transitions between states
remained unchanged, but the rewards of the two terminal states swapped (s → s $1;
3 5
s → s $10). In contrast, in the transition revaluation condition, the rewards remained the
4 6
same, but the transitions to the terminal states swapped (s → s $1; s → s $10).
3 6 4 5
Finally, in the test phase, participants were asked to choose between the two initial states
ofthetwotrajectories(s ands ). Notethat, underbothrevaluationconditions, participants
1 2
should now prefer the initial state of the second trajectory (s ) as it now leads to the higher
2
reward.
Unlike previous revaluation studies, this design clearly disambiguates between the pre-
dictions of model-free, model-based, and SR learners. Since the initial states (s and s )
1 2
never appear during the revaluation phase, a pure model-free learner would not update the
cached values associated with those states and would still prefer the initial state of the first
trajectory (s ). On the other hand, a pure model-based learner would update its reward
1
ˆ ˆ
(R) or transition (T) estimates during the corresponding revaluation phase, allowing it to
simulate the new outcomes from each initial state and make the optimal choice (s ) during
2
the test phase. Critically, both model-free and model-based learners (and any hybrid be-
tween them) would exhibit the same preferences during the test phase in both revaluation
conditions.
In contrast, an SR learner would show differential responding in the test phase depending
on the revaluation condition, adapting and choosing optimally after reward revaluation but
not after transition revaluation. Specifically, during the learning phase, an SR learner would
learn the successor states for each initial state (the SR itself, i.e. s → s ; s → s ). In
1 5 2 6
ˆ
the reward revaluation condition, it would then update its reward estimates (R) for the
terminal states (s $1; s $10) during the revaluation phase, much like the model-based
5 6
learner. Then, during the test phase, it would combine the updated reward estimates with
the SR to compute the updated values of the initial states (s → s $1; s → s $10),
1 5 2 6
allowing it to choose the better one (s ). In contrast, in the transition revaluation condition,
2
the SR learner would not have an opportunity to update the SR of the initial states (s and
1
s ) since they are never presented during the revaluation phase, much like the model-free
2
learner. Then, during the test phase, it would combine the unchanged reward estimates with
its old but now incorrect SR to produce incorrect estimates for the initial states (s → s
1 5
$10; s → s $1) and choose the worse one (s ).
2 6 1
The pattern of human responses showed evidence of both model-based and SR learning:
participantsweresensitivetobothrewardandtransitionrevaluations, consistentwithmodel-
based learning, but they performed significantly better after reward revaluations, consistent
with SR learning. To rule out the possibility that this effect can be attributed to pure model-
ˆ ˆ
basedlearningwithdifferentlearningratesforreward(R)versustransition(T)estimates,the
researchers extended this Pavlovian design to an instrumental design in which participants’
34choices (i.e., their policy π) altered the trajectories they experienced. Importantly, this
would correspondingly alter the learned SR: unrewarding states would be less likely under
a good policy and hence not be prominent (or not appear at all) in the SR for that policy.
Such states could thus get overlooked by an SR learner if they suddenly became rewarding.
This subtle kind of reward revaluation (dubbed policy revaluation by the researchers) also
relies on changes in the reward structure R, but induces predictions similar to the transition
revaluation condition: SR learners would not adapt quickly, while model-based learners
would adapt just as quickly as in the regular reward revaluation condition.
Human responses on the test phase after policy revaluation were similar to responses
after transition revaluation but significantly worse than responses after reward revaluation,
ˆ ˆ
thus ruling out a model-based strategy with different learning rates for R and T. Overall, the
authors interpreted their results as evidence of a hybrid model-based-SR strategy, suggesting
that the human brain can adapt to changes in the environment both by updating its internal
model of the world and by learning and leveraging its cached predictive representations
(see also Kahn and Daw, 2023, for additional human behavioral data leading to similar
conclusions).
5.2 Multi-task learning
In the previous section, we saw that humans can adapt quickly to changes in the reward
structure of the environment (reward revaluation), as predicted by the SR (Momennejad
et al., 2017). Here we take this idea further and propose that humans use something like the
GPI algorithm to generalize across multiple tasks with different reward functions (§2.5.1;
Barreto et al., 2017, 2018, 2020).
In Tomov et al. (2021), participants were presented with different two-step tasks that
shared the same transition structure but had different reward functions determined by the
reward weights w (see Figure 5B for an illustration). Each state s was associated with
different set of features φ(s), which were valued differently depending on the reward weights
w for a particular task. On each training trial, participants were first shown the weight
vector w for the current trial, and then asked to navigate the environment in order to
train
maximize reward. At the end of the experiment, participants were presented with a single
test trial on a novel task w .
new
The main dependent measure was participant behavior on the test task, which was de-
signed (along with the training tasks, state features, and transitions) to distinguish between
several possible generalization strategies. Across several experiments, Tomov et al. (2021)
found that participant behavior was consistent with SF and GPI. In particular, on the first
(and only) test trial, participants tended to prefer the training policy that performed better
on the new task, even when this was not the optimal policy. This “policy reuse” is a key
behavioral signature of GPI. This effect could not be explained by model-based or model-
free accounts. Their results suggest that humans rely on predictive representations from
previously encountered tasks to choose promising actions on novel tasks.
355.3 Associative learning
RL provides a normative account of associative learning, explaining how and why agents
ought to acquire long-term reward predictions based on their experience. It also provides a
descriptive account of a myriad of phenomena in the associative learning literature (Sutton
and Barto, 1990; Niv, 2009; Ludvig et al., 2012). Two recent ideas have added nuance to
this story:
• Bayesian learning. Animals represent and utilize uncertainty in their estimates.
• Context-dependent learning. Animals partition the environment into separate
contexts and maintain separate estimates for each context.
We examine each idea in turn and then explore how they can be combined with the SR.
In brief, the key idea is that animals learn a probability distribution over context-dependent
predictive representations.
5.3.1 Bayesian RL
While standard RL algorithms learn point estimates of different unknown quantities like the
ˆ ˆ
transition function (T) or the value function (V), Bayesian RL posits that agents treat such
unknown quantities as random variables and represent beliefs about them as probability
distributions.
Generally, Bayesian learners assume a generative process of the environment according
to which hidden variables give rise to observable data. The hidden variables can be inferred
by inverting the generative process using Bayes’ rule. In one example of Bayesian value
learning (Gershman, 2015), the agent assumes that the expected reward R(s,w) is a linear
combination of the observable state features φ(s):8
R(s,w) = φ(s)⊤w (62)
where the hidden weights w are assumed to evolve over time according to the following
dynamical equations:
w ∼ N(0,Σ ), (63)
0 0
w ∼ N(w ,qI) (64)
t t−1
r ∼ N(R(s ,w ),σ2). (65)
t t t R
In effect, this means that each feature (or stimulus dimension) is assigned a certain reward
weight. The weights are initialized randomly around zero (with prior covariance Σ ) and
0
evolve according to a random walk (with volatility governed by the transition noise variance
q). Observed rewards are given by the linear model (Eq. 62) plus zero-mean Gaussian noise
with variance σ2.
R
8Note that the linear reward model is the same as assumed in much of the SF work reviewed above (see
Eq. 29).
36This formulation corresponds to a linear-Gaussian dynamical system (LDS). The pos-
terior distribution over weights given the observation history H = (s ,r ) follows from
t 1:t 1:t
Bayes’ rule:
p(H |w )p(w )
p(w |H ) = t t t = N(w ;wˆ ,Σ ). (66)
t t p(H ) t t t
t
The posterior mean wˆ and covariance Σ can be computed recursively in closed form using
t t
the Kalman filtering equations:
wˆ = wˆ +k δ (67)
t t−1 t t
Σ = Σ +qI−λ kk⊤, (68)
t t−1 t t
where
• δ = r −φ(s )⊤wˆ is the reward prediction error.
t t t t
• λ = φ(s )⊤(Σ +qI)φ(s )+σ2 is the residual variance.
t t t t R
• k = (Σ +qI)φ(s)/λ is the Kalman gain.
t t
This learning algorithm is generalizes the seminal Rescorla-Wagner model of associative
learning (Rescorla and Wagner, 1972), and its update rule bears resemblance to the error-
driven TD update (Eq. 11). However, there are a few notable distinctions from its non-
Bayesian counterparts:
• Uncertainty-modulated updating. The learning rate corresponds to the Kalman
gain k , which increases with posterior uncertainty (the diagonals of the posterior
t
covariance, Σ ).
t
• Non-local updating. The update is multivariate, affecting the weights of all features
simultaneously according to k . This means that weights of “absent” features (i.e.,
t
where φ (s) = 0) can be updated, provided those features have non-zero covariance
i
with observed features.
These properties allow the Kalman filter to explain a number of phenomena in associa-
tive learning that elude non-Bayesian accounts (Dayan and Kakade, 2000; Kruschke, 2008;
Gershman, 2015). Uncertainty-modulated updating implies that nonreinforced exposure to
stimuli, or even just the passage of time, can impact future learning. For example, in a phe-
nomenon known as latent inhibition, pre-exposure to a stimulus reduces uncertainty, which
in turn reduces the Kalman gain, retarding subsequent learning for that stimulus. Non-
local updating implies that learning could occur even for unobserved stimuli, if they covary
with observed stimuli according to Σ . For example, in a phenomenon known as backward
t
blocking, reinforcing two stimuli simultaneously (a compound stimulus, φ(s) = [1,1]) and
subsequently reinforcing only one of the stimuli (φ(s′) = [1,0]) reduces reward expectation
37for the absent stimulus (φ(s′′) = [0,1]) due to the learned negative covariance between the
two reward weights (Σ < 0).
1,2
Kalman filtering can also been extended to accommodate long-term reward expectations
in an algorithm known as Kalman TD (Geist and Pietquin, 2010) by replacing the state
features φ(s) with their discounted temporal difference:
h = φ(s )−γφ(s ). (69)
t t t+1
This recovers the Kalman filter model described above when the discount factor γ is set to
0. Importantly, this model provides a link between the Bayesian approach to associative
learning and model-free reinforcement learning algorithms; we exploit this link below when
we discuss Bayesian updating of beliefs about predictive representations. Gershman (2015)
showed that Kalman TD can explain a range of associative learning phenomena that are
difficulttoexplainbyBayesianmyopic(γ = 0)modelsandnon-myopic(γ > 0)non-Bayesian
models.
5.3.2 Context-dependent learning
Inadditiontoaccountingforuncertaintyintheirestimates, animalshavebeenshowntolearn
different estimates in different contexts (Redish et al., 2007). This kind of context-dependent
learning can also be cast in a Bayesian framework (Courville et al., 2006; Gershman et al.,
2010) by assuming that the environment is carved into separate contexts according to a
hypothetical generative process. One possible formalization is the Chinese restaurant process
(CRP), in which each observation is conditioned on a hidden context z that is currently
t
active and evolves according to:

N if k is a previously sampled context
P(z = k | z ) ∝ k (70)
t 1:t−1 α otherwise
where N
k
= P it =− 11 I[z
i
= k] is the number of previous time steps assigned to context k and
α ≥ 0 is a concentration parameter, which can be understood as controlling the effective
number of contexts.9
If each context is assigned its own probability distribution over observations, then infer-
ring a given context k is driven by two factors:
• How likely is the current observation under context k?
• How likely is context k given the previous context assignments z (Eq. 70)?
1:t−1
Inparticular, agivencontextismorelikelytobeinferredifthecurrentobservationsaremore
likely under its observation distribution and/or if it has been inferred more frequently in the
past (that is, if past observations have also been consistent with its observation distribution).
9Under the CRP, the expected number of contexts after t time steps is αlogt.
38Conversely,ifthecurrentobservationsareunlikelyunderanypreviouslyencounteredcontext,
a new context is induced with its own observation distribution.
This formulation has accounted for a number of phenomena in the animal learning lit-
erature (Gershman et al., 2010). For example, it explains why associations that have been
extinguished sometimes reappear when the animal is returned to the context in which the
association was first learned, a phenomenon known as renewal. It also provides an explana-
tion of why the latent inhibition effect is attenuated if pre-exposure to a stimulus occurs in
one context and reinforcement occurs in a different context.
Recently, a similar formulation has been used to explain variability in the way hip-
pocampal place cells change their firing patterns in response to changes in contextual cues,
a phenomenon known as remapping (Sanders et al., 2020). On this view, the hippocampus
maintains a separate cognitive map of the environment for each context, and hippocampal
remapping reflects inferences about the current context.
5.3.3 Bayesian learning of context-dependent predictive maps
Asdiscussedfurtherin§6.2, Stachenfeldetal.(2017)showedthatmanyaspectsofhippocam-
pal place cell firing patterns are consistent with the SR, suggesting that the hippocampus
encodes a predictive map of the environment. In this light, the view that the hippocampus
learns different maps for different contexts (Sanders et al., 2020) naturally points to the idea
of a context-dependent predictive map.
This idea was formalized by Geerts et al. (2023) in a model that combines SFs with
Bayesian learning and context-dependent learning. Their model learns a separate predictive
map of the environment for each context, where each map takes the form of a probability
distribution over SFs (Figure 5C). The generative model assumes that the SF is given by a
linear combination of state features:
ψ (s) = φ(s)⊤m , (71)
j,t j,t
where m is a vector of weights for predicting successor feature j. Analogous to the Kalman
j
TD model described above, Geerts et al. assume that the weight vectors for all features
evolve over time according to the following LDS:
m ∼ N(µ ,Σ ) (72)
j,0 0 0
m ∼ N(m ,qI) (73)
j,t j,t−1
φ (s ) ∼ N(h⊤m ,σ2). (74)
j,t t t j,t−1 φ
This means that the weight vector is initialized randomly (with prior mean µ and variance
0
Σ ) and evolves randomly according to a random walk (with transition noise variance q).
0
Observed features φ(s) are noisy differences in the SFs of successive states (with observation
noise variance σ2). As in Kalman TD, the posterior over the weight vector for feature j is
φ
also Gaussian with mean mˆ and variance Σ which can be computed using Kalman filtering
j
equations essentially the same as those given above (Eq. 67-68).
39The authors generalize this to multiple contexts using a switching LDS: each context has
a corresponding LDS and SF weights. The contexts themselves change over time based on
a “sticky” version of the Chinese restaurant process, which additionally favors remaining in
the most recent context:

P(z = k | z ) ∝
N
k
+νI[z
t−1
= k] if k is a previously sampled context
, (75)
t 1:t−1 α otherwise
where ν ≥ 0 is the “stickiness” parameter dictating how likely it is to remain in the same
context.
Thus a given context k is more likely to be inferred if:
• The current observations are consistent with its SFs (Eq. 74).
• It was encountered frequently in the past (driven by the N term in Eq. 75).
k
• It was encountered recently (driven by the I[z
t−1
= k] term in Eq. 75).
Conversely, a new context is more likely to be inferred if observations are inconsistent with
the current SFs (i.e., when there are large SF prediction errors).
The authors show that this model can account for a number of puzzling effects in the
animal learning literature that pose problems for both point estimation (TD learning) of the
SR/SF (Eq. 17-18) and Bayesian RL (Eq. 67-68). One example is the opposing effect that
pre-exposure to a context can have on learning. Brief exposure to a context can facilitate
learning (context pre-exposure facilitation), while prolonged exposure can inhibit learning
(latent inhibition; Kiernan and Westbrook, 1993). Context pre-exposure facilitation by itself
can be accounted for by TD learning of the SR alone (Stachenfeld et al., 2017): during
pre-exposure, the animal learns a predictive representation which facilitates propagation of
newly learned values. Latent inhibition by itself can be accounted for by Bayesian RL alone
(Gershman, 2015), as discussed previously: prolonged exposure reduces value uncertainty,
in turn reducing the Kalman gain k (the effective learning rate) and inhibiting learning
t
of new values. Kalman learning of SFs combines these two processes and can thus resolve
the apparent paradox: initially, the animal learns a predictive representation of the context,
whichfacilitateslearning,whereasafterprolongedexposure,thiseffectisoffsetbyareduction
in value uncertainty, which inhibits learning.
Another puzzling effect is the partial transition revaluation observed in Momennejad
et al. (2017) and discussed in §5.1, which cannot be accounted for by TD learning of the
SR. This led Momennejad et al. to propose a hybrid model-based-SR strategy that relies
on offline simulations. Kalman TD offers a more parsimonious account based on non-local
updating that does not appeal to model-based simulations. In particular, the covariance
matrix Σ learned during the learning phase captures the relationship between the initial
t
states (s and s ) and subsequent states (s and s ). Updating the transitions from those
1 2 3 4
subsequent states (s → s and s → s ) during the transition revaluation phase therefore
3 6 4 5
also updates the SR for the initial states (s and s ), even though they are not encountered
1 2
during the revaluation phase.
40Non-local updating can similarly explain reward devaluation of preconditioned stimuli, a
hallmark of model-based learning (Hart et al., 2020). This is similar to reward devaluation
discussed in §5.1, except with an additional preconditioning phase during which an associ-
ation is learned between two neutral stimuli (e.g., light → tone). During the subsequent
conditioning phase, the second stimulus is associated with a rewarding outcome (e.g., tone
→ food), which is then devalued (e.g., by inducing taste aversion) during the devaluation
phase. Finally, the animal is tested on the first neutral stimulus (e.g., light). Note that
since the first stimulus is never present during the conditioning phase, TD learning would
not acquire an association between the first stimulus and the reward, and would thus not
exhibit sensitivity to reward devaluation (Gardner et al., 2018). In contrast, during the pre-
conditioning phase, Kalman TD learns that the two stimuli covary, allowing it to update the
SF for both stimuli during the conditioning phase and consequently propagate the updated
value to both stimuli during the devaluation phase.
Notethatthephenomenasofarcanbeexplainedwithoutappealingtocontext-dependent
learning, since the experiments take place in the same context. Context-dependent Kalman
TD can additionally explain a number of intriguing phenomena when multiple contexts are
introduced.
One such phenomenon is the context specificity of learned associations (Winocur et al.,
2009). In this paradigm, an animal learns an association (e.g., tone → shock) in one context
(e.g., Context A) and is then tested in the same context or in another context (e.g., Context
B). The amount of generalization of the association across contexts was found to depend on
elapsed time: if testing occurs soon after training, the animal only responds in the training
context(A),indicatingcontextspecificity. However, iftestingoccursafteradelay, theanimal
responds equally in both contexts A and B, indicating contextual generalization. Even more
intriguingly, this effect is reversed if the animal is briefly reintroduced to the training context
(A) before testing, in which case responding is once again context-specific—a hippocampus-
dependent “reminder” effect.
The context-dependent model readily accounts for these effects. Shortly after training,
the uncertainty of the SR assigned to Context A is low (i.e., the animal is confident in its
predictive representation of Context A). Introduction to Context B therefore results in a
large prediction error, leading the animal to (correctly) infer a new context with a new SR,
leading to context-specific responding. However, as more time elapses, the uncertainty of
the SR assigned to Context A gradually increases (i.e., the animal becomes less confident in
its predictive representation of Context A, a kind of forgetting). Introduction to Context B
then results in a smaller prediction error, making it likely that the new observations are also
assigned to Context A, leading to generalization across contexts. Brief exposure to Context
A reverses this effect by reducing the uncertainty of the SR assigned to Context A (i.e.,
the animal’s confidence in its predictive representation of Context A is restored, a kind of
remembering), leading once again to context-specific responding.
Recallthatthedurationofcontextpre-exposurehasopposingeffectsonlearning, initially
facilitatingbutsubsequentlyinhibitinglearning(KiernanandWestbrook,1993). Butwhatif
theanimalistestedinadifferentcontext? Inafollow-upexperiment,KiernanandWestbrook
41(1993) showed that longer pre-exposure to the training context leads to less responding in
the test context, indicating that the learned association is not generalized. That is, longer
context pre-exposure has a monotonic inhibitory effect on generalization across contexts.
The context-dependent model can account for this with the same mechanism that accounts
for context pre-exposure facilitation: longer pre-exposure to the training context reduces the
uncertainty of its predictive estimate, leading to greater prediction errors when presented
with the text context, and increasing the probability that the animal will infer a new context,
leading to context-specific responding.
Overall, the results of Geerts et al. (2023) suggest that, rather than encoding a single
monolithic predictive map of the environment, the hippocampus encodes multiple separate
predictive maps, each associated with its own context. Both the context and the predictive
map are inferred using Bayesian inference: learning of the predictive map is modulated by
uncertainty, and supports non-local updating. A new context is inferred when the current
predictive map fails to account for current observations.
5.4 Spatial navigation
A rich body of work points to the hippocampus as encoding a kind of cognitive map of
the environment that mammals rely on for navigation in physical and abstract state spaces
(O’Keefe and Dostrovsky, 1971; O’Keefe and Nadel, 1978). As we discussed in the previous
section, and as we will discuss further in §6.2, this cognitive map is best understood as a pre-
dictive map in which states predict future expected states, consistent with the SR (Stachen-
feld et al., 2017). Yet despite many studies of navigation in humans and rodents—key model
species used to study spatial navigation (Epstein et al., 2017; Ekstrom and Ranganath, 2018;
Ekstrom et al., 2018; Gahnstrom and Spiers, 2020; Nyberg et al., 2022; Spiers and Barry,
2015)—until recently there was no direct comparison of human and rodent navigation in
a homologous task. This left open the question of whether spatial navigation in mammals
consistently relies on the same SR-dependent strategy.
A recent study by de Cothi et al. (2022) filled this gap by designing a homologous
navigation task for humans and rats. They devised a configurable open-field maze that
could be reconfigured between trials, allowing experimenters to assess a hallmark aspect of
spatial navigation: the ability to efficiently find detours and shortcuts. The maze consisted
of a 10-by-10 grid in which squares could be blocked off by the experimenters. The maze
was instantiated in a physical environment for rats and in a virtual reality environment for
humans.
On each trial, the participant was placed at a starting location and had to navigate to a
goal location to receive a reward (Figure 5D). The starting location varied across trials, while
the goal remained hidden at a fixed location throughout the experiment. Keeping the goal
location unobservable ensured that participants could not rely on simple visual heuristics
(e.g., proximity to the goal). At the same time, keeping the goal location fixed ensured that,
once it is identified, the key problem becomes navigating to it, rather than re-discovering it.
During the training phase of the experiment, all squares of the grid were accessible, allowing
participants to learn an internal map of the environment. During the test phase, participants
42were sequentially presented with 25 different maze configurations in which various sections
of the maze were blocked off. Participants completed 10 trials of each configuration before
moving on to the next.
Using this task, the authors compared human and rat navigation with three types of RL
algorithms:
• Model-free agent (§2.2). No internal map of the environment; optimal policy is
based on state-action value function Q, which is learned from experience (specifically,
using Q-learning (Eq. 11) with eligibility traces).
• Model-based agent(§2.2). Fullinternalmapoftheenvironment(transitionstructure
T and reward function R) is learned from experience; optimal policy is computed using
tree search at decision time (specifically, using A* search).
• SR agent (§2.3). Predictive map of the environment (SR matrix M and reward
function R) is learned from experience; optimal policy is computed by combining SR
and reward function (Eq. 19).
The key question that the authors sought to answer was which RL strategy best explains
human and rat navigation across the novel test configurations. Across a wide range of
analyses, the authors observed a consistent trend: both human and rodent behavior was
most consistent with the SR agent. Humans also showed some similarity to the model-based
agent, but neither species was consistent with the model-free agent.
First,theauthorssimulatedeachRLagentgenerativelyonthesametrialsastheparticipants—
i.e., they let the RL agent navigate and solve each trial as a kind of simulated participant,
learning from its own experience along the way. These closed-loop10 simulations show what
overall participant behavior would look like according to each RL strategy. This revealed
that:
• Model-free agents struggle on new maze configurations due to the slow learning of
the Q-function, which takes many trials to propagate values from the goal location to
possible starting locations.
• Model-based agents generalize quickly to new maze configurations, since local updates
to the transition structure T can be immediately reflected in the tree search algorithm.
• SR agents generalize faster than model-free but more slowly than model-based agents,
since updates to the SR matrix M reach farther than updates to the Q-function, but
still require several trials to propagate all the way to the possible starting locations.
Second, the authors clamped each RL agent to participant behavior—i.e., they fed the
agent the same sequence of states and actions experienced by a given participant. These
10We refer to the them as “closed-loop” since, at each step, the output of the RL agent (its action) is fed
back to change its position on the grid, influencing the agent’s input at the following step, and so on.
43open-loop simulations show what the participant would do at each step if they were follow-
ing a given RL strategy.11 By matching these predictions with participant behavior using
maximum likelihood estimation of model parameters, the authors quantified how consistent
step-by-step participant behavior is with each RL strategy. For both humans and rats, this
analysis revealed the greatest similarity (i.e., highest likelihood) with the SR agent, followed
by the model-based agent, with the model-free agent showing the least similarity (i.e., lowest
likelihood).
Third, the authors combined the above approaches by first training each fitted RL agent
with the state-action sequences observed by a participant on several maze configurations and
then simulating it generatively on another configuration. This hybrid open-loop training
(on past configurations) / closed-loop evaluation (on a new configuration) provides a more
global view than the step-by-step analysis above, by allowing comparison of predicted and
participant trajectories, rather than individual actions. This led to several findings:
• Configurations that were challenging for the SR agent were also challenging for bio-
logical agents, and vice versa. This pattern was less consistent for model-based and
model-free agents.
• Overall directedness and direction of participant trajectories (quantified by linear and
angular diffusivity) was most similar to SR trajectories.
• The step-by-step distance between participant and SR trajectories was consistently
lower compared to model-based and model-free trajectories.
All of these analyses show that the SR agent best explains both human and rat behavior.
Overall, the results of de Cothi et al. (2022) indicate that spatial navigation across mam-
malian species relies on a predictive map that is updated from experience in response to
changes in the environment.
5.5 Memory
As mentioned previously and discussed further in §6.2, the hippocampus supports spatial
navigation by encoding a predictive map of the environment that resembles the SR (Stachen-
feld et al., 2017). A rich body of work has also implicated the hippocampus and the adjacent
medial temporal lobe structures in another high-level cognitive function: episodic memory
(Ranganath, 2010). In this section, we review an influential model of episodic memory, the
temporal context model (TCM; Howard and Kahana, 2002), through the lens of RL, and
show that it can be partially understood as an estimator for the SR (Gershman et al., 2012).
We then discuss how this property can be used in a powerful decision-making algorithm that
bridges episodic memory and reinforcement learning systems (Zhou et al., 2023).
11We refer to them as “open-loop” since, at each step, the output of the RL agent has no effect on its
subsequent inputs or outputs.
445.5.1 The temporal context model
TCM is an influential model of memory encoding and retrieval originally designed to account
for a number of phenomena in free recall experiments (Howard and Kahana, 2002). In these
experiments, participants are asked to study a list of items and then recall as many of them
as they can, in any order. Experimenters observed that recall order is often not, in fact,
arbitrary: participants show better recall for recently studied items (the recency effect), and
tend to recall adjacent items in the list one after the other (the contiguity effect).
TCM accounts for these phenomena by positing that the brain maintains a temporal
context: a slowly drifting internal representation of recent experience that gets bound to
specific experiences (memories) during encoding, and can serve as a cue to bring those
experiences to mind during retrieval. When participants begin recalling the studied items,
the temporal context is most similar to the context associated with recently studied items
(due to the slow drift), which is why they are recalled better (the recency effect). Recalling
items reactivates the context associated with those items, which is similar to the context for
adjacent items (again, due to the slow drift), which is why they tend to be recalled soon
after (the contiguity effect). Neural evidence for drifting context comes from the finding
that lingering brain activity patterns of recent stimuli predicted whether past and present
stimuli are later recalled together (Chan et al., 2017). Human brain recordings have also
provided evidence for temporal context reactivation during recall (Gershman et al., 2013;
Folkerts et al., 2018).
Temporal context can be formalized as weighted average of recently encountered stimulus
vectors:
c = (1−ω)c +ωφ(s ), (76)
t+1 t t
where c is the current context vector, φ(s ) is the feature vector for the current stimulus
t t
s , which could correspond to a study item (in an episodic memory setting) or a state (in
t
a decision making setting), and the constant ω determines the drift rate—i.e., whether the
context evolves slowly (low ω) or quickly (high ω).
Stimuli and contexts are bound using an association matrix Mˆ which gets updated using
outer-product Hebbian learning: when a new stimulus is presented, its associations with
previous stimuli are strengthened in proportion to how active they are in the current context:
ˆ
∆M(i,j) ∝ φ (s )c . (77)
i t i,t
In early studies of this model, stimuli were encoded using one-hot vectors, φ i(s) = I[s = i],
although more flexible feature representations have been studied (e.g., Socher et al., 2009;
Howard et al., 2011).
5.5.2 TCM as an estimator for the SR
Note the similarity between the TCM learning rule (Eq. 77) and the SR TD learning rule
(Eq. 17 in §2.3). There are two main distinctions:
45• The TCM update (Eq. 77) is modulated by the context vector c, which is absent from
the SR TD update (Eq. 17).
• The Hebbian update in TCM lacks the prediction error terms from the SR TD error
δ (j) (Eq. 18).
M
The first distinction can be removed by setting a maximum drift rate of ω = 1 in Eq. 76,
which ensures that the context is always updated to the latest stimulus vector, c = φ(s ).
t+1 t
ˆ
For the one-hot encoding, this means that only M(s ,j) will be updated in the TCM
t−1
update, since c
i,t
= I[s
t−1
= i] in Eq. 77), just as in the SR TD update (Eq. 17). Conversely,
introducing a context term in the SR TD update (Eq. 17) results in a generalization of TD
learning using an eligibility trace (Sutton and Barto, 2018), a running average of recently
visited states. This is mathematically equivalent to temporal context (Eq. 76), and can
sometimes lead to faster convergence. In this way, the TCM learning rule is a generalization
of the vanilla SR TD learning rule that additionally incorporates eligibility traces.
The second distinction suggests a way to, in turn, generalize the TCM update rule by
replacing the Hebbian term with the prediction error term δ (j) (Eq. 18) from the SR TD
M
update. This leads to the following revised update equation for TCM:
ˆ
∆M(i,j) ∝ δ (j)c , (78)
M i,t
where δ (j) is the same as in Eq. 18. Gershman et al. (2012) showed that this new variant
M
of TCM can be understood as directly estimating the SR using TD learning, with temporal
context serving as an eligibility trace. It differs from the original version of TCM in two
keys ways. First, learning is error-driven rather than purely Hebbian, which means that
association strength only grows if there is a discrepancy between predicted and observed
stimuli (see DuBrow et al., 2017, for a discussion of empirical evidence). Second, associations
areadditionallylearnedbetweenthecontextandfutureexpectedstimuli, notjustthepresent
stimulus.
This new interpretation of TCM posits that the role of temporal context is to learn
predictions of future stimuli, rather than to merely form associations. This makes several
distinct predictions from the original version of TCM, one of which is the context repetition
effect: repeating the context in which a stimulus was observed should strengthen memory for
that stimulus, even if the stimulus itself was not repeated. This prediction was validated in
a study by Smith et al. (2013). The authors showed participants triplets of stimuli (images
in one experiment and words in another experiment), with the first two stimuli in each
triplet serving as context for the third stimulus. Participants were then presented with
an item-recognition test in which they had to indicate whether different stimuli are either
“old” or “new”. Memory performance was quantified as the proportion of test items correctly
recognizedasold. Thekeyfindingwasthatmemorywasbetterforstimuliwhosecontextwas
presented repeatedly, even if the stimuli themselves were only presented once. This held for
different modalities (images and words), and did not occur when context was generally not
predictive of stimuli. These findings (see also Manns et al., 2015) substantiate the predicted
context repetition effect and lend credence to idea that TCM learns predictions rather than
mere associations.
465.5.3 Combining TCM and the SR for decision making
The theoretical links between TCM and the SR point to a broader role for episodic memory
in RL. Previous studies have implicated the hippocampus in prediction and imagination
(Buckner, 2010), as well as replay of salient events (Momennejad et al., 2018), consistent
with some form of model-based RL or a successor model (§2.4). More generally, episodic
memory is thought to support decision making by providing the ingredients for simulating
possible futures (Schacter et al., 2015). This idea is corroborated by studies of patients with
episodic memory deficits, who also tend to show deficits on decision making tasks (Gupta
et al., 2009; Gutbrod et al., 2006; Bakkour et al., 2019). A related body of work focuses on
decision-by-sampling algorithms, according to which humans approximate action values by
sampling from similar past experiences from memory (Plonsky et al., 2015; Bornstein et al.,
2017; Lieder et al., 2018).
These loosely connected ideas were knitted together in a theoretical proposal by Zhou
et al. (2023) that builds upon the links between TCM and the SR, showing precisely how a
predictive version of TCM can support adaptive decision making. Their model incorporates
two key ideas (Figure 5E):
• During encoding (Figure 5E, top), incoming feature vectors φ(s ) update a slowly
t
drifting context c (Eq. 76). This context vector serves as an eligibility trace in a TD
t
update rule (Eq. 78) that learns the SR estimate Mˆ (Gershman et al., 2012).
• During retrieval (i.e., at decision time; Figure 5E, middle and bottom), possible future
stimulixaresampledforeachactionusingatree-searchalgorithmthatusestheSRasa
world model: p(s˜ ) ∝ Mˆ c , where τ indexes time steps at retrieval, and s˜ corresponds
τ τ 0
to the initial state of the retrieval process (the query stimulus, defined as the root of
the tree). Retrieval unfolds by recursively sampling states from this process. The
corresponding rewards then are averaged to compute a Monte Carlo value estimate for
each action.
During the tree search, the context vector c can be updated with the sampled feature vector
τ
φ(s ) to varying degrees, dictated by the drift rate ω (Eq. 76). This spans a continuum
t
between updating and retrieval regimes.
At one extreme, if the drift rate during retrieval is set to its lowest value (ω = 0), the
context is never updated after being initialized with the query stimulus (c = x = φ(s˜ )∀τ).
τ 0 0
This results in independent and identically distributed samples from the normalized SR. In
the limit of infinite samples, this reduces to simply computing action values by combining
the reward function and the SR (Eq. 19), as discussed in §5.1 and §5.2. For finite samples,
this produces an unbiased estimator of action values. Note that this estimate is only as
accurate as the SR matrix Mˆ , which is itself an estimate of the true SR matrix M. Hence,
this regime inherits all the pros and cons of using the SR (§2.3 and §5.1): it can be used to
efficiently compute action values, and it can adapt quickly to changes in the reward structure
R, but not the transition structure T of the environment.
At the other extreme, if the drift rate during retrieval is set to its highest value (ω = 1),
the context is always updated to the latest sampled stimulus (c = φ(s˜ )). If the discount
τ τ
47factor is minimal, γ = 0, the SR reduces to the one-step transition matrix (i.e., M = T) and
thesampledstimulis arenolongerindependentandidenticallydistributed,butinsteadform
τ
a trajectory through state space that follows the transition structure T and corresponds to a
single Monte Carlo rollout. Averaging rewards from such Monte Carlo rollouts also produces
an unbiased estimator of value (Sutton and Barto, 2018). This regime thus corresponds to
a fully model-based algorithm (§2.2) and inherits all the pros and cons of that approach:
it takes longer to compute action values (since trajectories need to be fully rolled out to
produce unbiased estimates, requiring more samples), but it can adapt quickly to changes
in both the reward structure R and the transition structure T of the environment.
Between these extremes lies a continuum (0 < ω < 1) that trades off between a sampling
approximation of the SR (ω → 0) and model-based Monte Carlo rollouts (ω → 1). Indeed,
results from free recall experiments are consistent with such an intermediate regime (Howard
and Kahana, 2002), indicating that context is partially updated during retrieval. This also
raises the intriguing possibility that the brain navigates this continuum by dynamically
adjusting the drift rate in a way that balances the pros and cons of both regimes, similarly
to the way in which the brain arbitrates between model-based and model-free RL systems
(Kool et al., 2018).
The authors also demonstrate how emotionally salient stimuli, such as high rewards,
can modulate learning by producing a higher learning rate for the SR update (Eq. 78).
This introduces a kind of bias-variance trade-off: the resulting SR skews towards stimuli
that were previously rewarded, which could speed up convergence (lower variance) but also
induce potentially inaccurate action values (higher bias). Finally, the authors show how
initiating the tree search with a retrieval context c that is associated with but different
0
from the query stimulus feature vector φ(s˜ ) can lead to bidirectional retrieval. This is
0
consistent with bidirectional recall in human memory experiments, and can be advantageous
in problems where state transitions can be bidirectional, such as spatial navigation.
In summary, the modeling and simulation results of Zhou et al. (2023) demonstrate how
a variant of TCM can be viewed as an estimator of the SR, and can serve as the basis for
a flexible sampling-based decision algorithm that spans the continuum between vanilla SR
and fully model-based search. This work illustrates how episodic memory can be integrated
with predictive representations to explain cognitive aspects of decision making.
6 Neuroscience applications
The links between cognitive processes and SR-like decision-making naturally leads to ques-
tions of how these mechanisms might be implemented neurally. We’ve already hinted at one
answer to this question: medial temporal lobe regions, and in particular the hippocampus,
appeartoencodepredictiverepresentations. Inthefollowingsubsections, wereviewevidence
for this claim and efforts to formalize its mechanistic basis in neurobiology.
486.1 A brief introduction to the medial temporal lobe
Before discussing evidence for predictive representations, it is important to take a sufficiently
broad view of the medial temporal lobe’s functional organization. Not everything we know
about these regions fits neatly into a theory of predictive representations. Indeed, classical
views are quite different, emphasizing spatial representation and episodic memory.
As outlined in §5.4, extensive evidence identifies the hippocampus and associated cor-
tical regions as providing a neural-level representation of space, often conceptualized as a
cognitive map (O’Keefe and Nadel, 1978; Morris et al., 1982). Integral to this framework
are the distinct firing patterns of various cell types found in structures across the hippocam-
pal formation (Figure 6). Place cells, in regions CA3 and CA1, offer a temporally stable,
sparse representation of self-location able to rapidly reorganize in novel environments—the
phenomenon of remapping introduced earlier (O’Keefe and Dostrovsky, 1971; Muller and
Kubie, 1987; Bostock et al., 1991).
Subiculum and dentate gyrus also contain spatially modulated neurons with broadly
similar characteristics, the former tending to be diffuse and elongated along environmental
boundaries (Lever et al., 2009) while the latter are extremely sparse (Jung and McNaughton,
1993; Leutgeb et al., 2007). In contrast, in medial entorhinal cortex (mEC), the primary
cortical partner of hippocampus, the spatially periodic firing patterns of grid cells effectively
tile the entire environment (Figure 6) and are organized into discrete functional modules of
different scale (Hafting et al., 2005; Barry et al., 2007; Stensola et al., 2012). The highly
structured activity of grid cells has provoked a range of theoretical propositions pointing
to roles in path integration (McNaughton et al., 2006; Burgess et al., 2007), vector based
navigation (Bush et al., 2015; Banino et al., 2018), and as an efficient basis set for spatial
generalization (Whittington et al., 2020). Notably, mEC also contains a ‘zoo’ of other cell
types with functional characteristics related to self-location, including head direction cells
(Sargolini et al., 2006), border cells (Solstad et al., 2008), speed cells (Kropff et al., 2015),
and multiplexed conjunctive responses (Sargolini et al., 2006; Hardcastle et al., 2017).
In summary, the medial temporal lobe exhibits remarkable functional diversity. We now
turn to the claim that predictive principles offer a unifying framework for understanding
aspects of this diversity.
6.2 The hippocampus as a predictive map
Accumulating evidence indicates that neurons within the hippocampus and its surrounding
structures, particularly place and grid cells, demonstrate predictive characteristics consistent
with a predictive map of spatial states. Stachenfeld et al. (2017) were the first to articulate
this perspective, establishing a connection between the responses of hippocampal neurons
andtheSR.Theyarguedthatplacecellswerenotinherentlyrepresentingtheanimal’sspatial
location, but rather its expectations about future spatial locations. Specifically, they argued
that the receptive fields of places cells correspond to columns of the SR matrix Mπ (Figure
7, left). This implies that each receptive field is a retrodictive code, in the sense that the
cells are more active in locations that tend to precede the cell’s “preferred” location (i.e., the
49Figure 6: Spatial representations in the medial temporal lobe. As a rodent navigates
space (e.g., a rectangular arena; top left), place cells recorded in regions CA1 and CA3 of
Hippocampus fire in stable, sparse representations of self-location (bottom left; hot colors
indicateincreasedneuronalactivity). Conversely,gridcellsinneighbouringmedialentorhinal
cortex (EC; bottom right), which provides input to both CA1 and CA3 directly, as well as
CA3 via dentate gyrus (DG; solid arrows imply directional connectivity between regions),
have spatially periodic hexagonal firing patterns tiling the entire environment. Additionally,
boundary responsive neurons in both medial entorhinal cortex and subiculum (SUB) fire
when the animal occupies specific positions relative to external and internal environmental
boundaries.
50locationofthepeakfiring). Thepopulationactivityofplacecellsatagiventimecorresponds
to a row of the SR matrix; this is a predictive code, in the sense that they collectively encode
expectations about upcoming states.
In line with this hypothesis, the tuning of place fields (Figure 7, bottom left) are in-
fluenced by the permissible transitions afforded by an environment: they do not typically
cross environmental boundaries like walls, tending to extend along them, mirroring the tra-
jectories animals follow (Alvernhe et al., 2011; Tanni et al., 2022). Alternations made to an
environment’s layout, affecting the available paths, influence the activity of adjacent place
cells, consistentwiththeSR(Stachenfeldetal.,2017). Notably, evenchangesinpolicyalone,
such as training rats to switch between foraging and directed behaviour, can markedly al-
ter place cell firing (Markus et al., 1995), also broadly consistent with the SR. Further,
when animals are trained to generate highly stereotyped trajectories, for example repeatedly
traversing a track in one direction, CA1 place fields increasingly exhibit a backwards skew,
opposite to the direction of travel (Mehta et al., 2000), thereby anticipating the animal’s
future state. This arises naturally from learning the SR, since the upcoming spatial states
become highly predictable when agents consistently move in one direction, resulting in a
backwards skew of the successor states (Figure 7, top left). The basic effect is captured
in simple grid worlds like those used by Stachenfeld et al. (2017), but when the anchoring
is replaced with continuous feature-based methods, the successor features also capture the
backwards shift in field peak observed in neural data (Mehta et al., 2000; George et al.,
2023).
While the properties of place cells are consistent with encoding the SR, grid cells appear
to resemble the eigenvectors of the SR (Figure 7, right). Specifically, eigendecomposition of
the SR matrix Mπ yields spatially periodic structures of varying scales, heavily influenced
by environmental geometry (Stachenfeld et al., 2017). Broadly, these resemble grid cells,
but notably lack the characteristic hexagonal periodicity, except when applied to hexagonal
environments. This discrepancy, however, is likely not significant because subsequent work
indicates that biological constraints, such as non-negative firing rates (Dordek et al., 2016;
Sorscher et al., 2019), efficiency considerations (Dorrell et al., 2023), and neurobiologically
plausible state-features (de Cothi and Barry, 2020) tend to move these solutions closer to the
expected hexagonal activity patterns (Figure 8A). The key point then is that environmental
geometries that polarize the transitions available to an animal produce SR eigenvectors
with commensurate distortions, matching observations that grid firing patterns are also
deformed under such conditions (Derdikman et al., 2009; Krupic et al., 2015). Notably, this
phenomenon is also observed in open-field environments with straight boundaries, where
biological grid cells and SR-derived eigenvectors both exhibit a tendency to orient relative
to walls (Krupic et al., 2015; de Cothi and Barry, 2020). Complementary evidence comes
from virtual reality studies of human subjects, where errors in distance estimates made by
participants mirrored distortions in eigenvector-derived grid cells (Bellmund et al., 2020).
Althoughplaceandgridcellsarepredominantlyconceptualizedasspatialrepresentations,
it is increasingly clear that these neurons also represent non-spatial state spaces (Constanti-
nescu et al., 2016; Aronov et al., 2017); in some cases, activity can be interpreted as encoding
51Figure 7: Successor representation model of the hippocampus and medial entorhi-
nal cortex. As an agent explores a linear track environment in a uni-directional manner,
the SR skews backwards down the track opposite to the direction of motion (top left; hot
colors indicate increased predicted occupancy of the agent’s depicted state), as observed in
hippocampal place cells (Mehta et al., 2000). In a 2D arena, the SR forms place cell-like
sparse representations of self-location (bottom left), while the eigenvectors of the SR form
spatially periodic activity patterns reminiscent of entorhinal grid cells (top right). Similarly
to real grid cells (Krupic et al., 2015), the periodicity of these eigenvectors is deformed in
polarized environments such as trapezoids (bottom right).
52an SR over such state spaces. For example, a study by Garvert et al. (2017) showed human
participants a series of objects on a screen in what appeared to be a random order. However,
unknown to the participants, the sequence was derived from a network of non-spatial rela-
tionships, where each object followed certain others in a predefined pattern. Brain imaging
found that hippocampal and entorhinal activity mirrored the non-spatial predictive relation-
ships of the objects, as if encoded by an SR (see also Brunec and Momennejad, 2022).
6.3 Learning a biologically plausible SR
In much of the neuroscience work, SRs are formulated over discrete state spaces, facilitating
analysis and enabling direct calculation for diffusive trajectories. In spatial contexts, this
corresponds to a grid world with one-hot location encoding, a method that can produce
neurobiologically plausible representations (Stachenfeld et al., 2017). However, the brain
must use biologically plausible learning rules and features derived from sensory information,
with the choice of state-features φ(s) exerting significant influence on the resultant SFs
ψπ(s).
de Cothi and Barry (2020) employed idealized boundary vector cells (BVCs)-—neurons
coding for distance and direction to enviromental boundaries—as a basis over which to
calculate a spatial SR. BVCs have been hypothesized as inputs to place cells (Hartley et al.,
2000). They resemble the boundary-responsive cells found in the mEC (Solstad et al., 2008)
and subiculum (Barry et al., 2006; Lever et al., 2009); hence, they are plausibly available
to hippocampal circuits (Figure 8A). The SFs of these neurobiological state-features and
their eigendecomposition resemble place and grid fields, as before, but also captured more
of the nuanced characteristics of these spatially tuned neurons – for example, the way in
whichplacefieldselongatealongenvironmentalboundaries(Tannietal.,2022)andduplicate
when additional walls are introduced (Figure 8B; Barry et al., 2006). Geerts et al. (2020)
employed a complementary approach, using an SR over place cell state-features in parallel
with a model-free framework trained on egocentric features. The dynamics of the interaction
between these two elements mirrored the behavioral preference of rodents, which initially
favoramap-basednavigationalstrategy, beforeswitchingtoonebasedonbodyturns(Geerts
et al., 2020; Packard and McGaugh, 1996).
Subsequent models expanded on these ideas. While these differ in terms of implementa-
tion and focus, they employ the common idea of embedding transition probabilities into the
weights of a network using biologically plausible learning rules. Fang et al. (2023) advanced
a framework using a bespoke learning rule acting on a recurrent neural network (RNN)
supplied with features derived from experimental recordings. The network was sufficient to
calculate an SR and could do so at different temporal discounts, producing SFs that resem-
bled place cells. Bono et al. (2023) applied spike-time dependent plasticity (STDP) (Bi and
Poo, 1998; Kempter et al., 1999), a Hebbian learning rule sensitive to the precise ordering
of pre- and post-synaptic spikes, to a single-layer spiking network. Because the ordering of
spikes from spatial inputs inherently reflects the sequence of transitions between them, this
configuration also learns an SR. Indeed, the authors were able to show that the synaptic
weights learnt by this algorithm are mathematically equivalent to TD learning with an eligi-
53Figure 8: A successor representation with neurobiological state features. (A)
Boundary responsive cells, present in subiculum and entorhinal cortex, are used as state
features for learning successor features and their eigenvectors (de Cothi and Barry, 2020).
These can capture more nuanced characteristics of both place and grid cells compared to
one-hot spatial features derived from a grid world. For example, the eigenvectors have in-
creased hexagonal periodicity relative to a grid world SR model. (B) The successor features
can convey duplicated fields when additional walls are inserted in the environment, as ob-
served in real place cells (Barry et al., 2006).
54bility trace. Furthermore, temporally accelerated biological sequences, such as replay Wilson
and McNaughton (1994); Ólafsdóttir et al. (2016), provide a means to quickly acquire SRs
for important or novel routes. Finally, George et al. (2023) followed a similar approach,
showing that STDP (Bi and Poo, 1998) applied to theta sweeps—the highly ordered se-
quences of place cell spiking observed within hippocampal theta cycles (O’Keefe and Recce,
1993; Foster and Wilson, 2007)—was sufficient to rapidly learn place field SFs that were
strongly modulated by agent behavior, consistent with empirical observations. Additionally,
because the speed and range of theta sweeps is directly linked to the size of the underlying
place fields, the authors also noted that the gradient of place field sizes observed along the
dorsal-ventral axis of the hippocampus inherently approximate SFs with decreasing temporal
discounts (Kjelstrup et al., 2008; Momennejad and Howard, 2018).
The formulation used by George et al. (2023) highlights a paradox: while place fields
can serve as state-features, they are also generated as SFs. This dual role might suggest a
functional distinction between areas such as CA3 and CA1, with CA3 potentially providing
the spatial basis and CA1 representing SFs. Alternatively, spatial bases could originate from
upstreamcircuits, suchasmEC,asproposedintheFangetal.(2023)model. Furthermore, it
isconceivablethattheinitialrapidformationofplacefieldsisgovernedbyadistinctplasticity
mechanism, such as behavioral-timescale plasticity (Bittner et al., 2017). Once established,
these fields would then serve as a basis for subsequent SF learning. Such a perspective is
compatible with observations that populations of place fields in novel environments do not
immediately generate theta sweeps (Silva et al., 2015).
These algorithms learn SFs under the premise that the spatial state is fully observable,
for example by a one-hot encoding in a grid world or the firing of BVCs computed across
the distances and directions to nearby walls. However, in reality states are often only par-
tially observable and inherently noisy due to sensory and neural limitations. Vértes and
Sahani (2019) present a mechanism for how the SR can be learnt in partially observable
noisy environments, where state uncertainty is represented in a feature-based approxima-
tion via distributed, distributional codes. This method supports RL in noisy or ambiguous
environments, for example navigating a corridor of identical rooms.
In summary, the SR framework can be generalized to a state space comprised of contin-
uous, non-identical, overlapping state-features, and acquired with biological learning rules.
As such, it is plausible that hippocampal circuits could in principle instantiate a predictive
map, or close approximation, based on the mixed population of spatially modulated neurons
available from its inputs. The models reviewed above demonstrate ways in which the SR
could be learnt online during active experience. Nonetheless, much learning in the brain is
achieved offline, during periods of wakeful rest or sleep. One candidate neural mechanism
for this is replay, rapid sequential activity during periods of quiescence.
6.4 Replay
During periods of sleep and awake rest, hippocampal place cells activate in rapid sequences
thatrecapitulatepastexperiences(WilsonandMcNaughton,1994;FosterandWilson,2007).
These reactivations, known as replay, often coordinate with activity in the entorhinal (Ólafs-
55dóttiretal.,2016)andsensorycortices(JiandWilson,2007;Rothschildetal.,2017), andare
widely thought to be a core mechanism supporting system-level consolidation of experiential
knowledge (Girardeau et al., 2009; Ego-Stengel and Wilson, 2010; Ólafsdóttir et al., 2015).
In linear track environments, hippocampal place cells typically exhibit directional tuning
with firing fields that disambiguate travel in either direction (Navratilova et al., 2012). This
directionality enables two functional classes of replay to be distinguished: ‘forward replay’
where the sequence reflects the order in which the animal experienced the world, and ‘reverse
replay’wherethebehaviouralsequenceistemporallyreversed(analogoustotheanimalwalk-
ing tail-first down the track). Intriguingly, the phase of a navigation task has been shown to
influence the type of replay that occurs; for example reverse replay is associated with receipt
of reward at the end of trials, while forward replay is more abundant at the start of trials
prior to active navigation (Diba and Buzsáki, 2007).
Mattar and Daw (2018) modelled the emergence of forward and reverse replays using
a reinforcement learning agent that accesses memories of locations in an order determined
by their expected utility. Specifically, the agent prioritizes replaying memories as a balance
of two requirements: the ‘need’ to evaluate imminent choices versus the ‘gain’ from propa-
gating newly encountered information to preceding locations. The need term for a spatial
state corresponds to its expected future occupancy given the agent’s current location, thus
utilizing the definition of the SR (Eq. 13) to provide a measure for how often in the near
future that state will tend to be visited. The gain term represents the expected increase
in reward from visiting that state. This mechanism produces sequences that favor adjacent
backups: upon discovery of an unexpected reward, the last action executed will have a posi-
tive gain, making it a likely candidate for replay. Thus, value information can be propagated
backward by chaining successive backups in the reverse direction, simulating reverse replay.
Conversely, at the beginning of a trial, when the gain differences are small and the need
term dominates, sequences that propagate value information in the forward direction will
be the likely candidates for replay, prioritizing nearby backups that extend forward through
the states the agent is expected to visit in the near future.
Sequential neural activity in humans has similarly been observed to exhibit orderings
consistent with sampling based on future need. Wittkuhn et al. (2022) showed participants
a series of animal images drawn from a ring-like graph structure in either a uni- or bi-
directional manner. Using fMRI data recorded during 10s pauses between trials, Wittkuhn
etal.(2022)foundforwardandreversesequentialactivitypatternsinvisualandsensorimotor
cortices, apatternwell-capturedbyanSRmodellearntfromthegraphstructureparticipants
experienced.
6.5 Dopamine and generalized prediction errors
As described in section 2, TD learning rules provide a powerful algorithm for value esti-
mation. The elegant simplicity of this algorithm led neuroscientists to explore if, and how,
TD learning might be implemented in the brain. Indeed, one of the celebrated successes of
neuroscience has been the discovery that the activity of midbrain dopamine neurons appears
to report reward prediction errors (Schultz et al., 1997). This successfully accounts for many
56aspectsofdopamineresponsesinclassicalandinstrumentalconditioningtasks(Starkweather
and Uchida, 2021).
While elegant, the classical view that dopamine codes for a scalar reward prediction
error does not explain more heterogeneous aspects of dopamine responses. For example, the
same dopamine neurons also respond to novel and unexpected stimuli (e.g., Ljungberg et al.,
1992; Horvitz, 2000), and to errors in predicting the features of rewarding events, even when
value remains unchanged (Chang et al., 2017; Takahashi et al., 2017; Stalnaker et al., 2019;
Keiflin et al., 2019). Considering this, Gardner et al. (2018) proposed an extension to the
classic view of dopamine, suggesting that it also encodes prediction errors related to sensory
features. According to this model, dopamine reports vector-valued TD errors suitable for
updating SFs, utilizing the fact that SFs obey a Bellman equation and hence are learnable
by TD. This model explains a number of phenomena that are puzzling under a classic TD
model based only on scalar reward predictions.
First, it explains why the firing rate of dopamine neurons increases after a change in
reward identity, even when reward magnitude is held fixed (Takahashi et al., 2017): changes
in reward identity induce a sensory prediction error that shows up as one component of
the error vector. Second, it explains, at least partially, why subpopulations of dopamine
neurons encode a range of different non-reward signals (Engelhard et al., 2019; de Jong
et al., 2022; Gonzalez et al., 2023). Third, it explains why optogenetic manipulations of
dopamine influence conditioned behavior even in the absence of reward (Chang et al., 2017;
Sharpe et al., 2017).
Howdodopamineneuronsencodeavector-valuederrorsignal? Onepossibilityisthatthe
errors are distributed across population activity. Pursuing this hypothesis, Stalnaker et al.
(2019) analyzed the information content of dopamine neuron ensembles. They showed that
reward identity can be decoded from these ensembles, but not from single neuron activity.
Moreover, they showed that this information content disappeared over the course of training
following an identity switch, consistent with the idea that error signals go to 0 as learning
proceeds. The question remains how a vector-valued learning system is implemented bio-
physically. Some progress in this direction (albeit within a different theoretical framework),
has been made by Wärnberg and Kumar (2023).
7 Conclusions
The goal of this survey was to show how predictive representations can serve as the building
blocks for intelligent computation. Modern work in AI has demonstrated the power of good
representations for a variety of downstream tasks; our contribution builds on this insight,
focusing on what makes representations useful for RL tasks. The idea that representing
predictions is particularly useful has appeared in many different forms over the last few
decades, but has really blossomed only in the last few years. We now understand much
better why predictive representations are useful, what they can be used for, and how to learn
them.
57Acknowledgments
This work was supported by the Kempner Institute for the Study of Natural and Artificial
Intelligence, ARO MURI under Grant Number W911NF-21-1-0328, and the Wellcome Trust
under Grant Number 212281/Z/18/Z.
References
Abdolshah, M., Le, H., George, T. K., Gupta, S., Rana, S., and Venkatesh, S. (2021). A
new representation of successor features for transfer across dissimilar environments. In
International Conference on Machine Learning, pages 1–9. PMLR.
Adams, C. D. (1982). Variations in the sensitivity of instrumental responding to reinforcer
devaluation. The Quarterly Journal of Experimental Psychology, 34(2):77–98.
Adams, C. D. and Dickinson, A. (1981). Instrumental responding following reinforcer deval-
uation. The Quarterly Journal of Experimental Psychology Section B, 33(2b):109–121.
Alegre,L.N.,Bazzan,A.,andDaSilva,B.C.(2022). Optimisticlinearsupportandsuccessor
features as a basis for optimal policy transfer. In International Conference on Machine
Learning, pages 394–413. PMLR.
Alver, S. and Precup, D. (2021). Constructing a good behavior basis for transfer using
generalized policy updates. arXiv preprint arXiv:2112.15025.
Alvernhe, A., Save, E., and Poucet, B. (2011). Local remapping of place cell firing in the
tolman detour task. European Journal of Neuroscience, 33(9):1696–1705.
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew,
B., Tobin, J., Pieter Abbeel, O., and Zaremba, W. (2017). Hindsight experience replay.
Advances in Neural Information Processing Systems, 30.
Aronov, D., Nevers, R., and Tank, D. W. (2017). Mapping of a non-spatial dimension by
the hippocampal–entorhinal circuit. Nature, 543:719–722.
Auer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. Journal of
Machine Learning Research, 3(Nov):397–422.
Bakkour, A., Palombo, D. J., Zylberberg, A., Kang, Y. H., Reid, A., Verfaellie, M., Shadlen,
M. N., and Shohamy, D. (2019). The hippocampus supports deliberation during value-
based decisions. elife, 8:e46080.
Banino, A., Barry, C., Uria, B., Blundell, C., Lillicrap, T., Mirowski, P., Pritzel, A., Chad-
wick, M. J., Degris, T., Modayil, J., et al. (2018). Vector-based navigation using grid-like
representations in artificial agents. Nature, 557(7705):429–433.
58Barreto, A., Borsa, D., Hou, S., Comanici, G., Aygün, E., Hamel, P., Toyama, D., Mourad,
S., Silver, D., Precup, D., et al. (2019). The option keyboard: Combining skills in rein-
forcement learning. Advances in Neural Information Processing Systems, 32.
Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M., Mankowitz, D., Zidek,
A., and Munos, R. (2018). Transfer in deep reinforcement learning using successor features
and generalised policy improvement. In International Conference on Machine Learning,
pages 501–510. PMLR.
Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H. P., and Silver,
D. (2017). Successor features for transfer in reinforcement learning. Advances in Neural
Information Processing Systems, 30.
Barreto, A., Hou, S., Borsa, D., Silver, D., and Precup, D. (2020). Fast reinforcement
learningwithgeneralizedpolicyupdates. Proceedings of the National Academy of Sciences,
117(48):30079–30087.
Barry, C., Hayman, R., Burgess, N., and Jeffery, K. J. (2007). Experience-dependent rescal-
ing of entorhinal grids. Nature neuroscience, 10(6):682–684.
Barry, C., Lever, C., Hayman, R., Hartley, T., Burton, S., O’Keefe, J., Jeffery, K., and
Burgess, N.(2006). Theboundaryvectorcellmodelofplacecellfiringandspatialmemory.
Reviews in the Neurosciences, 17(1-2):71–98.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.
Bellmund, J. L., de Cothi, W., Ruiter, T. A., Nau, M., Barry, C., and Doeller, C. F.
(2020). Deformingthemetricofcognitivemapsdistortsmemory. Nature human behaviour,
4(2):177–188.
Bi, G.-q. and Poo, M.-m. (1998). Synaptic modifications in cultured hippocampal neurons:
dependence on spike timing, synaptic strength, and postsynaptic cell type. Journal of
neuroscience, 18(24):10464–10472.
Bittner, K.C., Milstein, A.D., Grienberger, C., Romani, S., andMagee, J.C.(2017). Behav-
ioraltimescalesynapticplasticityunderliesca1placefields. Science, 357(6355):1033–1036.
Bono, J., Zannone, S., Pedrosa, V., and Clopath, C. (2023). Learning predictive cognitive
maps with spiking neurons during behavior and replays. Elife, 12:e80671.
Bornstein, A. M., Khaw, M. W., Shohamy, D., and Daw, N. D. (2017). Reminders of past
choices bias decisions for reward in humans. Nature Communications, 8(1):15958.
Borsa, D., Barreto, A., Quan, J., Mankowitz, D., Munos, R., Van Hasselt, H., Silver,
D., and Schaul, T. (2018). Universal successor features approximators. arXiv preprint
arXiv:1812.07626.
59Bostock, E., Muller, R. U., and Kubie, J. L. (1991). Experience-dependent modifications of
hippocampal place cell firing. Hippocampus, 1(2):193–205.
Brantley, K., Mehri, S., and Gordon, G. J. (2021). Successor feature sets: Generalizing suc-
cessor representations across policies. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 35, pages 11774–11781.
Brunec, I. K. and Momennejad, I. (2022). Predictive representations in hippocampal and
prefrontal hierarchies. Journal of Neuroscience, 42:299–312.
Buckner, R. L. (2010). The role of the hippocampus in prediction and imagination. Annual
Review of Psychology, 61:27–48.
Burgess, N., Barry, C., and O’keefe, J. (2007). An oscillatory interference model of grid cell
firing. Hippocampus, 17(9):801–812.
Bush, D., Barry, C., Manson, D., and Burgess, N. (2015). Using grid cells for navigation.
Neuron, 87(3):507–520.
Carvalho, W., Filos, A., Lewis, R. L., Singh, S., et al. (2023a). Composing task knowledge
with modular successor feature approximators. arXiv preprint arXiv:2301.12305.
Carvalho, W., Saraiva, A., Filos, A., Lampinen, A. K., Matthey, L., Lewis, R. L., Lee, H.,
Singh, S., Rezende, D. J., and Zoran, D. (2023b). Combining behaviors with the successor
features keyboard. arXiv preprint arXiv:2310.15940.
Chan, S. C., Applegate, M. C., Morton, N. W., Polyn, S. M., and Norman, K. A. (2017).
Lingeringrepresentationsofstimuliinfluencerecallorganization. Neuropsychologia, 97:72–
82.
Chang, C. Y., Gardner, M., Di Tillio, M. G., and Schoenbaum, G. (2017). Optogenetic
blockade of dopamine transients prevents learning induced by changes in reward features.
Current Biology, 27:3480–3486.
Chevalier-Boisvert, M., Dai, B., Towers, M., de Lazcano, R., Willems, L., Lahlou, S., Pal,
S., Castro, P. S., and Terry, J. (2023). Minigrid & miniworld: Modular & customizable
reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831.
Ciria, A., Schillaci, G., Pezzulo, G., Hafner, V.V., andLara, B.(2021). Predictiveprocessing
in cognitive robotics: a review. Neural Computation, 33:1402–1432.
Clark, A. (2013). Whatever next? predictive brains, situated agents, and the future of
cognitive science. Behavioral and Brain Sciences, 36:181–204.
Constantinescu, A. O., O’Reilly, J. X., and Behrens, T. E. (2016). Organizing conceptual
knowledge in humans with a gridlike code. Science, 352:1464–1468.
60Courville, A. C., Daw, N. D., and Touretzky, D. S. (2006). Bayesian theories of conditioning
in a changing world. Trends in Cognitive Sciences, 10(7):294–300.
Dasgupta, I. and Gershman, S. J. (2021). Memory as a computational resource. Trends in
Cognitive Sciences, 25:240–251.
Daw, N. D., Niv, Y., and Dayan, P. (2005). Uncertainty-based competition between pre-
frontal and dorsolateral striatal systems for behavioral control. Nature neuroscience,
8(12):1704–1711.
Dayan, P. (1993). Improving generalization for temporal difference learning: The successor
representation. Neural Computation, 5:613–624.
Dayan, P. and Kakade, S. (2000). Explaining away in weight space. Advances in Neural
Information Processing Systems, 13.
de Cothi, W. and Barry, C. (2020). Neurobiological successor features for spatial navigation.
Hippocampus, 30(12):1347–1355.
deCothi, W., Nyberg, N., Griesbauer, E.-M., Ghanamé, C., Zisch, F., Lefort, J.M., Fletcher,
L., Newton, C., Renaudineau, S., Bendor, D., et al. (2022). Predictive maps in rats and
humans for spatial navigation. Current Biology, 32(17):3676–3689.
de Jong, J. W., Fraser, K. M., and Lammel, S. (2022). Mesoaccumbal dopamine hetero-
geneity: what do dopamine firing and release have to do with it? Annual Review of
Neuroscience, 45:109–129.
Derdikman, D., Whitlock, J. R., Tsao, A., Fyhn, M., Hafting, T., Moser, M.-B., and Moser,
E. I. (2009). Fragmentation of grid cell maps in a multicompartment environment. Nature
neuroscience, 12(10):1325–1332.
Diba, K. and Buzsáki, G. (2007). Forward and reverse hippocampal place-cell sequences
during ripples. Nature neuroscience, 10(10):1241–1242.
Dickinson, A. (1985). Actions and habits: the development of behavioural autonomy. Philo-
sophical Transactions of the Royal Society of London. B, Biological Sciences,308(1135):67–
78.
Dolan, R. J. and Dayan, P. (2013). Goals and habits in the brain. Neuron, 80:312–325.
Dordek, Y., Soudry, D., Meir, R., and Derdikman, D. (2016). Extracting grid cell char-
acteristics from place cell inputs using non-negative principal component analysis. Elife,
5:e10094.
Dorrell, W., Latham, P. E., Behrens, T. E. J., and Whittington, J. C. R. (2023). Actionable
neural representations: Grid cells from minimal constraints. In The Eleventh International
Conference on Learning Representations.
61DuBrow, S., Rouhani, N., Niv, Y., and Norman, K. A. (2017). Does mental context drift or
shift? Current opinion in behavioral sciences, 17:141–146.
Ego-Stengel, V. and Wilson, M. A. (2010). Disruption of ripple-associated hippocampal
activity during rest impairs spatial learning in the rat. Hippocampus, 20(1):1–10.
Ekstrom, A. D. and Ranganath, C. (2018). Space, time, and episodic memory: The hip-
pocampus is all over the cognitive map. Hippocampus, 28(9):680–687.
Ekstrom, A. D., Spiers, H. J., Bohbot, V. D., and Rosenbaum, R. S. (2018). Human Spatial
Navigation. Princeton University Press.
Emukpere, D., Alameda-Pineda, X., andReinke, C.(2021). Successorfeatureneuralepisodic
control. arXiv preprint arXiv:2111.03110.
Engelhard, B., Finkelstein, J., Cox, J., Fleming, W., Jang, H. J., Ornelas, S., Koay, S. A.,
Thiberge, S. Y., Daw, N. D., Tank, D. W., et al. (2019). Specialized coding of sensory,
motor and cognitive variables in vta dopamine neurons. Nature, 570(7762):509–513.
Epstein, R. A., Patai, E. Z., Julian, J. B., and Spiers, H. J. (2017). The cognitive map in
humans: spatial navigation and beyond. Nature Neuroscience, 20(11):1504–1513.
Eysenbach, B., Salakhutdinov, R., and Levine, S. (2020). C-learning: Learning to achieve
goals via recursive classification. arXiv preprint arXiv:2011.08909.
Eysenbach, B., Zhang, T., Levine, S., and Salakhutdinov, R. R. (2022). Contrastive learning
as goal-conditioned reinforcement learning. Advances in Neural Information Processing
Systems, 35:35603–35620.
Fang, C., Aronov, D., Abbott, L., and Mackevicius, E. L. (2023). Neural learning rules
for generating flexible predictions and computing the successor representation. elife,
12:e80680.
Farebrother, J., Greaves, J., Agarwal, R., Lan, C. L., Goroshin, R., Castro, P. S., and
Bellemare, M. G. (2023). Proto-value networks: Scaling representation learning with
auxiliary tasks. arXiv preprint arXiv:2304.12567.
Filos, A., Lyle, C., Gal, Y., Levine, S., Jaques, N., and Farquhar, G. (2021). Psiphi-learning:
Reinforcement learning with demonstrations using successor features and inverse temporal
difference learning. In International Conference on Machine Learning, pages 3305–3317.
PMLR.
Folkerts, S., Rutishauser, U., and Howard, M. W. (2018). Human episodic memory retrieval
is accompanied by a neural contiguity effect. Journal of Neuroscience, 38(17):4200–4211.
Foster, D. J. and Wilson, M. A. (2007). Hippocampal theta sequences. Hippocampus,
17(11):1093–1099.
62Friston, K. and Kiebel, S. (2009). Predictive coding under the free-energy principle. Philo-
sophical Transactions of the Royal Society B: Biological Sciences, 364:1211–1221.
Fujimoto, S., Meger, D., and Precup, D. (2021). A deep reinforcement learning approach
to marginalized importance sampling with the successor representation. In International
Conference on Machine Learning, pages 3518–3529. PMLR.
Gahnstrom, C.J.andSpiers, H.J.(2020). Striatalandhippocampalcontributionstoflexible
navigation in rats and humans. Brain and Neuroscience Advances, 4:2398212820979772.
Gardner, M. P., Schoenbaum, G., and Gershman, S. J. (2018). Rethinking dopamine as
generalized prediction error. Proceedings of the Royal Society B, 285(1891):20181645.
Garvert, M. M., Dolan, R. J., and Behrens, T. E. (2017). A map of abstract relational
knowledge in the human hippocampal–entorhinal cortex. elife, 6:e17086.
Geerts, J. P., Chersi, F., Stachenfeld, K. L., and Burgess, N. (2020). A general model of
hippocampal and dorsal striatal learning and decision making. Proceedings of the National
Academy of Sciences, 117(49):31427–31437.
Geerts, J. P., Gershman, S. J., Burgess, N., and Stachenfeld, K. L. (2023). A probabilistic
successor representation for context-dependent learning. Psychological Review.
Geist, M. and Pietquin, O. (2010). Kalman temporal differences. Journal of Artificial
Intelligence Research, 39:483–532.
George, T. M., de Cothi, W., Stachenfeld, K. L., and Barry, C. (2023). Rapid learning of
predictive maps with stdp and theta phase precession. Elife, 12:e80663.
Gershman, S. J. (2015). A unifying probabilistic view of associative learning. PLoS Compu-
tational Biology, 11(11):e1004567.
Gershman, S. J. (2018). The successor representation: its computational logic and neural
substrates. Journal of Neuroscience, 38:7193–7200.
Gershman, S. J., Blei, D. M., and Niv, Y. (2010). Context, learning, and extinction. Psy-
chological Review, 117:197–209.
Gershman, S. J., Moore, C. D., Todd, M. T., Norman, K. A., and Sederberg, P. B. (2012).
Thesuccessorrepresentationandtemporalcontext. Neural Computation,24(6):1553–1568.
Gershman, S. J., Schapiro, A. C., Hupbach, A., and Norman, K. A. (2013). Neural context
reinstatement predicts memory misattribution. Journal of Neuroscience, 33(20):8590–
8595.
Girardeau, G., Benchenane, K., Wiener, S. I., Buzsáki, G., and Zugaro, M. B. (2009).
Selectivesuppressionofhippocampalripplesimpairsspatialmemory. Nature neuroscience,
12(10):1222–1223.
63Gonzalez, L. S., Fisher, A. A., D’Souza, S. P., Cotella, E. M., Lang, R. A., and Robinson,
J. E. (2023). Ventral striatum dopamine release encodes unique properties of visual stimuli
in mice. Elife, 12:e85064.
Gupta, R., Duff, M. C., Denburg, N. L., Cohen, N. J., Bechara, A., and Tranel, D. (2009).
Declarative memory is critical for sustained advantageous complex decision-making. Neu-
ropsychologia, 47(7):1686–1693.
Gupta, T., Mahajan, A., Peng, B., Böhmer, W., andWhiteson, S.(2021). Uneven: Universal
value exploration for multi-agent reinforcement learning. In International Conference on
Machine Learning, pages 3930–3941. PMLR.
Gutbrod, K., Kroužel, C., Hofer, H., Müri, R., Perrig, W., and Ptak, R. (2006). Decision-
making in amnesia: do advantageous decisions require conscious knowledge of previous
behavioural choices? Neuropsychologia, 44(8):1315–1324.
Ha, D. and Schmidhuber, J. (2018). Recurrent world models facilitate policy evolution.
Advances in Neural Information Processing Systems, 31.
Hafting, T., Fyhn, M., Molden, S., Moser, M.-B., and Moser, E. I. (2005). Microstructure of
a spatial map in the entorhinal cortex. Nature, 436(7052):801–806.
Han, D. and Tschiatschek, S. (2021). Option transfer and smdp abstraction with successor
features. arXiv preprint arXiv:2110.09196.
Hansen, S., Dabney, W., Barreto, A., Van de Wiele, T., Warde-Farley, D., and Mnih, V.
(2019). Fast task inference with variational intrinsic successor features. arXiv preprint
arXiv:1906.05030.
Hardcastle, K., Maheswaranathan, N., Ganguli, S., and Giocomo, L. M. (2017). A mul-
tiplexed, heterogeneous, and adaptive code for navigation in medial entorhinal cortex.
Neuron, 94(2):375–387.
Hart, E. E., Sharpe, M. J., Gardner, M. P., and Schoenbaum, G. (2020). Responding to
preconditioned cues is devaluation sensitive and requires orbitofrontal cortex during cue-
cue learning. Elife, 9:e59998.
Hartley, T., Burgess, N., Lever, C., Cacucci, F., and O’keefe, J. (2000). Modeling place fields
in terms of the cortical inputs to the hippocampus. Hippocampus, 10(4):369–379.
Hawkins, J. and Blakeslee, S. (2004). On Intelligence. Macmillan.
Hoang, C., Sohn, S., Choi, J., Carvalho, W., andLee, H.(2021). Successorfeaturelandmarks
for long-horizon goal-conditioned reinforcement learning. Advances in Neural Information
Processing Systems, 34:26963–26975.
64Holland, P. C. (2004). Relations between pavlovian-instrumental transfer and reinforcer
devaluation. Journal of Experimental Psychology: Animal Behavior Processes, 30(2):104.
Horvitz, J. C. (2000). Mesolimbocortical and nigrostriatal dopamine responses to salient
non-reward events. Neuroscience, 96(4):651–656.
Howard, M. W. and Kahana, M. J. (2002). A distributed representation of temporal context.
Journal of Mathematical Psychology, 46(3):269–299.
Howard, M. W., Shankar, K. H., and Jagadisan, U. K. (2011). Constructing semantic
representations from a gradually changing representation of temporal context. Topics in
Cognitive Science, 3(1):48–73.
Hunt, J., Barreto, A., Lillicrap, T., and Heess, N. (2019). Composing entropic policies using
divergencecorrection. InInternational Conference on Machine Learning, pages2911–2920.
PMLR.
Imani, E.andWhite, M.(2018). Improvingregressionperformancewithdistributionallosses.
In International conference on machine learning, pages 2157–2166. PMLR.
Janner, M., Mordatch, I., and Levine, S. (2020). gamma-models: Generative temporal differ-
ence learning for infinite-horizon prediction. Advances in Neural Information Processing
Systems, 33:1724–1735.
Janz, D., Hron, J., Mazur, P., Hofmann, K., Hernández-Lobato, J. M., and Tschiatschek,
S. (2019). Successor uncertainties: exploration and uncertainty in temporal difference
learning. Advances in Neural Information Processing Systems, 32.
Ji, D. and Wilson, M. A. (2007). Coordinated memory replay in the visual cortex and
hippocampus during sleep. Nature neuroscience, 10(1):100–107.
Jung, M. and McNaughton, B. (1993). Spatial selectivity of unit activity in the hippocampal
granular layer. Hippocampus, 3(2):165–182.
Kaelbling, L. P., Littman, M. L., and Moore, A. W. (1996). Reinforcement learning: A
survey. Journal of Artificial Intelligence Research, 4:237–285.
Kahn, A. E. and Daw, N. D. (2023). Humans rationally balance detailed and temporally
abstract world models. bioRxiv, pages 2023–11.
Keiflin, R., Pribut, H. J., Shah, N. B., and Janak, P. H. (2019). Ventral tegmental dopamine
neurons participate in reward identity predictions. Current Biology, 29:93–103.
Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and Jaśkowski, W. (2016). Vizdoom: A
doom-based ai research platform for visual reinforcement learning. In 2016 IEEE confer-
ence on computational intelligence and games (CIG), pages 1–8. IEEE.
65Kempter, R., Gerstner, W., and Van Hemmen, J. L. (1999). Hebbian learning and spiking
neurons. Physical Review E, 59(4):4498.
Kiernan, M. and Westbrook, R. (1993). Effects of exposure to a to-be-shocked environment
upontherat’sfreezingresponse: Evidenceforfacilitation,latentinhibition,andperceptual
learning. The Quarterly Journal of Experimental Psychology, 46(3):271–288.
Kim, S. H., Van Stralen, N., Chowdhary, G., and Tran, H. T. (2022). Disentangling suc-
cessor features for coordination in multi-agent reinforcement learning. arXiv preprint
arXiv:2202.07741.
Kjelstrup, K. B., Solstad, T., Brun, V. H., Hafting, T., Leutgeb, S., Witter, M. P., Moser,
E. I., and Moser, M.-B. (2008). Finite scale of spatial representation in the hippocampus.
Science, 321(5885):140–143.
Kool, W., Cushman, F. A., and Gershman, S. J. (2018). Competition and cooperation
between multiple reinforcement learning systems. Goal-directed Decision Making, pages
153–178.
Kropff, E., Carmichael, J. E., Moser, M.-B., and Moser, E. I. (2015). Speed cells in the
medial entorhinal cortex. Nature, 523(7561):419–424.
Krupic, J., Bauza, M., Burton, S., Barry, C., and O’Keefe, J. (2015). Grid cell symmetry is
shaped by environmental geometry. Nature, 518(7538):232–235.
Kruschke, J. K. (2008). Bayesian approaches to associative learning: From passive to active
learning. Learning & Behavior, 36:210–226.
Kulkarni, T. D., Saeedi, A., Gautam, S., and Gershman, S. J. (2016). Deep successor
reinforcement learning. arXiv preprint arXiv:1606.02396.
Lee, D., Srinivasan, S., and Doshi-Velez, F. (2019). Truly batch apprenticeship learning with
deep successor features. arXiv preprint arXiv:1903.10077.
Lehnert, L. and Littman, M. L. (2020). Successor features combine elements of model-free
and model-based reinforcement learning. The Journal of Machine Learning Research,
21(1):8030–8082.
Leutgeb, J. K., Leutgeb, S., Moser, M.-B., and Moser, E. I. (2007). Pattern separation in
the dentate gyrus and ca3 of the hippocampus. science, 315(5814):961–966.
Lever, C., Burton, S., Jeewajee, A., O’Keefe, J., and Burgess, N. (2009). Boundary
vector cells in the subiculum of the hippocampal formation. Journal of Neuroscience,
29(31):9771–9777.
Li, L., Walsh, T. J., and Littman, M. L. (2006). Towards a unified theory of state abstraction
for mdps. In AI&M.
66Lieder, F., Griffiths, T. L., and Hsu, M. (2018). Overrepresentation of extreme events in
decisionmakingreflectsrationaluseofcognitiveresources. Psychological Review, 125(1):1–
32.
Littman, M. and Sutton, R. S. (2001). Predictive representations of state. Advances in
Neural Information Processing Systems, 14.
Liu, H. and Abbeel, P. (2021). Aps: Active pretraining with successor features. In Interna-
tional Conference on Machine Learning, pages 6736–6747. PMLR.
Liu, Q., Li, L., Tang, Z., and Zhou, D. (2018). Breaking the curse of horizon: Infinite-horizon
off-policy estimation. Advances in Neural Information Processing Systems, 31.
Ljungberg, T., Apicella, P., and Schultz, W. (1992). Responses of monkey dopamine neurons
during learning of behavioral reactions. Journal of neurophysiology, 67(1):145–163.
Lotter, W., Kreiman, G., and Cox, D. (2016). Deep predictive coding networks for video
prediction and unsupervised learning. In International Conference on Learning Represen-
tations.
Ludvig, E. A., Sutton, R. S., and Kehoe, E. J. (2012). Evaluating the TD model of classical
conditioning. Learning & Behavior, 40:305–319.
Machado, M. C., Barreto, A., Precup, D., and Bowling, M. (2023). Temporal abstraction
in reinforcement learning with the successor representation. Journal of Machine Learning
Research, 24(80):1–69.
Machado, M. C., Bellemare, M. G., and Bowling, M. (2020). Count-based exploration
with the successor representation. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pages 5125–5133.
Machado, M. C., Rosenbaum, C., Guo, X., Liu, M., Tesauro, G., and Campbell, M.
(2017). Eigenoption discovery through the deep successor representation. arXiv preprint
arXiv:1710.11089.
Madarasz, T. and Behrens, T. (2019). Better transfer learning with inferred successor maps.
Advances in Neural Information Processing Systems, 32.
Manns, J. R., Galloway, C. R., and Sederberg, P. B. (2015). A temporal context repetition
effect in rats during a novel object recognition memory task. Animal Cognition, 18:1031–
1037.
Markus, E. J., Qin, Y.-L., Leonard, B., Skaggs, W. E., McNaughton, B. L., and Barnes,
C. A. (1995). Interactions between location and task affect the spatial and directional
firing of hippocampal neurons. Journal of Neuroscience, 15(11):7079–7094.
67Mattar, M. G. and Daw, N. D. (2018). Prioritized memory access explains planning and
hippocampal replay. Nature neuroscience, 21(11):1609–1617.
McLeod, M., Lo, C., Schlegel, M., Jacobsen, A., Kumaraswamy, R., White, M., and White,
A. (2021). Continual auxiliary task learning. Advances in Neural Information Processing
Systems, 34:12549–12562.
McNaughton, B. L., Battaglia, F. P., Jensen, O., Moser, E. I., and Moser, M.-B. (2006).
Path integration and the neural basis of the’cognitive map’. Nature Reviews Neuroscience,
7(8):663–678.
Mehta, M. R., Quirk, M. C., and Wilson, M. A. (2000). Experience-dependent asymmetric
shape of hippocampal receptive fields. Neuron, 25(3):707–715.
Momennejad, I. and Howard, M. W. (2018). Predicting the future with multi-scale successor
representations. BioRxiv, page 449470.
Momennejad, I., Otto, A. R., Daw, N. D., and Norman, K. A. (2018). Offline replay supports
planning in human reinforcement learning. elife, 7:e32548.
Momennejad,I.,Russek, E.M.,Cheong,J.H., Botvinick, M.M.,Daw,N.D.,andGershman,
S. J. (2017). The successor representation in human reinforcement learning. Nature human
behaviour, 1(9):680–692.
Morris, R. G., Garrud, P., Rawlins, J. a., and O’Keefe, J. (1982). Place navigation impaired
in rats with hippocampal lesions. Nature, 297(5868):681–683.
Moskovitz, T., Wilson, S. R., and Sahani, M. (2021). A first-occupancy representation for
reinforcement learning. arXiv preprint arXiv:2109.13863.
Muller, R. U. and Kubie, J. L. (1987). The effects of changes in the environment on the
spatialfiringofhippocampalcomplex-spikecells. Journal of Neuroscience,7(7):1951–1968.
Navratilova, Z., Hoang, L. T., Schwindel, C. D., Tatsuno, M., and McNaughton, B. L.
(2012). Experience-dependent firing rate remapping generates directional selectivity in
hippocampal place cells. Frontiers in neural circuits, 6:6.
Ng, A. Y., Russell, S., et al. (2000). Algorithms for inverse reinforcement learning. In
International Conference on Machine Learning, volume 1, page 2.
Niv, Y. (2009). Reinforcement learning in the brain. Journal of Mathematical Psychology,
53(3):139–154.
Nyberg, N., Duvelle, É., Barry, C., and Spiers, H. J. (2022). Spatial goal coding in the
hippocampal formation. Neuron.
O’Keefe, J. and Dostrovsky, J. (1971). The hippocampus as a spatial map: preliminary
evidence from unit activity in the freely-moving rat. Brain research.
68O’Keefe, J. and Nadel, L. (1978). The Hippocampus as a Cognitive Map. Oxford: Clarendon
Press.
O’Keefe, J. and Recce, M. L. (1993). Phase relationship between hippocampal place units
and the eeg theta rhythm. Hippocampus, 3(3):317–330.
Ólafsdóttir, H. F., Barry, C., Saleem, A. B., Hassabis, D., and Spiers, H. J. (2015). Hip-
pocampal place cells construct reward related sequences through unexplored space. Elife,
4:e06063.
Ólafsdóttir, H. F., Carpenter, F., and Barry, C. (2016). Coordinated grid and place cell
replay during rest. Nature neuroscience, 19(6):792–794.
Ostrovski, G., Castro, P.S., andDabney, W.(2021). Thedifficultyofpassivelearningindeep
reinforcement learning. Advances in Neural Information Processing Systems, 34:23283–
23295.
Packard, M. G. and McGaugh, J. L. (1996). Inactivation of hippocampus or caudate nucleus
with lidocaine differentially affects expression of place and response learning. Neurobiology
of learning and memory, 65(1):65–72.
Plonsky, O., Teodorescu, K., and Erev, I. (2015). Reliance on small samples, the wavy
recency effect, and similarity-based learning. Psychological Review, 122(4):621–647.
Poole, B., Ozair, S., Van Den Oord, A., Alemi, A., and Tucker, G. (2019). On variational
bounds of mutual information. In International Conference on Machine Learning, pages
5171–5180. PMLR.
Precup, D. (2000). Eligibility traces for off-policy policy evaluation. Computer Science
Department Faculty Publication Series, page 80.
Rabinowitz, N., Perbet, F., Song, F., Zhang, C., Eslami, S. A., and Botvinick, M. (2018).
Machine theory of mind. In International conference on machine learning, pages 4218–
4227. PMLR.
Ramesh, R., Tomar, M., and Ravindran, B. (2019). Successor options: An option discovery
framework for reinforcement learning. arXiv preprint arXiv:1905.05731.
Ranganath, C. (2010). A unified framework for the functional organization of the medial
temporal lobes and the phenomenology of episodic memory. Hippocampus, 20:1263–1290.
Redish, A., Jensen, S., Johnson, A., and Kurth-Nelson, Z. (2007). Reconciling reinforce-
ment learning models with behavioral extinction and renewal: Implications for addiction,
relapse, and problem gambling. Psychological Review, 114:784–805.
69Rescorla, R. A. and Wagner, A. R. (1972). A theory of of Pavlovian conditioning: variations
in the effectiveness of reinforcement and nonreinforcement. In Black, A. and Prokasy, W.,
editors, Classical Conditioning II: Current Research and theory, pages 64–99. Appleton-
Century-Crofts, New York, NY.
Rothschild, G., Eban, E., and Frank, L. M. (2017). A cortical–hippocampal–cortical loop of
information processing during memory consolidation. Nature neuroscience, 20(2):251–259.
Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z., et al. (2018). A tutorial on
Thompson sampling. Foundations and Trends® in Machine Learning, 11:1–96.
Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G.,
Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. (2019). The starcraft multi-agent
challenge. arXiv preprint arXiv:1902.04043.
Sanders, H., Wilson, M. A., and Gershman, S. J. (2020). Hippocampal remapping as hidden
state inference. Elife, 9:e51140.
Sargolini, F., Fyhn, M., Hafting, T., McNaughton, B. L., Witter, M. P., Moser, M.-B., and
Moser, E. I. (2006). Conjunctive representation of position, direction, and velocity in
entorhinal cortex. Science, 312(5774):758–762.
Schacter, D. L., Benoit, R. G., De Brigard, F., and Szpunar, K. K. (2015). Episodic fu-
ture thinking and episodic counterfactual thinking: Intersections between memory and
decisions. Neurobiology of Learning and Memory, 117:14–21.
Schramm, L., Deng, Y., Granados, E., and Boularias, A. (2023). Usher: Unbiased sam-
pling for hindsight experience replay. In Conference on Robot Learning, pages 2073–2082.
PMLR.
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A.,
Lockhart, E., Hassabis, D., Graepel, T., et al. (2020). Mastering atari, go, chess and shogi
by planning with a learned model. Nature, 588(7839):604–609.
Schultz, W., Dayan, P., and Montague, P. R. (1997). A neural substrate of prediction and
reward. Science, 275(5306):1593–1599.
Sharpe, M. J., Chang, C. Y., Liu, M. A., Batchelor, H. M., Mueller, L. E., Jones, J. L.,
Niv, Y., and Schoenbaum, G. (2017). Dopamine transients are sufficient and necessary for
acquisition of model-based associations. Nature Neuroscience, 20:735–742.
Silva, D., Feng, T., and Foster, D. J. (2015). Trajectory events across hippocampal place
cells require previous experience. Nature Neuroscience, 18(12):1772–1779.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrit-
twieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the
game of go with deep neural networks and tree search. Nature, 529:484–489.
70Smith, T. A., Hasinski, A. E., and Sederberg, P. B. (2013). The context repetition ef-
fect: Predicted events are remembered better, even when they don’t happen. Journal of
Experimental Psychology: General, 142(4):1298.
Socher, R., Gershman, S., Sederberg, P., Norman, K., Perotte, A., and Blei, D. (2009). A
Bayesian analysis of dynamics in free recall. Advances in neural information processing
systems, 22.
Solstad,T.,Boccara,C.N.,Kropff,E.,Moser,M.-B.,andMoser,E.I.(2008). Representation
of geometric borders in the entorhinal cortex. Science, 322(5909):1865–1868.
Sorscher, B., Mel, G., Ganguli, S., and Ocko, S. (2019). A unified theory for the origin of
grid cells through the lens of pattern formation. Advances in neural information processing
systems, 32.
Spiers, H. J. and Barry, C. (2015). Neural systems supporting navigation. Current Opinion
in Behavioral Sciences, 1:47–55.
Stachenfeld, K. L., Botvinick, M. M., and Gershman, S. J. (2017). The hippocampus as a
predictive map. Nature Neuroscience, 20(11):1643–1653.
Stalnaker, T. A., Howard, J. D., Takahashi, Y. K., Gershman, S. J., Kahnt, T., and Schoen-
baum, G. (2019). Dopamine neuron ensembles signal the content of sensory prediction
errors. Elife, 8:e49315.
Starkweather, C. K. and Uchida, N. (2021). Dopamine signals as temporal difference errors:
recent advances. Current Opinion in Neurobiology, 67:95–105.
Stensola, H., Stensola, T., Solstad, T., Frøland, K., Moser, M.-B., and Moser, E. I. (2012).
The entorhinal grid map is discretized. Nature, 492(7427):72–78.
Sutton, R. and Barto, A. (1990). Time-derivative models of Pavlovian reinforcement. In
Gabriel, M. and Moore, J., editors, Learning and Computational Neuroscience: Founda-
tions of Adaptive Networks, pages 497–537. MIT Press.
Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT
Press.
Sutton, R. S., Precup, D., and Singh, S. (1999). Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–
211.
Takahashi, Y. K., Batchelor, H. M., Liu, B., Khanna, A., Morales, M., and Schoenbaum,
G. (2017). Dopamine neurons respond to errors in the prediction of sensory features of
expected rewards. Neuron, 95(6):1395–1405.
71Tanni, S., De Cothi, W., and Barry, C. (2022). State transitions in the statistically sta-
ble place cell population correspond to rate of perceptual change. Current Biology,
32(16):3505–3514.
Thakoor, S., Rowland, M., Borsa, D., Dabney, W., Munos, R., and Barreto, A. (2022). Gen-
eralised policy improvement with geometric policy composition. In International Confer-
ence on Machine Learning, pages 21272–21307. PMLR.
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another
in view of the evidence of two samples. Biometrika, 25(3-4):285–294.
Tomov, M. S., Schulz, E., and Gershman, S. J. (2021). Multi-task reinforcement learning in
humans. Nature Human Behaviour, 5(6):764–773.
Touati, A., Rapin, J., and Ollivier, Y. (2022). Does zero-shot reinforcement learning exist?
arXiv preprint arXiv:2209.14935.
Tsividis, P. A., Loula, J., Burga, J., Foss, N., Campero, A., Pouncy, T., Gershman, S. J.,
and Tenenbaum, J. B. (2021). Human-level reinforcement learning through theory-based
modeling, exploration, and planning. arXiv preprint arXiv:2107.12544.
Veeriah, V., Hessel, M., Xu, Z., Rajendran, J., Lewis, R. L., Oh, J., van Hasselt, H. P.,
Silver, D., and Singh, S. (2019). Discovery of useful questions as auxiliary tasks. Advances
in Neural Information Processing Systems, 32.
Vértes, E.andSahani, M.(2019). Aneurallyplausiblemodellearnssuccessorrepresentations
in partially observable environments. Advances in Neural Information Processing Systems,
32.
Wärnberg, E. and Kumar, A. (2023). Feasibility of dopamine as a vector-valued feed-
back signal in the basal ganglia. Proceedings of the National Academy of Sciences,
120:e2221994120.
Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine Learning, 8:279–292.
Whittington, J. C., Muller, T. H., Mark, S., Chen, G., Barry, C., Burgess, N., and Behrens,
T. E. (2020). The tolman-eichenbaum machine: unifying space and relational memory
through generalization in the hippocampal formation. Cell, 183(5):1249–1263.
Wilson, M. A. and McNaughton, B. L. (1994). Reactivation of hippocampal ensemble mem-
ories during sleep. Science, 265(5172):676–679.
Winocur, G., Frankland, P. W., Sekeres, M., Fogel, S., and Moscovitch, M. (2009). Changes
in context-specificity during memory reconsolidation: selective effects of hippocampal le-
sions. Learning & Memory, 16(11):722–729.
72Wittkuhn, L., Krippner, L. M., and Schuck, N. W. (2022). Statistical learning of successor
representations is related to on-task replay. bioRxiv, pages 2022–02.
Yu, C., Burgess, N., Sahani, M., and Gershman, S. (2023). Successor-predecessor intrinsic
exploration. Advances in Neural Information Processing Systems 36.
Zahavy, T., O’Donoghue, B., Barreto, A., Mnih, V., Flennerhag, S., and Singh, S.
(2021). Discovering diverse nearly optimal policies with successor features. arXiv preprint
arXiv:2106.00669.
Zahavy, T., Veeriah, V., Hou, S., Waugh, K., Lai, M., Leurent, E., Tomaev, N., Schut, L.,
Hassabis, D., and Singh, S. (2023). Diversifying ai: Towards creative chess with alphazero.
In arXiv, volume abs/2308.09175.
Zhang, J., Springenberg, J. T., Boedecker, J., and Burgard, W. (2017). Deep reinforce-
ment learning with successor features for navigation across similar environments. In 2017
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
2371–2378. IEEE.
Zheng, C., Salakhutdinov, R., and Eysenbach, B. (2023). Contrastive difference predictive
coding. arXiv preprint arXiv:2310.20141.
Zhou, C. Y., Talmi, D., Daw, N., and Mattar, M. G. (2023). Episodic retrieval for model-
based evaluation in sequential decision tasks. PsyArXiv.
Zhu, Y., Gordon, D., Kolve, E., Fox, D., Fei-Fei, L., Gupta, A., Mottaghi, R., and Farhadi,
A. (2017). Visual semantic planning using deep successor representations. In Proceedings
of the IEEE international conference on computer vision, pages 483–492.
73