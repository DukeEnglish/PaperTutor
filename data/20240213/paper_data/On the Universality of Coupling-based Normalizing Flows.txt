On the Universality of Coupling-based Normalizing Flows
FelixDraxler1 StefanWahl1 ChristophSchno¨rr1 UllrichKo¨the1
Abstract distributionbothfasttoevaluatep (x)≈p(x)andsample
θ
fromx∼p (x)≈x∼p(x).
θ
Wepresentanoveltheoreticalframeworkforun-
derstanding the expressive power of coupling- Coupling blocks impose a strong architectural constraint
basednormalizingflowssuchasRealNVP(Dinh on invertible neural networks. Most strikingly, half of
et al., 2017). Despite their prevalence in scien- thedimensionsareleftunchangedineachblock, andthe
tificapplications,acomprehensiveunderstanding transformationoftheremainingdimensionsisrestrictedin
of coupling flows remains elusive due to their order to ensure invertibility. At the same time, even the
restricted architectures. Existing theorems fall simpleaffinecoupling-basednormalizingflowscanlearn
short as they require the use of arbitrarily ill- high-dimensionaldistributionssuchasimages(Kingma&
conditioned neural networks, limiting practical Dhariwal,2018).
applicability. Additionally,wedemonstratethat
Theoreticalexplanationsforthisarchitecture’sabilitytofit
these constructions inherently lead to volume-
complexdistributionsarelimited. Existingproofsmakeas-
preserving flows, a property which we show to
sumptionsthatarenotvalidinpractice,astheinvolvedcon-
beafundamentalconstraintforexpressivity. We
structionsrelyonill-conditionedneuralnetworks(Koehler
proposeanewdistributionaluniversalitytheorem
etal.,2021).
forcoupling-basednormalizingflows,whichover-
comesseverallimitationsofpriorwork. Ourre- We extend the theory in two ways: First, we prove that
sultssupportthegeneralwisdomthatthecoupling volume-preserving normalizing flows (Dinh et al., 2015;
architectureisexpressiveandprovideanuanced Sorrensonetal.,2019)arenotuniversalapproximatorsin
view for choosing the expressivity of coupling termsofKLdivergence,thepracticallossmeasure. Infact,
functions, bridging a gap between empirical re- theexistinguniversalapproximationtheoremsforcoupling-
sultsandtheoreticalunderstanding. basednormalizingflowsconstructvolume-preservingflows
(Teshimaetal.,2020a;Koehleretal.,2021),fundamentally
limiting their practical implications for learning distribu-
tions. Second, we introduce a new proof for the distri-
1.Introduction
butionaluniversalityofcoupling-basednormalizingflows.
Densityestimationandgenerativemodelingofcomplexdis- Thisproofisconstructive,showingthattraininglayersse-
tributionsisafundamentalprobleminstatisticsandmachine quentiallyconvergestothecorrecttargetdistribution,which
learning,withapplicationsrangingfromcomputervision weillustrateinFigure1.
(Rombachetal.,2022)tomoleculegeneration(Hoogeboom
Insummary,wecontribute:
etal.,2022)anduncertaintyquantification(Ardizzoneetal.,
2018b).
• Weshowthelimitsofvolume-preservingflowsasdis-
Normalizingflowsareacommonclassofgenerativemodels tributionaluniversalapproximatorinSection4.2.
thatmodelaprobabilitydensitywhichcanbetrainedfrom
• We then show that existing distributional universal-
samples via the maximum likelihood criterion. They are
ityproofsforaffinecoupling-basednormalizingflows
implemented by transporting a simple multivariate base
constructsuchvolume-preservingflowsinSection4.3.
densitysuchasthestandardnormalviaalearnedinvertible
function to the distribution of interest. One particularly • Wegiveanewuniversalityproofforcoupling-based
efficientvariantofsuchinvertibleneuralnetworksarebased normalizingflowsthatovercomespreviousshortcom-
on so-called couplings blocks, which make the resulting ingsinSection4.5.
1IWR,HeidelbergUniversity,Germany. Correspondenceto:
Ourresultsvalidateacrucialinsightpreviouslyobserved
FelixDraxler<felix.draxler@iwr.uni-heidelberg.de>.
onlyempirically: Affinecouplingblocks arean effective
Preprint,underreview. foundation for normalizing flows. Our proof elucidates
1
4202
beF
9
]GL.sc[
1v87560.2042:viXraOntheUniversalityofCoupling-basedNormalizingFlows
et al. (2020a) showed that that coupling flows are univer-
Rotated pn 1(z) Transform Latent pn(z) Data pn(x)
salapproximatorsforinvertiblefunctions,whichresultsin
distributional universality. Koehler et al. (2021) demon-
strated that affine coupling-based normalizing flows can
approximateanydistributionwitharbitraryprecisionusing
justthreecouplingblocks. However,theseworksassume
neuralnetworkswithexplodingderivativesforcouplings,
an unrealistic condition in practical scenarios. Our work
addressesthislimitationbyshowingthattraininganormal-
izingflowlayerbylayeryieldsuniversality. Weadditionally
demonstratethattheseworksconstructvolume-preserving
transformationsinSection4.3,anadditionalimportantlimi-
tation.
Someworksshowdistributionaluniversalityofaugmented
affinecoupling-basednormalizingflows,whichaddatleast
one additional dimension usually filled with exact zeros
(Huangetal.,2020;Koehleretal.,2021;Lyuetal.,2022).
Theproblemwithaddingadditionalzerosisthattheflow
is not exactly invertible anymore in the data domain and
usuallylosestractabilityofthechangeofvariablesformula
(Equation(1)). Leeetal.(2021)addi.i.d.Gaussiansasad-
ditionaldimensions,whichagainallowsdensityestimation,
buttheyonlyshowhowtoapproximatethelimitedclassof
log-concavedistributions. Ouruniversalityproofdoesnot
relyonsuchaconstruction.
Figure1. Ouruniversalityproofconstructsanormalizingflow
Othertheoreticalworkontheexpressivityofnormalizing
byiterativelyaddingaffinecouplingblocks.Weillustratethisby
constructingsuchaflowfromrealdata.Eachblockfirstrotatesthe flowsconsidersmoreexpressiveinvertibleneuralnetworks,
distributionp (z)fromthepreviousstep(firstcolumn),then includingSoSpolynomialflows,NeuralODEsandResid-
n−1
appliesanaffinecouplinglayerthattransformstheactivedimen- ualNeuralNetworks(Jainietal.,2019;Zhangetal.,2020;
sionstozeromeanandunitvarianceforeachpassivecoordinateb Teshimaetal.,2020b;Ishikawaetal.,2022). Anotherline
(secondcolumn).Theresultinglatentdistributionconvergesstep ofworkfoundthatthenumberofrequiredcouplingblocks
bystep(thirdcolumn)toastandardnormaldistribution,wherethe isindependentofdimensionDforGaussiandistributions
learnedadditionallayersessentiallylearntheidentity(lastrow).
compared to O(D) Gaussianization blocks that lack cou-
Thedatadistributionp (x)convergesinparallel(right).
θ plingsbetweendimensions(Koehleretal.,2021;Draxler
etal.,2022;2023).
howmoreexpressivecouplingfunctionscanachievegood
3.Coupling-basedNormalizingFlows
performancewithfewerlayers. Additionally,ourfindings
advisecautioninusingvolume-preservingflowsduetotheir Normalizing Flows are a class of generative models that
inherentlimitationsinexpressivity. representadistributionp (x)withparametersθbylearning
θ
an invertible function z = f (x) so that distribution of
θ
2.RelatedWork thelatent codes z ∈ RD obtainedfrom thedata x ∈ RD
aredistributedlikeastandardnormaldistributionp(z) =
Normalizingflowsformaclassofgenerativemodelsthatare N(z;0,I). Viathechangeofvariablesformula,see(Ko¨the,
basedoninvertibleneuralnetworks(Rezende&Mohamed, 2023)forareview,thisinvertiblefunctionyieldsanexplicit
2015). Wefocusonthewidely-usedcoupling-basedflows, formforthedensityp (x):
θ
whichinvolveasequenceofsimpleinvertibleblocks(Dinh
etal.(2015;2017),seeSection3). p (x)=p(z =f (x))|f′(x)|, (1)
θ θ θ
Thatcoupling-basednormalizingflowsworkwellinpractice
wheref′(x) = ∂ f (x)istheJacobianmatrixoff atx
despitetheirrestrictedarchitecturehassparkedtheinterest θ ∂x θ θ
and|f′(x)|isitsabsolutedeterminant.
ofseveralpapersanalyzingtheirdistributionaluniversality, θ
i.e.thequestionwhethertheycanapproximateanytargetdis- Equation(1)allowseasilyevaluatingthemodeldensityat
tributiontoarbitraryprecision(seeDefinition4.1). Teshima points of interest. Obtaining samples from p (x) can be
θ
2
1=n
2=n
3=n
02=n
001=nOntheUniversalityofCoupling-basedNormalizingFlows
achievedbysamplingfromthelatentstandardnormaland necessarytoinverttheactivehalfofdimensions:
applyingtheinversef−1(z)ofthelearnedtransformation:
θ  ˜b 
1
x=f−1(z)∼p (x)forz ∼p(z). (2)   . . .  
θ θ  
 ˜b 
x=f−1(x˜)= D/2 . (7)
cpl  c−1(a′;θ (˜b)) 
Thechangeofvariablesformula(Equation(1))canbeused  1 1 
 . 
directlytotrainanormalizingflow. Thecorrespondingloss  . . 
 
minimizestheKullback-Leiblerdivergencebetweenthetrue c−1(a′ ;θ (˜b))
D/2 D/2
datadistributionp(x)andthelearneddistribution, which
canbeoptimizedviaaMonte-Carloestimateoftheinvolved
Choosing the right one-dimensional invertible function
expectation:
c(x;θ)isthesubjectofactiveresearch,seeourlistinAp-
pendixAandthereviewbyKobyzevetal.(2021). Many
L=D KL(p(x)∥p θ(x)) (3) applicationsuseaffine-linearfunctionsc(x;s,t)=sx+t
=E [logp(x)−logp (x)] (4) wheres>0andtaretheparameterstobepredictedbythe
x∼p(x) θ
=E [−logp (x)]+const. (5)
θ(b)subnetworkasafunctionofthepassivedimensions.Es-
x∼p(x) θ peciallyforsmaller-dimensionalproblemsithasprovenuse-
fultousemoreflexiblecsuchasrational-quadraticsplines
This last variant makes clear that minimizing this loss is
(Durkanetal.,2019b). Ouruniversalityresultsarecompati-
exactlythesameasmaximizingthelog-likelihoodofthe
blewithallcouplingarchitecturesweareawareofexceptfor
trainingdata. Fortraining,theexpectationvalueisapproxi-
NICE.Atthesametime,ourconstructiongivesadirectrea-
matedusing(batchesof)trainingsamplesx .
1,...,N sonforusingmoreexpressivecouplings,astheycanlearn
InorderforEquations(1)and(2)tobeusefulinpractice, thesamedistributionswithfewerlayers(seeSection4.6).
f (x)musthave(i)atractableinversef−1(z)forfastsam-
θ θ Inordertobeexpressive,anormalizingflowconsistsofa
pling,and(ii)atractableJacobiandeterminant|f′(x)|for
θ stack of coupling layers, each with a different active and
fasttrainingwhile(iii)beingexpressiveenoughtomodel
passive subspace. This is realized by an additional layer
complicateddistributions. Theseconstraintsarenontrivial
beforeeachcouplingwhichrotatesanincomingvectorx
tofulfillatthesametimeandsignificantworkhasbeenput
viaarotationmatrixQ∈SO(D):
intoconstructingsuchinvertibleneuralnetworks.
f (x)=Qx, (8)
Inthiswork,wefocusontheclassofcoupling-basedneural rot
networks (Dinh et al., 2015; 2017). This design lies in a f−1(x˜)=QTx˜. (9)
rot
sweetspotofbeingexpressiveyeteasytoinvert(Draxler
Often,Qissimplychosenasapermutationmatrixthatis
etal.,2023)andexhibitsatractableJacobiandeterminant.
fixedduringtraining,butsomevariantsallowanyrotationQ
Itsbasicbuildingblockisthecouplinglayer,whichconsists
orlearningtherotationduringtraining(Kingma&Dhariwal,
ofoneinvertiblefunctionx˜ =c(x ;θ )foreachdimension,
i i i 2018). Our universality theorem will consider free-form
butwithatwist:Onlythesecondhalfofthedimensionsa=
rotationmatricesQ. Thisdoesnotrestrictitsapplicability
x (active)ischangedinacouplinglayer,andthe
D/2+1,...,D tosomearchitectures,sinceanyinvertiblelinearfunction
parameters θ = θ(b) are predicted by a neural network
canbe representedbya fixednumber ofcouplingblocks
thatdependsonthefirsthalfofdimensionsb = x
1,...,D/2 withfixedpermutations(Koehleretal.,2021).
(passive):
A rotation layer together with a coupling layer forms a
 b  couplingblock:
1
.
 . 
 .  f blk(x)=(f cpl◦f rot)(x)=f cpl(Qx). (10)
 
x˜=f cpl(x)= 
 c(a
1b ;D θ/ 12
(b))
  . (6) Intheremainderofthispaper,weareconcernedwithwhat
  distributionsp(x)apotentiallydeepconcatenationofcou-
 . . 
 .  plingblockscanrepresent.
c(a ;θ (b))
D/2 D
4.DistributionalUniversalityofCoupling
Theneuralnetworkθ(b)allowsformodelingdependencies
Flows
betweendimensionsinthecouplinglayer. Calculatingthe
inverseofthecouplinglayeriseasy,asb=˜bforthepassive Inthissection,wegiveournewdistributionaluniversality
dimensions. This allows computing the parameters θ(b) resultsforcoupling-basednormalizingflows.Westartoffby
3OntheUniversalityofCoupling-basedNormalizingFlows
explainingwhatwemeanbydistributionaluniversality. We 4.2.LimitationsofVolume-preservingFlows
thenshowanegativeresultconcerningvolume-preserving
In this section, we provide a negative universality result
flows,inthattheyarenotdistributionaluniversalapproxi-
of normalizing flows which have a constant Jacobian de-
matorintermsofKLdivergence. Thisshowsafundamental
terminant|f′(x)|=const,suchasnonlinearindependent
limitationofpreviousuniversalityproofsofcouplingflows. θ
componentsestimation(NICE)(Dinhetal.,2015)orgen-
We then give our proof that overcomes several of those
eralincompressible-flownetworks(GIN)(Sorrensonetal.,
shortcomings.
2019). Such flows are usually called volume-preserving
flowsorsometimesincompressibleflows.
4.1.DistributionalUniversality
For one-dimensional functions, this implies that f (x) is
θ
Bydistributionaluniversalitywemeanthatacertainclass
linear. Formultivariatefunctions,f (x)canbenonlinear,
θ
of generative models can represent any distribution p(x).
only that any volume change in one dimension must be
Duetothenatureofneuralnetworks,wecannothopefor
compensatedbyaninversevolumechangeintheremaining
ourgenerativemodeltoexactly(i.e.exactequalityinthe
dimensions. Forexample,GINrealizesvolume-preserving
mathematicalsense)representp(x). Thisbecomesclearvia couplingblocksbyrequiringthat(cid:80)D/2logs
(b)=const.
ananalogueinthecontextofregression: Aneuralnetwork i=1 i
ThisismoreexpressivethanNICE,whichsetalls (b)=1
withReLUactivationsalwaysmodelspiecewiselinearfunc- i
exceptinalinearrescalinglayer.
tions, and as such it can never exactly regress a parabola
y =x2. However,foreveryfinitevalueofϵ>0andgiven Whilevolume-preservingflowscanbeusefulincertainap-
moreandmorelinearpieces,itcanfollowtheparabolaever plicationssuchasdisentanglement(Sorrensonetal.,2019)
socloser,sothattheaveragedistancebetweenx2andf (x) or temperature-scaling in Boltzmann generators (Dibak
θ
vanishes: E [|x2−f (x)|2]<ϵ. Tocharacterizethe et al., 2022), they are at disadvantage in terms of what
x∼p(x) θ
expressivityofaclassofneuralnetworks,itisthusinstruc- distributionstheycanlearn.
tivetocallaclassofnetworksuniversaliftheerrorbetween
Toderivethis,letusadaptthechangeofvariablesformula
themodelandanytargetcanbereducedarbitrarily.
Equation(1)tovolume-preservingflows:
Intermsofrepresentingdistributionsp(x), thefollowing
p (x)=p(z =f (x))C, (11)
definitioncapturesuniversalityofaclassofmodeldistribu- θ θ
tions,similarto(Teshimaetal.,2020a,Definition3):
where C = |f′(x)|. This equation says that for every x
θ
the density modeled by the flow is exactly the density of
Definition4.1. AsetofprobabilitydistributionsP iscalled
thecorrespondinglatentcodez = f (x)uptoaconstant
adistributionaluniversalapproximatorifforeverypossible θ
factor–andlikewiseeverylatentcodemustlenditsrelative
targetdistributionp(x)thereisasequenceofdistributions
n→∞ likelihoodtoexactlyonepointinthedataspace.
p (x)∈P suchthatp (x)−−−−→p(x).
n n
Itturnsoutthatthisrestrictionisfatalfortheexpressivity
ofvolume-preservingflows:
The formulation of universality as a convergent series is
usefulasit(i)capturesthatthedistributioninquestionp(x) Theorem4.2. Thefamilyofnormalizingflowswithconstant
maynotlieinP,and(ii)theseriesindexnusuallyreflects Jacobian determinant |f′(x)| = const is not a universal
θ
ahyperparameteroftheunderlyingmodelcorrespondingto distributionapproximatorunderKLdivergence.
computationalrequirements(forexample,thedepthofthe
In the detailed proof in Appendix B.1, we construct a
network).
counter-example of a distribution that cannot be approx-
n→∞
Wehavelefttheexactdefinitionofthelimit“p n(x)−−−−→ imated in terms of KL divergence. Intuitively speaking,
p(x)” open as we may want to consider different varia- volume-preservingflowscanonlymorphthelatentdistribu-
tions of convergence. The existing literature on affine tionp(z)byshiftingregionsofitaround,buttheycannot
coupling-basednormalizingflowsconsidersweakconver- compressorinflatespacetovarythelocaldensitybyEqua-
gence(Teshimaetal.,2020a)respectivelyconvergencein tion(11). Thisimpliesthatthestructureofp (x)isessen-
θ
Wassersteindistance(Koehleretal.,2021). Wewillnotein tiallysharedwiththelatentdistributionp(z). Forexample,
Section4.3thattheconstructionsusedintheexistingproofs the local maxima of the learned density, usually referred
arefundamentallytiedtotheserelativelyweakconvergence to as its modes, are inherited from the latent distribution.
metrics. Manymetricsofconvergencehavebeenproposed, Thismeansthatthelearneddistributioncannotcreatemulti-
see(Gibbs&Su,2002)forasystematicoverview. modaldistributionsfromastandardnormallatentspace:
In this paper, we consider continuous target distributions Corollary 4.3. A normalizing flow p (x) with constant
θ
p(x)thathaveinfinitesupportandfinitemoments,which Jacobiandeterminant|f′(x)|=consthasthesamenumber
θ
coversdistributionsofpracticalinterest. ofmodesasthelatentdistributionp(z).
4OntheUniversalityofCoupling-basedNormalizingFlows
OurTheorem4.2and Corollary4.3identifyafundamen-
tallimitationforapplicationsbasedonvolume-preserving
flows. ItexplainswhyRealNVPsignificantlyoutperforms
NICEinpractice(Dinhetal.,2017). Workusingvolume-
preservingflowsmusttakethislimitedexpressivityandthe
resultingbiasesinthelearneddistributionsintoaccount. In
thenextsection,wewillfindthatthisproblemalsoapplies
inexistinguniversalityproofsforcoupling-basednormaliz-
ingflows.
4.3.ProblemswithExistingConstructions
Therearealreadyexistingproofsshowingthataffineand
more expressive coupling flows are distributional univer-
sal approximators. They make use of specially parame-
terized coupling blocks that results in convergence to ar-
bitrarydistributions(Teshimaetal.,2020a;Koehleretal.,
2021).Whiletechnicallycorrect,themetricsofconvergence
employedby(Teshimaetal.,2020a;Koehleretal.,2021)
areindifferenttotwoshortcomingsoftheseconstructions:
theyrequireill-conditionednetworksandconstructvolume-
preservingflows,whicharenotuniversalinKLdivergence
byourTheorem4.2. Wewilldemonstratetheshortcomings
viatheapproachpresentedinKoehleretal.(2021),butthe
sameargumentsalsoapplytotheconstructionin(Teshima
etal.,2020a).
ThekeyresultinKoehleretal.(2021,Theorem1)isthatfor
anyapproximationerrorϵ>0itispossibletoconstructa
couplingflowf consistingofthreeaffinecouplingsblocks
Figure2. A normalizing flow with constant Jacobian deter- θ
minantisnotabletomodelasimplebimodalmixture(first forwhichtheWassersteindistanceW 2issmallerthanϵ:
vs second row): The modeled density in both modes is almost
W (p (x),p(x))<ϵ. (12)
identicaldespitetheirdifferentweightinthegroundtruth.Also, 2 θ
thevolume-preservingflowreallyhasonlyonemaximumandthe
secondpseudomodeisconnectedtothefirstbyabridgeofhigh In the proof of this statement explicit formulas for the
density. AnormalizingflowwithvariableJacobindeterminant rotations, offset and scaling functions in the three affine
doesnothavetheseissues(thirdrow). couplinglayersaregiven. Tomakeourpoint, letustake
a closer look at the scaling terms s(b) of the affine cou-
plings a˜ = s(b) ⊙ a + t(b). For the three affine cou-
Figure 2 illustrates this shortcoming by learning a two- pling layers the scaling factors are given by s = ϵ′ and
1
dimensionaltargetdistributionwithanvolume-preserving s = s = ϵ′′ for each active dimension where ϵ′ and ϵ′′
2 3
flow. TheproblemisthatbyEquation(11)thereisaone-to- aretwoconstantssmallerthanϵ. Computingthenetwork’s
onecorrespondencebetweenmaximaofp θ(x)andp(z)and Jacobiandeterminantwefind(cid:12) (cid:12)f−1′(cid:12) (cid:12)=(ϵ′·ϵ′′·ϵ′′)D 2 and
θ
theneighborhoodsindataandlatentspace. Intheexample, |f′|=(ϵ′·ϵ′′·ϵ′′)−D
2.
thisconnectsthelearned“modes”byathinbridgeandthey θ
reallyformoneconnectedregionofhighprobabilitywith- The derived expressions for Jacobian determinants show
outabarrier. Inaddition,thedensityisoffatbothmodes. twoimportantshortcomingsfortheuniversalitytheorems.
Anormalizingflowwithflexiblevolumechangedoesnot Thefirstoneisthatastheapproximationerrorϵbecomes
havetheseissuesandcorrectlyapproximatesthebimodal very small, ϵ′ and ϵ′′ also becomes very small. For the
distribution. WegivethedetailedproofinAppendixB.2 forwardpassthisleadstoavanishingandfortheinverse
andexperimentaldetailsinAppendixE.2. passtoanexplodingJacobiandeterminant. Thisillustrates
thepointmadeinKoehleretal.(2021,Remark2)thatfor
Thiscanpartiallyberecoveredbyhavinga(learnable)multi-
smallapproximationerrors,thenetworkisill-conditioned,
modaldistributioninthelatentspace,butthisisnecessarily
makingtheconstructionunrealistic.
limited if the structure of the distribution to learn is un-
known. Thesecondpointisamorefundamentalissue. Thederived
5OntheUniversalityofCoupling-basedNormalizingFlows
expressionsfortheJacobiandeterminantsofthenormaliz- parameters:
ingflowwiththeconstructedparametersforthethreeaffine
couplingsasgivenin(Koehleretal.,2021)showthatthese minD KL(p θ∪φ(z)∥p(z)). (17)
φ
determinantsareconstantasϵ′andϵ′′areconstantfactors.
Theresultingconstructionisthereforeavolume-preserving Thisallowsustomeasuretheadditionallossimprovement
flow, which we considered in the previous section. This thatwasachievedbyaddingonemoreaffinecouplingblock:
meansthatbyourTheorem4.2andCorollary4.3,there-
∆ :=minD (p (z)∥p(z))−D (p (z)∥p(z)).
sultingnormalizingflowsarenotdistributionaluniversal affine KL θ∪φ KL θ
φ
approximatorsunderKLdivergenceandalwaysrepresent (18)
unimodaldistributionsregardlessofthedatadistribution. Notethatforourargumentitissufficienttoconsideraffine
couplingblocks,buttheresultsextendtomoreexpressive
Theinsightsfromthissection,thatexistingconstructions
couplingfunctionsaswell.
relyonill-conditionednormalizingflowsanddonotcon-
vergeunderKLdivergence,motivatetheintroductionofour Thefollowingtheoremallowsustousetheabovelossim-
newuniversalitytheorem. provement∆ asaconvergencemetricfordistributions.
affine
Itstatesthataddinganothercouplinglayercanalwaysim-
4.4.ConvergenceMetric prove on the loss L unless it has already converged to a
standardnormalinthelatentspace:
Ideally, wewouldmakeauniversalitystatementinterms
Theorem4.4. Letp(z)beacontinuousprobabilitydistri-
of the KL divergence in Equation (3) as our measure of
bution with finite first and second moment and p(x) > 0
convergence. Itisnotonlythemetricusedinpractice, it
for all x ∈ RRD. Then, the distribution is the standard
alsoastrongmetricofconvergencethatimpliesweakcon-
normaldistributionifandonlyifanaffinecouplingblock
vergence,thatensuresconvergenceofexpectationvalues,
withaReLUsubnetworkθ(x )containingatlisttwo
and it implies convergence of the densities (Gibbs & Su, 1,...,D/2
hiddenlayerscannotimprovetheKLdivergenceasgiven
2002).Also,asweshowedpreviouslyinSection4.2,theKL
byEquation(18):
divergenceisabletodistinguishtheexpressivitybetween
volume-preservingandnon-volume-preservingflows,but
p(z)=N(z;0,I)⇔∆ =0. (19)
weakconvergenceandWassersteindistancearenot(Sec- affine
tion4.3).
This shows that the maximally achievable loss improve-
Themetricofconvergenceweconsiderinourproofisin- ment∆ isausefulconvergencemetricfornormalizing
affine
deed related to the Kullback-Leibler divergence. To con- flows: Ifaddingmorelayershasnoeffectthenthelatent
structit,rewritethelossLinEquation(3)incomparingthe distributionhasconvergedtotherightdistribution.
currentlatentdistributionp (z)asthepushforwardofp(x)
θ
In the remainder of this section, we give a sketch of the
throughourflowf (x):
θ
proofofTheorem4.4,withtechnicaldetailsmovedtoAp-
pendixC.1. Wewillcontinuewithouruniversalitytheorem
L=D KL(p(x)∥p θ(x)) (13) inthenextsection.
(cid:90) p(x)
= p(x)log dx (14) Weproceedasfollows: First,weuseanexplicitformofthe
p(z =f(x))|f′(x)| maximallossimprovement∆∗ forinfinitelyexpressive
affine
(cid:90) p(f−1(z))|f−1′(z)| affinecouplingblocks(Draxleretal.,2020). Then,weshow
= p(z)|f−1′(z)|log θ dz (15)
θ p(z) inLemma4.5thatconvergenceoftheseunrealisticnetworks
isequivalenttoconvergenceoffiniteReLUnetworks. Fi-
=D (p (z)∥p(z)). (16)
KL θ nally,weshowthat∆ = 0impliesp(z) = N(z;0,I).
affine
While this derivation is constructed for affine coupling
This identity shows that the divergence between the true blocks,italsoholdsforcouplingfunctionswhicharemore
p(x)andthemodelp (x)canequallybemeasuredinthe expressive(seeAppendixAforallapplicablecouplingswe
θ
latent space, via the KL divergence between the current areawareof): Ifanaffinecouplingblockcannotmakean
latent distribution that the model generates from the data improvement,neithercanamoreexpressivecoupling. The
p (z)andthetargetlatentdistributionp(z). otherdirectionistrivial,sincebyp(z)=N(0,I),noloss
θ
improvementispossible.
Letusnowconsiderwhathappensifweappendonemore
affinecouplingblocktoanexistingnormalizingflowf (x), Ifweassumeforamomentthatneuralnetworkscanexactly
θ
resultinginaflowwhichwecallp (x). Letuschoose representarbitrarycontinuousfunctions,thenthishypothet-
θ∪φ
theparametersoftheadditionalcouplingblockφsuchthat icalmaximallossimprovementwascomputedbyDraxler
itmaximallyreducesthelosswithoutchangingtheprevious et al. (2020, Theorem 1). A single affine coupling block
6OntheUniversalityofCoupling-basedNormalizingFlows
with a fixed rotation layer Q, in order to maximally re- Finally,ifthefirsttwoconditionalmomentsofanylatent
duce the loss, will standardize the data by normalizing distributionp(z)arenormalizedforallrotationsQ:
the first two moments of the active half of dimensions
a = (Qx)
D/2+1,...,D
conditioned on the passive half of E ai|b[a i]=0, Var ai|b[a i]=1, (26)
dimensionsb=(Qx) . Themomentsbeforethecou-
1,...,D
then the distribution must be the standard normal distri-
pling
bution: p(z) = N(z;0,I). This can be obtained directly
E [a ]=m (b), Var [a ]=σ (b) (20) bycombiningGaussianidentificationresults(Eaton,1986;
ai|b i i ai|b i i
Bryc,1995).
aremappedto:
This concludes the proof sketch of Theorem 4.4 and we
arenowreadytopresentouruniversalityresult,employing
E [a˜ ]=0, Var [a˜ ]=1. (21)
a˜i|b i a˜i|b i ∆ affineasaconvergencemetric.
This is achieved via the following affine transformation,
4.5.AffineCouplingFlowsUniversality
shiftingthemeantozeroandscalingthestandarddeviation
toone: Toconstructouruniversalcouplingflow,wefollowasimple
1
a˜ (a ;b)= (a −m (b)). (22) iterativescheme. Westartwiththedatadistributionasour
i i σ (b) i i
i originalguessforthelatentdistribution: p (z)=p(x=z).
0
Intermsofloss,thistransformationcanatmostachievethe Then,weappendasingleaffinecouplingblockf blk(x)con-
followinglossimprovement,withacontributionfromeach sistingofarotationQandacouplingf cpl. Weoptimizethe
passivecoordinateb: newparameters to maximally reduce the loss as in Equa-
tion(17)andgetanewlatentestimatep (z)=p (z).
1 φ
D/2
∆∗ =max1 (cid:88) E (cid:2) m2(b)+σ2(b)−1−logσ2(b)(cid:3) . Thefollowingtheoremassuresusthatiteratingthisproce-
affine Q 2 b i i i dure makes the latent distribution p n(z) converge to the
i
(23) standardnormaldistributioninthelatentspacep(z):
With the asterisk, we denote that this improvement can Theorem4.6. Coupling-basednormalizingflowswithaffine
not necessarily be reached in practice with finite neural couplingsaredistributionaluniversalapproximatorunder
networks. Moreexpressivecouplingfunctionscanreduce theconvergencemetric∆ asgiveninSection4.4.
affine
thelossstronger. WepickuponthispointinSection4.6.
The proof idea is simple: The convergence metric ∆
affine
Whatlossimprovementcanbeachievedifwegobackto
measureshowmuchaddinganotheraffinecouplingblock
finiteneuralnetworks? Inthefollowingstatement,weshow
canreducethelossL,butthetotallossthatcanbereduced
that∆∗ >0isequivalenttotheexistenceofatwolayer
affine by the concatenation of many blocks is bounded. Thus,
ReLUsubnetworkwithfinitewidthwhichdeterminesthe
laterlayerscannotarbitrarilyimproveonthelossandtheir
parameters in an affine coupling block f that achieves
cpl lossimprovements∆ mustconvergetozero. ByThe-
∆ >0: affine
affine orem4.4,thefixedpointofthisprocedurehasastandard
Lemma4.5. Givenacontinuousprobabilitydensityp(z) normal distribution in the latent space. We give the full
onz ∈Rk. Then, proofinAppendixC.2.
∆∗ >0 (24) Figure1showsanexampleforhowTheorem4.6constructs
affine
thecouplingflowinordertolearnatoydistribution. The
ifandonlyifthereisaReLUneuralnetworkwithtwohidden affine coupling flow is able to learn the distribution well,
layerswithafinitenumberofneuronssuchthat: despitethedifficulttopologyoftheproblem.
Whileourproofremovesspuriousconstructionspresentin
∆ >0. (25)
affine previouswork,therearestillsomepropertieswehopecan
beimprovedinthefuture: First,theconstructiondoesnot
Thissaysthattheevents∆ =0and∆∗ =0canbe exploitthatadeepstackofblockscanundertakecoordinated
affine affine
usedinterchangeably. Theequivalencecomesfromthefact action,whichcanbefoundusingend-to-endtraining. Sec-
thatif∆ >0,thenwecanalwaysconstructatwo-layer ondly,itisunclearhowtheconvergencemetricSection4.4
affine
ReLUneuralnetworkthatscalestheconditionalstandard isrelatedtoconvergenceinthelossusedinpractice,theKL
deviationsclosertooneandtheconditionalmeanscloser divergencegiveninEquation(3). Weconjecturethatour
to zero. In the detailed proof in Appendix C.1.2 we also wayofsettingupthecouplingflowalsoconvergesinKL
makeuseofaclassicalregressionuniversalapproximation divergence. Thereverseholds: WeshowinCorollaryC.3in
theorem(Hornik,1991). AppendixC.3thatconvergenceinKLimpliesconvergence
7OntheUniversalityofCoupling-basedNormalizingFlows
wheretheexpectationagaingoesoverthepassivecoordinate
b=(Qx) .
1,...,D/2
Here, the additional loss improvement is the conditional
negentropyJ(b)=(cid:80)D/2D
(p (a |b)∥N(m (b),σ (b)),
i=1 KL θ i i i
whichmeasuresthedeviationofeachactivedimensionfrom
aGaussiandistributionwithmatchingmeanandvariance.
An affine coupling function c(a ;θ) = sa + t doesn’t
i i
influence this term, due to its symmetrical effect on both
sides of the KL in J(p) (Draxler et al., 2022, Lemma 1).
Moreexpressivecouplingblocks,however,areabletotapon
thislosscomponentiftheconditionaldistributionsp(a |b)
i
Figure3. Onlycouplingfunctionsstrictlymoreexpressivethan
aresignificantlynon-Gaussian,seeFigure3foranexample.
affinecanfitnon-Gaussianconditionalsp(a |b)inasingleblock,
i
resultinginafasterlossdecrease(Equation(27)).Notethatcou- Theimpactofthisgainlikelyvarieswiththedataset. For
plingflowsofbothkindsareuniversal. instance, inimages, thedistributionofonecolorchannel
ofonepixelconditionedontheothercolorchannelsinthe
entireimage,oftenshowsasimpleunimodalpatternwith
underournewmetric. Finally,ourproofgivesnoguarantee
lownegentropy.Thisasuccessfulscenarioinseparatingpas-
onthenumberofrequiredcouplingblocks. Wehopethat
siveandactivedimensionsinimages(Kingma&Dhariwal,
ourcontributionpavesthewaytowardsafullunderstanding
2018). WegiveadditionaltechnicaldetailsonEquation(27)
ofaffinecoupling-basednormalizingflows.
andthesubsequentargumentsinAppendixD.
4.6.ExpressiveCouplingFlowUniversality
5.Conclusion
The above Theorem 4.6 shows that affine couplings
c(a i;θ) = sa i+taresufficientforuniversaldistribution Ournewuniversalityproofsshowanintriguinghierarchy
approximation. AsmentionedinSection3, aplethoraof oftheuniversalityofdifferentcouplingblocks:
moreexpressivecouplingfunctionshavebeensuggested,
forexampleneuralsplineflows(Durkanetal.,2019b)that 1. Volume-preservingnormalizingflows,i.e.flowswith
use monotone rational-quadratic splines as the coupling aconstantvolumechangesuchasthecoupling-based
function. Itturnsoutthatbychoosingtheparametersinthe NICE and GIN (Dinh et al., 2015; Sorrenson et al.,
rightway,allcouplingfunctionsweareawareofcanexactly 2019)arenotuniversalinKLdivergence,meaningthat
representanaffinecoupling,exceptforvolume-preserving thereisafundamentallimitinwhatdistributionsthey
variants,seeAppendixA.Forexample,arationalquadratic canrepresent.
splinecanbeparameterizedasanaffinefunctionbyusing
equidistantknots(a ,a˜ )wherea˜ = sa +tandfixing 2. Affine coupling flows such as RealNVP (Dinh et al.,
k k k k
thederivativeateachknottos. 2017) are distributional universal approximators de-
spitetheirseeminglyrestrictivearchitecture.
Thus,theuniversalityofmoreexpressivecouplingfunctions
followsimmediatelyfromTheorem4.6,justlikeIshikawa 3. Coupling flows with more expressive coupling func-
etal.(2022)extendedtheirresultsfromaffinetomoreex- tionsarealsouniversalapproximators, buttheycon-
pressivecouplings: vergefasterbytappingonanadditionallosscomponent
Corollary4.7. Coupling-basednormalizingflowswithcou- inlayer-wisetraining.
pling functions at least as expressive as affine couplings
aredistributionaluniversalapproximatorundertheconver- Ourworktheoreticallygroundswhycouplingblocksarethe
gencemetric∆ asgiveninSection4.4. standardchoiceforpracticalapplicationswithnormalizing
affine
flows,combinedwiththeireasyimplementationandspeed
OurproofofTheorem4.6,constructedthroughlayer-wise
in training and inference. We remove spurious construc-
training, shows how more expressive coupling functions
tionspresentinpreviousproofsanduseasimpleprinciple
canoutperformaffinefunctionsusingthesamenumberof
instead: Constructaflowlayerbylayeruntilnomoreloss
blocks. Similartothelossimprovementforanaffinecou-
improvementcanbeachieved.
plinginEquation(18),letuscomputethemaximallypos-
siblelossimprovementforanarbitrarilyflexiblecoupling Using volume-preserving flows may have negatively af-
function: fectedexistingwork. Thisshortcomingcanbe(partially)
addressed by choosing or learning a more flexible latent
∆∗ universal =maxE b[J(b)+∆∗ affine(Q)]≥∆∗ affine, (27) distribution.
Q
8OntheUniversalityofCoupling-basedNormalizingFlows
ImpactStatement Dinh,L.,Krueger,D.,andBengio,Y. NICE:Non-linearIn-
dependentComponentsEstimation. InInternationalCon-
Thispaperpresentsworkwhosegoalistoadvancethefield
ferenceonLearningRepresentations, WorkshopTrack,
of Machine Learning. There are many potential societal
2015.
consequences of our work, none which we feel must be
specificallyhighlightedhere. Dinh,L.,Sohl-Dickstein,J.,andBengio,S. Densityesti-
mationusingRealNVP. InInternationalConferenceon
Acknowledgements
LearningRepresentations,2017.
Draxler,F.,Schwarz,J.,Schno¨rr,C.,andKo¨the,U. Char-
This work is supported by Deutsche Forschungsgemein-
acterizingtheRoleofaSingleCouplingLayerinAffine
schaft (DFG, German Research Foundation) under Ger-
NormalizingFlows. InGermanConferenceonPattern
many’sExcellenceStrategyEXC-2181/1-390900948(the
Recognition,2020.
Heidelberg STRUCTURES Cluster of Excellence). It
is also supported by the Vector Stiftung in the project
Draxler,F.,Schno¨rr,C.,andKo¨the,U. WhiteningConver-
TRINN(P2019-0092). Theauthorsacknowledgesupport
gence Rate of Coupling-based Normalizing Flows. In
bythestateofBaden-Wu¨rttembergthroughbwHPCandthe
NeurIPS,2022.
GermanResearchFoundation(DFG)throughgrantINST
35/1597-1FUGG.WethankArmandRousselotandPeter Draxler,F.,Ku¨hmichel,L.,Rousselot,A.,Mu¨ller,J.,Sch-
Sorrensonforthefruitfuldiscussionsandfeedback. noerr, C., and Koethe, U. On the convergence rate of
gaussianizationwithrandomrotations. InInternational
ConferenceonMachineLearning,2023.
References
Durkan, C., Bekasov, A., Murray, I., and Papamakarios,
Ardizzone,L.,Bungert,T.,Draxler,F.,Ko¨the,U.,Kruse,
G. Cubic-Spline Flows. In International Conference
J.,Schmier,R.,andSorrenson,P. FrameworkforEasily
onMachineLearning,WorkshopTrack,2019a. doi: 10.
InvertibleArchitectures(FrEIA),2018a.
48550/ARXIV.1906.02145.
Ardizzone,L.,Kruse,J.,Rother,C.,andKo¨the,U. Analyz-
Durkan, C., Bekasov, A., Murray, I., and Papamakarios,
ingInverseProblemswithInvertibleNeuralNetworks. In
G. Neuralsplineflows. Advancesinneuralinformation
InternationalConferenceonLearningRepresentations,
processingsystems,32,2019b.
2018b.
Eaton,M.L. Acharacterizationofsphericaldistributions.
Bryc,W. TheNormalDistribution,volume100ofLecture JournalofMultivariateAnalysis,20(2):272–276,Decem-
NotesinStatistics. SpringerNewYork,NewYork,NY, ber1986. ISSN0047259X. doi: 10.1016/0047-259X(86)
1995. ISBN978-0-387-97990-8978-1-4612-2560-7. doi: 90083-7.
10.1007/978-1-4612-2560-7.
Gibbs, A. L. and Su, F. E. On Choosing and Bounding
Probability Metrics. International Statistical Review /
Cambanis, S., Huang, S., and Simons, G. On the theory
RevueInternationaledeStatistique,70(3):419–435,2002.
ofellipticallycontoureddistributions. JournalofMulti-
ISSN03067734,17515823.
variateAnalysis,11(3):368–385,September1981. ISSN
0047259X. doi: 10.1016/0047-259X(81)90082-8.
Glorot, X. and Bengio, Y. Understanding the diffi-
culty of training deep feedforward neural networks.
Cardoso,J.-F. Dependence,CorrelationandGaussianityin
In Teh, Y. W. and Titterington, M. (eds.), Proceed-
IndependentComponentAnalysis. JournalofMachine
ings of the Thirteenth International Conference on Ar-
Learning Research, 4:1177–1203, 2003. ISSN 1532-
tificial Intelligence and Statistics, volume 9 of Pro-
4435.
ceedings of Machine Learning Research, pp. 249–
256, Chia Laguna Resort, Sardinia, Italy, 13–15 May
Chen, S. and Gopinath, R. Gaussianization. In Leen, T.,
2010. PMLR. URL https://proceedings.mlr.
Dietterich,T.,andTresp,V.(eds.),AdvancesinNeural
press/v9/glorot10a.html.
InformationProcessingSystems,volume13.MITPress,
2000. Harris,C.R.,Millman,K.J.,vanderWalt,S.J.,Gommers,
R.,Virtanen,P.,Cournapeau,D.,Wieser,E.,Taylor,J.,
Dibak, M., Klein, L., Kra¨mer, A., and Noe´, F. Temper- Berg,S.,Smith,N.J.,Kern,R.,Picus,M.,Hoyer,S.,van
ature steerable flows and Boltzmann generators. Phys. Kerkwijk,M.H.,Brett,M.,Haldane,A.,delR´ıo,J.F.,
Rev. Res., 4(4):L042005, October 2022. doi: 10.1103/ Wiebe,M.,Peterson,P.,Ge´rard-Marchant,P.,Sheppard,
PhysRevResearch.4.L042005. K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C.,
9OntheUniversalityofCoupling-basedNormalizingFlows
and Oliphant, T. E. Array programming with NumPy. Ko¨the, U. A review of change of variable formulas for
Nature,585(7825):357–362,2020. generativemodeling. arXivpreprintarXiv:2308.02652,
2023.
Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P.
Lee, H., Pabbaraju, C., Sevekari, A. P., and Risteski, A.
Flow++: ImprovingFlow-BasedGenerativeModelswith
Universalapproximationusingwell-conditionednormal-
VariationalDequantizationandArchitectureDesign. In
izingflows. InRanzato,M.,Beygelzimer,A.,Dauphin,
InternationalConferenceonMachineLearning,2019.
Y., Liang, P., and Vaughan, J. W. (eds.), Advances in
Hoogeboom,E.,Satorras,V.G.,Vignac,C.,andWelling, NeuralInformationProcessingSystems,volume34,pp.
M. Equivariantdiffusionformoleculegenerationin3d. 12700–12711.CurranAssociates,Inc.,2021.
In International Conference on Machine Learning, pp.
Lyu,J.,Chen,Z.,Feng,C.,Cun,W.,Zhu,S.,Geng,Y.,Xu,
8867–8887.PMLR,2022.
Z., and Chen, Y. Universality of parametric Coupling
Flowsoverparametricdiffeomorphisms. arXivpreprint
Hornik,K. Approximationcapabilitiesofmultilayerfeed-
arXiv:2202.02906,2022.
forwardnetworks. NeuralNetworks,4(2):251–257,1991.
ISSN08936080. doi: 10.1016/0893-6080(91)90009-T. Mu¨ller,T.,Mcwilliams,B.,Rousselle,F.,Gross,M.,and
Nova´k,J. NeuralImportanceSampling. ACMTransac-
Huang,C.-W.,Krueger,D.,Lacoste,A.,andCourville,A.
tionsonGraphics,38(5):1–19,2019. ISSN0730-0301.
NeuralAutoregressiveFlows. InInternationalConfer-
doi: 10.1145/3341156.
enceonMachineLearning,2018.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Huang,C.-W.,Dinh,L.,andCourville,A. AugmentedNor- Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga,
malizingFlows: BridgingtheGapBetweenGenerative L.,etal. Pytorch: Animperativestyle,high-performance
FlowsandLatentVariableModels. InInternationalCon- deep learning library. Advances in neural information
ferenceonLearningRepresentations, WorkshopTrack, processingsystems,32,2019.
2020.
Rezende,D.andMohamed,S. Variationalinferencewith
normalizingflows. InBach,F.andBlei,D.(eds.),ICML,
Hunter,J.D. Matplotlib: A2Dgraphicsenvironment. Com-
July2015.
putinginScience&Engineering,9(3):90–95,2007.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ishikawa, I., Teshima, T., Tojo, K., Oono, K., Ikeda,
Ommer,B. High-resolutionimagesynthesiswithlatent
M., and Sugiyama, M. Universal approximation prop-
diffusionmodels. InProceedingsoftheIEEE/CVFCon-
erty of invertible neural networks. arXiv preprint
ferenceonComputerVisionandPatternRecognition,pp.
arXiv:2204.07415,2022.
10684–10695,2022.
Jaini, P., Selby, K. A., and Yu, Y. Sum-of-Squares Poly- Sorrenson,P.,Rother,C.,andKo¨the,U. Disentanglement
nomialFlow. InInternationalConferenceonMachine bynonlinearICAwithgeneralincompressible-flownet-
Learning,2019. works(GIN). InInternationalConferenceonLearning
Representations,2019.
Kingma,D.P.andBa,J. Adam: Amethodforstochastic
optimization,2017. Teshima, T., Ishikawa, I., Tojo, K., Oono, K., Ikeda, M.,
and Sugiyama, M. Coupling-based Invertible Neural
Kingma, D. P. and Dhariwal, P. Glow: Generative flow NetworksAreUniversalDiffeomorphismApproximators.
withinvertible1x1convolutions. InAdvancesinNeural InAdvancesinNeuralInformationProcessingSystems,
InformationProcessingSystems,2018. 2020a.
Teshima,T.,Tojo,K.,Ikeda,M.,Ishikawa,I.,andOono,K.
Kobyzev, I., Prince, S. J., and Brubaker, M. A. Normal-
Universal Approximation Property of Neural Ordinary
izing Flows: An Introduction and Review of Current
DifferentialEquations. InAdvancesinNeuralInforma-
Methods. IEEE Transactions on Pattern Analysis and
tionProcessingSystems,WorkshopTrack,2020b.
Machine Intelligence, 43(11):3964–3979, 2021. ISSN
0162-8828,2160-9292,1939-3539.doi:10.1109/TPAMI.
Thepandasdevelopmentteam. Pandas-dev/pandas: Pandas,
2020.2992934.
February2020.
Koehler,F.,Mehta,V.,andRisteski,A. Representational Wehenkel,A.andLouppe,G. UnconstrainedMonotonic
aspectsofdepthandconditioninginnormalizingflows. Neural Networks. In Advances in Neural Information
InInternationalConferenceonMachineLearning,2021. ProcessingSystems,2019.
10OntheUniversalityofCoupling-basedNormalizingFlows
WesMcKinney. DataStructuresforStatisticalComputing
inPython. InvanderWalt,S.andJarrodMillman(eds.),
9thPythoninScienceConference,2010.
Zhang,H.,Gao,X.,Unterman,J.,andArodz,T. Approxi-
mationCapabilitiesofNeuralODEsandInvertibleResid-
ualNetworks. InInternationalConferenceonMachine
Learning,2020.
Ziegler,Z.andRush,A. LatentNormalizingFlowsforDis-
creteSequences.InInternationalConferenceonMachine
Learning,2019.
11OntheUniversalityofCoupling-basedNormalizingFlows
A.CompatibleCouplingFunctions
Thefollowinglistsallcouplingfunctionsc(a;θ)(seeEquation(6)foritsusage)weareawareof.Ouruniversalityguarantees
Theorem4.6andCorollary4.7holdforallofthem:
• AffinecouplingflowsasRealNVP(Dinhetal.,2017)andGLOW(Kingma&Dhariwal,2018):
c(x;θ)=sx+t. (28)
Here, θ = [s;t] ∈ R × R. Note that NICE (Dinh et al., 2015) is explicitly excluded from this list as it is a
+
volume-preservingflow(seeSection4.2).
• Nonlinearsquaredflow(Ziegler&Rush,2019):
c
c(x;θ)=ax+b+ , (29)
1+(dx+h)2
forθ =[a,b,c,d,h]∈R ×R4. Choosec=0toobtainanaffinecoupling.
+
• Flow++(Hoetal.,2019):
 
K (cid:18) (cid:19)
c(x;θ)=sσ−1 (cid:88) π jσ x−
σ
µ j +t. (30)
j
j=1
Here,θ =[s;t;(π ,µ ,σ )K ]∈R ×R×(R×R×R )K andσisthelogisticfunction. Chooseallπ =0except
j j j j=1 + + j
forπ =1,allµ =0andallσ =1toobtainanaffinecoupling.
1 j j
• SOSpolynomialflows(Jainietal.,2019):
(cid:90) x k (cid:32) r (cid:33)2
(cid:88) (cid:88)
c(x;θ)= a ul du+t. (31)
l,κ
0 κ=1 l=0
Here,θ =[t;(a ) ]∈R×Rrk. Choosealla =0exceptfora =stoobtainanaffinecoupling.
l,κ l,κ l,κ 1,0
• Splineflowsinallvariants: Cubic(Durkanetal.,2019a),piecewise-linear,monotonequadratic(Mu¨lleretal.,2019),
andrationalquadratic(Durkanetal.,2019b)splines. Asplineisparameterizedbyknotsθwithoptionalderivative
informationdependingonthesplinetype,andccomputesthecorrespondingsplinefunction. Choosethesplineknots
asy =sx +tforanaffinecoupling,choosethederivativesasx′ =sforanaffinecoupling.
i i i
• Neural autoregressive flow (Huang et al., 2018) use a feed-forward neural network to parameterize c(x;θ) by a
feed-forwardneuralnetwork. Theyshowthataneuralnetworkisguaranteedtobebijectiveifallactivationfunctions
arestrictlymonotoneandallweightspositive. OnecanconstructaReLUnetworkwithasinglelinearregiontoobtain
anaffinecoupling.
• Unconstrainedmonotonicneuralnetworks(Wehenkel&Louppe,2019)alsouseafeed-forwardneural,butrestrict
ittohavepositiveoutput. Toobtainc(x;θ),thisfunctionisthennumericallyintegratedwithalearnableoffsetfor
x=0. Chooseaconstantneuralnetworktoobtainanaffinecoupling.
B.ProofsonVolume-preservingFlows
B.1.ProofofTheorem4.2
In this section we want to present a two-dimensional example, for which no normalizing flow with constant Jacobian
determinantcanbeconstructedsuchthattheKL-divergencebetweenthedatadistributionandthedistributiondefinedbythe
normalizingflowiszero.

0.9 if (x,y)∈[−0.5,0.5]×[−0.5,0.5]
0.9−k·(|x|−0.5)
if |x|∈(cid:2) 0.5,0.9 +0.5(cid:3) ∧|y|∈[0,|x|]
p(x,y)= k (32)
0 0.9−k·(|y|−0.5) i ef lse|y|∈(cid:2) 0.5,0 k.9 +0.5(cid:3) ∧|x|∈[0,|y|]
12OntheUniversalityofCoupling-basedNormalizingFlows
The data distribution p(x,y) which has to be approximated by the model is defined in (32). This data distribution has
aconstantvalueof0.9inaboxcenteredaroundtheoriginwithasidelengthofone. Thisregionofconstantdensityis
skirtedbyamarginwherethedensitydecreaseslinearlytozero. Outsidethedecreasingregion,thedensityiszero. The
lineardeclineisgovernedbytheconstantkin(32)whichhastobechosensuchthatthedensityintegratestoone. Since
ourexampleonlyrequirestheregionofconstantdensitybutnotthedecayingtailsofit,theexactfunctionalformofthe
decayingregionsarenotrelevantaslongastheyleadtoaproperlynormalizeddistribution. Equation(32)onlyprovidesa
possibledefinitionofsuchadensity.
ToapproximatethisdatadistributionanormalizingflowasdefinedinSection3isconsidered. Inthisexample,wefocuson
normalizingflowswithconstantJacobiandeterminant. TosimplifynotationwedefineJ =|f′(x)|=const.
θ
A=(cid:8) (x,y)∈R2 :0.9−ϵ<p (x,y)(cid:9) (33)
θ
B =[−0.5,0.5]×[−0.5,0.5] (34)
A¯=B\A (35)
Wechooseϵ=0.1andusethisconstanttodefinethesetA(see(33)). InadditionwedefineBwhichistheregionofthe
dataspace,wherethedatadistributionhasaconstantvalueof0.9(see(34)). A¯isthecomplementofAonB(see(35)).
∆ (p,p )= sup |P(A)−P (A)| (36)
affine θ θ
Ameasurable
(cid:114)
1
∆ (p,p )≤ D (p||p ) (37)
affine θ 2 KL θ
Theaimofthisexample,istofindlowerboundsfortheKL-divergencebetweenthedatadistributionandthedistribution
definedbythenormalizingflow. TofindtheseboundsweusePinsker’sinequality(37)(Gibbs&Su,2002)whichlinksthe
totalvariationdistance(36)totheKullback-leiblerdivergence. Itisworthmentioning,thatconstructingonemeasurable
event for which |P(A)−P (A)| > 0 provides a lower bound for the total variation distance and therefore for the KL
θ
divergence.
To construct such an event, we consider two distinct cases, which consider different choice for the normalizing flow,
charaterisedbythevalueoftheabsoluteJacobiandeterminant.
Case1: A=∅
ThiscasearisesiftheabsoluteJacobindeterminantissosmall,thatthedistributiondefinedbythenormalizingflownever
exceedsthelimitdefiningAorifitischosensolarge,thatthevolumeofAvanishes.
Inthiscase,wefindA¯=Band|A¯|=1where|A¯|denotesthevolumeofthedataspaceoccupiedbyA¯. Usingthefactthat
thedatadistributionhasaconstantvalueof0.9inBandthatp <0.9−ϵinA¯=B,
θ
(cid:12) (cid:12)P(A¯)−P θ(A¯)(cid:12) (cid:12)=(cid:12) (cid:12)0.9−P θ(A¯)(cid:12) (cid:12) (38)
≥|0.9·1−(0.9−ϵ)·1| (39)
=|ϵ|=ϵ (40)
Using(40)asalowerboundforthetotalvariationdistance(40)wecanapply(37)tofind(43)asalowerboundfortheKL
divergence.
D (p||p )≥2·∆ (p,p )2 (41)
KL θ affine θ
≥2·ϵ2 (42)
=0.02 (43)
13OntheUniversalityofCoupling-basedNormalizingFlows
Case2: A̸=∅
Insertingthedefinitionofp (x,y)asgivenin(1)intothedefinitionofA(see33)andrewritingtheconditiondefiningthe
θ
setyields(44).
(cid:26) (cid:27)
0.9−ϵ
A= (x,y)∈R2 : <p(z =f (x,y)) (44)
J θ
ThisdefinesasetC inthelatentspacewhichisdefinedin(45).
(cid:26) (cid:27)
0.9−ϵ
C = z ∈R2 : <p(z) (45)
J
SincethenormalizingflowsconsideredinthisexamplehaveaconstantJacobian, thevolumeofAinthedataspaceis
directlylinkedtothevolumeofC inthelatentspacevia(46).
1
|A|= ·|C| (46)
J
ThedefinitionofC (45)shows,thatC isacirclearoundtheoriginofthelatentspace. TodeterminethevolumeofC we
computetheradiusofthiscircle. Thisisdonebyinsertingthedefinitionofthelatentdistribution,whichisatwodimensional
standard normal distribution into the condition defining C (see (45)). This yields (47). Since the latent distribution is
rotationalinvariantonecansimplylookatitasafunctionofthedistancerfromtheorigin. Solvingtheforrleadsto(48).
0.9−ϵ 1
(cid:18) r2(cid:19)
< ·exp − (47)
J 2π 2
(cid:115)
(cid:18) (cid:19)
2π·(0.9−ϵ)
⇒r = −2·log (48)
J
Inserting(48)intotheformulafortheareaofacircleandusing(46),yields(50)asanexpressionforthevolumeofA. The
lowerboundforthevolumeofAarisesfromfindingthelocalmaximum(whichisalsotheglobalmaximum)of(50)with
respecttotheabsoluteJacobiandeterminantJ.
1
|A|= ·π·r2 (49)
J
(cid:18) (cid:19)
1 J
= ·2π·log (50)
J 2π·(0.9−ϵ)
1
≤ (51)
e·(0.9−ϵ)
(52)
Asinthepreviouscase,wenowcompute(cid:12) (cid:12)P(A¯)−P θ(A¯)(cid:12) (cid:12).
(cid:12) (cid:12)P(A¯)−P θ(A¯)(cid:12) (cid:12)=(cid:12) (cid:12)|A¯|·0.9−P θ(A¯)(cid:12) (cid:12) (53)
≥(cid:12) (cid:12)|A¯|·0.9−|A¯|·(0.9−ϵ)(cid:12)
(cid:12) (54)
=|A¯|·ϵ (55)
≥(1−|A|)·ϵ (56)
(cid:18) (cid:19)
1
≥ 1− ·ϵ (57)
e·(0.9−ϵ)
14OntheUniversalityofCoupling-basedNormalizingFlows
Using(41)and(57)asalowerboundforthetotalvariationdistanceandinsertingourchoiceforϵyields(58)asalower
boundfortheKLdivergencebetweenthedatadistributionandthedistributiondefinedbythenormalizingflow.
(cid:18) (cid:18)
1
(cid:19)(cid:19)2
D (p||p )≥2· ϵ· 1− (58)
KL θ e·(0.9−ϵ)
≈0.0058 (59)
Wecanconclude,thatwehavederivedlowerboundsfortheKLdivergencebetweenthedatadistributionandthedistribution
definedbythenormalizingflow,whichcannotbeundercutbyanynormalizingflowwithaconstantabsoluteJacobian
determinant. Therefore,wehaveproventhattheclassofnormalizingflowswithconstant(absolute)Jacobiandeterminant
cannotapproximatearbitrarycontinuousdistributionsifoneusestheKLdivergenceasaconvergencemeasure.
B.2.ProofofCorollary4.3
DefinitionB.1. Givenaprobabilitydensityp(x)andaconnectedsetM ⊂RD. Then,M iscalledamodeofp(x)if
p(x)=p(y) ∀x,y ∈M, (60)
andthereisaneighborhoodU ofM suchthat:
p(x)>p(y) ∀x∈M,y ∈U \M. (61)
Withthisdefinitionofamode,letuscharacterizethecorrespondencebetweenmodesofp (x)andp(z)foranvolume-
θ
preservingflow:
LemmaB.2. Givenalatentprobabilitydensityp(z),adiffeomorphismf :RD →RD withconstantJacobiandeterminant
θ
|f′(x)|=constandamodeM ⊂RD. Then,f(M)isamodeofp (x).
θ θ
Proof. Weshowthatf(M)fulfilsDefinitionB.1. First,foreveryx,y ∈f(M): Thepre-imagesofx,yareuniqueinM as
f isbijective,thatis: f−1(x),f−1(y)∈M. AsM isamode:
p(f−1(x))=p(f−1(y)). (62)
Wefollow:
(f p)(x)=p(f−1(x))|J|=p(f−1(y))|J|=(f p)(y), (63)
♯ ♯
wherewehaveusedthechange-of-variablesformulaforbijectionsandthat|J|=const.
LetU beaneighborhoodofM suchthatEquation(61)isfulfilled. Asf iscontinuous,thereisaneighborhoodV off(M)
suchthatV ⊆f(U). Considerx∈f(M),y ∈V \f(M). AsM isamode:
p(f−1(x))>p(f−1(y)). (64)
Multiplyingbothsidesby|J|,wefind:
(f p)(x)=p(f−1(x))|J|>p(f−1(y))|J|=(f p)(y). (65)
♯ ♯
Thus,f(M)isamodeof(f p)(x)byDefinitionB.1.
♯
ThismakesusreadyfortheproofofageneralizationofCorollary4.3:
TheoremB.3. p(z)andp (x)havethesamenumberofmodes.
θ
Proof. ByLemmaB.2,everymodeofp(x)impliesamodeof(f p)(x). Also,everymodeof(f p)(x)impliesamodeof
♯ ♯
(f−1f p)(x)=p(x). Therefore,thereisaone-to-onecorrespondenceofmodesbetweenp(x)and(f p)(x).
♯ ♯ ♯
15OntheUniversalityofCoupling-basedNormalizingFlows
C.ProofsonAffineCouplingFlows
C.1.ProofofTheorem4.4
C.1.1.UNDERLYINGTHEOREMS
Here,werestatetheresultsfromtheliteraturethatourmainproofisbasedon:
First,(Eaton,1986)showthatifforsomevector-valuedrandomvariableX andeverypairoforthogonalprojectionsthe
meanofoneprojectionconditionedontheotheriszero,thenX followsasphericaldistribution:
TheoremC.1(Eaton(1986)). SupposetherandomvectorX ∈RD hasafinitemeanvector. Assumethatforeachvector
v ̸=0andforeachvectoruperpendiculartov(i.e.u·v =0):
E[u·X|v·X]=0. (66)
ThenX issphericalandconversely.
Secondly, Cambanis et al. (1981, Corollary 8a) identifies the Gaussian from all elliptically contoured (which includes
spherical)distributions. WewriteitintheformofBryc(1995,Theorem4.1.4):
TheoremC.2(Bryc(1995)). Letp(x)beradiallysymmetricwithE[∥x∥α]<∞forsomeα>0. If
E[∥x ∥α|x ]=const, (67)
1,...,m m+1,...,n
forsome1≤m<n,thenp(x)isGaussian.
Finally,Draxleretal.(2020,Theorem1)showthattheexplicitformofthemaximallyachievablelossimprovementbyan
affinecouplingblock∆∗ ifthedataisrotatedbyafixedrotationlayerQisgivenby:
affine
∆∗ (Q)=minD (p (z)∥p(z)) (68)
affine KL s,t|Q
s,t
= 1E (cid:2) m (b)2+σ (b)2−1−logσ (b)2(cid:3) . (69)
2 b i i i
Here,s,tarethescalingandtranslationinanaffinecouplingblock(seeEquation(28)),andweoptimizeovercontinuous
functionsfornow. Byp (z)wedenotethelatentdistributionachievedifa˜(a;b)=s(b)⊙a+t(b)isappliedtop(a,b),
s,t|Q
therotatedversionoftheincomingp(z). Thesymbolsm (b),σ (b)2areconditionalmomentsoftheactivedimensionsa
i i i
conditionedonthepassivedimensionsb:
m (b)=E [a ], σ (b)=E [a2]−m (b)2. (70)
i ai|b i i ai|b i i
Theseconditionalmomentsarecontinuousfunctionsofbifp(x)isacontinuousdistributionandp(b)>0forallpassive
b∈RD/2. TheimprovementinEquation(69)isachievedbytheaffinecouplingblockwiththefollowingsubnetwork:
1 m (b)
s∗(b)= , t∗(b)=− i . (71)
i σ (b) i σ (b)
i i
Notethats∗(b)andt∗(b)arecontinuousfunctionsandnotactualneuralnetworks. Inthenextsection, weshowthata
similarstatementonpracticallyrealizableneuralnetworksthatissufficientforouruniversality.
C.1.2.RELATIONTOPRACTICALNEURALNETWORKS
TorelateEquations(69)and(71)toactuallyrealizablenetworks,whichcannotexactlyfollowthearbitrarycontinuous
functionss∗(b),t∗(b),weshowLemma4.5assertingthatthefixpointofaddingcouplinglayerswithcontinuousfunctions
i i
isthesameasthatofsingle-layerneuralnetworks,derivedusingauniversalapproximationtheoremforneuralnetworks
(Hornik,1991):
Proof. First,notethat∆ ∗(Q)≥∆ (Q)≥0sincenopracticallyrealizablecouplingblockcanachievebetterthan
affine affine
Equation(69). Thus,if∆∗ (Q)=0,sois∆ (Q)=0.
affine affine
16OntheUniversalityofCoupling-basedNormalizingFlows
Forthereversedirection,wefixQ=I,andotherwiseconsiderarotatedversionofp. Also,withoutlossofgeneralization,
weconsideronesingleactivedimensiona inthefollowing,buttheconstructioncanthenberepeatedforeachotheractive
i
dimension.
Ifweapplyanyaffinecouplinglayerf (a;b)=s (b)a+t (b),thelosschangebythislayercanbecomputedfromthe
cpl,φ φ φ
theoreticalmaximalimprovement∆∗ (Q)beforeandafteraddingthislayer∆˜∗ (I):
affine affine
∆ (I)=∆∗ (I)−∆˜∗ (I)= 1E (cid:2) m (b)2+σ (b)2−1−logσ (b)2(cid:3) −1E (cid:2) m˜ (b)2+σ˜ (b)2−1−logσ˜ (b)2(cid:3) .
affine affine affine 2 b i i i 2 b i i i
(72)
Themomentsaftertheaffinecouplinglayersread:
m˜ (b)=s (b)m (b)+t (b), σ˜ (b)=s (b)σ (b). (73)
i φ i φ i φ i
Case1: E [σ (b)2−1−logσ (b)2]>0:
b i i
Then,withoutlossofgenerality,bycontinuityandpositivityofpandconsequentialcontinuityofσ (b)inb,thereisaconvex
i
opensetA ⊂ RD/2 withnon-zeromeasurep(A) > 0whereσ (b) > 1. Ifσ (b) < 1everywhere, applythefollowing
i i
argumentflippedaroundσ (b)=1.
i
Denotebyσ =max σ (b). Then,bycontinuityofσ (b)thereexistsB ⊂Asothatσ (b)>(σ −1)/2+1=:
max b∈A i i i max
σ forallb∈B. LetC ⊂Bbeamulti-dimensionalinterval[l ,r ]×···×[l ,r ]withp(C)>0insideofB.
max/2 1 1 D/2 D/2
Now,weconstructaReLUneuralnetworkwithtwohiddenlayerswiththefollowingproperty,whereF ⊂ E ⊂ C are
specifiedlaterwithp(F)>p(E)>0:

f φ(x)= σma1
x/2
x∈E ⊂D
1 ≤f (x)<1 x∈D (74)
fσm (a xx/ )2
=0
φ
else.
φ
Todoso,wemakefourneuronsforeachdimensioni=1,...,D/2:
ReLU(x −l ),ReLU(x −l −∆ ),ReLU(x −r ),ReLU(x −r +δ), (75)
i i i i affine i i i i
where 0 < ∆ < min (r −l )/4. If we add these four neurons with weights 1,−1,−1,1, we find the following
affine i i i
piecewisefunction:

0
x−l
i
x
l
i≤ <l
xi
<l i+δ
δ l +δ ≤x≤r −δ (76)
i i
r
0i−x rr i− ≤δ x.<x<r
i
i
Ifwe repeatthisfor eachdimension andaddtogether allneuronswith thecorrespondingweights intoasingle neuron
in the second layer, then only inside D = (l +∆ ,r −∆ )×···×(l +∆ ,r −∆ ) ⊂ C the
1 affine 1 affine D/2 affine D/2 affine
weighted sum would equal δD/2. By choosing ∆ as above, this region has nonzero volume. We thus equip the
affine
single neuron in second layer with a bias of −δD/2 + ϵ for some ϵ < δ, so that it is constant with value ϵ inside
E =(l +δ−ϵ,r −δ+ϵ)×···×(l +δ+ϵ,r −δ−ϵ)⊂DandsmoothlyinterpolatestozerointherestofD.
1 1 D/2 D/2
Fortheoutputneuronofournetwork,wechooseweight(σ −1)/ϵandbias1. Byinsertingtheaboveconstruction,
max/2
wefindthenetworkspecifiedinEquation(74).
Now,forallb∈D,
1<σ˜ (b)<σ (b), (77)
i i
sothat
m˜ (b)2+σ˜ (b)2−1−logσ˜ (b)2 <m (b)2+σ (b)2−1−logσ (b)2. (78)
i i i i i i
17OntheUniversalityofCoupling-basedNormalizingFlows
Thus,parametersφexistthatimproveontheloss. (Notethatthisconstructioncanbemademoreeffectiveinpracticeby
identifyingthesetswhereσ >1resp.σ <1andthenbuildingneuralnetworksthatoutputoneorscaletowardsσ˜(b)=1
everywhere. Becauseweareonlyinterestedinidentifyingimprovement,theaboveconstructionissufficient.)
Now,regradingt ,wefocusonE [m (b)2] > 0(otherwisechooset = 0asaconstant,whichcorrespondstoaReLU
φ b i φ
networkwithallweightsandbiasessettozero):
E [m (b)2]>E [(s (b)m (b)+t (b))2]. (79)
b i b φ i φ
By(Hornik,1991,Theorem1)therealwaysisat thatfulfillsthisrelation.
φ
Case 2: E [σ (b)2 −1−logσ (b)2] = 0. Then, choose the neural network s (b) = 1 as a constant. As ∆ > 0,
b i i φ affine
E [m (b)2]>0andwecanusethesameargumentfortheexistenceoft asbefore.
b∼p(a,b) i φ
C.1.3.MAINPROOF
WenowturntotheproofofTheorem4.4:
Proof. Theforwarddirectionistrivial: p(z)=N(0,I)andthereforeD (p(z)∥N(0,I))=0. Asaddingaidentitylayer
KL
isaviablesolutiontoEquation(17),thereisaφwithD (p (z)∥N(0,I))=0,andthus∆ =0.
KL φ affine
Forthereversedirection,startwith∆ =0. Then,byLemma4.5,also∆∗ =0.
affine affine
ThemaximallyachievablelossimprovementforanyrotationQisthengivenby:
∆∗ =max1E (cid:2) m (b)2+σ (b)2−1−logσ (b)2(cid:3) =0. (80)
affine Q 2 b i i i
Itholdsthatbothx2 ≥0andx2−1−logx2 ≥0. Thus,thefollowingtwosummandsarezero:
0= 1E (cid:2) m (b)2(cid:3) , (81)
2 b i
0= 1E (cid:2) σ (b)2−1−logσ (b)2(cid:3) . (82)
2 b i i
ThisholdsforallQsincethemaximumoverQiszero.
Bycontinuityofp(b)andm (b)inp,thisimpliesforallb:
1
E [a ]=0. (83)
a1|b 1
Fixb andmarginalizeouttheremainingdimensionsb tocomputethemeanofa conditionedonb :
1 2,...D/2 1 1
m =E [a ]=E [E [a ]]=E [0]=0. (84)
a1|b a1|b1 1 b1,...,D/2 a1|b 1 b1,...,D/2
Asa andb arearbitraryorthogonaldirectionssincetheaboveisvalidforanyQ,wecanemployTheoremC.1tofollow
1 1
thatp(x)issphericallysymmetric.
Weareleftwithshowingthatforasphericalp(x),ifforallQthereisnoimprovement∆ (Q),thenp(x)=N(0,I).
affine
Withoutlossofgenerality,wecanfixQ=I,as(Q p)(x)=p(x)forallQ. Wewritex=(p;a).
♯
As∆ =0,wecanfollowσ (b)=1likeabove. Thisimpliesthat:
affine i
D/2
E
[∥a∥2]=(cid:88)
(m (b)2+σ (b)2)=D/2. (85)
a|b i i
i=1
Inparticular,thisisindependentofbandwecanthusapplyTheoremC.2withα=2.
Finally,m(b)=0andσ (b)=1forallQimplythatp(x)=N(0,I).
i
18OntheUniversalityofCoupling-basedNormalizingFlows
Q θ
1 1
minℒ
Q 1,θ
1
1
Q θ Q θ
1 1 2 2
minℒ
Q 2,θ
2
2
Q θ Q θ Q θ
1 1 2 2 n n
minℒn
Qn,θn
Figure4. Thenormalizingflowweconstructinourproofisremarkablysimple: Weiterativelyaddcouplingblocks,optimizingthe
parametersofthenewblockwhilekeepingpreviousparametersfixed. Theorem4.4showsthatifaddinganotherblocksshowsno
improvementintheloss,theflowhasconvergedtoastandardnormaldistributioninthelatentspace. Sincethetotallossthatcanbe
removedisfinite,theflowconverges.
C.2.ProofofTheorem4.6
TheproofideaofiterativelyaddingnewlayerswhicharetrainedwithoutchangingpreviouslayersisvisualizedinFigure4.
Proof. Letusconsideracoupling-basednormalizingflowofdepthnandcallthecorrespondinglatentdistributionsp (z),
n
wheren=0correspondstotheinitialdatadistributionp(x). DenotebyL =D (p (z)∥p(z))thecorrespondingloss.
n KL n
Then,ifweaddanotherlayertotheflow,weachieveadifferenceinlossof∆ =L −L .
affine,n n+1 n
Withoutlossofgenerality,weassumethattherotationlayerQofthisadditionalblockcanbechosenfreely. Otherwiseadd
48couplingblockswithfixedrotationsthattogetherexactlyrepresenttheQwewant,asshownbyKoehleretal.(2021,
Theorem2).
WethenchoosetherotationQandsubnetworkparametersφoftheadditionalblocksuchthatitmaximallyreducestheloss
inthesenseofEquation(17),keepingtheparametersofthepreviouslayersfixed. Then,∆ attainsthevaluegivenin
affine,n
Equation(18).
Eachlayercontributesanon-negativeimprovementintheloss,whichintotalcanonlysumuptotheinitialloss. Fora
non-negativeserieswhosetotalsumisfinite,theseriesmustgotozero,whichshowsconvergenceintermsofSection4.4:
∞
(cid:88)
∆ ≤L <∞⇒∆ →0. (86)
affine,n 0 affine,n
n=0
C.3.RelationtoConvergenceinKL
CorollaryC.3. Givenaseriesofprobabilitydistributionsp (z). Then,convergenceinKLdivergence
n
n→∞
D (p (z)∥N(0,1))−−−−→0 (87)
KL n
impliesconvergenceintheconvergencemetricinSection4.4:
n→∞
∆ −−−−→0. (88)
affine,n
Proof. Byassumption,foreveryϵ>0thereexistsN ∈Nsuchthat:
D (p (z)∥N(0,1))<ϵ ∀n>N. (89)
KL n
19OntheUniversalityofCoupling-basedNormalizingFlows
Thisimpliesconvergenceof∆ ,bythefollowingupperboundviathesumofallpossiblefutureimprovementswhich
affine,n
isboundedfromabovebythetotalloss:
∞
(cid:88)
∆ ≤ ∆ ≤D (p (z)∥N(0,1))<ϵ ∀n>N. (90)
affine,n affine,m KL n
m=n
D.BenefitsofMoreExpressiveCouplingBlocks
Toseewhatthebestimprovementforaninfinitecapacitycouplingfunctioncaneverbe,wemakeuseofthefollowing
PythagoreanidentitycombinedfromvariantsinDraxleretal.(2022);Cardoso(2003);Chen&Gopinath(2000):
L=D (p (z)∥N(0,I))=P +E [D(b)+J(b)+S(b)]. (91)
KL θ b∼p(a,b)
ThesymbolsP,D(b),J(b),S(b)alldenoteKLdivergences:
Thefirsttwotermsremainunchangedunderacouplinglayer: TheKLdivergencetothestandardnormalinthepassive
dimensionsP =D (p (b)∥N(0,I )),whichareleftunchanged. ThedependencebetweenactivedimensionsD(b)=
KL θ D/2
D (p (a|b)∥p (a |b)···p (a |b)) measures the multivariate mutual information between active dimensions. It is
KL θ θ 1 θ D/2
unchangedbecauseeachdimensiona istreatedconditionallyindependentoftheothers(Chen&Gopinath,2000).
i
Theremainingtermsmeasurehowfareachdimensionp (a |b)differsfromthestandardnormal: Thenegentropymea-
θ i
sures the divergence to the Gaussian with the same first moments as p (a |b) in each dimension, summing to J(b) =
θ i
(cid:80)D/2D
(p (a |b)∥N(m (b),σ (b))). Finally, the non-Standardness S(b) =
(cid:80)D/2D
(N(m (b),σ (b))∥N(0,1))
i=1 KL θ i i i i=1 KL i i
measureshowfarthese1dGaussianareawayfromthestandardnormaldistribution.
Note that the total loss L is invariant under a rotation of the data. The rotation does, however, affect how that loss is
distributedintothedifferentcomponentsinEquation(91).
Ifwerestrictthecouplingfunctiontobeaffine-linearc(a ;θ)=sa +t(i.e.aRealNVPcoupling),thenthismeansthat
i i
alsoJ(b)isleftunchanged,essentiallybecausep (a |b)andN(m (b),σ (b))undergothesametransformation(Draxler
θ i i i
etal.,2022,Lemma1). Onlyanonlinearcouplingfunctionc(a ;θ)canthusaffectJ(b)andreduceittoJ˜(b)<J(b)(if
i
J(b)>0).
Takingthelossdifferencebetweentwolayers,wefindEquation(27).
E.Experimentaldetails
WebaseourcodeonPyTorch(Paszkeetal.,2019),Numpy(Harrisetal.,2020),Matplotlib(Hunter,2007)forplottingand
Pandas(WesMcKinney,2010;Thepandasdevelopmentteam,2020)fordataevaluation.
Weprovideourcodeathttps://github.com/vislearn/Coupling-Universality. Sequentiallyrunningall
experimentstakeslessthantwohoursonadesktopcomputerwithaGTX2080GPU.
E.1.Layer-wiseflow
InexperimentonatoydatasetforFigure1,wedemonstratethatacouplingflowconstructedlayerbylayerasinEquation(22)
learnsatargetdistribution. Weproceedasfollows:
WeconstructadatadistributiononacircleasaGaussianmixtureofM Gaussianswithmeansm =(rcosφ ,rsinφ ),
i i i
whereφ = 0, 1 2π,...,M−12π areequallyspaced,andσ = 0.3. Theadvantageofapproximatingtheringwiththis
i M M i
constructionisthatthisyieldsasimpletoevaluatedatadensity,whichweneedforaccuratelyplottingp (z):
θ
M
1 (cid:88)
p(x)= N(x;m ,σ2I). (92)
M i
i=1
Wethenfitatotal100layersinthefollowingway: First,treatp(x)astheinitialguessforthelatentdistribution. Then,
webuildtheaffinecouplingblockthatmaximallyreducesthelossusingEquation(22). Wethereforeneedtoknowthe
20OntheUniversalityofCoupling-basedNormalizingFlows
conditional mean m(b) and standard deviation σ(b) for each b. We approximate this from a finite number of samples
N which aregrouped by the passive coordinate b into B bins sothat N/B samples arein eachbin. Wethen compute
the empirical mean m and standard deviation σ over the active dimension in each bin i = 1,...,B. According to
i i
Equation(22),wedefines = 1 andt = − 1 m atthebincentersandinterpolatebetweenbinsusingacubicspline.
i σi i σi i
Outsideofthedomainofthesplines, weextrapolateconstants,twiththevalueoftheclosestbin. Wedonotdirectly
optimizeoverQ,butchoosetheQthatreducesthelossmostoutofN random2drotationmatrices.
Q
Welimitthestepsizeofeachlayertoavoidartifactsfromfinitetrainingdata,bymapping:
x˜=αx+(1−α)f (x). (93)
blk
Inaddition,weresamplethetrainingdatafromthegroundtruthdistributionaftereverysteptoavoidoverfitting.
We choose N = 226, B = 64, M = 20, α = 0.5, N = 10. The resulting flow has 64·2·100 = 12,800 learnable
Q
parameters.
E.2.Volume-preservingflows
The target distribution is a two-dimensional Gaussian Mixture Model with two modes. The two modes have the same
relativeweightbutdifferentcovariancematrices.
ThenormalizingflowwithaconstantJacobiandeterminantconsistsof15GINcouplingblocksasintroducedinSorrenson
etal.(2019). ThistypeofcouplingblockshasaJacobindeterminantofone. Toallowvolumescalingalayerwithalearnable
globalscalingisaddedafterthefinalcouplingblock. Thislearnableweightisinitializedasone. Forthenormalizingflow
withvariableJacobindeterminant,theGINcouplingismodifiedbyremovingthenormalizationofthescalingfactorsinthe
affinecouplings. ThisallowsthenormalizingflowtohavevariableJacobiandeterminants. Inthiscase,theglobalscaling
blockisomitted. ToimplementthenormalizingflowsweusetheFrEIApackage(Ardizzoneetal.,2018a)implementation
oftheGINcouplingblocks.
Inbothnormalizingflows,thetwosub-networksusedtocomputetheparametersoftheaffinecouplingsarefullyconnected
neuralnetworkswithtwohiddenlayersandahiddendimensionalityof128. ReLUactivationsareused. Theweightsof
thelinearlayersofthesubnetworksareinitializedbyapplyingthePyTorchimplementationoftheXavierinitialization
(Glorot&Bengio,2010). Inaddition,theweightsandbiasesofthefinallayerofeachsub-networksaresettozero.
ThenetworksaretrainedusingtheAdam(Kingma&Ba,2017)withPyTorch’sdefaultsettingsandainitiallearningrateof
1·10−3 whichisreducedbyafactoroftenafter5000,10000and15000trainingiterations. Intotal,thetrainingranfor
25000iterations. Ineachiteration,abatchofsize128wasdrawnfromthetargetdistributiontocomputethenegativelog
likelihoodobjective. Weuseastandardnormaldistributionaslatentdistribution.
21