TIC: Translate-Infer-Compile for accurate “text to plan” using
LLMs and logical intermediate representations
SudhirAgarwal and AnuSreepathy
IntuitAIResearch
MountainView,CA,USA.
{sudhir agarwal,anu sreepathy}@intuit.com
Abstract
Westudytheproblemofgeneratingplansforgiven
natural language planning task requests. On one
hand, LLMs excel at natural language processing
but do not perform well on planning. On the
other hand, classical planning tools excel at plan-
ning tasks but require input in a structured lan-
guagesuchasthePlanningDomainDefinitionLan-
guage (PDDL). We leverage the strengths of both
the techniques by using an LLM for generating
thePDDLrepresentation(taskPDDL)ofplanning
task requests followed by using a classical plan-
ner for computing a plan. Unlike previous ap-
proachesthatuseLLMsforgeneratingtaskPDDLs
directly, our approach comprises of (a) translate:
using an LLM only for generating a logically in- Figure1:Anoverviewofplanningbasedresponsegeneration
terpretable intermediate representation of natural
language task descriptions, (b) infer: deriving ad-
Huang et al., 2022; Zeng et al., 2022]. However, LLMs
ditional logically dependent information from the
are known to perform only shallow reasoning and cannot
intermediate representation using a logic reasoner
find complex plans [Valmeekam et al., 2023b; Valmeekam
(currently, Answer Set Programming solver), and
et al., 2023a; OpenAI, 2023]. On the other hand, classical
(c)compile: generatingthetargettaskPDDLfrom
planners can scale better in the number of actions they sup-
thebaseandinferredinformation. Weobservethat
port without compromising on the accuracy [Sohrabi, 2019;
usinganLLMtoonlyoutputtheintermediaterep-
Helmert,2006b].
resentationsignificantlyreducesLLMerrors. Con-
sequently, TIC approach achieves, for at least one Weaddresstheproblemofrespondingtoplanningrelated
LLM,highaccuracyontaskPDDLgenerationfor queries by combining the strengths of LLMs and classical
allsevendomainsofourevaluationdataset. planners.Figure1illustratesahighleveloverviewofthispro-
cess.Theworkpresentedin[Liuetal.,2023]hasshownthat,
for classical planning tasks, using an LLM to generate the
1 Introduction
taskPDDLfromanaturallanguageplanningtaskdescription
Customers of large organizations have a variety of ques- andthenusinganexternalclassicalplannertocomputeaplan
tions or requests (collectively known as queries in the fol- yieldsbetterperformancethanrelyingsolelyonanLLMfor
lowing) pertaining to the organization’s domain of opera- endtoendplanning.However,theirapproach,whichisbased
tion. Providing accurate responses to such user queries re- on in-context examples, does not achieve acceptable accu-
quiresathoroughanalysisoftheuser’scontext,productfea- racy, in particular, for slightly complex planning domains.
tures, domain knowledge, and organizational policies. Cer- One of the primary reasons for failure is that the LLM of-
tain user queries such as how-to questions and state chang- ten makes errors generating information that must abide by
ingrequestsrequiredynamiccompositionofpossibleactions theconstraintsspecifiedinthedomainknowledgeorthetask
(i.e., planning). Recently, transformer-based large language description.
models (LLMs) have shown wide success on many natural Throughout this paper, we use the seven planning do-
language understanding and translation tasks, also demon- mains presented in [Liu et al., 2023]. The natural lan-
strating some general reasoning and planning capability on guage task descriptions of each domain along with the do-
diversetaskswithouthavingtoberetrained[Ahnetal.,2022; main model PDDL can be found at https://github.
4202
beF
9
]LC.sc[
1v80660.2042:viXraFigure2:ExamplebarmantaskdescriptionandtaskPDDL
Youhave1shakerwith3levels,5shotglasses,3dispensersfor3
ingredients.Theshakerandshotglassesareclean,empty,andon
thetable.Yourleftandrighthandsareempty.Thefirstingredient
ofcocktail1isingredient2. Thesecondingredientofcocktail1is
ingredient1. Thefirstingredientofcocktail2isingredient1. The
secondingredientofcocktail2isingredient2. Thefirstingredient
ofcocktail3isingredient1. Thesecondingredientofcocktail3is
ingredient3. Thefirstingredientofcocktail4isingredient3. The
secondingredientofcocktail4isingredient2.Yourgoalistomake
4 cocktails. shot1 contains cocktail1. shot2 contains cocktail4.
shot3containscocktail3.shot4containscocktail2. Figure 3: TIC pipeline for generating task PDDLs from NL task
(define (problem prob) (:domain barman) descriptions
(:objects
shaker1 - shaker
aterepresentationanddomainknowledgerules. Wecall
shot1 shot2 shot3 shot4 shot5 - shot
theunionoffactsintheintermediaterepresentationand
...
)(:init theinferredfactsasthematerializedrepresentation.
(clean shaker1) (clean shot1) (clean shot2) 3. Compile the target task PDDL using the materialized
(clean shot3) (clean shot4) (clean shot5) representationanddomainPDDL.
...
(cocktail_part1 cocktail1 ingredient2) Consequently, we refer to our approach as Translate-Infer-
... Compile (TIC) approach. Section 3 presents details of the
(cocktail_part2 cocktail4 ingredient2) three steps of TIC. Section 4 dives deeper into the translate
)(:goal (and stepandpresentstwotechniquesforgeneratingintermediate
(contains shot1 cocktail1) representationofnaturallanguageplanningtaskdescriptions.
(contains shot2 cocktail4) Our experimental results (presented in Section 5) show that
(contains shot3 cocktail3)
1.TICsupportsmorecomplextaskdescriptionsthanotherin-
(contains shot4 cocktail2))))
termediaterepresentationbasedapproaches. 2.TICachieves
highaccuracy(100%insomecases)onallsevenplanningdo-
mainsoftheevaluationdatasetpresentedin[Liuetal.,2023].
com/Cranial-XIX/llm-pddl. Task PDDLs for these
3.TICisalsomoregeneralasitachieveshighaccuracy(near
and several other planning domains can be generated using
100% in some cases) on the same dataset but with variation
thecollectionpresentedin[Seippetal.,2022].
inlanguageofthetaskdescriptions.
Forexample,considertheplanningtaskdescriptioninFig-
ure2fromawell-knownplanningdomain-barman. Itcon-
2 PreliminariesandRelatedWork
tains cardinality information of shots (5 shot glasses), but
mentionsonly4shotinstancesexplicitly. However, thecor- Inthissection,weprovideabriefoverviewofrelevantback-
rect task PDDL should contain 5 shot instances. An LLM groundmaterialandpriorworkrelatedtorespondingtonat-
does not always abide by this relationship. Similarly, the urallanguageplanningtaskrequests.
statement in task description that all shots are clean should
2.1 ClassicalPlanningandPDDL
lead to generation of PDDL atoms (clean shoti) with
1 ≤ i ≤ 5. But, an LLM sometimes generates only 4 such Givenadescriptionofthepossibleinitialstatesoftheworld,
atoms(oneforeachmentionedshotinstance). a description of the desired goals, and a description of a set
Suchlogicaldependencyrulesinplanningdomainsarede- ofpossibleactions,theplanningprobleminvolvessynthesiz-
terministicandcanbecomputedinanerror-freemannerwith ingaplanthat, whenappliedtoanyinitialstate, generatesa
appropriate logic reasoners to augment the information ex- statewhichcontainsthedesiredgoals(goalstate)[Ghallabet
tractedbyLLMs. Themainideaunderlyingourapproachis al.,2004]. Theplanningdomaindefinitionlanguage(PDDL)
to combine the best of both these technologies (LLMs and servesasastandardizedencodingofclassicalplanningprob-
logicalreasoning)toachievemoreaccurateresultsthanpos- lems[Ghallabetal.,1998;Haslumetal.,2019]. ThePDDL
sible using either of them alone. More precisely, we break representation of a planning problem is typically separated
downtheproblemoftaskPDDLgenerationintothefollow- intotwofiles: adomainfileandataskfile,bothofwhichbe-
ingthreesteps(seealsoFigure3): comeinputstotheplanner. ThedomainPDDLfileincludes
asetofpredicatesthatdefineastatespace,andasetofpos-
1. Translate with an LLM, the natural language planning
sible actions with their preconditions and effects. The task
taskdescriptiontoalogicallanguagewithsufficientex-
PDDLfileprovidesalistofobjectstogroundthedomain,the
pressiveness to model facts, rules and constraints. We
problem’sinitialstateandgoalconditions.
callsuchatranslationanintermediaterepresentationof
thenaturallanguageplanningtaskdescription. 2.2 PlanningwithLLMs
2. Infer, with a reasoner for the chosen logical language, Somerecentworksinvestigatepromptingandfine-tuningap-
therestoftherequiredinformationusingtheintermedi- proachestoenableLLMstosolvePDDLplanningproblems.For example, [Silver et al., 2022] presents an approach of user query to a structured representation and then uses do-
prompting pre-trained LLMs with few-shot examples. The main rules (called Knowledge Modules) on the LLM output
approach presented in [Guan et al., 2023] employs LLMs to deterministically infer more information. It aims at di-
as an interface between PDDL and sources of corrective rectly computing the answer to the query over the provided
feedback, such as PDDL validators and humans. [Zhou et information. However,theapproachhasbeenshowntosup-
al., 2023] introduces ISR-LLM, a framework that improves port only simple input sentences that translate to ASP facts.
LLM-basedplanningthroughaniterativeself-refinementpro- Ourapproach,ontheotherhand,cansupportcomplexplan-
cess. Plansformer presented in [Pallagani et al., 2022] uses ning scenarios with complex data-structures and rules. This
an LLM pretrained on code generation and further trains it is achieved by using a set of rules for translating natural
onplanningproblemsandcorrespondingvalidplans. Frame- language sentences describing cardinality constraints, facts,
workssuchasLangChain[Chase,2022]enabledevelopersto rules, bijective relationships and two-dimensional grids to
combineLLMswithexternaltoolsorAPIs.Suchframeworks ASP. Furthermore, the former approaches have limited ap-
depend on LLMs for selecting and composing tools and do plicability for industrial use cases as it is often not feasible
notscalewellbeyondasmallsetoftools. Otherframeworks to send proprietary data to an LLM or load entire databases
suchasGorilla[Patiletal.,2023]scaletoalargenumberof into a knowledge module. In contrast, we aim at generating
APIsbutrequirefine-tuningofLLMsandhavelimitedplan- a representation that can be used by an appropriate tool to
ning capability. Although these works that rely on LLMs’ efficiently compute the final answer. Specifically, in case of
planning capabilities achieve promising outcomes, the state planningproblems,weaimatgeneratingtaskPDDLthatcan
of the art classical planning tools such as Pyperplan [Alk- besenttoaplannertoobtainthefinalplan.
hazraji et al., 2020] and Fast Downward Planner [Helmert,
2006a]remainfarmoreefficientthanLLMbasedend-to-end 3 TranslateInferCompileApproach
plan generation in terms of wall-clock time and accuracy of
In this section, we present the main steps of the Translate-
plan generation. Due to the limited correctness of LLMs in
Infer-Compile(TIC)approach. Theapproachisflexiblewith
generatingplans,particularlyinthepresenceofalargenum-
respect to the logical language used and can work with any
ber of possible actions, our approach aims at leveraging an
logical language that is expressive enough to semantically
externalplannerfortheplangenerationtask.
represent the information present in user requests of our in-
2.3 PlannerInputGenerationusingLLMs terest. In particular, the logical language should support
primitive datatypes, object types, objects, relationships be-
[Xie et al., 2023] show that LLMs are more adept at trans-
tween objects, cardinality and logical constraints, and infer-
lation than planning, in particular, by leveraging common-
ence rules. In this paper, we use Answer Set Programming
sense knowledge and reasoning to furnish missing details
(ASP) [Gelfond and Lifschitz, 1988; Lifschitz, 2008] as the
fromunder-specifiedgoals(asisoftenthecaseinnaturallan-
logicallanguage.WeusetheexampletaskdescriptioninFig-
guage). Withthis, LLMscantranslatenaturallanguagetask
ure2asarunningexampletoexplaintheTICapproach.
descriptionstoinputformatsrequiredbyanexternalplanner.
[Ranaetal.,2023]introducesSayPlan,anapproachtoLLM-
3.1 ASPSchemaforTaskPDDLs
based large-scale task planning for robotics using 3D scene
WefirstdefinetheASPschemaformodelingtheinformation
graph (3DSG) representations. SayPlan achieves scalability
intaskdescriptions(asintermediaterepresentations)aswell
by exploiting the hierarchical nature of 3DSGs and using a
classical path planner. The approach in [Lyu et al., 2023] as that required in target task PDDLs (as materialized rep-
resentations). In the following p/N stands for a predicate p
presentspreliminaryresultsofintegratingLLMswithPDDL
using the SayCan dataset [Ahn et al., 2022]. However, the witharity(i.e.,numberofarguments)N.
SayCandatasethasalimitedscope,asitcontainsonlythree object/2 predicate relates objects with their respective
predefined actions. The LLM+P approach presented in [Liu types. E.g., a fact object(cocktail1, cocktail)
etal.,2023]usesanLLMtotranslatenaturallanguageplan- meansthatcocktail1isanobjectoftypecocktail.
ning task description to task PDDL. However, as mentioned
cardinality/2 predicaterepresentsthecardinalityoftheset
in Section 1, the drop in accuracy seen in LLM+P approach of objects of a type. E.g., cardinality(shot, 5)
is due to an imprecise generation of logically dependent in- meansthattherearefiveshotglasses. Suchcardinalityinfor-
formationinthetaskPDDLbytheLLMs. mationisoftenpresentintaskdescriptionswhereasthetask
PDDL does not include the cardinality information but only
2.4 Two-StepGenerationofPlannerInput the set of objects that satisfy the cardinality constraint. The
To achieve greater consistency and coherence in sentence cardinalityrulesbelowgeneratenewobjectsofthegiventype
generation fortextual problems, [Nye et al., 2021] proposes when necessary to satisfy the cardinality constraints, where
make idisapredicateexternaltoanASPreasoner.
decomposingthegenerationprocessintotwoparts. Thisen-
tailshavinganLLMgeneratecandidatesentences,represent- {object(@make_id(X, T), T)} :-
ing system 1 thinking, and a logical pruning process imple- cardinality(T, N), X = 1..N.
:- #count {X: object(X, T)} = M,
mentedviaaseparatesymbolicmodule,representingsystem
cardinality(T, N), M != N.
2thinking.
The approach presented in [Yang et al., 2023] is similar For example, given cardinality(shot, 5),
to our approach as it also uses an LLM for translating a object(shot1, shot), object(shot2, shot),object(shot3, shot), and object(shot4, (shaker level shaker1 l0)becausetheknowledge
shot), the cardinality rules would generate the new fact thattheshakerlevelshaveanorderisdomainknowledgethat
object(shot5, shot). iscommonforallbarmanqueries.
An intermediate representation bridges the syntactic and
init/1 and goal/1 predicates differentiate initial state facts
semanticheterogeneitybetweenthetaskdescriptionandtar-
from goal state facts. All domain predicates defined in the
gettaskPDDL.Forexample,eachofthephrasesshotthree,
domain.pddl, the map/3 predicate (described below) and the
Shot 3, and third shot should be represented as shot3.
gridpredicates(describedbelow)mayoccurasargumentsof
init and goal. A task description describes the init and Sometimes a given domain.pddl does not have semantically
meaningful object types and predicate names. For exam-
thegoalstatesasasetoffactsthataretrueintherespective
ple, the blocksworld domain does not define the object type
states. For example, an initial state sentence shot3 is on the
table is represented as init(ontable(shot3)), and a
blockbutrequirestheblockstobeinstancesofthegeneric
object type object. In such cases, an intermediate repre-
goalstatesentenceshot3shouldbeonthetableisrepresented
asgoal(ontable(shot3)). sentationusestheobjecttypesandpredicatesthatarecloser
tonaturallanguagethantothoseofthetargetPDDL.
map/3 predicate is used as a shorthand for a bijective Some sentences in the task description are more natu-
relationship between two objects types (of the same cardi- rally represented intensionally as a rule rather than exten-
nality). For example, map(dispenser, dispenses, sionally by enumerating all implied facts. For example, the
ingredient) stands for dispenses(dispenser1, phrase3dispensersfor3ingredients. ismoredirectlyrepre-
ingredient1), ..., dispenses(dispenserN, sented as the shortcut map(dispenser, dispenses,
ingredientN), where N is the cardinality of type ingredient) rather than explicitly enumerating all the
dispenser as well as of type ingredient. The rules facts. In another example, the initial state sentence The
shakerandshotglassesarecleanisrepresentedwiththefol-
below expand a map shorthand to corresponding facts of
lowingrulesratherthanwithallimpliedfacts.
the initial state (analogous rules for the goal state) where
make factisapredicateexternaltoanASPreasoner. init(clean(X)) :- object(X, shaker).
init(clean(X)) :- object(X, shot).
1 {init(@make_fact(X,P,Y)) : object(X, T1)} 1
:- init(map(T1,P,T2)), object(Y, T2). Asaresult,whenanLLMgeneratesanintermediaterepre-
1 {init(@make_fact(X,P,Y)) : object(Y, T2)} 1 sentationofataskdescription,theLLMoutputismoreoften
:- init(map(T1,P,T2)), object(X, T1). correct. The LLM output is also shorter and doesn’t grow
Two-dimensional Grid Some planning domains require if the query contains a lot of derived information. In addi-
capturing the positions of certain objects on a two- tion, we require shorter in-context examples which help in
dimensional grid. For example, in the floortile domain, a keeping the prompt size small. In Section 4 we present two
tile tile x y means that the tile is on row x and column approachesfortranslatingnaturallanguageplanningtaskde-
y. We represent such information with a predicate grid/3 scriptionstointermediaterepresentations.
wheregridisaplaceholderfortheappropriategridname.For
3.3 InferMaterializedRepresentation
example, floortile grid(x, y, tile x y) repre-
sentsthepositionoffloortiletileobjecttile x y. Anintermediaterepresentationcontainsonlytheinformation
presentinataskdescription. Wecomputetheremainingin-
3.2 IntermediateRepresentation formationrequiredinthetaskPDDLdeterministicallyusing
ThecoreideaoftheTICapproachistheintroductionofthe anASPsolver. Amaterializedrepresentationistheunionof
intermediate representation. An intermediate representation the facts in the intermediate representation and the inferred
of a query is a logically interpretable representation of the facts. Inthissection,wepresentdifferentkindsofASPrules
query. It serves multiple purposes (described below) to ad- that may be relevant for inferring information in a planning
dress the accuracy bottleneck of LLMs when generating the domain. Note that not all kind of rules may be required for
taskPDDLdirectly. everyplanningdomain.
Unlikethein-contextexamplesofdirectPDDLgeneration 1. Rules to infer the type of objects that need to be
approaches, an intermediate representation does not contain declared in the task PDDL. For object types that are ar-
guments of a predicate, we can automatically generate the
informationthatisnotpresentinthetaskdescriptionbutre-
rules for deriving the object types from the predicate def-
quiredinthetaskPDDL.Insomecases,themissinginforma-
initions in domain.pddl. For example, from the predicate
tioncanbeinferredfromtheinformationinintermediaterep-
definition (cocktail part1 ?c - cocktail ?i -
resentation. Forexample,ifabarmantaskdescriptionstates ingredient)wecangeneratetherules
that there are 5 shots and 3 dispensers as well as mentions
shot1, shot2, shot3, and shot4, then the intermedi- object(X, cocktail):- cocktail_part1(X, _).
object(X, ingredient):- cocktail_part1(_, X).
ate representation should not contain inferable information
shot5,dispenser1,dispenser2,anddispenser3. In cases where there is no predicate to infer a certain
Inothercases,themissinginformationcanbeaddedwiththe object type, one has to add the corresponding rules manu-
helpofrulesderivedfromthedomainknowledge. Forexam- ally. This case doesn’t occur in the barman domain. How-
ple, a barman task description may not contain information ever, in the blocksworld domain, the task PDDL requires
correspondingtothePDDLfacts(next l0 l1),(next the blocks to be of type object, while they are of type
l1 l2), (shaker empty level shaker1 l0), and block in the intermediate representation. Therefore, inthe blocksworld domain, we require a rule object(X, The first approach uses an in-context example for each do-
object) :- object(X, block). to assignthe type mainwhereasthesecondapproachusesagenericpromptthat
objecttoeachblock. worksforalldomains.
The following two kinds of rules can be authored semi-
4.1 UsingIn-ContextDomainExamples
automatically based on domain knowledge that may already
bepresentinthedomain.pddlorsomedocumentation,orwith LLMs are known for their proficiency in learning in context
thehelpofdomainexperts. withouttheneedforfine-tuningtheirparameters. In-context
2. Rules to infer information derivable from other infor- learninginLLMsistheirabilitytohandleunseentasksusing
mation present in the query. For example, for the floor- only a few demonstrations represented as input-label pairs
tiledomain,theruleinit(clear(T)) :- object(T, [Brown et al., 2020]. In the in-context example based ap-
tile), not init(robot at( , T))infersthatatile proach,weuseanLLMwithadomainspecificin-contextex-
isclearifthereisnorobotatthattile. ample to generate the intermediate representation of a given
3. Rules to infer information that is not derivable from task description. The LLM prompt consists of an example
informationpresentinthequery. Forexample,inthebarman taskdescription,itsintermediaterepresentation,andthenew
domain, the task PDDL requires the order of shaker levels task task description. We refer to this approach as TIC-IC.
knownthroughdomainknowledge. Thefollowingrules (See Appendix A for the complete LLM prompt). Figure 4
init(@make_seq(N-1, l, next, 1)) :- showstheLLMgeneratedintermediaterepresentationofthe
cardinality(level, N). exampletaskdescriptioninFigure2.
first_level(X) :- object(X, level),
not init(next(_, X)). Figure 4: Intermediate representation of the example task
generate the facts init(next(level0, descriptioninFigure2
level1)), init(next(level1, level2)), and cardinality(shaker, 1).
first level(level0) for N=3, where make seq cardinality(level, 3).
is a predicate external to ASP for generating a sequence. cardinality(shot, 5).
See Appendix C for complete listing of rules for all seven cardinality(dispenser, 3).
cardinality(ingredient, 3).
planningdomainsconsideredinthiswork.
init(clean(X)) :- object(X, shaker).
init(clean(X)) :- object(X, shot).
3.4 CompileTargetRepresentation
...
In the last step, we generate the task PDDL. The material- init(handempty(left)).
ized representation contains all the information necessary in init(handempty(right)).
a task PDDL. So, this step is mostly a straightforward syn- init(map(dispenser, dispenses, ingredient)).
init(cocktail_part1(cocktail1,ingredient2)).
tactic transformation of facts from the ASP Logic Program
...
syntaxtothePDDLsyntax. Inprinciple,thisstepcanalsobe
init(cocktail_part2(cocktail4,ingredient2)).
performedusinganLLM.But,sincethetranslationprocessis
goal(contains(shot1, cocktail1)).
deterministic and simple, it is less likely to encounter errors
...
when performed through code. The code based transforma- goal(contains(shot4, cocktail2)).
tionconsistsofthreemainsteps:
1. Forallbindings(obj1, t), ..., (objn, t)in
4.2 UsingGenericPrompts
theanswerofthequeryobject(X, T),ifobjecttype
tisdefinedinthedomain.pddl,generateaPDDLstring Theapproachbasedonin-contextexamplesachieveshighac-
”obj1 ... objn - t”. Theconcatenationofsuch curacy but requires an in-context example for each domain.
stringsmakestheobjectssectionofthetaskPDDL. This can be a bottleneck in use cases that need to support
many, possibly previously unknown, types of user queries.
2. For all bindings p(obj1, ..., objn) of X in the
Inthissection,wepresentadomainindependentgeneralized
answertothequeryinit(X),ifthepredicatepisde-
promptthataddressesthislimitation. Theapproachconsists
fined in the domain.pddl, then generate a PDDL string
ofthethreesteps: 1.Cardinalityextraction2.Namedobjects
”(p obj1 ... objn)”. Theconcatenationofsuch
extraction3.Rulesextraction.
stringsmakesuptheinitsectionofthetaskPDDL.
Cardinality Extraction: This step extracts the explicitly
3. Similar to the previous step, generate the PDDL string
mentioned cardinalities of object types in the given task de-
forallbindingsofXforthequerygoal(X).Thestring
scription. The LLM prompt for for this step consists of in-
representingtheconjunctionofallsuchstringsmakesup
structionsfor1.splittingthetaskdescriptionintoinitialstate
thegoalsectionofthetaskPDDL.
and goal state, and 2. extracting for each object type T the
countofT thatisexplicitlymentionedintheinitialstatede-
4 TranslatingNLTaskDescriptionto scription(seeAppendixBforcompleteprompt). Thesetof
IntermediateRepresentationusingLLM providedobjecttypesincludesanameandashortdescription
foreachdomainobjecttype.
In this section, we present two approaches for generating FortheexampletaskdescriptioninFigure2,theLLMgen-
the intermediate representation of a given task description. eratesthefollowingoutput:{shaker: 1, level: 3, shot: 5, dispenser: 3, The steps for extracting cardinalities, named objects and
hand: 2, ingredient: 3} rules can be performed with three separate LLM calls (re-
ferred to as TIC-G3), or together in one LLM call (referred
Namedobjectsextraction: Thisstepextractstheexplicitly
toasTIC-G1). Whileextractednamedobjectsareprogram-
mentioned named objects for each object type in the given
maticallypassedontothenextpromptforruleextractionin
task description. The LLM prompt for this step consists of
TIC-G3, inTIC-G1, theLLMisinstructedtoextractnamed
instructionsfordetectingandformattingobjectinstances(see
objectsandusethemforruleextraction.RefertoAppendixB
AppendixBforcompleteprompt). Thesetofprovidedob-
forcompleteLLMpromptsofbothimplementationoptions.
jecttypesisthesameasthatinthepreviousstep.
FortheexampletaskdescriptioninFigure2,theLLMgen-
eratesthefollowingoutput: 5 Experiments
{shaker: [], level: [], shot: [shot1, shot2, Throughexperimentsweaddressthefollowingquestions. 1.
shot3, shot4], dispenser: [], hand: [left, Does introducing an intermediate representation and a logi-
right], cocktail: [cocktail1, cocktail2, cal reasoner improve the accuracy of PDDL generation? 2.
cocktail3, cocktail4], ingredient: Does the TIC approach generalize to language variations in
[ingredient1, ingredient2, ingredient3]} thenaturallanguagetaskdescription? 3. HowdoestheLLM
Rules extraction: In this step the LLM generates a Logic performwithgenerictranslationinstructionsforintermediate
Program representation of the given task description. The representation?
LLM prompt consists of steps to detect the initial and goal
5.1 Experimentsetup
states, translate each state description per state translation
rules,andoutputtheresultassyntacticallycorrectASPLogic We evaluate our TIC approach on seven planning domains
Program. TheLLMisinstructedtouseboththeprovidedset
thathavebeentakendirectlyfromLLM+P[Liuetal.,2023]
of extracted objects as well as the set of domain predicates. andarestandardplanningbenchmarks. Eachofthedomains
The latter consists of the predicate’s name, arity, argument include a domain description PDDL and 20 automatically
types, andabriefdescription. (seeAppendixBforexample
generatedplanningtasksfrom[Seippetal.,2022],witheach
objecttypesandpredicates). Thefollowingdescribethestate taskconsistingofanaturallanguagedescriptionandacorre-
translationruleswhere<state>isaplaceholderforinit sponding ground truth PDDL. For evaluating the in-context
orgoal. example based approach, we also include an example task
description and a corresponding intermediate representation
1. a property of all instances of an object type.
thatservesasthein-contextexampleforeachdomain. Inad-
E.g., All apples are clean should be translated
dition,wealsocreatedanewdatasetderivedfromtheLLM+P
as <state>(clean(X)) :- object(X,
datasetbyintroducinglanguagevariationineachofthenatu-
apple).
rallanguagetaskdescriptionsforthesevendomains.Weused
2. apropertyofanamedobject. E.g.,apple1isredshould GPT-3.5toparaphrasetheoriginaltaskdescriptionsandcall
betranslatedas<state>(red(apple1)). thistheLanguageVariationdataset. WeevaluatetheTICap-
proachontheLanguageVariationdatasetwithoutmodifying
3. a relationship between two or more named objects.
the in-context learning examples or the generic prompt. We
E.g., apple1 is on plate2 should be translated as
useClingo[Gebseretal.,2019]pythonpackage1astheASP
<state>(on(apple1, plate2)).
solver. For each of the tasks, we generate the task PDDL
4. a relationship between two or more unnamed ob- and compare it with the ground truth PDDL using an auto-
jects. E.g., cups’ handles are free should be translated mated process. Note that the task PDDL comparison algo-
as<state>(free(X, Y)) :- cup handle(X, rithm does not count the number of mismatches but deter-
Y). mines whether two PDDLs are equivalent or not (see Ap-
pendix D for details). We solely present the PDDL genera-
5. arelationshipbetweenallunnamedobjectsofanobject
tionaccuracysinceplangenerationusingaclassicalplanner
type with a named object. E.g., apples are on table1
isdeterministicandpreciseprovidedthatthePDDLisaccu-
shouldbetranslatedas<state>(on(X, table1))
rate for the given dataset. Below are details of the different
:- object(X, apple).
experiments conducted. We run the experiments on GPT-4,
6. a bijective relationship between instances of two ob- GPT-3.5TurboandPaLM2(chat-bison)models.
ject types of the same cardinality. E.g., 3 balls for 3
• LLM+P : Evaluating the LLM+P approach of directly
kids should be translated as <state>(map(ball,
generatingtaskPDDLusinganLLM,asabaseline
play, kid)).
• TIC-IC:EvaluatingourTICIn-Contextexamplebased
7. a 2 dimensional grid of cells with coordinates. E.g.
approach
cell 1 1,cell 1 2,cell 2 1,cell 2 2shouldbetranslated
• TIC-G3: EvaluatingourTICGenericPromptbasedap-
as <state>(cell grid(1, 1, cell 1 1)),
proachwiththreeLLMcalls
...,<state>(cell grid(2, 2, cell 2 2)).
• TIC-G1: EvaluatingourTICGenericPromptbasedap-
For the example task description in Figure 2, the LLM gen-
proachwithoneLLMcall
eratesthesameintermediaterepresentationasinFigure4ex-
ceptthecardinalityinformation. 1https://pypi.org/project/clingo/GPT-4 GPT3.5 PaLM2
Domain LLM+P TIC-IC TIC-G3 TIC-G1 LLM+P TIC-IC TIC-G3 TIC-G1 LLM+P TIC-IC TIC-G3 TIC-G1
Barman 100 100 100 98.33 98.33 100 0 0 75 100 100 80
Blocksworld 88.33 100 100 100 71.67 100 91.67 93.33 10 100 100 100
Floortile 0 100 100 100 26.67 100 100 68.33 0 90 90 60
Grippers 95 100 100 98.33 95 100 40 36.67 95 100 95 95
Storage 40 100 100 98.33 25 100 0 0 0 100 60 6.67
Termes 53.33 100 98.33 98.33 71.67 100 15 63.33 0 100 93.33 0
Tyreworld 71.67 100 95 95 55 100 8.33 0 35 100 0 0
Table1: %AccuracyofPDDLgenerationonLLM+Pdatasetaveragedover3runs(variance≤1)
GPT-4 GPT3.5 PaLM2
Domain LLM+P TIC-IC TIC-G3 TIC-G1 LLM+P TIC-IC TIC-G3 TIC-G1 LLM+P TIC-IC TIC-G3 TIC-G1
Barman 100 100 100 96.67 53.33 100 0 0 0 100 66.67 66.67
Blocksworld 100 100 100 100 51.67 95 55 63.33 0 100 95 95
Floortile 0 100 100 100 0 100 83.33 48.33 0 90 90 45
Grippers 0 100 96.67 96.67 0 90 15 35 0 100 60 60
Storage 0 100 93.33 68.33 0 80 0 0 0 100 50 0
Termes 0 100 100 96.67 5 100 0 21.67 0 100 65 0
Tyreworld 0 100 93.33 88.33 0 90 0 0 0 95 0 0
Table2: %AccuracyofPDDLgenerationonLanguageVariationdatasetaveragedover3runs(variance≤1)
5.2 Resultsandanalysis 6 Conclusion
In this paper we studied the problem of generating plans
Tables1and2presentourexperimentresults.TIC-ICoutper-
from natural language planning task requests. We presented
formsLLM+PapproachonboththeLLM+Pdatasetandthe
theTranslate-Infer-Compile(TIC)approachtogeneratetask
LanguageVariationdatasetinallsevendomains. Inparticu-
PDDLs automatically from natural language planning task
lar,TIC-ICachieves≈100%accuracyontheLLM+Pdataset
descriptions with high accuracy. Thus, TIC addresses the
evenincomplexdomainssuchasFloortile,StorageandTer-
limitationsofLLMsingeneratingaccurateplansinthepres-
meswithGPT-3.5Turbo,GPT-4andPaLM2. Thishighlights
ence of a large number of actions by enabling use of exter-
TIC’s ability to generate highly accurate PDDLs using log-
nal planners. The core idea of TIC lies in the introduction
ically interpretable intermediate representations of the natu-
ofalogicallyinterpretableintermediaterepresentationofthe
rallanguagetaskdescriptions. Thereasonforslightlylower
task description. TIC combines the strengths of LLMs and
accuracy of TIC-IC in Floortile with PaLM2 is because the
symbolic reasoning to address LLMs’ limitation in directly
chat-bison model truncates the output of 2 out of 20 prob-
generatingtaskPDDLsfromtaskdescriptions. TICnotonly
lems. WealsoobservethatTIC-ICresultsinahighaccuracy
outperformssimilarapproacheswithrespecttotheaccuracy
ontheLanguageVariationdatasetwithGPT-4andPaLM2.
oftaskPDDLgenerationbutisalsomoregeneral.
In general, the lower accuracy on the Language Variation Although, in this work, we have focused on generating
dataset compared to the LLM+P dataset is probably due to planningtaskPDDLs,itisworthnotingthattheTICapproach
more complex and longer sentences in the task descriptions isgenericenoughtobeapplicableforusecasesthatneedto
oftheformerdataset. SeeAppendixEforexamples. supportanaturallanguageinterfaceontopofstructuredtasks
Aswemoveontothegenericpromptbasedapproach,TIC- suchasmathematicalproblems(seealsoTool-integratedRea-
G3generalizeswelloverallonGPT-4. Itachieves≥95%ac- soningAgents(ToRA)[Gouetal.,2023]),APIcalls(seealso
curacyonallsevendomains(with100%onfivedomains)of ToolFormer[Schicketal.,2023],LangChain[Chase,2022],
theLLM+Pdatasetand≥93.33%onallsevendomains(with and Gorilla [Patil et al., 2023]), database queries (see also
100%onfourdomains)oftheLanguageVariationdataset. In Spiderdataset[Yuetal.,2018]andthereportonLLM’sac-
general, theloweraccuracyofTIC-G1ascomparedtoTIC- curacyonSQLdatabases[Sequedaetal.,2023])etc.
G3maybeduetothemorecomplexinstructionstoLLMfor While the ability to do logical reasoning helps us achieve
extractingcardinalities,objectsandrulesinasinglestep. We highaccuracy,itcomeswithapriceofmodelingthedomain.
also observe a drop in accuracy of TIC-G3 and TIC-G1 for However, since the domain modeling is an offline task, we
some domains on PaLM2 and GPT-3.5 models when com- believe that it can be accelerated with semi-automatic tech-
paredtoGPT-4. ThismaybeduetothefailureofPaLM2and niques,possiblyusingLLMsamongothers,e.g.,[Ishayetal.,
GPT-3.5inabidingbythegenericinstructionsinsomecases. 2023]. Inthiscontextofknowledgeengineering,webelieve
SomeofthecommonerrorsbytheseLLMsareinextracting thatDescriptionLogics[Baaderetal.,2003]asaformalism
bijectivemaprelationshipsandintensionalrulesaswellasin and the standard Web Ontology Language (OWL) [OWL-
generatingsyntacticallycorrectASPrules. WG,2012]maylendthemselvesasalternativestoASP.References LecturesonArtificialIntelligenceandMachineLearning.
Morgan&ClaypoolPublishers,2019.
[Ahnetal.,2022] Michael Ahn, Anthony Brohan, Noah
Brown, Yevgen Chebotar, ..., and Andy Zeng. Do as i [Helmert,2006a] M. Helmert. The fast downward plan-
can, not as i say: Grounding language in robotic affor- ning system. Journal of Artificial Intelligence Research,
dances,2022. 26:191–246,July2006.
[Alkhazrajietal.,2020] YusraAlkhazraji,MatthiasFrorath, [Helmert,2006b] Malte Helmert. The fast downward plan-
Markus Gru¨tzner, Malte Helmert, Thomas Liebetraut, ningsystem. J.Artif.Intell.Res.,26:191–246,2006.
Robert Mattmu¨ller, Manuela Ortlieb, Jendrik Seipp, [Huangetal.,2022] Wenlong Huang, Fei Xia, Ted Xiao,
Tobias Springenberg, Philip Stahl, and Jan Wu¨lfing. Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,
Pyperplan. https://doi.org/10.5281/zenodo. Jonathan Tompson, Igor Mordatch, Yevgen Chebotar,
3700819,2020. PierreSermanet,NoahBrown,TomasJackson,LindaLuu,
Sergey Levine, Karol Hausman, and Brian Ichter. Inner
[Baaderetal.,2003] Franz Baader, Diego Calvanese, Deb-
monologue: Embodied reasoning through planning with
orah L. McGuinness, Daniele Nardi, and Peter F. Patel-
languagemodels,2022.
Schneider, editors. The Description Logic Handbook:
Theory, Implementation, and Applications. Cambridge [Ishayetal.,2023] Adam Ishay, Zhun Yang, and Joohyung
UniversityPress,2003. Lee. Leveraging large language models to generate an-
swer set programs. In Pierre Marquis, Tran Cao Son,
[Brownetal.,2020] TomBrown,BenjaminMann,NickRy-
and Gabriele Kern-Isberner, editors, Proceedings of the
der,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
20thInternationalConferenceonPrinciplesofKnowledge
..., and Dario Amodei. Language models are few-shot
RepresentationandReasoning,KR2023,Rhodes,Greece,
learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.
September2-8,2023,pages374–383,2023.
Balcan,andH.Lin,editors,AdvancesinNeuralInforma-
tion Processing Systems, volume 33, pages 1877–1901. [Lifschitz,2008] VladimirLifschitz.Whatisanswersetpro-
CurranAssociates,Inc.,2020. gramming? In Dieter Fox and Carla P. Gomes, editors,
ProceedingsoftheTwenty-ThirdAAAIConferenceonAr-
[Chase,2022] Harrison Chase. Langchain. https://
tificial Intelligence, AAAI 2008, Chicago, Illinois, USA,
github.com/langchain-ai/langchain,2022.
July13-17,2008,pages1594–1597.AAAIPress,2008.
[Gebseretal.,2019] MartinGebser,RolandKaminski,Ben- [Liuetal.,2023] Bo Liu, Yuqian Jiang, Xiaohan Zhang,
jamin Kaufmann, and Torsten Schaub. Multi-shot ASP
QiangLiu,ShiqiZhang,JoydeepBiswas,andPeterStone.
solving with clingo. Theory Pract. Log. Program.,
Llm+p: Empoweringlargelanguagemodelswithoptimal
19(1):27–82,2019.
planningproficiency,2023.
[GelfondandLifschitz,1988] Michael Gelfond and [Lyuetal.,2023] Qing Lyu, Shreya Havaldar, Adam Stein,
VladimirLifschitz. Thestablemodelsemanticsforlogic Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki,
programming. In Robert A. Kowalski and Kenneth A. and Chris Callison-Burch. Faithful chain-of-thought rea-
Bowen, editors, Logic Programming, Proceedings of the soning,2023.
Fifth International Conference and Symposium, Seattle,
[Nyeetal.,2021] Maxwell Nye, Michael Henry Tessler,
Washington,USA,August15-19,1988(2Volumes),pages
JoshuaB.Tenenbaum, andBrendenM.Lake. Improving
1070–1080.MITPress,1988.
coherenceandconsistencyinneuralsequencemodelswith
[Ghallabetal.,1998] M. Ghallab, A. Howe, C. Knoblock, dual-system,neuro-symbolicreasoning,2021.
D. Mcdermott, A. Ram, M. Veloso, D. Weld, and
[OpenAI,2023] OpenAI. Gpt-4 technical report. arXiv:
D. Wilkins. PDDL—The Planning Domain Definition
2303.08774,2023.
Language,1998.
[OWL-WG,2012] W3C OWL-WG. Owl 2 web ontology
[Ghallabetal.,2004] Malik Ghallab, Dana S. Nau, and language document overview (second edition). https:
Paolo Traverso. Automated planning - theory and prac- //www.w3.org/TR/owl2-overview/,2012.
tice. Elsevier,2004.
[Pallaganietal.,2022] Vishal Pallagani, Bharath Mup-
[Gouetal.,2023] ZhibinGou,ZhihongShao,YeyunGong, pasani, Keerthiram Murugesan, Francesca Rossi, Lior
Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Horesh, Biplav Srivastava, Francesco Fabiano, and An-
WeizhuChen. Tora: Atool-integratedreasoningagentfor drea Loreggia. Plansformer: Generating symbolic plans
mathematicalproblemsolving,2023. usingtransformers,2022.
[Guanetal.,2023] Lin Guan, Karthik Valmeekam, Sarath [Patiletal.,2023] Shishir G. Patil, Tianjun Zhang, Xin
Sreedharan,andSubbaraoKambhampati. Leveragingpre- Wang, and Joseph E. Gonzalez. Gorilla: Large language
trained large language models to construct and utilize modelconnectedwithmassiveapis,2023.
worldmodelsformodel-basedtaskplanning,2023.
[Ranaetal.,2023] Krishan Rana, Jesse Haviland, Sourav
[Haslumetal.,2019] Patrik Haslum, Nir Lipovetzky, Garg,JadAbou-Chakra,IanReid,andNikoSuenderhauf.
DanieleMagazzeni,andChristianMuise. AnIntroduction Sayplan:Groundinglargelanguagemodelsusing3dscene
to the Planning Domain Definition Language. Synthesis graphsforscalablerobottaskplanning,2023.[Schicketal.,2023] Timo Schick, Jane Dwivedi-Yu, refined large language model for long-horizon sequential
Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke taskplanning,2023.
Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
Toolformer: Language models can teach themselves to
usetools,2023.
[Seippetal.,2022] JendrikSeipp,A´lvaroTorralba,andJo¨rg
Hoffmann. Pddlgenerators. https://doi.org/10.
5281/zenodo.6382173,March2022.
[Sequedaetal.,2023] Juan Sequeda, Dean Allemang, and
Bryon Jacob. A benchmark to understand the role of
knowledgegraphsonlargelanguagemodel’saccuracyfor
questionansweringonenterprisesqldatabases,2023.
[Silveretal.,2022] Tom Silver, Varun Hariprasad, Reece S
Shuttleworth,NishanthKumar,Toma´sLozano-Pe´rez,and
Leslie Pack Kaelbling. PDDL planning with pretrained
largelanguagemodels.InNeurIPS2022FoundationMod-
elsforDecisionMakingWorkshop,2022.
[Sohrabi,2019] Shirin Sohrabi. AI planning for enterprise:
Putting theory into practice. In Sarit Kraus, editor, Pro-
ceedingsoftheTwenty-EighthInternationalJointConfer-
enceonArtificialIntelligence,IJCAI2019,Macao,China,
August10-16,2019,pages6408–6410.ijcai.org,2019.
[Valmeekametal.,2023a] Karthik Valmeekam, Matthew
Marquez,SarathSreedharan,andSubbaraoKambhampati.
Ontheplanningabilitiesoflargelanguagemodels:Acrit-
icalinvestigation,2023.
[Valmeekametal.,2023b] Karthik Valmeekam, Alberto
Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
Large language models still can’t plan (a benchmark for
llmsonplanningandreasoningaboutchange),2023.
[Xieetal.,2023] Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin
Bai, Ze Gong, and Harold Soh. Translating natural lan-
guagetoplanninggoalswithlarge-languagemodels,2023.
[Yangetal.,2023] Zhun Yang, Adam Ishay, and Joohyung
Lee. Couplinglargelanguagemodelswithlogicprogram-
mingforrobustandgeneralreasoningfromtext. InAnna
Rogers,JordanL.Boyd-Graber,andNaoakiOkazaki,ed-
itors, Findings of the Association for Computational Lin-
guistics: ACL 2023, Toronto, Canada, July 9-14, 2023,
pages5186–5219.AssociationforComputationalLinguis-
tics,2023.
[Yuetal.,2018] Tao Yu, Rui Zhang, Kai Yang, Michihiro
Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene
Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and
DragomirR.Radev. Spider: Alarge-scalehuman-labeled
dataset for complex and cross-domain semantic parsing
andtext-to-sqltask. CoRR,abs/1809.08887,2018.
[Zengetal.,2022] AndyZeng,MariaAttarian,BrianIchter,
Krzysztof Choromanski, Adrian Wong, Stefan Welker,
Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas
Sindhwani,JohnnyLee,VincentVanhoucke,andPeteFlo-
rence. Socraticmodels: Composingzero-shotmultimodal
reasoningwithlanguage,2022.
[Zhouetal.,2023] Zhehua Zhou, Jiayang Song, Kunpeng
Yao, Zhan Shu, and Lei Ma. Isr-llm: Iterative self-A LLMPromptforGeneratingIntermediateRepresentationusingIn-ContextExample
LLMPromptfortheBarmandomainforgeneratingintermediaterepresentationusingdomainspecificin-contextexample
IwantyoutocreateASPLogicProgramrepresentationofaparagraph.
An example paragraph is: You have 1 shaker with 3 levels, 4 shot glasses, 3 dispensers for 3 ingredients. The shaker
andshotglassesareclean,empty,andonthetable. Yourleftandrighthandsareempty. Thefirstingredientofcocktail1is
ingredient3. The second ingredient of cocktail1 is ingredient1. The first ingredient of cocktail2 is ingredient1. The second
ingredientofcocktail2isingredient2. Yourgoalistomake3cocktails. shot1containscocktail1. shot2containscocktail2.
shot3containscocktail1.
TheASPLogicProgramrepresentationoftheexampleparagraphis:
cardinality(shaker, 1).
cardinality(level, 3).
cardinality(shot, 4).
cardinality(dispenser, 3).
cardinality(ingredient, 3).
cardinality(cocktail, 2).
init(clean(X)) :- object(X, shaker).
init(clean(X)) :- object(X, shot).
init(empty(X)) :- object(X, shaker).
init(empty(X)) :- object(X, shot).
init(ontable(X)) :- object(X, shaker).
init(ontable(X)) :- object(X, shot).
init(handempty(left)).
init(handempty(right)).
init(map(dispenser, dispenses, ingredient)).
init(cocktail_part1(cocktail1, ingredient3)).
init(cocktail_part2(cocktail1, ingredient1)).
init(cocktail_part1(cocktail2, ingredient1)).
init(cocktail_part2(cocktail2, ingredient2)).
goal(contains(shot1, cocktail1)).
goal(contains(shot2, cocktail2)).
goal(contains(shot3, cocktail1)).
Now I have a new paragraph: You have 1 shaker with 3 levels, 5 shot glasses, 3 dispensers for 3 ingredients. The shaker
andshotglassesareclean,empty,andonthetable. Yourleftandrighthandsareempty. Thefirstingredientofcocktail1is
ingredient2. The second ingredient of cocktail1 is ingredient1. The first ingredient of cocktail2 is ingredient1. The second
ingredientofcocktail2isingredient2. Thefirstingredientofcocktail3isingredient1. Thesecondingredientofcocktail3is
ingredient3. Thefirstingredientofcocktail4isingredient3. Thesecondingredientofcocktail4isingredient2. Yourgoalisto
make4cocktails. shot1containscocktail1. shot2containscocktail4. shot3containscocktail3. shot4containscocktail2.
ProvidemewiththeASPLogicProgramrepresentationofthenewparagraphdirectlywithoutfurtherexplanations.
B GeneratingIntermediateRepresentationusingGenericLLMPrompts
Text
Youhave2tables,5plates,4apples,and3ballsfor3kids. apple1isonplate 2. apple2isonplate 1. orange1isonplate 1.
orange3isonplate 2. ball1isinfirstdraweroffirsttable. ball3isinseconddraweroffirsttable. apple3isinfirstdrawerof
secondtable. apple2isinseconddrawerofsecondtable. Theapplesarered. Therearethreeoranges. Theplatesareclean.
Thekidsareinsidethehall. cell 0 1cell 0 2cell 1 1cell 1 2cell 2 1cell 2 2cell 3 1cell 3 2Thereare8cells,eachcell
hasarobot. Yourgoalistomovetheorangestoemptyplates. orange1shouldbeonplate 3,orange2shouldbeonplate 4,
orange3shouldbeonplate 5. Noorangeshouldbeunplated. Theplatesarenotclean. Eachkidshouldpaintaball. The
kidsareinsidethehall.Objecttypes
[
{’type’: ’apple’, ’description’: ’The set of apples.’},
{’type’: ’orange’, ’description’: ’The set of oranges.’},
{’type’: ’plate’, ’description’: ’The set of plates.’},
{’type’: ’table’, ’description’: ’The set of tables.’},
{’type’: ’ball’, ’description’: ’The set of balls.’},
{’type’: ’kid’, ’description’: ’The set of kids.’},
{’type’: ’room’, ’description’: ’The set of rooms.’},
{’type’: ’cell’, ’description’: ’The set of cells.’},
{’type’: ’robot’, ’description’: ’The set of robots.’}
]
Predicates
[
{’predicate’: ’on’, ’arity’: 2, ’argument types’: [’apple’, ’plate’], ’description’:
’on(a, b) means apple a is on plate b.’},
{’predicate’: ’red’, ’arity’: 1, ’argument types’: [’apple’], ’description’:
’red(a) means apple a is red.’},
{’predicate’: ’clean’, ’arity’: 1, ’argument types’: [’plate’], ’description’:
’clean(a) means plate a is clean.’},
{’predicate’: ’not_clean’, ’arity’: 1, ’argument types’: [’plate’], ’description’:
’not_clean(a) means plate a is not clean.’},
{’predicate’: ’empty’, ’arity’: 1, ’argument types’: [’plate’], ’description’:
’empty(a) means plate a is empty. A plate is empty if there is no apple on it.’},
{’predicate’: ’cell_grid’, ’arity’: 3, ’argument types’: [’row’, ’column’, ’cell’],
’description’: ’cell_grid(a, b, c) means cell a is on row b and column c.’},
{’predicate’: ’unplated’, ’arity’: 0, ’argument types’: [], ’description’:
’unplated means at least one orange is not on the plate.’},
{’predicate’: ’play’, ’arity’: 2, ’argument types’: [’ball’, ’kid’], ’description’:
’play is a bijective map between balls and kids. play(a, b)
means ball a is played with by kid b.’},
{’predicate’: ’cell_robot_map’, ’arity’: 2, ’argument types’: [’cell’, ’robot’],
’description’: ’cell_robot_map is a bijective map between cells and robots.
cell_robot_map(a, b) means cell a has robot b.’},
{’predicate’: ’in_1’, ’arity’: 2, ’argument types’: [’apple or ball’, ’table’],
’description’: ’in_1(a, b) means apple or ball a is in first drawer of table b.’},
{’predicate’: ’in_2’, ’arity’: 2, ’argument types’: [’apple or ball’, ’table’],
’description’: ’in_2(a, b) means apple or ball a is in second drawer of table b.’},
{’predicate’: ’inside’, ’arity’: 2, ’argument types’: [’kid’, ’room’],
’description’: ’inside(a, hall) means kid a is inside hall.’}
]LLMPromptforCardinalityExtraction
GivenalistofobjecttypesandtheirdescriptionsasJSON.Theprovidedtextdescribesaplanningtask. Theplanningtask
describes an init state and a goal state. I want you to extract information from the provided task description by following
belowsteps:
Step1. Detecttheinitstatedescriptionandthegoalstatedescription. Theinitstatedescriptionisthetextfromstarttobegin
ofgoaldescription. Thegoaldescriptiontypicallystartswith’Yourgoalis...’.
Step2. ExtractforeachobjecttypeTtheexplicitlymentionedcount,width,andheightofinstancesofTfromtheinitstate
description.
Objecttypes: [ASABOVE]
Text: [ASABOVE]
Answer:
{’table’: 2, ’apple’: 4, ’orange’: 3, ’plate’: 5, ’ball’: 3, ’kid’: 3, ’cell’: 8}
Objecttypes: 〈Taskdomainobjecttypes〉
Text: 〈Taskdescription〉
Answer:
LLMPromptfornamedobjectsextraction
GivenalistofobjecttypeswiththeirdescriptionsasJSON.Extracttheinstancesofeachobjecttypefromtheprovidedtext.
Followbelowstepsinordertodoso.
Step1. Instancesappearassingularnounsintext. Ignorepluralnouns.
Step 2. Consider only the instances that are explicitly mentioned in the provided text. Do not output instances that are not
mentionedinthetext.
Step3. Ifaninstanceidcontainsspacesorhyphens,replacethembyunderscores.
Step4. Ifaninstanceidbeginswithanuppercase,thenconvertittolowercase.
Objecttypes: [ASABOVE]
Text: [ASABOVE]
Answer:
{’apple’: [’apple1’, ’apple2’], ’orange’: [], ’plate’: [’plate_1’, ’plate_2’,
’plate_3’, ’plate_4’, ’plate_5’], ’table’: [’table1’, ’table2’], ’ball’: [’ball1’,
’ball3’], ’room’: [’hall’], ’kid’: [], ’robot’: [], ’cell’: [’cell_0_1’,
’cell_0_2’, ’cell_1_1’, ’cell_1_2’, ’cell_2_1’, ’cell_2_2’, ’cell_3_1’,
’cell_3_2’]}
Objecttypes: 〈Taskdomainobjecttypes〉
Text: 〈Taskdescription〉
Answer:LLMPromptforRulesExtraction
Given
(1)AJSONwithobjecttypesaskeysandlistofobjectsasvalues.
(2) A list of predicates. The predicates of arity one represent properties of objects. The predicates of arity two or more
representrelationshipsoftwoormoreobjects.
(3)Atextdescribingaplanningtask. Aplanningtaskdescriptionconsistsofinitialstateandgoalstatedescriptions.
Translatetheprovidedtextfollowingbelowsteps:
Step1. Extracttheinitialstateandgoalstatedescriptions.
Step 2. Translate each state description by following below rules as applicable. ’〈state〉’ is a placeholder for ’init’ or
’goal’ for sentences of initial state or goal state respectively. While doing so, use only the provided predicates and respect
theirarityandtypeofarguments.
Step3. OutputtheresultasavalidASPLogicProgramwithoutanycommentsoradditionaltext.
Statesentencetranslationrules:
Rule1. RepresentpropertiesofsetsofunnamedobjectsofthesametypeasASPLogicProgramrules. Forexample,translate
’applesareclean’as’〈state〉(clean(X)):-object(X,apple).’,where’apple’isanobjecttypeand’clean’isapredicateofarity
1andargumenttypeapple.
Rule 2. Represent properties of named objects as ASP Logic Program facts. For example, translate sentence ’apple1
isred’as’〈state〉(red(apple1)).’,where’apple1’isanobjectand’red’isapredicateofarity1andargumenttypeapple.
Rule 3. Represent a relationship all unnamed objects of an object type with a named object as a ASP Logic Pro-
gram rule. For example, translate ’apples are on the table’ as ’〈state〉(on(X, table)) :- object(X, apple).’ where ’apple’ is
anobjecttype,’table’isanobjectoftype’furniture’and’on’isapredicateofarity2andargumenttypes[’apple’,’furniture’].
Rule 4. Represent relationships between two or more named objects as ASP Logic Program facts. For example,
translate sentence ’apple1 is on plate2’ as ’〈state〉(on(apple1, plate2)).’, where ’apple1’ and ’plate2’ are objects of type
’apple’and’plate’resp,and’on’isapredicateofarity2andargumenttypes[’fruit,’plate’].
Rule 5. Represent relationships between two or more unnamed objects as ASP Logic Program rules. For example,
translatesentence’cupshandlesarefree’as’〈state〉(free(X,Y)):-cup handle(X,Y).’, where’cup handle’isapredicateof
arity2andargumenttypes[’cup’,’handle’]and’free’isapredicateofarity2andargumenttypes[’cup’,’handle’].
Rule 6. Represent a bijective map between two sets of unnamed objects as ASP Logic Program facts. For example,
translate sentence ’3 balls for 3 kids’ as ’〈state〉(map(ball, play, kid)).’, where ’ball’ and ’kid’ are object types, and ’play’
is a predicate of arity 2 with argument types [’ball’, ’kid’]. For example, translate sentence ’each cell has a ball’ as
’〈state〉(map(cell, cell ball map, ball)).’, where ’cell’ and ’ball’ are object types, and ’cell ball map’ is a bijective map
predicateofarity2withargumenttypes[’cell’,’ball’].
Rule 7. Represent objects that denote a cell on a two dimensional grid ’cell grid’ as ASP Logic Program facts. For
example, translate objects ’cell 2 3’ as ’〈state〉(cell grid(2, 3, cell 2 3)).’, where ’cell 2 3’ is an object and ’cell grid’ is a
providedtertiarypredicate.LLMPromptforRulesExtractionCtd.
Belowanexampleofobjects,predicates,textandtheanswer.
Objects:
{’apple’: [’apple1’, ’apple2’], ’orange’: [], ’plate’: [’plate_1’, ’plate_2’,
’plate_3’, ’plate_4’, ’plate_5’], ’table’: [’table1’, ’table2’],
’ball’: [’ball1’, ’ball3’], ’room’: [’hall’], ’kid’: [], ’robot’: [],
’cell’: [’cell_0_1’, ’cell_0_2’, ’cell_1_1’, ’cell_1_2’, ’cell_2_1’, ’cell_2_2’,
’cell_3_1’, ’cell_3_2’]}
Predicates: [ASABOVE]
Text: [ASABOVE]
Answer:
init(on(apple1, plate_2)).
init(on(apple2, plate_1)).
init(on(orange1, plate_1)).
init(on(orange3, plate_2)).
init(in_1(ball1, table1)).
init(in_2(ball3, table1)).
init(in_1(apple3, table2)).
init(in_2(apple2, table2)).
init(red(X)) :- object(X, apple).
init(map(ball, play, kid).
init(clean(X)) :- object(X, plate).
init(inside(X, hall)) :- object(X, kid).
init(cell_grid(0, 1, cell_0_1)).
init(cell_grid(0, 2, cell_0_2)).
init(cell_grid(1, 1, cell_1_1)).
init(cell_grid(1, 2, cell_1_2)).
init(cell_grid(2, 1, cell_2_1)).
init(cell_grid(2, 2, cell_2_2)).
init(cell_grid(3, 1, cell_3_1)).
init(cell_grid(3, 2, cell_3_2)).
init(map(cell, cell_robot_map, robot)).
goal(on(orange1, plate_3)).
goal(on(orange2, plate_4)).
goal(on(orange3, plate_5)).
goal(not_clean(X)) :- object(X, plate).
goal(map(kid, paint, ball)).
goal(inside(X, hall)) :- object(X, kid).
goal(not unplated)).
Objects: [ASEXTRACTEDINTHEOBJECTEXTRACTIONSTEP]
Predicates: 〈Predicatesofthetaskdomain〉
Text: 〈Taskdescription〉
Answer:GenericLLMPromptforgeneratingintermediaterepresentationinonestep
Given
(1)AJSONlistofobjecttypeswiththeirdescriptions.
(2) A list of predicates. The predicates of arity one represent properties of objects. The predicates of arity two or more
representrelationshipsoftwoormoreobjects.
(3)Atextdescribingaplanningtask. Aplanningtaskdescriptionconsistsofinitialstateandgoalstatedescriptions.
Translatetheprovidedtextfollowingbelowsteps.
Step1. Detecttheinitialstateandgoalstatedescriptions.
Step 2. Extract for each object type T the explicitly mentioned count, width, and height of instances of T from the
initstatedescription.
Step 3. Extract all instances of each object type from the provided text. Instances appear as singular nouns in the
text. Ignore the plural nouns. In order to do so, consider only the instances that are explicitly mentioned in the provided
text. Do not output instances that are not mentioned in the text. If an instance contains hyphens, replace the hyphens by
underscores. Ifaninstancebeginswithanuppercase, thenlowercaseit. Ifaninstancecontainsspaces, replacethespaces
byunderscores.
Step 4. Translate each state description by following below rules as applicable. ’〈state〉’ is a placeholder for ’init’ or
’goal’ for sentences of initial state or goal state respectively. While doing so, use only the provided predicates and respect
theirarityandtypeofarguments. AlsousetheobjecttypesandlistofobjectsextractedinStep3.
Step5. OutputtheresultasavalidASPProgramwithoutanycommentsoradditionaltext.
Statesentencetranslationrules: [SAMEASABOVE]
Objecttypes: [SAMEASABOVE]
Predicates: [SAMEASABOVE]
Text: [SAMEASABOVE]
Answer: [SAMEASABOVE]
Objectstypes: 〈Objecttypesofthetaskdomain〉
Predicates: 〈Predicatesofthetaskdomain〉
Answer:
C DomainKnowledgeasASPRules
C.1 Barman
init(@make_seq(N-1, l, next, 1)) :- cardinality(level, N).
first_level(X) :- object(X, level), not init(next(_, X)).
init(shaker_empty_level(X, Y)) :- object(X, shaker), first_level(Y).
init(shaker_level(X, Y)) :- object(X, shaker), first_level(Y).
object(X, hand) :- init(handempty(X)).
object(L, level) :- init(next(L, _)).
object(L, level) :- init(next(_, L)).
object(X, cocktail) :- init(cocktail_part1(X, _)).
object(X, cocktail) :- init(cocktail_part2(X, _)).
object(X, ingredient) :- init(cocktail_part1(_, X)).
object(X, ingredient) :- init(cocktail_part2(_, X)).object(X, shot) :- goal(contains(X, _)).
C.2 Blocksworld
object(B, block) :- init(on(B, _)).
object(B, block) :- init(on(_, B)).
object(B, block) :- init(on_table(B)).
object(B, block) :- init(clear(B)).
object(B, block) :- goal(on(B, _)).
object(B, block) :- goal(on(_, B)).
object(B, block) :- goal(on_table(B)).
object(B, block) :- goal(clear(B)).
object(B, object) :- object(B, block).
C.3 Floortile
floortile_grid(R,C,Z1) :- init(floortile_grid(R,C,Z1)).
init(up(Z1,Z2)) :- floortile_grid(R,C,Z1), floortile_grid(R-1,C,Z2).
init(down(Z2,Z1)) :- floortile_grid(R,C,Z1), floortile_grid(R-1,C,Z2).
init(right(Z1,Z2)) :- floortile_grid(R,C,Z1), floortile_grid(R,C-1,Z2).
init(left(Z2,Z1)) :- floortile_grid(R,C,Z1), floortile_grid(R,C-1,Z2).
init(robot_has(robot1, white)).
init(robot_has(robot2, black)).
object(Z, tile) :- floortile_grid(_,_,Z).
object(X, color) :- init(robot_has(_, X)).
object(X, robot) :- init(robot_has(X, _)).
object(X, color) :- init(available_color(X)).
object(X, robot) :- init(robot_at(X, _)).
object(X, tile) :- init(robot_at(_, X)).
object(X, color) :- goal(painted(_, X))
object(X, tile) :- goal(painted(X, _)).
init(clear(T)) :- object(T, tile), not init(robot_at(_, T)).
init(min_cost_metric("=(total-cost) 0)")).
init(available_color(X)) :- init(robot_has(_, X)).
init(available_color(X)) :- goal(painted(_, X)).
C.4 Grippers
object(X, robot) :- init(at_robby(X, _)).
object(X, room) :- init(at_robby(_, X)).
object(X, ball) :- init(at(X, _)).
object(X, room) :- init(at(_, X)).
object(X, ball) :- goal(at(X, _)).
object(X, room) :- goal(at(_, X)).
object(X, object) :- object(X, ball).
object(X, gripper):- init(free(_, X)).
object(X, robot):- init(free(X, _)).
object(X, gripper) :- object(X, left_gripper).
object(X, gripper) :- object(X, right_gripper).
robot_left_gripper_map(X,Y) :- init(robot_left_gripper_map(X,Y)).
robot_right_gripper_map(X,Y) :- init(robot_right_gripper_map(X,Y)).C.5 Storage
object(X, storearea) :- init(on(_, X)).
object(X, crate) :- init(on(X, _)).
object(X, hoist) :- init(at(X, _)).
object(X, storearea) :- object(X, depot_storearea).
depot_storearea_grid(R,C,Z) :- init(depot_storearea_grid(R,C,Z)).
adjacent(Z1,Z2) :- depot_storearea_grid(R,C,Z1), depot_storearea_grid(R-1,C,Z2).
adjacent(Z2,Z1) :- depot_storearea_grid(R,C,Z1), depot_storearea_grid(R-1,C,Z2).
adjacent(Z1,Z2) :- depot_storearea_grid(R,C,Z1), depot_storearea_grid(R,C-1,Z2).
adjacent(Z2,Z1) :- depot_storearea_grid(R,C,Z1), depot_storearea_grid(R,C-1,Z2).
init(connected(X, Y)) :- init(connected(Y, X)).
goal(connected(X, Y)) :- goal(connected(Y, X)).
C.6 Termes
init(@make_seq(N, n, succ, 0)) :- init(max_height(N)).
object(N, numb) :- init(succ(N, _)).
object(N, numb) :- init(succ(_, N)).
termes_pos_grid(R,C,Z) :- init(termes_pos_grid(R,C,Z)).
object(Z, position) :- termes_pos_grid(_,_,Z).
init(height(Z, n0)) :- termes_pos_grid(_,_,Z).
init(neighbor(Z1,Z2)) :- termes_pos_grid(R,C,Z1), termes_pos_grid(R-1,C,Z2).
init(neighbor(Z2,Z1)) :- termes_pos_grid(R,C,Z1), termes_pos_grid(R-1,C,Z2).
init(neighbor(Z1,Z2)) :- termes_pos_grid(R,C,Z1), termes_pos_grid(R,C-1,Z2).
init(neighbor(Z2,Z1)) :- termes_pos_grid(R,C,Z1), termes_pos_grid(R,C-1,Z2).
0 { goal(height(Z, n0)) } 1 :- termes_pos_grid(_,_,Z).
:- termes_pos_grid(_,_,Z), #count {X : goal(height(Z, X))} != 1.
C.7 Tyreworld
init(@make_map(T1, P, T2, N)) :- init(map(T1, P, T2)), cardinality(T1, N).
object(@gen_objects(N, 0, intact_tyre), intact_tyre) :- cardinality(intact_tyre, N).
object(@gen_objects(N, 0, flat_tyre), flat_tyre) :- cardinality(flat_tyre, N).
object(@gen_objects(N, 0, hub), hub) :- cardinality(hub, N).
object(@gen_objects(N, 0, nut), nut) :- cardinality(nut, N).
goal(@make_map(T1, P, T2, N)) :- goal(map(T1, P, T2)), cardinality(T1, N).
object(X, container) :- init(in(_, X)).
object(X, wheel) :- object(X, flat_tyre).
object(X, wheel) :- object(X, intact_tyre).
object(X, tool):- object(X, jack).
object(X, tool):- object(X, pump).
object(X, tool):- object(X, wrench).
init(intact(X)) :- object(X, intact_tyre).
object(X, container) :- init(in(_, X)).
object(X, hub) :- init(on(_, X)).
object(X, nut) :- init(tight(X,_)).
object(X, hub) :- init(tight(_,X)).
D PDDLComparision
Following algorithm illustrate the main steps of for comparing two PDDLs. We use this to compare the TIC generated task
PDDLwiththegroundtruthtaskPDDL.Algorithm1ComparePDDLs
1: taskPDDLTypes←getPDDLTypes(taskPDDL)
2: correctPDDLTypes←getPDDLTypes(correctPDDL)
3: ifset(taskPDDLTypes)̸=set(correctPDDLTypes)then
4: returnFalse
5: endif
6: taskPDDLObjects←getPDDLObjects(taskPDDL)
7: correctPDDLObjects←getPDDLObjects(correctPDDL)
8: iflen(taskPDDLObjects)̸=len(correctPDDLObjects)then
9: returnFalse
10: endif
11: taskInitStateAtoms←getInitStateAtoms(taskPDDL)
12: correctInitStateAtoms←getInitStateAtoms(correctPDDL)
13: iflen(taskInitStateAtoms)̸=len(correctInitStateAtoms)then
14: returnFalse
15: endif
16: taskGoaltStateAtoms←getGoalStateAtoms(taskPDDL)
17: correctGoalStateAtoms←getGoalStateAtoms(correctPDDL)
18: iflen(taskGoalStateAtoms)̸=len(correctGoalStateAtoms)then
19: returnFalse
20: endif
21: taskObjects←[]
22: correctObjects←[]
23: forallt∈sort(taskPDDLTypes)do
24: correctObjectsOfType←getObjectsOfType(correctPDDL,t)
25: typePerm←permutations(correctObjectsOfType)
26: appendtypePermtocorrectObjects
27: taskObjectsOfType←getObjectsOfType(taskPDDL,t)
28: appendtaskObjectsOfTypetotaskObjects
29: endfor
30: forallperm∈cartesianProduct(correctObjects)do //eachpermisalistcontainingonepermutationofobjectsforeach
objecttype
31: ifcheckStateEquivalence(taskObjects,perm,taskInitStateAtoms,correctInitStateAtoms)and
32: checkStateEquivalence(taskObjects,perm,taskGoaltStateAtoms,correctGoalStateAtomsthen
33: returnTrue
34: endif
35: endfor
36: returnFalse
checkStateEquivalenceoftwostatessandtistrueifstatescanbesimulatedbystatetandviceversa,otherwisefalse. A
statesissimulatedbyanotherstatetifeachatominscanbesimulatedbyanatominstatet. Thatis,thereexistsamapping
mfromobjectsofstatestoobjectsofstatetsuchthatforeveryatoma∈sthereexistsanatomb∈tsuchthatm(a)=b.
E ExamplesofLanguageVariation
BelowweprovideafewexamplesofLanguageVariationoftaskdescriptions. Pleaserefertosupplementarymaterialforthe
entireLanguageVariationdataset.
E.1
Barmantaskdescriptionfromoriginaldataset
Youhave1shakerwith3levels,4shotglasses,3dispensersfor3ingredients. Theshakerandshotglassesareclean,empty,
andonthetable. Yourleftandrighthandsareempty. Thefirstingredientofcocktail1isingredient2. Thesecondingredient
ofcocktail1isingredient1. Thefirstingredientofcocktail2isingredient2. Thesecondingredientofcocktail2isingredient3.
The first ingredient of cocktail3 is ingredient1. The second ingredient of cocktail3 is ingredient2. Your goal is to make 3
cocktails. shot1containscocktail1. shot2containscocktail3. shot3containscocktail2.BarmantaskdescriptionfromLanguageVariationdataset
Thereisashakerwith3levels,4shotglasses,and3dispensersfor3ingredients. Theshakerandtheshotglassesareonthe
table. The shaker and the shot glasses are clean and empty. Your left and right hands are empty. There are 3 cocktails to
bemade,eachwith2ingredients. Thefirstcocktailrequiresingredient2asthefirstingredientandingredient1asthesecond
ingredient. The second cocktail requires ingredient2 as the first ingredient and ingredient3 as the second ingredient. The
thirdcocktailrequiresingredient1asthefirstingredientandingredient2asthesecondingredient. Yourtaskistomakethe3
cocktailsandpourthemintoshotglasses. Thefirstcocktailshouldbepouredintoshotglass1,thesecondcocktailintoshot
glass3,andthethirdcocktailintoshotglass2.
E.2
Floortiletaskdescriptionfromoriginaldataset
Youhave5rowsand3columnsofunpaintedfloortiles.
tile-0-1tile-0-2tile-0-3
tile-1-1tile-1-2tile-1-3
tile-2-1tile-2-2tile-2-3
tile-3-1tile-3-2tile-3-3
tile-4-1tile-4-2tile-4-3
Youhave2robots.
Eachrobotcanpaintincolorwhiteorblack.
robot2isattile-1-1.
robot1isattile-2-3.
Yourgoalistopaintthegridinthefollowingpattern: tile-1-1iswhite; tile-1-2isblack; tile-1-3iswhite; tile-2-1isblack;
tile-2-2 is white; tile-2-3 is black; tile-3-1 is white; tile-3-2 is black; tile-3-3 is white; tile-4-1 is black; tile-4-2 is white;
tile-4-3isblack.
FloortiletaskdescriptionfromLanguageVariationdataset
Thereare15unpaintedfloortilesarrangedin5rowsand3columns.
tile-0-1tile-0-2tile-0-3
tile-1-1tile-1-2tile-1-3
tile-2-1tile-2-2tile-2-3
tile-3-1tile-3-2tile-3-3
tile-4-1tile-4-2tile-4-3
Tworobotsareavailabletopaintthetileseitherwhiteorblack.
Robot2iscurrentlypositionedattile-1-1whilerobot1isattile-2-3.
The objective is to paint the tiles in a specific pattern where tile-1-1 is white, tile-1-2 is black, tile-1-3 is white, tile-2-1 is
black,tile-2-2iswhite,tile-2-3isblack,tile-3-1iswhite,tile-3-2isblack,tile-3-3iswhite,tile-4-1isblack,tile-4-2iswhite,
andtile-4-3isblack.
E.3
Gripperstaskdescriptionfromoriginaldataset
Youcontrol3robots,eachrobothasaleftgripperandarightgripper.
Thereare4roomsand7balls.
robot3isinroom3.
robot2isinroom1.
robot1isinroom4.
ball1isinroom4. ball7isinroom3. ball3isinroom3. ball4isinroom2. ball6isinroom1. ball2isinroom3. ball5isin
room2.
Therobots’grippersarefree.
Yourgoalistotransporttheballstotheirdestinations.
ball1shouldbeinroom1.
ball2shouldbeinroom4.
ball3shouldbeinroom3.
ball4shouldbeinroom4.
ball5shouldbeinroom3.
ball6shouldbeinroom2.
ball7shouldbeinroom1.GripperstaskdescriptionfromLanguageVariationdataset
Thereare3robots. Eachrobothasaleftgripperandarightgripper.
Thereare4roomsand7balls.
Robot3isinroom3,robot2isinroom1,androbot1isinroom4.
Theballsaredistributedamongtheroomsasfollows: ball1isinroom4,ball7isinroom3,ball3isinroom3,ball4isin
room2,ball6isinroom1,ball2isinroom3,andball5areinroom2.
Therobots’grippersarecurrentlynotholdinganything.
The objective is to move the balls to their designated rooms: ball 1 to room 1, ball 2 to room 4, ball 3 to room 3, ball 4 to
room4,ball5toroom3,ball6toroom2,andball7toroom1.