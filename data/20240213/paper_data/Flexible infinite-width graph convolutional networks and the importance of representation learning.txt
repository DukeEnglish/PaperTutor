Flexible infinite-width graph convolutional networks and the importance of
representation learning
BenAnson1 EdwardMilsom1 LaurenceAitchison1
Abstract graphnetworks(Walker&Glocker,2019;Niuetal.,2023).
Acommontheoreticalapproachtounderstanding However,itisimportanttounderstandwhethertheseNNGP
neuralnetworksistotakeaninfinite-widthlimit, resultsapplyinpracticalneuralnetworksettings. Oneway
atwhichpointtheoutputsbecomeGaussianpro- ofassessingtheapplicabilityofNNGPresultsistolookat
cess(GP)distributed. Thisisknownasaneural performance. Specifically,ifinfinite-widthNNGPsperform
networkGaussianprocess(NNGP).However,the comparablywithfinite-widthNNs,wecanreasonablyclaim
NNGPkernelisfixed,andtunableonlythrough that infinite-width NNGPs capture the “essence” of what
asmallnumberofhyperparameters,eliminating makesfinite-widthNNsworksowell. Incontrast,iffinite-
any possibility of representation learning. This widthNNsperformbetter,thatwouldindicatethatNNGPs
contrastswithfinite-widthNNs,whichareoften aremissingsomething.
believedtoperformwellpreciselybecausethey
Why might NNGPs underperform relative to NNs? One
areabletolearnrepresentations. Thusinsimpli-
key property of the infinite-wdith NNGP limit is that it
fyingNNstomakethemtheoreticallytractable,
eliminates representation learning: the kernel of the GP
NNGPsmayeliminatepreciselywhatmakesthem
isfixed,andcannotbetunedexceptthroughaverysmall
work well (representation learning). This mo-
numberofhyperparameters. Thisfixedkernelmakesitvery
tivated us to understand whether representation
easy to analyse the behaviour of networks in the NNGP
learningisnecessaryinarangeofgraphclassi-
limit. However,thefixedkernelalsoindicatesaproblem. ficationtasks. Wedevelopaprecisetoolforthis
Inparticular,thetop-layerrepresentation/kernelishighly
task,thegraphconvolutionaldeepkernelmachine.
flexibleinfinitenetworks,andthisflexibilityiscriticalto
ThisisverysimilartoanNNGP,inthatitisanin-
theexcellentperformanceofdeepnetworks(Bengioetal.,
finitewidthlimitanduseskernels,butcomeswith
2013;LeCunetal.,2015). Thisindicatesthatinsomecases,
a“knob”tocontroltheamountofrepresentation
theinfinite-widthNNGPlimitcan“throwoutthebabywith
learning. Wefoundthatrepresentationlearningis
thebathwater”: intryingtosimplifythesystemtoenable
necessary(inthesensethatitgivesdramaticper-
theoreticalanalysis,theNNGPlimitcanthrowoutsomeof
formanceimprovements)ingraphclassification
themostimportantpropertiesofdeepnetworksthatleadto
tasksandheterophilousnodeclassificationtasks,
themperformingsowell.
butnotinhomophilousnodeclassificationtasks.
InthesettingofconvolutionalnetworksforCIFAR-10,this
is precisely what was found: namely, that infinite-width
1.Introduction
NNGPsunderperformfinite-widthNNs(Adlametal.,2023;
Garriga-Alonso et al., 2018; Shankar et al., 2020), and it
Afundamentaltheoreticalmethodforanalyzingneuralnet-
hasbeenhypothesisedthatthisdifferencearisesduetothe
worksinvolvestakinganinfinite-widthlimitofarandomly
lackofrepresentationlearninginNNGP(Aitchison,2020;
initializednetwork. Inthissetting,theoutputsconvergeto
MacKayetal.,1998).
a Gaussian process (GP), and this GP is known as a neu-
ralnetworkGaussianprocessorNNGP(Neal,1996;Lee In this paper, we consider the question of NNGP perfor-
et al., 2018; Matthews et al., 2018). NNGPs have been manceintheGraphConvolutionalNetwork(GCN)setting.
usedtostudyvariousdifferentneuralnetworkarchitectures, Surprisingly,priorworkhasshownthatthegraphconvolu-
rangingfromfully-connected(Leeetal.,2018)toconvolu- tionalNNGPperformscomparablytotheGCN(Niuetal.,
tional(Novaketal.,2018;Garriga-Alonsoetal.,2018)and 2023),perhapssuggestingthatrepresentationlearningisnot
importantingraphsettings.
1UniversityofBristol.Correspondenceto:LaurenceAitchison
<laurence.aitchison@gmail.com>. However,ourworkindicatesthatthispictureiswildlyin-
complete. Inparticular,weshowthatwhilethegraphcon-
1
4202
beF
9
]LM.tats[
1v52560.2042:viXraFlexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
volutionalNNGPiscompetitiveonsomedatasets,itisdra- – representationlearningmarkedlyimprovesperfor-
maticallyworsethanGCNsonotherdatasets. Thiswould manceinheterophilousnodeclassificationtasks
suggestahypothesis,thatrepresentationlearningingraph andgraphclassificationtasks
tasksisdatasetdependent,withsomedatasetsneedingrep-
– representation learning does not markedly im-
resentationlearning,andothersnot.
proveperformanceinhomophilousnodeclassifi-
Importantly though, testing this hypothesis rigorously is cationtasks.
difficult,astherearemanydifferencesbetweentheinfinite-
width,fixedrepresentationgraphconvolutionalNNGPand
2.RelatedWork
thefinite-width,flexiblerepresentationGCN,notjustthat
theGCNhasrepresentationlearning. Perhapsthebestap-
Graphconvolutionalnetworksareatypeofgraphneuralnet-
proachtotestingthehypothesiswouldbetodevelopavari-
work(Scarsellietal.,2008;Kipf&Welling,2017;Bronstein
antoftheNNGPwithrepresentationlearning,andtosee
etal.,2017;Velickovic´etal.,2017)originallyintroducedby
howperformancechangedaswealteredtheamountofrep-
Kipf&Welling(2017),motivatedbyalocalisedfirst-order
resentationlearningallowedbythisnewmodel. However,
approximationofspectralgraphconvolutions. Theyusethe
thisisnotpossiblewiththetraditionalNNGPframework.
adjacencymatrixto aggregateinformationfromanode’s
Instead,weneedtoturntorecentlydevelopedDeepkernel
neighbours(seeEq.13)ateachlayeroftheneuralnetwork.
machines(DKMs). DKMsarearecentfamilyofmethods
At the time, graph convolutional networks outperformed
thatstronglyresembleNNGPs(Yangetal.,2023;Milsom
existingapproachesforsemi-supervisednodeclassification
et al., 2023). In particular, DKMs, like NNGPs, are ob-
byasignificantmargin.
tainedviaaninfinite-widthlimit,andworkentirelyinterms
ofkernels. ThekeydifferenceisthatDKMsallowforrep- Infinite-widthneuralnetworkswerefirstconsideredinthe
resentation learning, while NNGPs do not. Specifically, 1990s(Neal,1996;Williams,1996)wheretheyonlyconsid-
DKMshaveatunableparameter,ν,whichcontrolsthede- eredshallownetworksatinitialisation. Cho&Saul(2009)
greeofrepresentationlearning. Forsmallvaluesofν,DKM derivedanequivalencebetweendeepReLUnetworksand
representationsarehighlyflexibleandlearnedfromdata. In arccoskernels. Thiswaslatergeneralisedtoarbitraryacti-
contrast,asν →∞,flexibilityiseliminated,andtheDKM vationfunctionsbyLeeetal.(2018),whousedaBayesian
becomesequivalenttotheNNGP.Thegraphconvolutional framework to show that any infinitely-wide deep neural
DKMthereforeformsaprecisetoolforstudyingtheneed network is equivalent to a Gaussian process, which they
forrepresentationlearningingraphtasks. dubbedtheneuralnetworkGaussianprocess(NNGP)(Lee
etal.,2020;2018;Matthewsetal.,2018). TheNNGPhas
Using the graph convolutional DKM, we examined the
sincebeenextended,forexampletoaccommodateconvolu-
needforrepresentationlearninginavarietyofgraph/node
tions(Garriga-Alonsoetal.,2018;Novaketal.,2018),graph
classification tasks. The graph classification tasks we ex-
structure(Niuetal.,2023;Huetal.,2020)andmore(Yang,
amined all seemed to require representation learning for
2019).
goodperformance. Fornodeclassificationtasks,wefound
that the situation was more complex. Specifically, node Using a similar infinite-width limit, but on SGD-trained
classificationtasksexistonaspectrumfromhomophilous, networksratherthanBayesiannetworks,Jacotetal.(2018)
whereadjacentnodesinagraphtendtobesimilar,tohet- studiedthedynamicsofneuralnetworktrainingundergradi-
erophilous,whereadjacentnodestendtobedissimilar. We entdescent,derivingtheneuraltangentkernel(NTK),which
found that homophilous tasks tended not to require rep- describestheevolutionoftheinfinite-widthnetworkover
resentation learning, while heterphilous tasks did require time. TheNTKsuffersfromananalogousproblemtothe
representationlearning. NNGP’slackofrepresentationlearning,inwhichthepartic-
ularNTKlimitchosenimpliesthattheintermediatefeatures
Concretely,ourcontributionsareasfollows:
do not evolve over time (Yang & Hu, 2021; Bordelon &
Pehlevan,2023;Vyasetal.,2023). Alternativelimits,such
• Wedevelopagraphconvolutionalvariantofdeepker- astherecentmu-Pparameterisation(Yang&Hu,2021)fix
nelmachines(Section4). thisproblembyalteringthescalingofparameters. However,
lineofworkonlytellsusthatfeature/representationlearning
• We develop a novel, scalable, interdomain inducing-
willhappen,notwhatthelearnedfeatures/representations
point approximation scheme for the graph convolu-
willbe. Thatisnecessarytostudyrepresentationlearning
tionalDKM(AppendixB).
in-depth,andisprovidedbytheDKMframework.
• Byconsideringtheperformanceofgraphconvolutional Applyingfixedkernelfunctionstographsisawellexplored
NNGPsrelativetoGCNsandbytuningν inthegraph endeavour (Shervashidze & Borgwardt, 2009; Kashima
convolutionalDKM,ourexperimentsshow: et al., 2003; Shervashidze et al., 2009). Kernels can be
2Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
defineddirectlyforgraphsandappliedinashallowfashion, 3.2.DGPsintermsofGrammatrices
shallowkernelcanbestacked(Achtenetal.,2023),orker-
Itturnsoutthattheacross-layerdependenciesinaDGPcan
nelscanbeusedasacomponentofalargerdeeplearning
besummarisedwithGrammatrices,withtraining-example
algorithm(Cosmoetal.,2021;Yanardag&Vishwanathan,
by training-example Gram matrices Gℓ ∈ RP P (which
2015). Ourworkdiffersfundamentallyinthatthekernel ×
thusresembleakernel),ratherthanfeaturesFℓ,where
functionitselfislearnedratherthanlearningfeaturesand
thenapplyingafixedkernel. Gℓ =G(Fℓ)= 1 Fℓ(Fℓ)T ∈RP P. (5)
Nℓ ×
3.Background Here,G(·)isafunctionthattakesfeaturesandcomputesthe
correspondingGrammatrix,whileGℓistheactualvalueof
Wegiveabackgroundoninfinite-widthNNGPs,followed theGrammatrixatlayerℓ.Thisseemsanoddthingtodo,as
byanoverviewofhowtomodifythisinfinite-widthlimit computingthekernelusuallyrequiresfeatures,Fℓ. Whilst
toobtainaDKM,whichissimilartotheNNGPbutretains itistruethatarbitrarykernelsrequirefeatures,itturnsout
representation learning. We also give an overview of the that in most cases of interest (e.g. arccos kernels, which
GCNandthegraphconvolutionalNNGP.Theseingredients arerelatedtoinfinite-widthReLUnetworks,andisotropic
allowustodefineaDKMinthegraphdomain,aso-called kernelslikethesquaredexponential),wecancomputethe
“graphconvolutionalDKM”,inSection4. kerneldirectlyfromtheGrammatrix,withoutneedingthe
underlyingfeatures:
3.1.NeuralNetworksasDeepGaussianProcesses
K (Fℓ)=K(G(Fℓ))=K(Gℓ). (6)
Considerafully-connectedNNwithfeaturesFℓ ∈RP ×Nℓ features
where P is the number of training datapoints, and N ℓ is Here, K features(·)∈RP ×Nℓ →RP ×P is a function
the width of layer ℓ. These features are computed from that computes the kernel from the features, while
thefeaturesatthepreviouslayerusingFℓ =ϕ(Fℓ −1)Wℓ, K(·)∈RP ×P →RP ×P is a function that computes the
whereWℓ ∈RNℓ−1×Nℓ aretheweightsandϕisthepoint- samekernelfromthecorrespondingGrammatrix(Eq. 5).
wise nonlinearity (e.g. ReLU). We put an IID Gaussian Wecanthereforerewritethemodelas,
prior Wℓ ∼ N(0, 1 ) on the weights. Marginalising
overtheij weights, thN eℓ d− i1 stributionoverthefeaturesatone (cid:89)Nℓ
layerconditionedonthefeaturesatthepreviousplayerbe-
P(Fℓ |Fℓ −1)= N(fℓ λ;0,K(G(Fℓ −1))), (7a)
comesGaussian,withacovariancekernelthatdependson λ=1
thepreviouslayerfeatures, ν (cid:89)L+1
P(Y |FL)= N(y ;0,K(G(FL))+σ2I), (7b)
λ
(cid:89)Nℓ λ=1
P(Fℓ |Fℓ −1)= N(fℓ λ;0,K features(Fℓ −1)), (1)
where all the across-layer dependencies go through the
λ=1
Grammatrices(Eq. 5).
ν (cid:89)L+1
P(Y |FL)= N(y ;0,K (FL)+σ2I),
λ features Summarising the across-layer dependencies in terms of
λ=1 Gram matrices allows us to understand flexibility in the
where fℓ is the value of the λth feature at layer ℓ for all DGPprior. Inparticular,noticethattheGrammatricescan
λ
trainingexamples,whileFℓisallfeaturesatlayerℓ, beunderstoodasanaverageofN ℓIIDfeatures,
Fℓ =(cid:0) fℓ fℓ ... fℓ (cid:1) . (2) Gℓ = 1 (cid:80)Nℓ fℓ(fℓ)T. (8)
1 2 Nℓ Nℓ λ=1 λ λ
ThisisusuallyknownasadeepGaussianprocess(DGP)
LookingatEq.equation7a, theexpectationofGℓ condi-
(Damianou & Lawrence, 2013; Salimbeni & Deisenroth,
tionedontheGrammatrixfromthepreviouslayeris,
2017). TogetaDGPthatisexactlyequivalenttoaNN,we
needtouseasomewhatunusualkernel, E[Gℓ|Gℓ −1]=K(Gℓ −1) (9)
K (Fℓ)= 1 ϕ(Fℓ)ϕT(Fℓ). (3)
NN Nℓ whilethevariancescaleswith1/N ℓ,
Cho&Saul(2009)showedthat,intheinfinite-widthlimit
withϕastheReLUfunction,wehave
V[Gℓ|Gℓ −1]∝1/N ℓ. (10)
Nl ℓi →m ∞N1 ℓϕ(Fℓ)ϕT(Fℓ)=K arccos(Fℓ −1). (4) T vah ru ias n, ca es gw oe esta toke zea rn o,in anfi dni tt he e-w pi rd ioth roli vm erit, thN eℓ Gr→ am∞ m, att rh ixe
and they give a simple, efficient functional form for the atonelayer,Gℓbecomesdeterministicandconcentratedat
functionK arccos(·). itsexpectation,K(Gℓ −1). Thus,thekernelateverylayer
3Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
isafixed,deterministicfunctionoftheinputsthatcanbe ν is the number of input node features, and ν is the
0 L+1
computedrecursively, numberofclasses.
K (Fℓ)=(K◦···◦K)( 1 XXT), (11) The post-activations in a GCN at the ℓth layer,
features (cid:124) (cid:123)(cid:122) (cid:125) ν0 Hℓ ∈RP ×Nℓ,aregivenby,
ℓ+1times
where 1 XXT is the kernel for the inputs, and F0 = X. Hℓ =ϕ(AˆHℓ −1Wℓ), (13)
ν0
Settingℓ=L,weseethatthisholdstrueattheoutputlayer,
implyingthattheoutputsofaNNorDGPareGPdistributed
whereWℓ ∈RNℓ−1×Nℓ isamatrixoflearnableparameters,
ϕisanon-linearity(e.g.ReLU),andAˆ isarenormalizedad-
withafixedkernelintheinfinite-widthlimit. Thisisknown
jacencymatrix. Therenomalizedadjacencymatrixisgiven
astheneuralnetworkGaussianprocess(NNGP)(Leeetal.,
1 1
2018). Ofcourse,afixedkernelimpliesnorepresentation byAˆ = D˜ −2A˜D˜ −2 whereA˜ = A+Iaddsself-loops,
learning. andD˜ isthediagonaldegreematrixwithD˜ ii =(cid:80) jA˜ ij.
For node classification the predictions have form
3.3.DeepKernelMachines Yˆ =softmax(AˆHLWL+1). One can also modify the
abovesetupforgraph-leveltasksonmulti-graphdatasets,
DeepKernelMachines(DKMs)(Yangetal.,2023;Milsom
by processing several graphs in a single mini-batch, and
etal.,2023)fixthelackofrepresentationlearningininfinite-
applyingagraphpoolinglayerbeforeperformingclassifica-
widthNNGPsbydefininganalternativeinfinite-widthlimit
tion.
inwhichrepresentationlearningisretained. Theydothis
bytreatingtheGrammatricesaslearnedparameters,rather
thanrandomvariables. Inpractice,theoptimalGrammatri- 3.5.GraphConvolutionalNeuralNetworkGaussian
ces,Gℓ,arelearnedusingthe“DKMobjective”. Processes
L(G1,...,GL)=logP(Y |GL) SimilarlytoSection3.2,itispossibletoconstructanNNGP
forthegraphdomain. ByEq.13,thepre-activationsFℓof
(cid:88)L (cid:16) (cid:17) agraphconvolutionlayerare,
− ν ℓD
KL
N(0,Gℓ)||N(0,K(Gℓ −1)) . (12)
ℓ=1 Fℓ =AˆHℓ 1Wℓ. (14)
−
Here,P(Y |GL)describeshowwellthelearnedrepresen-
WeplaceanIIDGaussianprioroverourweightsWℓ ∼
tationGL performsatthetask. Formally,thistermisthe µλ
N(0, 1 ). The NNGP construction tells us that as we
marginallikelihoodforaGaussianprocessdistributionover Nℓ−1
theoutputswithkernelK(GL). Thismarginallikelihood taketheinfinite-widthlimitofhiddenlayersN ℓ →∞,the
pre-activationsinEq. 14becomeGPdistributedwithkernel
is traded off against the KL-regularisation terms. Criti-
matrixK=E[Fℓ(Fℓ)T]. WeshowinAppendixCthatthis
cally,thesetermshaveanaturalinterpretationaspullingthe
learnedrepresentations,Gℓtowardsthevaluewewouldex- expectationhasclosed-form,
pectunderthefixed-representationinfinite-widthNNGP,i.e. K=AˆΦℓ 1AˆT, (15)
K(Gℓ −1). Infact,aswesendν
ℓ
→∞,theDKMbecomes −
exactlyequaltothestandardinfinitewidthNNGP.Andas whereΦℓ 1 =E[Hℓ 1(Hℓ 1)T]istheNNGPkernelofa
− − −
ν ℓbecomessmaller,weallowmoreflexibilityinthelearned fullyconnectednetwork(e.g. thearccoskernelinthecase
representationstofitthedatabetter. Thus,throughν ℓ,the ofaReLUnetwork). UsingthiskernelintheNNGPdefined
DKMgivesusaknobwhichwecanusetotunetheamount inEq. 11givesusthegraphconvolutionalNNGPrecursion,
ofrepresentationlearningallowedinourmodel,allowing
ustocleanlyestablishtheimportanceofrepresentationin Gℓ =AˆK(Gℓ −1)AˆT. (16)
varioustasksbychangingν intheDKM.
ℓ
4.Methods
3.4.GraphConvolutionalNetworks
WenowintroducegraphconvolutionalDKMs,anddevelop
Kipf&Welling(2017)derivedGraphConvolutionalNet-
aninducingpointschemeforthegraphconvolutionalDKM
works(GCNs)byapplyingapproximationstospectralmeth-
setting.
ods on graphs. They considered a semi-supervised set-
ting, and applied the GCN to graph datasets of the form
4.1.GraphConvolutionalDKMs
(X,Y,A),withnodes/datapointfeaturesX ∈ RP ×ν0,in-
completelabelsY ∈ RPlabelled×νL+1,andanadjacencyma- ThegraphconvolutionalNNGPsuffersfromthe“fixedrep-
trixA∈RP P. Here,P isthenumberofnodes/datapoints resentations”problemthatplaguesallNNGPmodels—its
×
inthegraph,P <P isthenumberoflabellednodes, kernelisafixedtransformationoftheinputs,regardlessof
labelled
4Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
thetargetlabels,meaningthatthereislittleflexibility(apart dataset,andthatthefixedrepresentationgraphconvolutional
fromthroughkernelhyperparameters)tolearnsuitablefea- NNGPiscompetitiveonlywhengraphsarehomophilous.
turesforthetaskathand. ThisisunlikeaGCN(Eq. 13),
Theexperimentswereconductedbroadlyontwotypesof
whichhasflexiblelearnedweights.
dataset:
WethereforedevelopagraphconvolutionalDKMthathas
learnable representations at each layer, by modifying the • Nodeclassificationdatasetswithasinglegraph. For
limitofthegraphconvolutionalNNGP.Doingsogivesthe thePlanetoiddatasets(Senetal.,2008), Cora,Cite-
objectivefunction, seer, and Pubmed, the task is to predict scien-
tific paper topics from citations; and the Squir-
(cid:16) (cid:17)
L(G1,...,GL |X,Y)=logP Y |GL (17) relandChameleontasks(Rozemberczkietal.,2021)
aretopredictdailytrafficforWikipediapagesrelated
−(cid:88)L ν ℓD KL(cid:16) N(0,Gℓ)||N(0,AˆK(Gℓ −1)AˆT )(cid:17) . Tto heth re emco ar ir ne is np gon nd oi dn eg ca ln ai sm sia fil cs af tir oo nm dt ah te ai sr eth syp we er rl eink ins -.
ℓ=1
troducedbyPlatonovetal.(2023). TheRomanEm-
Thisisanalogoustothe“fully-connected”DKMobjective piretaskistosyntacticallyclassifywordsontheRo-
inEq.12,exceptthatitincorporatesagraphkernel,using manEmpireWikipediaarticlebasedonsyntacticand
information from the adjacency matrix as in Eq. 16 (see positionalrelationships;Minesweeperinvolvessolving
AppendixC). agameofminesweeperusingagraphicalrepresenta-
tionofthegrid;AmazonRatingsrequirespredicting
Evaluating the DKM objective L has time complexity
user ratings of products; and the Tolokers dataset is
O(P3),andinvolvesoptimizingoveratleastLP2parame-
aboutpredictingwhetherusershavebeenbannedfrom
ters,whereP isthenumberofnodesinallgraphs. Clearly
theTolokersplatform,byusinginformationaboutother
thisisonlycomputationallyfeasibleforsmalldatasets. To
usersthathaveperformedthesametasks.
resolve this, Yang et al. (2023) and Milsom et al. (2023)
useaminibatched-trainingalgorithmthatapproximatesthe • Graph classification datasets with multiple
trainingsetusingasetofP inducingpoints. Thisscheme graphs (Morris et al., 2020). These are all molecule
i
hastimecomplexityO(L(P3+P2P))forLlayerswhere datasets with the molecules represented as graphs,
i i
P ≪ P,therebysidesteppingtheintractablecostoffull- where the task is to predict properties of the
i
rankcomputations. molecules.NCI1andNCI109arechemicalcompound
datasets, Proteins and Enzymes are protein datasets,
However,theirschemecannotbeapplieddirectlybecause
and Mutag and Mutagenicity are both datasets of
it applies to individual datapoints (Yang et al., 2023) or
compounds.
to individual images (Milsom et al., 2023). Importantly,
predictionsforeachdatapoint(Yangetal.,2023)orimage
AppendixA.1containsfurtherqualitativedetailsaboutthe
(Milsom et al., 2023) are independent, given the model
datasets,andTables1and2showdatasetsummarystatistics.
parameters. In contrast, the whole point of graph neural
networksistoallownearbynodestoinfluenceeachother’s We measure homophily of a graph by homophily ratio,
labels. TheinducingpointapproachinYangetal.(2023) which is the ratio of the number of edges between ‘like
and Milsom et al. (2023) is not designed for that setting, nodes’andthetotalnumberofnodes,whereby‘likenodes’
soweneededtodevelopanalternativeapproach. Wealso wemeannodeswiththesamelabel.Wecancalculatetheho-
needed to address the additional challenge that inducing mophilyratioforthenodeclassificationdatasets,butnotthe
points(e.g.intheGPsetting)aretypicallydefinedinterms graphclassificationdatasets(becauseforthesedatasetswe
offeatures,whereasfeaturesarenotavailableinaDKM: onlyhavegraph-levellabelsandnotnodelabels).Cora,Cite-
weonlyhavetheGrammatricesateachlayer. Forfurther seerandPubmedarehighlyhomophilous,withhomophily
detailsofourapproach,seeAppendixBandC. ratios > 0.8. The remaining node classification datasets
allhavehomophilyratios<0.7. Weexpectgraphclassifi-
5.Experiments cationdatasetstoexhibitbothhomophilyandheterophily,
becausetheyaremadeupofmolecules(Yeetal.,2022).
5.1.Overview
Inourexperiments,weusedgridsearchesoneachdataset
ToempiricallytestthegraphconvolutionalDKM,webench- tooptimizehyperparameters. WetrainedaGCNinaddition
markeditagainstthegraphconvolutionalNNGPandthe totheDKMandNNGPforthenodeclassificationdatasets,
GCN,onseveralnodeclassificationandgraphclassification where we used similar architectures for fair comparison.
datasets. We demonstrate that the flexible graph convo- ThemostimportanthyperparameterfortheDKMisthereg-
lutionalDKMiscompetitivewiththeGCNonallbutone ularizationν. Ratherthanhavingadifferentregularization
5Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
Table1. Nodeclassificationdatasetstatistics.
Dataset #nodes #edges Homophilyratio #features #classes
RomanEmpire 22,662 32,927 0.05 300 18
Squirrel 5,201 198,493 0.22 2,089 5
Chameleon 2,227 31,421 0.24 2,325 5
AmazonRatings 24,492 93,050 0.38 46 5
Tolokers 11,758 519,000 0.60 10 2
Minesweeper 10,000 39,402 0.68 7 2
Citeseer 3,327 9,104 0.74 3,703 6
Pubmed 19.717 88,648 0.8 500 3
Cora 2,708 10,556 0.81 1,433 7
Table2. Graphclassificationdatasetstatistics.
Dataset #graphs avg. #nodes avg. #edges #features #classes
Proteins 1,113 39.1 72.8 5 2
Enzymes 600 32.6 62.1 22 6
NCI1 4,110 29.9 32.3 38 2
NCI109 4,127 29.7 32.1 39 2
Mutag 188 17.9 19.8 8 2
Mutagenicity 4,337 30.3 30.8 15 2
Table3.Nodeclassificationtestaccuracies±onestandarddeviation.Wherecomparableaccuraciesareavailable,wereferencethem:Kipf
&Welling(2017)(⋆),Niuetal.(2023)(†).
Dataset(homophilyratio) GraphConv. DKM GraphConv. NNGP GCN
RomanEmpire(0.05) 75.6±0.7 67.1±0.4 83.4±0.4
Squirrel(0.22) 57.7±1.8 27.0±1.7 58.5±1.9
Chameleon(0.24) 70.8±1.9 39.1±3.0 67.8±2.8
AmazonRatings(0.38) 48.8±0.6 41.6±0.3 49.3±0.6
Tolokers(0.6) 80.7±0.6 78.2±0.1 80.7±0.3
Minesweeper(0.68) 85.7±0.5 80.1±0.1 86.6±0.6
Citeseer(0.74) 69.5±0.2 70.9 70.3⋆
†
Pubmed(0.8) 79.4±0.1 79.6 79.0⋆
†
Cora(0.81) 81.5±0.2 82.8 81.5⋆
†
Table4.Graphclassificationtestaccuracies,±onestandarddeviation.GCNaccuraciesaresourcedfromZhangetal.(2019)(⋆),Balcilar
etal.(2020)(†),andYangetal.(2020)(‡).
Dataset GraphConv. DKM GraphConv. NNGP GCN
Mutag 87.3±0.7 66.2±2.2 85.6±5.8
‡
Mutagenicity 79.3±0.4 66.5±0.3 79.8±1.6⋆
Proteins 71.4±0.5 67.0±0.4 75.2±3.6⋆
Enzymes 72.4±0.9 31.6±0.9 75.2±0.7
†
NCI1 71.0±0.7 58.5±1.4 76.3±1.8⋆
NCI109 70.2±0.4 57.0±0.9 75.9±1.8⋆
6Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
90 90 90
80 80 80
70 70 70
60 60 60
50 50 50
40 40 m toi ln oe ks ew rsee (p he =r(h 0.6= )0.68) 40 m mu ut ta ag
genicity
amazon-ratings(h=0.38) proteins
cora(h=0.81) chameleon(h=0.23) enzymes
30 pubmed(h=0.8) 30 squirrel(h=0.22) 30 nci1
citeseer(h=0.74) roman-empire(h=0.05) nci109
10−2 100 102 10−2 100 102 10−2 100 102
Regularizationstrength Regularizationstrength Regularizationstrength
Figure1.EffectofvaryingDKMregularizationstrength(ν)onvalidationperformanceforhomophilousnodeclassificationdatasets(left),
heterophilousnodeclassificationdatasets(middle),andgraphclassificationtasks(right).Validationaccuracieswereobtainedbytaking
optimalsettingsfromthegridsearchforthegraphconvolutionalNNGP,andvaryingν.
5.2.Results
Tables3and4summarizeaccuraciesoneachdataset. Ta-
cora
1.0 pubmed ble3showthatthegraphconvolutionalDKMiscompetitive
citeseer
minesweeper withtheGCNonallnodeclassificationdatasetswiththeex-
0.8 tolokers ceptionofRomanEmpire. Onepossiblereasonforthepoor
amazon-ratings
0.6 chameleon performanceonRomanEmpireisthattheGCNwastrained
squirrel
roman-empire withdropoutwhichboostsperformancesignificantly,butwe
0.4 didnottraintheDKMwithananalogousregularizer(specif-
ically, we found that a GCN without dropout performed
0.2
significantlyworseduringthegridsearchstage,withamean
0.0 validation accuracy of 77.0% without dropout vs. 83.3%
withdropout). Theperformanceofthegraphconvolutional
0.2 0.4 0.6 0.8
NNGP is close on Cora, Citeseer, Pubmed, Tolokers, but
Homophilyratio
significantlyworseonallotherdatasets. Table4showsthat
thegraphconvolutionalDKMisroughlycomparabletothe
Figure2.EffectofvaryingDKMregularizationstrength(ν)onval-
GCNonthegraphclassificationtasks,whereasthegraph
idationperformanceforhomophilousnodeclassificationdatasets
convolutionalNNGPisnotcomparableonanydataset.
(left), heterophilous node classification datasets (middle), and
graph classification tasks (right). Relative improvement is cal- Figure1summarizestheeffectofDKMregularization(ν)
culatedasthechangeintesterrorwhenchangingfromanNNGP onperformancefordifferentdatasets. WhenDKMregular-
toaDKM. izationishigher,themodelisregularizedmoretowardsthe
graphconvolutionalNNGP.Weseeondatasetswithhigh
homophily(Cora,Citeseer,Pubmed)thathigherregulariza-
tionispreferred,indicatingthatthefixedrepresentationsof
theNNGParesufficienttosolvethetask. Thesamecan-
at each layer as in Eq. 17, we set ν ℓ = ν, and for each notbesaidoftheremainingdatasets,whicharenothighly
dataset we perform a grid search over ν. We optimized homophilous — performance degrades either marginally
other hyperparameters suchas number of layers, number orsignificantlyasweregularizemoretowardstheNNGP
ofhiddenunits/inducingpoints,andmore,detailsofwhich (ν =∞).
canbefoundinAppendixA.2.
7
)%(cca
.laV
tnemevorpmi
.leRFlexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
Figure2showstherelationshipbetweenperformanceim- infiniteneuralnetworks. InICML,2020.
provementandhomophilyratio. Therelationshipissignifi-
cantwitht-value−2.54(df=7)andp-value0.04. Whenthe Balcilar,M.,Renton,G.,He´roux,P.,Gauzere,B.,Adam,S.,
homophilyratioishigh,weseethatthereislittleimprove- andHoneine,P. Bridgingthegapbetweenspectraland
mentinswitchingfromthefixedNNGPrepresentationsto spatialdomainsingraphneuralnetworks. arXivpreprint
the flexible DKM representations. Conversely, when the arXiv:2003.11702,2020.
homophilyratioislowweseebiggains. Thissuggeststhat
Bengio,Y.,Courville,A.,andVincent,P. Representation
the fixed representations of the NNGP are insufficient to
learning: Areviewandnewperspectives. IEEEtransac-
explaindataonheterophilousgraphs.
tionsonpatternanalysisandmachineintelligence,35(8):
1798–1828,2013.
6.ConclusionandDiscussion
Bordelon,B.andPehlevan,C. Self-consistentdynamical
Byleveragingthedeepkernelmachineframework,wehave
fieldtheoryofkernelevolutioninwideneuralnetworks.
developedaflexiblevariantofaninfinite-widthgraphcon-
JournalofStatisticalMechanics:TheoryandExperiment,
volutionalnetwork.ThegraphconvolutionalDKMprovides
2023(11):114009,2023.
a hyperparameter that allows us to interpolate between a
flexiblekernelmachineandthegraphconvolutionalNNGP Bronstein,M.M.,Bruna,J.,LeCun,Y.,Szlam,A.,andVan-
with fixed kernel. This feature of the DKM enabled us dergheynst,P. Geometricdeeplearning: goingbeyond
to examine the importance of representation learning on euclideandata. IEEESignalProcessingMagazine,34(4):
graphtasks. Remarkably,wefoundthatnotallgraphtasks 18–42,2017.
benefitfromrepresentationlearning. Moreover,wefound
thattheimportanceofrepresentationlearningforaparticu- Cho, Y. and Saul, L. Kernel methods for deep learning.
lartaskcorrelateswiththelevelofheterophily,withmore Advancesinneuralinformationprocessingsystems,2009.
heterophily requiring more flexibility. This finding helps
explainthefactthatNNGPshavepreviouslybeenshownto Cosmo,L.,Minello,G.,Bronstein,M.,Rodola`,E.,Rossi,
performwellongraphtasks,butnotinotherdomainssuch L.,andTorsello,A. Graphkernelneuralnetworks. arXiv
asimages. preprintarXiv:2112.07436,2021.
Despiteusinganinducingpointscheme,computationalcost Damianou,A.andLawrence,N. Deepgaussianprocesses.
of the graph DKMs is high. We expect that improving InArtificialIntelligenceandStatistics,2013.
computational efficiency can help improve performance,
as it would allow for easier architecture exploration, and Fey,M.andLenssen,J.E.Fastgraphrepresentationlearning
trainingofbiggermodels. Wealsoexpectthatmorecareful withPyTorchGeometric. InICLRWorkshoponRepre-
optimization could aid performance. However, we leave sentationLearningonGraphsandManifolds,2019.
theseissuesforfuturework.
Garriga-Alonso, A., Rasmussen, C. E., and Aitchison, L.
Deep convolutional networks as shallow gaussian pro-
7.ImpactStatement
cesses. arXivpreprintarXiv:1808.05587,2018.
Thispaperpresentsworkwhosegoalistoadvancethefield
Hu, J., Shen, J., Yang, B., and Shao, L. Infinitely wide
ofmachinelearning,andmorespecificallyourunderstand-
graphconvolutionalnetworks: semi-supervisedlearning
ing of representation learning in graphs. There are many
viagaussianprocesses. arXivpreprintarXiv:2002.12168,
potentialsocietalconsequencesofourwork,nonewhichwe
2020.
feelmustbespecificallyhighlightedhere.
Jacot,A.,Hongler,C.,andGabriel,F.Neuraltangentkernel:
References Convergenceandgeneralizationinneuralnetworks. In
NeurIPS,pp.8580–8589,2018.
Achten,S.,Tonin,F.,Patrinos,P.,andSuykens,J.A. Semi-
supervisedclassificationwithgraphconvolutionalkernel Kashima,H.,Tsuda,K.,andInokuchi,A. Marginalizedker-
machines. arXivpreprintarXiv:2301.13764,2023. nelsbetweenlabeledgraphs. InProceedingsofthe20th
internationalconferenceonmachinelearning(ICML-03),
Adlam,B.,Lee,J.,Padhy,S.,Nado,Z.,andSnoek,J.Kernel pp.321–328,2003.
regressionwithinfinite-widthneuralnetworksonmillions
ofexamples. arXivpreprintarXiv:2303.05420,2023. Kipf, T. N. and Welling, M. Semi-supervised classifica-
tion with graph convolutional networks. International
Aitchison,L. Whybiggerisnotalwaysbetter: onfiniteand ConferenceonLearningRepresentations,2017.
8Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
LeCun,Y.,Bengio,Y.,andHinton,G.Deeplearning.nature, Scarselli,F.,Gori,M.,Tsoi,A.C.,Hagenbuchner,M.,and
521(7553):436–444,2015. Monfardini,G. Thegraphneuralnetworkmodel. IEEE
transactionsonneuralnetworks,20(1):61–80,2008.
Lee,J.,Bahri,Y.,Novak,R.,Schoenholz,S.S.,Pennington,
J.,andSohl-Dickstein,J. Deepneuralnetworksasgaus- Sen,P.,Namata,G.,Bilgic,M.,Getoor,L.,Galligher,B.,
sian processes. International Conference on Learning andEliassi-Rad,T. Collectiveclassificationinnetwork
Representations,2018. data. AImagazine,2008.
Lee,J.,Schoenholz,S.,Pennington,J.,Adlam,B.,Xiao,L., Shankar,V.,Fang,A.,Guo,W.,Fridovich-Keil,S.,Ragan-
Novak,R.,andSohl-Dickstein,J. Finiteversusinfinite Kelley, J., Schmidt, L., and Recht, B. Neural kernels
neuralnetworks: anempiricalstudy. AdvancesinNeural withouttangents. InInternationalconferenceonmachine
InformationProcessingSystems,33:15156–15172,2020. learning,pp.8614–8623.PMLR,2020.
Shervashidze,N.andBorgwardt,K. Fastsubtreekernels
MacKay, D. J. et al. Introduction to gaussian processes.
ongraphs. Advancesinneuralinformationprocessing
NATOASIseriesFcomputerandsystemssciences,168:
systems,22,2009.
133–166,1998.
Shervashidze,N.,Vishwanathan,S.,Petri,T.,Mehlhorn,K.,
Matthews,A.G.d.G.,Rowland,M.,Hron,J.,Turner,R.E.,
and Borgwardt, K. Efficient graphlet kernels for large
andGhahramani,Z. Gaussianprocessbehaviourinwide
graphcomparison. InArtificialintelligenceandstatistics,
deepneuralnetworks. arXivpreprintarXiv:1804.11271,
pp.488–495.PMLR,2009.
2018.
Velickovic´, P., Cucurull, G., Casanova, A., Romero, A.,
Milsom, E., Anson, B., and Aitchison, L. Convolutional
Lio,P.,andBengio,Y. Graphattentionnetworks. arXiv
deepkernelmachines,2023.
preprintarXiv:1710.10903,2017.
Morris,C.,Kriege,N.M.,Bause,F.,Kersting,K.,Mutzel,
Vyas, N., Atanasov, A., Bordelon, B., Morwani, D.,
P.,andNeumann,M. Tudataset: Acollectionofbench-
Sainathan, S., and Pehlevan, C. Feature-learning net-
markdatasetsforlearningwithgraphs. arXivpreprint
works are consistent across widths at realistic scales.
arXiv:2007.08663,2020.
arXivpreprintarXiv:2305.18411,2023.
Neal,R.M. BayesianLearningforNeuralNetworks,Vol.
Walker,I.andGlocker,B.Graphconvolutionalgaussianpro-
118ofLectureNotesinStatistics. Springer-Verlag,1996.
cesses.InInternationalConferenceonMachineLearning,
pp.6495–6504.PMLR,2019.
Niu, Z., Anitescu, M., and Chen, J. Graph neural
network-inspiredkernelsforgaussianprocessesinsemi- Williams,C. Computingwithinfinitenetworks. InMozer,
supervisedlearning. arXivpreprintarXiv:2302.05828, M.,Jordan,M.,andPetsche,T.(eds.),AdvancesinNeural
2023. Inpress. InformationProcessingSystems,volume9.MITPress,
1996. URL https://proceedings.neurips.
Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G.,
cc/paper_files/paper/1996/file/
Hron, J., Abolafia, D. A., Pennington, J., and Sohl-
ae5e3ce40e0404a45ecacaaf05e5f735-Paper.
Dickstein,J. Bayesiandeepconvolutionalnetworkswith
pdf.
many channels are gaussian processes. arXiv preprint
arXiv:1810.05148,2018. Yanardag,P.andVishwanathan,S. Deepgraphkernels. In
Proceedingsofthe21thACMSIGKDDinternationalcon-
Platonov,O.,Kuznedelev,D.,Diskin,M.,Babenko,A.,and ferenceonknowledgediscoveryanddatamining,2015.
Prokhorenkova, L. A critical look at the evaluation of
gnnsunderheterophily: arewereallymakingprogress? Yang,A.X.,Robeyns,M.,Milsom,E.,Schoots,N.,Anson,
arXivpreprintarXiv:2302.11640,2023. B.,andAitchison,L. Atheoryofrepresentationlearning
in deep neural networks gives a deep generalisation of
Rozemberczki,B.,Allen,C.,andSarkar,R. Multi-scaleat- kernelmethods. InternationalConferenceonMachine
tributednodeembedding. JournalofComplexNetworks, Learning,2023. Inpress.
9(2):cnab014,2021.
Yang, G. Scaling limits of wide neural networks with
Salimbeni,H.andDeisenroth,M. Doublystochasticvaria- weightsharing: Gaussianprocessbehavior,gradientin-
tionalinferencefordeepgaussianprocesses. Advances dependence,andneuraltangentkernelderivation. arXiv
inneuralinformationprocessingsystems,2017. preprintarXiv:1902.04760,2019.
9Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
Yang,G.andHu,E.J. TensorprogramsIV:featurelearn-
ingininfinite-widthneuralnetworks. InICML,volume
139ofProceedingsofMachineLearningResearch,pp.
11727–11737.PMLR,2021.
Yang,Y.,Feng,Z.,Song,M.,andWang,X. Factorizable
graphconvolutionalnetworks. AdvancesinNeuralInfor-
mationProcessingSystems,33:20286–20296,2020.
Ye,W.,Yang,J.,Medya,S.,andSingh,A. Incorporating
heterophilyintographneuralnetworksforgraphclassifi-
cation. arXivpreprintarXiv:2203.07678,2022.
Zhang, Z., Bu, J., Ester, M., Zhang, J., Yao, C., Yu, Z.,
andWang,C. Hierarchicalgraphpoolingwithstructure
learning. arXivpreprintarXiv:1911.05954,2019.
10Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
A.Experimentaldetails
OurexperimentsinvolvedbenchmarkingthegraphconvolutionalDKMandagraphconvolutionalNNGPonseveralnode
andgraphclassificationdatasets. WealsobenchmarkedaGCNonthesamedatasetstoobtainasuitablebaseline.
A.1.Datasetdetails
Thedatasetswebenchmarkedsplitbroadlyintotwocategories: nodeclassificationandgraphclassification. Statisticsfor
eachdatasetaregiveninTables1and2,butherewedescribethedatasetsqualitatively.
Forthenodeclassificationdatasets,thetaskistopredictthelabelsofallnodesinasinglegraph,givenlabelsforonlylimited
numberofnodes.Cora,Citeseer,andPubmed(Planetoiddatasets)areallcitationnetworks,wherenodesrepresentscientific
publications,andedgesarecitationsbetweenthosepublications;one-hotkeywordencodingsareprovidedasfeatures,and
thegoalistoplacedocumentsintopredefinedcategories.ChameleonandSquirrelarebothWikipedianetworksontheir
respectivetopics,sothatnodescorrespondtoarticles,andedgescorrespondtolinksbetweenarticles;featuresareagain
one-hotasinthecitationnetworks,butrepresentthepresenceofrelevantnouns,andthetaskistocategoriespagesbased
ontheiraveragedailytraffic. Theremainingnodeclassificationdatasetshaveunrelateddomains,butwereallintroduced
byPlatonovetal.(2023):
• RomanEmpireisderivedentirelyfromtheRomanEmpirearticleonWikipedia. Nodesarewordsinthetext,andthe
wordsare‘connected’iftheyareeitheradjacentinasentenceoriftheyaredependentoneachothersyntactically.
Featuresarewordembeddings,andthetaskistolabeleachword’ssyntacticrole.
• TolokersisadatasetofworkersfromtheTolokaplatform. Thenodesareworkers,andnodesareconnectedifworkers
haveworkedonthesametask. Featuresarestatisticsabouteachworker,andthetaskistopredictifaworkerhasbeen
banned.
• Minesweeperisasyntheticdataset. Thegraphhas10,000nodescorrespondingtothecellsinthe100by100gridinthe
Minesweepergame. Nodesareconnectediftheassociatedcellsareadjacent. Featuresareone-hotencodednumbersof
neighbouringmines,plusaextrafeatureforwhenthenumberofneighbouringminesisunknown. Thetaskistopredict
whichnodesaremines.
• AmazonRatingsisagraphwherenodesareAmazonproducts,andnodesareconnectediftheyarecommonlypurchased
together. Featuresaretextembeddingsoftheproductdescriptions. Thetaskistopredicttheaverageuserrating.
Thegraphclassificationdatasetsallcontainmolecules.NCI1andNCI109bothcontaingraphsofchemicalcompounds
(nodes representing atoms, and with nodes connected if there are chemical bonds between them), where the goal is to
predicteffectofthecompoundsvs. lungandovariancancer.Enzymesisadatasetofproteintertiarystructures,andthe
task is to classify according to protein class. Proteins is a dataset of proteins, where the task is to classify as enzymes
ornon-enzymes.Mutagisadatasetofcompounds,andthetaskistopredictmutagenicityonSalmonellatyphimurium.
Finally,Mutagenicityisadatasetofdrugcompounds,andthetaskistocategorizeintomutagenornon-mutagen.
A.2.Detailsoftraining,hyperpameterselectionandfinalbenchmarking
For the node classification datasets, we used publicly available train/validation/test splits from the Pytorch Geometric
library(Fey&Lenssen,2019). Forthegraphclassificationdatasets,wecreatedtrain/test(90%/10%)splitsusingastratified
K-foldsimilarlytoBalcilaretal.(2020);forhyperparameterselection,wefurtherintroducedavalidationsetbyrandomly
selectinggraphsfromthetrainsettoobtaina80%/10%/10%split. Fordatapreprocessing,we(a)renormalizealladjacency
matricesasKipf&Welling(2017),(b)addanodedegreefeaturetothegraphclassificationdatasets,and(c)Z-scalenode
featuresintheEnzymesdataset.
Eachnodeclassificationmodelwastrainedfor200epochsusingtheAdamoptimizer. Weusedalearningrateof10 2for
−
thefirst100epochs,10 3forthenext50,10 4forthenext25,and10 5forthefinal25epochs. Forthegraphclassification
− − −
taskswetrainedfor120epochs,warming-upfor40epochsfrom10 3to10 2andthenhalvingeverysubsequent20epochs.
− −
Fortheplanetoiddatasets,weusedthesame2-layerGCNarchitectureasKipf&Welling(2017),butwithbatchnormForthe
graphclassificationdatasets,wealsousedthearchitecturefromKipf&Welling(2017),butwith6layers. Fortheremaining
datasets,weusethesameGCNarchitectureasPlatonovetal.(2023). WhentrainingthegraphconvolutionalDKMs,we
11Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
mirroredtheGCNarchitectures—inplaceoflinearlayersweusedGramlayers,inplaceofReLUnon-linearitiesweused
theReLUkernelnon-linearity(Cho&Saul,2009)withσ =0andσ =1,inplaceofbatchnorm,weusedglobalkernel
b w
normfromMilsometal.(2023).
TofindsuitablehyperparametersfortheGCN,weusedagridsearch.WhenusingtheGCNarchitecturefromKipf&Welling
(2017),wegridsearchedovernumberofhiddenunits∈{16,32,64,128},dropout∈{0,0.5},batchnorm∈{yes,no},and
rownormalizingfeatures∈{yes,no}. WhenusingthearchitecturefromPlatonovetal.(2023),wedothesame,exceptwe
searchoverhiddenunits∈{128,384}andadditionallynumberoflayers∈{2,3,4}.
TofindhyperparametersforthegraphconvolutionalDKMinthenodeclassificationsetting,weperformedtwoseparate
gridsearches. Wesearchedforregularizationν ∈{0,10 3,10 2,10 1,100,101,102,103,∞}. Oncethesesettingshad
− − −
beenobtained,weincreasedthenumberofinducingpointsfrom128to512,andgridsearchedoverdifferentinducing
Gram matrix initializations, and whether to learn a feature scaling in the input layer. For some datasets (e.g. Pubmed)
wefounditwasnecessarytousearandominitializationratherthananNNGPinitializationtoensurenumericalstability.
In the graph classification setting, we performed a single grid search over inducing points P ∈ {128,256,384}, ν ∈
i
{0,10 3,...,103,∞}andnumberoflayers∈{2,4,6}.
−
Forthefinalbenchmark,wereportthemeanaccuraciesonthetestsets. Fortheplanetoiddataset,weaveragedover20
randomseeds. Theremainingdatasetseachhave10splits—weperform50trainingruns,andreportthemeanaccuracy
overalloftheseruns.
ThecodewaswritteninPytorch,andwetrainedusingBristol’sAdvancedComputingResearchCentre(ACRC),onacluster
containingRTX3090’s,andA100s.
A.3.Extendedresults
Forcompleteness,weprovideallofourGCNandgraphconvolutionalNNGPresultsonthenodeclassificationdatasetsin
Table5,asinTable3weusedtheresultsofotherswherecomparableresultsareavailable.
Table5. Nodeclassificationtestaccuracies±onestandarddeviation.
Dataset(homophilyratio) GraphConv. DKM GraphConv. NNGP GCN
RomanEmpire(0.05) 75.6±0.7 67.1±0.4 83.4±0.4
Squirrel(0.22) 57.7±1.8 27.0±1.7 58.5±1.9
Chameleon(0.24) 70.8±1.9 39.1±3.0 67.8±2.8
AmazonRatings(0.38) 48.8±0.6 41.6±0.3 49.3±0.6
Tolokers(0.6) 80.7±0.6 78.2±0.1 80.7±0.3
Minesweeper(0.68) 85.7±0.5 80.1±0.1 86.6±0.6
Citeseer(0.74) 69.5±0.2 69.5±0.2 70.0±0.2
Pubmed(0.8) 79.4±0.1 79.4±0.1 78.1±0.3
Cora(0.81) 81.5±0.2 82.3±0.1 81.2±0.3
B.InducingPointApproximation
B.1.SparseDeepKernelMachines
SparseDKMsareinspiredbythesparseDGPliterature(Damianou&Lawrence,2013;Salimbeni&Deisenroth,2017),
wherethevariationalinference(VI)frameworkisused.OnecanobtainasparseDKMbyusinganapproximateposteriorthat
introducesinducingfeaturesFℓ
i
∈RPi×Nℓ inadditiontothe‘real’test/trainingfeaturesFℓ
t
∈RPt×Nℓ,ateachlayerinthe
DGP(Eq.1). TheimpliedGrammatrices(andkernels)aresimilarlysplitinto“inducing-inducing”(ii),“test/train-test/train”
(tt),and“inducing-test/train”(it/ti)blocks:
(cid:18) Fℓ(cid:19)(cid:18) Fℓ(cid:19)T (cid:18) Fℓ(Fℓ)T Fℓ(Fℓ)T(cid:19) (cid:18) Gℓ (Gℓ)T(cid:19)
Gℓ =Fℓ(Fℓ)T = i i = i i i t = ii ti (18)
Fℓ Fℓ Fℓ(Fℓ)T Fℓ(Fℓ)T Gℓ Gℓ
t t t i t t ti tt
This approximate posterior results in (see Yang et al. (2023) for details) the following evidence lower bound (ELBO),
12Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
sometimescalledthe“sparseDKMobjective”:
L (G1,...,GL,µ ,...,µ ,Σ)=GP-ELBO(Y |GL,µ ,...,µ ,Σ)
sparse ii ii 1 νL+1 1 νL+1
(cid:88)L (cid:16) (cid:17) (19)
− ν ℓD
KL
N(0,Gℓ ii)||N(0,K(Gℓ ii−1)) .
ℓ=1
Here,GP-ELBOistheELBOofa(shallow)sparseGPfittedonlabelsY. TheGPprioriszero-meanandhaskernelK(GL),
andweuseanapproximateposteriorN(µ ,Σ )overeachinducingoutputfeatureλ=1,...,ν . Weshareasingle
λ λ L+1
covariancematrixΣ =Σacrossalloutputfeaturestoreducecomputationalcost. Asusualinvariationalinference,we
λ
optimisehyperparameters, suchastheinducinginputlocationsX orkernel-specifichyperparameters, usingthissame
i
objective.
B.2.SparsegraphconvolutionalDKMs
InthegraphconvolutionalDKM,wetakeourinducingpointstobecompletelyisolatedinducingnodes,inthesensethat
theyareneitheradjacenttootherinducingpoints,nortest/trainnodes. Concretely,thegraphconvolutionalNNGPkernel
matrix(Eq. 15),whenusingthesparseinducingpointscheme,becomesablockmatrixasinEq. 18,withblocks
K (Gℓ
1)=(cid:32) Φ ii(G iℓ i−1) Φ it(Gℓ −1)AˆT (cid:33)
. (20)
GCN − AˆΦ ti(Gℓ −1) AˆΦ tt(Gℓ tt−1)AˆT
whereΦ ,Φ ,Φ refertotheblocksofthekernelmatrixresultingfromapplyingthe“basekernel”(e.g. thearccoskernel)
ii ti tt
toGℓ 1. ThefullderivationisgiveninAppendixC.Thisinducingpointschemecanbeusedfordatasetswithasingle
−
graphormultiplegraphs. Inthecasewherethedatasetisasinglegraph,weuseaminibatchsizeof1,andthe‘minibatch’
becomesallnodesinthegraph. Inthecasewherethedatasetcontainsseveralgraphs,thegraphscanbebatchedtogether.
Naively,thiswouldinvolvecreatingasinglelargeadjacencymatrixthatdescribesadjacencyinformationforgraphsinthe
minibatch,butinpracticethiscanbeefficientlyrepresentedwithasparsetensor.
B.3.ImprovingcomputationalcomplexitywithaNystro¨mapproximation
SectionB.2describeshowtoavoidanO(P3)timecomplexitybyusinginducingpointscheme,butthetest/trainGram
matricesG arestillacomputationalbottleneck. Specifically,storingcovariancesbetweennodesisexpensivebecauseit
tt
involvescalculatingandstoringanode-by-nodematrix,whichisprohibitivelycostlyforlargegraphs. Wedon’thavethis
problemwithadjacencymatrices,becausetheycanoftenbeassumedtobesparse. However,thereisnoreasontoassume
thatcovariancematricesaresparse,soasimilartrickcannotbeappliedforefficientlystoringG .
tt
WecanresolvethisissuebypropagatingonlythediagonalofGℓ throughtheDKM.Thisisfeasibleintheoutputlayer
tt
because we only need the diagonal at the output layer (since the DKM model assumes an IID distribution over output
features). WhenperformingthegraphconvolutioninagraphconvolutionalDKM,westillneedtopropagatethediagonalof
AˆΦ AˆT ,whichusuallyrequiresthefullcoavarianceΦ ,butthiscanbeavoidedbyusingaNystro¨mapproximation:
tt tt
Φ
tt
≈Φ tiΦ −ii1Φ it+D. (21)
Here, D is a diagonal correction, since we know the true diagonal. The graph convolution operation then becomes
K
tt
≈Aˆ(Φ tiΦ −ii1Φ it+D)AˆT ,andsinceweonlyneedthediagonal,thiscanbecomputedefficientlywhenAˆ issparse
usingthefollowingidentity,
diag(Aˆ(Φ tiΦ −ii1Φ it+D)AˆT )=colsum(K tiΦ −ii1⊙K ti)+colsum(Aˆ ⊙Aˆ ⊙ bdiag(D)), (22)
where ⊙ is the pointwise Hadamard product, and (Aˆ ⊙ diag(D)) = Aˆ diag(D) is the ‘broadcasted’ pointwise
b ij ij j
multiplicationofthediagonalcorrectionwitheachcolumnofAˆ.
WiththisNystro¨mapproximation,thememorycostfortheforwardpassisintheoryO(P P),andthetimecomplexity
t i
isO(LP P|E|)whereListhenumberoflayersand|E|isthenumberofedgesinthegraph. Unfortunately,duetolack
t i
of support in PyTorch for sparse tensors, we don’t implement Eq. 22 for our experiments. Instead, while we do use
theNystro¨mapproximation,weresorttocomputingthefulltest/traincovariancematriceswithEq.21.
13Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
ThecompletegraphconvolutionalDKMpredictionalgorithmisgiveninAlgorithm1. Thepredictionalgorithmaspresented
isfornoderegressionproblems. ItcanbemodifiedtoallowforclassificationproblemsbytreatingtheGaussianoutputsas
logits. Wecanalsomodifythealgorithmforgraphlevelproblems(e.g.graphclassification)byapplyingglobalaverage
poolingtothefinalkernel,K(GL).
Algorithm1DKMpredictionfornoderegression
Parameters: {ν }L
ℓ ℓ=1
InducingGrammatrices: {Gℓ}L+1
ii ℓ=1
Inducingandtrain/testinputs: X,X
i t
Inducingoutputsvariationalparameters: µ ,...,µ ,Σ
1 νL+1
InitializefullGrammatrix
(cid:18) G0 G0(cid:19) (cid:18) XXT XXT(cid:19)
ii it ← 1 i i i t
G0
ti
G0
tt
ν0 X tXT
i
X tXT
t
forℓin(1,...,L)do
Applykernelnon-linearity
(cid:18) Φℓ Φℓ(cid:19) (cid:18)(cid:18) Gℓ 1 Gℓ 1(cid:19)(cid:19)
ii it ←Φ ii− it−
Φℓ Φℓ Gℓ 1 Gℓ 1
ti tt ti− tt−
Performgraphconvolution
K ←Φℓ
ii ii
K ←AˆΦℓ
ti ti
K ←AˆΦℓ AˆT (oruseNystro¨mapproximationfromSectionB.3)
tt tt
Propagatethetest-testandtest-inducingblocks,usingconditionalGaussianformulae(Yangetal.,2023)
K
tti
←K tt−K tiK −ii1K it.
Gℓ· ←K K 1Gℓ
ti ti −ii ii
Gℓ
tt
←K tiK −ii1Gℓ iiK −ii1K it+K
tti
endfor ·
Outputlayer(L+1)predictionusingstandardsparseGaussianprocessexpressions
(cid:18) KL+1 KL+1(cid:19) (cid:18)(cid:18) GL GL(cid:19)(cid:19)
ii it ←Φ ii it
KL+1 KL+1 GL GL
ti tt ti tt
m ←KL+1(KL+1) 1µ
λ ti ii − λ
GL+1 ←KL+1−KL+1(KL+1) 1KL+1+KL+1(KL+1) 1Σ(KL+1) 1KL+1
tt tt ti ii − it ti ii − ii − it
GeneratesamplesoflatentoutputsFL+1 =(ft;L+1···ft;L+1)usingvariationaldistribution
(cid:16) (cid:17)
t 1 νL+1
Q(ft;L+1)∼ N m ,GL+1
λ ind λ tt
(cid:16) (cid:17)
regularizer←−(cid:80)L ν D N(0,Gℓ)||N(0,Kℓ)
ℓ=1 ℓ KL ii ii
(cid:16) (cid:17)
likelihood lb←E [P(Y |FL+1)]−(cid:80)NL+1D N(fL+1,Σ)||N(0,KL+1)
Q(FL+1) t t λ=1 KL λ ii
t
L ←likelihood lb+regularizer
LB
output←Q(FL+1), L
t LB
C.GraphconvolutionalNNGPkernelderivations
Inthisappendixweshowhowtoderivethekernel/covarianceexpressionsgiveninEq.15(forthefull-rankcase)andEq.20
(forthesparseinducingpointcase).
Withthe“completelyisolated”inducingnodesdescribedinthemaintext,thepre-activationfeaturesFℓ,Fℓfortheinducing
i t
andtest/trainblocksaregivenbythefollowing,insuffixnotation:
(cid:88)
F ii λ;ℓ = H ii; µℓ −1W
µλ
(23)
µ
(cid:88)
F st; λℓ = Aˆ ss′H st; ′ℓ µ−1W
µλ
(24)
s′µ
14Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
andwewishtocomputethecovariancesbetweenfeaturesasourkernel,i.e.
(cid:18)
K K
(cid:19) (cid:18)E[fi;ℓ(fi;ℓ)T] E[fi;ℓ(ft;ℓ)T](cid:19)
ii it = λ λ λ λ (25)
K
ti
K
tt
E[f λt;ℓ(f λi;ℓ)T] E[f λt;ℓ(f λt;ℓ)T]
whereweneedonlyconsideronefeatureλsincethefeaturesareIIDoverλ∈{1,...,N }.
ℓ
Fortheinducingblock,wehave
(cid:104) (cid:105)
Kii;ℓ =E Fi;ℓFi;ℓ (26)
ij i,λ j,λ
(cid:34) (cid:35)
(cid:88) (cid:88)
=E H ii; µℓ −1W µℓ
λ
H ji; νℓ −1W νℓ
λ
(27)
µ ν
=(cid:88) H ii; µℓ −1H ji; νℓ −1E(cid:2) W µℓ λW νℓ λ(cid:3) (28)
µν
(cid:88)
= Nℓ1
−1
H ii; µℓ −1H ji; νℓ −1δ
µ,ν
(29)
µν
(cid:88)
= Nℓ1
−1
H ii; µℓ −1H ji; µℓ −1 (30)
µ
=Φ iii j;ℓ −1 (31)
soKℓ ii(Gℓ ii−1)=Φ(Gℓ ii−1) (32)
whereΦ(·)isthekernelfunctionassociatedwiththeneuralnetworknon-linearity,e.g. anarccoskernel(Cho&Saul,2009)
inthecaseofaReLUnon-linearity. Hencetheinducing-inducingblockofthekernelmatrixactsjustlikeintheoriginal
“fully-connected”DKM(Yangetal.,2023).
Fortheinducing-test/trainblock,wehave
(cid:104) (cid:105)
Kit;ℓ =E Fi;ℓFt;ℓ (33)
is i,λ s,λ
(cid:34) (cid:35)
(cid:88) (cid:88)
=E H ii; µℓ −1W µℓ
λ
Aˆ ss′H sℓ ′−ν1W νℓ
λ
(34)
µ νs′
= (cid:88) H ii; µℓ −1Aˆ ss′H st; ′ℓ ν−1 E(cid:2) W µℓ ,λW νℓ ,λ(cid:3) (35)
µνs′
(cid:88)
= Nℓ1
−1
H ii; µℓ −1Aˆ ss′H st; ′ℓ ν−1δ
µ,ν
(36)
µνs′
(cid:88)
= Nℓ1
−1
H ii; µℓ −1Aˆ ss′H st; ′ℓ µ−1 (37)
µs′
(cid:88)
= Φ iit s;ℓ ′−1Aˆ ss′ (38)
s′
soKℓ it(Gℓ it−1)=Φ(G iℓ t−1)AˆT. (39)
Sincethekernelissymmetric,K =KT.
ti it
Finally,thetest/train-test/trainblockis,inourisolatedinducingnodesscheme,exactlythesameasthefull-rankcase,which
15Flexibleinfinite-widthgraphconvolutionalnetworksandtheimportanceofrepresentationlearning
isgivenby
(cid:104) (cid:105)
Ktt;ℓ =E Ft;ℓFt;ℓ (40)
sr sλ rλ
 
(cid:88) (cid:88)
=E  Aˆ ss′H st; µℓ −1W µℓ
λ
Aˆ rr′H rt; νℓ −1W νℓ λ (41)
µs′ νr′
= (cid:88) Aˆ ss′H st; µℓ −1Aˆ rr′H rt; νℓ −1E(cid:2) W µℓ λW νℓ λ(cid:3) (42)
µνs′r′
(cid:88)
= Nℓ1
−1
Aˆ ss′H st; µℓ −1Aˆ rr′H rt; νℓ −1δ
µ,ν
(43)
µνs′r′
(cid:88)
= Nℓ1
−1
Aˆ ss′H st; µℓ −1Aˆ rr′H rt; νℓ −1 (44)
µs′r′
(cid:88)
= Aˆ ss′Φ stt ′;ℓ r−′ 1Aˆ rr′ (45)
s′r′
soKℓ tt(Gℓ tt−1)=AˆΦ(Gℓ tt−1)AˆT. (46)
16