FaBERT: Pre-training BERT on Persian Blogs
MostafaMasumi♢†,SeyedSoroushMajd♢,MehrnoushShamsfard♢,andHamidBeigy†
♢ComputerScienceandEngineeringDepartment,ShahidBeheshtiUniversity
♢s.majd@mail.sbu.ac.ir,m-shams@sbu.ac.ir
†ComputerEngineeringDepartment,SharifUniversityofTechnology
†{m.masumi,beigy}@sharif.edu
Abstract inamyriadoftasks. Despitetheirsignificantcon-
tributions, finely-tuned LMs such as BERT still
We introduce FaBERT, a Persian BERT-
demonstraterobustperformance,achievingcompa-
base model pre-trained on the HmBlogs
rableresultsor,inmanycases,evenoutperforming
corpus, encompassing both informal and
LLMsintraditionalNaturalLanguageUnderstand-
formalPersiantexts. FaBERTisdesigned
ing (NLU) tasks, including Natural Language In-
to excel in traditional Natural Language
ference (NLI), Sentiment Analysis, Text Classifi-
Understanding(NLU)tasks,addressingthe
cation,andQuestionAnswering(QA)(Yangetal.,
intricacies of diverse sentence structures
2023).
andlinguisticstylesprevalentinthePersian
Additionally,LLMsoftencomewiththedraw-
language. Inourcomprehensiveevaluation
back of slower response times and increased la-
ofFaBERTon12datasetsinvariousdown-
tencycomparedtosmallermodels. Moreover,the stream tasks, encompassing Sentiment
use of LLMs typically demands advanced hard-
Analysis(SA),NamedEntityRecognition
ware, creating accessibility challenges for many
(NER),NaturalLanguageInference(NLI),
users. Privacyconcernsmayalsoemergewhenem-
Question Answering (QA), and Question
ployingLLMsonline. Ontheotherhand,smaller
Paraphrasing(QP),itconsistentlydemon-
LMs are more suitable for use in local standard
stratedimprovedperformance,allachieved
computersandsettingswithlimitedcomputingca-
within a compact model size. The find-
pabilities,giventheircompactdesign.
ings highlight the importance of utilizing
Our motivation is to develop FaBERT, a Per-
diverseandcleanedcorpora,suchasHm-
sian BERT-base model, to enhance performance
Blogs,toenhancetheperformanceoflan-
intraditionalNLUtasksandenableefficientpro-
guagemodelslikeBERTinPersianNatural
cessing of both formal and informal texts in the
Language Processing (NLP) applications.
FaBERT is openly accessible at https: language. WhileexistingPersianLMsexhibitcom-
//huggingface.co/sbunlp/fabert. mendablecapabilities,thereremainsroomforim-
provement,especiallyinhandlingthecomplexities
1 Introduction
ofPersianinformaltexts. InformalPersianinreal-
Inrecenttimes,we’veseentheriseofsophisticated worldcommunicationhasitsuniquefeatureslike
languagemodelslikeBERT(Devlinetal.,2018), flexiblesentencestructures,culturalreferences,in-
transforming the understanding of languages, in- formal lexicon, and slang. FaBERT is designed
cluding Persian. Whether designed for multiple to tackle these potential challenges and improve
languagesorspecificallyforPersian,thesemodels overallperformance.
havebeenemployedacrossvariousapplicationsin Ourfindingsrevealthatthecleanedcorpusfrom
PersianNaturalLanguageProcessing(NLP).Their Persianblogsenhancesthemodel’sperformance,
training encompassed a diverse range of textual leading to state-of-the-art results across various
sources,includingwebsiteslikeWikipediaandso- downstreamtasks. Themaincontributionsofthis
cial media platforms such as Twitter, as well as paperare:
newsarticlesandacademicjournals.
Morerecently,LargeLanguageModels(LLMs) 1. Pre-training a BERT-base model on Persian
withasubstantialincreaseinparameters,havesig- blogtextsintheHmBlogscorpus,andmaking
nificantlyreshapedthelandscapeofNLP,excelling itpubliclyaccessible.
4202
beF
9
]LC.sc[
1v71660.2042:viXra2. Evaluating the model’s performance on 12 versetrainingdataset,exceeding32gigabytes,in-
datasetsinvariousdownstreamtasks,includ- cludesconversational,formal,andhybridtexts.
ing sentiment analysis, irony detection, nat-
urallanguageinference, questionparaphras- Additionally,manyMultilingualLanguageMod-
ing, named entity recognition, and question els have been released since, and few of them in-
answering. cludePersian. MultilingualBERT,alsoknownas
mBERT, was introduced by (Devlin et al., 2018).
Thesubsequentsectionsofthepaperarestructured It was trained with NSP and MLM tasks on the
asfollows: Section2providesanintroductionand Wikipedia pages of 104 languages with a shared
comparison of various BERT models employed word-piece vocabulary. mBERT has shown im-
forPersianNLP.Section3delvesintothedetails pressivezero-shotcross-lingualtransferandisef-
of our corpus, model, and its pre-training proce- fectiveinutilizingtask-specificannotationsfrom
dure. Section4comparesFaBERT’sperformance onelanguageforfine-tuningandevaluationinan-
in downstream tasks with other models. Finally, other. Although mBERT has shown solid per-
Section5concludesthepaperbysummarizingour formanceacrossdifferentlanguages,monolingual
findings. BERTmodelsoutperformmBERTinmostdown-
streamtasks.
2 RelatedWorks
Similarly, XLM-R (Conneau et al., 2019), an
BERT,whichstandsforBidirectionalEncoderRep-
extensionoftheRoBERTamodelbyFacebookAI,
resentationsfromTransformers,hasdemonstrated
isdesignedforcross-lingualunderstanding. This
itsexceptionalabilitiesacrossawiderangeofnat-
modelwaspre-trainedwiththeMLMobjectiveon
ural language understanding tasks. Unlike tradi-
avastcorpuscomprisingmorethan2terabytesof
tional language models that process text in a uni-
textfrom100languagesandoutperformedmBERT
directional manner (left-to-right or right-to-left),
inmanydownstreamtasks.
BERTconsidersboththeleftandrightcontextof
words.
The models previously reviewed adhere to the
BERT’s pre-training involved two training ob- architectureintroducedbytheoriginalBERT-base
jectives: MaskedLanguageModeling(MLM)and model,featuring12layersand12attentionheads.
NextSentencePrediction(NSP).MLMrandomly Whilemaintainingthisconsistency,therearevaria-
masks words in a sentence, and the model learns tionsinvocabularysizeamongthesemodels.
topredictthemissingwordsbasedoncontext,en-
hancingitsabilitytograspthesemanticmeaning A larger vocabulary facilitates the capture of
andrelationshipsbetweenwordswithinsentences. more unique tokens and their relationships, but
Ontheotherhand,intheNSPtask,themodelhas it comes at the expense of increased parameters.
to predict whether sentence B logically succeeds This,inturn,necessitatesmoreextensivetraining
sentenceA. dataforlearningembeddings. Conversely,smaller
MLM and NSP are designed for the model to vocabulariesmaystruggletocaptureallthedetails
learnalanguagerepresentation,whichcanthenbe of language, potentially causing information and
usedtoextractfeaturesfordownstreamtasks. Con- contexttobelost.
tinuingthediscussion,wewillpresentaselection
ofPersian-languageBERTmodels. Aninstanceisfoundinthemultilingualmodel
Themostwell-knownPersianlanguagemodel mBERT, which supports 100 different languages
is ParsBERT (Farahani et al., 2021). It was pre- with a vocabulary size of only 100,000. Despite
trained using both MLM and NSP tasks, utiliz- thebroadlanguagecoverage,thischoiceleadsto
ing a training corpus collected from 8 different a limited set oftokens foreach language. Conse-
sources. ParsBERT has become the preferred quently, sentences are transformed into a greater
choice for Persian NLP tasks, thanks to its out- numberoftokens,potentiallyexceedingthemax-
standing performance. Ariabert (Ghafouri et al., imumsupportedsequencelengthandresultingin
2023)isanotherPersianlanguagemodelthatfol- the loss of information. Table 1 summarizes the
lowsRoBERTa’senhancements(Liuetal.,2019) vocabularysizeandnumberofparametersforeach
andutilizesByte-PairEncodingtokenizer. It’sdi- modelunderconsideration.Model VocabularySize(K) #ofParameters(M) Hyperparameter Value Hyperparameter Value
BatchSize 32 TotalSteps 18Million
BERT(English) 30 109
Optimizer Adam WarmupSteps 1.8Million
mBERT 105 167
LearningRate 6e-5 PrecisionFormat TF32
XLM-R 250 278
WeightDecay 0.01 Dropout 0.1
ParsBERT 100 162
AriaBERT 60 132
Table2: Pre-trainingHyperparameters
Table 1: Vocabulary Size and Parameter Count of Persian
BERTModels
self-attentionheads.
WeoptedfortheWordPiecetokenizeroveralter-
3 Methodology
nativessuchasBPE,aspriorevidenceindicatesno
performanceimprovement(GeipingandGoldstein,
3.1 TrainingCorpus
2023),andwithaconservativestance,wesetthevo-
Theselectionofanappropriatetrainingcorpusis
cabularysizeto50,000tokens. Thisdecisionaimed
apivotalelementinthepre-trainingofalanguage
at finding a balance between capturing linguistic
model. For this effort, we utilized the HmBlogs
detailsandmanagingthecomputationaldemands
corpus(KhansariandShamsfard,2021),acollec-
associatedwithlargervocabularies. It’sessential
tion of 20 million posts of Persian blogs over 15
tonotethatPersiantextincludeshalfspaces,afea-
years. HmBlogsincludesmorethan6.8billionto-
tureabsentinEnglish. Consequently,theFaBERT
kens,coveringawiderangeoftopics,genres,and
tokenizerhasbeenadaptedtohandlethisfeature,
writingstyles,includingbothformalandinformal
ensuringappropriaterepresentationoftextsduring
textstogether.
pre-trainingandfine-tuning.
Toensurehigh-qualitypre-training,aseriesof
The total number of parameters for FaBERT
pre-processingstepswereperformedonthecorpus.
is 124 million. In comparison to other Persian
Many posts written in the Persian alphabet were
andmultilingualbasemodelsoutlinedinTable1,
erroneouslyidentifiedasPersiandespitenotbeing
FaBERTismorecompactwithfewerparameters.
inthePersianlanguage. Thisconfusionarisesfrom
Duringpre-training,eachinputconsistedofone
the Persian alphabet’s resemblance to the alpha-
ormoresentencessampledcontiguouslyfromasin-
bets of other languages like Arabic and Kurdish.
gledocument. Thesampleswereofvaryinglengths
Additionally,someotherpostshadtypographical
to help the model effectively learn the positional
errors,veryrarewords,ortheexcessiveuseoflo-
encodings.
cal dialects. Therefore, a post-discriminator was
Weimplementeddynamicmasking,inspiredby
implementedtofilterouttheseimproperandnoisy
the methodology introduced in (Liu et al., 2019),
posts. Cleaning documents in Persian poses an-
andomittedtheNextSentencePredictiontaskfrom
otherchallengeduetothepresenceofnon-standard
ourpre-trainingprocess,asitwasdemonstratedto
characters1. ThesecharacterslookidenticaltoPer-
havenodiscerniblepositiveimpactonperformance.
siancharacters,buttheirdifferentcodescancause
Themaskingratefordynamicmaskingwassetto
problemsduringpre-training. SomePersianblogs
15%. We also utilized the whole word masking
mayalsousedecorativecharacterstomakethetext
approach for enhanced performance. Unlike tra-
visuallyappealing. Suchcharacterswerestandard-
ditionalMLM,whichrandomlymasksindividual
izedtoensureuniformrepresentationandavoidpo-
tokensinasentence,wholewordmaskinginvolves
tentialdiscrepancies. Additionally,numberswere
maskingentirewords. Table2detailsthehyperpa-
replacedwithadefaultvalue,andwordswithrepet-
rametersusedinthepre-trainingprocess.
itivecharacterswerecorrected.
ThetrainingwasconductedonasingleNvidia
A10040GBGPU,spanningadurationof400hours.
3.2 Pre-trainingProcedure
Thefinalvalidationperplexityachievedwas7.76,
WetrainedaBERT-basemodelsimilartothatpro-
andthetrainandvalidationlossplotispresented
posed by (Devlin et al., 2018). Our BERT-base
inFigure1.
model, FaBERT, replicates the original architec-
ture with 12 hidden layers, each comprising 12 4 ExperimentsandResults
(cid:248) (cid:188)
1Forinstance,Arabic’(cid:10)’and’ ’areoccasionallysubsti- Inthissection,weassesstheFaBERTmodelacross
(cid:248) (cid:184)
tutedforPersian’ ’and’ ’. fourdifferentcategoriesofdownstreamtasks. ForSBU-NLI
SBU-NLI is another dataset containing sentence
pairs categorized into three labels: Entailment,
Contradiction,andNeutral. Thisdataisgathered
fromvarioussourcestocreateabalanceddataset.
ParsiNLUQuestionParaphrasing
Thistaskinvolvesdeterminingtherelationshipbe-
tween pairs of questions, specifically classifying
whether they are paraphrases. The dataset is cre-
atedthroughtwomeans: first,byminingquestions
fromGoogleauto-completeandPersiandiscussion
Figure1:TrainandValidationMLMlossinpre-training
forums,andsecond,bytranslatingtheQQPdataset
withGoogleTranslateAPI.Asaresult,someques-
NLIandQuestionParaphrasing,sentencepairsare tionsarepresentedinaninformalfashion.
processedtogeneratelabelsbasedontheirrelation-
ship. InNER,entitieswithinsingleinputsentences Model FarsTail SBU-NLI Parsi-NLUQP
arelabeledatthetokenlevel. SentimentAnalysis
ParsBERT 82.52 58.41 77.60
andIronyDetectioninvolveprocessingindividual
mBERT 83.42 66.38 79.48
sentencesandassigningcorrespondinglabels. In
XLM-R 83.50 58.85 79.74
QuestionAnswering,modelsutilizeagivenques- AriaBERT 76.39 52.81 78.86
tionandtheprovidedparagraphtogeneratetoken-
FaBERT 84.45 66.65 82.62
level spans for answers. Lastly, we analyze the
efficiencyofFaBERT’stokenizerandcompareit
Table3: PerformanceComparisoninNLIandQuestion
withotherBERTmodels. Paraphrasing
Infine-tuningeachdataset,agridsearchisem-
ployed, utilizing train/validation/test splits. The AsobservedinTable3,FaBERTdemonstrates
reported scores correspond to the test set and are a+1%improvementinF1forFarsTail, compara-
based on hyperparameters that yield the best val- ble performance to mBERT in SBU-NLI, and a
idation scores. The scope of the grid search and +2.88%F1scoreintheinformalParsiNLUQues-
thesplitsizesforeachdatasetcanbefoundinAp- tionParaphrasingdataset.
pendixA.
4.2 NamedEntityRecognition
4.1 NaturalLanguageInferenceandQuestion
Inthissection,weassesstheefficacyofFaBERTin
Paraphrasing
NER,acommonlyemployedintermediatetaskthat
In this section, we analyze FaBERT’s ability to facilitatesinformationextractionandentityidenti-
understandlogicalandsemanticrelationshipsbe- ficationwithintextualdata. Ourassessmentlever-
tween sentences, focusing on tasks like Natural agedformalandinformaldatasets,includingParsT-
NLI and Question Paraphrasing. We assess its wiNER(Aghajanietal.,2021),PEYMA(Shahsha-
performance using the Farstail (Amirkhani et al., hani et al., 2018), and MultiCoNER v2 (Fetahu
2023),SBU-NLI(RahimiandShamsFard,2024), etal.,2023). Thecomparisonofdifferentmodels
and ParsiNLU Question Paraphrasing (Khashabi foreachentitytypeisdetailedinAppendixB.
etal.,2021)datasets.
ParsTwiNER
FarsTail
The ParsTwiNER offers a NER dataset gathered
TheFarsTailNLIdatasetissourcedfrommultiple- from7632tweetscollectedfromthePersianTwit-
choicequestionsfromvarioussubjects,specifically teraccounts,offeringdiverseinformalPersiancon-
collectedfromIranianuniversityexams. Eachof tent. Annotation by experts in natural language
these questions became the basis for generating processingresultedin24061namedentitiesacross
NLI instances with three different relationships: categoriessuchaspersons,organizations,locations,
Entailment,Contradiction,andNeutral. events,groups,andnations.PEYMA 2020)datasetsforevaluation.
The PEYMA NER dataset, derived from formal
DeepSentiPers
text extracted from ten news websites, classifies
The DeepSentiPers dataset comprises 9,000 cus-
wordsintodifferentcategories,encompassingper-
tomerreviewsofDigikala,anIranianE-commerce
sons,locations,organizations,time,date,andmore.
platform. Originally, each sentence’s polar-
PEYMAisknownasakeyassetfortrainingand
ity was annotated using a 5-class label set
evaluatingNERsystemsinthePersianlanguage.
E = {−2,−1,0,+1,+2}, representing senti-
MultiCoNERv2 mentsfromverydispleasedtodelighted. However,
ourinvestigationrevealedinconsistencies,particu-
Initially introduced as a part of SemEval task in
larlybetweenthe-1and-2categoriesfornegative
2022,MultiCoNERisamultilingualNERdataset
sentimentsandthe+1and+2categoriesforpositive
craftedtoaddresscontemporarychallengesinNER,
sentiments. Recognizingtheoverlapbetweenthese
suchaslow-contextscenarios,syntacticallycom-
closely related labels, we opted for a simplified
plexentitieslikemovietitles,andlong-tailentity
3-class labeling approach, classifying sentiments
distributions. Theenhancedversionofthisdataset
asnegative,neutral,orpositive.
was used in the following year as part of the Se-
mEval 2023 task. This version, known as Multi-
MirasOpinion
CoNERv2,expandedthesechallengesbyadding
MirasOpinion, the largest Persian Sentiment
fine-grainedentitiesandinsertingnoiseintheinput
dataset, comprises93,000reviewsgatheredfrom
text. GatheredfromWikidataandWikipedia,the
the Digikala platform. Through crowdsourcing,
datasetspans12languages,withPersianbeingthe
each review was labeled as Positive, Neutral, or
focusofourevaluations.
Negative. ThisdatasetwasincludedintheSPAR-
ROW, a benchmark for sociopragmatic meaning
Model ParsTwiner PEYMA MultiCoNERv2
understanding. Participating in the SPARROW
ParsBERT 81.13 91.24 58.09
benchmark (Zhang et al., 2023) allowed us to as-
mBERT 75.60 87.84 51.04
XLM-R 79.50 90.91 51.47 sessFaBERTagainstvariouslanguagemodels.
AriaBERT 78.53 89.76 54.00
MirasIrony
FaBERT 82.22 91.39 57.92
MirasIrony,a2-labeleddatasetdesignedforirony
Table 4: Performance Comparison in Named Entity detection, encompasses 4,339 manually labeled
Recognition Persiantweets. Inthisdataset,tweetsexhibitinga
disparity between their literal meaning and senti-
The evaluation metrics used include micro-F1 mentwerelabeledaspositive,whilethoselacking
forPEYMAandParsTwiNERdatasets,andmacro- thischaracteristicwerelabeledasnegative. Similar
F1 for MultiCoNER v2. Table 4 provides a de- toMirasOpinion,weassessedtheperformanceof
tailedoverviewofscoresachievedbyeachmodel. modelsonMirasIronyusingtheSPARROWbench-
Acrosstheboard,allmodelsdemonstratedcompa- mark.
rable performance in the PEYMA dataset. How-
Model DeepSentiPers MirasOpinion MirasIrony
ever, FaBERT model exhibited a slight improve-
mentbyachievinga+1.09%increaseinF1score ParsBERT 74.94 86.73 71.08
mBERT 72.95 84.40 74.48
fortheinformalParsTwiNERdataset. IntheMulti-
XLM-R 79.00 84.92 75.51
CoNERv2dataset,bothFaBERTandParsBERT
AriaBERT 75.09 85.56 73.80
outperformed other models. In general FaBERT
FaBERT 79.85 87.51 74.82
andParsBERTseemtobegreatoptionsforappli-
cationsinvolvingNER. Table5: PerformanceComparisoninSentimentAnaly-
sisandIronyDetection
4.3 SentimentAnalysisandIronyDetection
Inthissection,weassessFaBERT’sperformance Macro averaged F1 score serves as the evalua-
inclassifyingexpressions. WeemployedDeepSen- tionmetricforDeepSentiPersandMirasOpinion,
tiPers(Sharamietal.,2020),MirasOpinion(Asli while Accuracy is employed for MirasIrony. As
et al., 2020), and MirasIrony (Golazizian et al., presentedinTable5, FaBERTachievedthehigh-estscoresinsentimentanalysisforbothDeepSen- from Persian Wikipedia articles. The questions
tiPers and MirasOpinion. For irony detection in and their corresponding answers were generated
theMirasIronydataset,XLM-Routperformsother through a crowdsourcing process, where crowd-
models,securingtheleadingpositionwithascore workerswerepresentedwithpassagesandtasked
of 75.51%. FaBERT demonstrated notable per- withcraftingquestionsandcorrespondinganswers
formance as well, securing the second spot with based on the provided content. Inspired by the
74.8%accuracy. ThroughtheSPARROWbench- structure of SQuAD 2.0 (Rajpurkar et al., 2018),
markleaderboard,othermodelscanbecompared PQuAD designates 25% of its questions as unan-
withFaBERTonMirasOpinion2 andMirasIrony3 swerable, adding extra complexity to the dataset
tasks. andenhancingtheevaluativechallenge.
Inthisdataset,inadditiontoF1andEMscores,
4.4 QuestionAnswering
theevaluationcanbebrokendownintosubsetsof
To evaluate the question-answering capabilities
questions that have answers (HasAns) and those
of FaBERT, our experiments encompassed three
thatdonothaveanswers(NoAns). Byconsidering
datasets: ParsiNLU Reading Comprehension
thesemetrics,theperformanceofdifferentmodels
(Khashabi et al., 2021), PQuad (Darvishi et al.,
canbecomparedandanalyzedtodeterminetheir
2023), and PCoQA (Hemati et al., 2023). Each
effectivenessinansweringquestionsorabstaining
dataset is briefly introduced in the following sec-
fromanswering. Theauthorsalsoprovidedanesti-
tions. Table6summarizestheperformanceofdif-
mationofhumanperformancebyaskingagroup
ferentmodelsoneachdataset.
ofcrowdworkerstoanswerasubsetofquestions.
Both FaBERT and XLM-R demonstrate remark-
ParsiNLUReadingComprehensionDataset
ablecapabilitiesinquestionanswering,achievinga
ReadingComprehensionisoneofthetasksintro-
comparableF1scoreperformance. However,XLM-
duced in the ParsiNLU benchmark and involves
RslightlyoutperformsFaBERTinthisaspect.
extracting a substring from a given context para-
graph to answer a specific question. In order to
PCoQA:PersianConversationalQuestion
create this dataset, they used Google’s Autocom-
AnsweringDataset
plete API to mine questions deemed popular by
users. Starting with a seed set of questions, they PCoQA is the first dataset designed for answer-
repeatedlyqueriedpreviousquestionstoexpandon ing conversational questions in Persian. It com-
thesetandaddmoresophisticatedones. Afterfil- prises870dialogsandover9,000question-answer
teringoutinvalidquestions,nativeannotatorsthen pairssourcedfromWikipediaarticles. Inthistask,
chose the pertinent text span from relevant para- contextuallyconnectedquestionsareposedabout
graphsthatprovidedtheanswertoeachquestion. a given document, and models are required to
Theevaluationofmodelsonthisdatasetinvolves respond by extracting relevant information from
comparingtheanswersgeneratedbythemodelsto givenparagraphs. Thisdatasetprovidesasuitable
theprovidedgroundtruthanswers. Themainmet- contextforassessingthemodel’sperformancein
ricsusedaretheF1score,whichmeasurestheover- Persianconversationalquestionanswering,similar
lapbetweenthepredictedandgroundtruthanswers, totheEnglishdatasetCoQA(Reddyetal.,2019).
andtheexactmatch(EM)score,whichchecksif
ForthePCoQAdataset,inadditiontoF1andEM
the predicted answers exactly match the ground
scores, two variants of human equivalence score
truthanswers. FaBERTscored+6.24%higherinF1
(HEQ)aresuggestedbytheauthors. HEQ-Qmea-
comparedtoothermodelsintheParsiNLURead-
sures the percentage of questions for which sys-
ingComprehensiontask.
temF1exceedsormatcheshumanF1,andHEQ-
PQuAD:APersianquestionansweringdataset Mquantifiesthenumberofdialogsforwhichthe
modelachievesabetteroverallperformancecom-
PQuADisalarge-scale,human-annotatedquestion-
paredtothehuman. FaBERToutperformedother
answeringdatasetforthePersianlanguage. Itcon-
models with +2.55% higher F1 score, handling
tains80,000questionsbasedonpassagesextracted
bothanswerableandunanswerablequestionswell.
2https://sparrow.dlnlp.ai/
Additionally,thePCoQAdatasetprovestobechal-
sentiment-2020-ashrafi-fas.taskshow
lenging,withallmodelsscoringnoticeablylower
3https://sparrow.dlnlp.ai/
irony-2020-golazizian-fas.taskshow thanhumans.ParsiNLU PQuAD PCoQA
Model
ExactMatch F1 ExactMatch F1 HasAnsEM HasAnsF1 NoAns ExactMatch F1 HEQ-Q HEQ-M NoAns
ParsBERT 22.10 44.89 74.41 86.89 68.97 85.34 91.79 31.17 50.96 41.07 0.81 48.83
mBERT 26.31 49.63 73.68 86.71 67.52 84.66 93.26 26.89 46.11 36.94 1.63 31.62
XLM-R 21.92 42.55 75.16 87.60 69.79 86.13 92.26 34.52 51.12 44.81 0.81 54.88
AriaBERT 16.49 37.98 69.70 82.71 63.61 80.71 89.08 22.68 41.37 32.89 0 40.93
FaBERT 33.33 55.87 75.04 87.34 70.33 86.50 90.02 35.85 53.51 45.36 2.45 61.39
Human - - 80.3 88.3 74.9 85.6 96.80 85.5 86.97 - - -
Table6: PerformanceComparisoninQuestionAnswering
4.5 VocabularyImpactonInputLength Persian blogs. Notably, our model’s smaller vo-
cabulary size resulted in a more compact overall
ToevaluatetheimpactofFaBERT’schosenvocab-
sizecomparedtocompetitors. FaBERTperformed
ularysizeonitseffectivemaximuminputlength,a
exceptionallywellin12differentdatasets,outper-
comparativeanalysiswasconductedacrossdatasets
formingcompetitorsinnineofthem. Intheremain-
with longer sentences, including MirasOpinion,
ingtaskswhereitdidnotsecurethetopposition,
FarsTail,ParsiNLUReadingComprehension,and
it consistently ranked among the top performers,
PQuAD. The objective was to examine how dif-
closely following the highest-performing model.
ferent tokenizers, including the one trained for
Our results indicate that clean texts with diverse
FaBERT, influence the number of tokens in each
writingstyles,bothformalandinformal,foundin
inputsentence.
Persian blogs can significantly contribute to the
Table 7 provides a summary of median token
high-quality pre-training of language models, in-
counts across the aforementioned datasets. Both
cludingBERT.TheeffectivenessoftheHmblogs
multilingual models faced challenges due to the
corpusintheperformanceofourBERTmodelin
lack of sufficient Persian tokens in their vocabu-
downstream tasks demonstrates its potential for
laries,potentiallyimpactingtheirperformanceon
being used in pre-training both language models
longer inputs due to loss of information. Pars-
andlargelanguagemodelsalongsideotherrelevant
BERT’s tokenizer yields the most compact se-
Persiancorpora.
quences, closely followed by FaBERT. An inter-
esting observation arises in the PQuAD dataset,
whereParsBERToutperforms,likelyattributedto
References
PQuAD’srelianceonWikipedia,asignificantcom-
ponentofParsBERT’spre-trainingdata. MohammadMahdi Aghajani, AliAkbar Badri, and
Hamid Beigy. 2021. Parstwiner: A corpus for
Overall, FaBERT’stokenizer, despitehavinga
named entity recognition at informal persian. In
vocabulary size half that of ParsBERT, demon- ProceedingsoftheSeventhWorkshoponNoisyUser-
strated a comparable level of compression. The generatedText(W-NUT2021),pages131–136.
detailedboxplotsforeachdatasetareavailablein
HosseinAmirkhani,MohammadAzariJafari,Soroush
AppendixC.
Faridan-Jahromi, Zeinab Kouhkan, Zohreh Pourja-
fari,andAzadehAmirak.2023. Farstail: Apersian
Tokenizer MirasOpinion FarsTail ParsiNLURC PQuAD naturallanguageinferencedataset. SoftComputing,
ParsBERT 27 58 113.5 160 pages1–13.
mBERT 44 85 165 235
XLM-R 34 74 142.5 210 SeyedAradAshrafiAsli,BehnamSabeti,ZahraMajd-
AriaBERT 28 66 130 207 abadi,PreniGolazizian,RezaFahmi,andOmidMo-
FaBERT 28 62 119.5 189 menzadeh.2020. Optimizingannotationeffortusing
activelearningstrategies: Asentimentanalysiscase
Table 7: Median Token Count Yielded by Different studyinpersian. InProceedingsoftheTwelfthLan-
Tokenizers guageResourcesandEvaluationConference,pages
2855–2861.
AlexisConneau,KartikayKhandelwal,NamanGoyal,
5 Conclusion
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
Inthispaper,wepre-trainedFaBERT,aBERT-base
moyer,andVeselinStoyanov.2019. Unsupervised
modelfromscratchexclusivelyonthecleanedHm-
cross-lingualrepresentationlearningatscale. arXiv
Blogscorpus,consistingsolelyofrawtextsfrom preprintarXiv:1911.02116.KasraDarvishi,NewshaShahbodaghkhan,ZahraAb- Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
basiantaeb, and Saeedeh Momtazi. 2023. Pquad: Knowwhatyoudon’tknow:Unanswerablequestions
A persian question answering dataset. Computer forsquad. arXivpreprintarXiv:1806.03822.
Speech&Language,80:101486.
SivaReddy,DanqiChen,andChristopherDManning.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and 2019. Coqa: Aconversationalquestionanswering
KristinaToutanova.2018. Bert: Pre-trainingofdeep challenge. TransactionsoftheAssociationforCom-
bidirectionaltransformersforlanguageunderstand- putationalLinguistics,7:249–266.
ing. arXivpreprintarXiv:1810.04805.
Mahsa Sadat Shahshahani, Mahdi Mohseni, Azadeh
MehrdadFarahani,MohammadGharachorloo,Marzieh Shakery,andHeshaamFaili.2018. Peyma: Atagged
Farahani, and Mohammad Manthouri. 2021. Pars- corpus for persian named entities. arXiv preprint
bert: Transformer-basedmodelforpersianlanguage arXiv:1801.09936.
understanding. NeuralProcessingLetters,53:3831–
3847. Javad PourMostafa Roshan Sharami, Parsa Abbasi
Sarabestani,andSeyedAbolghasemMirroshandel.
Besnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg
2020. Deepsentipers: Noveldeeplearningmodels
Rokhlenko, and Shervin Malmasi. 2023. Multi-
trainedoverproposedaugmentedpersiansentiment
conerv2:alargemultilingualdatasetforfine-grained
corpus. arXivpreprintarXiv:2004.05328.
andnoisynamedentityrecognition. arXivpreprint
arXiv:2310.13213. JingfengYang, HongyeJin, RuixiangTang, Xiaotian
Han,QizhangFeng,HaomingJiang,BingYin,and
JonasGeipingandTomGoldstein.2023. Cramming:
XiaHu.2023. Harnessingthepowerofllmsinprac-
Trainingalanguagemodelonasinglegpuinoneday.
tice: Asurveyonchatgptandbeyond. arXivpreprint
InInternationalConferenceonMachineLearning,
arXiv:2304.13712.
pages11117–11143.PMLR.
Chiyu Zhang, Khai Duy Doan, Qisheng Liao, and
ArashGhafouri,MohammadAminAbbasi,andHassan
MuhammadAbdul-Mageed.2023. Theskippedbeat:
Naderi.2023. Ariabert: Apre-trainedpersianbert
Astudyofsociopragmaticunderstandinginllmsfor
modelfornaturallanguageunderstanding.
64languages. arXivpreprintarXiv:2310.14557.
PreniGolazizian,BehnamSabeti,SeyedAradAshrafi
Asli, Zahra Majdabadi, Omid Momenzadeh, and
Reza Fahmi. 2020. Irony detection in persian lan-
guage: Atransferlearningapproachusingemojipre-
diction. In Proceedings of the Twelfth Language
ResourcesandEvaluationConference,pages2839–
2845.
Hamed Hematian Hemati, Atousa Toghyani, Atena
Souri,SayedHesamAlavian,HosseinSameti,and
Hamid Beigy. 2023. Pcoqa: Persian conversa-
tional question answering dataset. arXiv preprint
arXiv:2312.04362.
HamzehMotahariKhansariandMehrnoushShamsfard.
2021. Hmblogs: Abiggeneralpersiancorpus. arXiv
preprintarXiv:2111.02362.
DanielKhashabi,ArmanCohan,SiamakShakeri,Pe-
dramHosseini,PouyaPezeshkpour,MaliheAlikhani,
MoinAminnaseri,MarziehBitaab,FaezeBrahman,
SarikGhazarian,etal.2021. Parsinlu: asuiteoflan-
guageunderstandingchallengesforpersian. Transac-
tionsoftheAssociationforComputationalLinguis-
tics,9:1147–1162.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXivpreprintarXiv:1907.11692.
Zeinab Rahimi and Mehrnoush ShamsFard. 2024. A
knowledge-basedapproachforrecognizingtextual
entailmentswithafocusoncausalityandcontradic-
tion. AvailableatSSRN4526759.AppendixFor"FaBERT:Pre-training
BERTonPersianBlogs"
A Fine-tuningHyperparameters
Thehyperparametersemployedforfine-tuningthe
modelsoneachdataset,alongwiththerespective
train/validation/testsplitsizes,areoutlinedinTable
8. For the ParsiNLU benchmark, we adhered to
the predefined hyperparameters in the ParsiNLU
sourcecode.
B DetailedNERResults
Tables9,10,and11presentF1scoresforentities
in PEYMA, MultiCoNER v2, and ParsTwiNER
datasets, providing a model comparison for each
Figure3:Tokencountdistributionacrossmodeltokeniz-
entity. Forinstance,InMultiCoNERv2,FaBERT
ersfortheParsiNLUReadingComprehensiondataset
excels in recognizing medical entities, and Pars-
BERTisbetteratidentifyingcreativeworks.
C TokenizerComparisonFigures
Figures 2, 3, 4, and 5 illustrate the distribution
oftokencountsforeachmodel’stokenizeracross
thefollowingdatasets: PQuAD,ParsiNLURead-
ing Comprehension, MirasOpinion, and FarsTail.
Theseboxplotsprovideavisualrepresentationof
thevariationintokencountsforeachmodel.
Figure4: Tokencountdistributionacrosstokenizersfor
theMirasOpiniondataset
Figure2: Tokencountdistributionacrosstokenizersfor
thePQuADdataset
Figure5: Tokencountdistributionacrosstokenizersfor
theFarsTaildatasetDatasets Train Validation Test NumberofLabels Metrics LearningRate BatchSize Epochs Warmup
DeepSentiPers 6320 703 1854 3 MacroF1 2e-5,3e-5,5e-5 8,16 3,7 0,0.2
MirasOpinion 75094 9387 9387 3 MacroF1 2e-5,3e-5,5e-5 8,16 1 0,0.2
MirasIrony 2352 295 294 2 Accuracy 2e-5,3e-5,5e-5 8,16 3,5 0,0.2
PQuAD 63994 7976 8002 - MicroF1 2e-5,3e-5,5e-5 8,16 2 0,0.2
PCoQA 6319 1354 1354 - MicroF1 3e-5,5e-5 8,16 3,7 0,0.2
ParsiNLURC 600 125 575 - MicroF1 3e-5,5e-5 4 3,7 0
SBU-NLI 3248 361 401 3 MicroF1 2e-5,3e-5,5e-5 8,16 3,7 0,0.2
FarsTail 7266 1564 1537 3 MicroF1 2e-5,3e-5,5e-5 8,16 3,7 0,0.2
ParsiNLUQP 1830 898 1916 2 MicroF1 3e-5,5e-5 8,16 3,7 0
PEYMA 8029 926 1027 - MacroF1 2e-5,3e-5,5e-5 8,16 3,7 0,0.2
MultiCoNERv2 16321 855 219168 - MicroF1 2e-5,3e-5,5e-5 8,16 3,7 0,0.2
ParsTwiNER 6418 447 304 - MicroF1 2e-5,3e-5,5e-5 8,16 3,7 0,0.2
Table8: DatasetSplitSizesandFine-TuningHyperparameters
EntityType FaBERT ParsBERT AriaBERT mBERT XLM-R Support
Date 89.16 85.65 85.11 84.56 86.73 208
Location 91.95 91.73 91.46 90.25 92.42 595
Currency 94.34 94.34 83.64 90.57 96.15 26
Organization 88.24 89.37 86.38 84.83 87.25 667
Percent 98.63 98.63 93.33 97.14 94.74 36
Person 95.45 95.29 94.6 90.1 95.75 434
Time 96.97 91.43 96.97 76.47 94.12 16
MicroAverage 91.39 91.24 89.76 87.84 90.91 1982
MacroAverage 93.53 92.35 90.21 87.7 92.45 1982
WeightedAverage 91.37 91.23 89.75 87.81 90.92 1982
Table9: ComparisonofF1ScoresforEachEntityTypeinPEYMA
EntityType FaBERT ParsBERT AriaBERT mBERT XLM-R Support
Event 0.5714 0.4444 0.4118 0.4865 0.2308 14
Location 0.8281 0.8414 0.7991 0.7802 0.8088 221
Nation 0.9 0.7385 0.7246 0.7123 0.7397 30
Organization 0.7364 0.6966 0.6691 0.6462 0.7126 129
Person 0.9344 0.8893 0.8745 0.8216 0.8629 244
PoliticalGroup 0.6364 0.6667 0.7442 0.7 0.8 22
MicroAverage 0.8222 0.8113 0.7853 0.756 0.795 660
MacroAverage 0.7301 0.7128 0.7039 0.6911 0.6925 660
WeightedAverage 0.8238 0.8119 0.7881 0.7573 0.7943 660
Table10: ComparisonofF1ScoresforEachEntityTypeinParsTwiNEREntityType FaBERT ParsBERT AriaBERT mBERT XLM-R Support
AerospaceManufacturer 0.7325 0.7127 0.7196 0.6269 0.638 1030
ORG 0.5809 0.5832 0.5348 0.5479 0.5325 18532
MusicalGRP 0.6282 0.6597 0.59 0.613 0.5954 4668
PrivateCorp 0.3822 0.4033 0.3851 0.2605 0.1749 148
CarManufacturer 0.6511 0.7031 0.6631 0.6291 0.6147 2085
PublicCorp 0.6109 0.6377 0.5819 0.5439 0.562 5926
SportsGRP 0.8159 0.8174 0.8012 0.8046 0.7949 6418
Medication/Vaccine 0.7067 0.6837 0.6342 0.6324 0.6582 4405
MedicalProcedure 0.6307 0.5965 0.5592 0.4904 0.5471 2132
AnatomicalStructure 0.6079 0.5827 0.5151 0.4824 0.4978 3940
Symptom 0.5656 0.5368 0.4671 0.4217 0.4109 821
Disease 0.646 0.6256 0.5737 0.5264 0.5652 3989
Artist 0.7384 0.7347 0.6936 0.7122 0.7155 51617
Politician 0.5786 0.6056 0.534 0.5213 0.5141 19760
Scientist 0.3328 0.3669 0.2952 0.2615 0.2625 3278
SportsManager 0.606 0.6232 0.5376 0.4332 0.4494 3009
Athlete 0.5796 0.5992 0.5356 0.5119 0.5357 12551
Cleric 0.5707 0.5535 0.4875 0.4627 0.4332 4526
OtherPER 0.4254 0.4225 0.3544 0.3647 0.3449 21127
Clothing 0.3912 0.3375 0.3293 0.2054 0.2716 239
Drink 0.5244 0.5683 0.5483 0.4646 0.5041 631
Food 0.6063 0.5971 0.574 0.4788 0.5591 3580
Vehicle 0.5388 0.5388 0.5171 0.4659 0.4952 2865
OtherPROD 0.5851 0.5843 0.5453 0.5109 0.5233 10897
ArtWork 0.0919 0.1085 0.1057 0.1077 0.0691 100
WrittenWork 0.5561 0.5541 0.5028 0.5006 0.5079 13530
VisualWork 0.7447 0.7463 0.7095 0.7445 0.7523 25054
Software 0.6448 0.6586 0.5991 0.5913 0.5911 8058
MusicalWork 0.5408 0.5714 0.5239 0.5492 0.545 6292
Facility 0.5673 0.5671 0.5283 0.5317 0.5347 11393
Station 0.7997 0.7863 0.7812 0.784 0.781 2532
HumanSettlement 0.7608 0.7676 0.7517 0.7658 0.7647 55741
OtherLOC 0.37 0.3348 0.3413 0.2965 0.2376 1241
MicroAverage 0.6451 0.6517 0.6081 0.6108 0.6145 312115
MacroAverage 0.5792 0.5809 0.54 0.5104 0.5147 312115
WeightedAverage 0.6491 0.6531 0.6101 0.6111 0.6131 312115
Table11: ComparisonofF1ScoresforEachEntityTypeinMultiCoNERv2