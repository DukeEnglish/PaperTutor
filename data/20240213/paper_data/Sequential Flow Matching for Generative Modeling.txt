Sequential Flow Straightening for Generative Modeling
JongminYoon1 JuhoLee1
Abstract
Straightening the probability flow of the RF DDIM
RF-Reflow
continuous-time generative models, such as
diffusion models or flow-based models, is the
6 PNDM
key to fast sampling through the numerical
solvers, existing methods learn a linear path by
directly generating the probability path the joint
distribution between the noise and data distri- 4
bution. One key reason for the slow sampling
speed of the ODE-based solvers that simulate
these generative models is the global truncation 3 SeqRF (Ours)
error of the ODE solver, caused by the high DPM-Solver
curvatureoftheODEtrajectory,whichexplodes 1 10 100
the truncation error of the numerical solvers in
Number of function evaluation (NFE)
the low-NFE regime. To address this challenge,
We propose a novel method called Sequential Figure1.TheoverallgenerationresultinCIFAR-10dataset,com-
Reflow (SEQRF), a learning technique that paring our method to existing diffusion and flow-based model
straightens the probability flow to reduce the solvers.TheblackstarredpointsstandforourproposedSeqRF
global truncation error and hence enable accel- method.
eration of sampling and improve the synthesis
quality.Inboththeoreticalandempiricalstudies,
we first observe the straightening property of ial Networks (GANs) (Goodfellow et al., 2014) and Vari-
our SEQRF. Through empirical evaluations via ational Autoencoders (VAEs) (Kingma & Welling, 2014).
SEQRF over flow-based generative models, Operatingwithinacontinuous-timeframework,thesemod-
We achieve surpassing results on CIFAR-10, elsacquireproficiencyindiscerningthetime-reversalchar-
CelebA-64×64,andLSUN-Churchdatasets.
acteristics of stochastic processes extending from the data
distributiontotheGaussiannoisedistributioninthecontext
ofdiffusionmodels.Alternatively,inthecaseofflow-based
1.Introduction models,theydirectlylearntheprobabilityflow,effectively
simulatingthevectorfield.
Inrecenttimes,continuous-timegenerativemodels,exem-
plified by diffusion models (Song & Ermon, 2019; Song Recently, Lipmanetal.(2023)introducedanovelconcept
etal.,2021b;Hoetal.,2020)andflow-basedmodels(Lip- termed flow matching within continuous-time generative
manetal.,2023;Liuetal.,2023),havedemonstratedsig- models.Thisapproachfocusesonlearningthevectorfield
nificant improvement across diverse generative tasks, en- connecting the Gaussian noise and the data distribution.
compassing domains such as image generation (Dhariwal The authors initially demonstrated that the marginal vec-
& Nichol, 2021), videos (Ho et al., 2022), and 3D scene tor field, representing the relationship between these two
representation (Luo & Hu, 2021), and molecular synthe- distributions, is derived by marginalizing over the gradi-
sis (Xu et al., 2022). Notably, these models have outper- entsoftheconditionalvectorfields.Moreover,theyfound
formedestablishedcounterpartslikeGenerativeAdversar- thattheconditionalvectorfieldyieldsoptimaltransportbe-
tween the data and noise distributions, particularly when
1KimJaechulGraduateSchoolofAI,Daejeon,Korea.Corre-
thenoisedistributionadherestothestandardGaussiandis-
spondenceto:JuhoLee<juholee@kaist.ac.kr>.
tribution.Learningthe(marginal)vectorfieldinvolvesin-
Preliminarywork.UnderreviewbytheInternationalConference dependent sampling from both the noise and data distri-
onMachineLearning(ICML).Donotdistribute. butions,followedbythemarginalizationoftheconditional
1
4202
beF
9
]GL.sc[
1v16460.2042:viXra
DIFSequentialRectifiedFlow
Figure2.Theconceptfigureofourmethod.Thered,yellowandbluetrianglesrepresentthetruncationerrorbeingaccumulatedinthe
correspondingtime.Comparedtotheredreflowmethod,sequentialreflow(SeqRF)mitigatesmarginaltruncationerrorbyrunningtime-
segmentedODE.
vectorfieldoverthedatadistributions.Despitetheadvanta- rorofnumericalODEsolversexperiencesexplosivegrowth,
geouspropertyofidentifyingoptimaltransportpathswith exhibitingsuperlinearescalation.Thisunderscoresthesig-
optimalcoupling,thismethodfaceschallengessuchashigh nificance of generating the joint distribution from seg-
learningvarianceandslowtrainingspeedattributabletothe mentedtimedomains,therebyensuringdiminishedglobal
independent drawing of training data from data and noise truncationerrorsthroughthestrategichaltingofthesolver
distributions, which results in high gradient variance even beforetheerrorreachescriticallevels.
atitsconvergence.ThisthreatensthestabilityoftheOrdi-
naryDifferentialEquation(ODE)solverduetotheaccumu-
Main Proposal: Sequential Reflow for Flow Matching
lationoftruncationerrors.Inresponsetothischallenge,Liu
Our primary contribution is the introduction of sequential
etal.(2023)proposedamethodtostraightenthetrajectory
reflowasamethodforflowmatchingingenerativemodel-
by leveraging the joint distribution. This involves running
ing.Thisflowstraighteningtechniqueaimstodiminishthe
the numerical ODE solver from the noise to approximate
curvatureofsegmentedtimeschedules.Itachievesthisby
thedatapoint.However,traversingalongpathfromnoise
generatingthejointdistributionbetweendatapointsatdif-
to image space still leaves exploding global truncation er-
ferenttimesteps.Specifically,thesourcedatapoint,origi-
rorsunaddressed.
natingfromthetrainingsetenvelopedinnoise,undergoes
To address the challenge posed by truncation errors, we theODEsolverconstructedbypre-trainedcontinuous-time
introduce a novel framework termed Sequential Reflow generativemodelssuchasflow-basedordiffusionmodels.
(SEQRF).Thisnovelapproachrepresentsastraightforward Thismarksadeparturefromconventionalreflowmethods,
and effective training technique for flow-based generative whichstraightentheentireODEtrajectory.
models,specificallydesignedtoalleviatethetruncationer-
ror issue. The key innovation lies in the segmentation of
Validation of Sequential Reflow We empirically vali-
the ODE trajectory with respect to the time domain. In datethatoursequentialreflowmethodacceleratesthesam-
this strategy, we harness the joint distribution by partially
plingprocess,therebyenhancingtheimagesynthesisqual-
traversingthesolver,asopposedtoacquiringthecomplete
ity; the sampling quality improves with the rectified flow,
data with the entire trajectory. The overall concept of our
achievedbyemployingthesequentialreflowmethodsubse-
methodisbrieflyintroducedinFigure2.
quenttotheinitialflowmatchingprocedure.Furthermore,
Ourcontributionissummarizedasfollows. Weimplementdistillationoneachtimefragment,enabling
the traversal of a single time segment at a single func-
tionevaluation.Thisstrategicapproachresultsinsuperior
Controlling the global truncation error We begin by performance, achieving a remarkable 3.19 Freche´t Incep-
highlighting the observation that the global truncation er- tionDistance(FID)withonly6functionevaluationsonthe
2SequentialRectifiedFlow
CIFAR-10dataset,assketchedinFigure5. is,L (θ)=L (θ)+CforaconstantC.Thedetailed
FM CFM
proofisdescribedinAppendixE.
2.Background:ContinuousNormalizing
2.1.ConstructingStraightVectorFieldwithFlow
FlowandFlowMatching
matching
Consider the problem of constructing the probability path
Lipman et al. (2023) proposed a natural choice of con-
between the two distributions π and π in the data space
0 1 structingflowbetweentwodistributionsbytaking
RD. Let the time-dependent vector field be u : RD ×
[0,1]→RDwitht∈[0,1].ThentheInitialValueProblem p ∼N(tx ,(1−(1−σ )t)2I) (5)
t 1 min
(IVP)onanODEgeneratedbythisvectorfielduisgivenas
at time t ∈ [0,1]. This setting is directly related to
dx t =u(x t,t)dt. (1) constructing the straightened flow between two distribu-
tions. McCann (1997) showed that with two Gaussian
Chen et al. (2018); Grathwohl et al. (2019) suggested
distributions at the endpoints p ∼ N(0,I) and p ∼
thegenerativemodelcalledContinuousNormalizingFlow 0 1
N(x ,σ2 I),thevectorfield
(CNF) that reshapes the simple, easy-to-sample density 1 min
(i.e., Gaussian noise p ) into the data distribution p , x −tx
by constructing the
vect0
or field u with a neural
networ1
k
u(x t,t|x 1,σ min)=
(1−σ
t )(1−1
t)+σ
(6)
min min
v (x,t),parameterizedbyv : (RD,R) → RD.Thisvec-
θ θ
achievestheconditionallystraightpathbetweenp andp .
tor field v is used to generate a time-dependent continu- 0 1
θ
ously differentiable map called flow ϕ t : RD → RD if Liu et al. (2023); Albergo & Vanden-Eijnden (2023) de-
therandomvariablegeneratedwithX t ∼ ϕ t satisfyEqua- signedthesourcedistributionp 1andtheconditionaldistri-
tion 1. A vector field v θ is called to generate the flow ϕ t butionbyapproachingσ min →0,as
ifitsatisfiesthepush-forward(i.e.(generalized)changeof
variables)equation p 1 ∼π 0×π 1, u(x t,t|x 1)= lim u(x t,t|x 1,σ min).
σmin→0
(7)
p =[ϕ ] p ,
t t ∗ 0 Then,theCFMobjectivebecomes
where [ϕ t] ∗p 0(x)=p 0(ϕ− t 1(x))(cid:12) (cid:12) (cid:12) (cid:12)det(cid:20) ∂ ∂ϕ x− t 1 (x)(cid:21)(cid:12) (cid:12) (cid:12) (cid:12). (2) E t,x0,x1,n[∥v θ(x t,t)−(tx 1+(1−t)x 0+n)∥], (8)
However, constructing the CNF requires backpropagation where (x 0,x 1) ∼ π 0 × π 1 and n ∼ N(0,I), which is
of the entire adjoint equation with the NLL objective, reducedto
whichrequiresfullforwardsimulationandgradientestima-
E [∥v (x ,t)−(tx +(1−t)x )∥] (9)
tionofthevectorfieldoverthetimedomain[0,1].Theflow t,x0,x1 θ t 1 0
matching(Lipmanetal.,2023;Liuetal.,2023)algorithm
whichcorrespondstotherectifiedflowobjective(Liuetal.,
overcomesthislimitwithasimulation-free(e.g.,doesnot 2023;2024).
requireacompleteforwardpassfortraining)algorithmby
replacingthiswiththeℓ -squareobjective
2 2.2.TheTruncationErrorsoftheNumericalSolvers
(cid:104) (cid:105) andRectifiedFlow
L (θ)=E ∥v (x,t)−u(x ,t)∥2 , (3)
FM t,x1 θ t 2
LettheIVPofanODEbegivenasfollows:
where u is true vector field defined in (1). However, the
(cid:40)
computational intractability of v makes it difficult to di- dx =v (x ,t)dt,
θ t θ t t∈[a,b], (10)
rectly learn from the true objective Equation 3. Lipman x =x(a)
a
et al. (2023) found that by using the conditional flow
matchingobjectiveinstead, where v is the (learned) neural network, x(a) is the
θ
initial value, and (a,b) are the initial and terminal
(cid:104) (cid:105)
L CFM(θ)=E t,x1,xt|x1 ∥v θ(x t,t)−u(x t,t|x 1)∥2 2 , points of the ODE, respectively. In general, a solver with
(4) theODEEquation10isdefinedbygeneratingthesequence
where p t(x t|x 1) is the conditional probability density of {x a = x t0,x ti,··· ,x tN = x b} with the following equa-
noisy data x given the data x , we can learn the flow tion
t 1
matching model with conditional flow matching objective
x =x +(t −t )A(x ,t ,h ;v ) (11)
basedonthepropositionbelow. ti+1 ti i+1 i ti i i θ
Proposition1(EquivalenceoftheFMandCFMobjective). where A(x ,t ;v ) is the increment function and h =
ti i θ i
The gradient of Equation 3 and Equation 4 is equal. That t −t istheintervalbetweentwoconsecutivesteps.Then
i+1 i
3SequentialRectifiedFlow
thetruncationerrorofasolverisdefinedbythedifference
between the true solution of (10) and the approximation
100
from(11).Therearetwokindsoftruncationerrors:thelo-
calandglobaltruncationerrors.
Definition 1 (truncation errors). Let x be an estimate
ti
with(11)ofODE(10)attimet i.Thenthelocaltruncation
error(LTE)τ(t i,h i)is
τ(t i,h i)=(xˆ ti+hi(x ti)−x ti)−A(x ti,t i,h i;v θ). 10 1 1 2- -R RF F
(12) 2-SeqRF
4-SeqRF
wherexˆ ti+hi(x ti)isthetruesolutionof(10)whichstarts 6-SeqRF
atpointx fromtimet tot +h .Theglobaltruncation 8-SeqRF
ti i i i 12-SeqRF
error(GTE)E(t i)attimetistheaccumulationofLTEover
50 100 150 200 250
time{t }i ,
j j=1 Number of function evaluations
i
(cid:88)
E(t i)=(x(t i)−x(a))− A(x tj,t j,h j;v θ). (13) Figure3.The global truncation error over NFE in CIFAR-10
j=1 dataset,comparedtotheoracleEuler-480stepsolver.OurSeqRF
methods,showninblueandredlines,deploylowerglobaltrun-
wherex(t)isthetruesolutionof(10)attimet.
cationerrors.
As the CFM objective (4) with flow matching or rectified
flowachievesconditionalOTmappingbetweentwocondi- the solver in a single step, (2) the variance of the drift v θ,
tional distributions, it is obviousthat with the same incre- and (3) the total interval of the numerical solver. In § 3.2,
ment function A(x ,t ;v ) of the numerical solver (11) weproposeournewmethod,SequentialReflow(SEQRF),
ti i θ
from a point x in the source distribution a to the con- that mitigates these errors to construct a fast and efficient
a
ditional target distribution p (·|x ) with one-step linear numericalsolverthatsuccessfullysimulatesanODE.
b a
solverwithtimestepintervalh=b−a,elicitingzeroglobal
truncationerror. 3.1.OntheGlobalTruncationErrorofODESolver
Despite that this provably holds for optimal transport In this section, we delve into the factors that cause high
mapping between conditional distributions, this generally global truncation error ofthe ODE solver to explode. The
does not hold for OT mapping between marginal distri- following Theorem 2 implies that the truncation error of
butions. Figure 3 depicts the global truncation error be- anODEsolverdependsonboththelocaltruncationerror
tweentheapproximatelytrueODEsolverandthesmall-step of the solver and the Lipschitz constant of v θ. Following
solverlearnedbytheflowmatchingalgorithm,comparing this,Theorem4showsthatthisisalsoaffectedbythetotal
the GTE between the few-step Euler sampler to the 480- intervallengthoftheODE.
stepEulersamplerasbaseline.Theboldblackline,which
Theorem2(UpperBoundofGTEw.r.t.LTE). Letthelo-
directly trains the flow matching model via the CFM ob-
caltruncationerroroftheODEsolverbeτ(t),andM (t)
sup
jective (4), has high GTE. The reflow method (Liu et al.,
bethemaximumLipschitzconstantoff attimet.ThenThe
2023), marked in a dashed black line, generates pairs of
globaltruncationerrorE(c)attimecoftheone-stepODE
noise and data points from the pre-trained flow matching
solverisboundedby
modelsandtakeseachpairasthenewjointdistribution.
(cid:90) c (cid:18)(cid:90) c (cid:19)
In the next section, We introduce Sequential Reflow E(c)≤ τ(t)exp M (t′)dt′ dt (14)
sup
(SEQRF)method,displayedbyblueandredlinesinFig- t=a t′=t
ure 3, which successfully suppress the GTE of the ODE
forallc∈[a,b].
solver and hence achieve high image generation quality
withasmallNumberofFunctionEvaluation(NFE).
Proof. PleaserefertoAppendixG.1.
3.Mainmethod
Then, Dahlquist (1963) provided the upper bound of the
Several factors cause the numerical solver to have a global truncation error of the generalized ODE solvers as
follows.
high GTE, implying that this solver is ill-posed. In § 3.1,
we investigate the aspects that affect to GTE by deriving Lemma 3 (Dahlquist Equivalence Theorem (Dahlquist,
the upper bounds of GTE. To summarize, the (1) LTE of 1963)). Let[a,b]bedividedbyhequidistributedintervals,
4
))t(E(
ETGSequentialRectifiedFlow
1-RF
2-RF
2-SeqRF
4-SeqRF
6 6-SeqRF
8-SeqRF
12-SeqRF
Figure4.The concept figure of k-SeqRF-Distill, contrast to the Distill
RF-Distillmethod(Liuetal.,2023).
4
i.e.,t =a+i(b−a).Thenthegeneralizedlinearmultistep
i h
methodisdefinedby
3
x a+(i+1)h = 1 10 100
p p Number of function evaluation (NFE)
(cid:88) (cid:88)
α x +h β v(x ,a−jh)), i≥p.
j a−jh j a−jh
j=0 j=−1 Figure5.GenerationperformanceofSequentialreflow,compared
(15) to the original rectified flow method. The black and red, and
Then for a linear multistep method of order p that is con- otherlinesrepresenttherectifiedflow(1-RF),thereflowedmodel
sistentwiththeODE(10)wherev tsatisfytheLipschitzcon- (2-RF),andthek-SeqRF(k ={1,2,4,6,8,12}models,respec-
dition and with fixed initial value x at t = a, the global tively.Thestarredpointsdenotetheperformanceofdistilledmod-
a
truncationerrorisO(hp). els.
WithLemma3,theglobaltruncationerroraccumulatedby
theODEsolverwiththesamecomplexityisreducedwhen
25 100 k=2
thetotallengthofthetimeintervalbecomesshorter.Tobe k=4
k=6
precise,theglobaltruncationerrorisreducedbytheorder 0 k k= =8 12
10 1 of Kp−1, where K is the number of intervals and r is the 25
k=2
orderoftheODEsolver,accordingtothefollowingTheo- k=4
50 k k= =6 8 10 2
rem4. k=12
0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8
Theorem4(GlobalTruncationErrorwithIncreasingTime time (t) time (t)
Segments). Suppose that a linear multistep method (15)
Figure6.Empiricalresultsonthe(a)averageLipschitznessover
successfully simulates the ODE (10). And suppose that x and(b)∥v (x ,t)∥2 overdifferentK inSeqRF.Inbothtwo
theNFEoflinearmultistepmethodfromtime(a,t)isgiven cat ses,theblacθ klit ne(22
-SeqRF)withminimalsegmentshasmax-
the same as b−a. Let K be the number of equidistributed imumvalues.
K
intervals that segment [a,b]. And let (15) have of order p,
thenthelocaltruncationerrorisO(hp+1),thenthefollow-
ingholds.
refining the pre-trained ODE (Liu et al., 2023; 2024; Al-
bergo&Vanden-Eijnden,2023;Albergoetal.,2023).Al-
1. The order of the local truncation error is τ(t,h) =
gorithm1summarizesourSEQRFmethod.
O(hp+1).
2. The global truncation error at time t is E(t) =
O(cid:0) K1−p(cid:1)
E(c)
GeneratingJointDistributionwithTimeSegmentation
of the ODE Trajectory Let the ODE-IVP problem be
defined by Equation 10. Then the entire time interval of
this ODE is given as [a,b]. In Theorem 4 of § 3.1, we
Proof. PleaserefertoAppendixG.2.
found that the global truncation error of the solver is in-
verselyproportionaltothe(p−1)th powerofthenumber
oftimesegmentsK forthesolverhavingorderp.Inspired
3.2.SequentialReflow:FlowStraighteningby
bythis,wefirstdividethe ODE trajectorybyK segments,
SuppressingtheTruncationError
{a=t ,t ,··· ,t =b},togeneratethejointdistribution,
0 1 K
Inthissection,weproposeournewmethod, sequentialre- wherethesourcedatapointattimet issampledfromthe
k
flow, that improves the ODE-based generative models by OTinterpolantp fromEquation5withσ → 0.Then
tk min
suppressingthetruncationerrorofthenumericalsolver,by thejointdistributiongeneratedbyourK-SeqRFmethodis
5
wolf
fo ssenztihcspiL
DIF
mron
derauqsSequentialRectifiedFlow
2.5
2.0
1.5
1.0
0.5
2 4 6 8 10 12
Number of segments K
(a)OverthenumberofsegmentsK. (b)Seq.straightnesswithk=6overt. (c)Straightnessof1-RFovert.
Figure7.Sequential straightness result for CIFAR-10 dataset, with 100-step Euler solver. t = 0 and t = 1 stand for near-data and
near-noiseregimes,respectively.Straightnessresultsforotherk’sareintroducedinAppendixF.2.
asfollows: • Different from the primary SeqRF training process,
We fix the time to the starting point of t instead of
k
(x ,xˆ (x ))∼p (·)×ODE-Solver(x ,t ,t ;v ) uniformlytrainingfromt∈(t ,t ).
tk tk+1 tk tk tk k k+1 θ k k+1
(16)
for k ∈ [0 : K − 1], where ODE-Solver denotes the Then, by distilling the SeqRF model, we directly take ad-
numericalsolveronuse,whichcanbeanylinearmultistep vantage of the straightened probability flow of the previ-
method defined by Equation 15, such as Euler, Heun, or ouslytrainedSeqRFmodel.
Runge-Kuttamethods.
VarianceReductionwithSequentialReflow Pooladian
Our SeqRF differs from the Reflow method (Liu et al.,
etal.(2023)providesthevariancereductionargumentthat
2023; 2024), which also attempted to straighten the flow-
sheds light on using joint distribution as training data, as
based models by constructing joint distribution. The Re-
follows.
flowmethodgeneratesthejointdistribution
Proposition 5 (Pooladian et al. (2023), Lemma 3.2). Let
(x a,xˆ b)∼N(0,I)×ODE-Solver(x a,a,b;v θ), (17) (x a,x b)bejointlydrawnfromthejointdensityπ = π a×
π .Andlettheflow-matchingobjectivefromjointdistribu-
b
bysolvingODEfromatob.Inparticular,thereflowmethod tionbegivenas
is the special case of our method with K = 1. We visu- (cid:104) (cid:105)
L =E ∥v (x ,t)−u (x |x )∥2 . (19)
alized how SeqRF differs from the na¨ıve reflow method JCFM xa,xb θ t t t 1 2
inFigure4.
Thentheexpectationofthetotalvarianceofthegradientat
afixed(x,t)isboundedas
Training with Joint Distribution Let the pair of data
(cid:104) (cid:16) (cid:17)(cid:105)
points drawn from the joint distribution with Equation 16 E Tr(Cov ∇ ∥v (x,t)−u (x|x )∥2
be{π(x ,xˆ (x ))}K−1.
t,x pt(x1|x) θ θ t a 2
tK tK+1 tK i=0 ≤max∥∇ v (x,t)∥2L . (20)
L seq =E
xs,k(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)v θ(x s,s)− x ts− −x
ttk−1(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
(18)
t,x θ θ 2 JCFM
i k−1 2
This proposition implies that training from joint distribu-
tions, instead of the product of independent distributions,
After training the probability path with joint distribution, improves training stability and thus efficiently constructs
we may fine-tune our model with distillation to move di- theoptimaltransport(OT)mappingbetweentwodistribu-
rectlytowardthewholetimesegment.Thatis,thenumber tions.
of function evaluations is the same as the number of time
segmentsafterdistillationfinishes.Thedistillationprocess Empirical Validation on Bounds M (t) (14) and
sup
isdoneasfollows: ∥∇ v (x,t)∥2 (20) are key aspects for upper bounds
θ θ 2
ofGTE.ToshowthatSeqRFsuccessfullymitigatestheGTE
• we use the same generated pairs of data points as andobtainmoreexactsolutionofthesolver,weempirically
inEquation16asthejointdistribution,withthesame showedthatM (t)and∥∇ v (x,t)∥2tendtodecreaseas
sup θ θ 2
objectiveEquation18. kincreases.
6
ssenthgiarts
.qeSSequentialRectifiedFlow
Flow Straightening Effect with Sequential Reflow To probability density of the noisy interpolant between two
validate how much the vector field approximates a distributionssatisfiestheFokker-Planckequationsofanex-
single-step solver, Liu et al. (2023) proposed a measure istingdiffusionmodel.
calledstraightnessofacontinuouslydifferentiableprocess
Z={Z }1 thatdefinestheODE(10),definedby
t t=0 VarianceReductionwithOptimalCoupling Ourwork
(cid:90) 1(cid:13) (cid:13) d (cid:13) (cid:13)2 canalsobeinterpretedtomatchthenoiseanddatadistribu-
S(Z)= (cid:13) (cid:13)(Z 1−Z 0)− dtZ t(cid:13)
(cid:13)
dt. (21) tionpairstomaketrainingmoreefficientbyreducingtrain-
0 ing variance. Pooladian et al. (2023) used the minibatch
coupling to generate joint distributions in a simulation-
S(Z) is zero if and only if v(x ) is locally Lipschitz with
t
free manner by constructing a doubly stochastic matrix
zero dilation M(t) almost surely. Hence, this implies that
with transition probabilities and obtaining coupling from
havinglowstraightnessrepresentsforlowtruncationerror.
thismatrix.Eventhoughthisgeneratesone-to-onematch-
To generalize this to the stiff ODE formulated by SEQRF, ing between noise and images, this is numerically in-
weproposeametricnamedsequentialstraightness, tractable when the number of minibatch increases. Tong
etal.(2023b)alsointroducedtheconceptofminibatchop-
S
(Z)=(cid:88)K (cid:90) ti (cid:13) (cid:13) (cid:13)Z ti
i
−Z ti
i−1 −
d Zi(cid:13) (cid:13) (cid:13)2
dt,
timaltransportbyfindingoptimalcouplingwithinagiven
seq (cid:13) t −t dt t(cid:13) minibatch. Tong et al. (2023a) further expanded this ap-
i=1
ti−1(cid:13) i i−1 (cid:13)
proach to generalized flow (e.g., Schro¨dinger bridge) in
(22)
whereZ = {Zi}K isacollectionofthetime-segmented termsofstochasticdynamics.
i=1
processes.LikeS(Z),zeroS (Z)denotesthateachtime-
seq
segmentedprocessiscompletelystraight,andthisintends 5.Experiments
that the sequential straightness is zero if and only if the
In this section, we take empirical evaluations of SeqRF,
dilationM(t)iszeroalmostsurelyforallt∈(a,b).
compared to other generative models, especially with dif-
Figure 7 demonstrates the sequential straightness of the fusion and flow-based models in CIFAR-10, CelebA and
rectifiedflowmodelstrainedonCIFAR-10datasetsforthe LSUN-Church datasets. We used the NCSN++ architec-
numberofsegments,wherethenumberofreflowanddistil- turebasedonthescore sde1 repository.Eachmodelis
lationpairs1M.Figure7ademonstratesthatthesequential trained with JAX/Flax packages on TPU-v2-8 (CIFAR-
straightness lowers in both datasets as we train with finer 10) and TPU-v3-8 (CelebA, LSUN-Church) nodes. In
intervals,implyingthatweachievedasuperiorstraighten- CIFAR-10andCelebAdatasets,wefirstgenerated1Mand
ingperformancebyusingtheSeqRFmethod.Forinstance, 300k reflow pairs to train our SeqRF models and for dis-
Figure 7b displays the straightness with respect to time at tillation. And for LSUN-Church dataset, we instead used
6-SeqRF,fromnear-noise regimeattime 0 tonear-dataat 100k reflow pairs. We did an ablation on the number of
time1. reflowpairsinAppendixC.1.Inaddition,thewholeexper-
imental details including the architectural design and hy-
4.Relatedwork perparametersaredescribedinAppendixB.
We introduce the most relevant works to our paper in the
5.1.ImageGenerationResult
main section. For additional related works, please refer
toAppendixH. We demonstrate our image synthesis quality in Table 1 in
CIFAR-10, compared to existing flow matching methods
Flow Matching Recently, straightening the continuous includingrectifiedflow(Liuetal.,2023),conditionalflow
flowbymitigatingthecurvatureoftheprobabilitypathhas matching(Lipmanetal.,2023),I-CFMandOT-CFM(Tong
been considered by considering the optimal transport reg- etal.,2023b).Forthebaselinerectifiedflow,weimported
ularizationbyintroducing CNF norms(Finlayetal.,2020; thepre-trainedmodelfromtheRectifiedFlow2 repos-
Onkenetal.,2021;Bhaskaretal.,2022)fortrainingneu- itory, and re-implemented the method using JAX/Flax
ral ODE, and Kelly et al. (2020) learned the straightened
packages.Forafaircomparison,wereporttheJAX/Flax
neuralODEsbylearningtodecreasethesurrogatehigher- results in our experiments. 3 Our generation result sur-
orderderivatives.Leeetal.(2023)showedthatminimizing passes existing flow-based methods and diffusion models,
thecurvatureinflowmatchingisequivalenttotheβ-VAE achieving 3.191 FID with 6 steps with distillation. This
invariationalinference.Asageneralizationoffindingop-
1github.com/yang-song/score sde
timal transport interpolant, Albergo et al. (2023); Albergo 2github.com/gnobitab/RectifiedFlow
&Vanden-Eijnden(2023)proposedstochasticinterpolant, 3ThebaselineFIDis0.01−0.02higherthanreportedresult
whichunifiestheflowsanddiffusionsbyshowingthatthe measuredwithPyTorch-basedcode.
7SequentialRectifiedFlow
Table1.CIFAR-10 image generation result. For {NFE, IS, Table2.The image generation result for CelebA-64 × 64 and
KID}/{IS}, lower and higher value represents better result, re- LSUN-Church-256×256datasets.TheKIDmeasureisdivided
spectively. The RK45-{tol} solver denotes the Runge-Kutta by104 and102 forCelebAandLSUN-Churchdatasets,respec-
solveroforder4withabsoluteandrelativetolerancetol.TheKID tively.E-MstandsforEuler-Maruyamasolver.
measureisdividedby104.Fork-SeqRFmethod,wechoosethe
CelebA64×64
NFE where the FID converges. The full FID result over NFEs
k-SeqRF(Ours)
aredemonstratedinFigure5.WecomparedourstoDDIM(Song
etal.,2021a),DPM-Solver(Luetal.,2022)andPNDM(Liuetal., Method FID KID NFE Solver
2022). 1-RF 2.327 12.95 113 RK45-10−4
Flow-basedmethods 2-RF 5.841 38.01 50 RK45-10−4
8.457 55.84 2 Euler
Method FID IS KID NFE Solver
1-RF 62.173 9.44 64.70 20 RK45-10−2
k-SeqRF(Ours)
k=2 4.672 29.28 2 Euler
3.176 9.81 9.63 50 RK45-10−3
k=4 5.322 38.02 4 Euler
2.607 9.60 7.62 100 RK45-10−4
k=2 3.878 25.56 52 RK45-10−3
2.593 9.60 7.49 254 RK45-10−5
k=4 5.268 36.98 68 RK45-10−3
1-RF-Distill(Liuetal.,2023)
k=1 4.858 9.08 17.56 1 Diffusion-basedMethods
k=2 4.710 9.06 23.72 2 - (Songetal.,2021a) 6.77 - 50 Ancestral
k=4 4.210 9.16 19.66 4 - (Hoetal.,2020) 45.20 - 50 E-M
k=6 4.018 9.14 18.28 6 - DiffuseVAE 5.43 - 50 Euler
k=8 3.822 9.22 15.35 8 -
LSUN-Church256×256
k=12 3.772 9.24 15.65 12 -
Method FID KID NFE Solver
2-RF 28.077 9.27 304.1 21 RK45-10−2
3.358 9.27 107.9 50 RK45-10−3 1-RF(Pre-trained) 315.317 35.06 2 Euler
124.461 12.62 5
OT-CFM 20.86 - - 10 Euler
45.042 4.090 10
I-CFM 10.68 - - 50 Euler
2-RF 104.417 11.99 2 Euler
k-SeqRF(Ours) 76.375 13.84 5
k=2 3.377 9.43 12.04 30 Euler 45.747 4.635 10
k=4 3.269 9.44 11.34 24 Euler
k-SeqRF(Ours)
k=6 3.191 9.46 11.70 40 Euler
k=2 104.417 11.99 2 Euler
k=8 3.221 9.47 11.46 24 Euler
56.569 5.922 5
k=12 3.184 9.53 11.25 24 Euler
45.747 4.635 10
k-SeqRF-Distill(Ours) k=4 35.833 3.601 4 Euler
k=2 4.081 9.28 19.56 2 - 30.883 2.959 5
k=4 3.297 9.38 13.06 4 - 28.066 2.632 10
k=6 3.192 9.43 12.58 6 -
k=8 3.199 9.44 12.68 8 -
k=12 3.207 9.52 12.17 12 -
Diffusion-basedmethods
DDIM 4.67 - - 50 Ancestral
PNDM 4.61 - - 20 PNDM 6.Conclusionanddiscussion
DPM-Solver 5.28 - - 12 RK45
We introduce sequential reflow, a straightforward and ef-
fectivetechniqueforrectifyingflow-basedgenerativemod-
els. This method initially subdivides the time domain into
multiple segments and subsequently generates a joint dis-
notonlysurpassesexistingdiffusionmodelapproaches,but tributionbytraversingoverpartialtimedomains.Through
also the existing distillation methods for flow-based mod- this, we successfully alleviate the global truncation error
els. associatedwiththeODEsolver.Consequently,thisprocess
yields a straighter path, thereby enhancing the efficiency
We present the image generation results for CelebA and
andspeedofthesamplingprocedure.
LSUN-ChurchdatasetsinTable2.Figure9displayssome
non-curated CIFAR-10 images generated from the same Although we have focused on the generative modeling
random seed with k-SeqRF models. Please refer to Ap- problem, our approach can be more broadened to other
pendix D.3 for further image examples such as LSUN- fieldssuchasimage-to-imagetranslations,andlatentgen-
ChurchandCelebAdatasets. erativemodelswhichcancoverlarge-scaledatasets.
8SequentialRectifiedFlow
Ethical aspects. Among the datasets we have used, dynamics for scalable reversible generative models. In
CelebAdatasetshavebiasedattributes,suchascontaining International Conference on Learning Representations
morewhitepeoplethanblackpeople,andmoremalesthan (ICLR),2019. 3,19
females.
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
Future societal consequences. Our paper proposes a bilisticmodels. InAdvancesinNeuralInformationPro-
new algorithm that contributes to the field of generative cessingSystems32(NeurIPS2019),2020. 1,8,19
model research, especially flow-based generative models.
Ho,J.,Salimans,T.,Gritsenko,A.A.,Chan,W.,Norouzi,
Flow-basedmodelscanbeappliedtothebroaderareaofar-
M.,andFleet,D.J.Videodiffusionmodels.InAdvances
tificialintelligence,suchasimage-to-imagetranslationthat
in Neural Information Processing Systems 35 (NeurIPS
canbeabusedforDeepfake.
2022),2022. 1
References Huang, C., Krueger, D., Lacoste, A., and Courville, A. C.
Neuralautoregressiveflows. InProceedingsofThe35th
Albergo, M. S. and Vanden-Eijnden, E. Building normal-
International Conference on Machine Learning (ICML
izingflowswithstochasticinterpolants. InInternational
2018),2018. 19
Conference onLearning Representations(ICLR), 2023.
3,5,7
Huang, C., Lim, J. H., and Courville, A. C. A varia-
Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. tionalperspectiveondiffusion-basedgenerativemodels
Stochasticinterpolants:Aunifyingframeworkforflows andscorematching. InAdvancesinNeuralInformation
anddiffusions. CoRR,abs/2303.08797,2023. 5,7 ProcessingSystems34(NeurIPS2021),2021. 19
Bhaskar, D., MacDonald, K., Fasina, O., Thomas, D., Kelly,J.,Bettencourt,J.,Johnson,M.J.,andDuvenaud,D.
Rieck, B., Adelstein, I., and Krishnaswamy, S. Diffu- Learningdifferentialequationsthatareeasytosolve. In
sioncurvatureforestimatinglocalcurvatureinhighdi- AdvancesinNeuralInformationProcessingSystems33
mensionaldata. InAdvancesinNeuralInformationPro- (NeurIPS2020),2020. 7
cessingSystems35(NeurIPS2022),2022. 7
Kingma,D.P.andDhariwal,P.Glow:Generativeflowwith
Chen,T.Q.,Rubanova,Y.,Bettencourt,J.,andDuvenaud, invertible 1x1 convolutions. In Advances in Neural In-
D. Neuralordinarydifferentialequations. InAdvances formationProcessingSystems31(NeurIPS2018),2018.
in Neural Information Processing Systems 31 (NeurIPS 19
2018),2018. 3,19
Kingma,D.P.andWelling,M. Auto-encodingvariational
Dahlquist, G. G. A special stability problem for
bayes. InInternationalConferenceonLearningRepre-
linear multistep methods - bit numerical mathemat-
sentations(ICLR),2014. 1
ics, 1963. URL https://link.springer.com/
article/10.1007/BF01963532. 4 Kingma, D. P., Salimans, T., Poole, B., and Ho, J. On
density estimation with diffusion models. In Advances
Dhariwal,P.andNichol,A.Q. Diffusionmodelsbeatgans
in Neural Information Processing Systems 34 (NeurIPS
onimagesynthesis. InAdvancesinNeuralInformation
2021),2021. 19
ProcessingSystems34(NeurIPS2021),2021. 1
Lee,S.,Kim,B.,andYe,J.C.Minimizingtrajectorycurva-
Finlay, C., Jacobsen, J., Nurbekyan, L., and Oberman,
tureofode-basedgenerativemodels. InProceedingsof
A. M. How to train your neural ODE: the world of
The40thInternationalConferenceonMachineLearning
jacobian and kinetic regularization. In Proceedings of
(ICML2023),2023. 7
The 37th International Conference on Machine Learn-
ing(ICML2020),2020. 7
Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M.,
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., andLe,M. Flowmatchingforgenerativemodeling. In
Warde-Farley, D., Ozair, S., Courville, A. C., and Ben- International Conference on Learning Representations
gio, Y. Generative adversarial nets. In Advances in (ICLR),2023. 1,3,7,14
NeuralInformationProcessingSystems27(NIPS2014),
Liu, L., Ren, Y., Lin, Z., and Zhao, Z. Pseudo numerical
2014. 1
methodsfordiffusionmodelsonmanifolds. InInterna-
Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, tionalConferenceonLearningRepresentations(ICLR),
I., and Duvenaud, D. FFJORD: free-form continuous 2022. 8
9SequentialRectifiedFlow
Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wo-
Learningtogenerateandtransferdatawithrectifiedflow. jna, Z. Rethinking the inception architecture for com-
In International Conference on Learning Representa- puter vision. In Proceedings of the IEEE Conference
tions(ICLR),2023. 1,2,3,4,5,6,7,8,13,14,15 on Computer Vision and Pattern Recognition (CVPR),
2016. 12
Liu,X.,Zhang,X.,Ma,J.,Peng,J.,andLiu,Q. Instaflow:
Onestepisenoughforhigh-qualitydiffusion-basedtext- Tong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang,
to-image generation. In International Conference on Y.,Huguet,G.,Wolf,G.,andBengio,Y.Simulation-free
LearningRepresentations(ICLR),2024. 3,5,6 schro¨dingerbridgesviascoreandflowmatching. arXiv
preprint2307.03672,2023a. 7
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
Dpm-solver:AfastODEsolverfordiffusionprobabilis- Tong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-
ticmodelsamplinginaround10steps. CoRR,2022. 8 Brooks,J.,Fatras,K.,Wolf,G.,andBengio,Y. Improv-
ingandgeneralizingflow-basedgenerativemodelswith
Luo, S. and Hu, W. Diffusion probabilistic models for
minibatchoptimaltransport.arXivpreprint2302.00482,
3d point cloud generation. In Proceedings of the IEEE
2023b. 7
ConferenceonComputerVisionandPatternRecognition
(CVPR),2021. 1 Xu, M., Yu, L., Song, Y., Shi, C., Ermon, S., and Tang,
J. Geodiff: A geometric diffusion model for molecular
McCann,R.J. Aconvexityprincipleforinteractinggases.
conformation generation. In International Conference
AdvancesinMathematics,1997. 3
onLearningRepresentations(ICLR),2022. 1
Onken,D.,Fung,S.W.,Li,X.,andRuthotto,L. Ot-flow:
Fast and accurate continuous normalizing flows via op-
timaltransport. InProceedingsoftheAAAIConference
onArtificialIntelligence,2021. 7
Papamakarios,G.,Murray,I.,andPavlakou,T.Maskedau-
toregressiveflowfordensityestimation. InAdvancesin
NeuralInformationProcessingSystems30(NIPS2017),
2017. 19
Pooladian,A.,Ben-Hamu,H.,Domingo-Enrich,C.,Amos,
B., Lipman, Y., and Chen, R. T. Q. Multisample flow
matching:Straighteningflowswithminibatchcouplings.
InProceedingsofThe40thInternationalConferenceon
MachineLearning(ICML2023),2023. 6,7
Sohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,and
Ganguli,S. Deepunsupervisedlearningusingnonequi-
librium thermodynamics. In Proceedings of The 32nd
International Conference on Machine Learning (ICML
2015),2015. 19
Song,J.,Meng,C.,andErmon,S. Denoisingdiffusionim-
plicitmodels. InInternationalConferenceonLearning
Representations(ICLR),2021a. 8,19
Song,Y.andErmon,S.Generativemodelingbyestimating
gradients of the data distribution. In Advances in Neu-
ralInformationProcessingSystems32(NeurIPS2019),
2019. 1,19
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er-
mon, S., and Poole, B. Score-based generative model-
ingthroughstochasticdifferentialequations. InInterna-
tionalConferenceonLearningRepresentations(ICLR),
2021b. 1,19
10SequentialRectifiedFlow
A.TheoverallalgorithmofSequentialReflow(SeqRF)
Algorithm1TrainingSequentialRectifiedFlowforGenerativeModeling
Require: Datadistributionπ ,Noisedistributionπ ,datadimensionD,NumberofdivisionsK,Flownetworkmodel
data noise
v :RD →RD,Timedivision{t }K ,a=t <t <···<t <t =b.
θ i i=0 0 1 K−1 K
Stage1.Pre-trainingrectifiedflow
whileNotconvergeddo
Draw(X ,X )∼π ×π ,t∈(0,1).
0 1 data noise
X ←(1−t)X +tX .
t 0 1
(cid:104) (cid:105)
UpdateθtominimizeE ∥v (X ,t)−(X −X )∥2 (Learningtheflownetwork)
Xt,t θ t 1 0 2
endwhile
Stage2.Sequentialreflow+Distillationwithsegmentedtimedivisions
whileNotconvergeddo
Samplex =(1−t )x +t x ,(x ,x )∼π ×π ,t ,k ∈[0:K−1]
ti k a k b a b noise data i
Obtainxˆ byrunningdX =v (X ,t)dtwithanODEsolverfromt →t .
t−1 t θ t i i−1
ifReflowthen
x ←(1−r)xˆ +rx ,s=t′+(t −t )r,r ∼U(0,1)
s ti−1 ti i i−1
elseifDistillthen
X ←X ,s=t
s ti
endif
UpdateθtominimizeE
(cid:20)(cid:13)
(cid:13)v (x ,s)−
xs−xtk−1(cid:13) (cid:13)2(cid:21)
(Learningwithsequentialreflow)
xs,r (cid:13) θ s ti−ti−1 (cid:13)
2
endwhile
B.ExperimentalDetails
B.1.DatasetDescription
CIFAR-10 The CIFAR-10 dataset is the image dataset that consists of 10 classes of typical real-world objects with
50,000 training images and 10,000 test images with 32×32 resolution. In our experiments, we did not use the image
labelsandconstructedunconditionalgenerativemodels.
CelebA TheCelebAdatasetisthefacedatasetthatconsistsof202,599trainingimagesfrom10,177celebritieswith40
binary attribute labels, with size 178×218. In our experiment, we resized and cropped the image to 64×64 resolution
tounifytheinputshapeofthegenerativemodels.Also,wedidnotusetheattributelabelsandconstructedunconditional
generativemodels.
LSUN-Church TheLarge-ScaleSceneUNderstanding(LSUN)-Church(-Outdoor)datasetisthedatasetthatconsistsof
thechurchimagesaswellasthebackgroundwhichsurroundsthem.Thisdataconsistsof126,227imageswith256×256
resolution.
B.2.DetailsonTrainingHyperparameters
WereportthehyperparametersthatweusedfortraininginTable3.Weuseda32-bitprecisionfloatingpointnumberfor
trainingalldatasets.
B.3.PerformanceMetrics
Weusedthetfganpackagetomeasurethefollowingmetrics.
Freche´t Inception Distance (FID) The Freche´t Inception Distance (FID) uses the third pooling layer of the
Inceptionv3 network of the ground-truth image and generated images as the intermediate feature to interpret. Let
themeanandcovarianceoftheground-truthimagesbe(µ ,Σ )andthoseofthegeneratedimagesas(µ ,Σ ).Thenthe
g g x x
11SequentialRectifiedFlow
Table3.Thehyperparameterusedtotrainourmodel.
CIFAR-10 CelebA LSUN-Church
Channels 128 128 128
Depth 3 3 4
Channelmultiplier (1,2,2,2) (1,2,2) (1,1,2,2,2,2,2)
Heads 4 4 4
Attentionresolution 16 16 16
Dropout 0.15 0.1 0.1
Batchsize 512 64 64
Baselinesteps 800,000 300,000 600,000
Reflowsteps 100,000 100,000 200,000
Jittedsteps 5 5 5
Reflowimages 1M 200k 1M
EMArate 0.9999 0.9999 0.999
Learningrate(Adam) 2×10−4 2×10−4 2×10−4
(β ,β )(Adam) (0.9,0.999) (0.9,0.999) (0.9,0.999)
1 2
FIDisdefinedby
(cid:114)
(cid:16) (cid:17)
FID(x)=∥µ x−µ g∥2 2+ Tr Σ x+Σ g−2(Σ xΣ g)1 2 . (23)
This represents the fidelity of the generated images, comparing the embedding distribution between the generated and
ground-truthimages.
InceptionScore(IS) Theinceptionscore(IS)isdefinedby
(cid:18) (cid:20) (cid:21)(cid:19)
p(y|x)
exp E E log , (24)
x y|x p(y)
wherexandp(y|x)aretheimagedataandtheprobabilitydistributionobtainedfromxusingtheInceptionV3(Szegedy
etal.,2016)architecture.TheInceptionScorerepresentsthediversityoftheimagescreatedbythemodelintermsofthe
entropyforImageNetclasses.
Kernel Inception Distance (KID) The kernel inception distance (KID) is the maximum mean discrepancy (MMD)
metric on the intermediate feature space. Let the distribution of the ground-truth and generated images be p and p ,
g x
respectively.ThentheKIDisdefinedby
KID=E [k(x ,x )]+E [k(x ,x )]−2E [k(x ,x )], (25)
x1,x2∼px 1 2 x1,x2∼pg 1 2 x1∼px,x2∼pg 1 2
wherethesimilaritymeasurek(x ,x )isdefinedby
1 2
k(x ,x )=(x ·x )3. (26)
1 2 1 2
C.Additionalablationstudies
EMArate Sincetheflowmatchingandrectifiedflowismuchhardertoconvergethantheconventionaldiffusionmodel
becauseofmatchingpairsoftheindependentdistributions,wetunedtheEMArateintwoways:
• WehaveincreasedtheEMArateto0.999999,higherthanothermodels.(Table)showstheablationstudyontheEMA
rateoftheimagesynthesisqualityfromCIFAR-10datasetsfrombaseline1-rectifiedflowmodelfora1000-stepEuler
solver.
12SequentialRectifiedFlow
• Warm-up training policy. Fixing the EMA rate high makes converging the model almost impossible because of
theextremelyhighmomentum.So,weintroducethewarmupphasewithrespecttothetrainingstepas
EMA rate=min((1 + step) / (10 + step), decay) (27)
wheredecayandsteparethegivenEMArateandthetrainingsteps,respectively.Introducingthiswarmupphase
further improved the FID of our implementation in 1.3M steps and 128 batch size to 3.03 in CIFAR-10 dataset
generationusinga1,000-stepEulersolver.
Table4.Theperformanceofthe1-rectifiedflowmodelwithanEulersolverwith250uniformtimesteps,trainedwithbatchsize128,
1.3MstepsanddifferentEMArates.ThemodeldoesnotconvergewithEMArate0.999999.
EMArate 0.999 0.9999 0.99999 0.999999 Warm-up
FID 5.78 4.77 3.91 - 3.03
C.1.Ablationonthenumberofreflowdatasets
Liu et al. (2023) had reported the proper amount of number of reflow pairs for convergence should be at least 1M. For
completeness,wereporthowtheimagegenerationperformancedropsandoverfitswithasmallernumberofreflowdatasets.
With a small number of reflow pairs such as 10k, the performance is lower than the more-data cases and even becomes
worseasthenumberoftrainingstepselapses.
8 10k, 2div 10k, 2div
10k, 4div 10 1M, 2div
7 100k, 2div 10k, 4div
100k, 4div 1M, 4div
8
6 1M, 2div
1M, 4div
5 6
4
4
20000 40000 60000 80000 100000 0 20 40 60 80 100
Number of training steps
Number of Function Evaluations (NFE)
Figure8.ThegenerationperformanceofCIFAR-10datasetswithrespecttothenumberofreflowdatasets.Left:Numberoftraining
stepsvs.FID,Right:Numberoffunctionevaluationsvs.FID.
13
DIF DIFSequentialRectifiedFlow
D.ExampleImages
D.1.UncuratedCIFAR-10ImagesGeneratedbySeqRF-DistillModels
Figure9.Non-curatedCIFAR-10imagesynthesisresultwithk-SeqRFafterdistillation.Fromuptodown:{2,4,6,8,12}-stepEuler
solver,eachwithanFIDscoreof{4.08,3.30,3.19,3.20,3.21},respectively.
D.2.UncuratedSeqRFDatasetsforTrainingCIFAR-10Models.
WeprovideexamplesofSeqRF(orthena¨ıvereflowmethod)datasetsusedtotrainCIFAR-10datasetsinFigure10.The
imagesintheupperrowrepresenttheoriginalimagesusedtogeneratethesourceimageinthemiddlerow.Theimagesin
thelowerrowrepresenttheimagesgeneratedbytheODE-Solver,initiatedfromthesourceimageatthemiddlerow.
D.3.UncuratedLSUN-ChurchImagesGeneratedbySeqRFModels
Figure 11 demonstrates the images generated from rectified flow models that are pre-trained from Liu et al. (2023), and
fromourproposedk-SeqRFmodels.Exceptfork = 4case(4-SeqRF)whichrequiresatleast4stepstogenerateimages,
wepresentedimageswithNFE=(2,5,10),fromtoptobottomrows.Forreproducibilityissues,allimagesaregenerated
fromthesameseedofuniformGaussiannoises.
E.EquivalenceofFlowMatchingandConditionalFlowMatching
Inthissection,werecaptheequivalenceofgradientsofflowmatchingobjectiveEquation3andconditionalflowmatching
objectiveEquation4.Forfurtherinformation,refertoLipmanetal.(2023),Theorem2.
Proposition 6 (Equivalence of Equation 3 and Equation 4). Let p (x) > 0,∀x ∈ RD and t ∈ [0,1], where p is the
t t
marginalprobabilitypathoverx.Then,∇ L (θ)=∇ L (θ).
θ FM θ CFM
Proof. First,weremindthetwoequations.
(cid:104) (cid:105)
L (θ)=E ∥u (x,θ)−v (x)∥2
FM t,x1 t t 2
(28)
(cid:104) (cid:105)
L (θ)=E ∥u (x ,θ)−v (x |x )∥2 .
CFM t,x1,xt|x1 t t t t 1 2
14SequentialRectifiedFlow
(a)Nosegment. (b)2segments.
(c)4segments. (d)6segments.
(e)8segments. (f)12segments.
Figure10.Sequential reflow dataset for CIFAR-10. The {upper, middle, lower} rows represent the original data, source data (noisy
originaldata),anddestinationdata(ODEsolveroutputfromthenoisyoriginaldata),respectively.Equation16describesthedetailed
procedureofconstructingthe(source,destination)pair.
(a)1-RF(Liuetal.,2023). (b)2-RF(1-SeqRF).
(NFE,FID)={(2,315.3),(5,124.5),(10,45.0)}. (NFE,FID)={(2,123.8),(5,76.4),(10,54.6)}.
(c)2-SeqRF(Ours). (d)4-SeqRF(Ours).
(NFE,FID)={(2,104.4),(5,56.6),(10,45.7)}. (NFE,FID)={(4,35.8),(5,30.9),(10,28.1)}.
Figure11.Uncuratedimagesgeneratedbyafew-stepEulersolverfrommodelslearnedbyRFandSeqRF.Exceptforthe4-SeqRFcase
(suchthatNFE=4forthefirstrow),theNFEstogenerateimagesatthreerowsare2,5,and10,respectively.Forreproducibilityissues,
allimagesaregeneratedfromthesameseedofuniformGaussiannoises.
15SequentialRectifiedFlow
Fromdirectcomputation,wehave
∥u (x,θ)−v (x)∥2 =∥u (x,θ)∥2−2u (x)·v (x)+∥v (x)∥2
t t 2 t 2 t t t 2 (29)
∥u (x,θ)−v (x)∥2 =∥u (x,θ)∥2−2u (x)·v (x|x )+∥v (x|x )∥2.
t t 2 t 2 t t 1 t 1 2
(cid:104) (cid:105) (cid:104) (cid:105)
SinceE ∥u (x,θ)∥2 =E ∥v(t)∥2 ,thefirsttermattheright-handsideisremoved.Sincev (x)andv (x|x )is
x t 2 x1,x|x1 2 t t 1
ananalyticallycalculatedformthatisindependentofθ,theonlyremainingtermisthedotproducts.Then
(cid:90)
E [u (x)·v (x)]= p (x)(u (x)·v (x))dx
pt(x) t t t t t
(cid:90) (cid:18) (cid:90) v (x|x )p (x|x )q(x ) (cid:19)
= p (x) u (x)· t 1 t 1 1 dx dx
t t p t(x) 1 (30)
(cid:90) (cid:90)
= u (x)q(x )(u (x)·v (x|x ))dx dx
t 1 t t 1 1
=E [u (x)v (x|x )]
x1,x t t 1
wherethechangeofintegrationispossiblesincetheflowisadiffeomorphism.
F.SequentialStraightness
F.1.Implementationdetails
Wemeasuredthesequentialstraightness
S
(Z)=(cid:88)K (cid:90) ti (cid:13) (cid:13) (cid:13)Z ti
i
−Z ti
i−1 −
d Zi(cid:13) (cid:13) (cid:13)2
dt (31)
seq (cid:13) t −t dt t(cid:13)
i=1
ti−1(cid:13) i i−1 (cid:13)
bythefollowingprocedure.
(1) Randomdrawthetimeinterval[t ,t ]withi∈[1:K],asintheSeqRFalgorithm.
i−1 i
(2) Sampletheoracleinputattimet asZ =(1−t )X +t X .
i ti i 0 i 1
(3) ThenruntheODEsolvertocollectallsamplewithin[t ,t ].
i−1 i
(4) Z =Xˆ ,thatis,theterminalvalueofthereverse-timeODEsolverattimet .
ti−1 ti−1 i−1
(5) Thevectorfieldiscalculatedas
Zti−Zti−1,andthederivativeisdefinedasthedifferencebetweenthevaluesoftwo
ti−ti−1
adjacentODEsolvertimesteps,dividedbythesizeoftimesteps.
F.2.Sequentialstraightnessovert.
InadditiontoFigure7,weprovidethesequentialstraightnessovertimefork ={2,4,8,12}inFigure12.
G.Proofs
Before we take account of the error bound of the ODE with respect to the Lipschitzness of the ODE, we first define the
localandglobaltruncationerror.
Definition 2 (truncation error of the numerical method). Let the ODE be defined as in (10). Then the (local) truncation
errorattimetwithtimestephisdefinedby
x(t+h)−x(t)
τ(t,h)= −f(x(t),t). (32)
h
Theglobaltruncationerrorattimec∈[a,b]isdefinedby
E(c)=x(c)−x . (33)
c
16SequentialRectifiedFlow
Figure12.Sequentialstraightnessresultsofk-SeqRFwithk={1,2,4,12},fromupperlefttolowerright.
Figure13.UncuratedrandomCelebAimageschosenfromrandompermutationsampledwithk-SeqRFwithoutdistillation.Fromupto
down:{1,2,4}-stepEulersolverusingk-SeqRFmodels.
Definition 3 (Bound of local truncation error of the ODE solver). Let the ODE be defined as in (10). Then if the local
truncationerrorisboundedby
τ(t,h)≤O(hp) (34)
forsomenumericalsolverODE-Solver(x ,t ,t ;v )forsomer,i.e.,
t1 1 2 θ
|τ(t,h)≤Khpforsome0<h≤|h | (35)
0
forsomefiniteh ,thenthissolveriscalledtohaveorderofaccuracyp.
0
G.1.ProofofTheorem2
Considerthegeneralone-stepmethod
x =x +hv(x ,t) (36)
t+h t t
wherevisacontinuousfunction.AndLetM (t)beitssupremumdilation,(e.g.,maximumLipschitzconstantattimet),
sup
thatis,
∥v(x ,t)−v(x′,t)∥≤M (t)∥x −x′∥. (37)
t t sup t t
forall{(t,x ),(t′,x′)}inaboundedrectangle
t t
D ={(t,x):t∈[a,b],∥x−x ∥≤C} (38)
0
17SequentialRectifiedFlow
forsomefiniteconstantC ≥0.
Then,theglobaltruncationerrorE isboundedby
(cid:90) c (cid:18)(cid:90) c (cid:19)
E(c)≤ τ(t)exp M (t′)dt′ dt. (39)
sup
t=a t′=t
forallc∈[a,b].
Proof. wefirstrewrite(32)as
x(t+h)=x(t)+hf(x(t),t)+hτ(t), (40)
thenweget
E(t+h)=E(t)+h[f(t,x(t))−f(t,x )]+hτ(t). (41)
t
ThenbytheLipschitzcondition,
E(t+h)≤E(t)+hM(t)E(t)+hτ(t), (42)
followingthat
E(t+h)≤(1+hM(t))E(t)+hτ(t). (43)
Byinductionthroughalltimesteps{a,a+h,··· ,a+Kh},
K−1 K−1
(cid:88) (cid:89)
E(t+Kh)≤ hτ(t+ih) (1+hM(t+jh))
i=0 j=i
(44)
K−1
≤
(cid:88) hτ(t+ih)e(cid:80)K j=− i1hM(t+jh).
i=0
Whenwetakeinfinitesimallimit,i.e.,h→0,withK → c−a,then
h
(cid:90) c (cid:90) c
E(c)≤ τ(t)exp M(t′). (45)
t=a t′=t
Asaspecialcase,letT =max τ andM(t)=M forallt∈[a,c],then
c c∈[a,c] c
T
E(c)≤ [exp(M(c−a))−1]. (46)
M
G.2.ProofofLemma3
Proof. LettheintervalofasinglestepoftheODEsolverbeh.Thenthelinearmultistepmethodisdefinedas
p p
(cid:88) (cid:88)
x t+h = α jx t−jh+h β jv t(x t−jh,t−jh), i≥p (47)
j=0 j=−1
where the interval of a single step is h, t = t + hi stands for i-th time step, and v is the vector field, or the drift
i 0 t
function. Let the linear multistep method have the convergence order of r. Then according to the Dahlquist equivalence
theorem,theglobaltruncationerroroftheODEsolverhastheorderofO(hr).Incaseofthesequentialreflowalgorithm
withthesameNFE,thesingleintervalofasinglestepisgivenas h whereK isthenumberofequidistribtedintervalsthat
K
divide[a,b]intotimesegments.ThentheglobaltruncationerroroftheODEsolverofthesequentialreflowalgorithmisof
O(cid:16)(cid:0)h(cid:1)r(cid:17)
.AswehaveK
segments,theorderoftheglobaltruncationerrorisK×O(cid:16)(cid:0)h(cid:1)r(cid:17)
=K1−rO(hr).
K K
18SequentialRectifiedFlow
H.Morerelatedworks
DesigningProbabilityFlowODE AsaproblemofdesigningODEsasprobabilitypath,Chenetal.(2018)hasopened
upanewwaybyshowingthatanODEcanbelearnedwithneuralnetworkobjectives,andGrathwohletal.(2019)further
appliedthisforthegenerativemodels.Generatingtheinvertibleflowhasbeenalsoachievedasarchitecturedesign(Kingma
&Dhariwal,2018)andfromautoregressivemodel(Papamakariosetal.,2017;Huangetal.,2018).Thoseearlymethods,
however,arecomputationallyinefficient,hadmemoryissue,ornotcompetitiveintermsofsynthesisqualityingenerative
modelliterature.
Continuous-Time Generative Modeling As a family of continuous-time generative modeling by learning dynam-
ics,Sohl-Dicksteinetal.(2015)interpretedthegenerativemodelingastheconstructionofthevectorfieldofthestochastic
processes.Lateron, Hoetal.(2020);Song&Ermon(2019);Songetal.(2021b)proposedefficienttechniquesfortraining
this by interpreting the reverse stochastic process as the denoising model. According to the diffusion models trained on
SDEs,Songetal.(2021a)proposedanefficienttechniquebysamplingfromageneralizednon-Gaussianprocess,which
resultsinafastdeterministicsamplingspeed.Further, Kingmaetal.(2021);Huangetal.(2021)unifiedtheframeworkof
continuous-timemodels,includingdiffusionmodelandflow-basedgenerativemodels.
19