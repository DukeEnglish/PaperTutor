Modelling Human Values for AI Reasoning
Nardine Osmana, Mark d’Invernob,a
aArtificial Intelligence Research Institute (IIIA-CSIC), Barcelona, Catalonia, Spain
bGoldsmiths, University of London, London, UK
Abstract
Oneoftoday’smostsignificantsocietalchallengesisbuildingAIsystems
whosebehaviour,orthebehaviouritenableswithincommunitiesofinteract-
ing agents (human and artificial), aligns with human values. To address this
challenge, we detail a formal model of human values for their explicit com-
putational representation. To our knowledge, this has not been attempted
as yet, which is surprising given the growing volume of research integrating
values within AI. Taking as our starting point the wealth of research inves-
tigating the nature of human values from social psychology over the last few
decades,wesetouttoprovidesuchaformalmodel. Weshowhowthismodel
can provide the foundational apparatus for AI-based reasoning over values,
and demonstrate its applicability in real-world use cases. We illustrate how
our model captures the key ideas from social psychology research and pro-
pose a roadmap for future integrated – and interdisciplinary - research into
humanvaluesinAI.Theabilitytoautomaticallyreasonovervaluesnotonly
helps address the value alignment problem but also facilitates the design of
AI systems that can support individuals and communities in making more
informed, value-aligned decisions. More and more, individuals and organi-
sations are motivated to understand their values more explicitly and explore
whether their behaviours and attitudes properly reflect them. Our work on
modelling human values will enable AI systems to be designed and deployed
to meet this growing need.
Keywords: human values, value representation, formal modelling, social
psychology
1. Introduction
Acrossgovernments, industryandthegeneralpublic, thereisanincreas-
ing recognition of the urgency for ethical approaches to AI (as evidenced by
February 12, 2024
4202
beF
9
]IA.sc[
1v95360.2042:viXrathe numerous publications of ethics guidelines, e.g. [25, 26, 36, 37, 86]). In
academia, a growing body of research investigates the role of human values
in designing ethical AI [12, 31, 74, 90]. Indeed, one of our leading AI re-
search luminaries, Stuart Russell, believes the overarching goal of AI should
changefrom“intelligence”to“intelligenceprovablyalignedwithhumanval-
ues” [74]. This call to arms gave birth to the value alignment problem.
This challenge of engineering values into AI in response to the value
alignment problem has resulted in a range of research areas: how human
values can be learnt [43, 44, 45, 91]; how individual values can be aggre-
gated to the level of groups [41]; how arguments that explicitly reference
valuescanbemade[7]; howdecisionmakingcanbevalue-driven[14,17,21];
how online institutions can ensure value-aligned behaviours in hybrid com-
munities [56, 57]; and how norms are selected or synthesised to maximise
value-alignment [55, 80, 83].
Yet despite these efforts, no formal model of values exists today that
provides a concrete foundational platform from which data structures and
algorithms can be designed to build AI architectures that address the value-
alignment problem. In response, we propose such a model built on the
following guiding principles:
1) we employ a formal language to be precise about modelling values and
related concepts [23, 47];
2) weconstructtheformalcomponentsofthismodeltoprovidethefoun-
dations for the data structures and algorithmic design that will enable
value-based reasoning;
3) we design the model to be agnostic on any specific implementation
of values, though we do provide example implementation scenarios to
illustrate the model’s ubiquity and practical applicability;
4) we set out the model to subsume and relate to established concepts in
AI research as much as possible;
5) we provide illustrative examples of building data structures and algo-
rithmsenablingvalue-basedreasoningtakenfromourongoingresearch
applied to real-world use cases;
6) we ensurethe model draws upon thewealth ofwork fromwithin social
psychology and explicitly demonstrate the grounding of our model
within this research; and
27) as with AI research, we also design the model to remain agnostic on
any specific theory of values from social psychology but demonstrate
how our model incorporates the agreed conceptual underpinnings of
this field.
The ongoing impact of this work comes through our partnership with
a team of medical doctors at Hospital del Mar, Barcelona. Through this
collaboration, we make use of our formal model of values to interrogate
and develop a greater structural understanding of their four overarching
bio-ethical values [6] (beneficence, non-maleficence, autonomy, and justice).
With those four values formally defined, we are now developing AI that pro-
videsfeedbackonthevaluealignmentofmedicaldecisions, leadingtobetter
value-aware decision-making in the hospital. We are also developing mech-
anisms to analyse their medical protocols and their resulting behaviours
from the perspective of alignment with these four basic values, allowing us
to provide feedback on when and how these protocols can evolve to im-
prove greater value alignment. This partnership, along with other ongoing
value-identifying “research in the wild” collaborations, helps us ensure the
practicality of our proposed formal model —theory and practice are thus
tightly coupled.
The narrative of this paper is set out as follows. Section 2 motivates
using values as critical modelling and architectural concepts for symbolic
AI systems. It outlines why values are considered vital to understanding
human behaviour and why explicit modelling and computational reasoning
of human values is needed in AI. Section 3 explores the nature of human
valuesfromsocialpsychology,providingthebackgroundandcanvasforusto
engineeranewformalmodelintoexistence. Weanalysedefinitionsofhuman
values from this work and consider their commonalities and distinctions.
Armed with this knowledge, we then set out the details of our formal model
in Section 4. We formally define values by introducing value taxonomies,
which cover concepts including value relations, value importance, and value
semantics. We discuss how individuals or groups may hold values, how
values change with context and time, and how our proposed model can
supportreasoningaboutthevaluealignmentofbehaviours. Withtheformal
model in place, we undertake, in Section 5, a detailed mapping back to
social psychology to assess the relationship of our modelling choices with
this research. This is a critical step in ensuring and demonstrating exactly
how our model is consistent with the research in this field. In Section 6, we
present a roadmap of AI research that needs to be undertaken to achieve
value-alignedbehaviour. Weclosewithareflectiononthecontributionsand
3further work in Section 7 along with some concluding remarks in Section 8.
2. Promoting Values as a fundamental construct for AI
Human values have been heavily investigated within the humanities and
socialsciencestounderstandtheirroleinbehaviourchoiceandourattitudes
towards various states of affairs. Schwartz, one of the prominent social psy-
chologists working in the field of human values and well-recognised for the-
ories of intrinsic human values, stated that “theorists have long considered
values central to understanding social behaviour... because they view val-
ues as deeply rooted, abstract motivations that guide, justify, and explain
attitudes, norms, opinions, and actions” [75].
Like Scharwtz and many AI researchers, we recognise the criticality of
factoring in values for automated reasoning. This is necessary both to guide
the operation of AI itself and to build AI systems that support individual
and collective human decision-making. Traditionally, AI has used differ-
ent human-associated qualities to guide behaviour, from calculating reward
or utility (such as game theory [11, 58]), or following (or not) the norms
of a community or society (normative systems [1, 82]), to the internal be-
liefs, goals and intentions inspired by cognitive sciences (agent BDI mod-
els [20, 22, 68]), to name a few. However, as AI becomes increasingly preva-
lent in our everyday lives and issues arise around the trust we have in AI
systems, it is becoming increasingly critical to consider how we design AI
systems that can explicitly reason over human values. Significant research
has made a compelling case for using human values in the design of ethical
artificial systems [12, 31, 74, 90]. There is a need to design artificial systems
that can explicitly reason about values to determine behaviour choices and,
subsequently, communicate to various stakeholders how that value-based
reasoning occurred. But how can we best incorporate values into the range
of other primary constructs in AI that guide behaviour, such as norms or
beliefs, for example?
In Scharwtz’s article on the Theory of Basic Values [77], he asks, “How
do values relate to attitudes, beliefs, traits and norms”? He states, ”[w]hen
trying to explain why individuals behave as they do, people often refer to
attitudes, beliefs, traits, or norms.” We agree that the constructs guiding
human behaviour are numerous, resulting in complex decision-making pro-
cessesthatselectcertainbehavioursoverothers. AsweillustrateinFigure1,
there are many factors, both at the individual and at the collective level,
which influence action choice and the resulting behaviour.
4Examples are an individual’s traits, beliefs, goals, norms and values, or
the values and norms of collectives. This list is not exhaustive, but it aims
to illustrate the complexity of the decision-making process individually and
collectively.
Investigatingthedynamicsbetweentheconstructsinfluencingbehaviour
is an issue that requires further scrutiny, as we later discuss in our roadmap
in Section 6. Here, we argue that values should be elevated to a primary
construct alongside beliefs, intentions, and norms (some of the constructs
traditionallyconsideredinAI-drivendecision-making). Wedosobydrawing
on the range and depth of investigations into values from social psychology.
We introduce values as a critical motivator of behaviour choice in Fig-
ure 1. In the past, research into AI considered mechanisms for behaviour
choice with respect to beliefs, goals and intentions, as well as norms and
maximising utility. One question was how behaviours can be understood in
terms of (or aligned with) these key constructs. Now, research efforts aim
to understand how values influence decisions about behaviour and how this
behaviour aligns with human values.
But how can value alignment of behaviour with values be analysed?
Figure 1 illustrates how the actions of human or artificial agents change
the world. They result in observable outcomes that can be used to eval-
uate chosen behaviour. Note that we use the words action and behaviour
interchangeably in this paper.
If the outcome of a chosen behaviour results in a state of the world that
Figure 1: Influencers of behaviour, and evaluating behaviour w.r.t. its alignment with
values
5respects, upholds or promotes a given value, we say that the behaviour that
brought about that outcome is aligned with that value. Furthermore, it
means that the processes which selected that action/behaviour —i.e. what-
ever constructs were used to make that behaviour choice, be it a norm, a
personal belief, some combination, and so on— are also aligned with that
value. Inotherwords,byevaluatingtheoutcomeofbehaviour(bottomlayer
in Figure 1), we can extrapolate to assess the behaviour itself (middle layer)
and the constructs bringing about that behaviour (top layer).
Evaluating a given behaviour and the decision-making process that pro-
duces it opens the door to AI (as well as AI-supported human) deliberation
about why decisions were made and how they should be made in the future
by considering the role of values.
We end this section by noting that the motivation for introducing values
intoAIreasoninganddecision-making, aspresentedinthediscussionabove,
is based on assuming the capability of evaluating outcomes (or states of the
world)withrespecttovalues. However, anexplicitcomputationalmodelfor
values is needed to achieve this. This is precisely the aim of this paper and
will be presented in Section 4. But before we do, we first consider research
intovaluesfromsocialpsychologytohelpusunderstandwhatvaluesareand
which will provide the foundation from which we design our computational
model.
3. UnderstandingInvestigationsinHumanValuesfromSocialPsy-
chology
A significant body of work has set out to define or characterise val-
ues from the humanities and social sciences. Rohan [71] acknowledges
that “[d]efinitional inconsistency has been epidemic in values theory and
research”, and that “[t]he status of values theory and research suffers be-
cause the word values is open to abuse and overuse by non-psychologists
and psychologists alike.” Kluckhohn [38] expands further on this:
“Reading the voluminous, and often vague and diffuse, litera-
ture on the subject in the various fields of learning, one finds
values considered as attitudes, motivations, objects, measurable
quantities, substantiveareasofbehavior, affect-ladencustomsor
traditions, and relationships such as those between individuals,
groups, objects, events.”
Assuch,beforeproposingourformalmodelforvalues,wefirstinvestigate
the range of different value definitions from the values literature. Figure 2
6provides a selection of those definitions from two review papers from social
psychology [13, 71]. We have set out to colour code this diagram to iden-
tify the various qualities characterising values and to support the reader in
understanding where consensus lies or does not.
• The text highlighted in yellow identifies the different characterisations
ofvaluesaseitherconceptions[34,38],beliefs[27,72,76],principles[9],
or convictions [88].
• Thetexthighlightedinorangeillustrateshowtheoristsagreeonvalues
referringtoeitheroutcomesoractions. Withinoutcomes, thedifferent
terminology used includes end states [72, 76] or goals [9, 27] (also
referred to as ends of action [34, 38]). Within actions, the different
terminology used includes referring directly to actions [35, 9] or ways
of behaving [27] (also referred to as modes of conduct [9, 72, 76], or
modes and means of action [34, 38]).
• The text in green highlights the various proposals on the purpose of
values. It includes guiding or influencing behaviour [42]. This is also
referred to as influencing the selection of modes, means and ends of
actions[38,34],orguidingselectionorevaluationofbehaviour,people,
and events [76]. Other values-related purposes include leading a good
life or realising a good society [88].
• The blue text identifies how researchers describe what is desirable,
preferable or important [9, 27, 30, 34, 38, 72, 76].
• The text highlighted in pink illustrates how values apply to the level
of the individual and to that of groups, communities or collectives [9,
30, 34, 38, 72, 88].
In our proposal for defining values, we remain neutral on whether val-
ues are considered beliefs, principles, convictions, or anything else identified
above or in other research. However, we adopt the other aspects of val-
ues outlined above, as many theorists’ outputs show significant agreement.
Namely, values guide behaviour by considering what actions or outcomes
are desirable or essential for either individuals or collectives.
In other words, we propose our formal model for value representation
by distilling the consensus of social psychology and building from there.
This will allow us to meet one of our guiding principles described in the
introduction; we do not identify with any specific theory of values (principle
7). We do not, for example, take Schwartz’s defined set of ten “universal”
7Kluckhohn [38, p.395]. A value is “a conception, explicit or implicit, dis-
tinctive of an individual, or characteristic of a group, of the desirable which
influences the selection from available modes, means, and ends of action”.
Lewin [42, p.41]. “Values influence behavior but have not the character of a goal (i.e.,
of a force field)... the individual does not try to ‘reach’ the value of fairness, but fairness
is ‘guiding’ his behavior... values are not force fields but they “induce” force fields.”
Guth and Tagiuri [34, p.124–125]. “A value can be viewed as a conception, explicit
or implicit, of what an individual or a group regards as desirable, and in terms of which
heortheyselect,fromamongalternativeavailablemodes,themeansandendsofaction”.
Hutcheon [35, p.184]. “... values are not the same as ideals, norms, desired objects,
or espoused beliefs about the ‘good’, but are, instead, operating criteria for action...”.
Rokeach [72, p.5]. “A value is an enduring belief that a specific mode
of conduct or end-state of existence is personally or socially preferable
to an opposite or converse mode of conduct or end-state of existence”.
Schwartz [76, p.20]. A value is “a belief pertaining to desirable end
states or modes of conduct that transcends specific situations; guides selec-
tion or evaluation of behavior, people, and events; and is ordered by the
importance relative to other values to form a system of value priorities”.
Feather [27, p.222]. “I regard values as beliefs about desirable or undesir-
able ways of behaving or about the desirability or otherwise of general goals.”
Braithwaite and Blamey [9, p.364]. “Values... are principles for ac-
tion encompassing abstract goals in life and modes of conduct that an in-
dividual or a collective considers preferable across contexts and situations”.
Friedman et al. [30, p.349]. “A value refers to what
a person or group of people consider important in life”.
van de Poel and Royakkers [88, p.72]. Values are “lasting convic-
tions or matters that people feel should be strived for in general and not
just for themselves to be able to lead a good life or realize a good society”.
Figure 2: A selection of value definitions, adapted from [13, 71]. Text highlighted in
yellow describes the nature of values, orange describes what values refer to or the issues
they address, green describes the purpose of values or what they are for, blue highlights
the notion of what is important or desirable, and pink describes who holds values or to
whom they apply.
8human values [77], or any other value theory [13], as the definitive theory.
We follow in the footsteps of some value-sensitive design approaches [19],
which argue that values are open-ended and should be elicited bottom-up
fromstakeholders. Whilstthereisevidenceforhumanssharingfundamental
human values (as illustrated by Schwartz [77]), there is also evidence that
new values are continuously emerging with new application domains and
technologies (as shown by van de Poel [87]). Our ongoing fieldwork and
collaboration with stakeholders in real-life use cases from various domains
further validated this. For example, Subsection 4.2.2 illustrates how a social
networking app can prioritise different values for different communities, and
our work with firefighters illustrates how values may also change from one
geographical location to another (such as the values of the firefighters in
Costa Rica being categorically different than those in Catalonia). In sum-
mary, the view that values may change is central to our model; hence, we
must remain agnostic toward the selected value theory and accommodate
value change.
We now move to a presentation of our formal model in Section 4. This
willthenbefollowedbyathoroughanalysisofthealignmentofourproposal
with selected works from social psychology in Section 5.
4. A Formal Computational Model for Human Values
Taking as our starting point the body of research from social psychology
described in the previous section, we set out to build a formal computa-
tional model for representing human values. This computational approach
for representing values provides the foundations for future developments of
computational mechanisms that reason over values, promoting the ability of
artificial systems to make value-aware decisions.
The proposed computational model of human values is presented in four
subsections. Thefirstsubsectiondetailsaformaldefinitionofvaluesthrough
the notion of value taxonomies, which covers concepts like value relations,
value importance, and value semantics. The second subsection extends our
model to consider the widely held belief that values change over time, pro-
vidinganinitialaccountofthedynamicnatureofvalues. Toachievethis,we
introduce the notion of contexts, enabling agents to evaluate their current
values within their current situation to determine future behaviour. The
third discusses modelling values for individuals and groups of individuals,
which we call collectives. In our proposal, a collective is a hybrid mul-
tiagent system containing artificial and human individuals, such as online
communities or other organisational setups. The final subsection considers
9the problem of ensuring the extent (or degree) to which the behaviours of
individuals or collectives are aligned with an agreed set of values; the value
alignment problem. A formal description of value alignment is then speci-
fied, demonstrating some of the utility of our model, which is the capability
to formally capture some of the key research questions in our research field.
Each of those four subsections is divided into three parts: our proposal,
a discussion of implementation choices arising from the model, and a run-
ning descriptive example of a system we have developed that demonstrates
a concrete implementation choice. The running example is introduced to
support the reader in understanding our proposed foundational model and
its significance.
4.1. What are human values? A computational response to value represen-
tation
As per Section 3, we take values to be human abstract concepts that
guidebehaviourandwhoseexactmeaningandinterpretationvarybothwith
the current context and over time. However, a concrete computational rep-
resentation of values requires a machine to reason with values. For example,
while one may talk about fairness in general, for a specific application sup-
porting a mutual aid community, fairness might be understood as “one does
not ask for help more than what they volunteer”. That is, the abstract con-
ceptoffairnessacquiresaconcretedefinition(groundingsemantics)through
apropertywhosesatisfaction(ordegreeofsatisfaction)canbeautomatically
verified in a given system (more on this example in Subsection 4.1.2). In
this case, the property specifies that a user does not ask for more help than
that which they volunteer.
Of course, there may be different levels of abstraction and grounding se-
mantics for a value. For example, say the application was to be adopted by
another community where volunteers explicitly support older people. The
new community might find the old view of fairness —that one only asks for
help precisely what they have volunteered— unsuitable because this com-
munity expects to have volunteers who support older people. In contrast,
older people usually only ask for help with their day-to-day tasks without
volunteering. As such, this new community should state that fairness is
not about balanced give and take but about balanced volunteer workload
division. Here, we encounter different levels of abstraction and grounding
properties, illustrated in Figure 3.
The top node in Figure 3 presents the abstract concept of fairness; the
middle nodes show different concepts of fairness that are more specific than
the top node, (such as reciprocity, which is understood as balanced give and
10Figure 3: Different levels of abstraction for the value fairness, with various grounding
semantics: numbers indicate node importance
take, or fair treatment that may be defined in terms of balanced division of
workload), highlighting different abstraction levels and interpretations, and
the bottom nodes present properties that ground the semantics of abstract
concepts and allow for a computational evaluation of values. For example,
the bottom left node represents the grounding semantics of fairness for the
mutual aid community. Conversely, the bottom right node represents the
grounding semantics of fairness for the community supporting older peo-
ple. We use square nodes for nodes that ground the semantics of abstract
concepts.
This interplay between the abstract and the grounding semantics of val-
ues is reflected through the relations between nodes, as shown in Figure 3.
Different abstraction levels may exist for value concepts and grounding se-
mantics that enable computational approaches over values.
Another key concept in social psychology is that of value priority, which
determines how important a value is for individuals or collectives. In other
words, it is not just the semantics of fairness that influences the behaviour
of an individual or a group, but how important fairness as a value is for
that individual or group. As Schwartz articulates in [77]: “Values influence
action when they are relevant in the context... and important to the actor”.
We model value importance by attaching a measure of importance to each
node of Figure 3. As for Schwartz’s reference to value relevance, that is
defined in terms of value importance, as we illustrate in Subsection 4.2.
Given our view of values having different abstraction levels and different
grounding semantics, we propose values to be defined through taxonomies
where the general concept becomes more specific as one travels down the
taxonomy, and it becomes concrete and computational at (some) leaf nodes.
This is consistent with research in value-sensitive design on value change
taxonomies [87], as well as more traditional value theories that introduce
11value hierarchies, such as Schwartz’s Theory of Basic Values [77] (Subsec-
tion 5.2 elaborates in detail on the alignment of our proposed taxonomy
with some of the works in social psychology).
Furthermore, using taxonomies allows for easy navigation between ab-
stract and concrete grounding semantics of values, which is especially useful
when deciding how to define values, how they evolve, and how to deliberate
and negotiate with others about these values (more on this in Section 6).
It may be argued that all a machine requires are the property nodes to
ensure behaviour alignment with values (as illustrated in Subsection 4.4).
However, in the case of a new community emerging, such as the community
of volunteers supporting older people, we saw how the new community had
its local and distinct view of the fairness value. A taxonomy representation
makes the deliberation process on the different semantics of fairness and the
evolution of these semantics over time and context possible. We expect se-
manticstocontinuouslyevolvewithnewinteractionrequirements, asseenin
Subsection 4.2. Moreover, a taxonomy can support the deliberation process
over values and their importance.
We conclude this discussion with Definition 1, which proposes represent-
ing values through taxonomies by defining a value taxonomy V as a triple
of nodes N (where nodes can be of two types, nodes representing abstract
concepts N and property nodes that ground abstract concepts N ), edges
l ϕ
between those nodes E that describe the relations between value concepts
(which is more general/specific than which), and an importance function I
which is a total or a partial mapping from the nodes to a number.
Definition 1 (Value taxonomy). A value taxonomy V = (N,E,I) is de-
fined as a directed acyclic graph, where:
1. The set of nodes N = N ∪ N represents value concepts, and it is
l ϕ
composed of two types of nodes: i) those that are specified through la-
bels, with N ⊂ L representing the set of label nodes and L is the set of
l
all value labels representing abstract value concepts like ‘fairness’ or
‘reciprocity’; and ii) those that are specified through concrete proper-
ties, with N ⊂ Φ representing the set of property nodes and Φ is the
ϕ
set of all value properties whose satisfaction can be automatically ver-
ified at different world states, such as having the number of times one
offers help in a mutual aid community to be greater than the number
of times one asks for help.
2. The set of edges E : N ×N is a set of directed edges (n ,n ) ∈ E that
p c
represent the relation between value concepts n and n (the parent
p c
12and child nodes, respectively) illustrating that the value concept n is
p
a more general concept than n .
c
3. The importance function I : N → CD either assigns an importance
⊥
measure from the codomain CD to value concepts in N, or assigns ⊥
to value concepts when their importance measure is undefined.
Note that we specify the value taxonomy as a directed acyclic graph,
as opposed to the more traditional taxonomy tree, because certain value
concepts (nodes) may be used to narrow the understanding of more than
one general concept: i.e. one value concept (node) may have more than
one parent node. For example, the value ‘equal treatment’ in the taxonomy
of Figure 4 can act as a more specific concept for both the ‘social justice’
and ‘equality’ values (where the social justice and equality values and their
being examples of the universalism value are taken from Schwartz’s value
hierarchies [77]).
Figure 4: Some value concepts may have more than one parent node
We also require property nodes to be restricted to leaf nodes. In other
words, the concrete grounding semantics of a value concept (specified as a
propertynode)cannotbemoregeneralthananothervalueconcept(specified
asanotherpropertyorlabelnode). Forexample,inFigure3,thepropertyof
having one’s help requests proportionate to their volunteering offers cannot
bemoregeneralthanabstractconceptssuchasbalancedgiveandtake. This
requirement is defined formally as follows by stating that there is no pair of
parent/child nodes in any set of edges where the parent node is a property
node:
∄ (n ,n ) ∈ E ·n ∈ N
p c p ϕ
As we illustrate next, we introduce this condition to simplify the construc-
tion and interpretation of value taxonomies. Recall that the children nodes
of any parent are more specific than that parent. Conceptually speaking,
it is arguably paradoxical for concrete property nodes to be parents of ab-
stract value concepts specified through labels. However, we may allow a
13property node to be a parent node of another property node if we ensure
that the parent node’s property subsumes the child node’s property. This,
however, would require mechanisms that check the subsumption of proper-
ties. Furthermore, we illustrate in Subsection 4.4 how property nodes are
used to assess value alignment. Value alignment is computed based on the
satisfaction of the different property nodes. Suppose property nodes may
appear anywhere in a taxonomy. In that case, a question that arises is
whether all property nodes should be treated equally or whether the depth
of a property node might affect its weight and, hence, the value alignment
computation. For all these reasons, we choose to simplify the structure of
value taxonomies for the time being by restricting property nodes to leaf
nodes. Future work may relax this requirement if property subsumption
mechanisms are introduced and value alignment mechanisms are designed
to consider the location of property nodes in a taxonomy.
Coherence is an important property regarding value importance within
a well-defined taxonomy. We articulate this coherence by requiring that a
parent node’s importance be aligned with the importance of its child nodes.
For example, if the importance of all children nodes is low, then the im-
portance of the parent node cannot be high, and vice versa. We formally
define the coherence of value importance to hold in a taxonomy if and only
ifallparentnodes’importancemeasuresareanaggregationoftheirchildren
nodes’ measures:
Definition 2 (Coherence of value importance). Importancewithinavalue
taxonomy V = (N,E,I) is said to be coherent if and only if, for all nodes
n ∈ N with children nodes (there exists (n,n ) ∈ E), the importance of the
c
parent node is an aggregation of the importance of its children nodes:
∀n ∈ {n p|(n p,n c) ∈ E} · I(n) = A I(n′) (1)
n′∈Xn
where X = {n |(n,n ) ∈ E} is the set of all children nodes of n, and
n c c
A : CDm → CD is an aggregation function that takes a set of size m ∈ N∗
ofimportancemeasuresinCD (specifiedasCDm)andreturnsanaggregation
of those measures, where the aggregation also falls in the same range CD.
The importance measures of some of the taxonomy nodes might either
be provided manually by humans or learned from other sources, such as
past interactions or experiences. A coherence mechanism is then needed to
ensure importance measures are coherent within a taxonomy. Furthermore,
propagationmechanismscouldbeconstructedtocalculatetheimportanceof
14nodes that have not been provided, building on existing propagation mech-
anisms [60, 61]. Any propagation mechanism must respect the coherence of
value importance within the taxonomy.
Let us now consider the aggregation function A. We argue that symme-
try, idempotence and monotonicity, which we define below, are some of the
desirable properties to be held by any such function. Symmetry states that
the ordering of the importance measures being aggregated does not matter;
it is precisely what we require from our model. For instance, when calcu-
lating a parent node’s importance, ordering its children nodes’ importance
measures should not affect the aggregation (symmetry). Idempotence states
that the aggregation of several instances of the same measure results in that
measure, which is also what we require from our model. If we assume all
children nodes of some parent node to have the same importance i, then we
believe the parent node should share that importance too (idempotence). It
should neither be more important nor less important than i. Monotonicity
states that any increase in the importance of aggregated measures implies a
non-decrease with respect to the aggregated value, which is, again, what we
require from our model. Increasing any importance measure of the children
nodes should not decrease the parent node’s importance (monotonicity).
Formal definitions of these properties follow.
Property 1 (Symmetry of aggregation). The aggregation function A
is symmetric if, for all sets of importance measures λ ∈ CDm and all per-
mutations π ∈ Π of those sets (where Π is the set of all permutations of
λ λ
λ), we have:
A(λ) = A(π)
The symmetry property states that the ordering of aggregated values does
not matter.
Property 2 (Idempotence of aggregation). AnaggregationfunctionA
is idempotent if, for all importance measures i ∈ CD, we have:
A(i,...,i) = i
The idempotence property states that aggregating several instances of the
same measure will return that same measure.
Property 3 (Monotonicity of aggregation). AnaggregationfunctionA
is monotonous if, for all sets of importance measures λ,λ′ ∈ CDm, we have:
( ∀ 0 < p ≤ m · λ ≤ λ′ ) ⇒ A(λ) ≤ A(λ′)
p p
where λ represents the element in position p of the set λ.
p
15The monotonicity property states that if the measures in set λ are smaller
than or equal to the measures of λ′ (per position), then the aggregation of
the measures of λ should be smaller than or equal to the aggregation of the
measures of λ′.
Our requirements for Properties 1–3 help define the type of aggrega-
tion operator, as we show next. First, from [49, p. 14], we know that the
idempotence and monotonicity properties imply compensativeness.
Proposition 1. If an aggregation function A satisfies the idempotence and
monotonicity properties (Properties 2 and 3), then A is a compensative
aggregation.
Compensative operators are the class of aggregation operators that fall out-
side the classes of conjunctive and disjunctive operators. These operators
are limited between the min and max, which are the bounds of the t-norm
and t-conorm families (the operations that generalise the logical conjunc-
tion and logical disjunction). This implies that for any set of importance
measures (for all λ ∈ CDm), the aggregation will fall between the minimum
and maximum measures in that set:
minλ ≤ A(λ) ≤ maxλ
We believe falling between the minimum and maximum is appropriate for
our context: a parent node should not be more important than its most
important child node nor less important than its least important child node.
Furthermore, from [49, p. 13], we also know that compensative aggrega-
tion operators that satisfy the symmetry and monotonicity properties are
averaging operators.
Proposition 2. If a compensative aggregation function A satisfies the sym-
metry and monotonicity properties (Properties 1 and 3), then A is an aver-
aging operator.
As such, we propose A to be an averaging operator, though the exact
choice of this operator is left for implementation.
4.1.1. Implementation Choices
Specifying property nodes. We use properties to specify the grounding se-
mantics of values, as properties have traditionally been used to describe the
worldstate,andtheirsatisfactioncanbecomputed[84]. Theexactlanguage
usedforspecifyingpropertiesisanimplementationdecision. Ourproposalis
agnosticregardingthechoiceoflanguage: weintentionallydonotattachour
16proposal to any implementation language or decision. Our framework can
encompass any theory written in propositional, first-order, deontic, modal
and other logics.
We note that the use of properties might initially appear to embody a
consequentialistview, whereonlytheoutcomeofbehaviouriswhatmatters.
However, in our model, properties can be designed to evaluate not only
action outcomes but also attached to actions themselves (such as whether
an action is permitted, e.g. lying is never acceptable). The expressiveness
of the chosen language for implementing properties plays a significant role
here.
However, in this paper, and as illustrated in our example in Subsec-
tion 4.1.2, we expressly limit the examples to propositional logic and, more
specifically, simple propositions to improve readability. Complex languages
with higher expressivity can be chosen. Alternative and more simplified ap-
proaches may also be investigated. For example, one may choose to describe
the world state through percepts instead of properties, where the detection
of percepts can also be verified or checked. Again, this is an implementation
choice. The bottom line is that the satisfaction of property nodes must be
computationally verifiable.
Choosing the codomain of value importance. Concerning the importance
measure of a value concept, choosing the codomain CD that evaluates this
importanceI isalsoanimplementationdecision. Schwartz,forinstance,uses
the range {−1,0,1,2,3,4,5,6,7} for people to specify the importance of a
value,suchas‘equality’(equalopportunityforall)or‘pleasure’(gratification
of desires), as a guiding principle in their lives, where −1 represented oppos-
ing a value, 0 represented considering the value to be non-important, and
positive numbers meant different degrees of supporting a value (‘supreme
importance’, ‘very important’, ‘important’, and so on) [77]. Other ranges
maybeconsidered. Forexample, importancecouldbespecifiedasanumber
in the range [0,1], where 0 represents complete non-importance of a value,
and 1 represents its utmost importance, or a number in the range [−1,1],
where −1 represents opposing a value, 1 represents utmost importance, and
0 represents indifference; or even a number from the set of integers Z that
does not limit the degree of importance/opposition.
Of course, alternative approaches, such as defining importance measures
as ranges instead of specific numbers or even a normal distribution, may
also be considered. Another possible approach is to define importance as a
partialortotalorderinsteadofhavingacodomain. Infact, Schwartzargues
that this order (the relevant importance) should be used when reasoning
17about values, where the order is deduced from the importance assigned by
people [77].
A partial order might be more intuitive for humans to provide, as op-
posed to giving numerical importance measures. Human stakeholders may
use partial orders to specify importance measures when this is the case. As
partial orders can easily be translated into numeric values, this fits our pro-
posalinDefinition1. Anexampleoftranslatingpartialordersintonumerical
values is provided by Serramia et al. [81].
The implementation choice must depend on the specific application’s
requirements. For example, when discussing the values of a medical use
case, all three doctors involved in the discussion concurred that (at least at
their hospital) justice was the most important value (understood as requir-
ing appropriate distribution of benefits, risks, and costs fairly), followed by
non-maleficency (requiring not causing harm to others), then autonomy (re-
quiring respect for the decision-making capacities of autonomous persons)
andfinallybeneficence(requiringthepreventionofharm,providingbenefits,
and balancing benefits against risks and costs).
In the example of this paper, we set the codomain to [−1,1] as reasoning
with numbers is computationally easier than reasoning with partial orders.
Furthermore, the range [−1,0] is more intuitive for describing opposition to
a value. This expands Schwartz’s opposition measures, as there may also be
degreesinopposingavalue,liketherearedegreesinsupportingavalue. The
choice to limit the range to [−1,1], as opposed to (say) Z, is an arbitrary
choice. Recall that partial orders provided by stakeholders can easily be
translated into the chosen range [−1,1], as illustrated above.
Ensuring the coherence of value importance. As for the aggregation func-
tion A that ensures the coherence of value importance within a taxonomy,
differentaveragingoperatorsmaybeinvestigated. Inthispaper, wepropose
a simple average:
(cid:88)
I(n )
c
I(n ) =
nc∈Xnp
(2)
p
|X |
np
As argued above, we assume the importance measure of some of the
nodes to either be provided manually or learned from ongoing behaviour.
A propagation mechanism may then be implemented to calculate the im-
portance of nodes that still need to be manually provided or learned. Dif-
ferent propagation mechanisms may be investigated to check the coherence
of existing importance measures as they propagate those measures to other
18nodes while ensuring the overall coherence of value importance within the
taxonomy (following Definition 2). One sample propagation mechanism is
presented in Algorithm 1.
ThealgorithmstartswiththePropagatefunction(line1),byobtaining
all root nodes and then commencing the propagation process with each of
these nodes. This is achieved by calling the Propagate2 function (lines 2
and 3). This function is recursive. After it propagates importance measures
with respect to a given node, it moves on to continue the propagation with
respecttothechildrennodes(lines29and43). Whenpropagatingforagiven
node, either we attempt to propagate the importance measure down to its
children nodes if the node already has an importance measure assigned to it
(case of lines 12–29), or we attempt to propagate the importance measures
ofthechildrenupifthenodedoesnothaveanimportancemeasureassigned
to it (case of lines 30–43).
Nothing must be done when propagating down if the node has no chil-
dren nodes (line 13). If the node has no children nodes with no assigned
importance measures, then nothing needs to be propagated, but the algo-
rithmcheckswhethertheparentnode’simportancemeasureiscoherentwith
those of its children nodes. If it is not, the algorithm prints an error mes-
sage and halts (case starting with line 14). If there is exactly one child node
with no assigned importance, and regardless of how many children nodes do
haveassignedimportancemeasures, thenthechildnodewithnoimportance
measure is assigned the measure that makes the parent node’s importance
measure the average of those of its children (case starting with line 19).
Suppose there are several (more than one) children nodes with no assigned
importance measures, and all of those nodes do not have any descendants
with assigned importance measures. In that case, we calculate the sum of
those nodes so that the parent node’s importance measure is the average
of those of its children. Then, we assume that the sum is divided equally
amongst all the children nodes with no assigned importance measures (case
starting with line 22). Finally, if there are several (more than one) chil-
dren nodes without assigned importance measures, and at least one of those
nodesdoeshaveadescendantwithanassignedimportancemeasure,thenno
propagation takes place (case starting with line 27). This is because those
specific nodes require careful consideration to ensure their coherence with
both their ancestor and descendant nodes. So, calculating their importance
measure is postponed.
Nothing must be done when propagating up if the node has no children
nodes(line31). Ifallthechildrennodeshaveassignedimportancemeasures,
then the importance of the parent node becomes the average of its children
19Algorithm 1 Propagation algorithm for value importance
Require: N to be a set of nodes, E to be set of edges between nodes (E : N ×N), and I to
describetheimportanceassignedtonodesinN (I:N×[−1,1]).
Require: GetRoots(N,E) returnsrootsin N, givenedges E; Children(n) returnschildrenof
n; Val(n) returns importance of n; VSum(N) returns sum of the importance of nodes in N,
andDVW(N)returnstrueifsomedescendantofN hasitsimportanceset,falseotherwise.
1: functionPropagate(N,E,I)
2: Roots←GetRoots(N,E);
3: It←Propagate2(Roots,E,I);
4: returnIt
5: endfunction
6: functionPropagate2(Nodes,E,I)
7: do
8: forn∈Nodesdo
9: C←Children(n);
10: C′←{c′∈C | Val(c′)̸=nil};
11: C′′←C\C′;
12: if Val(n)̸=nilthen
13: if C==∅then ▷donothing
14: elseif |C′′|==0then
15: if Val(n)̸=(VSum(C)/|C|)then
16: output“ERRORwithn’svalue”
17: returnnil
18: endif
19: elseif C′′=={c′′}then
20: imp←(Val(n)×|C|)−VSum(C′);
21: It=(c′′,imp)∪It;
22: elseif |C′′|>1 ∧ ¬DWV(C′′)then
23: imp←((Val(n)×|C|)−VSum(C′))/|C′′|;
24: forc′′∈C′′ do
25: It=(c′′,imp)∪It;
26: endfor
27: else ▷donothing
28: endif
29: It←Propagate2(C,E,It);
30: else
31: if C==∅then ▷donothing
32: elseif |C′′|==0 ∧ |C′|>0then
33: imp←VSum(C′)/|C′|;
34: It=(n,imp)∪It;
35: elseif |C′|>0 ∧ ¬DWV(C′′)then
36: imp←VSum(C′)/|C′|;
37: It=(n,imp)∪It;
38: forc′′∈C′′ do
39: It=(c′′,imp)∪It;
40: endfor
41: else ▷donothing
42: endif
43: It←Propagate2(C,E,It);
44: endif
45: endfor
46: whileIt̸=I
47: endfunction
20nodes (case starting with line 32). Suppose some of the children nodes have
an assigned importance measure, and some do not, and all of the latter’s
descendants have no descendants with assigned importance measures. In
that case, there is no information from the descendants, and as such, we can
only use the information from the children nodes with assigned importance
measures. So,wetaketheaverageoftheavailablemeasuresofchildrennodes
and assign it to the parent node. Then we propagate downwards again by
assigning that importance measure to those children nodes with no assigned
importance (case starting with line 35). In all other cases, we do nothing.
Thealgorithmrepeatsuntilnonewimportancemeasuresarepropagated
(line 46).
Please note that Algorithm 1 has been implemented in Prolog, and the
code is available on SWISH.1
4.1.2. The Running uHelp Example
The running example that we use throughout this subsection is uHelp.
uHelp is a fully implemented app2 developed at IIIA-CSIC within a team
which includes the first author of this article. It is available on Google Play3
and the Apple Store.4 The app is a social networking app that was initially
designed to support people in finding help within their social network with
their day-to-day activities, such as finding someone to substitute another at
work tomorrow or someone to lend chairs for a party [40, 39, 59].
To support the reader’s better understanding of our formal model de-
scribed in this subsection, we will specifically consider the value fairness.
This value emerged as essential in the participatory design meetings we held
with potential app users.
As mentioned, we choose not to confine our model to predefined value
systems, like Schwartz’s renowned universal human values [77], but opt for
values that may emerge from different stakeholders. In our case, fairness
aroseasacandidatevaluethroughvariousdiscussionswithuHelp’spotential
users.
We note here that recognising the relevant high-level values (root nodes
1Thecodeisavailableat: https://gitlab.iiia.csic.es/-/snippets/32.Thenodes,
edges, and importance predicates help define the value taxonomy. Running ?-
propagate(X). will return the importance of the remaining nodes, satisfying coherence.
Ifthereisanyincoherenceintheinputvalues,theuserisinformedandpropagationhalts.
2https://uhelpapp.com
3https://play.google.com/store/apps/details?id=es.csic.iiia.uhelpapp
4https://itunes.apple.com/es/app/uhelp/id1089461370
21in the taxonomy) may be achieved either through a bottom-up approach,
with the relevant values emerging from discussing the context with the rele-
vantstakeholders,orthroughatop-downapproach,withagivenstakeholder
handing the relevant values from prior knowledge. While fairness arose in
a bottom-up approach within the uHelp application domain, in another use
case within the medical field domain (specifically, in our discussions with
doctors from Hospital del Mar, Barcelona), relevant values were pre-defined
by the hospital. These were the four universal values of autonomy, benefi-
cence, non-maleficence, and justice agreed upon within the medical profes-
sion[6,5]. Thoughthesefourvalueswerepre-defined,additionalvaluessuch
as cost efficiency, pain control, and quality of life emerged in our discussions
with doctors. Our interactions with uHelp users, doctors, and firefighters
(from yet another real-life use case with a fire department in Barcelona)
provide evidence of how some relevant values are predefined by the rele-
vant institutions and supplied in a top-down approach. In contrast, others
emerge through discussions in a bottom-up approach.
After recognising the relevant high-level values, the rest of the taxon-
omy emerges from contemplating them and how they are understood in
a given context. With new contexts, the taxonomy gets updated to con-
sider the requirements of these contexts. For example, in the uHelp case,
when discussing the application with a community of single parents (from
Barcelona’s Association of Monoparental Families), fairness was understood
as balanced give and take (see Figure 5). In other words, the users wanted
members to refrain from being able to act in greedy ways by constantly
asking for help and never offering it. But when we later considered uHelp
for a community that supports older people, it was clear that this balanced
give and take was against their values. This was because the community
was designed to have volunteers who help members (older people) who (in
the eyes of the stakeholders) should feel free and be able to volunteer them-
selves. A new understanding of fairness emerged: the balanced division of
labour over volunteers. That is, while some community members will ask
for help and others will provide it, the workload among those providing help
should be balanced. This is a crucial lesson in how the higher-level values
can remain the same (such as fairness), but the decision-making strategy for
enacting those values changes in different contexts (balanced give-and-take
versus balanced division of labour). As such, uHelp’s initial value taxonomy
had the leftmost branch of Figure 5 specified. But when the community
supporting older people appeared, the branch with a balanced division of
labour emerged. For illustrative purposes and to present a richer taxon-
omy, we have added the rightmost branch that introduces fair reward as
22Figure 5: uHelp’s value taxonomy for the value fairness
another aspect of fairness, which would fall along with the balanced division
of labour under the abstract concept of proper treatment. Fitting reward
reflects the idea that not all tasks should be considered equal, as some are
more demanding than others. So, fair rewards would be considered. But as
this value concept was never specified for uHelp, it does not have a property
node that grounds it. This illustrates how uHelp’s value taxonomy for fair-
ness has been shaped over time and contexts so that fairness is understood
in terms of reciprocity and fair treatment, where reciprocity is understood
as a balanced give and take and fair treatment is understood in terms of
balanced division of labour as well as fitting reward. We follow by defin-
ing some of the grounding semantics of those abstract concepts, i.e., the
property leaf nodes. Property nodes are presented as squares, whereas label
nodes are shown as circles.
For illustrative purposes, we provide two different computational ap-
proaches defining balanced give and take in terms of properties p and p .
1 2
The first states that one’s help requests are proportionate to the number of
timestheuserofferedhelp,whereasthesecondstatesthatone’shelprequests
areproportionaltothenumberoftimestheuserwaschosentohelp(because
not all those offering help get selected). One straightforward approach to
specifying p and p is through the ratio of requests to offers/volunteering
1 2
being greater than 1, as illustrated in property definitions 3 and 4. Of
course, we provide simple properties for better readability. Still, we note
that properties can get as complex as the language allows (remember that
the choice of language for specifying properties is an implementation choice,
and one may choose to work with highly expressive languages).
#
def requests
p = > 1 (3)
1
#
offers
23#
def requests
p = > 1 (4)
2
#
volunteering
Thetwodifferentpropertiesfor‘balancedgiveandtake’illustratehowdiffer-
ent semantics may be provided for the same abstract value. In this specific
case, one can imagine p to have been initially defined. However, some users
2
are never chosen after some interactions despite volunteering. As such, the
system prevents them from asking for help so that balanced give and take is
respected. To compensate for this, p gets added to the taxonomy so that
1
balanced give and take considers the offers for help instead of being chosen.
This illustrates how the taxonomy may evolve by learning from experience
what the best grounding semantics (property) of a given abstract concept
for a given context are.
The computational approach, or the property node, describes the bal-
anced division of labour through property p , which states that tasks are
3
equally distributed amongst volunteers. One approach to specifying p is
3
through the difference between the uniform distribution U and the distri-
bution D of the numbers of tasks assigned to each volunteer, where this
difference should be smaller than a predefined threshold ϵ (property defini-
tion 5).
def
p = difference(D,U) < ϵ (5)
3
Thedifferencebetweentwodistributionsmaybecalculatedusingapproaches
like the Kullback–Leibler divergence [48] or the earth mover’s distance [73].
Thistaxonomydoesnotspecifythepropertynodeprovidingtheground-
ing semantics for fair reward. Some abstract concepts may remain abstract
for a while.
We note that while stakeholders discuss and agree on abstract concepts
of values, engineers must define the properties that ground the semantics
of those values. Alternatively, AI may be used to support constructing and
updatingthesevaluetaxonomies, suchasbylearningfrompastinteractions.
The construction of these taxonomies is one of the envisaged lines for future
work, as discussed in Subsection 6.1.
A crucial feature of the value taxonomy is the importance of nodes.
We imagine each context to have its own taxonomy with associated impor-
tance measures (see Subsection 4.2). As such, an adapted uHelp taxon-
omy for fairness will be made available for the community of single parents
and another for the community supporting older people. However, whether
importance measures are available for the general taxonomy of uHelp is
application-dependent. uHelp may or may not define importance measures
24for its general taxonomy for fairness. Nevertheless, for illustration purposes,
we assume that it does.
Getting stakeholders to specify the importance of all nodes is usually
challenging. Even if a partial order was provided, it might only cover some
nodes. Assuch,wegiveanexamplewhereimportanceisassignedtoasubset
of these nodes. In this example (Figure 5), we represent importance mea-
sures explicitly provided through bold underlined numbers. From those, our
propagation mechanism (Algorithm 1) can then calculate possible coherent
importance measures for the remaining nodes, presented in regular font and
non-underlined.
Finally, we have provided in this subsection the value taxonomy of one
uHelp value, fairness. We imagine applications to have several relevant
values and several such taxonomies. For example, for the medical case,
doctors at Hospital del Mar have already recognised seven relevant values
(as mentioned earlier). We then expect concurrent processing over these
various taxonomies.
4.2. How do values change with context? Context-based value taxonomies
Our stance is that values are context-dependent and change over time.
Thisisthestanceofvalue-sensitivedesign[87], aswellasthestanceofmany
social psychologists before them [71]. We all have a general view of what
a value is, defined through its taxonomy, and this view evolves with our
experiences, where new nodes (label-based and property-based) are contin-
uously added. If a new context requires adding new nodes, those are added
to the general taxonomy. This is how these general taxonomies evolve with
experiences. Suppose a new context necessitates eliminating existing nodes.
This is achieved by setting their importance to zero for that specific con-
text (if zero is the neutral-based measure of the chosen codomain). Moving
from a general taxonomy to a context-based one is implemented by simply
revisiting the importance measures of the general taxonomy (whether they
were defined or not), making some nodes or branches more prominent than
others. We note that if property-based leaf nodes did not exist for a promi-
nent branch, they must be added to the general taxonomy. Otherwise, a
computational approach considering those branches will not be possible.
A definition of a context-based value taxonomy is presented next, which
states formally that a context-based taxonomy is an alteration of a gen-
eral taxonomy where the importance measures are updated for the relevant
context:
25Definition 3 (Context-based value taxonomy). Acontext-basedtaxon-
omy V = {N,E,I } is an alteration of a general taxonomy V = {N,E,I}
c c
where the importance of nodes are updated for the given context. The im-
portance of nodes in the context-based taxonomy V is independent of the
c
importance of those nodes in the general taxonomy V. The function cal-
culating the importance of nodes with respect to a context c is defined as
I : N → COD , where COD defines the union of COD with the unde-
c ⊥ ⊥
fined variable (⊥).
Wereiteratethatifnewnodesneedtobeintroduced,theyareintroduced
to the general taxonomy before the context-based one is constructed.
Last, we say a context c may be defined by a set of properties P ⊆ P.
c
The context c is then considered to hold (holds(c)) —that is, we can say
that we are in that context— when its properties are satisfied: (∀p ∈ P (|=
c
p)) =⇒ |= holds(c). Whilstthispaperdoesnotinvestigatetheramifications
of this context definition further, we include it to give a high-level sense of
how a system may set about detecting its current context in general.
4.2.1. Implementation Choices
Constructing Context-Based Taxonomies. Different mechanisms for evalu-
ating the importance of nodes in context-based taxonomies may be devel-
oped. One implementation (Algorithm 2) is a bottom-up approach where
only the importance of property nodes for the given context is evaluated
(regardless of how they are obtained, whether they are provided manually
by stakeholders or learnt from similar past experiences of similar contexts).
The idea is that one can better assess the importance of concrete property
nodes in specific contexts. Lines 3–6 assign new importance measures to
the property nodes, assuming those measures have already been provided
for this context, regardless of how they are obtained. The mechanisms for
obtaining these measures, specified through function GetImportance, are
left for future work, as illustrated in Subsection 6.1. Then, to calculate
the importance of the remaining nodes in the taxonomy, line 7 propagates
importance measures to the rest of the taxonomy, following Algorithm 1 of
Subsection 4.1.1.
Alternative implementations may be investigated. Rather than starting
with property nodes and following a bottom-up approach, a top-down ap-
proach may be implemented to assess abstract concepts regardless of their
groundingsemantics. Sometimes, amixofbottom-up/top-downapproaches
might be appropriate. The domain might suggest which approach is best.
26Finally, we note that inconsistencies may arise between general and
context-based taxonomies. This is expected and entirely normal. For ex-
ample, while reciprocity might be considered very important as an abstract
concept for the uHelp app, it might be regarded as less important for a
specific context, such as for the community of volunteers supporting older
people (see Subsection 4.2.2).
VisualisingTaxonomies. Inadditiontoconstructingcontext-basedtaxonomies
through updating importance measures and for improved visualisation, we
propose an approach (Algorithm 3) that chops off parts of the taxonomy
that are deemed irrelevant for a given context. This step is unnecessary
but may be helpful when imagining growing taxonomies for a given context.
This also helps stakeholders quickly ascertain the relevant nodes for any
given context.
Different approaches may be followed when deciding which branches are
relevantandmaybevisualised. Algorithm3’sapproachusestheimportance
of property nodes to determine the relevant branches of the taxonomy. The
algorithm maintains the branches that lead to relevant property nodes and
eliminates those that lead to irrelevant ones.
Again, different approaches may be followed when deciding which prop-
erty nodes are relevant. In Algorithm 3, the irrelevant nodes are those with
an importance of zero (lines 3–7). But based on the application, one can
Algorithm 2 Constructing context-based value taxonomies
Require: a general value taxonomy V = (N,E,I)
Require: N ⊂ N to be the set of property nodes in N
ϕ
Require: a set of properties P ∈ P that define the context c
c
Require: GetImportance(n,P ) to be a function that obtains the impor-
c
tance of node n within context c (the specification of this function is left
for future work)
1: function ContextTaxonomy(V,P c)
2: I0 ← ∅;
c
3: for n ∈ N ϕ do
4: I c(n) ← GetImportance(n,P c);
5: I c0 ← I c(n)∪I c0;
6: end for
7: I c ← Propagate(N, E, I c0);
8: return (N, E, I c0∪I c);
9: end function
27imagine alternative implementations. For example, it could be decided that
only property nodes with a positive importance are relevant. (It would also
be possible to set another predefined threshold, other than 0, to filter rele-
vant importance values.) An alternative would be using the k-means clus-
tering algorithm [67] over the importance measures of all property nodes.
With the parameter k = 2, the nodes will be divided into two sets, with
the most important nodes grouped without needing a pre-defined threshold.
Thesearejustexamplesofpossibleimplementationsfordecidingonrelevant
property nodes.
Going back to Algorithm 3, after selecting the relevant property nodes,
thealgorithmvisualisestheprominentbranchesofthetaxonomyinlines11–
20 by keeping only the branches that lead to those relevant nodes.
Algorithm 3 Visualising value taxonomies
Require: a context-based value taxonomy V = (N,E,I )
c c
Require: N ⊂ N to be the set of property nodes in N
ϕ
1: function VisualiseTaxonomy(V c)
2: relevantNodes ← ∅;
3: for n ∈ N ϕ do
4: if I c(n) ̸= 0 then
5: relevantNodes ← {n}∪relevantNodes;
6: end if
7: end for
8: N v ← relevantNodes;
9: E v ← ∅;
10: I v ← {I(n) | n ∈ relevantNodes};
11: do
12: N v′ ← N v;
13: for n ∈ N v do
14: if (p,n) ∈ E ∧p ̸∈ N v then
15: N v ← p∪N v;
16: E v ← (p,n)∪E v;
17: I v ← I(p)∪I v;
18: end if
19: end for
20: while N v′ ̸= N v
21: return (N v, E v, I v);
22: end function
284.2.2. The Running uHelp Example
Figure 5 presented uHelp’s general taxonomy for fairness. This sub-
section presents examples of context-based taxonomies for different uHelp
communities. As illustrated in Subsection 4.1.2, the first community to
adopt uHelp was the community of single parents (whose context is speci-
fied as c ). For them, fairness implied a balanced give and take. We can
s
imagine that at the beginning, both the general and context-based taxon-
omy for the c community were defined through the taxonomy of Figure ef
s
fig:single-mothers-1. Butasanewcontextisconsidered, whichisthatofthe
community supporting older people (whose context is specified as c ), new
e
nodes were added to the general taxonomy, resulting in (say) the taxonomy
of Figure 5.
With this new general taxonomy for fairness, the community supporting
older people decides that the requirement of having a proportionate number
of help requests with respect to the number of help offers (p ) or actual
1
volunteering (p ) is considered irrelevant. In contrast, the requirement that
2
labour should be divided in a balanced way amongst all volunteers is of
utmost importance. As such, the importance of the different property nodes
of Figure 5 is specified accordingly:
I (p ) = 0 ; I (p ) = 0 ; I (p ) = 0.9
ce 1 ce 2 ce 3
The context-based taxonomy of this new community V is constructed
ce
by Algorithm 2, and is visualised by Algorithm 3, which eliminates all
branches leading to property nodes with zero importance, leaving us with
one single branch leading to property node p . The resulting taxonomy is
3
presented in Figure 6c. The importance of the upper nodes inherits the
importance of the property node, following Algorithm 1.
Nowassumethatwiththisnewgeneraltaxonomy(Figure5),thecommu-
nity of single parents decides to revisit their context-based taxonomy. They
have become interested in the new grounding semantics that has emerged
and decided that having requests proportionate to offers (p ) and equally
1
distributed amongst volunteers (p ) are both essential properties. This new
3
view is reflected in the importance measures they assign to the property
nodes of Figure 5:
I (p ) = 0.8 ; I (p ) = 0 ; I (p ) = 0.7
c 1 c 2 c 3
Algorithm 2 builds the new taxonomy V′ . Again, for visual clarity,
cs
Algorithm 3 eliminates the branches that lead to property nodes with zero
importance. The result is the taxonomy presented in Figure 6b. As before,
the importance of the upper nodes is calculated following Algorithm 1.
29(a)Vcs forthecommunityofsingleparents,whichhappenstobethesameastheinitialgeneral
uHelptaxonomyV
(b)V′ fortheupdatedcontext-basedtaxonomyforthecommunityofsingleparents
cs
(c)Vce foracommunityofvolunteerssupportingtheelderly
Figure 6: Different context-based value taxonomies for fairness in uHelp
304.3. Who holds values? Individuals versus collectives
Our stance is that entities hold values; values do not exist in isolation.
In other words, there is no universal value taxonomy for the fairness value.
Different people or groups will hold different views on values, leading to
different taxonomies. We use the notation Vx to represent the value taxon-
omy held by entity x, where x may represent an individual i or a collective
j
{i ,...,i }. We use the word collective to describe a group of interacting
1 n
individuals (usually called members), which may be a community, an or-
ganisation, an institute, a society, or a culture. When a collective holds a
value taxonomy, this is understood as the value taxonomy describing the
values of the collective as a whole and not its individuals (or interacting
members). Individuals may not all have their value taxonomy aligned with
the collective’s. The issue of how the collective agrees on its taxonomy is
left for future work, as our proposed roadmap illustrates in Subsection 6.2.
To simplify notation, in the rest of the paper, we drop the x from Vx when
it is clear who holds the taxonomy.
Humans and value-aware AI agents (as our model proposes) not only
understand their own values or the values of the collectives they belong to,
but they also observe others and form beliefs about the values of others. We
use the notation Vx>y to represent what x believes to be the values of y,
where each of x and y may represent an individual or a collective.
4.3.1. Implementation Choices
Theissueatstakehereishowtoimplementmechanismsthatwouldtake
in the value taxonomies of a set of individuals and compute the taxonomy
of the collective. This is a complex task, and the domain may dictate how
collective values are specified. For example, in some cases, the company
developing an assistive robot may pre-define the value taxonomy governing
the behaviour of that robot. In another case, such as a hospital, a board of
electedmedicalpersonnelwillconvenetocollectivelyagreeontheimportant
values of their hospital and how they are lived out in practice. Yet, in other
cases, such as with the uHelp application, one can imagine the entire com-
munity of users coming together to vote on their values. In other words, the
rules dictating whose view should be considered when specifying the value
taxonomies of a collective are dependent and left for future development
(see Subsection 6.2).
Different mechanisms may be developed to construct collective values
from individual ones. For example, negotiation and argumentation mecha-
nismscanhelpindividualsreachcollectiveagreementsontheirvalues. Com-
putational social choice may also be used to aggregate individual values into
31(a)Vuser1 (b)Vuser2
cs cs
(c)Vuser3 (d)Vcollective
ce ce
Figure7: IndividualandcollectivevaluetaxonomiesforfairnessinuHelp’scommunityof
single parents
collectiveones. Wepointtheinterestedreadertoongoingworkonthistopic
by Lera-Leri et al. [41], where the aggregation takes into consideration var-
ious ethical principles, such as utilitarian (maximum utility) or egalitarian
(maximum fairness).
4.3.2. The Running uHelp Example
This subsection highlights how both individuals and collectives can hold
value taxonomies. For example, imagine three uHelp members from the
community of single parents who hold the taxonomies of Figures 7a–7c. Of
course, more members would exist in reality, each with their own taxonomy,
but for simplicity, we only show the taxonomies of three individuals. Given
these taxonomies, the social choice mechanism of Lera-Leri et al. [41] can
thenbeusedtocomputetheimportanceofeachpropertynodeforthecollec-
tive. With those, the propagation mechanism of Algorithm 1 can propagate
thosetotherestofthetree. Theresultingandfinaltaxonomyisthenshown
in Figure 7d.
324.4. Why hold values? The value alignment problem
As presented in Section 2, it is well established that values are one of the
primary motivators of behaviour. The main objective of the work on values
in AI has been to ensure value-aligned behaviour by assessing the alignment
of behaviour with values.
Theproperty-basednodesofavaluetaxonomyintroducethefoundations
forlinkingabstractvalueconceptstoconcretecomputationalconstructsthat
can help formally assess the alignment of behaviour with those values. The
value alignment of an entity’s behaviour becomes the degree of satisfac-
tion of the property-based value nodes of the relevant taxonomy that this
behaviour brings about. In Definition 3, we have seen how importance is
assigned to different nodes of a context-based taxonomy (we believe value
alignment will be assessed within specific contexts, working with context-
basedvaluetaxonomiesasopposedtogeneralones). Theevaluationofvalue
alignment should consider these essential measures: the more important a
property-based node is, the higher its satisfaction contributes to the value
alignment of the evaluated behaviour, and vice versa. The alignment of an
entity e’s behaviour with a context-based value taxonomy V is then defined
c
accordingly, which formally states that the alignment of e’s behaviour with
the value taxonomy V is an aggregation of the satisfaction of all property
c
nodes of the taxonomy, taking into consideration the importance of each
property node, as we present below. We note, however, that context-based
value taxonomies are expected to describe the values held by some entity.
For the sake of simplifying notation, we drop the holder x (and possibly x’s
view of y’s values, if that was the case) and replace Vx with V (or Vx>y
c c c
withV ). Wealsonotethatx(oreveny)doesnotnecessarilyhavetobethe
c
same entity e whose behaviour is being assessed. In other words, if x ̸= e′,
then this describes the process of assessing how much is e aligned with the
values of x.
(cid:77)
A(e,V ) = f(sd(p,e),I (p)) (6)
c c
p∈N
ϕ,c
whereN representsthepropertynodesofthetaxonomyV , sd(p,e)repre-
ϕ,c c
sentsthedegreeofsatisfactionofpropertypwithrespecttothebehaviourof
entity e, and I (p) represents the importance of the property-node p within
c
thecontext-basedvaluetaxonomyV . Thefunctionf isusedtofactorinthe
c
importance of property nodes when considering their degree of satisfaction,
(cid:76)
whereas is used to aggregate the degree of satisfaction of all property
nodes in V (with value importance factored in).
c
With Equation 6, we provide the basis for calculating value alignment
33and supporting value-based reasoning and decision-making, which are the
main objectives of the work on values in AI.
4.4.1. Implementation Choices
One issue is how the satisfaction of property nodes sd(p,e) is calcu-
lated. In other words, given an entity e, how do we assess to what degree
the behaviour of e results in the satisfaction of property p? This requires
knowledge about how e behaves, and different implementation approaches
for specifying this knowledge can be followed. For example, suppose e is a
complex system of communicating entities. In that case, e’s model (usually
specified via a process calculi) will describe its behaviour through a labelled
transition system where the satisfaction of specific properties at different
states can be evaluated [84]. If e is a normative system, then the norms
can help map out the state diagram of the possible interaction outcome
and evaluate the satisfaction of relevant properties accordingly [52, 1, 16].
If e was an agent with a BDI model, then BDI reasoning mechanisms can
help assess the degree to which specific properties will be satisfied by e’s
behaviour [69].
In summary, a model of e describing its behaviour is necessary to assess
sd(p,e). As illustrated above, this issue has already been addressed in many
fields. Toensureourproposalisnotlimitedtoonemodellingchoice,weomit
the choice of modelling e’s behaviour and assume the degree of satisfaction
sd(p,e) to be attainable.
Returning to the alignment function A, there are other implementation
choices, such as the function f that factors in the importance of a property
(cid:76)
node and the aggregation operator . In this paper, we propose a straight-
forward implementation that follows a weighted average approach that uses
the importance of properties as the weight of their degree of satisfaction (so
f is implemented as a multiplication operator) and the aggregation over all
(cid:76)
relevant properties ( ) is implemented as a simple average:
(cid:88)
I (p)·sd(e,p)
c
p∈N
A(e,V ) = ϕ,c (7)
c
|N |
ϕ,c
If we assume the range of value importance I to be [−1,1], and the
degree of satisfaction sd(e,p) to be a percentage with the range [0,1], then
the range of A becomes [−1,1] where negative results describe the degree
of misalignment (or an alignment with detested values) and positive results
illustrate the degree of alignment with aspired values.
34Of course, alternative and more sophisticated approaches to alignment
can be explored. For example, one may consider not only the importance of
a property when assessing the satisfaction of properties but also its weight
in the value taxonomy, represented by the number of paths that lead to this
property node. In other words, the larger the number of paths that lead
to a property node, the larger its impact on alignment: that is, replacing
I(p)·sd(e,p) in Equation 7 with paths(p)·I(p)·sd(e,p), where paths(p) is
the number of paths in the value taxonomy that lead to the property node
p.
4.4.2. The Running uHelp Example
Let us return to our example and examine the context-based value tax-
onomy V′ for a mutual aid community represented by Figure 6b. The
cs
concrete definitions of properties p and p (property definitions 3 and 5)
1 3
illustrate what it means, computationally, for the behaviour of some entity
to be aligned with the value ‘fairness’ in this context. Next, we illustrate
howtheexactdegreeofsatisfactionofpropertiesp andp canbecomputed
1 3
according to these definitions. Equation 8 formally states that the degree of
satisfaction of p is the actual ratio of requests to offers, normalised to fall
1
into the range [−1,1].
 (R−1)
  , if R > 1
(maxR)−1
sd(e,p ) = (8)
1

R−1 , otherwise
where R = #requests/#offers represents the ratio of requests to offers,
and maxR is the maximum possible value for R. While the range of R is
[0,∞), a maximum value must be selected for our equations. We argue that
maxR is domain-specific and should be selected for each context.
Equation8statesthatthedegreeofsatisfactioniscomputedbymapping
the ratio R of requests to offers to the range [−1,1]. When this ratio is in
the range [1,maxR], then this gets normalised to the range [0,1] to describe
a positive degree of satisfaction (where 1 gets mapped to 0 and maxR gets
mapped to 1). And when the ratio is in the range [0,1], then this gets
translated to the range [−1,0] to describe a negative degree of satisfaction
(where 0 gets mapped to −1 and 1 gets mapped to 0).
In summary, p ’s degree of satisfaction depends on how far the ratio R is
1
from 1. The larger it is relative to 1, the higher the satisfaction. The closer
it is to 0, the higher the dissatisfaction.
35Next, Equation 9 defines the satisfaction of property p similarly by for-
3
mally stating that the degree of satisfaction of p is the actual difference
3
between the uniform distribution U and the distribution of tasks over vol-
unteers D, normalised to the range [-1,1].
 ∆
1− , if ∆ < ϵ
  ϵ
sd(e,p ) = (9)
3
(∆−ϵ)

 − , otherwise
((max∆)−ϵ)
where ∆ = difference(D,U) represents the difference between the distri-
bution of tasks over volunteers (D) and the uniform distribution (U), and
max∆ is the maximum possible value for ∆. The range of ∆, whether we
use the earth mover’s distance or the Kullback–Leibler divergence, is [0,∞),
but a maximum value must be selected for our equations. Again, we argue
that max∆ is domain-specific and must be chosen for each context.
Equation9statesthatthedegreeofsatisfactioniscomputedbymapping
the difference ∆ to the range [−1,1]. When this difference is in the range
[0,ϵ], then this gets inversely normalised to the range [0,1] to describe a
positive degree of satisfaction (where 0 gets mapped to 1 and ϵ gets mapped
to 0). And when the ratio is in the range [ϵ,max∆], then this gets inversely
normalised to the range [−1,0] to describe a negative degree of satisfaction
(where ϵ gets mapped to 0 and max∆ gets mapped to −1).
In summary, the degree of satisfaction of p depends on how far is the
3
difference ∆ from ϵ. The larger it is with respect to ϵ, the higher the degree
of dissatisfaction. The closer it is to 0, the higher the satisfaction.
Suppose a mutual aid community e provides incentives that motivate
peopletovolunteerandalreadyhavenormsensuringtasksarespreadequally
among volunteers. Suppose that these regimented norms result in a high
degree of satisfaction for p , whereas the incentives result in a mediocre
3
degree of satisfaction for p :
1
sd(e,p ) = 0.5 ; sd(e,p ) = 0.9
1 3
And say the importance of p is set to be twice that of p , as follows:
1 2
I (p ) = 1 ; I (p ) = 0.5
c 1 c 3
Following Equation 7, it is evident that the alignment of the mutual aid
community e with its understanding of the value fairness V′ (Figure 6b)
cs
becomes:
A(e,V ) = 0.475
c
364.5. Naming our model
To our knowledge, we have presented the first formal proposal for the
explicit computational representation of human values, which provides the
foundations for computational reasoning and value-aligned AI engineering.
From now on, we will refer to this model as the Value Taxonomy Model
(VTM). VTM can represent the values of humans, AI systems, and hybrid
communities of AI systems and humans. The value system of any of these
comprises a set of value taxonomies specified in VTM.
In the next section, we demonstrate how the attributes of our proposal
are consistent with research from social psychology.
5. Aligning our Model with Social Psychology Research
Here, we aim to assess the alignment of VTM with research from social
psychology. To undertake this, we specifically choose the highly influential
and continually well-cited work of Rohan because It provides a thorough
reviewofvalues-relatedtheoryandresearch. Shehighlightsfivemainaspects
of the values construct discussed in the literature before proposing a stance
on each aspect accordingly. Subsections 5.1–5.5 carefully examine how our
proposalforvaluerepresentationisalignedwiththediscussiononeachvalue
aspect. In Subsection 5.6, we then extend our analysis through the lens of
Rohan’s to an additional aspect, the context of values, that we believe is
crucialeventhoughitdidnotreceiveathoroughdiscussioninRohan’swork.
As we show our consistency with the review of values from Rohan, we
also make sure to compare to Schwartz’s work [77], since his theory of basic
human values is considered to be “one of the most commonly used and
tested transcultural theories in the field of behavioural research” [32], and
also because Schwartz’s view on values has become a reference within the
artificial intelligence and multiagent systems communities.
5.1. On the use of the word values
The first aspect identified by Rohan is the issue of using the word value
as a verb versus using the word value as a noun.
Values as a noun. When discussing value as a noun, Rohan summarises
Schwartz and Bilsky’s five features recurrently mentioned in the literature
to define values. In this sense, values have the following properties:
“(a)areconceptsorbeliefs, (b)pertaintodesirableendstatesor
behaviors,(c)transcendspecificsituations,(d)guideselectionor
37evaluationofbehaviorandevents,and(e)areorderedbyrelative
importance” [78].
Ourproposedvaluetaxonomymodelalignswiththisview. Inourmodel,
values are abstract constructs specified through label nodes on concepts like
fairness, reciprocity and equality (which could fall at any level in our taxon-
omy). Wedeliberatelychoosenottogetintothenatureofvaluesandequate
themtobeliefs(firstfeature)becausetheremightbesomediscordregarding
the nature of values, as we have seen in Section 3. The relation of values to
constructs like beliefs or principles is left for future work, especially when
we set out to introduce values into agents’ BDI models (see Subsection 6.3).
However, our proposal defines the semantics of values through desirable end
states (second feature), which we implement via the property nodes of our
taxonomy. The overarching focus of research into values for AI is concerned
with guiding behaviour (fourth feature), as discussed in Subsection 4.4. Our
proposed value alignment mechanism assesses to what extent behaviour is
aligned with selected values. Value importance forms an integral part of our
approach (fifth feature), where each node has an importance measure that
impacts the computation of value alignment (see Equation 6). While the
third feature states that values transcend specific situations, we concur that
value taxonomies do not change frequently. Still, we argue for the evolution
of these taxonomies over context and time, following in the footsteps of van
dePoel[87],asillustratedinSubsection4.2andreiteratedinSubsection5.6.
Inadditiontothesefivefeatures, Rohandiscusseshowvaluesareformed
from experiences. She recognises the need for more research to analyse the
link between values and affect, where affect is understood as the display or
experienceofemotion. Schwartzalsomentionsalinkbetweenvaluesandaf-
fect, though neither scholar gets beyond pointing out the link. Our example
has illustrated how values arise from experiences (Subsection 4.2.2) without
getting into the formal mechanisms of where values come from. We also
recognise the criticality of understanding value emergence and adaptation
(how they change over time), along with the issue of linking values to affect
and set it out in our proposed research roadmap in (Subsection 6.3).
Values as a verb. When discussing the word value used as a verb, Rohan
states it refers to a high-level evaluation of entities that relates them to
the relevant set of values (values as a noun). (Note that this is an issue
not explicitly addressed by Schwartz.) While very little attention has been
given to this evaluation (referred to as a ‘valuing process’), Feather [27]
describesthisprocessbest: “Werelatepossibleactionsandoutcomeswithin
38particular situations to our value systems, testing them against our general
conceptions about what we believe is desirable or undesirable in terms of
our own individual value priorities.”
Relating actions and outcomes to what is desirable with respect to value
priorities (which we refer to as value importance) is precisely the objective
of our work, as illustrated in Subsection 4.4 and formalised in Equations 6
and 7. While Rohan argues that the crucial link between behaviour and
values has been missing from many works and that the value construct later
lostitsinfluencewiththeriseofbehaviourism, Ourcomputationalapproach
for value alignment revives and strengthens the link between values and
behaviour in a concrete and explicit way. The property-based nodes of the
valuetaxonomyintroducethefoundationsforlinkingabstractvalueconcepts
to concrete computational constructs that can be evaluated algorithmically
to help formally assess behaviour (actions and outcomes) with respect to a
set of given values.
5.2. On values, value types, value priorities, and value systems
The second aspect identified by Rohan is understanding the difference
between values, value types, value priorities, and value systems. Again,
these aspects are explicitly modelled within VTM, as discussed here.
Values and value types. Rohantakesthestancethatvaluesaregroupedinto
value types, like those of Schwartz [77]. Examples of two of Schwartz’s value
types are presented in Figure 8, with a selection of values that fall under
each type. We argue that limiting values’ categorisation (or grouping) to
onlytwolevels(valuesandvaluestypes)isrestrictive. EvenSchwartzargues
that different value types can be further organised into a set of higher-order
groups. For example, for Schwartz, universalism and benevolence fall under
the self-transcendence group, which our taxonomy can happily represent
(see the dashed node and edges in Figure 8). In the field of value-sensitive
design [87], value taxonomies (referred to as value change taxonomies) have
been proposed to aid the evolution of values over time and contexts, and
our understanding is that these value taxonomies are not limited to two
levels (values and value types). Our proposal is aligned with Rohan’s and
Schwartz’sinthatselectedvaluescanbegroupedintomoregeneralconcepts,
but we follow the work of van de Poel [87] in not limiting the number of
levels of a value taxonomy to two.
Schwartz was able to limit his value system to a fixed number of levels
becauseheproposedapredefined,universalvaluesystemstructurethatdoes
not change with context.
39Figure 8: Values and value types
Our proposal is more general for representing value systems, as it al-
lows them to evolve with new contexts and experiences, and where new
values and value interpretations may continuously be added (both as label
or property nodes in our taxonomy) and updated. This is not only aligned
with the work on value change taxonomies in the field of value-sensitive de-
sign [87] but also supported by the work of many social scientists who have
proposed different value systems for different contexts [13]. For instance, in
one approach, values for management decisions are proposed based on what
is good for different groups: the business firm, the economy, the society,
and the individual [8]. A different value system is presented for focusing on
the values most relevant for business managers [24], while another proposal
emphasises the relationship between individual values and their relationship
to organisational needs [50].
By not restricting our value taxonomy to two levels, the naming ‘value
types’ (the name given to a group of values by Schwartz and adopted by
Rohan) no longer applies in our proposal as we refer to all the taxonomy
nodes as values. We only differentiate between abstract values (label nodes)
and grounding semantics (property nodes).
Value priorities (or value importance). Anotherkeyconceptthathasgained
a lot of attention in the field is value priorities, which describe the impor-
tance of a value. While Rohan uses the term value priorities, Schwartz talks
about value importance, which maps with our terminology. We remind the
reader that value importance and value priority are used interchangeably as
they both identify the same concept.
However, Rohan limits assigning value priorities to value types only. As
we adopt the multi-levelled taxonomy approach, we argue that nodes in the
taxonomy represent different levels of abstraction (they could be label or
property nodes), and they should all get assigned a priority (importance).
Forexample,lookingatthevaluetaxonomyofthevaluefairness inFigure5,
one can imagine how importance can be assigned to any of these nodes,
40including the property-based leaf nodes.
Of course, our only condition is for the assignment of importance mea-
sures to be coherent: for example, we say that if the importance of all
children nodes is low, then the importance of the parent node cannot be
high, and vice versa. Rohan does not address the issue of value coherence,
possiblybecausenotenoughattentionisgiventotherelationsbetweenthose
types in her model. Our proposal for coherence ensures that the importance
of any parent node is specified as some average over the importance of its
childrennodes(seeDefinition2andthecorrespondingpropertiesandpropo-
sitions). Our work on coherence is fully aligned with that of Schwartz, who
also happens to calculate the importance of value types as an average of the
importance of the individual values of that type [77, p.11].
Value systems. Rohan and Schwartz both talk about value systems that
include value types and priorities and relationships (such as adjacency) be-
tween those value types. Our proposed value taxonomy serves to represent
these notions as they specify the relations between value nodes and the
importance of those nodes. Furthermore, context-based taxonomies only
consider relevant values with respect to a given context, which maps with
Rohan’s view of value systems. However, to simplify the value specification
process, we argue that a value system may comprise more than one value
taxonomy. In other words, we do not force the specification of relations
between all root nodes of existing value taxonomies. Some root nodes may
not be related to others.
5.3. On value priorities and “best possible living”
Thethird aspectidentifiedbyRohanisunderstandingthefactorsupon
which value priorities depend. After a lengthy discussion referencing differ-
entviews,includingSchwartz’sviewthatvalueimportancedependsonthree
universal human requirements (the biological needs of organisms, the req-
uisites of coordinated social interaction, and the survival and welfare needs
of groups), Rohan concludes that value importance should be assessed with
respect to enabling “best possible living”. She argues this is a more general
and integrative approach grounded in the Aristotelian wisdom that eudai-
monia —human flourishing— is the ultimate human goal.
To Rohan, a value system is viewed as “providing a way to order which
requirements or desires are more or less important to best possible living”.
She gives the example that if one has a high priority for the power value,
then “a new car” may be viewed as necessary for that person to maintain
41theirsocialsuperiority. Forthem, maintainingsocialsuperioritycontributes
to their best possible living, achieved through having a new car.
Rohan’s example catches our attention by linking the power value with
attaining the world state of owning a new car. This is captured in VTM by
linking, through its taxonomies, abstract values (such as the power value)
with concrete, measurable properties (such as owning a car), where these
propertiesdescribethe“requirementsordesires”contributing(orotherwise)
to the notion of “best possible living”. Furthermore, the importance of
property nodes in a VTM helps specify “which requirements or desires are
more or less important to best possible living”.
We note that VTM not only links abstract concepts (e.g. power value)
with concrete property nodes (e.g. having a new car) but also ensures the
coherence of value importance between abstract values and their concrete
property nodes. In other words, if the power value is given high importance,
as in Rohan’s example, then its grounding property node will be aligned
withthatandwillbeconsideredofhighimportance, too(assumingnoother
property nodes were at play). The opposite is also true. If having a new
car is considered very important, then its ancestor value node, the power
value,willalsobeofhighimportance(again,undertheprovisothatnoother
property nodes are at play).
Our proposal is fully aligned with Rohan’s. The property nodes and
their importance specify the requirements and desires that define the best
possible life. We do not restrict the selection of those requirements and de-
sires to Schwartz’s three categories: the biological needs, the requisites of
coordinated social interaction, and the survival and welfare needs of groups.
Following in Rohan’s footsteps, we leave this selection open to the stake-
holders’ interpretation.
5.4. From personal values to collective values
Rohan confirms that “contemporary values theorists investigate the val-
ues construct always from the perspective of the person who evaluates enti-
ties in his or her environment” and never from the entity’s perspective. In
her fourth aspect [71], on personal, social, and cultural value systems, she
clarifies that a personal value system refers to a value system that considers
theindividual’svaluepriorities. Thisalignswithourproposal, asillustrated
inSubsection4.3,wherewestatethatvaluesystemscannotexistinisolation
and must be held by entities. We use the notation Vi to describe the value
system held by individual i.
Rohan [71] also talks about social value systems as systems that contain
one’s perceptions of the value priorities of others. She clarifies that “people
42may have perceptions of the value systems of all people and groups with
whom they interact”. We acknowledge the need to describe such social
value systems, as illustrated in Subsection 4.3, and we use the notation Vi>j
to describe i’s view of j’s value system, where j may represent an individual
or a collective.
Rohan distinguishes social value systems from cultural value systems,
where the latter describes the “values that a group endorses or promotes”.
We refer to these as the collective’s value taxonomy, specified in Subsec-
tion 4.3 as Vx, where x represents a collective {i ,...,i }.
1 n
Rohan does not go into the details of how the structure of collective
(or cultural) value systems differ from the others but hints that they may
hold the same structure. From a computational perspective, the formal
definition of value taxonomies of the individual, the individual’s view of
others, or the collective should all be the same (that is, the same model for
value taxonomies applies to all). The difference is essentially in how value
taxonomies are created, which brings us to Rohan’s discussion that follows.
Rohanmentionsthedifficultiesinconceptualisingthevaluesystemsthat
groups endorse. He clarifies that there needs to be more consensus about
whether to understand these in terms of the average of the group members’
personal value priorities or, for example, group leaders’ or other significant
members’ beliefs about the groups’ value priorities should be”. Again, the
VTM model aligns with this analysis. In the case of the collective’s value
system, who decides the value taxonomy? An individual, a group of indi-
viduals, or all individuals? How does a group agree on the collective value
taxonomy if it is the latter? If it is an individual or some subset of indi-
viduals, how is this individual or group determined? We argue that, just as
in human societies, norms must define how a collective’s group taxonomy is
created and managed. Precisely how these norms are specified and decided
upon is important for future research, and as such, we capture it within our
proposed research roadmap in Subsection 6.2.
We further extend Rohan’s proposal to have personal, social and cul-
tural value systems by introducing the notion of a collective’s view of some
entity’s value system: Vx>y, where x is a collective i ,...,i and y may
1 n
either represent an individual or a collective. As with the discussion im-
mediately above, how the collective’s view of others formed is important
for future research. As such, we capture it within our research roadmap in
Subsection 6.3.
Finally, VTM aligns with Rohan’s and Schwartz’s views on personal,
social, and cultural value systems. While Schwartz does not mention an
individual’s view of the value systems of others (referred to as social value
43systems by Rohan), he does believe that both individuals and groups can
hold values. He states that while the nature of values and their structure
may be universal, values held by individuals or groups differ only in the
relative importance attributed to each. In other words, this implies that
the value taxonomies of all individuals and groups share the same structure
but differ regarding the importance assigned to each node. Again, this
is an acceptable assumption for Schwartz because he focuses on universal
values shared across cultures and contexts. As we expect values to evolve
with experiences, assuming all individuals and collectives share the same
structure for value taxonomies becomes difficult.
5.5. On worldviews, ideologies, and other value-relevant concepts
In Rohan’s fifth aspect [71], she proposes the notion of worldviews and
ideologies. According to Rohan, worldviews can refer to beliefs about how
the world is or should be that are a function of value priorities. In contrast,
ideologies can refer to value-laden linguistic constructions that people use
for or after decision-making. Rohan explicitly introduces these concepts
because her literature review highlighted that values have been used in such
scenarios.
Specifically looking at worldviews, Rohan states that people’s value pri-
orities influence their perceptions. For example, it has been observed that
right-wing authoritarians believe that authority and conventions are crum-
bling so quickly that civilisation will collapse. They believe they will be
eaten in the resulting jungle. Whereas people with social dominance ori-
entation see life as “dog eat dog” and are determined to do the eating [3,
p. 75].
But value priorities do change as circumstances change. For example,
constant interaction with people with different personal value priorities may
change one’s beliefs about the world and, hence, one’s personal value pri-
orities. In our work, we neither analyse (yet) the impact of value priorities
on people’s perceptions and beliefs nor the impact of experiences on value
priorities. The dynamics between experiences, beliefs and values are worth
investigating, as we set out in our research roadmap (Subsection 6.3).
Turning her attention to ideologies, Rohan states that associated value
systems are used to support and justify more complex decision-making, and
wherethiscomplexityrequiresgreaterconscious, deliberativeandjustifiable
reasoning processes. She argues that the rhetoric associated with ideologies
can be manipulated to connect almost anything (behavioural choice or state
of affairs) to any constructed set of value priorities.
44As a result, Rohan states ideologies can be manipulated to be relevant
to a wide variety of contexts, making them “remarkably slippery social con-
structions that take on different meanings over time and across political
cultures” [85, p. 34]. One impact of our proposal and others working in
AI and the value alignment problem is that AI provides the opportunity to
reason automatically -with clear rules- about values. It has the potential to
help address the pitfalls of Rohan’s identification/categorisation of human
ideologies.
We recognise that not all decision-making can be fully coherent with the
range of relevant individual and collective value systems in any given con-
text. However, AI systems can support analysing the degree of alignment of
behaviours with various value systems openly and systematically. This sup-
ports achieving more informed and effective value-aligned decision-making
and behaviour.
To our knowledge, Schwartz’s work on values did not address world-
views or ideologies, although he did mention other value-relevant concepts.
For example, when defining values, he states that “values are beliefs linked
inextricably to affect” and that “when values are activated, they become
infused with feeling” [77, p. 3]. Again, we agree that a strong relationship
exists between affect and value priorities. Future work should consider the
role of affect on value priorities (or value importance), as we propose in the
roadmap (see Subsection 6.3).
Schwartz also mentions different concepts people use when justifying
their behaviour: attitudes, beliefs, traits, and norms. Attitudes are under-
stood as evaluations of things in light of our values. Schwartz’s example
is that if someone values stimulation highly while giving little importance
to the security value, then they are likely to have a positive attitude to-
ward bungee jumping. Beliefs are about how accurate some statements are,
like “war never solves problems,” and they refer to subjective probabilities.
Schwartz does not explicitly link beliefs to values but mentions them as
one of the concepts used in justifying actions. Norms are the rules that
dictate behaviour. Schwartz argues that because norms work on steering
the outcome of behaviour, we usually accept them (or not) depending on
how aligned they are with our values. Traits are our consistent tendency
to think, feel, and behave in a certain way. They differ from values: while
valuing wisdom highly, one may act foolishly. Like beliefs, Schwartz does
not directly link traits to values.
This discussion highlights the complexity of the decision-making process
and the role of other concepts in this process, such as traits and norms,
which supports the view presented in Section 2. We argue that future work
45should carefully assess the relation of these different concepts to decision-
making, their justification, and their relation to values, as presented in our
roadmap in Subsection 6.3.
5.6. On the context dependency of values
While Rohan and Schwartz hint at the context-dependency of values,
neither of their investigations discusses the topic in depth. Rohan [71], for
instance, states that “personal value priorities will change when circum-
stances change,” explaining that changing circumstances do not need to be
physical; they may reflect the impact of the people in one’s environment:
“Constant interaction with people who have different personal value priori-
ties may change people’s beliefs about the world; changes in people’s beliefs
about the world will be reflected in changes to their personal value priori-
ties”. Ourworkreflectsthisaswehavediscussedhowvaluetaxonomiesmay
evolve with experiences. This aligns with the view of those, like Rohan, who
believe that values may be understood in terms of Bartlett’s schemata [4,
p. 201]: “active organisations of past experience”.
Our proposed model is also aligned with the work on value change tax-
onomy in value-sensitive design [87], which discusses the different ways in
which values may change over time: (1) new values may emerge, (2) the
relevance of a value may change for a given context, (3) the importance of a
value may change, and (4) there may be changes in how values are concep-
tualised, as well as (5) changes in how values are specified, and translated
into norms and design requirements. All these different types of change are
reflected in our model through the possible addition of new abstract value
concepts,newpropertynodes,orchangesintheimportanceofexistingvalue
nodes.
The novelty of our proposal, however, is in visualising and hiding irrele-
vant branches of the taxonomy based on the current context. This enables
taxonomies to evolve and grow with experience over time into larger and
increasingly complex taxonomies that encode learnings through experiences
in related contexts, which then are temporarily pruned to match the current
context. This allows focusing on relevant values based on context.
AsforSchwartz, hehasarguedthathistheoryofbasicvaluestranscends
specific actions and situations. This is expected as his focus has been on
universalhumanvaluesthatdonotchange,likesecurity,power,benevolence,
etc. For Schwartz, the structure of these universal values is fixed. In our
representation, this is understood as fixing the nodes and edges. However,
Schwartz does note that the relative importance attributed to values differs
from one person to another or from one group to another and that “values
46influence action when they are relevant in the context (hence likely to be
activated) and important to the actor”. That is, the importance does not
only change for the different people holding these values but also depends
on the context, which fully aligns with our proposal.
In our case, however, it is not just value importance that may change
with context but also the nodes and edgesof the taxonomy. With new expe-
riences, new value nodes may be added to the taxonomy. This is evidenced
by the numerous domain-specific value systems that have been proposed by
different social scientists [13], as well as our work with stakeholders (see the
example of the evolving value taxonomy for uHelp in Subsection 4.2.2 and
Figure 6). Again, this is also aligned with the value-sensitive design work
on value change taxonomies [87].
Now that we have shown the relationship of our research on VTM with
that from social psychology, we next move to consider how our experiences
in developing the formal model and working with a range of stakeholders
on designing value-aware AI enable us to set out a roadmap for building
successful AI systems that can explicitly reason about values.
6. A Research Roadmap for Reasoning about Value-Aligned Be-
haviour
This section presents a roadmap of what we believe are the key research
challenges for achieving provable value-aligned behaviour. The overarching
objective of the research presented in this paper is to start an integrated
process to put ourselves in a position where we can design AI systems that
can explicitly reason about their behaviour and the behaviour of others
from the perspective of human values. The different types of systems whose
behaviourwewishanAIsystemtobeabletoanalysecanbe1)astand-alone
AIsystem(suchasarobotworkingwithapatient);2)ahumansystem(such
as a group of doctors who would like feedback from the AI on the alignment
of their potential actions with different relevant values); and 3) a hybrid
system of interacting human and AI agents (such as a factory in which
robots and human are working together, where the AI needs to explicitly
represent the values of the organisation so that continued negotiation and
understanding can take place).
Three key factors drive the design of our research roadmap [63]: a)
understand the needs of the research on values and AI; b) readily assess
the current state of research; and c) identify key research goals and plan
their research studies (strategies) accordingly. Furthermore, proposing this
47research roadmap can support researchers in understanding the big picture
of this field, maintaining focus on the vital research goals and priorities.
From our experience working in this field, which includes an investiga-
tion into social psychology, working with different stakeholders to identify
and formally specify their relevant values and developing value alignment
mechanisms, we identify four key challenges: 1) the value identification and
representation challenge, 2) the value aggregation & agreement challenge,
3) the value-aware decision-making challenge, and 4) the value-aligned mul-
tiagent system challenge.
Weaddresseachofthesechallengesinthefollowingfoursubsections,that
are structured as follows: 1) describes the research challenge, 2) presents
the key work in this area, and 3) set out the research roadmap by detailing
the research goals within each challenge and some associated proposals for
research strategies. VTM would benefit these research goals and strategies
and the potential integration of that research, which we reiterate in the fifth
and final subsection.
6.1. The value identification and representation challenge
Foranyreasoningovervaluestobepossible, therelevantvaluesinwhich
the AI system will operate should first be identified and then computation-
ally represented to allow for automatic reasoning. This is typically broken
down into two stages. The first is known as value identification. It is the
challenge of establishing the relevant values for a given individual or collec-
tive in the operating context within which the AI is being developed. In our
ownreal-worldexperiencesandlookingatkeyworkinthisfield(aspresented
shortly), this includes understanding what words are used as abstract value
concepts, such as “fairness” or “transparency”. The second stage is called
value representation. It is the challenge of providing formal computational
models of those values and their underlying structure, including identify-
ing their inter-relationships and relative importance and representing their
grounding semantics.
Broadly, three different approaches may be followed for value identifica-
tion and representation: 1) offline approaches, where relevant stakeholders
manually identify and specify their values; 2) online approaches, where AI
mechanisms like machine learning are used to identify and possibly specify
values; and 3) mixed approaches, where AI mechanisms collaborate with
users to help identify and represent values.
All these approaches aim to identify the relevant human values for an
individual or a collective. Whether values are manually provided or the AI
has learned those values from observing human behaviours, we stress that
48these values always represent what is significant for the human stakeholders
rather than what is significant for the AI.
Key research in this area. Current AI work on this topic has focused chiefly
on value identification, eliciting and learning relevant values from (typically,
written records of) the interactions of human agents. Natural language pro-
cessing techniques are being used to estimate underlying human values from
text in a (semi-) automatic manner. For instance, Liu et al. [45] analyses
values based on words used in e-commerce reviews, and Lin et al. [43] esti-
mates relevant values in tweets by combining textual features and context
knowledge from Wikipedia. Brugnoli et al. [10] uses a neural network model
to label tweets according to the Moral Foundations Theory [33]. However,
these techniques are employed only once a predefined high-level value list
has been selected, such as the well-known Schwartz value system [77]. Using
any pre-defined fixed list is a limitation not only in the assumption that the
list is appropriate for the context but also prevents values from changing
over time, a view we share with the value-sensitive design community [87].
Amongsttheapproachesnotstartingwithapredefinedvaluelistbutsetting
out to identify the relevant values can be found in the work of Wilson et
al. [91], which presents a crowd-powered algorithm to generate a hierarchy
of general values. Another is Axies [44], which uses human and automatic
techniquesforidentifyingcontext-specificvaluesusingnaturallanguagepro-
cessing.
Key research goals with some suggested strategies. The representation of
values in these existing approaches typically remains abstract. They are
articulated through textual headings or labels (such as ‘fairness’) without
further exploring the concrete relations, semantics, or importance of each
valuelisted, andthereisnomechanismfordeliberatingandreasoningabout
these values. The value representation we have presented uses property-
based nodes to formally specify the semantics of values (as property nodes
essentiallydefinehowavaluemaybeinterpretedandassessed)andexplicitly
specify value relations and importance.
Theidentifiedkeyresearchgoalswithassociatedstrategiesarepresented
next.
1. Extendingexistingresearchonvalueidentificationandelicitation(e.g.[44])
sothatrelationsbetweenthosevaluesinitiallyidentifiedbyhuman/AI
processes can be established. We argue that the relations between val-
ues are essential for deliberation. Once these relations are identified,
value taxonomies can be constructed to reflect them.
492. Developingmechanismsforconstructingpropertynodesforvaluesand
linking those property nodes to the abstract label nodes. This is cru-
cial for achieving any computational approach to building AI systems
that can explicitly reason about values, as property nodes make such
reasoning possible. Obtaining those property nodes, however, is a
challenging task, as we will illustrate shortly. Designers and engineers
must specify such property nodes formally, or an AI mechanism must
be designed to learn the property nodes that best describe a given
abstract value concept.
3. Developing mechanisms for eliciting value importance, as the impor-
tance of values is critical when understanding what values influence
which behaviours. As illustrated earlier, humans or AI may provide
such measures of importance. For example, an AI can learn which val-
ues are more important than others from past interactions. Humans
and AI can provide a partial order over a subset of the value nodes of
a taxonomy. Transformation mechanisms, such as that presented by
Serramia et al. [81], can transform a partial order into concrete mea-
sures for value importance assigned to each node appearing in that
partial order. Propagation mechanisms (such as our proposed Algo-
rithm 1) can then be developed to compute the importance measure
of the remaining nodes of the taxonomy to ensure coherence of im-
portance across the whole taxonomy. Such transformation and prop-
agation mechanisms will be helpful in practice because obtaining the
importancemeasureofeverytaxonomynodeisusuallyonlysometimes
feasible (as per the discussion below).
These three research goals focus on identifying value relations, value
semantics specified through property nodes, and value importance, which
are crucial elements for deliberating and reasoning over values and aid the
design of our value taxonomies.
However, if there is one thing we can learn from social psychology, it is
that values are complex and nuanced. It is only sometimes straightforward
forhumanstospecifytheirvaluetaxonomiesexplicitly. Whilemanyresearch
ethicists working in the field of value-sensitive design (e.g. [89]) have been
explicitlyelicitingrelevantvalueconceptsfromstakeholders, askingthatthe
users of technologies undertake such a process (as in the case of the uHelp
app) may be too demanding in practice. In addition to identifying abstract
value concepts, explicitly specifying value relations, importance, and formal
semantics is an even more significant challenge.
50Ourexperiencedemonstratesthatsomestakeholdersarereadytospecify
various value aspects, such as value concepts, relations, and importance.
This was the case of the medical doctors (see Subsections 4.1.1 and 4.1.2).
However,suchanexerciseisnotstraightforwardforallstakeholders. (Please
note that when we use the word stakeholders, we include all users in that
meaning.) When that is the case, our stance is that AI can be used to
help learn some of these value aspects from past interactions. We can then
expect stakeholders to have a broad understanding of what an AI system
has learned of their value systems and the explicit way the AI has chosen
to model them (i.e. the constructed value taxonomies). We can expect
these stakeholders to approve or disapprove of various aspects of the learned
value systems and guide the AI in learning and representing values. What is
difficult,ingeneral,istohaveanyexpectationthatallstakeholderswillhave
the time, willingness or capability to specify the value importance of each
node formally, the exact relationships between nodes, and the semantics of
different abstract value concepts. Integrating AI with human stakeholders
offers enormous potential benefits for helping individuals and organisations
specify their value systems.
6.2. The value aggregation and agreement challenge
While the value identification and representation challenge focuses on
identifying the relevant values of a single entity (e.g. individual, organisa-
tion, company) and their representation, the value aggregation and agree-
ment challenge focuses on the mechanisms required for defining the values
of a collective (a group of single entities). The objective is to move from a
set of individual value systems (with each value system in our model defined
as a set of value taxonomies) to a single collective one. That is, to identify
and formalise the values that best represent a collective by analysing the
values of the individuals in that collective. This is a challenging task, as
there usually is no clear consensus, resulting in inconsistencies between the
collective’s value system and individual ones.
Theresearchquestionsinclude: Howdowemovefromasetofindividual
value systems to one representing the collective? How do we maintain some
consistency between them (or not)? What does that consistency look like?
How do we deal with inconsistencies?
Key research in this area. Gabriel [31] argues that we live in a pluralis-
tic world with different entities holding different value systems. To ensure
behaviour in a multiagent system (MAS) is aligned with human values, de-
cisions are needed about the value system of the MAS as a whole. To arrive
51at the MAS value system, potential conflicting value systems of individuals
or subgroups of individuals need to be addressed. Gabriel [31] defines this
problem as identifying the value system that receives “reflective endorse-
ment despite widespread variation in people’s moral beliefs”. Pigmans et
al. [65, 64] highlight the challenges of addressing conflicting individual in-
terests in water policy-making and report on how deliberation around the
valuesystemsofdifferentstakeholderscanhelpaddresssuchconflicts. Other
work in this field uses computational social choice to aggregate individual
value systems and yield a consensus value system [41]. This approach con-
siders a range of ethical approaches, from utilitarian (maximum utility) to
egalitarian (maximum fairness).
Key research goals with some suggested strategies. Whilst research on value
aggregationandagreementsisbeginningtoemerge,manychallengesremain.
Aswiththevalueidentificationandrepresentationchallenge,thenoveltyand
challenge here will be to focus on value relations, value semantics (specified
through property nodes), and value importance, which are necessary for
deliberating and reasoning over values.
Theidentifiedkeyresearchgoalswithassociatedstrategiesarepresented
next.
1. Developing value aggregation mechanisms based on computational so-
cial choice. These mechanisms must consider the value relations, se-
mantics, and importance of the collective. In other words, complex
aggregation mechanisms are needed, advancing the work of Lera-Leri
et al. [41], not only to aggregate the value importance of individual
value concepts but to aggregate entire value taxonomies.
2. Developingvalueagreementmechanismsbasedonagreementtechnolo-
gies [62]. As an alternative to aggregation mechanisms that compute
thevaluesystemofacollective, agreementtechnologiesthatusemech-
anisms such as argumentation [66] and negotiation [46] can be used
to support the constituent individual’s reaching an agreement on the
adopted value system of a proposed collective by deliberating over
the relations, semantics and importance of values. The objective is
to collectively agree on the value taxonomy that best represents the
collective as a whole.
We note that in both these research goals, an individual’s value system
mayormaynotchange. Thefocushereisondeterminingavaluesystemfor
the collective. As such, conflicts between individual value systems and the
52valuesystemofthecollectivemightarise. Supposethedegreeofincoherence
is sufficiently strong. This may trigger the individualto take no further part
inthatcollectiveandlookforalternativecollectivesbetteralignedwiththeir
value system. In other situations, an individual might find it justifiable to
interact within the collective and recognise its value system, regardless of
whether they decide to take actions that adhere to that value system.
Cases might also arise where the value agreement process or the ex-
perience of interacting within a new collective value system might provide
evidence for individuals to update their own individual value systems. The
impact of experiences on the evolution of value systems over time is an ad-
ditional line of research that merits further study and analysis.
6.3. The value-aware decision making challenge
Identifying value systems of individuals and collectives (Subsections 6.1
and 6.2) provide the basis for reasoning over values. Armed with the knowl-
edge of their own value system, that of the collective to which they belong,
and those of fellow individuals in the collective, an agent can reason about
how to behave accordingly. Based on this understanding, they can decide
whichactionstoperformornot,includingwhichgroupsorcollectivestojoin
or leave. The computational challenge concerns developing enhanced value-
aware decision-making mechanisms considering different value systems.
Key research in this area. In value-driven decision-making, persuasion has
been one approach to motivating an agent to act in a specific way. In
the work of Bench-Capon and Atkinson [7], an argumentation framework
is presented where the stance is that persuasion relies on the strength of
arguments, which depends on advanced social values. In the work of di
Tosto and Dignum [21], an agent model is described where agent actions are
driven by their needs and values, where values are used to prioritise those
needs.
In the work of Chhogyal et al. [14], trust has been explored as a mecha-
nism for influencing decision-making, where the past reliability of an agent’s
actions is used to decide whether that agent can be trusted or not. The
argument is made that when past experiences cannot be used to assess the
reliability of others, the sharing of values between the trustor and trustee
can help, and an approach is developed to evaluate trust based on the de-
gree to which shared values can be established. In the work of Cranefield
et al. [17], reasoning about values is used to help agents make choices over
plans to adopt. Whereas in [70], values are used to help medical personnel
make more value-aware decisions.
53Key research goals with some suggested strategies. The main objective of
the work on values in AI is to prove that behaviour aligns with human val-
ues: thevaluealignmentproblem. Toachievethis,mechanismsforreasoning
aboutvaluesareneeded,includingdecision-makingandexplainabilitymech-
anisms. Introducing property nodes to our values taxonomies provides the
key construct for computational reasoning over values and, as such, the key
to developing such mechanisms.
Next, we present the vital research goals and associated potential re-
search strategies.
1. Developing mechanisms for reasoning about actions from the perspec-
tiveofgivenvaluesystems. Differentapproachescouldbeinvestigated
here, such as adopting practical reasoning in cognitive agent models.
One possibility is to extend existing BDI models to include value con-
cepts (and taxonomies). As illustrated in Subsection 5.5, the relation
between values and beliefs will require an in-depth multidisciplinary
assessment.
Another approach could investigate a value-enhanced theory of mind,
where agents can observe each others’ actions, build a model of each
other accordingly, and reason about those actions and their under-
lying intentions, as in [51, 54]. The main focus here would be on
incorporating value taxonomies into the agent models to reason over
the underlying values driving others’ behaviour. Understanding the
values driving others could help people make value-aware decisions.
For example, people can improve their strategies when they better
understand those they interact with. Also, as we discuss in our next
challenge, understanding the value system of others can help us better
persuade and influence them to modify their behaviour.
We reiterate here that while values have been chiefly used as labels in
the literature, value taxonomies that specify value relations, seman-
tics and importance are needed for enhanced value-aware reasoning
mechanisms.
2. Developing value-driven deliberation mechanisms that influence be-
haviourthroughpersuasion,argumentation,ornegotiation. Thiswould
extend existing work, such as that of [7], with more sophisticated
value-based reasoning by considering value relations, semantics and
importance.
Furthermore, instead of persuading how one should act based on the
value alignment of those actions, one could also try to persuade or
54argue about the value system of another and how it could be updated.
This is related to theevolution of values, mentioned at the endof Sub-
section 6.2. Convincing others to change their value taxonomies can
be an indirect approach to influencing their behaviour. This could be
achieved by developing negotiation/argumentation mechanisms that
individuals can use to influence each other’s value systems, either by
introducing new values or changing the importance of existing values.
Swaying individuals to modify their value systems can help find novel
solutions for mutual agreements. Traditional negotiation and argu-
mentation mechanisms try to find acceptable solutions for all parties
involved. By modifying individuals’ value systems, we can enhance
the traditional mechanisms as new value systems can open the door
to new solutions that would otherwise be off the table.
3. Developing value-based explainability mechanisms that help humans
understand the implications of their actions regarding the values they
promoteordemote,aswellasgainabetterunderstandingofthevalues
motivating other agents (human or software) to act in specific ways.
Thisrequiresmechanismsforreasoningaboutthepossibleimplications
ofchosenactionsandunderstandinghowvaluealignmentisevaluated,
which our value taxonomy provides through property nodes.
4. Conductingcomprehensiveinterdisciplinaryinvestigationsintotherole
of emotions and other constructs in motivating or influencing be-
haviour. While we stress the importance of considering the role of
values in guiding behaviour, Schwartz talks about the role of emo-
tions and how behaviour can be explained in terms of one’s attitudes,
beliefs, traits and social norms. Emotions have received growing at-
tention in the AI literature, and as such, relating affect (the display or
experience of emotion) to values and/or behaviour could be one of the
next steps in AI research on values. Schwartz [77], for example, states
that “[w]hen values are activated, they become infused with feeling.”
If the affect is linked to values and is not an independent motivator in
guiding behaviour, then linking the proposed value taxonomies with
that affect would constitute the first step in this novel research line.
However, in addition to studying the role of affect, one might learn
other motivators, such as attitudes, traits, or beliefs, and how they
relate to values, as already discussed in Subsection 5.5. This line of
enquirywillrequireacriticalmultidisciplinaryanalysisfromthesocial
sciences and humanities.
556.4. The value-aligned multiagent system challenge
While the third challenge focuses on the value alignment of an individ-
ual’s reasoning and decision-making process, this fourth challenge concerns
designing and developing MASs whose overall behaviour is provably aligned
with some set of human values. The objective is to focus not on the individ-
ual agents (human or software) but on the system mediating those agents’
behaviour and whether its mediation results in better-aligned behaviour.
The research questions here are: how do we design a system that pro-
motesvaluealignment? Howcansuchasystemevolveandadapttomaintain
optimal alignment?
Keyresearchinthisarea. Thedesignoftechnologiesalignedwithourhuman
values is a well-established field known as value-sensitive design (VSD) [29].
TheVSDapproachessentiallyrunsconceptualinvestigations(usuallythrough
participatory design workshops with stakeholder groups) to understand the
stakeholders of a given technology and their aspired values; empirical in-
vestigations to inform technology designers of those values; and technical
investigations to evaluate the adherence of the system behaviour with the
desired values and analyse how people use the technology. Sometimes, new
considerations emerge due to how people use the technology, and the pro-
cessisrepeated. WhilstVSDreliesonofflineparticipatorydesignandoffline
evaluations, the proposed AI research complements this approach by pro-
viding an online verification mechanism that computationally assesses the
degree to which these systems align with human values.
Since norms have been traditionally used in MAS to mediate behaviour,
proposed mechanisms that assess a MAS’s alignment have been reduced to
assessing the value alignment of the MAS’s norms. If a set of norms brings
about outcomes more aligned with a given value system, the set of norms
and its corresponding MAS are said to be aligned with that value system.
The research in this field has focused chiefly on choosing an optimal set of
normsthatoptimisesthevaluealignmentoftheMAS[55,80]. Intheworkof
Serramia et al. [80], norm synthesis is automated and based on preliminary
knowledge of which norms promote which values. The work of Montes and
Sierra [55] proposes a value-promoting norm synthesis approach that, in
essence, optimises the value alignment mechanism proposed by Sierra et
al. [83]. In that work [83], value preferences are understood as preferences
over world states and the value alignment of a set of given norms is based
on the degree to which those norms move us towards preferred states.
An interesting application area is that of [2, 18], where simulations are
usedtoanalysenorms(policies)fromtheperspectiveofthevaluesoffighting
56inequality and discrimination.
Key research goals with some suggested strategies. Subsection 6.3 discussed
the research goals for reasoning about values on a micro level: the agent
level and its decision-making processes. This subsection presents the re-
search goals for reasoning about values on a macro level: the multiagent
system level and its self-governance. As such, similar research goals are re-
peated here —namely, reasoning over value alignment, explainability, and
deliberation mechanisms— except that the focus here is on the alignment
of the MAS as a whole. The identified key research goals with associated
strategies are presented next.
1. Developing mechanisms for assessing the value alignment of norms.
Despite some success with preliminary research, some mechanisms
reason about values without fully understanding their semantics [80].
Providing value semantics (specified through property nodes in our
proposed taxonomy) enhances the ability to reason about value align-
ment, resulting in better explanations (discussed shortly).
Another pitfall of existing mechanisms is that they require significant
manualworkfromthehumansidetospecifyvaluesemantics. However,
this issue is addressed by the value identification and representation
challenge of Subsection 6.1.
2. Developing explanation mechanisms that support human users in un-
derstanding why one set of norms is preferred to another with respect
to a given value system. These mechanisms could build on the value
alignment mechanisms discussed above. By relying on value semantics
to assess the alignment, explanations can be more informative.
Explanations could support the design of new MAS but also support
policy-making and protocol design in general, where AI tools could
help explain when and how norms should evolve. Examples of such
norms include medical protocols, emergency protocols, or policies for
regulating irrigation practices.
3. Developingmechanismsforvalue-awareself-governanceinMAS.These
would allow a MAS to analyse for itself which norms promote specific
valuesbetterandadaptitsnormsaccordingly. Here,thereareacouple
of research lines that could be investigated.
First, there is the issue of discovering suitable norms for a given MAS.
Norm synthesis, formal analysis of norms, multiagent simulation, and
57AI-based optimisation techniques can be used to explore the space of
normative systems, searching for the optimally aligned set of norms
(building on previous work such as [52, 53]). The objective would be
to factor in relevant values when discovering suitable norms to help
analyse their value alignment and provide explanations accordingly.
Second, there is the issue of agents in a MAS reaching agreements on
the norms that best govern those societies. The objective here is to
develop mechanisms that support groups of agents in finding the best
set of norms to mediate their MAS interactions. Value-driven deliber-
ation mechanisms —assisted by norm discovery, norm assessment and
explanation mechanisms from the first research line— can provide the
support needed for reaching collective agreements on which norms to
adopt or how norms evolve.
6.5. A note on VTM’s contribution to future work
In setting out the research challenges, related research goals and asso-
ciated research strategies presented above, it becomes clear that one main
recurring drawback of current research is its need for more consideration
of certain aspects of values. For example, in some cases, value concepts
are identified but not their importance, although the importance of values
ultimately influences behaviour. In other cases, value importance is consid-
ered, butnotvaluesemantics, whichcriticallylimitsreasoningaboutvalues.
Value semantics are needed for enhanced value reasoning and explainability.
Thesepitfallsareanaturalresultofthelackofanyagreedconsensualformal
modelforrepresentingvalues. Ourproposedvaluetaxonomiestacklethisby
introducing value relations, semantics, and importance into a structurally
coherent and conceptually intuitive foundational model for value represen-
tation.
7. Reflections on the Strengths and Challenges of the VTM Model
After proposing our formal model of human values, showing its critical
relationship with research outside of computer science, and exploring how it
canbeusedtobuildaroadmapforvalue-alignedAIsystems, wenowreflect
on whether our work has fulfilled the principles we set out in Section 1.
Using Formal Methods. According to Rohan [71], research into values the-
ory has suffered from “definitional confusion” due to the word values being
abused and overused by non-psychologists and psychologists alike. The for-
mal approach to building the VTM model provides a clear, precise and
58consistent approach for defining various value-related concepts (like values,
value priorities and value systems) (Principle 1). Furthermore, VTM is
designed to be a foundation for data structures and algorithmic design nec-
essary for value-based reasoning (Principle 2). Recall that the interest in
workingwithvalues, asSchwartz[75]putsit, isbecausevaluesare“abstract
motivations that guide, justify, and explain attitudes, norms, opinions, and
actions.”. For us, the objective of modelling values computationally means
thatwecansystematicallyevaluatethealignmentofAIorhumanbehaviour
with values. Last, our proposed model also subsumes and relates to estab-
lished concepts in AI research (Principle 4). The roadmap illustrates how
our model can help extend and build upon existing research in the field.
Remaining Agnostic to Implementation Choices. In setting out any foun-
dational model, choices about representation and language must be made.
Each choice carries opportunities and limitations. It is essential to be mind-
ful of any implicit assumptions or representation limitations a choice may
introduce. Thisiswhyoneofourguidingprinciples(Principle3)istobeag-
nostic on specific implementation choices, so we focus on theoretical notions
that provide clarity over implementation choices. While we deliberately
leave the implementation choice open for system and research development,
we do not hesitate to propose some initial implementation choices that,
along with our running example, better illustrate our model’s significance
and implications.
Being Grounded in Social Psychology. When we assume the task of mod-
elling something as nuanced and complex as human values, we feel obli-
gated to start from the social sciences. In particular, we ground our VTM
in the wealth of research from social psychology (Principle 6), aligning with
the views of Schwartz and other leading researchers [71]. By doing so, we
have set out to design an academically significant and conceptually intuitive
model for non-computer-scientists as it is for AI researchers. This concep-
tual intuitiveness supports the promise of becoming an agreed conceptual
framework for future interdisciplinary research and the practical develop-
ment of value-based AI systems. It is with this in mind that we developed
the VTM research roadmap.
Remaining Agnostic to Specific Theories of Values. We also argue that a
computationalmodelofhumanvaluesshouldbeagnostictoaspecifictheory
of values (Principle 7) so it aligns with proposals from the social psychol-
ogy literature in general. This is precisely what VTM achieves. It allows
59the specification of contextual dynamic value systems that accommodate
different values, such as moral, economic and epistemic values.
Demonstrating Applicability to Real-Life Scenarios. An essential challenge
in modelling human values is ensuring the expressiveness and practicality
of the resulting model so it applies to real-life scenarios (Principle 5) with
demonstrablepracticalbenefit. OurVTMmodelwasinspiredbyourcollab-
oration with real stakeholders in different settings (e.g., social networking
applications, hospital working practices for doctors, and firefighter organi-
sations in fire stations). Our practical work has supported the development
of the formal model, and the formal model has given us the foundational
and structural grounding to design our value-aware AI systems. In addi-
tion, we have detailed a running example of a fully implemented system
that demonstrates our model’s applicability, impact, and potential.
Staying Mindful of the Challenges of Formal Modelling. Modelling features
of our natural world necessitates decisions about what to capture, what to
ignoreandwhatconceptsandrelationshipsarebesttodoitmosteffectively.
We make sense of our world through our models by determining all of the
relevantpropertieswewanttoholdandthencheckingthattheydo. Working
out the set of all relevant properties is a significant challenge, and once we
have them, verifying the model against those properties is usually more
straightforward.
When modelling human values, with the objective of reasoning about
behaviour from the perspective of these values, we face the same classical
challenge of capturing what is relevant and finding the best possible repre-
sentation. Values that we want to specify and embed in the decision-making
processes are human values, and as such, these values need to come from a
diverse range of human stakeholders: users, designers, owners of the artifi-
cial systems, other persons or bodies directly or indirectly affected by the
AI system and so on. This makes the capturing of relevant values an even
moresignificantchallenge. Furthermore, applyingVTMinpracticepresents
new challenges (more than, say, analysing text or speech to look for relevant
values) as there is a need to specify a significant amount of information: the
importanceofvalues,theirsemantics(thepropertynodes),andtherelations
between them. There is little doubt that applying our model to gain even a
partial understanding of values that can be used in a computational system
presents the greatest practical challenge.
608. Concluding Remarks
We have contributed to the urgent challenge of building value-aligned
AI by proposing a conceptually intuitive foundational model for human val-
ues. This model allows for future computational reasoning and opens up
opportunities to evidence how AI systems are provably aligned with human
values. The approach is grounded in social psychology, subsumes exist-
ing AI research concepts, and is formal, making it a coherent and intuitive
starting point for future interdisciplinary research investigation. Our expe-
riences working with professionals in various real-life application domains,
with the objective of understanding and formalising their values and devel-
opingmechanismstoassessthevaluealignmentofdifferentbehaviours,have
guided us in developing our model. To our knowledge, this is the first pro-
posal for the formal representation of human values and moves beyond the
state-of-the-art —which to date has defined values through labels [44, 80]
or goals [83, 55]— by explicitly introducing notions of value importance,
semantics, and relations.
Theultimateaimofthisresearchisnotjusttoprovidethefoundationfor
furthertheoreticalresearchbutalsotodrivethepotentialforpractical, real-
lifeapplicationsthatsupportthedecision-makingprocessofhumansandAI
systems. In addition to designing AI that is provably aligned with human
values, we also want to design AI that raises awareness of value systems
at the level of individuals, groups, and organisations, unlocking humans’
capability for value-aware decision-making.
The topic of value alignment, both in AI and in our everyday lives,
is gaining ground. A growing number of online sources promise methods
to help people discover their values to live them out effectively [15, 28,
79]. We hope that with this model and roadmap, new technologies can be
designed that support humans individually and collectively identify their
value systems more clearly, and enable “best possible living”, as proposed
by Rohan [71].
Acknowledgements
We are grateful to our friends and colleagues Carles Sierra, Pablo Nor-
iega and the IIIA community who have always been there to support us
during the development of our research. This work has been supported by
the EU-funded VALAWAI (# 101070930) project and the Spanish-funded
VAE (# TED2021-131295B-C31) and Rhymas (# PID2020-113594RB-100)
projects.
61References
[1] Thomas ˚Agotnes, Wiebe Van Der Hoek, Juan A Rodr´ıguez-Aguilar,
Carles Sierra, and Michael J Wooldridge. On the logic of normative
systems. In IJCAI, volume 7, pages 1175–1180, 2007.
[2] Alba Aguilera, Nieves Montes, Georgina Curto, Carles Sierra, and Nar-
dine Osman. Can poverty be reduced by acting on discrimination?
an agent-based model for policy making. In N. Alechina, V. Dignum,
M. Dastani, and J.S. Sichman, editors, AAMAS ’24: 23rd Interna-
tional Conference on Autonomous Agents and Multi-Agent Systems.
IFAAMAS, 2024.
[3] Bob Altemeyer. The other “authoritarian personality”. In Mark P.
Zanna, editor, Advances in Experimental Social Psychology, volume 30,
pages 47–92. Academic Press, 1998.
[4] F.C. Bartlett. Remembering: A Study in Experimental and Social Psy-
chology. Camb. Psychol. Libr. Cambridge University Press, 1995.
[5] T. L. Beauchamp. The ’four principles’ approach to health care ethics.
In Richard E. Ashcroft, editor, Principles of Health Care Ethics. Wiley,
2007.
[6] Tom L. Beauchamp and James F. Childress. Principles of Biomedical
Ethics. Oxford University Press, eighth edition, 2019.
[7] Trevor Bench-Capon and Katie Atkinson. Abstract argumentation and
values. In Guillermo Simari and Iyad Rahwan, editors, Argumentation
in Artificial Intelligence, pages 45–64. Springer US, Boston, MA, 2009.
[8] Wilmar F. Bernthal. Value perspectives in management decisions. The
Journal of the Academy of Management, 5(3):190–196, 1962.
[9] ValerieBraithwaiteandRussellBlamey. Consensus,stabilityandmean-
ing in abstract social values. Australian Journal of Political Science,
33(3):363–380, 1998.
[10] Emanuele Brugnoli, Pietro Gravino, and Giulio Prevedello. Moral val-
ues in social media for disinformation and hate speech analysis. In
Preproceedings of the Value Engineering in AI Workshop, at 26th Eu-
ropean Conference on Artificial Intelligence (ECAI 2023), 2023.
62[11] Georgios Chalkiadakis, Edith Elkind, and Michael Wooldridge. Com-
putational aspects of cooperative game theory. Synthesis Lectures on
Artificial Intelligence and Machine Learning, 5(6):1–168, 2011.
[12] Raja Chatila, Virginia Dignum, Michael Fisher, Fosca Giannotti,
Katharina Morik, Stuart Russell, and Karen Yeung. Trustworthy AI.
In Reflections on Artificial Intelligence for Humanity, volume 12600 of
Lecture Notes in Computer Science, pages 13–39. Springer, 2021.
[13] An-Shou Cheng and Kenneth R. Fleischmann. Developing a meta-
inventory of human values. In Proceedings of the 73rd ASIS&T Annual
Meeting on Navigating Streams in an Information Ecosystem - Volume
47, ASIS&T’10, USA,2010.AmericanSocietyforInformationScience.
[14] Kinzang Chhogyal, Abhaya C. Nayak, Aditya Ghose, and Hoa Khanh
Dam. A value-based trust assessment model for multi-agent systems.
In IJCAI, pages 194–200. ijcai.org, 2019.
[15] Irina Cozma. How to find, define, and use your values.
https://hbr.org/2023/02/how-to-find-define-and-use-your-values, Feb
2023. Accessed: 2023-12-15.
[16] Stephen Cranefield and Michael Winikoff. Verifying social expectations
bymodelcheckingtruncatedpaths. Journal of Logic and Computation,
21(6):1217–1256, 2011.
[17] Stephen Cranefield, Michael Winikoff, Virginia Dignum, and Frank
Dignum. No pizza for you: Value-based plan selection in bdi agents.
In Proceedings of the Twenty-Sixth International Joint Conference on
Artificial Intelligence, IJCAI-17, pages 178–184. ijcai.org, 2017.
[18] Georgina Curto, Nieves Montes, Carles Sierra, Nardine Osman, and
FlavioComim. Anormoptimisationapproachtosdgs: tacklingpoverty
byactingondiscrimination. InLucDeRaedt,editor,Proceedings of the
Thirty-First International Joint Conference on Artificial Intelligence,
IJCAI 2022, Vienna, Austria, 23-29 July 2022, pages 5228–5235. ij-
cai.org, 2022.
[19] Janet Davis and Lisa P Nathan. Value sensitive design: Applications,
adaptations,andcritiques.Handbookofethics, values, andtechnological
design: Sources, theory, values and application domains, pages 11–40,
2015.
63[20] LavindraDeSilva, FelipeRechMeneguzzi, andBrianLogan. Bdiagent
architectures: A survey. In Proceedings of the 29th International Joint
Conference on Artificial Intelligence (IJCAI), 2020, Jap˜ao., 2020.
[21] Gennaro di Tosto and Frank Dignum. Simulating social behaviour im-
plementing agents endowed with values and drives. In Francesca Gi-
ardini and Fr´ed´eric Amblard, editors, Multi-Agent-Based Simulation
XIII - International Workshop, MABS 2012, Valencia, Spain, June
4-8, 2012, Revised Selected Papers, volume 7838 of Lecture Notes in
Computer Science, pages 1–12. Springer, 2012.
[22] Mark d’Inverno, David Kinny, Michael Luck, and Michael Wooldridge.
Aformalspecificationofdmars. InMunindarP.Singh,AnandRao,and
Michael J. Wooldridge, editors, Intelligent Agents IV Agent Theories,
Architectures, and Languages, pages 155–176, Berlin, Heidelberg, 1998.
Springer Berlin Heidelberg.
[23] Mark d’Inverno and Michael Luck. Development and application of a
formal agent framework. In First IEEE International Conference on
Formal Engineering Methods, pages 222–231, 1997.
[24] GeorgeW.England. Personalvaluesystemsofamericanmanagers. The
Academy of Management Journal, 10(1):53–68, 1967.
[25] European Commission. Ethics guidelines for trustworthy AI, April
2019. https://ec.europa.eu/digital-single-market/en/news/
ethics-guidelines-trustworthy-ai.
[26] European Commission. Proposal for a regulation of the euro-
pean parliament and of the council: laying down harmonised rules
on artificial intelligence (Artificial Intelligence Act) and amending
certain union legislative acts. COM/2021/206 final, with Proce-
dure Number 2021/0106/COD, 2021. https://eur-lex.europa.eu/
legal-content/EN/ALL/?uri=CELEX:52021PC0206.
[27] Norman T Feather. Values, deservingness, and attitudes toward high
achievers: Researchontallpoppies. InCliveSeligman,JamesM.Olson,
and Mark P. Zanna, editors, The psychology of values: The Ontario
symposium, volume 8, pages 215–251. Lawrence Erlbaum Associates,
Inc, Mahwah, NJ, 1996.
[28] Forbes Expert Panel. 15 effective ways
to discover and articulate your core values.
64https://www.forbes.com/sites/forbescoachescouncil/2022/02/18/15-
effective-ways-to-discover-and-articulate-your-core-values/, Feb 2022.
Accessed: 2023-12-15.
[29] Batya Friedman, David G. Hendry, and Alan Borning. A survey of
valuesensitivedesignmethods.Found.TrendsHum.-Comput.Interact.,
11(2):63–125, nov 2017.
[30] Batya Friedman, Peter H Kahn, Alan Borning, and Alina Huldtgren.
Value sensitive design and information systems. Early engagement and
new technologies: Opening up the laboratory, pages 55–95, 2013.
[31] Iason Gabriel. Artificial intelligence, values, and alignment. Minds
Mach., 30(3):411–437, sep 2020.
[32] August Corrons Gim´enez and Llu´ıs Garay Tamaj´on. Analysis of the
third-order structuring of shalom schwartz’s theory of basic human val-
ues. Heliyon, 5(6):e01797, 2019.
[33] Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer,
Sean P. Wojcik, and Peter H. Ditto. Chapter two - moral foundations
theory: The pragmatic validity of moral pluralism. In Patricia Devine
and Ashby Plant, editors, Advances in Experimental Social Psychology,
volume 47, pages 55–130. Academic Press, 2013.
[34] William D Guth and Renato Tagiuri. Personal values and corporate
strategy. Harvard Business Review, 43(5):123–132, 1965.
[35] Pat Duffy Hutcheon. Value theory: Towards conceptual clarification.
The British Journal of Sociology, 23(2):172–187, 1972.
[36] IEEE Standards Association. The IEEE global initiative.
https://standards.ieee.org/industry-connections/ec/
autonomous-systems/.
[37] Anna Jobin, Marcello Ienca, and Effy Vayena. The global landscape of
ai ethics guidelines. Nature Machine Intelligence, 1(9):389–399, 2019.
[38] ClydeKluckhohn. Valuesandvalue-orientationsinthetheoryofaction:
An exploration in definition and classification. In Toward a general
theory of action, pages 388–433. Harvard university press, 1951.
[39] Andrew Koster, Jordi Madrenas-Ciurana, Nardine Osman, W. Marco
Schorlemmer, Jordi Sabater-Mir, Carles Sierra, Dave de Jonge, Angela
65Fabregues, Josep Puyol-Gruart, and Pere Garcia-Calv´es. u-help: Sup-
porting helpful communities with information technology. In Sascha
Ossowski, Francesca Toni, and George A. Vouros, editors, Proceedings
of the First International Conference on Agreement Technologies, AT
2012, volume 918, pages 378–392. CEUR-WS.org, 2012.
[40] Andrew Koster, Jordi Madrenas-Ciurana, Nardine Osman, W. Marco
Schorlemmer,JordiSabater-Mir,CarlesSierra,AngelaFabregues,Dave
de Jonge, Josep Puyol-Gruart, and Pere Garcia-Calv´es. u-Help: sup-
porting helpful communities with information technology. In Maria L.
Gini, Onn Shehory, Takayuki Ito, and Catholijn M. Jonker, editors,
International conference on Autonomous Agents and Multi-Agent Sys-
tems, AAMAS’13, SaintPaul, MN,USA,May6-10, 2013,pages1109–
1110. IFAAMAS, 2013.
[41] Roger Lera-Leri, Filippo Bistaffa, Marc Serramia, Maite L´opez-
S´anchez, and Juan A. Rodr´ıguez-Aguilar. Towards pluralistic value
alignment: Aggregating value systems through lp-regression. In Piotr
Faliszewski, Viviana Mascardi, Catherine Pelachaud, and Matthew E.
Taylor, editors, 21st International Conference on Autonomous Agents
and Multiagent Systems, AAMAS 2022, Auckland, New Zealand, May
9-13, 2022, pages 780–788. International Foundation for Autonomous
Agents and Multiagent Systems (IFAAMAS), 2022.
[42] Kurt Lewin. Constructs in field theory. In Kurt Lewin and
D. Cartwright, editors, Field Theory in Social Science: Selected Theo-
retical Papers, pages 30–42. Tavistock, London, 1952.
[43] YingLin, JoeHoover, MortezaDehghani, MarlonMooijman, andHeng
Ji. Acquiringbackgroundknowledgetoimprovemoralvalueprediction.
2018 IEEE/ACM International Conference on Advances in Social Net-
works Analysis and Mining (ASONAM), pages 552–559, 2018.
[44] Enrico Liscio, Michiel van der Meer, Luciano C. Siebert, Catholijn M.
Jonker, Niek Mouter, and Pradeep K. Murukannaiah. Axies: Identify-
ing and evaluating context-specific values. In Proceedings of the 20th
International Conference on Autonomous Agents and MultiAgent Sys-
tems, AAMAS ’21, page 799–808, Richland, SC, 2021. International
Foundation for Autonomous Agents and Multiagent Systems.
[45] Hui Liu, Yinghui Huang, Zichao Wang, Kai Liu, Xiangen Hu, and Wei-
jun Wang. Personality or value: A comparative study of psychographic
66segmentationbasedonanonlinereviewenhancedrecommendersystem.
Applied Sciences, 2019.
[46] Fernando Lopes, Michael Wooldridge, and Augusto Q Novais. Nego-
tiation among autonomous computational agents: principles, analysis
and challenges. Artificial Intelligence Review, 29:1–44, 2008.
[47] Michael Luck and Mark d’Inverno. Structuring a z specification to pro-
videaformalframeworkforautonomousagentsystems. InJonathanP.
Bowen and Michael G. Hinchey, editors, ZUM ’95: The Z Formal
Specification Notation, pages 46–62, Berlin, Heidelberg, 1995. Springer
Berlin Heidelberg.
[48] David J.C. MacKay. Information Theory, Inference and Learning Al-
gorithms. Cambridge University Press, 2003.
[49] J.-L. Marichal. Aggregation Operators for Multicriteria Decision Aid.
PhD thesis, Institute of Mathematics, University of Li`ege, Li`ege, Bel-
gium, 1998.
[50] Paul McDonald and Jeffrey Gandz. Identification of values relevant to
business research. Human Resource Management, 30(2):217–236, 1991.
[51] Nieves Montes, Michael Luck, Nardine Osman, Odinaldo Rodrigues,
and Carles Sierra. Combining theory of mind and abductive reason-
ing in agent-oriented programming. Auton. Agents Multi Agent Syst.,
37(2):36, 2023.
[52] Nieves Montes, Nardine Osman, and Carles Sierra. A computational
model of ostrom’s institutional analysis and development framework.
Artificial Intelligence, 311:103756, 2022.
[53] Nieves Montes, Nardine Osman, and Carles Sierra. A computational
model of ostrom’s institutional analysis and development framework
(extended abstract). In Proceedings of the Thirty-Second International
Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th Au-
gust 2023, Macao, SAR, China, pages 6937–6941. ijcai.org, 2023.
[54] Nieves Montes, Nardine Osman, and Carles Sierra. Perspective-
dependent value alignment of norms. In Preproceedings of the Value
Engineering in AI Workshop, at 26th European Conference on Artifi-
cial Intelligence (ECAI 2023), 2023.
67[55] Nieves Montes and Carles Sierra. Value-guided synthesis of parametric
normative systems. In Frank Dignum, Alessio Lomuscio, Ulle Endriss,
and Ann Now´e, editors, AAMAS ’21: 20th International Conference
on Autonomous Agents and Multiagent Systems, Virtual Event, United
Kingdom, May 3-7, 2021, pages 907–915. ACM, 2021.
[56] Pablo Noriega, Harko Verhagen, Julian Padget, and Mark d’Inverno.
Design heuristics for ethical online institutions. In Nirav Ajmeri, An-
dreasa Morris Martin, and Bastin Tony Roy Savarimuthu, editors, Co-
ordination, Organizations, Institutions, Norms, and Ethics for Gov-
ernance of Multi-Agent Systems XV, pages 213–230, Cham, 2022.
Springer International Publishing.
[57] Pablo Noriega, Harko Verhagen, Julian Padget, and Mark d’Inverno.
Addressing the value alignment problem through online institutions. In
NicolettaFornara, JithinCheriyan, andAsiminaMertzani, editors, Co-
ordination, Organizations, Institutions, Norms, and Ethics for Gover-
nance of Multi-Agent Systems XVI, pages 77–94, Cham, 2023. Springer
Nature Switzerland.
[58] Martin J Osborne and Ariel Rubinstein. A course in game theory. MIT
press, 1994.
[59] NardineOsman,BrunoRosell,CarlesSierra,MarcoSchorlemmer,Jordi
Sabater-Mir, and Lissette Lemus. uHelp: intelligent volunteer search
for mutual help communities. CoRR, abs/2301.11112, 2023.
[60] Nardine Osman, Carles Sierra, and Jordi Sabater-Mir. Propagation
of opinions in structural graphs. In Helder Coelho, Rudi Studer, and
Michael J. Wooldridge, editors, Proceedings of the 19th European Con-
ference on Artificial Intelligence, ECAI 2010, volume 215 of Frontiers
in Artificial Intelligence and Applications, pages 595–600. IOS Press,
2010.
[61] Nardine Osman, Carles Sierra, Jordi Sabater-Mir, Joseph R. Wakeling,
Judith Simon, Gloria Origgi, and Roberto Casati. Liquidpublications
and its technical and legal challenges. In Intelligent Multimedia: Man-
aging Creative Works in a Digital World,volume8ofLegal Information
and Communication Technologies. European Press Academic Publish-
ing, Florence, 2010.
[62] Sascha Ossowski. Agreement Technologies. Springer, Dordrecht, 2013.
68[63] Nora Pekker. How to build a compelling ux re-
search roadmap. https://medium.com/design-ibm/
how-to-build-a-compelling-ux-research-roadmap-227448005c82,
Sep 2021. Accessed: 2023-10-19.
[64] K. Pigmans, H. Aldewereld, V. Dignum, and N. Doorn. The role of
value deliberation to improve stakeholder participation in issues of wa-
ter governance. Water Resources Management, 33:4067–4085, October
2019.
[65] Klara Pigmans, Neelke Doorn, Huib Aldewereld, and Virginia Dignum.
Decision-making in water governance: From conflicting interests to
sharedvalues. InLotteAsveld,RietjevanDam-Mieras,TsjallingSwier-
stra, Saskia Lavrijssen, Kees Linse, and Jeroen van den Hoven, edi-
tors, Responsible Innovation 3: A European Agenda?, pages 165–178.
Springer International Publishing, Cham, 2017.
[66] I.Rahwan. Argumentation in Artificial Intelligence. SpringerUS,2014.
[67] Pradeep Rai and Shubha Singh. A survey of clustering techniques.
International Journal of Computer Applications, 7(12):1–5, 2010.
[68] Anand S Rao and Michael P Georgeff. Modeling rational agents within
a bdi-architecture. Readings in agents, pages 317–328, 1997.
[69] Anand S Rao and Michael P Georgeff. Decision procedures for bdi
logics. Journal of logic and computation, 8(3):293–343, 1998.
[70] Manel Rodriguez-Soto, Nardine Osman, Carles Sierra, Paula S´anchez
Veja, Rocio Cintas Garcia, Cristina Farriols Danes, Montserrat Garcia
Retortillo, and Silvia Minguez Maso. Towards value awareness in the
medicalfield. In16th International Conference on Agents and Artificial
Intelligence - ICAART 2024, 2024.
[71] Meg J. Rohan. A rose by any name? the values construct. Personality
and Social Psychology Review, 4(3):255–277, 2000.
[72] Milton Rokeach. The nature of human values. Free press, 1973.
[73] YossiRubner,CarloTomasi,andLeonidasJ.Guibas.Theearthmover’s
distance as a metric for image retrieval. International Journal of Com-
puter Vision, 40(2):99—-121, nov 2000.
69[74] S. Russell. Human Compatible: Artificial Intelligence and the Problem
of Control. Penguin Publishing Group, 2019.
[75] S. H. Schwartz. Value orientations: Measurement, antecedents and
consequences across nations. In R. Jowell, C. Roberts, R. Fitzgerald,
and G. Eva, editors, . Measuring attitudes cross-nationally: Lessons
from the European Social Survey, chapter 9, pages ::::161–193. Sage,
2007.
[76] Shalom H Schwartz. Are there universal aspects in the structure and
contents of human values? Journal of social issues, 50(4):19–45, 1994.
[77] ShalomH.Schwartz. Anoverviewoftheschwartztheoryofbasicvalues.
Online Readings in Psychology and Culture, 2(1), 2012.
[78] Shalom H. Schwartz and Wolfgang Bilsky. Toward a universal psycho-
logical structure of human values. Journal of Personality and Social
Psychology, 53:550–562, 1987.
[79] Meg Selig. 6 ways to discover and choose your core values.
https://www.psychologytoday.com/us/blog/changepower/201811/6-
ways-discover-and-choose-your-core-values, Nov 2018. Accessed:
2023-12-15.
[80] Marc Serramia, Maite L´opez-S´anchez, and Juan A. Rodr´ıguez-Aguilar.
A qualitative approach to composing value-aligned norm systems. In
Amal El Fallah Seghrouchni, Gita Sukthankar, Bo An, and Neil Yorke-
Smith, editors, Proceedings of the 19th International Conference on
Autonomous Agents and Multiagent Systems, AAMAS ’20, Auckland,
New Zealand, May 9-13, 2020, pages 1233–1241. International Founda-
tion for Autonomous Agents and Multiagent Systems, 2020.
[81] Marc Serramia, Maite Lopez-Sanchez, Juan A. Rodriguez-Aguilar,
Manel Rodriguez, Michael Wooldridge, Javier Morales, and Carlos An-
sotegui. Moral values in norm decision making. In Proceedings of the
17th International Conference on Autonomous Agents and MultiAgent
Systems, AAMAS ’18, page 1294–1302, Richland, SC, 2018. Interna-
tional Foundation for Autonomous Agents and Multiagent Systems.
[82] Yoav Shoham and Moshe Tennenholtz. On social laws for artificial
agent societies: off-line design. Artificial intelligence, 73(1-2):231–252,
1995.
70[83] Carles Sierra, Nardine Osman, Pablo Noriega, Jordi Sabater-Mir,
and Antoni Perell´o. Value alignment: a formal approach. CoRR,
abs/2110.09240, 2021.
[84] Colin Stirling. Modal and Temporal Properties of Processes. Springer
Verlag, 2001.
[85] P.E. Tetlock, R.S. Peterson, and J.S. Lerner. Revising the value of
pluralism model: Incorporating social content and context postulates.
In C. Seligman, J. Olson, and M. Zanna, editors, The Psychology of
Values: The Ontario Symposium, Volume 8, pages 25–51. Erlbaum,
Hillsdale, NJ, 1996.
[86] UNESCO. Outcome document: first draft of the recommendation
on the ethics of artificial intelligence, Sept 2020. https://unesdoc.
unesco.org/ark:/48223/pf0000373434_eng.
[87] Ibo van de Poel. Design for value change. Ethics and Information
Technology, 2018.
[88] Ibo van de Poel and Lamb`er Royakkers. Ethics, technology, and engi-
neering: An introduction. John Wiley & Sons, 2023.
[89] Jeroen van den Hoven, Pieter E. Vermaas, and Ibo van de Poel. Design
for values: An introduction. In Jeroen van den Hoven, Pieter E. Ver-
maas, and Ibo van de Poel, editors, Handbook of Ethics, Values, and
Technological Design: Sources, Theory, Values and Application Do-
mains, pages 1–7. Springer Netherlands, Dordrecht, 2015.
[90] T. L. van der Weide, F. Dignum, J. J. Ch. Meyer, H. Prakken, and
G. A. W. Vreeswijk. Practical reasoning using values. In Lecture Notes
in Computer Science, pages 79–93. Springer Berlin Heidelberg, 2010.
[91] Steven Wilson, Yiting Shen, and Rada Mihalcea. Building and vali-
dating hierarchical lexicons with a case study on personal values. In
Steffen Staab, Olessia Koltsova, and Dmitry I. Ignatov, editors, Social
Informatics, Lecture Notes in Computer Science (LCNS), pages 455–
470, Switzerland, September 2018. Springer International Publishing
AG. 10th International Conference on Social Informatics 2018, SocInfo
2018 ; Conference date: 25-09-2018 Through 28-09-2018.
71