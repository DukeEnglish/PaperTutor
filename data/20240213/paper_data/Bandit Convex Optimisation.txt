Bandit Convex Optimisation
Tor Lattimore
Google DeepMind
These notes are in draft. Please let me know if you find any
mistakes, typos or missing references.
4202
beF
9
]CO.htam[
1v53560.2042:viXraContents
1 Introduction and problem statement 5
1.1 Prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Bandit convex optimisation . . . . . . . . . . . . . . . . . 6
1.3 Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.4 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2 Overview of methods and history 12
2.1 Methods for bandit convex optimisation . . . . . . . . . . 12
2.2 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.3 Summary table . . . . . . . . . . . . . . . . . . . . . . . 18
2.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3 Regularity 21
3.1 Convex bodies and the Minkowski functional . . . . . . . . 21
3.2 Smoothness and strong convexity . . . . . . . . . . . . . . 22
3.3 Scaling properties . . . . . . . . . . . . . . . . . . . . . . 23
3.4 Convex functions are nearly Lipschitz . . . . . . . . . . . . 23
3.5 Near-optimality on the interior . . . . . . . . . . . . . . . 24
3.6 Extension . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.7 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.8 Meta algorithms . . . . . . . . . . . . . . . . . . . . . . . 27
3.9 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
4 Bisection in one dimension 32
4.1 Bisection method without noise . . . . . . . . . . . . . . . 32
4.2 Bisection method with noise . . . . . . . . . . . . . . . . 33
4.3 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
1CONTENTS 2
5 Gradient descent 39
5.1 Gradient descent . . . . . . . . . . . . . . . . . . . . . . . 39
5.2 Spherical smoothing . . . . . . . . . . . . . . . . . . . . . 42
5.3 Algorithm and regret analysis . . . . . . . . . . . . . . . . 44
5.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
6 Self-concordant regularisation 47
6.1 Self-concordant barriers . . . . . . . . . . . . . . . . . . . 47
6.2 Follow the regularised leader . . . . . . . . . . . . . . . . 49
6.3 Optimistic ellipsoidal smoothing . . . . . . . . . . . . . . . 50
6.4 Algorithms and regret analysis . . . . . . . . . . . . . . . 53
6.5 Smoothness and strong convexity . . . . . . . . . . . . . . 56
6.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
7 Linear and quadratic bandits 61
7.1 Covering numbers . . . . . . . . . . . . . . . . . . . . . . 61
7.2 Optimal design . . . . . . . . . . . . . . . . . . . . . . . 62
7.3 Exponential weights . . . . . . . . . . . . . . . . . . . . . 62
7.4 Linear bandits . . . . . . . . . . . . . . . . . . . . . . . . 63
7.5 Quadratic bandits . . . . . . . . . . . . . . . . . . . . . . 66
7.6 Linear bandits via self-concordance . . . . . . . . . . . . . 68
7.7 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
8 Continuous exponential weights 71
8.1 Continuous exponential weights . . . . . . . . . . . . . . . 71
8.2 Convex analysis for continuous exponential weights . . . . 72
8.3 A one-dimensional surrogate . . . . . . . . . . . . . . . . 73
8.4 Exploration by optimisation . . . . . . . . . . . . . . . . . 77
8.5 Bayesian convex bandits . . . . . . . . . . . . . . . . . . . 79
8.6 Duality and the information ratio . . . . . . . . . . . . . . 81
8.7 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
9 Online Newton step 83
9.1 The blessing and curse of curvature . . . . . . . . . . . . . 83
9.2 Online Newton step . . . . . . . . . . . . . . . . . . . . . 84CONTENTS 3
9.3 Quadratic surrogate . . . . . . . . . . . . . . . . . . . . . 85
9.4 Algorithm and analysis . . . . . . . . . . . . . . . . . . . 86
9.5 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
10 Ellipsoid method 93
10.1 Ellipsoid method . . . . . . . . . . . . . . . . . . . . . . . 94
10.2 Separation with a surrogate . . . . . . . . . . . . . . . . . 97
10.3 Finding a critical point . . . . . . . . . . . . . . . . . . . 99
10.4 Estimating the cutting plane . . . . . . . . . . . . . . . . 102
10.5 Algorithm and analysis outline . . . . . . . . . . . . . . . 102
10.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
11 Gaussian optimistic smoothing 107
11.1 Elementary properties . . . . . . . . . . . . . . . . . . . . 108
11.2 Properties of the gradient . . . . . . . . . . . . . . . . . . 110
11.3 Properties of the Hessian . . . . . . . . . . . . . . . . . . 111
11.4 Properties of the quadratic surrogate . . . . . . . . . . . . 112
11.5 Lower bound . . . . . . . . . . . . . . . . . . . . . . . . . 113
11.6 Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 115
11.7 Concentration . . . . . . . . . . . . . . . . . . . . . . . . 117
11.8 Concentration continued . . . . . . . . . . . . . . . . . . 119
11.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . 133
11.10Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
12 Outlook 137
A Bibliography 139
B Miscellaneous 146
B.1 Identities . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
B.2 Technical inequalities . . . . . . . . . . . . . . . . . . . . 146
C Concentration 148
C.1 Orlicz norms . . . . . . . . . . . . . . . . . . . . . . . . . 148
C.2 Concentration . . . . . . . . . . . . . . . . . . . . . . . . 149CONTENTS 4
D Computation 153
D.1 Approximating minimum volume enclosing ellipsoids . . . . 154Chapter 1
Introduction and problem statement
Bandit problems are most commonly thought of in terms of sequential
decision making or as an elementary version of reinforcement learning.
This is certainly true, but they are also intimately connected with opti-
misation. These notes focus on the convex bandit problem, where the set
of actions is a convex subset of euclidean space and the function mapping
actions to losses is convex. The learner chooses actions and observes the
corresponding loss, possibly with additive noise. The duty of the learner
is to minimise the loss. When phrased like this the problem seems more
like zeroth-order optimisation and these notes largely take that view-
point. We do borrow two key notions from the bandit community that
give the algorithms and analysis a unique flavour:
➳ We focus on the cumulative regret as a performance criterion; and
➳ We (mostly) consider the adversarial setting, where the loss func-
tion is changing from one query to the next.
These differences mean the algorithms and theorems presented here are
often not directly comparable to standard settings in the optimisation
literature. Generally speaking, the theorems presented here hold with
fewer assumptions and consequentially are sometimes weaker.
1.1 Prerequisites
Most readers will benefit from a reasonable knowledge of online learn-
ing [Cesa-Bianchi and Lugosi, 2006, Hazan, 2016, Orabona, 2019] and
bandits [Bubeck and Cesa-Bianchi, 2012, Slivkins, 2019, Lattimore and
Szepesva´ri, 2020]. We use some theory from interior point methods,
5INTRODUCTION AND PROBLEM STATEMENT 6
whichyoucouldrefreshbyreadingthelecturenotesbyNemirovski[1996].
None of this is essential, however, if you are prepared to take a few re-
sultsonfaith. Similarly, weuseafewsimpleresultsfromconcentrationof
measure. Our reference was the book by Vershynin [2018] but Boucheron
et al. [2013] also covers the material needed. We do use martingale ver-
sions of these results, which sadly do not appear in these books but are
more-or-less trivial extensions. The symbol ( ) on a proof, section or
chapter means that you could (should?) skip this part on your first pass.
1.2 Bandit convex optimisation
Let K Rd be convex and f ,...,f : Rd R be an unknown
1 n
⊂ → ∪{∞}
sequence of convex functions with K dom(f ) = x : f (x) < . A
t t
⊂ { ∞}
learner interacts with the environment over n rounds. In round t the
learner is told dom(f ) and chooses an action X dom(f ). They then
t t t
∈
observe a noisy loss Y = f (X ) + ε , where (ε )n is a sequence of
t t t t t t=1
noise random variables. The precise conditions on the noise are given in
Eq. (1.1) below but for now you could think of the noise as a sequence of
independent standard Gaussian random variables. The learner’s decision
X is allowed to depend on an exogenous source of randomness and the
t
data observed already, which is dom(f ),X ,Y ,...,X ,Y ,dom(f ).
1 1 1 t−1 t−1 t
In most applications dom(f ) = K. Our main performance metric is the
t
regret, which is
n
Reg = max (f (X ) f (x)).
n t t t
x∈K −
t=1
(cid:88)
The regret is a random variable with the randomness coming from both
the noise and the learner’s decisions. We will normally bound the regret
inexpectationorhighprobability, dependingonwhatismostconvenient.
Of course the regret also depends on the loss functions. In general we
will try to argue that our algorithms have small regret for any convex
losses within some class. Stronger assumptions (smaller classes) lead to
stronger results and/or simpler and/or more efficient algorithms. The
following definition and notation is sufficient for our purposes.
Definition 1.1. Let F be the space of convex functions from Rd to
R with K dom(f) and define the following properties of a
∪ {∞} ⊂INTRODUCTION AND PROBLEM STATEMENT 7
function f F:
∈
Prop (b) f is bounded: f(x) [0,1] for all x dom(f).
∈ ∈
Prop (l) f is Lipschitz: f(x) f(y) x y for all x dom(f).
− ≤ ∥ − ∥ ∈
Prop (sm) f is β-smooth: f(x) β x 2 is concave on dom(f).
− 2 ∥ ∥
Prop (sc) f is α-strongly convex: f(x) α x 2 is convex on dom(f).
− 2 ∥ ∥
Prop (lin) f is linear on dom(f).
Prop (quad) f is quadratic on dom(f).
Prop (u) dom(f) x+Bd where x+Bd = y : x y ϱ .
⊃ x∈K ϱ ϱ { ∥ − ∥ ≤ }
Prop (e) There exists a convex function g : Rd R such that f(x) =
(cid:83) (cid:0) (cid:1)
→
g(x) on dom(f) and g is β-smooth and α-strongly convex on Rd.
Property(u)meansthatf isdefinedonsomeexpansionofK andthe
learnermayquerythefunctionoutsideofK butisneverthelesscompared
to the best point in K. Property (e) is only relevant when f is assumed
to be smooth and strongly convex on dom(f) and asserts the existence
of a smooth and strongly convex extension of f to all of Rd but the
learner is still only allowed to query f on dom(f). The existence of such
an extension is often technically convenient but surprisingly not always
possible [Drori, 2018]. Fortunately there are ways to circumvent this
problem, which we explain in Chapter 3. We use the property symbols
to define subsets of F. For example:
➳ F = f F : f(x) [0,1] for all x K .
b
{ ∈ ∈ ∈ }
➳ F is the set of bounded convex functions that are smooth and
b,sm,sc
strongly convex.
When smoothness and strong convexity are involved, our bounds will
depend on the parameters α > 0 and 0 < β < , which we assume are
∞
known constants. Similarly, for the unconstrained setting the bounds
will depend on ϱ. The name “unconstrained” is a misnomer, since the
learner is still restricted to play in dom(f). The setting may seem very
unusual but arises naturally when ϱ = and K is some subset in which
∞
the minimiser of the losses is known to lie.INTRODUCTION AND PROBLEM STATEMENT 8
Noise Our assumption on the noise is that the sequences (ε )n is
t t=1
conditionally subgaussian. By this we mean that
E[ε X ,Y ,...,X ,Y ,X ] = 0; and
t 1 1 t−1 t−1 t
|
E exp ε2 X ,Y ,...,X ,Y ,X 2. (1.1)
t | 1 1 t−1 t−1 t ≤
(cid:2) (cid:0) (cid:1) (cid:3)
This definition of subgaussianity is based on the Orlicz norm definitions.
We give a brief summary in Appendix C or you can read the wonderful
book by Vershynin [2018]. Sometimes we work in the noise free setting
where ε = 0 almost surely.
t
1.3 Settings
There are many variants of bandit convex optimisation. We already
outlinedsomeoftheassumptionsonthefunctionclasstowhichthelosses
belong. The other major classification is whether or not the problem is
adversarial or stochastic.
Adversarial bandit convex optimisation In the adversarial setting
the most common assumption is that the noise ε = 0 while the functions
t
f ,...,f arechoseninanarbitrarywaybytheadversary. Sometimesthe
1 n
adversary is allowed to choose f at the same time as the learner chooses
t
X , in which case we say the adversary is non-oblivious. Perhaps more
t
commonly, however, the adversary is obliged to choose all loss functions
f ,...,f beforetheinteractionstarts. Adversariesofthiskindarecalled
1 n
oblivious. For our purposes it is convenient to allow non-zero noise even
intheadversarialcaseforreasonsthatwillbecomeapparentinChapter3.
In any case, the presence of noise does not hurt any of the results in these
notes except to change a few constants.
Stochastic bandit convex optimisation The stochastic setting is
more classical. The loss function is now constant over time: f = f for
t
all rounds t and unknown f. The standard performance metric in bandit
problems is the regret, but in the stochastic setting it also makes sense
to consider the simple regret. At the end of the interaction the learnerINTRODUCTION AND PROBLEM STATEMENT 9
is expected to output one last point X K and the simple regret is
∈
sReg = f(X)(cid:98) minf(x).
n
− x∈K
Thankstoconvexitythereisastrai(cid:98)ghtforwardreductionfromcumulative
regret to simple regret. Simply let X = 1 n X . Then by convexity,
n t=1 t
1 (cid:80)
(cid:98)
sReg Reg . (1.2)
n ≤ n n
Another standard measure of performance in the stochastic setting is the
sample complexity, which is the number of interactions needed before the
simple regret is at most ε > 0 with high probability. Our focus for the
remainder is on the cumulative regret, but we occasionally highlight the
sample complexity of algorithms in order to compare to the literature.
The arguments above show that bounds on the cumulative regret imply
bounds on the simple regret and sample complexity. The converse is not
true.
Regret is random You should note that Reg and sReg are random
n n
variables with the randomness arising from both the algorithm and the
noise. Most of our results either control E[Reg ] or prove that Reg is
n n
bounded by such-and-such with high probability. Bounds that hold with
high probability are generally preferred since they can be integrated to
obtain bounds in expectations. But we will not be too dogmatic about
this. Indeed, we mostly prove bounds in expectation to avoid tedious
concentration of measure calculations. As far as we know, these always
work out if you try hard enough.
1.4 Notation
Norms Thenorm istheeuclideannormforvectorsandthespectral
∥·∥
norm for matrices. For positive definite A, x,y = x⊤Ay and x 2 =
⟨ ⟩A ∥ ∥A
x,x . Given a random variable X, X = inf t : E[exp( X k/t)]
⟨ ⟩A ∥ ∥ψ k { | | ≤
2 for k 1,2 are the Orlicz norms. Remember, X is subgaussian
} ∈ { }
if X < and subexponential if X < . You can read more
∥ ∥ψ2 ∞ ∥ ∥ψ1 ∞
about the Orlicz norms in Appendix C.INTRODUCTION AND PROBLEM STATEMENT 10
Sets The ball is Bd = x Rd : x r and sphere embedded in
r { ∈ ∥ ∥ ≤ }
d dimensions is Sd−1 = x Rd : x = r . Hopefully the latter is
r { ∈ ∥ ∥ }
not confused with the space of positive definite matrices on Rd, which
we denote by Sd. Given x Rd and positive definite matrix A with
+ ∈
eigenvalues (λ )d , we let E(x,A) = y Rd : x y 1 , which
k k=1 { ∈ ∥ − ∥A−1 ≤ }
is an ellipsoid centered at x with principle axes of lengths (λ1/2)d . The
k k=1
space of probability measures on K Rd is ∆ where we always take
K
⊂
the Borel σ-algebra. The space of measures on K is M(K), also with
the Borel σ-algebra, denoted by B(K). For x,y Rd we let [x,y] =
∈
(1 λ)x+λy : λ [0,1] .
{ − ∈ }
Basic Given a set A Rd and x Rd we let x+A = x+a : a A
⊂ ∈ { ∈ }
be the Minkowski sum of A and x . The boundary of A is ∂A = x
{ } { ∈
A : Bd A for all ε > 0 and the interior is int(A) = A ∂A. The
ε ̸⊂ } \
polar of A is A◦ = u : sup x,u 1 . We use 1 for the identity
x∈A
{ ⟨ ⟩ ≤ }
matrix and 0 for the zero matrix or zero vector. Dimensions and types
will always be self-evident from the context. The euclidean projection
onto K is Π (x) = argmin x y . The interior of a convex body
K y∈K
∥ − ∥
K is int(K) = x : x+Bd K for some ε > 0 . The sign function is
{ ε ⊂ }
1 if x < 0
−
sign(x) = 0 if x = 0

 1 if x > 0.

Suppose that f : Rd A Ris differentiable at x A, then we
⊃ → ∈
write f′(x) for its gradient and f′′(x) for its Hessian. When f is convex
we write ∂f(x) for the set of subderivatives of f at x. More generally,
Df(x)[h] is the directional derivative of f at x in the direction h. Higher-
orderdirectionalderivativesaredenotedbyDkf(x)[h ,...,h ]. Densities
1 k
are always with respect to the Lebesgue measure. The diameter and
condition number of a set K are
min R : K x+Bd,x Rd
diam(K) = max x y cond(K) = { ⊂ R ∈ } .
x,y∈K∥ − ∥ max r : x+Bd K,x Rd
{ r ⊂ ∈ }INTRODUCTION AND PROBLEM STATEMENT 11
Given a convex set K and continuous function f : K R for which
→
directional derivatives exist on int(K), we let
lip (f) = sup sup Df(x)[ν].
K
x∈int(K)ν∈Sd−1
1
Note that convex functions always have directional derivatives on the
interior of their domain.
Probability spaces We will not formally define the probability space
on which the essential random variables X ,Y ,...,X ,Y live. You can
1 1 n n
see how this should be done in the book by Lattimore and Szepesva´ri
[2020]. In general P is the probability measure on some space carrying
these random variables and we let F = σ(X ,Y ,...,X ,Y ) be the σ-
t 1 1 t t
algebra generated by the first t rounds of interaction. We abbreviate
P ( ) = P( F ) and E [ ] = E[ F ].
t t t t
· ·| · ·|
Regret Recall the regret is
n
Reg = sup (f (X ) f (x)).
n t t t
−
x∈K
t=1
(cid:88)
We occasionally need the regret relative to a specific x K, which is
∈
n
Reg (x) = (f (X ) f (x)).
n t t t
−
t=1
(cid:88)Chapter 2
Overview of methods and history
2.1 Methods for bandit convex optimisation
Methods for bandit convex optimisation can be characterised into five
classes:
➳ Cutting plane methods are important theoretical tools for linear
programming and non-smooth convex optimisation. The high-level
idea is to iteratively cut away pieces of K that have large vol-
ume while ensuring that the minimiser stays inside the active set.
At least two works have adapted these ideas to stochastic convex
bandits, both based on the ellipsoid method [Agarwal et al., 2011,
Lattimore and Gyo¨rgy, 2021a]. Cutting plane methods are the geo-
metricversionofeliminationalgorithmsforbanditsandconsequen-
tially are typically analysed in the stochastic setting. We discuss
a simple bisection algorithm in Chapter 4 for one-dimensional con-
vex bandits and the ellipsoid method in Chapter 10 for the general
case.
➳ Gradient descent is the fundamental algorithm for (convex) opti-
misation and a large proportion of algorithms for convex bandits
use it as a building block [Kleinberg, 2005, Flaxman et al., 2005,
Saha and Tewari, 2011, Hazan and Levy, 2014, and more]. At a
high level the idea is to estimate gradients of a smoothed version
of the loss and use these in gradient descent in place of the real
unknown gradients. We explore this idea in depth in Chapters 5
and 6.
➳ Newton’s method is a second-order method that uses curvature in-
formation as well as the gradient. One of the challenges in ban-
12OVERVIEW OF METHODS AND HISTORY 13
dit convex optimisation is that algorithms achieving optimal regret
need to behave in a way that depends on the curvature. Second-
order methods that estimate the Hessian of the actual loss or a
surrogate have been used for bandit convex optimisation by [Sug-
gala et al., 2021, Lattimore and Gy¨orgy, 2023] and are the topic of
Chapter 9.
➳ Continuous exponential weights is a powerful algorithm for full in-
formation online learning and has been used for convex bandits by
Bubeck et al. [2017], who combined it with the surrogate loss func-
tion described in Chapter 11 along with many tricks to construct
the first polynomial time algorithm for bandit convex optimisa-
tion in the adversarial setting with O(√n) regret without smooth-
ness/strong convexity. Their algorithm is more complex than you
would hope for and is not discussed here except for the special case
when d = 1 where many details simplify and the approach yields a
reasonably practical algorithm. More details are in Chapter 8.
➳ Information-directed sampling is a principled Bayesian algorithm
for sequential decision making [Russo and Van Roy, 2014]. [Bubeck
et al., 2015] showed how to use information-directed sampling to
bound the Bayesian regret for one-dimensional convex bandits and
then applied minimax duality to argue that the minimax Bayesian
regret is the same as the adversarial regret. This idea was later
extended by Bubeck and Eldan [2018] and Lattimore [2020]. Al-
though these methods still yield the best known bounds for the ad-
versarial setting, they are entirely non-constructive thanks to the
application of minimax duality. We explain how these ideas relate
to continuous exponential weights and mirror descent in Chapter 8,
but do not go into the gory details.
2.2 History
Banditconvexoptimisationisarelativenewcomer, withtheearliestwork
apparently by Kleinberg [2005] and Flaxman et al. [2005], both of whom
use gradient-based methods in combination with gradient estimates of
the smoothed losses (explained in Chapter 5). At least for losses in F
b,l
they showed that the regret is at most O(n3/4). Agarwal et al. [2010]
showed that by assuming strong convexity and smoothness the regret ofOVERVIEW OF METHODS AND HISTORY 14
these algorithms could be improved to O(√n) in the unconstrained case
where the learner is allowed to query outside K.
The big question was whether or not O(√n) regret is possible with-
out assuming smoothness and strong convexity. A resolution in the
stochastic setting was provided by Agarwal et al. [2013], who used the
ellipsoid method in combination with the pyramid construction of [Ne-
mirovsky and Yudin, 1983, Chapter 9], which is classically used for noise
free zeroth-order optimisation. They established O(√n) regret without
smoothness or strong convexity but with a high-degree polynomial de-
pendence on d. Because their algorithm was essentially an elimination
method, the idea did not generalise to the adversarial setting where the
minimiser may appear to be in one location for a long time before moving
elsewhere.
Meanwhile, back in the adversarial setting Hazan and Levy [2014]
assumed strong convexity and smoothness to prove that a version of fol-
low the regularised leader achieves O(√n) regret without the assumption
that the learner can play outside the constraint set, thus improving the
results of Agarwal et al. [2010]. The observation is that the increased
variance of certain estimators when the learner is playing close to the
boundary can be mitigated by additional regularisation at the boundary
using a self-concordant barrier (Chapter 6).
One fundamental question remained, which is whether or not O(√n)
regret was possible in the adversarial setting without strong convexity or
smoothness. The first breakthrough in this regard came when Bubeck
et al. [2015] proved that O(√n) regret is possible in the adversarial set-
ting with no assumptions beyond convexity and boundedness, but only
when d = 1. Strikingly, their analysis was entirely non-constructive
with the argument relying on a minimax duality argument to relate the
Bayesianregrettotheadversarialregretandinformation-theoreticmeans
to bound the Bayesian regret [Russo and Van Roy, 2014].
Bubeck and Eldan [2018] subsequently extended the information-
theoretictoolstod > 1showingforthefirsttimethatpoly(d)√nregretis
possible in the adversarial setting. Later, Lattimore [2020] refined these
argumentstoprovethattheminimaxregretforadversarialbanditconvex
optimisation with no assumptions beyond boundedness and convexity is
at most d2.5√n. This remains the best known result in the adversarial
setting with losses in F . One last chapter in the information-theoretic
bOVERVIEW OF METHODS AND HISTORY 15
story is a duality between the information-theoretic means and classi-
cal approaches based on mirror descent. Lattimore and Gy¨orgy [2021b]
have shown that any bound obtainable with the information-theoretic
machinery of Russo and Van Roy [2014] can also be obtained using mir-
ror descent. Their argument is still non-constructive since the mirror
descent algorithm needs to solve an infinite-dimensional convex optimi-
sation problem. Nevertheless, we believe this is a promising area for
further exploration as we discuss in Chapter 8.
Meanwhile, the search for an efficient algorithm with O(√n) regret
for the adversarial setting and losses in F continued An interesting step
b
in this direction was given by Hazan and Li [2016] who proposed an
algorithm with O(√n) regret but super-exponential dependence on the
dimension. Their algorithm had a running time of O(log(n)poly(d)).
Finally, Bubeck et al. [2017] constructed an algorithm based on con-
tinuousexponentialweightsforwhichtheregretintheadversarialsetting
with losses in F is bounded by O(d10.5√n). Furthermore, the algorithm
b
can be implemented in polynomial time. Although a theoretical break-
through, there are several serious limitations of this algorithm. For one,
the dimension-dependence is so large that in practically all normal situ-
ations one of the earliest algorithms would have better regret. Further-
more, although the algorithm can be implemented in polynomial time,
it relies on approximate log-concave sampling and approximate convex
optimisation in every round. Practically speaking the algorithm is near-
impossible to implement. The exception is when d = 1 where many
aspects of the algorithm simplify. We explain how this works in Chap-
ter 8.
The remaining challenge at this point was to improve the practicality
of the algorithms and reduce the dimension-dependence in the regret.
LattimoreandGy¨orgy[2021a]usedtheellipsoidmethodinthestochastic
settingincombinationwiththesurrogatelossintroducedbyBubecketal.
[2017] to show that O(d4.5√n) regret is possible in that setting with a
semi-practical algorithm. Recently Lattimore and Gy¨orgy [2023] showed
that O(d1.5√n) regret is possible in the unconstrained stochastic setting.
This last algorithm is detailed in Chapter 9.
Lower bounds You should be wondering about lower bounds. What
are the fundamental limitations in bandit convex optimisation? The sit-OVERVIEW OF METHODS AND HISTORY 16
uation is a bit miserable (the optimist says “hopeful”). The best known
lower bound when the losses are in F is that the minimax regret is at
b
least Ω(d√n) [Dani et al., 2008]. What is upsetting about this is that the
lower bound was established using linear losses where the upper bound is
also O(d√n). Can it really be that the hardest examples in the enormous
non-parametric class of bounded convex functions F lie in the tiny sub-
b
set of linear functions? Our intuition from the full information setting
says it could be like this. Curvature always helps in the full informa-
tion setting. We discuss in Chapter 9 why in bandit convex optimisation
curvature both helps and hinders in a complicated way.
Lower bounds for specific classes have also been investigated. In par-
ticular, Shamir [2013] showed that even when the losses are assumed to
be in Fs , the minimax regret is at least Ω(d√n). In the uncon-
b,sm,sc
strained setting this matches the upper bound of Agarwal et al. [2010]
up to logarithmic factors. Another fascinating result by Shamir [2013]
is that the simple regret bound obtained via the argument in Eq. (1.2)
can be far from optimal. Specifically, he demonstrated that for losses in
Fs :
b,sm,sc,quad
➳ The minimax regret is Θ(d√n); and
➳ The minimax simple regret is Θ(d2).
n
When considering losses in Fs , then
b,sm,sc
➳ The minimax regret is Θ(d√n); and
➳ The minimax simple regret is Θ(√d ).
n
So the cumulative regret is the same for Fs and the much larger
b,sm,sc,quad
classFs whiletheminimaxsimpleregretfortheseclassesisdifferent.
b,sm,sc
Many of the algorithms in these notes are based on combining gradi-
ent descent with noisy gradient estimates of some surrogate loss function.
Hu et al. [2016] explore the limitations of this argument. Their idea is to
modify the information available to the learner. Rather than observing
the loss directly, the learner observes a noisy gradient estimate from an
oracle that satisfies certain conditions on its bias and variance. This al-
lows the authors to prove a lower bound in terms of the bias and variance
of the oracle that holds for any algorithm. The main application is to ar-
gue that any analysis using the spherical smoothing estimates explained
in Chapter 5 either cannot achieve O(√n) regret or the analysis mustOVERVIEW OF METHODS AND HISTORY 17
use some more fine-grained properties of the specific estimator than its
bias and variance alone.
Stochastic optimisation and non-convex methods These notes
are about a specific kind of zeroth-order optimisation. We spend a mo-
ment now comparing to the other standard setup. In most works on
zeroth-order stochastic optimisation there is some unknown convex func-
tion f : K R to be minimised. The learner has oracle access to some
→
function F : K Ω R and a probability measure ρ on measurable
× →
space (Ω,G) such that
F(x,ξ)dρ(x) = f(x) x K.
∀ ∈
(cid:90)Ω
A query corresponds to evaluating F at some point x K and ξ sampled
∈
from ρ. The big difference is that the learner can query x F(x,ξ)
(cid:55)→
at multiple points with the same exogenous randomness ξ. Structural
assumptions are then made on F, f or both. For example, Nesterov and
Spokoiny [2017] assume that f is convex and x F(x,ξ) is Lipschitz
(cid:55)→
almost surely. Our stochastic setting can more-or-less be modelled in
this setting but where the only assumption on F is that for all x K
∈
the random variable F(x,ξ) has well-behaved moments.
Whether or not you want to make continuity/Lipschitz/smoothness
assumptions on F depends on how your problem is modelled. Here are
two real-world examples.
➳ You are crafting a new fizzy beverage and need to decide how much
sugar to add. A focus group has been arranged and with each
person you can give a few samples and obtain their scores. You
want to find the amount of sugar that maximises the expected
score over the entire population. This problem fits the stochastic
optimisation viewpoint because you can have multiple interactions
with each person in your focus group.
➳ You operate a postal service using donkeys to transport mail be-
tween Sheffield and Hathersage. Donkeys are stoic creatures and
do not give away how tired they are. Every day you decide how
much to load your donkey. Overload and they might have a nap
along the way but obviously you want to transport as much postOVERVIEW OF METHODS AND HISTORY 18
Donkey Post
Figure 2.1: How tired is this donkey?
as possible. The success of a journey is a function of how much
mail was delivered and how long it took. You’ll get a telegraph
with this information at the end of the day. This problem is best
modelled using the bandit framework because the tiredness of the
donkey varies from day to day unpredictably and you only get one
try per day.
Two-point evaluation model Consider for a moment the noise free
adversarial setting. A number of authors have investigated what changes
if the learner is allowed to choose two points X ,X K and observes
t,1 t,2
∈
f (X ) and f (X ). One might believe that such a modification would
t t,1 t t,2
have only a mild effect but this is not at all the case. Having access to
two evaluations makes the bandit setup behave more like the stochastic
optimisation setup just described [Agarwal et al., 2010, Nesterov and
Spokoiny, 2017, Duchi et al., 2015].
2.3 Summary table
The table below summarises the past and current situation. The su-
perscript in the function classes indicate whether or not the work only
considers the stochastic setting (s).
author regret class notes
1 1 3
Flaxmanetal.[2005] d2diam(K)2n4 F b,l practical
5
" dn6 F b practicalOVERVIEW OF METHODS AND HISTORY 19
1 1 3
Thesenotes d2ϑ4n4 F b practical
Agarwaletal.[2010] d(cid:112)βn/α F b,sm,sc,u practical
1 1 2 2 2
SahaandTewari[2011] ϑ3β3diam(K)3d3n3 F b,sm practical
Agarwaletal.[2013] √ n F ls practical,d=1
Agarwaletal.[2013] d16√ n F bs ,l polytime
HazanandLevy[2014] d(cid:112)(ϑ+β/α)n F b,sm,sc practical
√
Bubecketal.[2015] n F b exptime,d=1only
HazanandLi[2016] 2(d4)√ n F b log(n)poly(d)runningtime
Bubecketal.[2017]
d10.5√
n F b polytime
√
" n F b practical,d=1
Bubecketal.[2018]
d18√
n F b exptime
Lattimore[2020]
d2.5√
n F b exptime
Ito[2020] d(cid:112)βn/α F b,sm,sc min.awayfromboundary
" d1.5(cid:112)βn/α F b,sm,sc polytime
Suggalaetal.[2021]
d16√
n F b,quad polytime
LattimoreandGy¨orgy[2021a] d4.5√ n F bs ellipsoidmethod
Thesenotes d4√ n F bs ellipsoidmethod
LattimoreandGy¨orgy[2023] d1.5√ n+d mi ia nm (1(K ,ϱ))d√ n F ls ,u practical
2.4 Notes
(a) There are some books on zeroth-order optimisation [Larson et al.,
2019, Conn et al., 2009, for example]. These works focus most of their
attention on noise free settings and without a special focus on convexity.
Nemirovsky and Yudin [1983] is a more theoretically focussed book with
one chapter on zeroth-order methods. there is also a nice short and quite
recent survey by Liu et al. [2020].
(b) Speaking of non-convexity, zeroth-order methods are also analysed
in non-convex settings. Sometimes the objective is still to find the global
minimum, but for many non-convex problems this cannot be done effi-
ciently. In such cases one often tries to find a point x K such that
∈
f′(x) issmall. Weonlystudyconvexproblemshere. Arecentreference
∥ ∥
for the non-convex case is the work by Balasubramanian and Ghadimi
[2022].
(c) There are esoteric settings that are quite interesting and may suit
some applications. For example, Bach and Perchet [2016] study a prob-OVERVIEW OF METHODS AND HISTORY 20
lem where the learner chooses two actions in each round. The learner
receives information for only the first action but is evaluated based on
the quality of the second. They also study higher levels of smoothness
than we consider here.
(d) Online learning has for a long time made considerable effort to prove
adaptive bounds that yield stronger results when the loss functions are
somehow nice or show that the learner adapts to changing environments.
Such results have also been commonplace in the standard bandit liter-
ature and are starting to appear in the convex bandit literature as well
[Zhao et al., 2021, Luo et al., 2022, Wang, 2023]
(e) We did not talk much about the efforts focussed on sample com-
plexity or simple regret for the stochastic setting. Jamieson et al. [2012]
consider functions in F and K = Rd and prove a sample complex-
sm,sc
ity bound of O(d3) for an algorithm based on coordinate descent with
ε2
polynomial dependence on the smoothness and strong convexity param-
eters hidden. Belloni et al. [2015] use an algorithm based on simulated
annealing to prove a sample complexity bound of O(d7.5) for losses in
ε2
F . In its current form their algorithm is not suitable for regret minimi-
b
sation though this minor deficiency may be correctable. Another thing
to mention about that work is that the algorithm is robust in the sense
that it can (approximately) find minima of functions that are only ap-
proximately convex. Slightly earlier Liang et al. [2014] also use a method
based on random walks but obtained a worse rate of O(d14).
ε2Chapter 3
Regularity ( )
Bandit convex optimisation is studied on a wide range of structural as-
sumptions on both the loss functions (f ) and the constraint set K. The
t
purpose of this chapter is to explore the relationships and interactions
between the properties like smoothness, strong convexity and Lipschitz-
ness. Besides this we explain two meta algorithms that can be used to
bootstrap subsequent algorithms to yield guarantees with fewer assump-
tions. For example, if you design an algorithm losses in F and
b,l,sm,sc
prove a bound on its regret that depends only logarithmically on the di-
ameter, smoothness and strong convexity parameters, then you can use
Algorithm 1 to produce an algorithm with the same regret guarantee up
to logarithmic factors for losses in F .
b
3.1 Convex bodies and the Minkowski functional
A convex set K Rd is a convex body if it is compact and contains a
⊂
non-empty interior. The latter corresponds to the existence of an x Rd
∈
and ε > 0 such that x + Bd K. Given an x Rd the Minkowski
ε ⊂ ∈
functional is
y x
πK(y) = inf t > 0 : x+ − K .
x t ∈
(cid:26) (cid:27)
Given x K and ε (0,1), let
∈ ∈
K = εx+(1 ε)y : y K = y K : πK(y) 1 ε .
x,ε { − ∈ } { ∈ x ≤ − }
Lemma 3.1. Suppose that x+Bd K and y K . Then y+Bd K.
r ⊂ ∈ x,ε rε ⊂
21REGULARITY 22
Proof. Let y K . By the definition of the Minkowski functional there
x,ε
∈
exists a z K such that y = εx + (1 ε)z. Since K is convex and
∈ −
x+Bd K, it follows that
r ⊂
K ε(x+Bd)+(1 ε)z = y +Bd .
⊃ r − εr
z
K
K
x x, 11 0
y = 1 x+ 1 1 z
10 − 10
(cid:0) (cid:1)
Figure 3.1: The Minkowski functional and an illustration of K for
x,ε
ε = 1 . In this example, π (y) = 9 .
10 x 10
3.2 Smoothness and strong convexity
You should check that if f F and f is twice differentiable, then
sm,sc
∈
α1 f′′(x) β1 for all x int(K).
⪯ ⪯ ∈
Besides this, the only properties of smoothness and strong convexity that
we need are as follows:
Lemma 3.2. If f F is α-strongly convex, then
∈
α
f(x) f(y)+Df(y)[x y]+ x y 2 .
≥ − 2 ∥ − ∥
Lemma 3.3. If f F is β-smooth and X is a random variable supported
∈
in K and x = E[X]. Then,
β
E[f(X) f(x)] E X x 2 .
− ≤ 2 ∥ − ∥
(cid:2) (cid:3)REGULARITY 23
Proof. Letg(x) = f(x) β X 2, which byassumption isconcave. Then,
−2 ∥ ∥
β
E[f(X) f(x)] = E[g(X) g(x)]+ E X 2 x 2
− − 2 ∥ ∥ −∥ ∥
β E X 2 x 2 (cid:2) since g i(cid:3) s concave
≤ 2 ∥ ∥ −∥ ∥
= β E(cid:2) X x 2 . (cid:3) since E[X] = x
2 ∥ − ∥
(cid:2) (cid:3)
3.3 Scaling properties
A class of problems is defined by the constraint set K and the function
class in which the losses lie (see Definition 1.1) as well as constraints on
the adversary (stochastic/non-stochastic) or the noise.. Regardless, we
hope you agree that simply changing the scale of the coordinates should
not affect the achievable regret. The following proposition describes how
the various constants change when the coordinates are scaled.
Proposition 3.4. Let f : K [0,1] be convex and twice differentiable.
→
Define g(y) = f(x/γ) and J = γx : x K . The following hold:
{ ∈ }
(a) g : J [0,1] is convex and twice differentiable.
→
(b) g′(y) = f′(x/γ).
γ
(c) g′′(y) = f′′(x/γ).
γ2
(d) diam(J) = γdiam(K).
From this we see that the product of the Lipschitz constant and di-
ameter is invariant under scaling. As is the ratio of strong convexity and
smoothness parameters. You should always check that various results are
compatible with these scaling results in the sense that the regret bound
should be invariant to scale if the assumptions permit scaling.
3.4 Convex functions are nearly Lipschitz
Let f : K [0,1] be a convex function. Obviously f does not have to be
→
Lipschitz with any constant. For example, K = [0,1] and f(x) = 1 √x
−REGULARITY 24
has gradients that explode as x 0. But f must be Lipschitz on the
→
interior of K in some sense. You should start by checking that if A is
convex and f has directional derivatives and Df(x)[η] L for all x A
≤ ∈
and η Sd−1, then for all x,y A, f(x) f(y) L x y .
1
∈ ∈ − ≤ ∥ − ∥
Proposition 3.5. Suppose that f F and x+Bd K, then
∈ b r ⊂
1
max Df(x)[η] .
η∈Sd−1 ≤ r
1
Proof. Theassumptionthatf isconvexandboundedin[0,1]onK shows
that for any η Sd−1,
1
∈
1 f(x+rη) f(x)+Df(x)[rη] Df(y)[rη] = rDf(y)[η].
≥ ≥ ≥
Therefore Df(y)[η] 1.
≤ r
3.5 Near-optimality on the interior
The observation that convex functions are Lipschitz on a suitable subset
of the interior of K suggests that if we want to restrict our attention to
Lipschitz functions, then we might pretend that the domain of f is not
K but rather a subset. This idea is only fruitful because bounded convex
functions are always nearly minimised somewhere on the interior in the
following sense. Recall the definition of K from Section 3.1.
x,ε
Proposition 3.6. Let K be a convex body and x int(K) and ε (0,1).
∈ ∈
Then
min f(y) inf f(y)+ε.
y∈Kx,ε ≤ y∈K
Proof. To begin, the minimum on the left-hand side exists because K
x,ε
is a closed subset of the interior of the compact K and hence K is a
x,ε
compact subset of int(K). Furthermore, convex functions are continuous
ontheinterioroftheirdomain,whichmeansthat n f iscontinuouson
t=1 t
K and hence has a minimiser. Let y K. Then z = (1 ε)y+ε K
x,ε x,ε
∈ (cid:80) − ∈
and by convexity, f(z) (1 ε)f(y) + εf(x) f(y) + ε. Taking the
≤ − ≤
infimum over all y K completes the proof.
∈REGULARITY 25
3.6 Extension
Occasionally we need to extend a convex function f : Rd R
with dom(f) ⊊ Rd to a convex function f¯ such that dom(f→¯ ) = R∪ d{ a∞ nd}
¯
f = f on dom(f). Sometimes this is not possible. For example, the
function defined by
1 √x if x 0
f(x) = − ≥
(cid:40) if x < 0
∞
cannot be extended to a convex function with domain R. Fortunately
when f is Lipschitz on its domain then an extension to Rd is always
possible.
Proposition 3.7. Suppose that f : Rd R is Lipschitz on
dom(f). Then there exists a Lipschitz conv→ ex fun∪ ct{ i∞ on} f¯ : Rd R with
¯ →
f(x) = f(x) for all x dom(f).
∈
¯
Proof. Simply let f(y) = sup f(x)+Df(x)[y x]. Convexity
x∈int(dom(f))
¯ −
follows because f is the supremum of convex functions. Lipschitzness is
left as a simple exercise.
Occasionally it would be convenient to be able to extend β-smooth
functions while preserving β-smoothness to all of Rd. Remarkably this is
not possible [Drori, 2018].
3.7 Smoothing
Let ϕ : Rd R be the twice differentiable function given by
→
1 (x)
ϕ(x) = Bd 1 1 x 2 3 C = 1 x 2 3 dx.
C −∥ ∥ −∥ ∥
Bd
(cid:90) 1
(cid:0) (cid:1) (cid:0) (cid:1)
Note that ϕ is the density of a probability measure on Rd. Given ε > 0,
let
ϕ (x) = ε−dϕ(x/ε), (3.1)
ε
which by a change of measure is also a probability density.REGULARITY 26
Proposition 3.8. Suppose that f : Rd R is convex and in F
l
→ ∪{∞}
and let g = f ϕ , which is defined on dom(g) = x Rd : x + Bd
∗ ε { ∈ ε ⊂
cl(dom(f)) . Then the following hold:
}
(a) g is twice differentiable on int(dom(g)).
(b) g F .
l
∈
(c) g is smooth: g′′(x) (d+1)(d+6) for all x J.
∥ ∥ ≤ ε ∈
(d) max f(x) g(x) ε.
x∈dom(g)
| − | ≤
Proof. Part (a) follows from Young’s convolutional inequality and the
fact that ϕ is twice differentiable and compactly supported (this is a
ε
good exercise). Part (b) is left as an exercise (convolution preserves the
Lipschitz property). For part (c), the constant C can be calculated by
integrating in polar coordinates:
C = 1 x 2 3 dx
−∥ ∥
Bd
(cid:90) 1
(cid:0) 1 (cid:1)
= dvol(Bd) rd−1 1 r2 3 dr
1 −
(cid:90)0
48vol(Bd) (cid:0) (cid:1)
= 1 .
(d+2)(d+4)(d+6)REGULARITY 27
By convexity of the spectral norm and naive calculation:
g′′(x) = f(x+u)ϕ′′(x)dx
∥ ∥ ε
(cid:13)(cid:90)Bd
ε (cid:13)
(cid:13) (cid:13)
= (cid:13) (f(x+u) f(x))(cid:13) ϕ′′(x)dx ϕ′′(x)dx = 0
(cid:13) (cid:13)(cid:90)Bd
ε
− (cid:13) ε
(cid:13)
Bd ε ε
(cid:13) (cid:13) (cid:82)
(cid:13)ε ϕ′′(x) dx (cid:13) f is Lipschitz
≤ (cid:13) ∥ ε ∥ (cid:13)
Bd
(cid:90) ε
1
= ϕ′′(x) dx
ε ∥ ∥
Bd
(cid:90) 1
1
= 24xx⊤(1 x 2) 61(1 x 2)2 dx
Cε −∥ ∥ − −∥ ∥
Bd
(cid:90) 1
1 (cid:13) (cid:13)
(cid:13) 24xx⊤(1 x 2)+61(1 x 2)2(cid:13) dx
≤ Cε −∥ ∥ −∥ ∥
Bd
(cid:90) 1
(cid:13) (cid:13)
(d+1)(d+6)
(cid:13) (cid:13)
= ,
ε
where the final equality follows by substituting the expression for C and
integrating in polar coordinates. For part (d), since f is Lipschitz,
g(x) f(x) = (f(x+u) f(x))ϕ (du)
ε
| − | −
(cid:12)(cid:90)Bd
ε (cid:12)
(cid:12) (cid:12)
(cid:12) u ϕ (du) ε,. (cid:12)
(cid:12) ε (cid:12)
≤ ∥ ∥ ≤
Bd
(cid:90) ε
3.8 Meta algorithms
We now describe a process which by algorithm designed for losses in
F can be used when the losses (f )n are in fact in F . The idea
b,l,sm,sc t t=1 b
is to initialise the learner on a different constraint set L and place an
interface between the learner and the environment so that
➳ From the learner’s perspective they are interacting with a sequence
of losses (g )n : L R in F .
t t=1 → b,l,sm,sc
➳ Minimising the losses (g )n on L corresponds to approximately
t t=1
minimising the losses (f )n on K.
t t=1
The next proposition collects all the necessary machinery.REGULARITY 28
Proposition 3.9. Let K be a convex body and f F . Suppose that
b
∈
x+Bd K for some r > 0 and let Ty = 1+ε(y x) and J = T(K) and
r ⊂ rε −
g(u) = (1 ε) f¯ T−1(u+v) ϕ (v)dv +εq(u),
ε
−
Bd
(cid:90) ε
(cid:0) (cid:1)
¯
where f is the extension of f defined in the proof of Proposition 3.7 and
q(u) = 1 u 2/diam(J)2.
2 ∥ ∥
(a) g is Lipschitz on J.
(b) g is β-smooth on Rd with β = (d+1)(d+6).
ε
(c) g is α-strongly on Rd convex with α = ε/diam(J)2.
(d) g(u) f(T−1u) 3ε for all u J.
− ≤ 2 ∈
(e) With L = x J : πJ(x) 1 ε , min g(u) min f(y)+5ε.
{ ∈ 0 ≤ − } u∈L ≤ y∈K 2
Proof. Part (a) follows from Proposition 3.5. Part (b) follows from
Proposition 3.8. Part (c) is immediate. Part (d) follows from Proposi-
tion 3.8 and the definition of g. Part (e) follows from Proposition 3.6,
which shows that
minf(y) min f(y)+ε,
y∈K ≤ y∈Kx,ε
and the fact that L = T(K ) and part (d).
x,ε
Proposition 3.10. Suppose that L is defined as in Proposition 3.9 and
(f )n is a sequence of losses in F and
t t=1 b
g (u) = (1 ε) f T−1(u+v) ϕ (v)dv +εq(u).
t ε
−
Bd
(cid:90) ε
(cid:0) (cid:1)
Then, for any sequence (u )n L and (v )n Bd,
t t=1 ∈ t t=1 ∈ ε
n n
max f (T−1(u +v )) f (x) max (g (u ) g (u))+5εn.
t t t t t t t
x∈K − ≤ u∈L −
t=1 t=1
(cid:88)(cid:0) (cid:1) (cid:88)
Proof. Let x = T−1(u +v ) and y = T−1u . Since u +Bd T(K),
t t t t t t 1+ε ⊂REGULARITY 29
x +Bd K and by Proposition 3.5,
t (1+ε)/γ ⊂
γ ε
f (x ) f (y ) x y .
t t t t t t
| − | ≤ 1+ε ∥ − ∥ ≤ 1+ε
Therefore, by Proposition 3.9(d),
5ε
f (T−1(u +v )) = f (x ) f (y )+ε g (u )+ .
t t t t t t t t t
≤ ≤ 2
The claim follows from Proposition 3.9(e).
We can now give the meta algorithm that accepts as input a convex
bandit optimisation algorithm designed for losses in F and uses it
b,l,sm,sc
to solve bandit problems with losses in F .
b
1 args: base algorithm B , constraint set K
2 find x K and 0 < r R such that x+Bd K x+Bd
∈ ≤ r ⊂ ⊂ R
3 let ε = 1 and γ = 1+ε and T(y) = γ(y x)
n rε −
4 let L = (1 ε)T(K) and initialise B on L
−
5 let q(x) =
∥x∥2
2R2
6 for t = 1 to n
7 request XL L from B
t ∈
8 sample V from ϕ defined in Eq. (3.1)
t ε
9 let XK = T−1(XL +V ) K
t t t ∈
10 observe YK = f (XK)+ε
t t t t
11 feed YL = (1 ε)YK +εq(XL) to B
t − t t
Algorithm 1: Meta algorithm
From the learner’s perspective they are observing losses in F
b,l,sm,sc
with smoothness, strong convexity and diameter parameters given by
rε2 (d+1)(d+6) (1+ε)diam(K)
α = β = diam(L) =
(1+ε)R2 ε εr
The classical application of this transformation is for learners B withREGULARITY 30
regret for losses in F of
b,l,sm,sc
E[Reg ] = poly(d,n)polylog(n,d,diam(K),α,β).
n
By using the meta algorithm A in Algorithm 1 with base algorithm B
one obtains using Proposition 3.10 a regret for losses in F of
b
E[Reg ] poly(d,n)polylog(n,d,diam(K)/r,R),
n
≤
where the polynomial dependence on d and n is the same. The same
statement holds for high probability bounds. An example where this
is used is the algorithm introduced in Chapter 10. The scaling of the
domain to introduce Lipschitzness is only effective when the regret of
the base algorithm depends just logarithmically on the diameter. In
Chapter 9 we will see an algorithm with regret analysis for losses in
F with a high probability bound on the regret of
b,l,sm,sc
P(Reg poly(d,n,diam(K))polylog(n,d,α,β,1/δ)) δ.
n
≥ ≤
Such results can be bootstrapped to losses in F by smoothing and
b,l
adding a small quadratic but skipping the scaling.
1 args: base algorithm B , constraint set K
2 find x K and r > 0 such that x+Bd K x+Bd
∈ r ⊂ ⊂ R
3 let ε = 1 and δ = 1 min(1,r) and let L = εx+(1 ε)K
n n −
4 initialise B on L
5 let q(y) =
∥y−x∥2
2R2
6 for t = 1 to n
7 request XL L from B
t ∈
8 sample V from ϕ defined in Eq. (3.1)
t δ
9 let XK = XL +V K
t t t ∈
10 observe YK = f (XK)+ε
t t t t
11 feed YL = (1 ε)YK +εq(XL) to B
t − t t
Algorithm 2: Meta algorithmREGULARITY 31
3.9 Notes
(a) Versions of some or all the properties used here have been exploited
in a similar fashion by Flaxman et al. [2005], Bubeck et al. [2017], Lat-
timore [2020] and others.
(b) Algorithms 1 and 2 need to find an x K and 0 < r R such that
∈ ≤
x + Bd K x + Bd. Such values exist by definition for any convex
r ⊂ ⊂ R
body but for the algorithms to be computationally efficient we need a
procedure for finding them. This depends on how K is represented.
Briefly, if K is a polytope or given by separation oracle, then finding
r and R that are within poly(d) factors of the optimal is practical and
relatively straightforward. We explain in detail in Appendix D.
(c) You should also note that if K is given by a separation oracle, then
the L defined in Algorithms 1 and 2 is nothing more than an affine
image of K and hence the separation oracle for K can be used to defined
a separation oracle for L.Chapter 4
Bisection in one dimension
We start with a simple but instructive algorithm for the one-dimensional
stochastic setting. Throughout this chapter we assume that:
(a) d = 1 and K is a non-empty interval; and
(b) The losses are stochastic: f = f for all t; and
t
(c) The losses are Lipschitz: f F .
l
∈
4.1 Bisection method without noise
The bisection method for deterministic zeroth-order convex optimisation
is very simple.
1 let K = K
1
2 for k = 1 to :
∞
3 let x = minK and y = maxK
k k
4 let x = 1x+ 2y, x = 2x+ 1y
0 3 3 1 3 3
5 if f(x ) f(x ): then K = [x,x ]
1 0 k+1 1
≥
6 else : K = [x ,y]
k+1 0
Algorithm 3: Bisection method without noise
Theorem 4.1. Let x = argmin f(x). Then the following holds for
⋆ x∈K
the sequence of intervals (K )∞ produced by Algorithm 3:
k k=1
(a) x K ; and
⋆ k
∈
32BISECTION IN ONE DIMENSION 33
(b) vol(K ) 2 k−1 vol(K).
k ≤ 3
Part (a) foll(cid:0)ow(cid:1)s from convexity of f while part (b) follows from the
definition of the algorithm. By the assumption that f is Lipschitz, for
all x K , f(x) f(x )+(2)k−1vol(K). Therefore all points in K are
∈ k ≤ ⋆ 3 k
near-optimal when k is only moderately large.
4.2 Bisection method with noise
The generalisation of the bisection method
to noisy optimisation is surprisingly subtle.
While Algorithm 3 divides the current inter- f
val into three blocks, in the noisy setting it g
turns out that four blocks are necessary. The h
situation is best illustrated by an example.
Suppose you have noisy (and therefore only
approximate) estimates of the loss at all of
x 0,1,2,3 . Notice how all three convex
∈ { }
functions f,g and h have very similar values
0 1 2 3
at these points but the minimiser could be
in any of (0,1), (1,2) or (2,3). Hence it will Figure 4.1
take many samples to identify which function
is the truth. Even worse, if the real function is f, then you are paying
considerable regret while trying to identify the region where the min-
imiser lies. The example illustrates the problem of exploring efficiently.
A good exploration strategy will ensure that if the regret is large, then
the information gain about the identity/location of a near-minimiser is
also large. The exploration strategy in Figure 4.1 is not good. The ex-
ample also illustrates the challenges of generalising methods designed for
deterministic zeroth-order optimisation to stochastic zeroth-order opti-
misation. Fundamentally the problem is one of stability. Algorithm 3 is
not a stable algorithm in the sense that small perturbation of its obser-
vations can dramatically change its behaviour.
We decompose the bisection method for stochastic convex optimi-
sation into two algorithms. The first accepts as input an interval and
interacts with the loss for a number of rounds. Eventually it outputs a
new interval such that with high probability all of the following hold:
➳ The minimiser of the loss is contained in the output interval.BISECTION IN ONE DIMENSION 34
➳ The new interval is three quarters as large as the input interval.
➳ The regret suffered during the interaction is controlled.
1 args: K = [x,y], δ (0,1)
∈
2 x = 1x+ 3y, x = 1x+ 1y, x = 3x+ 1y
0 4 4 1 2 2 2 4 4
3 for t = 1 to n:
4 c = 6 log n
t t δ
5 let (cid:113)X
t
= x(cid:0)tm(cid:1)od3 and observe Y
t
= f(X t)+ε
t
6 if t 0 mod 3:
≡
7 let fˆ (x ) = 3 t 1(u k mod 3)Y
t k t u=1 ≡ u
ˆ ˆ
8 if f (x ) f (x ) c : return [x,x ]
t 2 t (cid:80)1 t 2
− ≥
ˆ ˆ
9 if f (x ) f (x ) c : return [x ,y]
t 0 t 1 t 0
− ≥
Algorithm 4: Bisection episode
Proposition 4.2. Let [z,w] be the interval returned by the algorithm and
1
∆ = [f(x )+f(x )+f(x )] f(x ).
0 1 2 ⋆
3 −
With probability at least 1 δ the following both hold:
−
(a) The algorithm either does not return an interval or x [z,w]; and
⋆
∈
(b) The number of queries to the zeroth-order oracle is at most
100 n
3+ log .
∆2 δ
(cid:16) (cid:17)
Proof. By convexity, max(f(x ),f(x )) f(x ). Assume without loss
0 2 1
≥
of generality for the remainder of the proof that f(x ) f(x ) and let
2 1
≥
θ = f(x ) f(x ). By concentration of measure (Theorem C.6) and a
2 1
−BISECTION IN ONE DIMENSION 35
union bound, P(G) 1 δ, where G = G G with
12 32
≥ − ∪
n
ˆ ˆ
G = f (x ) f (x ) f(x )+f(x ) c
12 t 1 t 2 1 2 t
− − ≤
t (cid:91)=1(cid:110)(cid:12) (cid:12) (cid:111)
n (cid:12) (cid:12)
(cid:12)ˆ ˆ (cid:12)
G = f (x ) f (x ) f(x )+f(x ) c .
32 t 3 t 2 3 2 t
− − ≤
t (cid:91)=1(cid:110)(cid:12) (cid:12) (cid:111)
(cid:12) (cid:12)
(cid:12) (cid:12)
Suppose now that G holds. We claim that θ 1∆. Viewing x ,x ,x
≥ 2 0 1 2
and f(x ), f(x ), f(x ) as fixed, the question is how small can f(x ) be?
0 1 2 ⋆
There are two cases.
(1) [f(x ) f(x )]. Here the smallest possible f(x ) is when x = x
0 1 ⋆ ⋆
≤
and by convexity:
➳ f(x ) f(x ) 3θ; and f
2 ⋆
− ≤
➳ f(x ) f(x ) 2θ; and
1 ⋆
− ≤
➳ f(x ) f(x ) θ.
0 ⋆
− ≤
x x x x y
0 1 2
(2) [f(x ) > f(x )]: Here the smallest value of f(x ) does not exist but
0 1 ⋆
the limiting case is when x x from the right and
⋆ 0
→
➳ f(x ) f(x ) 2θ; and
2 ⋆
− ≤
➳ f(x ) f(x ) θ; and
1 ⋆
− ≤ f
➳ f(x ) f(x ) 2θ.
0 ⋆
− ≤
x x x x y
0 1 2
In both cases the result follows from the definition of ∆. We are now
in a position to establish the claims of the theorem. For part (a). By
assumption f(x ) f(x ) and hence x cannot be in [x ,y]. The algo-
2 1 ⋆ 2
≥
rithm cannot do any wrong if x [x ,x ]. Suppose that x [x,x ].
⋆ 0 2 ⋆ 0
∈ ∈
By convexity f(x ) f(x ) and hence on G,
0 1
≤
ˆ ˆ
f (x ) f (x ) < f(x ) f(x )+c c ,
t 0 t 1 0 1 t t
− − ≤
which means the algorithm does not return [x,x ]. For part (b), suppose
2BISECTION IN ONE DIMENSION 36
that c 1θ. Then, on event G,
t ≤ 2
ˆ ˆ
f (x ) f (x ) f(x ) f(x ) c = θ c c ,
t 2 t 1 2 1 t t t
− ≥ − − − ≥
which means the algorithm halts. Since θ 1∆, it follows that on G the
≥ 2
algorithm halts once c 1∆. The claim follows from the definition of
t ≤ 4
c .
t
1 args: K = [x,y]
2 let K = [x,y]
1
3 for k = 1 to
∞
4 run Algorithm 4 with input K
k
5 receive K as the output
k+1
Algorithm 5: Bisection method
Theorem 4.3. With probability at least 1 nδ, the regret of Algorithm 5
−
is bounded by
n log(n)
Reg 13vol(K)+10 nlog 1+ .
n ≤ (cid:115) δ log(4/3)
(cid:18) (cid:19)
(cid:16) (cid:17)
Proof. Let K = [x ,y ] and
k k k
1 1 3 1 1 3 1
∆ = f x + y +f x + y +f x + y f(x ).
k k k k k k k ⋆
3 4 4 2 2 4 4 −
(cid:20) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)(cid:21)
Since f is Lipschitz, provided that x K it holds that ∆ y x
⋆ k k k k
∈ ≤ − ≤
(3)k−1vol(K). Since every call to Algorithm 4 makes at least three
4
queries to f, there very naively cannot be more than n calls to Algo-
rithm 4. Hence, by a union bound and Proposition 4.2, with probability
at least 1 nδ, every call made to Algorithm 4 either ends with the bud-
−
get of queries being exhausted or returns a new interval containing theBISECTION IN ONE DIMENSION 37
optimum after at most n queries with
k
100 n
n 3+ log .
k ≤ ∆2 δ
k
(cid:16) (cid:17)
Assume this good event occurs and note that ∆ vol(K) for k k =
k ≤ n ≥ ⋆
1+ log(n) . Therefore, with probability at least 1 nδ,
log(4/3) −
n
Reg = (f(X ) f(x ))
n t ⋆
−
t=1
(cid:88)
∞
vol(K)+ 1(k < k )n ∆
⋆ k k
≤
k=1
(cid:88)
∞ 3 k−1 ∞ n
vol(K)+3 vol(K)+10 1(k < k ) n log
⋆ k
≤ 4 δ
(cid:88)k=1(cid:18) (cid:19) (cid:88)k=1 (cid:114) (cid:16) (cid:17)
n log(n)
13vol(K)+10 nlog 1+ .
≤ (cid:115) δ log(4/3)
(cid:18) (cid:19)
(cid:16) (cid:17)
4.3 Notes
(a) Algorithm 4 is due to Agarwal et al. [2011]. The basic principle
behind the bisection method is that the volume of K is guaranteed to
k
decrease rapidly with the number of iterations. Generalising this method
to higher dimensions is rather non-trivial. Agarwal et al. [2011] and Lat-
timore and Gy¨orgy [2021a] both used algorithms based on the ellipsoid
method, which we cover in Chapter 10.
(b) Algorithm 4 works with no assumptions on f beyond convexity and
Lipschitzness and ensures O(√nlog(n)) regret in the stochastic setting.
The algorithm is distinct from all others in these notes because its regret
depends only very weakly on the range of the loss function. This is
what one should expect from algorithms in the stochastic setting where
the magnitude of the noise rather than the losses should determine the
regret, as it does for finite-armed bandits.
(c) There are various ways to refine Algorithm 3 for the deterministic
casethatbetterexploitconvexity[OrseauandHutter,2023]. Theseideas
havenotyetbeenexploitedinthenoisy(bandit)setting. Bisection-basedBISECTION IN ONE DIMENSION 38
methods seem fast and require just O(log(1/ε)) queries to the zeroth-
order oracle to find an ε-optimal point. Remarkably, for suitably well-
behaved functions Newton’s method is exponentially faster with sample
complexity O(loglog(1/ε)).
(d) Cheshire et al. [2020] construct a more sophisticated algorithm for
which the expected simple regret is
loglog(n)
E[sReg ] = O ,
n n
(cid:32)(cid:114) (cid:33)
which they show is optimal. By applying Eq. (1.2) with Algorithm 5 and
δ = 1/n2 one obtains a worse bound of
log(n)
E[sReg ] = O .
n √n
(cid:18) (cid:19)
Exactly what the logarithmic dependence should be for the expected
regret (cumulative rather than simple) seems to be unknown.Chapter 5
Gradient descent
Throughout this chapter we assume that
(a) Bd K; and
1 ⊂
(b) The loss functions (f )n are in F ; and
t t=1 b,l
(c) There is no noise: ε = 0 almost surely.
t
That is, the constraint set contains the euclidean ball, the losses are
bounded and Lipschitz and there is no noise. In contrast to the previous
chapter, the setting is now adversarial.
5.1 Gradient descent
Gradient descent incrementally computes a sequence of iterates (x )n
t t=1
withx computedbytakingagradientstepfromx . Theeuclideanpro-
t+1 t
jection onto K is Π (x) = argmin x y . The abstract algorithm
K y∈K
∥ − ∥
is given below.
Importantly, the algorithm does not evaluate the loss function at x
t
but rather at some random point X and the distribution of this point
t
has not been specified yet. We have rather informally written that X
t
should be based on x , by which we mean that
t
P(X A F ) = ν(A,x )
t t−1 t
∈ |
for some probability kernel ν : B(K) K [0,1]. The kernel ν deter-
× →
mines how the algorithm explores. The gradient estimate g is usually
t
not an estimate of f′(x), which may well not even exist. Instead it is an
t
estimate of the gradient of some surrogate loss function s that is close
t
39GRADIENT DESCENT 40
1 args: learning rate η > 0
2 initialise x K
1
∈
3 for t = 1 to n
4 sample X from some distribution based on x
t t
5 observe Y
t
6 compute gradient estimate g using x , X and Y
t t t t
7 update x = Π (x ηg )
t+1 K t t
−
Algorithm 6: Abstract gradient descent
to f in a certain sense. We return to the problem of defining the ex-
t
ploration kernel, surrogate and gradient estimates momentarily. Before
that we give some details about gradient descent. The analysis of gra-
dient descent at our disposal from the online learning literature yields a
bound on the regret relative to the linear losses defined by the gradient
estimates g . Specifically, we have the following theorem:
t
Theorem 5.1. For any x K,
∈
n diam(K)2 η n
Reg (x) ≜ g ,x x + g 2 .
n ⟨ t t − ⟩ ≤ 2η 2 ∥ t ∥
t=1 t=1
(cid:88) (cid:88)
(cid:100)
Proof. Let x K. Then,
∈
1 1
x x 2 = Π (x ηg ) x 2
t+1 K t t
2 ∥ − ∥ 2 ∥ − − ∥
1
x x ηg 2
t t
≤ 2 ∥ − − ∥
1 η2
= x x 2 + g 2 η g ,x x .
t t t t
2 ∥ − ∥ 2 ∥ ∥ − ⟨ − ⟩GRADIENT DESCENT 41
Rearranging shows that
n
Reg (x) = g ,x x
n t t
⟨ − ⟩
t=1
(cid:88)
n
(cid:100) η 1 1
g 2 + x x 2 x x 2
t t t+1
≤ 2 ∥ ∥ 2η ∥ − ∥ − 2η ∥ − ∥
t=1 (cid:20) (cid:21)
(cid:88)
n
1 η
x x 2 + g 2
1 t
≤ 2η ∥ − ∥ 2 ∥ ∥
t=1
(cid:88)
diam(K)2 η n
+ g 2 .
t
≤ 2η 2 ∥ ∥
t=1
(cid:88)
What conditions are needed on the gradients (g )n if we want to
t t=1
bound the actual regret in terms of Reg ? We have
n
n
(cid:100)
E[Reg ] = maxE f (X ) f (x)
n t t t
x∈K (cid:34) − (cid:35)
t=1
(cid:88)
n
= maxE E [f (X )] f (x)
t−1 t t t
x∈K (cid:34) − (cid:35)
t=1
(cid:88)
(†) n
≲ maxE E [g ],x x
t−1 t t
x∈K (cid:34) ⟨ − ⟩ (cid:35)
t=1
(cid:88)
n
= maxE g ,x x = maxE Reg (x)
t t n
x∈K (cid:34) ⟨ − ⟩ (cid:35) x∈K
(cid:88)t=1 (cid:104) (cid:105)
(cid:100)
The question is how can we ensure that ( ) holds? Since x is not known,
⋆
†
the most obvious way is to choose g in such a way that for all x K,
t
∈
E [f (X )] f (x) ≲ E [g ],x x .
t−1 t t t t−1 t t
− ⟨ − ⟩
Furthermore, to bound E[Reg ] we need to bound E [ g 2]. Sum-
n t−1 t
∥ ∥
marising, a surrogate gradient estimate g will yield a good regret bound
t
if: (cid:100)
(a) E [f (X )] f (x) ≲ E [g ],x x for all x K; and
t−1 t t t t−1 t t
− ⟨ − ⟩ ∈
(b) E [ g 2] is small.
t−1 t
∥ ∥GRADIENT DESCENT 42
Does such an estimator exist? Read on!
5.2 Spherical smoothing
Let x K and f F . Our algorithm will play some action X that is
b,l
∈ ∈
a random variable and observe Y = f(X). We want a gradient estimator
g that is a function of X and Y such that
(a) E[f(X)] f(y) ≲ E[g],x y for all y K; and
− ⟨ − ⟩ ∈
(b) E[ g 2] is small.
∥ ∥
A simple and beautiful estimator is based on Stoke’s theorem. Let r
∈
(0,1) be a precision parameter and define s as the convolution between
f and a uniform distribution on Bd. That is,
r
1
s(x) = f(x+u)du.
vol(Bd)
r (cid:90)Bd
r
Some examples are plotted in Figure 5.1. The function s is convex be-
cause it is the convolution of convex functions. We have to be careful
about the domain of s. Because f is only defined on K, the surrogate s
is only defined on
J = x K : x+Bd K .
{ ∈ r ⊂ }
You should check that J is convex and since r (0,1), J is non-empty
∈
by the assumption that Bd K. By Stoke’s theorem, the gradient of s
1 ⊂
at x is
1 d 1 u
s′(x) = f′(x+u)du = f(x+u) du,
vol(Bd) r vol(Sd−1) r
r (cid:90)Bd r (cid:20) r (cid:90)Sd r−1 (cid:21)
wherewealsousedthefactfromPropositionB.1thatvol(Sd−1)/vol(Bd) =
r r
d. We estimate s′(x) by sampling U uniformly from Sd−1, letting X =
r r
x+U and defining the surrogate gradient estimate by
dYU
g = ,
r2GRADIENT DESCENT 43
3
s(x),r =1 s(x),r =1
0.4
−
s(x),r =1/4 s(x),r =1/2
2 f(x)= |x | 0.6 f(x)= −log(x)
−
0.8
1 −
1.0
−
0
2 0 2 1.5 2.0 2.5 3.0
−
x x
Figure 5.1: The smoothed surrogates for different functions and pre-
cisions. Because of convexity the surrogate function is always an upper
bound on the original function. Notice how much better the approxima-
tion is for log(x), which on the interval considered is much smoother
−
than x .
| |
which has expectation E[g] = s′(x). How well does this estimator satisfy
our criteria? Well, s is convex so
E[g],x y = s′(x),x y s(x) s(y) E[f(X)] f(y) 2r,
⟨ − ⟩ ⟨ − ⟩ ≥ − ≥ − −
(5.1)
where the final inequality follows from the definition of s and the fact
that f is Lipschitz. This seems fairly promising. When r is small, then
(a) above is indeed satisfied. Slightly less promising is that
d2 d2 d2
E[ g 2] = E[Y2] = E[f(X)2] , (5.2)
∥ ∥ r2 r2 ≤ r2
where we used the fact that U = r and the assumption that f F is
b
∥ ∥ ∈
bounded on K. The situation is at a standoff. To satisfy (a) we need r
to be fairly small, but then E[ g 2] will not be that small. Nevertheless,
∥ ∥
enough has been done to make progress.GRADIENT DESCENT 44
5.3 Algorithm and regret analysis
The surrogate and its gradient estimator can be cleanly inserted into
stochastic gradient descent to obtain the following simple algorithm for
bandit convex optimisation.
1 args: learning rate η > 0 and precision r (0,1)
∈
2 initialise x J = x K : x+Bd K
1 ∈ { ∈ r ⊂ }
3 for t = 1 to n
4 sample U uniformly from Sd−1 and play X = x +U
t r t t t
5 compute gradient estimate g = dYtUt
t r2
6 update x = Π (x ηg ).
t+1 J t t
−
Algorithm 7: Bandit gradient descent
Theorem 5.2. Suppose that
η =
2−1 d−1 diam(K)3 n−3
r =
2−1 d1 diam(K)1 n−1
.
2 2 2 4 2 2 2 4
Then the expected regret of Algorithm 7 is bounded by
E[Reg ] √8diam(K)1 d1 n3
2 2 4
n
≤
Proof. The surrogate in round t is
1
s (x) = f (x+u)du.
t vol(Bd) t
r (cid:90)Bd
r
Since the losses are in F and hence Lipschitz,
b,l
n n
min f (x) rn+min f (x)
t t
x∈J ≤ x∈K
t=1 t=1
(cid:88) (cid:88)GRADIENT DESCENT 45
Therefore, letting x = argmin n f (x),
⋆ x∈J t=1 t
n (cid:80)
E[Reg ] = maxE (f (X ) f (x))
n t t t
x∈K (cid:34) − (cid:35)
t=1
(cid:88)
n
rn+E (f (X ) f (x ))
t t t ⋆
≤ −
(cid:34) (cid:35)
t=1
(cid:88)
n
= 3rn+E g ,x x . By Eq. (5.1)
t t ⋆
⟨ − ⟩
(cid:34) (cid:35)
t=1
(cid:88)
By Theorem 5.2,
n diam(K)2 η n
E g ,x x + E g 2
t t ⋆ t
⟨ − ⟩ ≤ 2η 2 ∥ ∥
(cid:34) (cid:35) (cid:34) (cid:35)
t=1 t=1
(cid:88) (cid:88)
diam(K)2 ηnd2
+ . By Eq. (5.2)
≤ 2η 2r2
Combining shows that
diam(K)2 ηnd2
E[Reg ] + +3nr.
n ≤ 2η 2r2
The claim follows by substituting the constants (see Lemma B.3).
5.4 Notes
(a) Theorem 5.1 is due to Zinkevich [2003].
(b) Algorithm 7 essentially appears in the independent works by Flax-
man et al. [2005] and Kleinberg [2005]. The algorithm continues to work
without Lipschitzness but the regret increases to O(cond(K)1/2d1/2n5/6)
or O(dn5/6) as explained by Flaxman et al. [2005].
(c) As far as we know the spherical smoothing estimator was introduced
by Nemirovsky and Yudin [1983] who used it as a first-order oracle to
prove sample complexity bounds. Omitting dependence on everything
except accuracy, they proved a sample complexity bound of O(1/ε4).
Nemirovsky and Yudin [1983] also noticed that smoothness increases the
performance of the spherical estimator, which we explain in Chapter 6.GRADIENT DESCENT 46
(d) Wedidnotsaymuchaboutcomputation. Theonlycomplicatedpart
is computing the projections, the hardness of which depends on how K
is represented. Note the algorithm does not project onto K but rather
J = x K : x+Bd K . Practically speaking it is probably preferable
{ ∈ r ⊂ }
to project onto K = (1 r)x : x K . By assumption K contains the
r
{ − ∈ }
unit radius euclidean ball and therefore K J. But by Proposition 3.6,
r
⊂
K is large enough that min n f (x) nr + min n f (x).
r x∈Kr t=1 t ≤ x∈K t=1 t
As we discuss briefly in Appendix D, K inherits many nice computation
r
(cid:80) (cid:80)
properties from K and polynomial time euclidean projection is often
possible.
(e) Garber and Kretzu [2022] show there are alternative ways to keep
the iterates inside the constraint set. They assume that Bd K for some
δ ⊂
δ > 0 and design gradient-descent-based algorithms for which the regret
more-or-less matches Theorem 5.2 and that need either O(n) queries to
a linear optimisation oracle or O(n) queries to a separation oracle.Chapter 6
Self-concordant regularisation
Thealgorithmbasedongradientdescentinthepreviouschapterissimple
and computationally efficient. There are two limitations, however.
➳ We needed to assume the losses were Lipschitz and the regret de-
pended polynomially on the diameter.
➳ Exploiting smoothness and/or strong convexity is not straightfor-
ward due to boundary effects.
Both limitations will be removed using follow the regularised leader and
thebeautifulmachineryofself-concordantbarriers. Globallyinthischap-
ter we make the following assumptions:
(a) There is no noise: ε = 0 for all t.
t
(b) The losses are bounded: f F for all t.
t b
∈
6.1 Self-concordant barriers
A three times differentiable convex function R : K R is self-
→ ∪ {∞}
concordant if
(a) D3R(x)[h,h,h] 2(D2R(x)[h,h])3/2 for all h Rd; and
≤ ∈
(b) R is a barrier: R(x ) whenever x ∂K.
t t
→ ∞ →
It is called a ϑ-self-concordant barrier if additionally:
(c) DR(x)[h] ϑD2R(x)[h,h] where ϑ is a (hopefully small) positive
≤
real value.
(cid:112)
47SELF-CONCORDANT REGULARISATION 48
The local norm at x K associated with R is h = h and
∈ ∥ ∥x ∥ ∥R′′(x)
its dual is h = h . The Dikin ellipsoid of radius r at x is
∥ ∥x⋆ ∥ ∥R′′(x)−1
Ex = y : y x r .
r { ∥ − ∥x ≤ }
We collect the following facts about ϑ-self-concordant barriers:
Lemma 6.1. Suppose that R is self-concordant on K, then
(a) The Dikin ellipsoid is contained in K: Ex K for all x int(K).
1 ⊂ ∈
(b) For all x int(K) and y K,
∈ ∈
R(y) R(x)+ R′(x),y x +ρ( x y )
≥ ⟨ − ⟩ −∥ − ∥x
with ρ(s) = log(1 s) s.
− − −
(c) tr(R′′(x)−1) ddiam(K)2 for all x int(K).
≤ ∈
Suppose additionally that R is ϑ-self-concordant, then
(d) R(y) R(x) ϑlog 1 πK(y) for all x,y int(K).
≤ − − x ∈
Proof. ( ) Part (a) app(cid:0)ears as E(cid:1)quation (2.2) in the notes by Ne-
mirovski [1996]. Part (b) is Equation (2.4) in the same notes. Part (c)
follows from Part (a). To see why, let η Sd−1 and notice that y =
1
∈
x+R′′(x)−1/2η Ex K. Therefore η = x y diam(K).
∈ 1 ⊂ ∥ ∥R′′(x)−1 ∥ − ∥ ≤
The result follows because
d
tr(R′′(x)−1) = e 2
∥
k ∥R′′(x)−1
k=1
(cid:88)
with (e )d the standard basis vectors. Part (d) appears as Equation
k k=1
(3.7) in the notes by Nemirovski [1996].
Lemma 6.2. Suppose that Φ : K R is self-concordant and x =
→
argmin Φ(x) and Φ′(y) 1, then Φ(y) Φ(x) Φ′(y) 2 .
z∈K ∥ ∥y⋆ ≤ 2 − ≤ ∥ ∥y⋆
Proof. ( ) Abbreviate g = Φ′(y). Then,
Φ(x) Φ(y)+ g,x y +ρ( x y ) Lemma 6.1(b)
≥ ⟨ − ⟩ −∥ − ∥y
Φ(y) g x y +ρ( x y ) Cauchy-Schwarz
≥ −∥ ∥y⋆∥ − ∥y −∥ − ∥ySELF-CONCORDANT REGULARISATION 49
Figure 6.1: Dikinellpsoidsforapolytopeandtheballusingthebarriers
in Note (c).
Therefore,
Φ(y) Φ(x)+ g x y ρ( x y )
≤ ∥ ∥y⋆∥ − ∥y − −∥ − ∥y
Φ(x)+max r g ρ( r)
≤ r≥0 ∥ ∥y⋆ − −
(cid:104) (cid:105)
= Φ(x) log 1 g g
− −∥ ∥y⋆ −∥ ∥y⋆
Φ(x)+ g (cid:104)2 . (cid:105)
≤ ∥ ∥y⋆
whereinthefinalinequalityweusedtheelementaryandnaiveinequality:
log(1 t) t t2 for t 1.
− − − ≤ ≤ 2
6.2 Follow the regularised leader
Followtheregularisedleadercanbeviewedasageneralisationofgradient
descent, which for bandits has the following abstract form. Like gradient
descent, follow the regularised leader maintains a sequence of iterates
(x )n in K with x = argmin R(x).
t t=1 1 x∈K
As for gradient descent, to make this an algorithm we need to decide
on the conditional law of X and what to use for the gradient g . To get
t t
a handle on what is needed, we explain what is guaranteed on the regret
relative to the linear losses defined by g .
t
Theorem 6.3. Let x int(K) and suppose that η g 1/2 for all
∈ ∥ t ∥xt⋆ ≤SELF-CONCORDANT REGULARISATION 50
1 initialise x = argmin R(x)
1 x∈K
2 for t = 1 to n
3 compute x = argmin R(x)+ t−1 η g ,x
t x∈K u=1 ⟨ u ⟩
4 sample X based on x and observe Y
t t(cid:2) (cid:80) t (cid:3)
5 compute gradient estimate g using x , X and Y
t t t t
Algorithm 8: Follow the regularised leader
t. Then
n n
ϑ 1
Reg (x) = g ,x x log +η g 2 .
n ⟨ t t − ⟩ ≤ η 1 π (x) ∥ t ∥xt⋆
t=1 (cid:18) −
x1
(cid:19) t=1
(cid:88) (cid:88)
(cid:100)
The proof of Theoremo 6.3 is omitted because we give a proof of a
more general result later (Theorem 6.8).
6.3 Optimistic ellipsoidal smoothing
Let us momentarily drop the t indices and let x int(K) and f F .
b
∈ ∈
We will introduce a new kind of smoothing. Let Σ be positive definite
and E = y Rd : x y 1 , which is an ellipsoid centered at x.
{ ∈ ∥ −
∥Σ−1
≤ }
Let
1
s(y) = 2f 1z + 1y f (z) dz.
vol(E) 2 2 −
(cid:90)E
(cid:0) (cid:0) (cid:1) (cid:1)
The surrogate loss s behaves quite differently to the spherical smoothing
used in Chapter 5. Perhaps the most notable property is that s is op-
timistic in the sense that s(y) f(y) for all y K as we prove below.
≤ ∈
The second is that the surrogate is not a good uniform approximation of
the real loss, even when the precision r is very small. Some examples are
shown in Figure 6.2.SELF-CONCORDANT REGULARISATION 51
3
s(y),r =1,x= 3 s(y),r =1,x=1
10 8
s(y),r = 1,x= 3 s(y),r = 1 ,x=1
4 10 10
2 f(y)= y 6 f(y)= y
| | | |
4
1
2
0
0
2 0 2 2 0 2
− −
x x
Figure 6.2: The optimistic ellipsoid smoothing. Notice the poor global
approximation but good local approximation near x.
We want g to be an estimate of s′(x), which is
1
s′(x) = f′(1z + 1x)dz
vol(E) 2 2
(cid:90)E
1
= f′(x+ 1Σ1/2z)dz Change of variables
vol(Bd) 2
1 (cid:90)Bd
1
2Σ−1/2
= f(x+ 1Σ1/2ξ)ξdξ Stokes’ theorem
vol(Bd) 2
1 (cid:90)Sd 1−1
2dΣ−1/2
= f(x+ 1Σ1/2ξ)ξdξ Proposition B.1
vol(Sd 1−1)
(cid:90)S 1d−1
2
= 4dΣ−1E[f(X)(X x)]. X = x+ 1Σ1/2ξ
− 2
Please note we have cheated a little here by assuming that f is differen-
tiable and applying Stokes’ theorem. Fortunately the equality still holds
even without differentiability, which is a good exercise. Therefore an
unbiased estimator of s′(x) is
g = 4dΣ−1Y(X x).
−
The above considerations yield the following lemma:
Lemma 6.4. E[g] = s′(x).SELF-CONCORDANT REGULARISATION 52
The next lemma explores the properties of s.
Lemma 6.5. The following hold:
(a) s is convex; and
(b) s(y) f(y) for all y K.
≤ ∈
(c) If r (0,1) and Σ = r2R′′(x)−1, then E[f(X) s(x)] r .
∈ − ≤ 1−r
(d) If f is β-smooth, then E[f(X) s(x)] βtr(Σ).
− ≤ d
(e) If f is α-strongly convex, then s is α-strongly convex.
2
Proof. Part (a) follows immediately from convexity of f, noting that the
second (negated) term in the definition of s is constant as a function of
y. Part (b) follows from convexity of f as well:
1 1
s(y) = 2f 1z + 1y f (z) dz f(z) = f(z).
vol(E) 2 2 − ≥ vol(E)
(cid:90)E (cid:90)E
(cid:0) (cid:0) (cid:1) (cid:1)
For part (b), let R u h (u) = f(x + uΣ1/2ξ). By definition Σ =
ξ
∋ (cid:55)→
r2R′′(x)−1 so that for u [ 1/r,1/r], x+uΣ1/2ξ Ex K. Therefore h
∈ − ∈ 1 ⊂
is defined on [ 1/r,1/r]. Hence, by Proposition 3.5 and the assumption
−
that f F it follows that h (u) h (v) r|u−v| for all u,v [ 1,1].
∈ b ξ − ξ ≤ 1−r ∈ −
Therefore, letting v be uniformly distribution on Bd,
1
E[f(X) s(x)] = E[h (1/2)+h ( v ) 2h ( v /2)]
ξ ξ ξ
− ∥ ∥ − ∥ ∥
r 1 v v
E ∥ ∥ + v ∥ ∥
≤ 1 r 2 − 2 ∥ ∥− 2
− (cid:20)(cid:12) (cid:12) (cid:12) (cid:12)(cid:21)
r (cid:12) (cid:12) (cid:12) (cid:12)
. (cid:12) (cid:12) (cid:12) (cid:12)
≤ 1 r (cid:12) (cid:12) (cid:12) (cid:12)
−
For part (d),
E[f(X) s(x)] = E[h (1/2)+h ( v ) 2h ( v /2)]
ξ ξ ξ
− ∥ ∥ − ∥ ∥
2E[h (1) h (0)]
ξ ξ
≤ −
= 2E f(x+Σ1/2ξ) f(x)
−
βE (cid:2) Σ1/2ξ 2 (cid:3)
≤
βtr(cid:104)((cid:13)Σ)
(cid:13)
(cid:105)
= (cid:13) . (cid:13)
dSELF-CONCORDANT REGULARISATION 53
where in the first inequality we used convexity of h and the second we
ξ
used Lemma 3.3. Part (e) is left as a straightforward exercise.
6.4 Algorithms and regret analysis
We start by studying an algorithm that relies on neither smoothness nor
strong convexity.
1 args: learning rate η > 0, r (0,1)
∈
2 for t = 1 to n
3 compute x = argmin t−1 η g ,x +R(x)
t x∈K u=1 ⟨ u ⟩
4 sample ξ uniformly from Sd−1
t (cid:80) 1
5 play X = x + rR′′(x )−1/2ξ and observe Y
t t 2 t t t
6 compute gradient g =
4dYtR′′(xt)(Xt−xt)
t r2
Algorithm 9: Follow the regularised leader with ellipsoidal smoothing
The machinery developed in Section 6.3 combined with Theorem 6.3
leads to a straightforward analysis of Algorithm 9.
Theorem 6.6. Suppose the losses are in F and there is no noise and
b
η =
2−1 (ϑlog(n))3 d−1 n−3
r = min 1,
21 d1 n−1 (ϑlog(n))1
.
2 4 2 4 2 2 4 4
(cid:16) (cid:17)
The expected regret of Algorithm 9 is upper bounded by
E[Reg ] 1+4√2(ϑlog(n))1 d1 n3
4 2 4
n
≤
Proof. By definition, X x = r 1 and therefore X Ext K
∥ t − t ∥xt 2 ≤ 2 t ∈ 1 ⊂
where the inclusion follows from Lemma 6.1(a). Hence, the algorithm
always plays inside K. Using the fact that the losses are in F it holds
b
automatically that Reg n and when r > 1 this already implies the
n ≤ 2
bound in the theorem. Suppose for the remainder that r 1. Similarly,
≤ 2
forthesamereasonwemaysupposefortheremainderthatn 4ϑlog(n).
≥
Let
K = x K : πK(x) 1 1/n
1/n { ∈ x1 ≤ − }SELF-CONCORDANT REGULARISATION 54
and x = argmin n f (x) with ties broken arbitrarily. Such a
⋆ x∈K 1/n t=1 t
point is guaranteed to exist by Proposition 3.6, which also shows that
(cid:80)
E[Reg ] 1+E[Reg (x )].
n n ⋆
≤
Before using Theorem 6.3 we need to confirm that η g 1.
∥ t ∥xt⋆ ≤ 2
4ηd Y 2ηd Y 2ηd 1
η g = | t | R′′(x )(X x ) = | t | ,
∥ t ∥xt⋆ r2 ∥ t t − t ∥xt⋆ r ≤ r ≤ 2
where in the final inequality we used the assumption that n 4ϑlog(n).
≥
Let Σ = r2R′′(x )−1 and E = y : x y 1 = Ext. The
t t t { ∥ t − ∥Σ− t1 ≤ } r
surrogate in round t is
1
s (x) = 2f (1y + 1x) f (y) dy.
t vol(E ) t 2 2 − t
t (cid:90)Et
(cid:0) (cid:1)
Hence, by Theorem 6.3 and the results in Section 6.3,
n
E[Reg ] 1+E f (X ) f (x )
n t t t ⋆
≤ −
(cid:34) (cid:35)
t=1
(cid:88)
n
nr
1+ +E s (x ) s (x ) Lemma 6.5(b)(c)
t t t ⋆
≤ 1 r −
(cid:34) (cid:35)
− t=1
(cid:88)
n
nr
1+ +E s′(x ),x x Lemma 6.5(a)
≤ 1 r ⟨ t t t − ⋆ ⟩
(cid:34) (cid:35)
− t=1
(cid:88)
n
nr
1+ +E g ,x x Lemma 6.4
t t ⋆
≤ 1 r ⟨ − ⟩
(cid:34) (cid:35)
− t=1
(cid:88)
n
nr ϑlog(n)
1+ + +E η g 2 Theorem 6.3
≤ 1 r η ∥ t ∥t⋆
(cid:34) (cid:35)
− t=1
(cid:88)
ϑlog(n) 4ηnd2
1+2nr+ + ,
≤ η r2
where the final inequality follows since r 1/2 and Y [0,1] and
t
≤ ∈
4dY R′′(x )(X x ) 2 16ηd2 4ηd2
η g 2 = η t t t − t X x = .
∥ t ∥t⋆ r2 ≤ r4 ∥ t − t ∥R′′(xt) r2
(cid:13) (cid:13)xt⋆
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)SELF-CONCORDANT REGULARISATION 55
The result follows by substituting the values of the constants.
Notice how the dependence on the diameter that appeared in The-
orem 5.2 has been replaced with a dependence on the self-concordance
parameter ϑ and logarithmic dependence on the horizon. This can be a
significant improvement. For example, when K is a ball, then the bound
in Theorem 5.2 depends linearly on diam(K) while with a suitable self-
concordant barrier the regret in Theorem 6.6 replaces this with log(n).
(cid:112)
Essentially what is happening is that Algorithm 9 moves faster deep in
(cid:112)
the interior where the losses are necessarily more Lipschitz while Algo-
rithm 7 does not adapt the amount of regularisation to the location of x .
t
For smooth functions the rate can be improved by using Lemma 6.5(d)
instead of Lemma 6.5(c).
Theorem 6.7. Suppose the losses are in F and there is no noise and
b,sm
r ϑlog(n)
r2 = min 4, 22 d2 (νlog(n))1 β−1 diam(K)−4 n−1 η = .
3 3 3 3 3 3
d 2n
(cid:114)
(cid:16) (cid:17)
Then the expected regret of Algorithm 9 is upper bounded by
5d
E[Reg ] 1+ 2ϑnlog(n)+3 22 (ϑβdiam(K)2log(n))1 d2 n2 .
3 3 3 3
n ≤ 4 ·
(cid:112)
Proof. Note the condition that r2 4 is needed to ensure that X K.
t
≤ ∈
RepeattheargumentintheproofofTheorem6.6butreplaceLemma6.5(c)
with Lemma 6.5(d), which yields
ϑlog(n) 2ηnd2 β n
E[Reg ] 1+ + + tr(Σ )
n ≤ η r2 2d t
t=1
(cid:88)
ϑlog(n) 2ηnd2 βnr2diam(K)2
1+ + +
≤ η r2 2
where in the second inequality we used Lemma 6.1(c) and the definition
Σ = r2R′′(x )−1. The result follows by substituting the constants.
t t
The diameter now appears in the bound, as it must. Otherwise you
could scale the coordinates and make the regret vanish (Section 3.3).
There is no hope to remove the √n term from Theorem 6.7, since even
with β = 0 the lower bound for linear bandits says the regret should be
at least Ω(d√n).SELF-CONCORDANT REGULARISATION 56
6.5 Smoothness and strong convexity
We conclude the main body of this chapter by showing that a version of
follow the regularised leader can achieve O(√n) regret for smooth and
strongly convex loss functions. The main modification of the algorithm
is that the linear surrogate loss functions are replaced by quadratics.
Before that though we need to generalise Theorem 6.3.
Theorem 6.8. Suppose that (fˆ )n is a sequence of self-concordant func-
t t=1
tions from K to R and let
t−1
ˆ
x = argminR(x)+η f (x) and = .
t
x∈K
u ∥·∥t⋆ ∥·∥Φ′ t′(xt)−1
u=1
(cid:88)
Φt−1(x)
Then, provided that η fˆ′(x ) 1 for all t, for any x int(K),
∥ t t ∥t⋆ ≤ 2 ∈
n n
ϑ
Reg (x) = fˆ (x ) fˆ (x) log 1 πK(x) +η fˆ′(x ) 2 ,
n t t − t ≤ −η − x1 ∥ t t ∥t⋆
(cid:88)t=1 (cid:16) (cid:17)
(cid:0) (cid:1)
(cid:88)t=1
(cid:100)
ˆ
Theorem 6.3 is recovered by choosing f (x) = g ,x .
t t
⟨ ⟩SELF-CONCORDANT REGULARISATION 57
Proof. By the definition of Φ ,
t
n
ˆ ˆ
Reg (x) = f (x ) f (x)
n t t t
−
(cid:88)t=1 (cid:16) (cid:17)
n
(cid:100)1 Φ (x) R(x)
n
= (Φ (x ) Φ (x )) +
t t t−1 t
η − − η η
t=1
(cid:88)
n
1 Φ (x ) Φ (x) R(x) R(x )
n n+1 n 1
= (Φ (x ) Φ (x ))+ + −
t t t t+1
η − η − η η
t=1
(cid:88)
n
1 R(x) R(x )
1
(Φ (x ) Φ (x ))+ − Φ (x ) Φ (x)
t t t t+1 n n+1 n
≤ η − η ≤
t=1
(cid:88)
n
R(x) R(x )
η fˆ′(x ) 2 + − 1 Lemma 6.2
≤ ∥ t t ∥t⋆ η
t=1
(cid:88)
n
ϑ
log 1 πK(x) +η fˆ (x ) 2 . Lemma 6.1(d)
≤ −η − x1 ∥ t t ∥t⋆
t=1
(cid:0) (cid:1) (cid:88)
Note that in the application of Lemma 6.2 we used the fact that
fˆ′(x
) =
t t
Φ′(x ) and the assumption that η fˆ′(x ) 1.
t t ∥ t t ∥t⋆ ≤ 2
1 args: learning rate η > 0
2 for t = 1 to n
3 let x = argmin R(x)+η t−1 g ,x + α x x 2
t x∈K u=1 ⟨ u ⟩ 4 ∥ − u ∥
4 let Σ−1 = R′′(x )+ ηαt1
t t (cid:2)4 (cid:80) (cid:0) (cid:1)(cid:3)
5 sample ξ uniformly from Sd−1
t 1
6 play X = x + 1Σ1/2ξ and observe Y
t t 2 t t t
7 compute gradient g = 4dY Σ−1(X x )
t t t t t
−
Algorithm 10: Follow the regularised leader with ellipsoidal smoothing
Theorem 6.9. Suppose the losses are in F and there is no noise
b,sm,scSELF-CONCORDANT REGULARISATION 58
and
2 ϑ+ β [1+log(n)]
η = α .
d(cid:115) n
(cid:0) (cid:1)
Then the expected regret of Algorithm 10 is upper bounded by
4β
E[Reg ] 1+4d n ϑ+ [1+log(n)].
n ≤ (cid:115) α
(cid:18) (cid:19)
Proof. The same argument in the proof of Theorem 6.6 shows that X is
t
in the Dikin ellipsoid associated with R at x and therefore is in K. Let
t
K = x K : πK(x) 1 1/n
1/n { ∈ x1 ≤ − }
andx = argmin n f (x),whichbyProposition3.6andLemma6.5(b)(d)
⋆ x∈K 1/n t=1 t
means that
(cid:80)
E[Reg ] 1+E[Reg (x )]
n n ⋆
≤
n
= 1+E (f (X ) f (x ))
t t t ⋆
−
(cid:34) (cid:35)
t=1
(cid:88)
n
β
1+E s (x ) s (x )+ tr(Σ ) .
t t t ⋆ t
≤ − d
(cid:34) (cid:35)
t=1 (cid:18) (cid:19)
(cid:88)
Next, let fˆ (x) = g ,x x + α x x 2. By Lemma6.5(e), s is α-
t ⟨ t − t ⟩ 4 ∥ − t ∥ t 2
strongly convex and therefore
n n
α
E (s (x ) s (x )) E E [g ],x x x x 2
t t t ⋆ t−1 t t ⋆ t ⋆
− ≤ ⟨ − ⟩− 4 ∥ − ∥
(cid:34) (cid:35) (cid:34) (cid:35)
(cid:88)t=1 (cid:88)t=1 (cid:16) (cid:17)
n
= E (fˆ (x ) fˆ (x ))
t t t ⋆
−
(cid:34) (cid:35)
t=1
(cid:88)
n
ϑlog(n)
+ηE g 2 Theorem 6.8
≤ η ∥ t ∥Σt
(cid:34) (cid:35)
t=1
(cid:88)
ϑlog(n)
+4ηnd2.
≤ ηSELF-CONCORDANT REGULARISATION 59
Using the definition of Σ ,
t
n n
β 4β 1 4β
tr(Σ ) (1+log(n)).
t
d ≤ αη t ≤ αη
t=1 t=1
(cid:88) (cid:88)
Combining everything shows that
1 4β
E[Reg ] 1+4ηnd2 + ϑlog(n)+ (1+log(n)) .
n ≤ η α
(cid:18) (cid:19)
The result follows by substituting the definition of η.
6.6 Notes
(a) At no point in this chapter did we need Lipschitz losses. The anal-
ysis essentially exploits the fact that convex functions cannot have large
gradients except very close to the boundary where the regularisation pro-
videdbytheself-concordantbarrierpreventstheblowupinvariancefrom
severely impacting the regret.
(b) We have made several improvements to the statistical efficiency rel-
ative to the algorithm presented in Chapter 5. In exchange the algo-
rithms are more complicated and computationally less efficient. Algo-
rithms based on gradient descent run in O(d) time per round except
those rounds where a projection is needed. Furthermore, even when the
projection is needed it is with respect to the euclidean norm and likely to
be extremely fast. Meanwhile the algorithms in this chapter need a sin-
gular value decomposition to compute X , solve an optimisation problem
t
to find x and need oracle access to a ϑ-self-concordant barrier.
t
(c) The reader interested in knowing more about (ϑ-)self-concordant
barriers is referred to the wonderful notes by Nemirovski [1996]. The
most obvious question is whether or not these things even exist. We give
some examples:
➳ When K = x : a ,x b ,1 i k is a polytope defined
i i
{ ⟨ ⟩ ≤ ≤ ≤ }
by k half-spaces, then R(x) = k log(b a ,x ) is called the
− i=1 i −⟨ i ⟩
logarithmic barrier and is k-self-concordant.
(cid:80)
➳ When K = x : x ρ is a ball, then R(x) = log(ρ2 x 2) is
{ ∥ ∥ ≤ } − −∥ ∥
a 1-self-concordant barrier on K.SELF-CONCORDANT REGULARISATION 60
➳ For any convex body K there exists a ϑ-self-concordant barrier with
ϑ d. Specifically, the entropic barrier satisfies this [Chewi, 2023,
≤
Bubeck and Eldan, 2014].
(d) As far as we know Algorithm 9 has not been analysed previously.
Algorithm 10 and Theorem 6.9 is due to Hazan and Levy [2014] while
Theorem 6.7 is by Saha and Tewari [2011].
(e) Thesurrogatelossonlyappearsintheanalysis. Interestingly, Hazan
and Levy [2014] and Saha and Tewari [2011] analysed their algorithms
using the surrogate
1
s (y) = f(y +u)du,
t vol(E rxt)
(cid:90)Erxt−xt
which is the ellipsoidal analogue of the surrogate used in Chapter 5.
Except for a constant factor this surrogate has the same gradient at x
t
as the surrogate we used, which means the resulting algorithms are the
same. The difficulty is that the surrogate defined above is not defined on
all of K, which forces various contortions or assumptions in the analysis.
(f) Note that Theorem 6.6 and 6.7 bound the regret for the same al-
gorithm with different learning rates and smoothing parameters. You
should wonder if it is possible to obtain the best of both bounds with a
single algorithm by adaptively tuning the learning rates. At present this
is not known as far as we know.Chapter 7
Linear and quadratic bandits
Function classes like F are non-parametric. In this chapter we shift
b
gears a little by studying two important parametric classes: F and
b,lin
F . The main purpose of this chapter is to explain the differences
b,quad
betweenconvexbanditsandlinearbanditsandtoproveaminimaxbound
on the regret for quadratic bandits. The latter is not established for
a polynomial time algorithm but serves as a target in future chapters.
Before the algorithms and regret analysis we need two tools.
7.1 Covering numbers
Given A,B Rd, the covering number
⊂
N(A,B) = min : Rd,A (x+B)
|C| C ⊂ ⊂
(cid:40) (cid:41)
x∈C
(cid:91)
is the smallest number of translates of B needed to cover A.
Proposition 7.1 (Artstein-Avidan et al. 2015, Corollary 4.1.15). Sup-
pose that A is centrally symmetric, compact and convex. Then, for any
ε (0,1),
∈
2 d
N(A,εA) 1+ .
≤ ε
(cid:18) (cid:19)
Proposition 7.2. Suppose that A is compact and centrally symmetric.
61LINEAR AND QUADRATIC BANDITS 62
Then, for any ε (0,1),
∈
4 d
N(A,εconv(A)) 1+ .
≤ ε
(cid:18) (cid:19)
Proof. Let B = conv(A) and take a cover so that B (x+ εB)
C ⊂ x∈C 2
and then project points in onto A using , which is the norm such
C ∥·∥B (cid:83)
that B = x Rd : x 1 .
{ ∈ ∥ ∥B ≤ }
7.2 Optimal design
An optimal design for a compact set A Rd is a probability measure
⊂
on Rd that minimises the worst case variance of a least-squares estima-
tor over A. The following theorem gives the definition and establishes
existence.
Theorem 7.3 (Kiefer and Wolfowitz 1960). For any compact A subset
Rd with span(A) = Rd there exists a probability measure π supported on
A such that
x 2 = d for all x A,
∥
∥G− π1
∈
where G = xx⊤dπ(x)
π A
Remarkab(cid:82)ly the constant d is the best achievable for any compact A
with span(A) = Rd in the sense that:
min max x 2 = d.
π∈∆A x∈A ∥
∥G− π1
7.3 Exponential weights
Let be a finite set and ℓ ,...,ℓ a sequence of functions from R.
1 n
C C →
Given a learning rate η define a distribution q on by
t
C
exp η t−1 ℓ (a)
q (a) = − u=1 u .
t exp η t−1 ℓ (b)
b∈C (cid:0) −(cid:80) u=1 u(cid:1)
(cid:80) (cid:0) (cid:80) (cid:1)LINEAR AND QUADRATIC BANDITS 63
Theorem 7.4. Suppose that η ℓ (a) 1 for all 1 t n, then
t
| | ≤ ≤ ≤
n n
log η
max q (a)ℓ (a) ℓ (b) |C| + q (a)ℓ (a)2.
t t t t t
b∈C (cid:32) − (cid:33) ≤ η 2
t=1 a∈C t=1 a∈C
(cid:88) (cid:88) (cid:88)(cid:88)
Proof. We use the following two inequalities:
(a) exp(x) 1+x+x2 for all x 1; and
≤ | | ≤
(b) log(1+x) x for all x > 0.
≤
Let b = argmin n ℓ (a) and D = log(1/q (b)). Then,
a∈C t=1 t t t
1(cid:80)
D = log
t+1
q (b)
t+1
(cid:18) (cid:19)
q (a)exp( ηℓ (a))
= log a∈C t − t
q (b)exp( ηℓ (b))
(cid:18)(cid:80) t − t (cid:19)
= D +log q (a)exp( ηℓ (a)) ηℓ (b)
t t t t
− −
(cid:32) (cid:33)
a∈C
(cid:88)
D +log q (a) 1 ηℓ (a)+η2ℓ (a)2 ηℓ (b) by (a)
t t t t t
≤ − −
(cid:32) (cid:33)
a∈C
(cid:88) (cid:2) (cid:3)
= D +log 1+ q (a) ηℓ (a)+η2ℓ (a)2 ηℓ (b)
t t t t t
− −
(cid:32) (cid:33)
a∈C
(cid:88) (cid:2) (cid:3)
D η [q (a)ℓ (a) ℓ (b)]+η2 q (a)ℓ (a)2. by (b)
t t t t t t
≤ − −
a∈C a∈C
(cid:88) (cid:88)
Rearranging and summing over t and telescoping yields
n n
1 q (b)
q (a)ℓ (a) ℓ (b) log n+1 +η q (a)ℓ (a)2
t t t t t
− ≤ η q (b)
(cid:32) (cid:33) 1
t=1 a∈C (cid:18) (cid:19) t=1 a∈C
(cid:88) (cid:88) (cid:88)(cid:88)
n
1
log( )+η q (a)ℓ (a)2.
t t
≤ η |C|
t=1 a∈C
(cid:88)(cid:88)
7.4 Linear bandits
For this section we make the following assumption:LINEAR AND QUADRATIC BANDITS 64
(a) (f )n are in F .
t t=1 b,lin
(b) K Rd is compact but not necessarily convex.
⊂
(c) span(K) = Rd.
We let (θ )n be a sequence in Rd such that f (x) = x,θ . The plan is
t t=1 t ⟨ t ⟩
to use exponential weights on a finite K that is sufficiently large that
C ⊂
the optimal action in K can be approximated by something in . Let
C
A = conv(K K), which is a symmetric convex set. By the assumption
−
that the losses are bounded,
θ Θ = θ Rd : θ 1 .
t
∈ ∈ ∥
∥A◦
≤
(cid:8) (cid:9)
What we need from is that for all θ Θ,
C ∈
minmax x y,θ ε.
x∈C y∈K ⟨ − ⟩ ≤
Suppose that x y ε, then x y,θ x y θ ε.
∥ − ∥A ≤ ⟨ − ⟩ ≤ ∥ − ∥A∥ ∥A◦ ≤
Hence, it suffices to choose K such that K (x + εA). By
C ⊂ ⊂ x∈C
Proposition 7.2, such a cover exists with
(cid:83)
4 d
1+ .
|C| ≤ ε
(cid:18) (cid:19)
The algorithm for linear bandits plays actions in and uses importance-
C
weighted least squares to estimate x,θ for all x simultaneously.
t
⟨ ⟩ ∈ C
The distribution proposed by exponential weights is mixed with a small
amount of an optimal design on , which is needed so that the estimates
C
are suitably bounded as required by Theorem 8.1.
Theorem 7.5. Suppose that
1 log
η = |C| γ = ηd.
2 nd
(cid:114)
Then the regret of Algorithm 11 is bounded by
E[Reg ] 1+4 ndlog .
n
≤ |C|
(cid:112)LINEAR AND QUADRATIC BANDITS 65
1 args: η > 0, γ > 0, K
2 find K such that min max x y 1
C ⊂ x∈C y∈K ∥ − ∥A ≤ n
3 find optimal design π on
C
4 for t = 1 to n
5 let q (x) =
exp(−η(cid:80)t u− =1 1⟨x,θˆ u⟩)
t (cid:80) y∈Cexp(−η(cid:80)t u− =1 1⟨y,θˆ u⟩)
6 let p = (1 γ)q +γπ
t t
−
7 sample X from p and observe Y = f (X )
t t t t t
8 let G = p (a)aa⊤ and θˆ = G−1X Y
t a∈C t t t t t
(cid:80)
Algorithm 11: Exponential weights for linear bandits
Proof. Recall the definition of G from Section 7.2 and let
π
G = q (x)xx⊤.
qt t
x∈C
(cid:88)
With this notation, G = (1 γ)G +γG . Since the exponential weights
t
−
qt π
distributionissupportedonallx K andspan(K) = Rd,G isinvertible.
t
∈
Next,
E [θˆ ] = p (x)G−1x x,θ = θ .
t−1 t t t ⟨ t ⟩ t
x∈C
(cid:88)
Furthermore,
E x,θˆ 2 = E Y2x⊤G−1X X⊤G−1x x 2 ,
t−1 ⟨ t ⟩ t−1 t t t t t ≤ ∥ ∥G− t1
(cid:2) (cid:3) (cid:2) (cid:3)
where we used the fact that Y2 1. Therefore,
t ≤
E q (x) x,θˆ 2 q (x) x 2
t−1
(cid:34)
t
⟨
t
⟩
(cid:35)
≤
t
∥
∥G− t1
x∈C x∈C
(cid:88) (cid:88)
1
q (x) x 2
≤ 1 γ
t
∥
∥G− qt1
− x∈C
(cid:88)
d
= .
1 γ
−LINEAR AND QUADRATIC BANDITS 66
We also need to show the loss estimates are suitably bounded:
ηd
η x,θˆ = η x⊤G−1X Y η x X = 1.
|⟨ t ⟩| t t t ≤ ∥ ∥G− t1 ∥ t ∥G− t1 ≤ γ
(cid:12) (cid:12)
Define x = argmin(cid:12) n x ,(cid:12) θ . Then, the regret is bounded by
⋆ x∈C t=1⟨ ⋆ t ⟩
(cid:80) n
E[Reg ] = 1+E X x ,θ
n t ⋆ t
⟨ − ⟩
(cid:34) (cid:35)
t=1
(cid:88)
n
= 1+E p (x) x x ,θ
t ⋆ t
⟨ − ⟩
(cid:34) (cid:35)
t=1 x∈C
(cid:88)(cid:88)
n
1+nγ +E q (x) x x ,θ
t ⋆ t
≤ ⟨ − ⟩
(cid:34) (cid:35)
t=1 x∈C
(cid:88)(cid:88)
n
= 1+nγ +E q (x)s x x ,θˆ
t ⋆ t
−
(cid:34) (cid:35)
(cid:88)t=1 (cid:88)x∈C (cid:68) (cid:69)
n
log
1+nγ + |C| +E q (x) x,θˆ 2
t t
≤ η ⟨ ⟩
(cid:34) (cid:35)
t=1 x∈C
(cid:88)(cid:88)
log ηnd
1+nγ + |C| + .
≤ η 1 γ
−
The result follows by substituting the constants.
7.5 Quadratic bandits
Quadraticbanditsseemmuchharderthanlinearbanditsbutifyouignore
the computation complexity then it turns out that quadratic bandits are
linear bandits. Define a function ϕ(x) : Rd Rd(d+3)/2 by
→
ϕ(x) = (x ,...,x ,x2,x x ,...,x x ,x2,x x ,...,x2 ,x x ,x2).
1 d 1 1 2 1 d 2 2 3 d−1 d−1 d d
Soϕisthefeaturemapassociatedwiththepolynomialkernelofdegree2.
You should check that any f F can be written as f(x) = ϕ(x),θ
b,quad
∈ ⟨ ⟩
for some θ Rd(d+3)/2.
∈
Lemma 7.6. Given K Rd, there exists a K such that:
⊂ C ⊂
(a) log dlog 3d(d+3) .
|C| ≤ ε
(cid:16) (cid:17)LINEAR AND QUADRATIC BANDITS 67
(b) max min ϕ(x) ϕ(y) ε.
x∈K y∈C ∥ − ∥conv(ϕ(K)−ϕ(K)) ≤
The proof is left as an exercise. We can now simply write the ker-
nelised version of Algorithm 11.
1 args: η > 0, γ > 0 and K
2 find K satisfying (a),(b) in Lemma 7.6, ε = 1
C ⊂ n
3 find optimal design π on ϕ(a) : a
{ ∈ C}
4 for t = 1 to n
5 let q t(x) =
(cid:80)
xe ∈x Cp e( x− pη (−(cid:80) ηt u− (cid:80)=1 1
t
u⟨
−
=ϕ
1
1( ⟨x ϕ), (θˆ xu )⟩ ,θˆ)
u
⟩)
6 let p = (1 γ)q +γπ
t t
−
7 sample X from p and observe Y = f (X )
t t t t t
8 let G = p (a)ϕ(a)ϕ(a)⊤ and θˆ = G−1X Y
t a∈C t t t t t
(cid:80)
Algorithm 12: Exponential weights for quadratic bandits
An immediate corollary of Theorem 7.5 is the following bound on the
regret of Algorithm 12
Theorem 7.7. With parameters
1 log
η = |C| γ = nd(d+1)/3
2 nd(d+1/3)
(cid:115)
The expected regret of Algorithm 12 is bounded by
n(d+1)d2log(3nd(d+1))
E[Reg ] 1+4 .
n ≤ 3
(cid:114)
Ignoring logs and constants, Theorem 7.7 shows that one can obtain
d1.5√n regret for quadratic bandits. Note that we did not use anywhere
that F only included convex quadratics. Everything works for more
b,quad
general quadratic losses. Even if we restrict our attention to convex
quadratics, none of the algorithms we have presented so far can match
thisbound. Actuallynoefficientalgorithmisknownmatchingthisbound
except for special K.LINEAR AND QUADRATIC BANDITS 68
7.6 Linear bandits via self-concordance
Before we return to convex bandits, we want to illustrate why gradient-
based methods can be near-optimal for linear bandits but are hard to
make work for convex bandits. We start by adapting the algorithm in
Chapter 6 to the linear setting. For this section we assume that:
(a) The losses are in F ; and
b,lin
(b) K is a convex body in Rd; and
(c) R is ϑ-self-concordant barrier on K.
The algorithm we study is actually identical to Algorithm 9 except
thatwesetr = 1, whichmeans thealgorithmsamples fromtheboundary
oftheDikinellipsoidratherthanascaled-downDikinellipsoid. Notethat
we could not have chosen r larger because then the algorithm might play
outside of the constraint set K.
1 initialise x = argmin R(x)
1 x∈K
2 for t = 1 to n
3 compute x = argmin t η g ,x +R(x)
t x∈K u=1 ⟨ u ⟩
4 sample X uniformly from y : y x = 1
t (cid:80)
{ ∥ −
t ∥R′′(xt)
}
5 compute gradient g = dR′′(x )Y (X x )
t t t t t
−
Algorithm 13: SCRiBLe
Theorem 7.8. Suppose that
1 ϑlog(n)
η = .
d 2n
(cid:114)
Then the expected regret of Algorithm 13 is upper bounded by
E[Reg ] 1+d 2ϑnlog(n).
n
≤
(cid:112)
Proof. LetK = x K : π (x) 1 1/n wherex = argmin R(x).
1/n
{ ∈
x1
≤ − }
1 x∈KLINEAR AND QUADRATIC BANDITS 69
Next, let x = argmin n f (x), which by Proposition 3.6,
⋆ x∈K t=1 t
(cid:80)n
E[Reg ] = maxE f (X ) f (x)
n t t t
x∈K (cid:34) − (cid:35)
t=1
(cid:88)
n
1+E f (X ) f (x )
t t t ⋆
≤ −
(cid:34) (cid:35)
t=1
(cid:88)
n
= 1+E x x ,θˆ
t ⋆ t
−
(cid:34) (cid:35)
(cid:88)t=1 (cid:68) (cid:69)
n
ϑlog(n)
1+ +2ηE θˆ 2 . Theorem 6.3
≤ η ∥ t ∥xt⋆
(cid:34) (cid:35)
t=1
(cid:88)
The dual norm term is bounded as follows:
E θˆ 2 = d2E Y2 X x 2
t−1 ∥ t ∥xt⋆ t−1 t ∥ t − t ∥R′′(xt)
(cid:104) (cid:105) d2E (cid:2) X x 2 (cid:3)
≤
t−1
∥
t
−
t ∥R′′(xt)
= d2.
(cid:2) (cid:3)
Therefore,
ϑlog(n)
E[Reg ] 1+ +2nηd2.
n ≤ η
The result follows by substituting the definition of η.
We should compare Theorem 7.5 and Theorem 7.8.
7.7 Notes
(a) Our analysis of linear bandits was entirely geared towards applying
the naive covering argument to quadratic bandits showing that for this
class the regret is at most d1.5√n.
(b) Even when K is convex, (non-convex) quadratic programming is
computationally hard. For example, when K is a simplex and A is the
adjacency matrix of an undirected graph G, then a theorem by MotzkinLINEAR AND QUADRATIC BANDITS 70
and Straus [1965] says that
1 1 1
min( x⊤Ax) = 1 ,
2 x∈∆ − 2 ω(G) −
d (cid:18) (cid:19)
where ω(G) is the size of the largest clique in G. Since the clique decision
problem is NP-complete [Karp, 1972], there (probably) does not exist an
efficient algorithm for minimising quadratic functions over the simplex.
(c) By the previous note, Algorithm 12 cannot be implemented effi-
ciently, since its analysis did not make use of convexity of the losses.
Sadly, even if we restrict our attention to convex quadratic loss functions
and convex K, the kernel method does not seem amenable to efficient
calculations. Why not? The problem is that even if K is convex, the set
J = ϕ(a) : a K need not be convex.
{ ∈ }
(d) The assumption that K spans Rd in Section 7.4 was for simplicity
only. In case this does not hold you can either reparameterise every-
thing via an isometry between K and Rdim(K) or replace inverses with
pseudoinverses in the algorithm and analysis.
(e) You can find much more about linear bandits in the book by Lat-
timore and Szepesva´ri [2020]. Provided that K is given in terms of a
separation oracle, there is a polynomial time algorithm for which the
expected regret is upper bounded by d√n.
(f) Algorithm 13 is due to Abernethy et al. [2008] while Algorithm 11
is by Bubeck et al. [2012].Chapter 8
Continuous exponential weights
Throughout this chapter we will assume that:
(a) The losses are in F .
b
(b) There is no noise so that Y = f (X ).
t t t
The assumption on the noise is for simplicity only. You could have a
pleasant afternoon checking that everything goes through with no serious
changes in the presence of subgaussian noise.
8.1 Continuous exponential weights
Unlikegradientandsecond-ordermethods,continuousexponentialweights
uses the entire loss function and acts in the space of probability measures
on K. This introduces an interesting complexity. Rather than estimat-
ing a gradient and (maybe) Hessian, the learner now has to estimate the
entire loss function from a single query.
As we have seen in other abstract algorithms, we will insist that the
distribution p is constructed as a function of q only. You should think
t t
of q as encoding the state of the algorithm. Bayesian’s may like to think
t
of q as some kind of posterior. Very often in applications of continuous
t
exponential weights p is chosen to be equal to q . Or sometimes some-
t t
thing very close. For example, in linear bandits one commonly takes
p = (1 γ)q + γp where p is an optimal design on K [Lattimore and
t t
−
Szepesva´ri, 2020, Chapters 21 and 27].
71CONTINUOUS EXPONENTIAL WEIGHTS 72
1 args: learning rate η > 0
2 for t = 1 to n
3 compute density q (x) =
1K(x)exp(−η(cid:80)t u− =1 1fˆ u(x))
t (cid:82) Kexp(−η(cid:80)t u− =1 1fˆ u(x))
4 find distribution p supported on K
t
5 sample X from p and observe Y
t t t
6 compute estimator fˆ : K R using X ,Y ,p ,q
t t t t t
→
Algorithm 14: Abstract continuous exponential weights
8.2 Convex analysis for continuous exponential weights
Before progressing further we need to introduce some simplifying no-
tation and a few tools from convex analysis. Given p ∆ and f :
K
∈
K R let p,f = p(x)f(x)dx. The negative entropy function is
→ ⟨ ⟩ K
R : M(K) R defined by
→ (cid:82)
R(p) = log(p(x))dx p(x)dx,
−
(cid:90)K (cid:90)K
which is a convex function on M(K). We let D be the Bregman diver-
gence with respect to R, which is given by
D(p,q) = R(p) R(q) DR(p)[p q].
− − −
Youshouldcheckthatforp,q M(K),D(p,q) = p(x)log(p(x)/q(x))dx
∈ K
is the relative entropy. Remember that the Fenchel dual of R is
(cid:82)
R⋆(f) = sup p,f R(p).
p∈M(K)⟨ ⟩−
Lastly, the Bregman divergence with respect to R⋆ is denoted by
D⋆(f,g) = R⋆(f) R⋆(g) DR⋆(g)[f g].
− − −CONTINUOUS EXPONENTIAL WEIGHTS 73
Recalling the notation from Algorithm 14 and for p ∆ let
K
∈
n
ˆ
Reg (p) = q p,f
n t t
−
(cid:88)t=1 (cid:68) (cid:69)
(cid:100)
be the regret of the sequence of distributions p compared to p on esti-
t
mated loss functions (fˆ )n . The next theorem bounds Reg (p).
t t=1 n
Theorem 8.1. Given a distribution q ∆ and f : K R define
K (cid:100)
∈ →
(f) = D ( R(q) f, R(q)).
q ⋆
S ∇ − ∇
The following holds for any p ∆ ,
K
∈
n
D(p,q )
1 ˆ
(a) Reg (p) + (ηf ).
n ≤ η Sqt t
t=1
(cid:88)
(b) S(cid:100)uppose that η fˆ (x) 1 for all t and x K, then
t
| | ≤ ∈
n
D(p,q )
Reg (p) 1 +η fˆ (x)2q (x)dx.
n ≤ η t t
t=1 (cid:90)K
(cid:88)
(cid:100)
We omit the proof, which is the continuous analogue of Theorem 7.4.
8.3 A one-dimensional surrogate
For this section assume that d = 1 and K = [ 1,1] is the interval. Given
−
ε > 0 let K = [ε 1,1 ε] and note that by Proposition 3.5, any loss
ε
− −
f F is 1-Lipschitz on K .
∈ b ε ε
Wenowwanttousecontinuousexponentialweightswithanestimated
surrogate. If we are to use Theorem 8.1, then we need to somehow write
the expected regret in terms of the regret associated with the exponential
weights distribution of the surrogate loss. So if q ∆ is an exponential
K
∈
weights distribution and s : K R is a surrogate and p ∆ is the
K
→ ∈
distribution we actually sample from, we want to show that
p(x)f(x) f(x ) ≲ s(x)q(x)dx s(x ).
⋆ ⋆
− −
(cid:90)K (cid:90)KCONTINUOUS EXPONENTIAL WEIGHTS 74
Because x is not known, in actuality we need to show something of this
⋆
sort for all points in K. Let us simultaneously make the meaning of ≲
concrete. We want to find some λ > 0 and ε > 0 such that
1
p(x)f(x) f(y) s(x)q(x)dx s(y) +ε y K. (8.1)
− ≤ λ − ∀ ∈
(cid:90)K (cid:20)(cid:90)K (cid:21)
Naturally, we want λ to be large and ε to be small. A particularly
elegant way to try and construct a surrogate satisfying these properties
is by using a kernel. Let T : K K R be a function with y T(x,y)
× → (cid:55)→
a probability density for all x. Given q ∆ and f : K R, let
K
∈ →
(Tq)(y) = T(x,y)q(x)dx (T∗f)(x) = T(x,y)f(y)dy.
(cid:90)K (cid:90)K
Then, if we let s = T∗f and p = Tq, notice that
s(x)q(x)dx = T(x,y)f(y)dy q(x)dx
(cid:90)K (cid:90)K (cid:18)(cid:90)K (cid:19)
= f(y) T(x,y)q(x)dx dy
(cid:90)K (cid:18)(cid:90)K (cid:19)
= f(y)p(y)dy.
(cid:90)K
With this choice of surrogate establishing Eq. (8.1) corresponds to show-
ing that
(T∗f)(y) λf(y)+(1 λ) p(x)f(x)dx+ελ.
≤ −
(cid:90)K
Given a kernel T we now have a surrogate s = T∗f. Next we need a
way to estimate this surrogate when the learner samples X from p and
observes Y = f(X). Suppose that p(y) = 0 implies that T(x,y) = 0 for
all x. That is, q is absolutely continuous with respect to y T(x,y) for
(cid:55)→
all x. Then,
T(x,y)
s(x) = T(x,y)f(y)dy = f(y)p(y)dy,
p(y)
(cid:90)K (cid:90)KCONTINUOUS EXPONENTIAL WEIGHTS 75
which shows that the surrogate can be be estimated by
T(x,X)Y
sˆ(x) = . (8.2)
p(X)
Looking at Theorem 8.1, we will need to show that E[ p(x)sˆ(x)2] is not
K
too large. You can only do so much without specifying T, but at least
(cid:82)
we have
p(x)T(x,y)2f(y)2
E p(x)sˆ(x)2 = dx p(y)dy
p(y)2
(cid:20)(cid:90)K (cid:21) (cid:90)K (cid:18)(cid:90)K (cid:19)
p(x)
T(x,y)2dxdy.
≤ p(y)
(cid:90)K (cid:90)K
Summarising what we have discovered. We need to find a T, λ > 0 and
ξ > 0 such that the following hold:
(a) (T∗f)(y) λf(y)+(1 λ) p(x)f(x)dx+ξλ for all f F and
≤ − K ∈ b
y K; and
∈ (cid:82)
(b) p(x)T(x,y)2dxdy is small.
K K p(y)
A ver(cid:82)y i(cid:82)mportant point is that we are allowed to choose T to depend on
the exponential weights distribution q.
Defining the kernel Let q ∆ with mean µ = xq(x)dx and
∈ k K
define a kernel
(cid:82)
ν (y) if x µ 1/n2
µ,x
T(x,y) = | − | ≥
(cid:40)ν µ,µ+sign(x−µ)/n2(y) otherwiseCONTINUOUS EXPONENTIAL WEIGHTS 76
Note that when x µ < 1/n2, then W (ν ,ν ) 1/n2 and
1 µ,x µ+sign(x−µ)/n2
| − | ≤
hence
T∗f(x) = T(x,y)f(y)dy
(cid:90)K
2
f(y)ν (y)dy + f is 2n-Lipschitz on K
≤
x,µ
n
1/n−1/n2
(cid:90)K
1 1 2
f(x)+ f(µ)+ (Convexity of f)
≤ 2 2 n
1 1 4
f(x)+ f(ν)+ f is n-Lipschitz on K
1/n
≤ 2 2 n
1 1 4
f(x)+ p(x)f(x)dx+ . Convexity of f
≤ 2 2 n
(cid:90)K
Second, T(x,y) 1/max(1/n2, µ y ) so that
≤ | − |
T(x,y)2q(x) 1 T(x,y)p(q)
dydx dxdy
p(y) ≤ max(1/n2, y µ ) p(y)
(cid:90)K (cid:90)K (cid:90)K
| − |
(cid:90)K
1
= dy
max(1/n2, y µ )
(cid:90)K
| − |
4log(n).
≤
Summarising, we have shown that:
(a) T∗f(x) 1f(x)+ 1 f(y)p(y)dy + 4 for all x K .
≤ 2 2 K n ∈ 1/n
(b)
T(x,y)2q(x)
dydx(cid:82) 4log(n)
K K p(y) ≤
B(cid:82)efo(cid:82)re the analysis, some remarks on computation. There are essen-
tially two non-trivial operations in Algorithm 15.
➳ How to sample from p ?
t
➳ How to compute the p (X )?
t t
ˆ
The key observation is that f is a scaled indicator function of a uniform
t
distribution. Hence q is piecewise constant with at most O(t) pieces.
t
Using this means that both quantities can be computed in O(t) time. In
practice we suspect one can do much better with various tricks and good
data representation. A good challenge?CONTINUOUS EXPONENTIAL WEIGHTS 77
1 args: learning rate η > 0
2 let J = [1 1,1 1]
n − − n
3 for t = 1 to n
4 compute q (x) = 1 (x)
exp(−η(cid:80)t u− =1 1fˆ u(x))
t J (cid:82) Jexp(−η(cid:80)t u− =1 1fˆ u(y))dy
5 let µ = xq (x)dx and
t K t
(cid:82) ν (y) if x µ 1
6 T (x,y) = µt,x | − t | ≥ n2
t

ν (y) otherwise
 µt,µt+sign(x−µt)/n2
7 sample X p = T q and observe Y = f (X )
t
∼
t t t t t t
8 compute fˆ (x) = Tt(x,Xt)Yt
t pt(Xt)
Algorithm 15: Continuous exponential weights for bandits: d = 1
Theorem 8.2. The expected regret of Algorithm 15 is bounded by
E[Reg ] 5+√nlog(n).
n
≤
8.4 Exploration by optimisation
Let us start by giving the regret bound for Algorithm 14.
Theorem 8.3. The expected regret of Algorithm 14 is bounded by
n
log(n) 1
E[Reg (p)] + E p q ,f + p q ,fˆ + (ηfˆ ) .
n ≤ η ⟨ t − t t ⟩ ⟨ ⋆ − t t ⟩ ηSqt t
t=1 (cid:20) (cid:21)
(cid:88)CONTINUOUS EXPONENTIAL WEIGHTS 78
Proof. The proof follows immediately from Theorem 8.1
n
E[Reg ] = E[ p p ,f ]
n t ⋆ t
⟨ − ⟩
t=1
(cid:88)
n
= E p p ,f + p q ,fˆ + q p ,fˆ
t ⋆ t t t t ⋆ t
⟨ − ⟩ ⟨ − ⟩ ⟨ − ⟩
(cid:88)t=1 (cid:104) (cid:105)
n
D(p ,p )
⋆ 1 + E p p ,f q p ,fˆ + (ηfˆ ) ,
≤ η ⟨
t
−
⋆ t
⟩−⟨
t
−
⋆ t
⟩
Sqt t
(cid:88)t=1 (cid:104) (cid:105)
ˆ
where the second equality holds by adding and subtracting q p ,f
t ⋆ t
⟨ − ⟩
and the inequality by Theorem 8.1.
StandardmethodsforanalysingconcreteinstantiationsofAlgorithm14
essentially always bound the term inside the expectation uniformly for
all t, independently of f and q and p. Note that f and p are unknown,
t t t ⋆
while q is the exponential weights distribution which is known. In light
t
of this, a natural idea is to choose the distribution p and loss estimation
t
ˆ
function f that minimise the upper bound.
t
To simplify the notation, let q ∆ and G be the set of all functions
K
from Rd R. The estimation f∈ unction fˆ is a function from K to R
→
but the learner chooses it based on the observations X and Y. So let
E : K R G be the function that selects the estimation function
× →
based on the data. The decision for the learner is to choose a method
of constructing the estimation function E and a distribution p ∆ .
K
∈
Minimising the bound corresponds to the following saddle-point problem
Λ (q) = inf sup E[ p p ,f + p q,E(X,Y) + (ηE(X,Y))],
η ⋆ q
p E∈∆ ∈EK fp ∈⋆ F∈∆ (KK
)
⟨ − ⟩ ⟨ − ⟩ S
Λ(q,p,E)
where the expectation is over X p and Y = f(X). This is a well-
∼
behaved convex/concave saddle-point problem but with continuous do-
mains.
Anyway, having defined Λ(q) we almost immediately have the follow-
ing theorem.CONTINUOUS EXPONENTIAL WEIGHTS 79
1 args: learning rate η > 0, precision ε > 0
2 for t = 1 to n
3 compute distribution q (x) = 1K(x) exp η t−1 fˆ (x)
t zt − u=1 u
4 where z = exp η t−1 fˆ (x) dx (cid:16) (cid:80) (cid:17)
t K − u=1 u
5 find distr(cid:82)ibuti(cid:16)on p(cid:80)t ∆
K
an(cid:17)d E
t
such that
∈ ∈ E
6 Λ (q ,p ,E ) inf Λ (q ,p,E)+ε
η t t t p,E η t
≤
7 sample X from p and observe Y
t t t
ˆ
8 compute f = E(X ,Y )
t t t
Algorithm 16: Exploration by optimisation
Theorem 8.4. The expected regret of Algorithm 16 is bounded by
dlog(n)
E[Reg ] +n sup Λ (q)+nε.
n ≤ η η
q∈∆K
This is not very useful without control on sup Λ (q) and upper
q∈∆K η
bounding this quantity looks daunting. In the next section we explain
a connection between Λ (q) and a concept used for analysing Bayesian
η
bandit problems called the information ratio. This method eventually
shows that
sup Λ (q) ηd4logpoly(n,diam(K)),
η
≤
q∈∆K
which after optimising η leads to a bound on the expected regret of
Algorithm 16 of
E[Reg ] d2.5√nlogpoly(n,diam(K)).
n
≤
8.5 Bayesian convex bandits
In the Bayesian version of the convex bandit problem the learner is given
a distribution ξ on Fn. The loss functions (f )n are sampled from ξ
b t t=1CONTINUOUS EXPONENTIAL WEIGHTS 80
and the Bayesian regret of a learning algorithm A is
n
bReg (A,ξ) = E f (X ) f (X ) ,
n t t t ⋆
−
(cid:34) (cid:35)
t=1
(cid:88)
where X = argmin n f (x). Note that here the expectation inte-
⋆ x∈K t=1 t
grates over the randomness in the loss functions, optimal action as well
(cid:80)
as the actions of the learner. This is nothing more than the expectation
of the standard regret, integrating over the loss functions with respect to
the prior ξ. The minimax Bayesian regret is
bReg⋆ = supinfbReg (A,ξ).
n n
A
ξ
Compare this to the minimax adversarial regret, which is
Reg⋆ = inf sup Reg (A,(f )n ).
n n t t=1
A
(ft)n
t=1
Whenever you see expression like this, a minimax theorem should come
to mind. Indeed, the minimax adversarial regret can be rewritten as
Reg⋆ = infsupbReg (A,ξ).
n n
A
ξ
By interpreting an algorithm as a probability measure over deterministic
algorithms both A bReg (A,ξ) and ξ bReg (A,ξ) are linear
n n
(cid:55)→ (cid:55)→
functions and from this one should guess the following theorem as a
consequence of some kind of minimax theorem.
Theorem 8.5. Reg⋆ = bReg⋆.
n n
Theorem 8.5 means that one way to bound the adversarial regret
is via the Bayesian regret. One positive aspect of this idea is that the
existence of a prior makes the Bayesian setting more approachable. On
the other hand, constructing a prior-dependent algorithm showing that
the Bayesian regret is small for any prior does not give you an algorithm
for the adversarial setting. The approach is non-constructive.CONTINUOUS EXPONENTIAL WEIGHTS 81
8.6 Duality and the information ratio
We now briefly explain the main tool for bounding the Bayesian regret.
Let ν be a probability measure on K F(K) and p ∆ . Suppose that
K
× ∈
(X,X ,f) has law p ν and define
⋆
⊗
∆(p,ν) = E[f(X) f(X )]
⋆
−
I(p,ν) = E KL P ( ),P ( ) .
X⋆
·
X⋆|X,f(X)
·
(cid:2) (cid:0) (cid:1)(cid:3)
Intuitively, ∆(p,ν)istheregretsufferedwhensamplingX fromprelative
to X on loss f sampled from ν while I(p,ν) is the information gained
⋆
about the optimal action when observing X and f(X). The information
ratio captures the exploration/exploitation trade-off made by a learner
and is defined by
∆(p,ν)2
Ψ(p,ν) = ,
I(p,ν)
The information ratio will be small when the regret under p is small rel-
ative to the information gained about the optimal action. The minimax
information ratio is
sup min Ψ(p,ν).
ν∈∆ K×F(K)p∈∆K
Theorem 8.6. The Bayesian minimax regret is bounded by
bReg⋆ 1+C dnΨ⋆log(n),
n ≤
(cid:112)
where C > 0 is a universal constant.
The minimax theorem shows that the Bayesian regret and adversarial
regret are the same in the worst case.
Theorem 8.7. Λ⋆ ηΨ⋆ .
η ≤ 4
8.7 Notes
(a) Continuous exponential weights is a powerful algorithm. The lifting
to the space of probability measures can be viewed as a means of lin-
earisation. The algorithm plays distributions p ∆ and the expected
K
∈CONTINUOUS EXPONENTIAL WEIGHTS 82
loss is p(x)f(x)dx, which is linear in p. Consequentially, continuous
K
exponential weights in the full information setting does not depend on
(cid:82)
convexity of K or the losses. Though computation usually becomes im-
possible in the non-convex settings. Note that our results for bandits
all exploited convexity of both the losses and the constraint set in some
places.
(b) Information-directed sampling and the core analysis was introduced
by Russo and Van Roy [2014]. The application to convex bandits and
to prove bounds non-constructively for adversarial bandit problems is by
Bubeck et al. [2015], who were the first to show that √n regret is possible
for adversarial convex bandits for losses in F . The extension to higher
b
dimensions is by Bubeck and Eldan [2018] and [Lattimore, 2020]. The
latter shows that the minimax regret for adversarial bandits is at most
d2.5√n, which is the best known bound.
(c) The duality between mirror descent and the information ratio was
established by [Zimmert and Lattimore, 2019, Lattimore and Gyo¨rgy,
2021c] with the latter proving the difficult direction. These connections
have apparently inspired a beautiful theory on the complexity of sequen-
tialdecisionmakingingreatgenerality[Fosteretal.,2021,2022]. Inbrief,
algorithms like exploration-by-optimisation are provably near-optimal in
a minimax sense. There are many subtleties and you should just read
the aforementioned works.
(d) Besides inconsequential simplifications, the kernel-based method in
one dimension was designed by Bubeck et al. [2017]. They extended
the general idea to the higher dimensions to design a polynomial time
algorithm with regret d10.5√n, which was the first polynomial time algo-
rithm with poly(d)√n regret in the adversarial setting. Sadly there are
many challenges to generalising Algorithm 15 and ultimately the higher-
dimensional version is not realistically implementable.Chapter 9
Online Newton step
We can now present a simple method for obtaining O(√n) regret for
losses in F with the limitation that the analysis only works in the
b,l,u
stochastic setting where f = f for all rounds. Actually the majority
t
of our arguments are for losses in F but the regret guarantee
b,l,sm,sc,u,e
depends only logarithmically on the smoothness and strong convexity
parameters and hence the reduction in Chapter 3 yields an algorithm
for the larger class F . The boundedness assumption can be further
b,l,u
relaxed using a slightly more complicated algorithm and analysis as ex-
plained in Note (d). To ease the presentation and analysis we let L be
a sufficiently large logarithmic factor. Formally,
L = C[1+logmax(n,d,diam(K),β/α,1/δ)] ,
where C > 0 is a universal constant. Because we are working with loss
functions in F we will abuse notation by assuming that f is defined on
e
all of Rd while simultaneously ensuring the algorithm only ever queries
the loss at points in (x+Bd).
∪x∈K ϱ
9.1 The blessing and curse of curvature
The presence of curvature in bandit convex opti-
misation is both a blessing and a curse. The key
to obtaining optimal regret is to make sure you
exploit the positive aspects while taking care to
controlthenegativeones. Themaineffectsofcurvaturearethefollowing:
➳ Smoothing should be done on a smaller radius to maintain a suit-
ably small approximation error. This increases the variance of the
83ONLINE NEWTON STEP 84
gradient estimator.
➳ When there is significant curvature the regret decreases quadrat-
ically with distance to the minimiser, which means the algorithm
can move more slowly. This can be achieved by increased regu-
larisation. Mathematically this helps because the variance of the
gradient estimator is modulated by the Hessian of the regularisa-
tion. More regularisation leads to slower averaging of the gradient
estimates and helps mitigate any increase in noise.
YoucanseethatAlgorithm10exactlyexhibitsthisbehaviour. Theprob-
lem is that to tune its learning rate we needed to assume existence and
knowledge of upper/lower bounds on the smoothness/curvature. There
is another issue, however. Piecewise linear functions have enormous cur-
vature in some places and none elsewhere, which means they suffer from
(a) above but do not benefit from (b). The insight for overcoming this
is to make sure the surrogate loss function does so much smoothing that
it is nearly a quadratic on a region containing both the current iterate
x and the optimal point. Lastly, you must be careful that curvature
t
can be large in some directions and small in others. Any algorithm must
adapt to the curvature in a non-uniform way, which was not true of
Algorithm 10.
9.2 Online Newton step
Let qˆ ,...,qˆ : K R be a sequence of quadratic functions and consider
1 n
→
the full information setting where in round t the learner proposes x K
t
∈
observes the entire function qˆ and the regret relative to x K is
t
∈
n
Reg (x) = (qˆ(x ) qˆ(x)) .
n t t t
−
t=1
(cid:88)
(cid:100)
OnlineNewtonstepisasecond-ordermethodsummarisedinAlgorithm17.
WenotenowthatonlineNewtonstepcomesinmanyformsandgivesome
pointers in the notes. You may also wonder about what principles would
lead to this algorithm. More on that in the notes as well.ONLINE NEWTON STEP 85
1 args: η > 0, Σ−1 Sd and µ K
1 ∈ + 1 ∈
2 for t = 1 to n
3 let g = qˆ′(µ ) and H = qˆ′′(µ )
t t t t t t
4 update Σ−1 = Σ−1 +ηH
t+1 t t
5 update µ = argmin x [µ ηΣ g ] 2
t+1 x∈K
∥ −
t
−
t+1 t ∥Σ− t+1
1
Algorithm 17: Online Newton step for quadratic losses
Theorem 9.1. Suppose that Σ−1 0 for all 1 t n, then for any
t
≻ ≤ ≤
x K,
∈
1 1 η2 n
µ x 2 µ x 2 + g 2 ηReg (x).
2 ∥ n+1 − ∥Σ− n+1 1 ≤ 2 ∥ 1 − ∥Σ− 11 2 ∥ t ∥Σt+1 − n
t=1
(cid:88)
(cid:100)
Proof. By definition,
1 1
µ x 2 µ x ηΣ qˆ′(µ ) 2
2 ∥ t+1 − ∥Σ− t+1 1 ≤ 2 ∥ t − − t+1 t t ∥Σ− t+1 1
1 η2
= µ x 2 η qˆ′(µ ),µ x + qˆ′(µ ) 2
2 ∥ t − ∥Σ− t+1 1 − ⟨ t t t − ⟩ 2 ∥ t t ∥Σt+1
1 η2
= µ x 2 η(qˆ(µ ) qˆ(x))+ g 2 .
2 ∥ t − ∥Σ− t1 − t t − t 2 ∥ t ∥Σt+1
Summing over t from 1 to n and telescoping completes the proof.
9.3 Quadratic surrogate
Like in Chapter 6 we use a quadratic surrogate. Unlike that chapter,
however, the curvature of the surrogate now depends on the loss function
and needs to be estimated. Let X have law (µ,Σ) and f F
l,u,sm,sc
N ∈
and for λ (0, 1 ), define
∈ d+1
λ
q(x) = s′(µ),x µ + x µ 2 , (9.1)
⟨ − ⟩ 6 ∥ − ∥s′′(µ)ONLINE NEWTON STEP 86
where s : Rd R is the convex surrogate defined by
→
1 1
s(y) = E 1 f(X)+ f((1 λ)X +λy) .
− λ λ −
(cid:20)(cid:18) (cid:19) (cid:21)
WespendallofChapter11ontheintuitionsandanalysisofthissurrogate
loss. You can skip ahead to that chapter now or accept the following
properties as gospel.
Proposition 9.2. Let q be the function q defined in Eq. (9.1) and suppose
that x Rd satisfies λ x µ 1. Then,
∈ ∥ − ∥Σ−1 ≤ L
24
E[f(X)] f(x) q(µ) q(x)+ tr(q′′(µ)Σ) .
− ≤ − λ
Of course q also cannot be observed directly using X and Y. But it
can be estimated by
λ
qˆ(x) = g,x µ + x µ 2 ,
⟨ − ⟩ 6 ∥ − ∥H
where g and H are defined by
RYΣ−1(X µ) λRY Σ−1(X µ)(X µ)⊤Σ−1
g = − H = − − Σ−1 ,
1 λ (1 λ)2 (1 λ)2 −
− − (cid:20) − (cid:21)
with p the density of (µ,Σ) and
N
1 d p X−λµ
R = 1−λ
1 λ p(X)
(cid:18) − (cid:19) (cid:0) (cid:1)
You should check that E[g] = s′(µ) and E[H] = s′′(µ) or read Chapter 11
for a proof.
9.4 Algorithm and analysis
Theorem 9.3. Suppose that
min(1,ϱ2) 1 σ 1 1
σ2 = λ = min , η = .
dL dL diam(K)L√2 λ ndL5
(cid:18) (cid:19) (cid:114)ONLINE NEWTON STEP 87
1 args: η, λ, σ2
2 µ = 0, Σ = σ21
1 1
3 for t = 1 to n
4 sample X from (µ ,Σ ) with density p
t t t t
N
5 let R = 1 d
pt(Xt 1− −λ λµt)
t 1−λ pt(Xt)
6 compute g(cid:0)t =(cid:1)RtYtΣ− t 1−1( λXt−µt)
7 compute H = λRtYt
Σ− t1(Xt−µt)(Xt−µt)⊤Σ− t1
Σ−1
t (1−λ)2 (1−λ)2 − t
8 update Σ−1 = Σ−1 +(cid:104)ηH and µ = µ ηΣ (cid:105)g
t+1 t t t+1 t t+1 t
−
Algorithm 18: Online Newton Step for Convex Bandits
Then with probability at least 1 5δ the regret of Algorithm 18 is bounded
−
by
ddiam(K)L√n
Reg d1.5√nL3 + .
n ≤ min(1,ϱ)
Proof. The main complication is that the conclusion of Proposition 9.2
only holds for some x. Let
F = µ x 2 .
t
∥
t
−
⋆ ∥Σ− t1
1
In order to make our analysis go through we need to argue that λF2 1
t ≤ L
for all t with high probability. There are a few other complications.
Most notably, the algorithm is not properly defined if Σ fails to be
t
positive definite. Hence we need to prove also that this occurs with low
probability. Note that E[H ] is the Hessian of a convex function and
t
hence positive definite. Thus we will use concentration of measure to
show that Σ indeed stays positive definite with high probability. Define
t
the following quantities:
t t
S = H S¯ = E [H ] Σ¯−1 = Σ−1 +ηS¯ .
t u t u−1 u t 1 t−1
u=1 u=1
(cid:88) (cid:88)ONLINE NEWTON STEP 88
Definition 9.4. Let τ be the first round when one of the following does
not hold:
1
(a) λF2 1.
τ+1 ≤ L
(b) Σ is positive definite.
τ+1
(c) 1Σ¯−1 Σ−1 3Σ¯−1 .
2 τ+1 ⪯ τ+1 ⪯ 2 τ+1
In case (a)-(c) hold for all rounds t n, then τ is defined to be n.
≤
Note that F and Σ are measurable with respect to F , which
t+1 t+1 t
means that τ is a stopping time with respect to the filtration (F )n .
t t=1
Step 1: Concentration We need to show that the behaviour of the
various estimators used by Algorithm 18 is suitably regular with high
probability. Further, we need to show the algorithm plays in dom(f)
⊃
(x+Bd) with high probability. We start with a simple lemma.
x∈K ϱ
Lemma 9.5. With probability at least 1 2δ the following hold:
(cid:83)
−
(a) max X µ ϱ.
1≤t≤τ t t
∥ − ∥ ≤
(b) max Y √L.
1≤t≤τ t
| | ≤
Proof. By definition of τ, if t τ, then Σ 2Σ¯ 2Σ = 2σ21. Hence,
t t 1
≤ ⪯ ⪯
by Lemmas C.3 and C.1 and a union bound,
dσ2 2n
P max X µ 4 log δ.
t t
(cid:32)1≤t≤τ∥ − ∥ ≥ (cid:115) 3 δ (cid:33) ≤
(cid:18) (cid:19)
By the definition of σ2 = 1 min(1,ϱ2), it follows that
dL
P max X µ min(1,ϱ) δ.
t t
1≤t≤τ∥ − ∥ ≥ ≤
(cid:18) (cid:19)
Since µ K, whenever X µ ϱ it holds that X dom(f) and
t t t t
∈ ∥ − ∥ ≤ ∈
because f is Lipschitz and bounded in [0,1] on dom(f),
f(X ) f(X ) f(µ ) +1 X µ +1.
t t t t t
| | ≤ | − | ≤ ∥ − ∥ONLINE NEWTON STEP 89
Furthermore, by a union bound, the assumptions on the noise (Eq. (1.1))
and Lemma C.1,
P max ε log(2n/δ) δ.
t
1≤t≤τ| | ≥ ≤
(cid:18) (cid:19)
(cid:112)
The result follows since Y f (X ) + ε .
t t t t
| | ≤ | | | |
Since X µ ϱ implies that X dom(f), combining Lem-
t t t
∥ − ∥ ≤ ∈
mas 11.15 and 9.5 with a union bound shows that with probability at
least 1 (n+2)δ,
−
τ
g 2 ndL2. (9.2)
∥ t ∥Σt ≤
t=1
(cid:88)
By Proposition 11.18(a) with D = 1 , with probability at least 1 δ,
λL −
τ τ
2√n
(qˆ(µ ) qˆ(x )) (q (µ ) q (x ))+ . (9.3)
t t t ⋆ t t t ⋆
− ≤ − λ
t=1 t=1
(cid:88) (cid:88)
By the definition of τ, for any t τ, Σ−1 3Σ¯−1. Finally, by Proposi-
tion 11.19 with Σ−1 = 3Σ¯−1, with≤ probat bil⪯ ity2 att least 1 δ,
2 τ −
1 3λL
S¯ Σ¯−1 S¯ √ndΣ¯−1
τ − 2η τ ⪯ τ − 2 τ
S
τ
⪯
3λL
S¯
+
√ndΣ¯−1
⪯ τ 2 τ
1
S¯
+
Σ¯−1,
(9.4)
⪯ τ 2η τ
where the first and last inequalities follows because
3
η .
≤ 4λL√nd
ByaunionboundallthreeeventsinEq.(9.2), Eq.(9.3)andEq.(9.4)and
the two outlined in Lemma 9.5 occur with probability at least 1 (n+4)δ
−
and we assume for the remainder that indeed this happens. Note thatONLINE NEWTON STEP 90
when Eq. (9.4) holds, then
1 3
Σ−1 = Σ−1 +ηS Σ−1 +ηS¯ + Σ¯−1 Σ¯−1 ,
τ+1 1 τ ⪯ 1 τ 2 τ ⪯ 2 τ+1
where in the final inequality we used the fact that
Σ¯−1 Σ¯−1
. Similarly,
τ ⪯ τ+1
1 1
Σ−1 Σ−1 +ηS¯ Σ¯−1 Σ¯−1 .
τ+1 ⪰ 1 τ − 2 τ ⪰ 2 τ+1
Step 2: Trace/Logdet inequalities By the defintions of H and η
t
and the assumption that Y √L,
t
| | ≤
η Σ1/2H Σ1/2 1.
t t t
≤
(cid:13) (cid:13)
(cid:13) (cid:13)
Hence, by Lemma B.6,
(cid:13) (cid:13)
24 τ 100 τ ηq′′(µ )Σ
tr(q′′(µ )Σ ) logdet 1+ t t t
λ t t t ≤ λη 2
t=1 t=1 (cid:18) (cid:19)
(cid:88) (cid:88)
τ
100
logdet 1+ηq′′(µ )Σ¯
≤ λη t t t
t=1
(cid:88) (cid:0) (cid:1)
τ
100
= logdet
Σ¯ Σ¯−1
λη t t+1
t=1
(cid:88) (cid:0) ¯ (cid:1)
100d ηS
= logdet 1+ τ
λ σ2
(cid:18) (cid:19)
100d ηλβ1
logdet 1+
≤ λ σ2
(cid:18) (cid:19)
dL
,
≤ λ
where in the first inequality we used Lemma B.6. The second we used
¯
the fact that for t τ, Σ 2Σ . The third inequality holds because
t t
≤ ≤
q′′(µ ) λβ and the last by the definition of L.
t t ≤
Step 3: Regret By the assumption that Eq. (9.4) holds, for any t τ,
≤
g 2 2 g 2 2 g 2 3 g 2 . (9.5)
∥ t ∥Σt+1 ≤ ∥ t ∥Σ¯ t+1 ≤ ∥ t ∥Σ¯ t ≤ ∥ t ∥ΣtONLINE NEWTON STEP 91
By Theorem 9.1 and the assumption that Eqs. (9.2) to (9.4) hold,
1
F = x µ 2
τ+1
2 ∥
⋆
−
τ+1 ∥Σ− τ+1
1
(a)
diam(K)2 η2 τ
+ g 2 ηReg (x )
≤ 2σ2 2 ∥ t ∥Σt+1 − τ ⋆
t=1
(cid:88)
(b) diam(K)2 3η2ndL2 (cid:100)
+ ηReg (x )
≤ 2σ2 2 − τ ⋆
(c) diam(K)2 3η2ndL2 τ 2η√n
+ η(cid:100)(q (µ ) q (x ))+
≤ 2σ2 2 − t t − t ⋆ λ
t=1
(cid:88)
(d) diam(K)2 3η2ndL2 24η τ 2η√n
+ + tr(q′′(µ )Σ ) ηReg +
≤ 2σ2 2 λ t t t − τ λ
t=1
(cid:88)
(e) diam(K)2 3η2ndL2 dL 2η√n
+ + ηReg +
≤ 2σ2 2 λ − τ λ
(f) 1
ηReg ,
≤ 2λ2L2 − τ
where (a) follows from Theorem 9.1, (b) from Eq. (9.2) and Eq. (9.5),
(c) from Eq. (9.3), (d) from Proposition 9.2, the conditions of which are
met with x = x by the definition of the stopping time. (e) follows from
⋆
the previous step and (f) by the definition of the constants and naive
simplification. Therefore with probability 1 3δ all of the following hold:
−
(a) F 1 .
τ+1 ≤ λ2L2
(b) Reg 1 .
τ ≤ ηλ2L2
(c) Σ is positive definite and 1Σ¯−1 Σ−1 3Σ¯−1 .
τ+1 2 τ+1 ⪯ τ+1 ⪯ 2 τ+1
By Definition 9.4, on this event we have τ = n and hence Reg 1 .
n ≤ ηλ2L2
The result follows from the definitions of η and λ.
9.5 Notes
(a) Online Newton step is due to Hazan et al. [2007] and the application
to bandits is by Lattimore and Gyo¨rgy [2023].
(b) When K = Rd, then online Newton step for quadratic loss functions
is equivalent to continuous exponential weights with a Gaussian prior.ONLINE NEWTON STEP 92
That is, if q is the density with respect to the Lebesgue measure of
(0,Σ ) and
1
N
q(x)exp η t−1 fˆ (x)
− u=1 u
q (x) = ,
t exp η(cid:16) t−(cid:80)1 fˆ (y) q(y(cid:17) )dy
Rd − u=1 u
(cid:16) (cid:17)
(cid:82) (cid:80)
then q is the density of (µ ,Σ ). More details by van der Hoeven et al.
t t t
N
[2018].
(c) You should be a little unhappy about the non-specific constants in
Theorem 9.3. How can you run the algorithm if the constants depend on
universal constants and unspecified logarithmic factors? The problem is
that the theory is overly conservative. In practice both η and λ should
be much larger than the theory would suggest, even if you tracked the
constants through the analysis quite carefully. This is quite a standard
phenomenon, andusuallynotaproblem. Herethereisalittletwist, how-
ever. If you choose η or λ too large, then the algorithm can explode with
non-negligible probability. For example, the covariance matrix might
stop being positive definite at some point or F could grow too large and
t
the algorithm may move slowly relative to the regret suffered. We have
no particular recommendations here except that you should be cautious.
(d) We assumed that losses were bounded: f(x) [0,1] for all x
∈ ∈
dom(f). This was only used in Lemma 9.5. A simple modification of the
algorithm eliminates the need for this. The idea is to set replace Y in the
t
algorithm with Z = Y Y . This does not change the expectation and
t t t−1
−
has the property that Z2 can be controlled without using boundedness
t
of the loss. The only annoyance is that you need to show that µ and
t
µ are sufficiently close. Since Y does not exist you need to initialise
t−1 0
the algorithm by playing X = µ anywhere in K and then starting the
1 1
algorithm with µ = µ [Lattimore and Gy¨orgy, 2023].
2 1Chapter 10
Ellipsoid method
The ellipsoid method is a classical method for optimising non-smooth
convex functions and was famously the key ingredient in the first poly-
nomial time algorithm for linear programming. Though the algorithm
is not terribly practical, it remains an important theoretical tool. We
explain how to use the optimistic surrogate in combination with the el-
lipsoid method to control the regret for the stochastic setting with losses
in F . We start by recalling how to use the shallow cut ellipsoid method
b
for convex optimisation where gradients of the loss are available and then
explain how to construct a suitable separation oracle using the optimistic
surrogate loss.
LikeinChapter9wewillactuallyworkwithlossesinF . Then,
b,l,sm,sc
since our bound is logarithmic in diam(K), α and β, the meta algorithm
in Chapter 3 yields an algorithm for losses in F . To be concrete, unless
b
otherwise stated, for the remainder of the chapter we make the following
assumptions:
(a) The setting is stochastic, meaning that f = f for all t; and
t
(b) The loss f is in F .
b,l,sm,sc
To ease the presentation and analysis we let L be a sufficiently large
logarithmic factor. Formally,
L = C[1+logmax(n,d,diam(K),β/α)] ,
where C > 0 is a universal constant.
93ELLIPSOID METHOD 94
10.1 Ellipsoid method
For x Rd and positive definite matrix A, define the ellipsoid
∈
E(x,A) = y : y x 1 .
{ ∥ −
∥A−1
≤ }
Furthermore, given a convex body K Rd, let mvee(K) be the ellipsoid
⊂
of the smallest volume containing K, which is unique.
Lemma 10.1 (Artstein-Avidan et al. 2015, Proposition 2.1.7). For con-
vex body K Rd there exists a unique ellipsoid of minimum volume
⊂
E = mvee(K) such that K E.
⊂
The ellipsoid method for convex optimisation inductively defines a
sequence of ellipsoids (E )∞ starting with K E . Subsequent ellip-
k k=1 ⊂ 1
soids are constructed using a subgradient of the loss to cut away parts
of the constraint set in which the optimal cannot lie. There are many
versions of the ellipsoid method, all based on the same fundamental idea
but with subtle differences depending on computational and statistical
requirements. We give an abstract version in Algorithm 19 which is
non-standard because it is not computationally efficient. More classical
versions are described in the notes.
1 args: r [0,1/d)
∈
2 let K = K
1
3 for k = 1 to
∞
4 let E = mvee(K ) = E(x ,A )
k k k k
5 let z E(x ,r2A ) and g ∂f(z )
k k k k k
∈ ∈
6 let K = K x : g ,x z 0
k+1 k k k
∩{ ⟨ − ⟩ ≤ }
Algorithm 19: An ellipsoid method for convex optimisation
Theorem 10.2. Suppose that f F and ε > 0 and (z )∞ are the
∈ l k k=1
iterates produced by Algorithm 19. Let
1+log v vo ol l( (E K1 )) +dlog diam ε(K) (1 rd)2
k = γ = exp − .
⋆  (cid:16) (cid:17)log(γ) (cid:16) (cid:17) − 5d
− (cid:18) (cid:19)
 
 
 ELLIPSOID METHOD 95
Then, min f(z ) min f(x)+ε.
k≤k⋆ k
≤
x∈K
ByJohn’stheorem,E(x , 1 A ) K andthereforevol(E )/vol(K)
1 d2 1 ⊂ 1 ≤
dd and hence when r 1 Algorithm 19 has sample complexity at most
≤ 2d
ddiam(K)
k = O d2log .
⋆
ε
(cid:18) (cid:18) (cid:19)(cid:19)
Except for constants, when d = 1 this matches the rate of the noiseless
bisection method from Chapter 4. Before the proof of Theorem 10.2
we state the fundamental lemma about the ellipsoid method, which says
that:
➳ Given an ellipsoid E and a half-space H, there exists a closed-form
expression for mvee(E H); and
∩
➳ Provided that H cuts E deeply enough, then
vol(mvee(E H)) = O((1 1/d))vol(E).
∩ −
The proof is a difficult algebraic exercise and is omitted but you should
check out Figure 10.1.
Lemma 10.3. Let E = E(x,A) be an ellipsoid and H = y : y,c b .
{ ⟨ ⟩ ≤ }
Let r = c⊤x−b and suppose that r [ 1,1]. Then,
∥c∥ A ∈ −d
(a) Gr¨otschel et al. 2012, Eq. 3.1.15–3.1.17: mvee(E H) = E(y,B)
∩
with
(1+dr)Ac d2(1 r2) 2(1+dr)Acc⊤A
y = x B = − A .
− (d+1) ∥c ∥A (cid:18) d2 −1 (cid:19)(cid:32) − (d+1)(1+r) ∥c ∥2 A(cid:33)
(b) Gr¨otschel et al. 2012, Lemma 3.2.21:
(1 dr)2
vol(mvee(E H)) exp − vol(E).
∩ ≤ − 5d
(cid:18) (cid:19)
The quantity r in Lemma 10.3 is a measure of the depth at which the
half-space H cuts the ellipsoid with r = 0 equivalent to the half-space
cutting the ellipsoid in two. Anything positive corresponds to cuttingELLIPSOID METHOD 96
Figure 10.1: The ellipsoid method in action. The shaded region is the
intersection of the half-space and the large circle and the ellipse is the
minimum volume ellipsoid containing this intersection created using the
formula in Lemma 10.3.
away more than half. Suppose that H = y : y z,c = 0 , then by
{ ⟨ − ⟩ }
Cauchy-Schwarz,
c,x z
r = ⟨ − ⟩ x z .
c ≥ −∥ − ∥A−1
∥ ∥A
This means that if H is a half-space passing through z E(x,(2d)−2A),
∈
then
1 1
vol(mvee(E H)) exp vol(E) 1 vol(E).
∩ ≤ −20d ≤ − 20d
(cid:18) (cid:19) (cid:18) (cid:19)
Proof of Theorem 10.2. Let x = argmin f(x) and δ = ε and
⋆ x∈K diam(K)
A = (1 δ)x +δK K.
⋆ ⋆
− ⊂
Suppose that x A . Then, x x ε and hence f(x) f(x )+ε
⋆ ⋆ ⋆
∈ ∥ − ∥ ≤ ≤
since f F . Furthermore,
l
∈
vol(A ) = δdvol(K).
⋆
Let H = x : g ,x z 0 where g ∂f(z ). By convexity of f, if
k k k k k
{ ⟨ − ⟩ ≤ } ∈ELLIPSOID METHOD 97
x Hc, then f(x) f(z ). By Lemma 10.3,
∈ k ≥ k
vol(K ) vol(E ) γk⋆−1vol(E ) γk⋆−1 δdvol(K).
k⋆
≤
k⋆
≤
1
≤ ≤
Hence there exists a k k and x A such that g ,x z 0, which
⋆ ⋆ k k
≤ ∈ ⟨ − ⟩ ≥
means that f(x )+ε f(x) f(z ).
⋆ k
≥ ≥
10.2 Separation with a surrogate
In this section we explain how to use the optimistic surrogate in place of
the subgradient oracle used by Algorithm 19. Let K be a convex body
and f F and x = argmin f(x). Throughout this section we
b,l,sc,sm ⋆ x∈K
∈
let
λr2 1 1
ρ = r = λ = . (10.1)
30d 2d dL2
We will assume we have been given an ellipsoid E = E(x,A) such that
E(x, 1 A) = E(x,ρdLA) K E(x,A).
120d3L ⊂ ⊂
Note that E does not need to be the minimum volume enclosing ellipsoid
of K. In order to drive the ellipsoid method you need to find a half-space
H such that
➳ H E(x,r2A) = ; and
∩ ̸ ∅
➳ x H.
⋆
∈
This can be done with a subgradient oracle for f but it can also be
found using the optimistic surrogate used in the Chapter 9 and studied
extensively Chapter 11. Let X have law (x,Σ) with Σ = ρA and
N
1 1
s(z) = E 1 f(X)+ f((1 λ)X +λz) .
− λ λ −
(cid:20)(cid:18) (cid:19) (cid:21)
1
q(z) = s′(x),z x + z x 2 .
⟨ − ⟩ 6 ∥ − ∥s′′(x)
Sadly there is no particular reason why H = y : s′(x),y x 0
{ ⟨ − ⟩ ≤ }
should include x , which by convexity would be true if we used f′(x)
⋆
instead of s′(x). Suppose, however, that we can find a z such that s(z)
≥ELLIPSOID METHOD 98
f(x ) and let H = y : s′(z),y z 0 . Then, by Lemma 11.1(b)(a),
⋆
{ ⟨ − ⟩ ≤ }
f(x ) s(x ) s(z)+ s′(z),x z f(x )+ s′(z),x z .
⋆ ⋆ ⋆ ⋆ ⋆
≥ ≥ ⟨ − ⟩ ≥ ⟨ − ⟩
Therefore s′(z),x z 0 and consequentially x H. We also want
⋆ ⋆
⟨ − ⟩ ≤ ∈
H E(x,r2A) = , which can be ensured by choosing z E(x,r2A).
∩ ̸ ∅ ∈
The following lemma shows that such a point exists.
Lemma 10.4. Let ∆ = E[f(X)] f(x ). Then,
⋆
−
r∆
max (q(z)+s(x)) f(x)+ .
z∈E(x,r2A) ≥ 2
Proof. We have
∆ = E[f(X)] f(x )
⋆
−
1
s(x) s(x )+ tr(s′′(x)Σ) (Proposition 11.5, Lemma 11.1)
⋆
≤ − λ
1
s′(x),x x + tr(s′′(x)Σ) (Convexity of s)
⋆
≤ ⟨ − ⟩ λ
d
s′(x) x x + s′′(x)
≤ ∥ ∥A∥ − ⋆ ∥A−1 λ ∥ ∥Σ
d
s′(x) + s′′(x) . (x E(x,A))
≤ ∥ ∥A λ ∥ ∥Σ ⋆ ∈
Furthermore,
1
max q(z) = max s′(x),z x + z x 2
z∈E(x,r2A) z∈E(x,r2A) ⟨ − ⟩ 6 ∥ − ∥s′′(x)
(cid:20) (cid:21)
(a)
r2
max r s′(x) , s′′(x)
≥ ∥ ∥A 6 ∥ ∥A
(cid:18) (cid:19)
(b) 4 r2
f(x) s(x) tr(s′′(x)Σ)+max r s′(x) , s′′(x)
≥ − − λ ∥ ∥A 6 ∥ ∥A
(cid:18) (cid:19)
(c) 4d r2
f(x) s(x) s′′(x) +max r s′(x) , s′′(x)
≥ − − λ ∥ ∥Σ ∥ ∥A 6ρ ∥ ∥Σ
(cid:18) (cid:19)
(d) d r
f(x) s(x)+ s′′(x) + s′(x) .
≥ − λ ∥ ∥Σ 2 ∥ ∥A
where in (a) we used the fact that s′′(x) 0 and various choices of
≻
z to maximise either the first or the second term. (b) follows fromELLIPSOID METHOD 99
Proposition 11.5. (c) since tr( ) d and since Σ = ρA. (d) follows
· ≤ ∥·∥
since max(a,b) (a+b)/2 and by our choice of the constants λ, ρ and
≥
r. Combining the two displays completes the proof.
The lemma shows there exists a z in E(x,r2A) such that a gradient
oracle of the surrogate at z will provide a suitable cutting plane. This
suggests a clear plan of attack to use the ellipsoid method for bandit
convex optimisation:
➳ Collect data to estimate the quadratic surrogate and maximise over
E(x,r2A) to find a critical point.
➳ Estimate the gradient of the surrogate the critical point and use it
to define a cutting plane.
➳ Combine the above with the ellipsoid method in Algorithm 19.
10.3 Finding a critical point
Lemma 10.4 established the existence of a z E(x,r2A) for which q(z)+
∈
s(x) f(x) + 1r∆. We now show how find a comparable point with
≥ 2
zeroth-order access to the loss function f. The idea is very simple. We
estimate the quadratic q and maximise the estimate. The errors are
controlled using the concentration of measure properties established in
Chapter 11. There are really only two details:
➳ The algorithm also needs to output a constant-factor approxima-
tion of the gap q(z)+s(x) f(x) to be used later, which means we
−
need a careful choice of stopping time.
➳ Whilethequadraticsurrogateq isconvex,itsestimateisonlynearly
so. Hence we do not maximise the estimate but a biased version
of it that is convex with high probability. Note that maximising
a convex function over a convex domain is not normally computa-
tionally feasible. Luckily the case where the objective is quadratic
and the constraint set is an ellipsoid works out nicely. Read all
about it in Note (c).
Theorem 10.5. Suppose that Algorithm 22 is run with the parameters
given in Eq. (10.1) and an ellipsoid E(x,A) such that E(x,ρdLA) K.
⊂ELLIPSOID METHOD 100
Then with probability at least 1 nδ the algorithm halts after at most
−
d5L4
O .
∆2
(cid:18) (cid:19)
queries to the loss function and outputs a point z E(x,r2A) and esti-
∈
mate θ such that
(a) s(z) f(x)+ r∆.
≥ 4
(b) s(z) f(x) [1θ,2θ].
− ∈ 2
1 args: r (0,1), ρ (0,1), λ (0,1), E(x,A)
∈ ∈ ∈
2 let Σ = ρA and D = √r
ρ
3 for t = 1 to
∞
4 if t is odd:
5 sample X from (x,Σ) and observe Y = f(X )+ε
t t t t
N
6 let g =
r(Xt,x)YtΣ−1(Xt−x)
t 1−λ
7 let H = λr(Xt,x)Yt Σ−1(Xt−x)(Xt−x)⊤Σ−1 Σ−1
t (1−λ)2 (1−λ)2 −
8 else : (cid:104) (cid:105)
9 play X = x and observe Y = f(x)+ε
t t t
10 let q¯(y) = 2 t 1(u odd) g ,y x + 1 y x 2
t t u=1 ⟨ u − ⟩ 6 ∥ − ∥Hu
11 let f¯ = 2 t 1(u even)Y
t t u(cid:80)=1 u(cid:2) (cid:3)
12 let z
t
= ar(cid:80)gmax z∈E(x,r2A)q¯ t(z)+ λ 6L d
t
∥z −x ∥2
Σ−1
13 let c = [1+D+(1+1/6)λD2]L (cid:113)d
t t
14 if q¯(z ) f¯ 2c : return z (cid:113)and q¯(z ) f¯
t t t t t t t t
− ≥ −
Algorithm 20: Algorithm for finding critical point
Proof. Note that for z E(x,r2A) we have
∈
r
z x .
∥ − ∥Σ−1 ≤ √ρELLIPSOID METHOD 101
By a union bound and Proposition 11.18, with probability at least 1
−
nδ/2 the following holds for all even 1 t n,
≤ ≤
2d
max q(y) q¯(y) D+λD2 L . (10.2)
t
y∈E(x,r2A)| − | ≤
(cid:114)
t
(cid:2) (cid:3)
Since the noise is subgaussian we also have with probability at least
1 nδ/2 that for all even 1 t n,
− ≤ ≤
2L
¯
f f(x) . (10.3)
t
− ≤ t
(cid:114)
(cid:12) (cid:12)
Another union bound sho(cid:12)ws that b(cid:12)oth Eq. (10.2) and Eq. (10.3) hold
with probability 1 nδ. We assume this event occurs for the remainder
−
of the proof. Suppose the algorithm stops after round t. Then,
θ
¯ t
θ = q¯(z ) f q(z ) f(x) c q(z ) f(x) ,
t t t t t t t
− ≥ − − ≥ − − 2
which implies that q(z ) f(x) 3θ. Reversing the argument yields
t − ≤ 2
q(z ) f(x) θ. Furthermore,
t − ≥ 2
¯
θ = q¯(z ) f
t t t t
−
¯
= max q¯(z) f
t t
z∈E(x,r2A) −
max q(z) f(x) c
t
≥ z∈E(x,r2A) − −
r∆
c (By Lemma 10.4)
t
≥ 2 −
r∆ θ
t
. (Algorithm has halted)
≥ 2 − 2
Finally, we need to show that the algorithm halts once t is large enough.
Concretely, once c r∆, then repeating the argument above to the
t ≤ 6
second last line shows that θ r∆ = 2c and hence the algorithm halts.
t ≥ 3 t
The claim follows from the definition of c .
tELLIPSOID METHOD 102
10.4 Estimating the cutting plane
Theorem 10.6. Suppose that Algorithm 21 is run with the parameters
given in Eq. (10.1) and an ellipsoid E(x,A) such that E(x,ρdLA) K.
⊂
Then with probability at least 1 (n + 1)δ the algorithm halts after at
−
most
d6L4
O .
∆2
(cid:18) (cid:19)
queries to the loss function and outputs a half-space H such that H
∩
E(x,r2A) is nonempty and x H.
⋆
∈
Proof. With probability at least 1 nδ,
−
(a) s(z) f(x)+ r∆.
≥ 4
(b) s(z) f(x) [θ,2θ].
− ∈ 2
Therefore, by convexity of s (Lemma 11.1(a)),
s′(z),x z s(x ) s(z) f(x) s(z).
⋆ ⋆
⟨ − ⟩ ≤ − ≤ −
On the other hand, by Proposition 11.21, with probability at least 1 δ,
−
1
g¯,x z s′(z),x z + x z L
⟨ ⋆ − ⟩ ≤ ⟨ ⋆ − ⟩ ∥ ⋆ − ∥Σ−1 m
(cid:114)
1
= s′(z),x z + x z L
⟨ ⋆ − ⟩ ∥ ⋆ − ∥A−1 mρ
(cid:114)
1
s′(z),x z +L
⋆
≤ ⟨ − ⟩ mρ
(cid:114)
θ
f(x) s(z)+
≤ − 2
0.
≤
10.5 Algorithm and analysis outlineELLIPSOID METHOD 103
1 args: r (0,1), ρ (0,1), λ (0,1), E(x,A)
∈ ∈ ∈
2 run Algorithm 20 and obtain z E(x,r2A) and θ > 0
∈
3 let m = 4L2 and Σ = ρA
ρθ2
4 for t = 1(cid:108)to(cid:109)m:
5 sample X (x,Σ) and observe Y = f(X )+ε
t t t t
∼ N
6 let g = r(X ,z)Y Σ−1 Xt−λz µ
t t t 1−λ −
7 let g¯ = 1 m g
m t=1 t (cid:0) (cid:1)
8 return H = y : g¯,y z 0
(cid:80)
{ ⟨ − ⟩ ≤ }
Algorithm 21: Algorithm for finding critical point
1 args: r (0,1), ρ (0,1), λ (0,1)
∈ ∈ ∈
2 let K = K
1
3 for k = 1 to :
∞
4 let E = mvee(K )
k k
5 get H from Algorithm 21 with input r,ρ,λ,E
k k
6 let K = K H
k+1 k k
∩
Algorithm 22: Ellipsoid method for bandit convex optimisation
Theorem 10.7. With probability at least 1 n2δ the regret of Algo-
−
rithm 22 is at most
Reg = O d4L2.5√n .
n
(cid:0) (cid:1)
Proof sketch. With high probability, the number of episodes is at most
O(d2L). Let n be the number of rounds in episode k. By Theorem 10.5,
k
with high probability
d6L4
n = O ,
k ∆2
(cid:18) k (cid:19)
where ∆ is the expected instantaneous regret incurred by each query in
kELLIPSOID METHOD 104
episode k. Therefore the regret in episode k is bounded by
n ∆ = O d3L2√n .
k k k
(cid:0) (cid:1)
With probability at least 1 δ,
−
n n
Reg = (f(X ) f(x )) (E [f(X )] f(x ))+√nL.
n t ⋆ t−1 t ⋆
− ≤ −
t=1 t=1
(cid:88) (cid:88)
Therefore,
kmax
Reg n ∆ +√nL
n k k
≤
k=1
(cid:88)
kmax
O d3L2√n +√nL
k
≤
k=1
(cid:88) (cid:0) (cid:1)
kmax
O d3L2 k n +√nL
≤  (cid:118) max k 
(cid:117) k=1
(cid:117) (cid:88)
= Od4L2.5(cid:116) √n . 
(cid:0) (cid:1)
10.6 Notes
(a) The ellipsoid method was introduced and refined by Shor [1977],
Yudin and Nemirovskii [1977, 1976] and then made famous by Khachiyan
[1979] who modified the method to show that linear programming is
polynomial time. The book by Gr¨otschel et al. [2012] covers everything
you need to know about the ellipsoid method.
(b) Algorithm 22 is inspired by Lattimore and Gy¨orgy [2021a], who use
thesamesurrogatelossincombinationwiththeellipsoidmethodtoprove
aboundontheregretofd4.5√n. ThemaindifferenceisthatAlgorithm22
findsthepointz bydirectlyoptimisingtheestimatedquadraticsurrogate,
while Lattimore and Gy¨orgy [2021a] use a random search procedure that
is slightly less efficient.ELLIPSOID METHOD 105
K
g¯
z
E(x,r2A)
Gaussian concentrated here
E(x,A)
Figure 10.2: The ellipsoid method in action. The outer ellipsoid is
E(x,A) and the inner one is E(x,r2A). The lightly shaded polytope is
the current constraint set and the more deeply shaded polytope is the
new one after a cut has been made. Note that K E(x,A) is necessary
⊂
but we do not demand that E(x,r2A) K. The algorithm is playing
⊂
with overwhelming probability in the region near the origin.
(c) Algorithm22needstomaximiseaconvexquadraticoveranellipsoid.
Generallyspeakingconvexmaximisationiscomputationallyhard, butfor
this special case you will find the method of Lagrange multipliers yields
a solution for which the computational complexity involves a line search
over a single Lagrange multiplier.
(d) For simplicity the algorithm finds the minimum volume ellipsoid
mvee(K H ). This is generally computationally hard, even when K
k k k
∩
is a polytope. Fortunately all that is required is to find an ellipsoid
E = E(x,A) such that E(x,ρdLA) K E(x,A) and this is computa-
⊂ ⊂
tionally efficient when K is given by a separation oracle as explained by
Gro¨tschel et al. [2012, Chapter 4] or Lattimore and Gy¨orgy [2021a] for
this specific application.
(e) Like Algorithm 18, the parameters in Algorithm 22 depend on large
unspecifiedlogarithmicfactors. Likethatalgorithm, youshouldprobably
just drop all logarithmic factors. Furthermore, exact confidence intervalsELLIPSOID METHOD 106
should be replaced by standard statistical tests.
(f) The ellipsoid method has a bad reputation from a practical perspec-
tive. The main problems are numerical instability (the ellipsoid becomes
poorly conditioned) and the fact that the convergence rate in practice
usually more-or-less matches the theory.
(g) Another standard cutting plane method for convex optimisation is
the center of gravity method, which starts with K = K and updates
1
K from K by finding x = 1 xdx and then using a gradient
k+1 k k vol(K k) K
k
oracle: K = x K : g ,x x 0 with g ∂f(x ). By
k+1
{ ∈
k
⟨
k −(cid:82)k
⟩ ≤ }
k
∈
k
Gru¨nbaum’s inequality [Artstein-Avidan et al., 2015], vol(K ) (1
k+1
≤ −
1/e)vol(K ). This means the center of gravity method requires a factor
k
of d fewer iterations than the ellipsoid method. A natural question is
whether or not the techniques in this chapter can be modified to use the
center of gravity method rather than the ellipsoid method.Chapter 11
Gaussian optimistic smoothing
The purpose of this chapter is to introduce and analyse the surrogate loss
functions used in Chapters 9 and 10. We try to make the results here
as general as possible so that you may use them in your own algorithms
and analysis. Throughout we make the following assumptions:
(a) The loss functions are twice differentiable; and
(b) Loss functions are in F with domain K.
b,sm,sc,e
Since the losses are in F we can and will assume that the losses are
e
defined on all of Rd. Note, however, that the extension only preserves
strong convexity and smoothness to Rd. Boundedness only holds on the
domain K.
Because the analysis is quite intricate and we are not so concerned by
constants and logarithmic factors, we let L be a large logarithmic factor.
Precisely,
L = C[1+logmax(n,d,β/α,diam(K),1/δ,lip (F))] ,
K
where C > 0 is a large non-specified universal positive constant. We also
let (C ) be a collection of k-dependent universal positive constants. In
k
case you want a quick summary of the results, read until the end of this
section for the basic definitions and then head directly to Section 11.9.
Suppose that X is a random vector in Rd. We are interested in the
problem of estimating the entire function f F from a single
b,sm,sc,e
∈
observation Y = f(X) + ε where ε 1. Given a parameter λ
∥ ∥ψ2 ≤ ∈
107GAUSSIAN OPTIMISTIC SMOOTHING 108
(0, 1 ), define the surrogate by
d+1
1 1
s(x) = E 1 f(X)+ f((1 λ)X +λx) . (11.1)
− λ λ −
(cid:20)(cid:18) (cid:19) (cid:21)
For the remainder we assume that the law of X is Gaussian with mean
µ and covariance Σ 0. The density of X with respect to the Lebesgue
≻
measure is
d
1 2 1
p(x) = √detΣ−1exp x µ 2 .
2π −2 ∥ − ∥Σ−1
(cid:18) (cid:19) (cid:18) (cid:19)
The quadratic surrogate is
1
q(x) = s′(µ),x µ + x µ 2 , (11.2)
⟨ − ⟩ 6 ∥ − ∥s′′(µ)
which is related to the second-order expansion of s at µ but the zeroth-
order term is dropped and the leading constant of the quadratic term is
1 rather than 1.
6 2
11.1 Elementary properties
An immediate consequence of the definitions is that s is convex and a
lower bound on f.
Lemma 11.1. The function s is infinitely differentiable and
(a) s is convex; and
(b) s(x) f(x) for all x Rd; and
≤ ∈
(c) λα s′′(x) λβ for all x.
≤ ∥ ∥ ≤
Proof. Part (a) is immediate from the convexity of f. Part (b) also uses
convexityoff andJensen’sinequality. Part(c)followsimmediatelyfrom
the definition of s and the convexity of norms.
Perhapsthemostimportantpropertyofsisthatitisnottoofarbelow
f on an ellipsoidal region about µ. Establishing this is quite involved,
however, and relies on a better understanding of the Hessian of s.GAUSSIAN OPTIMISTIC SMOOTHING 109
3
s(x),µ=1,Σ= 1 ,λ= 1
10 2
s(x),µ= 1,Σ=1,λ= 1
3 2
2 s(x),µ= 2,Σ=1,λ= 1
− 2
f(x)= x
| |
1
0
1
−
3 2 1 0 1 2 3
− − −
x
Figure 11.1: The figure plots the surrogate for different choices of µ
and Σ with λ = 1 in all figures. Notice that the surrogate is always
2
optimistic in the sense that s(x) f(x) for all x. The surrogate is a
≤
reasonable approximation of f on the confidence region µ √Σ. For the
±
blueandgreencurvestheapproximationisextremelygoodindeed, which
youshouldexpectbecausef isapproximatelylinearonthecorresponding
confidence regions.GAUSSIAN OPTIMISTIC SMOOTHING 110
3.0
s(x),µ=0,Σ=1,λ= 4
5
s(x),µ=0,Σ=1,λ= 1
2.5 2
s(x),µ=0,Σ=1,λ= 1
5
2.0 f(x)= x
| |
1.5
1.0
0.5
0.0
3 2 1 0 1 2 3
− − −
x
Figure 11.2: The figure plots the surrogate for µ and Σ constant and
different choices of λ. You can see that smaller λ yields a smoother
surrogate, but also one that has more approximation error.
11.2 Properties of the gradient
We have assumed that f F and dom(f) = K. This means that
b,sm,sc,e
∈
f is β-smooth on all of Rd. With this information we can control the
gradients of s in terms of those of f.
Proposition 11.2. lip (s) lip (f)+β dP(X / K) Σ .
K K
≤ ∈ ∥ ∥
Proof. Let d(x,K) = inf x y . Sinc(cid:112)e f is β-smooth, for any x
y∈K
∥ − ∥ ∈
K,
s′(x) = E[f′((1 λ)X +λx)]
∥ ∥ ∥ − ∥
lip (f)+βE[d ((1 λ)X +λx)]
K K
≤ −
lip (f)+βE[d (X)]
K K
≤
lip (f)+βE[1 (X) X µ ]
K Kc
≤ ∥ − ∥
lip (f)+β P(X / K)E[ X µ 2]
K
≤ ∈ ∥ − ∥
lip (f)+β(cid:112)dP(X / K) Σ .
K
≤ ∈ ∥ ∥
(cid:112)GAUSSIAN OPTIMISTIC SMOOTHING 111
Generally speaking we will be designing algorithms where P(X / K)
∈
is miniscule and in these cases the surrogate is nearly as Lipschitz as f.
11.3 Properties of the Hessian
The next important property is a kind of continuity of the Hessian.
Proposition 11.3. If λ x y L−1/2, then s′′(x) 3s′′(z).
∥ −
∥Σ−1
≤ ⪯
Proof. Let ε = λ(x−y) and assume, by changing coordinates, that µ = 0.
1−λ
By definition,
s′′(x) = λE[f′′((1 λ)X +λx)]
−
= λ f′′((1 λ)z +λx)p(z)dz
−
Rd
(cid:90)
λ(y x)
= λ f′′((1 λ)w+λy)p w+ − dw
− 1 λ
(cid:90)Rd
(cid:18) − (cid:19)
p w+ λ(y−x)
1−λ
= λ f′′((1 λ)w+λy) p(w)dw
− (cid:16) p(w) (cid:17)
Rd
(cid:90)
p(w+ε)
= λ f′′((1 λ)w+λy) p(w)dw, (11.3)
− p(w)
Rd
(cid:90)
The density ratio is
p(w+ε) 1 1
= exp w+ε 2 + w 2
p(w) −2 ∥ ∥Σ−1 2 ∥ ∥Σ−1
(cid:18) (cid:19)
1
= exp ε 2 w,ε .
−2 ∥ ∥Σ−1 −⟨ ⟩Σ−1
(cid:18) (cid:19)
Next, let A = w : 1 ε 2 w,ε log(2) . Decomposing the
−2 ∥ ∥Σ−1 −⟨ ⟩Σ−1 ≤
integral in Eq. (11.3) into the integral over A and Ac,
(cid:8) (cid:9)
λ f′′((1 λ)w+λy)p(w+ε)dw 2λ f′′((1 λ)w+λy)p(w)dw
− ≤ −
(cid:90)A (cid:90)A
2λ f′′((1 λ)w+λy)p(w)dw
≤ −
Rd
(cid:90)
= 2s′′(y).GAUSSIAN OPTIMISTIC SMOOTHING 112
Moving now to bound the integral over Ac:
λ f′′((1 λ)w+λy)p(w+ε)dw λβ p(w+ε)dw
− ≤
(cid:13) (cid:90)Ac (cid:13) (cid:90)Ac
(cid:13) = λβP(X ε Ac) (cid:13)
(cid:13) (cid:13)
(cid:13) − ∈ (cid:13)
1
= λβP ε 2 X ε,Σ−1ε > log(2)
−2 ∥ ∥Σ−1 − −
(cid:18) (cid:19)
log(2) (cid:10) 1 ε 2 2 (cid:11)
λβexp − 2 ∥ ∥Σ−1 .
≤ (cid:32)− (cid:0) 2 ∥ε ∥2 Σ−1 (cid:1) (cid:33)
Therefore,
τ 1 ε 2 2
s′′(x) 2s′′(y)+λβexp − 2 ∥ ∥Σ−1 1 3s′′(y),
⪯ (cid:32)− (cid:0) 2 ∥ε ∥2 Σ−1 (cid:1) (cid:33) ⪯
where the last inequality follows for large enough L since
λ 2 1
ε 2 = x y 2
∥ ∥Σ−1 1 λ ∥ − ∥Σ−1 ≤ L(1 λ)2
(cid:18) − (cid:19) −
and using from Lemma 11.1(c) that s′′(y) λα1.
⪰
11.4 Properties of the quadratic surrogate
Obviously q inherits convexity from s. Optimism is not inherited every-
where, but q is optimistic on a ellipsoidal region about µ, which follows
from the continuity properties of the Hessian of s and the definition of q.
Proposition 11.4. Suppose that λ x µ 1, then s(µ) s(x)
∥ − ∥Σ−1 ≤ L − ≤
q(µ) q(x).
−
Proof. That s f we established already in Lemma 11.1(b). For the
≤
first inequality, by Proposition 11.3, 3s′′(y) s′′(µ) for all y [µ,x].
⪰ ∈GAUSSIAN OPTIMISTIC SMOOTHING 113
Therefore, by Taylor’s theorem,
1
s(x) = s(µ)+ s′(µ),x µ + x µ 2
⟨ − ⟩ 2 ∥ − ∥s′′(y)
1
s(µ)+ s′(µ),x µ + x µ 2
≥ ⟨ − ⟩ 6 ∥ − ∥s′′(µ)
= s(µ)+q(x).
The result follows by rearranging and because q(µ) = 0.
11.5 Lower bound
We have shown that s f holds everywhere. We now show that s is
≤
reasonably close to f at µ.
Proposition 11.5. Provided that λ 1 ,
≤ dL2
4
f(µ) E[f(X)] s(µ)+ tr(s′′(µ)Σ).
≤ ≤ λ
Proof. The first inequality is immediate from Jensen’s inequality. Let Z
be a random variable that is independent of X and has law (µ,ρ2Σ)
N
where ρ2 = 2−λ is chosen so that (1 λ)2 +λ2ρ2 = 1. Then
λ −
1 1
E[s(Z)] = E 1 f(X)+ f((1 λ)X +λZ)
− λ λ −
(cid:20)(cid:18) (cid:19) (cid:21)
1 1
= E 1 f(X)+ f(X)
− λ λ
(cid:20)(cid:18) (cid:19) (cid:21)
= E[f(X)] ,
where we used the fact that (1 λ)X + λZ has the same law as X.
−
Let us now compare E[s(Z)] to s(µ). By Taylor’s theorem there exists a
ξ [µ,z] such that
z
∈
1
E[s(Z)] = E s(µ)+s′(µ)⊤(z µ)+ Z µ 2 .
− 2 ∥ − ∥s′′(ξZ)
(cid:20) (cid:21)
By Proposition 11.3, if z is close enough to µ, then s′′(z) is close to s′′(µ).GAUSSIAN OPTIMISTIC SMOOTHING 114
Define
A = z Rd : λ z µ L−1/2 .
∈ ∥ −
∥Σ−1
≤
(cid:8) (cid:9)
Note that µ A and that A is convex. Hence, if z A, then ξ A and
z
∈ ∈ ∈
s′′(ξ ) 3s′′(µ). Therefore
z
≤
1
E[s(Z)] = E s(µ)+s′(µ)⊤(Z µ)+ Z µ 2
− 2 ∥ − ∥s′′(ξZ)
(cid:20) (cid:21)
1
= s(µ)+E Z µ 2 Since E[Z] = µ
2 ∥ − ∥s′′(ξZ)
(cid:20) (cid:21)
3 1
s(µ)+ E Z µ 2 + E Z µ 2 1 (Z)
≤ 2 ∥ − ∥s′′(µ) 2 ∥ − ∥s′′(z) Ac
3ρ2(cid:104) (cid:105)1 (cid:104) (cid:105)
= s(µ)+ tr(s′′(µ)Σ)+ E Z µ 2 1 (Z) .
2 2 ∥ − ∥s′′(ξZ) Ac
(cid:104) (cid:105)
The error term is bounded by showing that P(Ac) is miniscule. Let
W = Σ−1/2(X µ) and bound
−
1 β
E Z µ 2 1 (Z) E Z µ 4 P(Z / A)
2 ∥ − ∥s′′(ξz) Ac ≤ 2 ∥ − ∥ ∈
(cid:20) (cid:21) (cid:113)
(cid:2) (cid:3)
β 1
= E Z µ 4 P W 2
2 (cid:115) ∥ − ∥ ∥ ∥ ≥ λ2ρ2L
(cid:18) (cid:19)
(cid:2) (cid:3)
ρ2βtr(Σ) 1
3P W 2
≤ 2 (cid:115) ∥ ∥ ≥ λ2ρ2L
(cid:18) (cid:19)
βtr(Σ) 1
3P W 2 ,
≤ λ (cid:115) ∥ ∥ ≥ λ2ρ2L
(cid:18) (cid:19)
where in the second inequality we used Proposition B.2 and the final
inequalityfollowsfromthedefinitionofρ2 = 2−λ. Finally,byLemmaC.3,
λ
W 2 3d and by Lemma C.1,
∥ ∥ ψ1 ≤
(cid:13) (cid:13)
(cid:13) (cid:13) 1 1 1
P W 2 exp exp .
∥ ∥ ≥ λ2ρ2L ≤ −3dλ2ρ2L ≤ −6dλL
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)GAUSSIAN OPTIMISTIC SMOOTHING 115
Combining everything and naively simplifying shows that
3 3βtr(Σ) L
E[f(X)] s(µ)+ tr(s′′(µ)Σ)+ exp
≤ λ λ (cid:115) −6
(cid:18) (cid:19)
3
s(µ)+ tr(s′′(µ)Σ)+αtr(Σ)
≤ λ
4
s(µ)+ tr(s′′(µ)Σ).
≤ λ
11.6 Estimation
The surrogate function can be estimated from X and Y using a change
of measure. Precisely,
1 1
s(z) = 1 f(x)+ f((1 λ)x+λz) p(x)dx
− λ λ −
(cid:90)Rd(cid:18)(cid:18)
(cid:19) (cid:19)
1 p x−λz
= 1 + 1−λ f(x)p(x)dx
− λ λ(1 λ)dp(x)
Rd(cid:32) (cid:0) (cid:1) (cid:33)
(cid:90) −
1 r(x,z)
= 1 + f(x)p(x)dx, (11.4)
− λ λ
(cid:90)Rd(cid:18)
(cid:19)
where r(z) is the change of measure defined by
1 d p x−λz
r(x,z) = 1−λ . (11.5)
1 λ p(x)
(cid:18) − (cid:19) (cid:0) (cid:1)
From Eq. (11.4) it follows that an unbiased estimator of s(z) using the
data (X,Y) is
1 r(X,z)
sˆ(z) = 1 + Y .
− λ λ
(cid:18) (cid:19)
Proposition 11.6. E[sˆ(z)] = s(z).
Note that while s is convex, x sˆ(x) need not be (see Figure 11.3).
(cid:55)→
One may also want to estimate the gradient and Hessian of the surrogate.GAUSSIAN OPTIMISTIC SMOOTHING 116
Natural unbiased estimates are
X λz
sˆ′(z) = r(X,z)YΣ−1 − µ .
1 λ −
(cid:18) − (cid:19)
λr(X,z)Y X λz X λz ⊤
sˆ′′(z) = Σ−1 − µ − µ Σ−1 Σ−1 .
(1 λ)2 1 λ − 1 λ − −
(cid:34) (cid:35)
− (cid:18) − (cid:19)(cid:18) − (cid:19)
Generally speaking we are most interested in estimating gradients and
Hessians of the surrogate at µ, which satisfy
r(X,µ)YΣ−1(X µ)
sˆ′(µ) = − .
1 λ
−
λr(X,µ)Y Σ−1(X µ)(X µ)⊤Σ−1
sˆ′′(µ) = − − Σ−1 .
(1 λ)2 (1 λ)2 −
− (cid:20) − (cid:21)
Proposition 11.7. The following hold:
(a) E[sˆ′(z)] = s′(z); and
(b) E[sˆ′′(z)] = s′′(z).
TheidentityfortheunbiasedestimatoroftheHessianofthesurrogate
also gives us an opportunity to bound the magnitude of the Hessian. In
general the basic identity in Lemma 11.1(c) is not improvable. But a
bound in terms of the covariance is possible:
Proposition 11.8. Σ1/2s′′(µ)Σ1/2 4λd E[f((1 λ)X +λµ)2].
≤ −
Proof. Assume witho(cid:13)ut loss of gener(cid:13)ality th(cid:112)at µ = 0. Then,
(cid:13) (cid:13)
λr(x,µ)f(x) Σ−1/2xx⊤Σ−1/2
Σ1/2s′′(µ)Σ1/2 = 1 p(x)dx
(1 λ)2 (1 λ)2 −
(cid:13)(cid:90)Rd
− (cid:20) − (cid:21) (cid:13)
(cid:13) (cid:13) (cid:13) λf((1 λ)x) (cid:13)
(cid:13) (cid:13) = (cid:13) − Σ−1/2xx⊤Σ−1/2 1 p(x)dx(cid:13)
(cid:13) (cid:13)
(1 λ)2 −
(cid:13)(cid:90)Rd
− (cid:13)
(cid:13) λf((1 λ)x) (cid:2) (cid:3) (cid:13)
(cid:13) − Σ−1/2xx⊤Σ−1/2 1 p(x)dx(cid:13)
(cid:13) (cid:13)
≤ (1 λ)2 −
(cid:90)Rd(cid:12)
− (cid:12)
(cid:12) (cid:12)(cid:13) (cid:13)
λ
(cid:12)
(cid:12)
E[f((1(cid:12) (cid:12)(cid:13) λ)X)2]E WW⊤ (cid:13)1 2
≤ (1 λ)2 − ∥ − ∥
− (cid:114) (cid:104) (cid:105)
λ
= (d2 +1)E[f((1 λ)X)2],
(1 λ)2 −
−
(cid:112)GAUSSIAN OPTIMISTIC SMOOTHING 117
where in the first inequality we used convexity of the spectral norm (all
norms), in the second we used Cauchy-Schwarz and the last follows from
Proposition B.2 and because WW⊤ 1 2 = ( W 2 1)2.
− ∥ ∥ −
(cid:13) (cid:13)
(cid:13) (cid:13)
11.7 Concentration ( )
In this section we explore the tail behaviour of the estimators in the pre-
vious section. Almost all of the results here are only used as technical
lemmas in the next section and the proofs are endless applications of
Cauchy-Schwarz and the tail bounds in Appendix C. The only funda-
mental result is Lemma 11.15, so if you only want to read one proof,
read that one. We begin with a simple lemma:
Lemma11.9. Forallx,z Rd, r(x,z) 3exp λ x µ,z µ .
∈ ≤ (1−λ)2 ⟨ − − ⟩Σ−1
Proof. Letusassumewithoutlossofgeneralityth(cid:0)atµ = 0. Bydefinition,(cid:1)
p x−λz
r(x,z) = 1−λ
(1 λ)dp(x)
(cid:0) (cid:1)
−
1 1 x λz 2 1
= exp − + x 2
(1 λ)d −2 1 λ 2 ∥ ∥Σ−1
− (cid:32) (cid:13) − (cid:13)Σ−1 (cid:33)
(cid:13) (cid:13)
1 x λz(cid:13) 2 (cid:13)1
3exp − (cid:13) +(cid:13) x 2
≤ −2 1 λ 2 ∥ ∥Σ−1
(cid:32) (cid:13) − (cid:13)Σ−1 (cid:33)
(cid:13) (cid:13)
λ x(cid:13),z (cid:13)
3exp ⟨ (cid:13) ⟩Σ−1 (cid:13).
≤ (1 λ)2
(cid:18) − (cid:19)
The next lemma shows that z r(x,z) has a bounded gradient close
(cid:55)→
to µ.
Lemma 11.10. Let r(z) = r(x,z) and suppose that
(a) λ z µ 1/√dL.
∥ −
∥Σ−1
≤
(b) x µ √dL.
∥ −
∥Σ−1
≤
Then r′(z) Cλ Σ−1 dL.
∥ ∥ ≤ ∥ ∥
(cid:112)GAUSSIAN OPTIMISTIC SMOOTHING 118
Proof. Assume without loss of generality that µ = 0. Then,
λr(z)
r′(z) = Σ−1(x λz)
∥ ∥ (1 λ)2 −
−
λr(z) (cid:13) (cid:13)
(cid:13) Σ−1 x (cid:13)λz
≤ (1 λ)2 ∥ ∥∥ − ∥Σ−1
−
(cid:112)
Cλ Σ−1 dL.
≤ ∥ ∥
(cid:112)
Later we need some conservative upper bounds on the moments of
the various estimators. The easiest way to obtain these is to bound
the moments of the constituent parts and combine them using Ho¨lder’s
inequality.
Lemma 11.11. For any k 1, E[r(X,x)k] C exp C λ2 x µ 2 .
≥ ≤
k k
∥ −
∥Σ−1
Proof. By Lemma 11.9 and Proposition B.2, (cid:0) (cid:1)
1 dk λk
E[r(X,x)k] 3k E exp X µ,x µ
≤ 1 λ (1 λ)2 ⟨ − − ⟩Σ−1
(cid:18) − (cid:19) (cid:20) (cid:18) − (cid:19)(cid:21)
1 dk λ2k2
= 3k exp x µ 2
1 λ 2(1 λ)4 ∥ − ∥Σ−1
(cid:18) − (cid:19) (cid:18) − (cid:19)
C exp C λ2 x µ 2 .
≤
k k
∥ −
∥Σ−1
(cid:0) (cid:1)
Lemma 11.12. Suppose that µ K. Then, for any k 1,
∈ ≥
E[ f(X) k] C k 1+dk 2 lip K(f) Σ k 2 +dk Σ k .
| | ≤ ∥ ∥ ∥ ∥
(cid:104) (cid:105)
Proof. Since µ K and f F , f(µ) 1. Since f is β-smooth on Rd,
b
∈ ∈ | | ≤
β k
E[ f(X) k] E 1+lip (f) X µ + X µ 2
| | ≤ K ∥ − ∥ 2 ∥ − ∥
(cid:34) (cid:35)
(cid:18) (cid:19)
kβ
k +klip (f)kE[ X µ k]+ X µ 2k
≤ K ∥ − ∥ 2 ∥ − ∥
C k 1+dk 2 lip K(f)k Σ k 2 +dk Σ k ,
≤ ∥ ∥ ∥ ∥
(cid:104) (cid:105)
where we used Lemmas C.3 and C.2 to bound the moments of W .
∥ ∥
Remark 11.13. The same bound holds for E[ f((1 λ)X +λµ) k].
| − |GAUSSIAN OPTIMISTIC SMOOTHING 119
Lemma 11.14. Suppose that µ K and λ x µ 1. Then
∈ ∥ −
∥Σ−1
≤
(a) E[ sˆ(x) 2] C kλ−k 1+dk 2 f′(µ) k 2 Σ k 2 +βkdk Σ k .
| | ≤ ∥ ∥ ∥ ∥ ∥ ∥
(b) E ∥sˆ′(x) ∥2 Σ ≤ C k (cid:104) dk 2 +dk ∥f′(µ) ∥k 2 ∥Σ ∥k 2 +βkd3 2k ∥Σ ∥(cid:105) k .
(c) E(cid:2) ∥sˆ′′(x) ∥2 Σ(cid:3)
≤
C k(cid:104) λk dk +d3 2k ∥f′(µ) ∥k 2 ∥Σ ∥k 2 +βkd2k ∥Σ(cid:105) ∥k .
(d) E(cid:2) [qˆ(x)2] C(cid:3) kλ−k dk(cid:104) +d3 2k f′(µ) k 2 Σ k 2 +βkd2k Σ k . (cid:105)
≤ ∥ ∥ ∥ ∥ ∥ ∥
(cid:104) (cid:105)
Proof. Parts(a)–(c)followfromLemmas11.9, 11.11and11.12andthat
for any random variables U ,...,U , E[ m U k] m E[ U km]1/m.
1 m | i=1 m | ≤ i=1 | |
Besides this you will need many tedious applications of Lemma C.2 and
(cid:81) (cid:81)
C.3 to bounds moments of W . For part (d) note that
∥ ∥
1
qˆ(x) = sˆ′(µ),x µ + x µ 2
| | ⟨ − ⟩ 6 ∥ − ∥sˆ′′(µ)
(cid:12) (cid:12)
(cid:12) 1 (cid:12)
(cid:12)sˆ′(µ) x µ + sˆ′′(µ)(cid:12) x µ 2 .
≤ (cid:12) ∥ ∥Σ∥ − ∥Σ−1 6 ∥ (cid:12) ∥Σ∥ − ∥Σ−1
Then apply parts (b) and (c).
Lemma 11.15. P sˆ′(µ) 2 dLY2 δ.
∥ ∥Σ ≥ ≤
Proof. By Lemmas(cid:0)C.3 and C.1, (cid:1)
8dlog(4/δ)
P X µ 2 δ.
∥ − ∥Σ−1 ≥ 3 ≤
(cid:18) (cid:19)
The result follows since r(X,µ) 3 by Lemma 11.9 and because
≤
r(X,µ)2Y2
sˆ′(µ) 2 = X µ 2 .
∥ ∥Σ (1 λ)2 ∥ − ∥Σ−1
−
11.8 Concentration continued
We now focus on the sequential aspects of concentration. Let f ,...,f
1 n
be a sequence of convex functions satisfying the same conditions as f.
Assume that X ,Y ,...,X ,Y are the sequence of action/losses gener-
1 1 n n
ated by an algorithm interacting with a convex bandit, which is adapted
as usual to the filtration (F )n . Let τ be a stopping time with respect
t t=1
to the filtration (F ).
tGAUSSIAN OPTIMISTIC SMOOTHING 120
Assumption 11.16. The following hold:
(a) The conditional law of X given F is (µ ,Σ ).
t t−1 t t
N
(b) Almost surely for all t τ,
≤
Σ−1 1 Σ−1 Σ−1 1,
min ⪯ t ⪯ max
where Σ−1 and Σ−1 are constant positive reals.
min max
(c) There exists a constant Y such that P ( Y Y ) exp( L)
max t−1 t max
| | ≥ ≤ −
almost surely for all t τ.
≤
The surrogate function and its quadratic approximation now change
from round to round and are given by
1 1
s (z) = E 1 f (X )+ f ((1 λ)X +λz) .
t t−1 t t t t
− λ λ −
(cid:20)(cid:18) (cid:19) (cid:21)
1
q (z) = s′(µ ),z µ + z µ 2 .
t ⟨ t t − t ⟩ 6 ∥ − t ∥s′ t′(µt)
Note that even when f = f is unchanging, the surrogate depends on µ
t t
and Σ and may still change from round to round. Let p be the density
t t
of (µ ,Σ ). The estimator of s is
t t t
N
1 R (z) 1 d p Xt−λz
sˆ(z) = 1 + t Y R (z) = t 1−λ .
t t t
− λ λ 1 λ p (X )
(cid:18) (cid:19) (cid:18) − (cid:19) (cid:0)t t (cid:1)
Throughout we let g = sˆ′(µ ) and H = sˆ′′(µ ) and g¯ = s′(µ ) and
t t t t t t t t t
H¯ = s′′(µ ), which means an estimator of the quadratic surrogate is
t t t
1
qˆ(x) = g ,x µ + x µ 2 .
t ⟨ t − t ⟩ 6 ∥ − t ∥Ht
Objectives and plan The questions in this section concern concen-
tration of quantities like τ (sˆ s ). This is an entire function, so we
t=1 t − t
need to be precise about what is meant by concentration. Typical results
(cid:80)
show that functions like this are small at a specific x or for all x in some
set. The magnitude of the errors typically depends on the cumulative
predictable variation and our bounds reflect that. The change of mea-
sureR (x)thatappearsinthedefinitionoftheestimatorsiswell-behaved
t
when x is close enough to µ. Because of this most of the concentrationGAUSSIAN OPTIMISTIC SMOOTHING 121
boundsthatfollowonlyholdonasubsetofK. Anillustrativeexperiment
is given in Figure 11.3. Given r > 0, let
K (r) = x K : max λ x µ r ,
τ
∈ 1≤t≤τ ∥ −
t ∥Σ− t1
≤
(cid:26) (cid:27)
which is an intersection of ellipsoids and hence convex. The general
flavour of the results is as follows:
➳ Given a deterministic x, τ sˆ(x) is well-concentrated about its
t=1 t
mean provided that x K (1/√2L) almost surely.
τ
∈ (cid:80)
➳ The entire function τ sˆ(x) is well-concentrated about its mean
t=1 t
on K (1/√2dL).
τ
(cid:80)
➳ The function τ qˆ is well-concentrated about its mean with the
t=1 t
deviation at x depending on the radius r such that x K (r).
τ
(cid:80) ∈
The predictable variation of the estimators is mostly caused by the vari-
ance in the losses. Let V = τ E [Y2]. Generally speaking the
τ t=1 t−1 t
losses (Y ) will be bounded in [0,1] with overwhelming probability and
t
(cid:80)
in this case V = O(n). Our concentration bounds will be established
τ
using Freedman’s martingale version of Bernstein’s inequality, which is
a variance aware concentration inequality. A technical challenge is that
none of the estimators are bounded, which we circumvent by using a
truncation argument.GAUSSIAN OPTIMISTIC SMOOTHING 122
10 f(x)= x
| |
s(x)
8 n1 n t=1sˆ t(x)
P
6
4
2
0
10.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5 10.0
− − − −
x
Figure 11.3: The concentration of 1 n sˆ(x) with f = f = and
n t=1 t t |·|
n = 104 and µ = 1 and Σ = 1 and λ = 1. The thin lines correspond to
2 (cid:80)2
the first one hundred estimated surrogates. The estimate is very close to
the real surrogate on an interval around µ but can be extremely poorly
behaved far away, even with n so large. Note also that the estimated
surrogate is convex near µ but not everywhere.
Concentration bounds We start with a Bernstein-like concentration
bound for the sum of the surrogate loss estimators:
Proposition 11.17. The following hold:
(a) Let x Rd is a non-random vector such that x K (1/√2L) almost
τ
∈ ∈
surely. Then, with probability at least 1 δ,
−
τ
1
(sˆ(x) s (x)) 1+ LV +LY .
t t τ max
(cid:12) − (cid:12) ≤ λ
(cid:12)(cid:88)t=1 (cid:12) (cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
(b) With pr(cid:12)obability at least 1(cid:12) δ,
−
τ
1 dLY
max
max sˆ(x) s (x) dLV + .
x∈Kτ(1/√ 2dL)(cid:12)
(cid:12)(cid:88)t=1
t − t (cid:12)
(cid:12)
≤ λ
(cid:112)
τ λ
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)GAUSSIAN OPTIMISTIC SMOOTHING 123
Note the different radii in (a) and (b) that appear in K and also the
τ
extra √d factor that appears in (b). You should view these bounds as a
kind of Bernstein inequality with the term involving Y the lower-order
max
term and V = O(τ) with high probability.
τ
Proof. Thesingle-pointboundin(a)followsfromatruncationargument
followed by a standard application of martingale concentration. The
uniform bound in(b) more-or-less follows from a union bound argument
in combination with the analysis in part (a).
Proof of part (a) Let x Rd be such that x K (1/√2L) almost
τ
∈ ∈
surely and recall that
R (x) 1
t
sˆ(x) = 1+ − Y .
t t
λ
(cid:18) (cid:19)
Let E = R (x) 3exp(4), Y Y . By Lemma 11.9,
t t t max
{ ≤ | | ≤ }
0 R (x) 3exp 4λ X µ ,x µ ≜ 3exp(4Z) ,
≤
t
≤ ⟨
t
−
t
−
t ⟩Σ− t1
(cid:16) (cid:17)
where the last inequality serves as the definition of Z, which under P
t−1
has law (0,λ2 x µ 2 ). By assumption λ x µ 1/√2L and
N ∥ − t ∥Σ− t1 ∥ − t ∥Σ− t1 ≤
by Theorem C.5,
P(R (x) 3exp(4)) P(Z 1) exp( L).
t
≥ ≤ ≥ ≤ −
By assumption, P ( Y Y ) exp( L) and hence a union bound
t−1 t max
| | ≥ ≤ −
shows that P(Ec) 2exp( L) and by Cauchy-Schwarz,
t ≤ −
s (x) E [1 sˆ(x)] E [sˆ(x)2]P (Ec)
|
t
−
t−1 Et t
| ≤
t−1 t t−1 t
(cid:112)2exp( L)E t−1[sˆ t(x)2]
≤ −
1
(cid:112), (11.6)
≤ n
where in the final inequality we used Lemma 11.14(a) and made sure to
chooseLlargeenough. TheplanisnowtoapplyFreedman’sinequalityto
the sequence (1 sˆ(x))τ for which we need boundedness and a second
Et t t=1GAUSSIAN OPTIMISTIC SMOOTHING 124
moment bound. By the definition of E ,
t
R (x) 1 3exp(4)Y
t max
1 sˆ(x) = 1 Y 1+ − .
|
Et t
|
Et t
λ ≤ λ
(cid:12) (cid:18) (cid:19)(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Furthermore,
(cid:12) (cid:12)
R (x) 1 2 9exp(8)
E [1 sˆ(x)2] = E 1 Y2 1+ t − E [Y2].
t−1 Et t t−1 Et t λ ≤ λ2 t−1 t
(cid:34) (cid:35)
(cid:18) (cid:19)
Combining Eq. (11.6) and Eq. (11.7) with Theorem C.7 shows that with
probability at least 1 δ,
− 2
τ τ
(1 sˆ(x) s (x)) 1+ 1 sˆ(x) E [1 sˆ(x)]
(cid:12)
Et t
−
t
(cid:12) ≤ (cid:12)
Et t
−
t−1 Et
(cid:12)
(cid:12)(cid:88)t=1 (cid:12) (cid:12)(cid:88)t=1
(cid:0)
(cid:1)(cid:12)
(cid:12) (cid:12) (cid:12)1 (cid:12)
(cid:12) (cid:12) 1+(cid:12) LV +LY . (cid:12)
(cid:12) (cid:12)
≤
(cid:12)λ τ max (cid:12)
(cid:104)(cid:112) (cid:105)
Since P (Ec) 2exp( L) 1 , a union bound shows that with
t−1 t ≤ − ≤ 2nδ
probability at least 1 δ,
− 2
τ τ
(sˆ(x) s (x)) = (1 sˆ(x) s (x)) (11.7)
t
−
t Et t
−
t
t=1 t=1
(cid:88) (cid:88)
Part (a) is now established by combining the above two displays with a
union bound.
Proof of part (b) The argument follows along the same lines as
part (a) but now we need an additional covering and Lipschitz argu-
ment. Let be a finite cover of K such that for all y K there exists
C ∈
an x such that x y ε with
∈ C ∥ − ∥ ≤
−1
ε = max λ Σ−1 , Cn(1+√L) Σ−1 dL . (11.8)
max max
(cid:104) (cid:16) (cid:112) (cid:112) (cid:17)(cid:105)
By [Vershynin, 2018, Corollary 4.2.13], can be chosen so that
C
2diam(K) d
+1 ,
|C| ≤ ε
(cid:18) (cid:19)GAUSSIAN OPTIMISTIC SMOOTHING 125
which means that log dL. Given a non-empty set A Rd let
|C| ≤ ⊂
d(x,A) = inf x y and for x let τ = min t τ : d(x,K ) >
y∈A x t
∥ − ∥ ∈ C { ≤
ε . Let E be the event
t
}
E = X µ √2dL, Y Y .
t
∥
t
−
t ∥Σ− t1
≤ |
t
| ≤
max
(cid:110) (cid:111)
Likeintheproofofpart(a),R (x) 3exp(4)whenever X µ ,x µ
t
≤ ⟨
t
−
t
−
t ⟩Σ− t1
≤
1. Furthermore,
1
max X µ ,x µ X µ .
x∈Kt(1/√ 2dL)⟨ t − t − t ⟩Σ− t1 ≤ √2dL∥ t − t ∥Σ− t1
Therefore,
1 max R (x) 3exp(4).
Et √ t
≤
x∈Kt(1/ 2dL)
Repeating the argument in Part (a) along with a union bound over
C
shows that
τx
1 δ
P max (sˆ(x) s (x)) 1+ dV L+dLY .
t t τ max
(cid:32) x∈C (cid:12) − (cid:12) ≥ λ (cid:33) ≤ 4
(cid:12)(cid:88)t=1 (cid:12) (cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
Recall the following:
(cid:12) (cid:12)
➳ By assumption: P( Y Y ) exp( L); and
t max
| | ≥ ≤ −
➳ By Lemma C.3: X µ 8d/3.
∥∥
t
−
t ∥Σ− t1 ∥t−1,ψ2
≤
Therefore, by a union bound and Lemma C.1, t(cid:112)he following all hold with
probability at least 1 δ:
−
(a) max τx (sˆ(x) s (x)) 1+ 1 √dV L+dLY .
x∈C | t=1 t − t | ≤ λ τ max
(b) max Y Y .
1≤t≤n(cid:80) t max (cid:2) (cid:3)
| | ≤
(c) max X µ √dL.
1≤t≤n
∥
t
−
t ∥Σ− t1
≤
Assume for the remainder that all of (a)-(c) above hold and let y K .
τ
∈
By the construction of there exists an x such that x y ε
C ∈ C ∥ − ∥ ≤GAUSSIAN OPTIMISTIC SMOOTHING 126
and therefore d(x,K ) ε and τ = τ. Since y K ,
τ x τ
≤ ∈
λ x µ λ x y +λ y µ
∥ −
t ∥Σ− t1
≤ ∥ −
∥Σ− t1
∥ −
t ∥Σ− t1
1
λε Σ−1 +
≤ max √2L
√d(cid:112)
L,
≤
where we used the definition of K , the triangle inequality and the defi-
τ
nition of ε in Eq. (11.8) and naive bounding. Next, since y K ,
τ
∈
λ X µ ,y µ λ X µ y µ
⟨
t
−
t
−
t ⟩Σ− t1
≤ ∥
t
−
t ∥Σ− t1
∥ −
t ∥Σ− t1
λ√dL y µ
≤ ∥ −
t ∥Σ− t1
1,
≤
where we used Cauchy-Schwarz, item (c) above and the definition of K .
τ
Therefore
λ X µ ,x µ 1+λ X µ ,x y
⟨
t
−
t
−
t ⟩Σ− t1
≤ ⟨
t
−
t
−
⟩Σ− t1
1+λ X µ x y Σ−1
≤ ∥
t
−
t ∥Σ− t1
∥ − ∥
t
(cid:113)
1+ελ dL Σ−1 (cid:13) (cid:13)
t (cid:13) (cid:13)
≤
2, (cid:113) (cid:13) (cid:13)
≤ (cid:13) (cid:13)
where we used the fact that y K K and that x y ε. Com-
τ t
∈ ⊂ ∥ − ∥ ≤
bining the above with item (c) provides the conditions for Lemma 11.10,
which shows that
1
sˆ(x) sˆ(y) lip (sˆ) x y εlip (sˆ) .
| t − t | ≤ [x,y] t ∥ − ∥ ≤ [x,y] t ≤ n
where we used item (b) and the definition of ε in Eq. (11.8). By Propo-
sition 11.2, for any z [x,y],
∈
lip (s ) lip (f )+β d Σ .
K t K t t
≤ ∥ ∥
(cid:112)GAUSSIAN OPTIMISTIC SMOOTHING 127
Therefore, using the definition of ε,
τ τ
(sˆ(y) s (y)) 1+ (sˆ(x) s (x))
t t t t
(cid:12) − (cid:12) ≤ (cid:12) − (cid:12)
(cid:12)(cid:88)t=1 (cid:12) (cid:12)(cid:88)t=1 (cid:12)
(cid:12) (cid:12) (cid:12)1 (cid:12)
(cid:12) (cid:12) 2+(cid:12) dV L+dLY (cid:12) .
(cid:12) (cid:12)
≤
(cid:12)λ τ m(cid:12)ax
(cid:104)(cid:112) (cid:105)
More or less the same result holds for the quadratic surrogates esti-
mates.
Proposition 11.18. Let r [0,1]. The following hold:
∈
(a) Let x Rd be a non-random vector such that x K (r) almost
τ
∈ ∈
surely. Then, with probability at least 1 δ,
−
τ rL2
q (x) qˆ(x) V +Y .
t t τ max
(cid:12) − (cid:12) ≤ λ
(cid:12)(cid:88)t=1 (cid:12) (cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
(b) With proba(cid:12)bility at least 1 (cid:12)δ,
−
τ rL2
sup q (x) qˆ(x) dV +dY .
t t τ max
x∈Kτ(r)(cid:12)
(cid:12)(cid:88)t=1
− (cid:12)
(cid:12)
≤ λ
(cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
Notice that wi(cid:12)th r = 1 we see a(cid:12)bout the same uniform concentration
bound for the quadratic surrogate as for the non-quadratic version. The
difference is that for smaller r the concentration behaviour is better.
Unsurprisingly, in the extreme case that r = 0, then q (µ ) = qˆ(µ ) = 0
t t t t
almost surely.
Proof. Let x Rd be a non-random vector such that x K (r) almost
τ
∈ ∈
surely. Remember that by definition
1
qˆ(x) = g ,x µ + x µ 2 .
t ⟨ t − t ⟩ 6 ∥ − t ∥Ht
The first term is
R (µ )Y x µ ,X µ
g ,x µ =
t t t
⟨ −
t t
−
t ⟩Σ− t1
t t
⟨ − ⟩ 1 λ
−GAUSSIAN OPTIMISTIC SMOOTHING 128
The second is
1 x µ 2 = λR t(µ t)Y t ⟨x −µ t,X t −µ t ⟩2 Σ− t1 x µ 2 .
6 ∥ − t ∥Ht 6(1 λ)2
(cid:34)
(1 λ)2 −∥ − t ∥Σ− t1
(cid:35)
− −
Note that x µ ,X µ has law under P of (0, x µ ).
⟨ −
t t
−
t ⟩Σ− t1 t−1
N ∥ −
t ∥Σ− t1
Therefore,
δ
P x µ ,X µ x µ √L .
t−1
⟨ −
t t
−
t ⟩Σ− t1
≥ ∥ −
t ∥Σ− t1
≤ 2n
(cid:16)(cid:12) (cid:12) (cid:17)
(cid:12) (cid:12)
Define an event
(cid:12) (cid:12)
E = Y Y and x µ ,X µ x µ √L .
t
|
t
| ≤
max
⟨ −
t t
−
t ⟩Σ− t1
≤ ∥ −
t ∥Σ− t1
(cid:110) (cid:12) (cid:12) (cid:111)
By a union bound P( n E(cid:12) ) 1 2nexp( L(cid:12) ). By Lemma 11.14 and
∩t=1(cid:12) t
≥ − −
(cid:12)
using the fact that x K (r) K (r) for t τ,
τ t
∈ ⊂ ≤
1
E [qˆ(x)1 ] E [qˆ(x)2]P (Ec) .
t−1 t E tc ≤ t−1 t t−1 t ≤ n
(cid:112)
Furthermore, using the fact that r [0,1],
∈
rCL Y rCLY
t max
1 qˆ(x) | | .
Et| t
| ≤ λ ≤ λ
Therefore, by Theorem C.7, with probability at least 1 δ,
−
τ rL2
(qˆ(x) q (x)) V +Y .
t t τ max
(cid:12) − (cid:12) ≤ λ
(cid:12)(cid:88)t=1 (cid:12) (cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
Repeating more-or-less the same argument the covering argument in
(cid:12) (cid:12)
Proposition 11.17 shows that with probability at least 1 δ,
−
τ rL2
sup (qˆ(x) q (x)) dV +dY .
t t τ max
x∈Kτ(r)(cid:12)
(cid:12)(cid:88)t=1
− (cid:12)
(cid:12)
≤ λ
(cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
In the analysis of online Newton step we also make use of concentra-
tion of the Hessian estimates of the surrogate, which we now establish.GAUSSIAN OPTIMISTIC SMOOTHING 129
Proposition 11.19. Let S Sd be the (random) set of positive definite
⊂ +
matrices such that Σ−1 Σ−1 for all t τ and S (x) = t sˆ′′(x) and
S¯ (x) = t s′′(x). t Wi⪯ th probability at≤ least 1 δt , for all s Σ= −1 1t S,
t s=1 t − (cid:80) ∈
(cid:80)
S¯ (x) λL2 dV +d2Y Σ−1 S (x)
τ τ max τ
− ⪯
(cid:104)(cid:112) (cid:105) S¯ (x)+λL2 dV +d2Y Σ−1.
τ τ max
⪯
(cid:104)(cid:112) (cid:105)
Proof. The plan is conceptually the same as the all the previous results.
The standard method for proving matrix inequalities is to compare the
matrices in all directions, which like in the uniform concentration bounds
provenaboverequiresaunionbound. Letε > 0beaconstanttobetuned
later and K and Sd−1 be finite sets such that for all x K
K S 1
C ⊂ C ⊂ ∈
and u Sd−1 there exists a y and v Sd−1 such that x y ε
1 K 1
∈ ∈ C ∈ ∥ − ∥ ≤
and u v ε. Let r = 1/√2dL and suppose that y K (r) and
τ
∥ − ∥ ≤ ∈
x y ε. Then, for any t τ,
∥ − ∥ ≤ ≤
x µ y µ + x y r+ε Σ−1 1 2r.
∥ − t ∥Σ− t1 ≤ ∥ − t ∥Σ− t1 ∥ − ∥Σ− t1 ≤ ∥ t ∥2 ≤
Let x Rd and u Sd−1 and let
1
∈ ∈
τ = max t τ : x K (2r) .
x t
{ ≤ ∈ }
Suppose that y K (r). Then there exists an x such that
τ K
∈ ∈ C
x y ε and therefore x K (2r) and τ = τ. Define the event
τ x
∥ − ∥ ≤ ∈
E = Y Y , X µ √dL .
t
|
t
| ≤
max
∥
t
−
t ∥Σ− t1
≤
(cid:110) (cid:111)
We will prove in a moment that with probability at least 1 δ, τ E
− t=1 t
holds and for all x and u ,
K S
∈ C ∈ C (cid:84)
τx τx
u 2 u 2 +L2λ u 2 dV +d2Y .
∥
∥sˆ′′(x)
≤ ∥
∥s′′(x)
∥
∥Σ−1 τ max
(cid:88)t=1 (cid:88)t=1 (cid:104)(cid:112) (cid:105)
As we argued above, τ = τ for all x for which there is a y K and
x τ
∈
x y ε. The claim is completed using Lipschitz continuity of all the
∥ − ∥ ≤
relevant quantities on K for all t τ.
t
≤GAUSSIAN OPTIMISTIC SMOOTHING 130
Applying Freedman’s By assumption, P ( Y Y ) exp( L)
t−1 t max
| | ≥ ≤ −
and since Σ−1(X µ ) is a standard Gaussian under P we also have
t t t t−1
−
P ( X µ √dL) exp( L). Hence, with probability at least
t−1
∥
t
−
t ∥Σ− t1
≥ ≤ −
1 δ/2, for all x Rd,
− ∈
τ n
sˆ′′(x) = 1 sˆ′′(x)1 .
Et Et
t=1 t=1
(cid:88) (cid:88)
At the same time, for any x ,
K
∈ C
1
E 1 u 2 P(Ec)E u 4 .
E tc ∥ ∥sˆ′′(x) ≤ t t−1 ∥ ∥sˆ′′(x) ≤ n
(cid:114)
(cid:12) (cid:104) (cid:105)(cid:12) (cid:104) (cid:105)
(cid:12) (cid:12)
Therefore, f(cid:12)or any x
K
an(cid:12)d u
S
and t τ x,
∈ C ∈ C ≤
Σ−1
E u 2 E 1 u 2 min .
t−1 ∥ ∥Ht − t−1 Et∥ ∥Ht ≤ n
(cid:12) (cid:2) (cid:3) (cid:2) (cid:3)(cid:12)
We need to bou(cid:12)nd the various moments of u 2(cid:12) . Let
∥
∥sˆ′′(x)
X λx 1
V = Σ−1/2 t − µ = Σ−1/2[X µ+λ(µ x)] ,
t,x t 1 λ − 1 λ t t − −
(cid:18) − (cid:19) −
which is chosen so that
λR (x)Y
sˆ′′(x) = t t Σ−1/2V V⊤Σ−1/2 Σ−1 .
(1 λ)2 t t,x t,x t − t
− (cid:104) (cid:105)
Note that
1 √dL
1 V = Et X µ+λ(µ x) +2r 4√dL.
Et∥ t,x
∥ 1 λ ∥
t
− −
∥Σ− t1
≤ 1 λ ≤
− −
Bounding the absolute magnitude:
λR (x)Y
1 u 2 = 1 t t u⊤ Σ−1/2V V⊤Σ−1/2 Σ−1 u
Et∥ ∥sˆ′′(x) Et (1 λ)2 t x x t − t
− (cid:104) (cid:105)
CLdλY u 2 ,
≤
max
∥
∥Σ− t1GAUSSIAN OPTIMISTIC SMOOTHING 131
where we used the fact that
2
u⊤Σ−1/2V V⊤Σ−1/2u = Σ−1/2u,V u 2 V 2 16dL u 2
t x x t t x ≤ ∥ ∥Σ− t1 ∥ x ∥ ≤ ∥ ∥Σ− t1
(cid:68) (cid:69)
Bounding the second moment needs a little care to get the dimension-
dependence correct. We have
λ u,µ x 1
Σ−1/2u,V =d ⟨ − ⟩, u 2 .
t t,x N 1 λ (1 λ)2 ∥ ∥Σ− t1
(cid:68) (cid:69) (cid:18) − − (cid:19)
Hence the second moment is bounded by
9exp(8)λ2 2
E 1 u 4 E 1 Y2 Σ−1/2u,V
Et∥ ∥sˆ′′(x) ≤ (1 λ)4 t−1 Et t t t,x
(cid:104) (cid:105) − (cid:20) (cid:68) (cid:69) (cid:21)
L2λ2 u 4 E [Y ]2.
≤ ∥
∥Σ− t1 t−1 t
Therefore, by Theorem C.7, with probability at least 1 δ, for all u
S
− ∈ C
and x and Σ−1 S,
K
∈ C ∈
τx
u 2 u 2 λL2 u 2 dV +d2Y ,
(cid:12) ∥
∥s′′(x)
−∥
∥sˆ′′(x)
(cid:12) ≤ ∥
∥Σ−1 τ max
(cid:12)(cid:88)t=1 (cid:16) (cid:17)(cid:12) (cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
which is what we required.
(cid:12) (cid:12)
Proposition 11.19 provides a concentration guarantee for the second
derivatives of sˆ′′(x) for all x within a certain focus region. The next
lemma shows that the accumulation of second derivatives along the se-
quence µ ,...,µ is well-concentration, which is used in Chapter 9.
1 τ
Proposition 11.20. Let S Sd be the (random) set of positive definite
⊂ +
matrices such that Σ−1 Σ−1 for all t τ and S = t sˆ′′(µ ) and
S¯ = t s′′(µ ). Wit th ⪯ probability at lea≤ st 1 δ, fot r all Σs −= 11 t St ,
t s=1 t t − (cid:80) ∈
(cid:80)
S¯ λL2 dV +d2Y Σ−1 S
τ τ max τ
− ⪯
(cid:104)(cid:112) (cid:105) S¯ +λL2 dV +d2Y Σ−1.
τ τ max
⪯
(cid:104)(cid:112) (cid:105)
Proof. Repeat verbatim the proof of Proposition 11.19 except you no
longer need a covering of K. Note that controlling the change of measure
term is now trivial since R (µ ) 3 holds almost surely.
t t
≤GAUSSIAN OPTIMISTIC SMOOTHING 132
Proposition 11.21. The following hold:
(a) Suppose that max λ x µ √1 . Then, for any u Rd,
1≤t≤τ
∥ −
t ∥Σ− t1
≤ 2L ∈
with probability at least 1 δ,
−
τ
sˆ′(x) s′(x),u max u L√n.
(cid:12) ⟨ − ⟩(cid:12) ≤ 1≤t≤τ∥
∥Σ− t1
(cid:12)(cid:88)t=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(b) Let K be(cid:12)defined as in Propo(cid:12)sition 11.17 then with probability at
t
least 1 δ for all x K and u Rd,
t
− ∈ ∈
τ
sˆ′(x) s′(x),u max u L√dn.
(cid:12) ⟨ − ⟩(cid:12) ≤ 1≤t≤τ∥
∥Σ− t1
(cid:12)(cid:88)t=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Proof. For pa(cid:12) rt (a), let E = Y (cid:12) 1 +√L,r (x) 3exp(4) . As in
t t t
{| | ≤ ≤ }
the proof of Proposition 11.17(a), P(Ec) 1/P. Therefore,
t ≤
1
E [1 sˆ′(x),u ] u P(Ec) sˆ′(x) 2 u .
t−1 E tc ⟨ t ⟩ ≤ ∥ ∥Σ− t1 t ∥ t ∥Σt ≤ n ∥ ∥Σ− t1
(cid:113)
(cid:12) (cid:12)
Furth(cid:12)ermore, (cid:12)
X λx
t
1 sˆ(x),u = 1 r (x)Y u, − µ
∥ Et⟨ t ⟩∥t−1,ψ1 (cid:13)
(cid:13)
Et t t
(cid:28)
1 −λ − t (cid:29)Σ− t1(cid:13)
(cid:13)t−1,ψ1
(cid:13) (cid:13)
(cid:13) X λx (cid:13)
(cid:13)3exp(4)(1+√L) u, t − (cid:13)µ
t
≤ (cid:13)
(cid:13)(cid:28)
1 −λ − (cid:29)Σ− t1(cid:13)
(cid:13)t−1,ψ1
(cid:13) (cid:13)
9exp(4)(1+√L)(cid:13) (cid:13)
(cid:13)u . (cid:13)
≤ 1 λ ∥
∥Σ− t1
−
Therefore, by Theorem C.8, with probability at least 1 δ,
−
τ
sˆ′(x) s′(x),u max u 2 L√n.
(cid:12) ⟨ t − t ⟩(cid:12) ≤ 1≤t≤τ∥ ∥Σ− t1
(cid:12)(cid:88)t=1 (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
For part (b), combine the above with the covering argument in Propo-
(cid:12) (cid:12)
sition 11.17, covering both K and the set u : u 1 .
{ ∥ ∥ ≤ }
As mentioned several times, s is convex but sˆ is usually not. The
t tGAUSSIAN OPTIMISTIC SMOOTHING 133
following proposition shows that τ sˆ is nearly convex on a suitable
t=1 t
subset of K.
(cid:80)
Proposition 11.22. Let K be as in Proposition 11.17. Then, with
t
probability at least 1 δ, for all x K ,
t
− ∈
τ
sˆ′′(x) λ√dnLΣ−1.
t ⪰ −
t=1
(cid:88)
Proof. Let u Sd−1 and x K. Then,
1
∈ ∈
u 2 1 CY λ u 2
∥
∥sˆ′′(x) Et
t−1,ψ1 ≤
max
∥
∥Σ− t1
(cid:13) (cid:13)
(cid:13) (cid:13)
Let be a cover(cid:13)of Sd−1 so t(cid:13)hat for all v Sd−1 there exists a u
S 1 1
C ∈ ∈ C
such that u v ε. Similarly, let be a cover of K such that for
K
∥ − ∥ ≤ C
all y K there exists an x with x y ε. By a union bound,
K
∈ ∈ C ∥ − ∥ ≤
with probability at least 1 δ, for all x and u ,
K S
− ∈ C ∈ C
τ
u 2 ατ CY λ u 2 nlog |CK ||CS | .
∥ ∥sˆ′′(x) ≥ − max ∥ ∥Σ−1 (cid:115) δ
t=1 (cid:18) (cid:19)
(cid:88)
Repeating the naive Lipschitz analysis used in earlier proofs it follows
that for all x K and u Sd−1,
τ 1
∈ ∈
τ
u 2 CY λ u 2 nlog |CK ||CS |
∥ ∥sˆ′′(x) ≥ − max ∥ ∥Σ−1 (cid:115) δ
t=1 (cid:18) (cid:19)
(cid:88)
λ u 2√dnLΣ−1.
≥ − ∥ ∥
Equivalently, τ sˆ′′(x) λ√dnLΣ−1.
t=1 ⪰ −
(cid:80)
11.9 Summary
Let us summarise what has been shown. The surrogate loss function is
convex (Lemma 11.1(a)) and optimistic:
s(x) f(x) for all x Rd. Lemma 11.1(b)
≤ ∈GAUSSIAN OPTIMISTIC SMOOTHING 134
On the other hand, the surrogate evaluated at µ is relatively close to
f(µ):
4
E[f(X)] s(µ)+ tr(s′′(µ)Σ). Proposition 11.5
≤ λ
Furthermore, the quadratic surrogate offers the same benefits on the
focus region. Specifically, for any x such that λ x µ 1,
∥ − ∥Σ−1 ≤ L
4
E[f(X)] f(x) q(µ) q(x)+ tr(s′′(µ)Σ).
− ≤ − λ
The effectiveness of the quadratic surrogate arises from the fact that
s is nearly quadratic on the focus region. Specifically, provided that
λ x y L−1/2, then s′′(x) 3s′′(y).
∥ −
∥Σ−1
≤ ⪯
Tail bounds Regarding the tails of the estimators, the gradient of the
surrogate at µ is well-behaved in the sense that
P s′(µ) 2 dLY2 δ. Lemma 11.15
∥ ∥Σ ≥ ≤
(cid:16) (cid:17)
Sequential concentration Recall the notation of the sequential set-
ting explained in Section 11.8. Particularly, that
K (r) = x K : maxλ x µ r .
τ
∈ t≤τ ∥ −
t ∥Σ− t1
≤
(cid:26) (cid:27)
Remember also that V = τ E [Y2] and Y is a constant such
τ t=1 t−1 t max
that P( Y Y ) exp( L). In the sequential setting explained in
t max
| | ≥ ≤ −(cid:80)
Section 11.8, the surrogate is well-concentration in the sense that by
Theorem 11.17,
(a) Forx Rd suchthatx K (1/√2L)almostsurely,withprobability
τ
∈ ∈
at least 1 δ,
−
τ
1
(sˆ(x) s (x)) 1+ LV +LY .
t t τ max
(cid:12) − (cid:12) ≥ λ
(cid:12)(cid:88)t=1 (cid:12) (cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)GAUSSIAN OPTIMISTIC SMOOTHING 135
(b) With probability at least 1 δ,
−
τ
1
sup (sˆ(x) s (x)) 1+ dLV +dLY .
t t τ max
x∈Kτ(1/√ 2dL)(cid:12)
(cid:12)(cid:88)t=1
− (cid:12)
(cid:12)
≤ λ
(cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
Similar results h(cid:12)old for the quadra(cid:12)tic surrogate. Precisely, by Theo-
rem 11.18,
(a) Given any x Rd such that x K (r) almost surely, with proba-
τ
∈ ∈
bility at least 1 δ,
−
τ rL2
qˆ(x) q (x) V +Y .
t t τ max
(cid:12) − (cid:12) ≤ λ
(cid:12)(cid:88)t=1 (cid:12) (cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
(b) With proba(cid:12)bility at least 1 (cid:12) δ,
−
τ rL2
sup qˆ(x) q (x) dV +dY .
t t τ max
x∈Kτ(r)(cid:12)
(cid:12)(cid:88)t=1
− (cid:12)
(cid:12)
≤ λ
(cid:104)(cid:112) (cid:105)
(cid:12) (cid:12)
(cid:12) (cid:12)
TheHessianestim(cid:12)atesarealsoreaso(cid:12)nablywell-behaved. RecallthatP is
the random set of matrices Σ−1 such that Σ−1 Σ−1 for all t τ. Then,
t
⪯ ≤
withprobabilityatleast1 δ thefollowingholdsforallx K (1/√2dL),
τ
− ∈
S¯ (x) S (x) λL2 dV +d2Y ,
τ τ τ max
− ⪯
(cid:12) (cid:12)
(cid:104)(cid:112) (cid:105)
where S (x) =(cid:12) τ sˆ′′(x) and(cid:12) S¯ (x) = τ s′′(x) and A B means
τ t=1 t τ t=1 t | | ⪯
that A B and A B both hold. Under the same conditions, with
⪯ (cid:80)− ⪯ (cid:80)
probability at least 1 δ,
−
S¯ S λL2 dV +d2Y ,
τ τ τ max
− ⪯
(cid:12) (cid:12)
(cid:104)(cid:112) (cid:105)
where S = τ sˆ(cid:12)′′(µ ) an(cid:12)d S¯ = τ s′′(µ ).
τ t=1 t t τ t=1 t t
(cid:80) (cid:80)
11.10 Notes
(a) The optimistic surrogate was introduced in a slightly different form
by Bubeck et al. [2017] and in the present form by Lattimore and Gy¨orgyGAUSSIAN OPTIMISTIC SMOOTHING 136
[2021a]. The quadratic approximation was first used by Lattimore and
Gyo¨rgy [2023], who proved most of the results in this chapter or variants
there-of.
(b) The parameter λ determines the amount of smoothing. The change
ofmeasureinEq.(11.5)blowsupasλ 1/d. Meanwhile, forλ (0,1/d)
≥ ∈
there are trade-offs.
➳ A large value of λ increases the power of the lower bound of Propo-
sition 11.5 showing that s is not too far below f.
➳ A large value of λ decreases the focus region on which the quadratic
surrogate is close to the non-quadratic surrogate and where the con-
centration properties of the estimators are well-behaved.Chapter 12
Outlook
Thetool-chestforconvexbanditsandzeroth-orderoptimisationhasbeen
steadily growing in recent decades. Nevertheless, there are many inter-
esting open questions both theoretical and practical. The only purpose
of this short chapter is to highlight some of the most important (in the
author’s view, of course) open problems. We ignore all logarithmic fac-
tors here. The time will one day arrive when the only dregs left will be
chasing logarithmic factors, but it is not here yet.
(a) The most fundamental problem is to understand the minimax regret
for F . The lower bound is d√n and the upper bound is d2.5√n with
b
the latter bound obtained by non-constructive means. In light of the
many positive results for slightly more constrained classes a reasonable
conjecture is that the minimax regret is d1.5√n. That no one could yet
prove this is a lower bound, however, suggests that d√n may be the
minimax rate. At the moment it seems we need to push hard on both
ends.
(b) From a practical perspective the situation is still relatively dire for
d > 1. The algorithms that are simple and efficient to implement have
slow convergence rates without smoothness and strong convexity. Al-
gorithms with fast convergence rates have awkward assumptions. For
example, Online Newton step learns fast for Fs and is difficult to tune.
b,u
Is there a simple algorithm that works well in practice without too much
tuning and obtains the fast rate?
(c) AlgorithmsthatmanageO(√n)regretwithoutsmoothnessandstrong
convexity all estimate some kind of curvature or use continuous exponen-
tial weights. In particular, they use Ω(d2) memory. Can you prove this
is necessary?
137OUTLOOK 138
(d) In the stochastic setting the range of the losses should appear as a
second-order term in the regret. Sadly, the only algorithm for which this
is true is the bisection method from Chapter 4, which only works when
d = 1. This weakness should be rectified somehow.
(e) More adaptive algorithms are needed. We have seen a plethora of
results for this algorithm or that with such-and-such assumptions. But
whatifyoudon’tknowwhatclassthelossesliein. Canyouadapt? What
is the price? Very few works consider this or have serious limitations. A
good place to start is the paper by Luo et al. [2022].
(f) There is scope to refine the algorithms and analysis in this text to
the non-convex case. Of course, proving bounds relative to a stationary
point rather than a global minimum. Someone should push this program
through.
(g) We mentioned that the center of gravity method could potentially
replace the ellipsoid method using the tools in Chapter 10.
(h) Almost all of the properties we proved for the optimistic surrogate
relied on Gaussianity of the exploration distribution. Two properties
that do not rely on this, however, are optimism and convexity. This
leaves hope that something may be possible using an exponential weights
distribution rather than a Gaussian and this may interact better with the
geometry of the constraint set. This seems to have been the original plan
of Bubeck et al. [2017] before they resorted to approximating exponential
weights distributions by Gaussians. Perhaps you can make it work.
(i) Suggala et al. [2021] and Bubeck et al. [2017] both handle adversar-
ial problems by some sort of test to see if the adversary is moving the
minimum and proving that if this occurs, then the regret must be nega-
tive and it is safe to restart the algorithm. One might wonder if there is
some black box procedure to implement this program so that any algo-
rithm designed for the stochastic setting can be used in the adversarial
setting.
(j) It would be fascinating to gain a better understanding of Algo-
rithm 16. What loss estimators does it use? Maybe you can somehow
implement this algorithm when d = 1 or derive analytically what the
estimators look like for special cases.Appendix A
Bibliography
J. D. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An
efficient algorithm for bandit linear optimization. In Proceedings of the
21st Conference on Learning Theory, pages 263–274. Omnipress, 2008.
A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online con-
vex optimization with multi-point bandit feedback. In Conference on
Learning Theory, pages 28–40. Citeseer, 2010.
A. Agarwal, D. P. Foster, D. J. Hsu, S. M. Kakade, and A. Rakhlin.
Stochastic convex optimization with bandit feedback. In Advances
in Neural Information Processing Systems, pages 1035–1043. Curran
Associates, Inc., 2011.
A.Agarwal,D.P.Foster,D.Hsu,S.M.Kakade,andA.Rakhlin. Stochas-
tic convex optimization with bandit feedback. SIAM Journal on Op-
timization, 23(1):213–240, 2013.
S. Artstein-Avidan, A. Giannopoulos, and V. D. Milman. Asymptotic
geometric analysis, Part I, volume 202. American Mathematical Soc.,
2015.
F. Bach and V. Perchet. Highly-smooth zero-th order online optimiza-
tion. In Conference on Learning Theory, pages 257–283. PMLR, 2016.
K. Balasubramanian and S. Ghadimi. Zeroth-order nonconvex stochastic
optimization: Handling constraints, high dimensionality, and saddle
points. Foundations of Computational Mathematics, pages 1–42, 2022.
A. Belloni, T. Liang, H. Narayanan, and A. Rakhlin. Escaping the lo-
cal minima via simulated annealing: Optimization of approximately
139BIBLIOGRAPHY 140
convex functions. In Conference on Learning Theory, pages 240–265,
2015.
S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: A
nonasymptotic theory of independence. OUP Oxford, 2013.
S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and non-
stochastic multi-armed bandit problems. Foundations and Trends in
Machine Learning, 5(1):1–122, 2012.
S. Bubeck and R. Eldan. The entropic barrier: a simple and optimal uni-
versal self-concordant barrier. arXiv preprint arXiv:1412.1587, 2014.
S. Bubeck and R. Eldan. Exploratory distributions for convex functions.
Mathematical Statistics and Learning, 1(1):73–100, 2018.
S. Bubeck, N. Cesa-Bianchi, and S. Kakade. Towards minimax policies
for online linear optimization with bandit feedback. In Proceedings
of the 25th Conference on Learning Theory, pages 41–1. Microtome,
2012.
S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex opti-
mization: √T regret in one dimension. In Proceedings of the 28th
Conference on Learning Theory, pages 266–278, Paris, France, 2015.
JMLR.org.
S. Bubeck, Y.T. Lee, and R. Eldan. Kernel-based methods for bandit
convex optimization. InProceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2017, pages 72–85, New
York, NY, USA, 2017. ACM. ISBN 978-1-4503-4528-6.
S. Bubeck, R. Eldan, and J. Lehec. Sampling from a log-concave distri-
bution with projected langevinmonte carlo. Discrete & Computational
Geometry, Apr 2018.
N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cam-
bridge University Press, 2006.
J. Cheshire, P. M´enard, and A. Carpentier. The influence of shape con-
straintsonthethresholdingbanditproblem. InConferenceonLearning
Theory, pages 1228–1275. PMLR, 2020.BIBLIOGRAPHY 141
S.Chewi. Theentropicbarrierisn-self-concordant. InGeometric Aspects
of Functional Analysis: Israel Seminar (GAFA) 2020-2022, pages209–
222. Springer, 2023.
A. Conn, K. Scheinberg, and L. Vicente. Introduction to derivative-free
optimization. SIAM, 2009.
V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimiza-
tion under bandit feedback. In Proceedings of the 21st Conference on
Learning Theory, pages 355–366, 2008.
Y. Drori. On the properties of convex functions over open sets. arXiv
preprint arXiv:1812.02419, 2018.
J.Duchi, M.Jordan, M.Wainwright, andA.Wibisono. Optimalratesfor
zero-orderconvexoptimization: Thepoweroftwofunctionevaluations.
IEEE Transactions on Information Theory, 61(5):2788–2806, 2015.
A Flaxman, A Kalai, and HB McMahan. Online convex optimization in
the bandit setting: Gradient descent without a gradient. In SODA’05:
Proceedings of the sixteenth annual ACM-SIAM symposium on Dis-
crete algorithms, pages 385–394, 2005.
D.J.Foster,S.Kakade,J.Qian,andA.Rakhlin. Thestatisticalcomplex-
ity of interactive decision making. arXiv preprint arXiv:2112.13487,
2021.
D. J. Foster, A. Rakhlin, A. Sekhari, and K. Sridharan. On the complex-
ity of adversarial decision making. Advances in Neural Information
Processing Systems, 35:35404–35417, 2022.
M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval
Research Logistics Quarterly, 3(1-2):95–110, 1956.
D. Garber and B. Kretzu. New projection-free algorithms for online
convex optimization with adaptive regret guarantees. In Conference
on Learning Theory, pages 2326–2359. PMLR, 2022.
M. Gr¨otschel, L. Lova´sz, and A. Schrijver. Geometric algorithms and
combinatorial optimization, volume 2. Springer Science & Business
Media, 2012.BIBLIOGRAPHY 142
E. Hazan. Introduction to online convex optimization. Foundations and
Trends® in Optimization, 2(3-4):157–325, 2016.
E. Hazan and K. Levy. Bandit convex optimization: Towards tight
bounds. In Advances in Neural Information Processing Systems, pages
784–792, 2014.
E. Hazan and Y. Li. An optimal algorithm for bandit convex optimiza-
tion. arXiv preprint arXiv:1603.04350, 2016.
E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for
online convex optimization. Machine Learning, 69:169–192, 2007.
X. Hu, Prashanth L.A., A. Gy¨orgy, and Cs. Szepesva´ri. (Bandit) convex
optimization with biased noisy gradient oracles. In AISTATS, pages
819–828, 2016.
S. Ito. An optimal algorithm for bandit convex optimization with
strongly-convex and smooth loss. In Proceedings of the Twenty Third
International Conference on Artificial Intelligence and Statistics, vol-
ume 108 of Proceedings of Machine Learning Research, pages 2229–
2239. PMLR, 26–28 Aug 2020.
K. Jamieson, R. Nowak, and B. Recht. Query complexity of derivative-
freeoptimization. Advances in Neural Information Processing Systems,
25, 2012.
R. Karp. Reducibility among combinatorial problems. Complexity of
Computer Computations, pages 85–103, 1972.
L. Khachiyan. A polynomial algorithm in linear programming. In Dok-
lady Akademii Nauk, volume 244, pages 1093–1096. Russian Academy
of Sciences, 1979.
J. Kiefer and J. Wolfowitz. The equivalence of two extremum problems.
Canadian Journal of Mathematics, 12(5):363–365, 1960.
R. Kleinberg. Nearly tight bounds for the continuum-armed bandit prob-
lem. In Advances in Neural Information Processing Systems, pages
697–704. MIT Press, 2005.BIBLIOGRAPHY 143
J.Larson,M.Menickelly,andS.Wild. Derivative-freeoptimizationmeth-
ods. Acta Numerica, 28:287–404, 2019.
T. Lattimore. Improved regret for zeroth-order adversarial bandit convex
optimisation. Mathematical Statistics and Learning, 2(3/4):311–334,
2020.
T. Lattimore and A. Gy¨orgy. Improved regret for zeroth-order stochas-
tic convex bandits. In Mikhail Belkin and Samory Kpotufe, editors,
Proceedings of Thirty Fourth Conference on Learning Theory, volume
134 of Proceedings of Machine Learning Research, pages 2938–2964.
PMLR, 15–19 Aug 2021a.
T. Lattimore and A. Gy¨orgy. Mirror descent and the information ratio.
In Conference on Learning Theory, pages 2965–2992. PMLR, 2021b.
T. Lattimore and A. Gy¨orgy. Mirror descent and the information ratio.
In Conference on Learning Theory, pages 2965–2992. PMLR, 2021c.
T. Lattimore and A. Gy¨orgy. A second-order method for stochastic ban-
dit convex optimisation. arXiv preprint arXiv:2302.05371, 2023.
T. Lattimore and Cs. Szepesv´ari. Bandit algorithms. Cambridge Univer-
sity Press, 2020.
T.Liang, H.Narayanan, andA.Rakhlin. Onzeroth-orderstochasticcon-
vex optimization via random walks. arXiv preprint arXiv:1402.2667,
2014.
S. Liu, P-Y. Chen, B. Kailkhura, G. Zhang, A. Hero III, and P. Varsh-
ney. A primer on zeroth-order optimization in signal processing and
machine learning: Principals, recent advances, and applications. IEEE
Signal Processing Magazine, 37(5):43–54, 2020.
H. Luo, M. Zhang, and P. Zhao. Adaptive bandit convex optimization
with heterogeneous curvature. In Po-Ling Loh and Maxim Raginsky,
editors, Proceedings of Thirty Fifth Conference on Learning Theory,
volume 178 of Proceedings of Machine Learning Research, pages 1576–
1612. PMLR, 02–05 Jul 2022.
T. Motzkin and G. Straus. Maxima for graphs and a new proof of a
theoremoftura´n. Canadian Journal of Mathematics,17:533–540,1965.BIBLIOGRAPHY 144
A. Nemirovski. Lecture notes: Interior-point polynomial time methods
for convex programming. 1996.
A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method
Efficiency in Optimization. Wiley, 1983.
Y. Nesterov and V. Spokoiny. Random gradient-free minimization of
convexfunctions. Foundations of Computational Mathematics, 17:527–
566, 2017.
F. Orabona. A modern introduction to online learning. arXiv preprint
arXiv:1912.13213, 2019.
L. Orseau and M. Hutter. Line search for convex minimization. arXiv
preprint arXiv:2307.16560, 2023.
D. Russo and B. Van Roy. Learning to optimize via information-directed
sampling. In Advances in Neural Information Processing Systems,
pages 1583–1591. Curran Associates, Inc., 2014.
A. Saha and A. Tewari. Improved regret guarantees for online smooth
convex optimization with bandit feedback. In Proceedings of the Four-
teenth International Conference on Artificial Intelligence and Statis-
tics, pages 636–642, 2011.
O. Shamir. On the complexity of bandit and derivative-free stochastic
convex optimization. In Proceedings of the 26th Conference on Learn-
ing Theory, pages 3–24. JMLR.org, 2013.
N. Shor. Cut-off method with space extension in convex programming
problems. Cybernetics, 13(1):94–96, 1977.
A. Slivkins. Introduction to multi-armed bandits. Foundations and
Trends in Machine Learning, 12(1-2):1–286, 2019. ISSN 1935-8237.
A. Suggala, P. Ravikumar, and P. Netrapalli. Efficient bandit convex
optimization: Beyond linear losses. In Mikhail Belkin and Samory
Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learn-
ing Theory, volume 134 of Proceedings of Machine Learning Research,
pages 4008–4067. PMLR, 15–19 Aug 2021.BIBLIOGRAPHY 145
P. Vaidya. A new algorithm for minimizing convex functions over convex
sets. Mathematical programming, 73(3):291–341, 1996.
D. van der Hoeven, T. van Erven, and W. Kot(cid:32)lowski. The many faces
of exponential weights in online learning. In Proceedings of the 31st
Conference on Learning Theory, pages 2067–2092, 2018.
R. Vershynin. High-dimensional probability: An introduction with appli-
cations in data science, volume 47. Cambridge university press, 2018.
Y. Wang. On adaptivity in nonstationary stochastic optimization with
bandit feedback. Operations Research, 2023.
D. Yudin and A. Nemirovskii. Informational complexity and efficient
methods for the solution of convex extremal problems. Matekon, 13
(2):22–45, 1976.
D.YudinandA.Nemirovskii. Evaluationoftheinformationalcomplexity
of mathematical programming problems. ekonomikai matematicheskie
metody 12 (1976) 128-142. Matekon, 13(2):3–25, 1977.
P.Zhao, G.Wang, L.Zhang, andZ-H.Zhou. Banditconvexoptimization
in non-stationary environments. The Journal of Machine Learning
Research, 22(1):5562–5606, 2021.
J. Zimmert and T. Lattimore. Connections between mirror descent,
thompson sampling and the information ratio. In Advances in Neu-
ral Information Processing Systems, pages 11973–11982. Curran Asso-
ciates, Inc., 2019.
M. Zinkevich. Online convex programming and generalized infinitesimal
gradient ascent. In Proceedings of the 20th International Conference
on Machine Learning, pages 928–935. AAAI Press, 2003.Appendix B
Miscellaneous
B.1 Identities
Lemma B.1. vol(Sd−1) = d vol(Bd).
r r r
Proposition B.2. Suppose that W has law (0,Σ). Then,
N
(a) E[ W 2] = tr(Σ).
∥ ∥
(b) E[ W 4] = tr(Σ)2 +2tr(Σ2).
∥ ∥
(c) E[exp( W,a )] = exp 1 a 2 .
⟨ ⟩ 2 ∥ ∥Σ
(cid:0) (cid:1)
B.2 Technical inequalities
Lemma B.3. Suppose that A,B,C > 0. Then,
A ηB
min + +rC =
√8(AB)1 C1
4 2
η>0,r>0 η r2
(cid:18) (cid:19)
with the minimising values of η and r being
1 1
η =
2 2 A3 B−1
r =
2 2 (AB)1
.
4 4 4
C C
(cid:18) (cid:19) (cid:18) (cid:19)
Lemma B.4. Suppose that A,B,C > 0. Then,
A ηB
min + +r2C = 3(ABC)1 .
3
η>0,r>0 η r2
(cid:18) (cid:19)
146MISCELLANEOUS 147
with the minimising values of η and r being
η = A2 B−1 C−1 r2 = A1 B1 C−2 .
3 3 3 3 3 3
Lemma B.5. Suppose that u,v Sd−1 and A Rd×d is symmetric.
1
∈ ∈
Then,
u⊤Au v⊤Av +2 u v A .
≤ ∥ − ∥∥ ∥
Proof. Using symmetry of A and Cauchy-Schwarz and the definition of
the spectral norm,
u⊤Au v⊤Av = (u v)⊤A(u+v) u v A(u+v) 2 u v A .
− − ≤ ∥ − ∥∥ ∥ ≤ ∥ − ∥∥ ∥
Lemma B.6. Suppose that A is positive definite and A 1. Then
⪯
tr(A) 2logdet(1+A).
≤
Proof. Use the fact that the trace is the sum of the eigenvalues and the
determinant is the product and that x 2log(1+x) for x [0,1].
≤ ∈
Lemma B.7. Suppose that X has law (µ,Σ) and P(X / K) 1.
N ∈ ≤ 4
Then Σ 1/2 3 diam(K).
∥ ∥ ≤ 2
Proof. Suppose on the contrary that 1 Σ 1/2 > 1 diam(K). Then,
3 ∥ ∥ 2
1
2P(X / K)
2 ≥ ∈
P(X / K or X / K) Union bound
≥ ∈ − ∈
diam(K)
P X µ > Definition of diameter
≥ ∥ − ∥ 2
(cid:18) (cid:19)
1
> P X µ > Σ 1/2 Assumption
∥ − ∥ 3 ∥ ∥
(cid:18) (cid:19)
1
. Gaussian integral
≥ 2Appendix C
Concentration
C.1 Orlicz norms
Given a random variable X and k 1,2 let
∈ { }
X = inf t > 0 : E exp X/t k 2 .
∥ ∥ψ k | | ≤
(cid:8) (cid:2) (cid:0) (cid:1)(cid:3) (cid:9)
The random variable X is called subgaussian if X < and subex-
∥ ∥ψ2 ∞
ponential if X < . As explained in detail by Vershynin [2018], this
∥ ∥ψ1 ∞
definition is equivalent except for universal constants to the definitions
based on moments or the moment generating function, which appear, for
example, in the book by Boucheron et al. [2013].
Lemma C.1. Given any random variable X and t > 0,
(a) P( X t) 2exp t .
| | ≥ ≤ −∥X∥ ψ1
(cid:16) (cid:17)
(b) P( X t) 2exp t2 .
| | ≥ ≤ −∥X∥2
(cid:18)
ψ2(cid:19)
Proof. Both results follow from a standard method. For (a),
X t
P( X t) = P exp exp
| | ≥ X ≥ X
(cid:32) (cid:32)
∥
∥ψ1(cid:33) (cid:32)
∥
∥ψ1(cid:33)(cid:33)
t
2exp . Markov’s inequality
≤ − X
(cid:32)
∥
∥ψ1(cid:33)
Part (b) is left as an exercise.
Lemma C.2. Given any random variable X and k 1, E[ X k]
≥ | | ≤
Γ(1+k) X k where Γ(1+k) is the Gamma function.
∥ ∥ψ1
148CONCENTRATION 149
Proof. Since X is non-negative,
| |
∞
E[ X k] = P( X k t)dt
| | | | ≥
(cid:90)0
∞
= P( X t1/k)dt
| | ≥
(cid:90)0
∞ t1/k
2exp dt
≤ − X
(cid:90)0 (cid:32)
∥
∥ψ1(cid:33)
= 2Γ(1+k) X k .
∥ ∥ψ1
Lemma C.3 (Lattimore and Gy¨orgy 2023). Suppose that W is a stan-
dard Gaussian random variable in Rd. Then:
(a) x,W = 2 2/3 x .
∥⟨ ⟩∥ψ2 ∥ ∥
(b) tr(AWW⊤) (cid:112) 3tr(A).
ψ1 ≤
(c) (cid:13) W 2 (cid:13)8d/3.
(cid:13)∥ ∥ ψ1 ≤(cid:13)
(d) (cid:13) WW(cid:13)⊤ 1 5d.
(cid:13) (cid:13) − ψ1 ≤
Lem(cid:13) (cid:13)m(cid:13) (cid:13)a C.4. Sup(cid:13) (cid:13)p(cid:13) (cid:13)ose that X B. Then X B .
| | ≤ ∥ ∥ψ2 ≤ √log(2)
C.2 Concentration
The following are classical:
Theorem C.5. Suppose that X has law (0,1). Then, for any x 0,
N ≥
1 x2
P(X x) exp .
≥ ≤ 2 − 2
(cid:18) (cid:19)
Theorem C.6. Let X ,...,X be a sequence of independent random
1 n
variables. Then, for any δ (0,1),
∈
n
P (X E[X ]) σ nlog(2/δ) δ,
t t
− ≥ ≤
(cid:32) (cid:33)
t=1
(cid:88) (cid:112)
where σ = max X : 1 t n .
{∥ t ∥ψ2 ≤ ≤ }CONCENTRATION 150
Theorem C.7 (Freedman’s inequality). Let X ,...,X be a se-
1 n
quence of random variables adapted to filtration (F ) and τ be a stopping
t
time with respect to (F )n with τ n almost surely. Let E [ ] = E[ F ]
t t=1 ≤ t · ·| t
and X B almost surely for all t τ. Then, with probability at least
t
| | ≤ ≤
1 δ,
−
τ
log(1+ V ) 1
X (X E [X ]) C V log | τ | +Blog ,
t t t−1 t τ
(cid:12) − (cid:12) ≤ (cid:34)(cid:115) δ δ (cid:35)
(cid:12)(cid:88)t=1 (cid:12) (cid:18) (cid:19) (cid:18) (cid:19)
(cid:12) (cid:12)
w(cid:12) (cid:12)here V
τ
= τ t=1E t−1(cid:12) (cid:12)[(X
t
−
E t−1[X t])2 is the sum of the predictable
variations and C > 0 is a sufficiently large absolute constant.
(cid:80)
We need a simple martingale-style concentration inequality. Obvi-
ously these things are ten-a-penny but as usual this particular one is
apparently not in the literature.
TheoremC.8. LetX ,...,X beasequenceofrandomvariablesadapted
1 n
to filtration (F )n and τ be a stopping time with respect to (F )n with
t t=1 t t=1
τ n almost surely. Let E [ ] = E[ F ]. Then
t t
≤ · ·|
τ
1+ log(σ)
P (X E [X ]) Cσ nlog | | 1 δ.
t t−1 t
(cid:32)(cid:12) − (cid:12) ≤ (cid:115) δ (cid:33) ≥ −
(cid:12)(cid:88)t=1 (cid:12) (cid:18) (cid:19)
(cid:12) (cid:12)
(cid:12) (cid:12)
where C > 0 is a universal constant and σ = max X with
(cid:12) (cid:12) 1≤t≤τ ∥ t ∥t−1,ψ1
X = inf θ > 0 : E[exp( X /θ ) F ] 2
∥ t ∥t−1,ψ1 { | t | | t−1 ≤ }
the subexponential Orlicz norm with respect to P( F ).
t−1
·|
Proof. Throughout the proof we let c,C > 0 be suitably small/large
universal constants that may change from one expression to the next.
Given ε > 0 let
τ = min t τ : X > ε ,
ε { ≤ ∥ t+1 ∥t,ψ1 }
where the minimum of the empty set is defined to be τ. Note that
X is F -measurable so τ is a stopping time with respect to the
∥ t+1 ∥t,ψ1 t ε
filtration (F )n .
t t=1CONCENTRATION 151
Step 1: Point-wise bounds Given any λ > 0. Suppose that λ c
| | ≤ ε
with c > 0 a suitably small universal constant. Then, following the proof
of [Vershynin, 2018, Theorem 2.8.1],
E [exp(λX )] exp Cλ2ε2 .
t−1 t
≤
(cid:0) (cid:1)
Therefore
τε
E exp λ X Cτ λ2ε2 1.
t ε
− ≤
(cid:34) (cid:32) (cid:33)(cid:35)
t=1
(cid:88)
Hence, By Markov’s inequality,
τε
1
P exp λ X Cτ λ2ε2 1 δ.
t ε
− ≤ δ ≥ −
(cid:32) (cid:32) (cid:33) (cid:33)
t=1
(cid:88)
And on this event
τε
1
X Cτ λε2 + log(1/δ) .
t ε
≤ λ
t=1
(cid:88)
Choosing λ = min 1 log(1/δ), c shows that
ε n ε
(cid:18) (cid:113) (cid:19)
τε
X Cε nlog(1/δ)+log(1/δ) .
t
≤
(cid:88)t=1 (cid:104)(cid:112) (cid:105)
Step2: Unionbounds Let = 2−k : k Z . Notethat ∞ 1 =
E { ∈ } k=−∞ 4k(k+1)
1. Therefore, by the previous step, with probability at least 1 δ, for all
(cid:80)−
ε ,
∈ E
τε
k +1 k +1
X Cε nlog +log .
t
≤ (cid:34)(cid:115) δ δ (cid:35)
t=1 (cid:18) (cid:19) (cid:18) (cid:19)
(cid:88)
Let
ε = min θ : θ σ .
{ ∈ E ≥ }CONCENTRATION 152
Then τ = τ and with probability at least 1 δ,
ε
−
τ
1+ log(σ) k +1
X Cσ nlog | | +log .
t
≤ (cid:34)(cid:115) δ δ (cid:35)
t=1 (cid:18) (cid:19) (cid:18) (cid:19)
(cid:88)Appendix D
Computation
Our algorithms need to perform a number of operations on convex sets.
For example, Algorithm 7 needs to implement projections while Algo-
rithm 8 needs to compute a self-concordant barrier on K. The compu-
tation complexity of these operations depends on how K is represented.
Convex sets can be represented in many ways but some typical choices
are:
(a) As a polytope: K = x : Ax b .
{ ≤ }
(b) As the convex hull of a point cloud: K = conv(P) with P Rd.
⊂
(c) Via a separation oracle, which is an oracle that accepts a point
x Rd and (1) if x K, then it says so or (2) if x is not in K then it
∈ ∈
returns a nonzero vector η such that K y : y x,η 0 .
⊂ { ⟨ − ⟩ ≤ }
(d) Via a linear optimisation oracle, which is an oracle that accepts as
input some vector v Rd and outputs a point x argmin y,v .
y∈K
∈ ∈ ⟨ ⟩
(e) Via a membership oracle, which is an oracle that accepts a point
x Rd and returns 1 (x).
K
∈
Membership to separation A membership oracle is clearly weaker
than a separation oracle but there is an oracle polynomial time reduction
from a membership oracle to a separation oracle provided that K is
centered and well rounded, which means there is a known x Rd and
∈
0 < r < R < such that x + Bd K Bd [Gro¨tschel et al., 2012,
∞ r ⊂ ⊂ R
Theorem 3.2.4].
Convex hull of point-cloud to separation Given a point cloud P =
x ,...,x , the membership oracle component of the separation oracle
1 m
{ }
153COMPUTATION 154
corresponds to feasibility of the linear program: x = m p x with
k=1 k k
p ∆ . Suppose that x is not in K, then a separating vector can
m
∈ (cid:80)
also be found via linear feasibility. For example, finding an η such that
η 1 and x x,η 0 for all k = 1,...,m. By contrast, when
∥ ∥∞ ≤ ⟨ k − ⟩ ≤
K = conv(P), then a linear optimisation oracle can be implemented
trivially in O(m) time.
Projection Euclideanprojectionontoapolytopeismostefficientlyim-
plemented using Newton’s method, which typically converges extremely
fast. If you only have a separation oracle, then you may have to resort to
the ellipsoid method. If you have a linear optimisation oracle you could
use Frank–Wolfe [Frank and Wolfe, 1956].
Self-concordant barriers When K = x : Ax b is a polytope and
{ ≤ }
A = (a ,...,a )⊤ hasmrows, thenR(x) = m log( a ,x b )isan
1 m − k=1 ⟨ k ⟩− k
m-self-concordant barrier on K. The logarithmic barrier is not an intrin-
(cid:80)
sically defined barrier in the sense that it depends on the representation.
Furthermore, when m is very large it may be a very inefficient barrier.
The volumetric barrier is another simple to implement barrier that im-
proves the self-concordance to O(d√m) [Vaidya, 1996]. The Lee–Sidford
barrier improves the self-concordance parameter to O(dlog(m)) while
remaining practical. In general there always exists a ϑ-self-concordant
barrier with ϑ d [Bubeck and Eldan, 2014, Chewi, 2023] but sadly
≤
it is not practically computable. You can find many more examples of
ϑ-self-concordant barriers in the notes by Nemirovski [1996].
D.1 Approximating minimum volume enclosing ellipsoids
The material here summarises results by [Gr¨otschel et al., 2012, Chapter
4]. The objective is find a computationally efficient algorithm for ap-
proximating mvee(K) for convex bodies K represented as polytopes via
inequality constraints or a separation oracle. Finding the exact minimum
volume enclosing or enclosed ellipsoid is computationally hard even when
K is represented by a polytope but there do exist efficient approxima-
tions.
➳ When K = x : Ax b , then there exists a polynomial time algo-
{ ≤ }
rithm for finding an ellipsoid E = E(x,V) such that E(x, 1 V)
4d2 ⊂COMPUTATION 155
K E(x,V).
⊂
➳ When K is given by a separation oracle, then there exists a poly-
nomial time algorithm for finding an ellipsoid E = E(x,V) such
that E(x, 1 V) K E(x,V).
4d3 ⊂ ⊂
Remark D.1. Approximate minimum/maximum volume enclosing/en-
closedellipsoidscanbeusedtofindnear-maximalr suchthatx+Bd K
r ⊂
for some x. Define
r = max r : x+Bd K,x K .
⋆ { r ⊂ ∈ }
Given an ellipsoid E = E(x,A) such that E(x,αA) K E(x,A) and
⊂ ⊂
let λ be the smallest eigenvalue of A and r = α√λ. Then x+Bd K
r ⊂
and r r r/α.
⋆
≤ ≤
Polytopes Let K = x : Ax b be a polytope with m constraints so
{ ≤ }
that A Rm×d. The idea is to use the ellipsoid method. We describe a
∈
procedurethatacceptstheparametersofK andanellipsoidE = E(x,V)
with K E as inputs and either
⊂
(a) declares that E(x,r2V) K where r = 1 or;
⊂ 2d
(b) returns a new ellipsoid E(y,W) K with
⊃
1
vol(E(y,W) exp vol(E(x,V)). (D.1)
≤ −20d
(cid:18) (cid:19)
Start with some ellipsoid E K and iterate the procedure until it
1
⊃
declares some ellipsoid is contained in K. Since the volume reduces by a
constant factor in every iteration, the number of iterations needed before
the process ends is at most
vol(E )
1
O dlog .
vol(K)
(cid:18) (cid:18) (cid:19)(cid:19)
The procedure is fairly straightforward:
(a) Let a be the kth row of A and check that E(x,r2V) K if and
k
⊂COMPUTATION 156
only if for all 1 k m,
≤ ≤
Va
k
y = x+r K.
k
a ∈
∥ k ∥V
(b) Declare that E(x,r2V) K if y K for all 1 k m.
k
⊂ ∈ ≤ ≤
(c) Otherwise, let k be such that y / K so that
k
∈
K y : y y ,a 0 ≜ H.
k k
⊂ { ⟨ − ⟩ ≤ }
(d) Let E(y,W) = mvee(E(x,V) H ) and note that
∩ }
a ,x a ,y 1
k k k
⟨ ⟩−⟨ ⟩ = r = .
a 2d
(cid:12) ∥ k ∥V (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Therefore by Lemma(cid:12)10.3, vol(E(y,W)(cid:12)) exp 1 vol(E(x,V)).
≤ −20d
(cid:0) (cid:1)
Separation oracle When K is a separation oracle there is no straight-
forward procedure for checking whether or not E(x,r2V) K. What
⊂
we have instead is a procedure that accepts K and E as inputs and does
one of two things:
(a) Declares that E(x, r2V) K where r = 1 ; or
d ⊂ 2d
(b) Returns a new ellipsoid E(y,W) K such that Eq. (D.1) holds.
⊂
As for polytopes the procedure is simple:
(a) Letv ,...,v betheeigenvectorsofV andλ ,...,λ thecorrespond-
1 d 1 d
ing eigenvalues and y = x+rλ1/2v .
k k k
(b) Query the separation oracle on y ,...,y . Suppose that y / K,
1 d k
∈
then let H be the half-space returned by the separation oracle and return
E(y,W) = mvee(E(x,V) H).
∩
(c) Otherwise y ,...,y K and hence
1 d
⊂
E(x, r2V) conv(y ,...,y ) K.
d ⊂ 1 d ⊂COMPUTATION 157
Like for polytopes, the procedure is guaranteed to end after at most
vol(E )
1
O dlog
vol(K)
(cid:18) (cid:18) (cid:19)(cid:19)
iterations.
10
5
0
5
−
10
−
15
−
30 20 10 0 10 20
− − −
Figure D.1: The construction used for finding a near-optimal minimum
volume enclosed ellipsoid for a polytope. The outer ellipsoid is E(x,V)
and contains the polytope. The inner ellipsoid is E(x, 1 V). The pro-
r2
cedure tests if the supporting hyperplanes to E(x, 1 V) are separating
r2
hyperplanes for each normal defining the polytope. If none are, then
E(x, 1 V) is contained in K and otherwise you can continue the process
r2
with mvee(E H) for the separating hyperplane found.
∩