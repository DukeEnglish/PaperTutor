[
    {
        "title": "Image-based Deep Learning for the time-dependent prediction of fresh concrete properties",
        "authors": "Max MeyerAmadeus LangerMax MehltretterDries BeyerMax CoenenTobias SchackMichael HaistChristian Heipke",
        "links": "http://arxiv.org/abs/2402.06611v1",
        "entry_id": "http://arxiv.org/abs/2402.06611v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06611v1",
        "summary": "Increasing the degree of digitisation and automation in the concrete\nproduction process can play a crucial role in reducing the CO$_2$ emissions\nthat are associated with the production of concrete. In this paper, a method is\npresented that makes it possible to predict the properties of fresh concrete\nduring the mixing process based on stereoscopic image sequences of the\nconcretes flow behaviour. A Convolutional Neural Network (CNN) is used for the\nprediction, which receives the images supported by information on the mix\ndesign as input. In addition, the network receives temporal information in the\nform of the time difference between the time at which the images are taken and\nthe time at which the reference values of the concretes are carried out. With\nthis temporal information, the network implicitly learns the time-dependent\nbehaviour of the concretes properties. The network predicts the slump flow\ndiameter, the yield stress and the plastic viscosity. The time-dependent\nprediction potentially opens up the pathway to determine the temporal\ndevelopment of the fresh concrete properties already during mixing. This\nprovides a huge advantage for the concrete industry. As a result,\ncountermeasures can be taken in a timely manner. It is shown that an approach\nbased on depth and optical flow images, supported by information of the mix\ndesign, achieves the best results.",
        "updated": "2024-02-09 18:42:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06611v1"
    },
    {
        "title": "On the Out-Of-Distribution Generalization of Multimodal Large Language Models",
        "authors": "Xingxuan ZhangJiansheng LiWenjing ChuJunjia HaiRenzhe XuYuqing YangShikai GuanJiazheng XuPeng Cui",
        "links": "http://arxiv.org/abs/2402.06599v1",
        "entry_id": "http://arxiv.org/abs/2402.06599v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06599v1",
        "summary": "We investigate the generalization boundaries of current Multimodal Large\nLanguage Models (MLLMs) via comprehensive evaluation under out-of-distribution\nscenarios and domain-specific tasks. We evaluate their zero-shot generalization\nacross synthetic images, real-world distributional shifts, and specialized\ndatasets like medical and molecular imagery. Empirical results indicate that\nMLLMs struggle with generalization beyond common training domains, limiting\ntheir direct application without adaptation. To understand the cause of\nunreliable performance, we analyze three hypotheses: semantic\nmisinterpretation, visual feature extraction insufficiency, and mapping\ndeficiency. Results identify mapping deficiency as the primary hurdle. To\naddress this problem, we show that in-context learning (ICL) can significantly\nenhance MLLMs' generalization, opening new avenues for overcoming\ngeneralization barriers. We further explore the robustness of ICL under\ndistribution shifts and show its vulnerability to domain shifts, label shifts,\nand spurious correlation shifts between in-context examples and test data.",
        "updated": "2024-02-09 18:21:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06599v1"
    },
    {
        "title": "More than the Sum of Its Parts: Ensembling Backbone Networks for Few-Shot Segmentation",
        "authors": "Nico CatalanoAlessandro MaranelliAgnese ChiattiMatteo Matteucci",
        "links": "http://arxiv.org/abs/2402.06581v1",
        "entry_id": "http://arxiv.org/abs/2402.06581v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06581v1",
        "summary": "Semantic segmentation is a key prerequisite to robust image understanding for\napplications in \\acrlong{ai} and Robotics. \\acrlong{fss}, in particular,\nconcerns the extension and optimization of traditional segmentation methods in\nchallenging conditions where limited training examples are available. A\npredominant approach in \\acrlong{fss} is to rely on a single backbone for\nvisual feature extraction. Choosing which backbone to leverage is a deciding\nfactor contributing to the overall performance. In this work, we interrogate on\nwhether fusing features from different backbones can improve the ability of\n\\acrlong{fss} models to capture richer visual features. To tackle this\nquestion, we propose and compare two ensembling techniques-Independent Voting\nand Feature Fusion. Among the available \\acrlong{fss} methods, we implement the\nproposed ensembling techniques on PANet. The module dedicated to predicting\nsegmentation masks from the backbone embeddings in PANet avoids trainable\nparameters, creating a controlled `in vitro' setting for isolating the impact\nof different ensembling strategies. Leveraging the complementary strengths of\ndifferent backbones, our approach outperforms the original single-backbone\nPANet across standard benchmarks even in challenging one-shot learning\nscenarios. Specifically, it achieved a performance improvement of +7.37\\% on\nPASCAL-5\\textsuperscript{i} and of +10.68\\% on COCO-20\\textsuperscript{i} in\nthe top-performing scenario where three backbones are combined. These results,\ntogether with the qualitative inspection of the predicted subject masks,\nsuggest that relying on multiple backbones in PANet leads to a more\ncomprehensive feature representation, thus expediting the successful\napplication of \\acrlong{fss} methods in challenging, data-scarce environments.",
        "updated": "2024-02-09 18:01:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06581v1"
    },
    {
        "title": "Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learning",
        "authors": "Amir ZiaiAneesh Vartakavi",
        "links": "http://arxiv.org/abs/2402.06560v1",
        "entry_id": "http://arxiv.org/abs/2402.06560v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06560v1",
        "summary": "High-quality and consistent annotations are fundamental to the successful\ndevelopment of robust machine learning models. Traditional data annotation\nmethods are resource-intensive and inefficient, often leading to a reliance on\nthird-party annotators who are not the domain experts. Hard samples, which are\nusually the most informative for model training, tend to be difficult to label\naccurately and consistently without business context. These can arise\nunpredictably during the annotation process, requiring a variable number of\niterations and rounds of feedback, leading to unforeseen expenses and time\ncommitments to guarantee quality.\n  We posit that more direct involvement of domain experts, using a\nhuman-in-the-loop system, can resolve many of these practical challenges. We\npropose a novel framework we call Video Annotator (VA) for annotating,\nmanaging, and iterating on video classification datasets. Our approach offers a\nnew paradigm for an end-user-centered model development process, enhancing the\nefficiency, usability, and effectiveness of video classifiers. Uniquely, VA\nallows for a continuous annotation process, seamlessly integrating data\ncollection and model training.\n  We leverage the zero-shot capabilities of vision-language foundation models\ncombined with active learning techniques, and demonstrate that VA enables the\nefficient creation of high-quality models. VA achieves a median 6.8 point\nimprovement in Average Precision relative to the most competitive baseline\nacross a wide-ranging assortment of tasks. We release a dataset with 153k\nlabels across 56 video understanding tasks annotated by three professional\nvideo editors using VA, and also release code to replicate our experiments at:\nhttp://github.com/netflix/videoannotator.",
        "updated": "2024-02-09 17:19:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06560v1"
    },
    {
        "title": "Hybridnet for depth estimation and semantic segmentation",
        "authors": "Dalila Sánchez-EscobedoXiao LinJosep R. CasasMontse Pardàs",
        "links": "http://dx.doi.org/10.1109/ICASSP.2018.8462433",
        "entry_id": "http://arxiv.org/abs/2402.06539v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06539v1",
        "summary": "Semantic segmentation and depth estimation are two important tasks in the\narea of image processing. Traditionally, these two tasks are addressed in an\nindependent manner. However, for those applications where geometric and\nsemantic information is required, such as robotics or autonomous\nnavigation,depth or semantic segmentation alone are not sufficient. In this\npaper, depth estimation and semantic segmentation are addressed together from a\nsingle input image through a hybrid convolutional network. Different from the\nstate of the art methods where features are extracted by a sole feature\nextraction network for both tasks, the proposed HybridNet improves the features\nextraction by separating the relevant features for one task from those which\nare relevant for both. Experimental results demonstrate that HybridNet results\nare comparable with the state of the art methods, as well as the single task\nmethods that HybridNet is based on.",
        "updated": "2024-02-09 16:52:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06539v1"
    }
]