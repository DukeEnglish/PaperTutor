[
    {
        "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
        "authors": "Alexander PanErik JonesMeena JagadeesanJacob Steinhardt",
        "links": "http://arxiv.org/abs/2402.06627v1",
        "entry_id": "http://arxiv.org/abs/2402.06627v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06627v1",
        "summary": "Language models influence the external world: they query APIs that read and\nwrite to web pages, generate content that shapes human behavior, and run system\ncommands as autonomous agents. These interactions form feedback loops: LLM\noutputs affect the world, which in turn affect subsequent LLM outputs. In this\nwork, we show that feedback loops can cause in-context reward hacking (ICRH),\nwhere the LLM at test-time optimizes a (potentially implicit) objective but\ncreates negative side effects in the process. For example, consider an LLM\nagent deployed to increase Twitter engagement; the LLM may retrieve its\nprevious tweets into the context window and make them more controversial,\nincreasing engagement but also toxicity. We identify and study two processes\nthat lead to ICRH: output-refinement and policy-refinement. For these\nprocesses, evaluations on static datasets are insufficient -- they miss the\nfeedback effects and thus cannot capture the most harmful behavior. In\nresponse, we provide three recommendations for evaluation to capture more\ninstances of ICRH. As AI development accelerates, the effects of feedback loops\nwill proliferate, increasing the need to understand their role in shaping LLM\nbehavior.",
        "updated": "2024-02-09 18:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06627v1"
    },
    {
        "title": "The Complexity of Sequential Prediction in Dynamical Systems",
        "authors": "Vinod RamanUnique SubediAmbuj Tewari",
        "links": "http://arxiv.org/abs/2402.06614v1",
        "entry_id": "http://arxiv.org/abs/2402.06614v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06614v1",
        "summary": "We study the problem of learning to predict the next state of a dynamical\nsystem when the underlying evolution function is unknown. Unlike previous work,\nwe place no parametric assumptions on the dynamical system, and study the\nproblem from a learning theory perspective. We define new combinatorial\nmeasures and dimensions and show that they quantify the optimal mistake and\nregret bounds in the realizable and agnostic setting respectively.",
        "updated": "2024-02-09 18:45:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06614v1"
    },
    {
        "title": "RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization",
        "authors": "Ce FengParv Venkitasubramaniam",
        "links": "http://arxiv.org/abs/2402.06606v1",
        "entry_id": "http://arxiv.org/abs/2402.06606v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06606v1",
        "summary": "The rise of IoT devices has prompted the demand for deploying machine\nlearning at-the-edge with real-time, efficient, and secure data processing. In\nthis context, implementing machine learning (ML) models with real-valued weight\nparameters can prove to be impractical particularly for large models, and there\nis a need to train models with quantized discrete weights. At the same time,\nthese low-dimensional models also need to preserve privacy of the underlying\ndataset. In this work, we present RQP-SGD, a new approach for\nprivacy-preserving quantization to train machine learning models for low-memory\nML-at-the-edge. This approach combines differentially private stochastic\ngradient descent (DP-SGD) with randomized quantization, providing a measurable\nprivacy guarantee in machine learning. In particular, we study the utility\nconvergence of implementing RQP-SGD on ML tasks with convex objectives and\nquantization constraints and demonstrate its efficacy over deterministic\nquantization. Through experiments conducted on two datasets, we show the\npractical effectiveness of RQP-SGD.",
        "updated": "2024-02-09 18:34:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06606v1"
    },
    {
        "title": "Predictive representations: building blocks of intelligence",
        "authors": "Wilka CarvalhoMomchil S. TomovWilliam de CothiCaswell BarrySamuel J. Gershman",
        "links": "http://arxiv.org/abs/2402.06590v1",
        "entry_id": "http://arxiv.org/abs/2402.06590v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06590v1",
        "summary": "Adaptive behavior often requires predicting future events. The theory of\nreinforcement learning prescribes what kinds of predictive representations are\nuseful and how to compute them. This paper integrates these theoretical ideas\nwith work on cognition and neuroscience. We pay special attention to the\nsuccessor representation (SR) and its generalizations, which have been widely\napplied both as engineering tools and models of brain function. This\nconvergence suggests that particular kinds of predictive representations may\nfunction as versatile building blocks of intelligence.",
        "updated": "2024-02-09 18:10:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06590v1"
    },
    {
        "title": "More than the Sum of Its Parts: Ensembling Backbone Networks for Few-Shot Segmentation",
        "authors": "Nico CatalanoAlessandro MaranelliAgnese ChiattiMatteo Matteucci",
        "links": "http://arxiv.org/abs/2402.06581v1",
        "entry_id": "http://arxiv.org/abs/2402.06581v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06581v1",
        "summary": "Semantic segmentation is a key prerequisite to robust image understanding for\napplications in \\acrlong{ai} and Robotics. \\acrlong{fss}, in particular,\nconcerns the extension and optimization of traditional segmentation methods in\nchallenging conditions where limited training examples are available. A\npredominant approach in \\acrlong{fss} is to rely on a single backbone for\nvisual feature extraction. Choosing which backbone to leverage is a deciding\nfactor contributing to the overall performance. In this work, we interrogate on\nwhether fusing features from different backbones can improve the ability of\n\\acrlong{fss} models to capture richer visual features. To tackle this\nquestion, we propose and compare two ensembling techniques-Independent Voting\nand Feature Fusion. Among the available \\acrlong{fss} methods, we implement the\nproposed ensembling techniques on PANet. The module dedicated to predicting\nsegmentation masks from the backbone embeddings in PANet avoids trainable\nparameters, creating a controlled `in vitro' setting for isolating the impact\nof different ensembling strategies. Leveraging the complementary strengths of\ndifferent backbones, our approach outperforms the original single-backbone\nPANet across standard benchmarks even in challenging one-shot learning\nscenarios. Specifically, it achieved a performance improvement of +7.37\\% on\nPASCAL-5\\textsuperscript{i} and of +10.68\\% on COCO-20\\textsuperscript{i} in\nthe top-performing scenario where three backbones are combined. These results,\ntogether with the qualitative inspection of the predicted subject masks,\nsuggest that relying on multiple backbones in PANet leads to a more\ncomprehensive feature representation, thus expediting the successful\napplication of \\acrlong{fss} methods in challenging, data-scarce environments.",
        "updated": "2024-02-09 18:01:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06581v1"
    }
]