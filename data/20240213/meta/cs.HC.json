[
    {
        "title": "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment",
        "authors": "Mingzhe XingRongkai ZhangHui XueQi ChenFan YangZhen Xiao",
        "links": "http://arxiv.org/abs/2402.06596v1",
        "entry_id": "http://arxiv.org/abs/2402.06596v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06596v1",
        "summary": "Large language models (LLMs) have empowered intelligent agents to execute\nintricate tasks within domain-specific software such as browsers and games.\nHowever, when applied to general-purpose software systems like operating\nsystems, LLM agents face three primary challenges. Firstly, the action space is\nvast and dynamic, posing difficulties for LLM agents to maintain an up-to-date\nunderstanding and deliver accurate responses. Secondly, real-world tasks often\nrequire inter-application cooperation}, demanding farsighted planning from LLM\nagents. Thirdly, agents need to identify optimal solutions aligning with user\nconstraints, such as security concerns and preferences. These challenges\nmotivate AndroidArena, an environment and benchmark designed to evaluate LLM\nagents on a modern operating system. To address high-cost of manpower, we\ndesign a scalable and semi-automated method to construct the benchmark. In the\ntask evaluation, AndroidArena incorporates accurate and adaptive metrics to\naddress the issue of non-unique solutions. Our findings reveal that even\nstate-of-the-art LLM agents struggle in cross-APP scenarios and adhering to\nspecific constraints. Additionally, we identify a lack of four key\ncapabilities, i.e., understanding, reasoning, exploration, and reflection, as\nprimary reasons for the failure of LLM agents. Furthermore, we provide\nempirical analysis on the failure of reflection, and improve the success rate\nby 27% with our proposed exploration strategy. This work is the first to\npresent valuable insights in understanding fine-grained weakness of LLM agents,\nand offers a path forward for future research in this area. Environment,\nbenchmark, and evaluation code for AndroidArena are released at\nhttps://github.com/AndroidArenaAgent/AndroidArena.",
        "updated": "2024-02-09 18:19:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06596v1"
    },
    {
        "title": "What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices",
        "authors": "Neslihan SuzenEvgeny M. MirkesDamian RolandJeremy LevesleyAlexander N. GorbanTim J. Coats",
        "links": "http://dx.doi.org/10.1109/BigData59044.2023.10386194",
        "entry_id": "http://arxiv.org/abs/2402.06563v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06563v1",
        "summary": "Electronic patient records (EPRs) produce a wealth of data but contain\nsignificant missing information. Understanding and handling this missing data\nis an important part of clinical data analysis and if left unaddressed could\nresult in bias in analysis and distortion in critical conclusions. Missing data\nmay be linked to health care professional practice patterns and imputation of\nmissing data can increase the validity of clinical decisions. This study\nfocuses on statistical approaches for understanding and interpreting the\nmissing data and machine learning based clinical data imputation using a single\ncentre's paediatric emergency data and the data from UK's largest clinical\naudit for traumatic injury database (TARN). In the study of 56,961 data points\nrelated to initial vital signs and observations taken on children presenting to\nan Emergency Department, we have shown that missing data are likely to be\nnon-random and how these are linked to health care professional practice\npatterns. We have then examined 79 TARN fields with missing values for 5,791\ntrauma cases. Singular Value Decomposition (SVD) and k-Nearest Neighbour (kNN)\nbased missing data imputation methods are used and imputation results against\nthe original dataset are compared and statistically tested. We have concluded\nthat the 1NN imputer is the best imputation which indicates a usual pattern of\nclinical decision making: find the most similar patients and take their\nattributes as imputation.",
        "updated": "2024-02-09 17:27:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06563v1"
    },
    {
        "title": "Scalable Interactive Machine Learning for Future Command and Control",
        "authors": "Anna MadisonEllen NovosellerVinicius G. GoecksBenjamin T. FilesNicholas WaytowichAlfred YuVernon J. LawhernSteven ThurmanChristopher KelshawKaleb McDowell",
        "links": "http://arxiv.org/abs/2402.06501v1",
        "entry_id": "http://arxiv.org/abs/2402.06501v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06501v1",
        "summary": "Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.",
        "updated": "2024-02-09 16:11:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06501v1"
    },
    {
        "title": "\"When He Feels Cold, He Goes to the Seahorse\"-Blending Generative AI into Multimaterial Storymaking for Family Expressive Arts Therapy",
        "authors": "Di LiuHanqing ZhouPengcheng An",
        "links": "http://dx.doi.org/10.1145/3613904.3642852",
        "entry_id": "http://arxiv.org/abs/2402.06472v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06472v1",
        "summary": "Storymaking, as an integrative form of expressive arts therapy, is an\neffective means to foster family communication. Yet, the integration of\ngenerative AI as expressive materials in therapeutic storymaking remains\nunderexplored. And there is a lack of HCI implications on how to support\nfamilies and therapists in this context. Addressing this, our study involved\nfive weeks of storymaking sessions with seven families guided by a professional\ntherapist. In these sessions, the families used both traditional art-making\nmaterials and image-based generative AI to create and evolve their family\nstories. Via the rich empirical data and commentaries from four expert\ntherapists, we contextualize how families creatively melded AI and traditional\nexpressive materials to externalize their ideas and feelings. Through the lens\nof Expressive Therapies Continuum (ETC), we characterize the therapeutic\nimplications of AI as expressive materials. Desirable interaction qualities to\nsupport children, parents, and therapists are distilled for future HCI\nresearch.",
        "updated": "2024-02-09 15:25:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06472v1"
    },
    {
        "title": "What's in People's Digital File Collections?",
        "authors": "Jesse David DinneenCharles-Antoine Julien",
        "links": "http://dx.doi.org/10.1002/pra2.64",
        "entry_id": "http://arxiv.org/abs/2402.06421v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06421v1",
        "summary": "Thoughtfully designing services and rigorously testing software to support\npersonal information management (PIM) requires understanding the relevant\ncollections, but relatively little is known about what people keep in their\nfile collections, especially personal collections. Complementing recent work on\nthe structure of 348 file collections, we examine those collections' contents,\nhow much content is duplicated, and how collections used for personal matters\ndiffer from those used for study and work. Though all collections contain many\nimages, some intuitively common file types are surprisingly scarce. Personal\ncollections contain more audio than others, knowledge workers' collections\ncontain more text documents but far fewer folders, and IT collections exhibit\nunusual traits. Collection duplication is correlated to collections' structural\ntraits, but surprisingly, not to collection age. We discuss our findings in\nlight of prior works and provide implications for various kinds of information\nresearch.",
        "updated": "2024-02-09 14:09:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06421v1"
    }
]