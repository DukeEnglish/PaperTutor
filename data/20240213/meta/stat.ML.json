[
    {
        "title": "The Complexity of Sequential Prediction in Dynamical Systems",
        "authors": "Vinod RamanUnique SubediAmbuj Tewari",
        "links": "http://arxiv.org/abs/2402.06614v1",
        "entry_id": "http://arxiv.org/abs/2402.06614v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06614v1",
        "summary": "We study the problem of learning to predict the next state of a dynamical\nsystem when the underlying evolution function is unknown. Unlike previous work,\nwe place no parametric assumptions on the dynamical system, and study the\nproblem from a learning theory perspective. We define new combinatorial\nmeasures and dimensions and show that they quantify the optimal mistake and\nregret bounds in the realizable and agnostic setting respectively.",
        "updated": "2024-02-09 18:45:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06614v1"
    },
    {
        "title": "On the Universality of Coupling-based Normalizing Flows",
        "authors": "Felix DraxlerStefan WahlChristoph SchnörrUllrich Köthe",
        "links": "http://arxiv.org/abs/2402.06578v1",
        "entry_id": "http://arxiv.org/abs/2402.06578v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06578v1",
        "summary": "We present a novel theoretical framework for understanding the expressive\npower of coupling-based normalizing flows such as RealNVP. Despite their\nprevalence in scientific applications, a comprehensive understanding of\ncoupling flows remains elusive due to their restricted architectures. Existing\ntheorems fall short as they require the use of arbitrarily ill-conditioned\nneural networks, limiting practical applicability. Additionally, we demonstrate\nthat these constructions inherently lead to volume-preserving flows, a property\nwhich we show to be a fundamental constraint for expressivity. We propose a new\ndistributional universality theorem for coupling-based normalizing flows, which\novercomes several limitations of prior work. Our results support the general\nwisdom that the coupling architecture is expressive and provide a nuanced view\nfor choosing the expressivity of coupling functions, bridging a gap between\nempirical results and theoretical understanding.",
        "updated": "2024-02-09 17:51:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06578v1"
    },
    {
        "title": "Bandit Convex Optimisation",
        "authors": "Tor Lattimore",
        "links": "http://arxiv.org/abs/2402.06535v1",
        "entry_id": "http://arxiv.org/abs/2402.06535v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06535v1",
        "summary": "Bandit convex optimisation is a fundamental framework for studying\nzeroth-order convex optimisation. These notes cover the many tools used for\nthis problem, including cutting plane methods, interior point methods,\ncontinuous exponential weights, gradient descent and online Newton step. The\nnuances between the many assumptions and setups are explained. Although there\nis not much truly new here, some existing tools are applied in novel ways to\nobtain new algorithms. A few bounds are improved in minor ways.",
        "updated": "2024-02-09 16:49:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06535v1"
    },
    {
        "title": "Flexible infinite-width graph convolutional networks and the importance of representation learning",
        "authors": "Ben AnsonEdward MilsomLaurence Aitchison",
        "links": "http://arxiv.org/abs/2402.06525v1",
        "entry_id": "http://arxiv.org/abs/2402.06525v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06525v1",
        "summary": "A common theoretical approach to understanding neural networks is to take an\ninfinite-width limit, at which point the outputs become Gaussian process (GP)\ndistributed. This is known as a neural network Gaussian process (NNGP).\nHowever, the NNGP kernel is fixed, and tunable only through a small number of\nhyperparameters, eliminating any possibility of representation learning. This\ncontrasts with finite-width NNs, which are often believed to perform well\nprecisely because they are able to learn representations. Thus in simplifying\nNNs to make them theoretically tractable, NNGPs may eliminate precisely what\nmakes them work well (representation learning). This motivated us to understand\nwhether representation learning is necessary in a range of graph classification\ntasks. We develop a precise tool for this task, the graph convolutional deep\nkernel machine. This is very similar to an NNGP, in that it is an infinite\nwidth limit and uses kernels, but comes with a `knob' to control the amount of\nrepresentation learning. We found that representation learning is necessary (in\nthe sense that it gives dramatic performance improvements) in graph\nclassification tasks and heterophilous node classification tasks, but not in\nhomophilous node classification tasks.",
        "updated": "2024-02-09 16:37:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06525v1"
    },
    {
        "title": "Sequential Flow Matching for Generative Modeling",
        "authors": "Jongmin YoonJuho Lee",
        "links": "http://arxiv.org/abs/2402.06461v1",
        "entry_id": "http://arxiv.org/abs/2402.06461v1",
        "pdf_url": "http://arxiv.org/pdf/2402.06461v1",
        "summary": "Straightening the probability flow of the continuous-time generative models,\nsuch as diffusion models or flow-based models, is the key to fast sampling\nthrough the numerical solvers, existing methods learn a linear path by directly\ngenerating the probability path the joint distribution between the noise and\ndata distribution. One key reason for the slow sampling speed of the ODE-based\nsolvers that simulate these generative models is the global truncation error of\nthe ODE solver, caused by the high curvature of the ODE trajectory, which\nexplodes the truncation error of the numerical solvers in the low-NFE regime.\nTo address this challenge, We propose a novel method called SeqRF, a learning\ntechnique that straightens the probability flow to reduce the global truncation\nerror and hence enable acceleration of sampling and improve the synthesis\nquality. In both theoretical and empirical studies, we first observe the\nstraightening property of our SeqRF. Through empirical evaluations via SeqRF\nover flow-based generative models, We achieve surpassing results on CIFAR-10,\nCelebA-$64 \\times 64$, and LSUN-Church datasets.",
        "updated": "2024-02-09 15:09:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.06461v1"
    }
]