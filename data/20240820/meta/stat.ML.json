[
    {
        "title": "Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification",
        "authors": "Jing Li",
        "links": "http://arxiv.org/abs/2408.10193v1",
        "entry_id": "http://arxiv.org/abs/2408.10193v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10193v1",
        "summary": "Evaluation Metrics is an important question for model evaluation and model\nselection in binary classification tasks. This study investigates how\nconsistent metrics are at evaluating different models under different data\nscenarios. Analyzing over 150 data scenarios and 18 model evaluation metrics\nusing statistical simulation, I find that for binary classification tasks,\nevaluation metrics that are less influenced by prevalence offer more consistent\nranking of a set of different models. In particular, Area Under the ROC Curve\n(AUC) has smallest variance in ranking of different models. Matthew's\ncorrelation coefficient as a more strict measure of model performance has the\nsecond smallest variance. These patterns holds across a rich set of data\nscenarios and five commonly used machine learning models as well as a naive\nrandom guess model. The results have significant implications for model\nevaluation and model selection in binary classification tasks.",
        "updated": "2024-08-19 17:52:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10193v1"
    },
    {
        "title": "Robust spectral clustering with rank statistics",
        "authors": "Joshua CapeXianshi YuJonquil Z. Liao",
        "links": "http://arxiv.org/abs/2408.10136v1",
        "entry_id": "http://arxiv.org/abs/2408.10136v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10136v1",
        "summary": "This paper analyzes the statistical performance of a robust spectral\nclustering method for latent structure recovery in noisy data matrices. We\nconsider eigenvector-based clustering applied to a matrix of nonparametric rank\nstatistics that is derived entrywise from the raw, original data matrix. This\napproach is robust in the sense that, unlike traditional spectral clustering\nprocedures, it can provably recover population-level latent block structure\neven when the observed data matrix includes heavy-tailed entries and has a\nheterogeneous variance profile.\n  Our main theoretical contributions are threefold and hold under flexible data\ngenerating conditions. First, we establish that robust spectral clustering with\nrank statistics can consistently recover latent block structure, viewed as\ncommunities of nodes in a graph, in the sense that unobserved community\nmemberships for all but a vanishing fraction of nodes are correctly recovered\nwith high probability when the data matrix is large. Second, we refine the\nformer result and further establish that, under certain conditions, the\ncommunity membership of any individual, specified node of interest can be\nasymptotically exactly recovered with probability tending to one in the\nlarge-data limit. Third, we establish asymptotic normality results associated\nwith the truncated eigenstructure of matrices whose entries are rank\nstatistics, made possible by synthesizing contemporary entrywise matrix\nperturbation analysis with the classical nonparametric theory of so-called\nsimple linear rank statistics. Collectively, these results demonstrate the\nstatistical utility of rank-based data transformations when paired with\nspectral techniques for dimensionality reduction. Additionally, for a dataset\nof human connectomes, our approach yields parsimonious dimensionality reduction\nand improved recovery of ground-truth neuroanatomical cluster structure.",
        "updated": "2024-08-19 16:33:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10136v1"
    },
    {
        "title": "Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments",
        "authors": "Heeyoung LeeHoyoon ByunChangdae OhJinYeong BakKyungwoo Song",
        "links": "http://arxiv.org/abs/2408.10107v1",
        "entry_id": "http://arxiv.org/abs/2408.10107v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10107v1",
        "summary": "Accessing machine learning models through remote APIs has been gaining\nprevalence following the recent trend of scaling up model parameters for\nincreased performance. Even though these models exhibit remarkable ability,\ndetecting out-of-distribution (OOD) samples remains a crucial safety concern\nfor end users as these samples may induce unreliable outputs from the model. In\nthis work, we propose an OOD detection framework, MixDiff, that is applicable\neven when the model's parameters or its activations are not accessible to the\nend user. To bypass the access restriction, MixDiff applies an identical\ninput-level perturbation to a given target sample and a similar in-distribution\n(ID) sample, then compares the relative difference in the model outputs of\nthese two samples. MixDiff is model-agnostic and compatible with existing\noutput-based OOD detection methods. We provide theoretical analysis to\nillustrate MixDiff's effectiveness in discerning OOD samples that induce\noverconfident outputs from the model and empirically demonstrate that MixDiff\nconsistently enhances the OOD detection performance on various datasets in\nvision and text domains.",
        "updated": "2024-08-19 15:51:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10107v1"
    },
    {
        "title": "Parseval Convolution Operators and Neural Networks",
        "authors": "Michael UnserStanislas Ducotterd",
        "links": "http://arxiv.org/abs/2408.09981v1",
        "entry_id": "http://arxiv.org/abs/2408.09981v1",
        "pdf_url": "http://arxiv.org/pdf/2408.09981v1",
        "summary": "We first establish a kernel theorem that characterizes all linear\nshift-invariant (LSI) operators acting on discrete multicomponent signals. This\nresult naturally leads to the identification of the Parseval convolution\noperators as the class of energy-preserving filterbanks. We then present a\nconstructive approach for the design/specification of such filterbanks via the\nchaining of elementary Parseval modules, each of which being parameterized by\nan orthogonal matrix or a 1-tight frame. Our analysis is complemented with\nexplicit formulas for the Lipschitz constant of all the components of a\nconvolutional neural network (CNN), which gives us a handle on their stability.\nFinally, we demonstrate the usage of those tools with the design of a CNN-based\nalgorithm for the iterative reconstruction of biomedical images. Our algorithm\nfalls within the plug-and-play framework for the resolution of inverse\nproblems. It yields better-quality results than the sparsity-based methods used\nin compressed sensing, while offering essentially the same convergence and\nrobustness guarantees.",
        "updated": "2024-08-19 13:31:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.09981v1"
    },
    {
        "title": "Predicting path-dependent processes by deep learning",
        "authors": "Xudong ZhengYuecai Han",
        "links": "http://arxiv.org/abs/2408.09941v1",
        "entry_id": "http://arxiv.org/abs/2408.09941v1",
        "pdf_url": "http://arxiv.org/pdf/2408.09941v1",
        "summary": "In this paper, we investigate a deep learning method for predicting\npath-dependent processes based on discretely observed historical information.\nThis method is implemented by considering the prediction as a nonparametric\nregression and obtaining the regression function through simulated samples and\ndeep neural networks. When applying this method to fractional Brownian motion\nand the solutions of some stochastic differential equations driven by it, we\ntheoretically proved that the $L_2$ errors converge to 0, and we further\ndiscussed the scope of the method. With the frequency of discrete observations\ntending to infinity, the predictions based on discrete observations converge to\nthe predictions based on continuous observations, which implies that we can\nmake approximations by the method. We apply the method to the fractional\nBrownian motion and the fractional Ornstein-Uhlenbeck process as examples.\nComparing the results with the theoretical optimal predictions and taking the\nmean square error as a measure, the numerical simulations demonstrate that the\nmethod can generate accurate results. We also analyze the impact of factors\nsuch as prediction period, Hurst index, etc. on the accuracy.",
        "updated": "2024-08-19 12:24:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.09941v1"
    }
]