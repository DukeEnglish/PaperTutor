[
    {
        "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
        "authors": "Fuzhao XueYukang ChenDacheng LiQinghao HuLigeng ZhuXiuyu LiYunhao FangHaotian TangShang YangZhijian LiuEthan HeHongxu YinPavlo MolchanovJan KautzLinxi FanYuke ZhuYao LuSong Han",
        "links": "http://arxiv.org/abs/2408.10188v1",
        "entry_id": "http://arxiv.org/abs/2408.10188v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10188v1",
        "summary": "Long-context capability is critical for multi-modal foundation models. We\nintroduce LongVILA, a full-stack solution for long-context vision-language\nmodels, including system, model training, and dataset development. On the\nsystem side, we introduce the first Multi-Modal Sequence Parallelism (MM-SP)\nsystem that enables long-context training and inference, enabling 2M context\nlength training on 256 GPUs. MM-SP is also efficient, being 2.1x - 5.7x faster\nthan Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM in\ntext-only settings. Moreover, it seamlessly integrates with Hugging Face\nTransformers. For model training, we propose a five-stage pipeline comprising\nalignment, pre-training, context extension, and long-short joint supervised\nfine-tuning. Regarding datasets, we meticulously construct large-scale visual\nlanguage pre-training datasets and long video instruction-following datasets to\nsupport our multi-stage training process. The full-stack solution extends the\nfeasible frame number of VILA by a factor of 128 (from 8 to 1024 frames) and\nimproves long video captioning score from 2.00 to 3.26 (1.6x), achieving 99.5%\naccuracy in 1400-frames video (274k context length) needle in a haystack.\nLongVILA-8B also demonstrates a consistent improvement in performance on long\nvideos within the VideoMME benchmark as the video frames increase.",
        "updated": "2024-08-19 17:48:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10188v1"
    },
    {
        "title": "Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models",
        "authors": "Amey HenglePrasoon BajpaiSoham DanTanmoy Chakraborty",
        "links": "http://arxiv.org/abs/2408.10151v1",
        "entry_id": "http://arxiv.org/abs/2408.10151v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10151v1",
        "summary": "While recent large language models (LLMs) demonstrate remarkable abilities in\nresponding to queries in diverse languages, their ability to handle long\nmultilingual contexts is unexplored. As such, a systematic evaluation of the\nlong-context capabilities of LLMs in multilingual settings is crucial,\nspecifically in the context of information retrieval. To address this gap, we\nintroduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to\nassess a model's ability to retrieve relevant information (the needle) from a\ncollection of multilingual distractor texts (the haystack). This test serves as\nan extension of the multilingual question-answering task, encompassing both\nmonolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs\non MLNeedle. Our findings reveal that model performance can vary significantly\nwith language and needle position. Specifically, we observe that model\nperformance is the lowest when the needle is (i) in a language outside the\nEnglish language family and (ii) located in the middle of the input context.\nFurthermore, although some models claim a context size of $8k$ tokens or\ngreater, none demonstrate satisfactory cross-lingual retrieval performance as\nthe context length increases. Our analysis provides key insights into the\nlong-context behavior of LLMs in multilingual settings to guide future\nevaluation protocols. To our knowledge, this is the first study to investigate\nthe multilingual long-context behavior of LLMs.",
        "updated": "2024-08-19 17:02:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10151v1"
    },
    {
        "title": "In-Context Learning with Representations: Contextual Generalization of Trained Transformers",
        "authors": "Tong YangYu HuangYingbin LiangYuejie Chi",
        "links": "http://arxiv.org/abs/2408.10147v1",
        "entry_id": "http://arxiv.org/abs/2408.10147v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10147v1",
        "summary": "In-context learning (ICL) refers to a remarkable capability of pretrained\nlarge language models, which can learn a new task given a few examples during\ninference. However, theoretical understanding of ICL is largely under-explored,\nparticularly whether transformers can be trained to generalize to unseen\nexamples in a prompt, which will require the model to acquire contextual\nknowledge of the prompt for generalization. This paper investigates the\ntraining dynamics of transformers by gradient descent through the lens of\nnon-linear regression tasks. The contextual generalization here can be attained\nvia learning the template function for each task in-context, where all template\nfunctions lie in a linear space with $m$ basis functions. We analyze the\ntraining dynamics of one-layer multi-head transformers to in-contextly predict\nunlabeled inputs given partially labeled prompts, where the labels contain\nGaussian noise and the number of examples in each prompt are not sufficient to\ndetermine the template. Under mild assumptions, we show that the training loss\nfor a one-layer multi-head transformer converges linearly to a global minimum.\nMoreover, the transformer effectively learns to perform ridge regression over\nthe basis functions. To our knowledge, this study is the first provable\ndemonstration that transformers can learn contextual (i.e., template)\ninformation to generalize to both unseen examples and tasks when prompts\ncontain only a small number of query-answer pairs.",
        "updated": "2024-08-19 16:47:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10147v1"
    },
    {
        "title": "Instruction Finetuning for Leaderboard Generation from Empirical AI Research",
        "authors": "Salomon KabongoJennifer D'Souza",
        "links": "http://arxiv.org/abs/2408.10141v1",
        "entry_id": "http://arxiv.org/abs/2408.10141v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10141v1",
        "summary": "This study demonstrates the application of instruction finetuning of\npretrained Large Language Models (LLMs) to automate the generation of AI\nresearch leaderboards, extracting (Task, Dataset, Metric, Score) quadruples\nfrom articles. It aims to streamline the dissemination of advancements in AI\nresearch by transitioning from traditional, manual community curation, or\notherwise taxonomy-constrained natural language inference (NLI) models, to an\nautomated, generative LLM-based approach. Utilizing the FLAN-T5 model, this\nresearch enhances LLMs' adaptability and reliability in information extraction,\noffering a novel method for structured knowledge representation.",
        "updated": "2024-08-19 16:41:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10141v1"
    },
    {
        "title": "Rhyme-aware Chinese lyric generator based on GPT",
        "authors": "Yixiao YuanYangchen HuangYu MaXinjin LiZhenglin LiYiming ShiHuapeng Zhou",
        "links": "http://arxiv.org/abs/2408.10130v1",
        "entry_id": "http://arxiv.org/abs/2408.10130v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10130v1",
        "summary": "Neural language representation models such as GPT, pre-trained on large-scale\ncorpora, can effectively capture rich semantic patterns from plain text and be\nfine-tuned to consistently improve natural language generation performance.\nHowever, existing pre-trained language models used to generate lyrics rarely\nconsider rhyme information, which is crucial in lyrics. Using a pre-trained\nmodel directly results in poor performance. To enhance the rhyming quality of\ngenerated lyrics, we incorporate integrated rhyme information into our model,\nthereby improving lyric generation performance.",
        "updated": "2024-08-19 16:17:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10130v1"
    }
]