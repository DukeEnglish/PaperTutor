[
    {
        "title": "A Graph-based Approach to Human Activity Recognition",
        "authors": "Thomas PeroutkaIlir MurturiPraveen Kumar DontaSchahram Dustdar",
        "links": "http://arxiv.org/abs/2408.10191v1",
        "entry_id": "http://arxiv.org/abs/2408.10191v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10191v1",
        "summary": "Advanced wearable sensor devices have enabled the recording of vast amounts\nof movement data from individuals regarding their physical activities. This\ndata offers valuable insights that enhance our understanding of how physical\nactivities contribute to improved physical health and overall quality of life.\nConsequently, there is a growing need for efficient methods to extract\nsignificant insights from these rapidly expanding real-time datasets. This\npaper presents a methodology to efficiently extract substantial insights from\nthese expanding datasets, focusing on professional sports but applicable to\nvarious human activities. By utilizing data from Inertial Measurement Units\n(IMU) and Global Navigation Satellite Systems (GNSS) receivers, athletic\nperformance can be analyzed using directed graphs to encode knowledge of\ncomplex movements. Our approach is demonstrated on biathlon data and detects\nspecific points of interest and complex movement sequences, facilitating the\ncomparison and analysis of human physical performance.",
        "updated": "2024-08-19 17:51:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10191v1"
    },
    {
        "title": "Envisioning Possibilities and Challenges of AI for Personalized Cancer Care",
        "authors": "Elaine KongKuo-TingHuangAakash Gautam",
        "links": "http://dx.doi.org/10.1145/3678884.3681885",
        "entry_id": "http://arxiv.org/abs/2408.10108v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10108v1",
        "summary": "The use of Artificial Intelligence (AI) in healthcare, including in caring\nfor cancer survivors, has gained significant interest. However, gaps remain in\nour understanding of how such AI systems can provide care, especially for\nethnic and racial minority groups who continue to face care disparities.\nThrough interviews with six cancer survivors, we identify critical gaps in\ncurrent healthcare systems such as a lack of personalized care and insufficient\ncultural and linguistic accommodation. AI, when applied to care, was seen as a\nway to address these issues by enabling real-time, culturally aligned, and\nlinguistically appropriate interactions. We also uncovered concerns about the\nimplications of AI-driven personalization, such as data privacy, loss of human\ntouch in caregiving, and the risk of echo chambers that limit exposure to\ndiverse information. We conclude by discussing the trade-offs between\nAI-enhanced personalization and the need for structural changes in healthcare\nthat go beyond technological solutions, leading us to argue that we should\nbegin by asking, ``Why personalization?''",
        "updated": "2024-08-19 15:55:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10108v1"
    },
    {
        "title": "Working in Extended Reality in the Wild: Worker and Bystander Experiences of XR Virtual Displays in Real-World Settings",
        "authors": "Leonardo PavanattoVerena BienerJennifer ChandranSnehanjali KalamkarFeiyu LuJohn J. DudleyJinghui HuG. Nikki Ramirez-SaffyPer Ola KristenssonAlexander GiovannelliLuke SchlueterJörg MüllerJens GrubertDoug A. Bowman",
        "links": "http://arxiv.org/abs/2408.10000v1",
        "entry_id": "http://arxiv.org/abs/2408.10000v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10000v1",
        "summary": "Although access to sufficient screen space is crucial to knowledge work,\nworkers often find themselves with limited access to display infrastructure in\nremote or public settings. While virtual displays can be used to extend the\navailable screen space through extended reality (XR) head-worn displays (HWD),\nwe must better understand the implications of working with them in public\nsettings from both users' and bystanders' viewpoints. To this end, we conducted\ntwo user studies. We first explored the usage of a hybrid AR display across\nreal-world settings and tasks. We focused on how users take advantage of\nvirtual displays and what social and environmental factors impact their usage\nof the system. A second study investigated the differences between working with\na laptop, an AR system, or a VR system in public. We focused on a single\nlocation and participants performed a predefined task to enable direct\ncomparisons between the conditions while also gathering data from bystanders.\nThe combined results suggest a positive acceptance of XR technology in public\nsettings and show that virtual displays can be used to accompany existing\ndevices. We highlighted some environmental and social factors. We saw that\nprevious XR experience and personality can influence how people perceive the\nuse of XR in public. In addition, we confirmed that using XR in public still\nmakes users stand out and that bystanders are curious about the devices, yet\nhave no clear understanding of how they can be used.",
        "updated": "2024-08-19 13:53:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10000v1"
    },
    {
        "title": "WoW -- A System for Self-Service Collaborative Design Workshops",
        "authors": "Ilyasse BelkacemVasile CiornaFrank PetryMohammad Ghoniem",
        "links": "http://arxiv.org/abs/2408.09926v1",
        "entry_id": "http://arxiv.org/abs/2408.09926v1",
        "pdf_url": "http://arxiv.org/pdf/2408.09926v1",
        "summary": "In many working environments, users have to solve complex problems relying on\nlarge and multi-source data. Such problems require several experts to\ncollaborate on solving them, or a single analyst to reconcile multiple\ncomplementary standpoints. Previous research has shown that wall-sized displays\nsupports different collaboration styles, based most often on abstract tasks as\nproxies of real work. We present the design and implementation of WoW, short\nfor ``Workspace on Wall'', a multi-user Web-based portal for collaborative\nmeetings and workshops in multi-surface environments. We report on a two-year\neffort spanning context inquiry studies, system design iterations, development,\nand real testing rounds targeting design engineers in the tire industry. The\npneumatic tires found on the market result from a highly collaborative and\niterative development process that reconciles conflicting constraints through a\nseries of product design workshops. WoW was found to be a flexible solution to\nbuild multi-view set-ups in a self-service manner and an effective means to\naccess more content at once. Our users also felt more engaged in their\ncollaborative problem-solving work using WoW than in conventional meeting\nrooms.",
        "updated": "2024-08-19 12:03:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.09926v1"
    },
    {
        "title": "LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on Concept Discovery",
        "authors": "Weiji KongXun GongJuan Wang",
        "links": "http://arxiv.org/abs/2408.09899v1",
        "entry_id": "http://arxiv.org/abs/2408.09899v1",
        "pdf_url": "http://arxiv.org/pdf/2408.09899v1",
        "summary": "Explaining the decisions of Deep Neural Networks (DNNs) for medical images\nhas become increasingly important. Existing attribution methods have difficulty\nexplaining the meaning of pixels while existing concept-based methods are\nlimited by additional annotations or specific model structures that are\ndifficult to apply to ultrasound images. In this paper, we propose the Lesion\nConcept Explainer (LCE) framework, which combines attribution methods with\nconcept-based methods. We introduce the Segment Anything Model (SAM),\nfine-tuned on a large number of medical images, for concept discovery to enable\na meaningful explanation of ultrasound image DNNs. The proposed framework is\nevaluated in terms of both faithfulness and understandability. We point out\ndeficiencies in the popular faithfulness evaluation metrics and propose a new\nevaluation metric. Our evaluation of public and private breast ultrasound\ndatasets (BUSI and FG-US-B) shows that LCE performs well compared to\ncommonly-used explainability methods. Finally, we also validate that LCE can\nconsistently provide reliable explanations for more meaningful fine-grained\ndiagnostic tasks in breast ultrasound.",
        "updated": "2024-08-19 11:13:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.09899v1"
    }
]