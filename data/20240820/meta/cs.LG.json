[
    {
        "title": "KAN 2.0: Kolmogorov-Arnold Networks Meet Science",
        "authors": "Ziming LiuPingchuan MaYixuan WangWojciech MatusikMax Tegmark",
        "links": "http://arxiv.org/abs/2408.10205v1",
        "entry_id": "http://arxiv.org/abs/2408.10205v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10205v1",
        "summary": "A major challenge of AI + Science lies in their inherent incompatibility:\ntoday's AI is primarily based on connectionism, while science depends on\nsymbolism. To bridge the two worlds, we propose a framework to seamlessly\nsynergize Kolmogorov-Arnold Networks (KANs) and science. The framework\nhighlights KANs' usage for three aspects of scientific discovery: identifying\nrelevant features, revealing modular structures, and discovering symbolic\nformulas. The synergy is bidirectional: science to KAN (incorporating\nscientific knowledge into KANs), and KAN to science (extracting scientific\ninsights from KANs). We highlight major new functionalities in the pykan\npackage: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN\ncompiler that compiles symbolic formulas into KANs. (3) tree converter: convert\nKANs (or any neural networks) to tree graphs. Based on these tools, we\ndemonstrate KANs' capability to discover various types of physical laws,\nincluding conserved quantities, Lagrangians, symmetries, and constitutive laws.",
        "updated": "2024-08-19 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10205v1"
    },
    {
        "title": "Criticality Leveraged Adversarial Training (CLAT) for Boosted Performance via Parameter Efficiency",
        "authors": "Bhavna GopalHuanrui YangJingyang ZhangMark HortonYiran Chen",
        "links": "http://arxiv.org/abs/2408.10204v1",
        "entry_id": "http://arxiv.org/abs/2408.10204v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10204v1",
        "summary": "Adversarial training enhances neural network robustness but suffers from a\ntendency to overfit and increased generalization errors on clean data. This\nwork introduces CLAT, an innovative approach that mitigates adversarial\noverfitting by introducing parameter efficiency into the adversarial training\nprocess, improving both clean accuracy and adversarial robustness. Instead of\ntuning the entire model, CLAT identifies and fine-tunes robustness-critical\nlayers - those predominantly learning non-robust features - while freezing the\nremaining model to enhance robustness. It employs dynamic critical layer\nselection to adapt to changes in layer criticality throughout the fine-tuning\nprocess. Empirically, CLAT can be applied on top of existing adversarial\ntraining methods, significantly reduces the number of trainable parameters by\napproximately 95%, and achieves more than a 2% improvement in adversarial\nrobustness compared to baseline methods.",
        "updated": "2024-08-19 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10204v1"
    },
    {
        "title": "Area under the ROC Curve has the Most Consistent Evaluation for Binary Classification",
        "authors": "Jing Li",
        "links": "http://arxiv.org/abs/2408.10193v1",
        "entry_id": "http://arxiv.org/abs/2408.10193v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10193v1",
        "summary": "Evaluation Metrics is an important question for model evaluation and model\nselection in binary classification tasks. This study investigates how\nconsistent metrics are at evaluating different models under different data\nscenarios. Analyzing over 150 data scenarios and 18 model evaluation metrics\nusing statistical simulation, I find that for binary classification tasks,\nevaluation metrics that are less influenced by prevalence offer more consistent\nranking of a set of different models. In particular, Area Under the ROC Curve\n(AUC) has smallest variance in ranking of different models. Matthew's\ncorrelation coefficient as a more strict measure of model performance has the\nsecond smallest variance. These patterns holds across a rich set of data\nscenarios and five commonly used machine learning models as well as a naive\nrandom guess model. The results have significant implications for model\nevaluation and model selection in binary classification tasks.",
        "updated": "2024-08-19 17:52:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10193v1"
    },
    {
        "title": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models",
        "authors": "Aviv BickKevin Y. LiEric P. XingJ. Zico KolterAlbert Gu",
        "links": "http://arxiv.org/abs/2408.10189v1",
        "entry_id": "http://arxiv.org/abs/2408.10189v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10189v1",
        "summary": "Transformer architectures have become a dominant paradigm for domains like\nlanguage modeling but suffer in many inference settings due to their\nquadratic-time self-attention. Recently proposed subquadratic architectures,\nsuch as Mamba, have shown promise, but have been pretrained with substantially\nless computational resources than the strongest Transformer models. In this\nwork, we present a method that is able to distill a pretrained Transformer\narchitecture into alternative architectures such as state space models (SSMs).\nThe key idea to our approach is that we can view both Transformers and SSMs as\napplying different forms of mixing matrices over the token sequences. We can\nthus progressively distill the Transformer architecture by matching different\ndegrees of granularity in the SSM: first matching the mixing matrices\nthemselves, then the hidden units at each block, and finally the end-to-end\npredictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant\nbased on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid\nversion (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the\ntraining data typically used to train models from scratch, Phi-Mamba boasts\nsubstantially stronger performance compared to all past open-source\nnon-Transformer models. MOHAWK allows models like SSMs to leverage\ncomputational resources invested in training Transformer-based architectures,\nhighlighting a new avenue for building such models.",
        "updated": "2024-08-19 17:48:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10189v1"
    },
    {
        "title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models",
        "authors": "Anke TangLi ShenYong LuoShuai XieHan HuLefei ZhangBo DuDacheng Tao",
        "links": "http://arxiv.org/abs/2408.10174v1",
        "entry_id": "http://arxiv.org/abs/2408.10174v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10174v1",
        "summary": "Deep model training on extensive datasets is increasingly becoming\ncost-prohibitive, prompting the widespread adoption of deep model fusion\ntechniques to leverage knowledge from pre-existing models. From simple weight\naveraging to more sophisticated methods like AdaMerging, model fusion\neffectively improves model performance and accelerates the development of new\nmodels. However, potential interference between parameters of individual models\nand the lack of interpretability in the fusion progress remain significant\nchallenges. Existing methods often try to resolve the parameter interference\nissue by evaluating attributes of parameters, such as their magnitude or sign,\nor by parameter pruning. In this study, we begin by examining the fine-tuning\nof linear layers through the lens of subspace analysis and explicitly define\nparameter interference as an optimization problem to shed light on this\nsubject. Subsequently, we introduce an innovative approach to model fusion\ncalled zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which\nallows for the upscaling of source models into an MoE model without extra data\nor further training. Our approach relies on the observation that fine-tuning\nmostly keeps the important parts from the pre-training, but it uses less\nsignificant or unused areas to adapt to new tasks. Also, the issue of parameter\ninterference, which is intrinsically intractable in the original parameter\nspace, can be managed by expanding the dimensions. We conduct extensive\nexperiments across diverse scenarios, such as image classification and text\ngeneralization tasks, using full fine-tuning and LoRA fine-tuning, and we apply\nour method to large language models (CLIP models, Flan-T5 models, and\nMistral-7B models), highlighting the adaptability and scalability of SMILE.\nCode is available at https://github.com/tanganke/fusion_bench",
        "updated": "2024-08-19 17:32:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10174v1"
    }
]