[
    {
        "title": "Criticality Leveraged Adversarial Training (CLAT) for Boosted Performance via Parameter Efficiency",
        "authors": "Bhavna GopalHuanrui YangJingyang ZhangMark HortonYiran Chen",
        "links": "http://arxiv.org/abs/2408.10204v1",
        "entry_id": "http://arxiv.org/abs/2408.10204v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10204v1",
        "summary": "Adversarial training enhances neural network robustness but suffers from a\ntendency to overfit and increased generalization errors on clean data. This\nwork introduces CLAT, an innovative approach that mitigates adversarial\noverfitting by introducing parameter efficiency into the adversarial training\nprocess, improving both clean accuracy and adversarial robustness. Instead of\ntuning the entire model, CLAT identifies and fine-tunes robustness-critical\nlayers - those predominantly learning non-robust features - while freezing the\nremaining model to enhance robustness. It employs dynamic critical layer\nselection to adapt to changes in layer criticality throughout the fine-tuning\nprocess. Empirically, CLAT can be applied on top of existing adversarial\ntraining methods, significantly reduces the number of trainable parameters by\napproximately 95%, and achieves more than a 2% improvement in adversarial\nrobustness compared to baseline methods.",
        "updated": "2024-08-19 17:58:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10204v1"
    },
    {
        "title": "SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP",
        "authors": "Yusuke HirotaMin-Hung ChenChien-Yi WangYuta NakashimaYu-Chiang Frank WangRyo Hachiuma",
        "links": "http://arxiv.org/abs/2408.10202v1",
        "entry_id": "http://arxiv.org/abs/2408.10202v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10202v1",
        "summary": "Large-scale vision-language models, such as CLIP, are known to contain\nharmful societal bias regarding protected attributes (e.g., gender and age). In\nthis paper, we aim to address the problems of societal bias in CLIP. Although\nprevious studies have proposed to debias societal bias through adversarial\nlearning or test-time projecting, our comprehensive study of these works\nidentifies two critical limitations: 1) loss of attribute information when it\nis explicitly disclosed in the input and 2) use of the attribute annotations\nduring debiasing process. To mitigate societal bias in CLIP and overcome these\nlimitations simultaneously, we introduce a simple-yet-effective debiasing\nmethod called SANER (societal attribute neutralizer) that eliminates attribute\ninformation from CLIP text features only of attribute-neutral descriptions.\nExperimental results show that SANER, which does not require attribute\nannotations and preserves original information for attribute-specific\ndescriptions, demonstrates superior debiasing ability than the existing\nmethods.",
        "updated": "2024-08-19 17:57:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10202v1"
    },
    {
        "title": "MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model",
        "authors": "Minghua LiuChong ZengXinyue WeiRuoxi ShiLinghao ChenChao XuMengqi ZhangZhaoning WangXiaoshuai ZhangIsabella LiuHongzhi WuHao Su",
        "links": "http://arxiv.org/abs/2408.10198v1",
        "entry_id": "http://arxiv.org/abs/2408.10198v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10198v1",
        "summary": "Open-world 3D reconstruction models have recently garnered significant\nattention. However, without sufficient 3D inductive bias, existing methods\ntypically entail expensive training costs and struggle to extract high-quality\n3D meshes. In this work, we introduce MeshFormer, a sparse-view reconstruction\nmodel that explicitly leverages 3D native structure, input guidance, and\ntraining supervision. Specifically, instead of using a triplane representation,\nwe store features in 3D sparse voxels and combine transformers with 3D\nconvolutions to leverage an explicit 3D structure and projective bias. In\naddition to sparse-view RGB input, we require the network to take input and\ngenerate corresponding normal maps. The input normal maps can be predicted by\n2D diffusion models, significantly aiding in the guidance and refinement of the\ngeometry's learning. Moreover, by combining Signed Distance Function (SDF)\nsupervision with surface rendering, we directly learn to generate high-quality\nmeshes without the need for complex multi-stage training processes. By\nincorporating these explicit 3D biases, MeshFormer can be trained efficiently\nand deliver high-quality textured meshes with fine-grained geometric details.\nIt can also be integrated with 2D diffusion models to enable fast\nsingle-image-to-3D and text-to-3D tasks. Project page:\nhttps://meshformer3d.github.io",
        "updated": "2024-08-19 17:55:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10198v1"
    },
    {
        "title": "SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views",
        "authors": "Chao XuAng LiLinghao ChenYulin LiuRuoxi ShiHao SuMinghua Liu",
        "links": "http://arxiv.org/abs/2408.10195v1",
        "entry_id": "http://arxiv.org/abs/2408.10195v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10195v1",
        "summary": "Open-world 3D generation has recently attracted considerable attention. While\nmany single-image-to-3D methods have yielded visually appealing outcomes, they\noften lack sufficient controllability and tend to produce hallucinated regions\nthat may not align with users' expectations. In this paper, we explore an\nimportant scenario in which the input consists of one or a few unposed 2D\nimages of a single object, with little or no overlap. We propose a novel\nmethod, SpaRP, to reconstruct a 3D textured mesh and estimate the relative\ncamera poses for these sparse-view images. SpaRP distills knowledge from 2D\ndiffusion models and finetunes them to implicitly deduce the 3D spatial\nrelationships between the sparse views. The diffusion model is trained to\njointly predict surrogate representations for camera poses and multi-view\nimages of the object under known poses, integrating all information from the\ninput sparse views. These predictions are then leveraged to accomplish 3D\nreconstruction and pose estimation, and the reconstructed 3D model can be used\nto further refine the camera poses of input views. Through extensive\nexperiments on three datasets, we demonstrate that our method not only\nsignificantly outperforms baseline methods in terms of 3D reconstruction\nquality and pose prediction accuracy but also exhibits strong efficiency. It\nrequires only about 20 seconds to produce a textured mesh and camera poses for\nthe input views. Project page: https://chaoxu.xyz/sparp.",
        "updated": "2024-08-19 17:53:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10195v1"
    },
    {
        "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
        "authors": "Fuzhao XueYukang ChenDacheng LiQinghao HuLigeng ZhuXiuyu LiYunhao FangHaotian TangShang YangZhijian LiuEthan HeHongxu YinPavlo MolchanovJan KautzLinxi FanYuke ZhuYao LuSong Han",
        "links": "http://arxiv.org/abs/2408.10188v1",
        "entry_id": "http://arxiv.org/abs/2408.10188v1",
        "pdf_url": "http://arxiv.org/pdf/2408.10188v1",
        "summary": "Long-context capability is critical for multi-modal foundation models. We\nintroduce LongVILA, a full-stack solution for long-context vision-language\nmodels, including system, model training, and dataset development. On the\nsystem side, we introduce the first Multi-Modal Sequence Parallelism (MM-SP)\nsystem that enables long-context training and inference, enabling 2M context\nlength training on 256 GPUs. MM-SP is also efficient, being 2.1x - 5.7x faster\nthan Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM in\ntext-only settings. Moreover, it seamlessly integrates with Hugging Face\nTransformers. For model training, we propose a five-stage pipeline comprising\nalignment, pre-training, context extension, and long-short joint supervised\nfine-tuning. Regarding datasets, we meticulously construct large-scale visual\nlanguage pre-training datasets and long video instruction-following datasets to\nsupport our multi-stage training process. The full-stack solution extends the\nfeasible frame number of VILA by a factor of 128 (from 8 to 1024 frames) and\nimproves long video captioning score from 2.00 to 3.26 (1.6x), achieving 99.5%\naccuracy in 1400-frames video (274k context length) needle in a haystack.\nLongVILA-8B also demonstrates a consistent improvement in performance on long\nvideos within the VideoMME benchmark as the video frames increase.",
        "updated": "2024-08-19 17:48:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.10188v1"
    }
]