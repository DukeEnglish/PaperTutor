Robust spectral clustering with rank statistics
Joshua Cape jrcape@wisc.edu
Department of Statistics
University of Wisconsin–Madison
Madison, WI 53706, USA
Xianshi Yu xyu384@wisc.edu
Department of Computer Sciences
University of Wisconsin–Madison
Madison, WI 53706, USA
Jonquil Z. Liao zliao42@wisc.edu
Department of Statistics
University of Wisconsin–Madison
Madison, WI 53706, USA
Abstract
This paper analyzes the statistical performance of a robust spectral clustering method for
latent structure recovery in noisy data matrices. We consider eigenvector-based clustering
appliedtoamatrixofnonparametricrankstatisticsthatisderivedentrywisefromtheraw,
original data matrix. This approach is robust in the sense that, unlike traditional spec-
tral clustering procedures, it can provably recover population-level latent block structure
even when the observed data matrix includes heavy-tailed entries and has a heterogeneous
variance profile. Here, the raw input data may be viewed as a weighted adjacency matrix
whose entries constitute links that connect nodes in an underlying graph or network.
Ourmaintheoreticalcontributionsarethreefoldandholdunderflexibledatagenerating
conditions. First, we establish that robust spectral clustering with rank statistics can con-
sistently recover latent block structure, viewed as communities of nodes in a graph, in the
sensethatunobservedcommunitymembershipsforallbutavanishingfractionofnodesare
correctly recovered with high probability when the data matrix is large. Second, we refine
theformerresultandfurtherestablishthat,undercertainconditions,thecommunitymem-
bershipofanyindividual,specifiednodeofinterestcanbeasymptoticallyexactlyrecovered
withprobabilitytendingtooneinthelarge-datalimit. Third,weestablishasymptoticnor-
mality results associated with the truncated eigenstructure of matrices whose entries are
rankstatistics,madepossiblebysynthesizingcontemporaryentrywisematrixperturbation
analysis with the classical nonparametric theory of so-called simple linear rank statistics.
Collectively, these results demonstrate the statistical utility of rank-based data transfor-
mations when paired with spectral techniques for dimensionality reduction. Numerical
examples illustrate and support our theoretical findings. Additionally, for a dataset con-
sisting of human connectomes, our approach yields parsimonious dimensionality reduction
and improved recovery of ground-truth neuroanatomical cluster structure. We conclude
with a discussion of extensions, practical considerations, and future work.
Keywords: Clustering; dimensionality reduction; eigenvector estimation; network em-
bedding; nonparametric statistics.
©2024JoshuaCape,XianshiYu,andJonquilZ.Liao.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/.
4202
guA
91
]LM.tats[
1v63101.8042:viXraCape, Yu, and Liao
1 Introduction and background
Thispaperfocusesonspectralmethodsfordataanalysis,namelytoolsandalgorithmsbased
on factorizations, truncations, and perturbations of large data matrices. We primarily con-
sider matrices that are real-valued and symmetric which arise naturally, for example, when
quantifying pairwise interactions between distinct entities, as with general (dis)similarity
matrices in multivariate statistics and adjacency matrices for undirected graphs in network
analysis. Insuchcases, theumbrellaterm‘spectralmethods’commonlyreferstothebehav-
ior of matrix eigenvalues (spectrum) and eigenvectors, collectively comprising the spectral
decomposition, or equivalently, the matrix eigendecomposition.
Within the realm of spectral methods, embedding and clustering are popular techniques
that, broadly speaking, refer to producing and subsequently analyzing low-dimensional rep-
resentations of large, complex data (Shi and Malik, 2000; McSherry, 2001; Ng et al., 2002).
Considerableefforthasbeendevotedtostudyingthestatisticalfoundationsofspectralclus-
tering and embedding, with early works including Balakrishnan et al. (2011); Rohe et al.
(2011). Multiplesurveysandmonographsnowexist,providinganoverviewofspectralgraph
clustering (von Luxburg, 2007) and applications of spectral methods in data science (Chen
et al., 2021). Interest in these topics extends across computer science, machine learning,
engineering, and beyond, amassing methods and know-how found in recent review articles
such as Cui et al. (2018); Chen et al. (2020).
Spectral embedding and clustering are widely-employed techniques for exploratory data
analysis that simultaneously provide a starting point for subsequent inference. Given their
practical flexibility and computational feasibility, spectral embedding and clustering are
often applied to matrix-valued representations of graph or network data and to large data
matrices more generally. The rows and columns of such data matrices are frequently con-
ceptualized as nodes (i.e., vertices; entities of interest), whose connectivity patterns are
reflected in the matrix entries, frequently conceptualized as links (i.e., edges; connections).
As such, many works investigate spectral techniques in the context of random graph models
and statistical network analysis (Le et al., 2016; Ali and Couillet, 2018; Abbe et al., 2020;
Paul and Chen, 2020; Zhang et al., 2020; Arroyo et al., 2021; L¨offler et al., 2021; Noroozi
etal.,2021;Fanetal.,2022), surveyedinpartinAbbe (2018); Athreyaetal.(2018). Appli-
cations of spectral embedding and clustering methodology span the physical, natural, and
engineering sciences (Rohe et al., 2016; Liu et al., 2018; Priebe et al., 2019). Collectively,
these works form a backdrop for the present paper.
Recentyearshavewitnessedmountinginterestinrobustdataanalysistechniquesinsta-
tistical machine learning, for example, in the study of low-rank matrix estimation (Elsener
and van de Geer, 2018; Fan et al., 2021b), covariance matrix estimation (Fan et al., 2018;
Minsker,2018;Zhangetal.,2022a),high-dimensionalfactormodels(Fanetal.,2019,2021a),
and robust clustering (Jana et al., 2023, 2024). In theoretical computer science, there has
been particular interest in adversarially robust algorithms (Chen et al., 2011; Yi et al.,
2016; Yin et al., 2018) and learning mixture distributions whose component distributions
may have heavy tails (Dasgupta et al., 2005; Chaudhuri and Rao, 2008; Diakonikolas and
Kane, 2023). Correspondingly, growing interest has developed in pursuit of robust spectral
methods. The need for such methods is apparent since in practice data may be structured
yet contain outliers, artifacts, or significant heterogeneity more generally, qualities that can
2Robust spectral clustering with rank statistics
drastically degrade the performance of non-robust, baseline data analysis procedures. Still,
to date, comparatively few works in the literature address robustness considerations in the
context of spectral clustering and embedding. Existing works on robust spectral clustering
includeLietal.(2007);ChangandYeung(2008);Zhuetal.(2014);Bojchevskietal.(2017);
Srivastava et al. (2023) and collectively contribute novel methodology and algorithms. Still,
with the exception of works such as Cai and Li (2015); Srivastava et al. (2023), theoretical
guarantees for recently proposed methods are largely unavailable in the literature.
1.1 Overview and contributions of this paper
The principal novelty of this paper is to establish theoretical performance guarantees for
a user-friendly parameter-free robust spectral clustering paradigm based on the entrywise
ranktransformationa (cid:55)→ rank(a )/(N+1). Weestablishconsistencyresultsforrecovering
ij ij
latent block structure from observed data, in the form of finite-sample error bounds that
hold with high probability and go to zero in the large-matrix limit. Our results apply to
a broad range of model settings, including Cauchy and Pareto heavy-tailed data generating
distributions. We further develop asymptotic distributional properties associated with the
robust method via large-sample normality results for expressions involving normalized rank
statistics. Collectively, our contributions quantify large-sample patterns observed in point
cloud representations of data embeddings derived from large data matrices. Code for this
paper is available online on the first author’s webpage.
Throughout this paper, the raw input data matrix A = (a ) can be interpreted as a
ij
weighted adjacency matrix. From A, we derive a matrix of normalized rank statistics, R(cid:101)A,
whichitselfcanbeviewedasaweightedadjacencymatrixbutforanimplicitdependent-edge
inhomogeneous random graph. The vast majority of existing works on spectral clustering
assumeentrywise(conditional)independenceofmatrixelements, modulomatrixsymmetry.
In contrast, matrices of normalized rank statistics exhibit inherent entrywise dependencies,
with heterogeneous pairwise covariances, three-wise dependence, four-wise dependence, etc.
Consequently, standard tools for collections of independent random variables, such as Bern-
stein’s inequality or existing user-friendly operator norm concentration inequalities, cannot
be readily applied. As a result, we must adapt and modify existing approaches to handle
dependentmatrixentries. Ourproofanalysistracksentrywisedependenciesamongnormal-
ized rank statistics and establishes that probabilistic concentration and statistical consis-
tency still hold. Furthermore, this paper rigorously studies the truncated eigenstructure of
matricescomprisedofrankstatistics, anon-traditionalandseeminglyunder-exploredtopic.
A key ingredient that underpins our main results is a combinatorial bound for the
higher-order trace of a matrix of centered rank statistics, presented in Lemma 4 in Sec-
tion 5.1. Proving Lemma 4 involves a careful conditioning and counting argument applied
to collections of non-i.i.d. random variables. To establish weak consistency in Section 5.2,
we apply this key ingredient together with a modified version of the clustering analysis
paradigm in Lei and Rinaldo (2015) which employs the Davis–Kahan subspace perturba-
tion theorem together with the analysis of solutions to the (1 + ϵ)-approximate k-means
clustering problem. In Section 5.3, we then consider a more refined level of granularity
when analyzing the row-wise perturbations of vectors comprising data embeddings, in or-
der to quantify the large-sample behavior of individual node representations, leading to
3Cape, Yu, and Liao
node-specific asymptotic perfect clustering guarantees (strong consistency). Proving node-
wise strong consistency requires a well-chosen step-by-step matrix perturbation analysis to
which we apply a suite of concentration inequalities and bounds, notably the expectation
bound in Lemma 35. Thereafter, in Section 5.4, we establish component-wise and row-wise
asymptotic normality for the robust rank-based embedding. Our strong consistency and
asymptotic normality results are enabled by adaptations and modifications of the recent
entrywise matrix perturbation analysis programme in Cape et al. (2019a,b), together with
the classical theory of so-called simple linear rank statistics under multi-sample alternatives
developed extensively in H´ajek (1968); Dupac and Hajek (1969). All proof materials and
technical tools are collected in Appendix A.
1.2 Notation
The notation used in this paper is mostly standard and detailed below for completeness.
We write the product of a and b as ab, a·b, or a×b, depending on context, in an effort
to improve readability. We use the equivalence symbol ≡ to emphasize dependencies or
relationships that are sometimes suppressed for ease of shorthand, for example R(cid:101) ≡ R(cid:101)A
denotes the reliance of the term R(cid:101) on the term A.
Given sets I and J, let I\J and I −J both denote the set difference operation. Let
I denote the complement of the set I. For a general real-valued n × n matrix M and
sets of indices I,J ⊆ n = {1,2,...,n}, let M and M denote the submatrices of M
I⋆ ⋆J
consisting of the corres(cid:74) po(cid:75) nding rows and columns. Let Mn×K denote the set of all n×K
matrices where each row has a single entry equal to 1 and all remaining K −1 row entries
are zero. In other words, for each Θ ∈ Mn×K, the n rows of Θ are standard basis vectors
in RK. We call such matrices Θ membership matrices, and the block membership of row i
is denoted by g ∈ K ; in particular, Θ = 1. For each 1 ≤ k ≤ K, denote the set of node
i igi
(cid:74) (cid:75)
indices with block membership k by G ≡ G (Θ) = {1 ≤ i ≤ n : g = k} having cardinality
k k i
n = |G |. Let n = min n , let n = max n , and let n′ denote the
k k min 1≤k≤K k max 1≤k≤K k max
second-largest block size, namely the second-largest community size. Let JK×K denote the
set of K ×K permutation matrices.
. .
Wewrite=todenoteapproximate(truncated)numericalequality,forexample,π = 3.14
.
and 1/3 = 0.333. The vector of all ones in Rn is denoted by 1 , whereas the binary zero-one
n
indicator function evaluated at a condition {·} is written as 1{·}. The squared Euclidean
norm of x = (x ,...,x )T ∈ Rn is given by ∥x∥2 = (cid:80)n x2, while the Euclidean norm
1 n ℓ2 i=1 i
of the i-th row of a matrix M is denoted by ∥M∥ . The eigenvalues of a square matrix
i,ℓ2
M ∈ Rn×n are denoted by λ ≡ λ (M), for i = 1,...,n, and are ordered in the manner
i i
|λ | ≥ |λ | ≥ ··· ≥ |λ |. We call λ ,...,λ the top-K eigenvalues of M with associated
1 2 n 1 K
top-K orthonormal eigenvectors u ,...,u ∈ Rn provided |λ | > |λ |. The trace of a
1 K K K+1
square n×n matrix M is given by trace(M) =
(cid:80)n
M =
(cid:80)n
λ . The Frobenius norm
i=1 ii i=1 i
(cid:112)
ofagenericmatrixM isgivenby∥M∥ = trace(MTM). Theoperator(spectral)normof
F
(cid:112) (cid:112)
a matrix is given by ∥M∥ = sup ∥Mx∥ = max λ (MTM) = λ (MTM).
op ∥x∥ ℓ2=1 ℓ2 1≤i≤n i 1
The norm expression ∥M∥ counts the number of nonzero entries in M.
0
Given a vector x ∈ Rn, write diag(x) ∈ Rn×n to indicate the diagonal matrix whose
main diagonal entries are given by the entries of x. Given a square matrix M ∈ Rn×n, write
diag(M) ∈ Rn×n to denote the diagonal matrix consisting of the main diagonal elements
4Robust spectral clustering with rank statistics
of M. The Hadamard (entrywise) product between matrices A,B ∈ Rn×n is denoted by
A◦B = (A ×B ) ∈ Rn×n. Similarly, A◦2 ≡ A◦A. Entry (i,j) of the matrix M is
ij ij 1≤i,j≤n
at times denoted by m , m , M , or M , depending on context.
ij i,j ij i,j
Given a matrix M, its i-th row when viewed as a column vector, is denoted by M ≡
rowi
M where formally M = (eTM)T when e denotes the i-th standard basis vector. The
i⋆ rowi i i
identity matrix in dimension n is denoted by Id or I .
n n
Given a random variable ξ ≡ ξ
n
∈ R indexed by n, write ξ = oP(1) to denote that
for any choice of δ > 0, |ξ| ≤ δ holds with probability tending to one as n → ∞. We
write ξ = OP(f(n,ν)) when there exists a constant ν > 0 such that for some positive
constants C′,C,n , it holds that |ξ| ≤ Cf(n,ν) with probability at least 1−C′n−ν for all
0
n ≥ n . In this paper, 0 < ν ≪ 1. The deterministic analogues are written as o(·) and O(·),
0
respectively.
2 Robust spectral embedding and clustering
Algorithm 1 specifies the process of converting a symmetric input matrix to a matrix of
normalized rank statistics, colloquially called ‘passing to ranks’. This procedure amounts
to a nonparametric entrywise matrix transformation A (cid:55)→ R(cid:101)A ∈ [0,1]n×n that pre-processes
the original input data matrix. Subsequently, it will be shown that this normalization
enables the study of R(cid:101)A ≈ E[R(cid:101)A] for establishing spectral clustering guarantees, even when
E[A] does not exist.
Algorithm 1 Pass-to-ranks (PTR) rank-transform procedure
Require: A symmetric matrix A = (a ) ∈ Rn×n with distinct elements {a } .
ij ij i<j
1. Define N = n(n−1)/2.
2. For i < j, define the rank of a as r =
(cid:80)
1{a ≤ a }. Set r =
rij
.
ij ij 1≤i′<j′≤n i′j′ ij (cid:101)ij N+1
3. For i > j, set r = r . For each 1 ≤ i ≤ n, set r = 0.
(cid:101)ij (cid:101)ji (cid:101)ii
return The symmetric matrix of normalized rank statistics R(cid:101)A = (r (cid:101)ij) ∈ [0,1]n×n.
The theory subsequently developed in this paper holds for stochastic input matrices A
that have distinct upper-triangular entries with probability one, thereby yielding R(cid:101)A per
Algorithm 1. Elsewhere in practice, a tie-breaking procedure can be specified if the matrix
A is allowed to have repeated values among its entries. Setting the diagonal elements of R(cid:101)A
to zero can be interpreted as removing possible self-loops in a graph.
Given the matrix of normalized rank statistics R(cid:101)A, we next consider top-d eigenvector-
based spectral embedding and clustering as outlined in Algorithm 2. We shall focus our
study on the performance of (1+ϵ)-approximate k-means clustering, though in practice ap-
plyingothervariantsofk-meansclusteringcanalsobeappropriate. Further,oursubsequent
asymptoticnormalityresultssuggestthatclusteringspectralembeddingsviaGaussianmix-
ture modeling can also be appropriate.
5Cape, Yu, and Liao
Algorithm 2 Spectral embedding and clustering with matrices of rank statistics
Require: A symmetric matrix A = (a ) ∈ Rn×n with distinct elements {a } .
ij ij i<j
1. Obtain the matrix of normalized rank statistics R(cid:101)A via Algorithm 1.
2. Compute a truncated eigendecomposition of R(cid:101)A, keeping the 1 ≤ d ≤ n largest-
in-magnitude eigenvalues and associated orthonormal eigenvectors, denoted U(cid:98) ∈
R(cid:101)
Rn×d. Here, d could be chosen a priori or selected, for example, via an eigenvalue
ratio test, parallel analysis, or inspection of the scree plot.
3. Cluster the rows of (the embedding) U(cid:98) into K groups. Here, K could be chosen a
R(cid:101)
priori or selected, for example, via an information criterion.
return A clustering C = {C ,...,C } that partitions the set {1,...,n}.
1 K
3 Matrix-valued data with latent block structure
This section specifies a flexible data generating model for matrices with population-level
blockstructure. Blockstructurecanbeinterpretedasunobservedcommunitystructurethat
drives connectivity patterns between entities, such as in friendship networks derived from
social media platforms or in brain graphs derived from neuroimaging scans. Thereafter, in
Section5weshowthattheleadingeigenvectorsU(cid:98) ≡ U(cid:98) obtainedinAlgorithm2accurately
R(cid:101)
estimate population-level ground-truth eigenstructure.
Let F ≡ FK = {F : 1 ≤ k ≤ k′ ≤ K} denote a collection of K(K +1)/2 absolutely
(k,k′)
continuous cumulative distribution functions F . For notational convenience, define
(k,k′)
F = F whenever k > k′. We do not restrict the support sets of these distributions
(k,k′) (k′,k)
nor do we impose the existence of certain moments. For example, for K = 2, we may
associate F , F , and F with, say, the distributions Normal(µ,σ2), Gamma(α,β),
(1,1) (1,2) (2,2)
and Pareto(m,γ), respectively, or even contaminated versions thereof. Let Θ ≡ Θn×K ∈
Mn×K be a membership matrix as in Section 1.2. We consider symmetric data matrices A
with population-level block structure arising via Definition 1. This definition is reminiscent
of weighted stochastic blockmodel formulations put forth in the literature (Aicher et al.,
2015; Xu et al., 2020; Gallagher et al., 2023).
Definition 1 (Data matrices with blockmodel structure) We say that A ∈ Rn×n is
a symmetric random data matrix from the generative model (FK,Θn×K) when its entries
are random variables given by
(cid:40)
independent F if i ≤ j,
A =
(gi,gj)
(1)
ij
A if i > j.
ji
In particular, for each row (node) index i ∈ n , it holds that Θ = 1 and Θ = 0 for all
igi ik
(cid:74) (cid:75)
k ̸= g . Additionally, if A = 0 for all 1 ≤ i ≤ n is enforced, then A is said to be hollow.
i ii
We write A ∼ Blockmodel(FK,Θn×K).
Let B˘ denote a K ×K symmetric matrix of coefficients whose upper-triangular entries
are given by B˘ = Median [X] for all k ≤ k′. Here, B˘ always exists since each
kk′ X∼F (k,k′)
distribution function is absolutely continuous, while the possibility of non-unique medians
6Robust spectral clustering with rank statistics
does not pose any problems for this paper. Analogously, let B denote the K×K symmetric
matrix of coefficients whose upper-triangular entries are given by B = E [X] for
kk′ X∼F (k,k′)
all k ≤ k′, provided the expectations all exist. In particular, B is the matrix of block-wise
expectations for the entries of A. Analogously, define the matrix of variances, S◦2 ∈ RK×K,
≥0
in the entrywise manner S◦2 = Var [X] for all k ≤ k′, provided all the second
kk′ X∼F (k,k′)
moments exist.
Remark 2 (Population-level block structure) We emphasize that the parameter ma-
trices B˘, B, and S are determined indirectly, as a consequence of specifying the collection
of distributions FK. Further, the entrywise median, expectation, and variance matrices
associated with A, provided the latter exist, are block-structured with
Median[A] = ΘB˘ΘT, E[A] = ΘBΘT, Var[A] = ΘS◦2ΘT. (2)
If A is taken to be hollow, then Median[A] = ΘB˘ΘT−diag(ΘB˘ΘT) and similarly for E[A]
and Var[A], thereby amounting to approximate population-level block structure.
3.1 Matrices of normalized rank statistics
In Definition 1, the upper-triangular entries of A are independent random variables with
absolutely continuous cumulative distribution functions. Hence, they are distinct with
probability one, and Algorithm 1 can be applied to A to yield a symmetric random matrix
of normalized rank statistics, R(cid:101)A, whose entries are necessarily dependent.
At the population level, write B(cid:101) ∈ [0,1]K×K to denote the symmetric matrix where
(cid:40)
E(cid:104)
(R(cid:101)A)
ij(cid:105)
=
B(cid:101)gi,gj if i ̸= j,
(3)
0 if i = j.
In words, B(cid:101) is the matrix of block-wise expected normalized ranks derived from the model
Blockmodel(FK,Θn×K). This matrix of expected values is always well-defined since the
normalized ranks are discrete bounded random variables hence all their moments exist.
The pass-to-ranks procedure zeroes out the main diagonal elements of R(cid:101)A, hence its
expectation is a perturbation of ΘB(cid:101)ΘT, namely
E[R(cid:101)A] = ΘB(cid:101)ΘT−diag(ΘB(cid:101)ΘT). (4)
In what follows, the additive perturbation diag(ΘB(cid:101)ΘT) will be shown to have a negligible
effect, letting us write that E[R(cid:101)A] is itself approximately block-structured vis-`a-vis ΘB(cid:101)ΘT,
whose entries are necessarily bounded between zero and one.
Looking ahead, Appendices A.1.1 and A.1.2 provide exact, finite-sample formulas char-
acterizing E[(R(cid:101)A) ij], Var[(R(cid:101)A) ij], and Cov[(R(cid:101)A) ij,(R(cid:101)A) i′j′] for all entries of the normalized
ranks matrix R(cid:101)A. These expressions translate directly to expressions for B(cid:101) and S(cid:101); further,
they are crucial for the numerical examples provided throughout this paper. Notably, the
entries of B(cid:101) and S(cid:101) depend on the block sizes n
k
= |G k|, 1 ≤ k ≤ K, unlike for B and S.
7Cape, Yu, and Liao
3.2 Eigenvector preliminaries for block-structured matrices
This section highlights key aspects of the population-level eigenstructure in blockmodel
data matrices that are important throughout this paper. Firstly, B(cid:101) may have rank equal
to K even if B˘ does not have rank equal to K. Next, suppose that the matrices Θ, B˘, and
B(cid:101) each have rank equal to K. Importantly, the assumption that rank(B˘) = K places an
implicit requirement on the collection of distributions F (e.g., F ∈ F cannot all be identical
when K > 1), while assuming rank(B(cid:101)) = K places an implicit, joint requirement on F
√ √
and Θ. Next, define ∆ = (ΘTΘ)1/2 = diag( n ,..., n ) ∈ RK×K which is itself rank K
1 K
since rank(Θ) = K if and only if n > 0 for each k ∈ K , i.e., G ̸= ∅ for each k. The
k k
matrix Θ∆−1 ∈ Rn×K has orthonormal column vectors(cid:74) , he(cid:75) nce its columns span the top-
K dimensional eigenspace of both ΘB˘ΘT ≡ (Θ∆−1)∆B˘∆(Θ∆−1)T and ΘB(cid:101)ΘT. Namely,
the block structure underlying both Median[A] and E[R(cid:101)A] approximately share the same
K-dimensional leading eigenspace, noting that ∆B˘∆ and ∆B(cid:101)∆ are themselves full-rank
K ×K matrices. The same conclusion holds for B if it exists and satisfies rank(B) = K.
In fact, the leading population-level eigenvectors encode the memberships for all nodes
(rows)inblock-structureddatamatricesperLemma23. Thisobservationandthediscussion
above are well known in the literature on stochastic blockmodel random graphs where
spectralclusteringisappliedtosymmetricrandommatricesAwhoseentriesareindependent
Bernoulli random variables; see for example Lei and Rinaldo (2015). Consequently, a main
challenge in this paper is to show that the top-K truncated eigenspace of R(cid:101)A accurately
estimates the top-K eigenspace of ΘB(cid:101)ΘT by virtue of its close proximity to E[R(cid:101)A].
3.3 An example with contaminated normal distributions
We briefly pause to provide a numerical illustration. Consider a K = 2 blockmodel per
Definition 1 with data drawn from contaminated normal distributions of the form
F = (1−ϵ)·Normal(µ ,σ2 )+ϵ·Normal(µ ,1002·σ2 ), 1 ≤ k ≤ k′ ≤ 2.
(k,k′) kk′ kk′ kk′ kk′
Concretely, set ϵ = 0.01, µ = µ = 2, µ = 1, and σ = σ = σ = 3. Notice that the
11 22 12 11 12 22
standard deviation of the contaminating distribution is one hundred times larger than that
of the baseline normal distribution for each block.
We generate A ∈ Rn×n with n = 1000 according to the above model with equal block
sizes n = n = n/K = 500. We order the block memberships for ease of presentation
1 2
but emphasize that the memberships are themselves unobserved in practice. Our goal is to
recover the row (equivalently, column) memberships via spectral embedding and clustering.
Here, rank(B˘) = rank(B) = rank(B(cid:101)) = 2. Furthermore, here the matrices B and B(cid:101) have
the same eigenvectors, enabling easier visual comparison.
For both A and R(cid:101)A, we compute their ten largest eigenvalues. While this specification
technically does not guarantee that we retain the top ten largest-in-magnitude eigenvalues,
we make it here for the sake of visualization and because historically some practitioners
chose to ignore negative eigenvalues. Fig. 1 shows these eigenvalues in partial scree plots,
scaledbyn−1,alongsidethesign-adjustedleadingtwoeigenvectors,scaledbyn1/2. Evenfor
the specified small level of contamination, the spectrum of A is detrimentally influenced by
large-in-magnitudeentriesarisingfromthehugevarianceofthecontaminatingdistributions.
8Robust spectral clustering with rank statistics
Scree plot of data matrix Scree plot of PTR matrix
0.5
2.0
0.4
1.5
0.3
1.0
0.2
0.5 0.1
0.0 0.0
2 4 6 8 10 2 4 6 8 10
Eigenvalue index Eigenvalue index
1st eigenvector of data matrix 1st eigenvector of PTR matrix
2 2
1 1
0 0
−1 −1
−2 −2
0 200 600 1000 0 200 600 1000
Entry (node index) Entry (node index)
2nd eigenvector of data matrix 2nd eigenvector of PTR matrix
2 2
1 1
0 0
−1 −1
−2 −2
0 200 600 1000 0 200 600 1000
Entry (node index) Entry (node index)
Figure 1: Simulation example in Section 3.3 showing the truncated eigenstructure of A
(left column) and R(cid:101)A (right column). Blue lines denote the population-level ground truth
eigenvector behavior. Gray points are derived from the simulated data. Several outlier
values outside the interval [−2,2] are not shown. The two-dimensional eigentruncation of
the normalized ranks matrix recovers the unobserved two-block ground truth structure.
9
eulaV
eulaV
eulaV
eulaV
eulaV
eulaVCape, Yu, and Liao
The eigenvalues of A do not reflect the two-block ground truth structure, and neither do
its eigenvectors, even upon inspecting the third eigenvector, fourth eigenvector, etc.
ThespectrumofR(cid:101)A,ontheotherhand,exhibitsanoticeable‘elbow’atthesecondeigen-
value, correctly indicating approximate two-block structure. The corresponding n1/2-norm
eigenvectors are plotted against the ground-truth n1/2-norm eigenvectors, displayed in blue.
Here, the ground-truth second eigenvector reveals the two-block structure underlying the
data which is recovered by the corresponding eigenvector of the normalized ranks matrix.
Any sensible clustering algorithm, when applied to this second eigenvector, will perfectly
distinguish the two latent blocks based on the entrywise signs. Our node-specific strong
consistency results in Section 5.3 apply to this model setting, thereby theoretically justi-
fying spectral clustering applied to the pass-to-ranks matrix. In this particular simulation
example, the first two eigenvalues of R(cid:101)A are in fact its largest-in-magnitude eigenvalues.
The truncated eigenstructure of R(cid:101)A continues to successfully reflect the ground-truth
block structure of the model even when the contaminating distribution is more extreme,
such as Cauchy, both in theory and in simulations. In such situations B may not exist,
nevertheless rank(B(cid:101)) = 2 may still hold. Section 6 provides numerical illustrations for data
matrices whose entries have heavy-tailed distributions.
4 Statistical considerations for clustering
Section 4 provides background and context needed for the weak consistency results in Sec-
tion5.2. Thepresentationherefollowstheparadigmforadjacency-basedspectralclustering
of unweighted stochastic blockmodel random graphs in Lei and Rinaldo (2015).
In pursuit of weak consistency, we seek to recover the membership matrix Θ via an
estimate Θ(cid:98) ∈ Mn×K, modulo column-wise permutation Π ∈ JK×K. We consider two
popular criteria for evaluating estimation error.
1
Overall relative error: L(Θ(cid:98),Θ) = min ∥Θ(cid:98)Π−Θ∥ 0.
Π∈JK×K n
1
Worst case relative error: L(cid:101)(Θ(cid:98),Θ) = min max ∥(Θ(cid:98)Π)
G
⋆−Θ
G
⋆∥ 0.
Π∈JK×K1≤k≤K n k k k
In particular, we seek conditions under which L(Θ(cid:98),Θ),L(cid:101)(Θ(cid:98),Θ) → 0 as n → ∞, noting that
0 ≤ L(Θ(cid:98),Θ) ≤ L(cid:101)(Θ(cid:98),Θ) ≤ 2. Here, Θ(cid:98) yielding a small value L(Θ(cid:98),Θ) may have large relative
error for some blocks of comparatively smaller size, whereas L(cid:101)(Θ(cid:98),Θ) is a more stringent
condition that quantifies how well Θ(cid:98) uniformly estimates each block relative to its size.
We emphasize that the wide tilde notation in L(cid:101)(Θ(cid:98),Θ) is not associated with its usage for
normalized rank statistics in R(cid:101)A.
For the purposes of this paper, the k-means clustering problem is expressed formally as
(Θ(cid:98),T(cid:98)) = argmin ∥U(cid:98) −ΘT∥2 F,
(5)
Θ∈Mn×K,T∈RK×K
where U(cid:98) denotes a generic orthonormal input matrix of leading eigenvectors, and where
T scales and reorients Θ to enable commensurability with U(cid:98). A more computationally
10Robust spectral clustering with rank statistics
tractable and efficient alternative is to instead relax the above problem to the (1 + ϵ)-
approximate k-means problem, of the form
(cid:26) (cid:27)
(Θ(cid:98),T(cid:98)) ∈ Mn×K ×RK×K s.t. ∥U(cid:98) −Θ(cid:98)T(cid:98)∥2 ≤ (1+ϵ)· min ∥U(cid:98) −ΘT∥2 . (6)
F F
Θ∈Mn×K,T∈RK×K
Algorithm 3 combines these ideas in the language of traditional adjacency-based spectral
clustering. In this paper, our robust procedure applies Algorithm 3 to R(cid:101)A to obtain the
analogous quantities U(cid:98) and T(cid:98) and Θ(cid:98) .
R(cid:101) R(cid:101) R(cid:101)
Algorithm 3 Spectral clustering with approximate k-means; see Lei and Rinaldo (2015)
Require: A symmetric input matrix M ∈ Rn×n; number of communities (blocks) K ≥ 1;
approximation parameter ϵ > 0.
1. ComputeU(cid:98)M ∈ Rn×K,whosecolumnsconsistofK orthonormalleadingeigenvectors
of M, ordered in decreasing order of eigenvalue modulus.
2. Let (Θ(cid:98)M,T(cid:98)M) be a solution to the (1+ϵ)-approximate k-means problem in Eq. (6)
with K clusters and input matrix U(cid:98)M.
return Estimated membership matrix Θ(cid:98)M ∈ Mn×K.
5 Main results
Section 5.1 is a precursor to the ensuing consistency results and may be of independent
interest. We shall distinguish between ‘weak consistency’ and node-specific ‘strong consis-
tency’, based on terminology in the literature (Abbe, 2018). In Section 5.2, we establish
consistencyofourmatrix-valuedestimatorforthepopulation-leveltarget(i.e.,themember-
ship matrix; its top low-rank eigenspace). In Section 5.3, we consider a more refined level
of granularity by quantifying the behavior of individual nodes’ vector embedding represen-
tations. Of note, our strong consistency results are node-specific, in contrast to uniform
strongconsistencyresultsthatsimultaneouslyholdforallnodes. InSection5.4,weestablish
asymptotic normality results for the row vectors comprising data embeddings in RK.
5.1 Bounding matrices of normalized rank statistics
This section presents two technical lemmas which are proved in Appendix A.7. The first,
Lemma 3, bounds the higher-order trace of the matrix difference R(cid:101)A−E[R(cid:101)A] when K = 1.
Lemma 3 (Single block expected trace bound) Let A be a symmetric n×n random
matrix with i.i.d. entries A ∼ F for i ≤ j, where F is an absolutely continuous cumu-
ij
lative distribution function. Let R(cid:101)A denote the matrix of normalized rank statistics, per
Algorithm 1. If n ≥ 4, then
(cid:104) (cid:16) (cid:17)(cid:105) 1
0 ≤ E trace (R(cid:101)A−E[R(cid:101)A])4 ≤ n3. (7)
50
Lemma 4 generalizes Lemma 3 from K = 1 to the general case K ≥ 1.
11Cape, Yu, and Liao
Lemma 4 (Multiple block expected trace bound) Let A ∼ Blockmodel(FK,Θn×K)
per Definition 1. Let R(cid:101)A denote the matrix of normalized rank statistics, per Algorithm 1.
If n ≥ 5 for all k ∈ K , then
k
(cid:74) (cid:75)
(cid:104) (cid:16) (cid:17)(cid:105)
0 ≤ E trace (R(cid:101)A−E[R(cid:101)A])4 ≤ 66K2n2+2n3. (8)
5.2 Weak consistency
This section presents the weak consistency results which are proved in Appendix A.2. The-
orem 5 states our most general weak consistency result and relies on Lemma 4.
Theorem 5 (Weak consistency of robust spectral clustering with approximate k-
means clustering) Suppose A ∼ Blockmodel(FK,Θn×K) where rank(Θ) = rank(B(cid:101)) = K
and K ≤ n1/2 and n ≥ 5 for all k ∈ K . Let γ denote the smallest nonzero singular
k n
(cid:74) (cid:75)
value of ΘB(cid:101)ΘT. Let Θ(cid:98) be the output of spectral clustering using (1+ϵ)-approximate k-means
clustering per Algorithm 3 with input R(cid:101)A per Algorithm 1. There exists an absolute constant
c > 0 such that if
(cid:18) Kn1+ν(cid:19)
(2+ϵ) < c
γ2
n
for some ν > 0 with γ > C′n3/4+ν for some sufficiently large absolute constant C′ > 0,
n 2 2
then with probability at least 1 − n−ν there exist subsets S ⊂ G for 1 ≤ k ≤ K and a
k k
K ×K permutation matrix J such that Θ(cid:98)G⋆J = Θ G⋆, where G =
(cid:83)K
k=1(G k\S k) and
(cid:88)K
|S k|
≤
c−1(2+ϵ)(cid:18) Kn1+ν(cid:19)
.
n γ2
k n
k=1
Theorem 5 does not make any parametric or moment assumptions on the distributions
comprising FK. Further, the block-specific distributions may differ from one another, for
example, they may consist of Normal, Beta, Cauchy, Pareto, etc. As such, this consistency
result for robust spectral clustering with rank statistics holds under very flexible data gen-
erating conditions. Below, Corollary 6 translates Theorem 5 into an easier-to-interpret
statement involving the loss functions L and L(cid:101).
Corollary 6 (Weak consistency of robust spectral clustering via relative error
bounds) Assume the hypotheses in Theorem 5. There exists a constant c > 0 such that if
(cid:32) (cid:33)
Kn1+ν
(2+ϵ) < c
n2 λ2 (B(cid:101))
min K
for some ν > 0 with n min·|λ K(B(cid:101))| > C 2′n3/4+ν for some sufficiently large absolute constant
C′ > 0, then with probability at least 1−n−ν it holds that
2
(cid:32) (cid:33)
Knνn′
L(Θ(cid:98),Θ) ≤ 2c−1(2+ϵ) max ,
n2 λ2 (B(cid:101))
min K
12Robust spectral clustering with rank statistics
and simultaneously
(cid:32) (cid:33)
Kn1+ν
L(cid:101)(Θ(cid:98),Θ) ≤ 2c−1(2+ϵ) .
n2 λ2 (B(cid:101))
min K
Our interest is in regimes for which the upper bounds decay to zero as the matrix size
n increases. For example, when each n
k
is order n/K and |λ K(B(cid:101))| is an absolute constant,
then by Corollary 6, for 0 < ν < 1/4,
L(Θ(cid:98),Θ) =
OP(cid:0) K2·n−1+ν(cid:1)
, L(cid:101)(Θ(cid:98),Θ) =
OP(cid:0) K3·n−1+ν(cid:1)
.
Our bounds show that L(Θ(cid:98),Θ),L(cid:101)(Θ(cid:98),Θ) → 0 in probability as n → ∞ when K = O(1)
and even in regimes where K ≡ K → ∞ as n → ∞. It is also possible for the upper
n
bounds to behave more differently. For example, suppose there is one dominant block,
hence n′ ≪ n and so the upper bound on the overall relative error L is much smaller
max
than the upper bound on the worst case relative error L(cid:101).
Theorem 5 is a precise clustering statement for approximate k-means clustering. It
is derived by employing subspace perturbation bounds to show that the estimated top
eigenspace is very close to the population-level top eigenspace and hence that the estimated
membership matrix recovers the true membership matrix. Subspace perturbation bounds
themselvesmaintaintheinterpretationofweakconsistencybutwithoutreferencingaspecific
algorithm, namely they do not formally treat the approximation parameter ϵ. As such,
Eq. (9) in Theorem 7 provides yet another lens through which to view and understand the
weak consistency result.
Theorem 7 (Top eigenspace relative error of robust embedding) Suppose A ∼
Blockmodel(FK,Θn×K) where rank(Θ) = rank(B(cid:101)) = K and K ≤ n1/2 and n
k
≥ 5 for all
k ∈ K . Let UUT denote the orthogonal projection onto the column space of ΘB(cid:101)ΘT, and
(cid:74) (cid:75)
let U(cid:98)U(cid:98)T denote the orthogonal projection onto the leading K-dimensional eigenspace of R(cid:101)A.
Let γ
n
denote the smallest nonzero singular value of ΘB(cid:101)ΘT. There exist absolute constants
C,C′ > 0 such that if γ > C′n3/4+ν for some ν > 0, then
2 n 2
(cid:34) (cid:32) (cid:33)(cid:35)
Pr
∥U(cid:98)U(cid:98)T−UUT∥
F >
C n1/2+ν/2
≤ n−ν. (9)
∥UUT∥ F |λ K(B(cid:101))| n min
The result in Eq. (9) can alternatively be understood in the language of ‘matrix trace
correlation’, which is defined for any full column-rank matrices Y,Z ∈ Rn×K with corre-
sponding projections P = Y(YTY)−1YT and P = Z(ZTZ)−1ZT as
Y Z
(cid:114)
trace(P P )
Y Z
r (Y,Z) = , 0 ≤ r ≤ 1.
K K
K
In particular, weak consistency of Θ(cid:98) for Θ, via U(cid:98) for U, admits the equivalence
∥U(cid:98)U(cid:98)T−UUT∥ F P
= oP(1) ⇐⇒ r K(U(cid:98),U) → 1. (10)
∥UUT∥ F n→∞
13Cape, Yu, and Liao
In words, weak consistency establishes that on average (across the underlying graph)
the node memberships are recovered with high probability for large n. At most a vanishing
fraction of nodes are misclustered asymptotically with high probability, but this does not
preclude that, say, O(logn) nodes get misclustered as n → ∞. Indeed, the results in
Section 5.2 do not guarantee that the membership of a specific, individual node of interest
is necessarily perfectly recovered asymptotically with high probability.
5.3 Node-specific strong consistency
Thissectionestablishesthatindividualnodemembershipscanbeperfectlyrecoveredasymp-
totically with high probability in blockmodels under certain conditions. The results estab-
lished in Section 5.3 require different tools and a more refined analysis. Proofs are provided
in Appendix A.3.
Below, we obtain bounds for the individual row vectors of U(cid:98) −UW
⋆
where W
⋆
denotes
an appropriate K×K orthogonal matrix, and we use the subscript notation ⋆ to emphasize
its special role. The rows of U(cid:98) correspond to the vector-valued embedding representations
of the rows (columns) of the non-negative matrix R(cid:101)A. Similarly, the rows of U correspond
to the population-level embedding representations of nodes in low-dimensional Euclidean
space. The matrix W
⋆
reflects the general non-uniqueness of U(cid:98) and U as orthonormal bases
for K-dimensional subspaces in Rn. Accounting for the orthogonal transformation W is
⋆
required and serves to properly align the embeddings.
For the purposes of this paper, ‘node-specific strong consistency’ is said to hold for node
(n)
i if there exists a sequence of orthogonal transformations W ≡ W such that, as n → ∞,
⋆ ⋆
n1 gi/2∥U(cid:98) −UW ⋆∥
i,ℓ2
= oP(1). (11)
1/2
Per Section 1.2, here n denotes the size of block g ∈ K . Scaling by n is crucial, since
gi i gi
−1/2 (cid:74) (cid:75)
entrywise U(cid:98)ij = oP(1) and ∥U∥
i,ℓ2
= n
gi
. Omitting this term could otherwise trivially
yield a bound satisfying ∥U(cid:98) −UW ⋆∥
i,ℓ2
= oP(1) even if U(cid:98) were inconsistent for U.
Theorem 8 (Node-specific strong consistency of robust spectral clustering, gen-
eral case) Suppose A ∼ Blockmodel(FK,Θn×K) where rank(Θ) = rank(B(cid:101)) = K with
K ≤ n1/2 and n
k
≥ 5 for all k ∈ K . Assume that |λ K| ≡ |λ K(ΘB(cid:101)ΘT)| ≥ C · n3/4+ν
(cid:74) (cid:75)
for some sufficiently large constant C > 0 and some constant 0 < ν < 1/4. Obtain R(cid:101)A
using Algorithm 1 and compute U(cid:98), a matrix whose orthonormal columns are eigenvectors
for the K largest-in-magnitude eigenvalues of R(cid:101)A. Let U denote the corresponding matrix of
eigenvectors for ΘB(cid:101)ΘT. Then, there exists W
⋆
such that for any fixed choice of row index,
14Robust spectral clustering with rank statistics
i, it holds that
(cid:32)(cid:40)
n1 gi/2∥U(cid:98) −UW ⋆∥
i,ℓ2
= OP K1/2·nν/2·|λ K|−1
+K3/2·n1+3ν/2·|λ |−3
K
(cid:110) (cid:111)
+max n−1/4·n1/4,K3/4 ·n−1/4·n3/4+ν/2·|λ |−2
gi gi K
+K1/2·n1+3/4+3ν/2·|λ |−3
K
(cid:41) (cid:33)
+K ·n−1/2·n1+ν ·|λ |−2 ·n1/2 .
gi K gi
Theorem8hastheadvantageofgenerality,neverthelessitappearssomewhatunwieldyat
first glance. Under certain additional assumptions commonly encountered in the literature,
Theorem 8 simplifies to the following easier-to-interpret corollary.
Corollary 9 (Node-specific strong consistency of robust spectral clustering, spe-
cial case) Assume the setting in Theorem 8. Further assume that all block sizes n are
k
order n/K and that |λ K(B(cid:101))| is bounded above and below by positive absolute constants.
Then, for each fixed choice of row index, i, it holds that
(cid:16) (cid:110) (cid:111)(cid:17)
n1 gi/2∥U(cid:98) −UW ⋆∥
i,ℓ2
= OP K ·n−1/2+ν/2· 1+K2·n−1/4+ν .
In particular, the hypotheses imply K = O(n1/4−ν), hence
n1 gi/2∥U(cid:98) −UW ⋆∥
i,ℓ2
= oP(1).
Remark 10 (Population spectral gap and blockmodel distributions) The consis-
tency results depend on the population eigenvalue λ K(B(cid:101)) which reflects properties of the
block-specific distributions FK = {F : 1 ≤ k ≤ k′ ≤ K}. Unlike existing results in the
(k,k′)
literature, however, this population eigenvalue is a function of the block sizes and does not
admit a simple closed-form expression in general. Consequently, for many choices of FK,
it can only be evaluated numerically. Nevertheless, the relationship between λ K(B(cid:101)) and the
blockmodel distributions can be succinctly exhibited in the following example. Fix K ≥ 2,
and suppose that F = F for all 1 ≤ k ≤ K and F = F for all k ̸= k′. Let
(k,k) (1,1) (k,k′) (1,2)
X and X denote independent random variables satisfying X ∼ F and X ∼ F .
11 12 11 (1,1) 12 (1,2)
If n = n/K holds for all 1 ≤ k ≤ K, then applying Proposition 17 and the fact that
k
E[F (X )] = 1−E[F (X )] yields
(1,1) 12 (1,2) 11
nl →im ∞λ K(B(cid:101)) = (cid:0) 1− K1(cid:1)(cid:0)E[F (1,2)(X 11)]− 1 2(cid:1) − K1 (cid:0)E[F (1,1)(X 12)]− 21(cid:1)
= E[F (X )]− 1
(1,2) 11 2
= Pr[X < X ]− 1.
12 11 2
Remark 11 (Consistency under growing rank K in Theorem 8 and Corollary 9)
Although B(cid:101) is a population-level quantity, the entries of B(cid:101) change with n (see Proposi-
tion 17), hence so does |λ K(B(cid:101))| and ∥B(cid:101)−1∥
op
= |λ K(B(cid:101))|−1. Further, B(cid:101) ∈ [0,1]K×K, so
15Cape, Yu, and Liao
permitting K to grow with n in general implies that its spectrum depends on K. When seek-
ing to permit K
n
→ ∞ as n → ∞, one could further require liminf n→∞{κ Kn·|λ Kn(B(cid:101))|} > 0
for some sequence of positive scalars {κ Kn} n≥1, in which case the OP(·) upper bound would
inherit dependence on the behavior of κ . In short, fully general statements pertaining to
Kn
K → ∞ require extra care and are not pursued here.
n
5.4 Asymptotic normality
This section begins with two supplementary lemmas. Lemma 12 establishes univariate
asymptotic normality for simple linear rank statistics appearing in block-structured data
matrix settings. Lemma 13 subsequently establishes multivariate asymptotic normality
involving rank statistics when the population-level block structure is sufficiently balanced
and non-degenerate. These lemmas enable Theorem 14, the main result in this section,
which establishes asymptotic normality of the leading eigenvector entries of R(cid:101)A. Proofs are
provided in Appendix A.4. Below, let (S(cid:101)◦2)
kk′
= {Var[(R(cid:101)A) ij] : g
i
= k,g
j
= k′}.
Lemma 12 (Asymptotic normality, part I) Fix K, FK, and fix a tuple (α,β). Suppose
A ∼ Blockmodel(FK,Θn×K) where rank(Θ) = rank(B(cid:101)) = K. Write
(cid:20)
(cid:16) (cid:17)
(cid:21)−1/2(cid:16)
(cid:17)
Z(cid:101) α(n ,β) = Var (R(cid:101)A−E[R(cid:101)A])Θ (R(cid:101)A−E[R(cid:101)A])Θ .
α,β α,β
If both liminf n→∞min
k,k′∈ K
(S(cid:101)◦2)
kk′
≥ δ
Var
for some constant δ
Var
> 0 and n
β
→ ∞, then
for any fixed choice of ϵ >(cid:74)0,(cid:75)it eventually holds for large n that
(cid:12) (cid:104) (cid:105) (cid:12)
m x∈a Rx(cid:12) (cid:12)Pr Z(cid:101) α(n ,β) ≤ x −Φ 1(x)(cid:12)
(cid:12)
< ϵ, (12)
where Φ denotes the cumulative distribution function of the standard normal distribution.
1
(n)
Namely, Z(cid:101) converges in distribution to a standard normal random variable.
α,β
Lemma 13 (Asymptotic normality, part II) Fix K, FK, and fix a row index α. Sup-
pose A ∼ Blockmodel(FK,Θn×K) where rank(Θ) = rank(B(cid:101)) = K. Let π = (π 1,...,π K)T
be a probability vector of constants with 0 < π ,...,π < 1 and
(cid:80)K
π = 1 such that
1 K k=1 k
n
k
= n×π
k
for each k ∈ K . Assume that liminf n→∞min
k,k′∈ K
(S(cid:101)◦2)
kk′
≥ δ
Var
for some
constant δ > 0. Write(cid:74) (cid:75) (cid:74) (cid:75)
Var
(cid:104)(cid:16) (cid:17) (cid:105)−1/2(cid:16) (cid:17)
Z(cid:101) r( on w)
α
= Cov (R(cid:101)A−E[R(cid:101)A])Θ (R(cid:101)A−E[R(cid:101)A])Θ ,
rowα rowα
while treating each occurrence of row α as a column vector. For any fixed choice ϵ > 0, it
eventually holds for large n that
(cid:12) (cid:104) (cid:105) (cid:12)
max (cid:12) (cid:12)Pr Z(cid:101) r( on w)
α
≤⃗x −Φ K(⃗x)(cid:12)
(cid:12)
< ϵ, (13)
⃗x∈RK
where Φ denotes the cumulative distribution function of the K-dimensional standard nor-
K
(n)
maldistribution. Namely, Z(cid:101)rowα convergesindistributiontoamultivariatestandardnormal
random vector.
16Robust spectral clustering with rank statistics
Applyingtheaforementionedlemmaswithacarefulspectralperturbationanalysisyields
the following theorem.
Theorem 14 (Asymptotic multivariate normality of robust spectral embedding)
Assume the setting and hypotheses in Lemma 13. Further suppose that
(cid:104) (cid:105)
Γ(k) = lim Cov n−1/2((R(cid:101)A−E[R(cid:101)A])Θ)
rowi
: g
i
= k ,
n→∞
(cid:110) (cid:111)
Ξ = lim n1/2(ΘTΘ)−1/2 ·B(cid:101)−1·(cid:8) n(ΘTΘ)−1(cid:9) ,
n→∞
where each limit matrix is rank K. There exist sequences of K ×K orthogonal matrices
W ≡ W(n) ,W ≡ W(n), indexed by n, such that as n → ∞, the random vector
⋆ ⋆
(cid:16) (cid:16) (cid:17) (cid:17)
n U(cid:98) −UW
⋆
W : g
i
= k
rowi
converges in distribution to a K-dimensional multivariate normal random vector with mean
zero and covariance matrix Ξ·Γ(k)·ΞT.
Theorem 14 establishes asymptotic normality of the leading eigenvector components of
R(cid:101)A by leveraging the perturbation analysis used to establish Theorem 8. At a high level,
(cid:110)(cid:16) (cid:17) (cid:111)
n·(U(cid:98) −UW ⋆)
rowi
≈ n· R(cid:101)A−E[R(cid:101)A] Θ(ΘTΘ)−1B(cid:101)−1(ΘTΘ)−1/2WT .
rowi
Inparticular,forlargen,theindividualrowsoftheembeddingexhibittheblock-conditional
Gaussian approximation
n1/2·U(cid:98)rowi : g
i
= k ≈! Z⃗
k
∼ N K(mean(k),cov(k)/n) (14)
involving block-specific mean vectors and covariance matrices. Eq. (14) is useful because in
practice, data embeddings are commonly plotted all at once, e.g., the rows of n1/2 ·U(cid:98) are
plottedasapointcloudinRK. Nevertheless,Eq.(14)shouldnotberegardedasasubstitute
for the stated result, hence we emphasis caution through the use of an exclamation mark.
InTheorem14, thescalingisn, ratherthann1/2. Thisemergesduetothedelocalization
behavior of U(cid:98) and U in our blockmodel setting. Loosely speaking, a factor of n1/2 is needed
tostabilizetherowsofU(cid:98) andthenanadditionalfactorofn1/2 isneededtoestablishstability
of the fluctuations giving rise to convergence in distribution.
In the numerical examples that follow, in accordance with common practice in the
literature, we plot the rows of n1/2 ·U(cid:98) all at once given A. Collectively, these embedding
vectors plausibly approximately resemble a mixture distribution of Gaussian components,
though we emphasize that formally Theorem 14 holds only for individual vectors, not for
the entire point cloud all at once.
We emphasize that Lemmas 12 and 13 and Theorem 14 do not assume that any of the
data are normally distributed. Rather, asymptotic normality emerges due to the interplay
of having commensurate block sizes, being able to apply classical asymptotic normality
results for simple linear rank statistics, and careful entry-wise and row-wise eigenvector
perturbation analysis.
17Cape, Yu, and Liao
5.5 Selecting the embedding dimension
Throughout the main results, rank(B(cid:101)) is assumed known and equals K, the true number
of blocks or communities in the data. To facilitate discussion, define d = rank(B(cid:101)). If d
is unknown, then it is possible to devise a selection procedure based on an eigenvalue gap
in the scree plot of R(cid:101)A. Specifically, let d(cid:98)≡ d(cid:98)n denote the number of eigenvalues of R(cid:101)A
whose magnitude exceeds 4n3/4+ν for any user-specified constant 0 < ν < 1/4. Lemma 15
provides support for this choice of selection procedure and is proved in Appendix A.5.
Lemma 15 (Selecting the embedding dimension via the scree plot of R(cid:101)A) Suppose
A ∼ Blockmodel(FK,Θn×K) where rank(Θ) = rank(B(cid:101)) = K and K = O(1). Further
suppose that n
min
is order n/K, and assume that liminf n→∞|λ K(B(cid:101))| ≥ δ for some δ > 0.
Fix 0 < ν < 1/4. Then,
(cid:104) (cid:105)
lim Pr |λ k(R(cid:101)A)| > 4n3/4+ν = 1 for each 1 ≤ k ≤ K;
n→∞
(cid:104) (cid:105)
lim Pr |λ K+i(R(cid:101)A)| > 4n3/4+ν = 0 for each i ≥ 1.
n→∞
In simulations, the above rule tends to under-select for small-to-moderate values of n.
This happens in part because the absolute constant 4 has been chosen for convenience
and is not sharp. Moreover, the optimal rate is anticipated to be closer to n1/2, not n3/4.
Additional empirical explorations suggest that 1.001·n1/2 is often adequate. For example,
this threshold choice correctly selects d(cid:98)= 2 in Section 3.3.
6 Numerical examples
This section provides numerical examples that illustrate the theoretical results in Section 5
and suggest opportunities for future work.
6.1 Heterogeneous heavy-tailed data
Consider a K = 2 blockmodel per Definition 1 with block-specific distributions given by
F = Pareto(m ,1), F = Pareto(m ,2), F = Pareto(m ,3).
(1,1) 1 (1,2) 2 (2,2) 3
Of note, ξ ∼ Pareto(m,γ) with m,γ > 0 exhibits the tail behavior
(cid:40)(cid:0)m(cid:1)γ
if x ≥ m,
Pr[ξ > x] = x
1 if x < m.
Furthermore,
(cid:40) γ·m if γ > 1, (cid:40) γ·m2 if γ > 2,
E[ξ] = γ−1 and Var[ξ] = (γ−1)2(γ−2)
∞ if γ ≤ 1; ∞ if γ ≤ 2.
Moreover, the first T moments of Pareto(m,γ) exist (i.e., are finite) only when γ > T.
Consequently, heavy-tailedness can be present and inherent to the blockmodel here. There
is no concept of contamination or outliers per se.
18Robust spectral clustering with rank statistics
Matrix of raw data Matrix of normalized ranks
Value Value
15
0.75
10 0.50
5 0.25
0 0.00
Column Column
Figure 2: Simulation example in Section 6.1. Row and column indices indicate correspond-
ing node labels. Nodes are ordered to illustrate approximate block structure. Brown dots
on the left represent values of the original data below the 1% percentile or above the 99%
percentile; these values corrupt the raw data embedding in Fig. 3. Here, the matrix of
normalized ranks reflects K = 2 and satisfies the conditions for consistency in Section 5.
In this model, the entry B of B does not necessarily exist. In contrast, denoting block
11
sizes by n = π ×n, n = (1−π )×n, and considering the large-n limit yields an explicit
1 1 2 1
expression for B(cid:101)11 as follows.
 
1 2 3m m1 2 if m 1 <m 2 43 mm 31 if m 1 <m 3
nl →im ∞B(cid:101)11 = 2π 12+2(1−π 1)π 1×
3m2 1−m2
2 else
+(1−π 1)2×
4m3 1−m3
3 else
3m2 4m3
1 1
In fact, all limiting entries of both B(cid:101) and S(cid:101) admit closed-form expressions but are omitted
for brevity.
We simulate a single hollow data matrix A ∈ Rn×n with n = 400 and π = 0.25 and
1
m = i for 1 ≤ i ≤ 3. Consequently, for observables in block (1,1), neither the mean nor
i
the variance exists; for observables in block (1,2), the mean exists but not the variance;
for observables in block (2,2), both the mean and the variance exist but without higher
momentsexisting. TherawdatamatrixandmatrixofnormalizedranksareshowninFig.2.
Here, the membership matrix Θ ∈ {0,1}400×2 is deliberately ordered for visual convenience,
hence the latent block structure is apparent, yet we emphasize that Θ is neither observed
nor ordered in practice. In Fig. 3 we deliberately over-embed into four-dimensional space,
plotting the rows of n1/2·U(cid:98)A and n1/2·U(cid:98) R(cid:101). In Panel A, the embedding of the raw data is
manifestly degenerate and of no use for clustering. Notice the visible ‘localization’ of the
eigenvector components; this suggests that regularization or normalization may be needed
for spectral clustering. In Panel B, perfect clustering is manifest via the first and second
coordinate dimensions, even for this small value of n. Further, approximate multivariate
normality of each block-specific point cloud is plausible in Panel B of Fig. 3.
19
woR woRCape, Yu, and Liao
(A) Non−robust embedding (B) Robust embedding
Coord. 1 Coord. 2 Coord. 3 Coord. 4 Coord. 1 Coord. 2 Coord. 3 Coord. 4
40
15
20 10
5
0 0
5
0
0
10 2
0 0
−10 −2
10 2
0 0
−10 −2
0 5 10 0 5 −10 0 10 −10 0 10 0.81.0 0 −2 0 2 −2 0 2
Figure 3: Four-dimensional embedding of the raw data (Panel A) and after passing to
ranks (Panel B) in Section 6.1. Coordinates correspond to the four leading eigenvectors.
The underlying population-level true dimension equals two. Colors and point shapes both
denote unobserved block memberships. Perfect clustering is apparent in the first column of
Panel B displaying the robust rank-based embedding.
6.2 Robustness and stability in low-dimensional embeddings
This subsection illustrates that pass-to-ranks spectral embedding exhibits robust dimen-
sionality reduction properties when heavy-tailed contamination is introduced to original
data whose embedding is itself non-degenerate.
WesampleadatamatrixA ∈ Rn×n fromaK = 3blockmodelperDefinition1comprised
of block-specific Gaussian distributions, F = Normal(µ ,σ2 ) for 1 ≤ k ≤ k′ ≤ 3.
(k,k′) kk′ kk′
Specifically, we consider the balanced setting, n = n = n = n/K, with n = 1800 and
1 2 3
parameter values µ = 2, µ = 0.5, µ = 1.5, µ = 1, µ = 2.5, µ = 0.5 and σ = 8,
11 12 13 22 23 33 11
σ = 2, σ = 5, σ = 2, σ = 4, σ = 3. Note that the variance profile is block-
12 13 22 23 33
wise heteroskedastic. Further, one can directly proceed with eigenvector-based spectral
clustering applied to A, denoted as ‘Original’ in Fig. 4.
Next, we perturb A by replacing each upper-triangular entry A with probability 0.01
ij
byanindependentdrawfromCauchy(µ ,1), yieldingthestochasticmatrixA′ (preserving
gigj
symmetry). WethenapplyAlgorithms1and2toA′, yieldingtheafter-perturbationrobust
‘PTR’pointcloudsinFig.4. Noticethecloseagreementbetweenthem. Ontheotherhand,
an embedding of A′ (not shown) would exhibit the degenerate, uninformative behavior
similar to Panel A in Fig. 3.
6.3 Comparing asymptotic variances and covariances
Consider a K = 2 blockmodel per Definition 1 comprised of the distributions
F = Normal(µ ,σ2), F = Normal(µ ,σ2), F = Normal(µ ,σ2),
(1,1) 1 1 (1,2) 2 2 (2,2) 3 3
20
Coord.
1
Coord.
2
Coord.
3
Coord.
4
Coord.
1
Coord.
2
Coord.
3
Coord.
4Robust spectral clustering with rank statistics
Stability of robust embedding
Coord. 1 Coord. 2 Coord. 3
4 Corr: −0.496*** Corr: −0.482***
1:Original: −0.769*** 1:Original: −0.091*
1:PTR: −0.814*** 1:PTR: −0.405***
2 2:Original: −0.354*** 2:Original: −0.395***
2:PTR: −0.678*** 2:PTR: −0.390***
3:Original: −0.772*** 3:Original: −0.385***
0 3:PTR: −0.815*** 3:PTR: −0.527***
2 Corr: −0.515***
1:Original: −0.397***
1 1:PTR: −0.191***
2:Original: −0.474***
0 2:PTR: −0.399***
3:Original: 0.011
3:PTR: −0.049
2
1
0
0 1 2 0 1 2 0 1 2
Figure4: ShownareoverlaidembeddingpointcloudsderivedfromAandR(cid:101)A′ inSection6.2,
where A′ is corrupted by Cauchy entries. Coordinates correspond to the three leading
eigenvectors and have been rotated to the positive orthant. The above-diagonal panels
show correlations for cluster-specific embeddings, asterisks indicate their level of statistical
significance under the null hypothesis of zero correlation, and cluster labels are denoted by
‘1’, ‘2’, ‘3’. Using normalized rank statistics preserves properties of the original embedding.
specifically with µ = µ , µ ̸= µ , and in the balanced regime with n = n = n/K. For
1 3 1 2 1 2
this setting, the proof of Theorem 14 can be modified to show that for U(cid:98)A, there exists a
sequence of 2×2 matrices T(n) such that as n → ∞,
√ n(cid:18) T(n)·U(cid:98)A,rowi−(cid:20) −1 1(cid:21)(cid:19) : g
i
= 2 −d → N 2(0,Σ(2)),
where the asymptotic covariance matrix Σ(2) has upper-triangular entries Σ(2) = 2(σ 22+σ 32)
11 (µ2+µ3)2
(2) 2(σ2−σ2) (2) 2(σ2+σ2)
and Σ = 2 3 and Σ = 2 3 . Here, the sign difference in the second coordinate
12 µ2 2−µ2
3
22 (µ2−µ3)2
is responsible for distinguishing the first and second blocks, just as in Fig. 1.
Using Theorem 14, we obtain a similar asymptotic normality result but for U(cid:98) , again
R(cid:101)
centered around (1,−1)T but whose asymptotic covariance Σ(cid:101)(2) ≡ Σ(cid:101)(2)(F,Θ) is a function
of B(cid:101) and S(cid:101) rather than B and S. Consequently, using the concept of asymptotic relative
efficiency (ARE), it becomes possible to compare pass-to-ranks spectral clustering with
adjacency-based spectral clustering for large n via the ratio of asymptotic variances
(cid:46)
(2) (2)
Σ (F,Θ) Σ(cid:101) (F,Θ). (15)
22 22
Small values of Eq. (15) indicate that non-robust spectral clustering with A is relatively
‘more efficient’ in the present model, whereas larger values of Eq. (15) indicate that pass-
to-ranks robust spectral clustering with R(cid:101)A is comparatively ‘more efficient’.
Fig. 5 plots the ratio in Eq. (15) for two different settings. In the left panel, the
relationship between the block means is varied. In the right panel, the shared variance is
21
Coord.
1
Coord.
2
Coord.
3Cape, Yu, and Liao
Ratio Ratio
1.2 1.2
γ=2
1.0 1.0
γ=3
0.8 0.8
γ=4
σ=1
0.6 γ=5 0.6
σ=2
0.4 0.4
σ=3
0.2 0.2
σ=4
μ μ
5 10 15 20 5 10 15 20
Figure5: TheratioofasymptoticvariancesperEq.(15)isplottedasafunctionofunderlying
model parameters (µ,γ) and (µ,σ), respectively. Ratio values larger than one (i.e., above
the dashed black line) indicate asymptotic, theoretical grounds on which to favor robust
spectral clustering, and vice versa. Plotted curves are obtained by the numerical evaluation
of analytically derived functions, not by simulation. See Section 6.3 for details.
varied. Specifically, µ = µ = µ and γ = µ/µ and σ = σ = σ = σ . In the left panel, µ
1 3 2 1 2 3
and γ vary with σ = 1, while in the right panel, µ and σ vary with γ = 2. The blue curve
is identical in both plots.
The behavior in Fig. 5 is instructive. All else equal, larger values of µ correspond to
stronger signal strength vis-`a-vis separation of cluster centroids, behavior that is directly
reflected in A but diminished in R(cid:101)A. Transforming to normalized ranks typically reduces
bothblock-wisevariancesandexpectations,henceitsoveralleffectisnotalwaysimmediately
clear, but Fig. 5 shows robust spectral clustering with rank statistics can boost signal
particularly when the original block-wise variances are large.
6.4 Matrix rank versus number of communities
In the present paper, our theoretical results impose no explicit assumption on rank(B) but
rather require rank(B(cid:101)) = K. This requirement is generally milder since it is often satisfied
forentrywiseranktransformationsprovidedtheblock-specificdistributionsarenotidentical
orsymmetricaboutthesamelocationparameter. Seetheexplicit,finite-nexpressionforthe
entries of B(cid:101) in Appendix A.1. Below, we provide details for two specific example settings.
See also Section 8.1 for more discussion.
6.4.1 Example with block-specific Gamma distributions
ConsidertheGamma(α,θ)distributionhavingexpectationα×θwithshapeparameterα > 0
and scale parameter θ > 0. Consider a K = 2 blockmodel per Definition 1 where F =
(1,1)
Gamma(3,1/3) and F = Gamma(2,1/2) and F = Gamma(1,1) and n = n = n/K.
(1,2) (2,2) 1 2
Consequently, the blockwise mean and variance matrices are given by
(cid:20) (cid:21) (cid:20) (cid:21)
1 1 . .333 .5
B = , S◦2 = ,
1 1 .5 1
22Robust spectral clustering with rank statistics
with rank(B) = 1. On the other hand, for large n, the blockwise means and variances after
converting to ranks are given by
(cid:20) (cid:21) (cid:20) (cid:21)
. .531 .507 . .0615 .0790
B(cid:101) = , S(cid:101)◦2 = ,
.507 .452 .0790 .110
.
with rank(B(cid:101)) = 2 and |λ K(B(cid:101))| = .0169.
6.4.2 Example with block-specific Exponential distributions
Consider the Exponential(θ) distribution with expectation θ > 0. Consider a K = 2 block-
model per Definition 1 where F ,F ,F are each Exponential(·) such that
(1,1) (1,2) (2,2)
B = (cid:20) µ2 1 µ 1µ 2(cid:21) = (cid:20) µ 1(cid:21) (cid:2) µ µ (cid:3) , S◦2 = B◦B,
µ µ µ2 µ 1 2
1 2 2 2
where without loss of generality µ ≥ µ > 0. Further suppose n = π × n and n =
1 2 1 1 2
(1−π 1)×n for some 0 < π
1
< 1, hence rank(Θ) = 2. In the large-n limit, the entries of B(cid:101)
are given by the closed-form expressions
1 2µ µ2
nl →im ∞B(cid:101)11 = 2π 12+
µ
1+1
µ
2π 1(1−π 1)+
µ2
1+1
µ2
2(1−π 1)2,
µ µ
2 1
lim B(cid:101)12 = π 1+ (1−π 1),
n→∞ µ 1+µ 2 µ 1+µ 2
µ2 2µ 1
nl →im ∞B(cid:101)22 =
µ2
1+2
µ2
2π 12+
µ
1+2
µ
2π 1(1−π 1)+ 2(1−π 1)2.
Closed-form expressions are also available for the entries of S(cid:101)◦2 in the large-n limit but are
substantially more complicated and therefore omitted for brevity.
Observethatthecommunitymembershipmatrixhassizen×2andrank2,whilethe2×2
matrix B of connectivity probabilities has rank one. In this case, the leading eigenvector
of the raw data separates blocks, whereas the leading two eigenvectors of the pass-to-ranks
matrix discriminate between the blocks.
6.5 Extensions and complements
This section concludes with empirical findings that serve to inspire future work.
6.5.1 Towards optimal rates and constants
The results in Section 5 are seemingly the first of their kind to establish consistency results
for spectral clustering applied to matrices of normalized rank statistics. Still, there is room
for improvement. We conjecture that for some model settings with K fixed, ∥(U(cid:98)U(cid:98)T) −
R(cid:101)
UUT∥2 converges in probability to a non-zero constant as n → ∞ and that this occurs
F
at the same scaling as the convergence of ∥(U(cid:98)U(cid:98)T)
A
−UUT∥2 F. This conjecture posits an
improvement in the rate obtained in Theorem 7 and is motivated by existing results which
establish the concentration of ∥(U(cid:98)U(cid:98)T) A−UUT∥2
F
and closely related expressions for certain
symmetricdatamatricesAhavingindependententrieswithsufficientlylighttails(e.g.,Tang
23Cape, Yu, and Liao
Subspace recovery comparison
8
Ratio
6 1.6
1.2
4
0.8
2
0.0 0.2 0.4 0.6 0.8
Contamination level (epsilon)
Figure 6: Monte Carlo approximation of the theoretical ratio given by Eq. (16) in the
contaminated normal distribution example. The solid black curve illustrates parameter val-
ues (ϵ,τ) for which the approximated ratio equals one, namely for which there is identical
asymptotic performance as quantified by limiting leading constants. Robust spectral clus-
teringwithR(cid:101)A outperformstraditionalspectralclusteringwithAingeneralforlargevalues
of contamination and (or) large values of scale in the contaminating distribution.
et al. (2017); Tang and Priebe (2018); Xu (2018)). It remains to more deeply understand
the asymptotic, theoretical distinction between using A versus R(cid:101)A.
To briefly investigate this conjecture, consider a K = 2 blockmodel per Definition 1
whose data matrix entries are drawn from contaminated normal distributions of the form
F = (1−ϵ)·Normal(µ ,σ2 )+ϵ·Normal(µ ,τ ·σ2 ), 1 ≤ k ≤ k′ ≤ 2.
(k,k′) kk′ kk′ kk′ kk′
Set µ = µ = 6, µ = 4, and σ = σ = σ = 0.5. We vary τ > 0, the variance
11 22 12 11 12 22
multiplier in the contaminating distribution, and 0 ≤ ϵ ≤ 1, the contamination level.
We generate n
b
= 50 i.i.d. replicates A(1),...,A(n b) ∈ Rn×n, n = 500, each having equal
block sizes n = n/K for 1 ≤ k ≤ K. The contour plot in Fig. 6 shows
k
1 (cid:88)n b (cid:13) (cid:13)(cid:16) (cid:17)(b) (cid:13) (cid:13)2 (cid:46) 1 (cid:88)n b (cid:13) (cid:13)(cid:16) (cid:17)(b) (cid:13) (cid:13)2
(cid:13) U(cid:98)U(cid:98)T −UUT(cid:13) (cid:13) U(cid:98)U(cid:98)T −UUT(cid:13)
n b b=1(cid:13) R(cid:101) (cid:13) F n b b=1(cid:13) A (cid:13) F
which is an empirical approximation to the theoretical ratio
(cid:104) (cid:105) (cid:104) (cid:105)
E ∥(U(cid:98)U(cid:98)T) −UUT∥2 1−E r2 (U(cid:98) ,U)
R(cid:101) F K R(cid:101)
= (16)
(cid:104) (cid:105) (cid:104) (cid:105)
E ∥(U(cid:98)U(cid:98)T) A−UUT∥2
F
1−E r K2 (U(cid:98)A,U)
as a function of the parameter tuple (ϵ,τ). Values of Eq. (16) smaller than one indicate
the pass-to-ranks leading eigenvectors estimate the population eigenvectors better than the
leading eigenvectors of the raw data matrix, whereas the opposite is true for values of
Eq. (16) larger than one. In this example, neither approach dominates the other.
24
)uat(
reilpitlum
ecnairaVRobust spectral clustering with rank statistics
Latent mixed membership Rank−based embedding
0 1 0 1
Figure 7: Pass-to-ranks robust spectral embedding (right panel) can reflect latent geometry
(left panel) beyond mere block structure. Coordinate axes correspond to leading eigenvec-
tors. See Section 6.5.2.
The empirical ratio values displayed in Fig. 6 remain stable for larger values of n (not
shown), namely they do not appreciably change, supporting the above conjecture. In future
work, it would be interesting to derive an explicit closed-form expression for the limiting
ratio of leading constants suggested by Fig. 6.
6.5.2 Beyond block structure
Suppose instead that the rows of Θ lie within the unit simplex, for example, Θ ∼
rowi
Dirichlet(α⃗) i.i.d. for 1 ≤ i ≤ n. The population-level matrix ΘBΘT is then no longer
of pure block structure per se but rather exhibits soft or mixed block structure, akin to
mixed-membership stochastic blockmodel random graphs (Airoldi et al., 2008). Here then,
the population embedding becomes a simplicial region, rather than a collection of discrete
cluster centroids (Mao et al., 2021; Rubin-Delanchy et al., 2022; Jin et al., 2024). For the
purpose of illustration, Fig. 7 visually demonstrates that eigenvector embeddings derived
from normalized rank statistics are capable of reflecting simplicial latent geometry. In this
example, n = 1800, α⃗ = (1/2,1/3,1/6), B = 3, B = B = 2, B = B = B = 1, and
11 12 22 13 23 33
a symmetric random matrix of Gaussian noise having mean zero and standard deviation
equal to one-half perturbs the expectation matrix. Developing theoretical performance
guarantees for such settings will presumably require new technical tools.
7 Connectome data analysis
This section illustrates robust spectral clustering with rank statistics applied to a diffu-
sion magnetic resonance imaging (dMRI) dataset. The dataset comprises 114 connectomes
(loosely speaking, “brain graphs” or “brain networks”) derived from 57 human subjects
with two scans each. Each connectome is represented as a weighted network using the Neu-
roData MR Graphs data processing pipeline (Kiar et al., 2018). Vertices or nodes represent
spatially proximal brain sub-regions of interest, called ROIs, while edges or links represent
tensor-basedfiberstreamlinecountsconnectingdifferentsub-regions. Importantly,eachver-
tex in each network has an associated Left/Right brain hemisphere label and an associated
25
2
1
0
2
1
0Cape, Yu, and Liao
30 4e+05
3e+05
20
2e+05
10
1e+05
0 0e+00
1190 1200 1210 1 2 3 4 5 6 7 8 9 1011121314151617181920
Vertex count Index of graph
0.20
Raw, k−means PTR, k−means
8
0.15
6 0.10
0.05
4
0.00
4 6 8 1 10 20 30 40 50 60 70 80 90 100 110
Raw embedding dimension Index of graph
Figure 8: Connectome data summary and analysis. Top left: vertex counts for all graphs.
Top right: vertex strength (weighted degree) distributions for twenty graphs. Bottom left:
frequency of embedding dimensionality selection estimates. Bottom right: performance of
k-means clustering for recovering the Left/Right Gray/White partition.
Gray/Whitematterlabel. Theselabelsserveasabenchmarkforevaluatingtheperformance
of different clustering approaches and yield the ground-truth labels {LG,LW,RG,RW}.
WeconsiderthedatasetDS01876, publiclyavailablefromJohnsHopkinsUniversityand
documented in more detail in Lawrence et al. (2021). Each network is obtained by taking
the largest connected component of the induced subgraph on the vertices labeled as both
Left or Right and Gray or White. For one of the connectomes, a scanning error occurred
during acquisition, and consequently we omit it from our analysis and discussion. Our
analysis complements the recent work on spectral clustering in Priebe et al. (2019), though
therein the authors consider different methodology and a different data resolution.
The top two panels in Fig. 8 provide a partial visual summary of the data. The top-
left panel shows that all graphs have circa 1200 vertices, while the top-right panel shows
the vertex strength (i.e., weighted degree) distributions for twenty graphs. In fact, all
graphsexhibitvertexstrengthheterogeneitywithhighlyright-skeweddistributions. Table1
provides summary statistics highlighting the severe disparity of vertex strengths as well as
graph edge weights, suggesting the possible need for robust methods.
For each graph index, i, we separately analyze the raw weighted adjacency matrix A(i)
and its rank transformation R(cid:101)(i). Edge weights are not necessarily unique for graphs in
this dataset, so we use the corresponding average of normalized rank values when rank-
transforming the data. We apply the profile likelihood based approach of Zhu and Ghodsi
26
ycneuqerf
hparG
noisnemid
gniddebme
RTP
noitubirtsid
htgnerts
xetreV
xedni
dnaR
detsujdARobust spectral clustering with rank statistics
Median Mean Maximum
Edge weight 102 453 18267
Vertex strength 4841 18365 253215
Table 1: Connectome data summary statistics. Each cell value denotes the median of the
corresponding summary statistic computed across all graphs. Cell values are rounded to
the nearest integer.
(2006) to the 50 largest singular values of each matrix in order to select the number of
(i) (i)
leading eigenvectors to retain (i.e., embedding dimensionality), denoted by d(cid:98)raw and d(cid:98) ptr.
To mitigate the effect of singular value shadowing, we exclude the largest data singular
value when applying this method. The bottom-left panel of Fig. 8 plots these tuples as
(i) (i)
(d(cid:98)raw,d(cid:98) ptr), with larger black circles denoting more frequent occurrences. For 23 of the 113
graphs, the values in these tuples are equal to each other. More notably, for nearly two-
thirds of the graphs (75 of 113), fewer dimensions (i.e., fewer eigenvectors) are selected for
embedding and clustering when using the matrix of normalized rank statistics than when
using the raw adjacency matrix.
Using k-means clustering with four clusters, the bottom-right panel of Fig. 8 shows
that robust spectral embedding and clustering achieves higher adjusted Rand index (ari)
values (Hubert and Arabie, 1985) than traditional, non-robust adjacency-based spectral
embedding and clustering. Here, ari is computed relative to the ground-truth clustering
given by {LG,LW,RG,RW}, and we report the average value over one hundred runs of k-
means clustering applied to each graph embedding. For reference, the corresponding label
proportions across all subjects are approximately given by {32.5%,17%,32.5%,18%}.
In general, ari values typically lie between zero and one, with large values indicating
substantial partition agreement and small values indicating substantial partition disagree-
ment. Although the (average) ari values displayed in Fig. 8 are rather small, this is be-
cause the underlying clustering task is difficult due to non-stylized, weak signal in the data.
Furthermore, {LG,LW,RG,RW}yieldsanaturalchoiceofgroundtruthforcomparingper-
formance but is not necessarily the only neuroscientifically meaningful node partition for
comparison. See Priebe et al. (2019) for related findings and discussion.
In summary, this section briefly illustrates the usefulness of robust spectral embed-
ding and clustering in a neuroscientific data setting via improved clustering performance
(i) (i)
(i.e., partition recovery), together with the fact that d(cid:98)
ptr
≤ d(cid:98)raw for the majority of graph
indices i. This analysis shows that using rank statistics can de-noise the original data in a
parsimonious manner that reflects underlying neuroanatomical cluster structure.
8 Discussion
This section discusses extensions, practical considerations, and related open problems per-
taining to robust spectral clustering with rank statistics.
27Cape, Yu, and Liao
8.1 Embedding dimension versus the number of blocks
Within contemporary multivariate analysis, there is growing interest in understanding the
properties of data embeddings when the dimension d differs from the number of ground-
truthorestimatedblocks(communitiesorclusters)K. Inthelanguageofblockmodels, this
can be viewed in the manner d = rank(B) for a K×K connectivity matrix B. In practice,
outside of blockmodels, algorithms often yield estimates satisfying d(cid:98)≪ K(cid:98). Even so, the
majority of existing works on spectral methods assume d = K when establishing theoretical
guarantees, with two recent exceptions including Tang et al. (2022); Zhang et al. (2022b).
A more refined understanding of consistency and asymptotic normality when d < K is
currently underway.
8.2 Adjacency versus Laplacian representations of networks
This paper focuses on developing theory for the matrix of normalized rank statistics, R(cid:101)A. It
may be possible to derive similar results for graph Laplacians, L(R(cid:101)A), in part by adapting
the techniques and analysis found in Tang and Priebe (2018). That being said, by its very
nature, the matrix of normalized rank statistics cannot exhibit significant row-sum hetero-
geneity nor is it (even approximately) sparse. Consequently, the benefit of and motivation
for Laplacian-based clustering are less clear.
8.3 Communities in the presence of similarities and dissimilarities
Definition 1 is a flexible model for generating weighted graphs (networks), permitting both
positive edge weights (indicating similarity) and negative edge weights (indicating dissimi-
larity). Different block-specific distributions may have different support sets, for example,
supp(F ) ⊆ [0,∞) while supp(F ) ⊆ (−∞,0] while supp(F ) = R. Regardless, the
(1,1) (1,2) (2,2)
interpretation of inferred communities (memberships) is unchanged, namely connectivity
is driven by population-level low-rank structure that separates the block-specific distribu-
tions. While the matrix of normalized rank statistics and its expectation are entrywise
non-negative, the entries themselves encode the relationship between positive and negative
edge weights via their functional dependence on the block-specific distributions.
8.4 Rectangular data matrices
The rank-transform method in Section 2 can be naturally adapted to handle asymmetric
or more general rectangular data matrices A ∈ Rn×m by ranking all n × m (distinct)
entries. Inpreliminaryinvestigations,theleftandrightsingularvectorsofR(cid:101)Acanaccurately
recover the left-specific and right-specific latent block structure, respectively, when present.
Theoretical guarantees for such settings will be established in future work.
8.5 Literature on entrywise eigenvector analysis
This paper and its contributions are related to the literature studying entrywise eigenvector
estimation and high-dimensional PCA, building on classical tools in linear algebra such as
Weyl’s inequality, the Davis–Kahan theorem, and the Wedin sine theta subspace theorem
(e.g., see the original papers or the textbooks Stewart (1990); Bhatia (2013) for more). Be-
28Robust spectral clustering with rank statistics
yond the references cited in Section 1, recent works in this vein include Perry et al. (2018);
Zhong and Boumal (2018); Chen et al. (2019); Lei (2019); Cai et al. (2021); Li et al. (2021);
Abbeetal.(2022);Agterbergetal.(2022);Jinetal.(2022). Manyoftheseworkscontribute
results in so-called ‘signal-plus-noise’ model settings for adjacency matrices, covariance ma-
trices, Gram-type matrices, and factor model settings, albeit not involving rank statistics
as considered in this paper.
8.6 Precluding certain stochastic blockmodel graphs and variants
The contributions of this paper are distinct from but complement existing works studying
regularization and normalization in stochastic blockmodels (Amini et al., 2013; Sarkar and
Bickel, 2015; Joseph and Yu, 2016). In the social sciences, statistics, and machine learning
applications involving random graphs and network data, significant attention has been paid
to (unweighted) stochastic blockmodels (Holland et al., 1983) including mixed-membership
(Airoldi et al., 2008), degree-corrected (Karrer and Newman, 2011), and weighted variants
(Xu et al., 2020). Definition 1 excludes the traditional formulations of these models, since
a ∈ {0,1} would lead to ties among the normalized ranks, which conflicts with the setup
ij
in Algorithm 1. Rather, Definition 1 can be viewed as producing weighted adjacency ma-
trices A = (a ), associated with weighted complete graphs, whose entries represent nodes’
ij
pairwise edge weights, specified via the block-specific distributions F ∈ FK. Definition 1
permits (but does not require) both positive and negative edge weights, a ∈ R, in contrast
ij
to the classical paradigm for random graphs which typically requires a
ij
≥ 0. Finally, R(cid:101)A
can itself be viewed as a weighted non-negative adjacency matrix of dependent entries.
8.7 Concentration inequalities and existing benchmarks
In order to obtain consistency guarantees, we first establish Lemma 21 which yields ∥R(cid:101)A−
E[R(cid:101)A]∥
op
= OP(n3/4+ν). In contrast, the traditional bound OP(n1/2) holds for symmet-
ric random matrix models of the form M −E[M] when the entries m are independent,
ij
not-necessarily identically distributed, and sufficiently light-tailed, e.g., matrices with het-
erogeneous yet uniformly bounded variance profiles whose entries are Bernoulli-distributed
(Lei and Rinaldo, 2015), Gaussian-distributed (Bandeira and van Handel, 2016), or similar
(Tao, 2012). In the present paper, however, the centered, symmetric matrix of normal-
ized rank statistics, R(cid:101)A −E[R(cid:101)A], evidently does not consist of independent entries (up to
symmetry). Nevertheless, entrywise covariances are weak, behaving as O(n−2), and the
variance profile is well behaved given that all entrywise variances are uniformly bounded
(see Proposition 18). We conjecture that an improved bound of (almost) OP(n1/2) may
hold, requiring a more involved proof strategy, and will investigate this question in future
work. If true, then the weak consistency bounds herein can be upgraded to (almost) match
those found in Lei and Rinaldo (2015) for clustering stochastic blockmodel random graphs.
This paper establishes that the pass-to-ranks spectral embedding vectors, U(cid:98)rowi, prop-
erly transformed, are asymptotically multivariate Gaussian in a manner governed by their
block memberships. Such results have previously been established for eigenvectors of large
Bernoulli blockmodel matrices, e.g., see Cape et al. (2019a); Rubin-Delanchy et al. (2022)
andelsewhere. Incontrast,theresultsinthispaperareseeminglyamongthefirsttoaddress
robust rank statistic eigenvector-based embeddings and their asymptotics.
29Cape, Yu, and Liao
9 Conclusion
This paper rigorously studies a statistically principled approach for robust clustering and
dimensionreduction. Theideaofclusteringrank-transformeddatahasappearedpreviously,
as an applied tool and practical work-around, but receiving only brief attention, e.g., a
passing comment in Athreya et al. (2018), and a remark in the appendix of Levin et al.
(2020). Our in-depth investigation yields results that collectively address weak consistency,
node-specific strong consistency, and asymptotic normality, all at once.
Open questions remain. It will be interesting to determine optimal misclustering rates
and related properties of robust spectral clustering with rank statistics. The current paper
considers K ≪ n, so in the future it will be desirable to understand the fundamental
limits, trade-offs, and computational barriers involving K. Computing (exact) rank values
for all elements of a massive data matrix can be computationally expensive, so it will be
interesting to explore this challenge and compare it to the potential statistical benefit of
approximateranking. Aspointedoutbyareferee,beyondblockmodels,itwillbeinteresting
to study the extent to which ordinal transformations are useful when clustering Euclidean
data based on constructed neighborhood graphs. In addition, there may yet be further,
unexplored connections between robust spectral clustering and the classical literature on
nonparametric rank tests.
10 Acknowledgments and Disclosure of Funding
TheauthorsthanktheAssociateEditorandthereviewersforvaluablefeedbackandthought-
fulsuggestions. TheauthorsthankCareyE.Priebe, YoungserPark, andDanielL.Sussman
for inspiring conversations and for coining the phrase ‘passing to ranks’. The authors thank
Liza Levina, Marianna Pensky, Karl Rohe, Michael Trosset, and Bodhisattva Sen for con-
structive comments at various stages of this work. JC gratefully acknowledges support
from the National Science Foundation under grant DMS 2413552 and from the University
of Wisconsin–Madison, Office of the Vice Chancellor for Research and Graduate Education,
with funding from the Wisconsin Alumni Research Foundation.
Appendix A.
The appendix contains proof materials and additional details pertaining to the main text.
A.1 Rank statistics and their properties
Let X ,...,X denote a collection of independent absolutely continuous random variables
1 N
with cumulative distribution functions F ,...,F and corresponding densities f ,...,f .
1 N 1 N
Consequently, the set {X }N takes on distinct values with probability one. Denote the
i i=1
order statistics of{X }N byX ≤ X ≤ ··· ≤ X . LetR denotetherank ofX
i i=1 N(1) N(2) N(N) Ni i
among {X }N , i.e., define R according to the equation X = X . Correspondingly,
i i=1 Ni i N(RNi)
define the normalized rank of X
i
among {X i}N
i=1
in the manner R(cid:101)Ni = N1 +1R Ni. We are
principally interested in the normalized ranks, though for convenience in our derivations
and presentation we also define the intermediary normalized rank of X
i
as R(cid:101) N′
i
= N1R Ni,
30Robust spectral clustering with rank statistics
observing that R(cid:101) N′
i
= N N+1R(cid:101)Ni. For ease of notation, we often omit dependence on N,
writing simply R
i
and R(cid:101)i and R(cid:101) i′.
A.1.1 One-block properties of ranks
Suppose the absolutely continuous random variables X ,...,X are independent and iden-
1 N
tically distributed with common cumulative distribution function F and density f. Then,
the vector R = (R ,...,R ) is uniformly distributed on the set of all N! permutations
N N1 NN
of the set N = {1,2,...,N}, whence for each i the distribution of R is given by the
Ni
(cid:74) (cid:75)
discrete uniform distribution with support {1,2,...,N}. It is straightforward to compute
basicpropertiesoftheintermediarynormalizedrankspresentedinthefollowingproposition.
Proposition 16 (One-block rank properties) Assume the setup in Appendix A.1.1.
For each i,i′ ∈ N where i′ ̸= i, the intermediary normalized ranks satisfy
(cid:74) (cid:75)
(cid:104) (cid:105) 1 1 (cid:104) (cid:105) 1 1 (cid:104) (cid:105) −1 1
E R(cid:101) N′
i
=
2
+ 2N, Var R(cid:101) N′
i
=
12
− 12N2, Cov R(cid:101) N′ i,R(cid:101) N′
i′
=
12N
− 12N2. (17)
By basic algebra, it follows from Proposition 16 that the normalized ranks satisfy
(cid:104) (cid:105) 1 (cid:104) (cid:105) 1 1 (cid:104) (cid:105) −1
E R(cid:101)Ni = , Var R(cid:101)Ni = − , Cov R(cid:101)Ni,R(cid:101)Ni′ = . (18)
2 12 6(N +1) 12(N +1)
A.1.2 Multiple-block properties of ranks
Suppose instead that the collection of independent absolutely continuous random variables
X ,...,X is partitioned into m blocks (samples) with deterministic sizes {N } satis-
1 N ℓ ℓ∈ m
fying (cid:80) N = N, where N = |{i : X ∼ F }| for a collection of cumulative dis(cid:74)tri(cid:75)bution
ℓ∈ m ℓ ℓ i ℓ
functions {(cid:74)F(cid:75)} with corresponding densities {f } . Here, R denotes the rank of
ℓ ℓ∈ m ℓ ℓ∈ m Ni
X among {X } (cid:74) (cid:75) . (cid:74) (cid:75)
i i i∈ N
(cid:74) (cid:75)
Below,wecomputefinite-N expectations,variances,andcovariancesinvolvingtheinter-
mediarynormalizedrankstatisticsR(cid:101)′ . Inanefforttoimproveclarity,wewriteexpectation
Ni
expressions as E [·] to emphasize the underlying block membership X ∼ F . Similar nota-
ℓ ℓ
tional shorthand applies to variances and covariances.
Proposition17(Multi-blockrankexpectations) AssumethesetupinAppendixA.1.2.
For each ℓ ∈ m and i ∈ n , the expectation of R(cid:101) N′
i
when X
i
∼ F
ℓ
is given by
(cid:74) (cid:75) (cid:74) (cid:75)
E ℓ(cid:104) R(cid:101) N′ i(cid:105) = (cid:88) N Nℓ′ ×E ℓ[F ℓ′(X)]+ 21 N.
ℓ′∈ m
(cid:74) (cid:75)
Notably, E [F (X)] = 1.
ℓ ℓ 2
31Cape, Yu, and Liao
Proposition 18 (Multi-block rank variances) Assume the setup in Appendix A.1.2.
For each ℓ ∈ m and i ∈ n , the variance of R(cid:101) N′
i
when X
i
∼ F
ℓ
is given by
(cid:74) (cid:75) (cid:74) (cid:75)
   2
Var ℓ(cid:104) R(cid:101) N′ i(cid:105) =  (cid:88) N Nℓ′ N Nℓ′′ ×E ℓ[F ℓ′(X)F ℓ′′(X)]− (cid:88) N Nℓ′ ×E ℓ[F ℓ′(X)]
ℓ′,ℓ′′∈ m ℓ′∈ m
(cid:74) (cid:75) (cid:74) (cid:75) 
+ (cid:88) N Nℓ′ ×E ℓ(cid:2) 2F ℓ′(X)−2F ℓ(X)F ℓ′(X)−F ℓ2 ′(X)(cid:3)  N1
ℓ′∈ m
(cid:74) (cid:75)
1 1
− .
12N2
Notably, E [F2(X)]−E [F (X)]2 = 1 −(1)2 = 1 .
ℓ ℓ ℓ ℓ 3 2 12
In order to state the multi-block covariance formula, define g : R → [0,1] to be the
(X,Y)
(cid:82)z
map g (z) = F (y)f (y)dy, in terms of the cumulative distribution function and
(X,Y) −∞ X Y
probability density of the underlying independent random variables X and Y, respectively.
Let g (z) denote shorthand for the distributional assumption that the independent ran-
(ℓ,ℓ′)
dom variables satisfy X ∼ F and Y ∼ F .
ℓ ℓ′
Proposition 19 (Multi-block rank covariances) Assume the setup in Appendix A.1.2.
For each ℓ,ℓ′ ∈ m and i,j ∈ N where i ̸= j, the covariance of R(cid:101)′ and R(cid:101)′ when
Ni Nj
(cid:74) (cid:75) (cid:74) (cid:75)
X ∼ F and X ∼ F is given by
i ℓ j ℓ′
(cid:104) (cid:105)
Cov
(ℓ,ℓ′)
R(cid:101) N′ i,R(cid:101) N′
j
(cid:32)
= (cid:88) N m′ × E [(1−F (X))(1−F (X))]+E [g (X)]+E [g (X)]
N
m′ ℓ ℓ′ ℓ (m′,ℓ′) ℓ′ (m′,ℓ)
m′∈ m
(cid:74) (cid:75) (cid:33)
1
−E [F (X)]E [F (X)]−E [F (X)]E [F (X)]−E [F (X)]E [F (X)]
ℓ m′ ℓ′ m′ ℓ m′ ℓ′ ℓ ℓ ℓ′ ℓ′ m′
N
(cid:32) (cid:33)
+ E (cid:2) 2F (X)−F (X)F (X)−g (X)−g (X)− 1(cid:3) 1
ℓ ℓ′ ℓ ℓ′ (ℓ,ℓ′) (ℓ′,ℓ′) 3 N2
(cid:32) (cid:33)
+ E (cid:2) 2F (X)−F (X)F (X)−g (X)−g (X)− 1(cid:3) 1
ℓ′ ℓ ℓ ℓ′ (ℓ,ℓ) (ℓ′,ℓ) 3 N2
(cid:32) (cid:33)
1
+ E [F (X)]E [F (X)]− 1
ℓ ℓ′ ℓ′ ℓ 4 N2
1 1
− .
12N2
Notably, E [g (X)] = E [1F2(X)] = 1.
ℓ (ℓ,ℓ) ℓ 2 ℓ 6
Proposition 19 implies a loose yet user-friendly two-sided uniform bound for the inter-
(cid:104) (cid:105)
mediarynormalizedranksgivenby− N8 ≤ Cov
(ℓ,ℓ′)
R(cid:101) N′ i,R(cid:101) N′
j
≤ N7. Thesameboundholds
32Robust spectral clustering with rank statistics
for the normalized rank statistics R(cid:101)Ni since R(cid:101)Ni = NN +1R(cid:101) N′ i. Moreover,
(cid:18)
−3
−5(cid:19)(cid:18)
N
(cid:19)2
(cid:104) (cid:105)
(cid:18)
3 4
(cid:19)(cid:18)
N
(cid:19)2
N
+
N2 N +1
≤ Cov
(ℓ,ℓ′)
R(cid:101)Ni,R(cid:101)Nj ≤
N
+
N2 N +1
.
Propositions 17 to 19 illustrate how generalizing the expectation, variance, and covari-
ance beyond the one-block setting amounts to the following:
1. Leading-ordertermsariseasfunctionalsofthedistributions{F } viaappropriate
ℓ ℓ∈ m
integration and weighting. (cid:74) (cid:75)
2. The higher-order terms in the one-block setting persist in the multi-block setting.
3. Themulti-blockvarianceandcovarianceexpressionsintroduceadditionalhigherorder
interaction-type residual terms. These additional terms arise as a consequence of the
heterogeneity encountered in the multi-block setting and conversely vanish in the
homogeneous one-block setting.
A.2 Proofs: weak consistency
Remark 20 (The moment method and bounding the matrix trace) For any n×n
symmetric matrix M with eigenvalues λ ,...,λ , necessarily real, it holds that ∥M∥ =
1 n op
max |λ | and trace(M) =
(cid:80)n
λ . Moreover, for every positive integer p ≥ 1 it holds
1≤i≤n i i=1 i
that trace(Mp) = (cid:80)n λp, which follows directly from Schur’s triangularization theorem
i=1 i
together with the cyclic property of trace(·). Consequently, for any positive integer p ≥ 1,
it holds that ∥M∥2p ≤ trace(M2p) ≤ n∥M∥2p, which in turn yields the general inequality
op op
(cid:112) √
∥M∥ ≤ 2p trace(M2p) ≤ 2p n·∥M∥ . (19)
op op
For a more detailed treatment of the moment method and its applications throughout random
matrix theory, see for example Tao (2012, Chapter 2).
Lemma 21 (Spectral norm concentration of the centered normalized ranks ma-
trix) Let A be as in Definition 1 and R(cid:101)A be as in Algorithm 1. There exist absolute
constants C ,C > 0 such that for any choice ν > 0, provided K ≤ n1/2 and n ≥ C for
1 2 k 1
all k ∈ K , then
(cid:74) (cid:75) (cid:104) (cid:105)
Pr ∥R(cid:101)A−ΘB(cid:101)ΘT∥
op
> n3/4+ν ≤ C 2·n−ν. (20)
Alternatively, the operator norm exceeds 2C n3/4+ν with probability at most 1n−ν. Further-
2 2
more, the conclusion also holds for ∥R(cid:101)A−E[R(cid:101)A]∥ op.
Proof [Proof of Lemma 21] Consider the matrices R(cid:101)A and ΘB(cid:101)ΘT = E[R(cid:101)A]+diag(ΘB(cid:101)ΘT).
By the triangle inequality,
∥R(cid:101)A−ΘB(cid:101)ΘT∥
op
≤ ∥R(cid:101)A−E[R(cid:101)A]∥ op+∥diag(ΘB(cid:101)ΘT)∥ op.
Furthermore, ∥diag(ΘB(cid:101)ΘT)∥
op
= max 1≤k≤K|B(cid:101)kk| ≤ 1.
33Cape, Yu, and Liao
By Markov’s inequality, for any t > 0 it holds that Pr[∥M∥ > t] ≤
E∥M∥op.
The
√ op t
map t (cid:55)→ 4 t is concave on the domain (0,∞), hence by the concave version of Jensen’s
inequality it holds that E(cid:104)(cid:112) 4 trace(M4)(cid:105) ≤ (cid:112) 4 E[trace(M4)]. Combining these observation
with Eq. (19) in Remark 20 yields the bound
(cid:114)
(cid:104) (cid:16) (cid:17)(cid:105)
(cid:104) (cid:105)
1+ 4 E trace (R(cid:101)A−E[R(cid:101)A])4
Pr ∥R(cid:101)A−ΘB(cid:101)ΘT∥
op
> t ≤ . (21)
t
By applying Lemma 4, the key element of this proof, there exists a constant C > 0
1
such that if n ≥ C for all k ∈ K , then
k 1
(cid:74) (cid:75)
(cid:114)
(cid:104) (cid:16) (cid:17)(cid:105) (cid:112)
1+ 4 E trace (R(cid:101)A−E[R(cid:101)A])4 ≤ 1+ 4 66K2n2+2n3 ≤ C 2·n3/4,
where C > 0 denotes a new constant, using the hypothesis that K ≤ n1/2. Finally, set
2
t = n3/4+ν in Eq. (21). This establishes Lemma 21.
Remark 22 (Conditions and concentration bounds) The requirement K ≤ n1/2 in
Lemma 21 yielding ∥R(cid:101)A − E[R(cid:101)A]∥
op
= OP(n3/4+ν) is due to our proof approach and the
challenge of working with normalized rank statistics. Alternatively, we could have allowed
K = o(n) but at the expense of more complicated upper bound statements. For simplicity
and ease of presentation, we have chosen to simply require K ≤ n1/2.
Lemma 23 (Eigen-structure of block matrices, see Lemma 2.1 in Lei and Rinaldo
(2015)) Let Θ ∈ Mn×K and symmetric B ∈ RK×K each have full rank equal to K. Let
UDUT denote the eigen-decomposition of ΘBΘT. Then, U = ΘT for some T ∈ RK×K
(cid:113)
where ∥T −T ∥ = n−1+n−1 for all 1 ≤ k < ℓ ≤ K.
k⋆ ℓ⋆ ℓ2 k ℓ
Lemma 23 above holds for K ×K symmetric invertible matrices, including B(cid:101).
Lemma 24 (Principal subspace perturbation, see Lemma 5.1 in Lei and Rinaldo
(2015)) Let M(1) be any symmetric n × n matrix. Let M(2) ∈ Rn×n be a symmetric
matrix with rank K and with smallest nonzero singular value γ . Let U(1),U(2) ∈ Rn×K
n
denoteorthonormalmatriceswhosecolumnsareeigenvectorsfortheK largest-in-magnitude
eigenvalues of M(1) and M(2), respectively. Then, there exists a K ×K orthogonal matrix
Q such that
√
2 2K
∥U(1)−U(2)Q∥ ≤ ∥M(1)−M(2)∥ . (22)
F op
γ
n
√
We remark that K appearing Eq. (22) cannot be removed in general.
Lemma 24 is stated above for convenience and as a reference. However, it will be useful
to consider the following variant.
34Robust spectral clustering with rank statistics
√
Lemma 25 Assume the hypothesis in Lemma 24. If γ > (1−1/ 2)−1·∥M(1)−M(2)∥ ,
n op
then there exists a K ×K orthogonal matrix Q such that
2 (cid:16) (cid:17)
∥U(1)−U(2)Q∥ ≤ ∥ M(1)−M(2) U(2)∥ . (23)
F F
γ
n
Proof [Proof of Lemma 25] Let Q be an orthogonal matrix that minimizes the objective
function∥U(1)−U(2)W∥2 overallorthogonalmatricesW. Inotherwords,letQbeasolution
F
to the orthogonal Procrustes problem with input matrices U(1) and U(2). Applying Corol-
lary 2.8 in Chen et al. (2021) yields ∥U(1) −U(2)Q∥ = inf ∥U(1) −U(2)W∥ ≡
F Worthogonal F
dist (U(1),U(2)) ≤ 2 ∥(M(1)−M(2))U(2)∥ , as desired.
F γn F
Lemma 26 (Approximate k-means error bound, see Lemma 5.3 in Lei and Ri-
naldo (2015)) For ϵ > 0 and any two full column rank matrices U(1),U(2) ∈ Rn×K such
that U(2) = ΘT with Θ ∈ Mn×K, T ∈ RK×K, let (Θ(cid:98),T(cid:98)) be a (1 + ϵ)-approximate solu-
(1)
tion to the k-means problem and U = Θ(cid:98)T(cid:98). For any δ
k
≤ min ℓ̸=k∥T
ℓ⋆
−T k⋆∥ ℓ2, define
(1) (2)
S = {i ∈ G (Θ) : ∥U −U ∥ ≥ δ /2}. Then,
k k i⋆ i⋆ ℓ2 k
K
(cid:88)
|S |·δ2 ≤ 8·(2+ϵ)·∥U(1)−U(2)∥2. (24)
k k F
k=1
Moreover, if
8·(2+ϵ)·∥U(1)−U(2)∥2 /δ2 < n for all k, (25)
F k k
then there exists a K × K permutation matrix Π such that Θ(cid:98)G⋆ = Θ G⋆Π, where G =
(cid:83)K
(G \S ).
k=1 k k
(⋆)
We are now prepared to prove Theorem 5, our general weak consistency result.
Proof [ProofofTheorem5]ThisproofmodifiestheapproachusedtoestablishTheorem3.1
inLeiandRinaldo(2015). Keepinmindthatweconsiderthetruncatedeigendecomposition
of R(cid:101)A rather than of A.
Given ν > 0, define the event E
1,ν,n
= {∥R(cid:101)A −ΘB(cid:101)ΘT∥
op
≤ 2C 2n3/4+ν}, where C
2
>
0 denotes a sufficiently large universal constant. By Lemma 21, Pr[E ] ≥ 1 − 1n−ν.
1,ν,n 2
Similarly, define the event E
2,ν,n,K
= {∥(R(cid:101)A − ΘB(cid:101)ΘT)U∥
F
≤ 32K1/2n1/2+ν/2}. By the
proof of Lemma 29, Pr[E ] ≥ 1− 1n−ν. Consequently, the intersection of these “good
2,ν,n,K 2
events”, denoted by E ≡ E , holds with probability at least 1−n−ν for all values of n,
3,ν,n,K
n , and K satisfying the hypotheses in Lemma 21.
k
Letγ > C′n3/4+ν,perthehypothesis. Then,ontheeventE,itfollowsfromLemmas25
n 2
and 29 that there exists an orthogonal matrix Q and a positive constant C > 0 such that
(cid:32) (cid:33)
2 √ K1/2·n1/2+ν/2
∥U(cid:98) −UQ∥
F
≤ ∥(R(cid:101)A−ΘB(cid:101)ΘT)U∥
F
≤ 2 2·C · .
γ γ
n n
35Cape, Yu, and Liao
Importantly, U(cid:98) ∈ Rn×K is a matrix whose columns are K orthonormal eigenvectors for the
top-K eigenspace of R(cid:101)A.
Next, apply Lemma 26 to U(cid:98) and UQ. Lemma 23 yields UQ = ΘTQ = ΘT′ such that
(cid:113) (cid:113)
∥T′ −T′ ∥ = 1 + 1 for k ̸= ℓ. Choose δ = 1 + 1 in Lemma 26, which
k⋆ ℓ⋆ ℓ2 n k n ℓ k n k max{n ℓ:ℓ̸=k}
yields n δ2 ≥ 1 for all 1 ≤ k ≤ K. Using the above display equation, a sufficient condition
k k
for Eq. (25) to be valid for U(cid:98) and UQ with probability at least 1−n−ν is
Kn1+ν
82C2·(2+ϵ) < 1 ≤ min n δ2. (26)
γ n2 1≤k≤K k k
Now, set c = 1 > 0, whence with probability at least 1−n−ν, we have
64C2
K K (cid:18) (cid:19) K
(cid:88) |S k|
≤
(cid:88)
|S |
1
+
1
=
(cid:88)
|S |δ2
n k n max{n : ℓ ̸= k} k k
k k ℓ
k=1 k=1 k=1
≤ 8·(2+ϵ)∥U(cid:98) −UQ∥2
F
(cid:18) Kn1+ν(cid:19) (cid:18) Kn1+ν(cid:19)
≤ 82C2·(2+ϵ) = c−1(2+ϵ) .
γ2 γ2
n n
Theorem 5 is thereby established, since Lemma 26 guarantees that the block memberships
(cid:83)
are correctly recovered on the complement of the set S .
1≤k≤K k
Proof [Proof of Corollary 6] Recall that B(cid:101) is a full rank symmetric matrix with eigenvalues
|λ 1(B(cid:101))| ≥ ··· ≥ |λ K(B(cid:101))| > 0. Observe that
1 1
γ
n
= |λ K(∆B(cid:101)∆)| = ≥ = n min·|λ K(B(cid:101))|.
∥∆−1B(cid:101)−1∆−1∥
op
n− m1 in·∥B(cid:101)−1∥
op
By hypothesis, n min·|λ K(B(cid:101))| > C 2′n3/4+ν, hence Theorem 5 applies. Thus, with probability
at least 1−n−ν,
2 (cid:88)K 2n′ (cid:88)K (cid:18) 1 1 (cid:19)
L(Θ(cid:98),Θ) ≤ |S k| ≤ max |S k| +
n n n max{n : ℓ ̸= k}
k ℓ
k=1 k=1
(cid:32) (cid:33)
Knνn′
≤ 2c−1(2+ϵ) max .
n2 λ2 (B(cid:101))
min K
Furthermore,
K (cid:18) (cid:19)
|S k| (cid:88) 1 1
L(cid:101)(Θ(cid:98),Θ) ≤ 2 max ≤ 2 |S k| +
1≤k≤K n k n k max{n ℓ : ℓ ̸= k}
k=1
(cid:32) (cid:33)
Kn1+ν
≤ 2c−1(2+ϵ) .
n2 λ2 (B(cid:101))
min K
This completes the proof of Corollary 6.
36Robust spectral clustering with rank statistics
Proof [Proof of Theorem 7] We follow the proof approach used to establish Theorem 5.
Under the stated hypotheses, with probability at least 1−n−ν, the bound
√ (cid:32) (cid:33) (cid:32) (cid:33)
∥U(cid:98)U(cid:98)T−UUT∥
F
2∥U(cid:98) −UQ∥
F
C K1/2·n1/2+ν/2 C n1/2+ν/2
≤ ≤ ≤
∥UUT∥ F K1/2 K1/2 γ n |λ K(B(cid:101))| n min
√
holds for some absolute constant C > 0. We have used the fact that ∥UUT∥ = K. Fur-
√ F
thermore, ∥U(cid:98)U(cid:98)T−UUT∥
F
≤ 2×inf
Qorthogonal
∥U(cid:98)−UQ∥
F
by basic properties of principal
subspaces (e.g., Cape et al. (2019b); Cai and Zhang (2018)). This establishes Eq. (9) and
hence proves Theorem 7.
Proof [Proof of Eq. (10)] The stated equivalence holds as a consequence of basic properties
of subspace perturbations (e.g., Cape et al. (2019b); Cai and Zhang (2018)). In particular,
√ √
∥U(cid:98)U(cid:98) ∥T U− UTU ∥U FT∥
F = √
K2
∥sinΘ(U(cid:98),U)∥
F
= √
K2 (cid:113)
K −∥cosΘ(U(cid:98),U)∥2
F
√ √
2 (cid:113) 2 (cid:113)
= √ K −∥UTU(cid:98)∥2 = √ K −∥UUTU(cid:98)U(cid:98)T∥2
F F
K K
√
2 (cid:113) √ (cid:113)
= √
K
K −trace(P U(cid:98)P U) = 2 1−r K2 (U(cid:98),U).
The result follows for asymptotic regimes in which weak consistency holds, per Eq. (9).
A.3 Proofs: node-specific strong consistency
Remark 27 (Towards node-specific strong consistency: notation and setup) It
will be convenient to define the temporary notation M(cid:99) = R(cid:101)A, M = ΘB(cid:101)ΘT, and E =
(R(cid:101)A −E[R(cid:101)A])−diag(ΘB(cid:101)ΘT). Hence, M(cid:99) = M +E since E[R(cid:101)A] = ΘB(cid:101)ΘT −diag(ΘB(cid:101)ΘT).
The matrices M(cid:99), M, and E are all n×n and symmetric.
Write the full eigendecomposition of M(cid:99) as M(cid:99) = U(cid:98)Λ(cid:98)U(cid:98)T + U(cid:98)⊥Λ(cid:98)⊥U(cid:98) ⊥T, where the diag-
onal matrix Λ(cid:98) contains the K largest-in-magnitude eigenvalues of M(cid:99) with corresponding
orthonormal eigenvectors constituting the columns of U(cid:98). Using analogous notation, write
the eigendecomposition of M with rank(M) = K as M = UΛUT.
This section begins by establishing several technical preliminaries. We write N ≡
kℓ
N(k,ℓ) = |{(i,j) : i < j,g = k,g = ℓ}| to denote the number of random variables with
i j
block membership tuple (k,ℓ). Similarly, write n = |{i : g = k}| for each 1 ≤ k ≤ K.
k i
Lemma 28 Letν > 0. ForanychoiceofpositiveintegersK ≥ 1, n ≥ 1forall1 ≤ k ≤ K,
k
and n =
(cid:80)K
n , it holds that
k=1 k
(cid:104) (cid:105)
Pr ∥UTEU∥ > 1+K ·nν/2 ≤ 8n−ν. (27)
op
Consequently, ∥UTEU∥
op
= OP(K ·nν/2).
37Cape, Yu, and Liao
Proof [Proof of Lemma 28] First, by the triangle inequality it holds that ∥UTEU∥ ≤
op
∥UT(R(cid:101)A−E[R(cid:101)A])U∥ op+∥diag(ΘB(cid:101)ΘT)∥ op, where∥diag(ΘB(cid:101)ΘT)∥
op
= max 1≤k≤K|B(cid:101)kk| ≤ 1.
Given the basic norm relationship ∥ · ∥ ≤ ∥ · ∥ , consider the squared Frobenius norm
op F
quantity
∥UT(R(cid:101)A−E[R(cid:101)A])U∥2
F
= ∥∆−1ΘT(R(cid:101)A−E[R(cid:101)A])Θ∆−1∥2
F
 2
= (cid:88)  (cid:88) (R(cid:101)A−E[R(cid:101)A]) kℓ·Θ kiΘ ℓj ·n i−1/2 n j−1/2 
1≤i,j≤K 1≤k,ℓ≤n
 2
= (cid:88)  (cid:88) (cid:88) (R(cid:101)A−E[R(cid:101)A]) kℓ·n i−1/2 n j−1/2 
1≤i,j≤K k:g =iℓ:g =j
k ℓ
(cid:88)
=: ξ2.
ij
1≤i,j≤K
Observe that E[ξ ] = 0 for all pairs (i,j). Moreover, Propositions 18 and 19 together yield
ij
(cid:18) (cid:19)
n n 7n n n n
E[ξ2] = Var[ξ ] ≤ i j + i j · i j ≤ 8.
ij ij n n N n n
i j i j
Finally, putting all the pieces together and applying Markov’s inequality yields that for any
choice ν > 0,
(cid:104) (cid:105)
Pr ∥UTEU∥ > 1+K ·nν/2
op
(cid:104) (cid:105)
≤ Pr ∥UTEU∥ > 1+K ·nν/2
F
(cid:104) (cid:105)
≤ Pr ∥UT(R(cid:101)A−E[R(cid:101)A])U∥ F+1 > 1+K ·nν/2
 
 1/2
(cid:88)
= Pr  ξ i2 j > K ·nν/2

1≤i,j≤K
 
(cid:88)
= Pr ξ i2 j > K2·nν 
1≤i,j≤K
(cid:104) (cid:105)
E (cid:80) ξ2
1≤i,j≤K ij
≤
K2·nν
≤ 8n−ν.
This establishes Lemma 28.
38Robust spectral clustering with rank statistics
Lemma 29 Letν > 0. ForanychoiceofpositiveintegersK ≥ 1, n ≥ 1forall1 ≤ k ≤ K,
k
and n =
(cid:80)K
n , it holds that
k=1 k
(cid:104) (cid:105) (cid:104) (cid:105)
Pr ∥EU∥ > 1+K1/2·n1/2+ν/2 ≤ Pr ∥EU∥ > K1/2+K1/2·n1/2+ν/2 ≤ 8n−ν. (28)
op F
Consequently, ∥EU∥
op
= OP(K1/2·n1/2+ν/2).
Proof [Proof of Lemma 29] We mirror the proof of Lemma 28. First, by the triangle
inequality, ∥EU∥
op
≤ ∥(R(cid:101)A−E[R(cid:101)A])U∥ op+1, since ∥diag(ΘB(cid:101)ΘT)U∥
op
≤ 1. Furthermore,
∥(R(cid:101)A−E[R(cid:101)A])U∥2
F
= ∥(R(cid:101)A−E[R(cid:101)A])Θ∆−1∥2
F
 2
= (cid:88)  (cid:88) (R(cid:101)A−E[R(cid:101)A]) iℓ·n j−1/2 
1≤i≤n,1≤j≤K ℓ:g =j
ℓ
(cid:88)
=: ξ2,
ij
1≤i≤n,1≤j≤K
where it follows from Propositions 18 and 19 that uniformly in the tuples (i,j),
n
7n2
E[ξ2] = Var[ξ ] ≤ j + j ≤ 8.
ij ij n n N
j j
By an application of Markov’s inequality, we conclude that for any choice ν > 0,
(cid:104) (cid:105)
Pr ∥EU∥ > 1+K1/2·n1/2+ν/2 ≤ 8n−ν.
op
This establishes the first part of Lemma 29. The second part follows by the same ap-
proach, since ∥EU∥
F
≤ ∥(R(cid:101)A −E[R(cid:101)A])U∥
F
+∥diag(ΘB(cid:101)ΘT)U∥
F
and ∥diag(ΘB(cid:101)ΘT)U∥
F
≤
∥diag(ΘB(cid:101)ΘT)∥ op∥U∥
F
≤ K1/2.
Lemma 30 Letν > 0. ForanychoiceofpositiveintegersK ≥ 1, n ≥ 1forall1 ≤ k ≤ K,
k
and n =
(cid:80)K
n , it holds for each fixed row index i that
k=1 k
(cid:104) (cid:105)
Pr ∥EU∥ > n−1/2+K1/2·nν/2 ≤ 8n−ν. (29)
i,ℓ2 gi
Consequently, ∥EU∥
i,ℓ2
= OP(K1/2·nν/2).
Proof [Proof of Lemma 30] We again mirror the proof of Lemma 28. For any choice of row
index i, by the triangle inequality ∥EU∥
i,ℓ2
= ∥EΘ∆−1∥
i,ℓ2
≤ ∥(R(cid:101)A −E[R(cid:101)A])Θ∆−1∥
i,ℓ2
+
n− gi1/2 since ∥diag(ΘB(cid:101)ΘT)Θ∆−1∥
i,ℓ2
≤ ∥Θ∆−1∥
i,ℓ2
= n g− i1/2 . Furthermore,
∥(R(cid:101)A−E[R(cid:101)A])Θ∆−1∥2
i,ℓ2
 2
= (cid:88)  (cid:88) (R(cid:101)A−E[R(cid:101)A]) iℓ·n j−1/2 
1≤j≤K ℓ:g =j
ℓ
(cid:88)
=: ξ2.
ij
1≤j≤K
39Cape, Yu, and Liao
Here, Propositions 18 and 19 imply
n
7n2
E[ξ2] = Var[ξ ] ≤ j + j ≤ 8.
ij ij n n N
j j
By an application of Markov’s inequality, for any choice ν > 0,
(cid:104) (cid:105)
Pr ∥EU∥ > n−1/2+K1/2·nν/2 ≤ 8n−ν.
i,ℓ2 gi
This establishes Lemma 30.
Lemma 31 For any choice of row index i, the bound ∥E∥ ≤ 2n1/2 holds surely.
i,ℓ2
Proof [Proof of Lemma 31] All the entries of R(cid:101)A−E[R(cid:101)A] and diag(ΘB(cid:101)ΘT) are bounded in
modulus by one, hence for any choice of row index i, by the triangle inequality ∥E∥ =
i,ℓ2
∥(R(cid:101)A−E[R(cid:101)A])−diag(ΘB(cid:101)ΘT)∥
i,ℓ2
≤ 2n1/2.
Lemma 32 Assume the setting and notation in Section 5.3. Fix ν > 0. Write |λ | ≡
K
|λ K(ΘB(cid:101)ΘT)|. If |λ K| ≥ Cn3/4+ν for all n ≥ n
0
where n 0,C > 0 are large constants, then
∥U∥ = n−1/2, (30)
i,ℓ2 gi
∥(I −UUT)U(cid:98)∥
op
= OP(K1/2·n1/2+ν/2·|λ K|−1), (31)
∥U(cid:98) −UW ⋆∥
op
= OP(K1/2·n1/2+ν/2·|λ K|−1), (32)
∥UTU(cid:98) −W ⋆∥
op
= OP(K ·n1+ν ·|λ K|−2), (33)
where W
⋆
∈ arginf ∥U(cid:98) −UW∥2 F.
W orthogonal
Proof [Proof of Lemma 32] Eq. (30) follows directly from Lemma 23.
√
Eq. (31) holds since ∥(I − UUT)U(cid:98)∥
op
= ∥sinΘ(U(cid:98),U)∥
op
≤ |λK2 |∥ −E |λU K∥o +p
1|
= OP(K1/2 ·
n1/2+ν/2 ·|λ |−1), using Lemma 2.5 and Corollary 2.8 in Chen et al. (2021) together with
K
our Lemma 29 to bound ∥EU∥ . For context, here λ = 0, and by slight overload of
op K+1
notation, Θ(U(cid:98),U) is the ordered diagonal matrix of canonical angles between the subspaces
spanned by U(cid:98) and U.
Eq. (32) holds since ∥U(cid:98) − UW ⋆∥
op
≤ ∥(I − UUT)U(cid:98)∥
op
+ ∥UTU(cid:98) − W ⋆∥
op
≤ 2∥(I −
UUT)U(cid:98)∥
op
= OP(K1/2 ·n1/2+ν/2 ·|λ K|−1), where we have applied the triangle inequality
with the orthogonal decomposition I = UUT+(I −UUT) and Eqs. (31) and (33).
Eq. (33) holds since ∥UTU(cid:98) −W ⋆∥
op
≤ ∥sinΘ(U(cid:98),U)∥2
op
= ∥(I −UUT)U(cid:98)∥2
op
= OP(K ·
n1+ν·|λ |−2),whichholdsfromLemma6.7inCapeetal.(2019b)followedbyanapplication
K
of Eq. (31).
This completes the proof of Lemma 32.
40Robust spectral clustering with rank statistics
(⋆)
We are now prepared to prove Theorem 8, our general strong consistency result.
Proof [Proof of Theorem 8] We adopt the notation in Remark 27. For Lemma 21 and
each technical lemma proved above, say the i-th, associate to it the “good event” E on
i
which the stated OP(·) bound holds. Applying DeMorgan’s law and Boole’s inequality
yields that the “bad” event ∩ E satisfies Pr[∩ E ] = Pr[∪ E ] ≤ (cid:80)nE Pr[E ] ≤ Cn−ν since
i i i i i i i=1 i
n is an absolute constant and Pr[E ] ≤ C n−ν for each i. Finally, by taking complements,
E i i
Pr[∩ E ] ≥ 1−Cn−ν. Consequently, the analysis that follows holds with high probability
i i
on an underlying “good event”.
Weyl’s inequality (Bhatia, 2013, Theorem VIII.4.8) guarantees that
(cid:12) (cid:12) (cid:12) (cid:12)
max (cid:12) (cid:12)|λ i(M(cid:99))|−|λ i(M)|(cid:12)
(cid:12)
≤ max (cid:12) (cid:12)λ i(M(cid:99))−λ i(M)(cid:12)
(cid:12)
≤ ∥E∥ op,
1≤i≤n 1≤i≤n
hence it follows from the triangle inequality that |λ i(M(cid:99))| ≥ |λ i(M)|−∥E∥
op
for 1 ≤ i ≤ n.
Furthermore, ∥E∥
op
= OP(n3/4+ν) by Lemma 21, and |λ K| ≥ Cn3/4+ν implies |λ(cid:98)i|−1 =
OP(|λ i|−1) for each 1 ≤ i ≤ K.
First, rewriting U(cid:98) and applying the relationship M(cid:99)= M +E yields
U(cid:98) = M(cid:99)U(cid:98)Λ(cid:98)−1 = MU(cid:98)Λ(cid:98)−1+EU(cid:98)Λ(cid:98)−1. (34)
Expanding the first term on the right-hand side yields
MU(cid:98)Λ(cid:98)−1 = UΛUTU(cid:98)Λ(cid:98)−1
= UUTU(cid:98) −UUTEU(cid:98)Λ(cid:98)−1
= UW ⋆+U(UTU(cid:98) −W ⋆)−UUTEU(cid:98)Λ(cid:98)−1
= UW ⋆+U(UTU(cid:98) −W ⋆)−UUTEUUTU(cid:98)Λ(cid:98)−1−UUTE(I −UUT)U(cid:98)Λ(cid:98)−1,
where the first equality invokes the low-rank structure of M = UΛUT, the second equality
invokes the Sylvester-style observation that
ΛUTU(cid:98) = UTU(cid:98)Λ(cid:98) −UTEU(cid:98), (35)
the third equality introduces the Frobenius-optimal orthogonal Procrustes matrix,
W
⋆
∈ arginf ∥U(cid:98) −UW∥2 F, (36)
Worthogonal
and the final equality invokes the expansion
I = UUT+(I −UUT) = UUT+U UT. (37)
⊥ ⊥
41Cape, Yu, and Liao
Applying the above lemmas, together with basic properties of matrix norms, yields
∥U(UTU(cid:98) −W ⋆)∥
i,ℓ2
≤ ∥U∥ i,ℓ2∥UTU(cid:98) −W ⋆∥
op
= OP(K ·n g− i1/2·n1+ν ·|λ K|−2),
∥UUTEUUTU(cid:98)Λ(cid:98)−1∥
i,ℓ2
(cid:16) (cid:17)
≤ ∥U∥ i,ℓ2∥UTEU∥ op∥UTU(cid:98)∥ op∥Λ(cid:98)−1∥
op
= OP K ·n− gi1/2·nν/2·|λ K|−1 ,
∥UUTE(I −UUT)U(cid:98)Λ(cid:98)−1∥
i,ℓ2
(cid:16) (cid:17)
≤ ∥U∥ i,ℓ2∥UTE∥ op∥(I −UUT)U(cid:98)∥ op∥Λ(cid:98)−1∥
op
= OP K ·n g− i1/2·n1+ν ·|λ K|−2 .
Together, the sum of these terms is bounded in the manner
(cid:16) (cid:17)
OP K ·n g− i1/2·n1+ν ·|λ K|−2 . (38)
It remains to analyze EU(cid:98)Λ(cid:98)−1. Applying Eqs. (35) to (37) yields the decomposition
EU(cid:98)Λ(cid:98)−1
(cid:110) (cid:111) (cid:110) (cid:111)
= EUUTU(cid:98)Λ(cid:98)−1 + EU ⊥U ⊥TU(cid:98)Λ(cid:98)−1
(cid:110) (cid:16) (cid:17) (cid:111) (cid:110) (cid:111)
= EUΛ−1 ΛUTU(cid:98) Λ(cid:98)−1 + EU ⊥U ⊥TU(cid:98)Λ(cid:98)−1
(cid:110) (cid:111) (cid:110) (cid:111)
= EUΛ−1W ⋆+EUΛ−1(UTU(cid:98) −W ⋆)−EUΛ−1(UTEU(cid:98)Λ(cid:98)−1) + EU ⊥U ⊥TU(cid:98)Λ(cid:98)−1 .
The rightmost term above can be further decomposed as
EU ⊥U ⊥TU(cid:98)Λ(cid:98)−1 = EU ⊥U ⊥TEU(cid:98)Λ(cid:98)−2 = E2U(cid:98)Λ(cid:98)−2−EUUTEU(cid:98)Λ(cid:98)−2. (39)
Lemma 35 is an important technical result that enables this proof and the subsequent
derivation of asymptotic normality. Without it, we are merely able to deduce that
∥E2U(cid:98)Λ(cid:98)−2∥
i,ℓ2
= ∥E2(U(cid:98) −UW ⋆+UW ⋆)Λ(cid:98)−2∥
i,ℓ2
≤ ∥E∥ i,ℓ2∥E∥ op∥U(cid:98) −UW ⋆∥ op∥Λ(cid:98)−2∥ op+∥E∥ i,ℓ2∥EUW ⋆∥ op∥Λ(cid:98)−2∥
op
(cid:16) (cid:17)
= OP K1/2·n1+3/4+3ν/2·|λ K|−3+K1/2·n1+ν/2·|λ K|−2
(cid:16) (cid:17)
= OP K1/2·n1+ν/2·|λ K|−2 .
Instead, consider
∥E2U(cid:98)Λ(cid:98)−2∥
i,ℓ2
= ∥E2(UUT+U ⊥U ⊥T)U(cid:98)Λ(cid:98)−2∥
i,ℓ2
≤ ∥E2UUTU(cid:98)Λ(cid:98)−2∥
i,ℓ2
+∥E2U ⊥U ⊥TU(cid:98)Λ(cid:98)−2∥
i,ℓ2
≤ ∥E2U∥ i,ℓ2∥Λ(cid:98)−2∥ op+∥E∥ i,ℓ2∥EU ⊥∥ op∥U ⊥TU(cid:98)∥ op∥Λ(cid:98)−2∥
op
(cid:16) (cid:17)
≤ ∥E2U∥ i,ℓ2∥Λ(cid:98)−2∥ op+OP K1/2·n1+3/4+3ν/2·|λ K|−3 .
42Robust spectral clustering with rank statistics
We need to bound ∥E2U∥ . Expanding terms yields
i,ℓ2
∥E2U∥
i,ℓ2
(cid:13) (cid:13)
=
(cid:13) (cid:13)(cid:16)
R(cid:101)
−E[R(cid:101)]−diag(ΘB(cid:101)ΘT)(cid:17)2 U(cid:13)
(cid:13)
(cid:13) (cid:13)
i,ℓ2
(cid:13)(cid:16)
= (cid:13) (R(cid:101) −E[R(cid:101)])2−(R(cid:101) −E[R(cid:101)])diag(ΘB(cid:101)ΘT)
(cid:13)
(cid:17) (cid:13)
−diag(ΘB(cid:101)ΘT)(R(cid:101) −E[R(cid:101)])+diag2(ΘB(cid:101)ΘT) U(cid:13) ,
(cid:13)
i,ℓ2
where
∥diag2(ΘB(cid:101)ΘT)U∥
i,ℓ2
≤ n g− i1/2,
∥diag(ΘB(cid:101)ΘT)(R(cid:101) −E[R(cid:101)])U∥
i,ℓ2
= OP(K1/2nν/2),
∥(R(cid:101) −E[R(cid:101)])diag(ΘB(cid:101)ΘT)U∥
i,ℓ2
= OP(K1/2nν/2).
We must control ∥(R(cid:101) −E[R(cid:101)])2U∥
i,ℓ2
= ∥(R(cid:101) −E[R(cid:101)])2Θ∆−1∥
i,ℓ2
whose square is given by
∥(R(cid:101) −E[R(cid:101)])2Θ∆−1∥2
i,ℓ2
(cid:88) (cid:16) (cid:17)2
= eT i(R(cid:101) −E[R(cid:101)])2Θ∆−1e
j
1≤j≤K
 2
(cid:88) (cid:88)
=  (R(cid:101) −E[R(cid:101)]) ik(R(cid:101) −E[R(cid:101)]) kℓ(Θ∆−1) ℓj
1≤j≤K 1≤k,ℓ≤n
(cid:88) 1 (cid:88) (cid:88)
= (R(cid:101) −E[R(cid:101)]) ik(R(cid:101) −E[R(cid:101)]) ik′(R(cid:101) −E[R(cid:101)]) kℓ(R(cid:101) −E[R(cid:101)]) k′ℓ′.
n
j
1≤j≤K 1≤k,k′≤nℓ,ℓ′:g ℓ=j,g ℓ′=j
(cid:16) (cid:110) (cid:113) (cid:111)(cid:17)
By Lemma 35, its expectation is O max n2 , K3n3 . Markov’s inequality thus gives
ngi ngi
(cid:32) (cid:40) (cid:41) (cid:33)
n K3/4n3/4
∥(R(cid:101) −E[R(cid:101)])2Θ∆−1∥
i,ℓ2
= OP max 1/2,
1/4
·nν/2 .
n n
gi gi
Therefore,
∥E2U(cid:98)Λ(cid:98)−2∥
i,ℓ2
≤ ∥E2UUTU(cid:98)Λ(cid:98)−2∥
i,ℓ2
+∥E2U ⊥U ⊥TU(cid:98)Λ(cid:98)−2∥
i,ℓ2
(cid:32)(cid:40) (cid:40) (cid:41) (cid:41)
n K3/4n3/4
= OP max n1/2,
n1/4
·nν/2+K1/2nν/2+n g− i1/2 ·|λ K|−2 (40)
gi gi
(cid:33)
(cid:110) (cid:111)
+ K1/2n1+3/4+3ν/2 ·|λ |−3
K
(cid:32)(cid:40) (cid:40) (cid:41) (cid:41)
n K3/4·n3/4
= OP max , ·nν/2 ·|λ K|−2 (41)
1/2 1/4
n n
gi gi
(cid:33)
(cid:110) (cid:111)
+ K1/2·n1+3/4+3ν/2 ·|λ |−3 .
K
43Cape, Yu, and Liao
For the remaining term in Eq. (39), a preliminary bound is given by
(cid:16) (cid:17)
∥EUUTEU(cid:98)Λ(cid:98)−2∥
i,ℓ2
≤ ∥EU∥ i,ℓ2∥EU∥ op∥Λ(cid:98)−2∥
op
= OP K ·n1/2+ν ·|λ K|−2 .
Alternatively, the above term can be bounded in the manner
∥EUUTEU(cid:98)Λ(cid:98)−2∥
i,ℓ2
≤ ∥EUUTEUUTU(cid:98)Λ(cid:98)−2∥
i,ℓ2
+∥EUUTEU ⊥U ⊥TU(cid:98)Λ(cid:98)−2∥
i,ℓ2
(cid:110) (cid:111)
≤ ∥EU∥
i,ℓ2
∥UTEU∥ op∥UTU(cid:98)∥ op+∥EU∥ op∥U ⊥U ⊥TU(cid:98)∥
op
∥Λ(cid:98)−2∥
op
(cid:16) (cid:110) (cid:111) (cid:17)
= OP K1/2·nν/2· K ·nν/2+K ·n1+ν ·|λ K|−1 ·|λ K|−2
(cid:16) (cid:17)
= OP K3/2·n1+3ν/2·|λ K|−3 .
Hence, for this term,
(cid:16) (cid:110) (cid:111)(cid:17)
∥EUUTEU(cid:98)Λ(cid:98)−2∥
i,ℓ2
= OP min K ·n1/2+ν ·|λ K|−2,K3/2·n1+3ν/2·|λ K|−3 . (42)
Under the current assumptions, |λ | ≥ Cn3/4+ν and K ≤ n1/2, whence it follows that
K
|λ | ≥ CK1/2n1/2+ν/2. Finally, a preliminary analysis of the earlier leading order term
K
yields
∥EUΛ−1W ⋆+EUΛ−1(UTU(cid:98) −W ⋆)−EUΛ−1(UTEU(cid:98)Λ(cid:98)−1)∥
i,ℓ2
(cid:110) (cid:111)
≤ ∥EU∥
i,ℓ2
·∥Λ−1∥ op· 1+∥UTU(cid:98) −W ⋆∥ op+∥UTEU(cid:98)Λ(cid:98)−1∥
op
(cid:16) (cid:110) (cid:111)(cid:17)
= OP K1/2·nν/2·|λ K|−1· 1+min{1,K ·n1+ν ·|λ K|−2}+K1/2·n1/2+ν/2·|λ K|−1
(cid:16) (cid:17)
= OP K1/2·nν/2·|λ K|−1 . (43)
Combining Eqs. (38) and (41) to (43) yields
(cid:32)
∥U(cid:98) −UW ⋆∥
i,ℓ2
= OP K1/2·nν/2·|λ K|−1
(cid:110) (cid:111)
+min K ·n1/2+ν ·|λ |−2,K3/2·n1+3ν/2·|λ |−3
K K
(cid:110) (cid:111)
+max n−1/2·n,K3/4·n−1/4·n3/4 ·nν/2·|λ |−2
gi gi K
+K1/2·n1+3/4+3ν/2·|λ |−3
K
(cid:33)
+K ·n−1/2·n1+ν ·|λ |−2 .
gi K
Rewriting terms gives Theorem 8, as desired.
44Robust spectral clustering with rank statistics
Proof [Proof of Corollary 9] Since n is order n/K for each 1 ≤ k ≤ K, the condition
k
|λ | ≥ Cn3/4+ν further implies K ≤ C′n1/4−ν. We therefore have
K
∥U(cid:98) −UW ⋆∥
i,ℓ2
(cid:32)
= OP K3/2·n−1+ν/2
(cid:110) (cid:111)
+min K3·n−3/2+ν,K9/2·n−2+3ν/2
(cid:110) (cid:110) (cid:111) (cid:111) (cid:110) (cid:111)
+ max K1/2·n1/2,K ·n1/2 ·K2·n−2+ν/2 + K7/2·n−1−1/4+3ν/2
(cid:33)
+K7/2·n−3/2+ν
(cid:32)
= OP K3/2·n−1+ν/2
(cid:110) (cid:111)
+min K3·n−3/2+ν,K9/2·n−2+3ν/2
(cid:33)
+K3·n−3/2+ν/2+K7/2·n−1−1/4+3ν/2
(cid:32)
= OP K3/2·n−1+ν/2
(cid:110) (cid:111)
+min K3·n−3/2+ν,K9/2·n−2+3ν/2
(cid:33)
+K7/2·n−1−1/4+3ν/2
(cid:32) (cid:33)
= OP K3/2·n−1+ν/2+K7/2·n−1−1/4+3ν/2 .
Consequently,
(cid:16) (cid:110) (cid:111)(cid:17)
∥U(cid:98) −UW ⋆∥
i,ℓ2
= OP K3/2·n−1+ν/2· 1+K2·n−1/4+ν ,
and
(cid:16) (cid:110) (cid:111)(cid:17)
n1 gi/2∥U(cid:98) −UW ⋆∥
i,ℓ2
= OP K ·n−1/2+ν/2· 1+K2·n−1/4+ν = oP(1).
This establishes Corollary 9.
A further refinement of the leading order term in the above proof is possible. In fact, it
will be needed to establish asymptotic normality. Namely, we shall characterize EUΛ−1W
⋆
and ∥EUΛ−1∥ = ∥EUΛ−1W ∥ appearing in
i,ℓ2 ⋆ i,ℓ2
∥EUΛ−1W ⋆+EUΛ−1(UTU(cid:98)−W ⋆)−EUΛ−1(UTEU(cid:98)Λ(cid:98)−1)∥
i,ℓ2
= ∥EUΛ−1W ⋆∥ i,ℓ2·(1+oP(1)).
45Cape, Yu, and Liao
Lemma 33 Letν > 0. ForanychoiceofpositiveintegersK ≥ 1, n ≥ 1forall1 ≤ k ≤ K,
k
and n =
(cid:80)K
k=1n
k
with rank(B(cid:101)) = K, it holds for each fixed row index i that
  (cid:118)  
(cid:117) K
Pr∥EUΛ−1∥ i,ℓ2 >  n− m1 in+(cid:117) (cid:116)(cid:88) n− j 1·nν/2 ·∥B(cid:101)−1∥ op·n m−1 in/2  ≤ 8n−ν.
 
j=1
Proof [Proof of Lemma 33] Let M† denote the necessarily unique Moore–Penrose inverse
of M, given by
M† = (ΘB(cid:101)ΘT)† = Θ(ΘTΘ)−1B(cid:101)−1(ΘTΘ)−1ΘT,
which when squared equals
(M†)2 = Θ(ΘTΘ)−1B(cid:101)−1(ΘTΘ)−1B(cid:101)−1(ΘTΘ)−1ΘT.
The preceding matrix is positive semi-definite, hence there exists an orthogonal matrix W
Θ
such that
EUΛ−1W = E ·(cid:8) UΛ−1W (cid:9) ·(WTW )
⋆ Θ Θ ⋆
(cid:110) (cid:111)
= E · Θ(ΘTΘ)−1B(cid:101)−1(ΘTΘ)−1/2 ·(W ΘTW ⋆).
Here, the treatment of orthogonal transformations is crucial but does not modify the ℓ row
2
norm values.
From the above expansion, it holds by deterministic properties of norms that
∥EUΛ−1W ∥
⋆ i,ℓ2
(cid:13) (cid:110) (cid:111)(cid:13)
= (cid:13)E · Θ(ΘTΘ)−1B(cid:101)−1(ΘTΘ)−1/2 (cid:13)
(cid:13) (cid:13)
i,ℓ2
(cid:13)(cid:16) (cid:17) (cid:110) (cid:111)(cid:13)
= (cid:13)
(cid:13)
(R(cid:101)A−E[R(cid:101)A])−diag(ΘB(cid:101)ΘT) · Θ∆−2B(cid:101)−1∆−1 (cid:13)
(cid:13)
i,ℓ2
(cid:110) (cid:111)
≤ ∥(R(cid:101)A−E[R(cid:101)A])Θ∆−2∥
i,ℓ2
+n− m1
in
·∥B(cid:101)−1∆−1∥ op.
Furthermore,
∥(R(cid:101)A−E[R(cid:101)A])Θ∆−2∥2
i,ℓ2
 2
(cid:88) (cid:88)
=  (R(cid:101)A−E[R(cid:101)A]) iℓ·n−
j
1 
1≤j≤K ℓ:g =j
ℓ
(cid:88)
=: ξ2.
ij
1≤j≤K
For each value of j, invoking Propositions 18 and 19 yields
n
n2
7 8
E[ξ2] = Var[ξ ] ≤ j + j · ≤ .
ij ij n2 n2 N n
j j j
46Robust spectral clustering with rank statistics
Consequently, for ν > 0, by Markov’s inequality,
 (cid:118) 
(cid:117) K
(cid:117)(cid:88)
Pr∥(R(cid:101)A−E[R(cid:101)A])Θ∆−2∥ i,ℓ2 > (cid:116) n− j 1·nν/2  ≤ 8n−ν.
j=1
Combining the above observations yields
  (cid:118)  
(cid:117) K
Pr∥EUΛ−1W ⋆∥ i,ℓ2 >  n− m1 in+(cid:117) (cid:116)(cid:88) n− j 1·nν/2 ·∥B(cid:101)−1∥ op·n− m1 in/2  ≤ 8n−ν.
 
j=1
This concludes the proof of Lemma 33 since ∥·∥ is right-orthogonal invariant.
i,ℓ2
A.4 Proofs: asymptotic normality
We divide the proof of our theorem into multiple parts.
Proof [Proof of Lemma 12] Assume the hypotheses and fix a tuple (α,β) ∈ n × K .
(cid:74) (cid:75) (cid:74) (cid:75)
Define the associated random variable
n
(cid:88) (cid:88)
ξ
α,β
= (R(cid:101)AΘ)
α,β
= (R(cid:101)A) α,ℓΘ
ℓ,β
= (R(cid:101)A) α,ℓ. (44)
ℓ=1 ℓ:g =β
ℓ
It will be convenient to work with the following intermediary notation for converting prop-
ertiesofthevector(X ,...,X )intomatrixform. Specifically,definethehollowsymmetric
1 N
n×n matrix Y = (Y ) as Y = vech−1({X }N ), formed by the inverse half-vectorization
ij 0 i i=1
operation taking the upper-triangular elements by row and ignoring the main diagonal.
Define the hollow symmetric matrix Υ = (υ ) ∈ {0,1}n×n according to the binary indi-
ij
cator function 1{·} as υ = 1{i = α,g = β}, hence Υ depends on α and β. Further,
ij j
let c = (c ,...,c )T = vech (Υ), again defined in terms of the upper-triangular entries
1 N 0
excluding the diagonal, hence c ∈ {0,1} for all 1 ≤ ι ≤ N. Define a (ι) = φ(ι/(N +1))
ι N
with φ(t) = t for 0 < t < 1.
Per H´ajek (1968), c ,...,c denote the regression constants, while a (1),...,a (N)
1 N N N
denote the scores generated by φ. We seek to verify the asymptotic normality of the simple
linear rank statistic (H´ajek, 1968, Equation 2.3) given by Eq. (44), equivalently written as
N
(cid:88) (cid:88)
ξ
α,β
= υ
ij
·(R(cid:101)A)
ij
= c ι·a N(R ι), (45)
1≤i,j≤n ι=1
where R denotes the rank associated with the random variable X having distribution
ι ι
function F .
ι
As in H´ajek (1968, Equation 2.6), define c and H(y), respectively, as
N N
1 (cid:88) n β 1 (cid:88) (cid:88) N m′
c = c ≍ , H(y) = F (y) = F (y), (46)
ι ι m′
N N N N
ι=1 ι=1 m′∈ m
(cid:74) (cid:75)
47Cape, Yu, and Liao
where the second expression (property) holds for the present setting.
Fixϵ > 0. Fornsufficientlylarge(henceN sufficientlylarge),thefollowingdeterministic
relationship holds for some K > 0.
ϵ
(cid:88) (cid:88)
Var[ξ α,β] = Var[(R(cid:101)A) α,ℓ]+ 2·Cov[(R(cid:101)A) α,ℓ,(R(cid:101)A) α,ℓ′]
ℓ:g ℓ=β 1≤ℓ<ℓ′≤n:g ℓ=β,g ℓ′=β
n2
≥ C′n ·δ −C′′· β
β Var
N
≥ C′′′n
β
≫ K max (c −c¯)2.
ϵ ι
1≤ι≤N
The first line holds by the law of total variance. The second line holds by invoking a
uniform lower bound on the (co)variances and liminf n→∞min i,jVar[(R(cid:101)A) ij] ≥ δ
Var
> 0 and
n → ∞. The third line holds provided n is sufficiently large. For the fourth line, in the
β
language of H´ajek (1968, Equation 5.5),
(cid:104) √ (cid:105)2
K = 2·δ−1·1+(2·ϵ−1/2·β−1+1) 39 , 0 < max (c −c)2 < 1,
ϵ ϵ ϵ ι
1≤ι≤N
using the fact that δ ≡ δ > 0 is chosen according to an application of the Lindeberg
ϵ
theorem (H´ajek, 1968, Page 340), that sup |φ′(t)| = 1, that β ≡ β > 0 is governed
0<t<1 ϵ
by the cumulative distribution function of the standard normal distribution (H´ajek, 1968,
√
Page 340), and M1/2 = 39 where M = 2K +36K2, K = K + 1K , and K denotes an
2 1 2 2 i
upper bound on the squared i-th derivative of φ, with K = K = 1.
1 2
Thus, invoking H´ajek (1968, Theorem 2.1) for the aforementioned arbitrary choice ϵ > 0
yields asymptotic normality of the form
(cid:12) (cid:34) (cid:35) (cid:12)
(cid:12) ξ −E[ξ ] (cid:12)
max(cid:12)Pr α,β α,β ≤ x −Φ (x)(cid:12) < ϵ, (47)
(cid:12) (cid:112) 1 (cid:12)
x∈R (cid:12) Var[ξ α,β] (cid:12)
where Φ denotes the cumulative distribution function of the standard normal distribution
1
in R. This concludes the proof of Lemma 12 modulo a change of notation.
Proof [Proof of Lemma 13] Under the stated assumptions, the argument in the proof of
Lemma 12 holds for each choice of β ∈ K with index α held fixed. Now, consider the
K-dimensional random vector ξ ≡ ξ(n)(cid:74) =(cid:75) (ξ(n) ,...,ξ(n) )T. Let a ∈ RK be any speci-
α α α,1 α,K
fied non-zero (say unit norm) vector of constants. The random variable aTξ is itself a
α
linear combination of simple linear rank statistics under so-called multi-sample (block) al-
ternatives, so by another application of H´ajek (1968, Theorem 2.1) and the above proof
approach for Eq. (47), we obtain asymptotic (univariate) normality of aTξ after standard-
α
ization. An application of the Cram´er–Wold theorem (Billingsley, 2012, Theorem 29.4)
yields the asymptotic multivariate normality of ξ after standardization, namely
α
(cid:12) (cid:104) (cid:105) (cid:12)
max (cid:12)Pr Cov(ξ )−1/2(ξ −E[ξ ]) ≤ x −Φ (x)(cid:12) < ϵ.
(cid:12) α α α K (cid:12)
x∈RK
This concludes the proof of Lemma 13.
48Robust spectral clustering with rank statistics
(⋆)
We are now in a position to prove Theorem 14, the main asymptotic normality result.
Proof [Proof of Theorem 14] From the proof of Theorem 8, there exists a random matrix
E′ ∈ Rn×K such that with probability one,
U(cid:98) −UW
⋆
= EUΛ−1W ⋆+E′.
Here, K is fixed, |λ K| is order n/K, and ∥E′∥
i,ℓ2
= oP(n−1) for each row index i.
From the proof of Lemma 33, there exist orthogonal matrices W and W , each depend-
⋆ Θ
ing on n, such that
(cid:110) (cid:111)
EUΛ−1W
⋆
= {EΘ}· (ΘTΘ)−1B(cid:101)−1(ΘTΘ)−1/2 ·(W ΘTW ⋆).
Write WT = W ΘTW ⋆. Recall that E = (R(cid:101)A−E[R(cid:101)A])−diag(ΘB(cid:101)ΘT), whence these observa-
tions together yield
(cid:16) (cid:16) (cid:17) (cid:17)
n U(cid:98) −UW
⋆
W
rowi
(cid:16)(cid:110) (cid:111) (cid:110) (cid:111)(cid:17)
= n−1/2(R(cid:101)A−E[R(cid:101)A])Θ · n(ΘTΘ)−1·B(cid:101)−1·n1/2(ΘTΘ)−1/2 +oP(1).
rowi
By hypothesis,
(cid:104) (cid:105)
Γ(k) = lim Cov n−1/2((R(cid:101)A−E[R(cid:101)A])Θ)
rowi
: g
i
= k ,
n→∞
(cid:110) (cid:111)
Ξ = lim n1/2(ΘTΘ)−1/2 ·B(cid:101)−1·(cid:8) n(ΘTΘ)−1(cid:9) ,
n→∞
where each limiting matrix has full rank. Finally, by an application of Lemma 13 and
Slutsky’s theorem, the sequence of row vectors of the form
(cid:16) (cid:16) (cid:17) (cid:17)
n U(cid:98) −UW
⋆
W : g
i
= k
rowi
converges in distribution to a K-dimensional multivariate normal random vector with mean
zero and covariance matrix Ξ·Γ(k)·ΞT. This concludes the proof of Theorem 14.
A.5 Proofs: dimension selection
Proof [Proof of Lemma 15] By the proof of Lemma 21, for n sufficiently large it holds that
(cid:104) (cid:105)
Pr ∥R(cid:101)A−ΘB(cid:101)ΘT∥
op
> 4n3/4+ν ≤ n−ν.
Furthermore, by the proof of Corollary 6 it holds that
|λ K(ΘB(cid:101)ΘT)| ≥ n min·|λ K(B(cid:101))| > 0.
Weyl’s inequality (Bhatia, 2013, Theorem VIII.4.8) guarantees that for each i ∈ n ,
(cid:74) (cid:75)
(cid:12) (cid:12)
(cid:12) (cid:12)|λ i(R(cid:101)A)|−|λ i(ΘB(cid:101)ΘT)|(cid:12)
(cid:12)
≤ ∥R(cid:101)A−ΘB(cid:101)ΘT∥ op.
49Cape, Yu, and Liao
Hence, for each k ∈ K and n large, it holds with probability at least 1−n−ν that
(cid:74) (cid:75)
|λ k(R(cid:101)A)| ≥ |λ k(ΘB(cid:101)ΘT)|−∥R(cid:101)A−ΘB(cid:101)ΘT∥
op
≥ n minδ−4n3/4+ν ≫ 4n3/4+ν.
Similarly, for each i ≥ 1 and n sufficiently large, we have |λ K+i(R(cid:101)A)| > 0+4n3/4+ν with
probability at most n−ν. Taking limits with n → ∞ establishes Lemma 15 by the squeeze
principle.
A.6 Proofs: properties of rank statistics
This section begins by establishing notation and several preliminary observations.
LetX,Y,Z beindependentabsolutelycontinuousrandomvariableswithcumulativedis-
tribution functions F ,F ,F and densities f ,f ,f . A basic but important observation
X Y Z X Y Z
is that
Pr [X ≤ Y] = E [F (Y)].
(X,Y) Y X
Let g : R → R be the deterministic map g (z) = (cid:82)z F (y)f (y)dy. One can
(X,Y) (X,Y) −∞ X Y
show that
Pr [X ≤ Y ≤ Z] = E [g (Z)]. (48)
(X,Y,Z) Z (X,Y)
Also, keep in mind that E [F (X)] = 1 and E [F2(X)] = 1.
X X 2 X X 3
As before, let {X } be a collection of N independent random variables, and let
i i∈ N
{f } be a collection o(cid:74)f m(cid:75) density functions, where {N }m is a deterministic collection
ℓ ℓ∈ m ℓ ℓ=1
of inte(cid:74)ge(cid:75)rs counts satisfying N = |{i : X ∼ f }| with (cid:80)m N = N. Let F denote the
ℓ i ℓ ℓ=1 ℓ ℓ
cumulative distribution function associated with density f , and as before let E [X ] denote
ℓ ℓ i
taking expectation when X ∼ f , at times also written as X ∼ F .
i ℓ i ℓ
Let H : R → {0, 1,1} denote the Heaviside step function using the half-maximum
2
convention, namely

1 if x > 0,


H(x) = 1 if x = 0,
2

0 if x < 0.
Proof [Proof of Proposition 16] The stated result follows directly from numerous textbooks
and published papers. For example, see Example 2.1 in Viana (2001).
Proof [Proof of Proposition 17] Let the random variable R(cid:101)′ denote the normalized rank
Ni
value of X
i
among X 1,...,X N, such that the possible outcomes of R(cid:101) N′
i
are given by
(cid:8)1, 2,...,1(cid:9) . Conveniently, R(cid:101)′ is expressible in the form
N N Ni
1 (cid:88) 1
R(cid:101) N′
i
=
N
H(X i−X i′)+ 2N. (49)
i′∈ N
(cid:74) (cid:75)
Above, the term 1 is a consequence of H(0) = 1 when i′ = i and having specified that
2N 2
R(cid:101)′ ∈ (cid:8)1, 2,...,1(cid:9) .
Ni N N
50Robust spectral clustering with rank statistics
For k ∈ m , it follows from the above display equation that
(cid:74) (cid:75)
E k(cid:104) R(cid:101) N′ i(cid:105) = (cid:88) N Nℓ ×E k[F ℓ(X)]+ 21 N.
ℓ∈ m
(cid:74) (cid:75)
Finally,replaceℓwithℓ′ andk withℓintheindexnotation. ThisestablishesProposition17.
Proof [Proof of Proposition 18] We pursue an explicit variance calculation. Towards this
end, consider
 
1 (cid:88)
N2 ×E k H(X i−X i′)·H(X i−X j′), X i ∼ F k.
i′,j′∈ N
(cid:74) (cid:75)
Applying linearity of expectation, we shall count and characterize the various enumerations
of E [H(X −X )·H(X −X )]. Here, X ∼ F is a representative of N i.i.d. random
k i i′ i j′ i k k
variables, whereas X and X range over all N variables and m distributions.
i′ j′
1. For i = i′ = j′, there is one term of the form
E (cid:2) H(X −X )H(X −X )(cid:3) = E [H2(0)] = 1.
k i i′ i j′ k 4
2. For i = i′,i′ ̸= j′ where X ∼ F , there are, by symmetry, 2N −2 terms of the form
j′ ℓ
E (cid:2) H(X −X )H(X −X )(cid:3) = E (cid:2) H(0)H(X −X )(cid:3) = 1E [F (X)].
k i i′ i j′ k i j′ 2 k ℓ
3. For i ̸= i′,i′ = j′ with X ∼ F , there are N −1 terms of the form
i′ ℓ
E (cid:2) H(X −X )H(X −X )(cid:3) = E (cid:2) H2(X −X )(cid:3) = E [F (X)].
k i i′ i j′ k i i′ k ℓ
4. For distinct triples {i,i′,j′} where X ∼ F and X ∼ F , there are N2 −3N +2
i′ ℓ j′ m
terms of the form
E (cid:2) H(X −X )H(X −X )(cid:3) = P[{X < X }∩{X < X }]
k i i′ i j′ i′ i j′ i
= P[max(X ,X ) < X ]
i′ j′ i
= E [F (X)F (X)].
k ℓ m
Importantly, when ℓ = m = k, then E [F (X)F (X)] = E [F2(X)] = 1.
k k k k k 3
Item1abovecontributesthe(scaled)term 1 1 . ForeachofItem2andItem3above,scaling
4N2
(cid:16) (cid:17)
and aggregating yields 1 × (N −1)1 +(cid:80) N E [F (X)] . In total, these three cases
N2 k 2 ℓ̸=k ℓ k ℓ
contribute
2 (cid:88) N ℓE
[F (X)]−
3 1
. (50)
N N k ℓ 4N2
ℓ∈ m
(cid:74) (cid:75)
51Cape, Yu, and Liao
Item 4 above, the primary term of interest, contributes
(cid:32)
1 (cid:88)
× (N −1)(N −2)E [F2(X)]+2 (N −1)N E [F (X)F (X)]
N2 k k k k k ℓ k k ℓ
ℓ̸=k
(cid:33)
(cid:88) (cid:88)
+ N N E [F (X)F (X)]+ N (N −1)E [F (X)F (X)] .
ℓ ℓ′ k ℓ ℓ′ ℓ ℓ′ k ℓ ℓ′
ℓ̸=k,ℓ′̸=k,ℓ̸=ℓ′ ℓ̸=k,ℓ′̸=k,ℓ=ℓ′
The above expression can be viewed as an incomplete expansion of a quadratic form. To
complete the above expansion, we need to invoke a plus-zero trick with
    
1 (cid:18) 1(cid:19) (cid:88) (cid:88) 
±
N2
(3N
k
−2)
3
+2 N ℓE k[F k(X)F ℓ(X)]+ N ℓE k[F ℓ2(X)] . (51)
 
ℓ̸=k ℓ̸=k
The “plus” part of the zero trick yields the completed quadratic form
(cid:88) N ℓN ℓ′ ×E [F (X)F (X)]. (52)
k ℓ ℓ′
N N
ℓ,ℓ′∈ m
(cid:74) (cid:75)
The variance computation is nearly complete. Observe that the squared expectation of R(cid:101)′
Ni
is given by
 2
(cid:16) E k[R(cid:101) N′ i](cid:17)2 =  (cid:88) N Nℓ ×E k[F ℓ(X)] + N1 (cid:88) N Nℓ ×E k[F ℓ(X)]+ 41 N1 2,
ℓ∈ m ℓ∈ m
(cid:74) (cid:75) (cid:74) (cid:75)
while the second moment is given by
(cid:104) (cid:105)
E
k
(R(cid:101) N′ i)2
 2
1 (cid:88) 1
= E k H(X i−X i′)+  
N 2N
i′∈ N
(cid:74) (cid:75)
 2 
1 (cid:88) (cid:88) 1
=
N2
×E k H(X i−X i′) + H(X i−X i′)+ 4
i′∈ N i′∈ m
 (cid:74) (cid:75) (cid:74) (cid:75) 
= N1
2
×E k (cid:88) H(X i−X i′)H(X i−X j′)+ N1 (cid:88) N Nℓ ×E k[F ℓ(X)]+ 1 4N1 2.
i′,j′∈ N ℓ∈ m
(cid:74) (cid:75) (cid:74) (cid:75)
More immediately, by the shift-invariance of variance,
   2
Var k[R(cid:101) N′ i] = N1
2
×E k (cid:88) H(X i−X i′)H(X i−X j′)− (cid:88) N Nℓ ×E k[F ℓ(X)] .
i′,j′∈ N ℓ∈ m
(cid:74) (cid:75) (cid:74) (cid:75)
52Robust spectral clustering with rank statistics
Thus,
  2
Var k[R(cid:101) N′ i] =  (cid:88) N NℓN Nℓ′ ×E k[F ℓ(X)F ℓ′(X)]− (cid:88) N Nℓ ×E k[F ℓ(X)] 
ℓ,ℓ′∈ m ℓ∈ m
(cid:74) (cid:75) (cid:74) (cid:75) 
+ (cid:88) N Nℓ ×(cid:8)E k[2F ℓ(X)]−E k[2F k(X)F ℓ(X)]−E k[F ℓ2(X)](cid:9)  N1
ℓ∈ m
(cid:74) (cid:75)
1 1
− .
12N2
The result follows by replacing ℓ′ with ℓ′′, ℓ with ℓ′, and k with ℓ in the notation above.
This establishes Proposition 18.
Proof [Proof of Proposition 19] By the shift-invariance of covariance,
Cov (k,ℓ)(R(cid:101) N′ i,R(cid:101) N′ j)
= E[R(cid:101)′ R(cid:101)′ ]−E[R(cid:101)′ ]E[R(cid:101)′ ]
Ni Nj Ni Nj
 
1 (cid:88)
= E  N2 H(X i−X i′)H(X j −X j′)
i′,j′∈ N
(cid:74) (cid:75)   
1 (cid:88) 1 (cid:88)
−E  H(X i−X i′)×E  H(X j −X j′).
N N
i′∈ N j′∈ N
(cid:74) (cid:75) (cid:74) (cid:75)
WeproceedtoconsiderR(cid:101) N′
i
andR(cid:101) N′
j
fori ̸= j whereX
i
∼ F k,X
j
∼ F
ℓ
andX
i′
∼ F k′,X
j′
∼
F . Namely,wepursuebothwithin-blockcovariancesandbetween-blockcovariances. Keep-
ℓ′
ing these underlying distributions in mind, we analyze E[H(X −X )H(X −X )].
i i′ j j′
1. For (i′,j′) = (i,i),
E[H(X −X )H(X −X )] = E[H(0)H(X −X )] = 1E [F (X)].
i i′ j j′ j i 2 ℓ k
2. For (i′,j′) = (j,j),
E[H(X −X )H(X −X )] = E[H(X −X )H(0)] = 1E [F (X)].
i i′ j j′ i j 2 k ℓ
3. For (i′,j′) = (i,j),
E[H(X −X )H(X −X )] = E[H(0)H(0)] = 1.
i i′ j j′ 4
4. For (i′,j′) = (j,i),
E[H(X −X )H(X −X )] = P[{X < X }∩{X < X }] = 0.
i i′ j j′ j i i j
5. For i′ = i and j′ ∈/ {i,j}, occurring N −2 times,
E[H(X −X )H(X −X )] = E[H(0)H(X −X )] = 1E [F (X)].
i i′ j j′ j j′ 2 ℓ ℓ′
53Cape, Yu, and Liao
6. For i′ = j and j′ ∈/ {i,j}, occurring N −2 times,
E[H(X −X )H(X −X )] = P[X < X < X ] = E [g (X)].
i i′ j j′ j′ j i k (ℓ′,ℓ)
7. For i′ ∈/ {i,j} and j′ = j, occurring N −2 times,
E[H(X −X )H(X −X )] = E[H(X −X )H(0)] = 1E [F (X)].
i i′ j j′ i i′ 2 k k′
8. For i′ ∈/ {i,j} and j′ = i, occurring N −2 times,
E[H(X −X )H(X −X )] = P[X < X < X ] = E [g (X)].
i i′ j j′ i′ i j ℓ (k′,k)
9. For i′ = j′ with i′ ∈/ {i,j}, occurring N −2 times,
E[H(X −X )H(X −X )] = P[{X < X }∩{X < X }]
i i′ j j′ i′ i i′ j
= P[X < min(X ,X )]
i′ i j
= 1−P[min(X ,X ) ≤ X ]
i j i′
= E [(1−F (X))(1−F (X))].
k′ k ℓ
10. In all remaining cases, the independence and distinctness of i,j,i′,j′ yields
E[H(X −X )H(X −X )] = E[H(X −X )]E[H(X −X )] = E [F (X)]E [F (X)].
i i′ j j′ i i′ j j′ k k′ ℓ ℓ′
Aggregating over i′,j′ in Case 10 yields a partial collection of terms in the expansion of
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:80) N E [F (X)] × (cid:80) N E [F (X)] . We proceed to complete this expan-
k′∈ m k′ k k′ ℓ′∈ m ℓ′ ℓ ℓ′
sion by(cid:74)a(cid:75)dding and subtracting req(cid:74)uir(cid:75)ed terms, keeping the additional terms as remainders
which themselves can be suitably completed and organized.
First, in light of Case 9, introducing
(cid:110) (cid:111)
± E [(1−F (X))(1−F (X))]+E [(1−F (X))(1−F (X))] (53)
k k ℓ ℓ k ℓ
contributes a (subtracted) residual term and (via addition) completes the expansion
(cid:88)
N E [(1−F (X))(1−F (X))]. (54)
m′ m′ k ℓ
m′∈ m
(cid:74) (cid:75)
Furthermore, introducing
(cid:40) (cid:41)
(cid:88)
± N E [F (X)]E [F (X)] (55)
m′ k m′ ℓ m′
m′∈ m
(cid:74) (cid:75)
contributes a (subtracted) residual term and (via addition) completes the diagonal portion
of the complete-the-square problem.
Next, in light of Case 6, introducing
(cid:110) (cid:111)
± E [g (X)]+E [g (X)] (56)
k (k,ℓ) k (ℓ,ℓ)
54Robust spectral clustering with rank statistics
contributes a (subtracted) residual term and (via addition) completes the expansion
(cid:88)
N E [g (X)]. (57)
m′ k (m′,ℓ)
m′∈ m
(cid:74) (cid:75)
Furthermore, introducing
(cid:40) (cid:41)
(cid:88)
± N E [F (X)]E [F (X)]− 1E [F (X)] (58)
m′ k ℓ ℓ m′ 2 k ℓ
m′∈ m
(cid:74) (cid:75)
contributes a (subtracted) residual term and (via) addition completes the corresponding
partial row of the complete-the-square problem.
Next, in light of Case 8, introducing
(cid:110) (cid:111)
± E [g (X)]+E [g ] (59)
ℓ (k,k) ℓ (ℓ,k)(X)
contributes a (subtracted) residual term and (via addition) completes the expansion
(cid:88)
N E [g (X)]. (60)
m′ ℓ (m′,k)
m′∈ m
(cid:74) (cid:75)
Furthermore, introducing
(cid:40) (cid:41)
(cid:88)
± N E [F (X)]E [F (X)]− 1E [F (X)] (61)
m′ k m′ ℓ k 2 ℓ k
m′∈ m
(cid:74) (cid:75)
contributes a (subtracted) residual term and (via) addition completes the corresponding
partial row of the complete-the-square problem.
Putting all the pieces together yields the completed square, namely all the terms in the
expansion of
   
(cid:88) (cid:88)
 N k′E k[F k′(X)]× N ℓ′E ℓ[F ℓ′(X)]. (62)
k′∈ m ℓ′∈ m
(cid:74) (cid:75) (cid:74) (cid:75)
Additionally, the remaining residual can be organized into two expressions. The first, dom-
inant expression is given by
(cid:34)
(cid:88)
N E [(1−F (X))(1−F (X))]+E [g (X)]+E [g (X)]
m′ m′ k ℓ k (m′,ℓ) ℓ (m′,k)
m′∈ m
(cid:74) (cid:75) (cid:35)
−E [F (X)]E [F (X)]−E [F (X)]E [F (X)]−E [F (X)]E [F (X)] .
k m′ ℓ m′ k m′ ℓ k k ℓ ℓ m′
55Cape, Yu, and Liao
The second expression is given by
(cid:110) (cid:111)
1E [F (X)]+ 1E [F (X)]+ 1 +0
2 ℓ k 2 k ℓ 4
(cid:110) (cid:111)
− E [(1−F (X))(1−F (X))]+E [(1−F (X))(1−F (X))]
k k ℓ ℓ k ℓ
(cid:110) (cid:111)
− E [g (X)]+E [g (X)]
k (k,ℓ) k (ℓ,ℓ)
(cid:110) (cid:111)
− E [g (X)]+E [g (X)]
ℓ (k,k) ℓ (ℓ,k)
(cid:110) (cid:111)
+ 1E [F (X)]+ 1E [F (X)]
2 k ℓ 2 ℓ k
(cid:110) (cid:111)
+ E [F (X)]E [F (X)]− 1
k ℓ ℓ k 4
which can be written in the more interpretable form
(cid:40) (cid:41)
E (cid:2) 2F (X)−F (X)F (X)−g (X)−g (X)− 1(cid:3)
k ℓ k ℓ (k,ℓ) (ℓ,ℓ) 3
(cid:40) (cid:41)
+ E (cid:2) 2F (X)−F (X)F (X)−g (X)−g (X)− 1(cid:3)
ℓ k k ℓ (k,k) (ℓ,k) 3
(cid:110) (cid:111)
+ E [F (X)]E [F (X)]− 1
k ℓ ℓ k 4
(cid:40) (cid:41)
1
− .
12
Proposition 19 follows by scaling and re-indexing the above expressions.
A.7 Proofs: a combinatorial trace bound
Proof [Proof of Lemma 3] Define the matrix Γ = R(cid:101)A−E[R(cid:101)A] with entries Γ ≡ (γ ij), hence
γ = r −E[r ] = rij − 1 for i ̸= j. We focus on
ij (cid:101)ij (cid:101)ij N+1 2
trace(cid:0) Γ4(cid:1)
=
(cid:88)
γ γ γ γ . (63)
i1i2 i2i3 i3i4 i4i1
1≤i1,i2,i3,i4≤n
Here, Γ is a random symmetric matrix with zero-valued diagonal entries and is thus deter-
minedbyN = n(n−1)/2randomvariables. Fordifferentchoicesofindicesi ,i ,i ,i ∈ n ,
1 2 3 4
(cid:74) (cid:75)
the tuple (γ ,γ ,γ ,γ ) may be entrywise comprised of either four, two, or a
i1i2 i2i3 i3i4 i4i1
single unique random variable(s). We shall proceed to characterize all possible cases of
(γ ,γ ,γ ,γ ). In particular, we only need to consider situations when i ̸= i ,
i1i2 i2i3 i3i4 i4i1 1 2
i ̸= i , i ̸= i , and i ̸= i , since otherwise γ γ γ γ = 0 almost surely.
2 3 3 4 4 1 i1i2 i2i3 i3i4 i4i1
In what follows, it will be convenient to use the following new definition and notation.
Definition 34 Let (a,b,c,d) denote a random vector generated by uniformly selecting a
sequence of four distinct elements from the set of shifted, normalized rank statistic values
(cid:26) (cid:27)
i 1
− : i ∈ N .
N +1 2
(cid:74) (cid:75)
56Robust spectral clustering with rank statistics
Case 1: i , i , i , i are pairwise different. In this case, the distribution of the tuple
1 2 3 4
(γ ,γ ,γ ,γ ) is the same as that of (a,b,c,d). By elementary counting, there are
i1i2 i2i3 i3i4 i4i1
n(n−1)(n−2)(n−3) such tuples of indices i ,i ,i ,i .
1 2 3 4
Case 2: i ̸= i , i = i . In this case, the distribution of the tuple (γ ,γ ,γ ,γ )
1 3 2 4 i1i2 i2i3 i3i4 i4i1
is the same as that of (a,b,b,a). By elementary counting, there are n(n−1)(n−2) such
tuples of indices i ,i ,i ,i .
1 2 3 4
Case 3: i = i , i ̸= i . In this case, the distribution of the tuple (γ ,γ ,γ ,γ )
1 3 2 4 i1i2 i2i3 i3i4 i4i1
is the same as that of (a,a,b,b). By elementary counting, there are n(n−1)(n−2) such
tuples of indices i ,i ,i ,i .
1 2 3 4
Case 4: i = i , i = i . In this case, the distribution of the tuple (γ ,γ ,γ ,γ ) is
1 3 2 4 i1i2 i2i3 i3i4 i4i1
the same as that of (a,a,a,a). By elementary counting, there are n(n−1) such sequences
of indices i ,i ,i ,i .
1 2 3 4
By considering all possible configurations of i ,i ,i ,i in Eq. (63), we obtain
1 2 3 4
E(cid:2) trace(Γ4)(cid:3)
(cid:88)
= E[γ γ γ γ ]
i1i2 i2i3 i3i4 i4i1 (64)
1≤i1,i2,i3,i4≤n
= n(n−1)(n−2)(n−3)E[abcd]+2n(n−1)(n−2)E[a2b2]+n(n−1)E[a4],
where expectations E[·] are taken with respect to the uniform distribution over (a,b,c,d)
per Definition 34.
Next, we explicitly compute E[abcd], E[a2b2], and E[a4]. First,
57Cape, Yu, and Liao
E[abcd]
(cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17)
= 1 i − 1 j − 1 k − 1
N(N−1)(N−2)(N−3) N+1 2 N+1 2 N+1 2
i∈ N j∈ N −{i} k∈ N −{i,j}
(cid:88) (cid:16) (cid:74) (cid:75)(cid:17) (cid:74) (cid:75) (cid:74) (cid:75)
l − 1
N+1 2
l∈ N −{i,j,k}
(cid:74) (cid:75) (cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17)
= 1 i − 1 j − 1 k − 1
N(N−1)(N−2)(N−3) N+1 2 N+1 2 N+1 2
i∈ N j∈ N −{i} k∈ N −{i,j}
(cid:104) (cid:16) (cid:74)(cid:17)(cid:75) (cid:16) (cid:17)(cid:74) (cid:16)(cid:75) (cid:17)(cid:105) (cid:74) (cid:75)
NE[a]− i − 1 − j − 1 − k − 1
N+1 2 N+1 2 N+1 2
(cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17)
= 1 i − 1 j − 1
N(N−1)(N−2)(N−3) N+1 2 N+1 2
i∈ N j∈ N −{i}
(cid:26)(cid:104) (cid:16) (cid:74) (cid:75) (cid:17) (cid:16) (cid:17)(cid:74) (cid:105)2(cid:75) (cid:16) (cid:17)2 (cid:16) (cid:17)2(cid:27)
NE[a]− i − 1 − j − 1 −NE[a2]+ i − 1 + j − 1
N+1 2 N+1 2 N+1 2 N+1 2
(cid:88) (cid:16) (cid:17) (cid:88) (cid:16) (cid:17)
= 1 i − 1 j − 1
N(N−1)(N−2)(N−3) N+1 2 N+1 2
(65)
i∈ N j∈ N −{i}
(cid:26) (cid:16) (cid:17)2 (cid:74) (cid:104)(cid:75) (cid:16) (cid:74) (cid:75) (cid:17)(cid:105)(cid:16) (cid:17) (cid:16) (cid:17)2
2 j − 1 −2 NE[a]− i − 1 j − 1 −NE[a2]+ i − 1
N+1 2 N+1 2 N+1 2 N+1 2
(cid:104) (cid:16) (cid:17)(cid:105)2(cid:27)
+ NE[a]− i − 1
N+1 2
(cid:88) (cid:16) (cid:17)(cid:26) (cid:16) (cid:17)3
= 1 i − 1 2NE[a3]−2 i − 1
N(N−1)(N−2)(N−3) N+1 2 N+1 2
i∈ N
(cid:104) (cid:16) (cid:74) (cid:75) (cid:17)(cid:105)(cid:20) (cid:16) (cid:17)2(cid:21) (cid:104) (cid:16) (cid:17)(cid:105)3(cid:27)
−3 NE[a]− i − 1 NE[a2]− i − 1 + NE[a]− i − 1
N+1 2 N+1 2 N+1 2
(cid:88) (cid:16) (cid:17)(cid:26) (cid:16) (cid:17)3 (cid:16) (cid:17)2
= 1 i − 1 −6 i − 1 +6NE[a] i − 1
N(N−1)(N−2)(N−3) N+1 2 N+1 2 N+1 2
i∈ N
+3(cid:2) NE[a2]−N2(E(cid:74) [a(cid:75) ])2(cid:3)(cid:16) i − 1(cid:17) +N3(E[a])3+2NE[a3]−3N2E[a2]E[a](cid:111)
N+1 2
8N2E[a]E[a3]+N4(E[a])4−6N3E[a2](E[a])2−6NE[a4]+3N2(E[a2])2
= .
N(N −1)(N −2)(N −3)
Since E[a] = 0, the above expression simplifies to the form
3N2(E[a2])2−6NE[a4]
E[abcd] = .
N(N −1)(N −2)(N −3)
Next, again by direct computation, we evaluate E[a2b2] in the manner
1 (cid:88)
(cid:18)
i
1(cid:19)2
(cid:88)
(cid:18)
j
1(cid:19)2
E[a2b2] = − −
N(N −1) N +1 2 N +1 2
i∈ N j∈ N −{i}
(cid:74) (cid:75) (cid:40)(cid:74) (cid:75) (cid:41)
1 (cid:88)
(cid:18)
i
1(cid:19)2 (cid:18)
i
1(cid:19)2
= − NE[a2]− −
N(N −1) N +1 2 N +1 2
i∈ N
(cid:74) (cid:75)
N(E[a2])2−E[a4]
= .
N −1
58Robust spectral clustering with rank statistics
Thus, from Eq. (64),
(cid:26) (cid:27)
E(cid:2) trace(Γ4)(cid:3) = (E[a2])2 3Nn(n−1)(n−2)(n−3) + 2Nn(n−1)(n−2)
(N −1)(N −2)(N −3) N −1
(cid:26) (cid:27)
6n(n−1)(n−2)(n−3) 2n(n−1)(n−2)
−E[a4] + −n(n−1)
(N −1)(N −2)(N −3) N −1
(cid:18) 2n2(n−1)2(cid:19)(cid:26)
3
(cid:27)
= (E[a2])2 +1
n+1 (N −2)(n+2)
(cid:18) (cid:19)(cid:26) (cid:27)
n(n−1)(n−3) 24
+E[a4] 1− .
n+1 (N −2)(n+2)(n−3)
Recall that N = n(n−1)/2, hence by direct computation,
1 (cid:88)
(cid:18)
i
1(cid:19)2
N −1 1
E[a2] = − = ≤ ,
N N +1 2 12(N +1) 12
i∈ N
(cid:74) (cid:75)
1 (cid:88) (cid:18) i 1(cid:19)4 3N3−3N2−7N +7 3 1
E[a4] = − = ≤ = .
N N +1 2 240(N +1)3 240 80
i∈ N
(cid:74) (cid:75)
The hypothesis n ≥ 4 implies n2 ≤ 1n3, whence plugging in the above observations yields
4
(cid:18) (cid:19) (cid:18) (cid:19)
E(cid:2) trace(Γ4)(cid:3) ≤ 1 ·2· 9 n3+ 1 n2 ≤ 1 n3.
122 8 80 50
This concludes the proof of Lemma 3.
Proof [Proof of Lemma 4] Define the matrix Γ = R(cid:101)A−E[R(cid:101)A] with entries Γ ≡ (γ ij). Thus,
γ
ij
= r
(cid:101)ij
−E[r (cid:101)ij] = Nr +ij
1
−B(cid:101)gigj for i ̸= j. We focus on
trace(cid:0) Γ4(cid:1)
=
(cid:88)
γ γ γ γ . (66)
i1i2 i2i3 i3i4 i4i1
1≤i1,i2,i3,i4≤n
In this proof, we shall mirror the case-by-case discussion in the proof of Lemma 3. We first
observethatthereareatmost2n3termsinthesummationEq.(66)thatsatisfythecondition
i ̸= i ,i ̸= i ,i ̸= i and i ̸= i and with (i ,i ,i ,i ) having fewer than four distinct
1 2 2 3 3 4 4 1 1 2 3 4
entries. Note that |γ | ≤ 1 for all i,j ∈ n , which implies that |γ γ γ γ | ≤ 1 for
ij i1i2 i2i3 i3i4 i4i1
(cid:74) (cid:75)
any choice of indices i ,i ,i ,i ∈ n . Hence,
1 2 3 4
(cid:74) (cid:75)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:88) γ i1i2γ i2i3γ i3i4γ i4i1 − (cid:88) γ i1i2γ i2i3γ i3i4γ i4i1(cid:12) (cid:12)
(cid:12)
≤ 2n3. (67)
(cid:12)1≤i1,i2,i3,i4≤n i ,i ,i ,i ∈ n (cid:12)
(cid:12) 1 2 3 4 (cid:12)
(cid:12) pairwise differ(cid:74) en(cid:75) t (cid:12)
(cid:12) (cid:12)
To bound the second summation term in Eq. (67), we shall characterize the distribution of
γ
ij
= r
(cid:101)ij
−E[r (cid:101)ij] by using the population-level block structure governing A and R(cid:101)A.
59Cape, Yu, and Liao
For any choices k ,k ∈ K , define anassociated collection of random variables S =
1 2 k1k2
(cid:74) (cid:75)
{r : i ∈ G ,j ∈ G ,i < j}. By the definition of r , the sets S for 1 ≤ k ≤ k ≤ K
(cid:101)ij k1 k2 (cid:101)ij k1k2 1 2
are pairwise disjoint and collectively satisfy
(cid:26) (cid:27)
(cid:91) i
S = : i ∈ N .
k1k2
N +1
(cid:74) (cid:75)
1≤k1≤k2≤K
Next, in order to facilitate counting, for any k ,k ∈ K , define
1 2
(cid:74) (cid:75)

n n if k ̸= k ,
 k1 k2 1 2
N(k 1,k 2) = n k1(n
k1
−1)
if k = k .
1 2
2
We emphasize that N(k ,k ) is the cardinality of the set S . Further below, we use the
1 2 k1k2
notation card(·) ≡ |·| to indicate the cardinality of an underlying set.
For any partition of the set {i/(N + 1) : i ∈ N } that satisfies |Ψ | = N(k ,k ),
k1k2 1 2
(cid:74) (cid:75)
denoted by {Ψ : 1 ≤ k ≤ k ≤ K}, condition on the event that S = Ψ for
k1k2 1 2 k1k2 k1k2
all 1 ≤ k ≤ k ≤ K. Then, for any choice of k ,k , the joint conditional distribution of
1 2 1 2
the random variables in S is simply the distribution of uniformly sampling elements in
k1k2
Ψ without replacement. Moreover, by conditioning on the event that S = Ψ for
k1k2 k1k2 k1k2
all 1 ≤ k ≤ k ≤ K, the conditional distribution is independent between blocks. This
1 2
conditional within-block uniform distribution property is due to the fact that the entries of
A are i.i.d. within blocks and are independent across blocks. These observations will enable
us to analyze the summation in Eq. (66).
Next, weshallcharacterizeseveralmomentsofthewithin-blockuniformconditionaldis-
tribution. Let(a,b,c,d)bearandomvectordefinedasselectingafour-elementsubsequence
from Ψ uniformly at random, for an arbitrary, finite set satisfying |Ψ | = N(k ,k ) ≥
k1k2 k1k2 1 2
4. Write E [·] to denote taking an expectation with respect to this underlying distribu-
Ψ
k1k2
tion. Consequently,
(cid:104) (cid:105)
E
Ψ k1k2
a−B(cid:101)k1k2 = Ψ
k1k2
−B(cid:101)k1k2,
where Ψ denotes the average of all elements in Ψ . We compute
k1k2 k1k2
(cid:104) (cid:105)
E
Ψ
k1k2
(a−B(cid:101)k1k2)(b−B(cid:101)k1k2)
(cid:110) (cid:111)
(cid:80) (cid:80)
=
x∈Ψ
k1k2(x−B(cid:101)k1k2)
y∈Ψ
k1k2−{x}(y−B(cid:101)k1k2)
N(k ,k )[N(k ,k )−1]
1 2 1 2
(cid:110) (cid:111)
=
(cid:80)
x∈Ψ
k1k2(x−B(cid:101)k1k2) N(k 1,k 2)E
Ψ
k1k2[a−B(cid:101)k1k2]−(x−B(cid:101)k1k2) (68)
N(k ,k )[N(k ,k )−1]
1 2 1 2
(cid:16) (cid:17)2
=
N(k 1,k 2) E
Ψ
k1k2[a−B(cid:101)k1k2] −E
Ψ
k1k2[(a−B(cid:101)k1k2)2]
.
N(k ,k )−1
1 2
60Robust spectral clustering with rank statistics
The same, direct approach yields the expansion
(cid:104) (cid:105)
E
Ψ
k1k2
(a−B(cid:101)k1k2)2(b−B(cid:101)k1k2)(c−B(cid:101)k1k2)
=
(cid:80)
x∈Ψ
k1k2(x−B(cid:101)k1k2)2(cid:80)
y∈Ψ
k1k2−{x}(y−B(cid:101)k1k2)(cid:80)
z∈Ψ
k1k2−{x,y}(z−B(cid:101)k1k2)
N(k ,k )[N(k ,k )−1][N(k ,k )−2]
1 2 1 2 1 2
1
=
N(k ,k )[N(k ,k )−1][N(k ,k )−2]
1 2 1 2 1 2 (69)
(cid:26)
(cid:16) (cid:17)2
× N(k 1,k 2)3 E
Ψ
k1k2[a−B(cid:101)k1k2] E
Ψ
k1k2[(a−B(cid:101)k1k2)2]
(cid:16) (cid:17)2
−N(k 1,k 2)2 E
Ψ
k1k2[(a−B(cid:101)k1k2)2] +2N(k 1,k 2)E
Ψ
k1k2[(a−B(cid:101)k1k2)4]
(cid:111)
−2N(k 1,k 2)2E
Ψ
k1k2[a−B(cid:101)k1k2]E
Ψ
k1k2[(a−B(cid:101)k1k2)3]
and
(cid:104) (cid:105)
E
Ψ
k1k2
(a−B(cid:101)k1k2)(b−B(cid:101)k1k2)(c−B(cid:101)k1k2)(d−B(cid:101)k1k2)
1
=
N(k ,k )[N(k ,k )−1][N(k ,k )−2][N(k ,k )−3]
1 2 1 2 1 2 1 2
(cid:110)
× 8N(k 1,k 2)2E
Ψ
k1k2[a−B(cid:101)k1k2]E
Ψ
k1k2[(a−B(cid:101)k1k2)3]
(cid:16) (cid:17)4
+N(k 1,k 2)4 E
Ψ
k1k2[a−B(cid:101)k1k2]
(cid:16) (cid:17)2 (70)
−6N(k 1,k 2)3E
Ψ
k1k2[(a−B(cid:101)k1k2)2] E
Ψ
k1k2[a−B(cid:101)k1k2]
−6N(k 1,k 2)E
Ψ
k1k2[(a−B(cid:101)k1k2)4]
(cid:16) (cid:17)2(cid:111)
+3N(k 1,k 2)2 E
Ψ
k1k2[(a−B(cid:101)k1k2)2]
(cid:16) (cid:17)4
=
N(k ,k
)[NN (( kk
1
,, kk
2
)) −4 1E
][Ψ Nk1 (k
k2[a ,k− )B(cid:101)
−k1 2k ]2
[]
N(k ,k )−3]
+η(Ψ k1k2,B(cid:101)k1k2),
1 2 1 2 1 2 1 2
where η(·,·) is a constant depending on Ψ
k1k2
and B(cid:101)k1k2 that is bounded in the manner
(cid:12) (cid:12) 6[N(k ,k )+1]2
(cid:12) (cid:12)η(Ψ k1k2,B(cid:101)k1k2)(cid:12)
(cid:12)
≤
[N(k ,k
)−1][N(k1 ,k2
)−2][N(k ,k
)−3]. (71)
1 2 1 2 1 2
The first expression in Eq. (70) is obtained from deductions that mirror those in Eq. (65).
Eq. (71) holds since |a−B(cid:101)k1k2| ≤ 1 almost surely.
Below, we bound the second summation term in Eq. (67) by enumerating four different
cases for g ,g ,g ,g when i ,i ,i ,i are pairwise different.
i1 i2 i3 i4 1 2 3 4
61Cape, Yu, and Liao
Case 1: k = k , k = k and g = k , g = k , g = k , g = k . In this case, the
1 3 2 4 i1 1 i2 2 i3 3 i4 4
variables γ ,γ ,γ ,γ are all entries in the block Γ . Thus,
i1i2 i2i3 i3i4 i4i1 G k1G
k2
E[γ γ γ γ ]
i1i2 i2i3 i3i4 i4i1
(cid:88)
= Pr[S = Ψ ]
k1k2 k1k2
Ψ ⊂{ i :i∈ N }
k1k2 N+1 (cid:74) (cid:75)
×E
Ψ
k1k2[(a−B(cid:101)k1k2)(b−B(cid:101)k1k2)(c−B(cid:101)k1k2)(d−B(cid:101)k1k2)]
=
(cid:88)
Pr[S
k1k2
= Ψ k1k2]N(k 1,k 2)3(E
Ψ
k1k2[a−B(cid:101)k1k2])4
[N(k ,k )−1][N(k ,k )−2][N(k ,k )−3]
1 2 1 2 1 2
Ψ ⊂{ i :i∈ N }
k1k2 N+1 (cid:74) (cid:75) (72)
+Pr[S
k1k2
= Ψ k1k2]η(Ψ k1k2,B(cid:101)k1k2)
=
(cid:88) Pr[S k1k2 = Ψ k1k2]N(k 1,k 2)3(Ψ k1k2 −B(cid:101)k1k2)4
[N(k ,k )−1][N(k ,k )−2][N(k ,k )−3]
1 2 1 2 1 2
Ψ ⊂{ i :i∈ N }
k1k2 N+1 (cid:74) (cid:75)
(cid:104) (cid:105)
+E η(S k1k2,B(cid:101)k1k2)
(cid:104) (cid:105)
=
[N(k
,N
k
( )k −1, 1k
]2
[) N3E (k( ,S
kk1 )k2
−− 2]B(cid:101)
[Nk1 (k
k2)4
,k )−3]
+E(cid:104)
η(S
k1k2,B(cid:101)k1k2)(cid:105)
,
1 2 1 2 1 2
where the random variable S denotes the average of all random variables in S . In
k1k2 k1k2
order to bound the above expression further, we note that
(cid:104) (cid:105) (cid:104) (cid:105)
0 ≤ E (S
k1k2
−B(cid:101)k1k2)4 ≤ E (S
k1k2
−B(cid:101)k1k2)2 = Var(cid:2) S k1k2(cid:3) , (73)
where
 (cid:104) (cid:80) (cid:105)
Var r if k = k ,
1
 

i,j∈G k1,i<j(cid:101)ij 1 2
(cid:2) (cid:3)
Var S = ×
k1k2 N(k 1,k 2)2   (cid:104) (cid:80) (cid:105)
Var r if k ̸= k .
i∈G ,j∈G (cid:101)ij 1 2
k1 k2
The above variance can be expanded as the sum of covariance terms, in the manner
 (cid:80) (cid:80) (cid:2) (cid:3)
(cid:2) (cid:3)
1
 

i,j∈G k1,i<j i′,j′∈G k1,i′<j′Cov r (cid:101)ijr (cid:101)i′j′ if k 1 = k 2,
Var S = ×
k1k2 N(k ,k )2
1 2   (cid:80)
i∈G ,j∈G
(cid:80)
i′∈G ,j′∈G
Cov(cid:2) r (cid:101)ijr (cid:101)i′j′(cid:3) if k
1
̸= k 2.
k1 k2 k1 k2
Since r ∈ (0,1],
(cid:101)ij
Var[r ] ≤ 1 for all i,j ∈ n .
(cid:101)ij
(cid:74) (cid:75)
Likewise, as a consequence of Proposition 19, it holds that
8
|Cov[r ,r ]| ≤ , for all (i,j) ̸= (i′,j′).
(cid:101)ij (cid:101)i′j′
N
62Robust spectral clustering with rank statistics
Thus,
Var(cid:2) S (cid:3) ≤ 1 (cid:20) N(k ,k )+(cid:0) N(k ,k )2−N(k ,k )(cid:1) ×(cid:18) 8 (cid:19)(cid:21)
k1k2 N(k ,k )2 1 2 1 2 1 2 N
1 2
(74)
1 8
≤ + .
N(k ,k ) N
1 2
Using Eqs. (71) to (73) together with the inequality above yields
N(k ,k )2+8N(k1,k2)3 +6[N(k ,k )+1]2
|E[γ γ γ γ ]| ≤ 1 2 N 1 2 .
i1i2 i2i3 i3i4 i4i1
[N(k ,k )−1][N(k ,k )−2][N(k ,k )−3]
1 2 1 2 1 2
Provided n ≥ 5 for all k ∈ K , then N(k ,k ) ≥ 10 for all k ,k ∈ K . Furthermore,
k 1 2 1 2
N(k ,k ) − m ≥ (1 −
m)N(cid:74)
(k
(cid:75)
,k ) for each integer 0 ≤ m ≤ 9,
an(cid:74)
d
N(cid:75)
(k ,k ) + 1 ≤
1 2 10 1 2 1 2
11N(k ,k ). This yields
10 1 2
101010 (cid:18) 1 8 112 1 (cid:19) 33
|E[γ γ γ γ ]| ≤ + +6· ≤ .
i1i2 i2i3 i3i4 i4i1 9 8 7 N(k ,k ) N 102N(k ,k ) N(k ,k )
1 2 1 2 1 2
Given any choice of k ,k ,k ,k satisfying k = k and k = k , there are two possibilities
1 2 3 4 1 3 2 4
when counting the number of tuples (terms) with distinct entries i ,i ,i ,i ∈ n .
1 2 3 4
(cid:74) (cid:75)
1. Provided k = k , there are n (n −1)(n −2)(n −3) terms, and N(k ,k ) =
1 2 k1 k1 √k1 k1 1 2
n (n −1)/2. Since (n −2)(n −3) ≤ n n n n , it holds that
k1 k1 k1 k1 k1 k2 k3 k4
(cid:88) √
|E[γ γ γ γ ]| ≤ 2×33× n n n n .
i1i2 i2i3 i3i4 i4i1 k1 k2 k3 k4
i ,i ,i ,i ∈ n distinct
1 2 3 4
(cid:74) (cid:75)
g = k ,g = k ,
i1 1 i2 2
g = k ,g = k
i3 3 i4 4
2. Provided k ̸= k , there are n (n −1)n (n −1) terms, and N(k ,k ) = n n .
1 2 √ k1 k1 k2 k2 1 2 k1 k2
Since (n −1)(n −1) ≤ n n n n , it holds that
k1 k2 k1 k2 k3 k4
(cid:88) √
|E[γ γ γ γ ]| ≤ 1×33× n n n n .
i1i2 i2i3 i3i4 i4i1 k1 k2 k3 k4
i ,i ,i ,i ∈ n distinct
1 2 3 4
(cid:74) (cid:75)
g = k ,g = k ,
i1 1 i2 2
g = k ,g = k
i3 3 i4 4
Case 2: k = k , k = k , k ̸= k (or k = k , k = k , k ̸= k ) and g = k , g = k ,
1 2 3 4 1 3 2 3 1 4 1 2 i1 1 i2 2
g = k , g = k . We first analyze the situation k = k , k = k , k ̸= k . In this
i3 3 i4 4 1 2 3 4 1 3
situation, γ is an entry in block Γ while γ is an entry in block Γ , and both
i1i2 G k1G
k1
i3i4 G k3G
k3
63Cape, Yu, and Liao
γ and γ are entries in block Γ . Consequently, using Eq. (68) yields
i2i3 i4i1 G k1G
k3
E[γ γ γ γ ]
i1i2 i2i3 i3i4 i4i1
(cid:88)
= Pr[S = Ψ ,S = Ψ ,S = Ψ ]
k1k1 k1k1 k3k3 k3k3 k1k3 k1k3
Ψ ,Ψ ,Ψ ⊂{ i :i∈ N }
k1k1 k3k3 k1k3 N+1 (cid:74) (cid:75)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104)(cid:16) (cid:17)(cid:16) (cid:17)(cid:105)
×E
Ψ
k1k1
a−B(cid:101)k1k1 ×E
Ψ
k3k3
a−B(cid:101)k3k3 ×E
Ψ
k1k3
a−B(cid:101)k1k3 b−B(cid:101)k1k3
(cid:88)
= Pr[S = Ψ ,S = Ψ ,S = Ψ ]
k1k1 k1k1 k3k3 k3k3 k1k3 k1k3
Ψ ,Ψ ,Ψ ⊂{ i :i∈ N }
k1k1 k3k3 k1k3 N+1 (cid:74) (cid:75)
(cid:32) (cid:33)
×(cid:16)
Ψ
k1k1
−B(cid:101)k1k1(cid:17) ×(cid:16)
Ψ
k3k3
−B(cid:101)k3k3(cid:17)
×
N(k1,k3)(Ψ k1k3−B(cid:101)
Nk (1 kk 13
,) k2 3− )−E 1Ψk1k3[(a−B(cid:101)k1k3)2]
.
All entries of both R(cid:101)A and B(cid:101) are in the interval [0,1], which together with Eq. (74) yields
(cid:20) (cid:21)
N(k ,k ) (cid:16) (cid:17)2 1
|Eγ i1i2γ i2i3γ i3i4γ i4i1| ≤
N(k
,1
k
)3 −1E S
k1k3
−B(cid:101)k1k3 +
N(k ,k )−1
1 3 1 3
N(k 1,k 3) (cid:2) (cid:3) 1
≤ Var S +
N(k ,k )−1
k1k3
N(k ,k )−1
1 3 1 3
(cid:34) (cid:35)
N(k ,k ) 1 8 1
1 3
≤ + +
N(k ,k )−1 N(k ,k ) N N(k ,k )−1
1 3 1 3 1 3
12
≤ provided N(k ,k ) ≥ 10.
1 3
N(k ,k )
1 3
In this situation, N(k ,k ) = n n and there are n (n −1)n (n −1) distinct tuples
1 3 k1 k3 √ k1 k1 k3 k3
of i ,i ,i ,i . Since (n −1)(n −1) ≤ n n n n , it holds that
1 2 3 4 k1 k3 k1 k2 k3 k4
(cid:88) √
|Eγ γ γ γ | ≤ 1×12× n n n n .
i1i2 i2i3 i3i4 i4i1 k1 k2 k3 k4
i ,i ,i ,i ∈ n distinct
1 2 3 4
(cid:74) (cid:75)
g = k ,g = k ,
i1 1 i2 2
g = k ,g = k
i3 3 i4 4
The same analysis applies to the remaining situation k = k , k = k , k ̸= k , yielding
2 3 1 4 1 2
the same bound in the above display equation.
Case 3: k = k , k ̸= k (or k ̸= k , k = k ) and g = k , g = k , g = k , g = k .
1 3 2 4 1 3 2 4 i1 1 i2 2 i3 3 i4 4
We first analyze the situation k = k , k ̸= k . Without loss of generality, we assume that
1 3 2 4
n ≥ n . In this case, γ , γ are entries of block Γ and γ , γ are entries of
k2 k4 i1i2 i2i3 G k1G k2 i3i4 i4i1
block Γ . Thus,
G G
k1 k4
E[γ γ γ γ ]
i1i2 i2i3 i3i4 i4i1
(cid:88)
= Pr[S = Ψ ,S = Ψ ]
k1k2 k1k2 k1k4 k1k4
Ψ ,Ψ ⊂{ i :i∈ N }
k1k2 k1k4 N+1 (cid:74) (cid:75)
(cid:104)(cid:16) (cid:17)(cid:16) (cid:17)(cid:105) (cid:104)(cid:16) (cid:17)(cid:16) (cid:17)(cid:105)
×E
Ψ
k1k2
a−B(cid:101)k1k2 b−B(cid:101)k1k2 ×E
Ψ
k1k4
a−B(cid:101)k1k4 b−B(cid:101)k1k4
64Robust spectral clustering with rank statistics
and
|Eγ γ γ γ |
i1i2 i2i3 i3i4 i4i1
(cid:88)
≤ Pr[S = Ψ ,S = Ψ ]
k1k2 k1k2 k1k4 k1k4
Ψ ,Ψ ⊂{ i :i∈ N }
k1k2 k1k4 N+1 (cid:74) (cid:75)
(cid:16) (cid:17)2
×
N(k 1,k 2) Ψ
k1k2
−B(cid:101)k1k2 +E
Ψ
k1k2[(a−B(cid:101)k1k2)2]
.
N(k ,k )−1
1 2
The above inequality is obtained using Eq. (68) and |a − B(cid:101)k1k4| ≤ 1 for any a ∈ Ψ k1k4.
Furthermore, applying Eq. (74) yields
N(k ,k ) 1
|Eγ γ γ γ | ≤ 1 2 Var[S ]+
i1i2 i2i3 i3i4 i4i1
N(k ,k )−1
k1,k2
N(k ,k )−1
1 2 1 2 ,
12
≤ provided N(k ,k ) ≥ 10.
1 2
N(k ,k )
1 2
In particular,
(cid:88) √
|Eγ γ γ γ | ≤ 2×12× n n n n .
i1i2 i2i3 i3i4 i4i1 k1 k2 k3 k4
i ,i ,i ,i ∈ n distinct
1 2 3 4
(cid:74) (cid:75)
g = k ,g = k ,
i1 1 i2 2
g = k ,g = k
i3 3 i4 4
Here the constant 2 comes from considering the separate cases k = k and k ̸= k .
2 1 2 1
The above analysis also applies to the situation k ̸= k , k = k , yielding the same
1 3 2 4
upper bound in the display equation.
Case 4: Either of the five situations for k ,k ,k ,k and g = k , g = k , g = k ,
1 2 3 4 i1 1 i2 2 i3 3
g = k .
i4 4
• k ,k ,k ,k are pairwise different.
1 2 3 4
• k ,k ,k are pairwise different with k = k .
2 3 4 1 2
• k ,k ,k are pairwise different with k = k .
3 4 1 2 3
• k ,k ,k are pairwise different with k = k .
1 2 3 4 3
• k ,k ,k are pairwise different with k = k .
1 2 3 4 1
In all five situations, γ , γ , γ , γ belong to four different blocks of Γ. We have
i1i2 i2i3 i3i4 i4i1
E[γ γ γ γ ]
i1i2 i2i3 i3i4 i4i1
(cid:88)
= Pr[S =Ψ ,S =Ψ ,S =Ψ ,S =Ψ ]
k1k2 k1k2 k2k3 k2k3 k3k4 k3k4 k4k1 k4k1
Ψk1k2,Ψk2k3,Ψk3k4,Ψk4k1
⊂{ i :i∈ N }
N+1 (cid:74) (cid:75)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
×E
Ψk1k2
a−B(cid:101)k1k2 ×E
Ψk2k3
a−B(cid:101)k2k3 ×E
Ψk3k4
a−B(cid:101)k3k4 ×E
Ψk4k1
a−B(cid:101)k4k1
(cid:110)(cid:104) (cid:105)(cid:104) (cid:105)(cid:104) (cid:105)(cid:104) (cid:105)(cid:111)
=E S
k1k2
−B(cid:101)k1k2 S
k2k3
−B(cid:101)k2k3 S
k3k4
−B(cid:101)k3k4 S
k4k1
−B(cid:101)k4k1 .
65Cape, Yu, and Liao
Furthermore,
|Eγ γ γ γ |
i1i2 i2i3 i3i4 i4i1
(cid:115)
(cid:26)(cid:104) (cid:105)2(cid:104) (cid:105)2(cid:27) (cid:26)(cid:104) (cid:105)2(cid:104) (cid:105)2(cid:27)
≤ E S
k1k2
−B(cid:101)k1k2 S
k2k3
−B(cid:101)k2k3 E S
k3k4
−B(cid:101)k3k4 S
k4k1
−B(cid:101)k4k1
(cid:115)
(cid:26)(cid:104) (cid:105)2(cid:27) (cid:26)(cid:104) (cid:105)2(cid:27)
≤ E S
k1k2
−B(cid:101)k1k2 E S
k3k4
−B(cid:101)k3k4 (75)
(cid:113)
(cid:2) (cid:3) (cid:2) (cid:3)
≤ Var S Var S
k1k2 k3k4
9
≤ provided N(k ,k ),N(k ,k )≥10.
(cid:112) 1 2 3 4
N(k ,k )N(k ,k )
1 2 3 4
Here, the first inequality is obtained via the Cauchy–Schwarz inequality, and the fourth
inequality is obtained using Eq. (74). Consequently, in the present case,
(cid:88) √
|Eγ γ γ γ | ≤ 2×9× n n n n .
i1i2 i2i3 i3i4 i4i1 k1 k2 k3 k4
i ,i ,i ,i ∈ n distinct
1 2 3 4
(cid:74) (cid:75)
g = k ,g = k ,
i1 1 i2 2
g = k ,g = k
i3 3 i4 4
(⋆)
The above analysis considers all possible configurations of g , g , g , g when i , i ,
i1 i2 i3 i4 1 2
i , i are pairwise different. These four cases cover all possible scenarios for g ,g ,g ,g .
3 4 i1 i2 i3 i4
Provided n ≥ 5 for all k ∈ K , then for any k ,k ,k ,k ∈ K , we have established the
k 1 2 3 4
(cid:74) (cid:75) (cid:74) (cid:75)
upper bound
(cid:88) √
|Eγ γ γ γ | ≤ 66 n n n n (76)
i1i2 i2i3 i3i4 i4i1 k1 k2 k3 k4
i ,i ,i ,i ∈ n distinct
1 2 3 4
(cid:74) (cid:75)
g = k ,g = k ,
i1 1 i2 2
g = k ,g = k
i3 3 i4 4
which holds irrespective of the values taken by k ,k ,k ,k .
1 2 3 4
66Robust spectral clustering with rank statistics
Finally,
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:88) E[γ i1i2γ i2i3γ i3i4γ i4i1](cid:12) (cid:12)
(cid:12)
(cid:12) i ,i ,i ,i ∈ n (cid:12)
(cid:12) 1 2 3 4 (cid:12)
(cid:12) pairwise diffe(cid:74) ren(cid:75) t (cid:12)
(cid:12) (cid:12)
(cid:88) (cid:88)
≤ |Eγ γ γ γ |
i1i2 i2i3 i3i4 i4i1
k1,k2,k3,k4∈ (cid:74)K
(cid:75)
i 1,i 2,i 3,i
4
∈ n
(cid:74) (cid:75)
pairwise different,
g = k ,g = k ,
i1 1 i2 2
g = k ,g = k
i3 3 i4 4
 
(cid:88) √
≤ 66 n k1n k2n k3n k4
k1,k2,k3,k4∈ K
(cid:74) (cid:75)
 4
(cid:88) √
≤ 66 n k
k∈ K
(cid:16)√ (cid:74) (cid:75) (cid:17)4
≤ 66 Kn
≤ 66K2n2.
√
Above, we use the fact that for the K-dimensional vector of all ones, 1 and ⃗n =
√ √ √ K
( n ,..., n )T, we have ∥ ⃗n∥2 = n. By the Cauchy–Schwarz inequality,
1 K ℓ2
√ √ √
|⟨ ⃗n,1 ⟩| ≤ ∥ ⃗n∥ ∥1 ∥ = Kn.
K ℓ2 K ℓ2
Using the result above together with Eq. (67), we finally have
E(cid:2) trace(Γ4)(cid:3) ≤ 66K2n2+2n3.
This concludes the proof of Lemma 4.
A.8 Proofs: a combinatorial expectation bound
Lemma 35 (A combinatorial expectation bound) Let A ∼ Blockmodel(FK,Θn×K)
per Definition 1. Let R(cid:101)A denote the matrix of normalized rank statistics of A, per Algo-
rithm 1. There exists a universal constant C > 0 such that if n ≥ 4 for all k ∈ K , then
k
for each i ∈ n , (cid:74) (cid:75)
(cid:74) (cid:75)
(cid:12)  (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)r∈(cid:88)
K
n1 rE

h,l∈
n(cid:88) ;s,t∈Gr(cid:110) R(cid:101)A−E[R(cid:101)A](cid:111) ih(cid:110) R(cid:101)A−E[R(cid:101)A](cid:111) il(cid:110) R(cid:101)A−E[R(cid:101)A](cid:111) hs(cid:110) R(cid:101)A−E[R(cid:101)A](cid:111) lt(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:74) (cid:75)
(cid:40)
(cid:115)(cid:74) (cid:75)
(cid:41)
n2 K3n3
≤Cmax , .
n n
gi gi
(77)
67Cape, Yu, and Liao
Proof [Proof of Lemma 35] We adopt the notation used in the proof of Lemma 3, namely
Γ = R(cid:101)A−E[R(cid:101)A] whose entry (i,h) is denoted by γ ih. With this convention, we express the
left-hand side of Eq. (77) in the more compact form
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12). (78)
(cid:12) n (cid:12)
r
(cid:12)r∈ K h,l∈ n ;s,t∈Gr (cid:12)
(cid:74) (cid:75) (cid:74) (cid:75)
In what follows, we bound |E[γ γ γ γ ]| by considering different cases for indices h,l,s,t,
ih il hs lt
restricting to situations when i ̸= h, i ̸= l, h ̸= s and l ̸= t, since otherwise the expectation
vanishes. We group all possible configurations of h,l,s,t, under the above restriction, into
four cases. Importantly, {i,h}, {i,l}, {h,s} and {l,t} are four distinct sets if and only if
h ̸= l, s ̸= i, t ̸= i and (s,t) ̸= (l,h). The four cases of interest for h,l,s,t are given below.
Case 1: h ̸= l, s ̸= i, t ̸= i, and (s,t) ̸= (l,h),
Case 2: h = l,
Case 3: h ̸= l, i ∈ {s,t}, and (s,t) ̸= (l,h),
Case 4: (s,t) = (l,h).
We shall handle these cases by further dividing each one into several subcases which can
be analyzed separately. In this proof, we will repeatedly use the fact that, when n ≥ 4 for
k
all k ∈ K , then N(k ,k ) ≥ 6 for all k ,k ∈ K , with N(k ,k ) denoting the number of
1 2 1 2 1 2
(cid:74) (cid:75) (cid:74) (cid:75)
unique matrix entries in the (k ,k ) block, as defined in the proof of Lemma 4.
1 2
Case 1.1: h ̸= l, s ̸= i, t ̸= i, (s,t) ̸= (l,h), g = g and r ̸= g .
h l i
Denote g by k. In this case, the variables γ , γ are entries of the block Γ , and γ ,
γ are
enh
tries of the block Γ . Using the
i nh otai tl
ion for S , Ψ and
EGgiG k
as in
th hs
e
lt G kGr k1k2 k1k2 Ψ k1k2
proof of Lemma 4, we compute
|E[γ γ γ γ ]|
ih il hs lt
(cid:12)
(cid:12) (cid:88)
= (cid:12) Pr[S = Ψ ,S = Ψ ]
(cid:12) gik gik kr kr
(cid:12)
Ψ ,Ψ ⊂{ i :i∈ N }
gik kr N+1 (cid:74) (cid:75)
(cid:12)
(cid:104) (cid:105) (cid:104) (cid:105)(cid:12)
×E
Ψ gik
(a−B(cid:101)gik)(b−B(cid:101)gik) E
Ψ kr
(a−B(cid:101)kr)(b−B(cid:101)kr) (cid:12)
(cid:12)
(cid:12)
≤
(cid:88)
Pr[S = Ψ ]
N(k,r)E2
Ψ
kr[a−B(cid:101)kr]+E
Ψ
kr[(a−B(cid:101)kr)2] (79)
kr kr
N(k,r)−1
Ψ ⊂{ i :i∈ N }
kr N+1 (cid:74) (cid:75)
6 (cid:88) 1
≤ Pr[S
kr
= Ψ kr](Ψ
kr
−B(cid:101)kr)2+
5 N(k,r)−1
Ψ ⊂{ i :i∈ N }
kr N+1 (cid:74) (cid:75)
(cid:18) (cid:19)
6 1
≤ Var[S ]+ .
kr
5 N(k,r)
68Robust spectral clustering with rank statistics
ThefirstinequalityaboveisestablishedusingEq.(68)andthefactthatE
Ψ
gik[(a−B(cid:101)gik)(b−
B(cid:101)gik)] ∈ [−1,1]. The second inequality invokes the relation N(k,r)−1 ≥ 5 6N(k,r) since
N(k,r) ≥ 6. We showed in Eq. (74) that there exists a constant C > 0, such that
1
Var[S ] ≤ C /N(k ,k ) for any k ,k ∈ K . Thus,
k1k2 1 1 2 1 2
(cid:74) (cid:75)
6(C +1)
|E[γ γ γ γ ]| ≤ 1 . (80)
ih il hs lt
5N(k,r)
For any fixed k and fixed r ̸= g , the number of h,l,s,t tuples satisfying Case 1.1 does not
i
exceed n2n2. Therefore,
k r
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12)
(cid:12) n (cid:12)
r
(cid:12) (cid:12)r∈ (cid:74)K (cid:75) h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e1.1 (cid:12) (cid:12)
≤
(cid:88) 1 (cid:88) 6(C 1+1)n2 kn2
r
(81)
n 5N(k,r)
r
r∈ K −{gi} k∈ K
(cid:74) (cid:75) (cid:74) (cid:75)
6·2·4 (cid:88) 1 (cid:88)
≤ (C +1) n n
1 k r
5·3 n
r
r∈ K −{gi} k∈ K
(cid:74) (cid:75) (cid:74) (cid:75)
16
≤ (C +1)(K −1)n.
1
5
Above, we have used the fact that n ≥ 4 implies n /(n −1) ≤ 4/3.
k k k
Case 1.2: h ̸= l, s ̸= i, t ̸= i, (s,t) ̸= (l,h), g = g , and r = g .
h l i
We denote g by k. In this case, γ , γ ,γ , γ are all entries of the block Γ . Thus,
h ih il hs lt GgiG k
|E[γ γ γ γ ]|
ih il hs lt
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:88) (cid:104) (cid:105)(cid:12)
=(cid:12)
(cid:12)
(cid:12)
Pr[S
gik
=Ψ gik]E
Ψgik
(a−B(cid:101)gik)(b−B(cid:101)gik)(c−B(cid:101)gik)(d−B(cid:101)gik) (cid:12)
(cid:12)
(cid:12)
(cid:12)Ψgik⊂{ Ni +1:i∈ (cid:74)N (cid:75)} (cid:12)
(cid:104) (cid:105)
N(g i,k)3E (S gi,k−B(cid:101)gik)4 +6[N(g i,k)+1]2
≤
[N(g i,k)−1][N(g i,k)−2][N(g i,k)−3] (82)
N(g ,k)3Var[S ]+6[N(g ,k)+1]2
≤ i gik i
[N(g ,k)−1][N(g ,k)−2][N(g ,k)−3]
i i i
C N(g ,k)2+6[N(g ,k)+1]2
≤ 1 i i
[N(g ,k)−1][N(g ,k)−2][N(g ,k)−3]
i i i
(cid:18) (cid:19)
18C 147 1
≤ 1 + .
5 5 N(g ,k)
i
Above, the first inequality is established using Eq. (70) and Eq. (71), and the second
inequality is established using Eq. (73). For any fixed k, the number of h,l,s,t tuples
69Cape, Yu, and Liao
satisfying Case 1.2 does not exceed n2 n2. Thus,
gi k
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) (cid:12) ≤ 1 (cid:88) (cid:18) 18C 1 + 147(cid:19) n2 gin2 k
(cid:12) n (cid:12) n 5 5 N(g ,k)
(cid:12) (cid:12)r∈ (cid:74)K (cid:75)
r
h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e1.2 (cid:12) (cid:12)
gi
k∈ (cid:74)K (cid:75)
i
(83)
(cid:18) (cid:19)
48C 392
1
≤ + n.
5 5
Case 1.3: h ̸= l, s ̸= i, t ̸= i, (s,t) ̸= (l,h), g ̸= g , r ̸= g , and {g ,g } ≠ {g ,r}.
h l i h l i
In this case, γ , γ , γ , γ are in four different blocks of Γ, determined by the structure
ih il hs lt
in the membership matrix Θ. Denote g and g by k and k′, respectively. Then,
h l
|E[γ γ γ γ ]|
ih il hs lt
(cid:12)
(cid:12) (cid:88)
=(cid:12) Pr[S =Ψ ,S =Ψ ,S =Ψ ,S =Ψ ]
(cid:12) gik gik gik′ gik′ kr kr k′r k′r
(cid:12)
Ψgik,Ψ gik′,Ψkr,Ψ k′r⊂{ Ni +1:i∈ (cid:74)N (cid:75)}
(cid:12)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)(cid:12)
×E
Ψgik
(a−B(cid:101)gik) E
Ψ gik′
(a−B(cid:101)gik′) E
Ψkr
(a−B(cid:101)kr) E
Ψ k′r
(a−B(cid:101)k′r) (cid:12)
(cid:12) (cid:12) (84)
(cid:12) (cid:104) (cid:105)(cid:12)
=(cid:12) (cid:12)E (S gik−B(cid:101)gik)(S
gik′
−B(cid:101)gik′)(S kr−B(cid:101)kr)(S k′r−B(cid:101)k′r) (cid:12)
(cid:12)
(cid:113)
(cid:2) (cid:3) (cid:2) (cid:3)
≤ Var S Var S
gik k′r
C
≤ 1 .
(cid:112)
N(g ,k)N(k′,r)
i
Above, the first inequality is established using Eq. (75). For any fixed r,k,k′ ∈ K
satisfying r ̸= g , k ̸= k′ and {k,k′} ̸= {g ,r}, the number of h,l,s,t tuples satisfy(cid:74) ing(cid:75)
i i
Case 1.3 does not exceed n n n2. Thus,
k k′ r
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) (cid:12) ≤ (cid:88) 1 (cid:88) (cid:112) C 1n kn k′n2 r
(cid:12)
(cid:12) (cid:12)r∈ (cid:74)K (cid:75)
n r
h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e1.3
(cid:12)
(cid:12) (cid:12) r∈ (cid:74)K (cid:75)−{gi}
n r
k,k′∈ (cid:74)K (cid:75)
N(g i,k)N(k′,r)
(cid:114)
8 1 (cid:88) √
≤ C 1 √ n kn k′n r
3 n
gi k,k′,r∈ K (85)
(cid:74) (cid:75)
 3
(cid:114)
8 1 (cid:88) √
≤ C 1 √  n k
3 n
gi
k∈ K
(cid:114) (cid:115) (cid:74) (cid:75)
8 K3n3
≤ C .
1
3 n
gi
Case 1.4: h ̸= l, s ̸= i, t ̸= i, (s,t) ̸= (l,h), g ̸= g , r ̸= g and {g ,g } = {g ,r}.
h l i h l i
If (g ,g ) = (g ,r), then γ and γ are entries of blocks Γ and Γ , respectively,
h l i ih lt GgiGgi GrGr
70Robust spectral clustering with rank statistics
while γ and γ are entries of the block Γ . Thus,
il hs GgiGr
|E[γ γ γ γ ]|
ih il hs lt
(cid:12)
(cid:12) (cid:88)
= (cid:12) Pr[S = Ψ ,S = Ψ ,S = Ψ ]
(cid:12) gigi gigi rr rr gir gir
(cid:12)
Ψgigi,Ψrr,Ψgir⊂{ N+i 1:i∈ (cid:74)N (cid:75)}
(cid:12)
(cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)(cid:12)
×E
Ψgigi
(a−B(cid:101)gigi) E
Ψrr
(a−B(cid:101)rr) E
Ψgir
(a−B(cid:101)gir)(b−B(cid:101)gir) (cid:12)
(cid:12)
(cid:12)
(86)
≤
(cid:88)
Pr[S = Ψ ]
N(g i,r)E2 Ψgir[a−B(cid:101)gir]+E Ψgir[(a−B(cid:101)gir)2]
gir gir
N(g ,r)−1
i
Ψgir⊂{ N+i 1:i∈ (cid:74)N (cid:75)}
(cid:18) (cid:19)
6 1
≤ Var[S ]+
5
gir
N(g ,r)
i
6 1
≤ (C +1) .
1
5 N(g ,r)
i
The inequalities above are established in the same way as Eq. (79). Note that the last
inequality also holds for the situation where (g ,g ) = (g ,r). For any fixed r ∈ K , the
l h i
number of h,l,s,t tuples satisfying Case 1.4 does not exceed 2n n3. Therefore, (cid:74) (cid:75)
gi r
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) (cid:12) ≤ 6 (C 1+1) (cid:88) 1 2n gin3 r
(cid:12) n (cid:12) 5 n N(g ,r)
r r i
(cid:12) (cid:12)r∈ (cid:74)K (cid:75) h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e1.4 (cid:12) (cid:12) r∈ (cid:74)K (cid:75)−{gi}
(87)
12 (cid:88)
≤ (C +1) n
1 r
5
r∈ K −{gi}
(cid:74) (cid:75)
12
≤ (C +1)(n−n ).
5
1 gi
Case 1.5: h ̸= l, s ̸= i, t ̸= i, (s,t) ̸= (l,h), g ̸= g and r = g .
h l i
Denote g and g by k and k′, respectively. In this case, γ , γ are entries of the block
h l ih hs
Γ and γ , γ are entries of the block Γ . Thus,
GgiG k il lt GgiG k′
|E[γ γ γ γ ]|
ih il hs lt
(cid:12)
= (cid:12) (cid:12) (cid:88) Pr(cid:2) S = Ψ ,S = Ψ (cid:3)
(cid:12) gik gik gik′ gik′
(cid:12)
Ψ gik,Ψ gik′⊂{ N+i 1:i∈ (cid:74)N (cid:75)}
(cid:12)
(cid:104) (cid:105) (cid:104) (cid:105)(cid:12)
×E
Ψ gik
(a−B(cid:101)gik)(b−B(cid:101)gik) E
Ψ gik′
(a−B(cid:101)gik′)(b−B(cid:101)gik′) (cid:12)
(cid:12)
(cid:12)
(88)
(cid:26)(cid:18) (cid:19) (cid:18) (cid:19)(cid:27)
6 1 1
≤ min Var[S ]+ , Var[S ]+
5 gik N(g ,k) gik′ N(g ,k′)
i i
6 1
≤ (C +1) .
5 1 max{N(g ,k),N(g ,k′)}
i i
71Cape, Yu, and Liao
The inequalities above are established in the same way as Eq. (79). For fixed k,k′ ∈ K ,
the number of h,l,s,t tuples satisfying Case 1.5 does not exceed n n n2 . Therefore,(cid:74) (cid:75)
k k′ gi
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12)
(cid:12) n (cid:12)
r
(cid:12) (cid:12)r∈ (cid:74)K (cid:75) h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e1.5 (cid:12) (cid:12)
(89)
≤
1 (cid:88) 6
(C +1)
n kn k′n2 gi
n 5 1 max{N(g ,k),N(g ,k′)}
gi
k,k′∈ K ,k̸=k′
i i
(cid:74) (cid:75)
6
≤ (C +1)Kn.
1
5
Case 2.1: h = l and s = t = i. The total number of h,l,s,t tuples satisfying Case 2.1 is
n−1. Using the trivial bound |E[γ γ γ γ ]| ≤ 1 yields
ih il hs lt
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) ≤ n−1 . (90)
(cid:12) n (cid:12) n
(cid:12) (cid:12)r∈ (cid:74)K (cid:75)
r
h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e2.1 (cid:12) (cid:12)
gi
Case 2.2: h = l and s = t ̸= i. With any fixed r, the number of h,l,s,t tuples satisfying
Case 2.2 does not exceed n n. Therefore,
r
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) ≤ (cid:88) n rn ≤ Kn. (91)
(cid:12) n (cid:12) n
r r
(cid:12) (cid:12)r∈ (cid:74)K (cid:75) h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e2.2 (cid:12) (cid:12) r∈ (cid:74)K (cid:75)
Case 2.3: h = l, s ̸= t and i ∈ {s,t}. In this case, since i ∈ {s,t}, we have r = g .
i
Consequently, the total number of h,l,s,t tuples satisfying Case 2.3 does not exceed 2n n,
gi
hence
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) ≤ 2n gin ≤ 2n. (92)
(cid:12) n (cid:12) n
(cid:12) (cid:12)r∈ (cid:74)K (cid:75)
r
h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e2.3 (cid:12) (cid:12)
gi
72Robust spectral clustering with rank statistics
Case 2.4: h = l, s ̸= t, i ∈/ {s,t} and r = g . Let k denote g . In this case, γ γ γ γ =
i h ih il hs lt
γ2 γ γ and γ ,γ ,γ are all entries of the block Γ . Thus,
ih hs ht ih hs ht GgiG k
|E[γ γ γ γ ]|
ih il hs lt
(cid:12)
(cid:12) (cid:88)
=(cid:12) Pr[S =Ψ ]
(cid:12) gik gik
(cid:12)
Ψgik⊂{ Ni +1:i∈ (cid:74)N (cid:75)}
(cid:12)
(cid:104) (cid:105)(cid:12)
×E
Ψgik
(a−B(cid:101)gik)2(b−B(cid:101)gik)(c−B(cid:101)gik) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:88) 1
=(cid:12) Pr[S =Ψ ]
(cid:12) gik gik N(g ,k)[N(g ,k)−1][N(g ,k)−2]
(cid:12) i i i
Ψgik⊂{ Ni +1:i∈ (cid:74)N (cid:75)}
(cid:110)
× N(g i,k)3E2 Ψgik[a−B(cid:101)gik]E Ψgik[(a−B(cid:101)gik)2]
(93)
−N(g i,k)2E2 Ψgik[(a−B(cid:101)gik)2]+2N(g i,k)E Ψgik[(a−B(cid:101)gik)4]
(cid:12)
(cid:111)(cid:12)
−2N(g i,k)2E Ψgik[a−B(cid:101)gik]E Ψgik[(a−B(cid:101)gik)3] (cid:12)
(cid:12)
(cid:12)
≤
(cid:88)
Pr[S =Ψ
]N(g i,k)3(Ψ gik−B(cid:101)gik)2+3N(g i,k)2+2N(g i,k)
gik gik N(g ,k)[N(g ,k)−1][N(g ,k)−2]
i i i
Ψgik⊂{ Ni +1:i∈ (cid:74)N (cid:75)}
(cid:18) (cid:19)
9 (cid:2) (cid:3) 3 2
≤ Var S + +
5 gik N(g ,k) N(g ,k)2
i i
(cid:18) (cid:19)
9 10 1
≤ C + .
5 1 3 N(g ,k)
i
Above, the second equation is established using Eq. (69). With any fixed k, the number of
h,l,s,t tuples satisfying Case 2.4 does not exceed n n2 . Thus,
k gi
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) (cid:12) ≤ 1 (cid:88) 9 (cid:18) C 1+ 10(cid:19) n kn2 gi
(cid:12) n (cid:12) n 5 3 N(g ,k)
(cid:12) (cid:12)r∈ (cid:74)K (cid:75)
r
h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e2.4 (cid:12) (cid:12)
gi
k∈ (cid:74)K (cid:75)
i
(94)
(cid:18) (cid:19)
24
≤ C +16 K.
1
5
Case 2.5: h = l, s ̸= t, i ∈/ {s,t} and r ̸= g . Let k denote g . In this case, γ γ γ γ =
i h ih il hs lt
γ2 γ γ , where γ is an entry of the block Γ . Further, γ and γ are both entries
ih hs ht ih GgiG k hs ht
73Cape, Yu, and Liao
of the block Γ . Thus,
G kGr
|E[γ γ γ γ ]|
ih il hs lt
(cid:12)
(cid:12) (cid:88)
= (cid:12) Pr[S = Ψ ,S = Ψ ]
(cid:12) gik gik kr kr
(cid:12)
Ψ ,Ψ ⊂{ i :i∈ N }
gik kr N+1 (cid:74) (cid:75)
(cid:12)
(cid:104) (cid:105) (cid:104) (cid:105)(cid:12)
×E
Ψ gik
(a−B(cid:101)gik)2 E
Ψ kr
(a−B(cid:101)kr)(b−B(cid:101)kr) (cid:12)
(cid:12)
(cid:12)
≤
(cid:88)
Pr[S = Ψ ]
N(k,r)E2
Ψ
kr[a−B(cid:101)kr]+E
Ψ
kr[(a−B(cid:101)kr)2]
kr kr (95)
N(k,r)−1
Ψ ⊂{ i :i∈ N }
kr N+1 (cid:74) (cid:75)
6 (cid:88) 1
≤ Pr[S
kr
= Ψ kr](Ψ
kr
−B(cid:101)kr)2+
5 N(k,r)−1
Ψ ⊂{ i :i∈ N }
kr N+1 (cid:74) (cid:75)
(cid:18) (cid:19)
6 1
≤ Var[S ]+
kr
5 N(k,r)
6 1
≤ (C +1) .
1
5 N(k,r)
ThefirstinequalityaboveisestablishedusingEq.(68)andthefactthatE
Ψ
gik[(a−B(cid:101)gik)2] ≤
1. For any fixed r and k, the number of h,l,s,t tuples satisfying Case 2.5 does not exceed
n n2. Therefore,
k r
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) (cid:12) ≤ (cid:88) 1 (cid:88) 6 (C 1+1) n kn2 r
(cid:12) n (cid:12) n 5 N(k,r)
(cid:12) (cid:12)r∈ (cid:74)K (cid:75) r h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e2.5 (cid:12) (cid:12) r∈ (cid:74)K (cid:75)−{gi} r k∈ (cid:74)K (cid:75) (96)
16
≤ (C +1)(K −1)K.
1
5
Case 3.1: h ̸= l and s = t = i. The total number of h,l,s,t tuples satisfying Case 3.1 does
not exceed n2. Therefore, applying the simple bound |E[γ γ γ γ ]| ≤ 1 yields
ih il hs lt
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) (cid:12) ≤ n2 . (97)
(cid:12) n (cid:12) n
(cid:12) (cid:12)r∈ (cid:74)K (cid:75)
r
h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e3.1 (cid:12) (cid:12)
gi
Case 3.2: h ̸= l, s ̸= t, i ∈ {s,t}, (s,t) ̸= (l,h) and g = g . Denote g by k. When
h l h
s ̸= t = i, we have γ γ γ γ = γ γ2γ . Further, γ ,γ ,γ are all entries of the block
ih il hs lt ih il hs ih il hs
74Robust spectral clustering with rank statistics
Γ . Thus,
GgiG
k
|E[γ γ γ γ ]|
ih il hs lt
(cid:12)
(cid:12) (cid:88)
= (cid:12) Pr[S = Ψ ]
(cid:12) gik gik
(cid:12)
Ψ ⊂{ i :i∈ N }
gik N+1 (cid:74) (cid:75) (cid:12) (98)
(cid:104) (cid:105)(cid:12)
×E
Ψ gik
(a−B(cid:101)gik)2(b−B(cid:101)gik)(c−B(cid:101)gik) (cid:12)
(cid:12)
(cid:12)
(cid:18) (cid:19)
9 10 1
≤ C + .
1
5 3 N(g ,k)
i
The inequality above is established in the same way as Eq. (93). Moreover, this inequality
is also true for the situation where t ̸= s = i. For any fixed k, the number of h,l,s,t tuples
satisfying Case 3.2 does not exceed 2n2n . Thus,
k gi
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) (cid:12) ≤ 1 (cid:88) 18 (cid:18) C 1+ 10(cid:19) n2 kn gi
(cid:12) n (cid:12) n 5 3 N(g ,k)
(cid:12) (cid:12)r∈ (cid:74)K (cid:75)
r
h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e3.2 (cid:12) (cid:12)
gi
k∈ (cid:74)K (cid:75)
i
(99)
(cid:18) (cid:19)
48 n
≤ C +32 .
1
5 n
gi
Case 3.3: h ̸= l, s ̸= t, i ∈ {s,t}, (s,t) ̸= (l,h) and g ̸= g . Denote g and g by k and
h l h j
k′, respectively. In the situation when s ̸= t = i, we have γ γ γ γ = γ γ2γ , where
ih il hs lt ih il hs
γ ,γ are both entries of the block Γ , and γ is an entry of the block Γ . Thus,
ih hs GgiG k il GgiG k′
|E[γ γ γ γ ]|
ih il hs lt
(cid:12)
= (cid:12) (cid:12) (cid:88) Pr(cid:2) S = Ψ ,S = Ψ (cid:3)
(cid:12) gik′ gik′ gik gik
(cid:12)
Ψ gik′,Ψ gik⊂{ N+i 1:i∈ (cid:74)N (cid:75)}
(cid:12) (100)
(cid:104) (cid:105) (cid:104) (cid:105)(cid:12)
×E
Ψ gik′
(a−B(cid:101)gik′)2 E
Ψ gik
(a−B(cid:101)gik)(b−B(cid:101)gik) (cid:12)
(cid:12)
(cid:12)
6 1
≤ (C +1) .
1
5 N(g ,k)
i
The inequality above is established in the same way as Eq. (95). In the situation that
t ̸= s = i, the same method yields
6 1
|E[γ γ γ γ ]| ≤ (C +1) . (101)
ih il hs lt 5 1 N(g ,k′)
i
75Cape, Yu, and Liao
For any fixed k and k′, the number of h,l,s,t tuples satisfying Case 3.3 with s ̸= t = i
(t ̸= s = i) does not exceed n n n . Therefore,
k k′ gi
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12)
(cid:12) n (cid:12)
r
(cid:12) (cid:12)r∈ (cid:74)K (cid:75) h,l,sh ,t,l∈ sa(cid:74)tn is(cid:75)f; ys,t C∈ aG sr e3.3 (cid:12) (cid:12)
(cid:20) (cid:21) (102)
1 (cid:88) 6 1 1
≤ (C +1)n n n +
n 5 1 k k′ gi N(g ,k) N(g ,k′)
gi
k,k′∈ K
i i
(cid:74) (cid:75)
16 Kn
≤ (C +1) .
1
5 n
gi
Case 4: (s,t) = (l,h). For any fixed r, the number of h,l,s,t tuples satisfying Case 4 is
n2. Thus, since |E[γ γ γ γ ]| ≤ 1, we have
r ih il hs lt
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12) (cid:12) ≤ (cid:88) n2 r ≤ n. (103)
(cid:12) n (cid:12) n
r r
(cid:12) (cid:12)r∈ (cid:74)K (cid:75) h,l,h s, ,l t∈ s(cid:74)an t(cid:75)is;s fy,t∈ CG ar se4 (cid:12) (cid:12) r∈ (cid:74)K (cid:75)
We have finished analyzing all possible cases for the h,l,s,t tuple, while restricting to
the situation i ̸= h, i ̸= l, h ̸= s, and l ̸= t. Combining the above results yields
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:88) 1 (cid:88) E[γ ihγ ilγ hsγ lt](cid:12) (cid:12)
(cid:12) n (cid:12)
r
(cid:12)r∈ K h,l∈ n ;s,t∈Gr (cid:12)
(cid:74) (cid:75) (cid:18) (cid:74) (cid:75) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
22C +27 44C +403 24
≤ 1 Kn+ 1 n+ C +16 K2
1
5 5 5
(104)
(cid:32) (cid:114) 8(cid:33)(cid:115) K3n3 n2 (cid:18) 16 (cid:19) Kn (cid:18) 48 (cid:19) n
+ C + + (C +1) + C +33
1 1 1
3 n n 5 n 5 n
gi gi gi gi
(cid:40) (cid:115) (cid:41)
n2 K3n3
≤ Cmax , ,
n n
gi gi
where C > 0 is a constant. This concludes the proof of Lemma 35.
References
Emmanuel Abbe. Community detection and stochastic block models: recent developments.
Journal of Machine Learning Research, 18(177):1–86, 2018.
Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector
analysis of random matrices with low expected rank. Annals of Statistics, 48(3):1452–
1474, 2020.
76Robust spectral clustering with rank statistics
Emmanuel Abbe, Jianqing Fan, and Kaizheng Wang. An ℓ theory of PCA and spectral
p
clustering. Annals of Statistics, 50(4):2359–2385, 2022.
JoshuaAgterberg, ZacharyLubberts, andCareyEPriebe. Entrywiseestimationofsingular
vectors of low-rank matrices with heteroskedasticity and dependence. IEEE Transactions
on Information Theory, 68(7):4618–4650, 2022.
Christopher Aicher, Abigail Z Jacobs, and Aaron Clauset. Learning latent block structure
in weighted networks. Journal of Complex Networks, 3(2):221–248, 2015.
EdoardoMAiroldi,DavidMBlei,StephenEFienberg,andEricPXing. Mixedmembership
stochastic blockmodels. Journal of Machine Learning Research, 9(65):1981–2014, 2008.
Hafiz Tiomoko Ali and Romain Couillet. Improved spectral community detection in large
heterogeneous networks. Journal of Machine Learning Research, 18(225):1–49, 2018.
Arash A Amini, Aiyou Chen, Peter J Bickel, and Elizaveta Levina. Pseudo-likelihood
methods for community detection in large sparse networks. Annals of Statistics, 41(4):
2097–2122, 2013.
Jesu´sArroyo, AvantiAthreya, JoshuaCape, GuodongChen, CareyEPriebe, andJoshuaT
Vogelstein. Inference for multiple heterogeneous networks with a common invariant sub-
space. Journal of Machine Learning Research, 22(142):1–49, 2021.
AvantiAthreya,DonniellEFishkind,MinhTang,CareyEPriebe,YoungserPark,JoshuaT
Vogelstein, Keith Levin, Vince Lyzinski, and Yichen Qin. Statistical inference on random
dotproductgraphs: asurvey. Journal of Machine Learning Research,18(226):1–92,2018.
Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise thresh-
olds for spectral clustering. Advances in Neural Information Processing Systems, 24,
2011.
Afonso S Bandeira and Ramon van Handel. Sharp nonasymptotic bounds on the norm of
random matrices with independent entries. Annals of Probability, 44(4):2479–2506, 2016.
Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.
Patrick Billingsley. Probability and measure: anniversary edition. Wiley, New York, 2012.
Aleksandar Bojchevski, Yves Matkovic, and Stephan Gu¨nnemann. Robust spectral cluster-
ing for noisy data: modeling sparse corruptions improves latent embeddings. In Proceed-
ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 737–746, 2017.
Changxiao Cai, Gen Li, Yuejie Chi, H Vincent Poor, and Yuxin Chen. Subspace estimation
from unbalanced and incomplete data matrices: ℓ statistical guarantees. Annals of
2,∞
Statistics, 49(2):944–967, 2021.
T Tony Cai and Xiaodong Li. Robust and computationally feasible community detection
in the presence of arbitrary outlier nodes. Annals of Statistics, 43(3):1027–1059, 2015.
77Cape, Yu, and Liao
T Tony Cai and Anru Zhang. Rate-optimal perturbation bounds for singular subspaces
with applications to high-dimensional statistics. Annals of Statistics, 46(1):60–89, 2018.
JoshuaCape,MinhTang,andCareyEPriebe. Signal-plus-noisematrixmodels: eigenvector
deviations and fluctuations. Biometrika, 106(1):243–250, 2019a.
Joshua Cape, Minh Tang, and Carey E Priebe. The two-to-infinity norm and singular
subspace geometry with applications to high-dimensional statistics. Annals of Statistics,
47(5):2405–2439, 2019b.
Hong Chang and Dit-Yan Yeung. Robust path-based spectral clustering. Pattern Recogni-
tion, 41(1):191–203, 2008.
Kamalika Chaudhuri and Satish Rao. Beyond Gaussians: spectral methods for learning
mixtures of heavy-tailed product distributions. In 21st Annual Conference on Learning
Theory (COLT ’08), pages 21–32, 2008.
Fenxiao Chen, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo. Graph representation
learning: a survey. APSIPA Transactions on Signal and Information Processing, 9, 2020.
Yudong Chen, Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust matrix
completion and corrupted columns. In Proceedings of the 28th International Conference
on Machine Learning (ICML-11), pages 873–880, 2011.
YuxinChen,JianqingFan,CongMa,andKaizhengWang. Spectralmethodandregularized
MLE are both optimal for top-K ranking. Annals of Statistics, 47(4):2204–2235, 2019.
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Spectral Methods for Data Science:
A Statistical Perspective. Foundations and Trends in Machine Learning, 2021.
Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. A survey on network embedding. IEEE
Transactions on Knowledge and Data Engineering, 31(5):833–852, 2018.
Anirban Dasgupta, John Hopcroft, Jon Kleinberg, and Mark Sandler. On learning mix-
tures of heavy-tailed distributions. In 46th Annual IEEE Symposium on Foundations of
Computer Science (FOCS ’05), pages 491–500, 2005.
IliasDiakonikolasandDanielMKane. Algorithmic high-dimensional robust statistics. Cam-
bridge University Press, 2023.
Vaclav Dupac and Jaroslav Hajek. Asymptotic normality of simple linear rank statistics
under alternatives II. Annals of Mathematical Statistics, 40(6):1992–2017, 1969.
Andreas Elsener and Sara van de Geer. Robust low-rank matrix estimation. Annals of
Statistics, 46(6):3481–3509, 2018.
JianqingFan,WeichenWang,andYiqiaoZhong.Anℓ eigenvectorperturbationboundand
∞
its application to robust covariance estimation. Journal of Machine Learning Research,
18(207):1–42, 2018.
78Robust spectral clustering with rank statistics
Jianqing Fan, Yuan Ke, Qiang Sun, and Wen-Xin Zhou. FarmTest: Factor-adjusted ro-
bust multiple testing with approximate false discovery control. Journal of the American
Statistical Association, 114(528):1880–1893, 2019.
Jianqing Fan, Kaizheng Wang, Yiqiao Zhong, and Ziwei Zhu. Robust high-dimensional
factor models with applications to statistical machine learning. Statistical Science, 36(2):
303–327, 2021a.
Jianqing Fan, Weichen Wang, and Ziwei Zhu. A shrinkage principle for heavy-tailed data:
high-dimensional robust low-rank matrix recovery. Annals of Statistics, 49(3):1239–1266,
2021b.
Jianqing Fan, Yingying Fan, Xiao Han, and Jinchi Lv. SIMPLE: statistical inference on
membership profiles in large networks. Journal of the Royal Statistical Society Series B:
Statistical Methodology, 84(2):630–653, 2022.
IanGallagher, AndrewJones, AnnaBertiger, CareyEPriebe, andPatrickRubin-Delanchy.
Spectral embedding of weighted graphs. Journal of the American Statistical Association,
pages 1–10, 2023.
Jaroslav H´ajek. Asymptotic normality of simple linear rank statistics under alternatives.
Annals of Mathematical Statistics, 39(2):325–346, 1968.
PaulWHolland, KathrynBlackmondLaskey, andSamuelLeinhardt. Stochasticblockmod-
els: first steps. Social Networks, 5(2):109–137, 1983.
Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification, 2
(1):193–218, 1985.
Soham Jana, Kun Yang, and Sanjeev Kulkarni. Adversarially robust clustering with opti-
mality guarantees. arXiv preprint arXiv:2306.09977, 2023.
Soham Jana, Jianqing Fan, and Sanjeev Kulkarni. A general theory for robust clustering
via trimmed mean. arXiv preprint arXiv:2401.05574, 2024.
Jiashun Jin, Zheng Tracy Ke, and Shengming Luo. Improvements on score, especially for
weak signals. Sankhya A, 84(1):127–162, 2022.
Jiashun Jin, Zheng Tracy Ke, and Shengming Luo. Mixed membership estimation for social
networks. Journal of Econometrics, 239(2):105369, 2024.
Antony Joseph and Bin Yu. Impact of regularization on spectral clustering. Annals of
Statistics, 44(4):1765–1791, 2016.
Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in
networks. Physical Review E, 83(1):016107, 2011.
Gregory Kiar, Eric W Bridgeford, William R Gray Roncal, Vikram Chandrashekhar, Disa
Mhembere, Sephira Ryman, Xi-Nian Zuo, Daniel S Margulies, R Cameron Craddock,
Carey E Priebe, et al. A high-throughput pipeline identifies robust connectomes but
troublesome variability. bioRxiv, page 188706, 2018.
79Cape, Yu, and Liao
Ross M Lawrence, Eric W Bridgeford, Patrick E Myers, Ganesh C Arvapalli, Sandhya C
Ramachandran, Derek A Pisner, Paige F Frank, Allison D Lemmer, Aki Nikolaidis, and
Joshua T Vogelstein. Standardizing human brain parcellations. Scientific Data, 8(1):1–9,
2021.
Can M Le, Elizaveta Levina, and Roman Vershynin. Optimization via low-rank approxi-
mation for community detection in networks. Annals of Statistics, 44(1):373–400, 2016.
Jing Lei and Alessandro Rinaldo. Consistency of spectral clustering in stochastic block
models. Annals of Statistics, 43(1):215–237, 2015.
Lihua Lei. Unified ℓ eigenspace perturbation theory for symmetric random matrices.
2→∞
arXiv preprint arXiv:1909.04798, 2019.
Keith Levin, Carey E Priebe, and Vince Lyzinski. On the role of features in ver-
tex nomination: content and context together are better (sometimes). arXiv preprint
arXiv:2005.02151, 2020.
Gen Li, Changxiao Cai, Yuantao Gu, H Vincent Poor, and Yuxin Chen. Minimax estima-
tion of linear functions of eigenvectors in the face of small eigen-gaps. arXiv preprint
arXiv:2104.03298, 2021.
ZhenguoLi, JianzhuangLiu, ShifengChen, andXiaoouTang. Noiserobustspectralcluster-
ing. In 2007 IEEE 11th International Conference on Computer Vision, pages 1–8. IEEE,
2007.
FuchenLiu,DavidChoi,LuXie,andKathrynRoeder. Globalspectralclusteringindynamic
networks. Proceedings of the National Academy of Sciences, 115(5):927–932, 2018.
MatthiasL¨offler,AndersonYZhang,andHarrisonHZhou. Optimalityofspectralclustering
in the Gaussian mixture model. Annals of Statistics, 49(5):2506–2530, 2021.
Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. Estimating mixed member-
ships with sharp eigenvector deviations. Journal of the American Statistical Association,
116(536):1928–1940, 2021.
Frank McSherry. Spectral partitioning of random graphs. In Proceedings 42nd IEEE Sym-
posium on Foundations of Computer Science, pages 529–537, 2001.
Stanislav Minsker. Sub-gaussian estimators of the mean of a random matrix with heavy-
tailed entries. Annals of Statistics, 46(6A):2871–2903, 2018.
Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: analysis and
an algorithm. In Advances in Neural Information Processing Systems (NeurIPS), pages
849–856, 2002.
Majid Noroozi, Ramchandra Rimal, and Marianna Pensky. Estimation and clustering in
popularity adjusted block model. Journal of the Royal Statistical Society: Series B (Sta-
tistical Methodology), 83(2):293–317, 2021.
80Robust spectral clustering with rank statistics
Subhadeep Paul and Yuguo Chen. Spectral and matrix factorization methods for consistent
community detection in multi-layer networks. Annals of Statistics, 48(1):230–250, 2020.
Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and
sub-optimality of PCA I: spiked random matrix models. Annals of Statistics, 46(5):
2416–2451, 2018.
CareyEPriebe,YoungserPark,JoshuaTVogelstein,JohnMConroy,VinceLyzinski,Minh
Tang, Avanti Athreya, Joshua Cape, and Eric Bridgeford. On a two truths phenomenon
in spectral graph clustering. Proceedings of the National Academy of Sciences, 116(13):
5995–6000, 2019.
Karl Rohe, Sourav Chatterjee, and Bin Yu. Spectral clustering and the high-dimensional
stochastic blockmodel. Annals of Statistics, 39(4):1878–1915, 2011.
Karl Rohe, Tai Qin, and Bin Yu. Co-clustering directed graphs to discover asymmetries
and directional communities. Proceedings of the National Academy of Sciences, 113(45):
12679–12684, 2016.
Patrick Rubin-Delanchy, Joshua Cape, Minh Tang, and Carey E Priebe. A statistical
interpretationofspectralembedding: thegeneralisedrandomdotproductgraph. Journal
of the Royal Statistical Society, Series B, 84(4):1446–1473, 2022.
Purnamrita Sarkar and Peter J Bickel. Role of normalization in spectral clustering for
stochastic blockmodels. Annals of Statistics, 43(3):962–990, 2015.
Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.
Prateek R Srivastava, Purnamrita Sarkar, and Grani A Hanasusanto. A robust spectral
clustering algorithm for sub-gaussian mixture models with outliers. Operations Research,
71(1):224–244, 2023.
Gilbert W Stewart. Matrix perturbation theory. Academic Press, 1990.
MinhTangandCareyEPriebe. LimittheoremsforeigenvectorsofthenormalizedLaplacian
for random graphs. Annals of Statistics, 46(5):2360–2415, 2018.
Minh Tang, Avanti Athreya, Daniel L Sussman, Vince Lyzinski, Youngser Park, and
Carey E Priebe. A semiparametric two-sample hypothesis testing problem for random
graphs. Journal of Computational and Graphical Statistics, 26(2):344–354, 2017.
Minh Tang, Joshua Cape, and Carey E Priebe. Asymptotically efficient estimators for
stochastic blockmodels: the naive MLE, the rank-constrained MLE, and the spectral
estimator. Bernoulli, 28(2):1049–1073, 2022.
TerenceTao. Topics in random matrix theory,volume132. AmericanMathematicalSociety,
2012.
MarlosAGViana. Thecovariancestructureofrandompermutationmatrices. Contemporary
Mathematics, 287:303–326, 2001.
81Cape, Yu, and Liao
Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):
395–416, 2007.
Jiaming Xu. Rates of convergence of spectral methods for graphon estimation. In Interna-
tional Conference on Machine Learning, volume 80, pages 5433–5442, 2018.
Min Xu, Varun Jog, and Po-Ling Loh. Optimal rates for community estimation in the
weighted stochastic block model. Annals of Statistics, 48(1):183–204, 2020.
XinyangYi, DohyungPark, YudongChen, andConstantineCaramanis. Fastalgorithmsfor
robust PCA via gradient descent. Advances in Neural Information Processing Systems,
29, 2016.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust
distributed learning: towards optimal statistical rates. In International Conference on
Machine Learning, pages 5650–5659, 2018.
Anru R Zhang, T Tony Cai, and Yihong Wu. Heteroskedastic PCA: algorithm, optimality,
and applications. Annals of Statistics, 50(1):53–80, 2022a.
Hai Zhang, Xiao Guo, and Xiangyu Chang. Randomized spectral clustering in large-scale
stochastic block models. Journal of Computational and Graphical Statistics, 31(3):887–
906, 2022b.
Yuan Zhang, Elizaveta Levina, and Ji Zhu. Detecting overlapping communities in networks
using spectral methods. SIAM Journal on Mathematics of Data Science, 2(2):265–283,
2020.
Yiqiao Zhong and Nicolas Boumal. Near-optimal bounds for phase synchronization. SIAM
Journal on Optimization, 28(2):989–1016, 2018.
Mu Zhu and Ali Ghodsi. Automatic dimensionality selection from the scree plot via the use
of profile likelihood. Computational Statistics & Data Analysis, 51(2):918–930, 2006.
Xiatian Zhu, Chen Change Loy, and Shaogang Gong. Constructing robust affinity graphs
for spectral clustering. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1450–1457, 2014.
82