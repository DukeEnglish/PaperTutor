SpaRP: Fast 3D Object Reconstruction and
Pose Estimation from Sparse Views
Chao Xu1,2† , Ang Li3, Linghao Chen2,4†, Yulin Liu5,
Ruoxi Shi2,5†, Hao Su2,5‡ , and Minghua Liu2,5†‡
1 UCLA 2 Hillbot Inc. 3 Stanford University
4 Zhejiang University 5 UC San Diego
Fig.1: SpaRP handles open-world 3D reconstruction and pose estimation from un-
posed sparse-view images, delivering results in approximately 20 seconds.
Abstract. Open-world 3D generation has recently attracted consider-
ableattention.Whilemanysingle-image-to-3Dmethodshaveyieldedvi-
sually appealing outcomes, they often lack sufficient controllability and
tend to produce hallucinated regions that may not align with users’ ex-
pectations.Inthispaper,weexploreanimportantscenarioinwhichthe
inputconsistsofoneorafewunposed2Dimagesofasingleobject,with
little or no overlap. We propose a novel method, SpaRP, to reconstruct
a 3D textured mesh and estimate the relative camera poses for these
† This work is done while the author is an intern at Hillbot Inc. ‡ Equal advisory.
4202
guA
91
]VC.sc[
1v59101.8042:viXra2 Xu et al.
sparse-view images. SpaRP distills knowledge from 2D diffusion models
andfinetunesthemtoimplicitlydeducethe3Dspatialrelationshipsbe-
tweenthesparseviews.Thediffusionmodelistrainedtojointlypredict
surrogaterepresentationsforcameraposesandmulti-viewimagesofthe
object under known poses, integrating all information from the input
sparse views. These predictions are then leveraged to accomplish 3D re-
constructionandposeestimation,andthereconstructed3Dmodelcanbe
usedtofurtherrefinethecameraposesofinputviews.Throughextensive
experimentsonthreedatasets,wedemonstratethatourmethodnotonly
significantlyoutperformsbaselinemethodsintermsof3Dreconstruction
quality and pose prediction accuracy but also exhibits strong efficiency.
Itrequiresonlyabout20secondstoproduceatexturedmeshandcamera
poses for the input views. Project page: https://chaoxu.xyz/sparp.
Keywords: Sparse-View 3D Reconstruction · Pose Estimation · Open-
World Generation
1 Introduction
3D object reconstruction is a long-standing problem with applications span-
ning3Dcontentcreation,augmentedreality,virtualreality,androbotics,among
others. Although traditional photogrammetry [1,57,64] and recent neural field
methods [43,77,91] have made significant strides in reconstructing high-fidelity
geometryandappearance,theytypicallyrequiredenseviewinputs.However,in
many practical scenarios, such as in e-commerce and consumer capture situa-
tions,acquiringacomprehensivesetofhigh-resolutionimagesalongwithprecise
camera data is not always feasible.
Ontheotherendofthespectrum,thetasksofconvertingasingleimageto3D
andtextto3Dhaverecentlyseensubstantialprogress[28,30,32,46,62,80],thanks
to the rich priors embedded in 2D diffusion models [50,53,54] and pre-training
onextensive3Ddatasets[9].Thesemethodsmayachievehigh-qualitygeometry
and texture that matches the input view, but they also introduce ambiguities
in the regions not visible in the input image (such as the back view). Although
thesemethodsattempttohallucinatereasonableinterpretationsoftheseinvisible
areas, the generated regions may not always align with users’ expectations, and
users often lack sufficient control over these ambiguous regions.
In this paper, we explore a critical scenario where the input consists of one
or a few unposed 2D images of a single object. The images are captured from
arbitrarily distributed camera poses, often with little to no overlap. We tackle
boththe3Dreconstructionandposeestimationofinputimagesunderthissparse
viewsetting.Notethat,indenseviewsetting,traditionalStructure-from-Motion
(SfM) solvers (e.g., COLMAP [58]) are typically employed for pose estimation.
However,withsparseviewinputs,thesesolversoftenbecomeunreliableandtend
to fail due to insufficient overlapping visual cues. This issue is the main reason
why existing sparse view reconstruction methods [22,39,97] generally require
knowncameraposesasinput.Whilesomerecentmethodshaveattemptedpose-
free reconstruction and pose estimation for sparse views [17,18,29,63,95], they
are usually trained on a predefined small set of object categories and exhibit
poor generalization to unseen object categories.SpaRP 3
Inresponse,weproposeaninnovativeclass-agnosticapproachcalledSpaRP,
capable of processing arbitrary object categories with unposed sparse views.
Our inspiration comes from recent breakthroughs in open-domain single-image-
to-3D methods. They leverage 2D diffusion models (e.g., Stable Diffusion [53])
to generate novel viewpoints of an object [35], and even consistent multi-view
images from a single input image [28,36,38,60,62], by finetuning the diffusion
modelswithcorrespondingmulti-viewimagepairs.Thesediscoveriesimplythat
2D diffusion models harbor rich priors concerning 3D objects. Instead of merely
producing multi-view images, we contemplate leveraging 2D diffusion models to
examineasetofunposedinputimagesfromsparseviewpoints,infertheirspatial
interrelationships, and recover relative camera poses and underlying 3D shapes.
Specifically, we finetune a 2D diffusion model [53] to process sparse input
views by compositing them into a single image for conditioning. The diffusion
model is concurrently tuned to deduce the relative poses of the input images
and the underlying 3D objects. For the relative pose estimation branch, instead
of outputting camera poses as scalars, we task 2D diffusion models to produce
a surrogate representation: the NOCS maps [74] that embed pixel-wise corre-
spondencesacrossdifferentviewsandaremoresuitablefor2Ddiffusionmodels.
From these maps, we extract the relative camera poses for the sparse views us-
ingthetraditionalPnPalgorithm[2],assumingknowncameraintrinsics.Forthe
reconstruction branch, the diffusion model is tasked to produce multi-view im-
agesoftheobjectfromfixedknowncameraposes,coveringtheentire3Dobject.
This task requires the models to incorporate all information from input sparse
views and hallucinate invisible regions. We then feed the generated images with
fixed known poses into a pre-trained 3D reconstruction module [32] to create a
textured3Dmesh.Wecanfurtherrefinetheestimatedcameraposesbyaligning
the input views with the generated mesh through differentiable rendering [26].
We train SpaRP on the Objaverse [9] dataset with 1–6 unposed input views.
Unlike some previous methods that rely on costly per-shape optimization [83],
our method delivers 3D textured meshes along with camera poses in a much
more efficient manner, requiring only ∼16 seconds. As shown in Fig. 1, our ap-
proachcanfaithfullygenerate3Dassetsthatcloselyfollowthereferenceunposed
images,effectivelyovercomingtheambiguityissueofsingle-image-to-3D.Exten-
sive evaluation on three datasets demonstrates the superior performance of our
method over baselines in reconstructing 3D meshes with vivid appearance and
high-fidelity geometry, alongside precise pose estimation of the input images.
2 Related Work
2.1 Sparse-View 3D Reconstruction
Reconstructing 3D objects from sparse-view images is challenging due to the
lack of visual correspondence and clues. When a small baseline between im-
ages is assumed, several methods [4,19,24,37,39,51,52,70,76,79,89,93] have
pretrained generalizable models to infer surface positions by establishing pixel
correspondences and learning generalizable priors across scenes. However, these
methods often fail to produce satisfactory results when the sparse-view images4 Xu et al.
have a large baseline. Some studies have attempted to alleviate the dependence
on dense views by incorporating priors or adding regularization [16,22,45,61]
intotheNeRFoptimizationprocess.Othershaveemployed2Ddiffusionpriorsto
generatenovel-viewimagesasadditionalinputfortheNeRFmodel[3,21,68,97].
Forexample,ReconFusion[84]trainsaNeRFfromsparse-viewimagesandusesa
denoisingUNettoinfersomenovelviewimagesassupportfortheNeRFmodel.
EscherNet [23] utilizes Stable Diffusion for novel view synthesis and designs a
camera positional encoding module to yield more consistent images. Further-
more, some recent works [36,38,62] have integrated specialized loss functions
and additional modalities as inputs into NeRF-based per-scene optimization.
In contrast to these methods, our approach does not require camera poses
for the input sparse views. It is not limited to small baselines and is capable
of generating 360-degree meshes. Furthermore, without the need for per-shape
optimization,ourmethodcanquicklyproducebothtexturedmeshesandcamera
poses in about 20 seconds.
2.2 Pose-Free Reconstruction
Unlikethemethodsmentionedabove,whichassumeknowncameraposes,many
studies have aimed to solve the pose-free reconstruction challenge. When pro-
videdwithdenseimages,someapproaches [31,81,86]jointlyoptimizetheNeRF
representation along with camera parameters. However, due to the highly non-
convex nature of this optimization problem, such methods are susceptible to
initialposeguessesandcanbecometrappedinlocalminima.Thisissueworsens
wheninputimagesaresparse,withincreasingambiguityandreducedconstraint
availability. In response, numerous proposals have attempted to enhance op-
timization robustness. For example, SpaRF [71] uses dense image matches as
explicitoptimizationconstraints,whileFvOR[90]startswithcoarsepredictions
of camera poses and alternated updates between shape and pose.
In contrast to the optimization-based methods, there is a body of research
proposing generalizable solutions for this problem. VideoAE [25] infers scene
geometry from the first frame in a video series and estimates camera poses rel-
ative to that frame, which allows for warping scene geometry to decode new
viewpoints. SparsePose [63] first regresses and then iteratively refines camera
poses. FORGE [17] designs neural networks to infer initial camera poses, fuse
multi-view features, and decode spatial densities and colors. GRNN [72] offers
a GRU-based reconstruction method estimating the relative pose for each input
viewagainstaglobalfeaturevolume.TheRelPoseseries[29,95]useprobabilistic
modeling for relative rotation estimation between images. Other works [18,55]
eschew explicit camera pose estimations, instead employing transformers to en-
code input views into latent scene representations for novel view synthesis.
More recently, leveraging large vision models and diffusion models, which
have shown significant promise, new efforts have emerged for camera pose esti-
mation. PoseDiffusion [75] implements a diffusion model guided by 2D keypoint
matchestoestimateposes.PF-LRM[78]adaptstheLRMmodel[14]topredicta
pointcloudforeachinputimage,thenutilizesdifferentiablePnPforposeestima-
tion. iFusion [83] employs an optimization pipeline to assess relative elevationsSpaRP 5
and azimuths. It utilizes Zero123 [35] predictions as a basis and optimizes the
relativeposebetweentwoimagesbyminimizingthereconstructionlossbetween
the predicted and target images.
In contrast to these existing approaches, our proposal capitalizes on the ex-
tensive priors inherent in pre-trained 2D diffusion models, thereby providing
exceptional generalizability to handle a diverse range of open-world categories.
Ourmethodpredictscameraposesand3Dmeshgeometryinasinglefeedforward
pass, negating the need for per-shape optimization.
2.3 Open-World 3D Generation
Open-world single-image-to-3D and text-to-3D tasks have recently undergone
significant advancements. Recent 2D generative models [50,53,54] and vision-
languagemodels[48]havesuppliedvaluablepriorsaboutthe3Dworld,sparking
asurgeinresearchon3Dgeneration.Notably,modelssuchasDreamFusion[46],
Magic3D[30],andProlificDreamer[80]havepioneeredalineofapproachtoper-
shape optimization [5,6,10,15,27,40–42,44,47,49,59,66,67,73,87,88,94]. These
models optimize a 3D representation (e.g., NeRF) for each unique text or image
input, utilizing the 2D prior models for gradient guidance. Although they pro-
duceimpressiveresults,thesemethodsarehamperedbyprolongedoptimization
times, often extending to several hours, and “multi-face issue” problems.
Moreover, beyond optimization-based methods, exemplified by Zero123 [35],
numerous recent studies have investigated the employment of pre-trained 2D
diffusion models for synthesizing novel views from single images or text [36,38,
60,62,82,92]. They have introduced varied strategies to foster 3D-consistent
multi-view generation. The resulting multi-view images can then serve for 3D
reconstruction, utilizing either optimization-based methods [36,38,62] or feed-
forward models [28,32,34].
While most existing works focus on single-image-to-3D or text-to-3D, they
often hallucinate regions that are invisible in the input image, which provides
userswithlimitedcontroloverthoseareas.Inthispaper,weseektobroadenthe
inputtoencompassunposedsparseviewsandaddressboththe3Dreconstruction
and pose estimation challenges in a time-efficient way—within tens of seconds.
3 Method
Given n unposed input images {I | i = 1,...,n; 1 ≤ n ≤ 6}, which illustrate
i
a single object from arbitrary categories, we predict their relative camera poses
ξ and reconstruct the 3D model M of the object. As illustrated in Fig. 2,
ij
we first finetune a 2D diffusion model [53] to process the unposed sparse input
images (Sec. 3.1). The 2D diffusion model is responsible for jointly generating
grid images for both the NOCS maps of the input views, as well as multi-view
imageswithknowncameraposes.WeusethepredictedNOCSmapstoestimate
thecameraposesfortheinputviews(Sec.3.2).Theresultingmulti-viewimages
arefedintoatwo-stage3Ddiffusionmodelforacoarse-to-finegenerationofa3D
textured mesh (Sec. 3.3). This joint training strategy allows the two branches
to complement each other. It enhances the understanding of both the input6 Xu et al.
Fig.2: Pipeline Overview of SpaRP. We begin by taking a sparse set of unposed
images as input, which we tile into a single composite image. This composite image
is subsequently provided to the Stable Diffusion UNet to serve as the conditioning
input. The 2D diffusion model is simultaneously finetuned to predict NOCS maps for
the input sparse views and multi-view images under known camera poses. From the
NOCSmaps,weextractthecameraposescorrespondingtotheinputviews.Themulti-
view images are then processed by a reconstruction module to generate textured 3D
meshes.Optionally,thecameraposescanbe furtherrefinedusingthe generated mesh
for improved accuracy.
sparse views and the intrinsic properties of the 3D objects, thereby improving
theperformanceofbothposeestimation and3Dreconstruction.Optionally,the
generated3Dmeshcanalsobeusedtofurtherrefinethecameraposes(Sec.3.4).
3.1 Tiling Sparse View Images as Input Condition
Recently, numerous studies have shown that 2D diffusion models not only pos-
sess robust open-world capabilities but also learn rich 3D geometric priors. For
instance, Stable Diffusion [53], can be finetuned to include camera view con-
trol [28,35,36,38,60,62], enabling it to predict novel views of objects—a task
that necessitates significant 3D spatial reasoning. Consequently, we are inspired
to utilize the rich priors inherent in 2D diffusion models for the tasks of sparse
view 3D reconstruction and pose estimation.
Unlike most existing approaches that use a single RGB image as the condi-
tionandfocusonsynthesizingmulti-viewimages,ourgoalistotakeasparseset
of input images and stimulate Stable Diffusion to infer the spatial relationships
among the input views implicitly. To accomplish this, given 1 ∼ 6 sparse views
from arbitrary camera poses, we tile them into a 3×2 multi-view grid, as illus-
tratedinFig.3(c).Theimageinthefirstgridcelldeterminesacanonicalframe
(to be discussed later), while the order of the other views is inconsequential.
When there are fewer than 6 sparse views, we use empty padding for the re-
maining grid cells. This composite image then serves as the condition for Stable
Diffusion, which is expected to assimilate all information from the input sparse
views during the diffusion process.
We employ Stable Diffusion 2.1 as our base model. To adapt the original
text-conditioningtoourtiledmulti-viewimagecondition,wefollow[60]toapply
both local and global conditioning strategies. For local conditioning, we use the
reference-only attention mechanism [96], where we process the reference tiled
imagewiththedenoisingUNetmodelandappendtheattentionkeysandvaluesSpaRP 7
Fig.3: (a) Regardless of the poses of the sparse input views (in black), the output
multiviewsareuniformlydistributed(inred)andencompasstheentire3Dobject.(b)
The Normalized Object Coordinate Space (NOCS) of the object, whose orientation is
aligned with the azimuth of the first input view. (c) An example of input and output
tiled images. The elevation and azimuth of the first input view are denoted by θ and
0
ϕ , respectively. The camera poses of the output multiview images are determined by
0
ϕ .TheoutputNOCSmapscorrespondtotheinputsparseviews,andtheorientation
0
of the coordinate frame is also determined by ϕ .
0
from this image to corresponding layers in the denoising model for the target
images. This mechanism facilitates implicit yet effective interactions between
the diffusion model and the sparse views. For global conditioning, we integrate
the mean-pooled CLIP embedding of all input images—modulated by learnable
tokenweights—intothediffusionprocess,enhancingthemodel’sabilitytograsp
the overarching semantics and structure of the sparse views.
As depicted in Figs. 2 and 3, our objective is to concurrently generate grid
images for both NOCS maps of the input views and multi-view images from
known camera poses. To achieve this, we utilize a domain switcher [38] that
enables flexible toggling between the two domains. The switcher consists of two
learnable embeddings, one for each domain, which are then injected into the
UNet of the stable diffusion models by being added to its time embedding.
3.2 Image-to-NOCS Diffusion as a Pose Estimator
Conventional Structure-from-Motion (SfM) solvers, such as COLMAP [56], rely
onfeaturematchingforposeestimation.However,inscenarioswithsparseviews,
theremaybelittletonooverlapbetweeninputviews.Thelackofsufficientvisual
correspondence cues often renders the solvers unreliable and prone to failure.
Consequently, instead of relying on local correspondences, we leverage the rich
semantic priors embedded in 2D diffusion models for pose estimation.
One of the primary challenges is to enable 2D diffusion models to output
camera poses. While camera poses can be represented in various scalar formats
(e.g., 6-dimensional vector, four-by-four matrix, etc.), they are not native repre-
sentationsfora2Ddiffusionmodeltogenerate.Inspiredbyrecentworksdemon-
strating that 2D diffusion models can be used to predict normal maps [38]—a8 Xu et al.
domaindifferentfromnaturalimages—weproposeusingasurrogaterepresenta-
tion:theNormalizedObjectCoordinateSpace(NOCS)[74].WefinetuneStable
Diffusion to predict NOCS maps for each input view.
As depicted in Fig. 3(b), a NOCS frame is determined for each set of input
sparse view images and the underlying 3D object. Specifically, the 3D shape is
normalized into a unit cube, i.e., x,y,z ∈[0,1]. The shape’s upward axis aligns
with the dataset’s inherent upward axis of the 3D object, typically the gravity
axis. Predicting the object’s forward-facing direction may be ambiguous, so we
rotate the 3D shape in the NOCS frame to align its forward direction (zero
azimuth) with that of the first input view, thus unambiguously establishing
the NOCS frame. For each input view, we then render a NOCS map, where
each 2D pixel (r,g,b) represents the corresponding 3D point’s position (x,y,z) in
the defined NOCS frame, as shown in Fig. 3(c). These NOCS maps align with
the operational domain of 2D diffusion models, similar to the normal maps in
previous work [38].
To facilitate interactions between NOCS maps from different views and gen-
erate more 3D-consistent NOCS maps, we tile all NOCS maps into a 3×2 grid
image as the input condition (see Sec. 3.1), following the same tiling order and
the empty padding convention. We finetune Stable Diffusion to generate these
multi-view tiled NOCS maps, so the 2D diffusion model can attend to both the
input sparse views and their NOCS maps during the diffusion process.
After generating the NOCS maps for the input sparse views, we employ a
traditional Perspective-n-Point (PnP) solver [2] to compute the poses {ξpnp}
i
from the NOCS frame to the camera frames of each input view by minimizing
the reprojection error:
(cid:88)mi
ξpnp =arg min ∥p −proj(q ,ξ )∥2, (1)
i i,j i,j i 2
ξ ∈SE(3)
i j=1
where p represents the jth pixel’s location in the ith NOCS map; q is the
i,j i,j
corresponding 3D point location in the NOCS frame; m is the number of pix-
i
els for the ith view, and proj is the perspective projection operation. Note that
the PnP algorithm assumes known camera intrinsics and optimizes only for the
camera extrinsics. A RANSAC scheme is applied during the PnP computation
for outlier removal, enhancing the robustness of the pose prediction to bound-
ary noises and errors from the 2D diffusion model. As all NOCS maps share a
commonNOCSframe,wecanthusdeterminetherelativecameraposesbetween
views i and i′ through ξ−1ξ .
i i′
3.3 Multi-View Prediction for 3D Reconstruction
Wefollowtheparadigmofrecentsingle-image-to-3Dmethods[28,32]byinitially
generating multi-view images and subsequently using a feed-forward 3D recon-
struction module to convert these images into a 3D representation. It is note-
worthy that the input sparse views might not encompass the entire 3D objects,
nor provide adequate information for 3D reconstruction. Therefore, we propose
to predict multi-view images at uniformly distributed camera poses first, and
then use these predicted images for 3D reconstruction.SpaRP 9
Unlike traditional novel view synthesis [35], our approach employs a fixed
camera configuration for target multi-views. As depicted in Fig. 3, our target
multi-viewimagesconsistofsixviewswithalternating20◦ and−10◦ elevations,
and 60◦-spaced azimuths relative to the first input view. Although the eleva-
tion angles are set absolutely, the azimuth angles are relative to the azimuth of
thefirstinputsparseviewtoresolvetheambiguityinface-forwardingdirections.
Furthermore,Wemaintainconsistentcameraintrinsicsacrosstargetviews,inde-
pendentofinputviews.Thesestrategiesmitigatechallengesinpredictingcamera
intrinsics and elevation during the 3D reconstruction process. Existing methods
hindered by this issue may be sensitive to intrinsic variations and often depend
on predicting [34] or requiring user-specified [36,65,66] input image elevations.
SimilartoNOCSmapprediction,wetileallsixviewsintoa3×2gridimage
and finetune Stable Diffusion to generate this tiled image. The 2D diffusion
model,conditionedontheinputsparseviews,aimstoincorporateallinformation
from input views, deduce the underlying 3D objects, and predict the multi-view
imagesatthepredeterminedcameraposes.Althoughthepredictedposesofinput
sparse views (Sec. 3.2) are not directly employed in the 3D reconstruction, the
jointtrainingofNOCSpredictionandmulti-viewpredictionbranchesimplicitly
complement each other and boost the performance of both tasks.
Upon generating the multi-view images at known camera poses, we utilize
themulti-viewto3Dreconstructionmoduleproposedin[32]tolifttheseimages
to 3D. The reconstruction module adopts a two-stage coarse-to-fine approach,
which involves initially extracting the 2D features of the generated multi-view
images, aggregating them with the known camera poses, and constructing a 3D
cost volume. This 3D cost volume acts as the condition for the 3D diffusion
networks. In the coarse stage, a low-resolution 643 3D occupancy volume is pro-
duced. This is subsequently refined to yield a high-resolution 1283 SDF (Signed
DistanceField)volumewithcolors.Finally,atexturedmeshisderivedfromthe
SDF volume employing the marching cubes algorithm.
3.4 Pose Refinement with Reconstructed 3D Model
In Section 3.2, we finetune diffusion models for NOCS map prediction and cam-
era pose estimation. However, due to the hallucinatory and stochastic nature
of diffusion models, unavoidable errors may exist. The generated 3D mesh M,
though not perfect, provides a multi-view consistent and explicit 3D structure.
We can further refine the coarse poses predicted from the NOCS maps by lever-
aging the reconstructed 3D shape.
Pose Refinement via Differentiable Rendering.Startingwithinitialposes
{ξpnp} extracted from the predicted NOCS maps, we refine them through dif-
i
ferentiable rendering [26]. Specifically, we render the generated mesh M at op-
timizing camera poses ξ . We minimize the rendering loss between the rendered
i
image Ir = R(M,ξ ) and the input image I to obtain the optimally fitted
i i i
camera pose ξ∗. The optimization process can be formulated as:
i
ξ∗ =arg min (λ·L (Ir,I )+µ·L (Ir,I )), (2)
i mask i i rgb i i
ξ ∈SE(3)
i10 Xu et al.
Table 1: Evaluation Results for Pose Estimation.Wecompareourmethodwith
RelPose++ [29], FORGE [17], and iFusion [83] on three unseen datasets: OmniOb-
ject3D [85], GSO [12], and ABO [7]. 500 objects are sampled for each dataset.
Dataset Method Rot.Err↓Acc.@15◦↑Acc.@30◦↑Trans.Err↓Time↓
RelPose++ 103.24 0.011 0.033 4.84 3.6s
FORGE 111.40 0.004 0.020 4.21 440s
iFusion(ninit=1) 95.15 0.208 0.258 3.65 64s
GSO[12] iFusion(ninit=4) 8.61 0.651 0.759 0.49 256s
Ours(w/orefine) 13.02 0.537 0.616 0.58 10s
Ours(ninit=1) 9.87 0.563 0.617 0.42 27s
Ours(ninit=4) 5.28 0.750 0.787 0.23 57s
RelPose++ 105.05 0.008 0.046 7.38 3.6s
FORGE 99.27 0.014 0.063 7.27 440s
iFusion(ninit=1) 91.15 0.166 0.271 4.77 64s
OO3D[85]iFusion(ninit=4) 15.08 0.498 0.721 1.12 256s
Ours(w/orefine) 14.75 0.508 0.725 0.90 10s
Ours(ninit=1) 13.40 0.544 0.730 0.89 27s
Ours(ninit=4) 10.07 0.668 0.849 0.63 57s
RelPose++ 103.14 0.017 0.039 5.01 3.6s
FORGE 110.64 0.005 0.023 4.18 440s
iFusion(ninit=1) 96.65 0.186 0.219 3.88 64s
ABO[7] iFusion(ninit=4) 8.55 0.578 0.631 0.68 256s
Ours(w/orefine) 10.87 0.554 0.597 0.49 10s
Ours(ninit=1) 9.30 0.565 0.600 0.43 27s
Ours(ninit=4) 5.80 0.675 0.701 0.27 57s
whereL andL arethecross-entropyandMSElossescomputedforthe
mask rgb
foregroundmasksandtheRGBvalues,respectively,andλandµaretwoweight-
ing coefficients. The refinement process is lightweight and can be completed in
just one second, given the generated mesh.
MixtureofExperts(MoE).TheNOCSposepredictionsareinherentlystochas-
tic and may not produce an accurate pose in a single pass. For instance, with
objectspossessingcertainsymmetries,thediffusionmodelmaypredictonlyone
ofthepossiblesymmetricposes.WeemployaMixtureofExperts(MoE)strategy
to further refine the pose, which is simple but effective. Specifically, we generate
multiple NOCS maps for each input view using different seeds. We then select
the pose that minimizes the rendering loss based on the refinement results with
thegenerated3Dmesh.Thistechniqueeffectivelyreducesposeestimationerror,
as quantitatively validated by the ablation study in the Appendix.
4 Experiments
4.1 Evaluation Settings
Training Datasets and Details. We train our models on a curated subset
of 100k shapes from the Objaverse dataset [9]. Considering the variable qual-
ity of the original Objaverse dataset, we opted to filter out higher-quality data
by initially manually annotating 8,000 3D objects based on overall geometry
quality and texture preferences. Subsequently, we train MLP models for quality
rating classification and texture score regression, utilizing their multimodal fea-
tures [33]. Based on the predictions of these models, we select shapes that are
ratedashigh-qualityandhavetoptexturescores.Furtherdetailsaboutthedata
filtering process are included in the Appendix.SpaRP 11
Fig.4: Qualitative Results on 3D Reconstruction.Zero123XL[8],One2345[34],
and TripoSR [69] are single-image-to-3D methods, each utilizing only the first input
image. iFusion [83], EscherNet [23], and our approach take all input images (the first
row).Texturedmeshesandmeshnormalrenderingsareshown.Shapescomefromthe
OmniObject3D [85] and GSO [12] datasets.
For each 3D shape, we render 10 sets of images using BlenderProc [11].
Each set comprises 6 input images, 6 output multi-view images, and 6 NOCS
maps.Tomimicreal-worldconditionsandensuremodelrobustness,werandomly
sample camera intrinsics and extrinsics, as well as environment maps for the
inputimages.Fortheoutputmulti-viewimages,theirintrinsicsremainconstant,
while the extrinsics are derived from a fixed delta pose and the azimuth of the
inputimages.Eachsetofinputandoutputimagessharesthesameenvironment
map.Duringtraining,werandomlyselectedbetween1to6viewsassparseinput
views, with the first view of each set always being included. We train the model
utilizing 8 A100 GPUs for approximately 3 days.
Baselines. For 3D reconstruction, we compare our method with both state-of-
the-art single-image-to-3D and sparse-view-to-3D baselines. Single-view-to-3D
methods we evaluate include optimization-based approaches, such as Zero123
XL[8],SyncDreamer[36],andDreamGaussian[66],aswellasfeed-forwardmeth-
ods like One-2-3-45 [34] and Shap-E [20]. For sparse-view methods, we consider
tworecentopen-sourceapproachesasbaselines:iFusion[83]andEscherNet[23].12 Xu et al.
Table 2: Quantitative Comparison on 3D Reconstruction. Evaluated on the
complete GSO [12] dataset, which contains 1,030 3D objects. Five single-image-to-3D
methods [8,20,34,36,66] and two sparse-view methods [23,83] are compared.
ninput Method F-Score(%)↑ CLIP-Sim↑ PSNR↑ LPIPS↓ Time↓
Zero123XL[8] 91.6 73.1 18.16 0.136 20min
Shap-E[20] 91.8 73.1 18.96 0.140 27s
1 One-2-3-45[34] 90.4 70.8 19.07 0.133 45s
SyncDreamer[36] 84.8 68.9 16.86 0.145 6min
DreamGaussian[66] 81.0 68.4 17.88 0.147 2min
Ours 95.7 78.2 19.87 0.124 16s
iFusion[83] 88.5 66.7 16.2 0.151 28min
6 EscherNet[23] 94.8 65.9 16.6 0.139 9min
Ours 96.9 78.1 19.3 0.123 16s
Fig.5: Single-View vs. Sparse-View for 3D Reconstruction. We compare the
results of our method when using single-view and sparse-view inputs.
We utilize ThreeStudio [13]’s implementation for Zero123 XL and the official
implementations for the other baselines. Specifically, for iFusion, we use their
official reconstruction pipeline integrated with Zero123 XL.
For sparse-view pose estimation, we compare our method with state-of-the-
art approaches including RelPose++ [29], FORGE [17], and iFusion [83]. The
latter two are optimization-based while [29] is a feed-forward method.
Evaluation Datasets. For 3D reconstruction, we evaluate the methods on the
entire GSO [12] dataset, which comprises 1,030 3D shapes; none of these shapes
wereseenduringourtraining.Foreach3Dshape,werandomlyrendersixviews
as input images. For single-image-to-3D methods, a fixed-view image is taken
as input following [34]. We carefully align the predictions with the ground truth
meshes before calculating the metrics. Please refer to the Appendix for detailed
information on shape alignment and the evaluation metrics.
Forposeestimation,weevaluatetheapproachesonthreedatasets:OmniOb-
ject3D [85] and GSO [12], both captured from real scans, and ABO [7], a syn-
thetic dataset created by artists. For each dataset, we randomly choose 500
objects and render five random sparse views per shape. We follow iFusion [83]
toreporttherotationaccuracyandthemedianerrorinrotationandtranslation
across all image pairs. More details are provided in the Appendix.SpaRP 13
Table 3: Ablation Study on the Number of Input Views. Evaluated on the
GSO dataset [12].
nviews Rot.Err↓Acc.@5◦↑Trans.Err↓ F-Score(%)↑CLIP-Sim↑PSNR↑
1 – – – 89.1 74.9 17.7
2 8.56 0.32 0.42 93.3 76.5 18.3
4 6.03 0.43 0.28 96.0 77.6 19.0
6 5.28 0.48 0.25 96.9 78.1 19.3
Fig.6: Ablation Study on Pose Refinement. We showcase the input images,
predicted NOCS maps, and converted poses. The ground truth poses are in black,
whilethepredictedposesbeforeandafterrefinementareinblueandred,respectively.
4.2 Experiment Results
Pose Prediction. We report the pose estimation results in Tab. 1, where it is
evidentthatSpaRPoutperformsallbaselinemethodsbyasignificantmargin.It
is worth noting that RelPose++ [29] and FORGE [17] struggle to yield satisfac-
toryresultsforouropen-worldevaluationimages.iFusion,anoptimization-based
approach, is prone to becoming trapped in local minima. With only one initial
pose(n =1),italsofailstoproduceadequateresults.Incontrast,ourmethod
init
leverages priors from 2D diffusion models and can generate acceptable results
in a single forward pass. Even without any additional refinement (w/o refine),
ourmethodcanalreadyproduceresultssimilartoiFusionwithfourinitialposes
(n = 4), while being far more efficient, requiring just 1/25 of the runtime.
init
With the integration of further refinement through a mixture of experts, our
method achieves even better performance.
3D Reconstruction. We present the qualitative results in Fig. 4. With only
a single-view input, single-image-to-3D methods fail to produce meshes that
faithfully match the entire structure and details of the ground truth mesh. For
instance,mostsingle-viewbaselinemethodsareunabletoreconstructthestems
of the artichoke, the back of the firetruck, the red saddle on Yoshi, and the
two separate legs of Kirby standing on the ground. In contrast, sparse-view
methodsyieldresultsthataremuchclosertothegroundtruthbyincorporating
information from multiple sparse views. Compared to iFusion, EscherNet, our
method generates meshes with higher-quality geometry and textures that more14 Xu et al.
Table 4: Effect of Joint Training. Evaluated on 500 objects from GSO [12].
Method Rot.Err↓Acc.@15◦↑Acc.@30◦↑Trans.Err↓ F-Score(%)↑CLIP-Sim↑PSNR↑
Separate 9.87 0.563 0.617 0.42 96.81 78.36 19.03
Joint 8.57 0.601 0.677 0.37 97.12 78.90 19.42
accurately match the input sparse views. We report the quantitative results
in Tab. 2, where our method significantly outperforms both single-view-to-3D
and sparse-view approaches in terms of both 2D and 3D metrics. Moreover, our
methodexhibitssuperiorefficiency,beingmuchfasterthanthebaselinemethods.
4.3 Analysis
Single View vs. Sparse Views. In Fig. 5, we present the results obtained
by our method when provided with single-view and sparse-view inputs. With a
single-viewinput,ourmethodcanstillgeneratereasonableresults,yetitmaynot
accurately capture the structures and details of the regions that are not visible.
Ourmethoddemonstratesthecapabilitytoeffectivelyintegrateinformationfrom
all sparse-view inputs provided.
Number of Views. In Tab. 3, we quantitatively showcase the impact of the
number of views on both 3D reconstruction and pose estimation. We observe
that incorporating more input views enables the 2D diffusion network to better
grasptheirspatialrelationshipsandunderlying3Dobjects,boostingbothtasks.
Pose Refinement. While the predicted NOCS maps can be directly converted
intocameraposes,wehavefoundthattheseposescanbefurtherrefinedthrough
alignment with the generated 3D meshes. Fig. 6 showcases the predicted poses
beforeandafterrefinement.Althoughbotharegenerallyveryclosetotheground
truth poses, refinement can further reduce the error.
Number of Experts. We employ a mixture-of-experts strategy to address the
ambiguityissuesrelatedtoNOCSpredictionforsymmetricobjects.Byusingthis
strategy and increasing the number of experts, there is a substantial increase in
pose estimation accuracy. Please refer to the Appendix for more details and
quantitative ablation studies.
Joint Training. We finetune 2D diffusion models to jointly predict NOCS
maps and multi-view images from sparse, unposed views by leveraging a do-
main switcher. As shown in Tab. 4, this joint training strategy enables the two
branches to implicitly interact and complement each other, enhancing the inter-
pretation of both the input sparse views and the intrinsic properties of the 3D
objects, which in turn improves the performance of each task.
5 Conclusion
We present SpaRP, a novel method for 3D reconstruction and pose estimation
using unposed sparse-view images. Our method leverages rich priors embedded
in 2D diffusion models and exhibits strong open-world generalizability. Without
theneedforper-shapeoptimization,itcandeliverhigh-qualitytexturedmeshes,
along with accurate camera poses, in approximately 20 seconds.SpaRP 15
Acknowledgements: We thank Chong Zeng, Xinyue Wei for the discussion
and help with data processing, and Peng Wang for providing the evaluation set.
We also extend our thanks to all annotators for their meticulous annotations.
References
1. Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: A ran-
domized correspondence algorithm for structural image editing. ACM Trans.
Graph. 28(3), 24 (2009)
2. Bradski, G.: Perspective-n-point (pnp) pose computation (the opencv library).
https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html (2000)
3. Chan,E.R.,Nagano,K.,Chan,M.A.,Bergman,A.W.,Park,J.J.,Levy,A.,Aittala,
M.,DeMello,S.,Karras,T.,Wetzstein,G.:Genvs:Generativenovelviewsynthesis
with 3d-aware diffusion models (2023)
4. Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: Mvsnerf: Fast
generalizableradiancefieldreconstructionfrommulti-viewstereo.In:Proceedings
oftheIEEE/CVFInternationalConferenceonComputerVision.pp.14124–14133
(2021)
5. Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry
and appearance for high-quality text-to-3d content creation. arXiv preprint
arXiv:2303.13873 (2023)
6. Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint
arXiv:2309.16585 (2023)
7. Collins, J., Goel, S., Deng, K., Luthra, A., Xu, L., Gundogdu, E., Zhang, X.,
Vicente,T.F.Y.,Dideriksen,T.,Arora,H.,etal.:Abo:Datasetandbenchmarksfor
real-world3dobjectunderstanding.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 21126–21136 (2022)
8. Deitke, M., Liu, R., Wallingford, M., Ngo, H., Michel, O., Kusupati, A., Fan, A.,
Laforte, C., Voleti, V., Gadre, S.Y., et al.: Objaverse-xl: A universe of 10m+ 3d
objects. arXiv preprint arXiv:2307.05663 (2023)
9. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E.,
Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of
annotated3dobjects.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 13142–13153 (2023)
10. Deng,C., Jiang, C.,Qi, C.R., Yan, X., Zhou,Y., Guibas, L.,Anguelov, D.,et al.:
Nerdi: Single-view nerf synthesis with language-guided diffusion as general image
priors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 20637–20647 (2023)
11. Denninger, M., Sundermeyer, M., Winkelbauer, D., Zidan, Y., Olefir, D., El-
badrawy,M.,Lodhi,A.,Katam,H.:Blenderproc.arXivpreprintarXiv:1911.01911
(2019)
12. Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K.,
McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset
of3dscannedhouseholditems.In:2022InternationalConferenceonRoboticsand
Automation (ICRA). pp. 2553–2560. IEEE (2022)
13. Guo, Y.C., Liu, Y.T., Shao, R., Laforte, C., Voleti, V., Luo, G., Chen, C.H., Zou,
Z.X., Wang, C., Cao, Y.P., Zhang, S.H.: threestudio: A unified framework for
3d content generation. https://github.com/threestudio-project/threestudio
(2023)16 Xu et al.
14. Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K.,
Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv
preprint arXiv:2311.04400 (2023)
15. Jain,A.,Mildenhall,B.,Barron,J.T.,Abbeel,P.,Poole,B.:Zero-shottext-guided
objectgenerationwithdreamfields.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 867–876 (2022)
16. Jain, A., Tancik, M., Abbeel, P.: Putting nerf on a diet: Semantically consistent
few-shot view synthesis. In: Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision. pp. 5885–5894 (2021)
17. Jiang, H., Jiang, Z., Grauman, K., Zhu, Y.: Few-view object reconstruction with
unknown categories and camera poses. arXiv preprint arXiv:2212.04492 (2022)
18. Jiang,H.,Jiang,Z.,Zhao,Y.,Huang,Q.:Leap:Liberatesparse-view3dmodeling
from camera poses. arXiv preprint arXiv:2310.01410 (2023)
19. Johari,M.M.,Lepoittevin,Y.,Fleuret,F.:Geonerf:Generalizingnerfwithgeome-
trypriors.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 18365–18375 (2022)
20. Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv
preprint arXiv:2305.02463 (2023)
21. Karnewar, A., Vedaldi, A., Novotny, D., Mitra, N.J.: Holodiffusion: Training a 3d
diffusionmodelusing2dimages.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 18423–18433 (2023)
22. Kim,M.,Seo,S.,Han,B.:Infonerf:Rayentropyminimizationforfew-shotneural
volume rendering. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 12912–12921 (2022)
23. Kong,X.,Liu,S.,Lyu,X.,Taher,M.,Qi,X.,Davison,A.J.:Eschernet:Agenerative
model for scalable view synthesis. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 9503–9513 (2024)
24. Kulhánek, J., Derner, E., Sattler, T., Babuška, R.: Viewformer: Nerf-free neural
rendering from few images using transformers. In: European Conference on Com-
puter Vision. pp. 198–216. Springer (2022)
25. Lai, Z., Liu, S., Efros, A.A., Wang, X.: Video autoencoder: self-supervised disen-
tanglement of static 3d structure and motion. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 9730–9740 (2021)
26. Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., Aila, T.: Modular primi-
tivesforhigh-performancedifferentiablerendering.ACMTransactionsonGraphics
39(6) (2020)
27. Lee,H.H.,Chang,A.X.:Understandingpureclipguidanceforvoxelgridnerfmod-
els. arXiv preprint arXiv:2209.15172 (2022)
28. Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K.,
Shakhnarovich, G., Bi, S.: Instant3d: Fast text-to-3d with sparse-view generation
and large reconstruction model. arXiv preprint arXiv:2311.06214 (2023)
29. Lin,A., Zhang,J.Y., Ramanan,D., Tulsiani, S.:Relpose++:Recovering 6dposes
from sparse-view observations. arXiv preprint arXiv:2305.04926 (2023)
30. Lin,C.H.,Gao,J.,Tang,L.,Takikawa,T.,Zeng,X.,Huang,X.,Kreis,K.,Fidler,
S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 300–309 (2023)
31. Lin, C.H., Ma, W.C., Torralba, A., Lucey, S.: Barf: Bundle-adjusting neural radi-
ance fields. In: Proceedings of the IEEE/CVF International Conference on Com-
puter Vision. pp. 5741–5751 (2021)SpaRP 17
32. Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu,
J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-
view generation and 3d diffusion. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 10072–10083 (2024)
33. Liu, M., Shi, R., Kuang, K., Zhu, Y., Li, X., Han, S., Cai, H., Porikli, F., Su, H.:
Openshape:Scalingup3dshaperepresentationtowardsopen-worldunderstanding.
Advances in Neural Information Processing Systems 36 (2024)
34. Liu, M., Xu, C., Jin, H., Chen, L., Varma T, M., Xu, Z., Su, H.: One-2-3-45: Any
single image to 3d mesh in 45 seconds without per-shape optimization. Advances
in Neural Information Processing Systems 36 (2024)
35. Liu,R.,Wu,R.,VanHoorick,B.,Tokmakov,P.,Zakharov,S.,Vondrick,C.:Zero-
1-to-3:Zero-shotoneimageto3dobject.In:ProceedingsoftheIEEE/CVFInter-
national Conference on Computer Vision. pp. 9298–9309 (2023)
36. Liu,Y.,Lin,C.,Zeng,Z.,Long,X.,Liu,L.,Komura,T.,Wang,W.:Syncdreamer:
Generating multiview-consistent images from a single-view image. arXiv preprint
arXiv:2309.03453 (2023)
37. Liu, Y., Peng, S., Liu, L., Wang, Q., Wang, P., Theobalt, C., Zhou, X., Wang,
W.:Neuralraysforocclusion-awareimage-basedrendering.In:Proceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7824–
7833 (2022)
38. Long,X.,Guo,Y.C.,Lin,C.,Liu,Y.,Dou,Z.,Liu,L.,Ma,Y.,Zhang,S.H.,Haber-
mann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain
diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 9970–9980 (2024)
39. Long, X., Lin, C., Wang, P., Komura, T., Wang, W.: Sparseneus: Fast generaliz-
able neural surface reconstruction from sparse views. In: European Conference on
Computer Vision. pp. 210–227. Springer (2022)
40. Melas-Kyriazi,L.,Laina,I.,Rupprecht,C.,Vedaldi,A.:Realfusion:360degrecon-
struction of any object from a single image. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 8446–8455 (2023)
41. Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-nerf
for shape-guided generation of 3d shapes and textures. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12663–
12673 (2023)
42. Michel,O.,Bar-On,R.,Liu,R.,Benaim,S.,Hanocka,R.:Text2mesh:Text-driven
neural stylization for meshes. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 13492–13502 (2022)
43. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.Commu-
nications of the ACM 65(1), 99–106 (2021)
44. Mohammad Khalid, N., Xie, T., Belilovsky, E., Popa, T.: Clip-mesh: Generating
textured meshes from text using pretrained image-text models. In: SIGGRAPH
Asia 2022 conference papers. pp. 1–8 (2022)
45. Niemeyer,M.,Barron,J.T.,Mildenhall,B.,Sajjadi,M.S.,Geiger,A.,Radwan,N.:
Regnerf:Regularizingneuralradiancefieldsforviewsynthesisfromsparseinputs.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 5480–5490 (2022)
46. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using
2d diffusion. arXiv preprint arXiv:2209.14988 (2022)18 Xu et al.
47. Qian,G.,Mai,J.,Hamdi,A.,Ren,J.,Siarohin,A.,Li,B.,Lee,H.Y.,Skorokhodov,
I.,Wonka,P.,Tulyakov,S., etal.:Magic123: Oneimageto high-quality 3d object
generationusingboth2dand3ddiffusionpriors.arXivpreprintarXiv:2306.17843
(2023)
48. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
naturallanguagesupervision.In:Internationalconferenceonmachinelearning.pp.
8748–8763. PMLR (2021)
49. Raj, A., Kaza, S., Poole, B., Niemeyer, M., Ruiz, N., Mildenhall, B., Zada, S.,
Aberman, K., Rubinstein, M., Barron, J., et al.: Dreambooth3d: Subject-driven
text-to-3d generation. arXiv preprint arXiv:2303.13508 (2023)
50. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
Sutskever, I.: Zero-shot text-to-image generation. In: International Conference on
Machine Learning. pp. 8821–8831. PMLR (2021)
51. Rematas, K., Martin-Brualla, R., Ferrari, V.: Sharf: Shape-conditioned radiance
fields from a single view. arXiv preprint arXiv:2102.08860 (2021)
52. Ren,Y.,Zhang,T.,Pollefeys,M.,Süsstrunk,S.,Wang,F.:Volrecon:Volumeren-
deringofsignedraydistancefunctionsforgeneralizablemulti-viewreconstruction.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 16685–16695 (2023)
53. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 10684–10695 (2022)
54. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022)
55. Sajjadi, M.S., Meyer, H., Pot, E., Bergmann, U., Greff, K., Radwan, N., Vora,
S., Lučić, M., Duckworth, D., Dosovitskiy, A., et al.: Scene representation trans-
former: Geometry-free novel view synthesis through set-latent scene representa-
tions.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPat-
tern Recognition. pp. 6229–6238 (2022)
56. Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition.pp.4104–4113
(2016)
57. Schönberger, J.L., Zheng, E., Frahm, J.M., Pollefeys, M.: Pixelwise view selection
for unstructured multi-view stereo. In: Computer Vision–ECCV 2016: 14th Euro-
peanConference,Amsterdam,TheNetherlands,October11-14,2016,Proceedings,
Part III 14. pp. 501–518. Springer (2016)
58. Schönberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Conference
on Computer Vision and Pattern Recognition (CVPR) (2016)
59. Seo, J., Jang, W., Kwak, M.S., Ko, J., Kim, H., Kim, J., Kim, J.H., Lee, J., Kim,
S.: Let 2d diffusion model know 3d-consistency for robust text-to-3d generation.
arXiv preprint arXiv:2303.07937 (2023)
60. Shi,R.,Chen,H.,Zhang,Z.,Liu,M.,Xu,C.,Wei,X.,Chen,L.,Zeng,C.,Su,H.:
Zero123++: a single image to consistent multi-view diffusion base model. arXiv
preprint arXiv:2310.15110 (2023)
61. Shi,R.,Wei,X.,Wang,C.,Su,H.:Zerorf:Fastsparseview360{\deg}reconstruc-
tion with zero pretraining. arXiv preprint arXiv:2312.09249 (2023)
62. Shi,Y.,Wang,P.,Ye,J.,Long,M.,Li,K.,Yang,X.:Mvdream:Multi-viewdiffusion
for 3d generation. arXiv preprint arXiv:2308.16512 (2023)SpaRP 19
63. Sinha, S., Zhang, J.Y., Tagliasacchi, A., Gilitschenski, I., Lindell, D.B.: Sparse-
pose: Sparse-view camera pose regression and refinement. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.21349–
21359 (2023)
64. Stereopsis,R.M.:Accurate,dense,androbustmultiviewstereopsis.IEEETRANS-
ACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 32(8)
(2010)
65. Tang, J., Chen, Z., Chen, X., Wang, T., Zeng, G., Liu, Z.: Lgm: Large multi-
view gaussian model for high-resolution 3d content creation. arXiv preprint
arXiv:2402.05054 (2024)
66. Tang,J.,Ren,J.,Zhou,H.,Liu,Z.,Zeng,G.:Dreamgaussian:Generativegaussian
splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)
67. Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d:
High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint
arXiv:2303.14184 (2023)
68. Tewari, A., Yin, T., Cazenavette, G., Rezchikov, S., Tenenbaum, J.B., Durand,
F.,Freeman,W.T.,Sitzmann,V.:Diffusionwithforwardmodels:Solvingstochas-
tic inverse problems without direct supervision. arXiv preprint arXiv:2306.11719
(2023)
69. Tochilkin,D.,Pankratz,D.,Liu,Z.,Huang,Z.,Letts,A.,Li,Y.,Liang,D.,Laforte,
C., Jampani, V., Cao, Y.P.: Triposr: Fast 3d object reconstruction from a single
image (2024)
70. Trevithick, A., Yang, B.: Grf: Learning a general radiance field for 3d representa-
tionandrendering.In:ProceedingsoftheIEEE/CVFInternationalConferenceon
Computer Vision. pp. 15182–15192 (2021)
71. Truong,P.,Rakotosaona,M.J.,Manhardt,F.,Tombari,F.:Sparf:Neuralradiance
fields from sparse and noisy poses. In: CVF Conference on Computer Vision and
Pattern Recognition, CVPR. vol. 1 (2023)
72. Tung, H.Y.F., Cheng, R., Fragkiadaki, K.: Learning spatial common sense with
geometry-awarerecurrentnetworks.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 2595–2603 (2019)
73. Wang, H., Du, X., Li, J., Yeh, R.A., Shakhnarovich, G.: Score jacobian chaining:
Lifting pretrained 2d diffusion models for 3d generation. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12619–
12629 (2023)
74. Wang, H., Sridhar, S., Huang, J., Valentin, J., Song, S., Guibas, L.J.: Normal-
izedobjectcoordinatespaceforcategory-level6dobjectposeandsizeestimation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 2642–2651 (2019)
75. Wang, J., Rupprecht, C., Novotny, D.: Posediffusion: Solving pose estimation via
diffusion-aidedbundleadjustment.In:ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision. pp. 9773–9783 (2023)
76. Wang, P., Chen, X., Chen, T., Venugopalan, S., Wang, Z., et al.: Is attention all
nerf needs? arXiv preprint arXiv:2207.13298 (2022)
77. Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning
neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv
preprint arXiv:2106.10689 (2021)
78. Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z.,
Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape
prediction. arXiv preprint arXiv:2311.12024 (2023)20 Xu et al.
79. Wang,Q.,Wang,Z.,Genova,K.,Srinivasan,P.P.,Zhou,H.,Barron,J.T.,Martin-
Brualla,R.,Snavely,N.,Funkhouser,T.:Ibrnet:Learningmulti-viewimage-based
rendering.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 4690–4699 (2021)
80. Wang,Z.,Lu,C.,Wang,Y.,Bao,F.,Li,C.,Su,H.,Zhu,J.:Prolificdreamer:High-
fidelity and diverse text-to-3d generation with variational score distillation. arXiv
preprint arXiv:2305.16213 (2023)
81. Wang, Z., Wu, S., Xie, W., Chen, M., Prisacariu, V.A.: Nerf–: Neural radiance
fields without known camera parameters. arXiv preprint arXiv:2102.07064 (2021)
82. Weng, H., Yang, T., Wang, J., Li, Y., Zhang, T., Chen, C., Zhang, L.: Consis-
tent123: Improve consistency for one image to 3d object synthesis. arXiv preprint
arXiv:2310.08092 (2023)
83. Wu, C.H., Chen, Y.C., Solarte, B., Yuan, L., Sun, M.: ifusion: Inverting diffusion
for pose-free reconstruction from sparse views (2023)
84. Wu, R., Mildenhall, B., Henzler, P., Park, K., Gao, R., Watson, D., Srinivasan,
P.P., Verbin, D., Barron, J.T., Poole, B., et al.: Reconfusion: 3d reconstruction
with diffusion priors. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 21551–21561 (2024)
85. Wu, T., Zhang, J., Fu, X., Wang, Y., Ren, J., Pan, L., Wu, W., Yang, L., Wang,
J., Qian, C., et al.: Omniobject3d: Large-vocabulary 3d object dataset for realis-
tic perception, reconstruction and generation. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 803–814 (2023)
86. Xia, Y., Tang, H., Timofte, R., Van Gool, L.: Sinerf: Sinusoidal neural radi-
ance fields for joint pose estimation and scene reconstruction. arXiv preprint
arXiv:2210.04553 (2022)
87. Xu, D., Jiang, Y., Wang, P., Fan, Z., Wang, Y., Wang, Z.: Neurallift-360: Lifting
an in-the-wild 2d photo to a 3d object with 360deg views. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4479–
4489 (2023)
88. Xu, J., Wang, X., Cheng, W., Cao, Y.P., Shan, Y., Qie, X., Gao, S.: Dream3d:
Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion
models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 20908–20918 (2023)
89. Yang, H., Hong, L., Li, A., Hu, T., Li, Z., Lee, G.H., Wang, L.: Contranerf: Gen-
eralizable neural radiance fields for synthetic-to-real novel view synthesis via con-
trastive learning. In: Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition. pp. 16508–16517 (2023)
90. Yang, Z., Ren, Z., Bautista, M.A., Zhang, Z., Shan, Q., Huang, Q.: Fvor: Robust
jointshapeandposeoptimizationforfew-viewobjectreconstruction.In:Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 2497–2507 (2022)
91. Yariv, L., Gu, J., Kasten, Y., Lipman, Y.: Volume rendering of neural implicit
surfaces.AdvancesinNeuralInformationProcessingSystems34,4805–4815(2021)
92. Ye, J., Wang, P., Li, K., Shi, Y., Wang, H.: Consistent-1-to-3: Consistent im-
age to 3d view synthesis via geometry-aware diffusion models. arXiv preprint
arXiv:2310.03020 (2023)
93. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from
one or few images. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 4578–4587 (2021)SpaRP 21
94. Yu, C., Zhou, Q., Li, J., Zhang, Z., Wang, Z., Wang, F.: Points-to-3d: Bridging
thegapbetweensparsepointsandshape-controllabletext-to-3dgeneration.arXiv
preprint arXiv:2307.13908 (2023)
95. Zhang, J.Y., Ramanan, D., Tulsiani, S.: Relpose: Predicting probabilistic relative
rotation for single objects in the wild. In: European Conference on Computer
Vision. pp. 592–611. Springer (2022)
96. Zhang, L.: Reference-only control. https://github.com/Mikubill/sd-webui-
controlnet/discussions/1236 (2023)
97. Zhou, Z., Tulsiani, S.: Sparsefusion: Distilling view-conditioned diffusion for 3d
reconstruction.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition. pp. 12588–12597 (2023)22 Xu et al.
Fig.7: Real-World Examples: The input images are either sourced from ama-
zon.com or captured using an iPhone.
A Additional Real-World Examples
In Fig. 7, we demonstrate that SpaRP can be applied to real-world sparse-
viewimageswithoutcameraposes.Thisincludesimagescapturedbyuserswith
consumer devices (e.g., with an iPhone) or e-commerce product images (e.g.,
from amazon.com). SpaRP is capable of achieving commendable results in both
pose estimation and 3D reconstruction.
B Ablation Studies on Number of Experts
Duetotheinherentlystochasticnatureofdiffusionmodels,ourmulti-viewdiffu-
sionmodelmaysometimesfailtoaccuratelyunderstandthespatialrelationship
between input images of objects and estimate their relative poses in a singleSpaRP 23
Table 5: Ablation Study on the Number of Experts for Pose Estimation.
Evaluated on the OmniObject3D [85] and GSO [12] datasets.
n OmniObject3D [85] Google Scanned Objects [12]
init
experts
Rot. Err ↓Acc.@15◦ ↑Trans. Err↓ Rot. Err ↓Acc.@15◦ ↑Trans. Err↓
1 15.23 0.495 1.04 9.83 0.562 0.43
2 12.05 0.585 0.76 6.03 0.687 0.28
4 10.46 0.647 0.68 4.71 0.805 0.21
8 9.46 0.690 0.60 4.34 0.853 0.20
diffusion pass, especially with objects that have some symmetry. We found that
employing a Mixture of Experts (MoE) strategy effectively mitigates this issue.
Specifically, we run the diffusion models n times with different random seeds
init
to generate multiple sets of NOCS maps for pose prediction, selecting the opti-
mal one based on the minimum rendering loss from the pose refinement stage.
As shown in Tab. 5, increasing the number of experts (n ) from 1 to 8 led
init
to a significant improvement in the accuracy of relative pose predictions across
both the OmniObject3D [85] and GSO [12] datasets. This demonstrates that
the MoE strategy is simple yet effective in improving the robustness of our pose
prediction approach.
C Robustness to Varying Camera Intrinsics
Ourmulti-viewdiffusionmodeldemonstratesrobustperformanceacrossvarying
input image camera intrinsics. During its training, we randomize both the focal
length and optical center of input images. The input image field of view (FOV)
follows a normal distribution N(36◦,9◦), centered at 36 degrees. The optical
centeralsofollowsanormaldistributioncenteredattheimagecenter.Asshown
in Fig. 8, we tested the model’s performance across input FOVs ranging from
5 to 65 degrees, covering common photographic focal lengths. Using 20 different
objects, we calculated the average PSNR and LPIPS for predictions at various
FOVs.Ourmodeldemonstratedconsistentlyhighperformanceacrossthetested
range. This showcases its robustness to intrinsic variations in input images.
D Sparse-View Reconstruction using the Estimated
Poses
Our estimated poses can benefit numerous downstream applications, including
many existing sparse-view 3D reconstruction approaches that require camera
poses. Here, we demonstrate how our estimated poses can be utilized with Ze-
roRF [61], a sparse-view 3D reconstruction method. ZeroRF is an optimization-
basedmethodthatdoesnotrelyonpretrainedpriorsandrequirescameraposes24 Xu et al.
Fig.8: Our model achieves consistently high PSNR and low LPIPS across different
input-view FOVs, with no significant performance degradation due to focal length
variations.
as input. As depicted in Fig. 9, by using only five images along with the corre-
spondingpredictedposesasinput,ZeroRFiscapableofgeneratingaNeRFthat
synthesizesreasonablenovelviews.Theresultingmeshalsoshowscommendable
global geometry, considering the challenging nature of the task.
E Evaluation Details
E.1 3D Reconstruction
Toaccountforthescaleandposeambiguityofthegeneratedmesh,wealignthe
predicted mesh with the ground truth mesh prior to metric calculation. During
the alignment process, we sample 12 rotations (30◦ apart) as initial positions
and 10 scales from 0.6 to 1.4, which is dense enough in practice. We enumerate
thecombinationsoftheserotationsandscalesforinitializationandsubsequently
refinethealignmentwiththeIterativeClosestPoint(ICP)algorithm.Weselect
the alignment that yields the highest inlier ratio. Both the ground truth and
predicted meshes are then scaled to fit within a unit bounding box.
We adopt the evaluation metrics from [32] to assess the reconstruction qual-
ity from two perspectives: (1) geometric quality and (2) texture quality. For
geometricquality,weapplytheF-scoretoquantifythediscrepancybetweenthe
reconstructed and ground truth meshes, setting the F-score threshold at 0.05.
Toevaluatetexturequality,wecomputetheCLIP-Similarity,PSNR,andLPIPS
between images rendered from the reconstructed mesh and those of the ground
truth.Themeshesundergorenderingfrom24distinctviewpoints,encompassing
afull360-degreeviewaroundtheobject.Therenderedimageshavearesolution
of 512 × 512 pixels.
E.2 Pose Estimation
To evaluate pose estimation, we render five sparse views for each shape and as-
sess the relative poses between all ten pairs of views. We convert the predictedSpaRP 25
Input images Synthesized novel views Generated mesh
Fig.9: The camera poses predicted by our method can be utilized in ZeroRF [61],
which is an optimization-based sparse-view reconstruction method requiring camera
poses as input. The input images are sourced from the ABO dataset [7].
poses to the OpenCV convention and report the median rotation error, rotation
accuracy, and translation error across all pairs. The rotation error is the mini-
mum angular deviation between the predicted and the ground truth poses. In
contrast,thetranslationerroristheabsolutedifferencebetweenthecorrespond-
ing translation vectors. We present accuracies as the percentage of pose pairs
with rotation errors below the thresholds of 15◦ and 30◦. It should be noted
that iFusion [83] infers only the relative elevation, azimuth, and distance, and
cannot provide the 4x4 camera matrix without the absolute camera pose of the
reference image. For iFusion, we supplement the elevation angle of the reference
image using an external elevation estimation method [34], which has a median
prediction error of 5◦ on the GSO dataset. Additionally, many baseline meth-
ods do not require camera intrinsics as input, resulting in predicted poses with
varying distances from the camera to the shape, as reflected by the magnitude
of the translation vectors. To address this intrinsic ambiguity, we normalize the
predicted translation vectors for each method by using a scale factor that aligns
the first view’s predicted camera translation with the ground truth translation.
After normalization, we report the absolute translation errors. Furthermore, in
our ablation studies, we investigate the impact of the number of input views
and the number of experts on pose estimation performance using subsets of 100
shapes.
F Details of Dataset Curation
The Objaverse dataset [9] contains about 800,000 shapes. However, this dataset
includes numerous partial scans, scenes, and basic, textureless geometries that
are unsuitable for our task of generating single objects. To optimize the train-26 Xu et al.
ing process in terms of efficacy and efficiency, we curate a high-quality subset
consisting of single objects with high-fidelity geometry and vivid textural ap-
pearance. We begin by randomly selecting a subset of 3D models and then task
annotators with assessing the overall geometry quality and evaluating texture
aesthetic preferences. Subsequently, we train a simple network to predict such
annotations.
For assessing overall geometry quality, annotators are required to assign one
of three possible levels to each 3D model:
High quality:Objectsthatrepresentasingleentitywithaclearsemantic
meaning, such as avatars and animals.
Medium quality: Simple geometric shapes (e.g., cubes, spheres); geome-
tries that are abstract or have unclear semantic meaning; and repetitive
structures found in the Objaverse, such as skeletal frames of houses and
staircases.
Low quality: Point clouds; scenes with multiple elements; incomplete,
low-quality, or unidentifiable 3D scans.
Fortexturepreference,giventhedifficultyindefiningabsolutestandardsdue
toaestheticsubjectivity,weadoptabinarychoiceapproachforannotation.This
method presents annotators with pairs of 3D models, prompting them to select
the one with superior texture quality or visual appeal.
Overall,wehaverecruited10annotatorsandcollectedlabelsfor4,000pairsof
shapesintotal.Basedontheseannotations,wetrainedMLPnetworkstopredict
overall geometry quality ratings and texture scores, respectively. Both networks
take the multimodal features of each shape as input, which include image, text,
and 3D features, as encoded in OpenShape [33]. The rating classification MLP
predicts a one-hot encoded label across three levels, and is trained using the
cross-entropy loss. Meanwhile, the texture scoring MLP regresses a score for
each shape and is trained using a relative margin loss.
During the training of SpaRP, we utilized the trained MLPs to curate a
subsetofapproximately100,000objects.Theseobjectsareratedashigh-quality
and possess texture scores within the top 20%.