Instruction Finetuning for Leaderboard Generation
from Empirical AI Research
SalomonKabongo JenniferD’Souza
LeibnizUniversityofHannover TIB
Hannover,Germany Hannover,Germany
kabenamualu@l3s.de jennifer.dsouza@tib.eu
Abstract tractingleaderboardsfromempiricalAIresearch
publications in the form of (Task, Dataset, Met-
Thisstudydemonstratestheapplicationofin-
ric, Score) quadruples, or (T, D, M, S) hencefor-
struction finetuning of pretrained Large Lan-
ward (Hou et al., 2019). Leaderboards serve as
guageModels(LLMs)toautomatethegener-
ation of AI research leaderboards, extracting a critical tool for benchmarking and navigating
(Task,Dataset,Metric,Score)quadruplesfrom scientificprogress. Traditionalleaderboardshave
articles. It aims to streamline the dissemina- beencommunity-curated,exemplifiedbyplatforms
tion of advancements in AI research by tran- like PapersWithCode (PwC) or Open Research
sitioningfromtraditional,manualcommunity
KnowledgeGraph’sbenchmarksfeature. However,
curation, or otherwise taxonomy-constrained
textminingcanexpediteleaderboardconstruction,
natural language inference (NLI) models, to
capturingthe(T,D,M,S)quadrupleinformation
anautomated,generativeLLM-basedapproach.
buriedwithinthediscourseofscholarlyAIarticles. Utilizing the FLAN-T5 model, this research
enhancesLLMs’adaptabilityandreliabilityin Only two prior works, IBM-TDMS (Hou et al.,
informationextraction,offeringanovelmethod 2019) and AxCell (Kardas et al., 2020), have as-
forstructuredknowledgerepresentation. sessed automated text mining systems for the (T,
D, M, S) quadruple extraction task. IBM-TDMS
1 Introduction
achieved 7.5 micro F1 and 8.8 macro F1 scores,
Theburgeoningcomplexityandvolumeofscien- whileAxCellimproveduponthiswith25.8micro
tific literature (Fortunato et al., 2018; Bornmann F1 and 19.7 macro F1. These systems treated (T,
etal.,2021;AltbachandDeWit,2019)necessitate D, M, S) extraction as a Natural Language Infer-
sophisticated methods for distilling and structur- ence(NLI)task,reliantonapredefined(T,D,M)
ing vast amounts of data (Auer et al., 2020), par- taxonomy. The drawback of this approach is its
ticularly in fields like Artificial Intelligence (AI) inabilitytodetectnewlyintroduced(T,D,M)ele-
research. InstructionfinetuningofLargeLanguage mentsoutsidethetaxonomy,renderingthesystems
Models (LLMs) emerges as a pivotal innovation, impractical. In this work, we introduce a novel
addressingthisneedbyhoningmodels’abilitiesto objective: text generation within a given context,
preciselyinterpret(Weietal.,2021a)andexecute aimingtoovercometheselimitations. Furthermore,
specificinstructionsfortaskssuchasinformation thisworkadoptsinstructionfine-tuningtoaccom-
extraction. This precision is not just a technical plishSOTAasatextgenerationtask,andenhance
requirementbutatransformativeapproachtohow themodel’sadaptabilitytothedomain-specificnu-
modelsinteractwithandprocesstheunstructured ancesofAIresearch. SOTA,inourwork,aimsto
text, shifting the paradigm from broad, conversa- achievetwocoregoals: first,todetermineifanarti-
tionalresponsestotargeted,information-richout- clereportsaleaderboard,andsecond,toextractrel-
puts. Recentstudies(Luetal.,2023;Wangetal., evant(T,D,M,S)quadrupleswithinageneration
2023)underscoretheimportanceoffine-tuningin framework. Thisinnovativeapproachovercomes
guidingLLMstobetterunderstandandrespondto previous limitations of NLI systems, enabling us
nuanced task-specific directives, thereby enhanc- todetectnewlyintroduced(T,D,M)elementsand
ingtheirutilityacrossdiverseresearchandindustry renderingourapproachpracticallyfeasible. There-
applications. mainingresearchquestionweaddressinthiswork
At the heart of this study is the State-Of-The- isthechallengetomovetheneedleintermsofper-
Art (SOTA) task, an innovative venture into ex- formanceonSOTAsuchthatthesystemisindeed
1
4202
guA
91
]LC.sc[
1v14101.8042:viXrareliableinapracticalsetting. ning, 2011; QasemiZadeh and Schumann, 2016;
In this study, we harness the capabilities of Augensteinetal.,2017;Luanetal.,2018;D’Souza
the FLAN-T5 model (Chung et al., 2022), an and Auer, 2021; D’Souza and Auer, 2022), our
instruction-tuned variant from the T5 model task needs to analyze the entire paper addressing
class(Raffeletal.,2020),boasting780Mparame- document-levelIE.Relatedly,otherworksthatad-
tersandsourcedfromGoogle’sopen-accessrepos- dressdocument-levelIEviaextractionobjectives
itory on the Transformers library. There could that are similar to our (T, D, M, S) is the IBM-
have been one of two directions for this work: TDMSsystem(Houetal.,2019),AxCell(Kardas
scaling the models or instruction fine-tuning of a et al., 2020), SciREX (Jain et al., 2020) which
moderate-sizedLLM,i.e. withparametersinmil- addresses(Task,Dataset,Method,Metric)replac-
lionsversus1000xmoreinbillions. Wechosethe ingScorebyMethod,theORKG-TDM(Kabongo
latter. We believe that our choice makes model et al., 2023a,b) and SciNLPKG (Mondal et al.,
tuning more accessible within the research com- 2021)whichaddressonlythe(T,D,M)objective.
munity while empirically proving to be nonethe- While(Houetal.,2019)addressedthe(T,D,M,S)
less effective (experimental details in section 5). objective,theirexperimentaldatasetwasrelatively
For instruction-based finetuning, we use applica- smallandLLMswerenotthefocusoftheirexper-
bleinstructionsfromtheopen-sourcedinstruction iments. Nevertheless, they seminally introduced
generalizationeffortsintroducedasthe“Flan2022 theDocTAETcontextfeatureasashorter,focused
Collection”(Longpreetal.,2023). Ourapproach representationofthefullpaperinwhichthetask,
differsfromfinetuningapretrainedLMaswein- dataset, metric, and scores are most likely to be
stead finetune an instruction-tuned LM, enabling mentioned. TheTAETinDocTAETrepresentsthe
themodeltoeffectivelyfollowinstructionsithas Title,Abstracts,Experimentalsetup,andTablesin-
been trained on and adapt to a new domain and cludingcaptionsandheadersofthepaperextracted
task,withouttheneedtohandlevariabilityinlearn- withthehelpofcustomizedheuristic-basedextrac-
ingnewinstructionformats. Thismethodological tionparsersandsuppliedascontexttothemachine
choicenotonlyenhancesthemodel’sperformance learningmodel. Thiscontextrepresentationisalso
butalsopromotesreproducibilityandinnovationin usedinourwork. Notably,weemployLLMsfor
automatedinformationextractionandknowledge leaderboardconstructionandadoptanopen-world
representationwithinAIresearch. assumptionfortextgeneration,afirstinthiscon-
Summarily, our contributions include: 1) A text,movingawayfromtheclosed-worldmodels
novel methodological approach that employs reliantonafixed(T,D,M)taxonomy. Constrain-
“single-taskinstruction-finetuning”withtheFLAN- ing the (T, D, M) taxonomy via a closed-world
T5model,toenhanceitsdomainandtaskadapta- assumption does not reflect the real-world where
tion. Oursourcecodeisreleased. 2)Adeparture new tasks or datasets are constantly being intro-
from traditional NLI methods towards an LLM- duced. Thus the traditional reported NLI models
basedsystemthatutilizesmoderate-sizedmodels arenotgeneralizablecomparedtoourgeneration
for greater practical application viability. 3) The approach.
introductionofanewcorpusforexperimentalval-
idation, promoting standardized comparisons in 3 OurCorpus
future SOTA task research. 4) Demonstrated im-
provements in task performance, with our model Corpuswith(T,D,M,S)annotations. Wecreated
surpassingpreviousNLI-basedsystemsbynearly a new corpus as a collection of scholarly papers
10% in F1 scores, thereby validating the efficacy with their (T, D, M, S) quadruple annotations for
andfeasibilityofourapproach. evaluatingtheSOTAtask(Houetal.,2019). This
datasetisderivedfromthecommunity-curated(T,
2 RelatedWork D,M,S)annotationsforthousandsofAIarticles
available on PwC (CC BY-SA). Its articles span
At the heart of SOTA is a scientific information Natural Language Processing and Computer Vi-
extraction (IE) task. Different from most previ- sion domains, among other AI domains such as
ous work on IE from scientific literature which Robotics,Graphs,Reasoning,etc,thus,beingrep-
concentrates mainly on the titles or abstract sec- resentativeforempiricalAIresearch. Thespecific
tion or individual paragraphs (Gupta and Man- PwCsourcedownloadtimestampsisDecember09,
2OurCorpus PriorWork
Train Test-Zeroshot Train Testset
Papersw/leaderboards 7,987 241 170 167
Papersw/oleaderboards 4,401 548 - -
TotalTDM-triples 415,788 14,800 327 294
DistinctTDM-triples 11,998 1,267 78 78
DistinctTasks 1,374 236 18 18
DistinctDatasets 4,816 647 44 44
DistinctMetrics 2,876 412 31 31
Avg. no. ofTDMperpaper 5.12 6.11 2.64 2.41
Avg. no. ofTDMSperpaper 6.95 7.86 - -
Table1: OurCorpusvsPriorWork(Houetal.,2019)corporastatistics. The“papersw/oleaderboard”reffersto
papersthatdonotreportleaderboard.
2023. Assuchthecorpuscomprisedover7,500ar- andtestsets,respectively. Thesearticleswereran-
ticles. Corpuswith(T,D,M,S)annotations. We domly selected by leveraging the arxiv category
createdanewcorpusasacollectionofscholarlypa- feature,thenfilteringittopapersbelongingtodo-
perswiththeir(T,D,M,S)quadrupleannotations mains unrelated to AI/ML/Stats. These articles
for evaluating the SOTA task (Hou et al., 2019). wereannotatedwiththeunanswerablelabeltofine-
Thisdatasetisderivedfromthecommunity-curated tune our language model in recognizing papers
(T,D,M,S)annotationsforthousandsofAIarti- without(T,D,M,S)mentionsinthem.
cles available on PwC (CC BY-SA). Its articles OurfinalcorpusstatisticsarereportedinTable1.
spanNaturalLanguageProcessingandComputer Sinceinthiswork,themodelcomplexityandthe
Visiondomains,amongotherAIdomainssuchas timerequiredtofine-tunealanguagemodelisfar
Robotics,Graphs,Reasoning,etc,thus,beingrep- greaterthantheapproachesweusedinourprevious
resentativeforempiricalAIresearch. Thespecific work (Kabongo et al., 2023b), we only reported
PwC source download timestamps is December ourexperimentsbasedontheresultsfromFold1.
09,2023. Assuchthecorpuscomprisedover7,500 Furthermore,inthefirstmaincolumn,i.e. the“Our
articles. These articles, originally sourced from corpus”column,whencomparedwiththecorpus
arXivunderCC-BYlicenses,areavailableaslatex fromexistingworkby(Houetal.,2019),i.e. the
code source, each accompanied by one or more “Priorwork”column,ourcorpusshowsitselftobe
(T,D,M,S)annotationsfromPwC.Whilethere- significantlylargerthusshowingamorelarge-scale
spective articles’ metadata was directly obtained evaluationsetting.
fromthePwCdatarelease,thearticlescollection
TheSOTAtaskobjective. Wephrasedthefollow-
hadtobereconstructedbydownloadingthemfrom
ing question to formulate our task objective w.r.t.
arXiv under CC-BY licenses. Once downloaded,
the (T, D, M, S) extraction target: What are the
thearticlesbeingin.texformatneededtoundergo
valuesforthefollowingpropertiestoconstructa
pre-processing for tex-to-text conversion so that
Leaderboardforthemodelintroducedinthisarti-
theircontentscouldbemined. Forthis,thePandoc
cle: task,dataset,metric,andscore? Inessence,it
alongsideacustomscriptwasappliedtoextracttar-
encapsulatesanIEtask.
getedregionsofthepaperDocTEATwhichstands
Instructions for the LLM. LLMs progress
for DOCument, Title, Abstract, ExpSetup, and
throughinitialpretrainingandsubsequentfinetun-
TableInfo (Houetal.,2019). Eacharticle’sparsed
ingstages(Khashabietal.,2020;Xieetal.,2022;
text was then finally annotated with (T, D, M, S)
Wang et al., 2022; Sanh et al., 2022; Honovich
quadruplesviadistantlabeling.
etal.,2022;Longpreetal.,2023),buttheymight
Corpuswithnoleaderboards. Inadditiontoour stillstruggletointerpretinstructions. Thepractice
base dataset reported in Table 1, we additionally of instruction finetuning (Wei et al., 2021a) has
includedasetofapproximately4,401and548ar- surfacedasanessentialapproachforaugmenting
ticlesthatdonotreportleaderboardsintothetrain thecapabilityofLLMstointerpretandrespondto
3instructions (Lu et al., 2023; Wang et al., 2023). come in various sizes (Small 80M, Base 250M,
Assuchthechoiceoftheinstructionisalsocrucial Large780M,XL3B,andXXL11B).Thechoice
since it acts as a template that encodes the task oftheLargemodelstrikesabalancebetweenthe
anditsobjectives,instructingtheLLMonhowto Small and XXL models, offering an ample num-
achievethespecifiedobjective. ber of parameters for our intricate IE task while
The “Flan 2022 Collection” is an extensive, remaining practical for deployment. This deci-
open-sourcecompilationof62previouslyreleased sion stems from considerations of efficiency, as
NLP datasets, organized into 12 task types in- extensive-scaleLLMsweredeemedimpracticalfor
cluding reading comprehension, sentiment anal- a single task. Our choice of Flan-T5 was moti-
ysis,naturallanguageinference,andmore,making vated by prior empiricism (Longpre et al., 2023)
it a vital resource for developing generic multi- proving instruction-tuned models as more com-
task LLMs. Significantly, FLAN provided over putationallyefficientstartingcheckpointsfornew
10human-curatednaturalinstructionsperdataset, tasks–FLAN-T5requiredlessfinetuningtocon-
detailingthetasks,whichweutilizedtodirectour verge higher and faster than T5 on single down-
LLMforcomplexIEtasks. Wespecificallychose stream tasks (Longpre et al., 2023). Our model
instructionsfromtheSQuAD_v2(Rajpurkaretal., choice builds upon previous research, enhancing
2016,2018)andDROP(Duaetal.,2019)datasets, theT5text-to-textsequencegenerationmodel(Raf-
with 8 from SQuAD and 7 from DROP deemed feletal.,2020)withFLAN-T5(Chungetal.,2022)
appropriate. The general characteristic of the se- toimprovealignmentwithinstructionsinunseen
lectedinstructions,detailedinourappendixA,is tasksandzero-shotsettings. Ourresultingmodel
that they encode a context (in our case the Doc- iscalled SOTA-Flan-T5.
TAETrepresentationofanarticle)andtheSOTA
taskobjective,andinstructthemodeltofulfillthe 5 Evaluations
objective.
Experimental setup. For training, we had one
4 Approach mainexperimentalsettingbasedonthe15instruc-
tions. Aselicitedearlierinsection3,i.e. theCor-
Ourapproachexaminestheeffectivenessofsingle-
pussection,eachofthe15instructionwereinstan-
task instruction-finetuning on a novel task, i.e.
tiatedwiththe12,388(T,D,M,S)datainstances
the SOTA task, advancing the instruction-tuning
including both papers with leaderboard and w/o
paradigminitiallyproposedbyFLAN(Finetuned
leaderboards and the SOTA question resulting in
Language Net)(Wei et al., 2021b; Chung et al.,
atotalof185,820instancestoinstructionfinetune
2022; Longpre et al., 2023). Equipped with the
Flan-T5Large. Inthisscenario,wehypothesized
relevantsetof15totalinstructions(8SQuADand
thatthisrepetitioninthedatainstancesacrossthe
7 DROP), we needed to do two things: 1. For
instructions would cause the resulting model to
eachinstanceinthedataset, instantiatethe“Con-
overfitthetrainingdataset. Thustocontrolforthis,
text”placeholderintheinstructionswiththeDoc-
weappliedthefollowingexperimentalsetup. Each
TAETcontextfeatureofapaperandthe“Question”
instruction was instantiated with a random selec-
placeholderwithformulatedquestionfortheSOTA
tion of only half the total templates occurrences
objective. 2. The LLM could then be finetuned
of every data instances resulting in a finetuning
withtheinstruction-instantiatedtrainingset. From
datasetofasizeable92,910instances. Inthetest
Table 1, given our training dataset had approxi-
scenario,however,wereportperinstruction(T,D,
mately7,987(T,D,M,S)papersx15instructions
M,S)instantiateddataresults. AsshowninTable1,
x1SOTAobjectivequestion=119,805instruction-
for the test set with approximately 241 (T, D, M,
instantiateddatapointstotraintheLLM.Tothis,
S)and548paperswithandwithoutleaderboards
the4,401paperswithoutleaderboardsx15instruc-
respectively,evaluationsresultsareshownforeach
tions x 1 SOTA objective = 66,015 instruction-
instruction separately with a total of 789 under-
instantiateddatapointswereadded.
lying papersrepresenting thosewith and without
leaderboards. Modelhyperparamterdetailsarein
4.1 Model
AppendixB. Intermsofcompute,allexperiments
WeselecttheFLAN-T5XLmodel(Chungetal., includinginferencewererunonanNVIDIAh100
2022)fromitsrangeofpubliccheckpoints,which GPU.
4General General
Instruction Rouge1 Rouge2 RougeL RougeLsum -Accuracy Rouge1 Rouge2 RougeL RougeLsum -Accuracy Instruction
Drop1 73/62 11/8 73/62 73/62 96/91 73/62 11/8 73/62 73/62 96/91 Squad1
Drop2 73/62 11/8 73/62 73/62 96/91 72/62 11/8 72/63 72/62 95/91 Squad2
Drop3 73/62 11/8 73/62 73/62 96/92 73/62 11/8 73/62 73/62 96/91 Squad3
Drop4 73/62 11/8 73/62 73/62 96/91 73/62 11/8 73/62 73/62 96/91 Squad4
Drop5 73/61 11/8 72/62 72/61 96/91 73/62 11/8 73/62 73/62 96/91 Squad5
Drop6 73/62 11/8 73/62 73/62 96/91 73/62 11/8 73/62 73/62 96/91 Squad6
Drop7 73/61 11/8 73/61 73/61 96/90 73/63 11/8 73/63 73/63 96/92 Squad7
- - - - - - 73/62 11/8 73/62 73/62 96/91 Squad8
Table2: EvaluationresultsofSOTA-Flan-T5Largewithoutputevaluationsasastructuredsummarygeneration
task(reportedwithROUGEmetrics)aswellasbinaryclassificationbetweenpaperswithandwithoutleaderboards
(reportedasGeneralAccuracy)foreachofthe15instructionsfromDROPandSQuADdatasetsvsw/otemplates
instruction,respectively.
DropInstructions SQuADv2Instructions
Task Dataset Metric Score Overall Task Dataset Metric Score Overall
Exact 36/14 12/08 24/12 0.2/0.1 18/08 37/14 13/08 24/12 0.1/0.2 18/09
D1 S1
Partial 55/28 22/17 36/18 0.2/0.4 28/16 55/29 23/18 37/17 0.1/0.3 29/16
Exact 36/14 12/08 23/12 0.1/0.2 18/09 35/14 12/08 22/13 0.1/0.2 17/09
D2 S2
Partial 55/29 23/18 36/17 0.1/0.3 28/16 54/29 21/18 35/18 0.1/0.4 27/16
Exact 36/14 12/08 23/12 0.1/0.2 18/09 36/14 12/08 23/12 0.1/0.2 18/09
D3 S3
Partial 55/29 23/18 36/17 0.1/0.3 29/16 37/29 12/18 23/17 0.1/0.5 18/16
Exact 36/14 13/08 23/12 0.1/0.2 18/09 37/14 12/08 23/12 0.1/0.2 18/09
D4 S4
Partial 55/29 23/18 36/18 0.1/0.5 29/16 55/29 23/18 36/17 0.1/0.5 29/16
Exact 36/14 13/08 25/12 0.1/0.2 18/08 37/14 12/08 23/12 0.1/0.2 18/09
D5 S5
Partial 56/29 22/17 37/18 0.1/0.3 29/16 55/28 23/18 36/17 0.1/0.5 29/16
Exact 36/14 12/08 23/12 0.1/0.2 18/09 37/14 12/08 23/12 0.1/0.2 18/09
D6 S6
Partial 55/29 23/18 36/18 0.1/0.5 29/16 55/29 23/17 36/17 0.1/0.5 29/16
Exact 36/14 13/8 24/12 0.1/0.2 18/09 35/14 12/08 23/12 0.1/0.2 18/09
D7 S7
Partial 56/28 22/17 36/17 0.1/0.5 29/16 56/29 22/18 35/18 0.1/0.3 28/16
Exact - - - - - 35/14 12/08 23/12 0.1/0.2 18/09
- S8
Partial - - - - - 55/29 22/18 35/18 0.1/0.5 28/16
Table3: EvaluationresultsofSOTA-Flan-T5Largew.r.t. theindividual(Task,Dataset,Metric,Score)elementsand
OverallinthemodelJSONgeneratedoutputintermsofF1scoreforeachofthe15instructionsfromDROPand
SQuADdatasetsvsw/otemplatesinstructionrespectively.
Metrics. WeevaluatedtheSOTA-Flan-T5model 5.1 ResultsandDiscussion
intwosettings. Inthefirstsetting,wetreatedthe
Structuredsummarygenerationevaluations. Ta-
SOTAtaskobjectiveasastructuredsummarization
ble2resultsshowmodel’scapacityingenerating
task. In thissetting, we applied standardsumma-
structuredsummariespertheSOTAobjective. The
rization ROUGE metrics (Lin, 2004) (details in
results obtained were consistent across all 15 in-
AppendixC). Furthermore,wealsotestedthemod-
structionswhichindicatesthatthemodelsystemat-
elsabilitytoidentifypaperswithleaderboardsand
icallyfollowsallinstructionsandhandlesthemall
thosewithout. Thistaskwassimple. Forthepapers
inmoreorlessthesameway. Notably,thegeneral
withleaderboards,themodelrepliedwithastruc-
accuracy,i.e. theabilityofthemodeltodiscrimi-
turedsummaryandforthoseitidentifiedaswithout
natebetweenpaperswithleaderboardsandthose
itrepliedas“unanswerable.” Fortheseevaluations
withoutisnearlyperfectat95%indicatingacore
weappliedsimpleaccuracymeasure. Inthesecond
strengthofthemodel.
setting, we evaluated the model JSON output in
a fine-grained manner w.r.t. each of the individ- TheROUGEmetrics,whichmeasuretheover-
ual (T, D, M, S) elements and overall for which lapbetweenthemodel’soutputandreferencesum-
wereportedtheresultsintermsofthestandardF1 maries,haveimprovedbyapproximately10points
score. forROUGE1and3pointsforROUGE2whencom-
paring the instruction-conditioned model to the
baseline shown in Table 3. The improvement is
5indicativeofthemodel’senhancedabilitytogener- scoresaswellasTask,Dataset,MetricandScore
atesummariesthatarenotonlymorealignedwith element-wise F1 Score when the model is condi-
human judgments but also more informative and tionedwithFLAN-T5instructions.
concise.
SOTAobjective(Task,Dataset,Metric,Score) 6 ErrorAnalysis
element-wisegenerationevaluations. Nextweex-
Inthissection,weperformtheerroranalysisofour
aminetheresultsreportedinTable3. Specifically,
finetuned SOTA-Flan-T5model.
weexaminehowwellthefinetunedSOTA-Flan-T5
Type1-MissinginformationThemostprominent
modelperformswhenevaluatedtopreciselyextract
causeoferrorintheleaderboardgenerationisthe
eachofthe SOTA objectiveelementsi.e. theTask,
needfortheappropriateentitiesofinterestinthe
Dataset,Metric,andScoreinaresponseproduced
providedcontext,whichreferstoourDOCTEAT
asoneormorerelatedquadruplesperpaper. These
inthiscontext. FLAN-T5familyofmodelssuffers
resultsarereportedintermsofF1scoresinanex-
fromthesamelimitationof512maxtokenlength
actmatchandpartialmatchsettingsofthemodel
causedbythequadraticnatureoftheunderlyingat-
output. ConsistentwiththeresultsinTable2,the
tentionmechanism. Similarlyto(Houetal.,2019),
modelrespondsconsistentlyacrosstheDROPand
we obtained a summarized version of the paper,
SQuADinstructiontypes. Understandablythere-
called DOCTEAT which stands for DOCument,
sultsintheexactmatchsettingareatleast10points
Title,Abstract,ExpSetup,andTableInfo. Weno-
lowerthantheresultsinthepartialmatchsetting.
ticedthat,theabstractedrepresentationdoesn’tusu-
We see that across all four elements, the Task is
ally contain the score numeric value associated
easiesttoextractat∼36%exact-matchevaluations
with the dataset and metric reported in a paper.
and∼56%partial-matchevaluations. TheMetric
ThusmakingtheLanguageModellearntogener-
elementwasshowntobesecondeasiesttoextractat
ateavaluethatisnotavailableinthecontext.
∼25%exact-matchand∼37%partial-matchevalu-
Type2-Crowdsourcedlabeldiscrepancies Dis-
ationsfollowedtheDatasetelementat∼13%exact-
crepancies between all the Tasks, Datasets, Met-
match and ∼23% partial-match evaluations. The
rics, and Scores reported in a particular paper vs
modelfailedinextractingtheScoreelementindi-
themetadataavailablefrompaperswithcodedata
catingthatanalternatestrategyiswarrantedhere.
dumpisacauseofconfusionintheLLMtraining.
Conclusively,westartedoutwiththeresearchques-
We noticed instances of papers with code leader-
tionthatexaminedwhetherSOTAaddressedina
boardmentionsunrelatedtothepaper’smaincontri-
task generation objective would work at all and
bution,andcasesofmentionscompletelyunrelated
whethertheresultingLLMwouldbeeffective? Ex-
tothepaper. Wenoticedthat,thelanguagemodel
aminingtheresultsinthe“Overall”columnwesee
tendtolearnthegrammaticalstructureleadingto
ourapproachiscompetitivewiththepriorstate-of-
the mention of these entities in the DOCTEAT,
the-art,i.e. theAxCellsystem(Kardasetal.,2020).
butstruggletolearntheappropriaterepresentation
Additionally,apointtonotehereisthatourlabels
caused by the misalignment between the leader-
are all unnormalized and obtained directly from
boardentitiescapturedbypaperswithcodecom-
thecommunity-curatedPwClabelswhichcanac-
paredtothegroundtruthleaderboardaddressedin
countinpartforlowerscoresbyourapproachand
thepaper. Thus,anextrahumanvalidationofthe
ourzeroshottestsetcontainsatleastaleaderboard
datasetcuratedthroughPWCbecomesnecessary
that was not seen at training time. In this case,
forfutureexperiments.
ourannotatedtestdatasetwithdistantlysupervised
(Task,Dataset,Metric,Score)annotationsversus
7 ConclusionsandFutureWork
ourLLMpredictionscanbebrowsedherehttps:
//scinext-project.github.io/#/sota. In this paper, we have demonstrated how LLMs
TheincorporationoftheFLAN-T5instruction can be leveraged for the automated construction
collection into our model’s training regimen has of leaderboards from empirical AI research pa-
demonstrably enhanced its performance across pers modeled as the SOTA objective. As such,
both structured summarization and SOTA Objec- wespecificallyinvestigatedinstructionfinetuning
tive tasks. This effect is quantitatively evident in oftheFLAN-T5model. Ourexperimentalresults
theresultspresentedinTable2andTable3,which showed that the finetuned SOTA-Flan-T5 model
showcases a consistent improvement in ROUGE waseffectiveforthetask. Thisinturnimpactsfu-
6turedirectionsforthetaskfromanNLIparadigm onSemanticEvaluation(SemEval-2017),pages546–
aptlysituatingitintheareaofLLMresearchasa 555,Vancouver,Canada.AssociationforComputa-
tionalLinguistics.
textgenerationparadigminstead.
LutzBornmann,RobinHaunschild,andRüdigerMutz.
8 Limitations 2021. Growth rates of modern science: a latent
piecewise growth curve approach to model publi-
Our approach depends heavily on the quality of cationnumbersfromestablishedandnewliterature
databases. HumanitiesandSocialSciencesCommu-
dataprocessingandtheinherentlimitationsofthe
nications,8(1):1–15.
toolsemployed,suchasPandoc,forconvertingLa-
TeXdocumentstoplaintext. Errorsintroduceddur- Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
ingthisconversioncansignificantlyaffecttheex-
Wang,MostafaDehghani,SiddharthaBrahma,etal.
tractionaccuracyof(Task,Dataset,Metric,Score)
2022. Scalinginstruction-finetunedlanguagemodels.
quadruples. Additionally, our model’s general- arXivpreprintarXiv:2210.11416.
izability across various domains of academic re-
JenniferD’SouzaandSoerenAuer.2021. Pattern-based
searchbeyondcomputerscienceisnotyetverified.
acquisitionofscientificentitiesfromscholarlyarticle
Thedistinctformatsandterminologiesprevalentin titles. arXivpreprintarXiv:2109.00199.
differentdisciplinesmayposeachallenge,andas
DheeruDua,YizhongWang,PradeepDasigi,Gabriel
such,themodel’sapplicabilityacrossthesevaried
Stanovsky,SameerSingh,andMattGardner.2019.
fieldsremainsatopicforfutureresearch. Drop: Areadingcomprehensionbenchmarkrequir-
ingdiscretereasoningoverparagraphs. InProceed-
ingsofthe2019ConferenceoftheNorthAmerican
9 EthicsStatement
Chapter of the Association for Computational Lin-
guistics: HumanLanguageTechnologies,Volume1
Thedatasetsusedinthisstudyweresourcedfrom
(LongandShortPapers),pages2368–2378.
thearXivrepository,adheringtoopenaccesspoli-
cies. Despitethis,theautomatednatureofourinfor- Jennifer D’Souza and Sören Auer. 2022. Computer
sciencenamedentityrecognitionintheopenresearch
mationextractionposesethicalconsiderations,pri-
knowledge graph. In International Conference on
marilyduetopotentialmisinterpretationsorover- AsianDigitalLibraries,pages35–45.Springer.
simplificationsofnuancedacademiccontent. The
Santo Fortunato, Carl T Bergstrom, Katy Börner,
potentialofpropagationerrorsfromsourcemateri-
James A Evans, Dirk Helbing, Staša Milojevic´,
alstofinaloutputsduetopreprocessingtoolsunder-
Alexander M Petersen, Filippo Radicchi, Roberta
scorestheneedforclearcommunicationregarding Sinatra,BrianUzzi,etal.2018. Scienceofscience.
theselimitationstousersofoursystem. Thisiscru- Science,359(6379):eaao0185.
cialtoensurethattheinformationprovidedthrough
Sonal Gupta and Christopher Manning. 2011. Ana-
thegeneratedleaderboardsaccuratelyreflectsthe lyzing the dynamics of research by extracting key
advancements in AI research without misleading aspects of scientific papers. In Proceedings of 5th
InternationalJointConferenceonNaturalLanguage
theacademiccommunityorthepublic.
Processing,pages1–9,ChiangMai,Thailand.Asian
FederationofNaturalLanguageProcessing.
References OrHonovich,ThomasScialom,OmerLevy,andTimo
Schick. 2022. Unnatural instructions: Tuning lan-
PhilipGAltbachandHansDeWit.2019. Toomuch guagemodelswith(almost)nohumanlabor. arXiv
academicresearchisbeingpublished. International preprintarXiv:2212.09689.
HigherEducation,(96):2–3.
YufangHou,CharlesJochim,MartinGleize,Francesca
Bonin, and Debasis Ganguly. 2019. Identification
SörenAuer,AllardOelen,MuhammadHaris,Markus
of tasks, datasets, evaluation metrics, and numeric
Stocker, Jennifer D’Souza, Kheir Eddine Farfar,
scores for scientific leaderboards construction. In
Lars Vogt, Manuel Prinz, Vitalis Wiens, and Mo-
Proceedings of the57th Annual Meeting of the As-
hamad Yaser Jaradeh. 2020. Improving access to
sociation for Computational Linguistics, Florence,
scientificliteraturewithknowledgegraphs. Biblio-
Italy.AssociationforComputationalLinguistics.
thekForschungundPraxis,44(3):516–529.
Sarthak Jain, Madeleine van Zuylen, Hannaneh Ha-
Isabelle Augenstein, Mrinal Das, Sebastian Riedel, jishirzi,andIzBeltagy.2020. Scirex: Achallenge
LakshmiVikraman, andAndrewMcCallum.2017. datasetfordocument-levelinformationextraction. In
SemEval 2017 task 10: ScienceIE - extracting Proceedingsofthe58thAnnualMeetingoftheAsso-
keyphrasesandrelationsfromscientificpublications. ciationforComputationalLinguistics,pages7506–
InProceedingsofthe11thInternationalWorkshop 7516.
7SalomonKabongo,JenniferD’Souza,andSörenAuer. oftransferlearningwithaunifiedtext-to-texttrans-
2023a. Orkg-leaderboards: Asystematicworkflow former. TheJournalofMachineLearningResearch,
forminingleaderboardsasaknowledgegraph. arXiv 21(1):5485–5551.
preprintarXiv:2305.11068.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
SalomonKabongo,JenniferD’Souza,andSörenAuer.
Know what you don’t know: Unanswerable ques-
2023b. Zero-shotentailmentofleaderboardsforem-
tionsforsquad. InProceedingsofthe56thAnnual
piricalairesearch. InProceedingsoftheACM/IEEE
Meeting of the Association for Computational Lin-
JointConferenceonDigitalLibrariesin2023.
guistics(Volume2: ShortPapers),pages784–789.
MarcinKardas,PiotrCzapla,PontusStenetorp,Sebas-
PranavRajpurkar,JianZhang,KonstantinLopyrev,and
tianRuder,SebastianRiedel,RossTaylor,andRobert
PercyLiang.2016. Squad: 100,000+questionsfor
Stojnic. 2020. Axcell: Automatic extraction of re-
machinecomprehensionoftext. InProceedingsof
sultsfrommachinelearningpapers. InProceedings
the2016ConferenceonEmpiricalMethodsinNatu-
of the 2020 Conference on Empirical Methods in
ralLanguageProcessing,pages2383–2392.
NaturalLanguageProcessing(EMNLP),pages8580–
8594.
VictorSanh,AlbertWebson,ColinRaffel,StephenH
DanielKhashabi,SewonMin,TusharKhot,AshishSab- Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
harwal,OyvindTafjord,PeterClark,andHannaneh Chaffin,ArnaudStiegler,TevenLeScao,ArunRaja,
Hajishirzi.2020. Unifiedqa: Crossingformatbound- et al. 2022. Multitask prompted training enables
aries with a single qa system. In Findings of the zero-shottaskgeneralization. InICLR2022-Tenth
AssociationforComputationalLinguistics: EMNLP International Conference on Learning Representa-
2020,pages1896–1907. tions.
Chin-YewLin.2004. Rouge: Apackageforautomatic
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
evaluation of summaries. In Text summarization
Adaptivelearningrateswithsublinearmemorycost.
branchesout,pages74–81.
InInternationalConferenceonMachineLearning,
pages4596–4604.PMLR.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
XiaoWang,WeikangZhou,CanZu,HanXia,Tianze
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
Chen, Yuansen Zhang, Rui Zheng, Junjie Ye,
collection: Designingdataandmethodsforeffective
QiZhang,TaoGui,etal.2023. Instructuie: Multi-
instructiontuning. arXivpreprintarXiv:2301.13688.
taskinstructiontuningforunifiedinformationextrac-
tion. arXivpreprintarXiv:2304.08085.
KemingLu,XiaomanPan,KaiqiangSong,Hongming
Zhang,DongYu,andJianshuChen.2023. PIVOINE:
Instructiontuningforopen-worldentityprofiling. In Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
FindingsoftheAssociationforComputationalLin- molabashi, Yeganeh Kordi, Amirreza Mirzaei,
guistics: EMNLP2023,pages15108–15127,Singa- Anjana Arunkumar, Arjun Ashok, Arut Selvan
pore.AssociationforComputationalLinguistics. Dhanasekaran,AtharvaNaik,DavidStap,etal.2022.
Super-naturalinstructions:generalizationviadeclara-
YiLuan, LuhengHe, MariOstendorf, andHannaneh tiveinstructionson1600+tasks. InEMNLP.
Hajishirzi.2018. Multi-taskidentificationofentities,
relations, and coreferencefor scientific knowledge Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
graphconstruction. InProc.Conf.EmpiricalMeth- Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
odsNaturalLanguageProcess.(EMNLP).
drewMDai,andQuocVLe.2021a. Finetunedlan-
guagemodelsarezero-shotlearners. arXivpreprint
IshaniMondal,YufangHou,andCharlesJochim.2021.
arXiv:2109.01652.
End-to-end construction of NLP knowledge graph.
In Findings of the Association for Computational
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Linguistics: ACL-IJCNLP2021,pages1885–1895,
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
Online.AssociationforComputationalLinguistics.
drewMDai,andQuocVLe.2021b. Finetunedlan-
guagemodelsarezero-shotlearners. arXivpreprint
Behrang QasemiZadeh and Anne-Kathrin Schumann.
arXiv:2109.01652.
2016. TheACLRD-TEC2.0: Alanguageresource
forevaluatingtermextractionandentityrecognition
methods. InProceedingsoftheTenthInternational TianbaoXie,ChenHenryWu,PengShi,RuiqiZhong,
ConferenceonLanguageResourcesandEvaluation TorstenScholak,MichihiroYasunaga,Chien-Sheng
(LREC’16), pages 1862–1868, Portorož, Slovenia. Wu, Ming Zhong, Pengcheng Yin, Sida I Wang,
EuropeanLanguageResourcesAssociation(ELRA). etal.2022. Unifiedskg: Unifyingandmulti-tasking
structuredknowledgegroundingwithtext-to-textlan-
ColinRaffel,NoamShazeer,AdamRoberts,Katherine guagemodels. InProceedingsofthe2022Confer-
Lee,SharanNarang,MichaelMatena,YanqiZhou, ence on Empirical Methods in Natural Language
WeiLi,andPeterJLiu.2020. Exploringthelimits Processing,pages602–631.
8A Instructions: QualitativeExamples Instruction3(D3):
{Context}\n\n{Question}
In this section, we elicit each of the instructions
thatwereconsideredinthisworkasformulatedin Instruction4(D4):
theFLAN2022CollectionfortheSQuAD_v2and {Context}\nAnswerthisquestion: {Question}
DROPdatasets.
Instruction5(D5):
A.1 TheStanfordQuestionAnswering Read this article and answer this question {Con-
Dataset(SQuAD_v2) text}\n{Question}
Instruction1(S1):
Instruction6(D6):
{Context}\n\nPleaseansweraquestionaboutthis
{Context}\n\nBasedontheabovearticle,answer
article. Ifthequestionisunanswerable,say"unan-
aquestion. {Question}
swerable". {Question}
Instruction7(D7):
Instruction2(S2):
Context: {Context}\n \n Question: {Question}\n
{Context} \n {Question} If the question is unan-
\nAnswer:
swerable,say"unanswerable"
B OurExperimentalHyperparamters
Instruction3(S3):
{Context}\nTrytoanswerthisquestionifpossible
We used two main experimental settings in this
(otherwisereply"unanswerable"): {Question}
work. Thefirstconsistsofadatasetofarandomly
Instruction4(S4): selectedhalfofeveryindividualtemplateinstance,
{Context}\n\nPleaseansweraquestionaboutthis and the second one is a dataset with no template
article. Ifthequestionisunanswerable,say"unan- instancescalledbaselineinthepaper.
swerable". {Question}’{Context}\nTrytoanswer Given that the average context length of our
this question if possible (otherwise reply "unan- dataset was close to the 512 sequence length
swerable"): {Question} limit by T5 and the size of the available GPU, a
batch size of 4 and gradient_accumulation_steps
Instruction5(S5):
of 1 were used. All experiments were run on
{Context}\n If it is possible to answer this ques-
five epochs and we used AdafactorSchedule and
tion,answeritforme(else,reply"unanswerable"):
Adafactor optimizer (Shazeer and Stern, 2018)
{Question}
with scale_parameter=True, relative_step=True,
Instruction6(S6): warmup_init=True,lr=None.
{Context}\n\nAnswerthisquestion,ifpossible(if Theevaluationswerealldoneonadatasetmade
impossible,reply"unanswerable"): {Question} of individual template instructions separately, as
reportedintableTable2.
Instruction7(S7):
Read this: {Context}\n \n {Question} \n What is C ROUGEEvaluationMetrics
theanswer? (Ifitcannotbeanswered,return"unan-
swerable") The ROUGE metrics (Lin, 2004) are commonly
usedforevaluatingthequalityoftextsummariza-
Instruction8(S8):
tionsystems. ROUGE-1measurestheoverlapof
Readthis: {Context}\nNowanswerthisquestion,
unigram(singleword)unitsbetweenthegenerated
if there is an answer (If it cannot be answered,
summary and the reference summary. ROUGE-
return"unanswerable"): {Question}
2 extends this to measure the overlap of bigram
(two consecutive word) units. ROUGE-L calcu-
A.2 DiscreteReasoningoverParagraphs
latesthelongestcommonsubsequencebetweenthe
(DROP)Dataset
generated and reference summaries, which takes
Instruction1(D1):
intoaccounttheorderofwords. ROUGE-LSumis
Answer based on context:\n \n {Context}\n \n
anextensionofROUGE-Lthatconsidersmultiple
{Question}
referencesummariesbytreatingthemasasingle
Instruction2(D2): summary. Thesemetricsprovideaquantitativeas-
{Context}\n\nAnswerthisquestionbasedonthe sessment of the similarity between the generated
article: {Question} andreferencesummaries,helpingresearchersand
9developersevaluateandcomparetheeffectiveness
ofdifferentsummarizationapproaches. Theyhave
become widely used benchmarks in the field of
automaticsummarization.
10