MeshFormer: High-Quality Mesh Generation with
3D-Guided Reconstruction Model
MinghuaLiu∗1,2† ChongZeng∗3‡ XinyueWei1,2† RuoxiShi1,2†
LinghaoChen2,3† ChaoXu2,4† MengqiZhang2 ZhaoningWang2
XiaoshuaiZhang1,2† IsabellaLiu1 HongzhiWu3 HaoSu1,2
1UCSanDiego 2HillbotInc.3ZhejiangUniversity 4UCLA
ProjectWebsite: https://meshformer3d.github.io/
Figure1: Givenasparseset(e.g.,6)ofmulti-viewRGBimagesandtheirnormalmapsasinput,
MeshFormerreconstructshigh-quality3Dtexturedmesheswithfine-grained,sharpgeometricdetails
inafeed-forwardpassofjustafewseconds. Here,groundtruthmulti-viewRGBandnormalimages
areusedasinput.
Abstract
Open-world3Dreconstructionmodelshaverecentlygarneredsignificantattention.
However,withoutsufficient3Dinductivebias,existingmethodstypicallyentail
expensivetrainingcostsandstruggletoextracthigh-quality3Dmeshes. Inthis
work,weintroduceMeshFormer,asparse-viewreconstructionmodelthatexplicitly
leverages3Dnativestructure,inputguidance,andtrainingsupervision.Specifically,
∗Equalcontribution.†WorkdoneduringinternshipatHillbotInc.‡WorkdoneduringinternshipatUC
SanDiego.
Preprint.Underreview.
4202
guA
91
]VC.sc[
1v89101.8042:viXrainsteadofusingatriplanerepresentation,westorefeaturesin3Dsparsevoxelsand
combinetransformerswith3Dconvolutionstoleverageanexplicit3Dstructure
andprojectivebias. Inadditiontosparse-viewRGBinput,werequirethenetwork
totakeinputandgeneratecorrespondingnormalmaps. Theinputnormalmaps
canbepredictedby2Ddiffusionmodels,significantlyaidingintheguidanceand
refinementofthegeometry’slearning. Moreover,bycombiningSignedDistance
Function(SDF)supervisionwithsurfacerendering,wedirectlylearntogenerate
high-qualitymesheswithouttheneedforcomplexmulti-stagetrainingprocesses.
Byincorporatingtheseexplicit3Dbiases,MeshFormercanbetrainedefficiently
anddeliverhigh-qualitytexturedmesheswithfine-grainedgeometricdetails. It
canalsobeintegratedwith2Ddiffusionmodelstoenablefastsingle-image-to-3D
andtext-to-3Dtasks.
1 Introduction
High-quality3Dmeshesareessentialfornumerousapplications,includingrendering,simulation,
and3Dprinting. Traditionalphotogrammetrysystems[57,61]andrecentneuralapproaches,such
asNeRF[43],typicallyrequireadensesetofinputviewsoftheobjectandlongprocessingtimes.
Recently,open-world3Dobjectgenerationhasmadesignificantadvancements,aimingtodemocratize
3Dassetcreationbyreducinginputrequirements. Thereareseveralprevailingparadigms: traininga
native3Dgenerativemodelusingonly3Ddata[13,95]orperformingper-shapeoptimizationwith
ScoreDistillationSampling(SDS)losses[30,47]. Anotherpromisingdirectionistofirstpredict
asparsesetofmulti-viewimagesusing2Ddiffusionmodels[33,59]andthenliftthesepredicted
imagesintoa3Dmodelbytrainingafeed-forwardnetwork[31,32]. Thisstrategyaddressesthe
limitedgeneralizabilityofmodelstrainedsolelyon3Ddataandovercomesthelongruntimeand3D
inconsistencyofper-shape-optimization-basedmethods.
While many recent works explore utilizing priors from 2D diffusion models, such as generating
consistentmulti-viewimages[59,60]andpredictingnormalmapsfromRGB[12,37,59],thefeed-
forwardmodelthatconvertsmulti-viewimagesinto3Dremainsunderexplored. One-2-3-45[32]
leveragesageneralizableNeRFmethodfor3Dreconstructionbutsuffersfromlimitedqualityand
successrates. One-2-3-45++[31]improvesonthisbyusingatwo-stage3Ddiffusionmodel,yet
itstillstrugglestogeneratehigh-qualitytexturesorfine-grainedgeometry. Giventhatsparse-view
reconstructionofopen-worldobjectsrequiresextensivepriors,anotherfamilyofworkspioneered
bythelargereconstructionmodel(LRM)[16]combineslarge-scaletransformermodelswiththe
triplanerepresentationandtrainsthemodelprimarilyusingrenderingloss. Althoughstraightforward,
thesemethodstypicallyrequireoverahundredGPUstotrain. Moreover,duetotheirrelianceon
volumerendering,thesemethodshavedifficultyextractinghigh-qualitymeshes. Forinstance,some
recentfollow-upworks[79,85]implementcomplexmulti-stage“NeRF-to-mesh”trainingstrategies,
buttheresultsstillleaveroomforimprovement.
Inthiswork,wepresentMeshFormer,anopen-worldsparse-viewreconstructionmodelthattakesa
sparsesetofposedimagesofanarbitraryobjectasinputanddelivershigh-quality3Dtexturedmeshes
withasingleforwardpassinafewseconds. Insteadofrepresenting3Ddataas“2Dplanes”and
traininga“blackbox”transformermodeloptimizingonlyrenderingloss,wefindthatbyincorporating
varioustypesof3D-nativepriorsintothemodeldesign,includingnetworkarchitecture,supervision
signals, and input guidance, our model can significantly improve both mesh quality and training
efficiency. Specifically, we propose representing features in explicit 3D voxels and introduce a
novelarchitecturethatcombineslarge-scaletransformerswith3D(sparse)convolutions. Compared
totriplanesandpuretransformersmodelswithlittle3D-nativedesign,MeshFormerleveragesthe
explicit3Dstructureofvoxelfeaturesandthepreciseprojectivecorrespondencebetween3Dvoxels
and2Dmulti-viewfeatures,enablingfasterandmoreeffectivelearning.
UnlikepreviousworksthatrelyonNeRF-basedrepresentationintheirpipeline, weutilizemesh
representationthroughouttheprocessandtrainMeshFormerinaunified,single-stagemanner. Specif-
ically,weproposecombiningsurfacerenderingwithadditionalexplicit3Dsupervision,requiringthe
modeltolearnasigneddistancefunction(SDF)field. Thenetworkistrainedwithhigh-resolution
SDFsupervision,andefficientdifferentiablesurfacerenderingisappliedtotheextractedmeshes
for rendering losses. Due to the explicit 3D geometry supervision, MeshFormer enables faster
trainingwhileeliminatingtheneedforexpensivevolumerenderingandlearninganinitialcoarse
NeRF.Furthermore,inadditiontomulti-viewposedRGBimages,weproposeusingcorresponding
2normalmapsasinput,whichcanbecapturedthroughsensorsandphotometrictechniques[4,82]or
directlyestimatedbyrecent2Dvisionmodels[12,37,59]. Thesemulti-viewnormalimagesprovide
importantcluesfor3Dreconstructionandfine-grainedgeometricdetails. Wealsotaskthemodel
withlearninganormaltextureinadditiontotheRGBtexture,whichcanthenbeusedtoenhancethe
generatedgeometrythroughatraditionalpost-processingalgorithm[44].
Thankstotheexplicit3D-nativestructure,supervisionsignal,andnormalguidancethatwehave
incorporated,MeshFormercangeneratehigh-qualitytexturedmesheswithfine-grainedgeometric
details,asshowninFigure1. ComparedtoconcurrentmethodsthatrequireoveronehundredGPUs
orcomplexmulti-stagetraining,MeshFormercanbetrainedmoreefficientlyandconvenientlywith
justeightGPUsovertwodays,achievingon-parorevenbetterperformance. Itcanalsoseamlessly
integratewithvarious2Ddiffusionmodelstoenablenumeroustasks,suchassingle-image-to-3D
andtext-to-3D.Insummary,ourkeycontributionsinclude:
• WeintroduceMeshFormer,anopen-worldsparse-viewreconstructionmodelcapableofgenerating
high-quality3Dtexturedmesheswithfine-grainedgeometricdetailsinafewseconds. Itcanbe
trainedwithonly8GPUs,outperformingbaselinesthatrequireoveronehundredGPUs.
• We propose a novel architecture that combines 3D (sparse) convolution and transformers. By
explicitlyleveraging3Dstructureandprojectivebias,itfacilitatesbetterandfasterlearning.
• Weproposeaunifiedsingle-stagetrainingstrategyforgeneratinghigh-qualitymeshesbycombining
surfacerenderingandexplicit3Dgeometricsupervision.
• Wearethefirsttointroducemulti-viewnormalimagesasinputtothefeed-forwardreconstruction
network, providing crucial geometric guidance. Additionally, we propose to predict extra 3D
normaltextureforgeometricenhancement.
2 RelatedWork
Open-world 3D Object Generation Thanks to the emergence of large-scale 3D datasets [8, 9]
andtheextensivepriorslearnedby2Dmodels[50,51,55,56],open-world3Dobjectgeneration
haverecentlymadesignificantadvancements. ExemplifiedbyDreamFusion[47],alineofwork[5,
6, 10, 26, 30, 48, 58, 60, 62, 65, 70, 76] uses 2D models as guidance to generate 3D objects
throughper-shapeoptimizationwithSDS-likelosses. Althoughthesemethodsproduceincreasingly
better results, they are still limited by lengthy runtimes and many other issues. Another line of
work[16,20,40,45,84,96]trainsafeed-forwardgenerativemodelsolelyon3Ddatathatconsumes
textpromptsorsingle-imageinputs. Whilefastduringinference,thesemethodsstruggletogeneralize
tounseenobjectcategoriesduetothescarcityof3Ddata. Morerecently,workssuchasZero123[33]
haveshownthat2Ddiffusionmodelscanbefine-tunedwith3Ddatafornovelviewsynthesis. A
lineofwork[27,27,31,64,77,79,85], pioneeredbyOne-2-3-45[32], proposesfirstpredicting
multi-viewimagesthrough2Ddiffusionmodelsandthenliftingthemto3Dthroughafeed-forward
network,effectivelyaddressingthespeedandgeneralizabilityissues. Manyrecentworkshavealso
exploredbetterstrategiestofine-tune2Ddiffusionmodelsforenhancingthe3Dconsistencyofmulti-
viewimages[14,17,23,34,36,49,59,60,69,72,80,81,89,91]. Inadditiontothefeed-forward
models,thegeneratedmulti-viewimagescanalsobeliftedto3Dthroughoptimizations[14,34,37].
Sparse-ViewFeed-ForwardReconstructionModelsWhenasmallbaselinebetweeninputimages
isassumed,existinggeneralizableNeRFmethods[35,52,68,88]aimtofindpixelcorrespondences
andlearngeneralizablepriorsacrossscenesbyleveragingcost-volume-basedtechniques[3,38,90]
ortransformer-basedstructures[19,24,54,71,74]. Someofmethodshavealsoincorporateda2D
diffusionprocessintothepipeline[1,21,66]. However,thesemethodsoftenstruggletohandlelarge
baselinesettings(e.g.,onlyfrontal-viewreconstruction)orarelimitedbyasmalltrainingsetand
failtogeneralizetoopen-worldobjects. Recently,manymodels[27,64,73,77,79,85–87,92,94]
specificallyaimedatopen-world3Dobjectgenerationhavebeenproposed. Theytypicallybuildlarge
networksandaimtolearnextensivereconstructionpriorsbytrainingonlarge-scale3Ddatasets[9].
Forexample,thetriplanerepresentationandtransformermodelsareoftenused. Byapplyingvolume
renderingorGaussiansplatting[64,86,92],theytrainthemodelwithrenderinglosses. However,
thesemethodstypicallyrequireextensiveGPUstotrainandhavedifficultyextractinghigh-quality
meshes. Whilesomerecent(concurrent)works[79,85]utilizemulti-stage“NeRF-to-mesh”training
strategiestoimprovethequality,theresultsstillleaveroomforimprovement.
GeometryGuidancefor3DReconstructionManyrecentworkshaveshownthatinadditiontomulti-
viewRGBimages,2Ddiffusionmodelscanbefine-tunedtogenerateothergeometricmodalities,
suchasdepthmaps[75],normalmaps[12,37,41],orcoordinatemaps[28,77]. Theseadditional
modalitiescanprovidecrucialguidancefor3Dgenerationandreconstruction. Whilemanyrecent
3Figure 2: Pipeline Overview. MeshFormer takes a sparse set of multi-view RGB and normal
imagesasinput,whichcanbeestimatedusingexisting2Ddiffusionmodels. Weutilizea3Dfeature
volumerepresentation,andsubmodulesVoxelFormerandSparseVoxelFormershareasimilarnovel
architecture,detailedinthegrayregion. WetrainMeshFormerinaunifiedsinglestagebycombining
meshsurfacerenderingand5123SDFsupervision. MeshFormerlearnsanadditionalnormaltexture,
whichcanbeusedtofurtherenhancethegeometryandgeneratefine-grainedsharpgeometricdetails.
methodsutilizethesegeometriccuesasinverseoptimizationguidance[5,12,28,37,49,77],we
proposetotakenormalmapsasinputinafeed-forwardreconstructionmodelandtaskthemodelwith
generating3D-consistentnormaltextureforgeometryenhancementofsharpdetails.
3DNativeRepresentationsandNetworkArchitecturesin3DGenerationTheuseof3Dvoxel
representationsand3Dconvolutionsiscommoningeneral3Dgeneration. However,mostrecent
worksfocuson3D-nativediffusion[7,18,29,31,53,95],oneofthekeyparadigmsin3Dgeneration,
whichdiffersfromtheroutetakenbyMeshFormer. These3D-diffusion-basedmethodshavesome
commonlimitations. Forinstance, theyfocussolelyongeometrygenerationandcannotdirectly
predicthigh-qualitytexturesfromthenetwork[7,18,29,31,53,95]. Duetothelimitedavailability
of3Ddata,3D-nativediffusionmethodsalsotypicallystrugglewithopen-worldcapabilitiesandare
oftenconstrainedtoclosed-domaindatasets(e.g.,ShapeNet[2])intheirexperiments[7,29,95].
InMeshFormer,ourgoalistoachievedirecthigh-qualitytexturegenerationwhilehandlingarbitrary
objectcategories. Therefore,weadoptadifferentapproach: sparse-viewfeed-forwardreconstruction,
asopposedto3D-nativediffusion. Inthisspecifictasksetting,morecomparableworksarerecent
LRM-style methods [64, 67, 79, 85]. However, most of these methods rely on a combination of
triplanerepresentationandlarge-scaletransformers. Inthispaper,wedemonstratethat3D-native
representationsandnetworkscannotonlybeusedin3D-nativediffusionbutcanalsobecombined
withdifferentiablerenderingtotrainafeed-forwardsparse-viewreconstructionmodelusingrendering
losses. Inopen-worldsparse-viewreconstruction,wearenotlimitedtothetriplanerepresentation.
Instead,3D-nativestructures(e.g.,voxels),networkarchitectures,andprojectivepriorscanfacilitate
moreefficienttraining,significantlyreducingtherequiredtrainingresources.Whilescalablenetworks
arenecessarytolearnextensivepriors,scalabilityisnotexclusivetotriplane-basedtransformers. By
integrating3Dconvolutionswithtransformerlayers,scalabilitycanalsobeachieved.
3 Method
AsshowninFigure2,MeshFormertakesasparsesetofposedmulti-viewRGBandnormalimages
asinputandgeneratesahigh-qualitytexturedmeshinasinglefeed-forwardpass. Inthefollowing
sections, we will first introduce our choice of 3D representation and a novel model architecture
thatcombineslarge-scaletransformerswith3Dconvolutions(Sec.3.1). Then,wewilldescribeour
trainingobjectives,whichintegratesurfacerenderingandexplicit3DSDFsupervision(Sec.3.2).
Lastbutnotleast,wewillpresentournormalguidanceandgeometryenhancementmodule,which
playsacrucialroleingeneratinghigh-qualitymesheswithfine-grainedgeometricdetails(Sec.3.3).
3.1 3DRepresentationandModelArchitecture
Triplanevs. 3DVoxelsOpen-worldsparse-viewreconstructionrequiresextensivepriors,whichcan
belearnedthroughalarge-scaletransformer.Priorarts[27,67,77,79,85]typicallyutilizethetriplane
representation,whichdecomposesa3Dneuralfieldintoasetof2Dplanes. Whilestraightforwardfor
4processingbytransformers,thetriplanerepresentationlacksexplicit3Dspatialstructuresandmakes
ithardtoenablepreciseinteractionbetweeneach3Dlocationanditscorresponding2Dprojected
pixelsfrommulti-viewimages. Forinstance,thesemethodsoftensimplyapplyself-attentionacross
alltriplanepatchtokensandcross-attentionbetweentriplanetokensandallmulti-viewimagetokens.
Thisall-to-allattentionisnotonlycostlybutalsomakesthemethodscumbersometotrain. Moreover,
thetriplanerepresentationoftenshowsresultswithnotableartifactsattheboundariesofpatchesand
maysufferfromlimitedexpressivenessforcomplexstructures. Consequently,wechoosethe3D
voxelrepresentationinstead,whichexplicitlypreservesthe3Dspatialstructures.
CombiningTransformerwith3DConvolutionToleveragetheexplicit3Dstructureandthepower-
fulexpressivenessofalarge-scaletransformermodelwhileavoidinganexplosionofcomputational
costs,weproposeVoxelFormerandSparseVoxelFormer,whichfollowa3DUNetarchitecturewhile
integratingatransformeratthebottleneck. Theoverallideaisthatweuselocal3Dconvolutionto
encodeanddecodeahigh-resolution3Dfeaturevolume,whiletheglobaltransformerlayerhandles
reasoningandmemorizingpriorsforthecompressedlow-resolutionfeaturevolume. Specifically,
asshowninFigure2,a3Dfeaturevolumebeginswithalearnabletokensharedbyall3Dvoxels.
Withthe3Dvoxelcoordinates,wecanleveragetheprojectionmatrixtoenableeach3Dvoxelto
aggregate2Dlocalfeaturesfrommulti-viewimagesviaaprojection-awarecross-attentionlayer. By
iterativelyperformingprojection-awarecross-attentionand3D(sparse)convolution,wecancompress
the3Dvolumetoalower-resolutionone. Aftercompression,each3Dvoxelfeaturethenservesasa
latenttoken,andadeeptransformermodelisappliedtoasequenceofall3Dvoxelfeatures(position
encoded)toenhancethemodel’sexpressiveness. Finally,weusetheconvolution-basedinverseupper
branchwithskipconnectiontodecodea3Dfeaturevolumewiththeinitialhighresolution.
Projection-AwareCrossAttentionRegarding3D-2Dinteraction,theinputmulti-viewRGBand
normalimagesareinitiallyprocessedbya2Dfeatureextractor,suchasatrainableDINOv2[46],
togeneratemulti-viewpatchfeatures. Whilepreviouscost-volume-basedmethods[3,38]typically
use mean or max pooling to aggregate multi-view 2D features, these simple pooling operations
mightbesuboptimalforaddressingocclusionandvisibilityissues. Instead,weproposeaprojection-
awarecross-attentionmechanismtoadaptivelyaggregatethemulti-viewfeaturesforeach3Dvoxel.
Specifically,weprojecteach3DvoxelontothemviewstointerpolatemRGBandnormalfeatures.
WethenconcatenatetheselocalpatchfeatureswiththeprojectedRGBandnormalvaluestoformm
2Dfeatures. Intheprojection-awarecross-attentionmodule,weusethe3Dvoxelfeaturetocalculate
aqueryanduseboththe3Dvoxelfeatureandthem2Dfeaturestocalculatem+1keysandvalues.
Across-attentionisthenperformedforeach3Dvoxel,enablingpreciseinteractionbetweeneach3D
locationanditscorresponding2Dprojectedpixels,andallowingadaptiveaggregationof2Dfeatures,
whichcanbeformulatedas:
v ←CrossAttention(Q={v},K ={pv}m +{v},V ={pv}m +{v}) (1)
i i=1 i i=1
Where v denotes a 3D voxel feature, and pv denotes its projected 2D pixel feature from view i,
i
whichisaconcatenationoftheRGBfeaturefv,thenormalfeaturegv,andtheRGBandnormal
i i
valuescv andnv,respectively.
i i
Coarse-to-FineFeatureGenerationAsshowninFig.2,togenerateahigh-resolution3Dfeature
volume that captures the fine-grained details of 3D shapes, we follow previous work [31, 95] by
employingacoarse-to-finestrategy. Specifically,wefirstuseVoxelFormer,whichisequippedwith
full 3D convolution, to predict a low-resolution (e.g., 643), coarse 3D occupancy volume. Each
voxelinthisvolumestoresabinaryvalueindicatingwhetheritisclosetothesurface. Thepredicted
occupied voxels are then subdivided to create higher-resolution sparse voxels (e.g., 2563). Next,
we utilize a second module, SparseVoxelFormer, which features 3D sparse convolution [63], to
predictfeaturesforthesesparsevoxels. Afterthis,wetrilinearlyinterpolatethe3Dfeatureofany
near-surface3Dpoint,whichencodesbothgeometricandcolorinformation,fromthehigh-resolution
sparsefeaturevolume. ThefeaturesarethenfedintovariousMLPstolearnthecorrespondingfields.
3.2 UnifiedSingle-StageTraining: SurfaceRenderingwithSDFSupervision
ExistingworkstypicallyuseNeRF[42]andvolumerenderingor3DGaussiansplatting[22]since
theycomewitharelativelyeasyandstablelearningprocess.However,extractinghigh-qualitymeshes
fromtheirresultsisoftennon-trivial. Forexample,directlyapplyingMarchingCubes[39]todensity
fieldsoflearnedNeRFstypicallygeneratesmesheswithmanyartifacts. Recentmethods[78,79,85]
havedesignedcomplex,multi-stage“NeRF-to-mesh”trainingwithdifferentiablesurfacerendering,
but the generated meshes still leave room for improvement. On the other hand, skipping a good
5initializationanddirectlylearningmeshesfromscratchusingpurelydifferentiablesurfacerendering
lossesisalsoinfeasible,asitishighlyunstabletotrainandtypicallyresultsindistortedgeometry.
In this work, we propose leveraging explicit 3D supervision in addition to 2D rendering losses.
AsshowninFigure2,wetaskMeshFormerwithlearningasigneddistancefunction(SDF)field
supervisedbyahigh-resolution(e.g.,5123)groundtruthSDFvolume.TheSDFlossprovidesexplicit
guidancefortheunderlying3Dgeometryandfacilitatesfasterlearning. Italsoallowsustousemesh
representationanddifferentiablesurfacerenderingfromthebeginningwithoutworryingaboutgood
geometryinitializationorunstabletraining,astheSDFlossservesasastrongregularizationforthe
underlyinggeometry. Bycombiningsurfacerenderingwithexplicit3DSDFsupervision,wetrain
MeshFormerinaunified,single-stagetrainingprocess. AsshowninFigure2,weemploythreetiny
MLPsthattakeasinputthe3Dfeatureinterpolatedfromthe3Dsparsefeaturevolumetolearnan
SDFfield,a3Dcolortexture,anda3Dnormaltexture. WeextractmeshesfromtheSDFvolume
usingdualMarchingCubes[39]andemployNVDiffRast[25]fordifferentiablesurfacerendering.
Werenderboththemulti-viewRGBandnormalimagesandcomputetherenderinglosses,which
consistofboththeMSEandperceptuallossterms. Asaresult,ourtraininglosscanbeexpressedas:
L=λ Lcolor+λ Lcolor +λ Lnormal+λ Lnormal+λ L +λ L (2)
1 MSE 2 LPIPS 3 MSE 4 LPIPS 5 occ 6 SDF
whereL andL areMSElossesforoccupancyandSDFvolumes,andλ denotestheweightof
occ SDF i
eachlossterm. Notethatwedonotusemeshgeometrytoderivenormalmaps;instead,weutilizethe
learnednormaltexturefromtheMLP,whichwillbedetailedlater.
3.3 Fine-GrainedGeometricDetails: NormalGuidanceandGeometryEnhancement
Withoutdense-viewcorrespondences,3Dreconstructionfromsparse-viewRGBimagestypically
struggles to capture geometric details and suffers from texture ambiguity. While many recent
works[27,79,85]attempttoemploylarge-scalemodelstolearnmappingsfromRGBtogeometric
details,thistypicallyrequiressignificantcomputationalresources. Additionally,thesemethodsare
primarilytrainedusing3Ddata,butit’sstilluncertainwhetherthescaleof3Ddatasetsissufficient
forlearningsuchextensivepriors. Ontheotherhand,unlikeRGBimages,normalmapsexplicitly
encode geometric information and can provide crucial guidance for 3D reconstruction. Notably,
open-worldnormalmapestimationhasachievedgreatadvancements. Manyrecentworks[12,37,59]
demonstratethat2Ddiffusionmodels,trainedonbillionsofnaturalimages,embedextensivepriors
andcanbefine-tunedtopredictnormalmaps.Giventhesignificantdisparityindatascalebetween2D
and3Ddatasets,itmaybemoreeffectivetouse2Dmodelsfirstforgeneratinggeometricguidance.
InputNormalGuidanceAsshowninFigure2,inadditiontomulti-viewRGBimages,MeshFormer
alsotakesmulti-viewnormalmapsasinput,whichcanbegeneratedusingrecentopen-worldnormal
estimationmodels[12,37,59]. Inourexperiments,weutilizeZero123++v1.2[59],whichtrainsan
additionalControlNet[93]overthemulti-viewpredictionmodel. TheControlNettakesmulti-view
RGBimages,predictedbyZero123++,asaconditionandproducescorrespondingmulti-viewnormal
maps,expressedinthecameracoordinateframe. Giventhesemaps,MeshFormerfirstconvertsthem
toaunifiedworldcoordinateframe,andthentreatsthemsimilarlytothemulti-viewRGBimages,
usingprojection-awarecross-attentiontoguide3Dreconstruction. Accordingtoourexperiments
(Sec.4.4),themulti-viewnormalmapsenablethenetworkstobettercapturegeometrydetails,and
thusgreatlyimprovefinalmeshquality.
GeometryEnhancementWhilethestraightforwardapproachofderivingnormalmapsfromthe
learnedmeshandusinganormallosstoguidegeometrylearninghasbeencommonlyused,wefind
thatthisapproachmakesourmeshlearninglessstable. Instead,weproposelearninga3Dnormal
texture,similartoacolortexture,usingaseparateMLP.BycomputingthenormallossforMLP-
queriednormalmapsinsteadofmesh-derivednormalmaps,wedecouplenormaltexturelearning
fromunderlyinggeometrylearning.Thismakesthetrainingmorestable,asitiseasiertolearnasharp
3Dnormalmapthantodirectlylearnasharpmeshgeometry. Thelearned3Dnormaltexturecanbe
exportedwiththemesh,similartothecolortexture,tosupportvariousgraphicsrenderingpipelines.
Inapplicationsthatrequireprecise3Dgeometry,suchas3Dprinting,thelearnednormaltexturecan
alsobeusedtorefinethemeshgeometrywithtraditionalalgorithms. Specifically,duringinference,
afterextractinga3DmeshfromtheSDFvolume,weutilizeapost-processingalgorithm[44]that
takesasinputthe3DpositionsofthemeshverticesandthevertexnormalsestimatedfromtheMLP.
Thealgorithmadjuststhemeshverticestoalignwiththepredictednormalsinafewseconds,further
enhancingthegeometryqualityandgeneratingsharpgeometricdetails,asshowninFigure5.
6Figure 3: Qualitative Examples of Single Image to 3D (GSO dataset). Both the textured and
texturelessmeshrenderingsareshown. Pleasezoomintoexaminedetailsandmeshquality,andrefer
tothesupplementalmaterialforresultsofOne-2-3-45++[31]andCRM[77].
4 Experiments
4.1 ImplementationDetailsandEvaluationSettings
ImplementationDetailsWetrainedMeshFormerontheObjaverse[9]dataset. Thetotalnumber
ofnetworkparametersisapproximately648million. Wetrainedthemodelusing8H100GPUsfor
aboutoneweek(350kiterations)withabatchsizeof1perGPU,althoughwealsoshowthatthe
modelcanachievesimilarresultsinjusttwodays. Pleaserefertothesupplementaryformoredetails.
EvaluationSettingsWeevaluatethemethodsontwodatasets: GSO[11]andOmniObject3D[83].
Bothdatasetscontainreal-scanned3Dobjectsthatwerenotseenduringtraining.FortheGSOdataset,
weuseall1,0303Dshapesforevaluation. FortheOmniObject3Ddataset,werandomlysampleupto
5shapesfromeachcategory,resultingin1,038shapesforevaluation. Weutilizeboth2Dand3D
metrics. For3Dmetrics,weuseboththeF-scoreandChamferdistance(CD),calculatedbetweenthe
predictedmeshesandgroundtruthmeshes,following[31,85]. For2Dmetrics,wecomputeboth
PSNRandLPIPSfortherenderedcolorimages. Sinceeachbaselinemayuseadifferentcoordinate
frameforgeneratedresults,wecarefullyalignthepredictedmeshesofallmethodstothegroundtruth
meshesbeforecalculatingthemetrics. Pleaserefertothesupplementalmaterialformoredetails.
4.2 ComparisonwithSingle/Sparse-Viewto3DMethods
We compare MeshFormer with recent open-world feed-forward single/sparse-view to 3D meth-
ods, including One-2-3-45++ [31], TripoSR [67], CRM [77], LGM [64], InstantMesh [85], and
7Table 1: Quantitative Results of Single Image to 3D. Evaluated on the 1,030 and 1,038 3D
shapesfromtheGSO[11]andtheOmniObject3D[83]datasets,respectively. One-2-3-45++[31],
InstantMesh [85], MeshLRM [79], and our method all take the same multi-view RGB images
predictedbyZero123++[59]asinput. CDdenotesChamferDistance.
GSO[11] OmniObject3D[83]
Method
F-Score↑ CD↓ PSNR↑ LPIPS↓ F-Score↑ CD↓ PSNR↑ LPIPS↓
One-2-3-45++[31] 0.936 0.039 20.97 0.21 0.871 0.054 17.08 0.31
TripoSR[67] 0.896 0.047 19.85 0.26 0.895 0.048 17.68 0.28
CRM[77] 0.886 0.051 19.99 0.27 0.821 0.065 16.01 0.34
LGM[64] 0.776 0.074 18.52 0.35 0.635 0.114 14.75 0.45
InstantMesh[64] 0.934 0.037 20.90 0.22 0.889 0.049 17.61 0.28
MeshLRM[79] 0.956 0.033 21.31 0.19 0.910 0.045 18.10 0.26
Ours 0.963 0.031 21.47 0.20 0.914 0.043 18.14 0.27
Table2: Wecomparemethodsusinglimitedtrainingresources. EvaluatedontheGSO[11]dataset.
Method TrainingResources F-Score↑ CD↓ PSNR-C↑ LPIPS-C↓ PSNR-N↑ LPIPS-N↓
MeshLRM[79] 0.925 0.0397 21.09 0.26 21.69 0.22
8×H10048h
Ours 0.960 0.0317 21.41 0.20 23.01 0.15
MeshLRM [79]. Many of these methods have been released recently and should be considered
concurrentmethods. ForMeshLRM[79], wecontactedtheauthorsfortheresults. Fortheother
methods,weutilizedtheirofficialimplementations. Pleaserefertothesupplementaryfordetails.
Sinceinputsettingsdifferamongthebaselines,weevaluateallmethodsinaunifiedsingle-viewto
3Dsetting. FortheGSOdataset,weutilizedthefirstthumbnailimageasthesingle-viewinput. For
theOmniObject3Ddataset,weusedarenderedimagewitharandomposeasinput. ForOne-2-3-
45++[31],InstantMesh[85],MeshLRM[79],andourMeshFormer,wefirstutilizedZero123++[59]
to convert the input single-view image into multi-view images before 3D reconstruction. Other
baselinesfollowtheiroriginalsettingsandtakeasingle-viewimagedirectlyasinput. Inadditionto
theRGBimages,ourMeshFormeralsotakesadditionalmulti-viewnormalimagesasinput,which
are also predicted by Zero123++ [59]. Note that when comparing with baseline methods, we
neverusegroundtruthnormalimagestoensureafaircomparison.
InFig.3,weshowcasequalitativeexamples. OurMeshFormerproducesthemostaccuratemeshes
withfine-grained,sharpgeometricdetails.Incontrast,baselinemethodsproduceinferiormeshquality.
Forexample,TripoSRdirectlyextractsmeshesfromthelearnedNeRFrepresentation,resultingin
significantartifacts. WhileInstantMeshandMeshLRMusemeshrepresentationintheirsecondstage,
notableunevenartifactsarestillobservableuponazoom-ininspection. Additionally,allbaseline
methods incorrectly close the surface of the copper bell. We also provide quantitative results in
Tab.1. Althoughourbaselinesincludefourmethodsreleasedjustoneortwomonthsbeforethe
timeofsubmission,ourMeshFormersignificantlyoutperformsmanyofthemandachievesthebest
performanceonmostmetricsacrosstwodatasets. ForthecolorLPIPSmetric,ourperformanceis
verysimilartoMeshLRM’s,despiteaperceptuallossbeingtheirmaintraininglossterm. Wealso
highlightthatmanyofthebaselinesrequireoveronehundredGPUsfortraining,whereasourmodel
canbeefficientlytrainedwithjust8GPUs. PleaserefertoSec.4.4foranalysisontrainingefficiency.
4.3 Application: Textto3D
Inadditiontothesingleimageto3D,MeshFormercanalsobeintegratedwith2Ddiffusionmodels
to enable various 3D object generation tasks. For example, we follow the framework proposed
by[37]tofinetuneStableDiffusion[56]andbuildatext-to-multi-viewmodel. Byintegratingthis
model,alongwiththenormalpredictionfromZero123++[59],withMeshFormer,wecanenablethe
taskoftextto3D.Figure4showssomeinterestingresults,whereweconvertasingletextprompt
intoahigh-quality3Dmeshinjustafewseconds. Pleaserefertothesupplementalmaterialsfora
qualitativecomparisonwithoneofthestate-of-the-arttext-to-3Dmethods,Instant3D[27].
4.4 AnalysisandAblationStudy
Explicit3Dstructurevs. TriplaneInSection4.2,wedemonstratedthatMeshFormeroutperforms
baselinemethodsthatprimarilyutilizethetriplanerepresentation. Here,wehighlighttwoadditional
advantagesofusingtheexplicit3Dvoxelstructure: trainingefficiencyandtheavoidanceof“triplane
artifacts”. Without leveraging explicit 3D structure, existing triplane-based large reconstruction
8Figure4: Application: Textto3D.Givenatextprompt,a2Ddiffusionmodelfirstpredictsmulti-
viewRGBandnormalimages,whicharethenfedtoMeshFormerfor3Dreconstruction. Pleaserefer
tothesupplementaryforcomparisonswithInstant3D[27].
Table3: AblationStudyontheGSO[11]dataset. -Cdenotescolorrenderings,and-Ndenotes
normalrenderings. CDstandsforChamferdistance. Bydefault,groundtruthmulti-viewimagesare
usedtoexcludetheinfluenceoferrorsfrom2Ddiffusionmodels.
Setting PSNR-C↑ LPIPS-C↓ PSNR-N↑ LPIPS-N↓ F-Score↑ CD↓
a w/onormalinput 24.82 0.129 24.85 0.107 0.964 0.024
b w/oSDFsupervision 20.72 0.244 20.42 0.257 0.940 0.035
c w/otransformerlayer 26.63 0.101 29.80 0.036 0.992 0.013
d w/oprojection-awarecross-attention 25.48 0.155 29.01 0.045 0.991 0.013
e w/ogeometryenhancement 27.95 0.085 29.10 0.048 0.992 0.012
f w/prednormal 26.84 0.096 26.99 0.067 0.987 0.017
g full 28.15 0.083 29.80 0.036 0.992 0.012
modelsrequireextensivecomputingresourcesfortraining. Forexample,TripoSRrequires176A100
GPUsforfivedaysoftraining.InstantMeshreliesonOpenLRM[15],whichrequires128A100GPUs
forthreedaysoftraining. MeshLRMalsoutilizessimilarresourcesduringtraining. Byutilizing
explicit3Dstructureandprojectivebias,ourMeshFormercanbetrainedmuchmoreefficientlyusing
only8GPUs. Tobetterunderstandthegap,wetrainedbothMeshLRMandourMeshFormerunder
very limited training resources, and the results are shown in Table 2. When using only 8 GPUs
fortwodays,wefoundthatMeshLRMfailedtoconvergeandexperiencedsignificantperformance
degradationcomparedtotheresultsshowninTable1,whileourMeshFormerhadalreadyconverged
toadecentresult,closetothefully-trainedversion,demonstratingsuperiortrainingefficiency.
Weobservethatthetriplanetypicallygeneratesresultswithaxis-alignedartifacts,asshowninFig.3
(5throw,pleasezoomin). Asdemonstratedinthesupplementary(Fig.7),theseartifactsalsocause
difficultiesforMeshLRM[79]incapturingthewordsonobjects. Theselimitationsarelikelycaused
bythelimitednumberoftriplanetokens(e.g., 32×32×3), constrainedbytheglobalattention,
which often leads to artifacts at the boundaries of the triplane patches. In contrast, MeshFormer
leveragessparsevoxels,supportsahigherfeatureresolutionof2563,andisfreefromsuchartifacts.
NormalInputandSDFsupervisionAsshowninTable3(a),theperformancesignificantlydrops
whenmulti-viewinputnormalmapsareremoved,indicatingthatthegeometricguidanceandclues
providedbynormalimagesarecrucialforfacilitatingnetworktraining,particularlyforlocalgeometric
details. In(f),wereplacegroundtruthnormalmapswithnormalpredictionsbyZero123++[59]and
observeanotableperformancegapcomparedto(g). Thisindicatesthatalthoughpredictedmulti-view
normalimagescanbebeneficial,existing2Ddiffusionmodelsstillhaveroomforimprovementin
generatingmoreaccurateresults. Seesupplementaryforqualitativeexamples. Asshownin(b),ifwe
removetheSDFlossafterthefirstepochandtrainthenetworkusingonlysurfacerenderinglosses,
thegeometrylearningquicklydeteriorates,resultinginpoorgeometry. Thisexplainswhyexisting
9methods[27,79]typicallyemploycomplexmulti-stagetrainingandusevolumerenderingtolearn
acoarseNeRFintheinitialstage. Byleveragingexplicit3DSDFsupervisionasstronggeometric
regularization,weenableaunifiedsingle-stagetraining,usingmeshastheonlyrepresentation.
Projection-Aware Cross-Attention and Transformer Layers We propose to utilize projection-
aware cross-attention to precisely aggregate multi-
viewprojected2Dfeaturesforeach3Dvoxel. Incon-
ventional learning-based multi-view stereo (MVS)
methods[3,38],averageormaxpoolingistypically
employedforfeatureaggregation. InTable3(d),we
replacethecross-attentionwithasimpleaveragepool-
ingandweobserveasignificantperformancedrop.
Thisverifiesthatprojection-awarecross-attentionpro-
vides a more effective way for 3D-2D interaction
whilesimpleaveragepoolingmayfailtohandlethe
occlusionandvisibilityissues. Inthebottleneckof
the UNet, we treat all 3D (sparse) voxels as a se-
Figure5: Geometryenhancementgenerates
quence of tokens and apply transformer layers to
sharper details. Please zoom in to see the
them. Asshowninrow(c),afterremovingtheselay-
details.
ers,weobserveaperformancedropinmetricsrelated
totexturequality. Thisindicatesthattexturelearningrequiresmoreextensivepriorsandbenefits
morefromthetransformerlayers.
GeometryEnhancementWeproposetolearnanadditionalnormalmaptextureandapplyatra-
ditional algorithm as post-processing for geometry enhancement during inference. As shown in
Figure5,thegeometryenhancementalignsthemeshgeometrywiththelearnednormaltextureand
generates fine-grained sharp details. In some cases (such as the wolf), the meshes output by the
networkarealreadygoodenough,andthedifferencecausedbytheenhancementtendstobesubtle.
Row(e)alsoquantitativelyverifiestheeffectivenessofthemodule.
5 ConclusionandLimitations
WepresentMeshFormer,anopen-worldsparse-viewreconstructionmodelthatleveragesexplicit3D
nativestructure,supervisionsignals,andinputguidance. MeshFormercanbeconvenientlytrainedin
aunifiedsingle-stagemannerandefficientlywithjust8GPUs. Itgenerateshigh-qualitymesheswith
fine-grainedgeometricdetailsandoutperformsbaselinestrainedwithoveronehundredGPUs.
MeshFormerrelieson2Dmodelstogeneratemulti-viewRGBandnormalimagesfromasingleinput
imageortextprompt. However,existingmodelsstillhavelimitedcapabilitiestogenerateconsistent
multi-viewimages,whichcancauseaperformancedrop. Strategiestoimprovemodelrobustness
againstsuchimperfectpredictionsareworthfurtherexploration,andweleavethisasfuturework.
References
[1] EricRChan,KokiNagano,MatthewAChan,AlexanderWBergman,JeongJoonPark,AxelLevy,Miika
Aittala,ShaliniDeMello,TeroKarras,andGordonWetzstein. Genvs:Generativenovelviewsynthesis
with3d-awarediffusionmodels,2023.
[2] AngelXChang,ThomasFunkhouser,LeonidasGuibas,PatHanrahan,QixingHuang,ZimoLi,Silvio
Savarese,ManolisSavva,ShuranSong,HaoSu,etal. Shapenet:Aninformation-rich3dmodelrepository.
arXivpreprintarXiv:1512.03012,2015.
[3] AnpeiChen,ZexiangXu,FuqiangZhao,XiaoshuaiZhang,FanboXiang,JingyiYu,andHaoSu. Mvsnerf:
Fastgeneralizableradiancefieldreconstructionfrommulti-viewstereo. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages14124–14133,2021.
[4] GuanyingChen,KaiHan,BoxinShi,YasuyukiMatsushita,andKwan-YeeK.Wong. Sdps-net: Self-
calibratingdeepphotometricstereonetworks. InCVPR,2019.
[5] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fantasia3d:Disentanglinggeometryandappearance
forhigh-qualitytext-to-3dcontentcreation. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision(ICCV),October2023.
[6] ZilongChen,FengWang,YikaiWang,andHuapingLiu. Text-to-3dusinggaussiansplatting,2024.
10[7] Yen-ChiCheng,Hsin-YingLee,SergeyTulyakov,AlexanderGSchwing,andLiang-YanGui. Sdfusion:
Multimodal 3d shape completion, reconstruction, and generation. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages4456–4465,2023.
[8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan
Fan,ChristianLaforte,VikramVoleti,SamirYitzhakGadre,EliVanderBilt,AniruddhaKembhavi,Carl
Vondrick,GeorgiaGkioxari,KianaEhsani,LudwigSchmidt,andAliFarhadi. Objaverse-xl:Auniverseof
10m+3dobjects,2023.
[9] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs,OscarMichel,EliVanderBilt,LudwigSchmidt,
KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: Auniverseofannotated3dobjects,
2022.
[10] CongyueDeng,ChiyuJiang,CharlesRQi,XinchenYan,YinZhou,LeonidasGuibas,DragomirAnguelov,
et al. Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages20637–
20647,2023.
[11] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,
ThomasB McHugh, and VincentVanhoucke. Google scannedobjects: Ahigh-quality dataset of3d
scannedhouseholditems. In2022InternationalConferenceonRoboticsandAutomation(ICRA),pages
2553–2560.IEEE,2022.
[12] XiaoFu,WeiYin,MuHu,KaixuanWang,YuexinMa,PingTan,ShaojieShen,DahuaLin,andXiaoxiao
Long. Geowizard:Unleashingthediffusionpriorsfor3dgeometryestimationfromasingleimage. arXiv
preprintarXiv:2403.12013,2024.
[13] JunGao,TianchangShen,ZianWang,WenzhengChen,KangxueYin,DaiqingLi,OrLitany,ZanGojcic,
andSanjaFidler. Get3d: Agenerativemodelofhighquality3dtexturedshapeslearnedfromimages.
AdvancesInNeuralInformationProcessingSystems,35:31841–31854,2022.
[14] RuiqiGao*,AleksanderHolynski*,PhilippHenzler,ArthurBrussee,RicardoMartin-Brualla,PratulP.
Srinivasan,JonathanT.Barron,andBenPoole*. Cat3d:Createanythingin3dwithmulti-viewdiffusion
models. arXiv,2024.
[15] ZexinHeandTengfeiWang. Openlrm: Open-sourcelargereconstructionmodels. https://github.
com/3DTopia/OpenLRM,2023.
[16] YicongHong, KaiZhang, JiuxiangGu, SaiBi, YangZhou, DifanLiu, FengLiu, KalyanSunkavalli,
Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint
arXiv:2311.04400,2023.
[17] HanzheHu, ZhizhuoZhou, VarunJampani, andShubhamTulsiani. Mvd-fusion: Single-view3dvia
depth-consistentmulti-viewgeneration. InCVPR,2024.
[18] Ka-HeiHui,AdityaSanghi,AriannaRampini,KamalRahimiMalekshan,ZhengzheLiu,HoomanShayani,
and Chi-Wing Fu. Make-a-shape: a ten-million-scale 3d shape model. In Forty-first International
ConferenceonMachineLearning,2024.
[19] MohammadMahdiJohari, YannLepoittevin, andFrançoisFleuret. Geonerf: Generalizingnerfwith
geometrypriors.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages18365–18375,2022.
[20] HeewooJunandAlexNichol. Shap-e:Generatingconditional3dimplicitfunctions,2023.
[21] AnimeshKarnewar,AndreaVedaldi,DavidNovotny,andNiloyJMitra. Holodiffusion: Traininga3d
diffusionmodelusing2dimages. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages18423–18433,2023.
[22] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3dgaussiansplattingfor
real-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4),July2023.
[23] XinKong,ShikunLiu,XiaoyangLyu,MarwanTaher,XiaojuanQi,andAndrewJDavison. Eschernet:A
generativemodelforscalableviewsynthesis. arXivpreprintarXiv:2402.03908,2024.
[24] JonášKulhánek,ErikDerner,TorstenSattler,andRobertBabuška.Viewformer:Nerf-freeneuralrendering
from few images using transformers. In European Conference on Computer Vision, pages 198–216.
Springer,2022.
11[25] SamuliLaine,JanneHellsten,TeroKarras,YeonghoSeol,JaakkoLehtinen,andTimoAila. Modular
primitivesforhigh-performancedifferentiablerendering. ACMTransactionsonGraphics,39(6),2020.
[26] Han-HungLeeandAngelX.Chang. Understandingpureclipguidanceforvoxelgridnerfmodels,2022.
[27] JiahaoLi,HaoTan,KaiZhang,ZexiangXu,FujunLuan,YinghaoXu,YicongHong,KalyanSunkavalli,
Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large
reconstructionmodel. arXivpreprintarXiv:2311.06214,2023.
[28] WeiyuLi,RuiChen,XuelinChen,andPingTan. Sweetdreamer:Aligninggeometricpriorsin2ddiffusion
forconsistenttext-to-3d. arxiv:2310.02596,2023.
[29] YuhanLi,YishunDou,XuanhongChen,BingbingNi,YilinSun,YutianLiu,andFuzhenWang. Gen-
eralizeddeep3dshapepriorviapart-discretizeddiffusionprocess. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages16784–16794,2023.
[30] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang,KarstenKreis,
SanjaFidler,Ming-YuLiu,andTsung-YiLin. Magic3d:High-resolutiontext-to-3dcontentcreation. In
IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2023.
[31] MinghuaLiu,RuoxiShi,LinghaoChen,ZhuoyangZhang,ChaoXu,XinyueWei,HanshengChen,Chong
Zeng,JiayuanGu,andHaoSu. One-2-3-45++:Fastsingleimageto3dobjectswithconsistentmulti-view
generationand3ddiffusion. arXivpreprintarXiv:2311.07885,2023.
[32] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,MukundVarmaT,ZexiangXu,andHaoSu. One-2-
3-45:Anysingleimageto3dmeshin45secondswithoutper-shapeoptimization. AdvancesinNeural
InformationProcessingSystems,36,2024.
[33] RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov,SergeyZakharov,andCarlVondrick. Zero-
1-to-3:Zero-shotoneimageto3dobject. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages9298–9309,2023.
[34] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.
Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint
arXiv:2309.03453,2023.
[35] YuanLiu,SidaPeng,LingjieLiu,QianqianWang,PengWang,ChristianTheobalt,XiaoweiZhou,and
WenpingWang. Neuralraysforocclusion-awareimage-basedrendering. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages7824–7833,2022.
[36] YuxinLiu,MinshanXie,HanyuanLiu,andTien-TsinWong. Text-guidedtexturingbysynchronized
multi-viewdiffusion. arXivpreprintarXiv:2311.12891,2023.
[37] XiaoxiaoLong,Yuan-ChenGuo,ChengLin,YuanLiu,ZhiyangDou,LingjieLiu,YuexinMa,Song-Hai
Zhang,MarcHabermann,ChristianTheobalt,etal. Wonder3d:Singleimageto3dusingcross-domain
diffusion. arXivpreprintarXiv:2310.15008,2023.
[38] XiaoxiaoLong,ChengLin,PengWang,TakuKomura,andWenpingWang.Sparseneus:Fastgeneralizable
neuralsurfacereconstructionfromsparseviews. InEuropeanConferenceonComputerVision,pages
210–227.Springer,2022.
[39] WilliamE.LorensenandHarveyE.Cline. Marchingcubes:ahighresolution3Dsurfaceconstruction
algorithm,page347–353. AssociationforComputingMachinery,NewYork,NY,USA,1998.
[40] J.Lorraine,K.Xie,X.Zeng,C.Lin,T.Takikawa,N.Sharp,T.Lin,M.Liu,S.Fidler,andJ.Lucas. Att3d:
Amortizedtext-to-3dobjectsynthesis. In2023IEEE/CVFInternationalConferenceonComputerVision
(ICCV),pages17900–17910,LosAlamitos,CA,USA,oct2023.IEEEComputerSociety.
[41] YuanxunLu,JingyangZhang,ShiweiLi,TianFang,DavidMcKinnon,YanghaiTsin,LongQuan,Xun
Cao,andYaoYao. Direct2.5:Diversetext-to-3dgenerationviamulti-view2.5ddiffusion,2024.
[42] BenMildenhall,PratulP.Srinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,andRen
Ng. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. InECCV,2020.
[43] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,andRen
Ng. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. CommunicationsoftheACM,
65(1):99–106,2021.
12[44] DiegoNehab,SzymonRusinkiewicz,JamesDavis,andRaviRamamoorthi.Efficientlycombiningpositions
andnormalsforprecise3dgeometry. ACMTrans.Graph.,24(3):536–543,jul2005.
[45] AlexNichol,HeewooJun,PrafullaDhariwal,PamelaMishkin,andMarkChen. Point-e: Asystemfor
generating3dpointcloudsfromcomplexprompts,2022.
[46] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas,
WojciechGaluba,RussellHowes,Po-YaoHuang,Shang-WenLi,IshanMisra,MichaelRabbat,Vasu
Sharma,GabrielSynnaeve,HuXu,HervéJegou,JulienMairal,PatrickLabatut,ArmandJoulin,andPiotr
Bojanowski. Dinov2:Learningrobustvisualfeatureswithoutsupervision,2023.
[47] BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall.Dreamfusion:Text-to-3dusing2ddiffusion.
InICLR,2023.
[48] GuochengQian,JinjieMai,AbdullahHamdi,JianRen,AliaksandrSiarohin,BingLi,Hsin-YingLee,Ivan
Skorokhodov,PeterWonka,SergeyTulyakov,andBernardGhanem.Magic123:Oneimagetohigh-quality
3dobjectgenerationusingboth2dand3ddiffusionpriors. InTheTwelfthInternationalConferenceon
LearningRepresentations(ICLR),2024.
[49] LingtengQiu,GuanyingChen,XiaodongGu,QiZuo,MutianXu,YushuangWu,WeihaoYuan,Zilong
Dong,LiefengBo,andXiaoguangHan. Richdreamer:Ageneralizablenormal-depthdiffusionmodelfor
detailrichnessintext-to-3d,2023.
[50] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision,2021.
[51] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,and
IlyaSutskever. Zero-shottext-to-imagegeneration,2021.
[52] KonstantinosRematas,RicardoMartin-Brualla,andVittorioFerrari. Sharf:Shape-conditionedradiance
fieldsfromasingleview. arXivpreprintarXiv:2102.08860,2021.
[53] XuanchiRen,JiahuiHuang,XiaohuiZeng,KenMuseth,SanjaFidler,andFrancisWilliams. Xcube:
Large-scale3dgenerativemodelingusingsparsevoxelhierarchies. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages4209–4219,2024.
[54] YufanRen,TongZhang,MarcPollefeys,SabineSüsstrunk,andFangjinhuaWang. Volrecon: Volume
renderingofsignedraydistancefunctionsforgeneralizablemulti-viewreconstruction. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages16685–16695,2023.
[55] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer. High-resolutionimagesynthesiswith
latentdiffusionmodels. In2022IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages10674–10685,LosAlamitos,CA,USA,jun2022.IEEEComputerSociety.
[56] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,SeyedKamyarSeyed
Ghasemipour,BurcuKaragolAyan,S.SaraMahdavi,RaphaGontijoLopes,TimSalimans,JonathanHo,
DavidJFleet,andMohammadNorouzi. Photorealistictext-to-imagediffusionmodelswithdeeplanguage
understanding,2022.
[57] JohannesLSchonbergerandJan-MichaelFrahm. Structure-from-motionrevisited. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages4104–4113,2016.
[58] JunyoungSeo,WooseokJang,Min-SeopKwak,JaehoonKo,HyeonsuKim,JunhoKim,Jin-HwaKim,
JiyoungLee,andSeungryongKim. Let2ddiffusionmodelknow3d-consistencyforrobusttext-to-3d
generation. arXivpreprintarXiv:2303.07937,2023.
[59] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,ChaoXu,XinyueWei,LinghaoChen,Chong
Zeng,andHaoSu. Zero123++: asingleimagetoconsistentmulti-viewdiffusionbasemodel. arXiv
preprintarXiv:2310.15110,2023.
[60] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view
diffusionfor3dgeneration. arXiv:2308.16512,2023.
[61] RobustMultiviewStereopsis. Accurate,dense,androbustmultiviewstereopsis. IEEETRANSACTIONS
ONPATTERNANALYSISANDMACHINEINTELLIGENCE,32(8),2010.
13[62] JingxiangSun,BoZhang,RuizhiShao,LizhenWang,WenLiu,ZhendaXie,andYebinLiu.Dreamcraft3d:
Hierarchical3dgenerationwithbootstrappeddiffusionprior,2023.
[63] HaotianTang,ShangYang,ZhijianLiu,KeHong,ZhongmingYu,XiuyuLi,GuohaoDai,YuWang,and
SongHan. Torchsparse++:Efficienttrainingandinferenceframeworkforsparseconvolutionongpus. In
IEEE/ACMInternationalSymposiumonMicroarchitecture(MICRO),2023.
[64] JiaxiangTang,ZhaoxiChen,XiaokangChen,TengfeiWang,GangZeng,andZiweiLiu. Lgm: Large
multi-viewgaussianmodelforhigh-resolution3dcontentcreation. arXivpreprintarXiv:2402.05054,
2024.
[65] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGangZeng. Dreamgaussian:Generativegaussian
splattingforefficient3dcontentcreation. arXivpreprintarXiv:2309.16653,2023.
[66] AyushTewari,TianweiYin,GeorgeCazenavette,SemonRezchikov,JoshuaBTenenbaum,FrédoDurand,
WilliamTFreeman,andVincentSitzmann. Diffusionwithforwardmodels:Solvingstochasticinverse
problemswithoutdirectsupervision. arXivpreprintarXiv:2306.11719,2023.
[67] DmitryTochilkin,DavidPankratz,ZexiangLiu,ZixuanHuang,AdamLetts,YangguangLi,DingLiang,
ChristianLaforte,VarunJampani,andYan-PeiCao. Triposr:Fast3dobjectreconstructionfromasingle
image,2024.
[68] AlexTrevithickandBoYang. Grf:Learningageneralradiancefieldfor3drepresentationandrendering.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages15182–15192,2021.
[69] VikramVoleti,Chun-HanYao,MarkBoss,AdamLetts,DavidPankratz,DmitriiTochilkin,Christian
Laforte,RobinRombach,andVarunJampani. SV3D:Novelmulti-viewsynthesisand3Dgenerationfrom
asingleimageusinglatentvideodiffusion. arXiv,2024.
[70] HaochenWang, XiaodanDu, JiahaoLi, RaymondA.Yeh, andGregShakhnarovich. Scorejacobian
chaining: Liftingpretrained2ddiffusionmodelsfor3dgeneration. arXivpreprintarXiv:2212.00774,
2022.
[71] PeihaoWang,XuxiChen,TianlongChen,SubhashiniVenugopalan,ZhangyangWang,etal. Isattention
allnerfneeds? arXivpreprintarXiv:2207.13298,2022.
[72] PengWangandYichunShi. Imagedream:Image-promptmulti-viewdiffusionfor3dgeneration. arXiv
preprintarXiv:2312.02201,2023.
[73] PengWang,HaoTan,SaiBi,YinghaoXu,FujunLuan,KalyanSunkavalli,WenpingWang,ZexiangXu,
andKaiZhang. Pf-lrm:Pose-freelargereconstructionmodelforjointposeandshapeprediction. arXiv
preprintarXiv:2311.12024,2023.
[74] QianqianWang,ZhichengWang,KyleGenova,PratulPSrinivasan,HowardZhou,JonathanTBarron,
RicardoMartin-Brualla,NoahSnavely,andThomasFunkhouser.Ibrnet:Learningmulti-viewimage-based
rendering. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages4690–4699,2021.
[75] ZhenWang,QiangengXu,FeitongTan,MengleiChai,ShichenLiu,RohitPandey,SeanFanello,Achuta
Kadambi,andYindaZhang. Mvdd:Multi-viewdepthdiffusionmodels,2023.
[76] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-
dreamer:High-fidelityanddiversetext-to-3dgenerationwithvariationalscoredistillation. arXivpreprint
arXiv:2305.16213,2023.
[77] ZhengyiWang,YikaiWang,YifeiChen,ChendongXiang,ShuoChen,DajiangYu,ChongxuanLi,Hang
Su,andJunZhu. Crm:Singleimageto3dtexturedmeshwithconvolutionalreconstructionmodel. arXiv
preprintarXiv:2403.05034,2024.
[78] XinyueWei,FanboXiang,SaiBi,AnpeiChen,KalyanSunkavalli,ZexiangXu,andHaoSu.NeuManifold:
NeuralWatertightManifoldReconstructionwithEfficientandHigh-QualityRenderingSupport. arXiv
preprint,2023.
[79] XinyueWei,KaiZhang,SaiBi,HaoTan,FujunLuan,ValentinDeschaintre,KalyanSunkavalli,Hao
Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality mesh. arXiv preprint
arXiv:2404.12385,2024.
[80] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, C. L. Philip Chen, and Lei Zhang.
Consistent123:Improveconsistencyforoneimageto3dobjectsynthesis,2023.
14[81] SangminWoo,ByeongjunPark,HyojunGo,Jin-YoungKim,andChangickKim. Harmonyview:Harmo-
nizingconsistencyanddiversityinone-image-to-3d,2023.
[82] RobertJ.Woodham. Photometricmethodfordeterminingsurfaceorientationfrommultipleimages,page
513–531. MITPress,Cambridge,MA,USA,1989.
[83] TongWu,JiaruiZhang,XiaoFu,YuxinWang,JiaweiRen,LiangPan,WayneWu,LeiYang,JiaqiWang,
ChenQian,etal.Omniobject3d:Large-vocabulary3dobjectdatasetforrealisticperception,reconstruction
andgeneration. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages803–814,2023.
[84] Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler,
and Xiaohui Zeng. Latte3d: Large-scale amortized text-to-enhanced3d synthesis. arXiv preprint
arXiv:2403.15385,2024.
[85] JialeXu,WeihaoCheng,YimingGao,XintaoWang,ShenghuaGao,andYingShan.Instantmesh:Efficient
3dmeshgenerationfromasingleimagewithsparse-viewlargereconstructionmodels. arXivpreprint
arXiv:2404.07191,2024.
[86] YinghaoXu,ZifanShi,WangYifan,SidaPeng,CeyuanYang,YujunShen,andWetzsteinGordon. Grm:
Largegaussianreconstructionmodelforefficient3dreconstructionandgeneration. arxiv:2403.14621,
2024.
[87] YinghaoXu,HaoTan,FujunLuan,SaiBi,PengWang,JiahaoLi,ZifanShi,KalyanSunkavalli,Gordon
Wetzstein,ZexiangXu,etal. Dmv3d:Denoisingmulti-viewdiffusionusing3dlargereconstructionmodel.
arXivpreprintarXiv:2311.09217,2023.
[88] HaoYang,LanqingHong,AoxueLi,TianyangHu,ZhenguoLi,GimHeeLee,andLiweiWang.Contranerf:
Generalizableneuralradiancefieldsforsynthetic-to-realnovelviewsynthesisviacontrastivelearning. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages16508–
16517,2023.
[89] JianglongYe,PengWang,KejieLi,YichunShi,andHengWang. Consistent-1-to-3:Consistentimageto
3dviewsynthesisviageometry-awarediffusionmodels. arXivpreprintarXiv:2310.03020,2023.
[90] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa. pixelnerf:Neuralradiancefieldsfromoneor
fewimages. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages4578–4587,2021.
[91] WangboYu,LiYuan,Yan-PeiCao,XiangjunGao,XiaoyuLi,WenboHu,LongQuan,YingShan,and
YonghongTian. Hifi-123:Towardshigh-fidelityoneimageto3dcontentgeneration,2024.
[92] KaiZhang,SaiBi,HaoTan,YuanboXiangli,NanxuanZhao,KalyanSunkavalli,andZexiangXu. Gs-lrm:
Largereconstructionmodelfor3dgaussiansplatting. arXiv,2024.
[93] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusion
models. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages3836–3847,
2023.
[94] Xin-Yang Zheng, Hao Pan, Yu-Xiao Guo, Xin Tong, and Yang Liu. Mvd2: Efficient multiview 3d
reconstructionformultiviewdiffusion. InSIGGRAPH,2024.
[95] Xin-YangZheng,HaoPan,Peng-ShuaiWang,XinTong,YangLiu,andHeung-YeungShum. Locally
attentionalsdfdiffusionforcontrollable3dshapegeneration. ACMTransactionsonGraphics(ToG),
42(4):1–13,2023.
[96] Zi-XinZou,ZhipengYu,Yuan-ChenGuo,YangguangLi,DingLiang,Yan-PeiCao,andSong-HaiZhang.
Triplanemeetsgaussiansplatting:Fastandgeneralizablesingle-view3dreconstructionwithtransformers,
2023.
15A SupplementalMaterial
A.1 ComparisonwithInstant3D
InFigure6,weshowcasethecomparisonwithInstant3D[27]onthetext-to-3Dtask. Theresults
areobtainedfromthepaperauthors. WhileInstant3D[27]alsogenerates3Dshapesthatmatchthe
inputtextprompt,ourmethodgeneratesresultswithsuperiormeshqualityandfine-grained,sharp
geometricdetails.
Figure6: Application: Text-to-3D.ComparisonwithInstant3D[27].
A.2 TriplaneArtifacts
Figure7: Thetriplane-basedmethodMeshLRM[79]hasdifficultycapturingwordsonobjects,even
whengroundtruthmulti-viewRGBimagesareusedasinput.
16Figure8: Ablationstudyoninputnormalmaps. EvaluatedontheGSOdataset[11]. “w/onormal”
indicatesthatthemodelistrainedwithmulti-viewRGBimagesonly. “w/predictednormal”indicates
thatthemodelistrainedwithgroundtruthnormalmapsbutevaluatedwithpredictednormalsby
Zero123++[59]. “w/GTnormal”indicatesthatthemodelistrainedandtestedwithgroundtruth
normals.
Table4: Normalconsistency(angleerror)betweenthemeshgeometry(meshvertexnormals)andthe
predictednormalmaps,bothbeforeandafterthegeometryenhancementpost-processing. Theratio
ofmeshverticesbelowaspecificerrorthresholdisshown. EvaluatedontheGSOdataset.
angleerrorthreshold before after
<1◦ 8.83% 16.27%
<2◦ 26.39% 40.83%
<5◦ 60.55% 73.19%
<10◦ 78.79% 86.43%
<15◦ 86.46% 91.29%
AsshowninFig.7,MeshLRM[79]hasdifficultycapturingwordsonobjects,evenwhengroundtruth
multi-viewRGBimagesareusedasinput. Wespeculatethatthisisduetothelimitednumberof
triplanepatches(e.g.,32×32×3)restrictedbyglobalattention. Incontrast,ourmethodleverages
sparsevoxelsandsupportsamuchhigherfeatureresolutionof2563,makingitfreefromsuchissues.
A.3 AblationStudy: InputNormalMaps
InFigure8,wequalitativelydemonstratetheeffectofinputnormalmaps. Whenthemodelistrained
withoutmulti-viewnormalmaps,wefindthatthegeneratedmodelcanonlycapturetheglobal3D
shapebutfailstogeneratefine-grainedgeometricdetails.However,whenthemodelisgivenpredicted
normalmaps,theperformanceissignificantlybetter,althoughtherearestillsomesmallgapswhen
comparedtotheresultsofgroundtruthnormals(seethebreadholeofthetoasterandthewheelof
thetram). Thisindicateserrorsorinconsistenciesfromthe2Dnormalpredictionmodels.
A.4 AblationStudy: GeometryEnhancement
We propose asking the network to predict an additional normal texture, which can be used for
furthergeometricenhancementbyapplyingatraditionalalgorithmaspost-processing. Thegeometric
enhancementaimstoalignthemeshgeometrywiththepredictednormalmapbyadjustingthevertex
locations. However,thetraditionalalgorithmweusedcannotguaranteethatthemeshnormalswill
17Table5: Analysisofourmeshgenerationqualityovertrainingtime. EvaluatedontheGSO[11]
dataset.
TrainingTime PSNR-C↑ LPIPS-C↓ PSNR-N↑ LPIPS-N↓ CD↓ F-Score↑
8×H10012h 21.28 0.2135 22.89 0.1536 0.0330 0.960
8×H10024h 21.32 0.2076 22.96 0.1516 0.0320 0.960
8×H10048h 21.41 0.2033 23.01 0.1484 0.0317 0.960
8×H100120h 21.44 0.2029 23.04 0.1480 0.0314 0.961
8×H100168h 21.47 0.2010 23.09 0.1466 0.0313 0.963
befullyalignedwiththepredictednormalmapsafterprocessing. Thislimitationarisesbecausethe
algorithmoperatesinlocalspaceandavoidslargevertexdisplacements. Moreover,thepredicted
normalmapsmaycontainerrorsorinconsistencies,suchasconflictingneighboringnormals. The
adoptedalgorithmisaniterativenumericaloptimizationmethodanddoesnotcomputeananalytic
solution.
However,wehavequantitativelyverifiedthatthepost-processingmodulecansignificantlyimprove
normalconsistencywiththepredictednormalmap. Forexample,beforepost-processing,only26.4%
ofmeshverticeshadanormalangleerroroflessthan2degrees. Afterpost-processing,thisnumber
increasedto40.8%. Fora10-degreethreshold,theratioincreasesfrom78.8%to86.4%. Formore
details,pleaserefertoTable4.
A.5 AblationStudy: Trainingtime
OurMeshFormercanbetrainedefficientlyusingonly8GPUs,typicallyconverginginapproximately
twodays. Table5presentsaquantitativeanalysisofourmeshgenerationqualityoverthetraining
period. Weobservethatperformanceimprovesrapidlyandnearlyconverges,withonlymarginal
changesoccurringafterthetwo-daytrainingperiod.
A.6 TrainingDetailsandEvaluationMetrics
TrainingDetails: Wetrainedthemodelusingasubsetof395k3DshapesfilteredfromtheObja-
verse[9]dataset. TheseobjectshaveadistributableCreativeCommonslicenseandwereobtainedby
theObjaverseteamusingSketchfab’spublicAPI.Foreachfiltered3Dshape,werandomlyrotated
themeshandgenerated10datasamples. Foreachdatasample,wecomputea5123 groundtruth
SDFvolumeusingaCUDA-basedprogramandrendermulti-viewRGBandnormalimagesusing
BlenderProc. Inourexperiments,theresolutionsoftheoccupancyvolumeandsparsefeaturevolume
are64and256,respectively. TheresolutionofthepredictedandgroundtruthSDFvolumesis512.
ThemodelistrainedwiththeAdamoptimizerandacosinelearningratescheduler. Thelossweights
λ ,··· ,λ aresetto80,2,16,2,8,and8,respectively.
1 6
Alldatapreparation,includingimagerenderingandSDFcomputation,isperformedusinganinternal
cluster. Thisprocesscanbecompletedusing4000CPUcoresinroughlyoneweek. Thegenerated
datatakesupapproximately30TB.Allmodeltrainingtasksareconductedinpubliccloudclusters.
Ourmainmodelistrainedusing8H100GPUsforoneweek. Allexperimentslistedinthepapercan
becompletedin15daysusing32H100GPUs(runningmultipleparallelexperiments),excludingthe
preliminaryexplorationexperiments.
ArchitectureDetails: ForVoxelFormer,theUNetconsistsoffourlevelswithresolutionsof643,323,
163and163. EachlevelincludesaResNetmodule,aprojection-awarecross-attentionmodule,anda
downsamplingmodule,withchannelsizesof64,128,256,and512. Weadded6transformerlayers
atthebottleneckoftheUNet,witheach3Dvoxeltreatedasatoken,andtokenchannelssetto512.
ForSparseVoxelFormer,thesparseUNetconsistsofsixlevelswithresolutionsof2563,1283,643,
323,163,and163. EachlevelincludesasparseResNetmodule,aprojection-awarecross-attention
module,andadownsamplingmodule,withchannelsizesof16,32,64,128,512,and2,048. We
added16transformerlayersatthebottleneckoftheUNet,witheach3Dsparsevoxeltreatedasa
token,andtokenchannelssetto1,024. Thefeaturedimensionoftheoutputsparsefeaturevolume
(beforetheMLP)is32.
18Figure9: QualitativeResultsofOne-2-3-45++[31]andCRM[77]onSingleImageto3D.Both
thetexturedandtexturelessmeshrenderingsareshown.
Forbothofthem,askipconnectionisaddedtotheUNet.
Evaluation Metrics: To account for the scale and pose ambiguity of the generated mesh from
differentbaselines,wealignthepredictedmeshwiththegroundtruthmeshpriortotheevaluation
metric calculation. This alignment process involves uniformly sampling rotations and scales for
initializationandsubsequentlyrefiningthealignmentusingtheIterativeClosestPoint(ICP)algorithm.
We select the alignment that yields the highest inlier ratio. Both the ground truth and predicted
meshesarethenscaledtofitwithinaunitboundingbox.
For3Dmetrics,wesample100,000pointsonboththegroundtruthmeshandthepredictedmeshand
computetheF-scoreandChamferdistance,settingtheF-scorethresholdat0.05. Toevaluatetexture
quality,wecomputethePSNRandLPIPSbetweenimagesrenderedfromthereconstructedmeshand
thoseofthegroundtruth. FollowingInstantMesh[85],wesample24cameraposes,encompassing
afull360-degreeviewaroundtheobject,andutilizeBlenderProcforrenderingRGBandnormal
imageswitharesolutionof320x320. SinceweusetheVGGmodelforLPIPSlosscalculationduring
training,weemploytheAlexmodelforLPIPSlosscalculationduringevaluation.
A.7 TrainingDetailsofMeshLRM
All results of MeshLRM, except those in Table 2, were reproduced by the MeshLRM authors at
Hillbotfollowingtheoriginalsettingsasdescribedinthepaper. FortheresultsinTable2,wetrained
themodelusingthesametrainingdataasourmethodon8×H100GPUsfor48hours. Wemaintained
thesamebatchsizeasreportedinthepaperandproportionallyscaleddowntheoriginaltrainingtime
foreachstageofMeshLRMbasedonatotaltrainingtimeof48hours. Thisincluded5.8secondsper
iterationfor20,000iterationsinthe256-resolutionpre-training,12secondsperiterationfor4,000
iterationsinthe512-resolutionfine-tuning,and4.7secondsperiterationfor4,000iterationsinmesh
refinement.
A.8 QualitativeExamplesofOne-2-3-45++andCRM
Figure9showsqualitativeresultsofOne-2-3-45++[31]andCRM[77]onsingleimageto3Dand
ourmethodproducesbetterresults.
19A.9 BroaderImpact
Weintroduceanefficientapproachfortrainingopen-worldsparse-viewreconstructionmodels,which
hasthepotentialtosignificantlyreduceenergyconsumptionandcarbonemissions,asbaselinemodels
typicallyrequiremuchmorecomputingresourcesfortraining. Previously,thecreationof3Dassets
wasreservedforspecializedartistswhospenthoursorevendaysproducingasingle3Dmodel. Our
proposed technique allows even novice individuals without specialized 3D modeling knowledge
tocreatehigh-quality3Dassetsinseconds. Thisdemocratizationof3Dmodelinghasunleashed
unprecedentedcreativepotentialandoperationalefficiencyacrossvarioussectors.
However, like other generative AI models, it also carries the risk of misuse, such as spreading
misinformationandcreatingpornographymodels. Therefore,itiscrucialtoimplementstrictethical
guidelinestomitigatetheserisks.
20