In-Context Learning with Representations:
Contextual Generalization of Trained Transformers
Tong Yang∗ Yu Huang† Yingbin Liang‡ Yuejie Chi§
CMU UPenn OSU CMU
August 20, 2024
Abstract
In-context learning (ICL) refers to a remarkable capability of pretrained large language models, which
can learn a new task given a few examples during inference. However, theoretical understanding of
ICL is largely under-explored, particularly whether transformers can be trained to generalize to unseen
examples in a prompt, which will require the model to acquire contextual knowledge of the prompt for
generalization. Thispaperinvestigatesthetrainingdynamicsoftransformersbygradientdescentthrough
thelensofnon-linearregressiontasks. Thecontextualgeneralizationherecanbeattainedvialearningthe
template function for each task in-context, where all template functions lie in a linear space with m basis
functions. We analyze the training dynamics of one-layer multi-head transformers to in-contextly predict
unlabeledinputsgivenpartiallylabeledprompts,wherethelabelscontainGaussiannoiseandthenumber
of examples in each prompt are not sufficient to determine the template. Under mild assumptions, we
showthatthetraininglossforaone-layermulti-headtransformerconvergeslinearlytoaglobalminimum.
Moreover, the transformer effectively learns to perform ridge regression over the basis functions. To our
knowledge, this study is the first provable demonstration that transformers can learn contextual (i.e.,
template) information to generalize to both unseen examples and tasks when prompts contain only a
small number of query-answer pairs.
Keywords: transformers, in-context learning, contextual generalization
Contents
1 Introduction 2
1.1 Our contributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Problem Setup 5
3 Theoretical Analysis 6
3.1 Training time convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Inference time performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.3 Further interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4 Experiments 10
5 Conclusion 12
∗DepartmentofElectricalandComputerEngineering,CarnegieMellonUniversity;email: tongyang@andrew.cmu.edu.
†DepartmentofStatisticsandDataScience,WhartonSchool,UniversityofPennsylvania;email: yuh42@wharton.upenn.edu.
‡DepartmentofElectricalandComputerEngineering,TheOhioStateUniversity;email: liang.889@osu.edu.
§DepartmentofElectricalandComputerEngineering,CarnegieMellonUniversity;email: yuejiechi@cmu.edu.
1
4202
guA
91
]GL.sc[
1v74101.8042:viXraA Proof Preparation 16
A.1 Summary of key notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Auxiliary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B Proof of Theorem 1 17
C Proof of Theorem 2 23
D Proof of Key Lemmas 24
D.1 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.2 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.3 Proof of Lemma 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.4 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.5 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
1 Introduction
Transformers [Vaswani et al., 2017] have achieved tremendous successes in machine learning, particularly
in natural language processing, by introducing self-attention mechanisms that enable models to capture
long-range dependencies and contextualized representations. In particular, these self-attention mechanisms
endow transformers with remarkable in-context learning (ICL) capabilities, allowing them to adapt to new
tasks or domains by simply being prompted with a few examples that demonstrate the desired behavior,
without any explicit fine-tuning or updating of the model’s parameters [Brown et al., 2020].
A series of papers have empirically studied the underlying mechanisms behind in-context learning in
transformer models [Garg et al., 2022, Von Oswald et al., 2023, Wei et al., 2023, Olsson et al., 2022, Xie
et al., 2021, Chen and Zou, 2024, Agarwal et al., 2024], which have shown that transformers can predict
unseen examples after being prompted on a few examples. The pioneering work of Garg et al. [2022] showed
empirically that transformers can be trained from scratch to perform in-context learning of simple function
classes, providing a theoretically tractable in-context learning framework. Following this well-established
framework, several works have investigated various properties of in-context learning in transformers. For
instance, studies have explored generalization and stability [Li et al., 2023], expressive power [Bai et al., 2023,
Akyürek et al., 2022, Giannou et al., 2023], causal structures [Nichani et al., 2024, Edelman et al., 2024],
statistical properties [Xie et al., 2021, Jeon et al., 2024], to name a few.
In particular, analysis from an optimization perspective can provide valuable insights into how these
models acquire and apply knowledge that enable in-context learning. A few works [Huang et al., 2023, Chen
et al., 2024, Li et al., 2024, Nichani et al., 2024] thus studied the training dynamics of shallow transformers
with softmax attention in order to in-context learn simple tasks such as linear regression [Huang et al., 2023,
Chen et al., 2024], binary classification tasks [Li et al., 2024], and causal graphs [Nichani et al., 2024]. Their
theoretical analyses illuminated how transformers, given an arbitrary query token, learn to directly apply
the answer corresponding to it from the query-answer pairs that appear in each prompt. Therefore, they all
require the sequence length of each prompt to be large enough so that all query-answer pairs have been seen
in each prompt with sufficiently high probability, whereas practical prompts are often too short to contain
many query examples. This suggests that in-context learning can exploit inherent contextual information of
the prompt to generalize to unseen examples, which further raise the following intriguing theoretical question:
How do transformers learn contextual information from more general function classes to predict unseen
examples given prompts that contain only partial examples?
Since our paper studies ICL of non-linear function regression, the function mapping (which we also term
as “template”) naturally serves as the “contextual information” that can be learned for generalization to
unseen examples. When each prompt contains only a small number of (noisy) examples, the template that
generates the labels may be underdetermined, i.e., multiple templates could generate the same labels in the
prompt. Such an issue of underdetermination further raises a series of intriguing questions, such as:
When the template that generates a prompt is underdetermined, what is the transformer’s preference for
choosing the template and how good is such a choice?
21.1 Our contributions
In this paper, we answer the above questions by analyzing the training dynamics of a one-layer transformer
withmulti-headsoftmaxattentionthroughthelensofnon-linearregressiontasks. Inoursetting,thetemplate
function for each task lies in the linear space formed by m nearly-arbitrary basis functions that capture
representation (i.e., features) of data. Our goal is to provide insights on how transformers trained by gradient
descent (GD) acquire template information from more general function classes to generalize to unseen
examples and tasks when each prompt contains only a small number of query-answer pairs. We summarize
our contributions are as follows.
• Wefirstestablishtheconvergenceguaranteeofaone-layertransformerwithmulti-headsoftmaxattention
trained with gradient descent on general non-linear regression in-context learning tasks. We assume
each prompt contains only a few (i.e., partial) examples with their Gaussian noisy labels, which are not
sufficient to determine the template. Under mild assumptions, we establish that the training loss of
the transformer converges at a linear rate. Moreover, by analyzing the limit point of the transformer
parameters, we are able to uncover what information about the basic tasks the transformer extracts
and memorizes during training in order to perform in-context prediction.
• We then analyze the transformer’s behavior at inference time after training, and show that the
transformer chooses its generating template by performing ridge regression over the basis functions. We
also provide the iteration complexity for pretraining the transformer to reach ε-precision with respect
to its choice of the template given an arbitrary prompt at inference time. We further compare the
choice of the transformer and the best possible choice over the template class and characterize how the
sequence length of each prompt influences the inference time performance of the model.
• Undermorerealisticassumptions,ouranalysisframeworkallowsustoovercomeahandfulofassumptions
made in previous works such as large prompt length [Huang et al., 2023, Chen et al., 2024, Li et al.,
2024, Nichani et al., 2024], orthogonality of data [Huang et al., 2023, Chen et al., 2024, Li et al., 2024,
Nichani et al., 2024], restrictive initialization conditions [Chen et al., 2024], special structure of the
transformer [Nichani et al., 2024], and mean-field models [Kim and Suzuki, 2024]. Further, the function
classes we consider are a generalization of those considered in most theoretical works [Huang et al.,
2023, Chen et al., 2024, Li et al., 2024, Wu et al., 2023, Zhang et al., 2023a]. We also highlight the
importance of multi-head attention mechanism in this process.
To our best knowledge, this is the first work that analyzes how transformers learn contextual (i.e.,
template) information to generalize to unseen examples and tasks when prompts contain only a small number
of query-answer pairs. Table 1 provides a detailed comparison with existing theoretical works in terms of
settings, training analysis and generalization of in-context learning.
nonlinear multi task GD noisy representation
Reference
attention head shift convergence data learning
Wu et al. [2023] ✗ ✗ ✓ ✓ ✓ ✗
Zhang et al. [2023a] ✗ ✗ ✓ ✓ ✓ ✗
Huang et al. [2023] ✓ ✗ ✓ ✓ ✗ ✗
Li et al. [2024] ✓ ✗ ✓ ✓ ✓ ✗
Chen et al. [2024] ✓ ✓ ✗ ✗ ✓ ✗
Kim and Suzuki [2024] ✗ ✗ ✓ ✗ ✗ ✓
Ours ✓ ✓ ✓ ✓ ✓ ✓
Table 1: Comparisons with existing theoretical works that study the learning dynamics of transformers in
ICL. Here, the last column refers to the fact that the response in the regression task is generated by a linearly
weighted unknown representation (feature) model.
31.2 Related work
In-context learning. Recent research has investigated the theoretical underpinnings of transformers’ ICL
capabilities from diverse angles. For example, several works focus on explaining the in-context learning
of transformers from a Bayesian perspective [Xie et al., 2021, Ahuja et al., 2023, Han et al., 2023, Jiang,
2023, Wang et al., 2023, Wies et al., 2024, Zhang et al., 2023b, Jeon et al., 2024, Hahn and Goyal, 2023]. Li
et al. [2023] analyzed the generalization and stability of transformers’ in-context learning. Focusing on the
representation theory, Akyürek et al. [2022], Bai et al. [2023] studied the expressive power of transformers
on the linear regression task. Akyürek et al. [2022] showed by construction that transformers can represent
GD of ridge regression or the closed-form ridge regression solution. Bai et al. [2023] extended Akyürek et al.
[2022] and showed that transformers can implement a broad class of standard machine learning algorithms
in-context. Dai et al. [2022], Von Oswald et al. [2023] showed transformers could in-context learn to perform
GD.
More pertinent to our work, Guo et al. [2023] considered an ICL setting very similar to ours, where the
label depends on the input through a basis of possibly complex but fixed template functions, composed with
a linear function that differs in each prompt. By construction, the optimal ICL algorithm first transforms the
inputs by the representation function, and then performs linear ICL on top of the transformed dataset. Guo
et al. [2023] showed the existence of transformers that approximately implement such algorithms, whereas
our work is from a different perspective, showing that (pre)training the transformer loss by GD will naturally
yield a solution with the aforementioned desirable property characterized in Guo et al. [2023].
Training dynamics of transformers performing ICL. A line of work initiated by Garg et al. [2022]
aims to understand the ICL ability of transformers from an optimization perspective. [Zhang et al., 2023a,
Kim and Suzuki, 2024] analyzed the training dynamics of transformers with linear attention. Huang et al.
[2023], Chen et al. [2024], Li et al. [2024] studied the optimization dynamics of one-layer softmax attention
transformers performing simple in-context learning tasks, such as linear regression [Huang et al., 2023, Chen
et al., 2024] and binary classification [Li et al., 2024].
Among them, Huang et al. [2023] was the first to study the training dynamics of softmax attention, where
they gave the convergence results of a one-layer transformer with single-head attention on linear regression
tasks, assuming context features come from an orthogonal dictionary and each token in the prompts is drawn
from a multinomial distribution. In order to leverage the concentration property inherent to multinomial
distributions, they require the sequence length to be much larger than the size of dictionary. Their analysis
indicates that the prompt tokens that are the same as the query will have dominating attention weights,
which allows the transformer to copy-paste the correct answer from those prompt tokens.
Li et al. [2024] studied the training of a one-layer single-head transformer in ICL on binary classification
tasks. Same as Huang et al. [2023], they required the data to be pairwise orthogonal, and shared the same
copy-paste mechanism as in Huang et al. [2023]. To be precise, a fraction of their context inputs needs to
contain the same pattern as the query to guarantee that the total attention weights on contexts matching the
query pattern outweigh those on other contexts.
Chen et al. [2024] studied the dynamics of gradient flow for training a one-layer multi-head softmax
attention model for ICL of multi-task linear regression, where the coefficient matrix has certain spectral
properties. They required the sequence length to be sufficiently large [Chen et al., 2024, Assumption 2.1],
togetherwithrestrictiveinitializationconditions[Chenetal.,2024,Definition3.1]. Whileusingthecopy-paste
analysis framework as in Huang et al. [2023], Li et al. [2024], the attention probability vector in their work is
delocalized, so that the attention is spread out to capture the information from similar tokens in regression
tasks. Kim and Suzuki [2024] studied the dynamics of Wasserstein gradient flow for training a one-layer
transformer with an infinite-dimensional fully-connected layer followed by a linear attention layer for ICL of
linear regression, assuming infinite prompt length. Nichani et al. [2024] analyzed the optimization dynamics
of a simplified two-layer transformer with gradient descent on in-context learning a latent causal graph.
Notation. Boldface small and capital letters denote vectors and matrices, respectively. Sets are denoted
with curly capital letters, e.g., W. We let (Rd,∥·∥) denote the d-dimensional real coordinate space equipped
with norm ∥·∥. I is the identity matrix of dimension d. The ℓp-norm of v is denoted by ∥v∥ , where
d p
1≤p≤∞, and the spectral norm and the Frobenius norm of a matrix M are denoted by ∥M∥ and ∥M∥ ,
2 F
4T head
H
Value × softmax K e y × Q u e ry head 1 … …
…
…
Figure 1: The structure of a one-layer transformer with multi-head softmax attention.
respectively. M† stands for the Moore-Penrose pseudoinverse of matrix M, and M stands for its i-th
:,i
column vector. We let [N] denote {1,...,N}, and denote 1 to represent the all-one vector of length N, and
N
by 0 a vector or a matrix consisting of all 0’s. We allow the application of functions such as exp(·) to vectors
or matrices, with the understanding that they are applied in an element-wise manner. We use e to denote
i
the one-hot vector whose i-th entry is 1 and the other entries are all 0.
2 Problem Setup
In-context learning with representation. We consider ICL of regression with unknown representation,
similar to the setup introduced in Guo et al. [2023]. To begin, let f :Rd →Rm be a fixed representation map
that f(x)=(f (x),··· ,f (x))⊤ for any x∈Rd. The map f can be quite general, which can be regarded as
1 m
a feature extractor that will be learned by the transformer. We assume that each ICL task corresponds to a
map λ⊤f(·) that lies in the linear span of those m basis functions in f(·), where λ is generated according to
the distribution D . Thus, for each ICL instance, the (noisy) label of an input v (∀k ∈[K]) is given as
λ k
y =λ⊤(f(v )+ϵ ), λ∼D , ϵ i. ∼i.d. N(0,τI ) (1)
k k k λ k m
where τ >0 is the noise level.
The goal of ICL is to form predictions on query x given in-context labels of the form (1) on a few
query
inputs, known as prompts. In this paper, we use V to denote the dictionary set that contains all K unit-norm
distinct tokens, i.e., V := {v ,··· ,v } ⊂ Rd with each token ∥v ∥ = 1. We assume that each prompt
1 K k 2
P =P provides the first N tokens (with N ≪K) and their labels, and is embedded in the following matrix
λ
(cid:18) (cid:19) (cid:18) (cid:19)
v v ··· v V
EP := 1 2 N := ∈R(d+1)×N, (2)
y y ··· y y⊤
1 2 N
where
V :=(v ,··· ,v )∈Rd×N (3)
1 N
is the collection of prompt tokens, and y := (y ,··· ,y )⊤ is the prompt label. Given the prompt as the
1 N
input, the transformer predicts the labels for all the K tokens y ,··· ,y in the dictionary set.
1 K
Transformer architecture. We adopt a one-layer transformer with multi-head softmax attention [Chen
et al., 2024] — illustrated in Figure 1 — to predict the labels of all the tokens in the dictionary V, where H
is the number of heads. Denote the query embedding as
(cid:18) (cid:19)
v v ··· v
EQ := N+1 N+2 K ∈R(d+1)×(K−N), (4)
0 0 ··· 0
5and denote the embedding of both the prompt and the query as E :=(EP,EQ)∈R(d+1)×K. We define the
output of each transformer head as
(cid:16) (cid:17)
head (E):=WVEP ·softmax (EP)⊤(WK)⊤WQE , h∈[H], (5)
h h h h
where WQ ∈ Rde×(d+1), WK ∈ Rde×(d+1), and WV ∈ RK×(d+1) are the query, key, and value matrices,
h h h
respectively,andthesoftmaxisappliedcolumn-wisely,i.e.,givenavectorinputx,thei-thentryofsoftmax(x)
is given by exi/(cid:80) exj. The attention map of the transformer T(E) is defined as
j
head (E)
1
.
T(E):=WO . . ∈RK×K, (6)
 
head (E)
H
where WO is the output matrix. Following recent theoretical literature to streamline analysis [Huang et al.,
2023, Nichani et al., 2024, Deora et al., 2023, Chen et al., 2024], we assume that the embedding matrices take
the following forms:
WO :=(I ,··· ,I )∈RK×HK, WV :=(0,w )∈RK×(d+1), (7a)
K K h h
(cid:18) (cid:19)
Q 0
(WK)⊤WQ = h ∈R(d+1)×(d+1), ∀h∈[H], (7b)
h h 0 0
where w =(w ,··· ,w )⊤ ∈RK and Q ∈Rd×d are trainable parameters for all h∈[H].
h h,1 h,K h
The prediction of the labels is provided by the diagonal entries of T(E), which we denote by y =
(cid:98)
(y ,··· ,y )∈RK. Note that y takes the following form under our parameter specification:
(cid:98)1 (cid:98)K (cid:98)k
H
(cid:68) (cid:88) (cid:69)
∀k ∈[K]: y = y, w softmax(V⊤Q v ) . (8)
(cid:98)k h,k h k
h=1
Training via GD. Let θ = {Q ,w }H denote all trainable parameters of T. Let ϵ :=(ϵ ,··· ,ϵ ) ∈
h h h=1 1 K
Rm×K denote the noise matrix. Given training data over ICL instances, the goal of training is to predict
labels y for all v ∈V. Specifically, we train the transformer using gradient descent (GD) by optimizing the
k k
following mean-squared population loss:
(cid:34) K (cid:35)
L(θ):= 1 E 1 (cid:88) (y −y )2 . (9)
2 λ,ϵ K (cid:98)k k
k=1
We apply different learning rates η ,η >0 for updating {Q }H and {w }H , respectively, i.e., at the
Q w h h=1 h h=1
t-th (t≥1) step, we have
∀h∈[H]: Q(t) =Q(t−1)−η ∇ L(θ(t−1)), w(t) =w(t−1)−η ∇ L(θ(t−1)), (10)
h h Q Qh h h w wh
where θ(t) ={Q(t),w(t)}H is the parameter at the t-th step.
h h h=1
Inference time. At inference time, given a prompt P =P with N examples, where λ may not be in the
λ
support of the generation distribution D , the transformer applies the pretrained parameters and predicts the
λ
labels of all K tokens without further parameter updating.
3 Theoretical Analysis
3.1 Training time convergence
In this section, we show that the training loss L converges to its minimum value at a linear rate during
training, i.e., the function gap
∆(t) :=L(θ(t))−infL→0, t→∞ (11)
θ
at a linear rate, under some appropriate assumptions.
6Key assumptions. We first state our technical assumptions. The first assumption is on the distribution
D for generating the coefficient vector λ of the representation maps.
λ
Assumption1(AssumptionondistributionD ). Weassumethatin (1)eachentryλ isdrawnindependently
λ i
and satisfies E[λ ]=0 and E[λ2]=1 for all i∈[m].
i i
To proceed, we introduce the following notation:
Z :=(f(v )···f(v ))∈Rm×N, Z¯ :=(cid:0) Z⊤Z+mτI (cid:1)1/2 ∈RN×N, f¯ := max∥z¯∥ , (12)
1 N N max i 2
i∈[N]
where z¯ is the i-th column vector of Z¯ for i∈[N]. We further define C(t) (k ∈[K], t∈N ) and B(t) as
i k + k
follows:
C(t) :=softmax(V⊤Q(t)v ,··· ,V⊤Q(t)v )∈RN×H, B(t) =Z¯C(t) ∈RN×H. (13)
k 1 k H k k k
To guarantee the convergence, we require the initialization of the parameters θ(0) satisfies the following
condition.
Assumption 2 (Assumption on initialization). For all k ∈[K], B(0) has full row rank.
k
Before stating our main theorem, let us examine when the initialization condition in Assumption 2 is met.
Fortunately, we only require the following mild assumption on V to ensure our parameter initialization has
good properties.
Assumption 3 (Assumption on V). There exists one row vector x=(x ,··· ,x )⊤ of the prompt token
1 N
matrix V (cf. (3)) such that x ̸=x , ∀i̸=j.
i j
Assumption 3 implies that V has distinct tokens, i.e., v ≠ v when j ̸= k. It is worth noting that
j k
Assumption 3 is the only assumption we have on the dictionary V. In comparison, all other theoretical works
in Table 1 impose somewhat unrealistic assumptions on V. For example, Huang et al. [2023], Li et al. [2023],
Nichani et al. [2024] assume that the tokens are pairwise orthogonal, which is restrictive since it implies that
the dictionary size K should be no larger than the token dimension d, whereas in practice it is often the case
that K ≫d [Reid et al., 2024, Touvron et al., 2023]. In addition, Chen et al. [2024], Zhang et al. [2023a], Wu
et al. [2023] assume that each token is independently sampled from some Gaussian distribution, which also
does not align with practical scenarios where tokens are from a fixed dictionary and there often exist (strong)
correlations between different tokens.
The following proposition states that when the number of heads exceeds the number of prompts, i.e.
H ≥N, we can guarantee that Assumption 2 holds with probability 1 by simply initializing {Q }H using
h h=1
Gaussian distribution.
Proposition 1 (Initialization of {Q }H ). Suppose Assumptions 1, 3 hold and H ≥ N. For any fixed
h h=1
β >0, let Q(0)(i,j)i. ∼i.d. N(0,β2), then Assumption 2 holds almost surely.
h
Proof. See Appendix D.1.
Choice of learning rates. Define
(cid:110) (cid:16) (cid:17)(cid:111)
ζ := min λ B(0)B(0)⊤ , (14)
0 min k k
k∈[K]
where ∆(0) is the initial function gap (c.f. (11)). Assumption 2 indicates that ζ >0. Let γ be any positive
0
constant that satisfies
(cid:32) √ (cid:33)1/2
γ ≥ζ 0−5/4 √12 28 −2 1(cid:13) (cid:13)Z¯(cid:13) (cid:13)2 2√ Hf¯ maxK3/2∆(0) . (15)
We set the learning rates as
η ≤1/L and η =γ2η , (16)
Q w Q
7where1
L2 =(cid:18) 8√ 2H√ K∥Z¯∥2 2√ ∆(0)+1+ ∥Z⊤Z(cid:98)∥ 2(cid:19)2 (cid:13) (cid:13)Z¯(cid:13) (cid:13)4 ·(cid:16) 8γ2+ 4096K2N∆(0)(cid:17)
ζ0 mτ 2 K γζ 02
+2H2(cid:13) (cid:13)Z¯(cid:13) (cid:13)4(cid:16) γ4 + 16384K3(cid:13) (cid:13)Z¯(cid:13) (cid:13)2(cid:0) ∆(0)(cid:1)2(cid:17) . (17)
2 K2 γζ4 2
0
Theoretical guarantee. Now we are ready to state our first main result, regarding the training dynamic
of the transformer.
Theorem 1 (Training time convergence). Suppose Assumptions 1, 2 hold. We let w(0) = 0 and set the
k
learning rates as in (16). Then we have
(cid:18)
η ζ
(cid:19)t(cid:18) (cid:19)
L(θ(t))−infL(θ)≤ 1− w 0 L(θ(0))−infL(θ) , ∀t∈N. (18)
θ 2K θ
Proof. See Appendix B.
Theorem 1, together with Proposition 1, shows that the training loss converges to its minimum value
at a linear rate, under mild assumptions of the task coefficients and token dictionary. This gives the first
convergence result for transformers with multi-head softmax attention trained using GD to perform ICL
tasks (see Table 1). Our convergence guarantee (18) also indicates that the convergence speed decreases as
the size K of the dictionary or the number H of attention heads increases, which is intuitive because training
with a larger vocabulary size or number of parameters is more challenging. However, a small H will limit the
expressive power of the model (see Section 3.3 for detailed discussion), and we require H ≥N to guarantee
Assumption 2 holds, as stated in Proposition 1.
3.2 Inference time performance
We now move to examine the inference time performance, where the coefficient vector λ corresponding to the
inference task may not drawn from D . In fact, we only assume that the coefficient vector λ at inference
λ
time is bounded as in the following assumption.
Assumption 4 (Boundedness of λ at inference time). We assume that at inference time ∥λ∥ ≤B for some
2
B >0.
For notational simplicity, let ZQ ∈Rm×(K−N) denote
ZQ :=(f(v ),··· ,f(v ))∈Rm×(K−N). (19)
N+1 K
The following theorem characterizes the performance guarantee of the transformer’s output y (after sufficient
(cid:98)
training) at the inference time.
Theorem 2 (Inference time performance). Let λ(cid:98) be the solution to the following ridge regression problem:
(cid:110) (cid:111)
λ(cid:98) :=argmin
λ
21
N
(cid:80)N i=1(y i−λ⊤f(v i))2+ m 2Nτ ∥λ∥2
2
. (20)
Under the assumptions in Theorem 1, for any ε > 0 and δ ∈ (0,1), if the number of training iterates T
satisfies
log(cid:18) B2∆(0)(cid:18)
∥Z∥
+√ τ(cid:16) 2√ Nlog(1/δ)+2log(1/δ)+N(cid:17)1/2(cid:19)2(cid:46) (mτε)(cid:19)
2
T ≥ , (21)
log(1/(1−ηwζ0))
2K
then given any prompt P that satisfies Assumption 4 at the inference time, with probability at least 1−δ, the
output of the transformer y satisfies
(cid:98)
1 (cid:18) y (cid:19)
∥y−y⋆∥2 ≤ε, with y⋆ := . (22)
2K (cid:98) (cid:98) 2 (cid:98) (cid:0) ZQ(cid:1)⊤ λ(cid:98)
1Weleaveatighter,butmorecomplicated,expressionofLintheappendix(cf. (61))intheappendixandpresentasimplified
forminthemainpaperforreadability.
8Proof. See Appendix C.
In Theorem 2, (22) shows that after training, the transformer learns to output the given labels of the first
N tokensineachprompt,andmoreimportantly,predictsthelabelsoftherestK−N tokensbyimplementing
theridgeregressiongivenin(20). NotethatAkyüreketal.[2022]studiedtheexpressivepoweroftransformers
on the linear regression task and showed by construction that transformers can represent the closed-form
ridge regression solution. Interestingly, here we show from an optimization perspective that transformers can
in fact be trained to do so.
Generalization capabilities of the pretrained transformer. Theorem 2 captures two generalization
capabilities that the pretrained transformer can have.
i) Contextual generalization to unseen examples: Theorem 2 suggests that the transformer exploits the
inherent contextual information (to be further discussed in Section 3.3) of the function template in the
given prompt, and can further use such information to predict the unseen tokens.
ii) Generalization to unseen tasks: Theorem 2 also suggests that the pretrained transformer can generalize
to a function map corresponding to any λ∈Rm at the inference time (albeit satisfying Assumption 4),
which is not necessarily sampled from the support of its training distribution D .
λ
We note that the contextual generalization that the transformer has here is different in nature from the
prediction ability shown in previous works on ICL [Huang et al., 2023, Chen et al., 2024, Li et al., 2024,
Nichani et al., 2024]. Those work focuses on a setting where each prompt contains a good portion of tokens
similartothequerytoken,allowingthetransformertodirectlyusethelabelofthecorrespondinganswersfrom
the prompt as the prediction. However, in practical scenarios, prompts often contain only partial information,
and our analysis sheds lights on explaining how transformers generalize to unseen examples by leveraging
ridge regression to infer the underlying template.
How does the representation dimension affect the performance? Beyond the above discovery,
several questions are yet to be explored. For instance, while we demonstrate that transformers can be trained
to implement ridge regression, how good is the performance of the ridge regression itself? What is the best
choice of ridge regression we could expect? How close is the transformer’s choice to the best possible choice?
We address these questions as follows.
Given any prompt P at inference time, since there is no label information about the rest K−N tokens,
the best prediction we could hope for from the transformer shall be
(cid:18) y (cid:19)
ybest := , (23)
(cid:98) (cid:0) ZQ(cid:1)⊤
λ(cid:98)τ
where ZQ is defined in (19), and λ(cid:98)τ satisfies:
(cid:104) (cid:105)
λ(cid:98)τ :=argmin λE
ϵ˜
21
N
(cid:80)N i=1(y i−λ⊤(f(v i)+ϵ i))2 . (24)
In other words, we hope the transformer outputs the given N labels as they are. For the rest K−N labels,
the best we could hope for is that the transformer estimates the coefficient vector λ by solving the above
regression problem to obtain λ(cid:98)τ, and predict the k-th label by λ(cid:98)⊤
τ
f(v k) for k =N +1,··· ,K. Note that
(24) is equivalent to the following ridge regression problem (see Lemma 4 in the appendix for its derivation):
(cid:110) (cid:111)
λ(cid:98)τ =argmin
λ
21
N
(cid:80)N i=1(y i−λ⊤f(v i))2+ τ
2
∥λ∥2
2
. (25)
The only difference between the two ridge regression problems (20) and (25) is the coefficient of the
regularization term. This indicates that at the training time, the transformer learns to implement ridge
regression to predict the labels of the rest K −N tokens, assuming the noise level is given by mτ. This
N
observationalsoreflectshowthesequencelengthN affectsthetransformer’spreferenceforchoosingtemplates
and its performance at inference time:
9• The closer m is to N, the closer the transformer’s choice of templates is to the best possible choice, and
the better the transformer’s prediction will be;
• When N <m, the transformer tends to underfit by choosing a λ with small ℓ -norm;
2
• When N > m, the transformer tends to overfit since it underestimates the noise level and in turn
captures noise in the prediction.
3.3 Further interpretation
We provide more interpretation on our results, which may lead to useful insights into the ICL ability of the
transformer.
How does the transformer gain ICL ability with representations? Intuitively speaking, our
pretrained transformer gains in-context ability by extracting and memorizing some “inherent information” of
all basic function maps f (i∈[m]) during the training. Such information allows it to infer the coefficient
i
vector λ from the provided labels in each prompt and calculate the inner product ⟨λ,f(v )⟩ to compute y
k k
given any token v ∈V at inference time. To be more specific, the “inherent information” of all basic tasks
k
could be described by the N-by-K matrix A defined as follows (see also (34)):
A:=(cid:0) Z⊤Z+mτI N(cid:1)−1(cid:16) Z⊤Z(cid:98)+(mτI N,0)(cid:17) ∈RN×K,
where Z(cid:98) :=(f(v 1),··· ,f(v K))=(Z,ZQ)∈Rm×K. During training, the transformer learns to approximate
A by (cid:80)H w softmax(V⊤Q v ) for each k ∈[K].
:,k h=1 h,k h k
To further elaborate, we take a closer look at the special case when the labels do not contain any noise,
i.e., τ =0, and N ≥m. In this case, A becomes Z†Z(cid:98), and given any prompt P =P λ, the coefficient vector
λ could be uniquely determined from the provided token-label pairs in the prompt. It is straightforward to
verify that the label of each token v could be represented by the inner product of the given label vector y
k
and the k-th column of Z†Z(cid:98), i.e.,
(cid:68) (cid:69)
y
k
= y,Z†Z(cid:98):,k . (26)
Comparing the above equation with (8), it can be seen that in order to gain the in-context ability, the
transformer needs to learn an approximation of Z†Z(cid:98):,k by (cid:80)H h=1w h,ksoftmax(V⊤Q hv k) for each k ∈[K].
More generally, in the proof of Theorem 2, we show that
y⋆ =⟨y,A ⟩, (27)
(cid:98)k :,k
comparing which with (8) suggests that a small training error implies that (cid:80)H w softmax(V⊤Q v ) is
h=1 h,k h k
closetoA . Infact, thisisthenecessaryandsufficientconditionforthetraininglosstobesmall. Arigorous
:,k
argument is provided in Lemma 5.
The necessity and trade-offs of multi-head attention mechanism. Multi-head attention mechanism
is essential in our setting. In fact, it is generally impossible to train a shallow transformer with only one
attention head to succeed in the ICL task considered in our paper. This is because, as we have discussed
above, the key for the transformer is to approximate A by (cid:80)H w softmax(V⊤Q v ) for each k ∈[K].
:,k h=1 h,k h k
If H =1, the transformer could not approximate each A by w softmax(V⊤Q v ) in general since the
:,k 1,k 1 k
entries of the latter vector are either all positive or all negative. In addition, Proposition 1 indicates that
when H ≥N, the weights of the transformer with a simple initialization method satisfy our desired property
that is crucial to guarantee the fast linear convergence. However, (18) implies that we should not set H to be
too large, since larger H yields slower convergence rate.
4 Experiments
This section aims to provide some empirical validation to our theoretical findings and verify that some of our
results could be generalized to deeper transformers.
10Training Loss Training Loss
Inference Loss (in-domain) Inference Loss (in-domain)
Inference Loss (ood) Inference Loss (ood)
101 101
100
100
101
101
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Iteration number Iteration number
(a) 1-layertransformer (b) 4-layertransformer
Figure 2: Training and inference losses of (a) 1-layer and (b) 4-layer transformers, which validate Theorem 2,
as well as the transformer’s contextual generalization to unseen examples and to unseen tasks.
Setup. We conduct experiments on a synthetic dataset, where we randomly generate each token v and
k
their representation f(v ) from standard Gaussian distribution. We employ both the 1-layer transformer
k
described in Section 2 and a standard 4-layer transformer in Vaswani et al. [2017] with d = 256 and
model
d =512. We set the training loss to be the population loss defined in (9), and initialize {Q(0)} using
ff h h∈[H]
standard Gaussian and set {w(0)} to be 0, identical to what is specified in Section 3. We generate
h h∈[H]
λ from standard Gaussian distribution to create the training set with 10000 samples and in-domain test
set with 200 samples; we also create an out-of-domain (ood) test set with 200 samples by sampling λ from
N(1 ,4I ). Given λ, we generate the label y of token v using (1), for k ∈[K]. We train with a batch
m m k k
size 256. All experiments use the Adam optimizer with a learning rate 1×10−4.
Training and inference performance. We set N =30, K =200, d=100, m=20, and set H to be 64
and 8 for 1-layer and 4-layer transformers, respectively. Figure 2 shows the training and inference losses of
both 1-layer and 4-layer transformers, where we measure the inference loss by 1∥y−y⋆∥2 to validate (22):
K (cid:98) (cid:98) 2
after sufficient training, the output of the transformer y converges to y⋆. From Figure 2 we can see that
(cid:98) (cid:98)
for both 1-layer and 4-layer transformers, the three curves have the same descending trend, despite the
inference loss on the ood dataset is higher than that on the in-domain dataset. This experiment also shows
the transformer’s contextual generalization to unseen examples and to unseen tasks, validating our claim in
Section 3.2.
Figure 3 plots the performance gap K1 (cid:13) (cid:13)y (cid:98)⋆−y (cid:98)best(cid:13) (cid:13)2
2
of the one-layer transformer with respect to different
N ranging from 50 to 150, when we fix m = 100 and τ = 0.01. This verifies that the ridge regression
implemented by the pretrained transformer has a better performance when m is close to N, again verifying
our claim at the end of Section 2.
Impact of the number of attention heads. We now turn to examine the impact of the number of
attention heads. In this experiment, we use the population loss (9), and set the other configurations same as
those in Figure 2. Figure 4 shows the training loss curves for different H with respect the iteration number,
which validates our claims. From Figure 4, we can see that we need to set H large enough to guarantee the
convergenceofthetrainingloss. However,settingH toolarge(H =400)leadstoinstabilityanddivergenceof
the loss. Recall that in Proposition 1, we require H ≥N to guarantee our convergence results hold. Although
this condition may not be necessary, Figure 4 shows that when H <N =30, the loss stopped descending
even when it is far from the minimal value. On the other side, the loss keeps descending when H = 30
(though slowly).
We also explore how H affects the training of the 4-layer transformer, as displayed in Figure 5, where we
set K =200 and the configurations other than H are the same as in Figure 3. We fix the wall-clock time to
11
ssoL ssoLm = 100
102
103
104
60 80 100 120 140
N (Number of examples in prompt)
Figure 3: The performance gap K1 (cid:13) (cid:13)y (cid:98)⋆−y (cid:98)best(cid:13) (cid:13)2
2
with different N when m=100, which validates that the
closer N is to m, the better the transformer’s prediction is.
101
100
101
H=1
H=5
102 H=10
H=20
H=30
H=40
103 H H= =5 10
50
H=400
0 100 200 300 400 500 600
Iteration number
Figure 4: Training losses of the 1-layer transformer with different number of attention heads H, where H
should be large enough to guarantee the convergence of the training loss, but setting H too large leads to
instability and slower divergence.
be 100 seconds and plot the training loss curves with different H. Figure 5 (a) shows the final training and
inference losses with respect to H. It reflects that the losses converge faster with smaller H (here the final
training loss is the smallest when H = 4). The training curves in Figure 5 (b) corresponding to different
H within 100s may provide some explanation to this phenomenon: (i) transformers with larger H could
complete less iterations within a fixed amount of time (the curves corresponding to larger H are shorter); (ii)
the training loss curves corresponding to large H (H =32,64) descend more slowly. This suggests our claim
that larger H may yield slower convergence rate is still valid on deeper transformers. Note that unlike the
1-layer transformer, deeper transformers don’t require a large H to guarantee convergence. This is because
deeper transformers have better expressive power even when H is small.
5 Conclusion
We analyze the training dynamics of a one-layer transformer with multi-head softmax attention trained by
gradient descent to solve complex non-linear regression tasks using partially labeled prompts. In this setting,
the labels contain Gaussian noise, and each prompt may include only a few examples, which are insufficient
to determine the underlying template. Our work overcomes several restrictive assumptions made in previous
studies and proves that the training loss converges linearly to its minimum value. Furthermore, we analyze
the transformer’s strategy for addressing the issue of underdetermination during inference and evaluate its
performance by comparing it with the best possible strategy. Our study provides the first analysis of how
transformers can acquire contextual (template) information to generalize to unseen examples when prompts
contain a limited number of query-answer pairs.
12
2 2||tseby
y||K1
ssoL
noitalupoP101 H=1
101 H=2
H=4
H=8
H=16
H=32
H=64
100
100
Training Loss
101 Inference Loss (in-domain)
Inference Loss (ood)
0 10 20 30 40 50 60 0 5 10 15 20 25 30 35
Number of Attention Heads (H) Iteration
(a) finallossesvsH (b) traininglosscurvesfordifferentH
Figure 5: Training losses of a 4-layer transformer with different H, fixing wall-clock time to be 100s. This
experiment shows that unlike 1-layer transformers, deeper transformers don’t require H to be large to
guarantee convergence of the loss.
Acknowledgement
The work of T. Yang and Y. Chi is supported in part by the grants NSF CCF-2007911, DMS-2134080 and
ONRN00014-19-1-2404. TheworkofY.LiangwassupportedinpartbytheU.S.NationalScienceFoundation
under the grants ECCS-2113860, DMS-2134145 and CNS-2112471.
References
R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, S. Chan, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes,
E. Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024.
K. Ahuja, M. Panwar, and N. Goyal. In-context learning through the bayesian prism. arXiv preprint
arXiv:2306.04891, 2023.
E.Akyürek,D.Schuurmans,J.Andreas,T.Ma,andD.Zhou. Whatlearningalgorithmisin-contextlearning?
investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.
Y. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: Provable in-context learning
with in-context algorithm selection. Advances in neural information processing systems, 36, 2023.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems,
33:1877–1901, 2020.
S. Chen, H. Sheen, T. Wang, and Z. Yang. Training dynamics of multi-head softmax attention for in-context
learning: Emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442, 2024.
X. Chen and D. Zou. What can transformer learn with varying depth? case studies on sequence learning
tasks. arXiv preprint arXiv:2404.01601, 2024.
D. Dai, Y. Sun, L. Dong, Y. Hao, S. Ma, Z. Sui, and F. Wei. Why can gpt learn in-context? language models
implicitly perform gradient descent as meta-optimizers. arXiv preprint arXiv:2212.10559, 2022.
P.Deora,R.Ghaderi,H.Taheri,andC.Thrampoulidis. Ontheoptimizationandgeneralizationofmulti-head
attention. arXiv preprint arXiv:2310.12680, 2023.
B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in self-attention
mechanisms. In International Conference on Machine Learning, pages 5793–5831. PMLR, 2022.
13
ssoL
laniF
ssoL
gniniarTB. L. Edelman, E. Edelman, S. Goel, E. Malach, and N. Tsilivis. The evolution of statistical induction heads:
In-context learning markov chains. arXiv preprint arXiv:2402.11004, 2024.
S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of
simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.
A. Giannou, S. Rajput, J.-y. Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped transformers as
programmable computers. In International Conference on Machine Learning, pages 11398–11442. PMLR,
2023.
T. Guo, W. Hu, S. Mei, H. Wang, C. Xiong, S. Savarese, and Y. Bai. How do transformers learn in-context
beyond simple functions? a case study on learning with representations. arXiv preprint arXiv:2310.10616,
2023.
M. Hahn and N. Goyal. A theory of emergent in-context learning as implicit structure induction. arXiv
preprint arXiv:2303.07971, 2023.
C. Han, Z. Wang, H. Zhao, and H. Ji. In-context learning of large language models explained as kernel
regression. arXiv preprint arXiv:2305.12766, 2023.
Y. Huang, Y. Cheng, and Y. Liang. In-context convergence of transformers. arXiv preprint arXiv:2310.05249,
2023.
H. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context learning. arXiv
preprint arXiv:2401.15530, 2024.
H. Jiang. A latent space theory for emergent abilities in large language models. arXiv preprint
arXiv:2304.09960, 2023.
H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under
the Polyak-Łojasiewicz condition. In European Conference on Machine Learning and Knowledge Discovery
in Databases, pages 795–811, 2016.
J. Kim and T. Suzuki. Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on
the attention landscape. In Forty-first International Conference on Machine Learning, 2024.
B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Annals of
statistics, pages 1302–1338, 2000.
H. Li, M. Wang, S. Lu, X. Cui, and P.-Y. Chen. Training nonlinear transformers for efficient in-context
learning: A theoretical learning and generalization analysis. arXiv preprint arXiv:2402.15607, 2024.
Y.Li, M.E.Ildiz, D.Papailiopoulos, andS.Oymak. Transformersasalgorithms: Generalizationandstability
in in-context learning. In International Conference on Machine Learning, pages 19565–19594. PMLR, 2023.
Q. N. Nguyen and M. Mondelli. Global convergence of deep networks with one wide layer followed by
pyramidal topology. Advances in Neural Information Processing Systems, 33:11961–11972, 2020.
E. Nichani, A. Damian, and J. D. Lee. How transformers learn causal structure with gradient descent. arXiv
preprint arXiv:2402.14735, 2024.
C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen,
et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.
M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou,
O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens
of context. arXiv preprint arXiv:2403.05530, 2024.
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,
S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
2023.
14A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in neural information processing systems, 30, 2017.
J.VonOswald,E.Niklasson,E.Randazzo,J.Sacramento,A.Mordvintsev,A.Zhmoginov,andM.Vladymyrov.
Transformerslearnin-contextbygradientdescent. InInternational Conference on Machine Learning, pages
35151–35174. PMLR, 2023.
X. Wang, W. Zhu, M. Saxon, M. Steyvers, and W. Y. Wang. Large language models are implicitly topic
models: Explaining and finding good demonstrations for in-context learning. In Workshop on Efficient
Systems for Foundation Models@ ICML2023, 2023.
J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, et al. Larger
language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023.
N. Wies, Y. Levine, and A. Shashua. The learnability of in-context learning. Advances in Neural Information
Processing Systems, 36, 2024.
J. Wu, D. Zou, Z. Chen, V. Braverman, Q. Gu, and P. L. Bartlett. How many pretraining tasks are needed
for in-context learning of linear regression? arXiv preprint arXiv:2310.08391, 2023.
S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian
inference. arXiv preprint arXiv:2111.02080, 2021.
R. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. arXiv preprint
arXiv:2306.09927, 2023a.
Y. Zhang, F. Zhang, Z. Yang, and Z. Wang. What and how does in-context learning learn? bayesian model
averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023b.
15A Proof Preparation
A.1 Summary of key notation
We summarize the frequently used notation in Table 2 for ease of reference.
notation meaning
K ∈N total number of tokens
+
d∈N token dimension
+
m∈N number of basic tasks
+
H ∈N number of attention heads
+
N ∈N number of examples in each prompt
+
v ∈Rd, k ∈[K] the k-th token
k
f :Rd →R, i∈[m] the i-th basic task
i
λ∈Rm coefficient vector
y =λ⊤(f(v )+ϵ ),k ∈[K] the k-th label
k k k
Table 2: Notation for key parameters.
A.2 Auxiliary lemmas
We provide some useful facts that will be repeatedly used later on. Let
z :=f(v )=(f (v ),f (v ),··· ,f (v ))⊤ ∈Rm, ∀k ∈[K].
k k 1 k 2 k m k
Recalling (12), we can rewrite
Z :=(z ,··· ,z )∈Rm×N.
1 N
We further define sh ∈RN as follows:
k
sh :=softmax(V⊤Q v )=(sh ,··· ,sh )⊤, ∀k ∈[K],h∈[H]. (28)
k h k 1k Nk
Lemma 1 (Softmax gradient). For all j ∈[N],k ∈[K] and h∈[H], we have
∂sh
jk =sh
(cid:88)N
sh (v −v )v⊤, (29)
∂Q jk ik j i k
h
i=1
where sh is defined in (28).
jk
Proof. See the proof of Lemma A.1 in Huang et al. [2023].
Lemma 2 (Smoothness of softmax). For vectors ξ ,ξ ∈Rl, we have
1 2
∥softmax(ξ )−softmax(ξ )∥ ≤2∥ξ −ξ ∥ . (30)
1 2 1 1 2 ∞
Proof. See Corollary A.7 in Edelman et al. [2022].
We also need to make use of the following form of Young’s inequality.
Lemma 3. For any x ,··· ,x ∈Rp, we have
1 l
(cid:13) (cid:13)2
(cid:13) (cid:13)(cid:88)l
x
(cid:13)
(cid:13)
≤l(cid:88)l
∥x ∥2. (31)
(cid:13) i(cid:13) i 2
(cid:13) (cid:13)
i=1 2 i=1
The following lemma shows the equivalence between (24) and (25).
16Lemma 4 (Equivalence of the regression problems). Given any prompt P :=(v ,y ,··· ,v ,y ), we have
λ 1 1 N N
the following equivalence:
(cid:34) N (cid:35) N
E 1 (cid:88) (y −λ⊤(f(v )+ϵ ))2 = 1 (cid:88) (y −λ⊤f(v ))2+ τ ∥λ∥2. (32)
ϵ 2N i i i 2N i i 2 2
i=1 i=1
Proof. See Appendix D.2.
B Proof of Theorem 1
We first outline the proof. To prove Theorem 1, we first remove the expectation in the expression of the
loss function L in (9) by reformulating it to a deterministic form (see Lemma 5). With this new form, we
show by induction that the loss function L is smooth (Lemma 10) and satisfies the Polyak-Łojasiewicz (PL)
condition (c.f. (49)). Provided with both smoothness and PL conditions, we are able to give the desired linear
convergence rate [Karimi et al., 2016].
We define
(cid:40)(cid:80)H w sh−(cid:0) Z⊤Z+mτI(cid:1)−1 (z +mτe ), if k ∈[N],
δθ := h=1 h,k k k k (33)
k (cid:80)H w sh−(cid:0) Z⊤Z+mτI(cid:1)−1 z , if k ∈[K]\[N].
h=1 h,k k k
We also define the following matrices:
A:=(cid:0) Z⊤Z+mτI N(cid:1)−1(cid:16) Z⊤Z(cid:98)+(mτI N,0)(cid:17) ∈RN×K, (34)
(cid:32) H H (cid:33)
(cid:88) (cid:88)
A(cid:98)(θ):= w h,1sh 1,··· , w h,Ksh
K
∈RN×K, (35)
h=1 h=1
where Z(cid:98) :=(z 1,··· ,z K)∈Rm×K.
We first reformulate the loss function to remove the expectation in the population loss.
Lemma 5 (Reformulation of the loss function). Under Assumption 1, the loss function L(θ) could be
rewritten into the following equivalent form:
L(θ)= 21
K
(cid:88)K (cid:13) (cid:13) (cid:13)(cid:0) Z⊤Z+mτI(cid:1)1/2 δ kθ(cid:13) (cid:13) (cid:13)2 2+L⋆ = 21
K
(cid:88)K (cid:13) (cid:13)Z¯δ kθ(cid:13) (cid:13)2 2+L⋆, (36)
k=1 k=1
where
N
L⋆ = 1 (cid:88)(cid:16) −(cid:0) Z⊤z +mτe (cid:1)⊤(cid:0) Z⊤Z+mτI(cid:1)−1(cid:0) Z⊤z +mτe (cid:1) +∥z ∥2+mτ(cid:17)
2K k k k k k 2
k=1
K
+ 1 (cid:88) (cid:16) −(cid:0) Z⊤z (cid:1)⊤(cid:0) Z⊤Z+mτI(cid:1)−1(cid:0) Z⊤z (cid:1) +∥z ∥2(cid:17)
2K k k k 2
k=N+1
is a constant that does not depend on θ, and Z¯ is defined in (12).
Proof. See Appendix D.3.
Lemma 5 indicates that L⋆ is a lower bound of L. We’ll later show that L⋆ is actually the infimum of L,
i.e., L⋆ =inf L(θ).
θ
Lemma 5 also indicates that, the necessary and sufficient condition for L(θ(t)) to converge to L⋆ during
training is
∀k ∈[K]: δθt →0, t→∞, (37)
k
which folllows immediately that (37) is equivalent to
A(cid:98)(θ(t))−A→0, t→∞. (38)
To simplify the analysis, we introduce the following reparameterization to unify the learning rates of all
parameters, and we’ll consider the losses after reparameterization in the subsequent proofs.
17Lemma 6 (Reparameterization). Define
(cid:113)
γ := η /η , α :=w /γ, ∀h∈[H], (39)
w Q h h
and let
ξ :={Q ,α }H , ℓ(ξ):=L(θ). (40)
h h h=1
Then (10) is equivalent to
ξ(t) =ξ(t−1)−η ∇ ℓ(ξ(t−1)), ∀t∈[T]. (41)
Q ξ
Proof. See Appendix D.4.
We denote α as α:=(α ) ∈RH×K.
h,k h∈[H],k∈[K]
The following lemma bounds the gradient norms by the loss function, which is crucial to the proof of
Theorem 1.
Lemma 7 (Upper bound of the gradient norms). Suppose Assumption 1 holds and |α(t)|≤α. Then for all
h,k
h∈[H], we have
(cid:13) (cid:13)
(cid:13)∂ℓ(ξ(t))(cid:13) √ (cid:113)
(cid:13) (cid:13) ≤2 2γαf¯ ℓ(ξ(t))−L⋆. (42)
(cid:13)
(cid:13)
∂Q(t) (cid:13)
(cid:13)
max
h F
Proof. See Appendix D.5.
Now we are ready to give the main proof.
Proof of Theorem 1. To prove Theorem 1, it suffices to prove that under our assumptions, we have
(cid:13) (cid:13)
(Upper bound of the parameters:) (cid:13)α(t)(cid:13) ≤α, (43a)
(cid:13) h (cid:13)
2
(cid:16) (cid:17) ζ
(Lower bound of eigenvalues:) λ B(t)B(t)⊤ ≥ 0, (43b)
min k k 2
(cid:16) η σ(cid:17)t(cid:16) (cid:17)
(Linear decay of the loss:) L(θ(t))−L⋆ ≤ 1− Q L(θ(0))−L⋆ , (43c)
2
where
ζ γ2 √
4(cid:13) (cid:13)Z¯(cid:13)
(cid:13) (cid:113)
σ := 0 , α:= 2K 2 L(θ(0))−L⋆, (44)
K γζ
0
and γ,α is defined in (39), ζ is defined in (14). We shall prove (43a),(43b) and (43c) by induction.
h 0
Base case. It is apparent that (43a),(43b) and (43c) all hold when t=0.
Induction. We make the following inductive hypothesis, i.e., when s∈[t−1], (43a), (43b) and (43c) hold.
Below we prove that (43a),(43b) and (43c) hold when s=t by the following steps.
Step 1: verify (43b) and the Polyak-Łojasiewicz condition. We first compute the gradient of the loss
w.r.t. α:
∀k ∈[K]: ∂ ∂ℓ α(ξ) = 21
K
∂α∂ (cid:13) (cid:13)Z¯δ kθ(cid:13) (cid:13)2
2
= 21
K
∂α∂ (cid:13) (cid:13)Z¯(γC kα k−A :k)(cid:13) (cid:13)2
2
k k k
= γ (cid:0) Z¯C (cid:1)⊤ Z¯δθ = γ B⊤Z¯δθ, (45)
K k k K k k
where the first equality follows from Lemma 5, C ,B is defined in (13).
k k
Let bh denote the h-th column vector of B , h∈[H], i.e., B :=(b1,··· ,bH), then for any k ∈[K] and
k k k k k
t∈N , we have
+
(cid:13) (cid:13) (cid:13)(bh k)(t)−(bh k)(0)(cid:13) (cid:13) (cid:13)
2
≤(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)(sh k)(t)−(sh k)(0)(cid:13) (cid:13) (cid:13)
2
18≤(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)(sh k)(t)−(sh k)(0)(cid:13) (cid:13) (cid:13)
1
≤2(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)V⊤(Q( ht)−Q( h0))v k(cid:13) (cid:13) (cid:13)
∞
≤2(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2jm ∈[a Nx ]|v j⊤(Q( ht)−Q( h0))v k|
≤2(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)Q( ht)−Q( h0)(cid:13) (cid:13) (cid:13)
F
, (46)
where the third line uses Lemma 2, and that
(cid:13) (cid:13)
∀h∈[H]:
(cid:13) (cid:13)Q(t)−Q(0)(cid:13)
(cid:13)
≤(cid:88)t−1 η(cid:13) (cid:13)∂ℓ(ξ(s))(cid:13)
(cid:13)
(cid:13) h h (cid:13) F (cid:13) (cid:13) ∂Q(s) (cid:13) (cid:13)
s=0 h F
≤(cid:88)t−1 2√
2ηγαf¯
(cid:113)
ℓ(ξ(s))−L⋆
max
s=0
≤2√
2ηγαf¯
(cid:113) L(θ(0))−L⋆(cid:88)t−1(cid:18)(cid:114)
1−
ησ(cid:19)s
max 2
√ s=0
8 2γαf¯ (cid:113)
≤ max L(θ(0))−L⋆, (47)
σ
wherethesecondinequalityfollowsfromLemma7(cf.(42))andthethirdinequalityfollowsfromtheinductive
hypothesis and the fact that ℓ(ξ(s))=L(θ(s)), ∀s. Combining (47) with (46), we have
(cid:118)
(cid:13) (cid:13) (cid:13)B k(t)−B k(0)(cid:13) (cid:13) (cid:13)
F
≤2(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:117) (cid:117) (cid:116)(cid:88)H (cid:13) (cid:13) (cid:13)Q( ht)−Q( h0)(cid:13) (cid:13) (cid:13)2
F
h=1√
≤(cid:13) (cid:13)Z¯(cid:13)
(cid:13)
√ H16 2γαf¯ max(cid:113)
L(θ(0))−L⋆
2 σ
(cid:16) √ (cid:17)(cid:112)
≤ 1−1/ 2 ζ , (48)
0
where the last inequality follows from (15). The above inequality (48) indicates that
∀x∈RK :
(cid:13) (cid:13)x⊤B(t)(cid:13)
(cid:13)
≥(cid:13) (cid:13)x⊤B(0)(cid:13)
(cid:13)
−(cid:13) (cid:13)x⊤(B(t)−B(0))(cid:13)
(cid:13)
≥(cid:112)
ζ /2,
(cid:13) k (cid:13) (cid:13) k (cid:13) (cid:13) k k (cid:13) 0
2 2 2
which gives (43b).
Therefore, we obtain the following PL condition:
(cid:13)
(cid:13)∇
ℓ(ξ(t))(cid:13) (cid:13)2 ≥(cid:88)K (cid:88)H (cid:18) ∂ℓ(ξ)(cid:19)2
=
γ2 (cid:88)K (cid:16) Z¯δ(t)(cid:17)⊤
B(t)B(t)⊤Z¯δ(t)
(cid:13) θ (cid:13) F ∂α h,k K2 k k k k
k=1h=1 k=1
≥
ζ 0γ2 (cid:88)K (cid:13) (cid:13)Z¯δ(t)(cid:13) (cid:13)2 =σ(cid:16) ℓ(ξ(t))−L⋆(cid:17)
, (49)
2K2 (cid:13) k (cid:13)
2
k=1
where the equality comes from (45), and the last equality follows from (36).
Step 2: verify the smoothness of the loss function. We first give the following lemma that bounds
the Lipschitzness of bh and δθ, which will be used later on. For notation simplicity, we let B,Q,α denote
k k
B(θ),Q(θ),α(θ), respectively, and let B′,Q′,α′ denote B(θ′),Q(θ′),α(θ′), respectively.
Lemma 8 (Lipschitzness of bh and δθ). For all k ∈[K] and h∈[H], and all transformer parameters θ,θ′,
k k
if max{|α |,|α′ |}≤α, then we have
h,k h,k
(cid:13) (cid:13)bh k(θ)−bh k(θ′)(cid:13) (cid:13)
2
≤2(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2∥Q h−Q′ h∥
F
, (50)
(cid:118)
(cid:13) (cid:13)δθ−δθ′(cid:13) (cid:13) ≤2γ√ Hα(cid:117) (cid:117) (cid:116)(cid:88)H ∥Q −Q′∥2 +γ√ H∥α −α′∥ . (51)
(cid:13) k k (cid:13) h h F k k 2
2
h=1
19Proof. (50) follows from a similar argument in (46). Regarding the Lipschitzness of δθ, we have
k
(cid:13) (cid:13)
(cid:13) (cid:13)δθ−δθ′(cid:13)
(cid:13)
=γ(cid:13) (cid:13)(cid:88)H
α
(sh(θ)−sh(θ′))+(cid:88)H
(α −α′
)sh(θ′)(cid:13)
(cid:13)
(cid:13) k k (cid:13) (cid:13) h,k k k h,k h,k k (cid:13)
2 (cid:13) (cid:13)
h=1 h=1 2
H H
≤γ(cid:88) |α h,k|(cid:13) (cid:13)sh k(θ)−sh k(θ′)(cid:13) (cid:13) 2+γ(cid:88) |α h,k−α h′ ,k|(cid:13) (cid:13)sh k(θ′)(cid:13) (cid:13)
2
h=1 h=1
(cid:118)
√ (cid:117) H √
≤2γ Hα(cid:117) (cid:116)(cid:88) ∥Q −Q′∥2 +γ H∥α −α′∥ ,
h h F k k 2
h=1
where we use (46) again to bound the first term in the second line, and use the fact that (cid:13) (cid:13)sh(θ′)(cid:13) (cid:13) ≤1 and
k 2
Cauchy-Schwarz inequality to bound the second term in the second line.
We also need the following lemma which bounds the norm of B and δθ.
k k
Lemma 9 (Upper bounds of bh and δθ). For all k ∈[K] and h∈[H], if max{|α |,|α′ |}≤α, then we
k k h,k h,k
have
(cid:13) (cid:13)bh(cid:13) (cid:13) ≤(cid:13) (cid:13)Z¯(cid:13) (cid:13) , (52)
k 2 2
(cid:13) (cid:13)δθ(cid:13) (cid:13) ≤γHα+∥A∥ , (53)
k 2 2
where A is defined in (34).
Proof. (52) follows from
(cid:13) (cid:13)bh(cid:13)
(cid:13)
≤(cid:13) (cid:13)Z¯(cid:13)
(cid:13)
(cid:13) (cid:13)sh(cid:13)
(cid:13)
≤(cid:13) (cid:13)Z¯(cid:13)
(cid:13) .
k 2 2 k 2 2
(53) follows from
H
(cid:13) (cid:13)δ kθ(cid:13) (cid:13)
2
≤γ(cid:88) |α h,k|(cid:13) (cid:13)sh k(cid:13) (cid:13) 2+∥Ae k∥
2
≤γHα+∥A∥ 2.
h=1
As a consequence of Lemma 8 and Lemma 9, For all k ∈ [K], and all transformer parameters θ,θ′, if
max{|α |,|α′ |}≤α, we have
h,k h,k
∥∇ ℓ(ξ)−∇ ℓ(ξ′)∥
αk αk 2
(4 =5) γ (cid:13) (cid:13)(B −B′)⊤Z¯δθ+B′⊤ Z¯(δθ−δθ′ )(cid:13) (cid:13)
K (cid:13) k k k k k k (cid:13) 2
≤ Kγ (cid:13) (cid:13)Z¯(cid:13) (cid:13) 2∥B k−B k′∥ F (cid:13) (cid:13)δ kθ(cid:13) (cid:13) 2+ Kγ (cid:13) (cid:13)Z¯(cid:13) (cid:13) 2∥B k′∥ F (cid:13) (cid:13) (cid:13)δ kθ−δ kθ′(cid:13) (cid:13) (cid:13)
2
(cid:118)
≤ Kγ ·2(cid:13) (cid:13)Z¯(cid:13) (cid:13)2 2(2γHα+∥A∥ 2)(cid:117) (cid:117) (cid:116)(cid:88)H ∥Q h−Q′ h∥2
F
+ γ K2 H(cid:13) (cid:13)Z¯(cid:13) (cid:13)2 2∥α k−α′ k∥ 2, (54)
h=1
from which we obtain the smoothness of the ℓ w.r.t. α as follows:
∥∇ ℓ(ξ)−∇
ℓ(ξ′)∥2
α α F
K
=(cid:88)
∥∇ ℓ(ξ)−∇
ℓ(ξ′)∥2
αk αk 2
k=1
≤2K(cid:16) Kγ ·2(cid:13) (cid:13)Z¯(cid:13) (cid:13)2 2(2γHα+∥A∥ 2)(cid:17)2(cid:88)H ∥Q h−Q′ h∥2
F
+2 Kγ4 2H2(cid:13) (cid:13)Z¯(cid:13) (cid:13)4 2∥α−α′∥2
F
h=1
≤2(cid:18) 1 (cid:16) 2γ(cid:13) (cid:13)Z¯(cid:13) (cid:13)2 (2γHα+∥A∥ )(cid:17)2 + γ4 H2(cid:13) (cid:13)Z¯(cid:13) (cid:13)4(cid:19) ∥ξ−ξ′∥2 , (55)
K 2 2 K2 2 2
20where the first inequality uses Young’s inequality (c.f. Lemma 3).
To obtain the smoothness of the loss function w.r.t. Q , we first note that by (82) we have
h
K N N
∂ℓ(ξ) = γ (cid:88)(cid:88)(cid:0) Z¯δθ(cid:1)⊤ z ·α sh (cid:88) sh (v −v )v⊤. (56)
∂Q K k j h,k jk ik j i k
h
k=1j=1 i=1
Therefore, if max{|α |,|α′ |}≤α, we have
h,k h,k
(cid:13) (cid:13) (cid:13) (cid:13)∂ ∂ℓ Q(ξ h) − ∂ ∂ℓ Q(ξ h′)(cid:13) (cid:13) (cid:13) (cid:13)
F
≤ 2γf K¯ max k(cid:88)K =1(cid:26) (cid:88) jN =1(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)δ kθ−δ kθ′(cid:13) (cid:13) (cid:13) 2·αsh jk(θ)(cid:88) i=N 1sh ik(θ)
+(cid:88)N (cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)δ kθ′(cid:13) (cid:13) (cid:13) 2|α h,k−α h′ ,k|sh jk(θ)(cid:88)N sh ik(θ)
j=1 i=1
+(cid:88)N (cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)δ kθ′(cid:13) (cid:13) (cid:13) 2α|sh jk(θ)−sh jk(θ′)|(cid:88)N sh ik(θ)
j=1 i=1
+(cid:88)N (cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:13) (cid:13) (cid:13)δ kθ′(cid:13) (cid:13) (cid:13) 2αsh jk(θ′)(cid:88)N |sh ik(θ)−sh ik(θ′)|(cid:27)
j=1 i=1
≤
2γf¯ max(cid:13) (cid:13)Z¯(cid:13) (cid:13)
2
(cid:88)K (cid:26)(cid:13) (cid:13)δθ−δθ′(cid:13)
(cid:13)
α+(cid:13) (cid:13)δθ′(cid:13)
(cid:13) |α −α′ |
K (cid:13) k k (cid:13) 2 (cid:13) k (cid:13) 2 h,k h,k
k=1
+(cid:13) (cid:13)δθ′(cid:13)
(cid:13)
α(cid:88)N
|sh (θ)−sh
(θ′)|+(cid:13) (cid:13)δθ′(cid:13)
(cid:13)
α(cid:88)N
|sh (θ)−sh
(θ′)|(cid:27)
(cid:13) k (cid:13) jk jk (cid:13) k (cid:13) ik ik
2 2
j=1 i=1
≤
2γf¯ max(cid:13) (cid:13)Z¯(cid:13) (cid:13)
2
(cid:88)K (cid:26)(cid:13) (cid:13)δθ−δθ′(cid:13)
(cid:13)
α+(cid:13) (cid:13)δθ′(cid:13)
(cid:13) |α −α′ |
K (cid:13) k k (cid:13) 2 (cid:13) k (cid:13) 2 h,k h,k
k=1
+2(cid:13) (cid:13) (cid:13)δ kθ′(cid:13) (cid:13) (cid:13) 2α√ N(cid:13) (cid:13)sh k(θ)−sh k(θ′)(cid:13) (cid:13) 2(cid:27) , (57)
where the third inequality uses Cauchy-Schwarz inequality. Combining the above inequality (57) with
Lemma 8 and Lemma 9, we have
 (cid:118) 
(cid:13) (cid:13) (cid:13) (cid:13)∂ ∂ℓ Q(ξ) − ∂ ∂ℓ Q(ξ′)(cid:13) (cid:13) (cid:13)
(cid:13)
≤ 2γf¯ ma Kx(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:26) αγ√ H2Kα(cid:117) (cid:117) (cid:116)(cid:88)H ∥Q h−Q′ h∥2
F
+√ K∥α−α′∥ F
h h F h=1
√
+(γHα+∥A∥ 2) K(cid:13) (cid:13)α h,:−α′ h,:(cid:13) (cid:13)
2
√ (cid:27)
+(γHα+∥A∥ )·2α N ·2K∥Q′ −Q ∥ , (58)
2 h h F
where the last line uses (46) to bound (cid:13) (cid:13)sh(θ)−sh(θ′)(cid:13)
(cid:13)
. The above inequality (58) further gives
k k 2
H
(cid:88)
∥∇ ℓ(ξ)−∇
ℓ(ξ′)∥2
Qh Qh F
h=1
≤8·
γf¯ max(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:26) (2Kα)2(cid:104)
(αγH)2+4N(αγH +∥A∥
)2(cid:105)(cid:88)H
∥Q −Q′∥2
K 2 h h F
h=1
(cid:104) (cid:105) (cid:27)
+K (αγH)2+(αγH +∥A∥ )2 ∥α−α′∥2
2 F
≤8γf¯ max(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2·max(cid:110) 1,(2√ Kα)2(cid:111)(cid:104) (αγH)2+4N(αγH +∥A∥ 2)2(cid:105) ∥ξ′−ξ∥2 2, (59)
where the first inequality makes use of Young’s inequality (c.f. Lemma 3).
Combining the above two relations (55) and (59), we obtain the smoothness of ℓ w.r.t. ξ as follows:
21Lemma 10 (Smoothness of the loss function). Let γ :=(cid:112) η /η . For all transformer parameters ξ,ξ′, if
w Q
max{|α |,|α′ |}≤α, then we have
h,k h,k
∥∇ ℓ(ξ)−∇ ℓ(ξ′)∥ ≤L∥ξ−ξ′∥ , (60)
ξ ξ 2 2
where
L2 =2(cid:18) 1 (cid:16) 2γ(cid:13) (cid:13)Z¯(cid:13) (cid:13)2 (2γHα+∥A∥ )(cid:17)2 + γ4 H2(cid:13) (cid:13)Z¯(cid:13) (cid:13)4(cid:19)
K 2 2 K2 2 (61)
+8γf¯ max(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2·max(cid:110) 1,(2√ Kα)2(cid:111)(cid:104) (αγH)2+4N(αγH +∥A∥ 2)2(cid:105) .
Step 3: verify (43a). (45) implies
∂ℓ(ξ) γ
= (bh)⊤Z¯δθ,
∂α K k k
h,k
which, combining with (52), gives
∀k ∈[K],h∈[H]:
(cid:18) ∂ℓ(ξ)(cid:19)2
≤
γ2
(cid:13) (cid:13)Z¯(cid:13) (cid:13)2(cid:13) (cid:13)Z¯δθ(cid:13) (cid:13)2
.
∂α K2 2 k 2
h,k
Combining this with (36) we obtain
(cid:13)
(cid:13)
(cid:13)ℓ(ξ)(cid:13)
(cid:13)
(cid:13)2
≤(cid:13) (cid:13)Z¯(cid:13) (cid:13)2
2γ2
(ℓ(ξ)−L⋆),
(cid:13)∂α (cid:13) 2 K
h 2
which indicates
(cid:13) (cid:13) (cid:114)
(cid:13) (cid:13)∂ℓ(ξ)(cid:13) (cid:13) ≤(cid:13) (cid:13)Z¯(cid:13) (cid:13) γ 2 (ℓ(ξ)−L⋆). (62)
(cid:13) ∂α (cid:13) 2 K
h 2
Therefore, we have
(cid:13) (cid:13)
(cid:13) (cid:13)α(t)(cid:13)
(cid:13)
=(cid:13)
(cid:13)α(0)−η
(cid:88)t−1 ∂ℓ(ξ(i))(cid:13)
(cid:13)
(cid:13) h (cid:13) 2 (cid:13) (cid:13) h Q ∂α h (cid:13) (cid:13)
i=0 2
≤(cid:13) (cid:13) (cid:13)α( h0)(cid:13) (cid:13) (cid:13) 2+η Q(cid:88)t i=− 01(cid:13) (cid:13) (cid:13) (cid:13)∂ℓ ∂( αξ( hi))(cid:13) (cid:13) (cid:13) (cid:13)
2
≤(cid:13) (cid:13) (cid:13)α( h0)(cid:13) (cid:13) (cid:13) 2+η Q(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2(cid:114) 2 Kγ2 (cid:88)t−1(cid:113) ℓ(ξ(i))−L⋆
i=0
(cid:115)
≤(cid:13) (cid:13) (cid:13)α( h0)(cid:13) (cid:13) (cid:13) 2+η Q(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2
2γ2(cid:0)
L(θ
K(0))−L⋆(cid:1) (cid:88)t−1(cid:18)(cid:114)
1− η Q
2σ(cid:19)i
i=0
(cid:115)
(cid:0) (cid:1)
≤(cid:13) (cid:13) (cid:13)α( h0)(cid:13) (cid:13) (cid:13) 2+η Q(cid:13) (cid:13)Z¯(cid:13) (cid:13) 2 2γ2 L(θ K(0))−L⋆ · η Q4 σ,
where the second inequality follows from (62) and the third inequality follows from the induction hypothesis
(43c). (43a) follows from plugging σ defined in (44) into the above inequality and using the initializtion
condition that α(0) = 1w(0) =0.
γ
Step 4: verify the linear convergence rate (43c). Combining (43a), (60) and Lemma 4.3 in Nguyen
and Mondelli [2020], we have
L(cid:13) (cid:13)2
ℓ(ξ(t))−L⋆ ≤ℓ(ξ(t−1))−L⋆+⟨∇ ℓ(ξ(t−1)),ξ(t)−ξ(t−1)⟩+ (cid:13)ξ(t)−ξ(t−1)(cid:13) , (63)
ξ 2 (cid:13) (cid:13) 2
22which indicates when η ≤1/L, we have
Q
η (cid:13) (cid:13)2 (49)(cid:16) η σ(cid:17)(cid:16) (cid:17)
ℓ(ξ(t))−L⋆ ≤ℓ(ξ(t−1))−L⋆− Q (cid:13)∇ ℓ(ξ(t−1))(cid:13) ≤ 1− Q ℓ(ξ(t−1))−L⋆ , (64)
2 (cid:13) ξ (cid:13) F 2
which, combined with the fact that L(θ(s))=ℓ(ξ(s)) for all s (see Lemma 6), verifies (43c).
Note that (36) implies that L⋆ ≤ L(θ) holds for all θ. And from (43c) we know that L(θ(t)) → L⋆ as
t→∞. Therefore, it follows that
L⋆ =infL(θ).
θ
Consequently, (43c) is equivalent to (18).
C Proof of Theorem 2
By (43c) we know that L(θ(t))→L⋆ as t→∞. Thus from (36) we know that (37) and (38) hold.
By Sherman-Morrison-Woodbury formula, we have
(cid:0) mτI +Z⊤Z(cid:1)−1 = 1 I − 1 Z⊤(cid:0) mτI +ZZ⊤(cid:1)−1 Z. (65)
N mτ N mτ m
Thus we have
A(3 =4)(cid:0) Z⊤Z+mτI N(cid:1)−1(cid:16) Z⊤Z(cid:98)+(mτI N,0)(cid:17)
(6 =5) m1
τ
(cid:16) I
N
−Z⊤(cid:0) mτI m+ZZ⊤(cid:1)−1 Z(cid:17)(cid:16) Z⊤Z(cid:98)+(mτI N,0)(cid:17)
(cid:20)
= m1
τ
Z⊤Z(cid:101)+(mτI N,0)−Z⊤(cid:0) mτI m+ZZ⊤(cid:1)−1(cid:0) mτI m+ZZ⊤(cid:1) Z(cid:101)
(cid:21)
+mτZ⊤(cid:0)
mτI
m+ZZ⊤(cid:1)−1 Z(cid:101)−mτZ⊤(cid:0)
mτI
m+ZZ⊤(cid:1)−1
(Z,0)
=(I ,0)+Z⊤(cid:0) mτI +ZZ⊤(cid:1)−1 (0,ZQ)
N m
=(cid:16) I ,Z⊤(cid:0) mτI +ZZ⊤(cid:1)−1 ZQ(cid:17) , (66)
N m
where ZQ is defined in (19).
On the other hand, it’s straightforward to verify that λ(cid:98) defined in (20) admits the following closed form:
λ(cid:98) =(cid:0) mτI m+ZZ⊤(cid:1)−1 Zy. (67)
Combining the above two equations, we obtain
(cid:18) y (cid:19) (cid:18) y (cid:19)
A⊤y = = =y⋆,
(cid:0) ZQ(cid:1)⊤(cid:0)
mτI
m+ZZ⊤(cid:1)−1
Zy
(cid:0) ZQ(cid:1)⊤
λ(cid:98)
(cid:98)
where the last equality follows from (22).
Now we give the iteration complexity for the mean-squared error between the prediction y and the limit
(cid:98)
point y⋆ to be less than ε. Given any prompt P =P , where λ satisfies Assumption 4, we have
(cid:98) λ
y =λ⊤(z +ϵ )∼N(λ⊤z ,∥λ∥2τ).
i i i i 2
Letting x
i
= y ∥i λ− ∥λ⊤ √z τi, we have x
i
∼N(0,1). Define
2
N
Z =(cid:88) ∥λ∥2τ(x2−1)=(cid:13) (cid:13)y−Z⊤λ(cid:13) (cid:13)2 −Nτ∥λ∥2.
2 i 2 2
i=1
23By Laurent and Massart [2000, Lemma 1], we have
(cid:16) √ √ (cid:17)
∀s>0: P Z ≥2 N∥λ∥2τ s+2∥λ∥2τs ≤exp(−s).
2 2
By letting s=log(1/δ) and using the definition of Z, we have
P(cid:16)(cid:13) (cid:13)y−Z⊤λ(cid:13) (cid:13)2 ≥Nτ∥λ∥2+2(cid:112) Nlog(1/δ)∥λ∥2τ +2∥λ∥2τlog(1/δ)(cid:17) ≤δ. (68)
2 2 2 2
Thus with probability at least 1−δ, we have
∥y∥
≤(cid:13) (cid:13)Z⊤λ(cid:13)
(cid:13)
+(cid:13) (cid:13)y−Z⊤λ(cid:13)
(cid:13)
2 2 2
≤(cid:13) (cid:13)Z⊤λ(cid:13)
(cid:13) +∥λ∥
√ τ(cid:16)
N
+2(cid:112) Nlog(1/δ)+2log(1/δ)(cid:17)1/2
2 2
(cid:18) √ (cid:16) (cid:112) (cid:17)1/2(cid:19)
≤B ∥Z∥ + τ N +2 Nlog(1/δ)+2log(1/δ) . (69)
2
where we use (68) in the second inequality, and the third inequality follows from Assumption 4.
On the other hand, by (36) we have
1 (cid:13) (cid:13)2 mτ (cid:13) (cid:13)2
L(θ(t))= (cid:13)Z¯(A(cid:98)−A)(cid:13) +L⋆ ≥ (cid:13)A(cid:98)−A(cid:13) +L⋆,
2K (cid:13) (cid:13) 2 2K (cid:13) (cid:13) 2
which gives
(cid:13) (cid:13)A(cid:98)−A(cid:13) (cid:13) ≤(cid:114) 2K (cid:0) L(θ(T))−L⋆(cid:1) ≤(cid:114) 2K (cid:0) L(θ(0))−L⋆(cid:1)(cid:18) 1− γ2η Qζ 0(cid:19)T/2 . (70)
(cid:13) (cid:13) 2 mτ mτ 2K
Thus we know that w.p. at least 1−δ, we have
21
K
∥y (cid:98)−y (cid:98)⋆∥2
2
= 21
K
(cid:13) (cid:13) (cid:13) (cid:13)(cid:16) A(cid:98)−A(cid:17)⊤ y(cid:13) (cid:13) (cid:13) (cid:13)2
2
≤ 21
K
(cid:13) (cid:13) (cid:13)A(cid:98)−A(cid:13) (cid:13) (cid:13)2 2∥y∥2
2
≤ε,
where the last relation follows from (69), (70) and (21).
D Proof of Key Lemmas
D.1 Proof of Proposition 1
For notation simplicity we drop the superscript (0) in the subsequent proof.
Let D :=(cid:0) V⊤Q v ,··· ,V⊤Q v (cid:1) ∈RN×H. Note that
k 1 k H k
D =V⊤Q=V⊤(q ,··· ,q ), where Q(i,j)i. ∼i.d. N(0,β2∥v ∥2), ∀i∈[d],j ∈[H]. (71)
k 1 H k 2
This suggests the column vectors of D are i.i.d. and the density of each column vector is positive at any
k
point x∈R(V), where R(V)⊂RN is the row space of V.
Since Z¯ has full rank, to prove B has full rank a.s., we only need to argue that C (:,1:N) has full rank
k k
w.p. 1. Below we prove this by contradiction (recall that by definition C =softmax(D ), and we assume
k k
H ≥N).
Suppose w.p. larger than 0, there exists one of C (:,1 : N)’s column vector that could be linearly
k
represented by its other N −1 column vectors. Without loss of generality, we assume this colomn vector
is C (:,1) = softmax(D (:,1)). Let x = x(q ) := exp(D (:,1)) = exp(V⊤q ). Then x could be linearly
k k 1 k 1
represented by exp(D (:,i)), i=2,··· ,N.
k
Let A˜ :=exp(D (:,2:N)), then w.p. larger than 0, x∈C(A˜), where C(A˜) is the column vector space of
k
A˜. i.e., we have
(cid:90)
P(x∈C(A˜)|A˜)dµ(A˜)>0,
RN×(m−1)
24which further indicates that there exists A˜ ∈RN×(N−1) such that P(x∈C(A˜))>0. Since the dimension of
C(A˜) is at most N −1, there exists y ∈RN, y ̸=0 such that y⊥C(A˜). Therefore, we have
P(y⊤x=0)>0. (72)
By Assumption 3, without loss of generality, we assume that u = (v ,v ,··· ,v )⊤ has different
1 11 12 1N
entries. For any vector w =(w ,··· ,w )⊤ ∈Rd, we let w˜ =(w ,··· ,w )⊤ ∈Rd−1 denote the vector formed
1 d 2 d
by deleting the first entry of w. Let q =(q,q˜⊤)⊤. For any fixed q˜ ∈Rd−1, the function g(·|q˜ ):R→R
1 1 1 1
defined by
N N
g(q|q˜ 1):=(cid:88) y ieqv1i+q˜ 1⊤v˜i =(cid:88) y ieq˜ 1⊤v˜ieqv1i =(cid:10) y,exp(V⊤q 1)(cid:11) =⟨y,x(q 1)⟩
i=1 i=1
has finite zero points and thus {q ∈R|g(q|q˜ )=0} is a zero-measure set. Therefore, we have
1
(cid:90)
P(⟨y,x⟩=0)= P(g(q|q˜ )=0|q˜ )dµ(q˜ )=0,
1 1 1
Rd−1
which contradicts (72). Therefore, C (:,1:N) has full rank with probability 1.
k
D.2 Proof of Lemma 4
Lemma 4 can be verified by the following direct computation (recall that the noise in each label satisfies
ϵ i. ∼i.d N(0,τI ), ∀i∈[N]):
i m
(cid:34) N (cid:35)
1 (cid:88)
E (y −λ⊤(f(v )+ϵ ))2
ϵ 2N i i i
i=1
(cid:34) N (cid:35)
=E 1 (cid:88)(cid:0) (y −λ⊤f(v ))2−2λ⊤ϵ (y −λ⊤f(v ))+λ⊤ϵ ϵ⊤λ(cid:1)
ϵ 2N i i i i i i i
i=1
N
=
1 (cid:88)(cid:16)
(y −λ⊤f(v
))2+τ∥λ∥2(cid:17)
2N i i 2
i=1
N
= 1 (cid:88) (y −λ⊤f(v ))2+ τ ∥λ∥2.
2N i i 2 2
i=1
D.3 Proof of Lemma 5
We let ϵP :=(ϵ ,··· ,ϵ )∈Rm×N, ϵ:=(ϵ ,··· ,ϵ )∈Rm×K. Recall that y =(y ,··· ,y )⊤ ∈RN. Then
1 N 1 K 1 N
we have
y =(Z+ϵP)⊤λ, (73)
and
K (cid:34) K (cid:35)
L(θ)= 1 (cid:88) L (θ)= 1 E 1 (cid:88) (y −y )2 (74)
K k 2 λ,ϵ K (cid:98)k k
k=1 k=1
K
= 21
K
(cid:88) E λ,ϵ(cid:13) (cid:13)y⊤a (cid:98)k−λ⊤(z k+ϵ k)(cid:13) (cid:13)2
2
k=1
K
= 21
K
(cid:88) E λ,ϵ(cid:13) (cid:13)λ⊤(Z+ϵP)a (cid:98)k−λ⊤(z k+ϵ k)(cid:13) (cid:13)2
2
k=1
K
= 1 (cid:88) E (cid:2) (Z+ϵP)a −(z +ϵ )(cid:3)⊤ λλ⊤(cid:2) (Z+ϵP)a −(z +ϵ )(cid:3)
2K λ,ϵ (cid:98)k k k (cid:98)k k k
k=1
K
= 1 (cid:88) E (cid:2) (Z+ϵP)a −(z +ϵ )(cid:3)⊤(cid:2) (Z+ϵP)a −(z +ϵ )(cid:3)
2K ϵ (cid:98)k k k (cid:98)k k k
k=1
25K
= 21
K
(cid:88) E ϵ(cid:104) ∥Za (cid:98)k−z k∥2 2+2(Za (cid:98)k−z k)⊤(ϵPa (cid:98)k−ϵ k)+(cid:13) (cid:13)ϵPa (cid:98)k−ϵ k(cid:13) (cid:13)2 2(cid:105) , (75)
k=1
where a
(cid:98)k
denote the k-th column vector of matrix A(cid:98)(θ) defined in (35), and the fifth line uses Assumption 1.
Note that for all k ∈[K], we have
E (Za −z )⊤(ϵPa −ϵ )=0, (76)
ϵ (cid:98)k k (cid:98)k k
and that
E ϵ(cid:13) (cid:13)ϵPa (cid:98)k−ϵ k(cid:13) (cid:13)2
2
=mτ(cid:16) ∥a (cid:98)k∥2 2+1(cid:17) −2mτ (cid:98)a kk1{k ∈[N]}, (77)
where 1{k ∈[N]} is the indicator function that equals 1 if k ∈[N] and 0 otherwise, and we have made use
of the assumption that ϵ i. ∼i.d. N(0,τ2I ).
k m
Combining the above two equations with (75), we know that for k ∈[N], it holds that
1(cid:16) (cid:17)
L (θ)= ∥Za −z ∥2+mτ∥a −e ∥2 .
k 2 (cid:98)k k 2 (cid:98)k k 2
Reorganizing the terms in the RHS of the above equation, we obtain that
L (θ)= 1(cid:13) (cid:13)(cid:0) Z⊤Z+mτI(cid:1)1/2(cid:16) a −(cid:0) Z⊤Z+mτI(cid:1)−1(cid:0) Z⊤z +mτe (cid:1)(cid:17)(cid:13) (cid:13)2 + 1 c , (78)
k 2(cid:13) (cid:98)k k k (cid:13) 2 2 k
where c =−(cid:0) Z⊤z +mτe (cid:1)⊤(cid:0) Z⊤Z+mτI(cid:1)−1(cid:0) Z⊤z +mτe (cid:1) +∥z ∥2+mτ.
k k k k k k 2
By a similar argument, we can show that for k ∈[K]\[N], it holds thet
L (θ)= 1(cid:13) (cid:13)(cid:0) Z⊤Z+mτI(cid:1)1/2(cid:16) a −(cid:0) Z⊤Z+mτI(cid:1)−1 Z⊤z (cid:17)(cid:13) (cid:13)2 + 1 c′, (79)
k 2(cid:13) (cid:98)k k (cid:13) 2 2 k
where c′ =−(cid:0) Z⊤z (cid:1)⊤(cid:0) Z⊤Z+mτI(cid:1)−1(cid:0) Z⊤z (cid:1) +∥z ∥2.
k k k k 2
(78), (79) together with (33) and the definition of L⋆ give (36).
D.4 Proof of Lemma 6
First, it holds that
Q(t) =Q(t−1)−η ∇ ℓ(ξ(t−1))=Q(t−1)−η ∇ ℓ(ξ(t−1)). (80)
h h Q Qh h Q Qh
Second, note that
w(t) =w(t−1)−η ∇ L(θ(t−1))
h h w wh
1
=γα(t−1)−γ2· η ∇ ℓ(ξ(t−1))
h γ Q αh
(cid:16) (cid:17)
=γ α(t−1)−η ∇ ℓ(ξ(t−1)) .
h Q αh
Dividing both sides of the above equality by γ, we have
α(t) =α(t−1)−η ∇ ℓ(ξ(t−1)). (81)
h h Q αh
Hence, (41) follows from combining (80) and (81).
26D.5 Proof of Lemma 7
Throughout this proof, we omit the superscript (t) for simplicity. We first compute the gradient of L w.r.t.
Q . By (36) we know that
h
K
ℓ(ξ)=L(θ)= 21
K
(cid:88)(cid:13) (cid:13)Z¯δ k(cid:13) (cid:13)2 2,
k=1
and thus we have
 (cid:13) (cid:13)2
∂ ∂ℓ Q(ξ)
=
K1 (cid:88)K (cid:88)N ∂δ∂ 1 2(cid:13)
(cid:13)
(cid:13)(cid:88)N
δ ikz¯
i(cid:13)
(cid:13) (cid:13) 
∂∂ Qδ
jk
h jk (cid:13) (cid:13) h
k=1j=1 i=1 2
K N N
= γ (cid:88)(cid:88)(cid:0) Z¯δ (cid:1)⊤ z¯ ·α sh (cid:88) sh (v −v )v⊤. (82)
K k j h,k jk ik j i k
k=1j=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:Gh,jk
Note that
(cid:13) (cid:13)Gh,jk(cid:13) (cid:13) ≤2αsh , (83)
F jk
where we use the fact that (cid:13) (cid:13)(v j −v i)v k⊤(cid:13) (cid:13) 2 ≤ 2 (recall that we assume each v k has unit norm, k ∈ [K].)
Combining (82) and (83), we have the desired result
(cid:13) (cid:13) K N
(cid:13) (cid:13) (cid:13)∂ ∂ℓ Q(ξ)(cid:13) (cid:13)
(cid:13)
≤ Kγ (cid:88)(cid:88)(cid:13) (cid:13)Z¯δ k(cid:13) (cid:13) 2∥z¯ j∥ 2(cid:13) (cid:13)Gh,jk(cid:13) (cid:13)
F
h F k=1j=1
K N
≤ 2 Kγ (cid:88)(cid:88)(cid:13) (cid:13)Z¯δ k(cid:13) (cid:13) 2f¯ maxαsh
jk
k=1j=1
(cid:118)
≤
2γf¯ Kmaxα√ K(cid:117)
(cid:117)
(cid:116)(cid:88)K
(cid:13) (cid:13)Z¯δ k(cid:13) (cid:13)2
2
k=1
√
≤2 2γf¯ α(cid:112) ℓ(ξ)−L⋆, (84)
max
where f¯ is defined in (12) and the third line follows from Cauchy-Schwarz inequality.
max
27