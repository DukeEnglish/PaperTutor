SMILE: ZERO-SHOT SPARSE MIXTURE OF LOW-RANK EXPERTS
CONSTRUCTION FROM PRE-TRAINED FOUNDATION MODELS
APREPRINT
AnkeTang1,LiShen2,YongLuo1,ShuaiXie3,HanHu4,LefeiZhang1,BoDu1,DachengTao5
1WuhanUniversity,2SunYat-senUniversity,3JDExploreAcademy,
4BeijingInstituteofTechnology,5NanyangTechnologicalUniversity
1{anketang,luoyong,zhanglefei,dubo}@whu.edu.cn,2mathshenli@gmail.com,
3xieshuai@jd.com,4hhu@bit.edu.cn,5dacheng.tao@ntu.edu.sg
ABSTRACT
Deep model training on extensive datasets is increasingly becoming cost-prohibitive, prompting
thewidespreadadoptionofdeepmodelfusiontechniquestoleverageknowledgefrompre-existing
models.FromsimpleweightaveragingtomoresophisticatedmethodslikeAdaMerging,modelfusion
effectivelyimprovesmodelperformanceandacceleratesthedevelopmentofnewmodels. However,
potentialinterferencebetweenparametersofindividualmodelsandthelackofinterpretabilityinthe
fusionprogressremainsignificantchallenges. Existingmethodsoftentrytoresolvetheparameter
interference issue by evaluating attributes of parameters, such as their magnitude or sign, or by
parameterpruning. Inthisstudy,webeginbyexaminingthefine-tuningoflinearlayersthroughthe
lensofsubspaceanalysisandexplicitlydefineparameterinterferenceasanoptimizationproblem
toshedlightonthissubject. Subsequently,weintroduceaninnovativeapproachtomodelfusion
calledzero-shotSparseMIxtureofLow-rankExperts(SMILE)construction,whichallowsforthe
upscalingofsourcemodelsintoanMoEmodelwithoutextradataorfurthertraining. Ourapproach
reliesontheobservationthatfine-tuningmostlykeepstheimportantpartsfromthepre-training,butit
useslesssignificantorunusedareastoadapttonewtasks. Also,theissueofparameterinterference,
whichisintrinsicallyintractableintheoriginalparameterspace,canbemanagedbyexpandingthe
dimensions. Weconductextensiveexperimentsacrossdiversescenarios,suchasimageclassification
andtextgeneralizationtasks,usingfullfine-tuningandLoRAfine-tuning,andweapplyourmethod
tolargelanguagemodels(CLIPmodels,Flan-T5models,andMistral-7Bmodels),highlightingthe
adaptabilityandscalabilityofSMILE.Forfullfine-tunedmodels,about50%additionalparameters
canachievearound98-99%oftheperformanceofeightindividualfine-tunedViTmodels,whilefor
LoRAfine-tunedFlan-T5models,maintaining99%performancewithonly2%extraparameters.
Codeisavailableathttps://github.com/tanganke/fusion_bench.
Keywords MixtureofExperts·ModelFusion·SubspaceDecomposition·LargeLanguageModel
1 Introduction
In recent years, the field of deep learning has witnessed an exponential growth in model sizes and dataset scales,
makingthetrainingoflarge-scaledeepmodelsonextensivedatasetsincreasinglycost-prohibitive,bothintermsof
financialresourcesandenvironmentalimpact[Minaeeetal.,2024,Hadietal.,2023]. Deepmodelfusiontechniques
haveemergedasapromisingsolution,allowingtheintegrationofknowledgefrompre-existingmodelswithoutthe
needforextensiveretraining[Lietal.,2023,Zhengetal.,2023,Yangetal.,2024a]. Thisapproachnotonlyreduces
computationalcostsbutalsoenablesthecreationofmorerobustandversatilemodelsbycombiningthestrengthsof
multiplemodels.
FollowingthecategorizationinTangetal.[2024a],weclassifymodelfusionmethodsintothreemaincategories: model
ensemblemethods,modelmergingmethods,andmodelmixingmethods. Modelensembletechniquesaggregatethe
predictionsfromseveralmodelstoenhanceperformance[SagiandRokach,2018]. Whileresource-intensiveintermsof
memoryandcomputation,itimprovesknowledgedistillationtraining[Wanetal.,2024a,b]. Modelmergingmethods,
ontheotherhand,combinetheparametersofmultiplemodelsintoasinglemodel,oftenthroughweightedaveraging
4202
guA
91
]GL.sc[
1v47101.8042:viXraSMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
orparameteralignment[MatenaandRaffel,2022,Jinetal.,2022]. Modelmixingmethodsinvolvetheintegrationof
multiplemodelsthroughgatingmechanismsordepthconcatenation,allowingformoreflexibleandadaptivefusion
strategies[Komatsuzakietal.,2022,Kimetal.,2023]. Thesemethodsareparticularlyeffectiveinmulti-tasklearning
scenarios,wherethemergedmodelcansimultaneouslyperformmultipletasks.
However,despitethepromisingadvancementsinmodelfusion,severalcriticalchallengespersist,hinderingthefull
realizationofitspotential. Aprimaryconcernisthepotentialinterferencebetweenparametersofdifferentmodels,
which leads to suboptimal performance. Additionally, the lack of interpretability in the fusion process remains a
significanthurdle,ascurrentinsightsarelargelyconfinedtoheuristicobservationsorsimplifiedassumptions,such
aslinearmodeconnectivity,parametersignsorimportance[Ainsworthetal.,2022,Stoicaetal.,2023,Yadavetal.,
2023,Yuetal.,2024]. Understandinghowparametersaremergediscrucialforbuildingtrustinthemergedmodelsand
forfurtherimprovingfusiontechniques. Thesechallengesareparticularlypronouncedincomplex,high-dimensional,
non-linearmodelarchitectures,wheretheinteractionsbetweenparameterscanbeextremelyintricateandnon-intuitive.
Insteadofrelyingonheuristicmethodsorsimplifiedassumptions,we
proposeanovelsubspaceperspectiveonunderstandingandaddress- 90
ingtheparameterinterferenceprobleminthisstudy.Wefirstexamine Ours
thefine-tuningprocessinlinearlayersthroughthelensofsubspace Pretrained
80
analysisusingmatrixdecompositioninSection2. Thisallowsusto Simple Average
decomposethepredictionofafine-tunedmodelintodistinctcom- Task Arithmetic
ponents,encompassingthepre-trainedknowledgeandtask-specific 70 Ties-Merging
adaptation. Thisapproachprovidesinsightsintohowmodelsadapt Fisher Merging
todownstreamtaskswhilepreservingpre-trainedknowledge. Draw- RegMean
ingfromexperimentalobservations,webuildamorecomprehensive 60 Layer-Wise AdaMerging
Weight-Ensembling MoE
understandingoffine-tuning,wefurtherformulateparameterinter-
Individuals
ferenceasanoptimizationprobleminSection3,providingamore
50
rigorousandmeasurableperspective.
2 4 6 8
Basedonourinsights,weintroduceaninnovativeapproachcalled
Normalized Number of Parameters
zero-shotSparseMIxtureofLow-rankExperts(SMILE)construction,
enhancingexistingsourcemodelsintoamoreversatileMoEmodel. Figure1: Multi-taskmodelfusionexperiment
Thezero-shotaspectofourapproachisparticularlynoteworthy,as oneightimageclassificationtasksusingCLIP-
itfacilitatestheimmediatedeploymentoffusedmodelsinnewen- ViT-B/32models. Herewesetk =16and
gate
vironmentsortasks,drasticallyminimizingthetimeandresources kisvariedfrom4to128toinvestigatethetrade-
typicallyrequiredformodeladaptation. offbetweenperformanceandmodelsize.
Theeffectivenessofourproposedmethodisrootedintwokeyobser-
vationsderivedfromoursubspaceanalysis. Firstly,wefoundthatthefine-tuninglargelypreservesthemostimportant
pre-trainedweightsandprimarilyutilizeslesssignificantorpreviouslyunuseddimensionsoftheparameterspaceto
adapttonewtasks. Thispreservationensuresthatthecriticalpre-trainingknowledgeencodedintheoriginalmodelsis
notlostduringfine-tuningandimpliesthattheparametersubspacerequiredtoaccommodatenewknowledgemayvary
fromtasktotask. Secondly,wefoundthatwhileparameterinterferenceisinherentlydifficulttoaddressintheoriginal
parameterspace,itbecomesmoremanageablewhenweincreasethemodel’sdimensionality. Thisexpansioncreates
additional‘room’fortask-specificparameterupdatestocoexistwithoutmutualinterference.
Weconductedextensiveexperimentsacrossvarioustasksandmodelsinboththevisionandlanguagedomains,utilizing
traditionalfullfine-tuningaswellasLow-RankAdaptation(LoRA)[Huetal.,2021]. Theresultsshowthatformodels
that undergo full fine-tuning, adding approximately50% more parameters allows us toachieve around 98-99%of
theperformanceofeightindividualfine-tunedmodels. InthecaseofLoRAfine-tunedmodels,maintaining99%of
theindividualperformancerequiresonlya2%increaseinparameters. Thismethodalsoofferstrade-offsbetween
performanceandmodelsize,asillustratedinFigure1,wherewevarytherankkoflocalexperts.
Tosummarize,ourcontributionsinthisstudyareasfollows:
• Weprovideanovelsubspaceperspectiveonthefine-tuningprocess, sheddinglightonhowmodelsadapt
tonewtaskswhilepreservingpre-trainedknowledge. Inaddition,Weformulatetheparameterinterference
problemasanoptimizationproblem,providingamorerigorousandmeasurableperspectiveonthisissue.
• Weintroduceazero-shotSparseMixtureofLow-RankExperts(SMILE)constructionapproach,enablingthe
fusionofexistingmodelsintomoreunifiedversatileSMILEmodels. Wealsodiscussthecomplexityofour
method,highlightingitspotentialforbroaderapplicationsindeeplearningresearchandpractice.
• Wedemonstratetheeffectivenessofourmethodthroughextensiveexperimentsonavarietyoftasksandsetups,
showcasingitssuperiorperformanceandefficiencycomparedtoexistingmodelfusiontechniques.
2
ycaruccA
egarevASMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
2 RethinkingModelFine-TuningFromaSubspacePerspective
In this study, we aim to construct a unified versatile model from multiple fine-tuned models, which can perform
multipletasksorhandleinputsfrommultipledomainssimultaneously. Wedenotethenumberoffine-tunedmodelsasT.
Beforewedelveintotheproposedmethod’sdetails,wegaininsightsintothefine-tuningprocessfromasingularvalue
decomposition(SVD)subspaceperspective. Inthissection,weaimto(1)investigateandlocatethetaskinformationin
thefine-tunedweightsW ,and(2)understandhowitisrelatedtothepre-trainedweightsW.
ft
Consideralinearlayerofthepre-trainedmodelwithweightmatrixW ∈Rm×n andbiasvectorb∈Rm. Afterfull
fine-tuning on a downstream task, the weight matrix and bias vector are updated to W and b , respectively. To
ft ft
achieveadeeperunderstandingoftheseupdates,weneedtoemploymathematicaltoolsthatallowustodecomposethe
parameterspaceintodistinctrangesofimportance,i.e. subspaces. Westatethefollowingtheorem.
Theorem1 Giventwosetsoforthonormalvectors{u }p ⊂ Rm and{v }q ⊂ Rn,1 ≤ p ≤ mand1 ≤ q ≤ n,
i i=1 i i=1
thesetofmatrices{u vT}p,q formsanorthonormalbasisforasubspaceofRm×nwithdimensionpq.
i j i=1,j=1
Proof1 Forsimplicity,letx =u vT. TheFrobeniusinnerproductoftwomatricesx andx isdefinedas
ij i j ab cd
⟨x ,x ⟩=tr(cid:0) u vT(u vT)T(cid:1) =tr(cid:0) u vTv uT(cid:1) ∈R.w (1)
ab cd a b c d a b d c
Orthonormality: weconsiderthreecases:
1. Ifa=candb=d,then⟨x ,x ⟩=tr(cid:0) u uT(cid:1) =u⊤u =1.
ab cd a a a a
2. Ifb̸=d,thenvTv =0and⟨x ,x ⟩=0.
b d ab cd
3. Ifb=danda̸=c,thenvTv =1and⟨x ,x ⟩=tr(u u⊤)=u⊤u =0.
b d ab cd a c a c
Thus,{x }p,q isorthonormal.
ij i,j=1
LinearIndependence: assumethereexistsanonzeromatrixα ∈ Rp×q suchthat(cid:80)p (cid:80)q α x = 0. Forany
i=1 j=1 ij ij
a∈[p]andb∈[q],taketheinnerproductofbothsideswithx . Weobtainthefollowing:
ab
(cid:42) p q (cid:43)
(cid:88)(cid:88)
α x ,x =⟨0,x ⟩=0. (2)
ij ij ab ab
i=1j=1
Bythelinearityoftheinnerproductandtheorthogonality,weproved:
p q
(cid:88)(cid:88)
α ⟨x ,x ⟩=α ⟨x ,x ⟩=α =0. (3)
ij ij ab ab ab ab ab
i=1j=1
Sincethisholdsforanyaandb,weconcludethatallα =0. Thisleadstoacontradictiontotheassumptionthatαis
ij
nonzero. Therefore,theset{x }r islinearlyindependent,whichisthenecessaryandsufficientconditionsforaset
ij i,j=1
ofelementstoformabasisforavectorspacewithdimensionpq.
WestartbydecomposingtheweightmatrixW usingthereducedSVDasW = U Σ VT,whereU ∈ Rm×r and
r r r r
V ∈ Rr×n are orthonormal matrices containing the left singular vectors and right singular vectors, respectively,
r
Σ ∈Rr×risadiagonalmatrixcontainingthesingularvaluessortedindescendingorder,andristherankofthematrix
r
W [OlverandShakiban,2018]. InthecaseoffullSVD,thematricesareU ∈ Rm×m,Σ ∈ Rm×n,andV ∈ Rn×n,
whichpreserveallinformationaboutthematrixW,includingitskernel(nullspace)andcokernel(leftnullspace),as
showninFigure2a.
Remark1 AccordingtoTheorem1,thesetofmatrices{u vT|i ∈ [m],j ∈ [n]}formsanorthonormalbasisfora
i j
subspaceofRm×nwithdimensionmn. Inotherwords,foranyrealmatrixA∈Rm×n,wecanexpressitasaweighted
sumoftheelementsinthebasis,i.e. A=(cid:80)m (cid:80)n ⟨A,u vT⟩u vT ∈span({u vT}m,n).
i=1 j=1 i j i j i j i,j
Remark2 LetU andVbetwosubsetsof{u }m and{v }n ,respectively.{uvT|u∈U,v ∈V}formsaorthonormal
i i=1 i i=1
basisforasubspaceofRm×n,withdimension|U||V|≤mn.
Togaininsightsintohowfine-tuningmodifiesthepre-trainedweightstoadaptthemtoaspecifictask,weassumethe
fine-tunedlinearlayeracceptsaninputx ∈ Rn andoutputsy = W x+b . Becausetherowspace{v }n isan
ft ft i i=1
3SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
Pretrained Finetuned Space I Space II Space II & III
1.0
I
II
0.8
kernel
cokernel
III
0.6
0.4
or
SUN397 CARS RESISC45EuroSAT SVHN GTSRB MNIST DTD
(a)FullSVDandsubspacepartition. (b)Accuracycomparisonacrossdifferentsubspaceprojectionstrategies.
Figure2: HereweshowtheSVDdecompositionandsubspacepartitionofthesingularvaluematrixΣ,andtheaccuracy
comparisonofdifferentsubspaceprojectionstrategiesdiscussedinSection2.
orthonormalbasisforRn,wecandecomposexasx=(cid:80)n
⟨x,v ⟩v ,where⟨·,·⟩denotesthevectorinnerproduct. On
i=1 i i
theotherhand,W andb areupdatedfromthepre-trainedweightsW andb. Wecanderivethefollowingequation:
ft ft
y =W x+b =(W +∆W)x+b+∆b= Wx+b +∆Wx+∆b. (4)
ft ft
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
pre-trainedpart fine-tunedpart
Nowweexpandthepre-trainedpartandfine-tunedpartinEq.(4)separatelyasfollows:
 
n n r
(cid:88) (cid:88) (cid:88)
pre-trainedpart= W⟨x,v i⟩v i+b=  σ ju jv j⊤ ⟨x,v i⟩v i+b (5)
i=1 i=1 j=1
n r r
(cid:88)(cid:88) (cid:88)
= σ ⟨x,v ⟩u v⊤v +b= σ ⟨x,v ⟩u +b, (6)
j i j j i j j j
i=1j=1 j=1
 
n n m n
(cid:88) (cid:88) (cid:88)(cid:88)
fine-tunedpart= (W ft−W)⟨x,v i⟩v i+∆b=  δ jku jv k⊤ ⟨x,v i⟩v i+∆b (7)
i=1 i=1 j=1k=1
n m n m n
(cid:88)(cid:88)(cid:88) (cid:88)(cid:88)
= δ ⟨x,v ⟩u v⊤v +∆b= δ ⟨x,v ⟩u +∆b. (8)
jk i j k i jk k j
i=1j=1k=1 j=1k=1
Where δ = ⟨∆W,u v⊤⟩ = (U⊤∆WV) is the Frobenius inner product between the fine-tuned weight update
jk j k jk
∆W andtherank-onematrixu v⊤. Italsoquantifieshowmuchtheweightupdatesalignwiththedirectionspecified
j k
byu v⊤ andindicateswhichinput-outputtransformationisenhancedorsuppressed(orenhancedreversely)during
j k
fine-tuning,basedonitssignandmagnitude. Forexample,alargepositiveδ suggeststhattheconnectionbetweenthe
jk
k-thinputdirection(v )andj-thoutputdirection(u )isstrengthenedforthedownstreamtask. Thisdecomposition
k k
showshowdifferentthepre-trainedandfine-tunedpartscontributetotheoutput. Sofar,weonlyunderstandthatthe
fine-tunedupdate∆W potentiallyusesallmndimensions,whilethepre-trainedpartonlyusesrdimensions.
Wefurthersplitleft/rightsingularvectorsintothreedistinctsubsetsbasedonthedistributionofthesingularvalues
of∆W,anddesignanablationstudycorrespondingtodifferentzonesintheprojectioncoefficientmatrixU⊤∆WV:
(1)Thetop-leftzoneIcontainsthemostsignificantsingularvaluesthatcumulativelyaccountfor50%ofthetotal
sumofthesingularvalues,wedenotethenumberofsingularvaluesinthiszoneasr
,(cid:80)rhalf
σ ≈
(cid:80)r
σ /2.
half i=1 i i=1 i
Thiszoneiscrucialforpreservingpre-traininginformation. (2)ThemiddlezoneIIencompassesthesingularvalues
that make up the remaining half of the cumulative sum. These values are still important but less so than those in
zone I. (3) Zone III contains no information about the pre-trained weights, as its range is beyond rank(W). This
zone partition is illustrated as the Σ in Figure 2a. We fine-tune the pre-trained CLIP-ViT-B/32 model on eight
downstream tasks from different domains, including hand-written digit images, satellite images, regular patterns,
car images, and natural images. Then we project the ∆W onto the different subspaces, including (1) subspace I
span({u vT|i ∈ [r ],j ∈ [r ]}),(2)subspaceIIspan({u vT|i ∈ [r ,r],j ∈ [r ,r]}),and(3)subspace
i j half half i j half half
II & III span({u vT|i ∈ [r ,m],j ∈ [r ,n]}). We compare the performance of the pre-trained model, the
i j half half
fine-tunedmodels,andmodifiedfine-tunedmodelswithdifferentsubspaceprojectionstrategiesappliedon∆W in
Figure2b. Formoredetails,pleaserefertoAppendixA.
4
ycaruccASMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
Figure2bdemonstratesthatprojectingfine-tunedupdatesontosubspaceIresultinaslightimprovementinperformance
ondownstreamtaskscomparedtothepre-trainedmodel,sometimesshowingnoimprovementatall. Projectiononto
subspaceIIleadstomoderateimprovement,whileprojectionontosubspaceII&IIIresultsinsignificantperformance
gains,nearlyreachingthelevelofthefine-tunedmodel.Basedontheseobservations,wedrawthefollowingconclusions:
Fine-tuninglargelymaintainsthemostimportantpre-trainedfeatures,butleverageslesssignificantdimensions
fortask-specificlearningandactivatesorrepurposespreviouslyunuseddimensionsintheweightspace.
3 ParameterInterferenceBetweenTask-SpecificModels
Fromtheprevioussection,webuildanunderstandingofhowfine-tuningmodifiesthepre-trainedweightstoadaptto
aspecificdownstreamtask. Inthissection,weinvestigatetheparameterinterferencebetweenmodelsfine-tunedon
differenttasks,whichhasbeenwidelyexploredinmulti-tasklearningandmulti-taskmodelmerging,primarilywithin
themodelparameterspace. Weaddsuperscriptstodenotethetaskindex,e.g. W(i)andb(i)forthei-thtask.
ft ft
AssumewehaveT tasks,andeachtaskhasafine-tunedmodel. Inthesimplestcases,weconsiderthelinearlayers,
acceptingacommoninputxandoutputtingT differentoutputsy(1),y(2),...,y(T). AccordingtoEq.(4)andEq.(8),
eachy(i)canbedecomposedintothepre-trainedpartandfine-tunedpartasfollows:
r m n
y(i) =(cid:88) σ ⟨x,v ⟩u +b+(cid:88)(cid:88) δ(i)⟨x,v ⟩u +∆b(i). (9)
j j j jk k j
j=1 j=1k=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
pre-trainedpart fine-tunedpart
Wherethepre-trainedtermissharedandremainsconstantduringfine-tuningacrossalltasks. Inthecontextofmodel
merging,thesemodelsaremergedtoconstructaunifiedmulti-taskmodelthatcanperformalltaskssimultaneously.
A common approach is to use a weighted average of the fine-tuned weights, i.e. W = (cid:80)T λ W(l) and
merged l=1 l ft
b =(cid:80)T λ b(l). Thisisequivalenttomergingthefine-tunedpartsofthemodels,whilethepre-trainedpartsare
merged l=1 l ft
sharedacrossalltasks. Therefore,weexpresstheoutputofthemergedmodelas:
m n (cid:32) T (cid:33) T
y =pre-trainedpart+(cid:88)(cid:88) (cid:88) λ δ(l) ⟨x,v ⟩u +(cid:88) λ ∆b(l). (10)
merged l jk k j l
j=1k=1 l=1 l=1
Substitutetheinputxwithx(i)fromthei-thtask(domain),weaimtominimizethediscrepancybetweentheoutputof
themergedmodelandtheoutputofthei-thfine-tunedmodel. Weformulatetheoptimizationproblemasfollows:
m λi ln(cid:13) (cid:13) (cid:13)y merged−y(i)(cid:13) (cid:13) (cid:13)2
2
=m λi ln(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88) jm =1k(cid:88) =n 1(cid:34)(cid:32) (cid:88) l=T 1λ lδ j(l k)(cid:33) −δ j(i k)(cid:35) ⟨x(i),v k⟩u j +(cid:32) (cid:88) l=T 1λ l∆b(l)(cid:33) −∆b(i)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)2 . (11)
2
Usingtriangleinequality,wecandecomposetheerrorintotwopartsandassertanupperbound:
(cid:13) (cid:13) (cid:13)y merged−y(i)(cid:13) (cid:13) (cid:13)2 2 ≤(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:88) jm =1k(cid:88) =n 1(cid:34)(cid:32) (cid:88) l=T 1λ lδ j(l k)(cid:33) −δ j(i k)(cid:35) ⟨x(i),v k⟩u j(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 2+(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:32) (cid:88) l=T 1λ l∆b(l)(cid:33) −∆b(i)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 2. (12)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
weightterm biasterm
For the bias term, we have a closed-form solution for λ = (∆B⊤∆B)−1∆B∆b(i), where ∆B is a matrix with
l
l-th columns as ∆b(l). Notice that this solution varies with the task (domain) index of the coming input x(i), so
a more straightforward solution is to fix the bias term during the fine-tuning process, so that ∆b(i) = 0 for all
i. As for the weight term, parameter interference does not occur on subspace that is orthogonal to the input, i.e.
span({u vT|i∈[m],j ∈[n]and⟨x(i),v ⟩=0}),thusapossiblestrategyistoenlargetheinputsizetoincreasethe
i j j
dimensionoftheorthogonalsubspace. Thisexplainswhymodelmergingmethodsperformbetteronlargermodels
withmoredimensionredundancy. Whentheinputactivatescertaindimensions, i.e. fork suchthat⟨x(i),v ⟩ ≠ 0,
k
theinterferenceisinevitableunlessthedomaingapbetweendifferenttasksislargeenoughtomaketheactivation
dimensionsdisjoint. Notethatwecangainthesameconclusionwithintheoriginalmodelparameterspace,bysimply
replacingthebasisvectors{u } and{v } inthissectionwiththestandardEuclideanbasisvectors{e } .
i i i i i i
5SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
output SMoLE Router
hidden output routing
weights
concatenate
compute norm:
... ...
... ...
input routing input
Router
hidden weights hidden
Figure3: ThearchitectureoftheproposedSparseMIxtureofLow-rankExperts(SMILE)model.
4 ResolvingParameterInterferenceusingSparseMixtureofLow-RankExperts
Understandingthataddressingparameterinterferencebymodelmergingisdifficult,evenjustforthebiasterms,the
optimalmethodforweightcombinationhasaclosed-formsolutionthatvariesbytask. Tomanagethischallenge,we
introduceaninnovativeapproachwithaSparseMIxtureofLow-rankExperts(SMILE)modelinthissection,which
operatesinazero-shotfashion,meaningnodataortrainingisrequired. AnoverviewisshowninFigure3.
WeupscalethelinearlayersfromsourcemodelstotheSMILEmodel,whichconsistsofasharedpre-trainedpart,a
router,andseverallow-rankexperts. Figure3isorganizedintotwoprimarysections: theoverallmodelarchitecture
(left)andtheroutingmechanism(right).
RecalltheoutputdecompositioninEq.(4)andEq.(10),wecanexpresstheoutputofamergedmodelastheoutputof
thepre-trainedmodelplusaweightedsumofthefine-tunedpartsoftheindividualmodels. Ifwecanidentifythemost
relevantexpertsforagiveninput,wecandynamicallyselectthecorrespondingfine-tunedpartstocombinewiththe
pre-trainedpart. ThenthemergingerrorinEq.(11)canbeminimized. Mathematically,wecanexpressthisideaas:
m n (cid:32) T (cid:33) T
y =pre-trainedpart+(cid:88)(cid:88) (cid:88) λ (cid:16) x(i)(cid:17) δ(l) ⟨x(i),v ⟩u +(cid:88) λ (cid:16) x(i)(cid:17) ∆b(l). (13)
merged i jk k j i
j=1k=1 l=1 l=1
Here,λisafunctionthatmapstheinputtoaone-hotprobabilitydistributionoverthetasks,i.e. λ
(cid:0) x(i)(cid:1)
=1ifj =i,
j
andλ
(cid:0) x(i)(cid:1)
=0otherwise. However,anaiveimplementationofthisideawouldrequireatrainingprocesstolearnthe
j
parametersoftherouterandalargenumberofadditionalparameterstostorethefine-tunedweightsofalltasks. A
moreefficientapproachistoremovelesssignificanttermsfromthefine-tunedcomponentsinEq.(13),focusingon
retainingthemostpertinentknowledgeforeachtask. Therefore,theparameterspacemustberankedbytheimportance
ofitsdimensions. However,frompreviousfindingsinSection2,weknowthatthefine-tunedinformationisdistributed
acrosslesssignificantdimensions(SpaceII&III),whichisalargeportionofthewholespace. WeopttouseSVDto
decomposetheparameterdifferences∆W(i) foreachtask,andthenapplyalow-rankapproximationtoextractthe
mostimportantpartasfollows:
r(i) k
∆W(i) =UiΣ(i)V(i)⊤ =(cid:88) σ(i)u(i)v(i)⊤ ≈(cid:88) σ(i)u(i)v(i)⊤ =U(i)Σ(i)V(i)⊤ ,1≤k ≤r(i). (14)
j j j j j j k k k
j=1 j=1
Wherer(i)istherankofthefine-tunedweightmatrix∆W(i),andkistherankofthelow-rankapproximation,which
isdeterminedasahyperparameter. U(i) andV(i) containsthefirstk columnsofU(i) andV(i),respectively. Here
k k
wedropthetermswithindicesj > k inthesummation, whichcorrespondtothelesssignificantdimensions. Let
A(i) =Σ(i)V(i)⊤ andB(i) =U(i),wecanexpresstheapproximationsimilartoaLoRAadapter: ∆Wx=B(i)A(i)x.
k k k
Thefollowingtheoremstatestheoptimalityofthislow-rankapproximation.
6
trap
deniart-erp
derahsSMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
Theorem2 Given a matrix W ∈ Rm×n, its low-rank approximation W = U Σ V⊤ with rank k minimizes the
k k k k
FrobeniusnormofthedifferencebetweenW andW ,i.e. W =argmin ∥W −W′∥ .
k k rank(W′)=k F
AnotherkeycomponentoftheSMILEmodelistherouter,whichdeterminestheroutingweights. Theroutingweights
shouldreflecttheimportanceofeachexpertforagiveninput,andwehypothesizethatthemostimportantdimensions
oftheparameterupdateshavealargerprobabilityofaligningwiththeinputvector. Weprovidearationaleforthis
hypothesisbyexaminingthegradientflowthroughoutfine-tuninginAppendixB.Therefore,wedesigntherouting
logitsastheL normoftheprojectionsoftheinputontothelow-rankmatrices. Mathematically,wecanexpressthe
2
routingweightsas:
(cid:13) (cid:13)
r(i) =(cid:13) (cid:13) (cid:13) (cid:13)k (cid:88)gate(cid:68) x,v j(i)(cid:69)(cid:13) (cid:13) (cid:13) (cid:13) =(cid:13) (cid:13) (cid:13)V k( gi a)⊤ tex(cid:13) (cid:13) (cid:13) 2. (15)
(cid:13)j=1 (cid:13)
2
Wherek isthenumberofdimensionsusedforrouting,whichisahyperparameter. k shouldnotbeexcessively
gate gate
large,whichcoulddiminishthedistinctivenessofroutingweights. Intheextremecasewherek =n,r(i) =∥x∥ ,
gate 2
whichisequivalenttoauniformdistributionoverallexperts. Inourhyperparameteranalysis,wefindthatk =4or
gate
8isagoodchoiceformosttasks. Tosummarize,theoutputoftheSMILEmodulecanbeexpressedas:
T (cid:18) (cid:19)
y =(Wx+b)+(cid:88) λ i U(i)Σ(i)V(i)⊤ x+∆b(i) (16)
(cid:80)T
λ
k k k
i=1 j=1 i
(cid:26)
p , p ∈TopK({p }T ,K)
λ = i i j j=1 (17)
i 0, otherwise
p =softmax (r(i))=softmax (cid:16)(cid:13) (cid:13)V(i)⊤ x(cid:13) (cid:13) (cid:17) . (18)
i i i (cid:13) kgate (cid:13)
2
ComplexityAnalysis:Thelinearlayerhasm(n+1)parameters.TheupscaledSMILEmodulehasm(n+1)+T(mk+
nk+m)+nTk parameters,theadditionalparametershaveaspacecomplexityofO(T(mk+n(k+k ))).
gate gate
Foreveryinputtoken, anadditionalnumberofparametersofnTk +K(mk+nk+m)areactivated, withK
gate
representingthetop-Khyperparameter. Forinstance,withT =8,k =4,k =32,K =1andm=n=1024,the
gate
SMILEmodelhas565K additionalparameters,whichisabout53.9%oftheoriginalparametercount. 99K additional
parametersareactivatedforeachinputtoken,whichisonlyabout9.4%oftheoriginalparametercount.
ExtendingtoParameter-efficientfine-tuned(PEFT)models: ItisstraightforwardtoextendtheSMILEupscaling
toPEFTmodels, suchasLoRAfine-tunedmodels. Wecanstilldecomposethefine-tuningupdatesusingSVDas
∆W = B A . Notethatforparameter-efficientfine-tunedmodels,suchasLoRAfine-tunedmodels,
LoRA LoRA LoRA
k shouldbesettoasmallervaluethanthehyperparameterrankofLoRAr ,andk ≤r .
gate LoRA LoRA
5 Experiments
In this section, we evaluate the effectiveness of the Table1: Requirementsofdifferentmodelfusionmethods.
proposed SMILE on a variety of setups, including
imageclassificationandtextgeneralizationtasks, as Methods ValidationSet Test-TimeAdaptation
well as full fine-tuning and LoRA fine-tuning. De-
WeightAveraging
tailed information about the fine-tuned models is in Fisher-Merging ✓
Appendix C. We compare our method with several RegMean ✓
SOTAmodelfusiontechniques,includingSimpleAv- TaskArithmetic ✓
eraging[Wolfetal.,2019b],Fishermerging[Matena Ties-Merging ✓
and Raffel, 2022], RegMean [Jin et al., 2022], Task AdaMerging ✓
Arithmetic[Ilharcoetal.,2022],Ties-Merging[Yadav WEMoE ✓
et al., 2023], AdaMerging [Yang et al., 2024c], and
SMILE(Ours)
WEMoE[Tangetal.,2024c]. Tofurtherdemonstrate
thescalabilityofSMILEupscaling,wealsoconductexperimentsusingoff-the-shelflargelanguagemodelsfine-tuned
fromMistral-7B-v0.1. OurcodeimplementationisbasedonFusionBench[Tangetal.,2024a].
5.1 Multi-TaskModelFusiononOpen-VocabularyImageClassificationTasks
7SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
Table2: Multi-taskmodelfusionmethodsoneightimageclassificationtasksusingCLIP-ViT-B/32models. Herewe
showtwodifferenthyperparametersettingsforourmethod: (1)k = 16,k = 32andthenormalizedparameter
gate
countis1.61;(2)k =16,k =128andthenormalizedparametercountis3.07.
gate
Method SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Avg.
Individual 75.0 78.3 95.2 99.0 97.3 98.9 99.6 79.7 90.3(100%)
SimpleAveraging 65.4 62.6 70.8 76.9 64.5 54.9 86.3 50.9 66.5(73.6%)
FisherMerging 66.7 64.0 72.2 91.6 69.0 64.3 83.5 53.7 70.6(78.2%)
RegMean 67.8 68.9 82.5 94.4 90.6 79.2 97.6 63.2 80.5(89.1%)
TaskArithmetic 57.1 55.7 64.9 76.7 77.9 68.5 96.1 47.2 68.0(75.3%)
Ties-Merging 67.1 64.2 74.1 76.8 77.7 69.4 94.1 54.0 72.2(80.0%)
AdaMerging 67.9 71.3 83.5 92.7 87.4 92.9 98.2 67.0 82.6(91.5%)
WEMoE(×6.27) 73.7 76.8 93.4 98.2 96.8 98.2 99.6 76.6 89.2(98.8%)
SMILE(1,×1.61) 73.6 74.4 89.5 98.1 95.4 97.3 99.5 76.3 87.7(97.1%)
SMILE(2,×3.07) 73.6 77.8 92.0 98.3 96.9 98.1 99.6 78.1 89.3(98.9%)
Table3: Multi-taskmodelfusionmethodsoneightimageclassificationtasksusingCLIP-ViT-L/14models. Herewe
showtwodifferenthyperparametersettingsforourmethod: (1)k = 16,k = 32andthenormalizedparameter
gate
countis1.47;(2)k =16,k =128andthenormalizedparametercountis2.56.
gate
Method SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Avg.
Individual 82.8 92.9 97.4 99.2 97.9 99.2 99.8 85.5 94.3(100%)
SimpleAveraging 72.5 81.5 82.2 90.0 81.6 74.0 96.6 61.8 80.0(84.8%)
FisherMerging 70.6 79.4 84.1 98.1 74.7 85.0 89.5 61.0 80.3(85.2%)
RegMean 75.3 88.4 90.0 97.1 95.9 92.4 98.5 72.6 88.8(94.2%)
TaskArithmetic 72.0 79.0 80.5 86.0 87.5 83.5 98.0 58.8 80.7(85.6%)
Ties-Merging 74.7 83.3 86.4 91.3 89.7 85.2 97.8 63.9 84.0(89.1%)
AdaMerging 78.1 90.7 90.8 96.5 94.8 97.5 98.6 81.3 91.0(96.5%)
WEMoE(×6.40) 81.5 92.3 96.5 98.8 97.6 99.4 99.6 84.5 93.8(99.5%)
SMILE(1,×1.47) 79.9 91.0 94.3 99.0 97.9 98.6 99.7 82.2 92.8(98.4%)
SMILE(2,×2.56) 81.9 92.3 95.5 99.1 98.0 98.9 99.7 83.6 93.6(99.3%)
WefirstevaluateourproposedSMILEmethodoneightdiverseopen-
95
vocabularyimageclassificationtasksusingCLIPmodelsfromHug-
gingFace12. Foreachtask,thetextencoderofthepre-trainedmodel Ours
90
isfrozen,andonlythevisionencoderisfine-tuned. Table1presents Pretrained
therequirementsofdifferentmodelfusionmethods,highlightingthat 85 Simple Average
SMILEisatraining-freemodelfusionmethodthatdoesnotrequire Task Arithmetic
additionallabeledsamplesortest-timeadaptation. 80 Ties-Merging
Fisher Merging
Figures1and4illustratestheaverageaccuracyofthemergedmodel RegMean
75
acrossdifferentmethods,forSMILEk issetto16andkisvaried Layer-Wise AdaMerging
gate
from4to128. Thesetwofiguresdemonstratetheeffectivenessof Weight-Ensembling MoE
70
SMILEandthetrade-offbetweenperformanceandmodelsize. Individuals
65
In Tables 2 and 3, we compare the performance of various model
fusionmethodsonCLIP-ViT-B/32andCLIP-ViT-L/14models,re- 2 4 6 8
Normalized Number of Parameters
spectively. OurSMILEmethodachievescompetitiveresultsacross
all tasks. For instance, with CLIP-ViT-L/14 models, SMILE (2:
Figure4: Multi-taskmodelfusionexperiment
k = 16,k = 128)achieves99.3%oftheindividualmodelper-
gate oneightimageclassificationtasksusingCLIP-
formance while using only 2.56 times the parameters of a single
ViT-L/14models(k =16).
gate
model,comparedtoeightindividualfine-tunedmodelsandWeight-
EnsemblingMoEwhichrequires6.40timestheparameters.
1https://huggingface.co/openai/clip-vit-base-patch32
2https://huggingface.co/openai/clip-vit-large-patch14
8
ycaruccA
egarevASMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
Table4: Multi-taskperformancewhenmergingFlan-T5-base(fullfine-tuned)modelsonalleighttasks. Hereweshow
twodifferenthyperparametersettingsforourmethod: (1)k =4,k =16andthenormalizedparametercountis
gate
1.26;(2)k =8,k =32andthenormalizedparametercountis1.52.
gate
Method CoLA MNLI MRPC QNLI QQP RTE SST2 STSB Avg.
Individual 75.0 83.4 87.5 91.5 85.4 85.9 93.6 88.7 86.4(100%)
WeightAveraging 69.1 62.6 79.4 89.8 83.9 81.2 91.7 73.2 78.9(91.3%)
TaskArithmetic 70.5 57.8 78.4 90.2 83.6 80.5 92.3 77.8 78.9(91.3%)
Ties-Merging 70.3 65.0 78.9 90.2 83.5 81.6 91.7 78.3 79.9(92.5%)
SMILE(1,×1.26) 72.0 84.2 84.3 91.3 84.7 84.1 93.3 87.0 85.1(98.5%)
SMILE(2,×1.52) 73.2 84.2 85.0 91.3 84.9 84.8 93.5 87.3 85.5(99.0%)
Table5: Multi-taskperformancewhenmergingFlan-T5-base(LoRAfine-tuned)modelsonalleighttasks. Wechoose
k =2,k =4andthenormalizedparametercountis1.02.
gate
Method CoLA MNLI MRPC QNLI QQP RTE SST2 STSB Avg.
Individual 69.1 82.7 85.5 90.9 84.0 84.4 92.9 87.4 84.6(100%)
WeightAveraging 69.7 59.7 78.9 90.1 83.8 80.5 91.2 72.0 78.2(92.4%)
TaskArithmetic 68.8 55.2 78.7 89.8 83.7 79.1 91.5 72.4 77.4(91.5%)
Ties-Merging 68.3 56.3 79.4 89.8 83.7 79.4 91.6 71.2 77.5(91.6%)
SMILE(×1.02) 69.3 82.9 83.8 90.6 83.9 83.4 93.1 85.1 84.0(99.3%)
5.2 Multi-TaskModelFusiononTextGeneralizationTasks
WefurtherevaluateSMILEontextgeneralizationtasksusingFlan-T5-basemodels3,whicharefine-tunedoneight
tasksfromtheGLUEbenchmark[Wangetal.,2018]. Weusetwodifferentfine-tuningstrategies: fullfine-tuningand
LoRAfine-tuningwithr =16. WepresenttheresultsinTables4and5,forfullfine-tunedmodelsandLoRA
LoRA
fine-tunedmodels,respectively. Forfullyfine-tunedmodels,SMILEconsistentlyoutperformsotherfusionmethods
acrossalleighttasks. Withjust1.52timestheparametersofasinglemodel,SMILE(2: k =8,k =32)achieves
gate
99.0%oftheindividualmodelperformancewith1.52timestheparametersofasinglemodel. IntheLoRAfine-tuned
scenario,SMILEmaintainsstrongperformancewithminimalparameterincrease(1.02times). Itachieves99.3%ofthe
individualmodelperformance,significantlysurpassingothermulti-taskmodelfusionmethods.
5.3 SpareMixtureofLow-RankExpertsAnalysis
TobetterunderstandSMILE,wefurtherconductablationstudiesusingCLIPandFlan-T5models.
3https://huggingface.co/google/flan-t5-base
3.0 84.0
85.1 85.1 85.2 85.3 1.26 1.50 1.97 2.92 84.0 84.0 84.0 1.02 1.03 1.06 1.06
85.3 85.5 85.5 85.6 85.4 1.29 1.52 2.00 2.94 2.5
83.8 83.9 83.8 1.02 1.04 1.06 83.9
85.2 85.3 85.5 85.5 1.34 1.57 2.05 2.99 2.0 1.04
85.2
85.0 85.3 85.3 85.3 1.44 1.68 2.15 3.09 1.5 83.9 83.8 83.9 1.03 1.04 1.07
1.02
16 32 64 128 16 32 64 128 4 8 16 4 8 16
k k k k
(a) (b) (c) (d)
Figure 5: Hyperparameter analysis of the Flan-T5-Base models on eight tasks from GLUE benchmark. We show
howdifferentvaluesofhyperparameterskandk affecttheaverageperformanceandthenormalizednumberof
gate
parametersintheupscaledmodel.Subfigures(a),and(b)showtheresultsofthefullfine-tunedmodels,whilesubfigures
(c),and(d)showtheresultsofthefine-tunedmodelswithr =16.
LoRA
9
etagk
4
8
61
23
etagk
4
8
61
23
etagk
2
4
8
etagk
2
4
8SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
100 SVHN
Cars
RESISC45
90
EuroSAT
GTSRB
80 MNIST
DTD
70
SUN397
2 4 6 8 2 4 6 8
Top-K Top-K
(a) Average accuracy. (b) Accuracy of each task.
Figure6: AblationsontheTop-K routingforCLIP-ViT-B/32modelsoneightimageclassificationtasks(k =
gate
16,k =32). Hereweshowtheaverageaccuracyandtheaccuracyoneachtask,andthey-axisisshared.
Ablationsonthelow-rankapproximationrankkandroutingdimensionk (hyperparameteranalysis). Our
gate
hyperparameteranalysisdemonstratestheflexibilityandrobustnessofSMILEacrossdifferentmodelarchitecturesand
tasks. Figure5illustratestheimpactofhyperparameterskandk onperformanceandparametercountforFlan-
gate
T5-BasemodelsinbothfullandLoRAfine-tunedscenarios. ForCLIP-ViTmodels,Figures8and9providedetailed
heatmapsandlineplotsshowingtherelationshipbetweenhyperparameters,averageaccuracy,andparametercount.
Acrossallmodels,weobserveaconsistenttrend: increasingk andk generallyleadstoimprovedperformance,
gate
butwithdiminishingreturnsasparametercountgrows. Notably, SMILEachievesnear-optimal performance with
relativelysmallvaluesofkandk .ThisanalysishighlightstheeffectivenessofSMILEinbalancingperformanceand
gate
efficiency,allowinguserstofine-tunethetrade-offbasedontheirspecificrequirements. Thestabilityofperformance
acrossarangeofhyperparametervaluesalsounderscorestherobustnessofourmethod,makingitadaptabletovarious
multi-taskfusionscenarios. Formoredetails,pleaserefertoAppendixD.
AblationsonTop-Krouting(routinganalysis). HerewecomparedifferentvaluesKinthetop-Kroutingmechanism.
TheplotsinFigure6illustratetheimpactofvaryingK onboththeaverageaccuracyacrossalltasks(Figure6a)and
theaccuracyofeachindividualtask(Figure6b)whenusingtheCLIP-ViT-B/32modelacrosseightimageclassification
tasks. WeobservethattheaverageaccuracydecreasesslightlyasK increases. ThissuggeststhatlargervaluesofK,
whichallowmoreexpertstobeusedforeachinput,arenotnecessaryformulti-taskmodelfusionwhereeachexpertis
specializedforaspecifictask. Ingeneral,theperformanceofindividualtasksisrelativelystableacrossdifferentvalues
ofK,indicatingtherobustnessoftheroutingmechanismofSMILE(Equation(15)).
5.4 ScalabilitytoLarge-ScaleModels(Mistral-7Bmodels)
To demonstrate the scalability of SMILE to large-scale mod-
els, we conduct experiments on Mistral-7B models. We use 0.70 GSM8K Score of M1
Mistral-7B-v0.1 as our base pre-trained model, referred to as
0.65
M 0,andacquirethreespecializedmodelsfromHuggingFace[Wolf M0;1
et al., 2019a]. The expert models are respectively labeled as M 1, 0.60 M0;2
M 2,andM 34. Ofthesemodels,M 1standsoutasthesoleexpertin M0;3
mathematics. Todemonstratetheefficacyofthezero-shotrouting 0.55 M0;123
mechanisms,weconstructfourdistinctseriesofSMILEmodelswith
variousexpertcombinations. TheseseriesaredesignatedasM , 0.50
0;1
M ,M ,andM ,withM indicatingtheSMILEmodel
0;2 0;3 0;123 0;i1...in 0.45
thatcombinesM withtheexpertmodelsM ,...,M .
0 i1 in
0 100 200 300 400 500
In Figure 7, we present the GSM8K benchmark scores of the up- rank of local experts k
scaledMistral-7Bmodelswithvaryingrankoflocalexpertskwitha
Figure 7: The GSM8K benchmark scores of
constantrankofroutersk =8. Thisplothighlightsthetrade-offs
gate upscaledMistral-7Bmodelswithvaryingk.
in selecting expert rank k for the upscaled SMILE model, where
theGSM8Kscoregenerallyimprovesask increases,butthisimprovementismorepronouncedforspecificexpert
combinations,particularlyforM andM . Thissuggeststhattherouterssucceedinselectingtheproperexpert
0;1 0;123
formathproblems. InTable6,wecomparetheperformanceofindividualmodelsandtheupscaledmodelonvarious
benchmarktasks. Notably,theindividualexpertmodelsshowstrengthsinspecificbenchmarks,suchasM excellingin
1
4The expert models are meta-math/MetaMath-Mistral-7B, cognitivecomputations/dolphin-2.1-mistral-7b and
uukuguy/speechless-code-mistral-7b-v1.0respectively.
10
)%(
ycaruccA
egarevA
)%(
ycaruccA
erocS
hctaM
K8MSGSMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
Table6: ComparisonofindividualMistral-7Bmodelsandtheupscaledmodelonvariousbenchmarktasks.
Model MMLU TruthfulQA GSM8K ARCChallenge
M (pre-trained) 59.64 42.62 38.81 53.92
0
M 60.56 44.79 71.49 51.02
1
M 60.56 55.88 56.93 57.00
2
M 61.18 47.47 48.98 57.68
3
M (11.2B,k =8,k =512) 60.66 52.79 67.85 54.35
0;123 gate
theGSM8KbenchmarkandM intheARCChallenge. Thisindicatesthateachexpertbringsspecializedknowledge,
3
whichwhencombined,enhancestheoverallperformanceinadiversesetoftasks. MoredetailsareinAppendixE.
6 RelatedWork
MixtureofExperts. TheconceptofMixtureofExperts(MoE)isfirstintroducedbyJacobsetal.[1991],involving
trainingmultiplespecializedmodels. Thisconcepthasgainedsignificantattentioninrecentyears[Jiangetal.,2024,
Daietal.,2024],withmuchoftheinnovationfocusingonroutingmechanismsandexpertdesign. Muchinnovation
revolvesaroundthedesignofmoreefficientrouters. Forexample,theSwitchTransformer[Fedusetal.,2022b]selects
onlythetopexpertforeachtoken,simplifyingtheprocessandimprovingscalability. Similarly,[Lewisetal.,2021]use
alinearassignmenttooptimizetoken-expertaffinities,ensuringanequalspreadoftokensamongexperts. Fordetailed
reviewsonMoE,see[Fedusetal.,2022a]andforMoEinthecontextofmodelmerging,referto[Yadavetal.,2024].
DeepModelFusion. Modeconnectivityrevealsthatdifferentmodelsolutionscanbelinkedbylow-losspathinthe
parameterspace[FreemanandBruna,2016,NagarajanandKolter,2019,Draxleretal.,2018,Frankleetal.,2020,
Entezarietal.,2021,Garipovetal.,2018,Tatroetal.,2020,Yunisetal.,2022,Bentonetal.,2021],facilitatingmodel
fusionbyweightinterpolation[Izmailovetal.,2018,MatenaandRaffel,2022,Wolfetal.,2019b,Kaddour,2022,
Ilharcoetal.,2022,Yadavetal.,2023,Yangetal.,2024c,Wuetal.,2023]. However,thisstrategyalsoposeschallenges,
particularlywhenmergingmodelswithdiversestructures. Alignmenthelpsreducemodeldisparitiesbymatchingand
interpolatingcomponents[Lietal.,2015,Tatroetal.,2020]. Methodsinvolvematchingactivationsorweights[Stoica
etal.,2023,Jinetal.,2022,Yangetal.,2024b],usingchannel-wisegraphmatching[Liuetal.,2022],orapplying
permutationinvariance[Ainsworthetal.,2022]. Anotherlineofresearchismodelmixing,whichcombinesmodels
throughgatingmechanismsordepthconcatenation[Tangetal.,2024c,Luetal.,2024,Tangetal.,2024b,Kimetal.,
2023],allowingformoreflexibleandadaptivefusionstrategies.
7 Conclusion,Limitations,andFutureWork
Inthispaper,weintroducedtheSparseMixtureofLow-RankExperts(SMILE)modelasanovelapproachtomodel
fusion,Ourmethodleveragesazero-shotmechanism,eliminatingtheneedforadditionaltrainingdataorprocesses,
whichmakesithighlypractical. WhiletheMoEmethodisdesignedtobeefficientthroughsparseactivation,itstill
addsextracomputationaloverhead,especiallyasthenumberoftasksorexpertsincreases.
Understandingwhichsubspacescontributemosttotask-specificperformancecouldleadtomoretargetedandefficient
fine-tuningstrategies,potentiallyfocusingonupdatingspecificpartsofthemodelwhileleavingothersintact. Addition-
ally,thisapproachmightbeappliedtootherareas,likemulti-modallargelanguagemodels,wheredifferenttypesof
data(modalities)aretreatedasseparateexperts. Furthermore,itwouldbeworthexploringhowSMILEcanmanage
multi-objectiveoptimizationbyadjustingtheimportanceofdifferentroutingweights. What’smore,developmethods
todynamicallyadjustthenumberofexpertsK pertokenbasedontheinput,potentiallyimprovingefficiencywithout
sacrificingperformance.
11SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
References
SamuelKAinsworth,JonathanHayase,andSiddharthaSrinivasa. Gitre-basin: Mergingmodelsmodulopermutation
symmetries. arXivpreprintarXiv:2209.04836,2022.
GregoryBenton,WesleyMaddox,SanaeLotfi,andAndrewGordonGordonWilson. Losssurfacesimplexesformode
connectingvolumesandfastensembling. InInternationalConferenceonMachineLearning,pages769–779.PMLR,
2021.
GongCheng,JunweiHan,andXiaoqiangLu. Remotesensingimagesceneclassification: Benchmarkandstateofthe
art. ProceedingsoftheIEEE,105(10):1865–1883,2017.
MirceaCimpoi,SubhransuMaji,IasonasKokkinos,SammyMohamed,andAndreaVedaldi. Describingtexturesinthe
wild. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages3606–3613,2014.
DamaiDai,ChengqiDeng,ChenggangZhao,RXXu,HuazuoGao,DeliChen,JiashiLi,WangdingZeng,XingkaiYu,
YWu,etal. Deepseekmoe: Towardsultimateexpertspecializationinmixture-of-expertslanguagemodels. arXiv
preprintarXiv:2401.06066,2024.
FelixDraxler,KambisVeschgini,ManfredSalmhofer,andFredHamprecht. Essentiallynobarriersinneuralnetwork
energylandscape. InInternationalconferenceonmachinelearning,pages1309–1318.PMLR,2018.
RahimEntezari,HanieSedghi,OlgaSaukh,andBehnamNeyshabur. Theroleofpermutationinvarianceinlinearmode
connectivityofneuralnetworks. arXivpreprintarXiv:2110.06296,2021.
William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint
arXiv:2209.01667,2022a.
WilliamFedus,BarretZoph,andNoamShazeer. Switchtransformers: Scalingtotrillionparametermodelswithsimple
andefficientsparsity. JournalofMachineLearningResearch,23(120):1–39,2022b.
JonathanFrankle,GintareKarolinaDziugaite,DanielRoy,andMichaelCarbin. Linearmodeconnectivityandthe
lotterytickethypothesis. InInternationalConferenceonMachineLearning,pages3259–3269.PMLR,2020.
CDanielFreemanandJoanBruna. Topologyandgeometryofhalf-rectifiednetworkoptimization. arXivpreprint
arXiv:1611.01540,2016.
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFoster,LaurenceGolding,
JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,NiklasMuennighoff,ChrisOciepa,JasonPhang,Laria
Reynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,EricTang,AnishThite,BenWang,KevinWang,
andAndyZou. Aframeworkforfew-shotlanguagemodelevaluation,072024.
TimurGaripov,PavelIzmailov,DmitriiPodoprikhin,DmitryPVetrov,andAndrewGWilson. Losssurfaces,mode
connectivity,andfastensemblingofdnns. Advancesinneuralinformationprocessingsystems,31,2018.
MuhammadUsmanHadi, RizwanQureshi, AbbasShah, MuhammadIrfan, AnasZafar, MuhammadBilalShaikh,
NaveedAkhtar,JiaWu,SeyedaliMirjalili,etal. Largelanguagemodels: acomprehensivesurveyofitsapplications,
challenges,limitations,andfutureprospects. AuthoreaPreprints,2023.
PatrickHelber,BenjaminBischke,AndreasDengel,andDamianBorth. Eurosat: Anoveldatasetanddeeplearning
benchmarkforlanduseandlandcoverclassification. IEEEJournalofSelectedTopicsinAppliedEarthObservations
andRemoteSensing,12(7):2217–2226,2019.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
Lora: Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,2021.
GabrielIlharco,MarcoTulioRibeiro,MitchellWortsman,SuchinGururangan,LudwigSchmidt,HannanehHajishirzi,
andAliFarhadi. Editingmodelswithtaskarithmetic. arXivpreprintarXiv:2212.04089,2022.
PavelIzmailov,DmitriiPodoprikhin,TimurGaripov,DmitryVetrov,andAndrewGordonWilson. Averagingweights
leadstowideroptimaandbettergeneralization. arXivpreprintarXiv:1803.05407,2018.
RobertAJacobs, MichaelIJordan, StevenJNowlan, andGeoffreyEHinton. Adaptivemixturesoflocalexperts.
Neuralcomputation,3(1):79–87,1991.
ArthurJacot,FranckGabriel,andClémentHongler. Neuraltangentkernel: Convergenceandgeneralizationinneural
networks. Advancesinneuralinformationprocessingsystems,31,2018.
AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,DevendraSingh
Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint
arXiv:2401.04088,2024.
XisenJin,XiangRen,DanielPreotiuc-Pietro,andPengxiangCheng. Datalessknowledgefusionbymergingweightsof
languagemodels. arXivpreprintarXiv:2212.09849,2022.
12SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
JeanKaddour. Stopwastingmytime! savingdaysofimagenetandberttrainingwithlatestweightaveraging. arXiv
preprintarXiv:2209.14981,2022.
DahyunKim,ChanjunPark,SanghoonKim,WonsungLee,WonhoSong,YunsuKim,HyeonwooKim,YungiKim,
HyeonjuLee,JihooKim,etal.Solar10.7b:Scalinglargelanguagemodelswithsimpleyeteffectivedepthup-scaling.
arXivpreprintarXiv:2312.15166,2023.
AranKomatsuzaki,JoanPuigcerver,JamesLee-Thorp,CarlosRiquelmeRuiz,BasilMustafa,JoshuaAinslie,YiTay,
MostafaDehghani,andNeilHoulsby. Sparseupcycling: Trainingmixture-of-expertsfromdensecheckpoints. arXiv
preprintarXiv:2212.05055,2022.
JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei. 3dobjectrepresentationsforfine-grainedcategorization. In
ProceedingsoftheIEEEinternationalconferenceoncomputervisionworkshops,pages554–561,2013.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. ProceedingsoftheIEEE,86(11):2278–2324,1998.
MikeLewis,ShrutiBhosale,TimDettmers,NamanGoyal,andLukeZettlemoyer. Baselayers: Simplifyingtrainingof
large,sparsemodels. InInternationalConferenceonMachineLearning,pages6265–6274.PMLR,2021.
WeishiLi,YongPeng,MiaoZhang,LiangDing,HanHu,andLiShen. Deepmodelfusion: Asurvey,2023.
YixuanLi, JasonYosinski, JeffClune, HodLipson, andJohnHopcroft. Convergentlearning: Dodifferentneural
networkslearnthesamerepresentations? arXivpreprintarXiv:1511.07543,2015.
ChangLiu,ChenfeiLou,RunzhongWang,AlanYuhanXi,LiShen,andJunchiYan. Deepneuralnetworkfusion
viagraphmatchingwithapplicationstomodelensembleandfederatedlearning. InInternationalConferenceon
MachineLearning,pages13857–13869.PMLR,2022.
ZhenyiLu,ChenghaoFan,WeiWei,XiaoyeQu,DangyangChen,andYuCheng. Twin-merging: Dynamicintegration
ofmodularexpertiseinmodelmerging. arXivpreprintarXiv:2406.15479,2024.
MichaelSMatenaandColinARaffel.Mergingmodelswithfisher-weightedaveraging.AdvancesinNeuralInformation
ProcessingSystems,35:17703–17716,2022.
ShervinMinaee,TomasMikolov,NarjesNikzad,MeysamChenaghlu,RichardSocher,XavierAmatriain,andJianfeng
Gao. Largelanguagemodels: Asurvey. arXivpreprintarXiv:2402.06196,2024.
VaishnavhNagarajanandJZicoKolter. Uniformconvergencemaybeunabletoexplaingeneralizationindeeplearning.
AdvancesinNeuralInformationProcessingSystems,32,2019.
YuvalNetzer,TaoWang,AdamCoates,AlessandroBissacco,BaolinWu,AndrewYNg,etal. Readingdigitsinnatural
imageswithunsupervisedfeaturelearning. InNIPSworkshopondeeplearningandunsupervisedfeaturelearning,
volume2011,page4.Granada,2011.
PeterJ.OlverandChehrzadShakiban. AppliedLinearAlgebra. UndergraduateTextsinMathematics.SpringerInterna-
tionalPublishing,Cham,2018. ISBN978-3-319-91040-6978-3-319-91041-3. doi: 10.1007/978-3-319-91041-3.
URLhttp://link.springer.com/10.1007/978-3-319-91041-3.
OmerSagiandLiorRokach. Ensemblelearning: Asurvey. Wileyinterdisciplinaryreviews: dataminingandknowledge
discovery,8(4):e1249,2018.
JohannesStallkamp,MarcSchlipsing,JanSalmen,andChristianIgel. Manvs.computer: Benchmarkingmachine
learningalgorithmsfortrafficsignrecognition. Neuralnetworks,32:323–332,2012.
GeorgeStoica,DanielBolya,JakobBjorner,PratikRamesh,TaylorHearn,andJudyHoffman. Zipit! mergingmodels
fromdifferenttaskswithouttraining. arXivpreprintarXiv:2305.03053,2023.
AnkeTang,LiShen,YongLuo,YibingZhan,HanHu,BoDu,YixinChen,andDachengTao. Parameterefficient
multi-taskmodelfusionwithpartiallinearization. arXivpreprintarXiv:2310.04742,2023.
AnkeTang,LiShen,YongLuo,HanHu,BoDo,andDachengTao. Fusionbench: Acomprehensivebenchmarkofdeep
modelfusion. arXivpreprintarXiv:2406.03280,2024a.
Anke Tang, Li Shen, Yong Luo, Shiwei Liu, Han Hu, and Bo Du. Towards efficient pareto set approximation via
mixtureofexpertsbasedmodelfusion. arXivpreprintarXiv:2406.09770,2024b.
AnkeTang,LiShen,YongLuo,NanYin,LefeiZhang,andDachengTao. Mergingmulti-taskmodelsviaweight-
ensemblingmixtureofexperts. arXivpreprintarXiv:2402.00433,2024c.
NormanTatro,Pin-YuChen,PayelDas,IgorMelnyk,PrasannaSattigeri,andRongjieLai.Optimizingmodeconnectivity
vianeuronalignment. AdvancesinNeuralInformationProcessingSystems,33:15300–15311,2020.
13SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
FanqiWan,XintingHuang,DengCai,XiaojunQuan,WeiBi,andShumingShi. Knowledgefusionoflargelanguage
models. arXivpreprintarXiv:2401.10491,2024a.
FanqiWan,ZiyiYang,LongguangZhong,XiaojunQuan,XintingHuang,andWeiBi. Fusechat: Knowledgefusionof
chatmodels. arXivpreprintarXiv:2402.16107,2024b.
AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman. GLUE:AMulti-Task
Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP
WorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages353–355,Brussels,Belgium,
2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL http://aclweb.org/
anthology/W18-5446.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,PierricCistac,Tim
Rault,RémiLouf,MorganFuntowicz,etal.Huggingface’stransformers:State-of-the-artnaturallanguageprocessing.
arXivpreprintarXiv:1910.03771,2019a.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,PierricCistac,Tim
Rault,RémiLouf,MorganFuntowicz,etal.Huggingface’stransformers:State-of-the-artnaturallanguageprocessing.
arXivpreprintarXiv:1910.03771,2019b.
ChengyueWu,TengWang,YixiaoGe,ZeyuLu,RuisongZhou,YingShan,andPingLuo. π-tuning: Transferring
multimodal foundation models with optimal multi-task interpolation. In International Conference on Machine
Learning,pages37713–37727.PMLR,2023.
JianxiongXiao,JamesHays,KristaAEhinger,AudeOliva,andAntonioTorralba. Sundatabase: Large-scalescene
recognitionfromabbeytozoo.In2010IEEEcomputersocietyconferenceoncomputervisionandpatternrecognition,
pages3485–3492.IEEE,2010.
PrateekYadav,DerekTam,LeshemChoshen,ColinRaffel,andMohitBansal. Resolvinginterferencewhenmerging
models. arXivpreprintarXiv:2306.01708,1,2023.
PrateekYadav,ColinRaffel,MohammedMuqeeth,LucasCaccia,HaokunLiu,TianlongChen,MohitBansal,Leshem
Choshen,andAlessandroSordoni. Asurveyonmodelmoerging: Recyclingandroutingamongspecializedexperts
forcollaborativelearning. arXivpreprintarXiv:2408.07057,2024.
EnnengYang,LiShen,GuibingGuo,XingweiWang,XiaochunCao,JieZhang,andDachengTao. Modelmerging
inllms,mllms,andbeyond: Methods,theories,applicationsandopportunities. arXivpreprintarXiv:2408.07666,
2024a.
EnnengYang,LiShen,ZhenyiWang,GuibingGuo,XiaojunChen,XingweiWang,andDachengTao. Representation
surgeryformulti-taskmodelmerging. Forty-firstInternationalConferenceonMachineLearning,2024b.
EnnengYang, ZhenyiWang, LiShen, ShiweiLiu, GuibingGuo, XingweiWang, andDachengTao. Adamerging:
Adaptivemodelmergingformulti-tasklearning. TheTwelfthInternationalConferenceonLearningRepresentations,
2024c.
LeYu,BowenYu,HaiyangYu,FeiHuang,andYongbinLi. Languagemodelsaresupermario: Absorbingabilities
fromhomologousmodelsasafreelunch. InForty-firstInternationalConferenceonMachineLearning,2024.
DavidYunis,KumarKshitijPatel,PedroHenriquePamplonaSavarese,GalVardi,JonathanFrankle,MatthewWalter,
KarenLivescu,andMichaelMaire. Onconvexityandlinearmodeconnectivityinneuralnetworks. InOPT2022:
OptimizationforMachineLearning(NeurIPS2022Workshop),2022.
Hongling Zheng, Li Shen, Anke Tang, Yong Luo, Han Hu, Bo Du, and Dacheng Tao. Learn from model beyond
fine-tuning: Asurvey. arXivpreprintarXiv:2310.08184,2023.
14SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
A ProjectingFine-tunedUpdatesontoSubspaces
Thissectionprovidesanin-depthmathematicalexplanationoftheprojectionmergeexperimentsdiscussedinSection2.
Theseexperimentsaimtogainempiricalinsightsintothedistributionoftask-specificinformationacrossdifferent
subspacesoftheweightmatrixafterfine-tuningapre-trainedmodelondownstreamtasks.
LetW ∈Rm×nbetheweightmatrixofalinearlayerinthepre-trainedmodel,andW ∈Rm×nbethecorresponding
ft
weightmatrixafterfine-tuning. Wedefinetheweightupdateas∆W = W −W. Webeginbyperformingafull
ft
SingularValueDecomposition(SVD)onthepre-trainedweightmatrixW:
W =UΣV⊤ (19)
whereU ∈Rm×mandV ∈Rn×nareorthonormalmatricescontainingleftandrightsingularvectors,respectively,and
Σ∈Rm×ncontainsthesingularvaluesindescendingorder. ThefirstrdiagonalelementsofΣarenon-zero,where
r = rank(W),whiletheremainingelementsarezero. AccordingtoTheorem1,wecanleveragethepropertiesof
singularvaluedecomposition(SVD)togainadeeperunderstandingofthefine-tuningprocess. Thistheoremstatesthat
anymatrixA∈Rm×ncanbedecomposedintoasumofrank-onematricesusingtheleftsingularvectors{u }m and
i i=1
rightsingularvectors{v }n asbases:
i i=1
m n
(cid:88)(cid:88)
A= α u v⊤ =UΣ V⊤ (20)
ij i j A
i=1j=1
where α = ⟨A,u v⊤⟩ is the projection of A onto the basis u v⊤, Σ is a real matrix and Σ (i,j) = α . This
ij i j i j A A ij
decompositionprovidesapowerfulframeworkforanalyzingthefine-tuningprocess. Whenwefine-tuneapre-trained
model,wecaninterprettheweightupdates∆W asmodificationstothesingularvaluematrixΣ,whilethesingular
vectorsU andV remainconstant. ThenwepartitionthesingularmatrixΣintothreezones:
• ZoneI&SubspaceI:{1,...,r },wherer
ischosensuchthat(cid:80)rhalf
σ ≈
1(cid:80)r
σ . Thebasisof
half half i=1 i 2 i=1 i
thissubspaceis{u v⊤|1≤i,j ≤r }. Theprojectionmergedweightsinthissubspacecanbecomputedas
i j half
follows:
rhalfrhalf
(cid:88) (cid:88)
W =W + ⟨∆W,u v⊤⟩u v⊤ =W +U U⊤ ∆WV V⊤ . (21)
I i j i j rhalf rhalf rhalf rhalf
i=1 j=1
WhereU andV arethefirstr columnsofU andV,respectively.
rhalf rhalf half
• Zone II & Subspace II: {r + 1,...,r}, where r = rank(W). The basis of this subspace is
half
{u v⊤}r . The basis of subspace II & III is {u v⊤|r + 1 ≤ i ≤ r,r + 1 ≤ j ≤ r}.
i i i=rhalf+1 i j half half
Theprojectionmergedweightsinthissubspacecanbecomputedasfollows:
r r
(cid:88) (cid:88)
W =W + ⟨∆W,u v⊤⟩u v⊤ =W +U U⊤ ∆WV V⊤ .
II i j i j rhalf+1:r rhalf+1:r rhalf+1:r rhalf+1:r
i=rhalf+1j=rhalf+1
(22)
WhereU andV arethe(r +1)-thtor-thcolumnsofU andV,respectively.
rhalf+1:r rhalf+1:r half
• ZoneIII&SubspaceII+III:Thebasisofthissubspaceis{u v⊤|r+1≤i≤m,r+1≤j ≤n}andthe
i j
projectionmergedweightsinthissubspacecanbecomputedasfollows:
m n
(cid:88) (cid:88)
W =W + ⟨∆W,u v⊤⟩u v⊤ =W +U U⊤ ∆WV V⊤ . (23)
II+III i j i j r+1:m r+1:m r+1:n r+1:n
i=r+1j=r+1
WhereU andV arethe(r+1)-thtom-thcolumnsofU and(r+1)-thton-thcolumnsofV.
r+1:m r+1:n
Wethenevaluatetheperformanceofthesemodifiedweightmatricesonthedownstreamtasks. Theaccuracycomparison
inFigure2bisobtainedbyusingthesemodifiedweightmatricesinplaceoftheoriginalpre-trainedweights. Forother
layersinsteadofthelinearlayers,wekeepthepre-trainedweightsunchanged.
B TheGradientFlowDuringFine-tuning
Inthissection,weanalyzethegradientflowduringthefine-tuningprocesstogaininsightsintohowlinearlayersina
deepneuralnetworkadapttonewtasks. Wedecomposeafine-tuneddeepneuralnetworkf intothreecomponents:
15SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
thepre-linearlayersf ,thelinearlayersf (x)=W x+b ,andthepost-linearlayersf . Therefore,the
pre linear ft ft post
outputofthenetworkcanbeexpressedasf(x)=f (f (f (x))). Withoutlossofgenerality,thepre-linear
post linear pre
layersf canbedropped,asourfocusisonthefine-tuningprocessofthelinearlayerf .
pre linear
During fine-tuning, the model parameters are updated by minimizing a loss function L with respect to the model
weights. Usingstochasticgradientdescent(SGD)astheoptimizationalgorithm,theweightupdate∆W andbiasupdate
∆bateachstepcanbeexpressedas:
(cid:16) (cid:16) (cid:17)(cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
W(t+1)−W(t) =−η∇ L f W(t)x+b(t) , b(t+1)−b(t) =−η∇ L f W(t)x+b(t) . (24)
W post b post
Whereηisthelearningrate,and∇ Land∇ Larethegradientsofthelossfunctionwithrespecttotheweightsand
W b
biases,respectively. InEq.(24),weomitthepre-linearlayersf andusexastheinputtothelinearlayerf for
pre linear
simplicity. Lety =Wx+bbetheoutputofthelinearlayer,L′ =L◦f bethecomposedlossfunction. Starting
post
fromtheSGDupdaterulefortheweights,wehave:
(cid:16) (cid:17)
W(t+1)−W(t) =−η∇ L′ W(t)x+b(t) (25)
W
(cid:16) (cid:17)
=−η∇ L′·∇ W(t)x+b(t) (26)
y W
=−η∇ L′·xT, (27)
y
Inpractice,wetypicallyusemini-batchSGD,whereweaveragethegradientsoverabatchofsamples. Wecanrepresent
thisasanexpectation:
W(t+1)−W(t) =−ηE [∇ L·xT] (28)
x∼p(x) y
Giventhisgradientupdaterule,wecananalyzehowitrelatestoourchoiceofroutinglogitsinSMILE.Recallthatwe
definedtheroutinglogitsas:
(cid:13) (cid:13)
r(i) =(cid:13)V(i)⊤ x(cid:13) (29)
(cid:13) kgate (cid:13)
2
Let’sconsidertheexpectedweightupdateovermanyiterationswhenW isclosetoW:
ft
E[∆W]≈−ηE [∇ L·xT] (30)
x∼p(x) y
=−ηE [g·xT] (31)
x∼p(x)
whereg =∇ Listhegradientofthelosswithrespecttotheoutputofthelinearlayer. Now,let’sconsiderthesingular
y
valuedecomposition(SVD)oftheexpectedweightupdate:
E[∆W]=UΣVT (32)
TherightsingularvectorsV representthedirectionsintheinputspacethataremostimportantfortheweightupdates.
Ourroutinglogitsr(i)arebasedonprojectingtheinputontothetopk rightsingularvectorsofthefine-tunedweight
gate
difference ∆W(i). This choice is justified because: the right singular vectors of ∆W(i) are likely to be similar to
thoseofE[∆W],asbothrepresentimportantdirectionsfortask-specificupdates. Inaddition,byprojectingontothese
vectors,we’remeasuringhowmuchaninputalignswiththedirectionsthatweremostimportantduringfine-tuningfor
aspecifictask.
ANTKperspective. Ontheotherhand,wecananalyzethefine-tuningprocessfromaneuraltangentkernel(NTK)
perspective. Following[Tangetal.,2023],andaccordingtothelinearpropertyofthelinearlayer,wehave:
f (x;ϕ )−f (x;ϕ) (33)
linear ft linear
=∇ f (x;ϕ)⊤(ϕ −ϕ) (34)
ϕ linear ft
(cid:104) (cid:105)
≈−ηE ∇ f (x;ϕ)⊤∇ L′(f (x′;ϕ)) (35)
x′∼p(x) ϕ linear ϕ linear
(cid:104) (cid:105)
=−ηE ∇ f (x;ϕ)⊤∇ f (x′;ϕ)∇ L′(f (x′;ϕ)) (36)
x′∼p(x) ϕ linear ϕ linear flinear linear
=−ηE [K(x,x′;ϕ)∇ L′(f (x′;ϕ))], (37)
x′∼p(x) flinear linear
Whereϕdenotesthepre-trainedparametersofthef ,i.e. W andb. ϕ denotesthefine-tunedparametersW
linear ft ft
andb . K(x,x′;ϕ) = ⟨∇ f (x;ϕ),∇ f (x′;ϕ)⟩istheneuraltangentkernel(NTK)[Jacotetal.,2018]
ft ϕ linear ϕ linear
ofthelinearlayerf ,andL′ = L◦f isthecomposedlossfunction. Notethatforgivenx,K(x,x′;ϕ)isa
linear post
constantmatrix.
16SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
Table7: Performanceoffine-tunedCLIP-ViT-B/32modelsoneightdownstreamtasks.
Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD
Pre-trained 63.2 59.8 60.7 46.0 31.6 32.5 48.3 43.9
SUN397 75.0 47.0 54.3 46.5 28.3 26.4 44.3 41.6
Cars 56.6 78.3 50.9 38.4 30.2 30.6 49.7 41.8
RESISC45 52.0 47.2 95.2 56.9 23.9 24.3 39.7 35.9
EuroSAT 49.0 39.9 33.5 99.0 11.8 22.9 33.8 35.5
SVHN 40.5 36.3 18.9 9.8 97.3 27.3 81.8 23.2
GTSRB 36.8 33.0 20.6 21.3 41.2 98.9 30.9 23.9
MNIST 50.3 40.0 31.3 17.7 50.1 19.3 99.6 30.7
DTD 54.6 51.3 36.9 25.0 28.9 21.8 47.3 79.7
Table8: Performanceoffine-tunedCLIP-ViT-L/14modelsoneightdownstreamtasks.
Model SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD
Pre-trained 68.3 77.8 71.0 58.9 58.4 50.6 76.4 55.5
SUN397 82.8 68.4 58.1 49.9 55.0 46.3 79.5 52.8
Cars 67.8 92.9 68.7 56.4 51.7 47.7 80.5 55.6
RESISC45 65.6 69.0 97.4 64.3 38.3 46.6 77.7 49.9
EuroSAT 65.2 69.0 40.6 99.2 33.4 45.6 73.5 47.1
SVHN 66.4 69.0 54.0 19.7 97.9 48.7 92.2 50.1
GTSRB 63.4 64.8 38.7 19.6 71.0 99.2 75.1 45.8
MNIST 56.0 49.8 53.5 26.6 48.2 33.1 99.8 47.1
DTD 66.8 75.3 65.5 43.7 49.5 45.0 68.5 85.5
Table9: Performanceoffullfine-tunedFlan-T5-Basemodelsoneightdownstreamtasks.
Model CoLA MNLI MRPC QNLI QQP RTE SST2 STSB
Pre-trained 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2
CoLA 75.0 37.2 72.8 87.6 80.4 76.9 91.4 63.6
MNLI 65.9 83.4 75.7 89.2 82.6 78.0 90.6 66.2
MRPC 63.4 48.3 87.5 85.8 81.1 72.6 88.1 76.1
QNLI 68.7 39.2 75.5 91.5 81.3 78.3 91.6 68.2
QQP 59.1 50.4 73.8 88.3 85.4 81.2 90.8 75.9
RTE 65.4 51.1 69.6 88.7 80.8 85.9 90.3 68.9
SST2 67.8 54.0 76.5 87.8 83.4 80.5 93.6 63.6
STSB 69.3 49.3 76.5 89.0 81.7 77.6 90.1 88.7
Table10: PerformanceofLoRAfine-tuned(r =16)Flan-T5-Basemodelsoneightdownstreamtasks.
LoRA
Model CoLA MNLI MRPC QNLI QQP RTE SST2 STSB
Pre-trained 69.1 56.5 76.2 88.4 82.1 80.1 91.2 62.2
CoLA 69.1 39.9 75.2 89.1 81.1 81.9 90.7 54.0
MNLI 69.4 82.7 73.8 89.3 82.0 79.4 90.9 68.1
MRPC 64.0 44.9 85.5 82.6 81.0 69.0 88.6 73.6
QNLI 68.9 52.7 76.7 90.9 82.8 79.8 91.5 68.9
QQP 65.0 54.6 75.7 89.0 84.0 81.6 90.7 75.3
RTE 64.9 51.8 69.4 89.2 79.8 84.5 90.6 70.1
SST2 68.3 56.6 76.0 88.5 83.4 79.8 92.9 62.6
STSB 65.7 1.7 67.4 89.3 80.1 79.8 90.8 87.4
17SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
C Fine-TunedModelPerformance
Inthissection,wepresenttheperformanceofthefine-tunedmodelsontheircorrespondingtestsets. Theseresults
serveasabaselineforevaluatingtheeffectivenessofourproposedmodelfusiontechnique.
Tables 7 and 8 show the performance of fine-tuned CLIP-ViT-B/32 and CLIP-ViT-L/14 models, respectively, on
eight image classification tasks. These tasks include SUN397 [Xiao et al., 2010], Cars [Krause et al., 2013], RE-
SISC45[Chengetal.,2017],EuroSAT[Helberetal.,2019],SVHN[Netzeretal.,2011],GTSRB[Stallkampetal.,
2012], MNIST[LeCunetal.,1998], andDTD[Cimpoietal.,2014]. Forimageclassificationtasks, wereportthe
classificationaccuracy. Tables9and10presenttheperformanceofFlan-T5-Basemodelsoneighttextgenerationtasks
fromGLUEbenchmark[Wangetal.,2018],usingfullfine-tuningandLoRAfine-tuning(r =16)respectively.
LoRA
WereportSpearman’sρforSTSBandexactmatchaccuracyforothertasks. InparticularforSTSB,ifthetextoutputs
cannotbeparsedintoavalidfloatnumber,weassignascoreofzero. Thedatasetsandfine-tunedmodelsareaccessible
tothepubliconHuggingFace[Wolfetal.,2019a]. Forfurtherinformation,pleaseconsultFusionBench[Tangetal.,
2024a].
Severalkeyobservationscanbemadefromtheseresults:
1. Task-SpecificImprovements: Acrossallmodeltypes,fine-tuningconsistentlyimprovesperformanceonthe
targettaskcomparedtothepre-trainedmodel. Thisdemonstratestheeffectivenessoftask-specificadaptation.
2. NegativeTransfer: Insomecases,weobservenegativetransfer,wherefine-tuningononetaskharmsperfor-
manceonanothertask. Forexample,inTable7,theSVHN-tunedmodelperformsworseonEuroSAT(9.8%)
comparedtothepre-trainedmodel(46.0%).
3. TaskRelatedness: Somefine-tunedmodelsshowimprovedperformanceonrelatedtasks. Forexample,in
Table9,theMNLI-tunedmodelperformswellonQNLI,suggestingatransferofrelevantknowledgebetween
thesenaturallanguageinferencetasks.
4. Varying Task Difficulty: The diagonal entries reveal that some tasks are inherently more challenging than
others. Forinstance,intheCLIP-ViTmodels,EuroSATandMNISTconsistentlyachieveveryhighaccuracy,
whileSUN397,CarsandDTDprovemorechallenging.
5. Model Size Impact: Comparing CLIP-ViT-B/32 and CLIP-ViT-L/14 results, we generally see improved
performancewiththelargermodel,indicatingthatmodelcapacityplaysaroleinbothtask-specificperformance
andgeneralization.
D HyperparameterAnalysis
Inthissection,wepresentacomprehensiveanalysisofthehyperparameterskandk fortheCLIP-ViT-B/32and
gate
CLIP-ViT-L/14modelsacrosseightimageclassificationdatasets. Weexaminetheirimpactonbothmodelperformance
(averageaccuracy)andmodelcomplexity(numberofparameters). Wealsotesttheextremecaseswhenk =∞,which
correspondstofull-rankexperts,denotedas“Dense”inthefigures. Wenormalizethenumberofparametersbythe
numberofparametersintheoriginalmodel(87.5MforCLIP-ViT-B/32and303MforCLIP-ViT-L/14)tofacilitate
comparison.
CLIPModels. FromFigures8and9,weobservethat(1)Theperformanceoftheupscaledmodelsisgenerallybetter
thanthepre-trainedmodels,whichdemonstratestheeffectivenessofourfine-tuningstrategy. (2)increasingthevalues
ofkgenerallyimprovestheperformanceofboththeCLIP-ViT-B/32andCLIP-ViT-L/14models,thoughatthecost
ofincreasedmodelcomplexity. (3)Increasedvaluesofk improvetheperformanceoftheupscaledmodelsatthe
gate
beginning, but the performance starts to decrease when k is too large. This observation is consistent with our
gate
discussioninSection4thatalargerk mayresultinalessdiscriminativegatingmechanism. (4)Betterperformance
gate
preservationcanbeachievedwiththeCLIP-ViT-L/14modelthanwiththeCLIP-ViT-B/32model,whichisconsistent
withourdiscussioninSection3thatthelargermodelhasmoredimensionredundancyandislessseveretotheparameter
interferenceproblem.
Inpractice, whenselectinghyperparametersfortheupscaledmodels, itiscrucialtobalancethetrade-offbetween
performanceandparameteroverhead. TakeCLIP-ViT-B/32asanexample,agoodtrade-offbetweenperformance
andparameteroverheadcanbeachievedwithk ≈ 32andk ≈ 16. FortheCLIP-ViT-L/14model,k ≈ 64and
gate
k ≈8arerecommended. Bydoingso,weobtainamulti-taskmodelthatachievesaround98%oftheperformance
gate
ofthefine-tunedmodelwithonly20%ofthetotalparameterscomparedtomaintainingeightindividualfine-tuned
modelsforeachtask. NotethattheupscaledSMILEmodelissparselyinferenced,increasingthenumberofparameters
byN onlyincreasestheactivatedparametersbyaboutN/T foreachtoken. Evenwithanextremefocusonstorageand
18SMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
75.8 79.9 82.7 84.7 85.7 86.2 86.5 90
78.1 81.9 84.6 86.4 87.2 87.7 88.0 87.5
kgate
79.6 83.1 85.8 87.4 88.3 88.8 89.1 85.0 80 1
79.5 83.6 86.1 87.7 88.5 89.0 89.3 2
82.5
80.3 84.0 86.6 88.2 88.9 89.3 89.6 4
80.5 84.2 86.7 88.1 89.0 89.4 89.6 80.0 70 8
16
80.6 84.3 86.9 88.5 89.1 89.5 89.8 77.5 32
80.6 84.3 86.9 88.4 89.1 89.6 89.8 64
75.0 60 128
80.6 84.4 87.0 88.5 89.2 89.5 89.8
256
80.0 84.1 86.8 88.4 89.1 89.4 89.7 72.5 512
69.5 74.4 78.3 81.0 82.4 83.1 83.6 50 768
70.0
4 8 16 32 64 128 Dense 0 20 40 60 80 100 120
k k
(a)Heatmapofaverageaccuracy. (b)Lineplotofaverageaccuracy.
1.07 1.14 1.26 1.50 1.99 2.96 8.78 14 9
1.08 1.14 1.26 1.51 1.99 2.96 8.79 8
1.10 1.16 1.28 1.52 2.01 2.98 8.81
12 kgate
7 1
1.13 1.19 1.31 1.55 2.04 3.01 8.84 10 2
1.19 1.25 1.37 1.61 2.10 3.07 8.90 6 4
1.31 1.37 1.49 1.74 2.22 3.19 9.02 8 5 8
16
1.55 1.61 1.74 1.98 2.46 3.43 9.26 6 4 32 2.04 2.10 2.22 2.46 2.95 3.92 9.75 64
3.01 3.07 3.19 3.43 3.92 4.89 10.72 4 3 128
256
4.95 5.01 5.13 5.38 5.86 6.83 12.66 2 512
6.89 6.96 7.08 7.32 7.80 8.78 14.60 2 768
1
4 8 16 32 64 128 Dense 0 20 40 60 80 100 120
k k
(c)Heatmapofthenormalizednumberofparameters. (d)Lineplotofnormalizednumberofparameters.
Figure8: HyperparameteranalysisoftheCLIP-ViT-B/32modeloneightimageclassificationdatasets. Hereweshow
howdifferentvaluesofhyperparametersk andk affecttheaverageperformanceandthenumberofparameters
gate
(normalizedbythenumberofparametersintheoriginalmodel)intheupscaledmodel. Wealsoshowtheaverage
accuracyofpre-trainedmodelsandindividualfine-tunedmodelsinsubfigure(b).
inferencecosts,only7%oftheparametersoverheadcanachieveanaveragemulti-taskperformanceofabout90%of
theindividualfine-tunedmodels.
Flan-T5Models. Figure5showsthehyperparameteranalysisoftheFlan-T5-Basemodelsoneighttasksfromthe
GLUEbenchmark. Weconductthesameanalysisforbothfullfine-tunedmodelsandLoRAfine-tunedmodelswith
r = 16. Itisobservedthattheperformanceisrelativelystable, withmostconfigurationsyieldingaccuracies
LoRA
around85.1to85.6forthefullfine-tunedmodelsandaround83.8to84.0fortheLoRAfine-tunedmodels. Upscaling
LoRA fine-tuned models is very parameter-efficient, with the number of parameters increasing by only 2% to 7%
comparedtotheoriginaldensemodel. Forabalancedtrade-offbetweenperformanceandparameteroverhead,consider
settingk ≈32andk ≈8forthefullfine-tunedmodelfusion,andk ≈8andk ≈2fortheLoRAfine-tuned
gate gate
modelfusion.
E Large-ScaleModelExperiments
This appendix provides additional details and results for our experiments with large-scale models, specifically the
Mistral-7Bseries. Weusedthefollowingmodelsinourexperiments,whichareavailableontheHuggingFace:
• Basepre-trainedmodel(M ): mistralai/Mistral-7B-v0.1
0
• ExpertmodelM : meta-math/MetaMath-Mistral-7B
1
19
etagk
etagk
1
2
4
8
61
23
46
821652215867
1
2
4
8
61
23
46 821652215867
ycaruccA
egarevA
sretemaraP
fo
rebmuN
dezilamroNSMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
95
86.0 88.4 90.4 91.7 92.4 92.7 93.0 93
90
86.9 89.1 90.9 92.1 92.7 93.0 93.2 92
85 91
87.5 89.8 91.4 92.6 93.1 93.4 93.6
90 80
kgate
87.8 90.1 91.6 92.7 93.3 93.5 93.8 89 1
75 2
87.9 90.2 91.7 92.8 93.4 93.6 93.9 88 4
70 8
87 16
87.9 90.3 91.8 92.9 93.4 93.6 93.9
65 32
86
4 8 16 32 64 128 Dense 0 20 40 60 80 100 120
k k
(a)Heatmapofaverageperformance. (b)Lineplotofaverageperformance.
9
1.06 1.10 1.20 1.38 1.76 2.51 8.98 2.6
8
2.4
1.06 1.11 1.20 1.39 1.76 2.51 8.99 7
2.2
1.08 1.12 1.22 1.40 1.78 2.52 9.00 6 2.0
5
1.8
kgate
1.10 1.15 1.24 1.43 1.80 2.55 9.02 1
4 1.6 2
1.15 1.19 1.29 1.47 1.85 2.59 9.07 3 1.4 4
8
1.24 1.29 1.38 1.57 1.94 2.69 9.16 2 1.2 16
32
1.0
4 8 16 32 64 128 Dense 0 20 40 60 80 100 120
k k
(c)Heatmapofaverageperformance. (d)Lineplotofaverageperformance.
Figure9: HyperparameteranalysisoftheCLIP-ViT-L/14modeloneightimageclassificationdatasets. Hereweshow
howdifferentvaluesofhyperparametersk andk affecttheaverageperformanceandthenumberofparameters
gate
(normalizedbythenumberofparametersintheoriginalmodel)intheupscaledmodel. Wealsoshowtheaverage
accuracyofpre-trainedmodelsandindividualfine-tunedmodelsinsubfigure(b).
• ExpertmodelM : cognitivecomputations/dolphin-2.1-mistral-7b
2
• ExpertmodelM : uukuguy/speechless-code-mistral-7b-v1.0
3
For the SMILE models, the hyperparameter settings were as follows: k was consistently set to 8 across all
gate
experiments,whilek rangedfrom8to512(including8,16,32,64,128,256,384,and512),asshowninFigure7.
Table11providesamorecomprehensiveviewoftheperformanceofindividualmodelsandvariousSMILEmodels
withdifferentkvalues. WeuseEleutherAI/lm-evaluation-harness[Gaoetal.,2024]toevaluatethemodelson
thefourtasks: MMLU,TruthfulQA,GSM8K,andARCChallenge. Wemergethemodelsonhostmemoryandevaluate
themontwoNVIDIA4090GPUswith24GBofmemoryeach.
Itisnotablethatasthevalueofkincreases,wegenerallyseeimprovedperformance,especiallyintaskslikeGSM8K
andTruthfulQA.Theresultsalsoshowacleartrade-offbetweenmodelsizeandperformance. TheSMILEmodelwith
k =8(7.3Bparameters)alreadyachievescomparableorbetterresultsthanthepre-trainedmodelonalltasks,while
largermodels(k =512,11.2Bparameters)approachtheperformanceofindividualexpertmodels.
LimitationsandfutureworkforLLMs. Hereweprovideabriefdiscussionofthelimitationsofourexperimentsand
potentialdirectionsforfuturework.
• Limitedexpertmodelpool. IntheexperimentsforCLIPmodelsandFlan-t5models,weuseeightexpert
models to evaluate the performance of the SMILE model. However, in the Mistral-7B experiments, the
experiments are currently limited to three expert models, which may not fully demonstrate the method’s
20
etagk
etagk
1
2
4
8
61
23
1
2
4
8
61
23
ycaruccA
egarevA
sretemaraP
fo
rebmuN
dezilamroNSMILE:Zero-ShotSparseMixtureofLow-RankExpertsConstruction APREPRINT
Table11: DetailedperformancecomparisonofindividualmodelsandvariousSMILEmodelswithdifferentkvalues.
Forallupscaledmodels,thek valuewassetto8.
gate
Model MMLU TruthfulQA GSM8K ARCChallenge
M (pre-trained) 59.64 42.62 38.81 53.92
0
M 60.56 44.79 71.49 51.02
1
M 60.56 55.88 56.93 57.00
2
M 61.18 47.47 48.98 57.68
3
M (7.3B,k =8) 60.28 46.31 46.55 55.55
0;123
M (7.5B,k =32) 60.37 49.49 55.04 54.52
0;123
M (8.3B,k =128) 60.43 50.91 63.76 54.35
0;123
M (9.3B,k =256) 60.53 51.83 65.58 54.01
0;123
M (11.2B,k =512) 60.66 52.79 67.85 54.35
0;123
capabilitieswithalarger,morediversesetofexperts. Futureworkcouldexploretheimpactofadditional
expertmodelsontheperformanceoftheSMILEmodel.
• LoRAfine-tuning. Intheexperiments,weusefullfine-tunedMistral-7Bmodelsasexpertmodels. Where
thelinearmodelsareupscaledintoMoEmodulesandtheremainingpartsofthemodelarecopieddirectly
fromthepre-trainedmodel. ThereasonforthisisthatthetopMistral-7BmodelsavailableonHuggingFace
arenowfullyfine-tuned. Thisapproach,however,maynotfullyexploitSMILE’spotential. Amoreeffective
strategycouldinvolveusingLoRAfine-tunedmodelsasexpertmodels. Inthisscenario,onlyspecificlinear
layers would be fine-tuned using low-rank techniques, with the rest of the model remaining frozen. This
approachcouldpotentiallyenhanceSMILE’sefficiencyandeffectiveness. AswehaveshownintheFlan-T5
experiments, LoRA fine-tuning can significantly reduce the number of additional parameters required to
achievecomparableperformance.
21