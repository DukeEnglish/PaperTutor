LCE: A Framework for Explainability of DNNs for
Ultrasound Image Based on Concept Discovery
Weiji Kong1,2,3, Xun Gong1,2,3,∗, Juan Wang4
1School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China
2Engineering Research Center of Sustainable Urban Intelligent Transportation, Ministry of Education, Chengdu, China
3Manufacturing Industry Chains Collaboration and Information Support Technology Key Laboratory of Sichuan Province,
Southwest Jiaotong University, Chengdu, China
4Department of Ultrasound, The Third People’s Hospital of Chengdu,
Affiliated Hospital of Southwest Jiaotong University, Chengdu, China
Abstract—Explaining the decisions of Deep Neural Networks computer vision tasks, XAI tends to explain the output of
(DNNs) for medical images has become increasingly impor- the model by identifying what the most important pixels are.
tant. Existing attribution methods have difficulty explaining the
There are already a large number of XAI methods that can
meaning of pixels while existing concept-based methods are
provide a variety of explanations, such as attribution methods
limited by additional annotations or specific model structures
that are difficult to apply to ultrasound images. In this paper, and concept-based methods.
we propose the Lesion Concept Explainer (LCE) framework, Attribution methods can conveniently improve the explain-
whichcombinesattributionmethodswithconcept-basedmethods. ability of medical DNNs [4]–[6]. However, these methods are
We introduce the Segment Anything Model (SAM), fine-tuned
prone to false activations. They can only explain where in the
on a large number of medical images, for concept discovery to
image a pixel is located that is crucial for decision-making enableameaningfulexplanationofultrasoundimageDNNs.The
proposed framework is evaluated in terms of both faithfulness butareunabletoidentifywhichvisualfeaturesdrivedecision-
and understandability. We point out deficiencies in the popular makingattheselocations.Moreimportantly,explanationsthat
faithfulness evaluation metrics and propose a new evaluation do not match real-world meanings do not inspire confidence
metric. Our evaluation of public and private breast ultrasound
in doctors or patients.
datasets (BUSI and FG-US-B) shows that LCE performs well
Concept-based methods [7], [8] can provide more mean-
compared to commonly-used explainability methods. Finally,
we also validate that LCE can consistently provide reliable ingful explanations, but these techniques usually require ad-
explanations for more meaningful fine-grained diagnostic tasks ditional provision of manually predefined concepts. This is
in breast ultrasound. difficulttoimplementinthemedicalfield,especiallyforultra-
Index Terms—Explainability, Segment Anything Model, Con-
soundimages,whicharefastandlow-cost,andaddingseveral
cept Discovery, Ultrasound
timesthecostforexplanationislikeputtingthecartbeforethe
horse.Therearealsosomestudies[9],[10]thatattempttouse
I. INTRODUCTION
prototypes to bring explainability to medical DNNs, but these
Inrecentyears,DeepNeuralNetworks(DNNs)modelhave
requireaspecificmodelstructure,andprototypesderivedfrom
becomemainstreaminmedicalimageanalysis,outperforming
low-cost ultrasound images may be less accurate.
even human doctors on various tasks in different medical
Explainanyconcept(EAC)[11]pioneeredthecombination
image modalities [1]. As a black-box model, DNNs have
of the SAM [12], which has excellent zero-shot segmentation
difficulty explaining what happens to the input image within
capability, with Shapley value [13], for efficient explanation.
the complex structure of DNNs. Lack of explainability leads
ConceptdiscoveryisthemostfundamentalstepinEAC.How-
to difficulty in gaining the trust of doctors or patients, making
ever, it is difficult for EAC to discover medically meaningful
it challenging to put medical DNNs into clinical use, even if
concepts from medical images due to the huge gap between
they perform well. In addition, some laws and regulations [2],
normal images and medical images.
[3] restrict the use of AI in human society.
Inspired by EAC [11], in this paper, we propose a frame-
Explainable Artificial Intelligence (XAI) has increasingly
work named Lesion Concept Explainer (LCE), which uses
come to the fore as a way of unlocking the black-box. For
attribution methods to guide existing SAM fine-tuned on a
large amount of medical image data in combination with
*XunGongisthecorrespondingauthor(xgong@swjtu.edu.cn).
This study is partially supported by National Natural Science Foundation Shapley values for concept discovery and explanation. We
of China (62376231), Sichuan Science and Technology Program (24NS- implement LCE on the public breast ultrasound dataset BUSI
FSC1070), Science and Technology Project of Sichuan Provincial Health
[14] and the private breast ultrasound dataset FG-US-B using
Commission (23LCYJ022), Fundamental Research Funds for the Central
Universities(2682023ZDPY001,2682022KJ045). ResNet50 [15] as the target model.
This study adhered to the principles of the Helsinki Declaration and The faithfulness metrics Insertion and Deletion [16] can
obtainedinformedconsent(SWJTU-2208-SCS-072).Approvalwasobtained
reflect the consistency of the explanation with the decision
from the Clinical Research Ethics Committee of the Affiliated Hospital of
SouthwestJiaotongUniversity(ChengduThirdPeople’sHospital). process within the model. Since the effect of the explanation
4202
guA
91
]IA.sc[
1v99890.8042:viXraFig. 1: The Lesion Concept Explainer (LCE) Framework. For any black-box DNN model, attribution methods are first used to
generate explanations, which are then used to guide D to explore and obtain concept masks. The concept masks are then
SAM
post-processed. Finally, the Shapley value is applied to identify the explanations that are most crucial for model’s decisions.
size is not taken into account, the original image is assumed • We validate the stability of applying our framework in a
to be the best explanation, even if it does not make sense, fine-grained diagnostic task in breast ultrasound.
when evaluated by Insertion and Deletion. To solve the
II. RELATEDWORK
above problem, we add the explanation size as a weight in
InsertionandDeletionandproposeanewevaluationmetric, A. Segment Anything Model (SAM)
EffectScore.Intuitively,anexplanationisgiventoaperson, Inrecentyears,thedevelopmentofAIhasfocusedonlarge-
andanunderstandableexplanationisagoodone.Wecombine scale foundation models, resulting in a number of exceptional
the objective faithfulness evaluation of Effect Score and products [12], [17], [18]. These models, with billions or more
thehuman-centeredevaluationofunderstandabilitytoevaluate parameters, exhibit robust generalization and problem-solving
LCE and other popular explainability methods. The result potential after extensive training on large datasets, making
shows that LCE outperforms most popular methods in terms them versatile for various downstream tasks. Among them,
of faithfulness. In the evaluation of understandability, LCE is Segment Anything Model (SAM) [12] has caused a sensation
preferred by professional sonographers. with its powerful zero-shot segmentation performance. More
Finally, we explored the performance of LCE in the more importantly, SAM has learned general concepts about objects,
meaningful fine-grained diagnosis of breast ultrasound. The including those not seen during training.
results show that LCE consistently yields reasonable explana- However, the direct application of SAM to the ultrasound
tions regardless of fine-grained level. Notably, LCE is model- image segmentation task is not ideal due to the significant
agnostic and can provide explanations for any diagnostic differences between normal images and medical images [19].
model of medical images, and the whole framework requires Toaddressthisproblem,somestudieshaveproposedmethods
no additional cost, making it ideal for low-cost ultrasound toimproveSAMandmakeitapplicabletothefieldofmedical
images. image segmentation [20], [21]. Of these, MedSAM [21] has
The contributions of this paper are summarized as follows: fine-tuned SAM using a medical image dataset containing
• We present a framework for explainability that enables more than one million medical images with different diseases
understandableexplanationsofDNNstrainedonlow-cost andmodalities,anditsexcellentperformanceinmedicalimage
ultrasound images. segmentationhasbeenverifiedthroughextensiveexperiments.
• We integrate the attribution method with the concept- Similarly, SAMMed2D [20] used the largest medical image
basedmethod,synthesizingthestrengthsofbothmethods segmentation dataset to date and introduced Adapter [22] to
and improving the quality of explanations. fine-tuneSAM.Inthispaper,weuseexistingSAMfine-tuned
• We identify the shortcomings of popular faithfulness on medical images to explore the ability of SAM to discover
evaluation metrics and make adjustments accordingly. concepts in medical images without additional training costs.B. Attribution methods Algorithm 1 Lesion concept discovery
Input: Ultrasound Image I, Target Model M, Concept Dis-
Attribution methods primarily use techniques such as heat coverer D , Concept Guide G and G , Selection
SAM l g
maps [23] or saliency maps [24] to identify key regions that Vector q, Number of Concepts k.
provide visual cues for model classification. Class Activation Output: Set of Concepts C
o
Mapping (CAM) and its derivative methods [23], [25]can 1: C g ←∅, C l ←∅, C o ←∅
generate heat maps to show the contribution of each pixel 2: Map(x,y)←G g(I,M)
in an image to the classification. CAMs are often used to 3: Sort activation scores v i in Map in descending order to
explain medical image models, for example, [26] used Grad- obtain Param ={(v ,x ,y )|i=1,2,...,n}
g i i i
CAM [25] to analyze chest X-ray images and obtained clear 4: C g ←D SAM(I,Param g,q,k)
visualizations of pleural effusions, and [27] used Multi-Layer 5: S ←G l(I,M,k)
CAM to localize Confocal Laser Endomicroscopy Glioma 6: L b ←∅
images. 7: for each superpixel s i ∈S do
In addition to methods such as CAM, which obtains sig- 8: Get the bounding box p i =(x i1,y i1,x i2,y i2)
nificance scores from within the model, Local Interpretable 9: L b ←L b∪{p i}
Model-agnosticExplanations(LIME)[23]observeschangesin 10: end for
model output by simplifying complex models and perturbing 11: C l ←D SAM(I,L b,k)
input data for local superpixel explanations. [16] used a 12: C o ←C g∪C l
randomlysampledmasktoobtainpixelcontributions.[28]in- 13: return C o
troduced DeepLift to assign contribution scores by comparing
neuron activations. [29] proposed IntGrad, which combines
invariance and sensitivity. [30] incorporated Shapley value III. METHODOLOGY
[13] into LIME and DeepLift to form a unified framework
A. SAM-based concept discovery for lesions
for explanation, SHAP. [5] integrated perturbation operations
similartoLIMEandSHAPtoexplainimportantfeaturesinthe As shown in the first row of Fig. 1., a typical DNN black-
diagnosis of CT lung images. Similarly, [31] used LIME and box model f is processed through the softmax layer after
SHAP to explain a diagnostic model for renal abnormalities. receivinganimageI ∈Rw×h×c toobtainaprobabilityvector
However, the explanations given by these attribution methods of length k of the number of classification categories, where
are not always understood by medical image specialists. the largest probability corresponds to the classification result.
Such a decision-making process is entirely closed-off, and we
cannotdiscernwhattheinputimagehasundergonewithinthe
C. Concept-based methods black-box to arrive at the final classification result.
Now, we try to use SAM [12] for concept discovery in
In contrast to attribution methods, concept-based methods the black-box. As a zero-shot segmentation model, given an
prefer explanations that are consistent with real-world con- image I and either a set of points L of length n, a set
p
cepts. [7] proposed a framework to quantify the explainability of bounding boxes L of length n, or no prompt provided
b
of DNNs by evaluating the consistency between hidden units (in “everything” mode), SAM can output a set of n concept
and semantic concepts. [8] introduced the Test of Concept masks C = {c ,c ,...,c }, where each c may correspond
1 2 n i
Activation Vector (TCAV) to measure the sensitivity of the to certain concepts in real-world. In other words, SAM can
model to pre-defined concepts. [32] applied the TCAV to be seen as having the ability to discover concepts. Therefore,
the detection of cardiac disease in cine-MRI using clinically we redefine SAM as a concept discoverer D . Given the
SAM
known biomarkers as concepts. However, the pre-definition limited concept discovery capability of the original D on
SAM
of a concept set is challenging in the field of medical image medicalimages,theD usedinthispaperisSAMMed2D,
SAM
processing as it requires the intervention of experts. which is fine-tuned on a large medical image dataset (see
Some concept-based methods, which do not require hu- Section 4. B for comparison results between different training
man intervention, can automatically extract concepts from versions).
images. Prototype-based methods [33] can guide the network Concept discovery directly using D does not result
SAM
to use representative concepts for decision-making. [9] used in meaningful and high-quality concept blocks on ultrasound
prototypes to explain histological image classification. [10] images. This is because ultrasound images, as a low-cost im-
combined prototype learning with privileged information in age modality, contain relatively limited information compared
thoracic CT diagnosis, balancing explainability and accuracy. to normal, CT, or MRI images. In contrast, many objects
[11] proposed Explain Any Concept (EAC), which uses zero- in normal images have specific concepts that are easy to
shotsegmentationofSAMtodiscoverconcepts.Nevertheless, discover.Therefore,weconsidertheuseofattributionmethods
applying these methods to ultrasound images is challenging such as GradCAM [25] and LIME [34] as concept guides
due to their low-cost and limited information compared to CT to facilitate the discovery of concepts containing specific
and MRI. lesions information. Algorithm 1 describes the whole processof concept discovery, each concept is saved in the form of a
mask.
For pixel-level and superpixel-level attribution methods, we
employ different guiding approaches. (1) Concept guide G :
g
GradCAM[25]isapixel-levelattributionmethodthatutilizes
gradient information to obtain activation maps of the model
for a particular class outcome. Given an image I and a target
model M, GradCAM generates a heatmap Map(x ,y )=v ,
i i i
where v represents the attribution score obtained by Grad- Fig. 2: The Shapley value calculated when a black image is
i
CAM, and x , y are the coordinates of the corresponding diagnosed as benign with a 96% probability is not faithful.
i i
point. The triplets (v ,x ,y ) are sorted in descending order
i i i
based on the value of v to obtain the guiding parameters for
i
GradCAM, denoted as Param = {(v ,x ,y )}. Based on
IV. EXPERIMENTSANDANALYSIS
g i i i
the selection vector q, the corresponding triplets are obtained A. Experiment setting
from Param g in the specified percentages. These triplets are 1) Datasets: Breast Ultrasound Image (BUSI) [14] is a
then input into D SAM to obtain the concept set C g guided classification and segmentation dataset containing breast ul-
by GradCAM. (2) Concept guide G l: LIME is a perturbation- trasound images. For classification labeling, there are three
based attribution method where explanations are presented in categories,normal,benign,andmalignant,andthesamplesize
the form of superpixels. By using G l, the set of superpixels for each category is shown in Table I.
S can be obtained. For each superpixel s i in S, get the Fine-grained Ultrasound Images Of The Breast (FG-US-
bounding box and put it into L b. Through L b, the concept set B) is a private fine-grained classification dataset of breast
C l guidedbyLIMEcanbeobtainedusingD SAM.Finally,C l ultrasound images, and the classification labels are all based
is combined with C g to obtain the unprocessed C o. on the pathological results of the puncture procedure. The
B. Concepts post-processing and Shapley value calculation total number of images is 1016 and the sample size for each
category is shown in Table I.
TheconceptsmaskguidedbyG andG mayhaveobvious
g l The sample illustrations of the datasets used are shown in
overlaps, which can lead to unnecessary computational waste
Fig.3.TheFG-US-Bdatasetwascollectedfromtheultrasound
and explanation errors in the subsequent process. For each c
i department of the Affiliated Hospital of Southwest Jiaotong
∈ C , if the intersection between any two concepts is more
o University (Chengdu Third People’s Hospital).
than 90%, the concept with the smaller number of pixels
2) Implementation details: The main research aim of this
is discarded until all elements in C satisfy this condition.
o paper is a new framework for explainability rather than the
The explanation E, which contains the meaning of a lesion,
structureofaspecificmodel.Infact,ourframeworkismodel-
is a subset of C . As with EAC [11], we use the Shapley
o agnostic.WeuseResNet50[15]asthetargetmodel,wherethe
value[13],[30]forthefinalexplanation.Themodel’soriginal
images are resized to (224, 224) before being input into the
predictionfortheimagewillserveastheutilityfunctionu(T),
model. During training, we apply random horizontal flipping
where for concept c its marginal contribution to the model’s
i and calculate normalization coefficients separately for the
decision is defined as
two datasets. The optimization process uses decoupled weight
ϕ (I)=E(u(T ∪{c })−u(T)) (1) decay(AdamW)[35],withaninitiallearningrateof5×10−3,
ci i
gradually decreasing to 5 × 10−5 during fine-tuning. Each
Sincethenumberofconceptsisdeterminedbythehyperpa-
iteration uses a batch size of 8 samples. The implementation
rameters, there is no need to use Monte Carlo approximations
iscarriedoutusingthePyTorchframework(version2.1.2and
to calculate Shapley values [11]. In addition, avoiding the use
CUDA 12.3) [36], and all experiments are performed on a
of surrogate models [11] avoids a decrease in the faithfulness
GeForce RTX 3090 GPU. The classification accuracy of the
of the explanation. Finally, the optimal explanation is defined
target model on the test set after training is shown in Table I.
as
E =argmax E∈Cϕ E(I) (2) B. Evaluation metrics
When combining T, we observe that the concept of benign We evaluate the methods in terms of both understandability
tumors is intuitively very similar to the black pixel blocks and faithfulness. Faithfulness aims to respond to whether
usedformasking,asshowninFig.2.Blackimagehavea96% the explanation is the same as the model’s decision-making
probabilityofbeingdiagnosedasbenignwhenthemodeldoes process. Intuitively, a good explanation should be human-
not suffer from overfitting problems. Therefore, there is no understandable [37]. Based on the principle of human-in-the-
guarantee that the calculated Shapley Value faithfully reflects loop [38], we invite sonographers to perform an evaluation of
the marginal contribution of C when black masking is used. understandability, see Section 4.E.
i
For this reason, we decided to replace the black pixels with The Insertion and Deletion [16] are popular faithfulness
randomly selected pixels from a normal distribution whose metrics.TheInsertionmeasurestheimportanceofthesuper-
mean and variance matched those of the source dataset. pixels in constructing the whole image, while the DeletionTABLEI:Amountofsamplesinthedatasetsandclassification
accuracy of ResNet50 [15].
Dataset Class Amount Accuracy
Mastopathy(Ma) 78 30.76
Mastitis 126 53.65
Fibroadenoma(FN) 258 94.44
FG-US-B Intraductalpapilloma(IP) 133 67.44
Cyst 104 60.71
Ductalcarcinomainsitu(DCIS) 93 60.00
Invasiveductalcarcinoma(IDC) 224 80.00 (a) E 1 (EAC) (b) E 3 (EAC) (c) Insertion (EAC)
Normal 133 75.00
BUSI Benign 437 89.39
Malignant 210 85.71
(d) E (LIME) (e) E (LIME) (f)Insertion(LIME)
1 3
(a) Normal (b) Benign (c) Malignant (d) DCIS (e) Cyst
Fig. 4: The Insertion curve responds to the extent to which
the confidence of the model predictions correlates with the
original predictions as the percentage of explanation inserted
increases.TheEACgeneratessomemeaninglessexplanations,
but those explanations receive high evaluation results (first
(f) IP (g) IDC (h) Mastitis (i) MP (j) FN row). In contrast, LIME generates explanations that do not
match certain concepts, but are not as meaningless as EAC
Fig. 3: Examples of datasets.
(second row). E indicates that this is the i-th explanation ac-
i
cumulation image produced in the computation of Insertion.
quantifies the extent to which the probability decreases as
important superpixels are gradually removed from the image.
In the next experiments, in addition to Insertion and
We found that Insertion and Deletion could not reason-
Deletion, we use the Effect Score to evaluate the faith-
ablyevaluatethefaithfulnessofEAC[11]onbreastultrasound
fulness of methods.
images due to the inability to determine the number and
size of concepts discovered. Although EAC performs well on C. Comparison of SAMs trained on different data
Insertion (see the first row of Fig. 4), its evaluation is not
The concept discovery capability of SAM is crucial to our
reasonable.ThisisbecauseE (EAC)showsalmostthewhole
3 framework. We compare the faithfulness of different D ,
image, resulting in the best evaluation. In contrast, LIME SAM
including SAMs with different encoders [18] and SAMs
[34] ranks lower, but not so low as to be meaningless. We
fine-tuned on different medical image datasets (SAMMed2D
consider the size of the visual explanation to be a key factor
[20] and MedSAM [21]). To evaluate D reasonably, we
in evaluating the quality of the explanation. If the explanation SAM
exclude F and F . The experimental results are summarized
is too large, taking up the same proportion of the whole l g
in Table II.
image, then the explanation becomes meaningless. Evaluation
The original SAM (ViT-H version) has not been trained
on Deletion has similar results.
on a large amount of medical image data and has the best
For the above reasons, we modify these two faithfulness
Insertion and Deletion. As discussed in Section 4.B, this is
metrics.DefineP asthenumberofpixelsintheinputimage,
o due to the fact that its explanation is the image itself. After
and P denotes the average number of pixels in a set of
s re-evaluationusingtheEffectScore,thesehighlyunreliable
explanation accumulation images. The weighted evaluation
explanations are distinguished from the others. SAMMed2D
metrics Insertion and Deletion are defined as
w w is considered to be the best D due to its fine-tuning on
SAM
(cid:18) (cid:19)
Insertion = 1− P s ×Insertion (3) thelargestmedicalimagedatasettodate.Tomaintainthebest
w P performance, LCE uses SAMMed2D as D .
o SAM
(cid:18) (cid:19)
Deletion = 1− P s ×(1−Deletion) (4) D. Comparison of different explainability methods
w P
o In addition to the EAC, which is the most similar to the
Considering both Insertion and Deletion , the evalua- LCE, we select six commonly-used baseline methods for
w w
tion metric Effect Score is defined as comparison. These methods are all implemented by captum
[39]. We first generate concepts from the input images using
Insertion +Deletion
Effect Score= w w (5) superpixels [40] and then calculate the faithfulness of these
2TABLEII:ComparingthefaithfulnessofD ofdifferentencodersandD fine-tunedondifferentdata,↑and↓indicate
SAM SAM
that higher is better and lower is better, respectively.
BUSI FG-US-B
DSAM
Insertion↑ Deletion↓ EffectScore↑ Insertion↑ Deletion↓ EffectScore↑
SAM(ViT-B) 0.7540 0.5885 0.3293 0.4885 0.4817 0.1865
SAM(ViT-L) 0.7732 0.5803 0.2772 0.6473 0.3299 0.1793
SAM(ViT-H) 0.8675 0.5497 0.1768 0.6470 0.3123 0.1762
MedSAM[21] 0.5774 0.6000 0.3612 0.1214 0.1702 0.3247
SAMMed2d[20] 0.5954 0.5525 0.3785 0.1326 0.2035 0.3365
TABLE III: Comparing the faithfulness of different methods, ↑ and ↓ indicate that higher is better and lower is better,
respectively.
BUSI FG-US-B
Method
Insertion↑ Deletion↓ EffectScore↑ Insertion↑ Deletion↓ EffectScore↑
EAC[11] 0.8675 0.5497 0.1768 0.6470 0.3123 0.1762
LIME[34] 0.6205 0.4706 0.5073 0.1972 0.2344 0.3131
IntGrad[29] 0.5964 0.4779 0.3647 0.1849 0.2252 0.3118
GradSHAP[30] 0.6035 0.4771 0.3674 0.2144 0.2818 0.3029
DeepLIFT[28] 0.6510 0.5045 0.3733 0.2938 0.2309 0.3446
KernelSHAP[30] 0.4797 0.5763 0.2945 0.1778 0.2807 0.2915
GradCAM[25] 0.7175 0.4287 0.4172 0.1624 0.1936 0.3315
LCE 0.7585 0.4750 0.5572 0.4477 0.1889 0.4947
Input LCE EAC LIME IntGrad GradSHAP DeepLIFT KernalSHAP GradCAM
Fig. 5: Sample explanations generated by LCE, EAC [11] and six baseline methods [25], [28]–[30], [34].
concepts. The comparison results are shown in Table III. gives understandable explanations that are consistent with the
Among all the methods, LCE has the highest Effect Score. lesion concept. For example, in the third row, EAC generates
On the BUSI dataset, it has an Effect Score of 0.5572, anexplanationofverysmallsuper-pixels,andthisexplanation
which is 5% higher than the second-ranked LIME. Similarly, makes no sense. In contrast, other baseline methods generate
on the FG-US-B dataset, it is 15% higher than the second- explanations that are related to the tumor concept but are
ranked method. This suggests that LCE consistently produces less specific than LCE. While maintaining a certain level
more faithful breast ultrasound explanations than the baseline of faithfulness, after concept guidance, LCE can discover
method. meaningful concepts (e.g. the concept of a tumor) from low-
cost ultrasound images instead of presenting whole images or
We present several explanations generated by different meaningless almost completely black images.
methods in Fig. 5. Compared to other methods, LCE alwaysTABLE IV: Understandability is evaluated by five professional sonographers, and the table shows the average number of times
the explanation generated by each method is considered to be the best explanation.
DataSet LCE LIME IntGrad GradSHAP DeepLIFT KernelSHAP GradCAM
BUSI 64 30 54 56 45 28 45
FG-US-B 52 40 34 32 30 29 38
TABLE V: Explanations generated by LCE for models trained on data at different fine-grained levels.
Originalimage
and
explanations
Numberof
- 2 3 4 5 6 7
categories
Successful
- Yes Yes No Yes Yes Yes
diagnosis
EffectScore - 0.4429 0.4816 0.4539 0.5013 0.4926 0.4823
E. Understandability evaluation from sonographers grainedlevels.Whenthemodel’sdiagnosisisincorrect,wecan
alsofindthatitsexplanationbecomesambiguous,reflectingto
Feature maps can also be used as explanations, but such
some extent the reason for the incorrect diagnosis (e.g. failure
explanations are not valid because humans cannot under-
to correctly identify the lesion).
standthem.Manymethodsevaluateunderstandabilitythrough
human-centered experiments [11], [34], [41]. In this paper,
V. CONCLUSION
we select samples of benign, fibroadenoma, and mastitis for
understandability evaluation. In this paper, we integrate the attribution method with
We invite five experienced sonographers to evaluate the concept-based method by proposing the Lesion Concept Ex-
baselinemethodsandLCE.Weprovideeachsonographerwith plainer (LCE) framework, which uses SAM fine-tuned on a
the raw image, the model prediction, the ground truth, and large number of medical images for lesion concept discovery,
seven explanations for each image. The order in which each toexplainthedecisionsofDNNs.Toaddresstheshortcomings
explanation is presented is randomized, so the sonographers of the popular faithfulness evaluation metrics Insertion and
are unaware of which method each explanation comes from. Deletion,wemodifythembyusingthesizeoftheexplanation
They are required to rank the regions included in the seven as a weight and propose a new metric, Effect Score. We
explanations,withtheexplanationsthatcontributedmosttothe implement LCE on ResNet50 models trained on two breast
diagnostic results and are easier to understand being ranked ultrasound datasets (public and private). Compared to other
highest. The evaluation is completed when the sonographers popular methods, LCE is more effective in explaining breast
are fully rested. The average evaluation of the five sonog- ultrasound DNNs and performs best in the evaluation of
raphers is shown in Table IV. Based on the sonographers’ objective faithfulness and human-centered understandability.
ranking results, LCE clearly outperforms the other baseline Finally,weexperimentwithLCEatdifferentfine-grainedlev-
methods. The majority of the explanations provided by the els and show that it always faithfully provides understandable
LCE are accepted by the sonographers. In particular, this explanations.Moreimportantly,wedemonstratethatLCEcan
shows that LCE contributes to the realization of AI that can contributetothedevelopmentofmedicalAIbymakingDNNs
be trusted by medical professionals. in the medical domain more trustworthy.
F. Explanations at different diagnostic fine-grained levels REFERENCES
Fine-grainedclassificationsarethosewithsubtledifferences
[1] X.Chen,X.Wang,K.Zhang,K.-M.Fung,T.C.Thai,K.Moore,R.S.
within categories, and existing studies tend to classify breast Mannel, H. Liu, B. Zheng, and Y. Qiu, “Recent advances and clinical
tumors as benign or malignant only on a coarse-grained applicationsofdeeplearninginmedicalimageanalysis,”MedicalImage
Analysis,vol.79,p.102444,2022.
basis [42]. However, coarse-grained classifications serve a
[2] M. E. Kaminski and J. M. Urban, “The right to contest ai,” Columbia
weak auxiliary role for sonographers, who can easily derive a LawReview,vol.121,no.7,pp.1957–2048,2021.
diagnosis from several guidelines (e.g., TI-RADS [43]). [3] M. Kop, “Eu artificial intelligence act: the european approach to ai.”
Stanford-Vienna Transatlantic Technology Law Forum, Transatlantic
To test the performance of LCE on more significant fine-
Antitrust...,2021.
grained diagnosis, we create subsets of six different fine- [4] G. Zhao, B. Zhou, K. Wang, R. Jiang, and M. Xu, “Respond-cam:
grained levels by changing the number of categories using Analyzing deep models for 3d imaging data by visualizations,” in
MedicalImageComputingandComputerAssistedIntervention–MICCAI
the FG-US-B dataset, and Table V shows the differences in
2018:21stInternationalConference,Granada,Spain,September16-20,
the explanations generated by LCE as the change of fine- 2018,Proceedings,PartI. Springer,2018,pp.485–492.[5] P. Zhu and M. Ogino, “Guideline-based additive explanation for gradient-based localization,” in Proceedings of the IEEE international
computer-aided diagnosis of lung nodules,” in Interpretability of Ma- conferenceoncomputervision,2017,pp.618–626.
chineIntelligenceinMedicalImageComputingandMultimodalLearn- [26] J.Irvin,P.Rajpurkar,M.Ko,Y.Yu,S.Ciurea-Ilcus,C.Chute,H.Mark-
ing for Clinical Decision Support: Second International Workshop, lund, B. Haghgoo, R. Ball, K. Shpanskaya et al., “Chexpert: A large
iMIMIC2019,and9thInternationalWorkshop,ML-CDS2019,Heldin chestradiographdatasetwithuncertaintylabelsandexpertcomparison,”
Conjunction with MICCAI 2019, Shenzhen, China, October 17, 2019, inProceedingsoftheAAAIconferenceonartificialintelligence,vol.33,
Proceedings9. Springer,2019,pp.39–47. no.01,2019,pp.590–597.
[6] X. Li, Y. Zhou, N. C. Dvornek, Y. Gu, P. Ventola, and J. S. Duncan, [27] M. Izadyyazdanabadi, E. Belykh, C. Cavallo, X. Zhao, S. Gandhi,
“Efficientshapleyexplanationforfeaturesimportanceestimationunder L. B. Moreira, J. Eschbacher, P. Nakaji, M. C. Preul, and Y. Yang,
uncertainty,” in Medical Image Computing and Computer Assisted “Weakly-supervised learning-based feature localization for confocal
Intervention–MICCAI2020:23rdInternationalConference,Lima,Peru, laser endomicroscopy glioma images,” in Medical Image Computing
October4–8,2020,Proceedings,PartI23. Springer,2020,pp.792– and Computer Assisted Intervention–MICCAI 2018: 21st International
801. Conference,Granada,Spain,September16-20,2018,Proceedings,Part
[7] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, “Network II11. Springer,2018,pp.300–308.
dissection: Quantifying interpretability of deep visual representations,” [28] A. Shrikumar, P. Greenside, and A. Kundaje, “Learning important
inProceedingsoftheIEEEconferenceoncomputervisionandpattern features through propagating activation differences,” in International
recognition,2017,pp.6541–6549. conferenceonmachinelearning. PMLR,2017,pp.3145–3153.
[8] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas [29] M.Sundararajan,A.Taly,andQ.Yan,“Axiomaticattributionfordeep
et al., “Interpretability beyond feature attribution: Quantitative testing networks,” in International conference on machine learning. PMLR,
with concept activation vectors (tcav),” in International conference on 2017,pp.3319–3328.
machinelearning. PMLR,2018,pp.2668–2677. [30] S.M.LundbergandS.-I.Lee,“Aunifiedapproachtointerpretingmodel
[9] K. Uehara, M. Murakawa, H. Nosato, and H. Sakanashi, “Prototype- predictions,”Advancesinneuralinformationprocessingsystems,vol.30,
basedinterpretationofpathologicalimageanalysisbyconvolutionalneu- 2017.
ralnetworks,”inAsianConferenceonPatternRecognition. Springer, [31] M.Bhandari,P.Yogarajah,M.S.Kavitha,andJ.Condell,“Exploringthe
2019,pp.640–652. capabilities of a lightweight cnn model in accurately identifying renal
[10] L.Galle´e,M.Beer,andM.Go¨tz,“Interpretablemedicalimageclassifi- abnormalities:Cysts,stones,andtumors,usinglimeandshap,”Applied
cationusingprototypelearningandprivilegedinformation,”inInterna- Sciences,vol.13,no.5,p.3125,2023.
tionalConferenceonMedicalImageComputingandComputer-Assisted [32] J. R. Clough, I. Oksuz, E. Puyol-Anto´n, B. Ruijsink, A. P. King, and
Intervention. Springer,2023,pp.435–445. J.A.Schnabel,“Globalandlocalinterpretabilityforcardiacmriclassi-
[11] A.Sun,P.Ma,Y.Yuan,andS.Wang,“Explainanyconcept:Segment fication,”inInternationalConferenceonMedicalImageComputingand
anything meets concept-based explanation,” Advances in Neural Infor- Computer-AssistedIntervention. Springer,2019,pp.656–664.
mationProcessingSystems,vol.36,2024. [33] C.Chen,O.Li,D.Tao,A.Barnett,C.Rudin,andJ.K.Su,“Thislooks
[12] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, like that: deep learning for interpretable image recognition,” Advances
T.Xiao,S.Whitehead,A.C.Berg,W.-Y.Loetal.,“Segmentanything,” inneuralinformationprocessingsystems,vol.32,2019.
inProceedingsoftheIEEE/CVFInternationalConferenceonComputer [34] M.T.Ribeiro,S.Singh,andC.Guestrin,“”whyshoulditrustyou?”
Vision,2023,pp.4015–4026. explainingthepredictionsofanyclassifier,”inProceedingsofthe22nd
[13] L.S.Shapleyetal.,“Avalueforn-persongames,”1953. ACM SIGKDD international conference on knowledge discovery and
[14] W. Al-Dhabyani, M. Gomaa, H. Khaled, and A. Fahmy, “Dataset of datamining,2016,pp.1135–1144.
breastultrasoundimages,”Datainbrief,vol.28,p.104863,2020. [35] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
[15] K.He,X.Zhang,S.Ren,andJ.Sun,“Deepresiduallearningforimage arXivpreprintarXiv:1711.05101,2017.
recognition,”inProceedingsoftheIEEEconferenceoncomputervision [36] A.Paszke,S.Gross,S.Chintala,G.Chanan,E.Yang,Z.DeVito,Z.Lin,
andpatternrecognition,2016,pp.770–778. A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in
[16] V.Petsiuk,A.Das,andK.Saenko,“Rise:Randomizedinputsampling pytorch,”2017.
forexplanationofblack-boxmodels,”arXivpreprintarXiv:1806.07421, [37] K.Rasheed,A.Qayyum,M.Ghaly,A.Al-Fuqaha,A.Razi,andJ.Qadir,
2018. “Explainable,trustworthy,andethicalmachinelearningforhealthcare:
[17] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von A survey,” Computers in Biology and Medicine, vol. 149, p. 106043,
Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., 2022.
“On the opportunities and risks of foundation models,” arXiv preprint [38] I.Lage,A.Ross,S.J.Gershman,B.Kim,andF.Doshi-Velez,“Human-
arXiv:2108.07258,2021. in-the-loop interpretability prior,” Advances in neural information pro-
[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, cessingsystems,vol.31,2018.
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gellyetal., [39] N. Kokhlikyan, V. Miglani, M. Martin, E. Wang, B. Alsallakh,
“An image is worth 16x16 words: Transformers for image recognition J. Reynolds, A. Melnikov, N. Kliushkina, C. Araya, S. Yan, and
atscale,”arXivpreprintarXiv:2010.11929,2020. O. Reblitz-Richardson, “Captum: A unified and generic model inter-
[19] M.A.Mazurowski,H.Dong,H.Gu,J.Yang,N.Konz,andY.Zhang, pretabilitylibraryforpytorch,”2020.
“Segmentanythingmodelformedicalimageanalysis:anexperimental [40] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Su¨sstrunk,
study,”MedicalImageAnalysis,vol.89,p.102918,2023. “Slicsuperpixelscomparedtostate-of-the-artsuperpixelmethods,”IEEE
[20] J. Cheng, J. Ye, Z. Deng, J. Chen, T. Li, H. Wang, Y. Su, Z. Huang, transactions on pattern analysis and machine intelligence, vol. 34,
J.Chen,L.Jiangetal.,“Sam-med2d,”arXivpreprintarXiv:2308.16184, no.11,pp.2274–2282,2012.
2023. [41] J.Colin,T.Fel,R.Cade`ne,andT.Serre,“Whaticannotpredict,idonot
[21] J.Ma,Y.He,F.Li,L.Han,C.You,andB.Wang,“Segmentanythingin understand: Ahuman-centered evaluation frameworkfor explainability
medicalimages,”NatureCommunications,vol.15,no.1,p.654,2024. methods,”Advancesinneuralinformationprocessingsystems,vol.35,
[22] N.Houlsby,A.Giurgiu,S.Jastrzebski,B.Morrone,Q.DeLaroussilhe, pp.2832–2845,2022.
A.Gesmundo,M.Attariyan,andS.Gelly,“Parameter-efficienttransfer [42] W.-C.ShiaandD.-R.Chen,“Classificationofmalignanttumorsinbreast
learning for nlp,” in International conference on machine learning. ultrasoundusingapretraineddeepresidualnetworkmodelandsupport
PMLR,2019,pp.2790–2799. vectormachine,”ComputerizedMedicalImagingandGraphics,vol.87,
[23] B.Zhou,A.Khosla,A.Lapedriza,A.Oliva,andA.Torralba,“Learning p.101829,2021.
deepfeaturesfordiscriminativelocalization,”inProceedingsoftheIEEE [43] F.N.Tessler,W.D.Middleton,E.G.Grant,J.K.Hoang,L.L.Berland,
conferenceoncomputervisionandpatternrecognition,2016,pp.2921– S. A. Teefey, J. J. Cronan, M. D. Beland, T. S. Desser, M. C. Frates
2929. etal.,“Acrthyroidimaging,reportinganddatasystem(ti-rads):white
[24] K.Simonyan,A.Vedaldi,andA.Zisserman,“Deepinsideconvolutional
paperoftheacrti-radscommittee,”JournaloftheAmericancollegeof
networks: Visualising image classification models and saliency maps,”
radiology,vol.14,no.5,pp.587–595,2017.
arXivpreprintarXiv:1312.6034,2013.
[25] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via