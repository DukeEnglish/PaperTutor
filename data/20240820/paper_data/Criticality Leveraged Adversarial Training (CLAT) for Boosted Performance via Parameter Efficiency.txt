Criticality Leveraged Adversarial Training (CLAT) for
Boosted Performance via Parameter Efficiency
BhavnaGopal1,HuanruiYang2,JingyangZhang1,MarkHorton1,YiranChen1
1DepartmentofElectricalandComputerEngineering,DukeUniversity
2DepartmentofElectricalEngineeringandComputerScience,UniversityofCalifornia,Berkeley
1{bhavna.gopal,jingyang.zhang,mark.horton,yiran.chen}@duke.edu,2huanrui@berkeley.edu
Abstract
Adversarialtrainingenhancesneuralnetworkrobustnessbutsuffersfromatendency
tooverfitandincreasedgeneralizationerrorsoncleandata. Thisworkintroduces
CLAT,aninnovativeapproachthatmitigatesadversarialoverfittingbyintroducing
parameterefficiencyintotheadversarialtrainingprocess,improvingbothclean
accuracyandadversarialrobustness. Insteadoftuningtheentiremodel, CLAT
identifiesandfine-tunesrobustness-criticallayers—thosepredominantlylearning
non-robustfeatures—whilefreezingtheremainingmodeltoenhancerobustness.
Itemploysdynamiccriticallayerselectiontoadapttochangesinlayercriticality
throughoutthefine-tuningprocess. Empirically,CLATcanbeappliedontopof
existingadversarialtrainingmethods,significantlyreducesthenumberoftrainable
parametersbyapproximately95%,andachievesmorethana2%improvementin
adversarialrobustnesscomparedtobaselinemethods.
1 Introduction
Advancements in deep learning models have markedly improved image classification accuracy.
Despitethis,theirvulnerabilitytoadversarialattacks—subtlemodificationstoinputimagesthat
mislead the model — remains a significant concern [9, 28]. The research community has been
rigorouslyexploringtheoriestocomprehendthemechanicsbehindadversarialattacks[4]. Ilyasetal.
[14] uncover the coexistence of robust and non-robust features in standard datasets. Adversarial
vulnerabilitylargelystemsfromthepresenceofnon-robustfeaturesinmodelstrainedonstandard
datasets,which,whilehighlypredictiveandbeneficialforcleanaccuracy,aresusceptibletonoise[28].
Unfortunately,itisobservedthatdeeplearningmodelstendtopreferentiallylearnthesenon-robust
layers. Inkawhichetal.[15,16]furtherdemonstratethatadversarialimagesderivedfromthehidden
featuresofcertainintermediatenon-robust/“critical”layersexhibitenhancedtransferabilitytounseen
models. Thissuggestsacommonalityinthenon-robustfeaturescapturedbytheselayers. While
identifyingthesecriticallayerstoimprovetheirrobustnessisappealing,thisprocessoftenrequires
thecostlygenerationofattacksagainsteachindividualnetworklayer. Efficientmethodstoidentify
andeffectivelyaddressthecriticalityofsuchlayersarestilllacking.
Incontrasttolayer-wisefeaturevulnerabilityanalysis,adversarialtraining[3,19,8],involvestraining
entireneuralnetworkswithadversarialexamplesgeneratedinreal-time. Thisapproachinherently
encouragesalllayersinthemodeltolearnrobustfeaturesfromadversarialimages,therebyenhancing
themodel’sresilienceagainstattacks. However,giventhemorechallengingoptimizationprocessof
learningfromadversarialexamplesthanfromcleanexamples,adversarialtrainingalsobringshurdles
suchasheightenederrorsoncleandataandsusceptibilitytooverfitting,ultimatelydiminishingits
effectivenessinpracticalapplications[25,36,23,17]. Despitevariouseffortstoenhanceadversarial
training,suchasmodifyinginputdataandadjustinglossfunctions[12,23,36,30,32,21],theystill
frequentlyfallshortinalleviatingtheaforementionedchallenges.
Preprint.Underreview.
4202
guA
91
]GL.sc[
1v40201.8042:viXraIn light of these challenges, we introduce Frozen
CLAT, a paradigm shift in adversarial train- non-critical layers
ing, where we mitigate overfitting during Clean
input
adversarial training by identifying and tun-
Adversarial
ing only the robustness-critical model layers. input
CLATcommencesbypinpointingcriticallayers Critical layers finetuning with
within a model using our novel, theoretically Continue training criticality-aware loss Recompute critical
with new critical layers layers every 10 epochs
grounded,andeasilycomputable,“criticalityin-
dex”, which we developed to identify layers Clean
input
which have learned non-robust features dom- Adversarial
input
inantly. Subsequently, our algorithm meticu-
louslyfine-tunesthesecriticallayerstoremove Recomputed critical
theirnon-robustfeaturesandreducetheircriti- layers
cality,whilefreezingtheother,non-criticallay- Figure1: CLAToverview. CLATfine-tunesthe
ers. Dynamicselectionofcriticallayersiscon- selectedcriticallayers(red)whilefreezingother
ductedduringthetrainingprocesstoalwaysfo- layers (grey). fine-tuning objective is computed
custhefine-tuningonthemost-in-needlayers, perEquation(9). Criticallayersareadjustedperi-
avoidingtheoverfittingoffull-modeladversarial odically. PseudocodeisprovidedinAppendixA.
training. CLATthereforeenhancesbothclean
and adversarial accuracy compared to previous adversarial training methods. Besides reducing
overfitting,thecriticallayerselectionintrainingalsomakesCLATastreamlined,parameter-efficient
fine-tuningmethod,enablingmemorysavingswithitssignificantlyreducedtrainableparameters.
Insummary,wemakethefollowingcontributionsinthispaper:
• Weintroducethe“criticalityindex”,aquantitativemetricdesignedtoefficientlyidentify
criticallayersfortheadversarialvulnerabilityofthemodel.
• Wedevelopaspecializedadversarialtrainingobjectivefocusedonreducingthecriticalityof
theidentifiedcriticallayerstobolsteroverallmodelrobustness.
• WeproposeCLAT,aparameter-efficientadversarialfine-tuningalgorithmfocusingonlayer
criticalityreduction. CLATmitigatesoverfittingintheadversarialtrainingprocess, and
appliestovariousmodeltrainingscenariosandbaselineadversarialtrainingmethods.
CLATstandsoutbymarkedlyreducingoverfittingrisks, maintainingorevenslightlyimproving
thecleanaccuracywhilesignificantlyenhancingadversarialresiliencebyatleast2%witha96%
reductionintrainableparameters.
2 RelatedWork
2.1 Adversarialtraining
Adversarial Training (AT) was first introduced by Goodfellow et al. [9], who demonstrated how
theintegrationofadversarialexamplesintothetrainingprocesscouldsubstantiallyimprovemodel
robustness. ThisideaevolvedintoasophisticatedminimaxoptimizationapproachwithProjected
Gradient Descent Adversarial Training (PGD-AT) [19], which employs PGD attacks in training.
RegardedasthegoldstandardinAT,PGD-ATgeneratesadversarialtrainingsamplesusingmultiple
stepsofprojectedgradientdescent,leadingtosubstantiallyimprovedempiricalrobustness[6,3,8].
Furtherrefiningthisapproach,TRADES[36]optimizesanovellossfunctiontobalanceclassification
accuracy with adversarial robustness. Recent enhancements in AT, including model ensembling
and data augmentation, have also produced notable improvements in model resilience. [33, 34,
7]. Inkawhich et al. [15] propose “Activation Attacks” (AA) which underscore the efficacy of
leveraging intermediate model layers for generating stronger adversarial attacks, suggesting that
incorporatingAAinadversarialtrainingcouldfortifydefenses. Theirfindingsprovideafoundation
forourmethodwhichintegratestheseintermediatecriticallayersintoouradversarialtrainingstrategy.
2.2 Efficientadversarialtraining
AdversarialtrainingmethodslikePGD-ATandTRADESarecomputationallyintensive,necessitating
the multi-step generation of adversarial examples, optimization of complex objective functions,
2andcomprehensivemodeltuning[26]. Numerouseffortstoenhancetheefficiencyofadversarial
traininghavebeenproposed.Shafahietal.[26]introducedanefficientmodelknownas“Free”AT,
usingasinglebackpropagationstepforbothtrainingandPGDadversarygenerationtoaccelerate
adversarial training. However, this approach struggled with maintaining model robustness and
increased overfitting, primarily due to gradient alignment issues. Wong et al. [31] then proposed
“Fast” AT, which aimed to address these shortcomings, but was later found to be vulnerable to
similarproblems[2]. Asaresult,AndriushchenkoandFlammarion[2]developedGradAlignbutthis
approachtripledthetrainingtimeduetosecond-ordergradientcomputation. Lastly,RiFTleverages
networklayerredundanciestoimprovegeneralperformance,butitsheuristic-basedlayerredundancy
measurementlimitsfinalrobustness[37]. Incontrast,CLATusesatheoreticallygrounded,dynamic,
criticallayerselectionmechanism, resultinginamorerobustandparameter-efficientadversarial
trainingalgorithm. Furthermore,CLATdoesnotrequireafullytrainedmodelandcanbecombined
withexistingfast-ATmethodstoenhanceperformanceandmitigateoverfitting.
3 Methods
Buildingonpriorattackanddefenseresearch[15,16,37]whichdemonstratesthatnotallmodel
layersequallylearnnon-robustfeatures,weaimtoimprovemodelrobustnessandparameterefficiency
byidentifyingandfine-tuningonlythosecriticallayersthatarepronetolearningnon-robustfeatures,
whilekeepingthenon-criticallayersfrozen. Inthissection,webeginbydefiningandidentifying
critical layers, then outline our objectives for reducing their criticality. Finally, we present our
completeCLATalgorithm,whicheffectivelyenhancesmodelrobustness.
3.1 LayerCriticality
Consideradeeplearningmodelwithnlayers,andaninputx,definedas:
F(x)=f (f (...f (x))), (1)
n n−1 1
wherethefunctionalityofthei-thlayerisdenotedasf . Duringthestandardtrainingprocess,all
i
layers learn useful features which contribute to the correct outputs of the model. We denote the
hiddenfeaturelearnedattheoutputofthei-thlayerasF (x)=f (f (...f (x))).
i i i−1 1
Underadversarialperturbation,featuresfromalllayerswillbealtered,leadingtoincorrectoutputs.
Tounderstandhowthelearnedparametersofthemodelaffectthefeaturesunderperturbation,we
considerafunctionG (·)definedattheoutputoflayerioninputsclosetoacleandatapointx:
i
G (z)=||F (z)−F (x)||2. (2)
i i i 2
Clearly,z =xisaminimaofG (z).Theworst-casecurvatureofthefunctionG attheneighborhood
i i
ofz =xcanbeestimatedfollowingtheformulationbyMoosavi-Dezfoolietal.[20]as
∇G (x′)−∇G (x) ∇G (x′)
ν (x)= i i = i , (3)
i ||x′−x|| ||x′−x||
2 2
wherex′isaworst-caseperturbation(adversarialattack)maximizingG (z)inthevicinityofx,and
i
∇G (x)=0bydefinitiongivenitisaminimum.
i
Thenumeratorinthecurvatureformulationcanbefurtherderivedas
∂Fi(x′)T
(F (x′)−F (x))
ν (x)= ∂x′ i i . (4)
i ||x′−x||
2
Followingtheobservationin[20],ahighercurvatureindicatesthefeaturetobemoresensitiveto
adversarialexamples.Wecanthereforeusethecurvatureformulationν (x)underafixedperturbation
i
budget||x′−x|| ≤ϵtoestimatethelayersensitivity,orweakness.
p
Inpractice,itisdifficulttoexplicitlyinstantiate
∂Fi(x′)
foraneuralnetwork. Tothisend,wesimplify
∂x′
theformulationinEquation(4)byassuming
∂Fi(x′)
asauniformvector. Thisleadstoourdefinition
∂x′
oftheϵ-weaknessoflayeri’sfeatureas:
(cid:34) (cid:35)
1
W (F )= E sup ||F (x+δ)−F (x)|| , (5)
ϵ i N x i i 2
i ||δ||p≤ϵ
3whereN denotesthedimensionalityoftheoutputfeaturesatlayeri,thereforenormalizingtheweak-
i
nessmeasurementoflayerswithdifferentoutputsizes. Theweaknessmeasurementisproportional
tothecurvatureestimationinEquation(4). Ahigherweaknessvalueindicatesthatthefeaturevector
ismorevulnerabletoinputperturbations. Thefunctionalityofcascadinglayersfrom1toiaffectsthe
vulnerabilityofthehiddenfeatures,asdescribedbythisformulation.
Forthepurposeofefficientfine-tuning,wewanttoidentifythelayersthatarethemostcriticaltothe
lackofrobustness,characterizedbytheirincreasedsusceptibilitytoperturbationsfromadversarial
inputs. Wethereforeprovidethefollowingdefinition:
Definition3.1. Criticallayer: Alayerisconsideredcriticalifitexhibitsagreaterpropensityto
learnnon-robustfeaturesordemonstratesheightenedsensitivitytoadversarialinputperturbations
relativetootherlayersinthemodel.
Tothisend,wesingleoutthecontributionofeachlayer’sfunctionalitytotheweaknessofthefeatures
afteritwithaLayerCriticalityIndexC ,whichisformulatedas
fi
W (F )
C = ϵ i . (6)
fi W (F )
ϵ i−1
Forthefirstlayer,wedefineC =W (F )asonlythefirstlayercontributestotheweakness.
f1 ϵ 1
Asasanitycheck,thefeatureweaknessattheoutputoflayericanbeattributedtothecriticalityof
allpreviouslayersasW (F ) =
(cid:81)i
C . Conversely,alayerwithalargercriticalityindexwill
ϵ i k=1 fk
increasetheweaknessofthefeaturesafterit,indicatingthelayeriscriticalaccordingtoDefinition3.1,
asitheightensthefeaturesensitivitytotheadversarialinput.
OnedrawbackoftheformulationinEquation(6)isthatcomputingthefeatureweaknessinvolves
findingtheworst-caseperturbationagainstthehiddenfeaturesateachlayer,whichisacostlyprocess
toconductsequentiallyforalllayers. Inpractice,wefinditpossibletoapproximatetheworst-case
perturbationagainstfeatureswithanuntargetedPGDattackagainstthemodeloutput,sothatwecan
usethesamePGDperturbationδtoestimatethefeatureweaknessofalllayersfollowingEquation(5).
Inthisway,withareasonablysufficientbatchsize,wecancomputethecriticalindicesforalllayers
inamodelwithtwoforwardpasses: onewiththecleaninputxandonewiththePGDattackinput
x+δ. Wemakethefollowingproposition:
Proposition3.2. CriticallayersdefinedasinDefinition3.1canbeidentifiedasthelayerswiththe
largestcriticalityindicesargmax C .
i fi
To verify Proposition 3.2, we conduct an ablation study in Table 5, where we show that model
robustness is improved more byCLATfine-tuning of criticallayers compared toequivalent fine-
tuningofrandomlyselectedlayers. Wewilldiscusshowtoreducethecriticalityofthecriticallayers
andmakethemmorerobustinthenextsubsection.
3.2 Criticality-targetedfine-tuning
Oncethecriticallayersareidentified,wefine-tunethemtoreducetheircriticality,therebydecreasing
theweaknessofsubsequenthiddenfeaturesandenhancingmodelrobustness. Foracriticallayer
i, we optimize the trainable parameters to minimize C . Note that in the criticality formulation
fi
inEquation(6),theweaknessofthepreviouslayer’soutput,W (F ),isconstantwithrespecttof .
ϵ i−1 i
Thus,theoptimizationobjectiveforf canbesimplifiedas
i
(cid:34) (cid:35)
L (f )=E sup ||F (x+δ)−F (x)|| . (7)
C i x i i 2
||δ||p≤ϵ
Inthecasewheremultiplecriticallayersareconsideredinthefine-tuningprocess,thefine-tuning
objectivecanbeexpandedtoaccommodateallcriticallayerssimultaneously. Formally,supposewe
haveasetS wherelayersi ∈ S areallselectedforfine-tuning,thefine-tuningobjectiveforthese
criticallayerscanbeformulatedas
(cid:34) (cid:35)
(cid:88)
L (f )=E sup ||F (x+δ)−F (x)|| , (8)
C S x i i 2
||δ||p≤ϵ
i∈S
4whereasingleperturbationisutilizedtocapturetheweaknessacrossallcriticallayers. Inpractice,a
projectedgradientascentoptimizationwithrandomstartisusedtosolvetheinnermaximization.
MinimizingtheobjectiveinEquation(8)byadjustingthetrainablevariablesofthecriticallayerswill
reducetheirfeatureweaknesses. However,theremovalofnon-robustfeaturesintheselayersmay
affectthefunctionalityofthemodeloncleaninputs. Asatradeoff,wealsoincludethecrossentropy
lossL(·)inthefinaloptimizationobjective,whichderivestheoptimizationobjectiveonthecritical
layersduringthefine-tuningprocess
minE L(F(x),y)+λL (f ), (9)
x,y C S
fS
wherethehyperparameterλservesasabalancingfactorbetweenthetwolossterms. Notethatonly
theselectedcriticallayersf areoptimizedinEquation(9)whiletheothernon-criticallayersare
S
frozen,justifyingourfine-tuningtobeparameter-efficient.
3.3 CLATadversarialtraining
Similartoparameter-efficientfine-tuningtechniquesinotherdomains,wedesignCLATasafine-
tuningapproach,whichisappliedtoneuralnetworksthathaveundergonepreliminarytraining. The
pretrainingphaseallowsalllayersinthemodeltocaptureusefulfeatures,whichwillfacilitatethe
identificationofcriticallayersinthemodel. Notably,CLAT’sversatilityallowsittoadapttovarious
typesofpretrainedmodels,eitheradversariallytrainedortrainedonacleandatasetonly. Inpractice,
wefindthatmodelsdonotneedtofullyconvergeduringthepretrainingphasetobenefitfromCLAT
fine-tuning. Forexample,incaseoftheCIFAR-10dataset,50epochsofPGD-ATtrainingwouldbe
adequate. Weconsiderthenumberofadversarialpretrainingepochsasahyperparameterandprovide
furtheranalysisontheimpactofpretrainingepochsinSection4.3.1.
After the pretraining, CLAT begins by identifying and selecting critical layers in the pretrained
model. Wethenfine-tunecriticallayersonlywhilefreezingtherestofthelayers. Thisprocessis
illustratedinFigure1,andthepseudocodeisprovidedinAlgorithm1inAppendixAforgreater
clarity. Comparedtofull-modelfine-tuning,CLATnotonlyimprovesparameterefficiencywithfewer
trainableparameters,butalsoreducesoverfitting,acommonissueinstandardadversarialtraining. As
fine-tuningprogresses,thecriticallayerswillbeupdatedtoreducetheircriticality,makingthemless
criticalthansomeofthepreviouslyfrozenlayers. Subsequently,weperformperiodicreevaluationof
thetopkcriticalindices,ensuringcontinuousadaptationandoptimizationofthelayersthatarethe
mostinneedinthetrainingprocess. Throughhyperparameteroptimization,wefind10epochstobe
adequatetooptimizetheselectedcriticallayersforallmodelsthatwetested.
4 Experiments
4.1 Experimentalsettings
Datasetsandmodels Weconductedexperimentsusingtwowidelyrecognizedimageclassification
datasets,CIFAR10andCIFAR100. Eachdatasetincludes60,000colorimages,each32×32pixels,
dividedinto10and100classesrespectively[18].Forourexperiments,wedeployedasuiteofnetwork
architectures: WideResnets(34-10,70-16)[35],ResNets(50,18)[11],DenseNet-121[13],PreAct
ResNet-18[10],andVGG-19[27]. Inthispaper,thesearchitecturesarereferredtoasWRN34-10,
WRN70-16,RN50,RN18,DN121,PreactRN18andVGG19respectively.
Trainingandevaluation SinceCLATcanbelayeredovercleanpretraining,partialtraining,or
other adversarial methods, results incorporating CLAT are denoted in our tables as "X+ CLAT,"
where“X”specifiesthebaselinemethodusedpriortoapplyingCLAT.Typically,thisbaselinemethod
wasappliedforthefirst50epochs,followedbyafine-tuningphaseduringwhichCLATwasrunfor
anadditional50epochs. ModelstrainedexclusivelywithPGD-ATtypicallyrequire150epochsto
achievereportedperformances. WeusePGDforattackgenerationunlessstatedotherwiseinthe
baselinemethod,whichemploysarandomstart[19],withanattackbudgetofϵ=0.03undertheℓ
∞
norm,anupdatestepsizeofα=0.007,and10attacksteps. ExperimentswereconductedonaTitan
XPGPU,startingwithaninitiallearningrateof0.1,whichwasadjustedaccordingtoacosinedecay
schedule. Toensurethereliabilityofrobustnessmeasurements,weconductedeachexperimenta
minimumof10times,reportingthelowestadversarialaccuraciesweobserved.
5CLATsettings WeselectcriticallayersasdescribedinSection3.1. Table3outlinestheTop-5most
criticallayersforsomeofthemodelsandcorrespondingdatasetsatthestartoftheCLATfine-tuning,
afteradversariallytrainingwithPGD-ATfor50epochs. IncustomizingtheCLATmethodology
tovariousnetworksizes,weselectapproximately5%oflayersascriticalthroughhyperparameter
optimization. For instance, DN121 uses 5 critical layers, while WRN70-16, RN50, WRN34-10,
VGG19,andRN18use4,3,2,1,and1criticallayers,respectively.
4.2 CLATperformance
White-boxrobustness Table1presentsthecleanaccuracyandwhite-boxℓ PGD[6]accuracy
∞
acrossvariousadversarialtrainingtechniques. TheversatilityandeffectivenessofCLATaredemon-
strated by combining it with various standard adversarial training methods. CLAT mitigates the
overfittingcommonlyobservedintraditionaladversarialtrainingmethods,improvingbothcleanand
adversarialaccuracycomparedtothecorrespondingbaselines.
Wealsoshowthatreducingtrainableparametersalonedoesnotguaranteeaperformanceimprove-
mentbycomparingCLATagainstotherparameter-efficientfine-tuningmethodslikeLoRA[1]and
RiFT[37]. Ourtheoretically-groundedcriticalityindicesenablethepreciseidentificationofcritical
layersandthefocusedeliminationofnon-robustfeaturesfromthesepivotallayers. Furthermore,we
verifythatfastadversarialtrainingtechniquesasin[31]canbeappliedtosolvetheinnermaximiza-
tionintheCLATtrainingobjectiveinEquation(8),wherethe“CLAT(Fast)”methodalsoimproves
performanceandrobustnessoverFast-ATbaselines.
Besidesthemaintable,weprovideadditionalrobustnessresultsinAppendixB,whereweshowcase
CLAT’simprovedrobustnessagainstavariantofthewhite-boxAutoAttack[8]inTable8andverify
thegeneralizabilityoftherobustnesstoattacksofvariousstrengthsinFigure4.
Black-boxrobustness Table2evaluatestherobustnessagainstblack-boxattacksbetweenmodels
trainedsolelyusingPGD-ATandthoseaugmentedwithCLAT.Attacksettingsarethesameasthose
ofthewhite-boxattacks. Asasanitycheck, theaccuraciesunderblack-boxattacksurpassthose
observedunderwhite-boxscenarios,indicatingthatgradientmaskingdoesnotappearintheCLAT
model,andthatthewhite-boxrobustnessevaluationisvalid. Moresignificantly,modelstrainedwith
CLATconsistentlyoutperformthosetrainedwithPGD-AT,maintainingsuperiorresilienceinboth
black-boxandwhite-boxsettings,regardlessoftheattackmethodormodelsemployed.
4.3 AblationStudies
4.3.1 AblatingonpretrainingepochsbeforeCLAT
AsdiscussedinSection3.3,weapplyCLATafterthemodelhasbeenadversariallytrainedforsome
epochs. Here,weanalyzehowthenumberofpretrainingepochsaffectsCLATperformance. Figure2
showsthetrainingcurvesfordifferentallocationsofPGDpretrainingepochsandCLATfine-tuning
epochs within a 100-epoch training budget. The overfitting of PGD-AT is evident as adversarial
accuracyplateausanddeclinestowardstheend,asdocumentedinpreviousresearch[24]. Incontrast,
CLATcontinuestoimproveadversarialaccuracy,effectivelyaddressingthisissue. IncludingCLAT
atanystageoftrainingresultsinhighercleanaccuracyandrobustnessatconvergence. Additional
resultsonpretrainedcleanmodelsareprovidedinAppendixC.
Furthermore,anintriguingaspectofourexperimentsinvolvesrunningCLATfromscratch(0PGD-AT
epochs). AlthoughCLATultimatelysurpassesPGD-ATwithsufficientepochs,usingCLATwithout
any prior adversarial training results in significantly slower model convergence. We believe this
suggeststhat“layercriticality”emergesduringtheadversarialtrainingprocess, allowingcritical
layerstobeidentifiedasthemodelundergoesadversarialtraining. Thisphenomenonsupportsour
theoreticalinsightthatcriticalitycanbelinkedtothecurvatureofthelocalminimatowhicheach
layerconvergesduringadversarialtraining. Flatteningthelocalminimaofcriticallayersaidsin
modelgeneralization. Weplantoexplorethisphenomenonfurtherinfuturework.
4.3.2 Ablatingoncriticallayerselection
The choice of critical layer selection is another important feature impacting the performance of
CLAT.Webeginbyexaminingtheeffectofdynamiclayerselection. Table4highlightsthatdynamic
6Table1: ComparativeperformanceofCLATacrossvariousnetworksandadversarialtraining/fine-
tuningtechniques. Robustnessisevaluatedwithwhite-boxPGDattacks.
MODEL METHOD CIFAR-10ACC.(%) CIFAR-100ACC.(%)
CLEAN ADVERSARIAL CLEAN ADVERSARIAL
DN121 PGD-AT[19] 80.05 58.15 57.18 31.76
PGD-AT+CLAT 81.03 60.60 58.79 33.23
WRN70-16 PENGETAL.[22] 93.27 71.07 70.20 42.61
PENGETAL. +CLAT 93.56 72.25 71.94 44.12
BAIETAL.[5] 92.23 64.55 69.17 40.86
BAIETAL. +CLAT 92.77 64.92 70.17 41.64
RN50 PGD-AT 81.38 56.35 58.16 33.01
PGD-AT+CLAT 83.78 59.54 61.88 36.23
WRN34-10 PGD-AT 87.41 55.40 59.19 31.66
PGD-AT+LORA[1] 73.36 56.17 55.56 31.43
PGD-AT+RIFT[37] 87.89 55.41 62.35 31.64
PGD-AT+CLAT 88.97 57.11 62.38 32.05
TRADES[36] 87.60 56.61 60.56 31.85
TRADES+RIFT 87.55 56.72 61.01 32.03
TRADES+CLAT 88.23 57.89 61.45 33.56
VGG19 PGD-AT 78.38 50.35 50.16 26.54
PGD-AT+CLAT 79.88 52.54 50.98 28.41
RN18 PGD-AT 81.46 53.63 57.10 30.15
PGD-AT+LORA 76.57 55.38 48.49 32.36
PGD-AT+RIFT 83.44 53.65 58.74 30.17
PGD-AT+CLAT 83.89 55.37 59.22 32.04
TRADES 81.54 53.31 57.44 30.20
TRADES+RIFT 81.87 53.30 57.78 30.22
TRADES+CLAT 81.89 54.57 58.82 31.06
MART[29] 76.77 56.90 51.46 31.47
MART+RIFT 77.14 56.92 52.42 31.48
MART+CLAT 76.82 57.55 53.01 33.23
AWP[32] 78.40 53.83 52.85 31.00
AWP+RIFT 78.79 53.84 54.89 31.05
AWP+CLAT 79.01 55.27 55.39 32.08
SCORE[21] 84.20 54.59 54.83 29.49
SCORE+RIFT 85.65 54.62 57.63 29.50
SCORE+CLAT 86.11 55.78 57.66 30.23
PREACTRN18 FAST-AT[31] 81.46 45.55 50.10 27.72
FAST-AT+CLAT 84.46 52.13 54.33 29.22
FAST-AT+CLAT(FAST) 82.72 49.62 52.10 27.99
selectioniscrucialtoCLAT’sperformance. Usingthesamelayersthroughouttheprocesstendsto
causeoverfittingandresultsinloweraccuraciescomparedtothePGD-ATbaseline.
To verify the significance of the selected critical layers, we compare CLAT with an alternative
approach in which random layers are dynamically selected for fine-tuning instead of the critical
layers. The results of this comparison are detailed in Table 5 and Table 10 (in Appendix C)
on CIFAR-10 and CIFAR-100, respectively. The data demonstrates that selecting critical layers
significantlyenhancesthemodel’sadversarialrobustnessandyieldsamarginalincreaseinclean
accuracy. Moreover,Table3indicatesnear-identicalcriticallayerselectionswithinthesamemodel
evenacrossdiversedatasets. Thisevidencesupportsourassertionthatthevariationinlayercriticality
stemsfrominherentpropertieswithinthemodelarchitecture,whereincertainlayersarepredisposed
tolearningnon-robustfeatures.
Lastly,weablateonthenumberofcriticallayersusedinCLATforfine-tuning.Figure3illustratesthe
trade-offbetweenadversarialtestaccuracyandthequantityofcriticallayersselectedforfine-tuning.
Allowingmorelayerstobefine-tunedenhancesthemodelflexibility,whichinitiallyimprovesCLAT
performance. However, fine-tuninganexcessiveproportionofcriticallayersdiminishesCLAT’s
effectiveness. Thisislikelyduetothediversionofattentiontowardsfine-tuningless-criticallayers,
7Table2: ComparativeAnalysisofBlack-boxPGDAccuracyonCIFAR-10andCIFAR100. Modelin
eachrowistheattackerandeachcolumnthevictim.
NETWORK METHOD CIFAR-10ADV. ACC. (%) CIFAR-100ADV. ACC. (%)
RN50 DN121 VGG19 RN18 RN50 DN121 VGG19 RN18
RN50 PGD-AT - 74.83 68.01 67.44 - 46.82 40.55 40.10
CLAT - 76.45 71.25 70.12 - 48.49 44.34 43.91
DN121 PGD-AT 72.24 - 69.53 68.38 44.45 - 40.63 41.22
CLAT 74.55 - 71.78 70.56 46.78 - 43.62 42.88
VGG19 PGD-AT 65.72 67.56 - 62.26 47.86 46.56 - 40.55
CLAT 66.46 70.72 - 65.78 49.25 48.72 - 42.72
RN18 PGD-AT 74.82 70.21 61.83 - 46.28 45.59 39.21 -
CLAT 76.23 73.19 63.96 - 48.89 47.72 41.78 -
Figure2:ComparativeanalysisofCLATperformanceonWRN34-10:Cleanandadversarialaccuracy
onCIFAR-10acrosspartiallytrainedmodels. Note: PGD-ATrequires150epochstofullyconverge.
detractingfrommorecriticalones. Thispatternhighlightsthecrucialinfluenceofspecificlayerson
networkrobustnessandsuggestsaneedfordeeperresearchtounderstandtherolesanddynamicsof
individuallayersinfluencingnetworkrobustness.
4.4 Efficiencyandstabilityanalysis
Toverifythesignificanceoftheselectedcriticallayers,wecontrastCLATwithanalternativeapproach
inwhichrandomlayersaredynamicallyselectedforfine-tuninginsteadofthecriticallayers. The
Figure3: ComparativeanalysisonCLATperformance/adversarialaccuracywithrespecttonumber
ofcriticallayersusedduringCLAT
8Table3: Top-5criticalityindicesbymodeland Table4: ComparisonofPGD-AT,PGD-
dataset. LayersusedinCLATarebolded. AT+CLAT,andPGD-AT+CLAT(Non-
dynamic)onCIFAR-10Adv. Accuracy.
MODEL CIFAR10 CIFAR100
DN121 39,14,1,3,88 39,15,1,2,91 METHOD DN121 RN50
WRN70-16 4,17,1,59,62 3,17,2,59,61
RN50 34,41,48,3,36 34,43,45,6,32
PGD-AT 58.15 56.35
WRN34-10 26,130,3,28 26,2,30,3,27 CLAT 60.60 59.54
VGG19 9,11,5,3,1 8,13,5,3,1 CLAT(ND) 57.01 54.22
RN18 11,10,4,2,12 12,9,5,2,13
Table6: TrainableParametersduring
CLATinVariousNetworks
Table5: Criticalvs. randomlayersforCLATonCIFAR-10.
NETWORK TRAINABLEPARAMS %USED
NETWORK CIFAR-10 TOTAL CLAT
CRITICALLAYERS RANDOMLAYERS DN121 6.96M 217K 3.1%
CLEANACC. ADV. ACC. CLEANACC. ADV. ACC. WRN70-16 267M 8.29M 3.0%
RN50 23.7M 823K 3.4%
DN121 81.03 60.60 78.85 51.35
WRN34-10 46.16M 1.24M 2.7%
RN50 83.78 59.54 79.01 51.44 RN18 11.2M 590K 5.2%
RN18 83.89 55.37 78.02 51.03 VGG19 39.3M 236K 6.0%
resultsofthiscomparisonaredetailedinTable5andTable10(inAppendixC)onCIFAR-10and
CIFAR-100,respectively.
ThecostofoptimizingtheCLATtrainingobjectiveinEquation(9)issimilartothatofthestandard
adversarialtraininggivenitsmin-maxformulation. Here,weshowthatthecomputationaloverhead
fordeterminingcriticalindicesisnegligible. WeverifyinTable7thatcriticalityindicescanbestably
computedwithasingletrainingbatchassmallas10,withtop-rankinglayersconsistentwiththose
achievedwithalargerbatch. Wefurtherconductedover1000runsforeachnetworktorandomly
selectthedatausedforcriticalityestimation,wherewefindremarkableconsistencyincomputed
criticallayers,differinginlessthan5%ofcases,typicallyinvolvingonlyonelayerchangeamong
thetopfivecriticallayers. Thestabilitymeanswecanuseamere0.0002%ofthetrainingdatafor
criticalityestimationevery10epochs,whichintroducesneglectableadditionalcomplexitytothe
trainingprocessasindicatedbythetimemeasurementinthetable.
Table7: DenseNet-121criticallayersidentifiedwithdifferentamountofdata. Timetakentocompute
criticallayersevaluatedonTITANRTXGPU.Asareference,1PGD-ATepochtakes67s.
BATCHSIZE CIFAR10 CIFAR100
CRITICALLAYERS TIME(S) CRITICALLAYERS TIME(S)
10 39,14,1,3,90 2.64 39,15,1,2,91 2.82
30 39,14,1,3,88 2.72 39,15,1,2,88 2.91
50 39,14,1,3,89 2.83 39,15,1,3,91 3.14
100 39,14,1,3,88 3.15 39,15,1,2,91 3.54
5 Conclusions
Inthiswork,weintroduceCLAT,aninnovativeadversarialtrainingapproachthataddressesadversar-
ialoverfittingissuesbyfine-tuningonlythecriticallayersvulnerabletoadversarialperturbations.
Thismethodnotonlyemphasizeslayer-specificinterventionsforenhancednetworkrobustnessbut
also sheds light on the commonality in non-robust features captured by these layers, offering a
targetedandeffectivedefensestrategy. OurresultsrevealthatCLATreducestrainableparametersby
about95%whileensuringsignificantimprovementsincleanaccuracyandadversarialrobustness
acrossdiversenetworkarchitecturesandbaselineadversarialtrainingmethods. Welimitthescope
ofthisworktoimprovingtheempiricalrobustnessofthemodelbyutilizingthecriticallayers. In
thissense,openquestionsremainonwhythesespecificlayersbecomecritical,whethertheycan
beidentifiedmoreeffectively,andwhethertheissuescanberesolvedwitharchitecturalortraining
schemechanges. Weleaveamoretheoreticalunderstandingofthesequestionsasfuturework.
9References
[1] Sidra Aleem, Julia Dietlmeier, Eric Arazo, and Suzanne Little. Convlora and adabn based
domainadaptationviaself-training. arXivpreprintarXiv:2402.04964,2024.
[2] MaksymAndriushchenkoandNicolasFlammarion. Understandingandimprovingfastadver-
sarialtraining,2020.
[3] AnishAthalye,NicholasCarlini,andDavidWagner. Obfuscatedgradientsgiveafalsesenseof
security: Circumventingdefensestoadversarialexamples,2018.
[4] TaoBai, JinqiLuo, JunZhao, BihanWen, andQianWang. Recentadvancesinadversarial
trainingforadversarialrobustness,2021.
[5] YatongBai,BrendonG.Anderson,AerinKim,andSomayehSojoudi. Improvingtheaccuracy-
robustnesstrade-offofclassifiersviaadaptivesmoothing,2024.
[6] NicholasCarliniandDavidWagner. Towardsevaluatingtherobustnessofneuralnetworks,
2017.
[7] YairCarmon,AditiRaghunathan,LudwigSchmidt,PercyLiang,andJohnC.Duchi. Unlabeled
dataimprovesadversarialrobustness,2022.
[8] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an
ensembleofdiverseparameter-freeattacks,2020.
[9] IanJ.Goodfellow,JonathonShlens,andChristianSzegedy. Explainingandharnessingadver-
sarialexamples,2015.
[10] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Identitymappingsindeepresidual
networks,2016.
[11] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimage
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
[12] DorjanHitaj,GiulioPagnotta,IacopoMasi,andLuigiV.Mancini. Evaluatingtherobustnessof
geometry-awareinstance-reweightedadversarialtraining,2021.
[13] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKilianQWeinberger.Denselyconnected
convolutionalnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages4700–4708,2017.
[14] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and
AleksanderMadry. Adversarialexamplesarenotbugs,theyarefeatures. Advancesinneural
informationprocessingsystems,32,2019.
[15] NathanInkawhich, WeiWen, Hai(Helen)Li, andYiranChen. Featurespaceperturbations
yieldmoretransferableadversarialexamples. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),June2019.
[16] NathanInkawhich, KevinLiang, BinghuiWang, MatthewInkawhich, LawrenceCarin, and
YiranChen. Perturbingacrossthefeaturehierarchytoimprovestandardandstrictblackbox
attacktransferability. AdvancesinNeuralInformationProcessingSystems,33:20791–20801,
2020.
[17] AdelJavanmard,MahdiSoltanolkotabi,andHamedHassani. Precisetradeoffsinadversarial
trainingforlinearregression,2020.
[18] AlexKrizhevskyandGeoffreyHinton. Learningmultiplelayersoffeaturesfromtinyimages.
2009.
[19] AleksanderMadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,andAdrianVladu.
Towardsdeeplearningmodelsresistanttoadversarialattacks,2019.
10[20] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard.
Robustness via curvature regularization, and vice versa. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages9078–9086,2019.
[21] TianyuPang, MinLin, XiaoYang, JunZhu, andShuichengYan. Robustnessandaccuracy
couldbereconcilableby(proper)definition,2022.
[22] ShengYunPeng,WeilinXu,CoryCornelius,MatthewHull,KevinLi,RahulDuggal,Mansi
Phute,JasonMartin,andDuenHorngChau. Robustprinciples: Architecturaldesignprinciples
foradversariallyrobustcnns,2023.
[23] AditiRaghunathan,SangMichaelXie,FannyYang,JohnCDuchi,andPercyLiang.Adversarial
trainingcanhurtgeneralization. arXivpreprintarXiv:1906.06032,2019.
[24] LeslieRice,EricWong,andJ.ZicoKolter. Overfittinginadversariallyrobustdeeplearning,
2020.
[25] LudwigSchmidt,ShibaniSanturkar,DimitrisTsipras,KunalTalwar,andAleksanderMadry.
Adversariallyrobustgeneralizationrequiresmoredata,2018.
[26] AliShafahi,MahyarNajibi,AminGhiasi,ZhengXu,JohnDickerson,ChristophStuder,LarryS.
Davis,GavinTaylor,andTomGoldstein. Adversarialtrainingforfree!,2019.
[27] KarenSimonyanandAndrewZisserman. Verydeepconvolutionalnetworksforlarge-scale
imagerecognition. In3rdInternationalConferenceonLearningRepresentations,ICLR2015-
ConferenceTrackProceedings,2015.
[28] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,IanGood-
fellow,andRobFergus. Intriguingpropertiesofneuralnetworks,2014.
[29] YisenWang,DifanZou,JinfengYi,JamesBailey,XingjunMa,andQuanquanGu. Improving
adversarialrobustnessrequiresrevisitingmisclassifiedexamples. InInternationalconference
onlearningrepresentations,2019.
[30] YisenWang,DifanZou,JinfengYi,JamesBailey,XingjunMa,andQuanquanGu. Improving
adversarialrobustnessrequiresrevisitingmisclassifiedexamples.InInternationalConferenceon
LearningRepresentations,2020. URLhttps://openreview.net/forum?id=rklOg6EFwS.
[31] Eric Wong, Leslie Rice, and J. Zico Kolter. Fast is better than free: Revisiting adversarial
training,2020.
[32] DongxianWu, ShutaoXia, andYisenWang. Adversarialweightperturbationhelpsrobust
generalization,2020.
[33] CihangXie,MingxingTan,BoqingGong,JiangWang,AlanYuille,andQuocV.Le.Adversarial
examplesimproveimagerecognition,2020.
[34] HuanruiYang,JingyangZhang,HongliangDong,NathanInkawhich,AndrewGardner,Andrew
Touchet, Wesley Wilkes, Heath Berry, and Hai Li. Dverge: diversifying vulnerabilities for
enhancedrobustgenerationofensembles. AdvancesinNeuralInformationProcessingSystems,
33:5505–5515,2020.
[35] SergeyZagoruykoandNikosKomodakis. Wideresidualnetworks,2017.
[36] HongyangZhang,YaodongYu,JiantaoJiao,EricP.Xing,LaurentElGhaoui,andMichaelI.
Jordan. Theoreticallyprincipledtrade-offbetweenrobustnessandaccuracy,2019.
[37] KaijieZhu, JindongWang, XixuHu, XingXie, andGeYang. Improvinggeneralizationof
adversarialtrainingviarobustcriticalfine-tuning,2023.
11A PseudocodeofCLAT
TobetterfacilitateanunderstandingoftheCLATprocess,weillustratethepseudocodeofthedynamic
criticallayeridentificationprocessandthecriticality-targetedfine-tuningprocessinAlgorithm1.
Onlytheselectedcriticallayersarebeingfine-tunedwhilealltheotherlayersarefrozen.
Algorithm1CLATAlgorithm
1: Input: DatasetD,pre-trainedmodelF,batchsizebs,totalepochsN.
2: forepoch=1toN do
3: ifepoch%10==1then
4: # Find critical layers
5: x←BatchoftrainingdatainD
6: x+δ ←PGDattackagainstF
7: ComputeW ϵ(F i)foralllayerswithEquation(5)
8: ComputeC fi foralllayerswithEquation(6)
9: CriticallayersS ←TopK(C fi)
10: # fine-tune critical layers
11: minibatches←CreateMinibatches(D,bs)
12: forx,yinminibatchesdo
13: Perturbationδ ←Equation(8)innermaximization
14: L C(f S)←Equation(8)
15: Weightupdatew[S]withEquation(9)
B Additionalexperimentresults
Table8showcasesresultsunderavariantofthewhite-boxAutoAttack[8]withanepsilonvalueof
0.03andasinglerestartinTable1. Specifically,wereportadversarialaccuraciesofthe“APGD ”
CE
untargetedattack,acomponentoftheAutoAttackensemble,withsimilartrendsobservedforother
attacksintheensemble.
AlthoughCLATmodelsemployPGD-likeattacksonhiddenfeaturesduringtheadversarialtraining
phaseandhavenotseenAutoAttacks,therobustnessremainsunderAutoAttacks.Thishighlightsthe
similarityinnon-robustfeaturesutilizedbydifferentattacksindifferentnetworks. AsCLATresolves
thenon-robustfeatureswithcriticallayerfine-tuning,theresultingrobustnessisgeneralizableacross
attacksettingsandsourcemodelsoftheattacks.
Table8:Adversarialaccuracyunderthewhite-box“APGD ”variantofAutoAttack:Modelstrained
CE
usingCLATconsistentlyoutperformthosetrainedexclusivelywithPGD-AT.
DATASET NETWORK PGD-AT(%) PGD-AT+CLAT(%)
DN121 49.26 50.58
WRN70-16 54.48 56.90
CIFAR-10
RN50 48.54 50.65
WRN34-10 52.21 54.73
VGG19 45.35 47.01
RN18 46.99 48.20
DN121 24.67 26.31
WRN70-16 30.27 33.32
CIFAR-100
RN50 25.44 26.64
WRN34-10 27.62 31.53
VGG19 22.50 24.02
RN18 23.81 24.43
We further compare the CLAT model robustness with the robustness of the SAT model against
white-box attacks of various strengths. As illustrated in Figure 4, though both models are only
12trainedagainstanattackofonestrength(ϵ=0.03),theimprovedrobustnessofCLATisconsistent
acrossthefullspectrumofattackstrengths. ThisshowsthatCLATisnotoverfittingtothespecific
attackstrengthusedintraining.
Figure4: White-boxadversarialaccuracy(y-axis)onCIFAR-10formodelstrainedwithCLAT(red)
andPGD-AT(blue),againstPGDattacksofvaryingstrengths(x-axis)
C Additionalablationstudy
C.1 CLATonpretrainedcleanmodel
BesidesthediscussiononperformingCLATafteradversarialpretraininginSection4.3.1,Table9
detailstheperformanceofCLAToncleanpretrainedmodels. Althoughtheadversarialaccuraciesof
cleanpretrainedmodelsarerelativelylowcomparedtothoseofadversariallytrainedmodels,CLAT
demonstratesitscapabilitytofacilitateadversarialfine-tuningoncleanmodelseffectivelytosome
extent. Thisisanovelachievement,showcasingthealgorithm’sversatility.
Table 9: Adversarial and clean accuracies for performing CLAT on various PyTorch pretrained
modelsontheCIFAR-10dataset.
MODEL ADV. ACC. CLEANACC.
DN-121 39.21% 80.89%
WRN70-16 42.1% 83.35%
RN-50 35.67% 78.23%
WRN34-10 40.1% 81.78%
VGG-19 32.67% 75.05%
RN-18 34.45% 76.51%
C.2 Additionalresultsonlayerselection
HereinTable10,weprovideadditionalresultscontrastingCLATwithanalternativeapproachwhere
randomlayersaredynamicallyselectedforfine-tuninginsteadofthecriticallayersontheCIFAR-100
dataset.
Table10: AblatingthelayerchoicesforCLATfine-tuningonCIFAR-100.
NETWORK CIFAR-100
CRITICALLAYERS RANDOMLAYERS
CLEANACC. ADV. ACC. CLEANACC. ADV. ACC.
DN121 58.79 33.23 54.85 23.32
RN50 61.88 36.23 56.50 22.45
RN18 59.22 32.04 52.43 22.60
13NeurIPSPaperChecklist
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’scontributionsandscope?
Answer: [Yes]
Justification: Mainclaimsmadeintheabstractandintroductionaccuratelyreflectthescope
andcontributionofthepaper,supportedbyourtheoreticalderivationsandexperimental
results.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
madeinthepaper.
• Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor
NAanswertothisquestionwillnotbeperceivedwellbythereviewers.
• Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow
muchtheresultscanbeexpectedtogeneralizetoothersettings.
• Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
arenotattainedbythepaper.
2. Limitations
Question: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer: [Yes]
Justification: Thispaperislimitedtoproposinganempiricallyeffectivemethodtoidentify
adversariallycriticallayersandreducetheircriticality,withoutansweringthequestionon
whysuchdiscrepancyincriticalityappearsinthemodelinthefirstplace. Thelimitation
itselfandpotentialfuturedirectionstoresolveitarediscussedinSection5.
Guidelines:
• TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat
thepaperhaslimitations,butthosearenotdiscussedinthepaper.
• Theauthorsareencouragedtocreateaseparate"Limitations"sectionintheirpaper.
• Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors
shouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe
implicationswouldbe.
• Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas
onlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften
dependonimplicitassumptions,whichshouldbearticulated.
• Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
isloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe
usedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle
technicaljargon.
• Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms
andhowtheyscalewithdatasetsize.
• If applicable, the authors should discuss possible limitations of their approach to
addressproblemsofprivacyandfairness.
• Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitationsthataren’tacknowledgedinthepaper. Theauthorsshouldusetheirbest
judgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-
tantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers
willbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.
3. TheoryAssumptionsandProofs
14Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand
acomplete(andcorrect)proof?
Answer: [NA]
Justification: Instead of providing theoretical proof, we justify the formulation of our
proposedmethodempiricallythroughasetofcomparisonsagainstbaselines(Section4.2)
anddetailedablationstudies(Section4.3).
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.
• Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-
referenced.
• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif
theyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort
proofsketchtoprovideintuition.
• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
byformalproofsprovidedinappendixorsupplementalmaterial.
• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4. ExperimentalResultReproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer: [Yes]
Justification: WediscusstheexperimentsettingsinSection4.1.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived
well by the reviewers: Making the paper reproducible is important, regardless of
whetherthecodeanddataareprovidedornot.
• Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
tomaketheirresultsreproducibleorverifiable.
• Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully
mightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay
benecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame
dataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften
onegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed
instructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriatetotheresearchperformed.
• WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-
sionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe
natureofthecontribution. Forexample
(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
toreproducethatalgorithm.
(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe
thearchitectureclearlyandfully.
(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
themodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct
thedataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.
Inthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin
someway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers
tohavesomepathtoreproducingorverifyingtheresults.
155. Openaccesstodataandcode
Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-
tionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental
material?
Answer: [Yes]
Justification: Allexperimentsaredonewithpublicly-availabledatasets. Besidesdiscussing
our settings in Section 4.1, we will release the code and model checkpoints upon the
acceptanceofthepaper.
Guidelines:
• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy)formoredetails.
• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe
possible,so“No”isanacceptableanswer. Paperscannotberejectedsimplyfornot
includingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source
benchmark).
• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:
//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.
• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow
toaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.
• Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew
proposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they
shouldstatewhichonesareomittedfromthescriptandwhy.
• Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized
versions(ifapplicable).
• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe
paper)isrecommended,butincludingURLstodataandcodeispermitted.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: SeeSection4.1.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
thatisnecessarytoappreciatetheresultsandmakesenseofthem.
• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer: [No]
Justification: Wetrainallofourmodelswitharandomlyselectedfixedrandomseed. For
evaluation,weconductadversarialattacksoneachmodelforatleast10times,andreport
thelowestadversarialaccuracy(insteadofanerrorbar)asitreflectsthetruerobustnessof
themodelmorereliably.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
16• Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confi-
denceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport
themainclaimsofthepaper.
• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
runwithgivenexperimentalconditions).
• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,
calltoalibraryfunction,bootstrap,etc.)
• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).
• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror
ofthemean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
ofNormalityoferrorsisnotverified.
• Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
errorrates).
• Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow
theywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.
8. ExperimentsComputeResources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-
puterresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce
theexperiments?
Answer: [Yes]
Justification: WereportthecomputationequipmentusedinSection4.1. Wefurtherprovide
detaileddiscussioninSection4.4onthedifferenceincostbetweenourproposedmethod
andpreviousstandardadversarialtrainingresearch.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,
orcloudprovider,includingrelevantmemoryandstorage.
• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimentalrunsaswellasestimatethetotalcompute.
• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute
thantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat
didn’tmakeitintothepaper).
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: Weconfirmthatourpaper,ineveryrespect,withtheNeurIPSCodeofEthics.
Guidelines:
• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.
• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea
deviationfromtheCodeofEthics.
• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-
erationduetolawsorregulationsintheirjurisdiction).
10. BroaderImpacts
Question: Does the paper discuss both potential positive societal impacts and negative
societalimpactsoftheworkperformed?
Answer: [Yes]
17Justification: Thispaperprovidesamethodthatimprovestherobustnessofdeepneural
networks against adversarial attacks, which helps promoting trustworthy deep learning
applications. Nopotentialmisuseornegativesocialimpactcanbethinkof.
Guidelines:
• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.
• IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal
impactorwhythepaperdoesnotaddresssocietalimpact.
• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups),privacyconsiderations,andsecurityconsiderations.
• Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
anynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
generatedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
modelsthatgenerateDeepfakesfaster.
• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis
being used as intended and functioning correctly, harms that could arise when the
technologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing
from(intentionalorunintentional)misuseofthetechnology.
• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom
feedbackovertime,improvingtheefficiencyandaccessibilityofML).
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer: [NA]
Justification: Nopotentialmisuseornegativesocialimpactcanbethinkof.
Guidelines:
• TheanswerNAmeansthatthepaperposesnosuchrisks.
• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safetyfilters.
• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors
shoulddescribehowtheyavoidedreleasingunsafeimages.
• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo
notrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest
faitheffort.
12. Licensesforexistingassets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer: [Yes]
Justification: Wecitealltheassetsweusedproperly.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.
• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.
18• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.
• Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof
serviceofthatsourceshouldbeprovided.
• If assets are released, the license, copyright information, and terms of use in the
packageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasets
hascuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethe
licenseofadataset.
• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof
thederivedasset(ifithaschanged)shouldbeprovided.
• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto
theasset’screators.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
Answer: [Yes]
Justification: Wewillprovidecodeandpretrainedmodelswithproperdocumentations.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.
• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir
submissions via structured templates. This includes details about training, license,
limitations,etc.
• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
assetisused.
• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither
createananonymizedURLorincludeananonymizedzipfile.
14. CrowdsourcingandResearchwithHumanSubjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
wellasdetailsaboutcompensation(ifany)?
Answer: [NA]
Justification: Wehavenohumansubjectsinourpaper.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-
tionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
includedinthemainpaper.
• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
orotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata
collector.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
suchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution)wereobtained?
Answer: [NA]
Justification: Wehavenohumansubjectsinourpaper.
Guidelines:
19• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
shouldclearlystatethisinthepaper.
• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe
guidelinesfortheirinstitution.
• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if
applicable),suchastheinstitutionconductingthereview.
20