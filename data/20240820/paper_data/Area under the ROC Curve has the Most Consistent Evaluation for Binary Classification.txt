Area under the ROC Curve has the Most
Consistent Evaluation for Binary Classification
Jing Li1*
1*University of Illinois at Urbana-Champaign, 420 David Kinley Hall,
1407 W Gregory Drive, Urbana, 61801, Illinois, USA.
Corresponding author(s). E-mail(s): jingl8@illinois.edu;
Abstract
Background:EvaluationMetricsisanimportantquestionformodelevaluation
andmodelselectioninbinaryclassificationtasks.WhileaccuracyandAreaunder
theROCCurve havebeencommonlyusedto assessmodel performance, various
criticismshavebeenraisedagainsttheuseofthesetwometrics.Thispapertakes
a different view. We assess how consistent metrics are at evaluating a set of
different models across different prevalence levels.
Results: Analyzing over 150 data scenarios and 18 model evaluation metrics
using statistical simulation, the results presented in this paper show that for
binary classification tasks, evaluation metrics that are less influenced by preva-
lenceinevaluationofindividualmodelsalsooffermoreconsistentrankingofaset
ofmultipledifferentmodels.Inparticular,AreaUndertheROCCurve(AUC)has
smallestvarianceinrankingofdifferentmodels.Matthew’scorrelationcoefficient
asamorestrictmeasureofmodelperformancehasthesecondsmallestvariance.
These patterns holds across a rich set of data scenarios and five commonly used
machine learning models as well as a naive random guess model.
Conclusion: In conclusion, with respective to changing prevalence levels, AUC
is the best measure for obtaining consistent ranking among different models,
MCCissuitableforassessingpoint-wisealignmentbetweenpredictedandactual
class labels, and accuracy assesses alignment in probability between predicted
and actual class labels.
Keywords:Prevalence,EvaluationMetrics,BinaryClassification,AreaUnderthe
ROCCurve,Matthew’sCorrelationCoefficient,Accuracy
1
4202
guA
91
]LM.tats[
1v39101.8042:viXra1 Introduction
Evaluationmetricsforbinaryclassificationtaskshavereceivedaconsiderableamount
of scientific attention especially for imbalanced data [1–11]. The most widely used
metric, Area under the ROC Curve (AUC) has been criticized as a bad choice espe-
cially when data is class imbalanced. As a result, a number of alternatives have been
proposed. For example, Matthew’s correlation coefficient (MCC) has been proposed
as a better alternative because it has high values only when a model predicts well on
bothpositiveandnegativecases[6,7,11].However,research[5]hasshownthatMCC
can have skewed distribution when data is class imbalanced. Precision-recall curves
has been suggested as a better alternative to AUC in low prevalence level cases [12];
averagepositivepredictivevaluehasbeensuggestedanattractivealternativeforeval-
uation and comparison of medical screening tests because it plays more emphasis on
early positives than does AUC [13].
Here we revisit the question of model evaluation metrics for binary classification
and explicitly examine the relationships between class imbalance and 18 evaluation
metrics through a simulation study. The results show that evaluation metrics that
are less influenced by prevalence in evaluation of individual models also offer more
consistent ranking of a set of multiple different models including five commonly used
machine learning models and a naive model that mimics randomly assigning class
labelstoobservationsinthetestset.Inparticular,AreaundertheROCCurve(AUC),
which is mostly prevalence-independent, has the smallest variance in its ranking of
different models. As a practical guide for action, if the goal of model evaluation is to
find the most consistent ranking of models across all possible prevalence levels, then
AUC should be the metric of choice.
Thenoveltyofthispaperresidesinitsuseofstatisticalsimulation,whichgenerates
a much richer set of data scenarios compared with existing research that only provide
results for a small number of cases. As a result, we are able to show a more complete
pictureoftherelationshipsbetweendata,evaluationmetricsandmodelswhilekeeping
samplesizeandtherelationshipsbetweenrelevantvariablesconstantatthesametime.
In addition, the results presented below make an explicit link between prevalence,
evaluation of individual models, and ranking of multiple different models by different
metrics. Furthermore, a much larger set of model evaluation metrics are considered
here than existing research.
This paper is organized as follows, in section 2, we first provide background infor-
mation and notations on model evaluation metrics for binary classification tasks. In
section 3, we look at a prominent case of predicting crime recidivism and present a
simulationstudythatexamineshowdifferentconfusionmatrixmetricsevaluatemodel
differently under different levels of prevalence in the data. In section 4, we show the
analysis results. And in section 5, we offer discussions and conclusions.
22 Methods
2.1 Background and Basic Notations
Evaluation metrics are statistics used to evaluate model performance especially for
out of sample prediction tasks as is commonly the case for machine learning model
applications. Below we compare several evaluation metrics and look at how accurate
and reliable they are at capturing the model prediction performance for data of dif-
ferent prevalence level. We focus on binary classification tasks in this study. First, let
usintroduceafewconcepts.Foreverybinaryprediction/classificationtask,theresult
could fall into four different categories:
• true positives (TP), positive cases correctly predicted as positive;
• false negatives (FN), positive cases incorrectly predicted as negative;
• true negatives (TN), negative cases correctly predicted as negative;
• false positives (FP), negative cases incorrectly predicted as positive.
The partition of these four categories can be represented in a 2×2 tabular format
(cid:18) (cid:19)
TP FN
called confusion matrix: .
FP TN
Table1belowrepresentsthedifferentnumberofobservationsforeachtypeofcases
in the confusion matrix.
Table 1 ConfusionMatrix
Prediction
Yes No Total
Yes n11 n10 n1.
Actual No n01 n00 n0.
Total n.1 n.0 n
It can be observed that TP+FN=n , and TN+FP=n , and TP+FP=n ,
1. 0. .1
and TN+FN=n .
.0
The first prediction performance metric we look at is the most commonly used
accuracy measure, which has the following form:
TP+TN TP+TN
accuracy= =
n +n TP+FN+TN+FP
1. 0.
Then we have the true positive rate (TPR) or recall, which has the following form:
TP
True Positive Rate=
TP+FN
Andcorrespondinglywehavetruenegativerate(TNR),whichhasthefollowingform:
TN
True Negative Rate=
TN+FP
3Then there is positive predictive value (PPV) or precision, which has the following
form:
TP
PPV=
TP+FP
And correspondingly there is negative predictive value (NPV): NPV = TN .
TN+FN
In addition, false omission rate (FOR) = 1−NPV,false discovery rate (FDR) = 1−
PPV;False Positive Rate (FPR)=1−TNR,False Negative Rate (FNR)=1−TPR.
And a number of metrics that combine the True Positive Rate and True Nega-
tive Rate in certain ways. Balanced Accuracy (BA): BA = TPR+TNR, Bookmaker
2
informedness (BI): BI=TPR+TNR−1, Markedness (MK): MK=PPV+NPV−1,
√
geometric mean (Gmean): Gmean = TPR×TNR, Jaccard index (JI): JI =
TP , diagnostic odds ratio (DOR): DOR = TPR×TNR, Fowlkes-Mallows index
TP+FN+FP √ FPR×FNR
(FM): FM= PPV×TPR.
Cohen’s Kappa: Kappa = Accuracy−expAccuracy,expAccuracy =
1−expaccuracy
(TP+FN)·(TP+FP)+(TN+FP)·(TN+FN).
n2
F1 score: F1 score= 2 = 2TP .
recall−1+precision−1 2TP+FP+FN
F score: F = (1+β2)· PPV·TPR . In the following we use β = 2,0.5, two
β β (β2·PPV)+TPR
commonly used values.
Matthew’s correlation coefficient (MCC):
TP·TN−FP·FN
MCC= (minimum: -1, maximum: +1)
(cid:112)
(TP+FP)·(TP+FN)·(TN+FP)·(TN+FN)
.
2.2 Alternative Derivation of Confusion Matrix Metrics
As mentioned in the literature [3, 14], all entries of a confusion matrix and confusion
matrix metrics can be derived from the four quantities of sample size n, prevalence,
TPR and TNR. To show that this is indeed the case, below we redefine the confu-
sion matrix metrics introduced above in terms of these four quantities. We use ϕ to
represent prevalence, and ϕ= n1. = TP+FN.
n n
TP+TN
accuracy= =TPR·ϕ+TNR·(1−ϕ). (1)
n
And F1 score = 2TP = 2TPR·ϕ·n =
2TP+FP+FN 2TPR·ϕ·n+(1−TNR)·(1−ϕ)·n+(1−TPR)·ϕ·n
2 .
2+1−TNR1−ϕ+1−TPR
TPR ϕ TPR
Therefore,
2
F1 score= . (2)
2+ 1 ((1−TNR)1−ϕ +1−TPR)
TPR ϕ
TPR·ϕ·n TPR·ϕ
PPV= = . (3)
TPR·ϕ·n+(1−TNR)·(1−ϕ)·n TPR·ϕ+(1−TNR)·(1−ϕ)
4TNR·(1−ϕ)·n TNR·(1−ϕ)
NPV= = . (4)
TNR·(1−ϕ)·n+(1−TPR)·ϕ·n TNR·(1−ϕ)+(1−TPR)·ϕ
And FOR= (1−TPR)·ϕ ,FDR= (1−TNR)·(1−ϕ) .
TNR·(1−ϕ)+(1−TPR)·ϕ TPR·ϕ+(1−TNR)·(1−ϕ)
And Markedness (MK): MK = TPR·ϕ + TNR·(1−ϕ) −1,
TPR·ϕ+(1−TNR)·(1−ϕ) TNR·(1−ϕ)+(1−TPR)·ϕ
Jaccard index (JI): JI = TPR·ϕ , diagnostic odds ratio (DOR): DOR =
ϕ+(1−TNR)·(1−ϕ)
TPR×TNR . Fowlkes-Mallows index (FM): FM= TPR2·ϕ .
(1−TNR)×(1−TPR) TPR·ϕ+(1−TNR)·(1−ϕ)
Kappa = TPR·ϕ+TNR·(1−ϕ)−expAccuracy, expAccuracy =
1−expaccuracy
ϕ2TPR+ϕ(1−ϕ)(2−TPR−TNR)+(1−ϕ)2TNR.
n
An alternative expression of Matthew’s correlation coefficient is as follows:
√ √
MCC= PPV×TPR×TNR×NPV− FDR×FNR×FPR×FOR
. Then we have:
(cid:114) (cid:114)
MCC= TPR2·ϕ · TNR2·(1−ϕ) − (1−TNR)2·(1−ϕ) · (1−TPR)2·ϕ
TPR·ϕ+(1−TNR)·(1−ϕ) TNR·(1−ϕ)+(1−TPR)·ϕ TPR·ϕ+(1−TNR)·(1−ϕ) TNR·(1−ϕ)+(1−TPR)·ϕ
(5)
TPR+TNR−1
= .
(cid:113)
(TPR· ϕ +1−TNR)(TNR· 1−ϕ +1−TPR)
1−ϕ ϕ
2.3 Case Study: Crime Recidivism
First we look at a case where the outcome of interest contains more or less balanced
(positive and negative) classes, we leverage the Broward County data set [15, 16].
The data set consists of individuals arrested in Broward County, Florida between
2013 and 2014. The sample size is 6214, with 2775 positive cases, individuals who
reoffended and 3439 negatives, individuals who did not reoffend. The predictors for
the classifier include seven features of the defendants: gender, age, number of juvenile
misdemeanors,numberofjuvenilefelonies,numberofprior(nonjuvenile)crimes,crime
degree, and crime charge [15]. In the following, we train five commonly used machine
learning models and see how the different evaluation metrics listed above capture
the prediction performance of these five different models: logistic regression (GLM),
random forest (RF), k-nearest neighbors (KNN), linear discriminant analysis (LDA),
gradient boosting machine (GBM). In addition, we make a random guess model that
randomly assign class labels to observations based on the prevalence level of the test
set.Forexample,foratestsetwith60percentpositivecases,therandomguessmodel
will randomly assign positive label to 60 percent cases and negative label to the other
540 percent1. The sample data are randomly split into 80 percent training data and 20
percent test data. 10-fold cross-validation is used with the model training process.
Table 2 CrimeRecidivismPrediction:RankofDifferentModels
Metric GBM GLM KNN LDA RandomForest randomguess
True positives 1 4 3 5 2 6
False negatives 1 4 3 5 2 6
True negatives 5 3 4 1.5 1.5 6
False positives 5 3 4 1.5 1.5 6
TPR 1 4 3 5 2 6
TNR 5 3 4 1.5 1.5 6
PPV 3 4 5 2 1 6
NPV 1 3 5 4 2 6
Accuracy 1 3.5 5 3.5 2 6
BA 1 3 5 4 2 6
BI 1 3 5 4 2 6
F1 score 1 3 4 5 2 6
MCC 1 4 5 3 2 6
Gmean 1 3 5 4 2 6
Fowlkes Mallows 1 3 5 4 2 6
Markedness 2 4 5 3 1 6
Diag odds ratio 2 4 5 3 1 6
Jaccardindex 1 3 4 5 2 6
Cohens kappa 1 3 5 4 2 6
F beta score 0.5 1 3 4 5 2 6
F beta score 2 1 4 3 5 2 6
AUC 1 3 5 4 2 6
Table 2 lists the rankings of the different models by the different metrics. Table 4
inAppendixliststhecorrespondingconfusionmatrixentriesforthesixdifferentmodel
predictions (first four rows) and the values of 18 model evaluation metrics introduced
above.
Theresultsindicatethatallevaluationmetricsranktherandomguessmodelasthe
worst performing model as expected. The GBM model is most frequently ranked as
the best performing model, the random forest model is most frequently ranked as the
second best. The GLM model is most frequently ranked as either third or fourth. The
KNN model is most frequently evaluated as fifth but occasionally as third or fourth.
The LDA model is ranked between third, fourth, and fifth or 1.5. Ties in ranking are
brokenbyaverages.AsTP, FN, TN, FP, TPR, TNRconcernpositiveornegativeclass
only for either actual or predicted labels, it is no surprise that rankings are reversed
betweenmetricsfocusingontruepositiverateonlyandthosefocusingontruenegative
rate only. AUC calculations are based on various classification threshold values while
all other performance metric calculations are based on the 0.5 cutoff. These patterns
can also be observed from the corresponding values of model evaluation metrics for
different models in Table 4.
1Theprobabilityofpositiveclassisarandomnumberbetween0.51and0.99whileprobabilityofnegative
classisarandomnumberbetween0.01and0.49.
6While this example shows that different model evaluation metrics can converge
in their evaluation of classification outcomes, this is just one single case. And more
importantly, this is an example where the positive and negative class in the data
are more or less the same. To be precise, the prevalence in the data is 0.452. When
data becomes more imbalanced and either the positive or negative class dominates
the data, different metrics’ evaluation of model can be very different. To obtain a
more complete picture of the dynamics between different mix of positive and negative
cases and the evaluation of models by different metrics, we use statistical simulation
to create a series of data sets with various prevalence levels. The results below show
how change in prevalence level affects TPR and TNR, which in turn affect different
model evaluation metrics. The sample size n is kept constant and thus, we are able to
separate out the influence of factors associated with sample size.
2.4 Simulation Study
The following simulation is done with the Broward County data set [15, 16] used
above. The original data set has 2775 positive cases and 3439 negative cases, which is
equal to a prevalence level of about 0.452 2.
Tosimulatethedynamicsassociatedwithchangingprevalencelevels,weapplyup-
sampling and down-sampling at the same time but for different classes of the data.
First, we iteratively drop 30 randomly sampled positive cases while simultaneously
adding 30 randomly sampled negative cases, thus incrementally reducing prevalence
level while keeping the sample size n constant. In total, we run 76 such iterations,
which along with the original data sample give us 77 instances for each performance
metricandeachmodel.The77thdatasamplehas495positivecasesand5719negative
cases,equalingaprevalencelevelofabout0.08.Westopatthisiterationsincethetrue
positive rate decreases to almost 0 while the true negative rate increases to almost 1.
Next, we do the opposite. From the original data sample, we iteratively drop 30
randomly sampled negative cases while simultaneously adding 30 randomly sampled
positivecases,thusincrementallyincreasingprevalencelevelwhilekeepingthesample
size n constant. In this second phase, we run 79 iterations and stop when the true
positive rate increases to almost 1 while the true negative rate decreases to almost
0. The 79th data sample has 5145 positive cases and 1069 negative cases, equaling a
prevalence level of about 0.83.
Asunitsarerandomlydroppedfromoraddedtothepositiveandnegativeclasses,
thesimulationstepsdescribedaboveareabletokeeptherelationshipsamongthepre-
dictors and between the predictors and the outcome largely unchanged. In fact, the
bivariate correlations among all 8 variables have only minimal changes. The correla-
tion heat maps presented in the Supplementary Information showcase the bivariate
correlations among all the variables for the original data as well as 79 simulated data
samples(everyotheronesselectedfromallthedatasamples).Itcanbeobservedthat
the correlations among all the variables largely stay the same across all the simulated
2Here,prevalencelevelistheprevalenceofthesampledataasawhole.However,alltheabovedefinitions
of evaluation metrics in 2.1 refer to prevalence level of the test set. In this study, as the test sets are
randomlysampledfromthesampledata,theprevalencelevelofthetestsetsarethesameasthesample
data(differencesarenegligible).Prevalencelevelsofthesimulationresultspresentedbelowareforthetest
sets.
7Fig. 1 PrevalenceLevelandModelEvaluationofDifferentMetrics
GLM Random Forest KNN GLM Random Forest KNN
1.00 1.00 1.00 1.0
0.75 0.75 0.75 0.8 0.8 0.8 0.50 0.50 0.50 0.6 0.6
0.25 0.25 0.25 0.4 0.6 0.4
0.00 0.00 0.00 0.4 0.2
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence Prevalence Prevalence
LDA GBM randomguess LDA GBM randomguess
1.00 1.00
0.75 0.75 0.75 0.8 0.8 0.75
0.50 0.50 0.50 0.6 0.6 0.50
0.25 0.25 0.25 0.4 0.25
0.00 0.00 0.00 0.2 0.4 0.00
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence Prevalence Prevalence
TPR TNR PPV NPV
GLM Random Forest KNN GLM Random Forest KNN
0.75 0.75 0.75 0.75 0.75 0.75
0.50 0.50 0.50 0.50 0.50 0.50
0.25 0.25 0.25 0.25 0.25 0.25
0.00 0.00 0.00 0.00 0.00 0.00
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence Prevalence Prevalence
LDA GBM randomguess LDA GBM randomguess
0.8
0.75 0.75 0.75 0.75 0.75 0.6
0.50 0.50 0.50 0.50 0.50 0.4
0.25 0.25 0.25 0.25 0.25 0.2
0.00 0.00 0.00 0.00 0.00 0.0
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence Prevalence Prevalence
Accuracy BA BI F1_score F_beta_score_0.5 F_beta_score_2
GLM Random Forest KNN GLM Random Forest KNN
0.8
0.75 0.75 0.75 0.6 0.6 0.6
0.50 0.50 0.50 0.4 0.4 0.4
0.25 0.25 0.25 0.2 0.2 0.2
0.00 0.00 0.00 0.0 0.0
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence Prevalence Prevalence
LDA GBM randomguess LDA GBM randomguess
0.8 0.8
0.75 0.75 0.6 0.6 0.6 0.4
0.50 0.50 0.4 0.4 0.4 0.2
0.25 0.25 0.2 0.2 0.2
0.00 0.00 0.0 0.0 0.0
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence Prevalence Prevalence
Cohens_kappa Fowlkes_Mallows_index Jaccardindex Gmean AUC MCC
GLM Random Forest KNN GLM Random Forest KNN
0.7
00 .. 56 0.8 00 .. 45 20 30 10 0.4 0.6 0.3 20
00 .. 23 0.4 0.2 10 10 5
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence Prevalence Prevalence
LDA GBM randomguess LDA GBM randomguess
0.5 0.04 10 30
0.6
0.4 0.5 0.02 8 20 1.1
0.3 0.4 0.00 6 0.9
0.2 0.3 −0.02 4 10 0.7
0.1 2
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence Prevalence Prevalence
Markedness Diagnostic_odds_ratio
8
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulav
eulavdata samples (The other 76 simulated data samples have been verified to show the
same pattern).
3 Results
3.1 Simulation Results: Evaluation Metrics
Figure 1 shows the relationships between changing prevalence and various model
evaluation metrics. The curves are LOESS-smoothed ones.
First, we examine the behavior of true positive rate, true negative rate, positive
predictive value and negative predictive value. It can be observed that across the six
different models, decreasing prevalence level corresponds with decreasing level of true
positive rate and increasing level of true negative rate. And the two rates intersect
at around the prevalence level of 0.5. Similarly, in general, positive predictive value
decreases and negative predictive value increases as prevalence level decreases. How-
ever, for all models except the random guess model, there are significant fluctuations
in PPV when prevalence is low, while NPV shows most fluctuations when prevalence
is high. The two curves for PPV and NPV again intersect at around the 0.5 preva-
lence level, indicating that 0.5 is a critical point for PPV and NPV just as it is for
TPR and TNR.
Mathematically, from the alternative expressions of the evaluation metrics
in section 2.2, we know that PPV = TPR·ϕ = 1 .
TPR·ϕ+(1−TNR)·(1−ϕ) 1+1−TNR·1−ϕ
TPR ϕ
As rate of change in TPR is similar to the rate of change in TNR and the
rate of change in prevalence across the whole range of prevalence, and (1 −
TNR) increases, 1 increases,1−ϕ increases, therefore, we know that as prevalence
TPR ϕ
decreases, 1+ 1−TNR · 1−ϕ increases, and PPV decreases.
TPR ϕ
Similarly, NPV = TNR·(1−ϕ) = 1 , and as prevalence
TNR·(1−ϕ)+(1−TPR)·ϕ 1+1−TPR· ϕ
TNR 1−ϕ
decreases, (1−TPR)increases, 1 decreases, ϕ decreases, and 1−TPR ϕ decreases,
TNR 1−ϕ TNR 1−ϕ
and NPVincreases. The observed patterns of PPV and NPV in Figure 1 largely
correspond with their mathematically expected behavior 3
It can be observed that accuracy has a convex curve with respect to prevalence,
that is accuracy has lower values when prevalence is around 0.5 while it has higher
values when prevalence goes to either the upper or the lower extremes. Above the 0.5
prevalence level, accuracy is dominated by the positive class and decreasing level of
true positive rate while below 0.5, accuracy is dominated by the negative class and
increasingleveloftruenegativerate.Therefore,accuracyisdominatedbythemajority
class. Even the random guess model shows this behavior.
3Noticeably for changing prevalence, the values of TPR, TNR are more stable than those of PPV and
NPVespeciallyatextremelevelsofprevalence.Fromcorrespondingexpressionsinsection 2.1,weknowthat
denominators of both TPR and TNR are known before predictions are made, therefore, only numerators
ofTPRandTNRarechangingasaresultofchangeinprevalence.ForPPVandNPV,bothdenominator
andnumeratorareonlyknownafterpredictionsaremade.Therefore,bothdenominatorandnumeratorare
changingasaresultofchangeinprevalenceforPPVandNPV.Inotherwords,PPVandNPVvarymore
becauseofaparticularpredictiontaskwhileTPRandTNRvarylessandareproportionatelymoreaffected
bytheindependentinfluenceofprevalence,afactnotoftenemphasizedintheexistingliterature[7].
9On the contrary, both balanced accuracy and bookmaker informedness show a
concavecurvewithrespecttoprevalence.Thatisbothmetricshavehighervalueswhen
class is more balanced and have lower values when class is very imbalanced. This can
be understood from their expression as some form of summation of true positive rate
andtruenegativerate.Therandomguessmodelisconstantaround0.5,themedianof
the range for balanced accuracy and it is constant around 0, the median of the range
forbookmakerinformedness,indicatingtheamountofpredictionaccuracybyrandom
chance.
Values of the three F scores show a similar downward trend as prevalence goes
down.F1scoreshowsthehighestlevelofsteepness,F scorewithβ =2next,andF
β β
scorewithβ=0.5showstheleaststeepness.Apparently,Fscoreshaveamonotonically
increasing relationship with prevalence, which is verified by a simple mathematical
derivation for F1 score shown in Appendix A.
Similarly, both Falkes-Mallows index and Jaccard index show a monotonically
increasing relationship with prevalence. From the corresponding expressions of these
metrics in section 2.2, we know that both metrics are close to PPV in form, there-
fore, it is not surprising that both metrics show a similarly downtown trend as PPV
though with more steepness.
On the other hand, Cohen’s Kappa shows an concave curve with respect to preva-
lence. That is it has higher values when class is more balanced and lower values when
class is more imbalanced. From the corresponding expression in section 2.1, we know
Accuracy −1
that Kappa= expAccuracy , and expAccuracy is dominated by decreasing TPR when
1 −1
expAccuarcy
prevalenceisabove0.5andbyincreasingTNRwhenprevalenceisbelow0.5,thuslead-
ing to the observed pattern for Kappa. The random guess model is constant around
0, the median of the range for Cohen’s Kappa indicating the amount of prediction
accuracy by random chance.
Similarly, the geometric mean shows a concave curve with respect to preva-
√
lence, which can be understood from its expression TPR×TNR. That is it values
both high true positive and high true negative rate at the same time. The same
applies to Matthew’s correlation coefficient, which is essentially the Pearson’s corre-
lation coefficient for two binary variables. And to have high values, MCC requires
the two sequence of actual and predicted class labels to closely align with each
√
other. And from its alternative expression MCC = PPV×TPR×TNR×NPV−
√
FDR×FNR×FPR×FOR,weknowthatMCCalsotriestoachievebothhighTPR
and high TNR at the same time. The random guess model is constant around 0, the
median of the range for MCC.
As Area under the ROC Curve (AUC) assesses the performance of a classifier
at all possible classification thresholds, as a result, it cannot be constructed from
a single confusion matrix based on one classification threshold and cannot have an
exact comparison with other confusion matrix evaluation metrics calculated with the
0.5 probability cutoff. However, AUC values show a most obvious pattern in Figure
1. AUC values are mostly flat regardless of change in prevalence except for minor
fluctuations at extremely low prevalence.
Markedness does not show an obvious pattern with respect to prevalence across
thedifferentmodels.AsitisaformofsumofPPVandNPV,andbothPPVandNPV
10have quite fluctuating behavior at extreme levels of prevalence, therefore, it’s not a
surprise to see the amount of fluctuations in its trends including for random guess.
Diagnostic odds ratio shows a somewhat upward trend at low prevalence. How-
ever, for most of the range of prevalence, it has a flat trend, and there is a lot of
fluctuations at low prevalence. This can be understood from its alternative expres-
sion 1 ,wherethedivergingtrendsofTPRandTNRcompetewitheach
(1− 1 )(1− 1 )
TPR TNR
other.
Here is a summary of the above results. As TPR, TNR, PPV and NPV mostly
concern prediction accuracy for one class only, they have a monotonic relationship
with prevalence in either the positive or the negative direction though PPV and NPV
fluctuatemuchmore.Andanumberofmetricsappeartohaveamonotonicallyincreas-
ingrelationshipwithprevalenceincludingF1score,F scores,Fowlkes-Mallowsindex
β
and Jaccardindex. Meanwhile, balanced accuracy, bookmaker informedness, Cohen’s
Kappa, Matthew’s correlation coefficient and geometric mean all have a concave
curve with respect to decreasing prevalence though geometric mean appears to have
a much larger range of values. Accuracy has a convex curve with respect to decreas-
ing prevalence though geometric mean has the most variance in its values. AUC is
prevalence-independentandshowsflattrendsacrossdifferentmodels.Markednessand
diagnostic odds ratio do not have strong patterns with regard to prevalence.
3.2 Simulation Results: Comparisons of Models
Figure 2 shows the results in a different way. Instead of showing multiple different
metrics for a single model as in Figure 1, here we are showing all 6 models evaluated
by one metric in a single visualization. For presentation convenience, we organize the
18 metrics into the upper, middle and lower panes of Figure 2.
Theresultsindicatethatevaluationmetricsthataremoreinfluencedbyprevalence
as shown in Figure 1 also do a poorer job differentiating between different models.
As an example, we cannot visually determine from the three F scores which one
or two models perform better among the 5 trained machine learning models, the
only thing we can say is that these 5 models are better than random guess. This is
also true for Fowlkes Mallows index and Jaccard index. On the contrary, for both
MCC and AUC, it is clear to observe that the random forest model and the Gradient
Boosted Machine perform better than the other three models. This is also true for
balancedaccuracy,bookmakerinformednessandCohen’sKappa.Thegeneralpattern
is that metrics that are having a close to monotonic relationship with prevalence do a
bad job distinguishing among different models while metrics that are relatively more
prevalence-independent do a better job.
3.3 Simulation Results: Ranking of Different Models
Hereweexplicitlyshowhowdifferentmetricsrankthe6differentmodelsacrossdiffer-
entprevalencelevels,andmorespecificallywelookatthevarianceofrankingsforeach
model. Table 3 presents the variance values while Figure 3 shows the corresponding
actual rankings for different models.
11Fig. 2 ComparisonsofModelsbyDifferentMetrics
1.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00
0.00 0.00
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence
0.5 0.8
0.8 0.4 0.6
0.6 0.3
0.4
0.2
0.4 0.2
0.1
0.2 0.0 0.0
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence
GBM KNN Random Forest
Model
GLM LDA randomguess
0.8
0.75 0.75
0.6
0.50 0.50 0.4
0.25 0.25 0.2
0.00 0.0
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence
0.70 0.4
0.6
0.65 0.3
0.4 0.60 0.2
0.2 0.55 0.1
0.50 0.0
0.0
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence
GBM KNN Random Forest
Model
GLM LDA randomguess
0.4 0.8
0.9
0.3
0.7 0.8
0.2
0.7
0.6
0.1
0.6
0.0 0.5
0.5
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence
0.4 12
0.3 0.75 30
0.2 0.50 20
0.1 0.25 10
0.0 0.00
0
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence
GBM KNN Random Forest
Model
GLM LDA randomguess
RPT
VPN
erocs_1F
naemG
CCM
appak_snehoC
RNT
5.0_erocs_ateb_F
swollaM_seklwoF
AB
CUA
ssendekraM
VPP
2_erocs_ateb_F
xednidraccaJ
IB
ycaruccA
oitar_sddo_gaiDTable 3 VarianceofRankingsofModelsbyDifferentMetrics
Metric GBM GLM KNN LDA Random Forest randomguess
True positives 1.343 3.389 1.776 3.730 0.980 3.667
False negatives 1.343 3.389 1.776 3.730 0.980 3.667
TPR 1.343 3.389 1.776 3.730 0.980 3.667
F beta score 2 1.321 3.275 1.913 3.489 1.196 3.223
F beta score 0.5 1.404 2.458 1.780 2.425 1.649 2.063
True negatives 1.360 2.639 1.242 3.221 1.169 2.024
False positives 1.360 2.639 1.242 3.221 1.169 2.024
TNR 1.360 2.639 1.242 3.221 1.169 2.024
F1 score 1.263 2.206 1.798 1.966 1.768 1.653
Jaccardindex 1.263 2.206 1.798 1.966 1.768 1.653
Gmean 1.394 1.207 1.365 1.103 1.814 3.188
Fowlkes Mallows 1.354 1.598 1.502 1.908 1.287 0.291
NPV 1.028 1.404 1.714 1.591 1.294 0
PPV 1.183 1.244 1.402 1.804 0.745 0.057
BA 0.755 0.604 1.310 0.987 0.944 0.026
BI 0.755 0.604 1.310 0.987 0.944 0.026
Cohens kappa 0.715 0.664 1.341 0.962 0.886 0.006
Markedness 1.036 0.796 0.830 1.031 0.780 0.026
Diag odds ratio 0.912 0.794 0.826 0.995 0.819 0.026
MCC 0.588 0.676 1.251 0.810 0.714 0.026
Accuracy 0.699 0.534 0.933 0.691 0.573 0
AUC 0.271 0.419 0.112 0.309 0.412 0
Relating to results in section 3.1, metrics that have a close to monotone rela-
tionship with prevalence show least consistency in ranking of different models. Their
variances shown in Table 3 are larger compared with metrics that are less influ-
enced by prevalence. The first group of metrics include TPR, TNR, PPV, NPV, the
three F scores, Fowlkes-Mallows index, Jaccard index and geometric mean. The sec-
ond group of metrics include BA, BI, Kappa, MCC, AUC, Markedness and DOR.
Variances for the first group of metrics are almost always above 1, sometimes above
2 or 3 while variances for the second group of metrics are mostly below 1. Most
notably,AUChasthesmallestvarianceinitsrankingofdifferentmodelsasitismostly
prevalence-independent. MCC for a majority of the models is the second most con-
sistent evaluator. Figure 3 shows same patterns for the actual model rankings. The
geometric mean does not have a close to monotone relationship with prevalence in
Figure 1. However, it varies a lot with prevalence and has relatively large variance in
ranking of different models.
4 Discussion and Conclusion
The analysis presented above demonstrates that for binary classification tasks, evalu-
ationmetricsthataremoreinfluencedbyprevalencelevelorhaveaclosetomonotone
relationshipwithprevalenceinitsevaluationofindividualmodelsalsohappentohave
morevarianceinitsrankingofasetofmultipledifferentmodels.Inotherwords,met-
rics that are less influenced by prevalence offer more consistent comparison between
13Fig. 3 RankofModelsbyDifferentMetrics
GLM Random Forest GLM Random Forest
6 6 6
6
4 4 4 4
2
2 2 2
0
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence
TPR PPV F1_score Jaccardindex F_beta_score_2 Accuracy BI Gmean Diag_odds_ratio AUC
Metric Metric
TNR NPV Fowlkes_Mallows F_beta_score_0.5 BA MCC Markedness Cohens_kappa
KNN LDA KNN LDA
6 6 6
6
4 4 4 4
2 2 2 2
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence
TPR PPV F1_score Jaccardindex F_beta_score_2 Accuracy BI Gmean Diag_odds_ratio AUC
Metric Metric
TNR NPV Fowlkes_Mallows F_beta_score_0.5 BA MCC Markedness Cohens_kappa
GBM randomguess GBM randomguess
6 6 6 6
4 4 4 4
2 2 2 2
0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Prevalence Prevalence Prevalence Prevalence
TPR PPV F1_score Jaccardindex F_beta_score_2 Accuracy BI Gmean Diag_odds_ratio AUC
Metric Metric
TNR NPV Fowlkes_Mallows F_beta_score_0.5 BA MCC Markedness Cohens_kappa
different models. This is not a trivial result as we are moving from evaluation of
individual models to ranking of multiple different models 4
The simulation above covers a distribution of data scenarios in terms of how bal-
ancedthedatais,fromanoverwhelmingpositivesampletoanoverwhelmingnegative
sampleandeverythinginbetween.Andacrossthisfullspanofdatascenariosandfive
commonly used machine learning models as well as a random guess model, we obtain
similarpatternsonhow18differentmetricsevaluatemodels.Inparticular,Areaunder
theROCCurveoffersthemostconsistentmodelevaluationandMatthew’scorrelation
4Of course, the particular type of model being trained also has an influence, overall the GBM and
RandomforestmodelsappeartohavelessvarianceintheirrankingscomparedwithGLM,KNNorLDA.
Asexpected,therandomguessmodelismostfrequentlybeingrankedastheleastperformingmodeland
evaluationsofmanymetricsoverlaponthelineofrank6inFigure 3.
14
knar
knar
knar
knar
knar
knar
knar
knar
knar
knar
knar
knarcoefficient is second best. This is not a result by definition. The predicted probabil-
ities for the positive and negative classes are influenced by prevalence. However, the
fact that AUC evaluates model under all possible threshold values effectively offsets
the influence of changing prevalence levels on model evaluation.
If the goal of model evaluation is to obtain consistent ranking of different models
across different data scenarios, then AUC should be the metric to be used. If the goal
of model evaluation is to assess the overall correlation between predicted class labels
andthecorrespondingactualclasslabelsforallobservations,thenMCCisthemetric
to be used. And if the goal is to assess the overall alignment in probability between
predicted class labels and the corresponding actual class labels, then accuracy is the
measure to be used.
In this study we only consider binary classification tasks, similar analysis for data
setswithmulti-classoutcomeisoneworthwhileavenueforfutureresearch.Inaddition,
as the four basic quantities of sample size n, prevalence, TPR, and TNR define all
confusionmatrixevaluationmetrics,developingnovelclassificationmeasuresthattake
these four quantities into account is also a viable path to deepen our understanding
on this research topic.
Supplementary information
The Supplementary Information provide correlation heat maps that showcase the
bivariatecorrelationsamongthepredictorsandtheoutcomefortheoriginaldatasam-
ple as well as 79 simulated data samples (every other ones selected from all the data
samples). Simulation code used for this study is also included.
Statements and Declarations
• There is no funding source for this study.
• There is no potential conflict of interests.
• Data used for this study is publicly available.
• Code used for this study will be made publicly available.
Appendices
A F1 score as a result of prevalence change when
only sample size n is constant
From the alternative definition of F1 score above, we know that
2
F1 score=
2+ 1 ((1−TNR)1−ϕ +1−TPR)
TPR ϕ
,
inaddition,ϕdecreases,TPRdecreases,TNRincreases ⇒ 1 increases,(1−
TPR
TNR) decreases, 1−ϕ increases, and (1−TPR) increases.
ϕ
15Across the whole spectrum of prevalence, the rate of change in TPR is similar
to the rate of change in both TNR and prevalence itself, and as only (1 − TNR)
goes down while all other terms in the denominator of F1 score goes up, we have
that ((1−TNR)1−ϕ +1−TPR) increases and F1 score decreases as prevalence level
ϕ
decreases.
B Prediction Results for Crime Recidivism
Table 4 CrimeRecidivismModelEvaluationMetricValues
Metric GBM GLM KNN LDA RandomForest randomguess
True positives 339 292 296 286 311 267
False negatives 223 270 266 276 251 295
True negatives 517 537 521 543 543 386
False positives 164 144 160 138 138 295
TPR 0.603 0.520 0.527 0.509 0.553 0.475
TNR 0.759 0.789 0.765 0.797 0.797 0.567
PPV 0.674 0.670 0.649 0.675 0.693 0.475
NPV 0.699 0.665 0.662 0.663 0.684 0.567
Accuracy 0.689 0.667 0.657 0.667 0.687 0.525
BA 0.681 0.654 0.646 0.653 0.675 0.521
BI 0.362 0.308 0.292 0.306 0.351 0.042
F1 score 0.637 0.585 0.582 0.580 0.615 0.475
MCC 0.367 0.321 0.301 0.322 0.363 0.042
Gmean 0.677 0.640 0.635 0.637 0.664 0.519
Fowlkes Mallows 0.638 0.590 0.585 0.586 0.619 0.475
Markedness 0.373 0.335 0.311 0.338 0.377 0.042
Diag odds ratio 4.792 4.033 3.623 4.077 4.875 1.184
Jaccardindex 0.467 0.414 0.410 0.409 0.444 0.312
Cohens kappa 0.366 0.314 0.297 0.313 0.357 0.042
F beta score 0.5 0.352 0.321 0.319 0.317 0.338 0.264
F beta score 2 0.512 0.450 0.453 0.442 0.477 0.396
AUC 0.734 0.718 0.694 0.717 0.727 0.524
16References
[1] Garc´ıaV.,S.S.J.,R.,A.M.:Onthesuitabilityofnumericalperformancemeasures
for class imbalance problems. Proceedings of the 1st International Conference on
PatternRecognitionApplicationsandMethods,Algarve,Portugal,2012,310–313
(2012) https://doi.org/10.5220/0003783303100313
[2] Lever,J.,Krzywinski,M.,Altman,N.:Classificationevaluation.NatureMethods
13(8), 603–604 (2016) https://doi.org/10.1038/nmeth.3945
[3] Luque,A.,Carrasco,A.,Mart´ın,A.,DeLasHeras,A.:Theimpactofclassimbal-
ance in classification performance metrics based on the binary confusion matrix.
Pattern Recognition 91, 216–231 (2019) https://doi.org/10.1016/j.patcog.2019.
02.023
[4] Jadhav, A.S.: A novel weighted TPR-TNR measure to assess performance of the
classifiers.ExpertSystemswithApplications152,113391(2020)https://doi.org/
10.1016/j.eswa.2020.113391
[5] Zhu,Q.:Ontheperformanceofmatthewscorrelationcoefficient(MCC)forimbal-
anced dataset. Pattern Recognition Letters 136, 71–80 (2020) https://doi.org/
10.1016/j.patrec.2020.03.030
[6] Chicco, D., Jurman, G.: The advantages of the matthews correlation coeffi-
cient (MCC) over f1 score and accuracy in binary classification evaluation. BMC
Genomics 21(1), 6 (2020) https://doi.org/10.1186/s12864-019-6413-7
[7] Chicco,D.,T¨otsch,N.,Jurman,G.:Thematthewscorrelationcoefficient(MCC)
is more reliable than balanced accuracy, bookmaker informedness, and marked-
ness in two-class confusion matrix evaluation. BioData Mining 14(1), 13 (2021)
https://doi.org/10.1186/s13040-021-00244-z
[8] De Diego, I.M., Redondo, A.R., Fern´andez, R.R., Navarro, J., Moguerza, J.M.:
Generalperformancescoreforclassificationproblems.ApplIntell52(10),12049–
12063 (2022) https://doi.org/10.1007/s10489-021-03041-7
[9] Hicks,S.A.,Stru¨mke,I.,Thambawita,V.,Hammou,M.,Riegler,M.A.,Halvorsen,
P., Parasa, S.: On evaluation metrics for medical applications of artificial intelli-
gence. Sci Rep 12(1), 5979 (2022) https://doi.org/10.1038/s41598-022-09954-8
[10] Lavazza, L., Morasca, S.: Common problems with the usage of f-measure and
accuracymetricsinmedicalresearch.IEEEAccess11,51515–51526(2023)https:
//doi.org/10.1109/ACCESS.2023.3278996
[11] Chicco, D., Jurman, G.: The matthews correlation coefficient (MCC) should
replace the ROC AUC as the standard metric for assessing binary classification.
BioData Mining 16(1) (2023) https://doi.org/10.1186/s13040-023-00322-4
17[12] Ozenne, B., Subtil, F., Maucort-Boulch, D.: The precision–recall curve overcame
the optimism of the receiver operating characteristic curve in rare diseases. Jour-
nal of Clinical Epidemiology 68(8), 855–859 (2015) https://doi.org/10.1016/j.
jclinepi.2015.02.010
[13] Yuan,Y.,Su,W.,Zhu,M.:Threshold-freemeasuresforassessingtheperformance
of medical screening tests. Frontiers in Public Health 3 (2015) https://doi.org/
10.3389/fpubh.2015.00057
[14] Kruschke, J.: Bayes’ Rule: Doing Bayesian Data Analysis., pp. 99–120. Elsevier,
Amsterdam (2015)
[15] Dressel, J., Farid, H.: The accuracy, fairness, and limits of predicting recidivism.
Sci. Adv. 4(1), 5580 (2018) https://doi.org/10.1126/sciadv.aao5580
[16] Bansak, K.: Can nonexperts really emulate statistical learning methods? a com-
menton“theaccuracy,fairness,andlimitsofpredictingrecidivism”.Polit.Anal.
27(3), 370–380 (2019) https://doi.org/10.1017/pan.2018.55
18