Synthesis of Reward Machines
for Multi-Agent Equilibrium Design
∗
(Full Version)
Muhammad Najib1 and Giuseppe Perelli2
1Heriot-Watt University
2Sapienza University of Rome
Abstract
Mechanism design is a well-established game-theoretic paradigm for
designing games to achieve desired outcomes. This paper addresses a
closely related but distinct concept, equilibrium design. Unlike mecha-
nism design, the designer’s authority in equilibrium design is more con-
strained; she can only modify the incentive structures in a given game
to achieve certain outcomes without the ability to create the game from
scratch. Westudytheproblemofequilibriumdesignusingdynamicincen-
tive structures, known as reward machines. We use weighted concurrent
game structures for the game model, with goals (for the players and the
designer) defined as mean-payoff objectives. We show how reward ma-
chines can be used to represent dynamic incentives that allocate rewards
inamannerthatoptimisesthedesigner’sgoal. Wealsointroducethemain
decisionproblemwithinourframework,thepayoffimprovementproblem.
This problem essentially asks whether there exists a dynamic incentive
(represented by some reward machine) that can improve the designer’s
payoff by more than a given threshold value. We present two variants of
the problem: strong and weak. We demonstrate that both can be solved
in polynomial time using a Turing machine equipped with an NP oracle.
Furthermore, we also establish that these variants are either NP-hard or
coNP-hard. Finally,weshowhowtosynthesisethecorrespondingreward
machine if it exists.
1 Introduction
Overthepastdecade,Nashequilibrium(NE)andothergame-theoreticconcepts
have been extensively used to analyse concurrent and multi-agent systems (see
∗A conference version of this work will appear in the proceedings of the 27th European
ConferenceonArtificialIntelligence(ECAI’24).
1
4202
guA
91
]TG.sc[
1v47001.8042:viXrae.g., [10, 4, 31, 11]). In this research, systems are modelled as games with
agents acting rationally to fulfil their preferences. While preferences are often
expressedqualitatively(e.g. bytemporallogicformulae), manysystemsrequire
morecomplexmodelstocapturequantitativeaspectslikeresourceconsumption,
cost, or performance [12, 3, 2, 9]. Games with mean-payoff objectives [32]
provide such richer preference modelling.
The game-theoretical analysis of mean-payoff games (MPGs) is a significant
research area, especially in verifying their correctness [28, 5, 6, 26, 13, 7]. This
involvescheckingwhetheraformalpropertyissatisfiedinsomeorallequilibrium
outcomes. A pertinent question is: “what if the property is not satisfied in
any equilibrium outcomes?” Equilibrium design [14] addresses this question.
Inspired by the mechanism design paradigm [20, 16], equilibrium design offers
a way to rectify equilibrium outcomes. However, unlike mechanism design, the
designer in equilibrium design cannot create the game from scratch, but can
only modify the incentive structures of an existing game.
In [14], the authors proposed subsidy schemes to introduce equilibria in
concurrent MPGs satisfying some LTL formula [25]. In that setting, a subsidy
schemeismodelledbyafunctionmappingfromstatesandplayerstoadditional
rewards. If a player visits a certain state, the corresponding reward is paid
to the player. In this paper, we generalise this incentive model with reward
machines [18]. Such machines implement a reward mechanism that considers
the execution history to dynamically assign rewards. Thus, at each iteration of
the game, every agent receives a utility combining the original weight and an
authoritative reward based on the current game state and the internal reward
machinestate. Aswewillshowlater(seeExample1),thisgeneralisationallows
us to obtain a more expressive model of incentive.
We consider games where each agent has a weight function over the states,
with mean-payoff aggregation as their utility function. Additionally, a global
weight function measures the designer satisfaction, also as a mean-payoff value
over executions. We employ reward machines to improve the designer satisfac-
tion. Intuitively, these machines reconfigure weights after each iteration, thus
reshaping the set of equilibria. The objective is to improve the global payoff
overthesetofequilibriabyafixedamount∆. Thiscanbeachievedstrategically
bysynthesisingandimplementinganappropriaterewardmachine. Tomakethe
setting realistic, we assume the reward spent on each agent in every iteration is
subtracted from the global weight, factoring the cost into the resulting global
payoff.
In MPGs, infinite memory may be required to achieve optimal values [30].
As we will demonstrate later, infinite memory may be necessary to achieve the
optimal global payoff value. However, since a reward machine is typically rep-
resented by a finite-state machine (specifically, a Mealy machine in this work),
theremaybecaseswherenofinite-staterewardmachinecanimprovetheglobal
payoff by a given ∆. Therefore, we consider an approximate solution. In par-
ticular, we aim to find a reward machine that can improve the global payoff by
a value ε-close to ∆ for a given ε > 0. Moreover, ε can be arbitrarily small,
allowing for an arbitrary level of precision.
2In general, a game may have multiple equilibria. Therefore, we study the
problem under both the optimistic and pessimistic settings. Specifically, we
consider the problem of improving the global mean-payoff over the best possi-
ble NE by adopting an optimistic view that agents will select the equilibrium
most convenient for the designer. We call this the weak improvement prob-
lem. Conversely, we also consider improving the global mean-payoff over the
worst possible NE, considering the pessimistic case when agents select the least
convenient equilibrium for the designer. We call this the strong improvement
problem. Furthermore, we classify the complexity of these problems. We show
that both can be solved in PNP = ∆P and are at least NP-hard or coNP-hard.
2
To our knowledge, this is the first work that employs reward machines in the
context of MPGs and game-theoretic equilibria.
Related work As previously mentioned, this work is closely related to [14],
butitdiffersinseveralkeyaspects. Firstly, ourincentivemodelismoreexpres-
sive due to the use of reward machines. Furthermore, we measure the global
propertyusingaquantitativemetric(i.e. mean-payoffvalue),asopposedtothe
qualitative property in [14]. In this respect, we provide a richer modelling of
global preferences. Equilibrium design has a deep connection to mechanism de-
sign, but the two are not exactly the same. Typically in mechanism design, the
designer is not given a predetermined game structure, but instead is required
to provide one. Moreover, in mechanism design, the designer must ensure the
reward structure is incentive compatible, which is not the case in equilibrium
design, as the designer is primarily interested in the global payoff, unlike in
mechanism design where the designer is primarily interested in the agents’ pay-
offs.
On the other hand, the concept of a reward machine originated from the
field of reinforcement learning (RL). Much of the existing work is within the
domainofsingle-agentRL[17,27,18]. In[21],theauthorsexploredrewardma-
chines for multi-agent RL systems. However, in this work, the reward machine
ismanuallygenerated,asopposedtobeingautomaticallysynthesised. [29]tack-
les the problem of automatically synthesising reward machines in cooperative
multi-agent RL. Specifically, the reward machines are partly synthesised from
Alternating-time Temporal Logic (ATL) specifications. However, this line of
research focuses on RL systems, which differ from MPGs. Moreover, none of
these papers consider any game-theoretical solution concepts.
Anotherrelatedlineofworkinvolvesdesigningequilibriausingnorms. Norm-
based mechanism design has been studied in [8]. In particular, they studied
weak and strong implementability, which are related to the problems addressed
inourworkinthesensethattheycorrespondtooptimistic(“there is some good
behaviour”) and pessimistic (“all behaviours must be good”) assumptions. In
[15, 24, 1], automata-based norms, referred to as dynamic norms, are consid-
ered. All of these works fall within the domain of normative systems, which is
differentfromthesettingconsideredinthispaper. Webelievethatanincentive-
based equilibrium design provides a complementary approach to norm-based
3equilibrium design. This is because in some circumstances, a norm may not be
enforceable, but only incentives are possible (e.g., congestion/road pricing in
the Ultra Low Emission Zone (ULEZ) in London).
2 Preliminaries
In this section we introduce the basic notions that will be used throughout
the paper. We start with the definition of mean-payoff value and multi-player
mean-payoff games.
Mean-Payoff For an infinite sequence r ∈ Rω, let mp(r) be the mean-payoff
valueofr,thatis,mp(r)=liminf avg (r)where,forn∈N\{0},wedefine
n→∞ n
avg (r)= 1 (cid:80)n−1r , with r the (j+1)th element of r.
n n j=0 j j
Multi-Player Mean-Payoff Game A multi-player mean-payoff game is a
tuple G =⟨N,Ac,St,s ,(d ) ,tr,(w ) ,w ⟩ where
in i i∈N i i∈N g
• N = {1,...,n}, Ac, and St are finite non-empty sets of players, actions,
and states, respectively;
• s ∈St is the initial state;
in
• d : St → 2Ac\{∅} is a protocol function for player i returning possible
i
action at a given state;
• tr : St×A⃗c → St is a transition function mapping each pair consisting
of a state s ∈ St and an action profile ⃗a = (a ,...,a ) ∈ A⃗c = Acn,
1 n
one for each player, to a successor state—we write ⃗a for ⃗a and ⃗a
i {i} −i
for ⃗a . For two decisions ⃗a and ⃗a′, we write (⃗a ,⃗a′ ) to denote the
N\{i} C −C
decision where the actions for players in C ⊆N are taken from⃗a and the
actions for players in N\C are taken from⃗a′;
• w : St → Z is player i’s weight function mapping, for every player i and
i
every state of the game into an integer number; and
• w : St → Z is a global weight function mapping every state of the game
g
into an integer number.
We define the minimum and maximum weights appearing in G as follows.
Definition 1. For a given game G and its set of states St, define MinWG =
j
min{w (s)|s∈St} and MaxWG =max{w (s)|s∈St}.
j j j
A path is an infinite sequence π = s ,s ,s ,... ∈ Stω such that for each
0 1 2
k ∈ N, there is an action profile (in k-th step) ⃗ak ∈ (cid:81) d (s ), such that
i∈N i k
s = tr(s ,⃗ak). We write π to denote the prefix of π up to and including
k+1 k ≤k
s . Similarly, π denotes the suffix of π starting from s . Let Paths (s) be
k ≥k k G
the set of all possible paths in G starting from s.
4Astrategy foragentiisaMealymachineσ =(T ,t0,St,γ ,ρ ),whereT isa
i i i i i i
finiteandnon-emptysetofinternalstates,t0 istheinitialstate,γ :St×T →T
i i i i
is a deterministic internal transition function, and ρ :St×T →Ac an action
i i i
function. We say that a strategy σ is valid with respect to G if and only if
i
ρ (s,t )∈d (s). Fromnowon,werestrictourattentiontovalidstrategies,and,
i j i
unless otherwise stated, refer to them simply as strategies. We denote Str (G)
i
the set of valid strategies for player i in G. Moreover, for a given strategy σ
i
and a finite sequence πˆ ∈ St∗, by σ (πˆ) ∈ Ac we denote the action prescribed
i
by the action function ρ of σ after the sequence πˆ has been fed to the internal
i i
transition function γ . Note that the model of strategies implies that strategies
i
have perfect information and finite memory, although we impose no bounds on
memory size.
A strategy profile ⃗σ = (σ ,...,σ ) is a vector of strategies, one for each
1 n
player. As with actions, ⃗σ denotes the strategy assigned to player i in profile
i
⃗σ. Moreover, by (⃗σ ,⃗σ′ ) we denote the combination of profiles where players
B C
in disjoint B and C are assigned their corresponding strategies in ⃗σ and ⃗σ′,
respectively. We denote Str (G) the set of strategy profiles for the set A of
A
agents. We also use Str(G) = Str (G) to denote the strategy profiles for all
N
the agents in the game. Whenever the game is clear from the context, we also
simplyuseStr. Onceastatesandprofile⃗σ arefixed,thegamehasanoutcome,
a path in G, denoted by π(⃗σ,s). Because strategies are deterministic, π(⃗σ,s) is
the unique path induced by ⃗σ, that is, the sequence s ,s ,s ,... such that
0 1 2
• s =tr(s ,(ρ (s ,tk),...,ρ (s ,tk))), and
k+1 k 1 k 1 n k n
• tk+1 =γ (sk,tk), for all k ≥0.
i i i i
For a subset of agents C ⊆ N and strategies ⃗σ , we say that a path π is
C
compatible with ⃗σ if, for every k ∈ N, there exists an action profile ⃗ak with
C
ak = σ (π ) for each i ∈ C, such that s = tr(s ,⃗ak). Intuitively, π is
i i ≤k k+1 k
compatible with⃗σ if it can be generated when the agents in C play according
C
totheirrespectivestrategies. Wedenotebyout (s,⃗σ )thesetofpathsstarting
G C
from s and compatible with ⃗σ . Observe that Paths (s) can also be written as
C G
out (s,∅).
G
GivenagameG andastrategyprofile⃗σ,apathπ(⃗σ)induces,foreachplayer
i, an infinite sequence of integers w (π(⃗σ)) = w (s )w (s )···. Similarly, π(⃗σ)
i i in i 1
also induces such a sequence of integers for the global weight function w (·).
g
The payoff of player i in game G is payG(⃗σ) = mp(w (π(⃗σ))), and the global
i i
payoff of G is payG(⃗σ) = mp(w (π(⃗σ))). Whenever the game is clear from the
g g
context, we simply use pay (⃗σ) and pay (⃗σ), respectively.
i g
Nash Equilibrium Using payoff functions, we can define the game-theoretic
concept of Nash equilibrium [22]. For a multi-player game G, a strategy profile
⃗σ is a Nash equilibrium of G if, for every player i and strategy σ′ for player i,
i
we have
pay (⃗σ)≥pay ((⃗σ ,σ′)) .
i i −i i
5Wealsosaythat⃗σ isaj-fixedNashEquilibrium[19]ifpay (⃗σ)≥pay ((⃗σ ,σ′))
i i −i i
for every player i̸=j different from the fixed j.
LetNE(G)andNE (G)bethesetofNashEquilibriaandj-fixedNashEqui-
j
libria of G. We define bestNE(G) = sup {pay (⃗σ)} as the best global
⃗σ∈NE(G) g
payoff over the set of possible outcomes sustained by a Nash Equilibrium in
the game. Equivalently, we define worstNE(G) = inf {pay (⃗σ)} as the
⃗σ∈NE(G) g
worst global payoff over the set of possible outcomes sustained by a Nash Equi-
librium in the game. In the case of NE(G) is empty, in order to make the
values of bestNE(G) and worstNE(G) well defined, we assume that bestNE(G)=
worstNE(G)=MinWG.
g
3 Reward Machines for Equilibrium Design
In this section, we introduce a type of finite state machine, called a reward
machine (RM). A RM takes a path π as input, and outputs a sequence of
vectors⃗v ,⃗v ···∈(Nn)ω that corresponds to the reward granted to the players
0 1
at each step of the path. Formally, a RM is defined as a Mealy machine:
Definition2(RewardMachine). ARMisaMealymachineM=⟨QM,qM,δM,τM⟩,
0
where QM is a finite (non-empty) set of states, qM the initial state, δM :
0
QM×St→QM a deterministic transition function, and τM :QM×St→Nn
arewardfunctionwhereτM(q)=τM(q)(i)istherewardintheformofanatural
i
number k ∈ N imposed on player i if the play visits (s,q) ∈ St×QM. Some-
times, when it is clear from the context, the elements of the RM are denoted
without superscripts.
RewardMachineimplementation. ForagivengameG =⟨N,Ac,St,s ,(d ) ,tr,(w ) ,w ⟩,
in i i∈N i i∈N g
the implementation of M on G is the game
G†M=⟨N,Ac,St×Q,(s ,q ),(dM) ,trM,(wM) ,wM⟩,
in 0 i i∈N i i∈N g
where: (i)dM(s,q)=d (s),foreachagenti∈N;(ii)trM((s,q),⃗a)=(tr(s,⃗a),δ(s,q));
i i
(iii) wM(s,q)=w (s)+τ (s,q); (iv) wM(s,q)=w (s)−||τ(s,q)|| 1.
i i i g g
For a given natural number β ∈N, a β-RM, denoted M , is RM such that
β
||τ(s,q)|| ≤ β for each (s,q) ∈ St×Q. In this paper, we consider a budget β
being fixed and restrict our attention only to β-RMs.
Definition 3 (Global payoff improvement problems). For a given game G, a
budget β, and a threshold ∆. The global payoff weak improvement problem
consists in deciding whether there exists a β-RM M such that:
bestNE(G†M)−bestNE(G)>∆.
The global payoff strong improvement problem consists in deciding whether
there exists a β-RM M such that:
1By||⃗v||=(cid:80) i∈N|vi|wedenotetheclassicManhattandistance.
6l
l
T
M
L
s m t T m
R
M
T
r
r
Figure1: Graphicalrepresentation(left)andgamearena(right)forExample1.
worstNE(G†M)−worstNE(G)>∆.
Henceforth,forsimplicity,wewillusethetermimprovementproblem torefer
to the global payoff improvement problem.
At this point, it is important to note that the optimal values of bestNE and
worstNEmaynotbeachievablewithfinite-statestrategiesandrewardmachines.
Assuch,toguaranteetermination,wecomputetheapproximatevaluesinstead.
Moreover, our approach allows the values to be approximated to an arbitrary
level of precision. We discuss this in detail in Section 5.
Reward Machines vs Subsidy Schemes. As previously mentioned, the
reward model in this paper is a generalisation of the one considered in [14],
which is referred to as a subsidy scheme in that paper. A subsidy scheme
is defined as a function κ : St → Nn. This can be trivially expressed by a
rewardmachineM=(QM,qM,δM,τM)whereQM ={q},qM =q,andforall
0 0
s ∈ St,δM(s,q) = q,τM(s,q) = κ(s). In other words, subsidy schemes belong
to the subclass of “memoryless” reward machines2. However, there are some
cases in which memory is required. To illustrate this, consider the following
simple example.
Example 1. Consider a scenario where a robot is situated in an environment
shown in Figure 1 left, in which there are four locations t,l,r,m. The robot
can move from one location to another and is not allowed to stay in the same
location for two consecutive time steps. There are three doors separating the
locations, and they can only be passed according to their respective arrows. For
instance, the robot can only move from m to t through the middle door, and not
the other direction. Thus, the robot can reach m from t only through l or r.
However, location r is still under maintenance, and it is best to avoid passing
through it. Suppose that the designer wants to incentivise the robot to deliver
goodsfromttominfinitelyoften. WecanmodelthisasagameG withN={1}.
The game graph is shown in Figure 1 right. In each state, the actions available
2We note that the semantics of the budget used here is slightly different to the one used
in [14]. In this work budget can be thought as “capacity” of additional reward in each time
step,whereasin[14]itisthetotal“commitment”ofrewardinthegame.
7to the player correspond to the outgoing edges and their respective labels. Let
t be the initial state, and w (v) = 0 for all v ∈ St = {t,l,m,r}. Moreover,
1
let w (t) = w (r) = 0,w (l) = 1,w (m) = 2; the designer receives reward of 2
g g g g
when the robot visits m from t, furthermore, she receives extra reward of 1 when
the robot uses corridor l. Suppose that β = 1. Observe that worstNE(G) = 0
corresponding to the sequence p(t,r)ω for some finite prefix p. Suppose that we
want to synthesise M such that given ∆ = 1, the strong improvement problem
2
returns a positive answer. That is, worstNE(G†M)−worstNE(G)> 1.
2
A reward machine that satisfies the con-
straint in Example 1 is as follows: only give
rewards of 1 when the robot visits m from q0
l. More formally, the reward machine is t,0 t,0
l,0 t,0 r,0
shown in Figure 2 where τM(q ,m) = 1 and
1 1
τ 1M(q,t) = 0 for all (q,t) ̸= (q 1,m). The set q1
m,1
q3
m,0
q2
of Nash equilibria in (G†M) corresponds to
the sequence p(t,l,m)ω for some finite prefix Figure 2: Reward machine M.
p. Assuch,wehaveworstNE(G†M)= 2. Ob-
3
servethatsuchanincentiverequiresmemory, sinceitneedstorememberwhich
path leading to m is taken by the robot. This is not possible with memoryless
reward machine.
4 Reward Engineering
Inthisandthenextsection,weshowhowtosolveglobalpayoffimprovement
by constructing an auxiliary game that allows to look at the problem as an
equilibrium verification per se. More specifically, such construction regards
reward machines as the strategies of a designated agent in the game, whose
weight function corresponds to the global weights of the original game updated
with the rewards spent on the others at each iteration. First we provide the
definitionofsuchauxiliarygame,whichisinspiredfromtheconstructionsgiven
in [24, 1].
Definition 4. Given a game G and a budget β ∈ N, we define its auxiliary
game G′ =⟨N′,Ac′,St′,s′ ,(d ) ,tr′,(w′) ⟩, where (i) N′ ={0}∪N,Ac′ =
in i i∈N′ i i∈N′
Ac ∪ βn,St′ = St × βn,s′ = (s ,⃗0); (ii) tr′((s,⃗v),(⃗a,⃗v′)) = (tr(s,⃗a),⃗v′);
in in
(iii) d′(s,⃗v) = d (s),i ∈ N; (iv) d′(s,⃗v) = {⃗v : ||⃗v|| ≤ β}; (v) w′(s,⃗v) =
i i 0 i
w (s)+⃗v ; (vi) w′ =w −∥⃗v∥.
i i 0 g
Intuitively, we are adding agent 0 to the original game G, whose actions are
n-dimensional vectors representing the possible rewards assigned to every other
agent. All the other components of the auxiliary game are defined accordingly.
The protocol function remains the same for every original agent, whereas the
one for agent 0 prescribes that the amount of reward distributed to the agents
at each iteration does not exceeds the budget β. The set of states is augmented
8to record the amount of reward received by each agent, which is then reflected
in the corresponding weight function w′. Finally, the global weight function
i
is updated by subtracting the amount of reward established by agent 0 in the
current iteration.
In the next two constructions, we show how to transform a β-RM for G into
a strategy for agent 0 and viceversa.
Construction 1 (RM to Strategy). Given a RM M =⟨Q ,q0 ,δ ,τ ⟩ of
M M M M
G†M, we define the strategy of player 0 in G′ as σ =⟨T ,t0,St′,γ ,ρ ⟩ where
M 0 0 0 0
T =Q ,t0 =q0 , and the internal transition and action functions defined as
0 M 0 M
• γ ((s,⃗v),t)=δ (s,t)
0 M
• ρ ((s,⃗v),t)=τ (s,t)
0 M
for every (s,⃗v)∈St′ and t∈T .
0
Intuitively, the strategy σ uses the same internal states of the RM M,
M
while the transition and action functions of σ are defined by modifying those
M
of M to match with the types required to be considered a strategy for 0 in G′.
Such construction can be reverted by carefully modifying the types, in order
to move from a strategy of agent 0 in G′ to a RM for G, as it is shown in the
following.
Construction 2 (Strategy to RM). Given a strategy σ =⟨T ,t0,St′,γ ,ρ ⟩
0 0 0 0 0
in G′, we define the RM for G as M = ⟨Q ,q0 ,δ ,τ ⟩ where
σ0 Mσ0 Mσ0 Mσ0 Mσ0
Q =T×βn,q0 =(t0,⃗0), and the transition and reward functions defined
asMσ0 Mσ0 0
• δ (s,(t,⃗v))=(γ ((s,⃗v),t),ρ ((s,⃗v),t))
Mσ0 0 0
• τ (s,(t,⃗v))=ρ ((s,⃗v),t)
Mσ0 0
for every s∈St and (t,⃗v)∈Q .
Mσ0
We write π↾St to denote the sequence in Stω obtained from π by projecting
thecomponentinStandτ(π)thesequencein(Zn)ωobtainedfromwM(π),...,wM(π).
1 n
In the following Lemma, we prove that the constructions presented above
correctly translate RMs into strategies and viceversa, meaning that they make
a connection between paths of G†M and outcome of G′ when agent 0 uses the
corresponding strategy and viceversa.
Lemma 1. ForagivenG†ManditsassociatedauxiliarygameG′ thefollowing
hold:
(1) for every path π ∈Paths G†M((s in,q0)), there is a path π′ =(π↾St,τ(π))∈
out ((s ,⃗0),σ ), and wM(π) = w′(π′) for all i ∈ N and wM(π) =
G′ in M i i g
w′(π′);
0
9(2) foreverypathπ′ ∈out ((s ,⃗0),σ ),thereisapathπ =(π′ ,δ (π′))∈
G′ in 0 ↾St Mσ0
P wa ′t (h πs
′G
).†Mσ0((s in,q0)),andw iMσ0(π)=w i′(π′)foralli∈Nandw gMσ0(π)=
0
Proof. We prove the first item only, as the other has a similar proof.
Observe that the path π′ is uniquely identified from π. Moreover, from
the definitions of M and w′, it immediately follows that wM(π) = w′(π′).
σ0 0 g 0
Therefore, we only need to prove that π′ belongs to out ((s ,⃗0),σ ). We do
G′ in M
itbyinductiononk ∈Nbyshowingthateveryprefixπ′ ofπ′ canbeextended
≤k
compatibly with σ .
M
For the base case k = 0, we have that π′ = (s0,⃗0) which is trivially
≤0
extendable to any path in out ((s ,⃗0),σ ).
G′ in M
For the induction case k > 0, assume that π′ is extendable to a path
≤k
in out ((s ,⃗0),σ ). Then, consider π and ⃗a the joint action such that
G′ in M ≤k
π = tr(π ,⃗a), which exists since π ∈ Paths ((s ,q0)). Clearly, it holds
k+1 k G†M in
that π′ = tr′(π′ ,(σ (π′ ,⃗a))), which makes π′ extendable compati-
≥k+1 ≥k M ≥k ≤k+1
bly with σ .
M
Similarly to the correspondence between RMs and agent 0’s strategies, a
connection between strategies of any other agent i in G †M and G′ exists. In
other words, once a RM M and its corresponding strategy σ are fixed, every
M
strategy σ for agent i in G†M can be translated into a strategy σ′.
i i
Construction 3 (G † M to G′). For a game G † M and a strategy σ =
i
⟨T ,t0,γ ,ρ ⟩ in it, we define a strategy σˆ =⟨Tˆ,tˆ0,γˆ,ρˆ⟩ in the correspond-
i i i i i i i i i
ing game G′ as follows:
• Tˆ =T ×QM, and tˆ0 =(t0,qM);
i i i 0
• γˆ : (T × QM) × (St × βn) → (T × QM) such that γˆ((t,q),(s,⃗v)) =
i i
(γ (t,(s,q)),δ (s,q));
i M
• ρˆ : (T × QM) × (St × βn) → (T × QM) such that ρˆ((t,q),(s,⃗v)) =
i i
ρ (t,(s,q)).
i
By θ (σ )=σˆ we denote the strategy for player i in G′ obtained from σ
G†M i i i
by applying the construction above.
Ontheotherhand,onceastrategyσ foragent0inG′andthecorresponding
0
RMM arefixed,thetranslationfromstrategiesforagentiinG′ tostrategies
σ0
in G†M is possible.
σ0
Construction4(G′ toG†M). ForagameG′ andastrategyσˆ =⟨Tˆ,tˆ0,γˆ,ρˆ⟩,
i i i i i
we define a strategy σ =⟨T ,t0,γ ,ρ ⟩, in the corresponding game G †M as
i i i i i
follows:
• Tˆ =T ×QM, and tˆ0 =(t0,qM);
i i i 0
10• γˆ : (T × QM) × (St × βn) → (T × QM) such that γˆ((t,q),(s,⃗v)) =
i i
(γ (t,(s,q)),δ (s,q));
i M
• ρˆ : (T × QM) × (St × βn) → (T × QM) such that ρˆ((t,q),(s,⃗v)) =
i i
ρ (t,(s,q)).
i
By θ (σˆ) = σ we denote the strategy for player i in G†M obtained from
G′ i i
σˆ by applying the construction above.
i
The following two lemma shows that the connection among strategies in
between the games also preserves the payoff of agents.
Lemma 2. For a given game G, RM M, and strategy profile ⃗σ ∈ Str(G†M),
it holds that
payG†M(⃗σ)=payG′(σ ,θ (⃗σ))
i i M G†M
Proof. Observethatthepathπ =π(⃗σ,(s ,q0))belongstothesetPaths ((s ,q0)).
in G†M in
Moreover, by Construction 3, the path π′ = π((σ ,θ (⃗σ)),(s ,q0)) is ex-
M G†M in
actly the one such that wM(π) = w′(π′) as proved in the Item 1 of Lemma 1.
g 0
This straightforwardly shows that payG†M(⃗σ)=payG′(σ ,θ (⃗σ)).
i i M G†M
Lemma 3. For a given game G, a strategy σ ∈ Str (G′), and strategy profile
0 0
⃗σ ∈StrG†M, it holds that
payG i′ (σ 0,⃗σ)=payG
i
†Mσ0(θ G′(⃗σ))
Sketch. The proof is similar to the one of Lemma 2, with the use of Construc-
tion 4 and Item 2 of Lemma 1.
Byhavingthesamesetofpayoffs,itsimplyfollowsfromLemma1,Lemma2,
and Lemma 3, that the games G†M and G′, where agent 0 is bound to the use
of σ share the same set of Nash Equilibria.
M
Theorem 4. For a given game G and a budget β, the two following hold:
1. For every β-RM M and strategy profile ⃗σ in G†M, it holds that
⃗σ ∈NE(G†M) iff (σ ),⃗σˆ)∈NE (G′).
M 0
2. For every strategy profile (σ ,⃗σ) in G′, it holds that
0
(σ ,⃗σ)∈NE (G′) iff θ (⃗σ)∈NE(G†M )
0 0 G†Mσ0 σ0
11s,0
∗,0
s,0
m,1 l,0 s,0 m,0 l,0 r,0
q6 q5 q4 q3 q1 q0 q2
s,0
Figure 3: Reward machine M′.
5 Solving Improvement Problems
Inthissection,wepresentatechniqueforsolvingtheweakandstrongimprove-
ment problems. We also demonstrate how to synthesise the RM, if it exists.
With the definition of the improvement problems, it makes sense to start with
the problems of computing worstNE and bestNE. To this end, we introduce
NE threshold problem [28] that we will use as a subroutine in our algorithms.
This problem asks whether there exists a NE in G, such that the payoffs for the
players fall between two vectors ⃗x and ⃗y.
Definition 5 (NE Threshold Problem). Given a game G and vector ⃗x,⃗y ∈
(Q∪{±∞})n, decide whether there is ⃗σ ∈ NE(G) with x ≤ pay (⃗σ) ≤ y for
i i i
every i∈N.
When the players have pure strategies, the NE threshold problem can be
solved in NP [28].
We begin with the following observation. For a given game G, it holds
that MinWG ≤ worstNE(G) ≤ MaxWG and MinWG ≤ bestNE(G) ≤ MaxWG.
g g g g
Moreover, it also holds that for a given G′, we have MinWG′ ≤ worstNE(G′) ≤
0
MaxWG′ and MinWG′ ≤ bestNE(G′) ≤ MaxWG′ . As such, by using binary
0 0 0
search and the NE threshold problem subroutine, we can compute the values of
worstNE and bestNE for G and G′.
Aswepreviouslydiscussed,theoptimalvaluesofworstNE(G)andworstNE(G†
M) may not be achievable with finite-state strategies and RMs. To see this,
consider again Example 1. Suppose that we have a RM M′ shown in Figure 3,
where τM′(q ,m) = 1 and τM′(q,s) = 0 for all (q,s) ̸= (q ,m). Intuitively,
1 5 1 5
player 1 is only given a reward of 1 after it finishes two cycles of deliveries.
Clearly the set of NE still corresponds to the same sequence p(s,l,m)ω. How-
ever, since now the designer only needs to pay 1 unit for every two cycles, we
have worstNE(G†M′) = 5, which is strictly greater than worstNE(G†M) = 2
6 3
obtained by the RM in Figure 2. In fact, we can increase the number of cycles
needed to be done before giving 1 unit of reward by adding more states in the
RM, thus obtaining strictly greater worstNE value. Since the size of RM is not
bounded, we can do this indefinitely. A similar argument can also be given
for the optimal value of worstNE(G), the complete explanation can be found in
Appendix A.1. Observe that by multiplying pay with −1, we can also use the
g
example above to analogously reason about bestNE.
Theaboveargumentsshowsthatthebinarysearchforcomputingthevalues
of worstNE and bestNE may not terminate. To ensure termination, we compute
12Algorithm 1 Computing ε-worstNE
input: G,ε
1: a 1 ←MinWG g;a 2 ←MaxWG g
2: while a 2−a 1 ≥ε do
3: a′ ← a1+a2;
2
4: if ∃⃗σ ∈NE(G),a 1 ≤pay g(⃗σ)≤a′ then
5: a 2 ←a′
6: else
7: a 1 ←a′
8: end if
9: end while
10: return a 2
approximate values instead.
Definition 6. Given ε > 0, an approximate value of worstNE (resp. bestNE)
is a value a such that a−ε<o, where o is the optimal value of worstNE (resp.
bestNE). We referto suchan approximatevalue asε-worstNE(resp. ε-bestNE).
We provide Algorithm 1 for computing ε-worstNE given G and ε encoded
in binary. The check in Line 4 corresponds to the NE threshold problem from
Definition 5. Notice that the threshold vectors ⃗x,⃗y are not explicitly given, as
we are not interested in these values. Thus, we fix x = MinWG,y = MaxWG
i i i i
for each i ∈ N, i.e., they can be of any possible values. On the other hand, we
are interested in pay , which in fact does not correspond to the payoff of any
g
player. However, we can easily modify the underlying procedure for solving the
problem in [28] to handle this. Specifically, by [28, Lemmas 14 and 15], we can
specify an additional linear equation corresponding to the value of pay being
g
in between a and a′, thus yielding a procedure that is also in NP. Algorithm 1
1
can also be used to compute ε-worstNE(G′) with the following adaptation: Line
4 is slightly modified into ∃⃗σ ∈ NE (G),a ≤ pay (⃗σ) ≤ a′, that is, the NE
0 1 0
set corresponds to the 0-fixed NE. Just as with pay , pay is not the payoff of
g 0
any player in N. Therefore, we modify the underlying procedure for the NE
threshold problem using the same approach as the above.
To compute ε-bestNE, we can employ a similar technique. We make the
followingmodificationtoAlgorithm1: ineachiteration,insteadofcheckingthe
left-half part, we check the right-half part (i.e., instead of minimising, we are
maximising). This is done in Lines 4-8 of the algorithm by checking whether
∃⃗σ ∈ NE(G),a′ ≤ pay (⃗σ) ≤ a . If the check returns true, we set a ← a′,
g 2 1
otherwise a ← a′. Again, as with worstNE, we slightly modify Line 4 in order
2
to compute bestNE(G′).
Theorem 5. Given a game G (resp. G′) and ε > 0, the problems of comput-
ing ε-bestNE(G) and ε-worstNE(G) (resp. ε-bestNE(G′) and ε-worstNE(G′)) are
FPNP-complete.
13Proof. The upper bounds follows from Algorithm 1. The while loop runs in
polynomial number of steps (i.e., logarithmic in |G|· 1), and in each step calls
ε
a NP oracle. Observe that ε can be arbitrarily small (i.e., arbitrary precision).
ForthelowerboundwereducefromTSP CostwhichisFPNP-hard[23]. Given
aTSPCostinstance(G,c),G=(V,E)isagraph,c:E →Zisacostfunction,
we construct a game G such that the ε-worstNE(G) corresponds to the value of
optimum tour3. Let G be such a game where
• N=V,
• St={(e,v):e∈E∧v =trg(e)}∪{(⋆,sink)},
• s0 can be chosen arbitrarily from St\{(⋆,sink)},
• for each state (e,v)∈St and each player i∈N
– d ((e,v))={out(v)}∪{⋆} if i=v
i
– d ((e,v))={◦,⋆}, otherwise;
i
• for each state (e,v)∈St and action profile A⃗c
– tr((e,v),A⃗c)=(a ,trg(a )) if v ̸=sink and ∀i∈N,a ̸=⋆;
v v i
– tr((e,v),A⃗c)=(⋆,sink), otherwise;
• for each state (e,v)∈St and player i∈N
– w ((e,v))=|V|, if v =i and v ̸=sink,
i
– w ((e,v))=0, if v ̸=i and v ̸=sink,
i
– w ((e,v))=1, if v =sink;
i
• for each state (e,v)∈St
– w ((e,v))=max{c(e′):e′ ∈E}·|V|, if v =sink
g
– w ((e,v))=c(e)·|V|, otherwise;
g
where ◦,⋆,sink are fresh symbols. We also set ε = 1. The construction is
complete and polynomial to the size of (G,c).
We argue that ⌊ε-worstNE(G)⌋ is exactly the value of optimal valid tour.
First, observe that for any⃗σ ∈NE(G), it holds that either (1) π(⃗σ) visits every
v ∈V (i.e., visits every city), thus a valid tour, or (2) π(⃗σ) enters (⋆,sink) and
stays there forever. Case (1) holds because if π(⃗σ) does not visit v ∈ V, then
pay (⃗σ)=0 thus player v will deviate to (⋆,sink) and obtain better payoff. In
v
fact, ⃗σ visits each city exactly once, because otherwise, there is a player who
gets payoff strictly less than 1, and deviates to (⋆,sink). Case (2) is trivially
true; however, assuming that the costs are not uniform (otherwise TSP Cost
becomes trivial), it cannot be a solution to ε-worstNE. Let o be the optimal
3ForauxiliarygameG′,wecaneasilyadaptthereductionbysubstitutingwg withw0.
14tour cost, and suppose for a contradiction that ⌊ε-worstNE(G)⌋<o. Let⃗σˆ be a
correspondingstrategyprofile. BytheconstructionofG,thismeansthat⃗σˆ does
notvisitsomecitiesorvisitssomecitiesmorethanonce. However,by(1)above,
⃗σˆ cannot be in NE(G)—a contradiction. We can argue in a similar manner for
⌊ε-worstNE(G)⌋>o; itisnotpossiblebecauseeitherthecorrespondingstrategy
does not form a valid tour (and by (1) above, it is not a NE), or it is not the
optimal solution to ε-worstNE; again a contradiction. Finally, since ε-worstNE
approaches worstNE from the right, we have ⌊ε-worstNE(G)⌋=o.
For bestNE, we can use the same construction but with the following modi-
fication to w :
g
• w ((e,v))=−(max{c(e′):e′ ∈E}·|V|), if v =sink
g
• w ((e,v))=−(c(e)·|V|), otherwise;
g
and use similar argument as the above.
Approximateimprovementproblems Wedefinetheapproximateimprove-
ment problems as follows.
Definition 7 (ε-improvementproblem). Given a game G, a budget β, a thresh-
old ∆, and ε. The Γ ε-improvement problem, with Γ ∈ {strong,weak}, decides
whether there exists a β-RM M such that:
ε-ΓNE(G†M)−ε-ΓNE(G)>∆.
Havingtheproceduresforcomputingε-worstNEandε-bestNEforbothG and
G′, we can then directly solve the ε-improvement problem with the following
procedure.
1. Build the auxiliary game G′;
2. Compute ε-ΓNE(G) and ε-ΓNE(G′);
3. If ε-ΓNE(G′)−ε-ΓNE(G)>∆, then return “yes”; otherwise return “no”.
Theorem 6. Strong and weak ε-improvement problems are ∆P.
2
Proof. The upper bounds follow from the procedure described above. Steps 1
and 3 can be done in polynomial time, Step 2 only needs two calls to an FPNP
oracle. Thus we have a decision procedure that runs in PNP =∆P.
2
Theorem 7. Strong and weak ε-improvement problems are NP-hard and coNP-
hard, respectively.
Proof. Toshowthatstrongε-improvementproblemisNP-hard,wereducefrom
HamiltonianPathproblem: givenadirectedgraphG=(V,E),isthereapath
that visits each vertex exactly once; this problem is NP-hard [23]. We build a
gameGandfixβ,∆andεsuchthatthestrongε-improvementproblemreturns
yesifandonlyif HamiltonianPathreturnsyes. GivenaHamiltonianPath
instance G=(V,E), we construct a game G as follows.
15• N=V ∪{n+1,n+2}, where V ={1,...,n},
• St={(e,v):e∈E∧v =trg(e)}∪{(⋆,sink),(⋆,■),(⋆,△)},
• s can be chosen arbitrarily from St\{(⋆,sink),(⋆,■),(⋆,△)},
in
• for each state (e,v)∈St and each player i∈N
– d ((e,v))={out(v)}∪{⋆} if i=v
i
– d ((e,v))={◦,⋆}, otherwise;
i
• for each state (e,v)∈St and action profile A⃗c
– tr((e,v),A⃗c)=(a ,trg(a )) if v ̸=sink and ∀i∈V,a ̸=⋆;
v v i
– tr((e,v),A⃗c)=(⋆,sink), if v ̸=sink and ∃i∈V,a =⋆;
i
– tr((e,v),A⃗c)=(⋆,■), if v =sink and a =a ;
n+1 n+2
– tr((e,v),A⃗c)=(⋆,△), if v =sink and a ̸=a ;
n+1 n+2
– tr((e,v),A⃗c)=(⋆,v), if v ∈{■,△};
• for each state (e,v)∈St and player i∈{1,...,n}
– w ((e,v))=|V|, if v =i and v ̸∈{sink,■,△},
i
– w ((e,v))=0, if v ̸=i and v ̸∈{sink,■,△},
i
– w ((e,v))=1, if v ∈{sink,■,△};
i
• for each state (e,v)∈St and player i∈{n+1,n+2}
– w ((e,v))=0;
i
• for each state (e,v)∈St
– w ((e,v))=0, if v ∈{sink,■,△}
g
– w ((e,v))=|V|, otherwise;
g
where ◦,⋆,sink,■,△ are fresh symbols. We also set β =1,ε=1,∆= 1. The
2
construction is complete and polynomial to the size of G.
Observe that worstNE(G)=0, where the play goes to either (∗,■) or (∗,△)
and stays there forever. However, with β = 1, the designer can pay player
n+1 (resp. player n+2) with a payment of 1 when the play reaches (∗,■)
(resp. (∗,△)). Essentially, forcing n+1 and n+2 to play a matching pennies
game, a game with no Nash equilibrium. Thus, the play that goes to either
(∗,■) or (∗,△) no longer part of NE(G). Now, consider a run that visits each
v ∈ V exactly once, this is a Nash equilibrium. The reasoning is the same
as the one provided in the proof of Theorem 5. And by construction, such
a run can only be possible if and only if there is a Hamiltonian path in the
corresponding graph G. Let π be such a run, now we have pay (π) = 1 and
g
thus, ε-worstNE(G†M)−ε-worstNE(G)=1−0> 1.
2
The proof for weak ε-improvement problem is similar: through a reduction
from the complement of Hamiltonian Path. The proof is included in Ap-
pendix A.2.
16Synthesis of Reward Machines Given a game G, a budget β, a thresh-
old ∆, and ε, if the strong (resp. weak) ε-improvement problem returns a
positive answer, then we can synthesise the corresponding RM M as follows.
From the auxiliary game G′, find a strategy profile (σ ,⃗σ)∈NE (G′) such that
0 0
payG′ (σ ,⃗σ) = worstNE(G′) (resp. payG′ (σ ,⃗σ) = worstNE(G′)). Using Con-
0 0 0 0
struction2, weobtaintheRMM fromσ , whichcorrespondstotherequired
σ0 0
RM.
6 Conclusion
In this paper, we examined games where each agent had a weight function
over states, with their utility determined by the mean-payoff aggregation. A
global weight function was used to gauge designer satisfaction, also measured
through mean-payoff value. We utilised reward machines to enhance designer
satisfaction,reconfiguringweightsaftereachiterationtoreshapetheequilibrium
set. Our aim was to boost the global payoff over equilibria by at least a given
value ∆, achieved by strategically synthesising a suitable reward machine.
Among the other results, we first demonstrated that reward machines are
strictly more effective than subsidy schemes. However, we also found that in
somecases, althoughnorewardmachinecouldimprovetheglobalpayoffbythe
requiredvalue∆,anε-approximationcouldbefound. Thus,weintroducedand
addressedtheε-improvementproblemasamoregeneralapproachtoequilibrium
design.
Since multiple equilibria are possible in these games, we analysed the syn-
thesis problem from both optimistic and pessimistic perspectives. We aimed to
enhancetheglobalmean-payoffoverthebestandworstpossibleNashEquilibria,
considering scenarios where agents select the most or least convenient equilib-
rium from the designer’s viewpoint, respectively. We also provided complexity
classifications for these problems, demonstrating that each could be solved in
∆P and were at least NP-hard and coNP-hard.
2
Future work Several directions are possible from this. First, extensions of
designers and agents’ objectives should be considered. For example, in [14, 12]
the agents’ goals are represented as a combination of LTL and mean-payoff ob-
jectives, arranged in a lexicographic fashion. Also multi-valued logic such as
LTL[F] are considered for rational verification [3]. It would be interesting to
find out how to employ reward machines to boost the satisfaction value for this
case. Last but not least, an excursion into normative systems should be consid-
ered. Althoughdynamicnormsasdefinedin[15]areofthesametypeofreward
machines [18], their implementation to games provide very different effects. On
the one hand, norms disable agents’ actions. On the other hand, reward ma-
chines do not strictly forbid agents to execute their actions in the game, but
ratherreward-incentivisethosethataremoreconvenientfromtheglobalstand-
point. It would be interesting to combine the two approaches, finding the right
balance between obligation and recommendation modalities.
17Acknowledgments
Perelli was supported by the PNRR MUR project PE0000013-FAIR and the
PRIN2020projectsPINPOINT.HewasalsosupportedbySapienzaUniversity
ofRomeunderthe“ProgettiGrandidiAteneo”programme,grantRG123188B3F7414A
(ASGARD - Autonomous and Self-Governing Agent-Based Rule Design).
References
[1] Natasha Alechina, Giuseppe De Giacomo, Brian Logan, and Giuseppe
Perelli. Automatic synthesis of dynamic norms for multi-agent systems.
In Gabriele Kern-Isberner, Gerhard Lakemeyer, and Thomas Meyer, ed-
itors, Proceedings of the 19th International Conference on Principles of
Knowledge Representation and Reasoning, KR 2022, Haifa, Israel, July 31
- August 5, 2022, 2022.
[2] Natasha Alechina and Brian Logan. State of the art in logics for verifica-
tion of resource-bounded multi-agent systems. In Andreas Blass, Patrick
C´egielski, Nachum Dershowitz, Manfred Droste, and Bernd Finkbeiner,
editors, Fields of Logic and Computation III - Essays Dedicated to Yuri
Gurevich on the Occasion of His 80th Birthday, volume 12180 of Lecture
Notes in Computer Science, pages 9–29. Springer, 2020.
[3] S. Almagor, O. Kupferman, and G. Perelli. Synthesis of controllable Nash
equilibria in quantitative objective games. In IJCAI, pages 35–41, 2018.
[4] PatriciaBouyer,RomainBrenguier,NicolasMarkey,andMichaelUmmels.
Purenashequilibriainconcurrentdeterministicgames. Logical methods in
computer science, 11, 2015.
[5] Romain Brenguier. Robust equilibria in mean-payoff games. In Interna-
tional Conference on Foundations of Software Science and Computation
Structures, pages 217–233. Springer, 2016.
[6] L´eonard Brice, Jean-Fran¸cois Raskin, and Marie van den Bogaard.
Subgame-perfect equilibria in mean-payoff games. In Serge Haddad and
Daniele Varacca, editors, 32nd International Conference on Concurrency
Theory, CONCUR 2021, August 24-27, 2021, Virtual Conference, volume
203 of LIPIcs, pages 8:1–8:17. Schloss Dagstuhl - Leibniz-Zentrum fu¨r In-
formatik, 2021.
[7] L´eonardBrice,Jean-Fran¸coisRaskin,andMarievandenBogaard.Rational
verification for nash and subgame-perfect equilibria in graph games. In
48th International Symposium on Mathematical Foundations of Computer
Science (MFCS 2023). Schloss Dagstuhl-Leibniz-Zentrum fu¨r Informatik,
2023.
18[8] Nils Bulling and Mehdi Dastani. Norm-based mechanism design. Artificial
Intelligence, 239:97–142, 2016.
[9] NilsBullingandValentinGoranko. Combiningquantitativeandqualitative
reasoning in concurrent multi-player games. Auton. Agents Multi Agent
Syst., 36(1):2, 2022.
[10] J.Gutierrez,P.Harrenstein,andM.Wooldridge. IteratedBooleanGames.
Information and Computation, 242:53–79, 2015.
[11] J.Gutierrez, P.Harrenstein, andM.Wooldridge. FromModelCheckingto
Equilibrium Checking: Reactive Modules for Rational Verification. Artifi-
cial Intelligence, 248:123–157, 2017.
[12] J. Gutierrez, A. Murano, G. Perelli, S. Rubin, and M. Wooldridge. Nash
EquilibriainConcurrentGameswithLexicographicPreferences. InIJCAI,
pages 1067–1073, 2017.
[13] Julian Gutierrez, Muhammad Najib, Giuseppe Perelli, and Michael
Wooldridge. On the complexity of rational verification. Annals of Mathe-
matics and Artificial Intelligence, 91(4):409–430, 2023.
[14] Julian Gutierrez, Muhammad Najib, Giuseppe Perelli, and Michael J.
Wooldridge. Equilibrium design for concurrent games. In Wan J. Fokkink
and Rob van Glabbeek, editors, 30th International Conference on Con-
currency Theory, CONCUR 2019, August 27-30, 2019, Amsterdam, the
Netherlands, volume 140 of LIPIcs, pages 22:1–22:16. Schloss Dagstuhl -
Leibniz-Zentrum fu¨r Informatik, 2019.
[15] Xiaowei Huang, Ji Ruan, Qingliang Chen, and Kaile Su. Normative mul-
tiagent systems: A dynamic generalization. In Proceedings of the Twenty-
Fifth International Joint Conference on Artificial Intelligence, IJCAI’16,
page 1123–1129. AAAI Press, 2016.
[16] L. Hurwicz and S. Reiter. Designing Economic Mechanisms. Cambridge
University Press, 2006.
[17] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIl-
raith. Using reward machines for high-level task specification and decom-
positioninreinforcementlearning. InInternationalConferenceonMachine
Learning, pages 2107–2116. PMLR, 2018.
[18] Rodrigo Toro Icarte, Toryn Q Klassen, Richard Valenzano, and Sheila A
McIlraith. Rewardmachines: Exploitingrewardfunctionstructureinrein-
forcementlearning. Journal of Artificial Intelligence Research, 73:173–208,
2022.
[19] Orna Kupferman, Giuseppe Perelli, and Moshe Y. Vardi. Synthesis with
Rational Environments. In EUMAS’14, volume 8953 of Lecture Notes in
Computer Science, pages 219–235. Springer, 2014.
19[20] Roger B Myerson. Mechanism design. Springer, 1989.
[21] Cyrus Neary, Zhe Xu, Bo Wu, and Ufuk Topcu. Reward machines for
cooperative multi-agent reinforcement learning. In Proceedings of the 20th
International Conference on Autonomous Agents and MultiAgent Systems,
AAMAS ’21, page 934–942, Richland, SC, 2021. International Foundation
for Autonomous Agents and Multiagent Systems.
[22] M.J. Osborne and A. Rubinstein. A Course in Game Theory. MIT Press,
1994.
[23] C. Papadimitriou. Computational complexity. Addison-Wesley, Reading,
Massachusetts, 1994.
[24] Giuseppe Perelli. Enforcing equilibria in multi-agent systems. In Pro-
ceedings of the 18th International Conference on Autonomous Agents and
MultiAgent Systems, AAMAS ’19, page 188–196, Richland, SC, 2019. In-
ternational Foundation for Autonomous Agents and Multiagent Systems.
[25] A.Pnueli. TheTemporalLogicofPrograms. InFOCS,pages46–57.IEEE,
1977.
[26] Thomas Steeples, Julian Gutierrez, and Michael Wooldridge. Mean-payoff
games with ω-regular specifications. In Proceedings of the 20th Interna-
tional Conference on Autonomous Agents and MultiAgent Systems, pages
1272–1280, 2021.
[27] Rodrigo Toro Icarte, Ethan Waldie, Toryn Klassen, Rick Valenzano, Mar-
garitaCastro,andSheilaMcIlraith. Learningrewardmachinesforpartially
observablereinforcementlearning. Advancesinneuralinformationprocess-
ing systems, 32, 2019.
[28] M.UmmelsandD.Wojtczak. TheComplexityofNashEquilibriainLimit-
Average Games. In CONCUR, pages 482–496, 2011.
[29] Giovanni Varricchione, Natasha Alechina, Mehdi Dastani, and Brian Lo-
gan. Synthesising reward machines for cooperative multi-agent reinforce-
ment learning. In European Conference on Multi-Agent Systems, pages
328–344. Springer, 2023.
[30] Y. Velner, K. Chatterjee, L. Doyen, T. Henzinger, A. Rabinovich, and
J.Raskin.TheComplexityofMulti-Mean-PayoffandMulti-EnergyGames.
Information and Computation, 241:177–196, 2015.
[31] M. Wooldridge, J. Gutierrez, P. Harrenstein, E. Marchioni, G. Perelli, and
A. Toumi. Rational Verification: From Model Checking to Equilibrium
Checking. In AAAI, pages 4184–4191. AAAI Press, 2016.
[32] U. Zwick and M. Paterson. The Complexity of Mean Payoff Games on
Graphs. Theoretical Computer Science, 158(1):343 – 359, 1996.
20A Appendix
A.1 On Exact Optimal worstNE(G)
Exact optimal value of worstNE(G) may not be achievable with finite-memory
strategies. To see this, consider the game arena below.
t
(∗,∗) (0,0) (L,∗)
(∗,L) l r (R,∗)
(1,0) (0,1)
(∗,R) (∗,∗)
b
(0,0)
ThesetofplayersisN={1,2}andtheinitialstateist. Letw (s)=−w (s).
g 1
Consider the run π = (t,l,b,r)ω; this is a NE with pay (π) = −1. However,
g 4
increasing the number of loops in l and r also decreases pay . For instance
g
π′ = (t,l,l,b,r,r)ω is also an NE. To see this consider a “threat” strategy by
player1asfollows: ifplayer2doesnotloopatleastonetimeinl,thenneverloop
in r forever more. We can reason similarly for player 2 to player 1. Therefore,
it is a NE. However, we have pay (π′)=−1, thus pay (π)>pay (π′). We can
g 3 g g
continue increasing the number of loops, and decreasing the value of pay .
g
A.2 Proofforweakε-improvementproblemcoNP-hardness
We reduce from the complement of Hamiltonian Path problem. We build a
game G and fix β,∆ and ε such that the weak ε-improvement problem returns
yesifandonlyif Hamiltonian Pathreturnsno. GivenaHamiltonian Path
instance G=(V,E), we construct a game G as follows.
• N=V ∪{n+1,n+2}, where V ={1,...,n},
• St={(e,v):e∈E∧v =trg(e)}∪{(⋆,sink),(⋆,■),(⋆,△)},
• s can be chosen arbitrarily from St\{(⋆,sink),(⋆,■),(⋆,△)},
in
• for each state (e,v)∈St and each player i∈N
– d ((e,v))={out(v)}∪{⋆} if i=v
i
– d ((e,v))={◦,⋆}, otherwise;
i
• for each state (e,v)∈St and action profile A⃗c
– tr((e,v),A⃗c)=(a ,trg(a )) if v ̸=sink and ∀i∈V,a ̸=⋆;
v v i
21– tr((e,v),A⃗c)=(⋆,sink), if v ̸=sink and ∃i∈V,a =⋆;
i
– tr((e,v),A⃗c)=(⋆,■), if v =sink and a =a ;
n+1 n+2
– tr((e,v),A⃗c)=(⋆,△), if v =sink and a ̸=a ;
n+1 n+2
– tr((e,v),A⃗c)=(⋆,v), if v ∈{■,△};
• for each state (e,v)∈St and player i∈{1,...,n}
– w ((e,v))=|V|, if v =i and v ̸∈{sink,■,△},
i
– w ((e,v))=0, if v ̸=i and v ̸∈{sink,■,△},
i
– w ((e,v))=1, if v ∈{sink,■,△};
i
• for each state (e,v)∈St
– w ((e,v))=1 if v =■;
n+1
– w ((e,v))=0 otherwise;
n+1
• for each state (e,v)∈St
– w ((e,v))=1 if v =△;
n+2
– w ((e,v))=0 otherwise;
n+2
• for each state (e,v)∈St
– w ((e,v))=0, if v ∈{sink,■}
g
– w ((e,v))=2, if v =△
g
– w ((e,v))=|V|, otherwise;
g
where ◦,⋆,sink,■,△ are fresh symbols. We also set β =1,ε=1,∆= 1. The
2
construction is complete and polynomial to the size of G.
Observe that when the play goes to (∗,sink), players n+1 and n+2 is
forced to play matching pennies game. Thus, any run that ends up in (∗,sink)
cannot be in NE(G). Now consider a RM M where player n + 1 is paid a
payoff of 1 when the play reaches (∗,△). Let π be a run that ends up in
(∗,△). We have that π ∈ NE(G †M) since n+1,n+2 do not play matching
pennies game any more, and thus pay (G †M) = 1. Notice that pay (G) = 0
g g
if and only if NE(G) = ∅, which can only happen if there is no Hamiltonian
path in G; otherwise, the players {1,...,n} can play a run π′ that visits each
vertex v ∈ V exactly once such that for each player i ∈ N, pay (G) = 1, thus
i
corresponding to a Nash equilibrium (see proof of Theorem 5). Consequently,
ε-bestNE(G†M)−ε-bestNE(G)> 1 if and only if there is no Hamiltonian path
2
in G.
22