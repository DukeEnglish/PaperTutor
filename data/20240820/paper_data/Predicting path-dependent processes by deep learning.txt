JournalofMachineLearningResearchXX(2024) 1-30 SubmittedX/XX;RevisedX/XX;PublishedX/XX
Predicting path-dependent processes by deep learning
Xudong Zheng zxd22@mails.jlu.edu.cn
School of Mathematics
Jilin University
Changchun, Qianjin Street 2699, CHN
Yuecai Han hanyc@jlu.edu.cn
School of Mathematics
Jilin University
Changchun, Qianjin Street 2699, CHN
Editor: My editor
Abstract
In this paper, we investigate a deep learning method for predicting path-dependent pro-
cesses based on discretely observed historical information. This method is implemented
by considering the prediction as a nonparametric regression and obtaining the regression
functionthroughsimulatedsamplesanddeepneuralnetworks. Whenapplyingthismethod
to fractional Brownian motion and the solutions of some stochastic differential equations
driven by it, we theoretically proved that the L2 errors converge to 0, and we further
discussed the scope of the method. With the frequency of discrete observations tend-
ing to infinity, the predictions based on discrete observations converge to the predictions
based on continuous observations, which implies that we can make approximations by
the method. We apply the method to the fractional Brownian motion and the fractional
Ornstein-Uhlenbeck process as examples. Comparing the results with the theoreticalopti-
mal predictions and taking the mean square erroras a measure, the numerical simulations
demonstrate that the method can generate accurate results. We also analyze the impact
of factors such as prediction period, Hurst index, etc. on the accuracy.
Keywords: path-dependent processes, prediction, deep learning, fractional Brownian
motion
1 Introduction
Stochasticprocessesareoftenusedinprobabilityapplicationstomodelasystem. Forexam-
ple, analyzing stochastic processes in intricate systems is the goal of modeling intracellular
transport,and modelingassetprices by stochastic processesis a toolof financialmathemat-
ics. Markovian stochastic processes are widely used in these fields. However, recent studies
show that some actual data exhibit path dependence, which means that the behavior of a
real process at a given time t depends on the conditions at the point in time as well as the
whole history up to that moment. An increasing amount of evidence suggests that many
cellular systems show anomalous diffusion rather than classical Brownian motion, as a re-
sult of single-particle tracking experiments that directly see molecular movement (see, e.g.,
Caccetta et al. (2011)). In financial mathematics, recent studies have demonstrated that
rough stochastic volatility-type dynamics may better describe spot volatilities, and it can
©2024XudongZhengandYuecaiHan.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/. Attributionrequirementsareprovided
athttp://jmlr.org/papers/vXX/21-0000.html.
4202
guA
91
]LM.tats[
1v14990.8042:viXraZheng and Han
be confirmed that volatility is rough by price data of at-the-money options on the S&P500
index with short maturity (see, e.g., Livieri et al. (2018)).
Ouraiminthispaperistoproposeaframeworkforpredictingpath-dependentstochastic
processes (W , t 0) by discrete historical information and deep neural networks, i.e., to
t
≥
solve
E W D,V,N ,
T s
|F
D,V,N (cid:2) (cid:3)
where, := σ V ,...,V for 0 < t < < t = s <T and there are connections
Fs
{
t1 tN} 1
···
N
between thestochastic processes(V , t 0)and(W , t 0). Naturally, thecaseV = W is
t t t t
≥ ≥
also underourconsideration. Here, ”D” meansthatthehistorical information is ”discrete”,
and later in the paperwe use”C” to denote that the historical information is ”continuous”.
In this paper, we mainly focus on the predictions of the fractional Brownian motion (one
of the widely studied path-dependent stochastic processes) and some stochastic processes
driven by it. Without loss of generality, let t t = s/N in the following.
i i−1
−
The fractional Brownian motion (fBm) BH, t 0 with Hurst index H (0,1) is
t
≥ ∈
a self-similar process, i.e. BH, t 0 and (aHBH, t 0) have the same probability
at ≥ (cid:0) t (cid:1)≥
law for all a > 0. Compared to the standard Brownian motion (sBm), it displays long-
(cid:0) (cid:1)
range dependence and positive correlation when H (1/2,1) and negative correlation
∈
when H (0,1/2). This special property makes it a useful tool for modeling a wide
∈
range of phenomena, such as financial time series, turbulence, and climate data (see, e.g.,
Mandelbrot and Van Ness(1968)). Kepten et al.(2011)discoveredstrongevidencethatthe
telomeres motion obeys fractional dynamics, and the ergodic dynamics are seen empirically
to fit the fBm. In finance, Gatheral et al. (2018) showed that log-volatility behaves as the
fBm at any reasonable timescale by estimating volatility from high-frequency data.
Compared to the sBm, the fBm with H = 1/2 is neither a Markov process nor a
6
semi-martingale, therefore some classical stochastic analysis tools are not applicable. The-
oretical research on it has been active in recent years. Advances have also been made in
the study of its stochastic fractional calculus and statistical theory, which provides a frame-
work for modeling stochastic processes with it (see, e.g., Nualart and Saussereau (2009);
Wang (1996)). Decreusefond and U¨stu¨nel (1999) used the stochastic calculus of variations
todevelop stochastic analysis theoryfor thefunctionals offBms, whileDuncan et al.(2000)
defined the multiple and iterated integrals of the fBm and provided various properties of
these integrals. Elliott and Van Der Hoek (2003) presented an extended framework, en-
abling processes with all indexes to be considered under the same probability measure.
Biagini et al. (2004) introduced the theory of stochastic integration for the fBm based on
white-noise theory and differentiation.
Statistical inference for diffusion processes that satisfy stochastic differential equations
driven by Brownian motions has been studied previously, with Prakasa Rao (2010) pro-
viding a comprehensive overview of the various approaches. Attention has been given
to studying similar problems for stochastic processes driven by the fBm. Alain (1998)
studied parameter estimation and filtering for simple linear models driven by the fBm.
Kleptsyna and Breton (2002) investigated the parameter estimation problems for the frac-
tional Ornstein-Uhlenbeck-type process. Tudor and Viens (2007) studied the maximum
likelihood estimator for the drift parameter of stochastic processes satisfying stochastic
equations driven by the fBm. It was further demonstrated that the existence and strong
2Predicting path-dependent processes by deep learning
consistency of the maximum likelihood estimation for both linear and nonlinear equations.
The problem of parametric and nonparametric estimation of processes driven by the fBm
wascomprehensively discussedbyPrakasa Rao(2010). Dang and Jacques(2017)estimated
both the Hurst index and the stability indices of a H-self-similar stable process, and ob-
tained consistent estimators with convergence rate. Kubilius et al. (2017) gave an overview
of the Hurst index estimators for the fBm.
The subject of how to forecast the future based on observable data and its potential
accuracy naturally emerges once a model driven by the fBm has been established. In
other words, when using the fBm to simulate path-dependent stochastic processes, explicit
predictor formulas are useful tools. This issue is related to the flow property and prediction
theory. Nualart (2006) explained that there exists a sBm whose filtration is the same as
the one of the fBm, since the existence of a bijective transfer operator to express the sBm
as a Wiener integral with respect to the fBm. But since all of the historical data will play
a role, its non-Markovianity makes prediction more difficult.
When the observed information is continuous, representing the predictors of fBm as
integrals over the process’s observed portion makes sense. Gripenberg and Norros (1996)
investigated the prediction weight function by solving a weakly singular integral equation
and calculated the variance of the predictor. Additionally, they provided a characteriza-
tion of the conditional distribution of the future given the past. The result contains double
integralsandrequirescontinuousfiltrations,whichisdifficulttoimplement. Explicitexpres-
sions for some conditional expectations were provided by Duncan (2006) for the prediction
of some stochastic processes constructed as solutions of stochastic differential equations
with the fBm. Fink et al. (2013) constructed the conditional distributions of the fBm and
relatedprocesses,includingthefractionalOrnstein-Uhlenbeck(fOU)processesandthefrac-
tional Cox-Ingersoll-Ross (fCIR) processes, using a prediction formula for the conditional
expectation, Fourier techniques and Gaussianity.
When the observed information is discrete, however, the situation is different because
the filtrations generated by the sBm and the fBm are not equivalent anymore (see, Nualart
(2006)). In this case, the prediction problem can be transformed into a nonparametric re-
gression problem. More specifically, the aim is to construct an estimator called regression
function such that the mean square error (MSE) is minimized, and the regression function
that minimizes the MSE is the conditional expectation (see, e.g., Gy¨orfi et al. (2002)). For
the case where the filtrations are composed of discrete sBms, the polygonal approximation
of the fBm is the best approximation in the MSE sense (see, Hu (2005)). The optimal
predictor for long-memory time series, including the fractional auto-regressive integrated
moving average model, has been well studied (see, Bhansali and Kokoszka (2003)). An
asymptotic linear minimum-mean-square-error predictor for the discrete-time fBm was pro-
posed by Yao and Doroslovacki (2003). These results also show that due to the long-range
dependence,whenthenumberofdiscreteobservationsincreases,thepredictionofthefuture
(or the regression function) becomes quite complex because of the increased dimensionality
of the variables.
The use of neural networks has yielded impressive outcomes for high-dimensional prob-
lems across various fields (see, e.g., Shrestha and Mahmood (2019)). Deep learning, which
uses multi-layer neural networks with many hidden layers, is particularly well-suited for
high-dimensional problems and real-world data, as noted by Schmidhuber (2015). Deep
3Zheng and Han
learning performs well in solving fractional partial differential equations (see, Lu et al.
(2021)), classifying single-particle trajectories as fBms and estimating the Hurst index
(see, Granik et al. (2019)), and solving the problem of optimally stopping a fBm (see,
Becker et al. (2019)). In nonparametric regression, it was demonstrated that least squares
estimatesbasedonmultilayer feedforwardneuralnetworkscanavoidthecurseofdimension-
ality, provided that a smoothness condition and an appropriate limit on the structureof the
regression function exist (see, Bauer and Kohler (2019); Langer (2021); Kohler and Langer
(2021)).
The primary goal of this paper is to propose a deep learning method that can predict
path-dependent stochastic processes. The paper is organized as follows. We briefly recall
the properties of the fBm, nonparametric regression, and deep neural networks in Section
2. The main results are in Section 3. We theoretically demonstrate the applicability of the
method to the fBm and the solutions of some stochastic differential equations driven by
it, and further discuss the scope of the method. We also show that the outputs converge
to the predictions under continuous observations as the frequency of discrete observations
tends to infinity. In Section 4, we apply the method to two examples: the fBm and the fOU
process. We compare the results with the theoretical optimal predictions, and taking the
mean square error as a measure, the numerical simulations demonstrate that the method
can generate accurate results. We also compare the impact of factors such as prediction
period, Hurst index, etc. on the accuracy. We conclude the paper in Section 5.
2 Fractional Brownian motion, nonparametric regression and neural
networks
2.1 Fractional Brownian motion
A fBm BH, t 0 with Hurst index H (0,1) is a centered Gaussian process with
t
≥ ∈
covariance
(cid:0) (cid:1) 1
E BHBH = t2H +s2H t s 2H , s,t 0.
t s 2 −| − | ≥
(cid:0) (cid:1) (cid:0) 1 (cid:1)
When H = 1, it becomes a sBm and we denote B2 by B . We can use the following
2 t t
moving-average stochastic integral representation of the fBm, which is used in this paper
(see, Hu (2005)):
t
BH = K (t,s)dB , 0 t < , (1)
t H s
≤ ∞
Z0
where
K H(t,s) = κ H t H−1 2 (t s)H−1 2 H 1 s1 2−H t uH− 23 (u s)H− 21 du ,
s − − − 2 −
" (cid:18) (cid:19) (cid:18) (cid:19) Zs #
2HΓ 3 H
κ = 2 − .
H sΓ H + 21
(cid:0)
Γ(2 −(cid:1)2H)
If H > 21, then Z H(t,s) can be written a(cid:0)s (cid:1)
1 t
K H(t,s)= H κ Hs21−H uH−1 2(u s)H− 23 du.
− 2 −
(cid:18) (cid:19) Zs
4Predicting path-dependent processes by deep learning
Moreover, there exists a bijective transfer operator that expresses B as a integral with re-
t
spect to BH. Define C,BH := σ BH, v [0,s] , C,B := σ B , v [0,s] . The filtration
t s v s v
F { ∈ } F { ∈ }
of BH and B are equivalent, i.e. C,BH = C,B . In this paper, we focus on a given time
t t s s
F F
interval [0,T] with T > 0.
2.2 Nonparametric regression
In nonparametric regression analysis, we consider a random vector X Rd and a random
∈
variable Y R satisfying E Y2 < . Nonparametric regression is aimed at predicting the
∈ ∞
value of the response variable Y from the value of the observation vector X by constructing
(cid:2) (cid:3)
an optimal predictor m∗ : Rd R. It is usual that the aim is to minimize the MSE, i.e.,
→
E Y m∗(X) 2 = min E Y f(X)2 .
| − | f:Rd→R | − |
h i
(cid:2) (cid:3)
Let the regression function m(x) = E[Y X = x], and it is the optimal predictor since m
|
satisfies
E Y f(X)2 = E Y m(X)2 + f(x) m(x)2P (dx).
X
| − | | − | | − |
Z
(cid:2) (cid:3) (cid:2) (cid:3)
For the application, we are able to obtain a data set = (X ,Y ),...,(X ,Y ) , but do
n 1 1 n n
D { }
not know the distribution of (X,Y), where (X,Y),(X ,Y ),...,(X ,Y ) are independent
1 1 n n
andidenticallydistributed. Thetargetofnonparametricregressionistoconstructregression
estimatesm () = m ( , )suchthattheL errors m (x) m(x) 2P (dx)areassmall
n n n 2 n X
· · D | − |
as possible.
R
Compared to the parametric estimation, which assumes that the fixed structure of the
regression function depends only on a finite number of parameters, nonparametric estima-
tion methods do not assume that the regression function can be characterised by a finite
number of parameters, but rather estimate the entire function from the data. The class
of regression functions must be restricted in order to obtain theoretical results such as the
rate of convergence (see, Gy¨orfi et al. (2002)). In the following we introduce the definitions
of (p,C)-smooth and hierarchical composition models (see, e.g., Kohler and Langer (2021))
which are used to define the classes of regression functions discussed in this paper.
Definition 1 Let p = q + s for some q N and 0 < s 1. A function m : Rd R
0
is called (p,C)-smooth, if for every α =
(∈
α ,...,α )
Nd≤
with
d
α = q, the
pa→
rtial
1 d ∈ 0 j=1 j
derivative ∂qm/ ∂xα1...∂xα d exists and satisfies
1 d P
(cid:0) ∂qm (cid:1) ∂qm
(x) (z) C x z s,
∂xα1...∂xα
d −
∂xα1...∂xα
d ≤ k − k
(cid:12) 1 d 1 d (cid:12)
(cid:12) (cid:12)
for all x,z Rd, wh(cid:12) (cid:12)ere denotes the Euclidean norm.(cid:12) (cid:12)
∈ k·k
Definition 2 Let d N, m : Rd R and let be a subset of (0, ) N.
∈ → P ∞ ×
(1) We say that m satisfies a hierarchical composition model of level 0 with order and
smoothness constraint , if there exists K 1,...,d such that m(x) = x , for all
K
P ∈ { }
x = (x ,...,x ) Rd.
1 d
∈
(2) We say that m satisfies a hierarchical composition model of level l+1 with order and
5Zheng and Han
smoothness constraint , if there exist (p,K) , C > 0, g : RK R and f ,...,f :
1 K
P ∈ P →
Rd R, such that g is (p,C)-smooth, f ,...,f satisfy a hierarchical composition model
1 K
→
of level l with order and smoothness constraint and m(x) = g(f (x),...,f (x)), for all
1 K
P
x Rd.
∈
Definition 3 For l = 1 and some order and smoothness constraint (0, ) N, the
P ⊆ ∞ ×
space of hierarchical composition models becomes
(1, ) = h: Rd R : h(x)= g x ,...,x , where
π(1) π(K)
H P →
n g :RK R is (p,C)-s(cid:0)mooth for some ((cid:1)p,K) ,
→ ∈ P
C > 0 and π : 1,...,K 1,...,d .
{ } → { }
o
For l > 1, we recursively define
(l, ) := h : Rd R : h(x) = g(f (x),...,f (x)), where
1 K
H P →
n g : RK R is (p,C)-smooth for some (p,K) ,
→ ∈ P
C > 0 and f (l 1, ) .
i
∈H − P
o
Remark 4 Regression functions in (l, ) are able to characterize some input-output re-
H P
lationships in actuality. A reason for using the function class (l, ) is that it describes
H P
a complex relationship that can be constructed as a recursive construction by modules. In
other words, each module can describe a complex input-output relationship.
Remark 5 Another reason for using (l, ) in this paper is to overcome the curse of
H P
dimensionality. An optimal predictor can also be obtained with the (p,C)-smooth functions
as the target of nonparametric regression. Although it is optimal, the optimal minimax
− 2p
rate of convergence in nonparametric regression for (p,C)-smooth functions is n 2p+d (see,
Stone (1982)). We intend to consider the case where the number of discrete observations
is large in the prediction problem. If d is relatively large compared with p, the convergence
rate becomes significantly slower, which is often referred to as the ”curse of dimensionality”.
The only possible way to avoid it is to restrict the underlying function class.
Kohler and Langer (2021) showed that if the regression function satisfies a hierarchi-
cal composition model with (smoothness and order constraint), the convergence rate of
P
the L errors of least squares neural network regression estimates based on fully connected
2
neural networks with ReLU activation functions doesn’t depend on d, avoiding the curse of
dimensionality.
2.3 Neural network regression estimator
The architecture of a fully connected neural network can be represented by (L,λ), where
L is the number of the hidden layer, often called the depth of the neural network, and λ
is the width vector λ = (λ ,...,λ ) NL, which describes the number of neurons in each
1 L
∈
hidden layer. The neural network with the network architecture (L,λ) can be represented
as a function of the following form
yθ(x): Rλ0 RλL, x Γθσ Γθ ...σ Γθσ Γθx , (2)
→ 7→ L L−1 2 1
(cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17)
6Predicting path-dependent processes by deep learning
where Γθ is the λ (λ +1) matrix representing the parameters of the lth layer, and σ is
l l+1 × l
adeterministicnonlinearfunction,whichiscalled theactiviation function. Thecomponents
of the parameter θ Rq of yθ consist of the entries of the matrices A
1
Rλ1×d,...,A
L−1
∈ ∈ ∈
RλL−1×λL−2,A
L
RλL×λL−1 and the vectors b
1
Rλ1,...,b
L−1
RλL−1,b
L
RλL given by
∈ ∈ ∈ ∈
the affine functions
Γθ(x) = A x+b , i = 1,...,L.
i i i
In this paper, we choose the ReLU activation function
σ(s) = (max s ,0 ,...,max s ,0 )⊤,
1 d
{ } { }
where s is the ith element of the d-dimensional vector s. Given L hidden layers and r
i
neurons per layer,
(L,r):= yθ : yθ is of the form (2) with λ = λ = ... = λ = r, λ = 1 (3)
1 2 L−1 L
Y
n o
definesthespaceofneuralnetworksusedinthefollowing. Thenwedefinetheneuralnetwork
regressionestimatorastheminimizeroftheempiricalL -riskinthespaceofneuralnetworks
2
(L,r), that is, the estimator is defined by
Y
n
1 2
m˜ () := arg min yθ(X ) Y . (4)
n i i
· yθ∈Y(L,r)n −
Xi=1(cid:12) (cid:12)
(cid:12) (cid:12)
In numerical simulations, we use a gradient-based o(cid:12)ptimization (cid:12)method to solve the above
optimization problem to get m˜ ().
n
·
Remark 6 We use gradient-based methods in this paper since empirical studies show that
they have great generalization performance (see, e.g., Caccetta et al. (2011)). For simplic-
ity, it is assumed that the minimum in (4) is existing and available. When this is not the
case, our theoretical results also hold for the estimator m˜ (), with a small additional term.
n
·
More details about the gradient-based methods are illustrated by Bottou et al. (2018).
3 Main results
3.1 Predictions of discrete-time fractional Brownian motion
For the predictions and discrete approximation, we need the following lemma, which proves
that deep neural networks can be used to predict Gaussian processes. For d N, let
∈
Sd×d denote the space of all positive semidefinite symmetric matrices of dimension d, and
T u := max β,min β,u .
β
{− { }}
Lemma 7 Let random vectors (X,Y),(X ,Y ),...,(X ,Y )be Gaussian, independent and
1 1 n n
identically distributed. Let m0(x)= E[Y X = x], m0 = T m˜0, where
n clog(n) n
|
n
1 2
m˜0() := arg min yθ(X ) Y ,
n · yθ∈Y(Ln,rn) n
Xi=1(cid:12)
i − i
(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
K
L
n
= c
1
max n2(2p+K) logn , r
n
= r = c
2
.
·(p,K)∈P · ⌈ ⌉
(cid:24) (cid:25)
7Zheng and Han
for some c, c , c >0 sufficiently large. Then
1 2
lim E m0(x) m0(x) 2 P (dx) = 0.
n X
n→∞ −
Z
(cid:12) (cid:12)
Proof (X,Y) is multivariate norm(cid:12) ally distributed(cid:12) , and let (Y,X)⊤ N(µ,Σ) with mean
∼
µ Rd and variance-covariance matrix Σ Sd×d. Partition
∈ ∈
µ Σ Σ
µ = 1 and Σ = 11 12
µ Σ Σ
2 21 22
(cid:18) (cid:19) (cid:18) (cid:19)
with µ Rk, µ Rd−k, Σ Sk×k, Σ S(d−k)×(d−k), and Σ⊤ = Σ R(d−k)×k. We
1 ∈ 2 ∈ 11 ∈ 22 ∈ 12 21 ∈
can obtain an explicit formulation by the property of the multivariate normal distribution:
m0(x) = µ +Σ (Σ )−1(x µ )⊤.
1 12 22 2
−
Fromthedefinitionofhierarchicalcompositionmodels (l, ), itisclear thattheregression
H P
function m0( ) (l, ). By Theorem 1 of Kohler and Langer (2021),
· ∈ H P
E m0 n(x) m0(x) 2 P X(dx) c 3 (logn)6 max n− 2p2 +p K
− ≤ · ·(p,K)∈P
Z
(cid:12) (cid:12)
holds for some c an(cid:12)d sufficiently la(cid:12)rge n, c, c , c . We complete the proof.
3 1 2
As we demonstrate later, Lemma 7 is the basis of all the results in this paper. Because
we mainly discuss the prediction of path-dependent processes related to the fBm. Thus,
the multivariate normal distribution is part of the connections between (V , t 0) and
t
≥
(W , t 0) stated in the introduction.
t
≥
Remark 8 Notice that the regression function m0() is a (p,C)-smooth function, so we can
·
use some classical methods to derive the regression function directly, instead of defining a
hierarchical composition model. However, as stated in Remark 5, if a (p,C)-smooth function
is used directly as the target of nonparametric regression, the optimal rate of convergence
− 2p
achieved is n 2p+d. The least squares neural network based on fully connected neural net-
− 2p
works achieves a convergence rate n 2p+K up to a logarithmic factor, which does not depend
directly on d.
Remark 9 Lemma 7 illustrates that a sufficiently deep neural network can approximate
the regression function very well. Based on Theorem 1 of Kohler and Langer (2021), a
sufficiently wide neural network can also yield similar results. More specifically, replacing
L and r with
n n
K
L
n
= c
1
logn , r
n
= c
2
max n2(2p+K) ,
⌈ · ⌉ ·(p,K)∈P
(cid:24) (cid:25)
the Lemma 7 still holds.
We first consider a simple case of prediction of the fBm based on discrete observations,
i.e.,
m1(X) = E BH D,BH,N , (5)
T s
| F
h i
8Predicting path-dependent processes by deep learning
where D,BH,N := σ BH,...,BH for 0 < t < < t = s < T, X = BH,...,BH .
Fs t1 tN 1 ··· N t1 tN
Obtaining the prediction formula for (5) is not difficult, because of the Gaussianity of
(cid:8) (cid:9) (cid:0) (cid:1)
the fBm, although the increments are not independent. The following result shows that
there exists a deep neural network m1 () (L ,r ) converging to m1().
n n n
· ∈ Y ·
Proposition 10 Let BH, t [0,T] and BH, t [0,T] , i = 1,...,n, be independent
t ∈ t,i ∈
fractional Brownian m (cid:0)otions. Then (cid:1)(X,Y(cid:16)),(X 1,Y 1),...,(cid:17)(X n,Y n) are independent and
identically distibuted random vectors, where
Y = BH, Y = BH , X = BH,...,BH , X = BH ,...,BH .
T i T,i t1 tN i t1,i tN,i
(cid:0) (cid:1) (cid:0) (cid:1)
Let m1 = T m˜1 for some c > 0 sufficiently large, where
n clog(n) n
n
1 2
m˜1() := arg min yθ(X ) Y .
n · yθ∈Y(Ln,rn) n
Xi=1(cid:12)
i − i
(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Then
lim E m1(x) m1(x) 2 P (dx) = 0.
n X
n→∞ −
Z
(cid:12) (cid:12)
(cid:12) (cid:12)
Proof (X,Y) is multivariate normally distributed, hence we can obtain an explicit formu-
lation of (5) by the property of the multivariate normal distribution:
m1(X) = Σ1 Σ1 −1 X⊤, (6)
12 22
(cid:0) (cid:1)
where
.
.
.
Σ1 12 ⊤ =  1 2 T2H +t2 iH −|T −t i |2H  ∈ RN,
.
.
(cid:0) (cid:1)  (cid:0) . (cid:1) 
 
 
1
Σ1 = t2H +t2H t t 2H SN×N.
22 2 i j −| i − j | ∈
(cid:18) (cid:19)i,j=1,...,N
(cid:0) (cid:1)
By Lemma 7, it is clear that the regression function m1() (l, ), and we complete the
· ∈ H P
proof.
The case of the integral of the fBm Z = t f(s)dBH is considered in the following,
t 0 s
wheref is a bounded,measurable function. Although the stochastic process Z is driven by
t
R
the fBm, we actually cannot observe the path of BH, so the conditional expectation that
t
we are trying to work out is
m2(Z)= E Z D,Z,N , (7)
T s
| F
(cid:2) (cid:3)
D,Z,N
where := σ Z ,...,Z for 0 < t < < t = s < T, Z = (Z ,...,Z ).
Fs
{
t1 tN} 1
···
N t1 tN
Notice that Z is Gaussian, so we can obtain the proposition for (7) in a similar way.
t
9Zheng and Han
Proposition 11 Let BH, t [0,T] and BH, t [0,T] , i = 1,...,n, be independent
t ∈ t,i ∈
fractional Brownian m (cid:0)otions. Then ( (cid:1)Z t, t (cid:16)[0,T]) and (Z(cid:17)t,i, t [0,T]), i = 1,...,n, are
∈ ∈
independent stochastic processes, and (X,Y), (X ,Y ), ..., (X ,Y ) are independent and
1 1 n n
identically distibuted random vectors, where
t t
Z = f(s)dBH, Z = f(s)dBH,
t s t,i s,i
Z0 Z0
Y = Z , Y = Z , X = (Z ,...,Z ), X = (Z ,...,Z ).
T i t,i t1 tN i t1,i tN,i
Let m2 = T m˜2 for some c > 0 sufficiently large, where
n clog(n) n
n
1 2
m˜2() := arg min yθ(X ) Y .
n · yθ∈Y(Ln,rn) n
Xi=1(cid:12)
i − i
(cid:12)
(cid:12) (cid:12)
Then (cid:12) (cid:12)
lim E m2(x) m2(x) 2 P (dx) = 0.
n X
n→∞ −
Z
(cid:12) (cid:12)
Proof (X,Y) is multivariate norm(cid:12)ally distributed(cid:12), hence we can obtain an explicit formu-
lation of (7) by the property of the multivariate normal distribution:
m2 (Z)= E X D,Z,N = Σ2 Σ2 −1 X⊤,
n T |Fs 12 22
where (cid:2) (cid:3) (cid:0) (cid:1)
.
.
.
Σ2 ⊤ =  Cov T f(s)dBH, tif(s)dBH  RN,
12 0 s 0 s ∈
(cid:0) (cid:1)   h R . . . R i  
 
 ti tj 
Σ2 = Cov f(s)dBH, f(s)dBH SN×N.
22 s s ∈
(cid:18) (cid:20)Z0 Z0 (cid:21)(cid:19)i,j=1,...,N
By Lemma 7, it is clear that the regression function m2() (l, ), and we complete the
· ∈ H P
proof.
Remark 12 Define C,Z := σ Z , v [0,s] . Notice that C,Z = C,BH and therefore
s v s s
F { ∈ } F F
E Z C,Z = E Z C,BH . However, D,Z,N = D,BH,N with the consequence that
T s T s s s
| F | F F 6 F
Eh
Z
D,Zi,N =hE
Z
D,BiH,N
. Beyond that, almost surely (a.s.) as N (see,
T s T s
| F 6 |F → ∞
e.gh., (Dudley, 2i002, Lhemma 9.2.4.)),i
E Z D,Z,N E Z C,Z .
T s T s
| F → | F
E Z C,BH plays a rol(cid:2) e in some res(cid:3) earch f(cid:2) rameworks(cid:3) on the fBm, such as the mar-
T s
| F
tingalhe approach foir the fBm and related path dependent partial differential equations pro-
posed by Viens and Zhang (2019) and the framework for analyzing stochastic volatility prob-
lems proposed by Garnier and Sølna (2017). Calculating the element, however, presents
10Predicting path-dependent processes by deep learning
two difficulties. Firstly, the theories use filtration generated from continuous observations,
whereas real observations are always discrete. Secondly, the fBm driving Z cannot be di-
t
rectly observed.
The method proposed in the paper provides a way to address the problems. The deep
neural network’s input is observable Z rather than BH. In addition, as mentioned in the
t t
introduction, deep neural networks have advantages in processing high-dimensional data and
can perform better in the case of large N compared to traditional methods. These advantages
can also be preserved as we further extend the applicability of the framework.
3.2 Prediction of discrete-time stochastic processes related to fractional
Brownian motion
The first consideration is to predict the solution of a linear stochastic differential equation
with the fBm. Let (A , t 0) be the real-valued stochastic process that is the solution of
t
≥
the stochastic differential equation
dA = µ(t)A dt+dBH,
t t t
(8)
A = a ,
0 0
where a R, µ : R R is bounded and measurable. It can be verified that A has an
0 + t
∈ →
explicit solution
t
A
t
= a 0e R0tµ(s)ds + e Rstµ(s)dsdB sH, (9)
Z0
and thus σ A , v [0,s] = σ BH, v [0,s] . Notice that A is Gaussian, so we can
v v t
{ ∈ } { ∈ }
obtain the proposition for
m3(A) = E A D,A,N (10)
T s
| F
in a similar way, where D,A,N := σ A ,(cid:2)...,A for(cid:3)0 < t < < t = s < T,
Fs
{
t1 tN} 1
···
N
A= (A ,...,A ).
t1 tN
Proposition 13 Let BH, t [0,T] and BH, t [0,T] , i = 1,...,n, be independent
t ∈ t,i ∈
fractional Brownian m(cid:0)otions. Then (A(cid:1) , t
(cid:16)
[0,T]) and
A(cid:17)H,
t [0,T] , i= 1,...,n, are
t ∈ t,i ∈
independent stochastic processes, and (X,Y), (X 1,Y 1), (cid:16)..., (X n,Y n) a(cid:17)re independent and
identically distibuted random vectors, where
t t
A
t
= a 0e R0tµ(s)ds + e Rstµ(s)dsdB sH, AH
t,i
= a 0e R0tµ(s)ds + e Rstµ(s)dsdB sH ,i,
Z0 Z0
Y = A , Y = A , X = (A ,...,A ), X = (A ,...,A ).
T i T,i t1 tN i t1,i tN,i
Let m3 = T m˜3 for some c > 0 sufficiently large, where
n clog(n) n
n
1 2
m˜3() := arg min yθ(X ) Y .
n · yθ∈Y(Ln,rn) n
Xi=1(cid:12)
i − i
(cid:12)
(cid:12) (cid:12)
Then (cid:12) (cid:12)
lim E m3(x) m3(x) 2 P (dx) = 0.
n X
n→∞ −
Z
(cid:12) (cid:12)
(cid:12) (cid:12)
11Zheng and Han
Proof Notingthat(X,Y)isalsomultivariatenormallydistributed,theproofofProposition
13 is completed by Lemma 7, since
m3(A) = E A
T
|FsD,A,N = a 0e R0Tµ(s)ds +Σ3
12
Σ3
22
−1 X⊤, (11)
where (cid:2) (cid:3) (cid:0) (cid:1)
.
.
.
Σ3
12
⊤ =  Cov 0T e RsTµ(s)dsdB sH, 0tie Rstiµ(s)dsdB sH 
∈
RN,
(cid:0) (cid:1)   h R . . . R i  
 
Σ3
22
= Cov
ti
e Rstiµ(s)dsdB sH,
tj
e Rstjµ(s)dsdB sH

∈
SN×N.
(cid:18) (cid:20)Z0 Z0 (cid:21)(cid:19)i,j=1,...,N
and the regression function m3() (l, )
· ∈ H P
Remark 14 The Proposition 13 holds for all linear stochastic differential equations with
the fBm, including fOU process, which is the solution of the stochastic differential equation
dA = (k(t) a(t)A )dt+σ(t)dBH,
t t t
− (12)
A = a ,
0 0
where k, a, σ are bounded, measurable functions.
Consider a more general pathwise stochastic differential equation with the fBm, which
can represent some more general non-Gaussian cases:
dR = µ(R )dt+σ(R )dBH,
t t t t
(13)
R = r ,
0 0
for suitable coefficient functions µ(), σ() and a constant r . For H 1,1 , solutions to
· · 0 ∈ 2
(13) are given by
(cid:0) (cid:1)
R = f(A ),
t t
dA = aA dt+dBH, (14)
t t t
−
A = f−1(R ),
0 0
forsomemonotoneanddifferentiablef :R Randa > 0(see,Buchmann and Klu¨ppelberg
→
(2006)). For H (1/2,1), the fCIR process is the pathwise solution to the stochastic dif-
∈
ferential eqaution
dR = λR dt+σ R dBH,
t − t | t | t (15)
R = r ,
0 0 p
where λ,σ,r > 0. A solution of (15) is given by
0
R = f(A ),
t t
λ
dA = A dt+dBH, (16)
t −2 t t
A = f−1(R ),
0 0
12Predicting path-dependent processes by deep learning
where f(x) = sgn(x)σ2x2/4 (see, e.g., (Buchmann and Klu¨ppelberg, 2006, Proposition
5.7)).
The fCIR process can be written as f(X ), where f is a given function and X is a
t t
Gaussian process, but f is not a strictly monotone increasing function. It means that we
cannot complete the proof in the same way as before. For a more specific reason, we assert
that
m4(R)= E R D,R,N (17)
T s
| F
D,R,N
is not a continuous function anymore, where(cid:2)
Fs
:= σ(cid:3) {R t1,...,R
tN}
for 0 < t
1
<
···
<
t = s < T, R = (R ,...,R ). To address this issue, we introduce the edge function
N t1 tN
based on the notion of piecewise smooth functions proposed by Imaizumi and Fukumizu
(2019) and boundary fragment classes developed by Dudley (1974). In Appendix A, we
introduce a special class of functions that take 1 on some regions of the state space whose
boundaries are formed by a series of smooth functions, and demonstrate the existence of
neural networks that can approximate this class of functions.
Proposition 15 Let BH, t [0,T] and BH, t [0,T] , i = 1,...,n, be independent
t ∈ t,i ∈
fractional Brownian m (cid:0)otions. Then ( (cid:1)R t, t (cid:16)[0,T]) and (R(cid:17)t,i, t [0,T]), i = 1,...,n, are
∈ ∈
independent stochastic processes, and (X,Y), (X ,Y ), ..., (X ,Y ) are independent and
1 1 n n
identically distibuted random vectors, where R and R , i = 1,...,n, are respectively solu-
t t,i
tions of stochastic differential eqautions (16) with BH and BH for i = 1,...,n,
t t,i
Y = R , Y = R , X = (R ,...,R ), X = (R ,...,R ).
T i T,i t1 tN i t1,i tN,i
Let m4 = T m˜4 for some c > 0 sufficiently large, where
n clog(n) n
n
1 2
m˜4() := arg min yθ(X ) Y
n · yθ∈Y(Ln+L,rn+r)n
Xi=1(cid:12)
i − i
(cid:12)
(cid:12) (cid:12)
for some L, r sufficiently large. Then (cid:12) (cid:12)
lim E m4(x) m4(x) 2 P (dx) = 0.
n X
n→∞ −
Z
(cid:12) (cid:12)
Proof For r = (r ,...,r ) [0,+(cid:12) )N , m4(r) =(cid:12)E[R R = r ,...,R = r ]. When
1 N
∈ ∞
T
|
t1 1 tN N
r ,...,r > 0,
1 N
m4(r) = E[R R = r ,...,R = r ]
T
|
t1 1 tN N
sgn(A )σ2A2 4r 4r
= E T T A = 1 ,...,A = N
4 | t1 σ2 tN σ2
" r r #
+∞ σ2u2 −(u−µb)2
= e 2σb2 du,
Z0
4√2πσ2
where
b 4r 4r
µ = E A A = 1 ,...,A = N
T | t1 σ2 tN σ2
" r r #
b = 4 σr 20 e−λ 2T +Σ3
12
Σ3
22
−1 (a −µ 2)⊤,
r
(cid:0) (cid:1)
13Zheng and Han
4r 4r
σ2 = Var A A = 1 ,...,A = N
T | t1 σ2 tN σ2
" r r #
b = Σ3 Σ3 Σ3 −1 Σ ,
11− 12 22 21
. (cid:0). (cid:1)
. .
. .
T
a =  4 σr 2i , µ
2
=  4 σr 20e−λ 2ti 
∈
RN, Σ3
11
= Var e−λ(T 2−s) dB sH ,
 q. .   q . .  (cid:20)Z0 (cid:21)
 .   . 
   
   
.
.
.
Σ3
12
⊤ = Σ3
21
=  Cov 0T e−λ(T 2−s) dB sH, 0tie−λ(ti 2−s) dB sH 
∈
RN,
 (cid:20) (cid:21) 
(cid:0) (cid:1) (cid:0) (cid:1)  R . . R 
 . 
 
 
Σ3
22
= Cov
ti e−λ(ti 2−s)
dB sH,
tj e−λ(tj 2−s)
dB sH
∈
SN×N.
" Z0 Z0 #!
i,j=1,...,N
Here we consider the case where not all r , j = 1,...,Nare greater than zero. Without
j
loss of generality, we set r ,...,r > 0 and r ,...,r = 0.
1 M M+1 N
m4(r)=E[RT |Rt
1
=r1,...,RtN =rN]
=E "sgn(AT 4)σ2A2 T |At
1
= r4 σr 21 ,...,AtM = r4 σrM
2
,AtM+1 ≤0,...,AtN ≤0
#
+∞ σ2u2 −0 ∞... −0 ∞fx(u,a1,...,aM,xM+1,...,xN)dxM+1...dxN
= du,
Z0 4 −+ ∞∞ R−0 ∞...R−0 ∞fx(xT,a1,...,aM,xM+1,...,xN)dxM+1...dxNdxT
+∞ 0 0
=C
R
...
R u2fR
x(u,a1,...,aM,xM+1,...,xN)dxM+1...dxNdu,
Z0 Z−∞ Z−∞
where C is some constant, a = 4ri for i = 1,...,M,
i σ2
q
f x(x T,x 1,...x N)=
1 e−(x−µ)⊤Σ 2−1(x−µ)
,
(2π)N+1 Σ
| |
p
x
T
x
1
 
x =

. . . , µ = µ µ1 , Σ = Σ Σ3 1 31 Σ Σ3 1 32 , µ
1
= 4 σr 20 e−λ 2T .
 x i  (cid:18) 2 (cid:19) (cid:18) 21 22 (cid:19) r
 
 . . 
 . 
 
 
Althoughm4( )isnota(p,C)-smoothfunctionorevencontinuousfunctionin[0,+ )N.
But it is able to d· ivide [0,+ )N into regions such that in each region m4() is a (p∞ ,C)-
∞ ·
smooth function. Further, in each region, m4( ) (l, ). By Lemma 18 and Theorem 1
· ∈ H P
of Kohler and Langer (2021), there exist a neural network (L,r) that makes it possible to
14Predicting path-dependent processes by deep learning
divide [0,+ )N into regions, and a neural network yθ (L ,r ) that approximates the
n n
∞ ∈ Y
regression function in each region. Concatenate two neural networks in series and the proof
is complete.
Remark 16 From the above discussion, we can conclude that our proposed framework for
predicting path-dependent processes W is broadly applicable to situations where the relation-
t
ship between discrete observations of V and W can be described by a series of (p,C)-smooth
t t
functions. Therefore, we can also extend the framework to some cases where V = W . A
t t
6
simple example is that observed historical information is attached with independent noise,
similar to Kalman filtering, e.g., W = BH, V = BH +B , where BH and B are inde-
t t t t t t t
pendent. Moreover, Lemma 18 allows us to divide several input regions and consider the
input-output relationship mentioned in Remark 4 case by case.
4 Numerical simulation
In this section, we aim to demonstrate the effectiveness of the framework proposed in the
paper through numerical simulation. We apply the method to two examples: the fBm and
the fOUprocess. We want to show that the method is accurate underdiscrete observations.
Amethodistogenerate independentsamplepaths(S , t [0,T])anduseatrainedneural
t,i
∈
network yˆθ to predict each path separately. We can get the error between the predicted
valueyˆθ(S ,...,S )andthetruevalueofthesamplepathsS ,andcalculate themean
t1,i tN,i T,i
error (ME)
N′
1
S yˆθ(S ,...,S )
N′ T,i − t1,i tN,i
Xi=1(cid:16) (cid:17)
and mean square error (MSE)
N′
1 2
S yˆθ(S ,...,S ) .
N′ T,i − t1,i tN,i
Xi=1(cid:16) (cid:17)
Using MSE as an evaluation metric, we compare the theoretically optimal predictor with
the obtained deep neural network and also analyze the impact of factors such as prediction
period, Hurst index, etc. on the accuracy.
Moreover, weintendtodemonstratethatas N ,wecanapproximate theprediction
→ ∞
under continuous observation E S C,S with the prediction under discrete observation
T s
|F
yˆθ(S ,...,S ). We therefore phresent somie prediction theories with continuous filtrations
t1 tN
and use the theoretical output as benchmarks.
In the deep neural network training process, for each case, we trained 3000 batches,
generating 212 sample paths per batch for training. We adopt a decreasing learning rate
strategy, with an initial learning rate of 0.01, and the learning rate decreases to 0.95 of the
originallearningrateforevery 10trainingbatches. Wegenerate 10000 independentsamples
of each to test the method, i.e., N′ = 10000.
15Zheng and Han
4.1 Fractional Brownian motion
In Table 1, we demonstrate the results of predicting the fBms with different numbers of
equally spaced grid points N, Hurst index H and s. The MEs fluctuate within a small
range around zero. This suggests that, as a whole, our predictions are not yielding large
shifts. The MSEs show a decreasing trend as s increases. This is consistent with the fact
that the farther into the futureit is, the more difficult it is to predict accurately. It is worth
noting that the MSEs of the predictions are not significantly different as the number of
equally spaced grid points changes. This suggests that not the more historical information
isavailablefromdiscreteobservations,thesmallerthepredictionbias. Discreteobservations
here refer to the entirety of the historical information available to us, to be distinguished
from the case where a portion of our known information is taken for prediction purposes.
Taking the H = 0.5 case as a comparison, i.e., the sBm case, firstly the MSE is larger in
H = 0.1,0.3,0.9 cases but smaller in H = 0.7 case, which implies that the path dependence
may lead to larger or smaller prediction deviations. When s = 2, the MSE in the H = 0.1
case is smaller than the MSE in the H = 0.9 case. When s = 8, the situation is reversed.
TheforecasttimehorizonaffectstheroleoftheHurstindexonforecastaccuracy. InTable2,
let s = 5, N = 212 and adjust H, T. As T increases from 5.5 to 10, the MSE minimum case
changes from H = 0.8 to H = 0.7, and the MSE difference multiples for the H = 0.1,0.9
cases become smaller. This is consistent with the analysis results derived from Table 1.
In Figure 1, we compare the MSEs of the theoretically optimal predictor (6) with the
MSEs of the trained deep neural network. The MSE gap, although slightly larger with
increasing T, is always confined within a small range near 0. It demonstrates that the
predictions obtained through our proposed framework are not significantly different from
the theoretical optimal predictions, hence demonstrating the reliability of the method.
MSEs of the trained deep neural networks MSEs of the optimal predictors Difference in MSEs between the two methods
15 15 4
2
10 10
0
5 5
-2
0 0 -4
0.8 0.8 0.8
0.6 10 0.6 10 0.6 10
0.4 8 0.4 8 0.4 8
0.2 6 0.2 6 0.2 6
Figure 1: MSEs of the two methods in the fBm cases for s = 5, N = 212 and difference.
Finally, given sample paths, predictions are made based on different numbers of equally
spaced grid points in Figure 2. Consider the prediction of the fBm with continuous obser-
vations, i.e.,
E BH C,BH .
T s
|F
h i
16Predicting path-dependent processes by deep learning
N ME MSE ME MSE ME MSE ME MSE ME MSE
H =0.1 H =0.3 H =0.5 H =0.7 H =0.9
s=2
29 -0.042 20.978 0.020 19.517 0.030 18.370 -0.010 17.141 0.035 27.022
210 0.043 20.993 0.004 20.456 0.035 18.519 -0.054 17.136 0.037 26.476
211 -0.026 21.382 0.007 20.287 0.052 18.504 0.008 16.835 0.024 26.511
212 0.021 20.871 -0.007 20.240 0.026 18.484 0.007 16.730 -0.105 26.968
213 -0.079 21.081 0.043 20.022 -0.025 18.056 0.071 17.121 0.030 26.553
214 0.084 20.726 0.031 19.766 -0.005 18.648 -0.034 16.531 -0.012 26.655
215 0.065 20.463 -0.050 19.930 0.007 18.516 0.041 16.719 -0.023 26.131
216 -0.129 21.074 0.021 20.306 0.011 18.071 -0.012 16.902 0.056 26.460
s=5
29 0.094 13.660 0.079 11.591 -0.017 9.550 0.061 8.349 0.012 11.338
210 -0.062 13.749 0.051 11.485 -0.007 9.414 -0.011 8.350 -0.022 10.843
211 -0.027 13.488 -0.026 11.609 -0.004 9.466 -0.028 8.399 -0.006 11.045
212 -0.027 13.578 0.012 11.734 0.030 9.435 -0.022 8.267 0.058 10.828
213 0.026 13.748 -0.027 11.584 0.035 9.544 0.012 8.273 -0.005 11.026
214 -0.011 13.775 -0.025 11.511 -0.003 9.418 0.015 8.309 0.025 11.015
215 0.059 13.603 0.002 11.704 -0.004 9.582 0.029 8.194 0.012 11.009
216 0.013 13.738 -0.016 12.009 -0.034 9.506 -0.037 8.268 0.014 11.115
s=8
29 -0.042 5.266 0.017 3.689 -0.025 2.585 0.020 2.247 0.006 2.635
210 0.010 5.407 0.013 3.655 -0.023 2.617 -0.016 2.267 0.034 2.656
211 0.000 5.424 -0.015 3.684 0.010 2.699 -0.039 2.277 0.022 2.816
212 0.029 5.424 0.023 3.632 0.011 2.627 -0.013 2.300 -0.004 2.693
213 0.028 5.294 0.001 3.745 0.019 2.643 -0.010 2.327 0.004 2.765
214 0.024 5.356 0.010 3.654 -0.001 2.625 -0.028 2.237 -0.010 2.683
215 0.001 5.140 -0.039 3.692 -0.018 2.622 0.002 2.254 -0.030 2.696
216 0.019 5.451 0.014 3.687 0.020 2.632 -0.005 2.296 -0.002 2.705
Table 1: MEs and MSEs in the fBm cases with different N, H, s for T = 10.
H 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
T = 5.5 1.046 0.764 0.582 0.471 0.383 0.338 0.332 0.331 0.379
T = 6 2.226 1.721 1.412 1.179 1.004 0.892 0.860 0.887 1.036
T = 6.5 3.417 2.847 2.402 2.053 1.775 1.612 1.527 1.561 1.816
T = 7 4.773 4.040 3.441 3.020 2.646 2.394 2.270 2.308 2.768
T = 7.5 6.127 5.356 4.817 4.170 3.554 3.273 3.169 3.273 3.926
T = 8 7.651 6.617 5.974 5.235 4.578 4.254 4.033 4.170 5.127
T = 8.5 8.929 8.204 7.418 6.385 5.827 5.260 5.215 5.459 6.604
T = 9 10.638 9.574 8.744 7.916 6.921 6.361 5.992 6.515 7.886
T = 9.5 12.012 11.419 10.224 9.246 8.344 7.651 7.136 7.662 9.594
T = 10 13.487 12.350 11.702 10.616 9.625 8.658 8.557 9.006 10.738
Table 2: MSEs in the fBm cases with different H, T for s = 5, N = 212.
17Zheng and Han
ThispredictionproblemiswellsolvedbyGripenberg and Norros(1996)andPipiras and Taqqu
(2001):
s
E BH C,BH = BH + ΨH(s,T,v)dBH,
T s s v
|F
h i
Z0
where, for v (0,s),
∈
ΨH(s,T,v) =
sin H
−
1
2
π
v−H+ 21 (s v)−H+ 21
T zH−1 2(z −s)H− 21
dz,
π − z v
(cid:0)(cid:0) (cid:1) (cid:1) Zs −
and, for v 0,s , we have ΨH(s,T,v) = 0.
∈ { }
Let s = 5 and T = 10. Given sample paths, Figure 2 shows that the predictions based
on discrete observations obtained through the deep neural network method converge to
predictions based on continuous observations. When H = 0.1,0.3, convergence is faster.
4.2 Fractional Ornstein-Uhlenbeck process
Let k() = a = 0, a() = 1, and σ() = 1. In Table 3, we demonstrate the results of
· 0 · 2 ·
predicting the fOU processes with different numbers of equally spaced grid points N, Hurst
index H and s. The MEs also fluctuate within a small range around zero. This suggests
that, as a whole, our predictions are not yielding large shifts. The MSEs show a decreasing
trend as s increases. It is consistent with the fact that the farther into the future it is,
the more difficult it is to predict accurately. The MSEs of the predictions are also not
significantly different as the number of equally spaced grid points changes.
It is not the case that the larger or smaller the H, the smaller the MSE is. When s = 2,
the MSEs are minimum for the H = 0.7 case. When s = 5,8, the MSEs decrease with
increasing H. It implies that the path dependence may lead to larger or smaller prediction
deviations and the forecast time horizon jointly affects them. In Table 4, let s = 5, N = 212
and adjust H, T. The MSEs decrease with increasing H and the MSE difference multiples
for the H = 0.1,0.9 cases become smaller. This is consistent with the analysis results
derived from Table 3.
In Figure 3, we take the MSEs of the theoretically optimal predictor (11) as the bench-
marks, which are the smallest MSEs we can theoretically obtain. By Proposotion 6.3 in Hu
(2005),
Cov
T
e−T− 2s dB sH,
ti
e−ti 2−s dB sH
(cid:20)Z0 Z0 (cid:21)
=E
T
e−T− 2s dB sH
ti
e−ti 2−s dB sH
(cid:20)Z0 Z0 (cid:21)
T T
=E e−T− 2s dB sH e−ti 2−s 1 [0,ti](s)dB sH
(cid:20)Z0 Z0 (cid:21)
T
= Γ∗ H,Te−T− 2s Γ∗ H,Te−ti 2−s 1 [0,ti](s)ds,
Z0
where
1 T
Γ∗ H,Tf(s)= H
− 2
κ Hs21−H uH− 21 (u −s)H− 23 f(u)du.
(cid:18) (cid:19) Zs
18Predicting path-dependent processes by deep learning
Sample paths for fBms
2 -1.7
0 -1.8
-2 -1.9
-4 -2
0 1 2 3 4 5 6 8 10 12 14 16
1 0.34
0.5
0.32
0
0.3
-0.5
-1 0.28
0 1 2 3 4 5 6 8 10 12 14 16
4 0.3
2 0.25
0 0.2
-2 0.15
0 1 2 3 4 5 6 8 10 12 14 16
2 8
1 7.5
0 7
-1 6.5
-2 6
0 1 2 3 4 5 6 8 10 12 14 16
Predictions based on continuous observations
Predictions based on discrete observations
Figure 2: Sample paths, predictions based on discrete observations and continuous obser-
vations in the fBm cases for s = 5, T = 10.
19Zheng and Han
N ME MSE ME MSE ME MSE ME MSE ME MSE
H =0.1 H =0.3 H =0.5 H =0.7 H =0.9
s=2
29 0.012 16.003 0.014 15.655 0.038 14.610 0.004 14.145 -0.009 16.472
210 0.054 15.824 0.015 15.660 -0.009 14.486 0.054 13.759 -0.057 16.629
211 0.013 15.665 -0.014 16.038 -0.002 14.697 -0.020 14.175 0.032 16.414
212 -0.031 15.446 0.020 15.281 0.036 14.617 0.059 14.111 -0.063 16.872
213 0.074 15.559 0.026 15.730 0.027 14.163 -0.024 13.932 -0.105 16.244
214 0.015 15.528 -0.058 15.359 -0.020 14.545 -0.007 13.921 0.053 16.476
215 0.014 15.960 0.071 15.770 0.050 14.657 0.055 14.232 0.002 16.523
216 0.077 15.591 0.019 15.295 0.001 14.681 0.040 13.827 -0.020 16.627
s=5
29 -0.024 15.329 -0.031 14.383 -0.034 13.195 0.025 10.013 0.022 7.659
210 -0.038 15.397 0.042 14.268 -0.031 13.351 -0.043 10.139 0.025 7.670
211 0.040 15.004 0.044 14.226 -0.048 13.290 0.011 10.193 0.003 7.604
212 0.026 14.708 -0.004 13.706 0.046 13.160 0.004 10.123 0.033 7.521
213 -0.018 14.973 0.028 14.225 -0.036 13.328 -0.025 10.059 -0.013 7.355
214 0.016 15.019 0.012 14.242 -0.036 13.348 0.011 10.320 0.025 7.644
215 -0.019 14.934 0.046 14.364 -0.012 13.273 -0.040 10.194 -0.028 7.680
216 0.035 14.929 0.044 14.359 0.023 13.246 -0.068 10.294 0.003 7.702
s=8
29 0.006 10.017 -0.055 8.193 -0.015 5.597 -0.012 3.591 -0.023 2.402
210 -0.040 10.182 0.022 8.188 -0.003 5.646 -0.012 3.742 0.008 2.347
211 -0.007 10.423 -0.005 8.254 0.011 5.608 -0.004 3.660 -0.015 2.352
212 -0.047 10.278 -0.004 8.130 -0.004 5.518 0.038 3.588 -0.009 2.358
213 -0.032 10.299 0.001 8.116 0.009 5.592 0.012 3.671 -0.020 2.349
214 -0.005 10.126 0.000 7.986 -0.020 5.594 0.008 3.573 0.001 2.440
215 0.036 10.177 0.011 8.190 0.006 5.604 0.013 3.635 0.010 2.404
216 -0.002 10.400 0.021 8.230 -0.021 5.576 0.013 3.692 0.004 2.361
Table 3: MEs and MSEs in the fOU process cases with different N, H, s for T = 10.
H 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
T = 5.5 1.561 1.268 0.957 0.755 0.453 0.436 0.376 0.335 0.367
T = 6 3.320 2.945 2.359 2.004 1.524 1.262 1.034 0.850 0.849
T = 6.5 5.106 4.504 3.963 3.345 2.773 2.341 1.848 1.576 1.443
T = 7 6.859 6.273 5.627 4.937 4.321 3.551 2.866 2.370 2.142
T = 7.5 8.428 7.829 7.247 6.488 5.897 4.753 3.788 3.249 2.874
T = 8 9.727 9.270 8.605 7.894 7.333 6.087 5.189 4.307 3.802
T = 8.5 11.121 10.630 10.094 9.333 8.820 7.566 6.281 5.379 4.534
T = 9 12.522 12.161 11.575 10.900 10.426 8.919 7.605 6.400 5.603
T = 9.5 13.747 13.549 12.887 12.363 11.933 10.248 8.617 7.608 6.591
T = 10 15.049 14.701 14.254 13.616 13.219 11.675 10.309 8.830 7.571
Table 4: MSEs in the fOU process cases with different H, T for s= 5, N = 212.
20Predicting path-dependent processes by deep learning
Similarly, we can do the same for Cov 0tie Rstiµ(s)dsdB sH, 0tj e Rstjµ(s)dsdB sH , and conse-
quently get each item in (11). h i
R R
In Figure 3, we compare the MSEs of the theoretically optimal predictor (11) with the
MSEs of the trained deep neural network. The MSE gap is confined within a small range
near 0, which demonstrates that the predictions obtained through our proposed framework
are not significantly different from the theoretical optimal predictions.
MSEs of the trained deep neural networks MSEs of the optimal predictors Difference in MSEs between the two methods
20 20 4
2
10 10 0
-2
0 0 -4
0.8 0.8 0.8
0.6 10 0.6 10 0.6 10
0.4 8 0.4 8 0.4 8
0.2 6 0.2 6 0.2 6
Figure 3: MSEs of the two methods in the fOU process cases for s = 5, N = 212 and
difference.
Consider the predictions of the fOU processes based on continuous observations, i.e.,
E A C,A .
T s
|F
(cid:2) (cid:3)
where A is the solution of the SDE (12) with k() = a = 0, a() = 1, and σ() = 1. This
t · 0 · 2 ·
prediction problem is well solved by Fink et al. (2013). The solution A is given by
t
t
A
t
= e−t− 2v dB vH,
Z0
and
s
E A
T
sC,A = A se−T− 2s + ΨH
c
(s,T,v)dB vH,
| F
Z0
(cid:2) (cid:3)
where for v (0,s),
∈
ΨH
c
(s,T,v) = sin H π− 1 2 π v−H+1 2(s −v)−H+ 21 T zH−1 2( zz − vs)H− 21 e−T− 2v dz,
(cid:0)(cid:0) (cid:1) (cid:1) Zs −
and for v 0,s , we have ΨH(s,T,v) = 0.
c
∈ { }
Let s = 5 and T = 10. Given sample paths, Figure 4 shows that the predictions
based on discrete observations obtained through the deep neural network method converge
to predictions based on continuous observations. When H = 0.1,0.3,0.7, convergence is
faster.
21Zheng and Han
Sample paths for fOU processes
6 0.446
4 0.4455
2 0.445
0 0.4445
-2 0.444
0 1 2 3 4 5 6 8 10 12 14 16
1 -0.0108
-0.011
0
-0.0112
-1
-0.0114
-2 -0.0116
0 1 2 3 4 5 6 8 10 12 14 16
2 -0.59
-0.592
0
-0.594
-2
-0.596
-4 -0.598
0 1 2 3 4 5 6 8 10 12 14 16
3 1.65
2
1.6
1
1.55
0
-1 1.5
0 1 2 3 4 5 6 8 10 12 14 16
Predictions based on continuous observations
Predictions based on discrete observations
Figure 4: Sample paths, predictions based on discrete observations and continuous obser-
vations in the fOU process cases for s = 5, T = 10.
22Predicting path-dependent processes by deep learning
5 Conclusions
In this paper, we propose a deep learning method to predict path-dependent processes by
discretely observing historical information. This method is implemented by considering
the prediction as a nonparametric regression and obtaining the regression function through
simulatedsamplesanddeepneuralnetworks. Wetheoreticallydemonstratetheapplicability
ofthemethodtofBmandthesolutionsofsomestochasticdifferentialequationsdrivenbyit,
and further discuss the scope of the method. Our proposed framework for predicting path-
dependent processes W is broadly applicable to situations where the relationship between
t
discrete observations of V and W can be described by a series of (p,C)-smooth functions.
t t
By dividing several input regions, we can also consider the input-output relationship case
by case. Moreover, with the frequency of discrete observations tending to infinity, the
predictions based on discrete observations converge to the predictions based on continuous
observations.
In numerical simulation, we apply the method to the fBm and the fOU process as ex-
amples. The further into the future one needs to predict, the more difficult to predict
accurately. It is not the case that the larger or smaller the H, the smaller the prediction
error is. The forecast time horizon affects the role of the Hurst index on forecast accuracy.
Moreover, it is worth noting that the not the more historical information is available from
discrete observations, the smaller the prediction bias. Comparing the results with the the-
oretical optimal predictions and taking the MSE as a measure, the numerical simulations
demonstrate that the method can generate accurate results. Comparing the predictions
based on discrete observations and continuous observations, the predictions based on dis-
creteobservationsobtainedthroughthedeepneuralnetworkmethodconvergetopredictions
based on continuous observations, which implies that we can make approximations by the
method.
Acknowledgments and Disclosure of Funding
The authors are grateful for financial support from the National Key R&D Program of
China (No. 2023YFA1009200).
23Zheng and Han
Appendix A.
Based on the notion of piecewise smooth functions proposed by Imaizumi and Fukumizu
(2019) and boundary fragment classes developed by Dudley (1974), we introduce a special
classoffunctionsthattake1onsomeregionsofthestatespacewhoseboundariesareformed
by a series of smooth functions, and demonstrate the existence of neural networks that can
approximate this class of functions.
Definition 17 For λ,J,R N and [1, ) N, we define
∈ P ⊆ ∞ ×
R
(R, ) := f (x) = f (x) :f (x) (p ,D ) and (p ,D ) ,
i i i i i i
G P ∈ H ∈ P
( )
i=1
Y
J
(λ,J, ) := f (x)= max f (x) :f (x) (R , ),λ = R , .
j j j j j j
K P  1≤j≤J ∈ G P P ⊆ P
 Xj=1 
Obviously, the functions in the (λ,J, ) take 1 for some blocks and 0 forthe rest
K P
of the regions and the functions themselves are not differentiable or even discontinuous at
the edges of these regions. It is worth noting that each edge function can have a different
smoothness p = q+s and a different input dimension D.
Lemma 18 Let the training sets (X,Y),(X ,Y ),...,(X ,Y ) be independent and identi-
1 1 n n
cally distributed random values such that supp(X) is bounded and E ecY2 < for some
∞
constant c > 0. Let f (λ,J, ), where λ,J N, [1, ) hN, anid Q
i
N, i =
∈ K P ∈ P ⊆ ∞ × ∈
1,...,λ, sufficiently large. Then, there exist D , p , q , R , i = 1,...,λ, and a neural
i i i i
network fθ (L,r) with the property that
∈ Y
f(x) fθ(x) c
c4(max0≤i≤λqi+1)
max Q−2pi,
k − k2,[−c1,c1]d ≤ 2 1 0≤i≤λ i
where c and c are constants,
1 2
L = max 5QDi + log Q2pi+4Di(qi+1) e4(qi+1) (cid:16)QD i i−1 (cid:17)
i 4 i
1≤i≤λ
(cid:20) (cid:24) (cid:18) (cid:19)(cid:25)
log (max q ,D +1) + log
Q2pi
+4+ log d ,
2 1≤i≤λ{ i i } 4 i ⌈ 2 ⌉
(cid:24) (cid:25) (cid:21)
l (cid:16) (cid:17)m
λ
D +q
r = 132 2Di eDi i i max q +1,D 2 .
i i
· D i 1≤i≤λ
i=1 (cid:18) (cid:19)
X (cid:6) (cid:7) (cid:8) (cid:9)
Proof By the definition of (λ,J, ), for arbitrary f (x) (λ,J, ), there exists a set
K P J ∈ K P
of (p,C)-smooth functions f (x),i = 1,..., R , such that
i j=1 j
P
Pj m=1Rm
φ = f (x), φ = 1 (φ ), φ = φ ,
1,i i 2,i (0,∞) 1,i 3,j 2,i
i= Pj m− Y=1 1Rm+1
24Predicting path-dependent processes by deep learning
J
φ = φ , f(x)= 1 (φ ).
4 3,j (0,∞) 4
j=1
X
J
Thus, there exists a set of functions f (x), i = 1,..., R , which is (p ,C)-smooth
i j=1 j i
respectively, p = q +s for some q N and 0 < s 1. Based on Theorem 2 introduced
i i i i 0 i
by Kohler and Langer (2021), there e∈ xists φˆ (L≤ ,rP ) such that
1,i i i
∈ Y
f (x) φˆ (x) cic 4(qi+1)Q−2pi,
i − 1,i ∞,[−c1,c1]d ≤ 3 1 i
(cid:13) (cid:13)
(cid:13) (cid:13)
where ci is a constant,(cid:13) (cid:13)
3
L =5QDi + log Q2pi+4Di(qi+1) e4(qi+1) (cid:16)QD i i−1 (cid:17) log (max q ,D +1)
i i 4 i 2 1≤i≤λ{ i i }
(cid:24) (cid:18) (cid:19)(cid:25)(cid:24) (cid:25)
+ log
Q2pi
,
4 i
l (cid:16) (cid:17)m
D +q
r =132 2Di eDi i i max q +1,D 2 .
i i i
· D i 1≤i≤λ
(cid:18) (cid:19)
(cid:6) (cid:7) (cid:8) (cid:9)
Withoutloss of generality, for1 u D , let x = x , g = f +x andφ′ = φˆ +x ,
≤ ≤ i i1 u i i 1 1,i 1,i 1
2
1 1
{fi>0} − {φˆ 1,i>0
}
2,[−c1,c1]d
(cid:13) (cid:13)
(cid:13) (cid:13) 2
=(cid:13) 1 {fi>0} −1 φˆ 1,i>(cid:13) 0 dx
Z c(cid:16) 1 c1 { } (cid:17)
= ... 1 +1 dx ...dx .
fi>0, φˆ 1,i≤0 fi≤0, φˆ 1,i>0 1 Di
Z−c1 Z−c1 { } { }
For fixed (x ,...,x ) [ c ,c ]Di−1, 1 = 1 . Thus,
2 Di ∈ − 1 1 {fi>0, φˆ 1,i<0
}
[φ′ 1,t, gi)
c1
1 dx g φ′ 0.
{fi>0, φ1,i≤0} 1
≤
i
−
1,i
∨
Z−c1
(cid:0) (cid:1)
Similarly,
c1
1 dx φ′ g 0.
{fi≤0, φ1,i>0} 1
≤
1,i
−
i
∨
Z−c1
(cid:0) (cid:1)
d
Notice that (b 0)+( b 0) = b , for any x [ c ,c ] , we have
1 1
∨ − ∨ | | ∈ −
c1 c1
... 1 +1 dx ...dx
fi>0, φˆ 1,i≤0 fi≤0, φˆ 1,i>0 1 Di
Z−c1 Z−c1 { } { }
c1 c1
... g φ′ 0 + φ′ g 0 dx ...dx
≤
i
−
1,i
∨
1,i
−
i
∨
2 Di
Z−c1 Z−c1
cic 4(qi+1)Q−(cid:0)2(cid:0)pi (cid:1) (cid:1) (cid:0)(cid:0) (cid:1) (cid:1)
≤ 4 1 i
for a constant ci > 0.
4
25Zheng and Han
Based on Lemma 4 and Lemma 20 in Kohler and Langer (2021), there exists a network
f with the network architecture (1,18) satisfying f (m,n) = m + n m n , for
sq sq
| | − | − |
m,n [0,1]. Let
∈
w = ⌈log 2d ⌉, (z 1,...,z 2w) = (x 1,x 2,...,x d,1,...,1).
In the first layer of f , we compute
mult
f (z ,z ), f (z ,z ),...,f (z , z ),
sq 1 2 sq 3 4 q 2w−1 2q
which can be done by 18 2w−1 18d neurons. The output of the first layer is a vec-
· ≤
tor of length 2w−1. This process is repeated for the output vectors until the output is a
one-dimensional vector. If m = 0, then we have f (m,n)= n n = 0. Based on math-
sq
| |−| |
ematical induction, if a element of the vector x is equal to 0, f (x) = 0. Therefore, for
mult
d
x [0,1] , there exists a neural network f with the network architecture ( log d ,18d)
∈ mult ⌈ 2 ⌉
such that
>0, if x > 0, 0 i d,
i
f (x) ∀ ≤ ≤ (18)
mult
(=0, otherwise.
Let f (x) = σ( c σ(x)+1) + 1, where σ(x) is the ReLU activation function.
demo 2
− −
Then, for x [0,1], there exists a neural network f (3,2) such that
demo
∈ ∈ Y
f (x)= 1 (x), (19)
demo 1 ,∞
(cid:16)c5 (cid:17)
where c 1 is a constant.
5
≥
Let
J Pj i=1Ri
fθ(x)= 1 1 φˆ (x) .
(0,∞)  (0,∞) 1,i 
Xj=1 i= Pj iY=− 11Ri+1(cid:16) (cid:16) (cid:17)(cid:17)

  
By (18) and (19), given a sufficiently small c , 1 (x) and 1 (x) can be imple-
2 (0,∞) (0,∞)
mented through networks f (x) and f (x). Therefore, fθ (L,r), where
mult demo Q(cid:0) (cid:1) ∈ Y
L = max 5QDi + log Q2pi+4Di(qi+1) e4(qi+1) (cid:16)QD i i−1 (cid:17)
i 4 i
1≤i≤λ
(cid:20) (cid:24) (cid:18) (cid:19)(cid:25)
log (max q ,D +1) + log
Q2pi
+4+ log d ,
2 1≤i≤λ{ i i } 4 i ⌈ 2 ⌉
(cid:24) (cid:25) (cid:21)
l (cid:16) (cid:17)m
λ
D +q
r = 132 2Di eDi i i max q +1,D 2 .
i i
· D i 1≤i≤λ
i=1 (cid:18) (cid:19)
X (cid:6) (cid:7) (cid:8) (cid:9)
26Predicting path-dependent processes by deep learning
Moreover,
f(x) fθ(x)
k −
k2,[−c1,c1]d
J Pj i=1Ri Pj i=1Ri
I (x) I (x)
≤ Xj=1(cid:13) (cid:13) (cid:13) (cid:13)i= Pj iY=− 11Ri+1 {fi>0} − i= Pij == Y1−1Ri+1 {φˆ 1,i>0 } (cid:13) (cid:13) (cid:13)
(cid:13)2,[−c1,c1]d
(cid:13) (cid:13)
c m(cid:13)ax f (x) φˆ (x) (cid:13)
5 (cid:13) i 1,i (cid:13)
≤ 1≤i≤λ − 2,[−c1,c1]d
(cid:13) (cid:13)
≤c
6c4 1(max(cid:13) (cid:13)1≤i≤λqi+1)
1m ≤ia ≤x
λQ(cid:13) (cid:13)−
i
2pi,
where c ,c are constants.
5 6
Remark 19 Lemma 18 illustrates that a sufficiently deep neural network can approximate
the function very well. Based on Theorem 2 introduced by Kohler and Langer (2021), a
sufficiently wide neural network can also yield similar results. More specifically, replacing
L and r with
L =9+ log max
Q2pi
log max D ,q +1 +1
4 1≤i≤λ i 2 1≤i≤λ{ i i }
(cid:24) (cid:18) (cid:19)(cid:25)(cid:18)(cid:24) (cid:18) (cid:19)(cid:25) (cid:19)
+ log max R ,
2 j
1≤j≤J
(cid:24) (cid:25)
λ
D +q
r = 2Di64 i i D2(q +1)QDi.
D i i i
i
i=1 (cid:18) (cid:19)
X
The Lemma 18 still holds.
27Zheng and Han
References
L. B. Alain. Filtering and parameter estimation in a simple linear system driven by a
fractional brownian motion. Statistics and Probability Letters, 38(3):263–274, 1998.
B. Bauer and M. Kohler. On deep learning as a remedy for the curse of dimensionality in
nonparametric regression. Annals of Statistics, 47(4):2261–2285, 2019.
S. Becker, P. Cheridito, and A. Jentzen. Deep optimal stopping. Journal of Machine
Learning Research, 20(74):1–25, 2019.
R. Bhansali and P. Kokoszka. Prediction of long-memory time series: A tutorial review.
Springer, Berlin, Heidelberg, 2003.
F. Biagini, B. Øksendal, A. Sulem, and N. Wallner. An introduction to white-noise theory
and malliavin calculus for fractional brownian motion. Proceedings of The Royal Society
A Mathematical Physical and Engineering Sciences, 460(2041):347–372, 2004.
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223–311, 2018.
B.BuchmannandC.Klu¨ppelberg. Fractionalintegralequationsandstatespacetransforms.
Bernoulli, 12(3):431–456, 2006.
L. Caccetta, B. Qu, and G. Zhou. A globally and quadratically convergent method for
absolute value equations. Computational Optimization and Applications, 48(1):45–58,
2011.
T. T. N. Dang and I. Jacques. Estimation of the hurst and the stability indices of a h-self-
similar stable process. Electronic Journal of Statistics, 11(2):4103–4150, 2017.
L. Decreusefond and A. S. U¨stu¨nel. Stochastic analysis of the fractional brownian motion.
Potential Analysis, 10(2):177–214, 1999.
R.M.Dudley. Metric entropy of some classes of sets with differentiable boundaries. Journal
of Approximation Theory, 10(3):227–236, 1974.
R. M. Dudley. Real Analysis and Probability. Cambridge University Press, 2002.
T. E. Duncan. Prediction for some processes related to a fractional brownian motion.
Statistics and Probability Letters, 76(2):128–134, 2006.
T. E. Duncan, Y. Hu, and B. Pasik-Duncan. Stochastic calculus for fractional brownian
motion. i. theory. SIAM Journal on Control and Optimization, 38(2):582–612, 2000.
R. J. Elliott and J. Van Der Hoek. A general fractional white noise theory and applications
to finance. Mathematical Finance, 13(2):301–330, 2003.
H. Fink, C. Klueppelberg, and M. Zaehle. Conditional distributions of processes related to
factional brownian motion. Journal of Applied Probability, 50(1):166–183, 2013.
28Predicting path-dependent processes by deep learning
J. Garnier and K. Sølna. Correction to black–scholes formula due to fractional stochastic
volatility. SIAM Journal on Financial Mathematics, 8(1):560–588, 2017.
J. Gatheral, T. Jaisson, and M. Rosenbaum. Volatility is rough. Quantitative Finance, 18
(6):933–949, 2018.
N. Granik, L. E. Weiss, E. Nehme, M. Levin, M. Chein, E. Perlson, Y. Roichman, and
Y. Shechtman. Single-particle diffusion characterization by deep learning. Biophysical
Journal, 117(2):185–192, 2019.
G. Gripenberg and I. Norros. On the prediction of fractional brownian motion. Journal of
Applied Probability, 33(2):400–410, 1996.
L.Gy¨orfi,M. Kohler,A.Krzyz˙ak, andH.Walk. A distribution-free theory of nonparametric
regression. Springer New York, NY, 2002.
Y. Hu. Integral transformations and anticipative calculus for fractional Brownian motions.
American Mathematical Society, 2005.
M. Imaizumi and K. Fukumizu. Deep neural networks learn non-smooth functions effec-
tively. In Proceedings of Machine Learning Research, volume 89, pages 869–878, Naha,
2019. 22nd International Conference on Artificial Intelligence and Statistics.
E. Kepten, I. Bronshtein, and Y. Garini. Ergodicity convergence test suggests telomere
motion obeys fractional dynamics. Physical Review E, 83:041919, 2011.
M. L. Kleptsyna and A. Breton. Statistical analysis of the fractional ornstein–uhlenbeck
type process. Statistical Inference for Stochastic Processes, 5:229–248, 2002.
M.KohlerandS.Langer. Ontherateofconvergence offullyconnected deepneuralnetwork
regression estimates. Annals of Statistics, 49(4):2231–2249, 2021.
K. Kubilius, Y. Mishura, and K. Ralchenko. Parameter estimation in fractional diffusion
models. Springer Cham, 2017.
S. Langer. Analysis of the rate of convergence of fully connected deep neural network
regression estimates with smooth activation function. Journal of Multivariate Analysis,
182:104695, 2021.
G. Livieri, S. Mouti, A. Pallavicini, and M. Rosenbaum. Rough volatility: Evidence from
option prices. IISE Transactions, 50(9):767–776, 2018.
L. Lu, X. Meng, Z. Mao, and G. E. Karniadakis. Deepxde: A deep learning library for
solving differential equations. SIAM Review, 63(1):208–228, 2021.
B. B. Mandelbrot and J. W. Van Ness. Fractional brownian motions, fractional noises and
applications. SIAM Review, 10(4):422–437, 1968.
D. Nualart. The Malliavin calculus and related topics. Springer Berlin, Heidelberg, 2006.
29Zheng and Han
D.NualartandB.Saussereau. Malliavin calculus forstochastic differential equations driven
byafractionalbrownianmotion. Stochastic Processes and their Applications, 119(2):391–
409, 2009.
V. Pipiras and M. S. Taqqu. Are classes of deterministic integrands for fractional brownian
motion on an interval complete? Bernoulli, 7(6):873–897, 2001.
B. L. S Prakasa Rao. Statistical Inference for Fractional Diffusion Processes. John Wiley
& Sons, Ltd, 2010.
J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:
85–117, 2015.
A. Shresthaand A.Mahmood. Review of deep learningalgorithms andarchitectures. IEEE
Access, 7:53040–53065, 2019.
C. J. Stone. Optimal global rates of convergence for nonparametric regression. The Annals
of Statistics, 10(4):1040–1053, 1982.
C.A.TudorandF.G.Viens. Statistical aspectsofthefractionalstochasticcalculus. Annals
of Statistics, 35(3):1183–1212, 2007.
F. Viens and J. Zhang. A martingale approach for fractional brownian motions and related
path dependent pdes. The Annals of Applied Probability, 29(6):3489–3540, 2019.
Y. Wang. Function estimation via wavelet shrinkage for long-memory data. The Annals of
Statistics, 24(2):466–484, 1996.
L. Yao and M. Doroslovacki. Prediction of long-range-dependent discrete-time fractional
brownianmotion process. 2003 IEEE International Conference on Acoustics, Speech, and
Signal Processing, 2003. Proceedings, ICASSP ’03, 4:213–216, 2003.
30