KAN 2.0:
Kolmogorov-Arnold Networks Meet Science
ZimingLiu1,4∗ PingchuanMa1,3 YixuanWang2 WojciechMatusik1,3 MaxTegmark1,4
1MassachusettsInstituteofTechnology
2CaliforniaInstituteofTechnology
3ComputerScienceandArtificialIntelligenceLaboratory(CSAIL),MIT
4TheNSFInstituteforArtificialIntelligenceandFundamentalInteractions
Abstract
A major challenge of AI + Science lies in their inherent incompatibility: to-
day’s AI is primarily based on connectionism, while science depends on sym-
bolism. Tobridgethetwoworlds,weproposeaframeworktoseamlesslysyner-
gize Kolmogorov-Arnold Networks (KANs) and science. The framework high-
lightsKANs’usageforthreeaspectsofscientificdiscovery: identifyingrelevant
features, revealing modular structures, and discovering symbolic formulas. The
synergyisbidirectional: sciencetoKAN(incorporatingscientificknowledgeinto
KANs),andKANtoscience(extractingscientificinsightsfromKANs).Wehigh-
light major new functionalities in pykan: (1) MultKAN: KANs with multiplica-
tionnodes. (2)kanpiler: aKANcompilerthatcompilessymbolicformulasinto
KANs. (3)treeconverter: convertKANs(oranyneuralnetworks)totreegraphs.
Basedonthesetools,wedemonstrateKANs’capabilitytodiscovervarioustypes
of physical laws, including conserved quantities, Lagrangians, symmetries, and
constitutivelaws.
Figure1:SynergizingscienceandtheKolmogorov-ArnoldNetwork(KAN).
∗zmliu@mit.edu
Preprint.Underreview.
4202
guA
91
]GL.sc[
1v50201.8042:viXra1 Introduction
In recent years, AI + Science has emerged as a promising new field, leading to significant scien-
tific advancements including protein folding prediction [37], automated theorem proving [95, 83],
weather forecast [41], among others. A common thread among these tasks is that they can all be
wellformulatedintoproblemswithclearobjectives, optimizablebyblack-boxAIsystems. While
this paradigm works exceptionally well for application-driven science, a different kind of science
exists: curiosity-driven science. In curiosity-driven research, the procedure is more exploratory,
often lacking clear goals beyond “gaining more understanding”. To clarify, curiosity-driven sci-
ence is far from useless; quite the opposite. The scientific knowledge and understanding gained
throughcuriosityoftenlayasolidfoundationfortomorrow’stechnologyandfosterawiderangeof
applications.
Althoughbothapplication-drivenandcuriosity-drivenscienceareinvaluableandirreplaceable,they
askdifferentquestions.Whenastronomersobservethemotionofcelestialbodies,application-driven
researchers focus on predicting their future states, while curiosity-driven researchers explore the
physicsbehindthemotion. AnotherexampleisAlphaFold, which, despiteitstremendoussuccess
inpredictingproteinstructures, remainsintherealmofapplication-drivensciencebecauseitdoes
notprovidenewknowledgeatamorefundamentallevel(e.g., atomicforces). Hypothetically, Al-
phaFoldmusthaveuncoveredimportantunknownphysicstoachieveitshighlyaccuratepredictions.
However,thisinformationremainshiddenfromus,leavingAlphaFoldlargelyablackbox. There-
fore,weadvocatefornewAIparadigmstosupportcuriosity-drivenscience. Thisnewparadigmof
AI + Science demands a higher degree of interpretability and interactivity in AI tools so that they
canbeseamlesslyintegratedintoscientificresearch.
Recently,anewtypeofneuralnetworkcalledKolmogorov-ArnoldNetwork(KAN)[57],hasshown
promiseforscience-relatedtasks. Unlikemulti-layerperceptrons(MLPs),whichhavefixedactiva-
tion functions on nodes, KANs feature learnable activation functions on edges. Because KANs
candecomposehigh-dimensionalfunctionsintoone-dimensionalfunctions, interpretabilitycanbe
gainedbysymbolicallyregressingthese1Dfunctions.However,theirdefinitionofinterpretabilityis
somewhatnarrow,equatingitalmostexclusivelywiththeabilitytoextractsymbolicformulas. This
limiteddefinitionrestrictstheirscope,assymbolicformulasarenotalwaysnecessaryorfeasiblein
science. For example, while symbolic equations are powerful and prevalent and physics, systems
in chemistry and biology the systems are often too complex to be represented by such equations.
In these fields, modular structures and key features may be sufficient to characterize interesting
aspects of these systems. Another overlooked aspect is the reverse task of embedding knowledge
intoKANs: HowcanweincorporatepriorknowledgeintoKANs,inthespiritofphysics-informed
learning?
WeenhanceandextendKANstomakethemeasilyusedforcuriosity-drivenscience. Thegoalof
thispapercanbesummarizedasfollows:
Goal: SynergizeKolmogorov-ArnoldNetworks⇔Science.
⇐: BuildinscientificknowledgetoKANs(Section3).
⇒: ExtractoutscientificknowledgefromKANs(Section4).
To be more concrete, scientific explanations may have different levels, ranging from the coars-
est/easiest/correlationaltothefinest/hardest/causal:
• Importantfeatures: Forexample,“y isfullydeterminedbyx andx ,whileotherfactors
1 2
donomatter.” Inotherwords,thereexistsafunctionf suchthaty =f(x ,x ).
1 2
• Modularstructures: Forinstance,“x andx contributestoyindependentlyinanadditive
1 2
way.“Thismeansthereexistsfunctionsgandhsuchthaty =g(x )+h(x ).
1 2
• Symbolic formulas: For example, “y depends on x as a sine function and on x as an
1 2
exponentialfunction”. Inotherwords,y =sin(x )+exp(x ).
1 2
ThepaperreportsonhowtoincorporateandextractthesepropertiesfromKANs. Thestructureof
the paper is as follows (illustrated in Figure 1): In Section 2, we augment the original KAN with
multiplicationnodes,introducinganewmodelcalledMultKAN.InSection3,weexplorewaysto
embedscientificinductivebiasesintoKANs,focusingonimportantfeatures(Section3.1),modular
2Figure2: Top: comparingKANandMultKANdiagrams. MultKANhasextramultiplicationlayersM. Bot-
tom:Aftertrainingonf(x,y)=xy,KANlearnsanalgorithmrequiringtwoadditionnodes,whileMultKAN
requiresonlyonemultiplicationnode.
structures(Section3.2),andsymbolicformulas(Section3.3). InSection4,weproposemethodsto
extractscientificknowledgefromKANs,againcoveringimportantfeatures(Section4.1),modular
structures (Section 4.2), and symbolic formulas (Section 4.3). In Section 5, we apply KANs to
various scientific discovery tasks using the tools developed in the previous sections. These tasks
include discovering conserved quantities, symmetries, Lagrangians, and constitutive laws. Codes
are available at https://github.com/KindXiaoming/pykan and can also be installed via pip
install pykan. Although the title of the paper is “KAN 2.0”, the release version of pykan is
0.2.x.
2 MultKAN:AugmentingKANswithmultiplications
The Kolmogorov-Arnold representation theorem (KART) states that any continuous high-
dimensional function can be decomposed into a finite composition of univariate continuous func-
tionsandadditions:
2n+1 (cid:32) n (cid:33)
(cid:88) (cid:88)
f(x)=f(x ,··· ,x )= Φ ϕ (x ) . (1)
1 n q q,p p
q=1 p=1
Thisimpliesthatadditionistheonlytruemultivariateoperation,whileothermultivariateoperations
(including multiplication) can be expressed as additions combined with univariate functions. For
example,tomultiplytwopositivenumbersxandy,wecanexpressthisasxy =exp(logx+logy)2
whoseright-handsideonlyconsistsofadditionandunivariatefunctions(logandexp).
2Ifxandycanbenegative,onemaychoosealargec > 0andexpressxy = exp(log(x+c)+log(y+
c))−c(x+y)−c2.Otherconstructionsincludequadraticfunctions,suchasxy=((x+y)2−(x−y)2)/4
orxy=((x+y)2−x2−y2)/2.
3However, given the prevalence of multiplications in both science and everyday life, it is desirable
toexplicitlyincludemultiplicationsinKANs,whichcouldpotentiallyenhancebothinterpretability
andcapacity.
Kolmogorov-Arnold Network (KAN) While the KART Eq. (1) corresponds to a two-layer net-
work,Liuetal.[57]managedtoextendittoarbitrarydepthsbyrecognizingthatseeminglydifferent
outer functions Φ and inner functions ϕ can be unified through their proposed KAN layers. A
q q,p
depth-LKANcanbeconstructedsimplybystackingLKANlayers.Theshapeofadepth-LKANis
representedbyanintegerarray[n ,n ,··· ,n ]wheren denotesthenumberofneuronsinthelth
0 1 L l
neuronlayers.ThelthKANlayer,withn inputdimensionsandn outputdimensions,transforms
l l+1
aninputvectorx
l
∈Rnl tox
l+1
∈Rnl+1
 ϕ (·) ϕ (·) ··· ϕ (·) 
l,1,1 l,2,1 l,nl,1
ϕ (·) ϕ (·) ··· ϕ (·)
x l+1
=
 
l,1,
. .
.2 l,2,
. .
.2 l,nl
. .
.,2 
 x l, (2)
ϕ (·) ϕ (·) ··· ϕ (·)
l,1,nl+1 l,2,nl+1 l,nl,nl+1
(cid:124) (cid:123)(cid:122) (cid:125)
Φl
andthewholenetworkisacompositionofLKANlayers,i.e.,
KAN(x)=(Φ ◦···◦Φ ◦Φ )x. (3)
L−1 1 0
Indiagrams,KANscanbeintuitivelyvisualizedasanetworkconsistingofnodes(summation)and
edges(learnableactivations),asshowninFigure2topleft. Whentrainedonthedatasetgenerated
from f(x,y) = xy, the KAN (Figure 2 bottom left) uses two addition nodes, making it unclear
whatthenetworkisdoing. However,aftersomeconsideration,werealizeitleveragestheequality
xy =((x+y)2−(x−y)2)/4butthisisfarfromobvious.
MultiplicativeKolmogorov-ArnoldNetworks(MultKAN)Toexplicitlyintroducemultiplication
operations, we propose the MultKAN, which can reveal multiplicative structures in data more
clearly. A MultKAN (shown in Figure 2 top right) is similar to a KAN, with both having stan-
dardKANlayers. WerefertotheinputnodesofaKANlayerasnodes,andtheoutputnodesofa
KANlayersubnodes. ThedifferencebetweenKANandMultKANliesinthetransformationsfrom
thecurrentlayer’ssubnodestothenextlayer’snodes. InKANs,nodesaredirectlycopiedfromthe
previouslayer’ssubnodes.InMultKANs,somenodes(additionnodes)arecopiedfromcorrespond-
ingsubnodes,whileothernodes(multiplicationnodes)performmultiplicationonksubnodesfrom
thepreviouslayer. Forsimplicity,wesetk =2below3.
Based on the MultKAN diagram (Figure 2 top right), it can be intuitively understood that a
MultKAN is a normal KAN with optional multiplications inserted in. To be mathematically pre-
cise, we define the following notations: The number of addition (multiplication) operations in
layer l are denoted as na (nm), respectively. These are collected into arrays: addition width
l l
na ≡ [na,na,··· ,na] and multiplication width nm ≡ [nm,nm,··· ,nm]. When nm = nm =
0 1 L 0 1 L 0 1
··· = nm = 0, the MultKAN reduces to a KAN. For example, Figure 2 (top right) shows a
L
MultKANwithna =[2,2,1]andnm =[0,2,0].
AMultKANlayerconsistsofastandardKANLayerΦ andamultiplicationlayerM . Φ takesin
l l l
an input vector x l ∈ Rna l+nm l and outputs z l = Φ l(x) ∈ Rna l+1+2nm l+1. The multiplication layer
consists of two parts: the multiplication part performs multiplications on subnode pairs, while the
otherpartperformsidentitytransformation. WritteninPython,M transformsz asfollows:
l l
M l(z l)=concatenate(z l[:na l+1],z l[na
l+1
::2]⊙z l[na l+1+1::2])∈Rna l+1+nm l+1, (4)
where⊙iselement-wisemultiplication.TheMultKANLayercanbesuccinctlyrepresentedasΨ ≡
l
M ◦Φ . ThewholeMultKANisthus:
l l
MultKAN(x)=(Ψ ◦Ψ ◦···◦Ψ ◦Ψ )x. (5)
L L−1 1 0
Sincetherearenotrainableparametersinmultiplicationlayers,allsparseregularizationtechniques
(e.g., ℓ and entropy regularization) for KANs [57] can be directly applied to MultKANs. For
1
3Wesetk =2forsimplicity,butthepykanpackageallowsktobeanyintegerk ≥2. Userscanevenset
differentkvaluesfordifferentmultiplicationnodes. However,ifdifferentksvaluesareusedwithinthesame
layer,itcanbechallengingtoparallelizethesemultiplications.
4Figure 3: Adding auxiliary variables to inputs enhances interpretability. For the relativistic mass equation,
(cid:112)
m = m / 1−v2/c2, (a)atwo-layerKANisneededifonly(m ,v,c)areusedasinputs. (b)Ifweadd
0 0
(cid:112)
β ≡ v/candγ ≡ 1/ 1−β2asauxiliaryvariablestoKANs,aone-layerKANsuffices(seed0). (c)seed1
findsadifferentsolution,whichissub-optimalandcanbeavoidedthroughhypothesistesting(Section4.3).
the multiplication task f(x,y) = xy, the MultKAN indeed learns to use one multiplication node,
makingitperformsimplemultiplication,asallthelearnedactivationfunctionsarelinear(Figure2
bottomright).
AlthoughKANshavepreviouslybeenseenasaspecialcaseofMultKANs,weextendthedefinition
andtreat“KAN”and“MultKAN”assynonyms. Bydefault,whenwerefertoKANs,multiplication
isallowed. IfwespecificallyrefertoaKANwithoutmultiplication,wewillexplicitlystateso.
3 SciencetoKANs
In science, domain knowledge is crucial, allowing us to work effectively even with small or zero
data. Therefore, it is beneficial to adopt a physics-informed approach for KANs: we should in-
corporate available inductive biases into KANs while preserving their flexibility to discover new
physicsfromdata.
We explore three types of inductive biases that can be integrated into KANs. From the coars-
est/easiest/correlationaltothefinest/hardest/causal,theyareimportantfeatures(Section3.1),mod-
ularstructures(Section3.2)andsymbolicformulas(Section3.3).
3.1 AddingimportantfeaturestoKANs
Inaregressionproblem,thegoalistofindafunctionf suchthaty = f(x ,x ,··· ,x ). Suppose
1 2 n
wewanttointroduceanauxiliaryinputvariablea = a(x ,x ,...,x ), transformingthefunction
1 2 n
to y = f(x ,··· ,x ,a). Although the auxiliary variable a does not add new information, it can
1 n
increasetheexpressivepoweroftheneuralnetwork. Thisisbecausethenetworkdoesnotneedto
expend resources to calculate the auxiliary variable. Additionally, the computations may become
simpler, leading to improved interpretability. Users can add auxiliary features to inputs using the
augment_inputmethod:
model.augment_input(original_variables, auxiliary_variables, dataset) (6)
(cid:112)
Asanexample, considertheformulaforrelativisticmassm(m ,v,c) = m / 1−(v/c)2 where
0 0
m istherestmass,v isthevelocityofthepointmass,andcisthespeedoflight. Sincephysicists
0
(cid:112) (cid:112)
often work with dimensionless numbers β ≡ v/c and γ ≡ 1/ 1−β2 ≡ 1/ 1−(v/c)2, they
mightintroduceβandγ alongsidevandcasinputs. Figure3,showsKANswithandwithoutthese
auxiliaryvariables:(a)illustratestheKANcompiledfromthesymbolicformula(seeSection3.3for
theKANcompiler),whichrequires5edges;(b)(c)showsKANswithauxiliaryvariables,requiring
only2or3edgesandachievinglosesof10−6 and10−4, respectively. Notethat(b)and(c)differ
onlyinrandomseeds. Seed1representsasub-optimalsolutionbecauseitalsoidentifiesβ = v/c
(cid:112)
as a key feature. This is not surprising, as in the classical limit v ≪ c, γ ≡ 1/ 1−(v/c)2 ≈
1+(v/c)2/2 = 1+β2/2. The variation due to different seeds can be seen either as a feature or
abug: Asafeature,thisdiversitycanhelpfindsub-optimalsolutionswhichmayneverthelessoffer
5Figure4:BuildingmodularstructurestoKANs:(a)multiplicativeseparability;(b)symmetries.
interestinginsights;asabug,itcanbeeliminatedusingthehypothesistestingmethodproposedin
Section4.3.
3.2 BuildingmodularstructurestoKANs
Modularity is prevalent in nature: for example, the human cerebral cortex is divided into several
functionallydistinctmodules,eachofthesemodulesresponsibleforspecifictaskssuchaspercep-
tion or decision making. This modularity simplifies the understanding of neural networks, as it
allowsustointerpretclustersofneuronscollectivelyratherthananalyzingeachneuronindividually.
Structuralmodularityischaracterizedbyclustersofconnectionswhereintra-clusterconnectionsare
much stronger than inter-cluster ones. To enforce modularity, we introduce the module method,
which preserves intra-cluster connections while removing inter-cluster connections. The modules
arespecifiedbyusers. Thesyntaxis
model.module(start_layer_id, ‘[nodes_id]->[subnodes_id]->[nodes_id]...’)
(7)
For example, if a user wants to assign specific nodes/subnodes to a module – say, the 0th node
in layer 1, the 1st and 3rd subnode in layer 1, the 1st and 3rd node in layer 2 – they might use
module(1,‘[0]->[1,3]->[1,3]’). Tobeconcrete,therearetwotypesofmodularity:separabil-
ityandsymmetry.
Separability We say a function is considered separable if it can be expressed as a
sum or product of functions of non-overlapping variable groups. For example, a four-
variable function f(x ,x ,x ,x ) is maximally multiplicatively separable if it has the form
1 2 3 4
f (x )f (x )f (x )f (x ), creating four distinct groups (1),(2),(3),(4). Users can create these
1 1 2 2 3 3 4 4
modules by calling the module method four times: module(0,‘[i]->[i]’), i = 0,1,2,3,
shown in Figure 4 (a). The final call may be skipped since the first three are sufficient to de-
finethegroups. Weakerformsofmultiplicativeseparabilitymightbef (x ,x )f (x ,x )(calling
1 1 2 2 3 4
module(0,‘[0,1]->[0,1]’))orf (x )f (x ,x ,x )(callingmodule(0,‘[0]->[0]’)).
1 1 2 2 3 4
GeneralizedSymmetryWesayafunctionissymmetricinvariables(x ,x )iff(x ,x ,x ,···)=
1 2 1 2 3
g(h(x ,x ),x ,···). This property is termed symmetry because the value of f remains un-
1 2 3
changed as long as h(x ,x ) is constant, even if x and x vary. For example, a function f
1 2 1 2
(cid:112)
is rotational invariant in 2D if f(x ,x ) = g(r), where r ≡ x2+x2. When symmetry in-
1 2 1 2
volves only a subset of variables, it can be considered hierarchical since x and x interact first
1 2
through h (2-Layer KAN), and then h interacts with other variables via g (2-Layer KAN). Sup-
pose a four-variable function has a hierarchical form f(x ,x ,x ,x ) = h(f(x ,x ),g(x ,x )),
1 2 3 4 1 2 3 4
6Figure 5: KAN compiler (kanpiler) converts symbolic expressions to KANs. (a) how kanpiler works: the
symbolicformulaisfirstparsedtoanexpressiontree,whichisthenconvertedtoaKAN.(b)ApplyingKANs
to10equations(selectedfromtheFeynmandataset). (c)ExpandacompiledKANtoincreaseitsexpressive
power.
as illustrated in Figure 4 (b). We can use the module method to create this structure by call-
ing module(0,‘[0,1]->[0,1]->[0,1]->[0]’), ensuring that the variable groups (x ,x ) and
1 2
(x ,x )donotinteractinthefirsttwolayers.
3 4
3.3 CompilingsymbolicformulastoKANs
Scientists often find satisfaction in representing complex phenomena through symbolic equations.
However,whiletheseequationsareconcise,theymaylacktheexpressivepowerneededtocapture
7allnuancesduetotheirspecificfunctionalforms. Incontrast,neuralnetworksarehighlyexpressive
but may inefficiently spend training time and data to learn domain knowledge already known to
scientists. To leverage the strengths of both approaches, we propose a two-step procedure: (1)
compilesymbolicequationsintoKANsand(2)fine-tunetheseKANsusingdata. Thefirststepaims
to embed known domain knowledge into KANs, while the second step focuses on learning new
“physics”fromdata.
kanpiler(KANcompiler)ThegoalofthekanpileristoconvertasymbolicformulatoaKAN.The
process, illustrated in Figure 5 (a), involves three main steps: (1) The symbolic formula is parsed
intoatreestructure,wherenodesrepresentexpressions,andedgesdenoteoperations/functions. (2)
ThistreeisthenmodifiedtoalignwiththestructureofaKANgraph. Modificationsincludemoving
all leaf nodes to the input layer via dummy edges, and adding dummy subnodes/nodes to match
KANarchitecture. Thesedummyedges/nodes/subnodesonlyperformidentitytransformation. (3)
Thevariablesarecombinedinthefirstlayer,effectivelyconvertingthetreeintoagraph. Forvisual
clarity,1Dcurvesareplacedonedgestorepresentfunctions. Wehavebenchmarkedthekanpileron
theFeynmandatasetanditsuccessfullyhandlesall120equations. ExamplesareshowninFigure5
(b). The kanpiler takes input variables (as sympy symbols) and output expression (as a sympy
expression),andreturnsaKANmodel
model = kanpiler(input_variables, output_expression) (8)
NotethatthereturnedKANmodelisinthesymbolicmode,i.e.,thesymbolicfunctionsareexactly
encoded. If we instead use cubic splines to approximate these symbolic functions, we get MSE
lossesℓ∝N−8[57],whereN isthenumberofgridintervals(proportionaltothenumberofmodel
parameters).
Width/depthexpansionforincreasedexpressivepowerTheKANnetworkgeneratedbythekan-
piler is compact, without no redundant edges, which might limit its expressive power and hinder
further fine-tuning. To address this, we propose expand_width and expand_depth methods to
expandthenetworktobecomewideranddeeper,asshowninFigure5(c). Theexpansionmethods
initiallyaddzeroactivationfunctions,whichsufferfromzerogradientsduringtraining. Therefore,
the perturb method should be used to perturb these zero functions into non-zero values, making
themtrainablewithnon-zerogradients.
4 KANstoScience
Today’sblackboxdeepneuralnetworksarepowerful, butinterpretingthesemodelsremainschal-
lenging. Scientistsseeknotonlyhigh-performingmodelsbutalsotheabilitytoextractmeaningful
knowledge from the models. In this section, we focus on enhancing the interpretability of KANs
scientificpurposes.WewillexplorethreelevelsofknowledgeextractionfromKANs,fromthemost
basictothemostcomplex: importantfeatures(Section4.1), modularstructures(Section4.2), and
symbolicformulas(Section4.3).
4.1 IdentifyingimportantfeaturesfromKANs
Identifying important variables is crucial for many tasks. Given a regression model f where y ≈
f(x ,x ,...,x ),weaimtoassignscorestotheinputvariablestogaugetheirimportance. Liuet
1 2 n
al. [57], used the function L1 norm to indicate the importance of edges, but this metric could be
problematicasitonlyconsiderslocalinformation.
Toaddressthis,weintroduceamoreeffectiveattributionscorewhichbetterreflectstheimportance
of variables than the L1 norm. For simplicity, let us assume there are multiplication nodes, so we
do not need to differentiate between nodes and subnodes 4. Suppose we have an L-layer KAN
with width [n ,n ,··· ,n ]. We define E as the standard deviation of the activations on the
0 1 L l,i,j
(l,i,j)edge,andN asthestandarddeviationoftheactivationsonthe(l,i)node. Wethendefine
l,i
thenode(attribution)scoreA andtheedge(attribution)scoreB . In[57], wesimplydefined
l,i l,i,j
B = E and A = N . However, this definition fails to account for the later parts of the
l,i,j l,i,j l,i l,i
network;evenifanodeoranedgehasalargenormitself,itmaynotcontributetotheoutputifthe
restofthenetworkiseffectivelyazerofunction. Therefore,wenowcomputenodeandedgescores
4Forsubnodesbelongingtomultiplicationnode,thesubnodesinherittheirscoresfromthemultiplication
node.
8Figure 6: Identifying important features in KANs. (a) comparing the attribution score to the L1 norm used
in Liu et al. [57]. On two synthetic tasks, the attribution score brings more insights than the L1 norm. (b)
Attributionscorescanbecomputedforinputsandusedforinputpruning.
iterativelyfromtheoutputlayertotheinputlayer. Wesetalloutputdimensionstohaveunitscores,
i.e.,A =1,i=0,1,··· ,n −15,andcomputescoresasfollows:
L,i L
B =A
E
l,j , A
=(cid:88)nl
B , l=L,L−1,··· ,1. (9)
l−1,i,j l,jN l−1,i l−1,i,j
l+1,j
j=0
Comparing E and B We find that B provides a more accurate reflection of edge im-
l,i,j l,i,j l,i,j
portance. In Figure 6, we compare KANs trained on two equations y = exp(sin(πx )+x2) and
1 2
y =(x2+x2)2+(x2+x2)2andvisualizeKANswithimportancescoresbeingE (L1norm)orB
1 2 3 4
(attributionscore). Forthefirstequation,attributionsscoresrevealacleanergraphthanL1norms,
asmanyactiveedgesinthefirstlayerdonotcontributetothefinaloutputduetoinactivesubsequent
edges. Theattributionscoreaccountsforthis,resultinginamoremeaningfulgraph. Forthesecond
equationy = (x2 +x2)2 +(x2 +x2)2, wecantellfromthesymbolicequationthatallfourvari-
1 2 3 4
ablesareequallyimportant. Theattributionscorescorrectlyreflecttheequalimportanceofallfour
variables,whereastheL1normincorrectlysuggeststhatx andx aremoreimportantthanx and
3 4 1
x .
2
Pruning inputs based on attribution scores In real datasets, input dimensionality can be large,
but only a few variables may be relevant. To address this, we propose pruning away irrelevant
featuresbasedonattributionscoressothatwecanfocusonthemostrelevantones. Userscanapply
the prune_input to retain only the most relevant variables. For instance, if there are 100 input
features ordered by decreasing relevance in the function y = (cid:80)99 x2/2i,x ∈ [−1,1], and after
i=0 i i
training, only the first five features show significantly higher attribution scores, the prune_input
methodwillretainonlythesefivefeatures.Theprunednetworkbecomescompactandinterpretable,
whereastheoriginalKANwith100inputsistoodenseforstraightforwardinterpretation.
4.2 IdentifyingmodularstructuresfromKANs
Althoughtheattributionscoreprovidesvaluableinsightsintowhichedgesornodesareimportant,
itdoesnotrevealmodularstructures,i.e.,howtheimportantedgesandnodesareconnected. Inthis
part,weaimtouncovermodularstructuresfromtrainedKANsandMLPsbyexaminingtwotypes
ofmodularity: anatomicalmodularityandfunctionalmodularity.
9Figure7:Inducinganatomicalmodularityinneuralnetworksthroughneuronswapping.Theapproachinvolves
assigningspatialcoordinatestoneuronsandpermutingthemtominimizetheoverallconnectioncost. Fortwo
tasks (left: multitask parity, right: hierarchical majority voting), neuron swapping works for KANs (top) in
bothcasesandworksforMLPs(bottom)formultitaskparity.
4.2.1 Anatomicalmodularity
Anatomical modularity refers to the tendency for neurons placed close to each other spatially to
havestrongerconnectionsthanthosefurtherapart. Althoughartificialneuralnetworkslackphysical
spatial coordinates, introducing the concept of physical space has been shown to enhance inter-
pretability[51,52]. Weadopttheneuronswappingmethodfrom[51,52],whichshortensconnec-
tionswhilepreservingthenetwork’sfunctionality. Wecallthemethodauto_swap. Theanatomi-
calmodularstructurerevealedthroughneuronswappingfacilitateseasyidentificationofmodules,
even visually, for two tasks shown Figure 7: (1) multitask sparse parity; and (2) hierarchical ma-
jority voting. For multitask sparse parity, we have 10 input bits x ∈ {0,1},i = 1,2,··· ,10,
i
and output y = x ⊕ x ,j = 1,··· ,5, where ⊕ denotes modulo 2 addition. The task
j 2j−1 2j
exhibits modularity because each output depends only on a subset of inputs. auto_swap suc-
cessfully identifies modules for both KANs and MLPs, with the KAN discovering simpler mod-
ules. For hierarchical majority voting, with 9 input bits x ∈ {0,1},i = 1,··· ,9, and the output
i
y =maj(maj(x ,x ,x ),maj(x ,x ,x ),maj(x ,x ,x )),wheremajstandsformajorityvoting
1 2 3 4 5 6 7 8 9
(output 1 if two or three inputs are 1, otherwise 0). The KAN reveals the modular structure even
before auto_swap, and the diagram becomes more organized after auto_swap. The MLP shows
some modular structure from the pattern of the first layer weights, indicating interactions among
variables,buttheglobalmodularstructureremainsunclearregardlessofauto_swap.
4.2.2 Functionalmodularity
Functionalmodularitypertainstotheoverallfunctionrepresentedbytheneuralnetwork. Givenan
Oraclenetworkwhereinternaldetailssuchasweightsandhiddenlayeractivationsareinaccessible
(too complicated to analyze), we can still gather information about functional modularity through
forwardandbackwardpassesattheinputsandoutputs. Wedefinethreetypesoffunctionalmodu-
larity(seeFigure8(a)),basedlargelyon [84].
Separability: Afunctionf isadditivelyseparableif
f(x ,x ,···x )=g(x ,...,x )+h(x ,...,x ). (10)
1 2 n 1 k k+1 n
Notethat
∂2f
= 0when1 ≤ i ≤ k,k+1 ≤ j ≤ n. Todetecttheseparability,wecancompute
∂xi∂xj
the Hessian matrix H ≡ ∇T∇f (H = ∂2f ) and check for block structure. If H = 0 for
ij ∂xi∂xj ij
5Otherchoicescanbemadebasedontheperceivedimportanceofeachoutputdimension, thoughthisis
lesscriticalwhenoutputsaretypicallyone-dimensional.
10Figure8: DetectingfunctionalmodularityinKANs. (a)Westudythreetypesoffunctionalmodularity: sep-
arability(additiveormultiplicative),generalseparability,andsymmetry. (b)Applyingthesetestsrecursively
convertsafunctionintoatree. Herethefunctioncanbesymbolicfunctions(top),KANs(middle)orMLPs
(bottom). BothKANsandMLPsproducecorrecttreegraphsattheendoftrainingbutshowdifferenttraining
dynamics.
11all 1 ≤ i ≤ k and k +1 ≤ j ≤ n, then we know f is additively separable. For multiplicative
separability,wecanconvertittoadditiveseparabilitybytakingthelogarithm:
f(x ,x ,···x )=g(x ,...,x )×h(x ,...,x )
1 2 n 1 k k+1 n
(11)
log|f(x ,x ,··· ,x )|=log|g(x ,...,x )|+log|h(x ,...,x )|
1 2 n 1 k k+1 n
Todetectmultiplicativeseparability,wedefineH ≡
∂2log|f|,andcheckforblockstructure.
Users
ij ∂xi∂xj
cancalltest_separabilitytotestgeneralseparability.
Generalizedseparability: Afunctionf hasgeneralizedseparabilityif
f(x ,x ,···x )=F(g(x ,...,x )+h(x ,...,x )). (12)
1 2 n 1 k k+1 n
Todetectgeneralizedseparability,wecompute
∂f ∂F ∂g ∂f ∂F ∂h
= (1≤i≤k), = (k+1≤j ≤n)
∂x ∂g ∂x ∂x ∂h ∂x
i i j i
(13)
∂f/∂x ∂F/∂g ∂g/∂x ∂g/∂x 1
i = i = i =g (x ,x ,···x )× .
∂f/∂x ∂F/∂h∂h/∂x ∂h/∂x xi 1 2 k h (x ,··· ,x )
j j j xj k+1 n
wherewehaveused ∂F = ∂F. Notethat ∂f/∂xi ismultiplicativelyseparable,itcanbedetectedby
∂g ∂h ∂f/∂xj
the separability test proposed above. Users can call test_general_separability to check for
additiveormultiplicativeseparability.
GeneralizedSymmetry: Afunctionhasgeneralizedsymmetry(inthefirstkvariables)if
f(x ,x ,··· ,x )=g(h(x ,··· ,x ),x ,··· ,x ). (14)
1 2 n 1 k k+1 n
Wedenotey=(x ,··· ,x )andz=(x ,··· ,x ). Thispropertyiscalledgeneralizedsymme-
1 k k+1 n
try because f retains the same value as long as h is held constant, regardless of individual values
of x ,··· ,x . We compute the gradient of f with respect to y: ∇ f = ∂g∇ h. Since ∂g is a
1 k y ∂h y ∂h
scalar function, it does not change the direction of ∇ yh. Thus, the direction of ∇(cid:100)yf ≡ |∇ ∇yy ff
|
is
independentofz,i.e.,
∇ z(∇(cid:100)yf)=0, (15)
which is the condition for symmetry. Users can call the test_symmetry method to check for
symmetries.
Tree converter The three types of functional modularity form a hierarchy: symmetry is the most
general,generalseparabilityisintermediate,andseparabilityisthemostspecific. Mathematically,
Separability⊂GeneralizedSeparability⊂GeneralizedSymmetry (16)
To obtain the maximal hierarchy of modular structures, we apply generalized symmetry detection
recursively, forminggroupsassmallask = 2variablesandextendingtoallk = nvariables. For
example,letusconsideran8-variablefunction
f(x ,··· ,x )=((x2+x2)2+(x2+x2)2)2+((x2+x2)2+(x2+x2)2)2, (17)
1 8 1 2 3 4 5 6 7 8
which has four k = 2 generalized symmetries, involving groups (x ,x ), (x ,x ), (x ,x ),
1 2 3 4 5 6
(x ,x );twok =2generalizedsymmetries,involvinggroups(x ,x ,x ,x )and(x ,x ,x ,x ).
7 8 1 2 3 4 5 6 7 8
Assuch, eachk = 4groupcontainstwok = 2groups, demonstratingahierarchy. Foreachgen-
eralizedsymmetry,wecanalsotestifthegeneralizedsymmetryisfurthergeneralizedseparableor
separable. Userscanusethemethodplot_treetoobtainthetreegraphforafunction(thefunction
could be any Python expressions, neural networks, etc.). For a neural network model, users can
simplycallmodel.tree(). Thetreeplotcanhavethestyle‘tree’(bydefault)or‘box’.
Examples Figure 8 (b) provides two examples. When the exact symbolic functions are input to
plot_tree,thegroundtruthtreegraphsareobtained. Weareparticularlyinterestedinwhetherthe
tree converter works for neural networks. For these simple cases, both KANs and MLPs can find
thecorrectgraphifsufficientlytrained. Figure8(b)(bottom)showstheevolutionofthetreegraphs
duringKANandMLPtraining. Itisparticularlyinterestingtoseehowneuralnetworksgradually
learnthecorrectmodularstructure. Inthefirstcasef(x ,x ,x ,x ) = (x2+x2)2+(x2+x2)2,
1 2 3 4 1 2 3 4
bothKANandMLPgraduallypickupmoreinductivebiases(theirintermediatestatesaredifferent)
12Figure9: Threetrickstofacilitatesymbolicregression. TrickA(toprow): detectingandleveragingmodular
structures.TrickB(middlerow):sparseconnectioninitialization.TrickC(bottomrow):Hypothesistesting.
(cid:112)
untiltheyreachthecorrectstructure. Inthesecondcase,f(x ,x ,x )=sin(x )/ x2+x2,both
1 2 3 1 2 3
the models initially detect multiplicative separability for all three variables, showing even higher
symmetrythanthecorrectstructure. Aftertrainingprogresses,bothmodels“realize”that: inorder
tobetterfitdata(lossbecomeslower),suchhighsymmetrystructurecannolongerbemetandshould
berelaxedtoalessstringentstructure. AnadditionalobservationisthatKANhasanintermediate
structure not found in the MLP. There are two caveats we would like to mention: (1) results can
be seed and/or threshold-dependent. (2) all tests rely on second-order derivatives, which may not
berobustduetothemodelbeingtrainedonlyonzero-orderinformation. Adversarialconstructions
suchasf (x)=f(x)+ϵsin(x)couldleadtoissues,becausealthough|f (x)−f(x)|→0asϵ→0,
ϵ ϵ ϵ
|f′′(x)−f′′(x)|→∞asϵ→0. Althoughsuchextremecasesareunlikelyinpractice,smoothness
ϵ
isnecessarytoensurethesuccessofourmethods.
4.3 IdentifyingsymbolicformulasfromKANs
Symbolic formulas are the most informative, as they clearly reveal both important features and
modularstructuresoncetheyareknown. InLiuetal.[57],theauthorsshowedabunchofexamples
from which they can extract symbolic formulas, with some prior knowledge when needed. With
the new tools proposed above (feature importance, modular structures, and symbolic formulas),
userscanleveragethesenewtoolstoeasilyinteractandcollaboratewithKANs,makingsymbolic
regressioneasier. Wepresentthreetricksbelow,illustratedinFigure9.
Trick A: discover and leverage modular structures We can first train a general network and
probeitsmodularity. Oncethemodularstructureisidentified, weinitializeanewmodelwiththis
modularstructureasinductivebiases. Forinstance,considerthefunctionf(q,v,B,m)=qvB/m.
13We first initialize a large KAN (presumably expressive enough) to fit the dataset to a reasonable
accuracy. After training, the tree graph is extracted (ref Sec 4.2) from the trained KAN, which
showsmultiplicativeseparability. ThenwecanbuildthemodularstructureintoasecondKAN(ref
Sec3.2),trainit,andthensymbolifyall1Dfunctionstoderivetheformula.
TrickB:SparseinitializationSymbolicformulastypicallycorrespondtoKANswithsparsecon-
nections(seeFigure5(b)),soinitializingKANssparselyalignsthembetterwiththeinductivebiases
ofsymbolicformulas.Otherwise,denselyinitializedKANsrequirecarefulregularizationtopromote
sparsity.Sparseinitializationcanbeachievedbypassingtheargument“sparse_init=True”tothe
KANinitializer. Forexample,forthefunctionf(q,E,v,B,θ)=q(E+vBsinθ),asparselyinitial-
izedKANcloselyresemblesthefinaltrainedKAN,requiringonlyminoradjustmentsintraining. In
contrast,adenseinitializationwouldinvolveextensivetrainingtoremoveunnecessaryedges.
Trick C: Hypothesis Testing When faced with multiple reasonable hypotheses, we can try all of
them(branchinginto“paralleluniverses”)totestwhichhypothesisisthemostaccurateand/orsim-
plest. Tofacilitatehypothesistesting,webuildacheckpointsystemthatautomaticallysavesmodel
versions whenever changes (e.g., training, pruning) are made. For example, consider the function
(cid:112)
f(m ,v,c)=m / 1−(v/c)2.WestartfromarandomlyinitializedKAN,whichhasversion0.0.
0 0
(cid:112)
Aftertraining,itevolvestoversion0.1,whereitactivatesonbothβ =v/candγ =1/ 1−(v/c)2.
Hypothesizethatonlyβorγmightbeneeded.Wefirstsettheedgeonγtozero,andtrainthemodel,
obtaininga6.5×10−4 testRMSE(version0.2). Totestthealternativehypothesis,wewanttore-
vertbacktothebranchingpoint(version0.1)–wecallmodel.rewind(‘0.1’)whichrewindsthe
modelbacktoversion0.1. Toindicatethatrewindiscalled,version0.1isrenamedtoversion1.1.
Nowwesettheedgeonβtobezero,trainthemodel,obtaininga2.0×10−6testRMSE(theversion
becomes1.2). Comparingversions0.2and1.2indicatesthatthesecondhypothesisisbetterdueto
thelowerlossgiventhesamecomplexity(bothhypotheseshavetwonon-zeroedges).
5 Applications
The previous sections primarily focused on regression problems for pedagogical purposes. In this
section,weapplyKANstodiscoverphysicalconcepts,suchasconservedquantities,Lagrangians,
hiddensymmetries,andconstitutivelaws. Theseexamplesillustratehowthetoolsproposedinthis
papercanbeeffectivelyintegratedintoreal-lifescientificresearchtotacklethesecomplextasks.
5.1 Discoveringconservedquantities
Figure10:UsingKANstodiscoverconservedquantitiesforthe2Dharmonicoscillator.
Conserved quantities are physical quantities that remain constant over time. For example, a free-
fallingballconvertsitsgravitationalpotentialenergyintokineticenergy,whilethetotalenergy(the
sum of both forms of energy) remains constant (assuming negligible air resistance). Conserved
quantitiesarecrucialbecausetheyoftencorrespondtosymmetriesinphysicalsystemsandcansim-
plify calculations by reducing the dimensionality of the system. Traditionally, deriving conserved
quantities with paper and pencil can be time-consuming and demands extensive domain knowl-
edge. Recently, machine learning techniques have been explored to discover conserved quanti-
ties[55,53,54,58,32,89].
14WefollowtheapproachLiuetal.[53], whichderivedadifferentialequationthatconservedquan-
tities must satisfy, thus transforming the problem of finding conserved quantities into differential
equationsolving. Theyusedmulti-layerperceptrons(MLPs)toparameterizeconservedquantities.
WebasicallyfollowtheirprocedurebutreplaceMLPswithKANs. Tobespecific,theyconsidera
dynamicalsystemwiththestatevariablez∈Rdgovernedbytheequation dz =f(z).Thenecessary
dt
and sufficient condition for a function H(z) to be a conserved quantity is that f(z)·∇H(z) = 0
for all z. For example, in a 1D harmonic oscillator, the phase space is characterized by position
and momentum, z = (x,p), and the evolution equation is d(x,p)/dt = (p,−x). The energy
H = 1(x2+p2)isaconservedquantitybecausef(z)·∇H(z)=(p,−x)·(x,p)=0. Weparam-
2
(cid:12) (cid:12)2
eterize H using a KAN, and train it with the loss function ℓ = (cid:80)N (cid:12)f(z(i))·∇(cid:98)H(z(i))(cid:12) where
i=1(cid:12) (cid:12)
∇(cid:98) is the normalized gradient, and z(i) are the ith data point uniformly drawn from the hypercube
[−1,1]d.
We choose the 2D harmonic oscillator to test KANs, characterized by (x,y,p ,p ). It has three
x y
conservedquantities: (1)energyalongxdirection: H = 1(x2+p2);(2)energyalongydirection:
1 2 x
H = 1(y2+p2);(3)angularmomentumH =xp −yp . Wetrain[4,[0,2],1]KANswiththree
2 2 y 3 y x
differentrandomseeds,asshowninFigure10,whichcorrespondtoH ,H andH respectively.
1 2 3
5.2 DiscoveringLagrangians
Figure11: UseKANstolearnLagrangiansforthesinglependulum(top)andarelativisticmassinauniform
field(bottom).
In physics, Lagrangian mechanics is a formulation of classical mechanics based on the principle
ofstationaryaction. ItdescribesamechanicalsystemusingphasespaceandasmoothfunctionL
knownastheLagrangian. Formanysystems,L=T −V,whereT andV representthekineticand
potentialenergyofthesystem,respectively. Thephasespaceistypicallydescribedby(q,q˙),where
q and q˙ denotes coordinates and velocities, respectively. The equation of motion can be derived
fromtheLagrangianviatheEuler-Lagrangeequation: d(∂L)= ∂L,orequivalently
dt ∂q˙ ∂q
q¨ =(∇ ∇TL)−1[∇ L−(∇ ∇Tq˙ )] (18)
q˙ q˙ q q q˙
GiventhefundamentalroleoftheLagrangian,aninterestingquestioniswhetherwecaninfertheLa-
grangianfromdata. Following[19],wetrainaLagrangianneuralnetworktopredictq¨ from(q,q˙).
AnLNNusesanMLPtoparameterizeL(q,q˙),andcomputestheEq.(18)topredictinstantaccel-
erations q¨. However, LNNs face two main challenges: (1) The training of LNNs can be unstable
15duetothesecond-orderderivativesandmatrixinversioninEq.(18). (2)LNNslackinterpretability
becauseMLPsthemselvesarenoteasilyinterpretable. WeaddresstheseissuesusingKANs.
To tackle the first challenge, we note that the matrix inversion of the Hessian (∇ ∇TL)−1 be-
q˙ q˙
comes problematic when the Hessian has eigenvalues close to zero. To mitigate this, we initialize
(∇ ∇TL)asapositivedefinitematrix(orapositivenumberin1D).Since(∇ ∇TL)isthemass
q˙ q˙ q˙ q˙
minclassicalmechanicsandkineticenergyisusuallyT = 1mq˙2,encodingthispriorknowledge
2
intoKANsismorestraightforwardthanintoMLPs(usingthekanpilerintroducedinSection3.3).
ThekanpilercanconvertthesymbolicformulaT intoaKAN(asshowninFigure11). Weusethis
convertedKANforinitializationandcontinuetraining,resultinginmuchgreaterstabilitycompared
torandominitialization. Aftertraining,symbolicregressioncanbeappliedtoeachedgetoextract
outsymbolicformulas,addressingthesecondchallenge.
Weshowtwo1DexamplesinFigure11,asinglependulumandarelativisticmassinauniformfield.
ThecompiledKANsaredisplayedontheleft, withedgesonq˙ displayingquadraticfunctionsand
edgesonqaszerofunctions.
SinglependulumTheq˙partremainsaquadraticfunctionT(q˙)= 1q˙2 whiletheqpartlearnstobe
2
a cosine function, as V(q) = 1−cos(q). In Figure 11 top, the results from suggest_symbolic
displaythetopfivefunctionsthatbestmatchthesplines,consideringbothfitnessandsimplicity. As
expected,thecosineandthequadraticfunctionappearatthetopofthelists.
Relativistic mass in a uniform field After training, the kinetic energy part deviates from T =
1q˙2 because, for a relativistic particle, T = (1−q˙2)−1/2 −1. In Figure 11 (bottom), symbolic
2 r
regressionsuccessfullyfindsV(q)=q,butfailstoidentifyT duetoitscompositionalnature,asour
r
symbolicregressiononlysearchesforsimplefunctions. Byassumingthefirstfunctioncomposition
is quadratic, we create another [1,1,1] KAN to fit T and set the first function to be the quadratic
r
functionusingfix_symbolic,andtrainonlythesecondlearnablefunction. Aftertraining,wesee
that the ground truth x−1/2 appears among the top five candidates. However, x1/2 fits the spline
slightlybetter,asindicatedbyahigherR-squaredvalue. Thissuggeststhatsymbolicregressionis
sensitive to noise (due to imperfect learning) and prior knowledge is crucial for correct judgment.
Forinstance, knowingthatkineticenergyshoulddivergeasvelocityapproachesthespeedoflight
helpsconfirmx−1/2asthecorrectterm,sincex1/2doesnotexhibittheexpecteddivergence.
5.3 Discoveringhiddensymmetry
Figure12: RediscoveringthehiddensymmetryoftheSchwarzschildblackholewithMLPsandKANs. (a)
∆t(r) learned by the MLP is a globally smooth solution; (b) ∆t(r) learned by the KAN is a domain-wall
solution; (c)TheKANshowsalossspikeatthedomainwall; (d)AKANcanbeusedtofine-tunetheMLP
solutionclosetomachineprecision.
PhilipAndersonfamouslyarguedthat“itisonlyslightlyoverstatingthatcasetosaythatphysicsis
thestudyofsymmetry”,emphasizinghowthediscoveryofsymmetrieshasbeeninvaluableforboth
deepeningourunderstandingandsolvingproblemsmoreefficiently.
However, symmetriesaresometimesnotmanifestbuthidden, onlyrevealedbyapplyingsomeco-
ordinate transformation. For example, after Schwarzschild discovered his eponymous black hole
metric, it took 17 years for Painlevé, Gullstrand and Lemaître to uncover its hidden translational
symmetry. Theydemonstratedthatthespatialsectionscouldbemadetranslationallyinvariantwith
aclevercoordinatetransformation,therebydeepeningourunderstandingofblackholes[65]. Liu&
Tegmark[56]showedthattheGullstrand-Painlevétransformationcanbediscoveredbytrainingan
MLP in minutes. However, they did not get extremely high precision (i.e., machine precision) for
thesolution. WeattempttorevisitthisproblemusingKANs.
16SupposethereisaSchwarzschildblackholeinspacetime(t,x,y,z)withmass2M = 1,centered
atx=y =z =0witharadiusr =2M =1. TheSchwarzschildmetricdescribeshowspaceand
s
timedistortsaroundit:
1− 2M 0 0 0 
r
 0 −1− 2Mx2 − 2Mxy − 2Mxz 
g µν =  0 − 2( Mr− x2 yM)r2 −1−(r−2 2M M)r y2 2 −(r− 2M2M yz)r2   (19)
 (r−2M)r2 (r−2M)r2 (r−2M)r2 
0 − 2Mxz − 2Myz −1− 2Mz2 .
(r−2M)r2 (r−2M)r2 (r−2M)r2
ApplyingtheGullstrand-Painlevétransformationt′ =t+2M(2u+ln(u−1)),u≡(cid:112) r ,x′ =x,
u+1 2M
y′ =y,z′ =z,themetricinthenewcoordinatesbecomes:
 (cid:113) (cid:113) (cid:113) 
1− 2M − 2M x − 2M y − 2M z
r r r r r r r
 (cid:113) 
− 2M x −1 0 0 
g µ′ ν =  −(cid:113) 2Mr yr
0 −1 0
  , (20)
 r r 
 (cid:113) 
− 2M z 0 0 −1
r r
which exhibits translation invariance in the spatial section (the lower right 3 × 3 block is the
Euclidean metric). Liu & Tegmark [56] used an MLP to learn the mapping from (t,x,y,z) to
(t′,x′,y′,z′). DefiningtheJacobianmatrixJ ≡ ∂(t′,x′,y′,z′), g istranformedtog′ = J−TgJ−1.
∂(t,x,y,z)
Wetakethebottomright3×3blockofg′ andtakeitsdifferencetotheEuclideanmetrictoobtain
theMSEloss.ThelossisminimizedbydoinggradientdescentsontheMLP.Tomakethingssimple,
theyassumeknowingx′ =x,y′ =y,z′ =z,andonlyuseanMLP(1inputand1output)topredict
thetemporaldifference∆t(r)=t′−t=2M(2u+ln(u−1)),u≡(cid:112) r fromtheradiusr.
u+1 2M
MLP and KAN find different solutions We trained both an MLP and a KAN to minimize this
loss function, with results shown in Figure 12. Since the task has 1 input dimension and 1 output
dimension, the KAN effectively reduces to a spline. We originally expected KANs to outperform
MLPs,becausesplinesareknowntobesuperiorinlow-dimensionalsettings[63]. However,while
MLP can achieve 10−8 loss, the KAN gets stuck at 10−3 loss despite grid refinements. It turned
out that KAN and MLP learned two different solutions: while the MLP found a globally smooth
solution(Figure12(a)),theKANlearnedadomain-wallsolution(Figure12(b)). Thedomainwall
solution has a singular point that separates the whole curve into two segments. The left segment
learns ∆t(r) correctly, while the right segment learns −∆t(r), which is also a valid solution but
differsfromtheleftsegmentbyaminussign. Thereisalossspikeappearingatthesingularpoint
(Figure 12 (c)). One might consider this as a feature of KANs because domain wall solutions are
prevalent in nature. However, if one considers this a flaw, KANs can still obtain globally smooth
solutions by adding regularizations (to reduce spline oscillations) or experimenting with different
randomseeds(roughly1outof3randomseedsfindsaglobalsmoothsolution).
KANs can achieve extreme precision Although the MLP finds the globally smooth solution and
achieves10−8loss,thelossisstillfarfrommachineprecision.Wefoundthatneitherlongertraining
norincreasingtheMLP’ssizesignificantlyreducedtheloss. Therefore,weturnedtoKANs,which,
as splines in 1D, can achieve arbitrary accuracy by refining the grid (given infinite data). We first
usedtheMLPasateacher,generatingsupervisedpairs(x,y)totraintheKANtofitthesupervised
data. Thisway,theKANisinitializedtoagloballysmoothsolution. Wetheniterativelyrefinedthe
KANbyincreasingthenumberofgridintervalsto1000. Intheend,thefine-tunedKANsachievea
lossof10−15,closetomachineprecision(Figure12(d)).
5.4 Learningconstitutivelaws
Aconstitutivelawdefinesthebehaviorandpropertiesofamaterialbymodelinghowitrespondsto
externalforcesordeformations. OneofthesimplestformsofconstitutivelawisHooke’sLaw[34],
whichrelatesthestrainandstressofelasticmaterialslinearly. Constitutivelawsencompassawide
range of materials, including elastic materials [80, 68], plastic materials [64], and fluids [8]. Tra-
ditionally,theselawswerederivedfromfirstprinciplesbasedontheoreticalandexperimentalstud-
ies [79, 81, 6, 29]. Recent advancements, however, have introduced data-driven approaches that
leveragemachinelearningtodiscoverandrefinetheselawsfromdedicateddatasets[73,91,59,60].
17Figure13:Discoveringconstitutivelaws(relationsbetweenthepressuretensorP andthestraintensorF)with
KANsbyinteractingwiththem.Top:predictingthediagonalelementP ;bottom:predictingtheoff-diagonal
11
elementP .
12
18WefollowthestandardnotationsandexperimentalsetupsintheelasticitypartofNCLaw[59]and
define the constitutive law as a parameterized function E (F) → P, where F denotes the defor-
θ
mation tensor, P the first Piola–Kirchhoff stress tensor, and θ the parameters in the constitutive
law.
Manyisotropicmaterialshavelinearconstitutivelawswhendeformationissmall:
P =µ(F+FT −2I)+λ(Tr(F)−3)I. (21)
l
However, when deformation gets larger, nonlinear effects start to kick in. For example, a Neo-
Hookeanmaterialhasthefollowingconstitutivelaw:
P=µ(FFT −I)+λlog(det(F))I, (22)
whereµandλaretheso-calledLaméparametersdeterminedbytheso-calledYoung’smodulusY
andPoissonratioνasµ= Y ,λ= Yν . Forsimplicity,wechooseY =1andν =0.2,
2(1+ν) (1+ν)(1−2ν)
henceµ= 5 ≈0.42andλ= 5 ≈0.28.
12 18
AssumingweareworkingwithNeo-Hookeanmaterials,andourgoalistouseKANstopredictthe
PtensorfromtheFtensor. Supposewedonotknowtheyareneo-Hookeanmaterials,butwehave
the prior knowledge that the linear constitutive law is approximately valid for small deformation.
Due to symmetries, it suffices to demonstrate that we can accurately predict P and P from
11 12
the 9 matrix elements of F. We want to compile linear constitutive laws into KANs, which are
P = 2µ(F −1)+λ(F +F +F −3), and P = µ(F +F ). We want to extract
11 11 11 22 33 12 12 21
Neo-HookeanlawsfromtrainedKANs,whichareP =µ(F2 +F2 +F2 −1)+λlog(det(F)),
11 11 12 13
and P = µ(F F + F F + F F ). We generate a synthetic dataset by sampling F
12 11 21 12 22 13 23 ij
independentlyfromU[δ −w,δ +w](w = 0.2)andusingtheNeo-Hookeanconstitutivelawto
ij ij
computeP. OurinteractionwithKANsisillustratedinFigure13. Inbothcases, wesuccessfully
figuredoutthetruesymbolicformulasintheend,withtheaidofsomeinductivebiases.However,the
keytakeawayisnotthatwecanrediscovertheexactsymbolicformulas–giventhatpriorknowledge
skews the process – but rather in real-world scenarios, where the answers are unknown and users
canmakeguessesbasedonpriorknowledge,thepykanpackagemakesiteasytotestorincorporate
priorknowledge.
Predicting P In step 1, we compile the linear constitutive law P = 2µ(F −1)+λ(F +
11 11 11 11
F +F −3)toaKANusingkanpiler,resultingina10−2 loss. Instep2,weperturbtheKAN
22 33
sothatitbecomestrainable(indicatedbythecolorchangefromredtopurple;reddenotesapurely
symbolicpart,whilepurpleindicatesthatbothsymbolicandsplinepartsareactive). Instep3,we
train the perturbed model until convergence, giving a 6×10−3 loss. In step 4, assuming that the
determinantisakeyauxiliaryvariable,weuseexpand_width(fortheKAN)andaugment_input
(for the dataset) to include the determinant |F|. In step 5, we train the KAN until convergence,
giving a 2×10−4 loss. In step 6, we symbolify the KAN to obtain a symbolic formula P =
11
0.42(F2 +F2 +F2 −1)+0.28log(|F|),whichachievesa3×10−11loss.
11 12 13
Predicting P We experimented with and without encoding the linear constitutive law as prior
12
knowledge. With prior knowledge: in step 1, we compile the linear constitutive law to a KAN,
resultinginalossof10−2.Wethenperformaseriesofoperations,includingexpand(step2),perturb
(step3),train(step4),prune(step5)andfinallysymbolic(step6).Theinfluenceofpriorknowledge
isevident,asthefinalKANonlyidentifiesminorcorrectiontermstothelinearconstitutivelaw. The
finalKANissymbolifiedasP =0.42(F +F )+0.44F F −0.03F2 +0.02F2 whichyields
12 12 21 13 23 21 12
a7×10−3 loss,onlyslightlybetterthanthelinearconstitutivelaw. Withoutpriorknowledge: in
step1,werandomlyinitializetheKANmodel. Instep2,wetraintheKANwithregularization. In
step3,weprunetheKANtobeamorecompactmodel. Instep4,wesymbolifytheKAN,yielding
P = 0.42(F F +F F +F F ), which closely matches the exact formula, achieving a
12 11 21 12 22 13 23
6×10−9loss. Comparingthetwoscenarios–onewithandonewithoutpriorknowledge–revealsa
surprisingoutcome: inthisexample,priorknowledgeappearsharmful,possiblybecausethelinear
constitutive law is probably near a (bad) local minimum which is hard for the model to escape.
However, weshouldprobablynotrandomlyextrapolatethisconclusiontomorecomplicatedtasks
and larger networks. For more complicated tasks, finding a local minimum via gradient descent
mightbechallengingenough,makinganapproximateinitialsolutiondesirable. Additionally,larger
networksmightbesufficientlyover-parameterizedtoeliminatebadlocalminima,ensuringthatall
localminimaareglobalandinterconnected.
19Figure14:KANinterpolatesbetweensoftware1.0and2.0.(a)KANsstrikeabalancebetweeninterpretability
(software1.0)andlearnability(software2.0).(b)KANs’Paretofrontierontheinterpretability-scaleplane.The
amountofinterpretationwecangetfromKANsdependsonproblemscalesandinterpretabilitymethods.
6 Relatedworks
Kolmogorov-Arnold Networks (KANs), inspired by the Kolmogorov-Arnold representation the-
orem (KART), were recently proposed by Liu et al. [57]. Although the connection between
KART and networks has long been deemed irrelevant [30], Liu et al. generalized the origi-
nal two-layer network to arbitrary depths and demonstrated their promise for science-oriented
tasks given their accuracy and interpretability. Subsequent research has explored the application
of KANs across various domains, including graphs [12, 22, 38, 99], partial differential equa-
tions [87, 78] and operator learning [1, 78, 67], tabular data [70], time series [85, 28, 93, 27],
human activity recognition [49, 50],neuroscience [96, 33], quantum science [40, 46, 4], computer
vision[17,7,44,16,76,10],kernellearning[101],nuclearphysics[48],electricalengineering[69],
biology [71]. Liu et al. used B-splines to parameterize 1D functions, and other research have ex-
plored various activation functions, including wavelet [11, 76], radial basis function [47], Fourier
series[92]),finitebasis[35,82],Jacobibasisfunctions[2],polynomialbasisfunctions[75],ratio-
nalfunctions[3]. OthertechniquesforKANshavealsobeenproposedincludingregularization[5],
Kansformer (combining transformer and KAN) [15], adaptive grid update [72], federated learn-
ing [98] , Convolutional KANs [10]. There have been ongoing debates regarding whether KANs
reallyoutperformotherneuralnetworks(especiallyMLPs)onvariousdomains[7,16,42,77,97],
whichsuggeststhatwhileKANsshowpromiseformachinelearningtasks,furtherdevelopmentis
neededtosurpassstate-of-the-artmodels.
Machine Learning for Physical Laws A major goal for KANs is to aid in the discovery of
new physical laws from data. Previous research has shown that machine learning can be used to
learn various types of physical laws, including equations of motion [90, 13, 43, 20], conservation
laws [55, 53, 54, 58, 32, 89], symmetries [39, 56, 94], phase transitions [88, 14], Lagrangian and
Hamiltonian [19, 31], and symbolic regression [18, 61, 23, 74], etc. However, making neural net-
works interpretable often requires domain-specific knowledge, limiting their generality. We hope
thatKANswillevolveintouniversalfoundationmodelsforphysicaldiscoveries.
Mechanistic Interpretability seeks to understand how neural networks operate in a fundamental
level [21, 62, 86, 25, 66, 100, 51, 24, 45, 26]. Some research in this area focuses on designing
modelsthatareinherentlyinterpretable[24]orproposingtrainingmethodsthatexplicitlypromote
interpretability [51]. KANs fall into this category since the Kolmogorov-Arnold theorem decom-
posesahigh-dimensionalfunctionintoacollectionof1Dfunctions, whicharesignificantlyeasier
tointerpretthanhigh-dimensionalfunctions.
7 Discussion
KANinterpolatesbetweensoftware1.0and2.0ThekeydifferencebetweenKolmogorov-Arnold
Networks(KANs)andotherneuralnetworks(software2.0,atermcoinedbyAndrejKarpathy)lies
in their greater interpretability, which allows for manipulation by users, similar to traditional soft-
ware (software 1.0). However, KANs are not entirely traditional software, as they (1) learnability
(good),enablingthemtolearnnewthingsfromdata,and(2)reducedinterpretability(bad)asthey
20becomelessinterpretableandcontrollableasthenetworkscalesincrease.InFigure14(a)visualizes
thepositionofsoftware1.0,software2.0,andKANsontheinterpretability-learnabilityplane,illus-
tratinghowKANscanbalancethetrade-offsbetweenthesetwoparadigms. Thegoalofthispaper
istoproposevarioustoolsthatmakeKANsmorelikesoftware1.0,whileleveragingthelearnability
ofsoftware2.0.
EfficiencyimprovementTheoriginalpykanpackage[57]waspoorinefficiency. Wehaveincor-
poratedafewtechniquestoimproveitsefficiency.
1. Efficientsplinesevaluations.InspiredbyEfficientKAN[9],wehaveoptimizedsplineeval-
uationsbyavoidingunnecessaryinputexpansions. ForaKANwithLlayers, N neurons
perlayer,andgridsizeG,memoryusagehasbeenreducedfromO(LN2G)toO(LNG).
2. Enabling the symbolic branch only when needed. A KAN layer contains both a spline
branchandasymbolicbranch.Thesymbolicbranchismuchmoretime-consumingthanthe
splinebranchsinceitcannotbeparallelized(disastrousdoubleloopsareneeded).However,
inmanyapplications,thesymbolicbranchisunnecessary,sowecanskipitwhenpossible,
significantlyreducingruntime,especiallywhenthenetworkislarge.
3. Saving intermediate activations only when needed. To plot KAN diagrams, intermediate
activations must be saved. Initially, activations were saved by default, leading to slower
runtime and excessive memory usage. We now save intermediate activations only when
needed (e.g., for plotting or applying regularizations in training). Users can enable these
efficiencyimprovementswithasingleline: model.speed().
4. GPUacceleration. Initially,allmodelswererunonCPUsduetothesmall-scalenatureof
theproblems. WehavenowmadethemodelGPU-compatible6. Forexample, traininga
[4,100,100,100,1] with Adam for 100 steps used to take an entire day on a CPU (before
implementing1,2,3),butnowtakes20secondsonaCPUandlessthanonesecondona
GPU.However,KANsstilllagbehindMLPsinefficiency,especiallyatlargescales. The
communityhasbeenworkingtowardsbenchmarkingandimprovingKAN’sefficiencyand
theefficiencygaphasbeensignificantlyreduced[36].
Since the objective of this paper is to make KANs more like software 1.0, when facing trade-offs
between 1.0 (being interactive and versatile) and 2.0 (being efficient and specific), we prioritize
interactivityandversatilityoverefficiency.Forexample,westorecacheddatawithinmodels(which
consumesadditionalmemory),souserscansimplycallmodel.plot()togenerateaKANdiagram
withoutmanuallydoingaforwardpasstocollectdata.
Interpretability Although the learnable univariate functions in KANs are more interpretable than
weight matrices in MLPs, scalability remains a challenge. As KAN models scale up, even if all
splinefunctionsareinterpretableindividually,itbecomesincreasinglydifficulttomanagethecom-
bined output of these 1D functions. Consequently, a KAN may only remain interpretable when
the network scale is relatively small (Figure 14 (b), thick red line). It is important to note that
interpretability depends on both intrinsic factors (related to the model itself) and extrinsic factors
(related to interpretability methods). Advanced interpretability methods should be able to handle
interpretability at various levels. For example, by interpreting KANs with symbolic regression,
modularity discovery and feature attribution (Figure 14 (b), thin red lines), the Pareto Frontier of
interpretabilityversusscaleextendsbeyondwhataKANalonecanachieve. Apromisingdirection
for future research is to develop more advanced interpretability methods that can further push the
currentParetoFrontiers.
Future work This paper introduces a framework that integrates KANs with scientific knowledge,
focusingprimarilyonsmall-scale,physics-relatedexamples. Movingforward,twopromisingdirec-
tions include applying this framework to larger-scale problems and extending it to other scientific
disciplinesbeyondphysics.
Acknowledgement
We would like to thank Yizhou Liu, Di Luo, Akash Kundu and many GitHub users for fruitful
discussion and constructive suggestions. We extend special thanks to GitHub user Blealtan for
6ModelscanbetrainedonGPUs,butnotallfunctionalitiesalreadysupportedGPU.
21making public their awesome work on making KANs efficient. Z.L. and M.T. are supported by
IAIFIthroughNSFgrantPHY-2019786.
References
[1] D.W.Abueidda,P.Pantidis,andM.E.Mobasher. Deepokan: Deepoperatornetworkbased
onkolmogorovarnoldnetworksformechanicsproblems. arXivpreprintarXiv:2405.19143,
2024.
[2] A.A.Aghaei. fkan:Fractionalkolmogorov-arnoldnetworkswithtrainablejacobibasisfunc-
tions. arXivpreprintarXiv:2406.07456,2024.
[3] A.A.Aghaei.rkan:Rationalkolmogorov-arnoldnetworks.arXivpreprintarXiv:2406.14495,
2024.
[4] T. Ahmed and M. H. R. Sifat. Graphkan: Graph kolmogorov arnold network for small
molecule-protein interaction predictions. In ICML’24 Workshop ML for Life and Material
Science: FromTheorytoIndustryApplications,2024.
[5] M.G.Altarabichi. Dropkan: Regularizingkansbymaskingpost-activations. arXivpreprint
arXiv:2407.13044,2024.
[6] E. M. Arruda and M. C. Boyce. A three-dimensional constitutive model for the large
stretchbehaviorofrubberelasticmaterials. JournaloftheMechanicsandPhysicsofSolids,
41(2):389–412,1993.
[7] B.AzamandN.Akhtar. Suitabilityofkansforcomputervision:Apreliminaryinvestigation.
arXivpreprintarXiv:2406.09087,2024.
[8] G.K.Batchelor. Anintroductiontofluiddynamics. Cambridgeuniversitypress,2000.
[9] Blealtan. Blealtan/efficient-kan: An efficient pure-pytorch implementation of kolmogorov-
arnoldnetwork(kan).
[10] A. D. Bodner, A. S. Tepsich, J. N. Spolski, and S. Pourteau. Convolutional kolmogorov-
arnoldnetworks. arXivpreprintarXiv:2406.13155,2024.
[11] Z.BozorgaslandH.Chen. Wav-kan: Waveletkolmogorov-arnoldnetworks. arXivpreprint
arXiv:2405.12832,2024.
[12] R. Bresson, G. Nikolentzos, G. Panagopoulos, M. Chatzianastasis, J. Pang, and M. Vazir-
giannis. Kagnns: Kolmogorov-arnold networks meet graph learning. arXiv preprint
arXiv:2406.18380,2024.
[13] S.L.Brunton, J.L.Proctor, andJ.N.Kutz. Discoveringgoverningequationsfromdataby
sparse identification of nonlinear dynamical systems. Proceedings of the national academy
ofsciences,113(15):3932–3937,2016.
[14] J. Carrasquilla and R. G. Melko. Machine learning phases of matter. Nature Physics,
13(5):431–434,2017.
[15] Y. Chen, Z. Zhu, S. Zhu, L. Qiu, B. Zou, F. Jia, Y. Zhu, C. Zhang, Z. Fang, F. Qin, et al.
Sckansformer: Fine-grained classification of bone marrow cells via kansformer backbone
andhierarchicalattentionmechanisms. arXivpreprintarXiv:2406.09931,2024.
[16] M.Cheon. Demonstratingtheefficacyofkolmogorov-arnoldnetworksinvisiontasks. arXiv
preprintarXiv:2406.14916,2024.
[17] M.Cheon. Kolmogorov-arnoldnetworkforsatelliteimageclassificationinremotesensing.
arXivpreprintarXiv:2406.00600,2024.
[18] M.Cranmer. Interpretablemachinelearningforsciencewithpysrandsymbolicregression.jl.
arXivpreprintarXiv:2305.01582,2023.
[19] M.Cranmer,S.Greydanus,S.Hoyer,P.Battaglia,D.Spergel,andS.Ho. Lagrangianneural
networks. arXivpreprintarXiv:2003.04630,2020.
22[20] M.Cranmer,A.SanchezGonzalez,P.Battaglia,R.Xu,K.Cranmer,D.Spergel,andS.Ho.
Discoveringsymbolicmodelsfromdeeplearningwithinductivebiases. Advancesinneural
informationprocessingsystems,33:17429–17442,2020.
[21] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find
highlyinterpretablefeaturesinlanguagemodels. arXivpreprintarXiv:2309.08600,2023.
[22] G. De Carlo, A. Mastropietro, and A. Anagnostopoulos. Kolmogorov-arnold graph neural
networks. arXivpreprintarXiv:2406.18354,2024.
[23] O.Dugan,R.Dangovski,A.Costa,S.Kim,P.Goyal,J.Jacobson,andM.Soljacˇic´.Occamnet:
Afastneuralmodelforsymbolicregressionatscale. arXivpreprintarXiv:2007.10784,2020.
[24] N.Elhage,T.Hume,C.Olsson,N.Nanda,T.Henighan,S.Johnston,S.ElShowk,N.Joseph,
N.DasSarma,B.Mann,D.Hernandez,A.Askell,K.Ndousse,A.Jones,D.Drain,A.Chen,
Y.Bai,D.Ganguli,L.Lovitt,Z.Hatfield-Dodds,J.Kernion,T.Conerly,S.Kravec,S.Fort,
S. Kadavath, J. Jacobson, E. Tran-Johnson, J. Kaplan, J. Clark, T. Brown, S. McCan-
dlish, D. Amodei, and C. Olah. Softmax linear units. Transformer Circuits Thread, 2022.
https://transformer-circuits.pub/2022/solu/index.html.
[25] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds,
R. Lasenby, D. Drain, C. Chen, et al. Toy models of superposition. arXiv preprint
arXiv:2209.10652,2022.
[26] J. Engels, I. Liao, E. J. Michaud, W. Gurnee, and M. Tegmark. Not all language model
featuresarelinear. arXivpreprintarXiv:2405.14860,2024.
[27] R. Genet and H. Inzirillo. A temporal kolmogorov-arnold transformer for time series fore-
casting. arXivpreprintarXiv:2406.02486,2024.
[28] R. Genet and H. Inzirillo. Tkan: Temporal kolmogorov-arnold networks. arXiv preprint
arXiv:2405.07344,2024.
[29] A. N. Gent. A new constitutive relation for rubber. Rubber chemistry and technology,
69(1):59–61,1996.
[30] F. Girosi and T. Poggio. Representation properties of networks: Kolmogorov’s theorem is
irrelevant. NeuralComputation,1(4):465–469,1989.
[31] S.Greydanus,M.Dzamba,andJ.Yosinski.Hamiltonianneuralnetworks.Advancesinneural
informationprocessingsystems,32,2019.
[32] S.HaandH.Jeong. Discoveringconservationlawsfromtrajectoriesviamachinelearning.
arXivpreprintarXiv:2102.04008,2021.
[33] L. F. Herbozo Contreras, J. Cui, L. Yu, Z. Huang, A. Nikpour, and O. Kavehei. Kan-eeg:
Towardsreplacingbackbone-mlpforaneffectiveseizuredetectionsystem. medRxiv, pages
2024–06,2024.
[34] R. Hooke. Lectures de potentia restitutiva, or of spring explaining the power of springing
bodies. Number6.JohnMartyn,2016.
[35] A.A.Howard,B.Jacob,S.H.Murphy,A.Heinlein,andP.Stinis. Finitebasiskolmogorov-
arnold networks: domain decomposition for data-driven and physics-informed problems.
arXivpreprintarXiv:2406.19662,2024.
[36] Jerry-Master. Jerry-master/kan-benchmarking.
[37] J.Jumper,R.Evans,A.Pritzel,T.Green,M.Figurnov,O.Ronneberger,K.Tunyasuvunakool,
R. Bates, A. Žídek, A. Potapenko, et al. Highly accurate protein structure prediction with
alphafold. nature,596(7873):583–589,2021.
[38] M.Kiamari,M.Kiamari,andB.Krishnamachari.Gkan:Graphkolmogorov-arnoldnetworks.
arXivpreprintarXiv:2406.06470,2024.
[39] S.KrippendorfandM.Syvaeri.Detectingsymmetrieswithneuralnetworks.MachineLearn-
ing: ScienceandTechnology,2(1):015010,2020.
23[40] A. Kundu, A. Sarkar, and A. Sadhu. Kanqas: Kolmogorov arnold network for quantum
architecturesearch. arXivpreprintarXiv:2406.17630,2024.
[41] R.Lam,A.Sanchez-Gonzalez,M.Willson,P.Wirnsberger,M.Fortunato,F.Alet,S.Ravuri,
T. Ewalds, Z. Eaton-Rosen, W. Hu, et al. Learning skillful medium-range global weather
forecasting. Science,382(6677):1416–1421,2023.
[42] T. X. H. Le, T. D. Tran, H. L. Pham, V. T. D. Le, T. H. Vu, V. T. Nguyen, Y. Nakashima,
etal. Exploringthelimitationsofkolmogorov-arnoldnetworksinclassification: Insightsto
softwaretrainingandhardwareimplementation. arXivpreprintarXiv:2407.17790,2024.
[43] P.Lemos,N.Jeffrey,M.Cranmer,S.Ho,andP.Battaglia. Rediscoveringorbitalmechanics
withmachinelearning. MachineLearning: ScienceandTechnology,4(4):045002,2023.
[44] C.Li,X.Liu,W.Li,C.Wang,H.Liu,andY.Yuan.U-kanmakesstrongbackboneformedical
imagesegmentationandgeneration. arXivpreprintarXiv:2406.02918,2024.
[45] K. Li, A. K. Hopkins, D. Bau, F. Viégas, H. Pfister, and M. Wattenberg. Emergent world
representations: Exploring a sequence model trained on a synthetic task. In The Eleventh
InternationalConferenceonLearningRepresentations,2023.
[46] X. Li, Z. Feng, Y. Chen, W. Dai, Z. He, Y. Zhou, and S. Jiao. Coeff-kans: A paradigm to
addresstheelectrolytefieldwithkans. arXivpreprintarXiv:2407.20265,2024.
[47] Z. Li. Kolmogorov-arnold networks are radial basis function networks. arXiv preprint
arXiv:2405.06721,2024.
[48] H. Liu, J. Lei, and Z. Ren. From complexity to clarity: Kolmogorov-arnold networks in
nuclearbindingenergyprediction,2024.
[49] M.Liu,S.Bian,B.Zhou,andP.Lukowicz. ikan: Globalincrementallearningwithkanfor
humanactivityrecognitionacrossheterogeneousdatasets. arXivpreprintarXiv:2406.01646,
2024.
[50] M.Liu,D.Geißler,D.Nshimyimana,S.Bian,B.Zhou,andP.Lukowicz.Initialinvestigation
of kolmogorov-arnold networks (kans) as feature extractors for imu based human activity
recognition. arXivpreprintarXiv:2406.11914,2024.
[51] Z. Liu, E. Gan, and M. Tegmark. Seeing is believing: Brain-inspired modular training for
mechanisticinterpretability. Entropy,26(1):41,2023.
[52] Z.Liu,M.Khona,I.R.Fiete,andM.Tegmark.Growingbrains:Co-emergenceofanatomical
and functional modularity in recurrent neural networks. arXiv preprint arXiv:2310.07711,
2023.
[53] Z.Liu,V.Madhavan,andM.Tegmark. Machinelearningconservationlawsfromdifferential
equations. PhysicalReviewE,106(4):045307,2022.
[54] Z. Liu, P. O. Sturm, S. Bharadwaj, S. J. Silva, and M. Tegmark. Interpretable conservation
lawsassparseinvariants. Phys.Rev.E,109:L023301,Feb2024.
[55] Z. Liu and M. Tegmark. Machine learning conservation laws from trajectories. Phys. Rev.
Lett.,126:180604,May2021.
[56] Z. Liu and M. Tegmark. Machine learning hidden symmetries. Physical Review Letters,
128(18):180201,2022.
[57] Z.Liu,Y.Wang,S.Vaidya,F.Ruehle,J.Halverson,M.Soljacˇic´,T.Y.Hou,andM.Tegmark.
Kan: Kolmogorov-arnoldnetworks. arXivpreprintarXiv:2404.19756,2024.
[58] P.Y.Lu,R.Dangovski,andM.Soljacˇic´. Discoveringconservationlawsusingoptimaltrans-
portandmanifoldlearning. NatureCommunications,14(1):4744,2023.
[59] P.Ma,P.Y.Chen,B.Deng,J.B.Tenenbaum,T.Du,C.Gan,andW.Matusik.Learningneural
constitutivelawsfrommotionobservationsforgeneralizablepdedynamics. InInternational
ConferenceonMachineLearning,pages23279–23300.PMLR,2023.
24[60] P. Ma, T.-H. Wang, M. Guo, Z. Sun, J. B. Tenenbaum, D. Rus, C. Gan, and W. Matusik.
Llm and simulation as bilevel optimizers: A new paradigm to advance physical scientific
discovery. InForty-firstInternationalConferenceonMachineLearning,2024.
[61] G. Martius and C. H. Lampert. Extrapolation and learning equations. arXiv preprint
arXiv:1610.02995,2016.
[62] K.Meng,D.Bau,A.J.Andonian,andY.Belinkov. Locatingandeditingfactualassociations
in GPT. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural
InformationProcessingSystems,2022.
[63] E. J. Michaud, Z. Liu, and M. Tegmark. Precision machine learning. Entropy, 25(1):175,
2023.
[64] R. v. Mises. Mechanik der festen körper im plastisch-deformablen zustand. Nachrichten
von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse,
1913:582–592,1913.
[65] C.Misner,K.Thorne,andJ.Wheeler. Gravitation. PrincetonUniversityPress,2017.
[66] N.Nanda,L.Chan,T.Lieberum,J.Smith,andJ.Steinhardt. Progressmeasuresforgrokking
viamechanisticinterpretability. InTheEleventhInternationalConferenceonLearningRep-
resentations,2023.
[67] G.NehmaandM.Tiwari. Leveragingkansforenhanceddeepkoopmanoperatordiscovery.
arXivpreprintarXiv:2406.02875,2024.
[68] R.W.Ogden. Non-linearelasticdeformations. CourierCorporation,1997.
[69] Y.Peng,M.He,F.Hu,Z.Mao,X.Huang,andJ.Ding. Predictivemodelingofflexibleehd
pumpsusingkolmogorov-arnoldnetworks. arXivpreprintarXiv:2405.07488,2024.
[70] E. Poeta, F. Giobergia, E. Pastor, T. Cerquitelli, and E. Baralis. A benchmarking study of
kolmogorov-arnoldnetworksontabulardata. arXivpreprintarXiv:2406.14529,2024.
[71] P.Pratyush,C.Carrier,S.Pokharel,H.D.Ismail,M.Chaudhari,andD.B.KC.Calmphoskan:
Predictionofgeneralphosphorylationsitesinproteinsviafusionofcodonawareembeddings
withaminoacidawareembeddingsandwavelet-basedkolmogorovarnoldnetwork. bioRxiv,
pages2024–07,2024.
[72] S. Rigas, M. Papachristou, T. Papadopoulos, F. Anagnostopoulos, and G. Alexandridis.
Adaptive training of grid-dependent physics-informed kolmogorov-arnold networks. arXiv
preprintarXiv:2407.17611,2024.
[73] A.Sanchez-Gonzalez,J.Godwin,T.Pfaff,R.Ying,J.Leskovec,andP.Battaglia. Learning
to simulate complex physics with graph networks. In International conference on machine
learning,pages8459–8468.PMLR,2020.
[74] M.SchmidtandH.Lipson.Distillingfree-formnaturallawsfromexperimentaldata.science,
324(5923):81–85,2009.
[75] S. T. Seydi. Exploring the potential of polynomial basis functions in kolmogorov-arnold
networks: A comparative study of different groups of polynomials. arXiv preprint
arXiv:2406.02583,2024.
[76] S.T.Seydi. Unveilingthepowerofwavelets: Awavelet-basedkolmogorov-arnoldnetwork
forhyperspectralimageclassification. arXivpreprintarXiv:2406.07869,2024.
[77] H. Shen, C. Zeng, J. Wang, and Q. Wang. Reduced effectiveness of kolmogorov-arnold
networksonfunctionswithnoise. arXivpreprintarXiv:2407.14882,2024.
[78] K. Shukla, J. D. Toscano, Z. Wang, Z. Zou, and G. E. Karniadakis. A comprehensive and
faircomparisonbetweenmlpandkanrepresentationsfordifferentialequationsandoperator
networks. arXivpreprintarXiv:2406.02917,2024.
[79] E. Sifakis and J. Barbic. Fem simulation of 3d deformable solids: a practitioner’s guide to
theory,discretizationandmodelreduction. InAcmsiggraph2012courses,pages1–50.2012.
25[80] W. S. Slaughter. The linearized theory of elasticity. Springer Science & Business Media,
2012.
[81] B.Smith,F.D.Goes,andT.Kim. Stableneo-hookeanfleshsimulation. ACMTransactions
onGraphics(TOG),37(2):1–15,2018.
[82] H.-T.Ta. Bsrbf-kan: Acombinationofb-splinesandradialbasicfunctionsinkolmogorov-
arnoldnetworks. arXivpreprintarXiv:2406.11173,2024.
[83] T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without
humandemonstrations. Nature,625(7995):476–482,2024.
[84] S.-M.Udrescu,A.Tan,J.Feng,O.Neto,T.Wu,andM.Tegmark. Aifeynman2.0: Pareto-
optimal symbolic regression exploiting graph modularity. Advances in Neural Information
ProcessingSystems,33:4860–4871,2020.
[85] C.J.Vaca-Rubio,L.Blanco,R.Pereira,andM.Caus. Kolmogorov-arnoldnetworks(kans)
fortimeseriesanalysis. arXivpreprintarXiv:2405.08790,2024.
[86] K.R.Wang,A.Variengien,A.Conmy,B.Shlegeris,andJ.Steinhardt. Interpretabilityinthe
wild:acircuitforindirectobjectidentificationinGPT-2small. InTheEleventhInternational
ConferenceonLearningRepresentations,2023.
[87] Y. Wang, J. Sun, J. Bai, C. Anitescu, M. S. Eshaghi, X. Zhuang, T. Rabczuk, and Y. Liu.
Kolmogorovarnoldinformedneuralnetwork: Aphysics-informeddeeplearningframework
for solving pdes based on kolmogorov arnold networks. arXiv preprint arXiv:2406.11045,
2024.
[88] S.J.Wetzel. Unsupervisedlearningofphasetransitions: Fromprincipalcomponentanalysis
tovariationalautoencoders. PhysicalReviewE,96(2):022140,2017.
[89] S.J.Wetzel,R.G.Melko,J.Scott,M.Panju,andV.Ganesh.Discoveringsymmetryinvariants
andconservedquantitiesbyinterpretingsiameseneuralnetworks. Phys.Rev.Res.,2:033499,
Sep2020.
[90] T.WuandM.Tegmark. Towardanartificialintelligencephysicistforunsupervisedlearning.
PhysicalReviewE,100(3):033311,2019.
[91] H. Xu, F. Sin, Y. Zhu, and J. Barbicˇ. Nonlinear material design using principal stretches.
ACMTransactionsonGraphics(TOG),34(4):1–11,2015.
[92] J.Xu,Z.Chen,J.Li,S.Yang,W.Wang,X.Hu,andE.C.-H.Ngai. Fourierkan-gcf: Fourier
kolmogorov-arnold network–an effective and efficient feature transformation for graph col-
laborativefiltering. arXivpreprintarXiv:2406.01034,2024.
[93] K. Xu, L. Chen, and S. Wang. Kolmogorov-arnold networks for time series: Bridging pre-
dictivepowerandinterpretability. arXivpreprintarXiv:2406.02496,2024.
[94] J.Yang,R.Walters,N.Dehmamy,andR.Yu. Generativeadversarialsymmetrydiscovery. In
InternationalConferenceonMachineLearning,pages39488–39508.PMLR,2023.
[95] K. Yang, A. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil, R. J. Prenger, and
A. Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[96] S. Yang, L. Qin, and X. Yu. Endowing interpretability for neural cognitive diagnosis by
efficientkolmogorov-arnoldnetworks. arXivpreprintarXiv:2405.14399,2024.
[97] R. Yu, W. Yu, and X. Wang. Kan or mlp: A fairer comparison. arXiv preprint
arXiv:2407.16674,2024.
[98] E. Zeydan, C. J. Vaca-Rubio, L. Blanco, R. Pereira, M. Caus, and A. Aydeger. F-kans:
Federatedkolmogorov-arnoldnetworks. arXivpreprintarXiv:2407.20100,2024.
[99] F. Zhang and X. Zhang. Graphkan: Enhancing feature extraction with graph kolmogorov
arnoldnetworks. arXivpreprintarXiv:2406.13597,2024.
26[100] Z.Zhong,Z.Liu,M.Tegmark,andJ.Andreas.Theclockandthepizza:Twostoriesinmech-
anisticexplanationofneuralnetworks. InThirty-seventhConferenceonNeuralInformation
ProcessingSystems,2023.
[101] S. Zinage, S. Mondal, and S. Sarkar. Dkl-kan: Scalable deep kernel learning using
kolmogorov-arnoldnetworks. arXivpreprintarXiv:2407.21176,2024.
27