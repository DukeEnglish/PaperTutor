Algorithmic Contract Design with Reinforcement Learning Agents
DavidMolinaConcha1,KyeonghyeonPark2,Hyun-RokLee3,TaesikLee2,Chi-GuhnLee1
1UniversityofToronto
2KoreaAdvancedInstituteofScienceandTechnology
3InhaUniversity
david.molina@mail.utoronto.ca,kyeonghyeon.park@kaist.ac.kr,hyunrok.lee@inha.ac.kr,taesik.lee@kaist.ac.kr,
cglee@mie.utoronto.ca
Abstract eachcontractdesignevaluationrequiressolvingtheMGus-
ing Multi-Agent Reinforcement Learning (MARL), which
Weintroduceanovelproblemsettingforalgorithmiccontract
facessignificantcomputationalchallengesduetotheexpo-
design, named the principal-MARL contract design prob-
nentialincreaseinstateandactionspacerelativetothenum-
lem. This setting extends traditional contract design to ac-
berofagents(Quetal.2020),makingitinfeasibletoeval-
countfordynamicandstochasticenvironmentsusingMarkov
uateeverydesign.Additionally,accountingforthecontract
GamesandMulti-AgentReinforcementLearning.Totackle
this problem, we propose a Multi-Objective Bayesian Op- feasibility constraints for each agent leads to a highly con-
timization (MOBO) framework named Constrained Pareto strained problem (Chen et al. 2022). Therefore, extending
MaximumEntropySearch(cPMES).Ourapproachintegrates principal-multi-agent contract design problems to MGs re-
MOBOandMARLtoexplorethehighlyconstrainedcontract mainsanopenchallenge.
designspace,identifyingpromisingincentiveandrecruitment BayesianOptimization(BO)hasbeenappliedinalgorith-
decisions. cPMES transforms the principal-MARL contract
mic reward design within MGs (Mguni et al. 2019; Shou
design problem into an unconstrained multi-objective prob-
andDi2020),treatingtheoutcomesoflearnedpoliciesfrom
lem, leveraging the probability of feasibility as part of the
MARLasexpensive-to-evaluateblack-boxfunctions.How-
objectives and ensuring promising designs predicted on the
ever, extending these methods for contract design requires
feasibilityborderareincludedintheParetofront.Byfocus-
ing the entropy prediction on designs within the Pareto set, additionalefforts tohandle theset offeasibility constraints
cPMESmitigatestheriskofthesearchstrategybeingover- for the individual agents. We propose a Multi-Objective
whelmed by entropy from constraints. We demonstrate the BO(MOBO)frameworktooptimizecontractdesignsinthe
effectivenessofcPMESthroughextensivebenchmarkstudies presence of multiple Reinforcement Learning (RL) agents.
insyntheticandsimulatedenvironments,showingitsability Ourapproach,constrainedParetoMaximumEntropySearch
tofindfeasiblecontractdesignsthatmaximizetheprincipal’s (cPMES), is composed of two levels: the design optimiza-
objectives.Additionally,weprovidetheoreticalsupportwith
tion level and the MARL level. At the first level, we intro-
asub-linearregretboundconcerningthenumberofiterations.
duce the feasibility of the design as one of the objectives
tobeoptimized(Yangetal.2021),obtainingaParetofront
Introduction of non-dominated solutions that accounts for the trade-off
betweentheprincipal’sobjectiveandtheprobabilityoffea-
Inmulti-agentcontractdesignproblems,aprincipalaimsto
sibility.Then,weprioritizedesignsbasedonthepredictive
align the incentives of individual agents with system-level
information gain of the objective function and constraints.
objectives by designing contracts. Applications can range
AttheMARLlevel,cPMESmodelstheselecteddesignasa
fromdataoffloadingusingUAVs(Chenetal.2022)togov-
partiallyobservableMulti-TypeMarkovGame(POMTMG)
ernmentcontractsforsocialwelfare(DaiLi,Immorlica,and
and solves it using MARL. The contributions of this work
Lucier2021).Mostoftheliteratureassumesthatsystemout-
are:
comesstrictlydependonagents’actions,withoutconsider-
ingtheirinteractionwiththeenvironment.Thisassumption • We extend the principal-multi-agent contract design
isdifficulttomaintaininreal-lifescenarios,whereindivid- problemtoMGsandintroducetheprincipal-MARLcon-
ualandsystem-leveloutcomesareoftensubjecttoexternal tractdesignproblem.
factors that cannot be perfectly controlled or predicted by
• WeintroducecPMES,anefficientframeworkforsolving
theprincipaloragents(WeberandXiong2006).
theprincipal-MARLcontractdesignproblem.
External stochasticity can lead contracts to not yield the
• Weestablishasub-linearregretboundwithrespecttothe
expectedoutcomesfromthemulti-agentsystem,prompting
numberofiterations,supportingtheeffectivenessofour
recentresearchtoaccountfordynamicscenarios(Xieetal.
approach.
2023; Chen et al. 2022; Zhao et al. 2023). Markov Games
(MGs)areanaturalapproachtomodelingmulti-agentsys- • WeshowcasetheperformanceofthecPMESframework
tems in stochastic environments, capturing the interactions through benchmark studies in synthetic and simulated
amongmultipleagentsindynamicenvironments.However, environmentswithrelevantmethodsinconstrainedBO.
4202
guA
91
]AM.sc[
1v68690.8042:viXraRelatedwork variable that has been considered in this type of problems.
Recent work in contract theory has explored various set- One of the main motivations of (Babaioff et al. 2012) is to
tings for the principal-multi-agent contract design problem analysetheimpactsontherecruitmentpolicyintermsofthe
(Du¨ttingetal.2023;Castiglioni,Marchesi,andGatti2023; contribution to the task given some contract. In this work,
Xieetal.2023),howevertheyassumethatthesystem’sout- weconsidertherecruitmentofagentsaspartofthedecision
comesdependsolelyontheagents’actions,andtherecruit- variableofthecontractdesignproblem.
ment of agents is not considered a decision variable. Inte-
PartiallyObservableMulti-TypeMarkovGame
grationofalgorithmiccontractdesignandRLhasbeenpri-
marilyexploredinthecontextofsingle-agentsystems.The In the presence of heterogeneous agents, the incorporation
work by (Chen et al. 2022) is only applicable to stochastic ofmultipletypestocaptureagentdiversityhasprovensuc-
energy harvesting with UAVs and does not consider MGs. cessful in the realm of MARL (Subramanian et al. 2020).
Recently,(Zhaoetal.2023)addressedprincipal-multi-agent By extending the formulation of partially observable MG
contract design by modeling the problem as a Markov De- (POMG) to multiple types of agents, we can model the
cisionProcess(MDP)fromprincipal’sperspective,treating problemasapartiallyobservableMulti-TypeMarkovGame
themultipleagentsdecisionsaspartoftheenvironment. (POMTMG),definedbythetuple:
Related problems, such as the incentive design problem,
havebeenaddressedinthedomainofMGsandMARL.In <M,N,S,O,{σ i},A,{r i},T,γ >, (2)
the work of (Mguni et al. 2019; Shou and Di 2020), a bi-
where M is the number of available types for the agents
leveloptimizationframeworkisproposed,usingBOtoopti-
and N is the set of agents. Each team n ∈ N is an M-
mizeasinglerewardparameterandMARLtosolvetheun-
dimensional vector, where each entry in the vector is the
derlyingMG.However,theirworkdoesnotconsiderincen-
number of agents in the type. S is the joint state space of
tivefeasibilityconstraints,andrecruitmentisnotpartofthe
the N agents and O ≡ O ×...×O is the joint obser-
principal’sdecisionvariables.Introducingconstrainstoex- 1 N
vation space, with O ⊆ S for all i ∈ N. The joint action
pensivetoevaluateblack-box-functionshasledtheresearch i
spaceisdefinedasA≡A ×...×A ,{σ }and{r }rep-
community to develop constrained BO methods that can 1 N i i
resent the observation and reward functions for all i ∈ N,
be categorized as probability of feasibility based methods,
whereσ : S → O andr : S×A×S → R,respectively.
step look ahead methods and surrogate-assisted constraint i i i
ThemodelfollowsanstochasticstatetransitionT,givenby
handling methods (Wang et al. 2023). Relevant methods in
T : S×A×S → [0,1].Thediscountfactorisrepresented
thisfieldsuchasExpectedImprovement(cEI),constrained
asγ.
Max-valueEntropySearch(cMES)(Perroneetal.2019)and
This concept enables us to effectively use MARL to
MACE (Zhang et al. 2022) struggle to optimize in highly
trainheterogeneousteamsofagentswithdifferentincentives
constrainedproblems,sufferingfromlackofsamplingclose
and/oractionsspaces.
to the feasibility border and search strategies driven by the
feasibilityratherthantheobjectivefunction (Bagherietal.
Problemdefinition
2017).
The principal aims to modify a baseline situation to max-
Preliminaries imize system level performance by defining a set of lin-
Principal-Multi-AgentContracts ear of contracts that considers incentives or penalties for a
set of N agents. For this problem, we consider the num-
Weconsiderthedefinitionofprincipal-multi-agentproblem
berofagentsaspartoftheprincipal’scontractdesigndeci-
provided in (Castiglioni, Marchesi, and Gatti 2023), where
sions(Babaioffetal.2012).Themodificationofthenumber
the contract design is defined by the tuple < N,Ω,A >,
of agents is done by adding N new agents to the system,
where N is the finite set of agents in the system, Ω is a fi- a
assuming baseline agents,N , can not be fired. Therefore,
b
nite set of all possible individual outcomes of each agent
N ⊆N,N ⊆N,N ∩N =∅,N ∪N =N.Then,the
and A a finite set of agents’ actions. The principal aims to b a a b a b
decision variables of the principal are the linear weight for
maximizetheirexpectedutilitybycommittingtoacontract,
incentiveorpenaltyofeachagentα = [α ,..,α ]andthe
1 N
which specifies payments from the principal to each agent
additionalagentsN .
a
contingently on the actual individual outcome achieved by
Inspired by the contract design problem setting of (Cas-
the agent. The incentives of each agent can be expressed
tiglioni,Marchesi,andGatti2023),weassumeprincipalcan
asalinearcombinationbyintroducinganindividualincen-
observetheindividualoutcomesofeachagentand,givena
tiveweightα (Du¨ttingetal.2023).Theweightsymbolizes
i contract,theybehaveaccordingto:
theproportionofreturnofprincipalthatisawardedtoeach
agent,thentheutilityofagentibecomes: 1. BestResponseStrategies(BRS):thebestresponsestrat-
egyforanagentinthecontextofMARLisapolicythat
r i =α i∗G a−lc i, (1) maximizesitsexpectedcumulativerewardgiventhepoli-
ciesoftheotheragents:
whereG istheprincipal’sobjective,aisatuplecontaining
a
agents’actions,lisabinaryindicatorfunctionthatoutputs E [r ]≥E [r ] (3)
1 if agent i exerted effort, incurring in a cost c , and 0 in
πi,π−i i π′i,π−i i
i ∀i∈{1,··· ,N},∀π ,π′ ∈Π ,π ∈Π ,
any other case. The incentive weight is not the only design i i i −i −iwhere π symbolizes the policy of all other agents ex-
−i
cepti.Thepolicyπ isconsideredbestresponsestrategy
i
if provides a greater or equal expected return than any
otherpolicyavailableforagentπ′.
i
2. Individual Rationality (IR): this principle ensures that
agentshaveanincentivetoparticipateinthecontract.It
impliesthateachagent’sutilityunderthecontractshould
beatleastasmuchastheirbaselinesituation:
E [rαj]≥E [r ] (4)
πα,Na,πα,Na j πj,π−j j
j −j
∀j ∈{1,··· ,N },
b
the incentive parameter for agent j is α and π =
j
[π ,...,π ].Notethatthepolicyoftheagentsπα,Na and
1 N j
the reward functions are affected by the contract design
decisionsfromprincipal.
Sincetheadditionalagentslackbaselineutilityforcom-
parison, we set a minimum expected return c to assess
theIRprincipleasfollows:
Figure1:cPMESarchitectureusingMOBOtoobtaincandi-
E [rαk]≥c (5)
πα,Na k datesdesignsatthecontractoptimizationlevelandthejoint
∀k ∈{1,··· ,N }, policyπforthegiven[α,N ]attheMARLlevel.
a a
From principal’s perspective, the linear contract is feasi-
ble only if all agents accept the contract. Therefore, we in-
Methodology
tegrateEquations3,4,and5asconstraintstoformulatethe
principal-MARLcontractdesignproblemasfollows: Weproposeanovelframeworktailoredtoaddressthechal-
lenging characteristic of the principal-MARL contract de-
max G(α,N )=E [Rα,Na] signproblem,namedconstrainedParetoMaximumEntropy
a πα,Na
α,Na Search(cPMES).Ourframeworkleveragesonindependent
s.t.E [rαi]≥E [rαi], surrogates for MOBO (Belakaria et al. 2020) to overcome
πα,Na,πα,Na i π′α,Na,πα,Na i
i −i i −i thelimitationsofconstrainedBOmethodsandbalancingthe
E πα,Na,πα,Na[r jαj]−E πj,π−j[r j]≥0 informationgainfromtheconstrainsandtheprobabilityof
j −j
feasibility of each contract design. The general framework
E [rαk]−c≥0
πα,Na,πα,Na k of cPMES is shown in Figure 1, at the contract optimiza-
k −k
∀i∈{1,··· ,N},∀j ∈{1,··· ,N b},∀k ∈{1,··· ,N a}, tionlevelwefindpromisingdesigns[α,N a]andthen,atthe
MARL level, we obtain the system performance G(α,N )
∀s∈S,∀π ,π′ ∈Π ,π ∈Π , a
i i i −i −i bycomputingagents’jointpolicyforthegivencontractde-
where G(α,N a) is the principal objective and Rα,Na rep- sign.
resentsthereturnfromthesystemgivenacontract[α,N ]. Atthecontractoptimizationlevel,wesetaGPpriorover
a
Thefirstsetofconstraintsare|N|BRSconstraints,ensuring the principal’s objective, |N | GP priors over the IR con-
b
thatagentsselectstrategiesbasedonthebestresponseother straintsofthebaselinesagents.Toaddressthedynamicnum-
agents strategies. The second set of constraints are |N | IR berofconstraintscomingfromtheIRprincipleoftheaddi-
b
constraints for the existing agents and the last set of con- tional agents N , we define a feasibility indicator function
a
straints are |N | IR constraints for the new agents. We as- as:
a
sume new agents have the same the minimum expected re- (cid:40)
t vu ar rn ia. bN leot Net ah .enumberofconstraintsdependsonthedecision ϕ(α,N a)=
1
0
i of thE
eπ rkα w,N isa e,π .−α,
kNa[r kαk]−c≥0,∀k ∈N a.
TheexpectedperformanceofthesystemGunderthecon-
(6)
tract [α,N ] is a consequence of the learned joint policy
a
πα,Na,πα,Na∀i ∈ N, obtained by the MARL algorithm. Theoutputofthefunctionϕ(α,N )isindependentofthe
i −i a
Duetothediversenatureofbehavioursthatcanariseindif- numberofagents,therefore,wesetaGPpriorovertheϕto
ferent contracts and the extensive computational resources account for the feasibility of the IR constraints. By using
needed for multiple training episodes, optimization prob- equilibrium based MARL algorithms, such as Mean Field
lemsinvolvingMARLaremodeledasexpensive-to-evaluate Actor-Critic(Yangetal.2018a),thepolicieslearnedbythe
black-boxfunctions(ShouandDi2020;Mgunietal.2019). agentsfollowtheprincipleofbestresponsestrategies,then,
As consequence, solving the principal-MARL contract de- it is safe to assume that all policies from the MARL al-
sign problem requires frameworks that can deal with dy- gorithm are feasible from the perspective of the BRS con-
namicnumberofconstraintsinthecontextofexpensive-to- straints (Mguni et al. 2019). The proposed modelling leads
evaluateblack-boxfunctions. toatotalofN +2surrogatesforprediction.
bWe build a MO problem, using the AF score of the Appendix??.Ourframeworkprovidesasystematicandeffi-
principal’s objective AF (α,N ), the AF score of the fea- cientmethodologyforsolvingtheprincipal-MARLcontract
G a
sibility indicator function for IR of the additional agents design problem, relying on the feasibility probabilities and
AF (α,N ) and the product of the predicted feasibility of entropy-based selection within the Pareto front to enhance
ϕ a
eachIRconstraintforthebaselineagents,leadingtothefol- thereliabilityoftheproposedcontractdesigns.
lowingmulti-objectiveproblem: AhighlydesirablepropertyinBOalgorithmsistodemon-
strate that the approach becomes increasingly efficient and
maxAF G(α,N a),AF ϕ(α,N
a),| (cid:89)Nb|
Pr(IˆR j(α,N a)≥0),
e (Sff re inct ii vv ae so ev te ar l.ti 2m 0e 1i 2n ).te Wrm es po rof vm idin eim aniz uin pg pec ru bm ou ul na dtiv fe orre tg or tae lt
α,Na
j=1 regretanddiscussthetheoreticalpropertiesofcPMES.
(7) Our goal is to establish a sub-linear upper bound for cu-
mulative regret with respect the number of iterations when
where IˆR j(α,N a) is the surrogate of the IR constraint of solvingMOproblemdefinedin7.Forthepurposeofmath-
the baseline agent j, and Pr(IˆR (α,N ) ≥ 0) is the pre- ematical derivations, we assume the AF of G(α,N ) and
j a a
dictedprobabilityoffeasibilityoftheIRconstraint. ϕ(α,N )tobetheUCB.
a
We obtain the Pareto front for the Multi-objective prob- Theorem 1. Let [α,N ]∗ be a solution in the Pareto set
a
lem 7 by using the Non-dominated Sorting Genetic Algo- P∗ and [α,N ] a solution in the Pareto set P of the MO
a t t
rithmII(NSGA-II)(Debetal.2002),awidelyusedmulti- problemobtainedatthetthiterationofcPMES.Letthetotal
objectiveevolutionaryalgorithmthathasbeenappliedsuc- regretdefinedas:
cessfully in the context of MOBO (Belakaria et al. 2020). R([α,N ]∗)=∥R ([α,N ]∗),R ([α,N ]∗),R ([α,N ]∗)∥
a 1 a 2 a 3 a
TheParetofrontcontainsasetofnon-dominatedsolutions,
(10)
representingthetrade-offsbetweentheobjectivesandshow-
casing solutions that offer the best performance across all where∥.∥isthenormofthevectors:
objectives simultaneously. Therefore, designs that yield a T (cid:88)max
high predicted principal objective but low probability of R ([α,N ]∗)= (G([α,N ]∗)−G([α,N ] )) (11)
1 a a a t
feasibility are still considered as long as they are non-
t=1
dominatedsolutions.ToselectdesignsfromtheParetofront
P,wecomputethecMESscore,prioritizingtheevaluation R ([α,N
]∗)=T (cid:88)max
(ϕ([α,N ]∗)−ϕ([α,N ] )) (12)
2 a a a t
ofdesignstomaximizetheinformationgainoftheprincipal
t=1
objectiveandtheIRconstraints:
T (cid:88)max| (cid:88)Nb|
[α′,N′]= max cMES(α,N ). (8) R ([α,N ]∗)= [Pr(IR ([α,N ]∗)≥0)
a a 3 a j a
[α,Na]∈P
t=1 j=1
The extension of MOBO frameworks to parallel BO is −Pr(IˆR ([α,N ]∗)≥0)] (13)
j a
relatively straightforward (Zhang et al. 2022), cPMES can
then,thefollowingholdswithprobability1−γ:
sample a batch of designs from the Pareto front by follow-
ingtheentropypredictioncriteria.Thedesignorbatchofde- R([α,N a]∗)≤
signsselectedaretakentotheevaluationstepattheMARL (cid:118)
level.cPMESbuildsaPOMTMGgivenbytheselectedde-
(cid:117)
(cid:117) (cid:116)kT β (γG +γϕ
+| (cid:88)Nb|
γIRj ) (14)
signvariables[α,N a]asfollows: max Tmax Tmax Tmax Tmax
j=1
<M,N a∪N b,S,O,{σ i},A,{r iαi},T,γ > (9) wherekisaconstantandbetaistheexplorationfactorβ =
2log((|α|+|N |)π2t2/6δ),withδ ∈[0,1]asaconfidence
To solve the POMTMG, we propose Multi Type Mean a
Field Q-learning (MTMFQ) algorithm (Ganapathi Subra- parameter. The terms γ TG
max
and γ Tϕ
max
are the maximum
manian et al. 2020). MTMFQ addresses the complexity of information gain about G and ϕ after T max iterations. The
MARL by extending the Mean Field Approximation pre- proofcanbefoundinAppendix??.
sented in (Yang et al. 2018b) to multiple types of hetero- In complex search spaces, such as the principal-MARL
geneousgroupsofagents. contract design, large number of evaluations may be in-
Thealgorithmconsidersdifferentgroupsofagentsinthe volved.Theorem1statesthatasthenumberofiterationsin-
environment as types and each agent models its relation to creases,theregretincurredconvergestozeroatasub-linear
each type separately, selecting the BRS against the mean rate. The sub-linear upper bound underscores the robust-
field actionof the differenttypes (Ganapathi Subramanian ness and effectiveness of the approach in complex scenar-
etal.2020).Assumingthattheonlysourceofheterogeneity ios, making it a fundamental metric for assessing the prac-
inthePOMTMGisgivenbyα,agentswiththesamevalue ticalutilityandscalabilityofthealgorithm(Shahriarietal.
ofα canbeconsideredinthesametype. 2016).
i
Oncetheagentsaretrainedandthepoliciesareevaluated,
EmpiricalResults
we update the set of priors for the selected design and re-
peat the process until the evaluation budget is consumed. Inthissection,weillustratetheapplicationofourproposed
ThepseudocodeforcPMESisshowninAlgorithm??ofthe methodforwelfaremaximizationintheClean-upproblem.Weaddresstwodistinctcasestudies:asyntheticversionof Table1:Averageregretand95%confidenceintervalforeach
the Clean-up problem, and the Sequential Social Dilemma evaluationbudget.
(SSD) version with dynamic environment. As our work
T cEI cMES MACE cPMES
showcasestheintroductionoftheprincipal-MARLcontract max
designproblem,weconductbenchmarkexperimentsbyre- 4 1891.4 ± 1448.4 ± 745.0 ± 1308.2 ±
placing the MOBO portion of our approach with relevant 1917.5 1497.5 512.0 1624.9
constrained BO methods, cEI, cMES and MACE. Further- 8 1891.4 ± 686.0 ± 578.2 ± 1075.6 ±
more,weperformparallelBOexperimentstotestthebatch 1917.5 399.6 197.1 1689.6
performance of cPMES against MACE with batch sizes 2 12 1891.4 ± 449.8 ± 578.2 ± 465.4 ±
and4. 1917.5 372.9 197.1 80.4
TheClean-upenvironment,introducedby(Hughesetal. 16 1891.4 ± 449.8 ± 578.2 ± 465.4 ±
2018), is a SSD problem for MARL, which focuses on 1917.5 372.9 197.1 80.4
achievingwelfaremaximizationinmulti-agentsystemswith 20 1891.4 ± 449.8 ± 578.2 ± 375.0 ±
social dilemma properties (Leibo et al. 2017). The envi- 1917.5 372.9 197.1 233.0
ronment is split in two areas, river area and the harvesting
area. Agents can harvest apples from the harvesting area,
applewillcontinuetorespawnaslongastheriverisclean. tionalprincipal-multi-agentproblems.
Theriverbecomesprogressivelypolluted,soitrequiresfor WefittheprincipalobjectiveG andtheIRconstraintsfor
agentstocleanittoensurethelongtermavailabilityofap- harvesters IˆR ∀j ∈ N to a GP with mean 0 and a kernel
j b
ples.Harvestingapplesyieldsapositivereward,whileclean- function composed by the product of 2 squared exponen-
ingtheriverdoesnotyieldarewardfromtheenvironment. tial (SE) kernels, which works well in discrete search with
We define an instance of the clean-up problem where multiple design variables (Duvenaud 2014). The feasibility
agentsarerestrictedtotwocategories,harvestersandclean- indicator function ϕ is fit to a GP with mean 0 and a ker-
ers.WeassumeprincipalhasasystemofN b harvestersand nel function composed of the product of 2 Mate´rn kernels
he/shewouldliketoimprovethesystemwelfarebyrecruit- andaconstantkerneltocapturethevariabilityofthebinary
ing N a cleaners. Since cleaning the river does not yield an function.
intrinsicreward,principalwantstointroduceataxαtothe
harvesters’incomeandredistributeequallyamongcleaners. SyntheticProblem
Theprincipal-MARLcontractdesignfortheclean-upprob- In this set of experiments, we focus on the performance at
lemisdefinedby: the design level, bypassing the MARL level by randomly
assigningaperformancevaluetoeachcontractdesign,cre-
max G(α,N )=E
[| (cid:88)Nb|
r −c
−| (cid:88)Na|
c ] atingaknownperformancedistribution.Furtherimplemen-
α,Na a πα,Na
j=1
j j
k=1
k tationdetailscanbefoundinAppendix??.Thissettingen-
ablesustobenchmarktheempiricalregretofeachmethod,
s.t.E πα,Na,πα,Na[(1−α)∗r j −c j]≥ by comparing the best-suggested contract design within a
j −j
E [(1−α)∗r −c ], fixed budget of evaluations against the truly optimal con-
π′α,Na,πα,Na j j tract.
j −j
α
| (cid:88)Nb| Wemeasureregretastheabsolutedifferenceoftheprin-
E [ r −c ]≥ cipal objective between the best contract design found in
π kα,Na,π −α, kNa |N a| j k each method and the optimal contract under five different
j=1
budgets: T = 4,T = 8,T = 12, T = 16,
E [
α
| (cid:88)Nb|
r −c ],
T
max
=
2m 0.ax
We
consm ida ex
r |N b|
=m 5ax
and the
prm ina cx
ipal can
π′α k,Na,π −α, kNa |N a| j k recruit up to 3 cleaners. We setup five random seeds, each
j=1 priorsethasthelowerandupperboundofcombinationsof
E [(1−α)∗r −c ]−E [r −c ]≥0 design variable and three randomly selected designs, as in
πα,Na,πα,Na j j πj,π−j j j
j −j (ShouandDi2020).WereportinTable1theaverageregret
α
| (cid:88)Nb|
obtainedforeachmethodsacrossthesetoffivedifferentpri-
E [ r −c ]−c≥0
π kα,Na,π −α, kNa |N a| j k ors.InTable2wereporttheresultsforparallelexperiments
j=1 inbatchsizesofB =2andB =4.
∀i∈{1,··· ,N},∀j ∈{1,··· ,N },∀k ∈{1,··· ,N }, The highly constrained nature of the principal-MARL
b a
∀s∈S,∀π ,π′ ∈Π ,π ∈Π , contract design problem significantly affects the search for
i i i −i −i feasiblesolutions,withmostmethodsstrugglingtoimprove
where c and c are the cost function for harvesters and theregretinthefirst8iterations.cPMESsystematicallyre-
j k
cleaners,respectively.TheprincipalobjectiveG(α,N )isto ducestheregretasthenumberofevaluationsisincreasing,
a
maximizethewelfareofthesystem,expressasthesumma- achieves the lowest regret across all methods and demon-
tion of intrinsic reward for harvesting minus agents’ costs. strating promising results in this domain. Conversely, cEI
Notethatαindirectlyaffectstheprincipalobjectivetrough fails to effectively reduce regret as its acquisition function
thelearnedpolicyoftheagentsininteractionwiththeenvi- score overfits the best feasible design in the set of priors
ronment.Suchcomplexsettingcannotbeobtainedintradi- in highly constrained problems. cMES reduces regret at aTable2:Averageregretand95%confidenceintervalofpar-
allelmethodsforeachevaluationbudget.
BatchSizeB =2 BatchSizeB =4
T
max MACE cPMES MACE cPMES
4 880.2 ±1404.8 ±940.4 ±1404.8 ±
472.2 1536.9 448.6 1536.9
8 869.2 ±772.8 ±902.2 ±574.0 ±
443.3 560.1 472.0 179.4
12 791.6 ±772.8 ±788.0 ±574.0 ±
405.3 560.1 371.6 179.4
16 773.0 ±718.8 ±755.2 ±574.0 ±
405.0 618.6 376.4 179.4
20 594.6 ±628.6 ±698.6 ±530.4 ±
591.6 429.4 430.3 203.2
slowerpacethancPMES,howeveritmaintainsahighvari- Figure 2: Principal’s objective and recommended feasible
ance due to the entropy score promoting designs that safe contracts
intermsoffeasibility.MACEstrugglestofindbetterfeasi-
bledesignsduetoheavypenalizationofpredictedinfeasible
environments, andreact toincentives orpenalties. This ap-
designs and a random selection rule that does not promote
proachallowsustostudysystemsthatmorecloselyresem-
informationgain.
blereal-worlddynamics.
Benchmark studies for parallel approaches show that
cPMESwithabatchsizeoffourachievesthelowestregret Results We conduct experiments of cEI, cMES, MACE,
across experiments. MACE performs best with B = 2, re- and cPMES, across a series of 20 evaluations and using a
ducing the regret to 594.6 in average. Designs with higher set of 10 initial priors across five different random seeds.
entropyscoretendtobeclusteredinthedistribution,provid- TheFigure2showsaheatmapofallthefeasibledesignsob-
ingsimilarinformationtotheGP.Intheotherhand,MACE’s tainedacrossallexperiments.Mostmethodsaimfordesigns
random selection provides a more diverse selection of de- with the highest number of new agents and low α to avoid
signstothebatch.TheclusteringissueincPMESbecomes violatinganyoftheIRconstraints.Thebestcontractdesign
lessrelevantasthebatchsizeincreases. found is [N = 5,α = 0.04), providing a Principal’s ob-
a
TheresultsillustratetheefficacyofeachconstrainedBO jectiveof255.53,cleanersutilityof0.28andeachharvester
method in navigating the highly constrained design space obtainsanexpectedreturnover49.96,whileinthebaseline
ofthesyntheticproblem.WeobservethatcPMESleverages situation,eachharvestergetsanutilitynohigherthan30.27.
MOBO more effectively than MACE with batch size four, Weprovideananalysisofhowthelearningofthemulti-
achieving minimum regret in 8 or fewer iterations. Further agentsystemisaffectedbythecontractdesign.InFigure7,
experimentsinMARLcantesttherobustnessofthesemeth- we present the training curves of four different contracts,
ods.ThepromisingresultsobtainedwithcPMESarefurther each varying in the number of recruited agents N and the
a
validated in a dynamic environment in the subsequent sec- harvesttaxα.Thecollectiverewardsperepisodeareplotted
tion. overaseriesof35,000episodesforeachcontractdesign.
In the scenario presented in Sub-figure 3, the system
SequentialSocialDilemma
showsa rapidincreaseincollective rewardsduringthe ini-
We present the implementation of the SSD Clean-up for tialepisodes,thenthemovingaverageofcollectiverewards
MARL.Weconsideranenvironmentwithfivebaselinehar- stabilizes around 150. The curve indicates that a adding a
vesters and we allow principal to recruit up to five clean- singlecleanerwithalowαquicklyconvergestoajointstrat-
ers. The variable α acts as a tax for the harvesters income, egywithlowvarianceacrossthetraining.TheSub-figure4
which is equally distributed among cleaners, regardless of showcases five additional agents and the same low tax, the
their performance. The general layout and implementation system exhibits a slower convergence and more variance
detailscanbefoundinAppendix??. comparedtorecruitingasingle-agent.However,itachievesa
The experimental setup involves analyzing the joint be- movingaverageofcollectiverewardsaround225.Thissug-
haviorofagentsunderdifferentcontractdesignsassuminga gestthatthepresenceofadditionalagentsincreasethecom-
minimumexpectedreturnforcleanersof0.Theoutcomefor plexity of learning best response strategies due to multiple
theprincipalishighlydependentontheenvironmentdynam- equilibria.
ics,leadingtoaricheranalysiscomparedtothestaticsetup Inthecaseofsinglecleaneragentwithahighαshownin
oftraditionalcontractdesigns.ByemployingRLagents,we Sub-figure 5, the training curve shows a more volatile pat-
canperformdynamicandadaptiveanalysesthatarenotpos- tern.Despitethefluctuations,thereisaclearupwardtrend,
sible in static settings. For instance, we can evaluate how with the moving average of collective rewards steadily in-
agentsadapttheirstrategiesovertime,respondtochanging creasing throughout the training period, reaching approxi-Figure 3: Training perfor- Figure 4: Training perfor-
mance N = 1 and α = mance N = 5 and α =
a a
0.01 0.01
Figure 8: Comparison of agents behaviours between base-
Figure 5: Training perfor- Figure 6: Training perfor-
linesetup(upperrow)andbestcontractdesign(lowerrow).
mance N = 1 and α = mance N = 5 and α =
a a
0.99 0.99
Figure7:Trainingperformanceunderdifferentcontractde- propose a MOBO-based approach to optimize the contract
signs. designandaMARLalgorithmtotraintheagentsandevalu-
atethesystemperformance.Ourmethod,cPMES,integrates
theprobabilityoffeasibilityofthedesignsasanobjectiveto
mately 100 by 35,000 episodes. For five additional agents includepromisingnon-dominatedsolutionsintheborderof
withahightax,showcasedinSub-figure6,thetrainingper- feasibilityandselectingsolutionsbasedonthepredictedin-
formanceischaracterizedbygradualgainswithhighervari- formationgain.WeshowthecapabilitiesofcPMEStonav-
ability.Thisindicatesthatcontractswithhighvalueofαre- igatehighlyconstraintsearchspace,effectivelyfindingfea-
quiremoreepisodestoconvergetoastableperformance,due siblecontractdesignstomaximizeprincipalobjective.
tothefactthatthewealthgeneratoragents,theharvester,re- Applications of cPMES are not only limited to the
ceivelessfeedbackfromtheenvironment.makingitharder principal-MARL contract design and it can extended
forlearneffectivestrategies. to highly constraint with expensive-to-evaluate black-box
Thecomparisonofthebehaviorsobtainedinthebestrec- problem in other domains. A key area for future research
ommendedcontractdesignwiththebaselinesetting,which lies in addressing the primary bottleneck for solving the
only considers harvester agents, is shown in Figure 8. Ini- principal-MARL contract design problem—training agents
tially,theenvironmenthasplentyofapplestobeharvested. using MARL algorithms. Introducing a hyper-network that
However, around time-step 50, the apple spawn rate is re- cangeneralizepoliciesoverdifferentcombinationofdesign
ducedduetotheaccumulationofwasteintheriver.Atthis variablescouldofferapromisingavenuetomitigatethisbot-
point,weobservethattheharvestingbehaviorunderthebest tleneck,enablingtheuseofgeneralizedpoliciesacrossdif-
contractdesignismoreefficient,leadingtofasterdepletion ferentcontractdesignsandreducingthecomputationalbur-
ofapplescomparedtothesamenumberofharvestersinthe denoftheoptimizationprocess.
baseline setup. By time-step 150, the baseline setup shows
completedepletionofapples,whereasthebestcontractde-
References
sign maintains apple spawning thanks to the presence of
cleaners in the river. The policy learned by the harvesters Babaioff,M.;Feldman,M.;Nisan,N.;andWinter,E.2012.
under the cPMES contract design is superior, as the agents Combinatorialagency.JournalofEconomicTheory,147(3):
haveexperiencedmoreinstancesofapplecollectionthanthe 999–1034.
baselineharvesters,duetotheadditionalfivecleaners.
Bagheri,S.;Konen,W.;Allmendinger,R.;Branke,J.;Deb,
K.; Fieldsend, J.; Quagliarella, D.; and Sindhya, K. 2017.
Conclusions
Constrainthandlinginefficientglobaloptimization. InPro-
Wepresentanovelproblemsettingforalgorithmiccontract ceedings of the genetic and evolutionary computation con-
design, the principal-MARL contract design problem. We ference,673–680.Belakaria, S.; Deshwal, A.; Jayakodi, N. K.; and Doppa, Shahriari, B.; Swersky, K.; Wang, Z.; Adams, R.; and Fre-
J.R.2020.Uncertainty-AwareSearchFrameworkforMulti- itas,N.2016. Takingthehumanoutoftheloop:areviewof
ObjectiveBayesianOptimization. ProceedingsoftheAAAI bayesian optimization. Proceedings of the Ieee, 104: 148–
ConferenceonArtificialIntelligence,34(06):10044–10052. 175.
Castiglioni, M.; Marchesi, A.; and Gatti, N. 2023. Multi- Shou, Z.; and Di, X. 2020. Reward design for driver repo-
agent contract design: How to commission multiple agents sitioning using multi-agent reinforcement learning. Trans-
withindividualoutcomes. InProceedingsofthe24thACM portation research part C: emerging technologies, 119:
ConferenceonEconomicsandComputation,412–448. 102738.
Chen, C.; Gong, S.; Zhang, W.; Zheng, Y.; and Kiat, Y. C. Srinivas, N.; Krause, A.; Kakade, S.; and Seeger, M. 2012.
2022. Deep Reinforcement Learning based Contract In- Information-theoreticregretboundsforgaussianprocessop-
centiveforUAVsandEnergyHarvestAssistedComputing. timizationinthebanditsetting. IeeeTransactionsonInfor-
In GLOBECOM 2022-2022 IEEE Global Communications mationTheory,58:3250–3265.
Conference,2224–2229.IEEE. Subramanian,S.G.;Poupart,P.;Taylor,M.E.;andHegde,
N. 2020. Multi type mean field reinforcement learning.
DaiLi,W.;Immorlica,N.;andLucier,B.2021.Contractde-
arXivpreprintarXiv:2002.02513.
signforafforestationprograms.InInternationalConference
onWebandInternetEconomics,113–130.Springer. Wang, X.; Jin, Y.; Schmitt, S.; and Olhofer, M. 2023. Re-
cent advances in Bayesian optimization. ACM Computing
Deb, K.; Pratap, A.; Agarwal, S.; and Meyarivan, T. 2002.
Surveys,55(13s):1–36.
A fast and elitist multiobjective genetic algorithm: NSGA-
II. IEEE Transactions on Evolutionary Computation, 6(2): Weber,T.A.;andXiong,H.2006. Efficientcontractdesign
182–197. in multi-principal multi-agent supply chains. SSRN Elec-
tronicJournal.
Du¨tting,P.;Ezra,T.;Feldman,M.;andKesselheim,T.2023.
Xie, Y.; Ding, C.; Li, Y.; and Wang, K. 2023. Optimal in-
Multi-agent contracts. In Proceedings of the 55th Annual
centive contract in continuous time with different behavior
ACMSymposiumonTheoryofComputing,1311–1324.
relationships between agents. International Review of Fi-
Duvenaud, D. 2014. Automatic model construction with nancialAnalysis,86:102521.
Gaussianprocesses. Ph.D.thesis,UniversityofCambridge.
Yang, W.; Chen, L.; Li, Y.; and Zhang, J. 2021. A con-
GanapathiSubramanian,S.;Poupart,P.;Taylor,M.E.;and strained multi/many-objective particle swarm optimization
Hegde, N. 2020. Multi Type Mean Field Reinforcement algorithmwithatwo-levelbalancescheme. IEEEAccess,9:
Learning. InProceedingsofthe19thInternationalConfer- 122509–122531.
enceonAutonomousAgentsandMultiAgentSystems,411–
Yang,Y.;Luo,R.;Li,M.;Zhou,M.;Zhang,W.;andWang,
419.
J. 2018a. Mean field multi-agent reinforcement learning.
Hughes, E.; Leibo, J. Z.; Phillips, M.; Tuyls, K.; Duen˜ez- In International Conference on Machine Learning, 5571–
Guzman,E.;Castan˜eda,A.G.;Dunning,I.;Zhu,T.;McKee, 5580.PMLR.
K.; Koster, R.; et al. 2018. Inequity aversion improves co- Yang,Y.;Luo,R.;Li,M.;Zhou,M.;Zhang,W.;andWang,
operationinintertemporalsocialdilemmas. InProceedings J. 2018b. Mean field multi-agent reinforcement learning.
ofthe32ndInternationalConferenceonNeuralInformation In International Conference on Machine Learning, 5571–
ProcessingSystems,3330–3340. 5580.PMLR.
Leibo, J. Z.; Zambaldi, V.; Lanctot, M.; Marecki, J.; and Zhang,S.;Yang,F.;Yan,C.;Zhou,D.;andZeng,X.2022.
Graepel, T. 2017. Multi-agent Reinforcement Learning in AnEfficientBatch-ConstrainedBayesianOptimizationAp-
Sequential Social Dilemmas. In Proceedings of the 16th proachforAnalogCircuitSynthesisviaMultiobjectiveAc-
ConferenceonAutonomousAgentsandMultiAgentSystems, quisitionEnsemble. IEEETransactionsonComputer-Aided
464–473. DesignofIntegratedCircuitsandSystems,41(1):1–14.
Mguni, D.; Jennings, J.; Sison, E.; Valcarcel Macua, S.; Zhao,N.;Pei,Y.;Liang,Y.-C.;andNiyato,D.2023.ADeep
Ceppi, S.; and Munoz de Cote, E. 2019. Coordinating the Reinforcement Learning-Based Contract Incentive Mecha-
Crowd: Inducing Desirable Equilibria in Non-Cooperative nismforMobileCrowdsourcingNetworks. IEEETransac-
Systems. In Proceedings of the 18th International Confer- tionsonVehicularTechnology.
enceonAutonomousAgentsandMultiAgentSystems,386–
394.
Perrone, V.; Shcherbatyi, I.; Jenatton, R.; Archambeau, C.;
and Seeger, M. 2019. Constrained Bayesian optimization
withmax-valueentropysearch. InProceedingsofthe33rd
ConferenceonNeuralInformationProcessingSystems.
Qu, G.; Lin, Y.; Wierman, A.; and Li, N. 2020. Scalable
multi-agent reinforcement learning for networked systems
withaveragereward. AdvancesinNeuralInformationPro-
cessingSystems,33:2074–2086.