Parseval Convolution Operators
and Neural Networks
MichaelUnser[0000−0003−1248−2513] and
StanislasDucotterd[0009−0006−2047−5179]
Abstract We first establish a kernel theorem that characterizes all linear shift-
invariant (LSI) operators acting on discrete multicomponent signals. This result
naturally leads to the identification of the Parseval convolution operators as the
classofenergy-preservingfilterbanks.Wethenpresentaconstructiveapproachfor
thedesign/specificationofsuchfilterbanksviathechainingofelementaryParseval
modules, each of which being parameterized by an orthogonal matrix or a 1-tight
frame.OuranalysisiscomplementedwithexplicitformulasfortheLipschitzconstant
of all the components of a convolutional neural network (CNN), which gives us a
handle on their stability. Finally, we demonstrate the usage of those tools with the
design of a CNN-based algorithm for the iterative reconstruction of biomedical
images. Our algorithm falls within the plug-and-play framework for the resolution
ofinverseproblems.Ityieldsbetter-qualityresultsthanthesparsity-basedmethods
used in compressed sensing, while offering essentially the same convergence and
robustnessguarantees.
1 Introduction
The goal of this chapter is twofold. The first objective is to characterize a special
typeofconvolutionaloperatorsthatarerobustandinherentlystablebecauseoftheir
Parseval property. The second objective is to showcase the use of these operators
inthedesignofthrustworthyneural-network-basedalgorithmsforsignalandimage
processing. Our approach is deductive, in that it relies on the higher-level tools of
functional analysis to identify the relevant operators based on their fundamental
MichaelUnser
EPFL,Lausanne,Switzerland,e-mail:michael.unser@epfl.ch
StanislasDucotterd
EPFL,Lausanne,Switzerland,e-mail:stanislas.Ducotterd@epfl.ch
1
4202
guA
91
]PS.ssee[
1v18990.8042:viXra2 MichaelUnserandStanislasDucotterd
properties; namely, linearity, shift-invariance (LSI), and energy conservation (Par-
seval).
ThestudyofLSIoperators(a.k.a.filters)reliesheavilyontheFouriertransform
and is a central topic in linear-systems theory and signal processing [25, 39, 54].
Hence, the first step of our investigation is to extend the classic framework to
accommodate the kind of processing performed in convolutional neural networks
(CNNs), where the convolutional layers have multichanel inputs and outputs. We
do so by adopting an operator-based formalism with appropriate Hilbert spaces,
whichthenalsomakesthedescriptionofCNNsmathematicallyprecise.Asonemay
expect,thecorrespondingLSIoperatorsarecharacterizedbytheirimpulseresponse
or,equivalently,bytheirfrequencyresponse,theextensiontotheclassicsettingof
signalprocessingbeingthattheseentitiesnowbothhappentobematrix-valued(see
Theorem2).
OurfocusonParsevaloperatorsismotivatedbythedesiretocontrolthestabilityof
thecomponentsofCNNs,whichcanbequantifiedmathematicallybytheirLipschitz
constant (see Section 2.3). Indeed, it is known that the stability of conventional
deep neural networks degrades (almost) exponentially with their depth [59]. This
lack of stability partly explains why CNNs can occasionally hallucinate, which is
unacceptableforcriticalapplicationssuchas,forinstance,diagnosticimaging.Our
proposedremedyistoconstraintheLipschitzconstantofeachlayer,withthe“ultra-
stable” configuration being the one where each component is non-expansive; i.e.,
withaLipschitzconstantnogreaterthan1.Parsevaloperatorsareexemplarinthis
respectsincetheypreserveenergy,which,ineffect,turnsthe(worst-case)Lipschitz
boundintoanequality.Additionalfeaturesthatmotivatetheirusageareasfollows:
1. Parsevalconvolutionoperatorshavearemarkablysimpletheoreticaldescription,
whichisgiveninProposition3;
2. theyadmitconvenientparametricrepresentations(seeSection4)thataredirectly
amenabletoanoptimizationinstandardcomputationalframeworksformachine
learning,suchasPyTorch.
However,onemustacknowledgethatthereisnofreelunch.Anyattempttostabilize
aneuralnetworkbyconstrainingtheLipschitzconstantofeachlayerwillnecessarily
reduceitsexpressivity,asdocumentedin[23,18].Thegoodnewsisthatthiseffect
islesspronouncedwhenthelinearlayershavetheParsevalproperty,asconfirmedin
ourexperiments(Parsevalvs.spectralnormalization).Infact,weshalldemonstrate
that the use of Parseval CNNs (as substitute for the classic proximity operator of
convex optimization) results in a substantial improvement in the quality of image
reconstruction over that of the traditional sparsity-based methods of compressed
sensing,whileitoffersessentiallythesametheoreticalguarantees(consistencyand
stability).ParsevalConvolutionOperatorsandNeuralNetworks 3
1.1 RelatedWorksandConcepts
Conceptually, Parseval operators are the infinite-dimensional generalization of or-
thogonalmatrices(one-to-onescenario)and,moregenerally,of1-tightframes(one-
to-many scenario) [10]. The latter involve rectangular matrices A ∈ R𝑀×𝑁 with
𝑀 ≥ 𝑁 andthepropertythatATA = I𝑁 (identity).WhentheoperatorisLSI,then
itisdiagonalizedbytheFouriertransform—apropertythatcanbeexploitedforthe
designofParsevalfilterbanks.
Thespecificationoffiltersfortheorthogonalwavelettransform[16,33,36]isa
specialinstanceoftheone-to-onescenario.Infact,thereisacomprehensivetheory
for the design of perfect-reconstruction filterbanks [42, 53], the orthogonal ones
beingsometimesreferredtoaslosslesssystems[51].Itincludesgeneralfactoriza-
tion results for paraunitary matrices associated with finite impulse response (FIR)
filterbanksofagivenMcMillandegree[51,Theorem14.4.1,p.736]orofagiven
size[48],withthecaveatthattheseonlyholdintheone-dimensionalsetting.There
arealsoadaptationsofthoseresultsforlinear-phasefilters[41,45,47].
The one-to-many scenario (wavelet frames) caught the interest of researchers
in the late ’90s, motivated by an early application in texture analysis [49] that
involved a computational architecture that is a “handcrafted” form of CNN. Such
redundant wavelet designs are less constrained than the orthogonal ones. They go
under the name of oversampled filterbanks [15], oversampled wavelet transforms
[6],undecimatedwavelettransform[32],lappedtransforms[9],or,moregenerally,
tight(wavelet)frames[1,11,27,28,29].
TheuseofParsevaloperatorsinthecontextofneuralnetworksismorerecent.The
newtwistbroughtforthbymachinelearningisthatthefilterscannowbelearnedto
providethebestperformanceforagivencomputationaltask,whichisfeasibleunder
theavailabilityofsufficienttrainingdataandcomputationalpower.Thefirstattempts
toorthogonalizethelinearlayersofaneuralnetworkweremotivatedbythedesire
to avoid vanishing gradients and to improve robustness against adversarial attacks
[2,13,20,56].Severalresearchteams[22,30,43]thenproposedsolutionsforthe
training of orthogonal convolution layers that are inspired by the one-dimensional
factorizationtheoremsuncoveredduringthedevelopmentofthewavelettransform.
TherearealsoapproachesthatoperatedirectlyintheFourierdomain[46].
1.2 RoadMap
Thischapterisorganizedasfollows.
WestartwithapresentationofbackgroundmaterialinSection2.First,wesetthe
notation and introduce the Hilbert spaces for the representation of 𝑑-dimensional
vector-valued signals, such as ℓ𝑁(Z𝑑), whose elements are 𝑁-component discrete
2
signals or images. We then move on to the discrete Fourier transform in Section
2.2. This is complemented with a discussion of fundamental continuity/stability4 MichaelUnserandStanislasDucotterd
properties of operators in the general context of Banach/Hilbert spaces in Section
2.3.
InSection3,wefocusonthediscreteLSIsettingandidentifythecompletefamily
ofcontinuousLSIoperatorsT : ℓ𝑁(Z𝑑) → ℓ𝑀(Z𝑑) (Theorem2),includingthe
LSI 2 2
determination of their Lipschitz constant. These operators are multichannel filters
with𝑁-channelinputsand 𝑀-channeloutputs.Theyareuniquelyspecifiedbytheir
matrix-valued impulse response or, equivalently, by their matrix-valued frequency
response.AnimportantsubclassaretheLSI-Parsevaloperators;theseareidentified
inProposition3asconvolutionoperatorswithaparaunitaryfrequencyresponse.
InSection4,wedevelopaconstructiveapproachforthedesign/specificationof
Parseval filterbanks. The leading idea is to generate higher-complexity filterbanks
throughthechainingofelementaryParsevalmodules,eachbeingparameterizedby
aunitarymatrixor,eventually,a1-tightframe(seeTable1).
Section 5 is devoted to the application of our framework to the problem of
biomedical image reconstruction. Our approach revolves around the design of a
robust1-LipschitzCNNforimagedenoisingthatmimicsthearchitectureofDnCNN
[58]—a very popular image denoiser. The important twist is that, unlike DnCNN,
theconvolutionlayersofournetworkareconstrainedtobeParseval,whichmakes
our denoiser compatible with the powerful plug-and-play (PnP) paradigm for the
resolutionoflinearinverseproblems[8,26,44,52].Wefirstprovidemathematical
supportforthisprocedureintheformofconvergenceguaranteesandstabilitybounds.
WethendemonstratethefeasibilityoftheapproachforMRIreconstructionandreport
experimentalresultswhereitsignificantlyoutperformsthestandardtechnique(total-
variation-regularizedreconstruction)usedincompressedsensing.
2 MathematicalBackground
2.1 Notation
Weuseboldfaceloweranduppercaseletterstodenotevectorsandmatrices,respec-
tively(e.g.,u ∈ R𝑁 andU ∈ C𝑀×𝑁 ).Specificinstancesaree𝑛 (the𝑛thelementof
thecanonicalbasisinR𝑁 )andI𝑁 = [e 1...e𝑁] (theunitmatrixofsize𝑁).
Adiscretemultidimensionalscalarsignal(e.g.,theinputoroutputofaconvolu-
tionalneuralnetwork)isasequence(𝑥[𝒌]) 𝒌∈Z𝑑 ofrealnumbersthat,dependingon
thecontext,willbedenotedas𝑥 ∈ ℓ (Z𝑑) (i.e.,asamemberofaHilbertspace),or
2
𝑥[·],where“·”isaplaceholderfortheindexingvariable.Ouruseofsquarebrackets
follows the convention of signal processing, as reminder of the discrete nature of
theobjects.Avector-valuedsignalx[·] = (𝑥 1[·],...,𝑥 𝑁[·])isanindexedsequence
of vectors x[𝒌] = (𝑥 1[𝒌],...,𝑥 𝑁[𝒌]) ∈ R𝑁 with 𝒌 ranging over Z𝑑 . Likewise,
(cid:2) (cid:3)
X[·] = x 1[·] ··· x𝑀[·] is a matrix-valued signal or sequence. An alternative
representationofsuchsequencesisParsevalConvolutionOperatorsandNeuralNetworks 5
∑︁
X[·] = X[𝒏]𝛿[·−𝒏] (1)
𝒏∈Z𝑑
where𝛿[·−𝒏] denotesthe(scalar)Kroneckerimpulseshiftedby 𝒏,with𝛿[0] = 1
and𝛿[𝒌] =0for 𝒌 ∈Z𝑑\{0}.
In the same spirit, we use the notation 𝑓(·), f(·), F(·) to designate objects that
arerespectivelyscalar,vector-valued,andmatrix-valuedfunctionsofacontinuously
varyingindexsuchas𝒙 ∈R𝑑 or𝝎 ∈T𝑑 = [−𝜋,+𝜋]𝑑 (thefrequencyvariable).
Thesymbol∨ denotestheflippingoperatorwithx∨[𝒌] =△ x[−𝒌] forall 𝒌 ∈ Z𝑑 ,
while UH ∈ C𝑁×𝑀 is the Hermitian transpose of the complex matrix U ∈ C𝑀×𝑁
△
with [UH]𝑛,𝑚 = [U]𝑚,𝑛 (transposewithcomplexconjugation).
OurprimaryHilbertspaceofvector-valuedsignalsisℓ𝑁(Z𝑑) =ℓ ({1,...,𝑁}×
2 2
Z𝑑) =ℓ (Z𝑑)×···×ℓ (Z𝑑) (cid:0)𝑁 occurrencesofℓ (Z𝑑)(cid:1) ,whichisthedirect-product
2 2 2
extensionofℓ (Z𝑑).Specifically,
2
(cid:110) (cid:111)
ℓ 2𝑁(Z𝑑) = x[·] :Z𝑑 →R𝑁 s.t.∥x[·]∥ ℓ𝑁(Z𝑑) <∞
2
with
(cid:32) 𝑁 (cid:33)1/2
△ ∑︁ ∑︁
∥x[·]∥ ℓ𝑁(Z𝑑) = |𝑥 𝑛[𝒌]|2 . (2)
2
𝑛=1𝒌∈Z𝑑
Byinvokingadensityargument,wecaninterchangetheorderofsummationin(2)
sothat
∥x[·]∥ ℓ 2𝑁(Z𝑑) =(cid:13) (cid:13)(∥𝑥 1∥ ℓ 2(Z𝑑),...,∥𝑥 𝑁∥ ℓ 2(Z𝑑))(cid:13) (cid:13) 2 =(cid:13) (cid:13)∥x[·]∥ 2(cid:13) (cid:13) ℓ 2(Z𝑑),
where
(cid:32) 𝑁 (cid:33)1/2
△ ∑︁
∥u∥
2
= |𝑢 𝑛|2
𝑛=1
istheconventionalEuclideannormofthevectoru= (𝑢 1,...,𝑢 𝑁) ∈C𝑁 .
2.2 TheDiscreteFourierTransformandPlancherel’sIsomorphism
ThediscreteFouriertransformofasignal𝑥[·] ∈ℓ (Z𝑑) ⊂ ℓ (Z𝑑)isdefinedas
1 2
𝑥ˆ(𝝎) =△ F {𝑥[·]}(𝝎) = ∑︁ 𝑥[𝒌]e−j⟨𝝎,𝒌⟩, 𝝎 ∈R𝑑. (3)
d
𝒌∈Z𝑑
The function 𝑥ˆ : R𝑑 → C is continuous, bounded and 2𝜋-periodic. It is therefore
entirely specified by its main period T𝑑 = [−𝜋,𝜋]𝑑 . The original signal can be
recoveredbyinverseFouriertransformationas6 MichaelUnserandStanislasDucotterd
∫ d𝝎
𝑥[𝒌] = F−1{𝑎ˆ}[𝒌] = 𝑥ˆ(𝝎)ej⟨𝝎,𝒌⟩ , 𝒌 ∈Z𝑑. (4)
d
T𝑑
(2𝜋)𝑑
Byinterpretingtheinfinitesumin(3)asanappropriatelimit,onethenextendsthe
definitionoftheFouriertransformtoencompassallsquare-summablesignals.This
yieldstheextendedoperator F : ℓ (Z𝑑) → 𝐿 (T𝑑),where 𝐿 (T𝑑) isthespaceof
d 2 2 2
measurable complex Hermitian-symmetric and square-integrable functions on
T𝑑
.
ThelatterisaHilbertspaceequippedwiththeHermitianinnerproduct
△ ∫ d𝝎
⟨𝑥ˆ,𝑦ˆ⟩ 𝐿 2(T𝑑) = T𝑑𝑥ˆ(𝝎)𝑦ˆ(𝝎) (2𝜋)𝑑. (5)
TheFouriertransform F : ℓ (Z𝑑) → 𝐿 (T𝑑) isabĳectiveisometry(unitarymap
d 2 2
between two Hilbert spaces) with F−1 : 𝐿 (T𝑑) → ℓ (Z𝑑), where the inverse
d 2 2
transform is still specified by (4) with an extended Lebesgue interpretation of the
integral.Indeed,byinvokingtheCauchy-Schwartzinequality,wegetthat
∫ T𝑑(cid:12)
(cid:12)
(cid:12)𝑥ˆ(𝝎)ej⟨𝝎,𝒌⟩(cid:12)
(cid:12)
(cid:12)
(2d 𝜋𝝎
)𝑑
=∫
T𝑑
|𝑥ˆ(𝝎)|×1
(2d 𝜋𝝎
)𝑑
≤ ∥𝑥ˆ∥𝐿
2
(cid:18)∫
T𝑑1
(2d 𝜋𝝎 )𝑑(cid:19)1 2
= ∥𝑥ˆ∥𝐿 2,
whichensuresthewell-posednessedof(4)forall𝑥ˆ ∈ 𝐿 (T𝑑).
2
Thecornerstoneoftheℓ theoryoftheFouriertransformisthePlancherel-Parseval
2
identity
∀𝑥,𝑦 ∈ℓ 2(Z𝑑) : ⟨𝑥,𝑦⟩ ℓ (Z𝑑) =△ ∑︁ 𝑥[𝒌]𝑦[𝒌] = ⟨𝑥ˆ,𝑦ˆ⟩ 𝐿 (T𝑑). (6)
2 2
𝒌∈Z𝑑
ItensuresthattheinnerproductispreservedintheFourierdomain.
The vector-valued extension of these relations is immediate if one defines the
Fouriertransformofavector-valuedsignalx[·] ∈ℓ𝑁(Z𝑑)as
2
∀𝝎 ∈R𝑑 : F d{x[·]}(𝝎) = F
d
 

𝑥 1
. .
.[·] 
 

(𝝎) =△

 

𝑥ˆ 1(
. .
.𝝎) 
 

= (cid:98)x(𝝎) (7)
 𝑥 𝑁[·]   𝑥ˆ𝑁(𝝎)

   
anditsinverseas
F d−1{ (cid:98)x} = F
d−1



𝑥ˆ
.
.
.1 



=




F d−1
.
.
.{𝑥ˆ 1} 



=




𝑥 1
.
.
.[·] 

 . (8)
 𝑥ˆ𝑁   F−1{𝑥ˆ𝑁}

 𝑥 𝑁[·]

   d   
Thecorrespondingvector-valuedversionofPlancherel’sidentityreadsParsevalConvolutionOperatorsandNeuralNetworks 7
𝑁
∀x,y∈ℓ 2𝑁(Z𝑑) : ⟨x,y⟩ ℓ𝑁(Z𝑑) =△ ∑︁ ∑︁ 𝑥 𝑛[𝒌]𝑦 𝑛[𝒌]
2
𝑛=1𝒌∈Z𝑑
△ ∑︁𝑁 ∫ d𝝎
= ⟨ (cid:98)x, (cid:98)y⟩ 𝐿 2𝑁(T𝑑) =
𝑛=1
T𝑑𝑥ˆ𝑛(𝝎)𝑦ˆ𝑛(𝝎) (2𝜋)𝑑.
(9)
ThePlancherel-FourierisomorphismisthenexpressedasF :ℓ𝑁(Z𝑑) → 𝐿𝑁(T𝑑)
d 2 2
and F−1 : 𝐿𝑁(T𝑑) → ℓ𝑁(Z𝑑) where 𝐿𝑁(T𝑑) is the Hilbert space of complex-
d 2 2 2
valued Hermitian-symmetric functions associated with the inner product (9) for
(2𝜋)-periodicvector-valuedfunctions.
2.3 1-LipandParsevalOperators
The transformations that occur in a neural network can be described through the
actionofsomeoperatorsTthatmapanymember𝑥ofavectorspaceX(forinstance,
a specific input of the network or of one of its layers) into some element 𝑦 =
T{𝑥} of another vector space Y𝑖 (e.g., the output of the network or any of its
intermediate layers). These operators T: X → Y can be linear (as in the case of
a convolution layer) or, more generally, nonlinear. A minimal requirement is that
the T be continuous, which is a mathematical precondition tied to the underlying
topologies.
Definition1 Consider the (possibly nonlinear) mapping T: X → Y, where X =
(X,∥ · ∥ ) andY = (Y,∥ · ∥ ) aretwocompletenormedspaces(e.g.,Banachor
X Y
Hilbertspaces).Then,Tcanexhibitthefollowingformsofcontinuity.
1. Continuityat𝑥 ∈ X:Forany𝜖 > 0,thereexistssome 𝜇 > 0suchthat,forany
0
𝑥 ∈Xwith∥𝑥−𝑥 ∥ < 𝜇,itholdsthat∥T{𝑥}−T{𝑥 }∥ < 𝜖.
0 X 0 Y
2. UniformcontinuityonX:Forany𝜖 > 0,thereexistssome 𝜇 > 0suchthat,for
any𝑥,𝑥 ∈Xwith∥𝑥−𝑥 ∥ < 𝜇,itholdsthat∥T{𝑥}−T{𝑥 }∥ < 𝜖.
0 0 X 0 Y
3. Lipschitzcontinuity:Thereexistssomeconstant𝐿 >0suchthat
∀𝑥,𝑥 ∈X : ∥T{𝑥}−T{𝑥 }∥ ≤ 𝐿∥𝑥−𝑥 ∥ . (10)
0 0 Y 0 X
Thethirdform(Lipschitz)isobviouslyalsothestrongestwith3 ⇒ 2 ⇒ 1.The
smallest𝐿forwhich(10)holdsiscalledtheLipschitzconstantofTwith
∥T{𝑥}−T{𝑥 }∥
Lip(T) = sup 0 Y. (11)
∥𝑥−𝑥 ∥
∀𝑥,𝑥 0∈X, 𝑥≠𝑥 0 0 X
Definition2 AnoperatorT:X →Y issaidtobeof1-LiptypeifLip(T) =1.
The1-Lipoperatorsareofspecialinteresttousbecausetheyareinherentlystable:
asmallperturbationoftheirinputcanonlyinduceasmalldeviationoftheiroutput.8 MichaelUnserandStanislasDucotterd
Moreover, they can be chained at will without any degradation in overall stability
becauseLip(T ◦T ) ≤ Lip(T )Lip(T ) =1.
2 1 2 1
For linear operators, the graded forms of continuity in Definition 1 can all be
related to one overarching simplifying concept: the boundedness of the operator.
The two key ideas there are: (i) a linear operator is (locally) continuous at any
𝑥 ∈ X ifandonlyifitiscontinuousat0;and,(ii)itisuniformlycontinuousifand
0
onlyifitisbounded[12,Theorem2.9-2,p.84].Finally,thereisoneveryattractive
form of 1-Lip linear operators for which (10) holds as an equality, rather than a
“worst-case”inequality.Tomakethisexplicit,wenowrecallsomebasicproperties
of linear operators acting on Hilbert spaces and identify the subclass of Parseval
operators,whicharenorm-aswellasinner-product(angle)preserving.
Definition3 LetXandYbetwoHilbertspaces.ThemostbasicHilbertianproper-
tiesofalinearoperatorT:X →Y areasfollows.
1. Boundedness(continuity):Thereexistsaconstant𝐵 <∞suchthat
∀𝑥 ∈X : ∥T{𝑥}∥ ≤ 𝐵∥𝑥∥ , (12)
Y X
withthesmallest𝐵in(12)beingthenormoftheoperatordenotedby∥T∥.
2. Boundednessfrombelow(injectivity):Thereexistsaconstant0< 𝐴suchthat
∀𝑥 ∈X : 𝐴 ∥𝑥∥ ≤ ∥T{𝑥}∥ . (13)
X Y
3. Isometry:Tisnorm-preserving(orParseval),meaningthatboth(12)and(13)
holdwith 𝐴= 𝐵=1.
ToidentifythecriticalboundsinDefinition3,weobservethat,forany𝑥 ∈X\{0},
∥T{𝑥}∥ 𝑥
𝐴 ≤ Y = ∥T{𝑧}∥ ≤ 𝐵 with𝑧 = . (14)
∥𝑥∥ Y ∥𝑥∥
X X
ThisholdsbyvirtueofthelinearityofTandthehomogeneitypropertyofthenorm.
Inparticular,thisallowsustospecifytheinducednormoftheoperatoras
∥T∥ =△ sup
∥T{𝑥}∥
Y = sup ∥T{𝑧}∥ . (15)
∥𝑥∥ Y
𝑥∈X\{0} X 𝑧∈X: ∥𝑧∥X=1
Notethat(15)canbeobtainedbyrestricting(11)to𝑥 = 0,whichthenalsoyields
0
∥T∥ =Lip(T) duetothelinearityofT.Theisometryproperty(Item3)isbyfarthe
mostconstraining,asitimpliesthetwootherswith ∥T∥ = 1.Asitturnsout,ithas
otherremarkableconsequences,whichyieldsomealternativecharacterization(s).
Proposition1(Properties of Parseval operators) Let X and Y be two Hilbert
spaces. Then, the linear operator T: X → Y is a Parseval operator if any of the
followingequivalentconditionsholds.
1. Isometry
∀𝑥 ∈X : ∥𝑥∥ = ∥T{𝑥}∥ . (16)
X YParsevalConvolutionOperatorsandNeuralNetworks 9
2. Preservationofinnerproducts
∀𝑥 ,𝑥 ∈X, ⟨T{𝑥 },T{𝑥 }⟩ = ⟨𝑥 ,𝑥 ⟩ . (17)
1 2 1 2 Y 1 2 X
3. Pseudo-inversionviatheadjointsothatT∗◦T=Id:X →Y →X,wherethe
HermitianadjointT∗ :Y →Xistheuniquelinearoperatorsuchthat
∀(𝑥,𝑦) ∈X×Y : ⟨T{𝑥},𝑦⟩ = ⟨𝑥,T∗{𝑦}⟩ . (18)
Y X
Proof.
(i)1⇔2:Fromthebasicpropertiesof(real-valued)innerproductsandthelinearity
ofT,wehavethat
∥𝑥 −𝑥 ∥2 = ⟨𝑥 −𝑥 ,𝑥 −𝑥 ⟩ = ∥𝑥 ∥2 −2⟨𝑥 ,𝑥 ⟩ +∥𝑥 ∥2
2 1 X 2 1 2 1 X 2 X 1 2 X 1 X
∥T{𝑥 −𝑥 }∥2 = ∥T{𝑥 }−T{𝑥 }∥2 = ∥T{𝑥 }∥2 −2⟨T{𝑥 },T{𝑥 }⟩ +∥T{𝑥 }∥2 .
2 1 Y 2 1 Y 2 Y 1 2 Y 1 Y
Byequatingthesetwoexpressions,wereadilydeducethat(16)implies(17).Like-
wise,intheextendedcomplexsetting,wefindthatRe(⟨𝑥 ,𝑥 ⟩ ) =Re(⟨T{𝑥 },T{𝑥 }⟩ ),
1 2 X 1 2 Y
whichultimatelyalsoyields(17).Conversely,bysetting𝑥 =𝑥 in(17),wedirectly
1 2
get(16).
(ii)2⇔3:TheexistenceandunicityoftheadjointoperatorT∗in(18)isastandard
resultinthetheoryoflinearoperatorsonHilbert/Banachspaces.Bysetting𝑥 =𝑥 ,
1
𝑦 =T{𝑥 },andapplying(18),werewrite(17)as
2
∀𝑥 ,𝑥 ∈X : ⟨T{𝑥 },T{𝑥 }⟩ = ⟨T∗T{𝑥 },𝑥 ⟩ = ⟨𝑥 ,𝑥 ⟩ . (19)
1 2 1 2 Y 1 2 X 1 2 X
SincetheinnerproductseparatesallpointsintheHilbertspace(Hausdorffproperty),
the right-hand side of (19) is equivalent to T∗T{𝑥 } = 𝑥 for all 𝑥 ∈ X, which
1 1 1
translatesintoT∗T=T∗◦T=IdonX. ⊓⊔
The classic example of a Parseval operator is the discrete Fourier transform
F𝑑 : ℓ 2(Z𝑑) → 𝐿 2(T𝑑) withtheHilbertiantopologyspecifiedinSection2.2.The
fundamentalpropertythereisthattheHilbertspacesX = ℓ (Z𝑑) andY = 𝐿 (T𝑑)
2 2
areisomorphicwithF 𝑑−1 = F 𝑑∗beingatrueinverseofF𝑑 (bĳection),meaningthat,
inadditiontoItem3inProposition1,wealsohavethatF𝑑◦F 𝑑∗ =IdonY = 𝐿 2(T𝑑)
(right-inverseproperty).
By contrast, the Parseval convolution operators investigated in this paper will
typically not be invertible from the right, the reason being that the effective range
spaceY(cid:101)=T(X)isonlya(closed)subspaceofY.
An important observation is that, in addition to linearity and continuity, all the
operatorpropertiesinDefinition3areconservedthroughcomposition.
Proposition2 Let X, X , and X be three Hilbert spaces. If the linear operators
1 2
T : X → X and T : X → X are both bounded (resp, bounded below with
1 1 2 1 2
constants 𝐴 ,𝐴 or of Parseval type), then the same holds true for the composed
1 210 MichaelUnserandStanislasDucotterd
operator T = T ◦T : X → X → X with ∥T∥ ≤ ∥T ∥ ∥T ∥ (resp., with lower
2 1 1 2 1 2
bound 𝐴= 𝐴 𝐴 ).
1 2
Forinstance,ifT andT arebothboundedbelow,then,forall𝑥 ∈X,
1 2
∥T T {𝑥}∥ ≥ 𝐴 ∥T {𝑥}∥ ≥ 𝐴 𝐴 ∥𝑥∥ (20)
2 1 X 2 2 1 X 1 2 1 X
withT {𝑥} ∈X .
1 1
3 Vector-ValuedLSIOperatorsonℓ𝑵(Z𝒅)
2
Inthissection,weshallidentifyandcharacterizethespecialclassoflinearoperators
thatoperateondiscretevector-valuedsignalsandcommutewiththeshiftoperation.
Definition4 A discrete operator T is linear-shift-invariant (LSI) if it is linear
LSI
andif,foranydiscretevector-valuedsignalx[·] initsdomainandany 𝒌 ∈Z𝑑 ,
0
T {x[·−𝒌 ]} =T {x}[·−𝒌 ].
LSI 0 LSI 0
WeobservethattheLSIpropertyisconservedthroughlinearcombinationsand
composition.Moreover,weshallseethatallℓ -stableLSIoperatorsactingondiscrete
2
vector-valued signals can be identified as (multichannel) convolution operators, as
statedinTheorem2.
3.1 Refresher:ScalarConvolutionOperators
Tosetthecontext,wefirstpresentaclassicresultonthecharacterizationofscalar
LSI operators, together with a self-contained proof that will serve as model for
subsequentderivations.
Theorem1(KerneltheoremfordiscreteLSIoperatorsonℓ (Z𝑑))Foranygiven
2
ℎ ∈ℓ 2(Z𝑑),theoperatorTℎ :𝑥[·] ↦→ (ℎ∗𝑥)[·] with𝑥[·] ∈ℓ 2(Z𝑑)and
(ℎ∗𝑥)[𝒌] =△ ⟨ℎ,𝑥[𝒌−·]⟩ ℓ (Z𝑑) = ∑︁ ℎ[𝒎]𝑥[𝒌−𝒎], 𝒌 ∈Z𝑑 (21)
2
𝒎∈Z𝑑
islinear-shift-invariant.Moreover,Tℎ continuouslymapsℓ 2(Z𝑑) → ℓ 2(Z𝑑) ifand
only if ∥ℎˆ∥𝐿
∞
= esssup 𝝎∈[−𝜋,+𝜋]𝑑(cid:12) (cid:12)ℎˆ(𝝎)(cid:12) (cid:12) < ∞. Conversely, for every continuous
LSI operator T : ℓ (Z𝑑) → ℓ (Z𝑑), there is one and only one ℎ ∈ ℓ (Z𝑑) with
LSI 2 2 2
∥ℎˆ||𝐿
∞
= ∥T LSI∥suchthatT
LSI
:𝑥[·] ↦→ (ℎ∗𝑥)[·]wheretheconvolutionisspecified
by(21).
Proof.ParsevalConvolutionOperatorsandNeuralNetworks 11
Direct part. The assumption (ℎ,𝑥) ∈ ℓ (Z𝑑) ×ℓ (Z𝑑) ensures that (21) is well-
2 2
defined for any 𝒌 ∈ Z𝑑 . The shift-invariance is then an obvious consequence of
Definition4,as
Tℎ{𝑥}[𝒌−𝒌 0] = ⟨ℎ,𝑥[𝒌−𝒌 0−·]⟩ℓ
2
= ⟨ℎ,𝑥[(𝒌−·)−𝒌 0]⟩ℓ
2
=Tℎ{𝑥[·−𝒌 0]}[𝒌].
ByobservingthattheFouriertransformof𝑥[𝒌−·] ∈ℓ (Z𝑑)is𝑥ˆ(𝝎)ej⟨𝝎,𝒌⟩,wethen
2
invokePlancherel’sidentity(6)toshowthat
(ℎ∗𝑥)[𝒌] = ⟨ℎ,𝑥[𝒌−·]⟩ℓ
2
=∫
T𝑑
ℎˆ(𝝎)𝑥ˆ(𝝎)ej⟨𝝎,𝒌⟩ (2d 𝜋𝝎
)𝑑
= F d−1(cid:8)ℎˆ ×𝑥ˆ(cid:9) [𝒌],
wheretheidentificationoftheinverseFourieroperatorislegitimatesincethebound-
ednessofℎˆ impliesthatℎˆ×𝑥ˆ ∈ 𝐿 (T𝑑).Consequently,weareinthepositionwhere
2
wecaninvokeParseval’srelation
∫ d𝝎 ∫ d𝝎
∥ℎ∗𝑥∥2 = |ℎˆ(𝝎)|2|𝑥ˆ(𝝎)|2 ≤ ∥ℎˆ∥2 |𝑥ˆ(𝝎)|2 .
ℓ 2 T𝑑 (2𝜋)𝑑 𝐿 ∞ T𝑑 (2𝜋)𝑑
Thisyieldsthestabilitybound∥ℎ∗𝑥∥ℓ ≤ ∥ℎˆ∥𝐿 ∥𝑥∥ℓ ,whichimpliesthecontinuity
of Tℎ : ℓ 2(Z𝑑) → ℓ 2(Z𝑑). To show th2 at the lat∞ ter bo2 und is sharp (“if and only if”
part of the statement), we refer to the central, more technical part of the proof of
Theorem2.
△
Indirect Part. We define the linear functional ℎ : 𝑥 ↦→ ⟨ℎ,𝑥⟩ = T {𝑥∨}[0]. The
LSI
continuity of T
LSI
: ℓ 2(Z𝑑) → ℓ 2(Z𝑑) implies that T LSI{𝑥 𝑖∨}[0] = ⟨ℎ,𝑥 𝑖⟩ → 0
foranysequenceofsignals (𝑥 𝑖)𝑖∈N inℓ 2(Z𝑑) thatconvergesto0(or,equivalently,
𝑥∨ → 0).Thisensuresthatthefunctional ℎ : 𝑥 ↦→ ⟨ℎ,𝑥⟩ iscontinuousonℓ (Z𝑑),
𝑖 2
meaningthatℎ ∈ (cid:0)ℓ 2(Z𝑑)(cid:1)′ =ℓ 2(Z𝑑),whichallowsustowritethat⟨ℎ,𝑥⟩ = ⟨ℎ,𝑥⟩ℓ 2.
Wethenmakeuseoftheshift-invariancepropertytoshowthat
T LSI{𝑥}[𝒌] =T LSI{𝑥[·+𝒌]}(0) = ⟨ℎ,𝑥[·+𝒌]∨⟩ℓ
2
= ⟨ℎ,𝑥[𝒌−·]⟩ℓ
2
forany 𝒌 ∈Z𝑑 ,fromwhichwealsodeducethatT {𝛿} = ℎ. ⊓⊔
LSI
Theorem1tellsusthatanLSIoperatorT : ℓ (Z𝑑) → ℓ (Z𝑑) canalwaysbe
LSI 2 2
implementedasadiscreteconvolutionwithitsimpulseresponse ℎ = T {𝛿[·]}.It
LSI
also provides the Lipschitz constant of the operator, as Lip(T ) = ∥ℎˆ∥ (supre-
LSI ∞
mumofitsfrequencyresponse).(Werecallthat,foralinearoperator,theLipschitz
constantispreciselythenormoftheoperator.)Wealsonotethattheclassiccondi-
tionforstabilityfromlinear-systemstheory, ℎ ∈ ℓ (Z𝑑),issufficienttoensurethe
1
continuityoftheoperatorbecause |ℎˆ(𝝎)| ≤ ∥ℎ∥ℓ .However,thelatterconditionis
1
notnecessary;forinstance,theℓ -Lipschitzconstantofanideallowpasspassfilter
2
is1bydesign,whileits(sinc-like)impulseresponseisnotincludedinℓ (Z𝑑).
112 MichaelUnserandStanislasDucotterd
3.2 MultichannelConvolutionOperators
We now show that the concept carries over to vector-valued signals. To that end,
weconsideragenericmultichannelconvolutionoperatorthatactsonan 𝑁-channel
inputsignalx[·] ∈ℓ𝑁(Z𝑑) andreturnsan 𝑀-channeloutputy[·].Suchanoperator
2
(cid:2) (cid:3)
ischaracterizedthroughitsmatrix-valuedimpulseresponseH[·] with H[·] =
𝑚,𝑛
ℎ 𝑚,𝑛[·] ∈ℓ 2(Z𝑑)andH[𝒌] ∈R𝑀×𝑁 forany𝒌 ∈Z𝑑 .Fromnowon,weshalldenote
suchaconvolutionoperatorbyT andrefertoitasamultichannelfilter.
H
Tobenefitfromthetoolsandtheorydevelopedforthescalarcase,itisusefulto
express the multichannel convolution as the matrix-vector combination of a series
ofcomponent-wisescalarconvolutions (ℎ 𝑚,𝑛∗𝑥 𝑛)[·] with (𝑚,𝑛) ∈ {1,...,𝑀}×
{1,...,𝑁}.Thisiswrittenas
T H :x[·] ↦→ (H∗x)[𝒌] =△

  
(cid:205) 𝑛𝑁 =1(ℎ 1,𝑛
. . .
∗𝑥 𝑛)[𝒌] 
  , (22)
  (cid:205) 𝑛𝑁 =1(ℎ 𝑀,𝑛∗𝑥 𝑛)[𝒌] 

where
 ℎ 1,1[·] ··· ℎ 1,𝑁[·] 
H[·] =  

. . . ... . . .  

= [h 1[·] ··· h𝑁[·]] (23)
 ℎ 𝑀,1[·] ··· ℎ 𝑀,𝑁[·] 
 
withthe𝑛thcolumnoftheimpulseresponsebeingidentifiedas
 ℎ 1,𝑛[·] 
 . 
h𝑛[·] = 

. . 

=T H{e𝑛𝛿[·]}.
 ℎ 𝑀,𝑛[·]

 
We also note that the convolution in (22) has an explicit representation, given by
(25),whichisthematrix-vectorcounterpartofthescalarformula(21).
Asinthescalarscenario,themultichannelconvolutioncanbeimplementedbya
multiplicationintheFourierdomain,withthefrequencyresponseofthefilternow
havingtheformofamatrix.Specifically,foranyx[·] ∈ ℓ𝑁(Z𝑑) withvector-valued
2
Fouriertransformx= F {x[·]} ∈ 𝐿𝑁(T𝑑),wehavethat
(cid:98) d 2
(cid:8) (cid:9) (cid:104) (cid:105)
F𝑑 (H∗x)[·] (𝝎) =H(cid:98)(𝝎) (cid:98)x(𝝎) = (cid:98)h 1(𝝎) ··· (cid:98)h𝑁(𝝎) (cid:98)x(𝝎)
 ℎˆ 1,1(𝝎) ··· ℎˆ 1,𝑁(𝝎)   𝑥ˆ 1(𝝎) 
=   . . . ... . . .     . . .  , (24)
   

ℎˆ 𝑀,1(𝝎) ··· ℎˆ
𝑀,𝑁(𝝎)

 𝑥ˆ𝑁(𝝎)

   ParsevalConvolutionOperatorsandNeuralNetworks 13
where the matrix-valued function H(cid:98) : [−𝜋,𝜋]𝑑 → C𝑁×𝑀 , with [H(cid:98)]𝑚,𝑛 = ℎˆ 𝑚,𝑛 =
F d{ℎ 𝑚,𝑛}, is the component-by-component Fourier transform of the matrix filter
H[·].
3.3 KernelTheoremforMultichannelLSIOperators
The matrix-vector convolution specified by (22) is well-defined for any x[·] ∈
ℓ𝑁(Z𝑑) under the assumption that H[·] ∈ ℓ (Z𝑑)𝑀×𝑁 . Yet, we need to be a bit
2 2
moreselectivetoensurethattheoperatoris(Lipschitz-)continuouswithrespectto
theℓ -norm.WeshowinTheorem2thatthereisanequivalencebetweencontinuous
2
multi-channel LSI operators and bounded multichannel filters (convolution opera-
tors), while we also give an explicit formula for the norm of the operator. As one
may expect, the Schwartz kernel of the LSI operator is the matrix-valued impulse
responseofthemultichannelfilter.
Theorem2(Kernel theorem for LSI operators ℓ𝑁(Z𝑑) → ℓ𝑀(Z𝑑)) For any
2 2
given H[·] ∈ ℓ (Z𝑑)𝑀×𝑁, the convolution operator T : x[·] ↦→ (H∗x)[·] with
2 H
𝑁-vector-valuedinputx[·] ∈ℓ𝑁(Z𝑑)and𝑀-vector-valuedoutput
2
(H∗x)[𝒌] = ∑︁ H[ℓ]x[𝒌−ℓ], 𝒌 ∈Z𝑑 (25)
ℓ∈Z𝑑
islinear-shift-invariantandcharacterizedbyitsmatrix-valuedfrequencyresponse
F d{H[·]} = H(cid:98)(·) ∈ 𝐿 2(T𝑑)𝑀×𝑁. Moreover, T
H
continuously maps ℓ 2𝑁(Z𝑑) →
ℓ𝑀(Z𝑑)ifandonlyif
2
∥T H∥ =𝜎 sup,H = esssup 𝜎 max(cid:0) H(cid:98)(𝝎)(cid:1) <∞, (26)
𝝎∈[−𝜋,𝜋]𝑑
where𝜎 max(cid:0) H(cid:98)(𝝎)(cid:1) with𝝎fixedisthemaximalsingularvalueofthematrixH(cid:98)(𝝎) ∈
C𝑀×𝑁.
Conversely,foreverycontinuousLSIoperatorT :ℓ𝑁(Z𝑑) −→c. ℓ𝑀(Z𝑑),there
LSI 2 2
isoneandonlyoneH[·] ∈ℓ (Z𝑑)𝑀×𝑁 (thematrix-valuedimpulseresponseofT )
2 LSI
suchthatT LSI =T H :x[·] ↦→ (H∗x)[·] and∥T LSI∥ ℓ𝑁→ℓ𝑀 =𝜎 sup,H <∞.
2 2
Proof.
Direct Part. The 𝑚th entry of (H∗x)[𝒌] can be identified as (cid:2) (H∗x)[𝒌](cid:3) =
𝑚
⟨g𝑚,x[𝒌−·]⟩ ℓ𝑁(Z𝑑) withg𝑚[·] = (ℎ 𝑚,1[·],...,ℎ 𝑚,𝑁[·]) ∈ ℓ 2𝑁(Z𝑑) beingthe𝑚th
2
rowofthematrix-valuedimpulseresponseH[·].TheLSIproperty(seeDefinition
4)thenfollowsfromtheobservationthat
(cid:2) (H∗x)[𝒌−𝒌 0](cid:3) 𝑚 = ⟨g𝑚,x[(𝒌−𝒌 0)−·)]⟩ ℓ𝑁(Z𝑑)
2
= ⟨g𝑚,x[(𝒌−·)−𝒌 0]⟩ ℓ𝑁(Z𝑑) = (cid:2) (H∗x[·−𝒌 0])[𝒌](cid:3) 𝑚,
214 MichaelUnserandStanislasDucotterd
for𝑚 =1,...,𝑀 andany 𝒌,𝒌 ∈Z𝑑 .
0
The Fourier-domain equivalent of the hypothesis x ∈ ℓ𝑁(Z𝑑) (resp. H[·] ∈
2
ℓ 2(Z𝑑)𝑀×𝑁 )is (cid:98)x ∈ 𝐿 2𝑁(T𝑑) (resp.,H(cid:98)(·) ∈ 𝐿 2(T𝑑)𝑀×𝑁 ).Thekeyforthisequiva-
lenceisthevector-valuedversionofParseval’sidentitygivenby
∑︁𝑁 ∑︁𝑁 ∫ d𝝎 ∫ d𝝎
∥x∥ ℓ2
2𝑁(Z𝑑)
= 𝑛=1∥𝑥 𝑛∥ ℓ2
2
=
𝑛=1
T𝑑
|𝑥ˆ𝑛(𝝎)|2
(2𝜋)𝑑
=
T𝑑
∥ (cid:98)x(𝝎)∥2
2(2𝜋)𝑑
= ∥ (cid:98)x∥2
𝐿
2𝑁(T𝑑).
Likewise, under the assumption that H(cid:98)(·) (cid:98)x(·) ∈ 𝐿 2𝑀(T𝑑), we can evaluate the ℓ 2-
normoftheconvolvedsignalas
(cid:18)∫
d𝝎
(cid:19)1
2
∥H∗x∥ ℓ 2𝑀(Z𝑑) =
T𝑑
∥H(cid:98)(𝝎) (cid:98)x(𝝎)∥2 2(2𝜋)𝑑 = ∥H(cid:98)(cid:98)x∥ 𝐿 2𝑀(T𝑑), (27)
wherewearerelyingonthepropertythattheconvolutioncorrespondstoapointwise
multiplicationintheFourierdomain.
NormoftheOperator.Implicitinthespecificationof𝜎 sup,Hin(26)istherequirement
thatthematrix-valuedfrequencyresponseH(cid:98)(·) : T𝑑 → C𝑀×𝑁 bemeasurableand
boundedalmosteverywhere.ThismeansthatH(cid:98)(𝝎) with𝝎 fixedisawell-defined
matrix in C𝑀×𝑁 for almost any 𝝎 ∈ T𝑑 . In that case, we can specify its maximal
singularvaluesby
𝜎 max(cid:0) H(cid:98)(𝝎)(cid:1) = sup
∥H(cid:98)( ∥𝝎 u∥)u∥
2
u∈C𝑁\{0} 2
Consequently,foranyx(·) ∈ 𝐿𝑁(T𝑑),wehavethat
(cid:98) 2
∥H(cid:98)(𝝎) (cid:98)x(𝝎)∥ 2 ≤ ∥ (cid:98)x(𝝎)∥ 2·𝜎 max(cid:0) H(cid:98)(𝝎)(cid:1) ≤ ∥ (cid:98)x(𝝎)∥ 2·𝜎 sup,H
foralmostany𝝎 ∈T𝑑 .Thisimpliesthat
(cid:18)∫
d𝝎
(cid:19)1
2
∥H(cid:98)(cid:98)x∥ 𝐿 2𝑀(T𝑑) =
T𝑑
∥H(cid:98)(𝝎) (cid:98)x(𝝎)∥2 2(2𝜋)𝑑
(cid:18)∫
d𝝎
(cid:19)1
2
≤ 𝜎 sup,H
T𝑑
∥ (cid:98)x(𝝎)∥2 2(2𝜋)𝑑 =𝜎 sup,H·∥ (cid:98)x∥ 𝐿 2𝑁(T𝑑) (28)
which,duetotheFourierisometry,yieldstheupperbound∥T H∥ ≤ 𝜎 sup,H.
Likewise, (27) implies that ∥T H∥ = ∥H(cid:98)∥, which is the norm of the pointwise
multiplication operator (cid:98)x ↦→ H(cid:98)(cid:98)x and is equal to ∥𝜎 max(cid:0) H(cid:98)(·)(cid:1) ∥𝐿 ∞. Indeed, for any
(cid:98)x(·) ∈ S(T𝑑)𝑁 ⊂ 𝐿 2𝑁(T𝑑), the boundedness of H(cid:98) : 𝐿 2𝑁(T𝑑) → 𝐿 2𝑀(T𝑑) implies
thatParsevalConvolutionOperatorsandNeuralNetworks 15
∫ d𝝎 ∫ d𝝎
[−𝜋,𝜋]𝑑(cid:98)xH(𝝎)H(cid:98)H(𝝎)H(cid:98)(𝝎) (cid:98)x(𝝎)
(2𝜋)𝑑
≤ ∥H(cid:98)∥2
[−𝜋,𝜋]𝑑
∥ (cid:98)x(𝝎)∥2 2(2𝜋)𝑑,
whichisequivalentto
∫ (cid:16) (cid:17) d𝝎
[−𝜋,𝜋]𝑑(cid:98)xH(𝝎) ∥H(cid:98)∥2I𝑁 −H(cid:98)H(𝝎)H(cid:98)(𝝎) (cid:98)x(𝝎)
(2𝜋)𝑑
≥ 0.
(cid:16) (cid:17)
ThisrelationimpliesthattheHermitian-symmetricmatrix ∥H(cid:98)∥2I𝑁 −H(cid:98)H(𝝎)H(cid:98)(𝝎)
isnonnegative-definiteforalmostany𝝎 ∈ T𝑑 .Onthesideoftheeigenvalues,this
translatesinto
(cid:16) (cid:17) (cid:16) (cid:17)
∥H(cid:98)∥2−𝜆
max
H(cid:98)H(𝝎)H(cid:98)(𝝎) = ∥H(cid:98)∥2−𝜎 m2
ax
H(cid:98)(𝝎) ≥ 0 𝑎.𝑒.
(cid:16) (cid:17)
leadingto ∥𝜎 max H(cid:98)(·) ∥𝐿 ∞ = 𝜎 sup,H ≤ ∥H(cid:98)∥ = ∥T H∥.Sincewealreadyknowthat
∥T H∥ ≤ 𝜎 sup,H,wededucethat∥T H∥ =𝜎 sup,H.
(cid:104) (cid:105)
IndirectPart.Wedefinethelinearfunctionalsg𝑚 :x[·] ↦→ ⟨g𝑚,x⟩ = T LSI{x∨}[0]
𝑚
with 𝑚 ∈ {1,...,𝑀}. The continuity of T : ℓ𝑁(Z𝑑) → ℓ𝑀(Z𝑑) implies that
LSI 2 2
(cid:104) (cid:105)
T LSI{x∨
𝑖
}[0] = ⟨g𝑚,x𝑖⟩ →0foranyconvergingsequencex𝑖[·] →0(or,equiv-
𝑚
alently, x∨
𝑖
[·] → 0) inℓ 2𝑁(Z𝑑). This ensures that the functional g𝑚 : x ↦→ ⟨g𝑚,x⟩
iscontinuousonℓ 2𝑁(Z𝑑),whichisequivalenttog𝑚 = (𝑔 𝑛,𝑚[𝒌]) (𝑛,𝒌)∈{1,...,𝑁}×Z𝑑 ∈
(cid:0)ℓ 2𝑁(Z𝑑)(cid:1)′ = ℓ 2𝑁(Z𝑑). This then allows us to write ⟨g𝑚,x⟩ = ⟨g𝑚,x⟩ ℓ𝑁 for all
𝑚 ∈ {1,...,𝑀}.Wethenmakeuseoftheshift-invariancepropertytosho2 wthat
⟨g 1,x∨[·+𝒌]⟩
ℓ𝑁
(cid:169) . 2 (cid:170)
T LSI{x}[𝒌] =T LSI{x[·+𝒌]}[0] =(cid:173)
(cid:173)
. . (cid:174)
(cid:174)
(cid:173) ⟨g𝑀,x∨[·+𝒌]⟩ ℓ𝑁(cid:174)
(cid:171) 2 (cid:172)
(cid:205) 𝑛𝑁 =1(cid:205) 𝒎∈Z𝑑𝑔 𝑛,1[𝒎]𝑥 𝑛[𝒌−𝒎]
=(cid:169)
(cid:173)
. .
.
(cid:170)
(cid:174)
(cid:173) (cid:174)
(cid:171)(cid:205) 𝑛𝑁 =1(cid:205) 𝒎∈Z𝑑𝑔 𝑛,𝑀[𝒎]𝑥 𝑛[𝒌−𝒎]
(cid:172)
(cid:205) 𝑛𝑁 =1(𝑔 𝑛,1∗𝑥 𝑛)[𝒌]
=(cid:169) (cid:173) . . . (cid:170) (cid:174)= (H∗x)[𝒌]
(cid:173) (cid:174)
(cid:171)(cid:205) 𝑛𝑁 =1(𝑔 𝑛,𝑀 ∗𝑥 𝑛)[𝒌]
(cid:172)
forany 𝒌 ∈ Z𝑑 ,fromwhichwededucethatT = T withmatrix-valuedimpulse
LSI H
response H[·] whose entries are ℎ 𝑚,𝑛[𝒌] = (cid:2) g𝑚[𝒌](cid:3)
𝑛
= 𝑔 𝑛,𝑚[𝒌] with 𝑔 𝑛,𝑚[·] ∈
ℓ (Z𝑑). ⊓⊔
2
An immediate consequence is that the composition of the two continuous LSI
operatorsT : ℓ𝑁(Z𝑑) → ℓ𝑁 2(Z𝑑) andT : ℓ𝑁 2(Z𝑑) → ℓ𝑀(Z𝑑) yieldsastable
H1 2 2 H2 2 216 MichaelUnserandStanislasDucotterd
multi-filter T
H
= T
H2∗H1
: ℓ 2𝑁(Z𝑑) → ℓ 2𝑀(Z𝑑) with ∥T H∥ ≤ 𝜎 sup,H1𝜎 sup,H2. The
frequency response of the composed filter is the product H(cid:98)(𝝎) = H(cid:98)1(𝝎)H(cid:98)2(𝝎)
of the individual responses, as expected. On the side of the impulse response, this
translatesintothematrix-to-matrixconvolution
(H ∗H )[𝒌] =△ ∑︁ H [𝒎]H [𝒌−𝒎], 𝒌 ∈Z𝑑, (29)
2 1 2 1
𝒎∈Z𝑑
which is the matrix counterpart of (21). Beside the fact that the inner dimension
(𝑁 )ofthematricesmustmatch,animportantdifferencewiththescalarsettingis
2
thatmatrixconvolutionsaregenerallynotcommutative.
3.4 ParsevalFilterbanks
We now proceed with the characterization of the complete family of Parseval LSI
operatorsfromℓ𝑁(Z𝑑) →ℓ𝑀(Z𝑑).WeknowfromTheorem2thattheseareneces-
2 2
sarilyfilterbanksoftheformT :x[·] ↦→ (H∗x)[·],whichcanalsobespecifiedby
H
theirmatrix-valuedfrequencyresponseH(cid:98)(·).Moreover,Proposition1tellsusthat
theParsevalconditionisequivalenttoT∗ ◦T =Id.
H H
Consequently, the only remaining part is to identify the adjoint operator T∗ :
H
ℓ𝑀(Z𝑑) →ℓ𝑁(Z𝑑),whichisdonethroughthemanipulation
2 2
∀(x,y) ∈ℓ𝑁(Z𝑑)×ℓ𝑀(Z𝑑) :
2 2
⟨y,(H∗x)[·]⟩ ℓ𝑀(Z𝑑) = ⟨ (cid:98)y,H(cid:98)(cid:98)x⟩ 𝐿𝑀(T𝑑) = ⟨H(cid:98)H (cid:98)y, (cid:98)x⟩ 𝐿𝑁(T𝑑) = ⟨(HT∨∗y)[·],x)⟩ ℓ𝑁(Z𝑑),
2 2 2 2
whereweusedtheFourier-Plancherelisometry,apointwiseHermitiantransposition
to move the frequency-response matrix on the other side of the inner product, and
the property that a complex conjugation of the frequency response translates into
theflippingoftheimpulseresponse.Basedon(18),wecanthenidentifyT∗ :y↦→
H
(HT∨ ∗y)[·]. This shows that the adjoint of T is the convolution operator whose
H
matriximpulseresponseisHT∨[·] (theflippedandtransposedversionofH[·])and
whosefrequencyresponseisF d{HT∨[·]} =H(cid:98)H(·).
Proposition3(Characterization of Parseval-LSI operators) A linear operator
T:ℓ𝑁(Z𝑑) →ℓ𝑀(Z𝑑)with𝑀 ≥ 𝑁 isLSIandenergy-preserving(Parseval)ifand
2 2
onlyifitcanberepresentedasamultichannelfilterbankT=T :x[·] ↦→ (H∗x)[·]
H
whosematrix-valuedimpulseresponseH[𝒌] ∈ R𝑀×𝑁 with 𝒌 rangingoverZ𝑑 has
anyofthefollowingequivalentproperties.
1. Invertibilitybyflip-transposition:
(HT∨∗H)[·] =I𝑁𝛿[·],
whichisequivalenttoT∗ ◦T =Idonℓ𝑁(Z𝑑).
H H 2ParsevalConvolutionOperatorsandNeuralNetworks 17
2. Paraunitaryfrequencyresponse:
H(cid:98)H(𝝎)H(cid:98)(𝝎) =I𝑁 forall𝝎 ∈T𝑑,
whereH(cid:98) = F d{H[·]} ∈ 𝐿 2(T𝑑)𝑀×𝑁 isthediscreteFouriertransformofH[·].
3. Preservationofinnerproducts:
∀x,y∈ℓ 2𝑁(Z𝑑) : ⟨x,y⟩ ℓ𝑁(Z𝑑) = ⟨(H∗x)[·],(H∗y)[·]}⟩ ℓ𝑀(Z𝑑).
2 2
We also note that the LSI-Parseval property implies that ∥H[·]∥ ℓ𝑀×𝑁(Z𝑑) = 𝑁
2
and∥T ∥ =1,althoughthoseconditionsareobviouslynotsufficient.
H
WhileItem1suggeststhattheadjointT∗ = T : ℓ𝑀(Z𝑑) → ℓ𝑁(Z𝑑) actsas
H HT∨ 2 2
the inverse of T , this is only true for signals y[·] ∈ T (cid:0)ℓ𝑁(Z𝑑)(cid:1) ⊂ ℓ𝑀(Z𝑑) that
H H 2 2
areintherangeoftheoperator.Inotherwords,T∗ isonlyaleftinverseofT∗,while
H H
isfailstobearightinverseingeneral,unless 𝑀 = 𝑁.ThisisdenotedbyT∗ = T+
H H
(generalizedinverse).
Whilethe𝑀-to-𝑁filterT∗ isgenerallynotaParsevalfilter,itis1-Lipschitz(since
H
∥T∗ ∥ = ∥T ∥ = 1) with its Gram operator (T∗∗ ◦T∗) = (T ◦T∗) : ℓ𝑀(Z𝑑) →
H H H H H H 2
ℓ𝑀(Z𝑑)
beingtheorthogonalprojectorontherangeofT ,ratherthantheidentity.
2 H
Correspondingly, from the properties of the singular value decomposition (SVD),
wecaninferthatH(cid:98)H(𝝎)with𝝎fixedhasthesamenonzerosingularvaluesasH(cid:98)(𝝎)
(𝑁 singular values equal to one) and that these are complemented with (𝑀 − 𝑁)
additionalzerostomakeupforthefactthat𝑀 > 𝑁.
ThefilterbanksusedinconvolutionalneuralnetworkaregenerallyFIR,meaning
thattheirmatriximpulseresponseisfinitelysupported.Thisisthereasonwhythe
reminderofthechapterisdevotedtotheinvestigationofFIR-Parsevalconvolution
operators.Tosetthestage,westartwiththesingle-channelcase𝑁 = 𝑀 =1,which
hasthefewestdegreesoffreedom.
Proposition4 Thereal-valuedLSIoperatorTℎ :ℓ 2(Z𝑑) →ℓ 2(Z𝑑)isFIR-Parseval
if and only if ℎ = ±𝛿[·− 𝒌 0] for some 𝒌
0
∈ Z𝑑. Equivalently, Tℎ = ±S𝒌0 where
S𝒌0 :𝑥[·] ↦→𝑥[·−𝒌 0].
Proof. FromProposition3,weknowthattheLSI-Parsevalpropertyisequivalentto
(ℎ∨ ∗ ℎ)[𝒌] = (cid:205) 𝒎∈Z𝑑 ℎ[−𝒎]ℎ[𝒌 − 𝒎] = (cid:205) 𝒎∈Z𝑑 ℎ[𝒎]ℎ[𝒎 + 𝒌] = 𝛿[𝒌], which
is obviously met for ℎ = ±𝛿[·− 𝒌 ]. Now, if supp(ℎ) = {𝒎 ∈ Z𝑑 : ℎ[𝒎] ≠ 0} is
0
finiteandincludesatleasttwodistinctpoints,thentherealwaysexistssomecritical
offset 𝒌 ≠ 0 such that supp(ℎ) ∩supp(ℎ[·+ 𝒌 ]) = {𝒎 }; in other words, such
0 0 0
thattheintersectionofthesupportanditsshiftedversionby−𝒌 consistsofasingle
0
point. Consequently, (cid:205) 𝒎∈Z𝑑 ℎ[𝒎]ℎ[𝒎 + 𝒌 0] = ℎ[𝒎 0]ℎ[𝒎 0 + 𝒌 0] ≠ 0, which is
incompatiblewiththedefinitionoftheKroneckerdelta. ⊓⊔
Proposition4identifiestheshiftoperatorsasfundamentalLSI-Parsevalelements,
butthefamilyisactuallylargerifwerelaxtheFIRcondition.Thefrequency-domain
conditionforParsevalis |ℎˆ(𝝎)| = 1,whichtranslatesintothefilterbeingall-pass.18 MichaelUnserandStanislasDucotterd
Besideanypoweroftheshiftoperator,aclassicexamplefor𝑑 =1isℎˆ(𝜔) = e−j𝜔−𝑧 0 ,
1−𝑧 0e−j𝜔
withthecaveatthattheimpulseresponseofthelatterisinfinitelysupported.
4 ParametrizationofParsevalFilterbanks
Whilethedesignoptionsfor(univariate)FIRParsevalfiltersarefairlylimited(see
Proposition 4), we now show that the possibilities open up considerably in the
multichannelsetting.Thisisgoodnewsforapplications.
Our approach to construct trainable FIR Parseval filterbanks is based on the
definition of basic 1-to-𝑁, 𝑁-to-𝑁, and 𝑁-to-(𝑝𝑁) Parseval filters that can then
be chained, in the spirit of neural networks, to produce more complex structures.
Specifically, let T : ℓ𝑁𝑖(Z𝑑) → ℓ𝑁𝑖+1(Z𝑑),𝑖 = 1,...,𝐼 be a series of Parseval
H𝑖 2 2
filters with 𝑁 1 = 𝑁 ≤ 𝑁 𝑖 ≤ ··· ≤ 𝑁 𝐼+1 = 𝑀 and T∗ H𝑖 ◦T H𝑖 = Id on ℓ 2𝑁𝑖(Z𝑑).
Because the LSI and Parseval properties are preserved through composition, one
immediatelydeducesthatthecomposedoperator
T =T ◦···◦T :ℓ𝑁(Z𝑑) →ℓ𝑀(Z𝑑) (30)
H H𝐼 H1 2 2
isParseval-LSIwithimpulseresponseH[·] = (H𝐼∗···∗H 2∗H 1)[·] ∈ℓ 2(Z𝑑)𝑀×𝑁 .
Thisfilterisinvertiblefromtheleftwithitsgeneralizedinversebeing
T+ =T∗ =T∗ ◦···◦T∗ :ℓ𝑀(Z𝑑) →ℓ𝑁(Z𝑑), (31)
H H H1 H𝐼 2 2
which means that the inverse filtering can be achieved via a simple flow-graph
transpositionoftheoriginalfilterarchitecture.
Thus, our design concept is to rely on simple elementary modules, each being
parameterized by an orthogonal matrix U𝑖 ∈ C𝑀×𝑀 where 𝑀 is typically the
numberofoutputchannels.ThelistofourprimarymodulesissummarizedinTable
1.Additionaldetaileddescriptionsandexplanationsaregivenintheremainderof
thissection.
4.1 Normalizedpatchoperator
Ourfirsttoolisasimplemechanismtoaugmentthenumberofoutputchannelsof
thefilterbank.Itinvolvesapatchofsize𝑀 specifiedbyalistK 𝑀 = {𝒌 1,···𝒌𝑀}of
indices,whichwillthereafterbeusedtodescribethesupportoffiltersactingoneach
featurechannel.Ournormalizedpatchoperatorextractsthesignalvalueswithinthe
patchinrunningfashionasParsevalConvolutionOperatorsandNeuralNetworks 19
LSI-ParsevalOperators ImpulseResponse
PatchdescriptorK 𝑀 ={𝒌 1,...,𝒌𝑀}
I𝑁𝛿[·−𝒌 1]
PatchK𝑀 :ℓ 2𝑁(Z𝑑) →ℓ 2𝑀×𝑁(Z𝑑) √1 𝑀(cid:169) (cid:173)
(cid:173)
. . . (cid:170) (cid:174)
(cid:174)
I𝑁𝛿[·−𝒌𝑀]
UnitarymatrixU=[u 1...u𝑁] ∈C𝑁×𝑁 (cid:171) (cid:172)
MultU:ℓ 2𝑁(Z𝑑) →ℓ 2𝑁(Z𝑑) U𝛿[·]
𝑁
Th=UPatchK𝑁 :ℓ 2(Z𝑑) →ℓ 2𝑁(Z𝑑) h[·]= √1
𝑁
∑︁ u𝑛𝛿[·−𝒌𝑛]
𝑛=1
LargeunitarymatrixU=[U 1···U𝑝] ∈C𝑝𝑁×𝑝𝑁
𝑝
UPatchK𝑁 :ℓ 2𝑁(Z𝑑) →ℓ 2𝑝𝑁 (Z𝑑) √1 𝑝∑︁ U𝑛𝛿[·−𝒌𝑛]
𝑛=1
Generalizedshiftwith𝑲 = (𝒌 1,...,𝒌𝑁) ∈Z𝑑×𝑁
S𝑲 :ℓ 2𝑁(Z𝑑) →ℓ 2𝑁(Z𝑑) diag(𝛿[·−𝒌 1],...,𝛿[·−𝒌𝑁])
FramematrixA∈C𝑀×𝑁 s.t.AHA=I𝑁
𝑁
AS𝑲 :ℓ 2𝑁(Z𝑑) →ℓ 2𝑀(Z𝑑) ∑︁ a𝑛eH 𝑛𝛿[·−𝒌𝑛]
𝑛=1
UnitarymatricesU,V∈C𝑁×𝑁
𝑁
US𝑲 :ℓ 2𝑁(Z𝑑) →ℓ 2𝑁(Z𝑑) ∑︁ u𝑛eH 𝑛𝛿[·−𝒌𝑛]
𝑛=1
𝑁
US𝑲VH:ℓ 2𝑁(Z𝑑) →ℓ 2𝑁(Z𝑑) ∑︁ u𝑛vH 𝑛𝛿[·−𝒌𝑛]
𝑛=1
Rank-𝑘projectorP𝑘 =U𝑘UH
𝑘
∈R𝑁×𝑁 withP2
𝑘
=P𝑘 =PT
𝑘
PP𝑘,𝑛:ℓ 2𝑁(Z𝑑) →ℓ 2𝑁(Z𝑑) (I𝑁 −P𝑘)𝛿[·]+P𝑘𝛿[·−e𝑛]
Householderelementwithu∈C𝑁 s.t.∥u∥ =1
2
Hu,𝑛:ℓ 2𝑁(Z𝑑) →ℓ 2𝑁(Z𝑑) (I𝑁 −uuH)𝛿[·]+uuH𝛿[·−e𝑛]
Table1:ElementaryparametricParsevalmulti-filters.There,mostfiltersareparam-
eterized by a unitary matrix/frame and a list of neighborhood indices 𝒌 1,...,𝒌𝑁
(notnecessarilydistinct).Thevectore𝑛 with [e𝑛]𝑚 = 𝛿 𝑛−𝑚 isthe𝑛thelementofa
canonicalbasis.
x[·−𝒌 ]
1
PatchK𝑀 :x[·] ↦→ √1
𝑀
(cid:169) (cid:173)
(cid:173)
. . . (cid:170) (cid:174) (cid:174). (32)
x[·−𝒌𝑀]
(cid:171) (cid:172)
One easily checks that PatchK𝑀 : ℓ 2𝑁(Z𝑑) → ℓ 2𝑀×𝑁(Z𝑑) is LSI and Parseval,
because the ℓ 2-norm is invariant to a shift and conserved in eac√h of the output
components—the very reason why the output is normalized by 𝑀. Its adjoint,
Patch∗ :ℓ𝑀×𝑁(Z𝑑) →ℓ𝑁(Z𝑑),isthesignalrecompositionoperator
K𝑀 2 220 MichaelUnserandStanislasDucotterd
y [·]
1 𝑀
Patch∗ K𝑀 :(cid:169) (cid:173)
(cid:173)
. . . (cid:170) (cid:174) (cid:174)↦→ √1
𝑀
∑︁ y𝑚[·+𝒌𝑚], (33)
y𝑀[·] 𝑚=1
(cid:171) (cid:172)
where the y𝑚[·] are 𝑁-vector-valued signals. The fundamental property for this
construction is Patch∗
K𝑀
◦PatchK𝑀 = Id on ℓ 2𝑁(Z𝑑), as direct consequence of the
isometricnatureoftheoperator.
For 𝑁 = 1, the impulse response of Patch∗
K𝑀
is √1
𝑀
(cid:2)𝛿(·+𝒌 1) ··· 𝛿(·+𝒌𝑀)(cid:3)
whosevector-valuedFouriertransformis √1 (cid:2) ej⟨𝝎,𝒌1⟩ ··· ej⟨𝝎,𝒌𝑀⟩(cid:3) .Theparauni-
𝑀
tarynatureofthissystemisrevealedinthebasicrelation
e−j⟨𝝎,𝒌1⟩

√1 (cid:2) ej⟨𝝎,𝒌1⟩ ··· ej⟨𝝎,𝒌𝑀⟩(cid:3) √1

 
.
. .

  =
(cid:205) 𝑚𝑀 =1|ej⟨𝝎,𝒌𝑚⟩|2
=1, (34)
𝑀 𝑀   𝑀
 e−j⟨𝝎,𝒌𝑀⟩

 
whichholdsforanychoiceofthe 𝒌𝑚.
4.2 Parametric1-to-𝑵 ParsevalModule
Thenecessaryandsufficientconditionfora1-to-𝑁operatorT :ℓ (Z𝑑) →ℓ𝑁(Z𝑑)
h 2 2
tohavetheParsevalpropertyis
𝑁
∑︁
(hT∨∗h)[·] = (ℎ∨
𝑛
∗ℎ 𝑛)[·] =𝛿[·],
𝑛=1
which,oncestatedininthefrequencydomain,is
𝑁
∀𝝎 ∈T𝑑 : ∥(cid:98)h(𝝎)∥2 =∑︁ |ℎˆ 𝑛(𝝎)|2 =1. (35)
𝑛=1
This indicates that the frequency responses of the component filters ℎ 𝑛 should be
power complementary. This is a standard requirement in wavelet theory and the
constructionoftightframes,whichhasbeenthebasisforvariousparametrizations
[53,42].
Whatweproposehereisasimplematrix-basedconstructionofsuchfilterswith
the support of each filter also being of size 𝑁. The filtering window, which is
common to all channels and assimilated to a patch, is specified by the index set
K 𝑁 = {𝒌 1,...,𝒌𝑁}.Theseindicesareusuallychosentobecontiguousandcentered
around the origin. For instance, K = {−1,0,1} specifies centered filters of size 3
3
indimension𝑑 =1.GivensomeorthogonalmatrixU= (cid:2) u
1
··· u𝑁(cid:3) ∈ R𝑁×𝑁 ,our
basicparametric1-to-𝑁 filteringoperatoristhengivenbyParsevalConvolutionOperatorsandNeuralNetworks 21
T
h
=Mult U◦PatchK𝑁 :ℓ 2(Z𝑑) →ℓ 2𝑁(Z𝑑), (36)
where Mult : x[·] ↦→ Ux[·] is the pointwise matrix-multiplication operator.
U
This succession of operations yields the vector-valued impulse response h[·] =
√1
𝑁
(cid:205) 𝑛𝑁 =1u𝑛𝛿[·−𝒌𝑛].ThisfilterisParsevalbyconstructionbecauseitisthecom-
positionoftwoParsevaloperators.
𝑁
0A <s 𝑁va ,r wia hn ict, hw the enm ra ey sua ll ts so inco an ss hi od re tr era Pr ae rd su evc ae ld fip lta et rch h[K ·]𝑁
=0
√=
1
𝑁{𝒌 (cid:205)1,
𝑛𝑁
=.
0
1.. u, 𝑛𝒌 𝛿𝑁 [·0} −w 𝒌𝑛it ]h
.
Thelatterisparameterizedbythe“truncated”matrixU
0
= (cid:2) u
1
0 ··· u𝑁 0(cid:3) ∈ R𝑁×𝑁 0,
whichissuchthatUT 0U
0
=I𝑁
0
(1-tightframeproperty).
4.3 Parametric 𝑵-to-𝒑𝑵 ParsevalModule
TheconcepthereisessentiallythesameasinSection4.2,exceptthatwenowhaveto
usealargerortho-matrixU∈R𝑝𝑁×𝑝𝑁 andapatchneighorhoodK 𝑝 = {𝒌 1,...,𝒌𝑝}.
Thisthenyieldsthemulti-filter
T
H
=Mult U◦PatchK𝑝 :ℓ 2𝑁(Z𝑑) →ℓ 2𝑝𝑁 (Z𝑑), (37)
whichisguaranteedtohavetheParsevalproperty,basedonthesameargumentsas
before.Itsadjointis
T∗ =T+ =Patch∗ ◦Mult :ℓ𝑝𝑁 (Z𝑑) →ℓ𝑁(Z𝑑). (38)
H H K𝑝 UT 2 2
(cid:2) (cid:3)
ToidentifytheimpulseresponseofMult U◦PatchK𝑝,wepartitionU= U
1
···U𝑝
into 𝑝submatricesU𝑖 ∈R𝑝𝑁×𝑁 ,eachassociatedwithitsshift 𝒌𝑖,whichyields
𝑝
1 ∑︁
H[·] = √
𝑝
U𝑖𝛿[·−𝒌𝑖]. (39)
𝑖=1
BytheorthonormalityofthecolumnvectorsofU,wethenexplicitlyevaluate
𝑝 𝑝
1 ∑︁∑︁
(HT∨∗H)[·] =
𝑝
UT 𝑚U𝑛𝛿[·+𝒌𝑚−𝒌𝑛]
𝑚=1𝑛=1
𝑝 𝑝
1 ∑︁ 1 ∑︁
=
𝑝
UT 𝑛U𝑛𝛿[·] =
𝑝
I𝑁𝛿[·] =I𝑁𝛿[·], (40)
𝑛=1 𝑛=1
whichconfirmsthatT∗ =T+.
H H22 MichaelUnserandStanislasDucotterd
4.4 GeneralizedShiftComposedwithaTightFrame
Withtheviewofextending(36)tovector-valuedsignals,weintroducethegeneralized
shift(orscrambling)operatorS𝑲 : ℓ𝑁(Z𝑑) → ℓ𝑁(Z𝑑),withtranslationparameter
2 2
𝑲 = (𝒌 1,...,𝒌𝑁) ∈Z𝑑×𝑁 ,as
S𝒌1{𝑥 1[·]} 𝑥 1[·−𝒌 1]
S𝑲{x[·]} =(cid:169) (cid:173) . . . (cid:170) (cid:174)=(cid:169) (cid:173) . . . (cid:170) (cid:174). (41)
(cid:173) (cid:174) (cid:173) (cid:174)
S𝒌𝑁{𝑥 𝑁[·]} 𝑥 𝑁[·−𝒌𝑁]
(cid:171) (cid:172) (cid:171) (cid:172)
Itisthemultichannelextensionofthescalarshiftby𝒌
0
∈Z𝑑 denotedbyS𝒌0 :𝑥[·] ↦→
𝑥[·− 𝒌 ]. The generalized shift is obviously LSI-Parseval and has the convenient
0
semigrouppropertyS𝑲0S𝑲 =S(𝑲0+𝑲) :ℓ𝑁(Z𝑑) →ℓ𝑁(Z𝑑)withS𝑲−𝑲 =S0𝑁 =Id,
2 2
forany𝑲,𝑲 ∈Z𝑑×𝑁 .ThisisparalleltothescalarsettingwherewehavethatS0 =Id
0
andS𝒌0S𝒌 =S(𝒌0+𝒌) :ℓ 2(Z𝑑) →ℓ 2(Z𝑑) forany 𝒌,𝒌
0
∈ Z𝑑 ,sothat (S𝒌0)−1 =S−𝒌0
withallshiftoperatorsbeingunitary.
Now,letA= (cid:2) a
1
··· a𝑁(cid:3) = (cid:2) b
1
··· b𝑀(cid:3)T ∈R𝑀×𝑁 with𝑀 ≥ 𝑁 bearectangu-
larmatrixsuchthatATA=I𝑁 (tight-frameproperty).Thegeometryissuchthatthe
columnvectors {a𝑛} 𝑛𝑁
=1
formanorthonormalfamilyinR𝑀 (butnotabasisunless
𝑀 = 𝑁), while the row vectors {b𝑚} 𝑚𝑀
=1
form a 1-tight frame of R𝑁 that is the
redundant counterpart of an ortho-basis. Here too, the defining property is energy
conservation: (cid:205) 𝑚=1|⟨b𝑚,x⟩|2 = ⟨Ax,Ax⟩ = ⟨ATAx,x⟩ = ∥x∥2
2
for all x ∈ R𝑁
(Parseval),albeitinthesimplerfinite-dimensionalsetting.
Given such a tight-frame matrix A ∈ R𝑀×𝑁 and a set of shift indices 𝑲 =
(𝒌 1,...,𝒌𝑁) ∈Z𝑑×𝑁 ,wethenspecifytheoperator
T =Mult ◦S𝑲 =AS𝑲 :ℓ𝑁(Z𝑑) →ℓ𝑀(Z𝑑). (42)
H A 2 2
The matrix-valued frequency response of this filter is H(cid:98)(𝝎) = Adiag(e−j⟨𝝎,𝒌1⟩,
...,e−j⟨𝝎,𝒌𝑁⟩),whichisparaunitary,irrespectivelyofthechoiceoftheshifts 𝒌𝑚.
4.5 𝑵-to-𝑵 ParsevalFilters
WeknowfromProposition3thatT
:ℓ𝑁(Z𝑑) →ℓ𝑁(Z𝑑)isaParsevalmulti-filterif
H 2 2
andonlyifH(cid:98)H(𝝎)H(cid:98)(𝝎) =I𝑁forall𝝎 ∈T𝑑 .Bytakinginspirationfromthesingular-
valuedecomposition,thissuggeststheconsiderationofparaunitaryelementsofthe
form:U𝚲(cid:98)(𝝎),𝚲(cid:98)(𝝎)VH,orU𝚲(cid:98)(𝝎)VH,whereU = (cid:2) u
1
··· u𝑁(cid:3) ∈ C𝑁×𝑁 andV =
(cid:0) v
1
... v𝑁(cid:1) ∈ C𝑁×𝑁 are unitary matrices, and 𝚲(cid:98)(𝝎) = diag(cid:0)𝜆ˆ 1(𝝎),...,𝜆ˆ 𝑁(𝝎)(cid:1)
wherethe𝜆ˆ 𝑛(𝝎)areall-passfilterswith|𝜆ˆ 𝑛(𝝎)| =1forall𝝎 ∈T𝑑 .
Since our focus is on FIR filters, we invoke Proposition 4 to deduce that the
only acceptable form of diagonal matrix is 𝚲(cid:98)(𝝎) = diag(e−j⟨𝝎,𝒌1⟩,...,e−j⟨𝝎,𝒌𝑁⟩)ParsevalConvolutionOperatorsandNeuralNetworks 23
with shift parameter (𝒌 1,...,𝒌𝑁) = K ∈ Z𝑑×𝑁 , which is precisely the frequency
response of the generalized shift operator S𝑲. The resulting parametric Parseval
operatorsareUS𝑲,S𝑲VH,andUS𝑲VH.Theimpulseresponsesofthesefiltersare
sumsofrank-1elementswiththeirsupportbeingspecifiedbythe 𝒌𝑚,whichneed
notbedistinct.Specifically,wehavethat
𝑁
US𝑲VH =T
H
:ℓ 2𝑁(Z𝑑) →ℓ 2𝑁(Z𝑑) with H[·] =∑︁ u𝑛vH 𝑛𝛿[·−𝒌𝑛], (43)
𝑛=1
which encompasses the two lighter filter variants by taking U or V equal to I𝑁 =
(cid:2) (cid:3)
e
1
··· e𝑁 .Anothercanonicalconfigurationof (43)isobtainedbytakingU = V
which,asweshallsee,makesaninterestingconnectionwithtwoclassicfactorization
ofparaunitarysystems.WhiletheoperatorUS𝑲VH isobviouslyageneralizationof
US𝑲,thereiscomputationalmeritwiththelighterversion,especiallyinthecontext
ofcomposition.
Proposition5 Let W 1,...,W𝑀+1 ∈ C𝑁×𝑁 and U 1,...,U𝑀+1 ∈ C𝑁×𝑁 be two
series of orthogonal matrices, and 𝑲 1,...,𝑲𝑀 ∈ Z𝑑×𝑁 some corresponding shift
indices.Then,thecomposedparametricoperators
W𝑀+1S𝑲𝑀W𝑀···W 3S𝑲2W 2S𝑲1W
1
(44)
and
UH 𝑀+1(U𝑀S𝑲𝑀UH 𝑀)···(U 2S𝑲2UH 2)(U 1S𝑲1UH 1) (45)
spanthesamefamilyof𝑁-to-𝑁 Parsevalmulti-filters.
Indeed,bysettingW
1
= UH 1,W
2
= UH 2U 1,...,W𝑀 = UH 𝑀U𝑀−1 andW𝑀+1 =
UH 𝑀+1U𝑀,wecanuse(44)toreplicate(45).Again,thekeyisthatthemultiplication
(composition) of two unitary matrices yields another unitary matrix. Conversely,
(45) reproduces (44) if we set UH = W , UH = W UH = W W ,..., UH =
1 1 2 2 1 2 1 𝑀
W𝑀−1UH
𝑁−1
=W𝑀−1W𝑀−2···W 1,andUH
𝑀+1
=W𝑀UH
𝑁
=W𝑀···W 1.
4.6 Projection-basedParsevalFilterbanks
These 𝑁-to-𝑁 filterbanks are parameterized by a projection matrix P. They are
multi-dimensional adaptations of classic canonical structures that were introduced
by Vaidyanathan and others for the factorization of paraunitry matrices for 𝑑 = 1
[41].Toexplaintheconcept,werecallthatamatrixPisamemberofP(𝑁,𝑘) (the
setofallorthogonalprojectionmatricesinR𝑁 ofrank𝑘)ifandonlyifitfulfilsthe
followingconditions:
1. Rank:P∈R𝑁×𝑁 withrank(P) = 𝑘.
2. Idempotence:PP=P.24 MichaelUnserandStanislasDucotterd
3. Symmetry: PT = P, which together with Item 2, implies that P is an ortho-
projector.
AnyP∈P(𝑁,𝑘)canbeparameterizedasP=(cid:205) 𝑛𝑘 =1u𝑛uT
𝑛
=Proj span(u1,...,u𝑘),where
u 1,...,u𝑘 is a set of orthogonal vectors in R𝑁 . Since P is an ortho-projector, it
inducesthedirect-sumdecompositionR𝑁 =Ran(P)⊕Ker(P),wherethemembers
ofRan(P) areeigenvectorsofPwitheigenvalue1(projectionproperty),whilethe
membersofKer(P) =Ran(P)⊥areeigenvectorswitheigenvalue0.Theparametriza-
tionofPthensimplyfollowsfromtheSVD,withthevectorsu 1,...,u𝑘 beingany
setoforthogonalmembersofRan(P).Inparticular,therank-1ortho-projectorsare
parameterizedbyasingleunitvector,withP(𝑁,1) = {uuT ∈ R𝑁×𝑁 s.t.∥u∥ = 1}.
2
Finally, two projection matrices P ∈ P(𝑁,𝑘) and(cid:101)P ∈ P(𝑁,𝑁 − 𝑘) are said to be
complementary if P+(cid:101)P = I𝑁. In fact, P𝑘 ∈ P(𝑁,𝑘) has a single complementary
projectorthatisgivenbyI𝑁 −P∈P(𝑁,𝑁 −𝑘).
A basic FIR-Parseval projection element is characterized by a matrix impulse
responseoftheform
H P,𝒌1[·] = (I𝑁 −P)𝛿[·]+P𝛿[·−𝒌 1], (46)
whereP ∈ R𝑁×𝑁 isaprojectionmatrixand 𝒌 ∈ [−1,1]𝑑\{0}issomeelementary
1
(multidimensional)unitshift.WeshallrefertosuchastructurebyPROJ-𝑘,with 𝑘
beingtheranktheprojector.TheimpulseresponseofaPROJ-1elementis
H u,𝒌1[·] = (I𝑁 −uuT)𝛿[·]+uuT𝛿[·−𝒌 1] (47)
which, similarly to a Householder matrix, can be parameterized by a single unit
vectoru.MoregenerallyforPROJ-𝑘,theconditionP ∈ P(𝑁,𝑘) translatesintothe
existenceofanortho-matrixU= [u 1···u𝑁] ∈R𝑁×𝑁 suchthatP=(cid:205) 𝑛𝑘 =1u𝑛uT 𝑛and
(I𝑁 −P) = (cid:205) 𝑛𝑁 =𝑘+1u𝑛uT 𝑛. This then allows us to express the convolution operator
x co[· n] sis↦→
ts
o( fH tP w,𝒌 o1 s∗ hx if) t[ s·] ona ls y:T 𝒌HP,𝒌 r1 ep= eatU edS𝑲 𝑘1U tiH mew si ,th ana ds 0cr (a zm erb ol )in fg orm that eri rx em𝑲 a1 int ih na gt
1
entries.Consequently,(46)and(47)aretwospecialcasesof (43),whichconfirms
theirParsevalproperty.
Whilethe 𝑁-to-𝑁 schemewepresentedinSection4.5ismoregeneralandalso
suggests some natural computational streamlining (see Proposition 5), arguments
canbemadeinfavoroftheuseofPROJ-𝑘 filteringcomponents,eachofwhichhas
aminimalsupportofsize2.
Thestrongestargumentistheoreticalbutonlyholdsfor𝑑 = 1[19].Specifically,
ithasbeenshownthatallParsevalfiltersofafixedMcMillandegree(i.e.,thetotal
number of delays required to implement the filterbank) admit a factorization in
termsofProj-1elements[41].Likewise,anyfilterbankwithfiltersoffixedsupport
𝑀 admits a factorization in terms of Proj-𝑘 elements, which ensures that such a
parametrizationiscomplete[48].Thetrickypartinthislattertypeoffactorization
isthatitalsorequirestheadjustmentoftherank𝑘 𝑖 ofeachcomponent.
Unfortunately,suchresultsdonotgeneralizetohigherdimensionsbecauseofthe
lack of a general polynomial factorization theorem for 𝑑 > 1. Simply stated, thisParsevalConvolutionOperatorsandNeuralNetworks 25
means that there are many multidimensional filters that cannot be realized from a
compositionofelementaryfiltersofsize2.For 𝑑 = 1,theelementaryshiftin(46)
and(47)issetto 𝑘 = 1,butitisnotclearhowtoproceedsystematicallyinhigher
1
dimensions.
Inthecontextofaconvolutionalneuralnetworkwheremanydesignchoicesare
adhoc,thelackofguaranteeofcompletenessamongallParsevalfiltersofsize 𝑀
(onearbitraryfamilyoffiltersamongmanyothers)isnotparticularlytroublesome.
Themoreimportantissueistobeabletoexploittheavailabledegreesoffreedomby
adjustingtheparametersforbestperformanceduringthetrainingprocedure.Thisis
achievedeffectivelyfor 𝑑 = 2intheblock-convolutionorthogonalparametrization
(BCOP) framework [30], which relies on the composition of PROJ-𝑘 with 𝒌 ∈
1
{(0,1),(1,0)} (inalternation).Byformulatingthetrainingproblemwithtwicethe
numberofchannels(halfofwhicharedummyandconstrainedtohavezerooutput)
with P ∈ P(2𝑁,𝑁), the authors are also able to optimally adjust the parameter 𝑘
(rankoftheprojector)foreachunit.
5 ApplicationtoDenoisingandImageReconstruction
WenowdiscusstheapplicationofParsevalfilterbankstobiomedicalimagerecon-
struction.Specifically,weshallrelyon1-LipschitzneuralnetworksthatuseParseval
convolution layers and that are trained for the denoising of a representative set of
images.
Dependingonthecontext,theimagetobereconstructedisdescribedasasignal
𝑠[·] ∈ ℓ (Z𝑑) or as the vector s = (𝑠[𝒌]) ∈ R𝐾 , where Ω ⊂ Z𝑑 is a region
2 𝒌∈Ω
of interest composed of a finite number 𝐾 of pixels. Our computational task is to
recovers∈R𝐾
fromthenoisymeasurementvector
y=As+n∈R𝑀,
(48)
wherenissome(unknown)noisecomponentandwhereA ∈ R𝑀×𝐾 isthesystem
matrix that models the physics of the acquisition process. A simplified version of
(48)with𝑀 =𝐾andA=I(identity)isthebasicdenoisingproblem,wherethetask
istorecoversfromthenoisysignal
z=s+n∈R𝐾.
(49)
5.1 FromVariationaltoIterativePlug-and-PlayReconstruction
Tomakesignal-recoveryproblemswell-posedmathematically,oneusuallyincorpo-
ratespriorknowledgeabouttheunknownimagesbyimposingregularityconstraints
onthesolution.Thisleadstothevariationalreconstruction26 MichaelUnserandStanislasDucotterd
s∗ =argmin(𝐽(y,As)+𝑅(s)), (50)
s∈R𝐾
where𝐽: R𝑀×R𝑀 →R+isadata-fidelitytermand𝑅: R𝐾 →R+isaregularization
functional that penalizes “non-regular” solutions. If s ↦→ 𝐽(y,As) is differentiable
and𝑅isconvex,then(50)canbesolvedbytheiterativeforward-backwardsplitting
(FBS)algorithm[14]with
s𝑘+1 =prox(cid:8) s𝑘 −𝛼∇∇∇𝐽(y,As𝑘)(cid:9).
(51)
𝛼𝑅
Here,∇∇∇𝐽(y,As) isthegradientof 𝐽 withrespecttos, 𝛼 ∈ Risthestepsizeofthe
update,andprox istheproximaloperatorof𝑅definedas
𝑅
(cid:16) (cid:17)
prox{z} =argmin 1∥z−s∥2+𝑅(s) . (52)
2
𝑅 s∈R𝐾
Animportantobservationisthatprox
:R𝐾 →R𝐾
actuallyreturnsthesolutionof
𝑅
thedenoisingproblemwithavariationalformulationthatisaparticularcaseof(50)
withA=Iandthequadraticdataterm𝐽(z,s) = 1∥z−s∥2.
2
The philosophy of PnP algorithms [52] is to replace prox with an off-the-
𝛼𝑅
shelf denoiser D: R𝐾 → R𝐾 . While not necessarily corresponding to an explicit
regularizer 𝑅,thisapproachhasledtoimprovedresultsinimagereconstruction,as
shownin[40,44,57].TheconvergenceofthePnP-FBSiterations
s𝑘+1 =D(cid:8) s𝑘 −𝛼∇∇∇𝐽(y,As𝑘)(cid:9)
(53)
canbeguaranteed[21,Proposition15]if
• thedenoiserDisaveraged,whichmeansthatistakestheformD= 𝛽R+(1−𝛽)Id
with 𝛽 ∈ (0,1)andanoperatorR:R𝐾 →R𝐾 suchthatLip(R) ≤ 1;
• the data term 𝐽(y,H·) is convex, differentiable with 𝐿-Lipschitz gradient, and
𝛼 ∈ (0,2/𝐿).
Moreover, it is possible to prove that the solution(s) of the PnP algorithm satisfies
thepropertiesexpectedofafaithfulreconstruction.Thefirstsuchpropertyisajoint
formofconsistencybetweenthereconstructedimages∗(outcomeofthealgorithm)
andthemeasurementy(input).
Proposition6 Let s∗ and s∗ be fixed points of (53) for measurements y and y ,
1 2 1 2
respectively.IftheoperatorDisaveragedwith𝛽 ≤ 1/2and𝐽(y,As) = 1∥y−As∥2,
2 2
thenitholdsthat
∥As∗−As∗∥ ≤ ∥y −y ∥. (54)
1 2 1 2
Proof. IfDis 𝛽-averagedwith 𝛽 ≤ 1/2,then(2D−Id)is1-Lipschitzsince
∥(2D−Id){z −z }∥ = ∥2𝛽(R{z }−R{z }+(1−2𝛽)(z −z )∥
1 2 1 2 1 2
≤ 2𝛽∥R{z }−R{z }∥+(1−2𝛽)∥z −z ∥
1 2 1 2
≤ ∥z −z ∥, ∀z ,z ∈R𝐾. (55)
1 2 1 2ParsevalConvolutionOperatorsandNeuralNetworks 27
Usingthisproperty,wegetthat
∥(2D−Id){s∗−𝛼∇∇∇𝐽(As∗,y )}−(2D−Id){s∗−𝛼∇∇∇𝐽(As∗,y )}∥
1 1 1 2 2 2
≤ ∥(s∗−𝛼∇∇∇𝐽(As∗,y ))−(s∗−𝛼∇∇∇𝐽(As∗,y ))∥. (56)
1 1 1 2 2 2
Fromthefixed-pointpropertyofs∗ ands∗,itfollowsthat
1 2
∥2(s∗−s∗)−(s∗−𝛼∇∇∇𝐽(As∗,y ))+(s∗−𝛼∇∇∇𝐽(As∗,y ))∥
1 2 1 1 1 2 2 2
≤ ∥(x∗−𝛼∇∇∇𝐽(As∗,y ))−(s∗−𝛼∇∇∇𝐽(As∗,y ))∥. (57)
1 1 1 2 2 2
Next,weusethefactthat∇∇∇𝐽(As,y) =AT(As−y)anddevelopbothsidesas
⟨s∗−s∗,AT(As∗−y )−AT(As∗−y )⟩ ≥ 0. (58)
1 2 2 2 1 1
Finally,wemoveAT totheothersideoftheinnerproductandinvoketheCauchy-
Schwartzinequalitytogetthat
∥A(s∗−s∗)∥∥y −y ∥ ≥ ⟨A(s∗−s∗),y −y ⟩ ≥ ∥A(s∗−s∗)∥2, (59)
1 2 1 2 1 2 1 2 1 2
whichisequivalentto(54). ⊓⊔
WhenAisinvertible,(54)yieldsthedirectrelation
1
∥s∗−s∗∥ ≤ ∥y −y ∥. (60)
1 2 𝜎 (ATA) 1 2
min
ItensuresthattheiterativereconstructionalgorithmisitselfgloballyLipschitzstable.
Inotherwords,asmalldeviationoftheinputcanonlyresultinalimiteddeviation
oftheoutput,whichintrinsicallyprovidesprotectionagainsthallucinations.Under
slightlystrongerconstraintsonD,wehaveacomparableresultfornon-invertibleA.
Proposition7 InthesettingofProposition6andfora𝐿 -LipschitzdenoiserDwith
0
𝐿 <1,itholdsthat
0
𝛼∥A∥𝐿
∥s∗−s∗∥ ≤ 0∥y −y ∥. (61)
1 2 1−𝐿 1 2
0
Proof.
∥s𝑘 −s𝑘∥ = ∥D{s𝑘−1−𝛼AT(As𝑘−1−y )}−D{s𝑘−1−𝛼AT(As𝑘−1−y )}∥
1 2 1 1 1 2 2 2
≤ 𝐿 ∥(I−𝛼ATA)(s𝑘−1−s𝑘−1)−𝛼AT(y −y )∥
0 1 2 1 2
≤ 𝐿 ∥s𝑘−1−s𝑘−1∥+𝛼𝐿 ∥A∥∥y −y ∥
0 1 2 0 1 2
≤ 𝐿2∥s𝑘−2−s𝑘−2∥+𝛼∥A∥(𝐿 +𝐿2)∥y −y ∥
0 1 2 0 0 1 2
𝑘
≤ 𝐿𝑘∥s0−s0∥+𝛼∥A∥∥y −y ∥∑︁ 𝐿𝑛. (62)
0 1 2 1 2 0
𝑛=128 MichaelUnserandStanislasDucotterd
Takingthelimit𝑘 →∞,wegetthat∥s∗−s∗∥ ≤ 𝛼∥A∥𝐿 0∥y −y ∥. ⊓⊔
1 2 1−𝐿 0 1 2
Sinceitisformulatedasadatafittingproblem,thereconstruction(50)generally
has better data consistency than the one provided by end-to-end neural-network
frameworksthatdirectlyreconstructsfromy[24,34,55,31].Thoselatterapproaches
arealsoknowntosufferfromstabilityissues[3].Moreimportantly,theyhavebeen
foundtoremoveorhallucinatestructure[38,37],whichisunacceptableindiagnostic
imaging.TheusageofempiricalPnPmethodswithoutstrictLipschitzcontrolwithin
the loop is also subject to caution, as they do not offer any guarantee of stability.
By contrast, the PnP approach (53) with averagedness constraints comes with the
stability bounds (54), (60) and (61). This is a step toward reliable deep-learning-
based image reconstruction as it intrinsically limits the ability of the method to
overfitandtohallucinate.
5.2 LearninganAveragedDenoiserforPnP
Ourapproachtoimproveuponclassicimagereconstructionistolearntheoperator
D= 𝛽R+(1−𝛽)Idin(53).Wepretrainitforthebestperformanceinthedenoising
scenario(49).Tothatend,weimposethestructureofthe1-LipLSIoperatorRasan
𝐿-layer convolutional neural network with all intermediate layers being composed
of the same number (𝑁) of feature channels. Specifically, by reverting back to the
notationofSection3,wehavethatR:ℓ (Z𝑑) →ℓ (Z𝑑)with
2 2
R=T
H𝐿
◦𝝈𝐿 ◦T
H𝐿−1
◦𝝈𝐿−1◦...◦T
H2
◦𝝈 2◦T H1, (63)
whereT
H𝑘
areLSIoperatorswithmatrix-valuedimpulseresponseH𝑘[·] and𝝈𝑘 =
(𝜎 𝑘,1,...,𝜎 𝑘,𝑁)arepointwisenonlinearitieswiththesharedactivationprofile𝜎 𝑘,𝑛 :
R → Rwithineachfeaturechannel.Asforthedomainandrangeoftheoperators,
we have that T : ℓ (Z𝑑) → ℓ𝑁(Z𝑑) and𝑇 : ℓ𝑁(Z𝑑) → ℓ (Z𝑑) for the input
H1 2 2 H𝐿 2 2
andoutputlayers,while𝑇 :ℓ𝑁(Z𝑑) →ℓ𝑁(Z𝑑)for𝑘 =2,...,(𝐿−1).Likewise,
H𝑘 2 2
𝝈𝑘 :ℓ𝑁(Z𝑑) →ℓ𝑁(Z𝑑),withtheeffectofthenonlinearlayerbeingdescribedby
2 2
∀𝒌 ∈Z𝑑 :
𝝈𝑘(cid:169) (cid:173)𝑥 1
. .
.[·]
(cid:170)
(cid:174)
[𝒌] =(cid:169) (cid:173)
𝜎 𝑘,1(𝑥
. .
.1[𝒌])
(cid:170) (cid:174) (64)
(cid:173)
𝑥
𝑁[·](cid:174) (cid:173)
𝜎 𝑘,𝑁(𝑥
𝑁[𝒌])(cid:174)
(cid:171) (cid:172) (cid:171) (cid:172)
withactivationfunctions𝜎
𝑘,𝑛
:R→Rfor𝑛=1,...,𝑁.
SincetheLipschitzconstantofthecompositionoftwooperatorsisboundedby
theproductoftheirindividualLipschitzconstant,wehavethat
𝐿
(cid:214)
Lip(R) ≤ Lip(T H1) Lip(𝝈𝑘)Lip(T H𝑘), (65)
𝑘=2ParsevalConvolutionOperatorsandNeuralNetworks 29
whichmeansthatwecanensurethatLip(R) ≤ 1byconstrainingeach𝝈𝑘 andT
H𝑘
tobe1-Lipschitz.
5.2.1 Specificationof1-LipConvolutionLayers
WeconsidertwowaysofenforcingLip(T ) =1.Botharesupportedbyourtheory.
H𝑘
• Spectral normalization (SN) [40]: During the learning process, we repeatedly
renormalize the denoising filters H𝑘 by dividing them by their spectral norm
Lip(T H𝑘) = ∥T H𝑘∥ =𝜎 sup,H𝑘 (seeTheorem2).
• BCOP[30]:TheParsevalfiltersT areparameterizedexplicitlyusingorthogonal
H𝑘
matricesU𝑘 ∈ R𝑁×𝑁 ,asdescribedinSections4.5-4.6.Weusetheimplementa-
tionprovidedbytheBCOPframeworkofLietal.Asforthelast𝑁-to-1multifilter
T ,itisnotliterallyParseval,butrathertheadjointofaParsevaloperator,which
H𝐿
preservesthe1-Lippropertyaswell.
5.2.2 Specificationof1-LipActivationFunctions
TheLipschitzconstantofanonlinearscalaractivation 𝑓 :R→Risgivenby
d𝑓(𝑡)|
Lip(𝑓) =sup| |. (66)
𝑡∈R
d𝑡
Thisresultcanthenbeappliedtothefullnonlinearlayer𝝈𝑘 : ℓ𝑁(Z𝑑) → ℓ𝑁(Z𝑑)
2 2
throughthepoolingformula(68).
Proposition8 Let 𝒇 : ℓ𝑁(Z𝑑) → ℓ𝑁(Z𝑑) beagenericpointwisenonlinearmap-
2 2
pingspecifiedby
𝑓 𝒌,1(𝑥 1[𝒌])
𝒇(cid:8) x[·](cid:9) [𝒌] =(cid:169) (cid:173) . . . (cid:170) (cid:174), 𝒌 ∈Z𝑑 (67)
(cid:173) (cid:174)
𝑓 𝒌,𝑁(𝑥 𝑁[𝒌])
(cid:171) (cid:172)
Then, 𝒇 :ℓ𝑁(Z𝑑) →ℓ𝑁(Z𝑑)isLipschitzcontinuousifandonlyifallthecomponent-
2 2
wise transformations 𝑓 𝒌,𝑛 : R → R, with (𝒌,𝑛) ∈ Z𝑑 ×{1,...,𝑁} are Lipschitz-
continuous.ItsLipschitzconstantisthengivenby
Lip(𝒇) = 𝐿
𝒇
= sup Lip(𝑓 𝒌,𝑛) <∞. (68)
(𝒌,𝑛)∈Z𝑑×{1,...,𝑁}
Proof. Undertheassumptionthatthe 𝑓 𝒌,𝑛 areLipschitzcontinuous,foranyx,y ∈
ℓ𝑁(Z𝑑)wehavethat
230 MichaelUnserandStanislasDucotterd
𝑁
∥𝒇{y}− 𝒇{x}∥ ℓ2
𝑁
=∑︁ ∑︁ (cid:12) (cid:12)𝑓 𝒌,𝑛(𝑦 𝑛[𝒌])− 𝑓 𝒌,𝑛(𝑥 𝑛[𝒌])(cid:12) (cid:12)2
2 𝑛=1𝒌∈Z𝑑
𝑁
≤ ∑︁ ∑︁ Lip(𝑓 𝒌,𝑛)2(cid:12) (cid:12)𝑦 𝑛[𝒌]−𝑥 𝑛[𝒌](cid:12) (cid:12)2
𝑛=1𝒌∈Z𝑑
(cid:32) (cid:33)2
≤ sup Lip(𝑓 𝒌,𝑛) ∥y−x∥ ℓ2 𝑁,
(𝒌,𝑛)∈Z𝑑×{1,...,𝑁} 2
which proves that Lip(𝒇) ≤ 𝐿 . From the definition of the supremum, for any
𝒇
𝜖 > 0, there exists some (𝒌 ,𝑛 ) ∈ Z𝑑 × {1,...,𝑁} such that Lip(𝒇) ≤ 𝐿 ≤
0 0 𝒇
(1 + 𝜖)𝐿 0 with 𝐿 0 = Lip(𝑓 𝒌0,𝑛 0). Likewise, since 𝑓 = 𝑓 𝒌0,𝑛 0 : R → R is 𝐿 0-
Lipschitz continuous, for any 𝜖′ > 0 there exist some 𝑥,𝑦 ∈ R with 𝑥 ≠ 𝑦 such
that(1+𝜖′)|𝑓(𝑦)− 𝑓(𝑥)| ≥ 𝐿 |𝑦−𝑥|.Wethenconsiderthecorresponding(worst-
0
case) signals x˜ = 𝑥e𝑛 0𝛿[· − 𝒌 0] and y˜ = 𝑦 𝐿e𝑛 0𝛿[· − 𝒌 0], for which we have that
Lip(𝒇)∥y˜−x˜∥
ℓ𝑁
≥ ∥𝒇(y˜)− 𝒇(x˜)∥
ℓ𝑁
≥ (1+𝜖)(𝒇 1+𝜖′)∥y˜−x˜∥ ℓ𝑁.Since𝜖′and𝜖canbe
chosenarbitrari2 lysmall,theLipschi2 tzboundissharpwithL2 ip(𝒇) = 𝐿 .Thesame
𝒇
kind of worst-case signals can also be used to show the necessity of the Lipschitz
continuityofeach 𝑓 𝒌,𝑛 :R→R. ⊓⊔
Accordingly,inourexperiments,wehaveconsideredtwoconfigurations.
• Fixedactivationasarectifiedlinearunit(ReLU)withLip(ReLU) = ∥1 +∥𝐿
∞
=1.
• Learnable linear spline (LLS) [18], with learned activations 𝜎 𝑘,𝑛 : R → R s.t.
Lip(𝜎 𝑘,𝑛) = 1.Thesenonlinearitiesaresharedwithineachconvolutionchannel
(𝑘,𝑛) ∈ {2,...,𝐿}×{1,...,𝑁}.TheyareparameterizedusinglinearB-splines
subjecttoasecond-ordertotal-variationregularizationthatpromotescontinuous
piecewise-linearsolutionswiththefewestlinearsegments[5,50].
5.2.3 ImageDenoisingExperiments
Wetrain1-Lipdenoiserswith𝐿 =8,𝑁 =64,andfiltersofsize(3×3).Thetraining
datasetconsistsof238400patchesofsize (40×40) takenfromtheBSD500image
dataset[4].Allnoise-freeimagessin(49)arenormalizedtotakevaluesin [0,1].
TheyarethencorruptedwithadditiveGaussiannoiseofstandarddeviation𝜎totrain
the denoiser D for the regression task D{z} ≈ s. The performance on the BSD68
test set is provided in Table 2 for 𝜎 = 5/255,10/255. The general trend for each
experimental condition is the same: The Parseval filters parameterized by BCOP
consistentlyoutperformthe1-Lipfiltersobtainedbysimplespectralnormalization.
Thereisalsoasystematicbenefitintheutilizationoflearned1-Lipsplineactivations
(LLS),ascomparedtothestandardReLUdesign.ParsevalConvolutionOperatorsandNeuralNetworks 31
Table2:PSNRandSSIMonBSD68fortwonoiselevels.
Noiselevel 𝜎=5/255 𝜎=10/255
Metric PSNR SSIM PSNR SSIM
ReLU-SN 35.78 0.9297 31.48 0.8533
ReLU-BCOP 36.10 0.9386 31.92 0.8735
LLS-SN 36.68 0.9504 32.36 0.8883
LLS-BCOP 36.86 0.9546 32.55 0.8962
5.3 NumericalResultsforPnP-FBS
We now demonstrate the deployment of our learned denoisers in the PnP-FBS
algorithmforimagereconstruction.Tothatend,weselectthedata-fidelitytermas
𝐽(y,As) = 1∥y−As∥2, where the matrix A simulates the physics of biomedical
2 2
imageacquisitions[35].Toensuretheconvergenceof (53),weset 𝛼 = 1/∥ATA∥.
Our denoiser is defined as D = 𝛽R+ (1− 𝛽)Id, while the constant 𝛽 ∈ [0,1) and
the training noise level 𝜎 ∈ {5/255,10/255} are tuned for best performance. In
ourexperiments,wenoticedthatthebest 𝛽 isalwayslowerthan1/2,whichmeans
thatthemathematicalassumptionsforProposition6aremet.Wealsocompareour
reconstructionalgorithmswiththeclassictotal-variation(TV)method[7].
InourMRIexperiment,thegoalisrecoversfromy=MFs+n∈C𝑀 ,whereM
isasubsamplingmask(identitymatrixwithsomemissingentries),Fisthediscrete
Fourier-transformmatrix,andnisarealizationofacomplex-valuedGaussiannoise
characterizedby𝜎 fortherealandimaginaryparts.Weinvestigatedthree 𝑘-space
n
sampling schemes (random, radial, and Cartesian=uniform along the horizontal
direction),eachgivingrisetoaspecificsub-samplingmask.
Thereconstructionperformanceforvarious𝑘-spacesamplingconfigurationsand
designchoicesisreportedinTable3.Similarlytothedenoisingexperiment,BCOP
alwaysoutperformsSN,whileLLSbringsadditionalimprovements.OurCNN-based
methodsgenerallyperformbetterthanTV(standardreconstructionalgorithm),while
they essentially offer the same theoretical guarantees (consistency and stability)
[17].TheonlynotableexceptionistheTV-regularizedreconstructionofBrainwith
Cartesiansampling,whichisofbetterqualitythantheoneobtainedwithSN-ReLU.
The results for Brain and Bust with the Cartesian mask are shown in Figures 1
and2,respectively.InthelowerpanelofFigure1,weobservestripe-likestructures
in the zero-fill reconstruction. These are typical aliasing artifacts that result from
thesubsamplinginthehorizontaldirectioninFourierspace.Theyaresignificantly
reducedwiththehelpofTV(whichisroutinelyusedforthatpurpose)aswellasin
theLLS-BCOPreconstruction,whichoverallyieldsthebestvisualquality.32 MichaelUnserandStanislasDucotterd
Ground Truth Zero-filling TV ReLU-SN ReLU-BCOP LLS-SN LLS-BCOP
21.57 24.43 24.14 24.42 25.09 25.18
22.88 27.06 26.01 26.50 27.79 28.07
Fig.1:Groundtruth,zero-fillreconstructionHTy,andPnP-FBSreconstructionusing
severalnetworkparameterizationsontheBrainimagewiththeCartesianmask.
Lowerpanel:zoomofaregionofinterest.TheSNRisevaluatedwithrespecttothe
groundtruth(leftimage)andisoverlaidinwhite.
Ground Truth Zero-filling TV ReLU-SN ReLU-BCOP LLS-SN LLS-BCOP
23.44 27.69 27.77 28.02 28.48 28.86
21.73 26.58 26.31 26.55 27.09 27.61
Fig.2:Groundtruth,zero-fillreconstructionHTy,andPnP-FBSreconstructionusing
severalnetworkparameterizationsontheBustimagewiththeCartesianmask.
Lowerpanel:zoomofaregionofinterest.TheSNRisevaluatedwithrespecttothe
groundtruth(leftimage)andisoverlaidinwhite.
Table3:PSNRandSSIMfortheMRIreconstructionexperiment.
Subsamplingmask Random Radial Cartesian
Imagetype Brain Bust Brain Bust Brain Bust
Zero-filling 24.6827.3123.8525.1321.5723.44
TV 30.3732.2929.4631.5824.4327.69
ReLU-SN 32.4533.3630.9232.3324.1427.77
ReLU-BCOP 32.5333.6730.9332.7224.4228.02
LLS-SN 33.3434.3231.8233.3525.0928.48
LLS-BCOP 33.6134.6732.0933.7225.1828.86ParsevalConvolutionOperatorsandNeuralNetworks 33
6 Conclusion
Inthischapter,wehaveconductedasystematicinvestigationofmultichannelconvo-
lutionoperatorswithaspecialemphasisontheclassofLSIParsevaloperators.What
setstheParsevaloperatorsapartfromstandardfilterbanksistheirlosslessnature(en-
ergyconservation).Thismakesthemultra-stableandparticularlyeasytoinvertby
mere flow-graph transposition of the computational architecture. The other impor-
tantfeatureisthattheParsevalpropertyispreservedthroughcomposition.Formally,
thismeansthattheParsevalfilterbanksforma(non-commutative)operatoralgebra.
Onthemorepracticalside,thisenablestheconstructionofhigher-complexityfilters
through the chaining of elementary parametric modules, as exemplified in Section
4.
ThesepropertiesmakeParsevalfilterbanksespeciallyattractiveforthedesignof
robust(e.g,1-Lip)convolutionalnetworks.Wehavedemonstratedtheapplicationof
such Parseval CNNs for the reconstruction of biomedical images. We have shown
that the use of pre-trained Parseval filterbanks generally improves the quality of
iterative image reconstruction, while it offers the same mathematical guarantees
as the conventional “handcrafted” reconstruction schemes. The training of such
structures is straightforward—it is done before hand on a basic denoising task.
Furthertopicsofresearchinclude(i)theinvestigationandcomparisonofdifferent
factorizationschemeswiththeviewofidentifyingthemosteffectiveones,and(ii)
the determination of the performance limits of CNN-based approaches under the
mathematicalconstraintofstability/trustworthiness.
References
1. A. Aldroubi. Portraits of frames. Proceedings of the American Mathematical Society,
123(6):1661–1668,1995.
2. C.Anil,J.Lucas,andR.Grosse.SortingoutLipschitzfunctionapproximation.InProceedings
ofthe36thInternationalConferenceonMachineLearning,pages291–301.PMLR,May2019.
3. V.Antun,F.Renna,C.Poon,B.Adcock,andA.C.Hansen. Oninstabilitiesofdeeplearning
inimagereconstructionandthepotentialcostsofAI.ProceedingsoftheNationalAcademyof
Sciences,117(48):30088–30095,May2020.
4. P.Arbeláez,M.Maire,C.Fowlkes,andJ.Malik. Contourdetectionandhierarchicalimage
segmentation. IEEETransactionsonPatternAnalysisandMachineIntelligence,33(5):898–
916,2011.
5. P.Bohra,J.Campos,H.Gupta,S.Aziznejad,andM.Unser. Learningactivationfunctions
indeep(spline)neuralnetworks. IEEEOpenJournalofSignalProcessing,1:295–309,Nov.
2020.
6. H.Bolcskei,F.Hlawatsch,andH.Feichtinger. Frame-theoreticanalysisofoversampledfilter
banks. IEEETransactionsonSignalProcessing,46:3256–3268,1998.
7. A.Chambolle. Analgorithmfortotalvariationminimizationandapplications. Journalof
MathematicalImagingandVision,20(1-2):89–97,2004.
8. S. H. Chan, X. Wang, and O. A. Elgendy. Plug-and-play ADMM for image restoration:
Fixed-point convergence and applications. IEEE Transactions on Computational Imaging,
3(1):84–98,2016.34 MichaelUnserandStanislasDucotterd
9. A.ChebiraandJ.Kovacevic. Lappedtightframetransforms. InProc.IEEEInternational
ConferenceonAcoustics,SpeechandSignalProcessing,volume3,pages857–860,Honolulu,
HI,USA,2007.
10. O.Christensen.Framesandpseudo-inverses.JournalofMathematicalAnalysisandApplica-
tions,195(2):401–414,1995.
11. O.Christensen. AnIntroductiontoFramesandRieszBases. Birkhauser,2003.
12. P.G.Ciarlet.LinearandNonlinearFunctionalAnalysiswithApplications,volume130.SIAM,
2013.
13. M.Cisse,P.Bojanowski,E.Grave,Y.Dauphin,andN.Usunier.Parsevalnetworks:Improving
robustnesstoadversarialexamples. InProceedingsofthe34thInternationalConferenceon
MachineLearning,pages854–863.PMLR,July2017.
14. P.CombettesandV.Wajs.Signalrecoverybyproximalforward-backwardsplitting.Multiscale
Modeling&Simulation,4:1168–1200,2005.
15. Z.CvetkovicandM.Vetterli. Oversampledfilterbanks. IEEETransactionsonSignalPro-
cessing,46(5):1245–1255,May1998.
16. I.Daubechies. TenLecturesonWavelets. SocietyforIndustrialandAppliedMathematics,
Philadelphia,PA,1992.
17. P.delAguilaPla,S.Neumayer,andM.Unser. Stabilityofimage-reconstructionalgorithms.
IEEETransactionsonComputationalImaging,9:1–12,2023.
18. S. Ducotterd, A. Goujon, P. Bohra, D. Perdios, S. Neumayer, and M. Unser. Improving
Lipschitz-constrainedneuralnetworksbylearningactivationfunctions. JournalofMachine
LearningResearch,25(65):1–30,2024.
19. X. Gao, T. Nguyen, and G. Strang. On factorization of M-channel paraunitary filterbanks.
IEEETransactionsonSignalProcessing,49(7):1433–1446,July2001.
20. M.Hasannasab,J.Hertrich,S.Neumayer,G.Plonka,S.Setzer,andG.Steidl.Parsevalproximal
neuralnetworks.JournalofFourierAnalysisandApplications,26(4):PaperNo.59,31,2020.
21. J.Hertrich,S.Neumayer,andG.Steidl. ConvolutionalproximalneuralnetworksandPlug-
and-Playalgorithms. LinearAlgebraandItsApplications,631:203–234,2021.
22. L.Huang,L.Liu,F.Zhu,D.Wan,Z.Yuan,B.Li,andL.Shao.Controllableorthogonalization
intrainingDNNs.In2020IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages6428–6437,Seattle,WA,USA,June2020.IEEE.
23. T.Huster,C.-Y.J.Chiang,andR.Chadha. LimitationsoftheLipschitzconstantasadefense
againstadversarialexamples. InECMLPKDD2018Workshops,LectureNotesinComputer
Science,pages16–29,Cham,2019.SpringerInternationalPublishing.
24. K. H. Jin, M. T. McCann, E. Froustey, and M. Unser. Deep convolutional neural network
forinverseproblemsinimaging. IEEETransactionsonImageProcessing,26(9):4509–4522,
Sept.2017.
25. T.Kailath. LinearSystems,volume156. Prentice-HallEnglewoodCliffs,NJ,1980.
26. U.S.Kamilov,C.A.Bouman,G.T.Buzzard,andB.Wohlberg. Plug-and-playmethodsfor
integratingphysicalandlearnedmodelsincomputationalimaging:Theory,algorithms,and
applications. IEEESignalProcessingMagazine,40(1):85–97,Jan.2023.
27. J.KovacevicandA.Chebira. Anintroductiontoframes. FoundationsandTrendsinSignal
Processing,2(1):1–94,2007.
28. J.KovacevicandA.Chebira. Lifebeyondbases:Theadventofframes(PartI). IEEESignal
ProcessingMagazine,24:86–104,2007.
29. J.KovacevicandA.Chebira. Lifebeyondbases:Theadventofframes(PartII). IEEESignal
ProcessingMagazine,24:115–125,2007.
30. Q. Li, S. Haque, C. Anil, J. Lucas, R. Grosse, and J.-H. Jacobsen. Preventing gradient
attenuationinLipschitzconstrainedconvolutionalnetworks. AdvancesinNeuralInformation
ProcessingSystems,32:15390–15402,Dec.2019.
31. D.J.Lin,P.M.Johnson,F.Knoll,andY.W.Lui. ArtificialintelligenceforMRimagerecon-
struction:anoverviewforclinicians. JournalofMagneticResonanceImaging,53(4):1015–
1028,2021.
32. F.Luisier,T.Blu,andM.Unser. ImagedenoisinginmixedPoisson-Gaussiannoise. IEEE
TransactionsonImageProcessing,20(3):696–708,Mar.2011.ParsevalConvolutionOperatorsandNeuralNetworks 35
33. S.Mallat. AWaveletTourofSignalProcessing. AcademicPress,SanDiego,1998.
34. M.McCann,K.Jin,andM.Unser. Convolutionalneuralnetworksforinverseproblemsin
imaging—areview. IEEESignalProcessingMagazine,34(6):85–95,Nov.2017.
35. M.McCannandM.Unser. Biomedicalimagereconstruction:Fromthefoundationstodeep
neuralnetworks. FoundationsandTrendsinSignalProcessing,13(3):280–359,Dec.2019.
36. Y.Meyer. OndelettesetopérateursI:Ondelettes. Hermann,Paris,France,1990.
37. M.J.Muckley,B.Riemenschneider,A.Radmanesh,S.Kim,G.Jeong,J.Ko,Y.Jun,H.Shin,
D.Hwang,M.Mostapha,S.Arberet,D.Nickel,Z.Ramzi,P.Ciuciu,J.-L.Starck,J.Teuwen,
D. Karkalousos, C. Zhang, A. Sriram, Z. Huang, N. Yakubova, Y. W. Lui, and F. Knoll.
Resultsofthe2020fastMRIchallengeformachinelearningMRimagereconstruction. IEEE
TransactionsonMedicalImaging,40(9):2306–2317,Sept.2021.
38. G.NatarajandR.Otazo.Model-freedeepMRIreconstruction:Arobustnessstudy.InISMRM
WorkshoponDataSamplingandImage,2020.
39. A.V.Oppenheim,R.W.Schafer,andJ.R.Buck. Discrete-timeSignalProcessing. Prentice
Hall,UpperSaddleRiver,2ndedition,1999.
40. E.Ryu,J.Liu,S.Wang,X.Chen,Z.Wang,andW.Yin. Plug-and-playmethodsprovably
convergewithproperlytraineddenoisers. InInternationalConferenceonMachineLearning,
pages5546–5557.PMLR,2019.
41. A. Soman, P. Vaidyanathan, and T. Nguyen. Linear phase paraunitary filter banks: theory,
factorizations and designs. IEEE Transactions on Signal Processing, 41(12):3480–3496,
1993.
42. G.StrangandT.Nguyen. WaveletsandFilterBanks. Wellesley-Cambridge,Wellesley,MA,
1996.
43. J. Su, W. Byeon, and F. Huang. Scaling-up diverse orthogonal convolutional networks by
aparaunitaryframework. InProceedingsofthe39thInternationalConferenceonMachine
Learning,pages20546–20579.PMLR,June2022. ISSN:2640-3498.
44. Y.Sun,Z.Wu,X.Xu,B.Wohlberg,andU.S.Kamilov. Scalableplug-and-playADMMwith
convergenceguarantees. IEEETransactionsonComputationalImaging,7:849–863,2021.
45. T.Tran,R.deQueiroz,andT.Nguyen.Linear-phaseperfectreconstructionfilterbank:Lattice
structure,design,andapplicationinimagecoding. IEEETransactionsonSignalProcessing,
48:133–147,2000.
46. A.TrockmanandJ.Z.Kolter.OrthogonalizingconvolutionallayerswiththeCayleytransform.
InICLR,May2021.
47. R. Turcajová. Factorizations and construction of linear phase paraunitary filter banks and
highermultiplicitywavelets. NumericalAlgorithms,8(1):1–25,1994.
48. R.TurcajováandJ.Kautsky.Shiftproductsandfactorizationsofwaveletmatrices.Numerical
Algorithms,8(1):27–45,1994.
49. M.Unser. Textureclassificationandsegmentationusingwaveletframes. IEEETransactions
onImageProcessing,4(11):1549–1560,1995.
50. M.Unser. Arepresentertheoremfordeepneuralnetworks. JournalofMachineLearning
Research,20(110):1–30,2019.
51. P.P.Vaidyanathan. MultirateSystemsandFilterBanks. Prentice-Hall,EnglewoodCliffs,NJ,
1993.
52. S.V.Venkatakrishnan,C.A.Bouman,andB.Wohlberg.Plug-and-playpriorsformodelbased
reconstruction.In2013IEEEGlobalConferenceonSignalandInformationProcessing,pages
945–948,2013.
53. M.VetterliandJ.Kovacevic.WaveletsandSubbandCoding.PrenticeHall,EnglewoodCliffs,
NJ,1995.
54. M.Vetterli,J.Kovačević,andV.K.Goyal. FoundationsofSignalProcessing. Cambridge
UniversityPress,Cambridge,UK,2014.
55. G. Wang, J. C. Ye, and B. De Man. Deep learning for tomographic image reconstruction.
NatureMachineIntelligence,2(12):737–748,Dec.2020.
56. L.Xiao,Y.Bahri,J.Sohl-Dickstein,S.Schoenholz,andJ.Pennington. Dynamicalisometry
and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural
networks. InProceedingsofthe35thInternationalConferenceonMachineLearning,pages
5393–5402.PMLR,July2018.36 MichaelUnserandStanislasDucotterd
57. D.H.Ye,S.Srivastava,J.-B.Thibault,K.Sauer,andC.Bouman. Deepresiduallearningfor
model-basediterativeCTreconstructionusingPlug-and-Playframework. InIEEEInterna-
tionalConferenceonAcoustics,SpeechandSignalProcessing,pages6668–6672,2018.
58. K.Zhang,W.Zuo,Y.Chen,D.Meng,andL.Zhang. BeyondaGaussiandenoiser:Resid-
ual learning of deep CNN for image denoising. IEEE Transactions on Image Processing,
26(7):3142–3155,2017.
59. D.Zou,R.Balan,andM.Singh.OnLipschitzboundsofgeneralconvolutionalneuralnetworks.
IEEETransactionsonInformationTheory,66(3):1738–1759,2019.