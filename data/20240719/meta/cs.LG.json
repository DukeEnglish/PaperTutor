[
    {
        "title": "Random Latent Exploration for Deep Reinforcement Learning",
        "authors": "Srinath MahankaliZhang-Wei HongAyush SekhariAlexander RakhlinPulkit Agrawal",
        "links": "http://arxiv.org/abs/2407.13755v1",
        "entry_id": "http://arxiv.org/abs/2407.13755v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13755v1",
        "summary": "The ability to efficiently explore high-dimensional state spaces is essential\nfor the practical success of deep Reinforcement Learning (RL). This paper\nintroduces a new exploration technique called Random Latent Exploration (RLE),\nthat combines the strengths of bonus-based and noise-based (two popular\napproaches for effective exploration in deep RL) exploration strategies. RLE\nleverages the idea of perturbing rewards by adding structured random rewards to\nthe original task rewards in certain (random) states of the environment, to\nencourage the agent to explore the environment during training. RLE is\nstraightforward to implement and performs well in practice. To demonstrate the\npractical effectiveness of RLE, we evaluate it on the challenging Atari and\nIsaacGym benchmarks and show that RLE exhibits higher overall scores across all\nthe tasks than other approaches.",
        "updated": "2024-07-18 17:55:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13755v1"
    },
    {
        "title": "Multi-Label Learning with Stronger Consistency Guarantees",
        "authors": "Anqi MaoMehryar MohriYutao Zhong",
        "links": "http://arxiv.org/abs/2407.13746v1",
        "entry_id": "http://arxiv.org/abs/2407.13746v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13746v1",
        "summary": "We present a detailed study of surrogate losses and algorithms for\nmulti-label learning, supported by $H$-consistency bounds. We first show that,\nfor the simplest form of multi-label loss (the popular Hamming loss), the\nwell-known consistent binary relevance surrogate suffers from a sub-optimal\ndependency on the number of labels in terms of $H$-consistency bounds, when\nusing smooth losses such as logistic losses. Furthermore, this loss function\nfails to account for label correlations. To address these drawbacks, we\nintroduce a novel surrogate loss, multi-label logistic loss, that accounts for\nlabel correlations and benefits from label-independent $H$-consistency bounds.\nWe then broaden our analysis to cover a more extensive family of multi-label\nlosses, including all common ones and a new extension defined based on\nlinear-fractional functions with respect to the confusion matrix. We also\nextend our multi-label logistic losses to more comprehensive multi-label\ncomp-sum losses, adapting comp-sum losses from standard classification to the\nmulti-label learning. We prove that this family of surrogate losses benefits\nfrom $H$-consistency bounds, and thus Bayes-consistency, across any general\nmulti-label loss. Our work thus proposes a unified surrogate loss framework\nbenefiting from strong consistency guarantees for any multi-label loss,\nsignificantly expanding upon previous work which only established\nBayes-consistency and for specific loss functions. Additionally, we adapt\nconstrained losses from standard classification to multi-label constrained\nlosses in a similar way, which also benefit from $H$-consistency bounds and\nthus Bayes-consistency for any multi-label loss. We further describe efficient\ngradient computation algorithms for minimizing the multi-label logistic loss.",
        "updated": "2024-07-18 17:51:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13746v1"
    },
    {
        "title": "Optimistic Q-learning for average reward and episodic reinforcement learning",
        "authors": "Priyank AgrawalShipra Agrawal",
        "links": "http://arxiv.org/abs/2407.13743v1",
        "entry_id": "http://arxiv.org/abs/2407.13743v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13743v1",
        "summary": "We present an optimistic Q-learning algorithm for regret minimization in\naverage reward reinforcement learning under an additional assumption on the\nunderlying MDP that for all policies, the expected time to visit some frequent\nstate $s_0$ is finite and upper bounded by $H$. Our setting strictly\ngeneralizes the episodic setting and is significantly less restrictive than the\nassumption of bounded hitting time {\\it for all states} made by most previous\nliterature on model-free algorithms in average reward settings. We demonstrate\na regret bound of $\\tilde{O}(H^5 S\\sqrt{AT})$, where $S$ and $A$ are the\nnumbers of states and actions, and $T$ is the horizon. A key technical novelty\nof our work is to introduce an $\\overline{L}$ operator defined as $\\overline{L}\nv = \\frac{1}{H} \\sum_{h=1}^H L^h v$ where $L$ denotes the Bellman operator. We\nshow that under the given assumption, the $\\overline{L}$ operator has a strict\ncontraction (in span) even in the average reward setting. Our algorithm design\nthen uses ideas from episodic Q-learning to estimate and apply this operator\niteratively. Therefore, we provide a unified view of regret minimization in\nepisodic and non-episodic settings that may be of independent interest.",
        "updated": "2024-07-18 17:49:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13743v1"
    },
    {
        "title": "Dynamic Pricing in Securities Lending Market: Application in Revenue Optimization for an Agent Lender Portfolio",
        "authors": "Jing XuYung Cheng HsuWilliam Biscarri",
        "links": "http://arxiv.org/abs/2407.13687v1",
        "entry_id": "http://arxiv.org/abs/2407.13687v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13687v1",
        "summary": "Securities lending is an important part of the financial market structure,\nwhere agent lenders help long term institutional investors to lend out their\nsecurities to short sellers in exchange for a lending fee. Agent lenders within\nthe market seek to optimize revenue by lending out securities at the highest\nrate possible. Typically, this rate is set by hard-coded business rules or\nstandard supervised machine learning models. These approaches are often\ndifficult to scale and are not adaptive to changing market conditions. Unlike a\ntraditional stock exchange with a centralized limit order book, the securities\nlending market is organized similarly to an e-commerce marketplace, where agent\nlenders and borrowers can transact at any agreed price in a bilateral fashion.\nThis similarity suggests that the use of typical methods for addressing dynamic\npricing problems in e-commerce could be effective in the securities lending\nmarket. We show that existing contextual bandit frameworks can be successfully\nutilized in the securities lending market. Using offline evaluation on real\nhistorical data, we show that the contextual bandit approach can consistently\noutperform typical approaches by at least 15% in terms of total revenue\ngenerated.",
        "updated": "2024-07-18 17:42:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13687v1"
    },
    {
        "title": "Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review",
        "authors": "Masatoshi UeharaYulai ZhaoTommaso BiancalaniSergey Levine",
        "links": "http://arxiv.org/abs/2407.13734v1",
        "entry_id": "http://arxiv.org/abs/2407.13734v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13734v1",
        "summary": "This tutorial provides a comprehensive survey of methods for fine-tuning\ndiffusion models to optimize downstream reward functions. While diffusion\nmodels are widely known to provide excellent generative modeling capability,\npractical applications in domains such as biology require generating samples\nthat maximize some desired metric (e.g., translation efficiency in RNA, docking\nscore in molecules, stability in protein). In these cases, the diffusion model\ncan be optimized not only to generate realistic samples but also to explicitly\nmaximize the measure of interest. Such methods are based on concepts from\nreinforcement learning (RL). We explain the application of various RL\nalgorithms, including PPO, differentiable optimization, reward-weighted MLE,\nvalue-weighted sampling, and path consistency learning, tailored specifically\nfor fine-tuning diffusion models. We aim to explore fundamental aspects such as\nthe strengths and limitations of different RL-based fine-tuning algorithms\nacross various scenarios, the benefits of RL-based fine-tuning compared to\nnon-RL-based approaches, and the formal objectives of RL-based fine-tuning\n(target distributions). Additionally, we aim to examine their connections with\nrelated topics such as classifier guidance, Gflownets, flow-based diffusion\nmodels, path integral control theory, and sampling from unnormalized\ndistributions such as MCMC. The code of this tutorial is available at\nhttps://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq",
        "updated": "2024-07-18 17:35:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13734v1"
    }
]