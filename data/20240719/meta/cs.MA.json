[
    {
        "title": "Matching-Driven Deep Reinforcement Learning for Energy-Efficient Transmission Parameter Allocation in Multi-Gateway LoRa Networks",
        "authors": "Ziqi LinXu ZhangShimin GongLanhua LiZhou SuBo Gu",
        "links": "http://arxiv.org/abs/2407.13076v1",
        "entry_id": "http://arxiv.org/abs/2407.13076v1",
        "pdf_url": "http://arxiv.org/pdf/2407.13076v1",
        "summary": "Long-range (LoRa) communication technology, distinguished by its low power\nconsumption and long communication range, is widely used in the Internet of\nThings. Nevertheless, the LoRa MAC layer adopts pure ALOHA for medium access\ncontrol, which may suffer from severe packet collisions as the network scale\nexpands, consequently reducing the system energy efficiency (EE). To address\nthis issue, it is critical to carefully allocate transmission parameters such\nas the channel (CH), transmission power (TP) and spreading factor (SF) to each\nend device (ED). Owing to the low duty cycle and sporadic traffic of LoRa\nnetworks, evaluating the system EE under various parameter settings proves to\nbe time-consuming. Consequently, we propose an analytical model aimed at\ncalculating the system EE while fully considering the impact of multiple\ngateways, duty cycling, quasi-orthogonal SFs and capture effects. On this\nbasis, we investigate a joint CH, SF and TP allocation problem, with the\nobjective of optimizing the system EE for uplink transmissions. Due to the\nNP-hard complexity of the problem, the optimization problem is decomposed into\ntwo subproblems: CH assignment and SF/TP assignment. First, a matching-based\nalgorithm is introduced to address the CH assignment subproblem. Then, an\nattention-based multiagent reinforcement learning technique is employed to\naddress the SF/TP assignment subproblem for EDs allocated to the same CH, which\nreduces the number of learning agents to achieve fast convergence. The\nsimulation outcomes indicate that the proposed approach converges quickly under\nvarious parameter settings and obtains significantly better system EE than\nbaseline algorithms.",
        "updated": "2024-07-18 00:54:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.13076v1"
    },
    {
        "title": "Information Compression in Dynamic Games",
        "authors": "Dengwang TangVijay SubramanianDemosthenis Teneketzis",
        "links": "http://arxiv.org/abs/2407.12318v1",
        "entry_id": "http://arxiv.org/abs/2407.12318v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12318v1",
        "summary": "One of the reasons why stochastic dynamic games with an underlying dynamic\nsystem are challenging is since strategic players have access to enormous\namount of information which leads to the use of extremely complex strategies at\nequilibrium. One approach to resolve this challenge is to simplify players'\nstrategies by identifying appropriate compression of information maps so that\nthe players can make decisions solely based on the compressed version of\ninformation, called the information state. For finite dynamic games with\nasymmetric information, inspired by the notion of information state for\nsingle-agent control problems, we propose two notions of information states,\nnamely mutually sufficient information (MSI) and unilaterally sufficient\ninformation (USI). Both these information states are obtained with information\ncompression maps independent of the strategy profile. We show that Bayes-Nash\nEquilibria (BNE) and Sequential Equilibria (SE) exist when all players use\nMSI-based strategies. We prove that when all players employ USI-based\nstrategies the resulting sets of BNE and SE payoff profiles are the same as the\nsets of BNE and SE payoff profiles resulting when all players use full\ninformation-based strategies. We prove that when all players use USI-based\nstrategies the resulting set of weak Perfect Bayesian Equilibrium (wPBE) payoff\nprofiles can be a proper subset of all wPBE payoff profiles. We identify MSI\nand USI in specific models of dynamic games in the literature. We end by\npresenting an open problem: Do there exist strategy-dependent information\ncompression maps that guarantee the existence of at least one equilibrium or\nmaintain all equilibria that exist under perfect recall? We show, by a\ncounterexample, that a well-known strategy-dependent information compression\nmap used in the literature does not possess any of the properties of MSI or\nUSI.",
        "updated": "2024-07-17 05:08:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12318v1"
    },
    {
        "title": "A Graph-based Adversarial Imitation Learning Framework for Reliable & Realtime Fleet Scheduling in Urban Air Mobility",
        "authors": "Prithvi PoddarSteve PaulSouma Chowdhury",
        "links": "http://arxiv.org/abs/2407.12113v1",
        "entry_id": "http://arxiv.org/abs/2407.12113v1",
        "pdf_url": "http://arxiv.org/pdf/2407.12113v1",
        "summary": "The advent of Urban Air Mobility (UAM) presents the scope for a\ntransformative shift in the domain of urban transportation. However, its\nwidespread adoption and economic viability depends in part on the ability to\noptimally schedule the fleet of aircraft across vertiports in a UAM network,\nunder uncertainties attributed to airspace congestion, changing weather\nconditions, and varying demands. This paper presents a comprehensive\noptimization formulation of the fleet scheduling problem, while also\nidentifying the need for alternate solution approaches, since directly solving\nthe resulting integer nonlinear programming problem is computationally\nprohibitive for daily fleet scheduling. Previous work has shown the\neffectiveness of using (graph) reinforcement learning (RL) approaches to train\nreal-time executable policy models for fleet scheduling. However, such policies\ncan often be brittle on out-of-distribution scenarios or edge cases. Moreover,\ntraining performance also deteriorates as the complexity (e.g., number of\nconstraints) of the problem increases. To address these issues, this paper\npresents an imitation learning approach where the RL-based policy exploits\nexpert demonstrations yielded by solving the exact optimization using a Genetic\nAlgorithm. The policy model comprises Graph Neural Network (GNN) based encoders\nthat embed the space of vertiports and aircraft, Transformer networks to encode\ndemand, passenger fare, and transport cost profiles, and a Multi-head attention\n(MHA) based decoder. Expert demonstrations are used through the Generative\nAdversarial Imitation Learning (GAIL) algorithm. Interfaced with a UAM\nsimulation environment involving 8 vertiports and 40 aircrafts, in terms of the\ndaily profits earned reward, the new imitative approach achieves better mean\nperformance and remarkable improvement in the case of unseen worst-case\nscenarios, compared to pure RL results.",
        "updated": "2024-07-16 18:51:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.12113v1"
    },
    {
        "title": "Map of Elections",
        "authors": "Stanisław Szufa",
        "links": "http://arxiv.org/abs/2407.11889v1",
        "entry_id": "http://arxiv.org/abs/2407.11889v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11889v1",
        "summary": "Our main contribution is the introduction of the map of elections framework.\nA map of elections consists of three main elements: (1) a dataset of elections\n(i.e., collections of ordinal votes over given sets of candidates), (2) a way\nof measuring similarities between these elections, and (3) a representation of\nthe elections in the 2D Euclidean space as points, so that the more similar two\nelections are, the closer are their points. In our maps, we mostly focus on\ndatasets of synthetic elections, but we also show an example of a map over\nreal-life ones. To measure similarities, we would have preferred to use, e.g.,\nthe isomorphic swap distance, but this is infeasible due to its high\ncomputational complexity. Hence, we propose polynomial-time computable\npositionwise distance and use it instead. Regarding the representations in 2D\nEuclidean space, we mostly use the Kamada-Kawai algorithm, but we also show two\nalternatives.\n  We develop the necessary theoretical results to form our maps and argue\nexperimentally that they are accurate and credible. Further, we show how\ncoloring the elections in a map according to various criteria helps in\nanalyzing results of a number of experiments. In particular, we show colorings\naccording to the scores of winning candidates or committees, running times of\nILP-based winner determination algorithms, and approximation ratios achieved by\nparticular algorithms.",
        "updated": "2024-07-16 16:18:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11889v1"
    },
    {
        "title": "Learning to Imitate Spatial Organization in Multi-robot Systems",
        "authors": "Ayomide O. AgunloyeSarvapali D. RamchurnMohammad D. Soorati",
        "links": "http://arxiv.org/abs/2407.11592v1",
        "entry_id": "http://arxiv.org/abs/2407.11592v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11592v1",
        "summary": "Understanding collective behavior and how it evolves is important to ensure\nthat robot swarms can be trusted in a shared environment. One way to understand\nthe behavior of the swarm is through collective behavior reconstruction using\nprior demonstrations. Existing approaches often require access to the swarm\ncontroller which may not be available. We reconstruct collective behaviors in\ndistinct swarm scenarios involving shared environments without using swarm\ncontroller information. We achieve this by transforming prior demonstrations\ninto features that sufficiently describe multi-agent interactions before\nbehavior reconstruction with multi-agent generative adversarial imitation\nlearning (MA-GAIL). We show that our approach outperforms existing algorithms\nin all investigated swarm scenarios, and can be used to observe and reconstruct\na swarm's behavior for further analysis and testing, which might be impractical\nor undesirable on the original robot swarm.",
        "updated": "2024-07-16 10:50:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11592v1"
    }
]