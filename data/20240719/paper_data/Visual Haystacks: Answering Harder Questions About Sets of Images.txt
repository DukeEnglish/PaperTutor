Visual Haystacks: Answering Harder Questions
About Sets of Images
Tsung-HanWu GiscardBiamby JeromeQuenum RitwikGupta
JosephE.Gonzalez TrevorDarrell DavidM.Chan
UniversityofCalifornia,Berkeley
Abstract
RecentadvancementsinLargeMultimodalModels(LMMs)havemadesignificant
progressinthefieldofsingle-imagevisualquestionanswering.However,thesemodels
facesubstantialchallengeswhentaskedwithqueriesthatspanextensivecollections
ofimages,similartoreal-worldscenarioslikesearchingthroughlargephotoalbums,
findingspecificinformationacrosstheinternet,ormonitoringenvironmentalchanges
throughsatelliteimagery.ThispaperexploresthetaskofMulti-ImageVisualQuestion
Answering(MIQA):givenalargesetofimagesandanaturallanguagequery,thetask
istogeneratearelevantandgroundedresponse.Weproposeanewpublicbenchmark,
dubbed"VisualHaystacks(VHs),"specificallydesignedtoevaluateLMMs’capabilities
in visual retrieval and reasoning over sets of unrelated images, where we perform
comprehensive evaluations demonstrating that even robust closed-source models
strugglesignificantly.Towardsaddressingtheseshortcomings,weintroduceMIRAGE
(Multi-Image Retrieval Augmented Generation), a novel retrieval/QA framework
tailoredforLMMsthatconfrontsthechallengesofMIQAwithmarkedefficiencyand
accuracyimprovementsoverbaselinemethods.OurevaluationshowsthatMIRAGE
surpassesclosed-sourceGPT-4omodelsbyupto11%ontheVHsbenchmarkand
offersupto3.4ximprovementsinefficiencyovertext-focusedmulti-stageapproaches.
1 Introduction
Thedevelopmentofhigh-qualityvisualquestion-answering(VQA)technologiesbasedonlargefoundation
modelsmarksasignificantmilestoneinbridgingthegapbetweenhumansandmachines,enablingthe
extractionofinsightfulinformationfromvisualcontentthroughnaturallanguagequestions.However,the
scopeofexistingVQAapproacheshaspredominantlybeenrestrictedtosingle-imageanalysis,narrowingits
utilityforaddressingmoreintricateinquiriesthatinvolveextensivecollectionsofvisualdata.Take,forexam-
ple,thechallengesofdiscerningpatternsinvastarraysofmedicalimages,monitoringdeforestationthrough
satelliteimagery,mappingurbanchangesusingautonomousnavigationdata,analyzingthematicelements
acrosslargeartcollections,orunderstandingconsumerbehaviorfromretailsurveillancefootage.Eachof
thesescenariosentailsnotonlyvisualprocessingacrosshundredsorthousandsofimagesbutalsonecessi-
tatescross-imageprocessingofthesefindings—ataskthatexceedsthereachoftraditionalVQAsystems.
Addressing this gap necessitates the development of robust Multi-Image Visual Question Answering
(MIQA)systemscapableofefficientlyprocessingandanalyzingextensivecollectionsofimages. The
corechallengesinMIQAlieintheabilityto(1)accuratelyretrieverelevantimagesfromavastpoolof
potentiallyunrelatedimagesand(2)integraterelevantvisualinformationfromtheseimagestocorrectly
answerthequestion. WhilerecentmodelscapableofMIQAsuchasGemini1.5-proandGPT-4Vare
capableoftakingmultipleimagesasinput(MII-capable)andhaveovercomekeytechnicalhurdlesto
MIQA(suchaslong-contextlearning),itremainsunknowniftheycanperformtheretrievalandintegration
tasks,particularlyasthevolumeandvariabilityofimagesincreases.
Preprint.Underreview.
4202
luJ
81
]VC.sc[
1v66731.7042:viXraFigure1: (a)UnlikeexistingvisualNeedle-In-A-Haystack(NIAH)challenges[37]thatoverlayneedle
informationastextdirectlyontotheimage,our"VisualHaystacks"(VHs)benchmarkisvisuallyfocused,
requiring the model to retrieve the needle image from the haystack and then reason to answer the
specificquestionusingvisualcontents.(b)WebenchmarkexistingLMMsunderdifferentNIAHsettings
whereonlyoneneedleimageispresentamongtenimages. WhiletraditionalvisualNIAHchallenges
overemphasizetextretrieval, whichcanbeeasilyhackedbystate-of-the-artmodelswithstrongOCR
capabilities,theyareunabletosolvethesimplevisualquestionsinVHs.
Inthiswork, webeginbyrigorouslyevaluatingthereadinessofexistingMIIcapablemodelsforthe
MIQAtaskbyintroducinganewbenchmarkforMIQAdubbed“VisualHaystacks(VHs),”designedto
explicitlytestamodel’sabilitytofirstretrieverelevantimagesandthenintegrateinformationacrossthose
images. Whilebasedonexistingneedle-in-a-haystack(NIAH)evaluations[17,37],VHsgoesbeyond
simpletext/OCR-basedneedles(Forexample,Gemini1.5Proisevaluatedusingtext-overlaysonimages),
andleveragesneedleswhichrequirevisualreasoningcapability(“Fortheimagecontainingacat,isthere
adog?”)inboththeretrievalandintegrationstages.Evaluatingbothopenandclosed-sourcemodelson
VHsdemonstratesasurprisingresult:existingMII-capablemodelsfailtoanswersimpleMIQAproblems,
leadingtoperformancedegradationofupto50%overnon-retrievalQAproblems.Wealsonoticeadramatic
positional-biasphenomenonacrossthesemodelssimilartothatnoticedinnatural-languagetasksbyLiuet
al.[29]:whentherelevantimageisplaceddeeperwithinthecontext,performancedegradesevenfurther.
Toaddresstheseissues,weintroduceasimplesingle-stagetrainingparadigm,“MIRAGE”(Multi-ImageRe-
trievalAugmentedGeneration),whichextendstheLLaVA[27]modeltohandleMIQAtasks.Ourproposed
paradigmconsistsofseveralcomponents,eachdesignedtoalleviatekeyissuesintheMIQAtask,among
them(1)augmentingtheimage-encoderwithacompressiveimageencodingand(2)leveragingaretrieval-
based,query-aware,relevancefilter,bothaimedatallowingmoreimagesgiventhesamecontextbudget,
and(3)augmentingthetrainingprocesswithadditionaltargetedsyntheticandrealMIQAdata.Through
resultsonVHsandexistingbenchmarksforMIQAincludingRetVQA[19],wedemonstratethatour
proposedapproachoutperformsstandardretrieval-augmentedmethodsonMIQAbenchmarks,isrobustto
depthinvisual-haystackquestions,andismoreefficientthantext-focusedmulti-stageplanningapproaches.
Tosummarize,ourcontributionsareasfollows:
• Weintroduceanewbenchmark,“VisualHaystacks(VHs)”whichexplicitlytestsMIQAmodels
ontheirabilitytoretrieveandintegratevisualinformation.
• Weconductcomprehensiveevaluationsofexistingopenandclosed-sourcemodelsontheVHs
benchmark,anddemonstratethatnotonlyareexistingmodelsincapableofsolvingtheVHs
task,butdemonstratenotablepositionalbiasforMIQAtasks(similartothe“lost-in-the-middle”
phenomena).
• WeintroduceMIRAGE(Multi-ImageRetrievalAugmentedGeneration),anovelretrieval-aware
trainingframeworkdesignedexplicitlyforMIQA,anddemonstratethatitcanperformGPT-4o
byupto11%(N=3,SingleNeedle)onVHs,whileremainingupto3.4xtimesmoreefficient
thantext-focusedmulti-stageplanningapproaches.
2 TheVisualHaystacksBenchmark(VHs)
The"Needle-In-A-Haystack"(NIAH)challenge[17,37]hasrecentlybecomeoneofthemostpopular
methodsforevaluatingLLM/LMMsystems’abilitytoprocesslong-contextinputs.Inthistask,essential
2information(“theneedle”),whichcontainstheanswertoaspecificquestion,isembeddedwithinavast
amountofdata(“thehaystack”).Thesystemmustthenretrievetherelevantinformationandanswerthe
questioncorrectly. BuildingontheNIAHconcept,weexploreitsapplicationintheMultimodalImage
QuestionAnswering(MIQA)task.Formally,theMIQAtaskrequiresamodeltoanswerasinglequestion
(“Q”)givenalargenumberofimages(I=I ,I ,I ,...,I ). Inthisscenario,atleastonetargetimage
0 1 2 N
containstheanswer, whiletherestserveasdistractors. Evaluatingmodelsonthistasknecessitatesa
robustbenchmarkthatcanaccuratelymeasuretheirperformanceinretrievingthecorrectinformation
todoquestionansweringwithextensivevisualinputs.Inthissection,wefirstelaborateonthelimitations
of existing benchmarks and introduce a new benchmark, dubbed "Visual Haystacks" (VHs). Then,
we conduct a comprehensive analysis of existing methods, including open-source, closed-source and
long-contextLMMsolutions,onthisbenchmarkdatasetandhighlighttheirlimitations.
2.1 BenchmarkConstruction
Currently,noexistingNIAHdatasetisdesignedforreasoningacrossalargenumberofimages(insteadofa
textcounterpart).TheclosestexampleisthedemointheGemini-v1.5technicalreport[37],whichcreated
“needle”framesinalongvideobyoverlayingtextreading“ThesecretisNeedle”onaframewithinthe
video,andaskingthemodeltoretrievethesecrettext. Whilesuchanapproachcanevaluateamodel’s
abilitytoretrievecontentfromalargecontext,thisneedleimagecreationapproachisoverlyOCR-centric,
essentiallyallowingthemodeltoperformtextretrievalfollowedbytextreasoning,whichde-emphasizes
theimageretrievalandimagereasoningcomponents.Furthermore,thechallengeincludedonlyasingle
testcase,ratherthanmultipletestcasesacrossdiversescenarios,makingtheevaluationlessconvincing.
To address these shortcomings, we introduce a new benchmark which we call "Visual Haystacks
(VHs)."TheVHsdataset(whichwereleaseunderaCC-BY-4.0license)isconstructedusingtheCOCO
dataset (CC-BY-4.0 license) [24], which already has accurate object-level annotations. To generate
a question/answer pair, we begin by selecting two objects from COCO’s label set for the question.
We then generate the text of the question in one of two settings: a single-needle setting, for which
thequestionisframedas “For the image with anchor object, is there target object?”
and a multi-needle setting where the question is framed as either “For all images with anchor
object, do all of them contain target object?” or “For all images with anchor
object, do any of them contain target object?”.Theanswersarebinary—YesorNo—and
arecarefullycuratedtoensurethatguessingorrelyingoncommonsensereasoningwithoutviewingthe
imageresultsina50%accuracyrate. Thisdesignensuresthattheanchorobjectservesasthekeyfor
imageretrieval,whilethetargetobjectformsthebasisoftheactualquestionduringimagereasoning.
After selecting the object pairs and determining the answers, we compile the corresponding image
haystacksbyfirstextractingtheneedleimages(imagescontainingboththeanchorobjects).Theneedle
imagescontainthe"anchorobject"andmayormaynotcontainthetargetobjectbasedontheanswer.
Then,theseimagesarepairedwithmultiplenegativedistractorstoformvariedhaystacksizes,ranging
from1to10,000haystackimages.Thedistractorimagesarecarefullyselectedtonotcontainanyanchor
object(followingCOCO’sobjectannotations),however,someofthemcontainthetargetobjectssoas
tocreatemeaningfuldistractors.Thesingle-needlemodecontainsonlyoneneedleimageinthehaystack,
whilethemulti-needlemodeincludesbetweentwoandfiveneedleimages. Wecarefullyfilteroutall
imagespresentintheLLaVAinstructionfine-tuningset.VHsconsistsof880and1000question-answer
pairsforsingle-andmulti-needlesetting,withanexplicitsmallsubsetVHs consistingof100questions,
small
whichcanbeusedforeconomicalevaluationofexpensiveclosed-sourcemodels.
AsshowninFigure1(a),VHisclosertoreal-worldscenarioscomparedwiththepriorGemini-style
challenges.Itrequiresmodelstoassesstherelevanceofeach"visualcontext"tothepromptedquestion
and utilize visual reasoning to formulate precise answers. Figure 1 (b) presents an experiment over
Gemini-styledatasetandours. Theresultsdemonstratethatshiftingthebenchmarkfromtext-oriented
tovisual-orientedsignificantlyincreasesthedifficultyforexistingMII-capablemethods,suchasGPT-4
andGemini,highlightingtheimportanceofourdataset.
2.2 Single-NeedleChallenge
InourinitialexperimentsontheVHsdataset,weevaluateseveralstate-of-the-artMII-capablemethods(in-
cludingbothopen-sourceandproprietarymodelsincludingLLaVA-v1.5[26],GPT-4o[32],Claude-3Opus,
andGemini-v1.5[42].Wefurtherincludea“Captioning”baseline,atwo-stageapproachwheretheimages
3Method Tokens/Img N=1(Oracle) N=3 N=5 N=10 N=50 N=100 N=1K N=10K
Naive
QuestionOnly(LLama3) - 0.52 - - - - - - -
Caption-Based(LLaVA+LLama3) 576 0.79 0.67 0.69 0.68 0.59 E E E
LMM
LLaVA-v1.5-7B 576 0.87 0.70 E E E E E E
Claude-3Opus ≈64 0.67 0.54 0.51 0.47 E E E E
Gemini-1.5 ≈258 0.87 0.73 0.68 0.64 0.58 0.59 E E
GPT-4o(low-res) ≈85 0.82 0.68 0.68 0.64 0.57 0.53 E E
RAG-based
MIRAGE(Ours) 32 0.81 0.76 0.71 0.66 0.60 0.55 0.49 0.43
Table1: ResultsforseveralmodelsonVHsforsingle-needlequestions. Allmodelsexperiencesignificantfalloff
asthesizeofthehaystack(N)increases,indicatingthatexistingapproachesarenotrobusttocomplexvisual-linguistic
processingoverlongvisualcontexts.E:Exceedscontextlength.
Method Oracle N=5 N=10 N=50 N=100 N=1K N=10K
QuestionOnly(LLama3) 0.48 - - - - - -
Naive
Caption-Based(LLaVA+LLama3) 0.70 0.70 0.66 0.56 E E E
Claude-3Opus 0.55 0.49 0.48 E E E E
LMM Gemini-1.5 0.56 0.51 0.54 0.50 0.52 E E
GPT-4o(low-res) 0.71 0.65 0.63 0.49 0.52 E E
RAG-based MIRAGE(Ours) 0.57 0.56 0.54 0.51 0.50 0.48 0.49
Table2:PerformanceonVHsformulti-needlequestions.Wecanseethatallvisually-awaremodelsperformpoorly,
indicatingthatmodelsfinditchallengingtoimplicitlyintegratevisualinformation.E:Exceedscontextlength.
arefirstcaptionedusingLLaVA,andthenthequestionisansweredusingthetextcontentoftheresulting
captions.Foreachsetting,weleveragedtheVHS dataset,andreportedthebootstrappingaverageresults
small
using75ofthe100samplesforeachbootstrapsample,resultinginstandarddeviationsbetween3-4%for
allexperiments.TheresultspresentedinTable1revealseveralcriticalinsights.Overall,wenoticethateven
thoughthemodelsperformwellonexistingNIAHdatasets(Figure1),performanceissignificantlyworse
ontheVHsdatasetacrosstheboard.Becauseoracleresultsremainhigh,itislikelythatexistingapproaches
primarilystrugglewithvisualretrieval,particularlyinthepresenceofchallengingdetractorsamples.
Interestingly,chainingacaptioningmodel(LLaVA[27])withanLLMaggregator(LLama-3[1])isstraight-
forwardandeffective,suggestingthattheissuesonthetaskarelargelylimitedtovisualprocessingcapabil-
itiesandnotinformationprocessingbehavior.Unfortunately,thisapproachisgenerallyinadequateforthe
MIQAchallengeduetoitscomputationalexpenseandlimitedcontextlengths.Forinstance,whenprocess-
ing100images,ittakesover3minutestoanswerasinglequestion(seeFigure4forfullruntimeresults),
and100captionsexceedthe8KcontextlengthofLLama3,leadingtoinaccurateandnonsensicaloutputs.
Beyondtheperformanceofthetext-onlybaselines,itisimportanttolookatthedividebetweenopen-source
and closed-source LMMs. Unfortunately, open-source LMMs such as LLaVA suffer primarily from
context-lengthissues,takingatmostthreeimagesintheirshortertrainedcontextlengths. Proprietary
long-contextmodelscanhandleuptoonehundredimages,butdespitetheirtechnicalcapabilityofhandling
suchlongcontexts,thesemodelsarehighlysusceptibletodistractionsfromirrelevantimages. Further,
eventhoughthesemodelsarelong-context,theyarenotcapableofhandlingverylargehaystacks. For
1Kandlargerhaystacks,Gemini-v1.5andGPT-4owiththelow-resolutionencodingschemerejectmost
APIrequestsasitexceedsthepayloadsizelimits.
Inadditiontobeingpronetodistractions,wealsofoundtheaccuracyofLMMsisverysensitivetothe
positionoftheneedleimagewithintheinputsequence,asshowninFigure2. LLaVAperformsbetter
whentheneedleimageisplacedimmediatelybeforethequestion(sufferinguptoa26.5%degradation
whenthisisnotthecase),whileproprietarymodelsperformbetterwhentheimageisatthebeginning
(leadingtouptoa28.5%performancedegradation). Thispatternsuggestsachallengesimilartothe
"loss-in-the-middle" phenomenon in NLP [29], where relevant information is contained in either the
beginningorendofthecontext,leadingtoinherentpositionalbiasesintheperformanceofthemodel.
Interestingly,thispathologywasnotobservedinthepriorGemini-styleNIAHchallenge,whichrequired
onlytextretrievalandreasoning,asshowninthetoprowinFigure1(b). Thisinterestingresultclearly
highlightstheimportanceofvision-specificNIAHbenchmarkssuchasVHs.
4Figure2: Plotsshowingneedleposition,vs. performanceontheVHsbenchmarkforseveralimagesettings. We
canseethatforexistingLMMs,theneedlepositionisextremelyimportant,withperformancedegradationofupto
41%whentheneedleisnotplacedintheoptimallocationintheinputcontext.ThegrayboxesintheLLaVAresults
indicatethattheseexperimentsexceedtheavailablecontextlength(andperformanceisaccordinglyrandom).
2.3 Multi-NeedleChallenge
We use a similar setup to subsection 2.2 to evaluate in the multi-needle setting. The results of the
multi-needle challenge are shown in Table 2. Overall, we see similar (yet more dramatic) results to
thesingle-needlesetup. Here,however,thecaptioningapproachhasthebestperformance,suggesting
thatusinganexternalLLMtointegratemultipletextinputsissuperiortoanLMM’sinternalcapability
toaggregateinformationfrommultipleimageinputs. Similartothesingle-needlecase, however, the
captioningbaselinesuffersfromthesamecomputationalburdenandperformspoorlywhenN=100due
tocontextlengthlimitations.ItisnotablethatLMMsaloneperformquitepoorlyonthistask,specifically,
Gemini-1.5andClaude-3’sperformanceonmulti-imagereasoningwasverypoor,almostguessingthe
answermostofthetime,indicatingitsinabilitytoaggregateinformationfrommultipleimageseffectively.
3 MIRAGE:Multi-ImageRetrievalAugmentedGeneration
Todevelopanopen-sourceframeworkforMIQAcapableofimprovedperformanceontheVHsbenchmark,
weintroduceMIRAGE,anewframeworkfortraininganopen-sourceMII-capablemodel.Thisframework
addressesseveralcriticalchallengesforbaselineperformanceinMIQAincluding:(1)reducingtherequired
tokensperimagetoallowtheinputofmoreimages, (2)incorporatingalightweightretrievermodule
trainedinlinewiththeLMMtofilteroutirrelevantimages,and(3)providinganopen-sourcetraining
datasetandrecipedesignedformulti-imagereasoningthatisrobustagainstnoisyimagesandtheposition
ofthecorrectimage.Weelaborateonthesecomponentsinthesubsequentsections.
3.1 ModelArchitecture
AsshowninFigure3,MIRAGEisaRAG-basedsolutionextendedfromLLaVA-v1.5-7B[26],wherethe
inputprompttothemodelisstructuredas "Image 1: <img_token>, Image 2: <img_token>,
... <Actual Question>". First, similartoLLaVA,eachimageispassedthroughafrozenCLIP
imageencodertoextractpatch-levelfeatures(576tokens).Toreducethetoken-intensity,wefirstapply
animagecompressor,designedtoreducetheoveralltokenintensitybyreducingthenumberoftokens
perimage. Inthiswork,weleverageaQ-former[21],alight-weighttransformermodelwithK=32
learnedqueryvectorscross-attendingtothefull576patchfeatures,tocompressthepatch-levelfeatures
from576to32tokens/image(a18xreductionintokenintensity). Afterthisinitialreduction,following
LLaVA,theoutputpassesthroughtwoMLPlayerswithGELUactivationfunctions[13]whichhelpsadapt
totheLLM’sinputdimension.WeexplorethischoicecomparedtoothercompressorsfurtherinTable4.
WhilethetokenreductionviatheQ-formeraloneallowsMIRAGEtohandlemorethan50imagesas
inputwithminimalreductioninaccuracy(seeTable3),inMIQAweoftenwanttoanswerquestionsover
manyhundredsorthousandsofimages.Suchlimitationslendthemselvestoretrieval-basedapproaches,
inwhichasmallamountofadditionalprocessingisperformedper-imagetodetermineiftheimageis
relevant,andtheLMM(whichiscontext-lengthaware)isrunagainstthereducedrelevantset.Inorderto
5Figure3: ModelOverview: MIRAGEhandlesquestionsandimagesthroughseveralsteps: encoding
featureswithCLIP,compressingimagefeatureswithourQ-Former,calculatingrelevancescoreswith
aretriever, andfeedingonlyrelevantimagestotheLLM.Duringinstructionfinetuning, themodelis
supervisedforthenexttokenpredictionandtherelevancepredictiontask,utilizingBinaryCrossEntropy
lossbetweenthegroundtruth0,1andthepredictednumber.
enablethiscapability,weaugmentourfilterwithahard-passfilteror“retriever.”Whilethishard-passfilter
cantakemanyforms(suchasaCLIP-similaritythreshold),wefounditeffectivetotrainaquery-aware
retrievalmodelin-linewiththenext-tokenpredictiontask.Formally,givenasetofimages,Imadeupof
imagetokens{i ,...,i },itisourgoaltoretaintheminimalsubsetI ⊂Isuchthatwecanstillaccurately
0 n min
answerthequeryQ.Lettheimageencodingmoduleasafunctionψ(I),wecanget:
F,R =ψ(I,Q), (1)
i i i
whereF representsimagefeaturesandR representstherelevancescoreoftheimageofImageI. In
i i
practice,ourretrievermoduleconsistsofasingle-layerMLPontopofthetemporallypooledqueryand
imagefeatures,alongwithasigmoidactivationtopredicta0/1value.Suchanapproachhassignificant
benefits,astheretrieverisquery-aware,andcanmakeuseoftheinformationfromthequerytodetermine
image relevance. A downside to this approach, however, is that we can no longer use efficient data
structures(suchasLSH)fork-nearestneighborsrecall(unlikeasplit-head/metric-basedmodelsuchas
CLIP[35]).Itremainsinterestingandexcitingfutureworktoexploreifhigh-recallquery-awaresplit-head
modelscanbecreated. Theoutputoftheretrievermoduleisascorebetween0and1indicatingthe
relevanceoftheimagetothequery. Duringtraining,thisoutputisoptimizedagainstthegroundtruth
relevance,whileduringinference,onlyimagesdeemedrelevant(inourexperiments,havingrelevance
score>0.5areforwardedtotheLMM).Weexplorethischoicefurtherinsection4.
Afterapplyingtheimagetokenreductionandretrievalfiltering,theLLMtakestheresultingsetofvisual
featuresandtheencodedquestionsasinput. SimilartoexistingLMMs,weconcatenatetheimageand
queryinputsandfine-tunethemodelfromanexistingLLMusingnext-tokenprediction.
3.2 ModelTraining
TrainingData:Whilesingle-imageQAdataisrelativelyplentiful,open-sourcedataforMIQAisquitelim-
itedinthecommunity.Thus,thetrainingdataforMIRAGEcontainsdatafromtwokeysources:(1)Existing
MIQAdata,and(2)SyntheticMIQAdataderivedfromsingle-imageQAdatasets.Wefirstincludedallpub-
liclyavailableMIQAtrainingsets,includingRetVQA[34],SlideVQA[41],andWebQA[7].RETVQAhas
thelargestavailabledataset,withabout377KquestionscuratedfromVisualGenomedata.However,these
multi-imagequestionscomefromscenegraphsandcoveraverylimiteddomain,suchascounting,relation-
ships,andobjectattributes.WebQAcontainsdiverseimagesandquestionsfromWikipedia,andSlideVQA
6MIQA(RetVQA) Single-ImageQA
Method
Recall Precision VQAAcc. VQAv2 GQA Vizwiz TextVQA POPE MMB MMB-CN MM-Vet
Qwen-VL-Chat[2] - - 0.0* 78.2 57.5 38.9 61.5 - 60.6 56.7 -
LLaVA-v1.5-7B[26] - - 30.6 78.5 62.0 50.0 58.2 85.9 64.3 58.3 31.1
GPT-4o[32] - - 34.6 - - - - - - - -
GPT-4[33] - - - 77.2 - - 78.0 - - - -
Gemini-v1.5[42] - - 32.2 73.2 - - 73.5 - - - -
LWM[28] - - - 55.8 44.8 11.6 18.8 75.2 - - 9.6
MI-BART[34] - - 76.5 - - - - - - - -
MIRAGE(Ours) 80.5 49.9 70.8 70.0 55.2 40.1 46.3 83.4 57.6 48.8 25.8
Table3:Comparativeperformanceofmethodsonmulti-imageandsingle-imageQAtasks.WecanseethatMIRAGE
has strong recall/precision capabilities, strong multi-image QA performance, and competitive single-image QA
performance. *Qwen-VL-Chatisalsoover-fittotheVGdataset,andproducesobjectboundingboxesinsteadof
answersfortheRetVQAqueries.
Method Tokens/Img VQAv2 GQA Vizwiz TextVQA POPE MMB MMB-CN MM-Vet
OriginalLLaVA 576 78.5 62.0 50.0 58.2 85.9 64.3 58.3 31.1
3x3Max-Pooling 64 68.7 56.2 41.3 48.5 83.0 59.2 49.3 24.3
GlobalAvg.Pooling 1 62.5 51.3 37.7 45.5 79.6 55.0 45.5 18.9
MIRAGE/Q-Former(Ours) 32 72.8 56.6 48.0 47.1 83.9 61.5 55.0 27.3
Table4:Explorationofvarioustokenreductionmethods.WecanseethattheQ-formerismostefficientatreducing
thenumberoftokenswhileretainingmostofthegeneralQAperformance.
includescomplexquestionsformultipleslides,butthesedatasetsonlyinclude19Kand12Kquestions,
respectively.Inthesethreedatasets,thereareonlyoneortworelevantimages,inasetof15-30totalimages.
Second,toaugmentourtrainingdataandenhancediversity,weadaptedtheLLaVAsingle-imagetraining
datatoamulti-imageformat. Whileastraightforwardmethodforsyntheticdataaugmentationwould
betofindmanydistractorimagesforasingle-imageQA,identifyingsuitabledistractorsischallenging,
and many questions naturally apply to multiple images, complicating dataset construction. To avoid
suchsituations,wefilterquestionsforsimilarcontent(basedonkeywordfrequency,retainingunique
questions),andthenrandomlysampletwototendistractorimagesfromunrelatedsubsets.Thisprocess
resultsinaninstruction-tuningdatasetcontaining1.2Msamples.Wefurthershuffletheimageswithineach
instruction-tuningpairduringtrainingtoensuretherelevantimagescanappearinanyposition,aiming
tomakeourmodelinsensitivetotheneedleimage’sposition.
TrainingProcedure:SimilartoLLaVA,MIRAGEfollowsatwo-stagetrainingparadigm:pre-training
andfine-tuning.Inthepre-trainingstage,wefreezetheCLIPvisualencoderandLLMbackbone,disable
theretrieverfunctions,andonlytraintheQ-FormercompressorandtheMLPfeaturealignmentlayers
usingthenext-tokenpredictiontask.WeusetheShare-GPTcaptioningdata[27]forpretraining.Inthe
fine-tuningstage,weapplytheretrieverandfine-tunetheentiremodel(exceptforthefixedCLIPvisual
encoder)usingthetrainingdatasetdescribedabove.Inadditiontotheconventionalnexttokenprediction
loss,wealsoco-traintheretrieverwithbinarycross-entropyloss,assigningaweightof5.0topositive
samplestoemphasizerecallandaddresstheimbalanceduetothemanyzerocases.Wetrainthemodelfor
oneepochon8A100GPUs(takingapproximatelytwodays),duringwhich,inthefirst75%oftraining,
wealwayspassonlythefeaturesofrelevantimagestotheLLM.Fortheremainingtime,weoccasionally
feedseveraldistractorimagestothemodel,toimproverobustnesstoimperfectretrieval.
4 Results&Discussion
VisualHaystacks:TheperformanceofMIRAGEontheVHstaskisshowninTable1andTable2. In
thesingle-needlechallenge,ourmodelachievescompetitiveoracleperformancecomparedtoexisting
approaches,howeverperformanceissomewhatlowerduetotheunderlyingtokencompression.Overall,MI-
RAGEoutperformsGPT-4o(low-resolution)acrossallsettingsandsurpassesGemini-v1.5-proandClaude-
3Opuswithhaystacksizesbelow50images(despitebeingsignificantlycheaperonaper-tokenbasis).In
themulti-needlesetting,MIRAGEoutperformsGemini-v1.5-pro(eveninoracleperformance),suggesting
thatMIRAGEissignificantlybetterthanGeminiatsimplereasoningacrossmultipleimages(whichmakes
sense,giventhemulti-imageawaretraining).Unfortunately,MIRAGEunder-performscomparedtoGPT-
7(A) Top-1 Retrieval Accuracy Comparison (B) Runtime Comparison of Different Methods
1.0
CLIP 104 CLIP-LLaVA
MIRAGE (Ours) MIRAGE (Ours)
0.8 GPT-4o
103 Caption-Based
0.6
102
0.4 101
0.2 100
0.0 101 102 103 100 101 102 103
Number of Images Number of Images
Figure4:ComparisonbetweenMIRAGEretrieverandCLIPretrieveracrossbothperformanceandruntime.Wecan
seethatwhileCLIPisslightlyfaster,MIRAGEhassignificantlyhigherrecall,whichimpactsdownstreamperformance.
BothMIRAGEapproacheswitharetrievalcomponentaresignificantlymoreefficientthanGPT-4oandCaption-based
baselines.
4oandcaption-basedbaselinesinoracleperformance,suggestingthatthereremainssignificantroomfor
futureworkinthereasoningcomponent.Wehypothesizethatsuchdegradedoracleperformanceisbecause
existingtrainingsetsonlyrequiremodelstolearnfromatmosttwoimages,limitingourmodel’sability
tohandletasksrequiringreasoningacrossmorethanthreeimages,anissuethatcanberesolvedwithfuture
developmentstoMIQAdatasets.MIRAGEshines,however,inretrievalperformance,andenablesmuch
longercontextsthanGPT-4oandGeminialone,whichcannothandlethe1Kand10Kimagescenarios.
ConventionalVQATasks:WealsocompareMIRAGEwithothermethodsonconventionalVQAtasks,
includingtheRetVQAmulti-imagetask[34]andseveralsingle-imageQAtasks,VQA-v2[10],GQA[15],
VizWiz[11],TextVQA[39],POPE[22],MMB[30],MMB-CN[30],andMM-Vet[51].Inallcases,we
followstandardevaluationproceduresfromeitherRetVQA[34]orLLaVA[27]. AsshowninTable3,
MIRAGEistheonlymethodthatiscapableofbothmulti-imageQAandsingle-imageQA.Open-source
models(LLaVA[27],Qwen-VL-Chat[2])strugglewiththemulti-imageQAtask,likelyduetoinsufficient
contextlengthandover-specializedtraining(e.g.,over-fittinginQwen-VL-Chatorlackofmulti-imageQA
diversityinLLaVA’sdataset).Similarly,commerciallong-contextLMMsalsoperformpoorlyonthemulti-
imagereasoningtask,confirmingtheresultsdemonstratedbyVHsinsection2,likelyduetosimilarreasons.
ImageTokenCompressionScheme:ThegoalofMIRAGEistoenablefutureresearchintoMIQAtasks.
Oneofthemostimportantquestionsinsuchtasksishowwecanreducethecontextlengthrequiredper
image.Toevaluatethis,wecomparedseveraldifferenttokencompressionschemes,includingtokenmax
poolingandaverage-poolingcompressionofanimagetoasingletoken. Inthisexperiment,allmodels
aretrainedonLLaVA’ssingle-imageinstructionfine-tuningdataset,andtheretrieverisdisabled. The
resultsareshowninTable4.WhilepreviousworkclaimsQ-Formerissub-optimal[9],ourresultsshow
thatQ-Formermaintainsreasonableperformancewhilereducingcontextlength.
MIRAGEretrievervs. CLIP:WhileMIRAGEtrainstheretrievalmodelinline,itisalsopossibleto
useanexternalretrievalmodulesuchasCLIP[35]toperformlookuppriortopassingimagestothe
downstreamLLM.Toexplorethisquestion,wecomparethesingle-stageMIRAGEretrieverwithusing
CLIPastheretrieverontheVHsbenchmarkinFigure4.ThisexperimentdemonstratesthatwhileCLIP
issomewhatfaster,itstruggleswithrecall,acriticalmetricfortheMIRAGEsystem,aspoorrecallleads
tonecessaryinformationbeingdroppedbeforeevenmakingittotheLLMforanalysis.
4.1 LimitationsandSocietalImpact
VHs:WhileVHsisthefirstbenchmarktoinvestigatehowLMMscanretrieveandreasonacrosssets
ofimages,ithassomenotablelimitations. Theprimarylimitationofthebenchmarkisthescope:since
itisbasedonMS-COCOimages,itinheritstheimplicitbiasesinthedatasetincludingnotablegender,race,
andlocationbiases[12,5,14,44]. Itisimportanttoworkinthefuturetowarddevelopingbenchmarks
thatdonotfavormodelsthatpreferdatahavingsuchbiases.Beyondsuchimplicitbiases,COCOobjects
arealsolimitedto80categories-strongperformanceontheVHsbenchmarkdoesnotimplythatthe
modelwillgeneralizewelltoallMIQAproblems.Finally,theVHsbenchmarkisprimarilytemplate-based,
whichmeansthatitdoesnotevaluatethelanguagereasoningcapabilitiesoftheLLM.Amorecomplex
benchmarkwouldrequiremakingmoredetailedinferences, andrequiremulti-hopreasoningacrossa
widerrangeofopen-domainobjectsets.
8
ycaruccA
laveirteR
1-poT
)yreuq/sdnoces(
emitnuRMIRAGE:MIRAGEissignificantlymoreefficientthanitsLLaVAbase,whilesimultaneouslyperforming
better on many MIQA benchmarks. To perform well on MIQA benchmarks, however, we note that
MIRAGEsacrificessomesingle-imageperformance,likelyduetoinefficienciesinthemulti-tasktraining
setup.Itremainsinterestingandnecessaryforfutureworktoexplorehowsuchapproachescanretainsingle-
imageperformancewhileimprovingmulti-imagecapabilities.Itisalsoimportanttorecognizethatasalarge
multi-modalmodel,thepotentialformisuseofthemodelexists.Manyoftheimpactsofsuchmodelsare
wellstudiedinotherrelatedworks[4,27,26].Recognizingthis,MIRAGEinheritsthesafetymechanisms
fromtheLLaVAcode-base[27],andincludesrelevanttrainingdetailsinafullypubliccoderelease.
5 RelatedWork
OursisnotthefirstworktoaddresstheMIQAproblem.Similartoourwork,Bansaletal.[3]introduce
amethodformulti-imagequestionanswering,howeverintheirapproach,alloftheimages(upto10)
arerelevanttothequestion,andtheirmethoddoesnotcontainaretrievalcomponent. Penamakuretal.
[34]proposeadatasetandmethodforretrieval-basedvisualquestionanswering(RetVQA),andshare
withourworkthefactthatanswersmustbegleanedfromasetofbothrelevantandirrelevantimages.
However,theirworkdiffersfromoursintwokeyways:(1)RetVQAquestioncontextscontainatmost
tworelevantimages,meaningthatmodelsdonothavetoreasonacrossmanyimages,largelysidestepping
theproblemoflimitedcontextlength,and(2)theirapproachonlyallowsforsmallimageworlds(upto
30),meaningthattheycanuseapairwiseencoderforeachimage,andtheydonotneedtosearchovera
largedatasetofimagespotentiallycontainingdistractors,limitingtheefficiencyoftheirapproach.Finally,
theyareonlyabletoperformquestionansweringovertheimages,whereaswepursueamethodthatcan
additionallyperformmorecomplexreasoningtasksgiventheimagesinthedataset.
SimilartoourMIRAGEapproachisChenetal.[8]andYasunagaetal.[49]whichbothretrievemultiple
visualdocumentstoanswerqueries. InthecaseofChenetal.[8],thequeriesareopen-endedquestion
answering,andthus,arenotgroundedwithinaparticularsetofcontextimages. Yasunagaetal.[49]
focusesononeandfew-shotclassificationandimagegeneration, anddoesnotusetheirmulti-image
retrievaltoansweraggregatedquestionsaboutthoseimages.Insingle-imageQA,imageretrievalinlarge
multimodalmodels(LMMs)hasbeenexploredusing“retrieval-tokens”[20],however,itisunclearhow
suchanapproachwouldscaletomulti-imageQAproblemswithmultiplerelevantimages.Inadditionto
generalQA,severalotherworkscontaindomain-specificmulti-imageQAproblems,includingSlideVQA
[41],MultimodalQA[40],WebQA[7]andDocumentVQA[43],however,noneareexplicitlydesigned
toanswerintegrationquestionsacrossmultipleimagesfromasubsetoflarge,unrelatedimages(likeVHs).
OutsideofVQA,IntraditionalNLP,retrievingsmallpassagesorsingledocumentsfromlarge-scalecorpuses
hasproveneffective.Zhangetal.[52]introducesamethodwhichfine-tunesLLMsonbothrelevantand
irrelevantdocumentstosupportbetterRAGperformance,butdoesnottrainanexplicitquery-awarefilteror
compressionmodule.ATLAS[16]treatsdocumentsaslatentvariablesduringtraining,allowingforefficient
retrieval,butrequiresacomplexjoint-trainingsetup.Similarly,severalmethods[38,36,6,18]havedemon-
stratedsuccesswithzeroorfew-shotaugmentationofstandardLLMcontextswithretrieveddocuments.
Beyondcontextaugmentation,severaltraditionalNLPapproaches[25,45,47,31,46,48]havedemon-
stratedthatfine-tuningLLMstoberobusttonoisyRAGoutputscanleadtoperformanceimprovements.
MIQAvs.Video-QA:Whileseveralmethodshavebeendevelopedforvideoquestionanswering(such
asVideo-LLaVA[23]),thetaskofansweringquestionsovervideoisfundamentallydifferentfromMIQA.
WhilemodelsthatcansolveMIQAcanoftensolvevideotasks,theinverseisnottrue,asvideoframes
containmanyframe-wiseinter-dependenciesthatareexploitedbyencodermodels(suchasMAG-ViT
[50]whichusetemporalblocks,orframe-subsampling,whichdropscloselyrelatedframes).MIQAhas
independentimages,renderingmostvideomodelsinadequateforthistask. Itisaninterestingdirection
forfutureworktoexplorehowtoconnectMIQAandVideo-QA,particularlyacrossthedatadimension,
whereinVideo-QAdatasetscouldprovideusefultrainingdataforMIQAmodels.
6 Conclusion
Inthiswork,weintroduce“VisualHaystacks”(VHs),anewbenchmarkformulti-imagequestionanswering,
designedtotestalargemulti-modalmodel’sabilitytoretrieverelevantimagesfromalargecollectionofunre-
latedinputs,andreasonaboutthoseimages.WeshowthatVHsissignificantlymoreinformativethanexist-
ingNIAHbenchmarksbasedonOCR/Text-findingcapabilitiesandthatbothopenandclosed-sourcemodels
9strugglewiththistask.Tohelpclosetheperformancegap,wefurtherintroduceMIRAGE,atrainingframe-
workdesignedforopen-sourcemodelscapableofansweringbothsingle-imageandmulti-imagequestions,
anddemonstratethatitcanoutperformGemini1.5intheVHsbenchmarkinalmostallscenarioswhilebe-
ing3xmoreefficientthanmulti-stagepipelinemethods.BothVHsandMIRAGErepresentsomeofthefirst
concretestepstowardslargeMIImodelscapableofansweringquestionsoverthousandsortensofthousands
ofunrelatedimages.Whiletheyrepresentstrongstepsforward,theproblemisfarfromsolved,andwehope
thatthisbenchmarkandframeworkwillinspirecontinuedresearchinMIImodelsandtheMIQAproblem.
Acknowledgements WethankLisaDunlap,BoyiLi,andXudongWangfortheirinvaluablefeedback
duringthediscussions. ThisprojectwasprimarilysupportedbyagrantfromtheNationalGeospatial-
IntelligenceAgency(GrantNo.HM0476-22-1-2001).Authors,aspartoftheiraffiliationwithUCBerkeley,
weresupportedinpartbytheNationalScienceFoundation,USDepartmentofDefense,and/ortheBerkeley
Artificial Intelligence Research (BAIR) industrial alliance program, as well as gifts from Anyscale,
Astronomer, Google, IBM,Intel, Lacework, Microsoft, MohamedBinZayedUniversityofArtificial
Intelligence,SamsungSDS,Uber,andVMware.Anyopinions,findings,conclusionsorrecommendations
expressedinthismaterialarethoseoftheauthor(s)anddonotnecessarilyreflecttheviewsofNGA,DoD,
ortheUSgovernment.ThepaperisapprovedforpublicreleaseinaccordancewithNGA-U-2024-01397.
References
[1] MetaAI.Introducingmetallama3:Themostcapableopenlyavailablellmtodate,2024.
[2] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl:Aversatilevision-languagemodelforunderstanding,localization,textreading,and
beyond. arXivpreprintarXiv:2308.12966,2023.
[3] AnkanBansal,YutingZhang,andRamaChellappa. Visualquestionansweringonimagesets. InComputer
Vision–ECCV2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartXXI
16,pages51–67.Springer,2020.
[4] EmilyMBender,TimnitGebru,AngelinaMcMillan-Major,andShmargaretShmitchell. Onthedangersof
stochasticparrots:Canlanguagemodelsbetoobig? InProceedingsofthe2021ACMconferenceonfairness,
accountability,andtransparency,pages610–623,2021.
[5] ShrutiBhargavaandDavidForsyth.Exposingandcorrectingthegenderbiasinimagecaptioningdatasetsand
models. arXivpreprintarXiv:1912.00578,2019.
[6] SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMillican,GeorgeBm
VanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.Improvinglanguagemodelsbyre-
trievingfromtrillionsoftokens.InInternationalconferenceonmachinelearning,pages2206–2240.PMLR,2022.
[7] YingshanChang,MriduNarang,HisamiSuzuki,GuihongCao,JianfengGao,andYonatanBisk. Webqa:
Multihopandmultimodalqa. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages16495–16504,2022.
[8] WenhuChen,HexiangHu,XiChen,PatVerga,andWilliamCohen.Murag:Multimodalretrieval-augmented
generatorforopenquestionansweringoverimagesandtext.InProceedingsofthe2022ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,pages5558–5570,2022.
[9] XiaoranFan,TaoJi,ChanghaoJiang,ShuoLi,SenjieJin,SiruiSong,JunkeWang,BoyangHong,LuChen,
GuodongZheng,etal.Mousi:Poly-visual-expertvision-languagemodels. arXivpreprintarXiv:2401.17221,
2024.
[10] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. Makingthevinvqamatter:
Elevatingtheroleofimageunderstandinginvisualquestionanswering,2017.
[11] DannaGurari,QingLi,AbigaleJ.Stangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,andJeffreyP.
Bigham.Vizwizgrandchallenge:Answeringvisualquestionsfromblindpeople,2018.
[12] LisaAnneHendricks,KayleeBurns,KateSaenko,TrevorDarrell,andAnnaRohrbach.Womenalsosnowboard:
Overcomingbiasincaptioningmodels.InProceedingsoftheEuropeanconferenceoncomputervision(ECCV),
pages771–787,2018.
[13] DanHendrycksandKevinGimpel.Gaussianerrorlinearunits(gelus). arXivpreprintarXiv:1606.08415,2016.
[14] YusukeHirota,YutaNakashima,andNoaGarcia.Genderandracialbiasinvisualquestionansweringdatasets.In
Proceedingsofthe2022ACMConferenceonFairness,Accountability,andTransparency,pages1280–1292,2022.
[15] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and
compositionalquestionanswering,2019.
10[16] GautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,
ArmandJoulin, SebastianRiedel, andEdouardGrave. Atlas: Few-shotlearningwithretrievalaugmented
languagemodels. JournalofMachineLearningResearch,24(251):1–43,2023.
[17] GregoryKamradt.Llmtest_needleinahaystack,2023.GitHubrepository.
[18] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis. Generalizationthrough
memorization:Nearestneighborlanguagemodels. arXivpreprintarXiv:1911.00172,2019.
[19] AbhinavKhattar,AviralJoshi,HarSimratSingh,PulkitGoel,andRohitPrakashBarnwal.Analysisonimage
setvisualquestionanswering. arXivpreprintarXiv:2104.00107,2021.
[20] JingYuKoh,RuslanSalakhutdinov,andDanielFried.Groundinglanguagemodelstoimagesformultimodal
inputsandoutputs.2023.
[21] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels.InInternationalconferenceonmachinelearning,pages
19730–19742.PMLR,2023.
[22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object
hallucinationinlargevision-languagemodels,2023.
[23] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[24] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,and
CLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InComputerVision–ECCV2014: 13th
EuropeanConference,Zurich,Switzerland,September6-12,2014,Proceedings,PartV13,pages740–755.
Springer,2014.
[25] XiVictoriaLin,XilunChen,MingdaChen,WeijiaShi,MariaLomeli,RichJames,PedroRodriguez,Jacob
Kahn,GergelySzilvasy,MikeLewis,etal.Ra-dit:Retrieval-augmenteddualinstructiontuning. arXivpreprint
arXiv:2310.01352,2023.
[26] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.Improvedbaselineswithvisualinstructiontuning,2023.
[27] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.InNeurIPS,2023.
[28] HaoLiu,WilsonYan,MateiZaharia,andPieterAbbeel. Worldmodelonmillion-lengthvideoandlanguage
withringattention. arXivpreprintarXiv:2402.08268,2024.
[29] NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.
Lostinthemiddle:Howlanguagemodelsuselongcontexts. TransactionsoftheAssociationforComputational
Linguistics,12:157–173,2024.
[30] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,JiaqiWang,Con-
ghuiHe,ZiweiLiu,KaiChen,andDahuaLin.Mmbench:Isyourmulti-modalmodelanall-aroundplayer?,2024.
[31] ZihanLiu,WeiPing,RajarshiRoy,PengXu,MohammadShoeybi,andBryanCatanzaro. Chatqa:Building
gpt-4levelconversationalqamodels. arXivpreprintarXiv:2401.10225,2024.
[32] OpenAI.Hellogpt-4o,2024.
[33] OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,RedAvila,IgorBabuschkin,SuchirBalaji,
ValerieBalcom,PaulBaltescu,HaimingBao,MohammadBavarian,JeffBelgum,IrwanBello,JakeBerdine,
GabrielBernadett-Shapiro,ChristopherBerner,LennyBogdonoff,OlegBoiko,MadelaineBoyd,Anna-Luisa
Brakman,GregBrockman,TimBrooks,MilesBrundage,KevinButton,TrevorCai,RosieCampbell,Andrew
Cann,BrittanyCarey,ChelseaCarlson,RoryCarmichael,BrookeChan,CheChang,FotisChantzis,DerekChen,
SullyChen,RubyChen,JasonChen,MarkChen,BenChess,ChesterCho,CaseyChu,HyungWonChung,
DaveCummings,JeremiahCurrier,YunxingDai,CoryDecareaux,ThomasDegry,NoahDeutsch,Damien
Deville,ArkaDhar,DavidDohan,SteveDowling,SheilaDunning,AdrienEcoffet,AttyEleti,TynaEloundou,
DavidFarhi,LiamFedus,NikoFelix,SimónPosadaFishman,JustonForte,IsabellaFulford,LeoGao,Elie
Georges,ChristianGibson,VikGoel,TarunGogineni,GabrielGoh,RaphaGontijo-Lopes,JonathanGordon,
MorganGrafstein,ScottGray,RyanGreene,JoshuaGross,ShixiangShaneGu,YufeiGuo,ChrisHallacy,
JesseHan,JeffHarris,YuchenHe,MikeHeaton,JohannesHeidecke,ChrisHesse,AlanHickey,WadeHickey,
PeterHoeschele,BrandonHoughton,KennyHsu,ShengliHu,XinHu,JoostHuizinga,ShantanuJain,Shawn
Jain,JoanneJang,AngelaJiang,RogerJiang,HaozhunJin,DennyJin,ShinoJomoto,BillieJonn,HeewooJun,
TomerKaftan,ŁukaszKaiser,AliKamali,IngmarKanitscheider,NitishShirishKeskar,TabarakKhan,Logan
Kilpatrick,JongWookKim,ChristinaKim,YongjikKim,JanHendrikKirchner,JamieKiros,MattKnight,
DanielKokotajlo,ŁukaszKondraciuk,AndrewKondrich,ArisKonstantinidis,KyleKosic,GretchenKrueger,
VishalKuo,MichaelLampe,IkaiLan,TeddyLee,JanLeike,JadeLeung,DanielLevy,ChakMingLi,Rachel
Lim,MollyLin,StephanieLin,MateuszLitwin,TheresaLopez,RyanLowe,PatriciaLue,AnnaMakanju,
KimMalfacini,SamManning,TodorMarkov,YanivMarkovski,BiancaMartin,KatieMayer,AndrewMayne,
BobMcGrew,ScottMayerMcKinney,ChristineMcLeavey,PaulMcMillan,JakeMcNeil,DavidMedina,Aalok
11Mehta,JacobMenick,LukeMetz,AndreyMishchenko,PamelaMishkin,VinnieMonaco,EvanMorikawa,
DanielMossing,TongMu,MiraMurati,OlegMurk,DavidMély,AshvinNair,ReiichiroNakano,RajeevNayak,
ArvindNeelakantan,RichardNgo,HyeonwooNoh,LongOuyang,CullenO’Keefe,JakubPachocki,AlexPaino,
JoePalermo,AshleyPantuliano,GiambattistaParascandolo,JoelParish,EmyParparita,AlexPassos,Mikhail
Pavlov,AndrewPeng,AdamPerelman,FilipedeAvilaBelbutePeres,MichaelPetrov,HenriquePondede
OliveiraPinto,Michael,Pokorny,MichellePokrass,VitchyrH.Pong,TollyPowell,AletheaPower,BorisPower,
ElizabethProehl,RaulPuri,AlecRadford,JackRae,AdityaRamesh,CameronRaymond,FrancisReal,Kendra
Rimbach,CarlRoss,BobRotsted,HenriRoussez,NickRyder,MarioSaltarelli,TedSanders,ShibaniSanturkar,
GirishSastry,HeatherSchmidt,DavidSchnurr,JohnSchulman,DanielSelsam,KylaSheppard,TokiSherbakov,
JessicaShieh,SarahShoker,PranavShyam,SzymonSidor,EricSigler,MaddieSimens,JordanSitkin,Katarina
Slama,IanSohl,BenjaminSokolowsky,YangSong,NatalieStaudacher,FelipePetroskiSuch,NatalieSummers,
IlyaSutskever,JieTang,NikolasTezak,MadeleineB.Thompson,PhilTillet,AminTootoonchian,Elizabeth
Tseng,PrestonTuggle,NickTurley,JerryTworek,JuanFelipeCerónUribe,AndreaVallone,ArunVijayvergiya,
ChelseaVoss,CarrollWainwright,JustinJayWang,AlvinWang,BenWang,JonathanWard,JasonWei,CJ
Weinmann,AkilaWelihinda,PeterWelinder,JiayiWeng,LilianWeng,MattWiethoff,DaveWillner,Clemens
Winter,SamuelWolrich,HannahWong,LaurenWorkman,SherwinWu,JeffWu,MichaelWu,KaiXiao,Tao
Xu,SarahYoo,KevinYu,QimingYuan,WojciechZaremba,RowanZellers,ChongZhang,MarvinZhang,
ShengjiaZhao,TianhaoZheng,JuntangZhuang,WilliamZhuk,andBarretZoph.Gpt-4technicalreport,2024.
[34] AbhiramaSubramanyamPenamakuri,ManishGupta,MithunDasGupta,andAnandMishra.Answermining
fromapoolofimages:towardsretrieval-basedvisualquestionanswering. arXivpreprintarXiv:2306.16713,2023.
[35] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,
AmandaAskell,PamelaMishkin,JackClark,etal.Learningtransferablevisualmodelsfromnaturallanguage
supervision.InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.
[36] OriRam,YoavLevine,ItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-Brown,andYoav
Shoham.In-contextretrieval-augmentedlanguagemodels. TransactionsoftheAssociationforComputational
Linguistics,11:1316–1331,2023.
[37] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-baptisteAlayrac,
RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.Gemini1.5:Unlockingmultimodal
understandingacrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024.
[38] WeijiaShi,SewonMin,MichihiroYasunaga,MinjoonSeo,RichJames,MikeLewis,LukeZettlemoyer,and
Wen-tauYih.Replug:Retrieval-augmentedblack-boxlanguagemodels. arXivpreprintarXiv:2301.12652,2023.
[39] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,andMarcus
Rohrbach.Towardsvqamodelsthatcanread,2019.
[40] AlonTalmor,OriYoran,AmnonCatav,DanLahav,YizhongWang,AkariAsai,GabrielIlharco,Hannaneh
Hajishirzi,andJonathanBerant.Multimodalqa:Complexquestionansweringovertext,tablesandimages. arXiv
preprintarXiv:2104.06039,2021.
[41] RyotaTanaka,KyosukeNishida,KosukeNishida,TakuHasegawa,ItsumiSaito,andKunikoSaito.Slidevqa:
Adatasetfordocumentvisualquestionansweringonmultipleimages. arXivpreprintarXiv:2301.04883,2023.
[42] GoogleDeepMindGeminiTeam.Ournext-generationmodel:Gemini1.5,2024.
[43] RubènTito,DimosthenisKaratzas,andErnestValveny. Documentcollectionvisualquestionanswering. In
DocumentAnalysisandRecognition–ICDAR2021: 16thInternationalConference,Lausanne,Switzerland,
September5–10,2021,Proceedings,PartII16,pages778–792.Springer,2021.
[44] AngelinaWang,AlexanderLiu,RyanZhang,AnatKleiman,LeslieKim,DoraZhao,IrohaShirai,Arvind
Narayanan, and Olga Russakovsky. Revise: A tool for measuring and mitigating bias in visual datasets.
InternationalJournalofComputerVision,130(7):1790–1810,2022.
[45] BoxinWang, WeiPing, LawrenceMcAfee, PengXu, BoLi, MohammadShoeybi, andBryanCatanzaro.
Instructretro:Instructiontuningpostretrieval-augmentedpretraining. arXivpreprintarXiv:2310.07713,2023.
[46] ChongXiang,TongWu,ZexuanZhong,DavidWagner,DanqiChen,andPrateekMittal. Certifiablyrobust
ragagainstretrievalcorruption. arXivpreprintarXiv:2405.15556,2024.
[47] PengXu,WeiPing,XianchaoWu,LawrenceMcAfee,ChenZhu,ZihanLiu,SandeepSubramanian,Evelina
Bakhturina,MohammadShoeybi,andBryanCatanzaro.Retrievalmeetslongcontextlargelanguagemodels.
arXivpreprintarXiv:2310.03025,2023.
[48] RanXu,WenqiShi,YueYu,YuchenZhuang,YanqiaoZhu,MayDWang,JoyceCHo,ChaoZhang,and
CarlYang. Bmretriever: Tuninglargelanguagemodelsasbetterbiomedicaltextretrievers. arXivpreprint
arXiv:2404.18443,2024.
[49] MichihiroYasunaga,ArmenAghajanyan,WeijiaShi,RichJames,JureLeskovec,PercyLiang,MikeLewis,
LukeZettlemoyer,andWen-tauYih. Retrieval-augmentedmultimodallanguagemodeling. arXivpreprint
arXiv:2211.12561,2022.
12[50] LijunYu,YongCheng,KihyukSohn,JoséLezama,HanZhang,HuiwenChang,AlexanderGHauptmann,
Ming-HsuanYang,YuanHao,IrfanEssa,etal.Magvit:Maskedgenerativevideotransformer.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages10459–10469,2023.
[51] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,andLijuan
Wang.Mm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities,2023.
[52] TianjunZhang,ShishirGPatil,NamanJain,ShengShen,MateiZaharia,IonStoica,andJosephEGonzalez.
Raft:Adaptinglanguagemodeltodomainspecificrag. arXivpreprintarXiv:2403.10131,2024.
13Appendix
Inthisappendixwediscussseveraladditionalexperimentalresultsanddetailsnotincludedinthemainpaper.
• AppendixAdiscussesthecoderelease.
• AppendixBdiscussesthetrainingdatasetusedforMIRAGE.
A CodeRelease
Our code and the VHs benchmark datasets are publicly available at https://visual-haystacks.
github.io.
B MIRAGETrainingDataset
ThefulldistributionoftrainingdataisdetailedinFigureB.1.Ingeneral,thedataisprimarilycomposed
ofmulti-imageaugmenteddatafromtheLLaVAtrainingset,howeveritalsocontainsamixofdatafrom
othermulti-imagesourcesincludingRetVQA[34],SlideVQA[41],andWebQA[7].
FigureB.1:DetailedDistributionofourInstructionFinetuningData
14