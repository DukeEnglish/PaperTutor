Optimistic Q-learning
for average reward and episodic reinforcement learning
Priyank Agrawal Shipra Agrawal
Columbia University Columbia University
pa2608@columbia.edu sa3305@columbia.edu
Abstract
We present an optimistic Q-learning algorithm for regret minimization in average reward
reinforcement learning under an additional assumption on the underlying MDP that for all
policies, the expected time to visit some frequent state s0 is finite and upper bounded by H.
Our setting strictly generalizes the episodic setting and is significantly less restrictive than the
assumptionofboundedhittingtimeforallstatesmadebymostpreviousliteratureonmodel-free
algorithmsinaveragerewardsettings. WedemonstratearegretboundofO˜(H5S√AT),whereS
andAarethenumbersofstatesandactions,andT isthehorizon. Akeytechnicalnoveltyofour
1 H h
work is to introduce an L operator defined as Lv = L v where L denotes the Bellman
H h=1
operator. We showthatunder the givenassumption,the Loperatorhasa strictcontraction(in
P
span) even in the average reward setting. Our algorithm design then uses ideas from episodic
Q-learningto estimate andapply this operatoriteratively. Therefore,we provide a unified view
ofregretminimizationinepisodicandnon-episodicsettingsthatmaybeofindependentinterest.
1 Introduction
Reinforcement Learning (RL) is a paradigm for optimizing the decisions of an agent interacting
sequentially with an unknown environment over time. RL algorithms must carefully balance ex-
ploration, i.e., collecting more information, and exploitation, i.e., using the information collected so
far to maximize immediate rewards. The mathematical model underlying any RL formulation is a
Markov Decision Process (MDP). The algorithmic approaches for RL are typically categorized as
model-based or model-free, depending on whether they learn the underlying MDP model explicitly
or implicitly to learn the optimal decision policy.
Model-free approaches such as Q-learning and policy gradient work by directly learning the opti-
malvaluesorpolicy. Theyhave gainedpopularity inpractice becauseoftheirsimplicity andflexibil-
ity, andunderlie mostsuccessfulmodern deepRLalgorithms (e.g., DQN [Mnih et al.,2013], DDQN
[van Hasselt et al.,2015],A3C[Mnih et al.,2016],TRPO[Schulman et al.,2015],etc.). Technically,
an algorithm is declared to be model-free if its space complexity is o(S2A), preferably O(SA), with
S,Abeginthenumber ofstates andactions respectively [Li et al.,2021]. Atamoreconceptual level
though, the aim is to design algorithms that enjoy the structural simplicity and ease of integration
of methods like Q-learning, value iteration, policy iteration, etc.
On the theoretical side, sample complexity and regret bounds for model-free approaches have
often lagged behind the corresponding model-based approaches. The existing literature is divided
based on whether they consider settings with repeated episodes of fixed horizon H (aka episodic
1
4202
luJ
81
]GL.sc[
1v34731.7042:viXrasetting) or average reward under a single thread of experience with no restarts (aka average reward
setting). For episodic settings, recent work [Jin et al., 2018, Li et al., 2021] presented variants of
optimistic Q-learning with near-optimal regret upper bounds. In average reward settings, however,
simple UCB-based extensions of Q-learning have only been able to achieve a O˜(T2/3) regret bound
[Wei et al., 2020]. Recent √T regret bounds with model-free algorithms do so by either introducing
some elements of model-based learning (e.g. tracking pairwise state-visit counters in Zhang and Xie
[2023], Hong et al. [2024], storing exploration data in Abbasi-Yadkori et al. [2019b]) and/or by in-
troducing strong assumptions like worst-case hitting time and mixing times (e.g., Wei et al. [2021],
Hao et al. [2021]). Furthermore, (arguably) these cleverly designed algorithms often do not enjoy
the samesimplicity and flexibility that make model-free approaches like Q-learning attractive inthe
first place.
In this paper, we present an optimistic Q-learning algorithm for regret minimization in tabular
RL that is applicable in both episodic and average reward settings. The contribution of our work is
threefold.
First, we introduce a novel formulation of the problem through an assumption of "bound H on
the expected time to visit a frequent state s ". This assumption is naturally satisfied by episodic
0
settings and is significantly less restrictive than the commonly made assumption of bounded worst-
case hitting time for all states in average reward settings. Furthermore, it admits many practical
settings like repeated runs of a game or robotic task where each ‘episode’ is unlikely to last beyond
a certain maximum number of steps, but episode lengths may vary significantly across runs and
policies.
Second, we introduce a novel L operator defined as Lv = 1 H Lhv where L denotes the
H h=1
standard Bellman operator with discount factor 1. We show that under the given assumption, the
P
L operator has a strict contraction (in span) even in the average reward setting. Our algorithm
designthenusesideas fromepisodicQ-learning toestimate andapply thisoperator iteratively. This
new way to achieve strict contraction in average reward settings may be of independent interest.
Finally, we use the above insights to design a model-free algorithm that improves the existing
literature both in terms of regret bounds and simplicity of algorithmic design. Specifically, we
present an optimistic Q-learning algorithm with a regret bound of O˜(H5S√AT) in average reward
setting1 A regret bound of O˜(H6S√AT) in episodic setting follows as a corollary of this result.
In the next section, we formally present our setting, main results, and comparison to related
work. Algorithm design along with an overview of challenges and novel techniques is presented in
Section 3. Section 4 gives an outline of our regret analysis. All missing proofs are in the appendix.
2 Our setting and main results
2.1 Our setting: average reward weakly communicating MDP
We consider a Reinforcement Learning (RL) problem with an underlying Markov Decision Process
(MDP) described by tuple = ( , ,P,R), where , are state space and action space of finite
M S A S A
sizeS,Arespectively, P isthetransitionmatrixandRistherewardfunction. Theproblemproceeds
indiscreteandsequential timestepst =1,...,T. Ateachtimestept,theRLagent observesastate
s and takes an action a . The agent then observes a new state s generated according to
t t t+1
∈S ∈A
1Here, O˜ hides logarithmic factors in H,S,A,T,and additivelower order termsin T havebeen omitted.
2transition probability Pr(s = s s ,a )= P (s), and the receives a bounded reward2 r [0,1]
t+1 ′
|
t t st,at ′ t
∈
with E[r s ,a ] = R(s ,a ). The transition and reward models P,R of the underlying MDP are
t t t t t
| M
apriori unknown to the agent. Our main assumption on the MDP is the following.
M
Assumption 1 (Expected time H to hit a frequent state). There exists a state s such that
0
∈ S
under any policy (stationary or non-stationary), starting from any state s , the expected time to
∈ S
visit the state s is upper bounded by H.
0
Given Assumption 1, it is easy to see that the MDP is weakly communicating [Puterman,
M
2014]. We provide a proof in Appendix A.1 for completeness. Intuitively, this assumption also
generalizes the episodic setting with episode length H since the terminal state/starting state s is
0
visitedeveryH steps. Later,wewillusethisobservationtoprovideaformalreductionfromepisodic
to average reward setting so that our algorithm will apply to both paradigms. We assume that the
RL agent knows the upper bound H satisfying Assumption 1 but not necessarily the identity of
state s .
0
The goal of the agent is to optimize total reward over a horizon T, or equivalently minimize
regret which compares the algorithm’s total reward to the optimal reward. Specifically, in average
reward settings, regret is defined with respect to the optimal asymptotic average reward (aka gain
ρ ) of the MDP. For weakly communicating MDPs, the gain ρ is independent of the starting state
∗ ∗
and achieved by a stationary policy (see Theorem 8.3.2 of Puterman [2014]). That is,
ρ
∗
= max
π:
S→∆Aρπ(s); where ρπ(s) := lim
T
→∞
T1E T t=1R(s t,a t) |s
1
= s;a
t
= π(s t) ,
h i
P
for all s. Then, in line with the existing literature, we define regret as
Reg(T) := T (ρ R(s ,a )).
t=1 ∗ − t t
P
Let π denote optimal stationary policy. Then, along with the so called ‘bias vector’ V , defined as
∗ ∗
V (s)= lim 1E T (R(s ,a ) ρ )s = s;a = π (s ) .
∗ T →∞ T t=1 t t − ∗ | 1 t ∗ t
h i
P
ρ satisfies following Bellman equations (see [Puterman, 2014], Chapter 8) which connects it to
∗
dynamic programming based algorithms like Q-learning:
ρ +V (s)= maxR(s,a)+P V , s.
∗ ∗ s,a ∗
a · ∀
Or equivalently, using the Bellman operator L defined as [Lv](s) = max R(s,a)+P v,
a s,a
·
ρ 1+V = LV .
∗ ∗ ∗
Here 1 denotes the vector of all ones. The bias vector (or value vector) V is unique up to any
∗
constant shift. Also, under Assumption 1 it is easy to show (see Lemma A2) that the span of vector
V is bounded, specifically span(V ) 2H where span(V ) := max V (s) min V (s).
∗ ∗ ∗ s ∗ s ∗
≤ −
2Can be extendedto Gaussian or sub-Gaussian rewards using standard techniquesin theliterature.
32.2 Episodic setting as a special case
WeshowthattheproblemofregretminimizationinepisodicMDPsformsaspecialcaseofoursetting.
Consistentwiththeliterature(e.g., [Jin et al.,2018,Domingues et al.,2021]),wedefinetheepisodic
settingusingatime-inhomogeneousMDPM = ( , ,P,R,H),where(P,R) = Ph,Rh H . Under
S A { }h=1
any policy, afterexactly H steps,theMDPreaches theterminal state(say s )which isanabsorbing
0
state with reward 0. The value function Vπ(s) of (possibly non-stationary) policy π = (π ,...,π )
h 1 H
at step h is defined as the H h+ 1 step expected reward starting from state s. Then optimal
−
value for any h = 1,...,H is given by,
V (s) = max Vπ(s); where Vπ = E[ H Rh(s ,π (s ))s = s].
h∗ π h h j=h j h j | h
P
Unlike weakly communicating MDPs, here the optimal value depends on the starting state and the
optimal policy is non-stationary. The long-term regret minimization problem in episodic setting
seeks to minimize total regret over T/H episodes i.e.,
Regepisodic(T) = T V (s ) T/H H Rh(s ,a ),
H 1∗ 1 − k=1 h=1 k,h k,h
where s ,a denote the state, action at step
hP
in
ktPh
episode. It is easy to reduce the above
k,h k,h
problem toregretminimization inanaverage rewardsetting withMDPM thathasaslightly larger
′
(HS) state space. Simply construct MDPM by augmenting each state s with indices h = 1,...,H,
′
andmodifyingthetransitionmodelsothats isnotanabsorbingstatebutinsteadtransitionstothe
0
starting state s
1
with probability 1. Then, we show (Appendix A.2) that Rege Mpisodic(T) = Reg M′(T)
and M . Importantly, M satisfies Assumption 1, so that our algorithm and regret analysis directly
′ ′
applies to the episodic setting with S replaced by HS. In Appendix A.2, we also illustrate the
connection between theBellmanequations forthetwosettings whichmay beofpedagogicalinterest.
2.3 Main results
Ourmaincontributionisamodel-freeoptimisticQ-learningalgorithmfortheaveragerewardsetting
as defined in Section 2.1. A main difficulty in designing algorithms for average reward setting is
that the L operator does not have a strict contraction property when discount factor is 1. Our key
novel insight is that under Assumption 1, an operator L that we define as
Lv := 1(LHv+LH 1v+ +Lv),
H − ···
has astrictcontraction property inspan. Specifically, we prove (seeLemma 2)3 thatforany v RS,
∈
span Lv V 1 1 span(v V ). (1)
− ∗ ≤ − 4H − ∗
(cid:16) (cid:17) (cid:16) (cid:17)
This result forms the basis of our algorithm design that uses ideas from episodic Q-learning to
estimate L operator as an average of Lh operators for h = 1,...,H. Our algorithm assumes the
knowledge of T,S,A,H but does not need to know the identity of the frequent state s . Our main
0
result is the following regret bound for our algorithm.
3Obtained by substituting(H,p) by(2H,1) in Lemma 2. For completeness, in Lemma B4, we also provea more
2
standard form of span contraction property: for all v 1,v 2, span(Lv 1−Lv 2)≤(cid:0)1− 41 H(cid:1)span(v 1−v 2).
4Theorem 1 (Average reward setting). Given Assumption 1, there exists an optimistic Q-learning
algorithm (specifically, Algorithm 1 with input (2H, 1,2H)) that achieves a regret bound of
2
Reg(T) = O H5S AT log(SAT/δ)log2(T)+H9S2A log(SAT/δ)log4.5(T)
= O˜((cid:16) H5S√pAT +H9S2A), p (cid:17)
for any starting state s , with probability 1 δ.
1
−
Note that when T H9S2A, we get an O˜(H5S√AT) regret bound. Following the discussion in
≥
Section 2.2, we obtain the following corollary for the episodic setting.
Corollary 1 (Episodic setting). For episodic setting with episode length H, our algorithm (i.e.,
Algorithm 1 with input (H,1,H) and states augmented by indices h = 1,...,H) achieves a regret
bound of RegEpisodic(T) =O˜(H6S√AT +H11S2A).
Admittedly, the above regret bound is not optimal for the episodic setting where optimistic
Q-learning algorithms have achieved optimal regret bounds of O˜(H√SAT) [Li et al., 2021]. Those
algorithms,however, fundamentallyrelyonthefixedlengthepisodicstructureandcannotbeapplied
to the average reward setting even with Assumption 1. On the other hand, most algorithms for
average reward settings make assumptions like diameter and worst-case hitting time (for all states)
that are not satisfied by episodic settings (see Section 2.4 for details). Our work provides a unified
view and an algorithm that covers both these paradigms. Furthermore, as we discuss next, it
significantly improves the state-of-the-art model-free algorithms for average reward settings, both
in terms of regret bounds and simplicity of algorithm design.
Finally, our regret bounds also imply a PAC guarantee. Specifically, let π ,...,π denote the
1 T
policy used by our algorithm (Algorithm 1) in each of the T time steps. Then, we show that
picking a policy π randomly from this set (and repeating this experiment multiple times for a high
probability guarantee) provides a policy that is ǫ-optimal (i.e., ρ ρπ ǫ) with probability 1 δ,
∗
− ≤ −
where ǫ = 3Reg(T) +O(H2 Slog(T)log(1/δ) ). Its proof is deferred to Appendix F. Substituting the
T T
O˜(H5S√AT) regret boundqfrom Theorem 1, this provides a way to get (ǫ,δ)-PAC policy using
O˜(H10S2A)
samples.
ǫ2
2.4 Comparison to related work
Our work falls under the umbrella of online reinforcement learning, specifically on regret minimiza-
tion in tabular average reward settings with a weakly communicating MDP. Jaksch et al. [2010]
proved a regret lower bound of Ω(√DSAT) for this setting where D, referred to as the diameter of
the MDP, bounds the time to reach any recurrent state from another state under some policy.
Most of the earlier works on this topic focus on model-based algorithms with near-optimal regret
guaranteeofO˜(DS√AT)[Jaksch et al.,2010,Agrawal and Jia,2017],orO˜(H S√AT)Bartlett and Tewari,
∗
2012. Recently, severalpapers(e.g.,Fruit et al.[2018],Zhang and Ji[2019])improvethedependence
on S and D, with Zhang and Ji [2019] closing the gap to achieve an O˜(√H SAT) regret bound
∗
whereH = span(V ) D,however their algorithm is notefficiently implementable. Very recently,
∗ ∗
≤
[Boone and Zhang, 2024] claimed to have an algorithm that is tractable and achieves the minimax
optimal regret of O˜(√H SAT).
∗
More recently, increased interest has been in designing model-free algorithms with provable
regret bounds. However, unlike episodic MDP, where variants of Q-learning have shown to achieve
5near-optimal regret bounds [Jin et al., 2018, Li et al., 2021], there is still a significant gap between
model-free and model-based algorithms in average reward settings. Table 1 lists state-of-the-art
regret bounds for model-free algorithms (when applied to the tabular MDP case). The table may
not be comprehensive but highlights the most relevant related results.
Wei et al. [2020] presented a simple extension of episodic optimistic Q-learning from Jin et al.
[2018]totheaverage rewardcasewithregretthatgrows asT2/3. Mostsubsequentworks makemore
restrictive assumptions in order to achieve a √T regret bound. These include bounds on the mixing
time for allpolicies (t )and the time to reach any state fromany other stateunder any stationary
mix
µ∗(s)
policy (t ). Specifically, Wei et al. [2020], Hao et al. [2021] assume a bound of η on max
hit π s µπ(s)
where µπ,µ are the stationary distributions of policy π and optimal policy respectively, so that
∗ P
η S. Other works for the linear function approximation setting [Hao et al., 2021, Wei et al.,
≥
2021, Abbasi-Yadkori et al., 2019a,b] involve a parameter σ when applied to the tabular case. This
parameter σ lower bounds the probability to visit any state and action under any stationary policy,
so that 1 SA. In comparison to the above literature, our work only assumes a bound H on
σ ≥
hitting one frequent state (s ) and does not require uniform mixing. Given t = t (ǫ) for
0 mix mix
ǫ 1 , it is easy to show that our Assumption 1 is strictly weaker and holds in these settings with
≤ 2thit
H = t +2t , and similarly for t replaced by η, 1. In practice, H can be much smaller than
mix hit hit σ
t ,η, 1 especially when the state space is large and not all policies explore all states uniformly.
hit σ
One exception to the above literature is [Zhang and Xie, 2023] that requires only a bound H
∗
on span(V ), the weakest possible assumption in average reward settings. However, their regret
∗
bound suffers with a high dependence (S5) on the size of the state space. Furthermore, while in
terms of memory usage their algorithm qualifies as model-free, some features like pair-wise state-
visit counters arguably make the algorithm design closer to a model-based algorithm. In contrast,
our algorithm keeps the basic structure of optimistic Q-learning from Jin et al. [2018] intact with
some key intuitive modifications (based on the Bellman equations) to handle the average reward
setting.
Average reward
Optimistic Q-learning [Wei et al., 2020] O˜(H ∗(SA)1 3T2 3)
MDP-OOMD [Wei et al., 2020] O˜( t3 ηAT)
mix
MDP-EXP2 [Wei et al., 2021] O˜(q1 t3 T)
σ mix
UCB-AVG ([Zhang and Xie, 2023]) O˜(S5Aq2H √T)
∗
Optimistic Q-learning [this paper] O˜(H5S√AT)
Episodic
Optimistic Q learning [Jin et al., 2018] O˜(√H3SAT)
Q-EarlySettled-Advantage [Li et al., 2021] O˜(√H2SAT)
Optimistic Q-learning [this paper] O˜(H6S√AT)
Table 1: Comparison of our results to literature on model-free algorithms. Note that by preceding
discussion, η S, 1 S. Also, H t +2η and H t + 2.
≥ σ ≥ ≤ mix ≤ mix σ
Finally, aconcurrent work by Hong et al. [2024] provides anoptimistic value-iteration algorithm
6withO˜(H √S3A3T)regretbound. However, theiralgorithmcanbebestdescribedasacombination
∗
of model-based and model-free design since they need to keep track of the model (covariate matrix
Λ in linear function approximation setting reduces to tracking an estimate Pˆ of the transition
t
matrix in the tabular setting).
3 Algorithm design
Our algorithm (Algorithm 1) extends the Optimistic Q-learning algorithm of Jin et al. [2018] to a
more general setting that includes episodic settings and non-episodic settings satisfying Assump-
tion 1. For technical convenience, we present our algorithm design and analysis under the following
assumption instead, which in fact holds whenever Assumption 1 holds.
Assumption 2. There exists a state s such that under any policy (stationary or non-stationary),
0
starting from any state, the probability of visiting state s in time H at least p.
0
More precisely, for any (non-stationary) policy π = (π ,...,π ,π ...), let P denote the tran-
1 H H+1 πi
sition probability matrix for policy π , and µ be any starting state distribution. Then, we have
i
µ P +P P + + H P p1 .
⊤ π1 π1 π2 ··· i=1 πi ≥ ⊤s0
(cid:16) (cid:17)
Q
Using Markov inequality, it is easy to derive the following.
Lemma 1. Given Assumption 1, under any policy starting from any state, the probability of visiting
state s in time 2H at least p = 1/2. Therefore, Assumption 2 holds with parameters (2H, 1)
0 2
whenever Assumption 1 holds with parameter H.
Following the above observation, in the remaining paper, we work with Assumption 2 only. The
main result under Assumption 1 will be derived by simply substituting (H,p) by (2H,1/2).
3.1 Challenges and techniques
To understand the challenges and ideas used for our algorithm design, let us first consider the
Q-learning algorithm for episodic setting with a fixed episode length H. The Q-learning update
rule makes the following update to the (H h+1)-step value (and Q-value) functions Qh,Vh for
−
h 1,...,H on making an observation (s,a,s,r) (state, action, next state, reward):
′
∈{ }
• Qh(s,a) (1 α)Qh(s,a)+α(r+Vh+1(s))
′
← −
• Vh(s) max Qh(s,a)
a
←
where the terminal value VH+1(s) = 0; and α is referred to as the learning rate. Convergence of
episodic Q-learning is based on the observation that with enough samples for all states and actions,
Vh converges to LH hVH+1, so that V1 = LH0 =V . Jin et al. [2018] extend this algorithm to
− ∗
include UCB-based exploration and careful choice of α so that its episodic regret is bounded.
A natural way to extend this algorithm to a non-episodic setting satisfying Assumption 1 is to
consider steps between two consecutive visits of s as an episode. However, then Qh(s,a) represents
0
Q-value of a state and action when the expected number of remaining steps in the episode is at
most H h+1. This means we no longer have VH+1(s) = 0. With enough samples, Vh would
−
still converge to LH hVH+1, but in order to ensure convergence to V , VH+1 needs to be carefully
− ∗
updated.
7Novel ideas. The main new innovation in our algorithm is to include an update step for VH+1,
so that it converges (in span) to V . This update step (line 15 in Algorithm 1) essentially sets
∗
VH+1 as (running) average of Vh,h = 1,...,H, and is of the form:
• VH+1(s) 1 VH+1(s)+ 1 1 1 H Vh(s)
← N(s) · − N(s) H h=1
where N(s) is the number of visits(cid:16)of state s(cid:17)s(cid:16)o faPr. To see w(cid:17)hy this kind of update may lead to
convergence of VH+1 to V , observe that if Vh LH hVH+1, then after our update step
∗ −
≈
VH+1 1 H Vh 1 H LH h+1VH+1 =: LVH+1
≈ H h=1 ≈ H h=1 −
where we define operator L as LP := 1 H LHPh+1. Therefore, our algorithm essentially performs
H h=1 −
VH+1 LVH+1 repeatedly (in epochs ℓ = 1,2,...). Our main new technical insight is that under
← P
Assumption 2, L operator has the following span contraction property. Proof is in Appendix B.
Lemma 2 (Span Contraction). Define operator L : RS RS as: for any v RS, Lv :=
1 H Lhv. Then, given any V RS such that span(V→ LV ) = 0, and any∈ v RS, un-
H h=1 ∗ ∈ ∗ − ∗ ∈
der Assumption 2, we have
P
p
span(Lv V ) (1 ) span(v V ).
∗ ∗
− ≤ − H −
Therefore,byrepeatedlyapplyingVH+1 LVH+1,ouralgorithmensuresthatVH+1 getscloser
←
and closer to V in span.
∗
Another subtle difference inour algorithm compared to the episodic setting is that in every step,
thealgorithm uniformly samplesanh 1,...,H andpicks theargmax action: argmax Qh(s,a).
a
∈{ }
This uniform sampling is important to ensure that for each state s all values of h are explored.
In the next section, we provide algorithmic details that include setting exploration bonus and
learning rates similar to Jin et al. [2018], and careful epoch design for efficient estimation of VH+1.
3.2 Algorithm details
Algorithm 1 provides the detailed steps of our algorithm. It takes as input, parameters (H,p)
satisfying Assumption 2) and an upper bound H on span(V ). We assume that the time horizon
∗ ∗
T and the size of state space and action space S,A are also fixed and known. The algorithm uses
these along with parameters (H,p,H ) to define quantities C,K,b in line 2.
∗ n
The algorithm proceeds in epochs ℓ = 1,2,... of geometrically increasing duration. The
epoch break condition (see line 18) is such that the number of epochs is upper bounded by
ζ := CSlog(T) = O(1H2Slog2(T)). In each epoch, the algorithm resets and re-estimates Q-
p
values and V-values Qh(s,a),Vh(s) for all s,a and h = 1,...,H. The vector VH+1 on the other
hand is updated across epochs.
Specifically, in the beginning of every epoch ℓ, Qh(s,a),Vh(s) are reset to large enough values
for all s,a,h 1,...,H (line 5). The initialization is chosen
∈ { }
Then, in every round t of epoch ℓ, the algorithm observes state s and uniformly samples
t
h 1,...,H . Action a is picked as the arg max action: a = argmax Qht(s ,a). On playing
t t t a t
∈ { }
action a , the reward r and next state s is observed (line 7-9). The tuple (s ,a ,s ,r ) is
t t t+1 t t t+1 t
then used to update Qh(s ,a ),Vh(s ) for all h 1,...,H (and not just for h ). For each h,
t t t t
∈ { }
a Q-learning style update is performed (line 12-13). A subtle but important point to note is
8Algorithm 1 Optimistic Q-learning for average reward MDP
1: Input: Parameters (H,p,H ∗).
2: Define: For any n
≥
1, b n := 24H ∗ Clog(8 nS +A 1TH/δ) and α n := CC ++ n1; and K = 2 pH log(T) ,
C = 2K(H +2). q l m
3: Initialize: V1,H+1 = VH+1 = H,Vinit = V1,H+1, ℓ 0, τ ℓ = 0. s , N 0(s) = 0. t = 1.
≥ ∀ ∈S
4: for ℓ = 1,2,... do
5: Reset: For all s,a,1 h H, Qh(s,a) = Vh(s) = Vinit(s)+ 4(K + H), Vℓ+1,H+1(s) =
≤ ≤
Vinit(s)+4(K +H), and N(s)= N(s,a) = 0.
6: repeat
7: Observe s t.
8: Generate h t Uniform 1,...,H . Play a t = argmax aQht(s t,a).
∼ { }
9: Observe reward r t and next state s t+1.
10: n :=N(s t,a t) N(s t,a t)+1; N(s t) N(s t)+1;
← ←
11: for h = H,H 1,...,1 do
−
12: Qh(s t,a t) (1 α n)Qh(s t,a t)+α n(r t +Vh+1(s t+1)+b n)
← −
13: Vh(s t) max aQh(s t,a)
←
14: end for
15: Vℓ+1,H+1(s t)
←
N(1
st)
·Vℓ+1,H+1(s)+ 1
−
N(1
st)
H1 H h=1Vh(s t)
16: t ← t+1; τ ℓ ← τ ℓ+1. (cid:16) (cid:17)(cid:16) P (cid:17)
17: until N(s) (1+1/C)N ℓ 1(s) for some s or τ ℓ (1+1/C)τ ℓ 1
≥ − ≥ −
18: For all s, set N ℓ(s) N(s).
←
19: If (ℓ mod K) =0, set Vℓ+1,H+1 PVℓ+1,H+1, and Vinit max Vℓ −K+1,H+1,Vℓ+1 .
← ← { }
20: VH+1 Vℓ+1,H+1.
←
21: end for
that Qh,Vh are updated in the reverse order of h, i.e., h = H,H 1,...,1 (see line 11). This
−
ensures that the latest updated value for h+1 is used to construct the target for h. The updated
Vh(s ), h = 1,...,H are then used to obtain an updated value of Vℓ+1,H+1(s ) (line 15). This
t t
update is such that at the end of the epoch Vℓ+1,H+1(s ) is set as the average of Vh(s ) over all h
t t
and all rounds t in epoch ℓ.
At the end of epoch ℓ, a projection operation Vℓ+1,H+1 PVℓ+1,H+1 (line 19) is performed
←
occasionally (roughly every 2H log(T) epochs) defined as: for any v RS,
p ∈
[Pv](s) := min 2H ,v(s) min v(s) +min v(s). (2)
∗ s s
{ − ∈S } ∈S
This projection trims down the span of vector Vℓ+1,H+1 to at most 2H .
∗
4 Regret Analysis
In this section we analyze the regret of Algorithm 1. Specifically, we prove the following theorem.
9Theorem 2. Under Assumption 2, with probability at least 1 δ, the T round regret of Algorithm 1
−
is upper bounded by the following
Reg(T)= O 1 H4H S AT log(SAT/δ)log2(T)+ 1 H8H S2A log(SAT/δlog4.5(T)
p2 ∗ p4.5 ∗
= O˜ (cid:16) 1 H H4S√p AT + 1 H H8S2A . p (cid:17)
p2 ∗ p4.5 ∗
(cid:16) (cid:17)
Following the observation in Lemma 1 and the span bound on bias vector span(V ) 2H
∗
≤
under Assumption 1, Theorem 1 is a simple corollary of Theorem 2 on substituting (2H, 1,2H) for
2
(H,p,H ).
∗
In the rest of the section we provide a proof outline for Theorem 2 along with some important
intermediate lemmas. All the missing proofs from this section are in Appendix C, with some
supporting lemmas in Appendix D.
Let Qt,h,Vt,h,Nt denote the value of Qh,Vh,N at the beginning of time step t in epoch ℓ (i.e.,
before the updates of round t) in the algorithm. And, for n 1,1 i n, define
≥ ≤ ≤
αi := α n (1 α ),
n i j=i+1 − j
where as defined in the algorithm, α = (C+1Q)/(C+n). For notational convenience, we also define
n
α0 as 1 for n = 0 and 0 otherwise. Then, by algorithm construction, for any t τ +1, and any
n ≥ 0
s,a with n := Nt(s,a) 1, we have
≥
Qt,h(s,a) = R(s,a)+ n αi(Vti+1,h+1(s )+b ); (3)
i=1 n ti+1 i
otherwise (i.e., when n = 0), Qt,h(s,a) = VinPit(s) + 4(K + H) as initialized in the beginning of
epoch. And,
Vt,h(s ) = Qt,h(s ,a ), where a := argmaxQt,h(s ,a). (4)
t t t,h t,h t
a
Also, define for ℓ 2, and all s such N (s) 1,
ℓ 1
≥ − ≥
vℓ(s) := 1 1 H Vti,h(s).
Nℓ−1(s) H h=1
ti ∈epochℓ −1:sti=s
P P
Then, by algorithm construction, we have for such ℓ,s,
[Pvℓ](s), if (ℓ 1 mod K)= 0,
Vℓ,H+1(s) = − (5)
vℓ(s), otherwise.
(
(For other s, Vℓ,H+1(s) = Vinit(s)+4(K+H)as initialized inthe beginning of epoch). As discussed
inthealgorithmdesignoverview, eachepochofouralgorithmattempts toupdateVH+1 asVH+1
←
LVH+1 sothat itgets closerand closerto V due tothespan contraction property of L(see Lemma
∗
2). And the Q-learning update attempts to maintain Vh LVh+1 LH h+1VH+1. Therefore, at
−
≈ ≈
any givenepochℓ andk ℓ 1, weexpectVℓ,H+1 tobeclosetoLk Vℓ k,H+1 andVℓ,h tobecloseto
−
≤ −
LH h+1Lk Vℓ k,H+1. We show that this observation holds but with some errors due to exploration
− −
bonus and sampling estimation errors. Below, we recursively define two quantities gk(t,h) and
Gk(ℓ,s), for any k 1, aimed at capturing these errors in estimates Vt,h(s ),h = 1,...,H and in
t
≥
Vℓ,H+1(s), respectively. Here, ζ = CSlog(T) denotes an upper bound on the number of epochs in
the algorithm.
10• gk(t,h) is defined as follows: g0(t,H +1) := 0, t; and for k 1,h H,
∀ ≥ ≤
gk(t,h) := b + nt,hαi gk(t +1,h+1)+α0 (4ζKHb )
nt,h i=1 nt,h i nt,h 0
where n = Nt(s ,a ),a = arP gmax Qt,h(s ,a), and
t,h t t,h t,h a t
gk(t,H +1) := Gk 1(ℓ ,s ),k 1.
− t t
≥
where ℓ denote the epoch where times step t appears.
t
• Gk(ℓ,s) is defined as follows: G0(ℓ,s) := 0, ℓ,s; and for k 1,N (s) 1,
ℓ 1
∀ ≥ − ≥
Gk(ℓ,s) := 1 1 H gk(t ,h),
Nℓ−1(s) H h=1 i
ti ∈epochℓ −1:sti=s
P P
For s,ℓ with N (s)= 0, we set Gk(ℓ,s) = 4ζKHb .
ℓ 1 0
−
Given these definitions, our first main lemma is the following optimism property which formalizes
the intuition discussed earlier about convergence of VH+1 to Lk Vℓ k and Vh to LH hLk Vℓ k.
− − −
Lemma 3 (Optimism). With probability at least 1 δ, we have for all epochs ℓ, t epoch ℓ,
− ∈
0 Vt,h(s ) [LH h+1Lk Vℓ k,H+1](s ) 4 gk+1(t,h), (6)
t − − t
≤ − ≤
for all h = 1,...,H +1, and
0 Qt,ht(s ,a ) R(s ,a )+P LH htLk Vℓ k,H+1 4 gk+1(t,h ), (7)
≤
t t
−
t t st,at
·
− −
≤
t
(cid:16) (cid:17)
where k = ℓ 1 for ℓ K +1 and k = ℓ K ℓ K 1 1, otherwise.
− ≤ − ⌊
−K−
⌋−
Here K = 2H log(T) as defined in Algorithm 1. The proof of above lemma is in Appendix C.1.
p
Now, fix aln ℓ,t epmoch ℓ, and set k as in Lemma 3. Denote v = LH htLk Vℓ k,H+1. Then,
− −
∈
Lemma 3 provides that
Vt,ht(s ) [Lv](s ), and
t t
≥
Qt,ht(s ,a ) R(s ,a )+P v+4 gk+1(t,h ).
t t
≤
t t st,at
·
t
By the choice of action a in the algorithm, we also have Vt,ht(s ) = max Qt,ht(s ,a) = Qt,ht(s ,a ).
t t a t t t
Therefore, subtracting the above two inequalities we get
R(s ,a ) [Lv](s ) P v 4 gk+1(t,h ). (8)
t t
≥
t
−
st,at
−
t
Now, using the Bellman equations for average reward MDP (see Section 2.1), we have
ρ = [LV ](s ) V (s ) (9)
∗ ∗ t ∗ t
−
where ρ denote the optimal asymptotic average reward. Subtracting the last two inequalities and
∗
using the definition of span, we can obtain
ρ R(s ,a ) 2span(v V )+4 gk+1(t,h )+(P V V (s )) (10)
∗
−
t t
≤ −
∗ t st,at ∗
−
∗ t
Then, the following bound on the per-round regret of Algorithm 1 follows by applying the span
contraction property from Lemma 2 to bound span(v V ). Detailed Proof is in Appendix C.2.
∗
−
11Lemma 4 (Per round regret). Under Assumption 2, with probability at least 1 δ, for all epochs
−
ℓ,t epoch ℓ , the per round regret of Algorithm 1 is bounded as:
∈
ρ R(s ,a ) 8H (1 p)k +4 gk+1(t,h )+(P V V (s )) (11)
∗ − t t ≤ ∗ − H t st,at ∗ − ∗ t
where k = ℓ 1 for ℓ K +1 and k = ℓ K ℓ K 1 1, otherwise.
− ≤ − ⌊
−K−
⌋−
We obtain our cumulative regret bound by summing the above per-round regret bound over all
t epoch ℓ for all ℓ. Let k denote the value of k defined in Lemma 4 for epoch ℓ. Then,
ℓ
∈
Reg(T) = Tρ
∗ −
T t=1R(s t,a t)
≤ ℓ,t epoch ℓ
8H ∗(1
−
Hp)kℓ +4gkℓ+1(t,h t) + tδ
t
∈
(cid:16) (cid:17)
P P P
where δ := (P V V (s )). We use the careful setting of k to bound the above expression.
t st,at ∗
−
∗ t ℓ
Note that a large k is desirable for the first term which may be interpreted as ‘bias’ of our esti-
ℓ
mates compared to V . But for the second term that captures the ‘variance’ in our estimates and
∗
accumulates the exploration bonus over k +1 epochs, a small k is desirable. The choice of k in
ℓ ℓ
Lemma 4 ensures that k [K,2K 1] for ℓ K +1. When k K = 2H log(T) , clearly the
ℓ ∈ − ≥ ℓ ≥ p
first term is small enough. For bounding the second term, we prove the follolwing lemmma (Appendix
C.3).
Lemma 5. Given any sequence of 1 k +1 2K for all epochs ℓ. Then,
ℓ
≤ ≤
H T
H
H1 gkℓ+1(t,h)
≤
2eK b
nt,h
+20ζeK3H2SAb 0, (12)
h=1ℓ,t epoch ℓ h=1t=1
P ∈ P XX
where n = Nt(s ,a ),a = argmax Qt,h(s ,a), and ζ = CSlog(T) denotes an upper bound on
t,h t t,h t,h a t
the number of epochs in Algorithm 1.
The sum of all bonuses b can then be bounded using standard algebraic arguments
t,h nt,h
(see Lemma D9). The proof of regret bound stated in Theorem 2 then follows by applying the
P
above observations along with a standard concentration inequality to bound the δ term. All
t t
the missing steps of this proof are provided in Appendix C.4.
P
5 Conclusion
We presented an optimistic Q-learning algorithm for online reinforcement learning under a setting
that unifies episodic and average reward settings. Specifically, we consider MDPs with some (un-
known) frequent state s such that the expected time to reach this state from any other state is
0
bounded by a known constant H under all policies. A main technical contribution of our work is
to introduce an operator L = 1 H Lh and demonstrate its strict span contraction property in
H h=1
our setting. Using this property, we demonstrate an average reward regret bound of O˜(H5S√AT)
P
for our algorithm, with a corollary of O˜(H6S√AT) for the episodic setting. An avenue for future
research is to improve the dependence on H in our regret bound. Such an improvement was not a
focus of this work, but may be possible by employing techniques in some recent work on improving
dependence on H for episodic Q-learning, particularly [Li et al., 2021].
12References
Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba Szepesvari, and Gellért
Weisz. Politex: Regret bounds for policy iteration using expert prediction. In International
Conference on Machine Learning, pages 3692–3702. PMLR, 2019a.
Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Exploration-enhanced
politex. arXiv preprint arXiv:1908.10479, 2019b.
ShipraAgrawalandRandyJia. Optimisticposteriorsamplingforreinforcementlearning: worst-case
regret bounds. In Advances in Neural Information Processing Systems, 2017.
Shipra Agrawal and Randy Jia. Learning in structured mdps with convex cost functions:: Improved
regret bounds for inventory management. Operations Research, 70(3):1646–1664, 2022.
Peter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. arXiv preprint arXiv:1205.2661, 2012.
Victor Boone and Zihan Zhang. Achieving tractable minimax optimal regret in average reward
mdps. arXiv preprint arXiv:2406.01234, 2024.
Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, and Michal Valko. Episodic rein-
forcement learning in finite mdps: Minimax lower bounds revisited. In Algorithmic Learning
Theory, pages 578–598. PMLR, 2021.
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient bias-span-
constrained exploration-exploitation in reinforcement learning. In International Conference on
Machine Learning, pages 1578–1586. PMLR, 2018.
Botao Hao, Nevena Lazic, Yasin Abbasi-Yadkori, Pooria Joulani, and Csaba Szepesvári. Adaptive
approximate policy iteration. In International Conference on Artificial Intelligence and Statistics,
pages 523–531. PMLR, 2021.
KihyukHong,YufanZhang,andAmbujTewari. Provablyefficientreinforcementlearningforinfinite-
horizon average-reward linear mdps. arXiv preprint arXiv:2405.15050, 2024.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11:1563–1600, 2010.
ChiJin,ZeyuanAllen-Zhu,SebastienBubeck,andMichaelIJordan.IsQ-learningprovablyefficient?
Advances in neural information processing systems, 31, 2018.
GenLi,LaixiShi,YuxinChen,YuantaoGu,andYuejieChi. Breakingthesamplecomplexitybarrier
to regret-optimal model-free reinforcement learning. Advances in Neural Information Processing
Systems, 34:17762–17776, 2021.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013. URL https://arxiv.org/pdf/1312.5602.pdf.
13Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,
TimHarley,DavidSilver,andKoray Kavukcuoglu. Asynchronousmethodsfordeepreinforcement
learning, 2016.
Ian Osband, Daniel Russo, and Benjamin Van Roy. (More) efficient reinforcement learning via
posterior sampling. Advances in Neural Information Processing Systems, 26, 2013.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
region policy optimization. In Francis Bach and David Blei, editors, Proceedings of
the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Ma-
chine Learning Research, pages 1889–1897, Lille, France, 07–09 Jul 2015. PMLR. URL
https://proceedings.mlr.press/v37/schulman15.html.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning, 2015.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-free
reinforcement learning in infinite-horizon average-reward markov decision processes. In Interna-
tional conference on machine learning, pages 10170–10180. PMLR, 2020.
Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, and Rahul Jain. Learning infinite-horizon
average-reward mdps with linear function approximation. In International Conference on Artifi-
cial Intelligence and Statistics, pages 3007–3015. PMLR, 2021.
Zihan Zhang and Xiangyang Ji. Regret minimization for reinforcement learning by evaluating the
optimal bias function. Advances in Neural Information Processing Systems, 32, 2019.
Zihan Zhang and Qiaomin Xie. Sharper model-free reinforcement learning for average-reward
markov decision processes. In The Thirty Sixth Annual Conference on Learning Theory, pages
5476–5477. PMLR, 2023.
14A Preliminaries: some implications of Assumption 1
A.1 Weakly communicating and bounded span
Lemma A1. For any stationary policy π, define bias vector Vπ RS as follows,
∈
Vπ(s)= lim 1E T (R(s ,a ) ρπ(s ))s = s;a = π(s ) .
T →∞ T t=1 t t − t | 1 t t
h i
P
where ρπ(s) := lim 1E T R(s ,a )s = s;a = π(s ) is the asymptotic average reward of
T →∞ T t=1 t t | 1 t t
policy π starting from statehs. Then, under Assumption 1,i the span of the vector Vπ is upper
P
bounded by 2H.
Proof. Let Jπ(s) be the n-step value of playing policy π starting from the state s. Let state s is
n 0
reached in τ steps. Then, by our assumption E[τ] H. Therefore, since the reward in each time
≤
step is upper bounded by H, we have for every s,
Jπ(s ) H Jπ(s) Jπ(s )+H,
n 0 − ≤ n ≤ n 0
so that span(Jπ) 2H. Now, using the known result [Puterman, 2014] that for all s ,s ,
n ≤ 1 2
Vπ(s ) Vπ(s )= lim Jπ(s ) Jπ(s )
1 − 2 n n 1 − n 2
→∞
we have that the bound on span(Jπ) implies bound on the bias vector as span(Vπ) 2H.
n ≤
Lemma A2. Under Assumption 1, the MDP is weakly communicating and the span of optimal bias
vector V is upper bounded by 2H.
∗
Proof. TheweaklycommunicatingpropertycanbeseenbyapplyingProposition8.3.1(b)ofPuterman
[2014]. Consider the Markov chain induced by any stationary policy. Under Assumption 1 there is
a positive probability to go from s to s for every state s in n steps for some finite n 1. Now, for
0
≥
any recurrent state s there must be a positive probability of going from s to s in n steps for some
0
n 1 because otherwise the probability of revisiting s would be strictly less than 1. Therefore all
≥
recurrent states are reachable from each other and form a single irreducible class. All the remaining
states are transient by definition.
For a weakly communicating MDP, the optimal policy is known to be stationary Puterman
[2014]. Therefore, applying Lemma A1, we obtain span(V ) 2H
∗
≤
A.2 Episodic MDP as a special case of Average reward MDP
Here we provide a reduction that shows that the setting of regret minimization in episodic MDP as
aspecialcaseoftheweakly communicating MDPssatisfyingAssumption1. This reductionprovides
a unified view of regret minimization across episodic and non-episodic settings, and serves as the
basis for our algorithm design as an extension of an episodic Q-learning algorithm.
We describe an episodic setting consistent with the recent literature (e.g., [Jin et al., 2018,
Osband et al., 2013]). In the episodic setting with finite horizon H, we have a time-inhomogeneous
MDP described by the tuple ( , ,P,R,H), where (P,R) = Ph,Rh H . At each time step
S A { }h=1
h = 1,...,H, the learner observes the state s , and takes an action a . The MDP transitions
h h
∈ A
to a new random state s Ph and the agent receives a reward Rh(s ,a ). After H steps,
h+1 ∼ sh,ah h h
15under any policy, the MDP reaches terminal state (say s ) which is an absorbing state with reward
0
0. Optimal policy aims to optimize the value function Vπ(s) at every step h of the episode, defined
h
as the H h+1 step expected reward starting from state s under possibly non-stationary policy
−
π = (π ,...,π ):
1 H
H
Vπ(s)= E[ Rh(s ,π (s ))s = s],
h j h j | h
j=h
X
j
where the expectation is taken over the sequence s P ,a π (s ) for j = h,...,H. Then,
j+1
∼
sj,aj j
∼
h j
by dynamic programming, the optimal value is given by
V (s)= maxVπ(s)= [LV ](s) = [LH h+10](s)
h∗ h h∗+1 −
π
And, regret of an episode is defined as
V (s ) H Rh(s ,a ),
1∗ 1 − h=1 h h
P
where s ,a are the state,action at step h in the episode. Unlike weakly communicating MDPs, the
h h
optimal reward in an episode depends on the starting state and the optimal policy is non-stationary.
Thelong-term regret minimizationproblem in episodic settingseekstominimizetotalregretover
a large number (T/H) of episodes, i.e.,
T/H H
T
Regepisodic(T) = V (s ) Rh(s ,a )
H
1∗ 1
−
k,h k,h
k=1h=1
XX
where s ,a denote the state, action at step h in kth episode. We show that this problem of
k,h k,h
regret minimization in episodic MDPs is in fact equivalent to the regret minimization problem in
a weakly communicating average reward (homogenous) MDP with slightly bigger state space (HS
states). Specifically, we show the following result:
Lemma A3. Given any episodic MDP M = ( , ,P,R,H), there exists a weakly communicating
S A
MDP M = ( , ,P ,R) satisfying Assumption 1 with = H , = such that
′ ′ ′ ′ ′ ′ ′
S A |S | |S| |A| |A|
Rege Mpisodic(T) = Reg M′(T),
where Rege Mpisodic(T) and Reg M′(T) denote the episodic and average reward regrets under the MDP
M and M respectively.
′
Proof. We prove this by the following simple reduction. Given episodic MDP M = ( , ,P,RH),
S A
construct (time-homogeneous) MDP M = ( , ,P ,R) where corresponding to every state s ,
′ ′ ′ ′
S A ∈ S
the new state space contains H states denoted as sh,h = 1,...,H . A visit to state s at step
′
S { }
h in the episodic setting is then a visit to sh in MDP M . And for all s ,a ,h, we define
′
∈ S ∈ A
P (sh,a) = Ph(s,a),R(sh,a) = Rh(s,a). Further, the transition model P is modified so that s is
′ ′ ′ 0
not an absorbing state but instead transitions to starting state s with probability 1. Then any non-
1
stationary policyintheepisodicMDPM isequivalent toastationary policyπ inthenewMDPM ,
′ ′
with π (sh) = π (s). Therefore, optimal non-stationary policy for the episodic MDP corresponds
′ h
to a stationary policy for MDP M . Because s is visited every H steps, the constructed MDP M
′ 0 ′
16trivially satisfies Assumption 1 Therefore, M is weakly communicating with span of optimal bias
′
vector bounded by H.
Since V (s ) is the maximum reward obtainable per episode of fixed length H, clearly, the
1∗ 1
optimal asymptotic average reward ρ for M isρ = 1V (s ). Construct vector V as V (sh) =
∗ ′ ∗ H 1∗ 1 ∗ ∗
V (s) (H h+1)ρ ; then the dynamic programming equation for the episodic MDP implies the
h∗
− −
∗
average reward Bellman optimality equations for ρ ,V . To see this recall that we have by dynamic
∗ ∗
programming
V = LV = = LH 1V = LH0
1∗ 2∗
···
− H∗
so that for every state sh in M , we have
′
[LV ](sh) V (sh) = [LV ](s) (H h)ρ V (s)+(H h+1)ρ
∗
−
∗ h∗+1
− −
∗
−
h∗
−
∗
= ρ
∗
Therefore, V is the optimal bias vector (up to span) for MDP M . Now, in the expression for
∗ ′
Regepisodic(T) above, substitute ρ = V (s )/H and Rh(s ,a ) = R(s ,a ) where s is the
∗ 1∗ 1 k,h k,h ′ t t t
∈
S′
corresponding state to s and a = a . We obtain
k,h t k,h
T
Rege Mpisodic(T) = Tρ
∗
−
R ′(s t,a t) = Reg M′(T)
t=1
X
i.e.,theaverage(overepisodes)ofepisodicregretisthesameastheaveragerewardoftheconstructed
homogenous MDP.
This discussion demonstrates that any algorithm constructed for weakly communicating MDP
under Assumption 1 can be seamlessly applied to time in-homogeneous episodic MDP setting. The
regret bound obtained will hold almost as it is (only difference being that the size of state space S
will be changed to SH). Thus, our setting provides a unified view of the episodic and non-episodic
analysis.
B Span contraction: Proof of Lemma 2
Lemma B4. Define operator L : RS RS as: for any v RS, Lv := 1 H Lhv. Then, given
→ ∈ H h=1
any v ,v RS, under Assumption 2, we have
1 2
∈ P
span(Lv Lv ) (1 p)span(v v ).
1 − 2 ≤ − H 1 − 2
Proof. For this proof, first we show by induction: for i = 1,2,...
Liv Liv i P (v v ), (13)
1 − 2 ≤ j=1 π i1 1 − 2
(cid:16) (cid:17)
Liv Liv Qi P (v v ). (14)
1 − 2 ≥ j=1 π i2 1 − 2
(cid:16) (cid:17)
where π1 := argmax r +P Li 1v , and π2Q := argmax r +P Li 1v .
i π ∈Π π π · − 1 i π ∈Π π π · − 2
For i = 1, consider Lv Lv :
1 2
−
Lv Lv = argmaxr +P v argmaxr +P v
1 2 π π 1 π π 2
− −
π Π " π Π #
∈ ∈
P (v v ),
≥ π 12 1 − 2
17where in the last inequality we use π2 := argmax r +P v . For the upper bound, we have:
1 π Π π π 2
∈
Lv Lv = argmaxr +P v argmaxr +P v
1 2 π π 1 π π 2
− −
π Π " π Π #
∈ ∈
P (v v ),
≤ π 11 1 − 2
where in the last inequality we use π1 := argmax r +P v . Assume inequalities (13), (14) are
1 π Π π π 1
∈
true for i 1, then we have:
−
Liv Liv = L(Li 1v ) L(Li 1v )
1 2 − 1 − 2
− −
(applying the upper bound for i= 1)
P (Li 1v Li 1v )
≤ π 11 − 1 − − 2
(applying the upper bound for i 1)
−
P P (v v ),
≤ π 11 ··· π i1 1 − 2
where π1 := argmax r + P Li 1v . Similarly, we can show the lower bound statement by
i π ∈Π π π · − 1
induction.
Liv Liv = L(Li 1v ) L(Li 1v )
1 2 − 1 − 2
− −
(applying the lower bound for i = 1)
P (Li 1v Li 1v )
≥ π 12 − 1 − − 2
(applying the lower bound for i 1)
−
P P (v v ),
≥ π 12 ··· π i2 1 − 2
where π2 := argmax r +P Li 1v .
i π ∈Π π π · − 2
This completes the proof of inequalities (13),(14). We have by Assumption 2, for all starting
state distributions µ over states in ,
S
µT( H P P ) p1T , µT( H P P ) p1T .
i=1 π 11 ··· π i1 ≥ s0 i=1 π 12 ··· π i2 ≥ s0
In particular using abovP e with µ = 1 , the Dirac deltaP distribution for state s, and substituting the
s
inequalities (13), we have for all s,
H
Liv Liv (s) = 1T H (Liv Liv )
1 − 2 s i=1 1 − 2
Xi=1h i (cid:16)
P
(cid:17)
1T H P P (v v )
≤ s i=1 π 11 ··· π i1 1 − 2
≤
p1T s(cid:16) 0(Pv
1
−v 2)+(H −(cid:17) p)m sa ′x(v 1(s ′) −v 2(s ′)
= p(v (s ) v (s ))+(H p)max v (s) v (s) ,
1 0 2 0 1 ′ 2 ′
− − s′ { − }
18and similarly substituting the inequality (14)
H
Liv Liv (s) = 1T H (Liv Liv )
1 − 2 s i=1 1 − 2
Xi=1h i (cid:16)
P
(cid:17)
1T H P P (v v )
≥ s i=1 π 12 ··· π i2 1 − 2
≥
p1T s(cid:16) 0(Pv
1
−v 2)+(H −(cid:17) p)m si ′n(v 1(s ′) −v 2(s ′)
= p(v (s ) v (s ))+(H p)min v (s) v (s) .
1 0 2 0 1 ′ 2 ′
− − s′ { − }
Therefore, on subtracting the above two inequalities we have,
span 1 H (Liv Liv ) 1 p span(v v ).
H i=1 1 − 2 ≤ − H 1 − 2
(cid:16) (cid:17)
P (cid:0) (cid:1)
As a corollary to Lemma B4, we obtain Lemma 2, stated again here for easy reference.
Lemma 2 (Span Contraction). Define operator L : RS RS as: for any v RS, Lv :=
1 H Lhv. Then, given any V RS such that span(V→ LV ) = 0, and any∈ v RS, un-
H h=1 ∗ ∈ ∗ − ∗ ∈
der Assumption 2, we have
P
p
span(Lv V ) (1 ) span(v V ).
∗ ∗
− ≤ − H −
Proof. We use span(v +v ) span(v )+span(v ) and span(cv) = cspan(v), to get,
1 2 1 2
≤
span Lv V span Lv LV + 1 H span LiV V ,
− ∗ ≤ − ∗ H i=1 ∗ − ∗
(cid:16) (cid:17) (cid:16) (cid:17)
Then, substituting the result from Lemma B4 for the firstPterm, along(cid:0) with the ob(cid:1) servation that
i i
span(LiV V ) span(LjV Lj 1V ) span(LV V )= 0,
∗ ∗ ∗ − ∗ ∗ ∗
− ≤ − ≤ −
j=1 j=1
X X
we get the lemma statement.
C Missing Proofs from Section 4
C.1 Optimism: Proof of Lemma 3
Here, we prove Lemma 3. We restate it below:
Lemma 3 (Optimism). With probability at least 1 δ, we have for all epochs ℓ, t epoch ℓ,
− ∈
0 Vt,h(s ) [LH h+1Lk Vℓ k,H+1](s ) 4 gk+1(t,h), (6)
t − − t
≤ − ≤
for all h = 1,...,H +1, and
0 Qt,ht(s ,a ) R(s ,a )+P LH htLk Vℓ k,H+1 4 gk+1(t,h ), (7)
≤
t t
−
t t st,at
·
− −
≤
t
(cid:16) (cid:17)
where k = ℓ 1 for ℓ K +1 and k = ℓ K ℓ K 1 1, otherwise.
− ≤ − ⌊
−K−
⌋−
19Proof. We prove the first inequality (6) by induction on ℓ,h. The second inequality (7) about
Q-values will follow from the proof of (6).
In order to facilitate our induction-based proof, we prove the statement in (6) for two values
of k (denoted as k ) for large ℓ. Specifically, we show that the statement holds for k = ℓ 1 for
ℓ
−
ℓ K+1, and for ℓ > K+1 it holds both for k = ℓ K ℓ K 1 1 and k = ℓ K ℓ 1 1. Note
≤ − ⌊
−K−
⌋− − ⌊
K−
⌋−
that for all these values of k, we have k+1 2K and ℓ k = mK +1 for some integer m 1.
≤ − ≥
Our induction-based proof involves proving two inductive statements that depend on each other.
Induction statement 1 proves inequality (6) for ℓ,h given that it holds for ℓ,h+1,...,H +1, and
Induction statement 2 proves the inequality for ℓ,H +1 given that it holds for ℓ 1,h = 1,...,H.
−
Base case: consider ℓ = 1,h = H +1, and k = 0, then (6) reduces to
V1,H+1(s ) Vt,H+1(s ) V1,H+1(s )+4g1(t,H +1),
t t t
≤ ≤
fortinepoch1. Thisistriviallytruesinceg1(t,H+1) = G0(1,s ) = 0andVt,H+1(s ) = V1,H+1(s ).
t t t
Induction statement 1:
Induction hypothesis 1. Fix an epoch ℓ 1, and an h 1,...,H . The induction hypothesis 1 is
≥ ∈ { }
for pair (ℓ,h+1) and for the (potentially two) feasible values of k defined above: assume that with
ℓ
(ℓ 1)δ (H h)δ
probability at least 1 − − T − H− T , (6) holds for h ′ = h+1,...,H +1 for t ∈ epoch ℓ,k ℓ, and
for h ′ = 1,...,H +1,t epoch ℓ ′,k ℓ′ for ℓ ′ ℓ 1. Then, above base case provides the base case
∈ ≤ −
for this hypothesis (for ℓ = 1,h+1= H +1).
Induction step 1. We show that the inequality (6) holds for h and for the above-mentioned (poten-
tially two) values of k, for every t epoch ℓ with probability at least 1 δ . By taking union
∈ − T3H
bound over t,k, this will establish the inductive statement 1 for pair (ℓ,h), i.e., the inequality (6)
holds with probability at least 1
−
(ℓ − T1)δ
−
(H T− Hh)δ
−
T2τ 3ℓ Hδ
≥
1
−
(ℓ − T1)δ
−
(H − Hh T+1)δ for all epochs
ℓ ℓ 1,t epoch ℓ,h = 1,...,H +1 and for t epoch ℓ,h = 1,...,H +1.
′ ′ ′ ′
≤ − ∈ ∈
Let Qt,h,Vt,h,Nt denote the value of Qh,Vh,N at the beginning of time step t in epoch ℓ (i.e.,
not counting the sample at time t).
Fix a t epoch ℓ. First, let us consider the case that Nt(s ,a) = 0 for some a. In this
t
∈
case Vt,h(s ) = max Qt,h(s ,a) Vinit(s ) + 4(K + H) Now, consider the values of k that are
t a t t
≥
of interest: for ℓ < K + 1 we consider k = ℓ 1, and for ℓ > K +1, we consider two potential
−
values k = ℓ Km K 1,ℓ Km 1 where m = ℓ K 1 . Note that for ℓ > K + 1,
ℓ
− − − − − ⌊
−K−
⌋
we have that ℓ k can take two values Km + 1 and Km + K + 1. For both these values, we
ℓ
−
have k
ℓ
2K and span(Vℓ −kℓ,H+1) 4H
∗
(Lemma D7). By algorithm construction, for such ℓ,
≤ ≤
Vinit = max VKm+1,H+1,VKm+K+1,H+1 .
{ }
Therefore, if Nt(s ,a) = 0 for some a, we have the lower bound
t
Vt,h(s t) 4(K +H)+Vinit(s t) 4(K +H)+Vℓ −kℓ,H+1(s t)
≥ ≥
H h+1+2K +4H ∗+Vℓ −kℓ,H+1(s t)
≥ −
H h+1+2K +maxVℓ −kℓ,H+1(s ′)
≥ − s′
[LH −h+1LkℓVℓ −kℓ,H+1](s t) (15)
≥
where in the last inequality we used that since rewards are bounded in [0,1], each L operator can
add at most 1 to the max value of the vector.
20Similarly, we can show the upper bound for the case when n = Nt(s ,a )= 0, where a =
t,h t t,h t,h
argmax Qt,h(s ,a). InLemmaD8,weshowthatforanym 0,s, VKm+1,H+1 VKm+K+1,H+1
a t
≥ | − |≤
5(m +1)KHb 6ζHb , where ζ is an upper bound on the number of epochs in the algorithm.
0 0
≤
Therefore,
Vt,h(s ) = Qt,h(s,a ) = 4(K +H)+Vinit(s )
t t,h t
= 4(K +H)+max VKm+1,H+1(s ),VKm+K+1,H+1(s )
t t
{ }
10ζKHb 0+Vℓ −kℓ,H+1(s t)
≤
10ζKHb 0+4H ∗+minVℓ −kℓ,H+1(s ′)
≤ s′
10ζKHb 0+4H ∗+H h+1+2K +[LH −h+1LkℓVℓ −kℓ,H+1](s t)
≤ −
16ζKHb 0+[LH −h+1LkℓVℓ −kℓ,H+1](s t)
≤
= 4gkℓ+1(t,h)+[LH −h+1LkℓVℓ −kℓ,H+1](s t) (16)
where last inequality follows by initialization of gk(t,h) for the case when n = 0.
t,h
To summarize, the lower bound in induction statement 1 holds trivially if Nt(s,a) =0 for some
a, and the upper bound holds trivially if n = Nt(s ,a ) = 0.
t,h t t,h
Let us now show the lower bound for the case Nt(s ,a) 1 for all a. By algorithm design, after
t
≥
the update, for any s,a with Nt(s,a) 1, the Q estimate available at the beginning of time step t
≥
is
n
Qt,h(s,a) = αi(r +Vti+1,h+1(s )+b ),
n ti ti+1 i
i=1
X
where n = Nt(s,a), and n αi = 1.
i=1 n
Then (with some abuse of notation, in below we use n := Nt(s ,a) where identity of action a
t
P
is clear from the context). k is whichever feasible value of k (out of potentially two values) we are
ℓ
proving the inequality for.
Vt,h(s ) = maxQt,h(s ,a)
t t
a
n
= max αi(r +Vti+1,h+1(s )+b ) , where n = Nt(s ,a)
a  n ti ti+1 i  t
i=1
X
(sinc e s = s , using Lemma D11 with σ = 1, we get with probability 1 δ ,) (17)
ti t − 4TH3
n
b
max αi(R(s ,a)+Vti+1,h+1(s )+b ) n
≥ a  n t ti+1 i − 2 
i=1
X
 n 
(using b b for i 1, and αi = 1 since n 1)
i ≥ n ≥ n ≥
i=1
X
n
b
max αi(R(s ,a)+Vti+1,h+1(s )+ i )
≥ a  n t ti+1 2 
i=1
X
(using the induction hypothesis) 
21n
≥
m aax αi
n
R(s t,a)+LH −hLkℓVℓ −kℓ,H+1(s ti+1)+b i/2
Xi=1 (cid:16) (cid:17)
n
≥
m aaxR(s t,a)+ αi
n
LH −hLkℓVℓ −kℓ,H+1(s ti+1) −P(s t,a)LH −hLkℓVℓ −kℓ,H+1+b i/2
Xi=1 (cid:16) (cid:17)
+P(s t,a)LH −hLkℓVℓ −kℓ,H+1
(Lemma D11 (σ 4H from Lemma D7) give with probability 1 δ )
≤ ∗ − 4TH3
maxR(s t,a)+P(s t,a)LH −hLkℓVℓ −kℓ,H+1 (18)
≥ a
(by definition of L())
·
= [LH −h+1LkℓVℓ −kℓ,H+1](s t). (19)
For applying the concentration bound from Lemma D11 in above, we use that Vℓ −kℓ,H+1 =
VKj+1,H+1 for some integer j and due to projection step, in Lemma D7 we can show that σ =
span(VKj+1) 4H . This step explains the significance of the projection operator.
∗
≤
Similarly, for the upper bound, restrict to the case with n 1. Then,
t,h
≥
Vt,h(s ) = Qt,h(s ,a )
t t t,h
nt,h
= αi (r +Vti+1,h+1(s )+b )
nt,h ti ti+1 i
i=1
X
nt,h nt,h
= R(s ,a )+ αi (Vti+1,h+1(s )+b )+ αi r R(s ,a )
t t,h nt,h ti+1 i nt,h ti − t t,h
i=1 i=1
X X
n
(using the induction hypothesis, αib 2b , and Lemma D11, with probability 1 δ )
n i ≤ n − 4T3H
i=1
X
nt,h
≤
R(s t,a t,h)+ αi
nt,h
[LH −hLkℓVℓ −kℓ,H+1](s ti+1)+4gkℓ+1(t
i
+1,h+1) +3b
nt,h
Xi=1 (cid:16) (cid:17)
(Lemma D11 (σ 4H from Lemma D7) gives with probability 1 δ )
≤ ∗ − 4T3H
nt,h
≤
R(s t,a t,h)+P(s t,a t,h)LH −hLkℓVℓ −kℓ,H+1+4b
nt,h
+ αi
nt,h
4gkℓ+1(t
i
+1,h+1)
Xi=1 (cid:16) (cid:17)
= R(s t,a t,h)+P(s t,a t,h)LH −hLkℓVℓ −kℓ,H+1

nt,h

+4 b + αi gkℓ+1(t +1,h+1) (20)

nt,h nt,h i

 Xi=1 
 
 =gkℓ+1(t,h) 
 
LH − h|+1LkℓVℓ −kℓ,H+1{+z gkℓ+1(t,h), } (21)
≤
Therefore, we have that (6) holds for h, (potentially two feasible values of) k , and every t in epoch
ℓ
ℓ with probability at least 1 δ .
− T3H
22Induction statement 2:
Induction hypothesis 2: Fix an epoch ℓ 2, and an h 1,...,H . Assume that with probability
≥ ∈ { }
(ℓ 1)δ
at least 1 − − T , inequality (6) holds for all epochs ℓ ′ ≤ ℓ −1,h ′ = 1,...,H +1,t ∈ epoch ℓ ′
and (potentially two values of) k ℓ′ defined earlier. The base case of this statement is established by
applying Induction statement 1 for pair (ℓ 1,1).
−
Induction step 2: We show that (6) holds for all t epoch ℓ, h = H + 1, and k = k ℓ
ℓ
∈ ∈ { −
K ℓ K 1 1,ℓ K ℓ 1 1 for ℓ > K +1, and k = k = ℓ 1 otherwise. This statement will
⌊
−K−
⌋− − ⌊
K−
⌋− }
ℓ
−
provide the base case of inductive statement 1 for pair (ℓ,H +1) for ℓ 2.
≥
For h = H +1,k = k , (6) reduces to
ℓ
0 Vt,H+1(s t) [LkℓVℓ −kℓ,H+1](s t) 4 gkℓ+1(t,H +1).
≤ − ≤
By construction, for t epoch ℓ, we have Vt,H+1(s ) = Vℓ,H+1(s ). Also, by definition gk+1(t,H+
t t
∈
1) =Gk(ℓ,s ), and therefore above statement is same as: for ℓ 2,
t
≥
0 Vℓ,H+1(s t) [LkℓVℓ −kℓ,H+1](s t) 4 Gkℓ(ℓ,s t). (22)
≤ − ≤
In fact, we prove above for all s.
First consider the case when N (s) = 0. In this case Vℓ,H+1(s) does not get updated during
ℓ 1
epoch ℓ 1, and will take the ini− tial value Vℓ,H+1(s) = Vinit(s) + 4(K + H), where Vinit =
−
max VKm+1,H+1,VKm+K+1,H+1 with m = ℓ K 1 . Then, we can use the arguments similar to
{ } ⌊
−K−
⌋
(15) and (16) to show that the above statement holds trivially, since
Vinit(s)+4(K +H) Vℓ −kℓ,H+1(s)+4(K +H)
≥
maxVℓ −kℓ,H+1(s ′) 4H ∗+4(K +H)
≥ s′ −
[LkℓVℓ −kℓ,H+1](s), (23)
≥
and using Lemma D8, we have
Vinit(s)+4(K +H) minVℓ −kℓ,H+1(s ′)+6ζKHb 0+4H ∗+4(H +K)
≤ s′
4Gkℓ(ℓ,s)+[LkℓVℓ −kℓ,H+1](s) (24)
≤
where in the last equality we used that for ℓ,s with N (s) = 0 we defined Gk(ℓ,s) = 4ζKHb .
ℓ 1 0
−
Therefore, we can now restrict to the case when N (s) 1. For such s, and ℓ 2, we have
ℓ 1
− ≥ ≥
H
1 1
vℓ(s) := Vti,h(s)
N (s) H
ℓ 1
− ti ∈epoc Xhℓ −1:sti=s Xh=1
And, by algorithm construction, we have
[Pvℓ](s), if (ℓ 1 mod K) =0
Vℓ,H+1(s) = − (25)
vℓ(s), otherwise
(
23Fix any ℓ 2. First consider ℓ such that ℓ = mK + 1 for some integer m > 0. For such ℓ,
≥ 6
Vℓ,H+1 = vℓ, and k = k 1 for both the definitions of k . Then, since induction hypothesis
ℓ 1 ℓ ℓ
holds for (ℓ 1,1 h− H,),− we can use it for Vti,h,t epoch ℓ 1 to conclude
i
− ≤ ≤ ∈ −
H
1 1
vℓ(s) LH −h+1Lkℓ−1Vℓ −1 −kℓ−1,H+1(s)
≥ N (s) H
ℓ 1
− ti
∈
epoc Xh ℓ −1:sti=s Xh=1
= LkℓVℓ −kℓ,H+1 (s). (26)
h i
Similarly, for the upper bound, using the induction hypothesis, we have
H
1 1
vℓ(s) [LH −h+1Lkℓ−1Vℓ −1 −kℓ−1,H+1](s)+4gkℓ−1+1(t i,h)
≤ N (s) H
ℓ 1
− ti
∈
epoc Xh ℓ −1:sti=s Xh=1(cid:16) (cid:17)
= LkℓVℓ −kℓ,H+1 (s)+4Gkℓ(ℓ,s).
h i
It remains to prove for ℓ of form ℓ = Km+1 for some m 1. In this case we want to prove for
≥
k = ℓ (m 1)K 1 = K and k = ℓ (m)K 1 = 0. In particular, for such ℓ = Km+1 the
ℓ ℓ
− − − − −
statements we need to prove are
for k = K: 0 Vℓ,H+1(s) [LK Vℓ K,H+1](s) 4 GK(ℓ,s) , (27)
ℓ −
≤ − ≤
for k = 0: 0 Vℓ,H+1(s) [Vℓ,H+1](s) 4 G0(ℓ,s) = 0 , (28)
ℓ
≤ − ≤
The second statement in above is trivial. For the first statement we use the induction statement 1
with k = ℓ 1 K ℓ 2 1, so that k = K 1 = k 1. Therefore, we can use induction
ℓ −1
− − ⌊
K−
⌋−
ℓ −1
−
ℓ
−
hypothesis forℓ 1,h = 1,...,H, and (similar to the previous derivation for ℓ = mK+1)to obtain,
− 6
LK Vℓ K,H+1 (s) vℓ(s) LK Vℓ K,H+1 (s)+4GK(ℓ,s).
− −
≤ ≤
h i h i
Now, for ℓ = Km+1, we have Vℓ,H+1 = Pvℓ. For the upper bound, we use the property Pv
≤
v (see Lemma D6 (b)), so that Vℓ,H+1(s) = [Pvℓ](s) vℓ(s). Therefore, the upper bound in
≤
above holds for Vℓ,H+1(s) as well. For the lower bound we use the monotonicity property (Pv
≥
Pu,v u) and property that Pv = v, when span(v) 2H (see Lemma D6 (c,d)). To apply the
∗
≥ ≤
latter property observe that since ℓ K is of form Kj +1 for some j, using Lemma D7, we have
span(LK Vℓ K,H+1) 2H . And th− erefore,
− ∗
≤
Vℓ,H+1 = Pvℓ P(LK )Vℓ K,H+1 = LK Vℓ K,H+1.
− −
≥
Finally, applying the induction statement 1 for the last epoch (say ℓ = L) and h = 1, we obtain
the lemma statement with probability at least 1 Lδ 1 δ.
− T ≥ −
Inequality (7) about Q-values:
Nowweextendtheaboveprooftoderivestatement(7)aboutQ-values. Firstconsiderthecasewhen
Nt(s ,a ) = 0. Then Qt,ht(s ,a ) = Vinit(s )+4(K +H) and gk+1(t,h ) = 4ζKHb . Therefore,
t t t t t t 0
using the same derivation as that of the lower and upper bounds in (15) and (16), we can derive
that (7) holds trivially.
24AssumenowthatNt(s ,a ) 1. Letk denoteanyvalueofk forwhichinequality (6)wasproven
t t ℓ
≥
forepochℓ. Notethatbythechoiceofactiona ,wehaveVt,ht(s )= max Qt,ht(s ,a) = Qt,ht(s ,a ),
t t a t t t
where h 1,...,H . Therefore, for the lower bound, we can use the inequality (18) in the above
t
∈ { }
derivation to get
Qt,ht(s t,a t) = Vt,ht(s t) R(s t,a t)+P(s t,a t)LH −htLkℓVℓ −kℓ,H+1. (29)
≥
And the upper bound on Qt,ht follows from (20) by substituting h= h ,a = a , so that
t t,ht t
Qt,ht(s t,a t) = Vt,ht(s t) R(s t,a t)+P(s t,a t)LH −htLkℓVℓ −kℓ,H+1+4gkℓ+1(t,h t). (30)
≤
C.2 Proof of Lemma 4
We prove Lemma 4 restated here for convenience.
Lemma 4 (Per round regret). Under Assumption 2, with probability at least 1 δ, for all epochs
−
ℓ,t epoch ℓ , the per round regret of Algorithm 1 is bounded as:
∈
ρ R(s ,a ) 8H (1 p)k +4 gk+1(t,h )+(P V V (s )) (11)
∗ − t t ≤ ∗ − H t st,at ∗ − ∗ t
where k = ℓ 1 for ℓ K +1 and k = ℓ K ℓ K 1 1, otherwise.
− ≤ − ⌊
−K−
⌋−
Proof. Fix an ℓ,t epoch ℓ. Define
∈
ℓ 1, if ℓ K
k = − ≤
ℓ K ℓ K 1 1 [K,2K 1], if ℓ K +1
(
− ⌊
−K−
⌋− ∈ − ≥
Let v =LH htLk Vℓ k,H+1. Lemma 3 proves that
− −
Vt,ht(s ) [Lv](s )
t t
≥
Qt,ht(s ,a ) R(s ,a )+P v+4 gk+1(t,h )
t t
≤
t t st,at
·
t
By algorithm design, we have Vt,ht(s ) = max Qt,ht(s ,a) = Qt,ht(s ,a ). Therefore, subtracting
t a t t t
the above two inequalities we get
R(s ,a ) [Lv](s ) P v 4 gk+1(t,h ) (31)
t t
≥
t
−
st,at
−
t
On the other hand using Bellman equation, we have
ρ = [LV ](s ) V (s )
∗ ∗ t ∗ t
−
= [LV ](s ) P V +δ (32)
∗ t
−
st,at ∗ t
where we define δ := P V V (s ). Subtracting the last two equations, we get
t st,at ∗
−
∗ t
ρ R(s ,a ) [LV ](s ) Lv(s ) P (v V )+4 gk+1(t,h )+δ
∗
−
t t
≤
∗ t
−
t
−
st,at
−
∗ t t
2span(v V )+4 gk+1(t,h )+δ (33)
∗ t t
≤ −
25where we used that by the definition of L operator, for any two vectors V,V see that for any two
′
vectors LV Lv span(V V ), and also P (v V ) span(v V ). Now, substituting the
−
′
≤ −
′ st,at
−
∗
≤ −
∗
value of v, and using that span(V LV ) = 0, along with the span contraction property from
∗ ∗
−
Lemma 2 we can derive
span(v V ) = span(LH htLk Vℓ k,H+1 V )
∗ − − ∗
− −
= span(LH htLk Vℓ k,H+1 LH htLk V )
− − − ∗
−
p
(1 )kspan(Vℓ k,H+1 V )
− ∗
≤ − H −
p
(1 )k(4H ) (34)
∗
≤ − H
whereweusedthatspan(Vℓ k,H+1) 4H byLemma D7. This lemmaisapplicable sinceby above
− ∗
≤
definition of k, we have ℓ k = Kj+1 for some integer j. Substituting the bound on span(v V )
∗
− −
back in (33), we get
p
ρ R(s ,a ) 8H (1 )k +4 gk+1(t,h )+δ (35)
∗ t t ∗ t t
− ≤ − H
for all ℓ,t epoch ℓ.
∈
C.3 Proof of Lemma 5
In this section, we prove Lemma 5. The lemma is restated below.
Lemma 5. Given any sequence of 1 k +1 2K for all epochs ℓ. Then,
ℓ
≤ ≤
H T
H
H1 gkℓ+1(t,h)
≤
2eK b
nt,h
+20ζeK3H2SAb 0, (12)
h=1ℓ,t epoch ℓ h=1t=1
P ∈ P XX
where n = Nt(s ,a ),a = argmax Qt,h(s ,a), and ζ = CSlog(T) denotes an upper bound on
t,h t t,h t,h a t
the number of epochs in Algorithm 1.
We first prove the following lemma for bounding the summation over one epoch.
Lemma C5. For any epochs ℓ, h= 1,...,H and all k [1,ℓ], the following holds
∈
H
1
gk(t,h) (1+ )j h 5ζKHb SA+ b
≤ C
−  0 nt,j
t e Xpoch ℓ Xj=h t e Xpoch ℓ
∈  ∈ 
 
H
1 1
+(1+ )H h+2 gk 1(t,h) . (36)
− − ′
C  H 
t epochℓ 1 h′=1
∈ X − X
 
Proof. The proof follows by induction on h= H,H 1,...1.
−
Base Case: consider h= H, then by definition
nt,H
gk(t,H) = b +α0 (4ζKHb )+ αi Gk 1(ℓ,s ).
nt,H nt,H 0 nt,H − ti+1
i=1
X
26Therefore,
nt,H
gk(t,H) = b + α0 (4ζKHb )+ αi Gk 1(ℓ,s )
nt,H  nt,H 0 nt,H − ti+1 
t e Xpoch ℓ t e Xpoch ℓ t e Xpoch ℓ Xi=1
∈ ∈ ∈
 
b + αNt(st,at) Gk 1(ℓ,s )
≤ nt,H  n  − t+1
t
∈
e Xpoch ℓ t: t+1 ∈Xepoch ℓ n ≥N Xt(st,at)
+4ζKHb SA  
0
(using that for all i, αi 1+ 1 from Lemma E12 (c))
n i n ≤ C
≥
1
b + P (1+ )Gk 1(ℓ,s )+4ζKHb SA
≤
nt,H
C
− t+1 0
t e Xpoch ℓ t: t+1Xepoch ℓ
∈ ∈
1
b + (1+ )Gk 1(ℓ,s )+4ζKHb SA
≤
nt,H
C
− t 0
t e Xpoch ℓ t e Xpoch ℓ
∈ ∈
1
b +(1+ ) Nℓ(s)Gk 1(ℓ,s)+4ζKHb SA
≤
nt,H
C
− 0
t e Xpoch ℓ Xs
∈
(note that for s such that N (s) = 0, by definition Gk 1(ℓ,s) = 4ζKHb )
ℓ 1 − 0
−
(also by epoch break condition N (s) 1)
ℓ
≤
s:NℓX−1(s)=0
1 Nℓ(s) 1 H
= b +(1+ ) gk 1(t,h)
nt,H
C N (s) H
−
ℓ 1
t e Xpoch ℓ s:Nℓ X−1(s)>0 − t epoc Xh ℓ 1:st=s Xh=1
∈ ∈ −
+4ζKHb SA+4ζKHb
0 0
(using the epoch break condition in Algorithm 1)
H
1 1
b +(1+ )2 gk 1(t,h)
≤
nt,H
C H
−
t e Xpoch ℓ Xs t epoc Xh ℓ 1:st=s Xh=1
∈ ∈ −
+5ζKHb SA
0
H
1 1
b +(1+ )2 gk 1(t,h)+5ζKHb SA.
≤
nt,H
C H
− 0
t e Xpoch ℓ t ep Xoch ℓ 1 Xh=1
∈ ∈ −
Induction step: Assume (36) holds for h+1. We show that it holds for h.
nt,h
gk(t,h) = b + α0 (4ζKHb )+ αi gk(t +1,h+1)
nt,h  nt,h 0 nt,h i 
t e Xpoch ℓ t e Xpoch ℓ t e Xpoch ℓ Xi=1
∈ ∈ ∈
b +
 αNt(st,at)gk(t+1,h+1) 
≤ nt,h n
t
∈
e Xpoch ℓ (t+1) ∈Xepoch ℓn ≥N Xt(st,at)
+4ζKHb SA
0
(Lemma E12 (c) gives)
271
b + (1+ )gk(t+1,h+1)+4ζKHb SA
≤
nt,h
C
0
t e Xpoch ℓ (t+1) Xepoch ℓ
∈ ∈
1
b + (1+ )gk(t,h+1)+4ζKHb SA
≤
nt,h
C
0
t e Xpoch ℓ t e Xpoch ℓ
∈ ∈
(applying induction hypothesis for h+1)
H
1 1
b +(1+ ) (1+ )j (h+1)(5ζKHb SA+ b )
≤
nt,h
C C
− 0 nt,j
t e Xpoch ℓ j= Xh+1 t e Xpoch ℓ
∈ ∈
H
1 1 1
+5ζKHb SA+(1+ )(1+ )H h+1 gk 1(t,h)
0 − − ′
C C H
t ep Xoch ℓ 1 h X′=1
∈ −
H
= (1+ 1)j h(5ζKHb SA+ b )
C − 0 nt,j
Xj=h t e Xpoch ℓ
∈
H
1
+(1+ 1)H h+2 gk 1(t,h)
C − H − ′
t ep Xoch ℓ 1 h X′=1
∈ −
Corollary C5. For any epochs ℓ and all k [1,ℓ], the following holds
∈
H H
1 1
gk(t,h) (1+ )k(H+2) 5ζKHb SAk+ b
H ≤ C
 0 nt,h
Xh=1t e Xpoch ℓ Xh=1 t epo Xch ℓ...,ℓ k
∈  ∈ − 
 
Proof. Lemma C5 gives
H H
1 1
gk(t,h) (1+ )H 5ζKHb SA+ b
H ≤ C
 0 nt,h
Xh=1t e Xpoch ℓ Xh=1 t e Xpoch ℓ
∈  ∈ 
 H 
1 1
+(1+ )H+2( gk 1(t,h))
− ′
C H
t ep Xoch ℓ 1 h X′=1
∈ −
Upon recursively applying the above for k 1,k 2,...,1 we get the required statement.
− −
Lemma 5 then follows as a corollary of above results.
Proof of Lemma 5. Wesumthestatement inCorollaryC5overallℓwhilesubstitutingk byk +1
ℓ
≤
2K in the statement forepoch ℓ. Then, note that in the summation in the right hand side, the term
( H b ) for each epoch ℓ can appear multiple times (in the corresponding terms
t epoch ℓ h=1 nt,h
∈
f Por some ℓ ′ ≥Pℓ such that ℓ
≥
ℓ ′ −(k ℓ′ +1)). Indeed, because k ℓ′ +1
≤
2K for all ℓ ′, each epoch
28term can appear in at most 2K terms which gives a factor 2K, and we get
H H
1 1
gkℓ+1(t,h) 2K(1+ )2K(H+2) b +10ζK2Hb SA
H ≤ C
 nt,h 0 
Xℓ t e Xpoch ℓXh=1 Xh=1 Xℓ t e Xpoch ℓ
∈  ∈ 
 
Finally, we use that (1 + 1)2K(H+2) = (1 + 1)C e using C = 2K(H + 2) to get the lemma
C C ≤
statement.
C.4 Proof of Theorem 2
We restate the theorem for completeness
Theorem 2. Under Assumption 2, with probability at least 1 δ, the T round regret of Algorithm 1
−
is upper bounded by the following
Reg(T)= O 1 H4H S AT log(SAT/δ)log2(T)+ 1 H8H S2A log(SAT/δlog4.5(T)
p2 ∗ p4.5 ∗
= O˜ (cid:16) 1 H H4S√p AT + 1 H H8S2A . p (cid:17)
p2 ∗ p4.5 ∗
(cid:16) (cid:17)
Proof. Note that for any t epoch ℓ, Lemma 4 gives a per-round regret with k = k , where
ℓ
∈
ℓ 1, for ℓ K
k := − ≤
ℓ ℓ K ℓ K 1 1 [K,2K], for ℓ K +1
(
− ⌊
−K−
⌋− ∈ ≥
Therefore, using Lemma 4 with k = k for t epoch ℓ summing over all ℓ,t epoch ℓ, we get
ℓ
∈ ∈
T
p
Reg(T) = Tρ
∗
R(s t,a t) 8H ∗(1 )kℓ + 4gkℓ+1(t,h t)
− ≤ − H
Xt=1 ℓ,t e Xpoch ℓ ℓ,t e Xpoch ℓ
∈ ∈
+ (P V V (s )). (37)
st,at ∗
−
∗ t
t
X
Since k +1 = ℓ K ℓ K 1 2K, we can use Lemma 5 to bound the expected sum of the
ℓ
− ⌊
−K−
⌋ ≤
gk(, ) terms in the above. Using Lemma 5, we get,
· ·
H T H
E[ gkℓ+1(t,h)] 2eK b +20ζeK3H2SAb .
≤
nt,h 0
Xh=1ℓ,t e Xpoch ℓ Xt=1 Xh=1
∈
Further using Lemma D9 we have,
T H
b 24HH SACζ(T +ζSA)log(8SAT/δ).
nt,h
≤
∗
t=1h=1
XX p
And using Lemma D10, we have (note that k min ℓ 1,K where K = 2H log(T) ),
ℓ ≥ { − } p
l m
p 8H2H 8H
8H ∗(1 )kℓ ∗ + ∗ .
− H ≤ p2 T
ℓ,t e Xpoch ℓ
∈
29Further from Lemma E13 and using P(s ,a )V V (s ) H , we have with probability at
t t ∗ ∗ t+1 ∗
| − | ≤
least 1 δ,
−
T T 1
−
(P(s ,a )V V (s )) (P(s ,a )V V (s )) +H H T log(2/δ)+H .
t t ∗ ∗ t t t ∗ ∗ t+1 ∗ ∗ ∗
| − | ≤ | − | ≤
t=1 t=1
X X p
Therefore,
T
E[ (P(s ,a )V V (s ))] H T log(2/δ)+δT +H .
t t ∗ ∗ t+1 ∗ ∗
| − | ≤
t=1
X p
Substituting back in (37), we get that
Reg(T) 24eKHH SACζ(T +ζSA)log(8SAT/δ)+H T log(2/δ)
∗ ∗
≤
10H2H
+ ∗p +2δT +4(20ζeK3H2SAb ), p
p2 0
which gives a high probability regret bound with probability 1 δ as,
−
10H2H
∗
Reg(T) 24eKHH SACζ(T +ζSA)log(8SAT/δ)+
∗
≤ p2
+2δT +2Hp T log(1/δ)+4(20ζeK3H2SAb ).
∗ 0
p
Replacing δ by δ/2T, we complete the proof of the lemma. Therefore, with probability at least 1 δ
−
10H2H
∗
Reg(T) 26eKHH SACζ(T +ζSA)log(16SAT/δ)+
∗
≤ p2
+4(20ζeK3p H2SAb ).
0
Then,substitutingζ = CSlog(T)whereC = 2K(H+2),K = 2H log(T) ,andb = 24H Clog(8SATH/δ),
p 0 ∗
we obtain that with probability at least 1 δ: l m p
−
H4H S2AH8H
Reg(T) = O ∗ S AT log(SAT/δ)log2(T)+ ∗ log(SAT/δ)log4.5(T) .
p2 p4.5
!
p p
D Supporting lemma
D.1 Properties of Projection Operator
Definition 1 (Projection). Define operator P :RS RS as: for any v RS
→ ∈
[Pv](s) := min 2H ,v(s) minv(s) +minv(s).
∗
{ − s } s
∈S ∈S
We show that this operator satisfies the following properties, which will be useful in our analysis
later.
30Lemma D6. For any vector v RS,
∈
(a) span(Pv) 2H ,
∗
≤
(b) Pv v, and
≤
(c) for any vector u v, Pu Pv.
≤ ≤
(d) for any v with span(v) 2H , Pv = v.
∗
≤
Proof of (a),(b), and (d). These statements are trivially true by definition of the P operator.
Proof of (c). Fix an s. For any vector v, we say that Pv(s) is ‘clipped’ if Pv(s) = v(s), i.e., iff
6
v(s) min s′v(s ′) > 2H ∗, so that Pv(s) = 2H
∗
+min s′v(s ′) < v(s). We compare Pu(s),Pv(s) by
−
analyzing all possible four cases:
• If both Pu(s)&Pv(s) are not clipped, then Pu(s)= u(s) Pv(s) = v(s).
≤
• If both Pu(s)&Pv(s) are clipped, then Pu(s) = 2H +min u(s) 2H +min v(s) = Pv(s).
∗ s s
≤
• If Pu(s) is clipped but Pv(s) is not clipped, i.e. Pu(s) < u(s) but Pv(s) = v(s) then clearly
Pu(s) < u(s) v(s) = Pv(s).
≤
• Finally, if Pv(s) is clipped, i.e. Pv(s) < v(s) but Pu(s) = u(s), then we have u(s)
≤
2H +min u(s) and Pv(s) = 2H +min v(s). Therefore, Pv(s) Pu(s).
∗ s ∗ s
≥
D.2 Some properties of Vℓ,H+1
In this subsection, we prove some properties of the quantities Vℓ,H+1, for any ℓ of form ℓ = Kj+1
for some integer j.
Lemma D7. For any integer j 0,
≥
span LH h+1Lk VKj+1,H+1 4H , h 1,...,H +1 .
− ∗
≤ ∈ { }
(cid:16) (cid:17)
And, further for k K, we have
≥
span LH h+1Lk VKj+1,H+1 2H , h 1,...,H +1 .
− ∗
≤ ∈ { }
(cid:16) (cid:17)
Proof. Fix an h 1,...,H +1. Consider,
∈
span LH h+1Lk VKj+1,H+1 span LH h+1Lk VKj+1,H+1 V +span V
− − ∗ ∗
≤ −
(cid:16) (cid:17) (from(cid:16)Lemma 2) (cid:17) (cid:0) (cid:1)
1 p/H k span VKj+1,H+1 V +H
∗ ∗
≤ − −
(cid:16) (cid:17)
s(cid:0)pan VK(cid:1)j+1,H+1 V +H
∗ ∗
≤ −
4H , (cid:16) (cid:17)
∗
≤
31where in the last inequality we used that either from initialization (j = 0) or from projection
operation (j 1), we have span(VKj+1) 2H , so that span(VKj+1,H+1 V ) 3H . For
∗ ∗ ∗
≥ ≤ − ≤
k K, we can extend above to obtain
≥
span LH h+1Lk VKj+1,H+1 1 p/H k span VKj+1,H+1 V +H
− ∗ ∗
≤ − −
(cid:16) (cid:17) (cid:0)1 p/H(cid:1)K (3H (cid:16) )+H (cid:17)
∗ ∗
≤ −
2H ,
(cid:0) ∗ (cid:1)
≤
where in the last inequality we used that K = 2H log(T) (assuming T 3H ).
p ≥ ∗
l m
Lemma D8. For all s , and any integer m 0,
∈ S ≥
VKm+1,H+1(s) VKm+K+1,H+1(s) 5K(m+1)Hb (38)
0
| − | ≤
Proof. Fix a state s . We prove by induction on m. For the base case, consider m = 0. Then,
∈ S
(38) reduces to:
V1,H+1(s) VK+1,H+1(s) 5KHb .
0
| − | ≤
Letℓ ′ bethelargestepochsuchthat 1< ℓ ′ K+1andN ℓ′ 1(s) = 0. Ifnosuchℓ ′ exists,theninall
these epochs, Vℓ,H+1(s) is an average of V≤ h(s) at steps wh− ere s = s, which itself is some weighted
t
average of targets r + Vh+1(s ) and b , where n := Nt(s ,a ). Therefore, since rewards are
t t+1 nt t t t
boundedtobein[0,1]andb b ,wehaveacrudeupperboundVh(s) (H h)+Vℓ 1,H+1(s)+b
nt
≤
0
≤ −
− 0
for each h, yielding Vℓ,H+1(s) Vℓ 1,H+1(s) Hb . And applying this for ℓ = 2,...,K +1 we
− 0
| − | ≤
have VK+1,H+1(s) V1,H+1(s) KHb .
0
O|
n the other
ha−
nd, if such
| ℓ≤
exists, then
Vℓ′,H+1(s)
doesn’t get updated and remains at its
′
initial value 4(K+H)+Vinit(s) = 4(K+H)+V1,H+1(s) by the choice of initialization in the reset
step of Algorithm 1. And,
VK+1,H+1(s) V1,H+1(s)
| − |
VK+1,H+1(s) Vℓ′,H+1(s) + Vℓ′,H+1(s) V1,H+1(s)
≤ | − | | − |
(K +1 ℓ)Hb +4(K +H)
′ 0
≤ −
5KHb .
0
≤
wherethesecondlastinequalityfollowsasimilarargumentasthepreviouscasewhennointermediate
epochs had N (s) = 0.
ℓ
Next,forinduction, weassumethat(38)istrueforall0 m m 1,andprove itform. Letℓ
′ ′
≤ ≤ −
bethelargestepochsuchthatKm+1< ℓ ′ Km+K+1andN ℓ′ 1(s)= 0. Ifnosuchℓ ′ exists,then
using same reasoning as in the base case, w≤ e have, VKm+K+1,H+−1(s) VKm+1,H+1(s) KHb . If
0
such an ℓ exists, then Vℓ′,H+1 remains as initializ| ed in the reset step− , i.e., Vℓ′,H+1(s)| =≤ Vinit(s)+
′
4(K +H) with Vinit(s) = max VKm+1,H+1(s),VK(m 1),H+1(s) . And,
−
{ }
VKm+K+1,H+1(s) VKm+1,H+1(s)
| − |
(Km+K +1 ℓ)Hb +4(K +H)+ VKm+1,H+1(s) VK(m 1)+1,H+1(s)
′ 0 −
≤ − | − |
5K(H +1)(b )+ VKm+1,H+1(s) VK(m 1)+1,H+1(s)
0 −
≤ | − |
5K(m+1)Hb ,
0
≤
where the last inequality is by using the induction hypothesis.
32D.3 Bonus term summation
Lemma D9. We have the following,
T H
b 24HH SACζ(T +ζSA)log(8SAT/δ)
nt,h
≤
∗
t=1h=1
XX p
where n = Nt(s ,a ) with a = argmax Qt,h(s ,a), and ζ is an upper bound on the number of
t,h t t,h t,h a t
epochs (ℓ = 1,2,... in the algorithm.
Proof. From Lemma D11 We have,
T H T H
Clog(8SAT/δ)
b = 24H
nt,h ∗
s n t,h+1
t=1h=1 t=1h=1
XX XX
To bound the above, we need a bound on T H 1 .
t=1 h=1 √nt,h+1
Letn
t
= Nt(s t,a t). Then, observethatPn
t
=Pn t,ht; andgivens
t
andhistory beforetimet,h
t
= h
w.p. 1/H for all h [1,H]. Therefore,
∈
T H T
1 1 1
= E[ ].
H n +1 √n +1
t=1h=1 t,h t=1 t
XX X
Define Nℓ(s,a) as the number of visitspto (s,a) by the end of the ℓ epoch. Now,
th
ζ
1 1
=
√n +1 √n +1
t t
Xt Xℓ=1t e Xpoch ℓ
∈
ζ
1
=
√n +1
Xℓ=1 Xs,a t:st= Xs,at=a t
Nℓ(s,a)+1
1
≤ √i
ℓ s,a i=1
XX X
N (s,a)+1
ℓ
≤
ℓ s,a
XXp
(using N (s,a) = T and Cauchy-Schwartz)
ℓ s,a ℓ
ζSA(PT +PζSA).
≤
Substituting back we get, p
T H T H
1
b = 24H Clog(8SAT/δ)
nt,h ∗
n +1
t=1h=1 t=1h=1 t,h
XX p XX
T p 1
= 24HH Clog(8SAT/δ)E[ ]
∗
√n +1
t
t=1
p X
24HH Clog(8SAT/δ)( ζSA(T +ζSA).
∗
≤
p p
33D.4 Bound on the summation
ℓ,t
epoch
ℓ8H ∗(1
−
Hp)kℓ
∈
Lemma D10. For k min ℓ 1,K P with K 2H log(T) , we have the following
ℓ ≥ { − } ≥ p
l m
p 8H2H 8H
8H ∗(1 )kℓ ∗ + ∗ .
− H ≤ p2 T
ℓ,t Xepoch ℓ
∈
Proof. Recall that ζ denotes the number of epochs ℓ in the algorithm, and τ is the number of time
ℓ
steps in epoch ℓ. For ℓ K, we have k = ℓ 1, then
≤ −
K
p p
8H ∗(1 )kℓ = 8H
∗
τ ℓ(1 )kℓ
− H − H
ℓ,t e Xpoch ℓ Xℓ=1
∈
K
p
= 8H τ (1 )ℓ 1
∗ ℓ −
− H
ℓ=1
X
K
p p
8H (1+ )ℓ 1(1 )ℓ 1,
∗ − −
≤ H − H
ℓ=1
X
where we used that τ (1 + p)ℓ 1 due to the epoch break condition of τ (1 + 1)τ
(1+ p)τ .
ℓ ≤ H − ℓ ≤ C ℓ −1 ≤
H ℓ 1
−
8H ∞ (1
p2
)ℓ 1
8H2H
∗ .
∗ −
− H2 ≤ p2
ℓ=1
X
Using K = 2H log(T) , we have (1 p/H)K 1/T2. Therefore, for ℓ > K,
p − ≤
l m
T
p 8H
8H ∗(1 )kℓ ∗
− H ≤ T2
ℓ,t e Xpoch ℓ Xt=1
∈
8H /T.
∗
≤
D.5 Concentration results
Lemma D11. For a collection of random trajectory acrrued by Algorithm 1 = (s ,a ,s ,...,s )
n 1 1 2 n
G
and a given fixed vector V RS with span(V) σ and for all i= 1,...n such that,
∈ ≤
E[V(s )s ,a ,...,s ,a ]= P V, (39)
i+1
|
1 1 i i si,ai
·
where n is a stopping time with respect to filtration composed of σ field generated by and all
i i
F − G
other randomness until time index of (s ,a ), then the following holds with probability at least 1 δ,
i i
−
n
2Clog(SAT/δ)
αi V(s ) P V σ
(cid:12) n i+1 − si,ai · (cid:12) ≤ n
(cid:12)i=1 (cid:12) r
(cid:12)X (cid:0) (cid:1)(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12) 4Clog(SAT/δ)
(cid:12) (cid:12) σ .
≤ n+1
r
34Proof. Define x = αi(V(s ) P V) and consider filtration . (V)(s ) P (V)
span(V). Usingi Lemmn a E1i 2+1 (b)− , wesi h,a ai v· e nx 2Cσ2/n. We applF yi Azuma-i H+ o1 eff− dins gi,a ini e· qualit≤ y
i i ≤
(see Lemma E13 combined with an union bound over all (s,a) and all possible values of
P ∈ S ×A
n Nt(s,a) to get the following with probability at least 1 δ,
≤ −
n
2Clog(SAT/δ)
x σ .
i
≤ n
r
i=1
X
We complete the proof by using the observation that 1 1 .
n+1 ≥ 2n
E Technical preliminaries
Lemma E12 (Lemma 4.1 in Jin et al. [2018]). The following holds:
(a) 1 n αi n 2 .
√n ≤ i=1 √i ≤ √n
(b) max P αi 2C and n (αi)2 2C.
i ∈n n ≤ t i=1 n ≤ t
(c) ∞n=iαi
n ≤
1+1/C. P
LemmPa E13 (Corollary 2.1 in Wainwright [2019]). Let ( {A i,
Fi
}∞i=1) be a martingale difference
sequence, and suppose A d almost surely for all i 1. Then for all η 0,
i i
| |≤ ≥ ≥
n 2η2
P A η 2exp − . (40)
| i | ≥  ≤ n d2
i=1 i=1 i !
X
  P
In other words, with probability at most δ, we have,
n ln 2/δ n d2
A i=1 i (41)
i
| |≥ s 2
i=1 (cid:0) (cid:1)P
X
F PAC guarantee
Inadditiontoregretguarantees, ourresultsimplythatAlgorithm1isalsoaPAC-learningalgorithm.
For PAC-guarantee of (ǫ,δ), we seek a policy π such that with probability 1 δ, the policy is ǫ-
−
optimal in the sense that ρ ρπ ǫ.
∗
− ≤
We show that we can use Algorithm 1 to construct a policy with ǫ,δ)-PAC guarantee using T
samples where ǫ = 3Reg(T) +O(H ζ log(1/δ)), withζ beingthe number of epochs inthealgorithm.
T T
Substituting ζ = O(H2Slog(T)) aqnd the O˜(H5S√AT) regret bound from Theorem 1, this provides
a way to get (ǫ,δ)-PAC policy using
O˜(H10S2AT)
samples.
ǫ2
The desired policy can simply be constructed at the end of T time steps by picking one out of
the T policies π ,...,π used by the algorithm uniformly at random. As we show in Lemma F14
1 T
in the appendix, such a policy π is ǫ-optimal with probability at least 2/3. Then, repeating this
experiment 3log(1/δ) times and picking the best policy (by estimating ρπ of each policy, which by
Lemma 3 from Agrawal and Jia [2022] can be done efficiently under Assumption 1), we can obtain
the desired (ǫ,δ)-PAC-guarantee.
35Lemma F14 (PAC guarantee). Let π ,...π denote the policy used by Algorithm 1 at time step
1 T
t = 1,...,T. Consider the policy π constructed by picking one of these T policies uniformly at
random. That is, π Uniform π ,π ,...,π . Then, with probability at least 2/3, ρ ρπ ǫ,
1 2 T ∗
∼ { } − ≤
Reg(T) ζTlog(1/δ)
where ǫ = +O(H ), and ζ = CSlog(T) denotes an upper bound on the number of
T T
epochs in Algorithm 1. q
Proof. By Lemma A1, we have that ρπ(s ) = ρπ(s ) =: ρπ for all s ,s (see Puterman [2014]). To
1 2 1 2
prove that π is ǫ-optimal with probability 2/3, we first prove that for any state s, E[ρπ] ρ ǫ.
≥ ∗ − 3
Then, by Markov inequality Pr(ρ ρπ(s) ǫ) 1, and we get the desired result.
∗ − ≥ ≤ 3
Now, observe that by construction of Algorithm 1, we have
T L
1 1
E[ρπ] = ρπt = τ ρπℓ,
ℓ
T T
t=1 ℓ=1
X X
where π denotes the (stationary) policy used by the algorithm in epoch ℓ. In Lemma A1, we show
ℓ
that for every stationary policy the span of bias vector is bounded by 2H under Assumption 1.
Therefore, applying the result from Agrawal and Jia [2022] (Lemma 3, restated in our notation as
Lemma F15) that provides a bound on the difference between asymptotic average and empirical
average reward of any stationary policy that has bounded bias, we get that with probability 1 δ,
−
τ ρπℓ R(s ,a ) O(H τ log(1/δ))
ℓ t t ℓ
− ≤
(cid:12) (cid:12) t ∈ e Xpoch ℓ (cid:12) (cid:12) p
(cid:12) (cid:12)
(cid:12) (cid:12)
Summing over (at most) ζ epochs, and substituting back in the expression for E[ρπ], we get
T
TE[ρπ] R(s ,a ) O(H ζT log(1/δ))
t t
≥ −
t=1
X p
Then, substituting the definition of regret Reg(T) = Tρ T R(s ,a ), we get the desired bound
∗ − t=1 t t
on E[ρπ]:
P
Reg(T) ζT log(1/δ)
E[ρπ] ρ O(H )
∗
≥ − T − T
r
Lemma F15 (Restatement of Lemma 3 of Agrawal and Jia [2022]). Given any policy π with bias
bounded by H, let s ,a ,...,s ,a denote the states visited and actions taken at time steps k =
1 1 τ τ
1,...,τ on running the policy starting from state s for time τ which is a stopping time relative to
1
filtration = s ,a ,...,s ,a . Then,
k 1 1 k k
F { }
τ
τρπ R(s ,a ) 2H 2τ log(2/δ) (42)
k k
| − | ≤
k=1
X p
36