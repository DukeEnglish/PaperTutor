IBM LongContextGraniteCodeModels
Scaling Granite Code Models to 128K Context
MattStallone VaibhavSaxena LeonidKarlinsky BridgetMcGinn TimBula
MayankMishra AdrianaMezaSoria GaoyuanZhang AdityaPrasad YikangShen
SapthaSurendran ShanmukhaGuttula HimaPatel ParameswaranSelvam
Xuan-HongDang YanKoyfman AtinSood RogerioFeris
NirmitDesai DavidD.Cox RuchirPuri† RameswarPanda†
IBMResearch
†CorrespondingAuthors
ruchir@us.ibm.com, rpanda@ibm.com
Abstract
Thispaperintroduceslong-contextGranitecodemodelsthatsupportef-
fective context windows of up to 128K tokens. Our solution for scaling
contextlengthofGranite3B/8Bcodemodelsfrom2K/4Kto128Kconsists
ofalight-weightcontinualpretrainingbygraduallyincreasingitsRoPE
basefrequencywithrepository-levelfilepackingandlength-upsampled
long-context data. Additionally, we also release instruction-tuned mod-
els with long-context support which are derived by further finetuning
thelongcontextbasemodelsonamixofpermissivelylicensedshortand
long-context instruction-response pairs. While comparing to the origi-
nalshort-contextGranitecodemodels,ourlong-contextmodelsachieve
significant improvements on long-context tasks without any noticeable
performance degradation on regular code completion benchmarks (e.g.,
HumanEval). Wereleaseallourlong-contextGraniteCodemodelsunder
anApache2.0licenseforbothresearchandcommercialuse.
https://github.com/ibm-granite/granite-code-models
1 Introduction
Withtheemergenceanddevelopmentofrepository-levelcodingtasks(Liuetal.,2024;2023b)
andsoftwaredevelopmentagents(OpenDevinTeam,2024),longcontextlengthbecomesan
importantfeatureforcodelanguagemodels.Whilemanyproprietarylargelanguagemodels,
likeGPT4, Gemini, andClaude, support verylongcontextwindows, mostopen-source
codelanguagemodelscouldonlyproviderelativelyshortcontextwindows(CodeGemma
Teametal.,2024;Rozie`reetal.,2023). Thisshortcontextlengthlimitsthepracticalityof
open-sourcecodelanguagemodelsinreal-worldsoftwaredevelopment.
In this paper, we introduce the long-context Granite code 3B and 8B, a series of code
languagemodelsthatsupporteffectivecontextlengthsupto128Ktokens. Toachievethe
extendedcontextlength,wefirstcontinuepretrainGraniteCode3B/8Bbasemodelswith
a repository-level code corpus and upsample the longer context repositories. Then, we
instructiontunethecontinuepretrainedmodelonacombinationofshortandlongcontext
instructiondata. Duetothelackoflongcontextinstructiondata,wegeneratemulti-turn
instructiondatafromrepository-levelfile-packeddocumentswithouroriginalGranite-8B-
Code-Instructmodeltoavoidthedependencyonanexistinglongcontextmodel. More
detailsoflongcontextextensioncanbefoundinSection2.
To evaluate the ability of long-context Granite Code models, we conduct extensive ex-
perimentsonbothshortandlong-contexttasks,includingHumanEvalPack,LongCode
Completion,RepoBench-P,RepoQA,andKeyRetrieval. Experimentresultsshowthatour
long-contextmodelssignificantlyimprovelong-contextperformanceswithoutnoticeable
degradationinshort-contextperformances. Weopen-sourceallourlong-contextGranite
CodemodelsunderanApache2.0licenseforresearchandcommercialuse.
1
4202
luJ
81
]IA.sc[
1v93731.7042:viXraIBM LongContextGraniteCodeModels
2 LongContextModeling
Our solution for scaling context length of Granite code models consists of a continual
pretrainingandaninstructiontuningphase. Similartopriorworks(Fuetal.,2024),wehold
thebasichypothesisthattheabilitytoutilizeinformationatarbitraryinputlocations,is
acapabilitythatismostlyalreadyacquiredthroughlarge-scalepretraining,andthatthis
capabilitycanbereadilyextendedtocontextssubstantiallylongerthanseenduringoriginal
pretraining(e.g.,4Kto128K)throughlightweighttrainingonappropriatedatamixture.
2.1 ContinualPretraining
We continue pretrain the full attention Granite code base models using sequence paral-
lelism1(Lietal.,2021)bygraduallyincreasingitsRoPEbasefrequencywithoutusingany
sparseorlinearattention. Specifically,wecontinuepretrainGraniteCode3B/8Bbasemod-
elsusingtheoriginalpretrainingdatausedinMishraetal.(2024)butwithrepository-level
filepackingandper-languagecontextlengthupsampling,thatwefoundtobecriticalfor
long-context continual pretraining. This continued training stage focused on a curated
selection of programming languages, such as Python, C, C++, Go, Java, JavaScript, and
TypeScript,asinPinnaparajuetal.(2024).
To create long-context data, we develop a new approach that packs files from the same
repositorytogether,arrangingthemtoprioritizesemanticdependencies. Weidentifythese
dependencies by analyzing file imports and create a directed acyclic graph, where each
file is a node and edges represent API imports between files. After breaking any cycles
inthegraph,weperformatopologicalsorttoestablishanorderingoffilesbasedontheir
semanticdependencies. Wethenorganizethefilesinarepositorybyplacingdocumentation
andbuildfilesfirst,followedbytheorderedsetoffileswithsemanticdependencies,and
finallytheremainingnon-connectedfiles. Thesenon-connectedfilesarearrangedaccording
totheirfolderstructure,usingadepth-firstsearchtotraversetherepository. Finally,we
determinethedominantprogramminglanguageofarepositorybasedonfileextensions
andpresenceofbuildfiles,toorganiserepo-orderedfilesbyprogramminglanguages.
Thedocuments’lengthsandtheirsourcedomains/languagesaretwocloselyrelatedcon-
foundingfactorsindataengineeringbecauselongdatausuallycomefromparticularsources.
Thus,inadditiontorepository-levelfilepacking,weartificiallyoversampledlongerdocu-
mentsequencesonaper-languagebasistoensurethequantityoflongsequences,thereby
improvingtheoverallqualityofourtrainingdatacorpus,asin Fuetal.(2024);Yu(2023). In
particular,wedownsampledocumentsunder4096tokenstoarateof10%,whichwefindto
ensureasufficientnumberoftotaltokensanddocuments. Thetotalnumberofdocuments
withinthetrainingcorpusafterprocessingis173,336withameanlengthof73,451.
WeadjusttheRoPEbasefrequency,introducedinXiongetal.(2023),tosupportlongcontext
windowsupto128Kwherethebasemodelitselfistrainedon2K/4Kcontextlength. For
training,weadoptaprogressiveapproachwherewedoubledthecontextwindowuntil
it reached the desired length of 128K. We train for 500 steps with a batch size of 32 and
searchfortheoptimalRoPEthetaandlearningrateforeachiteration. ForRoPEtheta,we
finfoptimalvaluesof100K,250K,500K,2M,and10Mforcontextwindowsof8K,16K,32K,
64K,and128K,respectively. WetrainwithdataparallelismandFlashAttention2until64K
tokensandthen usedRingAttention (Liuet al.,2023a)toreach128Ktokens. Thefinal
modelsaretrainedforanextra4Btokenswhichisonly0.1%oforiginalpretrainingdata.
2.2 InstructionTuning
Ourtrainingdataforlongcontextinstructmodelsconsistsofacombinationofpermissively
licenseddatausedintrainingtheoriginalGranitecodeinstructmodels(Mishraetal.,2024),
inadditiontosyntheticallygeneratedcodeinstructiondatasetstailoredforsolvinglong
contextproblems. Specifically,the128Klongcontextinstructmodelsarederivedbyfurther
finetuningthelongcontextbasemodelsonamixofshortandlongcontextdataasfollows.
1https://github.com/jzhang38/EasyContext
2IBM LongContextGraniteCodeModels
Short-Context Instruction Data. Our short context instruction data consists of a com-
bination of CommitPackFT (Muennighoff et al., 2023), MathInstruct2 (Yue et al., 2023),
MetaMathQA(Yuetal.,2023),Glaive-Code-Assistant-v33,Self-OSS-Instruct-SC24,Glaive-
Function-Calling-v25,NL2SQL6,HelpSteer(Wangetal.,2023b),OpenPlatypus7(Leeetal.,
2023),andafewsyntheticallygenerateddatasetsforAPIcalling (Basuetal.,2024),and
multi-turncodeinteractionswithexecutionfeedback.
Long-ContextInstructionData. Thelongcontextinstructiondatawassyntheticallygener-
atedbybootstrappingthepretrainingdata. Foreachrepository-levelfile-packeddocument,
wecreatedamulti-turndatasetwheretheinstructionswithineachsamplewerehuman-
designedforthepurposeofenhancingthelong-contextperformanceinspecifictaskslike
generation,retrievalandtranslation. Theresponseswereeitherparsedsemanticallyfrom
theoriginaldocumentorgeneratedusingGranite-8b-Code-Instruct-4K.Thedatasetfirst
parses the document into classes, methods, and stand-alone functions. It then requests
andextractstheimplementationsofarandomsubsetoftheextractedfunctions/methods
(upto5perfileinthedocument)andthenasksforanexplanationofthatimplementation
usingavailabledocumentation. Additionally,itgeneratesinstructionsforimplementing
thesampledfunctions(methods)basedontheremainingdocumentationandcodewiththe
functionexcluded. Thesequestionsandinstructionswererepeatedfordifferentfunctions
untilthedesiredlengthwasachieved.
By exposing the model to both short and long context data, we aim to enhance its long
contextcapabilitywithoutsacrificingcodegenerationperformanceatshortinputcontext.
For finetuning, we use a multiturn loss mask for each sample, as in Wang et al. (2023a).
Thisisparticularlyimportantasourfinetuningdatacorpusconsistsofinstruction-response
pairswithmultipleturns. However,whencomposingasequence,weappendanEOStoken
aftereachresponsefromthemodeltopreventrunawaygenerationduringinference. We
followedthesametrainingparametersthatproducedourpreviousshort-contextinstruct
models(Mishraetal.,2024): 128globalbatchsize,2e-5learningrate,anoisemultiplierof5
forinputembeddings,andpadding-freetransformers.
3 Results
We evaluate our long-context Granite code models on a wide variety of benchmarks by
measuringkeyretrievalaccuracyandperformanceduringgenerationoncodecompletion
tasksatbothshortandlong-contextlengthasfollows.
3.1 Benchmarks
LongCodeCompletion. LongCodeCompletion(LCC)(Guoetal.,2023)testsamodel’s
abilitytopredictthenextlineofcodefromlongrepository-basedcontextforPython,Java,
andC#. Whilethebenchmark’scontextlengthspans1/2Kthrough8K+tokens,itisheavily
weightedaround2Ktokens. Thus,followingBaietal.(2024)andRozie`reetal.(2023),we
rebalancethisdatasetforequalrepresentationwitheachcontextlengthbucket(<4K,2–4K,
4–8K,8K+),whereeachbuckethas100sampleswhenpossible.
RepoBench-P.LikeLCC,RepoBench-P(Liuetal.,2023c)teststhemodel’snextlinecode
completionabilityforlong-contextinput. Wefollowthemethodologyin(Baietal.,2024)by
selectingtheCross-File-FirstdatabutthenwerebalancethebucketsbasedontheStarcoder
tokenizerusedfortrainingoutGranitecodemodels.
RepoQA.RepoQA(Liuetal.,2024)isanadvancedNeedle-in-the-Haystacktestthatfocuses
ontestingLLMs’capabilitiesonlong-contextcodeunderstandingandretrieval. Specifically,
2WeremovedGSM8K-RFTandCamel-MathfromMathInstructduetounknownorNClicense.
3https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3
4https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k
5https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2
6https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction
7https://huggingface.co/datasets/garage-bAInd/Open-Platypus
3IBM LongContextGraniteCodeModels
Table 1: Exact Match (EM) performance on Long Code Completion (LCC) benchmark
(Balanced). Long-context Granite code models consistently outperforms original base
modelsatdifferentinputcontextfrom4Kto32K.
Model 4KEM 8KEM 16KEM 32KEM
Granite-3b-Code-Base-2K 24.5 15.4 11.4 10.0
Granite-3b-Code-Base-128K 54.6 56.8 52.2 57.8
AbsoluteGap +30.1 +41.4 +40.8 +47.8
Granite-8b-Code-Base-4K 41.9 23.7 19.1 15.0
Granite-8b-Code-Base-128K 56.5 60.1 51.8 57.4
AbsoluteGap +14.6 +36.4 +32.7 +42.4
Table2: ExactMatch(EM)scoresonRepoBench-P(Balanced)benchmark.
Model 4KEM 8KEM 16KEM 32KEM
Granite-3b-Code-Base-2K 22.0 17.9 15.4 14.0
Granite-3b-Code-Base-128K 39.8 46.8 43.1 45.3
AbsoluteGap +17.8 +28.9 +27.7 +31.3
Granite-8b-Code-Base-4K 27.9 23.0 15.7 7.8
Granite-8b-Code-Base-128K 42.7 44.0 44.8 44.5
AbsoluteGap +14.8 +21.0 +29.1 +36.7
given a long chunk of source code and a precise function description, and the model is
askedtofindthefunctioninthecontextthatcorrespondstothedescription.Thisbenchmark
focusesonretrieving10needlefunctionsfromeachof5languagesx10repositories(500
sub-tasks/tests)withasetcontextsizeof16Ktokens.
Key Retrieval. This is a synthetic benchmark that tests the model’s ability to find and
executeaPythonfunctionburiedwithinhigh-quality, syntacticallycorrectPythoncode.
AsproposedinRozie`reetal.(2023),wetooktheCodeContestfinetuningdatasetfromLi
etal.(2022)andconcatenatedPythonsolutionsaroundthekeyfunction. Wethenaskedthe
modeltoreturntheoutputofthekeyfunctionbyemulatingaPythoninterpretershell. We
createdsequencesoflengthsof512tokensandkeyoffsetsof512tokens.
HumanEvalPack. Toevaluatemodelperformanceatshort-contextlength,weadoptHu-
manEvalPack(Muennighoffetal.,2023),whichextendsPythonproblemsofHumaneval
Benchmarktofiveadditionalcommonlyusedprogramminglanguages,namelyJavaScript,
Java,Go,C++,Rusttotestthreecodingtasks(generation,explanationandfixing). Weevalu-
ateourlong-contextmodelsinazero-shotmannerusinggreedydecodingwithcompletion
formatforthebasemodels,andwithinstructiontemplatefortheinstruction-tunedmodels.
3.2 BaseModelEvaluations
Table1andTable2showtheresultsofGranite3B/8Bcodemodelsbeforeandafterlong-
contextextensiononLCCandRepoBench-Pbenchmarksrespectively. PriorGranitecode
modelswith2K/4Ksupportfailtogeneratemeaningfulcompletionsonlongsequences.
Ontheotherhand,acrossallthecontextlength(4Kto32K),modelsscaledtohandlelong
contextsupto128Kachievesignificantlyhigherperformance. Thisdemonstratesthatlong
contextsareinformativeforcodecompletion,andlong-contextGranitecodemodelsareable
toeffectivelyleveragethisinformationtoimprovetheirgenerationsonbothbenchmarks.
In Table 3, we compare the performance of Granite code base models to their counter-
parts prior to long-context extension. Our long-context models exhibit strong retrieval
performanceacrossdifferentmatchingthresholds,whiletheshortcontextversionsmostly
fail in finding the needle function successfully. The absolute differences averaged over
5programminglanguagesareverysignificant,e.g.,+38.6%forGranite8Bmodelwitha
4IBM LongContextGraniteCodeModels
Table3: Retrievalaccuracy(%)ofGranitecodebasemodelsonRepoQAbenchmarkeval-
uatedusing16Kcontextlengthatmultiplethresholdsofmatchsimilarity. Allmodelsare
evaluatedusinggreedydecodingwith256newtokenlimit.
Threshold 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Granite-3b-Code-Base-2K
Python 6.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
C++ 6.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Java 4.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
TypeScript 7.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Rust 1.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Average 4.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Granite-3b-Code-Base-128K
Python 76.0 57.0 54.0 49.0 44.0 40.0 34.0 30.0 28.0 25.0 20.0
C++ 58.0 48.0 44.0 41.0 39.0 36.0 33.0 31.0 30.0 24.0 17.0
Java 59.0 50.0 44.0 42.0 40.0 37.0 35.0 31.0 26.0 20.0 16.0
TypeScript 58.0 38.0 34.0 33.0 29.0 27.0 23.0 23.0 23.0 16.0 7.0
Rust 57.0 38.0 36.0 32.0 30.0 29.0 28.0 24.0 24.0 19.0 16.0
Average 61.6 46.2 42.4 39.4 36.4 33.8 30.6 27.8 26.2 20.8 15.2
AbsoluteGap +56.7 +46.2 +42.4 +39.4 +36.4 +33.8 +30.6 +27.8 +26.2 +20.8 +15.2
Granite-8b-Code-Base-4K
Python 9.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
C++ 10.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 1.0 1.0
Java 11.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
TypeScript 9.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Rust 11.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Average 10.0 0.8 0.8 0.8 0.8 0.6 0.6 0.6 0.6 0.4 0.4
Granite-8b-Code-Base-128K
Python 85.0 73.0 69.0 68.0 66.0 65.0 62.0 58.0 54.0 51.0 45.0
C++ 60.0 45.0 42.0 40.0 37.0 35.0 35.0 34.0 32.0 27.0 23.0
Java 57.0 52.0 48.0 44.0 42.0 39.0 38.0 36.0 32.0 28.0 23.0
Typescript 64.0 55.0 49.0 48.0 44.0 40.0 38.0 36.0 35.0 28.0 12.0
Rust 74.0 67.0 65.0 59.0 57.0 54.0 51.0 46.0 43.0 38.0 31.0
Average 68.0 58.4 54.6 51.8 49.2 46.6 44.8 42.0 39.2 34.4 26.8
AbsoluteGap +58.0 +57.6 +54.8 +51.0 +48.6 +46.0 +44.2 +41.4 +38.6 +34.0 +26.4
matchingthresholdof0.8. Bylookingatthescoredistributionacrossdifferentprogramming
languages,wecanseethatbothmodelsaredoingbestatPython,with8Bmodelconsistently
outperformingthe3Bmodel. Thisresultshowsthatourlong-contextGranitecodemodels
canbetterunderstandnaturallanguagedescriptionbeforeretrieval,whichalignswiththe
useofadvancedcodesearchinmanypracticalsituations.
3.3 InstructModelEvaluations
Table4comparestheperformanceoflong-contextinstructmodelstotheirshort-context
counterparts on RepoQA benchmark. As can be seen, our long-context instruct models
significantlyoutperformsshort-contextversionsonall5programminglanguagesacross
different similarity thresholds. As an illustration, figure 1 demonstrates the difference
betweenshortandlong-contextmodelsatsimilaritythresholdof0.5,wheretheperformance
ofboth3Band8Binstructmodelswith2K/4Kcontextlengthsupportfailstoachievea
retrievalaccuracyofmorethan2%across5languages(onaverage0.6%vs61.6%for8B
instruct model). We attribute the improvements to the knowledge learned from newly
introducedsyntheticlongdataforinstructiontuning.
InFigure2,weinvestigatekeyretrievalperformanceofourlong-contextinstructmodelson
asyntheticbenchmarkbuiltontopofPythonsolutionsaroundakeyfunctionfromCode
Contestfinetuningdataset(Lietal.,2022). Notethatthisretrievaltaskisanalogoustothefa-
mousfamousNeedle-in-a-Haystacktest,albeittailoredtocodemodels. Ascanbeseenfrom
Figure2,our8Binstructmodelbeforelong-contextextensiononlyexhibitstrongretrieval
performanceupto4Klength,i.e.,onthesequencelengththeywereoriginallytrainedon.
5IBM LongContextGraniteCodeModels
Table4: Retrievalaccuracy(%)ofGranitecodeinstructmodelsonRepoQAbenchmarkat
differentmatchingthresholds(largerrepresentclosertoexactmatch).
Threshold 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Granite-3b-Instruct-Base-2K
Python 15.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
C++ 10.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0
Java 8.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
TypeScript 11.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Rust 9.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Average 10.6 0.2 0.2 0.2 0.2 0.2 0.0 0.0 0.0 0.0 0.0
Granite-3b-Code-Instruct-128K
Python 76.0 60.0 55.0 54.0 50.0 48.0 42.0 41.0 40.0 38.0 33.0
C++ 58.0 48.0 44.0 41.0 39.0 36.0 33.0 31.0 30.0 24.0 17.0
Java 59.0 51.0 43.0 42.0 40.0 38.0 35.0 31.0 26.0 21.0 19.0
TypeScript 80.0 68.0 54.0 50.0 43.0 39.0 36.0 35.0 29.0 20.0 9.0
Rust 67.0 44.0 36.0 33.0 32.0 29.0 28.0 26.0 24.0 20.0 16.0
Average 68.0 54.0 46.4 44.0 42.6 38.0 34.8 32.8 29.8 24.6 18.8
AbsoluteGap +77.4 +53.8 +46.2 +43.8 +42.4 +37.8 +34.8 +32.8 +29.8 +24.6 +18.8
Granite-8b-Code-Instruct-4K
Python 3.0 2.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
C++ 10.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 1.0
Java 8.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.
TypeScript 10.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Rust 4.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Average 7.0 1.0 0.8 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.2
Granite-8b-Code-Instruct-128K
Python 89.0 83.0 81.0 79.0 76.0 73.0 67.0 63.0 58.0 52.0 48.0
C++ 63.0 51.0 46.0 42.0 41.0 37.0 36.0 30.0 24.0 15.0 3.0
Java 91.0 84.0 79.0 77.0 76.0 73.0 69.0 66.0 63.0 46.0 39.0
TypeScript 86.0 84.0 80.0 72.0 68.0 62.0 56.0 49.0 40.0 25.0 11.0
Rust 83.0 78.0 73.0 67.0 65.0 63.0 60.0 55.0 53.0 48.0 40.0
Average 82.4 76.0 71.8 67.4 65.2 61.6 57.6 52.6 47.6 37.2 28.2
AbsoluteGap +75.4 +75.0 +71.0 +66.8 +64.6 +61.0 +57.0 +52.0 +47.0 +36.6 +28.0
Ontheotherhand,ourcontextscalingdemonstratesaperfect-all-greenperformancethough
wetendtoviewthatthislevelofretrievalisrelativelyeasyforlong-contextcodeLLMs.
3.4 ShortContextEvaluations
Whileourlong-contextmodelsareveryeffectiveonlongsequences,weobservethatour
long-contextscalingdoesnotsignificantlychangetheshort-contextgenericcapabilityon
standardcodesynthesisbenchmarksconsistingofshortsequences. Table5summarizesthe
resultsonHumanEvalPack,wherewefindonlyanaverage∼1%degradationforthepass@1
metricon3Band8Bmodelsrespectively. WealsotesttheHumanEval-Pythonperformance
inFigure3andobservethatlongcontextextensionhasanynoticeableperformancedegra-
dation. Interestingly,wenoticeimprovementsinHumanEvalperformanceoflong-context
instructmodels, whichweattributetoournewlong-contextsyntheticdataaddedtoin-
structiontuning. Tosummarize,whilelong-contextextensioncomesataminimalcostfor
shortsequences,webelievethiscostismorethanoffsetbythepotentialofhandlinglong
sequencesformanyrealdownstreamapplications.
4 Conclusion
Wepresentlong-contextGranitecodemodels(3Band8B)thatsupporteffectivecontext
lengths up to 128K tokens. We perform long context scaling by leveraging a simple yet
effectivestrategyconsistingofalightweightcontinualpretrainingfollowedbyinstruction
tuningonamixofshortandlong-contextdata. Ourlong-contextmodelsdemonstratemuch
superiorperformancecomparedtotheirshort-contextcounterpartswithoutsignificantly
affecting the short-context generic capability. We believe that given our current results,
6IBM LongContextGraniteCodeModels
Figure1: RetrievalaccuracyofGranite3B/8Bcodeinstructmodelsbeforeandafterscaling
to128KcontextlengthonRepoQAbenchmark(withamatchingthresholdof0.5).
Figure 2: Key retrieval (a.k.a Needle-in-a-Haystack) performance of Granite-8B-Code-
Instructwithcontextscaling. X-axisrepresentssequencelength(tokens)andY-axisrepre-
sentskeyoffsetpercentinretrieval. Bestviewedincolor.
methodstoenableevenlongercontextlengthandcircumventthequadraticcomputational
complexityofattentioncomputationwillcontinuetofurtherevolve(Gu&Dao,2023). We
plantocontinuouslyreleaseupdatestothesemodelstoimprovetheirperformanceand
bringingthebestofbreedapproachestoIBMGraniteFamily.
7IBM LongContextGraniteCodeModels
Table5: Pass@1performanceonHumanEvalPackbenchmark(Muennighoffetal.,2023). All
modelsareevaluatedusinggreedydecodingwithcompletionformatforthebasemodels,
andinstructiontemplatefortheinstruction-tunedmodels.
Model Prompt Synthesis Fix Explain Avg.
Granite-3b-Code-Base-2K Completion 33.0 19.5 22.2 24.9
Granite-3b-Code-Base-128K Completion 30.5 19.9 22.4 24.2
Granite-8b-Code-Base-4K Completion 43.1 29.1 25.4 32.5
Granite-8b-Code-Base-128K Completion 40.2 25.2 28.2 31.2
Granite-3b-Code-Instruct-2K Instruct 39.6 27.3 26.0 31.0
Granite-3b-Code-Instruct-128K Instruct 41.4 26.2 25.1 30.9
Granite-8b-Code-Instruct-4K Instruct 49.6 40.9 40.4 43.6
Granite-8b-Code-Instruct-128K Instruct 51.4 38.3 38.9 42.9
Figure3: Effectoflong-contextextensiononHumanEvalbenchmark. Whileweobservea
slightdegradationinperformanceforbasemodels,instructmodelsseeanimprovement
with long-context scaling, most likely due to our mixing of short-context SFT data with
long-contextmulti-turnsyntheticdata. Bestviewedincolor.
Acknowledgments
WewouldlikesincerelythankIBMResearchleaders-DarioGil,SriramRaghavan,Mukesh
Khare,DannyBarnett,TaliaGershon,PriyaNagpurkar,NicholasFullerfortheirsupport.
ThanksandacknowledgementtoMicheleMerler,ShivdeepSingh,ManishSethi,Pengyuan
Li, Kun-Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju
Pavuluri,BorisLublinsky,MaximiliendeBayser,IbrahimAbdelaziz,KinjalBasu,Mayank
Agarwal,YiZhou,ChrisJohnson,AanchalGoyal,YousafShah,PetrosZerfos,HeikoLud-
wig, Asim Munawar, Maxwell Crouse, Pavan Kapanipathi, Shweta Salaria, Bob Calio,
SophiaWen,SeetharamiSeelam,BrianBelgodere,CarlosFonseca,ColmMalone,RayRose,
AmithSinghee,TrentGray-Donald,XuanLiu,LuisAngelBathen,AbrahamDaniels,Anita
Govindjee,KateSoule,andLanHoang.
8IBM LongContextGraniteCodeModels
References
YushiBai,XinLv,JiajieZhang,HongchangLyu,JiankaiTang,ZhidianHuang,Zhengxiao
Du,XiaoLiu,AohanZeng,LeiHou,YuxiaoDong,JieTang,andJuanziLi. Longbench:
A bilingual, multitask benchmark for long context understanding, 2024. URL https:
//arxiv.org/abs/2308.14508.
KinjalBasu,IbrahimAbdelaziz,SubhajitChaudhury,SohamDan,MaxwellCrouse,Asim
Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, and Luis A
Lastras. Api-blend: Acomprehensivecorporafortrainingandbenchmarkingapillms.
arXivpreprintarXiv:2402.15491,2024.
CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo,
Heri Zhao, Jane Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua Howland, Kshitij
Bansal,LukeVilnis,MateoWirth,NamNguyen,PaulMichel,PeterChoy,PratikJoshi,
RavinKumar,SarmadHashmi,ShubhamAgrawal,SiqiZuo,TrisWarkentin,andZhitao
et al. Gong. Codegemma: Open code models based on gemma. 2024. URL https:
//goo.gle/codegemma.
YaoFu,RameswarPanda,XinyaoNiu,XiangYue,HannanehHajishirzi,YoonKim,and
HaoPeng. Dataengineeringforscalinglanguagemodelsto128kcontext. arXivpreprint
arXiv:2402.10171,2024.
AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselectivestatespaces.
arXivpreprintarXiv:2312.00752,2023.
DayaGuo,CanwenXu,NanDuan,JianYin,andJulianMcAuley. Longcoder: Along-range
pre-trainedlanguagemodelforcodecompletion,2023. URLhttps://arxiv.org/abs/2306.
14893.
Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful
refinementofllms. 2023.
ShengguiLi,FuzhaoXue,ChaitanyaBaranwal,YongbinLi,andYangYou. Sequenceparal-
lelism: Longsequencetrainingfromsystemperspective. arXivpreprintarXiv:2105.13120,
2021.
YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,Re´miLeblond,
Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter
Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang,
JohannesWelbl,SvenGowal,AlexeyCherepanov,JamesMolloy,DanielJ.Mankowitz,
EsmeSutherlandRobson,PushmeetKohli,NandodeFreitas,KorayKavukcuoglu,and
Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):
1092–1097,December2022. ISSN1095-9203. doi: 10.1126/science.abq1158. URLhttp:
//dx.doi.org/10.1126/science.abq1158.
HaoLiu,MateiZaharia,andPieterAbbeel. Ringattentionwithblockwisetransformersfor
near-infinitecontext,2023a. URLhttps://arxiv.org/abs/2310.01889.
JiaweiLiu,JiaLeTian,VijayDaita,YuxiangWei,YifengDing,YuhanKatherineWang,Jun
Yang,andLingmingZhang. Repoqa: Evaluatinglongcontextcodeunderstanding. arXiv
preprintarXiv:2406.06025,2024.
TianyangLiu,CanwenXu,andJulianMcAuley. Repobench: Benchmarkingrepository-level
codeauto-completionsystems. arXivpreprintarXiv:2306.03091,2023b.
TianyangLiu,CanwenXu,andJulianMcAuley. Repobench: Benchmarkingrepository-level
codeauto-completionsystems,2023c. URLhttps://arxiv.org/abs/2306.03091.
MayankMishra,MattStallone,GaoyuanZhang,YikangShen,AdityaPrasad,AdrianaMeza
Soria,MicheleMerler,ParameswaranSelvam,SapthaSurendran,ShivdeepSingh,etal.
Granitecodemodels: Afamilyofopenfoundationmodelsforcodeintelligence. arXiv
preprintarXiv:2405.04324,2024.
9IBM LongContextGraniteCodeModels
NiklasMuennighoff,QianLiu,ArmelZebaze,QinkaiZheng,BinyuanHui,TerryYueZhuo,
Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack:
Instructiontuningcodelargelanguagemodels,2023.
OpenDevinTeam. OpenDevin: AnOpenPlatformforAISoftwareDevelopersasGeneralist
Agents. https://github.com/OpenDevin/OpenDevin, 2024. Accessed: ENTER THE
DATEYOUACCESSEDTHEPROJECT.
NikhilPinnaparaju,ReshinthAdithyan,DuyPhung,JonathanTow,JamesBaicoianu,Ashish
Datta,MaksymZhuravinskyi,DakotaMahan,MarcoBellagente,CarlosRiquelme,etal.
Stablecodetechnicalreport. arXivpreprintarXiv:2404.01226,2024.
BaptisteRozie`re,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,
YossiAdi, JingyuLiu, TalRemez, Je´re´myRapin, ArtyomKozhevnikov, IvanEvtimov,
JoannaBitton,ManishBhatt,CristianCantonFerrer,AaronGrattafiori,WenhanXiong,
Alexandre De´fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas
Usunier,ThomasScialom,andGabrielSynnaeve. Codellama: Openfoundationmodels
forcode,2023.
YizhongWang,HamishIvison,PradeepDasigi,JackHessel,TusharKhot,KhyathiRaghavi
Chandu,DavidWadden,KelseyMacMillan,NoahA.Smith,IzBeltagy,andHannaneh
Hajishirzi. Howfarcancamelsgo? exploringthestateofinstructiontuningonopen
resources,2023a. URLhttps://arxiv.org/abs/2306.04751.
ZhilinWang,YiDong,JiaqiZeng,VirginiaAdams,MakeshNarsimhanSreedhar,Daniel
Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii
Kuchaiev. Helpsteer: Multi-attributehelpfulnessdatasetforsteerlm,2023b.
WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,PrajjwalBhargava,RuiHou,Louis
Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective
long-contextscalingoffoundationmodels. arXivpreprintarXiv:2309.16039,2023.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T
Kwok,ZhenguoLi,AdrianWeller,andWeiyangLiu. Metamath: Bootstrapyourown
mathematicalquestionsforlargelanguagemodels. arXivpreprintarXiv:2309.12284,2023.
YijiongYu. ”paraphrasingtheoriginaltext”makeshighaccuracylong-contextqa. arXiv
preprintarXiv:2312.11193,2023.
XiangYue,XingweiQu,GeZhang,YaoFu,WenhaoHuang,HuanSun,YuSu,andWenhu
Chen. Mammoth: Buildingmathgeneralistmodelsthroughhybridinstructiontuning.
arXivpreprintarXiv:2309.05653,2023.
10