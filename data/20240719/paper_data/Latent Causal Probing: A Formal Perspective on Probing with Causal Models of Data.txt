PublishedasaconferencepaperatCOLM2024
Latent Causal Probing:
A Formal Perspective on Probing with Causal Models of Data
CharlesJin
MITCSAIL
ccj@csail.mit.edu
Abstract
Aslanguagemodels(LMs)deliverincreasingperformanceonarangeof
NLPtasks,probingclassifiershavebecomeanindispensabletechniqueinthe
efforttobetterunderstandtheirinnerworkings. Atypicalsetupinvolves
(1)defininganauxiliarytaskconsistingofadatasetoftextannotatedwith
labels,then(2)supervisingsmallclassifierstopredictthelabelsfromthe
representations of a pretrained LM as it processed the dataset. A high
probing accuracy is interpreted as evidence that the LM has learned to
perform the auxiliary task as an unsupervised byproduct of its original
pretrainingobjective. Despitethewidespreadusageofprobes,however,
therobustdesignandanalysisofprobingexperimentsremainsachallenge.
Wedevelopaformalperspectiveonprobingusingstructuralcausalmodels
(SCM).Specifically,givenanSCMwhichexplainsthedistributionoftokens
observedduringtraining,weframethecentralhypothesisaswhetherthe
LMhaslearnedtorepresentthelatentvariablesoftheSCM.Empirically,
weextendarecentstudyofLMsinthecontextofasyntheticgrid-world
navigation task, where having an exact model of the underlying causal
structureallowsustodrawstronginferencesfromtheresultofprobing
experiments. Our techniques provide robust empirical evidence for the
abilityofLMstolearnthelatentcausalconceptsunderlyingtext.
1 Introduction
As large LMs pretrained on massive amounts of unlabeled text continue to reach new
heightsinNLPtasks(andbeyond),thequestionofwhatkindsofinformationsuchmodels
encodeabouttheirtrainingdataremainsatopicofintensediscussionandresearch. One
prominenttechniqueistosupervisesmallprobingclassifierstoextractsomelinguistically
relevantpropertyfromtherepresentationsofthepretrainedLM(Shietal.,2016;Adietal.,
2017;Alain&Bengio,2018),withtheintuitionbeingthatthesuccessoftheprobereveals
theLMhas,infact,learnedtoencodethepropertyofinterestasabyproductofitstraining.
Despite their widespread usage, however, probes themselves are also an active area of
research,withanumberofinterconnectedopenquestionsinthedesignandinterpretation
ofprobingexperiments(Belinkov,2022),including:
(Q1) Control and interpretation. Given that the probe itself is directly supervised to
performtheauxiliarytask,theobservedoutcomescoulddependnotonlyontheinformation
inherentlyencodedintheLMbutalsotheabilityoftheprobetoextracttheinformationitself.
Forinstance,researchershavefoundthattrainingprobestopredictrandomizedlabelscan
oftenyieldcomparablyhighaccuraciesoncertaintasks,callingintoquestionthesignificance
ofpriorresults(Hewitt&Liang,2019). Asaresult,drawingrobustconclusionsfromthe
classificationaccuracyofaproberemainsupfordebate.
(Q2)Classifierselectionandtraining. Tocombattheriskofmeasuringtheprobe’scapacity
tolearntheauxiliarytask,researchersoftenlimitprobestolowcapacityarchitecturessuch
as linear classifiers (Maudslay et al., 2020). However, other works have countered with
evidencethatLMsencodemorecomplexconceptsusingnon-linearrepresentations,which
canonlybeaccuratelymeasuredusinghighercapacityclassifiers(Belinkov&Glass,2019;
1
4202
luJ
81
]LC.sc[
1v56731.7042:viXraPublishedasaconferencepaperatCOLM2024
Lietal.,2022). Arelatedquestionwhichhasreceivedlittleattentionishowthetraining
procedureitself(e.g.,optimizerselection,traininghyperparameters,auxiliarydatasetsize)
interactswiththeoutcomeoftheprobingexperiment.
(Q3)Auxiliarytaskdesign. Finally,aslarge,pretrainedLMshaveprogressedfromproduc-
inghuman-liketexttoexhibitingincreasingly“intelligent”behaviorssuchasreasoningand
in-contextlearning(Brownetal.,2020),thereisanemergingneedtobetterunderstandthe
limitationsandcapabilitiesofLMsalongdimensionssuchworldknowledgeandtheoryof
mind. Thesedomainspresentadistinctsetofchallengescomparedtotraditionallinguistic
taskssuchaspart-of-speechtagginganddependencyparsing.
Thetheoreticalsectionofthispaperdevelopsaformalperspectiveonprobingusingthe
languageofstructuralcausalmodels(SCM).Specifically,givenacausalmodelwhichexplains
the distribution of tokens observed during training, we pose the central hypothesis as
determiningwhethertheLMhaslearnedtorepresentthelatentcausalvariablesoftheSCM:
conceptsthatexplainhowthetextwasgenerated,butareneverdirectlyobservedduring
training. Wethenintroduceprobesasameansofempiricallytestingsuchhypotheses,by
extractingthevalueofthelatentconceptsgivenonlytheLMrepresentationsasinput. Our
settingnaturallycapturesbroaderquestionsabouttheinductivebiasofLMstrainedsolely
ontext,andthelatentconceptstheyacquireoverthecourseoftraining(Q3).
Next,byextendingtheSCMbeyondthedatagenerationprocesstocoverthetrainingof
theLM(unsupervised)andprobe(supervised),wefurthershowthatQ1andQ2canbe
understoodasthemediatingandmoderatingeffectsoftheprobe,respectively. Wepropose
ageneraltechniquebasedoncausalmediationanalysiswhichisolatesthecausalpaththrough
theLMwhileexcludingtheprobe’sinfluence. Ouranalysisyieldsclear,testableconditions
foracceptingorrejectingourhypothesesbasedonaprobingexperiment’soutcomes.
Finally,weconductanempiricalstudythatextendstheexperimentalsettingintroduced
byJin&Rinard(2023),whouseprobestoquantifytheextenttowhichLMsarecapable
oflearning“meaning”fromtext,asoperationalizedbythesemanticsofasyntheticpro-
gramminglanguageforgrid-worldnavigation. Byleveragingtheproposedlatentcausal
probingframework,ourexperimentsallowustodrawpreciseconclusionsaboutthecausal
relationshipbetweenthelatentdynamicsthatgeneratedthetrainingdataandwhatislearned
bytheLM.Inparticular,wefindevidencethat(1)theLMhas,infact,learnedtorepresent
thelatentvariablescorrespondingtotheunderlyingsemanticsofthelanguage,and(2)the
LMrepresentationsexhibitaninductivebiasthatgeneralizestonovelactionsequences. Our
studymarksthefirstrigorousempiricalevaluationofthehypothesisthatlanguagemodelsare
latentconceptlearners,revealingintriguinginsightsintohowlanguagemodelsmightacquire
anunderstandingoflanguage.
2 Structuralcausalmodelsoftext
Thissectionintroducesthesettingofourframeworkforprobing, whichisbasedonthe
ideathatthetextusedtotrainLMsmayexhibitlatentcausalstructure;weformalizethese
conceptsusingtheapproachofstructuralcausalmodels.
2.1 Background: structuralcausalmodels
Structuralcausalmodelsaregraphicalmodelswhichrepresentcausalrelationshipsina
datagenerationprocessasadirectedgraphicalmodel(Pearletal.,2000). Weillustratethe
keyconceptswithanexample;wereferthereadertoPearl(2010)foramorecomprehensive
overview. Suppose that we are interested in the effect the weather has on employees
bringing an umbrella to work. In this case, we may hypothesize a SCM like the one in
Figure 1a. Each node represents a different random variable: the weather, the weather
forecast,whethertheemployee’smorninggetsofftoalatestart,andwhethertheemployee
bringsanumbrellatowork. Nodeswithoutparentsareexogenousvariables,whosecauses
areleftunexplained; theyareoftenusedtomodelnature, randomness, orotheraspects
ofphysicalreality,suchasgeneticorenvironmentalfactors. Theexogenousvariablesin
Figure1aaretheweatherandhavingalatestart.Nodeswithaparentindicatethepossibility
2PublishedasaconferencepaperatCOLM2024
Forecast Late start do(Forecast = sun) Late start
Weather Umbrella Weather Umbrella
(a)TheoriginalSCM. (b)Interveningontheforecast.
Figure1: AnSCMforbringinganumbrellatowork.
ofacausalrelationship,e.g.,theedgefromweathertoforecastindicatesthattheweather
mightinfluencetheforecast. Inparticular,everymissingedgeintheSCMassertsthelack
ofacausalrelationship. Astandardassumptionofcausalanalysisisthattheunderlying
causalgraphisMarkovian(oracyclic);weadoptthisassumptionaswell.
Mediatorsandmoderators. Returningnowtoouroriginalquestionofhowtheweather
affectsemployeesbringinganumbrellatowork,wenotetheSCMhypothesizes3possible
causes: theweather,theweatherforecast,andhavingalatestart. Theforecastisamediator
becausetotalcausaleffectoftheweatheronumbrellainpartiallytransferredbythepath-
specificeffectovertheweather-forecast-umbrellapathway(Avinetal.,2005;Imaietal.,2010).
Anaturalquestionishowmuchtheforecastisresponsiblefortheincreaseinlikelihood
thatanemployeebringsanumbrellatoworkwhen,forinstance,theweatherchangesfrom
sunnytorainy. Oneanswerisgivenbythenecessaryindirecteffect,whichquantifieshow
muchthepresenceofthecausalpaththroughthemediatorcontributestothetotalmeasured
effect(Weinberger,2019):
NIE (Forecast)
=E(cid:2) Umbrella|Weather=rain(cid:3)
rain,sun
−E(cid:2) Umbrella|Weather=rain,do(Forecast=sun)(cid:3)
,
wheredo(Forecast=sun)isacausalintervention. Theinterventioncanbeconceptualizedas
forcingtheweatherstationtoforecastasunnydayindependentoftheweather,therebysev-
eringtheweather-forecast-umbrellapathway. Figure1bdepictstheSCMpost-intervention.
The late start variable is not a mediator of the weather-umbrella causal effect (because
thereisnopathfromtheweathertoumbrellathatpassesthroughit),butitcouldstillbe
amoderator: variablesthatdonotdirectlymediateacausaleffect,butaffectthestrength
(andpossiblydirection)ofanothercausalpath(Baron&Kenny,1986). Forinstance,the
forecast’seffectonwhetheranemployeebringsanumbrella(i.e.,theNIE)mightbelowerif
theemployeehasalatestartandrushesoutthedoorwithoutcheckingtheforecast.
2.2 Casestudy: causalstructureinprogramminglanguages
Jin & Rinard (2023) propose an experiment to study whether LMs are able to ground a
sequenceofactionsintoasequenceofstates,havingonlyseenexamplesoftheinitialand
finalstateduringtraining. Specifically,theytraina350MparameterTransformer(Vaswani
et al., 2017) on a corpus of specification-program pairs using a standard causal language
modelingobjective. Theprogramsarestringsinagrid-worldnavigationlanguagewith
5 actions (move, turn_right, turn_left, put_marker, pick_marker), sampled uniformly
betweenlengths6and10,inclusive. Thespecificationsconsistoftheinitialandfinalstate,
whichare8x8grids. Executingtheprogramnavigatesasinglerobotintheinitialstateto
thefinalstate. WerefertoJin&Rinard(2023)formoredetailsaboutthelanguage.
Figure2displaysanSCMofthedatagenerationprocess(alongwithanexampleassignment
ofvaluestoeachvariable). Theexogeneousvariablesaretheinitialstateandtheprogram
actions.Eachactionproducesalatentstate(green),saveforthelastaction,whichisobserved
asthefinalstate. Atrainingsampleconsistsofthesequence: s ,s ,p ,...,p ,whereeach
0 n 1 n
gridworldisconvertedtotextbyscanninginroworder,withonetokenperentry.
3PublishedasaconferencepaperatCOLM2024
intermediate states
initial state final state
1 1 1 1
1 1 1 ... 1
1 1 1 1 1 1 1 1
...
Figure2: AnSCMofthedatagenerationprocessforthegridworldcorpus. Theexogenous
variables are the initial state and the actions; latent variables are green and observed
variablesaregray. Thetrainingcorpusconsistsofprogramsoflengthbetween6and10.
ConsidernowmodelingadistributionoftextdrawnfromthisSCM.Inparticular,foreach
sample x there is an assignment e to the exogenous variables in the SCM M such that
M(e) = x. OnestrategywouldbetolearnamodeloftheSCMdepictedinFigure2,and
integratethelatentvariablesduringinference. Forinstance,knowingthattherobotisone
spaceawayfroms
n
ins n−1couldhelpalearnerpredict p
n
=move.
Moregenerally,givenobservationsgeneratedaccordingtosomeunknowncausalmecha-
nism,alearnercouldproposevariousSCMsoftheunderlyingcausalstructureconsistent
withtheobservations,thenusetheseSCMstoinformfuturepredictions. Afundamental
challengeinlearningcausalstructureistheproblemoflatentvariableinduction,orinferring
whatlatentvariablestodefinecandidateSCMsover. Inthiswork,wefocusonthecausal
structureofprogramminglanguages,wheretheunderlyingcausaldynamicsaregoverned
byapreciseformalsemantics,andthelatentvariablesaregivenbyprogramstates. Having
formallydefinedsemanticsandlatentvariablesenablesusinterprettheresultsofourprob-
ingexperimentsinanunambiguousway;wereferthereadertoSloman(2005);Federetal.
(2022)forsurveysofcausalstructureinnaturallanguage.
3 Latentcausalprobing
Wepresentlatentcausalprobing,aformalframeworkforempiricallytestingthehypothesis
Languagemodelsarelatentcausalconceptlearners.
At a high level, given an SCM that models the training data as the observed variables,
weprobetheLMforrepresentationsofthelatentvariablesoftheSCM.Ourmaininsight,
asillustratedinFigure2,isthatknowingthelatentvalueof s n−1 couldhelppredictthe
observed value of p ; hence, an LM trained to predict p might eventually induce the
n n
existenceofthelatentvariables n−1. Moregenerally,werefertoanylatentvariablewitha
causaleffectonthedistributionofthetrainingdataasalatentcausalconcept.
3.1 Probingforlatentvariables
Webeginbydefiningtheauxiliarytaskanddatasetforprobing. Fixsomestructuralcausal
model M,andlet v bethelatentvariableofinterest. Givensometext x,weuse v (x)
M M
todenotethevalueofthelatentvariableintheSCMoftext x. Forinstance,thevalueof
v = s inthesamplexfromFigure2isthegriddepictedinthes node. Weassumethat
M 1 1
thevalueofeachlatentvariableisuniquelydeterminedbyxand M.
Given a language model LM with parameters θ, we denote an arbitrary representation
functionas LM(x;θ). Theauxiliarydatasetconsistsofinputfeatures{LM(x;θ) | x ∈ X}
andlabels{v (x) | x ∈ D},where D = {x }N isacorpusoftext. Wethensplit D into
M i i=1
twoauxiliarydatasets: oneforcalibrationandoneformeasurement. Theprobeistrained
4PublishedasaconferencepaperatCOLM2024
calibration measurement
bound free
bound deductiveknowledge inductivebias(inference)
free deductivebias(consistency) inductiveknowledge
Table1: Interpretingprobingwithdifferentcalibrationandmeasurementdatasets.
topredictv (x)givenLM(x;θ)onthecalibrationdata,andtheaccuracyistakenoverthe
M
measurementdata. Wenextdiscussthedesignandinterpretationofthesetwodatasets.
Bound vs. free latent variable outcomes. In general, there may exist several causal
dynamicsthatexplainthedataequallywell. Forinstance,thefollowingdynamicscould
alsogeneratethedatainFigure2:
put_marker Jumptoarandomlocation.
turn_right Returntothelastposition,putamarker,thenturnright.
Thesedynamicsassignadifferentvaluetos ,butexplaintheobservedvariablesequally
1
well. Assumingthetrainingcorpusconsistsentirelyofthissingleexample, itwouldbe
impossibletodistinguishbetween Mand M′ onthebasisofdataalone. Inotherwords:
1. Mand M′ sharethesamesetofsetoflatent,observed,andexogenousvariables;
2. Mand M′ agreeontheobserveddata;and
3. thereexistsanassignmentetotheexogenousvariablessuchthatv M(x) ̸= v M′(x′)
forx = M(e)andx′ = M′(e).
Inthiscase,wesaythatthelatentvariablevisfreeovertheassignmente. Moregenerally,
givenahypothesisclassMofSCMsoverthesamesetofvariables,denotetheLMtraining
data as D and define M| to be the subset of SCMs that generate D . The free
train train train
latentvariableoutcomesconsistofpairsoflatentvariablesandassignments(v,e)suchthat
thereexist M,M′ ∈ M| trainwherev M(M(e)) ̸= v M′(M′(e)). Anylatentvariableoutcome
(v,e)whichisnotfreeisbound,i.e.,thetrainingdataD fullyspecifiestheoutcomeofv
train
ontheassignmente,giventhehypothesisclassM.
Probingwithfreevs. boundsplits. Table1detailsfourpossibleprobingsetupswhen
separatingtheauxiliarydatasetDintofreeandboundsplits. Inparticular,whencalibration
andmeasurementoccuronthesamesplit,theprobequantifiestheknowledge,orinforma-
tioncontent,thatcanbeextractedfromtheLMrepresentations;conversely,probingwith
different splits measures the transferability of the representations across different splits,
whichreliesonimplicitbias. Additionally,becausetheboundvariablesoutcomes,can,by
definition,bededucedfromthegivendata(andhypothesisclassM),measuringonthe
boundsplitrelatestothedeductiveabilityoftheLM;conversely,measuringonthefreesplit
isinherentlyaninductiveprocess. Wehighlightthattheinductivebiascanbeunderstood
asquantifyingthecapacityoftheLMrepresentationstoinfervaluesinunseendatabyapplying
theories derived from known data, a form of inductive inference; while the deductive bias
measurestheextenttowhichtheLMrepresentationsproducetheoriesofunseendatathatare
consistentwiththeobserveddata,akeytenetofdeductivelogic.
3.2 Causalmediationanalysisofprobing
Wenextturntocontrollingfortheprobe(Q1). Intuitively,thechallengeisanymeasurement
usingasupervisedprobeconflatestheLM’srepresentationoftheauxiliarylabelswiththe
probe’sabilitytolearntheauxiliarytask(Hewitt&Liang,2019). Whilethereexistanumber
ofproposalsforcontrollingforthecontributionoftheprobe,suchtechniquestypicallydo
notprovideanyformalguarantees,renderingtheircorrectapplicationandinterpretationa
challenge(Belinkov,2022).
5PublishedasaconferencepaperatCOLM2024
LM
LM training
Probe
calibration
M
probe
Probe
measurement
accuracy
Figure3: AnSCMdepictingtheLMtraining,probecalibration,andprobemeasurement.
Weuseplatenotationforrepeatediidsamples,e.g.,wedrawNsamplesforLMtraining.
Weproposeamethodofdisentanglingthetwoeffectsusingtheformalframeworkofcausal
mediationanalysis,andspecifically,path-specificeffects,whichanalyzehowcausaleffects
decomposeovermultiplecausalpaths(Avinetal.,2005;Imaietal.,2010). Tobegin,we
extendtheSCMofthedatagenerationprocesstoinclude(1)theLMtraining,(2)theprobe
calibration, and (3) the probe measurement. Figure 3 illustrates an example where the
hypothesisclassMconsistsof3-variableSCMswithasingleexogenous,observedvariable
s ,alatentvariablev ,andanotherobservedvariables .
0 M 1
ObservethattherearethreecausalpathsfromthetrueSCMofthedatagenerationprocess
to the auxiliary task accuracy, each of which is mediated by a different set of the latent
variablesv : (1)duringLMtraining,theLMistrainedonadatasetwhosetextiscausally
M
affectedbyv ;(2)duringprobecalibration,theprobeiscalibratedusingv directly;and
M M
(3)duringprobemeasurement,theprobeisevaluatedforaccuracyonv directly. However,
M
weonlycaretomeasuretheeffectoverthefirstofthesecausalpaths,i.e.:
TowhatextentcantheauxiliarytaskperformancebeattributedtowhatLMlearnsfromthelatent
causalconceptsinitstrainingdata?
Thisquestioncanbeposedformallyasthenecessaryindirecteffectofthepathsmediated
bytheLM’slearnedrepresentationforsomebaselinecausaldynamics M′:
NIE M,M′(θ LM) =
E(cid:2)
accuracy|LMistrainedon M,probeiscalibratedandmeasuredon
M(cid:3)
−E(cid:2) accuracy| do(LMistrainedon M′ ),probeiscalibratedandmeasuredon M],
Althoughpath-specificeffectsofferacrispconceptualframeworkforisolatingthecontri-
butionoftheLMinprobingexperiments,actuallycomputingNIEisnotstraightforward.
First,pickingaproperbaseline M′ iscritical;intuitively,ifwepickaninappropriate M′,
thentheNIEwillmeasurethedifferencebetween Mand M′ inadditiontothelatentcausal
conceptshypotheticallymediatedbytheLMrepresentations. Second,measuringtheef-
fectrequiresinterveningalongthepathofinterestwhileholdingtheotherpathsconstant,
i.e.,wewouldneedtoretrainLMaccordingtothebaseline M′,whichwouldrenderthe
techniqueprohibitivelyexpensiveforlargepretrainedLMs.
Letacc(M ,M )denotethe(expected)auxiliarytaskaccuracyaftertheLMistrainedusing
0 1
theSCMM andtheprobeiscalibratedandmeasuredonM .Thefollowingresultaddresses
0 1
thesechallenges(proofinAppendixC).
6PublishedasaconferencepaperatCOLM2024
Definition3.1(Validbaseline). M′ isavalidbaselinefor Mif
acc(M′ ,M′) ≥ acc(M,M) (1)
acc(M,M′) ≥ acc(M′ ,M). (2)
Proposition3.2. Let M′ beavalidbaselinefor M. Then
acc(M,M)−acc(M,M′) >0
impliesbothNIE M,M′(θ LM) >0andNIE M′,M(θ L′ M) >0.
Intuitively, M′ isavalidbaselinewhenmeasuring M′ iseasierthanmeasuring Munder
both normal or intervened circumstances. The conclusion then states that, so long as
acc(M,M)−acc(M,M′) > 0(whichcanbeevaluatedbyrunningprobecalibrationand
measurementtwiceratherthantrainingtheLMtwice),thereisnobiasinwhichSCMis
usedtotraintheLMandwhichisthebaseline: theLMrepresentationsalwaysmediatea
positiveamountofthemeasuredeffect.
ApositiveNIEnowalsohasarigorousinterpretationastheLMhavinglearnedlatentcausal
concepts,assomepositiveamountofcausaleffectistransferredthroughtherepresentations
oftheLM.Forinstance,apositivemediatedmeasurementforinductivebiasimpliesthat
ThepresenceoflatentcausalconceptsinthepretrainingdatacausestheLMtolearnrepresentations
thatgeneralizetounknowndata.
3.3 Discussion
Wesummarizethelatentcausalprobingframeworkasfollows:
1. Selectasetofexogenous,latent,andobservedvariablesandpickahypothesisclass
MofSCMs(fromthesetofallMarkovianSCMsoverthevariables).
2. FixaspecifictargetSCM M ∈ Mandasetoflatentvariablesv ∈ Mtotest.
3. Constructtheauxiliarydatasetandcreatetheboundvs. freepartition(ifpossible).
4. Identifyavalidbaseline M′ andperformthemediationanalysis.
Asignificantmeasurement acc(M,M)−acc(M,M′) > 0isinterpretedevidencethatthe
LMencodescausalconceptsinitsrepresentations. Weconcludewithsomeremarks.
Interventions,andprobingfornon-causallatentvariables. Ourmediationtechnique
requiresknowing“whatwouldthetexthavebeeniftheunderlyingdynamicsweredif-
ferent?”, which could be difficult (especially in non-synthetic domains). Similarly, for
non-causallatentvariables,suchaspart-of-speech,producingahypothesisclassMwith
morethanoneSCMmaynotbepossible: whatwouldthedatalooklikeinacounterfac-
tualworldinwhich“dog”isactuallyanadverb? Unfortunately,ouranalysisshowsthat
a baseline M′ which induces a different distribution of text is a necessary precondition,
sinceotherwiseNIE M,M′(θ LM)andNIE M′,M(θ L′ M)cannotbothbepositive(asMandM′are
indistinguishablewhenusedtotraintheLM).Intuitively,weinterpretthisresultassaying
anymeasurementisinherentlybiasedwhentheauxiliarytaskhasonlyone“right”answer.
Probe architecture and hyperparameters. Our framework also explicates the role of
the probe’s architecture and other hyperparameters in the training process, such as the
optimizer,learningrate,datasetsize,etc.,aspotentialmoderators,butnotmediators(Q2).
Inotherwords, solongasthereexistsachoiceofhyperparameterssuchthattheNIEis
positive,theanalysisconcludesthatthereexistsacausaleffectmediatedbythemodel’s
parameters (although certain settings of the moderator variables could offer additional
interpretations). Practicallyspeaking,ourframeworkalsooffersanovelwaytointerpret
(andjustify)complexprobes(Voita&Titov,2020;Pimentel&Cotterell,2021).
4 Experiments
WeconductempiricalstudyofwhetheranLM,trainedfromscratchonacorpusofprogram
data,learnsthelatentcausalconceptsintheunderlyingdatagenerationprocess.
7PublishedasaconferencepaperatCOLM2024
4.1 Methods
WedescribethekeystepsaccordingtotheframeworkinSection3.3;AppendixA.1contains
fullexperimentaldetails(e.g.,LMandprobearchitectureandtraining,LMrepresentations).
Hypothesisclass. Theexogenousvariablesaretheinitialstateandprogram. Thelatent
variablesaretheintermediatestates,andtheobservedvariablesaretheinitialandfinalstate
andtheprogram. ThehypothesisclassMisallMarkovianSCMsoverthevariables.
TargetSCMandlatentvariables. ThetargetSCM M ∈ M isthetruedatageneration
processinFigure2.Thetargetlatentvariablesconsistoftherobot’sposition,facingdirection,
andwhethertherobotisfacingarockforeachintermediatestate.
Auxiliarydatasetconstructionandboundandfreelatentvariableoutcomes. Forthe
auxiliarydataset,weusethesamedatagenerationprocess,exceptthatprogramsrangein
lengthbetween1and15,andwereplacethefinalstateinthespecificationwiththeinitial
state. Duetothesizeofthetrainingcorpus(1millionsamples),weassumetheLMobserved
all combinations of the exogenous variables. Because the programs in the LM training
corpusarebetweenlength5and9,theboundlatentvariablesares tos (theyareobserved
6 10
duringtrainingasthefinalstate). Thefreelatentvariablesares tos ands tos .
1 5 11 15
Valid baseline. We construct valid baselines according to a counterfactual state of the
world where the intermediate states are generated by executing the program according
toadifferentsetofcausaldynamics. Specifically,wedefine M′ usingthesameSCM,but
permute the causal dynamics of the turn_right, turn_left, and move actions (e.g., the
robotturnsleftwhenexecutingaturn_rightaction). As Mand M′ areclearlysymmetric
fromalanguagemodelingperspective,Definition3.1(andhenceProposition3.2)holds.
4.2 Results
Figure4plotsthemainresults. Forallfourmeasurements(deductiveknowledge,inductive
bias,deductivebias,andinductiveknowledge)andacrossallthreeprobes(linear,1-layer
MLP,2-layerMLP),themediatedmeasurementsaresignificantlypositive(abovethedashed
lineat0%)bytheendoftraining. Wethusconcludethatapositivefractionoftheobserved
measurementsofcausalconceptscanbeattributedtowhatislearnedbytheLM’srepresentations.
We also note that the linear probe exhibits the lowest mediated measurements of the 3
probesacrossallfourtasks. Thissuggeststhatmorecomplexauxiliarytasksrequiremore
complexprobesand,moregenerally,highlightstheimportanceofprobingframeworksthat
canaccommodatedeeperprobes. AppendixA.2containsfurtherresultsandanalyses.
5 Relatedwork
Causal interpretability of LMs Several prior lines of work apply causal techniques to
theinterpretabilityofLMs. Theseworkstypicallyinterveneoneitherthemodel’srepre-
sentations(Elazaretal.,2021;Geigeretal.,2021;Mengetal.,2022;Abrahametal.,2022;
Lietal.,2022)orthemodel’sinputs(Kaushiketal.,2020;Vigetal.,2020;Gangal&Hovy,
2020;Aminietal.,2023),andanalyzethecausaleffectontheLM’soutputs. Incontrast,we
presentaformalframeworkthat,conceptually,intervenesonthemodel’strainingdata,and
measuresthecausaleffectontheLM’sinternalrepresentationsasaproxyforknowledge. To
thebestofourknowledge,Elazaretal.(2022)istheonlyotherworkthatstudiesthecausal
relationshipbetweenthetrainingdataandtheLM,presentingatechniqueforestimating
thecausaleffectofdatasetstatisticsonthefactualityofLM’soutputs.
CausalknowledgeandreasoninginLMs Anumberofbenchmarkstestforcausalknowl-
edgeinpretrainedLMsbyelicitingoutputsoncausalreasoningtasks;werefertoYangetal.
(2023)forasurvey.Recently,however,researchershaveraisedconcernsthatperformanceon
suchbenchmarksmaynotcorrespondtotruecausalreasoning(Zhangetal.,2023;Zecˇevic´
etal.,2023). AsYangetal.(2023)state,“TheissuehereisthattheLLMdoesnotneedto
actuallyreasonatall: itcansimplyaccessitstrainingdataset,whichcontainsmillionsof
storiesaboutweatherandumbrellas,andapproximatelyretrievearesponse.” Similarly,Wu
8PublishedasaconferencepaperatCOLM2024
(a)Deductiveknowledge. (b)Deductiveknowledge,mediated.
(c)Inductivebias. (d)Inductivebias,mediated.
(e)Deductivebias. (f)Deductivebias,mediated.
(g)Inductiveknowledge. (h)Inductiveknowledge,mediated.
Figure4: Resultsfromthemainexperiments. Solid,dashed,anddottedgreenlinesplotthe
accuracyofalinear,1-layerMLP,and2-layerMLPprobingclassifier,respectively. Thefinal
modelachieves92.4%accuracyongeneratingcorrectprogramsforunseenspecifications.
etal.(2023)presentempiricalevidencethatpretrainedLMsfailtoreasononcounterfactual
taskvariants,andattributetheirapparentreasoningcapabilitiestorecallfromthetraining
data. In contrast, we conduct our experiments in a controlled setting where the LM is
trainedfromscratch. OurfocusisalsoontheabilityoftheLMtorepresentandgeneralize
thelatentcausalconcepts,ratherthanknowledgeofthecausalrelationshipsbetweenthe
concepts. Finally,thehypothesisthatLMscanlearnlatentcausalconceptsisalsohighly
9PublishedasaconferencepaperatCOLM2024
relatedtothepositiondevelopedbyAndreas(2022),whoarguesthatLMscouldmodel
propertiesofagentsthatarelikelytohaveutteredthelanguageintheirtrainingdata.
Frameworksforprobing Anumberofworkshaveproposedframeworkstowardamore
rigorousunderstandingofprobing. Onelinetakesaninformation-theoreticviewonthe
informationrepresentedbytheLM(Zhu&Rudzicz,2020;Pimenteletal.,2020;Voita&
Titov,2020;Pimentel&Cotterell,2021). Incontrast,weuseprobestoidentifycausaleffects
andinductivebiases.Immeretal.(2022)proposeaninterpretationofprobingasquantifying
theinductivebiasofpretrainedrepresentationsfordownstreamtasks,buttheirframework
differssignificantlyfromoursinthatthemodelisunderstoodasarepresentation-probe
pair. Incontrast,ourapproachunifiestheLMtrainingandprobecalibrationprocedures
underasinglecausalframeworkforanalysis,yieldingformalguaranteesforthecontrolof
probes. Ouranalysisalsorevealssettingsinwhichpriortechniques,suchascontroltasks
(Hewitt&Liang,2019),canyieldbiasedestimatesoftheintendedauxiliarymeasurement.
6 Conclusion
This paper presents latent causal probing, a probing framework that studies whether
LMslearnlatentcausalconceptsasabyproductofthelanguagemodelingobjective. Our
frameworkoffersrobusttoolsforinterpretingexperimentresultsthroughthelensofcausal
analysis,andinparticular,rigorouslycontrolsfortheprobe’scontributioninlearningthe
auxiliarytask. Experimentally,weextendapreviousstudyofwhetherTransformerscan
infertheintermediatestatesthatunderlieasequenceofactions. Ourresultsprovidestrong
empiricalevidencethatLMscaninducelatentcausalconceptsfromtextualpretraining.
References
Eldar David Abraham, Karel D’Oosterlinck, Amir Feder, Yair Ori Gat, Atticus Geiger,
ChristopherPotts,RoiReichart,andZhengxuanWu. CEBab:Estimatingthecausaleffects
ofreal-worldconceptsonNLPmodelbehavior. InAliceH.Oh,AlekhAgarwal,Danielle
Belgrave,andKyunghyunCho(eds.),AdvancesinNeuralInformationProcessingSystems,
2022. URLhttps://openreview.net/forum?id=3AbigH4s-ml.
YossiAdi,EinatKermany,YonatanBelinkov,OferLavi,andYoavGoldberg. Fine-grained
analysisofsentenceembeddingsusingauxiliarypredictiontasks. InInternationalCon-
ference on Learning Representations, 2017. URL https://openreview.net/forum?id=
BJh6Ztuxl.
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear
classifierprobes,2018.
AfraAmini,TiagoPimentel,ClaraMeister,andRyanCotterell. NaturalisticCausalProbing
forMorpho-Syntax. TransactionsoftheAssociationforComputationalLinguistics,11:384–403,
052023. ISSN2307-387X. doi: 10.1162/tacl_a_00554. URLhttps://doi.org/10.1162/
tacl_a_00554.
JacobAndreas. Languagemodelsasagentmodels. InYoavGoldberg,ZornitsaKozareva,
and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP
2022, pp. 5769–5779, Abu Dhabi, United Arab Emirates, December 2022. Association
forComputationalLinguistics. doi: 10.18653/v1/2022.findings-emnlp.423. URLhttps:
//aclanthology.org/2022.findings-emnlp.423.
ChenAvin,IlyaShpitser,andJudeaPearl. Identifiabilityofpath-specificeffects. InIJCAI
InternationalJointConferenceonArtificialIntelligence,pp.357–363,2005.
ReubenMBaronandDavidAKenny. Themoderator–mediatorvariabledistinctioninsocial
psychologicalresearch: Conceptual,strategic,andstatisticalconsiderations. Journalof
personalityandsocialpsychology,51(6):1173,1986.
10PublishedasaconferencepaperatCOLM2024
Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Com-
putational Linguistics, 48(1):207–219, March 2022. doi: 10.1162/coli_a_00422. URL
https://aclanthology.org/2022.cl-1.7.
YonatanBelinkovandJamesGlass. AnalysisMethodsinNeuralLanguageProcessing: A
Survey. TransactionsoftheAssociationforComputationalLinguistics,7:49–72,042019. ISSN
2307-387X. doi: 10.1162/tacl_a_00254. URLhttps://doi.org/10.1162/tacl_a_00254.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Languagemodelsarefew-shotlearners. Advancesinneuralinformationprocessingsystems,
33:1877–1901,2020.
YanaiElazar,ShauliRavfogel,AlonJacovi,andYoavGoldberg.AmnesicProbing:Behavioral
ExplanationwithAmnesicCounterfactuals.TransactionsoftheAssociationforComputational
Linguistics,9:160–175,032021. ISSN2307-387X. doi: 10.1162/tacl_a_00359. URLhttps:
//doi.org/10.1162/tacl_a_00359.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Amir Feder, Abhilasha Ravichander, Mar-
ius Mosbach, Yonatan Belinkov, Hinrich Schütze, and Yoav Goldberg. Measuring
causal effects of data statistics on language model’sfactual’predictions. arXiv preprint
arXiv:2207.14251,2022.
Amir Feder, Katherine A Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach
Wood-Doughty,JacobEisenstein,JustinGrimmer,RoiReichart,MargaretERoberts,etal.
Causalinferenceinnaturallanguageprocessing: Estimation,prediction,interpretation
andbeyond. TransactionsoftheAssociationforComputationalLinguistics,10:1138–1158,2022.
Varun Gangal and Eduard Hovy. BERTering RAMS: What and how much does BERT
alreadyknowabouteventarguments? -astudyontheRAMSdataset. InAfraAlishahi,
YonatanBelinkov,GrzegorzChrupała,DieuwkeHupkes,YuvalPinter,andHassanSajjad
(eds.),ProceedingsoftheThirdBlackboxNLPWorkshoponAnalyzingandInterpretingNeural
Networks for NLP, pp. 1–10, Online, November 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.blackboxnlp-1.1. URLhttps://aclanthology.org/
2020.blackboxnlp-1.1.
AtticusGeiger,HansonLu,ThomasFIcard,andChristopherPotts. Causalabstractionsof
neuralnetworks. InA.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan(eds.),
AdvancesinNeuralInformationProcessingSystems,2021. URLhttps://openreview.net/
forum?id=RmuXDtjDhG.
JohnHewittandPercyLiang. Designingandinterpretingprobeswithcontroltasks. In
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingand
the9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.
2733–2743,2019.
Kosuke Imai, Luke Keele, and Dustin Tingley. A general approach to causal mediation
analysis. Psychologicalmethods,15(4):309,2010.
AlexanderImmer,LucasTorrobaHennigen,VincentFortuin,andRyanCotterell. Probing
asquantifyinginductivebias. InProceedingsofthe60thAnnualMeetingoftheAssociation
forComputationalLinguistics(Volume1: LongPapers),pp.1839–1851,2022.
Charles Jin and Martin Rinard. Evidence of meaning in language models trained on
programs,2023.
DivyanshKaushik,EduardHovy,andZacharyLipton. Learningthedifferencethatmakes
adifferencewithcounterfactually-augmenteddata. InInternationalConferenceonLearning
Representations,2020. URLhttps://openreview.net/forum?id=Sklgs0NFvr.
KennethLi,AspenKHopkins,DavidBau,FernandaViégas,HanspeterPfister,andMartin
Wattenberg. Emergentworldrepresentations: Exploringasequencemodeltrainedona
synthetictask. InTheEleventhInternationalConferenceonLearningRepresentations,2022.
11PublishedasaconferencepaperatCOLM2024
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternational
ConferenceonLearningRepresentations,2019. URLhttps://openreview.net/forum?id=
Bkg6RiCqY7.
RowanHallMaudslay,JosefValvoda,TiagoPimentel,AdinaWilliams,andRyanCotterell.
Ataleofaprobeandaparser. InDanJurafsky, JoyceChai, NatalieSchluter, andJoel
Tetreault(eds.),Proceedingsofthe58thAnnualMeetingoftheAssociationforComputational
Linguistics,pp.7389–7395,Online,July2020.AssociationforComputationalLinguistics.
doi: 10.18653/v1/2020.acl-main.659. URLhttps://aclanthology.org/2020.acl-main.
659.
KevinMeng,DavidBau,AlexAndonian,andYonatanBelinkov.Locatingandeditingfactual
associationsingpt. AdvancesinNeuralInformationProcessingSystems, 35:17359–17372,
2022.
ErikNijkamp,BoPang,HiroakiHayashi,LifuTu,HuanWang,YingboZhou,SilvioSavarese,
andCaimingXiong. Codegen: Anopenlargelanguagemodelforcodewithmulti-turn
programsynthesis. ICLR,2023.
JudeaPearl. Anintroductiontocausalinference. Theinternationaljournalofbiostatistics,6(2),
2010.
JudeaPearletal. Models,reasoningandinference. Cambridge,UK:CambridgeUniversityPress,
19(2):3,2000.
TiagoPimentelandRyanCotterell.ABayesianframeworkforinformation-theoreticprobing.
InMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.),
Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.
2869–2887,OnlineandPuntaCana,DominicanRepublic,November2021.Association
for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.229. URL https:
//aclanthology.org/2021.emnlp-main.229.
TiagoPimentel,JosefValvoda,RowanHallMaudslay,RanZmigrod,AdinaWilliams,and
RyanCotterell. Information-theoreticprobingforlinguisticstructure. InProceedingsofthe
58thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.4609–4622,2020.
XingShi,InkitPadhi,andKevinKnight. Doesstring-basedneuralmtlearnsourcesyntax?
InProceedingsofthe2016conferenceonempiricalmethodsinnaturallanguageprocessing,pp.
1526–1534,2016.
Steven Sloman. Locating Causal Structure in Language. In Causal Models: How Peo-
ple Think about the World and Its Alternatives. Oxford University Press, 08 2005. ISBN
9780195183115. doi: 10.1093/acprof:oso/9780195183115.003.0011. URL https://doi.
org/10.1093/acprof:oso/9780195183115.003.0011.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinforma-
tionprocessingsystems,30,2017.
JesseVig,SebastianGehrmann,YonatanBelinkov,SharonQian,DanielNevo,YaronSinger,
andStuartShieber. Investigatinggenderbiasinlanguagemodelsusingcausalmediation
analysis. Advancesinneuralinformationprocessingsystems,33:12388–12401,2020.
ElenaVoitaandIvanTitov. Information-theoreticprobingwithminimumdescriptionlength.
InBonnieWebber,TrevorCohn,YulanHe,andYangLiu(eds.),Proceedingsofthe2020
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 183–196,
Online,November2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2020.emnlp-main.14. URLhttps://aclanthology.org/2020.emnlp-main.14.
NaftaliWeinberger. Path-specificeffects. BritishJournalforthePhilosophyofScience,70(1):
53–76,2019. doi: 10.1093/bjps/axx040.
12PublishedasaconferencepaperatCOLM2024
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony
Moi,PierricCistac,TimRault,RemiLouf,MorganFuntowicz,JoeDavison,SamShleifer,
PatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,Syl-
vainGugger,MariamaDrame,QuentinLhoest,andAlexanderRush. Transformers: State-
of-the-artnaturallanguageprocessing. InProceedingsofthe2020ConferenceonEmpirical
MethodsinNaturalLanguageProcessing: SystemDemonstrations,pp.38–45,Online,October
2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-demos.6.
URLhttps://aclanthology.org/2020.emnlp-demos.6.
ZhaofengWu,LinluQiu,AlexisRoss,EkinAkyürek,BoyuanChen,BailinWang,Najoung
Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabili-
ties and limitations of language models through counterfactual tasks. arXiv preprint
arXiv:2307.02477,2023.
LinyingYang,OscarClivio,VikShirvaikar,andFabianFalck. Acriticalreviewofcausal
inferencebenchmarksforlargelanguagemodels. InAAAI2024Workshopon”AreLarge
LanguageModelsSimplyCausalParrots?”,2023. URLhttps://openreview.net/forum?
id=mRwgczYZFJ.
MatejZecˇevic´,MoritzWillig,DevendraSinghDhami,andKristianKersting. Causalparrots:
Largelanguagemodelsmaytalkcausalitybutarenotcausal. TransactionsonMachine
Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=
tv46tCzs83.
ChengZhang,StefanBauer,PaulBennett,JiangfengGao,WenboGong,AgrinHilmkil,Joel
Jennings,ChaoMa,TomMinka,NickPawlowski,etal. Understandingcausalitywith
largelanguagemodels: Feasibilityandopportunities. arXivpreprintarXiv:2304.05524,
2023.
ZiningZhuandFrankRudzicz.Aninformationtheoreticviewonselectinglinguisticprobes.
InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing
(EMNLP),pp.9251–9262,2020.
13PublishedasaconferencepaperatCOLM2024
A Additionalexperimentaldetailsandresults
A.1 Languagemodelandprobedetails
FollowingJin&Rinard(2023),thelanguagemodelisa350MparameterCodeGenmodel
(Nijkampetal.,2023)takenfromtheHuggingFaceTransformerslibrary(Wolfetal.,2020).
Themodelwastrainedfor2.5billiontokens,whichwasroughly6passesor80000training
batchesoverthetrainingcorpus. WerefertoJin&Rinard(2023)forfurtherdetails.
Wenextdescribethedesignandtrainingoftheprobingclassifiers; thesenotesapplyto
all the probing experiments, unless otherwise noted. The linear probe is a single linear
layer. TheMLPprobeshaveReLU,batch_norm,thendropout(p=.2)aftereachlinearlayer.
Thehiddendimensionsofthe1-layerand2-layerMLPprobeswere(256,)and(256,1024),
respectively. The auxiliary datasets consisted of 500000 randomly selected samples. To
extract representations from the LM, we use the same strategy as Jin & Rinard (2023),
averagingtheLMhiddenstatesoverthelayerdimensionafterprocessingeachprogram
token. ProbesweretrainedusingAdamW(Loshchilov&Hutter,2019)withweightdecay
of1e-4. Thelearningratestartsat0.01,thendecaysby.1at75%and90%throughtraining.
Allprobesaretrainedfor2000000stepsusingabatchsizeof256.
ForthemediatedresultsreportedinFigure4,wegeneratedtheauxiliarydatasetusingan
SCMthatmapsturn_righttoturn_left,turn_lefttomove,andmovetoturn_right.
A.2 Ablationstudies
Thissectionpresentsomeablationstudiesonthesetupoftheprobingexperiments.
A.2.1 Validbaselineselection
Totestthesensitivityofthemediatedresults(andhence,overallconclusions)onthechoice
ofvalidbaseline,wegeneratetwoadditionalauxiliarydatasetswiththefollowingSCMs:
1. swapmoveandturn_left
2. swapturn_rightandturn_left
TheresultsareplottedinFigure5andFigure6,respectively. Wefindthatthemediated
measurementsinthefirstcasearenearlyidenticaltothoseinFigure4,despiteonlyswap-
ping two actions (instead of permuting 3). However, in the second case, the mediated
measurementsareessentiallynoise,centeredaround0. Weattributethistothefactthat
theresultinglabelsareextremelysimilar, as, inmostcases, therobotissimplyreflected
alongthestartingaxis. Weemphasizethatanegativeresultforonevalidbaselinedoesnot
constituteevidencetorejectthehypothesis,andthatasinglepositiveresultfromavalid
baselineissufficienttoacceptthehypothesis.
14PublishedasaconferencepaperatCOLM2024
(a)Deductiveknowledge,mediated. (b)Inductivebias,mediated.
(c)Deductivebias,mediated. (d)Inductiveknowledge,mediated.
Figure5: Mediatingwiththevalidbaselinethatswapsmoveandturn_left.
15PublishedasaconferencepaperatCOLM2024
(a)Deductiveknowledge,mediated. (b)Inductivebias,mediated.
(c)Deductivebias,mediated. (d)Inductiveknowledge,mediated.
Figure6: Mediatingwiththevalidbaselinethatswapsturn_rightandturn_left.
16PublishedasaconferencepaperatCOLM2024
A.2.2 Probearchitectureandhyperparameters
(a)Deductiveknowledge,mediated. (b)Inductivebias,mediated.
(c)Deductivebias,mediated. (d)Inductiveknowledge,mediated.
Figure7: Mediatingwiththevalidbaselinefromthemaintext. Boththeoriginalandthe
mediated measurements are retaken using the probe architecture and hyperparameters
fromJin&Rinard(2023).
Wenextablatetheprobearchitectureandhyperparametersbyadoptingthesettingsused
inJin&Rinard(2023). Thedifferencesare: nodropout,abatchsizeof1024,trainingthe
probefor10000000steps,andusing100000samplesintheauxiliarydataset. Weusethe
samevalidbaselineasinFigure4ofthemaintext.
TheresultsareplottedinFigure7. Weobservethatthegeneraltrendsarepreserved,and
allfourmediatedmeasurementsendingabove0%bytheendoftraining. However, we
note that both deductive and inductive knowledge measure slightly lower, which is an
exampleofthemoderatingeffectoftheprobearchitectureandtraininghyperparameters.
Weattributetheeffecttotheincreasedbatchsizeandlackofdropout,whichcouldencourage
theprobetoconvergemorequicklytoaglobaloptimum,giventhattheriskofoverfittingis
low(duetothelargesizeandhighqualityofthetrainingdataset). Thisisalsoconsistent
with (1) the general intuition that simpler (or less optimal) probes are a proxy for “ease
ofextraction,”whichisofteninterpretedasevidencethattherepresentationsare“more
aligned”withthetargetfeatures(Hewitt&Liang,2019),and(2)thetheoreticalfindingsin
Pimenteletal.(2020),whoconcludethatprobesofinfinitecapacityaremostinformativefor
measuringsyntacticknowledge.
17PublishedasaconferencepaperatCOLM2024
B ComparisonwithJin&Rinard(2023)
Inthissection,wehighlightseveralkeydeparturesfromtheexperimentaldesigninJin&
Rinard(2023).
First,theydonotsplittheirauxiliarydatasetintoboundandfreelatentvariableoutcomes,
andhencetheirresultsdonotyieldfine-grainedinterpretationsaboutprobingwithdifferent
calibrationandmeasurementdatasets.
Second, our analysis reveals the presence of possible confounders in the design of their
interventionalbaseline,leadingtouncontrolledeffects. Inparticular,theauxiliarydatasetis
constructedusingprogramsgeneratedbytheLMitself,ratherthanrandomlysampledas
wedo. Intuitively,thismeansthattheLM“sees”boths ands ,whichrevealsinformation
0 n
abouttheoriginalcasualdynamics. Formally,therepresentationsoftheLMusedforprobing
mediatesall3causalpathways,ratherthanthesimplecausalpathwayfromtheLMtraining
data(asinFigure3),andhencetheirinterventionalbaselineisnotapropermeasurement
ofthecausaleffectmediatedbytheLMrepresentations. Oursolutionistouserandomly
sampled programs and replace the occurrence of s with s in the construction of the
n 0
auxiliarydataset,whichbreaksthiscausaldependence.
Finally,Jin&Rinard(2023)donotverifythattheirinterventionalbaselinessatisfythecondi-
tionsinEquations(1)and(2). Inparticular,oneoftheirbaselinesmaptheput_markerand
pick_markeractionstoturn_rightandturn_left,respectively,inadditiontopermuting
theturn_right,turn_left,andmoveactions. Becausetheextractedfeaturesallrelateto
thepositionanddirectionoftherobot,thenewdynamicscouldpresentamoredifficulttask
(forboththeLMandtheprobe)duetoreplacingwhatwereeffectivelyno-ops(put_marker
andpick_marker)withnewoperationsthataffectthepositionordirection(turn_right,
turn_left,andmove). Hence,theobserveddropinaccuracypost-interventioncouldbe
attributabletoincreasedtaskdifficulty,ratherthanthelearnedrepresentationsoftheLM.
C Proofs
ProofofProposition3.2. Theprooffollowsdirectlyfromsubstitutingtheappropriateassump-
tionsintothedefinitionsofNIE.Recallthat
NIE M,M′(θ LM) := acc(M,M)−acc(M′ ,M) (3)
NIE M′,M(θ L′ M) := acc(M′ ,M′)−acc(M,M′), (4)
and,byDefinition3.1,
acc(M′ ,M′) ≥ acc(M,M) (5)
acc(M,M′) ≥ acc(M′ ,M). (6)
ApplyingEquation(5)tothedefinitionsofNIE,
NIE M,M′(θ LM) ≤ acc(M′ ,M′)−acc(M′ ,M) (7)
=NIE M′,M(θ L′ M). (8)
ApplyingEquation(6)tothedefinitionofNIE,
NIE M,M′(θ LM) = acc(M,M)−acc(M′ ,M) (9)
≥ acc(M,M)−acc(M,M′). (10)
Hence,
acc(M,M)−acc(M,M′) ≤NIE M,M′(θ LM) ≤NIE M′,M(θ L′ M) (11)
18