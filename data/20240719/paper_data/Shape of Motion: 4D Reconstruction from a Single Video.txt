Shape of Motion:
4D Reconstruction from a Single Video
Qianqian Wang1,2*, Vickie Ye1∗, Hang Gao1∗,
Jake Austin1, Zhengqi Li2, and Angjoo Kanazawa1
1 UC Berkeley 2 Google Research
Abstract. Monoculardynamicreconstructionisachallengingandlong-
standingvisionproblemduetothehighlyill-posednatureofthetask.Ex-
istingapproachesarelimitedinthattheyeitherdependontemplates,are
effectiveonlyinquasi-staticscenes,orfailtomodel3Dmotionexplicitly.
In this work, we introduce a method capable of reconstructing generic
dynamic scenes, featuring explicit, full-sequence-long 3D motion, from
casually captured monocular videos. We tackle the under-constrained
nature of the problem with two key insights: First, we exploit the low-
dimensional structure of 3D motion by representing scene motion with
a compact set of SE(3) motion bases. Each point’s motion is expressed
asalinearcombinationofthesebases,facilitatingsoftdecompositionof
thesceneintomultiplerigidly-movinggroups.Second,weutilizeacom-
prehensive set of data-driven priors, including monocular depth maps
and long-range 2D tracks, and devise a method to effectively consoli-
date these noisy supervisory signals, resulting in a globally consistent
representationofthedynamicscene.Experimentsshowthatourmethod
achievesstate-of-the-artperformanceforbothlong-range3D/2Dmotion
estimation and novel view synthesis on dynamic scenes. Project Page:
shape-of-motion.github.io
Keywords: Long-range3Dmotiontracking·Monoculardynamicnovel
view synthesis · 4D Reconstruction
1 Introduction
Reconstructingthepersistentgeometryandtheir3Dmotionacrossavideoiscru-
cialforunderstandingandinteractingwiththeunderlyingphysicalworld.While
recent years have seen impressive progress in modeling static 3D scenes [39,61],
recovering the geometry and motion of complex dynamic 3D scenes, especially
from a single video, remains an open challenge. A number of prior dynamic re-
construction and novel view synthesis approaches have attempted to tackle this
problem. However, most methods rely on synchronized multi-view videos [8,19,
19,58,99]oradditionalLIDAR/depthsensors[24,55,68,91,97].Recentmonocu-
lar approaches can operate on regular dynamic videos, but they typically model
* Equal Contribution
4202
luJ
81
]VC.sc[
1v46731.7042:viXra2 Authors Suppressed Due to Excessive Length
Fig.1: Shape of Motion. Our approach enables joint 3D long-range tracking and
novel view synthesis from a monocular video of a complex dynamic scene. Here, we
demonstrate our ability to render moving scene elements at fixed viewpoint with dif-
ferentmomentsintime.Additionally,wevisualizetheestimatedlong-range3Dmotion
as colorful trajectories. These trajectories reveal distinct geometric patterns that en-
capsulate the movement of each point through 3D space and time, which leads us to
the term “Shape of Motion”.
3D scene motion as short-range scene flow between consecutive times [20,51,52]
or deformation fields that maps between canonical and view space [64,65,108],
failing to capture 3D motion trajectories persistent over a video.
The longstanding challenge for more general in-the-wild videos lies in the
poorlyconstrainednatureofthereconstructionproblem.Inthiswork,wetackle
this challenge with two key insights. The first is that, while the image space
dynamics can be complex and discontinuous, the underlying 3D motion is a
compositionofcontinuoussimplerigidmotions.Oursecondinsightisthatdata-
drivenpriorsprovidecomplementary,thoughnoisycues,thataggregatewellinto
a globally coherent representation of the 3D scene geometry and motion.
Motivated by these two insights, we represent the dynamic scene as a set of
persistent3DGaussians,andrepresenttheirmotionacrossthevideointermsof
acompactsetofsharedSE(3)motionbases.Unliketraditionalsceneflow,which
computes3Dcorrespondencebetweenconsecutiveframes,ourrepresentationre-
covers a persistent 3D trajectory over the whole video, enabling long-range 3D
tracking. As the 3D trajectories produced by our method capture the geometric
patterns that trace each point’s movement through 3D space and time, we refer
to our approach “Shape of Motion”, as shown in Figure 1. We show how to fit
our explicit scene representation to a general video in-the-wild, by fusing to-
gether complementary cues from two main sources: monocular depth estimates
per-frame, and 2D track estimates across frames. We conduct extensive eval-
uations on both synthetic and real-world dynamic video datasets, and show
that our proposed approach significantly outperforms prior monocular dynamic
novel view synthesis methods and 3D tracking baselines in both long-range 2D
and 3D tracking accuracy. Moreover, we achieve state-of-the-art novel view syn-
thesis quality among all existing methods. In summary, our key contributions
include: (1) A new dynamic scene representation enabling both real-time novel
view synthesis and globally consistent 3D tracking for any point at any time.
(2) A carefully designed framework that optimizes the representation on posed
monocular video by leveraging physical motion priors and data-driven priors.Shape of Motion 3
2 Related Work
Correspondences and Tracking. Monocular 3D long-range tracking has not
beenexploredbroadlyintheliterature,butthereexistmanyapproachestoper-
form tracking in 2D image space. A typical way for determining 2D point corre-
spondences relies on optical flows. This involves estimating dense motion fields
between image pairs [4,6,7,9,15,27,28,28,31,35–37,57,71,79,85,86,101]. While
effective forconsecutiveframes, accuratelong-termtrackingin videosremains a
challengewithopticalflowmethods.Sparsekeypointmatchingmethodscanen-
ablelongtrajectorygeneration[2,12,53,56,73],butthesemethodsareprimarily
intended for sparse 3D reconstruction. Long-range 2D trajectory estimation for
arbitrarypointshasbeenexploredinearlierworks,whichreliedonhand-crafted
priors to generate motion trajectories [3,72,75,76,80,92]. Recently, there has
been a resurgence of interest in this problem, with several works showcasing im-
pressive long-term 2D tracking results on challenging, in-the-wild videos. These
approachesemployeithertest-timeoptimizationwheremodelsconsolidatenoisy
short-range motion estimates into a global representation for producing long-
term correspondences [62,94], or data-driven strategies [14,25,38], where neural
networkslearnlong-termcorrespondenceestimatesfromsyntheticdata[13,112].
Whilethesemethodseffectivelytrackany2Dpointthroughoutavideo,theylack
the knowledge of underlying 3D scene geometry and motions.
Scene flow or 3D motion trajectory is a common representation to model 3D
scene motion and point correspondences. Most prior work estimates scene flow
directlyfromLidarpointclouds[24,55,68,91,97]orRGBDimages[34,60,69,84,
88,89].Inmonocularsetting,afewrecentworksproposedtoestimate3Dmotions
through self-supervised learning or test-time optimization strategies [30,51,52,
102–104], but these approaches either focus on a single object, require template
priors, or only produce short-range motion correspondences. In contrast, our
method does not rely on template priors and is capable of producing long-range
3D trajectories, making it suitable for modeling complex scenes with multiple
moving objects.
Dynamic Reconstruction and View Synthesis. Our work also relates to
dynamic3Dscenereconstructionandnovelviewsynthesisproblems.Innon-rigid
reconstruction, early methods often required RGBD sensors [5,16,32,63,113] or
strong hand-crafted priors [45,70,74]. Recent work has demonstrated progress
towardthereconstructionofdynamicscenesinthewildbyintegratingmonocular
depth priors [43,50,59,110,111]. Recently, Neural Radiance Fields (NeRF) [61]
andGaussianSplat[40]basedapproacheshaveachievedstate-of-the-artresults.
Mostofthesemethods[1,8,10,19,19,48,49,81,83,93]requiresimultaneousmulti-
viewvideoobservationsorpredefinedtemplates[33,47,98]forhigh-qualitynovel
view outputs. Template-free monocular approaches model dynamic scenes with
differenttypesofrepresentationssuchasvideodepthmaps[109],time-dependent
NeRFs [17,51,52,64,65,67,90,100], and dynamic 3D Gaussians [18,99,107,108].
While significant progress has been made, as DyCheck [21] pointed out, many4 Authors Suppressed Due to Excessive Length
Fig.2: System Overview. Given a single RGB video sequence with known camera
poses, along with monocular depth maps and 2D tracks computed from off-the-shelf
models[14,106]asinput,weoptimizeadynamicscenerepresentationasasetofpersis-
tent3DGaussiansthattranslateandrotateovertime.Tocapturethelow-dimensional
natureofscenemotion,wemodelthemotionwithasetofcompactSE(3)motionbases
sharedacrossallsceneelements.Each3DGaussian’smotionisrepresentedasalinear
combination of these global SE(3) motion bases, weighted by motion coefficients spe-
cific to each Gaussian. We supervise our scene representation (canonical 3D Gaussian
parameters, per-Gaussian motion coefficients, and global motion bases) by comparing
the rendered outputs (RGB, depths and 2D tracks) with the corresponding input sig-
nals.Thisresultsinadynamic3Drepresentationofthescenewithexplicitlong-range
3D scene motion.
approachesfocus onscenarioswithcamera teleportation [44,108]orquasi-static
scenes, which do not represent real-world monocular videos. In this work, we
focus on modeling casual videos captured by a single camera, a more practical
and challenging setup.
3 Method
Our method takes as input a sequence of T video frames {I ∈ RH×W×3} of a
t
dynamicscene,thecameraintrinsicsK ∈R3×3,andworld-to-cameraextrinsics
t
E ∈ SE(3) of each input frame I . From these inputs, we aim to recover the
t t
geometry of the entire dynamic scene and the full-length 3D motion trajectory
of every point in the scene. Unlike most prior dynamic NeRFs methods [21,
50,52,90,100] which render the scene contents through volumetric ray casting
and represent the motion implicitly at fixed 3D locations, we model the dense
scene elements as a set of canonical 3D Gaussians, but allow them to translate
and rotate over entire video through full-length motion trajectories. We adopt
explicitpoint-basedrepresentationbecauseitsimultaneouslyallowsforboth(1)Shape of Motion 5
high-fidelityrenderinginreal-timeand(2)full-length3Dtrackingofanysurface
point from any input time.
Optimizinganexplicitrepresentationofdynamic3DGaussiansfromasingle
videoisseverelyill-posed–ateachpointintime,themovingsubjectsinthescene
are observed at those poses from only a single viewpoint. In order to overcome
this ambiguity, we make two insights: First, we note that, while the projected
2D dynamics might be complex in the video, the underlying 3D motion in the
sceneislow-dimensional,andcomposedofsimplerunitsofrigidmotion.Second,
powerful data-driven priors, namely monocular depth estimates and long-range
2D tracks, provide complementary but noisy signals of the underlying 3D scene.
We propose a system that allows us to fuse these noisy estimates together into
a globally conherent representation of both the scene geometry and motion.
Thefollowingsectionsintroduceourframework.Werepresentthescenecon-
tents as a globally consistent set of 3D Gaussians that live in a canonical space
(Section 3.1). To recover consistent motion trajectories, we bind the 3D Gaus-
sians to a compact and low-dimensional motion parameterization. In particular,
we represent the full-length motion trajectories of each dynamic scene element
with a set of compact and low-dimensional SE(3) motion bases (Section 3.2).
Finally, we fit these motion bases to the input video frames, constraining our
optimizationwithstructuralpriorsanddata-drivenpriors(Section3.3).Weshow
a schematic of our pipeline in Figure 2.
3.1 Preliminaries: 3D Gaussian Splatting
We represent the appearance and geometry of dynamic scene contents with a
global set of 3D Gaussians, an explicit and expressive differentiable scene rep-
resentation [39] for efficient optimization and rendering. We define parameters
of each 3D Gaussian in the canonical frame t as g ≡ (µ ,R ,s,o,c), where
0 0 0 0
µ ∈ R3, R ∈ SO(3) are the 3D mean and orientation in the canonical frame,
0 0
and s ∈ R3 the scale, o ∈ R the opacity, and c ∈ R3 the color, are persistent
properties shared across time. To render a set of 3D Gaussians from a camera
with world-to-camera extrinsics E and intrinsics K, the projections of the 3D
Gaussians in the image plane are obtained by 2D Gaussians parameterized as
µ′ ∈R2 and Σ′ ∈R2×2 via affine approximation
0 0
µ′(K,E)=Π(KEµ )∈R2, Σ′(K,E)=J Σ J⊤ ∈R2×2, (1)
0 0 0 KE 0 KE
where Π is perspective projection, and J is the Jacobian of perspective pro-
KE
jection with K and E at the point µ . The 2D Gaussians can then be efficiently
0
rasterized into RGB image and depth map via alpha compositing as
ˆI(p)= (cid:88) T α c , Dˆ(p)= (cid:88) T α d ,
i i i i i i (2)
i∈H(p) i∈H(p)
where α =o ·exp(cid:0) − 1(p−µ′)T Σ′ (p−µ′)(cid:1) , and T =(cid:81)i−1(1−α ). H(p)
i i 2 0 0 0 i j=1 j
is the set of Gaussians that intersect the ray shoot from the pixel p. This pro-
cess is fully differentiable, and enables direct optimization of the 3D Gaussian
parameters.6 Authors Suppressed Due to Excessive Length
3.2 Dynamic Scene Representation
Scene Motion Parameterization. To model a dynamic 3D scene, we keep
track of N canonical 3D Gaussians and vary their positions and orientations
over time with a per frame rigid transformation. In particular, for a moving 3D
Gaussian at time t, its pose parameters (µ ,R ) are rigidly transformed from
t t
the canonical frame t to t via T =(cid:2) R t (cid:3) ∈SE(3):
0 0→t 0→t 0→t
µ =R µ +t , R =R R . (3)
t 0→t 0 0→t t 0→t 0
Rather than modeling the 3D motion trajectories independently for each Gaus-
sian, we define a set of B ≪ N learnable basis trajectories {T(b) }B that
0→t b=1
are globally shared across all Gaussians [44]. The transformation T at each
0→t
time t is then computed by a weighted combination of this global set of basis
trajectories through per-point basis coefficients w(b) via
B
T =(cid:88) w(b) T(b) , (4)
0→t 0→t
b=0
where ∥w(b)∥ = 1. In our implementation, we parameterize T(b) as 6D rota-
0→t
tion [26] and translation, and perform the weighted combination separately on
eachwiththesameweightw(b). Duringoptimization,wejointlylearnthesetof
globalmotionbasesandmotioncoefficientsofeach3DGaussian.Thesecompact
motionbasesexplicitlyregularizethetrajectoriestobelow-dimensional,encour-
aging the 3D Gaussians that move similarly to each other to be represented by
similar motion coefficients.
Rasterizing 3D trajectories.Giventhisrepresentation,wenowdescribehow
weobtainpixelwise3DmotiontrajectoryatanyqueryframeI .wetakeasimilar
t
approach to Wang et al. [94] and rasterize the motion trajectories of 3D Gaus-
siansintoqueryframeI .Namely,foraquerycameraattimetwithintrinsicsK
t t
andextrinsicsE ,weperformrasterizationtoobtainamapwXˆ ∈RH×W×3
t t→t′
that contains the expected 3D world coordinates of the surface points corre-
sponding to each pixel at target time t′
wXˆ (p)= (cid:88) T α µ , (5)
t→t′ i i i,t′
i∈H(p)
where H(p) is the set of Gaussians that intersect the pixel p at query time t.
The 2D correspondence location at time t′ for a given pixel p, Uˆ (p), and
t→t′
the corresponding depth value at time t′, Dˆ (p) can then be written as
t→t′
Uˆ (p)=Π(cid:0) K cXˆ (p)(cid:1) , Dˆ (p)=(cid:16) cXˆ (p)(cid:17) (6)
t→t′ t′ t→t′ t→t′ t→t′
[3]
where cXˆ (p) = E wXˆ (p), Π is a perspective projection operation,
t→t′ t′ t→t′
and (·) is the third element of a vector.
[3]Shape of Motion 7
3.3 Optimization
We prepare the following estimates using off-the-shelf methods in our optimiza-
tion: 1) masks for the moving objects for each frame {M }, which can be eas-
t
ily obtained using Track-Anything [42,105] with a few user clicks, 2) monocu-
lar depth maps {D } computed using state-of-the-art relative depth estimation
t
method Depth Anything [106] and 3) long-range 2D tracks {U } for fore-
t→t′
groundpixelsfromstate-of-the-artpointtrackingmethodTAPIR[14].Wealign
the relative depth maps with the metric depth maps by computing a per-frame
global scale and shift and use them for our optimization, as we found relative
depth maps tend to contain finer details. We treat the lifted 2D tracks unpro-
jected with the aligned depth maps as noisy initial 3D track observations {X }
t
for the moving objects. For the static part of the scene, we model them using
standard static 3D Gaussians and initialize their 3D locations by unprojecting
them into the 3D space using the aligned depth maps. The static and dynamic
Gaussians are jointly optimized and rasterized together to form an image. We
focus on describing the optimization process for dynamic Gaussians below.
Initialization.Wefirstselectthecanonicalframet tobetheframeinwhichthe
0
most 3D tracks are visible, and initialize the Gaussian means in the canonical
frame µ as N randomly sampled 3D tracks locations from this set of initial
0
observations.Wethenperformk-meansclusteringonthevectorizedvelocitiesof
thenoisy3Dtracks{X },andinitializethemotionbases{T(b) }B fromthese
t 0→t b=1
B clusters of tracks. Specifically, for the set of trajectories {X } belonging
t b
to cluster b, we initialize the basis transform T(b) using weighted Procrustes
0→τ
alignment between the point sets {X } and {X } for all τ = 0,...,T, where
0 b τ b
the weights are computed using uncertainty and visibility scores from TAPIR
predictions. We initialize w(b) of each Gaussian to decay exponentially with its
distance from the center of cluster b in the canonical frame. We then optimize
µ ,w(b),andsetofbasisfunctions{T(b) }B tofittheobserved3Dtrackswith
0 0→t b=1
an ℓ -loss under temporal smoothness constraints.
1
Training.WesupervisethedynamicGaussianswithtwosetsoflosses.Thefirst
set of losses comprise our reconstruction loss to match the per-frame pixelwise
color, depth, and masks inputs. The second set of losses enforce consistency of
correspondences across time. Specifically, during each training step, we render
the image ˆI , depth Dˆ , and mask Mˆ from their corresponding training cam-
t t t
eras (K ,E ) according to Equation 2. We supervise these predictions with a
t t
reconstruction loss applied independently per-frame
L =∥ˆI−I∥ +λ ∥Dˆ −D∥ +λ ∥Mˆ −1∥ . (7)
recon 1 depth 1 mask 1
The second set of losses supervises the motion of the Gaussians between
frames. Specifically, we additionally render the 2D tracks uˆ and reprojected
t→t′
depths Dˆ , for a pair of randomly sampled query time t and target time t′.
t→t′
Wesupervisetheserenderedcorrespondenceswiththeliftedlong-range2Dtrack8 Authors Suppressed Due to Excessive Length
estimates via
L =∥U −Uˆ ∥ , and L =∥dˆ −Dˆ(U )∥ . (8)
track-2d t→t′ t→t′ 1 track-depth t→t′ t→t′ 1
Finally,weenforceadistance-preservinglossbetweenrandomlysampleddy-
namic Gaussians and their k-nearest neighbors. Let Xˆ and Xˆ denote the lo-
t t′
cation of a Gaussian at time t and t′, and C (Xˆ ) denote the set of k-nearest
k t
neighbors of Xˆ , we define
t
(cid:13) (cid:13)2
L =(cid:13)dist(Xˆ ,C (Xˆ ))−dist(Xˆ ,C (Xˆ ))(cid:13) , (9)
rigidity (cid:13) t k t t′ k t′ (cid:13)
2
where dist(·,·) measures Euclidean distance.
Implementation Details. For in-the-wild videos, we obtain their camera pa-
rameters with the following procedure: we first run UniDepth [66] to get the
camera intrinsics and metric depth maps, and then run Droid-SLAM [87] in
RGB-D mode with UniDepth’s depth maps to obtain the camera poses. This
process isefficient andprovidesaccurate camera parameters. For evaluatingour
methods on public benchmark, we use the camera annotations that come with
the datasets (e.g., from COLMAP [77] or simulation).
We optimize our model using Adam Optimizer [41]. We perform 1000 itera-
tionsofoptimizationfortheinitialfittingand500epochsforjointoptimization,
respectively.ThenumberofSE(3)basesB issetto20forallofourexperiments.
Weinitialize40kdynamicGaussiansforthedynamicpartand100kstaticGaus-
siansforthestaticpartofthescene,respectively.Weperformthesameadaptive
GaussiancontrolsfordynamicandstaticGaussiansasper3D-GS[40].Training
onasequenceof300framesof960×720resolutiontakesabout2hourstofinish
on an A100 GPU. Our rendering FPS is around 40 fps.
4 Experiments
We evaluate our performance quantitatively and qualitatively on a broad range
of tasks: long range 3D point tracking, long-range 2D point tracking, and novel
view synthesis. We focus our evaluation in particular on datasets that exhibit
substantial scene motion. In particular, the iPhone dataset [21] features casual
capturesofreal-worldscenesthatcloselymatchourtargetscenarios.Itprovides
comprehensiveannotations,includingsimultaneousvalidationviews,lidardepth,
sparse 2D point correspondences across the entire video, and can be used to
evaluate our performance on all three tasks. Given the challenge of obtaining
precise 3D track annotations for real data, we also evaluate performance using
the scenes from the synthetic MOVi-F Kubric dataset [23].
4.1 Task Specification
Long-range 3D tracking. Our primary task is estimating 3D scene motion
for any pixel over long period of time (up to over 10 seconds). For this purpose,Shape of Motion 9
Table 1: Evaluation on iPhone dataset. Our method achieves SOTA perfor-
mancealltasksof3Dpointtracking,2Dpointtracking,andnovelviewsynthesis.The
baselines that perform best on 2D and 3D tracking (TAPIR [14]+DA [106] and Co-
Tracker[38]+DA[106])areunabletosynthesizenovelviewpointsofthescene,whilethe
methodsthatperformbestinnovelviewsynthesis(T-NeRF[21]andHyperNeRF[65])
struggle with or fail to produce 2D and 3D tracks. Our method achieves a significant
boost in all three tasks above baselines.
3D Tracking 2D Tracking View Synthesis
Method
EPE↓ δ.05↑ δ.10↑ AJ↑ <δ ↑ OA↑ PSNR↑ SSIM↑ LPIPS↓
3D 3D avg
T-NeRF[21] - - - - - - 15.60 0.55 0.55
HyperNeRF[65] 0.182 28.4 45.8 10.1 19.3 52.0 15.99 0.59 0.51
DynIBaR[52] 0.252 11.4 24.6 5.4 8.7 37.7 13.41 0.48 0.55
Deformable-3D-GS[108] 0.151 33.4 55.3 14.0 20.9 63.9 11.92 0.49 0.66
CoTracker[38]+DA[106] 0.202 34.3 57.9 24.1 33.9 73.0 - - -
TAPIR[14]+DA[106] 0.114 38.1 63.2 27.8 41.5 67.4 - - -
Ours 0.082 43.0 73.3 34.4 47.0 86.6 16.72 0.63 0.45
we extend the metrics for scene flow evaluation introduced in RAFT-3D [88]
to evaluate long-range 3D tracking. Specifically, we report the 3D end-point-
error (EPE), which measures the Euclidean distance between the ground truth
3D tracks and predicted tracks at each target time step. In addition, we report
the percentage of points that fall within a given threshold of the ground truth
3D location δ.05 =5cm and δ.10 =10cm in metric scale.
3D 3D
Long-range 2D tracking. Our globally consistent 3D motion representa-
tion can be easily projected onto image plane to get long-range 2D tracks. We
therefore also evaluate 2D tracking performance in terms of both position ac-
curacy and occlusion accuracy following the metrics introduced in the TAP-Vid
benchmark [13], and report the Average Jaccard (AJ), average position accu-
racy (<δ ), and Occlusion Accuracy (OA).
avg
Novel view synthesis. Wemeasureourmethod’snovelviewsynthesisquality
as a comprehensive assessment for geometry, appearance, and motion. We eval-
uate on the iPhone dataset [21] which provides validation views and report co-
visibilitymaskedimagemetrics[21]:mPSNR,mSSIM[96]andmLPIPS[22,29].
4.2 Baselines
Our method represents dynamic 3D scene comprehensively with explicit long-
range 3D scene motion estimation, which also allows for novel view synthesis.
Whilenoexistingmethodachievestheexactsamegoalsasours,therearemeth-
ods that focus on sub-tasks related to our problem, such as dynamic novel view
synthesis, 2D tracking, or monocular depth estimation. We therefore adapt ex-
isting methods as our baselines which are introduced below.
While dynamic novel view synthesis approaches focus primarily on the pho-
tometricreconstructionqualityanddonotexplicitlyoutput3Dpointtracks,we10 Authors Suppressed Due to Excessive Length
Ours TAPIR + DA∗ D-3DGS HyperNeRF
Fig.3: 3D Track visualization on iPhone dataset. We render novel views for
each method and overlay their predicted 3D tracks on top of the rendered images.
For clarity, we only display a segment of the entire trails spanning 50 frames for a
specificsetofgridquerypoints.However,itisimportanttonotethatourmethodcan
generate dense, full-length 3D tracks. ∗Note that TAPIR + DA cannot produce novel
view rendering, and we overlay their tracking results with our rendering to make it
easier to interpret.
canadaptsomeoftheirrepresentationstoderive3Dpointtracksforourevalua-
tion. For HyperNeRF [65], we compose the learned inverse mapping (from view
space to canonical space) and a forward mapping solved via root-finding [11,21]
to produce 3D tracks at query points. DynIBaR [52] produces short-range view-
to-view scene flow, which we chain into long-range 3D tracks for our evaluation.
Deformable-3D-GS (D-3DGS) [108] represents dynamic scenes using 3D Gaus-
sians [39] in the canonical space and a deformation MLP network that deforms
the canonical 3D Gaussians into each view, which naturally allows 3D motion
computation. Finally, T-NeRF [21] models dynamic scenes using time as MLP
inputinadditionto3Dlocations,whichdoesnotprovideamethodforextracting
3D motion, and hence they are not considered for 2D/3D tracking evaluation.
kcapkcaB
llimdniw-repaP
nipSShape of Motion 11
TrainView GT Ours HyperNeRF TNeRF DynIBaR D-3DGS
Fig.4: Qualitative comparison of novel-view synthesis on iPhone dataset.
The leftmost image in each row shows the training view at the same time step as
the validation view. The regions highlighted in green indicate areas excluded from
evaluation due to the lack of co-visibility between training and validation views.
Weevaluatethe3DtracksofthesemethodsonlyontheiPhonedataset,because
we found none can handle the Kubric scene motion.
In addition to dynamic view synthesis baselines, we would also like to in-
clude baselines that focus on estimating 3D tracks. However, we did not find
prior work that produces long-range 3D tracks from generic monocular videos,
hence we adapt existing SOTA methods for long-range 2D tracking and monoc-
ulardepthestimation.Specifically,wetakethestate-of-artlong-range2Dtracks
from TAPIR [14] and CoTracker [38] and lift them into 3D scene motion using
monocular depth maps produced by Depth Anything (DA) [106]. We compute
the correct scale and shift for each relative depth map from Depth Anything to
alignthemwiththescene.Thetworesultingbaselinesarecalled“CoTracker[38]
+ DA [106]” and “TAPIR [14] + DA [106]”. Note that these baselines can only
produce3Dtracksforvisibleregions,asthedepthvaluesforoccludedpointsare
unknownfromsucha2.5Drepresentation.Incontrast,ourglobalrepresentation
allows for modeling 3D motion through occlusions.
4.3 Evaluation on iPhone Dataset
iPhone dataset [21] contains 14 sequences of 200-500 frames featuring various
types of challenging real world scene motion. All sequences are recorded using
a handheld moving camera in a casual manner, with 7 of them additionally
featuring two synchronized static cameras with a large baseline for novel view
synthesis evaluation. It also comes with metric depth from the lidar sensors
and annotations of 5 to 15 keypoints at ten equally spaced time steps for each
llimdniw-repaP
nipS
yddeT12 Authors Suppressed Due to Excessive Length
Input PCA Coef. Pred. Depth Input PCA Coef. Pred. Depth
Fig.5: Visualization of motion coefficients after PCA and predicted depth
mapsoniPhonedataset.Themotioncoefficientsencodeinformationregardingrigid
movingcomponents.Forinstance,ourmotioncoefficientPCAproducesconstantcolor
for the block in the second example which exhibits rigid motion.
sequence.For3Dtrackingevaluation,wegeneratethegroundtruth3Dtracksby
lifting the 2D keypoint annotations into 3D using lidar depth, and masking out
points that are occluded or with invalid lidar depth values.
All experiments are conducted with the original instead of half resolution as
in [21] given that our method can handle high-res video input. We also discard
Space-out and Wheel scenes due to camera and lidar error. We find that the
originalcameraannotationsfromARKitisnotaccurateenough,andrefinethem
using global bundle adjustment from COLMAP [78].
We report the quantitative comparison in Tab. 1 which shows that our
methodoutperformsallbaselinesonalltasksbyasubstantialmargin.On3Dand
2D tracking, we show clear improvement over naive baselines of 2D tracks plus
depthmaps,andsignificantimprovementoverallnovelviewsynthesismethods,
e.g.,nearlyhalvingtheEPEofthesecondbestmethodDeformable-3D-GS[108].
Our method also achieves the best novel view synthesis quality across dynamic
NeRF, IBR, and 3D-GS-based baselines.
Fig. 3 shows qualitative comparison of the 3D tracking results. To illustrate
3D tracking quality, we render the novel views and plot the predicted 3D tracks
of the given query points onto the novel views. Since “TAPIR + DA” cannot
perform novel view synthesis, we overlay their track predictions onto our ren-
derings to aid interpretation. D-3DGS [108] and HyperNeRF [65] fail to capture
the significant scene motion in the paper-windmill and spin sequence, resulting
in structure degradation and blurry rendering. “TAPIR + DA” can track large
motions, but their 3D tracks tend to be noisy and erroneous. In contrast, our
method not only generates the highest-quality novel views but also the most
smooth and accurate 3D tracks. Fig. 4 provides additional novel view synthesis
comparison on the validation views. In Fig. 5, we provide additional visualiza-
tion of the outputs from our representation. We visualize the rendering of the
first three components of PCA decomposition for the motion coefficients, which
correlates well with the rigid groups of the moving object.Shape of Motion 13
Table 2: 3D Tracking evaluation on
Kubric dataset.
Method EPE↓δ.05 ↑δ.10 ↑
3D 3D
CoTracker [38]+DA [106] 0.19 34.4 56.5 Input PCA Coef.
TAPIR [14]+DA [106] 0.20 34.0 56.2
Fig.6: First three PCA com-
Ours 0.16 39.8 62.2 ponents of the optimized mo-
tion coefficients.
4.4 Evaluation on the Kubric dataset
The Kubric MOVi-F dataset contains short 24-frame videos of scenes of 10-
20 objects, rendered with linear camera movement and motion blur. Multiple
rigid objects are tossed onto the scene, at a speed that far exceeds the speed of
the moving camera, making it a similar capture scenario to in-the-wild capture
scenarios. It provides dense comprehensive annotations, including ground truth
depthmaps,cameraparameters,segmentationmasks,andpointcorrespondences
that are dense across time. We use 30 scenes from the MOVi-F validation set to
evaluate the accuracy of our long-range 3D point tracks for all points in time.
We demonstrate our method on the synthetic Kubric MOVi-F dataset in
long-range 3D point tracking across time. Of the above baselines, only “Co-
Tracker [38]+DA [106]” and “TAPIR [38]+DA [106]” yield 3D tracks for these
scenes. For all baselines, we provide the ground truth camera intrinsics and ex-
trinsics, and monocular depth estimates that have been aligned to the ground
truth depth map. We obtain point tracks for all non-background pixels for each
method.
We report our quantitative 3D point tracking metrics in Table 2, and find
thatacrossallmetrics,ourmethodoutperformsthebaselines.Moreover,wefind
qualitatively that the optimized motion coefficients of the scene representation
are coherently grouped with each moving object in the scene. We demonstrate
this in Figure 6, where we show the first 3 PCA components of our optimized
motion coefficients of evaluation scenes.
4.5 Ablation studies
WeablatevariouscomponentsofourmethodontheiPhonedatasetin3Dtrack-
ing in Table 3. We first validate our choices of motion representation, namely
the SE(3) motion bases parameterization, with two ablations: 1) “Per-Gaussian
Transl.”: replacing our motion representation with naive per-Gaussian transla-
tionalmotiontrajectories,and2)“TranslBases.”:keepingthemotionbasesrep-
resentation but only using translational bases instead of SE(3). We find SE(3)
bases lead to a noticable improvement in 3D tracking compared to both trans-
lational bases and per-Gaussian translational motion representation.
Next, we ablate our training strategies including initialization and supervi-
sion signal. We conduct an ablation of “No SE(3) Init.”, where instead of per-
forming our initial SE(3) fitting stage, we initialize the translational part of the14 Authors Suppressed Due to Excessive Length
Table 3: Ablation Studies on iPhone dataset.
Methods SE(3) MotionBasis 2Dtracks Initialization EPE↓ δ.05↑ δ.10↑
3D 3D
Ours(Full) ✓ ✓ ✓ ✓ 0.082 43.0 73.3
Transl.Bases ✓ ✓ ✓ 0.093 42.3 69.9
Per-GaussianTransl. ✓ ✓ 0.087 41.2 69.2
NoSE(3)Init. ✓ ✓ ✓ 0.111 39.3 65.7
No2DTracks ✓ ✓ 0.141 30.4 57.8
motion bases with randomly selected noisy 3D tracks formed by directly lifting
input 2D tracks using depth maps into 3D, and the rotation part as identity.
We find that skipping this initialization noticeably hurts performance. Lastly,
weremovethe2Dtracksupervisionentirely(“No2DTracks”)andfindittolead
tosignificantdropinperformance,whichverifiestheimportanceofthe2Dtrack
supervision for 3D tracking.
5 Discussion and Conclusion
Limitations. Although our approach demonstrates promising steps towards
long-range 3D tracking and accurate reconstruction of complex 3D dynamic
scenes, several limitations remain. Similar to most prior monocular dynamic
view synthesis methods, our system still requires per-scene test-time optimiza-
tion, which hinders its use in streamable applications. In addition, it cannot
handle large changes in camera viewpoint, which require some kind of genera-
tive capabilities or data-driven priors to hallucinate unseen regions. Moreover,
our approach relies on accurate camera parameters obtained from off-the-shelf
algorithms, suggesting potential failure in scenes dominated by texture-less re-
gions or moving objects. Finally, our method relies on user input to generate
the mask of moving objects. A promising research direction would be to de-
sign a feed-forward network approach for jointly estimating camera poses, scene
geometry, and motion trajectories from an unconstrained monocular video.
Please note that since we wrote this paper, there have been several con-
current works posted on arXiv [46,54,82,95] that also address this setup of
monocular 4D reconstruction from causal videos. As far as we know, all of them
areoptimization-basedapproaches,alsoutilizingstrongoff-the-shelfdata-driven
priors. We leave it to future work to compare these approaches.
Conclusion. We presented a new method for joint long-range 3D tracking and
novel view synthesis from dynamic scene captured by a monocular video. Our
approachmodelsdensedynamicsceneelementswithaglobalsetof3DGaussians
thattranslateandrotateovertime.Weregularizethefull-length3Dmotiontra-
jectories of each Gaussian, ensuring smoothness and low dimensionality, by pa-
rameterizingthemaslinearcombinationsofacompactsetofrigidmotionbases.
To overcome the ill-posed nature of this problem in monocular capture settings,
we further design the model to regularize by consolidating noisy data-drivenShape of Motion 15
observations into a globally consistent estimate of scene appearance, geome-
try, and motion. Extensive evaluations on both synthetic and real benchmarks
demonstratethatourapproachsignificantlyimprovesuponpriorstate-of-the-art
methods in both 2D/3D long-range tracking and novel view synthesis tasks.
Acknowledgement. We thank Ruilong Li, Noah Snavely, Brent Yi and Alek-
sander Holynski for helpful discussion. This project is supported in part by
DARPA No. HR001123C0021. and IARPA DOI/IBC No. 140D0423C0035. The
views and conclusions contained herein are those of the authors and do not
represent the official policies or endorsements of these institutions.
References
1. Bansal, A., Vo, M., Sheikh, Y., Ramanan, D., Narasimhan, S.: 4d visualization
of dynamic events from unconstrained multi-view videos. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.5366–
5375 (2020) 3
2. Bay, H., Tuytelaars, T., Gool, L.V.: Surf: Speeded up robust features. In: Euro-
pean Conference on Computer Vision (2006) 3
3. Birchfield,S.T.,Pundlik,S.J.:Jointtrackingoffeaturesandedges.In:2008IEEE
Conference on Computer Vision and Pattern Recognition. pp. 1–6. IEEE (2008)
3
4. Black,M.J.,Anandan,P.:Aframeworkfortherobustestimationofopticalflow.
In: 1993 (4th) International Conference on Computer Vision. pp. 231–236. IEEE
(1993) 3
5. Bozic, A., Zollhöfer, M., Theobalt, C., Nießner, M.: Deepdeform: Learning non-
rigidrgb-dreconstructionwithsemi-superviseddata.2020IEEE/CVFConference
on Computer Vision and Pattern Recognition (CVPR) pp. 7000–7010 (2019) 3
6. Brox,T.,Bregler,C.,Malik,J.:Largedisplacementopticalflow.2009IEEECon-
ference on Computer Vision and Pattern Recognition pp. 41–48 (2009) 3
7. Brox, T., Bruhn, A., Papenberg, N., Weickert, J.: High accuracy optical flow
estimationbasedonatheoryforwarping.In:EuropeanConferenceonComputer
Vision (2004) 3
8. Broxton, M., Flynn, J., Overbeck, R., Erickson, D., Hedman, P., Duvall, M.,
Dourgarian, J., Busch, J., Whalen, M., Debevec, P.: Immersive light field video
withalayeredmeshrepresentation.ACMTransactionsonGraphics(TOG)39(4),
86–1 (2020) 1, 3
9. Bruhn,A.,Weickert,J.,Schnörr,C.:Lucas/kanademeetshorn/schunck:Combin-
inglocalandglobalopticflowmethods.Internationaljournalofcomputervision
61, 211–231 (2005) 3
10. Cao,A.,Johnson,J.:Hexplane:Afastrepresentationfordynamicscenes.In:Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. pp. 130–141 (2023) 3
11. Chen, X., Zheng, Y., Black, M.J., Hilliges, O., Geiger, A.: Snarf: Differentiable
forward skinning for animating non-rigid neural implicit shapes. In: Proceedings
oftheIEEE/CVFInternationalConferenceonComputerVision.pp.11594–11604
(2021) 1016 Authors Suppressed Due to Excessive Length
12. DeTone,D.,Malisiewicz,T.,Rabinovich,A.:Superpoint:Self-supervisedinterest
pointdetectionanddescription.2018IEEE/CVFConferenceonComputerVision
and Pattern Recognition Workshops (CVPRW) pp. 337–33712 (2017) 3
13. Doersch, C., Gupta, A., Markeeva, L., Recasens, A., Smaira, L., Aytar, Y., Car-
reira, J., Zisserman, A., Yang, Y.: Tap-vid: A benchmark for tracking any point
inavideo.AdvancesinNeuralInformationProcessingSystems35,13610–13626
(2022) 3, 9
14. Doersch, C., Yang, Y., Vecerik, M., Gokay, D., Gupta, A., Aytar, Y., Carreira,
J., Zisserman, A.: Tapir: Tracking any point with per-frame initialization and
temporal refinement. arXiv preprint arXiv:2306.08637 (2023) 3, 4, 7, 9, 11, 13
15. Dosovitskiy,A.,Fischer,P.,Ilg,E.,Häusser,P.,Hazirbas,C.,Golkov,V.,vander
Smagt, P., Cremers, D., Brox, T.: Flownet: Learning optical flow with convolu-
tionalnetworks.2015IEEEInternationalConferenceonComputerVision(ICCV)
pp. 2758–2766 (2015) 3
16. Dou, M., Khamis, S., Degtyarev, Y., Davidson, P.L., Fanello, S., Kowdle, A.,
Orts, S., Rhemann, C., Kim, D., Taylor, J., Kohli, P., Tankovich, V., Izadi, S.:
Fusion4d. ACM Transactions on Graphics (TOG) 35, 1 – 13 (2016) 3
17. Du, Y., Zhang, Y., Yu, H.X., Tenenbaum, J.B., Wu, J.: Neural radiance flow for
4d view synthesis and video processing. In: 2021 IEEE/CVF International Con-
ference on Computer Vision (ICCV). pp. 14304–14314. IEEE Computer Society
(2021) 3
18. Duan, Y., Wei, F., Dai, Q., He, Y., Chen, W., Chen, B.: 4d gaussian splat-
ting: Towards efficient novel view synthesis for dynamic scenes. arXiv preprint
arXiv:2402.03307 (2024) 3
19. Fridovich-Keil,S.,Meanti,G.,Warburg,F.R.,Recht,B.,Kanazawa,A.:K-planes:
Explicit radiance fields in space, time, and appearance. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12479–
12488 (2023) 1, 3
20. Gao,C.,Saraf,A.,Kopf,J.,Huang,J.B.:Dynamicviewsynthesisfromdynamic
monocularvideo.In:ProceedingsoftheIEEEInternationalConferenceonCom-
puter Vision (2021) 2
21. Gao, H., Li, R., Tulsiani, S., Russell, B., Kanazawa, A.: Dynamic novel-view
synthesis: A reality check. In: NeurIPS (2022) 3, 4, 8, 9, 10, 11, 12
22. Gatys,L.A.,Ecker,A.S.,Bethge,M.,Hertzmann,A.,Shechtman,E.:Controlling
perceptualfactorsinneuralstyletransfer.In:ProceedingsoftheIEEEconference
on computer vision and pattern recognition. pp. 3985–3993 (2017) 9
23. Greff,K.,Belletti,F.,Beyer,L.,Doersch,C.,Du,Y.,Duckworth,D.,Fleet,D.J.,
Gnanapragasam,D.,Golemo,F.,Herrmann,C.,etal.:Kubric:Ascalabledataset
generator.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 3749–3761 (2022) 8
24. Gu, X., Wang, Y., Wu, C., Lee, Y.J., Wang, P.: Hplflownet: Hierarchical permu-
tohedrallatticeflownetforsceneflowestimationonlarge-scalepointclouds.2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
pp. 3249–3258 (2019) 1, 3
25. Harley,A.W.,Fang,Z.,Fragkiadaki,K.:Particlevideorevisited:Trackingthrough
occlusionsusingpointtrajectories.In:EuropeanConferenceonComputerVision.
pp. 59–75. Springer (2022) 3
26. Hempel, T., Abdelrahman, A.A., Al-Hamadi, A.: 6d rotation representation for
unconstrained head pose estimation. In: 2022 IEEE International Conference on
Image Processing (ICIP). pp. 2496–2500. IEEE (2022) 6Shape of Motion 17
27. Horn, B.K.P., Schunck, B.G.: Determining optical flow. In: Other Conferences
(1981) 3
28. Huang, Z., Shi, X., Zhang, C., Wang, Q., Cheung, K.C., Qin, H., Dai, J., Li, H.:
Flowformer:Atransformerarchitectureforopticalflow.In:EuropeanConference
on Computer Vision. pp. 668–685. Springer (2022) 3
29. Huh, M., Zhang, R., Zhu, J.Y., Paris, S., Hertzmann, A.: Transforming and pro-
jecting images into class-conditional generative networks. In: Computer Vision–
ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,Pro-
ceedings, Part II 16. pp. 17–34. Springer (2020) 9
30. Hur, J., Roth, S.: Self-supervised monocular scene flow estimation. 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
pp. 7394–7403 (2020) 3
31. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: Flownet
2.0: Evolution of optical flow estimation with deep networks. In: Proceedings of
the IEEE conference on computer vision and pattern recognition. pp. 2462–2470
(2017) 3
32. Innmann, M., Zollhöfer, M., Nießner, M., Theobalt, C., Stamminger, M.: Vol-
umedeform:Real-timevolumetricnon-rigidreconstruction.In:EuropeanConfer-
ence on Computer Vision (2016) 3
33. Işık, M., Rünz, M., Georgopoulos, M., Khakhulin, T., Starck, J., Agapito, L.,
Nießner,M.:Humanrf:High-fidelityneuralradiancefieldsforhumansinmotion.
arXiv preprint arXiv:2305.06356 (2023) 3
34. Jaimez,M.,Souiai,M.,Jiménez,J.G.,Cremers,D.:Aprimal-dualframeworkfor
real-timedensergb-dsceneflow.2015IEEEInternationalConferenceonRobotics
and Automation (ICRA) pp. 98–104 (2015), https://api.semanticscholar.
org/CorpusID:16757067 3
35. Janai,J.,Guney,F.,Ranjan,A.,Black,M.,Geiger,A.:Unsupervisedlearningof
multi-frameopticalflowwithocclusions.In:ProceedingsoftheEuropeanconfer-
ence on computer vision (ECCV). pp. 690–706 (2018) 3
36. Jiang, S., Campbell, D., Lu, Y., Li, H., Hartley, R.: Learning to estimate hid-
den motions with global motion aggregation. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 9772–9781 (2021) 3
37. Jiang, S., Lu, Y., Li, H., Hartley, R.: Learning optical flow from a few matches.
In: Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. pp. 16592–16600 (2021) 3
38. Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., Rupprecht, C.:
CoTracker: It is better to track together (2023) 3, 9, 11, 13
39. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023)
1, 5, 10
40. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (July
2023), https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ 3, 8
41. Kingma, D.P., Ba, J.A., Adam, J.: A method for stochastic optimization. arxiv
2014. arXiv preprint arXiv:1412.6980 106 (2020) 8
42. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,
T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.B.: Segment
anything.2023IEEE/CVFInternationalConferenceonComputerVision(ICCV)
pp. 3992–4003 (2023) 718 Authors Suppressed Due to Excessive Length
43. Kopf, J., Rong, X., Huang, J.B.: Robust consistent video depth estimation. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 1611–1621 (2021) 3
44. Kratimenos, A., Lei, J., Daniilidis, K.: Dynmf: Neural motion factorization for
real-time dynamic view synthesis with 3d gaussian splatting. arXiV (2023) 4, 6
45. Kumar, S., Dai, Y., Li, H.: Monocular dense 3d reconstruction of a complex
dynamicscenefromtwoperspectiveframes.2017IEEEInternationalConference
on Computer Vision (ICCV) pp. 4659–4667 (2017) 3
46. Lei,J.,Weng,Y.,Harley,A.,Guibas,L.,Daniilidis,K.:Mosca:Dynamicgaussian
fusionfromcasualvideosvia4dmotionscaffolds.arXivpreprintarXiv:2405.17421
(2024) 14
47. Li, R., Tanke, J., Vo, M., Zollhöfer, M., Gall, J., Kanazawa, A., Lassner, C.:
Tava: Template-free animatable volumetric actors. In: European Conference on
Computer Vision. pp. 419–436. Springer (2022) 3
48. Li,T.,Slavcheva,M.,Zollhoefer,M.,Green,S.,Lassner,C.,Kim,C.,Schmidt,T.,
Lovegrove,S.,Goesele,M.,Newcombe,R.,etal.:Neural3dvideosynthesisfrom
multi-view video. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 5521–5531 (2022) 3
49. Li,Z.,Chen,Z.,Li,Z.,Xu,Y.:Spacetimegaussianfeaturesplattingforreal-time
dynamic view synthesis. arXiv preprint arXiv:2312.16812 (2023) 3
50. Li, Z., Dekel, T., Cole, F., Tucker, R., Snavely, N., Liu, C., Freeman, W.T.:
Learningthedepthsofmovingpeoplebywatchingfrozenpeople.In:Proceedings
of the IEEE/CVF conference on computer vision and pattern recognition. pp.
4521–4530 (2019) 3, 4
51. Li, Z., Niklaus, S., Snavely, N., Wang, O.: Neural scene flow fields for space-time
view synthesis of dynamic scenes. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 6498–6508 (2021) 2, 3
52. Li, Z., Wang, Q., Cole, F., Tucker, R., Snavely, N.: Dynibar: Neural dynamic
image-based rendering. In: Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 4273–4284 (2023) 2, 3, 4, 9, 10
53. Liu,C.,Yuen,J.,Torralba,A.:Siftflow:Densecorrespondenceacrossscenesand
itsapplications.IEEETransactionsonPatternAnalysisandMachineIntelligence
33, 978–994 (2011) 3
54. Liu,Q.,Liu,Y.,Wang,J.,Lv,X.,Wang,P.,Wang,W.,Hou,J.:Modgs:Dynamic
gaussian splatting from causually-captured monocular videos. arXiv preprint
arXiv:2406.00434 (2024) 14
55. Liu, X., Qi, C., Guibas, L.J.: Learning scene flow in 3d point clouds. ArXiv
abs/1806.01411 (2018) 1, 3
56. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Interna-
tional Journal of Computer Vision 60, 91–110 (2004) 3
57. Lucas,B.D.,Kanade,T.:Aniterativeimageregistrationtechniquewithanappli-
cationtostereovision.In:InternationalJointConferenceonArtificialIntelligence
(1981) 3
58. Luiten,J.,Kopanas,G.,Leibe,B.,Ramanan,D.:Dynamic3dgaussians:Tracking
by persistent dynamic view synthesis. ArXiv abs/2308.09713 (2023) 1
59. Luo, X., Huang, J.B., Szeliski, R., Matzen, K., Kopf, J.: Consistent video depth
estimation. ACM Transactions on Graphics (ToG) 39(4), 71–1 (2020) 3
60. Mehl, L., Jahedi, A., Schmalfuss, J., Bruhn, A.: M-fuse: Multi-frame fusion for
scene flow estimation. In: Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision. pp. 2020–2029 (2023) 3Shape of Motion 19
61. Mildenhall,B.,Srinivasan,P.P.,Tancik,M.,Barron,J.T.,Ramamoorthi,R.,Ng,
R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In:
ECCV (2020) 1, 3
62. Neoral, M., Šerých, J., Matas, J.: Mft: Long-term tracking of every pixel. In:
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV). pp. 6837–6847 (January 2024) 3
63. Newcombe,R.A.,Fox,D.,Seitz,S.M.:Dynamicfusion:Reconstructionandtrack-
ing of non-rigid scenes in real-time. 2015 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) pp. 343–352 (2015) 3
64. Park, K., Sinha, U., Barron, J.T., Bouaziz, S., Goldman, D.B., Seitz, S.M.,
Martin-Brualla, R.: Nerfies: Deformable neural radiance fields. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision. pp. 5865–5874
(2021) 2, 3
65. Park, K., Sinha, U., Hedman, P., Barron, J.T., Bouaziz, S., Goldman, D.B.,
Martin-Brualla, R., Seitz, S.M.: Hypernerf: A higher-dimensional representation
for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228
(2021) 2, 3, 9, 10, 12
66. Piccinelli, L., Yang, Y.H., Sakaridis, C., Segu, M., Li, S., Van Gool, L., Yu, F.:
UniDepth: Universal monocular metric depth estimation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
(2024) 8
67. Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-nerf: Neural
radiancefieldsfordynamicscenes.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 10318–10327 (2021) 3
68. Puy,G.,Boulch,A.,Marlet,R.:Flot:Sceneflowonpointcloudsguidedbyoptimal
transport. In: European Conference on Computer Vision (2020) 1, 3
69. Quiroga,J.,Brox,T.,Devernay,F.,Crowley,J.L.:Densesemi-rigidsceneflowes-
timationfromrgbdimages.In:EuropeanConferenceonComputerVision(2014),
https://api.semanticscholar.org/CorpusID:10240196 3
70. Ranftl, R., Vineet, V., Chen, Q., Koltun, V.: Dense monocular depth estimation
incomplexdynamicscenes.In:ProceedingsoftheIEEEConferenceonComputer
Vision and Pattern Recognition (CVPR) (June 2016) 3
71. Ren, Z., Gallo, O., Sun, D., Yang, M.H., Sudderth, E.B., Kautz, J.: A fusion ap-
proachformulti-frameopticalflowestimation.In:2019IEEEWinterConference
on Applications of Computer Vision (WACV). pp. 2077–2086. IEEE (2019) 3
72. Rubinstein,M.,Liu,C.:Towardslongerlong-rangemotiontrajectories.In:British
Machine Vision Conference (2012) 3
73. Rublee,E.,Rabaud,V.,Konolige,K.,Bradski,G.R.:Orb:Anefficientalternative
tosiftorsurf.2011InternationalConferenceonComputerVision pp.2564–2571
(2011) 3
74. Russell,C.,Yu,R.,Agapito,L.:Videopop-up:Monocular3dreconstructionofdy-
namicscenes.In:Europeanconferenceoncomputervision.pp.583–598.Springer
(2014) 3
75. Sand, P.: Long-range video motion estimation using point trajectories. Ph.D.
thesis, Ph. D. dissertation, Cambridge, MA, USA, 2006, adviser-Teller, Seth 3
76. Sand, P., Teller, S.: Particle video: Long-range motion estimation using point
trajectories. International journal of computer vision 80, 72–91 (2008) 3
77. Schonberger,J.L.,Frahm,J.M.:Structure-from-motionrevisited.In:Proceedings
oftheIEEEconferenceoncomputervisionandpatternrecognition.pp.4104–4113
(2016) 820 Authors Suppressed Due to Excessive Length
78. Schönberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Conference
on Computer Vision and Pattern Recognition (CVPR) (2016) 12
79. Shi, X., Huang, Z., Bian, W., Li, D., Zhang, M., Cheung, K.C., See, S., Qin, H.,
Dai, J., Li, H.: Videoflow: Exploiting temporal cues for multi-frame optical flow
estimation. arXiv preprint arXiv:2303.08340 (2023) 3
80. Sivic, J., Schaffalitzky, F., Zisserman, A.: Object level grouping for video shots.
International Journal of Computer Vision 67, 189–210 (2004) 3
81. Song, L., Chen, A., Li, Z., Chen, Z., Chen, L., Yuan, J., Xu, Y., Geiger, A.:
Nerfplayer: A streamable dynamic scene representation with decomposed neu-
ral radiance fields. IEEE Transactions on Visualization and Computer Graphics
29(5), 2732–2742 (2023) 3
82. Stearns, C., Harley, A., Uy, M., Dubost, F., Tombari, F., Wetzstein, G., Guibas,
L.:Dynamicgaussianmarblesfornovelviewsynthesisofcasualmonocularvideos.
arXiv preprint arXiv:2406.18717 (2024) 14
83. Stich,T.,Linz,C.,Albuquerque,G.,Magnor,M.:Viewandtimeinterpolationin
imagespace.In:ComputerGraphicsForum.vol.27,pp.1781–1787.WileyOnline
Library (2008) 3
84. Sun, D., Sudderth, E.B., Pfister, H.: Layered rgbd scene flow estimation. 2015
IEEEConferenceonComputerVisionandPatternRecognition(CVPR)pp.548–
556 (2015) 3
85. Sun, D., Yang, X., Liu, M.Y., Kautz, J.: Pwc-net: Cnns for optical flow using
pyramid, warping, and cost volume. 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition pp. 8934–8943 (2017) 3
86. Teed, Z., Deng, J.: Raft: Recurrent all-pairs field transforms for optical flow. In:
ComputerVision–ECCV2020:16thEuropeanConference,Glasgow,UK,August
23–28, 2020, Proceedings, Part II 16. pp. 402–419. Springer (2020) 3
87. Teed, Z., Deng, J.: Droid-slam: Deep visual slam for monocular, stereo, and rgb-
d cameras. Advances in neural information processing systems 34, 16558–16569
(2021) 8
88. Teed, Z., Deng, J.: Raft-3d: Scene flow using rigid-motion embeddings. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR) (2021) 3, 9
89. Vedula, S., Rander, P., Collins, R., Kanade, T.: Three-dimensional scene flow.
IEEE transactions on pattern analysis and machine intelligence 27(3), 475–480
(2005) 3
90. Wang, C., Eckart, B., Lucey, S., Gallo, O.: Neural trajectory fields for dynamic
novel view synthesis. arXiv preprint arXiv:2105.05994 (2021) 3, 4
91. Wang, C., Li, X., Pontes, J.K., Lucey, S.: Neural prior for trajectory estima-
tion. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) pp. 6522–6532 (2022) 1, 3
92. Wang,H.,Schmid,C.:Actionrecognitionwithimprovedtrajectories.2013IEEE
International Conference on Computer Vision pp. 3551–3558 (2013) 3
93. Wang, L., Zhang, J., Liu, X., Zhao, F., Zhang, Y., Zhang, Y., Wu, M., Yu, J.,
Xu, L.: Fourier plenoctrees for dynamic radiance field rendering in real-time.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 13524–13534 (2022) 3
94. Wang, Q., Chang, Y.Y., Cai, R., Li, Z., Hariharan, B., Holynski, A., Snavely,
N.: Tracking everything everywhere all at once. In: International Conference on
Computer Vision (2023) 3, 6
95. Wang, S., Yang, X., Shen, Q., Jiang, Z., Wang, X.: Gflow: Recovering 4d world
from monocular video. arXiv preprint arXiv:2405.18426 (2024) 14Shape of Motion 21
96. Wang,Z.,Bovik,A.C.,Sheikh,H.R.,Simoncelli,E.P.:Imagequalityassessment:
fromerrorvisibilitytostructuralsimilarity.IEEEtransactionsonimageprocess-
ing 13(4), 600–612 (2004) 9
97. Wang,Z.,Li,S.,Howard-Jenkins,H.,Prisacariu,V.A.,Chen,M.:Flownet3d++:
Geometric losses for deep scene flow estimation. 2020 IEEE Winter Conference
on Applications of Computer Vision (WACV) pp. 91–98 (2019) 1, 3
98. Weng,C.Y.,Curless,B.,Srinivasan,P.P.,Barron,J.T.,Kemelmacher-Shlizerman,
I.:Humannerf:Free-viewpointrenderingofmovingpeoplefrommonocularvideo.
In: Proceedings of the IEEE/CVF conference on computer vision and pattern
Recognition. pp. 16210–16220 (2022) 3
99. Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang,
X.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint
arXiv:2310.08528 (2023) 1, 3
100. Xian, W., Huang, J.B., Kopf, J., Kim, C.: Space-time neural irradiancefields for
free-viewpointvideo.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 9421–9431 (2021) 3, 4
101. Xu,H.,Zhang,J.,Cai,J.,Rezatofighi,H.,Tao,D.:Gmflow:Learningopticalflow
via global matching. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 8121–8130 (2022) 3
102. Yang, G., Sun, D., Jampani, V., Vlasic, D., Cole, F., Chang, H., Ramanan, D.,
Freeman, W.T., Liu, C.: Lasr: Learning articulated shape reconstruction from a
monocular video. 2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) pp. 15975–15984 (2021), https://api.semanticscholar.
org/CorpusID:234093797 3
103. Yang,G.,Sun,D.,Jampani,V.,Vlasic,D.,Cole,F.,Liu,C.,Ramanan,D.:Viser:
Video-specific surface embeddings for articulated 3d shape reconstruction. In:
Ranzato,M.,Beygelzimer,A.,Dauphin,Y.,Liang,P.,Vaughan,J.W.(eds.)Ad-
vancesinNeuralInformationProcessingSystems.vol.34,pp.19326–19338.Cur-
ran Associates, Inc. (2021), https://proceedings.neurips.cc/paper_files/
paper/2021/file/a11f9e533f28593768ebf87075ab34f2-Paper.pdf 3
104. Yang, G., Vo, M., Neverova, N., Ramanan, D., Vedaldi, A., Joo, H.: Banmo:
Buildinganimatable3dneuralmodelsfrommanycasualvideos.2022IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR)pp.2853–2863
(2021), https://api.semanticscholar.org/CorpusID:245425066 3
105. Yang,J.,Gao,M.,Li,Z.,Gao,S.,Wang,F.,Zheng,F.:Trackanything:Segment
anything meets videos (2023) 7
106. Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., Zhao, H.: Depth anything:
Unleashing the power of large-scale unlabeled data. arXiv:2401.10891 (2024) 4,
7, 9, 11, 13
107. Yang,Z.,Yang,H.,Pan,Z.,Zhu,X.,Zhang,L.:Real-timephotorealisticdynamic
scene representation and rendering with 4d gaussian splatting. arXiv preprint
arXiv:2310.10642 (2023) 3
108. Yang, Z., Gao, X., Zhou, W., Jiao, S., Zhang, Y., Jin, X.: Deformable 3d gaus-
sians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint
arXiv:2309.13101 (2023) 2, 3, 4, 9, 10, 12
109. Yoon, J.S., Kim, K., Gallo, O., Park, H.S., Kautz, J.: Novel view synthesis of
dynamicsceneswithgloballycoherentdepthsfromamonocularcamera.In:Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. pp. 5336–5345 (2020) 322 Authors Suppressed Due to Excessive Length
110. Zhang,Z.,Cole,F.,Li,Z.,Rubinstein,M.,Snavely,N.,Freeman,W.T.:Structure
andmotionfromcasualvideos.In:EuropeanConferenceonComputerVision.pp.
20–37. Springer (2022) 3
111. Zhang, Z., Cole, F., Tucker, R., Freeman, W.T., Dekel, T.: Consistent depth of
moving objects in video. ACM Transactions on Graphics (TOG) 40(4), 1–12
(2021) 3
112. Zheng, Y., Harley, A.W., Shen, B., Wetzstein, G., Guibas, L.J.: Pointodyssey:
A large-scale synthetic dataset for long-term point tracking. In: Proceedings of
the IEEE/CVF International Conference on Computer Vision. pp. 19855–19865
(2023) 3
113. Zollhöfer,M.,Nießner,M.,Izadi,S.,Rhemann,C.,Zach,C.,Fisher,M.,Wu,C.,
Fitzgibbon,A.W.,Loop,C.T.,Theobalt,C.,Stamminger,M.:Real-timenon-rigid
reconstructionusinganrgb-dcamera.ACMTransactionsonGraphics(TOG)33,
1 – 12 (2014) 3