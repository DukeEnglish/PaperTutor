How Private is Low-Frequency Speech Audio in the Wild? An Analysis of
Verbal Intelligibility by Humans and Machines
AilinLiu1,2,PepijnVunderink2,JoseVargasQuiros2,ChiragRaman2,HayleyHung2
1RWTHAachenUniversity,Germany
2DelftUniversityofTechnology,Netherlands
ailin.liu@rwth-aachen.de, p.j.vunderink@student.tudelft.nl, j.d.vargasquiros@tudelft.nl,
c.a.raman@tudelft.nl, h.hung@tudelft.nl
Abstract
Low-frequency audio has been proposed as a promising
privacy-preserving modality to study social dynamics in real-
worldsettings. Tothisend, researchershavedevelopedwear-
abledevicesthatcanrecordaudioatfrequenciesaslowas1250
Hz to mitigate the automatic extraction of the verbal content
of speech that may contain private details. This paper inves-
tigatesthevalidityofthishypothesis,examiningthedegreeto Figure1: Overviewofthestudy. Fromdatasetswithandwith-
whichlow-frequencyspeechensuresverbalprivacy.Itincludes outminglesetting(Section3.1),weprocesstheaudiosamples
simulatingapotentialprivacyattackinvariousnoiseenviron- intolow-frequencyspeechaudio(Section3.2)andbandwidth-
ments. Further, it explores the trade-off between the perfor- extendedlow-frequencyspeechaudio(Section3.3).
mance of voice activity detection, which is fundamental for
understanding social behavior, and privacy-preservation. The
evaluationincorporatessubjectivehumanintelligibilityandau-
of social dynamics. Employing automatic techniques, includ-
tomaticspeechrecognitionperformance, comprehensivelyan-
ing speech-to-text conversion and bandwidth-extension meth-
alyzing the delicate balance between effective social behavior
ods, we aim to explore the potential of extracting meaningful
analysisandpreservingverbalprivacy.
insightsfromtheserecordingswhilesafeguardingtheprivacyof
IndexTerms:socialsignalprocessing,privacy
individualsinvolvedintheconversations.Whileothertechnical
strategiesexistforpreservingsemanticprivacypost-recording,
1. Introduction
low-frequencyaudiopromisestoprovideuserswithagencyin
Speech,asafundamentalmodality,servesasarichsourcefor providing informed consent. Informing users beforehand that
studyingvariousparalinguisticaspectsofhumanbehavior,en- theiraudioistoberecordedatlowfrequenciesforprivacysen-
compassingelementssuchasprosody, intonation, andrhythm sitivityreasonsiscrucialforempoweringuserstochoosehow
[1, 2, 3]. Analyzing these features not only provides insights their data is used for research and promoting social and emo-
intoemotionalstates,socialdynamics,andcommunicationpat- tionalwell-beinginthisresearchspace.
terns[4,5]butalsocontributestoadvancementsinfieldssuch
as linguistics, psychology, and human-computer interaction. 2. Relatedwork
However, analyzing human behavior through speech analysis
presentsasignificantchallengeinensuringprivacy,especially Anapproachforanalyzingturn-takingusingvoiceactivityde-
inreal-worldsettingswhereindividualsmayinadvertentlydis- tection(VAD)inprivacy-sensitivespeechistoextractaudiofea-
closesensitiveinformationinnaturalconversations. tures [9] which cannot be used to reconstruct intelligible ver-
Strikingabalancebetweenextractingvaluableparalinguis- bal speech content. Applying the Principal Component Anal-
ticinsightsandpreservingprivacybecomesparamountinethi- ysis method to an audio spectrogram has been proposed to
calandresponsibleresearchpractices,especiallyinreal-lifeap- detect non-speech activity and prevent speech reconstruction
plications. Onepromisingstrategyistouselow-frequencyau- [10]. Moreover,encryptionmethodsonprivacy-sensitiveaudio
diorecordingsinsmartbadges[6,7]whichallowsforanalysis areavailabletohideverbalcontent[11]forspeakersegmenta-
ofparalinguisticfeatureswhilemitigatingtheriskofinferring tiontasksorobfuscationinurbansoundrecording[12]. Low-
verbalcontent. Recordingatalowfrequencymakesitpossible frequencyaudio[13]hasbeenusedforgroupgenderclassifica-
toinferessentialnonverbalelementsofsocialandemotionalbe- tionunderprivacy-sensitivespeech.Soundshredding[14],slic-
haviorsuchasturn-takingandprosodicfeatureswithoutcom- ing[15],subsampling[16],anddegradation[17]aremethodsto
promisingtheprivacyoftheverbalcontent.Thisisparticularly mutatetherawsoundwhichmakesitdifficulttorecoverthever-
relevantinthewild,whereunscriptedandspontaneousinterac- balcontentoftheoriginalrecordingbutmaintainsomeacoustic
tionsoccur,reflectinggenuinehumanbehavior. featuresofit. Alternatively,replacingtheoriginaldatawithar-
This study investigates the feasibility of leveraging low- tificialspeechgeneratedfromGenerativeAdversarialNetwork
frequency audio recordings captured in real-world settings to (GAN)architectures[18]isused.Also,someworkproposesus-
infer plausible verbal content, which could be any words per- ingspeechembeddingstopreserveprivacy[19].Theadvantage
ceivedartificiallyorbyhumanlisteners.Ourprimaryemphasis of utilizing low-frequency audio lies in its transparent nature.
isonverbalprivacy,asnonverbalfeatures,includinggenderand Userscanconceptualizethesoundoflow-frequencyrecordings
personalattributes[8],arecommonlyinvestigatedinthecontext or actively listen to them, gaining a clear understanding that
4202
luJ
81
]DS.sc[
1v66231.7042:viXratheirprivacyissafeguarded. Incontrasttoalternativemethods
like encryption, where users must rely on trust in researchers
toensuredatausagealignswithconsent,low-frequencyrecord-
ingseliminatecertainpotentialmisuses,becausespecificinfor-
mationisinherentlyabsentfromtherecordedsignal,providing
userswithatangibleandreassuringlayerofprivacy.
3. AnalysisofLow-FrequencyAudio
Weexaminedtheperformanceoflow-frequencyaudioonVAD,
automaticspeechrecognition (ASR),andextendedshort-term
objectiveintelligibility(eSTOI)[20]. Toapplyanintuitiveat-
tack, we used bandwidth-extension (BWE) methods to poten- Figure 2: Performances (means and standard deviations) of
tially improve the intelligibility. Bandwidth extension of au- rVADondifferentsampleratescomparingtooriginalones
dio is a task aiming to enhance speech quality over narrow-
bandtelephoneconnectionsbyextrapolatinghigherfrequencies
missinginthelow-resolutioninput. Toassesstheeffectiveness 3.2.1. Voiceactivitydetection
ofthepotentialattack,thehumanandmachineintelligibilityof
Inthisstudy,weusedrVAD[25]fortheVADtask. Itisanun-
thebandwidth-extendedaudioismeasured. Figure1showsan
supervised segment-based method and is compared favorably
overview of our study. In section 3.1 we present three audio
with some existing methods, such as Kaldi Energy VAD [26]
datasetsthatwereused,eachbeingrecordedindifferentnoise
and VQVAD [27]. To evaluate the performance of rVAD on
settings. In section 3.2 we make a comparison across sample
differentsamplerates,falseerrorrates(FER)arecalculatedas
ratesofVADperformance(3.2.1)andautomaticspeechintel-
theratiobetweenthenumberofwronglycategorizedeventsand
ligibility. Insection3.3weextendtheanalysistobandwidth-
thetotalnumberofactualevents. Thesamedatasetwasdown-
extendedaudio, bothformachinespeechintelligibility(3.3.2)
sampledtodifferentfrequenciesbeforebeingusedforevalua-
andspeechintelligibilitybyhumans(3.3.3).
tiononrVADtotestperformanceacrossfrequencies. 27audio
samples(allsamplesarefromdifferentparticipants)fromPop-
3.1. Datasets
glassand6audiosamplesfromp225ofVCTKaretakeninto
account. Pop-glasssamplesarecutintosegmentsof20to30
We used three datasets in our study: Pop-glass [21, 22],
secondsfrom1houroftheminglingevent. Allsampleswere
VCTK [23], and REWIND [24]. Pop-glass and REWIND
down-sampledto300,800,1250,2000,3200,5000,8000,and
wererecordedinminglingenvironments. InPop-glass,speech
20000Hz,chosentobelogarithmicallyincreasing. Anorder8
is mainly in English, while in REWIND speech is mainly in
ChebyshevtypeILow-passfilteringwasappliedbeforedown-
Dutch. VCTK,ontheotherhand,wasrecordedincleanaudio
samplingtoavoidaliasing.
conditionsandisinEnglish. Furtherdetailsabouteachdataset
Figure2showsthattheFERdropsdramaticallywhengo-
aredescribedbelow.
ingfroma300toan800HzsamplerateontheVCTKaudio.
Pop-glassconsistsof32peoplewhoparticipatedinamingling
Asimilar,thoughlessdramaticdropinFER,isobservedfrom
eventwiththeofficialspokenlanguageinEnglish.Eachrecord-
2000HzonwardsonsamplesfromPop-glass. Eventhoughthe
ingwasapproximately1hourlong. Everyparticipantworean
performanceofVADissensitivetothesamplerates,itisrea-
omnidirectionalLavaliermicrophoneattachedtotheface. The
sonabletousetheVADabove800Hzforcleanspeechaudio
originalfrequencyoftheaudiois44.1kHz.27outof32record-
and2000Hzforspeechaudioinaminglingenvironment.
ings are included in our study after filtering out completely
silentaudioandaudiofrommalfunctioningmicrophones.
3.2.2. Speechintelligibility
VCTK is an English multi-speaker corpus provided in the
CSTR voice cloning toolkit. Each speaker reads a different In this study, automatic speech intelligibility is evaluated in
setofsentencesfromanewspaperarticleinaquietandsingle- termsoftheperformancesofASRandeSTOI.Theperformance
speakersetting. Theoriginalfrequencyoftheaudiois48kHz. of ASR evaluates whether machines can transcribe audio into
The audio of a female speaker is used which aligns with the text. eSTOI is an automated intelligibility listening test that
open-sourcedpre-trainedmodelavailableforspeechenhance- comparesnoisyaudiosourcestoacleanreference.
mentonVCTK. Weemployedtheopen-sourcedASRmodelWhisper[28]
REWIND contains personal audio recorded from individual trainedonmultilingualandmultitasksuperviseddatafromthe
microphones. The setting is a professional networking event web to evaluate samples in different frequencies. To evaluate
witharound100attendees.43consentedtowearamicrophone the performance of the ASR model on different frequencies,
and from this data, 16 people’s audio data were selected for worderrorrate(WER)[29]wascalculated.OutputsoftheASR
ourexperimenttoensureadiversesetofspeakers. Themicro- modelwerepre-processedbylower-casetransformation,white
phoneusedandtheoriginalrecordingfrequencyisthesameas space removal, and bag-of-words reduction before computing
Pop-glass. MostoftheaudioisinDutch,althoughsometimes theWERmetrics.
Englishisalsospoken. Figure3showsthattheWERis∼10%forPop-glassand
is0%forthesamplesfromVCTKat20000Hz. Itshowsthat
open-sourced ASR works well for high-frequency speech au-
3.2. Ananalysisoflow-frequencyspeechaudio
dio. However, the WER is higher than 97.5% for 300 - 800
Duringtheanalysis,themainmotivationwastounderstandhow HzVCTKaudioandhigherthan98%for300-1250HzPop-
thefrequencyofinputspeechaffectsthestate-of-the-artVAD glass audio. This indicates that ASR performance is signifi-
andopen-sourcedASRsystems. cantlyworseonlow-frequencyspeechaudiocomparedtohigh-frequencyspeechaudio. Table1: SNRresultofapplyingmodelstrainedonVCTKand
REWINDandtestedonVCTKandPop-glassaudiorespectively.
A higher score in eSTOI represents a prediction that the
speech intelligibility performance will be better, compared to
a given reference speech signal. The scores range between 0
and 100 (as clear as the original audio). As the eSTOI result VCTKmodel REWINDmodel
shows, samples from both VCTK and Pop-glass maintain 20
samplerates SNR SNR
and40respectively,whenthesampleratesarelowerthan2000
Hz, compared to their high-frequency speech audio. Further- 800Hz 2.3135 0.7124
more,thereislittleimprovementintheintelligibilityprediction 1250Hz 3.6029 1.0873
between800and2000Hz. Generally, asexpected, automatic 2000Hz 6.5619 1.9398
speechintelligibilitydecreaseswithlowerspeechfrequency.
3.3. Analysisofbandwidth-extendedlow-frequencyspeech
To understand the effect of the potential attack on low-
frequency audio, we performed an analysis of ASR perfor-
manceandauserstudyafterabandwidth-extensionprocesson
thesameaudiosamplesmentionedearlier,evaluatingintelligi-
bilitybybothmachinesandhumans.
3.3.1. SimulatinganattackviaBandwidthExtension
By”hallucinating”higherfrequencieswhichareabsentinthe
low-resolutioninput,bandwidth-extensionofaudioaimstoim- Figure4:PerformancesofASRwithBWEandwithoutBWEon
proveaudioqualityandintelligibilityofspeech. Inthisstudy, Pop-glassandVCTKaudiorespectivelywithsamplerates,800,
we used neural bandwidth-extension models [30]. The two 1250,and2000Hzcomparedtothegroundtruthtranscripts
models were trained on REWIND and VCTK respectively to
simulateaprivacyviolationsituation.TheVCTKmodeltrained
andtestedonaudiofromthesamespeakerandnoiseconditions 3.3.2. Machineintelligibility
simulates the lower bound of such an attack. The REWIND
To evaluate how bandwidth-extended audio improves WER
modelsimulatesamoreaggressive,possiblymorerealisticin-
compared with the original low-frequency audio samples, the
formedattackwhereonlythenoiseconditionsofthesampleare
samemodelofWhisperisappliedtoboth.
knownbeforehandandexploitedaspartofapre-trainedBWE
Figure 4 shows that there is a reasonable improvement of
approachusingotherdata(inourcase,Pop-glass). Theopen-
WERachievedbythebandwidthextensionmodelsinthePop-
sourceVCTKmodelistrainedon16kHzaudiofromthesin-
glassaudiosampleswithasamplerateof1250or2000Hzand
glespeakerofVCTKandtheREWINDmodelistrainedon8
theVCTKaudiosampleswithasamplerateof800, 1250, or
and5kHzaudiofrommultiplespeakersofREWIND.Signal-
2000 Hz. The decrease in WER can be interpreted as an im-
to-Noise Ratio (SNR) describes the ratio of signal power to
provement in automatic speech intelligibility. However, most
noise in the signal in the time domain, measuring BWE per-
of the words recovered from the bandwidth extension models
formance. Itrepresentstheintensityoferrorinpredictedaudio
arestop-wordswhichmightbelessinformativeonprivacy.
signalstotheintensityoftheircorrespondingreferencesignals.
AsTable1shows, thehighertheSNR,thebetterthemodel’s
3.3.3. Humanintelligibility
performance. We selected 6 out of 27 samples in Pop-glass
containing the minimum, 25th percentile, two medians, 75th We conducted a perceptual experiment on speech intelligibil-
percentile,andmaximumfundamentalfrequencyF0,asrepre- ity to investigate how much speech intelligibility is preserved
sentativesamples. ToalignwithformantsF1(500Hz)andF2 inlow-frequencyaudio.Typically,speechintelligibilityismea-
(1500 Hz) [31], sample rates at 800, 1250, and 2000 Hz are suredviaratingscales[33]andwordrecognitiontests[34].We
evaluatedintheintelligibilityanalysis,becauseF2hasbeenin- recruited6participantsincluding4malesand2females.Allthe
dicatedforcontributingthemosttointelligibility[32]. participantsconfirmedtheydidn’thaveanyhearingimpairment
and carried out the intelligibility test inside a sound-isolated
listeningbooth. Theywereasked towear headphonesforthe
study,butthevolumewasnotrestricted.Theywerepermittedto
Figure 3: Performances of Whisper on different frequencies
comparedtothegroundtruthtranscriptsandofspeechintelligi-
bilitypredictionfromeSTOIondifferentfrequenciescompared
totheoriginalspeechsignalsrespectively. Figure5:MeanandstandarddeviationofQ1andQ2increaseordecreasethevolumeandlistentotheaudiosamples Table2: Resultoftheproposedmetricsforevaluatingspeech
multiple times. 14 audio samples were used; 6 of them from intelligibilitybyhumans.
Pop-glass, and 8 from VCTK. After listening to each sample,
#recognizable #perceivable
theywereaskedtofilloutaquestionnaireontheintelligibility
dataset frequency Mean SD Mean SD
oftheaudiocontentbasedona7-pointLikertscale.
800Hz 0.75 1.69 8.416 12.62
Q1:Areyouabletohearanythingintheaudiofile?
Pop-glass 1250Hz 3.42 3.2 12.17 10.81
Q2:Areyouabletohearspeechintheaudiofile? 2000Hz 14.58 9.01 22.92 9.38
Q3: Please transcribe the audio file word by word (mark all 800Hz 0.19 0.53 1.25 1.44
perceivedbutnotrecognizedwordswithacharacterX). VCTK 1250Hz 1.6 2.09 5.13 3.42
Q4:Doyouhearmorethanonespeakerintheconversation?If 2000Hz 3.24 4.15 5.64 3.83
youcan,stateroughlyhowmanyspeakersintheconversation ratioofrecognizable longestchain
youthinkthereare. dataset frequency Mean SD Mean SD
800Hz 0.02 0.06 0.42 0.95
Q1andQ2aremeasuredonaLikertscale(1to7,1being
Pop-glass 1250Hz 0.25 0.26 2.17 1.82
“Not at all” and 7 being “Very clearly”). Figure 5 illustrates
2000Hz 0.59 0.24 8.08 5.68
the results. For both datasets, higher sample rates correlated 800Hz 0.07 0.2 0.19 0.53
with higher speech intelligibility scores. However, 2000 Hz VCTK 1250Hz 0.26 0.34 1.4 1.89
audio is not perceived as significantly clearer than 1250 Hz 2000Hz 0.4 0.41 3.0 4.0
audio. Q3ismeasuredbyWERfirstandFigure6showswhen pairwisecosinesimilarity
transcribinglow-frequencyspeechrecordings,humansperform dataset frequency Mean SD
marginally worse than the open-sourced ASR. Q3 is also 800Hz 0.05 0.10
Pop-glass 1250Hz 0.02 0.04
analyzedbyothermetricsinthenextparagraph. Q4isposed
2000Hz 0.24 0.12
for gaining contextual information about whether the main
800Hz 0 0
speaker was transcribed. The number of recognized speakers
VCTK 1250Hz 0 0
in Pop-glass followed a mean and std of 0.67(0.62) at 800 2000Hz 0.05 0.13
Hz; 1.417(0.64)at1250Hz; and1.58(0.64)at2000Hz. For
VCTK,atallsamplerates,themeansandstdwerefoundtobe
1(0). Consequently, the results on VCTK are representative
of the primary speaker. The results of Pop-glass indicate that Thelongestchainofrecognizablewordsoftheaudiosamples
cross-talk constitutes another source of privacy threat; the hasbeenchosentodeterminewhethertherecognizedwordsare
question of whose information is leaked is beyond the scope located randomly or continuously in a sentence. Continuous
ofthepresentanalysisfocusingontheverbalintelligibilityof recognizablewordstendtoprovidemoreinformationthancor-
low-frequencyaudiobutwarrantsfutureinvestigation. poralocatedrandomlyinsentences.
Pairwisecosinesimilarityofeachaudiosamplewaschosento
Metrics for Human Speech Intelligibility: Beyond evaluating measure the similarity between transcripts of two participants
theWERofthetranscripts,asshowninTable2,weintroduce listeningtothesameaudio. Ahigherpairwisecosinesimilar-
thefollowingmetricstomeasurehumanspeechintelligibility: itymeansmoreidenticalwordsaresharedinthetranscriptsof
Thenumberofrecognizablewordsrepresentwordsthatpar- participants. Audioat2000Hzinaminglesettinghasasignif-
ticipants can write down. The number of recognizable words icantlyhigherpairwisecosinesimilaritythanothers. Itreveals
evaluates how many words could be perceived regardless of manywordsthatcanbeidentifiedat2000Hzbutnotat1250Hz
whethertheyweretrulyspoken. forallparticipants.Thus,1250Hzisareasonablethresholdthat
Thenumberofperceivablewordsestimateshowmanywords blocksmostoftheintelligibleverbalcontentduringmingling.
areperceived,includingrecognizablewordsandthosethatcan-
notbespelledbyparticipants,butthebeginningandtheendof
4. Conclusion
whichcanbeidentified.Itprovidesagoodinsightintowhether
theaudiocanbeusedtodetectmultiplepotentialwords.
Weinvestigatedtheprivacy-preservingnatureoflow-frequency
The ratio of recognizable and perceivable words measures
speech audio. While estimating voice activity is desirable for
howmanywordsarerecognizedfromallwordsthatwereper-
turn-taking dynamics in interactions, the ability to transcribe
ceivedtohavebeenspokenintheaudiosample.
specific verbal content is a privacy risk. Our results indicate
that 800 Hz and 2000 Hz are reasonable thresholds for main-
taining VAD functionality whilst blocking intelligible content
in clean and mingling-setting audio. Further, human intel-
ligibility of bandwidth-extended low-frequency speech audio
was slightly lower than an open-source ASR trained on web
data, highlighting the challenges in transcribing such audio.
While low-frequency recording shows promise in preserving
privacy by obstructing intelligible speech, it is not a compre-
hensive solution. It remains an open question whether more
advancedattacksmightstillextractsensitiveinformationfrom
low-frequencyaudio(e.g.modelfine-tuning).
Figure 6: WER Performances of ASR and Human perception
AcknowledgementsThankstoMarthaLarsonforfeedbackon
onPop-glassandVCTKbandwidth-extendedaudiorespectively
ourfinaldraft.ThisworkwaspartiallyfundedbytheErasmus+
with sample rates, 800, 1250, and 2000 Hz compared to the
funding program and the Netherlands Organization for Scien-
groundtruthtranscripts
tificResearch,projectnumber639.022.606.5. References
[17] D. Liang, W. Song, and E. Thomaz, “Characterizing the effect
ofaudiodegradationonprivacyperceptionandinferenceperfor-
[1] A.Vinciarelli,M.Pantic,andH.Bourlard,“Socialsignalprocess-
manceinaudio-basedhumanactivityrecognition,”in22ndInter-
ing: Surveyofanemergingdomain,”ImageandVisionComput-
nationalConferenceonHuman-ComputerInteractionwithMo-
ing,vol.27,no.12,pp.1743–1759,2009.
bileDevicesandServices,ser.MobileHCI’20. NewYork,NY,
[2] B.R.Myers,M.D.Lense,andR.L.Gordon,“Pushingtheen- USA:AssociationforComputingMachinery,2020.
velope: Developments in neural entrainment to speech and the
[18] K. Vatanparvar, V. Nathan, E. Nemati, M. M. Rahman, and
biologicalunderpinningsofprosodyperception,”Brainsciences,
J.Kuang,“Agenerativemodelforspeechsegmentationandob-
vol.9,no.3,p.70,2019. fuscationforremotehealthmonitoring,”in2019IEEE16thInter-
[3] J.J.Guyer,L.R.Fabrigar,andT.I.Vaughan-Johnston,“Speech nationalConferenceonWearableandImplantableBodySensor
rate,intonation,andpitch: Investigatingthebiasandcueeffects Networks(BSN),2019,pp.1–4.
ofvocalconfidenceonpersuasion,”PersonalityandSocialPsy- [19] F. Teixeira, A. Abad, B. Raj, and I. Trancoso, “Towards end-
chologyBulletin,vol.45,no.3,pp.389–405,2019. to-end private automatic speaker recognition,” arXiv preprint
[4] H.Hung,Y.Huang,G.Friedland,andD.Gatica-Perez,“Estimat- arXiv:2206.11750,2022.
ingDominanceinMulti-PartyMeetingsUsingSpeakerDiariza- [20] J. Jensen and C. H. Taal, “An algorithm for predicting the in-
tion,”IEEETransactionsonAudio,Speech,andLanguagePro- telligibility of speech masked by modulated noise maskers,”
cessing,vol.19,no.4,pp.847–860,may2011. IEEE/ACMTransactionsonAudio, Speech, andLanguagePro-
[5] H. Hung and D. Gatica-Perez, “Estimating cohesion in small cessing,vol.24,no.11,pp.2009–2022,2016.
groups using audio-visual nonverbal behavior,” IEEE Transac- [21] K. Schellekens, E. Giaccardi, D. Day, H. Hung, L. Cabrera-
tionsonMultimedia,vol.12,no.6,pp.563–575,2010. Quiros,E.Gedik,andC.Martella,“Impactofconnectedobjects
[6] C.Raman,J.V.Quiros,S.Tan,A.Islam,E.Gedik,andH.Hung,
onsocialencounters,”ReframingDesign.Proceedingsofthe4th
“Conflab: Adatacollectionconcept,dataset,andbenchmarkfor
ParticipatoryInnovationConference,2015.
machineanalysisoffree-standingsocialinteractionsinthewild,” [22] K. Schellekens, “Design for social encounters,” Master Thesis,
inThirty-sixthConferenceonNeuralInformationProcessingSys- 2015.
temsDatasetsandBenchmarksTrack,2022.
[23] J.Yamagishi,C.Veaux,andK.MacDonald,“CSTRVCTKcor-
[7] O. Lederman, A. Mohan, D. Calacci, and A. S. Pentland, pus:Englishmulti-speakercorpusforcstrvoicecloningtoolkit,”
“Rhythm: Aunifiedmeasurementplatformforhumanorganiza- TheCentreforSpeechTechnologyResearch(CSTR),2019.
tions,”IEEEMultiMedia,vol.25,no.1,pp.26–38,2018.
[24] J. V. Quiros, C. Raman, S. Tan, E. Gedik, L. Cabrera-Quiros,
[8] N.Wolfson,“Thesocialdynamicsofnativeandnonnativevaria- andH.Hung,“Rewinddataset: Privacy-preservingspeakingsta-
tionincomplimentingbehavior,”inThedynamicinterlanguage: tussegmentationfrommultimodalbodymovementsignalsinthe
Empiricalstudiesinsecondlanguagevariation. Springer,1989, wild,”2024.
pp.219–236.
[25] Z.-H. Tan, A. Sarkar, and N. Dehak, “rVAD: An unsupervised
[9] S.Parthasarathi,D.Gatica-Perez,H.Bourlard,andM.Magimai- segment-basedrobustvoiceactivitydetectionmethod,”Computer
Doss,“Privacy-sensitiveaudiofeaturesforspeech/nonspeechde- Speech&Language,vol.59,pp.1–21,2020.
tection,” IEEE Transactions on Audio, Speech, and Language
[26] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,
Processing,vol.19,pp.2538–2551,122011.
N. Goel, M. Hannemann, P. Motl´ıcˇek, Y. Qian, P. Schwarz,
[10] E.C.Larson,T.Lee,S.Liu,M.Rosenfeld,andS.N.Patel,“Ac- J.Silovsky´,G.Stemmer,andK.Vesel,“TheKaldispeechrecog-
curateandprivacypreservingcoughsensingusingalow-costmi- nitiontoolkit,”IEEE2011WorkshoponAutomaticSpeechRecog-
crophone,”inProceedingsofthe13thInternationalConference nitionandUnderstanding,012011.
onUbiquitousComputing,ser.UbiComp’11. NewYork,NY,
[27] T.KinnunenandP.Rajan,“Apractical,self-adaptivevoiceactiv-
USA:AssociationforComputingMachinery,2011,p.375–384.
itydetectorforspeakerverificationwithnoisytelephoneandmi-
[11] D. Wyatt, T. Choudhury, and J. Bilmes, “Conversation detec- crophonedata,”in2013IEEEInternationalConferenceonAcous-
tionandspeakersegmentationinprivacy-sensitivesituatedspeech tics,SpeechandSignalProcessing,2013,pp.7229–7233.
data,” inProceedingsofInterspeech2007, vol.1, 082007, pp. [28] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey,
586–589. and I. Sutskever, “Robust speech recognition via large-
[12] A. Cohen-Hadria, M. Cartwright, B. McFee, and J. P. Bello, scale weak supervision,” 2022. [Online]. Available: https:
“Voiceanonymizationinurbansoundrecordings,”in2019IEEE //arxiv.org/abs/2212.04356
29th International Workshop on Machine Learning for Signal [29] A.Morris,V.Maier,andP.Green,“FromWERandRILtoMER
Processing(MLSP). IEEE,2019,pp.1–6. and WIL: improved evaluation measures for connected speech
[13] J.Shen,O.Lederman,J.Cao,F.Berg,S.Tang,andA.Pentland, recognition,”inProceedingsofInterspeech2004,102004.
“Gina: Groupgenderidentificationusingprivacy-sensitiveaudio [30] V. Kuleshov, S. Z. Enam, and S. Ermon, “Audio super
data,” in 2018 IEEE International Conference on Data Mining resolution using neural networks,” 2017. [Online]. Available:
(ICDM),2018,pp.457–466. https://arxiv.org/abs/1708.00853
[14] S.Kumar,L.T.Nguyen,M.Zeng,K.Liu,andJ.Zhang,“Sound [31] T.Nawka,L.C.Anders,M.Cebulla,andD.Zurakowski,“The
shredding: Privacy preserved audio sensing,” in Proceedings speaker’sformantinmalevoices,”JournalofVoice,vol.11,no.4,
of the 16th International Workshop on Mobile Computing pp.422–428,1997.
SystemsandApplications,ser.HotMobile’15. NewYork,NY,
[32] Y.HanandF.Chen, “Relativecontributionsofformantstothe
USA:AssociationforComputingMachinery,2015,p.135–140.
intelligibilityofsine-wavesentencesinmandarinchinese,” The
[Online].Available:https://doi.org/10.1145/2699343.2699366
JournaloftheAcousticalSocietyofAmerica,vol.141,no.6,pp.
[15] M. Maouche, B. M. L. Srivastava, N. Vauquier, A. Bellet, EL495–EL499,2017.
M. Tommasi, and E. Vincent, “Enhancing speech privacy with
[33] S.S.StevensandE.H.Galanter,“Ratioscalesandcategoryscales
slicing,” in Interspeech 2022-Human and Humanizing Speech
foradozenperceptualcontinua,” JournalofExperimentalPsy-
Technology,2022.
chology,pp.1377–411,541957.
[16] S.Kumar,L.T.Nguyen,M.Zeng,K.Liu,andJ.Zhang,“Sound
[34] K.M.YorkstonandD.R.Beukelman, “Acomparisonoftech-
shredding: Privacypreservedaudiosensing,”HotMobile2015-
niquesformeasuringintelligibilityofdysarthricspeech,”Journal
16thInternationalWorkshoponMobileComputingSystemsand
ofCommunicationDisorders,vol.11,no.6,pp.499–512,1978.
Applications,pp.135–140,022015.