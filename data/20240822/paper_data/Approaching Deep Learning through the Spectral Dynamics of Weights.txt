Approaching Deep Learning through the Spectral Dynamics of Weights
DavidYunis1 KumarKshitijPatel1 SamuelWheeler2 PedroSavarese1
GalVardi3 KarenLivescu1 MichaelMaire4 MatthewR.Walter1
Abstract
We propose an empirical approach centered on
thespectraldynamicsofweights—thebehaviorof
singularvaluesandvectorsduringoptimization—
to unify and clarify several phenomena in deep
learning. We identify a consistent bias in opti-
mizationacrossvariousexperiments,fromsmall-
scale“grokking”tolarge-scaletaskslikeimage
classification with ConvNets, image generation (a)Schematic (b)SVs
withUNets,speechrecognitionwithLSTMs,and
languagemodelingwithTransformers. Wealso Figure1: Left: Schematicforthespectraldynamicsofa
demonstratethatweightdecayenhancesthisbias weight matrix. As training proceeds top singular vectors
beyond its role as a norm regularizer, even in becomestableandtopsingularvaluesgrowdisproportion-
practicalsystems. Moreover,weshowthatthese ately large. Right: Singular value evolution for a single
spectral dynamics distinguish memorizing net- matrixinaTransformer,whereeachlineisasinglesingular
works from generalizing ones, offering a novel valueandcolorrepresentsrank. Weseeadisproportionate
perspectiveonthislongstandingconundrum. Ad- trendwherelargesingularvaluesgrowlargerfaster. Weex-
ditionally, weleveragespectraldynamicstoex- plorethesespectraldynamicsofweightsandconnectthem
plore the emergence of well-performing sparse togeneralization,regularization,andseeminglyunrelated
subnetworks(lotterytickets)andthestructureof phenomenalikelinearmodeconnectivity.
thelosssurfacethroughlinearmodeconnectivity.
Ourfindingssuggestthatspectraldynamicspro-
videacoherentframeworktobetterunderstand
2014) of neural networks trained via stochastic optimiza-
the behavior of neural networks across diverse
tion. Even basic questions regarding the role of regular-
settings.
ization like weight decay (Hanson & Pratt, 1988; Krogh
& Hertz, 1991; Zhang et al., 2018a) have only partial an-
swers(VanLaarhoven,2017;Andriushchenkoetal.,2023;
1.Introduction
Yarasetal.,2023b). Perhapsmostvexing,welackacom-
pleteexplanationforhowneuralnetworksgeneralize,de-
Interestinneuralnetworkshasexplodedinthepastdecade.
spitehavingthecapacitytoperfectlymemorizethetraining
Capabilitiesarerapidlyimproving,anddeploymentisever-
data(Zhangetal.,2021). Suchanexplanationmayallowus
increasing.Yet,althoughissueswiththesetechnologiesnow
todesignbetteralgorithms,howeveralackofunderstand-
havesocialrepercussions(Benderetal.,2021;Bommasani
ingmakesthedeploymentofneuralnetworksvulnerable
etal.,2021),manyfundamentalquestionsregardingtheir
touninterpretableerrorsacrossfields(Szegedyetal.,2013;
behaviorremainunanswered.
Ilyasetal.,2019;Hendrycksetal.,2021;Zouetal.,2023).
Forinstance,despiteextensiveresearch,westilllackacom-
Althoughtheoreticalexplanationshavebeenputforwardto
pleteunderstandingoftheimplicitbiases(Neyshaburetal.,
makeanalysistractable,thesestudiesoftenrelyonspecial
1ToyotaTechnologicalInstituteatChicago,Chicago,IL,USA settingslikedeeplinearnetworks(Aroraetal.,2018;2019)
2ArgonneNationalLaboratory,Lemont,IL,USA3WeizmannInsti- orinfinite-widthsystems(Jacotetal.,2018),andarguments
tuteofScience,Israel,workdoneprimarilyatTTIC4Department
mayrelyonunsubstantiatedorimpracticalassumptionslike
ofComputerScience,UniversityofChicago,Chicago,IL,USA.
nearzeroinitialization. Ontheempiricalside, agrowing
Correspondenceto:DavidYunis<dyunis@ttic.edu>.
bodyofworkininterpretabilityhasattemptedtoreverse-
Preprint.Copyright2024bytheauthor(s). engineer neural networks (Rahaman et al., 2019; Barak
1
4202
guA
12
]GL.sc[
1v40811.8042:viXraApproachingDeepLearningthroughtheSpectralDynamicsofWeights
etal.,2022;Nandaetal.,2023),butgiventhedifficultyof eralization,grokkingistypicallystudiedonsynthetictasks,
thetasks,thesystemsofinteresthavebeenverysmall-scale, verysmall-scalemodelslikesingle-layerTransformersor
andthemethodologyforanalysisquitebespokeanddifficult smallMLPs,andrequiresveryparticularhyperparameter
toscale. Athirdcategoryofworkaimsatunderstanding settings(Poweretal.,2022;Nandaetal.,2023;Gromov,
empiricalbehaviorfromahigherlevel(Zhangetal.,2021; 2023;Kumaretal.,2023). Ifourperspectiveistobeuseful,
Huhetal.,2022;Yu&Wu,2023),butwhiletheseworks itneedstoscaletolargersystems. Thus,weturntocom-
oftenstudylarger-scalesystems,theyoftenfocusonmore monempiricaltasksdrawnfromtheliteraturelikeimage
abstractobjectslikethegrammatrix(Huhetal.,2022)or classification,imagegeneration,speechrecognitionandlan-
NTK(Fortetal.,2020),andthusdonothavethegranularity guagemodelingaswellasvariedandlargernetworkslike
andpredictivepoweroftheprevioustwocategories. VGG(Simonyan&Zisserman,2014),UNet(Ronneberger
et al., 2015), LSTM (Hochreiter & Schmidhuber, 1997b)
Tobridgethesegaps,weproposeatask-agnostic,unifying
andTransformers(Vaswanietal.,2017).
perspective of many disparate phenomena in deep learn-
ingacrossmanydifferentpracticaltasksandarchitectures, InSection4,wedemonstratethatthespectraldynamicsare
includingimageclassificationwithConvNets,imagegen- biasedtowardeffectiverankminimizationacrossvarious
eration with UNets, speech recognition with LSTMs and practicalneuralnetworksincomplexsettings.Althoughthis
languagemodelingwithTransformers. Throughextensive behaviorechoestheoreticalpredictionsinthedeeplinear
experiments,weexaminethedynamicsofsingularvalues setting,wefindthatthebehaviorofnetworksdisagreeswith
and singular vectors of weight matrices and the spectral acommontheoreticalassumptionaboutlow-rankdynamics:
dynamicsofweights. Weshowthatsuchdynamicsunder- alignmentofsingularvectorsinconsecutivelayers(Saxe
liemanydistinctphenomenaandappearintimatelytiedto etal.,2014;Aroraetal.,2018;2019;Milanesietal.,2021).
generalization. Wearemotivatedtostudythesedynamics Thus, therankminimizationmechanismmaydifferfrom
specificallyasoptimizationisthefundamentalprocessdriv- whatthetheorydescribes. Itisnotabletoothatourhyper-
ingdeeplearning(Nagarajan&Kolter,2019;Zhangetal., parametersettingsaredrawnfromexistingliterature,thus
2021),thematrixparametersformthecoreidentityofany the trend toward rank minimization coincides with well-
neuralnetwork,andthesingularvaluedecompositionisa generalizingnetworksacrosssettings.
fundamentalwaytoviewanymatrix. Wedetailourspecific
Still, inthesemorepracticalsettings, wedonotseesuch
contributionsinthefollowingparagraphs.
anobviousrankminimizationasingrokking,noristherea
Asatestbedforunderstandinggeneralization,Poweretal. suddentransitionfrommemorizationtogeneralization. One
(2022) introduce the “grokking” phenomenon, where a particularlynotableingredientforgrokkingwasaveryhigh
small-scalemodelinitiallyminimizesthetraininglossbut levelofweightdecay. Weightdecayhasalonghistoryas
performspoorlyonvalidationdata,thenwithmuchmore aregularizerexplicitlypenalizingparameternorm,which
trainingsuddenlyminimizesthevalidationloss. Inpartic- canbeusedfornorm-basedgeneralizationbounds(Bartlett,
ular, Nanda et al. (2023) showed that in simple modular 1996),buttheseboundsdonotseemtoexplainthesuccess
arithmetictasks,thespecificsolutionlearnedbyoptimiza- ofpracticalsystems(Nagarajan&Kolter,2019;Jiangetal.,
tioncouldbereverse-engineeredfromtheweightmatrices. 2019). Recently,othershavesuggestedanimpliciteffectof
Althoughthisdescriptionisprecise,inSection3,wenotice weightdecayonparameterrankintheoreticalorsmall-scale
atask-agnosticviewofgrokking,observingthatthedrop empiricalstudies(Galantietal.,2022;Timoretal.,2023)
invalidationlossduringgrokkingcoincideswiththesimul- whichmaybeconnectedtogeneralization(Razin&Cohen,
taneousdiscoveryoflow-ranksolutionsacrossallweight 2020). Assuch,weexplorethiseffectofweightdecayin
matricesinthenetwork. Theconnectionbetweenrankand practicalsettings.
generalization might intuitively be through Occam’s Ra-
InSection5,weempiricallyconnectrankminimizationto
zor. Furthermore,echoingexistingworks(Lyuetal.,2023;
weight decay, showing that weight decay promotes rank
Liuetal.,2023),wefindthatweightdecayclearlyaffects
minimization across architectures and tasks. In addition,
grokkingandrankminimization: neitherbehavioroccursas
in some cases it also appears to promote singular vector
stronglywithoutit. Havingsaidthat,wedemonstratethat
alignmentinconsecutiveweightsdespitethenonlinearities
increasingthetrainingdatacanpartiallycompensateforthe
betweenlayers,whichindicatesfurthercompressionofthe
absenceofweightdecayintermsofgeneralizationandrank
model. Althoughweightdecayexplicitlypenalizesnorm,
minimization,andinallcases,weseeacorrelationbetween
studyingspectraldynamicsallowsustoobservetheimplicit
low-rankmatricesandgeneralization. Thus, wefindthat
effect on rank. Such an effect can help in understanding
examiningspectraldynamicsprovidesatask-agnosticview
generalizationasnormregularizationisinsufficient.
ofgrokking.
Giventhesuggestiveconnectionbetweenrankandgeneral-
Thoughthissuggestsaconnectionbetweenrankandgen-
ization,weturntotheclassicmemorizationexperimentsof
2ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
Zhangetal.(2021). Zhangetal.(2021)demonstratedthat • Rankminimizationisageneralphenomenoninmore
evensmallnetworkscanmemorizerandomlabels. Hence, complextasks;
any arguments about generalization need to take into ac-
• Weightdecayactsimplicitlyasalow-rankregularizer;
countthestructureofthedataandtheoptimizationprocess.
InSection6,weshowthattrainingwithrandomlabelsleads
• Generalizingsolutionshavealowerrankthanmemo-
tohigh-ranksolutions,whilerankwithtruelabelsismuch
rizingones;and
lower. Wealsofindthatwhilerandomlabelsdonotalign
consecutivelayers,truelabelsdo,whichissurprisinggiven • Topsingularvectorsarepreservedwhenperforming
thenon-linearitiesbetweenlayers. Throughspectraldynam- magnitude pruning while linearly interpolating be-
ics,weseeacleardifferencebetweentheoptimizationof tweenconnectedmodes.
generalizingandmemorizingnetworks,whichprovidesa
footholdintheclimbtowardbettertheoreticalunderstand- Allofthesephenomenaandeffectshavepreviouslybeen
ing. studied in isolation to varying degrees, but by approach-
ingdeeplearningthroughspectraldynamics,weaimata
Ourresultssuggestthatviewingneuralnetworksthrough
commonlanguageforneuralnetworks. Codeforallexperi-
the lens of spectral dynamics can shed light on several
mentsisreleasedathttps://github.com/dyunis/
generalization-relatedphenomena,butwesuspectthereare
spectral_dynamics.
broader connections. In the literature, many curious and
unexplained phenomena regarding neural networks exist.
2.RelatedWork
We take two as case studies. First, the lottery ticket hy-
pothesis(LTH)(Frankle&Carbin,2018),whichhasfound
2.1.Grokking
the existence of sparse sub-networks with similar perfor-
mance. Suchaphenomenonprovidesevidencethat,despite Poweretal.(2022)firstnoticedasurprisingphenomenon
ever-increasingparametercountsandenergycosts,efficient theycalled“grokking”wheremodelsquicklyfitthetrain-
smallernetworksalreadyexist. Understandingthesourceof ingdataontoytasks, thenafteralongperiodoftraining,
suchefficiencymayhelpusalleviatedeploymentcosts. Sec- veryquicklygeneralizeonthevalidationdata. Later,others
ond,linearmodeconnectivity(LMC)(Nagarajan&Kolter, found that this phenomenon can occur in a relaxed fash-
2019;Frankleetal.,2020;Neyshaburetal.,2020),which ion(Thilaketal.,2022)onverysimplemodelsanddifferent
findsthatmodelssharingaportionoftheoptimizationtra- datasets(Liuetal.,2022;Gromov,2023;Kumaretal.,2023;
jectorycanbeaveragedtogetherinweight-spacetoyield Xuetal.,2023)andthatweightdecayseemscriticaltocause
a stronger model (Wortsman et al., 2022; Ramesh et al., it(Lyuetal.,2023;Liuetal.,2023;Tan&Huang,2023).
2022). This phenomenon indicates that, after some train- Somepositatransitionfromthekernel(Jacotetal.,2018)to
ing, the loss surface is quite convex in a subspace, even rich(Atanasovetal.,2023)regimeexplainsgrokking(Ku-
thoughtheoptimizationproblemistheoreticallyextremely mar et al., 2023; Mohamadi et al., 2023). There is also
nonconvex. As any finetuning from pre-trained models empiricalevidenceforaconnectionbetweendoubledescent
staysinthisconvexspace(Neyshaburetal.,2020;Lietal., andgrokking(Daviesetal.,2022)thediscoveryofasparse
2022;Sadrtdinovetal.,2023),anexplanationforwhatun- solution(Merrilletal.,2023),thesimplificationofdecision
derlies model-averaging would help to clarify the role of boundaries(Humayunetal.,2024)andtheleadingindicator
pretraining,wouldshedlightonthecommonly-usedlow- of loss oscillation (Thilak et al., 2022; Notsawo Jr et al.,
rankadaptation(Huetal.,2021),andcouldleadtobetter 2023). Noneoftheseworkshaveexplicitlyexaminedthe
optimization. connectionwithrank,whichwedoinSection3,andpro-
videsacommonframeworkthroughwhichtoviewmanyof
InSection7,wefindthatglobalmagnitudepruning,astan-
theseresults.
dard procedure for finding lottery tickets, preserves top
singularvectorsandactslikealow-rankpruning. Wealso
2.2.SingularValueDynamics
seethattheabilitytointerpolatebetweenmodelsinLMC
stronglycorrelateswithsharingtopsingularvectors. With Prior work on deep linear networks (Arora et al., 2019;
theseresults,wenotethatthetwophenomenacanbeseen Milanesi et al., 2021) suggests that rank minimization
as aspects of the spectral dynamics of weights and bring may describe implicit regularization in deep matrix fac-
themundertheumbrellaofpriorsections. torization better than simple matrix norms. See (Arora
etal.,2018)(AppendixA)foradetailedargument. How-
Tosummarizethediscussionabove,bystudyingthespectral
ever, a critical assumption used in these works is “bal-
dynamicsofweights,wefind:
ancedinitialization.” Thismeansthatforconsecutivema-
(cid:81)
tricesW andW intheproductmatrix W ,wehave
i i+1 j j
• Grokkingisintimatelylinkedtorankminimization; W⊤ W =W W⊤atinitialization. Decomposingthese
i+1 i+1 i i
3ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
matriceswithSVDsandleveragingorthogonality,thissim- 3.GrokkingandRankMinimization
plifiestoV Σ2 V⊤ =U Σ2U⊤whereU andV are
i+1 i+1 i+1 i i i i i+1 Motivated by theoretical work that proposes connections
orthogonalmatrices. Sincetheseareorthogonaldecomposi-
between rank and generalization (Razin & Cohen, 2020)
tionsofthesamematrix,theirdiagonalsmustbeequivalent,
weight decay and rank (Galanti et al., 2022; Timor et al.,
allowing for the permutation of elements with the same
2023;Yarasetal.,2023b;Zangrandoetal.,2024),andthe
value. This leads to U = V O up to signs, where O
i i+1
importanceofweightdecayforgrokking(Poweretal.,2022;
isablockdiagonalpermutationmatrixthatmaypermute
Lyuetal.,2023;Liuetal.,2023)insimplesettings,weeval-
the rows of equivalent diagonal elements. Notably, if all
uate the potential connection between rank and grokking
diagonalelementsaredistinctandU andV aresquare
i i+1
inneuralnetworks. Examininggrokkingthroughthelens
matrices,thenU =V uptosigns.
i i+1
of rank (and more generally spectral dynamics) offers a
Under the balanced initialization assumption, all product complementaryperspectiveongrokkingwithotherdescrip-
matriceswillbealigned. Consequently,theproductofthe tions such as fourier decomposition (Nanda et al., 2023),
diagonalswillevolveinaclosed-formmanner,withlarger thesimplificationoflineardecisionboundaries(Humayun
singularvaluesgrowingfasterthansmallerones. Asshown etal.,2024),theconnectiontodoubledescent(Daviesetal.,
by(Aroraetal.,2019),thistranslatestorank-minimizing 2022),andthediscoveryofasparsesolution(Merrilletal.,
behaviorwithincreasingdepthinthematrixproducts. This 2023).
formulaisalsoempiricallyvalidatedforlinearmatrixfac-
We largely follow the setting of Nanda et al. (2023), op-
torizationproblems. Similarresultshavebeenderivedfor
timizing a single-layer Transformer for modular addition
tensor products and other structured settings (Saxe et al.,
(detailsinAppendixA).Inspiredbyworkinthedeeplinear
2014; Yaras et al., 2023a). More generally, (Ji & Telgar-
case(Saxeetal.,2014;Aroraetal.,2019;Milanesietal.,
sky,2019)showthatfordeeplinearnetworkswithinfinite
2021;Yarasetal.,2023b),wetracktheevolutionofsingular
trainingalignmentbetweenlayerswillhappen. InSection4,
valuesforindividualweightmatrices. Togainahigh-level
we explore how these conclusions and assumptions hold
overviewofallparameterevolutions,wecomputethe(nor-
formuchlarger,practicalneuralnetworksthatarefarfrom
malized)effectiverankofamatrixW (Roy&Vetterli,2007)
linear.
withrankRas
2.3.Low-RankProperties EffRank(W):=−(cid:88)R σ
i log
σ
i , (1)
(cid:80) (cid:80)
Anotherlineofresearchfocusesonmoregenerallow-rank
i=1
jσ j jσ j
biases. Earlyworkexplorednormsasanimplicitbias(Gu- EffRank(W)
nasekaretal.,2017). Theoreticalanalysesrevealthatnorms NormEffRank(W):= , (2)
R
orclosed-formfunctionsofweightsmightbeinsufficientto
where σ ’s are the singular values of matrix W and
explainimplicitregularization,buttheydonotnecessarily i
EffRank(W)istheentropyofthenormalizedsingularvalue
contradict the possibility of rank minimization (Razin &
distribution. Astheprobabilitymassconcentrates,theeffec-
Cohen, 2020; Vardi & Shamir, 2021). Numerous studies
tiverankdecreases. WeplotNormEffRank(W)tocompare
investigatelow-rankbiasesinvariousmatrices,including
acrosslayersandtime.
theJacobian(Penningtonetal.,2018),weightmatrices(Le
& Jegelka, 2021; Martin & Mahoney, 2020; 2021; Frei In addition, inspired by the assumptions of balancedness
et al., 2022; Ongie & Willett, 2022), Gram matrix (Huh madebypriorwork(Aroraetal.,2018;2019),weexamine
et al., 2022), and features (Yu & Wu, 2023; Feng et al., thealignmentofconsecutiveweightmatricesintheTrans-
2022). Additionally,researchsuggeststhatdynamicsinflu- former. Toexamineandquantifythisalignmentbetween
encethedecayofrank(Lietal.,2020;Chenetal.,2023; consecutivematricesinanetworkattrainingtimet,i.e.,
Wang&Jacot,2023). Someworksestablishconnections
R
betweenweightdecayandrankminimizationinidealized (cid:88)
W = σ (t)u (t)v (t)⊤ ,
settings(Ziyinetal.,2022;Galantietal.,2022;Zangrando i j j j
etal.,2024;Ergen&Pilanci,2023;Parhi&Nowak,2023; j=1
Shenoudaetal.,2023).Weareparticularlyinterestedinhow (cid:88)R
W = σ′(t)u′(t)v′(t)⊤ ,
fartheseconnectionsextendinpractice. InSection5,we i+1 k k k
presentempiricalevidencethatsometimesdisagreeswith, k=1
butalsoexpands, argumentsfromtheoryandsmall-scale wecompute,
systemstomuchlargerones.
A(t) =|⟨u (t),v′(t)⟩|, (3)
jk j k
where the absolute value is taken to ignore sign flips in
the SVD computation. We then plot the diagonal of this
4ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
(a)Error (b)SVEvolution (c)EffectiveRank (d)Alignment
Figure2: GrokkingandSpectralDynamics. Toprow: 30%dataandnoweightdecay. 2ndrow: 30%dataandweight
decay1.0(grokking),usinghyperparametersfromNandaetal.(2023). 3rdrow: 70%datawithnoweightdecay(slingshot),
usinghyperparametersfromThilaketal.(2022). Bottomrow: 90%dataandnoweightdecay. 2ndcolumn: Singular
valueevolutionisvisualizedforthefirstattentionparameter,whereeachlinerepresentsasinglesingularvalueandthe
colorrepresentstherank. 4thcolumn: Alignment(Eqn.3)betweentheembeddingandthefirstattentionparameterisalso
visualized,wherethey-axiscorrespondstoindexiofthediagonal. 3rdcolumn: Onecanseethatgrokkingco-occurswith
low-rankweights(effectiverankisEqn.1). Inaddition,thereisanalignmentthatbeginsearlyintrainingthatevolvesup
thediagonal. Withoutweightdecayandwithlessdata,neithergrokkingnortheotherphenomenaoccurduringtheentire
trainingbudget,butusingmoredata,evenwithoutweightdecay,leadstolow-ranksolutionsfromthebeginningoftraining.
Theslingshotcasefollowsasimilartrend,thoughthevalidationlossisgraduallyfit. Acrosscaseswithgoodgeneralization,
parametersarelowerrank,andalignmentisalsomoreprevalentinthetopranks.
5ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
matrixA(t) ∀i≤100overtime. Forexactdetailsonhow 2019;Milanesietal.,2021;Yarasetal.,2023a),weapply
ii
alignmentiscomputedfordifferentarchitecturesandlayers the same analysis to larger, more practical systems. We
thataremorecomplexthanthefullyconnectedcase,please show that the trends we saw in the analysis of grokking
seeAppendixA. mostlyholdtrueacrossnetworksandtasksatamuchlarger
scale,eventhoughourfindingsdooccasionallydeviatefrom
InFigure2,weseeatightalignment: thesuddendropin
theoreticalpredictions.
validation loss coincides precisely with the onset of low-
rankbehaviorinthesingularvalues. Examininginter-layer
4.1.Methodology
alignmentduringtraining,weobservethatthefinallow-rank
solutiongraduallyemergesfromthemodel’smiddleranks. Ourexperimentsaimtoexaminereasonablysizedneural
Conversely, the grokking phenomenon is absent without networksacrossavarietyoftasks. Weselectmodelsand
weightdecay,andnolow-ranksolutionseemstodevelop. tasksthatarerepresentativeofcurrentapplications. Specifi-
We also replicate the setting of Thilak et al. (2022) who cally,wefocuson:
showaformofgrokkingwithoutweightdecay,butwhen
plottedonalinear-scalex-axisthegeneralizationappears
• ImageclassificationwithCNNs(VGG-16(Simonyan
muchlesssuddenthanthesettingwithweightdecay. Addi-
&Zisserman,2014))onCIFAR10(Krizhevsky,2009);
tionally,whenusing90%ofthedataandnoweightdecay,
generalizationstillcoincideswitheffectiverankminimiza- • ImagegenerationthroughdiffusionwithUNets(Ron-
tion. ThefamiliarreaderwillnotethatNandaetal.(2023) nebergeretal.,2015)onMNIST(LeCun,1998);
previouslyshowedthattheparticularsolutionfoundinmod-
ular addition is a low rank fourier decomposition, so our • Speech recognition with LSTMs (Hochreiter &
observationsonlowrankweightswilldirectlyfollow.While Schmidhuber,1997b)onLibriSpeech(Panayotovetal.,
suchadescriptionformodularadditionisimpressivelypre- 2015);and
cise,itmaybedifficulttoobtainformorecomplextasks.
• Language modeling with Transformers (Vaswani
Infollowingsectionswearguethatrankminimizationisa
etal.,2017)onWikitext-103(Merityetal.,2016).
perspectivethatcanapplyinmorecomplexsettingswhen
one does not know what to look for in the weights, and
Traininghundredsofrunsforeachoftheaboveexperiments
it may even be possible to eventually interpret the neural
iscomputationallyexpensive,limitingthescaleofmodels
networkviathetopranks(Praggastisetal.,2022).
wecanexplore. Weprimarilyadopthyperparametersfrom
Wealsowanttohighlightacomplicationbetweennormand existingliterature,withminormodificationsforsimplicity.
generalizationthatourresultsindicate,indialoguewithLiu This ensures that any correlations observed are likely a
etal.(2023).Onecanseethatwithoutweightdecaybutwith reflectionofcommonpractices,notintroducedbiasonour
90%ofthedata,thegeneralizingsolutionthatisdiscovered part. We hope that the broad scope of these experiments
hasmanysingularvalueslargerthan1,andthemaximum willallowforamoregeneralperspectiveonneuralnetwork
singularvalueisalsolargerthanthatofthegrokkedsolution, optimization.
for which most singular values are near zero. Recall the
norm of matrix W is given by ∥W∥ = (cid:112)(cid:80) σ2 where Theprimaryevidenceinthissectioncomesfromcomputing
2 i i
the SVDs of weight matrices within the models. Conse-
σ are the singular values. Thus, the large-data solution
i
quently,wedisregard1Dbiasandnormalizationparameters
has a much higher norm than the grokked solution. Still,
inouranalysis. However,previousresearchsuggeststhat
bothsettingsgeneralizewithacertainlow-rankbehavior.
insomecasestheseparametersarenotcrucialforperfor-
The confounding factor is that, with high weight decay,
mance (Zhang et al., 2018b; Mohan et al., 2019; Karras
thesmallersingularvaluesdisappear,whilewithout,they
etal.,2023). Duetothelargenumberofmatricesinthese
donot. Wecurrentlylackapreciseunderstandingofhow
models, we present plots of individual layers’ matrix pa-
low-rankbehaviordirectlyrelatestogeneralization,andit
rametersandstatisticssummarizingbehavioracrosslayers
mightnotalwaysholdtrue–high-ranksolutionsmightalso
forconcisenessofpresentation. Hundredsofthousandsof
generalize. Nonetheless,fromOccam’sRazor’sperspective,
plots were generated for this study, making it impossible
a low-rank solution is conceptually “simpler” due to its
to include them all. Full experimental details, including
lowerdimensionality,makingforacompellingexplanation.
hyperparameters, areavailableinAppendixA,wherewe
takehyperparametersfromexistingsettingsintheliterature.
4.SpectralDynamicsAcrossTasks
Inspiredbytheresultsongrokkingandpriorworkondeep 4.2.EffectiveRankMinimization
linearnetworks,whichstudiestheevolutionoftheSVDof
Buildingontheoretical(Saxeetal.,2014;Aroraetal.,2019;
theweightmatrices(Saxeetal.,2014;Aroraetal.,2018;
Milanesietal.,2021;Boix-Adsera` etal.,2023;Yarasetal.,
6ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
(a)VGG (b)UNet (c)LSTM (d)Transformer (a)VGG (b)UNet (c)LSTM (d)Transformer
Figure3: Toprow: Singularvalueevolutionforasingle Figure4: Toprow: Traininglossesforalltasks. Bottom
matrixinthemiddleofeachmodel. Eachlinerepresentsa row: Validationlossesforalltasks. Redisthefullmodel.
singularvalue,whereascolorrepresentsrank. Noticethe Blueispost-trainingpruningthebottomhalfoftheSVDfor
unequalevolutionwheretopsingularvaluesgrowatadis- everymatrixinthemodelthatisnotthefinallayer. Green
proportionaterate. Bottomrow: Normalizedeffectiverank ispost-trainingpruningthetophalfoftheSVD.Noticethat
(Eqn.1)evolutionvisualizedincolorfordifferentmatrices forallmodels,keepingthetophalfoftheSVDiscloseto
acrossarchitecturesandtime. Aswemovedownthey-axis, thefullmodelperformance,supportingtheideathatthetop
thedepthoftheparametersinthemodelincreases,whilethe directionsprovideabetterapproximationtothefunction.
x-axistrackstrainingtime. Noticedecreasingeffectiverank
acrossnearlyallparameters,thoughthemagnitudediffers
acrosslayers. Theblock-likepatternsintheVGGcaseare number of small magnitude singular values may provide
likelyduetodifferentchanneldimensionsizes.Thebanding someimportantnoisingeffect,thusempiricalconfirmation
intheUNet,LSTM,andTransformercasesisduetothedif- isnecessary. Anotherwaytoarguethisisthat,thougheach
ferencesbetweenconvolutionalandlinearlayers,residual individualmatrixiswell-approximatedineuclideanspace
blockconnections,andattentionandfullyconnectedlayers, byitstopranks,itisunclearthetotaleffectontheneural
respectively. Thesharptransitionmidwaythroughtraining network function from pruning these matrices simultane-
intheVGGcaseislikelyduetoa10×learningratedecay. ously. Wewillrelyinlatersectionsonthisobservationthat
largesingularvaluesaremoreimportanttothefunctionof
thenetwork.
2023a)andempirical(Dittmeretal.,2019;Martin&Ma-
4.3.AlignmentofSingularVectorsBetweenLayers
honey,2020;2021;Boix-Adsera` etal.,2023)findings,we
investigateeffectiverankminimizationacrossparameters Similartotheanalysisofgrokking,weinvestigatethealign-
in larger models and on a more diverse variety of tasks. ment between consecutive layers in the larger neural net-
Figure 3 reveals a consistent trend: the effective rank of worksconsideredinthissection. Wenotonlyemploythe
networkparametersgenerallydecreasesthroughouttraining, alignmentmatrixdefinedinEqn.3butalsoderiveandplot
regardlessofthespecificparameterornetworkarchitecture. ascalarmeasureforalignmentbasedonthetopdiagonal
Thissuggestsaprogressive“simplification”ofthenetwork entries:
astrainingprogresses,andechoesourpreviousfindingsin 1 (cid:88)10
thehigh-dataregimeonmodularaddition. a(t)= 10 A(t) ii (4)
i=1
We further conduct a singular-value pruning experiment
Forspecificdetailsoncalculatingthismeasureindiverse
toexploretherelationshipbetweenlow-rankbehaviorand
architecturesandcomplexlayers(beyondfullyconnected
modelperformance. Wepruneeitherthetoporbottomhalf
layers),pleaserefertoAppendixA.
of the singular values for each weight matrix in the net-
workandthenevaluatetheprunedmodelateachtraining Figure5revealsakeyfinding: thetheoreticalassumptionof
step. Intuitively,weexpectthetopsingularvaluestocap- balancedinitialization,whichpositsalignedsingularvalue
turethemostcriticalinformationforthenetwork’sfunction. decompositions (SVDs) between weight matrices (Arora
Figure4confirmsthishypothesis,demonstratingthatthe etal.,2018;Saxeetal.,2014),doesnotholdtrueatthestart
prunedparameters,withoutfurthertraining,cancloselyap- oftrainingintheselargernetworks. Additionally,unlikethe
proximatethefullmodel’sperformance. Thismaynothave linearcasediscussedinDuetal.(2018),thealignmentdoes
beenthecase. Inparticular,simultaneouslypruninglower notappeartoremainstaticthroughouttraining. However,
componentsacrossalllayersmayleadtolosingsomecriti- aweaksignalofalignmentinthetopranksdevelops. This
calsignalthatmustbepassedbetweenlayers,orthelarge trendissomewhatreminiscentofthetheoreticalresultpro-
7ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
rankR. Wesawthatlargersingularvaluesofneuralnet-
worksgrowfaster(Fig.3toprow)andthatthetopsingu-
larvectorsaremuchmoreusefulforminimizingtaskloss
than the bottom ones (Fig. 4). Thus, with minor weight
decayregularization,onestraightforwardsolutionforthe
network may be to minimize the rank of a given weight
matrixwhilepreservingthetopsingularvaluestominimize
L(W). Timoretal.(2023)argueasimilareffectasifall
singularvaluesarelessthan1,thenormofactivationswill
shrinkwithdepth,soitwillbeimpossibletopasssignals
(a)VGG (b)UNet (c)LSTM (d)Transformer withsufficientlydeepnetworks. Thusitisimportantfora
fewsingularvaluestobesufficientlylarge.
Figure5: Neighboringlayeralignmentofsingularvectors.
Top row: The diagonal of the alignment matrix A(t) Figure6showsthataddingweightdecayproducesthisexact
ii
(Eqn. 3) vs. training time for a single pair of matrices in low-rankbehavior,whiletoomuchweightdecayleadsto
themiddleofeachmodel. Weseeasmallamountofalign- completenormcollapse. Theexactchoiceof“toomuch”
mentinthetopranksbetweenlayersshortlyaftertraining variesacrossarchitecturesandtasks,thoughitmaybedue
begins,butthisbecomesmorediffuseovertime. Bottom totheuseofSGDforVGG(Simonyan&Zisserman,2014)
row: Alignmentmetric(Eqn.4)forpairsofmatricesfor insteadofAdamW(Loshchilov&Hutter,2017)fortherest.
depth vs. training time. It is hard to make out a global
Inaddition,evenmoresurprisingly,largeamountsofweight
trendacrossmodels,thoughtheLSTMshowsaweaksignal
decaypromoteatighteralignmentinthetopsingularvectors
aroundEpoch1whentheinitialalignmentoccurs,andthe
ofconsecutivelayersoftheTransformer,butnotothernet-
Transformercasehasabandingpatternwithdepthdueto
works. WeseethisinFigure7. Thisbehaviorisquiterem-
alignmentbetweenthequeryandkeymatricesthathaveno
iniscentofthebalancednesscondition(Aroraetal.,2018;
nonlinearityinbetween.
2019;Duetal.,2018),thoughtheTransformerconsidered
herehasnonlinearitiesandmuchmorecomplexstructures.
Itiscuriousthatthetrendissostrongforonlythisarchitec-
videdbyMulayoff&Michaeli(2020)forlinearnetworks
ture. WealsoprovideadditionalevidenceinAppendixA
undertheassumptionofwhitenedinputdata.Still,theweak-
where Figure 14 shows that the solutions with very high
nessoftheobservedsignalmeansthatexistingtheoretical
weight decay are still performant, even though they are
modelsdonotfullycapturethecomplexitiesofreal-world
muchlowerrank. Thoughitisdifficulttoargueassimplea
neuralnetworktraining.
trendas“lowerrankequalsbettergeneralization”because
onedoesnotknowtheminimalranknecessaryforagiven
5.TheEffectofWeightDecay task,itisnotablethattheroleofweightdecayforimproving
generalizationistiedupwithitsfunctionasarankregular-
In light of the previously observed evolution of singu-
izer. Inaddition,althoughwelackprecisetoolstoentirely
lar values, we investigate a proposed effect of weight de-
interpretcomplexmodels,whenthereareonlyafewranks
cay. Thoughweightdecayexplicitlypenalizesthenormof
per matrix it may become possible to extend analysis ef-
weights,thereisevidencethatcomplicatestheconnection
forts(Nandaetal.,2023;Praggastisetal.,2022)tomore
betweennormandgeneralizationforneuralnetworks(Razin
complexdomains.
&Cohen,2020;Andriushchenkoetal.,2023),meaningwe
do not have a full understanding as to why weight decay
6.SpectralDynamicswithRandomLabels
maybeuseful.Alternatively,sometheoretical(Boix-Adsera`
et al., 2023; Razin & Cohen, 2020; Yaras et al., 2023a;
Giventheobservationsconnectinggeneralizationandrank
Timor et al., 2023; Ongie & Willett, 2022; Galanti et al.,
thusfar,andtheenlighteningviewontheimpliciteffects
2022;Zangrandoetal.,2024)andempiricalworks(Galanti
of weight decay, we are interested in seeing whether the
etal.,2022;Boix-Adsera`etal.,2023)proposeaconnection
perspectivedevelopedshedsanylightontheclassicrandom
with the rank of matrices in constrained settings. Still, a
labelmemorizationexperimentsofZhangetal.(2021).
comprehensiveconnectiontolargerempiricalnetworkshas
notyetbeendemonstrated. SimilartoZhangetal.(2021),wetrainasimpleMLPtofit
randomortruelabelsonCIFAR10. PleaseseeAppendixA
We speculate on the intuition of the mechanism in more
forthedetailsregardingtheexperimentalsetup. Zhangetal.
practicalsettings. Noticeinitssimplestformthatweightde-
(2021)decaythelearningratetozero,andtherandomlabel
cayasksforargmin L(W)+λ∥W∥2,where∥W∥2 =
W F F experimentsonlyconvergelateintraining. Consequently,
(cid:80)R σ2withsingularvaluesσ ofweightmatrixW with
i=1 i i
8ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
(a)VGG (b)UNet (c)LSTM (d)Transformer
Figure6: SVevolutionforasinglematrixandnormalizedeffectiverank(Eqn.1)acrossmatricesovertime,wherethe
rowsusedifferingamountsofweightdecay. Fromtoptobottom,forVGGweusecoefficients{0,0.001,0.01,0.1},while
for other networks we use coefficients {0,0.1,1,10}. Higher weight decay coefficients promote more aggressive rank
minimization. VGGusesSGDandmomentum,whiletherestuseAdamW(Loshchilov&Hutter,2017),whichmayexplain
theearliernormcollapse.
weuseaconstantlearningratetocontrolthisphenomenon. generalization. Still,thelensofspectraldynamicscanbe
WeseeinFigure8thatbothcasesareabletoachievezero appliedmorebroadly. Inthefollowingsubsections,weex-
error, though with different singular value evolution and plore two phenomena: lottery tickets (Frankle & Carbin,
alignmentinthemiddlelayer. 2018)andlinearmodeconnectivity(Frankleetal.,2020).
Beyond shedding further light on neural networks, these
Surprisingly,weseethatwithtruelabelstheinnerlayersare
phenomenahaveimplicationsformoreefficientinference
lowrank,whilewithrandomlabelstheyaremuchhigher
andstorage,aswellasunderstandingtheimportanceofpre-
rank. Thismaybeexplainedbythesharedstructureinthe
training(Neyshaburetal.,2020).Wefindthatlotterytickets
trueclassesofthedataset,whichmanifestsintheparame-
areasparseapproximationoffinal-checkpointtopsingular
ters. Evenmoresurprisingly,wefindherethatevenwithout
vectors. Theabilitytolinearlyinterpolatebetweenfaraway
weightdecay,innerlayersalignwithtruelabels,whilewith
checkpointsandimproveperformancecoincidesstrongly
randomlabels,thisalignmentoccursandthendisappears
withtopsingularvectorsharingbetweencheckpoints. Such
withmoretraining. Thisisparticularlyintriguingasthere
observationsmayformafoundationforabetterunderstand-
arenon-linearitiesthatcouldtheoreticallyseparatethenet-
ing compression and model averaging (Wortsman et al.,
workfromthelinearcase,andyetstrongalignmentoccurs
2022;Ilharcoetal.,2022).
despitethat. Suchalignmenthasnotyetbeenleveragedby
existingtheory,andmightprovidestructuredassumptions
7.1.TopSingularVectorsBecomeStableEarlier
fornewunderstanding. Insummary,theseresultssuggest
thatviewinggeneralizationthroughthelensofrankmaybe Beforeweexplorethephenomena,wefirstmakeanother
fruitful. observation that will be helpful. As top singular values
growdisproportionatelylarge,itwouldbenaturalthattop
7.BeyondGeneralization singularvectorsbecomestableindirectionasthegradients
remainsmall. Todemonstratethis,foragivenmatrixinthe
We have seen over the course of many experiments that networkW (t)=(cid:80)R σ (t)u (t)v (t)⊤ attrainingtime
i j=1 j j j
deep models are biased toward low rank, and that there
is a tempting connection between rank minimization and
9ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
(a)VGG (b)UNet (c)LSTM (d)Transformer
Figure7: Diagonalofalignmentforasinglepairovertime(Eqn.3)andalignmentmetricacrosspairsofmatricesovertime
(Eqn.4)wherethey-axisrepresentsdepth. Fromtoptobottom,forVGGweusecoefficients{0,0.001,0.01,0.1},while
forothernetworksweusecoefficients{0,0.1,1,10}. Weseethatthemaximumalignmentmagnitudeishigherwithlarge
weightdecay,andinparticular,theTransformerhasthestrongestalignmentevenwhennonlinearitiesseparatetheMLP
layers.
(a)TrainErr. (b)Val.Err. (c)SVs (d)Eff.Rank (e)Alignment
Figure8: Toprow: resultswithtruelabels. Bottomrow: resultswithrandomlabels. Weseethatthemiddlelayershavea
lowereffectiverankwhenusingtruelabelsandthatalignmentinthemiddlelayerspersiststhroughouttraining,unlikeinthe
randomlabelcase. Weemphasizethisalignmentoccursdespitethenonlinearities.
t,wecompute whereT isthefinalstepoftraining,andtheabsolutevalueis
takentoignoresignflipsintheSVDcomputation. Wethen
plotthediagonalofthismatrixS(t) ∀i≤100overtime.
ii
Wealsouseascalarmeasureofthediagonaltosummarize
S(t) =|⟨u (t)v (t)⊤,u (T)v (T)⊤⟩|, (5)
jk j j k k
10ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
InFigure10,weplotthesingularvectoragreement(SVA,
Eqn. 5) between the final model, masked and unmasked,
whereweseeexactlythatmagnitudemaskspreservethetop
singularvectorsofparameters,andwithincreasingsparsity
fewerdirectionsarepreserved. Eventhoughpriorworkhas
remarkedthatitispossibletouselow-rankapproximations
forneuralnetworks(Yuetal.,2017), andothershaveex-
plicitlyoptimizedforlow-ranklotterytickets(Wangetal.,
2021;Schottho¨feretal.,2022),weratherarepointingout
thatthemagnitudepruningprocedureseemstorecovera
(a)VGG (b)UNet (c)LSTM (d)Transformer low-rankapproximation.
Figure9: Toprow: Singularvectoragreementforasingle Wealsocomputethesingularvectoragreement(SVA)be-
matrix in the middle of each model (diagonal of Eqn. 5). tween the masked model trajectory and the original un-
Noticetopsingularvectorsbecomestableindirectionear- maskedmodeltrajectory(diagonalofEqn.5). Weseein
lier. Bottomrow: Summaryscoreforeachmatrixacross Figure 10 that there is no agreement between the bottom
architectures. Aswemovedownthey-axis,thedepthofthe singularvectorsatall,butthereisstilllooseagreementin
parametersinthemodelincreases,whilethex-axistracks thetopsingularvectors. Thus,itseemsthemaskallowsthe
trainingtime. Thesharptransitionmidwaythroughtraining dynamicsofonlythetopsingularvectorstoremainsimi-
intheVGGcaseislikelyduetoa10xlearningratedecay. lar, whichweknowaremostimportantfromthepruning
analysisinFigure4.
Preserving top singular vectors by pruning seems like a
likeinthealignmentcase:s(t)= 1 (cid:80) S(t) .InFigure9,
10 i ii naturaloutcomeoflargematrices,soasacontrol,wefollow
weseethattopsingularvectorsconvergeindirectionearlier
exactly the same protocol except we generate the mask
thanbottomvectors.
randomlywiththesamelayerwisesparsity. Wecanseein
Figure 10 that this results in much lower preservation of
7.2.LotteryTicketsPreserveFinalTopSingularVectors topsingularvectordynamics,andalsoperformsworse,as
in (Frankle et al., 2020). It would not be surprising that
Aslargesingularvectorswillbecomestablelateintraining,
randompruningisworseifsimplyevaluatedattheendof
wewonderabouttheconnectiontomagnitudepruningand
training, but masking is applied quite early in training at
thelotterytickethypothesis. Frankle&Carbin(2018)first
epoch4of164longbeforeconvergence,soit’sstrikingthat
showedevidenceforthelotterytickethypothesis,theidea
thenetworknowfailstolearnfurthereventhoughitisfar
thatthereexistsparsesubnetworksofneuralnetworksthat
from convergence. We interpret this as evidence that the
canbetrainedtoacomparableperformanceasthefullnet-
mask has somehow cut signal flow between layers, so it
work,wherethesparsemaskiscomputedfromthelargest
is now impossible for the network to learn further, while
magnitude weights of the network at the end of training.
magnitudepruningandrewindingstillallowssignalstopass
Frankle et al. (2020) build further on this hypothesis and
thateventuallybecomeimportant.
noticethat,forlargernetworks,themaskingcannotbeginat
initialization,butratheratsomepointearlyintraining. Still,
7.3.SpectralDynamicsandLinearModeConnectivity
themaskmustcomefromtheendoftraining.
Thereasonforthisparticularchoiceofmaskmaybecon- Wecometothefinalphenomenonthatweseektodescribe:
nectedtothedynamicswepreviouslyobserved.Specifically, linearmodeconnectivity. Linearmodeconnectivity(LMC)
at the end of training large singular values are dispropor- is the property that one can interpolate linearly between
tionatelylarger,sohigh-magnitudeweightsmaycorrespond twodifferentminimainweightspaceandeveryparameter
closely to weights in the top singular vectors at the end setalongthatpathperformswell,whichgivestheimpres-
oftraining. Ifmagnitudemaskswerecomputedatthebe- sion that the loss surface of neural networks is somehow
ginning,thedirectionsthatwouldbecomethetopsingular convexdespiteitstheoreticalnonconvexity. Thiswasfirst
vectorsmightbeprematurelymaskedastheyhavenotyet demonstrated in small networks with the same initializa-
stabilized,whichmaypreventlearningonthetask. tion(Nagarajan&Kolter,2019),thenexpandedtolarger
networks and connected to lottery tickets (Frankle et al.,
HerewetrainanunmaskedVGG-16(Simonyan&Zisser-
2020;Pauletal.,2022). Entezarietal.(2021)firstconjec-
man, 2014) on CIFAR10, then compute either a random
turethattwoarbitraryminimashowLMCuptopermutation,
mask, or a global magnitude mask from the end of train-
anddemonstrateitinsimplemodels. Thiswasexpandedto
ing,andrewindtoanearlypoint(Frankleetal.,2020)to
widemodels(Ainsworthetal.,2022;Jordanetal.,2022;Qu
startsparseretraining. PleaseseeAppendixAfordetails.
11ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
(a)Loss (b)PrunedSVA (c)AllLayers (d)SVAevol. (e)AllLayers
Figure10: Toprow: Magnitudepruning. Bottomrow: randompruning. Firstcolumn: Trainingloss. Weseethatat
5%sparsitymagnitudepruningissignificantlybetterthanrandompruningofthesamelayerwisesparsity. 2ndcolumn:
Singularvectoralignmentpre-andpost-pruningattheendoftrainingforasinglelayer(the3rdconvolution). Weseethat
magnitudepruningapproximatesthetopsingularvectors,whilerandompruningatthesameleveldoesnot. 3rdcolumn:
Singularvectoralignmentscorepre-andpost-pruningacrossalllayers. Agreementishigheracrossalllayersformagnitude
pruning,thoughlaterlayersdonotagree,likelyaslaterlayersarewidersoweightsarelowermagnitude. 4thcolumn:
Singularvectoralignmentbetweentheprunedandunprunedmodelsalongthetrainingtrajectory. Weseethatthemagnitude
pruningstillhassimilardynamicsinitstopsingularvectors,whilerandompruningdoesnot. Lastcolumn: Singularvector
alignmentscorebetweenprunedandunprunedmodelsacrosslayersandtime. Againevolutionissimilarforearlylayers
withmagnitudepruning,andcompletelydifferentforrandompruning.
&Horvath,2024),andcanbeproveninvariousways(Ku- matricesateitherendpointofbranches:
ditipudietal.,2019;Breaetal.,2019;Simseketal.,2021;
Ferbachetal.,2023),butitdoesnotholdforstandardmod- (cid:88)R
W(1)(T)= σ (T)u (T)v (T)⊤ ,
els(Qu&Horvath,2024). LMChasalsobeenexploitedfor j j j
model-averagingandperformancegains(Wortsmanetal., j
2022;Ilharcoetal.,2022;Rameetal.,2022).Stilldespiteall (cid:88)R
W(2)(T)= σ′(T)u′(T)v′(T)⊤ ,
ofthiswork,welackadescriptionforwhyLMCoccurs. In k k k
particular: whyisthereaconvex,highdimensional(Yunis k
etal.,2022)basinthatmodelsfindshortlyintraining(Fran-
spawned from the same initialization in training. If the
kleetal.,2020),orafterpretraining(Neyshaburetal.,2020;
branchesaresplitfromaninitializationonatrunktrajectory
Sadrtdinovetal.,2023)? Wedonotanswerthisquestion
W(t), we call t the split point or epoch. We visualize
in full, but find an interesting view through the singular thediagonalof|⟨u (T)v (T)⊤,u′(T)v′(T)⊤⟩| vs.split
j j k k jk
vectors.
epoch,wheretheabsolutevalueistakentoignoresignflips
inSVDcomputation.
7.3.1.LINEARMODECONNECTIVITYCORRELATES
WITHTOPSINGULARVECTORAGREEMENT Toremindthereader,LMConlyoccursafterasmallamount
oftrainingtimehaspassed.Tooearlyandthefinalmodelsof
Aswesawearlierdirectionalconvergenceoftopsingular
eachbranchwillshowabump,orbarrier,inthelosssurface
vectorsinFigure9,itsuggeststhedynamicsofthosecompo-
along the linear interpolation (Frankle et al., 2020). To
nentsaremorestable,sowemightexpectmode-connected
measurethisprecisely,weusethedefinitionfromNeyshabur
solutionstosharethesecomponents. Toexaminethis,we
etal.(2020),whichisthemaximumdeviationfromalinear
plotagreementbetweenthesingularvectorsoftheweight
interpolationintheloss,anempiricalmeasureforconvexity
inthislineardirection.Whenthisdeviationis0,weconsider
thecheckpointstoexhibitLMC.PleaseseeAppendixA.9
fordetailsonthecalculation. GivenevidenceinFigure4
thattopcomponentsarethemostimportantforprediction,
12ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
LMCpropertyshouldnotsharetopsingularvectors,while
checkpointsthatdoexhibittheLMCpropertyshouldshare
top singular vectors. We see in Figure 11 that this is the
caseacrossmodelsandtasks,wherethealignmentbetween
endpointsismuchstrongerintopsingularvectors. Wealso
see no LMC and poor agreement in top components be-
tweenbranchesthathaveinitializationsfromdifferenttrunk
trajectories,butwiththesamesplitepochtandthesame
branchdataorderinFigure12. Thus,thesetopdirections
arenotauniquepropertyofthearchitectureanddata,but
ratheraredependentoninitialization. Itisnotablethatcon-
currentwork(Itoetal.,2024)arrivesatasimilarconclusion:
permutationsolversbetweenoptimamatchtopsingularvec-
tors. Thoughtheconclusionsaresimilar,theirexperiments
areprimarilyconductedonsmallerscalesettings,andonly
(a)VGG (b)UNet (c)LSTM (d)Transformer
forpermutationmatchingattheendoftraining. Herewe
connecttheseobservationstotheoptimizationbehaviorof
Figure11:Toprow:Barriersizevs.splitstep. Middlerow:
networksthroughouttraining.
singularvectoragreementforasinglematrixparameterbe-
tweenbranchendpointsthatshareacommontrunk.Bottom
row:summarystatisticforsingularvectoragreementacross
7.3.2.PERTURBINGBREAKSLINEARMODE
layersvs.splitstep. WeseethatasmodelsexhibitLMC,
CONNECTIVITYANDSINGULARVECTOR
theyalsosharetopsingularvectors.
AGREEMENTSIMULTANEOUSLY
Tomaketheconnectionbetweentopsingularvectorsand
LMCeventighter,weinterveneinthenormaltrainingpro-
cess. If we add random perturbations to destabilize the
componentsthatwillbecomethetopcomponentslongbe-
foretheyhaveconverged,andifsingularvectoragreement
istiedtoLMC,wewouldliketoseethatfinalmodelsno
longerexhibittheLMCproperty. Indeedthisisthecase. In
Figure 13, when increasingly large random perturbations
areapplied,thebarrierbetweenfinalcheckpointsincreases
andtheLMCbehaviordisappears. PleaseseeAppendixA
fordetails.Inaddition,thepreviously-strongsingularvector
agreement disappears simultaneously. Thus it seems this
agreementistiedtolinearmodeconnectivity.
Wespeculatethat,duetotheresultsinFigure4thatshow
thetophalfoftheSVDsaremuchmorecriticalforperfor-
(a)VGG (b)UNet (c)LSTM (d)Transformer mance,ifthesecomponentsaresharedtheninterpolating
willnotaffectperformancemuch. Rather,interpolationwill
Figure12:Toprow:Barriersizevs.splitstep. Middlerow: eliminate the orthogonal bottom components which may
singular vector agreement for a single matrix parameter onlymakeaminorimpactonperformance. Ifhoweverthe
betweenbranchendpointsthatdonotshareacommontrunk, topcomponentsarenotshared,theninterpolatingbetween
but do share split time and branch data order. Bottom twomodelswillremovethesecomponents,leadingtopoor
row:summarystatisticforsingularvectoragreementacross performance in between. Such observations may help in
layers. Weseethatwhenbranchesdonotshareacommon explainingtheutilityofpretraining(Neyshaburetal.,2020),
trunk,thereisneitherLMCnorsingularvectoragreement, weightaveraging(Rameetal.,2022;Wortsmanetal.,2022;
eventhoughtheoptimizationisotherwisethesame. Ilharcoetal.,2022)ortheuseofLoRA(Huhetal.,2022)
toreplacefullfinetuning.
andthattopcomponentsbecomestablebeforetraininghas
8.Discussion
finished,itisplausiblethatLMCisconnectedtothestability
oftopsingularvectorsinthelaterportionoftraining. We provide an empirical perspective to understand deep
learningthroughthelensofSVDdynamics. Wefirstnote
Thiswouldmeanthatcheckpointsthatdonotexhibitthe
13ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
areaofresearch(Nandaetal.,2023),andtherealreadyexist
earlyeffortstointerpretsingularvectorsofconvolutional
weights(Praggastisetal.,2022). Theremayalsobecon-
nectionstootherunexplainedphenomenasuchasdouble
descent(Belkinetal.,2019;Nakkiranetal.,2021;Davies
etal.,2022)oradversarialexamples(Szegedyetal.,2013;
Ilyasetal.,2019;Hendrycksetal.,2021). Thesolutionsto
theseproblemsmayhelpusdiagnosedeploymentrisksin
thewild,andideallyhelptodesignbetteralgorithms. We
believeourresultscontributeanotherstepalongthispath.
Acknowledgements
DYwouldliketothankthefollowingpeopleforhelpfuldis-
cussionatmanydifferentstages: WeiHuforaconversation
(a)VGG (b)UNet (c)LSTM (d)Transformer
onconnectingLMCtorankdynamicsindeepmatrixfactor-
ization;JonathanFrankleforveryearlysupportandperiodic
Figure13: Toprow: Barriersizevs.perturbationmagni-
discussions on aiming to understand the source of LMC;
tude. Middlerow: singularvectoragreementforasingle
ShubhamToshniwalandShaneSettleforsupportinfollow-
matrixparameterbetweenbranchendpointsvs.perturbation
ing this line of work in the early stages; Mikhail Belkin,
magnitude. Bottom row: summary statistic for singular
TengyuMa,andRobNowakforearlyconversationsanden-
vectoragreementacrosslayerswithperturbationmagnitude.
couragementonstudyingrankminimizationinsmall-scale
We see that whereas without perturbation models would
cases;SudarshanBabu,HaochenWang,TakumaYoneda,
exhibitLMCaftertraining, withincreasingperturbations
andXiaoZhangfordiscussionsandenthusiasmonresum-
theLMCpropertydisappearssimultaneouslywiththeagree-
ingthisworkwhenithadbeenpaused;SamBuchananfor
mentintopsingularvectors.
similarenthusiasmonresumingtheworkandveryhelpful
commentsonpresentation; AnmolKabraforsuggestions
onadditionalexperimentsandvisualizationstoroundout
a tendency toward rank minimization on a small scale in thework;andDavidMcAllesterforveryhelpfulcomments
grokking,thenexpandthesefindingstopracticalnetworks onpresentation. DYwouldalsoliketothankAdamBoh-
and tasks. In addition we find that weight decay, though landerforhisworkinmaintainingtheTTICcluster,which
itexplicitlypenalizesnorm,implicitlypromotesthislow- made the many experiments in this paper possible. This
rank bias. We also show through the developed analysis materialisbaseduponworksupportedbytheNationalSci-
thatgeneralizationandmemorizationdifferintherankand enceFoundationGraduateResearchFellowshipProgram
alignmentofsolutionsfoundbyoptimization. underGrantNo. 1754881. KKPwassupportedthroughthe
NSF TRIPOD Institute on Data, Economics, Algorithms
We go beyond remarks on generalization and show that
andLearning(IDEAL)andotherawardsfromDARPAand
magnitudepruningforlotteryticketsactssimilarlytolow-
NSF.
rankpruning,andlinearmodeconnectivitycoincideswith
the sharing of top singular vectors between checkpoints.
Lowrankmodelshavemoreefficientinference, andfine-
tuningfrompretrainedmodelsalwaysoperatesintheLMC
regime (Neyshabur et al., 2020), thus this understanding
canleadtomoreefficientinferenceandcompressionand
possiblystrongeroptimizationpractices.
Whileacomprehensivetheoryforourresultsremainselu-
sive,theseobservationscanactasaplatformforadeeper
understandingofdeeplearning. Notably,theobservedspec-
traldynamicsappearconsistentacrossdiversesettings,even
withoutrestrictiveassumptionslikebalancedinitialization,
linearity,orsmallweightscales. Thissuggestsacommon
underlyingmechanism.
Ontheempiricalside,severalinterestingproblemspresent
themselves. Interpretabilityofneuralnetworksisagrowing
14ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
References Boix-Adsera`, E., Littwin, E., Abbe, E., Bengio, S., and
Susskind,J.M. Transformerslearnthroughgradualrank
Ainsworth, S., Hayase, J., andSrinivasa, S. Gitre-basin:
increase. InThirty-seventhConferenceonNeuralInfor-
Merging models modulo permutation symmetries. In
mationProcessingSystems,2023.
ProceedingsoftheInternationalConferenceonLearning
Representations(ICLR),2022. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
Arora,S.,vonArx,S.,Bernstein,M.S.,Bohg,J.,Bosse-
Andriushchenko,M.,D’Angelo,F.,Varre,A.,andFlammar-
lut,A.,Brunskill,E.,etal. Ontheopportunitiesandrisks
ion,N. Whydoweneedweightdecayinmoderndeep
offoundationmodels. arXivpreprintarXiv:2108.07258,
learning? arXivpreprintarXiv:2310.04415,2023.
2021.
Arora,S.,Cohen,N.,andHazan,E. Ontheoptimizationof Brea,J.,Simsek,B.,Illing,B.,andGerstner,W. Weight-
deepnetworks: Implicitaccelerationbyoverparameteri- spacesymmetryindeepnetworksgivesrisetopermuta-
zation. InProceedingsoftheInternationalConference tionsaddles,connectedbyequal-lossvalleysacrossthe
onMachineLearning(ICML),2018. losslandscape. arXivpreprintarXiv:1907.02911,2019.
Arora, S., Cohen, N., Hu, W., andLuo, Y. Implicitregu- Chen, F., Kunin, D., Yamamura, A., and Ganguli, S.
larizationindeepmatrixfactorization. InAdvancesin Stochastic collapse: How gradient noise attracts SGD
NeuralInformationProcessingSystems(NeurIPS),2019. dynamicstowardssimplersubnetworks. InAdvancesin
NeuralInformationProcessingSystems(NeurIPS),2023.
Atanasov,A.,Bordelon,B.,Sainathan,S.,andPehlevan,C.
Davies, X., Langosco, L., and Krueger, D. Unifying
Theonsetofvariance-limitedbehaviorfornetworksinthe
grokking and double descent. In NeurIPS ML Safety
lazyandrichregimes.InProceedingsoftheInternational
Workshop,2022.
ConferenceonLearningRepresentations(ICLR),2023.
Dittmer,S.,King,E.J.,andMaass,P. Singularvaluesfor
Barak,B.,Edelman,B.,Goel,S.,Kakade,S.,Malach,E.,
ReLU layers. IEEE Transactions on Neural Networks
andZhang,C. Hiddenprogressindeeplearning: SGD
andLearningSystems,31(9):3594–3605,2019.
learnsparitiesnearthecomputationallimit. InAdvances
in Neural Information Processing Systems (NeurIPS), Du,S.S.,Hu,W.,andLee,J.D. Algorithmicregularization
2022. inlearningdeephomogeneousmodels: Layersareauto-
maticallybalanced. InAdvancesinNeuralInformation
Bartlett,P. Forvalidgeneralizationthesizeoftheweights ProcessingSystems(NeurIPS),2018.
ismoreimportantthanthesizeofthenetwork. Advances
inneuralinformationprocessingsystems,9,1996. Entezari,R.,Sedghi,H.,Saukh,O.,andNeyshabur,B. The
roleofpermutationinvarianceinlinearmodeconnectivity
Belkin,M.,Hsu,D.,Ma,S.,andMandal,S. Reconciling ofneuralnetworks. InProceedingsoftheInternational
modernmachine-learningpracticeandtheclassicalbias– ConferenceonLearningRepresentations(ICLR),2021.
variancetrade-off. ProceedingsoftheNationalAcademy
Ergen, T.andPilanci, M. Path regularization: A convex-
ofSciences,116(32):15849–15854,2019.
ityandsparsityinducingregularizationforparallelrelu
Bender, E. M., Gebru, T., McMillan-Major, A., and networks. InAdvancesinNeuralInformationProcessing
Shmitchell,S. Onthedangersofstochasticparrots: Can
Systems(NeurIPS),2023.
languagemodelsbetoobig? InProceedingsofthe2021
Feng,R.,Zheng,K.,Huang,Y.,Zhao,D.,Jordan,M.,and
ACMconferenceonfairness,accountability,andtrans-
Zha, Z.-J. Rank diminishing in deep neural networks.
parency,pp.610–623,2021.
InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2022.
Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley,
H., O’Brien, K., Hallahan, E., Khan, M. A., Purohit, Ferbach, D., Goujaud, B., Gidel, G., and Dieuleveut, A.
S., Prashanth, U. S., Raff, E., et al. Pythia: A suite Proving linear mode connectivity of neural networks
foranalyzinglargelanguagemodelsacrosstrainingand viaoptimaltransport. arXivpreprintarXiv:2310.19103,
scaling. InProceedingsoftheInternationalConference 2023.
onMachineLearning(ICML),2023.
Fort, S.,Dziugaite,G.K.,Paul,M., Kharaghani,S.,Roy,
Biewald,L. Experimenttrackingwithweightsandbiases, D. M., and Ganguli, S. Deep learning versus kernel
2020. URLhttps://www.wandb.com/. Software learning: an empirical study of loss landscape geome-
availablefromwandb.com. try and the time evolution of the neural tangent kernel.
15ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
InAdvancesinNeuralInformationProcessingSystems Hu,E.J.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,S.,Wang,
(NeurIPS),2020. L.,Chen,W.,etal. Lora: Low-rankadaptationoflarge
language models. In Proceedings of the International
Frankle, J. and Carbin, M. The lottery ticket hypothesis:
ConferenceonLearningRepresentations(ICLR),2021.
Findingsparse,trainableneuralnetworks.InProceedings
oftheInternationalConferenceonLearningRepresenta- Huh, M., Mobahi, H., Zhang, R., Cheung, B., Agrawal,
tions(ICLR),2018. P., and Isola, P. The low-rank simplicity bias in deep
networks. TransactionsonMachineLearningResearch,
Frankle,J.,Dziugaite,G.K.,Roy,D.,andCarbin,M.Linear 2022.
modeconnectivityandthelotterytickethypothesis. In
ProceedingsoftheInternationalConferenceonMachine Humayun, A. I., Balestriero, R., and Baraniuk, R. Deep
Learning(ICML),2020. networks always grok and here is why. arXiv preprint
arXiv:2402.15555,2024.
Frei, S., Vardi, G., Bartlett, P., Srebro, N., and Hu, W.
Implicit bias in leaky ReLU networks trained on high- Hunter,J.D. Matplotlib: A2Dgraphicsenvironment. Com-
dimensional data. In Proceedings of the International putinginScience&Engineering,9(3):90–95,2007.
ConferenceonLearningRepresentations(ICLR),2022.
Ilharco, G., Ribeiro, M. T., Wortsman, M., Schmidt, L.,
Hajishirzi,H.,andFarhadi,A. Editingmodelswithtask
Galanti,T.,Siegel,Z.S.,Gupte,A.,andPoggio,T.SGDand
arithmetic. InProceedingsoftheInternationalConfer-
weightdecayprovablyinducealow-rankbiasinneural
enceonLearningRepresentations(ICLR),2022.
networks. arXivpreprintarXiv:2206.05794,2022.
Ilyas,A.,Santurkar,S.,Tsipras,D.,Engstrom,L.,Tran,B.,
Gromov,A. Grokkingmodulararithmetic. arXivpreprint
andMadry,A. Adversarialexamplesarenotbugs,they
arXiv:2301.02679,2023.
arefeatures. Advancesinneuralinformationprocessing
Gunasekar, S., Woodworth, B. E., Bhojanapalli, S., systems,32,2019.
Neyshabur, B., and Srebro, N. Implicit regularization
Ioffe,S.andSzegedy,C. Batchnormalization: Accelerat-
inmatrixfactorization. InAdvancesinNeuralInforma-
ingdeepnetworktrainingbyreducinginternalcovariate
tionProcessingSystems(NeurIPS),2017.
shift. InProceedingsoftheInternationalConferenceon
Hanson, S. and Pratt, L. Comparing biases for minimal MachineLearning(ICML),2015.
networkconstructionwithback-propagation.InAdvances
Ito,A.,Yamada,M.,andKumagai,A. Analysisoflinear
in Neural Information Processing Systems (NeurIPS),
modeconnectivityviapermutation-basedweightmatch-
1988.
ing. arXivpreprintarXiv:2402.04051,2024.
Harris,C.R.,Millman,K.J.,vanderWalt,S.J.,Gommers,
Jacot,A.,Gabriel,F.,andHongler,C. Neuraltangentker-
R.,Virtanen,P.,Cournapeau,D.,Wieser,E.,Taylor,J.,
nel: Convergenceandgeneralizationinneuralnetworks.
Berg,S.,Smith,N.J.,Kern,R.,Picus,M.,Hoyer,S.,van
InAdvancesinNeuralInformationProcessingSystems
Kerkwijk,M.H.,Brett,M.,Haldane,A.,delR´ıo,J.F.,
(NeurIPS),2018.
Wiebe,M.,Peterson,P.,Ge´rard-Marchant,P.,Sheppard,
K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., Ji,Z.andTelgarsky,M.Gradientdescentalignsthelayersof
and Oliphant, T. E. Array programming with NumPy. deeplinearnetworks. InProceedingsoftheInternational
Nature,585(7825):357–362,September2020. ConferenceonLearningRepresentations(ICLR),2019.
Hendrycks,D.,Zhao,K.,Basart,S.,Steinhardt,J.,andSong, Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and
D. Naturaladversarialexamples. InProceedingsofthe Bengio,S. Fantasticgeneralizationmeasuresandwhere
IEEE/CVF conference on computer vision and pattern tofindthem. arXivpreprintarXiv:1912.02178,2019.
recognition,pp.15262–15271,2021.
Jordan, K., Sedghi, H., Saukh, O., Entezari, R., and
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionprob- Neyshabur,B. REPAIR:REnormalizingPermutedAc-
abilistic models. In Advances in Neural Information tivationsforInterpolationRepair. InProceedingsofthe
ProcessingSystems(NeurIPS),2020. InternationalConferenceonLearningRepresentations
(ICLR),2022.
Hochreiter, S.andSchmidhuber, J. Flatminima. Neural
Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila,
computation,9(1):1–42,1997a.
T., and Laine, S. Analyzing and improving the train-
Hochreiter,S.andSchmidhuber,J.Longshort-termmemory. ing dynamics of diffusion models. arXiv preprint
Neuralcomputation,9(8):1735–1780,1997b. arXiv:2312.02696,2023.
16ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
Krizhevsky,A. Learningmultiplelayersoffeaturesfrom Lyu, K., Jin, J., Li, Z., Du, S. S., Lee, J. D., and Hu, W.
tinyimages,2009. Dichotomy of early and late phase implicit biases can
provablyinducegrokking. InProceedingsoftheInterna-
Krogh, A. and Hertz, J. A simple weight decay can im-
tionalConferenceonLearningRepresentations(ICLR),
provegeneralization. InAdvancesinNeuralInformation
2023.
ProcessingSystems(NeurIPS),1991.
Martin, C. H. and Mahoney, M. W. Heavy-tailed univer-
Kuditipudi,R.,Wang,X.,Lee,H.,Zhang,Y.,Li,Z.,Hu,W.,
sality predicts trends in test accuracies for very large
Ge,R.,andArora,S. Explaininglandscapeconnectivity
pre-traineddeepneuralnetworks. InProceedingsofthe
oflow-costsolutionsformultilayernets. InAdvancesin
SIAMInternationalConferenceonDataMining(ICDM),
NeuralInformationProcessingSystems(NeurIPS),2019.
2020.
Kumar, T., Bordelon, B., Gershman, S. J., and Pehlevan,
Martin, C. H. and Mahoney, M. W. Implicit self-
C. Grokkingasthetransitionfromlazytorichtraining
regularizationindeepneuralnetworks: Evidencefrom
dynamics. arXivpreprintarXiv:2310.06110,2023.
randommatrixtheoryandimplicationsforlearning. Jour-
Le,T.andJegelka,S. Traininginvariancesandthelow-rank nalofMachineLearningResearch,22(165):1–73,2021.
phenomenon: beyondlinearnetworks. InProceedings
oftheInternationalConferenceonLearningRepresenta- Merity,S.,Xiong,C.,Bradbury,J.,andSocher,R. Pointer
tions(ICLR),2021. sentinelmixturemodels. InProceedingsoftheInterna-
tionalConferenceonLearningRepresentations(ICLR),
LeCun, Y. The MNIST database of handwritten digits. 2016.
http://yann.lecun.com/exdb/mnist/,1998.
Merrill, W., Tsilivis, N., and Shukla, A. A tale of two
Lhoest, Q., delMoral, A.V., Jernite, Y., Thakur, A., von
circuits: Grokking as competition of sparse and dense
Platen, P., Patil, S., Chaumond, J., Drame, M., Plu, J.,
subnetworks. In ICLR 2023 Workshop on Mathemati-
Tunstall,L.,Davison,J.,Sˇasˇko,M.,Chhablani,G.,Malik,
calandEmpiricalUnderstandingofFoundationModels,
B.,Brandeis,S.,Scao,T.L.,Sanh,V.,Xu,C.,Patry,N.,
2023.
McMillan-Major,A.,Schmid,P.,Gugger,S.,Delangue,
C., Matussie`re, T., Debut, L., Bekman, S., Cistac, P., Milanesi,P.,Kadri,H.,Ayache,S.,andArtie`res,T. Implicit
Goehringer, T., Mustar, V., Lagunas, F., Rush, A. M., regularizationindeeptensorfactorization.InProceedings
andWolf,T. Datasets: Acommunitylibraryfornatural oftheInternationalJointConferenceonNeuralNetworks
languageprocessing. arXivpreprintarXiv:2109.02846, (IJCNN),2021.
2021.
Mohamadi, M. A., Li, Z., Wu, L., and Sutherland, D.
Li,M.,Gururangan,S.,Dettmers,T.,Lewis,M.,Althoff,T.,
Grokkingmodulararithmeticcanbeexplainedbymargin
Smith,N.A.,andZettlemoyer,L. Branch-train-merge:
maximization. InNeurIPS2023WorkshoponMathemat-
Embarrassinglyparalleltrainingofexpertlanguagemod-
icsofModernMachineLearning,2023.
els. arXivpreprintarXiv:2208.03306,2022.
Mohan, S., Kadkhodaie, Z., Simoncelli, E. P., and
Li,Z.,Luo,Y.,andLyu,K. Towardsresolvingtheimplicit
Fernandez-Granda, C. Robust and interpretable blind
biasofgradientdescentformatrixfactorization: Greedy
imagedenoisingviabias-freeconvolutionalneuralnet-
low-ranklearning. InProceedingsoftheInternational
works. arXivpreprintarXiv:1906.05478,2019.
ConferenceonLearningRepresentations(ICLR),2020.
Mulayoff,R.andMichaeli,T. Uniquepropertiesofflatmin-
Liu,Z.,Kitouni,O.,Nolte,N.S.,Michaud,E.,Tegmark,M.,
imaindeepnetworks.InProceedingsoftheInternational
andWilliams,M. Towardsunderstandinggrokking: An
ConferenceonMachineLearning(ICML),2020.
effectivetheoryofrepresentationlearning.InAdvancesin
NeuralInformationProcessingSystems(NeurIPS),2022.
Nagarajan,V.andKolter,J.Z. Uniformconvergencemay
Liu, Z., Michaud, E. J., and Tegmark, M. Omnigrok: be unable to explain generalization in deep learning.
Grokking beyond algorithmic data. In Proceedings of InAdvancesinNeuralInformationProcessingSystems
the International Conference on Learning Representa- (NeurIPS),2019.
tions(ICLR),2023.
Nakkiran,P.,Kaplun,G.,Bansal,Y.,Yang,T.,Barak,B.,
Loshchilov, I. and Hutter, F. Fixing weight decay regu- and Sutskever, I. Deep double descent: Where bigger
larization in Adam. arXiv preprint arXiv:1711.05101, models and more data hurt. Journal of Statistical Me-
2017. chanics: TheoryandExperiment,2021(12),2021.
17ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
Nanda, N., Chan, L., Lieberum, T., Smith, J., and Stein- Qu,X.andHorvath,S. Rethinkmodelre-basinandthelin-
hardt,J. Progressmeasuresforgrokkingviamechanistic earmodeconnectivity. arXivpreprintarXiv:2402.05966,
interpretability. arXivpreprintarXiv:2301.05217,2023. 2024.
Neyshabur,B.,Tomioka,R.,andSrebro,N. Insearchofthe Rahaman,N.,Baratin,A.,Arpit,D.,Draxler,F.,Lin,M.,
realinductivebias: Ontheroleofimplicitregularization Hamprecht, F., Bengio, Y., and Courville, A. On the
indeeplearning. arXivpreprintarXiv:1412.6614,2014. spectralbiasofneuralnetworks. InProceedingsofthe
InternationalConferenceonMachineLearning(ICML),
Neyshabur, B., Sedghi, H., andZhang, C. Whatisbeing 2019.
transferredintransferlearning? InAdvancesinNeural
InformationProcessingSystems(NeurIPS),2020. Ramachandran,P.,Zoph,B.,andLe,Q.V. Searchingfor
activationfunctions. arXivpreprintarXiv:1710.05941,
Notsawo Jr, P., Zhou, H., Pezeshki, M., Rish, I., Dumas, 2017.
G.,etal. Predictinggrokkinglongbeforeithappens: A
lookintothelosslandscapeofmodelswhichgrok. arXiv Rame,A.,Kirchmeyer,M.,Rahier,T.,Rakotomamonjy,A.,
preprintarXiv:2306.13253,2023. Gallinari,P.,andCord,M. Diverseweightaveragingfor
out-of-distributiongeneralization. InAdvancesinNeural
Ongie, G. and Willett, R. The role of linear layers InformationProcessingSystems(NeurIPS),2022.
in nonlinear interpolating networks. arXiv preprint
arXiv:2202.00856,2022. Ramesh,A.,Dhariwal,P.,Nichol,A.,Chu,C.,andChen,
M. Hierarchicaltext-conditionalimagegenerationwith
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. CLIPlatents. arXivpreprintarXiv:2204.06125,2022.
Librispeech: anasrcorpusbasedonpublicdomainau-
dio books. In Proceedings of the IEEE International Razin, N. and Cohen, N. Implicit regularization in deep
ConferenceonAcoustics,SpeechandSignalProcessing learningmaynotbeexplainablebynorms.InAdvancesin
(ICASSP),2015. NeuralInformationProcessingSystems(NeurIPS),2020.
Parhi, R. and Nowak, R. D. Deep learning meets sparse Ronneberger,O.,Fischer,P.,andBrox,T. U-net: Convolu-
regularization: A signal processing perspective. IEEE tionalnetworksforbiomedicalimagesegmentation. In
SignalProcessingMagazine,40(6):63–74,2023. ProceedingoftheInternationalConferenceonMedical
ImageComputingandComputer-AssistedIntervention
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., (MICCAI),2015.
Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga,
L.,etal. Pytorch: Animperativestyle,high-performance Roy,O.andVetterli,M. Theeffectiverank: Ameasureof
deeplearninglibrary. InAdvancesinNeuralInformation effectivedimensionality. InProceedingsoftheEuropean
ProcessingSystems(NeurIPS),2019. SignalProcessingConference(EUSIPCO),2007.
Paul,M.,Chen,F.,Larsen,B.W.,Frankle,J.,Ganguli,S., Sadrtdinov,I.,Pozdeev,D.,Vetrov,D.P.,andLobacheva,E.
and Dziugaite, G. K. Unmasking the lottery ticket hy- Tostayornottostayinthepre-trainbasin: Insightson
pothesis: What’sencodedinawinningticket’smask? In ensemblingintransferlearning. InAdvancesinNeural
ProceedingsoftheInternationalConferenceonLearning InformationProcessingSystems(NeurIPS),2023.
Representations(ICLR),2022.
Saxe,A.,McClelland,J.,andGanguli,S. Exactsolutionsto
Pennington,J.,Schoenholz,S.,andGanguli,S. Theemer- thenonlineardynamicsoflearningindeeplinearneural
genceofspectraluniversalityindeepnetworks. InPro- networks.InProceedingsoftheInternationalConference
ceedings of the International Conference on Artificial onLearningRepresentations(ICLR),2014.
IntelligenceandStatistics(AISTATS),2018.
Schottho¨fer,S.,Zangrando,E.,Kusch,J.,Ceruti,G.,and
Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Tudisco, F. Low-rank lottery tickets: Finding efficient
Misra, V. Grokking: Generalization beyond overfit- low-rank neural networks via matrix differential equa-
ting on small algorithmic datasets. arXiv preprint tions. In Advances in Neural Information Processing
arXiv:2201.02177,2022. Systems(NeurIPS),volume35,2022.
Praggastis, B., Brown, D., Marrero, C. O., Purvine, E., Shenouda,J.,Parhi,R.,Lee,K.,andNowak,R.D. Vector-
Shapiro, M., and Wang, B. The SVD of convolu- valuedvariationspacesandwidthboundsforDNNs: In-
tionalweights: aCNNinterpretabilityframework. arXiv sights on weight decay regularization. arXiv preprint
preprintarXiv:2208.06894,2022. arXiv:2305.16534,2023.
18ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
Simonyan, K. and Zisserman, A. Very deep convolu- Waskom,M.L. seaborn: statisticaldatavisualization. Jour-
tionalnetworksforlarge-scaleimagerecognition. arXiv nalofOpenSourceSoftware,6(60):3021,2021.
preprintarXiv:1409.1556,2014.
Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R.,
Simsek,B.,Ged,F.,Jacot,A.,Spadaro,F.,Hongler,C.,Ger- Gontijo-Lopes, R., Morcos, A. S., Namkoong, H.,
stner,W.,andBrea,J. Geometryofthelosslandscapein Farhadi,A.,Carmon,Y.,Kornblith,S.,andSchmidt,L.
overparameterizedneuralnetworks: Symmetriesandin- Modelsoups: averagingweightsofmultiplefine-tuned
variances.InProceedingsoftheInternationalConference modelsimprovesaccuracywithoutincreasinginference
onMachineLearning(ICML),2021. time. InProceedingsoftheInternationalConferenceon
MachineLearning(ICML),2022.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
Ganguli,S. Deepunsupervisedlearningusingnonequi-
Xu,Z.,Wang,Y.,Frei,S.,Vardi,G.,andHu,W. Benign
libriumthermodynamics. InProceedingsoftheInterna-
overfittingandgrokkinginReLUnetworksforXORclus-
tionalConferenceonMachineLearning(ICML),2015.
terdata. InProceedingsoftheInternationalConference
Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan, onLearningRepresentations(ICLR),2023.
D.,Goodfellow,I.,andFergus,R.Intriguingpropertiesof
Yaras,C.,Wang,P.,Hu,W.,Zhu,Z.,Balzano,L.,andQu,Q.
neuralnetworks. arXivpreprintarXiv:1312.6199,2013.
Invariantlow-dimensionalsubspacesingradientdescent
Tan,Z.andHuang,W. Understandinggrokkingthrougha forlearningdeepmatrixfactorizations. InNeurIPS2023
robustnessviewpoint. arXivpreprintarXiv:2311.06597, WorkshoponMathematicsofModernMachineLearning,
2023. 2023a.
Thilak, V., Littwin, E., Zhai, S., Saremi, O., Paiss, R.,
Yaras,C.,Wang,P.,Hu,W.,Zhu,Z.,Balzano,L.,andQu,
and Susskind, J. The slingshot mechanism: An em-
Q. Thelawofparsimonyingradientdescentforlearning
pirical study of adaptive optimizers and the grokking
deeplinearnetworks. arXivpreprintarXiv:2306.01154,
phenomenon. arXivpreprintarXiv:2206.04817,2022.
2023b.
Timor,N.,Vardi,G.,andShamir,O. Implicitregularization
Yu, H. and Wu, J. Compressing transformers: Features
towards rank minimization in relu networks. In Pro-
are low-rank, but weights are not! In Proceedings of
ceedingsoftheInternationalConferenceonAlgorithmic
theNationalConferenceonArtificialIntelligence(AAAI),
LearningTheory(ALT),2023.
2023.
VanLaarhoven,T.L2regularizationversusbatchandweight
Yu, X., Liu, T., Wang, X., and Tao, D. On compressing
normalization. arXivpreprintarXiv:1706.05350,2017.
deepmodelsbylowrankandsparsedecomposition. In
Vardi, G. and Shamir, O. Implicit regularization in relu ProceedingsoftheIEEE/CVFConferenceonComputer
networks with the square loss. In Proceedings of the VisionandPatternRecognition(CVPR),2017.
AnnualConferenceonLearningTheory(COLT),2021.
Yunis,D.,Patel,K.K.,Savarese,P.H.P.,Vardi,G.,Frankle,
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
J.,Walter,M.,Livescu,K.,andMaire,M. Onconvexity
L.,Gomez,A.N.,Kaiser,Ł.,andPolosukhin,I. Atten-
andlinearmodeconnectivityinneuralnetworks. InOPT
tionisallyouneed. InAdvancesinNeuralInformation
2022:OptimizationforMachineLearning(NeurIPS2022
ProcessingSystems(NeurIPS),2017.
Workshop),2022.
Wang, B. and Vastola, J. ML from scratch: Sta-
Zangrando,E.,Deidda,P.,Brugiapaglia,S.,Guglielmi,N.,
ble diffusion, day 2, 2022. URL https:
andTudisco,F. Neuralrankcollapse: Weightdecayand
//colab.research.google.com/drive/
smallwithin-classvariabilityyieldlow-rankbias. arXiv
1Y5wr91g5jmpCDiX-RLfWL1eSBWoSuLqO?
preprintarXiv:2402.03991,2024.
usp=sharing#scrollTo=9is-DXZYwIIi.
Wang,H.,Agarwal,S.,andPapailiopoulos,D. Pufferfish: Zhang,C.,Bengio,S.,Hardt,M.,Recht,B.,andVinyals,O.
Communication-efficientmodelsatnoextracost. Pro- Understanding deep learning (still) requires rethinking
ceedingsofMachineLearningandSystems,3:365–386, generalization. CommunicationsoftheACM,64(3):107–
2021. 115,2021.
Wang, Z. and Jacot, A. Implicit bias of SGD in l {2}- Zhang,G.,Wang,C.,Xu,B.,andGrosse,R. Threemech-
regularizedlinearDNNs: One-wayjumpsfromhighto anisms of weight decay regularization. arXiv preprint
lowrank. arXivpreprintarXiv:2305.16038,2023. arXiv:1810.12281,2018a.
19ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
Zhang,H.,Dauphin,Y.N.,andMa,T. Fixupinitialization: linearnetwork. InAdvancesinNeuralInformationPro-
Residuallearningwithoutnormalization. InProceedings cessingSystems(NeurIPS),2022.
oftheInternationalConferenceonLearningRepresenta-
tions(ICLR),2018b. Zou,A.,Wang,Z.,Kolter,J.Z.,andFredrikson,M. Uni-
versalandtransferableadversarialattacksonalignedlan-
Ziyin,L.,Li,B.,andMeng,X. Exactsolutionsofadeep guagemodels. arXivpreprintarXiv:2307.15043,2023.
20ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
A.ExperimentalDetails
Forallexperiments,weuse3randomseedsandaverageallplotsoverthose3. Thisisrelativelysmall,buterrorbarstendto
beverytight,andduetothehighvolumeofrunsrequiredforthisworkwelacktheresourcestorunmuchmore.
Inordertocomputealignmentweconsideronlypairsoflayersthatdirectlyfeedintoeachother,andignoretheinfluenceof
residualconnectionssoastocutdownonthenumberofcomparisons. Specificsonindividualarchitecturesaregivenbelow.
A.1.ImageClassificationwithVGG
WetrainaVGG-16(Simonyan&Zisserman,2014)onCIFAR-10(Krizhevsky,2009)for164epochs,followinghyperpa-
rametersandlearningrateschedulein(Frankleetal.,2020),butwithoutdataaugmentation. FortheoptimizerweuseSGD
withbatchsize128,initiallearningrate0.1andmomentumof0.9. Wealsodecaythelearningrate3timesbyafactorof10
atepoch82,epoch120,andfinallyatepoch160. Wealsouseaminoramountofweightdecaywithcoefficient0.0001.
VGG-16usesReLUactivationsandbatchnormalization(Ioffe&Szegedy,2015),andincludesbothconvolutionaland
linearlayers. ForlinearlayerswesimplycomputetheSVDoftheweightmatrix. Forconvolutionallayers,theparameters
aretypicallystoredasa4Dtensorofshape(c ,c ,h,w)fortheoutputchannels,inputchannels,heightandwidthofthe
out in
filtersrespectively. Asthefilterscomputeatransformationfromeachpositionandinputchanneltoanoutputchannel,we
computetheSVDoftheflattenedtensor(c ,c ·h·w),whichmapsallinputstooutputs,similartoPraggastisetal.(2022).
out in
ThisisnottheSVDoftheentiretransformationofthefeaturemaptothenextfeaturemap,butratherthetransformation
fromasetofadjacentpositionstoaparticularpositioninthenextlayer. FortheindividualSVevolutionplot,weusethe
12thconvolutionallayer.
Inordertocomputealignmentofbasesbetweenconsecutiveconvolutionallayers,V⊤ U weneedtomatchthedimen-
i+1 i
sionalitybetweenU andV . Forconvolutionallayerswearepresentedwithaquestionastohowtohandlethespatial
i i+1
dimensions h and w as naively the input dimension of the next layer will be a factor of h·w larger dimension. We
experimentedwithmultiplecases,includingaligningateachspatialpositionindividuallyoraveragingoverthealignmentat
allspatialpositions,andeventuallysettledataligningtheoutputofonelayertothecenterspatialinputofthenextlayer.
Thatis,fora3x3convolutionmappingtoafollowing3x3convolution,wecomputethealignmentonlyforposition(1,1)of
thenextlayer. Thisseemedreasonabletousasonaveragetheedgesofthefiltersshowedpooreralignmentoverall. Forthe
individualalignmentplot,weusethealignmentbetweenthe11thand12thconvolutionallayersatthecenterspatialposition
ofthe12thconvolutionallayer.
A.2.ImageGenerationwithUNets
WetrainaUNet(Ronnebergeretal.,2015)diffusionmodel(Sohl-Dicksteinetal.,2015;Hoetal.,2020)onMNIST(LeCun,
1998)generation. Wetakemodeldesignandhyperparametersfrom(Wang&Vastola,2022). Inparticularweusea4-layer
residualUNetandtrainwithAdamW(Loshchilov&Hutter,2017)withbatchsize128,andlearningrateof0.0003for100
epochs. Thismodelusesswish(Ramachandranetal.,2017)activationsandacombinationoflinearandconvolutional,as
wellastransposedconvolutionallayers.
Computing SVDs and alignment is similar to the image classification case described above, except in the case of the
transposed convolutions where an extra transpose of dimensions is needed as parameters are stored with the shape
(c ,c ,h,w). FortheindividualSVevolutionplot,weusethe3rdconvolutionallayer. Forthealignmentplot,weusethe
in out
alignmentbetweenthe3rdand4thconvolutionallayersatthecenterspatialpositionofthe4thconvolutionallayer.
A.3.SpeechRecognitionwithLSTMs
WetrainabidirectionalLSTM(Hochreiter&Schmidhuber,1997a)forautomaticspeechrecognitiononLibriSpeech(Panay-
otovetal.,2015). Wetuneforasimpleandwell-performinghyperparametersetting. WeuseAdamW(Loshchilov&Hutter,
2017)withbatchsize32,learningrate0.0003andweightdecay0.1for50epochs. Wealsouseacosineannealinglearning
rateschedulefrom1to0overtheentire50epochs.
TheLSTMonlyhasmatrixparametersandbiases,soitisstraightforwardtocomputeSVDsofthematrices. Forindividual
SVevolutionplots,weplotthe3rdlayerinputparameter. Inthecaseofalignment,wemakeanumberofconnections: first
downdepthfortheinputparameters,thenconnectingthepreviousinputparametertothecurrenthiddenparameterinboth
directions,thenconnectingtheprevioushiddenparametertothecurrentinputparameter. InparticulartheLSTMparameters
21ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
arestoredasastackof4matricesinPyTorch,andwefindalignmentishighestforthe”gate”submatrix,sowechoosethat
forallplots. Fortheindividuallayeralignment,weplotalignmentbetweenthe3rdand4thlayerinputparameters.
A.4.LanguageModelingwithTransformers
WetrainaTransformer(Vaswanietal.,2017)languagemodelonWikitext-103(Merityetal.,2016).Webasehyperparameter
choicesonthePythiasuite(Bidermanetal.,2023),specificallythe160millionparameterconfigurationwithsinusoidal
positionembeddings,12layers,modeldimension768,12attentionheadsperlayer,andhiddendimension768. Weuse
AdamW(Loshchilov&Hutter,2017)withbatchsize256,learningrate0.0006andweightdecay0.1. Weuseacontext
lengthof2048andclipgradientstoamaximumnormof1. Wealsousealearningrateschedulewithalinearwarmupand
cosinedecayto10%ofthelearningrate,likeBidermanetal.(2023).
ForSVDs,forsimplicitywetaketheSVDoftheentire(3d ,d )parameterthatcomputesqueries,keysandvalues
model model
from the hidden dimension inside the attention layer, without splitting into individual heads. This is reasonable as the
splittingisdoneafterthefactinternally. WealsotaketheSVDoftheoutputparameters,andlinearlayersoftheMLPs,
whichare2dimensionalmatrices. FortheindividualSVevolutionplot,weplottheSVsofW ofthe8thlayerMLP
1
Foralignment,weconsiderthealignmentofW andW matrices,W andW matrices,computingalignmentbetween
Q K V O
headsindividuallythenaveragingoverallheads. WealsoconsiderthealignmentbetweenW andW oftheMLPblock,
O 1
betweenW andW oftheMLPblock,andbetweenW andthenextattentionlayer. Fortheindividuallayeralignment,we
1 2 2
plotalignmentbetweenW andW ofthe8thlayerMLP.
1 2
A.5.WeightDecayExperiments
All tasks are trained in exactly the same fashion as mentioned previously, with increasing weight decay in the set
{0,0.0001,0.001,0.01,0.1,1.0,10.0}. For ease of presentation we consider a subset of settings across tasks. In Fig-
ure14weincludetrainedmodelperformanceandprunedmodelperformancetoshowthat,evenwithhighlevelsofweight
decay,modelsdonotentirelybreakdown. Moreso,theapproximationoftheprunedmodeltothefullmodelgetsbetter
withhigherweightdecay.
A.6.GrokkingExperiments
WemostlyfollowthesettingsandarchitectureofNandaetal.(2023),exceptweusesinusoidalpositionalencodingsinstead
oflearned.
For the slingshot case we follow hyperparameter settings in Thilak et al. (2022), Appendix B except with the 1-layer
architecturefromNandaetal.(2023)insteadofthe2-layerarchitecturespecified. Wperformadditionmodulo97. The
originalgrokkingplotinThilaketal.(2022)appearsmuchmoredramaticasitlog-scalesthex-axis,whichwedonotdo
hereforclarity.
A.7.RandomLabelExperiments
Wetraina4-layerMLPonCIFAR10(Krizhevsky,2009)witheithercompletelyrandomlabels,orthetruelabels. Weuse
SGDwithmomentumof0.9andconstantlearningrateof0.001,andtrainfor300epochstoseetheentiretrendoftraining.
ThemajordifferencetothesettingofZhangetal.(2021)istheuseofaconstantlearningrate,astheiruseofalearningrate
schedulemightconflatetheresults.
A.8.MagnitudePruningExperiments
WeusethesameVGGsetupasdescribedpreviously. Inthiscasewetraintiltheend,thencomputeaglobalmagnitude
mask. Todothisweflattenalllinearandconvolutionalweightsintoasinglevector,exceptforthelastlinearlayer,and
sortbymagnitude. Thenwekeepthetop5%ofweightsglobally,andreshapebacktothelayerwisemasks. Thisresults
indifferentsparsitylevelsfordifferentlayers,sowhengeneratingtherandommasks,weusetheper-layersparsitiesthat
resultedfromtheglobalmagnitudemask.
Toretrainthenetwork,werewindtoepoch4,thencontinuetrainingwiththemask,alwayssettingotherweightsandtheir
gradientsto0. Weaverageallresultsover3randomseeds.
22ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
(a)VGG (b)UNet (c)LSTM (d)Transformer
Figure14: Traininglossovertime,wheretherowsusedifferingamountsofweightdecay. Fromtoptobottom,forVGG
weusecoefficients{0,0.001,0.01,0.1},whileforothernetworksweusecoefficients{0,0.1,1,10}. Weseethatitisstill
possibletoachievelowtraininglossunderhighweightdecay,andasweincreasetheamountofweightdecay,thegap
betweenprunedandunprunedparameterscloses,lendingsupporttotheideathattheparametersbecomelowerrank.
A.9.LMCExperiments
Wesave5evenly-spacedcheckpointsinthefirstepoch,aswellasattheendofthenext4epochsfor10intializationsin
total. Wetrain3trunks,andsplit3branchesfromeachtrunkforatotalof9brancheswhichweaverageallplotsover.
FollowingNeyshaburetal.(2020),wecomputethebarrierbetweencheckpointsasfollows: givenW(1)(T)andW(2)(T)
23ApproachingDeepLearningthroughtheSpectralDynamicsofWeights
thatwerebranchedfromW(t)wecompute
b(t)=( max L((1−α)W(1)(T)+αW(2)(T))−((1−α)L(W(1)(T))+αL(W(2)(T))) (6)
α∈[0,1]
whenthisquantityis0,weconsiderthecheckpointstoexhibitLMC.
WerecomputebatchnormalizationparametersafterinterpolatingforVGG-16,andgroupnormalizationparametersforthe
UNet,asthesedonotnecessarilyinterpolatewell(Frankleetal.,2020). Wealsocomputesingularvectoragreementforthe
sameparameterbetweeneitherbranchendpoint.
Toplotthesingularvector(dis)agreementandLMCbetweendifferentmodes,wemake11evenlyspacedmeasurements
interpolating between branch endpoints that had the same split epoch, and the same branch seed, but different trunk
initializations.
A.10.PerturbedLMCExperiments
WeperturballweightsW afterthepointofdynamicsstabilitywhereweexpecttoseeLMCattheendoftraining(epoch
4issufficientlylateinallcases)usingrandomlysamplednormalperturbationsϵ ∼ N(0,I)with∥ϵ∥ = η∥W∥where
η ∈{0.0,0.1,0.25,0.5,1.0,2.5}.Wedonotperturbtheoutputlayer,asthishasaverysubstantialeffectontheoptimization.
WealsodonotperturbtheinputlayerfortheTransformerasitistoocomputationallyexpensiveforourresources.
B.Limitations
Thereareafewkeylimitationstoourstudy. Asmentioned,welackthecomputationalresourcestorunmorethan3random
seedsperexperiment,thoughwedofinderrorbarstobequitetightingeneral(exceptforthegeneralizationepochinthe
grokkingexperiments). Inaddition,asdiscussedweignore1Dparametersintheneuralnetworks,whichmaybeparticularly
crucial(especiallynormalization). Inaddition,duetocomputationalconstraintswedonotconsideralignmentoflayers
acrossresidualconnectionsasthisquicklybecomescombinatorialindepth,thustheremaybeotherinterestinginteractions
thatwedonotobserve. Finally,duetocomputationalconstraintsweareunabletoinvestigateresultsonlargermodelsthan
the12layerTransformer,whichmayhavedifferentbehavior.
C.ComputeResources
All experiments are performed on an internal cluster with on the order of 100 NVIDIA 2080ti GPUs or newer. All
experimentsrunonasingleGPUinlessthan8hours,thoughitisextremelyhelpfultoparallelizeacrossmachines. We
estimatethatend-to-enditmighttakeafewdaysontheseresourcestorerunalloftheexperimentsinthispaper. Additionally,
thestoragerequirementsforallofthecheckpointswilltakeontheorderof5terabytes.
D.CodeSources
WeusePyTorch(Paszkeetal.,2019)andNumPy(Harrisetal.,2020)forallexperimentsandWeights&Biases(Biewald,
2020)forexperimenttracking. WemakeplotswithMatplotlib(Hunter,2007)andSeaborn(Waskom,2021). Wealsouse
HuggingFaceDatasets(Lhoestetal.,2021)forWikitext-103(Merityetal.,2016).
24