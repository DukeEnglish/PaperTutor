ProceedingsoftheASME2024
InternationalMechanicalEngineeringCongressandExposition
IMECE2024
November17–21,2024,Portland,OR
IMECE2024-145673
DEEPREINFORCEMENTLEARNINGFORDECENTRALIZEDMULTI-ROBOTCONTROL:ADQNAPPROACHTO
ROBUSTNESSANDINFORMATIONINTEGRATION
BinWu,C.SteveSuh∗
TexasA&MUniversity,CollegeStation,TX
ABSTRACT Thesescenariosoftenrequirerobotstocollaboratewithoutacen-
The superiority of Multi-Robot Systems (MRS) in various tralcontrolunit,whichplaceshigherdemandsontheefficiency
complex environments is unquestionable. However, in complex androbustnessofdecentralizedcontrolmechanisms. Traditional
situationssuchassearchandrescue,environmentalmonitoring, robot control systems often rely on centralized control strate-
andautomatedproduction,robotsareoftenrequiredtoworkcol- gies, but in practical applications, this approach often proves
laborativelywithoutacentralcontrolunit. Thisnecessitatesan inadequateinthefaceofenvironmentalcomplexityanddynamic
efficientandrobustdecentralizedcontrolmechanismtoprocess changes[4]. Therefore,developingadecentralizedcontrollerthat
local information and guide the robots’ behavior. In this work, can effectively integrate local information and improve system
we propose a new decentralized controller design method that adaptabilityandfaulttoleranceisparticularlyimportant[1,5].
utilizes the Deep Q-Network (DQN) algorithm from deep rein- Toaddressthesechallenges,deepreinforcementlearningof-
forcement learning, aimed at improving the integration of local fers a new solution [6–8]. This paper proposes a decentralized
informationandrobustnessofmulti-robotsystems. Thedesigned controllerdesignmethodbasedontheDeepQ-Network(DQN)
controller allows each robot to make decisions independently [9, 10], aimed at enhancing the efficiency and robustness of in-
basedonitslocalobservationswhileenhancingtheoverallsys- formationintegrationinmulti-robotsystems. Ourmethodallows
tem’s collaborative efficiency and adaptability to dynamic envi- eachrobottomakedecisionsindependentlybasedonitsobserved
ronmentsthroughasharedlearningmechanism. Throughtesting localstatewhileenhancingtheentiresystem’scollaborativeeffi-
in simulated environments, we have demonstrated the effective- ciencyandadaptabilitytodynamicenvironmentthroughashared
ness of this controller in improving task execution efficiency, learningmechanism. Testsinasimulatedenvironmenthaveval-
strengtheningsystemfaulttolerance,andenhancingadaptability idatedtheeffectivenessofthiscontrollerinimprovingtaskexe-
totheenvironment. Furthermore,weexploredtheimpactofDQN cutionefficiency,strengtheningsystemfaulttolerance,andenvi-
parametertuningonsystemperformance,providinginsightsfor ronmentaladaptability. Additionally,thisstudyalsoexploresthe
further optimization of the controller design. Our research not impact of DQN parameter adjustments on system performance,
only showcases the potential application of the DQN algorithm providinginsightsforfurtheroptimizationofcontrollerdesign.
inthedecentralizedcontrolofmulti-robotsystemsbutalsooffers Thisresearchnotonlydemonstratesthepotentialapplication
a new perspective on how to enhance the overall performance oftheDQNalgorithminthecontrolofdecentralizedmulti-robot
and robustness of the system through the integration of local systems but also offers a new perspective on how to enhance
information. overall system performance and robustness by integrating local
information. ByexperimentallycomparingourCommunication-
Keywords:DecentralizedController,Multi-Robot,Reinforce-
Embedded DQN (CE-DQN) algorithm with the standard DQN
mentLearning
algorithmacrossdifferentrobotteamcompositionsandtasksizes,
we further validate the effectiveness and practicality of our ap-
1. INTRODUCTION
proach.
Inthecurrentfieldofroboticsresearch,thecoordinatedcon-
trolofMulti-RobotSystems(MRS)[1]hasbecomeanimportant
2. METHODS
direction,especiallyincomplexscenariossuchassearchandres-
2.1 Problemformulation
cue[2],environmentalmonitoring[3],andautomatedproduction.
Indecentralizedmulti-robotsystems,consideringthedecen-
∗Correspondingauthor:ssuh@tamu.edu,wubin@tamu.edu tralized nature of the system, we can use distributed MDP or
Documentationforasmeconf.cls:Version1.37,August22,2024. Decentralized Partially Observable Markov Decision Processes
1 Copyright©2024byASME
4202
guA
12
]OR.sc[
1v93311.8042:viXra(Dec-POMDP)[11,12]todescribetheproblem. Here,weadopt incompleteness of local information makes it more difficult to
the definition framework of Multi-agent Markov Decision Pro- predictandevaluateglobalconsequences. Theoptimaldecisions
cesses(MAMDP)[13,14]. ofrobotsmaydependontheunknownactionsandstatesofother
There are 𝑁 robots, with the set of robots denoted as R = robots,whichcomplicatesthedesignofstrategiesbasedonlocal
{1,2,...,𝑁}. The system operates on discrete time steps 𝑡 = information for making independent decisions. In the absence
0,1,2,.... 𝑠 𝑡 ∈ Srepresents the global state of the system at ofcentralcoordination, ensuringthatlocaldecisionseffectively
time 𝑡, which may include all robots’ positions, environmental contributetotheglobalobjectiveisacriticalissue.
status, etc. 𝑠𝑖 represents the local state of the𝑖-th robot at time Toaddressthechallengesofcontrollingdecentralizedmulti-
𝑡
𝑡. The local state can be the robot’s local sensory information, robotsystems,weproposeasolutionbasedondeepQ-networks
such as the sensed nearby environment. 𝑎𝑖 ∈ A𝑖 represents the (DQN)enhancedwithanembeddedcommunicationstrategynet-
𝑡
actiontakenbythe𝑖-throbotattime𝑡. a𝑡 = (𝑎1 𝑡,𝑎2 𝑡,...,𝑎 𝑡𝑁) ∈ A worktofacilitateinformationexchangebetweenrobots. Thisde-
represents the combined actions of all robots at time 𝑡. Global sign aims to optimize decision-making quality in decentralized
Transition Function: 𝑃(𝑠 𝑡+1|𝑠 𝑡,a𝑡) indicates the probability of multi-robot systems, improve overall task execution efficiency,
the system transitioning to state 𝑠 𝑡+1 under the global state 𝑠 𝑡 and enhance system robustness through effective information
and joint action a𝑡. 𝑟 𝑡𝑖(𝑠 𝑡,𝑎𝑖 𝑡) represents the immediate reward sharing. Below, we provide a detailed explanation of our al-
obtained by the 𝑖-th robot based on its local state and action. gorithm.
𝑅(𝑠 𝑡,a𝑡) representstheimmediaterewardobtainedbytheentire Step1: Initialization.
ts hy este stm rau ten gd yer ot fhe ths eta 𝑖t -e th𝑠 𝑡 ra on bd otjo fi on rt ca hc oti oo sn ina g𝑡. a𝜋 c𝑖 t( io𝑎 n𝑖 𝑡 s|𝑠 𝑡𝑖 b) ar se ep dre os nen it ts
s
• NetworkInitialization. Foreachrobot𝑖,initializethreeneu-
flo oc ra al llst ra ot be o. tΠ sb(a as𝑡 e|𝑠 d𝑡) onre tp hr ees ge ln ot bs at lh se tas tt er .ategyforchoosingactions r va al lun ee st .w To ark rgs e: tB Ne eh ta wv oio rkrN 𝑄ˆe 𝑖t :w Ao ir dk s𝑄 in𝑖: sU tas be ild izt io ngpr te hd eic let aa rc nti io nn
g
processandassistsinupdatingthebehaviornetwork. Com-
In multi-robot systems, the optimization objective is usu-
municationStrategyNetwork𝐶 𝑖: Determineswhentosend
ally to maximize the system’s expected cumulative reward. To
orreceiveinformation.
achievethis, multi-robotsystemsneedtofindajointstrategy Π
thatmaximizestheexpectedcumulativerewardstartingfromthe • Experience Replay Buffer: Initialize an experience replay
initialstate𝑠 0. Thiscanbeexpressedas: buffer𝐷
𝑖
foreachrobot𝑖,usedtostoreexperiencesamples
includingstatetransitionsandrewardsreceived.
[︄ ∞ ]︄
𝑉Π(𝑠 0) =E ∑︂ 𝛾𝑡𝑅(𝑠 𝑡,a𝑡) | 𝑠 0,Π (1) Step2: ExecutionandInformationExchange
𝑡=0
• Perform at Each Time Step 𝑡: Each robot 𝑖 selects and
where 𝑠 0 is the initial global state. 𝛾 is the discount factor, executes an action 𝑎𝑡 based on the current local state 𝑠𝑡
0 ≤ 𝛾 <1,usedtocontroltheweightoffuturerewards,ensuring 𝑖 𝑖
theconvergenceofthecumulativereward. 𝑅(𝑠 𝑡,a𝑡)istheglobal
a thn ed rt oh be ob te oh ba sv ei ro vr en se tt hw eor imk m𝑄 e𝑖. diaA tefte rr ewex ae rdcu 𝑟ti 𝑡ng ant dhe tha ect nio en w,
immediaterewardattimestep𝑡, giventhestate 𝑠 𝑡 andthejoint
local state
𝑠𝑡+1.
Store the transition
(𝑠𝑡,𝑎𝑡,𝑖 𝑟𝑡,𝑠𝑡+1)
in the
action a𝑡. E represents the expectation, reflecting all possible correspondin𝑖 gexperiencereplaybuffer𝑖
𝐷
𝑖.𝑖 𝑖 𝑖
futurestatesandsequencesofactions.
Indecentralizedcontrol,eachrobot𝑖adoptsalocalstrategy • Decision to Share Information: Use the communication
𝜋𝑖 basedonitslocalinformation. Thesystem’sjointstrategyΠ strategy network 𝐶 𝑖 to evaluate whether the current state
isthecombinationofalllocalstrategiesΠ = (𝜋1,𝜋2,...,𝜋𝑁). 𝑠𝑡 isappropriateforsendinginformation:
𝑖
2.2 ControllerArchitecture
𝑐𝑡
𝑖
=𝜎(𝐶 𝑖(𝑠 𝑖𝑡)) (2)
Solving the optimization problem in decentralized multi-
If𝑐𝑡
exceedsacertainthreshold,sendkeyinformation(such
robot cooperation presents several challenges, particularly from 𝑖
as state, observed events, etc.) to other robots. Check for
theperspectivesofcomputationalcomplexity,systemscalability,
information received from other robots and integrate the
andtheutilizationoflocalinformation.
In multi-robot systems, the dimensions of the state space receivedinformation𝑏 𝑖 intotheirownstaterepresentation.
andactionspacegrowexponentiallywiththenumberofrobots. Step3: LearningUpdate
The state and possible actions of each robot not only increase
the complexity of individual decisions but also rapidly increase • Sampling from the Experience Replay Buffer: Randomly
thecomplexityoftheproblemwhenconsideringthejointstates drawamini-batchofexperiencesfromtheexperiencereplay
and actions of all robots. For each possible state and action
𝐷
𝑖
foreachrobot𝑖forlearningpurposes.
combination, it is necessary to calculate the expected reward,
• CalculateTargetQ-Values: Foreachsample,usethetarget
w thh ai tc ih soin fv teo nlv ie ms pt rr aa cv te ir cs ai ln ig na rl el ap lio tyss .ible subsequent states—a task network 𝑄ˆ 𝑖 and the updated state 𝑠 𝑖′ ⊕ 𝑏 𝑖 to compute the
targetvalues:
Additionally,indecentralizedsystems,eachrobotmayonly
be able to observe limited local information. This limits their
ability to make optimal decisions without a global view. The 𝑦 𝑖 =𝑟 𝑖 +𝛾max𝑄ˆ 𝑖(𝑠 𝑖′ ⊕𝑏 𝑖,𝑎′) (3)
𝑎′
2 Copyright©2024byASMEAction Network Action Network
Observe
Observe
Communication Stratefy Network Communication Stratefy Network
Observe
Share info Execute
Observe
Execute
Receive info Robot Robot Share info
Receive info
Observe
Action Network Action Network
Observe
Execute
Communication Stratefy Network Environment Execute Communication Stratefy Network
Observe Observe
Share info Share info
Receive info Robot Robot Receive info
Communication Channel
Action Network Action Network
Observe
Observe
Communication Stratefy Network Communication Stratefy Network
Observe
Share info Execute
Receive info Robot Execute Robot Obs Se hr av re e info
Receive info
Action Network Observe Action Network Robot
Observe
Communication Stratefy Network Execute Environment Execute Communication Stratefy Network
Observe Observe
Share info Share info
Receive info Robot Robot Receive info
Communication Channel
FIGURE1: ProposedFrameworkLayerDiagram.
• Update Behavior Network and Communication Strategy Robot
Network: Update𝑄 𝑖 byminimizingthepredictionerror: Single Task Single-Robot Single Task Single Robot
𝐿 𝑖 = (𝑦 𝑖 −𝑄 𝑖(𝑠 𝑖 ⊕𝑏 𝑖,𝑎 𝑖))2 (4) 1 Fail 3 Success
Robot Robot
Simultaneouslyupdatethecommunicationstrategynetwork 3 3
𝐶 𝑖, encouraging or discouraging information Ft ar ilansmission Box
2 Box 2 2 2
inspecificstates.
Single Task Single-Robot Robot Single Task Single Robot Robot 2 1
• PeriodicallyUpdatetheTargetNetwork: Periodicallycopy
𝑄th
ˆ
𝑖e tw oe mig ah it ns tao if nth ste abb ie lh ita yvi io nr len ae rtw nio nr gk .𝑄 𝑖tothetargetnetw Fao ilrk Rob1
ot Rob3
B
o3
o tx
Fail
2Fail
Ro3
bot B3
oxSuccess
2
Ro3
bot Success2 2
2
2
1
Step4: IterativeRepetition Rob 6ot Robot 5 2 1
Box 2
• ThealgorithmrepeatedlyexecutesSteps2and3overmul- 3 Fail Box 2 3 23 1
Robot RobRoobtot Success Robot
tiplecycles,progressivelyoptimizingeachrobot’sdecision- Single Task Mylti-R6obot 5 Single Task Mylti-Robot
makingandcommunicationstrategies. Box 2 Box 3
Robot Robot
Single Task Mylti-RFoboItGURE2: TaskTypeSinagle nTaskd MyltAi-RobcottionExecution
Throughthisapproach,eachrobotautonomouslylearnsandad-
justs its behavior based on its own experiences and interactions
with other robots, thereby achieving effective task execution
and information sharing in a decentralized environment. This the task (or subtask) is considered complete. 2. Multi-Robot,
method, which combines DQN with a dynamic communication Single Task: In this mode, multiple robots collaborate to move
strategy, optimizes the utilization of local information and the the same box. The subtask is only considered complete when
overallcollaborativeefficiencyofthesystem. thetotalloadcapacityofallinvolvedrobotsexceedstheweight
ofthebox. Thepositionoftheboxinthegridworldisrandom,
requiringtherobotstosearchandlocatethebox. Taskcompletion
3. VALIDATIONS
criteriaarecategorizedintotwotypes: Sub-taskCompletion: The
In this section, we will preliminarily validate the decen-
momentoneormorerobotssuccessfullyliftthebox,thesubtask
tralized controller framework we previously proposed in a cus-
isconsideredcomplete. Main-taskCompletion: Themaintaskis
tomizedenvironment.
onlyconsideredcompletewhenasetnumberofboxeshavebeen
successfullylifted.
3.1 SimulationEnvironment
Intheexperimentalenvironment,wehaveagroupof𝑁robots This experimental setting simulates real-world scenarios in
operatingina10x10gridworld,whereeachrobothasadefined multi-robotsystemswhererobotsmustefficientlyallocateandco-
loadcapacity,whichisthemaximumweightitcanlift. Eachbox ordinatetaskstoachieveacommongoal. Thesetupalsoinvolves
hasaspecificweight,whichaffectswhetheritcanbemovedbya dynamictaskallocationandcollaborationstrategiesamongmulti-
specificrobotoracombinationofrobots. Therearetwotypesof plerobots. Thisframeworkprovidesacontrolledenvironmentto
tasks,seeFigure2: 1. SingleRobot,SingleTask: Inthismode, developandtestalgorithmsfordistributeddecision-makingand
onerobotisresponsibleforfindingandattemptingtocarryabox. cooperativeproblem-solving,crucialforadvancingautonomous
If the robot’s load capacity is sufficient to move the box, then roboticsystemsinpracticalapplications.
3 Copyright©2024byASME3.2 EnvironmentSettings
1.0
Inthissetup,ourobjectiveistoexperimentallyvalidateand CE-DQN DQN
compare the performance of decentralized controllers based on
two types of deep reinforcement learning approaches: the stan- 0.8
dard Deep DQN and a DQN embedded with a communication
strategy network. DQN is a basic deep reinforcement learn- 0.6
ing model that does not include any additional communication
mechanisms. CE-DQN integrates a communication layer into
0.4
thebasicDQNframework,allowingrobotstoshareinformation
(such as location, status, or observed box weight) during task
0.2
execution. The comparison will focus on two main aspects: 1.
Learning Curve Comparison, evaluate and compare the differ-
0.0
encesinlearningefficiencyduringthetrainingprocessbetween 0 1000 2000 3000 4000 5000 6000
thetwomethods. 2. TaskCompletionTimeComparison, mea- Training Episodes
suretheefficiencyofthetwomethodsinperformingactualtasks,
FIGURE3: LearningCurveComparison
specifically, the time required to complete tasks. The experi-
mentinvolvestwotypesofrobots: StandardRobots, capableof
completing designated load tasks. Disturbance Robots, unable
differencewassmallerthaninTeam1. Thismightbeduetothe
tocompletethedesignatedloadtasks. Weinvolvefourdifferent
presenceofjammingmakingtaskcompletionmorechallenging,
compositionsofrobotteamsasshowninthetable1:
yet CE-DQN still maintained better efficiency. In Team 3, CE-
DQN’sperformanceadvantagebecamemorepronounced,partic-
Team Standard Disturbance Total
ularlyatlargertasksizes. ThisindicatesthatCE-DQNisbetterat
Team1 6 0 6
adaptingandoptimizingresourceallocationincomplexenviron-
Team2 5 1 6
mentscomparedtoDQN.InTeam4,inenvironmentswithmore
Team3 4 2 6
interference,CE-DQNexhibitedsignificantperformanceadvan-
Team4 3 3 6
tages,especiallyatlargertasksizes,suggestingthatCE-DQNis
TABLE1: RobotTeamCompositions moreeffectiveinhandlinghighcomplexityanduncertainty. From
theseresults,itisevidentthattheCE-DQNalgorithmgenerally
performs better than the DQN algorithm across different team
3.3 ResultsAnalysis compositionsandtasksizes. ThismayberelatedtoCE-DQN’s
First, we compare the learning efficiency of our proposed decision-making process considering more and more complex
CE-DQN algorithm and the DQN algorithm in the presence of factors, enabling it to better optimize strategies and decisions
jamming robots within a robot team. In Figure 3, the learning when facing interference and complex tasks. Additionally, CE-
curveofCE-DQNrisesfasterthanthatofDQNintheearlystages DQN’sstabilityandperformanceadvantagesacrossallteamsand
oftraining(approximately0to1000trainingcycles),indicating task sizes indicate it may have better generalization capabilities
that CE-DQN may be more effective initially and can adapt to andtheabilitytohandlecomplexdynamicenvironments. These
theenvironmentmorequickly. Thetwocurvesbecomesmoother characteristics make CE-DQN of significant practical value in
and show less fluctuation near 6000 training cycles, with CE- real applications, particularly in tasks that require coordination
DQN’scurvebeingsmootherthanthatofDQN,suggestingthat amongrobotsofvaryingcapabilities.
CE-DQN’sperformanceismorestableduringtraining,withbet-
ter adaptability and consistency in learning. In the later stages 4. CONCLUSION
of training, the average reward of CE-DQN is higher than that Thisstudyenhancestheoverallperformanceofmulti-robot
of DQN, indicating that under the same conditions, CE-DQN systemsincomplexenvironmentsbyembeddingcommunication
can achieve a better performance level. Overall, the CE-DQN strategiesintotheDQNframeworktomoreeffectivelyintegrate
algorithmshowssuperiorlearningefficiency, stability, andfinal local information. Our results clearly show that in experiments
performancecomparedtothestandardDQNalgorithm. withdifferentrobotteamconfigurationsandtasksizes,CE-DQN
ThenwecomparedthecompletiontimerequiredbytheCE- notonlyadaptsfasterandmoreefficientbutalsomaintainsitsad-
DQN and DQN algorithms for handling tasks of different sizes vantagesasenvironmentalcomplexityandinterferencelevelin-
in four different compositions of robot teams. The team com- crease. Particularlyinscenarioswithhighlevelsofinterference,
positionsrangedfromallnormalrobotstoamixofnormaland CE-DQN demonstrates significant advantages, underscoring its
disturbancerobots,asshowninFigure4. robustness and adaptability—qualities that are crucial for real-
InTeam1,forsmallertasks(size20),thecompletiontimes world application in dynamic and unpredictable environment.
ofCE-DQNandDQNwereclose,butasthetasksizeincreased, Moreover,theintegrationofcommunicationstrategieswithinthe
CE-DQN showed a clear advantage over DQN in completion DQN framework has proven to be key in achieving these im-
time, especially when the task size reached 100, where the gap provements. By facilitating better information sharing among
wasthelargest. InTeam2,asthetasksizeincreased,CE-DQN robots, the algorithm enhances the collective decision-making
also demonstrated better performance than DQN, although the process, thereby optimizing task allocation and execution effi-
4 Copyright©2024byASME
draweR
edosipE
naeMconference on industrial and information systems (ICIIS):
Team 1 Team 2
1.0 CE-DQN 1.0 CE-DQN pp.1–5.2012.IEEE.
DQN 0.9 DQN
0.8 0.8 [5] Wu,BinandSuh,CSteve.“DecentralizedMulti-RobotMo-
0.7 tionPlanningApplicabletoDynamicEnvironment.”ASME
0.6 0.6
0.5 International Mechanical Engineering Congress and Ex-
0.4 0.4 position, Vol. 59414: p. V004T05A095. 2019. American
0.3
0.2 0.2 SocietyofMechanicalEngineers.
20 40 60 80 100 20 40 60 80 100
Task Size Task Size [6] Omidshafiei, Shayegan, Pazis, Jason, Amato, Christopher,
Team 3 Team 4
1.0 CE-DQN 1.0 CE-DQN How, Jonathan P and Vian, John. “Deep decentralized
DQN DQN
multi-task multi-agent reinforcement learning under par-
0.8 0.8
tial observability.” International Conference on Machine
0.6 0.6
Learning: pp.2681–2690.2017.PMLR.
0.4 0.4 [7] Agrawal, Aakriti, Bedi, Amrit Singh and Manocha, Di-
0.2 0.2 nesh. “Rtaw: An attention inspired reinforcement learn-
20 40 Task S6 i0 ze 80 100 20 40 Task S6 i0 ze 80 100 ing method for multi-robot task allocation in warehouse
environments.” 2023 IEEE International Conference on
FIGURE4: TaskCompletionTimeComparison
Robotics and Automation (ICRA): pp. 1393–1399. 2023.
IEEE.
ciency. This research paves the way for future exploration of [8] Kapoor, Sanyam. “Multi-agent reinforcement learning:
further improvements in decentralized control systems. Sub- A report on challenges and approaches.” arXiv preprint
sequent work could focus on refining communication protocols arXiv:1807.09427(2018).
within multi-robot systems or expandingthe application of CE- [9] Fan, Jianqing, Wang, Zhaoran, Xie, Yuchen and Yang,
DQNtootherdomains,suchasnetworksofautonomousvehicles Zhuoran. “A theoretical analysis of deep Q-learning.”
or swarm robotics in exploration tasks. In summary, the CE- Learning for dynamics and control: pp. 486–489. 2020.
DQN algorithm represents a significant advancement in the de- PMLR.
velopment of decentralized control mechanisms for multi-robot
[10] Ong, Hao Yi, Chavez, Kevin and Hong, Augus-
systems,offeringarobust,efficient,andadaptablesolution.
tus. “Distributed deep Q-learning.” arXiv preprint
arXiv:1508.04186(2015).
REFERENCES
[11] Guicheng,ShenandYang,Wang.“ReviewonDec-POMDP
[1] Arai, Tamio, Pagello, Enrico, Parker, Lynne E et al. “Ad-
modelforMARLalgorithms.” SmartCommunications,In-
vances in multi-robot systems.” IEEE Transactions on
telligentAlgorithmsandInteractiveMethods: Proceedings
robotics and automation Vol. 18 No. 5 (2002): pp. 655–
of 4th International Conference on Wireless Communica-
661.
tions and Applications (ICWCA 2020): pp. 29–35. 2022.
[2] Queralta,JorgePena,Taipalmaa,Jussi,Pullinen,BilgeCan,
Springer.
Sarker, Victor Kathan, Gia, Tuan Nguyen, Tenhunen,
[12] Kraemer, Landon and Banerjee, Bikramjit. “Multi-agent
Hannu, Gabbouj, Moncef, Raitoharju, Jenni and Wester-
reinforcementlearningasarehearsalfordecentralizedplan-
lund, Tomi. “Collaborativemulti-robotsearchandrescue:
Planning,coordination,perception,andactivevision.”Ieee ning.” NeurocomputingVol.190(2016): pp.82–94.
AccessVol.8(2020): pp.191617–191643. [13] Choudhury, Shushman, Gupta, Jayesh K, Morales, Peter
[3] Bai, Yang, Asami, Koki, Svinin, Mikhail and Magid, Ev- and Kochenderfer, Mykel J. “Scalable Online planning
geni. “Cooperative multi-robot control for monitoring an for multi-agent MDPs.” Journal of Artificial Intelligence
expandingfloodarea.” 202017thInternationalConference ResearchVol.73(2022): pp.821–846.
onUbiquitousRobots(UR):pp.500–505.2020.IEEE. [14] Littman, Michael L. “Markov games as a framework for
[4] Gautam, Avinash and Mohan, Sudeept. “A review of re- multi-agentreinforcementlearning.”Machinelearningpro-
searchinmulti-robotsystems.”2012IEEE7thinternational ceedings1994. Elsevier(1994): pp.157–163.
5 Copyright©2024byASME
emiT
emiT
emiT
emiT