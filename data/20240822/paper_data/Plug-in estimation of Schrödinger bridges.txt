Plug-in estimation of Schro¨dinger bridges
∗ †
Aram-Alexandre Pooladian Jonathan Niles-Weed
August 22, 2024
Abstract
We propose a procedure for estimating the Schro¨dinger bridge between two
probability distributions. Unlike existing approaches, our method does not
require iteratively simulating forward and backward diffusions or training neural
networks to fit unknown drifts. Instead, we show that the potentials obtained
from solving the static entropic optimal transport problem between the source
and target samples can be modified to yield a natural plug-in estimator of the
time-dependent drift that defines the bridge between two measures. Under
minimal assumptions, we show that our proposal, which we call the Sinkhorn
bridge, provablyestimatestheSchro¨dingerbridgewitharateofconvergencethat
depends on the intrinsic dimensionality of the target measure. Our approach
combines results from the areas of sampling, and theoretical and statistical
entropic optimal transport.
1 Introduction
Modern statistical learning tasks often involve not merely the comparison of two
unknown probability distributions but also the estimation of transformations from
one distribution to another. Estimating such transformations is necessary when we
want to generate new samples, infer trajectories, or track the evolution of particles in
a dynamical system. In these applications, we want to know not only how “close” two
distributions are, but also how to “go” between them.
Optimal transport theory defines objects that are well suited for both of these
tasks (Villani, 2009; Santambrogio, 2015; Chewi et al., 2024). The 2-Wasserstein
distance is a popular tool for comparing probability distributions for data analysis
in statistics (Carlier et al., 2016; Chernozhukov et al., 2017; Ghosal and Sen, 2022),
machine learning (Salimans et al., 2018), and the applied sciences (Bunne et al., 2023b;
∗Center for Data Science, New York University. aram-alexandre.pooladian@nyu.edu
†Center for Data Science and Courant Institute for Mathematical Science, New York University.
jnw@cims.nyu.edu
1
4202
guA
12
]LM.tats[
1v68611.8042:viXraManole et al., 2022). Under suitable conditions, the two probability measures that
we want to compare (say, µ and ν) induce an optimal transport map: the uniquely
defined vector-valued function which acts as a transport map1 between µ and ν such
that the distance traveled is minimal in the L2 sense (Brenier, 1991). Despite being a
central object in many applications, the optimal transport map is difficult to compute
and suffers from poor statistical estimation guarantees in high dimensions; see Hu¨tter
and Rigollet (2021); Manole et al. (2021); Divol et al. (2022).
These drawbacks of the optimal transport map suggest that other approaches
for defining a transport between two measures may often be more appropriate. For
example, flow based or iterative approaches have recently begun to dominate in
computational applications—these methods sacrifice the L2-optimality of the optimal
transport map to place greater emphasis on the tractability of the resulting transport.
The work of Chen et al. (2018) proposed continuous normalizing flows (CNFs), which
use neural networks to model the vector field in an ordinary differential equation
(ODE). This machinery was exploited by several groups simultaneously (Albergo and
Vanden-Eijnden, 2022; Lipman et al., 2022; Liu et al., 2022b) for the purpose of
developing tractable constructions of vector fields that satisfy the continuity equation
(see Section 2.1.2 for a definition), and whose flow maps therefore yield valid transports
between source and target measures.
An increasingly popular alternative method for iterative transport is based on
the Fokker–Planck equation (see Section 2.1.3 for a definition). This formulation
incorporates a diffusion term, and the resulting dynamics follow a stochastic differential
equation (SDE). Though there exist many stochastic dynamics that give rise to
valid transports, a canonical role is played by the Schr¨odinger bridge (SB). Just
as the optimal transport map minimizes the L2 distance in transporting between
two distributions, the SB minimizes the relative entropy of the diffusion process,
and therefore has an interpretation as the “simplest” stochastic process bridging the
two distributions—indeed, the SB originates as a Gedankenexperiment (or “thought
experiment”) of Erwin Schro¨dinger in modeling the large deviations of diffusing gasses
(Schro¨dinger, 1932). There are many equivalent formulations of the SB problem (see
Section 2), though for the purposes of transport, its most important property is that
it gives rise to a pair of SDEs that interpolate between two measures µ and ν:
√
dX = b⋆(X )dt+ εdB , X ∼ µ,X ∼ ν, (1)
t t t t 0 1
√
dY = d⋆(Y )dt+ εdB , Y ∼ ν,Y ∼ µ, (2)
t t t t 0 1
where ε > 0 plays the role of thermal noise.2 Concretely, (1) indicates that samples
from ν can be obtained by drawing samples from µ and simulating an SDE with
drift b⋆, and (2) shows how this process can be performed in reverse. Though these
t
1T is a transport map between µ and ν if given a sample X ∼ µ, its image under T satisfies
T(X)∼ν.
2We assume throughout our work that the reference process is Brownian motion with volatility ε;
see Section 2.2.
2dynamics are of obvious use in generating samples, the difficulty lies in obtaining
estimators for the drifts.
Nearly a century later, Schr¨odinger’s thought experiment has been brought to
reality, having found applications in the generation of new images, protein structures,
and more (Kawakita et al., 2022; Liu et al., 2022a; Nusken et al., 2022; Thornton
et al., 2022; Shi et al., 2022; Lee et al., 2024). The foundation for these advances is
the work of De Bortoli et al. (2021), who propose to train two neural networks to act
as the forward and backward drifts, which are iteratively updated to ensure that each
diffusion yields samples from the appropriate distribution. This is reminiscent of the
iterative proportion fitting procedure of Fortet (1940), and can be interpreted as a
version of Sinkhorn’s matrix-scaling algorithm (Sinkhorn, 1964; Cuturi, 2013) on path
space.
While the framework of De Bortoli et al. (2021) is popular from a computational
perspective, it is worth emphasizing that this method is relatively costly, as it necessi-
tates the undesirable task of simulating an SDE at each training iteration. Moreover,
despite the recent surge in applications, current methods do not come with statistical
guarantees to quantify their performance. In short, existing work leaves open the
problem of developing tractable, statistically rigorous estimators for the Schr¨odinger
bridge.
Contributions
We propose and analyze a computationally efficient estimator of the Schr¨odinger
bridge which we call the Sinkhorn Bridge. Our main insight is that it is possible to
estimate the time-dependent drifts in (1) and (2) by solving a single, static entropic
optimal transport problem between samples from the source and target measures. Our
ˆ
approach is to compute the potentials (f,gˆ) obtained by running Sinkhorn’s algorithm
on the data X ,...,X ∼ µ and Y ,...,Y ∼ ν and plug these estimates into a simple
1 m 1 n
formula for the drifts. For example, in the forward case, our estimator reads
ˆ b (z) := (1−t)−1(cid:16) −z + (cid:80)n j=1Y jexp(cid:0) (gˆ j − 2(11 −t)∥z −Y j∥2)/ε(cid:1) (cid:17) .
t (cid:80)n exp(cid:0) (gˆ − 1 ∥z −Y ∥2)/ε(cid:1)
j=1 j 2(1−t) j
ˆ
See Section 3.1 for a detailed motivation for the choice of b . Once the estimated
t
potential gˆ is obtained from a single use of Sinkhorn’s algorithm on the source and
target data at the beginning of the procedure, computing ˆ b (z) for any z ∈ Rd and
t
any t ∈ (0,1) is trivial.
We show that the solution to a discretized SDE implemented with the estimated
ˆ
drift b closely tracks the law of the solution to (1) on the whole interval [0,τ], for any
t
τ ∈ [0,1). Indeed, writing P⋆ for the law of the process solving (1) on [0,τ] and
[0,τ]
ˆ
P for the law of the process obtained by initializing from a fresh sample X ∼ µ
[0,τ] 0
3ˆ
and solving a discrete-time SDE with drift b , we prove bounds on the risk
t
E[TV2(Pˆ ,P⋆ )]
[0,τ] [0,τ]
that imply that, for fixed ε > 0 and τ ∈ [0,1), the Schro¨dinger bridge can be estimated
at the parametric rate. Moreover, though it is well known that such bounds must
diverge as ε → 0 or τ → 1, we demonstrate that the rate of growth depends on
the intrinsic dimension k of the target measure rather than the ambient dimension
d. When k ≪ d, this gives strong justification for the use of the Sinkhorn Bridge
estimator in high-dimensional problems.
To give a particular example in a special case, our results provide novel estimation
rates for the Fo¨llmer bridge, an object which has also garnered interest in the machine
learning community (Vargas et al., 2023; Chen et al., 2024b; Huang, 2024). In this
setting, the source measure is a Dirac mass, and we suppose the target measure ν is
supported on a ball of radius R contained within a k-dimensional smooth submanifold
of Rd. Taking the volatility level to be unity, we show that the F¨ollmer bridge up to
time τ ∈ [0,1) can be estimated in total variation with precision ϵ using n samples
TV
and N SDE-discretization steps, where
n ≍ R2(1−τ)−k−2ϵ−2 , N ≲ dR4(1−τ)−4ϵ−2 .
TV TV
As advertised, for fixed τ ∈ [0,1), these bounds imply parametric scaling on the
number of samples—which matches similar findings in the entropic optimal transport
literature, see, e.g., Stromme (2023b)—and exhibit a “curse of dimensionality” only
with respect to the intrinsic dimension of the target, k. As our main theorem shows,
these phenomena are not unique to the Fo¨llmer bridge, and hold for arbitrary volatility
levels and general source measures. Moreover, by tuning τ appropriately, we show how
these estimation results yield guarantees for sampling from the target measure ν, see
Section 4.3. These guarantees also suffer only from a “curse of intrinsic dimensionality.”
Since the drifts arising from the Fo¨llmer bridge can be viewed as the score of a kernel
density estimator of ν with a Gaussian kernel (see (27)), this benign dependence on
the ambient dimension is a significant improvement over guarantees recently obtained
for such estimators in the context of denoising diffusion probabilistic models (Wibisono
et al., 2024). Our improved rates are due to the intimate connection between the
SB problem and entropic optimal transport in which intrinsic dimensionality plays a
crucial role (Groppe and Hundrieser, 2023; Stromme, 2023b). We expound on this
connection in the main text.
We are not the first to notice the simple connection between the static entropic
potentials and the SB drift. Finlay et al. (2020) first proposed to exploit this con-
nection to simulate the SB by learning static potentials via a neural network-based
implementation of Sinkhorn’s algorithm; however, due to some notational inaccuracies
and implementation errors, the resulting procedure was not scalable. This work shows
the theoretical soundness of their approach, with a much simpler, tractable algorithm
and with rigorous statistical guarantees.
4Outline. Section 2 contains the background information on both entropic optimal
transport and the Schro¨dinger bridge problem, and unifies the notation between these
two problems. Our proposed estimator, the Sinkhorn bridge, is described in Section 3,
and Section 4 contains our main results and proof sketches, with the technical details
deferred to the appendix. Simulations are performed in Section 5.
Notation
We denote the space of probability measures over Rd with finite second moment by
P (Rd). We write B(x,R) ⊆ Rd to indicate the (Euclidean) ball of radius R > 0
2
centered at x ∈ Rd. We denote the maximum of a and b by a∨b. We write a ≲ b
(resp. a ≍ b) if there exists constants C > 0 (resp. c,C > 0 such that a ≤ Cb (resp.
cb ≤ a ≤ Cb). We let path := C([0,1],Rd) be the space of paths with X : path → Rd
t
given by the canonical mapping X (h) = h for any h ∈ path and any t ∈ [0,1]. For a
t t
path measure P ∈ P(path) and any t ∈ [0,1], we write P := (X ) P ∈ P(Rd) for the tth
t t ♯
marginal of P . Similarly, for s,t ∈ [0,1], we can define the joint probability measure
t
P := (X ,X ) P. We write P for the restriction of the P to C([0,t],Rd). Since path
st s t ♯ [0,t]
is a Polish space, we can define regular conditional probabilities for the law of a path
given its value at time t, which we denote P . For any s > 0, we write Λ := (2πs)−d/2
|t s
for the normalizing constant of the density of the Gaussian distribution N(0,sI).
1.1 Related work
On Schr¨odinger bridges. The connection between entropic optimal transport
and the Schr¨odinger bridge (SB) problem is well studied; see the comprehensive
survey by L´eonard (2013). We were also inspired by the works of Ripani (2019),
Gentil et al. (2020), as well as Chen et al. (2016, 2021b) (which cover these topics
from the perspective of optimal control), and the more recent article by Kato (2024)
(which revisits the large-deviation perspective of this problem). The special case of
the F¨ollmer bridge and its variants has been a topic of recent study in theoretical
communities (Eldan et al., 2020; Mikulincer and Shenfeld, 2024).
Interest in computational methods for SBs has been explosive in over the last few
years, see De Bortoli et al. (2021); Shi et al. (2022); Bunne et al. (2023a); Tong et al.
(2023); Vargas et al. (2023); Yim et al. (2023); Chen et al. (2024b); Shi et al. (2024) for
recent developments in deep learning. The works by Bernton et al. (2019); Pavon et al.
(2021); Vargas et al. (2021) use more traditional statistical methods to estimate the
SB, with various goals in mind. For example Bernton et al. (2019) propose a sampling
scheme based on trajectory refinements using a approximate dynamic programming
approach. Pavon et al. (2021) and Vargas et al. (2021) propose methods to compute
the (intermediate) density directly based on maximum likelihood-type estimators:
Pavon et al. (2021) directly model the densities of interest and devise a scheme to
update the weights; Vargas et al. (2021) use Gaussian processes to model the forward
and backward drifts, and update them via a maximum-likelihood type loss.
5On entropic optimal transport. Our work is closely related to the growing
literature on statistical entropic optimal transport, specifically on the developments
surrounding the entropic transport map. This object was introduced by Pooladian
and Niles-Weed (2021) as a computationally friendly estimator for optimal transport
maps in the regime ε → 0; see also Pooladian et al. (2023) for minimax estimation
rates in the semi-discrete regime. When ε is fixed, the theoretical properties of the
entropic maps have been analyzed (Chiarini et al., 2022; Conforti, 2022; Chewi and
Pooladian, 2023; Conforti et al., 2023; Divol et al., 2024) as well as their statistical
properties (del Barrio et al., 2022; Goldfeld et al., 2022b,a; Gonzalez-Sanz et al., 2022;
Rigollet and Stromme, 2022; Werenski et al., 2023). Nutz and Wiesel (2021); Ghosal
et al. (2022) study the stability of entropic potentials and plans in a qualitative sense
under minimal regularity assumptions. Most recently, Stromme (2023b) and Groppe
and Hundrieser (2023) established the connections between statistical entropic optimal
transport and intrinsic dimensionality (for both maps and costs). Daniels et al. (2021)
investigates sampling using entropic optimal transport couplings combined with neural
networks. Closely related are the works by Chizat et al. (2022) and Lavenant et al.
(2021), which highlight the use of entropic optimal transport for trajectory inference.
A more flexible alternative to the entropic transport map was recently developed
by Kassraie et al. (2024), who proposed a transport that progressively displaces the
source measure to the target measure by computing a new entropic transport map
at each step to approximate the McCann interpolation (McCann, 1997).
2 Background
2.1 Entropic optimal transport
2.1.1 Static formulation
Let µ,ν ∈ P (Rd) and fix ε > 0. The entropic optimal transport problem between µ
2
and ν is written as
(cid:90)(cid:90)
OT (µ,ν) := inf 1∥x−y∥2dπ(x,y)+εKL(π∥µ⊗ν), (3)
ε 2
π∈Π(µ,ν)
where Π(µ,ν) ⊆ P (Rd ×Rd) is the set of joint measures with left-marginal µ and
2
right-marginalν, calledthesetofplans orcouplings, andwherewedefinetheKullback–
Leibler divergence as
(cid:90)
(cid:16) dπ(x,y) (cid:17)
KL(π∥µ⊗ν) := log dπ(x,y),
dµ(x)dν(y)
whenever π admits a density with respect to µ⊗ν, and +∞ otherwise. Note that
when ε = 0, (3) reduces to the 2-Wasserstein distance between µ and ν (see, e.g.,
Villani (2009); Santambrogio (2015)). The entropic optimal transport problem was
6introduced to the machine learning community by Cuturi (2013) as a numerical scheme
for approximating the 2-Wasserstein distance on the basis of samples.
Equation (3) is a strictly convex problem, and thus admits a unique minimizer,
called the optimal entropic plan, written π⋆ ∈ Π(µ,ν).3 Moreover, a dual formulation
also exists (see Genevay (2019))
OT (µ,ν) = sup Φµν(f,g)
ε (4)
(f,g)∈F
where F = L1(µ)×L1(ν) and
(cid:90) (cid:90) (cid:90)(cid:90)
(cid:16) 1 (cid:17)
Φµν(f,g) := f dµ+ gdν −ε Λ e(f(x)+g(y)− 2∥x−y∥2)/ε −1 dµ(x)dν(y), (5)
ε
where we recall Λ = (2πε)−d/2. Solutions are guaranteed to exist when µ,ν ∈
ε
P (Rd), and we call the dual optimizers the optimal entropic (Kantorovich) potentials,
2
written (f⋆,g⋆). Csisza´r (1975) showed that the primal and dual optima are intimately
connected through the following relationship:4
(cid:16)f⋆(x)+g⋆(y)− 1∥x−y∥2(cid:17)
dπ⋆(x,y) = Λ exp 2 dµ(x)dν(y). (6)
ε
ε
Though f⋆ and g⋆ are a priori defined almost everywhere on the support of µ and ν,
they can be extended to all of Rd (see Mena and Niles-Weed (2019); Nutz and Wiesel
(2021)) via the optimality conditions
(cid:90)
(cid:16) (cid:17)
f⋆(x) = −εlog Λ e(g⋆(y)−∥x−y∥2/2)/εdν(y) ,
ε
(cid:90)
(cid:16) (cid:17)
g⋆(y) = −εlog Λ e(f⋆(x)−∥x−y∥2/2)/εdµ(x) .
ε
At times, it will be convenient to work with entropic Brenier potentials, defined as
(φ⋆,ψ⋆) := (1∥·∥2 −f⋆, 1∥·∥2 −g⋆).
2 2
Note that the gradients of the entropic Brenier potentials5 are related to barycentric
projections of the optimal entropic coupling
∇φ⋆(x) = E [Y|X = x], ∇ψ⋆(y) = E [X|Y = y].
π⋆ π⋆
3Though π⋆ and the other objects discussed in this section depend on ε, we will omit this
dependence for the sake of readability, though we will track the dependence on ε in our bounds.
4ThenormalizationfactorΛ isnottypicallyusedinthecomputationaloptimaltransportliterature,
ε
but it simplifies some formulas in what follows. Since the procedure we propose is invariant under
translation of the optimal entropic potentials, this normalization factor does not affect either our
algorithm or its analysis.
5Passing the gradient under the integral is permitted via dominated convergence under suitable
tail conditions on µ and ν.
7See Pooladian and Niles-Weed (2021, Proposition 2) for a proof of this fact. By
analogy with the unregularized optimal transport problem, these are called entropic
Brenier maps. The following relationships can also be readily verified (see Chewi and
Pooladian (2023, Lemma 1)):
∇2φ⋆(x) = ε−1Cov [Y|X = x], ∇2ψ⋆(y) = ε−1Cov [X|Y = y]. (8)
π⋆ π⋆
2.1.2 A dynamic formulation via the continuity equation
Entropic optimal transport can also be understood from a dynamical perspective. Let
(p ) be a family of measures in P (Rd), and let (v ) be a family of vector
t t∈[0,1] 2 t t∈[0,1]
fields. We say that the pair satisfies the continuity equation, written (p ,v ) ∈ C, if
t t
p = µ, p = ν, and, for t ∈ [0,1],
0 1
∂ p +∇·(v p ) = 0. (9)
t t t t
Solutions to (9) are understood to hold in the weak sense (that is, with respect to
suitably smooth test functions).
The continuity equation can be viewed as the analogue of the marginal constraints
being satisfied (i.e., the set Π(µ,ν) above): it represents both the conservation of mass
and the requisite end-point constraints for the path (p ) . With this, we can cite a
t t∈[0,1]
clean expression of the dynamic formulation of (3) (see Conforti and Tamanini (2021)
or Chizat et al. (2020)) if µ and ν are absolutely continuous and have finite entropy:
(cid:90) 1(cid:90) 1 ε2
(cid:0) (cid:1)
OT (µ,ν)+C (ε,µ,ν) = inf ∥v (x)∥2 + ∥∇logp (x)∥2 dp (x)dt, (10)
ε 0 t t t
(pt,vt)∈C 0 2 8
where C (ε,µ,ν) := εlog(Λ )+ ε(H(µ)+H(ν)) is an additive constant, with H(µ) :=
(cid:82) 0 ε 2
log(dµ)dµ, similarly for H(ν).
The case ε = 0 reduces to the celebrated Benamou–Brenier formulation of optimal
transport (Benamou and Brenier, 2000).
2.1.3 A stochastic formulation via the Fokker–Planck equation
Yet another formulation of the dynamic problem exists, this time based on the Fokker–
Planck equation, which is said to be satisfied by a pair (p ,b ) ∈ F if p = µ, p = ν,
t t 0 1
and, for t ∈ [0,1],
ε
∂ p +∇·(b p ) = ∆p .
t t t t t
2
Then, under the same conditions as above,
(cid:90) 1(cid:90)
1
OT (µ,ν)+C (ε,µ) = inf ∥b (x)∥2dp (x)dt, (11)
ε 1 t t
(pt,bt) 0 2
8where C (ε,µ) = εlog(Λ )+εH(µ). The equivalence between the objective functions
1 ε
(10) and (11), as well as the continuity equation and Fokker–Planck equations, is
classical. For completeness, we provide details of these computations in Appendix A.
A key property of this equivalence is the following relationship which relates the
optimizers of (10), written (p⋆,v⋆) and (11), written (p⋆,b⋆):
t t t t
ε
b⋆ = v⋆ + ∇logp⋆.
t t 2 t
We stress that the minimizer p⋆ is the same for both (10) and (11).
t
2.2 The Schro¨dinger Bridge problem
We will now briefly develop the required machinery to understand the Schr¨odinger
bridge problem. We will largely following the expositions of L´eonard (2012, 2013);
Ripani (2019); Gentil et al. (2020).
For ε > 0, we let R ∈ P(path) denote the law of the reversible Brownian motion
on Rd with volatility ε, with the Lebesgue measure as the initial distribution.6 We
write the joint distribution of the initial and final positions under R by R (dx,dy) =
01
Λ exp(−1∥x−y∥2/ε)dxdy.
ε 2
With the above, we arrive at Schro¨dinger’s bridge problem over path measures:
min εKL(P∥R) s.t. P = µ,P = ν, (12)
0 1
P∈P(path)
where µ,ν ∈ P (Rd) and are absolutely continuous with finite entropy. Let P⋆ be the
2
unique solution to (12), which exists as the problem is strictly convex. L´eonard (2013)
shows that there exist two non-negative functions f⋆,g⋆ : Rd → R such that
+
P⋆ = f⋆(X )g⋆(X )R, (13)
0 1
where Law(X ) = µ and Law(X ) = ν.
0 1
A further connection can be made: if we apply the chain-rule for the KL divergence
by conditioning on times t = 0,1, the objective function (12) decomposes into
εKL(P∥R) = εKL(P ∥R )+εE KL(P ∥R ).
01 01 P |01 |01
Under the assumption that µ and ν have finite entropy, it can be shown that the first
term on the right-hand side is equivalent to the objective for the entropic optimal
transport problem in (4). Moreover, the second term vanishes if we choose the measure
P so that the conditional measure P is the same as R , i.e., a Brownian bridge.
|01 |01
Therefore, the objective function in (12) is minimized when P⋆ = π⋆ and when P
01
6The problem below remains well posed even though R is not a probability measure; see L´eonard
(2013) for complete discussions.
9writes as a mixture of Brownian bridges with the distribution of initial and final points
given by π⋆:
(cid:90)(cid:90)
P⋆ = R(·|X = x ,X = x )π⋆(dx ,dx ). (14)
0 0 1 1 0 1
Much of the discussion above assumed that µ and ν are absolutely continuous with
finite entropy; indeed, the manipulations in this section as well as in Sections 2.1.2
and 2.1.3 are not justified if this condition fails. Though the finite entropy conditioned
is adopted liberally in the literature on Schro¨dinger bridges, in this work we will have
to consider bridges between measures that may not be absolutely continuous (for
example, empirical measures). Noting that the entropic optimal transport problem (3)
has a unique solution for any µ,ν ∈ P (Rd), we leverage this fact to use (14) as the
2
definition of the Schr¨odinger bridge between two probability measures: for any pair
of probability distributions µ,ν ∈ P (Rd), their Schr¨odinger bridge is the mixture
2
of Brownian bridges given by (14), where π⋆ is the solution to the entropic optimal
transport problem (3).
3 Proposed estimator: The Sinkhorn bridge
Our goal is to efficiently estimate the Schro¨dinger bridge (SB) on the basis of samples.
Let P⋆ denote the SB between µ and ν, and define the the time-marginal flow of the
bridge by
p⋆ := P⋆, t ∈ [0,1]. (15)
t t
This choice of notation is deliberate: when µ and ν have finite entropy, the t-marginals
of P⋆ for t ∈ [0,1] solve the dynamic formulations (10) and (11) (L´eonard, 2013,
Proposition 4.1). In the existing literature, p⋆ is sometimes called the the entropic
t
interpolation between µ and ν. See L´eonard (2012, 2013); Ripani (2019); Gentil
et al. (2020) for interesting properties of entropic interpolations (for example, their
ˆ
relation to functional inequalities). Our goal is to provide an estimator P such that
E[TV2(Pˆ ,P⋆ )] is small for all τ < 1. In particular, this marginals of our estimator
[0,τ] [0,τ]
Pˆ are estimators pˆ of p⋆ for all t ∈ [0,1).7
t t
We call our estimator the Sinkhorn bridge, and we outline its construction below.
Ourmainobservationinvolvesrevisitingsomefinerpropertiesofentropicinterpolations
as a function of the static entropic potentials. Once everything is concretely expressed,
a natural plug-in estimator will arise which is amenable to both computational and
statistical considerations.
7For reasons that will be apparent in the next section, time τ = 1 must be excluded from the
analysis.
103.1 From Schro¨dinger to Sinkhorn and back
We outline two crucial observations from which our estimator naturally arises. First,
we note that p⋆ can be explicitly expressed as the following density (L´eonard, 2013,
t
Theorem 3.4)
p⋆(dz) := H [exp(g⋆/ε)ν](z)H [exp(f⋆/ε)µ](z)dz, (16)
t (1−t)ε tε
where H is the heat semigroup, which acts on a measure Q via
s
(cid:90)
1
Q (cid:55)→ H [Q](z) := Λ e− 2s∥x−z∥2 Q(dx).
s s
This expression for the marginal of distribution p⋆ follows directly from (14):
t
(cid:90)(cid:90)
p⋆(z) := R (z|X = x ,X = x )π⋆(dx ,dx )
t t 0 0 1 1 0 1
(cid:90)(cid:90)
= N(z|ty +(1−t)x,t(1−t)ε)π⋆(dx,dy)
(cid:90)(cid:90)
1
= Λ
e((f⋆(x)+g⋆(y)− 2∥x−y∥2)/ε)N(z|ty
+(1−t)x,t(1−t)ε)µ(dx)ν(dy)
ε
(cid:90) (cid:90)
= eg⋆(y)/εN(z|y,(1−t)ε)ν(dy) ef⋆(x)/εN(z|x,tε)µ(dx)
= H [exp(g⋆/ε)ν](z)H [exp(f⋆/ε)µ](z)
1−t t
where throughout we use N(z|m,σ2) to denote the Gaussian density with mean m and
covariance σ2I, and the fourth equality follows from computing the explicit density of
the product of two Gaussians.
Also, L´eonard (2013, Proposition 4.1) shows that when µ and ν have finite entropy,
the optimal drift in (11) is given by
b⋆(z) = ε∇logH [exp(g⋆/ε)ν](z),
t (1−t)ε
whence the pair (p⋆,b⋆) satisfies the Fokker–Planck equation. This fact implies that if
t t
X solves
t
√
dX = b⋆(X )dt+ εdB , X ∼ µ, (17)
t t t t 0
then p∗ = Law(X ). In fact, more is true: the SDE (17) give rise to a path measure,
t t
which exactly agrees with the Schr¨odinger bridge. Though L´eonard (2013) derives
these facts for µ and ν with finite entropy, we show in Proposition 3.1, below, that
they hold in more generality.
Further developing the expression for b⋆, we obtain
t
1
(cid:82) (g⋆(y)− ∥z−y∥2)/ε
(cid:16) ye 2(1−t) dν(y)(cid:17)
b⋆(z) = (1−t)−1 −z +
t (cid:82) (g⋆(y)− 1 ∥z−y∥2)/ε (18)
e 2(1−t) dν(y)
=: (1−t)−1(−z +∇φ⋆ (z)).
1−t
11Thus, our final expression for the SDE that yields the Schro¨dinger bridge is
√
dX = (−(1−t)−1X +(1−t)−1∇φ⋆ (X ))dt+ εdB . (19)
t t 1−t t t
Once again, we emphasize that our choice of notation here is deliberate: the drift is
expressed as a function of a particular entropic Brenier map, namely, the entropic
Brenier map between p⋆ and ν with regularization parameter (1−t)ε.
t
We summarize this collection of crucial properties in the following proposition;
see Appendix A.2 for proofs. We note that this result avoids the finite entropy
requirements of analogous results in the literature (L´eonard, 2013; Shi et al., 2024).
Proposition 3.1. Let π be a probability measure of the form
π(dx ,dx ) = Λ exp((f(x )+g(x )− 1∥x −x ∥2)/ε)µ (dx )µ (dx ), (20)
0 1 ε 0 1 2 0 1 0 0 1 1
for any measurable f and g and any probability measures µ ,µ ∈ P (Rd). Let M the
0 1 2
path measure given by a mixture of Brownian bridges with respect to (20) as in (14),
with t-marginals m for t ∈ [0,1]. The following hold:
t
1. The path measure M is Markov;
2. The marginal m is given by
t
m (dz) = H [exp(g/ε)µ ](z)H [exp(f/ε)µ ](z)dz;
t (1−t)ε 1 tε 0
3. M is the law of the solution to the SDE
√
dX = ε∇logH [exp(g/ε)µ ](X )dt+ εdB , X ∼ µ ;
t (1−t)ε 1 t t 0 0
4. The drift above can be expressed as b (z) = (1−t)−1(z−∇φ (z)), where ∇φ
t 1−t 1−t
is the entropic Brenier map between m and ρ with regularization strength (1−t)ε,
t
where
(cid:0) (cid:1)
ρ(dx ) = µ (dx )exp g(x )/ε+logH [ef/εµ ](x ) .
1 1 1 1 ε 0 1
If (20) is the optimal entropic coupling between µ and µ , then ρ ≡ µ .
0 1 1
3.2 Defining the estimator
In light of (18), it is easy to define an estimator on the basis of samples. Let
X ,...,X ∼ µ and Y ,...,Y ∼ ν, and let µ :=
m−1(cid:80)m
δ , and similarly
1 m 1 n m i=1 Xi
ν := n−1(cid:80)n δ . Let (fˆ ,gˆ) ∈ Rm×Rn be the optimal entropic potentials associated
n j=1 Yj
with OT (µ ,ν ), which can be computed efficiently via Sinkhorn’s algorithm (Cuturi,
ε m n
122013; Peyr´e and Cuturi, 2019) with a runtime of O(mn/ε) (Altschuler et al., 2017). A
natural plug-in estimator for the optimal drift is thus
ˆ b (z) := ε∇logH [exp(gˆ/ε)ν ]
t (1−t)ε n
=
(1−t)−1(cid:16)
−z +
(cid:80)n j=1Y jexp(cid:0) (gˆ j − 2(11 −t)∥z −Y j∥2)/ε(cid:1) (cid:17)
(21)
(cid:80)n exp(cid:0) (gˆ − 1 ∥z −Y ∥2)/ε(cid:1)
j=1 j 2(1−t) j
=: (1−t)−1(−z +∇φˆ (z))
1−t
Further discussions on the numerical aspects of our estimator are deferred to Section 5.
Since we want to estimate the path given by P⋆, our estimator is given by the solution
to the following SDE:
√
dXˆ = (−(1−kη)−1Xˆ +(1−kη)−1∇φˆ (Xˆ ))dt+ εdB , (22)
t kη 1−kη kη t
for t ∈ [kη,(k +1)η], where η ∈ (0,1) is some step-size, and k is the iteration number.
Though it is convenient to write the drift in terms of a time-varying entropic Brenier
map, (21) shows that for all t ∈ (0,1), our estimator is a simple function of the
potential gˆ obtained from a single call to Sinkhorn’s algorithm.
Remark 3.2. Tothebestofourknowledge, theideaofusingstaticpotentialstoestimate
the SB drift was first explored by Finlay et al. (2020). However, their proposal had
someinconsistencies. Forexample, theyassumeafiniteentropyconditiononthesource
and target measures, and perform a standard Gaussian convolution on Rd instead of
our proposed convolution H [exp(gˆ/ε)ν ]. The former leads to a computationally
(1−t)ε n
intractable estimator, whereas, as we have shown above, the former has a simple form
that is trivial to compute.
Remark 3.3. An alternative approach to computing the Schr¨odinger bridge is due
to Stromme (2023a): Given n samples from the source and target measure, one
can efficiently compute the in-sample entropic optimal coupling πˆ on the basis of
samples via Sinkhorn’s algorithm. Resampling a pair (X′,Y′) ∼ πˆ and computing
the Brownian bridge between X′ and Y′ yields an approximate sample from the
Schro¨dinger bridge. We remark that the computational complexity of our approach is
significantly lower than that of Stromme (2023a). While both methods use Sinkhorn’s
algorithm to compute an entropic optimal coupling between the source and target
measures, Stromme’s estimator necessitates n fresh samples from µ and ν to obtain
a single approximate sample from the SB. By contrast, having used our method to
estimatethedrifts, freshsamplesfromµcanbeusedtogenerateunlimitedapproximate
samples from the SB.
4 Main results and proof sketch
We now present the proof sketches to our main result. We first present a sketch
focusing purely on the statistical error incurred by our estimator, and later, using
13standard tools (Chen et al., 2022; Lee et al., 2023), we incorporate the additional
time-discretization error. All omitted proofs in this section are deferred to Appendix B.
4.1 Statistical analysis
We restrict our analysis to the one-sample estimation task, as it is the closest to
real-world applications where the source measure is typically known (e.g., the standard
Gaussian) and the practitioner is given finitely many samples from a distribution of in-
terest(e.g.,images). Thus,weassumefullaccesstoµandaccesstoν throughi.i.d.data,
ˆ
and let (f,gˆ) correspond to the optimal entropic potentials solving OT (µ,ν ), which
ε n
giverisetoanoptimalentropicplanπ . Formally, thiscorrespondstothem → ∞limit
n
of the setting described in Section 3.2; the estimator for the drift (21) is unchanged.
˜
Let P be the Markov measure associated with the mixture of Brownian bridges
defined with respect to π . By Proposition 3.1, the t-marginals are given by
n
p˜ (z) = H [exp(gˆ/ε)ν ](z)H [exp(fˆ /ε)µ](z), (23)
t (1−t)ε n tε
and the one-sample empirical drift is equal to
ˆ
b (z) = ε∇logH [exp(gˆ/ε)ν ](z).
t (1−t)ε n
˜ ˜
Thus, P is the law of the following process with X ∼ µ
0
√
˜ ˆ ˜
dX = b (X )dt+ εdB . (24)
t t t t
Note that this agrees with our estimator in (22), but without discretization. This
process is not technically implementable, but forms an important theoretical tool in
our analysis.
Our main result of this section is the following theorem.
Theorem 4.1 (One-sample estimation; no discretization). Suppose both µ,ν ∈ P (Rd),
2
and ν is supported on a k-dimensional smooth submanifold of Rd whose support is
˜
contained in a ball of radius R > 0. Let P (resp. P) be the path measure corresponding
to (24) (resp. (18)). Then it holds that, for any τ ∈ [0,1),
(cid:16)ε−k/2−1 R2ε−k (cid:17)
E[TV2(P˜ ,P⋆ )] ≲ √ + .
[0,τ] [0,τ] n (1−τ)k+2n
As mentioned in the introduction, the parametric rates will not be surprising given
the proof sketch below, which incorporates ideas from entropic optimal transport. The
rates diverge exponentially in k as τ → 1; this is a consequence of the fact that the
ˆ
estimated drift b enforces that the samples exactly collapse onto the training data at
t
terminal time, which is far from the true target measure.
14The proof of Theorem 4.1 uses key ideas from Stromme (2023b): We introduce
the following entropic plan
π¯ (x,y) := Λ exp(cid:0) (f¯ (x)+g⋆(y)− 1∥x−y∥2)/ε(cid:1) µ(dx)ν (dy), (25)
n ε 2 n
where g⋆ is the optimal entropic potential for the population measures (µ, ν), and
where we call f¯ : Rd → R a rounded potential, defined as
n
(cid:16) (cid:88) (cid:17)
f¯ (x) := −εlog Λ ·n−1 exp((g⋆(Y )− 1∥x−Y ∥2)/ε) .
ε j 2 j
j=1
Note that f¯ can be viewed as the Sinkhorn update involving the potential g⋆ and
measure ν , and that π¯ ∈ Γ(µ,ν¯ ), where ν¯ is a rescaled version of ν . We
n n n n n
again exploit Proposition 3.1. Consider the path measure associated to the mixture
of Brownian bridges with respect to π¯ , denoted P¯ (with t-marginals p¯ ), which
n t
corresponds to an SDE with drift
¯ b (z) = ε∇logH [exp(g⋆/ε)ν ](z)
t 1−t n
=
(1−t)−1(cid:16)
−z +
(cid:80)N j=1Y jexp((g⋆(Y j)+ 2(11 −t)∥z −Y j∥2)/ε)(cid:17)
.
(26)
(cid:80)N exp((g⋆(Y )+ 1 ∥z −Y ∥2)/ε)
j=1 j 2(1−t) j
¯
Introducing the path measure P into the bound via triangle inequality and then
[0,τ]
applying Pinsker’s inequality, we arrive at
E[TV2(P˜ ,P⋆ )] ≲ E[TV2(P˜ ,P¯ )]+E[TV2(P¯ ,P⋆ )]
[0,τ] [0,τ] [0,τ] [0,τ] [0,τ] [0,τ]
≲ E[KL(P˜ ∥P¯ )]+E[KL(P⋆ ∥P¯ )],
[0,τ] [0,τ] [0,τ] [0,τ]
We analyse the two terms separately, each term involving proof techniques developed
by Stromme (2023b). We summarize the results in the following propositions, which
yield the proof of Theorem 4.1.
Proposition 4.2. Assume the conditions of Theorem 4.1, then for any τ ∈ [0,1)
1
E[KL(P˜ ∥P¯ )] ≤ E[OT (µ,ν )−OT (µ,ν)] ≤ ε−(k/2+1)n−1/2.
[0,τ] [0,τ] ε n ε
ε
Proposition 4.3. Assume the conditions of Theorem 4.1, then
R2ε−k
E[KL(P⋆ ∥P¯ )] ≤ (1−τ)−k−2.
[0,τ] [0,τ] n
154.2 Completing the results
ˆ
We now incorporate the discretization error. Letting P denote the path measure
induced by the dynamics of (22), we use the triangle inequality to introduce the path
˜
measure P:
E[TV2(Pˆ ,P⋆ )] ≲ E[TV2(Pˆ ,P˜ )]+E[TV2(P˜ ,P⋆ )].
[0,τ] [0,τ] [0,τ] [0,τ] [0,τ] [0,τ]
The second term is precisely the statistical error, controlled by Theorem 4.1. For the
first term, we employ a now-standard discretization argument (see e.g., Chen et al.
(2022)) which bounds the total variation error as a function of the step-size parameter
η and the Lipschitz constant of the empirical drift, which can be easily bounded in
our setting.
Proposition 4.4. Suppose µ,ν ∈ P (Rd). Denoting L for the Lipschitz constant of
2 τ
ˆ
b (recall Equation (21)) for t ∈ [0,1) and η the step-size of the SDE discretization, it
τ
holds that
E[TV2(Pˆ ,P˜ )] ≲ (ε+1)L2dη.
[0,τ] [0,τ] τ
In particular, if supp(ν) ⊆ B(0;R), then
E[TV2(Pˆ ,P˜ )] ≲ (ε+1)(1−τ)−2dη(1∨R4(1−τ)−2ε−2).
[0,τ] [0,τ]
We now aggregate the statistical and approximation error into one final result.
Theorem 4.5. Suppose µ,ν ∈ P (Rd) with supp(ν) ⊆ B(0,R) ⊆ M, where M is
2
a k-dimensional submanifold of Rd. Given n i.i.d. samples from ν, the one-sample
Sinkhorn bridge Pˆ estimates the Schr¨odinger bridge P⋆ with the following error
(cid:16)ε−k/2−1 R2ε−k (cid:17)
E[TV2(Pˆ ,P⋆ )] ≲ √ +
[0,τ] [0,τ] n (1−τ)k+2n
+(ε+1)(1−τ)−2dη(1∨R4(1−τ)−2ε−2).
Assuming R ≥ 1 and ε = 1, the Schro¨dinger bridge can be estimated in total variation
distance to accuracy ϵ with n samples and N Euler–Maruyama steps, where
TV
R2 dR4
n ≍ ∨ϵ−4 , N ≲ .
(1−τ)k+2ϵ2 TV (1−τ)4ϵ2
TV TV
Note that our error rates improve as ε → ∞; since this is also the regime in which
Sinkhorn’s algorithm terminates rapidly, it is natural to suppose that ε should be large
in practice. This is misleading, however: as ε grows, the Schro¨dinger bridge becomes
less and less informative,8 and the marginal p⋆ only resembles ν when τ becomes very
τ
close to 1. We elaborate on the use of the SB for sampling in the following section.
8In other words, the transport path is more and more volatile.
164.3 Application: Sampling with the Fo¨llmer bridge
Theorem 4.5 does not immediately imply guarantees for sampling from the target
distribution ν. Obtaining such guarantees requires arguing that simulating the
Sinkhorn bridge on a suitable interval [0,τ] for τ close to 1 yields samples close to the
true density (without completely collapsing onto the training data). We provide such
a guarantee in this section, for the special case of the Fo¨llmer bridge. We adopt this
setting only for concreteness; similar arguments apply more broadly.
The F¨ollmer bridge is a special case of the Schr¨odinger bridge due to Hans
F¨ollmer (F¨ollmer, 1985). In this setting, µ = δ for any a ∈ Rd, and our estimator
a
takes a particularly simple form:
ˆ bF(z) =
(1−t)−1(cid:16)
−z +
(cid:80)n j=1Y jexp(cid:0) (1 2∥Y j∥2 − 2(11 −t)∥z −Y j∥2)/ε(cid:1) (cid:17)
, (27)
t (cid:80)n exp(cid:0) (1∥Y ∥2 − 1 ∥z −Y ∥2)/ε(cid:1)
j=1 2 j 2(1−t) j
Notethatinthisspecialcase, calculatingthedriftdoesnotrequiretheuseofSinkhorn’s
algorithm, and the drift, in fact, corresponds to the score of a kernel density estimator
applied to ν . We provide a calculation of these facts in Appendix B.3 for completeness.
n
We then have the following guarantee.
Corollary 4.6. Consider the assumptions of Theorem 4.5, further suppose that µ = δ
0
and ε = 1 and that the second moment of ν is bounded by d. Suppose we use n
samples from ν to estimate the Fo¨llmer drift, and simulate the resulting SDE using N
Euler–Maruyama iterations until time τ = 1−ϵ2 /d, with
W2
R2dk+2 R4d5
n ≍ ∨ϵ−4 N ≲ .
ϵ2k+4ϵ2 TV ϵ8 ϵ2
W2 TV W2 TV
Then the density given by the Sinkhorn bridge at time τ iterations will be ϵ -close
TV
in total variation to a measure which is ϵ -close to ν in the 2-Wasserstein distance.
W2
Notethatthechoiceε = 1wasmerelyoutofconvenience. Ifinsteadthepractitioner
was willing to pay the computational price of solving Sinkhorn’s algorithm for small ε
and large n, then the number of requisite iterations N would decrease. Finally, notice
that the number of samples scales exponentially in the intrinsic dimension k ≪ d
instead of the ambient dimension d. This is, of course, unavoidable, but improves
upon recent work that uses kernel density estimators to prove a similar result for
denoising diffusion probabilistic models (Wibisono et al., 2024).
Remark 4.7. Recently, Huang (2024) also proposed (27) to estimate the F¨ollmer
drift. They provide no statistical estimation guarantees of the drift, nor any sampling
guarantees; their contributions are largely empirical, demonstrating that the proposed
estimator is tractable for high-dimensional tasks. The work of Huang et al. (2021)
also proposes an estimator for the F¨ollmer bridge based on having partial access to
the log-density ratio of the target distribution (without the normalizing constant).
17Algorithm 1 Sinkhorn bridges
Input: Data {X }m ∼ µ, {Y }n ∼ ν, parameters ε > 0, τ ∈ (0,1), and N ≥ 1
i i=1 j j=1
Compute: Sinkhorn potentials (fˆ ,gˆ) ∈ Rm ×Rn ▷ Using POT or OTT
Initialize: x(0) = x ∼ µ, k = 0, stepsize η = τ/N
while k ≤ N −1 do
√
x(k+1) = x(k) +ηˆ b (x(k))+ ηεξ ▷ ξ ∼ N(0,I)
kη
k ← k +1
end while
Return: x(N)
5 Numerical performance
Our approach is summarized in Algorithm 1, and open-source code for replicating our
experiments is available at https://github.com/APooladian/SinkhornBridge.9
ˆ
For a fixed regularization parameter ε > 0, the runtime of computing (f,gˆ) on
the basis of samples has complexity O(mn/(εδ )), where δ is a required tolerance
tol tol
parameter that measures how closely the the marginal constraints are satisfied (Cuturi,
2013; Peyr´e and Cuturi, 2019; Altschuler et al., 2022). Once these are computed, the
ˆ
evaluation of b is O(n), with the remaining runtime being the number of iteration
kη
steps, denoted by N. In all our experiments, we take m = n, thus the total runtime
complexity of the algorithm is a fixed cost of O(n2/(εδ ), followed by O(nN) for
tol
each new sample to be generated (which can be parallelized).
5.1 Qualitative illustration
As a first illustration, we consider standard two-dimensional datasets from the machine
learning literature. For all examples, we use n = 2000 training points from both the
source and target measure, and run Sinkhorn’s algorithm with ε = 0.1. For generation,
we set τ = 0.9, and consider N = 50 Euler–Maruyama steps. Figure 1 contains
the resulting simulations, starting from out-of-sample points. We see reasonable
performance in each case.
5.2 Quantitative illustrations
We quantitatively assess the performance of our estimator using synthetic examples
from the deep learning literature (Bunne et al., 2023a; Gushchin et al., 2023).
5.2.1 The Gaussian case
We first demonstrate that we are indeed learning the drift and that the claimed rates
are empirically justified. As a first step, we consider the simple case where µ = N(a,A)
9Our estimator is implemented in both the POT and OTT-JAX frameworks.
18Figure 1: Schr¨odinger bridges on the basis of samples from toy datasets.
and ν = N(b,B) for two positive-definite d×d matrices A and B and arbitrary vectors
a,b ∈ Rd. In this regime, the optimal drift b⋆ and p⋆ has been computed in closed-form
τ τ
by Bunne et al. (2023a); see equations (25)-(29) in their work.
To verify that we are indeed learning the drift, we first draw n samples from
ˆ
µ and ν, and compute our estimator, b for any τ ∈ [0,1). We then evaluate the
τ
mean-squared error
MSE(n,τ) = ∥ˆ b −b⋆∥2 ,
τ τ L2(p⋆)
τ
by a Monte Carlo approximation, with n = 10000. For simplicity, with d = 3, we
MC
choose A = I and randomly generate a positive-definite matrix B, and center the
Gaussians. We fix ε = 1 and vary n used to define our estimator, and perform the
simulation ten times to generate error bars across various choices of τ ∈ [0,1); see
Figure 2.
It is clear from the plot that the constant associated to the rate of estimation gets
worse as τ → 1, but the overall rate of convergence appears unchanged, which hovers
around n−1 for all choices of τ shown in the plot, as expected from e.g., Proposition 4.2.
5.2.2 Multimodal measures with closed-form drift
The next setting is due to Gushchin et al. (2023); they devised a drift that defines the
Schro¨dinger bridge between a Gaussian and a more complicated measure with multiple
modes. This explicit drift allowed them to benchmark multiple neural network based
methods for estimating the Schro¨dinger bridge for non-trivial couplings (e.g., beyond
the Gaussian to Gaussian setting). We briefly remark that the approaches discussed in
their work fall under the “continuous estimation” paradigm, where researchers assume
they can endlessly sample from the distributions when training (using new samples
per training iteration).
19Figure 2: MSE for estimating the Gaussian drift as (n,τ) vary, averaged over 10 trials.
We consider the same pre-fixed drift as found in their publicly available code, which
transports the standard Gaussian to a distribution with four modes. We consider the
case d = 64 and ε = 1, as these hyperparameters are most extensively studied in their
work, where they provide the most details on the other models. We use n = 4096
training samples from the source and target data they construct (which is significantly
less than the total number of samples required for any of the neural network based
models) and perform our estimation procedure, and we take N = 100 discretization
steps (which is half as many as most of the works they consider) to simulate to time
τ = 0.99. To best illustrate the four mixture components, Figure 3 contains a scatter
plot of the first and fifteenth dimension, containing fresh target samples and our
generated samples.
Wecomparetotheground-truthsamplesusingtheunexplainedvariancepercentage
(UVP) based on the Bures–Wasserstein distance (Bures, 1969):
BW2(N ,N )
µ (cid:55)→ BW-UVP (µ) := 100 µ ν ,
ν
0.5·Var(ν)
where N = N(E [X],Cov (X)), and same for N .10 While seemingly ad hoc, the
µ µ µ ν
BW-UVP is widely used in the machine learning literature as a means of quantifying
the quality of the generated samples (see e.g., Daniels et al. (2021)). We compute the
BW-UVP with 104 generated samples from the target and our approach, averaged
over 5 trials, and used the results of Gushchin et al. (2023) for the remaining methods
(MLE-SB is by Vargas et al. (2021), EgNOT is by Mokrov et al. (2023), and FB-SDE-A
is by Chen et al. (2021a)). We see that the Sinkhorn bridge has significantly lower
10For us, these quantities are computed on the basis of samples.
20Method BW-UVP
Ours 0.41 ± 0.03
MLE-SB 0.56
EgNOT 0.85
FB-SDE-A 0.65
Table 1: Comparison to neural network
approaches in BW-UVP for d = 64.
Figure 3: Plotting generated and re-
sampled target data in d = 64.
BW-UVP compared to the other approaches while requiring less compute resources
and training data.
6 Conclusion
This work makes a connection between the static entropic optimal transport problem,
the Schr¨odinger bridge problem, and Sinkhorn’s algorithm, which appeared to be
lacking in the literature. We proposed and analyzed a plug-in estimator of the
Schr¨odinger bridge, which we call the Sinkhorn bridge. Due to a Markov property
enjoyed by entropic optimal couplings, our estimator relates Sinkhorn’s matrix-scaling
algorithm to the optimal drift that arises in the Schr¨odinger bridge problem, and
existing theory in the statistical optimal transport literature provide us with statistical
guarantees. A novelty of our approach is the reduction of a “dynamic” estimation
problem to a “static” one, where the latter is easy to analyze.
Several questions arise from our work, we highlight some here:
Further connections to other processes: Our arguments for the Schr¨odinger
bridge used the particular form of the reversible Brownian motion. It would be
interesting to develop this approach for other types of reference processes for the
purposes of developing statistical guarantees. The Sinkhorn bridge estimator
can also be implemented through an ordinary differential equation (ODE) and
not necessarily through an SDE. This gives rise to the probability flow ODE
in the generative modeling literature (Song et al., 2020). Chen et al. (2024a)
showed that this approach can achieve results comparable to those obtained by
diffusion models (Chen et al., 2022; Lee et al., 2023). We anticipate analogous
results would hold in our setting.
Lower bounds: Entropic optimal transport suffers from a dearth of lower bounds
21in the literature. It is unclear whether our approach is optimal in terms of
its dependence on ε and τ. Developing estimators with better performance or
nontrivial lower bounds would help establish how far our estimators are from
optimality.
Computation in practice: On the computational side, one can ask if are there
better estimators of the drift b⋆ than the plug-in estimator we outlined (possibly
t
amenable to statistical analysis), and to consider using our estimator on non-
synthetic problems. For example, it seems advisable to compute the Sinkhorn
bridge in a latent space, and reverting the latent transformation later (Rombach
et al., 2022).
Acknowledgements
AAP thanks NSF grant DMS-1922658 and Meta AI Research for financial support.
JNW is supported by the Sloan Research Fellowship and NSF grant DMS-2339829.
References
Albergo, M. S. and Vanden-Eijnden, E. (2022). Building normalizing flows with
stochastic interpolants. arXiv preprint arXiv:2209.15571.
Altschuler, J., Weed, J., and Rigollet, P. (2017). Near-linear time approximation
algorithms for optimal transport via Sinkhorn iteration. In Advances in Neural
Information Processing Systems 30.
Altschuler, J. M., Niles-Weed, J., and Stromme, A. J. (2022). Asymptotics for
semidiscrete entropic optimal transport. SIAM Journal on Mathematical Analysis,
54(2):1718–1741.
Benamou, J.-D. and Brenier, Y. (2000). A computational fluid mechanics solution to
the Monge–Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375–
393.
Bernton, E., Heng, J., Doucet, A., and Jacob, P. E. (2019). Schr¨odinger bridge
samplers. arXiv preprint arXiv:1912.13170.
Brenier, Y. (1991). Polar factorization and monotone rearrangement of vector-valued
functions. Comm. Pure Appl. Math., 44(4):375–417.
Bunne, C., Hsieh, Y.-P., Cuturi, M., and Krause, A. (2023a). The Schro¨dinger bridge
between Gaussian measures has a closed form. In International Conference on
Artificial Intelligence and Statistics, pages 5802–5833. PMLR.
22Bunne, C., Stark, S. G., Gut, G., Del Castillo, J. S., Levesque, M., Lehmann, K.-V.,
Pelkmans, L., Krause, A., and Ra¨tsch, G. (2023b). Learning single-cell perturbation
responses using neural optimal transport. Nature methods, 20(11):1759–1768.
Bures, D. (1969). An extension of Kakutani’s theorem on infinite product measures
to the tensor product of semifinite w*-algebras. Transactions of the American
Mathematical Society, 135:199–212.
Carlier, G., Chernozhukov, V., and Galichon, A. (2016). Vector quantile regression:
An optimal transport approach. The Annals of Statistics, 44(3):1165–1192.
Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018). Neural
ordinary differential equations. Advances in neural information processing systems,
31.
Chen, S., Chewi, S., Lee, H., Li, Y., Lu, J., and Salim, A. (2024a). The probability
flow ODE is provably fast. Advances in Neural Information Processing Systems, 36.
Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2022). Sampling
is as easy as learning the score: Theory for diffusion models with minimal data
assumptions. arXiv preprint arXiv:2209.11215.
Chen, T., Liu, G.-H., andTheodorou, E.A.(2021a). LikelihoodtrainingofSchro¨dinger
bridge using forward-backward SDEs theory. arXiv preprint arXiv:2110.11291.
Chen, Y., Georgiou, T. T., and Pavon, M. (2016). On the relation between optimal
transport and Schr¨odinger bridges: A stochastic control viewpoint. Journal of
Optimization Theory and Applications, 169:671–691.
Chen, Y., Georgiou, T. T., and Pavon, M. (2021b). Stochastic control liaisons:
Richard Sinkhorn meets Gaspard Monge on a Schr¨odinger bridge. Siam Review,
63(2):249–313.
Chen, Y., Goldstein, M., Hua, M., Albergo, M.S., Boffi, N.M., andVanden-Eijnden, E.
(2024b). Probabilistic forecasting with stochastic interpolants and Fo¨llmer processes.
arXiv preprint arXiv:2403.13724.
Chernozhukov,V.,Galichon,A.,Hallin,M.,andHenry,M.(2017). Monge–Kantorovich
depth, quantiles, ranks and signs. The Annals of Statistics, 45(1):223–256.
Chewi, S., Niles-Weed, J., and Rigollet, P. (2024). Statistical optimal transport.
Chewi, S. and Pooladian, A.-A. (2023). An entropic generalization of Caffarelli’s
contraction theorem via covariance inequalities. Comptes Rendus. Math´ematique,
361(G9):1471–1482.
23Chiarini, A., Conforti, G., Greco, G., and Tamanini, L. (2022). Gradient estimates
for the Schr¨odinger potentials: Convergence to the Brenier map and quantitative
stability. arXiv preprint arXiv:2207.14262.
Chizat, L., Roussillon, P., L´eger, F., Vialard, F.-X., and Peyr´e, G. (2020). Faster
Wasserstein distance estimation with the Sinkhorn divergence. Advances in Neural
Information Processing Systems, 33:2257–2269.
Chizat, L., Zhang, S., Heitz, M., and Schiebinger, G. (2022). Trajectory inference
via mean-field Langevin in path space. Advances in Neural Information Processing
Systems, 35:16731–16742.
Conforti, G. (2022). Weak semiconvexity estimates for Schro¨dinger potentials and loga-
rithmic Sobolev inequality for Schro¨dinger bridges. arXiv preprint arXiv:2301.00083.
Conforti, G., Durmus, A., and Greco, G. (2023). Quantitative contraction rates for
Sinkhorn algorithm: Beyond bounded costs and compact marginals. arXiv preprint
arXiv:2304.04451.
Conforti, G. and Tamanini, L. (2021). A formula for the time derivative of the entropic
cost and applications. Journal of Functional Analysis, 280(11):108964.
Csisza´r, I. (1975). I-divergence geometry of probability distributions and minimization
problems. Ann. Probability, 3:146–158.
Cuturi, M. (2013). Sinkhorn distances: Lightspeed computation of optimal transport.
Advances in Neural Information Processing Systems, 26.
Daniels, M., Maunu, T., and Hand, P. (2021). Score-based generative neural networks
for large-scale optimal transport. Advances in neural information processing systems,
34:12955–12965.
De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. (2021). Diffusion Schro¨dinger
bridge with applications to score-based generative modeling. Advances in Neural
Information Processing Systems, 34:17695–17709.
del Barrio, E., Gonzalez-Sanz, A., Loubes, J.-M., and Niles-Weed, J. (2022). An
improvedcentrallimittheoremandfastconvergenceratesforentropictransportation
costs. arXiv preprint arXiv:2204.09105.
Divol, V., Niles-Weed, J., and Pooladian, A.-A. (2022). Optimal transport map
estimation in general function spaces. arXiv preprint arXiv:2212.03722.
Divol, V., Niles-Weed, J., and Pooladian, A.-A. (2024). Tight stability bounds for
entropic Brenier maps. arXiv preprint arXiv:2404.02855.
24Eldan, R., Lehec, J., and Shenfeld, Y. (2020). Stability of the logarithmic Sobolev
inequality via the Fo¨llmer process.
Finlay,C.,Gerolin,A.,Oberman,A.M.,andPooladian,A.-A.(2020). Learningnormal-
izing flows from Entropy-Kantorovich potentials. arXiv preprint arXiv:2006.06033.
Fo¨llmer, H. (1985). An entropy approach to the time reversal of diffusion processes. In
Stochastic differential systems (Marseille-Luminy, 1984), volume 69 of Lect. Notes
Control Inf. Sci., pages 156–163. Springer, Berlin.
Fortet, R. (1940). R´esolution d’un syst`eme d’´equations de m. Schro¨dinger. Journal de
Math´ematiques Pures et Appliqu´ees, 19(1-4):83–105.
Genevay, A. (2019). Entropy-regularized optimal transport for machine learning. PhD
thesis, Paris Sciences et Lettres (ComUE).
Gentil, I., L´eonard, C., Ripani, L., and Tamanini, L. (2020). An entropic interpolation
proof of the HWI inequality. Stochastic Processes and their Applications, 130(2):907–
923.
Ghosal, P., Nutz, M., and Bernton, E. (2022). Stability of entropic optimal transport
and Schro¨dinger bridges. Journal of Functional Analysis, 283(9):109622.
Ghosal,P.andSen,B.(2022). Multivariateranksandquantilesusingoptimaltransport:
consistency, rates and nonparametric testing. Ann. Statist., 50(2):1012–1037.
Goldfeld, Z., Kato, K., Rioux, G., and Sadhu, R. (2022a). Limit theorems for
entropic optimal transport maps and the Sinkhorn divergence. arXiv preprint
arXiv:2207.08683.
Goldfeld, Z., Kato, K., Rioux, G., and Sadhu, R. (2022b). Statistical inference with
regularized optimal transport. arXiv preprint arXiv:2205.04283.
Gonzalez-Sanz, A., Loubes, J.-M., and Niles-Weed, J. (2022). Weak limits of entropy
regularized optimal transport; potentials, plans and divergences. arXiv preprint
arXiv:2207.07427.
Groppe, M. and Hundrieser, S. (2023). Lower complexity adaptation for empirical
entropic optimal transport. arXiv preprint arXiv:2306.13580.
Gushchin, N., Kolesov, A., Mokrov, P., Karpikova, P., Spiridonov, A., Burnaev, E.,
and Korotin, A. (2023). Building the bridge of Schro¨dinger: A continuous entropic
optimal transport benchmark. Advances in Neural Information Processing Systems,
36:18932–18963.
Huang, H. (2024). One-step data-driven generative model via Schr¨odinger bridge.
arXiv preprint arXiv:2405.12453.
25Huang,J.,Jiao,Y.,Kang,L.,Liao,X.,Liu,J.,andLiu,Y.(2021). Schro¨dinger–Fo¨llmer
sampler: Sampling without ergodicity. arXiv preprint arXiv:2106.10880.
Hu¨tter, J.-C. and Rigollet, P. (2021). Minimax estimation of smooth optimal transport
maps. The Annals of Statistics, 49(2):1166–1194.
Kassraie, P., Pooladian, A.-A., Klein, M., Thornton, J., Niles-Weed, J., and Cu-
turi, M. (2024). Progressive entropic optimal transport solvers. arXiv preprint
arXiv:2406.05061.
Kato, K. (2024). Large deviations for dynamical Schro¨dinger problems. arXiv preprint
arXiv:2402.05100.
Kawakita, G., Kamiya, S., Sasai, S., Kitazono, J., and Oizumi, M. (2022). Quantifying
brain state transition cost via Schro¨dinger bridge. Network Neuroscience, 6(1):118–
134.
Lavenant, H., Zhang, S., Kim, Y.-H., and Schiebinger, G. (2021). Towards a mathe-
matical theory of trajectory inference. arXiv preprint arXiv:2102.09204.
Lee, D., Lee, D., Bang, D., and Kim, S. (2024). Disco: Diffusion Schr¨odinger bridge
for molecular conformer optimization. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 38, pages 13365–13373.
Lee, H., Lu, J., and Tan, Y. (2023). Convergence of score-based generative modeling
for general data distributions. In International Conference on Algorithmic Learning
Theory, pages 946–985. PMLR.
L´eonard, C. (2012). From the Schro¨dinger problem to the Monge–Kantorovich problem.
Journal of Functional Analysis, 262(4):1879–1920.
L´eonard, C. (2013). A survey of the Schro¨dinger problem and some of its connections
with optimal transport. arXiv preprint arXiv:1308.0215.
Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M.(2022). Flow matching
for generative modeling. arXiv preprint arXiv:2210.02747.
Liu, G.-H., Chen, T., So, O., and Theodorou, E. (2022a). Deep generalized Schro¨dinger
bridge. Advances in Neural Information Processing Systems, 35:9374–9388.
Liu, X., Gong, C., and Liu, Q. (2022b). Flow straight and fast: Learning to generate
and transfer data with rectified flow. arXiv preprint arXiv:2209.03003.
Manole, T., Balakrishnan, S., Niles-Weed, J., and Wasserman, L. (2021). Plugin
estimation of smooth optimal transport maps. arXiv preprint arXiv:2107.12364.
26Manole,T.,Bryant,P.,Alison,J.,Kuusela,M.,andWasserman,L.(2022). Background
modeling for double Higgs boson production: Density ratios and optimal transport.
arXiv preprint arXiv:2208.02807.
McCann, R. J. (1997). A convexity principle for interacting gases. Advances in
mathematics, 128(1):153–179.
Mena, G. and Niles-Weed, J. (2019). Statistical bounds for entropic optimal transport:
sample complexity and the central limit theorem. Advances in Neural Information
Processing Systems, 32.
Mikulincer, D. and Shenfeld, Y. (2024). The Brownian transport map. Probability
Theory and Related Fields, pages 1–66.
Mokrov, P., Korotin, A., Kolesov, A., Gushchin, N., and Burnaev, E. (2023). Energy-
guided entropic neural optimal transport. arXiv preprint arXiv:2304.06094.
Nusken, N., Vargas, F., Ovsianas, A., Fernandes, D., Girolami, M., and Lawrence, N.
(2022). Bayesian learning via neural Schro¨dinger–Fo¨llmer flows. STATISTICS AND
COMPUTING, 33.
Nutz, M. and Wiesel, J. (2021). Entropic optimal transport: Convergence of potentials.
Probability Theory and Related Fields, pages 1–24.
Pavon, M., Trigila, G., and Tabak, E. G. (2021). The data-driven Schro¨dinger bridge.
Communications on Pure and Applied Mathematics, 74(7):1545–1573.
Peyr´e, G. and Cuturi, M. (2019). Computational optimal transport. Foundations and
Trends® in Machine Learning, 11(5-6):355–607.
Pooladian, A.-A., Divol, V., and Niles-Weed, J. (2023). Minimax estimation of
discontinuous optimal transport maps: The semi-discrete case. arXiv preprint
arXiv:2301.11302.
Pooladian, A.-A. and Niles-Weed, J. (2021). Entropic estimation of optimal transport
maps. arXiv preprint arXiv:2109.12004.
Rigollet, P. and Stromme, A. J. (2022). On the sample complexity of entropic optimal
transport. arXiv preprint arXiv:2206.13472.
Ripani, L. (2019). Convexity and regularity properties for entropic interpolations.
Journal of Functional Analysis, 277(2):368–391.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-
resolution image synthesis with latent diffusion models. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 10684–
10695.
27Salimans, T., Zhang, H., Radford, A., and Metaxas, D. (2018). Improving GANs
using optimal transport. In International Conference on Learning Representations.
Santambrogio, F. (2015). Optimal transport for applied mathematicians. Birk¨auser,
NY, 55(58-63):94.
Schr¨odinger, E. (1932). Sur la th´eorie relativiste de l’´electron et l’interpr´etation de
la m´ecanique quantique. In Annales de l’institut Henri Poincar´e, volume 2, pages
269–310.
Shi, Y., De Bortoli, V., Campbell, A., and Doucet, A. (2024). Diffusion Schr¨odinger
bridge matching. Advances in Neural Information Processing Systems, 36.
Shi, Y., De Bortoli, V., Deligiannidis, G., and Doucet, A. (2022). Conditional simula-
tion using diffusion Schr¨odinger bridges. In Uncertainty in Artificial Intelligence,
pages 1792–1802. PMLR.
Sinkhorn, R. (1964). A relationship between arbitrary positive matrices and doubly
stochastic matrices. The annals of mathematical statistics, 35(2):876–879.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B.
(2020). Score-based generative modeling through stochastic differential equations.
arXiv preprint arXiv:2011.13456.
Stromme, A.(2023a). SamplingfromaSchro¨dingerbridge. InInternational Conference
on Artificial Intelligence and Statistics, pages 4058–4067. PMLR.
Stromme, A. J. (2023b). Minimum intrinsic dimension scaling for entropic optimal
transport. arXiv preprint arXiv:2306.03398.
Thornton, J., Hutchinson, M., Mathieu, E., De Bortoli, V., Teh, Y. W., and Doucet, A.
(2022). Riemannian diffusion Schro¨dinger bridge. arXiv preprint arXiv:2207.03024.
Tong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G., Wolf, G.,
and Bengio, Y. (2023). Simulation-free Schr¨o”dinger bridges via score and flow
matching. arXiv preprint arXiv:2307.03672.
Vargas, F., Ovsianas, A., Fernandes, D., Girolami, M., Lawrence, N. D., and Nu¨sken,
N. (2023). Bayesian learning via neural Schr¨odinger–F¨ollmer flows. Statistics and
Computing, 33(1):3.
Vargas, F., Thodoroff, P., Lamacraft, A., andLawrence, N.(2021). SolvingSchro¨dinger
bridges via maximum likelihood. Entropy, 23(9):1134.
Vempala, S. and Wibisono, A. (2019). Rapid convergence of the unadjusted langevin
algorithm: Isoperimetry suffices. Advances in neural information processing systems,
32.
28Villani, C. (2009). Optimal transport: old and new, volume 338. Springer.
Werenski, M., Murphy, J. M., and Aeron, S. (2023). Estimation of entropy-regularized
optimal transport maps between non-compactly supported measures. arXiv preprint
arXiv:2311.11934.
Wibisono, A., Wu, Y., and Yang, K. Y. (2024). Optimal score estimation via empirical
Bayes smoothing. arXiv preprint arXiv:2402.07747.
Yim, J., Trippe, B. L., De Bortoli, V., Mathieu, E., Doucet, A., Barzilay, R., and
Jaakkola, T. (2023). Se (3) diffusion model with application to protein backbone
generation. arXiv preprint arXiv:2302.02277.
A Dynamic entropic optimal transport
A.1 Connecting the two formulations
In this section, we reconcile (at a formal level) two versions of the dynamic formulation
for entropic optimal transport. We will start with (11) and show that this is equivalent
to (10) by a reparameterization.
We begin by recognizing that ∆p = ∇·(p ∇logp ), which allows us to write the
t t t
Fokker–Planck equation as
∂ p +∇·((v − ε∇logp )p ) = 0, (28)
t t t 2 t t
Inserting b := v − ε∇logp into (11), we expand the square and arrive at
t t 2 t
(cid:90) 1(cid:90) 1 ε2 ε
(cid:0) (cid:1)
inf ∥b (x)∥2 + ∥∇logp (x)∥2 + b⊤∇logp p (x)dxdt.
(pt,bt) 0 2 t 8 t 2 t t t
Up to the cross-term, this aligns with (10); it remains to eliminate the cross term.
Using integration-by-parts and (28), we obtain
(cid:90) 1(cid:90) (cid:90) 1(cid:90) (cid:90) 1(cid:90)
(b p )⊤∇logp dxdt = − ∇·(b p )logp dxdt = (∂ p )logp dxdt.
t t t t t t t t t
0 0 0
Though, we have (by product rule) the equivalence
∂ (p logp )−∂ p = (∂ p )logp .
t t t t t t t t
29Exchangingpartialderivativesundertheintegral,thisyieldsthefollowingsimplification
(cid:90) 1(cid:90) (cid:90) 1(cid:90) (cid:90) 1(cid:90)
(∂ p )logp dxdt = ∂ (p logp )dxdt− ∂ p dxdt
t t t t t t t t
0 0 0
(cid:90) 1 (cid:90) (cid:90) 1 (cid:90)
= ∂ p logp dxdt− ∂ p dxdt
t t t t t
0 0
(cid:90) 1
= ∂ H(p )dt+0
t t
0
= H(p )−H(p ),
1 0
where p = ν and p = µ. We see that (11) is equivalent to
1 0
ε (cid:90) 1(cid:90) (cid:16)1 ε2 (cid:17)
(H(ν)−H(µ))+ inf ∥b (x)∥2 + ∥∇logp (x)∥2 p (x)dxdt.
t t t
2 (pt,bt) 0 2 8
A.2 Connecting Markov processes and entropic Brenier maps
Here we prove Proposition 3.1. To continue, we require the following lemma.
Lemma A.1. Fix any t ∈ [0,1]. Under M, the random variables X and X are
0 1
conditionally independent given X .
t
Proof. A calculation shows that the joint density of X , X , and X with respect to
0 1 t
µ ⊗µ ⊗Lebesgue equals
0 1
Λ Λ
e− 2εt(1 1−t)∥xt−((1−t)x0+tx1)∥2 e(f(x0)+g(x1)−1 2∥x0−x1∥2)/ε
ε t(1−t)ε
= F (x ,x )G (x ,x ),
t t 0 t t 1
where
1
F (x ,x ) = Λ ef(x0)/εe− 2εt∥xt−x0∥2
t t 0 εt
1
G (x ,x ) = Λ
eg(x1)/εe− 2ε(1−t)∥xt−x1∥2
.
t t 1 (1−t)ε
Since this density factors, the law of X and X given X is a product measure, proving
0 1 t
the claim.
Proof of Proposition 3.1. First, we prove that M is Markov. Let (X ) be dis-
t t∈[0,1]
tributed according to M. It suffices to show that for any integrable a ∈ σ(X ),b ∈
[0,t]
σ(X ), we have the identity
[t,1]
E[ab|X ] = E[a|X ]E[b|X ] a.s.
t t t
30Using the tower property and the fact that, conditioned on X and X , the law of the
0 1
path is a Brownian bridge between X and X , and hence is Markov, we have
0 1
E [ab|X ] = E[E[ab|X ,X ,X ]|X ]
M t 0 t 1 t
= E[E[a|X ,X ]E[b|X ,X ]|X ].
0 t t 1 t
By Lemma A.1, the sigma-algebras σ(X ,X ) and σ(X ,X ) are conditionally inde-
0 t t 1
pendent given X , hence
t
E[E[a|X ,X ]E[b|X ,X ]|X ] = E[E[a|X ,X ]|X ]E[E[b|X ,X ]|X ] = E[a|X ]E[b|X ],
0 t t 1 t 0 t t 0 t t t t
as claimed.
The proof of the second statement follows directly from the computations presented
below (16), which hold under no additional assumptions.
We now prove the third statement. Following the approach of Fo¨llmer (1985), the
representation of M as a mixture of Brownian bridges shows that the law of X for
√ [0,t]
any t < 1 has finite entropy with respect to the law of X + εB , for X ∼ µ . Hence,
0 t 0
to verify the representation in terms of the SDE, it suffices to compute the stochastic
derivative:
1
lim E[X −X |X ],
t+h t [0,t]
h→0 h
where the limit is taken in L2. Using the the fact that the process is Markov and,
conditioned on X and X , the path is a Brownian bridge, we obtain
0 1
1 1
lim E[X −X |X ] = lim E[E[X −X |X ,X ,X ]|X ]
t+h t [0,t] t+h t 0 t 1 t
h→0 h h→0 h
1
= E[X −X |X ].
1 t t
1−t
Recalling the computations in Lemma A.1, we observe that, conditioned on X = x ,
t t
the variable X has µ density proportional to G (x ,x ). Since π is a probability
1 1 t t 1
measure, inparticularwehavethateg liesinL1(µ ). Wecanthereforeapplydominated
1
convergence to obtain
(cid:82)
1 x1−xtG (x ,x )µ (dx )
E[X −X |X = x ] = 1−t t t 1 1 1 = ε∇logH [exp(g/ε)µ ](x ),
1 t t t (cid:82) (1−t)ε 1 t
1−t G (x ,x )µ (dx )
t t 1 1 1
as desired.
For the fourth statement, we require the following claim.
Claim: The joint probability measure π (z,x ), defined as
t 1
exp((−(1−t)f (z)+(1−t)g(x )− 1∥z −x ∥2))/((1−t)ε))m (dz)µ (dx ),
1−t 1 2 1 t 1 1
is the optimal entropic coupling from m to ρ with regularization parameter (1−t)ε,
t
where f (z) := εlogH [eg/εµ ](z). Under this claim, it is easy to verify that
1−t (1−t)ε 1
31the definition of ∇φ is precisely this conditional expectation, which concludes the
1−t
proof.
To prove the claim, we notice that π is already in the correct form of an optimal
t
entropic coupling, and π ∈ Γ(m ,?) by construction. Thus, it suffices to only check
t t
the second marginal. By the second part, above, we have that
m (z) = H [exp(g/ε)µ ](z)H [exp(f/ε)µ ](z).
t (1−t)ε 1 tε 0
Integrating, performing the appropriate cancellations, and applying the semigroup
property, we have
(cid:90)
π (z,dx )dz = eg(x1)/εµ (dx )H [H [ef/εµ ]](x )
t 1 1 1 (1−t)ε tε 0 1
= eg(x1)/εµ (dx )H [ef/εµ ](x ),
1 1 ε 0 1
which proves the claim.
B Proofs for Section 4
B.1 One-sample analysis
˜ ¯
Proof of Proposition 4.2. First, we recognize that a path with law P (resp. P) can
be obtained by sampling a Brownian bridge between (X ,X ) ∼ π (resp. π¯ ), by
0 1 n n
Proposition 3.1. Thus, by the data processing inequality,
E[KL(P˜ ∥P¯
)] ≤
E[KL(P˜ ∥P¯
)]
[0,τ] [0,τ]
≤ E[KL(π ∥π¯ )]
n n
(cid:90)
(cid:104) (cid:105)
= E log(π /π¯ )dπ ,
n n n
where the above manipulations are valid as both π and π¯ have densities with respect
n n
to µ⊗ν . Completing the expansion by explicitly writing out the densities, we obtain
n
(cid:90)
1 (cid:104) (cid:105)
E[KL(P˜ ∥P¯ )] ≤ E (fˆ +gˆ−f¯ −g⋆)dπ
[0,τ] [0,τ] n
ε
(cid:90) (cid:90)
1
= E[OT (µ,ν )− f¯ dµ− g⋆dν ].
ε n n
ε
¯
We now employ the rounding trick of Stromme (2023b): the rounded potential f
satisfies
f¯ = argmaxΦµνn(f,g⋆);
f∈L1(µ)
32Therefore, in particular, Φµνn(f¯ ,g⋆) ≥ Φµνn(f⋆,g⋆). Continuing from above, we obtain
(cid:90) (cid:90)
1
E[KL(P˜ ∥P¯ )] ≤ E[OT (µ,ν )− f⋆dµ− g⋆dν ]
[0,τ] [0,τ] ε n n
ε
(cid:90) (cid:90)
1
= E[OT (µ,ν )− f⋆dµ− g⋆dν]
ε n
ε
1
= E[OT (µ,ν )−OT (µ,ν)],
ε n ε
ε
where in the penultimate equality we observed that g is independent of the data
Y ,...,Y . Combined with Theorem 2.6 of Groppe and Hundrieser (2023), the proof
1 n
is complete.
Proof of Proposition 4.3. We start by applying Girsanov’s theorem to obtain a differ-
ence in the drifts, which can be re-written as differences in entropic Brenier maps:
(cid:90) τ
E[KL(P⋆ ∥P¯ )] ≤ E∥¯ b −b⋆∥2 dt
[0,τ] [0,τ] t t L2(pt)
0 (29)
(cid:90) τ
= (1−t)−2E∥∇φ¯ −∇φ⋆ ∥2 dt.
1−t 1−t L2(pt)
0
The result then follows from Lemma B.1, where we lazily bound the resulting integral:
R2ε−k (cid:90) τ
E[KL(P⋆ ∥P¯ )] ≤ (1−t)−k−2dt
[0,τ] [0,τ] n
0
R2ε−k
≤ (1−τ)−k−2.
n
Lemma B.1 (Point-wise drift bound). Under the assumptions of Proposition 4.3, let
φ¯ be the entropic Brenier map between p¯ and ν¯ and φ⋆ be the entropic Brenier
1−t t n 1−t
map between p⋆ and ν, both with regularization parameter (1−t)ε. Then
t
R2
E∥∇φ¯ −∇φ⋆ ∥2 ≲ ((1−t)ε)−k.
1−t 1−t L2(pt) n
Proof. Setting some notation, we express ∇φ⋆ as the conditional expectation of the
1−t
optimal entropic coupling π⋆ between p⋆ and ν (recall Proposition 3.1), where we
t t
write π⋆(z,y) = γ⋆(z,y)p⋆(dz)ν(dy).
t t t
The rest of our proof follows a technique due to Stromme (2023b): by triangle
inequality, we can add and subtract the following term
n
1 (cid:88)
Y γ⋆(z,Y ),
n j t j
j=1
33into the integrand in (29), resulting in
E∥∇φ¯ −∇φ⋆ ∥2 ≲ E∥∇φ¯ −n−1(cid:80)n Y γ⋆(·,Y )∥2
1−t 1−t L2(p⋆) 1−t j=1 j t j L2(p⋆)
t t (30)
+E∥n−1(cid:80)n Y γ⋆(·,Y )−∇φ⋆ ∥2 .
j=1 j t j 1−t L2(p⋆)
t
For the second term, with the same manipulations as Stromme (2023b, Lemma 20),
we obtain a final bound of
R2 R2
E∥n−1(cid:80)n Y γ⋆(·,Y )−∇φ⋆ ∥2 = ∥γ⋆∥2 ≤ ((1−t)ε)−k,
j=1 j t j 1−t L2(p⋆ t) n t L2(p⋆ t⊗ν) n
where the final inequality is also due to Stromme (2023b, Lemma 16). To control the
first term in (30), we also appeal to his calculations of the same theorem: observing
that, from (26)
1 (cid:88)n exp((g⋆(Y j)− 2(11 −t)∥z −Y j∥2)/ε)
∇φ¯ (z) = Y
1−t n j 1 (cid:80)n exp((g⋆(Y )− 1 ∥z −Y ∥2)/ε)
j=1 n j=1 j 2(1−t) j
n
1 (cid:88)
= Y γ¯ (z,Y ).
j t j
n
j=1
Since the following equality is true
γ⋆(z,Y )
γ¯ (z,Y ) = t j ,
t j 1 (cid:80)n γ⋆(z,Y )
n k=1 t k
we can verbatim apply the remaining arguments of Stromme (2023b, Lemma 20).
Indeed, for fixed x ∈ Rd, we have
∥n−1(cid:80)n Y (γ⋆(x,Y )−γ¯ (x,Y ))∥2 ≤ R2(cid:12) (cid:12)(cid:80)n γ⋆(x,Y )−1(cid:12) (cid:12)2 .
j=1 j t j t j j=1 t j
Taking the L2(p⋆) norm and the outer expectation, we see that the remaining term is
t
nothing but the first component of the gradient of the dual entropic objective function
(see Lemma C.3), which can be bounded via Lemma C.4, resulting in the chain of
inequalities
R2 R2
E∥n−1(cid:80)n Y (γ⋆(·,Y )−γ¯ (·,Y ))∥2 ≲ ∥γ⋆∥2 ≤ ((1−t)ε)−k,
j=1 j t j t j L2(p⋆ t) n t L2(p⋆ t⊗ν) n
where the last inequality again holds via Stromme (2023b, Lemma 16).
34B.2 Completing the results
Proof of Proposition 4.4. This proof closely follows the ideas of Chen et al. (2022).
Applying Girsanov’s theorem, we obtain
N−1(cid:90) (k+1)η
(cid:88)
TV2(Pˆ ,P˜ ) ≲ KL(P˜ ∥Pˆ ) = E ∥ˆ b (X )−ˆ b (X )∥2dt.
[0,τ] [0,τ] [0,τ] [0,τ] P˜ kη kη t t
[0,τ]
kη
k=0
Recall that η ∈ (0,1) is a chosen step-size based on N, the number of steps to be
taken. As in prior analyses, we hope to uniformly bound the integrand above for any
t ∈ [kη,(k +1)η]. Adding and subtracting the appropriate terms, we have
E ∥ˆ b (X )−ˆ b (X )∥2 ≲ E ∥ˆ b (X )−ˆ b (X )∥2
P˜ kη kη t t P˜ kη kη t kη
[0,τ] [0,τ] (31)
+E ∥ˆ b (X )−ˆ b (X )∥2.
P˜ t kη t t
[0,τ]
By the semigroup property, we first notice that
H [egˆ/εν ] = H [H [egˆ/εν ]].
1−kη n t−kη 1−t n
We can verbatim apply Lemma 16 of Chen et al. (2022) with q := H [egˆ/εν ],
1−t n
M = id and M = (t−kη)I, since H [egˆ/εν ] = q ∗N(0,(t−kη)I). This gives
0 1 1−kη n
(cid:13) q ∗N(0,(t−kη)I) (cid:13)2
∥ˆ b (X )−ˆ b (X )∥2 = (cid:13)ε∇log (X )(cid:13)
kη kη t kη (cid:13) q kη (cid:13)
≲ L2ηd+L2η2∥ε∇logq(X )∥2.
t t kh
Since εlogq is L -smooth, we obtain the bounds
t
E ∥ε∇logq(X )∥2 ≲ E ∥ε∇logq(X )∥2 +L2∥X −X ∥2
P˜ kh P˜ t t t kh
[0,τ] [0,τ]
≤ εL d+L2E ∥X −X ∥2.
t t P˜ t kh
[0,τ]
where the final inequality is a standard smoothness inequality (see Lemma C.2).
Similarly, the second term on the right-hand side of (31) can be bounded by
E ∥ˆ b (X )−ˆ b (X )∥2 ≤ L2E ∥X −X ∥2.
P˜ t kη t t t P˜ kη t
[0,τ] [0,τ]
Combining the terms, we obtain
E ∥ˆ b (X )−ˆ b (X )∥2 ≲ εL2ηd+L2E ∥X −X ∥2,
P˜ kη kη t t t t P˜ kη t
[0,τ] [0,τ]
where, to simplify, we use the fact that η ≤ 1/L (with L ≥ 1), and that η2 ≤ η for
t t
˜
η ∈ [0,1]. We now bound the remaining expectation. Under P , we can write
[0,τ]
(cid:90) t √ (cid:90) kη √
ˆ ˆ
X = b (X )ds+ εB ,X = b (X )ds+ εB ,
t s s t kh s s kη
0 0
35and thus
(cid:90) t √
ˆ
X −X = b (X )ds+ ε(B −B ).
t kη s s t kη
kη
Taking squared expectations, writing δ := t−kη ≤ η (recall that t ∈ [kη,(k +1)η)),
we obtain (through an application of the triangle inequality and Jensen’s inequality)
(cid:90) t
E ∥X −X ∥2 ≲ εE ∥B −B ∥2 +δ E ∥ˆ b (X )∥2ds
P˜ t kη P˜ t kη P˜ s s
[0,τ] [0,τ] [0,τ]
kη
≲ εηd+δ2L d
t
≤ (ε+1)ηd
where we again used Lemma C.2. Combining all like terms, we obtain the final result.
The estimates for the Lipschitz constant follow from Lemma C.1.
B.3 Proofs for Section 4.3
B.3.1 Computing Equation 27
The F¨ollmer drift is a special case of the Schr¨odinger bridge, where µ = δ for any
a
a ∈ Rd. Let (fF,gF) denote the optimal entropic potentials in this setting. Note that
they these potentials are defined up to translation (i.e., the solution is the same if
we take fF +c and gF −c for any c ∈ R). So, we further impose the condition that
fF(a) = 0 = c. Then the optimality conditions yield
1
gF(y) = ∥y∥2. (32)
2ε
Plugging this into the expression for the Schro¨dinger bridge drift, we obtain
1
bF(z) = ε∇logH [e2ε∥·∥2 ν](z)
t (1−t)ε
1 1
(cid:82) ∥y∥2− ∥z−y∥2
(cid:16) ye2ε 2(1−t)ε ν(dy)(cid:17)
= (1−t)−1 −z + .
1 1
(cid:82) ∥y∥2− ∥z−y∥2
e2ε 2(1−t)ε ν(dy)
Replacing the integrals with respect to ν with their empirical counterparts yields the
estimator.
B.3.2 Proof of Proposition 4.6
Our goal is to prove the following lemma.
Lemma B.2. Let p be the F¨ollmer bridge at time τ ∈ [0,1) between µ = δ and
τ 0
ν ∈ P (Rd) with ε = 1 and suppose the squared second moment of ν is bounded above
2
by d. Then
W2(p ,ν) ≤ d(1−τ).
2 τ
36Proof. Note that p = P , where P is the reverse bridge, which starts at ν and
τ 1−τ 1−τ
ends at µ = δ . This reverse bridge is well known to satisfy a simple SDE F¨ollmer
0
(1985): the measure P is the law of Y , where Y solves
1−τ 1−τ s
Y
s
dY = − ds+dB , Y ∼ ν,
s s 0
1−s
which has the explicit solution
(cid:90) s 1
Y = (1−s)Y +(1−s) dB .
s 0 r
1−r
0
In particular, we obtain
W2(P ,ν) ≤ E∥Y −Y ∥2
2 s s 0
=
E(cid:13) (cid:13)
(cid:13)−sY
+(1−s)(cid:90) s 1
dB
(cid:13) (cid:13) (cid:13)2
0 r
(cid:13) 1−r (cid:13)
0
= s2E∥Y ∥2 +ds(1−s)
0
≤ ds,
which proves the claim.
C Technical lemmas
Lemma C.1 (Hessian calculation and bounds). Let (p ,b ) be the optimal density-drift
t t
pair satisfying the Fokker–Planck equation (11) between µ and µ . For t ∈ [0,1), b
0 1 t
is Lipschitz with constant L given by
t
1 (cid:16) (cid:17)
L := sup∥∇b (x)∥ ≤ 1∨∥∇2φ (x)∥ ,
t t op 1−t op
(1−t)
x
where ∇φ is the entropic Brenier map between p and µ with regularization pa-
1−t t 1
rameter (1−t)ε. Moreover, if the support of µ is contained in B(0,R), then
1
L ≤ (1−t)−1(1∨R2((1−t)ε)−1). (33)
t
Proof. Taking the Jacobian of b , we arrive at
t
∇b (x) = (1−t)−1(∇2φ (x)−I),
t 1−t
As entropic Brenier potentials are convex (recall that their Hessians are covariance
matrices; see (8)), we have the bounds
−(1−t)−1I ⪯ ∇b (x) ⪯ (1−t)−1∇2φ (x).
t 1−t
37The first claim follows by considering the larger of the two operator norms of both
sides.
The second claim follows from the fact that since φ is an optimal entropic
1−t
Brenier potential, its Hessian is the conditional covariance of an optimal entropic
coupling π ∈ Γ(p ,µ ), so
t t 1
1 R2
∥∇2φ (z)∥ = ∥Cov [Y|X = z]∥ ≤ ,
1−t op
(1−t)ε
πt t op
(1−t)ε
since supp(µ ) ⊆ B(0,R).
1
Lemma C.2. Let (p ,b ) be the optimal density-drift pair satisfying the Fokker–Planck
t t
equation (11) between µ and µ . Then for any t ∈ [0,1)
0 1
ε
E ∥b ∥2 ≤ L d.
pt t
2
t
Proof. This proof follows the ideas of Vempala and Wibisono (2019, Lemma 9). We
note that the generator given by the forward Schr¨odinger bridge with volatility ε is
ε
L f = ∆f −⟨b ,∇f⟩,
t t
2
for a smooth function f. Writing b = ∇(εlogH [eg/εµ ]), we obtain
t 1−t 1
ε ε
0 = E L (εlogH [eg/εµ ]) =⇒ E ∥b (X )∥2 = E [∇·b ] ≤ L d.
pt t 1−t 1 pt t t
2
pt t
2
t
Lemma C.3. (Stromme, 2023b, Proposition 3.1) Let P,Q be probability measures
on Rd. For every pair h = (f ,g ) ∈ L∞(P) × L∞(Q), there exists an element of
1 1 1
L∞(P)×L∞(Q) which we denote by ∇ΦPQ(f ,g ) such that for all h = (f ,g ) ∈
ε 1 1 0 0 0
L∞(P)×L∞(Q),
(cid:90) (cid:90)
(cid:16) (cid:17)
⟨∇ΦPQ(h ),h ⟩ = f (x) 1− e−ε−1(c(x,y)−f1(x)−g1(y))dQ(y) dP(x)
ε 1 0 L2(P)×L2(Q) 0
(cid:90) (cid:90)
(cid:16) (cid:17)
+ g (y) 1− e−ε−1(c(x,y)−f1(x)−g1(y))dP(x) dQ(x).
0
In other words, the gradient of ΦPQ at (f ,g ) is the marginal error corresponding to
ε 1 1
(f ,g ).
1 1
Lemma C.4. Following Lemma C.3, suppose P = µ and Q = ν , where ν is the
n n
empirical measure of some measure ν on the basis of n i.i.d. samples. Let (f,g) be
the optimal entropic potentials between µ and ν, which induce an optimal entropic
coupling π (recall (6)). Then
∥γ∥2
E∥∇Φµνn(f,g)∥2 ≲ L2(µ⊗ν) ,
L2(µ)×L2(νn) n
where the expectation is with respect to the data, and γ = dπ .
d(µ⊗ν)
38Proof. Writing out the squared-norm of the gradient explicitly in the norm L2(µ)×
L2(ν ), we obtain
n
(cid:90) n
(cid:16)1 (cid:88) (cid:17)2
E∥∇Φµνn(f,g)∥2 = E γ(x,Y )−1 µ(dx)
L2(µ)×L2(νn) n j
j=1
n (cid:90)
1 (cid:88)(cid:16) (cid:17)2
+E γ(x,Y )µ(dx)−1 .
j
n
j=1
(cid:82)
Note that by the optimality conditions, γ(x,Y )µ(dx) = 1 for all Y . Thus, writing
j j
Z := γ(x,Y ) which are i.i.d., we see that
j j
(cid:90) n (cid:90) n
(cid:16)1 (cid:88) (cid:17)2 (cid:16)1 (cid:88) (cid:17)2
E γ(x,Y )−1 µ(dx) = E (Z −E[Z ])
j j j
n n
j=1 j=1
n
(cid:16)1 (cid:88) (cid:17)
= Var Z
µ⊗ν j
n
j=1
1
= Var (Z ).
µ⊗ν 1
n
The remaining component of the squared gradient vanishes, and we obtain
1
∥γ∥2
E∥∇Φµνn(f,g)∥2 = Var (γ) ≤ L2(µ⊗ν) .
L2(µ)×L2(νn) n µ⊗ν n
39