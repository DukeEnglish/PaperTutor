1
Learning Flock: Enhancing Sets of Particles for
Multi Sub-State Particle Filtering with Neural
Augmentation
Itai Nuri and Nir Shlezinger
Abstract—A leading family of algorithms for state estimation knowledgeoftherelatedmodels,thataffectstheupdateofthe
in dynamic systems with multiple sub-states is based on particle particles and may limit application in real-time systems [10].
filters (PFs). PFs often struggle when operating under complex
To tackle these challenges, PFs often employ, e.g., adaptation
or approximated modelling (necessitating many particles) with
ofthenumberofparticles[11],[12],robustdesigns[13],[14],
lowlatencyrequirements(limitingthenumberofparticles),asis
typically the case in multi target tracking (MTT). In this work, orintricatesamplingandresamplingrealizations[7],[8],[15]–
we introduce a deep neural network (DNN) augmentation for [17], typically at the cost of increased complexity and latency
PFs termed learning flock (LF). LF learns to correct a particles- and/or reduced performance.
weights set, which we coin flock, based on the relationships
Over the last decade, deep neural networks (DNNs) have
between all sub-particles in the set itself, while disregarding the
emerged as powerful data-driven tools, that allow learning
setacquisitionprocedure.OurproposedLF,whichcanbereadily
incorporatedintodifferentPFsflow,isdesignedtofacilitaterapid complex and abstract mappings from data [18]. The dramatic
operation by maintaining accuracy with a reduced number of success of deep learning in domains such as computer vision
particles. We introduce a dedicated training algorithm, allowing and natural language processing has also lead to a growing
bothsupervisedandunsupervisedtraining,andyieldingamodule
interestinitscombinationwithclassicsignalprocessingalgo-
that supports a varying number of sub-states and particles
rithms via model-based deep learning [19]–[21]. For tracking
without necessitating re-training. We experimentally show the
improvements in performance, robustness, and latency of LF in dynamic systems, DNN-aided implementations of Kalman-
augmentation for radar multi-target tracking, as well its ability typefilters,thataretypicallysuitablefortrackingasinglestate
tomitigatetheeffectofamismatchedobservationmodelling.We in a Gaussian dynamic system, were studied in [22]–[27].
alsocompareandillustratetheadvantagesofLFoverastate-of-
In the context of PFs, various approaches were proposed
the-art DNN-aided PF, and demonstrate that LF enhances both
in the literature to incorporate deep learning [28]–[35]. In
classic PFs as well as DNN-based filters.
the last few years, arguably the most common approach uses
DNNs to learn the sampling distribution, from which the
I. INTRODUCTION particles are then individually sampled [28]–[34]. This can
be done via supervised [28]–[30] and unsupervised [31]–[34]
The tracking of a time-evolving hidden state from noisy
learning. The latter is typically done by imposing a specific
measurementsisafundamentalsignalprocessingtask.Particle
distribution model on the signals [31]–[33], thus inducing
filters (PFs) are a family of algorithms that allow tracking in
limitationswhenthisassumptionisviolated.Alternatively,one
(possibly) non-linear and non-Gaussian dynamic systems [2].
can learn by mimicking each particle of another PF with the
PFsnaturallysupporttrackingasinglestate,aswellasastate
same number of particles that is considered accurate [34],
comprised of multiple sub-states, i.e., multi target tracking
and thus be bounded by its reference performance, which
(MTT). Such filters are widely popular, and are utilized in
is often restrictive when operating with a limited number of
many areas, ranging from robotics [3], through communica-
particles. In multi sub-state settings, DNNs were used mainly
tion [4], and to positioning [5] and radar tracking [6]–[8].
aspreprocessing[35],lesssoasanintegralpartofthePFflow.
PFs employ Monte Carlo simulation to iteratively update
Whiletheaboveexistingneuralaugmentationswereshownto
a set of samples coined particles, that represent sampling
enhance PFs, their design is typically tailored to a specific
points of a continuous probability density function (PDF),
task, limiting transferability to other filters. Moreover, they
andtheircorrespondingweights,whichrepresenttheirrelative
operate in a per-particle manner, thus do not fully leverage
correctness [9, Ch. 12]. Their update is done according to a
the relationship between particles, e.g. in order to disperse
sequence of observations and the statistical modelling of the
clusters of particles or to align outliers. This motivates the
state evolution and the measurements, via different sampling-
design of a generic learning-aided improvement to PFs that
based stochastic procedures. While this operation enables the
induces a flexible collective distribution between all particles,
tracking of complex distributions, it also gives rise to (i)
bycompactlyutilizingavailablemodelsknowledgeintheform
increasedcomplexityduetotheneedtomaintainalargenum-
of the pertinent PF, and by leveraging data to exploit shared
ber of particles; and (ii) sensitivity to lacking or mismatched
information between particles.
Partsofthisworkwereacceptedforpresentationinthe2024IEEESignal In this work we present a novel approach of augmenting
ProcessingAdvancesinWirelessCommunications(SPAWC)asthepaper[1]. multi sub-state PFs with DNN, coined learning flock (LF).
TheworkwassupportedbytheIsraelInnovationAuthority.Theauthorsare
Our algorithm is based on the insight that the core challenges
withtheSchoolofECE,Ben-GurionUniversityoftheNegev,Israel(email:
itai5n@gmail.com;nirshl@bgu.ac.il). of PFs with a limited number of particles can be tackled
4202
guA
12
]PS.ssee[
1v84311.8042:viXra2
by learning from data to jointly correct particles and their in Subsection II-B. Then, on Subsection II-C we review PFs
weights, that are otherwise independent, at specific points in general, and lay out the key principles that we base upon
in the PF flow, so that the particles and the weights at the our LF algorithm, detailed in Section III.
end of each PF iteration better reflect the state probability
distribution. Accordingly, our proposed neural augmentation
A. Signal Model
acts as a correction term to the particles and their weights, by
We consider a dynamic system formulated as a continuous-
adjusting the set collectively. The resulting augmentation is
valued state-space model in discrete-time. The state vector
designed to be readily transferable, such that the LF module
canbeinfactintegratedintodifferentPFalgorithms,andeven
at time k, denoted xk ∈ Rdp, describes the states of t
dynamic sub-states. Accordingly, xk is comprised of t sub-
combined in a complementary fashion with alternative DNN-
vectors {xk}t of size d ×1, with d =t·d , such that
aided PFs. j j=1 sp p sp
In particular, we design a DNN architecture for jointly xk =(cid:2) xk 1,xk 2,..,xk j,..,xk t(cid:3) . For instance, different sub-states
processing varying number of particles with varying number can represent different targets in MTT [40].
of sub-states. We identify a core permutation equivariance of Each sub-state vector evolves in time independently of
particle-weightpairs,recognizingthattheinduceddistribution the other sub-states, obeying a first-order Markov process.
is invariant to their ordering. Accordingly, we design our LF Specifically, we write the jth sub-state vector as x0 j:k =
DNNarchitecturetoemploydedicatedembeddingmodulesto {x0 j,...,xk j}, and assume that it obeys a motion model such
handle the invariance of sub-state indexing, and incorporate that its conditional PDF satisfies
compact, trainable self-attention modules [36] to account for p(cid:0) xk|x0:k−1(cid:1) =p(cid:0) xk|xk−1(cid:1)
, ∀j ∈{1,...,t}. (1)
the fact that the filter is invariant to the particles’ order. j j j j
We introduce an algorithm that boosts transferability by Accordingly,bywritingx0:k ={x0,...,xk},theoverallstate
training the LF module as a form of generative learning [37]. evolution PDF satisfies
This is achieved using a dedicated loss function that evaluates
p(cid:0) xk|x0:k−1(cid:1) =p(cid:0) xk|xk−1(cid:1)
, (2)
a set of particles and weights, in (i) state recovery; and
(ii) the similarity of the particles spread to a desired target
where
p(cid:0) xk|xk−1(cid:1) =(cid:81)t p(cid:0) xk|xk−1(cid:1)
.
pattern.Weproposebothsupervisedandunsupervisedlearning j=1 j j
Ateachtime-stepk,thestateisobservedvianoisy,possibly
schemes. The latter is realized while boosting operation with non-linearmeasurements,denotedzk ∈Rdm.Therelationship
a limited number of particles by utilizing an accurate teacher
between the observations, i.e., the sensory data, and the state
PF, possibly with a large number of particles, as a form of
are reflected in the sensor model PDF
knowledge distillation [38]. Instead of distilling by aiming to
mimic the particles and their weights, our loss evaluates the
p(cid:0) zk|x0:k(cid:1) =p(cid:0) zk|xk(cid:1)
. (3)
similarityintheparticles’patternsinducedbythethetwoPFs,
We particularly focus in settings where the observation model
boosting exploitation of cross-particle relationships without
is invariant to the indexing of the sub-states. Mathematically,
requiring a large number of particles.
this indicates that the observation model in (3) is invariant
WenumericallyexemplifythegainsofourproposedLFfor to replacing xk = (cid:2) xk,...,xk(cid:3) with (cid:2) xk ,...,xk(cid:3) for any
augmentingPFsindifferenttasks.Weshowthatitcanenhance 1 t j1 jt
permutation j ,...,j of 1,...,t.
different PFs for different synthetic data distributions. More- 1 t
over,weshowthatitcannotonlyoutperformalternativeDNN-
aidedPFs,butcanalsobeintegratedandenhancetheoperation B. Problem Formulation
of such algorithms. We also evaluate the LF for radar MTT,
Our objective is to design a system for recovering the state
augmenting auxiliary PFs [39] in single and multiple radar
vector from all available data. Accordingly, for each time
targettrackingsettings[7].Weshowsystematicimprovements instance k, we are interested in providing an estimate of xk
whenaugmentingourLFmoduleinperformance,latency,and basedonallpastandcurrentmeasurements,i.e.,z1:k.Tofocus
in overcoming modelling mismatches.
on tracking tasks, we assume that the initial state is known.
The rest of this paper is organized as follows: Section II
We particularly consider multi sub-state tracking subject to
reviews the system model and recalls PF basics; Section III
the following requirements:
derives the LF augmented PF (LF-PF), introducing its ratio-
R1 The state evolution PDF (2) and measurement model (3)
nale, architecture and training algorithm. Section IV presents
can be non-Gaussian.
anexperimentalstudy.Finally,SectionVconcludesthepaper.
R2 The measurement model (3) is given yet may be mis-
Throughout the paper, we use boldface letters for vectors,
matched.
e.g., x. Upper-cased boldface letters denote matrices,e.g., X.
R3 The system must operate in real-time with low latency.
Calligraphic letters, such as X, are used for sets, with |X|
R4 Thenumberofsub-statestcantakedifferentvalues.Yet,
being the cardinality of X.
for any given set of observations, we assume that it is
known.
II. SYSTEMMODELANDPRELIMINARIES
To cope with the above challenges, we consider settings
Thissectionreviewsthesystemmodelalongwithnecessary
wherewehaveaccesstodatafordesignpurposes.Weconsider
preliminaries. We first present the signal model in Subsec-
two possible settings:
tion II-A, based on which we formulate the tracking problem3
S1 Unsupervised settings - here the data is comprised of of a decomposable form [2], such that q(x0:k|z1:k) =
a set of n series of measurements along κ steps long q(xk|x0:k−1,z1:k)q(x0:k−1|z1:k−1). This choice of q(·) and
t
trajectories, denoted D
=(cid:8)(cid:0) z1:κ(cid:1)(cid:9)
,|D|=n . π(·) leads to a generic weight update rule (Step 4) in which
t
S2 Supervisedsettings-herewealsohaveaccesstothetrue
π(zk|xk)π(xk|xk−1)
states corresponding to the measurements as well as the wk ∝ i i i wk−1. (9)
measurements, i.e., D
=(cid:8)(cid:0) z1:κ,x¯1:κ(cid:1)(cid:9)
,|D|=n t.
i q(xk i|x0 i:k−1,z1:k) i
C. Particle Filters Algorithm 1: A generic PF iteration on time k
PFsare apopular familyof algorithmssuitable fortracking Input: Particles-weights set {xk−1,wk−1}N ,
i i i=1
in non-Gaussian dynamics [2], [41] (thus meeting R1). These measurements zk
Monte-Carlo algorithms [9] track a representation of the 1 for i=1,...,N do
posterior p(xk|z1:k), by approximating its Bayesian recursive 2 Sample updated particles: xk i ∼q(cid:0) xk|·(cid:1) ;
formulation which holds under the Markovian model in (1)
3 for i=1,...,N do
and (2). Specifically, by Bayes’ law it holds that [42] 4 Update weights: w ik ←π(cid:0) xk i|·(cid:1) /q(cid:0) xk i|·(cid:1) ;
p(zk|xk)p(xk|z1:k−1)
p(xk|z1:k)= . (4) 5 for i=1,...,N do
p(zk|z1:k−1) 6 Normalize: w ik ←w ik/(cid:80)N j=1w jk ;
Under model assumptions (1)-(2), the posterior at time k in
Output: Particles-weights set {xk,wk}N
(4) can be related to the posterior at time k − 1 based on i i i=1
Chapman–Kolmogorov equation, i.e.,
For increasedaccuracy, PFsrequire moreparticles, andthis
(cid:90)
requirementgrowsdramaticallywiththestatedimension[44].
p(xk|z1:k−1)= p(xk|xk−1)p(xk−1|z1:k−1)dxk−1 (5)
TheNumberofparticlesgreatlyaffectsthecomplexityofPFs,
performancehoweverisnotnecessarilydictatedbythenumber
and on
(cid:90) of particles N, and is typically influenced by the effective
p(zk|z1:k−1)= p(zk|xk)p(xk|z1:k−1)dxk, (6) number of particles, defined as [43]
N
turning (4) into a recursive update rule.
N = . (10)
Directrecursiveupdatingoftheposteriorbasedon(4)-(6)is eff 1+Var(cid:0) {wk}N (cid:1)
i i=1
oftenchallengingorinfeasibletocompute.PFsapproximateit
Specifically, small N indicates a phenomenon referred
usingasetofN particles{xk i}N i=1 andtheirweights{w ik}N i=1. to as particles degenerae cff y [2], [42], where the posterior is
Eachparticlerepresentsahypothesisonthesystem’sstate,and poorly estimated due to being dominated by a small portion
theweightsindicatetheirtrajectory’srelativeaccuracy.Asthe of the overall particles. Having a large N is typically a
eff
statevectorrepresentstsub-states,eachparticlexk i consistsof desired property of PFs. Nonetheless, on its own it does not
t sub-particles {xk j,i}t j=1 that share the same weight w ik. The ensure a good representation of the posterior, as one may
posterior PDF can be estimated as a weighted sum of kernel still encounter sample impoverishment [2], [42] (or particle
functions{K j,i(·)}t j, ,N i=1 evaluatedatthesub-particles[42]via collapse),wheremanyparticlesconvergetothesamelocation.
Applying particles resampling [2], [15], [16] when N goes
t N eff
pˆ(cid:0) xk|z1:k(cid:1) = (cid:89)(cid:88) w ikK j,i(cid:0) xk
j
−xk j,i(cid:1) . (7) below a threshold N th tackles both phenomena to a degree.
PFs rely on stochastic procedures, and are prone to produc-
j=1i=1
ing outliers or deformations, particularly when the number of
By using an adequate number of particles, PFs can approxi-
particlesN issmall.However,thehandlingofalargenumber
mate any probability distribution. The state is then recovered
ofparticles,evenifadaptive[45],comeswithadditionalover-
as a weighted average of the particles, i.e.,
headandlimitrealtimeapplicationsimperilingR3.Moreover,
N (cid:34) N N (cid:35) the PDF induced by N → ∞ particles does not necessarily
(cid:88) (cid:88) (cid:88)
xˆk = w ikxk i = w ikxk 1,i,..., w ikxk t,i . (8) converge to the true PDF, and may depend on the sampling
i=1 i=1 i=1 distribution [46], [47]. PFs thus require knowledge and the
Based on the principle of the importance sampling [43], abilitytoapproximate(2)and(3).Wheneitherismismatched,
Algorithm 1 describes a generic PF iteration, which is aspointedinR2,performanceconsiderablydegrades,possibly
comprised of two main stages: (i) sampling current time- yielding an estimation bias that cannot be rectified solely
step particles {xk}N using the importance density [43] through increasing N.
q(cid:0) xk(cid:1) (Steps 1-2i ),i= a1 nd (ii) update the weights {wk}N Consequently, a main consideration when designing a PF
i i=1
according to the sampled density q(xk) and real distri- revolves around the choice of the importance sampling func-
bution π(xk) to represent the relative accuracy of each tion q(xk), with the number of particles tuned to reach a
trajectory with the new particles (Steps 3-4). By the desired performance, and different PFs incorporate various
Markovian nature of the signal, the real distribution satis- methods to that aim. Such approaches, combined with ded-
fies π(x0:k|z1:k) ∝ π(zk|xk)π(xk|xk−1)π(x0:k−1|z1:k−1). icated resampling procedures, are the basis of the regularized
The sampling distribution q(·) is typically chosen to be PF [48], the Auxiliary Parallel Partition PF (APP) [7], the4
target resampling APP [7], and the progressive proposal PF Algorithm 2: A generic LF-PF iteration on time k
[49], each with its own associated computational costs and Input: Particles-weights set {xk−1,wk−1}N ,
accumulatedlatency.TheproliferationofPFtechniques,com- i i i=1
measurements zk
bined with availability of data in S1-S2, motivate exploring
1 for i=1,...,N do
a PDc Fom rep pli rm ese en nt ta ar ty iond ;at oa n-a eid the ad t a cp anpr bo eac ih nteto grae tf efi dci ie nn tt oly anim yp gr ivo ev ne 2 Sample updated particles: x˘k i ∼q(cid:0) xk|·(cid:1) ;
PF algorithm, as studied in the following section. 3 for i=1,...,N do
(cid:16) (cid:17) (cid:16) (cid:17)
4 Update weights: w˘ ik ←π x˘k i|· /q x˘k i|· ;
III. LEARNINGFLOCKPF
5 LF update of the particles-weights set:
Here, we propose LF, designed to enhance a given PF to {xk,wk}N ←{x˘k,w˘k}N +f ({x˘k,w˘k}N );
i i i=1 i i i=1 θ i i i=1
meetR1-R4,providingasolutionthatcanbeappliedtoalmost 6 for i=1,...,N do
anyPF.Accordingly,inouralgorithm,forboththearchitecture 7 Normalize: w ik ←w ik/(cid:80)N j=1w jk ;
and training, we utilize only the most basic elements that are
Output: Particles-weights set {xk,wk}N
common to PFs, the particles and weights. The high-level de- i i i=1
scription of LFis explained in Subsection III-A.We elaborate
on the architecture of the LF module in Subsection III-B, and
go through our novel loss function and training procedures weights{x˘k,w˘k}N intoacorrectiontermf ({x˘k,w˘k}N ).
i i i=1 θ i i i=1
in Subsections III-C-III-D, respectively. We conclude with a The LF module design is based on the following consider-
discussion in Subsection III-E. ations: (i) every particle-weight correction should take into
account all other particles and weights; (ii) the particles and
A. High-Level Description weights pairs, and their update, is invariant to their ordering
andtotheinternalorderingofthesub-sates;(iii)theLFblock
We build on the understanding that the key factors limiting
should not induce considerable latency.
the ability of PFs to adequately represent a PDF when N is
Accordingly, we propose the architecture illustrated in
limited (R3), and in the presence of stochasticity and model
Fig. 1. The architecture consists of J flock-update blocks,
mismatches (R2), are encapsulated in the complete particles-
each combining two parallel sub-particle embedding net-
weights set. We recognize that PF algorithms typically handle
works, with permutation-invariant SA architecture. In the
eachfulltrajectoryx0:k separately[7],[42],[48]–[50].There-
i basic flock-update block, each state particle-weight pair
fore,eachparticleevolveswithonlyminimalconsiderationof
other particles through the relativity of their weights (Steps 5-
[x˘k i,w˘ ik] ∈ Rdp+1 is separated to t single sub-state sub-
6 of Algorithm 1). We thus propose to tackle the challenging
particles {[x˘k j,i,w˘ ik]}t
j=1
∈ Rdsp+1 (where the weight is
shared among all sub-particles). These are transformed into
factors mentioned in Subsection II-C in a manner that can be
two sub-particle embeddings of length P × 1, main and
integrated into existing PF by a per iteration application of a
secondary, by two separate embedding fully connected (FC)
correction term to each particle, which accounts for all parti-
networks,coinedEmb−Net1andEmb−Net2,respectively.
cles. This methodology can prevent PFs typical deterioration
ForeachoftheN particles,eachsub-particlemainembedding
of individual particles as time advances, aligning outliers and
isaddedtothemeanofthet−1othersub-particlessecondary
correctinginaccuratesampling,mitigatingparticledegeneracy
embeddingstogetitstfullembeddingsofthesamedimension
and sample impoverishment. Doing so leads to a substantial
P ×1. Note that this procedure is invariant to the order of
improvementofthecaptureofthestateprobabilitydistribution
the sub-particles, thus maintaining the desired permutation
along the trajectory, as we demonstrate in Section IV.
invariance. Moreover, the same two embedding networks are
In particular, we use deep learning tools as correction
usedforallsub-particles,andthusthearchitectureisinvariant
terms via neural augmentation [19], [21] in conjunction with
to the number of sub-states t, satisfying R4.
the pertinent PF algorithm, to modify a flock of particles
ThefullembeddingsarethencombinedusingdedicatedSA
by information sharing. An augmentation example of our
blocks (e.g., in the architecture illustrated in Fig. 1, two such
proposed LF into a generic PF is exemplified as Algorithm 2.
blocks are used). In these blocks, for each of the t sub-states,
There,f (·)representstheLFDNNwithtrainableparameters
θ
all N full sub-particles embeddings from all particles are
θ.ThisDNNistrainedtocombineinformationofallparticles
updated in the same SA layer (SA−Net) considering all N
in the flock in order to correct each individual particle and
fullembeddingsassociatedwiththesamesub-state.Thislayer
overcome stochastic predicaments, as well as the need to
is followed by an FC network (FC−Net) applied in parallel
accurately implement the importance sampling and weight
to each of the N ·t SA outputs, thus maintaining the particle
adjustments. We next detail on the architecture of this LF
permutation invariance of the SA layer. To get each particle
module and its training procedure.
update, at the output of the last FC layer, the t sub-states
updates are concatenated while the weights are averaged. For
B. Architecture intermediate filtering stages applications where each particle’s
The LF module, parameterized by θ, is incorporated into a sub-state is assigned with its own weight, the initial weight
given PF algorithm chosen for a specific task. As detailed in sharing and final weight averaging can be skipped.
Algorithm 2 (Step 5), it maps a complete set of particles and Inspiredbymulti-headattentionmechanisms[36],weapply5
Fig. 1: LF block architecture block diagram. A set of N particle-weight pairs {x˘k,w˘k}N is decomposed into N ×t sub-
i i i=1
particles {x˘k ,w˘k}t,N , and processed by J flock-update blocks in parallel. Each block contains two parallel embedding
j,i i j,i=1
networks in series with two self attention (SA) blocks and FC layers, outputting a correction term to each full-particle.
J flock-update blocks in parallel. The J outputs are summed MTT. Specifically, we employ the OSPA distance with order
upwiththeinitialparticle-weightpairstogetthefinalparticle. of 2 and infinite cutoff, calculated per time-step, i.e.,
On flock-update block J, J >1, prior to the last FC network,
κ
all embeddings for each sub-state are averaged, providing a Lacc(z1:κ,x¯1:κ;θ)=(cid:88) OSPA(cid:16) x¯k,xˆk(z1:k;θ)(cid:17)
. (12)
single embedding per sub-state, that is turned into a single
k=1
particle correction term at the final output of the block. That
singleparticletermactsasbaselinepersub-stateandisadded In (12), xˆk(z1:k;θ) represents the state estimation computed
to all particles correction terms (from all other flock-update via (8) and the corrected set of particles on time-step k.
blocks), shifting their entire induced PDF accordingly. Property (ii) is evaluated in Lhm(·), by comparing the
Wenotethatthearchitectureanditstrainableparameters,as distributioninducedbytheresultingparticleswithsomeoracle
well as the described particles and sub-particles processing, is distribution. In particular, this loss term is comprised of
invariant to the number of sub-states t and to their internal the ℓ 2 norms between the oracle true/desired posterior and
ordering (thus holding R4). They are also invariant to the its sub-states’ per dimension variances, denoted pˆ oracle(xk)
numberofparticle-weightpairsN andtotheirordering,while and {Vˆk }t , respectively, and the corresponding recon-
oracle|j j=1
enablingparalleloperation(thusfacilitatingR3).Accordingly, structed values obtained from the particles, corrected using
the same trained architecture can be applied in settings with the LF block. For the latter, the reconstructed PDF, denoted
differentnumbersofsub-statesandparticles,e.g.,anarchitec- pˆ θ(xk|z1:k), is computed via (7), while the variances term of
ture trained for MTT with a small number of targets (and/or sub-particles j denoted Vark j(z1:k;θ) is computed as the d sp
particles) can be readily applied to track a larger number of variances across the d sp dimensions of the N sub-particles of
targets (and/or particles), as we show in Section IV. sub-state j. The resulting loss term is given by
Lhm(z1:κ,x¯1:k;θ)=
C. Loss Function κ
(cid:88)(cid:16)
∥pˆ (xk|z1:k,x¯k)−pˆ (xk|z1:k)∥
Thetrainingoff (·)tunesθ,encouragesitscorrectionterm oracle θ 2
θ
k=1
tosatisfytwomainproperties:(i)togaugetheaccuracyofthe
t
state estimate (8); (ii) to align the PDF represented by a set +λ (cid:88) ∥Vˆk −Vark(z1:k;θ)∥ (cid:17) , (13)
3 oracle|j j 2
of particles, and approximated using (7), with a desired PDF.
j=1
We next formulate the proposed loss assuming supervised
settings (S2), after which we show how it can be specialized where λ 3 is a hyperparameter. The PDFs comparison is per-
to unsupervised scenarios (S1). formed over a set of points in the state-space, hereby referred
1) Loss Formulation: The loss used for training θ based to as the grid points, selected in one of two ways that are
on data D is comprised of two main parts, Lacc(·) and detailed in Subsection III-C3; and the variances comparison
Lhm(·),correspondingtoaccuracy(Property(i))andheatmap aims to align particles that stray too far out from the grid
representation (Property (ii)), respectively. The resulting loss points region to be accounted for.
is given by 2) Evaluating the Loss: Evaluating (11) requires, for each
time instance k, access to the true/desired state x¯k, the oracle
L D(θ)= |D1
|
(cid:88) κ1(cid:0) λ 1Lacc(z1:κ,x¯1:κ;θ) p oo bs tate inrio thr epˆ o or ra ac cle le(x pk o| sz te1 r:k io, rx¯ ak n) dan vd arv iaa nri ca en sc ,e ws e{V uˆ sok era tc hle e|j p} et j r= ti1 n. eT no
t
(z1:κ,x¯1:κ)∈D
+λ
Lhm(z1:κ,x¯1:k;θ)(cid:1)
, (11)
PFwithN˜ ≫N particles,withtheoraclevariancesattainedas
2 described for Vark(z1:k;θ) using the reference flow particles.
j
where λ and λ are non-negative hyperparameters balancing Obtaining the oracle posterior and the true state differs based
1 2
the contribution of each loss term. in the settings (S1 or S2):
Focusing on multiple sub-states tracking, for Property (i), O1 In the unsupervised settings S1, we use the reference PF
evaluatedinLacc(·),weemploytheoptimalsubpatternassign- withN˜ asaformofknowledgedistillation.Weutilizethe
ment(OSPA)measure[51],oftenusedtorepresentaccuracyin referenceflowparticlesandweightswithafixedGaussian6
Fig. 2: The construction of the actual PDF c) using the
adapting kernels functions b) based on the desired PDF a).
kernel, K (·) ≡ K(·), via (7) to estimate pˆ (xk),
j,i oracle
and via (8) for the desired state x¯.
O2 InthesupervisedsettingsS2,thetruestatex¯ isavailable
in the data, and pˆ (xk) is isotropic multivariate
oracle
Fig. 3: staged meshgrid: The heatmap loss is calculated as
Gaussiandistributionscenteredinthetruesub-states,with
variances equal to the average Vˆk . the sum of L squared error between L pairs of heatmaps
oracle|j
grids, desired (top row) and actual (bottom row), on different
Thelastremainingcomponentneededtoevaluate(11)isthe
resolutions and scales.
PDF induced by the particles and their weights, pˆ (xk|z1:k).
θ
We do that using (7) and a set of d -dimensional Gaussian
sp the covered area is bigger and probability density is lower;
kernel functions {K (·)}t,N we named adapting kernels
j,i j,i=1 there, the adapting kernels will tend to be wider and cover
illustratedinFig.2.TheseKernelfunctionsaresettohaveco-
larger area so can be captured by the lower resolution stages.
variances{σ2 I},with{σ }determinedsuchthatallkernels
j,i j,i We implement and test the staged meshgrid method in Sub-
have equal volume that sum up to t, and with peak height of
section IV-B.
p (xk ).Usingthesesettings,particlesinhighprobability
oracle j,i Random Grid: Despite the aforementioned benefits of our
regions(wheretheyarelikelytohaveotherparticlescloseby)
stagedmeshgridapproach,itmaybecomputationallyintensive
will be assigned with high and narrow kernels, and in case
in some settings, particularly when the sub-state is of a high
that they are too close, will be encouraged to disperse by the
dimension d . An alternative approach uses a random grid.
locally lower desired PDF on the loss. Similarly, particles in sp
Here, instead of the evenly spread points as on a meshgrid,
low probability regions (where they are likely to be isolated)
we randomize sampling points according to a predetermined
will be assigned with wide kernel functions, and so, will
distribution. Similar to the staged meshgrid, the random
be encouraged by the loss to be more evenly distributed.
grid points sampling distribution can have different density
This formulation supports adapting the grid points resolutions
in different regions to accommodate changing resolutions
according to the particles predicted density, as well as boosts
and tackle particle degeneracy and sample impoverishment.
a smooth and accurate PDF reconstruction.
Pseudo-random sampling points will result in more evenly
3) PDF Comparison: The comparison of the two PDFs,
distributed points and a better cover of the area of interest,
i.e.,theterm∥pˆ (xk|z1:k,x¯k)−pˆ (xk|z1:k)∥ in(13),is
oracle θ 2 and may induce faster and better learning, however this is
approximated by computing their differences over a grid, and
left for future work. The random grid method is numerically
theselectionofthegridpointshighlyimpactstheusefulnessof
evaluated in Subsection IV-A.
theloss.Wepresenttwomethodsforchoosingthegridpoints
around the sub-states locations:
D. Training
Staged Meshgrid: The first method selects the grid points
tobeevenlydistributedonad -dimensionalcube,oramesh- We note that the loss function in (11) expresses a sum of κ
sp
grid,whileadaptivelytuningitsscale,resolutionandlocation. time-steps components of (12), and (13), computed separately
As illustrated in Fig. 3, the PDFs in (13) are compared L>0 at each time-step k when an LF block with parameters θ
times with L different grids of the same size and different is applied. We can leverage this property to enable training
resolutions,co-centeredaccordingtox¯k andxˆk(z1:k;θ).The LF on each (or selected) time-step seperately using con-
j j
overlaps between the different resolutions is accounted for ventional stochastic gradient descent (SGD)-based learning,
once on the higher resolution, that is also considered in the whileavoidingtheneedfordifferentiabilitybetweeniterations
grid points weighting in the loss. (and avoiding the need to backpropagate through sampling
Usingadaptingkernelstogetherwithstagedmeshgridtack- operatorsusingspecialsamplingadaptations [52],[53],orby
les two of the main challenges encountered in PFs – sample introducing approximated operators [28], [29]).
impoverishmentandparticledegeneracy–aswellasstochastic A candidate training method based on mini-batch SGD for
outliers. Sample impoverishment tends to happen close to an LF-PF is formulated in Algorithm 3. There, the LF-PF
the center of the support of the PDF, or in high resolution is executed in parallel with the reference PF flow (Steps 6,
meshgrid stages, that can accommodate the more narrow 7), that maintain particles-weights sets for each trajectory,
kernels that are likely to be used there. Particle degeneracy respectively denoted Pk,Rk for the kth time step of the qth
q q
usually occurs in the edges of the support of the PDF where batch. Training is done by loss calculation (Steps 9-11) and7
its backpropagation (Step 12), while nullifying gradient prop- OurLFmodule,trainedwithaspecificnumberofparticles,
agation between iterations and through sampling operations. allows PFs to operate reliably with different, and even lower
To achieve a fast and continuous convergence, on each time- valuesofN,boostinglowlatencyfiltering.However,theappli-
step k we mix the loss of different trajectories by running a cation of LF comes at the cost of some excessive complexity,
bigger batch of trajectories D , while training on a random depending on the parameters J, d , t, N, P, as well as
q sp
subset of the batch that changes between time-steps (Step 8). the flock-update parameters, number of sub-embeddings E
For tracking stabilization during training, particularly at the (1, or 2 to include secondary embedding), number of SA
beginning, once an estimated sub-state strays farther than a blocks S, and FC width multiplier B. The total floating
predetermined distance from the desired sub-state (Step 14) point multiplications (FPM) requirement per particle can be
the whole trajectory is eliminated from D for the training on divided to three parts with accordance to Fig. 1: broadly
q
the following time-steps (Step 15). speaking, (i) the embedding networks Emb−Net1/2 re-
quireJEt·BP (P +d +3BP)FPM;(ii)theSAnetworks
sp
Algorithm 3: LF-PF training SA−Net0/1 involve
JSt·2P2(cid:0)
2+N/P
+B+B2(cid:1)
; and
Init: Set augmented and reference PFs;
(iii)thefinalFClayersrequiret·BP (BP +d sp)FPM.While
Initialize LF parameters θ; this excessive complexity varies considerably with the system
Fix learning rate µ>0; parameters, it is noted that the operation of such compact
Set sub-state stray distance threshold ζ; DNNs is highly suitable for parallelization and hardware
Input: Training set D, initial states {x¯0} acceleration,thatoftennotablyreducesinferencespeedofPFs
1 for e=1,... until convergence do with similar performance, as demonstrated in Section IV.
2 Randomly divide D into Q batches {D q}Q q=1; OurproposedLFjointlycorrectsparticlesandtheirweights
utilizing only the particles and weights (that are merely
3 for q =1,...,Q do
updatedbytheLFblock).Thisboostsversatilityandflexibility
4 Initialize LF-PF and reference PF |D q|
particles-weights sets P0 and R0 to {x¯0}; enabling a trained LF block to be conveniently integrated
q q and apply a fix to a flock at any point on different types of
5 for k =1,...,κ do
PFs flow, whether classical or DNN augmented, or even to
6 ApplyLF-PFwithθ toD q,P qk−1 togetP qk;
a different PF than the one it was trained on (as shown in
7 Apply reference PF to D q, Rk q−1 to get Rk q;
Subsection IV-A). Nonetheless, one can potentially extend LF
8 For a random sub-batch D˜ qk ⊆D q:
to process additional information when updating the particles,
9 Evaluate Vˆ ok racle and pˆ oracle(xk) using suchaspastparticles(smoothing),themeasurements,anddata
Rk and O1 or O2;
q structures [55]. Moreover, the LF block can be adapted to
10 Evaluate Vark(z1:k;θ) and pˆ θ(xk|z1:k) changeitsoutput,forinstance,toincludeanestimatedstateor
using Pk and pˆ (xk);
q oracle additional embedding for other purposes. It can possibly also
11 Calculate loss L D˜k(θ) via (11); be integrated in PFs involving detection (e.g., track-before-
q
12 Update θ ←θ−µ∇ θL D˜k(θ); detect [7]). We leave these extensions for future work.
q
13 for each (z1:κ,x¯1:κ)∈D q do
14 if ∃j s.t. ∥x¯k j,xˆk j(z1:k;θ)∥≥ζ then IV. EXPERIMENTALSTUDY
15 Remove D q ←D q/{(z1:κ,x¯1:κ)}; In this section, we numerically evaluate our LF algorithm
in terms of performance and latency. We first compare it
16 return θ to state-of-the-art PF neural augmentation for synthetic state
estimation in Subsection IV-A. Then, in Subsection IV-B we
combineourLFwithawell-establishedauxiliaryPF(APF)for
a radar MTT setup1. All timing/latency tests were computed
E. Discussion on the same processor, AMD EPYC 7343 16-Core 3.2GHz.
The proposed trainable LF module is trained as part of a
specific PF, and possibly with an additional more complex A. Synthetic State Estimation
PF serving as reference. Nonetheless, once trained, it can
Here, we evaluate LF-PF in comparison to state-of-the-art
be readily transferred into alternative PFs that meet R1, as
neural augmentation of PFs, based on algorithm unrolling
demonstrated in Subsection IV-A. Moreover, by harnessing
[31]. Our goal here is to showcase the performance gains of
supervised data, the LF-PF can also cope with R2 without
our proposed LF algorithm compared to alternative types of
incorporating additional mechanisms to explicitly mitigate it
DNN augmentations, and that its operation is complementary
as in [54]; while enabling the pertinent PF to operate with
to other enhancements of PFs.
a smaller number of particles with minimal computational
1) Simulation Setup: We adopt the synthetic simulation
cost to meet R3. As discussed in Subsection III-B, the LF
setup used in [31]. Here, the state is comprised of d = 10
architectureisinvarianttothenumberofsub-statest,and,e.g., sp
dimensional variables, and is tracked based on observations
the same module can be trained and evaluated for tracking
a different number of sub-states, thus meeting R4. These
1The source code and hyperparameters used in this experimental study is
properties are consistently demonstrated in Subsection IV-B. availableathttps://github.com/itainuri/LF-PF MTT.git.8
zk of dimension d = 8 over κ = 12 time-steps. The state
m
evolution model is given by
xk =ϕ(Axk−1)+vk, (14)
while the observation model is
zk =Cxk+ek. (15)
In (14)-(15), A ∈ Rdsp×dsp and C ∈ Rdm×dsp are fixed
matricestakenfrom[31,Ch.4.1]andthenoisevectorsvk and
ek are zero-mean with covariances Σ and Σ , respectively.
v e
Specifically, the noise distributions and the mapping ϕ(·) are (a) Particles and PDF, LF input (b) Particles and PDF, LF output
taken from the following setups:
X1 A linear Gaussian system, where ϕ(x) = x and vk
and wk are zero-mean white Gaussian noises with fixed
covariancematricesΣ andΣ takenfrom[31,Ch.4.1].
v e
X2 Gaussian noise as in X1, with non-linear ϕ(·) given by
the absolute value function (taken element-wise).
X3 A non-Gaussian system, where ϕ(x) = x, with non-
Gaussianuniformnoiseandmismatchedassumedcovari-
ance matrices Σ =σ2I and Σ =σ2I . (c) Sorted weights. (d) Weights histograms.
v dsp e dm
The noise covariances have the same ℓ 2 norm, i.e., ∥Σ v∥ 2 = Fig. 4: LF particles and weights adjustment example with
∥Σ e∥ 2, dictating the signal-to-noise ratio (SNR). the LF-SISPF in the X3 settings of Subsection IV-A. Top:
2) Estimation Algorithms: Our main baseline PF through- reconstructed PDF cross-section and particles at the input
out this section is the Sequential Importance Sampling (SIS) (Fig. 4a) and output (Fig. 4b) of the LF block. The particles
PF (SISPF) [16]. This estimator implements Algorithm 1 projection on [x ,x ] plane are marked with red dots, and
1 2
with a Gaussian sampling distribution q(xk), whose inverse the desired state’s and its estimate’s projections are marked
c Σov (cid:16)a Σri −an 1c ϕe
(cid:0)
Ais xΣ k− −1 1(cid:1)= +Σ C− v T1 Σ+ −1C zkT (cid:17)Σ .− e1C, and mean is µ = w soi rt th edgr wee en iga hn td sr (e Fd igc .ro 4s cs )es a, nr des wpe ec igti hv te sly h. isB too gtt ro am m: sth (e Fip ga .r 4ti dc )les a’
t
v i e
BasedontheformulationoftheSISPFtotheabovemodels, the input and output of the LF module.
we compare the following estimation algorithms:
• SISPF with N =25 and with N =300 particles. SNR=0 dB and fine-tuned for each SNR. While the same
• An LF augmented SISPF (LF-SISPF) with N = 25 LF pre-trained weights were applied on all trajectories of the
particles. With d dictated by the settings, and with sameexperiment,theUrPFwastrainedpertesttrajectory(per
sp
P = 64, B = 2, J = 2, and S = 2, the LF module experiment). Specifically, we trained UrPF for 200 epochs on
has 4.19·105 trainable parameters θ, and 4.23·105 FPM that κ=12 time-steps trajectory without resampling, keeping
per particle per iteration with 25 particles. The reference thebestweightsfortestingaccordingtotwocriteria,ξ forthe
1
PFusedfortraininganisSISPFwithN˜ =300particles, full trajectory average, and ξ for the last time-step average.
2
withtheheatmaplossPDFsin(13)comparedover4·106 3) Results: We test each of the benchmarks in all three
random grid points with Gaussian distribution. settings with different SNRs. All benchmarks have particles
• The unrolled PF (UrPF) of [31], which unrolls κ PF resampling procedure cooperated to their iteration flow, and
time-steps. All steps use the same covariance DNN and testing is performed twice, once with N = N//3 and once
th
each step uses a dedicated mean DNN for a multivariate with N =0 (without resampling). For clarity considerations
th
normalimportancesamplingfunction.TheDNNsreceive we chose to present the experiment with the better results out
a particle and the measurements as input, and have of the two, where the SISPF with N = 300 particles was
1.41·(1+κ)·105 learnable parameters ξ, and require always implemented with resampling.
2.82·105 FPM per particle per 25 particles iteration. We first visualize the ability of the LF module in form-
• LF augmented UrPF (LF-UrPF), Augmenting the trained ing desirable particle patterns. To that aim, we observe the
data-based UrPF with an LF module, trained with the operation of a trained LF module for the settings X3 at a
model-based SISPF (both with the same θ and ξ). given time instance k =7 and SNR of 0 dB. We illustrate in
All data-driven estimators were trained in an unsupervised Figs.4a-4bhowthetrainedLFblockherdstheparticlescloser
manner: LF-SISPF via O1, and UrPF as described in [31]. to the desired location (on x and x axes), and how the new
1 2
We use the mean-squared error (MSE) as our performance induced PDF is more confined and the estimation is more
measure, evaluated over a 100 test trajectories, a 100 times accurate.Wealsoexaminetheweightsattheinputandoutput
each. For each scenario we trained the LF-SISPF model on ofourLFblockinFigs.4c-4d.There,weobserveareduction
κ = 15 long trajectories, keeping the best weights according in the variance of the weights and an increase in the effective
to time-step k = 12, and considering time-steps k = 9−15 numberofparticlesN (withoutexplicitlyencouragingitvia
eff
for the loss. For each setup, training was initially done with the loss), which potentially indicate an improved capture of9
SISPF SISPF LF-SISPF UrPF LF-UrPF 1) Experimental Setup: In the considered scenarios, which
N 300 25 25 25 25 are based on [7, Sec. VI], the state xk represents the posi-
mS 335.499 31.323 72.183 177.197 213.320
tions and velocities of a fixed number of targets t in two-
TABLE I: Average tracking latency in milliseconds over κ=
dimensional space over trajectories of length κ = 100, with
12 time-steps trajectories with N =0.
th uptot=10targets.Eachtargetisindependentofothertargets
and follows 14 with ϕ(x) = x and A and vk taken from [7,
the PDF. This constant correction of the particles and weights
Sec. VI.A]. The measurements zk capture a 13×13 sensor
alleviates the degeneracy phenomenon, as can be seen by the
response solely to the targets’ locations, following the sensor
initial high N on time-step k =7.
eff model of [7, Sec. VI.A]. We consider three different settings:
We next show that the favourable particles and weights
alternationsachievedbyLFonasingletime-step,aspresented Y1 Single target in calibrated settings, tracking with sen-
in Fig. 4, translate into improved tracking accuracy. To that sors located 10 meters apart on 120 × 120 meters 2-
aim,wereportintheFig.5theresultingOSPAvaluesachieved dimensional plane, with different SNRs.
by the considered tracking algorithms for settings X1-X3 in Y2 Single target in mismatched settings, same assumed
recoveringtheentiretrajectory(Figs.5a-5c)andinrecovering model where the actual sensors locations are perturbed
solely the final time-step k =12 (Figs. 5d-5f), being the task with random Gaussian noise, with different SNRs.
for which UrPF is originally designed for, in [31]. Y3 MTTof1−10targets,asingleSNR,calibratedsettings.
As observed in Fig. 5, on all experiments the LF-SISPF 2) Estimation Algorithms: Similar to Algorithm 2, we
notably improves the SISPF and achieves results that are augment the APP, a version of the APF proposed in [7], prior
close to those of its teacher PF, namely, the SISPF with to its integrated Kalman filter that tracks the target velocity.
N = 300. In the straightforward X1 settings our LF-SISPF Accordingly, we compare the following PFs:
accuracy surpasses that of the UrPF in all experiments (Figs.
• APP from [7] with single and multiple target support.
5aand5d),whileoperatingwithasimilarnumberofFPMand
• LF augmented APP (LF-APP) for single target. LF mod-
with a number of trainable parameters that is independent of
ule with P = 32, E = 1, B = 1; while J,S = 1
trajectory length, and without considering the measurements.
(for calibrated settings) and J,S = 2 (for mismatched
On the more complex X2 (Figs. 5b and 5e) and X3 (Figs.
settings).
5c and 5f) cases, LF outperforms the overall accuracy of the
• MTT LF-APP, with LF module with P = 64, B = 3,
UrPF (Figs. 5b and 5c), but outperformed by it on the last
E =2, J =2, and S =1.
time-step criterion (Figs. 5e and 5f). This can be attributed
to the inferiority of the reference PF used in training, i.e., All LF-APP benchmarks were trained according to Algo-
SISPF Rs 300, as well as to the specialized training of the rithm 3, via O1 in the calibrated settings and via O2 in the
UrPF, that is trained separately for each specific trajectory mismatched settings. All training was done on a single SNR
and each specific task. This specialization may be the cause using N = 100 particles and N˜ = 5000 with batches and
of the deteriorated accuracy of the UrPF on one task when sub-batches sizes between 50 − 250 and 5 − 25. Training,
trained for the other (UrPF Rs ξ on the top sub-figures validationandtestingperformancesaretheaverageOSPA[51]
2
and UrPF Rs ξ on the bottom sub-figures). metric (assigned per time-step), evaluated over all κ = 100
1
The LF-UrPF experiments demonstrate the versatility of time-steps, with cutoff of 10 (and infinite cutoff on training)
our algorithm, incorporating our LF module to the DNN and order of 2 (see [51] for details). The heatmap loss was
augmented UrPF on selected time-steps. Even though the computedusingstagedmeshgridwithL=5and120×120(on
LF module was trained to improve another (classical) PF, Y1,Y2) and 160×160 (on Y3) grid points. For all settings,
its augmentation to the UrPF ξ 2 has improved it and the we randomly combine a set of n t = 105 single target sub-
LF (−1) −UrPF ξ 2 accuracy on Figs. 5b and 5c surpasses trajectories for training and two such sets of n t = 104, for
those of UrPF Rs ξ on the overall accuracy while main- validation and testing.
1
taining accuracy on the last time-step on Figs. 5e and 5f. 3) Results: we divide our results into three parts with
Moreover, while the LF module comes at the cost of some respect to Y1-Y3 and compare the benchmarks for accuracy,
excessivecomplexity,asanalyzedinSubsectionIII-E,itsgains performance, and latency, with accuracy results averaged over
inperformancegreatlyoutweighitsadditionalinducedlatency 104trajectories.Wevisualizeasingletargettrajectorytracking
that is reported in Table I. The runtime values, reported in on Y2 as well as an MTT one in Y3 settings with t = 10
TableI,togetherwithFig.5,showcasetheadvantagesofLF,in targets. As in Subsection IV-A, we also visualize the ability
allowingtonotablyenhanceperformancewithlimitedparticles ofLFtoenhancetheparticlesandweightsandtheirassociated
without significantly increasing inference latency. reconstructed PDF on a single time-step on MTT with t=3.
Singletargetincalibratedsettings(Y1):Wefirstcompare
B. Radar Target Tracking
the accuracy achieved by the APP to that of the LF-APP,
We proceed to evaluating LF-PF for non-linear radar track- trained on SNR = 20. The resulting OSPA versus SNR,
ing, considering both a single and multiple targets. We use reported in Fig. 6, demonstrate that, although trained with
the non-linear settings outlined in [7, Sec. VI], which allow N = 100 particles and a single SNR, the correction term
us to compare the performance and latency of PFs to their LF added by our LF block allows APP to systematically achieve
augmented versions in a scenario of practical importance. performanceequivalentto2−3timesmoreparticlesonawide10
(a) X1, entire trajectory accuracy (b) X2, entire trajectory accuracy (c) X3, entire trajectory accuracy
(d) X1, last time-step accuracy (e) X2, last time-step accuracy (f) X3, last time-step accuracy
Fig.5:Overall(top)andlasttime-stepk =κ(bottom)trackingaccuracyforsettingsX1-X3overκ=12time-stepstrajectories.
UrPF is utilized for overall and last time-step accuracy with its respective task oriented trained parameters ξ or ξ . ( Rs)
1 2
implies resampling with N =N//3, and (LF−) or (LF −) implies the utilization of the LF including or excluding the
th (−1)
last time-step. SISPF Rs 300 was tested with N =300 particles, while all other benchmarks were tested with N =25.
Fig. 6: OSPA vs. SNR of APP and LF-APP with different Fig. 7: OSPA vs. SNR of APP and LF-APP with different
numbers of particles, (Y1) experiment, calibrated settings. numbers of particles, (Y2) experiment, mismatched settings.
rageofSNRs.Wealsonotethattheimprovement,inducedby inTableIIindicatethatwhilelatencygrowsdramaticallywith
our Algorithm is more significant on lower SNRs. N, the excessive latency induced by incorporating our LF
block is minor. Combining this with Figs. 6 and 7 showcases
Single target in mismatched settings (Y2): For the mis-
the ability of LF-APP in allowing PFs (such as the APP) to
matched case, we employ LF-APP with J,S = 2, trained
meet R1-R3 on a wide range of single target scenarios.
on SNR = 10. The results, reported in Fig. 7 show that
LF-APPnotablyimprovesperformancewithsimilarconsistent MTT in calibrated settings (Y3): We proceed to compare
improvement with increased N, but also yields an error floor the APP and the LF-APP for their accuracy, performance and
for the model-based APP. To show that this gain is directly latency for a known and fixed number of targets t, taking
translated into improved tracking, we illustrate in Fig. 8 values between 1 and 10 and with different numbers of
the tracking of a single sub-state trajectory, where the LF particlesN.Trainingwasdonebyjointlylearning[56]ondata
augmentation is shown to notably improve tracking accuracy. corresponding to 1, 3, 5 (with equal probability) and 8 (10%
WeproceedbyevaluatingtheexcessivelatencyofLF-APP. oftrajectories)targetstrackingscenarioswithSNR=20,and
We report in Table II the latency of both APP and LF-APP withvalidationont=4targetstracking.Sincethebasicform
used for both experiments in milliseconds. The timing results of our LF tested here has a per time-step operation, and in11
(a) LF input j =1 (b) LF input j =2 (c) LF input j =3
(a) APP, OSPA=2.46. (b) LF-APP, OSPA=1.07.
Fig. 8: Radar single target simulation example with mis-
matched observation modelling on SNR = 10 using the
APP (8a) and the LF-APP (8b). Tracking is done on [x,y]
plane over κ = 100 time-steps shown as k axis, using (d) LF output j =1 (e) LF output j =2 (f) LF output j =3
N = 100 particles. True targets trajectories are shown as
circles and reconstructed trajectories are shown as solid lines.
Both trajectories colors represent the time-step, and arrows
highlight tracking gaps. OSPA is computed with cutoff=∞.
N APP LF-APP, LF-APP,
(J,S)=(1,1) (J,S)=(2,2)
50 8.200 11.548 14.039
(g) Sorted weights. (h) Weights histograms.
100 13.993 19.400 20.828
200 34.147 36.177 38.082
Fig.9:LFMTTsub-particlesandweightsadjustmentexample
300 50.221 53.046 55.666
with three targets on the MTT LF-APP experiment. The LF
400 66.195 69.797 73.393
500 81.986 86.749 91.245 module input (Figs. 9a-9c) and output (Figs. 9d-9f) sub-
TABLEII:Averagetrackinglatencypertime-stepinmillisec- particles and their respective induced PDF as part of the
onds for the single target benchmarks of Subsection IV-B. heatmap loss. The particles are marked with dots; the desired
Tested on κ=100 time instances long trajectories. state and its estimate are marked with green and red crosses,
respectively. Fig. 9g shows the sorted weights (common to all
three sub-particles) and Fig. 9g shows the histograms of the
order to isolate its evaluation, we address training and testing
weights at the input and output of the LF module.
inthesamemanner,andthemappingbetweenestimationsand
true targets is done separately on each time-step according to
highlighted on Fig. 10 with arrows of respective colors.
the best OSPA, effectively disregarding target swaps between
The translation of these improvements into tracking accu-
time-steps.
racy is showcased on Fig. 11. rained with 100 particles and
Fig. 9 illustrates the operation of our LF block on a single
applied with varying number of particles, Fig. 11 illustrates
frame three target tracking on time-step k = 19. Figs. 9a-9f
that LF provides greater improvement in performance as the
visualize its improving of the spread of the three sets of sub-
numberoftargetsincreases.Whiletrainedwithupto8targets
particlesbydispersingclustersandreducingoutliers.Figs.9g-
on only 10% of the trajectories, LF achieves a significant
9h show that our trained LF module encourages low variance
reduction in the required number of particles when handling
oftheweightswithoutexplicitlydoingsointheloss,implying
t=10 targets, consistently reducing it by a notable factor of
abetterutilizationoftheparticles.WeagainobservethatN
eff 8×, and up to 10× with a 100 particles.
is close to N = 100 already at the input of our LF module,
Complementary to Fig. 11, Table III compares the tracking
hinting the effectiveness of our LF algorithm in describing
latency of the APP and LF-APP with different numbers of
the PDF along the whole trajectory, as well as strengthens its
particles and targets. Combining the results in Fig. 11 and
effectiveness [57],asitisusefulalsoforenhancinginsettings
Table III, and comparing them to the Y1 results in Fig. 6
where N ≈N.
eff and Table II, consistently demonstrate that the benefit of our
Single time-step improvements on the spread of the sub-
LF algorithm in coping with the challenging task of tracking
particles as presented on Fig. 9, applied to each time-step,
multiple targets with limited latency is more prominent and
haveamajoreffectontheoveralltrackingofthetargetsalong
potentially more beneficial than in single target settings.
the full state trajectory. Fig. 10 illustrates this benefit on a 10
targetMTTsimulation.There,wecanseetheimprovementin
terms of a diminished number of lost targets, better targets-
V. CONCLUSIONS
estimationsmappinginjectivenessandbetteraccuracy.Further, We introduced a neural augmentation of PFs using a novel
even though we did not explicitly encourage it in our loss concept of LF, that supports changing numbers of sub-states
function, we also witness less targets-estimations mapping and particles. LF is designed to enhance state estimation by
swaps. Examples of the aforementioned improvements are adjusting a PF’s particles at any point in its flow, solely based12
(a) APP, OSPA=3.63, and 7.36 with cutoffs 10 and ∞. (b) LF-APP, OSPA=0.55, with both cutoffs.
Fig. 10: Radar MTT simulation example with t = 10 targets using the APP (10a) and the LF-APP (10b). Tracking is done
on [x,y] plane over κ=100 time-steps shown as k axis, using N =100 particles. True targets trajectories and reconstructed
ones are shown as circles and solid lines, respectively. Colors represent the time-step, and arrows highlight tracking gaps.
ontheinterplaybetweentheparticles,resultinginanimproved
output. We realize LF using a dedicated DNN architecture
that accounts for the permutation equivariance property of the
particle-weightpairsinPFs,andintroduceadedicatedtraining
framework that supports both supervised and unsupervised
learning, while accommodating high-dimensional sub-state
tracking and with indifference to the particles acquirement
procedure. Once trained, the LF module is easily transferable
to most PFs, and can even be combined with alternative DNN
PF augmentations. We experimentally exemplified the gains
of LF in terms of performance, latency, and robustness and
even in overcoming mismatched modelling, demonstrating its
potential for enhancing variety of PFs and as a foundation for
enhancement of more intricate PF filtering tasks.
Fig.11:OSPAvs.numberoftargetst,withdifferentnumbers
REFERENCES
of particles, APP and LF-APP, on the MTT (Y3) experiment.
[1] I. Nuri and N. Shlezinger, “Neural augmented particle filtering with
learningflockofparticles,”inIEEEInternationalWorkshoponSignal
ProcessingAdvancesinWirelessCommunication(SPAWC),2024.
[2] P. M. Djuric, J. H. Kotecha, J. Zhang, Y. Huang, T. Ghirmai, M. F.
t Bugallo,andJ.Miguez,“Particlefiltering,”IEEESignalProcess.Mag.,
1 2 4 6 8 10
N vol.20,no.5,pp.19–38,2003.
[3] S.Thrun,“Particlefiltersinrobotics.”inUncertaintyinAI(UAI),vol.2.
APP 14.62 28.24 49.34 73.32 101.393 135.10
100 Citeseer,2002,pp.511–518.
LF-APP 24.08 41.33 66.24 98.32 135.52 172.35
[4] P. Djuric´, J. Zhang, T. Ghirmai, Y. Huang, and J. H. Kotecha, “Appli-
200 APP 35.52 53.89 95.29 142.35 198.011 260.86 cationsofparticlefilteringtocommunications:Areview,”in200211th
LF-APP 44.66 79.95 127.64 189.11 263.19 337.53 EuropeanSignalProcessingConference. IEEE,2002,pp.1–4.
APP 52.12 79.51 141.14 211.19 295.751 386.41 [5] F. Gustafsson, F. Gunnarsson, N. Bergman, U. Forssell, J. Jansson,
300
LF-APP 64.94 116.26 189.63 288.43 391.55 505.67 R. Karlsson, and P.-J. Nordlund, “Particle filters for positioning, nav-
800 APP 129.93 203.29 362.40 542.73 803.78 999.30 igation,andtracking,”IEEETrans.SignalProcess.,vol.50,no.2,pp.
425–437,2002.
1600 APP 257.77 404.08 720.87 1087.01 1624.70 1996.31
[6] Y.BoersandJ.Driessen,“Multitargetparticlefiltertrackbeforedetect
2400 APP 385.63 595.86 1046.25 1569.93 2184.01 2824.22
application,” IEE Proceedings-Radar, Sonar and Navigation, vol. 151,
TABLE III: Average tracking latency per time-step in mil- no.6,pp.351–357,2004.
liseconds for the study of Fig. 11. Tested on κ = 100 long [7] L. Ubeda-Medina, A. F. Garcia-Fernandez, and J. Grajal, “Adaptive
auxiliary particle filter for track-before-detect with multiple targets,”
trajectories.
IEEE Trans. Aerosp. Electron. Syst., vol. 53, no. 5, pp. 2317–2330,
2017.13
[8] N. Ito and S. Godsill, “A multi-target track-before-detect particle filter [33] B. Cox, S. Pe´rez-Vieites, N. Zilberstein, M. Sevilla, S. Segarra, and
usingsuperpositionaldatainnon-Gaussiannoise,”IEEESignalProcess. V. Elvira, “End-to-end learning of Gaussian mixture proposals using
Lett.,vol.27,pp.1075–1079,2020. differentiableparticlefiltersandneuralnetworks,”inIEEEInternational
[9] J. Durbin and S. J. Koopman, Time series analysis by state space ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),2024,
methods. OxfordUniversityPress,2012. pp.9701–9705.
[10] S. Buzzi, M. Lops, and L. Venturino, “Track-before-detect procedures [34] M.Piavanini,L.Barbieri,M.Brambilla,andM.Nicoli,“Deepunfolded
forearlydetectionofmovingtargetfromairborneradars,”IEEETrans. annealedsteinparticlefilterforvehicletracking,”inIEEEInternational
Aerosp.Electron.Syst.,vol.41,no.3,pp.937–954,2005. ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),2024,
[11] G. Grisetti, C. Stachniss, and W. Burgard, “Improving grid-based pp.13226–13230.
slamwithRao-Blackwellizedparticlefiltersbyadaptiveproposalsand [35] Y.Xia,S.Qu,S.Goudos,Y.Bai,andS.Wan,“Multi-objecttrackingby
selectiveresampling,”inIEEEinternationalconferenceonroboticsand mutualsupervisionofcnnandparticlefilter,”PersonalandUbiquitous
automation,2005,pp.2432–2437. Computing,pp.1–10,2021.
[12] P. Closas and C. Ferna´ndez-Prades, “Particle filtering with adaptive [36] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
numberofparticles,”inIEEEAerospaceConference,2011. Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
[13] D. Xu, C. Shen, and F. Shen, “A robust particle filtering algorithm neuralinformationprocessingsystems,vol.30,2017.
withnon-gaussianmeasurementnoiseusingstudent-tdistribution,”IEEE [37] N.ShlezingerandT.Routtenberg,“Discriminativeandgenerativelearn-
SignalProcess.Lett.,vol.21,no.1,pp.30–34,2013. ingforlinearestimationofrandomsignals[lecturenotes],”IEEESignal
[14] A. T. Fisch, I. A. Eckley, and P. Fearnhead, “Innovative and additive Process.Mag.,vol.40,no.6,pp.75–82,2023.
outlierrobustkalmanfilteringwitharobustparticlefilter,”IEEETrans. [38] J.Gou,B.Yu,S.J.Maybank,andD.Tao,“Knowledgedistillation:A
SignalProcess.,vol.70,pp.47–56,2021. survey,”InternationalJournalofComputerVision,vol.129,pp.1789–
[15] T. Li, M. Bolic, and P. M. Djuric, “Resampling methods for particle 1819,2021.
filtering: classification, implementation, and strategies,” IEEE Signal [39] M.K.PittandN.Shephard,“Filteringviasimulation:Auxiliaryparticle
Process.Mag.,vol.32,no.3,pp.70–86,2015. filters,”JournaloftheAmericanstatisticalassociation,vol.94,no.446,
[16] A. Doucet, S. Godsill, and C. Andrieu, “On sequential monte carlo pp.590–599,1999.
sampling methods for bayesian filtering,” Statistics and computing, [40] B.-n. Vo, M. Mallick, Y. Bar-Shalom, S. Coraluppi, R. Osborne,
vol.10,pp.197–208,2000. R. Mahler, and B.-t. Vo, “Multitarget tracking,” Wiley encyclopedia of
[17] M.Piavanini,L.Barbieri,M.Brambilla,andM.Nicoli,“AnnealedStein electricalandelectronicsengineering,no.2015,2015.
particle filter for mobile positioning in indoor environments,” in IEEE [41] N. Bergman, A. Doucet, and N. Gordon, “Optimal estimation and
International Conference on Communications (ICC), 2023, pp. 2510– crame´r-raoboundsforpartialnon-gaussianstatespacemodels,”Annals
2515. oftheInstituteofStatisticalMathematics,vol.53,pp.97–112,2001.
[18] I.Goodfellow,Y.Bengio,andA.Courville,DeepLearning. MITPress, [42] M.S.Arulampalam,S.Maskell,N.Gordon,andT.Clapp,“Atutorial
2016,http://www.deeplearningbook.org. onparticlefiltersforonlinenonlinear/non-GaussianBayesiantracking,”
[19] N.Shlezinger,J.Whang,Y.C.Eldar,andA.G.Dimakis,“Model-based IEEETrans.SignalProcess.,vol.50,no.2,pp.174–188,2002.
deeplearning,”Proc.IEEE,vol.111,no.5,pp.465–499,2023. [43] N. Bergman, “Recursive bayesian estimation: Navigation and tracking
[20] N.Shlezinger,Y.C.Eldar,andS.P.Boyd,“Model-baseddeeplearning: applications,”Ph.D.dissertation,Linko¨pingUniversity,1999.
On the intersection of deep learning and optimization,” IEEE Access, [44] P. Rebeschini and R. Van Handel, “Can local particle filters beat the
vol.10,pp.115384–115398,2022. curseofdimensionality?”Ann.Appl.Probab.,vol.25,no.5,pp.2809–
[21] N. Shlezinger and Y. C. Eldar, “Model-based deep learning,” Founda- 2866,2015.
tions and Trends® in Signal Processing, vol. 17, no. 4, pp. 291–416, [45] O. Straka and M. Sˆimandl, “A survey of sample size adaptation tech-
2023. niquesforparticlefilters,”IFACProceedingsVolumes,vol.42,no.10,
[22] G.Revach,N.Shlezinger,X.Ni,A.L.Escoriza,R.J.VanSloun,and pp.1358–1363,2009.
Y. C. Eldar, “KalmanNet: Neural network aided Kalman filtering for [46] P. D. Moral, “Nonlinear filtering: Interacting particle resolution,”
partially known dynamics,” IEEE Trans. Signal Process., vol. 70, pp. ComptesRendusdel’AcademiedesSciences-SerieI-Mathematique,vol.
1532–1547,2022. 325,no.6,pp.653–658,1997.
[23] G. Revach, N. Shlezinger, T. Locher, X. Ni, R. J. van Sloun, and [47] P.DelMoralandL.Miclo,Branchingandinteractingparticlesystems
Y.C.Eldar,“Unsupervisedlearnedkalmanfiltering,”inEuropeanSignal approximations of Feynman-Kac formulae with applications to non-
ProcessingConference(EUSIPCO),2022,pp.1571–1575. linearfiltering. Springer,2000.
[24] I. Buchnik, G. Revach, D. Steger, R. J. Van Sloun, T. Routtenberg, [48] C.Musso,N.Oudjane,andF.LeGland,“Improvingregularisedparticle
and N. Shlezinger, “Latent-KalmanNet: Learned Kalman filtering for filters,”inSequentialMonteCarlomethodsinpractice. Springer,2001,
tracking from high-dimensional signals,” IEEE Trans. Signal Process., pp.247–271.
vol.72,pp.352–367,2023. [49] P. Bunch and S. Godsill, “Particle filtering with progressive Gaussian
[25] A. Ghosh, A. Honore´, and S. Chatterjee, “DANSE: Data-driven non- approximations to the optimal importance density,” in IEEE Interna-
linear state estimation of model-free process in unsupervised learning tionalWorkshoponComputationalAdvancesinMulti-SensorAdaptive
setup,”IEEETrans.SignalProcess.,vol.72,pp.1824–1838,2024. Processing(CAMSAP),2013,pp.360–363.
[26] G. Revach, X. Ni, N. Shlezinger, R. J. van Sloun, and Y. C. Eldar, [50] J. Kronander and T. B. Scho¨n, “Robust auxiliary particle filters using
“RTSNet: Learning to smooth in partially known state-space models,” multipleimportancesampling,”inIEEEWorkshoponStatisticalSignal
IEEETrans.SignalProcess.,vol.71,pp.4441–4456,2023. Processing(SSP),2014,pp.268–271.
[27] G. Choi, J. Park, N. Shlezinger, Y. C. Eldar, and N. Lee, “Split- [51] D. Schuhmacher, B.-T. Vo, and B.-N. Vo, “A consistent metric for
KalmanNet: A robust model-based deep learning approach for state performance evaluation of multi-object filters,” IEEE Trans. Signal
estimation,”IEEETrans.Veh.Technol.,vol.72,no.9,pp.12326–12331, Process.,vol.56,no.8,pp.3447–3457,2008.
2023. [52] A.S´cibiorandF.Wood,“Differentiableparticlefilteringwithoutmod-
[28] X. Chen, H. Wen, and Y. Li, “Differentiable particle filters through ifyingtheforwardpass,”arXivpreprintarXiv:2106.10314,2021.
conditional normalizing flow,” in IEEE International Conference on [53] M. Zhu, K. Murphy, and R. Jonschkowski, “Towards differentiable
InformationFusion(FUSION),2021. resampling,”arXivpreprintarXiv:2004.11938,2020.
[29] R. Jonschkowski, D. Rastogi, and O. Brock, “Differentiable particle [54] M. U¨ney, B. Mulgrew, and D. E. Clark, “A cooperative approach to
filters: End-to-end learning with algorithmic priors,” arXiv preprint sensorlocalisationindistributedfusionnetworks,”IEEETrans.Signal
arXiv:1805.11122,2018. Process.,vol.64,no.5,pp.1187–1199,2015.
[30] X.Ma,P.Karkus,D.Hsu,andW.S.Lee,“Particlefilterrecurrentneural [55] I.Buchnik,G.Sagi,N.Leinwand,Y.Loya,N.Shlezinger,andT.Rout-
networks,”inAAAIConferenceonArtificialIntelligence,vol.34,no.04, tenberg, “GSP-KalmanNet: Tracking graph signals via neural-aided
2020,pp.5101–5108. kalmanfiltering,”IEEETrans.SignalProcess.,vol.72,pp.3700–3716,
[31] F. Gama, N. Zilberstein, R. G. Baraniuk, and S. Segarra, “Unrolling 2024.
particles: Unsupervised learning of sampling distributions,” in IEEE [56] T.Raviv,S.Park,O.Simeone,Y.C.Eldar,andN.Shlezinger,“Adaptive
International Conference on Acoustics, Speech and Signal Processing and flexible model-based AI for deep receivers in dynamic channels,”
(ICASSP),2022,pp.5498–5502. IEEEWirelessCommun.,2024,earlyaccess.
[32] F. Gama, N. Zilberstein, M. Sevilla, R. G. Baraniuk, and S. Segarra, [57] S. Sa¨rkka¨ and L. Svensson, Bayesian filtering and smoothing. Cam-
“Unsupervised learning of sampling distributions for particle filters,” bridgeuniversitypress,2023,vol.17.
IEEETrans.SignalProcess.,vol.71,pp.3852–3866,2023.