Leveraging Chemistry Foundation Models to Facilitate Structure Focused
Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and
Materials Design
Nathaniel H. Park,1,* Tiffany J. Callahan,1 James L. Hedrick,1 Tim Erdmann,1 and Sara Capponi1
1IBM Research–Almaden, 650 Harry Rd. San Jose, CA 95120
*Corresponding author. Email: npark@us.ibm.com
Abstract
Molecular property prediction and generative design via deep learning models has been the subject
of intense research given its potential to accelerate development of new, high-performance
materials. More recently, these workflows have been significantly augmented with the advent of
large language models (LLMs) and systems of LLM-driven agents capable of utilizing pre-trained
models to make predictions in the context of more complex research tasks. While effective, there
is still room for substantial improvement within the agentic systems on the retrieval of salient
information for material design tasks. Moreover, alternative uses of predictive deep learning
models, such as leveraging their latent representations to facilitate cross-modal retrieval
augmented generation within agentic systems to enable task-specific materials design, has
remained unexplored. Herein, we demonstrate that large, pre-trained chemistry foundation models
can serve as a basis for enabling semantic chemistry information retrieval for both small-
molecules, complex polymeric materials, and reactions. Additionally, we show the use of
chemistry foundation models in conjunction with image models such as OpenCLIP facilitate
unprecedented queries and information retrieval across multiple characterization data domains.
Finally, we demonstrate the integration of these systems within multi-agent systems to facilitate
structure and topological-based natural language queries and information retrieval for complex
research tasks.
Introduction
The advent of accessible large language models (LLMs) and their respective capabilities has
enabled a dramatic expansion of LLM-based workflows across the domains of chemistry and
materials. These include applications of chatbots or multi-agent workflows to metal-organic
frameworks,1,2 inorganic materials,3 automated synthesis,4 organic synthesis,5 and protein
discovery.6 Besides the underlying language models of the agents and their respective interactions
to produce a final output, the critical components of these workflows are the tools made available
to individual agents. This can include to access to retrosynthetic software,5 predictive models or
computational tools,3 or writing task-specific code for automated experimentation4—all of which
provide salient information needed by an LLM or LLM-powered agent to complete a given task.
All of these are closely related to and frequently used in combination with standard retrieval
augmented generation (RAG) wherein a natural-language query is run against a corpus of chunked
text documents within a vector database to retrieve the most relevant documents and provide them
to the LLM as part of its prompt.7–9 As with other types of agent tools, this approach is effective
in enhancing LLM performance for answering a particular question, it does require significantoptimization of the choice of embedding model, chunk size, LLM choice and its available context
window, question rephrasing, use of metadata filtering, and similarity metrics used for chunk
retrieval.8,9,9
In many of the agent-based workflows noted above, the tools used may utilize structural
information in the form of a SMILES string or molecular formula to perform a computation or
property prediction,6,10,11 but not directly in RAG operations focused on structural similarity.
Retrieval of relevant information based on structural similarity, whether focused small-molecules,
materials, or reactions, is one of the most critical tasks during any research endeavor. Therefore,
enabling researchers to use natural-language accompanied by chemical language to query
structure-linked information resources would provide a powerful augmentation of LLM
capabilities. Achieving structure-based RAG operations necessitates a vector-based representation
of a given compound or material whereby similarity queries may be conducted. While there are a
significant variety of molecular fingerprints available through cheminformatics packages such as
RDKit,12 the use chemistry language foundation models is highly attractive due to their potential
dual use within an agentic system as an embedding model and a predictive tool. Despite these
advantages, there exist only a few reports on the utilization of deep learning models as embedding
models for similarity-based searches—limited largely to organic molecules and inorganic
materials.13,14 And while the LLM systems have been demonstrated to search APIs such as
PubChem or the general internet in the performance of chemistry-focused tasks,5 these search
interfaces do not offer the potential breadth of both query options or structural similarity features
that may be possible when using a chemistry foundation model to embed structural information
and with relevant metadata. Additionally, queries based on other modalities, such as images of
characterization data, have largely not been evaluated within an LLM-based agentic system nor
within traditional chemistry database interfaces in spite of the immense benefit such features would
confer to researchers. To overcome these limitations and dramatically expand the capabilities of
multi-agent systems for materials design and development tasks, we surmised that: 1) a single,
high-performance chemistry foundation model could be adapted to facilitate semantic structure
searches across small-molecules, polymers, and reactions; 2) such a model could be used in
combination with an image embedding model to enable multimodal semantic queries; and 3) these
systems can be integrated within multi-agent systems to provide richer context while performing
materials design tasks.
Before building a complete semantic structure search-based RAG pipeline within multi-
agent workflows, it was imperative to evaluate the effectiveness of both the latent representations
generated by the chemistry language foundation models for structural similarity queries. There
exist many potentially competent embedding models to support semantic structural queries. Prior
work focused on the use of ChemBERTa models to enable similarity searches for functional
analogues of drugs,14–16 however there are other potential models that could work including Mol-
BERT,17 Mol2Vec,18 GPT-MolBERTa,19 or ChemGPT.20 For our investigation, we selected
MoLFormer as a baseline embedding model which has recently been shown to be highly
performant across numerous benchmarks from MoleculeNet,21 effective at capturing molecular
similarity, as well as potentially learning 3D spatial relationships from SMILES inputs.22 To
evaluate structural similarity queries, we compiled a focused dataset of ~2.5M organic small-
molecules using open-source data and historical data from our own work.23–26 The SMILES
representations were canonicalized and vector embeddings were computed for each compound
using MoLFormer before insertion into a Milvus vector database (see Supporting Information for
more details).27 Using this dataset we evaluated the ability of using MoLFomer embeddings tofacilitate retrieval of structurally similar compounds from sample queries on known
organocatalysts for ring-opening polymerization (ROP) using the Milvus vector search capabilities
(Fig. 1).28,29
For the result of each query, several metrics relating to the structural similarity based on
either MoLFormer embedding distance (cosine, Euclidean, Fig.1) or fingerprints for each
compound generated using RDKit (Tanimoto, RDKit, MACC, and Dice, Fig 1). Each score
represents the similarity between the result compound and the query compound. For 1,8-
diazabicyclo[5.4.0]undec-7-ene (DBU) 1a, the top result was itself followed by several closely
related structural analogues based on the cosine or Euclidean similarity (Fig. 1A). Interestingly,
the second rank compound 1b was given much lower similarity scores from most fingerprint
derived metrics despite differing from 1a only in the saturation of the imine (Fig. 1A). For 2a, a
organocatalyst for ROP designed with the aid of generative AI,29 we find that while the original
query was not present in the dataset, most of the core functional group features—an cyclic 5-
membered guanidine and an aromatic ring—based on visual inspection (Fig. 2B). As with 1a, most
of the fingerprint-based similarity metrics indicate the result compounds as highly dissimilar, with
exception of MACC (Fig. 2B). Overall, these examples further substantiate prior results22
demonstrating MoLFormer embeddings do indeed capture relevant structural information and
similarity despite differing substantially in several cases from fingerprint-based metrics.
A B
1a 2b
1b 2c
1c 2d
1d 2e
1e 2f
Cosine Euclidean Tanim oto R D Kit M A C C Dice Cosine Euclidean Tanim oto R D Kit M A C C Dice
Me Me
N N N Bn N Me N N Me N NH+
N N N Me Me N N Me N N
H
1a 1b 1c 2a 2b 2c
Query Query
Cl
Me
N H N+ Me N N O S O
N N N N
Me N N H Me O N NH
N
N N
H H
1d 1e 2d 2e 2f
Figure 1. A) Heatmap of similarity metrics for the results of the query of 1a against a small-molecule collection. Structures of each
of the top five closest compounds are shown beneath the heatmap. B) Heatmap of similarity queries of 2a against a small-molecule
collection. Structures of each of the top five closest compounds. For the similarity metrics displayed in the heatmap, cosine andEuclidean refer to the respective distance between the query embedding and the result embedding converted into similarity scores.
Tanimoto, RDKit, MACC, and Dice refer to similarity scores from molecular fingerprints generated using the RDKit package for
the query and returned compound. For all metrics, a value closer to 1 indicates high similarity whereas a value closer to zero
indicates lower similarity. See Supplementary Information for details.
With the success of the similarity queries using MoLFormer embeddings in conjunction
with a Milvus vector store, we sought next to expand search capabilities beyond simple small-
molecule queries in order to maximize the diversity of search options available to LLM agents. In
natural language, differences in word vector embeddings have been demonstrated to capture
lexical relationships between different words, with classic examples of King – Man + Woman =
Queen or Paris – France + Poland = Warsaw capturing the relationships of gender and capital
city.30 Given that MoLFormer is a chemistry language model based on SMILES syntax, we
surmised differences between vector embeddings between two compounds within the base model
latent space should correspond to differences in structure (Fig. 1B). Moreover, this implies that
combinations of vector embeddings from compounds of interest can be used to identify novel
analogues based on functional group features of both compounds as well as utilize scalar property
values to manipulate the magnitude of the vector—facilitating unique similarity queries to access
different sets of data. To implement these ideas, we first created an identical collection of ~2.5M
molecules used in Fig. 1 where instead each vector was now scaled by the compounds’
corresponding molecular weight. This was tested on guanidine 3a, which was used as a search
input to either the molecular weight scaled collection, or the base collection used previously (Fig.
2A). The results in Fig. 2A demonstrated the success of both approaches in retrieving structurally
relevant compounds, however, the metadata filtering is limited to identical top results within
certain molecular weight ranges (3f, Fig. 2A)—potentially requiring different filtering strategies
depending on the desired output.
In addition to utilizing the vector magnitude to influence similarity search results, we also
investigated whether addition and subtraction of vectors corresponding to different functional
groups resulted in similar lexical relationships as observed with examples like: King – Man +
Woman = Queen. This approach was evaluated on two catalysts 4a and 4b where their
corresponding vector embeddings were subjected such operations. With 4a the subtraction of the
vector corresponding to dimethylurea and the addition of dimethylthiourea resulted in a vector
embedding that provided corresponding thioureas (4c) when queried against the database (Fig.
2B). With 4b the results of similar operation were less obviously successful, but this is anticipated
behavior based on a limited collection of molecules (Fig. 2B). Nonetheless, the results did provide
similar examples with fluorine containing amines. Finally, the vector embeddings of 4a and 4b
were averaged and queried against the database with the top results bearing the structural features
of both the parent molecules (Fig. 2C).A
NH
Me Me
N N
Scale Vector Embedding H H Metadata Filter by MW
3a
Query
NH O O
Me NH2
Me
N
H
N
H
Me H2N
N
H
N
H
Me
3b 3c 3f
MW: 58.05 MW: 89.05 MW: 117.09
Scale Factor: 50 Scale Factor: 100 MW Filter: 50, 100
S O O
Cl
N N
NH2 nPr
N N N N N N
nPr
H H H H H H H H
3d 3g
MW: 181.04 MW: 358.30
Scale Factor: 250 MWFilter: 250
O S
H H H H H H
N N N nPr Bn N N N
nPr N N N N N N Bn
H H H H H H
O 3e 3h S
MW: 372.32 MW: 528.03
Scale Factor: 500 MW Filter: 500
B
Me Me
N ±Vector Embeddings F
O ±Vector Embeddings S F
H2N F
F3C 4N
H
a
N
H
O S
F3C N
H
4cN
H
OMe
4N
b
Me
N
Me
Me
CF3
F 4dF
Me Me Me Me Me
N N N N
H H H H
Average Vector Embeddings
N
O N
N O
N N N H N NH
Me H H
O
4e 4f 4f
Rank 1 Rank 4 Rank 5
Figure 2. A) Comparison of results obtained for queries using run using metadata filters compared to normalized vector embeddings
scaled by a target molecular weight. B) Schematic depiction of search results obtained through mathematical manipulation of query
vector embeddings through addition-subtraction of corresponding functional group vectors or averaging two queries.
Having demonstrated the effectiveness of using MoLFormer vector embeddings on a variety
of different similarity search strategies, we next sought to evaluate their feasibility towards
polymers and reactions. MoLFormer was trained compounds with SMILES strings containing less
than 200 tokens were filtered from the pre-training dataset, precluding large SMILES strings which
may potentially represent macromolecules.22 Outside of biological materials, such as peptides,
nucleic acids or large natural products, macromolecules and other complex materials are oftenexhibit stochastic features relating to their structure and are hence poorly represented by discrete,
one-dimensional SMILES strings. This problem is frequently circumvented in deep learning
models through a reduction in the complexity by treating polymers as single, discrete repeating
units via SMILES strings with variable attachment points denoted by the asterisk character.31,32
While this approach tends to overlook the stochastic nature of materials as well as neglecting to
account for end groups or more complex polymer topologies, it does produce systems capable of
providing predicted values for polymer properties within these restrictions.31,32 As the tokenizer
for MoLFormer covers the entire SMILES grammar,22,33 including the use of special tokens such
as variable attachment points, we hypothesized that it could also serve as a suitable source of
embeddings to facilitate queries based on both polymer structural and topological similarity. It
was unclear, however, as to whether these latent embeddings will capture the same level of
molecular similarity given SMILES fragments with asterisks were unlikely to have been part of
any pre-training set for the model. With this in mind, we leveraging dot-separated SMILES strings
containing asterisks to represent repeat units or other polymer components to evaluate their
suitability for use in semantic queries for polymeric materials.
To evaluate the use of MoLFormer embeddings applied to polymers, we first curated a set of
polymer data from both open literature and our own historical data totaling in ~2.5M SMILES
strings representing predominantly homopolymers.34–39 As with the small-molecule embeddings,
these were inserted into a Milvus collection, against which similarity queries may be run. First, we
tested the embeddings on very simple queries such as polyimide 5a and polystyrene 5b, both of
which returned sets of highly similar compounds (Fig. 3A). Averaging the embeddings of 5a and
5b enabled the search to identify features of both, consistent with results observed on the small-
molecules and again indicative of the ability of MoLFormer embeddings to capture semantic
structural relationships. Next, we investigated the ability to query for two similar block copolymers
6a and 6b, identical in their monomer and end-group components, but differing only in block order
(Fig. 3B). The spatial distance—both cosine and Euclidean—of the embedding vectors generated
for each SMILES string is small, but suggestive that the model does indeed differentiate between
the two. As a consequence, queries for both 6a and 6b give the identicals first ranked result, 6c
(Fig. 3B). It is only at the 5th ranked polymer where differences are enough to provide distinct
results (6d and 6e, Fig. 3B).A
O O O O
O
5aonly n 5aonly n
N N N N Se
O
O O O O
n n
5a 5c 5b 5d
Average vector embeddings 5a& 5b
O N n O m
B(OH)2
5e
B
O O
O O
H
O O O O Bn O
Et O O H
Me Et n m n Me m
6c
6a Rank 1 (6a& 6b)
O O
O O
H Me Me
O O O O H S Ph H
Et O O O MeO O O
Me n Et m n CN CO2m Me S Et Et n
6b
6d 6e
Distance (L2) 6a& 6b:0.24 Rank 5 (6a) Rank5(6b)
Distance(Cosine)6a& 6b: 0.029
Figure 3. A) Depiction of top results for a example query for a polyimide (5a) and polystyrene (5b) for their individual MoLFormer
embeddings, providing results 5c and 5d, respectively, as well as the average of their individual embeddings providing result 5e.
The result 5d is rank 2 as the top result of the query for 5b was 5b. The brackets drawn for 5e are arbitrary, as it is unclear from the
source data exactly which copolymer topology the SMILES string was intended to represent. See Supplementary Figures for full
results of all queries. B). Depiction of influence of block order in co-polymers 6a and 6b on query results.
As with polymers, reactions—represented using reaction SMILES syntax40—has not been
examined directly using MoLFormer and despite the existence of many transformer-based models
for reactions,33,41 we were interested in probing the versatility of MoLFormer embedding model
reaction similarity queries. Based on our results with polymeric systems, we anticipated reactions
to behave in a similar manner given a similar syntactical construction. The data for the evaluation
was sourced from an open ~2M reaction dataset sourced from the USPTO used previously in
transformer models for reactions, open publications, and historical experimental polymerization
reaction data.42 In addition to querying on whole reactions, we also focused on queries involving
one to two reagents (Fig. 4A). In these examples, 4a, 4b, and their average vector where each used
to search the database and provided reaction examples where structurally related reagents were
used or the query molecules themselves (Fig. 4A). Finally, testing the order of 4a and 4b, either in
a dot separated substrate series or on either side of angle brackets (>>) separating substrates and
products produced markedly different results, indicating the importance of order within the
reaction SMILES sequence. The difference in results when angle brackets are used is
understandable considering that this would indicate two very different reactions when the order isreversed. However, unlike with polymers, the distinction between the ordering of reactants or
products in reaction SMILES has less relevance unless some order of addition is being encoded.
In total, these results indicate that the base model of MoLFormer is well-suited to capture structural
relationships among small-molecules, polymers, and reactions.
A
R R
NH R = H NMe2 N NH2 N N NMe2 Me R =NO2
O + N N
F3C R = H MeO OMe N N N Me R = NH2
Query: 4a Query: 4b Query: average of 4a and 4b
B O O N N
Me O
N H NH2 N H MN e H2N Bn N H N H
Query: 4a.4b and 4a>>4b Query: 4b.4a
N
H
N O
+ O NH
C N
F3C N N
F3C
Query: 4b>>4a
Figure 4. A) Top results of queries against 2.5 M USPTO reaction collection of embedded reaction SMILES for either 4a, 4b, or
their average. B) Comparison of top results from concatenated query SMILES strings of 4a and 4b either dot separated, such as in
4a.4b, or angle-bracket separated, such as in 4a>>4b. Other reagents and solvents are excluded for clarity.
The demonstration of MoLFormer as an effective embedding model beyond simple small
molecules prompted us to examine use of chemical structural embeddings with additional
modalities. Association of chemical structure with different types of characterization data is a
highly important task for any project within chemistry and materials domains. While
characterization data is available in many formats, we opted to leverage the data pre-plotted and
saved as images as input for creation of their corresponding vector embeddings. The ability to
query available characterization data by either image or structure would be a powerful addition to
both traditional data infrastructures and RAG pipelines as well. To implement this task would
typically require either some form contrastive learning or latent fusion, necessitating fine-tuning
of an existing model or training of an additional model. Instead, we opted to test alternative
strategies where the chemical components of a particular piece of characterization data would be
embedded using MolFormer while the image components would be embedded using OpenCLIP
image model (see Supplementary Information for details).43,44 While CLIP models for images can
also co-embed text captions, it is not anticipated these embeddings would be able to effectively
capture nuances in chemical structure as a chemistry focused model like MoLFormer. Instead, the
text embedder of the OpenCLIP model can be used to embed information captured as a natural
language caption, which can be automatically generated from the metadata from the
characterization data files, adding additional natural language query capabilities. To evaluate this
approach, we compiled a small dataset of labeled images of 1H, 13C, and 19F NMR spectra (see
Supplementary Information). The labels for the spectra included their corresponding chemical
components as SMILES strings and natural language captions generated from the spectra
metadata. From this dataset we can test both image-based and structure-based queries on the
embedded dataset with excellent results across numerous collection organization strategies (Fig.5). Despite there being only minor differences between the image features plots—colors,
whitespace, axes, and image size—of the embedded plots, OpenCLIP can effectively differentiate
between different characterization techniques without additional metadata filtering on the query.
This implies that the differentiation is likely occurring due to shape of the plotted traces or spectra
(Fig. 5).
O
HO OH
N
O O
Me Me
O O F
Me
F 7b
F F
F
7a
Rank 1
QueryNMR Spectra
Distance (L2): 0.0109
OH OH
Me
O O F
F
F F
F
7c
Rank 2
Distance (L2): 0.0112
Figure 5. Example of NMR spectra similarity search using OpenClip image embeddings. Each spectrum is labeled with the
structure of its corresponding compound. The original query is the dataset and as expected, the first result returned. Hence, the
result ranks reflect those obtained after filtering out the top result.
The use of chemistry LLMs such as MoLFormer and image models like OpenCLIP, in combination
with post-embedding pooling and compression strategies, has facilitated the creation of a variety
of vector stores which in turn can support a large variety of similarity queries. This alone would
make a powerful addition to traditional database architectures and search capabilities, yet it’s the
connection of such vector databases within a larger RAG framework coupled to LLM-powered
agent workflows which can offer the potential of significant time savings in complex research
tasks requiring the merging and summarization of data retrieved from complex structure, image,
and natural language-based queries. In this regard, access to different vector stores and their
respective embedding strategies is provided to LLM agents as tools which may be used in thecontext of a particular task. We utilized the LangGraph library (v 0.1.19)45 to develop a hierarchical
multi-agent adaptive self-reflective RAG system (Fig. 6), which takes a question as input and
outputs, the solution as a formatted research-style report. As shown in Fig. 6, the hierarchical
workflow is directed by a supervisor agent which leverages query analysis46 to adaptively route a
user’s question to the correct worker agent. The system consists of four worker agents, which
specialize in small molecules, polymers, chemical reactions, or NMR spectra. Each of these
workers autonomously implement a multi-agent-based self-reflective RAG workflow based off
of.47 As detailed in Fig. 6, the workflow consists of several steps including retrieving and
evaluating documents, generating responses using retrieved documents, revising user input to
improve retrieved documents, checking generated responses for hallucinations and verifying each
response completely addresses a user’s question. Each step of this process is autonomously
directed by the worker agents, which runs until all checks are passed. Once all checks are passed,
the answer is sent to the report generation tool, which summarizes the agent’s findings (Figs 7-8).
This report also summarizes the vector store retrieved content used by the agent to generate the
report in order to improve the overall transparency of the workflow.
Figure 6. Overview of the Hierarchical Multi-Agent RAG System. An adaptive supervisor agent leverages query analysis to select
the correct self-reflective RAG worker agent given a user query. Each self-reflective multi-agent worker autonomously performs
the following tasks: (1) retrieves documents from a vector store; (2) evaluates the relevance of each retrieved document. If all
documents are found to be irrelevant, a new set of documents is retrieved; (3) generates a response to the input using relevant
retrieved documents; (4) inspects the generated response for hallucinations. If a response contains hallucinations, a new response
is generated, and (5) determines whether the response fully addresses the user’s question. If the response does not fully answer a
user’s question, an alternative version of the user’s question is generated, and the process begins again. When all tasks are
successfully completed, a response is sent to the report generation tool, which summarizes the answer to the user’s question. Self-
reflective RAG workflow adapted in part from Langraph documentation.48
This workflow was demonstrated on a variety of tasks, each of which was able to
effectively utilize the structure and/or image-based vector store tools in combination with other
natural language-based vector databases from historical manuscripts. Example reports generatedfrom these tasks are shown in Fig. 7 and Fig. 8 below. For Fig. 7, the agentic system was provided
the following query:
“I am interested in aromatic polyethers similar in structure and function to
O=S(C1=CC=C(O[*:1])C=C1)(C2=CC=C([*:2])C=C2)=O. Please find analogues and
comment on potential means of synthesis and thermal properties.”
Figure 7. Example of a report generated for a question routed to the self-reflective RAG agent for polymers.As shown in Fig. 7, the agent was able to successfully complete the user query using the
self-reflective RAG agent for polymers returning two relevant, similar analogs and provided
additional content to justify the selection of these analogs stating:
“These structures feature sulfur-oxygen bonds and aromatic rings, similar to the original
compound. For synthesis, aromatic polyethers can typically be synthesized through nucleophilic
substitution reactions or using organocatalysts for polymerization. The thermal properties of these
compounds can vary based on their specific structures and substituents, but generally, aromatic
polyethers exhibit good thermal stability due to the strength of the aromatic bonds.”
For Fig. 8, the agentic system was asked to identify similar 13C NMR spectra to the one of
a diethanolamine-based carbonate monomer precursor.49 As Fig. 6, the question was routed to the
multi-modal NMR agent who reviewed and summarized the characterization data of four 13C NMR
spectra images (i.e., three retrieved images and the image corresponding to the input) summarizing
the location of the peaks in these images that similar to the input NMR spectra. The report also
provides a visualization of the input image, which makes enables easier review and interpretation
of the provided results. Both reports were reviewed by a domain expert confirmed who confirmed
the validity of the findings. In this example, the agent was not instructed to filter the RAG results
if they contained the original query, yet it still successfully found several spectra of closely related
diethanolamine-based cyclic carbonates or their precursors.Figure 8. Example of a report generated for a question routed to the multi-modal self-reflective RAG agent for NMR spectra.
Conclusion
The accelerated development of novel materials and catalysts necessitates dramatic improvement
of human–AI interactions to facilitate both effective co-designs as well as realistic
implementations within an experimental setting. LLM-based multiagent systems integrated with
chat interfaces hold significant promise to become useful assistants for researchers operating in
laboratory settings. Here, we have demonstrated that chemical foundation models coupled with
powerful image recognition models can facilitate unique types of multimodal structural or
architectural focused queries on small-molecules, polymers, and reactions. This represents a
significant enhancement of search capabilities not typically found in traditional database systems
for chemistry research. Moreover, the coupling of these systems within a LLM-based multiagent
system can provide a significant advantage for reducing the time needed to retrieve and summarizerelevant multimodal structure-based information commonly required across all material research
projects.
Code and Data Availability
Data and code will be made available upon final publication.
Conflict of Interest
A patent application on aspects of multimodal semantic search for materials has been filed by IBM
(application number 18/798970, inventors: N.H.P, T.E., and J.L.H). All authors are employees of
IBM.
References
(1) Zheng, Z.; Zhang, O.; Borgs, C.; Chayes, J. T.; Yaghi, O. M. ChatGPT Chemistry Assistant
for Text Mining and the Prediction of MOF Synthesis. J. Am. Chem. Soc. 2023, 145 (32),
18048–18062. https://doi.org/10.1021/jacs.3c05819.
(2) Kang, Y.; Kim, J. ChatMOF: An Artificial Intelligence System for Predicting and Generating
Metal-Organic Frameworks Using Large Language Models. Nat. Commun. 2024, 15 (1),
4705. https://doi.org/10.1038/s41467-024-48998-4.
(3) Chiang, Y.; Hsieh, E.; Chou, C.-H.; Riebesell, J. LLaMP: Large Language Model Made
Powerful for High-Fidelity Materials Knowledge Retrieval and Distillation. arXiv June 2,
2024. https://doi.org/10.48550/arXiv.2401.17244.
(4) Boiko, D. A.; MacKnight, R.; Kline, B.; Gomes, G. Autonomous Chemical Research with
Large Language Models. Nature 2023, 624 (7992), 570–578. https://doi.org/10.1038/s41586-
023-06792-0.
(5) M. Bran, A.; Cox, S.; Schilter, O.; Baldassari, C.; White, A. D.; Schwaller, P. Augmenting
Large Language Models with Chemistry Tools. Nat. Mach. Intell. 2024, 6 (5), 525–535.
https://doi.org/10.1038/s42256-024-00832-8.
(6) Ghafarollahi, A.; J. Buehler, M. ProtAgents: Protein Discovery via Large Language Model
Multi-Agent Collaborations Combining Physics and Machine Learning. Digit. Discov. 2024,
3 (7), 1389–1409. https://doi.org/10.1039/D4DD00013G.
(7) Li, H.; Su, Y.; Cai, D.; Wang, Y.; Liu, L. A Survey on Retrieval-Augmented Text Generation.
arXiv.org. https://arxiv.org/abs/2202.01110v2 (accessed 2024-08-16).
(8) Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.;
Yih, W.; Rocktäschel, T.; Riedel, S.; Kiela, D. Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks. arXiv April 12, 2021.
https://doi.org/10.48550/arXiv.2005.11401.
(9) Zhao, P.; Zhang, H.; Yu, Q.; Wang, Z.; Geng, Y.; Fu, F.; Yang, L.; Zhang, W.; Jiang, J.; Cui,
B. Retrieval-Augmented Generation for AI-Generated Content: A Survey. arXiv June 21,
2024. https://doi.org/10.48550/arXiv.2402.19473.
(10) Buehler, M. J. Generative Retrieval-Augmented Ontologic Graph and Multiagent
Strategies for Interpretive Large Language Model-Based Materials Design. ACS Eng. Au
2024, 4 (2), 241–277. https://doi.org/10.1021/acsengineeringau.3c00058.(11) Ghafarollahi, A.; Buehler, M. J. AtomAgents: Alloy Design and Discovery through
Physics-Aware Multi-Modal Multi-Agent Artificial Intelligence. arXiv July 13, 2024.
https://doi.org/10.48550/arXiv.2407.10022.
(12) Boldini, D.; Ballabio, D.; Consonni, V.; Todeschini, R.; Grisoni, F.; Sieber, S. A.
Effectiveness of Molecular Fingerprints for Exploring the Chemical Space of Natural
Products. J. Cheminformatics 2024, 16 (1), 35. https://doi.org/10.1186/s13321-024-00830-3.
(13) Yang, Y.; Liu, M.; Kitchin, J. R. Neural Network Embeddings Based Similarity Search
Method for Atomistic Systems. Digit. Discov. 2022, 1 (5), 636–644.
https://doi.org/10.1039/D2DD00055E.
(14) Kosonocky, C. W.; Feller, A. L.; Wilke, C. O.; Ellington, A. D. Using Alternative
SMILES Representations to Identify Novel Functional Analogues in Chemical Similarity
Vector Searches. Patterns 2023, 4 (12), 100865. https://doi.org/10.1016/j.patter.2023.100865.
(15) Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa: Large-Scale Self-Supervised
Pretraining for Molecular Property Prediction. arXiv October 23, 2020.
https://doi.org/10.48550/arXiv.2010.09885.
(16) Ahmad, W.; Simon, E.; Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa-2:
Towards Chemical Foundation Models. arXiv September 4, 2022.
https://doi.org/10.48550/arXiv.2209.01712.
(17) Li, J.; Jiang, X. Mol-BERT: An Effective Molecular Representation with BERT for
Molecular Property Prediction. Wirel. Commun. Mob. Comput. 2021, 2021 (1), 7181815.
https://doi.org/10.1155/2021/7181815.
(18) Jaeger, S.; Fulle, S.; Turk, S. Mol2vec: Unsupervised Machine Learning Approach with
Chemical Intuition. J. Chem. Inf. Model. 2018, 58 (1), 27–35.
https://doi.org/10.1021/acs.jcim.7b00616.
(19) Balaji, S.; Magar, R.; Jadhav, Y.; Farimani, A. B. GPT-MolBERTa: GPT Molecular
Features Language Model for Molecular Property Prediction. arXiv October 10, 2023.
https://doi.org/10.48550/arXiv.2310.03030.
(20) Frey, N. C.; Soklaski, R.; Axelrod, S.; Samsi, S.; Gómez-Bombarelli, R.; Coley, C. W.;
Gadepally, V. Neural Scaling of Deep Chemical Models. Nat. Mach. Intell. 2023, 5 (11),
1297–1305. https://doi.org/10.1038/s42256-023-00740-3.
(21) Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse, C.; Pappu, A. S.; Leswing,
K.; Pande, V. MoleculeNet: A Benchmark for Molecular Machine Learning. Chem. Sci.
2018, 9 (2), 513–530. https://doi.org/10.1039/C7SC02664A.
(22) Ross, J.; Belgodere, B.; Chenthamarakshan, V.; Padhi, I.; Mroueh, Y.; Das, P. Large-Scale
Chemical Language Representations Capture Molecular Structure and Properties. Nat. Mach.
Intell. 2022, 4 (12), 1256–1264. https://doi.org/10.1038/s42256-022-00580-7.
(23) Gallarati, S.; Gerwen, P. van; Laplaza, R.; Vela, S.; Fabrizio, A.; Corminboeuf, C.
OSCAR: An Extensive Repository of Chemically and Functionally Diverse Organocatalysts.
Chem. Sci. 2022, 13 (46), 13782–13794. https://doi.org/10.1039/D2SC04251G.
(24) Haas, B.; Hardy, M.; V, S. S. S.; Adams, K.; Coley, C.; Paton, R.; Sigman, M. Rapid
Prediction of Conformationally-Dependent DFT-Level Descriptors Using Graph Neural
Networks for Carboxylic Acids and Alkyl Amines. ChemRxiv February 23, 2024.
https://doi.org/10.26434/chemrxiv-2024-m5bpn.
(25) Zheng, J. IUPAC/Dissociation-Constants: V1.0, 2022.
https://doi.org/10.5281/zenodo.7236453.(26) Wu, J.; Wan, Y.; Wu, Z.; Zhang, S.; Cao, D.; Hsieh, C.-Y.; Hou, T. MF-SuP-pKa: Multi-
Fidelity Modeling with Subgraph Pooling Mechanism for pKa Prediction. Acta Pharm. Sin.
B 2023, 13 (6), 2572–2584. https://doi.org/10.1016/j.apsb.2022.11.010.
(27) Wang, J.; Yi, X.; Guo, R.; Jin, H.; Xu, P.; Li, S.; Wang, X.; Guo, X.; Li, C.; Xu, X.; Yu,
K.; Yuan, Y.; Zou, Y.; Long, J.; Cai, Y.; Li, Z.; Zhang, Z.; Mo, Y.; Gu, J.; Jiang, R.; Wei, Y.;
Xie, C. Milvus: A Purpose-Built Vector Data Management System. In Proceedings of the
2021 International Conference on Management of Data; SIGMOD ’21; Association for
Computing Machinery: New York, NY, USA, 2021; pp 2614–2627.
https://doi.org/10.1145/3448016.3457550.
(28) Kiesewetter, M. K.; Shin, E. J.; Hedrick, J. L.; Waymouth, R. M. Organocatalysis:
Opportunities and Challenges for Polymer Synthesis. Macromolecules 2010, 43 (5), 2093–
2107. https://doi.org/10.1021/ma9025948.
(29) Park, N. H.; Manica, M.; Born, J.; Hedrick, J. L.; Erdmann, T.; Zubarev, D. Y.; Adell-
Mill, N.; Arrechea, P. L. Artificial Intelligence Driven Design of Catalysts and Materials for
Ring Opening Polymerization Using a Domain-Specific Language. Nat. Commun. 2023, 14
(1), 3686. https://doi.org/10.1038/s41467-023-39396-3.
(30) Vylomova, E.; Rimell, L.; Cohn, T.; Baldwin, T. Take and Took, Gaggle and Goose, Book
and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning. arXiv
August 13, 2016. https://doi.org/10.48550/arXiv.1509.01692.
(31) Xu, C.; Wang, Y.; Barati Farimani, A. TransPolymer: A Transformer-Based Language
Model for Polymer Property Predictions. Npj Comput. Mater. 2023, 9 (1), 1–14.
https://doi.org/10.1038/s41524-023-01016-5.
(32) Kuenneth, C.; Ramprasad, R. polyBERT: A Chemical Language Model to Enable Fully
Machine-Driven Ultrafast Polymer Informatics. Nat. Commun. 2023, 14 (1), 4099.
https://doi.org/10.1038/s41467-023-39868-6.
(33) Schwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Hunter, C. A.; Bekas, C.; Lee, A. A.
Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction.
ACS Cent. Sci. 2019, 5 (9), 1572–1583. https://doi.org/10.1021/acscentsci.9b00576.
(34) Ma, R.; Luo, T. PI1M: A Benchmark Database for Polymer Informatics. J. Chem. Inf.
Model. 2020, 60 (10), 4684–4690. https://doi.org/10.1021/acs.jcim.0c00726.
(35) Aldeghi, M.; W. Coley, C. A Graph Representation of Molecular Ensembles for Polymer
Property Prediction. Chem. Sci. 2022, 13 (35), 10486–10498.
https://doi.org/10.1039/D2SC02839E.
(36) Volgin, I. V.; Batyr, P. A.; Matseevich, A. V.; Dobrovskiy, A. Yu.; Andreeva, M. V.;
Nazarychev, V. M.; Larin, S. V.; Goikhman, M. Ya.; Vizilter, Y. V.; Askadskii, A. A.; Lyulin,
S. V. Machine Learning with Enormous “Synthetic” Data Sets: Predicting Glass Transition
Temperature of Polyimides Using Graph Convolutional Neural Networks. ACS Omega 2022,
7 (48), 43678–43691. https://doi.org/10.1021/acsomega.2c04649.
(37) Hu, J.; Li, Z.; Lin, J.; Zhang, L. Prediction and Interpretability of Glass Transition
Temperature of Homopolymers by Data-Augmented Graph Convolutional Neural Networks.
ACS Appl. Mater. Interfaces 2023, 15 (46), 54006–54017.
https://doi.org/10.1021/acsami.3c13698.
(38) Yang, J.; Tao, L.; He, J.; McCutcheon, J. R.; Li, Y. Machine Learning Enables
Interpretable Discovery of Innovative Polymers for Gas Separation Membranes. Sci. Adv.
2022, 8 (29), eabn9545. https://doi.org/10.1126/sciadv.abn9545.(39) Ohno, M.; Hayashi, Y.; Zhang, Q.; Kaneko, Y.; Yoshida, R. SMiPoly: Generation of a
Synthesizable Polymer Virtual Library Using Rule-Based Polymerization Reactions. J.
Chem. Inf. Model. 2023, 63 (17), 5539–5548. https://doi.org/10.1021/acs.jcim.3c00329.
(40) Daylight Theory: SMILES.
https://www.daylight.com/dayhtml/doc/theory/theory.smiles.html (accessed 2024-08-21).
(41) Bran, A. M.; Schwaller, P. Transformers and Large Language Models for Chemistry and
Drug Discovery. arXiv October 9, 2023. https://doi.org/10.48550/arXiv.2310.06083.
(42) Schwaller, P.; Probst, D.; Vaucher, A. C.; Nair, V. H.; Kreutter, D.; Laino, T.; Reymond,
J.-L. Mapping the Space of Chemical Reactions Using Attention-Based Neural Networks.
Nat. Mach. Intell. 2021, 3 (2), 144–152. https://doi.org/10.1038/s42256-020-00284-w.
(43) Ilharco, G.; Wortsman, M.; Carlini, N.; Taori, R.; Dave, A.; Shankar, V.; Namkoong, H.;
Miller, J.; Hajishirzi, H.; Farhadi, A.; Schmidt, L. OpenCLIP, 2021.
https://doi.org/10.5281/zenodo.5143773.
(44) Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.;
Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; Sutskever, I. Learning Transferable Visual
Models From Natural Language Supervision. In Proceedings of the 38th International
Conference on Machine Learning; PMLR, 2021; pp 8748–8763.
(45) Langchain-Ai/Langgraph, 2024. https://github.com/langchain-ai/langgraph (accessed
2024-08-21).
(46) Jeong, S.; Baek, J.; Cho, S.; Hwang, S. J.; Park, J. C. Adaptive-RAG: Learning to Adapt
Retrieval-Augmented Large Language Models through Question Complexity. arXiv March
28, 2024. https://doi.org/10.48550/arXiv.2403.14403.
(47) Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; Hajishirzi, H. Self-RAG: Learning to Retrieve,
Generate, and Critique through Self-Reflection. arXiv October 17, 2023.
https://doi.org/10.48550/arXiv.2310.11511.
(48) langgraph/examples/rag/langgraph_crag_local.ipynb at main · langchain-ai/langgraph.
GitHub. https://github.com/langchain-
ai/langgraph/blob/main/examples/rag/langgraph_crag_local.ipynb (accessed 2024-08-21).
(49) Hedrick, J. L.; Piunova, V.; Park, N. H.; Erdmann, T.; Arrechea, P. L. Simple and
Efficient Synthesis of Functionalized Cyclic Carbonate Monomers Using Carbon Dioxide.
ACS Macro Lett. 2022, 11 (3), 368–375. https://doi.org/10.1021/acsmacrolett.2c00060.
(50) ibm/MoLFormer-XL-both-10pct · Hugging Face.
https://huggingface.co/ibm/MoLFormer-XL-both-10pct (accessed 2024-08-12).Supplementary Information
Materials and Methods
Vector Database Setup. All SMILES embeddings were computed using the ibm/MoLFormer-XL-
both-10pct model available through HuggingFace using canonicalized SMILES (with the
exception of the USPTO dataset, which was used as is).50 For the example queries in Figures 1-4,
the vector embeddings for each compound were L2 normalized prior to insertion into a Milvus lite
vector database using and HNSW index and the L2 distance as the metric. For the multimodal
examples (Fig. 5) SMILES embeddings were not L2 normalized and used in a Milvus standalone
vector database using IVF_FLAT index and the L2 distance metric. For spectra containing more
than one identifiable compound, including the NMR solvent, their corresponding embeddings were
averaged prior to insertion. Image embeddings were computed using OpenCLIP via the Langchain
library. The model used was ViT-g-14 with the laion2b_s34b_b88k checkpoint. Image embeddings
were stored in a separate Milvus collection, using an IVF_FLAT index and L2 distance metric, and
cross-referenced with their corresponding compound embeddings.
Similarity Metrics. For small molecule and polymer example queries, similarity metrics were
computed on the basis of both the MoLFormer embeddings and molecular fingerprints computed
using RDKit. Cosine similarity was computed by subtracting the cosine distance between the query
and result embeddings (measured using the SciPy library) from 1. Euclidean similarity (E ) was
s
computed by the following equation:
E = 1 / (1 + E ) (1)
s d
Where E is the Euclidean distance between the query and result embeddings. Tanimoto and Dice
d
similarities were computed using the corresponding query and result Morgan fingerprints with a
radius of 2 and a dimension of 2048. RDKit similarity was computed using the built-in RDKit
fingerprints of the query and compound. MACCS similarity was computed using the MACCS
Keys fingerprints of the query and the compound using the RDKit package.
Language Agent Network. As noted in the main text, the multi-agent framework was assembled
using the Langgraph library. The supervisor agent utilized GPT-4o mini model while other agents
used either llava 7b or llama3.1 8b models. All Milvus collections were instantiated as separate
vector stores with customized embedding functions with either MoLFormer or OpenCLIP, prior to
connection with LLM agents as retrievers. Full code for agent network will be released in both
subsequent drafts of preprint and final publication.