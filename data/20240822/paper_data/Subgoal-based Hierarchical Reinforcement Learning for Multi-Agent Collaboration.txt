Subgoal-based Hierarchical Reinforcement
Learning for Multi-Agent Collaboration
ChengXu,ChangtianZhang,YuchenShi,RanWang,ShihongDuan,
YadongWan,andXiaotongZhang*†‡
Abstract
Recentadvancementsinreinforcementlearninghavemadesignificantimpacts
acrossvariousdomains, yettheyoftenstruggleincomplexmulti-agentenviron-
ments due to issues like algorithm instability, low sampling efficiency, and the
challenges of exploration and dimensionality explosion. Hierarchical reinforce-
ment learning (HRL) offers a structured approach to decompose complex tasks
into simpler sub-tasks, which is promising for multi-agent settings. This paper
advances the field by introducing a hierarchical architecture that autonomously
generateseffectivesubgoalswithoutexplicitconstraints,enhancingbothflexibil-
ityandstabilityintraining. Weproposeadynamicgoalgenerationstrategythat
adaptsbasedonenvironmentalchanges. Thismethodsignificantlyimprovesthe
adaptability and sample efficiency of the learning process. Furthermore, we ad-
dress the critical issue of credit assignment in multi-agent systems by synergiz-
ingourhierarchicalarchitecturewithamodifiedQMIXnetwork,thusimproving
overallstrategycoordinationandefficiency. Comparativeexperimentswithmain-
stream reinforcement learning algorithms demonstrate the superior convergence
speedandperformanceofourapproachinbothsingle-agentandmulti-agenten-
vironments,confirmingitseffectivenessandflexibilityincomplexscenarios. Our
codeisopen-sourcedat:https://github.com/SICC-Group/GMAH.
Keywords: multi-agent collaboration; subgoal learning; reinforcement learning;
hierarchicallearning.
1 Introduction
Reinforcementlearning(RL)hasundergonerevolutionaryadvancementsinrecentyears,
markedsignificantlybytheintegrationofQ-learning[1]withdeepneuralnetworks,re-
sultinginthedevelopmentofDeepQ-Networks(DQN)[2]. Thelandmarksuccesses
*This work was supported in part by the National Natural Science Foundation of China (NSFC)
under Grant 62101029, Guangdong Basic and Applied Basic Research Foundation under Grant
2023A1515140071,andinpartbytheChinaScholarshipCouncilAwardunderGrant202006465043and
202306460078.(Correspondingauthors:ShihongDuanandYadongWan)
†TheauthorsarewithSchoolofComputerandCommunicationEngineering,UniversityofScienceand
TechnologyBeijing. TheyarealsowithShundeInnovationSchool,UniversityofScienceandTechnology
Beijing. ChengXuisalsowithSchoolofElectricalandElectronicEngineering,NanyangTechnological
University. RanWangisalsowithSchoolofComputerScienceandEngineering, NanyangTechnologi-
calUniversity(email: xucheng@ustb.edu.cn; changtizh14264@163.com; shiyuchen199@sina.com; wan-
gran423@foxmail.com;duansh@ustb.edu.cn;wyd@ustb.edu.cn;zxt@ies.ustb.edu.cn).
‡This work has been submitted to the IEEE for possible publication. Copyright may be transferred
withoutnotice,afterwhichthisversionmaynolongerbeaccessible.
1
4202
guA
12
]AM.sc[
1v61411.8042:viXrahighlight RL’s capability to address intricate challenges in various domains, includ-
ing autonomous driving [3], robotic navigation [4], and complex strategic games [5].
Additionally,theadventofmodelslikeOpenAI’sChatGPT[6],whichleveragesrein-
forcementlearningtoalignwithhumanpreferences,furtheremphasizestheexpanding
scopeandsignificanceofRL.
Despitethesestrides,thedeploymentofRLinpracticalscenariosishinderedbythe
curseofdimensionality[7],aphenomenonwheretheincreaseinstateoractionspace
complexity exponentially complicates the training process. This complexity is par-
ticularlypronouncedinmulti-agentsettingswhereexplorationbecomesaformidable
challengeduetothevaststatespacesandtheinherentlimitationsinagents’exploration
strategies. Thesporadicandgoal-orientednatureofrewardsinsuchenvironmentsfur-
therexacerbatesthisissue,leadingtoprolongedandinefficientlearningphases.
Tomitigateissuesstemmingfromsparserewards,researchershaveinnovatedwith
intrinsicrewardsthatderivefromenvironmentalcuesandtask-specificgoalstoenrich
the agents’ learning context. Techniques like density-based exploration guidance [8]
and curiosity-driven exploration [9] aim to encourage comprehensive environmental
interactions. Additionally, Hierarchical Reinforcement Learning (HRL), inspired by
Sutton’soptionsframework[10],hasintroducedamulti-layeredapproachwherehigh-
levelpoliciesdictatemacro-actions,streamliningthedecision-makingprocessandmit-
igatingthecurseofdimensionality.
DespitetheeffectivenessofHRLinsingle-agentscenarios,itsapplicationtocom-
plex,multi-agentenvironments,whicharemorereflectiveofreal-worldconditions,re-
mainsnascent.Multi-agentreinforcementlearning(MARL)[11]introducesadditional
complexities,includingnon-stationarityandintricaterewarddistributionmechanisms
among agents, which can obscure training objectives. The adaptation of value-based
methodslikeDQN[2],andpolicy-basedmethodssuchasProximalPolicyOptimiza-
tion(PPO)[12]andDeepDeterministicPolicyGradient(DDPG)[13]toMARLset-
tings shows promise, particularly in cooperative scenarios where the goal is to maxi-
mizecollectiveoutcomes. Furtherenhancingagentexplorationandsampleefficiency
inthesesettingsareapproacheslikeHindsightExperienceReplay(HER)[14],which
leverage past unsuccessful experiences by recontextualizing them as successful in al-
ternative scenarios. This approach, along with training on universal value function
approximators(UVFA)[15],significantlyimproveslearningefficiency.
IncorporatingintrinsicmotivationsintoRL,akintonaturalexploratorybehaviorsin
humans,representsapivotalshifttowardsmoreadaptableandresilientlearningagents.
Innovative strategies like Exploration By Random Network Distillation (RND) [16]
and Never Give Up (NGU) [17] harness these intrinsic motivations to propel explo-
ration,emphasizingengagementwithlessfamiliarstatestofosteramorecomprehen-
sive understanding of the environment. The concept of intrinsic reward reshaping,
centraltothisresearch,advancesthenotionofautonomousgoal-settinginRL.Bydy-
namicallyadjustingintrinsicrewardsbasedonenvironmentalinteractionsandlearned
experiences, our approach facilitates a more nuanced and effective learning process,
showing promising results in capturing complex exploratory and exploitative behav-
iorswithinastructuredrewardframework.
This paper significantly contributes to the field by introducing a task tree-based
hierarchical architecture that integrates effectively within multi-agent systems. Our
approach demonstrates through empirical analysis that it can outperform traditional
methods in multi-agent settings, thus validating the potential of hierarchical learning
architecturesincomplexscenarios. Thekeycontributionsofthispaperareasfollows:
2• Task Tree-Based Subgoal Generation Method: We innovate the subgoal space
design to enhance its comprehensibility and relevance for low-level policies,
which simplifies intrinsic reward function designs and improves policy perfor-
mance.
• Adaptive Subgoal Generation Strategy: We propose a dynamic subgoal adjust-
mentmethodthatrespondstosignificantenvironmentalfeaturechanges,ensur-
ingamorerobustandefficientlearningprocess.
• Goal Mixing Network Fine-Tuning: We introduce a novel mixing network that
fine-tunesthehigh-levelpolicyviaajointgoalvaluefunctiontrainedwithglobal
rewards,extendingthehierarchicalframeworktomulti-agentenvironmentsand
addressingcomplexissuessuchasdimensionalityandrewarddistribution.
Thestructureofthispaperisoutlinedasfollows:Section2reviewstherelatedwork
inthefield,providingacontextfortheresearchandhighlightingsignificantcontribu-
tions from previous studies. Section 3 elaborates on the subgoal-based hierarchical
reinforcementlearningapproachdesignedformulti-agentcollaboration, detailingthe
theoretical framework and methodology. Section 4 presents the experimental results
andprovidesadiscussiononthefindings,assessingtheeffectivenessandimplications
of the proposed method. Section 5 summarizes the key points of the paper, drawing
conclusionsandsuggestingpotentialavenuesforfutureresearch.
2 Related Work
2.1 Multi-agentReinforcementLearning
Multi-agent reinforcement learning (MARL) can be regarded as a partially observ-
able Markov decision process (POMDP), generally abstracted as the tuple G =<
I,A,S,O,P,r,γ > [18], where I = I ,...,I represents n agents, A is the action
1 n
spacewithagentschoosinganactiona ∈Aatanygiventimet,formingajointaction
t
a. S denotesthestatespace,Otheobservationspace,P :S×A→S thestatetransi-
tionfunctionP(s |s ,a )indicatingtheprobabilitythatagentI transitionstostate
t+1 t t i
s fromstates bytakingactiona ,r istherewardfunctionr(s,a) : S×A → R,
t+1 t t
and γ ∈ [0,1) is the discount factor. Each agent i has an action-observation history
(trajectory)τi =<o ,a ,...,o ,a >,whichisbasedontheagent’spolicyπi(a |τ).
0 0 T T t
In the realm of multi-agent reinforcement learning (MARL), centralized and de-
centralizedlearningframeworksdominate. Centralizedmethods[19]implementauni-
fied policy to direct all agents’ collective actions, while decentralized methods [20]
permit each agent to optimize its reward independently. Bridging these approaches,
CentralizedTrainingwithDecentralizedExecution(CTDE)[21]usesglobalstatein-
formation for training but allows agents to act independently during execution based
on local observations. Prominent CTDE methodologies like COMA [22] and MAD-
DPG[23]utilize anactor-criticstructureto trainacentralizedcritic withglobalstate
inputs. Similarly,algorithmssuchasQTRAN[24],VDN[25],andQMIX[26]apply
valuedecompositiontorepresentthejointQ-functionthroughindividualagents’local
Q-functions,settingbenchmarksinMARL.
Althoughsuccessfulinsingle-agentsettings,ProximalPolicyOptimization(PPO)
hasseenlimiteduseinMARL.ChaoYuetal.[27]attributethistoPPO’slowersam-
pleefficiencyandchallengesinadaptingsingle-agenttuningstrategiestomulti-agent
3contexts. TheirresearchextendsPPOtoMARLbymodifyingthepolicydistribution
and centralized value function to depend on global rather than local states. Influen-
tialfactorsforPPO’seffectivenessinMARLincludeGeneralizedAdvantageEstima-
tion(GAE)[28],observationnormalization,gradientclipping,valuefunctionclipping,
layernormalization,andReLUactivationfunctionswithorthogonalinitialization,col-
lectivelyknownasMAPPO.
Furthermore,QMIX[26]creativelymergesindividualagents’localvaluefunctions
usingamixingnetworkthatincorporatesglobalstatedataduringtrainingtoenhance
performance. It posits that to decentralize policies effectively, complete decomposi-
tionofthejointQ-functionisunnecessary;ensuringthattheglobalargmaxoperation
onthejointaction-valuefunctionQ alignswithindividualargmaxoperationson
total
each agent’s Q-function Q suffices. QMIX also improves operational efficiency by
i
scalingthecomputationofQ linearlywiththenumberofagents. Thisefficiency
total
is achieved through the design of its DRQN-based networks and a mixing network
configured with a nonlinear mapping network in its final layer, ensuring robust inte-
grationofstateinformationandoptimizingcomputationalloadincomplexmulti-agent
scenarios.
2.2 HierarchicalReinforcementLearning
Hierarchical Reinforcement Learning (HRL) represents a sophisticated branch of RL
that organizes agent policies into distinct layers, which can be individually trained
usingvalue-basedorpolicygradientmethods. Bystructuringthepolicyhierarchically,
HRL allows for differentiated control over decision-making across varying temporal
scales,facilitatingcomplextaskdecompositionintomanageablesubtasksorpredefined
skills.
The utility of HRL is especially pronounced in tasks characterized by prolonged
durationsanddelayedrewards.However,theinherentnon-stationarityoftrainingenvi-
ronments,wherehigherlayerpoliciesdependontheevolvingpoliciesoflowerlayers,
poses significant challenges. Such dynamics lead to fluctuating transition functions,
complicatingthestabilizationofoptimalpolicies. Addressingthesetrainingcomplex-
ities,Nachumetal. introducedHierarchicalReinforcementLearningwithOff-policy
Correction(HIRO)[29].HIROincorporatesatwo-layeredpolicystructurewherehigh-
levelpoliciesdesignategoalsg ∈ Rds atfixedintervalsc,andlow-levelpoliciesopti-
mizearewardfunctionbasedontheproximitytothesegoals. Thehigh-levelpolicies
samplegoalsandaggregaterewardsovercsteps,whilelow-levelpoliciesareupdated
using transitions influenced by these sampled goals, incorporating off-policy correc-
tionstoenhancesampleefficiency. HIRO’sstrategyofselectingoptimalgoalsfroma
setofcandidateswithinanoriginalstatetransitiondomainhasdemonstratedsuperior
efficiencyinenvironmentssuchasMuJoCo[30],significantlyoutperformingbaselines
likeFuNs[31]andVIME[32].
Parallel to HIRO, the Hierarchical Actor-Critic (HAC) [33] system tackles non-
stationaritybyenablingtheconcurrenttrainingofmultiplepolicylayers. Oncelower-
level tasks achieve near-optimal solutions, HAC stabilizes the training of upper-level
policies. This method employs Hindsight Experience Replay (HER) [14] at each hi-
erarchical level to effectively learn from both successful and unsuccessful subgoals,
promotingfasterconvergenceinbothgridworldandsimulatedrobotictasks.
Stochastic Neural Networks for Hierarchical RL (SNN4HRL) [34] represents a
skill-based approach within HRL that addresses sparse rewards and complex, long-
term tasks. SNN4HRL initially focuses on skill acquisition through a pre-training
4Figure1: TheoverallGMAHstructure.
phaseusingsurrogaterewards,followedbytrainingahigh-levelpolicytodeploythese
skills based on the current state. Employing Stochastic Neural Networks (SNNs),
thismethodenablesdiverseskillrepresentationandencouragesexplorationthroughan
information-theoreticregularization. DeepSuccessorReinforcementLearning(DSR)
[35]offersauniqueperspectivebyutilizingSuccessorRepresentation(SR),whichsep-
arates the value function into successor mapping and reward prediction components.
Thisdecompositionallowsforrapidadaptationtochangesinrewardstructuresandthe
strategicidentificationofsubgoals,showcasinganotherinnovativefacetofhierarchical
learninginreinforcementlearningenvironments.
3 Subgoal-based HRL
Thissectiondelineatesthecommunication-restrictedmulti-agentcooperativeenviron-
ment explored in this study. The environment features N agents, each limited to its
own local observations oi. These agents operate concurrently, making independent
t
decisions and interacting with the environment. They each receive individual local
rewards ri, but lack the capability to communicate or access the rewards of others.
t
The environment stops when all agents collectively complete the final task T, which
is structured into a sequence of subtasks g . These subtasks can be accomplished in-
i
dividuallyorthroughcooperativeefforts,presentingvariousroutestoachievethefinal
objective.
3.1 TheGeneralArchitecture
Weintroduceanefficientsubgoal-basedmulti-mgenthierarchicalreinforcementlearn-
ing approach, designated as GMAH, which is visualized in Fig. 1. This method seg-
mentseachagent’spolicyintotwolevels:ahigh-levelpolicyπhandalow-levelpolicy
πl. The high-level policy in the GMAH method decomposes the main task into sim-
pler, attainable subgoals based on prior knowledge. These subgoals are crucial steps
for completing the task and are executed by agents under the strategic direction of
thehigh-levelpolicy,whichutilizesenvironmentalrewardsfornavigation. Simultane-
5Figure2: Atypicaldiagramoftasktree.
ously,thelow-levelpolicy,motivatedbyintrinsicrewards,guidesagentsinachieving
thesesubgoals.
ThecentralcomponentoftheGMAHmethodisthetrainingofsubgoalvaluefunc-
tion that utilizes global rewards to enhance the efficiency of the high-level policy in
allocatingsubgoalsamongagents. Thishierarchicalarchitecturepromotesbettercoor-
dinationandtaskexecutionacrosstheagentcollective. Furthersectionswilldetailthe
hierarchical architecture applied to each agent in the GMAH algorithm, highlighting
advancementsoverconventionalhierarchicalmodels,suchasthetask-treemethodfor
subgoalgeneration. Additionally,anadaptivegoalgenerationstrategyisintroducedto
refinethehierarchicalexecutionlogic,alongsideamethodforfine-tuningthegoalmix-
ingnetwork.Thisapproachaidsinexpandingthesophisticatedhierarchicalframework
towidermulti-agentenvironments.
3.2 Task-TreeStyleSubgoalGeneration
HierarchicalreinforcementlearningtypicallydefinesthegoalspaceGasasubspaceof
thestatespaceS orobservationspaceO,i.e.,G⊆S orG⊆O. Thelow-levelreward
isdefinedassomedistancemetricbetweenthegoalspaceandstatespace[29,36]oras
abinaryfunction[8]. Thesedefinitionsrepresentanticipatedfuturestatesagentsaim
to reach. However, due to the inherent uncertainty in neural network outputs, high-
levelpolicynetworksoftenstruggletogeneratesubgoalrepresentationsthatalignwith
environmentaldesigns,causingissuesinmanyscenarios. Toaddressthecomplexities
ofabstractsubgoaldefinitions,thispaperproposesatask-treestylesubgoalgeneration
methodthateschewsgoalspacesinfavorofimpartingactualsignificancetosubgoals,
trainingthehigh-levelpolicytogeneratesubgoalsinatask-treestructure.
Thecoreideaofthetask-treestylesubgoalgenerationmethodistoreplacecomplex
abstractgoalspaceswithsimple,explicitsets. Assumetheenvironment’sultimatetask
is T , which, upon analysis of the task content and environment, is deemed decom-
E
6(a) (b)
(c)
Figure 3: (a) High-Policy Network Model. (b) Hierarchical architecture applied by
singleagentinGMAHalgorithm. (c)NetworkModelofGMAH.
posable into N simple tasks. Completing T requires achieving N independent and
E
distinctsimpletasksorreachingcertainintermediarystates,referredtoassubgoalsT.
i
These N distinct subgoals form a set T , with each element representing a subgoal.
N
The high-level policy’s decision-making objective is to select a subgoal from this set
for the low-level policy to achieve. This task-tree structure is depicted in Fig. 3-(a),
starting from the root node (the environment’s initial state) and proceeding through
subgoals selected by the high-level policy, with the edges weighted by the extrinsic
rewardsobtainedduringtheprocess,repeatinguntiltheterminalstates (environment
T
terminationorcompletionofT ). Thehigh-levelpolicy’sultimategoalistocomplete
E
T , generally by maximizing environmental rewards, i.e., J(πh) = maxR(τ). This
E
tree-structuredapproachsignificantlysimplifieshigh-levelpolicydecision-makingby
reducing it to a problem of planning subgoals or solving for the optimal path in an
N-arytree.
DespitethedynamicprogrammingapproachtypicallyusedforsolvingN-arytree
pathproblems,itisimpracticalinareinforcementlearningcontextduetothestochas-
tic nature of environment initialization and the non-stationarity of any given state or
observation within the task tree. Thus, neural networks are still employed to model
this strategy. The high-level policy network structure, as shown in Fig. 3-(a), out-
puts a probability distribution over the set of subgoals, from which a subgoal g is
sampled, i.e., g ∼ πh(o ),g ∈ T . This design transforms the high-level policy’s
t N
decision-making from a state-space to subgoal-set mapping, drastically reducing the
dimensionalityofthepolicy’soutputsandtheoveralllearningdifficulty.
7Theclarityofobjectivessetbythehigh-levelpolicyenhancestheexecutioncapa-
bilitiesofthelow-levelpolicy.
Inthehierarchicalarchitecture, thelow-levelpolicyisdrivenbyintrinsicrewards
thatareconditionedontheagent’sperformancetowardsachievingthesesubgoals. We
use rule-based evaluations to determine the achievement of subgoals, but with a sig-
nificantenhancement. Thetask-treestylesubgoalgenerationstrategyimpartsconcrete
meaningtoeachsubgoal,simplifyingtheevaluationruleswhileachievinggreaterac-
curacy than distance metrics. Our model modifies the standard indicator function by
introducing a time-decaying factor to the intrinsic reward function for the low-level
policy,whichisdefinedasfollows:
(cid:40)
1−β t ifgetg
R tl(o t,a t|g)=
0
TM
else
(1)
Here, T represents the maximum time steps per episode, t is the current time step,
M
andβisadiscountfactorthatadjuststherewardbasedonthetimetakentoachievethe
subgoalg.
The hierarchical structure applied to each agent aligns with traditional models,
maintaining a consistent high-level to low-level policy framework as shown in Fig.
3-(b)and(c). Ateachtimestept,thehigh-levelpolicyoutputsprobabilitiesforeach
subgoalbasedontheobservationo andsamplesasubgoalg . Subsequently, ateach
t t
time step t′, the low-level policy generates actions a based on the current obser-
t′
vation o and the subgoal g , executing these actions. The maximum time steps
t′ t′
between subgoal generations is defined as C. Once the subgoal is achieved or the
action sequence exceeds C time steps, a new subgoal g is sampled, and transi-
t+c
(cid:80)
tion data ⟨o ,g , R ,o ⟩ is collected for training the high-level policy, with
t t t:t+c t+c
⟨o ,g ,rl,o ⟩gatheredforthelow-levelpolicy.
t t t+1
The approach leverages a task-tree style subgoal generation method that employs
prior knowledge to construct a higher-level abstraction model. This model predeter-
mines a set of subgoals, which both the high-level and low-level policies rely on for
training, independent of each other. This decouples the training of the high and low-
levelpolicies,addressingakeyinstabilityissueinhierarchicallearning.Bytrainingthe
low-levelpolicyfirsttoasatisfactoryperformancelevelbeforetrainingthehigh-level
policy, this approach simplifies the overall training process and significantly reduces
thetrainingcomplexity,layingasolidfoundationforextendingthehierarchicalframe-
worktomulti-agentenvironments.
3.3 AdaptiveGoalGenerationStrategy
The GMAH algorithm incorporates a robust hierarchical architecture optimized for
multi-agent interactions. One significant challenge within this architecture is manag-
ing the goal update interval, which affects both the high-level and low-level policy.
Thisintervalencompassesthetimestepsbetweenthedecisionpointsofthehigh-level
strategy to generate new subgoals and the duration required by the low-level policy
to achieve these subgoals. In response, this paper introduces an adaptive goal update
strategyfortheGMAHalgorithmthatfeaturesflexibleupdateintervalsandproactive
goalupdates.
8(a) (b)
(c) (d)
Figure4: (a)Trajectoryexampleofagentc-stepinteraction. (b)AdjacencyConstraint
on Goal Space of HRAC. Goal Relabel on Trajectory of agent c-step interaction: (c)
relabeltheabstractsubgoal,and(d)relabelsubgoalofGMAH.
3.3.1 FlexibleUpdateIntervals
Mostmethods,suchasHIROandHRAC[29,37],adoptafixedinterval. HIROdefines
the goal space as a subspace of the state space and treats the goal update interval as
a hyperparameter c, where the high-level policy generates a subgoal g every c time
t
steps.ThesubgoalsinHIROrepresenttheexpectedrelativedistancebetweenthetarget
state the agent aims to reach and its current state. Additionally, HIRO designed a
goal transition function h(s , g , s ) = s + g − s , which updates the
t t t+1 t t t+1
subgoalbasedontheagent’sstatetransitions. Fundamentally,theagent’sgoalwithin
c time steps is a fixed expected position. HIRO uses the Frobenius norm to measure
the distance between the state and subgoal, with an intrinsic reward function being a
binaryfunction. Bysettinganappropriatethresholdε,arewardof1isassignedifthe
distanceislessthanthisthreshold, otherwise, arewardof0isgiven. However, since
HIRO does not explicitly constrain the "distance" represented by subgoals generated
by the high-level policy, nor does it ensure that the agent reaches the goal within the
time interval c and receives the corresponding reward, the data produced by failed c-
step interactions cannot be used for training. These ineffective data scenarios can be
categorizedasfollows:
91. Theagentnearlyreachesthegoalafterctimestepsbutdoesnotmeetthethresh-
olddistanceforgoalcompletion.
2. Theagentisfartherfromthegoalafterctimesteps,evenmorethanatthetime
beforecsteps.
ExamplesofthesetrajectoriesareshowninFig.4-(a),wheretrajectoryarepresents
thefirstscenario,andtrajectoriesb,c,anddrepresentthesecondscenario. Trajectory
a points out the direction for the agent to complete the goal, while trajectory b, c,
anddarenot. Butforthereinforcementlearningmethod,thevalueofthesefourfailed
trajectoriesarethesame.HRACmakesacriticalenhancementoverHIRObyimposing
a K-step neighborhood constraint. This constraint limits the goals generated by the
high-level policy to the space within a K Manhattan distance around the agent, thus
makingiteasierfortheagenttoachievethegoalwithinctimesteps. Theconstraints
onthegoalspacebyHRACareillustratedinFig. 4-(b).
Besides neighborhood constraints, HRAC also utilizes an important technique to
increasesampleefficiency: hindsightexperiencereplay(HER).Forfailedexploration
trajectories,itstillsamplestheirstatetransitions⟨s ,a ,rl,s ,g ⟩,butitre-labelsthe
t t t t+1 t
statetheagentactuallyreachesafterctimestepsasthesubgoalforthatbatchoftran-
sition data g = s , and calculates the corresponding intrinsic reward r . The data
(cid:98)t t+c (cid:98)t
⟨s ,a ,r(cid:98)l,s ,g ⟩arethenusedfortrainingthelow-levelpolicy. However,although
t t t t+1 (cid:98)t
HRACimposesconstraintsontherangeofsubgoals,therebyenhancingthelow-level
policy’s ability to achieve them and subsequently improving algorithm performance,
it still lacks an effective strategy to ensure the low-level policy can achieve the con-
strainedsubgoals. Thispaperadoptsaflexiblegoalupdateintervalcombinedwiththe
techniqueofhindsightexperiencereplay. Thisapproachdiscardsthefixedgoalupdate
interval, assessingwhetherthesubgoalisachievedwitheachstepofinteractionafter
thelow-levelpolicyreceivesasubgoal. Ifthesubgoalisachieved,thehigh-levelpol-
icyispromptedtoupdatethesubgoal. Ifitisnotachieved,thesubgoalisproactively
updatedafterreachingthemaximumupdateintervalc.
Inaddition,thispaperalsousesHERtoimprovesampleefficiency. However,due
to the different definition of target space, the logic of re-marking agent trajectory of
GMAH algorithm is different from that of HIRO, HRAC and other algorithms. In
order to use the failed trajectory to learn, it is necessary to judge whether the agent
has completed other sub-targets in the interaction process between the agent and the
environmentinadvance. Pre-recordthesub-goalsactuallyachievedandcalculatethe
correspondingintrinsicrewards. ThedifferencebetweenmethodssuchasGMAHand
HIROusingHERforremarkingisshowninFig. 4-(c)and(d).
3.3.2 ProactiveGoalUpdating
Thispaperexplorestheadaptivegenerationofsubgoalswithinahierarchicalreinforce-
mentlearningframework,advocatingthathigh-levelpolicyshoulddynamicallyupdate
subgoalsnotonlyuponthecompletionofexistinggoalsorreachingpredeterminedin-
tervals but also in reaction to significant environmental changes. A key challenge is
determiningtheprecisemomentsforthehigh-levelpolicytoupdatesubgoals.
Thus, we propose a dual-stage decision-making process. The initial phase em-
ploys an Auto-Encoder that is considerably more compact than the high-level policy
model. This Auto-Encoder consists of an Encoder, which condenses the input into a
low-dimensional feature vector f, and a Decoder, which reconstructs the input from
10Figure5: Auto-EncoderwithSuccessorFeatureCorrection.
thisvector. Trainingisbasedonminimizingthedifferencesbetweenactualinputsand
theirreconstructions,guidedbythelossfunctiondetailedinEquation2.
ℓ(θ ,θ )=∥ϕ (s )−ϕ (sˆ )∥2 (2)
e d θd t θd t+1
Inspired by DSR, the feature vector f output by the Encoder network is input into a
fullyconnectedlayerparameterizedbyω,outputtingamappingfromstatetoreward.
Thismappingrepresentstheestimatedvaluebasedonthestate. Throughthisdesign,
theEncodernetwork’soutputfeaturevectorsareusedwiththeagent’sstatetransition
datatotrainthejointnetwork⟨θ ,w⟩usingthelossfunctionshowninEquation3,with
e
thecompletemodelstructureillustratedinFig. 5:
ℓ(θ
e,ω)=(cid:13)
(cid:13)ϕ θe(s t)·ω−R
th(cid:13) (cid:13)2
(3)
Thefeaturevectorsoutputbythedual-trainedEncodernetworkrepresentthelow-
dimensional embedding of states. In the actual GMAH training, this type of autoen-
coder is pre-trained. During the high-level policy training of the agent, the agent’s
observationsateachmomentareinputintotheencoder,recordingthefeaturevectors
f = ϕ (s ),f = ϕ (s )forconsecutivetimesteps. Thecosinesimilaritybe-
t θe t t+1 θe t+1
tweenthesefeaturevectorsiscalculatedasshowninEquation4. Cosinesimilarity,a
measureofthedegreeofsimilaritybetweenthedirectionsoftwovectors,isdetermined
bythecosineoftheanglebetweenthevectors:
f ·f
s= t t+1 (4)
∥f ∥·∥f ∥
t t+1
Whensfallsbelowathresholdε ,itisdeemedthattheEncodernetworkhascap-
1
turedasignificantchangeinstate,promptingthesecondstageoftheprocess. During
thisstage, theobservationsatthecurrentandpreviousmoments, o ando , arein-
t t+1
put into the high-level policy network to obtain two subgoal probability distributions
p andp . TheKLdivergencebetweenthesedistributionsiscalculatedasshownin
t t+1
Equation5:
d =H(p (g),p (g))−H(p
(g))=(cid:88)
p (g)log(
p t(g))
(5)
KL t t+1 t t p (g)
t+1
g
Then,anewsubgoalissampledfromthecurrentprobabilitydistributionandcommu-
nicatedtothelow-levelpolicy.
Thissetupallowsthehigh-levelpolicytoaccuratelydetectenvironmentalshiftsand
dynamicallyadjustsubgoalsbasedonstatesuccessorfeatures,thusgreatlyenhancing
11Algorithm1Adaptivesubgoalupdatepolicy
Input: Encoderϕ ,Decoderϕ ,high-levelpolicyπh,low-levelpolicyπl,Agentset
θe θd
I ,Targetupdateintervalc
N
Output: Subgoaldistributionp
1: Initializeθ e,θ d,ε 1,ε 2
2: fortin0,...,T −1do
3: foragentiinI N do
4: ift mod c≡0oragentachievegthen
5: Subgoalsareobtainedfromobservations,i.e.,g ←πh(o t)
6: endif
7: Performtheactiona t ←π l(o t,g),obtainnewobservationo t+1,rewardr t
8: Collectdata⟨o t,a t,g,o t+1,r t⟩
9: Calculatethesimilarityofstatefeaturevectors
10: s= ϕθe(st)·ϕθe(st+1)
∥ϕθe(st)·∥∥ϕθe(st+1)∥
11: ifsimilaritys<ε 1then
12: CalculatetheKLdivergenceofthehigh-levelpolicydistribution
13: d KL(p t ∥p t+1)=(cid:80) gp t(g)log( pp tt +( 1g () g) )
14: ifd KL >ε 2then
15: updateg′ ←πh(o t,I i)
16: endif
17: endif
18: UpdateEncoder-Decoder
19: endfor
20: endfor
theadaptabilityofthehierarchicalarchitectureandreducingpotentialsubgoalconflicts
inmulti-agentenvironments. Thecomprehensivealgorithmicworkflowisdetailedin
Algorithm1.
3.4 Fine-TuningofGoalMixingNetwork
In communication-limited multi-agent cooperation scenarios, a critical issue is the
credit assignment problem, which concerns quantifying an individual agent’s contri-
butiontothecollectiveoutcome. Typically,agentsdevelopanactionvaluefunctionor
policyfunction, selectingactionsthatmaximizeeithertheactionvalueorcumulative
reward. Eachagentoperatesbasedonlocalobservationstosecureindividualrewards,
whiletheoverarchingaimistomaximizeglobalrewards. Optingforactionsthaten-
hance individual gains may, however, compromise the total global rewards. In sce-
narioswhereonlyindividualrewardsareaccessible,itbecomeschallengingforagents
to make decisions that optimize global outcomes. This dilemma is a central focus of
researchinmulti-agentreinforcementlearning.
Thispaperseekstointegratetheproposedhierarchicalarchitecturewithestablished
multi-agentreinforcementlearningstrategiestoformulateacomprehensiveGMAHal-
gorithm, thereby extending the advantages of hierarchical designs to the multi-agent
context. WeintegratetheconceptofQMIXwithhierarchicalarchitecture. Toimple-
mentthisidea,QMIXusesaspecialmixingnetworktoensurethatQ hasamono-
tot
tonicconstraintoverQ . ThegeneralapproachistoapplyaDRQNnetworktoeach
a
agent, constructing a special mixing network that receives the outputs of all DRQN
12(a) (b)
Figure6: (a)GoalMixingNetwork. (b)Hyper-NetandWeight-Network.
networksasinputsandoutputsthejointactionvaluefunctionQ .
tot
Themixingnetworkisatwo-layerfeedforwardneuralnetworkthattakestheout-
putfromeachagent’snetworkasinputandmonotonicallymixesthemtoproducethe
jointactionvaluefunctionQ . Toensurethemonotonicityconstraint,theweightsof
tot
the mixing network (excluding biases) are restricted to be non-negative. This allows
the mixing network to arbitrarily closely approximate any monotonic function [38],
with the weights of the mixing network generated by separate hypernetworks. Each
hypernetworktakesthestates asinputandgeneratestheweightsforonelayerofthe
t
mixing network. Each hypernetwork consists of a single linear layer followed by an
absolutevalueactivationfunctiontoensuretheoutputvectorisnon-negative,whichis
then reshaped into an appropriately sized matrix to serve as the weight matrix of the
mixingnetwork. Thebiasesforthefirstlayerofthemixingnetworkaregeneratedin
thesamemanner,butarenotrestrictedtobenon-negative. Thebiasesforthelastlayer
aregeneratedbyahypernetworkwithaReLUactivationfunction. Itisnotedthatthe
state s information is only used by the hypernetworks and not directly passed to the
t
mixing network. Typically, for individual agents, we use methods such as temporal
differenceasshowninEquation6tofittheiractionvaluefunctionsQ(s,a)basedon
rewards:
Loss=∥Q(s,a)−(r+maxQ(s′,a′))∥2 (6)
a′
∂Q (o ,g|s )
tot t t ≥0 (7)
πh(o )
i t
Inthehierarchicalarchitectureproposedinthispaper,thebehaviorofthesubgoals
outputbythehigh-levelpolicycanbeviewedastheagent’sabstractactionunderhigh-
dimensional temporal sequences. It has a fixed action space (the size of the subgoal
set is fixed) and its interaction with the environment conforms to the definition of a
Markov decision process. We directly use the high-level policy network to replace
the DRQN network in QMIX, changing the training target of the mixing network to
thejointgoalvaluefunctionQ (o ,g|s ),adoptinganetworkdesignconsistentwith
tot t t
QMIXtoensurethemonotonicityconstraintofthejointgoalvaluefunction,asshown
inEquation7. TheoverallarchitectureofthisgoalmixingnetworkisshowninFig. 6,
whereeachagent’shigh-levelpolicyreceiveslocalobservationsandoutputsasubgoal.
Thegoalmixingnetworkreceivesthesubgoalsoutputbyallagents’high-levelpolicy
alongwithglobalstateasinputandoutputsthejointgoalvalue.
13(a) (b)
Figure7: Mini-GridDoorKeyEnvironmentDiagram: (a)inthesameroom,and(b)in
differentrooms.
Fig. 6-(a)showsthegoalmixingnetwork,wheretheHyper-Netrepresentsthehy-
pernetworkthatgeneratestheweightvectorbasedonthestates ,andtheW-Network
t
represents the weight network composed of reconstructed weight matrices and off-
sets. The structure of the hypernetwork and weight network is shown in Fig. 6-(b).
ComparedtoQMIX,whichusesfourhypernetworkstoseparatelygeneratetheweight
network and offsets for each layer of the mixing network, this paper uses only one
hypernetwork connected to different fully connected layers with different activation
functionstogeneratethecorrespondinginformation. Thefinalgoalmixingnetworkis
trainedaccordingtothelossfunctionshowninEquation8tolearnthejointgoalvalue
function:
(cid:88)
ℓ(θ)=∥ rh+γmaxQθ (o ,g′|s )−Qθ (o ,g|s )∥2 (8)
tot t+1 t+1 tot t t
g′
i
Insummary,thecompleteGMAHalgorithmfollowstheframeworkshowninFig.
1andistrainedaccordingtothefollowingsteps:
1. Trainthelow-levelpolicyπlbasedonapredefinedsetofsubgoals.Eachepisode
i
randomly samples a subgoal g from the subgoal set and fixes it, sampling the
statetransitiondataofeachagent’sinteractions⟨o ,a ,rl,g,o ⟩andtraining
t t t t+1
accordingtotheobjectivefunctionJ(θl)=max(cid:80) Rl.
2. Once the low-level policy converges, train the high-level policy πh. Collect
i
dataduringtheintervalbetweentwooutputsofsubgoalsbythehigh-levelpol-
icy ⟨o ,g ,(cid:80) rh,o ⟩ and train according to the objective function J(θh) =
t t t t+c
max(cid:80) Rh. Controlthesubgoalupdateprocessusingtheadaptivegoalgenera-
tionstrategy.
3. Oncethehigh-levelpolicytendstowardstability, collectjointdataateachstep
⟨o ,g ,s ,(cid:80) rh,s ,o ⟩andtrainaccordingtothelossfunctionEquation
t t t i t+1 t+1
8tofine-tunethehigh-levelpolicy.
4 Experiments and Discussion
In order to verify the performance of GMAH algorithm and visually see the effects
oftasktreesub-targetgeneration,goalmixingnetworkfine-tuningandadaptivetarget
14Figure 8: Result of GMAH-adapt, GMAH-no, PPO and A2C deuring traing with
T=256.
generationstrategies,thispaperchoosestoconductexperimentsintheMini-Gridand
Trash-Grid [39] environment, verifying GMAH in single/multi-agent conditions, re-
spectively. Ourcodeisopen-sourcedat: https://github.com/SICC-Group/
GMAH.
4.1 Mini-Grid: SingleAgent
Mini-Grid is an environment based on gym [40] that comprises a series of 2D grid
worldenvironmentswithgoal-orientedtasks. Theseenvironmentsfeatureagentswith
discrete action spaces represented by triangles, and the tasks involve solving various
maze maps and interacting with different objects such as doors, keys, or boxes. The
designofthisenvironmentaimsforsimplicity, speed,andeaseofcustomization,and
itiswidelyusedforsingle-agentresearch[39,41,42].
4.1.1 EnvironmentalSetup
TheDoor-Keyscenarioisdesignedtovalidatesingle-agentalgorithmsincomplextask
decisioncontexts. SchematicrepresentationsofthisscenarioareshowninFig. 7. The
scenariofeaturestworooms,thesizesofwhicharerandombutwhosetotalsizeisfixed.
Theagentisrandomlypositionedintheleftroom. Thetworoomsareseparatedbya
wallandadoor,withthedoorinitiallylocked. Akeyisplacedinthesameroomasthe
agent,whichcanbeusedtounlockthedoor. Anobject(Box)islocatedinoneofthe
rooms,andtheagent’staskistofindandactivate(Toggle)thisobject. Uponsuccessful
activationoftheobject,theagentreceivesatime-discountedrewardr ,definedas:
t
(cid:18) (cid:19)
t
r = 1−β ×R (9)
t T
wheretisthetimestepofactivation,T isthemaximumnumberoftimesteps,βisthe
discountfactorsetto0.5,andRisfixedat1,withr rangingfrom0tojustbelow1.
t
15Figure9: MinRewardofGMAH-adaptandGMAH-noduringtrainingwithT=256.
TheDoor-Keymapusedinthisexperimenthasafixedtotalsizeof8x8.Theagent’s
actionspaceconsistsofForward,Left,Right,Pickup,Drop,Toggle,Done. Theagent
operatesinapartiallyobservablestate,withitsobservationspacebeinga7x7x3matrix
that represents the type and state information of the grid cells directly in front of the
agent. Ifawallorboundaryispresentinthese7x7cells,thecellsbeyondthewallor
boundaryarerenderedinvisible. Thelocationofthetargetitemisrandomlygenerated,
with a 20% probability of appearing in the current room and an 80% probability of
appearingintheotherroom. Ideally,ifthetargetitemisinthecurrentroom,theagent
shouldfindandactivateitdirectly. Ifitisintheotherroom,theagentshouldfirstfind
thekey,unlockthedoor,thenfindandactivatetheitem. Theenvironmentterminates
and resets either when the agent activates the item or when the interaction with the
environmentreachesthemaximumtimelimit.
4.1.2 ComparativeExperiments
The final experiments are conducted at T=64 and T=256. At T=64, both GMAH-
adaptandGMAH-noemploythelow-levelmodeltrainedatT=64asthecommonlow-
level policy for both methods. In addition, PPO and A2C algorithms are used for
comparative experiments to demonstrate the advantages of hierarchical architectures
overtraditionalmethods.PPOandA2CalsousetherewardfunctionshowninEquation
(9) for training. There are no intermediate rewards set for these four algorithms, and
theyusethesamehyperparameters. PPOandA2Cuseanetworkstructurewithouta
goalinputmodule,consistentwiththehigh-levelmodelsofGMAH-adaptandGMAH-
no. TheresultsoffouralgorithmsatT=64areshowninFig. 10.
As depicted, even when the training curves of all four algorithms at T=64 are
smoothedusinganexponentialsmoothingmethodwithaweightof0.89[43],thetrain-
ingprocessesofPPOandA2Cremainhighlyunstable,convergingtoaround0.75.The
convergencevaluesofGMAH-noareroughlythesameasthoseofPPOandA2C,while
GMAH-adaptisslightlyhigher,thoughtheimprovementisminimal. Adistinctdiffer-
encebetweenGMAH-adaptandGMAH-nocomparedtoPPOandA2Cistheirhigher
initial performance and faster convergence rate, although the improvement is not as
16Figure 10: Result of GMAH-adapt, GMAH-no, PPO and A2C during training with
T=64.
pronouncedaswithPPOandA2C.
AtT=256,GMAH-adaptandGMAH-nousethelow-levelpolicymodeltrainedat
T=256. ThetrainingresultsofthefouralgorithmsareshowninFig. 8. PPOandA2C
showa greaterfinal convergencereward comparedto T=64, while GMAH-adaptand
GMAH-nodemonstrateamoresignificantimprovement. Basedonpreviousanalysis,
theincreaseinconvergencevaluesforPPOandA2CismainlyduetotheparameterT,
whereastheimprovementsforGMAH-adaptandGMAH-noaresubstantial. Although
the differences between GMAH-adapt and GMAH-no appear minimal and could be
attributed to the randomness of training, observations of the training reward minima
provetheadvantageofGMAH-adapt,asshowninFig. 9,whereGMAH-adaptclearly
outperformsGMAH-nointermsofminimumrewardvalues.
4.1.3 ResultDemonstration
During the testing phase, interactions in various randomized scenarios demonstrated
the superiority of the GMAH-adapt algorithm over GMAH-no. To highlight the ad-
vantages of the adaptive goal generation strategy used in GMAH, key interactions of
agentstrainedwithbothGMAH-adaptandGMAH-noalgorithmsweredocumentedin
a test scenario, as shown in Fig. 11 and 12. Each instance labeled "g" represents a
subgoalissuedbythehigh-levelpolicy.
In the scenario depicted in Fig. 11, the agent begins in a room with the target
item, facing upwards with no other objects in its observable range. The high-level
policy of GMAH-no initially sets a subgoal for the agent to fetch the key based on
its observations. The agent then performs a sequence of actions: moving left twice,
turningleft, movingforward, andpickingupthekey, coveringtheseactionsbetween
moments t=0 and t=7. During this sequence, the agent spots the final target item.
After retrieving the key and completing the first subgoal, the high-level policy shifts
thesubgoaltoopenthedooroncetheitemleavestheagent’ssight. Afteropeningthe
door and not locating the item in the next room, the strategy updates the subgoal to
activatetheitem,leadingtheagentbacktotheoriginalroom. Thetrajectorypostt=13
17Figure11: TestexampleofAgenttrainedbyGMAH-noinDoor-Keyenvironment.
Figure12: TestexampleofAgenttrainedbyGMAH-adaptinDoor-Keyenvironment.
illustratestheagentlocatingandactivatingtheitemtocompletethetask.
Utilizing the same environmental setup as in Fig. 11, the agent trained with the
GMAH-adapt method was also tested. As depicted in Fig. 12, the GMAH-adapt’s
high-level policy directly issues the subgoal "Activate the Item," allowing the agent
to skip unnecessary steps such as fetching the key and opening the door. This ap-
proachsignificantlyexpeditestaskcompletion,savingconsiderabletimecomparedto
theGMAH-nostrategy. Occasionally,evenininstanceswherethefinaltargetandthe
agentstartinthesameroom,GMAH-nomaygeneratethedirectsubgoal"ToggleThe
Box,"thoughlessfrequently. Theresults,illustratedinFig. 9and12,confirmthatthe
adaptivegoalgenerationstrategysignificantlyenhancesbothefficiencyandeffective-
nessintaskexecution.
4.1.4 AnalysisandDiscussion
ThisarticlesynthesizesexperimentalresultsfromtheMini-GridDoorKeyenvironment
tovalidatetheeffectivenessofGMAH’shierarchicalarchitecture,featuringatask-tree-
based subgoal generation method and an adaptive goal generation strategy. Notably,
the GMAH algorithm demonstrates a distinct advantage in that the high-level policy
exhibits high performance from the onset of training. When compared to the PPO
and A2C algorithms, GMAH achieves rapid convergence, though the improvement
post-convergencerelativetoitsinitialperformanceismodest. Thisobservationcanbe
attributedtotwoprimaryfactors:
18Figure13: Tras-Gridenvironmentdiagram.
1. Simplified Decision-Making: The hierarchical structure’s high-level policy re-
ducescomplexitybycondensingextensivetemporalsequencesintoshorterepisodes
delineatedbysubgoals. Insteadoftacklingtheentiretaskdirectly, thestrategy
focusesontheorganizationandintegrationofalimitedarrayofsubgoals,neces-
sitatingfewerdatasamplesforeffectivelearning. Thisefficiencyisasignificant
contributortotheobservedswiftconvergence.
2. InterrelatedSubgoals: WithintheMini-Gridenvironment,thesubgoalsarede-
signedtoaligncloselywiththeultimatetask. Atypicalstrategicsequencemight
include"Pickupthekey,""Openthedoor,"and"ToggletheBox"tofinalizethe
task.Subgoalslike"ToggletheBox"encapsulatecriticalactionsintegraltocom-
pletingthetask.Earlyselectionofsuchasubgoalbythehigh-levelpolicyallows
theagenttopromptlyconcludethetask,enhancinginitialtrainingeffectiveness.
However,thismightalsoleadthehigh-levelpolicytofavorchoosingdirectac-
tionsubgoalsprematurely,potentiallyattheexpenseofstrategicallysequencing
multiplesubgoals.
4.2 Trash-Grid: MultiAgents
For multi-agent studies involving the GMAH algorithm, we designed a multi-robot
trashcollectionenvironmentcalledTrash-Grid,basedonthePettingZoo[44]andMini-
Gridrenderingframeworks,whichenablessequentialexecutionofmultipleagentsand
updatesstateinformationineachcycle.
4.2.1 EnvironmentalSetup
TheTrash-Grid, asshowninFig. 13, featuresa10x10gridwithN=3agents(robots)
oriented downwards, randomly distributed with K1=5 small ’trash’ items and K2=5
large’trash’items. A2x4gridareainthebottomrightcornerservesasthe"recycling
station."Theagents’taskistotransportalltrashtotherecyclingstationintheshortest
time possible. Each small trash item weighs 1, and an agent can carry up to 3 small
trashitemssimultaneously. Largetrashitemscannotbecarriedbutmustbesplitinto1
19(a) (b)
Figure14: TrainingresultofGMAH,MAPPO,COMAandQMIXinTrash-Griden-
vironmentwithT=128. (a)Theexperimentalresultsoftraining1000000stepsbyfour
methods,(b)Resultsof100,000stepsoffine-tuningtrainingforGMAHusingagoal
mixingnetwork.
smalltrashitembytheagent. Theagents’actionspaceincludesbehaviorsasthesets
{0 : Forward}, {1 : Left}, {2 : Right}, {3 : Pickup}, {4 : Putdown}, and{5 :
Split}. ThesubgoalspacefortheGMAHalgorithmispredefinedasshowninthesets
{0:FindTrash},{1:PickupSTrash},{2:PickupBTrash},{3:PutTrash}.
Agentscannotpassthroughotheragentsortrashitems;collisionsresultinanega-
tiverewardasapenalty. Agentsmustfacethe"trash"topickitup. Ifanagent’sload
isfull,itcanstillmoveorsplitlargetrashbutcannotcarrymore. Agentscandropall
carried"trash"atanypositionwithintherecyclingstationareaandreceiveadiscounted
rewardrbasedontheweightofthetrashdropped,similartoEquation9,whereRisa
positiveinteger. Additionally, agentsreceiveanegativerewardateachtimestepasa
penalty.
TheobservationspaceforagentsisadiscretevectoroflengthN×4+(K1+K2)×3+8×2,
representingthecurrentagent’sposition,load,orientation,relativepositions,loads,and
orientations of other agents, and the position and type of all "trash" items. The state
space is a 10x10 discrete matrix. Agents do not have access to other agents’ actions
andgoals. Theobservationspaceisnormalizedduringactualtraining.
4.2.2 ComparativeExperiments
Figures 15 and 16 display heatmaps of agents’ trajectories during several training
episodesusingtheMAPPOandGMAHalgorithms, respectively. Theseheatmapsil-
lustrate the frequency of visits to each grid cell, showing a progression from top to
bottomwiththeagents’startingpositionsatthetopofthegrid.
TheMAPPOalgorithm’srewardsconvergedtozerowithminimalfluctuations,sug-
gestingthattheagentsadoptedasuboptimalstrategyofminimalmovementandtrash
collection. Thisbehaviorisattributedtotherewardstructure,whichheavilypenalizes
agent collisions, thus encouraging a strategy that avoids movement to prevent penal-
ties. Duringtrainingepisodes200-350and1300-1400,itindicatesthatagentstrained
20Figure15: DuringthetrainingofMAPPO,theheatmapcountsthemovementtrajec-
toriesoftheagents(thenumberofvisitstoeachgrid)inseveralepisodes,representing
theresultsofthethreeagentsfromlefttoright.
withMAPPOshowreduceddownwardexplorationovertime,primarilyremainingnear
theirinitialpositions.
Conversely,agentstrainedwiththeGMAHalgorithmdemonstratedabroaderspread
ofmovementacrossthegridinsubsequentrounds,withnoanomaliesobservedforthe
third agent. This pattern indicates that the GMAH algorithm facilitated more effec-
tivedatasampling,indirectlyconfirmingthathierarchicalapproachescansignificantly
improveagents’exploratorybehaviors.
4.2.3 DemonstratingResults
Whilethefinalconvergenceofrewardvaluesduringtrainingreflectsalgorithmperfor-
mance,therandomnessoftrashplacementandquantityintheTrash-Gridenvironment,
combined with time-discounted rewards, introduces uncertainty that complicates the
analysis of agent behavior based solely on reward data. Consequently, the execution
processoftheGMAHalgorithmisqualitativelyanalyzedusingtheresultsfromatyp-
icalexperimentalrun. Figure17illustratestheGMAHalgorithminactionwithinthe
Trash-Gridenvironment,capturingsixkeymomentsfromatestround. Theseimages,
spanning from t=0 to t=77, demonstrate the dynamic changes in the environment’s
state,providinginsightsintotheGMAHalgorithm’soperationalprocess.
Initially,threeagentsarepositionedatthetopofthegrid,facingdownwardtoward
a recycling station located in the bottom right corner, depicted in gray. By t=11, the
agents have interacted with both small and large trash items, with subsequent frames
depictingtheirprogressintransportingtrashtotherecyclingstation.Theimageatt=77
showsonlytwolargepiecesoftrashremaining,withagentsapproachingthem,likely
tobreakthemdown. Theprogressionfromt=0tot=11suggeststhatwhileagentsef-
ficientlyachievedtheirimmediateobjectives,thereispotentialtoenhancetheGMAH
algorithm’stimeefficiency,asevidencedbysomeunnecessarystepstakenduringthese
21Figure16: DuringthetrainingofGMAH,theheatmapcountsthemovementtrajecto-
riesoftheagents(thenumberofvisitstoeachgrid)inseveralepisodes, representing
theresultsofthethreeagentsfromlefttoright.
earlyphases. Additionally,thetransitionsbetweent=38andt=59revealpossiblecon-
flictsbetweenagentobjectives, identifyingareaswheretheGMAHalgorithm’scoor-
dinationcouldbefurtherimproved.
4.2.4 AnalysisandDiscussion
ComparedtotheMini-Grid,thetrainingphaseintheTrash-Gridexperimentswiththe
GMAH algorithm reveals a pronounced learning curve despite similar dependencies
between subgoals. In Trash-Grid, the final task requires repeated accomplishment of
subgoals, which challenges the high-level policy’s planning capabilities more than in
Mini-Grid. Thestrategyinvolvingrepeatedplanningforcollectingandrecyclingtrash
provesmoreeffectivethanthosefocusingdirectlyonrecycling.
However, the performance of the GMAH algorithm’s low-level policy in Trash-
Grid falls short of its performance in Mini-Grid, indicating significant opportunities
forenhancement. Theneedforapproximately30stepstoachievesubgoalsina10x10
gridunderscoresthepotentialforimprovingthelow-levelpolicy. Thischallengearises
partlyfromtheunstableenvironment,whichcomplicatesthecompletionofsubgoals.
Furthermore,ithighlightsapotentiallimitationofthehierarchicalarchitecture: unlike
traditional,non-layeredalgorithms,thelow-levelpolicymustlearnspecificbehaviors
forvarioussubgoals,compoundedbythecomplexityofsubgoalinputs.Ifthesubgoals
closelymimicthefinaltask,thisincreasesthelearningdifficulty.
Insummary,thedesignofaneffectivesubgoalspaceiscriticalforthesuccessofthe
GMAHmethod. Adetailedanalysisoftheenvironmentisessentialtodecomposethe
maintaskintoeffectiveandsufficientlyindependentsubgoals,ensuringthecompletion
ofthefinaltaskwhilepreservingflexibility.
22Figure 17: Example of test results of GMAH in Trash-Grid environment. The figure
showstheenvironmentstatesatsixintermediatemomentsinatestround.
5 Conclusion
This paper explores the concept of hierarchical reinforcement learning in complex
multi-agent environments with limited communication. We introduce the GMAH al-
gorithm, which uses a subgoal-based approach and a task-based subgoal generation
methodthatdefinesclearsubgoalsbasedonpriorknowledge. TheGMAHalgorithm
incorporates a goal mixing network to extend its hierarchical architecture effectively
intomulti-agentsettings. Additionally,anadaptivegoalgenerationstrategyallowsthe
high-level policy to flexibly and timely adjust subgoals, enhancing the GMAH algo-
rithm’sflexibility.TheperformanceoftheGMAHalgorithm’shierarchicalarchitecture
andadaptivegoalgenerationstrategywastestedintheMini-Gridenvironmentforini-
tialverification,followedbymorecomprehensivetestingintheTrash-Gridmulti-agent
environment. ResultsfrombothenvironmentsdemonstratethattheGMAHalgorithm
surpassestraditionalreinforcementlearningmethods,particularlyinsolvingcomplex
problemsandachievingfasterconvergencerates,evenwithamodelsizetwicethatof
traditionalmethods.
While the GMAH algorithm shows promise, there is still potential for improve-
ment.Theexecutionoflow-levelpoliciescouldbeoptimizedfurther,andtheeffective-
nessoftheadaptivetargetupdatestrategycouldbeenhanced.Nevertheless,theGMAH
algorithmoffersasolidfoundationforfutureresearchinthemulti-agentdomain. Fu-
ture work will focus on refining the low-level policy to better achieve subobjectives
andstabilizingthetrainingprocessofthehigh-levelpolicy.
References
[1] C.J.Watkins,P.Dayan,Q-learning,Machinelearning8(1992)279–292.
[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Human-level
controlthroughdeepreinforcementlearning,nature518(7540)(2015)529–533.
23[3] P. S. Chib, P. Singh, Recent advancements in end-to-end autonomous driving
usingdeeplearning: Asurvey,IEEETransactionsonIntelligentVehicles.
[4] D.Shah,A.Sridhar,A.Bhorkar,N.Hirose,S.Levine,Gnm:Ageneralnavigation
model to drive any robot, in: 2023 IEEE International Conference on Robotics
andAutomation(ICRA),IEEE,2023,pp.7226–7233.
[5] A. K. Shakya, G. Pillai, S. Chakrabarty, Reinforcement learning algorithms: A
briefsurvey,ExpertSystemswithApplications(2023)120495.
[6] L.Ouyang, J.Wu, X.Jiang, D.Almeida, C.Wainwright, P.Mishkin, C.Zhang,
S.Agarwal,K.Slama,A.Ray,etal.,Traininglanguagemodelstofollowinstruc-
tionswithhumanfeedback, Advancesinneuralinformationprocessingsystems
35(2022)27730–27744.
[7] Z. Hu, K. Shukla, G. E. Karniadakis, K. Kawaguchi, Tackling the curse of di-
mensionality with physics-informed neural networks, Neural Networks (2024)
106369.
[8] D. Yang, H. Zhang, X. Lan, J. Ding, Density-based curriculum for multi-goal
reinforcementlearningwithsparserewards,arXivpreprintarXiv:2109.08903.
[9] L. Zheng, J. Chen, J. Wang, J. He, Y. Hu, Y. Chen, C. Fan, Y. Gao, C. Zhang,
Episodic multi-agent reinforcement learning with curiosity-driven exploration,
AdvancesinNeuralInformationProcessingSystems34(2021)3757–3769.
[10] R.S.Sutton,D.Precup,S.Singh,Betweenmdpsandsemi-mdps: Aframework
fortemporalabstractioninreinforcementlearning,Artificialintelligence112(1-
2)(1999)181–211.
[11] J.Hao,T.Yang,H.Tang,C.Bai,J.Liu,Z.Meng,P.Liu,Z.Wang,Exploration
in deep reinforcement learning: From single-agent to multiagent domain, IEEE
TransactionsonNeuralNetworksandLearningSystems.
[12] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy
optimizationalgorithms,arXivpreprintarXiv:1707.06347.
[13] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,
D.Wierstra,Continuouscontrolwithdeepreinforcementlearning,arXivpreprint
arXiv:1509.02971.
[14] M.Andrychowicz,F.Wolski,A.Ray,J.Schneider,R.Fong,P.Welinder,B.Mc-
Grew,J.Tobin,O.PieterAbbeel,W.Zaremba,Hindsightexperiencereplay,Ad-
vancesinneuralinformationprocessingsystems30.
[15] T.Schaul,D.Horgan,K.Gregor,D.Silver,Universalvaluefunctionapproxima-
tors, in: Internationalconferenceonmachinelearning, PMLR,2015, pp.1312–
1320.
[16] Y. Burda, H. Edwards, A. Storkey, O. Klimov, Exploration by random network
distillation,arXivpreprintarXiv:1810.12894.
[17] A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kapturowski,
O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt, et al., Never give up: Learning
directedexplorationstrategies,arXivpreprintarXiv:2002.06038.
24[18] Y.Chen,H.Kurniawati,Pomdpplanningforobjectsearchinpartiallyunknown
environment,AdvancesinNeuralInformationProcessingSystems36.
[19] C.Lu,Q.Bao,S.Xia,C.Qu,Centralizedreinforcementlearningformulti-agent
cooperativeenvironments,EvolutionaryIntelligence17(1)(2024)267–273.
[20] W.Li,B.Jin,X.Wang,J.Yan,H.Zha,F2a2:Flexiblefully-decentralizedapprox-
imate actor-critic for cooperative multi-agent reinforcement learning, Journal of
MachineLearningResearch24(178)(2023)1–75.
[21] R. Azzam, I. Boiko, Y. Zweiri, Swarm cooperative navigation using centralized
traininganddecentralizedexecution,Drones7(3)(2023)193.
[22] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, S. Whiteson, Counterfactual
multi-agentpolicygradients,in:ProceedingsoftheAAAIconferenceonartificial
intelligence,Vol.32,2018.
[23] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,I.Mordatch,Multi-agent
actor-criticformixedcooperative-competitiveenvironments,Advancesinneural
informationprocessingsystems30.
[24] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, Y. Yi, Qtran: Learning to fac-
torizewithtransformationforcooperativemulti-agentreinforcementlearning,in:
Internationalconferenceonmachinelearning,PMLR,2019,pp.5887–5896.
[25] P.Sunehag,G.Lever,A.Gruslys,W.M.Czarnecki,V.Zambaldi,M.Jaderberg,
M.Lanctot, N.Sonnerat, J.Z.Leibo, K.Tuyls, etal., Value-decompositionnet-
worksforcooperativemulti-agentlearning,arXivpreprintarXiv:1706.05296.
[26] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, S. Whiteson,
Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearn-
ing,JournalofMachineLearningResearch21(178)(2020)1–51.
[27] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, Y. Wu, The surpris-
ing effectiveness of ppo in cooperative multi-agent games, Advances in Neural
InformationProcessingSystems35(2022)24611–24624.
[28] J. Schulman, P. Moritz, S. Levine, M. Jordan, P. Abbeel, High-dimensional
continuous control using generalized advantage estimation, arXiv preprint
arXiv:1506.02438.
[29] O.Nachum,S.S.Gu,H.Lee,S.Levine,Data-efficienthierarchicalreinforcement
learning,Advancesinneuralinformationprocessingsystems31.
[30] E.Todorov,T.Erez,Y.Tassa,Mujoco:Aphysicsengineformodel-basedcontrol,
in: 2012 IEEE/RSJ international conference on intelligent robots and systems,
IEEE,2012,pp.5026–5033.
[31] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver,
K.Kavukcuoglu,Feudalnetworksforhierarchicalreinforcementlearning,in:In-
ternationalconferenceonmachinelearning,PMLR,2017,pp.3540–3549.
[32] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, P. Abbeel, Vime:
Variationalinformationmaximizingexploration,Advancesinneuralinformation
processingsystems29.
25[33] A.Levy,G.Konidaris,R.Platt,K.Saenko,Learningmulti-levelhierarchieswith
hindsight,arXivpreprintarXiv:1712.00948.
[34] C.Florensa,Y.Duan,P.Abbeel,Stochasticneuralnetworksforhierarchicalrein-
forcementlearning,arXivpreprintarXiv:1704.03012.
[35] T.D.Kulkarni,A.Saeedi,S.Gautam,S.J.Gershman,Deepsuccessorreinforce-
mentlearning,arXivpreprintarXiv:1606.02396.
[36] S. Li, J. Zhang, J. Wang, Y. Yu, C. Zhang, Active hierarchical exploration with
stablesubgoalrepresentationlearning,arXivpreprintarXiv:2105.14750.
[37] T. Zhang, S. Guo, T. Tan, X. Hu, F. Chen, Generating adjacency-constrained
subgoalsinhierarchicalreinforcementlearning,Advancesinneuralinformation
processingsystems33(2020)21579–21590.
[38] C.Dugas, Y.Bengio, F.Bélisle, C.Nadeau, R.Garcia, Incorporatingfunctional
knowledgeinneuralnetworks.,JournalofMachineLearningResearch10(6).
[39] D.Furelos-Blanco, M.Law, A.Jonsson, K.Broda, A.Russo, Hierarchiesofre-
wardmachines,in:InternationalConferenceonMachineLearning,PMLR,2023,
pp.10494–10541.
[40] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d. Cola, T. Deleu,
M. Goulão, A. Kallinteris, A. KG, M. Krimmel, R. Perez-Vicente, A. Pierré,
S. Schulhoff, J. J. Tai, A. T. J. Shen, O. G. Younis, Gymnasium (Mar. 2023).
doi:10.5281/zenodo.8127026.
URLhttps://zenodo.org/record/8127025
[41] S. Alver, D. Precup, Minimal value-equivalent partial models for scalable and
robust planning in lifelong reinforcement learning, in: Conference on Lifelong
LearningAgents,PMLR,2023,pp.548–567.
[42] A.Andres,E.Villar-Rodriguez,J.DelSer,Anevaluationstudyofintrinsicmo-
tivationtechniquesappliedtoreinforcementlearningoverhardexplorationenvi-
ronments,in: InternationalCross-DomainConferenceforMachineLearningand
KnowledgeExtraction,Springer,2022,pp.201–220.
[43] E.S.GardnerJr,Exponentialsmoothing:Thestateoftheart,Journalofforecast-
ing4(1)(1985)1–28.
[44] J.Terry, B.Black, N.Grammel, M.Jayakumar, A.Hari, R.Sullivan, L.S.San-
tos, C. Dieffendahl, C. Horsch, R. Perez-Vicente, et al., Pettingzoo: Gym for
multi-agentreinforcementlearning, AdvancesinNeuralInformationProcessing
Systems34(2021)15032–15043.
26