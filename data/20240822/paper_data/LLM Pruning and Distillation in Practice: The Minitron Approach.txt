2024-8-22
LLM Pruning and Distillation in Practice: The
Minitron Approach
Sharath Turuvekere Sreenivas*, Saurav Muralidharan*, Raviraj Joshi, Marcin Chochowski,
Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz and Pavlo Molchanov
Abstract: We present a comprehensive report on compressing the Llama 3.1 8B and Mistral NeMo 12B
models to 4B and 8B parameters, respectively, using pruning and distillation [1]. We explore two distinct
pruning strategies: (1) depth pruning and (2) joint hidden/attention/MLP (width) pruning, and evaluate the
results on common benchmarks from the LM Evaluation Harness [2]. The models are then aligned with NeMo
Aligner and tested in instruct-tuned versions. This approach produces a compelling 4B model from Llama 3.1
8B and a state-of-the-art Mistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo
12B. We found that with no access to the original data, it is beneficial to slightly fine-tune teacher models on
the distillation dataset. We open-source our base model weights on Hugging Face with a permissive license.
Models on Hugging Face: Mistral-NeMo-Minitron-8B-Base | Llama-3.1-Minitron-4B-Width-Base
| Llama-3.1-Minitron-4B-Depth-Base
Introduction
Teacher
Pretrained model Correction Corrected
LLM providers often train an entire family of models
(M LLis at Mra al - 3N .1e M 8Bo - e1 t2 cB ), (127B) Teacher
from scratch, each with a different size (number of
Pruning
parameters, e.g. Llama 3.1 8B, 70B, 405B); this
Distillation
is done to aid users targeting different deployment (<400B)
Minitron model Student
scales, sizes and compute budgets. However, training
multiple multi-billion parameter models from scratch
is extremely time-, data- and resource-intensive. Figure1|High-leveloverviewofourproposedpruning
anddistillationapproach. Thetotalnumberoftokens
Recent work [1] has demonstrated the effectiveness
used for each step is indicated in parentheses.
of combining weight pruning with knowledge distilla-
tion to significantly reduce the cost of training LLM
model families. Here, only the biggest model in the
(MN-Minitron-8B) which outperforms all similarly-
family is trained from scratch; other models are ob-
sized models across the board on common language
tained by successively pruning the bigger model(s)
modeling benchmarks. Our Llama-3.1-Minitron-4B
and then performing knowledge distillation to recover
models (both depth and width-pruned variants) also
the accuracy of pruned models.
exhibit strong accuracy compared to the teacher
In this report, we successfully apply the Minitron Llama 3.1 8B model and the previous-generation
compression strategy [1] to two state-of-the-art mod- Minitron-4B model [1]; among the two variants, the
els: Llama 3.1 8B [3] and Mistral NeMo 12B [4], width-pruned variant outperforms the depth-pruned
compressing them down to 4B and 8B parameters, one. In terms of runtime inference performance mea-
respectively. Figure 1 provides a high-level overview sured using TensorRT-LLM, the MN-Minitron-8B
of our approach. model provides an average speedup of 1.2× over the
teacher Mistral NeMo 12B model. Similarly, the
While following the original paper [1], we make a
Llama-3.1-Minitron-4B models provide an average
key modification: due to lack of access to the original
speedup of 2.7× and 1.8× for the depth and width
training data, we fine-tune the teacher model on our
prunedvariants,respectively,comparedtotheteacher
own dataset before pruning and distillation. We refer
Llama 3.1 8B model.
tothisstepasteacher correction. Figure4showsthat
omittingteachercorrectioncausesadatadistribution
mismatch, negatively impacting distillation. Methodology
Table1providesasummaryofourresults: ourcom-
pression strategy yields a state-of-the-art 8B model A high-level overview of our approach is illustrated
in Figure 1. Here, the teacher model is first lightly
*Equalcontribution.
© 2024NVIDIA.Allrightsreserved.
4202
guA
12
]LC.sc[
1v69711.8042:viXraLLMPruningandDistillationinPractice: TheMinitronApproach
Benchmarks(shots) Gemma2 Minitron Llama-3.1-Minitron Gemma Mistral Llama3.1 MN-Minitron MistralNeMo
2B* 4B 4B-Depth 4B-Width 7B 7B 8B 8B 12B-Base 12B-FT
TotalParams 2.6B 4.2B 4.5B 4.5B 8.5B 7.3B 8B 8.4B 12.2B 12.2B
Non-Emb. Params 2B 2.6B 3.7B 3.7B 7.7B 7B 7B 7.3B 10.9B 10.9B
TrainingTokens 2T 94B 94B 94B 6T 8T 15T 380B - +0.1T
Winogrande(5) 70.9 74.0 72.1 73.5 78 78.5 77.3 80.4 82.2 82.7
Arc_challenge(25) 55.4 50.9 52.6 55.6 61 60.3 57.9 64.4 65.1 62.3
MMLU(5) 51.3 58.6 58.7 60.5 64 64.1 65.3 69.5 69.0 70.1
Hellaswag(10) 73.0 75.0 73.2 76.1 82 83.2 81.8 83.0 85.2 85.3
GSM8k(5) 23.9 24.1 16.8 41.2 50 37.0 48.6 58.5 56.4 55.7
Truthfulqa(0) - 42.9 38.2 42.9 45 42.6 45.0 47.6 49.8 48.3
XLSumen(20%)(3) - 29.5 27.2 28.7 17 4.8 30.0 32.0 33.4 31.9
MBPP(0) 29.0 28.2 30.7 32.4 39 38.8 42.3 43.8 42.6 47.9
HumanEval(n=20)(0) 20.1 23.3 - - 32.0 28.7 24.8 36.2 23.8 23.8
Table 1 | Accuracy numbers for our MN-Minitron-8B and Llama-3.1-Minitron-4B models. We compare our
models to similarly-sized SoTA open models on a variety of common language modeling benchmarks. All
evaluations are conducted by us, except entries marked with * (taken from corresponding papers).
Benchmarks Gemma Phi-2 Gemma2 Qwen2 Minitron Llama-3.1-Minitron
2B 2.7B 2B 1.5B 4B 4B-Depth 4B-Width
TotalParams 2.5B 2.7B 2.6B 1.5B 4.2B 4.5B 4.5B
Non-Emb. Params 2B 2.5B 2B 1.3B 2.6B 3.5B 3.7B
Tokens 3T 1.4T 2T 7T 94B 94B 94B
IFEval 40.5 44.0 64.5 39.8 44.8 42.6 52.4
MT-Bench 5.2 4.3 7.7 5.2 5.6 5.6 6.3
ChatRAG* 33.3 37.6 37.5 32.8 41.1 40.1 44.0
BFCL 47.0 23.1 35.6 32.8 64.2 66.8 64.9
Table 2 | Accuracy numbers for the aligned Llama-3.1-Minitron models. We compare our models to similarly-
sized SoTA open aligned models on a variety of benchmarks. All evaluations are conducted by us. * Denotes
resultsobtainedonarepresentativesubsetofthebenchmark. Bestinbold, secondunderlined. The alignment
of MN-Minitron-8B is underway and will be posted once ready.
finetuned on the target dataset to be used for dis- 1. Trained LLM 2. Estimate importance
tillation - we refer to this step as teacher correction. Emb1 Emb1 Head1 Emb1 CH 1 Emb2 Emb2 Head2 Emb2 CH 2
Next, pruning is applied to compress the model, fol- Emb3 Emb3 Head3 Emb3 CH 3
Emb4 Emb4 Head4 Emb4 CH 4
lowing which distillation is used to recover any lost
5. Distillation Iterative
model accuracy. We refer the reader to the Minitron
4. Trim 3. Rank
paper [1] for the full description of the pruning and Emb4 Emb4 Head3 Emb4 CH 1
Emb4 Emb4 Head3 Emb4 CH 1 Emb2 Emb2 Head1 Emb2 CH 4 distillation method. Emb2 Emb2 Head1 Emb2 CH 4 Emb1 Emb1 Head4 Emb1 CH 2
Head4
Emb3 Emb3 Head2 Emb3 CH 3
Figure2|Pruninganddistillationprocessoutlinedin
Pruning the original paper [1]. We follow the same approach
in this work.
Weight pruning is a powerful and well-known tech-
niqueforreducingmodelsize. Inthisreport,wefocus
on structured pruning, where blocks (or channels) of
all the axes we consider (depth, neuron, head, and
nonzero elements are removed at once from model
embedding channel) using a small calibration dataset
weights; examples of structured pruning techniques
and only forward propagation passes. We consider
include neuron, attention head, convolutional filter,
depth pruning as a special case and do not combine
and depth pruning [1]. In case of LLMs, as shown in
it with compressing other dimensions.
Figure 2, we start the pruning process by first com-
puting the importance of each layer, neuron, head, We compute the importance of each head, neuron
and embedding dimension. We then sort these impor- and embedding channel by examining the activations
tance scores to compute a corresponding importance produced by the multi-head attention (MHA), multi-
ranking. layer perceptron (MLP) and LayerNorm layers, re-
spectively. We use a small calibration dataset (1024
Importance Estimation: We use a purely
samples) for this purpose.
activation-based importance estimation strategy that
simultaneously computes sensitivity information for For depth pruning, we consider three distinct met-
2
gniddebmE remrofsnarT kcolB mron
reyaL
noitnettA mron
reyaL
PLM L reyaL
L reyaL
1 reyaL
1 reyaL
L reyaL
L reyaLLLMPruningandDistillationinPractice: TheMinitronApproach
rics for evaluating layer importance: (1) LM vali- Step 1. Teacher correction
dation loss, (2) Block Importance (BI) [5] and (3)
accuracy on the downstream task. For loss-based
Logits
ranking, we simply remove a single or a block of con-
tiguous layers and compute its effect on LM loss; this
Cross-
serves as the “importance” or sensitivity of the layer. entropy loss
BI uses the cosine distance between the input and
Frozen Trainable Loss Next token
output of a layer or a block of layers. We notice
thatBIandLMlossmetricsarehighlycorrelatedbut Step 2. Finetuning via Distillation
do not produce the most accurate pruned model on Teacher
downstream tasks as shown in Figures 8 and 9. We
thus evaluate layer importance using the Winogrande Logits
benchmark [6].
KL
Model Trimming: AsshowninFigure2,foragiven Divergence
architecture configuration, we first rank the elements
of each axis according to the computed importance Logits
and perform trimming (reshaping) of the correspond-
ing weight matrices directly. For neuron and head Student
pruning,wetrimMLPandMHAlayerweights,respec-
tively. Inthecaseofembeddingchannels,wetrimthe Figure 3 | Overview of Distillation: If the original
embedding dimension of the weight matrices in MLP, trainingdataisunavailable,aslightfine-tuningofthe
MHA, and LayerNorm layers. The original approach teacher model is recommended. Distillation is then
([1]) uses Neural Architecture Search (NAS) to find performedbyminimizingKLdivergenceonthelogits,
the best architecture; in this work, we skip this step withtheoriginalmodelastheteacherandthepruned
and instead utilize the network architecture-related model as the student.
learnings from the original paper.
tokens. We start with the corresponding Base models
Retraining with Distillation
that are openly available online on Hugging Face.
Weusethetermretrainingtorefertotheaccuracyre-
covery process following pruning. In this work, we ex-
ploretworetrainingstrategies: (1)conventionaltrain- Dataset: WeusetheNemotron-4curatedcontinued
ing, leveraging ground truth labels, and (2) knowl- training dataset(CT) [8] [9] for all our experiments.
edge distillation using supervision from the unpruned
model (teacher). Knowledge Distillation (KD) in-
volves transfer of knowledge from a larger or more Pruning
complexmodelcalledtheteachertoasmaller/simpler
Our simplified pruning recipe is based on the best
model called the student. The knowledge transfer is
practices outlined in the Minitron paper [1] and is
achieved by having the student model mimic the out-
describedintheMethodologysection. Specifically,for
put and/or the intermediate states of the teacher
width pruning, we (1) use l2-norm and mean as the
model. In our case, the uncompressed and pruned
aggregation functions across the batch and sequence
modelscorrespondtotheteacherandstudent, respec-
dimensions, respectively, and (2) perform single-shot
tively. For distillation, we follow best practices from
pruning, avoiding iterative approaches. For depth
our previous work [1] and use forward KL Divergence
pruning, as described in the Methodology section,
loss [7] on the teacher and student logits only. This
we follow the observations from Gromov et al. [10]
is illustrated in Figure 3.
and drop a continuous subgroup of layers that results
in the least accuracy drop on Winogrande [6]. In
Training Details this work, we skip the lightweight neural architecture
search (NAS) phase, and go with a manual architec-
ture configuration for both Llama-3.1-Minitron-4B
Pre-training
and MN-Minitron-8B. The architectures we come up
Llama 3.1 8B [3] and Mistral NeMo [4] 12B are pre- withareinspiredbytheMinitron-4BandMinitron-8B
trained on different proprietary datasets, which we models, and are detailed in Table 3. We now describe
do not have access to. According to the Llama 3.1 the pruning recipes for each of our target compressed
tech report [3], the 8B model is pretrained on 15T models:
3
nekot
tupnI
nekot
tupnI
gniddebmE
gniddebmE
idde
gb nmE
remrofsnarT
remrofsnarT
remrofsnarT
sreyaL
sreyaL
sreyaL
daeh
ML
daeh
ML
daeh
ML
xamtfoS
xamtfoS
xamtfoSLLMPruningandDistillationinPractice: TheMinitronApproach
Llama-3.1-Minitron-4B-Width: LM Validation Loss vs Training Steps
Original 12B Teacher Fine-tuned 12B Teacher
• Starting model: Llama 3.1 8B 2.0
• Hidden dimension: 4096 → 3072
• MLP hidden dimension: 14336 → 9216 1.9
• Attention heads: unchanged
• Depth: unchanged 1.8
1.7
2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5
Llama-3.1-Minitron-4B-Depth:
Training Tokens(B)
• Starting model: Llama 3.1 8B Figure 4 | Training convergence plot for the com-
• Hidden dimension: unchanged pressed 8B student model. We compare supervision
• MLP hidden dimension: unchanged from the original teacher and the corrected teacher.
• Attention heads: unchanged
• Depth: 32 → 16
this to be an artifact of the dataset used for fine-
tuning.
MN-Minitron-8B:
• Starting model: Mistral NeMo 12B
Retraining: Following the learnings in the Mini-
• Hidden dimension: 5120 → 4096
tron work [1], we opt for logit-only distillation, mini-
• MLP hidden dimension: 14336 → 11520
mizing the forward KL Divergence [7] loss across the
• Attention heads: unchanged
teacher and student probabilities, and ignore the LM
• Depth: unchanged
cross-entropylossaltogether. Here,theunprunedand
prunedmodelscorrespondtotheteacherandstudent,
respectively. We use the hyperparameters listed in
Distillation
Table 4 during distillation. We use 32 NVIDIA DGX
Teacher Correction: UsingtheMistralNeMo12B H100 nodes for our training jobs.
modeldirectlyasateacherperformssub-optimallyon
our dataset. This is due to the change in distribution
Llama-3.1- MN-Minitron
of sub-word tokens across the original dataset the Minitron-4B 8B
teacher model was trained on vs. the dataset being
Peaklearningrate 1e-4 1e-4
distilledon. Toaccountforthis, wefirstfine-tunethe Minlearningrate 1e-5 4.5e-7
teacheronourdatasetusing∼127Btokens. Asshown Warm-upsteps 40steps 60steps
LRdecayschedule Cosine Cosine
inFigure4,suchacorrectionisessentialiftheoriginal Globalbatchsize 1152 768
dataset is not available during distillation. We thus Contextlength 8192 8192
Totaltokens 94B 380B
apply this technique on both the Mistral-NeMo and
Llama-3.1 teacher models. The fine-tuning process
Table 4 | Hyperparameters used during distillation-
has a minor effect on the teacher model’s accuracy
based retraining.
on downstream tasks, with some tasks improving and
somedegradingasshowninTable 1. Wehypothesize
Instruction Tuning
LLaMa-3.1-Minitron-4B MN-Minitron
Width Depth 8B
To evaluate the instruction-following capabilities of
Totalparams 4.5B 4.5B 8.4B our distilled models, we perform supervised fine-
Non-Embparams 3.7B 3.5B 7.3B
tuning (SFT) on the Llama-3.1-Minitron 4B mod-
Hiddensize 3072 4096 4096
Vocabulary 128256 128256 131072 els using NeMo-Aligner [11] with the instruction
MLPhiddendim 9216 14336 11520 tuning dataset used for Nemotron-4 340B [12]. As
Depth 32 16 40
shown in Table 2, we evaluate the aligned models for
Attentiongroups 8 8 8
Queryheads 32 32 32 instruction- following and roleplay (IFEval [13] and
Headdimension 128 128 128 MT-Bench [14]), RAG QA (ChatRAG-Bench [15]),
and function-calling capabilities (BFCL [16]).
Table 3 | Architecture details of our compressed mod-
els.
4
ssoL
noitadilaV
MLLLMPruningandDistillationinPractice: TheMinitronApproach
Analysis
LM Validation Loss vs Training Steps
Random Init + Distillation Pruning + LM Loss
3.00 Random Pruning + Distillation Pruning + Distillation
We perform a series of ablation studies to better
understand the compression characteristics of these 2.75
newer models. We report our results in this section.
2.50
2.25
Width vs Depth Pruning: Figure 5 shows the
2.00
training curve of Llama-3.1-Minitron-4B pruned for
widthvs. depth. Wenoticethatwidthpruningresults 1 2 3 4 5 6 7 8 9
Training Tokens(B)
in smaller initial loss and consistently outperforms
thedepth-prunedmodel,despitebothvariantshaving
Figure6|TrainingconvergenceplotforMistralNemo
the same number of parameters.
12B compressed model. We compare (a) random
initialization with distillation, (b) randomly pruned
PruningandDistillation: Figure 6demonstrates weights with distillation, (c) pruning with standard
orthogonal benefits of our proposed approach with LM loss, and (d) our pipeline with pruning and dis-
pruning and distillation. We compare (1) random tillation.
weightinitializationanddistillation,(2)randomprun-
ing and distillation, where components are pruned LM Validation Loss vs Training Steps
Prune corrected teacher + distill corrected teacher
randomly ignoring the importance scores, (3) our
Prune original teacher + distill continuously corrected teacher
proposed pruning with typical cross entropy based
1.90
LM loss training and (4) our proposed pruning with
1.85
distillation-based training. We notice that prun-
ing results in a significantly better starting point 1.80
compared to random initialization, and also that 1.75
distillation-based training outperforms conventional
1.70
training methods while requiring significantly fewer
training tokens (up to 50× in our case). 20 40 60 80 100 120
Training Tokens(B)
LM Validation Loss vs Training Steps Figure7|TrainingconvergenceplotforMistralNemo
Llama-3.1-Minitron-4B-Width Llama-3.1-Minitron-4B-Depth 12B compressed model. We compare (1) pruning and
2.4
distilling the corrected teacher with (2) pruning the
original teacher and distilling from a continuously
2.2
corrected teacher.
2.0
Depth Pruning Metrics: when examining how
1.8
LM validation loss increases as contiguous blocks of
0 20 40 60 80 100 layers are removed (Figure 8), we observe that the
Training Tokens(B)
layers at the beginning and end are the most im-
portant. Removing non-contiguous layers can result
Figure 5 | Convergence of width- and depth-pruned
in even better LM validation loss (the dashed line).
Llama 3.1 8B to 4B models. Width pruning consis-
However, this observation does not necessarily hold
tently outperforms depth pruning for a given param-
when evaluating downstream task performance. Fig-
eter budget.
ure 9showsthatdropping16layersselectedbasedon
per-layer importance ( [5, 17]) yields a random Wino-
grande accuracy of 0.5, while removing layers 16 to
Teacher Correction: We compare two approaches 31 continuously ( [10]) results in an accuracy of 0.595.
for teacher correction: (1) pruning and distilling the Thegapholdsduringdistillation-basedretrainingand
correctedteacher,and(2)pruningtheoriginalteacher we opt for the latter approach.
and distilling from a continuously corrected teacher.
The results in Figure 7 suggest that teacher correc-
tion doesn’t affect the optimality of pruning, and Evaluation
that distillation from a corrected teacher is crucial.
Teacher correction can be performed in parallel with Benchmarks following Touvron et al. [18], we eval-
distillation to bridge the gap. uateourcompressedmodelsonaseriesofdownstream
5
ssoL
noitadilaV
ML
ssoL
noitadilaV
ML
ssoL
noitadilaV
MLLLMPruningandDistillationinPractice: TheMinitronApproach
LM Validation loss for different set of layers dropped Base Models
Baseline (32 layers) drop 2 layers drop 16 layers Base model evaluation results are shown in Table 1.
drop 1 layer drop 8 layers drop 16 non-continuous
Compared to similarly-sized models, MN-Minitron-
12
8B demonstrates superior accuracy across the board,
10
outperforming the recent Llama 3.1 8B model using
8
40× fewer training tokens (380B vs. 15T). Similarly,
6 the Llama-3.1-Minitron 4B models perform favorably
4 compared to the teacher Llama 3.1 8B model using
2 150×fewertrainingtokens(94Bvs. 15T);ourpruned
4 8 12 16 20 24 28 32 Llamamodelsalsooutperformthepreviousgeneration
layer no.
Minitron 4B model. We note from Table 1 that the
Figure 8 | LM loss value on validation set after re- width-pruned variant outperforms the depth-pruned
moving 1, 2, 8 or 16 contiguous layers with Llama one. Theseresultsclearlydemonstratetheadvantages
3.1 8B. For example, the purple line at layer no. 16 ofourmethodology: state-of-the-artaccuracycoupled
indicatestheLMlossifwedroppedthefirst16layers. with an order of magnitude improvement in training
Layer no. 17 indicates the LM loss if we leave the efficiency.
first layer intact and drop layers 2 to 17. The dashed
line corresponds to LM loss value when removing 16
Instruct Models
non-contiguous layers least increasing the loss.
The performance of the instruction-tuned Llama-3.1-
Minitron4BvariantsisshowninTable2. Wecompare
theLlama-3.1-Minitron4Bvariantstoothersimilarly-
Accuracy for different set of 16 layers dropped
sized baselines and notice that our models demon-
Baseline (32 layers) Dropped 16 layers non-continuous strate strong instruction-following and roleplay capa-
Dropped 16 layers
bilities, only lagging behind Gemma2 in IFEval [13]
0.75
and MT-Bench [14]. On retrieval based question an-
0.70
swering (ChatRAG-Bench [15]) and function-calling
0.65
(BFCL[16]), Minitronmodelsachievestate-of-the-art
0.60 drop 16..31
performance.
0.55 drop 1..16
0.50
Insights
16 18 20 22 24 26 28 30 32
layer no.
In this Section, we summarize some interesting and
Figure 9 | Accuracy on the Winogrande task when
surprising observations.
removing 16 contiguous layers with Llama 3.1 8B.
Layer no. 17 indicates the LM loss if we leave the
General
first layer intact and drop layers 2 to 17. The dashed
line corresponds to the accuracy when removing 16
1. Teacher correction is crucial for distillation to
non-contiguous layers least increasing the loss.
work optimally on a new, unseen dataset. Fine-
tuningtheteacherwiththedatasetusedfordistil-
lation in this manner yields over a 6% reduction
inLMvalidationloss. Teachercorrectiondoesn’t
tasks, including MMLU [19], HumanEval [20] for
affect the optimality of pruning and can even be
Python code generation, several question-answering
performed in parallel with distillation.
datasets for common-sense reasoning: Arc-C [21],
2. In line with the Minitron paper’s observations,
HellaSwag [22], TruthfulQA [23] and WinoGrande [6]
we require only 380B tokens to achieve state-of-
and XL-Sum English [24] for summarization. We
the-art accuracy post pruning with distillation.
report the 5-shot performance on MMLU, 5-shot on
3. For width pruning, we achieve stronger accuracy
Winogrande, 25-shot on ARC-Challenge, 10-shot on
by retaining attention heads and pruning the
HellaSwag, 0-shot on 20% of XL-Sum and average
other dimensions (MLP intermediate dimension,
pass@1scoresforHumanEvalandMBPP.Forpass@1
embedding channels).
scores we use a temperature of 0.2 and nucleus sam-
pling [25] with top-p = 0.95. For instruction-tuned
Mistral NeMo 12B to MN-Minitron-8B:
models, we use MT-Bench [14], Instruction-Following
Eval (IFEval) [13], ChatRAG-Bench [15], and Berke- 1. Our compressed model outperforms the teacher
ley Function Calling Leaderboard (BFCL) [16]. on two benchmarks, GSM8k and HumanEval
6
ssol
noitadilaV
)tohs-5(
ednargoniWLLMPruningandDistillationinPractice: TheMinitronApproach
after pruning and distillation: GSM8k increases [2] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Bider-
from 55.7% to 58.5% and HumanEval increases man,SidBlack,AnthonyDiPofi,CharlesFoster,Lau-
from 23.8% to 36.2%. This improvement is likely renceGolding,JeffreyHsu,AlainLeNoac’h,Haonan
influenced by the dataset. However, retraining is Li,KyleMcDonell,NiklasMuennighoff,ChrisOciepa,
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
performed using the distillation loss alone.
Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
Thite, Ben Wang, Kevin Wang, and Andy Zou. A
Llama 3.1 8B to Llama-3.1-Minitron 4B: framework for few-shot language model evaluation,
12 2023.
1. Width pruning delivers better accuracy with
[3] Abhimanyu Dubey and Abhinav Jauhri et al. The
MMLU at 60.5%, while depth pruning yields
llama 3 herd of models, 2024.
58.7%, for Llama-3.1 compression.
2. Reasoning ability is impacted further signifi- [4] Mistral AI team. Mistral nemo.
cantly, with GSM8K accuracy at 41.24% for https://mistral.ai/news/mistral-nemo, 2024.
width and 16.8% for depth. Accessed: 2024.
3. Depth pruning boosts throughput, achieving ∼
[5] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning
2.7× speedup over Llama-3.1 8B, while width
Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and
pruning provides ∼1.7× speed up.
WeipengChen. ShortGPT:LayersinLargeLanguage
4. For depth pruning, we observe that dropping
ModelsareMoreRedundantThanYouExpect,2024.
contiguous layers from the model is more ef-
fective than using non-contiguous, importance- [6] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-
based pruning. vatula,andYejinChoi. WinoGrande: Anadversarial
winogradschemachallengeatscale. Commun. ACM,
64(9), 2021.
Acknowledgments
[7] Solomon Kullback and Richard A. Leibler. On in-
formation and sufficiency. Annals of Mathematical
This work would not have been possible without con- Statistics, 22(1):79–86, 1951.
tributions from many people at NVIDIA. To mention
[8] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jen-
a few:
nings, Mostofa Patwary, Sandeep Subramanian, Dan
Foundational Model: Sharath Turuvekere Sreeni- Su, Chen Zhu, Deepak Narayanan, Aastha Jhun-
vas,SauravMuralidharan,RavirajJoshi,MarcinCho- jhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei
chowski, Pavlo Molchanov, Mostofa Patwary, Daniel Liu, Ameya Mahabaleshwarkar, Osvald Nitski, An-
nika Brundyn, James Maki, Miguel Martinez, Jiax-
Korzekwa, Ashwath Aithal, Mohammad Shoeybi,
uan You, John Kamalu, Patrick LeGresley, Denys
Bryan Catanzaro and Jan Kautz
Fridman, Jared Casper, Ashwath Aithal, Oleksii
Alignment: AmeyaSunilMahabaleshwarkar,Hayley Kuchaiev, Mohammad Shoeybi, Jonathan Cohen,
Ross, Brandon Rowlett, Oluwatobi Olabiyi, Shizhe and Bryan Catanzaro. Nemotron-4 15b technical
Diao and Yoshi Suhara report, 2024.
Datasets: Sanjeev Satheesh, Jupinder Parmar, [9] JupinderParmar,SanjevSatheesh,MostofaPatwary,
ShengyangSun,JiaqiZeng,ZhilinWang,YiDong,Zi- Mohammad Shoeybi, and Bryan Catanzaro. Reuse,
han Liu, Rajarshi Roy, Wei Ping, Makesh Narsimhan don’t retrain: A recipe for continued pretraining of
language models, 2024.
Sreedhar and Oleksii Kuchaiev
TensorRT-LLM: Bobby Chen, James Shen and [10] Andrey Gromov, Kushal Tirumala, Hassan
Shapourian, Paolo Glorioso, and Daniel A. Roberts.
Chenhan Yu
Theunreasonableineffectivenessofthedeeperlayers.
Hugging Face Support: Ao Tang, Yoshi Suhara 2024.
and Greg Heinrich
[11] Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi
Zeng,YiDong,DanielEgert,ShengyangSun,Jimmy
Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz
References
Ausin,AshwathAithal,andOleksiiKuchaiev. Nemo-
aligner: Scalabletoolkitforefficientmodelalignment,
[1] Saurav Muralidharan, Sharath Turuvekere Sreenivas, 2024.
RavirajJoshi,MarcinChochowski,MostofaPatwary,
Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, [12] Nvidia, :, Bo Adler, Niket Agarwal, Ashwath Aithal,
andPavloMolchanov. Compactlanguagemodelsvia Dong H. Anh, Pallab Bhattacharya, Annika Brun-
pruning and knowledge distillation. arXiv preprint dyn, Jared Casper, Bryan Catanzaro, Sharon Clay,
arXiv:2407.14679, 2024. Jonathan Cohen, Sirshak Das, Ayush Dattagupta,
7LLMPruningandDistillationinPractice: TheMinitronApproach
Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Bhosale, Dan Bikel, Lukas Blecher, Cristian Can-
Egert, Ellie Evans, Aleksander Ficek, Denys Frid- ton Ferrer, Moya Chen, Guillem Cucurull, David
man, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,
Tomasz Grzegorzek, Robert Hero, Jining Huang, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Na-
Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, man Goyal, Anthony Hartshorn, Saghar Hosseini,
JohnKamalu,SadafKhan,OleksiiKuchaiev,Patrick RuiHou,HakanInan,MarcinKardas,ViktorKerkez,
LeGresley,HuiLi,JiweiLiu,ZihanLiu,EileenLong, Madian Khabsa, Isabel Kloumann, Artem Korenev,
AmeyaSunilMahabaleshwarkar,SomshubraMajum- Punit Singh Koura, Marie-Anne Lachaux, Thibaut
dar, James Maki, Miguel Martinez, Maer Rodrigues Lavril,JenyaLee,DianaLiskovich,YinghaiLu,Yun-
de Melo, Ivan Moshkov, Deepak Narayanan, Sean ing Mao, Xavier Martinet, Todor Mihaylov, Pushkar
Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,
Nitski, Vahid Noroozi, Guruprasad Nutheti, Christo- Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
pher Parisien, Jupinder Parmar, Mostofa Patwary, Alan Schelten, Ruan Silva, Eric Michael Smith, Ran-
Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, jan Subramanian, Xiaoqing Ellen Tan, Binh Tang,
Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Saba- Ross Taylor, Adina Williams, Jian Xiang Kuan,
vat, Sanjeev Satheesh, Jane Polak Scowcroft, Ja- Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,
son Sewall, Pavel Shamis, Gerald Shen, Moham- AngelaFan,MelanieKambadur,SharanNarang,Au-
mad Shoeybi, Dave Sizer, Misha Smelyanskiy, Fe- relien Rodriguez, Robert Stojnic, Sergey Edunov,
lipe Soares, Makesh Narsimhan Sreedhar, Dan Su, and Thomas Scialom. Llama 2: Open foundation
Sandeep Subramanian, Shengyang Sun, Shubham and fine-tuned chat models. ArXiv, abs/2307.09288,
Toshniwal,HaoWang,ZhilinWang,JiaxuanYou,Ji- 2023.
aqiZeng,JimmyZhang,JingZhang,VivienneZhang,
Yian Zhang, and Chen Zhu. Nemotron-4 340b tech- [19] Dan Hendrycks, Collin Burns, Steven Basart, Andy
nical report, 2024. Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. Measuring massive multitask language under-
[13] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid- standing. In International Conference on Learning
dharthaBrahma,SujoyBasu,YiLuan,DennyZhou, Representations, 2021.
andLeHou.Instruction-followingevaluationforlarge
language models. arXiv preprint arXiv:2311.07911,
[20] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
2023.
Yuan, Henrique Ponde, Jared Kaplan, Harrison Ed-
wards,YuraBurda,NicholasJoseph,GregBrockman,
[14] LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Alex Ray, Raul Puri, Gretchen Krueger, Michael
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Petrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,
Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Joseph E Gonzalez, and Ion Stoica. Judging llm-as-
Pavlov, Alethea Power, Lukasz Kaiser, Moham-
a-judge with mt-bench and chatbot arena. In A. Oh,
mad Bavarian, Clemens Winter, Philippe Tillet, Fe-
T. Naumann, A. Globerson, K. Saenko, M. Hardt,
lipe Petroski Such, David W. Cummings, Matthias
and S. Levine, editors, Advances in Neural Infor-
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
mation Processing Systems, volume 36, pages 46595–
Herbert-Voss, William H. Guss, Alex Nichol, Igor
46623. Curran Associates, Inc., 2023.
Babuschkin, Suchir Balaji, Shantanu Jain, Andrew
Carr,JanLeike,JoshuaAchiam,VedantMisra,Evan
[15] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu,
Morikawa, Alec Radford, Matthew M. Knight, Miles
Chankyu Lee, Mohammad Shoeybi, and Bryan
Brundage, Mira Murati, Katie Mayer, Peter Welin-
Catanzaro. Chatqa: Surpassing gpt-4 on conversa-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
tional qa and rag. arXiv preprint arXiv:2401.10225,
Ilya Sutskever, and Wojciech Zaremba. Evaluat-
2024.
ing large language models trained on code. ArXiv,
abs/2107.03374, 2021.
[16] Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji,
Tianjun Zhang, Shishir G. Patil, Ion Stoica, and
[21] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar
JosephE.Gonzalez. Berkeleyfunctioncallingleader-
Khot, Ashish Sabharwal, Carissa Schoenick, and
board. https://gorilla.cs.berkeley.edu/blogs/
Oyvind Tafjord. Think you have solved question
8_berkeley_function_calling_leaderboard.html,
answering? try ARC, the AI2 reasoning challenge.
2024.
ArXiv, abs/1803.05457, 2018.
[17] Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich,
Thomas Breuel, Jan Kautz, David Krueger, and [22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Pavlo Molchanov. A deeper look at depth pruning Farhadi, and Yejin Choi. HellaSwag: Can a ma-
of llms. arXiv preprint arXiv:2407.16286, 2024. chinereallyfinishyoursentence? InAnnaKorhonen,
David Traum, and Lluís Màrquez, editors, Proceed-
[18] Hugo Touvron, Louis Martin, Kevin Stone, Peter ings of the 57th Annual Meeting of the Association
Albert, Amjad Almahairi, Yasmine Babaei, Nikolay for Computational Linguistics, Florence, Italy, July
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti 2019. Association for Computational Linguistics.
8LLMPruningandDistillationinPractice: TheMinitronApproach
[23] Stephanie Lin, Jacob Hilton, and Owain Evans.
Truthfulqa: Measuring how models mimic human
falsehoods, 2022.
[24] Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Is-
lam, Kazi Samin, Yuan-Fang Li, Yong-Bin Kang,
M. Sohel Rahman, and Rifat Shahriyar. Xl-sum:
Large-scale multilingual abstractive summarization
for 44 languages, 2021.
[25] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes,
and Yejin Choi. The curious case of neural text
degeneration. ArXiv, abs/1904.09751, 2019.
9