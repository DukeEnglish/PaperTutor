Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion
Models
Chun-YenShih12*,Li-XuanPeng2*,Jia-WeiLiao12,ErnieChu2,
Cheng-FuChou1,Jun-ChengChen2
1NationalTaiwanUniversity,
2ResearchCenterforInformationTechnologyInnovation,AcademiaSinica
Abstract
DiffusionModelshaveemergedaspowerfulgenerativemod-
els for high-quality image synthesis, with many subsequent SDEdit
imageeditingtechniquesbasedonthem.However,theease
oftext-basedimageeditingintroducessignificantrisks,such
as malicious editing for scams or intellectual property in-
Original Image Original Edit Result
fringement.Previousworkshaveattemptedtosafeguardim-
ages from diffusion-based editing by adding imperceptible
AtkPDM
perturbations.Thesemethodsarecostlyandspecificallytar-
getprevalentLatentDiffusionModels(LDMs),whilePixel-
domainDiffusionModels(PDMs)remainlargelyunexplored
Protection Against Diffusion-based Image Editing
and robust against such attacks. Our work addresses this
gap by proposing a novel attacking framework with a fea-
turerepresentationattacklossthatexploitsvulnerabilitiesin
denoising UNets and a latent optimization strategy to en-
hancethenaturalnessofprotectedimages.Extensiveexper- SDEdit
iments demonstrate the effectiveness of our approach in at-
tackingdominantPDM-basededitingmethods(e.g.,SDEdit)
whilemaintainingreasonableprotectionfidelityandrobust-
ness against common defense methods. Additionally, our Protected Image Protected Edit Result
frameworkisextensibletoLDMs,achievingcomparableper-
formancetoexistingapproaches. Figure1:Overviewofourattackscenario.Diffusion-based
image editing can generate high-quality image variation
1 Introduction based on the clean input image. However, by adding care-
fully crafted perturbation to the clean image, the diffusion
Inrecentyears,GenerativeDiffusionModels(GDMs)(Ho, process will be disrupted, producing a corrupted image or
Jain, and Abbeel 2020; Song, Meng, and Ermon 2021) unrelatedimagesemanticstotheoriginalimage.
emerged as powerful generative models that can produce
high-qualityimages,propellingadvancementsinimageedit-
ingandartisticcreations.Theeaseofusingthesemodelsto
tentDiffusionModels(LDMs)byminimizingthelatentdis-
edit(Mengetal.2021;Wang,Zhao,andXing2023;Zhang
tancebetweentheprotectedimagesandtheirtargetcounter-
etal.2023)orsynthesizenewimage(DhariwalandNichol
parts. PhotoGuard first introduce attacking either encoders
2021;Rombachetal.2022)hasraisedconcernsaboutpoten-
or diffusion process in LDMs via Projected Gradient De-
tialmalicioususageandintellectualpropertyinfringement.
scent (PGD) (Madry et al. 2018) for the protection pur-
For example, malicious users could effortlessly craft fake
pose;however,itrequiresbackpropagatingtheentirediffu-
imageswithsomeone’sidentityormimicthestyleofaspe-
sionprocess,makingitprohibitivelyexpensive.Subsequent
cificartist.Aneffectiveprotectionagainstthesethreatsisre-
worksAdvDM(Liangetal.2023)andMist(LiangandWu
garded as the diffusion model generating corrupted images
2023)leveragethesemanticlossandtexturallosscombined
or unrelated images to original inputs. Researchers have
with Monte Carlo method to craft adversarial images both
madesignificantstridesincraftingimperceptibleadversarial
effectively and efficiently. Diff-Protect (Xue et al. 2024a)
perturbationonimagestosafeguardthemfrombeingedited
further improve adversarial effectiveness and optimization
bydiffusion-basedmodels.
speed via Score Distillation Sampling (SDS) (Poole et al.
PreviousworkssuchasPhotoGuard(Salmanetal.2023)
2022),settingthestate-of-the-artperformanceonLDMs.
and Glaze (Shan et al. 2023) have effectively attacked La-
However,previousworksprimarilyfocusonLDMs,and
*Theseauthorscontributedequally. attacks on Pixel-domain Diffusion Models (PDMs) remain
4202
guA
12
]VC.sc[
1v01811.8042:viXralargelyunexplored.Xueetal. (Xueetal.2024a)alsohigh- whichaimstodisruptthelatentrepresentationsoftheVari-
lightedacriticallimitationofcurrentmethods:theattacking ationalAutoencoder(VAE)oftheLDMs,andthesecondis
effectivenessismainlyattributedtothevulnerabilityofthe theDiffusionAttack,whichfocusesmoreondisruptingthe
VAEencodersinLDM;however,PDMsdon’thavesuchen- entirediffusionprocessoftheLDMs.
coders, making current methods hard to transfer to PDMs. TheEncoderAttackissimpleyeteffective,buttheattack-
The latest work (Xue and Chen 2024) has attempted to at- ingresultsaresub-optimalduetoitslessflexibilityforopti-
tackPDMs,buttheresultsuggeststhatPDMsarerobustto mizationthantheDiffusionAttack.AlthoughtheDiffusion
pixel-domain perturbations. Our goal is to mitigate the gap Attack achieves better attack results, it is prohibitively ex-
betweentheselimitations. pensive due to its requirement of backpropagation through
In this paper, we propose an innovative framework de- all the diffusion steps. In the following, we further intro-
signedtoeffectivelyattackPDMs.Ourapproachincludesa ducemorerelevantworkfortheseattacksalongwithanother
novelfeatureattackinglossthatexploitsthevulnerabilities commonattackfordiffusionmodels.
in denoising UNet to distract the model from recognizing
DiffusionAttacks. DespitethecostofperformingtheDif-
the correct semantics of the image, a fidelity loss that acts
fusionAttack,thehighergeneralizabilityanduniversallyap-
as optimization constraints that ensure the imperceptibility
plicablenaturedrivepreviousworksfocusingondisrupting
ofadversarialimageandcontrolstheattackbudget,andala-
theprocesswithlowercost.Liangetal.(Liangetal.2023)
tentoptimizationstrategyutilizingvictim-model-agnostic
proposed AdvDM to utilize the diffusion training loss as
VAEs to further enhance the naturalness of our adversarial
their attacking semantic loss. Then, AdvDM performs gra-
imageclosertotheoriginal.Withextensiveexperimentson
dientascentwiththeMonteCarlomethod,aimingtodisrupt
differentPDMs,theresultsshowthatourmethodiseffective
thedenoisingprocesswithoutcalculatingfullbackpropaga-
and affordable while robust to traditional defense methods
tion. Mist (Liang and Wu 2023) also incorporates seman-
andexhibitingattacktransferabilityintheblack-boxsetting.
tic loss and performs constrained optimization via PGD to
In addition, our approach outperforms current semantic-
achievebetterattackingperformance.
loss-based and PGD-based methods, reaching state-of-the-
art performance on attacking PDMs. Our contributions are Encoder Attacks. On the other hand, researchers found
summarizedasfollows:
thatVAEsinwidelyadoptedLDMsaremorevulnerableto
1. We propose a novel attack framework targeting PDMs attackatalowercostwhileavoidingattackingtheexpensive
, achieving state-of-the-art performance in safeguarding diffusionprocess.(Salmanetal.2023;LiangandWu2023;
imagesfrombeingeditedbySDEdit. Shanetal.2023;Xueetal.2024b),focusondisruptingthe
latent representation in LDM via PGD and highlights the
2. Weproposeanovelfeatureattackinglossdesigntodis-
encoderattacksaremoreeffectiveagainstLDMs.
tractUNetfeaturerepresentationeffectively.
3. We propose a latent optimization strategy via model- Conditional Module Attacks. Most of the LDMs con-
agnostic VAEs to enhance the naturalness of our adver- tain conditional modules for steering generation, previous
sarialimages. works(Shanetal.2023,2024;Loetal.2024)exploitedthe
vulnerabilityoftextconditioningmodules.Bydisruptingthe
2 RelatedWork cross-attentionbetweentextconceptsandimagesemantics,
thesemethodscouldeffectivelyinterferewiththediffusion
2.1 EvasionAttackforDiffusionModel modelforcapturingtheimage-textalignment,therebyreal-
Theprevalenceofdiffusion-basedimageeditingtechniques izingtheattack.
has posed the challenges of protecting images from mali-
Limitations of Current Methods. To our knowledge,
ciously editing. These editing techniques are mainly based
previous works primarily focus on adversarial attacks for
on SDEdit (Meng et al. 2021) or its variations which can
LDMs, while attacks on PDMs remain unexplored. Xue et
beappliedtobothPDMandLDMtoproducetheeditingre-
al. (Xue and Chen 2024) further emphasized the difficulty
sults.Ingeneral,theeditingstartsfromfirsttransformingthe
of attacking PDMs. However, in our work, we find that by
cleanimage(orthecleanlatent)intothenoisyonebyintro-
craftinganadversarialimagetocorrupttheintermediaterep-
ducingGaussiannoisefortheforwarddiffusionprocessfol-
resentationofdiffusionUNet,wecanachievepromisingat-
lowedbyperformingaseriesofreversediffusionsampling
tack performance for PDMs, while the attack is also com-
steps with new conditions. In addition, based on different
patiblewithLDMs.Moreover,inspiredby(Laidlaw,Singla,
numberofforwarddiffusionsteps,themethodcouldcontrol
andFeizi2021;Liuetal.2023)whichutilizesLPIPS(Zhang
theextentoftheeditingresultsobeyingthenewconditions
et al. 2018) as distortion measure, we also propose a novel
while preserving the original image semantics. Notably, a
attackinglossasthemeasuretocraftbetteradversarialim-
smallnumberofforwardstepsallowtheeditingresultsfaith-
agesforPDMs.
fultotheoriginalimage,andmorevariationsareintroduced
whenlargerforwardstepvalueisapplied.
3 BackgroundonDiffusionModels
TocounteractSDEdit-basedediting,H.Salmanetal.first
proposedPhotoGuard(Salmanetal.2023)tointroducetwo Score-based models and diffusion models allow to gener-
attacking paradigms based on Projected Gradient Descent ate samples starting from easy-to-sample Gaussian noise
(PGD)(Madryetal.2018).ThefirstistheEncoderAttack, to complex target distributions via iteratively applying thescore function of learned distribution during sampling,
i.e., the gradient of underlying probability distribution
xadv
xlogp(x) with respect to x. However, the exact estima- x t t
t∇ ionsofthescorefunctionsareintractable. t
To bypass this problem, Yang Song et al. proposed slice
score matching (Song et al. 2020), and Ho et al. proposed t 1
−
DenoisingDiffusionProbabilityModel(DDPM)(Ho,Jain,
andAbbeel2020)thatfirstgraduallyperturbsthecleandata max
attack
with linear combinations of Gaussian noise and clean data, L
as x = √α¯ x + √1 α¯ ϵ via the predefined timestep
t t t t
−
schedulers where t [0,T] and ϵ t (0,I), then they Forward
∈ ∼ N
finally become isotropicGaussian noise as timereaches T,
Process
thisisalsoreferredasforwarddiffusion.Thegoalistotrain
a time-dependent neural network that can learn to denoise
noisysamplesgivencorrespondingtimestept.Specifically,
SDEdit(xadv)
the training objective is the expectation over noise estima-
tionMSE,whichisformulatedasE t,x,ϵt[ ∥ϵ t −ϵ θ(x t,t) ∥2 2], min fidelity
where ϵ denotes the parametrized neural network, DDPM L
θ
adopted UNet (Ronneberger, Fischer, and Brox 2015) as xadv
their noise estimating network. During inference time, we x
SDEdit(x)
firstgeneratearandomGaussiansample,theniterativelyap-
ply the noise estimation network ϵ and perform denoising
θ Image Manifold
operations to generate a new clean sample of the learned
M
distribution.Particularly,Songet.alproposedDDIM(Song,
Meng, and Ermon 2021) that generalized the DDPM sam- Figure 2: Conceptual illustration of our method. We ran-
plingformulationas: domlyforwardboththecleanimagexandadversarialimage
xadvtonoiselevelt,thenutilizeourfeatureattackinglossto
(cid:18) (cid:19)
x =√α¯ x t −√1 −α¯ tϵ θ(x t,t) maximize the feature distance between noisy latent x t and
t−1 t−1 √α¯ xadv in the reverse process of diffusion models while im-
t (1) t
(cid:113) posing our fidelity loss as a constraint to ensure the adver-
+ 1 −α¯ t−1 −σ t2ϵ θ(x t,t)+σ tϵ t. sarial image from being deviated from the original image.
(cid:112) (cid:112) Weupdatethexadv inlatentspaceinsteadofinpixelspace
With σ = (1 α¯ )/(1 α¯ ) 1 α¯ /α¯ ,
t − t−1 − t · − t t−1 toensurethenaturalnessofxadv.
Equation 1 becomes DDPM, and when σ = 0, the sam-
t
pling process become deterministic as proposed in DDIM
sincetheaddednoiseduringeachsamplingstepisnull.
and conditional editing pipelines. Formally, our objective
toeffectivelysafeguardimageswhilemaintainingfidelityis
4 Methodology
formulatedas:
4.1 ThreatModelandProblemSetting
Themalicioususercollectsanimagexfromtheinternetand max d(SDEdit(x,t),SDEdit(xadv,t))
uses SDEdit (Meng et al. 2021) to generate unauthorized xadv∈M (2)
image translations or editing, denoted as SDEdit(x,t), that subjecttod′(x,xadv) δ,
≤
manipulatestheoriginalinputimagex.
where indicatesnaturalimagemanifold,d,d′indicate
Our work aims to safeguard the input image x from the M
imagedistancefunctionsandϵdenotesthefidelitybudget.
authorized manipulations by crafting an adversarial image
In the following sections, we first present a conceptual
xadv byaddingimperceptibleperturbationtodisruptthere-
illustration of our method, followed by our framework for
versediffusionprocessofSDEditforcorruptededitions.
solvingtheoptimizationproblem.Wethendiscussthenovel
Forexample,wewantthemainobjectoftheimage,e.g.,
design of our attacking loss and fidelity constraints, which
the cat in the source image x as shown in Figure 2 unable
provide more efficient criteria compared to previous meth-
tobereconstructedbythereversediffusionprocess.Mean-
ods for solving the image protection optimization problem
while, the adversarial image should maintain similarity to
usingPGD.Finally,weintroduceanadvanceddesigntoen-
thesourceimagetoensurefidelity.Thereasonwhywetar-
hance image protection quality by latent optimization via
get SDEdit as our threat model is that it is recognized as
victim-model-agnosticVAE.
the most common and general operation in diffusion-based
unconditionalimagetranslationsandconditionalimageedit-
4.2 Overview
ing.Additionally,ithasbeenincorporatedintovariousedit-
ingpipelines(TsabanandPassos2023;Zhangetal.2023). Toachieveeffectiveprotectionagainstdiffusion-basededit-
Here we focus on the unconditional image translations for ing,weaimtopushtheprotectedsampleawayfromtheorig-
our main study, as they are essential in both unconditional inalcleansamplebydisruptingtheintermediatestepintheImage Fidelity Constraint Feature Attack
Add Noise
θ
U
One-time
Initialization Original Image x x t f t= N(µ t,Σ t)
W2(ft,f tadv)
E
Add Noise
θ
U
zadv D Adversarial Imagexadv xa tdv f tadv= N(µa tdv,Σa tdv)
Lfidelity Lattack
Alternating Latent Optimization
Figure3:OverviewofourAtkPDM+ algorithm:Startingfromtheleftmostlatentoftheinitialadversarialimagezadv,wefirst
decodebacktopixel-domaintoperformforwarddiffusionwithbothxandxadvandfeedthemtofrozenvictimUNet.Wethen
extract the feature representation in UNet to calculate our , aiming to distract the recognition of image semantics. We
attack
alsocalculateour inpixel-domaintoconstraintheopL timization.Finally,thezadv isbeingalternativelyupdatedbyloss
fidelity
L
gradients.
reverse diffusion process. For practical real-world applica- throughallthediffusionsteps.Attackinglossisdesignedto
tions,it’sessentialtoensuretheprotectedimageisperceptu- distract the feature representation in denoising UNet; Pro-
allysimilartotheoriginalimage.Inpractice,weuniformly tection loss is a constraint to ensure the image quality. For
sample the value of the forward diffusion step t [0,T] notation simplicity, we first define the samples x,xadv in
∼
to generate noisy images and then perform optimization to differentforwardedsteps.
craft the adversarial image xadv via our attacking and fi- Let (x,t,ϵ) = √α¯ x+√1 α¯ ϵbethediffusionfor-
t t
F −
delity losses, repeating the same process n times or until ward process. Given timestep t sample from [0,T], noises
convergence. Figure 2 depicts these two push-and-pull cri- ϵ ,ϵ sample from (0,I). We denote x = (x,t,ϵ ),
1 2 t 1
teria during different noise levels, the successful attack is andxadv = (xadvN ,t,ϵ ). F
t F 1
reflected in the light orange line where the reverse sample
Attacking Loss. Our goal is to define effective criteria
movesfarawayfromthenormaleditionoftheimage.More
that could finally distract the reverse denoising process.
specifically,ourmethodcanbeformulatedasfollows:
PhotoGuard (Salman et al. 2023) proposed to backpropa-
max E (cid:2) (x ,xadv)(cid:3) gate through all the steps of the reverse denoising process
xadv∈M t,xt|x,xa tdv|x −Lattack t t (3) viaPGD,however,thisapproachisprohibitivelyexpensive,
subjectto (x,xadv) δ, Diff-Protect (Xue et al. 2024b) proposed to avoid the mas-
fidelity
L ≤ sivecostbyleveragingScoreDistillation(Pooleetal.2022)
whereδ denotestheattackingbudget.Thedetailsoftheat-
inoptimization.However,Diff-protectreliesheavilyongra-
tacking loss and the fidelity loss will be dis-
Lattack Lfidelity dients of attacking encoder of an LDM as stated in their
cussedinthefollowingsections.
results. In PDM, we don’t have such an encoder to attack;
Framework. OurframeworkisillustratedinFigure3.We nevertheless, we find that the denoising UNet has a simi-
fixtwoidenticalvictimUNetstoextractfeaturerepresenta- larstructuretoencoder-decodermodels,andsomeprevious
tionsofcleanandprotectedsamplesforoptimizingtopush works(LinandYang2024;Lietal.2023)characterizethis
away from each other. A protection loss is jointly incorpo- propertytoaccelerateandenhancethegeneration.Fromour
rated to constrain the optimization. After N iterations, we observationsofthefeaturerolesindenoisingUNets,wehy-
segment out only the protecting main object of the image pothesizethatdistractingspecificinherentfeaturerepresen-
forbetterimperceptibilityofimageprotection. tation in UNet blocks could lead to effectively crafting an
adversarial image. In practice, we first extract the feature
4.3 ProposedLosses representations of forwarded images x and xadv in frozen
t t
We propose two novel losses as optimization objectives UNet blocks of timestep t. Then, we adopt 2-Wasserstein
to craft an adversarial example efficiently without running distance (Arjovsky, Chintala, and Bottou 2017) to measurethediscrepancyinfeaturespace.Notethatwetaketheneg- Algorithm1:AtkPDM+
ative of the calculated distance, as we aim to pull the xadv
t 1: Input:Imagetobeprotectedx,attackbudgetδ>0,stepsize
awayfromx .Formally,theattackingloss isdefined
t attack γ ,γ >0,VAEencoderE,andVAEdecoderD
L 1 2
as: 2: Initialization:xadv ←x,L ←∞
(x ,xadv)= (cid:16) (mid)(x ), (mid)(xadv)(cid:17) . (4) 3: Encodeadversarialimagetoa lt ata tc ek ntspace:zadv ←E(xadv)
Lattack t t −W2 Uθ t Uθ t 4: whileL notconvergentdo
attack
Assuming the feature distributions approximate Gaussian 5: Decodeadversariallatenttopixelspace:xadv ←D(zadv)
distributions expressed by mean µ and µadv, and non- 6: Sampletimestep:t∼[0,T]
t t
singular covariance matrices Σ and Σadv. The calculation 7: Samplenoise:ϵ 1,ϵ 2 ∼N(0,I)
t t 8: Computeoriginalnoisysample:x ←F(x,t,ϵ )
ofthe2-Wassersteindistancebetweentwonormaldistribu- t 1
tions is viable through the closed-form solution (Dowson 9: Computeadversarialnoisysample:xa tdv ←F(xadv,t,ϵ 2)
andLandau1982;OlkinandPukelsheim1982;Chen,Geor- 10: UpdatezadvbyGradientDescent:
zadv ←zadv−γ sign(∇ L (x ,xadv))
giou,andTannenbaum2018): 1 zadv attack t t
11: whileL (x,D(zadv))>δdo
fidelity
12: zadv ←zadv−γ ∇ L (x,D(zadv))
2( (µ ,Σ ), (µadv,Σadv))= µ µadv 2 2 zadv fidelity
W2 N t t N t t ∥ t − t ∥2 (5) 13: endwhile
+trace(Σ t+Σa tdv −2(Σa tdv21 Σ tΣa tdv1 2)1 2). 1 14 5: : e Dn ed cow dh ei ale dversariallatenttopixelspace:xadv ←D(zadv)
FidelityLoss. Tocontroltheattackbudgetforadversarial 16: return xadv
image quality, we design a constraint function that utilizes
thefeatureextractorfromapretrainedclassifierforcalculat-
ing fidelity loss. In our case, we sum up the 2-Wasserstein DetailedlatentoptimizationloopisprovidedinAlgorithm1.
feature losses of L different layers. Specifically, we define
fidelityas: 5 ExperimentResults
L
(cid:88)L Inthissection,weexaminetheattackeffectivenessandro-
Lfidelity(x t,xa tdv)= W2(ϕ ℓ(x),ϕ ℓ(xadv)), (6) bustnessofourapproachunderextensivesettings.
ℓ=1
where denotes 2-Wasserstein distance and ϕ denotes 5.1 ExperimentSettings
2 ℓ
W
layerℓofthefeatureextractor. ImplementationDetails. Weconductallourexperiments
in white box settings and examine the effectiveness of our
4.4 AlternatingOptimizationforAdversarial
attacks using SDEdit (Meng et al. 2021). For the Varia-
Image
tional Autoencoder (VAE) (Kingma and Welling 2014) in
We solve the constrained optimization problems via alter- our AtkPDM+, we utilize the VAE provided by StableDif-
natingoptimizationtocrafttheadversarialimages,detailed fusionV1.5 (Rombachetal.2022).Werunallofourexper-
optimizationloopofAtkPDM+isprovidedinAlgorithm 1. imentswith300optimizationsteps,whichisempiricallyde-
AtkPDMalgorithmandthederivationofthealternatingop- termined,tobalanceattackingeffectivenessandimagepro-
timizationareprovidedinAppendix. tectionqualitywithreasonablespeed.Otherlossparameters
andrunningtimeareprovidedintheAppendix.Theimple-
4.5 LatentOptimizationviaPretrained-VAE
mentation is built on the Diffusers library. All the experi-
Previousworkssuggestthatdiffusionmodelshaveastrong mentsareconductedwithasingleNvidiaTeslaV100GPU.
capability of resisting adversarial perturbations (Xue and
Victim Models and Datasets. We test our approach
Chen 2024), making them hard to attack via pixel-domain
on PDMs with three open-source checkpoints on Hug-
optimization.Moreover,theyareevenconsideredgoodpu-
gingFace, specifically “google/ddpm-ema-church-256”,
rifiersofadversarialperturbations(Nieetal.2022).Herewe
”google/ddpm-cat-256” and “google/ddpm-ema-celebahq-
proposealatentoptimizationstrategythatcraftsthe“pertur-
256”.FortheresultsreportedinTable1,werun30images
bation” in latent space. We adopt a pre-trained Variational
for each victim model. Additionally, for generalizability in
Autoencoder(VAE) (KingmaandWelling2014)toconvert
practical scenarios, we synthesize the data with half ran-
images to their latent space, and the gradients will be used
domly from the originally trained dataset and another half
to update the latent, after N iterations or losses converge,
fromrandomlycrawledwithkeywordsfromtheInternet.
we decode back via decoder to pixel domain as our fi-
D
nalprotectedimage.ThemotivationforadoptingVAEisin- Baseline Methods and Evaluation Metrics. To the best
spiredbyMPGD(Heetal.2024).Thisstrategyiseffective of our knowledge, previous methods have mainly focused
forcraftingarobustadversarialimageagainstpixel-domain on LDMs, and effective PDM attacks have not yet been
diffusionmodelswhilealsobetterpreservingtheprotection developed, however, we still implement Projected Gradi-
quality rather than only incorporating fidelity constraints. ent Ascent (PGAscent) with their proposed semantic loss
Theconstraintoptimizationtherebybecomes: by (Salman et al. 2023; Liang et al. 2023; Liang and Wu
maxE (cid:2) (x ,xadv)(cid:3) 2023; Xue et al. 2024b). Notably, Diff-Protect (Xue et al.
zadv
t,xt|x,xa tdv|D(zadv) −Lattack t t
(7) 2024b) proposed to minimize the semantic loss is surpris-
subjectto (x, (zadv)) δ. ingly better than maximizing the semantic loss, we also
fidelity
L D ≤AdversarialImageQuality AttackingEffectiveness
Methods
SSIM↑ PSNR↑ LPIPS↓ SSIM↓ PSNR↓ LPIPS↑ IA-Score↓
PGAscent 0.37±0.09 28.17±0.22 0.73±0.16 0.89±0.05 31.06±1.94 0.17±0.09 0.93±0.04
Diff-Protect 0.39±0.07 28.03±0.12 0.67±0.11 0.82±0.05 31.90±1.08 0.23±0.07 0.91±0.04
AtkPDM(Ours) 0.75±0.03 28.22±0.10 0.26±0.04 0.75±0.04 29.61±0.23 0.40±0.05 0.76±0.06
atkPDM+(Ours) 0.81±0.03 28.64±0.19 0.13±0.02 0.79±0.04 30.05±0.47 0.33±0.07 0.81±0.06
PGAscent 0.48±0.09 28.34±0.18 0.65±0.12 0.96±0.02 32.32±2.49 0.10±0.05 0.97±0.03
Diff-Protect 0.33±0.10 28.03±0.15 0.80±0.15 0.90±0.05 33.94±1.93 0.18±0.08 0.95±0.03
atkPDM(Ours) 0.71±0.06 28.47±0.18 0.29±0.05 0.83±0.03 30.73±0.51 0.39±0.05 0.81±0.04
atkPDM+(Ours) 0.83±0.04 29.41±0.37 0.09±0.02 0.93±0.01 33.02±0.74 0.18±0.02 0.92±0.01
PGAscent 0.48±0.05 28.75±0.18 0.64±0.10 0.99±0.00 37.96±1.75 0.02±0.01 0.99±0.00
Diff-Protect 0.25±0.04 28.09±0.20 0.91±0.11 0.95±0.02 35.33±1.62 0.08±0.04 0.96±0.02
atkPDM(Ours) 0.56±0.04 28.01±0.22 0.36±0.04 0.74±0.03 29.14±0.36 0.40±0.05 0.62±0.07
atkPDM+(Ours) 0.81±0.04 28.39±0.20 0.12±0.03 0.86±0.03 30.26±0.72 0.24±0.07 0.80±0.08
Table 1: Quantitative Results in attacking different PDMs. The best is marked in red and the second best is marked in blue.
Errorsdenoteonestandarddeviationofallimagesinourtestdatasets.
AttackingEffectiveness AttackingEffectiveness
DefenseMethod Setting
SSIM↓ PSNR↓ LPIPS↑ IA-Score↓ SSIM↓ PSNR↓ LPIPS↑ IA-Score↓
Crop-and-Resize 0.68 29.28 0.42 0.79 WhiteBox 0.79 30.05 0.33 0.81
JPEGComp. 0.78 29.82 0.36 0.79 BlackBox 0.86 30.25 0.29 0.85
None 0.79 30.05 0.33 0.81 Difference 0.07 0.20 0.04 0.04
Table 2: Quantitative results of our adversarial images Table3:Quantitativeresultsofblackboxattack.Weusethe
against defense methods. Both Crop-and-Resize and JPEG same set of adversarial images and feed to white box and
Compressionfailtodefendourattack.“None”indicatesno blackboxmodelstoexaminetheblackboxtransferability.
defenseisapplied,asthebaselineforcomparison.
vious works (Xue et al. 2024b; Xue and Chen 2024). For
adoptedthismethodinattackingPDMsanddenoteasDiff- AtkPDM+,combinedwithourlatentoptimizationstrategy,
Protect.Toquantifytheadversarialimagevisualquality,we the adversarial image quality has enhanced while slightly
adopted Structural Similarity (SSIM) (Wang et al. 2004), affectingtheattackingeffectiveness,stilloutperformingthe
Peak Signal-to-Noise Ratio (PSNR), and Learned Percep- previousmethods.
tualImagePatchSimilarity(LPIPS) (Zhangetal.2018).We
alsoinheritthesethreemetrics,butnegativelytoquantifythe
5.3 AgainstDefenseMethods
effectiveness of our attack. We also adopted Image Align-
ment Score (IA-Score) (Kumari et al. 2023) that leverages
We examine the robustness of our approach against two
CLIP(Radfordetal.2021)tocalculatethecosinesimilarity
widely recognized and effective defense methods for de-
of image encoder features. In distinguishing from previous
fendingagainstadversarialattacksasreportedinTable2.
methods, to more faithfully reflect the attack effectiveness,
wefixthesameseedoftherandomgeneratorwhengenerat-
CropandResize. NotedbyDiff-Protect,cropandresize
ingcleanandadversarialsamples,thencalculatethescores
issimpleyetthemosteffectivedefensemethodagainsttheir
basedonthepairedsamples.
attacks on LDMs. We also test our method against this de-
fenseusingtheirsettings,i.e.,cropping20%oftheadversar-
5.2 AttackEffectivenessonPDMs
ialimageandthenresizingittoitsoriginaldimensions.
AsquantitativelyreportedinTable1andqualitativeresults
in Figure 4, compared to previous PGD-based methods in- JPEG Compression. Sandoval-Segura et al. (Sandoval-
corporatingsemanticloss,i.e.,negativetraininglossofdif- Segura, Geiping, and Goldstein 2023) demonstrated that
fusionmodels,ourmethodexhibitssuperiorperformancein JPEG compression is a simple yet effective adversarial de-
both adversarial image quality and attacking effectiveness. fensemethod.Inourexperiments,weimplementtheJPEG
Andourreportedfigureshasgenerallystableasreflectedin compressionataqualitysettingof25%.Thequantitativere-
lowerstandard deviation.It isworth notingthat evenif the sultsinTable2demonstratethatourmethodisrobustagainst
adversarial image qualities of the PGD-based methods are thesetwodefensemethods,withfourofthemetricslistedin
far worse than ours, their attacking effectiveness still falls Table 2 are not worse than no defenses. Surprisingly, these
short, suggesting that PDMs are robust against traditional defensemethodsevenmaketheadversarialimagemoreef-
perturbation methods, this finding is also aligned with pre- fectivethancaseswithoutdefense.
hcruhC
taC
ecaFFigure 4: Qualitative results compared to previous methods: our adversarial images can effectively corrupt the edited results
withoutsignificantfidelitydecrease.Thesamecolumnsharesthesamerandomseedforfaircomparison.
AdversarialImageQuality AttackingEffectiveness
Losses VAE
SSIM↑ PSNR↑ LPIPS↓ SSIM↓ PSNR↓ LPIPS↑ IA-Score↓
L 0.37±0.09 28.17±0.22 0.73±0.16 0.89±0.05 31.06±1.94 0.17±0.09 0.93±0.04
semantic
L ✓ 0.80±0.05 29.78±0.42 0.17±0.03 0.82±0.05 30.43±0.75 0.15±0.06 0.92±0.04
semantic
L +L ✓ 0.82±0.05 30.30±0.81 0.13±0.03 0.90±0.03 31.24±1.19 0.08±0.03 0.96±0.02
semantic fidelity
L +L (Ours) 0.75±0.03 28.22±0.10 0.26±0.04 0.75±0.04 29.61±0.23 0.40±0.05 0.76±0.06
attack fidelity
L +L (Ours) ✓ 0.81±0.03 28.64±0.19 0.13±0.02 0.79±0.04 30.05±0.47 0.33±0.07 0.81±0.06
attack fidelity
Table4:Quantitativeresultsofablationstudy.Thebestisinboldandthesecondbestisunderlined.Errorsdenoteonestandard
deviationofallimagesinourtestdatasets.
5.4 BlackBoxTransferability incorporating our with current PGD-based method
fidelity
L
will drastically decrease the adversarial image quality de-
We craft adversarial images with the proxy model,
spiteitsattackperformingbetterthanours.Thismaybedue
“google/ddpm-ema-church-256”, in white-box settings and
todifferentconstrainedoptimizationproblemsettings.
testtheirtransferabilitytoanother“google/ddpm-bedroom-
256” model for black-box attacks. Under identical valida-
tionsettings,Table3revealsonlyaslightdecreaseinattack 6 Conclusion
effectivenessmetrics,indicatingsuccessfulblack-boxtrans-
Inthispaper,wepresentthefirstframeworkdesignedtoef-
ferability.
fectivelyprotectimagesmanipulationbyPixel-domainDif-
fusionModels(PDMs).Wedemonstratethatwhiledenois-
5.5 EffectivenessofLatentOptimizationviaVAE
ingUNetsappearrobusttoconventionalPGD-basedattacks,
WefirstincorporateourVAElatentoptimizationstrategyin their feature space remains vulnerable to attack. Our pro-
the previous semantic-loss-based PGAscent. From Table 4, posed feature attacking loss exploit the vulnerabilities to
without using , latent optimization has significantly empoweradversarialimagestomisleadPDMs,therebypro-
fidelity
L
enhanced the adversarial image quality and even slightly ducing low-quality output images. We approach this image
improved the attacking effectiveness. Adopting latent op- protection problem as a constrained optimization problem,
timization in our approach enhances visual quality with a solvingitthroughalternatingoptimization.Additionally,our
negligible decrease in attacking effectiveness. Surprisingly, latent optimization strategy via VAE enhances the natural-ness of our adversarial images. Extensive experiments val- Liu, J.;Wei, C.; Guo, Y.;Yu, H.; Yuille,A.; Feizi, S.; Lau,
idate the efficacy of our method, achieving state-of-the-art C. P.; and Chellappa, R. 2023. Instruct2Attack: Language-
performanceinattackingPDMs. Guided Semantic Adversarial Attacks. arXiv preprint
arXiv:2311.15551.
References Lo, L.; Yeo, C. Y.; Shuai, H.-H.; and Cheng, W.-H. 2024.
Distraction is All You Need: Memory-Efficient Image Im-
Arjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasser-
munizationagainstDiffusion-BasedImageEditing. InPro-
steingenerativeadversarialnetworks. InInternationalCon-
ceedingsoftheIEEE/CVFConferenceonComputerVision
ferenceonMachineLearning(ICML),214–223.
andPatternRecognition(CVPR),24462–24471.
Chen, Y.; Georgiou, T. T.; and Tannenbaum, A. 2018. Op-
Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and
timaltransportforGaussianmixturemodels. IEEEAccess,
Vladu,A.2018. Towardsdeeplearningmodelsresistantto
7:6269–6278.
adversarial attacks. In International Conference on Learn-
Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat ingRepresentations(ICLR).
gansonimagesynthesis.InAdvancesinNeuralInformation
Meng, C.; He, Y.; Song, Y.; Song, J.; Wu, J.; Zhu, J.-Y.;
ProcessingSystems(NeurIPS),volume34,8780–8794.
and Ermon, S. 2021. Sdedit: Guided image synthesis and
Dowson, D.; and Landau, B. 1982. The Fre´chet distance editingwithstochasticdifferentialequations. arXivpreprint
betweenmultivariatenormaldistributions. Journalofmulti- arXiv:2108.01073.
variateanalysis,12(3):450–455. Nie, W.; Guo, B.; Huang, Y.; Xiao, C.; Vahdat, A.; and
Goodfellow, I.; Shlens, J.; and Szegedy, C. 2015. Explain- Anandkumar, A. 2022. Diffusion Models for Adversarial
ingandHarnessingAdversarialExamples. InInternational Purification. arXivpreprintarXiv:2312.09608.
ConferenceonLearningRepresentations(ICLR). Olkin, I.; and Pukelsheim, F. 1982. The distance between
tworandomvectorswithgivendispersionmatrices. Linear
He,Y.;Murata,N.;Lai,C.-H.;Takida,Y.;Uesaka,T.;Kim,
AlgebraanditsApplications,48:257–263.
D.;Liao,W.-H.;Mitsufuji,Y.;Kolter,J.Z.;Salakhutdinov,
R.; and Ermon, S. 2024. Manifold Preserving Guided Dif- Poole, B.; Jain, A.; Barron, J. T.; and Mildenhall, B.
fusion. InInternationalConferenceonLearningRepresen- 2022. DreamFusion:Text-to-3Dusing2DDiffusion. arXiv
tations(ICLR). preprintarXiv:2209.14988.
Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion Radford,A.;Kim,J.W.;Hallacy,C.;Ramesh,A.;Goh,G.;
probabilistic models. In Advances in Neural Information Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
ProcessingSystems(NeurIPS),volume33,6840–685. et al. 2021. Learning transferable visual models from nat-
ural language supervision. In International Conference on
Kingma,D.P.;andWelling,M.2014.Auto-EncodingVaria-
MachineLearning(ICML),8748–8763.
tionalBayes. InInternationalConferenceonLearningRep-
resentations(ICLR). Rombach,R.;Blattmann,A.;Lorenz,D.;Esser,P.;andOm-
mer, B. 2022. High-resolution image synthesis with latent
Kumari,N.;Zhang,B.;Zhang,R.;Shechtman,E.;andZhu,
diffusionmodels. InProceedingsoftheIEEE/CVFConfer-
J.-Y. 2023. Multi-concept customization of text-to-image
enceonComputerVisionandPatternRecognition(CVPR),
diffusion. In Proceedings of the IEEE/CVF Conference on
10684–10695.
Computer Vision and Pattern Recognition (CVPR), 1931–
Ronneberger,O.;Fischer,P.;andBrox,T.2015.U-net:Con-
1941.
volutionalnetworksforbiomedicalimagesegmentation. In
Laidlaw,C.;Singla,S.;andFeizi,S.2021. PerceptualAd- MedicalImageComputingandComputer-AssistedInterven-
versarialRobustness:DefenseAgainstUnseenThreatMod- tion(MICCAI),234–241.
els. In International Conference on Learning Representa-
Salman,H.;Khaddaj,A.;Leclerc,G.;Ilyas,A.;andMadry,
tions(ICLR).
A.2023. RaisingtheCostofMaliciousAI-PoweredImage
Li,S.;Hu,T.;Khan,F.S.;Li,L.;Yang,S.;Wang,Y.;Cheng, Editing. arXivpreprintarXiv:2302.06588.
M.-M.;andYang,J.2023. FasterDiffusion:Rethinkingthe
Sandoval-Segura, P.; Geiping, J.; and Goldstein, T. 2023.
RoleofUNetEncoderinDiffusionModels. arXivpreprint
JPEGcompressedimagescanbypassprotectionsagainstai
arXiv:2312.09608.
editing. arXivpreprintarXiv:2304.02234.
Liang, C.; and Wu, X. 2023. Mist: Towards Improved Ad- Shan, S.; Cryan, J.; Wenger, E.; Zheng, H.; Hanocka, R.;
versarial Examples for Diffusion Models. arXiv preprint and Zhao, B. Y. 2023. Glaze: Protecting Artists from
arXiv:2305.12683. Style Mimicry by Text-to-Image Models. arXiv preprint
Liang, C.; Wu, X.; Hua, Y.; Zhang, J.; Xue, Y.; Song, T.; arXiv:2302.04222.
Xue, Z.; Ma, R.; and Guan, H. 2023. Adversarial Exam- Shan, S.; Ding, W.; Passananti, J.; Wu, S.; Zheng, H.; and
ple Does Good: Preventing Painting Imitation from Dif- Zhao, B. Y. 2024. Nightshade: Prompt-Specific Poison-
fusion Models via Adversarial Examples. arXiv preprint ing Attacks on Text-to-Image Generative Models. arXiv
arXiv:2302.04578. preprintarXiv:2310.13828.
Lin,S.;andYang,X.2024.DiffusionModelwithPerceptual Simonyan,K.;andZisserman,A.2014.VeryDeepConvolu-
Loss. arXivpreprintarXiv:2401.00110. tionalNetworksforLarge-ScaleImageRecognition. CoRR.Song,J.;Meng,C.;andErmon,S.2021.Denoisingdiffusion
implicit models. In International Conference on Learning
Representations(ICLR).
Song,Y.;Garg,S.;Shi,J.;andErmon,S.2020. Slicedscore
matching: A scalable approach to density and score esti-
mation. In Uncertainty in Artificial Intelligence, 574–584.
PMLR.
Tsaban, L.; and Passos, A. 2023. LEDITS: Real Im-
ageEditingwithDDPMInversionandSemanticGuidance.
arXiv:2307.00522.
Wang,Z.;Bovik,A.C.;Sheikh,H.R.;andSimoncelli,E.P.
2004. Image quality assessment: from error visibility to
structuralsimilarity. IEEETransactionsonImageProcess-
ing,13(4):600–612.
Wang,Z.;Zhao,L.;andXing,W.2023.Stylediffusion:Con-
trollabledisentangledstyletransferviadiffusionmodels. In
ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision(ICCV),7677–7689.
Xue,H.;Araujo,A.;Hu,B.;andChen,Y.2024a. Diffusion-
based adversarial sample generation for improved stealthi-
nessandcontrollability. InAdvancesinNeuralInformation
ProcessingSystems(NeurIPS),volume36.
Xue, H.; and Chen, Y. 2024. Pixel is a Barrier: Diffusion
Models Are More Adversarially Robust Than We Think.
arXivpreprintarXiv:2404.13320.
Xue, H.; Liang, C.; Wu, X.; and Chen, Y. 2024b. Toward
effectiveprotectionagainstdiffusionbasedmimicrythrough
scoredistillation. arXivpreprintarXiv:2311.12832.
Zhang,R.;Isola,P.;Efros,A.A.;Shechtman,E.;andWang,
O. 2018. The unreasonable effectiveness of deep features
as a perceptual metric. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR),586–595.
Zhang, Y.; Huang, N.; Tang, F.; Huang, H.; Ma, C.; Dong,
W.; and Xu, C. 2023. Inversion-based style transfer with
diffusionmodels. InProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition(CVPR),
10146–10156.Supplementary Material
figurations.i.utilizesonlysemanticloss;ii.utilizesseman-
ticlosswithourlatentoptimizationstrategy;iii.utilizesboth
A MoreImplementationDetails semanticloss,ourproposed andlatentoptimization.
fidelity
L
Theresultsshowthatour andlatentoptimizationcan
The feature extractor for calculating is VGG16 (Si- fidelity
fidelity L
L enhance the adversarial image quality of PGAscent. More-
monyan and Zisserman 2014) with IMAGENET1K-V1
over,comparingourproposedtwomethods,AtkPDM+gen-
checkpoint. We use the SDEdit with the forward step t =
eratesamorenaturaladversarialimagethanAtkPDMwhile
500forourmainstudyresultsasitbalancesfaithfulnessto
maintainingattackeffectiveness.
theoriginalimageandflexibilityforediting.Empirically,we
choosetorandomlysampletheforwardstept [0,500]to
∼
enhancetheoptimizationefficiency.Theaveragetimetoop-
timize300stepsforanimageonasingleNvidiaTeslaV100 B.4 DifferentForwardTime-stepSampling
isabout300seconds.Theestimatedaveragememoryusage
isabout24GB.Table5providesthedetailsofthestepsizes
WhenusingMonteCarlosamplingforoptimization,thefor-
thatweusetoattackdifferentmodels.
ward time step t∗ is sampled uniformly. We study the sce-
nariothatwhent∗isfixedforoptimization.AsshowninFig-
Models StepSize ure7,aprimaryresultshowsthatwhenattackingt∗ = 400
γ 1(L attack) γ 2(L fidelity) tot∗ = 500,theattackingeffectivenessisbetterthanother
google/ddpm-ema-church-256 100/255 40/255 time steps. In practice, we can not know user-specified t∗
google/ddpm-cat-256 100/255 5/255 foreditinginadvance;however,thissuggeststhatdiffusion
google/ddpm-ema-celebahq-256 50/255 35/255 models have a potential temporal vulnerability that can be
furtherexploitedtoincreaseefficiency.
Table5:Thestepsizesusedfordifferentmodelsduringop-
timization.
B.5 MoreQualitativeResults
B MoreExperimentalResults
WeprovidemorequalitativeresultsinFigure8toshowcase
B.1 AttackEffectivenessonLatentDiffusion thatourmethodcansignificantlychangeorcorruptthegen-
Models eratedresultswithlittlemodificationonadversarialimages.
We propose the feature representation attacking loss which In contrast, previous methods add obvious perturbation to
can be adapted to target any UNet-based diffusion models. adversarial images but still fail to change the edited results
Hence, it is applicable to attack LDM using our proposed toachievethesafeguardinggoal.
framework. We follow the evaluation settings of the previ-
ous works (Xue et al. 2024b) for fair comparisons. Quan-
titativeresultsareshowninTable6.Comparedtoprevious B.6 ExampleofLossCurves
LDM-specifiedmethods,ourmethodcouldachievecompa-
rableresults.Thisfindingreflectsthegeneralvulnerabilityin
Figure 9 shows an example of our loss trends among opti-
UNet-based diffusion models that can be exploited to craft
mizationsteps. hasdecreasingtrendastheoptimiza-
adversarialimagesagainsteitherPDMsorLDMs. Lattack
tionstepincreases. hasanincreasingtrendandcon-
fidelity
L
B.2 QualitativeDemonstrationofCorrupting
vergestosatisfytheconstraintoftheattackbudgetδ.
UNetFeatureduringSampling
Wequalitativelyshowanexampleofourattackeffectiveness
C Limitations
regardingUNetrepresentationdiscrepanciesinFigure5.We
compare a clean and an adversarial image using the same
denoising process. Then, we take the feature maps of the WhileourmethodcandeliveracceptableattacksonPDMs,
second-lastdecoderblocklayer,closetothefinalpredicted its visual quality is still not directly comparable to the re-
noise, to demonstrate their recognition of input image se- sults achieved on LDMs, indicating room for further im-
mantics.TheresultsinFigure5showthatfromt=500,the provement.MoregeneralizedPDMattacksshouldbefurther
featuremapsofeachpairstartwithasimilarstructure,then explored.
as the t decreases, the feature maps gradually have higher
discrepancies,suggestingourmethod,byattackingthemid-
dle representation of UNet, can effectively disrupt the re-
D SocietalImpacts
versedenoisingprocessandmisleadtocorruptedsamples.
B.3 QualitativeResultsofLossAblation
Our work will not raise potential concerns about diffusion
Figure6presentsqualitativeresultsoflossablationwherei., modelabuses.Ourworkisdedicatedtoaddressingtheseis-
ii.,andiii.indicateperformingPGAscentwithdifferentcon- suesbysafeguardingimagesfrombeinginfringed.AdversarialImageQuality AttackingEffectiveness
Methods
SSIM↑ PSNR↑ LPIPS↓ SSIM↓ PSNR↓ LPIPS↑ IA-Score↓
AdvDM(+) 0.85±0.03 30.42±0.15 0.23±0.06 0.19±0.05 28.00±0.16 0.71±0.04 0.49±0.06
Mist(+) 0.81±0.03 29.45±0.13 0.25±0.05 0.14±0.03 27.95±0.13 0.76±0.04 0.48±0.05
Diff-Protect(−) 0.79±0.03 29.92±0.15 0.24±0.06 0.15±0.03 28.00±0.14 0.71±0.04 0.48±0.05
Diff-Protect(+) 0.79±0.04 29.47±0.11 0.26±0.06 0.17±0.04 28.00±0.15 0.69±0.04 0.49±0.06
AtkPDM(Ours) 0.82±0.02 30.40±0.27 0.24±0.05 0.14±0.03 27.96±0.17 0.74±0.02 0.47±0.04
AtkPDM+(Ours) 0.61±0.07 29.17±0.32 0.20±0.02 0.27±0.06 28.07±0.18 0.66±0.05 0.51±0.06
AdvDM(+) 0.86±0.04 30.68±0.24 0.25±0.09 0.21±0.05 28.03±0.21 0.70±0.07 0.53±0.04
Mist(+) 0.81±0.04 29.63±0.22 0.27±0.08 0.14±0.04 27.96±0.17 0.77±0.06 0.52±0.04
Diff-Protect(−) 0.78±0.05 30.12±0.24 0.27±0.08 0.16±0.05 27.96±0.15 0.72±0.06 0.52±0.03
Diff-Protect(+) 0.77±0.06 29.56±0.16 0.28±0.09 0.17±0.05 27.98±0.16 0.71±0.06 0.53±0.04
AtkPDM(Ours) 0.84±0.02 30.79±0.49 0.25±0.07 0.18±0.04 28.00±0.19 0.72±0.05 0.52±0.03
AtkPDM+(Ours) 0.68±0.13 29.68±0.74 0.16±0.03 0.31±0.10 28.13±0.27 0.64±0.06 0.54±0.04
AdvDM(+) 0.83±0.02 30.81±0.22 0.32±0.06 0.26±0.05 28.07±0.28 0.74±0.05 0.47±0.07
Mist(+) 0.79±0.03 29.75±0.22 0.34±0.06 0.19±0.05 27.99±0.21 0.81±0.05 0.46±0.08
Diff-Protect(−) 0.74±0.04 30.34±0.13 0.33±0.06 0.21±0.05 28.03±0.21 0.76±0.06 0.45±0.07
Diff-Protect(+) 0.72±0.05 29.68±0.09 0.36±0.06 0.21±0.04 28.05±0.22 0.74±0.06 0.47±0.07
AtkPDM(Ours) 0.83±0.02 31.21±0.44 0.31±0.05 0.21±0.04 28.03±0.26 0.78±0.04 0.44±0.06
AtkPDM+(Ours) 0.82±0.05 30.05±0.51 0.14±0.03 0.41±0.08 28.24±0.39 0.63±0.07 0.52±0.07
Table6:QuantitativeresultsinattackingLDM.
E DetailsofOurProposedAlgorithm followingidentities:
(cid:90)
E.1 AtkPDMAlgorithmwithoutLatent
µ µadv 2dπ(x ,xadv)= µ µadv 2,
Optimization ∥ t − t ∥2 t t ∥ t − t ∥2
(cid:90)
x µ 2dπ(x ,xadv)=trace(Σ ),
∥ t − t ∥2 t t t
Algorithm2:AtkPDM (cid:90)
xadv µadv 2dπ(x ,xadv)=trace(Σadv),
1: Input:Imagetobeprotectedx,attackbudgetδ >0,andstep ∥ t − t ∥2 t t t
sizeγ 1,γ 2 >0 (cid:90)
32 :: I wn hit ii la el Lization n: ox tca odv nv← ergx en, tL dat otack ←∞ (x t −µ t)⊤(xa tdv −µa tdv)dπ(x t,xa tdv)
attack
4: Sampletimestep:t∼[0,T] =trace(cid:0)E[(x µ )(xadv µadv)⊤(cid:1) .
5: Samplenoise:ϵ 1,ϵ 2 ∼N(0,I) t − t t − t
6: Computeoriginalnoisysample:x t ←F(x,t,ϵ 1) Thus,the2-Wassersteindistancecanbeexpressedas:
7: Computeadversarialnoisysample:xadv ←F(xadv,t,ϵ )
8: UpdatexadvbyGradientDescent: t 2 W22( Nt, Ntadv)= ∥µ t −µa tdv ∥2 2
9:
wxa hd iv le←
L
fix dea litd yv (x− adγ v1 ,s xi )gn >(∇ δx da odvL attack(xa tdv,x t)) +trace(Σ t)+trace(Σa tdv) −2m J⪰a 0xtrace(C), (8)
10: xadv ←xadv−γ ∇ L (xadv,x)
2 xadv fidelity whereJ isthejointcovariancematrixof and adv,de-
11: endwhile Nt Nt
finedas:
12: endwhile
13: return xadv (cid:20) Σ C (cid:21)
J = t ,
C⊤ Σadv
t
E.2 2-WassersteinDistanceBetweenTwoNormal andC isthecovariancematrixbetween and adv:
Distribution
Nt Nt
C =E(cid:2) (x µ )(xadv µadv)⊤(cid:3) .
Consider the normal distributions := (µ ,Σ ) and t − t t − t
t t t
adv := (µadv,Σadv). Let Π( N , adv)N denote a joint BytheSchurcomplement,theproblemcanbeformulated
N dist tributionN ovt er thet product spN act eNRt n Rn. The 2- asasemi-definiteprogramming(SDP)problem:
Wassersteindistancebetween and
adv×
isdefinedas:
Nt Nt maximum trace(C),
(9)
(cid:90) subjectto Σ C⊤(Σadv)−1C 0.
2( , adv)= min x xadv 2dπ(x ,xadv). t − t ⪰
W2 Nt Nt π∈Π(Nt,N tadv) ∥ t − t ∥2 t t Theclosed-formsolutionforC derivedfromtheSDPis:
Usingpropertiesofthemeanandcovariance,wehavethe C =Σ t21 (Σ t1 2Σa tdvΣ t1 2)1 2Σ−
t
1 2.
hcruhC
taC
ecaFSampling Step t=500 t=400 t=300 t=200 t=100 t=0
Figure5:QualitativeexampleofcorruptingfeaturerepresentationsinUNet:asthedenoisingprocessproceeds,thesimilarity
ofthefeaturemapdecreases,suggestingtherepresentationiscorrupted.
Finally, the closed-form solution for the 2-Wasserstein ForEquation15,weutilizeGradientDescent,resultingin
distancebetweenthetwonormaldistributionsisgivenby: thefollowingupdate:
W22( Nt, Ntadv)= ∥µ t −µa tdv ∥2 2
(10)
yi+1 =yi+1 2 −γ˜ 2 ∇yi+1 2λF 2(x,yi+ 21 ) (17)
+trace(Σ t)+trace(Σa tdv) −2(Σ t1 2Σa tdvΣ t1 2)21 . =yi+ 21 −γ
2
∇yi+21F 2(x,yi+1 2). (18)
E.3 AlternatingOptimization NotethatthegradientofF canbederivedasfollows:
2
L obe jt ey cti= vex fuad nv c, tib oy nL caa ngr ba eng ee xpre rela sx sea dtio an s:(Liuetal.2023),the ∇yF 2(x,y)=I C′ ⊙∇xa tdv Lfidelity(x,y),
where I is indicator function with constraint
C′
F(x,y)=F (x,y)+λF (x,y), (11)
1 2 = y (x,y) ϵ .
fidelity
C { ∈M|L ≤ }
whereλ > 0istheLagrangemultiplierandF ,F arede-
1 2 Please note that after references, we also provide more
finedas
results presented in Figures 6, 7, 8, and 9, please refer to
F (x,y)= ( (x,t,ϵ ), (y,t,ϵ )), (12) subsequentpages.
1 attack 1 2
L F F
F (x,y)=max(ϵ (x,y),0). (13)
2 fidelity
−L
The optimization is carried out in an alternating manner
asfollows:
yi+1 2 =argmin(cid:0) F 1(x,y)+λF 2(x,yi)(cid:1) , (14)
y
(cid:16) (cid:17)
yi+1 =argmin F 1(x,yi+1 2)+λF 2(x,y) . (15)
y
TosolveEquation14,weemploytheFastGradientSign
Method (FGSM) (Goodfellow, Shlens, and Szegedy 2015).
Theupdateisgivenby:
yi+1/2 =yi γ sign(cid:0) F (x,yi)(cid:1) , (16)
1 yi 1
− ∇
)005=t
,x(tidEDS
erutaeF
reyaL
)005=t
,vdax(tidEDS
erutaeF
reyaLClean i. ii. iii. AtkPDM(Ours) AtkPDM+(Ours)
Figure6:Qualitativeexampleofdifferentlossconfigurations.i.onlysemanticloss;ii.semanticlossandlatentoptimization;
iii.semanticloss, andlatentoptimization.
fidelity
L
Clean t*=0 t*= 100 t*= 200 t*= 300 t*= 400
t*= 500 t*= 600 t*= 700 t*= 800 t*= 900 t*= 999
Figure7:Qualitativeresultsofoptimizingdifferentfixeddiffusionforwardsteps.
vdax
egamI
lairasrevdA
)005=t
,vdax(tidEDS
lairasrevdA
tidEDS
lairasrevdA
tidEDS
vdax
egamI
)005=t
,vdax(
vdax
egamI
)005=t
,vdax(Clean Image x SDEdit (x, t=500) Clean Image x SDEdit (x, t=500) Clean Image x SDEdit (x, t=500)
Adversarial Image xadv SDEdit (xadv, t=500) Adversarial Image xadv SDEdit (xadv, t=500) Adversarial Image xadv SDEdit (xadv, t=500)
Clean Image x SDEdit (x, t=500) Clean Image x SDEdit (x, t=500) Clean Image x SDEdit (x, t=500)
Adversarial Image xadv SDEdit (xadv, t=500) Adversarial Image xadv SDEdit (xadv, t=500) Adversarial Image xadv SDEdit (xadv, t=500)
Figure 8: Qualitative results compared to previous methods: our adversarial images can effectively corrupt the edited results
withoutsignificantfidelitydecrease.Thesamecolumnsharesthesamerandomseedforfaircomparison.
naelC
tnecsAGP
tcetorP-ffiD
)sruO(+MDPktA
naelC
tnecsAGP
tcetorP-ffiD
)sruO(+MDPktAOptimization Step
Optimization Step
Figure9:Losscurvesofour and againstoptimizationstep.
attack fidelity
L L