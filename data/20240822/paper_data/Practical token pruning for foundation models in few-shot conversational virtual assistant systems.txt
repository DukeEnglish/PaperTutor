Practical Token Pruning for Foundation Models in Few-shot
Conversational Virtual Assistant Systems
HaodeQi∗,ChengQian∗,JianNi,PratyushSingh,RezaFazeli,GengyuWang,ZhongzhengShu,EricWayne,JuergenBross
IBM
Abstract efficiencyandenduserexperienceofaVAsystem
(Qietal.,2020). Additionally,althoughalarge,bal-
In an enterprise Virtual Assistant (VA) sys-
anced,andcleantrainingsetisdesiredfortraining
tem, intent classification is the crucial com-
machinelearningalgorithmsingeneral,commer-
ponent that determines how a user input is
cial VA systems are expected to handle training
handled based on what the user wants. The
setswithlimitedsize. Sincegoodtrainingdataare
VA system is expected to be a cost-efficient
SaaSservicewith lowtrainingand inference costlytocreate,chatbotdesignersoftenstartwith
timewhileachievinghighaccuracyevenwith onlyafewexamplesperintentduringearlyphases
a small number of training samples. We pre- ofthebotdesignlifecycleorwhennewintentsare
trainatransformer-basedsentenceembedding added. This poses a challenge for the few-shot
modelwithacontrastivelearningobjectiveand
capabilitiesofintentclassificationalgorithmsde-
leverage the model’s embeddings as features
ployedincommercialVAsystems.
whentrainingintentclassificationmodels. Our
approach achieves the state-of-the-art results The transformer architecture (Vaswani et al.,
forfew-shotscenariosandperformsbetterthan 2017)hasshowngreatperformanceinawiderange
othercommercialsolutionsonpopularintent oftasksincludingintentclassification. Supervised
classificationbenchmarks. However,generat- SimCSE (Gao et al., 2021) shows that using pos-
ingfeaturesviaatransformer-basedmodelin-
itive sentences pairs from NLI (Gao et al., 2021)
creasestheinferencetime,especiallyforlonger
substantiallyimprovestheperformanceofsentence
user inputs, due to the quadratic runtime of
embedding model with transformer architecture.
thetransformer’sattentionmechanism. Ontop
We use a similar approach by gathering intents
ofmodeldistillation,weintroduceapractical
multi-taskadaptationapproachthatconfigures from different datasets across domains and treat
dynamic token pruning without the need for examples from the same intent as positive pairs.
task-specific training for intent classification. Thissimpleapproachdemonstratesstrongperfor-
We demonstrate that this approach improves manceinextremefew-shotintentclassificationsce-
theinferencespeedofpopularsentencetrans-
nario. However,thehighaccuracycomesatacost
formermodelswithoutaffectingmodelperfor-
ofsubstantialcomputeresourcesandmemoryca-
mance.
pacity, which can lead to a high latency or high
1 Introduction operatingexpensesforcommercialVAsystems. In
thispaper,weproposeapracticalmultitasktoken
Intentclassification,thetaskofidentifyingthepur- pruningproceduretosignificantlyreducethecom-
poseofauserinpututterance,isacrucialcompo- putationalcostfortransformerarchitectures,which
nentinmoderntask-orientedvirtualassistant(VA) does not require additional adaptation. We show
systems. The detected intent, combined with ex- theapproachcanworkseamlesslywithout-of-the-
tracted entities, is used to determine the proper boxsentencetransformermodelswithoutaffecting
dialognodesinapre-designeddialogtreetotrig- accuracy.
gercorrespondingresponsestotheenduser(Wang
et al., 2022). The training time and single query
2 RelatedWork
inferencetimeof theunderlyingintentclassifica-
tionalgorithmbothplayanimportantroleforthe Thetransformerarchitecture(Vaswanietal.,2017)
hasbecomeubiquitousinvariousdomainsandap-
∗Equal contributions from the corresponding authors:
{haode.qi,cheng.qian}@ibm.com. plications due to its state-of-the-art performance.
4202
guA
12
]LC.sc[
1v99711.8042:viXraDifferenttypesoftransformer-basedmodelshave regularizationterminadditiontotheoriginalloss
beenappliedtointentclassificationinthecontext function. However,enterpriseVAsystemsrequire
ofVAsystems. IntentBert(Zhangetal.,2021)uses minimal adaption for new tasks. This technique
therepresentationofthe[CLS]tokenfromanen- requiresanewthresholdtobelearnedforeachnew
coderonlyBERT-basedmodel(Devlinetal.,2019) task,renderingitunsuitableforenterpriseVAsys-
totrainaclassifierhead. SentenceBERTmodels temswherethetypicaltrainingtimeislessthan10
(Reimers and Gurevych, 2019a) which are based minutes.
on a Siamese and triplet architecture are used as The second type of token pruning techniques
featureextractorsforintentclassification. Recently, can be applied directly to a trained model (post
Parikhetal.(2023)performzero-shotintentclas- trainingtokenpruning). Wangetal.(2021)propose
sificationbyincludingadescriptionofeachintent cascade token pruning. For each layer, there is
classinthepromptforalargelanguagemodel. The a configurable ratio, based on which tokens will
state-of-the-art performance of these large trans- be pruned on the fly without additional training.
formermodelscomeswithachallengeintermsof However, configuring a parameter for each layer
complexity,memoryconsumption,andscalability canbedifficultfordeepmodelsoverawidevariety
inrealworldproductionsystems. oftasks. Additionally,whilethetechniquereduces
Many approaches have been proposed to im- computationandmemoryconsumption,theyalso
provetheefficiencyoftransformer-basedmodels pointoutthattokenpruningcouldincreasememory
suchastokenpruning(Goyaletal.2020,Kimand access.
Cho2021,Yeetal.2021,Kimetal.2022,Wang
et al. 2021), quantization (Jacob et al. 2017, Lin 3 SentenceEmbeddingModel
etal.2023,Xiaoetal.2023,Dettmersetal.2022), PretrainingandOptimizationforIntent
distillation(Hintonetal.2015,Mobahietal.2020, Classification
Allen-ZhuandLi2023,Sanhetal.2020),sparse
Inthissection,webrieflydescribethecontrastive-
andlow-rankapproximation(Lietal.2023,Tahaei
learningtrainingandoptimizationprocedureand
et al.2021, Chen etal. 2021, Wang et al. 2020a),
give an overview of the practical token pruning
andmoreefficientimplementationoftheexactat-
technique that has been deployed on our VA sys-
tentionalgorithm(Daoetal.2022).
teminproduction. Eventhoughthistechniqueis
Token pruning aims to discard a subset of to-
applied to intent classification, it is task-agnostic
kenswhicheffectivelydecreasestheinputsequence
andcanbeappliedtoanytransformer-basedmodel.
length to achieve speedup. There are two main
types of token pruning techniques. The first one
3.1 PretrainingandDistillation
introduces an additional procedure during pre-
training or finetuning for adapting token pruning We pretrain our sentence embedding model with
toaspecifictaskordataset(token-pruning-awared Multiple Negative Loss (Henderson et al., 2017)
training). For example, Goyal et al. (2020) intro- usingproprietaryintentclassificationdatasets. We
ducesoft-extractlayerswithtrainable-parameters createpositiveexamplepairs(a i,p i)bysampling
intotransformerencoderblocksduringfinetuning fromthesameintent. Weconstructacollectionof
for a model to learn which tokens to drop. Kim examplepairsfromintentsacrossdatasetsfromdif-
andCho(2021)introduceLengthDropandLayer- ferentdomains. Afterpretraining,astudentmodel
Droptothetransformerarchitectures,whichcannot withfewerlayersisdistilledfromtheteachermodel
be directly applied to an off-the-shelf pretrained using MSE Loss (Reimers and Gurevych, 2020).
transformer-basedmodelwithoutadditionaltrain- Thetokenpruningprocedureisappliedtothestu-
ingorfinetuning. Moreover,bothoftheaforemen- dentmodeldeployedinproduction. Weintroduce
tionedtechniquespruneallinputsentencestothe thistokenpruningprocedurefortransformermod-
same length. In our production environment, the elsinthefollowingsubsections.
lengthofuserutterancescanvaryfromasingleto-
3.2 AttentionMechanism
kento1000+characters,makingthesetechniques
unpracticalforproductionintentclassificationsys- The notation mostly follows the transformer pa-
tems. Toovercomethis,Kimetal.(2022)propose per(Vaswanietal.,2017). Transformerlayersare
a trainable threshold per layer to perform token thebuildingblocksoftransformer-basedarchitec-
pruning. Theseparametersaretrainedviaaddinga tures. In the context of NLP, a transformer layertakesabatchoftokenizedandfeaturizedsentences padded tokens. It’s worth noting that the compu-
X ∈ RB×n×d model as input and outputs a tensor tation of attention score based token importance
of the same size, where B is the batch size, n is shouldnotinvolveanypaddedtokens,sothatthe
thesequencelength,andd istheembedding importancescoresfortokensinthesamesentence
model
dimension. are consistent regardless of the max length of all
Within each transformer layer, there are two sentences within the same batch. With this def-
sub-layers,amulti-headself-attentionblock,and inition of token importance, for each sentence,
a simple token-wise feed-forward network with onlythetokenswiththehighesttopq importance
two fully connected layers. Let H be the num- scores will be kept. For a tokenized sentence
berofheadsinthemulti-headself-attentionblock. of length n, whose matrix representation is of
Each head is parameterized by three matrices shape 1×n×d, the corresponding output from
WQ ∈ Rd model×d k, WK ∈ Rd model×d k, and a token-pruned self-attention block would be of
i i
WV ∈ Rd model×d k,whereiistheindexforahead, shape 1×n′×d, where n′ ≤ n and 1 being the
i
andd ×H = d . Aninputtoaself-attention placeholderforbatchsizedimension.
k model
head is linearly transformed into three matrices
3.3.2 Protectionagainstexcessivepruning
queryQ,keyK,andvalueV bythethreeparame-
termatrices,respectively. Attentionscoresandthe Throughexperiments,wenoticeperformingtoken
finalrepresentationarecalculatedas pruningatasingleearlytransformerlayerusually
sufficestoachieveagoodtrade-offbetweeninfer-
QKT
A(Q,K)=softmax( √ ) (1) encespeed-upandaccuracy. Bypruningatanearly
d
k
layerl,allsubsequentlayerswillbenefitintermsof
Attention(Q,K,V)=A(Q,K)V (2) inferencespeed-up,becauseduringforward-pass,
the pruned tokens will not reach the subsequent
Thecomplexityofaself-attentionblockisO(n2· layersafterthelayerwheretheyhavebeenpruned.
d+d2·n),wheredistherepresentationdimension. By limiting the number of layers we apply token
pruning to, we also limit the additional overhead
3.3 ProposedMethod
from performing token pruning: calculation and
Token pruning aims to reduce the complexity of sortingofaveragedattentionscoresonnon-padded
a self-attention block by discarding a subset of tokens,andremovaloftheprunedtokens.
tokens, resulting in a quadratic reduction of the Based on traffic patterns in our VA system, it
complexityofself-attention(andthereforeallsub- is common to see short sentences with less than
sequent layers). The proposed method is config- tentokens. Forsuchshortsentences,thequadratic
uredwiththreeparameters: s denotestheminimum complexityoftransformerblocksislessofacon-
lengthofasequence(measuredbythenumberof cern. Inaddition,topreventimportantinformation
tokens)towhichpruningwillbeapplied;q denotes frombeingdiscardedduringtokenpruning,weaim
thequantilepercentwhichdefinesathresholdonto- toconfigures,theminimumnumberoftokensin
kenimportancetodiscardtokens,andl denotesthe a sentence for it to be token pruned, to balance
indexofthelayerwheretokenpruningisapplied. betweenspeedupandpreservinginformation.
3.3.1 Attentionscorebasedtokenimportance 3.3.3 Practicalmultitaskadaptation
Followingexistingworks,wemeasuretheimpor- EnterpriseVAsystemsareexpectedtoservealarge
tance of a token based on attention scores. For- number of customers from a wide variety of do-
mally,givenanattentionheadh,theattentionscore
mains. Therefore, any algorithms applied are ex-
Ah
i,j
measurestheamountofattentionatokenihas
pected to generalize well on datasets of different
ontokenj. Atatransformerlayer,wefirstaverage characteristics, such as number of training exam-
allH attentionscorematricesfromtheH attention ples,averagelengthoftrainingexamplesandthe
heads. Theaveragedattentionscorematrixisde- domains of these examples. When applying dy-
noted as A. Secondly, for each token j, the final namic token pruning technique to enterprise VA
importancemeasurementiscalculatedas systems,itisdesirabletodevisewithaconfigura-
(cid:88) A (3) tion that works for a wide variety of tasks across
i,j
i differentdomainsoutofthebox.
where the summation is taken over all the non- We propose a simple and robust post-trainingadaptation approach that requires no further tun- as feature extractors in inference mode then we
ing of the token pruning configuration for new trainlinearclassifierheadswiththeextractedfea-
tasks. We perform hyperparameter search on a tures. DFT++ (Zhang et al., 2023) directly fine-
hold-out set of intent classification datasets for tunes a model with augmented unlabeled dataset
the optimal token pruning configuration for our (ContextAugmentation)andthenperformsSequen-
production VA system. To illustrate this process, tialSelf-Distillation(SSD).ForDFT++,wefine-
we introduce some additional notations: a trans- tune a BERT model for each dataset. Due to the
formermodelE isusedasafeatureextractor,on code of DFT++ not being released yet as of the
topofwhichwetrainaclassifierheadf foreach dateofwritingthispaper,weareonlyabletoimple-
dataset, and evaluation metric for intent classifi- mentaworkingversionofDFT++(w/SSD)(but
cation such as accuracy and F1 is denoted as M. notthefullDFT++(w/CA,SSD)),followingthe
Forahold-outcollectionofnintentclassification originalpaperonabesteffortbasis. Forothermod-
tasks T = {(t ,d ),(t ,d ),...,(t ,d )} where els, wedownloadthereleasedacademiccodefor
1 1 2 2 n n
(t ,d )isthetraininganddevelopmentsetpairfrom ourexperiments.
i i
task , we evaluate combinations of (s,q,l) itera-
i
4.1.1 Datasetsandmetric
tivelyonalltasksandfindtheonethatoptimizes
Experimentsareconductedwiththreeintentclas-
1 (cid:88)n sification datasets: BANKING77 (Zhang et al.,
s∗,q∗,l∗ = argmax M(f(E(t |s,q,l)),d ).
s,q,ln i i 2022b), CLINC150 (Larson et al., 2019), and
i=1
HWU64(Liuetal.,2019). FollowingWangetal.
(4)
(2022),werandomlysample1-shot,2-shot,3-shot,
The resulting set of optimal configuration
and 5-shot variants of the training sets; and ran-
s∗,q∗,l∗ willbeappliedtoallunseentasks.
domly sample more difficult versions of test sets
basedonjaccarddistanceandtf-idf. Weuseaccu-
4 Experiments
racyasthemetric.
In this section, we conduct three sets of experi-
4.1.2 Results
ments. We first compare our production system
1withtheproposedoptimizationsagainstbothaca- Outofthetotalof36settingsreportedinTable1,
demic and commercial intent classification algo- oursystemperformsthebestin24ofthem.
rithms. Thenweextendbeyondourproductandap-
plyourtokenpruningproceduretoanopen-sourced 4.2 ExperimentII:Comparingagainst
transformer model and verify its general efficacy commercialsolutions
andapplicability.
We compare our system with other commercial
solutionsonintentclassificationperformance.
4.1 ExperimentI:Comparingagainst
academicintentclassificationalgorithms
4.2.1 Datasetandmetrics
Wecompareourdeployedproductionintentclas-
We conduct benchmarking on the 3 commonly
sification algorithm against the following recent
usedintentclassificationdatasetsHWU64,BANK-
academic intent classification baselines. IsoIn-
ING77,andCLINC150. Foreachdataset,weuse
tentBert (Zhang et al., 2022a) further pretrains
the 10-shot version of the training set following
aBERTmodel(Devlinetal.,2019)onpartofthe
Mehri et al. (2020). For evaluation, we measure
CLINC150(Larsonetal.,2019)datasetwithaddi-
weightedaveragesofprecision,recall,andF1fol-
tionalregularizationterms. SBERT-NLI,SBERT-
lowingthesetupofarecentCognigyblog2.
para,andSimCSE-NLI(Maetal.,2022)utilize
sentenceencodersforintentclassification. Inour 4.2.2 ResultsandAnalysis
experiment,theseaforementionedmodelsareused
Weaddthemetricsfromoursystemtothecompar-
1AccessiblethroughUIandAPI.Inthispaper,wereferto isonintheCognigyblogandputthebest(highest)
twoversionsofintentclassificationalgorithmsavailableinour metrics in bold in Table 3. Our system is consis-
product:15-Apr-2023(Latest)and20-Dec-2022(Previous).
tentlythebestacrossthe3datasetsandmetrics.
Latestusestransformer-basedmodelswiththetokenpruning
describedinthispaper,thusbeingreferredtoasIBMWat-
sonx(LM)inthefollowingsections. Formoredetailabout 2https://www.cognigy.com/blog/benchmarking-nlu-
modelversioning,refertoourdocumentation. engines-comparing-market-leadersAccuracy
Trainingsetvariant Dataset Algorithm
Fulltestset Difficulttestset(jaccard) Difficulttestset(tfidf)
DFT++(w/SSD) 28.60% 25.22% 18.65%
IsoIntentBert 40.97% 35.97% 32.21%
SBERT-NLI 43.21% 37.19% 35.32%
BANKING77
SBERT-para 46.55% 40.57% 36.49%
SimCSE-NLI 42.62% 36.28% 34.55%
IBMWatsonx(LM) 47.72% 42.73% 34.68%
DFT++(w/SSD) 48.87% 43.81% 34.43%
SBERT-NLI 58.71% 55.20% 45.07%
1-shot
CLINC150 SBERT-para 59.66% 52.13% 47.73%
SimCSE-NLI 57.75% 51.47% 45.73%
IBMWatsonx(LM) 67.40% 60.80% 54.00%
DFT++(w/SSD) 40.48% 40.55% 36.39%
IsoIntentBert 56.34% 54.87% 50.81%
SBERT-NLI 46.56% 44.35% 41.13%
HWU64
SBERT-para 44.14% 44.35% 40.97%
SimCSE-NLI 44.79% 43.71% 40.81%
IBMWatsonx(LM) 52.78% 50.00% 47.74%
DFT++(w/SSD) 53.05% 43.25% 36.00%
IsoIntentBert 55.20% 46.39% 39.53%
SBERT-NLI 60.42% 49.80% 44.68%
BANKING77
SBERT-para 62.33% 53.45% 46.62%
SimCSE-NLI 59.18% 51.24% 42.99%
IBMWatsonx(LM) 63.02% 51.95% 46.88%
DFT++(w/SSD) 71.92% 61.84% 52.24%
SBERT-NLI 71.75% 65.60% 56.00%
2-shot
CLINC150 SBERT-para 73.17% 67.07% 59.73%
SimCSE-NLI 71.26% 65.87% 57.87%
IBMWatsonx(LM) 78.13% 66.27% 61.60%
DFT++(w/SSD) 62.29% 58.87% 53.58%
IsoIntentBert 66.78% 65.26% 59.26%
SBERT-NLI 64.49% 60.65% 55.65%
HWU64
SBERT-para 63.29% 58.55% 53.39%
SimCSE-NLI 63.66% 60.32% 56.13%
IBMWatsonx(LM) 68.68% 66.29% 61.13%
DFT++(w/SSD) 59.82% 49.45% 45.06%
IsoIntentBert 59.35% 50.18% 45.19%
SBERT-NLI 61.81% 50.33% 45.58%
BANKING77
SBERT-para 64.18% 53.06% 46.49%
SimCSE-NLI 59.44% 48.89% 43.77%
IBMWatsonx(LM) 65.29% 54.29% 50.00%
DFT++(w/SSD) 78.70% 70.21% 61.12%
SBERT-NLI 77.42% 70.80% 64.67%
3-shot
CLINC150 SBERT-para 78.24% 72.93% 67.60%
SimCSE-NLI 77.39% 70.27% 64.80%
IBMWatsonx(LM) 82.91% 74.93% 68.53%
DFT++(w/SSD) 66.97% 67.00% 59.39%
IsoIntentBert 71.00% 70.45% 63.74%
SBERT-NLI 68.68% 68.23% 61.77%
HWU64
SBERT-para 69.24% 67.74% 60.81%
SimCSE-NLI 66.72% 65.16% 60.32%
IBMWatsonx(LM) 70.50% 69.35% 63.71%
DFT++(w/SSD) 75.08% 64.23% 55.53%
IsoIntentBert 69.19% 59.12% 51.56%
SBERT-NLI 72.92% 62.16% 56.36%
BANKING77
SBERT-para 73.47% 63.59% 54.29%
SimCSE-NLI 70.75% 60.99% 54.03%
IBMWatsonx(LM) 76.88% 67.66% 59.35%
DFT++(w/SSD) 85.16% 76.16% 67.60%
SBERT-NLI 82.02% 76.67% 68.93%
5-shot
CLINC150 SBERT-para 83.44% 76.93% 70.27%
SimCSE-NLI 81.28% 76.80% 68.67%
IBMWatsonx(LM) 88.53% 80.93% 71.73%
DFT++(w/SSD) 77.62% 76.00% 71.68%
IsoIntentBert 74.52% 72.29% 67.29%
SBERT-NLI 73.69% 74.52% 66.77%
HWU64
SBERT-para 74.44% 74.52% 67.58%
SimCSE-NLI 74.35% 74.35% 67.58%
IBMWatsonx(LM) 76.39% 75.32% 70.16%
Table1: Accuracycomparisonbetweenoursolution(Ours)vsacademicintentclassifiersonfewshotscenario. Best
resultsareinbold.DATASET Trainingsetvariant TokenPruned Accuracy(Logisticregressionhead) Time(togenerateembeddingsfortrainingset) Changeinaccuracy Speedup
3-shot N 79.33% 1.30
5-shot N 84.73% 3.02
CLINC150
3-shot Y 80.91% 1.01 1.58% 23%
5-shot Y 87.47% 2.00 2.73% 34%
3-shot N 67.47% 0.49
5-shot N 76.77% 0.84
HWU64
3-shot Y 70.26% 0.39 2.79% 20%
5-shot Y 76.30% 0.64 -0.46% 24%
3-shot N 68.57% 1.81
5-shot N 77.47% 4.00
BANKING77
3-shot Y 71.62% 1.24 3.05% 31%
5-shot Y 79.35% 2.67 1.88% 33%
Table2: ComparingMiniLM-L12vstokenprunedMiniLM-L12onintentclassificationaccuracyandtime(in
seconds)takentogeneratesentenceembeddingsfortrainingset.
DATASET SolutionProvider precision recall F1 MiniLM model. We set the search space of q to
Cognigy 0.84 0.79 0.8 discrete values ranging from 0.6 to 0.8, l from
MicrosoftCLU 0.81 0.8 0.79 {1,2,3}(onlyoneoftheearlytransformerlayers
HWU64 GoogleDialogflow 0.74 0.63 0.66
will be pruned), and s from 10 to 20 during post-
IBMWatsonx(Previous) 0.84 0.75 0.78
IBMWatsonx(LM) 0.84 0.82 0.82 trainingadaptation. Theresultingoptimalconfig-
Cognigy 0.81 0.8 0.8 urations,q,lis15,0.8,1. Wemeasurebothaccu-
MicrosoftCLU 0.74 0.7 0.7 racyandthetimetakentoproducesentenceembed-
BANKING77 GoogleDialogflow 0.76 0.74 0.74
dings,includingtokenizing,forward-passing,mean
IBMWatsonx(Previous) 0.82 0.81 0.81
IBMWatsonx(LM) 0.86 0.85 0.85 pooling, and normalizing. The time is averaged
Cognigy 0.91 0.87 0.88 across 7 runs using 4 cores of Intel(R) Xeon(R)
MicrosoftCLU 0.86 0.84 0.84 Gold6258RCPU@2.70GHz.
CLINC150 GoogleDialogflow 0.8 0.76 0.77
IBMWatsonx(Previous) 0.89 0.87 0.87
4.3.1 ResultandAnalysis
IBMWatsonx(LM) 0.92 0.91 0.91
FromtheresultsinTable2,wenoticethatbyapply-
Table3: Weightedaverageprecision,recall,andF1on ingourtokenpruningtechniquetoMiniLM-L12,
10-shotdatasetsbydifferentcommercialVAsolutions.
weachieveaspeed-upfrom20%to34%without
Note:ResultsforallmethodsexceptIBMWatsonx(LM)
muchlossinaccuracy. Insomesettings,weeven
areobtainedfromtheCognigyblog.
observeanincreasedaccuracywithtokenpruning,
whichmightbeattributedtotheremovalofunim-
4.3 ExperimentIII:Applyingourtoken portanttokens. Thisexperimentisanexamplethat
pruningtechniquetoopen-sourced theproposedtechniquecanbeextendedbeyondour
transformermodelsforintent productandappliedtoopen-sourcedoff-the-shelf
classification transformermodelsforintentclassificationtasks.
Weapplyourtokenpruningtechniquetoacommon 5 Limitations
sentenceembeddingmodelMiniLM-L12(Reimers
andGurevych(2019b),Wangetal.(2020b))3 and Thestudyissubjecttoafewlimitations. (1)Our
study the trade-off between inference speed and benchmark only concerns few-shot cases that do
intentclassificationaccuracy. Weusetheoff-the- notfullyreflectthechallengesofaproductionVA
shelfpre-trainedMiniLM-L12(withandwithout system. Inproduction,thelackoftrainingsamples
tokenpruning)asafeatureextractorwithoutfine- in some intents is also a symptom of data imbal-
tuning,andusetheextractedsentenceembeddings ance. (2) We only study the inference speedup
totrainlogisticclassifiers. Theconfigurationforto- usingenvironmentssimilartoourproduction. The
kenpruningtheMiniLMmodelisobtainedbycon- inferencespeedupofthetokenpruningtechnique
ductingtheofflinemultitaskadaptationdescribed couldvarydependingontheimplementationand
inSubsection3.3.3onaninternallycuratedhold- hardwarearchitecture.
outsetofintentclassificationtasksusingasmaller
3Weights are downloaded from
https://huggingface.co/sentence-transformers/all-MiniLM-
L12-v2References onNaturalLanguageProcessing(EMNLP-IJCNLP),
pages1311–1316,HongKong,China.Association
Zeyuan Allen-Zhu and Yuanzhi Li. 2023. Towards
forComputationalLinguistics.
understandingensemble,knowledgedistillationand
self-distillationindeeplearning.
Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang,
PengchengHe,WeizhuChen,andTuoZhao.2023.
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri
Losparse: Structuredcompressionoflargelanguage
Rudra,andChristopherRé.2021. Scatterbrain: Uni-
modelsbasedonlow-rankandsparseapproximation.
fyingsparseandlow-rankattentionapproximation.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang,
andChristopherRé.2022. Flashattention: Fastand Xingyu Dang, Chuang Gan, and Song Han. 2023.
memory-efficientexactattentionwithio-awareness. Awq: Activation-awareweightquantizationforllm
compressionandacceleration.
TimDettmers,MikeLewis,YounesBelkada,andLuke
Zettlemoyer.2022. Llm.int8(): 8-bitmatrixmultipli- XingkunLiu,ArashEshghi,PawelSwietojanski,and
cationfortransformersatscale. VerenaRieser.2019. Benchmarkingnaturallanguage
understanding services for building conversational
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and agents.
KristinaToutanova.2019. Bert: Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstand- Tingting Ma, Qianhui Wu, Zhiwei Yu, Tiejun Zhao,
ing. and Chin-Yew Lin. 2022. On the effectiveness of
sentenceencodingforintentdetectionmeta-learning.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
In2022AnnualConferenceoftheNorthAmerican
SimCSE: Simple contrastive learning of sentence
Chapter of the Association for Computational Lin-
embeddings. InEmpiricalMethodsinNaturalLan-
guistics (NAACL 2022). Association for Computa-
guageProcessing(EMNLP).
tionalLinguistics.
Saurabh Goyal, Anamitra R. Choudhury, Saurabh M.
S. Mehri, M. Eric, and D. Hakkani-Tur. 2020.
Raje,VenkatesanT.Chakaravarthy,YogishSabhar-
Dialoglue: A natural language understanding
wal, and Ashish Verma. 2020. Power-bert: Accel-
benchmark for task-oriented dialogue. ArXiv,
erating bert inference via progressive word-vector
abs/2009.13570.
elimination.
Matthew L.Henderson, RamiAl-Rfou, BrianStrope, Hossein Mobahi, Mehrdad Farajtabar, and Peter L.
Yun-HsuanSung,LászlóLukács,RuiqiGuo,Sanjiv Bartlett.2020. Self-distillationamplifiesregulariza-
Kumar,BalintMiklos,andRayKurzweil.2017. Effi- tioninhilbertspace.
cientnaturallanguageresponsesuggestionforsmart
reply. CoRR,abs/1705.00652. Soham Parikh, Quaizar Vohra, Prashil Tumbade, and
Mitul Tiwari. 2023. Exploring zero and few-shot
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. techniquesforintentclassification.
Distillingtheknowledgeinaneuralnetwork.
Haode Qi, Lin Pan, Atin Sood, Abhishek Shah,
Benoit Jacob, Skirmantas Kligys, Bo Chen, Men-
Ladislav Kunc, Mo Yu, and Saloni Potdar. 2020.
glong Zhu, Matthew Tang, Andrew G. Howard,
Benchmarkingcommercialintentdetectionservices
Hartwig Adam, and Dmitry Kalenichenko. 2017.
with practice-driven evaluations. arXiv preprint
Quantization and training of neural networks for
arXiv:2012.03929.
efficient integer-arithmetic-only inference. CoRR,
abs/1712.05877.
Nils Reimers and Iryna Gurevych. 2019a. Sentence-
bert: Sentence embeddings using siamese bert-
Gyuwan Kim and Kyunghyun Cho. 2021. Length-
networks. CoRR,abs/1908.10084.
adaptivetransformer: Trainoncewithlengthdrop,
useanytimewithsearch.
Nils Reimers and Iryna Gurevych. 2019b. Sentence-
SehoonKim,ShengShen,DavidThorsley,AmirGho- bert: Sentence embeddings using siamese bert-
lami, Woosuk Kwon, Joseph Hassoun, and Kurt networks. InProceedingsofthe2019Conferenceon
Keutzer.2022. Learnedtokenpruningfortransform- EmpiricalMethodsinNaturalLanguageProcessing.
ers. AssociationforComputationalLinguistics.
Stefan Larson, Anish Mahendran, Joseph J. Peper, Nils Reimers and Iryna Gurevych. 2020. Mak-
Christopher Clarke, Andrew Lee, Parker Hill, ing monolingual sentence embeddings multilin-
JonathanK.Kummerfeld,KevinLeach,MichaelA. gual using knowledge distillation. arXiv preprint
Laurenzano,LingjiaTang,andJasonMars.2019. An arXiv:2004.09813.
evaluationdatasetforintentclassificationandout-of-
scopeprediction. InProceedingsofthe2019Confer- Victor Sanh, Lysandre Debut, Julien Chaumond, and
enceonEmpiricalMethodsinNaturalLanguagePro- ThomasWolf.2020. Distilbert,adistilledversionof
cessingandthe9thInternationalJointConference bert: smaller,faster,cheaperandlighter.Marzieh S. Tahaei, Ella Charlaix, Vahid Partovi Nia,
AliGhodsi,andMehdiRezagholizadeh.2021. Kro-
neckerbert: Learningkroneckerdecompositionfor
pre-trainedlanguagemodelsviaknowledgedistilla-
tion.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. InAdvancesinNeuralInformationPro-
cessingSystems,volume30.CurranAssociates,Inc.
Gengyu Wang, Cheng Qian, Lin Pan, Haode Qi,
LadislavKunc,andSaloniPotdar.2022. Benchmark-
inglanguage-agnosticintentclassificationforvirtual
assistantplatforms. InProceedingsoftheWorkshop
on Multilingual Information Access (MIA), pages
69–76,Seattle,USA.AssociationforComputational
Linguistics.
HanruiWang,ZhekaiZhang,andSongHan.2021. Spat-
ten: Efficientsparseattentionarchitecturewithcas-
cadetokenandheadpruning.
SinongWang,BelindaZ.Li,MadianKhabsa,HanFang,
andHaoMa.2020a. Linformer: Self-attentionwith
linearcomplexity.
WenhuiWang,FuruWei,LiDong,HangboBao,Nan
Yang, and Ming Zhou. 2020b. Minilm: Deep self-
attentiondistillationfortask-agnosticcompression
ofpre-trainedtransformers.
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu,
JulienDemouth,andSongHan.2023. Smoothquant:
Accurateandefficientpost-trainingquantizationfor
largelanguagemodels.
Deming Ye, Yankai Lin, Yufei Huang, and Maosong
Sun. 2021. TR-BERT: dynamic token reduc-
tion for accelerating BERT inference. CoRR,
abs/2105.11618.
HaodeZhang,HaowenLiang,LimingZhan,Xiao-Ming
Wu, andAlbertY.S.Lam.2023. Revisitfew-shot
intentclassificationwithplms: Directfine-tuningvs.
continualpre-training.
Haode Zhang, Haowen Liang, Yuwei Zhang, Liming
Zhan,Xiao-MingWu,XiaoleiLu,andAlbertY.S.
Lam.2022a. Fine-tuningpre-trainedlanguagemod-
els for few-shot intent detection: Supervised pre-
trainingandisotropization.
Haode Zhang, Yuwei Zhang, Li-Ming Zhan, Jiaxin
Chen, GuangyuanShi, Xiao-MingWu, andAlbert
Y. S. Lam. 2021. Effectiveness of pre-training for
few-shotintentclassification.
Jianguo Zhang, Kazuma Hashimoto, Yao Wan, Zhi-
weiLiu, YeLiu, CaimingXiong, andPhilipS.Yu.
2022b. Arepretrainedtransformersrobustinintent
classification? amissingingredientinevaluationof
out-of-scopeintentdetection.