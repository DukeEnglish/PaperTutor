DreamFactory: Pioneering Multi-Scene Long Video Generation with a
Multi-Agent Framework
ZhifeiXie♠ (cid:66) DanielTang♣ DingweiTan♢
JacquesKlein♣ TegawendéF.Bissyandé♣ SaadEzzini♡♣
♠TsingHuaUniversity ♣UniversityofLuxembourg ♢BeijingInstituteofTechnologyfilicos.thu@gmail.com xunzhu.tang@uni.lu tdw03edu@outlook.com
jacques.klein@uni.lu tegawende.bissyande@uni.lu s.ezzini@lancaster.ac.uk
Abstract way),(Guetal.,2023)]. However,craftingpracti-
cal,multi-scenevideosthatmeetreal-worldneeds
Currentvideogenerationmodelsexcelatcre-
remainsaformidablechallenge. Thisincludesen-
ating short, realistic clips, but struggle with
suringconsistencyincharacterportrayal,stylistic
longer, multi-scene videos. We introduce
coherence,andbackgroundacrossdifferentscenes,
DreamFactory,anLLM-basedframeworkthat
proficiently maneuvering professional linguistic
tackles this challenge. DreamFactory lever-
agesmulti-agentcollaborationprinciplesand tools,andmanagingcomplexproductionstepsbe-
aKeyFramesIterationDesignMethodtoen- yondmerelyassemblingbriefvideoclipsgenerated
sureconsistencyandstyleacrosslongvideos. bycurrenttechnologies. Therefore,thereisanur-
ItutilizesChainofThought(COT)toaddress gentneedwithinthefieldofvideogenerationfora
uncertaintiesinherentinlargelanguagemod-
modelcapableofdirectlyproducinglong-duration,
els. DreamFactory generates long, stylisti-
high-quality videos with high consistency, thus
cally coherent, and complex videos. Evalu-
enabling AI-generated video to gain widespread
ating these long-form videos presents a chal-
acceptance and become a premier producer of
lenge. WeproposenovelmetricssuchasCross-
Scene Face Distance Score and Cross-Scene contentforhumancultureandentertainment.
StyleConsistencyScore. Tofurtherresearch At the current stage, substantial advancements
in this area, we contribute the Multi-Scene inthevideodomainutilizediffusion-basedgenera-
Videos Dataset containing over 150 human-
tivemodels, achievingexcellentvisualoutcomes
rated videos. DreamFactory1 paves the way
[ (Blattmann et al., 2023), (runway), (openai)].
forutilizingmulti-agentsystemsinvideogen-
Nonetheless,duetotheintrinsiccharacteristicsof
eration.
diffusionmodels,thevideosproducedaretypically
1 Introduction short segments, usually limited to four seconds.
Forgeneratinglongervideos,modelslikeLSTM
Video, integrating both visual and auditory
andGANsareemployed(Guptaetal.,2022),how-
modalities—the most direct sensory pathways
ever, these models struggle to meet the demands
throughwhichhumansperceiveandcomprehend
forhighimagequalityandarerestrictedtosynthe-
the world—effectively conveys information with
sizingvideos oflower resolution. Thesestate-of-
compellingpersuasivenessandinfluence,progres-
the-art approaches attempt to use a single model
sivelybecomingapowerfultoolandmediumfor
to address all sub-challenges of video generation
communication[(TangandIsaacs,1992),(Owen
end-to-end,encompassingattractivescriptwriting,
and Wildman, 1992), (Armes, 2006), (Harris,
characterdefinition,andartisticshotdesign. How-
2016),(Merktetal.,2011)]. Traditionalvideopro-
ever,thesetasksaretypicallycollaborativeandnot
ductionisanarduousandtime-intensiveprocess,
thesoleresponsibilityofasinglemodel.
particularly for capturing elusive real-life scenes.
In addressing complex tasks and challenges in
Owingtotherapidadvancementsindeeplearning,
problem-solvingandcoding,researchershavebe-
AI-driven video generation techniques now facil-
gunutilizingLLMmulti-agentcollaborativetech-
itate the acquisition of high-quality images and
niques,modeledonhumancooperativebehaviors,
videosegmentswithease[(pika),(Blattmannetal.,
andhaveobservednumerouspotentagents. With
2023), (openai), (Blattmann et al., 2023), (run-
theintegrationoflargemodelsthatincludevisual
capabilities,multi-agentcollaborativetechnologies
1Wewillmakeourframeworkanddatasetspublicafter
paperacceptance. have now developed an AI workflow capable of
4202
guA
12
]IA.sc[
1v88711.8042:viXrathatourvideossurpasstheaveragequalityofthose
producedmanually. Someexamplesgeneratedby
theframeworkareshowninFigure1.
2 Relatedwork
LLM-basedAgents. Inrecentyears,thecapabili-
tiesoflargelanguagemodelshavebeencontinually
enhanced, exemplified by advancements such as
Figure1: KeyframedataproducedbyDreamFactory. GPT-4 (openai), Claude-3 (Claude), and LLama-
Itcanbeseenthatthecharacter’sfacialfeatures,visual 2(meta),amongothers. Subsequently,exploration
style,andevenclothingareconsistent.
intoenhancingtheabilitiesoftheselargelanguage
models has emerged, introducing methodologies
such as CoT (Wei et al., 2022), ToT (Yao et al.,
tacklingchallengesintheimageandvideodomain.
2024),ReACT(Yaoetal.,2022),Reflexion(Shinn
Inthispaper,weintroducemulti-agentcollabora- etal.,2024),andvariousotherapproachestofacili-
tivetechniquestothedomainofvideogeneration, tateiterativeoutputandcorrectioncycles. Within
developing a multi-scene long video generation this context, the notion of Multi-agents has sur-
frameworknamedDreamFactory,whichsimulates faced, with early research efforts including no-
anAIvirtualfilmproductionteam. Agentsbased table works such as Camel (Li et al., 2024), Voy-
on LLMs assume roles akin to directors, art di- ager (Wang et al., 2023), MetaGPT (Hong et al.,
rectors,screenwriters,andartists,collaboratively 2023), ChatDev (Qian et al., 2023), and Auto-
engaginginscriptwriting,storyboardcreation,char- GPT(Yangetal.,2023). Recently,powerfulMulti-
acterdesign,keyframedevelopment,andvideosyn- agentsframeworkshaveproliferatedacrossdiverse
thesis. We define the concept of keyframe in the domains, with prominent instances in fields such
longvideogenerationfieldtomaintainconsistency ascoding,includingnotablecontributionssuchas
acrossvideosegments. InDreamFactory,wedraw CodeAgent (Tang et al., 2024), CodeAct (Wang
onthesuccessfulCoTconceptfromthemulti-agent etal.,2024),andCodepori(Rasheedetal.,2024).
reasoning process to devise a keyframe iteration UtilitariantoolssuchasToolformer(Schicketal.,
methodspecifictovideo. Toaddressthedriftphe- 2024), HuggingGPT (Shen et al., 2024), Tool-
nomenoninlargelanguagemodels,aMonitorrole llm (Qin et al., 2023), and WebGPT (Nakano
is introduced to ensure consistency between dif- et al., 2021) have also been employed. Other
ferent frames. DreamFactory also establishes an noteworthyendeavorsencompassprojectslikeWe-
integrated image vector database to maintain the bArena(Zhouetal.,2023),RET-LLM(Modarressi
stabilityofthecreativeprocess. Basedonthealgo- etal.,2023),andOpenAGI(Geetal.,2024),each
rithmsdiscussed,DreamFactorycanautomatethe contributingtotheadvancementandproliferation
production of multi-scene videos of unrestricted ofMulti-agentsparadigms.
lengthwithconsistentimagecontinuity. Video synthesis. In the field of video genera-
Toevaluateourframework,weemployedstate- tion,traditionalmethodsprimarilyutilizeGenera-
of-the-art video generation models as tools, mea- tive Adversarial Networks (GANs) for video cre-
suringvideogenerationperformanceontheUTF- ation,asdemonstratedintheworksofTimBrooks
101andHMDB51datasets. Furthermore,giventhe et al. (Brooks et al., 2022) and the foundational
noveltyofourtask,withfewpriorventuresintothis contributions of Ian Goodfellow et al. (Goodfel-
area, we compared long videos generated by our low et al., 2014) However, in recent years, a sig-
frameworkagainstthoseproducedusingtheorig- nificantshifthasoccurredtowardsleveragingthe
inal tools. We found that our model significantly potentcapabilitiesofdiffusionprocesses,withpio-
outperformed the existing native models regard- neeringresearchconductedbyJaschaetal.(Esser
ingevaluationmechanisms. Finally,wecollected et al., 2023), and Song et al. (Song et al., 2020).
AI-generated short videos currently available on The forefront of this evolution is marked by the
theinternetandassessedthemusingmechanisms development of Latent Video Diffusion Models.
suchastheInceptionScore,alongsideevaluations This approach is exemplified in the seminal ef-
conductedbyhumanjudges. Ourfindingsindicate fortsofAndreasBlattmannetal.(Blattmannetal.,2023),Guetal.(Guetal.,2023),Guoetal.(Guo InFigure3,panels(a)and(b)featureschematic
etal.,2023),Heetal.(Heetal.,2022)andWang illustrationsofacharacterbeingdefinedandiniti-
et al. (Wang et al., 2023). Currently, the most ating role play. The complete architecture of the
formidableadvancementsinthisareaarefourmain entirecompanyisfullyintroducedinFigure8. For
models: Pika(pika),StableVideo(Blattmannetal., each, we defined a role card, which contains: 1)
2023),Runway(runway),andSora(openai). The role name is put on the left-upper corner of
each card; 2) The phases of the role involved are
3 DreamFactory putontheright-uppercornerofeachcard;3)On
eachrolecard,weshowtherole-involvedconversa-
Our DreamFactory framework utilizes multiple
tionandcollaborativeroles;4)Weshowtheinter-
largelanguagemodels(LLMs)toformasimulated
mediateoutputoftheroleontheright-handsideof
animationcompany,takingonrolessuchasCEO,
thecard;and5)Finally,weputthecorresponding
Director,andCreator. Givenastory,theycollabo-
filesorcontentoutofconversationsonthebottom
rate and create a video through social interaction
ofthecard.
andcooperation. ThisframeworkallowsLLMsto
simulatetherealworldbyusingsmallvideogen- 3.2 DreamFactoryFrameworkpipeline
eration models as tools to accomplish a massive
Inthissection,weintroducethespecificpipeline
task. Thissectiondetailsthemethodologybehind
of DreamFactory. Figure 2 illustrates the main
ourinnovativeDreamFactoryframework. Wefirst
phasesandindicateswhichagentsengageincon-
describethedefinedrolecardsinSection3.1and
versations. Beforedelvingintoourentirepipeline,
discussthepipelineinSection3.2. Finally,wewill
it’s essential to first outline its fundamental com-
discussthekeyframeiterationdesignmethod.
ponents: phasesandconversations. Asdepictedin
Figure 3 (c, a phase represents a complete stage
3.1 RoleDefinition
that takes some textual or pictorial content as in-
In the architecture of our simulation animation put. Agents,composedofGPT,engageinroleplay,
company DreamFactory, the following roles are discussion,andcollaborationforprocessing,ulti-
included: CEO, movie director, film producer, mately yielding some output. A conversation is
Screenwriter,Filmmaker,andReviewer.Withinthe a basic unit of a phase, with typically more than
DreamFactoryframework,theyfunctionsimilarly one roundof conversationencompassed within a
to their real-world counterparts, taking on roles phase. After a fixed number of conversations, a
such as determining the movie’s style, writing phaseisapproachingitsconclusion,atwhichpoint
scripts,anddrawing. DreamFactory will save certain interim conclu-
Thedefinitionpromptsfortheirrolesprimarily sionsgeneratedwithinthisphasethatwewishto
consist of three main parts: Job, Task and Re- retain. For instance, in the Phase style decision,
quirements. For instance, the definition prompt the final conclusion will be preserved. Further-
foramovie’screatorwouldincludethefollowing more, during subsequent phases, DreamFactory
sentences: (a) You are the Movie Art Director. willprovidethenecessaryprecedents,suchasin-
Now,wearebothworkingatDreamFactory,... (b) vokingpreviousstylesandscriptswhendesigning
Yourjobistogenerateapictureaccordingtothe keyframeslateron.
scenery given by the director...and (c) you must Recently,largelanguagemodelswerefoundto
obeythereal-worldrules,likecolorunchanged... havetheircapabilitieslimitedbyfinitereasoning
For tasks such as plot discussions, we also limit abilities, akin to how overly complex situations
theirdiscussionstonotexceedaspecificnumber inreallifecanleadtocarelessnessandconfusion.
ofrounds(dependingontheuser’ssettingsandthe Therefore,themainideaofthisframework,inthe
company’ssizedefinition). Wehaveincludedthe videodomain,istodecomposethecreationoflong
followingprompttoensurethis: "Yougivemeyour videosintospecificstages,allowingspecificlarge
thoughtandstory, andweshouldbrainstormand modelstoplaydesignatedrolesandleveragetheir
critique each other’s idea. After discussing more powerful capabilities in analyzing specific prob-
than5ideas,anyofusmustactivelyterminatethe lems. Like a real-life film production company,
discussionbypickingupthebeststyleandreplying DreamFactoryadoptsaclassicworkflow,starting
withasingleword<INFO>,followedbyourlatest with scriptwriting followed by drawing. Overall,
styledecision,e.g.,cartoonstyle." the framework encompasses six primary stages:User DreamFactory Framework
1. User requirement input
Original Input Requirement Analysis 1. Documenting
I want a movie about Requirement doc
a soldier defeating a Requirement Planning
Video-making plan
dragon.
w
Job plan Framework preparation 2. Database setting o
l
f
User Character database k
Preparation Script generation Background database r o
2. Few-shot script input(OP) w
3. Local Memory
Script Scenes design s
I want a video like this. t
Script document n
e
Scenes Shots design Scenes document g a
upload
Shots document -
i
User t l
Shots Key-frames Generation 4. Key-frames Generation u M
3. Few-shot script input(OP)
This is the video I want! Keyframes Video Generation
5. Movie Generation
download
Output
video
Sora/Stability
Figure2: AnoverviewoftheDreamFactoryframework. Theframeworktransposestheentirefilmmakingprocess
intoAI,forminganAI-drivenvideoproductionteam.
tion,asdiscussedinsection3.1. Wecanrefertothe
notationinGuohaoLi’sarticle[1]todefinethecol-
laborationprocessofagentswithinDreamFactory.
Werefertotheassistantsystemprompt/messageby
PaandthatoftheuserbyPu. Thesystemmessages
are passed to the agents before the conversations
start. Let F1 and F2 denote two large-scale au-
toregressive language models. When the system
Figure 3: The Figure demonstrates how GPT begins
messageispassedtothosemodelsrespectively,we
role-playingasadirectorandcommencescommunica- cangetA ← FPA , U ← FPU whicharereferred
tionwithotherGPTsasadirectorwould. 1 2
toastheassistantanduseragentsrespectively. In
continuation,weassumethatthetextprovidedby
Task Definition, Style Decision, Story Prompt- theuser(instructor)ateachinstanceisdenotedas
ing,ScriptDesign,andKey-frameDesign. The It, and the response given by the assistant is de-
specificmethodforthefinalstage,keyframeiter- notedasAt. TheOutputattimesteptalternating
ative design, will be introduced in the following conversationsbetweenthetwocanberepresented
section;itisusedtomaintaintheconsistencyand as: O t = ((I 1,A 1),(I 2,A 2),...,(I t,A t)).
continuityofimagesgeneratedatvariousstages. In Following the five critical phases mentioned
thefirstfourphases,ourrolesareconversational. above, five significant outputs will be achieved.
In each phase, every agent shares a "phase Intheprompt,eachphase’soutputO isrequired
t
prompt" that includes the following key points: tofollow<INFO>forsummarization,whichalso
our roles, our tasks, the conclusions we aim to allows us to systematically obtain and preserve,
draw,theformofourdiscussion,andsomeother forming the Local memory information of the
requirements. Followingthis,eachagentisfurther DreamFactory framework. This is also one of
informedbyitsuniquepromptaboutitsroledefini- theprimarypurposesofproposingthisframework,consecutive,consistentimages,iteratingandgen-
erating forward with each step. Figure 4 demon-
stratestheprocessofeachiteration.
KeyframeIterationDesignMethodleverages
theinferentialcapabilitiesoflargelanguagemod-
elstotransformlong-termmemoryintoiterations
ofshort-termmemorytoensureconsistency. The
first frame of the image is the beginning of the
Figure4: Anoverviewofthekeyframeiterativedesign.
entirevideoandestablishesessentialinformation
such as the style, painting technique, characters,
maintainingtheconsistencyofcriticalinformation. and background for the entire long video. There-
Finally, after generating the tasks, styles, stories, fore,werefertothefirstframeastheBase. Atthe
scripts, and keyframe images, a long video with beginning,wewillgenerateapainterP,adirector
consistentstyleisobtained. D and a monitor M, represented by P ← FPP ,
1
D ← FPD ,M ← FPM ,thesemodelsplayedby
3.3 KeyframeIterationDesign 2 3
visuallargelanguagemodels,willengageinacycli-
During the generation of long videos, the most
calprocessofgenerationanddiscussionuntilthey
challengingproblemtoaddressisthatavideocom-
produceacrucialframe,whichisthefirstkeyframe,
prisesalongsequenceofimagecollections. There-
referred to as the Base Frame. At this point, the
fore,whengenerating,themodelneedstomaintain
Monitor D, composed of a visual large language
along-term,consistentmemorytoensurethateach
modelaswell,willconductathoroughanalysisto
frameproducedbythemodelcoherentlycomposes
extractinformation,detaileddescriptionoffeatures
aconsistentvideo. Thistypeofmemoryincludes
suchasstyle,background,andcharactertraitsthat
two kinds: short-term memory knowledge and
shouldbepreservedforanextendedperiod. This
long-termmemorysystem.
results in the Base Description, note as B . S1
D
short-term memory knowledge is embedded
represents the script for the first frame. We have
withinvideosofafixedscene. Betweenadjacent (cid:0) (cid:1)
O = Gen p ,d ,S ,whereB ← M(O ).
t t t 1 D t
frames,theanimationineachframeshouldbecon-
In subsequent generations, when iterating the
nected,thecharactersshouldbeunified,andthere
keyframeformomentt,wewillusethepreviously
shouldbenosignificantchangesincolor,style,etc.
inputS asthedescriptionofthescene. Tomain-
As of now, the latest video models perform very t
tain continuity in the context of adjacent scenes,
wellintermsofshort-termmemory. Nonetheless,
we will employ the nurtured method to generate
wehavestilladdedaMonitortosupervisewhether
the description for the moment t − 1, which we
our video model is performing sufficiently well.
also refer to as the contextual environment de-
AsillustratedinFigure4,thereisareviewprocess
noted as C − 1. At the same time, to maintain
after the generation of each frame. Therefore, to t
long-distance memory, B will also serve as an
maintain short-term consistency, the supervisory D
input. By referencing the basic features of the
mechanismweintroducedhasaddressedthisissue.
previous frame and the Base features, it can en-
long-term memory system, however, pose a
sure that the necessary information is essentially
challenge that troubles most current models and
graspedinthenextiteration,enablingthedrawing
representsthemostpressingissueinvideogenera-
ofcontinuouskeyframeswiththesamestyle,con-
tiontoday. Particularly,withinaGPT-basedfully
sistent characters, and uniform background. We
automatedmulti-agentframework,theinherentran- (cid:0) (cid:1)
haveO = Gen p ,d ,S ,C .
domness and drift phenomena of large language t t t t t−1
Upon the previous generation of keyframes,
modelsmakethisproblemdifficulttotackle. Long-
we can obtain the contextual environment and
termmemoryimpliesthatacrossscenetransitions,
proceed with the next round of generation. We
the model should be able to maintain the consis-
have C = M(O ), p = P(S ,C ) , d =
tency of the drawing style, character continuity, t t t+1 t t t+1
D(S ,C ,p ). Ultimately,weachievethegener-
andnarrativeflow. Toupholdlong-termmemory, t t t+1
ationofthekeyframesforthemomentt+1.
wehaveintroducedtheKeyframeIterationDesign
method,whichtransformslong-termmemoryinto Inpracticalapplication,controllingthedetailsof
short-term memory by guiding the generation of charactersprovestobethemostchallengingaspect.Therefore, underourcarefullymodifiedprompts, erate1400images,fromwhichwecalculatedFID,
with increased emphasis on parts that performed IS, and CLIP Score. As for FVD and KVD, we
poorlyinmultipleexperiments,theKeyframeIter- selected100samplesfromourmulti-scenevideo
ationMethodcannowgenerateaveryconsistent dataset and manually extracted 10 keyframes for
andpracticallyvaluableseriesofimages. each one, Which can be used to generate multi-
scalevideos.
4 Experiments DatainTable1indicatesthatthequalityofim-
ages generated using scripts is on average more
4.1 TraditionalVideoQualityEvaluati1on
refinedthanthoseproducedusingeverydayprompt
Evaluati1onMetrics-Tovalidatethecontinuity words. This may be attributable to the extent to
ofthekeyframesandthequalityofthevideospro- which GPT acts as a prompt, and contemporary
ducedbytheframework,weembeddedvarioustool models are generally adept at processing longer
models(suchasRunway,Diffusion,GPT)within prompts. However, within the DreamFactory
thearchitecturetoassessthequalityofvideosgen- framework, the application of keyframe iterative
erated by different tools. In our experiments, we design, in conjunction with storyboard creation,
principallyemployedthefollowingevaluationmet- detaileddescriptionsofcharacters,settings,light-
rics: (1) Fréchet Inception Distance (FID) score: ing, and style determination, has led to a marked
measuresthesimilaritybetweengeneratedimages improvement in the quality of image generation.
andrealimages. (2)InceptionScore(IS):gauges A similar enhancement is also evident in videos
thequalityanddiversityofgeneratedimages. (3) whichisshowninTable2.
CLIPScore: evaluatesthetextualdescriptionaccu-
racy of generated images. (4) Fréchet Video Dis- ModelsComposition FID IS CLIPScore
Dalle-e3(Regular) 9.30 133.46 26.69
tance(FVD)score: extensionoftheFIDforvideos,
Diffusion(Regular) 9.15 158.23 26.58
comparingthefeaturesdistributionofrealvideos Midjourney(Regular) 11.23 163.20 25.91
GPT3.5-Script+Dalle-e3 9.78 153.43 29.58
versussynthesizedonesbasedonFréchetdistance
GPT3.5-Script+Diffusion 8.63 168.90 30.57
and(5)KernelVideoDistance(KVD):utilizesker- GPT3.5-Script+Midjourney 10.81 174.45 29.32
GPT4-Script+Dalle-e3 8.53 159.12 29.84
nelfunctiontocomparethefeaturesdistributionof
GPT4-Script+Diffusion 8.32 169.97 30.73
realvideosversussynthesizedones. GPT4-Script+Midjourney 10.26 178.14 29.75
DreamFactory(GPT4)+Dalle-e3 6.57 160.94 30.76
Our dataset, during the Regular phase, com-
DreamFactory(GPT4)+Diffusion 7.03 169.71 30.92
prised conventional prompts consisting of 70 DreamFactory(GPT4)+Midjourney 7.15 178 30.39
keywords and brief sentences randomly selected
byexperimentalpersonnelfromtheCOCOdataset. Table 1: The statistical analysis of Text2Image task.
Thiswasutilizedtoevaluatethegeneratedimage All models can generate higher-quality images after
quality of the fundamental tool models and the prompts augmentation, but the quality of the images
generatedbyourframeworkstandsout.
degree of alignment between the images and the
text. For the Script phase, scripts pertaining to
70 randomly extracted tasks from our provided
ModelsComposition FVD KVD
dataset were employed during the script-filling
Runway(Regular) 1879 125
stage. Thisguidedthemodelgenerationbasedon
StableVideo(Regular) 3560 182
therelevantplottoassessthefunctionofthe"An- DreamFactory+Runway 732 62
imation Department" within the DreamFactory DreamFactory+StableVideo 1376 113
framework. TheDreamFactorylabeldenotesthe
keyframeimagesproducedbytheframeworkthat Table2: ThestatisticalanalysisofImage2Videotask.
correspondstotheScript. The improvement of our framework for generating
multi-scenelongvideosisremarkable.
OutputQualityStatistics-Theimagesgener-
atedusingmodelssuchasDALL·EandDiffusion
are of high quality and have reached the state-of-
4.2 Multi-sceneVideosEvaluationScores
the-artlevelinvariousindices. Toquantitativelyan-
alyzethequalityofthegeneratedimages,weinput Cross-Scene Face Distance Score - In the gen-
theimagescorrespondingtotheoriginalprompts erationofsequentialvideos,addressingcharacter
intoGPTtogettheGPT-Scriptandthenusedorig- consistencyisparamount. Discrepanciesintheap-
inalpromptsortheGPT-Scriptaspromptstogen- pearance of characters can lead not only to poorvisualperceptionbutalsototheaudience’sinabil-
itytounderstandtheplotandcontent. Maintaining
characterconsistencyensuresthecoherenceofthe
storyline revolving around the characters and en-
hancesthevisualappealofthevideo. Especially,in
thedomainoflong-durationvideos,avideoistypi-
callycomposedofmultiplescenes. Thisrepresents
an unprecedented area of research, where there
is a pressing need for robust evaluation metrics
to assess the consistency of characters appearing
acrosscomplex,multi-scenevideos. Againstthis
backdrop,weexperimentallyintroducetheconcept
of the Cross-Scene Face Distance Score(CSFD
Score),aimedatvalidatingtheissueofcharacter
Figure5: Schematicdiagramandpseudocodeforthe
facialfeatureconsistencyacrossdifferentscenes. calculationofCross-SceneFaceDistanceScore.
Inthecomputationalprocess,eachkeyframecor-
responds to a face, and using the dlib library, the Algorithm1CalculateCSFDScore
position of the face can be extracted. The face-
1: total ← 0
recognitionlibrarycanbeusedtocalculatethesim-
2: count ← n*(n-1)/2
ilarityscore. Forthefacialsegmentofeachframe,
3: fori ← 1tondo
wecancomputeitssimilaritywithallsubsequent
4: forj ← i+1tondo
framesandthentaketheaverage. Bythismethod,
5: similarity ← CFS(F i,F j)
wecanaccuratelydeterminewhetherthefacesin
6: total ← total+similarity
the video are consistent. The relevant schematic
7: endfor
diagramandthepseudocodeforthecalculationare
8: endfor
providedinAlgorithm1.
9: averageScore ← total/count
Cross-SceneStyleConsistencyScore-Inthe
10: returnaverageScore
production of long videos, maintaining stylis-
tic consistency is equally important. A consis-
tent style makes the video appear as a cohesive Models CSFDScore CSSCScore av-ClipScore
GPT4-Script+Dalle-e3 0.77 0.85 0.29
whole. Basedonthisconcept,wehaveintroduced GPT4-Script+Diffusion 0.75 0.83 0.28
GPT4-Script+Midjourney 0.68 0.66 0.26
the Cross-Scene Style Consistency Score(CSSC DreamFactory(GPT4)+Dalle-e3 0.89 0.97 0.31
Score). However,tomyknowledge,therecurrently
isn’tamaturemethodtorapidlydeterminethestyle Table3: Thestatisticalanalysisofcross-scenescoreon
ofavideo,soatthisstage,wewillrelyontheassis- differentmodels.
tanceoflargelanguage-visualmodels. Essentially,
wedividethevideointoseveralcategories,which
include: anime, illustration, origami, oil paint- thefourthsceneisclassifiedasanimestyle. Conse-
ing,realism,cyberpunk,andinkwash. quently,themaximumnumberofdistinctstylesis
The calculation method for the Cross-Scene three,resultinginacross-scenestyleconsistency
Style Consistency Score is as follows: For each score of 75%. The other relevant schematic dia-
keyframe,adividerplayedbyaGPT-4Visusedto gram and the pseudocode for the calculation are
determinetheclassification. Onceallsceneshave providedinAlgorithm2.
beenclearlydividedintocategories,theproportion AverageKey-FramesCLIPScore-Inthegen-
ofthemostnumerouscategorytothetotalnumber eration of long videos with multiple scenes, it is
of key frames is calculated. Figure 6 presents a crucial to assess the alignment of each scene’s
partialoutputwheretheinputis"anelderlyperson keyframeswiththecorrespondingtext. Theyhave
making a traditional Chinese lantern in real life". incorporatedasignificantamountofadditionalin-
Scene4depictsananimatedlanterncreatedusing formationtoensureconsistency,whichcouldlikely
Dalle,withGPT-4Vservingasthediscriminator. It leadtodeviationsfromthetextduringgeneration.
isobservablethatamongthefourscenes,thefirst Thismayresultintheoverallvideonotadheringto
threearecategorizedunderarealisticstyle,while the script. Therefore, in this section, we proposeandthenarrative,aswellasimagery.
In our Cross-Scene facial distance scoring ex-
periment,weemployedthefacelocationsmethod
from the face-recognition library to locate 68 fa-
ciallandmarks,therebyfocusingtheportraitpho-
tographsonthefacialarea. Duringtheimageen-
codingphase,weutilizedtheViTmodelfromthe
openai-cliprepositorytoinputthefacialregionand
computethevectorrepresentations. Subsequently,
a vector dot product operation was performed to
determine the final facial distance score. Owing
totheinherentsimilarityamongthefacialimages,
allthescoreswerepredominantlyabove0.5. The
specificreferencefacialmatch-scorepairsareex-
hibited in Figure 7. In the analysis of both the
CSSCscoreandtheaverageCLIPscore,thesame
setofseventyrandomsampleswasutilizedasdata.
Figure 6: Schematic diagram and pseudocode for the TheCSSCScoreemployedGPT-4Versionasthe
calculationofCross-SceneStyleConsistencyScore. stylisticanalyst.
Algorithm2CalculateCSSCScore
1: n ← numberofkeyframes
2: categories ←
arrayinitializedto0ofsizenumberofcategories
3: fori ← 1tondo
4: category ← JUDGE(F i)
5: categories[category] ←
categories[category]+1
6: endfor
7: maxCount ← max(categories)
8: crossSceneStyleScore ← maxCount ×100 Figure 7: The distance between different faces when
n
9: returncrossSceneStyleScore usingopenai-clipastheencoder.
5 Conclusion
theAverageKey-FramesCLIPScoretoensurethe
consistency of key frame scenes with the script. WeintroduceDreamFactory: amulti-agent-based
The calculation method is straightforward: com- frameworkforgeneratinglongvideoswithmulti-
putetheCLIPscoreforeachkeyframeagainstthe plescenes. DreamFactoryincorporatestheideaof
scenegeneratedduringscenepromptingandtake multi-agentsintothefieldofvideogeneration,pro-
theaverage. ducingconsistent,continuous,andengaginglong
Results-Intable3,ourdataselectioncomprised videos. DreamFactoryintroducesakeyframeiter-
seventy character-centric entries from the Multi- ationmethodtoensurealignmentofstyle,charac-
SceneVideosDataset,producedbyDreamFactory ters,andscenesacrossdifferentframesandcanbe
+ GPT-4 + DALL-E 3. The baseline utilizes the builtontopofanyimageorvideogenerationtool.
DALL-E3modelwithscriptinputsfromthisseg- Furthermore,DreamFactoryproposesnewmetrics
ment. Furthermore,evaluationswereconductedon tovalidateitscapabilitiesbymeasuringthequality
theaforementioned(1)cross-scenefacialdistance, ofgeneratedcontentthroughcross-scenefaceand
(2)cross-scenestylescores,and(3)averageCLIP style consistency, as well as text-to-visual align-
Score. Thesemetricswereusedtoassessthecon- ment. Onthetestset,theDreamFactoryframework
sistency of facial features within our framework, canachievehighlyconsistentsequentialstorygen-
the consistency of scene attributes, and the align- eration,markingagroundbreakingdevelopment.
mentbetweenpromptsgeneratedbyourframework6 Limitations Lorenz, Yam Levi, Zion English, Vikram Voleti,
AdamLetts,etal.2023. Stablevideodiffusion: Scal-
Inthispaper,wepresentamulti-agentvideogener- ing latent video diffusion models to large datasets.
ationframeworkcapableofproducingvideoswith arXivpreprintarXiv:2311.15127.
highconsistencyacrossmultiplescenesandplot-
AndreasBlattmann,RobinRombach,HuanLing,Tim
lines. However, we still face several limitations.
Dockhorn, Seung Wook Kim, Sanja Fidler, and
Firstly,ourcurrentrelianceonpromptstocontrol Karsten Kreis. 2023b. Align your latents: High-
agents means that the agents are not capable of resolutionvideosynthesiswithlatentdiffusionmod-
els. In Proceedings of the IEEE/CVF Conference
highly creative tasks, such as devising plots with
onComputerVisionandPatternRecognition,pages
artisticmerit. Suchtasksrequiretheaccumulation
22563–22575.
of specific datasets for model fine-tuning. Sec-
ondly,theeditingofallvideosegmentsiscentered TimBrooks,JanneHellsten,MiikaAittala,Ting-Chun
Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu,
aroundsynthesizedspeechcontent,whichresults
Alexei Efros, and Tero Karras. 2022. Generating
inafinalproductthatmayappearasamereassem-
longvideosofdynamicscenes. AdvancesinNeural
blyofclips. Thisnecessitatestheintroductionofa InformationProcessingSystems,35:31769–31781.
uniqueframeworkdesigntoenhancethefluidityof
Claude. Claude3.
thevideos. Lastly,videogenerationstillinvolves
substantialresourceconsumption. PatrickEsser,JohnathanChiu,ParmidaAtighehchian,
JonathanGranskog,andAnastasisGermanidis.2023.
7 EthicsStatements Structure and content-guided video synthesis with
diffusionmodels. InProceedingsoftheIEEE/CVF
The development and deployment of DreamFac- InternationalConferenceonComputerVision,pages
7346–7356.
tory,amulti-agentframeworkforlongvideogener-
ation,raiseseveralethicalconsiderationsthatmust
Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan,
beaddressed. Thepotentialforthemisuseofgen- Shuyuan Xu, Zelong Li, Yongfeng Zhang, et al.
eratedvideos,suchasthecreationofdeepfakesor 2024. Openagi: When llm meets domain ex-
perts. Advances in Neural Information Processing
thepropagationofmisinformation,isasignificant
Systems,36.
concern. To mitigate these risks, we commit to
implementingrobustsafeguards,includingwater- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
markinggeneratedcontentandcollaboratingwith BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville, and Yoshua Bengio. 2014. Generative
fact-checkingorganizations. Additionally,wewill
adversarial nets. Advances in neural information
ensuretransparencyinourresearchandmakeour
processingsystems,27.
methods and datasets publicly available, subject
to ethical use guidelines. We also recognize the JiaxiGu,ShicongWang,HaoyuZhao,TianyiLu,Xing
Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-
importanceofdiversityandinclusioninthetrain-
Gang Jiang, and Hang Xu. 2023. Reuse and dif-
ingdatatopreventbiasesinthegeneratedcontent.
fuse: Iterativedenoisingfortext-to-videogeneration.
Finally,wewillengagewiththebroadercommu- arXivpreprintarXiv:2309.03549.
nity to establish ethical standards for the use of
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang,
AI-generatedvideocontent,promotingresponsible
YuQiao,DahuaLin,andBoDai.2023. Animated-
innovationanduseofthistechnology.
iff: Animateyourpersonalizedtext-to-imagediffu-
sionmodelswithoutspecifictuning. arXivpreprint
arXiv:2307.04725.
References
SonamGupta,ArtiKeshari,andSukhenduDas.2022.
RoyArmes.2006. Onvideo. Routledge. Rv-gan:Recurrentganforunconditionalvideogener-
ation. InProceedingsoftheIEEE/CVFConference
Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Her- onComputerVisionandPatternRecognition,pages
rmann,RoniPaiss,ShiranZada,ArielEphrat,Jun- 2024–2033.
hwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li,
MichaelRubinstein,TomerMichaeli,OliverWang, AnneMHarris.2016. Videoasmethod. OxfordUni-
Deqing Sun, Tali Dekel, and Inbar Mosseri. 2024. versityPress.
Lumiere: A space-time diffusion model for video
generation. Preprint,arXiv:2401.12945. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan,
andQifengChen.2022. Latentvideodiffusionmod-
Andreas Blattmann, Tim Dockhorn, Sumith Ku- els for high-fidelity long video generation. arXiv
lal, Daniel Mendelevitch, Maciej Kilian, Dominik preprintarXiv:2211.13221.Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng YongliangShen,KaitaoSong,XuTan,DongshengLi,
Cheng,JinlinWang,CeyaoZhang,ZiliWang,Steven WeimingLu,andYuetingZhuang.2024. Hugging-
KaShingYau,ZijuanLin,LiyangZhou,etal.2023. gpt: Solving ai tasks with chatgpt and its friends
Metagpt:Metaprogrammingformulti-agentcollabo- in hugging face. Advances in Neural Information
rativeframework. arXivpreprintarXiv:2308.00352. ProcessingSystems,36.
Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Noah Shinn, Federico Cassano, Ashwin Gopinath,
Khizbullin, and Bernard Ghanem. 2024. Camel: Karthik Narasimhan, and Shunyu Yao. 2024. Re-
Communicative agents for" mind" exploration of flexion: Language agents with verbal reinforce-
large language model society. Advances in Neural ment learning. Advances in Neural Information
InformationProcessingSystems,36. ProcessingSystems,36.
MartinMerkt,SonjaWeigand,AnkeHeier,andStephan YangSong,JaschaSohl-Dickstein,DiederikPKingma,
Schwan. 2011. Learning with videos vs. learning Abhishek Kumar, Stefano Ermon, and Ben Poole.
withprint: Theroleofinteractivefeatures. Learning 2020. Score-based generative modeling through
andInstruction,21(6):687–704. stochastic differential equations. arXiv preprint
arXiv:2011.13456.
meta. Llama-2.
DanielTang,ZhenghanChen,KisubKim,YeweiSong,
Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Haoye Tian, Saad Ezzini, Yongfeng Huang, and
HinrichSchütze.2023. Ret-llm: Towardsageneral JacquesKleinTegawendeFBissyande.2024. Col-
read-writememoryforlargelanguagemodels. arXiv laborative agents for software engineering. arXiv
preprintarXiv:2305.14322. preprintarXiv:2402.02172.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu, John C Tang and Ellen Isaacs. 1992. Why do users
Long Ouyang, Christina Kim, Christopher Hesse, like video? studies of multimedia-supported col-
ShantanuJain,VineetKosaraju,WilliamSaunders, laboration. ComputerSupportedCooperativeWork
et al. 2021. Webgpt: Browser-assisted question- (CSCW),1:163–196.
answering with human feedback. arXiv preprint
arXiv:2112.09332. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-
dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and
openai. Gpt-4. AnimaAnandkumar.2023. Voyager: Anopen-ended
embodiedagentwithlargelanguagemodels. arXiv
openai.b. Sora. preprintarXiv:2305.16291.
Bruce M Owen and Steven S Wildman. 1992. Video Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya
economics. LaEditorial,UPR. Zhang,XiangWang,andShiweiZhang.2023b. Mod-
elscopetext-to-videotechnicalreport. arXivpreprint
pika. Pika. arXiv:2308.06571.
Chen Qian, Xin Cong, Cheng Yang, Weize Chen, XingyaoWang,YangyiChen,LifanYuan,YizheZhang,
YushengSu,JuyuanXu,ZhiyuanLiu,andMaosong YunzhuLi,HaoPeng,andHengJi.2024. Executable
Sun.2023. Communicativeagentsforsoftwarede- codeactionselicitbetterllmagents. arXivpreprint
velopment. arXivpreprintarXiv:2307.07924. arXiv:2402.01030.
YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,Lan JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Yan,YaxiLu,YankaiLin,XinCong,XiangruTang, Bosma,FeiXia,EdChi,QuocVLe,DennyZhou,
Bill Qian, et al. 2023. Toolllm: Facilitating large etal.2022. Chain-of-thoughtpromptingelicitsrea-
languagemodelstomaster16000+real-worldapis. soninginlargelanguagemodels. Advancesinneural
arXivpreprintarXiv:2307.16789. informationprocessingsystems,35:24824–24837.
Zeeshan Rasheed, Muhammad Waseem, Mika Saari, HuiYang,SifuYue,andYunzhongHe.2023. Auto-gpt
Kari Systä, and Pekka Abrahamsson. 2024. Code- foronlinedecisionmaking: Benchmarksandaddi-
pori: Large scale model for autonomous software tionalopinions. arXivpreprintarXiv:2306.02224.
developmentbyusingmulti-agents. arXivpreprint
arXiv:2402.01411. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
TomGriffiths,YuanCao,andKarthikNarasimhan.
runway. Runway. 2024. Treeofthoughts: Deliberateproblemsolving
with large language models. Advances in Neural
TimoSchick,JaneDwivedi-Yu,RobertoDessì,Roberta InformationProcessingSystems,36.
Raileanu,MariaLomeli,EricHambro,LukeZettle-
moyer, Nicola Cancedda, and Thomas Scialom. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
2024. Toolformer: Languagemodelscanteachthem- Shafran,KarthikNarasimhan,andYuanCao.2022.
selvestousetools. AdvancesinNeuralInformation React: Synergizingreasoningandactinginlanguage
ProcessingSystems,36. models. arXivpreprintarXiv:2210.03629.Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou,
RobertLo,AbishekSridhar,XianyiCheng,Yonatan
Bisk,DanielFried,UriAlon,etal.2023. Webarena:
Arealisticwebenvironmentforbuildingautonomous
agents. arXivpreprintarXiv:2307.13854.A Appendix
A.1 DreamFactoryResponsibilityallocation
As shown in Figure 8, our DreamFactory frame-
work utilizes multiple large language models
(LLMs) to form a simulated animation company,
takingonrolessuchasCEO,Director,andCreator.
Givenastory,theycollaborateandcreateavideo
through social interaction and cooperation. This
frameworkallowsLLMstosimulatetherealworld
byusingsmallvideogenerationmodelsastoolsto
accomplish a massive task. As illustrated in Fig-
ure 8, under their collaboration, it is possible to
generateaseriesofconsistent,stable,multi-scene
longvideosastheplotprogresses.
A.2 UserStudy
Quantitative evaluation of human preference for
videoisacomplexanddifficultproposition,sowe
employed human evaluators to verify the quality
ofmulti-scenevideosgeneratedbyourframework.
Wecollected150multi-sceneshortvideosgener-
ated by AI from the internet and compare them
with videos from our framework. Through this
approach,weaimedtoassesswhetherourvideos
couldachieveanadvantageinhumanpreferences
comparedtoexistingAIvideosonthenetwork.
In our study, We adopt the Two-alternative
Forced Choice (2AFC) protocol, as used in pre-
viousworks[(Blattmannetal.,2023),(Blattmann
etal.,2023), (Bar-Taletal.,2024)]. Inthisproto-
col,eachparticipantwillberandomlyshownapair
ofvideoswiththesamestory,oneisashortvideo
collected on web platforms and the other is gen-
erated by our framework. Participants were then
asked to select the superior side on five metrics:
role consistency, scene consistency, plot quality,
storyboard fluency, and overall quality. We col-
lected1320humanscoresforthisstudy,utilizing
schools,communities,andnetworkplatforms. As
illustrated in Figure 9, our method was preferred
better.
A.3 CaseStudy
ComprehensiveKeyframeCountStatistics-The
versioncurrentlyprovidedtousersisbalancedbe-
tweencostanduserexperience,usingtheShortgen-
erationmode,typicallyaroundtenscenes. Thespe-
cificnumberisrelatedtotheuser’staskinput. The
lengthofvideosgeneratedusingrandomprompts
isshowninthefigure10.CEO Y ano du war ee sC hh ai re ef aE x ce oc mut miv oe n O inff ti ec re er s. tN io nw co, w llae b a or re a tb ino gth t ow o surk ci cn eg s sa ft u lD lyr e ca om m pFa lec tt eo r ay Director Y Fao cu t oa rr ye . M Yoo uv ri e m D ai ir ne c reto spr. o N no siw b, i lw itye ia nr ce l ub do et sh aw rgo urk inin gg w a it t hD r Ce Ea Om a bout
movie with ideas given by a new customer. the movie and making agreements come true.
Phases managed Phases managed
Style Decision
Character Design
S a reio m a, li iw n ste g i' c r f e o v ira b g a e r me foe o rd r t e ht h eima mt m w oee vr' isr eie v e W t eh xae pt ls afh ie no e u l ts hl d l ei k a pev l oo it ti 'd .s oan ny ly d ti ha elo reg u toe d pS ooo n wm ' ete rs ft a uim y l. e cs a, n w bh ea t ju c sh t a ar sa cters Design 1 : Design 2 : Design 3 :
B thu et r t eh , e isr ne' 's t ia t? fine line A dig ar lee ce td s. aA nn dd aw cch ea nt ta sb ?out sD eee f win hit ee rl ey . wI' em c e ax nc i tt ae kd e t to h is
Temporal Agreement : Temporal Agreement : Final Agreement :
Reality Style , 4K, 35mm Anime cartoon style Cyberpunk future style
Story Decision
Script 1 : Script 2 : Script 3 :
"So, our hero's village has I was thinking, what if, instead of "So, they've formed this unlikely
been terrorized by this dragon killing the dragon, our hero ends bond. The next logical step in
j dof euo tcrr hn idd ee ie y n dc g ra s a hd t go oe ous c n. l od ,I t n d sf f ote ra o ee r n sl ts n t w 'l a ti ik nt ie h td ? h "h ki is im l l u mp rep ou ai nv ln io szd tete ea sr rl s tm ht ha eeon tmd d hi re on an ug gt w o i wt n g? h h iT se th n r ie te 't r w h te h ae's e s . a th dte h rir ae g j to m o ru unar s.nn t I e t ale 'y nsa dt aro n psg i yn ae mg rt th nbte o eor r l r s w oi hdo f ie pu t htld h e e irb e A m iw tha rr ir l io ar r s mho ou rld . be clad in H wo ew a ta hb eo ru -t b lo ean tg en h fa ai cr e a ？nd a N tho ew d rwT ae gh ona net …e' ds tc oo do esl i. gn
R d herig a ah g lit o n. n gH b his e i sj co oou wmrn nee s py a a t so p t a wk ti h oll utt o nh de s. T a thbh aa o tt u ' fs t o a t rh m b e sr bil bl ai ea t ttn wlet e t b ew u ni ts tt t h. h I eet m' s b .n oo nt d T uo ng ste ot ph per a, b t lh ee .y're Background Design Key phase prompt - …Now, we are designing the base backgound of the movie. ……
story I'm picturing the soldier's battle with the dragon
unfolding in the polar regions, a stark landscape
I dn e t feh ae t a , aft e lorm nea t sh o lo df i ea r c na at mas etr do phic o tyf p i ic ce a la fn ied r ys n lao iw r. that really sets it apart from the
M m A fosya l iMs ar tc geu a er rs i ,c o r huue est s r n de E a ia n svt cc is ogh i a van en tt e ro t sse t d ath h nFe e o a dr ne e cs n it es. ne t T t th hh e ee r s ee t ‘x ot r sr ye . am B lse u o t c awo hl hd ia d ta d d if ed nis n w a th os iu rs l r dfv r i iv oga fid l m e wl ae a gm s it ce e .n lat n t do ,
dragon, wounded and hiding
ArtDirector Y D joo r beu a i sa m r te oF t a dh c ee t so A ir gyr nt a D tn hi dr ee w sc ct eo e s nr. h e a sN r feo o w a r , ec w vo eme r ya m r ke o e nb y o in ft rth ae mrw e eo s str .k t Y oin o mg u r aa kt m ea in f D t t w O b o dr h a ee no e i vle te t s s em h mw isp d r r et ei r uo th t ha eie tf fu meg un o hn sh o a e re mut i ln ,e s p m rm a ' ar esf es ne a afa s. i r na n cpj aier eesj en, u s .n cwM dtr d tii ch e dsa a s o hcr nr,c a r i db pr eu ge o as aofc tn o t a nu se d rl , hrl m n ei e b and d s s rug . es i l dot to ld T t dir rmu age e, l oT s t m eh to nh sheem se se d — d aem i mert l ia lha tv ueg 'g i a srn doi l e c g i cn .s e lc . e K at v o h H t sa ntu a slo ii t il gn bd cw e g h .a emb t Mctv se khp e , a dart o c y,i t r ae w b ot sd s ehe pe t er let ts eo r h hm e s at ea ' ,h ss s n a m e ms tc n o h asi doe o gm is n rl ie d cet i t cl e ho or ir n me a g, g p a eai b ln lios nt u g t . a
the film more fluency …… originates from there, something lost to the
modern eye but still alive in legend."
Phases managed
Key Frames Checking
Scenes Design Key phase prompt - …Now, we are designing the Scene Series of the movie. ……
… A d w t T …h hr i… em a t e …hC g ca moo i rn t en , a r ae a ne i tn n n u rc dP r ito de h sl u e.eo u sn c ft t to ce herr ees s ss da ft u, r m alb l gyaa otl te t nalv e mo bsl aee csn kt E s sL u c h mxhe s a ot a oce‘ wr otn a l ts yW d eF y b o cc h , hnoo t ie ua s le e a yaour i g cm rd n .r uoct . h at e Idh a g vh tct f se 'n e hoi a t sipn t e r i rs c sl yak d r aet o ea , co e ld gw tlvr e t p or to rhe y e lb ot wt eu r e' hos u w ds n st bh t f ihd td fn tl ao hn oo ee h tu f o e rw v eti ak fl l at e nd t net, e c rled ,y doh a hp er a o p n.ma n pl "naw sg . lt d in o os th cm io m te oto e n h p na u en p. e rn " snt e l o ? oy rts t g s e . t hI s ,t rt I aac eultn i te n sk rg en snd ceie iae ,v o d t ns e w nhs s d ie tto ho S s h o S t w tr ht cc i f e ie s etee t e p hS en nh dss ye eec , t r e ht 21 aae lh s e u g :: n fr r om io mAT eke nuh i rme an c 'ge e snS g ei h t d m f e i n ibn d s et ar ta h ag r r i ran yt te e rh tgt o lhc bs d e eo wa e r e n t eu e ln o yf . ant is i we tso de hr ue u oc f er …es do i sl n gry …,g e g ia ns zt g, e Artist Y a fono rdu N a c e e on m wo a vmd, p r e et it e ech h r ra e o s ye tl r h i htc .m koh ae ea a r r g r e yeAa se c e r fti ams e t r r at b cs Do l moeo a i r mrab e een r sti m cg o .th o oot , nr. i nN to ew re, s w t. e Y a or ue r b mo at ih n w joo br k isin O G tg ooK a , pf tt o i h cDr ta rt ut eh r’ ae em s tn hc e Fo ex a o t sc l cto. o en nre y e. s
Scenes Picturing Key phase prompt - …Now, we are Picturing the Scene Series of the movie. …… Phases managed
Scen sve ie lS h nci ot de u u: n er e ne e t ss ti ee : i n tA s ht h o is r c o it knhl ekit e tia n ajr guy h n eis ng ao tl dl e od ., i te h hr i es Scene a hS u i sci m e s n io: le h dr o ei v u n: e e sA n ett t s u e to hr el s ii chsta k ri er in ny tt k o as ino ht gl ehd aei ie dn jr .tu onw g ti ht leh e, Scene S coc ti e l ho n+ ese s f a oi t1 l h+ l i d ra or1 g: a u e: g g ,W o hitn i s tt hh b r eoo u au r ts r rt t e ew s e cta shh .r on ro ii nn u gg g , h a The kS nc S ige c hen tn e e res i t u:P rnic st tu or ing Key phase prompt - …Now, we are P Si cc et nu eri n i +g t 1h :e Scene Series of the movie. ……
his homeland astride And he spots enemy
the mighty dragon. forces laying siege to
his cherished realm.
we sa hr oin uN g ldo a, h t r ah m ve e s o ao rl cd , e i ae rtnr a ds in h t o h aeu n l gvd i l d eb e .e o Go for theC no eo xl t! one! Go for theC no eo xl t! one! Monitor , Film-maker and the other agents based on different task…
Figure 8: This figure presents the responsibility allocation chart for all employees within the DreamFactory
architecture. Foreachemployee,theupperleftcornerdisplaystheirroleandportrait,whiletheupperrightcorner
outlinesthestagesofparticipationandtheirroles. Theessentialpartsofthepromptaredepictedbelow.
Figure9: Humanevaluationcomparisonofvideosgen- Figure10: ThekeyframenumberscountStatisticsof
eratedbyDreamFactoryandinternetAIvideos. DreamFactory.