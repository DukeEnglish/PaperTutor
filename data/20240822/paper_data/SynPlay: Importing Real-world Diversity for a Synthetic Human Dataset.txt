SynPlay: Importing Real-world Diversity for a
Synthetic Human Dataset
JinsubYim1∗ HyungtaeLee2∗ SungminEum3∗
Yi-TingShen1 YanZhang1 HeesungKwon3 ShuvraS.Bhattacharyya1
1UniversityofMarylandCollegePark 2BlueHalo 3DEVCOMArmyResearchLaboratory
Figure 1: SynPlay dataset is constructed while players play six traditional games in a virtual
playground also introduced in the Netflix TV show “Squid game” [24]. We have diversified the
humanappearancesinthescenesbyfocusingontwofactors: i)leveragingreal-worldhumanmotions
andii)adoptingmultipleviewpoints.
Abstract
WeintroduceSyntheticPlayground(SynPlay),anewsynthetichumandatasetthat
aimstobringoutthediversityofhumanappearanceintherealworld. Wefocus
ontwofactorstoachievealevelofdiversitythathasnotyetbeenseeninprevious
works: i)realistichumanmotionsandposesandii)multiplecameraviewpoints
towards human instances. We first use a game engine and its library-provided
elementarymotionstocreategameswherevirtualplayerscantakeless-constrained
andnaturalmovementswhilefollowingthegamerules(i.e.,rule-guidedmotion
design as opposed to detail-guided design). We then augment the elementary
motions with real human motions captured with a motion capture device. To
render various human appearances in the games from multiple viewpoints, we
usesevenvirtualcamerasencompassingthegroundandaerialviews,capturing
abundantaerial-vs-groundanddynamic-vs-staticattributesofthescene. Through
extensive and carefully-designed experiments, we show that using SynPlay in
model training leads to enhanced accuracy over existing synthetic datasets for
humandetectionandsegmentation. ThebenefitofSynPlaybecomesevengreater
fortasksinthedata-scarceregime,suchasfew-shotandcross-domainlearning
tasks. TheseresultsclearlydemonstratethatSynPlaycanbeusedasanessential
datasetwithrichattributesofcomplexhumanappearancesandposessuitablefor
modelpretraining. SynPlaydatasetcomprisingover73kimagesand6.5Mhuman
instances,isavailablefordownloadathttps://synplaydataset.github.io/.
∗Authorscontributedequally
Preprint.Underreview.
4202
guA
12
]VC.sc[
1v41811.8042:viXra1 Introduction
Large-scale synthetic datasets, known for their scalability, provide a practical solution to the in-
creasingdemandfortraininglarge-capacitymodels. Recentlydevelopedrenderingengines(e.g.,
Unity[49],Unreal[16],etc.) havesignificantlyenhancedtherealismofsyntheticdata,broadening
itsapplicabilityacrossvariouscomputervisiontasks. Despiteeffortstoscalesyntheticdatatomatch
theextensivecurationofreal-worlddata,thedesiredlevelofdiversityhasnotyetbeenachieved.
Thisinsufficiencyindiversityislargelyduetotheinadequateconsiderationandintegrationofkey
factorsthatareessentialtoreal-worlddiversityintheprocessofcreatingsyntheticdata.
Recently,severalattemptshavebeenmadetoincreasehumanappearancediversitybycontrolling
innatecharacteristics(e.g.,race,gender)[6],bodyshape(e.g,height)[36,6],orclothing[6]. These
datasetshavedemonstratedtheireffectivenessintasksaimedatidentifyinghumancharacteristics
from close-up images, e.g., human body/pose estimation [36, 6] and shape reconstruction [33].
However,thesedatasetshaveseldomyieldedadiscerniblepositiveimpactoncomputervisiontasks
aimedatidentifyinghumansfromadistance,e.g.,humandetectionandsegmentation.
Whenitcomestorecognizingtheoverallhumanappearancefromadistance,themotionsandposes
exhibitedbytheindividualsplaymorevitalrolesthanothercharacteristics. Despitepriorattempts
to synthesize various human poses, the quality of the rendering remained suboptimal, lacking in
realism[39]anddiversity[45]. AMASS[33]wastheoutputofanearlyendeavoraimedatachieving
bothrealismanddiversity,whereamotionscannerwasutilizedtocollectrealhumanmotions. Before
capturingthesemotions,detaileddescriptionswereprovidedtoarticulatespecificmovements—e.g.,
5secondswavingabovetheheadwithbotharms2,whileadheringtophysicalconstraintsthatlimit
largemovementsinmotion-captureenvironments. Thisdetail-guidedmotiondesignoftenresults
incapturingarestrictedrangeofmotionstiedtospecificdescriptionswhilemissingoutonallthe
motionsthatdefyeasydescription.
Weclaimthatprovidingrelativelyhigh-level,less-detailedguidancegreatlyhelpsinbreakingoutfrom
theaforementionedlimitationsandprovidesmorefreedomtowardstheexpansionofthediversityin
humanmotions. Inconstructingourdataset,wefollowthenewrule-guidedmotiondesignapproach
providinggame“rules”orwinningstrategiesforthevirtualplayerstofollow,whichserveasasetof
significantlycoarserguidelineswhencomparedtodetail-guidedapproaches. Inthisway,themotions
thattheymanifestarenotconfinedtopredetermined/easily-describablemotions. Asforthe“rules,”
weoptedtoborrowthemfromthesixtraditionalKoreangamesthatwerealsoplayedintheNetflix
TVseries“SquidGame”[24]. Thesegamesinvolvesubstantialamountsofphysicalmovements,
whichnaturallyprovideroomforadiversesetofhumanposesandmotions. Thediversityisfurther
influencedbyin-gamefactorssuchastheuniquelydefinedrulesofeachgame,thenumberofplayers,
andtheinteractionsbetweenthem.
Underourrule-guidedmotiondesignapproach,eachscenariorun(i.e.,oneroundofagameplayed
with specific settings) in the virtual environment is initialized by carrying out a scenario design
stepwhichisfollowedbytheincorporationofreal-worldmotions. Thescenariodesigninvolves
thesetupofalltheknobsthatcontroltheappearance,players(winnersandlosers),gamedynamics
(e.g.,how/wheneachgameends),andthehumanmotionevolutionsforeachspecificscenario. This
iswherethehigh-levelrulesofagivengamearedefined,andthecoarseboundaryofhowhuman
motionscanevolvewithinthegameisset. Theincorporationofreal-worldmotionisthephasewhere
a rich variety of motions truly comes to life. Details on the entire pipeline will be elaborated in
Section3.
Inaddition, wealsotookintoaccountthathumanappearancecanvarygreatlydependingonthe
perspectivefromwhichitisviewed. Accordingly,wecaptureeveryscenefrommultipleviewpoints
byimplementingseveralimage-capturingdevicestotakeadvantageofdifferentperspective-related
characteristics: threeUnmannedAerialVehicles(UAVs),threeClosed-CircuitTelevisions(CCTVs),
andoneUnmannedGroundVehicle(UGV).ThethreeUAVsflywithrandomtrajectoriesatdifferent
altitudes,thethreeCCTVsarelocatedatthefront,side,andbackofthegameplayground,andthe
UGVmovesrandomlywithintheplaygroundwherethegameisbeingplayed. Thesedevicesoffera
varietyofimage-capturingproperties,includingaerial-vs-groundanddynamic-vs-static. Ourstrategy,
designedtoprovideverydiverseviewpointsinscenecaptureprocess,notonlyservestoensurethat
2This motion description was used to construct the Mocap Database HDM05 in AMASS (https:
//resources.mpi-inf.mpg.de/HDM05/05-01/index.html)
2the dataset includes more diverse human appearances but also broadens the potential tasks (e.g.,
re-identification,multi-viewapplications,aerial-to-groundscenematching,etc.) forwhichthedataset
canbeused.
Byleveragingtheaforementionedhumanappearance-diversifyingstrategies,weconstructalarge-
scalesynthetichumandatasetcalledSynPlaythatcontainsmorethan73kimageswith6.5Mhuman
instances,seesampleimagesinFig1. TodemonstrateSynPlay’sabilitytorepresentavarietyof
humanappearancestotheextentseenintherealworld,weconductaseriesofexperimentswherewe
evaluatetheimpactofleveragingSynPlayalongsidereal(non-synthetic)datasetscuratedforavariety
ofhuman-relatedtasks,i.e.,,aerial-view/ground-viewhumandetectionandsegmentation. Forall
thetasks,trainingwithSynPlayoutperformsitscounterparts(i.e.,training-from-scratch,usingother
syntheticdata)acrossavarietyofdatasets. ExperimentsalsodemonstratethattheSynPlaydataset
significantlyimprovesmodelperformanceondata-scarcetasks,highlightingitsvalueinscenarios
thatrequiresubstantialsupplementarytrainingdata.
2 RelatedWorks
Synthetichumandata. Thecreationofvarioussynthetichumandatasetshasbeenfacilitatedby
theadvancementofmodernsyntheticdatarenderingenginessuchasBlender[7],Unity[49],and
Unreal [16], alongside human modeling tools like MakeHuman [34] and Character Creator [47].
These rendering engines enable a realistic representation of humans in 3D virtual environments,
while the modeling tools give creators precise control over the design of virtual characters. The
creatorsofthesedatasetsleveragedthesetoolstometiculouslycontrolkeydesignfactors,ensuring
suitabilityforspecifictasks,e.g.,SOMAset[4],PersonX[48],UnrealPerson[52],CARGO[51]for
Re-Identification,SURREAL[50]forposeestimation,GTA5[39]forsemanticsegmentation,and
Archangel-Synthetic[45]fordetection.
Recently,severalattemptshavebeenmadetoenhancetherealismofvirtualhumanmodels,with
theaimofbringingthemclosertothequalityoftheirreal-worldcounterparts. SMPL-X[37]and
AMASS[33]usedmotioncapturingdevicestocapturenaturalhumanmotions. BEDLAM[6]tried
toimprovethediversityofvariousfactorssuchasskin-tonesorclothingthataffecthumanouter
appearance, still relying on the SMPL-X. However, motion scanners imposed constraints on the
environment,particularlyincapturinglargemotionsoreventsinvolvingmultiplehumans. While
AGORA [36] and ScoreHMR [46] sought to digress from the usage of the motion scanners by
fittinghumanbodymodelstothehumanmotionsinreal-worldimages/videos, thequalityofthe
fittedhumanmodelsdeclineddrasticallyontheimagestakenfromadistance. Oneofourgoalsfor
ourdatasetwastoincorporatemultipleviewpoints,includingdistantviewsofthetargetscene. To
avoidcompromisingthequalityofhumanmotions/poses,wechosetousemotioncapturedevices,
whileimplementingourapproachtoensurethatfinalresultsinthedatasetarenotlimitedbythe
environmentsinwhichthedeviceswereused.
Naturalhumanmotionacquisition. Whetherwearecreatingarealorsyntheticdataset,images
ofmotionscapturedbydirectinghumanstoperformspecificactionsbasedonadescriptionoften
appearawkwardratherthannatural. Becauseofthat,mostdatasetsaimtoincludehumansengagedin
dailyactivities(e.g.,MSCOCO[29],MPIIHumanPose[3]),performingtaskssuchassports(e.g.,
UCF-Sports[41],SoccerNet[11],SportsMOT[13])orart(e.g.,Human-Art[26])tocapturetheir
motionsandposesinthemostnaturalstatespossible. However,itisself-contradictorytoartificially
createavirtualeventtocapturenaturalmotionsassociatedwiththeevent. Inthispaper,weaimto
detourthisself-contradictbyinitiallydesigningthevirtualevents(i.e.,aforementionedgames)using
existingbutnon-naturalvirtualmotions,whicharethenreplacedbyreal-worldmotionscaptured
usingamotioncapturedevice.
Supplementaldatasetsfortraining. Enhancingmodelperformancebysupplementingthetraining
withadditionaldatahasbeenacommonstrategy[38,27]. Initially,thisinvolvedcombiningdatasets
constructedwiththesamepurpose,likeMSCOCO[29]andPASCALVOC[17],fortaskssuchasob-
jectdetection. Someapproachesutilizedlarge-scaledatasets(e.g.,ImageNet[14]orInstagram[32]),
which were not necessarily designed for the target task, to build foundational features, followed
byatransferlearningsuchaspretrain-finetune[20]orPTL[43]toadjustthemodelonthetarget
dataset. Asmodelshavegrowninsizeandcomplexity(e.g.,ViT[15]),thedemandforlarge-scale,
high-qualitydatasetshasincreased,butthehighcostsofannotationpresentasignificantbarrier. To
3Scenario design Incorporation of real-world motions Game sequence rendering
Game scenario pu ll motion 1 ... ...
- Players ID: 3, 4, 7, 14, … - Motion evolution: tug-of-war
- Background: urban1
- 1M . eo ntt ri yo (n fo rt i 1m sei cn .)g --ss tttt ooaa -- nn ssdd iitt ssii tt motion 2
2. stand (for 1.7 sec.) motion 3 Real motion capture
3. transition to grab (for 0.3 sec.) motion 4
4 …. grab (for 2.3 sec.) mmoovvee ... ...
- Team
1. team A: 3, 45, … ggrraa bb
2: team B: 4, 19, … eennttrryy ssttaanndd motion add
...
- Individual player ppuu llll
- ID 3
3D location: (31.4, 27.5, 0)
win/lose: lose @ 7th motion wwii nn llooss ee
...
...
Figure 2: Game sequence generation pipeline. This illustrate how we create a sequence for a
tug-of-wargamewhichincludesanexampleofhowweincorporatereal-worldmotionstowards
the elementary motion state of pull. In the motion evolution graph, the start and end nodes are
indicatedbygreenandredcircles,respectively. Adiversesetofpullmotioninstancesisshown
belowtheimageoftherenderedscene.
addressthis,label-agnostictrainingmethodslikeself-supervisedlearning[9,8,21,22]andsynthetic
datasetgenerationwithcost-freeannotationshaveemergedasviablesolutions. Inresponse,wehave
specificallydesignedSynPlaytosupplementvariouscomputervisiontasksthatrequirealarge-scale,
highly-diversifiedhumanappearanceset.
3 SynPlayDataset
Inanutshell,weaimtocreateasyntheticdatasetinavirtualenvironmentthatcapturesthediversity
ofhumanappearancesfoundintherealworld.
Accordingly, wedesignourdatasetwithafocusontwokeyaspectsthatnaturallydiversifiesthe
humanappearancescapturedinthescenes: i)expandingthemotionsetwithincreasedreality,andii)
capturingeachscenefromadiversesetofcameraviewpoints.
Diverseyetrealistichumanmotions. Weuserule-guidedmotiondesigninourSynPlaydataset,
borrowing “rules” from six traditional Korean games, also featured in the Netflix series “Squid
Game” [24]3. This approach offers coarser motion guidance for the virtual players, facilitating
thegenerationofawidespectrumofnaturalmotions, evenincludingtheonesthatdefydetailed
description.
Our rule-guided motion design is effectively baked into the overall sequence-generating design
pipeline,asshowninFigure2,thatconsistsofthescenariodesignfollowedbytheincorporation
of real-world motions. The scenario design involves the setup of all the knobs that control the
appearance,players(winnersandlosers),gamedynamics(e.g.,how/wheneachgameends),andthe
humanmotionevolutionsforeachspecificscenario. Alltheitemswithineachscenariothatdonot
havetobehard-coded(e.g.,gamerules)areselectedrandomlywhendesigningeachscenario. The
motionevolutionofeachvirtualplayerinaspecificgameisgovernedbyagraphstructurewhere
allpossibleelementarymotionsandtheirpotentialtransitionsarerepresentedasnodesanddirected
edges,respectively. Eachnodeistiedwithapoolofmotionsthatfallunderthesameelementary
motionstate(e.g.,move,sit). Asthegameprogresses,avirtualplayerevolvesitsmotionfollowing
the directed edges and stays there according to the ‘motion timing’, also defined in the scenario
design. At each state, the virtual human randomly chooses to exhibit one of the motions in the
correspondingpool. Notethat,whileauniquelydesignedscenarioisusedforauniquesequence,the
samemotionevolutiongraphisusedforallthesequencescapturedunderthesamegamerule.
Beforeincorporatingthereal-worldmotions,weleveragetwotechniquestopre-diversifytheelemen-
taryhumanmotionsreadilyavailableinhumanmotionlibrariessuchasMixamo[2]: i)dynamically
blendingtwoexistingmotionsofsimilartypestogenerateanewmotiontype(e.g.,blendingslow-
walkingandrunningtogeneratehasty-walking),ii)usingelementarymotionsasanimationlayersto
3Ourgamescenariosaredesignedbasedonthetraditionalgamerules,withouttakinganyspecificsituations
fromtheshow.
4UAVlow-alt UAVmed-alt UAVhigh-alt CCTVfront CCTVside CCTVback UGV
Figure3: MultipleviewpointsusedinSynPlay. Onthetop-rightcornerofeachimage,weplacetheenlargedcropofonehuman
instancewhoisvisiblefromallsevenviewpointsineachscenario. Multiplecameraviewpointsallowsubstantialvariationsin
appearanceforthesamehumansubjectwithidenticalpose.
makeanewmotion(e.g.,raisinghandswhilewalking). Ontopofthepre-diversifiedsetofmotions
foreachgame,arealhumanplayerwearingamotioncapturedevice4 isaskedtoeithersimilarly
mimicornewlycreatemotionsthatalignwiththegivengamerule(thus,rule-guided). Forexample,
for the game of tug-of-war, human players were provided with the game rule, and then asked to
reenactanypossiblemotionwiththefreedomofchoosingthewinningorlosingside.Forsomegames,
morethanoneplayerswereaskedtoactuallyplaythegametogethertocapturethemotionsthat
cannaturallyariseatthetimeofphysicalinteractions. Astheresultofincorporatingthereal-world
motions,thetotalnumberofuniquemotionsinSynPlayincreasedfrom104to257.
Multipleviewpoints. ThecameraviewpointswithinSynPlayarediversifiedbyimplementingthree
widelyusedtypesofimage-capturingplatformsintherealworld: UAV,UGV,andCCTV.Theycover
avarietyofimage-capturingpropertieslikestatic/dynamicandground/aerial. Viewpointdiversityis
acquiredbycontrollingthelocationsandthefocalpointsofthecameras. ThreeUAVs,threeCCTVs,
andoneUGVhavebeendeployed(Figure3),resultinginsevenuniqueviewpointsforeverygame
sequence. TheUAVsaredeployedtoflyatvariousrandomlocationswhilemaintainingaltitudes
oflow(∼30m), medium(∼50m), andhigh(∼100m). CCTVsarelocatedataheightof15mat
thefront,back,andeithersideofthegameplayground. UGVimagesarecapturedassumingthat
avehicleisrandomlyroamingontheground. Thefocalpointsaresetatseverallocationscloseto
theareawherethegameusuallytakesplace. FortheUAVs,thefocalpointischangedtoarandom
locationevery10sec,whereeachchangetakes5sectobefixedatalocationforanother5sec. Focal
pointsoftheCCTVsandtheUGVdonotchangeoncedetermined.
Dataset specification. We created ten different scenarios for each game, resulting in 60 game
scenarios in total. Frames were rendered from seven different camera viewpoints at 1 fps with
the resolution of 1920×1080, resulting in a total of 73,892 images with more than 6.5M human
instances. Theframegenerationratewasselectedas1fpstoavoidincludinghighlyredundanthuman
poses. Takingfulladvantageofthegameengine’sabilitytogenerateannotationswhilerenderingthe
scenes,weprovidedvarioustypesofgroundtruthannotationsusefulforvariouscomputervision
tasks: 2D/3Dboundingboxes,instance-levelsegmentationmasks,depthmaps,andhumankeypoint
locations.
3.1 Otherdesignfactors
Characters. Wehavedesigned456virtualcharactersusingtheCharacterCreator[47],whereeach
characterisinvolvedinmultiplegamescenarios. Tovarytheappearanceofthecharactersandavoid
generatingbiases,eachcharacterwasuniquelydesignedwithgender,skincolor,age,height,obesity
(bodytype),hair(stylesandcolors),andoutfits. Wekeptthegenderratiobetweenmaleandfemale
at1:1andtheratioofskincoloramongwhite,black,yellow,andbrownat1:1:1:1. Forage,each
characterwasdesignedtofallintooneofthreecategories: child,middle-aged,andelderly,andthe
ratiowassetat1:2:1. Foreachgenderandagegroup,heightsweremodeledtofollowabell-shaped
4EachrealplayerusedaSmartSuitProIIandapairofSmartglovesfromRokoko(http://rokoko.com)
5Table1: Comparisonwithothersyntheticdatasetsonaerial-viewhumandetectionandsemanticsegmentation. Thenumbers
inparenthesesarethegapsfromthemodeltrainedwithoutsyntheticdata(‘real’). Positiveandnegativegapsareindicatedingreen
andredfonts,respectively. Thebestaccuracyforeachsettingisshowninbold. Notations: ‘+real’representsamodelpre-trained
withsyntheticdataandfine-tunedona‘real’dataset,where‘real’isatrainingsetofdatasetusedforevaluation. ‘s’,‘m’,and‘l’
representthreeYOLOv8modelswithdifferentarchitectures.
(a)humandetection (b)semanticseg.
VisDrone[54] Okutama-action[5] SemanticDrone[1] SemanticDrone Aeroscapes
dataintraining
s m l s m l s m l [1] [35]
real 19.72/47.43 21.14/49.52 21.60/51.10 27.40/75.17 28.99/76.60 31.53/78.78 44.00/ 77.20 44.52/ 78.52 42.62/ 79.87 0.66 22.25
Archangel[45] 0.23/ 0.63 0.38/ 0.98 0.59/ 1.48 2.59/ 8.45 3.90/10.13 2.83/ 9.12 0.64/ 1.59 2.42/ 5.37 0.94/ 1.62 0.74 0.04
SynDrone[40] 0.31/ 0.81 0.36/ 0.84 0.71/ 1.89 0.00/ 0.00 0.00/ 0.01 0.00/ 0.00 0.00/ 0.00 0.00/ 0.00 0.00/ 0.00 0.07 0.00
SynPlay 5.29/11.75 4.31/ 9.12 2.79/ 5.87 12.74/40.86 8.19/25.43 8.15/25.23 7.02/ 12.21 9.60/ 15.51 15.71/ 23.59 8.03 6.44
Archangel+real 18.77/45.39 20.25/48.52 20.82/49.51 30.72/80.35 32.36/80.63 31.71/79.63 46.60/ 74.07 48.60/ 75.86 44.62/ 73.23 9.28 20.61
(-0.95/-2.04) (-0.89/-1.00) (-0.78/-1.59) (+3.32/+5.18) (+3.37/+4.03) (+0.18/+0.85) ( +2.60/ -2.13) ( +4.08/ -2.66) ( +1.00/ -6.64) ( +8.62) (-1.64)
SynDrone+real 18.78/45.79 20.94/49.44 21.97/51.51 29.70/77.71 31.39/79.42 31.24/78.71 50.93/ 82.28 53.71/ 85.47 59.59/ 85.02 5.56 24.59
(-0.94/-1.64) (-0.20/-0.08) (+0.37/+1.41) (+2.30/+2.54) (+2.40/+2.82) (-0.29/-0.07) ( +6.93/ +5.08) ( +9.19/ +6.95) (+16.97/ +5.15) ( +4.90) (+2.34)
SynPlay+real 20.88/49.31 22.34/52.12 22.98/52.93 32.47/81.60 31.96/81.13 33.17/82.52 66.52/ 90.33 69.46/ 91.35 68.82/ 91.37 23.32 32.19
(+1.16/+1.88) (+1.20/+2.60) (+1.38/+1.83) (+5.07/+6.43) (+2.97/+4.53) (+1.64/+3.74) (+22.52/+13.13) (+24.94/+12.83) (+26.20/+11.50) (+22.66) (+9.94)
distribution,resultinginanoveralldatasetrangeof140to190cm. Wemanuallydesignatedevery
characterwithauniqueoutfit,whilesettingthehairandobesityaspectasdiverseaspossible.
Backgrounds. Foreachscenario,wesetdifferentenvironmentalfactors: sites,lightingconditions,
andweather. Agametakesplaceatoneoftencustom-builtsites. Theseincludefiveurbanlocations
(threecommoncityareas,aconstructionsite,andafactorysite)andfivenaturalsites(agreenarea,a
snowyfield,adesert,ameadow,andabeach). Multiplelocationswithineachsitemapcanbeusedas
localplaygrounds. Tovarythelightingconditions,wetakeintoaccountfivedifferenttimesofday:
dawn,morning,noon,afternoon,andsunset. Weconsiderthreetypesofweather: sunny,foggy,and
rainy. Allofthesearerandomlydeterminedforeachscenario.
4 TaskEvaluation
Inlinewiththeinherentpurposeofsyntheticdatatoserveassupplementaltrainingdata,weusethe
entireSynPlaydatasettotrainmodelsforvariouscomputervisiontasksandevaluateitspositive
impact towards task performance. Our major counterpart models in evaluation are trained-from-
scratch,whicharetrainedonlyonrealimages(denotedasrealinevaluationtables). Wealsovalidate
theadvantageofusingSynPlayoverothersyntheticdatasets.
4.1 Generaltasks: detectionandsegmentation
WeevaluatetheSynPlaydatasetontwogeneralvisiontasks, humandetectionandsegmentation.
Thesetasksrequiretheabilitytoidentifydiversehumanappearancesinimagescapturedatadistance.
To leverage synthetic data during training, we adopt a pretrain-finetune strategy, where a model
is pre-trained on synthetic data and fine-tuned on target real-world data. The detectors used in
theexperimentsareYOLOv8models[25]withthreedifferentarchitecturesizes(small,medium,
andlarge). Mask2former[10]withtheSwin-Base[30]backbonewasusedforsegmentation. For
evaluationmetrics,weusetheCOCO-styleAPswhicharetwoboundingboxAPs(APbbandAPbb)5
50
forhumandetectionandIntersection-over-Union(IoU)forthesegmentation.
The main tasks are conducted on aerial-view datasets, which feature a wider range of human
appearances,makingthemidealforvalidatingthedesignphilosophybehindtheSynPlaydataset. We
alsoconductexperimentsonground-viewdatasetstoevaluateSynPlayonamorewidelystudiedtask
inthecommunity.
Aerial-viewtasks. Table1showstheresultsforaerial-viewhumandetectionandsemanticsegmenta-
tiontasks. Overall,forbothtasks,usingSynPlayintrainingprovidesremarkablybetteraccuracy
thanallthecomparedcases,includingrealandalltheothervariationsinvolvingothersyntheticdata.
5Detectionaccuracyforeachmodelinthefollowingtablesarereportedwithtwonumbersintheformof
APbb/APbb.
50
6Table2: ImpactofSynPlayonMSCOCO(personcategory). Table3: SynergyimpactwithMSCOCOonaerial-view
Notation: ‘SynPlay-UGV’and‘SynPlay-all’areaUGVsubsetof human detection. YOLO v8 model with a medium size
SynPlayandtheentireSynPlay,respectively. architectureisused.
dataintraining VisDrone Okutama-action SemanticDrone
(a)humandetection (b)sem.seg.
real 21.14/49.52 28.99/76.60 44.52/78.52
COCO 7.16/16.46 15.17/48.28 34.74/56.39
dataintraining s m l
SynPlay 4.31/ 9.12 8.19/25.43 9.61/15.52
real 46.19/65.91 50.10/69.86 52.52/72.15 15.10
COCO+SynPlay 11.49/25.20 14.68/49.82 18.60/31.03
SynPlay-UGV+real 46.53/66.18 50.70/70.37 52.69/72.29 20.18
COCO+real 22.11/51.73 32.26/80.10 65.72/89.20
(+0.34/+0.27) (+0.60/+0.51) (+0.17/+0.14) (+5.08) SynPlay+real 22.34/52.13 31.96/81.13 69.46/91.35
SynPlay-all+real 46.84/66.70 51.12/70.74 53.00/72.59 21.57 COCO+SynPlay+real 22.78/53.01 33.82/79.44 73.52/92.80
(+0.65/+0.79) (+1.02/+0.88) (+0.48/+0.44) (+6.47)
Notably,theresultsshowthatwarmingupthemodelwithsyntheticdatabeforeincorporatingrealdata
generallydoesnotimproveperformance,exceptinthecaseofSynPlay. Inotherwords,unlessthe
syntheticdatasetisproperlydesignedandconstructed,wecannotexpectperformanceimprovement
simplyfromaddingsyntheticdatatothetrainingprocess. Moreover,amongcasesusingsynthetic
dataonlyintraining,SynPlaypresentsunparalleledaccuracy. Infact,theresultsusingothersources
ofsyntheticdataaresopoorthattheothersourcescanbeconsidereduselessforthistypeofdataset
utilization. Basedonthesetwoobservations,ourdesignstrategiesforenhancingthediversityand
realismofhumanappearanceareshowntobehighlyeffectiveinmeetingexpectations.
Ground-viewtasks. Table2explorestheimpactofusingSynPlayforthegeneralcomputervision
tasksofground-viewhumandetectionandsemanticsegmentation. Wealsoevaluatehowmodels
performwhenonlythesubsetwithmatchingviewpoint(i.e.,UGVimagesinSynPlay)isusedin
training. Overall,usingtheentireSynPlayyieldsthehighestaccuracyonbothtasks,whileusingthe
UGV-subsetstilloutperformsthemodeltrainedwithoutSynPlay. Theseresultsdemonstratethatour
insightinensuringdiversitybyvaryingthecameraviewpointsiseffectiveevenintasksthatdonot
containsuchmultipleviewpoints.Inaddition,thegreaterimprovementinsemanticsegmentationover
objectdetectionshowsthatensuringdiversityismoreeffectiveintasksthatrequiremoredetailed
humanrepresentationmodels.
Combination with MS-COCO for pre-training dataset. The effect of using pre-training can
be greater when applying two or more datasets with complementary properties. Here, we aim
to investigate the potential synergy achieved by integrating MS COCO, a real dataset primarily
comprisingground-viewimages,withSynPlayforthetaskofaerial-viewhumandetection. Table3,
shows all combinations of SynPlay and MS COCO datasets when used for pre-training. The
anticipatedsynergisticeffectappearsinallcasesexceptinonecase(APbbresultsonOkutama-action)
50
whenfine-tunedonthetargetdataset. Moreover,usingSynPlayonlyprovidescomparableaccuracy
tousingMSCOCOwhenusedindirectlythroughfine-tuningontherealdataset.
Interestingly,theresultswithoutfine-tuningshowadifferenttrend. OnOkutama-actionandSemantic
Dronecases,usingMSCOCOperformedbetterthanothertwobaselines,whileSynPlayspecifically
showingamuchloweraccuracy. Weobservethatsyntheticdatastilllagsbehindreal-worlddatain
manyrespects,highlightingtheneedforfurtherresearchtobridgethegap.
4.2 Data-scarcetasks: Few-shotandcross-domainlearning
Inthissection,wecompareSynPlaywithothersyntheticdatasetsonitsabilitytomeetthedemand
foradditionaldataindata-scarcetasks. Fordata-scarcetasks,weadoptfew-shotandcross-domain
learningtasksonaerial-viewhumandetection,whichsuffersmoreseverelyfromalackoftraining
datathanground-viewdetection. Followingthedata-scarcetasksetupsof[43],wetrainmodelswith
twofew-shotregimesusing20and50imagesofVisDrone(denotedby‘Vis-20/50’). Wetestthe
modelsonVisDrone,Okutama-action,andSemanticDronewheretheevaluationsdoneonthelast
twodatasetscanbeseenas‘cross-domain’. Toattenuatethepotentialrandomeffectsthatmayarise
whenselectingrealtrainingimages,allreportednumbersareaverageaccuracyoverthreeruns.
Asbaselinemethodsleveragingsyntheticdataintraining,weuseapretrain-finetunestrategy(PT-
FT)andProgressiveTransformationLearning(PTL)[43]. PTLisaprogressivedataaugmentation
approach that iteratively expands the training set by adding a subset of synthetic data, which is
transformed to look real. In each PTL iteration, a subset of the synthetic data is selected, such
7Table 4: Few-shot and cross-domain learning accuracy (APbb/APbb) on aerial-view human
50
detection. Toclarify,theaccuracyonOkutama-actionandSemanticDronereferstocross-domain
learningperformance. Notation: ‘Archangel*’isanexpanded‘Archangel’tobepose-diversified[44].
Vis-20 Vis-50
dataintraining method VisDrone Okutama-action SemanticDrone VisDrone Okutama-action SemanticDrone
real 0.58/ 2.27 3.64/ 14.54 0.62/ 1.89 0.76/ 3.30 7.82/ 28.66 1.30/ 5.65
+Archagel 2.07/ 6.72 7.90/ 31.53 8.81/ 33.71 2.92/ 9.26 11.49/ 42.51 8.98/ 33.21
+Archagel* 2.26/ 7.39 8.95/ 36.97 6.45/ 26.13 2.99/ 9.42 12.89/ 47.24 6.29/ 25.50
PTL
+SynPlay 3.08/ 9.03 14.39/ 49.53 6.94/ 24.22 3.71/11.20 15.67/ 52.06 7.74/ 26.99
(+2.50/+6.76) (+10.75/+34.99) ( +6.32/+22.33) (+2.95/+7.90) (+7.85/+23.40) (+6.44/+21.34)
+Archagel 0.76/ 2.48 4.24/ 17.17 6.53/ 23.67 1.29/ 3.76 5.32/ 20.96 7.10/ 27.95
+Archagel* 1.21/ 4.02 9.14/ 34.70 8.20/ 28.80 1.84/ 5.37 10.39/ 36.83 8.63/ 30.09
PT-FT
+SynPlay 2.94/ 9.38 12.19/ 40.88 11.32/ 37.30 3.72/11.87 13.66/ 44.06 12.76/ 41.66
(+2.36/+7.11) ( +8.55/+26.34) (+10.70/+35.41) (+2.96/+8.57) (+5.84/+15.40) (+11.46/+36.01)
bbox AP50: VisDrone bbox AP50: Okutama-action bbox AP50: Semantic Drone
40
9 50
30
6 40
20
30
PTL: Archangel PTL: Archangel PTL: Archangel
3 PTL: Archangel* PTL: Archangel* PTL: Archangel*
PTL: SynPlay PTL: SynPlay 10 PTL: SynPlay
PT-FT: Archangel 20 PT-FT: Archangel PT-FT: Archangel
PT-FT: Archangel* PT-FT: Archangel* PT-FT: Archangel*
PT-FT: SynPlay PT-FT: SynPlay PT-FT: SynPlay
1,080 4,320 17,280 34,994 73,892 1,080 4,320 17,280 34,994 73,892 1,080 4,320 17,280 34,994 73,892
# img (log scale) # img (log scale) # img (log scale)
Figure4: ScalingbehaviorofsyntheticdatasetsundertheVis-20setup(APbb). Scalingbehaviorof
50
eachdatasetiscomparedbyrandomlysampledsubsetsof1,080,4,320,and17,280images,which
correspondto1/16ththesize,1/4ththesize,andthesizeofArchangel. Forreference,thesizesof
Archange*andSynPlayare34,994and73,892,respectively.
thatsyntheticdatathatisclosertotherealdatasetisselectedmoreoften. Forthedata-scarcetasks
experimentedin[43],PTLwasbetterthanPT-FTwhilebothoutperformedthecaseswithoutsynthetic
data. WeusedRetinaNet[28]asthedetector.6
Comparison with other synthetic data. In Table 4, we compare the detection accuracy of the
modelstrainedwithdifferentsyntheticdatasetsonthefew-shotandcross-domainlearningtasks.
WithPT-FT,SynPlayachievedsignificantlybetteraccuracythanothersyntheticdatasetsacrossall
three datasets. With PTL, SynPlay performed the best on VisDrone and Okutama-action. These
resultswereconsistentforbothVis-20andVis-50settings. EvenonSemanticDrone,whichshows
anunusualperformancetrend,thebestperformancewasachievedwhenSynPlaywasusedviaPT-FT.
In addition, compared to SynPlay’s performance improvement on general tasks (in Table 1), the
improvementachievedondata-scarcesettingbySynPlayismuchgreateronVisDroneandOkutama-
actionondata-scarcetasks. ThisdemonstratesthatSynPlayeffectivelymeetsthedemandforaddi-
tionaldataindata-scarcesetting,whichisgreaterthanthatingeneraltasks. Wewilldiscussthe
unexpectedperformancetrendsonSemanticDroneinmoredetailinSec.5.
Scalingbehaviors. Tofairlyvalidatetheperformancecomparisonwithoutbeingaffectedbydataset
size, we explore the scaling behavior of synthetic datasets. In Fig 4, we compare the detection
accuracy of three synthetic datasets at multiple points where the datasets are randomly sampled
tohavethesamesize. Onallthreetestsets,thebestperformingmodelsuseSynPlayintraining,
i.e.,SynPlay+PTLonVisDroneandOkutama-action,SynPlay+PT-FTonSemanticDrone. The
performancegainachievedusingSynPlayisnotsimplyduetothelargesizeofthedataset.
4.3 Imagequalityevaluation
InTable5,foralltrainingdatasetsinvolvedinourexperiments,wecalculateFID(FréchetInception
Distance)[23]toassesstheirfidelityanddiversity. SynPlaypresentsthebestscorecomparedtoother
syntheticdatasets,whichisaresultthatalignswellwithourtaskresults. Theseresultssuggestthat
6PTLwasdesignedtobesuitableforRetinaNet.ForafaircomparisonbetweenPTLandPT-FT,weused
RetinaNetinsteadofYOLOv8forthisexperiment.
8Table 5: FID comparison. In FID calculation, VisDrone serves as a reference representing real
aerial-viewhumandata.
COCO Archangel Archangel* SynDrone SynPlay
48.16 67.20 67.20 21.66 18.36
Table 6: Proportion of nadir-view instances in synthetic datasets used in data-scarce tasks.
Instanceswithcameraviewinganglefromgroundgreaterthan71.57◦areconsideredasnadir-view
instances.7
Archangel Archangel* SynPlay
25.00% 12.82% 4.24%
SynPlay’ssuperiortaskperformanceisachievedbybetterfidelityanddiversity,whichareourgoals
indesigningSynPlay. Moreover,thebetterFIDofSynPlaycomparedtoMSCOCO,whichmainly
includes ground-view images, is also reported, supporting the hypothesis that adopting multiple
viewpointseffectivelydiversifieshumanappearances.
5 DiscussionandConclusion
PeculiarperformancetrendonSemanticDrone. Followingconflictingphenomenawereobserved
inexperimentalresultswhentestedonSemanticDrone:
– When using synthetic data in training via PTL for data-scarce tasks, involving SynPlay
under-performedwhencomparedtocasesusingothersyntheticdatasets.
– The performance gain acquired by incorporating a synthetic data during training is re-
markablylargeforSemanticDronewhencomparedtotheothertwocases,whileSynPlay’s
incorporationshowcasingthemostsubstantialgain.
MosthumaninstancesinSemanticDronearetakenfromnadirviews,whileVisDroneandOkutama-
actionhaveinstancescapturedwithfewernadir-views.Theportionofnadir-viewinstancesinSynPlay
isthesmallestamongthesyntheticdatasetsusedindata-scarcetasks(Table6). AsPTLcontinuesto
prioritizesamplesfromsyntheticdatathatcloselyresembletheseeddata(i.e.,VisDrone)fortraining,
the reduced selection of nadir-view instances from the SynPlay may result in a lower gain (first
phenomenon). Ontheotherhand,thesecondphenomenonindicatesthatensuringgreaterdiversity
viausingsupplementalsyntheticdatahasgreaterimpactonSemanticDrone,whichlacksdiversity
duetoitslimitedviewpoints. Moreover,SynPlaythatislesssimilartoSemanticDronewhilebeing
morediverseinrelationtothecomparedsyntheticdatasets,showsthelargestimpact,supportingour
claimthatimprovingthediversityisgenerallyeffectiveinconstructingabettersyntheticdata.
Broaderimpact. Utilizingrealhumandatasetsfrequentlyentailsinherentprivacyconcerns. We
hopethatourendeavorstoenhancesynthetichumandata,movingitonestepclosertoreal-world
fidelity,willcontributetoalleviatingthesechallenges.
Limitations. SynPlay was developed to provide richer representations of human appearance for
tasksthatinvolvelocalizinghumaninthescenes. Werecognizethesignificanceofincorporating
distinctivefeaturesfromdiverseobjectcategories. Forfuturework,weareaimingtoexpandthe
SynPlaytoencompassawiderarrayofcategories,therebyenrichingitstrainingcapabilities.
Conclusion. Whatmotionahumanperformsandwhereapersonisviewedfromaretwocrucial
factorsthatmakeadifferenceinhowahumanlooks. Wecreateasynthetichumandatasetcalled
SynPlay with the aim of expanding the diversity human appearance by diversifying these factor.
EnsuringthediversityallowedSynPlaytohaveagreaterpositiveimpacttowardsmodeltrainingwhen
comparedtoothercounterparts(train-from-scratch,usingothersyntheticdata)onaerial-view/ground-
viewobjectdetectionandsemanticsegmentation. ThispositiveimpactofSynPlaybecomeseven
greaterindata-scarcetasks,wheresyntheticdataisstronglydesiredassupplementaltrainingdata.
Acknowledgement. ThisresearchwassponsoredbytheDefenseThreatReductionAgency(DTRA)
and the DEVCOM Army Research Laboratory (ARL) under Grant No. W911NF2120076. This
7ThiscriterionisthelargestviewinganglefromgroundonArchangel*.
9researchwasalsosponsoredinpartbytheArmyResearchOfficeandArmyResearchLaboratory
(ARL) under Grant Number W911NF-21-1-0258. The views and conclusions contained in this
documentarethoseoftheauthorsandshouldnotbeinterpretedasrepresentingtheofficialpolicies,
either expressed or implied, of the Army Research Office, Army Research Laboratory (ARL) or
theU.S.Government. TheU.S.Governmentisauthorizedtoreproduceanddistributereprintsfor
Governmentpurposesnotwithstandinganycopyrightnotationherein.
Appendix
A ComparisontoOtherDatasets
Table7providesacomparativeanalysisofvarioushumandatasets,categorizedasrealorsynthetic
andcapturedfromeithergroundoraerialperspectives. Keyobservationsfromthiscomparisonare
outlinedbelow:
1. Aerial-viewsets,thankstotheirwideviewingangles,generallyhavemorehumaninstances
perimagethanground-viewsets, exceptforfewcasesthatemployedafixednumberof
actorsinarealsetordesigningoneinstanceperimageinasyntheticset.
2. Aerial-viewsetsgenerallycontainawiderrangeofviewpoints. (mostlynear∼far)
3. Forexistingsyntheticdatasets,aerial-viewsetstypicallyfeaturefewermotionvariations
comparedtoground-viewsets. Thisisbecauseaerial-viewdatasetsoftenprioritizeleverag-
ingawiderangeofviewpointsoverexpandingthevarietyofhumanmotions.
4. Rule-guideddesigncanutilizesignificantlylargerrangeofhumanmotionscomparedto
detail-guideddesign.
ThecomparisonshowninthetablealsodemonstratesthatSynPlaysuccessfullyaddressestheshortfall
of aerial-view synthetic datasets (3rd observation), while maximizing the benefits of aerial-view
datasets(1stand2ndobservations).
Moreover,the4thobservationsupportsthatourproposedrule-guideddesignissuccessfulinsecuring
thediversityofhumanmotionsintheset. ItisnoteworthythatwhileSURREAL[50](constructed
with ‘detail+mocap’) contains a comparable number (6.5M) of human instances as SynPlay, the
numberofmotionsmanifestedinthedatasetisextremelylimitedwhencomparedtoSynPlay(23vs.
uncountable).
B SynPlayStatistics
Here,weprovideseveralstatisticsfromtheSynPlay. Fig6showsthedistributionofboundingbox
sizesoverhumaninstancescapturedbyeachdevice. Themajorityofboundingboxsizesaresmall,
which illustrates a characteristic of aerial-view datasets. Interestingly, UAVs can capture human
instanceswithlargerboundingboxesthanCCTVs. Thiscouldbeduetothefactthat,althoughUAVs
aretypicallypositionedathigheraltitudesthanCCTVs,therearemorecaseswhereUAVsgetcloser
toreal-timeeventsandhumaninstances,differentfromthefixedCCTVs.
Fig7showsthedistributionofhumanheightwithrespecttogenderandage. Asmentionedinthe
mainmanuscript,eachdistributionisformedasbeingbell-shaped.Wecreate456virtualcharactersby
controllinghumanheight,gender,andageaccordingtothesedistributionsanduniquelydiversifying
otherfactors(skincolor,obesity,hair,outfit,etc)asmuchaspossible,asshowninFig5.
C ImplementationDetails
C.1 Motionevolutiongraph
Fig8showsmotionevolutiongraphsusedindesigningthegamescenariosfortheSynPlaydataset.
Evenwithinthesamegame,thescenariomaychange,butthemotionevolutiongraphwillremain
consistent. Itisnoteworthytomentionthat, despitethewiderangeofsituationsandavarietyof
motionsinvolvedinthegames, themotionevolutiongraphforeachgameconsistsofonlyafew
10Table 7: Comparison of human datasets. ‘#inst/img’ is acquired only on images that contain
human. ‘#motion’indicatesthenumberofuniquemotionsdepictedinthedataset,excepttheones
withthesubscript‘pose’whichindicatethenumberofstaticposes. Sinceasinglemotioncanconsist
ofmultiplenumberofuniqueposes,#motionisgenerallysmallerthanthenumberofposes. For
certaindatasets,thetestsetwithoutavailablelabelsisexcludedfromthiscomparison. ‘uncountable’
indicatesthatthenumberofhumanmotionsincludedinthesetiscountless/uncountable.
dataset domain #inst #img #inst/img naturalmotion #motion viewpoint
ground-view
VOC12[17] real 10K 11.5K 2.48 daily uncountable near
KITTI[18] real 4.5K 7.5K 2.52 daily 2 near
COCODev17[29] real 649K 164K 9.72 daily uncountable near
MPIIHumanPose[3] real 40K 24.9K 1.61 daily 20 near
Cityscapes[12] real 21.4K 5K 7.85 daily 2 near
ADE20K[53] real 30K 27.5K 4.36 daily uncountable near
Human-Art[26] real 123K 50K 2.46 art uncountable near
GTA5[39] synth 1.4M 1.4M 1 ✗ 20K near
pose
SURREAL[50] synth 6.5M 6.5M 1 detail+mocap 23 near
SOMAset[4] synth 100K 100K 1 detail+mocap 250 near
pose
PersonX[48] synth 273K 273K 1 ✗ 4 near
pose
UnrealPerson[52] synth 120K 120K 1 ✗ 2 near
AGORA[36] synth · 19K 1∼15 detail+mocap 4,240 near
pose
BEDLAM[6] synth · 380K 1∼10 detail+mocap 2,311 near
pose
aerial-view
Okutama-action[5] real · 77K ∼9 detail 12 med
SemanticDrone[1] real 1.5K 400 4.16 daily unspecified med
UAVid[31] real 4.7K 420 20.06 daily unspecified med∼far
VisDrone[54] real 109K 40.0K 15.42 daily unspecified med
Archangel-real[45] real 165.6K 41.4K 4 detail 3 near∼far
pose
Archangel-mannequin[45] real · 178.8K 6∼7 detail 3 near∼far
pose
Archangel-synth[45] synth 4.4M 4.4M 1 ✗ 3 near∼far
pose
SynDrone[40] synth 803K 72K 11.15 ✗ 2 med∼far
CARGO[51] synth 108K 108K 1 ✗ 2 near∼far
SynPlay synth 6.5M 73K 88.40 rule+mocap uncountable near∼far
*naturalmotion
·daily:humanmotionsengagedindailyactivity
·art:humanmotionsshowninworksofart
·detail:humanmotionscapturedby‘detail-guideddesign’
·rule:humanmotionscapturedby‘rule-guideddesign’
·+mocap:humanmotionscapturedusingamotionscanner
motionnodesandtheirtransitions. Giventhateachnode(representedasanelementarymotionstate
inthemainmanuscript)encompassesarangeofmotions,thisillustratestheessenceofarule-based
designapproachwhereonlybasicgamerulesareprovidedtofreelyallowthediversearrayofhuman
motionstobemanifested.
C.2 Experimentalsetting
In our experiments, our goal is to explore the capabilities of SynPlay as supplementary training
data on a variety of tasks. We mostly adhere to the original settings and implementations of the
methodsusedinourexperiments,withminimalmodifications. Thespecificmodificationsusedinour
experimentsaredescribedbelow.
Architecturemodification. Ourtasks,specificallyhumandetectionandsemanticsegmentation,can
beviewedasone-classproblems. Therefore,allmethodarchitectures,particularlythedimensionsof
thelastlayer,havebeenadjustedaccordingly.
ImagesizeappliedinYOLOv8training/inference. Weusetheimagesizeof1280×1280forall
datasetsexceptforCOCO,whichusesanimagesizeof640×640. Thisdecisionsimplytakesinto
accounttheoriginalimagesizeofthedatasets.Evenafterrescaling,thesizerangeofhumaninstances
11Figure5: 456virtualplayersinSynPlaycreatedusingCharacterCreator[47].
bbox size distributions distribution w.r.t. gender, age, height
1M UAV low-alt Male, child
UAV med-alt Male, middle-aged
UAV high-alt Male, elderly
CCTV front Female, child
CCTV side 40 Female, middle-aged
CCTV back Female, elderly
UGV
10K
30
100 20
0
10,000 40,000 70,000 100,000 140m 150m 160m 170m 180m 190m
bbox size height
Figure6:Boundingboxsizedistributionfor Figure7: Characterheightdistributionthat
eachimage-capturingdevice. variesaccordingtogenderandage.
inthecompareddatasetsremainssimilar. Whenusingothermodels,i.e.,Mask2Formerinsemantic
segmentationtasksandretinaNetindata-scarcetasks,theimagesize/scalingrecommendedinthe
originalsettingswasused.
TrainingMask2Formerwithoutthelarge-scalejittering(LSJ)augmentation[19]. Wedidnot
use the default LSJ augmentation in training the Mask2Former segmentation models solely for
performancereasons. Inallcases,segmentationaccuracywerefoundtobesignificantlylowerwhen
LSJaugmentationwasused. LSJaugmentation,whichgreatlyexpandstherangeofimagescaling,
maynotbesuitableforaerial-viewdetection,whichmainlyincludessmall-sizedhumaninstances.
ThisperformancedegradationwithLSJaugmentationisalsoobservedin[22],whichisareputable
literatureinthefieldofself-supervisedlearning.
SettingsforPT-FT.WhenusingPT-FTinthegeneraltasks,trainingspecifications,includingtraining
epochsandlearningrate,didnotdifferbetweenpre-trainingandfine-tuning. Indata-scarcetasks,we
followallthesettingsof[43]asoutlinedinPTL,whileleavingouttheprogressivecomponent.
Settingsfordata-scarcetasks. Forallexperimentsperformedfordata-scarcetasksincludingthe
scalingbehaviorstudy,wefollowallthesettingsandexperimentalenvironmentsof[44].
C.3 Quantitativemeasures
Weprovidethedetailonhowwecalculatethetwometricsusedforthequantitativeanalysisinthe
mainmanuscript.
FréchetInceptionDistance(FID)[23]. WeutilizedthePyTorchimplementationofFIDin[42]
withthedefaultsetuptoassessthefidelityanddiversityforallthetrainingdatasetsinvolvedinour
12
)elacs
gol(
yalPnyS
ni
tnuoc
yalPnyS
ni
tnuocred light, green light sugar candy tug-of-war marbles
eennttrryy ssttaanndd mmoo vvee wwaa llkk --ss tttt ooaa -- nn ssdd iitt ssii tt wwaa llkk
aapppprroo aacchh
mmoovvee
wwii nn llooss ee eennttrryy ssttaanndd ppll aayy eennttrryy ssttaanndd ppaauu ssee
ggrraa bb tthhrroo ww
ssuuggaagg rr ee cctt aa nnddyy wwii nn llooss ee eennttrryy ssttaanndd ppuu llll wwii nn llooss ee
wwii nn llooss ee
ssaa ttaann ttyy ee --ss tttt ooaa --nn ssdd iitt ss iitt
wwii nn llooss ee
stepping stones Squid (attacker) Squid (defender)
ssii tt &&ssttSS aallii gg pp ggeerr ffaakk ee
--ss tttt ooaa --nn ssdd iitt wwaa llkk eennttrryy ppaauussee eennttrryy ssttaanndd ffoollllooww
jjuumm pp
eennttrryy ssttaanndd wwaa iitt eemmbbaa rraassss oo jjnn uuee mm--ll ppeegg ttww jjuuoo mm -- ll ppeegg jjuumm pp wwii nn llooss ee
llaann dd
wwii nn llooss ee
wwii nn
ggrraabb ggrraabb
((iinntteerraa ccttiioonn)) ((iinntteerraa ccttiioonn))
ssaa ttaann ttyy
ee
((iinntteepp rraauu ccllll
tt iioonn))
eexx iitt ssaa ttaann ttyy
ee
((iinntteepp rraauu ccllll
tt iioonn))
eexx iitt
ppuusshh ppuusshh
((iinntteerraa ccttiioonn)) ((iinntteerraa ccttiioonn))
llooss ee wwii nn
Figure8: Motionevolutiongraphs. Thestartnode(‘entry’)andtheendnodes(‘win’or‘lose’)are
indicatedbygreenandredcircles,respectively. Forthegameswheresecondarygraphsareavailable
(i.e.,sugarcandyorsquid),atanygiventime(exceptatthestartorendnode),thecurrentstateinthe
maingraphcanmovetothe‘anystate’node(blue-filledcircle)inthesecondarygraph. Whenthe
‘end’node(red-borderedcircle)isreachedwithinthesecondarygraph,thecurrentstatemovesits
waybacktothelatestnodethatwastouchedinthemaingraphbeforeenteringthesecondarygraph.
experiments. Wedidnotperformimagescalingontheinputforanydataset,andthefinalaverage
poolingfeatureswereusedtocomputeFID.
Proportionofnadir-viewinstances. Aninstancewithanelevationanglegreaterthan71.57◦relative
totheUAVasisconsideredtobeanadir-viewinstance,representingthemaximumelevationanglefor
Archangel*[44].Toidentifynadir-viewinstancesforArchangel,weutilizedthedatasetmetadata,i.e.,
UAVposition. Similarly,forArchangel*,wedeterminedifaninstancewasannadir-viewinstance
basedonthesourceinstance,alsousingthedatasetmetadata. InthecaseofSynPlay,wecomputed
theelevationangleforeachinstanceusingtheabsolute3DcoordinatesoftheinstanceandtheUAV
providedbySynPlay.
D QualitativeAnalysis
D.1 Blendingandanimationlayer
Fig9and10showseveralexamplesoftheblendingprocessandhowtheanimationlayersarelever-
aged: thetwotechniquesforexpandinghumanmotionswithinthevirtualenvironments,respectively.
Interestingly, themotionscreatedbyblendingislargelydifferentfromtheircorrespondinginput
motions,whilethemotionscreatedvialeveragingtheanimationlayersstillexhibittheappearances
13crawling running
crippling dragging
walkingwhilebending runningofexhaustedperson
Figure9: Twomotionblendingexamples. Foreachexample(leftorrightcolumn),thetwomotions
(topandmiddlerow)isblendedtogethertogenerateanewmotion(bottomrow). Theblendingratio
betweenthetwoinputmotionscanbecontrolled. Thenamesforthemotionsarenotcomputationally
involvedintheblendingprocess.
anddynamicsresemblingboththeinputmotions. Thesetwotechniquesarereadilyavailableforuse
withintheUnityenvironment.
D.2 Virtualmotionandreal-worldmotion
Fig 11 shows several examples of real-world motions. Real-world motions are created either by
havingtherealhumanwearingthemotioncapturedevicemimicthepre-providedreferencemotions
or by demonstrating potential in-game motions under the given game rules. It is observed that
real-worldmotionscanexpressawiderrangeofspecificactionswhilemaintainingasenseofrealism.
Moreover,motionsthataredifficulttopinpointordescribecanalsobecreated,e.g.,multi-person
wrestlingmotions.
D.3 SynPlaysampleimages
Fig12includesadditionalsampleimagesfromtheSynPlaydataset. Varioushumanappearances
dependingonhumanmotiondifferentlytakenaccordingtothegamescenario,andcameraviewpoints
areobserved. Varioushumanappearancesareobservedthatchangedependingonhumanmotions
taken differently according to the game scenario, and different camera viewpoints. In addition,
variouscharactersandbackgroundsusedforcreatingSynPlayarealsovisible.
References
[1] Aerialsemanticsegmentationdronedataset.http://dronedataset.icg.tugraz.at
[2] Adobe:Mixamo,https://www.mixamo.com/#/
[3] Andriluka,M.,Pishchulin,L.,Gehler,P.,Schiele,B.:2Dhumanposeestimation:Newbenchmarkand
stateoftheartanalysis.In:Proc.CVPR(2014)
[4] Barbosa,I.B.,Cristani,M.,Caputo,B.,Rognhaugen,A.,Theoharis,T.: Lookingbeyondappearances:
SynthetictrainingdatafordeepCNNsinre-identification.Comput.Vis.ImageUnderst.167,50–62(Feb
2018)
14sitting walking kneelingdown
raisingsomethingup
throwingsomething cheering
andlookingatit
sitting,raisingsomethingup throwingsomething cheering
andlookingatit whilewalking whilekneelingdown
Figure10: Threeexamplesofleveraginganimationlayers. Foreachexample(left,middle,or
right),theresultingmotionofleveragingtheanimationlayersovertwoinputmotions(topandmiddle
rows)isshowninthebottomrow. Notethatthesemanticlabels(e.g.,walking,cheering)werenot
providedatthetimeofcapture;theyareincludedinthefigureonlyfortheconvenienceofthereaders.
[5] Barekatain,M.,Martí,M.,Shih,H.F.,Murray,S.,Nakayama,K.,Matsuo,Y.,Prendinger,H.:Okutama-
action:Anaerialviewvideodatasetforconcurrenthumanactiondetection.In:Proc.CVPRWorkshop
(2017)
[6] Black,M.J.,Patel,P.,Tesch,J.,Yang,J.: BEDLAM:Asyntheticdatasetofbodiesexhibitingdetailed
lifelikeanimatedmotion.In:Proc.CVPR(2023)
[7] BlenderInstitute:Blender,https://www.blender.org
[8] Caron,M.,Misra,I.,Mairal,J.,Goyal,P.,Bojanowski,P.,Joulin,A.: Unsupervisedlearningofvisual
featuresbycontrastingclusterassignments.In:Proc.NeurIPS(2020)
[9] Chen,T.,Kornblith,S.,Norouzi,M.,Hinton,G.:Asimpleframeworkforcontrastivelearningofvisual
representations.In:Proc.ICML(2020)
[10] Cheng,B.,Misra,I.,Schwing,A.G.,Kirillov,A.,Girdhar,R.: Masked-attentionmasktransformerfor
universalimagesegmentation.In:Proc.CVPR(2022)
[11] Cioppa,A.,Giancola,S.,Deli‘ege,A.,Kang,L.,Zhou,X.,Cheng,Z.,Ghanem,B.,Droogenbroeck,M.V.:
SoccerNet-Tracking:Multipleobjecttrackingdatasetandbenchmarkinsoccervideos.In:Proc.CVPRW
(2022)
[12] Cordts,M.,Omran,M.,Ramos,S.,Rehfeld,T.,Enzweiler,M.,adUweFranke,R.B.,Roth,S.,Schiele,B.:
Thecityscapesdatasetforsemanticurbansceneunderstanding.In:Proc.CVPR(2016)
[13] Cui,Y.,Zeng,C.,Zhao,X.,Yang,Y.,Wu,G.,Wang,L.:SportsMOT:Alargemulti-objecttrackingdataset
inmultiplesportsscenes.In:Proc.ICCV(2023)
[14] Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:ImageNet:Alarge-scalehierarchicalimage
database.In:Proc.CVPR(2009)
[15] Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,
Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16×16 words:
Transformersforimagerecognitionatscale.In:Proc.ICLR(2021)
15[16] EpicGames:Unrealengine,https://www.unrealengine.com
[17] Everingham,M.,Eslami,S.M.A.,Gool,L.V.,Williams,C.K.I.,Winn,J.,Zisserman,A.:ThePASCAL
visualobjectclasseschallenge:Aretrospective.Int.J.Comput.Vis.111(1),98–136(Jan2015)
[18] Geiger,A.,Lenz,P.,Urtasun,R.: Arewereadyforautonomousdriving? theKITTIvisionbenchmark
suite.In:Proc.CVPR(2012)
[19] Ghiasi,G.,Cui,Y.,Srinivas,A.,Qian,R.,Lin,T.Y.,Cubuk,E.D.,Le,Q.V.,Zoph,B.:Simplecopy-pasteis
astrongdataaugmentationmethodforinstancesegmentation.In:Proc.CVPR(2021)
[20] Girshick,R.,Donahue,J.,Darrell,T.,Malik,J.:Region-basedconvolutionalnetworksforaccurateobject
detectionandsegmentation.IEEETrans.PatternAnal.Mach.Intell.38(1),142–158(Jan2016)
[21] Grill,J.B.,Strub,F.,Altché,F.,Tallec,C.,Richemond,P.H.,Buchatskaya,E.,Doersch,C.,Pires,B.A.,
Guo,Z.D.,Azar,M.G.,Piot,B.,Kavukcuoglu,K.,Munos,R.,Valko,M.:Bootstrapyourownlatent:A
newapproachtoself-supervisedlearning.In:Proc.NeurIPS(2020)
[22] He,K.,Chen,X.,Xie,S.,Li,Y.,Dollár,P.,Girshick,R.:Maskedautoencodersarescalablevisionlearners.
In:Proc.CVPR(2022)
[23] Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:GANstrainedbyatwotime-scale
updateruleconvergetoalocalnashequilibrium.In:Proc.NeurIPS(2017)
[24] Hwang, D.h.(Writer and Director).: Squid game (2021), https://www.netflix.com/title/
81040344?source=35
[25] Jocher, G., Chaurasia, A., Qiu, J.: UltralyticsYOLO(2023), https://github.com/ultralytics/
ultralytics
[26] Ju,X.,Zeng,A.,Wang,J.,Xu,Q.,Zhang,L.: Human-Art: Aversatilehuman-centricdatasetbridging
naturalandartificialscenes.In:Proc.CVPR(2023)
[27] Lee,H.,Eum,S.,Kwon,H.:MER-CNN:Multi-expertR-CNNforobjectdetection.IEEETrans.Image
Process.29,1030–1044(Sep2019)
[28] Lin,T.Y.,Goyal,P.,Girshick,R.,He,K.,Dollár,P.:Focallossfordenseobjectdetection.In:Proc.ICCV
(2017)
[29] Lin,T.Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ramanan,D.,Dollár,P.,Zitnick,C.: Microsoft
COCO:Commonobjectsincontext.In:Proc.ECCV(2014)
[30] Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,Guo,B.: Swintransformer: Hierarchical
visiontransformerusingshiftedwindows.In:Proc.ICCV(2021)
[31] Lyu,Y.,Vosselman,G.,Xia,G.S.,Yilmaz,A.,Yang,M.Y.:UAVid:Asemanticsegmentationdatasetfor
uavimagery.ISPRSJ.PhotogrammRemoteSens.(P&RS)165,108–119(Jul2020)
[32] Mahajan,D.,Girshick,R.,Ramanathan,V.,He,K.,Paluri,M.,Li,Y.,Bharambe,A.,vanderMaaten,L.:
Exploringthelimitsofweaklysupervisedpretraining.In:Proc.ECCV(2018)
[33] Mahmood,N.,Ghorbani,N.,Troje,N.F.,Pons-Moll,G.,Black,M.J.:AMASS:Archiveofmotioncapture
assurfaceshapes.In:Proc.ICCV(2019)
[34] MakeHumanCommunity: MakeHuman,https://static.makehumancommunity.org/makehuman.
html
[35] Nigam,I.,Huang,C.,Ramanan,D.:Ensembleknowledgetransferforsemanticsegmentation.In:Proc.
WACV(2018)
[36] Patel, P., Huang, C.H.P., Tesch, J., Hoffmann, D.T., Tripathi, S., Black, M.J.: AGORA: Avatars in
geographyoptimizedforregressionanalysis.In:Proc.CVPR(2021)
[37] Pavlakos,G.,Choutas,V.,Ghorbani,N.,Bolkart,T.,Osman,A.A.A.,Tzionas,D.,Black,M.J.:Expressive
bodycapture:3Dhands,face,andbodyfromasingleimage.In:Proc.CVPR(2019)
[38] Ren,S.,He,K.,Girshick,R.,Sun,J.: FasterR-CNN:Towardsreal-timeobjectdetectionwithregion
proposalnetworks.IEEETrans.PatternAnal.Mach.Intell.39(6),1137–1149(Jun2016)
16[39] Richter,S.R.,Vineet,V.,Roth,S.,Koltun,V.:Playingfordata:Groundtruthfromcomputergames.In:
Proc.ECCV(2016)
[40] Rizzoli, G., Barbato, F., Caligiuri, M., Zanuttigh, P.: Syndrone-multi-modal UAV dataset for urban
scenarios.In:Proc.ICCVWorkshop(2023)
[41] Rodriguez,M.D.,Ahmed,J.,Shah,M.:ActionMACH:Aspatio-temporalmaximumaveragecorrelation
heightfilterforactionrecognition.In:Proc.CVPR(2008)
[42] Seitzer,M.:pytorch-fid:FIDScoreforPyTorch.https://github.com/mseitzer/pytorch-fid(Au-
gust2020),version0.3.0
[43] Shen,Y.T.,Lee,H.,Kwon,H.,Bhattacharrya,S.S.: Progressivetransformationlearningforleveraging
virtualimagesintraining.In:Proc.CVPR(2023)
[44] Shen, Y.T., Lee, H., Kwon, H., Bhattacharyya, S.S.: Diversifying human pose in synthetic data for
aerial-viewhumandetection.arXiv:2405.15939(2024),https://arxiv.org/abs/2405.15939
[45] Shen,Y.T.,Lee,Y.,Kwon,H.,Conover,D.M.,Bhattacharyya,S.S.,Vale,N.,Gray,J.D.,Leongs,G.J.,
Evensen,K.,Skirlo,F.:Archangel:AhybridUAV-basedhumandetectionbenchmarkwithpositionand
posemetadata.IEEEAccess11,80958–80972(2023)
[46] Stathopoulos,A.,Han,L.,Metaxas,D.:Score-guideddiffusionfor3Dhumanrecovery.In:Proc.CVPR
(2024)
[47] StudioChacre:Thecharactercreator,https://charactercreator.org
[48] Sun,X.,Zheng,L.:Dissectingpersonre-identificationfromtheviewpointofviewpoint.In:Proc.CVPR
(2019)
[49] UnityTechnologies:Unity,https://unity.com/
[50] Varol,G.,Romero,J.,Martin,X.,Mahmood,N.,Black,M.J.,Laptev,I.,Schmid,C.: Learningfrom
synthetichumans.In:Proc.CVPR(2017)
[51] Zhang,Q.,Wang,L.,Patel,V.M.,Xie,X.,Lai,J.:View-decoupledtransformerforpersonre-identification
underaerial-groundcameranetwork.In:Proc.CVPR(2024)
[52] Zhang,T.,Xie,L.,Wei,L.,Zhuang,Z.,Zhang,Y.,Li,B.,Tian,Q.:UnrealPerson:Anadaptivepipeline
towardscostlesspersonre-identification.In:Proc.CVPR(2021)
[53] Zhou,B.,Zhao,H.,Puig,X.,Fidler,S.,Barriuso,A.,Torralba,A.:SceneparsingthroughADE20Kdataset.
In:Proc.CVPR(2017)
[54] Zhu,P.,Wen,L.,Du,D.,Bian,X.,Fan,H.,Hu,Q.,Ling,H.:Detectionandtrackingmeetdroneschallenge.
IEEETrans.PatternAnal.Mach.Intell.44(11),7380–7399(Nov2022)
17(a)Usingvirtualmotionsasreference
Virtualmotion Real-worldmotion
jumping
slipping
(b)Withoutreference
dodging
wrestling(multi-playermotion)
Figure11: Real-worldmotionexamples. Real-worldmotionsareacquiredeither(a)bymimicking
referencemotionsor(b)byexhibitingpotentialin-gamemotionswithoutanyreferencethatalign
withthegivengamerules.Wearablemotionscannersareusedforallthecases.Notethatthesemantic
labels(e.g.,jumping,dodging)werenotprovidedatthetimeofcapture; theyareincludedinthe
figureonlyfortheconvenienceofthereaders.
18(1)redlight,greenlight
UGV UAVmed-alt UAVlow-alt
CCTVback UAVhigh-alt CCTVside
(2)sugarcandy
CCTVside UAVhigh-alt UGV
UAVlow-alt CCTVfront CCTVback
(3)tug-of-war
UAVlow-alt CCTVfront UAVhigh-alt
CCTVback UAVlow-alt UGV
19(4)marbles
UGV CCTVfront UAVmed-alt
CCTVside UAVmed-alt UAVlow-alt
(5)steppingstones
CCTVfront UAVmed-alt UAVhigh-alt
UGV UAVmed-alt CCTVside
(6)squid
UGV CCTVfront UAVhigh-alt
CCTVside UGV UAVlow-alt
Figure12: MoreexampleimagesfromSynPlayareshownforallsixKoreantraditionalgames,
eachwithvariouscameraviewpoints.
20