On Quasi-Localized Dual Pairs in Reproducing Kernel Hilbert
Spaces
Helmut Harbrecht1, Ru¨diger Kempf2, and Michael Multerer3
1 Departement Mathematik und Informatik, Universit¨at Basel, Spiegelgasse 1, 4051 Basel, Switzerland
2Applied and Numerical Analysis, Department of Mathematics, University of Bayreuth, 95440
Bayreuth, Germany
3 Istituto Eulero, Universit`a della Svizzera italiana, Via la Santa 1, 6962 Lugano, Switzerland
August 22, 2024
Abstract
In scattered data approximation, the span of a finite number of translates of a chosen
radial basis function is used as approximation space and the basis of translates is used for
representing the approximate. However, this natural choice is by no means mandatory and
differentchoices,like,forexample,theLagrangebasis,arepossibleandmightofferadditional
features. In this article, we discuss different alternatives together with their canonical duals.
We study a localized version of the Lagrange basis, localized orthogonal bases, such as the
Newton basis, and multiresolution versions thereof, constructed by means of samplets. We
argue that the choice of orthogonal bases is particularly useful as they lead to symmetric
preconditioners. All bases under consideration are compared numerically to illustrate their
feasibilityforscattereddataapproximation.Weprovidebenchmarkexperimentsintwospa-
tialdimensionsandconsiderthereconstructionofanimplicitsurfaceasarelevantapplication
from computer graphics.
1 Introduction
Kernelmethodshavebecomemoreandmorepopularovertheyears.Applicationsrangefrom
scattered data approximation to machine learning, see for example [11, 43, 46] and the ref-
erencestherein.Ingeneral,thesemethodsaimatthereconstructionofanunknownfunction
byonlyusingscattereddata,i.e.,tuplesofdatasitesandcorrespondingmeasurementsofthe
function. Assuming that the data generating process f is contained in a reproducing kernel
(cid:8)(cid:0) (cid:1)(cid:9)
Hilbert space H, we can always find a set of dual pairs ϕ i,ϕ(cid:101)i such that the orthogonal
projection s of f in span{ϕ } can be written as
f i
N N
(cid:88)(cid:10) (cid:11) (cid:88)
s
f
= f,ϕ(cid:101)i Hϕ
i
or, equivalently, s
f
= ⟨f,ϕ i⟩ Hϕ(cid:101)i.
i=1 i=1
Givenasetofdatasites{x ,...,x },themostcommonlyknowndualpairis{(K(·,x ),χ )}.
1 N i i
Herein,{K(·,x )},isthebasisofkerneltranslatesinducedbythereproducingkernelK ofH
i
and χ is the corresponding Lagrange basis. This specific pair allows the easy representation
i
of the orthogonal projection by the interpolant
N
(cid:88)
s = f(x )χ .
f i i
i=1
Thisrepresentationhastheadvantagethatitseparatesthedatafromthedatasites.However,
evenifwedonotconsiderthecosttocomputetheLagrangebasis{χ },everyevaluationofthe
i
interpolants entailsacostofO(N2),sinceeverybasiselementχ isalinearcombinationof
f i
1
4202
guA
12
]AN.htam[
1v98311.8042:viXratheN kerneltranslatesthatneedtobeevaluated.Numerically,thisisnotfeasible.Therefore,
a framework was established in a series of articles [20, 12, 19] in order to compute localized
approximate Lagrange functions for very specific classes of kernels.
In the present article, we give an interpretation of the localization of the Lagrange ba-
sis by considering it as an approximation/sparsification of the inverse of the kernel matrix
A = [K(x ,x )] by means of a very specific pattern, which we call footprint. This approxi-
i j
mateinverseistypicallynotsymmetricalthoughthekernelmatrixitselfis.Hence,wecannot
use it as a preconditioner in a conjugate gradient method. To address this issue, we discuss
modifications of the approach by involving the matrix square root of the footprints or their
Cholesky decomposition. In the limit, for growing footprint size, both approaches yield or-
thonormal basis functions, which means that ϕ˜ = ϕ for i = 1,...,N. Particularly, the
i i
second approach is a localized version of the Newton basis, see [35]. Either choice may then
be used to construct symmetric and sparse preconditioners for notoriously bad conditioned
kernel matrices, e.g., issuing from the widely used Mat´ern kernels.
A multiresolution approach for the construction of dual pairs, that we consider here, are
samplets[21].Sampletsarediscretesignedmeasuresconstructedsuchthatpolynomialsupto
a degree of our choice vanish. Therefore, the kernel matrix becomes quasi-sparse in samplet
coordinates and can be compressed to a sparse matrix. The resulting matrix pattern is
finger band structured with easy to compute Cholesky decomposition whence a graph based
reorderingstrategyisused[21].Furthermore,ithasbeenshownin[23]thatsampletsestablish
even a sparse arithmetic for kernel matrices, enabling efficient scattered data analysis.
This article is organized as follows. In Section 2, we recall the basics of scattered data
approximation and the setting we are studying. In Section 3, we recall general dual pairs
and the Lagrange basis in particular. Moreover, we discuss their connection to the theory of
pseudo-differentialoperators.Thisgivesanalternativepointofviewtoimportantproperties
of kernel matrices which we want to use in the subsequent section. There, in Section 4, we
consideralternativedualpairs.First,inSection4.1,wegiveareviewofthequasi-localization
of the Lagrange basis by using the exponential decay of the inverse kernel matrix. Then, we
use these ideas to derive a new way to construct preconditioners for kernel matrices in Sec-
tion 4.2. The underlying basis functions are orthonormal, including especially a localized
version of the Newton basis. As an alternative approach, we review the basics of samplets
in Section 5. In Section 6, we compare the different dual pairs under consideration numer-
ically. Extensive numerical tests in two and three spatial dimensions are provided. Finally,
concluding remarks are stated in Section 7.
2 Scattered Data Approximation
LetΩ⊆Rddenoteadomain,andHHilbertspaceoffunctionsmappingΩtoR.IfH⊆C(Ω),
then H is a reproducing kernel Hilbert space (RKHS) and there exists a unique reproducing
kernel K: Ω × Ω → R such that K(·,x) ∈ H for all x ∈ Ω and each element f ∈ H
can be point-wise recovered, i.e., f(x) = ⟨f,K(·,x)⟩ . It is well-known, see, e.g. [46], that
H
reproducing kernels are symmetric and positive semi-definite, i.e., the kernel matrix A =
[K(x ,x )] issymmetricandpositivesemi-definiteforall{x ,...,x }⊆ΩandallN ∈N.
i j i,j 1 N
In this article, we focus on kernels that are induced by radial functions. A function
Φ: Rd →R is called radial, iff there is a function ϕ:[0,∞)→R such that Φ(x)=ϕ(∥x∥ ).
2
IfthefunctionΦispositivedefiniteinthesensethattheinducedkernelK(x,y):=Φ(x−y)
is positive definite, then it gives rise to a reproducing kernel.
One of the most popular of these radial functions is known as Mat´ern kernel or Sobolev
spline Φ :Rd →R, dependent on the smoothness parameter ν >d/2. It is defined by
ν
21−ν
ϕ ν(r)= Γ(ν)rν−d 2K
ν−d
2(r), r ≥0, (1)
where Γ is the Riemannian Gamma function and K is the modified Bessel function of the
β
second kind, see [34] for example. For specific values of the smoothness parameter ν the
representation of ϕ simplifies significantly, we give a selection of specific representations in
ν
Table 1.
2ν ϕ (r) smoothness
ν
1/2 exp(−r) C0
3/2 (1+r)exp(−r) C2
5/2 (3+3r+r2)exp(−r) C4
∞ exp(−r2) C∞
Table 1: Examples for different smoothness parameters ν of the Mat´ern kernel.
It is well-known that the Fourier transform of Φ decays algebraically, i.e., there holds
ν
Φ(cid:99)ν(ω)=(1+∥ω∥2 2)−ν−d 2, ω ∈Rd,
see [46, Theorem 6.13] for example. This means in particular, see, e.g., [46], that Φ is the
ν
reproducing kernel of the Sobolev-Hilbert space Hν−d 2(Rd). Especially, the kernel defines a
pseudo-differential operator on Rd, a fact which we will exploit later on.
Having fixed the kernel of interest, we now describe the problem of function approxima-
tion. We are interested in recovering an unknown function f ∈H, where we are given only a
finite set of data {(x ,f ),...,(x ,f )} ⊂ Ω×R. We collect the abscissae x in a set of
N N N N i
data sites X := {x ,...,x } ⊂ Ω. Associated to this set are two characteristic quantities:
1 N
the fill distance
h := sup min ∥x−x ∥
X,Ω i 2
x∈Ωxi∈X
and the separation radius
1
q := min∥x −x ∥ .
X i j 2
2 i̸=j
For the theoretical results we present later, we require that the set of data sites is quasi-
uniform, i.e., that there is a constant c >0 such that
qu
q ≤h ≤c q .
X X,Ω qu X
An easy comparison of volumes then yields, that there are constants c and c such that
1 2
c 1N− d1 ≤h
X,Ω
≤c 2N− d1 ,
see, e.g., [46, Proposition 14.1]. Clearly, a similar estimate holds for the separation radius,
too.
To recover the unknown function, we look at two approximation methods, interpolation
and, more generally, regularized least squares approximation or kernel ridge regression. Both
can be seen as finding the minimizer of the functional J within the subspace
λ
H :=span{K(·,x ),...,K(·,x )}
X 1 N
spanned by the basis of kernel translates within an RKHS H.
N
(cid:88)
J (s):= |f −s(x )|2+λ∥s∥2 , s∈H ,
λ i i H X
i=1
where λ ≥ 0 is a regularization parameter. If we set λ = 0 we force interpolation, i.e., the
minimizer s :=s∗ of J satisfies s (x )=f , for i=1 ...,N. This should only be chosen if
f 0 0 f i i
we deal with no noise on the data, that is, if it holds f =f(x ). In this case, the minimizer
i i
s coincides with the orthogonal projection of f onto H , since there holds ⟨f−s ,v⟩ =0
f X f H
for any v ∈H by the reproducing property.
X
In any other case, one should use λ > 0. Even so, the numerical procedure is the same,
stated in the next theorem, which is a version of the representer theorem, see, e.g., [43].
3Theorem 2.1. Let Ω ⊆ Rd be a domain. Let K be the reproducing kernel of an RKHS H
on Ω and X ={x ,...,x }⊆Ω be a set of sites. Then, for all λ≥0, there exists a unique
1 N
solution s∗ of minJ (s). In addition, there exists a coefficient vector α∈RN such that
λ λ
N
(cid:88)
s∗ = α K(·,x ),
λ j j
j=1
i.e., s∗ ∈H . Furthermore, α is the unique solution of the linear system
λ X
(A+λI)α=f, (2)
where A = [K(x ,x )] ∈ RN×N is the kernel matrix, I ∈ RN×N is the identity matrix,
i j i,j
and f =[f ]∈RN is the data vector.
i
3 Other Bases for H
X
By definition of the approximation space, H is the linear span of the basis of kernel trans-
X
lates.However,thismightnotbethemostpracticalbasis.Forexample,itiswell-knownthat
there is a Lagrange basis {χ } satisfying χ (x )=δ , i,j =1...,N. The Lagrange basis is
j j i i,j
well-understoodandwerefertothevastamountofavailableliterature,see[20,12,10,46]and
the references therein. Another basis is the Newton basis, which amounts to an orthogonal
basisofH ,compare[35,36].Thegeneralconceptofdual pairs isintroducedinSection3.1.
X
We repeat some important properties in Section 3.2 and give an alternative proof of the
exponential decay property by interpreting the underlying operators as pseudo-differential
operators. This view, although well-established in the theory of partial differential equation,
is new in the context of approximation theory.
3.1 Dual Pairs
(cid:8)(cid:0) (cid:1)(cid:9) (cid:10) (cid:11)
A dual pair ϕ i,ϕ(cid:101)i is a set of pairs of basis functions for H
X
such that ϕ i,ϕ(cid:101)j
H
= δ
i,j
for all i,j =1,...,N. Thus, we immediately obtain the representations
N N
(cid:88)(cid:10) (cid:11) (cid:88)
s
f
= f,ϕ(cid:101)i Hϕ
i
= ⟨f,ϕ i⟩ Hϕ(cid:101)i
i=1 i=1
for the sought kernel interpolant. Its computation can therefore be accelerated if the basis
functions under consideration admit sparse representations.
Obviously, the kernel matrix
A=[K(x ,x )]N =[⟨K(·,x ),K(·,x ⟩ ]N
i j i,j=1 i j H i,j=1
is the Gramian of basis of kernel translates. Hence, any choice of γ ∈R yields a dual pair by
defining
(cid:2) ϕ(cid:101)1,...,ϕ(cid:101)N(cid:3) =k(·)A−1−γ and [ϕ 1,...,ϕ N]=k(·)Aγ,
where we set
k(x)=[K(·,x ),...,K(·,x )].
1 N
Canonical choices are γ = 0, yielding the Lagrange basis, and γ = −1/2, yielding an or-
thogonal basis, where we have ϕ
i
= ϕ(cid:101)i for all i = 1,...,N. Furthermore, we notice that we
have
A−1 =A−1/2A−1/2 =LLT,
where L is the Cholesky factor of A−1. Hence, there exists an isometry such that L =
QA−1/2, which implies that another orthonormal basis of the space H is obtained by the
X
Cholesky decomposition of A−1. It is called Newton basis in literature, since the i-th basis
function has i zeros, compare [35, 36].
43.2 About the Lagrange Basis
We shall first collect some results for the Lagrange basis. Since the basis elements χ are
i
elements of H , we can express them as
X
N
χ =(cid:88) α(i) K(·,x ), i=1,...,N,
i k k
k=1
for certain coefficient vectors α(i) ∈ RN for any i = 1,...,N. There are different ways to
compute these coefficient vectors. We consider two of them.
The first one allows us to compute these vectors a-priori in an offline phase, as soon as
thesetofdatasitesisfixed.TheLagrangeconditionχ (x )=δ directlyleadstothelinear
j i i,j
system
Aα(i) =e
i
forthei-thcoefficientvector,whereA=[K(x ,x )] isagainthekernelmatrixande ∈RN
i j i,j i
is the i-th unit vector.
With the second method we can compute all χ (x), i.e., the Lagrange basis evaluated in
i
a x∈Rd, simultaneously, by solving the linear system
[χ (x),...,χ (x)]A=k(x),
1 N
which has a unique solution for every x∈Rd. This leads to the representation
N
(cid:88)
χ (x)= [A−1] K(x,x ) (3)
i i,j j
j=1
=k(x)A−1e , i=1,...,N. (4)
i
With the Lagrange functions at hand, we can represent the minimizer s of J by
f 0
N
(cid:88)
s (x)= f(x )χ (x). (5)
f i i
i=1
Often, this representation is associated with a quasi-interpolation process, see, e.g., [7], how-
ever, in the context of this paper, we do not expect to have polynomial reproduction, a
property commonly expected of quasi-interpolation.
Motivated by the desire to obtain a similar representation for s∗ with λ > 0, we define
λ
the modified Lagrange basis via
N
(cid:88)(cid:2) (cid:3)−1
χ (x)= A+λI K(x,x ) (6)
λ,i i,j j
j=1
=k(x)(A+λI)−1e , i=1,...,N. (7)
i
Employing the modified Lagrange basis, we can represent s∗ as
λ
N
(cid:88)
s∗(x)= f χ (x). (8)
λ i λ,i
i=1
For the Mat´ern kernel K(x,y) = Φ (x−y), it turns out that the dual basis associated
ν
to the basis of kernel translates, i.e., the Lagrange basis, is highly localized, that is, |χ (x)|
i
decays exponentially fast if ∥x−x ∥ grows. This is a direct consequence of the fact that
i 2
the entries of the inverse of the kernel matrix A exhibit an exponential off-diagonal decay.
These can be quantified as follows.
Corollary 3.1. Let X = {x ,...,x } be a quasi-uniform set of sites with fill distance
1 N
h <h andseparationradiusq .LetΦ betheMat´ernkernelwithsmoothnessparameter
X,Ω 0 X ν
5ν, such that ν−d/2∈N. Then there is a constant C >0 such that the entries of the inverse
of the kernel matrix satisfy the estimate
(cid:18) (cid:19)
(cid:12) (cid:12)(cid:2) A−1(cid:3) (cid:12) (cid:12)≤Cqd−2νexp −η∥x j −x k∥ 2 , (9)
j,k X h
X,Ω
where η = η(ν,d) < 1 is a positive constant. This implies the following decay condition on
the Lagrange basis
|χ
(x)|≤C(cid:18)
h
X,Ω(cid:19)ν−d
2
exp(cid:18)
−µ∥x−x i∥
2(cid:19)
, x∈Rd, for i=1,...,N,
i
q h
X X,Ω
with µ=−h0 log(η).
4
The first proofs of this corollary were given in [12] for Ω = Sd−1, the d−1 dimensional
sphere. In [20], it was refined to the version given here for general bounded domains. In the
next subsection, we offer an alternative proof by considering the kernel in the context of
pseudo-differential operators.
We see in (4) and (7) that the main computational task to evaluate the (modified) La-
grange basis {χ } and {χ }, respectively, is the inversion of the dense, positive definite
i λ,i
matrix A or A+λI, respectively, whose entries decay exponentially apart from the diago-
nal. Hence, the desired fast evaluation of the approximation s∗ can be achieved by finding
λ
efficient ways to approximate the kernel matrix A and its inverse A−1.
3.3 Alternative Point of View: Pseudo-differential Operators
In this subsection, let Ω ⊂ Rd be a smooth domain. Then, the reproducing kernel under
consideration gives rise to a compact integral operator on L2(Ω) according to
(cid:90)
K:L2(Ω)→L2(Ω), u(cid:55)→ K(·,y)u(y)dy. (10)
Ω
For many kernel functions, K constitutes a classical pseudo-differential operator of negative
order s < 0. We refer to e.g. [25, 37, 42, 45] for the details of this theory, including the
subsequent developments.
We are especially interested in pseudo-differential operators K which belong to the sub-
class OPSs of analytic pseudo-differential operators, see [37]. Their kernels are known to
cl,1
be asymptotically smooth, meaning that there holds the decay property
∀α,β ∈Nd : ∃ρ>0:
(|α|+|β|)! (11)
|∂α∂βK(x,y)|≤C , (x,y)∈(Ω×Ω)\∆
x y (cid:0) ρ∥x−y∥s+d(cid:1)|α|+|β|
2
apart from the diagonal ∆ := {(x,y) ∈ Ω×Ω : x = y} for some constant C > 0. This
estimate is the key for the computational treatment of the underlying kernel matrices by
means of hierarchical matrix techniques [15] or wavelet matrix compression [5, 9].
A special feature of pseudo-differential operators is that they form an algebra, which
we can be exploited for kernel matrices. Besides addition, the concatenation of two pseudo-
differentialoperatorsKandK′ essentiallycorrespondstotheproductoftherespectivekernel
matricesAandBundertheassumptionthatthesetofdatasitesisasymptoticallyuniformly
distributedmoduloone.Thismeans,wehaveforeveryRiemannintegrablefunctionf: Ω→
R that
(cid:12) (cid:12)(cid:90) |Ω|(cid:88)N (cid:12)
(cid:12)
(cid:12) f(x)dx− f(x i)(cid:12)→0 as N →∞,
(cid:12) N (cid:12)
Ω i=1
compare [30]. Under this assumption, the concatenation K◦K′ corresponds to the matrix-
matrix product AB. Indeed, the kernel K′′ of the concatenated pseudo-differential operator
satisfies
(cid:90) |Ω|(cid:88)N
K′′(x ,x )= K(x ,z)K′(z,x )dz ≈ K(x ,x )K′(x ,x )=[A] [B] .
i j i j i k k j i,: :,j
N
Ω k=1
6This means that the matrix AB approximates the kernel matrix induced by K′′. As this is
acompressible pseudo-differentialoperator,AB isalsocompressible,compare[23,Theorem
4]. This property can be used to show that the inverse kernel matrix is compressible.
Consider a symmetric and positive definite kernel K such that the associated pseudo-
differential operator satisfies K ∈ OPSs with s < 0. Then, the inverse of K +λI is of
cl,1
the form λ−1I −K′ with K′ being likewise a pseudo-differential operator from OPSs , see
cl,1
[23, Lemma 2]. Especially, the Schwarz kernel K′ which underlies the operator K′ by the
Schwartzkerneltheorem,see,e.g.,[26,Chap.5],issymmetric,positivedefinite,andlikewise
asymptotically smooth. Thus, the inverse of a (regularized) kernel matrix is compressible in
view of the above mentioned link between the matrix-matrix product and the concatenation
of the associated pseudo-differential operators.
Weliketomentionthatthetheoryofpseudo-differentialoperatorsisatheoreticalfounda-
tion for the observations made already earlier. Indeed, it has been observed that the inverse
of a kernel matrix is compressible, compare for example [14, 16] and more recently [2] in
case of H-matrices and [4, 40] for the case of wavelet matrix compression. Especially, in
the case of the Mat´ern kernel, the associated pseudo-differential operator corresponds to a
Bessel potential operator. The symbol of the Mat´ern kernel is (1+∥ξ∥2)d/2−ν, which im-
plies that the inverse pseudo-differential operator has the symbol (1+∥ξ∥2)ν−d/2. In case
of ν = 1/2,3/2,..., this represents a differential operator and hence a local operator. This
explains the exponential decay (9) of the inverse to the kernel matrix in this case, see also
[38]. Note that, the symbol is an analytic function in r = ∥ξ∥ for ν ̸= 1/2,3/2,..., hence
the Schwartz kernel of the inverse kernel matrix still decreases rapidly as r →∞. We should
finally mention that one may weaken the strong assumptions on the kernels under con-
sideration, if the Gevrey extension of the symbolic calculus for classical pseudo-differential
operators is used as developed in [6, 29, 37].
4 Efficient Computation of the Lagrange Basis
WenowturntointroducingaframeworktoefficientlycomputetheLagrangebasis.Asalready
discussed above and quantified in Corollary 3.1, the kernel matrix of the Mat´ern kernel and
more generally kernels of pseudo-differential operators exhibit a fast off-diagonal decay. The
authors of [20, 12, 19] used this fact to derive localizations of the Lagrange basis. First, we
review the basic ideas of their approach and use them afterwards to derive symmetric and
sparse preconditioners for the notoriously ill conditioned kernel matrix.
4.1 Cut-off and Localized Lagrange Functions
InthecaseoftheMat´ernkernel,wecanexploittheexponentialdecayin(9)andeitherignore
coefficientsintheexpansions(3)and(6),respectively,thataresmallerthanagiventhreshold
or directly construct functions that satisfy the Lagrange condition only on a small neigh-
borhood of x ∈ X. The resulting functions are called cut-off Lagrange basis and localized
i
Lagrange basis, respectively. We now go into more detail.
Motivated by (9), we define the footprint X of the i-th Lagrange basis element as
i
X ={x ∈X : ∥x −x ∥ ≤κh |log(h )|}, (12)
i j i j 2 X,Ω X,Ω
with a free constant κ > 0, independent of i. Then, the cut-off Lagrange basis element χco
i
is given as
χco(x)= (cid:88) (cid:2) A−1(cid:3) K(x,x ).
i i,j j
j:xj∈Xi
Analogously, we have
χco(x)= (cid:88) (cid:2) (A+λI)−1(cid:3) K(x,x ), (13)
λ,i i,j j
j:xj∈Xi
for the modified cut-off Lagrange basis. Note that we only changed the number of indices
entering the summation and kept the coefficients in the expansion the same as in the full
Lagrange basis.
7The idea of the localized Lagrange basis is now to enforce the Lagrange condition only
on the footprint, i.e., we define the basis elements χloc and χloc as
i λ,i
χloc := (cid:88) α(i) K(·,x ), and χloc := (cid:88) β(i) K(·,x )
i j j λ,i j j
j:xj∈Xi j:xj∈Xi
wherethecoefficientvectorsα(i),β(i) ∈R|Xi|aretherespectiveuniquesolutionsofthelinear
systems
A| α(i) =e and (A| +λI)β(i) =e (14)
Xi m Xi m
with A| = [K(x ,x )] being the restriction of the kernel matrix A to indices
Xi j ℓ xj,xℓ∈Xi
in X
i
and e
m
∈ R|Xi| being the unit vector whose 1-entry is in the (local) coordinate
m∈{1,...,|X |} that relates to the (global) coordinate i∈{1,...,N}.
i
While the cut-off Lagrange basis still requires to assemble the full kernel matrix we can
achieve faster evaluation of the approximation s∗,co. The localized Lagrange basis, on the
λ
other hand, reduces the computational cost since we have to solve for every i = 1,...,N
a linear system of the size |X |×|X |. Hence, if the number of points in the footprints is
i i
sufficiently small, this is cheaper than the cost to compute the full or cut-off Lagrange basis.
Followingtherepresentationsoftheinterpolantorpenalizedleast-squaresapproximation
in(5)and(8),respectively,wecanusethecut-offandlocalizedLagrangefunctionstoderive
representations of approximations
N N
(cid:88) (cid:88)
sco = f(x )χco and s∗,co = f χco (15)
f i i λ i λ,j
i=1 i=1
or
N N
(cid:88) (cid:88)
sloc = f(x )χloc and s∗,loc = f χloc. (16)
f i i λ i λ,i
i=1 i=1
For the localized Lagrange basis, the following estimate is known, see [18]. Clearly, this
translates immediately into an estimate for the difference s −sloc.
f f
Proposition4.1. WiththenotationandassumptionsofCorollary3.1,wherem:=ν−d/2∈
N, let the Lagrange basis element χ be defined as in (3) and the localized Lagrange basis
i
element χloc be defined as in (13) with footprint (12). Then there exists a constant C > 0
i
such that the error estimate
(cid:13) (cid:13)χ i−χl ioc(cid:13) (cid:13)
Wσ(Rd)
≤Chκ Xη 2 ,Ω+d−2m , i=1,...,N,
p
holds for 1≤p≤∞ and σ <2m−d+ d.
p
We now want to discuss the cost of computing the localized Lagrange basis. Since we
assume that the set of data sites X is quasi-uniform, we can bound the number of points in
the footprint X , independently of i, by
i
(cid:16)κ (cid:17)d
|X |≤C logN (17)
i
d
with a constant C >0, independent of N, if κlogN >1.
d
Hence, we have to solve N dense linear systems of size O(cid:0) (logdN) × (logdN)(cid:1) . This
means, a naive ansatz, i.e., not using any further properties the local matrices exhibit, com-
putes the coefficients for the localized Lagrange basis with cost O(Nlog3dN). However, we
emphasizethatthesystemsareallindependentofeachother,whichmakesitpossibletoeffi-
cientlyparallelizethecomputation.Further,wenotethatweonlyneedtokeepO(NlogdN)
coefficients in memory, compared to the O(N2) coefficients for the full Lagrange basis. This
means, from a different point of view, we find an elegant compression of the inverse of the
kernel matrix A.
84.2 Symmetrized Version for Preconditioning
A drawback of the construction in Section 4.1 is that it is non-symmetric in the following
sense. Collecting the column vectors α(i) and β(i) for all i=1,...,N in a global matrix B
yields a (sparse) matrix that satisfies AB ≈ I or (A+λI)B ≈ I, respectively. Nonethe-
less, there holds B ̸= BT although we have A = AT. This rules out the use of B as
a preconditioner in the conjugate gradient method [24]. Indeed, in [12] for example, only
the generalized minimal residual method [39] has been used which, however, is much more
expensive compared to the conjugate gradient method.
Weshallmodifytheconstructionin(14).ThematrixAissymmetricandpositivedefinite,
so is the footprint matrix A| is for any subset X ⊂X. Therefore, the first possibility we
Xi i
haveistousethematrixsquarerootA|1/2
ofthefootprintmatrix.Then,wesolveincomplete
Xi
analogy to (14) the systems
A|1/2α(i) =e (18)
Xi (cid:101) m
for all i = 1,...,N. Likewise to above, we then form a (sparse) matrix C from all of the
columnvectorsα˜(i),1≤j ≤N,whichsatisfiesA1/2C ≈I andhenceCTA1/2 ≈I.Thus,we
alsoconcludeCTAC ≈I andCCT definesasymmetricandpositivedefinitepreconditioner
for A.
Alternatively,wecanalsousetheCholeskydecompositionA| =L LTofsize|X |×|X |
Xi i i i i
of the kernel matrix restricted to the footprints, solving the systems
LTα˜(i) =e
i m
for all i = 1,...,N. If we then form the (sparse) matrix C from all of the column vectors
α(i) , i = 1,...,N, then there holds LTC ≈ I and hence CTL ≈ I, where LLT = A is
(cid:101)
the Cholesky decomposition of A. Consequently, CTLLTC ≈ I and CCT defines likewise
a symmetric and positive definite preconditioner for A.
In case of the original footprint systems (14) and in case of the matrix square root (18),
theordering ofthe local indices X doesnot affectthe soconstructed globalmatrices B and
i
C,respectively.Thereasonforthisisthatthesolutionvectorsα(i) andα˜(i),respectively,are
then just permuted versions of the same vectors. This is completely different in case of the
Cholesky decomposition. Only in the case of an ordering of the local indices which respects
the ordering of the global ones, C is an upper triangular matrix since each vector α(i) has
(cid:101)
zero entries for all indices being larger than the respective local index m. In this case, we
have hence constructed a localized Newton basis, compare [36].
In complete analogy, we can also address the regularized kernel matrices
(A+λI)|1/2β˜(i)
=e or
LTβ˜(i)
=e ,
Xi m j m
whereL LT istheCholeskydecompositionoftheregularizedkernelmatrixA| +λI.This
i i Xi
yieldsasymmetricandpositivedefinitepreconditionerforA+λI.Weshouldfinallymention
thatthecomputationofthematrixCCT shouldbeavoided,meaningthatthematrix-vector
multiplication z = CCTx is computed in two steps in accordance with y = CTx and
z =Cy.
Inordertogiveavisualideatheofbasisofkerneltranslates,thecorrespondingLagrange
basisandtheorthogonalbasisobtainedfromtheinversesquarerootofthekernelmatrixA,
we consider the Sobolev space H1(R), equipped with the usual norm
∥u∥2 =∥u∥2 +∥u′∥2 .
H1(R) L2(R) L2(R)
ItsreproducingkernelisgivenbyK(x,y)= 1exp(|x−y|),see[46]forinstance.Fig.1shows
2
different basis elements and the corresponding Lagrange basis elements. The bottom row
shows the associated orthonormal basis, each for N =200 equidistant data sites in [−1,1].
5 Dual Pairs in Samplet Coordinates
In Section 4.1, we leveraged the exponential decay of the entries of the inverse kernel matrix
(9)toconstructdualpairsandquasi-interpolationstothegivendataandspeed-uptheevalu-
ationofthese.Inthissection,wepursueamultiresolutionapproachbasedonasampletbasis
91 1 1
ϕ ϕ ϕ
ϕ(cid:101) ϕ(cid:101) ϕ(cid:101)
0.5 0.5 0.5
0 0 0
−1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 −1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 −1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1
j=0 j=1 j=2
0.5 0.5 0.5
ϕ(cid:98) ϕ(cid:98) ϕ(cid:98)
0.25 0.25 0.25
0 0 0
−1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 −1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 −1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1
Figure 1: Basis of kernel translates, localized Lagrange basis for H1(R) (top row). The bottom
row shows the corresponding orthogonal functions obtained with the square root of the inverse
Gramian A−1/2.
embedded into a reproducing kernel Hilbert space. This way, we obtain a sparse representa-
tionofthekernelmatrixandacorrespondingdirectsolver.Weemphasize,however,thatthe
resulting approximations will most likely not satisfy any Lagrange condition. Nevertheless,
we will be able to define quasi-interpolations in the style of (15) or (16). For the reader’s
convenience, we start by briefly recalling the construction of a samplet basis.
5.1 Samplet Basis Construction
The first building block in the samplet construction is a cluster tree for the set X. This is a
treeT withrootX andeachnodeτ ∈T isthedisjointunionofitschildren.Wereferto[21]
for details. The cluster tree T directly induces a support based hierarchical clustering of the
subspaceH′ :=span{δ ,...,δ }⊂H′ spannedbytheDirac-δ-distributionssupportedat
X x1 xN
the data sites in X. With a slight abuse of notation, we will refer to this cluster tree also by
T and to its elements by τ. Based on the multilevel hierarchy generated by the cluster tree,
we shall now construct a multiresolution basis for H′ . We start by introducing a two-scale
X
transform between basis elements on a cluster τ of level j. To this end, we represent scaling
distributions Φτ = {φτ } and samplets Στ = {στ } recursively as linear combinations of
j j,k j j,k
thescalingdistributionsΦτ ofτ’schildclusters.Thisamountstothe refinement relations
j+1
nτ nτ
j+1 j+1
(cid:88) (cid:88)
φτ = qτ φτ and στ = qτ φτ with nτ :=|Φτ |,
j,k j,Φ,ℓ,k j+1,ℓ j,k j,Σ,ℓ,k j+1,ℓ j+1 j+1
ℓ=1 ℓ=1
which may be written in matrix notation as
[Φτ,Στ]=Φτ Qτ =Φτ (cid:2) Qτ ,Qτ (cid:3) . (19)
j j j+1 j j+1 j,Φ j,Σ
To obtain vanishing moments and orthonormality, the transform Qτ is derived from an
j
orthogonal decomposition of the moment matrix Mτ
j+1
∈Rmq×nτ j+1, given by
 (x0,φ j+1,1) Ω ··· (x0,φ j+1,nτ ) Ω
j+1
Mτ := . . . . =[(xα,Φτ ) ] .
j+1  . .  j+1 Ω |α|≤q
(xα,φ j+1,1) Ω ··· (xα,φ j+1,nτ ) Ω
j+1
10Herein, m
=(cid:0)q+d(cid:1)
denotes the dimension of the space P (Ω) of total degree polynomials.
q d q
In the original construction by Tausch and White [44], the matrix Qτ is obtained from
j
the singular value decomposition of Mτ . For the construction of samplets, we follow the
j+1
idea from [1] and rather employ the QR decomposition, which results in samplets with an
increasing number of vanishing moments. We compute
(Mτ )T =QτR=:(cid:2) Qτ ,Qτ (cid:3) R (20)
j+1 j j,Φ j,Σ
The moment matrix for the cluster’s scaling distributions and samplets is now given by
(cid:2) Mτ ,Mτ (cid:3) =(cid:2) (xα,[Φτ,Στ]) (cid:3)
j,Φ j,Σ j j Ω |α|≤q
=(cid:2) (xα,Φτ [Qτ ,Qτ ]) (cid:3) =Mτ [Qτ ,Qτ ]=RT.
j+1 j,Φ j,Σ Ω |α|≤q j+1 j,Φ j,Σ
SinceRT isalowertriangularmatrix,thefirstk−1entriesinitsk-thcolumnarezero.This
amounts to k−1 vanishing moments for the k-th distribution generated by the orthogonal
transform Qτ =[Qτ ,Qτ ]. Defining the first m distributions as scaling distributions and
j j,Φ j,Σ q
theremainingonesassamplets,weobtainsampletswithvanishingmomentsatleastoforder
q+1.
For leaf clusters, we define the scaling distributions by Dirac-δ-distributions supported
atthepointsx ,i.e.,Φτ :={δ :x ∈τ, τ ∈L(T)}.Thescalingdistributionsofallclusters
i xi i
on a specific level j then generate the spaces
X :=span(cid:8) φτ :k ∈IΦ,τ, level(τ)=j(cid:9) . (21)
j j,k j
In contrast, the samplets span the detail spaces
S :=span(cid:8) στ :k ∈IΣ,τ, level(τ)=j(cid:9) . (22)
j j,k j
Combining the scaling distributions of the root cluster with all clusters’ samplets gives rise
to the samplet basis
(cid:91)
Σ:=ΦX ∪ Στ. (23)
τ∈T
5.2 Samplet Representation of the Lagrange Basis
The samplet basis introduced in the previous subsection can be represented according to
N
(cid:88)
σ = [T] δ , k ∈∇ ,j =0,...,J, (24)
j,k (j,k),i xi j
i=1
where ∇ denote appropriate index sets. Here, the matrix T ∈RN×N describes the samplet
j
transform. It is an orthogonal matrix and can be applied with linear cost by means of the
fast samplets transform, see [21, 33]. The key feature is that they have vanishing moments
of order q+1, meaning that
(cid:112) (cid:18) d(cid:19)q+1 diam(τ)q+1
|(f,σ ) |≤ |τ| ∥f∥ (25)
j,k Ω Cq+1(O)
2 (q+1)!
provided that f ∈Cq+1(O), where O ⊂Ω is any open set containing the samplet’s support.
Here, τ denotes the cluster where the samplet is supported and |τ| denotes the number of
data sites contained in the cluster.
The samplet basis (24) gives rise to a multiresolution basis in H by means of the Riesz
X
isometry J: H′ →H according to
N
(cid:88)
ψ :=Jσ = [T] K(·,x ), k ∈∇ , j =0,...,J.
j,k j,k (j,k),i i j
i=1
We call these functions embedded samplets. The associated Lagrange basis is given by
(cid:88)
ψ(cid:101)j,k = [A− Σ1] (j,k),(j′,k′)ψ j′,k′, k ∈∇ j, j =0,...,J.
j′,k′
11Here, the kernel matrix A ∈RN×N has to be taken in samplet coordinates, i.e., we have
Σ
A :=TATT. (26)
Σ
Note that there holds
(cid:10) ⊺ (cid:11)
(ΦU) ,Φ(cid:101)U =I
H
(cid:0) (cid:1)
for any orthogonal matrix U and any dual pair Φ,Φ(cid:101) . In particular, since the samplet
transform T is orthogonal, the basis {ψ } corresponds to the samplet transformed basis of
j,k
(cid:8) (cid:9)
kernel translates, while ψ(cid:101)j,k is the samplet transformed Lagrange basis.
2.5 0.5 0.5
ψ ψ ψ
ψ(cid:101) ψ(cid:101) ψ(cid:101)
0 0
0
−1 −0.5 −0.5
−1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 −1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 −1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1
j=0 j=1 j=2
2.5 0.5 0.5
ψ(cid:98) ψ(cid:98) ψ(cid:98)
0 0
0
−1 −0.5 −0.5
−1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 −1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1 −1−0.8−0.6−0.4−0.2 0 0.2 0.4 0.6 0.8 1
j=0 j=1 j=2
Figure 2: H1(R)-embedded primal and dual scaling distribution (top left), samplet on level
j = 1 (top middle) and samplet on level j = 2 (top right) for N = 200 equidistant data sites and
q +1 = 3 vanishing moments. The bottom row shows the corresponding orthogonal samplets
−1/2
obtained with the square root of the inverse kernel matrix A .
Σ
Similar to the single scale bases, we want to give a visual idea of the embedded samplet
basis,theirdualbasisandthecorrespondingorthogonalbasis.WeagainconsidertheSobolev
spaceH1(R).ThetoprowofFig.2showsanembeddedscalingdistribution(leftplot)andtwo
embedded samplets (middle and right plots) with q+1=3 vanishing moments, constructed
forN =200equidistantdatasites.Thebottomrowshowsthecorrespondingelementsofthe
orthogonal basis obtained from the inverse square root of the inverse kernel matrix
A−1/2
.
Σ
Different from the single scale case, the orthogonal basis remains a local basis in samplet
coordinates, cp. Fig. 1.
5.3 Kernel Matrix Compression and Associated Algebra
Basedontheasymptoticsmoothness(11)ofthekernelfunctionunderconsideration,theker-
nelmatrixAfrom(26)canbecompressedinsampletcoordinates,suchthatonlyO(NlogN)
matrix entries remain, see [21]. The resulting pattern is found in the left plot of Fig. 3 for
uniformrandompointsontheunitsquare.AccordingtoSection3.3,weknowthattheinverse
kernel matrix is also compressible in samplet coordinates, using the same matrix pattern.
Therefore, it remains to compute the inverse in an efficient way. One approach would be
the use the Newton-Schulz iteration to compute an approximate matrix inverse, see [27, 41]
for details. However, it turns out that selected inversion is much more efficient in terms of
computing times, compare [23]. This issues from the fact that the samplet compressed ker-
nel matrix can be reordered by means of nested selection [13, 32] such that the associated
12d = 1
bins: 512, n: 131072, binsize: 256, nnz: 398824 bins: 512, n: 131072, binsize: 256, nnz: 398824 bins: 512, n: 131072, binsize: 256, nnz: 321572
d = 2
bins: 512, n: 131072, binsize: 256, nnz: 21639824 bins: 512, n: 131072, binsize: 256, nnz: 21639824 bins: 512, n: 131072, binsize: 256, nnz: 79421653
d = 3
Figure 3:binsT: 51y2, np: 1i3c107a2,l binmsize:a 25t6,r nnizx: 171p582a59t8tern of tbhinse: 512s, na: 1m3107p2, blienstize: c256o, nmnz: 1p71r58e25s98sed kernbeinls: 5m12, na: 1t31r07i2x, binsfizoe: r256,d nnz:= 1182126982(6left), its
reordering by means of nested dissection (middle), and the associated Cholesky factor (right).
Cholesky factors have low fill-in. The reordered kernel matrix is found in the middle plot of
Fig. 3 while the associated Cholesky factor is found in the right plot. In the plots, lighter
blocks correspond to less entries. Selected inversion computes the inverse then based on this
rather sparse Cholesky factor, see [31]. The application of the samplet compressed, inverse
kernel matrix to a data vector f is computable in complexity O(NlogN), independent of
the data vector being given in the samplet basis or – in view of the fast samplet transform
– as point evaluations.
We finally like to mention that the rapid computation of the matrix power Aγ for γ ∈R
is also possible by means of its contour integral representation [17]. We refer the reader to
[23] for all the details and numerical results.
6 Numerical Tests
We now turn to testing the methods introduced above. Although the motivation to study
the construction of the localized Lagrange and the samplet basis was to obtain dual pairs
that allow faster evaluation of the quasi-interpolants, both methods can be interpreted as
compression schemes of the (inverse) kernel matrix. The compression power then can be
directly translated into the approximation error of the interpolant by the respective quasi-
interpolants. Hence, one part of the numerical tests we want to discuss is how good the
matrices obtained from either compression method approximate the real, unobtainable in-
verseofthekernelmatrix.Thesecondpartisdedicatedtoshowcasingthepowerofthenovel
symmetricpreconditionerintroducedinSection4.2.Thisisespeciallydonebyanapplication
relevant problem in three spatial dimensions.
6.1 Setup
Since the theory of localized Lagrange functions was mainly developed for Mat´ern kernels
K = Φ , we will mostly focus on these kernel functions, albeit with varying parameter
ν
ν = 0.5,1.0,1.5. We again refer to Table 1 for explicit formulas, however, for the numerical
tests, we choose the length scale parameter δ = 0.1, i.e., we consider the radial function
Φ (·/δ). This could also be considered for the theoretical results presented above, but only
ν
leading to different, δ-dependent constants.
The set of data sites X is drawn randomly from a uniform distribution on [0,1]2 and we
select N =1000,10000,100000 using the random number generator from the C++ standard
library. The fill distance h , that is important for determining the footprint (12), is then
X,Ω
estimated by using a clustering algorithm. Although most of the theoretical results assume
quasi-uniform sets of sites, the numerical examples we conducted show that in practice
this restriction can be dropped provided that a small regularization term is added. In our
experiments, we consider the regularized systems M +λI with λ = 10−6N. Indeed, it is
customary to regularize kernel systems since even for a low number of quasi-uniform points
the unregularized kernel matrices are notoriously ill conditioned.
13The samplet matrix compression is implemented as described in [21]. We compute the
samplet compressed matrix using q +1 = 6 vanishing moments. For the fast assembly of
the samplet compressed matrix, we exploit the fast multipole method along as proposed in
[21].Thepolynomialdegreeforthedegeneratekernelexpansionissetto10whilethecut-off
parameterischosenas1/2(see[21]fordetails).Aftercomputingthecompressedmatrix,we
apply different sizes of the a-posteriori compression parameter to steer the compression rate
of the samplet compressed matrix. Then, we apply a sparse Cholesky decomposition using
METIS [28].
All computations have been performed at the Centro Svizzero di Calcolo Scientifico
(CSCS) on up to 20 nodes of the Alps cluster with two AMD EPYC 7742 @2.25GHz CPUs
and up to 512GB of main memory, resulting in a usage of up to 2560 cores.
6.2 Comparing the Compression
Westartbyprovidingresultsonthecompressionoftheinversematrix.Wemeasuretheerror
in the following way. As explained in Section 4.2, we collect the column vectors β(i) of the
matrix(A+λI)−1 inaglobal(sparse)matrixB.Sincethisshouldbeagoodapproximation
to the inverse of (A+λI), the error measure
error=∥(A+λI)B−I∥ ,
2
wouldbebestsuitedtocomparethemethods.Notethatthespectralerrorisarelativeerror
due to ∥I∥ =1. To compute the spectral error we use 200 power iterations.
2
In the theoretical results from Section 4.1, the localization of the Lagrange basis leads
to a matrix compression and the resulting error depends on the parameter κ which influ-
ences the size of the footprint. However, to compare the numerical results with the samplet
compression, we decide to use a different parameter. Since we are mainly interested in how
good compressed matrices for the single methods approximate the inverse, we consider the
compression rate
number of non-zero entries
compression rate= .
N2
This seems to be the right reference value to allow a fair comparison between the methods,
since the compression rate directly reflects the memory requirements for the approximate
dual basis.
In Fig. 4, we plot the error as a function of the compression rate for the local Lagrange
approach. It is interesting to see that the error behaves for every choice of ν qualitatively
thesameandthemethodseemstoconverge,achievinggoodapproximationerrorsevenwith
high compression. The results indicate that the localization of the Lagrange basis yields a
simple to implement approach, which gives accuracies dependent just on the compression
rate. Indeed, looking at Fig. 4, the error seems to be basically independent of the sample
size N and the smoothness parameter ν.
Incontrast,asseeninFig.5,thecompressionrateofthesampletmethodimprovesasN
increases. The reason for this is that the samplet compression gives a fixed error in basically
(i.e.,uptoapolylogarithmicfactor)linearcost,meaningthatthecompressionrateimproves
as N increases. This can clearly be observed from Fig. 5. Note that the compression rate
specified in this figure is the one of the sparse Cholesky factor since the approximate inverse
matrix is symmetric and only half the matrix needs to be stored. When the whole inverse
should be stored, the compression rate is twice the given values.
We may conclude from comparing Fig. 4 and Fig. 5 (note that the scaling of the axes is
thesame)thatsampletmatrixcompressionissuperiortothelocalLagrangebasisapproach,
at least for large data sets. For small data sets, however, the local Lagrange basis approach
approximates the inverse matrix quite well and is an attractive alternative since it is much
easier to implement than the samplet matrix compression.
6.3 Testing the Symmetric Preconditioner
We now turn to testing the new symmetric preconditioner introduced in Section 4.2 using
the localized Lagrange basis. We consider the same setup as before but restrict ourselves
14Footprints,N=1000,ν=0.5
Footprints,N=10000,ν=0.5
102 Footprints,N=100000,ν=0.5
Footprints,N=1000,ν=1.0
Footprints,N=10000,ν=1.0
Footprints,N=100000,ν=1.0
Footprints,N=1000,ν=1.5
100 Footprints,N=10000,ν=1.5
Footprints,N=100000,ν=1.5
10−2
10−4
10−6
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
compression rate
Figure 4: Localized Lagrange approach: Errors as a function of the compression rate in case of
varying smoothness parameters ν = 0.5,1.0,1.5 and sample sizes N = 103,104,105.
to a fixed amount of N = 100000 data sites. To assess the quality of the preconditioner,
weareinterestedincomparingthenumberof(preconditioned)conjugategradientiterations
(see [24] for details) when solving the linear system (2) of equations for the right-hand side
given by the vector of all ones versus the footprint parameter κ. To this end, the conjugate
gradient method is stopped when the residual is of order 10−9.
Cholesky decomposition Matrix square root
κ #X ν = 0.5 ν = 1.0 ν = 1.5 ν = 0.5 ν = 1.0 ν = 1.5
j
0.5 90 208 339 463 60 63 78
1.0 354 92 136 174 29 28 26
1.5 785 56 77 93 21 18 17
2.0 1374 39 49 58 16 14 13
2.5 2115 30 35 41 14 11 10
3.0 3000 24 28 31 11 10 9
Table2: ConjugategradientiterationsincaseofCholeskydecompositionandthematrixsquare
root versus the footprint size. The matrix square root is more stable since it performs better for
the same footprint size, but it requires also more computation time.
We observe from Table 2 that the matrix square root of the footprints provides a better
preconditioner compared to their Cholesky decomposition. Indeed, the number of iterations
arebyaboutafactor2–3smallerforthesamefootprintsize.Nonetheless,thecomputationof
thematrixsquarerootofthefootprintsismuchmoreexpensivecomparedtotheirCholesky
decomposition. First, we need only half the memory to store the Cholesky factor compared
to the matrix square root. Second, the computation of the matrix square root involves the
computation of all eigenvalues and eigenvectors by the QR-method, which is costly. If the
problem size does not fit any more into the cache of the computer, we require a factor of
about 100 more computation time per footprint for the matrix square root compared to the
Cholesky decomposition.
From these experiments, we may thus conclude that the Cholesky decomposition of the
footprintmatricesprovidesagood,symmetricpreconditionerforlargekernelmatrices,being
superioroverthematrixsquarerootapproach.Wewilldemonstratethisfindingbyafurther
experiment in the next subsection.
15
rorreSamplets,N=1000,ν=0.5
Samplets,N=10000,ν=0.5
102 Samplets,N=100000,ν=0.5
Samplets,N=1000,ν=1.0
Samplets,N=10000,ν=1.0
Samplets,N=100000,ν=1.0
Samplets,N=1000,ν=1.5
100 Samplets,N=10000,ν=1.5
Samplets,N=100000,ν=1.5
10−2
10−4
10−6
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
compression rate
Figure 5: Samplet matrix compression: Errors as a function of the compression rate in case of
varying smoothness parameters ν = 0.5,1.0,1.5 and sample sizes N = 103,104,105.
6.4 Signed Distance Function Interpolation
Figure 6: Signed distance function interpolation from a surface mesh of the Laokoon group:
Subsample of the signed distance function data (left), levelsets for the values {−5,−4,...,5}
(middle) and zero levelset (right).
To demonstrate the capabilities of the developed preconditioner also for real world ap-
plications, we consider a signed distance function interpolation problem in three spatial
dimensions and the respective surface reconstruction, similarly to [8].
Given a planar triangulation from a 3D scan of the Laokoon group (the scan is provided
by the Statens Museum for Kunst), we generate uniform samples of the signed distance
by using the about 500000 vertices the surface mesh and another 250000 random points
within the bounding box of the Laokoon group. This results in N =750000 samples of the
signed distance function in total. The left image in Fig. 6 shows a uniform subsample of size
100000 of the data points. For the interpolation, we consider the exponential kernel Φ
1/2
with lengthscale parameter δ = 0.01, where the data sites are rescaled to the hypercube
(0,1)3. The kernel matrix is compressed using a hierarchical matrix with adaptive cross
approximation, see [3], to compress the far-field of the kernel. The achieved compression
16
rorreerror is 2.37305·10−7 with a compression rate of 0.009375. The kernel matrix is regularized
with a regularization parameter λ=10−8N.
We use the variant of the preconditioner which is based on the Cholesky decomposition
as it has proven to be computationally significantly cheaper than the one which is based on
the matrix square root. We set κ = 0.25, which results in a compression rate of 0.000268.
The corresponding spectral error is 0.657666 and the conjugate gradient method takes only
12 iterations to achieve a relative residual error of 5.33355·10−9.
The interpolated signed distance function is evaluated at a uniform grid with 500 points
per axis direction, resulting in 125000000 points in total. For the evaluation, we employ the
fastmultipolemethoddevelopedin[22]withpolynomialsoftotaldegree4forthedegenerate
kernelexpansionofthekernelinthefar-field.ThemiddleimageofFig.6showsthelevelsets
for the values {−5,−4,...,5}, while the zero levelset, corresponding to the reconstructed
surface, is found on the right hand side of the figure.
7 Conclusion
MotivatedbyfindingdualpairsforfinitedimensionalsubspacesofreproducingkernelHilbert
spaces, we have discussed two methods to approximate the canonical dual pair. On the one
hand, we used the exponential decay of the inverse of the Mat´ern kernel to obtain the
local Lagrange functions. The ideas are also applicable to other kernels, those that induce a
pseudo-differential operator.
On the other hand, we gave a short overview of the samplet approximation of Lagrange
functions, a method that works also for a wider class of kernels, in particular those whose
inverse only exhibit algebraic decay.
Bothapproachescanalsobeusedtocompressthe(inverse)ofthekernelmatrix.Thema-
trixresultingfromthelocalizedLagrangefunctionshasalreadybeenusedforpreconditioning
of the GMRES method, we, for the first time however, derived a symmetric preconditioner
for the kernel matrix of the Mat´ern kernel. This means that the preconditioner also applies
to the conjugate gradient method.
We have presented extensive numerical tests. On the one hand, we have compared the
compressivepowerofthetwoapproaches.Here,wesawthatonlymeasuringthecompression,
the samplet method are clearly superior compared to the local Lagrange basis method.
However,thelattermethodisveryeasytoimplementandisarbitrarilyscalablewhereasthe
samplet method needs very sophisticated algorithms to perform. Finally, we have tested the
newpreconditioningmethodinanacademicexampleaswellasapossiblereal-liveapplication
to demonstrate its feasibility.
Acknowledgement. HH and RK were funded in parts by the Swiss National Science
Foundation through the grant “Adaptive Boundary Element Methods Using Anisotropic
Wavelets” (200021 192041). MM was funded in parts by the SNSF starting grant “Multires-
olution methods for unstructured data” (TMSGI2 211684).
References
[1] D. Alm, H. Harbrecht, and U. Kr¨amer. The H2-wavelet method. Journal of Computa-
tional and Applied Mathematics, 267:131–159, 2014.
[2] N. Angleitner, M. Faustmann, and J.M. Melenk. H-inverses for rbf interpolation. Ad-
vances in Computational Mathematics, 49:85, 2023.
[3] M. Bebendorf and S. Rjasanow. Adaptive low-rank approximation of collocation ma-
trices. Computing, 70(1):1–24, 2003.
[4] G.Beylkin. Wavelets,multiresolutionanalysisandfastnumericalalgorithms. InG.Er-
lebacher,M.Y.Hussaini,andL.M.Jameson,editors,WaveletsTheoryandApplications,
pages 182–262, Oxford, 1996. Oxford University Press.
[5] G. Beylkin, R. Coifman, and V. Rokhlin. Fast wavelet transforms and numerical algo-
rithms I. Communications on Pure and Applied Mathematics, 44(2):141–183, 1991.
17[6] L. Boutet de Monvel and P. Kr´ee. Pseudo-differential operators and Gevrey classes.
Universit´e de Grenoble. Annales de l’Institut Fourier, 17(fasc. 1):295–323, 1967.
[7] Martin Buhmann and Janin J¨ager. Quasi-Interpolation. Cambridge Monographs on
Applied and Computational Mathematics. Cambridge University Press, 2022.
[8] J.C.Carr,R.K.Beatson,J.B.Cherrie,T.J.Mitchell,W.R.Fright,B.C.McCallum,
and T. R. Evans. Reconstruction and representation of 3D objects with radial basis
functions. In Proceedings of the 28th annual conference on Computer graphics and
interactive techniques, SIGGRAPH ’01, pages 67–76, New York, 2001. Association for
Computing Machinery.
[9] W. Dahmen, H. Harbrecht, and R. Schneider. Compression techniques for boundary
integral equations. asymptotically optimal complexity estimates. SIAM Journal on
Numerical Analysis, 43(6):2251–2271, 2006.
[10] Stefano De Marchi and Holger Wendland. On the convergence of the rescaled localized
radial basis function method. Applied Mathematics Letters, 99:105996, 2020.
[11] G.E. Fasshauer. Meshfree Approximation Methods with MATLAB. World Scientific,
River Edge, 2007.
[12] E. Fuselier, T. Hangelbroek, F.J. Narcowich, J.D. Ward, and G.B. Wright. Localized
bases for kernel spaces on the unit sphere. SIAM Journal on Numerical Analysis,
51(5):2538–2562, 2013.
[13] A. George. Nested dissection of a regular finite element mesh. SIAM Journal on Nu-
merical Analysis, 10(2):345–363, 1973.
[14] W. Hackbusch. A sparse matrix arithmetic based on H-matrices. Part I: Introduction
to H-matrices. Computing, 62(2):89–108, 1999.
[15] W.Hackbusch.HierarchicalMatrices:AlgorithmsandAnalysis.Springer,Berlin-Heidel-
berg, 2015.
[16] W. Hackbusch and B.N. Khoromskij. A sparse matrix arithmetic based on H-matrices.
Part I: Application to multi-dimensional problems. Computing, 64(1):27–47, 2000.
[17] N. Hale, N.J. Higham, and L.N. Trefethen. Computing aα, log(a), and related matrix
functionsby contourintegrals. SIAM Journal on Numerical Analysis,5(46):2505–2523,
2008.
[18] T. Hangelbroek, F.J. Narcowich, C. Rieger, and J.D. Ward. An inverse theorem on
bounded domains for meshless methods using localized bases. ArXiv e-prints, 2014.
[19] T. Hangelbroek, F.J. Narcowich, X. Sun, and J.D. Ward. Kernel approximation on
manifoldsii:Thel normofthel projector. SIAM Journal on Mathematical Analysis,
∞ 2
43(2):662–684, 2011.
[20] Thomas Hangelbroek, Francis Narcowich, Christian Rieger, and Joseph Ward. An in-
verse theorem for compact lipschitz regions in Rd using localized kernel bases. Mathe-
matics of Computation, 87(312):1949–1989, 2018.
[21] H.HarbrechtandM.Multerer. Samplets:Constructionandscattereddatacompression.
Journal of Computational Physics, 471:111616, 2022.
[22] H.Harbrecht,M.Multerer,andJ.Quizi. Thedimensionweightedfastmultipolemethod
for scattered data approximation. arXiv:2402.09531, 2024.
[23] Helmut Harbrecht, Multerer Multerer, Olaf Schenk, and Christoph Schwab. Multireso-
lution kernel matrix algebra. Numerische Mathematik, 3(156):1085–1114, 2024.
[24] Magnus R. Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving
linear systems. Journal of Research of the National Institute of Standards, 49:409–436,
1952.
[25] L. H¨ormander. The analysis of linear partial differential operators. III. Classics in
Mathematics. Springer, Berlin, 2007. Pseudo-differential operators, Reprint of the 1994
edition.
[26] Lars H¨ormander. The analysis of linear partial differential operators. I. Classics in
Mathematics. Springer, Berlin, 2003. Distribution theory and Fourier analysis, Reprint
of the second (1990) edition.
18[27] A.S.Householder. The Theory of Matrices in Numerical Analysis. Blaisdell,NewYork,
1964.
[28] G. Karypis and V. Kumar. Metis: A software package for partitioning unstructured
graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices.
Technical Report 97-061, Department of Computer Science, University of Minnesota,
1997.
[29] Paul Kr´ee. Les noyaux des op´erateurs pseudo-diff´erentiels classiques (OPDC). Univer-
sit´e de Grenoble. Annales de l’Institut Fourier, 19(fasc. 1):179–194 x, 1969.
[30] G. Leobacher and F. Pillichshammer. Introduction to Quasi-Monte Carlo Integration
and Applications. Springer International Publishing, Cham, 2010.
[31] L.Lin,C.Yang,J.C.Meza,J.Lu,L.Ying,andW.E. Selinv—analgorithmforselected
inversion of a sparse symmetric matrix. ACM Transactions on Mathematical Software,
37(4):40, 2011.
[32] R.J. Lipton, D.J. Rose, and R.E. Tarjan. Generalized nested dissection. SIAM Journal
on Numerical Analysis, 16(2):346–358, 1979.
[33] S.G. Mallat. A theory for multiresolution signal decomposition: The wavelet represen-
tation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2(7), 1989.
[34] Bertil Mat´ern. Spatial variation, volume 36 of Lecture Notes in Statistics. Springer,
Berlin, second edition, 1986.
[35] Stefan Mu¨ller and Robert Schaback. A newton basis for kernel spaces. Journal of
Approximation Theory, 161(2):645–655, 2009.
[36] Maryam Pazouki and Robert Schaback. Bases for kernel-based spaces. Journal of
Computational and Applied Mathematics, (236):575–588, 2011.
[37] Luigi Rodino. Linear partial differential operators in Gevrey spaces. World Scientific
Publishing Co., Inc., River Edge, NJ, 1993.
[38] H.RueandL.Held.GaussianMarkovRandomFields:TheoryandApplications,volume
104ofMonographsonStatisticsandAppliedProbability. Chapman&Hall/CRCpress,
London, 2005.
[39] Youcef Saad and Martin H. Schultz. GMRES: A Generalized Minimal Residual Al-
gorithm for solving nonsymmetric linear systems. SIAM Journal on Scientific and
Statistical Computing, 7:856–869, 1986.
[40] R. Schneider and T. Weber. Wavelets for density matrix computation in electronic
structure calculation. Applied Numerical Mathematics, 56(10-11):1383–1396, 2006.
[41] G. Schulz. Iterative Berechnung der reziproken Matrix. ZAMM – Journal of Applied
Mathematics and Mechanics, pages 57–59, 1933.
[42] R.T. Seeley. Topics in pseudo-differential operators. In Pseudo-Differential Operators
(C.I.M.E., Stresa, 1968), page 167–305. Edizioni Cremonese, Rome, 1969.
[43] IngoSteinwartandAndreasChristmann. SupportVectorMashines.InformationScience
and Statistics. Springer, New York, 2008.
[44] J. Tausch and J. White. Multiscale bases for the sparse representation of bound-
ary integral operators on complex geometry. SIAM Journal on Scientific Computing,
24(5):1610–1629, 2003.
[45] M.E.Taylor. Pseudodifferential operators,volume34ofPrinceton Mathematical Series.
Princeton University Press, Princeton, NJ, 1981.
[46] Holger Wendland. Scattered Data Approximation. Cambridge Monographs on Applied
and Computational Mathematics. Cambridge University Press, Cambridge, 2004.
19