Networked Communication for Mean-Field Games
with Function Approximation and Empirical Mean-Field Estimation
PatrickBenjaminAlessandroAbate
UniversityofOxford
patrick.benjamin@jesus.ox.ac.uk,alessandro.abate@cs.ox.ac.uk
Abstract (NE) in a finite-agent game, with the error in the solution
reducingasthenumberofagentsN tendstoinfinity(Saldi,
Recent works have provided algorithms by which decen- Bas¸ar, and Raginsky 2018; Anahtarci, Kariksiz, and Saldi
tralised agents, which may be connected via a communica-
2023; Yardim, Goldman, and He 2024; Toumi, Malhame,
tionnetwork,canlearnequilibriainMean-FieldGamesfrom
and Le Ny 2024; Hu and Zhang 2024). MFGs have thus
asingle,non-episodicrunoftheempiricalsystem.However,
been applied to a wide variety of real-world problems: see
thesealgorithmsaregivenfortabularsettings:thiscomputa-
Laurie`reetal.(2022)forapplicationsandreferences. tionallylimitsthesizeofplayers’observationspace,mean-
ing that the algorithms are not able to handle anything but Recent works argue that classical algorithms for solving
small state spaces, nor to generalise beyond policies de- MFGs rely on assumptions and methods that are likely to
pending on the ego player’s state to so-called ‘population- be undesirable in real-world applications, emphasising that
dependent’policies.Weaddressthislimitationbyintroduc- desirable qualities for practical MFG algorithms include:
ing function approximation to the existing setting, drawing
learningfromtheempiricaldistributionofN agents(with-
ontheMunchausenOnlineMirrorDescentmethodthathas
outgenerationormanipulationofthisdistributionbytheal-
previously been employed only in finite-horizon, episodic,
gorithmitselforanexternaloracle/simulator);learningfrom
centralisedsettings.Whilethispermitsustoincludethepop-
asingle,continuedsystemrunthatisnotarbitrarilyresetas
ulation’s mean-field distribution in the observation for each
player’spolicy,itisarguablyunrealistictoassumethatdecen- in episodic learning; model-free learning; decentralisation;
tralisedagentswouldhaveaccesstothisglobalinformation: and fast practical convergence (Yardim et al. 2023; Ben-
wethereforeadditionallyprovidenewalgorithmsthatallow jamin and Abate 2024). While these works address these
agentstoestimatetheglobalempiricaldistributionbasedon desiderata,theydosoonlyinsettingsinwhichthestateand
alocalneighbourhood,andtoimprovethisestimateviacom- action spaces are small enough that the Q-function can be
munication over a given network. Our experiments show- representedbyatable,limitingtheirapproaches’scalability.
case how the communication network allows decentralised
Moreover, in those works, as in many others on MFGs,
agentstoestimatethemean-fielddistributionforpopulation-
agents only observe their local state as input to their Q-
dependent policies, and that exchanging policy information
helpsnetworkedagentstooutperformbothindependentand function(whichdefinestheirpolicy).Thisissufficientwhen
even centralised agents in function-approximation settings, thesolvedMFGisexpectedtohaveastationarydistribution
byanevengreatermarginthanintabularsettings. (‘stationary MFGs’) (Laurie`re et al. 2022; Xie et al. 2021;
Anahtarci,Kariksiz,andSaldi2023;uzZamanetal.2023;
Yardim et al. 2023; Benjamin and Abate 2024). However,
1 Introduction
inrealitytherearenumerousreasonsforwhichagentsmay
The mean-field game (MFG) framework (Lasry and Lions benefitfrombeingabletorespondtothecurrentdistribution.
2007; Huang, Malhame´, and Caines 2006) can be used to Recent work has thus increasingly focused on these more
address the difficulty faced by multi-agent reinforcement generalsettingswhereitisnecessaryforagentstohaveso-
learning(MARL)regardingcomputationalscalabilityasthe called ‘master policies’ (a.k.a. population-dependent poli-
numberofagentsincreases.Itmodelsarepresentativeagent cies)whichdependonboththemean-fielddistributionand
asinteractingnotwithotherindividualagentsinthepopula- theirlocalstate(Cardaliaguetetal.2015;Carmona,Delarue,
tiononaper-agentbasis,butinsteadwithadistributionover andLacker2016;Perrinetal.2022;Wuetal.2024;Laurie`re
theotheragents,knownasthemeanfield.TheMFGframe- etal.2022;Lauriereetal.2022;Laurie`reetal.2022).
workanalysesthelimitingcasewhenthepopulationconsists The distribution is a high-dimensional (and thus large)
ofaninfinitenumberofsymmetricandanonymousagents, observation object, and also takes a continuum of values.
that is, they have identical reward and transition functions Thereforeapopulation-dependentQ-functioncannotberep-
which depend on the mean-field distribution rather than on resentedexactlyinatableandmustbeapproximated.Toad-
the actions of specific other players. The solution to this dresstheselimitations whilemaintainingthedesiderata for
game is the mean-field Nash equilibrium (MFNE), which real-worldapplicationsgiveninrecentworks,weintroduce
can be used as an approximation for the Nash equilibrium functionapproximationtotheMFGsettingofdecentralised
4202
guA
12
]AM.sc[
1v70611.8042:viXraagents learning from a single, non-episodic run of the em- • We present numerical results showcasing these algo-
pirical system, allowing this setting to handle larger state rithms, especially the two benefits of the decentralised
spacesandtoacceptthemean-fielddistributionasanobser- communicationscheme,whichsignificantlyoutperforms
vation input. To overcome the difficulties of training non- boththeindependentandcentralisedsettings.
linear approximators in this context, we use the so-called
Thepaperisstructuredasfollows.Preliminariesaregiven
‘Munchausen’ trick, introduced by Vieillard, Pietquin, and
inSec.2andourcorelearningandpolicyimprovemental-
Geist (2020) for single-agent RL, and extended to MFGs
gorithm is presented in Sec. 3. Our mean-field estimation
by Lauriere et al. (2022), and to MFGs with population-
andcommunicationalgorithmsaregiveninSec.4,andex-
dependentpoliciesbyWuetal.(2024).
periments are presented in Sec. 5. See Appx. B for a more
We particularly explore this in the context of networked
detailed ‘Related work’ section. Ways to extend our algo-
communication between decentralised agents (Benjamin
rithmsarediscussedinthe‘FutureWork’section(Appx.C).
and Abate 2024). We demonstrate that this communication
bringstwospecificbenefitsoverthepurelyindependentset-
2 Preliminaries
ting (while also removing the undesirable assumption of a
centralised learner, which in the real world may be unre- 2.1 Mean-fieldgames
alistic, a computational bottleneck and a vulnerable single Weusethefollowingnotation.N isthenumberofagentsin
point of failure). Firstly, when the Q-function is approxi- apopulation,withS andArepresentingthefinitestateand
mated rather than exact, some updates lead to better per-
common action spaces, respectively. The set of probability
forming policies than others. Allowing networked agents measuresonafinitesetX isdenoted∆ ,ande ∈∆ for
X x X
topropagatebetterperformingpoliciesthroughthepopula- x∈X isaone-hotvectorwithonlytheentrycorresponding
tionleadstofasterpracticalconvergencethaninthepurely to x set to 1, and all others set to 0. For time t ≥ 0, µˆ
t
independent-learning case, and often even than in the cen- = 1 (cid:80)N (cid:80) 1 e ∈ ∆ is a vector of length |S|
tralised case. Secondly, we argue that in the real world it N i=1 s∈S si t=s s S
denoting the empirical categorical state distribution of the
is unrealistic to assume that decentralised agents, endowed
N agentsattimet.Foragenti ∈ 1...N,i’spolicyattime
withlocalstateobservationsandlimited(ifany)communi-
t depends on its observation oi. We explore three different
cationradius,wouldbeabletoobservetheglobalmean-field t
formsthatthisobservationobjectcantake:
distribution and use it as input to their Q-functions / pol-
icy. We therefore further contribute two setting-dependent • Intheconventionalsetting,theobservationissimplyi’s
algorithms by which decentralised agents can estimate the currentlocalstatesi,suchthatπi(a|oi)=πi(a|si).
t t t
global distribution from local observations, and further im-
• When the policy is population-dependent, if we assume
provetheirestimatesbycommunicationwithneighbours.
perfectobservabilityoftheglobalmean-fielddistribution
We focus on ‘coordination games’, where agents can in- thenwehaveoi =(si,µˆ ).
t t t
creasetheirindividualrewardsbyfollowingthesamestrat-
• Itisunrealistictoassumethatdecentralisedagentswith
egy as others and therefore have an incentive to communi-
a possibly limited communication radius can observe
cate policies, even if the MFG setting itself is technically
theglobalmean-fielddistribution,soweallowagentsto
non-cooperative.Thusourworkcanbeappliedtoreal-world
formalocalestimateµ˜ˆi whichcanbeimprovedbycom-
problems in e.g. traffic signal control, formation control in t
swarmrobotics,andconsensusandsynchronisatione.g.for municationwithneighbours.Herewehaveoi t =(si t,µ˜ˆi t).
sensornetworks.Insummary,ourcontributionsare: In the following definitions we focus on the population-
dependent case when oi = (si,µˆ ), and clarify afterwards
• We introduce, for the first time, function approximation t t t
the connection to the other observation cases. Thus the set
toMFGsettingswithdecentralisedagents.Todothis:
of policies is Π = {π : S × ∆ → ∆ }, and the set of
S A
– WeuseMunchausenRLforthefirsttimeinaninfinite- Q-functionsisdenotedQ={q :S×∆ ×A→R}.
S
horizon MFG context, unlike the finite-horizon set-
Definition1(N-playersymmetricanonymousgames)
tingsinLauriereetal.(2022)andWuetal.(2024).
An N-player stochastic game with symmetric, anonymous
– In doing so, we also present the first use of function
agentsisgivenbythetuple⟨N,S,A,P,R,γ⟩,whereAis
approximationforsolvingMFGsfromcontinued,non-
theactionspace,identicalforeachagent;S istheidentical
episodic runs of the empirical system; this is unlike
state space of each agent, such that their initial states are
Yardimetal.(2023)andBenjaminandAbate(2024),
{si}N ∈SN andtheirpoliciesare{πi}N ∈ΠN.P :S
whichlookattabularsettings. 0 i=1 i=1
× A × ∆ → ∆ is the transition function and R : S ×
S S
• Functionapproximationallowsustoexplorelargerstate A × ∆ → [0,1] is the reward function, which map each
S
spaces, and also settings where agents’ policies depend agent’s local state and action and the population’s empir-
onthemean-fielddistributionaswellastheirlocalstate. ical distribution to transition probabilities and bounded
rewards,respectively,i.e.∀i=1,...,N:
• Rather than assuming that agents have access to this
global knowledge, as in prior works, we present two si ∼P(·|si,ai,µˆ ), ri =R(si,ai,µˆ ).
novel algorithms that allow decentralised agents to lo- t+1 t t t t t t t
cally estimate the empirical distribution and to improve AtthelimitasN → ∞,theinfinitepopulationofagents
theseestimatesbycommunicationwithothers. can be characterised as a limit distribution µ ∈ ∆ ; the
Sinfinite-agentgameistermedanMFG.Theso-called‘mean- π with respect to its induced mean-field flow µ = I(π )
k k
fieldflow’µisgivenbytheinfinitesequenceofmean-field to compute its Q-function Q . To stabilise the learning
k+1
distributionss.t.µ=(µ ) . process, we then use a weighted sum over this and past Q-
t t≥0
functions,andsetπ tobethesoftmaxoverthisweighted
Definition2(Inducedmean-fieldflow) WedenotebyI(π) k+1 (cid:16) (cid:17)
themean-fieldflowµinducedwhenalltheagentsfollowπ, sum,i.e. π k+1(·|s,µ) = softmax τ1
q
(cid:80)k κ=0Q κ(s,µ,·) .
wherethisisgeneratedfromπasfollows: τ isatemperatureparameterthatscalestheentropyinMun-
q
(cid:88) chausenRL(Vieillard,Pietquin,andGeist2020);notethat
µ (s′)= µ (s)π(a|s,µ )P(s′|s,a,µ ).
t+1 t t t thisisadifferenttemperaturetotheoneagentsusewhense-
s,a lecting which communicated parameters to adopt, denoted
Whenthemean-fieldflowisstationarysuchthatthedis-
τcommanddiscussedinSec.3.2.
k
tribution is the same for all t, i.e. µ = µ ∀t ≥ 0, IftheQ-functionisapproximatednon-linearlyusingneu-
t t+1
the policy πi(a|si,µ ) need not depend on the distribu- ral networks, it is difficult to compute this weighted sum.
t t
tion, such that πi(a|si,µ ) = πi(a|si), i.e. we recover the Theso-called‘Munchausentrick’addressesthisbycomput-
t t t
classical population-independent policy. However, for such ingasingleQ-functionthatmimicstheweightedsumusing
a population-independent policy the initial distribution µ implicit regularisation based on the Kullback-Leibler (KL)
0
must be known and fixed in advance, whereas otherwise it divergence between π k and π k+1 (Vieillard, Pietquin, and
neednotbe.Wealsogivethefollowingdefinitions. Geist2020).UsingthisreparametrisationgivesMunchausen
OMD(MOMD),detailedfurtherinSec.3.1(Lauriereetal.
Definition3(Mean-fielddiscountedreturn) In a MFG
2022;Wuetal.2024).MOMDdoesnotbiastheMFNE,and
where all agents follow policy π giving a mean-field flow
hasthesameconvergenceguaranteesasOMD(Hadikhanloo
µ=(µ ) ,theexpecteddiscountedreturnoftherepresen-
t t≥0 2017;Pe´rolatetal.2022a;Wuetal.2024).
tativeagentisgivenby
V(π,µ)=E(cid:34) (cid:88)∞ γt(R(s t,a t,µ t))(cid:12) (cid:12) (cid:12) (cid:12)st+a 1t ∼∼ Ps π0 ((∼ ·· || sµ s tt0 ,, aµ tt ,) µt)(cid:35) . 2 W.3 ecoN nce et iw veo or fk ts hefinitepopulationasexhibitingtwotime-
t=0 varyingnetworks.Thebasicdefinitionofsuchanetworkis:
Definition4(Best-response(BR)policy) A policy π∗ is a
Definition6(Time-varyingnetwork) The time-varying
best response (BR) against the mean-field flow µ if it max-
network {G } is given by G = (N,E ), where N is the
imisesthediscountedreturnV(·,µ);thesetofthesepolicies t t≥0 t t
setofverticeseachrepresentinganagenti=1,...,N,and
isdenotedBR(µ),i.e.
the edge set E ⊆ {(i,j) : i,j ∈ N} is the set of undirected
t
π∗ ∈BR(µ):=argmaxV(π,µ). linkspresentattimet.
π One of these networks Gcomm defines which agents are
Definition5(Mean-fieldNashequilibrium(MFNE)) A able to communicate informt ation to each other at time t.
pair (π∗,µ∗) is a mean-field Nash equilibrium (MFNE) if The second network Gobs is a graph defining which agents
thefollowingtwoconditionshold: t
areabletoobserveeachother’sstates,whichweuseingen-
• π∗isabestresponsetoµ∗,i.e.π∗ ∈BR(µ); eralsettingsforestimatingthemean-fielddistributionfrom
• µ∗isinducedbyπ∗,i.e.µ∗=I(π∗). localinformation.Thestructureofthetwonetworksmaybe
identical(e.g.ifembodiedagentscanbothobservetheposi-
π∗ is thus a fixed point of the map BR ◦ I, i.e. π∗ ∈
tion(state)of,andexchangeinformationwith,otheragents
BR(I(π∗)). If a population-dependent policy is a MFNE
withinacertainphysicaldistancefromthemselves),ordif-
policyforanyinitialdistributionµ ,itisa‘masterpolicy’.
0 ferent (e.g. if agents can observe the positions of nearby
Previous works have shown that, in tabular settings, it
agents,butonlyexchangeinformationwithagentsbywhich
is possible for a finite population of decentralised agents
theyareconnectedviaatelephoneline,whichmayconnect
(each of which is permitted to have a distinct population-
agentsoverlongdistances).
independent policy πi) to learn the MFNE using only the
We also define an alternative version of the observation
empirical distribution µˆ , rather than the exactly calculated
t graph that is useful in a specific subclass of environments,
mean-field flow (Yardim et al. 2023; Benjamin and Abate
which can most intuitively be thought of as those where
2024).ThisMFNEisinturnanapproximateNEfortheorig-
agents’ states are positions in physical space. When this is
inalfinite-agentgame.Inthisworkweprovidealgorithmsto
thecase,weusuallythinkofagents’abilitytoobserveeach
perform this in non-tabular and population-dependent set-
otherasdependingmoreabstractlyonwhetherstatesarevis-
tings,anddemonstratethemempirically.
ibletoeachother.Wedefinethisvisibilitygraphasfollows:
2.2 (Munchausen)OnlineMirrorDescent Definition7(Time-varyingstate-visibilitygraph) The
Instead of performing the computationally expensive pro- time-varying state visibility graph {G tvis} t≥0 is given by
cess of finding a BR at each iteration, we can use a form Gvis=(S′,Evis),whereS′isthesetofverticesrepresenting
t t
of policy iteration for MFGs called Online Mirror Descent the environment states S, and the edge set Evis ⊆ {(m,n)
t
(OMD). This involves beginning with an initial policy π , : m,n ∈ S′} is the set of undirected links present at time t,
0
and then at each iteration k, evaluating the current policy indicatingwhichstatesarevisibletoeachother.We view an agent in s as able to obtain a count of the Algorithm 1: Networked learning with non-linear function
numberofagentsins′ifs′isvisibletos.Thebenefitofthis approximation
graphGvis overGobs isthatthereismutualexclusivity:ei-
t t Require: loopparametersK,M,L,E,C ,learningparam-
theranagentinstatesisabletoobtainatotalcountofallof etersγ,τ ,|B|,cl,ν,{τcomm} p
theagentsinstates′(ifs′isvisibletos),oritcannotobtain q k k∈{0,...,K−1}
Require: initialstates{si} ,i=1,...,N;t←0
information about any agent in state s′ (if those states are 0 i
1: ∀i : Randomly initialise parameters θi of Q-networks
not visible to each other). Additionally, this graph permits (cid:16)0 (cid:17)
a sn ′aa sg le on nt gin asst sa ′t ie ss vit so ibo lb es te orv se .Tth ha et st eh ber ee nea fire tsn ao reag ne on tt as vi an ils at ba lt ee
2:
fQˇ oθ r0i k(o =i t,· 0) ,, .a .n .d ,s Ket −π 0i 1(a d| oo)=softmax τ1 qQˇ θ 0i(o,·) (a)
iftheobservabilitygraphisdefinedstrictlybetweenagents 3: ∀i:Emptyi’sbuffer
asinG tobs,suchthatusingG tvis facilitatesmoreefficientes- 4: form=0,...,M−1do
timationoftheglobalmean-fielddistributionfromlocalin- 5: Take step ∀i : ai ∼ πi(·|oi),ri =
formationinsettingswhereG tvisisapplicable(seeSec.4). R(si t,ai t,µˆ t),si
t+1
∼P(t ·|si t,ai t,µˆ t)k ;t←t t+t 1
6: ∀i:Addζitoi’sbuffer
3 Learningandpolicyimprovement t
7: endfor
3.1 Q-networkandupdate 8: forl=0,...,L−1do
Lines 1-14 of our novel Alg. 1 contain the core Q- 9: ∀i:SamplebatchB ki ,l fromi’sbuffer
function/policy update method. Agent i maintains a neural 10: UpdateθtominimiseLˆ(θ,θ′)asinDef.8
network parametrised by θi to approximate its Q-function: 11: Ifl mod ν =0,setθ′ ←θ
k
Qˇ (o,·). The agent’s policy is given by π (a|o) = 12: endfor
θi θi
s tho efk tm poa lx ic(cid:16) yτ1
q
πQˇ ki(θ aki( |oo ),· f) o(cid:17) r(a si) m. pW lich ie tyn
.
Eap ap cr hop ar gi ea nte t, mwk ae intd ae inn sot ae 1 13 4:
:
Q ∀ˇ iθ :ki + S1 e( to π, ki·) +← 1(aQ |ˇ oθ )ki , =L( so o, f· t) max(cid:16)
τ1 qQˇ θ ki
+1(o,·)(cid:17)
(a)
buffer (with size M) of collected transitions of the form 15: ∀i:σi =0
(cid:0) oi t,a t,r t,oi t+1(cid:1) .Ateachiterationk,theyemptytheirbuffer 16: fore=k+ 01 ,...,E−1evaluationstepsdo
(Line 3) before collecting M new transitions (Lines 4-7); 17: Take step ∀i : ai ∼ πi(·|oi),ri =
each decentralised agent i then trains its Q-network Qˇ θ ki R(si t,ai t,µˆ t),si t+1 ∼P(t ·|si t,ai t,µˆ t)k ;t←t t+t 1
via L training updates as follows (Lines 8-12). For train- 18: ∀i:σi =σi +γe(ri+h(πi (si)))
ing purposes, i also maintains a target network Qˇ θi,′ with 19: t←tk ++1 1 k+1 t k+1 t
the same architecture but parameters θi,′ copied frk o, ml θi 20: endfor
k,l k,l 21: forC proundsdo
lessregularlythanθ ki ,l themselvesareupdated,i.e.onlyev- 22: ∀i:Broadcastσ ki +1,π ki +1
ae gry enν
t
sle aa mrn pi ln eg
s
aite rara nt dio on ms b(L ati cn he B11 ki) ,.
l
A oft |e Bac |h trait ne sr ia tt ii oo nn sl f, roth me 2 23 4:
:
∀ ∀ii:J :ti S← ele{ cj
t
∈ adN op: te( di, ij) ∼∈E Pt r}
(cid:0) adoptedi =j(cid:1) =
its buffer (Line 9). It then trains its neural network using
stochastic gradient descent to minimise the following em- exp(σ kj +1/τ kcomm) ∀j ∈Ji
piricalloss(Line10):
(cid:80) x∈Jtiexp(σ kx +1/τ kcomm) t
25: ∀i:σi ←σadoptedi ,πi ←πadoptedi
Definition8(EmpiricallossforQ-network) The empiri- k+1 k+1 k+1 k+1
26: Take step ∀i : ai ∼ πi(·|oi),ri =
callossisgivenby t k t t
R(si,ai,µˆ ),si ∼P(·|si,ai,µˆ );t←t+1
Lˆ(θ,θ′)= 1 (cid:88) (cid:12) (cid:12)Qˇ (o ,a )−T(cid:12) (cid:12)2 , 27: endfort t t t+1 t t t
|B| (cid:12) θ ki ,l t t (cid:12) 28: endfor
transition∈B ki ,l 29: return policies{π Ki } i,i=1,...,N
wherethetargetT is
T =r + [τ lnπ (a |o )]0 +
t q θi,′ t t cl
k,l toimprovelocalestimatesofthemean-fielddistribution(see
(cid:18) (cid:19)
γ (cid:88) π (a|o ) Qˇ (o ,a)−τ lnπ (a|o ) . Sec. 4). The other, described here, is used to privilege the
θi,′ t+1 θi,′ t+1 q θi,′ t+1 spreadofbetterperformingpolicyupdatesthroughthepop-
k,l k,l k,l
a∈A
ulation,allowingfasterconvergenceinthisnetworkedcase
Forcl<0,[·]0 isaclippingfunctionusedinMunchausen thanintheindependentandevencentralisedcases.
cl
RL to prevent numerical issues if the policy is too close to We extend the work in Benjamin and Abate (2024) to
deterministic,asthelog-policytermisotherwiseunbounded thefunction-approximationcase,whereinourworkagents
(Vieillard,Pietquin,andGeist2020;Wuetal.2024). broadcasttheparametersoftheQ-networkthatdefinestheir
policy, rather than the Q-function table. At each iteration
3.2 Communicationandadoptionofparameters
k,afterindependentlyupdatingtheirQ-networkandpolicy
We use the communication network Gcomm to share two (Lines3-14),agentsapproximatelyevaluatetheirnewpoli-
t
typesofinformationatdifferentpointsinAlg1.Oneisused cies by collecting rewards for E steps, and assign the es-Algorithm2:Mean-fieldestimationandcommunicational- Algorithm3:Mean-fieldestimationandcommunicational-
gorithmingeneralsettings gorithmforenvironmentswithGvis
t
Require: Time-dependent observation graph Gobs, time- Require: Time-dependent visibility graph Gvis, time-
t t
dependent communication graph Gcomm, states dependent communication graph Gcomm, states
t t
{si}N ,numberofcommunicationroundsC {si}N ,numberofcommunicationroundsC
t i=1 e t i=1 e
1: ∀i,s:Initialisecountvectorυˆi[s]with∅ 1: ∀i,s:Initialisecountvectorυˆi[s]with∅
t t
2: ∀i:υˆ ti[sj t]←{IDj}
j∈N:(i,j)∈E tobs
2: ∀ (cid:80)i and ∀s′ ∈ 1S′ : (si t,s′) ∈ E tvis : υˆ ti[s′] ←
3: forc ein1,...,C edo j∈1,...,N:sj t=s′
4 5:
:
∀ ∀i i: :B
J
tr io ←adc {a jst ∈υˆ Nti ,ce
:(i,j)∈E tcomm}
43 :: for ∀c ie :i Bn r1 o, a. d. c. a, sC tυˆe tid ,co
e
876 : :: e ∀n id∀ :i cf, o os r u: nυˆ tt ei , d(ce a+ g1 e) n[s t] s←
i
←υˆ ti (cid:80),ce[s]∪{υˆ tj ,ce |[ υˆs i]} [sj ]∈ |J ti 5 6: : ∀ υ∀ ˆi i j,: sJ [st ai ]n= ̸=di ∅∀∪ j{j ∈∈ JN ti : :(i υ, ˆj ti ,) (c∈ e+E 1t )c [o sm ]m ←} υˆ tj ,ce[s] if
t s∈S:υˆ ti[s]̸=∅ t t,ce
9: ∀i:uncounted agentsi ←N −counted agentsi 7: endfor
10: ∀i,s:µ˜ˆi t[s]← uncoun Nt ×et d |Sa |gentsi t t 98 :: ∀∀ ii :: uco nu cn oute nd tea dge an gt es ni t t←
si
←(cid:80) s N∈S −:υˆ ti c[ os] u̸= n∅ tυˆ eti d[s a]
gentsi
11: ∀i,swhereυˆ ti[s]isnot∅:µ˜ˆi t[s]←µ˜ˆi t[s]+ |υˆ t Ni[s]| 10: ∀i:unseen statesi
t
←t(cid:80) s∈S:υˆi[s]=∅1 t
12: return mean-fieldestimates∀iµ˜ˆi t 11: ∀i,swhereυˆi[s]isnot∅:µ˜ˆi[s]←t υˆ ti[s]
t t N
12:
∀i,swhereυˆi[s]is∅:µ˜ˆi[s]← uncountedagentsi
t
t t N×unobservedstatesi
t
timated value to σi (Lines 15-20). They then broadcast 13: return mean-fieldestimates∀iµ˜ˆi
k+1 t
theirQ-networkparametersalongwithσi (Line22).Re-
k+1
ceiving these from their neighbours on the communication
network, agents select which set of parameters to adopt by
as full of ∅ (‘no count’) markers for each state. Then, for
taking a softmax over their own and the received estimate
eachagentj withwhichagentiisconnectedviatheobser-
values σ kj +1 (Lines 23-25). They repeat this broadcast and vation graph, i places j’s unique ID in its count vector in
adoption process for C p rounds, which is distinct from the thecorrectstateposition.Next,forC ≥ 0communication
e
C communicationroundsdiscussedinSec.4.Bythispro-
e rounds,agentsexchangetheirlocalcountswithneighbours
cess, decentralised agents can adopt policy parameters that
onthecommunicationnetwork,andmergethesecountswith
areestimatedtobebetterperformingthantheirown,bring-
theirowncountvector,filteringouttheuniqueIDsofthose
ingthebenefitstoconvergencespeeddiscussedinSec.5.1. that have already been counted. If C = 0 then the local
e
countwillremainpurelyindependent.Byexchangingthese
4 Mean-fieldestimationandcommunication partiallyfilledvectors,agentsareabletoimprovetheirlocal
countsbyaddingthestatesofagentsthattheyhavenotbeen
We first describe the most general version of our algo-
abletoobservedirectlythemselves.
rithmfordecentralisedestimationoftheempiricalcategor-
After the C communication rounds, each state posi-
icalmean-fielddistribution,assumingthemoregeneralset- e
ting where Gobs applies (see discussion in Sec. 2.3). We tion υˆ ti[s] either still maintains the ∅ marker if no agents
t have been counted in this state, or it contains x > 0
subsequently detail how the algorithm can be made more s
unique IDs. The local mean-field estimate µ˜ˆi is then ob-
efficient in environments where the more abstract visibil- t
ity graph Gvis applies, as in our experimental settings. In tained from υˆ ti as follows. All states that have a count x s
t
have this count converted into the categorical probability
both cases, the algorithm runs to generate the observation
x /N (we assume that agents know the total number of
object when a step is taken in the main Alg. 1, i.e. to pro- s
duce oi = (si,µ˜ˆi) for the steps ai ∼ πi(·|oi) in Lines 5, agents in the finite population, even if they cannot observe
t t t t k t them all at each t). The total number of agents counted in
17and26.Bothversionsofthealgorithmaresubjecttoim- υˆi isgivenbycounted agents=(cid:80) x ,andtheagents
plicitassumptions,whichwehighlightanddiscussmethods t s∈S s
that have not been observed are uncounted agents = N
foraddressinginour‘FutureWork’sectioninAppx.C.
- counted agents. In this general setting, the unobserved
Algorithm for the general setting The method for the agentsareassumedtobeuniformlydistributedacrossallthe
general setting functions is given in Alg 2. In this setting, states,souncounted agents/(N ×|S|)isaddedtoallthe
weassumethateachagentisassociatedwithauniqueIDto valuesinµ˜ˆi,replacingthe∅markerforstatesforwhichno
t
avoid the same agents being counted multiple times. Each agentshavebeenobserved.
agentmaintainsa‘count’vectorυˆioflength|S|i.e.itisthe
t
same shape as the vector denoting the true empirical cate- Algorithm for visibility-based environments We now
goricaldistributionofagents.Eachstatepositioninthevec- describethedifferencesinourestimationalgorithm(Alg.3)
tor can hold a list of IDs. Before any actions are taken at forthesubclassofenvironmentswhereGvisappliesinplace
t
each time step t, each agent’s count vector υˆi is initialised of Gobs, i.e. the mutual observability of agents depends in
t tFigure 1: ‘Target agreement’ task, population-independent Figure 2: ‘Cluster’ task, population-independent policies,
policies,50x50grid. 100x100grid.
turn on the mutual visibility of states. Recall the benefit of
Gvis over Gobs is that the former allows an agent in state s
t t
toobtainacorrect,completecountx ≥0ofalltheagents
s′
instates′,foranystates′ thatisvisibletos(notethecount
maybezero).UniqueIDsarethusnotrequiredasthereisno
riskofcountingthesameagenttwicewhenreceivingcom-
municatedcounts:eitherallagentsins′ havebeencounted,
ornocounthasyetbeenobtainedfors′.Thissimplifiesthe
algorithmandhelpspreserveagentanonymityandprivacy.
Secondly,uncountedagentscannotbeinstatesforwhich
a count has already been obtained, since the count is com-
plete and correct, even if the count is x = 0. Therefore
s′
after the C communication rounds, the uncounted agents
e
uncounted/N needbeuniformlydistributedonlyacrossthe
positions in the vector that still have the ∅ marker, and not
acrossallstatesasinthegeneralsetting.Thismakesthees-
Figure3:‘Pushobject’task,population-dependentpolicies
timationmoreaccurateinthisspecialsetting.
withestimatedmean-fielddistribution,10x10grid.
5 Experiments
full technical description. The first two tasks are those
We provide two sets of experiments. The first set show-
usedwithpopulation-independentpoliciesinBenjaminand
cases that our function-approximation algorithm (Alg. 1)
Abate (2024), though while they show results for an 8x8
can scale to large state spaces for population-independent
anda‘larger’16x16grid,ourresultsforthesetasksarefor
policies, and that in such settings networked, communicat-
100x100and50x50grids:
ingagentscanoutperformpurely-independentandevencen-
tralisedagents,anddosobyanevengreatermarginthanin • Cluster.Agentsarerewardedforgatheringtogether.The
the tabular settings from Benjamin and Abate (2024). The agentsaregivennoindicationwheretheyshouldcluster,
secondsetdemonstratesthatAlg.1canhandlepopulation- agreeingthisthemselvesovertime.
dependentpolicies,aswellastheabilityofAlg.3topracti- • Targetagreement.Agentsarerewardedforvisitingany
callyestimatethemean-fielddistributionlocally. ofagivennumberoftargets,buttheirrewardispropor-
Forthetypesofgameusedinourdemonstrationswefol- tionaltothenumberofotheragentsco-locatedatthetar-
lowthegoldstandardinpriorworksonMFGs,namelygrid- get.Agentsmust thuscoordinateonwhichsingle target
worldenvironmentswhereagentscanmoveinthefourcar- theywillallmeetattomaximisetheirindividualrewards.
dinal directions or remain in place (uz Zaman et al. 2023;
We also showcase the ability of our algorithm to handle
Lauriere et al. 2022; Algumaei et al. 2023; Laurie`re 2021;
two more complex tasks, using population-dependent poli-
Cui, Fabian, and Koeppl 2023; Lauriere et al. 2022; Ben-
ciesandestimatedmean-fieldobservations:
jamin and Abate 2024; Wu et al. 2024). We present re-
sults from four coordination tasks defined by the reward/- • Evade shark in shoal. At each t, a ‘shark’ in the envi-
transition functions of the agents - see Appx. A.1 for a ronmenttakesasteptowardsthegridpointcontainingthemeasureofproximitytotheMFNE.Itquantifieshowmuch
abest-respondingagentcanbenefitbydeviatingfromtheset
ofpoliciesthatgeneratethecurrentmean-fielddistribution,
with a decreasing exploitability meaning the population is
closertotheMFNE.However,thereareseveralissueswith
this metric in our setting, such that it may give limited or
noisyinformation(seeAppx.A.2forafulldiscussion).We
thus also give a second metric, as in Benjamin and Abate
(2024):thepopulation’saveragediscountedreturn.Thisal-
lows us to compare how much agents are learning policies
thatincreasetheirreturns,evenwhentheexploitabilitymet-
ricgivesuslimitedabilitytodistinguishbetweenthedesir-
abilityoftheMFNEstowhichpopulationsareconverging.
Population-independent policies in large state-spaces
Figs. 1 and 2 illustrate that introducing function approxi-
mationtoalgorithmsinthissettingallowsthemtoconverge
Figure 4: ‘Evade’ task, population-dependent policies with
withinapracticalnumberofiterations(k ≪ 100),evenfor
estimatedmean-fielddistribution,10x10grid.
large state spaces (100x100 grids). By contrast, the tabular
algorithms in Benjamin and Abate (2024) appear only just
toconvergebyk = 200forthesametasksforthelargerof
mostagentsaccordingtotheempiricalmean-fielddistri-
theirtwogrids,whichisonly16x16.
bution. The shark’s position forms part of agents’ local
In Fig. 1 the networked agents generally have lower ex-
states in addition to their own position. Agents are re-
ploitability than both centralised and independent agents,
warded more for being further from the shark, and also
whileinFig.2thenetworkedagents’exploitabilityissimi-
for clustering with other agents. As well as featuring a
lartothatoftheothercases.However,thenetworkedagents
non-stationary distribution, we add ‘common noise’ to
significantly outperform the other architectures in terms of
the environment, with the shark taking a random step
average return on both tasks, indicating that the communi-
with probability 0.01. Such noise that affects the local
cationschemehelpsagentstofindsubstantially‘preferable’
states of all agents in the same way, making the evo-
equilibria. Moreover, the margin by which the networked
lution of the distribution stochastic, makes population-
agentscanoutperformthecentralisedagentsismuchgreater
independentpoliciessub-optimal(Laurie`reetal.2022).
thaninBenjaminandAbate(2024),showingthatthebene-
• Pushobjecttoedge.Agentsarerewardedforhowclose fits of the communication scheme are even greater in non-
they are to an ‘object’ in the environment, and for how tabular settings. See Appx. A for further experiments with
close this object is to the edge of the grid. The object’s theselargestatespaces.
position forms part of agents’ local states in addition to
theirownposition.Theobjectmovesinadirectionwith Population-dependentpoliciesincomplexenvironments
aprobabilityproportionaltothenumberofagentsonits Figs.3and4,whereagentsestimatethemean-fielddistribu-
oppositeside,i.e.agentsmustcoordinateonwhichside tionviaAlg.3,differminimallyfromFigs.5and6inAppx.
oftheobjectfromwhichto‘push’it,toensureitmoves A, where agents directly receive the global mean-field dis-
towardtheedgeofthegrid. tribution. This shows that our estimation algorithm allows
agents to appropriately estimate the distribution, even with
In these spatial environments, both the communication
only one round of communication for agents to help each
networkGcommandthevisibilitygraphGvisaredetermined
t t other improve their local counts. Only in the ‘push object’
bythephysicaldistancefromagenti;weshowplotsforvar-
taskinFig.3,andthereonlywiththesmallerbroadcastradii,
ious radii, expressed as fractions of the maximum possible
doagentsslightlyunderperformthereturnsofagentsinthe
distance (the diagonal length of the grid). To demonstrate
globalobservabilitycaseinFig.5,asisreasonable.
the benefits of the networked architecture by comparison,
ForthereasonsgiveninAppx.A.2,theexploitabilitymet-
we also plot the results of modified versions of our algo-
ricgiveslimitedinformationinthe‘pushobject’taskinFig.
rithm for centralised and independent learners. In the cen-
3. In the ‘evade’ task in Fig. 4, exploitability suggests that
tralised setting, the Q-network updates of agent i = 1 are
centralisedlearnersoutperformtheothercases.However,all
automaticallypushedtoallotheragents,andthetrueglobal
ofthenetworkedcasessignificantlyoutperformtheindepen-
mean-field distribution is always used in place of the local
dent learners in terms of the average return to which they
estimate i.e. µ˜ˆi t = µˆ t. In the independent case, there are no convergeinbothtasks.Inthe‘pushobject’tasknetworked
linksinG tcomm orG tvis,i.e.E tcomm = E tvis = ∅.Hyperpa- learnersalsosignificantlyoutperformcentralisedlearnersin
rametersarediscussedinAppx.A.3. all but the case with the smallest broad communication ra-
dius,whileinthe‘evade’taskallnetworkedcases perform
5.1 Results
similarly to the centralised case. Recall though that in the
We evaluate our experiments via two metrics. Exploitabil- realworldacentralisedarchitectureisastrongassumption,
ityisthemostcommonmetricinworksonMFGs,andisa acomputationalbottleneckandsinglepointoffailure.References and Multiagent Systems, AAMAS ’22, 1028–1037. Rich-
land,SC:InternationalFoundationforAutonomousAgents
Algumaei,T.;Solozabal,R.;Alami,R.;Hacid,H.;Debbah,
andMultiagentSystems. ISBN9781450392136.
M.; and Takac, M. 2023. Regularization of the policy up-
datesforstabilizingMeanFieldGames. arXiv:2304.01547. Pe´rolat, J.; Perrin, S.; Elie, R.; Laurie`re, M.; Piliouras, G.;
Geist,M.;Tuyls,K.;andPietquin,O.2022b. ScalingMean
Anahtarci, B.; Kariksiz, C. D.; and Saldi, N. 2023. Q-
Field Games by Online Mirror Descent. In Proceedings of
learninginregularizedmean-fieldgames. DynamicGames
the 21st International Conference on Autonomous Agents
andApplications,13(1):89–117.
and Multiagent Systems, AAMAS ’22, 1028–1037. Rich-
Benjamin, P.; and Abate, A. 2024. Networked Commu- land,SC:InternationalFoundationforAutonomousAgents
nication for Decentralised Agents in Mean-Field Games. andMultiagentSystems. ISBN9781450392136.
arXiv:2306.02766. Perrin,S.;Laurie`re,M.;Pe´rolat,J.;E´lie,R.;Geist,M.;and
Cardaliaguet, P.; Delarue, F.; Lasry, J.-M.; and Lions, P.-L. Pietquin, O. 2022. Generalization in mean field games by
2015. Themasterequationandtheconvergenceproblemin learning master policies. In Proceedings of the AAAI Con-
meanfieldgames. arXiv:1509.02505. ferenceonArtificialIntelligence,volume36,9413–9421.
Carmona,R.;Delarue,F.;andLacker,D.2016. MeanField Perrin,S.;Laurie`re,M.;Pe´rolat,J.;Geist,M.;E´lie,R.;and
Games with Common Noise. The Annals of Probability, Pietquin,O.2021.MeanFieldGamesFlock!TheReinforce-
44(6):3740–3803. mentLearningWay. InIJCAI.
Cui, K.; Fabian, C.; and Koeppl, H. 2023. Multi-Agent Perrin,S.;Pe´rolat,J.;Laurie`re,M.;Geist,M.;Elie,R.;and
Reinforcement Learning via Mean Field Control: Com- Pietquin, O. 2020. Fictitious Play for Mean Field Games:
mon Noise, Major Agents and Approximation Properties. Continuous Time Analysis and Applications. In Proceed-
arXiv:2303.10665. ings of the 34th International Conference on Neural Infor-
mationProcessingSystems,NIPS’20.RedHook,NY,USA:
Cui,K.;andKoeppl,H.2021.ApproximatelySolvingMean
CurranAssociatesInc. ISBN9781713829546.
FieldGamesviaEntropy-RegularizedDeepReinforcement
Saldi,N.;Bas¸ar,T.;andRaginsky,M.2018. Markov–Nash
Learning.
Equilibria in Mean-Field Games with Discounted Cost.
Guo, X.; Hu, A.; Xu, R.; and Zhang, J. 2020. A General
SIAM Journal on Control and Optimization, 56(6): 4256–
FrameworkforLearningMean-FieldGames.
4287.
Hadikhanloo, S. 2017. Learning in anonymous nonatomic Subramanian, J.; and Mahajan, A. 2019. Reinforcement
games with applications to first-order mean field games. LearninginStationaryMean-FieldGames. InProceedings
arXiv:1704.00378. ofthe18thInternationalConferenceonAutonomousAgents
Hu, A.; and Zhang, J. 2024. MF-OML: Online Mean- andMultiAgentSystems,AAMAS’19,251–259.Richland,
Field Reinforcement Learning with Occupation Measures SC: International Foundation for Autonomous Agents and
forLargePopulationGames. arXiv:2405.00282. MultiagentSystems. ISBN9781450363099.
Subramanian,S.G.;Poupart,P.;Taylor,M.E.;andHegde,
Huang,M.;Malhame´,R.P.;andCaines,P.E.2006. Large
N. 2022. Multi Type Mean Field Reinforcement Learning.
populationstochasticdynamicgames:closed-loopMcKean-
arXiv:2002.02513.
Vlasov systems and the Nash certainty equivalence princi-
ple. CommunicationsinInformation&Systems,6(3):221– Subramanian, S. G.; Taylor, M. E.; Crowley, M.; and
252. Poupart, P. 2020. Partially Observable Mean Field Rein-
forcementLearning.
Lasry, J.-M.; and Lions, P.-L. 2007. Mean Field Games.
Subramanian, S. G.; Taylor, M. E.; Crowley, M.; and
JapaneseJournalofMathematics,2(1):229–260.
Poupart,P.2021. DecentralizedMeanFieldGames.
Lauriere,M.;Perrin,S.;Girgin,S.;Muller,P.;Jain,A.;Ca-
Toumi, N.; Malhame, R.; and Le Ny, J. 2024. A mean
bannes, T.; Piliouras, G.; Perolat, J.; Elie, R.; Pietquin, O.;
field game approach for a class of linear quadratic discrete
and Geist, M. 2022. Scalable Deep Reinforcement Learn-
choice problems with congestion avoidance. Automatica,
ing Algorithms for Mean Field Games. In Chaudhuri, K.;
160:111420.
Jegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and Sabato,
uzZaman,M.A.;Koppel,A.;Bhatt,S.;andBas¸ar,T.2023.
S., eds., Proceedings of the 39th International Conference
Oracle-free Reinforcement Learning in Mean-Field Games
on Machine Learning, volume 162 of Proceedings of Ma-
alongaSingleSamplePath. arXiv:2208.11639.
chineLearningResearch,12078–12095.PMLR.
Vieillard,N.;Pietquin,O.;andGeist,M.2020.Munchausen
Laurie`re, M. 2021. Numerical Methods for Mean Field
Reinforcement Learning. In Larochelle, H.; Ranzato, M.;
GamesandMeanFieldTypeControl. arXiv:2106.06231.
Hadsell,R.;Balcan,M.;andLin,H.,eds.,AdvancesinNeu-
Laurie`re, M.; Perrin, S.; Geist, M.; and Pietquin, O. 2022. ralInformationProcessingSystems,volume33,4235–4246.
LearningMeanFieldGames:ASurvey. CurranAssociates,Inc.
Pe´rolat, J.; Perrin, S.; Elie, R.; Laurie`re, M.; Piliouras, G.; Wu,Z.;Lauriere,M.;Chua,S.J.C.;Geist,M.;Pietquin,O.;
Geist,M.;Tuyls,K.;andPietquin,O.2022a. ScalingMean and Mehta, A. 2024. Population-aware Online Mirror De-
Field Games by Online Mirror Descent. In Proceedings of scentforMean-FieldGamesbyDeepReinforcementLearn-
the 21st International Conference on Autonomous Agents ing. arXiv:2403.03552.Xie, Q.; Yang, Z.; Wang, Z.; and Minca, A. 2021. Learn- Technical appendices to ‘Networked Communication
ingWhilePlayinginMean-FieldGames:Convergenceand for Mean-Field Games with Function Approximation
Optimality. InMeila,M.;andZhang,T.,eds.,Proceedings andEmpiricalMean-FieldEstimation’
ofthe38thInternationalConferenceonMachineLearning,
volume139ofProceedingsofMachineLearningResearch, A Experiments
11436–11447.PMLR.
Forreproducibility,thecodetorunourexperimentsispro-
Yang,Y.;Luo,R.;Li,M.;Zhou,M.;Zhang,W.;andWang, vided with ourSupplementary Materials, and will be made
J.2018. MeanFieldMulti-AgentReinforcementLearning. publiclyavailableuponpublication.
InDy,J.;andKrause,A.,eds.,Proceedingsofthe35thIn- Experiments were conducted on a Linux-based machine
ternational Conference on Machine Learning, volume 80 with2xIntelXeonGold6248CPUs(40physicalcores,80
ofProceedingsofMachineLearningResearch,5571–5580. threadstotal,55MiBL3cache).WeusetheJAXframework
PMLR. to accelerate and vectorise our code. Random seeds are set
Yardim, B.; Cayci, S.; Geist, M.; and He, N. 2023. Pol- inourcodeinafixedwaydependentonthetrialnumberto
icyMirrorAscentforEfficientandIndependentLearningin allow easy replication of experiments - please see the code
MeanFieldGames.InInternationalConferenceonMachine filesintheSupplementaryMaterialsfordetails.
Learning,39722–39754.PMLR.
A.1 Games
Yardim, B.; Goldman, A.; and He, N. 2024. When is
Mean-Field Reinforcement Learning Tractable and Rele- Weconductnumericaltestswithfourgames(definedbythe
vant? arXivpreprintarXiv:2402.05757. agents’reward/transitionfunctions),chosenforbeingpartic-
ularly amenable to intuitive and visualisable understanding
Yongacoglu,B.;Arslan,G.;andYu¨ksel,S.2022. Indepen-
ofwhethertheagentsarelearningbehavioursthatareappro-
dent Learning in Mean-Field Games: Satisficing Paths and
priateandexplainablefortherespectiveobjectivefunctions.
ConvergencetoSubjectiveEquilibria.
In all cases, rewards are normalised in [0,1] after they are
computed.
Cluster. This is the inverse of the ‘exploration’ game
in (Lauriere et al. 2022), where in our case agents are
encouraged to gather together by the reward function
R(si,ai,µˆ ) = log(µˆ (si)). That is, agent i receives a re-
t t t t t
ward that is logarithmically proportional to the fraction of
thepopulationthatisco-locatedwithitattimet.Wegivethe
populationnoindicationwheretheyshouldcluster,agreeing
thisthemselvesovertime.
Agree on a single target. Unlike in the above ‘cluster’
game, theagents aregiven optionsof locationsat which to
gather,andtheymustreachconsensusamongthemselves.If
theagentsareco-locatedwithoneofanumberofspecified
targets ϕ ∈ Φ (in our experiments we place one target in
each of the four corners of the grid), and other agents are
alsoatthattarget,theygetarewardproportionaltothefrac-
tion of the population found there; otherwise they receive
a penalty of -1. In other words, the agents must coordinate
onwhichofanumberofmutuallybeneficialpointswillbe
theirsinglegatheringplace.Definethemagnitudeofthedis-
tancesbetweenx,y attasdist (x,y).Therewardfunction
t
isgivenbyR(si,ai,µˆ )=r (r (µˆ (si))),where
t t t targ collab t t
(cid:26) x if∃ϕ∈Φs.t.dist (si,ϕ)=0
r (x)= t t
targ −1 otherwise,
(cid:26) x ifµˆ (si)>1/N
r (x)= t t
collab −1 otherwise.
Evade shark in shoal. Define the magnitude of the hori-
zontalandverticaldistancesbetweenx,yattasdisth(x,y)
t
and distv(x,y) respectively. The state si now consists of
t t
agent i’s position xi and the ‘shark’s’ position ϕ . At each
t t
time step, the shark steps towards the most populated grid
point x∗ = argmax µˆ (x). A horizontal step is taken
t x∈S tif disth(ϕ ,x∗) ≥ distv(ϕ ,x∗), otherwise a vertical step Definition9(Exploitabilityofπ) TheexploitabilityExof
t t t t t t
is taken. Additionally, the shark moves in a random direc- policyπisgivenby:
tionwithprobability0.01.Agentsarerewardedmoreforbe-
Ex(π)=V(BR(I(π)),I(π))−V(π,I(π)).
ingfurtherfromtheshark,andalsoforclusteringwithother
agents.Therewardfunctionisgivenby If π has a large exploitability then an agent can signifi-
cantlyimproveitsreturnbydeviatingfromπ,meaningthat
R(si,ai,µˆ )=disth(ϕ ,xi)+
t t t t t t π isfarfromπ∗,whereasanexploitabilityof0impliesthat
distv t(ϕ t,xi t)+norm dist(log(µˆ t(xi t))), π =π∗.Priorworksconductingempiricaltestinghavegen-
wherenorm (·)indicatesthatthefinaltermisnormalised erally focused on the centralised setting, so this classical
dist
tohavethesamemaximumandminimumvaluesasthetotal definition, as well as most evaluations, only consider ex-
combinedverticalandhorizontaldistance. ploitabilitywhenallagentsarefollowingasinglepolicyπ k.
However, Benjamin and Abate (2024) notes that purely in-
Push object to edge. As before, define the magnitude dependent agents, as well as networked agents, may have
of the horizontal and vertical distances between x,y at t divergentpoliciesπi ̸= πj ∀i,k ∈ 1,...,N,asinourown
as disth(x,y) and distv(x,y) respectively. The state si k k
t t t setting.Wethereforeareinterestedinthe‘exploitability’of
consistsofagenti’spositionxi andtheobject’spositionϕ .
t t thepopulation’sjointpolicyπ:=(π1,...,πN)∈ΠN.
The number of agents in the positions surrounding the ob-
SincewedonothaveaccesstotheexactBRpolicyasin
jectattimetgeneratesaprobabilityfieldaroundtheobject,
some related works (Lauriere et al. 2022; Wu et al. 2024),
such that the object is most likely to move in the direction
wemustinsteadapproximatetheexploitability,similarlyto
awayfromthesidewiththemostagents.Assuch,ifagents
(Perrin et al. 2021; Benjamin and Abate 2024). We freeze
are equally distributed around the object, it will be equally
the policy of all agents apart from a deviating agent, for
likely to move in any direction, but if they coordinate on
which we store its current policy and then conduct 50 k
choosing the same side, they can ‘push’ it in a certain
loops of policy improvement. To approximate the expecta-
direction.IfEdges={edge1,...,edge4}arethegridedges,
tionsinDef.9,wetakethebestreturnofthedeviatingagent
theclosestedgetotheobjectattimetisgivenbyedge∗ =
argmin (cid:0) min(disth(ϕ ,edge),disth(ϕ ,edge)t(cid:1) . across 10 additional k loops, as well as the mean of all the
edge∈Edges t t t t otheragents’returnsacrossthesesame10loops.(Whilethe
Agents are rewarded for how close they are to the object,
policies of all non-deviating agents is π in the centralised
andforhowclosetheobjectistotheedgeofthegrid.The k
case,ifthenon-deviatingagentsdonotshareasinglepolicy,
rewardfunctionisgivenby
then this method is in fact approximating the exploitability
R(si t,ai t,µˆ t)=disth t(ϕ t,xi t)+distv t(ϕ t,xi t)+ oftheirjointpolicyπ− kd,wheredisthedeviatingagent.)We
disth(ϕ ,edge∗)+distv(ϕ ,edge∗). thenreverttheagentbacktoitsstoredpolicy,beforelearn-
t t t t t t ingcontinuesforallagentsasperthemainalgorithm.Due
Diffuse. We conduct an additional experiment not men-
to the expensive computations required for this metric, we
tionedinthemaintext,whichissimilartothe‘exploration’
evaluate it every second k iteration of the main algorithm
tasks in Lauriere et al. (2022), Wu et al. (2024) and other
for Figs. 1, 2, 7, 8 and 9, and every fourth iteration for the
MFG works. In our version agents are rewarded for being
population-dependentexperiments.
locatedinmoresparselypopulatedareasbutonlyiftheyare
The exploitability metric has a number of limitations in
stationary.TherewardfunctionisgivenbyR(si,ai,µˆ ) =
t t t oursetting.OurapproximationtakesplaceviaMOMDpol-
r (−µˆ (si)),where
stationary t t icyimprovementsteps(asinthemainalgorithm)forainde-
(cid:26) x ifai is‘remainstationary’ pendent,deviatingagentwhilethepoliciesoftherestofthe
r stationary(x)=
−1
othet
rwise.
population are frozen. As such, the quality of our approxi-
mationislimitedbythenumberofpolicyimprovement/ex-
A.2 Experimentalmetrics pectation rounds, which must be restricted for the sake of
running speed of the experiments. Moreover, since one of
Togiveasinformativeresultsaspossibleaboutbothperfor-
the findings of our paper is that networked agents can im-
manceandproximitytotheMFNE,weprovidetwometrics
prove their policies faster than independent or centralised
foreachexperiment.Bothmetricsareplottedwithmeanand
agents, especially when non-linear function approximation
standarddeviation,computedoverthetentrials(eachwitha
is used, it is arguably unsurprising that approximating the
randomseed)ofthesystemevolutionineachsetting.
BR by an independently deviating agent sometimes gives
Exploitability Works on MFGs most commonly use the anunclearandnoisymetric.Thisincludestheexploitability
exploitability metric to evaluate how close a given policy goingbelowzero,whichshouldnotbepossibleifthepoli-
π is to a NE policy π∗ (Lauriere et al. 2022; Perrin et al. ciesanddistributionsarecomputedexactly.
2020; Laurie`re et al. 2022; Algumaei et al. 2023; Pe´rolat Moreover, in coordination games (the setting on which
et al. 2022b; Benjamin and Abate 2024; Wu et al. 2024). we focus), agents benefit by following the same behaviour
Themetricusuallyassumesthatallagentsarefollowingthe as others, and so a deviating agent generally stands to gain
samepolicyπ,andquantifieshowmuchanagentcanbenefit lessfromaBRpolicythanitmightinthenon-coordination
bydeviatingfromπbymeasuringthedifferencebetweenthe gamesonwhichmanyotherworksfocus.Forexample,the
returngivenbyπandthatofaBRpolicywithrespecttothe returnofabest-respondingagentinthe‘pushobject’game
distributionindicatedbyπ: stilldependsontheextenttowhichotheragentscoordinateonwhichdirectioninwhichtopushthebox,meaningitcan-
notsignificantlyincreaseitsreturnbydeviating.Thismeans
that the downward trajectory of the exploitability metric is
lessclearinourplotsthaninotherworks.Thisislikelywhy
theapproximatedexploitabilitygetslowerinthe‘disperse’
task in Fig. 8 than in the other tasks. Given the limitations
presented by approximating exploitability, we also provide
thesecondmetrictoindicatetheprogressoflearning.
Average discounted return We record the average dis-
countedreturnoftheagents’policiesπi duringtheM iter-
k
ations-thisallowsustoobservethatsettingsthatconverge
to similar exploitability values may not have similar aver-
ageagentreturns,suggestingthatsomealgorithmsarebetter
thanothersatfindingnotjustNE,butpreferableNE.Seefor
exampleFigs.1and7,wherethenetworkedagentsconverge
to similar exploitability as the independent and centralised
Figure5:‘Pushobject’task,population-dependentpolicies
agents,butreceivehigheraveragereturns.
with global observability of mean-field distribution, 10x10
grid.
A.3 Hyperparameters
See Table 1 for our hyperparameter choices. We can group
ourhyperparametersintothosecontrollingthesizeoftheex-
periment,thosecontrollingthesizeoftheQ-network,those
controlling the number of iterations of each loop in the al-
gorithms and those affecting the learning/policy updates or
policyadoption.
Inourexperimentswegenerallywanttodemonstratethat
our communication-based algorithms outperform the cen-
tralised and independent architectures by allowing policies
that are estimated to be better performing to proliferate
throughthepopulation,suchthatconvergenceoccurswithin
fewer iterations and computationally faster, even when the
Q-functionispoorlyapproximatedand/orthemean-fieldis
poorly estimated, as is likely to be the case in real-world
scenarios.Moreoverwewanttoshowthatthereisabenefit
eventoasmallamountofcommunication,sothatcommu-
nication rounds themselves do not excessively add to time
complexity. As such, we generally select hyperparameters Figure 6: ‘Evade’ task, population-dependent policies with
atthelowestendofthosewetestedduringdevelopment,to globalobservabilityofmean-fielddistribution,10x10grid.
show that our algorithms are particularly successful given
whatmightotherwisebeconsidered‘undesirable’hyperpa-
rameterchoices. radii outperform them in terms of average return, indicat-
ingthatthecommunicationschemehelpspopulationsreach
A.4 Additionalexperiments betterperformingequilibria.
We provide additional experiments on large grids in Figs. In the additional ‘disperse’ task in Fig. 8, networked
7,8and9.Inthe‘targetagreement’taskinFig.7,thenet- agentssignificantlyoutperformindependentandcentralised
worked agents all significantly outperform both centralised agentsintermsofaveragereturn.Theyalsooutperformcen-
and independent agents in term of average return, despite tralised agents in terms of exploitability, and significantly
theindependentagentsappearingtohaveslightlylowerex- outperformindependentagentsintermsofexploitability.
ploitability. This is because independent agents are hardly
Explanationofbetterperformancebynetworkedagents
improvingtheirpoliciesatall,suchthatthereislittleadevi-
than centralised agents By adopting policy parameters
atingagentcandotoincreaseitsreturninthiscoordination
πj from neighbours with probability related to their es-
game,suchthatexploitabilityappearslow,despitethisbeing k+1
an undesirable equilibrium (see Appx. A.2 for further dis- timatedvalueσj ,networkedagentscanadoptparameters
k+1
cussiononthelimitedinformationprovidedbytheexploita- that are estimated to be better performing than their own,
tion metric). In the ‘cluster’ task in Fig. 9, the networked whichintuitivelymeansnetworkedagentsimprovetheirre-
agents obtain significantly higher return than the indepen- turn quicker than agents learning entirely independently.
dent agents. While centralised agents have the lowest ex- Moreover,sincethecentralisedcaseinvolvesasinglecentral
ploitability, networked agents of almost all communication agenttrainingitsQ-networkandthenpushingitsupdateau-Figure 7: ‘Target agreement’ task, population-independent Figure 9: ‘Cluster’ task, population-independent policies,
policies,100x100grid. 50x50grid.
broader survey of MFGs. Our work is most closely related
toBenjaminandAbate(2024),whichintroducednetworked
communication to the infinite-horizon MFG setting. How-
ever, this work focuses only on tabular settings rather than
usingfunctionapproximationasinours,andonlyaddresses
population-independentpolicies.
Lauriere et al. (2022) uses Munchausen Online Mirror
Descent (MOMD), similar to our method for learning with
neural networks, but there are numerous differences to our
setting:mostrelevantly,theystudyafinite-horizonepisodic
setting, where the mean-field distribution is updated in
an exact way and an oracle supplies a centralised learner
with rewards and transitions for it to learn a population-
Figure 8: ‘Disperse’ task, population-independent policies, independent policy. Wu et al. (2024) uses MOMD to learn
100x100grid. population-dependentpolicies,albeitalsowithacentralised
method that exactly updates the mean-field distribution in
a finite-horizon episodic setting. Perrin et al. (2022) learns
population-dependentpolicieswithfunctionapproximation
tomaticallytoalltheothers,thissinglesetofupdatedparam-
ininfinite-horizonsettingslikeourown,butdoessoinacen-
eterswillbetakenonbyallagentswhetherornottheyare
tralised, two-timescale manner without using the empirical
actually particularly well performing. Since the networked
mean-fielddistribution.
agents are more likely to select better performing parame-
ters out of a variety of options according to our algorithm,
Yongacoglu,Arslan,andYu¨ksel(2022)addressesdecen-
wefindnetworkedagentsusuallyinfactlearnquicker than
tralisedlearningfromacontinuous,non-episodicrunofthe
centralisedagents,andindeedthebenefitisevengreaterin
empirical system using either full or compressed informa-
the function approximation setting than in the tabular case
tion about the mean-field distribution, but agents are as-
given in Benjamin and Abate (2024). It is significant that
sumed to receive this information directly, rather than es-
ourcommunicationschemenotonlyallowsustoavoidthe
timatingitlocallyasinthealgorithmwenowpresent.They
undesirableassumptionofacentralisedlearner,butevento
also do not consider function approximation or inter-agent
outperformit.
communicationintheiralgorithms.Inthecloselyrelatedbut
distinct area of Mean-Field RL, Subramanian et al. (2020)
B Relatedwork
doesestimatetheempiricalmean-fielddistributionfromthe
MFGsareaquicklygrowingresearcharea,soweonlydis- localneighbourhood,howeveragentsareseekingtoestimate
cusstheworksmostcloselyrelatedtothispresentwork,and themeanactionratherthanthemean-fielddistributionover
instead refer the reader to Benjamin and Abate (2024) for statesasinourMFGsetting.Theiragentsalsodonothave
detailed discussion around the setting of networked com- access to a communication network by which they can im-
munication for MFGs, and to Laurie`re et al. (2022) for a provetheirestimates.C Extensionsandfuturework approximationsetting,forfuturework.
Alg.3,describedinSec.4assumesthatifastates′ iscon-
nectedtosonthevisibilitygraphGvis,anagentinsisable D Additionalremarks
t
to accurately count all the agents in s′, i.e. it either counts In our Algs. 2 and 3, agents share their local counts with
the exact total or cannot observe the state at all. While we neighboursonthecommunicationnetworkGcomm,andonly
t
assume this for simplicity, this is not inherently the case, after the C communication rounds do they complete their
e
since a real-world agent may have only noisy observations estimated distribution by distributing the uncounted agents
evenofotherslocatednearby,duetoimperfectsensors.We along their vectors. An alternative would be for each agent
suggest two ways to deal with this case. Firstly, if agents toimmediatelyformalocalestimatefromtheirlocalcount
share unique IDs as in Alg. 2, then when communicating obtainedviaGobsorGvis,whichisonlythencommunicated
t t
their vectors of collected IDs with each other via Gcomm, and updated via the communication network. However, we
t
agents would gain the most accurate picture possible of all take the former approach to avoid poor local estimations
the agents that have been observed in a given state. How- spreadingthroughthenetworkandleadingtowidespreadin-
ever,aswenoteinthemainbodyofthepaper,therearevari- accuracies. Information that is certain (the count) is spread
ousreasonswhysharingIDsmightbeundesirable,including as widely as possible, before being locally converted into
privacyandscalability.Ifinsteadonlycountsaretaken,and an estimate of the total mean field. The same would be the
ifthenoiseoneachagents’countisassumedtobeindepen- case in our proposed extension for averaging noisy counts
dentand,forexample,subjecttoaGaussiandistribution,the i.e. only the counts would be averaged, with the estimates
algorithm can easily be updated such that communicating completedbydistributingtheremainingagentsaftertheC
e
agents compute averages of their local and received counts communicationrounds.
toimprovetheiraccuracy,ratherthansimplyusingcommu-
nication to fill in counts for previously unobserved states.
(Note that we can also consider the original case without
noisetoinvolveaveraging,sinceaveragingidenticalvalues
equates to using the original value). Since the algorithm is
intended to aid in local estimation of the mean-field distri-
bution,whichisinherentlyapproximateduetotheuniform
method for distributing the uncounted agents, we are not
concernedwithreachingexactconsensusbetweenagentson
the communicated counts, such that we do not require re-
peatedaveragingtoensureasymptoticconvergence.
We may also wish to consider more sophisticated meth-
ods for distributing the uncounted agents across states, in
placeoftheuniformdistributionweusenow.Suchchoices
maybedomain-specificbasedonknowledgeofaparticular
environment.Forexample,onemightinfactusethecounts
toperformBayesianupdatesonaspecificprior,wherethis
prior may relate to the estimated mean-field distribution at
theprevioustimestept−1.Ifagentsseektolearntopredict
theevolutionofthemeanfieldbasedontheirownpolicyor
by learning a model, the Bayesian prior may also be based
onforwardpredictionfromtheestimatedmean-fielddistri-
butionatt−1.Futureworkliesinconductingexperiments
inallofthesemoregeneralsettings.
Perrinetal.(2022)notesthatingrid-worldsettingssuch
as those in our experiments, passing the (estimated or true
global) mean-field distribution as a flat vector to the Q-
network ignores the geometric structure of the problem.
They therefore propose to create an embedding of the dis-
tribution by first passing the vector to a convolutional neu-
ral network, essentially treating the categorical distribution
as an image. This technique is also followed in (Wu et al.
2024)(fortheiradditionalexperiments,butnotinthemain
body of their paper). As future work, we can test whether
suchamethodimprovestheperformanceofouralgorithms.
As in the closely-related work by Wu et al. (2024), we
conductextensivenumericalexperimentstodemonstratethe
benefits of our algorithm over baselines, and leave the the-
oreticalanalysis,suchproofofconvergenceinthefunctionHyperparam. Value Comment
Trials 10 Werun10trialswithdifferentrandomseedsforeachexperiment.Weplotthemeanandstandard
deviationforeachmetricacrossthetrials.
Gridsize 10x10 / Experimentswithpopulation-dependentpoliciesarerunonthe10x10grid(Figs.3,4,5and6),
50x50 / whileexperimentsonlargestatespacesarerunon50x50and100x100grids(Figs.1,2,7,8and
100x100 9).
Population 500 Wechose500forourdemonstrationstoshowthatouralgorithmcanhandlelargepopulations,
indeedoftenlargerthanthosedemonstratedinothermean-fieldworks,especiallyforgrid-world
environments,whilealsobeingfeasibletosimulatewrt.timeandcomputationconstraints(Yon-
gacoglu,Arslan,andYu¨ksel2022;CuiandKoeppl2021;Cui,Fabian,andKoeppl2023;Guo
etal.2020;SubramanianandMahajan2019;Yangetal.2018;Subramanianetal.2020,2022,
2021;Wuetal.2024;BenjaminandAbate2024).
Number of cf. Theagent’spositionisrepresentedbytwoconcatenatedone-hotvectorsindicatingtheagent’s
neurons in com- rowandcolumn.Anadditionaltwosuchvectorsareaddedfortheshark’s/object’spositioninthe
inputlayer ment ‘evade’and’pushobject’tasks.Forpopulation-dependentpolicies,themean-fielddistribution
isaflattenedvectorofthesamesizeasthegrid.Assuch,theinputsizeinthe‘evade’and’push
object’tasksis[(4×dimension)+(dimension2)];intheothersettingsitis[2×dimension].
Neurons cf. We draw inspiration from common rules of thumb when selecting the number of neurons in
per hidden com- hidden layers, e.g. it should be between the number of input neurons and output neurons / it
layer ment shouldbe2/3thesizeoftheinputlayerplusthesizeoftheoutputlayer/itshouldbeapower
of2forcomputationalefficiency.Usingtheserulesofthumbasroughheuristics,weselectthe
numberofneuronsperhiddenlayerbyroundingthesizeoftheinputlayerdowntothenearest
powerof2.Thelayersareallfullyconnected.
Hidden lay- 2 We experimented with 2 and 3 hidden layers in the Q-networks. While 3 hidden layers gave
ers similarorslighlybetterperformance,weselected2forincreasedcomputationalspeedforcon-
ductingourexperiments.
Activation ReLU ThisisacommonchoiceindeepRL.
function
K 100 K ischosentobelargeenoughtoseeatleastoneofthemetricsconverging.
M 50 WetestedM in{50,100}andfoundthatthelowervaluewassufficienttoachieveconvergence
whileminimisingtrainingtime.ItmaybepossibletoconvergewithevensmallerchoicesofM.
L 50 WetestedLin{50,100}andfoundthatthelowervaluewassufficienttoachieveconvergence
whileminimisingtrainingtime.ItmaybepossibletoconvergewithevensmallerchoicesofL.
E 20 We tested E in {20,50,100}, and choose the lowest value to show the benefit to convergence
even from very few evaluation steps. It may be possible to reduce this value further and still
achievesimilarresults.
C 1 As in Benjamin and Abate (2024), we choose this value to show the convergence benefits
p
brought by even a single communication round, even in networks that may have limited con-
nectivity;higherchoicesarelikelytohaveevenbetterperformance.
C 1 SimilartoC ,wechoosethisvaluetoshowtheabilityofouralgorithmtoappropriatelyestimate
e p
the mean field even with only a single communication round, even in networks that may have
limitedconnectivity.
γ 0.9 StandardchoiceacrossRLliterature.
τ 0.03 Wetestedτ in{0.01,0.02,0.03,0.04,0.05},aswellaslinearlydecreasingτ from0.05→0as
q q q
k increases. However, only 0.03 gave stable increase in return. Note that this is the value also
choseninVieillard,Pietquin,andGeist(2020).
|B| 32 Thisisacommonchoiceofbatchsizethattradesoffnoisyupdatesandcomputationalefficiency.
cl -1 WeusethesamevalueasinVieillard,Pietquin,andGeist(2020).
ν L−1 We tested ν in {1,4,20,L − 1}. We found that in our setting, updating θ′ ← θ once per k
iteration s.t. θ′ = θ ∀l gave sufficient learning that was similar to the other potential
k+1,l k,l
choicesofν,sowedothisforsimplicity,ratherthanarbitrarilychoosingafrequencytoupdate
θ′ during each k loop. Setting the target to be the policy from the previous iteration is similar
tothemethodinLauriereetal.(2022).WhilstWuetal.(2024)updatesthetargetwithintheL
loopsforstability,wedonotfindthistobeaprobleminourexperiments.
Optimiser Adam AsinVieillard,Pietquin,andGeist(2020),weusetheAdamoptimiserwithinitiallearningrate
0.01.
τ cf. τ increases linearly from 0.001 to 1 across the K iterations. This is a simplification of the
k k
com- annealingschemeusedinBenjaminandAbate(2024).Furtheroptimisingtheannealingprocess
ment mayleadtobetterresults.
Table1:Hyperparameters