Deviations from the Nash equilibrium and emergence of tacit collusion
in a two-player optimal execution game with reinforcement learning
Fabrizio Lillo1,2∗ and Andrea Macrì1†
1 Scuola Normale Superiore, Pisa, Italy
2 Dipartimento di Matematica University of Bologna, Bologna, Italy
August 22, 2024
Abstract
Theuseofreinforcementlearningalgorithmsinfinancialtradingisbecomingincreasinglypreva-
lent. However, the autonomous nature of these algorithms can lead to unexpected outcomes that
deviate from traditional game-theoretical predictions and may even destabilize markets. In this
study, we examine a scenario in which two autonomous agents, modeled with Double Deep Q-
Learning, learn to liquidate the same asset optimally in the presence of market impact, using the
Almgren-Chriss (2000) framework. Our results show that the strategies learned by the agents de-
viate significantly from the Nash equilibrium of the corresponding market impact game. Notably,
the learned strategies exhibit tacit collusion, closely aligning with the Pareto-optimal solution. We
further explore how different levels of market volatility influence the agents’ performance and the
equilibriatheydiscover,includingscenarioswherevolatilitydiffersbetweenthetrainingandtesting
phases.
1 Introduction
Theincreasingautomationoftradingoverthepastthreedecadeshasprofoundlytransformedfinancial
markets. The availability of large, detailed datasets has facilitated the rise of algorithmic trading—a
sophisticated approach to executing orders that leverages the speed and precision of computers over
human traders. By drawing on diverse data sources, these automated systems have revolutionized
how trades are conducted. In 2019, it was estimated that approximately 92% of trading in the for-
eign exchange (FX) market was driven by algorithms as reported in [1]. The rapid advancements
in Machine Learning (ML) and Artificial Intelligence (AI) have significantly accelerated this trend,
leading to the widespread adoption of autonomous trading algorithms. These systems, particularly
those based on Reinforcement Learning (RL), differ from traditional supervised learning models. In-
stead of being trained on labeled input/output pairs, RL algorithms explore a vast space of potential
strategies, learning from past experiences to identify those that maximize long-term rewards. This
approach allows for the continuous refinement of trading strategies, further enhancing the efficiency
and effectiveness of automated trading.
The flexibility of RL comes with significant potential costs, particularly due to the opaque, black-
boxnatureofthesealgorithms. Thisopacitycanleadtounexpectedoutcomesthatmaydestabilizethe
system they control—in this case, financial markets—through the actions they take, such as executing
trades. Thecomplexityandriskincreasefurtherwhenmultipleautonomousagentsoperatesimultane-
ously within the same market. The lack of transparency in RL algorithms may result in these agents
inadvertently learning joint strategies that deviate from the theoretical Nash equilibrium, potentially
leading to unintended market manipulation. Among the various risks, the emergence of collusion is
particularly noteworthy. Even without explicit instructions to do so, RL agents may develop coop-
erative strategies that manipulate the market, a phenomenon known as tacit collusion. This type
of emergent behavior is especially concerning because it can arise naturally from the interaction of
multiple agents, posing a significant challenge to market stability and fairness.
In this paper, we investigate the equilibria and the emergence of tacit collusion in a market where
twoautonomousagents, drivenbydeepRL,engageinanoptimalexecutiontask. Whiletacitcollusion
∗fabrizio.lillo@sns.it
†andrea.macri@sns.it
1
4202
guA
12
]RT.nif-q[
1v37711.8042:viXrain RL settings has been explored in areas such as market making (as reviewed below), much less is
known about how autonomous agents behave when trained to optimally trade a large position. We
adopt the well-established Almgren-Chriss framework first introduced in [2], for modeling market
impact, focusing on two agents tasked with liquidating large positions of the same asset. The agents
are modeled using Double Deep Q-Learning (DDQL) and engage in repeated trading to learn the
optimal equilibrium of the associated open-loop game.
Forthisspecificsetup,theNashequilibriumofthegamehasbeenexplicitlyderivedin[3],providing
a natural benchmark against which we compare our numerically derived equilibria. In addition, we
explicitly derive the Pareto-optimal strategy and numerically characterize the Pareto-efficient set of
solutions. Our primary goal is to determine whether the two agents, without being explicitly trained
to cooperate or compete, can naturally converge to a collusive equilibrium, given the existence and
uniqueness of the Nash equilibrium.
OurfindingsrevealthatthestrategieslearnedbytheRLagentsdeviatesignificantlyfromtheNash
equilibrium of the market impact game. Across various levels of market volatility, we observe that
the learned strategies exhibit tacit collusion, closely aligning with the Pareto-optimal strategy. Given
that financial market volatility is time-varying, we also examine the robustness of these strategies
when trained and tested under different volatility regimes. Remarkably, we find that the strategies
learned in one volatility regime remain collusive even when applied to different volatility conditions,
underscoring the robustness of the training process.
Literature review. Optimal execution has been extensively studied in the financial literature.
Starting from the seminal contributions in [4] and in [2], many authors have contributed to extend
the model’s consistency with reality. Notable examples in this area are: [5]–[13]. In its basic setting,
the optimal execution problem considers just one agent unwinding or acquiring a quantity q of assets
0
within a certain time window. However, if many other agents are considered to be either selling or
buying, thus pursuing their own optimal execution schedules in the same market, the model becomes
more complicated since it now requires modelling other agents’ behaviour the same market. Using a
large dataset of real optimal execution, [14] shows that the cost of an execution strongly depends on
the presence of other agents liquidating the same asset. The increased complexity when treating the
problem in this way allows for more consistency with reality and, at the same time, opens the path
for further studies on how the agents interact with each other in such a context.
The optimal execution problem with n-agents operating in the same market has thus been studied
under a game theoretic lens in both its closed-loop (in [15], [16]) and open-loop (in [3], [17]–[20])
formulations. In more recent works, rather than just optimal execution, liquidity competition is also
analysed (as in [21], [22]) along with market making problems (as in [23], [24]).
In recent times, with the advancements of machine learning techniques, the original optimal exe-
cution problem has been extensively studied using RL techniques. Among the many1, some examples
in the literature on RL techniques applied to the optimal execution problem are found in [27]–[29].
Applications of multi-agent RL to financial problems are not incredibly numerous if compared
to those that study the problem in a single agent scenario. Optimal execution in a multi-agent
environment is tackled in [30], where the authors analyse the optimal execution problem form the
standpoint of a many agents environment and an RL agent trading on Limit Order Book. The
authors test and develop a multi-agent Deep Q-learning algorithm able to trade on both simulated
and real order books data. Their experiments converge to agents who adopt only the so-called Time
WeightedAveragePrice(TWAP)strategiesinbothcases. Ontheotherhand,stillonoptimalexecution
problem’s applications of multi-agent RL, the authors in [31] analyse the interactions of many agents
on their respective optimal strategies under the standpoint of cooperative competitive behaviours,
adjusting the reward functions in the Deep Deterministic Policy Gradient algorithm used, in order to
allow for either of the two phenomena, but still using the basic model introduced in [2], and modelling
the interactions of the agents via full reciprocal disclosure of their rewards and not considering the
sum of the strategies to be a relevant feature for the permanent impact in the stock price dynamics.
The existence of collusion and the emergence of collusive behaviours are probably the most in-
teresting behaviours based on agents interaction in a market, as these are phenomena that might
naturally arise in markets even if no instruction on how to and whether to collude or not, are given to
themodelledagents. Theemergingoftacitcollusivebehaviourshasbeenanalysedinvariouscontexts.
1For a more comprehensive overview of the state of the art on RL methods in finance, please refer to [25] and [26]
2One of the first interesting examples is found in [32], where using Q-learning the authors show how
competing producers in a Cournot oligopoly learn how to increase prices above Nash equilibrium by
reducing production, thus firms learn to collude even if usually do not learn to make the highest possi-
ble joint profit. Hence the firms converge to a ‘collusive’ behaviour rather than to a ‘proper’ collusion
equilibrium. StillinCournotoligopoly, authorsin[33]applymulti-agentQ-learninginelectricitymar-
kets, the evidences of a collusive behaviour do still arise and the authors postulate that the collusive
behaviour may arise from imperfect exploration of the agents in such a framework. Similarly, authors
in [34] conclude that the use of deep RL leads to collusive behaviours faster than simulated tabular
Q-learningalgorithms. Forwhatconcernsfinancialmarkets, theproblembecomesmoreinvolvedsince
the actors in market do not directly set the price in a ‘one sided’ way as is the case for production
economies that are studied in the previously cited works. Among the many contributions that study
the emergence of collusive behaviours in financial markets, in [35] it is shown how tacit collusion arises
between market makers, modelled using deep RL, in a competitive market. In [36] the authors show
how market making algorithms tacitly collude to extract rents, and this behaviour strictly depends on
tick size and coarseness of price grid. Finally, in [24] the authors use a multi-agent deep RL algorithm
to model market makers competing in the same market, the authors show how competing market
makers learn how to adjust their quotes, giving rise to tacit collusion by setting spread levels strictly
above the competitive equilibrium level.
Thepaperisorganisedasfollows: Section2introducesthemarketimpactgametheoreticalsetting,
Section 3 introduces the DDQL algorithm for the multi-agent optimal execution problem, Section 4
discusses the results for different parameter settings, and finally, Section 5 provides conclusions and
outlines further research directions.
2 Market impact game setting
The Almgren-Chriss model. ThesetupofourframeworkisbasedontheseminalAlmgren-Chriss
model first introduced in [2] for optimal execution. In this setting, a single agent wants to liquidate
an initial inventory of q shares within a time window [0,T], which, in the discrete-time setting, is
0
divided into N equal time increments of length τ =T/N. The main assumption of the model is that
the mid-price (or the efficient price) evolves according to a random walk with a drift depending on the
traded quantity. Moreover, the price obtained by the agent differs from the mid-price by a quantity
which depends on the quantity traded in the interval. More formally, let S and S˜ be the mid-price
t t
and the price received by the agent at time t, and let v be the number of shares traded by the agent
t
in the same interval; then the dynamics is given by
S t=S t−1−g(v t/τ)τ+στ1 2ξ
(1)
S˜ =S −h(v /τ)
t t−1 t
S evolves because of a diffusion part ξ∼N(0,1) multiplied by the price volatility σ and a drift part,
t
termed permanent impact, g (v /τ), assumed to be linear and constant: g(v /τ)=κv /τ. The price
t t t t
S˜ received by the agent is equal to the mid-price S but impacted by a temporary impact term also
t t
assumed to be linear and constant in time: h(v /τ)=αv /τ.
t t
The aim of the agent is to unwind their initial portfolio maximizing the cash generated by their
trading over the N time steps. This objective can be rewritten in terms of Implementation Shortfall
(IS) that is defined in the single agent case as:
N
IS(⃗v)=S q −X S˜ v (2)
0 0 t t
t=1
where ⃗v = (v ,...,v )′ ∈ RN is the vector containing the traded quantity in each time step. The
1 N
optimisation problem of the agent can be written as
N
min E[IS(⃗v)]−λV[IS(⃗v)] s.t. X v =q (3)
t 0
⃗v
t=1
whereλistheriskaversionparameteroftheagent. Underthelinearityassumptionofthetwoimpacts,
the problem can be easily solved analytically. In the following, we are going to consider risk neutral
3agents (λ=0) and in this case the optimal trading schedule is the TWAP, i.e. v =q /N, where the
t 0
trading velocity is constant.
Almgren-Chriss market impact game. Now we consider two agents selling an initial inventory
q shares within the same time window time window [0,T]. The traded quantities by the two agents
0
in the time step t are indicated with v(1) and v(2) and V =v(1)+v(2) is the total quantity traded in
t t t t t
the same interval. The equations for the dynamics are
S t=S t−1−g(V t/τ)τ+στ1 2ξ
(4)
S˜(k)=S −h(cid:16) v(k) /τ(cid:17) k=1,2
t t−1 t
i.e. the mid-price is affected by the total traded volume V , while the price received by each agents
t
depend on the quantity she trades.
Since more than one agent is optimising her trading and the cash received depends on the trading
activity of the other agent, the natural setting to solve this problem is the one of game theory. Since
each agent is not directly aware of the selling activity of the other agent and the two agents interact
through their impact on the price process, the resulting problem is an open-loop game. The existence
and uniqueness of a Nash equilibrium in such a symmetric open-loop game has been studied in [3] in
thecaseofn differentplayersandlinearimpactfunctions. Firstofall,thedefinedtheNashequilibrium
as:
Definition 1 (Nash equilibrium ([3])). In an n players game, where n ∈ N, with q(1) ,...,q(n) ∈ R
0 0
initial inventory holdings and λ ,...,λ non-negative coefficients of risk aversion. A Nash equilibrium
1 n
for mean-variance optimisation as in Eq. (3) is a collection of q∗ ={⃗q∗,...,⃗q∗} inventory holdings
1 n
such that for each k∈n, ⃗q ∈A the mean variance functional is minimised:
k det
E[IS(⃗q |q∗ )]−λ V[IS(⃗q |q∗ )]
k −k k k −k
for each agent k.2 Where q∗ are the strategies of the other players minus the strategy of the k-agent
−k
that is being considered.
Then they proved that there exists a unique Nash equilibrium for the mean-variance optimisation
problem. The unique Nash equilibrium strategy q∗(k), the Nash remaining inventory of agent k at
t
time t, for n players is given as a solution to the second-order system of differential equations:
λ(k)σ2q(k) −2ατ¨q(k)=κX q˙(k)+ατX¨q(k) (5)
t t t t
j̸=k j̸=k
with two-point boundary conditions
q(k)=q and q(k)=0, ∀k=1,...,n (6)
0 0 T
Focusing on the special case of two players, in [3] it is proved that the selling schedule at the unique
Nash equilibrium is:
1 1
q∗(1)= (Σ +∆ ) and q∗(2)= (Σ −∆ ) (7)
t 2 t t t 2 t t
where:
(cid:18) √ (cid:19) (cid:18) √ (cid:19)
sinh (N−t) κ2+12αλσ2 sinh (N−t) κ2+4αλσ2
Σ t=V te− 6κ αt
(cid:16) √
6α
(cid:17)
and ∆ t=V te2κ αt
(cid:16) √
2α
(cid:17)
(8)
sinh N κ2+12αλσ2 sinh N κ2+4αλσ2
6α 2α
ItcanbenoticedthattheNashinventoryholdings, andthusthetradingrates, nowdependalsoonthe
permanent impact κ, contrary to what happens in the single agent case studied in [2]. The open loop
setting of the game excludes that the agents have knowledge of each others inventory holdings and, as
said above, they interact only through the permanently-impacted price S . The price level does not
t
enter directly into the optimal inventory formula, but the permanent impact and the volatility of the
asset on the other hand do, proportionally to agents’ risk aversion.
2Where A is the set of all admissible deterministic trading strategies, for a more formal definition see [3].
det
4Beyond the Nash equilibrium. Inthissetting, aNashequilibriumexistsandisunique. However,
one could wonder if, apart from the Nash equilibrium, there are other solutions that might be non-
Nash and collusive or, generally speaking, either better or sub-optimal, in terms of average costs
obtained by either agent, when compared to the Nash solution. Motivated by this, we aim at studying
how and under which conditions agents interacting in such an environment can adopt manipulative
or collusive behaviours that consistently deviate from the Nash equilibrium. We use RL to train the
agents to trade optimally in the presence of the other agent and we study the resulting equilibria.
The aim is to ascertain whether while trading, sub-optimal or collusive cost profiles for either of the
agents are attainable. In order to do so, we focus on the case where two risk neutral agents want to
liquidate the same initial portfolio made by an amount q of the same asset. We let the agents play
0
multiple episode instances of the optimal execution problem in a multi-agent market. This means
that, defining a trading episode to be the complete unwinding of the initial inventory q over time
0
[0,T] in N time steps by both agents, the overall game is made by iterated trades at each t=1,...,N
time-steps and by B iterations of trading, each containing a full inventory liquidation episode. Each
iteration is described by two vectors ⃗v(1) and ⃗v(2) containing the trading schedule of the two agents
(i) (i)
in episode i. Moreover, each vector is associated with its IS, according to Eq. 2. We now define the
set of admissible strategies and the average IS.
Definition 2. In an iterated game with B independent iterations, the set A of admissible selling
strategies in iteration i is composed by the pair of vectors ⃗v(1,2)=(⃗v(1) ,⃗v(2)), such that for k=1,2:
(i) (i) (i)
• PN v(k) =q
t=1 (i),t 0
• ⃗v(k) is (F ) - adapted and progressively measurable
(i),t t t≥0
• PN (v(k) )2<∞
t=1 (i),t
(1,2)
For any ⃗v ∈A the average Implementation Shortfall is defined as:
(i)
1 N
I¯ S(⃗v(1,2))= X IS(⃗v(1,2) ,t)=IS(⃗v(1))+IS(⃗v(2)) (9)
(i) N (i) (i) (i)
t=1
Leveraging on [24] and [23], we define collusive strategies and we show that they are necessarily
Pareto-optimal.
Definition 3 (Collusion). In an iterated game with B independent iterations, a pair of vectors of
selling
schedules⃗v(1,2)=(⃗v(1) ,⃗v(2))
∈A is defined to be a collusion if for each iteration i and
∀⃗v(1,2)
∈
c c c (i)
A:
I¯ S(⃗v(1,2))≤I¯ S(⃗v(1,2))
c (i)
Definition 4 (Pareto Optimum). In an iterated game with B independent iterations, a pair of vectors
of selling schedules
⃗v(1,2) =(⃗v(1) ,⃗v(2))
∈A is a Pareto optimal strategy if and only if there does not
p p p
(1,2)
exist ⃗v ∈A such that:
(i)
∀i=1,...,M, I¯ S(⃗v(1,2))≥I¯ S(⃗v(1,2))
p (i) (10)
∃j =1,...,M, I¯ S(⃗v(1,2))>I¯ S(⃗v(1,2))
p (j)
Proposition1(CollusiveParetooptima). Foranoptimalexecutionstrategyproblemwheretwoagents
minimise costs as in Eq. (3) in a market as in Eq. (4), a collusive selling strategy ⃗v(1,2) in the sense
c
of Definition 3 is necessarily a Pareto optimum as defined in Definition 4. We call this a Collusive
(1,2)
Pareto optimal strategy ⃗v .
cp
Proof. By contradiction, suppose that ⃗v(1,2) is not Pareto optimal, then there must exist an iteration
c
i and a strategy ⃗u(1,2)=(⃗u(1) ,⃗u(2)) such that:
(i) (i) (i)
I¯ S(⃗u(1,2))≤I¯ S(⃗v(1,2)) (11)
(i) c
Butthiscontradictsthehypothesisthat⃗v(1,2) iscollusiveandhence⃗v(1,2) mustbePareto-optimal.
c c
5Having shown that a collusive strategy is in fact the Pareto-optimal strategy, we aim at finding
the set of all Pareto-efficient strategies, i.e. those selling strategies that result in IS levels where it is
impossible to improve trading costs for one agent without deteriorating the one of the other agent.
These strategies are defined to be collusions in this game setup. We define the set of Pareto solutions
and, within this set and we find the Pareto-optimum as the minimum within the set of solutions.
Definition 5 (Pareto-efficient set of solutions). For a two-player game, with B independent iterations
and risk neutral players that aim to solve the optimal execution problem in a market model defined as
in Eq. (4), the Pareto-efficient set of strategies is the set of strategies ⃗v(1,2) such that the IS of one
agent cannot be improved without increasing the IS value of the other agent.
We now provide the conditions that allows to find the set of Pareto-efficient solutions.
Theorem 1 (Pareto-efficient set of solutions). The Pareto-efficient set of strategies are the solutions
to the multi-objective optimisation problem:
 (cid:16) (cid:17)
min ⃗v(1,2)f⃗ ⃗v(1,2)
(P1)
s.t.{⃗v(1,2) ∈A, ⃗g(⃗v(1,2))=⃗0}
(i)
where:
h (cid:16) (cid:17) (cid:16) (cid:17)i
f⃗(⃗v(1,2))= E IS(⃗v(1)|⃗v(2)) ,E IS(⃗v(2)|⃗v(1))
" N ! N !#
⃗g(⃗v(1,2))= X v(1) −q , X v(2) −q
t 0 t 0
t=1 t=1
Proof. Problem (P1) is a convex multi-objective optimisation problem with n=N design variables
(⃗v(1,2)), k=2 objective functions, and m=2 constraint functions. Leveraging on [37], in order to find
the Pareto-efficient set of solutions, Problem (P1) can be restated in terms of Fritz-Johns conditions.
We define the L∈R(n+m)×(k+m) matrix as:
" #
∇ f⃗(⃗v(1,2)) ∇ ⃗g(⃗v(1,2))
L= ⃗v(1,2) ⃗v(1,2) (12)
⃗0 ⃗g(⃗v(1,2))
then, the strategy ⃗v(1,2) is a solution to the problem:
p
L·⃗δ=⃗0 (P2)
where ⃗δ =(ω⃗,⃗λ)∈Rk+m. Moreover, ⃗v(1,2) is a Pareto-efficient solution if ⃗δ exists, it is a non-trivial
p
solution if ⃗δ exists and ⃗δ̸=⃗0.
Leveraging on [37], a solution ⃗v(1,2),∗ to the Problem (P2) is a non-trivial Pareto-efficient solution
if:
det(L(⃗v(1,2),∗)TL(⃗v(1,2),∗))=⃗0 (13)
Where L(⃗v(1,2),∗) denotes the matrix L where the argument of F(·) and G(·) functions is a strategy
⃗v(1,2),∗. Thus we say that a necessary condition for generic strategy (⃗v(1,2),∗) to be Pareto-efficient is
to satisfy Eq. (13). In general, Eq. (13) gives the analytical formula that describes the Pareto-optimal
set of solutions obtainable from Problem (P2).
The analytical derivation of the Pareto-efficient set of strategies, i.e. of the Pareto-front, for this
kind of problem is quite involved and cumbersome, although obtainable via standard numerical multi-
objective techniques. Later in the paper, we will solve this problem numerically to find the set for
the problem at hand. It is instead possible to derive the absolute minimum of the Pareto-efficient
set of solutions since the problem is a sum of two convex functions and is symmetric. We call these
strategies, one per agent, Pareto-optimal strategies in the spirit of Definition 4.
Theorem 2 (Pareto optimal strategy). For a two-player game with B independent iterations and
risk-neutral players, the Pareto optimal strategy for all iterations i defined as the solution of the
problem:
 argmin F(⃗v(1,2))

s.t.
(cid:16)
PN
v⃗v (( 11 )), −⃗v(2 q)
(cid:17)
=0 (14)
t=1 t 0
 (cid:16) PN v(2) −q (cid:17) =0
t=1 t 0
6where:
(cid:16) (cid:17) (cid:16) (cid:17)
F(⃗v(1,2))=E IS(⃗v(1)|⃗v(2)) +E IS(⃗v(2)|⃗v(1)) (15)
is ∀ t=1,...,N and ∀i=1,...M:
q q
v(1)= 0 , v(2)= 0 (16)
t,p N t,p N
i.e. the TWAP strategy.
Proof. Consideringjustaniterationiofthetwoplayers’game,wedefine⃗v(1)=argmin IS(⃗v(1)|⃗v(2)),
⃗v(1)
meaning that the strategy for agent 1 is a function of the strategy of agent 2. And thus⃗v(1) the strat-
egy that minimises the IS of agent 1 given the strategy of agent 2. The same consideration holds for
agent 2, where ⃗v(2)=argmin IS(⃗v(2)|⃗v(1)).
⃗v(2)
Leveraging on Proposition 1, the Pareto optimal strategies⃗v(1) ,⃗v(2) of the agents considered solve:
p p
argmin F(⃗v(1,2)) (P1)
⃗v(1),⃗v(2)
where:
(cid:16) (cid:17) (cid:16) (cid:17)
F(⃗v(1,2))=E IS(⃗v(1)|⃗v(2)) +E IS(⃗v(2)|⃗v(1)) (17)
we then set up a constrained optimisation problem, where the constraint binds the agents to just sell
their initial inventory q over the time window considered:
0
" N ! N !#
∇ F(cid:16) ⃗v(1,2)(cid:17) +λ X v(1) −q +λ X v(2) −q =0 (18)
⃗v(1,2) 1 t 0 2 t 0
t=1 t=1
this can be decoupled into two distinct problems:
" N !#
∇ L =∇ E(cid:16) IS(⃗v(1)|⃗v(2))(cid:17) +λ X v(1) −q =0
⃗v(1),λ1 1 ⃗v(1),λ1 1 t 0
t=1 (19)
" N !#
∇ L =∇ E(cid:16) IS(⃗v(2)|⃗v(1))(cid:17) +λ X v(2) −q =0
⃗v(2),λ2 2 ⃗v(2),λ2 2 t 0
t=1
Considering now just the first problem in the previous equation, we notice that:
N t N
E(cid:16) IS(⃗v(1)|⃗v(2))(cid:17) =−X κv(1)X(v(1)+v(2))−X αv(1)2
t j j t
t=1 j=1 t=1
and thus, for every t=1,...,N:
  ∂∂ vL (11 ) =−κ(cid:16) (2q 0−q t(1) −q t(2))+v t(1)(cid:17) −2αv t(1)+λ 1=0 (20)
t (cid:16) (cid:17)
 ∂L1 = PN v(1) −q =0
∂λ1 t=1 t 0
where Pt v(1) =(q −q ) and q is the level of inventory held at time-step t=1,...,N, we notice
j=1 j 0 t t
that:
κ(2q −q(1) −q(2))+λ
v(1)=− 0 t t 1
t κ+2α
thus:
N 1
0=X − κ(cid:16) (2q −q(1) −q(2))+λ(cid:17) −q
κ+2α 0 t t 0
t=1
q (κ+2α)
λ= 0 +κ(2q −q(1) −q(2))
N 0 t t
κ(2q −q(1) −q(2)) q (κ+2α) κ(2q −q(1) −q(2))
v(1)=− 0 t t + 0 + 0 t t
t κ+2α N(κ+2α) κ+2α
7Finally, the first and the last terms cancel out and we obtain that the Pareto-optimal strategy⃗v(1) is:
p
q
v(1)= 0 , ∀ t=1,...,N (21)
t,p N
for v(2) same considerations hold since the problem is symmetric.
t,p
To study whether Collusive Pareto-optima or more generally non-Nash equilibria arise in this
setting,wemodelthetwoagentsbyequallyandsimultaneouslytrainingthemwithRLbasedonDouble
DeepQLearning(DDQL)algorithmwheretwoagentsinteractinaniterativemanner. Wethenanalyse
the results under the light of the collusion strategies derived above. The aim is to understand how
and which equilibria will eventually be attained, and whether in attaining an equilibrium they tacitly
collude in order to further drive the costs of their trading down. We will analyse their interactions
in some limiting cases in order to study if and how the equilibrium found changes and adapts to the
market dynamics in a model agnostic setting such as the DDQL algorithm.
3 Double Deep Q Learning for multi-agent impact trading
We model the algorithmic agents by using RL based on Double Deep Q-Learning (DDQL). The
setup is similar to the single agent algorithm as in [27] and in [29], but now we consider two agents
interacting in the same environment. Each agent employs two neural networks, namely the main
Q-net (Q ) for action selection and the target Q-net (Q ) for state evaluation. The four nets are
main tgt
exactly the same and updated at exactly the same rate. The only difference between the two agents
is the timing at which they act and the time at which their nets are updated. In fact, in a given
time-step, the agent trading as second pays the price impact generated by the first one. To make the
game symmetric, similarly to [20], at each time step a coin toss decides which agent trades first. This
ensures the symmetry and guarantees that, as the game unfolds, no advantage in terms of trading
timing is present for either of the two agents.
We divide the overall numerical experiment into a training and a testing phase. In the training
phase we train both the agents to solve the optimal execution problem. In the testing phase, we
employ what we learned in terms of Q weights, letting the trained agents play an iterated trading
main
game. All the results shown in Section 4 are obtained in the testing phase.
3.1 Setting of the numerical experiments
We consider two risk neutral agents whose goal is to unwind an initial position of q =100 shares
0
with initial value S =10$ within a time window [0,T]. The window is divided into N =10 time-
0
steps of length T/N = τ. The mid-price S evolves as in Eq. (4), and the agents sell their whole
t
inventory during the considered time-window. This is called an iteration and in order to train the
DDQL algorithm we consider a number of C iterations. This is called a run. Thus, a run of the game
is defined to be an iterated game over both N time-steps and in C trading iterations.
Over the C iterations, each agent learns how to trade via an exploration-exploitation scheme,
thus using ϵ-greedy policies. This is obtained by changing the weights of their Q net in order to
main
individually choose the best policies in terms of rewards, related to the obtained IS. The scheme of the
algorithm is thus symmetric, and for each agent it is divided into two phases: an exploration and an
exploration phase managed by the parameter ϵ∈(0,1], that is common to both the agents, that will
decrease geometrically during training and globally initialised as ϵ=1. Depending on the phase, the
way in which the quantities to sell v are chosen changes, and it does so for each agent symmetrically.
t
When the agents are exploring, they will randomly select quantities to sell v in order to explore
t
different states and rewards in the environment, alternatively, when the agents are exploiting, they
will use their Q net to select v .
main t
3.1.1 Action selection and reward function
For each of the N time-steps and C trading iterations, the agents’ knowledge of the state of market
environmentisthetuplegi=(t,qi,S )fori=1,2. Thus,eachagentknowsthecurrenttime-step,her
t t t−1
individual remaining inventory, and the permanently impacted mid-price at the previous time step.
8Clearly, only the first and the last are common knowledge of the two agents, while no information on
inventory q or past selling actions v is shared between the agents.
t t
Then, depending on the current value of ϵ, a draw ζ, different for each agent, from a uniform
distribution determines whether the agent performs exploration or exploitation. This means that with
probability ϵ the agent chooses to explore and thus she chooses at time-step t the quantity to sell v
t
sampling from a normal distribution with mean µ= qt and standard deviation δ =| qt |. In this
N−t N−t
way, on average we favour a TWAP execution, allowing for both positive and negative values of v ,
t
meaningthatsellandbuyactionscanbeselected. Intheexplorationphasewebindthequantitytobe
sold to be v ∈[−q ,q ], i=1,2. Alternatively, with probability 1−ϵ, the agent chooses the optimal
i,t 0 0
Q-action as the one that maximises the Q-values from Q net, thus exploiting what learnt in the
main
exploration phase. We bind the agents to sell all their inventory within the considered time window,
still exploring a large number of states, rewards, and actions.
Once every m actions taken during training iterations by both the agents, ϵ is multiplied by a
constant c<1 such that ϵ→0 as m→∞. In this way, for a large number of iterations C, ϵ converges
to zero and the algorithm gradually stops exploring and starts to greedily exploit what the agent has
learned in terms of weights θ of the Q net. Notice that the update for ϵ happens at the same
main
rate for both the agents, thus they explore and exploit contemporaneously, while the draw from the
uniform distribution ζ is different for every agent. For each time step t we decide which of the two
agents trades first with a coin toss. Once that the ordering of the trades is decided, the action decision
rule in the training phase unfolds as:
ϵ∈(0,1), ζ ∼U(0,1)
( ∼N(µ= qt ,δ=| qt |) ,ifζ ≤ϵ (22)
v = N−t N−t
t argmax Q (gi,v′|θ ) ,else
v′∈[0,qt] M t main
After this, each agent calculates the reward as:
r =S v −αv2 ,i=1,2 (23)
t,i t−1 t,i t,i
Notice that the actions of the other agent indirectly impact on the reward of the agent i through the
price S , while nothing but the agent’s own actions are known in the reward.
t−1
Overall, the rewards for every t∈[1,N] are:
r =S v −αv2
1,i 0 1,i 1,i
r =(S +κ(v +v )+ξ)v −αv2
2,i 1 1,i 1,i− 2,i 2,i
(24)
.
.
.
r =(S +κ(v +v )+ξ)v −αv2
N,i N−1 N−1,i N−1,i− N,i N,i
Where by i− we denote the the other agent, assuming that we are looking at the reward for agent i,
and ξ∼N(0,σ).
Thus, for each time step t the agent i sees the reward from selling v shares, and stores the state
t,i
of the environment, g , where the sell action was chosen along with the reward and the next state
t,i
g where the environment evolves. At the end of the episode, the reward per episode per agent is
t+1,i
−q S +PN S v −αv2 . Written in this form, the aim of the agent is to cumulatively maximise
0 0 t=1 t−1 t,i t,i
such reward, such that the liquidation value of the inventory happens as close as possible to the initial
value of the portfolio.
3.1.2 Training scheme
Bearing in mind that the procedure is exactly the same for both agents, we focus now on the training
scheme of one of them, dropping the subscript i. We let the states g , actions v , rewards r and
t t t
subsequent future states g obtained by selling a quantity v in state g , to form a transition tuple
t+1 t t
that is stored into a memory of maximum length L, we have two different memories, one for each
agent. As soon as the memory contains at least b transitions, the algorithm starts to train the Q-nets.
To this end, the algorithm samples random batches of length b from the memory of the individual
agent, and for each sampled transition j it individually calculates:
( rj if t=N;
yj(θ )= t (25)
t tgt rj+γQ (gj ,v∗|θ ) else
t tgt t+1 tgt
9Table 1: Fixed parameters used in the DDQL algorithm. The parameters not shown in the table
change depending on the experiments and are reported accordingly.
DDQL parameters Model parameters
NN layers 5 C train its. 5,000 N intervals 10
Hidden nodes 30 M test its. 2,500 S price 10$
0
ADAM lr 0.0001 m reset rate 75 acts. q inventory 100
0
Batch size b 64 c decay rate 0.995 α t. impact 0.002
L mem. len. 15,000 γ discount 1 κ p. impact 0.001
InEq.(25),rj istherewardforthetimestepconsideredinthetransitionj,gj isthesubsequentstate
t t+1
reachedint+1,knownsinceitisstoredinthesametransitionj,whilev∗=argmax Q (gj,v|θ ).
v main t main
γ is a discount factor that accounts for risk-aversion3. Each agent individually minimises the mean
squared error loss between the target y(θ ) and the values obtained via the Q net. In formulae:
tgt main
1 b
L(θ ,θ )=
X(cid:16)h
yℓ(θ )−Q (gℓ,vℓ|θ
)i(cid:17)2
main tgt b t tgt main t t main
ℓ=1
θ∗ =argminL(θ ,θ )
main main tgt
θmain
We then use back propagation and gradient descent in order to update the weights of the Q net.
main
This procedure is repeated for each agent and for each random batch of transition sampled from the
agent’s memory. Overall, once both agents have individually performed m actions, we decrease ϵ by
a factor c<1 and we set Q =Q
tgt main
Once both agents have been simultaneously trained to optimally execute a quantity q while
0
interacting only through the midprice, we let the agents interact on another number M <C of trading
iterations. This is the testing phase, and now actions for each agent are selected using just her Q
main
net. As said above, the results analysed below are those obtained in the testing phase.
The features of the Q-nets for each agent are (q ,t,S ,v ) and are normalised in the domain
i,t t−1 i,t
[−1,1] using the procedure suggested in [27], whereas normalised mid-prices S¯ are obtained via min-
t
max normalisation. In our setup we use fully connected feed-forward neural networks with 5 layers,
each with 30 hidden nodes. The activation functions are leakyReLu, and finally we use ADAM
for optimisation. With the exception of the volatility parameter σ which will be specified later,
the parameters used in the algorithm are reported in Table 1, the training algorithm is reported in
Algorithm 1.
3We set a γ=1 since we model risk neutral agents
10Algorithm 1 Training of Double Deep Q-Learning multi-agent impact game
Require:
Set ϵ=1, b batch size, C train iterations;
Set market dynamics parameters;
For each agent i=1,2 initialise with random weights Q and make a copy Q ;
main tgt
For each agent i=1,2 initialise the memory with max length L.
for k in C do
Set Sk =S ;
0 0
for t in N do
u∼Bin(1,0.5); ▷ Decides the order of execution
if u=0 then i=(1,2) else i=(2,1)
end if ▷ Vector of priority ordering of the agents
for each agent i in the order decided above do
g ←(q ,t,Sk);
i,t i,t t
(sample N(µ= qt ,δ=| qt |) ,with probability ϵ
v ← N−t N−t ;
i,t argmax Q (g ,v′|θ ) ,with probability (1−ϵ)
v′∈[0,qt] M t main
r ←Si v −αv2;
t t−1 t t
Sk →Sk; ▷ Generate Sk from Sk
t−1 t t t−1
g ←(q ,t+1,Sk) ;
i,t+1 i,t+1 t
Memory for agent i ←(g ,r ,v ,g ); ▷ Memory storing
i,t i,t i,t i,t+1
if Length of memory ≥ b then
for j in b do
Sample a batch of (gj ,rj ,vj ,gj ) from memory;
i,t i,t i,t i,t+1
v∗=argmax Q (gj ,vj |θ );
i v i,main i,t i,t i,main
( rj if t=N;
yj (θ )= i,t
i,t i,tgt rj +γQ (gj ,v∗|θ ) else
i,t i,tgt i,t+1 i i,tgt
end for
θ∗ =argmin L(θ ,θ ) via gradient descent with loss to minimise:
i,main θi,main i,main i,tgt
1 b
L(θ ,θ )=
X(cid:16)h
yℓ(θ )−Q (gℓ ,vℓ |θ
)i(cid:17)2
i,main i,tgt b t i,tgt i,main i,t i,t i,main
ℓ=1
if Length of agent i memory = L then halve the length of agent i memory
end if
end if
After m iterations decay ϵ=ϵ×c;
After m iterations θ ←θ ;
i,tgt i,main
end for
end for
end for
11Average IS per iteration
3.0
Average IS per testing run
Nash Equilibrium IS
2.5 Collusive area
Linear combinations of IS(v(2))+IS(v(1))
p p
Pareto-optimal IS
2.0
1.5
11 5
178
1.0 1 163 92 120 171534
8 914
10
116
0.5
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
IS(v(2)) for each iteration in test
Figure 1: Scatter plot of the IS of the two agents for 20 testing runs of 2500 iterations in the zero
noise case (σ=10−9).
4 Results
In this Section we present the results obtained by using the RL Algorithm 1 in the game setting
outlined in Section 2. The experiments aim at studying the existence and the form of the learnt
equilibria, by analysing the policies chosen by the two interacting trading agents. We then compare
the equilibria with the Nash equilibrium, the Pareto-efficient set of solutions, and the Pareto optimal
strategy. We consider different scenarios, using the market structure as outlined in Eq. (4).
It can be easily noticed how the theoretical Nash equilibrium for this game does not depend on
the volatility level of the asset. In fact, when the risk aversion parameter λ=0, the Nash equilibrium
of Eq. (7) depends only on the permanent and temporary impact coefficients κ and α, respectively.
However, in the numerical determination of the equilibrium solution, volatility σ and the associated
diffusion play the role of a disturbance term, since an agent cannot determine whether an observed
price change is only due to the impact generated by the trading of the other agent or if it is a random
fluctuation due to volatility. In this sense, volatility plays the role of a noise-to-signal ratio here.
To quantify the effect of volatility on the learnt equilibria, we perform three sets of experiments
for different levels of the volatility parameter σ, leaving unchanged the impact parameters. More
specifically we consider, for both training and testing phase the case where σ =10−9 (termed zero
noise case), σ=10−3 (moderate noise) and σ=10−2 (large noise) case. In all three cases, we use the
same temporary and permanent impacts α=0.002 and κ=0.001.
We employ 20 training and testing runs, each run is independent from the others, meaning that
the weights found in one run are not used in the others. In this way we aim at independently (run-
wise) train the agents to sell their inventory. Each testing run has M =2,500 iterations of N =10
time-steps.
4.1 The zero noise case
Whenσ=10−9 wemodeltheinteractionofbothagentsinalimitingsituationwhenthemarketisvery
illiquid and almost no noise traders enter the price formation process. In this case, the permanent
impact is essentially the only driver of the mid-price dynamics in Eq. (4), thus price changes are
triggered basically only by the selling strategies used by the agents throughout the game iterations.
The results of the experiments are displayed in Figure 1, which shows, as a scatter plot, the IS of
both agents in each iteration of the testing phase. Each colour represents the results of one of the 20
12
tset
ni
noitareti
hcae
rof
))1(v(SISelling schedules over 20 testing runs, =10 9
Testing run 0 Testing run 1 Testing run 2 Testing run 3 Testing run 4
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 5 Testing run 6 Testing run 7 Testing run 8 Testing run 9
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 10 Testing run 11 Testing run 12 Testing run 13 Testing run 14 Legend:
100 100 100 100 100 Agent 1
Agent 2
50 50 50 50 50
Mean Agents
Nash Agent
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 Pareto optimal
Testing run 15 Testing run 16 Testing run 17 Testing run 18 Testing run 19
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Time Time Time Time Time
Figure 2: Optimal execution strategies for 20 testing runs of 2500 iterations, using σ=10−9.
runs of 2,500 iterations each. The blue circles shows the IS centroids per testing run (the number in
the circle identifies the run). For comparison the plot reports as a red star the point corresponding
to the Nash equilibrium of Eq. (7), which is used to divide the graph into four quadrants, delimited
by the red dashed lines. The top-right quadrant contains point which are sub-optimal with respect
to the Nash equilibrium for both players, while the points in the top-left and bottom-right quadrant
report points where the found equilibrium favours one of the two agents at the expenses of the other.
In particular, in these regions one of the agents achieve an IS smaller than the one of the Nash
equilibrium, while the other agents performs worst. In a sense, one of the agents predates on the other
in terms of reward. The bottom-left quadrant is the most interesting, since here both agents are able
to achieve an IS smaller than the one in the Nash equilibrium and thus contain potential collusive
equilibria. For comparison, the black star indicates the IS of the Pareto-optimal strategy found in
Theorem 2, while the magenta line is the linear combination of the Pareto-optimal IS. Finally, the
green rectangle denotes the area between the Nash and the Pareto-optimal ISs, and we call it the
collusive area. In fact, in that area we find the ISs for both the agents that lie between the proper
collusion defined by the Pareto-optimum and the Nash equilibrium.
Looking at Figure 1 we can notice that the ISs per testing run concentrate in the collusive area
of the graph (green rectangle) between the Nash and the Pareto-optimal IS. This means that when
the noise is minimal, it is easier for the agents to adopt tacit collusive behaviour and obtain costs that
fall near the collusion IS, i.e. the Pareto-optimum. The majority of the remaining ISs per iteration
still lie within the second and fourth quadrant of the graph, meaning that again they are in general
able to find strategies that, at each iteration, do not allow one agent to be better without worsening
the other.
ConsideringthestrategiesfoundbytheagentsinFigure2,wenoticehowtheagentskeepontrading
at different speed, i.e. their selling policies are consistent with the presence of a slow trader and a fast
trader. It can be seen how the policies followed by the agents in almost all the 20 simulations depend
inversely on the strategy adopted by the competitor. Moreover, in most of the runs in the collusive
region,theaveragestrategyoftheagentsisverysimilartotheTWAPstrategy. Thisispossiblethanks
to very low noise, and in this case the agents are able to find more easily the Pareto-optimal strategy,
which corresponds to an equilibrium where they both pay the lowest amount possible in terms of IS
by tacitly colluding with their trading.
These evidences underline the quite intuitive fact that, when the noise to signal ratio is low it
is simple for the agents to disentangle their actions from those of other agents. The agents find an
incentive to deviate from the Nash-equilibrium adopting strategies that allow for lower costs. The
majority of these strategies correspond to an average collusive ISs, thus with lower cost than in the
Nash equilibrium, but still slightly greater than the Pareto-optimum in terms of ISs. In general, per
13
yrotnevnI
yrotnevnI
yrotnevnI
yrotnevnIAverage IS per iteration
3.0
Average IS per testing run
Nash Equilibrium IS
2.5 Collusive area
Linear combinations of IS(v(2))+IS(v(1))
p p
Pareto-optimal IS
2.0 18
1.5
19
5
8
15710
1.0 11 11 72 132
4 19 410 3 16 6
0.5
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
IS(v(2)) for each iteration in test
Figure 3: Scatter plot of the IS of the two agents in 20 testing runs of 2,500 iterations in the moderate
noise case (σ=10−3).
iteration neither agent can be better off without increasing the other agent cost level. There is thus a
strong evidence of tacit collusion, which in turn substantiates in the way the agents trade at different
speeds. The phenomenon is evident thanks to the very low level of asset’s volatility. Thus, evidences
of tacit collusive behaviour in this first case exist, moreover the collusive behaviour is adopted by the
agents even if they have neither information about the existence nor about the strategy of the other
agent trading in the market.
4.2 The moderate noise case
In this setting, the mid-price dynamics is influenced both by the trading of the agents and by the
volatility. The first thing that can be noticed in Figure 3 is that the IS per iteration distributes
almost only in the second, third and fourth quadrant of the plot. Moreover, the distribution of the
points suggests an inverse relation between the ISs of the two agents, i.e. a lower IS of an agent is
typicallyassociatedwithaworseningoftheIS oftheotheragent. Thecentroidsdistributeaccordingly.
Infact, thecentroidsthatlieinthecollusiveareaofthegraphareasnumerousasthosethatlieoutside
of that area on either the second or fourth quadrant. It can be seen that, outside the green rectangle,
the agents tend to behave in a predatory way, meaning that one agent has consistently lower costs
than the other. This happens tacitly, meaning that no information about the existence or about the
strategy followed by the other agent is part of the information available either in the market or in
each agent’s memory. Notice that predatory strategies are still consistent with the definition of Pareto
efficiency but are not a proper collusion since they do differ from the Pareto-optimal IS, even if they
appear to overlap in Figure 3.
14
tset
ni
noitareti
hcae
rof
))1(v(SISelling schedules over 20 testing runs, =10 3
Testing run 0 Testing run 1 Testing run 2 Testing run 3 Testing run 4
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 5 Testing run 6 Testing run 7 Testing run 8 Testing run 9
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 10 Testing run 11 Testing run 12 Testing run 13 Testing run 14 Legend:
100 100 100 100 100 Agent 1
Agent 2
50 50 50 50 50 Mean Agents
Nash Agent
0 0 0 0 0
Pareto optimal
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 15 Testing run 16 Testing run 17 Testing run 18 Testing run 19
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Time Time Time Time Time
Figure 4: Optimal execution strategies for 20 testing runs of 2500 iterations, using σ=10−3.
Looking at the average strategies implemented by the agents in each testing run (Figure 4), it
can be noticed how similarly as in the zero noise case, roughly speaking there is almost always one
agent that trades faster than the other and again as in the previous case, the one agent that trades
slower is the one with an higher IS. The comparison of Figure 4 and Figure 3shows that the agent
that ‘predates’ the other is a fast trader and obtains lower costs of execution. This behaviour is more
pronounced in this case, and we postulate that this is due to the increased level of noise for this
experiment.
We conclude once again that, even if no explicit information about the trading strategies is shared
bytheagents,duringtherepeatedgametheyareabletoextrapolateinformationonthepolicypursued
bythecompetitorthroughthechangesinthemid-pricetriggeredbytheirowntradingandbytheother
agent’s trading. Thus, it seems plausible that RL-agents modelled in this way are able to extrapolate
information on the competitor’s policy and tacitly interact through either a collusive behaviour that
leads to lower costs, either for both or for just one of them, resulting in a predatory behaviour or
through a predatory behaviour where the costs of one agent are consistently higher than those of the
other. Generally speaking, the set of solutions achieved is in line with the definition of the Pareto
efficient set of solutions.
4.3 The large noise case
Finally, we study the interactions between the agents when the volatility of the asset is large. This
corresponds to a relatively liquid market, since price changes are severely affected by the volatility
level. LookingatFigure5,wenoticethatinthismarketsetupthehigherlevelofvolatilitysignificantly
affects the distribution of the IS per iteration. In fact, the points in the scatter plot now distribute
obliquely, even if for the most part they still lie in the second, third, and fourth quadrants. Because
of the higher volatility level, very low IS values for both agents might be attained per iteration and
the structure of the costs per iteration is completely different with respect to to the previous cases.
However, the distribution of the average ISs per testing run, i.e. the positions of the centroids, is still
concentrated for the greatest part in the collusive strategies area in between the Pareto-optimum and
the Nash equilibrium costs.
15
yrotnevnI
yrotnevnI
yrotnevnI
yrotnevnIAverage IS per iteration
3.0
Average IS per testing run
Nash Equilibrium IS
2.5 Collusive area
Linear combinations of IS(v(2))+IS(v(1))
p p
Pareto-optimal IS
2.0
1.5 18
5 234
1.0 1 012 9 1115714 96 8
1113 7 16 10
0.5
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
IS(v(2)) for each iteration in test
Figure 5: Scatter plot of the IS of the two agents for 20 testing runs of 2500 iterations in the large
noise case (σ=10−2).
Figure 6 shows the trading strategies for the agents. It can still be noticed how in general if one
agent is more aggressive with its trading, the other tends to be not. This tacit behaviour of the agents
resulting in faster and slower traders still takes place, and again the agent that is trading slower pays
more in terms of IS, hence coupling the centroids’ distribution and the selling schedules in Figure 6
and Figure 5 reveals how a slower trading speed worsens the cost profile of one agent to the benefit of
the other. The average trading strategy of the agents per testing run in the collusive cases basically
revolves around a TWAP strategy that is in turn the Pareto-optimum for the considered problem.
4.4 Summary of results and comparison with the Pareto front
We have seen above that especially in the presence of significant volatility, the points corresponding to
the iterations of a run tend to distribute quite widely in the scatter plot. The centroid summarises the
average behaviour in a given run and provides a much more stable indication of the relation between
the IS of the two agents. To have a complete overview of the observed behaviour across the three
volatilityregimes,inFigure7weshowinascatterplotthepositionofthecentroidsofthe20×3testing
runs. It can be seen that the centroids tend to concentrate in the collusive area, irrespective of the
volatility level for the experiment. We notice how the agents’ costs tend to cluster in this area and to
be close to the Pareto-optimal IS. To have a more detailed comparison of the simulation results with
the theoretical formulation of the game, we numerically compute the Pareto-efficient set of solutions
(or Pareto-front)4 of and represent it on the scatter plot. The Figure shows that, as expected, the
centroids lie on the right of the front. More interestingly, they are mostly found between the front and
the Nash equilibrium for all the levels of volatility and, in line with the definition of the Pareto-front,
for an agent to get better the other has to be worse off. We further notice that the majority of points
for the zero noise case lies very close the Pareto optimum or in the collusive area, thus the lower
the volatility the easier it is to converge to the collusion strategy. In the other two cases we notice
how, even roughly half of the centroids lie in the collusive area, the rest mostly lie not far from the
numerical Pareto front.
4The numerical Pareto front has been obtained using ‘Pymoo’ package in Python introduced in [38].
16
tset
ni
noitareti
hcae
rof
))1(v(SISelling schedules over 20 testing runs, =10 2
Testing run 0 Testing run 1 Testing run 2 Testing run 3 Testing run 4
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 5 Testing run 6 Testing run 7 Testing run 8 Testing run 9
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 10 Testing run 11 Testing run 12 Testing run 13 Testing run 14 Legend:
100 100 100 100 100 Agent 1
Agent 2
50 50 50 50 50 Mean Agents
Nash Agent
0 0 0 0 0
Pareto optimal
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 15 Testing run 16 Testing run 17 Testing run 18 Testing run 19
100 100 100 100 100
50 50 50 50 50
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Time Time Time Time Time
Figure 6: Optimal execution strategies for 20 testing runs of 2500 iterations, using σ=10−2.
Average IS per iteration
2.2
Average IS per testing run, =0.01
2.0 18 Average IS per testing run, =0.001
Average IS per testing run, =10 9
1.8 Nash Equilibrium IS
Linear combinations of IS(v(2))+IS(v(1))
p p
1.6
Pareto-optimal IS
Numerical Pareto-front
18
1.4
19
11 5
5
1.2 8
1557 341017
8
2
1630
1.0 11 011 1 712 9 12 921 12 17 11 38511 25371 44
9 9 6 14 8 0
0.8 11 4 19 134137
1
16 1616 10 6
10
0.6
0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2
IS(v(2)) for each iteration in test
Figure 7: Scatter plot of the IS centroids of the two agents in the 20×3 testing runs of 2500 iterations
each, for all the considered values of σ.
17
yrotnevnI
yrotnevnI
yrotnevnI
yrotnevnI
tset
ni
noitareti
hcae
rof
))1(v(SIAverage Selling Schedules for the three experiments
Average Selling Schedule =10 9 Average Selling Schedule =0.001 Average Selling Schedule =0.01
100 100 100 Legend:
Agent 1
Agent 2
80 80 80
Pareto
Mean Agents
60 60 60
Nash Agent
40 40 40
20 20 20
0 0 0
0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10
Time Time Time
Figure 8: Average optimal execution strategy over the 20 testing runs of 2500 iterations, for all the
values of σ considered.
Figure 8 shows that, irrespective of the values considered for the volatility level σ, the average
strategy does not differ much from the Pareto-optimum TWAP strategy. In the zero noise case, the
averagestrategyofthetwoagentstendstobeslightlymorefrontloaded,butstillliesbetweentheNash
and the Pareto-optimum. As the σ value increases, and thus in the moderate noise and large noise
cases, we can see how both the agents tend to be less aggressive at the beginning of their execution
in order to adopt a larger selling rate towards the end of their execution. This behaviour is stronger
the higher the σ, due to the increased uncertainty brought by the asset volatility in addition to the
price movements triggered by the trading of both the agents.
4.5 Variable volatility and misspecified dynamics
Our simulation results show that volatility plays an important role in determining the IS in an
iteration of the testing phase, although when averaging across the many iterations of a run, the results
are more stable and consistent. In financial market, returns are known to be heteroscedastic, i.e.
volatility varies with time. Thus, also from the practitioner’s point of view, it is interesting to study
how agents, which are trained in an environment with a given level of volatility level, perform in a
testing environment where the level of volatility is different. In particular, it is not a priori clear
whether a collusive relationship between the two agents would still naturally arise when the price
dynamics is different in the testing and in the training phase. Moreover, it is interesting to study how
a change in the environment would impact the overall selling schedule and the corresponding costs.
In the following, we study the agents’ behaviour in extreme cases, in order to better appreciate their
behaviour under time-varying conditions. Specifically, using the same impact parameters as above in
both phases, we learn the the weights of the DDQN algorithm in a training setting with σ=10−9 and
then we employ them in a testing environment where σ =10−2. We then repeat the experiment in
the opposite case with the volatility parameters switched, i.e. training with σ=10−2 and testing with
σ=10−9.As before, we run 10 testing runs of 2,500 iterations each.
4.5.1 Training with zero noise and testing with large noise
When the agents are trained in an environment where σ=10−9 and the weights of this training are
used in a testing environment with σ =10−2, we find that the distribution of the ISs per iteration
is similar to the one that would be obtained in a both testing and training with σ =10−2 scenario
(see Figure 9). The centroids are still mostly distributed in the second, third and fourth quadrant,
although some iterations and even a centroid, might end up in the first quadrant as in the σ=10−9
case. In general, it can be seen how the centroids mostly lie in the collusive area of the graph, i.e in
the square between the Pareto-optimal IS and the Nash equilibrium, pointing out at the fact that
collusive behaviours are still attainable even when the training dynamics are misspecified with respect
to the testing ones.
Looking at Figure 10, we notice that the selling schedules are mostly intertwined, i.e. rarely the
agents trade at the same rate. Traders can still be slow or fast depending on the rate adopted by the
other agent. Comparing Figure 10 with Figure 9 we see that, as in the correctly specified case, the
agent that trades faster gets the lower cost, while the slow trader achieves a larger IS.
18
yrotnevnIAverage IS per iteration
3.0
Average IS per testing run
Nash Equilibrium IS
2.5 Collusive area
Linear combinations of IS(v(2))+IS(v(1))
p p
Pareto-optimal IS
2.0
1.5
9
7
8
1
1.0 0624
5
3
0.5
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0
IS(v(2)) for each iteration in test
Figure 9: IS scatter for 20 testing runs of 2500 iterations, training σ=10−9 and testing σ=10−2.
Selling schedules over 10 testing runs, training =10 2 and testing =10 9
Testing run 0 Testing run 1 Testing run 2 Testing run 3 Testing run 4 Legend:
100 100 100 100 100 Agent 1
75 75 75 75 75 Agent 2
50 50 50 50 50 Mean Agents
25 25 25 25 25 Nash Agent
0 0 0 0 0 TWAP
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 5 Testing run 6 Testing run 7 Testing run 8 Testing run 9
100 100 100 100 100
75 75 75 75 75
50 50 50 50 50
25 25 25 25 25
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Time Time Time Time Time
Figure 10: Optimal execution strategies for 10 testing runs of 2500 iterations, training σ=10−9 and
testing σ=10−2.
4.5.2 Training with large noise and testing with zero noise
Figure 11 shows the result of the experiment where agents are trained in an environment where
σ =10−2 and tested in a market with σ =10−9. We observe that the ISs are distributed in a way
similar to the case where the agents were both tested and trained in an environment with σ=10−9.
Still, the centroids lie in all but the first quadrant, and the majority of them lies in the collusive area,
showing how even with misspecified dynamics, a collusive behaviour naturally arises in this game.
The trading schedules adopted show the same kind of fast-slow trading behaviour between the agents,
where still the slower trader has to pay higher costs in terms of IS (see Figure 12).
Finally, irrespective of the encountered levels of volatility, it seems that what is more important is
the market environment experienced during the training phase, thus the selling schedule implemented
in a high volatility scenario with DDQN weights coming from a low volatility scenario are remarkably
similar to the one found with both training and testing with σ=10−9, and vice versa when σ=10−2
is used in training and the low volatility is encountered in the test. It might be concluded that once
that the agents learn how to adopt a collusive behaviour in one volatility regime, they are able to still
adopt a collusive behaviour when dealing with another volatility regime.
19
yrotnevnI
yrotnevnI
tset
ni
noitareti
hcae
rof
))1(v(SIAverage IS per iteration
3.5
Average IS per testing run
Nash Equilibrium IS
3.0 Collusive area
Linear combinations of IS(v(2))+IS(v(1))
p p
Pareto-optimal IS
2.5
2.0
1.5
0
9
1.0 1 82
4 56
7 3
0.5
0.5 1.0 1.5 2.0 2.5 3.0 3.5
IS(v(2)) for each iteration in test
Figure 11: IS scatter for 20 testing runs of 2500 iterations, training σ=10−2 and testing σ=10−9.
Selling schedules over 10 testing runs, training =10 9 and testing =10 2
Testing run 0 Testing run 1 Testing run 2 Testing run 3 Testing run 4 Legend:
100 100 100 100 100 Agent 1
75 75 75 75 75 Agent 2
50 50 50 50 50 Mean Agents
25 25 25 25 25 Nash Agent
0 0 0 0 0 TWAP
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Testing run 5 Testing run 6 Testing run 7 Testing run 8 Testing run 9
100 100 100 100 100
75 75 75 75 75
50 50 50 50 50
25 25 25 25 25
0 0 0 0 0
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Time Time Time Time Time
Figure 12: Optimal execution strategies for 10 testing runs of 2500 iterations, training σ=10−2 and
testing σ=10−9.
5 Conclusions
In this paper we have studied how collusive strategies arise in an open loop two players’ optimal
execution game. We first have introduced the concept of collusive Pareto optima, that is, a vector
of selling strategies whose IS is not dominated by the IS of other strategies for each iteration of the
game. We furthermore showed how a Pareto-efficient set of solutions for this game exists and can
be obtained as a solution to a multi-objective minimisation problem. Finally, we have shown how
the Pareto-optimal strategy, that is indeed the collusion strategy in this setup, is the TWAP for
risk-neutral agents.
As our main contribution, we have developed a Double Deep Q-Learning algorithm where two
agents are trained using RL. The agents were trained and tested over several different scenarios where
they learn how to optimally liquidate a position in the presence of the other agent and then, in the
testingphase, theydeploytheirstrategyleveragingonwhatlearntinthepreviousphase. Thedifferent
scenarios, large, moderate and low volatility, helped us to shed light on how the trading interactions
on the same asset made by two different agents, who are not aware of the other competitor, give rise
to collusive strategies, i.e. strategies with a cost lower than a Nash equilibrium but higher than a
proper collusion. This, in turn, is due to agents who keep trading at a different speed, thus adjusting
the speed of their trading based on what the other agent is doing. Agents do not interact directly
and are not aware of the other agent’s trading activity, thus strategies are learnt from the information
20
yrotnevnI
yrotnevnI
tset
ni
noitareti
hcae
rof
))1(v(SIcoming from the impact on the asset price in a model-agnostic fashion.
Finally, we have studied how the agents interact when the volatility parameter in the training part
is misspecified with respect to the one observed by the agents in the testing part. It turns out that the
existence of collusive strategies are still arising, and thus robust with respect to settings when models
parameters are time varying.
There are several possible extensions of our work. One obvious extension is to the setting where
morethantwoagentsarepresentand/orwheremoreassetsareliquidated,leadingtoamulti-assetand
multi-agent market impact games, leveraging on the work done in [39]. Second, impact parameters
are constant, while the problem becomes more interesting when liquidity is time-varying (see [29] for
RL optimal execution with one agent). Third, we have considered a linear impact model, whereas it is
known that impact coefficients are usually non-linear and might follow non-trivial intraday dynamics.
Finally, the Almgren-Chriss model postulates a permanent and fixed impact, while many empirical
results point toward its transient nature (see [6]). Interestingly, in this setting the Nash equilibrium
of the market impact game shows price instabilities in some regime of parameters (see [20]), which
are similar to market manipulations. It could be interesting to study if different market manipulation
practicesmightarisewhenagentsaretrained, asinthispaper, withRLtechniques. Theanswerwould
be certainly of great interest also to regulators and supervising authorities.
Acknowledgements
The authors thank Sebastian Jaimungal for the useful discussions and insights. AM thanks Felipe
Antunes for the useful discussions and insights. FL acknowledges support from the grant PRIN2022
DDN.104ofFebruary2,2022”Liquidityandsystemicrisksincentralizedanddecentralizedmarkets”,
codice proposta 20227TCX5W - CUP J53D23004130006 funded by the European Union NextGener-
ationEU through the Piano Nazionale di Ripresa e Resilienza (PNRR).
References
[1] Kissell, R., Algorithmic trading methods: Applications using advanced statistics, optimization,
and machine learning techniques. Academic Press, 2020.
[2] Almgren, R., Chriss, N., “Optimal execution of portfolio transactions,” Journal of Risk, vol. 3,
pp. 5–39, 2000.
[3] Schied, A., Zhang, T., “A state-constrained differential game arising in optimal portfolio liqui-
dation,” Mathematical Finance, vol. 27, no. 3, pp. 779–802, 2017.
[4] Bertsimas, D., Lo, A. W., “Optimal control of execution costs,” Journal of Financial Markets,
vol. 1, no. 1, pp. 1–50, 1998.
[5] Bouchaud, J.-P., Gefen, Y., Potters, M., Wyart, M., “Fluctuations and response in financial
markets: The subtle nature ofrandom’price changes,” Quantitative finance, vol. 4, no. 2, p. 176,
2003.
[6] Bouchaud, J.-P., Farmer, J. D., Lillo, F., “How markets slowly digest changes in supply and
demand,” in Handbook of financial markets: dynamics and evolution, Elsevier, 2009, pp. 57–160.
[7] Gueant, Olivier, Lehalle, Charles-Albert, Fernandez-Tapia, Joaquin, “Optimal portfolio liquida-
tion with limit orders,” SIAM Journal on Financial Mathematics, vol. 3, no. 1, pp. 740–764,
2012.
[8] Gatheral, J., Schied, A., Slynko, A., “Transient linear price impact and fredholm integral equa-
tions,” Mathematical Finance: An International Journal of Mathematics, Statistics and Finan-
cial Economics, vol. 22, no. 3, pp. 445–474, 2012.
[9] Obizhaeva, A. A., Wang, J., “Optimal trading strategy and supply/demand dynamics,” Journal
of Financial markets, vol. 16, no. 1, pp. 1–32, 2013.
[10] Guéant, O., Lehalle, C.-A., “General intensity shapes in optimal liquidation,” Mathematical
Finance, vol. 25, no. 3, pp. 457–495, 2015.
[11] Cartea, A., Jaimungal, S., Penalva, J., Algorithmic and high-frequency trading. Cambridge Uni-
versity Press, 2015.
21[12] Cartea, A., Jaimungal, S., “Incorporating order-flow into optimal execution,” Mathematics and
Financial Economics, vol. 10, pp. 339–364, 2016.
[13] Casgrain, P., Jaimungal, S., “Trading algorithms with learning in latent alpha models,” Mathe-
matical Finance, vol. 29, no. 3, pp. 735–772, 2019.
[14] Bucci, F., Mastromatteo, I., Eisler, Z., Lillo, F., Bouchaud, J.-P., Lehalle, C.-A., “Co-impact:
Crowding effects in institutional trading activity,” Quantitative Finance, vol. 20, no. 2, pp. 193–
205, 2020.
[15] Carmona, R., Yang, J., “Predatory trading: A game on volatility and liquidity,” Preprint. URL:
http://www. princeton. edu/rcarmona/download/fe/PredatoryTradingGameQF. pdf, 2011.
[16] Micheli, A., Muhle-Karbe, J., Neuman, E., “Closed-loop nash competition for liquidity,” Math-
ematical Finance, vol. 33, no. 4, pp. 1082–1118, 2023.
[17] Brunnermeier, M. K., Pedersen, L. H., “Predatory trading,” The Journal of Finance, vol. 60,
no. 4, pp. 1825–1863, 2005.
[18] Carlin,B.I.,Lobo,M.S.,Viswanathan,S.,“Episodicliquiditycrises:Cooperativeandpredatory
trading,” The Journal of Finance, vol. 62, no. 5, pp. 2235–2274, 2007.
[19] Schöneborn, T., Schied, A., “Liquidation in the face of adversity: Stealth vs. sunshine trading,”
in EFA 2008 Athens Meetings Paper, 2009.
[20] Schied, A., Zhang, T., “A market impact game under transient price impact,” Mathematics of
Operations Research, vol. 44, no. 1, pp. 102–121, 2019.
[21] Drapeau, S., Luo, P., Schied, A., Xiong, D., “An fbsde approach to market impact games with
stochastic parameters,” arXiv preprint arXiv:2001.00622, 2019.
[22] Neuman, E., Voß, M., “Trading with the crowd,” Mathematical Finance, vol. 33, no. 3, pp. 548–
617, 2023.
[23] Cont, R., Guo, X., Xu, R., “Interbank lending with benchmark rates: Pareto optima for a class
of singular control games,” Mathematical Finance, vol. 31, no. 4, pp. 1357–1393, 2021.
[24] Cont, R., Xiong, W., “Dynamics of market making algorithms in dealer markets: Learning and
tacit collusion,” Mathematical Finance, 2022.
[25] Sun,S.,Wang,R.,An,B.,“Reinforcementlearningforquantitativetrading,”ACM Transactions
on Intelligent Systems and Technology, vol. 14, no. 3, pp. 1–29, 2023.
[26] Hambly, B., Xu, R., Yang, H., “Recent advances in reinforcement learning in finance,” Mathe-
matical Finance, vol. 33, no. 3, pp. 437–503, 2023.
[27] Ning, B., Lin, F. H. T., Jaimungal, S., “Double deep q-learning for optimal execution,” Applied
Mathematical Finance, vol. 28, no. 4, pp. 361–380, 2021.
[28] Schnaubelt, M., “Deep reinforcement learning for the optimal placement of cryptocurrency limit
orders,” European Journal of Operational Research, vol. 296, no. 3, pp. 993–1006, 2022.
[29] Macrì, A., Lillo, F., “Reinforcement learning for optimal execution when liquidity is time-
varying,” arXiv e-prints: 2402.12049, 2024.
[30] Karpe, M., Fang, J., Ma, Z., Wang, C., “Multi-agent reinforcement learning in a realistic limit
order book market simulation,” in Proceedings of the First ACM International Conference on
AI in Finance, 2020, pp. 1–7.
[31] Bao, W., Liu, X.-y., “Multi-agent deep reinforcement learning for liquidation strategy analysis,”
arXiv preprint arXiv:1906.11046, 2019.
[32] Waltman, L., Kaymak, U., “Q-learning agents in a cournot oligopoly model,” Journal of Eco-
nomic Dynamics and Control, vol. 32, no. 10, pp. 3275–3293, 2008.
[33] Abada, I., Lambin, X., “Artificial intelligence: Can seemingly collusive outcomes be avoided?”
Management Science, vol. 69, no. 9, pp. 5042–5065, 2023.
[34] Hettich, M., “Algorithmic collusion: Insights from deep learning,” Available at SSRN 3785966,
2021.
[35] Xiong, W., Cont, R., “Interactions of market making algorithms,” Association for Computing
Machinery, 2022.
22[36] Cartea, A., Chang, P., Penalva, J., “Algorithmic collusion in electronic markets: The impact of
tick size,” Available at SSRN 4105954, 2022.
[37] Gobbi, M., Levi, F., Mastinu, G., Previati, G., “On the analytical derivation of the pareto-
optimal set with applications to structural design,” Structural and Multidisciplinary Optimiza-
tion, vol. 51, pp. 645–657, 2015.
[38] Blank, J., Deb, K., “Pymoo: Multi-objective optimization in python,” IEEE Access, vol. 8,
pp. 89497–89509, 2020.
[39] Cordoni,F.,Lillo,F.,“Instabilitiesinmulti-assetandmulti-agentmarketimpactgames,”Annals
of Operations Research, vol. 336, no. 1, pp. 505–539, 2024.
23