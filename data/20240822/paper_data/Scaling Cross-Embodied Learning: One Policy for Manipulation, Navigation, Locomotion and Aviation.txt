Scaling Cross-Embodied Learning: One Policy for
Manipulation, Navigation, Locomotion and Aviation
RiaDoshi‚àó1 HomerWalke‚àó1 OierMees1 SudeepDasari2 SergeyLevine1
1UCBerkeley 2CarnegieMellonUniversity
https://crossformer-model.github.io
Abstract:Modernmachinelearningsystemsrelyonlargedatasetstoattainbroad
generalization, and this often poses a challenge in robot learning, where each
robotic platform and task might have only a small dataset. By training a single
policyacrossmanydifferentkindsofrobots,arobotlearningmethodcanleverage
much broader and more diverse datasets, which in turn can lead to better gener-
alizationandrobustness. However,trainingasinglepolicyonmulti-robotdatais
challengingbecauserobotscanhavewidelyvaryingsensors, actuators, andcon-
trol frequencies. We propose CrossFormer, a scalable and flexible transformer-
basedpolicythatcanconsumedatafromanyembodiment. WetrainCrossFormer
on the largest and most diverse dataset to date, 900K trajectories across 20 dif-
ferent robot embodiments. We demonstrate that the same network weights can
control vastly different robots, including single and dual arm manipulation sys-
tems,wheeledrobots,quadcopters,andquadrupeds.Unlikepriorwork,ourmodel
doesnotrequiremanualalignmentoftheobservationoractionspaces. Extensive
experimentsintherealworldshowthatourmethodmatchestheperformanceof
specialist policies tailored for each embodiment, while also significantly outper-
formingthepriorstateoftheartincross-embodimentlearning.
Keywords: ImitationLearning,Cross-Embodiment
Quadrupeds
900k Robot Trajectories Single Arms
ü§ñ 30 Embodiments Cross-Embodied
üõû Navigation Transformer
ü¶ø Locomotion Navigation
ü¶æ Manipulation üñº Flexible Observation Spaces
ü¶æ Bimanual üïπFlexible Action Spaces ‚àà[2,‚Ä¶,1400]
‚è∞ Control Freq. ‚àà[5Hz,‚Ä¶,20Hz]
Bimanual Arms
Quadcopters
Figure 1: We introduce CrossFormer, a transformer-based policy trained on 900K trajectories of diverse,
multi-embodiedrobotdatathatcancontrolvastlydifferentrobotsincludingsingleanddualarmmanipulation
systems,wheeledrobots,quadcopters,andquadrupeds,whilematchingtheperformanceofspecialistpolicies
targetedtoeachembodimentandoutperformingpriorworkincross-embodimentlearning.
1 Introduction
Muchoftherecentsuccessinmachinelearninghasbeendrivenbytraininggeneral-purposemodels
onincreasinglydiverseandmulti-taskdata. Forexample,visualandlanguagetasks,oncehandled
by task-specific methods, are now performed more effectively by general vision-language models
thatcantransferknowledgeacrosstasks[1,2,3,4]. Similarly,inrobotics,recentdataaggregation
efforts [5] have made it possible to train general-purpose policies on robot data collected across
*Equalcontribution.Correspondingemail:{riadoshi, homer walke}@berkeley.edu
4202
guA
12
]OR.sc[
1v21811.8042:viXra
ü¶æ
ü¶æmultipleembodiments,tasks,andenvironments. Thesegeneralistpoliciescanout-performnarrow
policiestrainedwithdatafromonlythetargetrobotandtaskbytransferringvisualrepresentations
andskills[6,5]. Inadditiontothebenefitsofpositivetransfer, traininggeneralistcross-embodied
policiesminimizestheamountofengineeringthatmustbedonetodesignandtunepolicyarchitec-
turesforeachrobot.
However, trainingageneral-purposerobotpolicyisuniquelychallenging, sincerobotsystemscan
vary widely in their camera views, proprioceptive inputs, joint configurations, action outputs, and
control frequencies. Initial efforts at training large-scale cross-embodied policies have often been
limitedtosinglerobotarmsorgroundnavigationrobotswhichcanbecontrolledwithasinglecam-
era view and relative waypoint action for the base or end-effector [5, 6, 7, 8]. Further increasing
thediversityofembodimentsthesepoliciescancontrolrequiresamodelarchitecturethatsupports
conditioningonanynumberofcameraviewsorproprioceptiveobservations, aswellaspredicting
actionsofanydimension. Followingpriorwork,wetakeasequentialmodelingapproachtocross-
embodiedimitationlearning[9,10]. Weproposeatransformer-basedpolicythatsupportsvariable
observationsandactionsbycastinginputsandoutputstoasequence.Wescalethisapproachtocon-
trolthemostdiversesetofembodimentswithasinglepolicytodate,includingsingleandbimanual
robotarms,groundnavigationrobots,quadcoptersandquadrupeds.
With our transformer policy, we can train on robot data with any number of camera views or pro-
prioceptive sensors by simply tokenizing and arranging the observations into a sequence. At the
same time, we can predict actions of any dimension, crucially without the need to manually align
the action spaces of different embodiments [8]. For each action type, we insert a set of action
readouttokensintotheinputtokensequence. Then,wepassthecorrespondingoutputembeddings
intoaction-spacespecificheadstoproduceavectorofthecorrectdimensions.Ourpolicycanaccept
tasksintheformoflanguageinstructionsorgoalimages,allowingausertochoosethetaskmodality
thatismostnaturalforagivenembodiment.
Our primary contribution is a cross-embodied robot policy trained on the largest and most diverse
robot dataset to date with 900K trajectories and 20 distinct embodiments. Our policy can control
robots with varying observation and action types, from a quadruped with proprioceptive sensors
and 12 joints to a bimanual robot with 3 cameras and 14 joints. We find in extensive real world
experiments that our policy matches the performance of the same architecture trained on just the
targetrobotdata,aswellasthebestpriormethodineachsetting,demonstratingthatourarchitecture
can absorb heterogeneous robot data without negative transfer, while performing comparably to
state-of-the-art specialist methods tailored for each robot. Additionally, we find that our method
outperformsthestateoftheartincross-embodimentlearningwhilealleviatingtheneedformanually
aligningobservationandactionspaces.
2 RelatedWork
Earlyworkoncross-embodiedrobotpolicylearninghasexploredanumberoftechniques,including
conditioning on explicit or learned representations of the embodiment, [11, 12], domain random-
ization and adaptation [13, 14, 15, 16, 17, 18], modular policies [19, 20, 21, 22], or model-based
RL[23,24,25]. Generally,thesepriorprojectshaveoperatedonasmallerscale,onlyevaluatingin
simulationortrainingpoliciesonsmallamountsofrobotdatatocontrolasmallnumberofrobots
forafewtasks.
Anumberofpriorworkshavesoughttoscaleuprobotlearningbyusinglargeamountsofdatafrom
a single robot embodiment, collected either autonomously [26, 27, 28, 29, 30, 31] or with human
teleoperation[32,33,34,35,36,37,38,39]. Weaddressthechallengesinvolvedintrainingpolicies
onevenbroaderrobotdatathatincludesmultipledifferenttypesofrobots. Otherpriorworkstrains
ondatafrommultiplerobotsbutrequireeachrobottohavethesameobservationandactionspace
[7, 40, 5, 41, 42, 43, 44]. For example, Shah et al. [42] train one policy across many navigation
robots using an egocentric camera view and 2D waypoint actions, and the RT-X models [5] are
trainedacrosssingleroboticarmsusinga3rdpersoncameraviewand7-DoFend-effectorposition
actions. Ourmethoddoesnotrequirethedatatohaveacommonobservationandactionspace, so
2we can simultaneously control robots with disjoint sets of sensors and actuators, e.g., robot arms
andquadrupeds.
Therehavebeenafewlarge-scaleeffortstotrainasinglepolicyonrobotdatawithvaryingobser-
vationsandactionspaces[10,9,6,8]. Octo[6]canbefine-tunedonrobotswithobservationsand
actionsthataredistinctfromthoseseenduringpre-training. However,Octoonlypre-trainsondata
from single robot arms and does not explore co-training on more heterogeneous data. Reed et al.
[9]andBousmalisetal.[10]proposeaflexibletransformer-basedpolicythathandlesvaryingobser-
vationandactionspacesduringpre-training. Theydemonstratetheirpolicycancontrolrobotarms
with differentaction spaces, including 4-DoF,6-DoF, 7-DoF, and 14-DoF(using a 3-finger hand).
Weexplorethechallengesinincreasingboththediversityofembodimentsandenvironmentsasin-
glepolicycanoperatein. Alongwithincreasingthenumberofarmembodimentswecancontrol,
e.g., high-frequency bimanual arms, we demonstrate navigation and quadrupedal locomotion, and
weevaluateacrossawiderangeofreal-worldsettings,whiletheylimittheirevaluationtoastandard-
izedcage. Wealsoexplicitlyevaluatefortransferacrossembodimentsbycomparingpolicieswith
and without cross-embodiment co-training, while their focus is on autonomous self-improvement.
Perhapsmostrelatedtoourwork,Yangetal.[8]studytransferacrossmanipulationandnavigation
data. However,theirfocusisonleveragingthefactthategocentricmotioninnavigationlookssimi-
lartoegocentricmotionfromwristcamerasinmanipulation,andtheyperformmanualalignmentof
theactionsacrossthesetwoembodiments. Instead,ourfocusisontrainingapolicythatcancontrol
embodimentsbeyondarmsandgroundnavigationrobots,includingthosewithobservationsandac-
tionspacesthatcannotbecasttoacommonformat.Ourmethod,CrossFormer,isthefirsttoco-train
onfourdistinctactionspaces(single-armmanipulation,groundnavigation,bimanualmanipulation,
andquadrupedallocomotion)withoutanyobservationspaceconstraintsoraction-spacealignment
whilemaintainingstate-of-the-artperformanceoneachrobot.
Traininggeneralist,cross-embodiedpoliciesrequiresmulti-robotdatasets,andtherehavebeensev-
eraleffortstocollectsuchlarge-scalecross-embodieddatasets[45,41,42,5,46]. Inparticular,the
OpenCross-Embodimentdataset(OXE)[5]aggregates1.5millionepisodesofrobotdata, andwe
trainonasubsetof900Ktrajectories.WenotethattheOcto[6]andRT-X[5]modelsarealsotrained
ontheOXEdataset,howevertheyonlyuseasubsetwithsinglerobotarms. Weadditionallyusethe
GNM[41]navigationandDROID[35](large-scaleFrankamanipulation)datasets,alongwithGo1
quadrupedandALOHA[47]bimanualdatacollectedforthisproject.
3 DesigningaCross-EmbodiedPolicy
The primary challenge in robotic learning with many embodiments lies in handling widely vary-
ing observation and actions spaces, as well as differences in control frequency and other aspects
oftheroboticsystem. Roboticsystemscanhavevaryingnumbersofcameraviewsorpropriocep-
tive sensors, and they maybe controlled by avariety of differentaction representations, including
jointangles, Cartesianpositions, andmotortorques. Inordertostandardizethedatatoacommon
format,somepriorworkontrainingcross-embodiedpolicieshasignoredcertainobservationtypes
(such as either wrist or third person views in manipulation) [5, 7] or aligned action spaces across
robots[8]. Instead, followingotherpriorwork[9,10,6], wecastcross-embodiedimitationlearn-
ingasasequence-to-sequenceproblemandchooseatransformer-basedpolicyarchitecturethatcan
handlevaryinglengthsequentialinputsandoutputs.
Duetotheirsequentialnature,atransformerpolicyenablesencodingallavailableobservationtypes
fromeachembodimentbyserializingthemintoaflatsequence. Similarly,wecandecodevariable
length actions which allows us to use the optimal action type for each embodiment. Using this
flexibleoutput,wecanalsopredictvariablysizedchunksofactions. Actionchunking[48,47,49]
improvesthetemporalconsistencyofactionsandreducescompoundingerrorwhichisparticularly
importantforhigh-frequencyfinemanipulation.Together,atransformerbackboneandactionchunk-
ingenablesourpolicytocontrolrobotsrangingfromabimanualALOHAsystemwithjointposition
controlat20Hz,togroundandaerialnavigationrobotswith2Dwaypointcontrolat5Hz.
3Observation Image Tokenization Observation Tokens
Workspace Image Navigation Image Wrist Image Quadruped ProprioBimanual Proprio Readout Tokens
Current ImageGoal Image
Sweep the objects
into the dustpan Cross-Embodied Transformer
Language Encoder
FiLM Conditioning ResNet
p Quadruped Single Arm Navigation Bimanual
Action Head Action Head Action Head Action Head
p
Figure 2: Policy architecture. Our architecture enables cross-embodied policy learning through
a transformer backbone. Our policy accepts variable observation inputs by tokenizing images and
proprioceptiveinformation,predictsvariableactionoutputsusingactionreadouttokens,andcondi-
tionsonlanguageinstructionsorgoalimages.
Atahighlevel, ourtransformerpolicyfollowspriorworkthattrainstransformersonmulti-modal
data[9,10,6]. Observationsandataskspecificationaretokenizedbymodality-specifictokenizers,
assembledintoatokensequence,andfedintoacausal,decoder-onlytransformerbackbonethatis
sharedacrossallembodiments. Theoutputembeddingsarethenfedintoseparateactionheadsfor
eachclassofembodimenttoproduceactionsofthecorrespondingdimension. SeeFigure2foran
overviewofourarchitecture. Below, wedescribeourtrainingdataandtheindividualcomponents
ofourarchitectureingreaterdetail.
3.1 Trainingdata
Our training data mixture covers 20 different robot em-
bodiments, varying widely in observation space, action ü¶øQu8a%druped
space, and control frequency. We start from the single- 8%
armmanipulationsubsetoftheOpenCross-Embodiment ü¶æ Bimanual
17%
dataset used by Octo [6]. Then, we add the DROID
Franka manipulation dataset [35], 7K trajectories of Dataset
ALOHA data collected across two institutions (referred Embodiment
Sampling
to as ALOHA-multi-task), 60 hours of navigation from Weights
58%
theGNMdataset[41],25minutesofwalkingdatafroma Navigation ü¶æ Single Arm
17%
Go1quadruped(referredtoasGo1-walk),and200trajec-
toriesof additionalFrankadatacollected inourownlab
Figure 3: Training Data Mix. We
(referredtoasFranka-tabletop). Weclassifythedatasets
groupthe20embodimentsinourtrain-
that are most relevant to each of our evaluation settings
ing data into categories and visualize
(seeSection4)asthetargetdatasetsandup-weightthem
their contribution to our data mix. The
relative to the other datasets during training. Our tar- piechartshowstheaveragecomposition
getdatasetsareBridgeData[36]forWidowXevaluation, ofeachtrainingbatchbasedonthesam-
ALOHA-multi-task for ALOHA evaluation, GNM [41] plingweights.
fornavigationevaluation, Go1-walkforquadrupedeval-
uation,andFranka-tabletopforFrankaevaluation.WecollectedtheGo1databyrollingoutanexpert
policytrainedwithRLinsimulation[50].SeeFigure3forourtrainingdatamixandAppendixAfor
furtherdetailsonthecompositionofdatasetscollectedourselvesalongwiththesamplingweights.
3.2 Tokenizingvariableobservationtypesandtaskspecifications
Thefirststepintrainingourcross-embodiedpolicyiscreatingtheinputsequence. Thetrajectories
in our robot training data are sequences of time-steps where each time-step contains image obser-
vations I, proprioceptive observations P, and an action. Data from each embodiment may have a
differentnumberofcameraviewspertime-stepandmayormaynotincludeproprioceptiveobser-
vations. Tocreatetheinputsequence,wefirstdefineanobservationhistorylengthkandchunkeach
trajectoryintok-lengthsegments,[I ,P ...,I ,P ]. Wethentokenizeeachobservationaccord-
t t t+k t+k
ingtoitsmodality. ImagesareprocessedwithaResNet-26encoder[51]toproduceafeaturemap
4
ü¶æthat is flattened along the spatial dimensions and projected to the token embedding size. Proprio-
ceptiveobservationsaresimplyprojectedtothetokenembeddingsize. Alongwiththeobservation
sequence,thepolicyalsoacceptsataskspecification. Importantlyforcross-embodiedcontrol,our
policyacceptstaskspecificationseitherintheformofalanguageinstructionloragoalimageg. In
somesettings,likenavigation,tasksaremorenaturallyspecifiedasanimagegoalwhereasinother
settings,likemanipulation,tasksaremoreeasilyspecifiedwithlanguage. Languageinstructionsare
jointlyprocessedwithimageobservationsusingFiLM[52]. Goalimagesarestackedonthecurrent
imagealongthechanneldimensionbeforebeingfedintotheimageencoder.
Because our training data consists of data from single-arm manipulators, dual-arm manipulators,
quadrupeds, and ground navigation robots, our policy supports conditioning on any subset of the
following observation types: (1) Workspace Image: A 3rd person camera view in manipulation
settings. (2) Navigation Image: An egocentric camera view in navigation settings. (3) Wrist
Image: A view from a wrist-mounted camera in manipulation settings. (4) Quadruped Propri-
oception: Joint positions and velocity estimates for quadrupeds. (5) Bimanual Proprioception:
Jointpositionsforbimanualmanipulationsettings.
Tomaximizetransferacrossembodiments,weshareimageencoderweightsforcameraviewsofthe
sametype.So,forexample,theworkspaceimagesinsinglearmandbimanualmanipulationsettings
areprocessedbythesameResNetimageencoder. Intotalweusefourimageencoders: oneforthe
workspaceviewinmanipulationsettings,onefortheegocentricviewfromgroundnavigationrobots,
and two for wrist cameras in manipulation settings. After input tokenization, we have a sequence
ofobservationtokens[I1:L,P1:M...,I1:L,P1:M],whereLandM denotethenumberoftokensfor
t t t+k t+k
imageandproprioceptiveobservations.
3.3 Predictingvariablelengthactions
Aftercreatingtheinputsequence,thenextstepistoprocesstheinputsequencewithatransformer
topredictanactionofanappropriatedimensionforeachembodiment. Weuseatransformerwitha
block-wisecausalattentionmasksuchthatobservationtokenscanonlyattendtoobservationtokens
atthesameorpriortime-stepst. Followingpriorwork[6],weinsertspecialreadouttokensRafter
the observation tokens at each time-step in the input token sequence. These readout tokens can
only attend to prior observation tokens, and thus serve as a convenient representation from which
to predict actions. The final input sequence is [I1:L,P1:M,R1:N...,I1:L,P1:M,R1:N] where N
t t t t+k t+k t+k
denotesthenumberofreadouttokens. Wepasstheinputtokensequencethroughthetransformerto
obtainanembeddingsequence. Wethenapplyanactionheadtotheembeddingscorrespondingto
thereadouttokenstoproduceactions. Thereareseveralpossibilitiesfortheactionhead. Pastwork
hasexploredregressionwithanL1orL2loss,classificationwithacross-entropyloss,ordiffusion.
WechoosetopredictcontinuousactionsandemployanL1lossbecauseofitssuccessinpriorwork
on high-frequency bimanual manipulation [47]. Accordingly, our action heads simply project the
readouttokenembeddingstotheactiondimension. Forsomeembodiments,wepredictachunkof
sequentialactions. Actionchunkinghasbeenshowntoimprovepolicyperformanceinpriorwork
[47,49]andisessentialforembodimentswithhighcontrolfrequencieswherecompoundingerror
wouldbuilduptooquickly.Sinceouractionheadsprojectthereadouttokenstotheactiondimension
size, we match the number of readout tokens to the action chunk size for each embodiment. Our
policy has 4 action heads which produce chunked actions of the following types: (1) Single arm
Cartesianpositions: A7-dimensionalactiondenotingtherelativechangeinCartesianpositionof
the end-effector and the gripper actuation. We predict a chunk of 4 actions and execute on single
robotarmsat5-15Hz[6]. (2)Navigationwaypoints: A2-dimensionalactiondenotingawaypoint
relativetotherobot‚Äôscurrentposition. Wepredictachunkof4actionsandexecuteonnavigation
robots at 4Hz [42] . (3) Bimanual joint positions: A 14-dimensional action denoting the joint
positionsofbotharms. Wepredictachunkof100actionsandexecuteonbimanualrobotarmsat
20Hz[47]. (4)Quadrupedjointpositions: A12-dimensionalaction,predictedwithnochunking,
denoting the joint positions of the legs. We predict only 1 action and execute on a quadruped at
20Hz[50]. Actionchunksizesaretakenfrompriorworkineachrobotsetting.
5WidowX ALOHA Franka LoCoBot Go1 TELLO
Figure4: EvaluationSettings: Ourtasksincludesingle-armmanipulationsettings,dexterousand
bimanualtasksettings,navigation,andaviation. PleaserefertoSec4forafullbreakdown.
3.4 Trainingdetails
Inpractice, wemaskobservationsthataremissingforanembodiment, sothateachbatchelement
contains all observation types and all readout token groups, and these token groups occupy fixed
locationswithinthecontextwindow. Alternatively,formemoryefficiency,observationandreadout
tokens could be densely packed to remove padding and fit more time-steps of context for embod-
iments with fewer observation types. This is a strategy used by prior work [9]. However, by not
fixingobservationandreadouttokentypestoasetlocationinthecontextwindow,themodelwould
needtoinfertheembodimentpurelyfromtheobservationsinordertopredictactionsofthecorrect
type(ratherthanrelyingonthepositionalembeddingsofthereadouttokens). Theobservationsfor
someembodimentscanlooksimilar(suchasnavigationandmanipulationwithonlyawristcamera),
sothisdesignmayrequireappendingaprefixtothetokensequenceindicatingtheembodiment.
Our transformer backbone has 12 layers, 8 attention heads, an MLP dimension of 2048, and a to-
ken embedding size of 512. In total, along with the ResNet-26 image encoders and action heads,
ourmodelhas130Mparameters. WeinitializetheResNet-26encoderswithImageNetpre-trained
weights. Weuseacontextwidowsizeof2135tokens,whichfits5time-stepsofcontextwithallob-
servationandreadouttokengroupspresent.Wefoundthatgoodperformanceonnavigationrequired
5time-stepsofobservationhistoryandusingthiscontextlengthdidnothurtperformanceforother
embodiments. Wetrainedfor300Kgradientstepswithabatchsizeof512whichtook47hourson
aTPUV5e-256pod. WeusetheAdamWoptimizer[53],aninversesquarerootdecaylearningrate
schedule[54],weightdecayof0.1,andgradientclippingof1.0. Weapplystandardimageaugmen-
tations. Duringtraining,weusehindsightgoalrelabelingandsamplefutureobservationsuniformly
atrandomtouseasgoals[55]. Ifalanguageinstructionisavailableforatrajectory, werandomly
mask either the language or goal so that at test time we can condition our policy using either task
specification[56].
4 Evaluation
Our experiments are designed to test whether our single cross-embodied policy can control multi-
plerobotembodimentswithoutsacrificingperformancewhencomparedtorobot-specificimitation
learning approaches. Specifically, we aim to answer the following questions: (1) Can our policy
trained on many robots match the performance of a policy trained on data from only the target
robot? (2) Does our policy match the performance of the best prior imitation learning method in
each robot setting? (3) How does our policy compare to prior cross-embodied policies that align
observationandactionspaces?
Evaluation setups. We evaluate over a wide range of tasks and embodiments (see Fig. 4): (1)
WidowX Manipulation: We use the Bridge setup from Walke et al. [36]. We use an over-the-
shouldercameraviewandandsampleactionsfromthesinglearmheadofourpolicy. Weperform
48trialsperpolicyovertwolanguage-conditionedtasksandtwogoal-conditionedtasks.(2)Franka
Manipulation: WeusetheDROIDsetupfromKhazatskyetal.[35]. Weuseanover-the-shoulder
cameraviewandsampleactionsfromthesinglearmheadofourpolicy. Weperform39trialsper
policy over two language-conditioned tasks. (3) ALOHA Bimanual Manipulation: We use the
ALOHA setup from Zhao et al. [47]. We use 3 camera views, one overhead and two wrist, and
sample actions from the bimanual head of our policy. We perform 20 trials per policy over two
language-conditionedtasks. (4)LoCoBotNavigation: WeusetheLoCoBotsetupfromShahetal.
61 1
0.75 0.75
0.5 0.5
0.25 0.25
0
Average WidowX Franka ALOHA LoCoBot Go1 TELLO
Best Prior Method Single-Robot Dataset CCrroossssFFoormrmeerr
Figure5: RealEvaluation. WecompareCrossFormertothesamearchitecturetrainedonjustthe
datafromthetargetrobot,aswellasthebestpriormethodonthedatafromthetargetrobot.
[42] which has one camera view. We evaluate on suite of three skills: path-following, obstacle
avoidance, and sharp corner-turning. We collect a topological map of goals similar to Shah et al.
[42]andevaluatesuccessbasedondistancetotheclosestnodewithinthetopologicalmapofgoal
images.Wesampleactionsfromthenavigationheadofourpolicy.Weevaluatein6locations,using
onetrialperlocationandpolicyasdoneinpriorwork[41,42]. (5)Go1Quadruped: Weevaluate
on a Unitree Go1 which uses proprioceptive observations o ‚àà R59. We sample actions from the
t
quadrupedheadofourpolicy. Asourevaluationmetricwereporttheaveragerewardachievedover
25minutesnormalizedbythetherewardachievedbytheRL-trainedexpertpolicythatgeneratedthe
data(seeSection3.1). (6)TelloQuadcopter: Finally,weexperimentwithaTelloquadcopterusing
the navigation head of our policy. Since the navigation head outputs 2-D relative waypoints, we
maintainastaticheightthroughoutthetrajectory[42,41]. Notably,wedonottrainonquadcopter
datasothissettingrequireszero-shotgeneralizationtoanewembodiment(thoughnottoanewset
of observation inputs and action outputs). We evaluate in 3 locations, using one trial per location
andpolicyasdoneinpriorwork[41,42].
Baselines and comparison methods. To evaluate how co-training on data from other embodi-
mentsaffectsperformanceforagivenembodiment,wetrainourpolicyarchitectureonjustthedata
from the target embodiment using the target dataset (see Section 3.1) for each evaluation setting.
Becausethesedatasetsaresmallerthanourfulldatamix, wecreatesmallerversionsofourarchi-
tecture, ranging from 5M to 95M parameters, to avoid overfitting. We refer to this method as the
single-robotdatasetbaseline. Wealsocompareourmethodtothebestpriormethodineachset-
tingtoevaluatewhetherwecanmatchtheperformanceofanapproachthatwasspecificallytuned
foreachrobot.FortheWidowXweuseOcto[6],fortheFrankaweuseOpenVLA[57],forALOHA
we use ACT [47], for the LoCoBot and Tello quadcopter we use ViNT [42]. We do not compare
to a prior state-of-the-art method for the Go1 setting due to the lack of a corresponding imitation
learningmethodforquadrupedallocomotion. Finally,inthenavigationandmanipulationsettings,
wecomparetothepolicyfromYangetal.[8]. Wedirectlyusethe186Mparameterpolicytrained
bytheauthors. Yangetal.[8]leveragethefactthattheegocentricviewinnavigationlookssimilar
to wrist camera views in manipulation, and they align the action spaces of these two settings ac-
cordingly. Thiscomparisonevaluateswhetherourapproachcanperformsimilarlywithnomanual
alignmentofactionspaces.
4.1 CanCrossFormermatchtheperformanceoftrainingononlyasingle-robotdatasetas
wellasthebestpriormethodineachsetting?
Figure 5 shows the performance of our method compared to our architecture trained on just the
single-embodiment target datasets for each setting (see Section 3.1) and the best prior method for
each setting. Overall, we find that our method performs comparably to training on just the target
datasetinallevaluations,demonstratingthatourarchitecturecanabsorbdatafromwidelyvarying
71
0.75
0.5
0.25
0
Basic Corner Obstacle Avoidance Sharp Turns Pick & Place
Yang et. al. CCrroossssFFoorrmmeerr
Figure6:ComparisontoYangetal.[8]. WecompareCrossFormertoYangetal.[8],whichaligns
actionsfornavigationandmanipulationandonlyusesasinglecameraviewatatime. CrossFormer
outperformsYangetal.[8]by3xoverall,onbothtabletopmanipulationfromathirdpersoncamera
viewaswellascommonnavigationtasks.
embodimentswithnonegativetransfer. Onaverageoverallembodiments,CrossFormerhasa73%
success rate, while the single-robot dataset baseline has a 67% success rate. Additionally, Cross-
Former performs similarly to the best prior imitation learning method in each evaluation setting,
demonstrating that our policy architecture can match state-of-the-art methods that may have been
specificallydevelopedandtunedforeachrobot.Onaverageoverallembodiments,CrossFormerhas
a73%successratewhilethebestpriormethodhasa51%successrate. Notethatbecausethereis
nosuitablepriorimitationlearningmethodfortheGo1,itisnotincludedinthereportedbestprior
methodaverage.
4.2 Isaligningtheobservationandactionspacesreallyneeded?
Lastly, we compare our policy to Yang et al. [8] in the LoCoBot navigation and WidowX manip-
ulation settings. In the navigation setting, our policy significantly outperforms Yang et al. [8] in
alltestedscenariosincludingturningcorners,avoidingobstacles,andmakingsharpturns. Qualita-
tively,wealsofoundourpolicytobemuchsmoother,navigatinginstraighterlineswithfewerstops
andstarts. BothpoliciesaretrainedonthesameamountofLoCoBotdata(fromtheGNMdataset),
sothisresultindicatesthatourpolicyarchitecturecanbetterfititscross-embodimenttrainingdata.
Surprisingly, in the WidowX manipulation setting, we found that Yang et al. [8] obtained a zero
success rate, despite training on the same relevant WidowX data as CrossFormer (i.e the Bridge
dataset)andadditionalWidowXdatacollectedintheirownlab. Wehypothesizethatthisisbecause
weevaluatefromthe3rd-personcameraview. WhileYangetal.[8]trainwithboth3rd-personand
wristcameraviews,onlyoneviewisinputtothepolicyatatime,potentiallyresultinginunderfitting
to3rd-personcontrol. CrossFormeronlyusesthe3rd-personviewfortheWidowXsettingduetoa
lackofpubliclyavailableWidowXwrist-cameradata. However,wenotethatourpolicycanaccept
avariednumberofcameraviews(usinguptothreesimultaneouslyforbimanualmanipulation)and
does not require forcing two camera views into the same input slot. In summary, our policy out-
performs Yang et al. [8] in both settings, demonstrating that manual alignment of observation and
action spaces is not necessary for good performance, and that our flexible architecture allows our
policytobetterfitheterogeneouscross-embodieddata.
5 DiscussionandConclusion
We introduced CrossFormer, a scalable and flexible transformer policy trained on the largest and
mostdiversedatasettodate,900Ktrajectoriesacross20differentrobotembodiments. Wedemon-
strated a principled way to learn a single policy that can control vastly different embodiments in-
cluding single and dual arm manipulation systems, wheeled robots, quadcopters, and quadrupeds.
OurresultsshowthatCrossFormermatchestheperformanceofspecialistpoliciestailoredforindi-
vidualembodiments,whilealsosignificantlyoutperformingthestateoftheartincross-embodiment
learning. However, our work does have limitations. Our results do not yet show significant posi-
tivetransferacrossembodiments. Weanticipatethataswetrainonlargerrobotdatasetswithmore
8embodiments, we will see greater positive transfer. Another limitation is that our data mix uses
hand-pickedsamplingweightstoavoidover-trainingondatasetswithmanyrepetitiveepisodesand
under-trainingonthedatamostrelevanttoourevaluationsettings. Inprinciple,aswescalemodel
size, the policy should have the capacity to fit all the data equally well without any data weight-
ing. Finally,giventhatweneedlargemodelstofitlargemulti-robotdatasets,themodel‚Äôsinference
speed can become a limiting factor. In this work we successfully applied our policy to a high-
frequency, fine-grained bimanual manipulation task, but as we scale the model‚Äôs size we may not
be able to control these higher frequency embodiments. Future hardware improvements will help
toalleviatetheissue,butfurtherresearchisneededontechniquesforusinglargemodelstocontrol
high-frequencyrobots. Futureworkcouldalsoincludeexploringtechniquestoenablegreaterpos-
itive transfer across embodiments while maintaining the flexibility of our architecture, techniques
for data curation, and incorporating even more diverse data sources like sub-optimal robot data or
action-freehumanvideo. Wehopethatthisworkwillopenthedoorformoregeneral-purposeand
flexiblerobotpoliciesthatcanefficientlylearnandtransferknowledgefromtheexperiencecollected
acrossdiverserobotembodiments.
Acknowledgments
WethankLauraSmith,KyleStachowicz,KarlPertsch,AjaySridhar,JonathanYang,andCatherine
Glossopforhelpwithsettinguptherobotsandbaselines. ThisresearchwassupportedbytheTPU
ResearchCloud,NSFIIS-2150826,ARLDCISTCRAW911NF-17-2-0181,andONRN00014-20-
1-2383.
References
[1] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,
J.Bohg,A.Bosselut,E.Brunskill,E.Brynjolfsson,S.Buch,D.Card,R.Castellon,N.Chat-
terji, A.Chen, K.Creel, J.Q.Davis, D.Demszky, C.Donahue, M.Doumbouya, E.Durmus,
S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel,
N.Goodman,S.Grossman,N.Guha,T.Hashimoto,P.Henderson,J.Hewitt,D.E.Ho,J.Hong,
K.Hsu,J.Huang,T.Icard,S.Jain,D.Jurafsky,P.Kalluri,S.Karamcheti,G.Keeling,F.Khani,
O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee,
T.Lee,J.Leskovec,I.Levent,X.L.Li,X.Li,T.Ma,A.Malik,C.D.Manning,S.Mirchan-
dani,E.Mitchell,Z.Munyikwa,S.Nair,A.Narayan,D.Narayanan,B.Newman,A.Nie,J.C.
Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech,
E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz,
J. Ryan, C. Re¬¥, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin,
R.Taori, A.W.Thomas, F.Trame`r, R.E.Wang, W.Wang, B.Wu, J.Wu, Y.Wu, S.M.Xie,
M.Yasunaga,J.You,M.Zaharia,M.Zhang,T.Zhang,X.Zhang,Y.Zhang,L.Zheng,K.Zhou,
andP.Liang. Ontheopportunitiesandrisksoffoundationmodels,2022.
[2] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al.
Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079,
2023.
[3] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning,2023.
[4] OpenAI. Gpt-4technicalreport,2024.
[5] Open X-Embodiment Collaboration, A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog,
A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, A. Raffin, A. Wahid, B. Burgess-
Limerick, B.Kim, B.Scho¬®lkopf, B.Ichter, C.Lu, C.Xu, C.Finn, C.Xu, C.Chi, C.Huang,
C. Chan, C. Pan, C. Fu, C. Devin, D. Driess, D. Pathak, D. Shah, D. Bu¬®chler, D. Kalash-
nikov,D.Sadigh,E.Johns,F.Ceola,F.Xia,F.Stulp,G.Zhou,G.S.Sukhatme,G.Salhotra,
G. Yan, G. Schiavi, H. Su, H.-S. Fang, H. Shi, H. B. Amor, H. I. Christensen, H. Furuta,
H.Walke,H.Fang,I.Mordatch,I.Radosavovic,I.Leal,J.Liang,J.Kim,J.Schneider,J.Hsu,
J.Bohg,J.Bingham,J.Wu,J.Wu,J.Luo,J.Gu,J.Tan,J.Oh,J.Malik,J.Tompson,J.Yang,
9J. J. Lim, J. Silve¬¥rio, J. Han, K. Rao, K. Pertsch, K. Hausman, K. Go, K. Gopalakrishnan,
K.Goldberg,K.Byrne,K.Oslund,K.Kawaharazuka,K.Zhang,K.Majd,K.Rana,K.Srini-
vasan,L.Y.Chen,L.Pinto,L.Tan,L.Ott,L.Lee,M.Tomizuka,M.Du,M.Ahn,M.Zhang,
M. Ding, M. K. Srirama, M. Sharma, M. J. Kim, N. Kanazawa, N. Hansen, N. Heess, N. J.
Joshi,N.Suenderhauf,N.D.Palo,N.M.M.Shafiullah,O.Mees,O.Kroemer,P.R.Sanketi,
P. Wohlhart, P. Xu, P. Sermanet, P. Sundaresan, Q. Vuong, R. Rafailov, R. Tian, R. Doshi,
R. Mart¬¥ƒ±n-Mart¬¥ƒ±n, R. Mendonca, R. Shah, R. Hoque, R. Julian, S. Bustamante, S. Kirmani,
S.Levine,S.Moore,S.Bahl,S.Dass,S.Song,S.Xu,S.Haldar,S.Adebola,S.Guist,S.Nasiri-
any, S.Schaal, S.Welker, S.Tian, S.Dasari, S.Belkhale, T.Osa, T.Harada, T.Matsushima,
T. Xiao, T. Yu, T. Ding, T. Davchev, T. Z. Zhao, T. Armstrong, T. Darrell, V. Jain, V. Van-
houcke,W.Zhan,W.Zhou,W.Burgard,X.Chen,X.Wang,X.Zhu,X.Li,Y.Lu,Y.Chebotar,
Y.Zhou,Y.Zhu,Y.Xu,Y.Wang,Y.Bisk,Y.Cho,Y.Lee,Y.Cui,Y.huaWu,Y.Tang,Y.Zhu,
Y.Li, Y.Iwasawa, Y.Matsuo, Z.Xu, andZ.J.Cui. OpenX-Embodiment: Roboticlearning
datasetsandRT-Xmodels. https://arxiv.org/abs/2310.08864,2023.
[6] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna,
C. Xu, J. Luo, T. Kreiman, Y. Tan, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and
S.Levine. Octo: Anopen-sourcegeneralistrobotpolicy. InProceedingsofRobotics: Science
andSystems,Delft,Netherlands,2024.
[7] J.H.Yang,D.Sadigh,andC.Finn.Polybot:Trainingonepolicyacrossrobotswhileembracing
variability.In7thAnnualConferenceonRobotLearning,2023.URLhttps://openreview.
net/forum?id=HEIRj51lcS.
[8] J.Yang,C.Glossop,A.Bhorkar,D.Shah,Q.Vuong,C.Finn,D.Sadigh,andS.Levine. Push-
ingthelimitsofcross-embodimentlearningformanipulationandnavigation. arXivpreprint
arXiv:2402.19432,2024.
[9] S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-Maron,M.Gimenez,
Y. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint
arXiv:2205.06175,2022.
[10] K. Bousmalis, G. Vezzani, D. Rao, C. Devin, A. X. Lee, M. Bauza, T. Davchev, Y. Zhou,
A.Gupta,A.Raju,etal.Robocat:Aself-improvingfoundationagentforroboticmanipulation.
arXivpreprintarXiv:2306.11706,2023.
[11] T. Chen, A. Murali, and A. Gupta. Hardware conditioned policies for multi-robot transfer
learning. AdvancesinNeuralInformationProcessingSystems,31,2018.
[12] C.Schaff,D.Yunis,A.Chakrabarti,andM.R.Walter.Jointlylearningtoconstructandcontrol
agents using deep reinforcement learning. In 2019 international conference on robotics and
automation(ICRA),pages9798‚Äì9805.IEEE,2019.
[13] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic
controlwithdynamicsrandomization. In2018IEEEinternationalconferenceonroboticsand
automation(ICRA),pages3803‚Äì3810.IEEE,2018.
[14] A. Kumar, Z. Fu, D. Pathak, and J. Malik. Rma: Rapid motor adaptation for legged robots.
arXivpreprintarXiv:2107.04034,2021.
[15] G. Feng, H. Zhang, Z. Li, X. B. Peng, B. Basireddy, L. Yue, Z. Song, L. Yang, Y. Liu,
K. Sreenath, et al. Genloco: Generalized locomotion controllers for quadrupedal robots. In
ConferenceonRobotLearning,pages1893‚Äì1903.PMLR,2023.
[16] A.Rajeswaran,S.Ghotra,B.Ravindran,andS.Levine.Epopt:Learningrobustneuralnetwork
policiesusingmodelensembles. arXivpreprintarXiv:1610.01283,2016.
10[17] J.Tobin, R.Fong, A.Ray, J.Schneider, W.Zaremba, andP.Abbeel. Domainrandomization
for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ
internationalconferenceonintelligentrobotsandsystems(IROS),pages23‚Äì30.IEEE,2017.
[18] W.Yu,J.Tan,C.K.Liu,andG.Turk. Preparingfortheunknown: Learningauniversalpolicy
withonlinesystemidentification. arXivpreprintarXiv:1702.02453,2017.
[19] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network
policies for multi-task and multi-robot transfer. In 2017 IEEE international conference on
roboticsandautomation(ICRA),pages2169‚Äì2176.IEEE,2017.
[20] W. Huang, I. Mordatch, and D. Pathak. One policy to control them all: Shared modular
policiesforagent-agnosticcontrol. InInternationalConferenceonMachineLearning,pages
4455‚Äì4464.PMLR,2020.
[21] Y. Chen, Y. Chen, Z. Hu, T. Yang, C. Fan, Y. Yu, and J. Hao. Learning action-transferable
policywithactionembedding. arXivpreprintarXiv:1909.02291,2019.
[22] H.You,T.Yang,Y.Zheng,J.Hao,andM.ETaylor. Cross-domainadaptivetransferreinforce-
mentlearningbasedonstate-actioncorrespondence. InUncertaintyinArtificialIntelligence,
pages2299‚Äì2309.PMLR,2022.
[23] E.S.Hu,K.Huang,O.Rybkin,andD.Jayaraman. Knowthyself: Transferablevisualcontrol
policiesthroughrobot-awareness. arXivpreprintarXiv:2107.09047,2021.
[24] J.Fu,S.Levine,andP.Abbeel. One-shotlearningofmanipulationskillswithonlinedynamics
adaptationandneuralnetworkpriors. In2016IEEE/RSJInternationalConferenceonIntelli-
gentRobotsandSystems(IROS),pages4019‚Äì4026.IEEE,2016.
[25] G. Salhotra, I.-C. A. Liu, and G. S. Sukhatme. Learning robot manipulation from cross-
morphologydemonstration. In7thAnnualConferenceonRobotLearning,2023.
[26] L.PintoandA.Gupta. Supersizingself-supervision: Learningtograspfrom50ktriesand700
robothours.In2016IEEEinternationalconferenceonroboticsandautomation(ICRA),pages
3406‚Äì3413.IEEE,2016.
[27] S.Levine,P.Pastor,A.Krizhevsky,J.Ibarz,andD.Quillen. Learninghand-eyecoordination
for robotic grasping with deep learning and large-scale data collection. The International
journalofroboticsresearch,37(4-5):421‚Äì436,2018.
[28] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly,
M. Kalakrishnan, V. Vanhoucke, et al. QT-Opt: Scalable deep reinforcement learning for
vision-basedroboticmanipulation. arXivpreprintarXiv:1806.10293,2018.
[29] C.FinnandS.Levine. Deepvisualforesightforplanningrobotmotion. In2017IEEEInter-
nationalConferenceonRoboticsandAutomation(ICRA),pages2786‚Äì2793.IEEE,2017.
[30] A. Gupta, A. Murali, D. P. Gandhi, and L. Pinto. Robot learning in homes: Improving gen-
eralizationandreducingdatasetbias. Advancesinneuralinformationprocessingsystems,31,
2018.
[31] T.Lampe,A.Abdolmaleki,S.Bechtle,S.H.Huang,J.T.Springenberg,M.Bloesch,O.Groth,
R. Hafner, T. Hertweck, M. Neunert, et al. Mastering stacking of diverse shapes with large-
scaleiterativereinforcementlearningonrealrobots. arXivpreprintarXiv:2312.11374,2023.
[32] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,J.Dabis,C.Finn,K.Gopalakrishnan,K.Haus-
man,A.Herzog,J.Hsu,etal. Rt-1:Roboticstransformerforreal-worldcontrolatscale. arXiv
preprintarXiv:2212.06817,2022.
11[33] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid,
etal. Rt-2: Vision-language-actionmodelstransferwebknowledgetoroboticcontrol. In7th
AnnualConferenceonRobotLearning,2023.
[34] Y.Jiang,A.Gupta,Z.Zhang,G.Wang,Y.Dou,Y.Chen,L.Fei-Fei,A.Anandkumar,Y.Zhu,
andL.Fan. VIMA:Robotmanipulationwithmultimodalprompts. InA.Krause,E.Brunskill,
K.Cho,B.Engelhardt,S.Sabato,andJ.Scarlett,editors,Proceedingsofthe40thInternational
ConferenceonMachineLearning,volume202ofProceedingsofMachineLearningResearch,
pages 14975‚Äì15022. PMLR, 23‚Äì29 Jul 2023. URL https://proceedings.mlr.press/
v202/jiang23b.html.
[35] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany,
M.K.Srirama,L.Y.Chen,K.Ellis,etal. Droid: Alarge-scalein-the-wildrobotmanipulation
dataset. arXivpreprintarXiv:2403.12945,2024.
[36] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch,
Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine. Bridgedata v2: A dataset
forrobotlearningatscale,2023.
[37] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and
S.Levine. Bridgedata: Boostinggeneralizationofroboticskillswithcross-domaindatasets.
arXivpreprintarXiv:2109.13396,2021.
[38] A.Mandlekar,Y.Zhu,A.Garg,J.Booher,M.Spero,A.Tung,J.Gao,J.Emmons,A.Gupta,
E.Orbay,etal. Roboturk: Acrowdsourcingplatformforroboticskilllearningthroughimita-
tion. InConferenceonRobotLearning,pages879‚Äì893.PMLR,2018.
[39] H.Bharadhwaj,J.Vakil,M.Sharma,A.Gupta,S.Tulsiani,andV.Kumar. Roboagent: Gener-
alizationandefficiencyinrobotmanipulationviasemanticaugmentationsandactionchunking.
arXivpreprintarXiv:2309.01918,2023.
[40] K.Kang,G.Kahn,andS.Levine.Hierarchicallyintegratedmodels:Learningtonavigatefrom
heterogeneousrobots. arXivpreprintarXiv:2106.13280,2021.
[41] D. Shah, A. Sridhar, A. Bhorkar, N. Hirose, and S. Levine. Gnm: A general navigation
modeltodriveanyrobot. In2023IEEEInternationalConferenceonRoboticsandAutomation
(ICRA),pages7226‚Äì7233.IEEE,2023.
[42] D.Shah,A.Sridhar,N.Dashora,K.Stachowicz,K.Black,N.Hirose,andS.Levine. ViNT:A
foundationmodelforvisualnavigation. In7thAnnualConferenceonRobotLearning,2023.
URLhttps://arxiv.org/abs/2306.14846.
[43] A.Loquercio,A.I.Maqueda,C.R.Del-Blanco,andD.Scaramuzza. Dronet: Learningtofly
bydriving. IEEERoboticsandAutomationLetters,3(2):1088‚Äì1095,2018.
[44] M. Shafiee, G. Bellegarda, and A. Ijspeert. Manyquadrupeds: Learning a single locomotion
policyfordiversequadrupedrobots. arXivpreprintarXiv:2310.10486,2023.
[45] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and
C.Finn. Robonet: Large-scalemulti-robotlearning. InConferenceonRobotLearning,pages
885‚Äì897.PMLR,2020.
[46] H.-S.Fang, H.Fang, Z.Tang, J.Liu, C.Wang, J.Wang, H.Zhu, andC.Lu. Rh20t: Acom-
prehensiveroboticdatasetforlearningdiverseskillsinone-shot. TowardsGeneralistRobots:
LearningParadigmsforScalableSkillAcquisition@CoRL2023,3:5,2023.
[47] T.Z.Zhao, V.Kumar, S.Levine, andC.Finn. Learningfine-grainedbimanualmanipulation
withlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
12[48] D.Q.MayneandH.Michalska.Recedinghorizoncontrolofnonlinearsystems.InProceedings
ofthe27thIEEEConferenceonDecisionandControl,pages464‚Äì465.IEEE,1988.
[49] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and
Systems(RSS),2023.
[50] L.Smith,I.Kostrikov,andS.Levine. Awalkinthepark: Learningtowalkin20minuteswith
model-freereinforcementlearning. arXivpreprintarXiv:2208.07860,2022.
[51] K.He, X.Zhang, S.Ren, andJ.Sun. Deepresiduallearningforimagerecognition. InPro-
ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770‚Äì778,
2016.
[52] E.Perez,F.Strub,H.DeVries,V.Dumoulin,andA.Courville. Film: Visualreasoningwith
ageneralconditioninglayer. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume32,2018.
[53] I.LoshchilovandF.Hutter. Decoupledweightdecayregularization. InInternationalConfer-
enceonLearningRepresentations,2018.
[54] X.Zhai,A.Kolesnikov,N.Houlsby,andL.Beyer.Scalingvisiontransformers.InProceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104‚Äì
12113,2022.
[55] M.Andrychowicz,F.Wolski,A.Ray,J.Schneider,R.Fong,P.Welinder,B.McGrew,J.Tobin,
P.Abbeel,andW.Zaremba. Hindsightexperiencereplay. InNeurIPS,2017.
[56] C.LynchandP.Sermanet. Languageconditionedimitationlearningoverunstructureddata. In
RSS,2021.
[57] M. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster,
P.Sanketi,Q.Vuong,T.Kollar,B.Burchfiel,R.Tedrake,D.Sadigh,S.Levine,P.Liang,and
C.Finn. Openvla: Anopen-sourcevision-language-actionmodel. arXivpreprint,2024.
[58] E.Jang,A.Irpan,M.Khansari,D.Kappler,F.Ebert,C.Lynch,S.Levine,andC.Finn. Bc-z:
Zero-shottaskgeneralizationwithroboticimitationlearning. InConferenceonRobotLearn-
ing,pages991‚Äì1002.PMLR,2022.
[59] S.Belkhale,Y.Cui,andD.Sadigh. Hydra: Hybridrobotactionsforimitationlearning. arxiv,
2023.
[60] C.Lynch,A.Wahid,J.Tompson,T.Ding,J.Betker,R.Baruch,T.Armstrong,andP.Florence.
Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters,
2023.
[61] E.Rosete-Beas,O.Mees,G.Kalweit,J.Boedecker,andW.Burgard. Latentplansfortaskag-
nosticofflinereinforcementlearning. InProceedingsofthe6thConferenceonRobotLearning
(CoRL),2022.
[62] O. Mees, J. Borja-Diaz, and W. Burgard. Grounding language with visual affordances over
unstructureddata. InProceedingsoftheIEEEInternationalConferenceonRoboticsandAu-
tomation(ICRA),London,UK,2023.
[63] M.Heo, Y.Lee, D.Lee, andJ.J.Lim. Furniturebench: Reproduciblereal-worldbenchmark
forlong-horizoncomplexmanipulation. InRobotics: ScienceandSystems,2023.
[64] R.Shah,R.Mart¬¥ƒ±n-Mart¬¥ƒ±n,andY.Zhu. MUTEX:Learningunifiedpoliciesfrommultimodal
task specifications. In 7th Annual Conference on Robot Learning, 2023. URL https://
openreview.net/forum?id=PwqiqaaEzJ.
13[65] S. Nasiriany, T. Gao, A. Mandlekar, and Y. Zhu. Learning and retrieval from prior data for
skill-basedimitationlearning. InConferenceonRobotLearning(CoRL),2022.
[66] A.Mandlekar,Y.Zhu,A.Garg,J.Booher,M.Spero,A.Tung,J.Gao,J.Emmons,A.Gupta,
E.Orbay,S.Savarese,andL.Fei-Fei. RoboTurk: Acrowdsourcingplatformforroboticskill
learning through imitation. CoRR, abs/1811.02790, 2018. URL http://arxiv.org/abs/
1811.02790.
[67] G.Zhou,V.Dean,M.K.Srirama,A.Rajeswaran,J.Pari,K.Hatch,A.Jain,T.Yu,P.Abbeel,
L.Pinto, C.Finn, andA.Gupta. Trainoffline, testonline: Arealrobotlearningbenchmark,
2023.
[68] H. Liu, S. Nasiriany, L. Zhang, Z. Bao, and Y. Zhu. Robot learning on the job: Human-in-
the-loopautonomyandlearningduringdeployment. InRobotics: ScienceandSystems(RSS),
2023.
[69] L. Y. Chen, S. Adebola, and K. Goldberg. Berkeley UR5 demonstration dataset. https:
//sites.google.com/view/berkeley-ur5/home.
[70] S. Saxena, M. Sharma, and O. Kroemer. Multi-resolution sensing for real-time control with
vision-language models. In 7th AnnualConference onRobot Learning, 2023. URL https:
//openreview.net/forum?id=WuBv9-IGDUA.
[71] Y.Zhu,A.Joshi,P.Stone,andY.Zhu. Viola:Imitationlearningforvision-basedmanipulation
withobjectproposalpriors,2023.
[72] X.Zhu,R.Tian,C.Xu,M.Ding,W.Zhan,andM.Tomizuka. Fanucmanipulation: Adataset
forlearning-basedmanipulationwithfanucmate200idrobot. 2023.
[73] Z. J. Cui, Y. Wang, N. M. M. Shafiullah, and L. Pinto. From play to policy: Conditional
behaviorgenerationfromuncuratedrobotdata. InTheEleventhInternationalConferenceon
LearningRepresentations,2022.
[74] S. Dass, J. Yapeter, J. Zhang, J. Zhang, K. Pertsch, S. Nikolaidis, and J. J. Lim. CLVR jaco
playdataset,2023. URLhttps://github.com/clvrai/clvr_jaco_play_dataset.
[75] J.Luo,C.Xu,X.Geng,G.Feng,K.Fang,L.Tan,S.Schaal,andS.Levine. Multi-stagecable
routingthroughhierarchicalimitationlearning. arXivpreprintarXiv:2307.08927,2023.
[76] Y. Zhu, P. Stone, and Y. Zhu. Bottom-up skill discovery from unsegmented demonstrations
forlong-horizonrobotmanipulation.IEEERoboticsandAutomationLetters,7(2):4126‚Äì4133,
2022.
[77] R.Mendonca, S. Bahl, andD.Pathak. Structured worldmodelsfromhuman videos. CoRL,
2023.
[78] G. Quere, A. Hagengruber, M. Iskandar, S. Bustamante, D. Leidner, F. Stulp, and J. Vogel.
SharedControlTemplatesforAssistiveRobotics. In2020IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),page7,Paris,France,2020.
14Moreinformationandvideoscanbefoundonourwebsite: crossformer-model.github.io.
A TrainingData
We list the sampling weights for each dataset in our data mixture in Table 1. We up-weight our
targetdatasets: Bridge,ALOHA-multi-task,GNM,Go1-walk,andFranka-tabletop.
CrossFormerDatasetMixture
Fractal[32] 17%
Kuka[28] 2.2%
BC-Z[58] 2.2%
StanfordHydraDataset[59] 0.015%
LanguageTable[60] 1.5%
TacoPlay[61,62] 1.2%
FurnitureBenchDataset[63] 0.83%
UTAustinMutex[64] 0.76%
AustinSailorDataset[65] 0.74%
Roboturk[66] 0.79%
Toto[67] 0.68%
AustinSiriusDataset[68] 0.59%
BerkeleyAutolabUR5[69] 0.41%
IAMLabCMUPickupInsert[70] 0.31%
Viola[71] 0.32%
BerkeleyFanucManipulation[72] 0.26%
NYUFrankaPlayDataset[73] 0.28%
JacoPlay[74] 1.6%
BerkeleyCableRouting[75] 0.089%
AustinBudsDataset[76] 0.072%
CMUStretch[77] 0.053%
DLREDANSharedControl[78] 0.019%
DROID[35] 0.022%
Bridge[37,36] 17%
GNM[41] 17%
ALOHA-multi-task 17%
Go1-walk 8.5%
Franka-tabletop 8.5%
Table 1: The CrossFormer training data mixture uses datasets from the Open X-Embodiment
dataset[5]andadditionaldatacollectedforthisproject.
B TrainingHyperparameters
InTable2welistthehyperparametersfortheoptimizerandpolicyarchitecture. Intotal,alongwith
theResNet-26imageencodersandactionheads,ourmodelhas130Mparameters. Weinitializethe
ResNet-26encoderswithImageNetpre-trainedweights. Trainingtook80hoursonaTPUV5e-256
pod. Weapplycolorjitterandrandomresizing/croppingimageaugmentations. Duringtraining,we
use hindsight goal relabeling and sample future observations uniformly at random to use as goals
[55]. If a language instruction is available for a trajectory, we randomly mask either the language
orgoalsothatattesttimewecanconditionourpolicyusingeithertaskspecification[56]. Forour
method and baselines we trained, we selected checkpoints based on the validation mean squared
errorsincewefoundthisveryroughlycorrelateswithrobotperformance.
C EvaluationSetups
Below we provide further details on our evaluation settings (see Fig. 4 for images). For goal-
condtionedtasks,wearrangethesceneandtakethegoalimagebeforerollingoutthepolicy. Forthe
language-conditionedtasks,weuselanguageinstructionsthataresemanticallyinthedistributionof
thetrainingdata(thoughmaynotappearexactlyinthetrainingdata).
15Hyperparameter Value
Optimizer AdamW[53]
LearningRate 3e-4
WarmupSteps 2000
LRScheduler reciprocalsquare-root
WeightDecay 0.1
GradientClipThreshold 1
BatchSize 512
Layers 12
Attentionheads 8
Tokenembeddingsize 512
MLPdimension 2048
Contextlength 2135
Totaltrainingsteps 300K
Table2: TraininghyperparametersforCrossFormer.
For the manipulation settings, we ran the models on a machine with an NVIDIA 4090 (or similar
GPU)thatwasconnectedtotherobot. Forthegroundnavigationanddroneexperiments,weranthe
modelsonaseparateserverandsentactionsoverawirelessnetwork.
WidowX Manipulation We use the Bridge setup from Walke et al. [36]. We use an over-the-
shouldercameraviewandandsampleactionsfromthesinglearmheadofourpolicy. Weevaluated
twolanguage-conditionedtasks,puttingaspoononaclothandputtingacarrotonaplate,andtwo
goal-conditioned tasks, putting a mushroom in a pot and putting a cloth on a saucer. We used 12
trialsperpolicyandtask. Positionsoftheobjectswerevariedbetweentrials.
FrankaManipulation WeusetheDROIDsetupfromKhazatskyetal.[35]. Weuseanover-the-
shouldercameraviewandsampleactionsfromthesinglearmheadofourpolicy. Weevaluatedfor
27 trials per policy on the language-conditioned task of using a sponge to sweep pinecones into a
dustpan.Wealsoevaluatedfor12trialsperpolicyonthetaskofflippingapotupright.Thepositions
oftheobjectswerevariedbetweentrials.
ALOHABimanualManipulation WeusetheALOHAsetupfromZhaoetal.[47]. Weusethree
cameraviews,oneoverheadandtwowrist,andsampleactionsfromthebimanualheadofourpolicy.
Weevaluatedtwolanguage-conditionedtasks,takingthecapoffofapenandpickingupaknifeand
usingittocutapieceofsushi. Weused10trialsperpolicyandtask. Thepositionofthepen,knife,
andsushiwasvariedbetweentrials.
LoCoBot Navigation We use the LoCoBot setup from Shah et al. [42] which has one camera
view. We evaluate on suite of three skills: path-following, obstacle avoidance, and sharp corner-
turning. Wesampleactionsfromthenavigationheadofourpolicy. Wecombineourpolicywiththe
graph-basedplanneranddistancefunctionfromShahetal.[42]. Wefirstobtainatopologicalmap
Moftheenvironmentbyteleoperatingtherobot. Then,ateverytime-stepwefindtheclosestnode
in M, search the graph for the shortest path from this node to the goal, and command the policy
withthemostimmediatesubgoalinthepath. Weevaluatethesuccessofatrajectorybasedonthe
numberofsubgoalsbetweentheclosestnodeattheendofthetrajectoryandthegoal. Weevaluate
in6locations,usingonetrialperlocationandpolicyasdoneinpriorwork[41,42].
Go1 Quadruped We evaluate on a Unitree Go1 which uses proprioceptive observations o ‚àà
t
R59. We sample actions from the quadruped head of our policy, and the task is to walk forward.
Importantly, unlike prior work [8], we directly control the quadruped‚Äôs joints rather than doing
higherlevelcontrolwithnavigationwaypoints. Asourevaluationmetricwereportrewardachieved
16Task Single-RobotDataset BestPriorMethod CrossFormer
Putthespoononthecloth 0.25(12) 0.25(12) 0.25(12)
Putthemushroominthepot 0.00(12) 0.17(12) 0.25(12)
WidowX
Puttheclothonthesaucer 0.75(12) 0.67(12) 0.75(12)
Putthecarrotontheplate 0.67(12) 0.25(12) 0.33(12)
Sweepthepineconesintothedustpan 0.41(27) 0.52(27) 0.41(27)
Franka
Flipthepotupright 0.83(12) 0.67(12) 0.83(12)
Uncapthepen 0.60(10) 0.70(10) 0.80(10)
ALOHA
Usetheknifetocutthesushi 0.40(10) 0.30(10) 0.60(10)
Obstacleavoidance 0.95(2) 0.30(2) 0.95(2)
LoCoBot Cornering 0.95(2) 0.85(2) 0.95(2)
Sharpcornering 0.85(2) 0.30(2) 0.88(2)
TelloQuadcopter Cornering 0.68(3) 0.68(3) 0.82(3)
Go1 Walking 1 N/A 1
Average 0.68(116) 0.51(116) 0.73(116)
Table 3: Real Evaluation (detailed). We compare CrossFormer to the same architecture trained
on just the data from the target robot, as well as the best prior method on the data from the target
robot. Thenumberoftrialsisinparentheses. Forgroundandaerialnavigation(LoCoBotandTello
Quadcopter)thesuccessmetricistheproportionofsubgoalsreachedonthepathtowardsthegoal.
FortheGo1,wereportrewardachievedover25minutesnormalizedbytherewardachievedbythe
RL-trained expert policy that generated the data. Because there is no suitable best prior imitation
learningmethodfortheGo1,wedonotincludeitinthebestpriormethodaverage.
over25minutesnormalizedbytherewardachievedbytheRL-trainedexpertpolicythatgenerated
thedata(seeSection3.1).
TelloQuadcopter Finally,weperformevaluationonaTelloquadcopterusingthenavigationhead
ofourpolicy. Sincethenavigationheadoutputs2-Drelativewaypoints,wemaintainastaticheight
throughout the trajectory [42, 41]. Notably, we do not train on quadcopter data so this setting
requires zero-shot generalization to a new embodiment. We evaluated on the skill of turning a
corner. We evaluated in 3 locations, using one trial per policy and location as done in prior work
[41,42].
D EvaluationResults
InTable3weprovideamoredetailedbreakdownofourevaluationresultsbytask.
17