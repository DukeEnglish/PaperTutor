The Problems with Proxies: Making Data Work Visible through Requester
Practices
Annabel Rothschild1, Ding Wang2, Niveditha JayakumarVilvanathan1, Lauren Wilcox1, Carl
DiSalvo1, Betsy DiSalvo1
1GeorgiaInstituteofTechnology
855thStNW,Atlanta,Georgia30332USA
2GoogleResearch
1105WPeachtreeStNW,Atlanta,GA30309
{arothschild,nvilvanathan3,cdisalvo}@gatech.edu,drdw@google.com,lgw231@acm.org,bdisalvo@cc.gatech.edu
Abstract thefieldofdataannotationthatworkintherequesters’favor
(Miceli, Schuessler, and Yang 2020; Kapania, Taylor, and
FairnessinAIandMLsystemsisincreasinglylinkedtothe
Wang2023).Theoversightofthisrealityisstarklyapparent
propertreatmentandrecognitionofdataworkersinvolvedin
in the widely acclaimed White House Blueprint for an AI
trainingdatasetdevelopment.Yet,thosewhocollectandan-
BillofRights(OSTP2022),which,infocusingexclusively
notatethedata,andthushavethemostintimateknowledgeof
on consumer protection and system accountability, fails to
itsdevelopment,areoftenexcludedfromcriticaldiscussions.
Thisexclusionpreventsdataannotators,whoaredomainex- specifically acknowledge the pivotal role of data workers.
perts,fromcontributingeffectivelytodataset contextualiza- Thisomissionunderscoresasignificantgapinpolicy—-the
tion.Ourinvestigationintothehiringandengagementprac- lackofrecognitionandprotectionfordataworkersasinte-
tices of 52 data work requesters on platforms like Amazon graltotheethicaldevelopmentandoperationofAIsystems.
Mechanical Turk reveals a gap: requesters frequently hold
The focus on better understanding datasets from work-
naive or unchallenged notions of worker identities and ca-
ers’ perspectives is not misplaced. Data annotators bring a
pabilities and rely on ad-hoc qualification tasks that fail to
lived experience and perspective to the datasets they work
respect theworkers’ expertise. Thesepracticesnot onlyun-
on(Dentonet al. 2021a).We strugglewith the behaviorof derminethequalityofdatabutalsotheethicalstandardsofAI
development.Torectifytheseissues,weadvocateforpolicy systems that generalize off training datasets we don’t un-
changes to enhance how data annotation tasks are designed derstandthecompositionof;suchsystemsintroducesignif-
andmanagedandtoensuredataworkersaretreatedwiththe icant problems, such as severe discrimination, into AI and
respecttheydeserve. MLsystems.(Barrett,Chen,andZhang2023;Buolamwini
and Gebru 2018; Birhane, Prabhu, and Kahembwe 2021;
CouldryandMejias2019).Giventhelongevityandenduring
Introduction
relevance of these massive datasets (Thylstrup et al. 2022;
Muchattentionhasbeenpaidtothecreationofdatasets,in Denton et al. 2021b; Paullada et al. 2021), this issue is a
areas spanninghuman-centereddata science (Aragonet al. pressing one. Workers have a unique position to observe,
2022; Muller et al. 2019a), to critical reflections on the identify, and report on the emergence of these trends, e.g.,
values shaping dataset construction (Denton et al. 2021b; inadequaterepresentationofdifferentsubgroups(Rolfetal.
Scheuerman,Hanna, and Denton2021),with an eye to the 2021).Historically,though,workershavenotbeenaskedto
effects of dataset creation on overall system performance sharetheirperspectives.Instead,theyhavebeenactivelydis-
of Artificial Intelligence (AI) and Machine Learning (ML) couragedfromdoingsobyrequesters,aspartofthetrendto-
systems (Vasudevanet al. 2022).Effortsin AI ethics, such wardsatomizationindigitalpiecework(GrayandSuri2019;
asthedevelopmentoftransparencyartifactslikeDatasheets Jones2021;SambasivanandVeeraraghavan2022).
forDatasets(Gebruetal.2021)andsimilarinitiatives(e.g.,
How do we reset the relationship between data workers
(D´ıazetal.2022;Rostamzadehetal.2022;Srinivasanetal.
andrequesters?Ifdataworkersaretreatedasdomainexperts
2021)),typicallyrequirethoserequestingdatasetstoprovide
andpartnersinethicaldatacollection(JoandGebru2020),
detailedinformationabouthowthedatasetswerecompiled.
theirperspectivescanenricheffortstodocumentdatasetde-
Crowdworkers who collected or annotated the dataset are
velopmentandfairuse.Suchinclusionisnotthestatusquo,
bestpositionedtogivethesedetails;however,ratherthanbe-
however.Further,wedonotknowhowrequestersconceptu-
ing seen as domainexperts(Miceliand Posada 2022)with
alizeworkers,theiridentities,values,commitments,androle
valuableperspectivestoshare(Wang,Prabhat,andSamba-
indatasetcreationforAI andML systems.By understand-
sivan 2022), crowdworkers are more commonly treated as
ingtheseengagementparadigms,wecanbettercomprehend
agents of potential bias (Miceli, Posada, and Yang 2022).
howthedynamicsbetweenrequestersandworkersnotonly
This is consistent with the overarchingpower dynamics in
affectthewell-beingoftheworkersbutalsonegativelyim-
pacttheintegrityandutilityofthedata.
Copyright©2024,AssociationfortheAdvancement ofArtificial
Intelligence(www.aaai.org).Allrightsreserved. Ourworkisthusalarge(n=52)interviewcorpusofre-
4202
guA
12
]CH.sc[
1v76611.8042:viXraquestersactiveondigitaltaskplatforms(e.g.,AmazonMe- search conferences. To be clear, this is not a policy paper.
chanicalTurk,Clickworker,Appen)whopostdatatasksas Insteadofproposingnewinfrastructureorsystems,weaim
part of the development, testing, and/or fine-tuning of AI toexplicatingthestatusquoofrequester–annotationengage-
andMLsystems.Asrequestersarguablyholdthemostinflu- ment.Indoingso,wecallupontheresearchcommunityto
encein digitaltask platformecosystems—wherebothplat- collaborativelyimproveasystemthatiscurrentlyfrustrating
forms and workers rely on them for profits and employ- forbothrequestersandannotators.
ment, we turn to them. Prior work, such as the Turkopti-
conproject(IraniandSilberman2013)andotherstudiesof RelatedWork
crowdworkers,e.g.,(Miceli,Posada,andYang2022;Han- To underscore the significant power disparity between re-
rahanetal.2018;BrawleyandPury2016;Xiaetal.2017), questersandannotators,webeginbyexaminingdigitaldata
have documented the experiences of data workers on such work as a form of invisible collar labor. Traditional cate-
platforms and demonstrated platform resistance to change. goriesofworkintheUnitedStatesincludeblue,white,and
This suggests a need to focus on the role of other stake- pink collar, each indicating the nature of the work. Blue
holders (namely, requesters). Subsequently, our study pur- collar refers to manual-industrial work (Wikipedia. “Blue-
suestworesearchquestions:(RQ1)howdorequestersper- collar work”), white collar to office and managerial tasks
ceive the identity of platformworkers, and (RQ2)what (Wikipedia. “White-collar work”), and pink collar denotes
are their views on the workers’ motivations and work feminized labor, often consisting of underpaid clerical or
methods, to enable us to envision strategies for improving care roles (Howe 1978). Invisible collar work represents a
workerexperienceandeffectivelyutilizingtheextensiveex- new category of labor, characterized by its transient, site-
pertiseoftheworkers(Micelietal.2022). agnostic nature that is typical of many low-status digital
Inshiftingthecritiqueofdataannotationfromthosewho roles.Thistermcapturestheoftenunseenandundervalued
performit(workers)tothosewhorequestit(requesters),we effortswithinthedigitalworkspace.Dataannotation,asin-
canseehowworkersareoftenrenderedinvisibleinthepro- visible collar work, is consistent with well-studied power
cess(Gray and Suri 2019; Wang, Prabhat, and Sambasivan dynamics found in platform-mediated data work, which
2022).Requesterstypicallyhighlightonlywhattheyseeas we detail in the second subsection below. On these plat-
‘good’dataresultingfromtheirtasks,andreferencethehu- forms, the pseudo-anonymous nature of relationships be-
manworkersbehindthedataonlytoblamethemas‘badac- tweenworkersandrequestersresultsinafocusonquantifi-
tors’ or sources of dataset corruption.Recognizing the hu- cationasconcretemeasuresofproductivityandquality.This
manity of these workers is crucial, however. We must em- contrastswiththecomprehensiveperformancereviewstyp-
brace it if we want to leverage data annotators’ extensive ical in traditional employee-employer relationships. In the
knowledgeanddatasetunderstandings. finalsubsection,weconnectthistrendofquantificationtoa
We reporta series ofproxies thatrequestersuse to iden- largerconversationthatviewsdataasaproductofadesign
tify‘good’dataandfilterout‘bad’actors.Wedefineproxies process(Feinberg2017),ratherthananoutcomeofisolated
as requirements and pre-task trials designed by requesters technicalprocedures.
toselectpotentialworkersandtoverifythequalityoftheir
workuponcompletionofthetask.Thescopeandnatureof InvisibleCollarWorkandLaborProtections
theseproxiesunderscorethatdatasetproductionisaprocess Digital task platforms such as Amazon Mechanical Turk
ofmeticulouscuration,atrendincreasinglyseenasadesign (MTurk) often operate under a principle of pseudo-
practiceamongrequesters.Whiletheproxiesrangefromar- anonymity,a stark contrast to traditionalplace-based work
bitrary to reasonable, they often overlook the workers’ ac- where supervisors and subordinatesshare a physical space
tual lived experiences and their expertise in handling data. (Xia et al. 2017; Sannon and Cosley 2019). The name
Asmeasurementinstruments,theseproxiesfailfairnessand ’MTurk’referstoan18th-centuryhoax:achess-playingma-
bias criteria for ‘construct reliability’ and ‘construct valid- chine seemingly automated but secretly operated by a hid-
ity’asadaptedbyJacobsandWallach(2021).Consequently, den human (Pontin 2007; Schwartz 2019). This historical
datasetscreatedthroughprocessesemployingtheseproxies reference illustrates Amazon’s approach to MTurk’s work-
arepotentiallyunfitforuse.Further,asquantitative–andof- ers(or,’Turkers’),whoperform’humanintelligencetasks’
ten punitive–measures,theydo notfosterthe respectfulre- (HITs). The design intentionally obscures the humans be-
lationship with data annotators needed to engage them as hindthetasks,creatinganillusionofseamlesstechnicaleffi-
skilleddatasetcontributors. ciencyandrenderingtheworkersvirtuallyinvisible(Vertesi
Our call is not just to reform proxy usage; we urge 2014; Tubaro, Casilli, and Coville 2020). This invisibility
pro-social treatment of workers to empower and promote underscoresa critical issue in digital labor, where workers
data workers as domain experts within a broader program. are not merely remote but are systematically hidden from
As professionals with the most intimate knowledge of the view while contributing essential services in AI develop-
dataset,dataworkersarebestpositionedtoreportonemerg- ment.
ing trends or concerns. Our findings demonstrate that the Virtual work lacks the workplace protectionscommonly
conditions (as set by requesters) needed to foster positive, foundinotheremploymentsectors(Jones2021).Crainetal.
collaborative engagement between requesters and annota- refertothisas’invisiblelabor,’describingitastheoftenhid-
tors are not present. In our discussion, we suggest sites denandoutsourcedonlineworkassociatedwithpoorcondi-
where positive engagement could be fostered, such as re- tionsandlowpay(Crain,Poster,andCherry2016).Unlikeblueorwhitecollarworkers,thosein’invisible’collarroles is often one-sided,favoringthe requesters. As clients, they
struggleeven to be recognizedby their de-factoemployers command the platform’s attention and shape the employ-
(Cherry 2016), a critical first step toward receiving work- ment and experience of the worker(Hanrahan et al. 2018;
place protection. Virtual work shares some characteristics BrawleyandPury2016).Conversely,workersarefrequently
withbluecollarjobs,e.g.thetimed,routinenatureoftasks. ignored or actively silenced (Miceli and Posada 2022). A
Other times, it resembles white collar work, being primar- notableexampleofthisimbalanceisAmazon’sdelayedre-
ily computer-based. However, unlike these traditional cat- sponsetotheissueofmassrejections,whererequestersre-
egories, invisible collar workers lack the protective frame- ject legitimate work to obtain additional labor at no extra
works:theyarenotcoveredbyU.S.OccupationalSafetyand cost, unfairly penalizing workers(Coworker.org). This dis-
Health Administration (OSHA) regulation that safeguards parityextendsbeyondplatformtreatmenttosocietalpercep-
bluecollarworkers,nordotheybenefitfromthecompliance tions.Whiledatascientistsanddataanalystreceiverespect
mechanism white collar workers have in human resources andresources,thedataworkerswhoproducethedatasetsare
(HR) departments. Invisible collar work falls through the often undervalued and overlooked (Sambasivan and Veer-
cracks,consistentwithalargererosionoftraditionalwork- araghavan2022;Sambasivanetal.2021).
place protections(Bernhardtetal. 2008).While discussing The treatment of workers on data work platforms is a
requesterpractices,wereturntoCrain’scompendiumofthe well-established concern in AI ethics, noted particularly
challengesfacinginvisiblecollarworkers,emphasizingthat for poor labor conditionsand the limited inclusion of their
making these roles more equitable necessitates both legal perspectives(Thebault-Spiekeretal.2023).Hawkins&Mid-
andpolicyinterventions. delstadt point out that AI researchers are subject to rela-
Invisible collar work also mirrorspink collar roles in its tively few ethical guidelines, compared to other fields em-
physical and psychosocial aspects. Pink collar work, often ploying crowdworking for purposes of data collection and
feminized and under-recognized, includes roles like child- curation(HawkinsandMittelstadt2023).Barrettetal.ques-
care,nursing,orsecretarialandadministrativelabor(Howe tionthevaluesandmetricsimposedbyrequestersandtheir
1978).Likepinkcollarworkers,invisiblecollarworkersare implicationsin computervision (Barrett, Chen, and Zhang
typically underpaid.However,distance between supervisor 2023).Whilethispreviousworkhasfocusedontheexperi-
andworkerismorepronounced,potentiallyspanningphysi- enceoftheworkersortheplatform,westudythepracticesof
callocations,timezones,languages,andcultures.Thissepa- datasetrequesterstounderstandhowtheirdecisionsimpact
rationalsoextendstoco-workers,complicatingbothcollec- data workers. This insightenables us to highlightthe criti-
tiveorganizingandindividualactions.Giventhatcomputing cal role requesters play in shaping working conditionsand
disciplinesareacommunitylargelyresponsibleforgenerat- datasetintegrity,therebyinformingthedevelopmentofpol-
ing invisible collar labor, it is our responsibility to ensure icychangesaimedatcreatingjustandequitableworkenvi-
proper,pro-socialtreatment, as advocated by Hawkins and ronmentsandensuringtheresponsibleconstructionanduse
Mittelstadt(2023)andRothschildetal.(2022). ofdatasets.
PowerDynamicsonDataWorkPlatforms DataisDesigned
Asnotedintheabovesubsection,crowdworkerslacktradi- Itisacommonlyheldideaincriticaldatastudiesthatdatais
tionalmethodsandrecourseofworkplaceprotection.They designed(Feinberg2017).Feinberg’suseofdesigningdata
are directly exposedto the conditionsset forthby the plat- makesconcretetwoideas.First,itformalizesBowker’sno-
form and requesters alike. Previous efforts such as the tionthat data is neverraw (Bowker2005;Gitelman 2013),
Turkopticon(IraniandSilberman2013,2016)andDynamo as any attempt to capture and curate a dataset necessarily
(Salehietal.2015)projectshavesoughttocompelplatforms imprints existing sociotechnical contexts and choices unto
totakeontheresponsibilitytoprotectandsupportworkers. thatdataset (Hasselbalch 2021;D’Ignazioand Klein 2020;
However, the platforms resisted these changes, ultimately Bates, Lin, andGoodale 2016).Second,design calls atten-
placing the burden back on the workers themselves (Irani tiontothehumanroleinshapingdataanddatasets,fromthe
and Silberman 2016). Thus, we focus, instead, on the role collectionmethodstotheanalysissoftwareused.Inthecon-
the requesters play in data work. While platforms should textofplatform-mediateddatawork,thepseudo-anonymous
still change for the better, we are more optimistic that re- relationshipresultsinanideathatthedataproducedis—or
questerscan and will do so, particularlywith the buy in of can be—‘authentic’ and ‘correct,’ rather than acknowledg-
policybodiesattheinter-institutionallevel. ingthedesignednatureofthetaskandtheresultingdata.
This work examines data work mediated by large plat- Whereandhowtosource‘good’dataremainsakeychal-
forms (e.g., MTurk), focusing on the relationship between lengeintheendlessquestforBigData(Mayer-Scho¨nberger
requestersandprofessionalswhocompleteit,aspartofthe and Cukier 2014; boyd and Crawford 2012), as a means
broader conversation on AI ethics (Birhane et al. 2022). of refining,re-tuning,and ultimately improvingthe perfor-
Dataworkfacilitatedbylargeplatformsischaracterizedby manceofMLandAImodels.However,previousgoldstan-
a triangular relationship of stakeholders (Fieseler, Bucher, dard datasets have been exposed as sites of error. Among
andHoffmann2019).Whilelaborrequestersandperformers others, significant benchmark datasets, such as ImageNet
can sometimes communicate directly, they are often doing (Vasudevanet al. 2022;Conner-Simons2021)and 80 Mil-
so throughtheplatform’smechanisms,sometimeswith the lion Tiny Images(Birhane and Prabhu 2021),remindus of
platformservingasanintermediary.Theflowofinformation the need to direct attention to how datasets come into be-ing. Having a better sense of where, how, when, and why approvedbyourInstitutionalReviewBoard(IRB).Thefirst
a dataset is assembled can help ensure that we understand authorand51 studentsin a graduate-levelclass onqualita-
1) the contents of the dataset itself and 2) help determine tivemethodsconductedtheseinterviews.Whilethestudents
reasonableandappropriategeneralizeduse,specificallyfor in the class produceda total of 87 interviews, we removed
the use of AI and ML systems. These datafication moves, interviewsthatdidnotmeetthestudycriteria,wherethere-
as described by Valdivia & Tazzioli (Valdivia and Tazzioli questerwasworkingonanAIorMLtask,oriftheinterview
2023)oftenreifyexistingpowerimbalancesanddiscrimina- wasoflowquality.
toryregimes(Mittelstadt,Wachter,andRussell2023;Leavy, Thestudentswerewellpreparedtoconducttheinterviews
Siapera,andO’Sullivan2021). after several months of instruction and completing assign-
Further,therequesters’relianceonquantificationtomake ments on gathering qualitative data and instruction on hu-
senseofworkersandtheirworkrecallsEspeland&Stevens’ mansubjectsresearch.Twostudentsparticipatedineachin-
sociology of quantification (Espeland and Stevens 2008). terview to help keep notes and ensure no questions or op-
They employ computational methods to accept or reject portunitiesforfurtherprobingwereoverlooked.Allstudents
workers,anecessitygiventhevastscaleofdatasetsrequired haddone at least one interviewas a class assignmentprior
for training AI and ML systems. This approachformalizes tothis,andmostteamsconsistedofatleastonestudentwith
thedatasetdesignprocess.Recognizingthisdesignprocess previoushuman-computerinteractionresearchexperiencein
is crucial not only for understanding the labor situation of industryoracademia.
digital pieceworkers but also for contextualizing datasets,
Thelastauthortaughtthecourseandofferedstudentsthe
whichaidsindeterminingtheirappropriateuse(Gebruetal.
opportunitytocontinueparticipationintheresearchproject
2021;Boyd2021).
foracademiccredit;twostudentselectedtodoso,withone
Recognizingtheroleofallindividualsinvolvedinthede-
droppingoutand onewho is the third authorof thispaper.
velopment of ML and AI systems—from data workers to
Using a real research protocol and tools for analysis is a
scientistsandengineers—isfundamentaltohuman-centered
uniqueopportunityforstudentstoexperiencepartofanau-
datascience.Thisapproachappreciatesthe humanisticand
thentic qualitative research practice, which typically takes
subjective dimensions of data science practices (Aragon
morethanayeartoconductand,thus,cannotbecompleted
etal.2022)andconsiderstheimplicationsforhowdatasub-
inaone-semestercourse.
jectsarerepresented(ChasalowandLevy2021;Qadrietal.
Interviewsservedasthebasisofthecourse’sfinalproject,
2023). Understanding the creators of AI systems and their
wherein students were tasked with identifying participants
developmentprocesses(Mulleretal.2019b)iscrucialtoad-
(with support from the first and last authors) and conduct-
dressing issues of fairness and bias (Gerchick et al. 2023).
ingasemi-structuredinterviewusingtheinterviewprotocol
Itisequallyimportanttocomprehendhowtheydesignand
createdbytheauthors.Theobjectiveoftheresearchandin-
construct the datasets they use (Li et al. 2023; Papakyri-
terview protocol was thoroughly explained to the students
akopoulosetal.2023).
before they began work and all students were part of the
Ourresearchintegratesthesethreestrandstomakeacom-
IRB’sapprovedhumansubjectprotocol.Theywereencour-
pellingargument:dataannotatorsarerelegatedtotheroleof
agedtospeaktoarequesterwhohadpostedataskfordata
invisiblecollarlaborers;thispositioningcreatespronounced
annotation,witheachstudentexpectedtofindandleadone
power imbalances that disproportionately favor requesters.
interview.
To truly engage data annotators as critical contributors to
datasets,itisessentialtorecognizetheinfluentialroleofre-
questersasarchitectsofboththedatasetsandtheconditions ParticipantSelection
underwhichlaboroccurs.Thisunderstandingiscrucialfor
fostering a collaborative environment that promotes effec- Thecriteriaforparticipantselectionwastoincludeindustry
tive informationexchange.This is also to our own benefit. andacademicprofessionalswhohaveusedacrowdsourcing
Whenwethinkoftheprocessof“flagging”(orcallingatten- platform(e.g.,AmazonMTurk)tosourcedataordatawork
tion)toaproblem,e.g.,indatawork,doingsorequiresinter- foruseinalarge,data-intensivesystem(mainlyforAIand
facingwithsomeoneinadecision-makingrole.Understand- ML). Potential participants were identified by the students
ing data annotators as invisible collar laborers contributes througha rangeofsources,e.g.,forumsforparticulardigi-
to complications to the process of flagging: first, because taltaskplatformsandstudents’professionalnetworks,along
it must be conducteddigitally and often indirectly through with those of the course teaching staff. Students recruited
thirdpartyplatforms,and,second,becausetherequesterand participantsthroughwordofmouth,emailandsocialmedia
annotatorrelationshipismarkedbya powerimbalance,fa- posts.Whilethesamplesizeof52waslargeforaninterview
voringtherequester. study,werecognizethatpatternsinrecruitment,geography,
andsharedprofessionalnetworkswerenotrepresentativeof
Methods
alltypesofrequestors.Wedonotintendforourparticipants’
To understand the requester–worker relationship, we inter- experiencestobeseenasrepresentingallrequesters,rather
viewed52participantsbetweenNovember2022andJanuary theyserveasabroadsampleofrequestersfromdifferentor-
2023.Thiswasadiversepoolofparticipantsfrombothaca- ganizations.Participantswere notcompensatedin anyway
demicandindustrysettings,whoactedasrequestersforboth fortheir participationin the study,due to GeorgiaInstitute
research projects and commercialproducts. Our study was ofTechnologyregulations.InterviewStructure another 15 participants turned to search engines and dis-
covered articles, blog posts, and GitHub repositories with
Semi-structured interviews (Edwards and Holland 2013),
suggestions. The same number (15) referenced platform-
following the interview protocol, were conducted with all
publisheddocumentation.Anothernineparticipantslearned
the participants. This was followed by a 10 minute think-
from YouTube tutorials, with eight receiving instruction
aloud session where participants walked the interviewer
from their academic advisors. A fair number (7) consulted
throughataskonaplatformtheyhadusedforcrowdsourced
no explicit instruction materials and instead learned from
datacollectionorannotation.Allinterviewswereconducted
self-explorationoftheplatform.Amuchsmallernumber(3)
via Zoom. On average interviews lasted about 45 minutes.
received explicit support from the company operating the
At the end of the interview, participantswere tasked to fill
platform.
out a demographic survey (age, gender, employment sta-
Estimating task timing: Task timing provides some pre-
tus,andeducationalbackground).Theyalsofilledaseparate
liminary insight into how participants made sense of the
formtoexpresstheirconsent(ifgranted)tobecontactedfor
workers, though not all participants described their model
follow-up interview. Interviews were then transcribed and
for estimating task timing. For example, how requesters
anonymized.
comparetheirownreadingspeedto thatofpotentialwork-
Interview questions focused on the participants’ experi-
ers. The majority of participants (12) did a rough, self-
ence of crowdsourcingand perception of the workers who
determined estimate of how long it would take workers,
completedtheir task(s). Participantswere also promptedto
while nine asked their fellow lab members to try the task
share more about certain nuances like payment calibration
andaveragetheirtimetaken.Afew(3)conductedaformal
andmetricstoqualifyworkersforatask,aspartofthethink-
pilottestwithworkerstogetanaveragecompletiontime.
aloudsession.
Calibrating payment: The most commonmodel (11) for
determiningpaymentwaspayingtheirlocalminimumwage
DataAnalysis
(often $12/hour) based on how long they thought the task
Thefirstauthordevelopedapreliminarycodebookafterre- took to complete, with an additional two paying “slightly
viewing all interviews and performing open coding (Flick more” than minimum wage. Some participants determined
2018), in consultation with the last author (course instruc- howmuch to payworkersbased on directinstruction from
tor), who had listened to each student group present their asuperior(advisororboss)(2)orasderivedfromtotalbud-
findings from their interviews as part of the final project. getdividedbynumberoftasks(3).Others(5)differentiated
The whole research team then read a selection of inter- based on what they saw as the types of questions or tasks,
viewsandupdatedthecodebook,beforethefirstandsecond to which they had a pre-assigned mental model for appro-
authors re-coded the entire interview corpus with the new priatepayment.Severalparticipants(11)assignedarandom
codebook. Throughout the re-coding process, incremental amountofpayment(e.g.,50cents)theyfeltwasappropriate,
changestothecodebookweremadeinconsultationwiththe orassignedarandomnumber.Asmallnumber(2)followed
larger research team through weekly meetings over about whattheplatformsuggested(amountvariedbyplatform).
three months. After the re-coding was performed, the re- Participants requested tasks in three main categories
searchteamdiscussedemergingthemesandrefinedtheopen (interviewees may have multiple kinds of task requests).
codes via axial coding, based on the grounded theory ap- Dataannotationandclassification (30 total): Including text
proach(StraussandCorbin1997). annotation(e.g., sentiment annotation,or rating violent in-
tentioninsocialmediaposts)andimagelabeling(e.g.,draw-
ParticipantBackgrounds ing boundingboxes, or describingimage contents).Gener-
allyrefiningorlabelinganexistingdatasetforuseinmodel
We highlight four aspects of participants’ backgrounds, as training.Datacollection(12total):Usedtoprocureadataset
knowntous,tohelpcontextualizetheirexperiences: formodeltraining,includingtaskslikerequestingYouTube
Workcontext:Weroughlydelineateparticipantsinto“re- viewinghistoryandcollectingspeechsamplesandtextgen-
search”(38)and“commercial”(14)requesters.Thisdistinc- erationforNLPprojects.Toolorsystemfeedback(19total):
tionisbasedonthepurposeofthetaskstherequestermakes; UsertestingorreviewsofAIorMLsystems.Sometimesofa
“commercial” tasks involve fueling or refining a commer- non-standalonealgorithm,suchasratingaccuracyofimage
cialproduct(e.g.,avoicetechnologystartup).Fundamental classification system that will be incorporatedinto a larger
knowledge or insight is grouped into “research” requests, product.Otherexamplesincluderatingordescribingexpe-
e.g., at a university lab, to test the naturalness of genera- riencewithachatbot,ExplainableAIsystem,orthenatural-
tivetextsystems.Fromthisbackgroundfactor,wecanmake nessofagenerativeNLPsystem.
senseofwhereinterventionsmightbemostuseful(e.g.,re- Participants used various digital task platforms and sev-
searchconferencevscorporateguidelines). eralparticipantsactive on multiple.Platformsused include
Learning how to ‘request’: We are concerned with how Amazon MTurk (46), CrowdFlower & FigureEight & Ap-
requesters come to understand a platform and design tasks pen1(8),Prolific(5),Microworkers(1),Upwork(1),Fiverr
forit.Onesiteofinterventionmightbeinthematerialsre-
questersusetolearnhowtocreateataskonaplatform.Sev- 1CrowdFlower was renamed FigureEight, which wasthen ac-
eralparticipantsusedmultipleresources:co-workersorcol- quiredbyAppen inthespanof afewyears; wereport theseina
laboratorswerea frequentsourceofinstruction(17),while groupsincesomeparticipantsreferredtothecompanybyitsname(1),DialCrowd(1),Labeler(1),Tokola(1),iMerit(1). communicatedregularlywith a few Amazon MTurk work-
erswhowereongovernmentdisabilitybenefitsandsupple-
Findings mentedthosebyworkingfromhomeonMTurk.Fromthese
interactions, P1 formed a general impression that many of
We present our findings in three sections: the first cov- theirworkershadsimilarcircumstances.However,theyrec-
ers how requesters perceive the workers they hire on dig- ognizedthattheirviewmightbeskewed,noting,’[I]mostly
ital task platforms. Second are the proxies requesters use imagined those people even though I know they’re not the
as worker selection criteria. Third, the curated nature of majority...they’rejusttheoneswhocommunicatemore.’”
datasetsemergingfromthedyadicrequester-workerinterac-
Most requesters, however, stuck to the idea that work-
tions,focusingonhowproxiessystematicallyexcludework-
ersontheseplatformsrepresentedthegeneralpublic.These
ers’perspectivesandaffectdataquality.
requesters then implemented various measures to secure
“gooddata.”Participantswerefrequentlyconcernedwiththe
Requesters’PerceptionsofWorkerIdentity qualityofthedata(ortaskperformance),asitcontributesto
the larger outcome dataset and subsequent model develop-
We saw significant variation in the vocabulary requesters
ment.Toget“good”data,requesterstookstrenuousefforts
used to describe platform workers. The workers employed
toprotectagainstthebiggestperceivedthreat:“badactors”.
by our participants were referred to as “users,” “partici-
pants,” as well as “workers.” Interestingly, individual re-
Good data vs bad actors Though participants had only
questersoftenemployedseveralofthesetermsinterchange-
vague notions of who was completing their tasks, they ex-
ablyduringtheirinterviewswhiledescribingthesamegroup
pressed significant concerns about these “bad actors,” or
ofworkers.Theterm“employee”wasnotablyusedjustonce
workerswhosubmittedpoororfraudulentresponsesjustto
(amongthe52interviews),byP42,todrawastrictdistinc-
receivepayments.Notably,whilethedataqualityproduced
tionbetweenplatformworkersandthefull-timeannotators
by a worker might be deemed “good,” individual workers
(referredtoas“employees”)atP42’scompany.
were never described in any of the interviews as “good”.
Who requesters think workers are Two prevalent This implies that requesters, when content with the data
paradigmsemergedforhiring or determiningworkereligi- quality,didnotquestiontheidentityof the worker.Forex-
bilityfortasksondigitaltaskplatforms.Onnon-anonymous ample,P5believedthatofferingaminimumof$20perhour
platformslikeUpwork,whereworkershaveaclearidentity wouldensure“goodannotations”.Thisreflectsa common
or personalbrandvisible to requesters,selection hingeson belief in freelancing that higher pay attracts better candi-
the worker’s portfolio and profile. In contrast, on pseudo- dates. However, P5’s focus was solely on the data quality,
anonymousplatforms, such as MTurk, requestersknow al- notthecharacteristicsofthepersonproducingit.
mostnothingabouttheworker.However,theycaneitherap- Incontrast,participantsdissatisfiedwiththedatatheyre-
ply platform sponsored filters, or create their own screen- ceived cited “bad workers” whom they saw as performing
ing tasks to target specific groups, like college students. thetaskspoorlyormaliciously.Inthesecases,requestersat-
Thissubsectionfocusesonthelattermodel,specificallythe tributed the data quality to the worker’s shortcomings. For
requester-led screening processes typical of most pseudo- instance, P9 spoke about implementing quality checks to
anonymousdigitaltaskplatformssuchasMTurk. avoidworkerswho“usuallycheatintheseplatforms.”P39
Workers on digital task platforms can theoretically be explicitlydescribedmethodsfor“filtering outbadactors,”
anyone with an internet connection and a suitable com- implyingthatworkerswereresponsibleforundesirabledata.
puter,subjectonlytoplatform-specificrequirementsregard- Similarly,P13,whousuallyacceptedmostallsubmissions,
ing country of residence, age, and language skills. While statedthattheironlyreasonforrejectionwaswhen“some-
theexactdemographicandexperientialcompositionofthese onedeliberatelytriestopoisonthedata.”P13continuedde-
workershasbeenestimatedinvariousstudies,wefoundthat scribingtheirmethodsformanuallyreviewingtasksubmis-
requestersholdonlyavagueunderstandingofthepotential sionstodeterminewhichworkers“aredoingbad.”
worker pool.This observationaligns with what Kapania et Many academic requesters, who posted components of
al. observed (Kapania, Taylor, and Wang 2023). Many re- academicstudiesastasks,perceivedworkersprimarilymo-
questersperceivedtheworkerbaseofdigitaltaskplatforms tivatedbyfinancialcompensationasathreattoresearchin-
as“thegeneralpublic”.Forinstance,P31explainedthatthey tegrity.Theseacademics(oftenimplicitly)expectedworkers
imposed no specific restrictions on which workers could tohaveasincereinterestincontributingtothedevelopment
complete on their tasks, aiming for “the generalworkers.” ofnewknowledge.EventhoughP44recognizedthat“Ama-
Similarly,P36admittedtohaving“noidea”whowascom- zonTurkers isa groupofpeoplethatdoesthis formoney,”
pletingtheirtasks,describingtheworkersas“random”.P6 they were frustrated by workers or research participants,
echoedthissentiment,deemingworkerbackgroundinsignif- who they felt were “just clicking, you know, like randomly
icanttotheirtaskandtheirrelationshipwiththeworkersas or very quickly, to get you [the requester] the [completed]
“strangersontheplatform.” surveytogetmoney.”Thisisonlyonetellingexampleofthe
A minority of requesters had specific (albeit often anec- conflictbetweentheacademicexpectationofearnestpartic-
dotal)insightsintotheirworkerpopulation.Forexample,P1 ipationandthepracticalmotivationtheseplatformworkers.
Thegooddatavsbadactorsdichotomyreflectsabroader
atthetimeofrequestingandothersbyitscurrentname. dynamicintherelationshipbetweenrequestersandworkers.When task submissions, as contributionsto a dataset, meet identify the worker’s “ethnic background” and whether it
expectations, requesters typically don’t question the work- alignedwiththeirtargetedregionsornot.
ers’ identities, expertise, or experience. Yet, if the data is
unsatisfactory,thefocusshiftstotheworkersasindividuals. Proxying aptitude Requesters employed proxies to as-
Suddenly, the requester holds the worker (as a bad actor) certain how well a given worker would complete—or did
responsibleforthesubparoutcomes.Thisshiftunderscores complete—a given task. The proxies are split into pre and
howworkers’humanidentityandexperienceareoverlooked post hoc, or those applied to test worker aptitude based on
whenthingsgowell,butscrutinizedwhenissuesarise. theirplatform-specifictrackrecordorthoseusedtotesthow
wellaworkerperformedontherequester’stask.
ProxiesforSelectingWorksandAssessingQuality Prehocproxies:
Approvalrating:Requesterscommonlyusedapprovalrat-
To discern workers who would produce high-quality data
ings, which reflect a worker’s aggregateratings from tasks
from potential bad actors, our interviewees developed a
completedforotherrequesters,asafilteringcriterion.How-
numberofproxies,ormicro-teststoapproximatewhethera
ever, what constituted an acceptable prior approval rating
workerholdsordeliversacertainquality(seeallproxiesina
varied between requesters: P47 considered a rating above
summarytableinAppendix1).Therearetwotypesofprox-
95%as“good”,while P10set theirminimumat98%,and
ies: 1) those determining a worker’s identity and suitabil-
forP11,thethresholdwasevenhigherat99%.
ityforaparticulartask,and2)thoseevaluatingtheirability
Number of tasks completed: Requesters often gauged
toproducehigh-qualitydata.Notably,manyrequestersused
worker’s reputation by their previous task history, i.e., the
multiple proxies in tandem, filtering out elements of what
number of tasks they completed. Similar to approval rat-
theybelievedtobethegeneral(impliedUS)population.For
ing, the threshold for desirable task history varied greatly.
example, P11, who wanted to recruit members of the gen-
ForP21thatnumberwas1,000priortasks,whereP48seta
eralpublicfortheirtask,expressedadesiretoavoidbiases
benchmarkat100,andP10atmerely50.
such as English fluency in their task outcomes. However,
Posthocproxies:
P11 used a numberof proxies, includingfiltering potential
Keyboard interaction: Requesters calculated how much
workersbasedontheirpriortaskacceptancerate.
a worker engaged with the keyboard—and the speed of
Proxyingworkeridentity Thefollowingisasynopsisof keyboard interactions—to ascertain how sincerely workers
the proxiesused to establish the workers’ identityon these completed their tasks. P16 wrote a tool to check if work-
quasi-anonymousdataworkplatforms. erscopiedandpastedintofree-textboxes,inordertoensure
Englishfluency:Interviewees(allU.S.-based)frequently that data collected “was not useless.” P12 assumed that a
emphasized the importance of potential workers’ English fastrepetitionofclicksmeanttheworkerwasabot,andso
fluency.P30,requestingdatacollectionforNLPtasks,prior- employed a keyloggerto make sure workers were genuine
itizedhiring“Englishnativespeakers”andemployedplat- withtheirreplies.P12’sconcernwasworkersjust“tryingto
formfilterstoexcludeworkersaccordingly.Incontrast,P19 gettheirtwocentsorwhatever.”P39performedan“entropy
described their strategy for ensuring English fluency: se- analysis”oftheamountoftimespentonthetask,premised
lecting workers from locations where English is a primary on the idea that if the worker spent far less time than the
language. Though they admitted to not finding a reliable teamdeterminedappropriateintrials,theworker’ssubmis-
methodonMTurk“toensurethatthatrequirementismet.” sionwouldberejected.
Age:Duetolegalrequirements,manyintervieweesstipu- Answerpattern:Requesterssawthevisualpatternofan-
latedthatworkersmustbeatleast18yearsold.Inacademic swers or choices (on sequential multiple choice and Likert
tasks, requesters typically asked workers to simply affirm scale questions) that workers selected as being a signal of
they were were 18 or older, a standard practice for U.S.- genuineeffort.Forexample,ifaworkeralwaysselectedOp-
based human subjects research. Conversely,in commercial tionBinamultiplechoicequestion,thatwasaredflagfor
or industrial tasks, interviewees more often relied on plat- P12.Similarly, P3 wouldmanuallyreview all task submis-
forms’officialfiltersandpurviewforageverification. sions, checking if participantschoose only the first option,
Location and time zone: Location-based proxies were oranotherpattern.
popular among requesters doing geographically-specific Attentioncheck: Requestersused assorted wording(e.g.,
projects.E.g.,P16wantedworkersfamiliarwithU.S.news “qualitychecks”)[P9],“dummyquestions”[P52])forwhat
andeventsandlimitedtheirselectiontoU.S.residentsonly. iscommonlyknownasanattentioncheck.Thesequestions
Similarly,P18,whowasinterestedinhavingdataannotated verify is not a bot and is focused on the task at hand. Re-
with specific English dialects (UK, Australia, Canada, Ire- questerscommonlycomplainedthatgradingattentioncheck
land,New Zealand),selected forworkersfromthesecoun- results was time-consuming, and perhaps “the most time-
triesusingplatformfilters.However,P18alsoknewworkers consuming activity” of posting a task (P9). P34 explained
coulduseVPNstoappearasiftheywereinthesecountries. that they began including attention checks because they
P37,whoalso soughtworkersfromthe UK,Australia, and “didn’ttrustthem[theworkers]anymore.”CAPTCHAsare
theU.S.,postedtasksduringregularworkinghoursinthese commonlyusedtofilteroutbots,whilereadingcomprehen-
regions,instead ofthe requester’sownlocaltimezone.P37 sionquestionsareoftenusedtomakesureworkersarecare-
alsoinformallyreviewedlong-formtextsubmissionsbeliev- fullyreadingdirections.Forexample,P1triedtowritewhat
ing “from the way they [the workers] speak,” they could they felt were fair attention checks, or ones that are “re-ally, really obvious where you can only get them wrong if priorapprovalrating.P18feltthatusingonlyMasterTurk-
you’reliterally notreading...it’sa multiple choicequestion ers was contrary to their goal of capturing a broad cross-
that says do not choose the other option,the other options sectionofworkers;theywantedawiderannotatorpoolthat
saysyouhavetochoosethisoption.”Perhapsthebestsum- whattheywouldfindinanacademicresearchenvironment.
mationofthenatureofattentionchecksistheexperienceof P18feltMasterTurkersrepresentedanelitewithinthelarger
P21, who used a series of attention check methods and, in MTurk worker population. Similarly, P5 was not sure the
turn, received feedback from workers in which they called MasterTurkerdesignationwasmeaningful—theywerealso
P21an“evilgenius”duetothedifficultyofproperlycom- concernedwith workers being unsure of how to obtain the
pletingthetask. designation,whichtheysawasunfair.
Gutreaction:Severalrequestersreliedonagutreaction,
or instinctual feeling of whether or not task submissions CuratedDatasets
were correctly done. P36 had a two-part strategy for veri-
Reflectingonthelargerpatternsofdatacurationasobserved
fyingthequalityofworkersubmissions,thesecondstepbe-
in our findings, we identify a composite insight into the
ing “more or less ad hoc...[it] was subjective...becausewe
behaviorsof requesters posting ML and AI tasks on semi-
wouldlookattheannotationsandbelike, ‘okay,yeah,this
anonymousdigitaltaskplatforms.Theserequestersattempt
person has been producing consistently good annotations,
toascertainthefollowing:
so he shouldcontinuewith it.”’ P50,meanwhile,described
going through submissions for a generative task manually • task-relevantinformationaboutaworker’sidentity,
and deciding “these ones are good, these ones are bad,” a • apromiseofqualitybasedonpastperformance,
processtheyultimatelyfound“annoying.”
• confirmation of the quality, authenticity, or sincerity of
Coherenceastrust:Comparinga worker’sanswerorse-
workersubmissions,varyingbytasktype.
lection for a subset of task activities was a common way
to determine how correct a submission was. Two methods Requesters typically view workers through one of three
wereusedtodothis.Inmajorityconsensus,requesterspick lenses. Firstly, some requesters are indifferent to workers’
themostcommonlysubmittedlabelorannotationandcheck identities,focusingsolelyontask completion,whichraises
all submissions against that correct answer, rejecting sub- concernsaboutthedatasets’abilitytorepresentdiversepop-
missionsthatdon’tcomply.P2setathresholdforagreement ulations accurately. Secondly, others assume their workers
andoncethatagreementwasmet,acceptedthatlabelasthe represent the general public or specific subgroups with-
correct one. P16 similarly employed agreement scores be- out concrete evidence, relying instead on outdated stud-
tweenworkerstodeterminethe acceptableannotation.The ies.Thirdly,somerequestersperceiveworkersasvoluntary
othermethodwasgoldstandardadherence.P5,forexample, study participantsrather than paid employees, highlighting
checks the agreement score of different annotators against a fundamentalmisunderstandingthat impactsmutualsatis-
their(requesterdeveloped)answersandifthescoreishigh faction.
enough,acceptsthesubmission.Similarly,P50usedasmall The use of proxies poses challenges for both requesters
poolofself-labeleddatatotestworkersubmissionsagainst. and workers. Proxies are intended to approximate worker
Anotherindicatorofpotentialworkersubmissionquality identity and labor quality, but developing effective proxies
used by requesters operating on MTurk was the Amazon- isproblematicduetothelackofstandardizedmethods.For
designed Master Turker distinction. Of the qualification, instance, some requesters resort to copying methods from
AmazonvaguelystatesthatMasterTurkershavecompleted academicpapers(e.g., P28)or seek advicefrom onlinefo-
thousands of tasks on the platform and have done so with rums (e.g., P37) to establish these measures. Despite these
a high level of performance.2 Notably, hiring only Mas- efforts,workersoftenfindthemselvesneedingtore-qualify
ter Turkers as a requester on MTurk carries an additional fortaskstheyarealreadyskilledat,wastingtimeandeffort.
platform-imposedsurcharge. Requesters in our corpus had Moreover,proxiesmayinaccuratelymeasuretherealvalue
mixedfeelingsaboutMasterTurkersandwhatthequalifica- thatrequestersseek.Forexample,thepreferencefor’native’
tionmeantaboutworkersholdingit.Somefeltitwasanas- Englishspeakersoverfluentspeakersraisesquestions:Why
suranceofreputablework;P51postedtasksonlyforMaster wouldanimmigrantinamajorityEnglish-speakingcountry
Turkers,feelingtheirworkqualitywasfarsuperiortothose not be fluent enoughfor the task? This overlooksthe rich-
of non-Masters. P20 felt the surcharge for Master Turkers nessofvariedEnglishvernaculars,whichcouldbevaluable
wasworthit,since“weranacostanalysisandrealizedthat inlarge-scaleMLorAIapplications.
itwasactuallycheapertopaysignificantlymoreforMaster Workersface additionalburdensfromproxies,havingto
Turkersbecausetheresultsthattheygaveusbackwerejust provetheir competenciesrepeatedly,whichmaynotreflect
somuchbetter.” their actual domain expertise. Proxies may also be a poor
Others, however, felt the designation to be relatively test of actual domain experience and knowledge. Consider
meaningless.P37,forexample,stoppedusingMasterTurk- the minimum time limits many requesters use to track the
ers after reading on a forum that the same level of worker sincerityofworkereffort.Ifaworkerspendsmuchoftheir
couldbeselected forusinga mixofotherfilters, including day(giventhatmanydigitaltaskplatformworkersdosoas
their main income source (Hitlin 2016)),developinga par-
2https://aws.amazon.com/blogs/aws/amazon-mechanical-turk- ticular skill e.g., drawing bounding boxes for image label-
master-workers/ ing, they are highly likely to complete it far quicker thana requester who has perhaps drawn only a few for prac- structvalidityentailsensuringthatmeasurementsareaccu-
tice. Workers must also spend time qualifying for tasks ratelygroundedintheintendedconstructandencompassall
throughpre-screeners—whilesomerequestersstillcompen- its relevant aspects (Jacobs and Wallach 2021). The prox-
sated workers for these pre-task activities, many more did ies requesters used do not meet validity in this regard, as
not. thereisnoconsensusorevidenceconfirmingthesemeasures
These proxies also force workers to engage in sub- accuratelyandcomprehensivelyrepresentworker’sidentity
standard work should their ratings fall due to simple mis- orthequalityoftaskcompletion.Theseproxiesseldomof-
understandings, or even malicious requester actions. The fer a fair assessment of labor. Continuing with the exam-
Turkopticon “End the harm of mass rejections” campaign, pleofEnglish-languageskill,somerequesterpriorizenative
for example, highlights how requesters can take advantage speakers over fluent speakers, failing to recognize that flu-
ofworkers,basicallybycollectingtheirlaborandfailingto ency and native-speaking are related yet distinct concepts.
paythemforit,forcingworkerstopickupultra-lowpaying Meanwhile,othersstriveforageneralconsensus,whichcan
tasksinordertorebuildtheirratings(Coworker.org). inadvertently suppress diverse perspectives. This approach
Finally,theimplementationofproxiescanleadtotheex- can lead to the silencing and even penalization of work-
clusionofvaluabledata,asrequesterscuratewhattheycon- ers who express minority opinions or unique lived experi-
sider undesirable submissions. This selective process con- ences,asnotedbyKapaniaetal.(Kapania,Taylor,andWang
tradicts the goal of capturing a broad spectrum of inputs 2023). Further, domain experience is paradoxically under-
from the general public, limiting the datasets’ authenticity valued, as seen when workers who complete tasks rapidly
andusefulness. are excluded from the final dataset. This raises significant
concerns about the fairness and efficacy of proxies in ac-
Discussion curatelyassessingandvaluingthecontributionsofplatform
workers.
In this paper, we’ve highlighted significant issues in how
Proxiesdemonstrateseveralproblemswithrequesterun-
digital task platform workers are evaluated. Workers face
derstanding of workers and the work workers perform. As
vague,scale-drivenproxiesthatnotonlydisadvantagethem
a tool, proxies are meant to check whether work will be
butalsocompromisedatasetintegrity.Thesedatasets,often
(or has been) completed correctly; however, the proxy ex-
skewedbytheunderrepresentationofminorityvoices,form
ists because the requester does not know how the work is
the basis of AI and ML systems, posing risks to both the
done.Proxiesalso lackbothconstructreliabilityandvalid-
general public and developers. Moreover,the lack of stan-
ity.Proxiesalsoimposeunfairexpectationsonworkers.Re-
dardization in these practices makes them unreliable, rein-
questers frequently use attention checks and review visual
forcing’theinvisiblecollar’arounddataannotators.Insub-
answerpatterns,expectingaleveloffocusfromworkersthat
sequentsections,weproposestrategiestosafeguardworker
isoftenunattainableorsimplyinaccurate.Thisraisesanim-
rightsandassesstheimplicationsofsourcingdatasets,em-
portantquestion:Whyisthereanexpectationfordigitaltask
phasizingtheuseofexistingchecks-and-balancesystemsfor
platform workers, many of whom rely on these platforms
efficientchangeimplementation.
astheirprimaryincomesource,tomaintainintenseconcen-
tration for their entire workday or on each micro-task? It’s
ThePitfallsofProxies
unlikelythattheserequestersthemselvesmaintainsuchhy-
Returning to the impact of quantification in platform- perfocus throughout their own multi-hour workdays. This
mediateddata work,as discussed byEspelandand Stevens disparity highlights a potential double standard: Why is it
(2008),proxiesemergeasaprimeexample.Atafundamen- deemed fair to expect digital task platform workers to ex-
tal level, proxies present a concerning practice: as valida- hibitaleveloffocusthatisnotexpectedinother(lowlycom-
tionimplements,theyareofquestionableconstructreliabil- pensated)professionalcontexts?Thiscontrastdemonstrates
ityandconstructvalidity. once again the power imbalance between data work anno-
Drawing on Jacobs & Wallach’s exploration of fairness tators and requesters (Miceli, Schuessler, and Yang 2020;
andbias,constructreliabilityisakintoreplicability(Jacobs Wang,Prabhat,andSambasivan2022).
and Wallach 2021).Appliedto proxies,thereis little repli-
LaborProtectionsforInvisibleCollarWorkers
cability among requesters, as each implements a concep-
tualproxydifferently.Withinourcorpus,therewerenouni- Requesters on digital platforms often view workers as pri-
versalpracticesforanygivenproxy.Instead,requestersof- marily driven by financial gain. This misperception lim-
tenrelyonpracticalwisdomfromprofessionaloracademic its workers’ incentives to exceed minimum quality stan-
sourcestocreatetheirowninterpretations,leadingtosignif- dards,contrastingsharplywith the paradigmin citizen sci-
icantvarianceinhowtheyestablishwhetherornotaworker encewherevolunteersaredrivenbypersonalfulfillmentor
hasa quality,suchasperceivedEnglishfluency.Forexam- acommitmenttoabroadermission,oftenresultinginhigher
ple, some requestersrely on a platformsown characteriza- quality data contributions (Maund et al. 2020). However,
tion(orlabel)ofaworker’slanguageproficiency,whileoth- digital platform workers typically face limited job security
ers allow onlyworkersin majority English-speakingcoun- and scant prospects for advancement, leading to a dimin-
triestocompletetheirtasks. ishedsenseofownershipandinvestmentintheirtasks.
Proxiesnotonlyfailconstructreliabilitybutalsolackro- Theextensiveuseofproxieshighlightstheurgentneedto
bustconstructvalidity.AccordingtoJacobs&Wallach,con- reevaluate the requester-workerrelationship, especially ac-knowledgingtheabsenceoflaborandworkplaceprotections • Transparency Requirements: Implement policies that
for data workers on these platforms. As partof the ’invisi- require requesters to disclose the criteria and rationale
ble collar’class, these workers,despite oftenearningregu- behindtheirproxymeasures.Thiswouldenableworkers
lar incomes, are notrecognizedasemployeesby platforms contestunfairorinaccurateproxies.
orrequesters.Thislackofrecognitionexacerbatestheinvis-
• Worker Involvement in Proxy Design: Encourage or
ibilityoftheirlabor,identity,andcritically,theirrightsand
requiretheinvolvementofworkerrepresentativesinthe
protections.
design and review of proxies.This involvementcan en-
Recognizing digital task platform workers as legitimate
sure that the proxiesreflectthe actualexpertiseandcir-
employeesiscrucialforextendinglaborprotectionstothem.
cumstancesoftheworkers.
While protectionsforwhite collar workersinvolvecompli-
• Regular Audits and Reviews: Introduceregular audits
ancemechanisms(e.g.,HR) andbluecollarprotectionsin-
of proxyuse by an independentbodyto ensure compli-
cludesafetyregulationsandtheabilitytounionize,theseare
ancewithfairnessandreliabilitystandards.
not perfect. For instance, union activity can face corporate
resistance, and HR departments may prioritize legal com- • Protection Against Proxy Abuse: Establish protective
plianceoveremployeewell-being(Sainato2023).However, measures for workers against potential abuses of proxy
anyprotectionisbetterthanthecurrentsituationforinvisi- measures, such as unwarranted rejections or unfair task
blecollarworkers,wholacksuchsafeguards. allocation. This could include mechanisms for workers
Furthermore,theWhiteHouseBlueprintforanAIBillof toappealdecisionsmadebasedonproxyassessments.
Rights(OSTP 2022)failsto addressthe role ofdata work-
• CompensationforQualificationTasks:Mandatecom-
ers,despiteitsfocusonimprovingAIandMLsystemover-
pensation for workers’ time spent on qualification tests
sight.Thisoversightisparticularlyconcerninggiventhesig-
or pre-task screenings. This policy would acknowledge
nificantrisksassociatedwithsuchwork,highlightedbythe
thetimeandeffortworkersinvestinaccessingwork.
severe impacts reported by data workers in Kenya (Rowe
• ReductionofOver-relianceonProxies:Encouragere-
2023) and ongoing legal challenges involving major tech
questerstosupplementproxymeasureswithotherforms
companies(Njanja 2022;Perrigo 2022).As consumerpro-
of evaluation where possible, such as direct feedback,
tectionsinAIincrease,suchasthoseseenintheEUAIAct,
trialperiods,orcontinuousperformanceevaluations.
itisimperativetoextendsimilarsafeguardstodataworkers,
recognizingtheircriticalroleintheAIecosystem.
Limitations
RethinkingDatasetSourcingPoliciesatScale
Our work is centered on the perspectives& experiencesof
Followingthe discussion on the needfor better protections requesters working in the U.S. This study population was
fordataworkers,it’simportanttorecognizethatrequesters intentional,asitisreflectiveofAmericanacademicandin-
from different sectors face varied pressures that influence dustrytrainingonhowto serveasa requesteronplatforms
how they engage with these workers. Academics are typi- like MTurk, but it is not necessarily indicative of practices
callygovernedbythestandardsofpublicationvenues,Insti- globally.Consistentwithtraditionallimitationsofinterview-
tutionalreviewboard(IRB)requirements,andfundingbody basedwork,wereportoninterviewees’ownexperiencesand
regulations, such as those from the U.S. National Science perspectivesintheirownwords,whichmaynotbeaperfect
Foundation,which focus on research integrity and compli- representationofhowtheyactuallybehaveinsitu.
ance.Incontrast,industryresearchersareinfluencedbycor-
poratepolicies,whichprioritizebusinessobjectivesandop- Conclusion
erationalefficiencies.Thesesector-baseddifferencesunder-
Traditionally, data scientists and system developers priori-
score the necessity for comprehensive policy interventions
tize the technicalaspects of datasetconstruction.However,
thatspanbothacademicandindustrydomainstoensurecon-
ourinterviewswith52digitaltaskplatformrequestershigh-
sistent and fair treatmentof data workersacross the board.
light the importance of understanding requesters’ perspec-
To bridge these gaps and create a unified standard, we ad-
tivesonworkeridentity,skill,andqualification.Despitecon-
vocatefortheimplementationofindustry-widepoliciesthat
stant proxy-based evaluations of their qualifications, pro-
cover all institutions involved in data work. The following
ductivity, and work quality, we found that workers remain
measuresareproposedwaystosafeguarddataworkersand
largelyinvisibleunlesstherequesterisdissatisfiedwiththeir
enhance the fairness and integrity of digital platforms but
task performance. These proxies compromise the integrity
shouldbesubjecttofurtherstudy:
of worker evaluations and data quality, as they lack repro-
• Standardization of Proxies: Develop industry-wide ducibility, objectivity, and standardization. Such practices
standards for the use of proxies in platform-mediated obscure workers’ experiences and perspectives, contribut-
data work. These standards should focus on improving ingtobroaderpatternsofworkererosionindatasetcuration.
construct reliability and validity, ensuring that proxies Understandingtheseissuesiscrucialtoaddressingtheover-
are replicable across differentrequestersand accurately sight of worker status. Consequently, we advocate for the
measurewhattheyclaimto.Guidelinesshouldbeestab- revision of dataset sourcing policies in both academic and
lishedfordeveloping,testing,andrevisingproxiestoen- industrial settings, focusing on improving the treatment of
suretheyarefairandeffective. dataworkersandadvancingAIethicsinitiatives.Research Ethics and SocialImpact inthismaterialarethoseoftheauthorsanddonotnecessar-
EthicalConsiderationsStatement ily reflect the views of the National Science Foundationor
othersupporters.
Asthispaperdetailsworkperformedwithqualitativemeth-
ods (namely interviews), our research ethics concerns are
References
primarilyrelatedtoprotectingintervieweeprivacy.Ouren-
tire protocol and data management plan was approved by Aragon,C.;Guha,S.;Kogan,M.;Muller,M.;andNeff,G.
[institution name]’s IRB, including a waiver of informed 2022.Human-centereddatascience:anintroduction. Cam-
consentthatexplainedthestudy’spurposeandhowthedata bridge, Massachusetts London, England: The MIT Press.
collected would be used and stored. Participants’ names ISBN978-0-262-54321-7.
were not collected by the research team, unless they con- Barrett,T.;Chen,Q.;andZhang,A.2023.SkinDeep:Inves-
sented to being potentially contacted for a follow up inter- tigatingSubjectivityinSkinToneAnnotationsforComputer
view,inwhichcasetheirnameandcontactinformationwas Vision Benchmark Datasets. In Proceedings of the 2023
stored in encrypted storage, and has already been deleted ACM Conference on Fairness, Accountability, and Trans-
(following completion of data analysis). Other precautions parency,FAccT’23,1757–1771.NewYork,NY,USA:As-
included following standard procedure for protecting the sociationforComputingMachinery.ISBN9798400701924.
identity of participants, including storing all direct tran-
Bates,J.;Lin,Y.-W.;andGoodale,P.2016. Datajourneys:
scriptsinencryptedstoragetowhichonlytheresearchteam
Capturingthesocio-materialconstitutionofdataobjectsand
had access, with all researchers having completed [institu-
flows. BigData&Society,3(2):2053951716654502. Pub-
tion name]-sponsoredIRB approvedhuman subjects train-
lisher:SAGEPublicationsLtd.
ing.Further,intheinterviewsthemselves,intervieweeswere
Bernhardt,A.;Boushey,H.;Dresser,L.;andTilly,C.2008.
asked not to share their organizationalaffiliations (besides
AnOverviewoftheGloves-OffEconomy:WorkplaceStan-
describingthemasacademicorindustrial),toavoidcreating
dardsattheBottomofAmerica’sLaborMarket. Centerfor
repercussionsforanyparticipantwhospokecontrarytotheir
SocialPolicyPublications.
organization’sofficialpoliciesorwithoutorganizationalap-
proval. Birhane,A.;andPrabhu,V.U.2021. Largeimagedatasets:
A pyrrhic win for computer vision? In 2021 IEEE Winter
ResearcherPositionality Statement Conference on Applications of Computer Vision (WACV),
1536–1546.
Additionally, our positionality is that of academic and in-
dustryresearcherswhohaveworkedasbothdataworkplat- Birhane,A.;Prabhu,V.U.;andKahembwe,E.2021. Mul-
formrequestersandworkers.Ourdisciplinarybackgrounds timodal datasets: misogyny, pornography, and malignant
arediverse,rangingfromlearningsciencesandcriticaldata stereotypes. arXiv:2110.01963.
studies to health informatics and wellness and responsible Birhane, A.; Ruane, E.; Laurent, T.; S. Brown, M.; Flow-
AI.Severalauthorsonthispaperhaveworkedonplatforms ers,J.;Ventresque,A.;andL.Dancy,C.2022. TheForgot-
fordataannotationascrowdworkers,includingthefirstau- tenMarginsofAIEthics. InProceedingsofthe2022ACM
thorwhohascompletedmorethan200tasksonseveralma- ConferenceonFairness,Accountability,andTransparency,
jorplatforms,includingAMT. FAccT’22,948–958.NewYork,NY,USA:Associationfor
ComputingMachinery. ISBN978-1-4503-9352-2.
AdverseImpact
Bowker,G.C.2005. Memorypracticesinthesciences. In-
Themainconcern,withregardstoadverseimpacts,thatwe
sidetechnology.Cambridge,Mass:MITPress. ISBN 978-
canimaginestemmingfromthispaperisprofessionalreper-
0-262-02589-8.OCLC:ocm60776866.
cussionsaimedatparticipantswhowereinterviewedaspart
boyd,d.;andCrawford,K.2012. CRITICALQUESTIONS
ofthiswork.Wehavemadestrenuousefforttoprotectiden-
FORBIGDATA:Provocationsforacultural,technological,
titiesofallparticipantstoavoidthis.
andscholarlyphenomenon.Information,Communication&
Anotherconcernisthatthisworkcouldbemisconstrued
Society,15(5):662–679.
tosuggestthatdataworkshouldnothappenondigitalplat-
forms. This is the not the case—our argument is that re- Boyd, K. L. 2021. Datasheets for Datasets help ML En-
questers need to recognize their responsibility in acting as gineers Notice and Understand Ethical Issues in Training
employers to platform data workers and, in many cases, Data. ProceedingsoftheACMonHuman-ComputerInter-
reevaluatethewaytheyemploythoseworkers. action,5(CSCW2):1–27.
Brawley,A.M.;andPury,C.L.2016. Workexperienceson
Acknowledgments MTurk:Jobsatisfaction,turnover,andinformationsharing.
We thank our anonymous reviewers for their feedback on ComputersinHumanBehavior,54:531–546.
this work. Lauren Klein gave helpfulfeedbackon an early Buolamwini,J.;andGebru,T.2018. GenderShades:Inter-
iterationofthiswork.ThisworkissupportedbyNSFgrant sectionalAccuracyDisparitiesinCommercialGenderClas-
#1951818DataWorks:BuildingSmartCommunityCapacity sification. InFriedler,S.A.;andWilson,C.,eds.,Proceed-
anda Googlecollaborationgift,“Examiningthe dataprac- ingsof the 1st Conferenceon Fairness, Accountabilityand
tices of human-in-the-loop ML development”. Any opin- Transparency, volume 81, 77–91. Proceedings of Machine
ions, findings, conclusions, or recommendationsexpressed LearningResearch:PMLR.Chasalow, K.; and Levy, K. 2021. Representativeness in Melbourne:SAGE,2ndeditionedition. ISBN978-1-4739-
Statistics, Politics, and Machine Learning. In Proceedings 1200-7.
of the 2021 ACM Conference on Fairness, Accountability, Gebru,T.;Morgenstern,J.; Vecchione,B.; Vaughan,J. W.;
andTransparency,FAccT’21,77–89.NewYork,NY,USA: Wallach,H.;Iii,H.D.;andCrawford,K.2021. Datasheets
AssociationforComputingMachinery. ISBN 978-1-4503- fordatasets. CommunicationsoftheACM,64(12):86–92.
8309-7.
Gerchick,M.;Jegede,T.;Shah,T.;Gutierrez,A.;Beiers,S.;
Cherry,M.A.2016.Virtualworkandinvisiblelabor.Invisi- Shemtov, N.; Xu, K.; Samant, A.; and Horowitz, A. 2023.
blelabour:Hiddenworkinthecontemporaryworld,28–46.
The Devil is in the Details: Interrogating Values Embed-
Conner-Simons,A. 2021. Major ML datasets have tens of ded in the Allegheny Family Screening Tool. In Proceed-
thousandsoferrors—MITCSAIL. MITCSAILNews. Ci- ingsofthe2023ACMConferenceonFairness,Accountabil-
tationKey:MajorMLDatasetsa. ity, and Transparency, FAccT ’23, 1292–1310.New York,
Couldry,N.;andMejias,U.A.2019.DataColonialism:Re- NY, USA: Association for Computing Machinery. ISBN
thinkingBig Data’s Relation to the ContemporarySubject. 9798400701924.
Television&NewMedia,20(4):336–349. Gitelman,L.,ed.2013.”Rawdata”isanoxymoron.Infras-
Coworker.org.2023. ENDTHEHARMOFMASSREJEC- tructuresseries. Cambridge,Massachusetts; London,Eng-
TIONS. land:TheMITPress. ISBN978-0-262-51828-4.
Crain, M. G.; Poster, W. R.; and Cherry,M. A., eds. 2016. Gray,M.L.;andSuri,S.2019.Ghostwork:howtostopSil-
Invisible Labor: Hidden Work in the Contemporary World. iconValleyfrombuildinganewglobalunderclass. Boston:
UniversityofCaliforniaPress,1edition. ISBN978-0-520- HoughtonMifflinHarcourt. ISBN978-1-328-56628-7.
28640-5. Hanrahan,B. V.; Martin, D.; Willamowski, J.; and Carroll,
Denton, E.; D´ıaz, M.; Kivlichan, I.; Prabhakaran, V.; and J.M.2018.InvestigatingtheAmazonMechanicalTurkMar-
Rosen, R. 2021a. Whose ground truth? accounting for in- ketThroughToolDesign.ComputerSupportedCooperative
dividualandcollectiveidentitiesunderlyingdatasetannota- Work(CSCW),27(3-6):1255–1274.
tion. arXivpreprintarXiv:2112.04554. Hasselbalch,G.2021. DataEthicsofPower:AHumanAp-
Denton, E.; Hanna, A.; Amironesei, R.; Smart, A.; and proachintheBigDataandAIEra. EdwardElgarPublish-
Nicole, H. 2021b. On the genealogy of machine learning ing. ISBN978-1-80220-311-0.
datasets:A criticalhistoryof ImageNet. Big Data& Soci- Hawkins, W.; and Mittelstadt, B. 2023. The ethical ambi-
ety,8(2):205395172110359. guity of AI data enrichment: Measuring gaps in research
D´ıaz, M.; Kivlichan, I.; Rosen, R.; Baker, D.; Amironesei, ethicsnormsandpractices.InProceedingsofthe2023ACM
R.; Prabhakaran, V.; and Denton, E. 2022. CrowdWork- ConferenceonFairness,Accountability,andTransparency,
Sheets: Accountingfor Individualand Collective Identities FAccT’23,261–270.NewYork,NY,USA:Associationfor
Underlying Crowdsourced Dataset Annotation. In 2022 ComputingMachinery. ISBN9798400701924.
ACM Conference on Fairness, Accountability, and Trans- Hitlin,P.2016. ResearchintheCrowdsourcingAge,aCase
parency,FAccT’22,2342–2351.NewYork,NY,USA:As- Study. Technicalreport,PewResearchCenter.
sociationforComputingMachinery.ISBN9781450393522.
Howe,L.K.1978.PinkCollarWorkers:InsidetheWorldof
D’Ignazio,C.;andKlein,L.F.2020.Datafeminism.Strong Women’sWork. Avon. ISBN978-0-380-01924-3. Google-
ideas series. Cambridge, Massachusetts: The MIT Press. Books-ID:10K7AAAAIAAJ.
ISBN978-0-262-04400-4.
Irani, L. C.; and Silberman, M. S. 2013. Turkopticon: in-
Edwards,R.; and Holland,J. 2013. Whatis qualitativein- terruptingworkerinvisibilityinamazonmechanicalturk. In
terviewing? What is? Research methods series. London : Proceedingsof the SIGCHI Conferenceon HumanFactors
NewDelhi:Bloomsbury. ISBN978-1-78093-852-3978-1- inComputingSystems,611–620.ParisFrance:ACM. ISBN
84966-809-5. OCLC:ocn855705441. 978-1-4503-1899-0.
Espeland,W.N.;andStevens,M.L.2008. ASociologyof Irani, L. C.; and Silberman, M. S. 2016. Stories We Tell
Quantification. European Journal of Sociology / Archives About Labor: Turkopticonand the Trouble with ”Design”.
Europe´ennesdeSociologie,49(3):401–436. InProceedingsofthe2016CHIConferenceonHumanFac-
Feinberg,M.2017. ADesignPerspectiveonData. InPro- torsinComputingSystems,4573–4586.SanJoseCalifornia
ceedings of the 2017 CHI Conference on Human Factors USA:ACM. ISBN978-1-4503-3362-7.
inComputingSystems,2952–2963.DenverColoradoUSA: Jacobs, A. Z.; and Wallach, H. 2021. Measurement and
ACM. ISBN978-1-4503-4655-9. Fairness. In Proceedings of the 2021 ACM Conference
Fieseler, C.; Bucher, E.; and Hoffmann, C. P. 2019. Un- onFairness,Accountability,andTransparency,FAccT’21,
fairnessbyDesign?ThePerceivedFairnessofDigitalLabor 375–385.NewYork,NY,USA:AssociationforComputing
on Crowdworking Platforms. Journal of Business Ethics, Machinery. ISBN978-1-4503-8309-7.
156(4):987–1005. Jo,E.S.;andGebru,T.2020.LessonsfromArchives:Strate-
Flick,U.2018. Doinggroundedtheory. Number8.volume gies for Collecting Sociocultural Data in Machine Learn-
inTheSAGEqualitativeresearchkit/editedbyUweFlick. ing. In Proceedings of the 2020 Conference on Fairness,
LosAngelesLondonNewDehliSingaporeWashingtonDC Accountability,andTransparency,FAT*’20,306–316.NewYork, NY, USA: Association for Computing Machinery. Muller, M.; Lange, I.; Wang, D.; Piorkowski, D.; Tsay, J.;
ISBN9781450369367. Liao,Q.V.;Dugan,C.;andErickson,T.2019b. HowData
Jones,P.2021. Workwithouttheworker:labourintheage ScienceWorkersWork with Data: Discovery,Capture,Cu-
ofplatformcapitalism. Brooklyn:VersoBooks. ISBN978- ration, Design, Creation. In Proceedings of the 2019 CHI
ConferenceonHumanFactorsinComputingSystems,1–15.
1-83976-043-3.
GlasgowScotlandUk:ACM. ISBN978-1-4503-5970-2.
Kapania,S.;Taylor,A. S.;andWang,D. 2023. A huntfor
the Snark: Annotator Diversity in Data Practices. In Pro- Njanja,A.2022.MetasuedbyEthiopiansandKenyanrights
ceedingsofthe2023CHIConferenceonHumanFactorsin group for fueling Tigray War. TechCrunch. Citation Key:
ComputingSystems,1–15.HamburgGermany:ACM.ISBN njanjaMetaSuedEthiopians2022.
978-1-4503-9421-5. OSTP. 2022. Blueprint for an AI Bill of Rights. Citation
Leavy,S.;Siapera,E.;andO’Sullivan,B.2021.EthicalData Key:BlueprintAIBill.
Curation for AI: An Approach based on Feminist Episte- Papakyriakopoulos, O.; Choi, A. S. G.; Thong, W.; Zhao,
mology and Critical Theories of Race. In Proceedings of D.; Andrews, J.; Bourke, R.; Xiang, A.; and Koenecke, A.
the 2021 AAAI/ACM Conference on AI, Ethics, and Soci- 2023. AugmentedDatasheetsforSpeechDatasetsandEth-
ety,695–703.VirtualEventUSA:ACM. ISBN978-1-4503- ical Decision-Making. In Proceedings of the 2023 ACM
8473-5. ConferenceonFairness,Accountability,andTransparency,
Li,H.;Vincent,N.;Chancellor,S.;andHecht,B.2023. The FAccT’23,881–904.NewYork,NY,USA:Associationfor
Dimensions of Data Labor: A Road Map for Researchers, ComputingMachinery. ISBN9798400701924.
Activists, and Policymakers to Empower Data Producers. Paullada, A.; Raji, I. D.; Bender, E. M.; Denton, E.; and
In Proceedings of the 2023 ACM Conference on Fairness, Hanna, A. 2021. Data and its (dis)contents: A survey of
Accountability, and Transparency, FAccT ’23, 1151–1161. dataset developmentand use in machine learning research.
New York, NY, USA: Association for ComputingMachin- Patterns,2(11):100336.
ery. ISBN9798400701924.
Perrigo,B.2022. MetaAccusedOfHumanTraffickingand
Maund, P. R.; Irvine, K. N.; Lawson, B.; Steadman, J.; Union-BustinginKenya. TIME.
Risely, K.; Cunningham, A. A.; and Davies, Z. G. 2020.
Pontin,J.2007. ArtificialIntelligence,WithHelpFromthe
Whatmotivatesthemasses:Understandingwhypeoplecon-
Humans. TheNewYorkTimes.
tribute to conservation citizen science projects. Biological
Qadri,R.;Shelby,R.;Bennett,C.L.;andDenton,E.2023.
Conservation,246:108587.
AI’s Regimes of Representation: A Community-centered
Mayer-Scho¨nberger,V.; and Cukier, K. 2014. Big data: a
StudyofText-to-ImageModelsinSouthAsia. InProceed-
revolutionthatwilltransformhowwelive,work,andthink.
ings of the 2023 ACM Conference on Fairness, Account-
Boston: Mariner Books, Houghton Mifflin Harcourt, first
ability,andTransparency,FAccT’23,506–517.NewYork,
marinerbookseditionedition. ISBN978-0-544-22775-0.
NY, USA: Association for Computing Machinery. ISBN
Miceli,M.;andPosada,J.2022. TheData-ProductionDis- 9798400701924.
positif. Proceedings of the ACM on Human-ComputerIn-
Rolf, E.; Worledge, T. T.; Recht, B.; and Jordan, M. 2021.
teraction,6(CSCW2):460:1–460:37.
Representation matters: Assessing the importance of sub-
Miceli, M.; Posada, J.; and Yang, T. 2022. Studying Up groupallocationsintrainingdata. InInternationalConfer-
Machine Learning Data: Why Talk About Bias When We enceonMachineLearning,9040–9051.PMLR.
MeanPower?ProceedingsoftheACMonHuman-Computer
Rostamzadeh, N.; Mincu, D.; Roy, S.; Smart, A.; Wilcox,
Interaction,6(GROUP):1–14.
L.; Pushkarna, M.; Schrouff, J.; Amironesei, R.; Moorosi,
Miceli, M.; Schuessler, M.; and Yang, T. 2020. Between N.; and Heller, K. 2022. Healthsheet: Development of a
subjectivity and imposition:Power dynamicsin data anno- Transparency Artifact for Health Datasets. In Proceed-
tation for computer vision. Proceedings of the ACM on ingsofthe2022ACMConferenceonFairness,Accountabil-
Human-ComputerInteraction,4(CSCW2):1–25. ity, and Transparency, FAccT ’22, 1943–1961.New York,
Miceli,M.;Yang,T.;AlvaradoGarcia,A.;Posada,J.;Wang, NY, USA: Association for Computing Machinery. ISBN
S. M.; Pohl, M.; and Hanna, A. 2022. Documenting Data 9781450393522.
Production Processes: A Participatory Approach for Data Rothschild,A.;Booker,J.;Davoll,C.;Hill,J.;Ivey,V.;DiS-
Work. Proceedings of the ACM on Human-Computer In- alvo,C.;RydalShapiro,B.;andDiSalvo,B.2022. Towards
teraction,6(CSCW2):1–34. fair and pro-socialemploymentof digital pieceworkersfor
Mittelstadt,B.;Wachter,S.;andRussell,C.2023. TheUn- sourcing machine learning training data. In Extended Ab-
fairnessofFairMachineLearning:Levellingdownandstrict stracts of the 2022 CHI Conference on Human Factors in
egalitarianismbydefault. ComputingSystems,CHIEA’22,1–9.NewYork,NY,USA:
AssociationforComputingMachinery. ISBN 978-1-4503-
Muller,M.;George,T.;John,B.E.;Passi,S.;Feinberg,M.;
9156-6.
Jackson, S. J.; and Kery, M. B. 2019a. Human-Centered
Study of Data Science Work Practices. In Extended Ab- Rowe, N. 2023. ‘It’s destroyed me completely’: Kenyan
stractsofthe2019CHIConference,8.CitationKey:muller- moderators decry toll of training of AI models. The
HumanCenteredStudyData2019. Guardian.Sainato,M.2023.‘Warofattrition’:whyunionvictoriesfor Valdivia,A.;andTazzioli,M.2023. DataficationGenealo-
USworkersatAmazonhavestalled. TheGuardian. gies beyond Algorithmic Fairness: Making Up Racialised
Subjects. In Proceedings of the 2023 ACM Conference
Salehi, N.; Irani, L. C.; Bernstein, M. S.; Alkhatib, A.;
onFairness,Accountability,andTransparency,FAccT’23,
Ogbe,E.;Milland,K.;andClickhappier.2015. WeAreDy-
840–850.NewYork,NY,USA:AssociationforComputing
namo: Overcoming Stalling and Friction in Collective Ac-
Machinery. ISBN9798400701924.
tionforCrowdWorkers. InProceedingsofthe33rdAnnual
ACMConferenceonHumanFactorsinComputingSystems, Vasudevan,V.;Caine,B.;GontijoLopes,R.;Fridovich-Keil,
1621–1630.SeoulRepublicofKorea:ACM. ISBN 978-1- S.;andRoelofs,R.2022.Whendoesdoughbecomeabagel?
4503-3145-6. AnalyzingtheremainingmistakesonImageNet. InKoyejo,
S.; Mohamed,S.; Agarwal, A.; Belgrave,D.; Cho, K.; and
Sambasivan,N.;Kapania,S.;Highfill,H.;Akrong,D.;Pari-
Oh,A.,eds.,Advancesinneuralinformationprocessingsys-
tosh,P.;andAroyo,L.M.2021. ”Everyonewantstodothe
tems,volume35,6720–6734.CurranAssociates,Inc.
model work, not the data work”: Data Cascades in High-
Stakes AI. In Proceedings of the 2021 CHI Conference Vertesi,J.2014.SeamfulSpaces:HeterogeneousInfrastruc-
on Human Factors in Computing Systems, CHI ’21, 1–15. turesinInteraction. Science,Technology,&HumanValues,
New York, NY, USA: Association for ComputingMachin- 39(2):264–284.
ery. ISBN978-1-4503-8096-6. Wang, D.; Prabhat, S.; and Sambasivan, N. 2022. Whose
Sambasivan, N.; and Veeraraghavan, R. 2022. The AIDream?Insearchoftheaspirationindataannotation. In
DeskillingofDomainExpertiseinAIDevelopment.InPro- Proceedingsofthe2022CHIConferenceonHumanFactors
ceedingsofthe2022CHIConferenceonHumanFactorsin inComputingSystems,1–16.
Computing Systems, CHI ’22, 1–14. New York, NY, USA: Wikipedia.“Blue-collarwork”.2024.
AssociationforComputingMachinery. ISBN 978-1-4503- Wikipedia.“White-collarwork”.2024.
9157-3.
Xia, H.; Wang, Y.; Huang, Y.; and Shah, A. 2017. ”Our
Sannon,S.;andCosley,D.2019. Privacy,Power,andInvis- PrivacyNeedsto be Protectedat AllCosts”: Crowd Work-
ibleLaboronAmazonMechanicalTurk. InProceedingsof ers’ Privacy Experiences on Amazon Mechanical Turk.
the2019CHIConferenceonHumanFactorsinComputing Proceedingsof the ACM on Human-ComputerInteraction,
Systems,1–12.GlasgowScotlandUk:ACM. ISBN 978-1- 1(CSCW):1–22.
4503-5970-2.
Scheuerman, M. K.; Hanna, A.; and Denton, E. 2021. Do
Appendix
DatasetsHavePolitics?DisciplinaryValuesinComputerVi-
Foranoverviewoftheproxiesemployedbyrequesters,see
sionDatasetDevelopment.Proc.ACMHum.-Comput.Inter-
act.,5(CSCW2). Table1(nextpage).
Schwartz, O. 2019. Untold History of AI: How Ama-
zon’sMechanicalTurkersGotSqueezedInsidetheMachine.
IEEESpectrum. CitationKey:UntoldHistoryAI2019.
Srinivasan,R.; Denton,E.;Famularo,J.; Rostamzadeh,N.;
Diaz,F.; andColeman,B. 2021. Artsheetsforartdatasets.
InThirty-fifthconferenceon neuralinformationprocessing
systemsdatasetsandbenchmarkstrack(round2).
Strauss,A.L.;andCorbin,J.M.,eds.1997. Groundedthe-
ory in practice. ThousandOaks:Sage Publications. ISBN
978-0-7619-0747-3978-0-7619-0748-0.
Thebault-Spieker,J.;Venkatagiri,S.;Mine,N.;andLuther,
K. 2023. Diverse Perspectives Can Mitigate Political Bias
in Crowdsourced Content Moderation. In Proceedings
of the 2023 ACM Conference on Fairness, Accountabil-
ity, and Transparency, FAccT ’23, 1280–1291. New York,
NY, USA: Association for Computing Machinery. ISBN
9798400701924.
Thylstrup, N. B.; Hansen, K. B.; Flyverbom, M.; and
Amoore, L. 2022. Politics of data reuse in machine learn-
ing systems: Theorizingreuse entanglements. Big Data &
Society,9(2):20539517221139785.
Tubaro,P.;Casilli,A.A.;andCoville,M.2020.Thetrainer,
the verifier,the imitator:Threeways in whichhumanplat-
formworkerssupportartificialintelligence. BigData&So-
ciety,7(1):2053951720919776.Table1:Proxiesuserbyrequesterstoapproximateworker
identityoraptitude
Category Sub-category Description
Workeridentity Englishfluency Worker English fluency or skill
level
Age Workerlegalage
Locationandtimezone WorkerIPaddressandtimezoneas
signalofphysicallocation
Workeraptitude Prehoc:Approvalrating Aggregateapprovalratingforprior
tasks completed on platform, as
ratedbyrequester
Pre hoc: Number of tasks Totalnumberpriortaskscompleted
completed ontheplatform
Post hoc: Keyboardinterac- Speed of typing, calculating
tion amountoftextcopiedandpasted
Posthoc:Answerpattern On sequential multiple choice and
Likert scale questions, visual pat-
tern(whetherornotuniform)ofan-
swerselectionbetweenquestions
Posthoc:Attentioncheck Requiringhuman-onlyreview(i.e.,
catches bots) and/or misleading
questions meant to trip up non-
carefulreaders
Posthoc:Gutreaction An instinctual feeling of whether
or not requester felt task done cor-
rectly
Posthoc:Coherenceastrust Assuming workers who answered
consistently with majority choice
performed work sufficiently; or
worker submission matched “gold
standard”label