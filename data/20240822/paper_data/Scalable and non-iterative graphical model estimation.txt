Scalable and non-iterative graphical model
estimation
Kshitij Khare, Dept. of Statistics, University of Florida
Syed Rahman, Apple Inc.
Bala Rajaratnam, Dept. of Statistics, UC Davis
Jiayuan Zhou, Freddie Mac
Abstract
Graphical models have found widespread applications in many areas of modern
statistics and machine learning. Iterative Proportional Fitting (IPF) and its variants
have become the default method for undirected graphical model estimation, and are
thus ubiquitous in the field. As the IPF is an iterative approach, it is not always
readily scalable to modern high-dimensional data regimes. In this paper we propose
a novel and fast non-iterative method for positive definite graphical model estimation
in high dimensions, one that directly addresses the shortcomings of IPF and its vari-
ants. In addition, the proposed method has a number of other attractive properties.
First, we show formally that as the dimension p grows, the proportion of graphs
for which the proposed method will outperform the state-of-the-art in terms of com-
putational complexity and performance tends to 1, affirming its efficacy in modern
settings. Second, the proposed approach can be readily combined with scalable non-
iterative thresholding-based methods for high-dimensional sparsity selection. Third,
the proposed method has high-dimensional statistical guarantees. Moreover, our nu-
merical experiments also show that the proposed method achieves scalability without
compromising on statistical precision. Fourth, unlike the IPF, which depends on the
Gaussian likelihood, the proposed method is much more robust.
Keywords: Graphical models, ultra high-dimensions, non-iterative estimation, Cholesky de-
composition
1 Introduction
Sparse covariance estimation and graphical models have become a staple of modern statis-
tical inference and machine learning. They have also found widespread use in a spectrum
of application areas. In this paper we specifically focus on the large class of undirected (or
concentration) graph models, where some entries of the inverse covariance matrix Ω are re-
stricted to be zero. These models have been of significant interest in recent years, especially
as high-dimensional datasets with tens of thousands of variables or more have become in-
creasingly common. The cases when the underlying distribution is Gaussian has also been of
1
4202
guA
12
]EM.tats[
1v81711.8042:viXraspecialinterest. Underthisassumption, zeroesintheconcentrationmatriximplyconditional
independencies.
Suppose Y ,Y ,··· ,Y are i.i.d. observations from a distribution on Rp. Let Σ denote
1 2 n
the covariance matrix, and let Ω = Σ−1 denote the concentration matrix or the inverse co-
variance matrix of this distribution. There are often two tasks involved in the statistical
analysis of undirected graphical models. The first is model or graph selection, or identi-
fying the pattern of structural zeros in Ω. The second is positive definite estimation, i.e.,
finding a positive definite (p.d.) estimate of Σ under the discovered sparsity pattern. Such
positive definite estimates are often required for downstream applications where the covari-
ance estimate is a critical ingredient in multivariate analysis techniques. Examples of such
applications are widespread in the biomedical, environmental and social sciences.
A number of useful methods have been proposed for positive definite inverse covariance
matrix estimation consistent with a given sparsity pattern. For Gaussian undirected graphi-
cal models, the MLE cannot be obtained in closed form unless the pattern of zero restrictions
corresponds to a decomposable or chordal graph (see Lauritzen [1996]). A novel iterative al-
gorithm called iterative proportional fitting (IPF) was proposed by Speed and Kiveri [1986]
to obtain the MLE in this case. Over the last few decades, the IPF estimation method
has become synonymous with undirected graphical model estimation for a given sparsity
pattern. Several faster modifications and adaptations of IPF have been proposed in recent
years, highlighting the contemporaneous nature of this line of research, see for example Hara
and Takemura [2010], Xu et al. [2011, 2012, 2015], Hojsgaard and Lauritzen [2023]. These
iterative algorithms rely on sequential updates to relevant sub-matrices of Σ or Ω based on
the given sparsity pattern.
A different approach which obtains the MLE by modifying the Glasso algorithm to take
the known sparsity pattern into account has been developed in [Hastie et al., 2009, Algo-
rithm 17.1]. The likelihood is maximized one row at a time using block coordinate-wise
ascent. We refer to this algorithm as the G-IPF algorithm. There is also substantial lit-
erature on Bayesian estimation for Gaussian undirected graphical models, see Dawid and
Lauritzen [1993], Roverato [2002], Letac and Massam [2007], Rajaratnam et al. [2008], Mit-
sakakis et al. [2011], Lenkoski [2013] and the references therein. Numerical approaches for
estimation in undirected graphical models such as the IPF and its variants, and the G-IPF,
all tend to employ computationally intensive iterative algorithms which have to be repeated
until convergence. The iterative nature of these algorithms thus does not always render
them immediately scalable to the modern setting where scalability is a real necessity. Such
modern regimes pose a significant challenge in the sense that they require scalability without
compromising on statistical precision.
Asmentionedabove, theiterativenatureofcurrentmethodsisanimmediateimpediment
to scalability, in the sense that numerical computations of a high order have to be repeated,
thus resulting in a prohibitive computational burden. Hence, it is critical to construct a
possibly non-iterative method which is able to provide a positive definite estimate of Ω which
is consistent with a given general sparsity pattern. A new estimation framework, one which
is specifically designed for and addresses the high dimensional nature of the problem, is re-
quired. We address this crucial gap in the literature by introducing a non-iterative method,
called CCA, for positive definite estimation in undirected graphical models with an arbi-
trary sparsity pattern in the concentration matrix. The method is comprised of two steps
and can be summarized as follows: In the first step, estimates are obtained for a pattern of
2zeros corresponding to an appropriate larger chordal graph. As the approximating graph is
chordal, estimatescanbeimmediatelycomputedinclosedform, circumventingrepeatediter-
ations. In the second step, the Cholesky decompositions of such estimates are then suitably
modified to obtain estimates with the original pattern of zeroes in the concentration matrix.
These steps together yield an approach which obviates the need to employ computationally
expensive iterative algorithms when dealing with non-chordal concentration graphical mod-
els. Specifically, the chordal graph serves as an important conduit to obtaining the desired
concentration graph estimates. Notably, methods such as the IPF and its modern variants
require repetition of its iterations until convergence, while no such repetition is necessary
for the CCA algorithm. Thus, the proposed CCA algorithm provides significant computa-
tional savings compared to IPF or G-IPF. Moreover, in several settings the computational
complexity of the CCA algorithm is smaller than even a single iteration of the G-IPF al-
gorithm (see Section 3.2 for examples), bearing in mind that the G-IPF iterations have to
be repeated thereafter till convergence. We demonstrate both theoretically and numerically
that substantial computational improvements can be obtained using the proposed approach,
while maintaining similar levels of statistical precision as state-of the art approaches.
In addition to the introduction of a novel non-iterative estimation framework, the pro-
posedCCAapproachhasmultipleotherbenefits-frombothatheoreticalandmethodological
perspective. Recall that the IPF and its variants fundamentally depend on the functional
form of the Gaussian likelihood. On the contrary, the proposed CCA approach can instead
also be formulated in a non-parametric way yielding a more robust estimator, thus mak-
ing it more immune to the effect of outliers. CCA also enjoys high dimensional statistical
guarantees. The proposed CCA approach has yet another significant benefit in the sense
it can be readily coupled with any scalable thresholding method for obtaining a sparsity
pattern/ pattern of zeros in the inverse covariance matrix. More specifically, non-iterative
thresholding based methods for model selection, such as the HUB screening method in Hero
and Rajaratnam [2012], and the recently proposed FST method in Zhang et al. [2021], are
equipped with high-dimensional statistical guarantees and are generally much faster and
scalable to significantly higher dimensional settings than the penalized likelihood methods
which use iterative optimization algorithms. To clarify, the term non-iterative sparsity se-
lection method refers to a method which requires a fixed and finite amount of time, and
does not need repetitions until convergence. However, such thresholding based methods,
though very useful, are not guaranteed to produce positive definite estimates of Ω, which
in turn could hinder their use in downstream applications which require a positive definite
estimate of the covariance matrix (see for example Ledoit and Wolf [2003], Guillot et al.
[2015]). Coupling the proposed CCA approach with scalable thresholding methods however
guarantees computational efficiency and scalability of the whole procedure as well as posi-
tive definiteness of the final inverse covariance matrix estimate. In principle, the proposed
CCA approach can also be combined with ℓ -penalized methods: Recall that ℓ -penalized
1 1
methods such as CONCORD (Khare et al. [2015]) yield sparse inverse covariance matrix
estimates but are not guaranteed to yield positive definite estimates. Here, one can employ
the proposed CCA approach to produce a p.d. estimator which is consistent with a sparsity
pattern induced by any ℓ -penalized method.
1
The paper is organized as follows. Section 2 provides required preliminaries. The pro-
posed positive definite estimation method is introduced in Section 3. A thorough evaluation
of its computational complexity is provided in Section 3.2. In Section 4 we establish asymp-
3toticconsistencyoftheproposedestimatesinthehigh-dimensionalsettingwherethenumber
of variables increases with sample size. Detailed experimental evaluation, validation and ap-
plications to real data is undertaken in Section 5.
2 Preliminaries
In this section we provide the required background material from graph theory and matrix
algebra.
2.1 Graphs and vertex orderings
A graph G = (V,E) consists of a finite set of vertices V (not necessarily ordered) with
|V| =: p, and a set of edges E ⊆ V × V. We consider undirected graphs with no loops.
Hence, (v,v) ∈/ E for every v ∈ V, and (u,v) ∈ E ⇒ (v,u) ∈ E. Next, we define the notions
of vertex ordering and ordered graphs. These are necessary in subsequent analysis.
Definition 1 (Vertex ordering) A vertex ordering σ of the vertex set V is a bijection from
V to V := {1,2,··· ,p}.
p
Definition 2 (Ordered graph) Consider the undirected graph G = (V,E) and an ordering σ
of V. The ordered graph G has vertex set V and edge set E = {(σ(u),σ(v)) : (u,v) ∈ E}.
σ p σ
2.2 Cholesky decomposition
Given a positive definite matrix Ω ∈ P+, there exists a unique lower triangular matrix L
p
with positive diagonal entries such that Ω = LLT, where L is the “Cholesky factor” of Ω.
The Cholesky decomposition will be a useful ingredient in the estimators constructed in this
paper.
2.3 The spaces P and L
G G
σ σ
Let G = (V,E) be a graph on p vertices and let σ be an ordering of V. Let P+ denote the
space of p×p positive definite matrices, and L+ denote the space of p×p lower triangular
matrices with positive diagonal entries. We consider two matrix spaces that will play an
important role in our analysis. The first matrix space
P = (cid:8) Ω ∈ P+ : Ω = 0 if i ̸= j and (i,j) ∈/ E (cid:9) ,
Gσ ij σ
is the collection of all p×p positive definite matrices where the (i,j)th entry is zero if i and
j do not share an edge in the ordered graph G . The other matrix space
σ
L = (cid:8) L ∈ L+ : LLT ∈ P (cid:9) ,
Gσ Gσ
is the collection of all p×p lower triangular matrices with positive diagonal entries, which
can be obtained as a Cholesky factor of a matrix in P .
Gσ
42.4 Decomposable graphs
An undirected graph G is said to be decomposable if it is connected and does not contain
a cycle of length greater than or equal to four as an induced subgraph. The reader is
referredtoLauritzen[1996]forallthecommonnotionsofgraphicalmodels(andinparticular
decomposable graphs) that we will use here. One such important notion is that of a perfect
vertex elimination scheme.
Definition 3 An ordering σ of V is defined to be a perfect vertex elimination scheme
for a decomposable graph G if for every u,v,w ∈ V such that σ(u) > σ(v) > σ(w) and
(σ(u),σ(w)) ∈ E ,(σ(v),σ(w)) ∈ E , then (σ(u),σ(v)) ∈ E .
σ σ σ
2.5 Cholesky factors and fill-in entries
Let G be a decomposable graph, and σ an ordering of V. Paulsen et al. [1989] prove that σ
is a perfect vertex elimination scheme for G if and only if
L = (cid:8) L ∈ L+ : L = 0 if i > j,(i,j) ∈/ E (cid:9) ,
Gσ ij σ
i.e, the structural zeroes in every matrix in P are also reflected in its Cholesky factor.
Gσ
However, if the graph G is non-decomposable, or if G is decomposable but σ is not a
perfect vertex elimination scheme, then for any matrix in P , the zeroes in the Cholesky
Gσ
factor are in general a subset of the structural zeroes in P . The extra non-zero entries in
Gσ
the Cholesky factor are refered to as the fill-in entries corresponding to P . For example,
Gσ
consider the non-decomposable 4-cycle graph G = (V,E) below where σ is such that E =
σ
{(1,2),(2,3),(3,4),(4,1)} with Ω and L specified as below.
1 2
   
3 1 0 1 1.732 0 0 0
1 3 1 0 0.577 1.633 0 0 
, Ω =   and L =  .
0 1 3 2  0 0.612 1.620 0 
1 0 2 3 0.577 −0.204 1.312 0.951
4 3
(1)
Note that Ω ∈ P , and it can be shown that L above is its Cholesky factor. Clearly, the
Gσ
zero entries in the lower triangle of L are a strict subset of the zero entries in Ω. The fill-in
entry in this case is the (4,2)th entry. Next, we define the notion of a filled graph.
Definition 4 Given an ordered graph G , the filled graph for G , denoted by GD = (V ,ED),
σ σ σ p σ
is defined as follows:
1. GD is an ordered decomposable graph, i.e., G is a decomposable graph and σ is a
σ D
perfect vertex elimination scheme for G .
D
2. For every Ω ∈ P , the corresponding Cholesky factor L satisfies L = 0 if i >
Gσ ij
j,(i,j) ∈/ ED.
σ
3. The edge set ED of GD is a subset of the edge set of every graph satisfying properties
σ σ
1 and 2.
5Essentially, the filled graph for G is the “minimal” (ordered) decomposable graph whose
σ
edge set contains E along with all the edges corresponding to the fill-in entries for matrices
σ
in L , i.e., a “decomposable cover” for G . In the 4-cycle example considered above, the
Gσ σ
filled graph GD has edge set given by
σ
ED = {(1,2),(2,3),(3,4),(4,1),(4,2)}.
σ
For given G and σ, a simple procedure to obtain GD called the elimination tree method (see
σ
Davis [2006]) is given below.
1. Start with ED = E . Set k = 1.
σ σ
2. Let N> = {j : j > k,(j,k) ∈ ED}. Add necessary edges to ED so that N> is a clique.
k σ σ k
3. Set k = k +1. If k < p−1, go to Step 1.
2.6 Choice of the ordering σ
The proposed CCA method presented in the next section will require Cholesky decomposi-
tions of appropriate positive definite matrices. Since the Cholesky decomposition inherently
uses an ordering of the variables, the choice of this ordering is an important issue which
needs to be addressed in a principled way. In several applications, a natural domain-specific
ordering of the variables based on time, location etc. is available (see examples in Huang
et al. [2006], Rajaratnam et al. [2008], Shojaie and Michailidis [2010], Yu and Bien [2017],
Khare et al. [2019]). However, in other applications a natural ordering is not always avail-
able. In such settings, we can choose the ordering of the variables based on computational
considerations as follows. Note that the decomposable cover GD is a function of the ordering
σ
σ. As we shall see in Section 3, it is important to choose σ which gives as fewer fill-in entries
as possible, i.e., the number of extra edges in ED as compared to E . This problem is well-
σ σ
studied in the computer science literature. In fact, it is an NP-hard problem which makes
it challenging to solve in practice (see Davis [2006] and the references therein). However,
several heuristic methods have been developed to solve this problem, including the Reverse
Cuthill-Mckee (RCM) method, which is implemented in the chol function in R. We refer
to the ordering produced by this method as the RCM fill reducing ordering. In subsequent
analysis, we choose σ as the RCM ordering in case a natural ordering of the variables is not
available.
3 A non-iterative Gaussian graphical model estimator
3.1 Methodology: The constrained Cholesky approach
Suppose Y ,Y ,··· ,Y are i.i.d. observations from a distribution on Rp with covariance
1 2 n
matrix Σ and concentration matrix Ω = Σ−1. The sparsity pattern in Ω is assumed to be
given, as encoded by the graph G. When the distribution is multivariate Gaussian, the
zeros in Ω signify conditional independence between the relevant variables, and G can be
interpreted as a conditional independence graph. Even when the underlying distribution is
not Gaussian, sparsity in Ω signifies the lack of relevant partial correlations, and G can be
interpreted as a partial correlation graph. In several applications, G is not available based on
6domain-specific knowledge, in which case we use an estimate of G provided by a thresholding
based approach such as FST (Zhang et al. [2021]). As specified earlier, similar to the IPF
our goal is to find a p.d. estimate of Ω consistent with a given sparsity pattern, but one that
is scalable to modern data regimes. For a given σ (or one obtained by RCM fill reducing
approach), the task considered in this sub-section is to find a positive-definite estimate of Ω
lying in P , without recourse to iterative approaches.
Gσ
Before we formally describe the specific aspects of the CCA methodology, we first pro-
vide an intuitive description of the two steps which encapsulate the proposed CCA method.
Recall that graphical model estimation for a general non-decomposable graph faces three
significant challenges (especially in high-dimensional settings): (1) lack of a closed form
solution warranting an iterative procedure (such as the IPF), (2) the graphical model con-
straint where estimates need to lie in the correct parameter space (i.e., the prescribed zero
pattern), and (3) the more nuanced and “delicate” positivity requirement (note that the
sparsity and positivity constraints constitute two distinct hurdles).
CCA specifically recognizes the above three challenges and provides a solution which
overcomes all three of them in a computationally tractable manner. The order in which
this is undertaken is crucial. First, CCA circumvents the need for an iterative procedure by
transferring the problem from the given graph G to a decomposable cover of G (as denoted
by GD). Doing so, ensures that a closed form solution is obtained for a problem which is
“close” to the original estimation problem. Transferring the problem to a decomposable
cover, however, introduces additional edges in the graph GD which are not present in the
original graph G, i.e., some of the required zeros in the precision matrix estimator are lost
(the additional parameters in
ΩˆD
are referred to as “fill-in entries”, the reason for which will
become clear very soon). More specifically, the issue with the resulting decomposable graph
estimatoristhatitdoesNOThavethecorrectsparsitypattern-thoughitispositivedefinite.
As positivity is a more “delicate” property, it can be easily lost if one zeros out the resulting
decomposable estimator to achieve the desired sparsity pattern. In anticipation of the risk
of losing positivity while aiming for the correct sparsity pattern, the resulting decomposable
graph estimator is first expressed in terms of its Cholesky decomposition (which is available
in closed form thus avoiding any iterative procedure). There are two reasons for this. The
ˆ
first reason is that if the correct vertex ordering is used, the zeros in ΩD are preserved. The
second reason is that the Cholesky entries do not have to obey any positivity constraints and
arethereforemucheasiertoworkwith. Itisthusanidealsettingtoworkintotryandobtain
the desired sparsity pattern since deforming a Cholesky does not affect positivity (provided
the diagonals are positive). In summary, obtaining the decomposable cover estimator and
its corresponding Cholesky transform constitute Step 1 of the CCA approach.
Armed with the closed form Cholesky estimator for the decomposable graph, the remain-
ing task is to obtain the correct sparsity pattern for the final estimate of Ω, i.e., to ensure
that the final estimate lies in P . Here, the CCA recognizes that Cholesky entries which
Gσ
correspond to non-zero entries in Ω can be regarded as functionally independent parameters
intheCholesky, whereaszerosinΩwhicharenotzerosintheCholeksyareactuallyfunction-
ally dependent parameters (even if they appear as non-zeros in the corresponding Cholesky).
This insight follows from realizing that Ω and its Cholesky are simply two representations
of the same quantity - hence the number of true functionally independent parameters in the
Cholesky has to be the same as that of the Ω (i.e., the dimension of the parameter set cannot
change because of a transformation). Thus, the entries in the Choleksy which correspond
7to zero entries in Ω can be regarded as functionally dependent. With this insight in hand,
CCA modifies the entries of the Cholesky corresponding to these functional independent
parameters (i.e., “fill-in” entries), to obtain zeros in the corresponding positions at the level
of Ω. This process is essentially a projection of the Cholesky to the correct parameter space.
Doing so enforces the desired sparsity pattern at the level of Ω. This process of modifica-
tion or adjustment of the Cholesky is facilitated by the fact that Ω is simply its Cholesky
factor multiplied by the transpose of the Cholesky factor. In other words, the quadratic
relationship between Ω and its Cholesky factor renders the adjustment of the fill-in entries
a straightforward task (and importantly in closed form). In summary, note that the original
graph G has a given zero pattern - some of which are lost by transferring the problem to the
graph GD. The additional edges introduced by going to GD, and the sparsity that is lost be-
cause of this, is regained through adjusting the Cholesky parameter of
ΩˆD.
The adjustment
of the Cholesky to achieve the desired sparsity pattern at the level of Ω constitutes Step 2
of the CCA approach.
Altogether, the above closed form estimation and adjustment operations completely cir-
cumvent recourse to any iterative procedure. Specifics of the CCA methodology are now
described through the aforementioned two step process.
Step I: The first step in our approach is to find an estimator of Ω in P . Recall that GD,
GD σ σ
which denotes the filled graph for G , is a decomposable graph whose edge set subsumes
σ
the edge set of G . When Ω ∈ P and if the underlying distribution is Gaussian, the MLE
σ GD
σ
Ω(cid:98)D for Ω is well-defined as long as n is equal to or larger than the maximal clique size in
GD, and can be obtained in closed form (see Lauritzen [1996]). Let L(cid:98)D denote the Cholesky
σ
factor of Ω(cid:98)D. It can be shown (see for example Atay-Kayis and Massam [2005], Roverato
[2002]) that the non-zero entries of the jth column of L(cid:98)D can be obtained as
(S>j)−1S> 1
L(cid:98)D ·j,> = −
(cid:113)
·j and L(cid:98)
jj
=
(cid:113)
(2)
S −(S>)T(S>j)−1S> S −(S>)T(S>j)−1S>
jj ·j ·j jj ·j ·j
for every 1 ≤ j ≤ p. Here for any matrix A, A>j := ((A )) and A> :=
kl k,l>j,(k,j),(l,j)∈E σD ·j
(A kj) k>j,(k,j)∈ED. Note that Ω(cid:98)D lies in P
GD
and generally will not lie in the desired space
σ σ
P . We will refer to Ω(cid:98)D and its Cholesky factor L(cid:98)D as intermediate estimators. Note that
Gσ
Step I produces estimators for Ω-entries corresponding not only to edges in E but also for
σ
additional edges in ED \E .
σ σ
Step II: The second step in our approach is to modify or adjust the Cholesky factor L(cid:98)D
to a lower triangular matrix L(cid:98) such that Ω(cid:98) = L(cid:98)L(cid:98)T ∈ P , i.e., to eliminate the additional
Gσ
non-zero entries in our estimate of Ω that arise from working with a decomposable cover so
that our estimator lives in the desired space. In particular, we construct L(cid:98) by sequentially
changing each fill-in entry of L(cid:98)D such that the corresponding entry of the concentration
matrix (obtained by multiplying the resulting lower triangular matrix with its transpose)
is zero. We start with the second row and move from entries with the lowest index to the
highest index just before the diagonal. Whenever we encounter a fill-in entry along the way,
say (i,j), we set L(cid:98) as follows:
ij
(cid:80)j−1
L(cid:98) L(cid:98)
L(cid:98) = − k=1 ik jk , (3)
ij
L(cid:98)
jj
8thereby ensuring that the (i,j)th entry of the corresponding concentration matrix is zero.
Note that the update in (3) only uses entries from the ith and jth row. Also, any fill-in entries
appearing on the right hand side in (3) have already been updated at this point, and will
not be changed subsequently. Hence, the resulting concentration matrix at the end of the
second step of our algorithm lies in P (see Lemma 1 below for formal proof). The above
Gσ
construction by design also guarantees that L(cid:98) = 0 = L(cid:98)D if i > j,(i,j) ∈/ ED, and L(cid:98) = L(cid:98)D
ij ij σ ij ij
if (i,j) ∈ E or i = j, i.e., only the fill-in entries of L(cid:98)D are changed.
σ
We call the proposed statistical method Constrained Cholesky Approach (CCA) and the
correspondng algorithm the CCA algorithm - see Algorithm 1 below. Note that Ω(cid:98) (the
resulting CCA estimate) is obtained in a fixed and finite number of steps, and there is no
repetition of an identical sequence of steps until convergence. The following lemma (proof in
Supplemental Section C) establishes that Ω(cid:98) produced by CCA lies in P , i.e., it is positive
Gσ
definite and has the required pattern of zeros.
Lemma 1 Suppose n is greater than the largest clique size in GD. Let Ω(cid:98) be the estimate of
σ
the concentration matrix produced by CCA method (see also Algorithm 1). Then Ω(cid:98) ∈ P .
Gσ
Remark 1 The key idea used in the CCA algorithm is to first find a cover of the given
graph for which a principled estimator can be easily computed (Step I), and then adjust the
Cholesky values corresponding to the extra edges in the cover to obtain the desired sparsity
in the precision matrix (Step II). This process works well for decomposable covers. A natural
question to ask is whether there is some known class of graphs which would work even better,
i.e., is the class of decomposable graphs maximal or optimal in some sense, or can we do
better? We argue below that the proposed approach would either not work as effectively, or
would even break down for other well-known classes of graphs studied in the graphical models
literature. One candidate to consider is the class of homogeneous graphs (Letac and Massam
[2007], Khare and Rajaratnam [2012]), which is a well-studied subclass of decomposable
graphs. While the Gaussian MLE restricted to a homogeneous sparsity pattern is available
in closed form, a homogeneous cover typically needs many more added edges compared to
a decomposable cover. This would lead to many more fill-in entries, and hence increase
the computational burden in Step II. Too many additional entries can also affect statistical
precision. The same line of reasoning can be used to show that any special class of graphs
which is a subset of the class of decomposable graphs (such as tress) would require additional
edges, thus substantially increasing computational burden in Step II without any material
benefit in Step I. On the other side, the class of Generalized Bartlett (GB) graphs (Khare
et al. [2018]) contains the class of decomposable graphs as a special case. Unlike decomposable
graphs, the Gaussian MLE for general GB graphs is not available in closed form. Hence,
an iterative algorithm would be needed for Step I, which in turn would significantly increase
computational burden. So, any gains and justification thereof for using a Generalized Bartlett
cover would have to come from Step II, since there are fewer extra zeros to correct in the
intermediate estimator of Ω. However, these savings cannot reverse the substantial additional
computational cost required for Step I. As the class of decomposable graphs is the maximal
class of graphs for which the Gaussian MLE and its Cholesky factor can be obtained in closed
form, any larger class of graphs (such as the GB class) would not be amenable to the proposed
CCA approach. The above arguments underscore the correct balance that decomposable graphs
achieve.
9Another question is whether the CCA estimators Ω(cid:98) and L(cid:98) can be obtained as solutions to
an optimization problem. Note that by definition the intermediate estimator Ω(cid:98)D in Step
I above minimizes the negative Gaussian log-likelihood (subject to Ω ∈ P ). Using the
GD
σ
decomposability of GD, the intermediate Cholesky estimator L(cid:98)D can thus be expressed as
σ
(cid:8) (cid:9)
L(cid:98)D = argmin
L∈L
GD
σ
tr(LLTS)−2log|L| . (4)
ˆ
The next lemma (proof in Supplemental Section D) identifies the CCA estimator L obtained
in Step II above as the unique solution to an optimization problem (given the intermediate
estimator L(cid:98)D obtained in Step I).
Lemma 2 Given the intermediate estimator L(cid:98)D, the CCA estimator L(cid:98) uniquely minimizes
(over the set L Gσ) the function h(L) = (cid:80)
(i,j)∈Eσ or
i=j(L(cid:98)D
ij
−L ij)2.
Since the intermediate estimator L(cid:98)D is itself a solution to an optimization problem, the
above lemma can be combined with (4) to obtain the following representation of the final
CCA estimator.
Lemma 3 Given the sample covariance matrix S, the ordered graph G and its filled graph
σ
GD, the final CCA estimator L(cid:98) can be expressed as
σ
(cid:88)
(cid:18)
(cid:16) (cid:8) (cid:9)(cid:17)
(cid:19)2
L(cid:98) = argmin
L∈L
Gσ or
argmin
L∈L
GD σ
tr(LLTS)−2log|L|
ij
−L
ij
.
(i,j)∈Eσ i=j
TheabovelemmashowshowCCAavoidsaniterativeoptimizationapproachbydecomposing
theestimationproblemintotwosteps. Thefirststepprojectstheproblemtoarelevantspace
involving chordal graphs, and has a closed form solution. The second step projects this
solution on to the desired space thus ensuring the correct sparsity pattern and preserving
positive definiteness, and requires a single pass through the fill-in entries of L(cid:98)D. These
two steps together circumvent iterative/repetitive high-dimensional computations: a single
iteration is all that is required at each of the two steps of CCA.
Remark 2 (Non-parametric interpretation of CCA) We now note that the intermediate
estimator L(cid:98)D can in fact be seen as a non-parametric estimator which does not rely on
the Gaussian functional form. In particular, the corresponding precision matrix estimator
Ω(cid:98)D = L(cid:98)D(L(cid:98)D)T is the unique matrix in P
GD
which satisfies the constraint that the (i,j)th
σ
entry of its inverse equals S (the (i,j)th entry of the sample covariance matrix) for every
ij
(i,j) ∈ ED. To see this more formally, as in Letac and Massam [2007], let I be the linear
σ GD σ
space of symmetric incomplete matrices A with the (i,j)th-entry missing for every (i,j) ∈/ E.
Let Q be the collection of all A ∈ I such that any submatrix of A corresponding to a
GD G
σ
maximal clique of GD is positive definite. For any A ∈ Q , the decomposability of GD
σ GD σ σ
˜
along with a result in Grone et al. [1984] implies the existence of a unique completion A of A
such that A˜−1 ∈ P . Now, let SD denote the matrix in I obtained by removing all entries
G GD
σ
with indices outside ED. If n is larger than the largest clique size in GD, then it follows that
σ σ
SD ∈ Q . It now follows by [Letac and Massam, 2007, Theorem 2.1] that Ω(cid:98)D = (S˜D)−1.
G
Note specifically that Step 1 on the CCA approach can be implemented without invoking
the Gaussian functional form. As Lemma 2 demonstrates, the second step of the CCA
10approach (constructing the CCA estimator
Lˆ
for
LˆD)
is again non-parametric/distribution-
free in nature and only adjusts the fill-in entries of L(cid:98)D to ensure that the CCA precision
estimator lies in P . Given the non-parametric flavor of the entire CCA procedure, the
Gσ
CCA estimator is more robust. It is thus more immune to outliers compared to the IPF and
its variants which in turn fundamentally depend on the Gaussian likelihood.
Algorithm 1: The CCA Algorithm
INPUT - Sample covariance matrix S
INPUT - Sparsity pattern encoded in a graph G
Compute RCM fill reducing ordering σ
Compute filled graph GD
σ
Compute Cholesky factor L(cid:98)D of Ω(cid:98)D (as specified in (2))
Set L(cid:98) ← L(cid:98)D
for all i = 2 to p do
for all j = 1 to i−1 do
if (i,j) ∈/ E then
σ
Set L(cid:98) =
−(cid:80) kj− =1 1L(cid:98)ikL(cid:98)jk
ij
L(cid:98)jj
end if
end for
end for
OUTPUT - Return final estimates L(cid:98) and Ω(cid:98) = L(cid:98)L(cid:98)T
3.2 Computational aspects of the CCA Algorithm
In this section, we provide relevant computational details and evaluate the computational
complexity of each step in the CCA algorithm. (Algorithm 1). As part of calculating the
computational complexity of the CCA algorithm, note the following.
• The complexity of the step to determine the RMC fill reducing ordering σ is O(|E |)
σ
(see Chan and George [1980]).
• The complexity of computing GD (the decomposable cover) using the elimination tree
σ
(cid:16) (cid:17)
method outlined in Section 2.5 is O (cid:80)p n2 , where n = {i : 1 ≤ j < i ≤ p,(i,j) ∈
j=1 j j
ED} for every 1 ≤ j ≤ p.
σ
• Computational complexity of Step 1: The complexity of computing L(cid:98)D based on eq.
(cid:16) (cid:17)
(2) is calculated as O p+(cid:80)p n3 , since the complexity of computing the inverse of
j=1 j
a matrix is the cube of the dimension of the matrix. Of course, it only makes sense to
use (2) to compute L(cid:98)D if GD is sparse. For dense GD we can use a direct Cholesky
σ σ
11factorization to obtain L(cid:98)D with complexity O(p3). As a practical rule, we use direct
computation of LˆD when |ED| > p5/3, since in this case by Jensen’s inequality
σ
p
(cid:32)
p
(cid:33)3
(cid:88) 1 (cid:88) |ED|3
n3 ≥ p n = σ > p3. (5)
j p j p2
j=1 j=1
• Computational complexity of Step 2: The second step of obtaining L(cid:98) from L(cid:98)D can be
efficiently implemented by observing the following. Note that Ω(cid:98) ∈ P
Gσ
⊆ P
GD
σ. Since
GD is the filled graph for G , it follows that L(cid:98) = 0 = L(cid:98)D if i > j,(i,j) ∈/ ED. Also,
σ σ ij ij σ
by construction, L(cid:98) = L(cid:98)D if (i,j) ∈ E or i = j. Hence, the only entries which will be
ij ij σ
changed in the second step of Algorithm 1 are the fill-in entries or the edges in ED\E .
σ σ
The complexity of modifying these entries in the algorithm can be easliy calculated as
(cid:0) (cid:12) (cid:12)(cid:1)
O p(cid:12)ED \E (cid:12) .
σ σ
Straightforwardcalculationsshowthatthecomplexitiesinthefirsttwobulletsaredominated
by those in the last two bullets above, leading to the following lemma.
Lemma 4 (Computational complexity of CCA) For any connected ordered graph G ,
σ
the computational complexity of the entire CCA algorithm is
(cid:32) (cid:32) (cid:33)(cid:33)
p
(cid:88) (cid:12) (cid:12)
O min p+ n3 +p(cid:12)ED \E (cid:12), p3 .
j σ σ
j=1
We now quantify the computational complexity of state-of-the-art methods. Recall that
Iterated Proportional Fitting (IPF) is the classical “go-to method” for computing the MLE
for Ω in a Gaussian concentration graphical model when the underlying sparsity pattern
(encoded in the graph G) is known (Speed and Kiveri [1986]). In particular, the goal is to
compute
arg min tr(ΩS)−logdet(Ω). (6)
Ω∈P+
G
Note that each iteration of the IPF algorithm requires sequential conditional maximization
of the Gaussian likelihood with respect to sub-matrices corresponding to maximal cliques
of the given graph G. If a given maximal clique has c vertices, then the corresponding
conditional maximization involves inversion of a (p−c)-dimensional matrix and thus requires
O((p − c)3) iterations. This can be expensive for sparse graphs where c << p in general.
Several modifications of the original IPF algorithm which increase computational efficiency
in such settings have been proposed over the last two decades. The line of work in Hara and
Takemura [2010], Xu et al. [2011, 2012] and others focuses on ‘local’ updates of the clique
based sub-matrices using triangulations and junction trees to avoid large matrix inversions.
As pointed out in Xu et al. [2015], while these methods can significantly lower the amount of
computations, they need significantly more memory. The IPSP method in Xu et al. [2015]
employs the strategy of partitioning the set of maximal cliques of G into non-overlapping
blocks, and then performing local updates for the clique sub-matrices in each block. All
of the above methods are iterative and need an enumeration of the maximal cliques of the
graph G. While such an enumeration can be done efficiently for some family of sparse
graphs, it can also be very expensive in general and takes exponential time in the worst-case
12Method Overall complexity
(cid:16) (cid:16) (cid:17)(cid:17)
CCA O min p+(cid:80)p n3 +p(cid:12) (cid:12)ED \E (cid:12) (cid:12), p3 ,
j=1 j σ σ
(cid:16) (cid:17)
G-IPF T ×O (cid:80)p n3 +p|E |
Gl j=1(cid:101)j σ
FIPF T ×O(p2|E |)
F σ
IPSP T
×O(cid:0) p2(cid:80) |U|(cid:1)
+T
I U∈U enum
Table 1: Overall computational complexity of various undirected/concentration graph model
estimators (for connected G ). For disconnected graphs, complexities can be computed
σ
over all the connected components and then added together. Here T ,T ,T denote
Gl F I
the number of repetitions needed to achieve the convergence threshold for G-IPF, FIPF,
IPSP respectively, T is the time needed to enumerate the maximal cliques of G,
enum
n = |{i : 1 ≤ i ≤ p,(i,j) ∈ E }|, n = {i : 1 ≤ j < i ≤ p,(i,j) ∈ ED}, C is the collec-
(cid:101)j σ j σ
tion of all maximal cliques of G, and U is a partition of the C.
setting (see Example 5 below). In very recent work, Hojsgaard and Lauritzen [2023] cleverly
employ a version of Woodbury’s identity in Harville [1977] to avoid enumeration of maximal
cliques of G as well as large matrix inversions in the conditional maximization steps of the
IPF algorithm. We refer to this algorithm, based on the “edgewise” and “covariance-based”
version of IPF developed in Hojsgaard and Lauritzen [2023], simply as FIPF (Fast IPF).
An alternative approach to find the MLE in the Gaussian setting is described in [Hastie
et al., 2009, Algorithm 17.1] and implemented in the glasso package. Essentially, this ap-
proach uses a modification of the well-known Glasso algorithm (i.e., without the ℓ penalty
1
term which accounts for the fact that the sparsity pattern in Ω is known). We will refer to
this algorithm as the G-IPF algorithm. Note that similar to the IPF and its adaptations,
G-IPF repeats its iterations until an appropriate convergence threshold has been achieved.
However, unlike these algorithms (except F-IPF), G-IPF does not require an enumeration of
the maximal cliques of G. A straightforward analysis of [Hastie et al., 2009, Algorithm 17.1]
and results in Xu et al. [2015], Hojsgaard and Lauritzen [2023] can be used to calculate the
overall computational complexity of the IPSP, FIPF and G-IPF algorithms. These complex-
ities, along with the complexity of the proposed CCA algorithm (based on the discussion at
the beginning of this subsection) are summarized in Table 1.
The next lemma (proof in Supplemental Section E) compares the computational com-
plexity of CCA with FIPF and IPSP.
Lemma 5 (Computational Complexity: CCA vs. FIPF and IPSP) For an arbitrary graph
G, the computational complexity of even one iteration of the IPSP algorithm or the FIPF
algorithm is greater than or equal to that of the entire CCA algorithm.
The next lemma (proof in Supplemental Section F) compares the computational complexity
of CCA with G-IPF especially in the context when p is large. To facilitate this analysis, we
consider the class of p-vertex graphs with at least p5/3 edges, denoted by A . This is a large
p
class of graphs with elements ranging from moderately sparse graphs (since p5/3 =
o(cid:0)(cid:0)p(cid:1)(cid:1)
),
as well as very dense graphs (with more than
0.5(cid:0)p(cid:1)
edges).
2
2
Lemma 6 (Computational Complexity: CCA vs. G-IPF ) Consider the class A of p-vertex
p
graphs with more than p5/3 edges. Then
13(a) For all graphs G ∈ A , the CCA algorithm is computationally more efficient than even
p
one iteration of the G-IPF algorithm.
(b) The class of p-vertex graphs belonging to A as a proportion of the total number of
p
p-vertex graphs converges super-exponentially to 1 as p → ∞. In particular,
(cid:114)
|A | 2
p ≥ 1− exp(cid:0) −(cid:0) 0.17p2 −3p5/3logp(cid:1)(cid:1) for p ≥ 100.
(p) π
2 2
In summary, compared to competing methods the computational complexity of the proposed
CCA method is always lower for any dimension (when compared to FIPF and IPSP) or
almost always lower (when compared to G-IPF) as the dimension p gets larger, underscoring
its direct relevance in modern settings. Equally or more importantly, this lower complexity is
for a single iteration, bearing in mind that CCA requires only one iteration, while competing
methods require many. These two properties together make a very strong and compelling
case for the use of CCA in modern settings.
We now turn to comparing the CCA and G-IPF approaches in the sparse graph setting
(graphs with less than p5/3 edges). Though a general comparison of the computational
complexities of CCA and G-IPF is not available in this setting, important insights can
be obtained through a case-by-case analysis. Recall that a single iteration of the G-IPF
algorithm has complexity of the order (cid:80)p n3 +p|E |, while the single and only iteration
j=1(cid:101)j σ
of Algorithm 1 has computational complexity of the order (cid:80)p n3 + p + p(cid:12) (cid:12)ED \E (cid:12) (cid:12) for
j=1 j σ σ
adequately sparse graphs. Since the G-IPF algorithm does not use the filled graph GD,
(cid:12) (cid:12) σ
comparing |E | and (cid:12)ED \E (cid:12) (also n˜ and n ) is not straightforward and hence general
σ σ σ j j
results comparing these quantities are not available. However, we are able to compare these
quantities (and consequently the computational complexities of CCA vs. G-IPF, and those
of FIPF and IPSP) in some key examples in the next subsection.
3.3 Examples and illustrations
Example 1 (grid) An a × b grid is a common sparse non-decomposable graph with p =
(a + 1)(b + 1) vertices. For the default column-based ordering σ (depicted in Figure 1
(middle) with a = b = 3) the filled graph GD can be obtained by adding a NE-SW diagonal
σ
in each of the ab unit squares in the grid. The RCM ordering (Figure 1, right) again leads to
the same NE-SW diagonals as fill-in edges. For the default row-based ordering σ (depicted
in Figure 2 (left) with a = b = 3) the filled graph GD can be obtained by adding a NW-
σ
SE diagonal in each of the ab unit squares in the grid. It is important to note that the
orientation of these additional diagonals depends on the imposed ordering σ. For example,
if we add diagonals with a NW-SE orientation, then the resulting graph does not admit
the column-based ordering in Figure 1 (middle) as a perfect vertex elimination scheme. In
general, adding diagonals with different orientations in each unit square of the grid may
not lead to a decomposable graph (see Figure 2 (right) for an example). An example where
addingdiagonalsofdifferentorientationsleadstoadecomposablegraphisprovidedinFigure
2 (middle), and a corresponding perfect vertex elimination scheme is also provided.
Let σ be the RCM ordering depicted in Figure 1 (right). In this case, straightforward
141 5 9 13 1 3 6 10
6 10 5 9
2 14 2 13
7 11 8 12
15 15
3 4
4 8 12 16 7 11 14 16
Figure 1: (left) A 3 × 3 grid with 16 vertices. (middle) Filled graph for column-based
ordering of vertices with added diagonals in NE-SW orientation. (right) Filled graph for
RCM ordering.
13 14 15 16 11 13 15 12
10 11 14 16
9 12 10 9
6 7 6
5 8 8 7 5
1 2 3 4 4 3 2 1
Figure 2: (left) Filled graph for row-based ordering of vertices with added diagonals in NW-
SE orientation. (middle) An example when adding diagonals of different orientations leads
to a decomposable graph, and the corresponding perfect vertex elimination ordering. (right)
An example when adding diagonals of different orientations does not lead to a decomposable
graph, as the induced subgraph on the highlighted vertices is a 4-cycle.
151 3 5 7 1 3 5 7
2 4 6 8 2 4 6 8
Figure 3: An 8-cycle with RCM ordering of vertices (left), corresponding filled graph (right).
computations show that
(cid:12) (cid:12)
(cid:12)ED \E (cid:12) = ab < 2ab+a+b = |E |
σ σ σ
and
p p
(cid:88) (cid:88)
n3 = 27ab+a−11b < 64ab−10a−10b−12 = n˜3.
j j
j=1 j=1
In this case the CCA algorithm and a single iteration of the G-IPF algorithm both have
complexity O(p2). Also, the underlying graph has O(p) maximal cliques of size 2, and it
follows from Table 1 that both FIPF and IPSP have computational complexity O(p3) in this
case.
Withtheabovecomparisonsinmind, inmodernenvironmentalandearthscienceapplica-
tions, thereisacompellingneedtomodelcovariatesonaspatialgridandtheircorresponding
dependencies in a sparse manner. In particular, the sparse modeling of partial correlations
(and conditional independences) on a grid has been shown to be highly effective in modern
climate and paleo-climate applications. In such applications the dimension can be much
larger than the available sample size (see Guillot et al. [2015]). Here the goal is to not just
estimate a sparse Markov Random Field, but it is critically important to also quantify the
uncertainty inherent to such estimation, through resampling or other methods (see Guillot
et al. [2015]). When estimation is not in closed form, quantifying uncertainty through multi-
ple bootstrap estimations of the inverse covariance matrix can be extremely computationally
prohibitive. The grid example above thus demonstrates direct applicability of the proposed
CCA method in a contemporary application domain.
Example 2 (cycle). The p-cycle is another standard example of a sparse non-decomposable
(cid:12) (cid:12)
graph. A simple induction argument shows that (cid:12)ED \E (cid:12) = p−3 < p = |E | if σ is chosen
σ σ σ
to be the RCM fill reducing ordering of the vertices (see Figure 3). In this case, n˜ = n = 2
j j
for every 1 ≤ j ≤ p − 1, n˜ = 2 and n = 0. Hence, (cid:80)p n3 < (cid:80)p n˜3. Hence, both
p p j=1 j j=1 j
the CCA and one iteration of G-IPF have complexity O(p2) in this case. When considering
the competing methods of FIPF and IPSP, note that since a p-cycle has p maximal cliques
of size 2, it follows from Table 1 that both these methods have per iteration computational
complexity O(p3) in this case. The IHT algorithm in Xu et al. [2011] has a complexity of
O(p) (per iteration) in this case. However, as mentioned in Xu et al. [2015], this algorithm
can be prohibitively memory intensive in general.
Example 3 (almost complete graph). The largest/densest non-decomposable graph with p
verticescanbeobtainedbytakingacompletegraph(withσ beingtheidentitypermutation),
16and removing the edges (p,p−2) and (p−1,p−3). Then the induced subgraph on the subset
of vertices {p,p−1,p−2,p−3} is a 4-cycle, which implies E is not a decomposable graph.
σ
In this case, the filled graph ED can be obtained by adding the edge (p,p−2) to E . In this
σ σ
dense case, the computational complexity of CCA, based on the discussion corresponding to
eq. (5), is O(p3). On the other hand for G-IPF,
p p−1
(cid:88) (cid:88) (p−1)2p2
n 3 = k3 −(33 −23) = −19,
j
4
j=1 k=1
whichimpliesthatthecomputationalcomplexityofoneiterationofG-IPFisO(p4): anentire
orderofmagnitudelarger. WhenconsideringFIPFandIPSP,notethattheunderlyinggraph
has four maximal cliques of size p−2. Hence, the computational complexity of both FIPF
and IPSP (per iteration) is O(p3) by Table 1.
Example 4 (Bipartite graph). Consider a bipartite graph setting, where V = A ⊎ B with
|A| = m and |B| = p−m, and (u,v) ∈ E if and only if u ∈ A,v ∈ B or u ∈ B,v ∈ A. We
assume without loss of generality that m ≤ p/2. For this non-decomposable graph, consider
any vertex ordering σ which assigns labels {p,p−1,··· ,p−m+1} to vertices in A, and the
remaining labels to vertices in B. Such an ordering can be converted to an RCM ordering
with just one change to the vertex labels, but we consider the above partition based ordering
for simplicity of exposition. In this case, GD can be obtained by adding edges so that each
σ
vertex in A shares an edge with all other vertices in A. In particular, |ED \E | =
(cid:0)m(cid:1)
. It
σ σ 2
follows that n = m for 1 ≤ j ≤ p − m, and n = p − j for j > m. On the other hand
j j
n˜ = p − m for 1 ≤ j ≤ m and n˜ = m for j > m. Consider the ‘sparse’ case, when
j j
m = o(p2/3) and LˆD is computed using (2). In this case, note that
p m
(cid:88) (cid:88)
p+ n3 +p|ED \E | = (p−m)m3 + j3 +p+pm2 ≤ Cpm3
j σ σ
j=1 j=1
Since p/2 < p − m < p, it follows that the computational complexity of CCA is O(pm3).
Similarly, note that
p
(cid:88)
n˜3 +p|E | = (p−m)m3 +m(p−m)3 +pm(p−m)
j σ
j=1
whichimpliesthatthecomputationalcomplexityofG-IPF(periteration)isO(mp3). Nowlet
us consider the other competing methods FIPF and IPSP. Since there are m(p−m) maximal
cliques, each of size 2, it follows that the computational complexity of FIPF and IPSP is
O(mp3) as well. Since m = o(p2/3), we conclude that the computational complexity of the
entire CCA algorithm is an order of magnitude smaller than the per iteration complexity of
the other three algorithms. In the ‘dense’ case where m is at least p2/3, and LˆD is computed
using the usual Cholesky factorization approach, the complexity of CCA is O(p3), while the
complexity for the competing methods is at least O(p3+2/3).
Example 5 (Multipartite graph - exponential number of cliques). Consider a setting, where
p = 3m, V = ⊎m A with each A having 3 vertices, and (u,v) ∈ E if and only if u ∈ A ,v ∈
i=1 i i k
A for k ̸= k′. Consider any ordering σ which assigns labels {3i−2,3i−1,3i} to vertices
k′
17Non-decomp. graph Sparsity CCA G-IPF FIPF IPSP
Grid Sparse O(p2) T ×O(p2) T ×O(p3) T ×O(p3)
G F I
Cycle Sparse O(p2) T ×O(p2) T ×O(p3) T ×O(p3)
G F I
Bipartite Sparse O(pm3) T ×O(mp3) T ×O(pm3) T ×O(pm3)
G F I
(m,p−m) partition Dense O(p3) T ×Ω(p11/3) T ×Ω(p11/3) T ×Ω(p11/3)
G F I
Almost complete Dense O(p3) T ×O(p4) T ×O(p3) T ×O(p3)
G F I
Multipartite Dense O(p3) T ×O(p4) T ×eO(p) T ×eO(p)
G F I
exponential # cliques
Table 2: Computational complexity comparison of CCA with other competing methods
for some key non-decomposable graphs. For iterative methods, the complexity of a single
iteration is multiplied with the total number of iterations (T for G-IPF, T for FIPF, T
G F I
for IPSP). The lowest computational complexity for each row is highlighted in bold.
in A for 1 ≤ i ≤ m. This is a dense graph with
(cid:0)p(cid:1)
− p edges, and the computational
i 2
complexity of CCA is O(p3). Since n˜ = p−2 for every j, it follows that the computational
j
complexity of G-IPF (per iteration) is O(p4). With regards to the other two methods FIPF
and IPSP, thereare an exponentialnumber ofmaximal cliques (3p/3 in particular), and hence
the computational complexity of these methods (per iteration) is exponential in p.
For comparison purposes, the results in the examples above are summarized in Table 2.
The analysis above facilitates a detailed understanding of the properties of the proposed
CCA method. In particular, the computational advantages of the proposed CCA algorithm
stem from three properties: (a) G-IPF , FIPF and IPSP have to repeat their iterations T
times, while no such repetition is necessary for the CCA method, (b) FIPF and IPSP (not
G-IPF though) require an enumeration of the maximal cliques of the graph G, which can
be expensive, especially for moderately sparse or dense graphs, and (c) in a large majority
of settings, even a single iteration of the CCA algorithm is in general computationally more
efficientthanevenasingleiterationoftheG-IPFandotheralgorithms. Together, thesethree
properties contribute to the significant computational advantage of the CCA approach.
4 High-dimensional asymptotic properties
In this section we examine the large sample properties of the proposed estimators in a
high-dimensional setting. Consider once more the setting in Section 3. We will allow the
number of variables p = p to increase with the sample size n. We consider a true data
n
generating model under which i.i.d. observations Y ,··· ,Y ∈ Rpn are generated from
n,1 n,n
a distribution with mean 0 and precision matrix Ω¯ ∈ P+ for every n ≥ 1. In other words,
¯
n pn
¯
{Ω } denote the sequence of true inverse covariance matrices. Let {L } denote the
n n≥1 n n≥1
¯ ¯
corresponding sequence of Cholesky factors of Ω , and let {Σ } denote the corresponding
n n n≥1
¯
sequence of true covariance matrices. The graph G = G encodes the sparsity pattern in Ω ,
n n
σ = σ denotes a given fill-reducing ordering for G, and GD denotes the corresponding filled
n σ
¯
graph. Let P denote the probability measure underlying the true data generating model
described above.
We now introduce some quantities related to G and GD which will be needed for speci-
σ σ
fying the convergence rate of the CCA based estimators. Let aD denote the product of the
n
18¯
maximum number of non-zero entries in any column of L and the maximum number of
n
¯
non-zero entries in any row of L . In particular
n
(cid:18) (cid:19) (cid:18) (cid:19)
aD = 1+ max |{u > j : (u,j) ∈ ED}| × 1+ max |{u < i : (i,u) ∈ ED}| .
n σ σ
1≤j≤p−1 2≤i≤p
The quantity a˜D is similarly defined below, although focusing exclusively on the fill-in entries
n
only as opposed to the entire filled graph. In particular
(cid:18) (cid:19) (cid:18) (cid:19)
a˜D = max |{u > j : (u,j) ∈ ED \E }| × max |{u < i : (i,u) ∈ ED \E }| .
n σ σ σ σ
1≤j≤p−1 2≤i≤p
Let c = |ED \E | be the number of fill-in entries. These fill-in entries can be ordered based
σ σ
on the traversal path described in Step II of the CCA algorithm. For every 1 ≤ r ≤ c, let
FD denote the collection of previous fill-in entries which contribute to the computation of
r
the rth fill-in entry in Algorithm 1, see RHS of (3). In particular if the rth fill-in entry is
(i,j), then
(cid:8) (cid:9)
FD = r˜: 1 ≤ r˜≤ r,∃k < j < i such that r˜th fill-in entry is (j,k) with (i,k) ∈ ED
r σ
(cid:8) (cid:9)
⊎ r˜: 1 ≤ r˜≤ r,∃k < j < i such that r˜th fill-in entry is (i,k) with (j,k) ∈ ED .
σ
We now state two of the three assumptions that are needed for our asymptotic results. Both
of these assumptions are quite standard in the high-dimensional asymptotic literature.
• (A1 - Bounded Eigenvalues) There exists δ > 0 (not depending on n) such that 0 <
δ ≤ λ (Ω¯ ) ≤ λ (Ω¯ ) ≤ δ−1 < ∞.
min max
• (A2 - Sub Gaussianity) The random vectors Y ,...,Y are sub-Gaussian i.e., there
n,1 n,n
(cid:104) (cid:105)
exists a constant c > 0 such that for every x ∈ Rp, E ex′Yi ≤ ecx′x. Along with
Assumption A1, this in particular implies that the sub-Gaussian norm of αTYi, for
any α with αTα = 1, is uniformly bounded by a constant κ.
AssumptionA1statesthattheeigenvaluesofthesequenceoftruecovariancematricesareuni-
formly bounded. As mentioned above, these are standard assumptions in high-dimensional
covariance asymptotics, see Bickel and Levina [2008a], Rothman et al. [2008], Peng et al.
[2009], Xiang et al. [2015]. Before stating the third required assumption, we define a function
gD which maps {1,2,··· ,c} to R recursively as follows.
+
6 3 (cid:88)
gD(1) = and gD(r) = gD(r˜)
δ δ
r˜∈FD
r
for 2 ≤ r ≤ c. This function is critical in capturing the propagation of errors while mod-
ifying the fill-in entries to compute the CCA Cholesky estimate
Lˆ
from
LˆD.
The third
required assumption stipulates that p increases with n at an appropriate sub-exponential
rate depending on the graph based quantities defined above.
• (A3 - Growth rate for p ) sCCAlogp → 0 as n → ∞, where
n n
(cid:18) (cid:19)2
(cid:0) (cid:1)
s = min aD,|ED|+1 a˜D 1+ max gD(r) .
CCA n σ n
1≤r≤c
19With the required assumptions and notation in hand, we establish our consistency result for
CCA. Part (a) of the result establishes bounds for the intermediate estimator
LˆD,
and part
ˆ ˆ
(b) established bounds for the final CCA estimators L and Ω.
Theorem 1 (Convergence rates for CCA) Suppose that assumptions A1-A3 are satisfied.
Then the following results hold.
(a) There exists a constant K∗ such that
(cid:32) (cid:114) (cid:33)
(cid:13) (cid:13) min(aD,|ED|+1)logp
P¯ (cid:13)L(cid:98)D −L¯(cid:13) ≤ K∗ n σ → 1 as n → ∞.
(cid:13) (cid:13) n
2
(b) There exist constants K and K′ such that
(cid:32)(cid:40) (cid:114) (cid:41) (cid:40) (cid:114) (cid:41)(cid:33)
(cid:13) (cid:13) s logp (cid:13) (cid:13) s logp
P¯ (cid:13)L(cid:98)−L¯(cid:13) ≤ K CCA ∩ (cid:13)Ω(cid:98) −Ω¯(cid:13) ≤ K′ CCA → 1 as n → ∞.
(cid:13) (cid:13) n (cid:13) (cid:13) n
2 2
Like IPF, our work/model assumes that the graph G is known. When it is not known, it
can be constructed using thresholding methods for sparse inverse covariance estimates as in
Hero and Rajaratnam [2012] or Zhang et al. [2021].
Remark 3 (Weak convergence in the fixed p setting) When p does not vary with n, distribu-
√
¯
tional convergence results which show that Ω(cid:98) is n-consistent for Ω can be established under
mild moment assumptions. In particular, assume that the underlying distribution F of the
i.i.d. observations Y ,Y ,··· ,Y satisfies the assumption (cid:82) (cid:81)p |x |aidF(x) < ∞, where
1 2 n Rp i=1 i
a ,a ,··· ,a are non-negative integers which satisfy
(cid:80)p
a = 4. Under this assumption,
1 2 p √ i=1 i
¯
it can be shown that n(Ω(cid:98) −Ω) converges to a multivariate Gaussian limit as n → ∞ (see
Supplemental Section B).
4.1 Comparison with operator norm convergence rate for graph-
ical Gaussian MLE
High-dimensional convergence rates for the graphical Gaussian MLE (solution of (6)) are not
directly available in the current literature. However, minor modifications to the argument in
(cid:113)
[Rothmanetal.,2008, Theorem1]leadtoaFrobeniusnormconvergencerateof (1+|Eσ|)logp
n
for the graphical Gaussian MLE, under Assumptions A1-A2 (and a modified Assumption
(cid:113)
A3 which states that (1+|Eσ|)logp → 0 as n → ∞). Since the Frobenius norm dominates
n
(cid:113)
the operator norm, the above rate of (1+|Eσ|)logp also applies to the operator norm. To
n
our knowledge, this is the best available operator norm convergence rate for the constrained
Gaussian MLE (with a general graph G). Hence, the comparison between the available
operator norm convergence rates for the CCA and the graphical Gaussian MLE reduces to
the comparison between the order of the quantities s and |E |.
CCA σ
Note that s is a product of two factors: min(aD,|ED|+1) (corresponding to Step I of
CCA n σ
Algorithm 1) and
a˜D(cid:0)
1+max
gD(r)(cid:1)2
(corresponding to Step II of Algorithm 1). We
n 1≤r≤c
are able to obtain the term min(aD,|ED|+1) (instead of just |ED|+1) in the operator norm
n σ σ
20bounds because the intermediate estimators
LˆD
and
ΩˆD
are available in closed form. While
|ED| > |E |forallorderedgraphs, inmanysettingsaD, theproductofthemaximumnumber
σ σ n
¯
of non-zero entries in any row of L and the maximum number of non-zero entries in any
¯
column of L, is of a lower order than |E |. Hence, if the multiplicative factor corresponding
σ
to Step II does not grow too fast, s can be of a lower order than |E |. To summarize,
CCA σ
key properties of the CCA approach – (i) the availability of the intermediate estimator in
closed form, and (ii) the final estimator requiring a finite number of tractable algebraic
changes to the intermediate estimator – together allow us to directly analyze the operator
norm of the estimation error and establish corresponding rates. Importantly, we have a high-
dimensionalsettingwherebettercomputationalpropertiesoftheCCAapproachtranslateto,
and are directly relevant for, establishing better statistical properties. In this sense, these
two properties are complementary. Note that the graphical Gaussian MLE however does
not readily allow convergence in the operator norm, forcing existing analyses to target the
Frobenius norm of the estimation error. The corresponding rates for the Frobenius norm are
then borrowed for the operator norm, leading to looser bounds in high-dimensional settings.
While a general comparison of s and E is not available, we undertake a comparison
CCA σ
in some of the key examples considered at the end of Section 3.2. It turns out that in most
of these examples, s is of a smaller (or same) order as |E |. We now consider the five
CCA σ
classes of graphs from Section 3.3, and compare the available statistical accuracy rates of
the CCA algorithm with that of the graphical Gaussian MLE for each of these classes.
Example 1 (grid) contd. Considerana×bgridwithp = (a+1)(b+1)vertices. Ifσ istheRCM
fill reducing ordering (see Figure 1, right), then the maximum number of non-zero entries in
any row or column of L¯ is 4, which implies that min(aD,|ED|) = O(1) << 2ab+a+b = |E |.
n σ σ
In this case FD = ∅ for every 1 ≤ r ≤ c, which implies that the second multiplicative factor
r
for s is O(1). Hence s = O(1) is an order of magnitude smaller than |E | = O(p).
CCA CCA σ
Example 2 (cycle) contd. If σ is the RCM fill reducing ordering (see Figure 3), it then
¯
follows that the maximum number of non-zero entries in any row or column of L is bounded
by 4, hence aD = O(1), and min(aD,|ED|) = O(1) << p = |E |. However, the second
n n σ σ
multiplicative factor for s turns out to be exponential in p. In this case, c = p − 3,
CCA
and straightforward calculations show that max gD(r) = gD(c) = O((3δ−1)p−4) (see
1≤r≤c
Assumption A1 above). Hence, overall |E | is of a much smaller order than s . We
σ CCA
show however that through more targeted arguments specific to the p-cycle setting, and an
¯ (cid:112)
additional constraint on L, a significantly better convergence rate than s logp/n can
CCA
be obtained for the CCA estimator. In particular, for proving such a result, Assumption A3
above needs to be replaced by the following assumption.
• (A3′) The quantity max |L¯ j,j−2| is uniformly (in n) bounded away from 1, and
3≤j≤p−1 L¯
jj
|Eσ|logp → 0 as n → ∞.
n
The first part of Assumption A3′ essentially constrains the non fill-in entry in most rows of
L to be smaller in magnitude than the corresponding diagonal entry. The following lemma
0
now establishes a Frobenius norm convergence rate for the CCA estimator that matches the
Frobenius norm convergence rate for the grahical Gaussian MLE. The proof of this lemma
is provided in the Supplemental material at the end of the paper.
Lemma 7 (Improved CCA bounds for p-cycle) Consider the setting where G = (V,E ) is
σ σ
a p-cycle equipped with the RCM based fill reducing ordering illustrated in Figure 3. Then,
21under Assumptions A1, A2 and A3′, there exist constants K˜ and K˜′ such that the estimators
ˆ ˆ
L and Ω satisfy
(cid:32)(cid:40) (cid:114) (cid:41) (cid:40) (cid:114) (cid:41)(cid:33)
(cid:13) (cid:13) (|E |+1)logp (cid:13) (cid:13) |E |+1)logp
P¯ (cid:13)L(cid:98)−L¯(cid:13) ≤ K˜ σ ∩ (cid:13)Ω(cid:98) −Ω¯(cid:13) ≤ K˜′ ( σ → 1
(cid:13) (cid:13) n (cid:13) (cid:13) n
F F
as n → ∞.
Example 3 (almost complete graph) contd. In this case both min(aD,|ED|) and |E | are of
n σ σ
order p2, and since there is only fill-in entry (c = 1), it follows that the second multiplicative
factor for s is O(1). Hence, in this case both s and |E | are of order p2.
CCA CCA σ
Example 4 (Bipartite graph) contd. Consider a bipartite graph with sets A and B forming a
partition of the vertex set V, such that |A| = m and |B| = p−m with m = O(1). Consider
the ordering σ as defined in Section 3.2 for this setting. In this case both min(aD,|ED|) and
n σ
|E | are of order p. Again, since c =
(cid:0)m(cid:1)
= O(1), it follows that the second multiplicative
σ 2
factor for s is O(1). Hence, in this case both s and |E | are of order p.
CCA CCA σ
Example 5 (Multipartite graph) contd. In this case, both min(aD,|ED|) and |E | are of order
n σ σ
p2. Since fill-in edges are only allowed within each 3-element partition sets A , it can be
i
shown that max gD(r) and a˜D are O(1). To conclude, s and |E | are both order p2.
1≤r≤c n CCA σ
In summary, the examples above demonstrate that excpet in settings when the underlying
graphs are highly sparse (such as cycles), the proposed CCA approach has demonstrably
better statistical performance than the MLE (in terms of best available operator norm con-
vergence rate) for moderately sparse and dense graphs. This combined with its superior
computational complexity suggests that in modern high-dimensional settings, the CCA ap-
proach should be the preferred method. The only exceptions are cases when the underlying
graph is a cycle or a graph close to a cycle.
5 Numerical Illustrations and real data applications
5.1 Simulation experiment: evaluation of CCA when G is given
In this section, we evaluate the high-dimensional empirical performance of the CCA algo-
rithm in a simulation setting where the underlying graph G is assumed to be given/known.
Against the backdrop of the results on computational complexity in Section 3.2 (Lemma 5,
Lemma 6 and Table 1) we compare the performance of the CCA algorithm and the G-IPF
algorithm. For each value of p ∈ {500,1000,1500,2000,5000}, the following procedure is
repeated: The true Ω has a random sparsity pattern of approximately 2p edges. To generate
Ω = LtLforagivenp, weinitallygenerateL, alowertriangularmatrix, inthefollowingman-
ner: Randomly select some entires of L to be zero (approximately less than 2p). Amongst
these we randomly set half of them to be positive and the other half to be negative. In
addition, the off-diagonal entries of L are drawn from the uniform distribution over [0.3,0.7].
Finally, we choose the diagonal entries from the uniform distribution over [2,5]. The sam-
ple size takes values as follows: n = {0.5p,0.75p,p,2p}. We generate 50 datasets for each
of these settings from both the multivariate Gaussian and the heavy-tailed multivariate-T
distribution with 3 degrees of freedom.
22For the Gaussian case, both the CCA algorithm and the G-IPF algorithm are then used
to obtain estimates of Ω. We also present results from a hybrid approach, which uses the
CCA estimate as the initial estimate in G-IPF as opposed to the diagonal matrix with
diagonal entries {1/S }p . The results presented below are the average relative Frobenius
ii i=1
∥Ωˆ−Ω∥
norm estimation errors ( ) over the 50 datasets generated for each sample size and
∥Ω∥
underlying distribution. The results of the simulations for the multivariate Gaussian case
are provided in Table 3. CCA produces significantly better wall-times compared to G-IPF
without compromising on accuracy.
Forthemultivariate-T datasets, wecomparetheperformanceoftheCCAalgorithmtoG-
IPF, and the tlasso-IPF algorithm from [Feingold and Drton, 2011, Section 3.2]. The results
are provided in Table 4. The results show that the scalability of the CCA algorithm is even
more pronounced compared to G-IPF in the heavy-tailed T-distribution case. Note that the
tlasso-IPF uses the EM algorithm to obtain maximum likelihood estimates of concentration
matrices (with a given sparsity pattern) assuming that the underlying data is generated
from a multivariate-t distribution. It is a double iterative algorithm, as the M-step in each
iteration of the EM algorithm involves the iterative IPF algorithm. The maximum number
of iterations for the EM algorithm was set to 100, and for the IPF algorithm was set to 1000.
We report an “N/A” for estimation error values of the iterations for a given setting which
does not finish in 4 days. For p = 2000 and 5000, this is the case for the t-lasso algorithm.
For lower values of p, the time taken by t-lasso is anywhere between 500 to 2000 times
more as compared to the CCA algorithm. The estimation error for the resulting estimates
is still very high, which suggests that more iterations are needed for convergence of the EM
algorithm. To summarize, the results in Tables 3 and 4 demonstrate how state-of-the-art
iterative algorithms can be prohibitively expensive in high-dimensional settings, and provide
compelling reasons to use the proposed non-iterative CCA algorithm for fast and accurate
estimation in modern high-dimensional applications.
5.2 Simulation experiment: evaluation of CCA when G is not
given
Inpractice, thegraphGisnotgiven/known. Inthissetting, agraphselectionstepisrequired
prior to CCA, and we use the FST method [Zhang et al., 2021] for this purpose. FST is a
non-iterative thresholding method (extending ideas in Rothman et al. [2009]) that is scalable
to ultra high-dimensional settings, and is also theoretically supported by high-dimensional
statistical consistency results (see Section 5 of Zhang et al. [2021] for details). However, the
sparse estimate of Ω provided by FST is not guaranteed to be positive definite. Setting G as
the sparsity pattern in Ω estimated by FST, and then applying the proposed CCA method
in Section 3 yields a positive definite estimate of Ω with exactly the same sparsity pattern as
the FST estimate of Ω. We refer to this approach as FST+CCA. The FST based estimate
of G can have several connected components, in which case the CCA method is separately
applied on each component. Doing so enables us to use techniques such as sparse matrix
representation and parallel computation to gain efficiency.
WecomparetheFST+CCAmethodtotwoleadingmethods: GlassoandBigQUIC.These
two are state-of-the-art penalized likelihood methods for simultaneous sparsity selection
and positive definite estimation, and BigQUIC in particular is scalable to the ultra high-
dimensional p = 105 setting considered in this section. Implementations of both these
23CCA G-IPF CCA-GIPF Improvement Factor
p n Time Norm Time Norm Time Norm (G-IPF/CCA Time)
500 250 5.2 0.1005 28.6 0.1014 10.1 0.1009 5.5
500 375 5.3 0.0817 28.7 0.0822 10.1 0.0819 5.4
500 500 5.3 0.0700 27.9 0.0703 9.4 0.0701 5.3
500 1000 5.3 0.0492 24.2 0.0493 8.6 0.0493 4.6
1000 500 34.8 0.0762 536.3 0.0765 340.2 0.0762 15.4
1000 750 34.8 0.0626 516.5 0.0627 332.8 0.0625 14.8
1000 1000 34.5 0.0536 513.2 0.0537 333.5 0.0536 14.9
1000 2000 35.3 0.0376 495.1 0.0377 306.8 0.0376 14.0
1500 750 112.7 0.0648 2661.3 0.0643 1885.0 0.0641 23.6
1500 1125 112.9 0.0529 2622.1 0.0525 1841.2 0.0524 23.1
1500 1500 113.0 0.0457 2580.0 0.0453 1809.0 0.0452 22.8
1500 3000 116.0 0.0322 2471.5 0.0318 1737.1 0.0319 21.3
2000 1000 289.8 0.0555 8933.6 0.0551 5179.3 0.0549 30.8
2000 1500 298.9 0.0454 8844.7 0.0450 5112.5 0.0449 29.5
2000 2000 299.5 0.0391 8740.4 0.0387 5016.7 0.0387 29.2
2000 4000 309.3 0.0276 8590.3 0.0273 4870.1 0.0273 27.8
5000 2500 4911.3 0.0320 120252.4 0.0321 33776.3 0.0320 24.5
5000 3750 4945.8 0.0261 113356.6 0.0261 28047.8 0.0261 22.9
5000 5000 5034.5 0.0226 114867.7 0.0226 29037.7 0.0226 22.8
5000 10000 5156.2 0.0160 114133.3 0.0160 29248.7 0.0160 22.1
Table 3: Running time and estimation error (in relative Frobenius norm) for CCA, G-IPF,
and CCA-GIPF (use G-IPF with the CCA estimator as the initial value) with normal data.
All methods have very similar estimation errors, but CCA is significantly faster. The ratio
of running times of G-IPF to CCA is provided in bold.
CCA G-IPF tlasso-IPF Improvement Factor
p n Time Norm Time Norm Time Norm (G-IPF /CCA Time)
500 250 4.0 0.369 54.41 0.372 1933.06 6.862 13.6
500 375 4.0 0.280 36.26 0.282 1792.392 6.817 9.1
500 500 3.9 0.238 37.52 0.239 1772.345 6.779 9.6
500 1000 3.9 0.232 37.27 0.232 1891.426 6.753 9.6
1000 500 27.0 0.322 1548.9 0.323 36508.994 8.878 57.4
1000 750 26.9 0.217 719.9 0.218 35761.71 8.846 26.8
1000 1000 27.9 0.208 650.1 0.208 35364.331 8.838 23.3
1000 2000 28.8 0.170 677.4 0.170 34951.69 8.802 23.5
1500 750 95.193 0.258 6807.133 0.258 185566.52 10.231 71.5
1500 1125 95.517 0.190 3559.467 0.189 185772.6 10.195 37.7
1500 1500 92.459 0.196 3846.347 0.195 180279.2 10.182 41.6
1500 3000 94.754 0.153 3392.462 0.152 174894.6 10.166 35.8
2000 1000 234.100 0.228 12738.264 0.219 >4 days NA 54.4
2000 1500 263.570 0.200 17389.751 0.200 >4 days NA 66.0
2000 2000 308.789 0.172 10757.530 0.172 >4 days NA 34.8
2000 4000 356.389 0.143 11920.13 0.142 >4 days NA 33.4
5000 2500 5019.258 0.165 163465.7 0.139 >4 days NA 32.6
5000 3750 4817.014 0.118 155700.3 0.118 >4 days NA 32.3
5000 5000 5051.542 0.140 159974.9 0.131 >4 days NA 31.7
5000 10000 4594.165 0.097 139444.2 0.097 >4 days NA 30.4
Table 4: Running time and estimation error (in relative Frobenius norm) for CCA, G-IPF ,
and tlasso-IPF with t data. G-IPF and CCA again have very similar estimation errors, but
CCA is significantly faster. The ratio of running times of G-IPF to CCA is provided in bold,
as tlasso-IPF values are not available for some of the settings.
24FST+CCA Glasso Improvement Factor
p n Time Norm Time Norm Glasso
RBM FST+CCA
1000 500 2.90 secs 0.206 1.44 mins 0.225 29.79 times
2000 1000 21.01 secs 0.124 14.74 mins 0.177 42.09 times
FST+CCA Glasso Improvement Factor
p n Time Norm Time Norm Glasso
RBL FST+CCA
1000 500 1.25 secs 0.096 2.27 mins 0.241 108.96 times
2000 1000 31.18 secs 0.059 18.19 mins 0.201 35.00 times
Table 5: Running time and estimation error (in relative Frobenius norm) for FST+CCA
and Glasso, in both RBM and RBL designs, using 1 core. FST+CCA is more accurate and
significantly faster than Glasso for both designs. The ratio of running times of Glasso to
FST+CCA is provided in bold.
methods are available in R. Simulations comparing the FST+CCA method with Glasso
consider the setting p = {1000,2000} with n = 0.5p, and with BigQUIC use p = {104,105},
withn = 1000. WhileGlassoisabletocomfortablyhandlesettingswithacoupleofthousand
variables, it can be really expensive to allocate enough memory for Glasso to scale to p >
104. Both the dense matrix representations used in the internal steps and the fact that
Glasso cannot be straightforwardly parallelized using multiple cores hinder its space and
time scalability to ultra-high dimensions. BigQUIC, on the other hand, is able to scale to
p >> 104 by employing several innovations which include performing block-wise updates and
the use of a fast approximation for the Hessian (see detailed discussion in Hsieh et al. [2013]).
As the algorithm is iterative in nature, a ‘bad’ partition of the variables during blocking can
hinder its performance. Here we perform the comparative study of the FST+CCA method in
a single core situation with Glasso when p is moderately large, and in a multi-core situation
with BigQuic with ultra large p.
For each choice of p, a true Ω is obtained by the random block model (RBM) procedure
and also by a random banded line graph (RBL) design procedure, which randomly generates
the sub-diagonal entries in the precision matrix that corresponds to a line graph. Both these
are described in detail in [Zhang et al., 2021, Section5]. Once this is done, we simulate
multivariate Gaussian data with the given true Ω as the precision matrix. FST is then
applied to this data to determine the graph structure and CCA is applied in parallel to each
connected component resulting from FST to obtain the final positive definite estimate of
Ω. Tuning parameters for FST, Glasso and BigQUIC are determined using five fold cross
validation. A total of 50 datasets are generated for each case and the average errors in
relative Frobenius norm and average running time are provided in Table 5, 6, and 7.
In Table 5, we provide results for comparison of FST+CCA with Glasso in the p ∈
{1000,2000} setting. The Glasso implementation available in R cannot use parallel process-
ing via multiple cores. While FST+CCA has this parallelizable property, for comparison
purposes we employ only a single core for all the experiments in Table 5. Even without the
benefit of parallel processing, FST+CCA is at least 30 to 100 times faster than Glasso, and
performs competitively or better than Glasso in terms of accuracy in all settings.
Table 6 provides results for comparing FST+CCA with BigQuic in the p ∈ {1000,2000}
setting, using 16 cores. It shows that FST+CCA yields lower wall times and delivers an even
better advantage in accuracy. In Table 7, we provide results for comparison of FST+CCA
25FST+CCA BigQuic Time Improvement Factor
p n Time Norm Time Norm BigQuic
RBM FST+CCA
1000 500 0.77 secs 0.204 1.06 secs 0.286 1.38 times
2000 1000 2.47 secs 0.133 6.76 secs 0.284 2.73 times
FST+CCA BigQuic Time Improvement Factor
p n Time Norm Time Norm BigQuic
RBL FST+CCA
1000 500 0.43 secs 0.151 1.44 secs 0.303 3.35 times
2000 1000 2.67 secs 0.074 9.46 secs 0.312 3.54 times
Table 6: Running time and estimation error (in relative Frobenius norm) for FST+CCA and
BigQUIC, with p = 1000,2000, in both RBM and RBL designs, using 16 cores. FST+CCA
is more accurate and faster than BigQUIC for both designs. The ratio of running times of
BigQUIC to FST+CCA is provided in bold.
FST+CCA BigQUIC Time Improvement Factor
p n Time Norm Time Norm (BigQUIC/FST+CCA Time)
RBM
104 1000 13.39 secs 0.062 105.13 secs 0.288 7.85 times
105 1000 29.77 mins 0.144 183.61 mins 0.150 6.16 times
FST+CCA BigQUIC Time Improvement Factor
p n Time Norm Time Norm (BigQUIC/FST+CCA Time)
RBL
104 1000 16.65 secs 0.041 196.45 secs 0.301 11.80 times
105 1000 31.01 mins 0.087 151.70 mins 0.319 4.76 times
Table 7: Running time and estimation error (in relative Frobenius norm) for FST+CCA and
BigQUIC, with p = 104,105, in both RBM and RBL designs, using 16 cores. FST+CCA
is more accurate and faster than BigQUIC for both designs. The ratio of running times of
BigQUIC to FST+CCA is provided in bold.
with BigQUIC in the p ∈ {104,105} setting. As mentioned previously, the Glasso implemen-
tation in R can break down in these settings due to the use of dense matrix representations.
Both FST+CCA and BigQUIC however have the ability to use multiple cores for parallel
processing, and each method is provided with 16 cores for the experiments in Table 7. In
all settings, FST+CCA is at least 5 to 12 times faster than BigQUIC, and can also sub-
stantially improve accuracy compared to BigQUIC. To summarize, the simulations in this
section illustrate that the proposed FST+CCA can be significantly faster than state of the
art ultra high-dimensional iterative approaches, while providing better estimation accuracy.
The results above also underscore how CCA can be readily coupled with high-dimensional
sparsity selection methods.
5.3 Application to minimum variance portfolio rebalancing
5.3.1 S&P 500 data with 398 stocks when G is given (CCA vs. G-IPF)
The minimum variance portfolio selection problem is defined as follows. Given p risky assets,
let r denote the return of asset i over time period k (for i = 1,2,··· ,p); which in turn is
ik
defined as the change in its price over time period k, divided by the price at the beginning of
the period. Let rT = (r ,r ,··· ,r ) denote the vector of returns over time period k. Let
k 1k 2k pk
26wT = (w ,w ,...,w ) denote the vector of portfolio weights, i.e., w denotes the weight
k 1k 2k pk ik
of asset i in the portfolio for the k-th time period. A long position or a short position for
asset i during period k is given by the sign of w , i.e., w > 0 for long, and w < 0 for
ik ik ik
short positions respectively. The budget constraint can be written as 1Tw = 1, where 1
k
denotes the vector of all ones. Let Σ denote the covariance matrix of the vector of returns.
Note that the risk of a given portfolio, as measured by the standard deviation of its return,
is simply (wTΣ w )1/2. The minimum variance portfolio selection problem for investment
k k k
period k can now be formally defined as follows: min wTΣ w subject to 1Tw = 1. As
w k k k k k
the minimum variance portfolio problem is a simple quadratic program, it has an analytic
solution w∗ = (1TΣ−11)−1Σ−11. The most basic approach to the portfolio selection problem
k k k
often makes the unrealistic assumption that returns are stationary in time. A standard
approach to dealing with the non-stationarity in such financial time series is to use a periodic
rebalancingstrategy. Inparticular,atthebeginningofeachinvestmentperiodk = 1,2,...,K,
portfolio weights wT = (w ,w ,...,w ) are computed from the previous N days of
k 1k 2k pk est
observed returns (N is called the estimation horizon). These portfolio weights are then
est
held constant for the duration of each investment period. The process is repeated at the
start of the next investment period and is often referred to as “rebalancing”.
WenowconsidertheproblemofinvestinginthestocksthatfeatureintheS&P500index.
The S&P 500 is a composite blue chip index consisting of 500 stocks. A number of stocks
were removed in our analysis due to limited data span of the corresponding companies as
constituents of the S&P 500 index - we ultimately used 398 stocks. For illustration purposes,
rebalancing time points were chosen to be every 64 weeks starting from 2000/01/01 to
2016/12/31 (approximately 17 years). Start and end dates of each period are selected to be
calendar weeks, and need not coincide with a trading day. The total number of investment
periods is 14, and the number of trading days in each investment period is approximately
320 days. We shall compare the performance of two approaches, ones which use G-IPF and
CCA for estimating the covariance matrix at each rebalancing point. We consider various
choices of N , in particular, N ∈ 375,450,525 in our analysis. Note that once a choice
est est
for N is made, it is kept constant until the next investment period.
est
Note that for G-IPF and CCA a graph for the zeros in the concentration matrix is re-
quired. When N is less than 398, we threshold the generalized inverse for the sample
est
covariance, and when N is larger than 398, we threshold the inverse till we achieve a
est
sparsity of approximately 95%. The Normalized wealth growth is defined as the accumulated
wealth derived from the portfolio over the trading period when the initial budget is normal-
ized to one. This measure is also designed to take into account both transaction costs and
borrowing costs. Figure 4 shows that in terms of Normalized Wealth Growth, the G-IPF
and CCA based approaches perform very similarly and both perform better than the passive
S&P500 index approach (no rebalancing) when N = 375. Very similar results are obtained
est
for N = 450 and 525 (not illustrated here due to space considerations). Overall, the results
est
demonstrate that the CCA and G-IPF based strategies are very competitive with each other
in terms of financial performance. However, as demonstrated in Table 8, the CCA algorithm
is much faster in terms of computational time compared to the G-IPF approach. Since the
inverse covariance matrix has to be estimated at the start of each investment horizon, the
gain in computational speed can add up very quickly. The above results reinforce our earlier
observations in Section 5.1 that the CCA algorithm provides a significantly more scalable
alternative to the G-IPF algorithm, while providing similar statistical performance.
275
4
plot_group
3 CCA
IPF
SP500
2
N
est
1
Method 375 450 525
CCA 1.92 1.74 1.67
2005 2010 2015 G-IPF 3.77 4.77 5.31
Figure4: Normalizedwealthgrowthafterad- Table 8: Average running time (in seconds)
justing for transaction costs (0.5% of princi- for estimating the covariance matrix of re-
pal) and borrowing costs (interest rate of 8% turns with CCA and G-IPF for various val-
APR) with N = 375 for S&P 500 data. ues of N for S&P 500 data
est est
5.3.2 Kaggle data with 3630 stocks/ETFs when graph G is unknown
In this section, we consider the minimum variance portfolio problem when investing in a
much larger number of stocks. The dataset is obtained from an open source publication
on Kaggle, provided by Marjanovic [2017]. Specifically 3630 stocks and ETFs which have
a complete record from 2014-06-09 to 2017-06-07 (featuring 756 trading days) are selected.
Daily returns are calculated. Similar to the previous subsection, we use a rebalancing scheme
in this application. A significant difference to address here is that we have fewer number of
observations but a much larger number of variables in this dataset. Thus, it is reasonable
to choose a relative smaller rebalancing time and larger N . We consider the choice of
est
N = 500, and rebalancing time of 30 days. This will give us a portfolio to be used from
est
days 501 to 756, which correspond to the period 2016-06-02 to 2017-06-07. More specifically,
at beginning, the first 500 daily returns are used to construct a portfolio which will be used
in days 501 to 530. Then for days 531 to 560, data from days 31 to 530 will be used for an
updated portfolio, and so forth. This scheme gives us a total of 9 time-points for portfolio
rebalancing. Here we compare the performance of three approaches, which respectively use
FST+CCA, Glasso and BigQuic for estimating the precision matrix at each rebalancing
point.
For the selection of tuning hyper-parameters, we used the portfolio loss as the loss func-
tion and a validation set of size 10. Specifically, for 500 daily return observations, 490
observations were used to construct the portfolio for different tuning parameter values and
the last 10 days of portfolio returns were used to determine the best tuning parameter.
The average running time for all the three approaches are recorded in Table 5.3.2. From
Table 5.3.2, FST+CCAtakesabout 1/5of thetimerequiredbyBigQuic, andlessthan 1/200
of the time used by Glasso, demonstrating the efficiency of the FST+CCA approach.
The normalized wealth growth derived from the portfolios is illustrated in Figure 5, and
28Approaches FST+CCA BigQuic Glasso
Running Time (Seconds) 6.883 34.531 1497.781
Table 9: Average running time (in seconds) for estimating the covariance matrix of returns
with FST+CCA, Glasso and BigQuic, using data on 3630 stocks and ETFs from Marjanovic
[2017]
1.050
1.025
plot_group
bigquic
1.000 fstcca
glasso
0.975
0.950
Jul 2016 Oct 2016 Jan 2017 Apr 2017
Date
Figure 5: Normalized wealth growth after adjusting for transaction and borrowing costs with
N = 500 for stock/ETF data from Marjanovic [2017].
est
shows that the FST+CCA approach has better overall wealth growth. The daily returns of
these portfolios are presented in Figure 6, demonstrating that the FST+CCA approach is
able to provide a portfolio which is less volatile compared to the other approaches. Overall,
FST+CCA performs better than the competing methods in terms of both computational
efficiency and wealth growth of the associated portfolio.
290.03
0.02
0.01
plot_group
bigquic
fstcca
glasso
0.00
−0.01
−0.02
Jul 2016 Oct 2016 Jan 2017 Apr 2017
Date
Figure 6: Daily returns corresponding to portfolios associated with various covariance esti-
mation strategies for stock/ETF data from Marjanovic [2017].
SUPPLEMENTAL MATERIALS
Section A: Proof of Theorem 1
(cid:13) (cid:13) (cid:13) (cid:13)
(a) We first obtain high probability bounds for
(cid:13)LˆD −L¯(cid:13)
. While bounds for
(cid:13)ΩˆD −Ω¯(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
2 2
can be obtained by minor changes to the proof of [Rothman et al., 2008, Theorem 1], sub-
(cid:13) (cid:13)
stantial conceptual and algebraic modifications are needed to derive bounds for
(cid:13)LˆD −L¯(cid:13)
,
(cid:13) (cid:13)
2
as presented below.
Let
D¯
=
D¯
denote the diagonal matrix whose diagonal entries match those of
Σ¯
=
Ω¯−1.
n n n
Let
R¯
=
R¯
=
D¯−1/2Σ¯ D¯−1/2
denote the population correlation matrix, and let
U¯
=
D¯1/2L¯
n n
denote the Cholesky factor of
R¯−1.
At the sample level, let
Dˆ
denote the diagonal matrix
whose diagonal entries match those of S, and
Rˆ
=
Dˆ−1/2SDˆ−1/2.
Consider the estimator
UˆD
defined by
(cid:110) (cid:16) (cid:17) (cid:111)
UˆD
= argmin tr
UUTRˆ
−2log|U| . (S.1)
U∈L
GD
σ
Using results in Roverato [2002], it follows that
(Rˆ>j)−1Rˆ>
1
UˆD,>
= −
·j
and
UˆD
= . (S.2)
·j (cid:113) jj (cid:113)
Rˆ −(Rˆ>)T(Rˆ>j)−1Rˆ> Rˆ −(Rˆ>)T(Rˆ>j)−1Rˆ>
jj ·j ·j jj ·j ·j
It follows from Equation (1) of the main paper and (S.2) that L(cid:98)D = Dˆ−1/2UˆD. Using a
(cid:13) (cid:13)
strategy in Rothman et al. [2008], we will first analyze
(cid:13)UˆD −U¯(cid:13),
and leverage this analysis
(cid:13) (cid:13)
30
snruter
oiloftroP(cid:13) (cid:13)
to bound
(cid:13)LˆD −L¯(cid:13).
(cid:13) (cid:13)
Let L∗ denote the space of lower triangular matrices whose (i,j)th entry is 0 whenever
GD
σ
(i,j) ∈/ ED (diagonal entries not restricted to be positive). Consider the function
σ
(cid:16) (cid:17)
G(∆) =
tr((U¯ +∆)(U¯ +∆)TRˆ )−2log|U¯
+∆|−
tr((U¯ U¯TRˆ )−2log|U¯
|
p (cid:18) (cid:19)
(cid:16) (cid:17) (cid:16) (cid:17) (cid:88) ∆
= tr
∆TRˆ
∆ +2tr
∆TRˆ U¯
−2 log 1+
ii
. (S.3)
¯
U
ii
i=1
¯
It is implicitly assumed that ∆ > −U for every 1 ≤ i ≤ p so that G(∆) is well-defined.
ii ii
Note that G is a convex function, and is minimized at
∆ˆ
=
UˆD −U¯
. Hence,
G(∆ˆ
) ≤ 0. Let
(cid:112)
ϵ = (|ED|+1)logp/n. We will now show that there exists a constant M > 0 such that
n σ
(cid:110) (cid:111)
inf ∆ ∈ L∗ : ∥∆∥ = Mϵ > 0
GD F n
σ
ˆ ˆ ˆ
It would then follow that the minimizer ∆ satisfies ∥∆∥ ≤ ∥∆∥ ≤ Mϵ . With this goal
2 F n
in mind, fix arbitrarily ∆ ∈ L∗ such that ∥∆∥ = Mϵ . Using (S.3) and the inequality
GD F n
σ
log(1+x) ≤ x for x > −1, we get
p
(cid:16) (cid:17) (cid:16) (cid:17) (cid:88) ∆
G(∆) ≥ tr
∆TRˆ
∆ +2tr
∆TRˆ U¯
−2
ii
¯
U
ii
i=1
p
=
tr(cid:16) ∆TRˆ ∆(cid:17) +2tr(cid:16) ∆T(Rˆ −R¯ )U¯(cid:17) +2tr(cid:0) ∆TR¯ U¯(cid:1) −2(cid:88) ∆ ii
¯
U
ii
i=1
p
=
tr(cid:16) ∆TRˆ ∆(cid:17) +2tr(cid:16) (Rˆ −R¯ )U¯ ∆T(cid:17) +2tr(cid:0) ∆T(U¯−1)T(cid:1) −2(cid:88) ∆ ii
(S.4)
¯
U
ii
i=1
Since ∆ ∈ L∗ , and U¯ is lower triangular, it follows that
GD
σ
p
2tr(cid:0) ∆T(U¯−1)T(cid:1) −2(cid:88) ∆ ii
= 0. (S.5)
¯
U
ii
i=1
Also, since σ is a perfect vertex elimination scheme for the decomposable graph GD and
σ
U¯ ∈ L , it follows that (U¯ ∆T) = 0 if (i,j) ∈/ ED. Also, by Assumptions A1 and A2, and
GD σ ij σ
˜
[Bickel and Levina, 2008b, Lemma 3], there exists a constant K such that
(cid:114) (cid:114)
logp logp
ˆ ¯ ˜ ¯ ˜
max |R −R | ≤ K and max |S −Σ | ≤ K (S.6)
ij ij ij ij
1≤i,j≤p n 1≤i,j≤p n
ˆ ¯
on an event C where P(C ) → 1 as n → ∞. Since the diagonal entries of R−R are zero,
n n
31it follows that on C
n
(cid:12) (cid:12)
(cid:12) (cid:12)
|tr(cid:16) (Rˆ −R¯ )U¯ ∆T(cid:17) | = (cid:12) (cid:12) (cid:88) (Rˆ −R¯ ) (U¯ ∆T) (cid:12) (cid:12)
ij ij
(cid:12) (cid:12)
(cid:12)(i,j)∈ED (cid:12)
σ
(cid:114)
≤
K˜
logp(cid:112)
|ED|∥∆U¯T∥
n σ F
(cid:114)
K˜ |ED|logp
≤ σ ∥∆∥
F
δ n
˜
K
≤ ∥∆∥2. (S.7)
Mδ F
Finally, restricting to C , using (∆∆T) = 0 for (i,j) ∈/ ED and Assumption A1, we get
n ij σ
(cid:16) (cid:17) (cid:16) (cid:17)
tr
∆TRˆ
∆ ≥
tr(cid:0) ∆TR¯ ∆(cid:1)
−|tr
∆T(Rˆ −R¯
)∆ |
(cid:16) (cid:17)
= tr(cid:0) ∆TR¯ ∆(cid:1) −|tr ∆T(Rˆ −R¯ )∆∆T |
(cid:12) (cid:12)
(cid:12) (cid:12)
≥ 1 ∥∆∥2 −(cid:12) (cid:12) (cid:88) (Rˆ −R¯ ) (∆∆T) (cid:12) (cid:12)
δ F (cid:12) ij ij (cid:12)
(cid:12)(i,j)∈ED (cid:12)
σ
(cid:114)
≥
1
∥∆∥2 −K˜
logp(cid:112)
|ED|∥∆∥2
δ F n σ F
1
≥ ∥∆∥2 (S.8)
2δ F
forlargeenoughn. Itfollowsby(S.4),(S.5),(S.7),(S.8)thatG(∆) > 1 ∥∆∥2 forM = 8K˜ /δ.
4δ F
Hence
(cid:32) (cid:33)
(cid:13) (cid:13) 8K˜
P
(cid:13)UˆD −U¯(cid:13)
≤ ϵ ≥ P(C ) (S.9)
(cid:13) (cid:13) δ n n
2
for large enough n. If we restrict to C , it follows from Assumption A1, (S.6) and (S.9) that
n
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)LˆD −L¯(cid:13)
=
(cid:13)Dˆ−1/2UˆD −D¯−1/2U¯(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
2 2
(cid:13) (cid:13) (cid:13) (cid:13)
≤
∥Dˆ
∥
(cid:13)UˆD −U¯(cid:13) +(cid:13)Dˆ−1/2 −D¯1/2(cid:13) ∥U¯
∥
2(cid:13) (cid:13) (cid:13) (cid:13) 2
2 2
(cid:114)
˜ ˜ ˜
16K 2K logp 18K
≤ ϵ + ≤ ϵ (S.10)
δ2 n δ3 n δ3 n
for large enough n. The above bound was obtained in terms of |ED|. Using the closed
σ
from representation of
LˆD
in Equation (1) of the main paper, we will now derive another
bound for ∥LˆD − L¯ ∥ in terms of aD = aD, where aD is the product of max n + 1
2 n n 1≤j≤pn j
and max r + 1. Here n = {i : 1 ≤ j < i ≤ p,(i,j) ∈ ED} and r = {j : 1 ≤ j <
1≤i≤pn i j σ i
i ≤ p,(i,j) ∈ ED}. For this purpose, we will first bound the ℓ -norm of the jth column of
σ 2
LˆD −L¯
for an arbitrary 1 ≤ j ≤ p. Using Equation (1) of the main paper, Assumption A1,
32Ω¯ ∈ P , and the triangle inequality, we get
GD
σ
∥LˆD −L¯ ∥ ≤ Lˆ (cid:13) (cid:13)(S>j)−1S> −(Σ¯>j)−1Σ¯>(cid:12) (cid:12)| +(cid:13) (cid:13)(Σ¯>j)−1Σ¯>(cid:13) (cid:13) |Lˆ −L¯ |. (S.11)
.j j 2 jj .j .j 2 .j 2 jj jj
To bound the above expression, it is key to bound ∥(S≥j)−1 − (Σ¯≥j)−1∥ . Here A≥j =
2
((A )) for any matrix A. For any u ∈ Rnj with ∥u∥ = 1, let U be an
kl k,l≥j,(k,j),(l,j)∈ED 2
(n + 1) × (n + 1) oσ rthogonal matrix with first row uT. Then US≥jUT and UΣ¯≥jUT are
j j
respectively the sample and population covariance matrix of UY≥j,UY≥j,··· ,UY≥j. Note
1 2 n
that the eigenvalues of UΣ¯≥jUT are still uniformly bounded above and below by δ−1 and
δ respectively, and uTS≥ju is the first diagonal entry of US≥jUT (same with uTΣ¯≥ju. It
follows by [Bickel and Levina, 2008b, Lemma 3] that
P
(cid:0)(cid:12) (cid:12)uTS≥ju−uTΣ¯≥ju(cid:12)
(cid:12) ≤
ν(cid:1)
≥ 1−c exp(−c nν2) whenever |ν| < κ,
1 2
where c ,c ,κ only depend on δ. Using the covering arguments in [Vershynin, 2012, Lemma
1 2
5.2], it follows that for any constant K
1
(cid:32) (cid:114) (cid:33)
P sup
(cid:12) (cid:12)uTS≥ju−uTΣ¯≥ju(cid:12)
(cid:12) ≤ K
(n j +1)logp
1
n
∥u∥2=1
≥ 1−exp(−K2(n +1)logp+2(n +1)log21)
1 j j
for large enough n. Since the above holds for every 1 ≤ j ≤ p, the union-sum inequality can
˜ ˜
be used to establish the existence of an event C such that P(C ) → 1 as n → ∞, and on
n n
˜
the event C
n
(cid:114)
(cid:13) (cid:13)S≥j −Σ¯≥j(cid:13) (cid:13) ≤ K (n j +1)logp ∀ 1 ≤ j ≤ p (S.12)
2 1 n
for an appropriate constant K . Since max n + 1 = o(n/logp), it follows from As-
1 1≤j≤p j
˜
sumption A1 and (S.12) that on the event C
n
(cid:114)
(cid:13) (cid:13)(S≥j)−1 −(Σ¯≥j)−1(cid:13) (cid:13) ≤ K (n j +1)logp ∀ 1 ≤ j ≤ p (S.13)
2 2 n
for an appropriate constant K . Using (S.13) along with the fact that the first diagonal
2
entries of (S≥j)−1 and (Σ¯≥j)−1 are Lˆ2 and L¯2 respectively, we get
jj jj
(cid:114)
(cid:12) (cid:12) (n +1)logp
(cid:12)Lˆ2 −L¯2 (cid:12) ≤ K j
(cid:12) jj jj(cid:12) 2 n
for 1 ≤ j ≤ p on
C˜
. Using Assumption A1 along with
|Lˆ −L¯
| =
|Lˆ2 −L¯2 |/|Lˆ +L¯
|,
n jj jj jj jj jj jj
we get
(cid:114)
(cid:12) (cid:12) K (n +1)logp
(cid:12)Lˆ −L¯ (cid:12) ≤ 2 j (S.14)
(cid:12) jj jj(cid:12) δ n
SinceLˆ2(S>j)−1S> isthefirstcolumnof(S≥j)−1 (sansthediagonalentry),andL¯2(Σ¯>j)−1Σ¯>
ii .j ii .j
33is the first column of
(Σ¯≥j)−1,
it follows from (S.11), (S.13) and (S.14) that
(cid:114)
(n +1)logp
∥LˆD −L¯
∥ ≤ K
j
(S.15)
.j j 2 3 n
for every 1 ≤ j ≤ p on C˜ . For any v ∈ Rp with ∥v∥ = 1, we get by (S.15) and the
n 2
Cauchy-Schwarz inequality that
 2
p
(cid:88) (cid:88)
∥(LˆD −L¯ )v∥2
2
=  (LˆD −L¯ ) ijv j
i=1 j≤i:i=jor (i,j)∈ED
σ
p
(cid:88) (cid:88)
≤ (r +1) (LˆD −L¯ )2 v2
i ij j
i=1 j≤i:i=jor (i,j)∈ED
σ
 
(cid:18) (cid:19) p
(cid:88) (cid:88)
≤ max r i +1 v j2  (LˆD −L¯ )2 ij
1≤i≤p
j=1 i≥j:i=jor (i,j)∈ED
σ
(cid:18) (cid:19) p
(cid:88)
= max r +1 v2∥LˆD −L¯ ∥2
i j .j j 2
1≤i≤p
j=1
aDlogp
≤ K2 n ∥v∥2
3 n 2
˜
on C . It follows by the above inequality and (S.10) that
n
(cid:32) (cid:33)(cid:114)
18K˜ min(aD,|ED|+1)logp
∥LˆD −L¯ ∥ ≤ max K , n σ (S.16)
2 3 δ3 n
˜ ˜
on C ∩C . Note that P(C ∩C ) → 1 as n → ∞. This establishes part (a).
n n n n
(b) Let c = |ED \E |. In other words, c is the number of fill-in entries. Consider row-wise
σ σ
traversalofalowertriangularmatrixasdescribedinStepIIoftheCCAalgorithm: startwith
the second row and move from entries with the lowest column index to the highest column
index before the diagonal. We order the fill-in entries based on when they are encountered
in this traversal. We define functions U and V from {1,2,··· ,c} to {1,2,··· ,p} such
that (U(r),V(r)) is the rth fill-in entry, for 1 ≤ r ≤ c. For 0 ≤ r ≤ c, let Lˆ(r) denote the
intermediate matrix which is obtained by updating the first r fill in entries as in in Algorithm
1 of the main paper, and
Ωˆ(r)
=
Lˆ(r)(Lˆ(r))T.
In particular,
Lˆ(0)
=
LˆD
while
Lˆ(c)
= L(cid:98) (the
CCA estimator).
˜
Henceforth, we will restrict to the event C ∩C . Let
n n
(cid:114)
min(aD,|ED|+1)logp
ϵ˜ = max(K ,18K˜ δ−3) n σ ,
n 3
n
√
and let N ∈ N be such that ϵ˜ (cid:0) 1+max gD(r)(cid:1) < δ for n > N. Note that the matrices
n 1≤r≤c 2
34Lˆ(1) and Lˆ(0) only differ in the (U(1),V(1))th entry. Hence
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Lˆ (1) −L¯(cid:13) ≤ (cid:13)Lˆ (1) −Lˆ (0)(cid:13) +(cid:13)Lˆ (0) −L¯(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
2 2 2
(cid:12) (cid:12) (cid:13) (cid:13)
= (cid:12)Lˆ(1) −Lˆ(0) (cid:12)+(cid:13)Lˆ (0) −L¯(cid:13)
(cid:12) U(1),V(1) U(1),V(1)(cid:12) (cid:13) (cid:13)
2
(cid:12)
(cid:12)
(cid:80) Lˆ(0) Lˆ(0) (cid:12)
(cid:12) (cid:13) (cid:13)
=
(cid:12)Lˆ(0)
+
k<V(1) U(1),k V(1),k(cid:12)+(cid:13)Lˆ(0) −L¯(cid:13)
(cid:12)
(cid:12)
U(1),V(1) Lˆ(0) (cid:12)
(cid:12)
(cid:13) (cid:13)
2
V(1),V(1)
(cid:12) (cid:12)
(cid:12)Ωˆ(0) (cid:12)
≤
(cid:12) U(1),V(1)(cid:12) +(cid:13)
(cid:13)Lˆ(0)
−L¯(cid:13)
(cid:13)
(cid:12) (cid:12) (cid:13) (cid:13)
(cid:12)Lˆ(0) (cid:12) 2
(cid:12) V(1),V(1)(cid:12)
(cid:12) (cid:12)
(cid:12)Ωˆ(0) −Ω¯ (cid:12)
= (cid:12) U(1),V(1) U(1),V(1)(cid:12) +(cid:13) (cid:13)Lˆ(0) −L¯(cid:13) (cid:13)
(cid:12) (cid:12) (cid:13) (cid:13)
(cid:12)Lˆ(0) (cid:12) 2
(cid:12) V(1),V(1)(cid:12)
Note that (cid:13) (cid:13)Ω¯(cid:13) (cid:13) = (cid:13) (cid:13)L¯ L¯T(cid:13) (cid:13) = (cid:13) (cid:13)L¯(cid:13) (cid:13)2 ≤ 1. It follows that
2 2 2 δ
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Ωˆ(0) −Ω¯(cid:13)
=
(cid:13)Lˆ(0)(Lˆ(0))T −L¯ L¯T(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
2 2
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤
(cid:13)Lˆ(0)(cid:13) (cid:13)Lˆ(0) −L¯(cid:13) +(cid:13) (cid:13)L¯(cid:13)
(cid:13)
(cid:13)Lˆ(0) −L¯(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) 2(cid:13) (cid:13)
2 2 2
(cid:13) (cid:13) (cid:13) (cid:13)
=
((cid:13)Lˆ(0)(cid:13) +(cid:13) (cid:13)L¯(cid:13)
(cid:13)
)(cid:13)Lˆ(0) −L¯(cid:13)
(cid:13) (cid:13) 2 (cid:13) (cid:13)
2 2
(cid:13) (cid:13) (cid:13) (cid:13)
≤
((cid:13)Lˆ(0) −L¯(cid:13)+2(cid:13) (cid:13)L¯(cid:13)
(cid:13)
)(cid:13)Lˆ(0) −L¯(cid:13)
(cid:13) (cid:13) 2 (cid:13) (cid:13)
2
3 (cid:13) (cid:13)
≤ √ (cid:13)Lˆ(0) −L¯(cid:13)
(cid:13) (cid:13)
δ 2
(cid:113)
onC ∩C˜ forlargeenoughn. Thelastinequalityfollowsfrom(S.16)and min(aD n,|E σD|+1)logp =
n n n
o(1). Also by (S.16), as L¯ ≥ √ δ and (cid:12) (cid:12)Lˆ(0)(cid:12) (cid:12) ≥ (cid:12) (cid:12)L¯ (cid:12) (cid:12)−(cid:12) (cid:12)L¯ −Lˆ(0)(cid:12) (cid:12), we have (cid:12) (cid:12)Lˆ(0)(cid:12) (cid:12) ≥ √ δ for
ii (cid:12) ii (cid:12) ii (cid:12) ii ii (cid:12) (cid:12) ii (cid:12) 2
˜
every 1 ≤ i ≤ p on C ∩C for large enough n. It follows that,
n n
(cid:12) (cid:12) 6
(cid:12)Lˆ(1) −Lˆ(0) (cid:12) ≤ ϵ˜ = gD(1) (S.17)
(cid:12) U(1),V(1) U(1),V(1)(cid:12) δ n
for n > N. We will now proceed by induction. Suppose
(cid:12) (cid:12)
(cid:12)Lˆ(r˜) −Lˆ(0) (cid:12) ≤ gD(r˜)ϵ˜
(cid:12) U(r˜),V(r˜) U(r˜),V(r˜)(cid:12) n
for 1 ≤ r˜≤ r−1. Note that
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:80) Lˆ(r−1)Lˆ(r−1) (cid:12)
(cid:12)
(cid:12)Lˆ(r) −Lˆ(0) (cid:12) = (cid:12)Lˆ(0) + k<V(r) U(r),k V(r),k(cid:12).
(cid:12) U(r),V(r) U(r),V(r)(cid:12) (cid:12)
(cid:12)
U(r),V(r) Lˆ(0) (cid:12)
(cid:12)
V(r),V(r)
Note that the entries of
Lˆ(r−1)
and
Lˆ(0)
match except for the first r−1 fill-in entries. Recall
that r˜∈ FD if and only if (U(r˜),V(r˜)) = (U(r),k) and ((V(r),k) ∈ ED for some k < V(r) or
r σ
35(U(r˜),V(r˜)) = (V(r),k) and(U(r),k) ∈ ED for somek < V(r). By theinduction hypothesis,
σ √
(S.16), and the choice of N, it follows that |Lˆ(r˜)| ≤ √3 whenever i > j, and |Lˆ(r˜)| ≥ δ
ij 2 δ ij 2
whenever i = j. Using |ab−a b | ≤ |a||b−b |+|b ||a−a | (in case there is a k < V(r) such
0 0 0 0 0
that (U(r),k) and (V(r),k) are both fill-in entries) along with the above observations we get
(cid:12) (cid:12)
(cid:12)Lˆ(r) −Lˆ(0) (cid:12)
(cid:12) U(r),V(r) U(r),V(r)(cid:12)
≤
(cid:12)
(cid:12)
(cid:12)Lˆ(0) +
(cid:80) k<V(r)Lˆ( U0 () r),kLˆ( V0 () r),k(cid:12)
(cid:12)
(cid:12)+
3 (cid:88) (cid:12)
(cid:12)Lˆ(r˜) −Lˆ(0)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
U(r),V(r) Lˆ(0) (cid:12)
(cid:12)
δ (cid:12) U(r˜),V(r˜) U(r˜),V(r˜)(cid:12)
V(r),V(r) r˜∈FD
r
(cid:12) (cid:12)
(cid:12)Ωˆ(0) −Ω¯ (cid:12)
(cid:12) U(r),V(r) U(r),V(r)(cid:12) 3 (cid:88)
≤ + gD(r˜)ϵ˜
(cid:12) (cid:12) n
(cid:12)Lˆ(0) (cid:12) δ
(cid:12) V(r),V(r)(cid:12) r˜∈F rD
 
3 (cid:88)
≤ 2+ gD(r˜)ϵ˜
n
= gD(r)ϵ˜ n.
δ
r˜∈FD
r
By induction, it follows that
(cid:12) (cid:12)
(cid:12)Lˆ(r) −Lˆ(0) (cid:12) ≤ gD(r)ϵ˜ ∀ 1 ≤ r ≤ c.
(cid:12) U(r),V(r) U(r),V(r)(cid:12) n
This implies
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Lˆ −L¯D(cid:13) = (cid:13)Lˆ(c) −Lˆ(0)(cid:13) ≤ ϵ˜ max gD(r),
(cid:13) (cid:13) (cid:13) (cid:13) n
max max 1≤r≤c
Using a similar argument as the one right before (S.16), we get
(cid:13) (cid:13) (cid:112)
(cid:13)L(cid:98)−L(cid:98)D(cid:13) ≤ ϵ˜ a˜D max gD(r)
(cid:13) (cid:13) n n
2 1≤r≤c
where a˜D is the product of the maximum number of fill-in entries in any given column and
n
˜
the maximum number of fill-in entries in any given row. It follows that on C ∩C
n n
(cid:18) (cid:19) (cid:114)
(cid:13) (cid:13)Lˆ −L¯(cid:13) (cid:13) ≤ ϵ˜ (cid:112) a˜D max gD(r)+1 ≤ K s CCAlogp (S.18)
(cid:13) (cid:13) 2 n n 1≤r≤c n
for an appropriate constant K. Finally, note that
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Ω(cid:98) −Ω¯(cid:13) = (cid:13)L(cid:98)L(cid:98)T −L¯ L¯T(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
2 2
(cid:13) (cid:13)
= (cid:13)L(cid:98)L(cid:98)T −L¯ L(cid:98)T +L¯ L(cid:98)T −L¯ L¯T(cid:13)
(cid:13) (cid:13)
2
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (cid:13)L(cid:98)−L¯(cid:13) (cid:13)L(cid:98)(cid:13) +(cid:13) (cid:13)L¯(cid:13) (cid:13) (cid:13)L(cid:98)−L¯(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) 2(cid:13) (cid:13)
2 2 2
≤
(cid:13) (cid:13)L(cid:98)−L¯(cid:13) (cid:13)2 +2(cid:13) (cid:13)L¯(cid:13)
(cid:13)
(cid:13) (cid:13)L(cid:98)−L¯(cid:13)
(cid:13)
(cid:13) (cid:13) 2(cid:13) (cid:13)
2 2
(cid:114)
s logp
≤ K′ CCA
n
36on C ∩C˜ for an appropriate constant K′. The last inequality follows from (S.18), Assump-
n n
tion A1 and Assumption A2. □
Section B: Weak convergence with fixed p
ˆ
We start by establishing two lemmas which imply that Ω is a differentiable function of Ω.
Lemma S.1 The function ϕ : P+ → P defined by
1,GD GD
σ σ
ϕ (A) =
(cid:88) (cid:2)
(A−1)
(cid:3)0
−
(cid:88) (cid:2)
(A−1)
(cid:3)0
1,GD C Sep
σ
C∈CD Sep∈SD
σ σ
is differentiable. Also, if A−1 ∈ P , then ϕ (A) = A−1.
GD 1,GD
σ σ
Proof: The differentiability follows by repeated application of the differentiability of the
matrix inverse for positive definite matrices. It is known that (see Lauritzen [1996]) ϕ (A)
1,GD
σ
uniquely minimizes the function
f(K) = tr(KA)−|KA|
for K ∈ P . Since A−1 uniquely minimizes f over the space of all positive definite matrices,
GD
σ
if A−1 ∈ P , it follows that ϕ (A) = A−1. □
GD 1,GD
σ σ
Let ϕ : L+ → L be defined as the function which takes L ∈ L+, and applies the last
2,Gσ Gσ
step of Algorithm 1 of the main paper (with the two for loops) to convert it to a matrix in
L .
Gσ
Lemma S.2 The function ϕ : P+ → P defined by
3,Gσ Gσ
ϕ (A) = ϕ (L )ϕ (L )T
3,GD
σ
2,Gσ A 2,Gσ A
is differentiable. Here L denotes the Cholesky factor of A. Also, if A ∈ P , then
A Gσ
ϕ (A) = A.
3,Gσ
Proof: The transformation of a positive definite matrix A to its Cholesky factor L is
A
clearly differentiable. It is clear from construction that for i > j, (ϕ (L )) is a polyno-
(cid:16) (cid:17)
2,Gσ A ij
mialin (L ) ,{ϕ (L ) } ,{ϕ (L ) } , 1 . Sinceallthediagonalentriesof
A ij 2,Gσ A ik k<j 2,Gσ A jk k<j (LA)
jj
ϕ (L ) are the same as the diagonal entries of L , it follows that the entries of ϕ (L )
2,Gσ A
(cid:110) (cid:111)
A 2,Gσ A
are polynomials in entries of L and 1 , which implies that all the entries of
A (LA)
ii 1≤i≤p
(cid:110) (cid:111)
ϕ (A) are polynomials in the entries of L and 1 . Hence, ϕ is a differen-
3,Gσ A (LA)
ii 1≤i≤p
3,Gσ
tiable transformation from P+ to P .
Gσ
Suppose A ∈ P . It follows that L ∈ L . We prove by induction that ϕ does not
Gσ A Gσ 2,Gσ
alter L . This would clearly imply ϕ (A) = A. Note that all the diagonal entries of L
A 3,Gσ A
are not altered by ϕ . Suppose that all the entries of ϕ (L ) and L are the same for
2,Gσ 2,Gσ A A
the first i−1 rows, and the first j −1 entries of the ith row, for some i > j. If (i,j) ∈ E ,
σ
then by construction ϕ (L ) = (L ) . If (i,j) ∈/ E , then by the induction hypothesis
2,Gσ A ij A ij σ
37and the fact that L ∈ L , it follows that
A Gσ
−(cid:80)j−1
ϕ (L ) ϕ (L )
ϕ (L ) = = k=1 2,Gσ A ik 2,Gσ A jk
2,Gσ A ij
ϕ (L )
2,Gσ A jj
−(cid:80)j−1(L
) (L )
= k=1 A ik A jk
(L )
A jj
= (L ) .
A ij
The result follows by induction. □
Using the above lemmas, we now establish weak convergence of the estimator corresponding
to Algorithm 1 of the main paper.
Theorem S.1 Under the assumption in Equation 5 of the main paper, the estimator Ω(cid:98)
√ √
¯
obtained from Algorithm 1 of the main paper is n-consistent. In particular, n(Ω(cid:98) − Ω)
converges to a multivariate Gaussian limit as n → ∞.
Proof: NotethatΩ(cid:98)D = ϕ (S)andΩ(cid:98) = ϕ (Ω(cid:98)D). Hence,wegetthatΩ(cid:98) = ϕ (ϕ (S)).
Since Ω¯ ∈ P , it follows1,G thσ at Ω¯ = ϕ (cid:0)3 ϕ,Gσ (Σ¯ )(cid:1) . Using the multivariate3 D,G eσ lta1 m,G eσ thod
Gσ 3,Gσ 1,Gσ
√
¯
along with these observations establishes the convergence of n(Ω(cid:98) − Ω) to a multivariate
Gaussian limit as n → ∞. □
Section C: Proof of Lemma 1
Note that the assumption that n is greater than the largest clique size in GD is required
σ
for the existence of Ω(cid:98)D, which in turn is needed for Algorithm 1 in the main paper. Let L(cid:98)
denote the Cholesky factor of Ω(cid:98). It is clear by construction that L(cid:98) ∈ L GD, and Ω(cid:98) ∈ P GD.
σ σ
Note that if i > j,(i,j) ∈/ ED \E , then L(cid:98) = −(cid:80)j−1 L(cid:98) L(cid:98) /L(cid:98) . Hence
σ σ ij k=1 ik jk jj
j j−1
(cid:88) (cid:88)
Ω(cid:98) = (L(cid:98)L(cid:98)T) = L(cid:98) L(cid:98) = L(cid:98) L(cid:98) + L(cid:98) L(cid:98) = 0.
ij ij ik jk ij jj ik jk
k=1 k=1
It follows that L(cid:98) ∈ L and Ω(cid:98) ∈ P . □
Gσ Gσ
Section D: Proof of Lemma 2
ˆ
The fact that L is a minimizer of h(·) over the relevant set follows from the fact that h is
ˆ ˜
non-negative, h(L) = 0, and Lemma 1 in the main paper. Suppose L is another minimizer
of h over the relevant set. Then h(L˜ ) = 0, which implies that L˜ = L(cid:98)D = Lˆ if (i,j) ∈ E
ij ij ij σ
or i = j. Suppose there are c fill-in entries for
LˆD
with row-column locations given by
(i ,j ),(i ,j ),··· ,(i ,j ). The entries are ordered based on the traversal of L(cid:98)D described
1 1 2 2 c c
in Step II above. Since
(L˜ L˜T)
= 0, it follows that
i1j1
(cid:80)j1−1L˜ L˜
L˜ = − k=1 i1k j1k .
i1j1 L˜
j1j1
38Since all entries on the RHS are not fill-in entries and Ω(cid:98) = (L(cid:98)L(cid:98)T) = 0, it follows that
i1j1 i1j1
˜
L = L(cid:98) . Similarly, if c ≥ 2, note that
i1j1 i1j1
(cid:80)jr−1L˜ L˜
L˜ = − k=1 irk jrk
irjr L˜
jrjr
for every 2 ≤ r ≤ c. The entries on the RHS are either non fill-in entries or among the first
˜
r−1 fill-in entries. Hence, by a striaghtforward induction argument it follows that L and L(cid:98)
share values even at the fill-in locations. □
Section E: Proof of Lemma 5
First let us assume G is connected. Then
p
(cid:88)
2p2|E | = p2 |{j : (i,j) ∈ E }| ≥ p2 ×p = p3.
σ σ
i=1
Similarly
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88) (cid:88)
p2 |U| = p2 1 = p2 1 ≥ p2 1 = p3.
U∈U U∈U i∈U i∈V U∈U:i∈U i∈V
The result now follows by Table 1 in the main paper. For disconnected graphs, the inequal-
ities above hold for each respective connected component. Since the overall complexity for
all algorithms is obtained by adding the complexities over all connected components, the
result follows for disconnected graphs as well. □
Section F: Proof of Lemma 6
Let c =
(cid:0)p(cid:1)
and d = p5/3 for convenience. Then
p 2 p
(cid:88)dp (cid:18)
c
(cid:19) (cid:18)
c
(cid:19)
|A | = 2cp −|Ac| = 2cp − p ≥ 2cp −(d +1) p
p p k p d
p
k=0
for p ≥ 100, since
(cid:0)n(cid:1)
increases in r for r < n/2 and d < c /2 for p ≥ 100. Using Stirling’s
r p p
approximation, it follows that
|A | ≥ 2cp −(d +1)cd pp ≥ 2cp −C(cid:112) d (cid:18) ec p(cid:19)dp ≥ 2cp −C(cid:112) d (ec )dp
p p p p p
d ! d
p p
(cid:112)
where C = 2/π. Since p2/4 ≤ c ≤ p2/2 for p ≥ 100, it follows that
p
|A |
(cid:18)
5
(cid:19) (cid:18) p2log2(cid:19)
p
≥ 1−Cexp logp+d +2d logp−c log2 ≥ 1−Cexp 3d logp− → 1
2cp 6 p p p p 4
as p → ∞. This establishes part (a). For part (b), note that for any ordering σ of the p
39vertices, (cid:12) (cid:12)ED(cid:12) (cid:12) ≥ |E | > p5/3. Hence, the CCA algorithm uses a direct computation of LˆD
σ σ
(see equation (3) in the main paper) and has computational complexity O(p3). On the other
hand, it can be seen using Jensen’s inequality (similar to equation (3) in the main paper)
that
p
(cid:32)
p
(cid:33)3
(cid:88) 1 (cid:88) |E |3
n˜3 ≥ p n˜ = σ ≥ p3.
j p j p2
j=1 j=1
□
Section G: Proof of Lemma 7
ˆ
Since Ω = 0, it follows that
32
ˆ ˆ
L L
ˆ 21 31
L = − .
32 ˆ
L
22
ˆ
Next, using Ω = 0 along with the above equality, it follows that
43
ˆ ˆ ˆ ˆ ˆ
L L L L L
ˆ 42 32 21 31 42
L = − = .
43 ˆ ˆ ˆ
L L L
33 22 33
Sequentially extending these calculations to include all fill-in entries, we obtain
ˆ i−1 ˆ
L (cid:89) L
Lˆ
=
(−1)i−2Lˆ 21 j,j−2
(S.19)
i,i−1 i,i−2 ˆ ˆ
L L
22 j=3 jj
for 4 ≤ i ≤ p−1. Since Ω ∈ P , identical calculations lead to
0 Gσ
¯ ¯ ¯ i−1 ¯
L L L (cid:89) L
L¯
= −
21 31
and
L¯
=
(−1)i−2L¯ 21 j,j−2
(S.20)
32 ¯ i,i−1 i,i−2 ¯ ¯
L L L
22 22 jj
j=3
for 4 ≤ i ≤ p−1. In (S.19) and (S.20), all fill-in entries of the respective Cholesky matrices
are expressed in terms of the non fill-in entries of these matrices. Note that aD = O(1) in
n
this setting. By repeating the arguments in the proof of part (a) of Theorem 1 after equation
(S.10), there exists an appropriate positive constant K∗ such that
(cid:114)
(cid:13) (cid:13) logp
(cid:13)L(cid:98)D −L¯(cid:13) ≤ K˜∗
(cid:13) (cid:13) n
2
˜ ¯ ˜ ˜
on an event D with P(D ) → 1 as n → ∞. Let δ > 0 be such that
n n
˜ ˜
2δ 4δ 1−ξ
+ < , (S.21)
δ δ3 2
where ξ := sup max |L¯ j,j−2| < 1 by Assumption A3’. Since LˆD = Lˆ for every
n≥1 3≤j≤p−1 L¯
jj
i,j ij
(i,j) ∈ E (Step 2 of the CCA algorithm leaves the non fill-in entries invariant), it follows
σ
40that
ˆ ¯ ˜
max |L −L | < δ
ij ij
(i,j)∈Eσ
¯
on an event D such that P(D ) → 1 as n → ∞. Note by Assumption 1 and (S.21) that on
n n
D
n
δ
ˆ ¯ ˆ ¯ ˜
L ≥ L −|L −L | ≥ δ −δ >
jj jj jj jj
2
for 1 ≤ j ≤ p, and
1 2
ˆ ¯ ˆ ¯ ˜
|L | ≤ |L |+|L −L | ≤ +δ <
ij ij ij ij
δ δ
˜
for every (i,j) ∈ E . It follows that on D ∩D
σ n n
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)Lˆ L¯ (cid:12) (cid:12)Lˆ Lˆ (cid:12) (cid:12)Lˆ L¯ (cid:12)
(cid:12) j,j−2 − j,j−2(cid:12) ≤ (cid:12) j,j−2 − j,j−2(cid:12)+(cid:12) j,j−2 − j,j−2(cid:12)
(cid:12)
(cid:12)
Lˆ
jj
L¯
jj
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Lˆ
jj
L¯
jj
(cid:12)
(cid:12)
(cid:12)
(cid:12)
L¯
jj
L¯
jj
(cid:12)
(cid:12)
Lˆ (cid:12) (cid:12) |Lˆ −L¯ |
≤ j,j−2 (cid:12)Lˆ −L¯ (cid:12)+ j,j−2 j,j−2
Lˆ L¯ (cid:12) jj jj(cid:12) L¯
jj jj jj
(cid:32)
2δ˜ 4δ˜
(cid:32)
2K˜∗
4K˜∗(cid:33)(cid:114) logp(cid:33)
≤ min + , +
δ δ3 δ δ3 n
(cid:32) (cid:32) (cid:33)(cid:114) (cid:33)
1−ξ
2K˜∗ 4K˜∗
logp
< min , + , (S.22)
2 δ δ3 n
and
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)Lˆ j,j−2(cid:12) (cid:12) ≤ (cid:12) (cid:12) (cid:12)L¯ j,j−2(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12)Lˆ j,j−2 − L¯ j,j−2(cid:12) (cid:12) < ξ + 1−ξ = 1+ξ (S.23)
(cid:12)
(cid:12)
Lˆ
jj
(cid:12)
(cid:12)
(cid:12) L¯
jj
(cid:12) (cid:12)
(cid:12)
Lˆ
jj
L¯
jj
(cid:12)
(cid:12)
2 2
˜
for 3 ≤ j ≤ p. By similar arguments as above, it can be shown that on D ∩D
n n
(cid:12) (cid:12) (cid:114) (cid:12) (cid:12)
(cid:12) Lˆ L¯ (cid:12) 8K˜∗ logp (cid:12) Lˆ (cid:12) 8
(cid:12)Lˆ 21 −L¯ 21(cid:12)
≤ and
(cid:12)Lˆ 21(cid:12)
≤ (S.24)
(cid:12)
(cid:12)
i,i−2 Lˆ
22
i,i−2 L¯ 22(cid:12)
(cid:12)
δ3 n (cid:12)
(cid:12)
i,i−2 Lˆ 22(cid:12)
(cid:12)
δ3
for 3 ≤ i ≤ p. Using the identity
(cid:12) (cid:12) (cid:32) (cid:33)(cid:32) (cid:33) (cid:32) (cid:33)
(cid:12)(cid:89)k (cid:89)k (cid:12) (cid:89)k (cid:88)k−1 (cid:89)i−1 (cid:89)k k (cid:89)−1
(cid:12) t − r (cid:12) ≤ |t −r | t + t r |t −r |+ t |t −r |
(cid:12) j j(cid:12) 1 1 j j j i i j k k
(cid:12) (cid:12)
j=1 j=1 j=2 i=2 j=1 j=i+1 j=1
along with (S.19), (S.20), (S.22), (S.23), (S.24) and Assumption A3′, we obtain
(cid:32)
8K˜∗ (cid:18) 1+ξ(cid:19)i−3
8
(cid:32)
2K˜∗
4K˜∗(cid:33)
(cid:18)
1+ξ(cid:19)i−4(cid:33)(cid:114)
logp
ˆ ¯
|L −L | ≤ +(i−3) +
i,i−1 i,i−1 δ3 2 δ3 δ δ3 2 n
˜
for every 3 ≤ i ≤ p−1 on D ∩D . Since (1+ξ)/2 < 1, it follows that for an appropriate
n n
˜
constant K
(cid:114)
logp
ˆ ¯ ˜
max |L −L | ≤ K
ij ij
(i,j)∈ED n
σ
41on D ∩D˜ . Since |E | = p and |ED| = 2p−3, it follows that
n n σ σ
(cid:114)
(|E |+1)logp
∥Lˆ −L¯ ∥2 ≤ K˜ σ
F n
˜ ˆ ¯
on D ∩D . The bound for ∥Ω−Ω∥ now follows by similar arguments as those at the end
n n F
of the proof of Theorem 1. □
References
A. Atay-Kayis and H. (2005) Massam. A monte carlo method for computing the marginal
likelihood in nondecomposable gaussian graphical models. Biometrika, 92:317–335, 2005.
P. J. Bickel and E. Levina. Covariance regularization by thresholding. Ann. Statist., 36:
2577–2604, 2008a.
P.J.BickelandE.Levina. Regularizedestimationoflargecovariancematrices. Ann.Statist.,
36:199–227, 2008b.
W.M. Chan and A. George. A linear time implementation of the reverse cuthill-mckee
algorithm. BIT Numerical Mathematics, 20:1–8, 1980.
T.A. Davis. Direct methods for sparse linear systems. SIAM, Philadelphia, 2006.
A. P. Dawid and S. L. Lauritzen. Hyper markov laws in the statistical analysis of decom-
posable graphical models. Ann. Statist., 36:1272–1317, 1993.
M. Feingold and M. Drton. Robust graphical modeling of gene networks using classical and
alternative t-distributions. Annals of Applied Statistics, 5:1057–1080, 2011.
R. Grone, C. Johnson, E. Sa´, and H. Wolkowicz. Positive definite completions of partial her-
mitian matrices. Linear Algebra and its Applications, 58:109–124, 1984. ISSN 0024-3795.
doi: https://doi.org/10.1016/0024-3795(84)90207-6. URL https://www.sciencedirect.
com/science/article/pii/0024379584902076.
D. Guillot, B. Rajaratnam, and J. Emile-Geay. Statistical paleoclimate reconstructions via
markov random fields. Annals of Applied Statistics, 9:324–352, 2015.
H. Hara and A. Takemura. A localization approach to improve iterative proportional scaling
in gaussian graphical models. Communications in Statistics - Theory and Methods, 39
(8-9):1643–1654, 2010. doi: 10.1080/03610920802238662. URL https://doi.org/10.1080/
03610920802238662.
D. A. Harville. Maximum likelihood approaches to variance component estimation and to
related concepts. Journal of the American Statistical Association, 72:320–338, 1977.
T.Hastie, R.Tibshirani, andJ.Friedman. ElementsofStatisticalLearning. Springer-Verlag,
2009.
A. Hero and B. Rajaratnam. Hub discovery in partial correlation graphs. IEEE Transactions
on Information Theory, 58:6064–6078, 2012.
42S. Hojsgaard and S. L. Lauritzen. On some algorithms for estimation in gaussian graphical
models. arXiv, 2023.
C. Hsieh, M. Sustik, I. Dhillon, P. Ravikumar, and R. Poldrack. Big & quic: Sparse inverse
covariance estimation for a million variables. Advances in neural information processing
systems, 26, 2013.
J. Huang, N. Liu, M. Pourahmadi, and L. Liu. Covariance selection and estimation via
penalised normal likelihood. Biometrika, 93:85–98, 2006.
K. Khare and B. Rajaratnam. Sparse matrix decompositions and graph characterizations.
Linear Algebra and its Applications, 437(3):932–947, 2012. ISSN 0024-3795. doi: https://
doi.org/10.1016/j.laa.2012.03.027. URL https://www.sciencedirect.com/science/article/
pii/S0024379512002583.
K. Khare, S. Oh, and B. Rajaratnam. A convex pseudo-likelihood framework for high
dimensional partial correlation estimation with convergence guarantees. Journal of the
Royal Statistical Society B, 77:803–825, 2015.
K. Khare, B. Rajaratnam, and A. Saha. Bayesian inference for gaussian graphical models
beyond decomposable graphs. Journal of the Royal Statistical Society. Series B (Statistical
Methodology), 80(4):pp. 727–747, 2018. ISSN 13697412, 14679868. URL https://www.
jstor.org/stable/26773177.
Kshitij Khare, Sang-Yun Oh, Syed Rahman, and Bala Rajaratnam. A scalable sparse
cholesky based approach for learning high-dimensional covariance matrices in ordered
data. Machine Learning, 108(12):2061–2086, 2019. doi: 10.1007/s10994-019-05810-5.
URL https://doi.org/10.1007/s10994-019-05810-5.
S.L. Lauritzen. Graphical models. Oxford University Press Inc., 1996.
O. Ledoit and M. Wolf. Improved estimation of the covariance matrix of stock returns with
an application to portfolio selection. Journal of Empirical Finance, 10:603–621, 2003.
A. Lenkoski. A direct sampler for G-Wishart variates. Stat, 2:119–128, 2013.
G. Letac and H. Massam. Wishart distributions for decomposable graphs. The Annals
of Statistics, 35(3):1278 – 1323, 2007. doi: 10.1214/009053606000001235. URL https:
//doi.org/10.1214/009053606000001235.
B. Marjanovic. Huge stock market dataset, 2017. https://www.kaggle.com/datasets/
borismarjanovic/price-volume-data-for-all-us-stocks-etfs.
N. Mitsakakis, H. Massam, and M. D. Escobar. A Metropolis-Hastings based method for
sampling from the G-Wishart distribution in Gaussian graphical models. Electron. J.
Statist., 5:18–30, 2011.
V.I. Paulsen, S.C. Power, and A.A. Smith. Schur products and matrix completions. Journal
of Functional Analysis, 85:151–178, 1989.
J. Peng, P. Wang, N. Zhou, and J. Zhu. Partial correlation estimation by joint sparse
regression models. Journal of the American Statistical Association, 104:735–746, 2009.
43B. Rajaratnam, H. Massam, and C.M. Carvalho. Flexible covariance estimation in graphical
gaussian models. Ann. Statist., 36:2818–2849, 2008.
A. Rothman, P. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance
estimation. Electronic Journal of Statistics, 2(none):494 – 515, 2008. doi: 10.1214/08-
EJS176. URL https://doi.org/10.1214/08-EJS176.
A. Rothman, E. Levina, and J. Zhu. Generalized thresholding of large covariance matrices.
Journal of the American Statistical Association, 104(485):177–186, 2009.
A. Roverato. Hyper inverse wishart distribution for non-decomposable graphs and its ap-
plication to bayesian inference for gaussian graphical models. Scandinavian Journal of
Statistics, 29:391–411, 2002.
A. Shojaie and G. Michailidis. Penalized likelihood methods for estimation of sparse high-
dimensional directed acyclic graphs. Biometrika, 97:519–538, 2010.
T.P. Speed and H.T. Kiveri. Gaussian markov distributions over finite graphs. Annals of
Statistics, 14:138–150, 1986.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices, Chapter 5 of
Compressed Sensing: Theory and Applications, edited by Eldar, Yonina C. and Kutyniok,
Gitta. Cambridge University Press, 2012. doi: 10.1017/CBO9780511794308.006.
R. Xiang, K. Khare, and M. Ghosh. High dimensional posterior convergence rates for de-
composable graphical models. Electron. J. Statist., 9:2828–2854, 2015.
P. Xu, J. Guo, and X. He. An improved iterative proportional scaling procedure for gaussian
graphicalmodels. JournalofComputationalandGraphicalStatistics,20(2):417–431,2011.
doi: 10.1198/jcgs.2010.09044. URL https://doi.org/10.1198/jcgs.2010.09044.
P.Xu, J.Guo, andM.Tang. Animprovedhara-takamuraprocedurebysharingcomputations
on junction tree in gaussian graphical models. Statistics and Computing, 22(5):1125–1133,
2012. doi: 10.1007/s11222-011-9286-4. URL https://doi.org/10.1007/s11222-011-9286-4.
P. Xu, J. Guo, and M. Tang. A localized implementation of the iterative proportional
scaling procedure for gaussian graphical models. Journal of Computational and Graphical
Statistics, 24(1):205–229, 2015. doi: 10.1080/10618600.2014.900499. URL https://doi.
org/10.1080/10618600.2014.900499.
G. Yu and J Bien. Learning local dependence in ordered data. Journal of Machine Learning
Research, 18:1–60, 2017.
J. Zhang, M. Wang, Q. Li, S. Wang, X. Chang, and B. Wang. Quadratic sparse gaus-
sian graphical model estimation method for massive variables. In Proceedings of the
Twenty-Ninth International Conference on International Joint Conferences on Artificial
Intelligence, pages 2964–2972, 2021.
44