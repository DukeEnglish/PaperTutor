PermitQA: A Benchmark for Retrieval Augmented Generation in Wind
Siting and Permitting domain
RounakMeyur,HungPhan,SrideviWagle,JanStrube,MahanteshHalappanavar,
SameeraHorawalavithanaAnuragAcharya,SaiMunikoti
PacificNorthwestNationalLaboratory
Richland,WA99354
{rounak.meyur, hung.phan, sridevi.wagle, jan.strube, Mahantesh.Halappanavar,
yasanka.horawalavithana, anurag.acharya,sai.munikoti}@pnnl.gov
Abstract generationoffictitiousorirrelevantcontent(Gao
etal.,2024;Lewisetal.,2021). Furthermore,con-
In the rapidly evolving landscape of Natural
Language Processing (NLP) and text genera- currentworkssuggestRAGisthemostsoughtap-
tion, the emergence of Retrieval Augmented proachforadaptingmodelstowardsnichescientific
Generation(RAG)presentsapromisingavenue domainsuchasnuclear,renewableenergy,environ-
forimprovingthequalityandreliabilityofgen- mentalpolicy,etc. (Munikotietal.,2024a,b;Phan
eratedtextbyleveraginginformationretrieved
etal.,2023)
from user specified database. Benchmarking
Asthisinnovativeapproachgainstractionwithin
is essential to evaluate and compare the per-
theresearchcommunityandindustryapplications,
formanceofthedifferentRAGconfigurations
itseffectivenessandrobustnessmustberigorously
intermsofretrieverandgenerator,providing
insightsintotheireffectiveness,scalability,and assessed against established benchmarks to en-
suitability for the specific domain and appli- sureitspracticalutilityandreliability(Chenetal.,
cations. In this paper, we present a compre- 2023a). Benchmarking is essential to establish
hensiveframeworktogenerateadomainrele- standardized evaluation metrics and dataset that
vantRAGbenchmark. Ourframeworkisbased
caneffectivelycapturethenuancesoftextquality,
onautomaticquestion-answergenerationwith
coherence, factual accuracy, and relevance. Fur-
Human(domainexperts)-AI(LargeLanguage
ther, it facilitates comparison between RAG and
Model(LLM))teaming. Asacasestudy,we
demonstratetheframeworkbyintroducingPer- existing text generation methods, shedding light
mitQA, a first-of-its-kind benchmark on the onitsstrengths,limitations,andpotentialareasfor
windsitingandpermittingdomainwhichcom- improvement(Xiongetal.,2024). Arobustbench-
prisesofmultiplescientificdocuments/reports marking framework can enable researchers and
relatedtoenvironmentalimpactofwindenergy
practitioners to systematically investigate the im-
projects. Ourframeworksystematicallyeval-
pactofvariousparameters,suchasretrievalstrate-
uatesRAGperformanceusingdiversemetrics
gies,modelarchitectures,andtrainingdata,onthe
andmultiplequestiontypeswithvaryingcom-
performanceofRAG(Ray,2023).
plexitylevel. Wealsodemonstratetheperfor-
manceofdifferentmodelsonourbenchmark. InbenchmarkingRAGfortextgeneration,itis
crucialtoevaluateitsperformanceacrossadiverse
1 Introduction
set ofquestions to ensureits efficacy in handling
In recent years, the advancements in LLM have various linguistic contexts and user intents (Lyu
revolutionizedvariousnaturallanguageprocessing etal.,2024). Asetofwellcuratedanddiverseques-
tasks,includingtextandresponsegeneration. How- tionsenableacomprehensiveassessmentofRAG’s
ever,textgenerationusingLLMoftenencounters abilitytogeneratecoherentandrelevantresponses
challengessuchasgeneratingirrelevantorincoher- acrossvariousdomains,ensuringitsapplicability
ent outputs, perpetuating biases ingrained in the inreal-worldscenarios. Togeneratesuchquestions,
training data, and struggling to maintain context automatedmethodsleveragingNLPtechniquescan
andfactualaccuracy. Theseissuesposesignificant be employed. These methods include rule-based
obstacles to achieving human-level performance approaches, template filling, and neural network-
in automated text generation systems. RAG ef- basedmodels,whichenabletheefficientcreation
fectively mitigates these common challenges by of diverse question sets by leveraging linguistic
incorporatingretrievedinformationtoenhanceco- patternsandsemantictransformations.
herenceandfactualaccuracy,thusminimizingthe Human-curatedquestionsofferaleveloflinguis-
4202
guA
12
]LC.sc[
1v00811.8042:viXraticrichnessandcontextualrelevancethatmaybe
challenging to achieve solely through automated
generationmethods(Zhangetal.,2024). Bylever-
aginghumanexpertiseanddomainknowledge,cu-
ratedquestionsetscanencompassabroaderspec- Figure1: Anoverviewofdataextractionandcuration
trumoflinguisticvariations,domain-specificcon- pipelinetogenerateavectordatabaseofrelevantwind
siderations, and nuanced semantics, providing a energyrelateddocuments.
morecomprehensiveevaluationofRAG’sperfor-
mance across diverse scenarios and applications.
objectiveandsubjectiveresponses. Theframework
Combiningautomatedgenerationwithhumancu-
alsogeneratesquestionsfromdifferentsectionsof
rationforbenchmarkingRAGoffersasynergistic
documents to evaluate LLM performance across
approachtoensurebothefficiencyandqualityin
various sections and question types, and (iv) we
questionsets. Thishybridapproachleveragesthe
utilizeexistingscoringframeworkslikeRAGASto
strengthsofbothautomatedandhuman-drivenpro-
evaluatemodelperformance,incorporatingdiffer-
cesses,thatprovideefficientandrobustevaluation
entLLMsasevaluatorsforscoring. Thisapproach
metricsforRAG’sperformance.
ensuresscalabilityandquickreproducibilityofthis
In this work, we present a hybrid workflow to
approach,whilealsoprovidingaholisticcompar-
benchmarkRAGs,whichcombinesrapidquestion
isonofLLMperformanceintermsofresponding
generationthroughautomatedmethods,augmented
toquestionsandassessingorcomparingLLMre-
withproperlydesignedhumanpromptstogenerate
sponseswiththegroundtruthanswers.
diversesetofquestions. Ourproposedbenchmark-
ingframeworkisusedtogeneratequestionsfrom 2 RelatedWorks
documentsrelatedtowindturbinesitingandper-
mitting. These questions serve as a tool to evalu- Therehavebeenalotofworkinthefieldofbench-
atetheperformanceofRAG-basedLLMs,which marking,particularlyforquestionanswering(QA)
aredesignedtoanswerqueriesrelatedtotheseex- task. ThesecanbebroadlydividedintogeneralQA
tensiveandcomprehensivedocuments. Giventhe anddomain-specificQA.
vastamountofinformationcontainedinthesedoc- The Stanford Question Answering Dataset
uments, manually reviewing them is impractical, (SQuAD) (Rajpurkar et al., 2016), consisting of
making RAG-based LLMs essential for generat- 100,000+questionsandareadingcomprehension
ing accurate responses to specific queries. Our dataset, is arguably the most famous general QA
benchmarking framework assesses the effective- benchmark of the field. They contain three sub-
ness of these models in accurately retrieving and tasks within QA: reading comprehension, Open-
respondingtoqueries,ensuringthattheycanreli- domain QA, and missing word prediction. The
ablyprocessandproviderelevantinformationfrom AI2 Reasoning Challenge (ARC) (Clark et al.,
thedocuments. 2018) is another major work, which contains al-
most8,000sciencequestionsinEnglish,andalso
Contributions The paper introduces a novel
includedquestionsthatneitheraretrieval-basedal-
benchmarkinaspecificdomainandalsoproposes
gorithmnorawordco-occurrencealgorithmwere
agenericframeworktoevaluatetheperformance
able to answer correctly. Similarly, the MCTest
of RAG-based LLMs in responding to different
dataset (Richardson et al., 2013) consists of 500
typesofquestions. Thisframeworkisdesignedto
stories and 2000 young children level multiple-
be adaptable across various domains, with a spe-
choice reading comprehension questions. Some
cific focus on wind energy-related documents in
othernotableQAbenchmarksincludeBigBench
thisstudy. Thecontributionsofthisresearchareas
follows: (i)WepresentPermitQA,1thefirstbench- (Srivastavaetal.,2022),ARC2(Bhakthavatsalam
etal.,2021),GLUE(Wangetal.,2018),Common-
mark in the Wind Siting and Permitting domain,
senseQA (Talmor et al., 2018), TriviaQA: 650K
(ii) our proposed framework is domain-agnostic,
QApairswithevidence(Joshietal.,2017),Search
soitcanbetailoredforanydesirednichedomain
QA(Dunnetal.,2017),NewsQA:10Knewsarti-
(iii)weintroduceahybridmethodtoautomatically
cles(Trischleretal.,2016),interalia.
generatevarioustypesofquestions,producingboth
More recently, there have been several bench-
1Thisbenchmarkwillbemadepubliclyavailable. marksthatfocusonscientificandadjacentfields.ThescientificportionsoftheMMLU(Hendrycks
etal.,2020)benchmarkisperhapsoneofthemost
widely used science benchmarks, which include
collegeandhighschoollevelquestionsinPhysics,
Chemistry, Biology, Computer Science, etc. Sci-
ence Questions: 1K multiple choice questions
in AI2R (Talmor et al., 2018) and SciQ Dataset:
(Welbletal.,2017)13,679multiplechoicescience
questions are two other major benchmarks in the
Figure2:AnoverviewoftheproposedRAGbenchmark-
scientific domain, as is the SciQA (Auer et al., ingframework. Multipleversionsofhybridquestions
2023),ascientificQAbenchmarkcreatedbyusing aregeneratedfromspecifictextchunksofsourcedocu-
knowledgegraphsofacademicarticles. SciRepE- mentswithhuman-in-the-looptoreviewthem. These
val(Singh et al., 2022) is a benchmark that has questions are used as prompts for the LLM or RAG
modelundertest.
fourdifferenttasktypes–classification,regression,
proximity–overscientificdocuments.
Similarly,someoftheothermostrecentworks
LLMs,weaimtoenhancethereliabilityandaccu-
include SciBench(Wang et al., 2023), a bench-
racyofresponsesrelatedtowindenergy,leveraging
mark of ∼700 questions sourced from textbooks
therichinformationwithinourextensivedocument
forcollege-levelscienceproblemsandQASA(Lee
collection. Thisapproachensuresthattheinforma-
etal.,2023),aQAbenchmarkof∼1800questions
tionprovidedisbothrelevantandgroundedinthe
totestreasoningonscientificarticles,specifically
sourcedmaterial.
in AI and ML domains. There are also bench-
We constructed a data extraction and curation
marksthataddressspecificfields,withTheoremQA
pipelinetoextracttext,image,andtableinforma-
(Chenetal.,2023b)formathematics,emrQA(Pam-
tion from wind energy-related documents as de-
parietal.,2018)formedicine,andBioRead(Pap-
pictedinFigure1. Utilizinglargelanguagemodel
pasetal.,2018)andBioMRC(Pappasetal.,2020)
(LLM)basedmethodssuchastheUnstructured.io
forbiology,andNukeBERT(Jainetal.,2020)and
tool(Raymond,2023),weefficientlyextractedin-
NuclearQA(Acharyaetal.,2023)forthenuclear
formation and converted it into JSON elements.
domain.
TheseJSONelementswerethenorganizedintoa
Whilethesescientificdomainsarerelatedtoour schema, creating a page-wise assortment of text,
task in terms of technological similarity, to our table,andimageelements. Thisstructuredformat
knowledge, there are no benchmarks for our spe- ensures that the extracted data is easily accessi-
cificfieldandthisisthefirstsuchwork. Theonly bleandcanbeaccuratelyreferencedduringmodel
onethatcomescloseistheNEPAQuADbenchmark trainingandevaluation.
(Phanetal.,2023)thatdealswithQAtaskforEn-
vironmentalImpactStatement(EIS)documents. 4 Methodology
3 DatasetCreation Whilepastworkshavegenerallypreferredtouse
crowdsourcingasawaytocraftdatasetsandbench-
Inthispaper,wefocusonwindenergy-relateddoc- marks(Sapetal.,2019;Acharyaetal.,2021),we
umentstoenabletheRAG-basedLLMstoanswer choosetoautomatedmethodsforbenchmarkques-
questions specific to this field. We gather PDF tiongeneration. Automaticallygeneratingbench-
documents,includingresearcharticlesandenviron- markingquestionsusingGPT-4allowsforefficient
mentalimpactstudiespublishedbytheDepartment andscalableevaluationofotherLLMsandRAG.
ofEnergy(DOE)undertheNationalEnvironmen- However,thisapproachcanintroduceerrors,lead-
talPolicyAct(NEPA).Accessinginformationfrom ing to poor quality of questions being generated.
this vast database is not straightforward, necessi- This makes it essential to incorporate a human-
tating the need for a trained LLM to accurately in-the-loop for reviewing and refining the ques-
retrieve and answer questions from the provided tions and responses. This paper proposes hybrid
context. Thechallengeistoensurethatthemodel’s approaches, where automated methods are com-
responsesarebasedontheactualdocumentsanddo binedwithhumancurationtoensuretheaccuracy
nothallucinateinformation. ByusingRAG-based and reliability of the benchmarking process. Byleveragingbothmachineandhumanexpertise,we
canachievemorerobustandcomprehensivebench-
markingframework.
Figure2providesanoverviewoftheproposed
LLM benchmarking framework. The core of the
benchmarking framework is the question genera-
tion aspect, where automatic generation of ques-
tionsformsthefoundation. Wecombinethiswith
human curation to select high-quality questions,
ensuringrelevanceandclarity. Correspondingan-
swerstothesequestionsarethenvalidatedbyhu-
mans, establishing a reliable ground truth. This
curated set of questions and validated answers is
usedtoevaluatetheresponsesofotherLLMsand
RAGmodels.
Figure 3: Summary of “introduction” section of a re-
Different question types. We generate multiple
port(Invenergy,2014)generatedbyGPT-4. Theques-
types of questions, including closed, open, com- tionandtheansweraregeneratedfromthesummarized
parison,evaluation,recall,process,andrhetorical textchunk. Theanswerisretrievedfromasubsetoftext
questions. Thisdiversityensuresacomprehensive inthechunk,shownhereinred.
benchmarking process, as each question type as-
sessesdifferentaspectsofthemodels’capabilities.
questionsfromthesummaries.
By incorporatinga widevarietyof questions, we
canmoreeffectivelyevaluateandcomparetheper-
Hybridprompts. WeuseGPT-4toautomatically
formanceofLLMsandRAGmodelsacrossvarious
generatequestionsfromagiventextchunkbypro-
dimensions. Thisapproachprovidesaholisticview
viding particular prompts for each question type.
oftheirstrengthsandweaknesses.
Thepromptisstructuredasfollows:
Eachofthesequestiontypeevaluatesdifferent
capabilities of the LLM under test. Open ques- Generate{num}questionsgiventhecontentprovidedin
thefollowingparagraph.Restrictthetypeofquestionsto
tionsrequiremodelstogeneratedetailed,free-form
{questiontype}questions.
responses, testingtheirabilitytoconstructcoher- {Textchunktogeneratethequestions.}
ent and informative answers. Comparison ques-
An important aspect of our approach is curat-
tionsaskmodelstocompareandcontrastdifferent
ing the automatically generated questions to en-
conceptsorentities,assessingtheiranalyticaland
hancethequality. Tothisend,wemanuallyiden-
comparativereasoningskills. Evaluationquestions
tifyquestionswhicharebestsuitedforthepurpose
require models to make judgments or provide as-
ofbenchmarkingLLMs. Weperformthisprocess
sessments, gaugingtheirabilitytoevaluateinfor-
for each type of question, so that we include par-
mation critically. Recall questions focus on the
ticular grammatical structures for each question
model’sabilitytoretrieveandreproducespecific
type. Thereafter,weusetheseidentifiedquestions
informationfrommemory,testingtheirfactualac-
asfew-shotexamplestoregeneratequestionsusing
curacy. Process questions ask models to explain
theautomaticquestiongenerationframework. The
processesorsequencesofactions,evaluatingtheir
updatedpromptlooksasfollows:
understanding of procedures and logical progres-
sion. Rhetoricalquestionsareusedtotestthemod- Generate{num}questionsgiventhecontentprovidedin
els’graspofnuancesinlanguageandtheirability thefollowingparagraph.Restrictthetypeofquestionsto
{questiontype}questions.
torecognizeandappropriatelyrespondtoquestions
{Textchunktogeneratethequestions.}
thatmaynotrequiredirectanswers. Youcangeneratesimilarquestions(butnotlimited)to
samplequestionsprovidedbelow.
Next,wepresenttwoapproachesforthehybrid
{Listofsamplequestions}
question generation procedure required for LLM
benchmarking purposes. The first approach Hybrid text chunks. A problem with the afore-
engineers the prompt to generate well curated mentioned approach is that a significant number
enhancedqualityquestions. Thesecondapproach ofquestionsaregeneratedonasinglesentenceba-
summarizestheprovidedtextchunksandgenerates sis. This is obtained by substituting the subjectTable 1: Land Cover Types, Coverage, and Composition Generate {num} questions given the table provided in
withinthePleasantRidgeProjectArea,BasedonNational HTMLformatinthefollowingparagraph?Generatethe
LandCoverDatabaseinMayof2014(Invenergy,2014) questionskeepinginmindthatthecaptionofthetableis
“‘{Tablecaptionobtainedfromdocument.}”’Restrictthe
Habitat Acres[Hectares] %Composition questionssuchthattheanswersareonlyfromtheprovided
tableinthehtmlformat.Foreachquestion,return3lines:
CultivatedCrops 55,946[22,641] 92.6
question/answer/proof.Makesuretherearenonewline
Developed 3,432[1,389] 5.7 charactersintheproof.
DeciduousForest 451[183] 0.7
Inputtable:
Hay/Pasture 347[140] 0.6 “‘{TableinHTMLformatextractedfromdocument}”’
OpenWater 122[49] 0.2
Table1showsatablefromthereport(Invenergy,
WoodyWetlands 111[45] 0.2 2014)andwegeneratequestionsfromthistableas
BarrenLand 19[8] 0.0 follows.
Herbaceous 3[1] 0.0
Question:WhatistheacreageofCultivatedCropswithin
Total 60,431[24,456] 100 the Pleasant Ridge Project Area based on the National
LandCoverDatabaseinMayof2014?
Answer:TheacreageofCultivatedCropswithinthePleas-
or object of a sentence with a ‘wh’ word. These antRidgeProjectAreais55,946acres.
Proof:Thetableentryunderthe“Habitat”columnfor
generatedquestionsaremeaningfulwhenwecon-
“CultivatedCrops”correspondswiththeentryunderthe
sider question types such as ‘closed’, ‘open’, or “Acres[Hectares]”columnthatreads“55,946[22,641]”
‘recall’,wheretheanswerscanbeasinglesentence
5 ResultsandDiscussion
fromtheprovidedtextchunk. However,‘process’,
‘evaluation’, and ‘comparison’ type questions of
WeevaluatethreeRAG-basedLLMs,namelyGPT-
enhancedqualityrequiretheanswertobeinferred
4, Gemini, and Claude, on our PermitQA bench-
from a larger portion of the given text chunk. To
mark. TheRAGASframeworkisemployedforthis
thisend,firstweuseGPT-4tosummarizetheentire
evaluation, utilizing an evaluator LLM to assess
textchunk(consistingofmorethan15sentences)
themodels’performance. Theassessmentincludes
intoasummarizedtextchunk(consisting5-8sen-
metricssuchasanswercorrectness,contextpreci-
tences)usingapromptasfollows:
sion,andcontextrecall,providingacomprehensive
Youareasmartassistant.Canyousummarizethisinput understanding of each model’s capabilities in re-
paragraphwithin{num}bulletpoints.Returnthesumma- trievingandgeneratingaccurateinformationfrom
rizedtext.
thegivencontext. Inourcase,wehaveusedGPT-
Inputparagraph:“‘{Textchunktosummarize}”’
4andGemini-1.5Proaschoicesfortheevaluator
Thereafter,weuseGPT-4withappropriateprompts LLMs. Figure 4 presents the answer correctness
togeneratequestionsfromthesesummarizedtext score, while context precision and context recall
chunks using the previous hybrid prompt along depictedinTable2showtheabilityofthemodels
with the list of sample questions. Here, we show toretrievethecontextaccurately.
anexamplequestiongeneratedusingthisapproach.
Observation1 Theobservedanswercorrectness
Weincludethesummarytextchunkgeneratedby
scores are notably low, indicating a robust and
GPT-4 in Figure 3 and highlight the text in red
challengingbenchmark.
color,fromwhichtheanswerforthe‘comparison’
typequestionisretrieved. Specifically, "evaluation" and "comparison" type
questions yield nearly zero answer correctness
Question:Howdoestheproportionofcultivatedcropland
scoresforallmodels,highlightingtheirdifficulty
withinthePleasantRidgeWindResourceArea(PRWRA)
comparedtotheproportionofdevelopedareas? inresponding. Recallthat,thesechallengingques-
Answer: Cultivated cropland covers 92.3% of the tionswerecraftedfromsummariesoftextchunks
PRWRAwhiledevelopedareascover5.1%. ratherthanthetextchunksthemselves,furthercom-
plicatingthemodels’abilitytogeneratecorrectan-
Questions from tables. Another important as-
swers. Thisunderscoresthecomplexityandrigor
pectofbenchmarkingRAGmodelsinthedomain
ofthebenchmarkingprocess,emphasizingtheneed
ofresearcharticlesandreportsistoevaluatetheir
formodelstoimprovetheirunderstandingandcon-
performanceinretrievinginformationfromtables.
textualextractioncapabilities.
Tablesareimportantcontentsinsideresearchdoc-
umentsandoftencontainusefulsummariesofthe Observation2 There is an alignment in evalua-
entiredocuments. tionsmadebythetwoevaluatorLLMsusedwithinGPT-4asEvaluator Gemini1.5ProasEvaluator
Model→ GPT Claude Gemini GPT Claude Gemini
Section↓ Type↓ Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec.
closed 0.467 0.314 0.500 0.330 0.570 0.385 0.392 0.435 0.424 0.448 0.467 0.563
comparison 0.556 0.596 0.607 0.672 0.587 0.628 0.429 0.597 0.480 0.637 0.454 0.632
Introduction process 0.565 0.608 0.598 0.625 0.586 0.602 0.457 0.568 0.467 0.603 0.483 0.591
recall 0.529 0.597 0.560 0.617 0.540 0.586 0.491 0.611 0.487 0.624 0.483 0.601
rhetorical 0.305 0.296 0.365 0.353 0.319 0.306 0.272 0.299 0.323 0.339 0.283 0.299
closed 0.162 0.119 0.168 0.139 0.094 0.082 0.128 0.176 0.144 0.174 0.084 0.093
open 0.364 0.431 0.431 0.540 0.378 0.471 0.333 0.455 0.383 0.511 0.367 0.446
evaluation 0.400 0.387 0.442 0.453 0.416 0.422 0.311 0.406 0.352 0.474 0.316 0.430
Method
process 0.270 0.275 0.270 0.293 0.282 0.302 0.209 0.282 0.162 0.268 0.210 0.306
recall 0.234 0.277 0.223 0.268 0.250 0.285 0.223 0.270 0.188 0.251 0.212 0.278
rhetorical 0.229 0.223 0.241 0.232 0.250 0.238 0.208 0.238 0.193 0.230 0.224 0.248
closed 0.143 0.077 0.102 0.072 0.076 0.059 0.120 0.101 0.093 0.099 0.070 0.086
open 0.284 0.328 0.263 0.280 0.325 0.320 0.230 0.306 0.192 0.265 0.253 0.320
Results comparison 0.167 0.174 0.139 0.141 0.172 0.173 0.128 0.157 0.098 0.119 0.134 0.156
evaluation 0.272 0.254 0.217 0.218 0.257 0.263 0.226 0.252 0.171 0.229 0.209 0.266
rhetorical 0.192 0.182 0.133 0.126 0.183 0.175 0.156 0.180 0.100 0.136 0.160 0.176
comparison 0.048 0.051 0.059 0.065 0.055 0.058 0.045 0.050 0.053 0.059 0.050 0.058
Conclusion evaluation 0.082 0.079 0.100 0.103 0.086 0.089 0.073 0.081 0.072 0.084 0.078 0.081
rhetorical 0.138 0.141 0.178 0.171 0.148 0.147 0.126 0.148 0.149 0.165 0.133 0.144
Table 2: Performance of the models on the PermitQA benchmark scored using the RAGAS framework across
evaluators. The"Prec."and"Rec."meanContextPrecisionandContextRecallrespectively,while"Type"refersto
theQuestionType. Thebestperformanceforeachquestiontypeperevaluatorishighlightedinbold.
the RAGAS framework, particularly visible for vealsahigheranswercorrectnessforresponsesto
‘closed’typequestions. ‘open’typequestionsthan‘closed’typequestions.
Thissimilarityarisesbecausetheanswerstothese From this observation, we conclude that RAG-
questionsareobjective(‘yes’or‘no’),leadingto based models generate more accurate subjective
equivalent correctness evaluations by both mod- responsesto‘open’questionsthanobjective(‘yes’
els. Although there are some mismatches in the or ‘no’) responses for ‘closed’ questions. This
evaluationsmadebythetwoevaluatormodels,the suggests that these models perform better when
numberofthesediscrepanciesisinsignificantcom- tasked with generating detailed, context-rich an-
paredtothenumberofmatchingevaluations. swersratherthansimple,binaryones,highlighting
Figure 5 displays the confusion matrix illus- their strength in handling nuanced and complex
trating the evaluations made by the two evalua- queries.
tor LLMs (GPT-4 and Gemini-1.5Pro) on the re-
sponses provided by the RAG-based Claude and Observation4 Theanswercorrectnessscoresfor
questionsderivedfromthe“Introduction”section
GPT-4modelstothebenchmarkingquestions. In
arehighercomparedtothosefromothersections.
thiscontext,atruepositiveoccurswhentheLLM
evaluator correctly identifies the model response
Thisisbecausethe“introduction”sectionistypi-
asmatchingthegroundtruth. Conversely,afalse
callylonger,moresimilartootherdocuments,and
positiveariseswhentheLLMevaluatorincorrectly
oftenincludesarelatedworkssection,whichaligns
statesthatthemodelresponsematchestheground
closely with content found in many other docu-
truth, while it does not. This matrix helps visual-
ments. As a result, the RAG-based LLMs can
izetheaccuracyandreliabilityoftheevaluations
moreeasilyextractrelevantinformationtoanswer
conducted by the LLMs, when used within the
questionsaccurately,leadingtohighercorrectness
RAGASframework. Wenotethatmajorityofeval-
scores. Additionally,thecontentinthe“introduc-
uationsmadebyeitherLLMevaluatormatchesthe
tion”sectionisprimarilytext-based,unlikeother
actualevaluationwhichindicatesthatbothofthem
sections which contain equations, tables, and fig-
arereliable.
ures. Therefore,themodelsprovidemoreaccurate
Observation3 Comparisonbetween‘closed’and responsestoquestionsfromthe“introduction”sec-
‘open’ type questions within the same section re- tioncomparedtothosefromothersections.Figure4:AnswercorrectnessscorescomputedusingtheRAGASscoringframeworkwithGPT-4andGemini-1.5Pro
asevaluatormodelsforresponsegeneratedbyallthreemodelsused.
forthemodelstoretrievetheappropriatecontext
and provide correct responses. The absence of
clear,concreteanswersinthesourcematerialcom-
plicates the models’ ability to generate accurate
and relevant responses, leading to lower correct-
nessscoresforthisquestiontype.
Observation6 Evaluations made by Gemini-
1.5Pro on the responses generated by all three
LLMs are higher than the evaluations made by
GPT-4, with the responses from Gemini LLM re-
ceivingsignificantlyhigherscores.
Figure6showsthescorescomputedbytheevalua-
torsfortheresponsesgeneratedbythethreeRAG-
basedLLMs. TheGemini-1.5Proevaluatortends
Figure 5: Confusion Matrix for evaluations by LLM
toratehighscoresevenwhentheLLMsrefuseto
evaluatorsonresponsesfromClaude(top)andGPT-4
answer. Anexampleislistedbelow:
(bottom)models
Question:IntheHTMLtablethatestimatestheannual
numberofbirdcollisionsatdifferentpercentagesofavoid-
Observation5 Theanswercorrectnessscoresfor ance,whatistheestimatednumberofcollisionsperyear
at98.0%avoidance?
‘rhetorical’questionsarelowerthanthoseforother
Expectedanswer:Theestimatednumberofcollisions
questiontypes. peryearat98.0%avoidanceis152.
Geminigeneratedanswer:Thisquestioncannotbean-
sweredfromthegivensource. Whilethetextdiscusses
Thisisbecause‘rhetorical’questionslackdefinite
birdcollisionswithwindturbines,itdoesnotprovideany
answersinthedocuments,makingitchallenging dataonestimatedcollisionnumbersatdifferentavoidanceFigure6: Answercorrectness(topleft),answersimilarity(topright),contextprecision(bottomleft)andrecall
(bottomright)scoresacrossdifferentevaluatorandgeneratormodels.
rates. domain-agnostic, meaning it can be used for cre-
GPT-4score:0.194 ating benchmark for any domain. Additionally,
Gemini-1.5Proscore0.813
ouruseoftheRAGASscoringframeworkcomes
withmultiplebenefits;itallowsforathorougheval-
Conversely, in instances where the LLMs gen-
uation of model performance, offering a holistic
eratecorrectanswers,Gemini-1.5Prohasbeenob-
assessmentofLLMcapabilities,whilealsohaving
servedtoevaluatethemasincorrect. Anexample
theadvantageofbeingeasyforotherresearchers
isdetailedbelow:
toadaptthisapproachfortheirownwork.
Question: Who is the GIS Technician in the ‘STUDY
PARTICIPANTS’table?
Expectedanswer:JRBoehrs 7 Limitations
Geminigeneratedanswer:SaifNomaniJRBoehrswas
theGISTechnician.
GPT-4score:0.703 Alimitationoftheproposedframeworkisthatthe
Gemini-1.5Proscore:0.200
automatic method of generating questions often
producesqueriesthataretoospecifictothedocu-
6 Conclusion
mentfromwhichtheywerederived. Whenthese
Inconclusion,thispaperpresentsaversatileframe- questions are posed to an LLM with a large doc-
workforevaluatingtheperformanceofRAG-based umentcorpus,themodelmaystruggletorespond
LLMsacrossvariousquestiontypesanddocument accurately,necessitatingthefilteringofambiguous
sections. We showcase this by introducing a hy- questionstoensurerelevanceandclarity. Addition-
brid, automated question-generation method that ally,theRAGASscoringframework,whichrelies
ensurescomprehensivecoverageofbothobjective onLLMsasevaluators,introducesuncertaintyin
andsubjectivequeries,andimplementthisforthe performancemetrics,asdifferentLLMevaluators
use case of wind energy related document and may score responses differently. While compar-
presentthePermitQAbenchmark,whichisafirst isons can be made for questions with objective
ofitskindbenchmarkinthewindSitingandPer- responses,evaluatingandcomparingsubjectivere-
mittingspace. However,theusefulnessofourwork sponsesacrossdifferentLLMsremainschallenging
goesbeyondthisnichedomainasourapproachis andlessconsistent.8 EthicalConsiderations Karras, Manolis Koubarakis, Dmitry Mouromtsev,
DmitriiPliukhin,DaniilRadyush,etal.2023. The
While we do not anticipate the novel work pre- sciqa scientific question answering benchmark for
sented here to introduce new ethical concerns in scholarlyknowledge. ScientificReports,13(1):7240.
andbythemselves,wedorecognizethattheremay
Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar
alsobepre-existingconcernsandissuesofthedata,
Khot, Bhavana Dalvi Mishra, Kyle Richardson,
models,andmethodologieswehaveusedforthis
Ashish Sabharwal, Carissa Schoenick, Oyvind
paper. Weacknowledgethatresearchersshouldnot Tafjord, and Peter Clark. 2021. Think you have
“simply assume that [...] research will have a net solved direct-answer question answering? try arc-
da,thedirect-answerai2reasoningchallenge. arXiv
positiveimpactontheworld”(Hechtetal.,2021).
preprintarXiv:2102.03315.
Inparticular,ithasbeenseenthatLargeLanguage
Models (LLMs), like the ones used in this work, Joy Buolamwini and Timnit Gebru. 2018. Gender
exhibitawidevarietyofbias–e.g.,religious,gen- shades: Intersectional accuracy disparities in com-
der,race,profession,andcultural–andfrequently mercialgenderclassification. InConferenceonfair-
ness,accountabilityandtransparency,pages77–91.
generateanswersthatareincorrect,misogynistic,
PMLR.
antisemitic,andgenerallytoxic(Abidetal.,2021;
BuolamwiniandGebru,2018;Liangetal.,2021; Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
Nadeem et al., 2021; Welbl et al., 2021). How- 2023a. Benchmarking large language mod-
els in retrieval-augmented generation. Preprint,
ever, when used within the parameters of our ex-
arXiv:2309.01431.
periments detailed in this paper, we did not see
such behaviour from any of the models. To our
Wenhu Chen, Ming Yin, Max Ku, Elaine Wan,
knowledge,whenusedasintended,ourmodelsdo Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang,
notposeadditionalethicalconcernsthananyother and Pan Lu. 2023b. Theoremqa: A theorem-
driven question answering dataset. arXiv preprint
LLM.
arXiv:2305.12524.
9 Acknowledgment
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,
AshishSabharwal,CarissaSchoenick,andOyvind
This research was sponsored by US DOE Wind
Tafjord.2018. Thinkyouhavesolvedquestionan-
EnergyTechnologies(WET)Officeundercontract
swering? tryarc,theai2reasoningchallenge. arXiv
XXXXX. This work was done at Pacific North- preprintarXiv:1803.05457.
west National Laboratory, a multi-program na-
tionallaboratoryoperatedbyBattellefortheU.S. MatthewDunn,LeventSagun,MikeHiggins,VUgur
Guney, Volkan Cirik, and Kyunghyun Cho. 2017.
Department of Energy under contract DE-AC05-
Searchqa: A new q&a dataset augmented with
76RLO1830. This article has been cleared by
context from a search engine. arXiv preprint
PNNLforpublicreleaseasPNNL-SA-xxxxxx. arXiv:1704.05179.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
References JinliuPan,YuxiBi,YiDai,JiaweiSun,QianyuGuo,
Meng Wang, and Haofen Wang. 2024. Retrieval-
AbubakarAbid,MaheenFarooqi,andJamesZou.2021. augmentedgenerationforlargelanguagemodels: A
Persistentanti-muslimbiasinlargelanguagemodels. survey. Preprint,arXiv:2312.10997.
InProceedingsofthe2021AAAI/ACMConference
onAI,Ethics,andSociety,pages298–306.
Brent Hecht, Lauren Wilcox, Jeffrey P Bigham, Jo-
hannesSchöning,EhsanHoque,JasonErnst,Yonatan
AnuragAcharya,SaiMunikoti,AaronHellinger,Sara
Bisk, Luigi De Russis, Lana Yarosh, Bushra An-
Smith,SrideviWagle,andSameeraHorawalavithana.
jum, et al. 2021. It’s time to do something: Miti-
2023. Nuclearqa: Ahuman-madebenchmarkforlan-
gatingthenegativeimpactsofcomputingthrougha
guagemodelsforthenucleardomain. arXivpreprint
change to the peer review process. arXiv preprint
arXiv:2310.10920.
arXiv:2112.09544.
Anurag Acharya, Kartik Talamadupula, and Mark A
DanHendrycks,CollinBurns,StevenBasart,AndyZou,
Finlayson.2021. Towardsanatlasofculturalcom-
monsenseformachinereasoning. InWorkshopon MantasMazeika,DawnSong,andJacobSteinhardt.
CommonSenseKnowledgeGraphs(CSKGs)@AAAI 2020. Measuringmassivemultitasklanguageunder-
ConferenceonArtificialIntelligence. standing. arXivpreprintarXiv:2009.03300.
Sören Auer, Dante AC Barone, Cassiano Bartz, Ed- Invenergy.2014. Birdandbatconservationstrategyfor
uardo G Cortes, Mohamad Yaser Jaradeh, Oliver Invenergy’spleasantridgewindproject.Ayush Jain, Dr NM Meenachi, and Dr B Venkatra- DimitrisPappas,IonAndroutsopoulos,andHarrisPapa-
man.2020. Nukebert: Apre-trainedlanguagemodel georgiou.2018. Bioread: Anewdatasetforbiomed-
for low resource nuclear domain. arXiv preprint icalreadingcomprehension. InProceedingsofthe
arXiv:2003.13821. EleventhInternationalConferenceonLanguageRe-
sourcesandEvaluation(LREC2018).
MandarJoshi,EunsolChoi,DanielSWeld,andLuke
Zettlemoyer.2017. Triviaqa: Alargescaledistantly Dimitris Pappas, Petros Stavropoulos, Ion Androut-
supervisedchallengedatasetforreadingcomprehen- sopoulos, and Ryan McDonald. 2020. Biomrc: A
sion. arXivpreprintarXiv:1705.03551. datasetforbiomedicalmachinereadingcomprehen-
sion. InProceedingsofthe19thSIGBioMedWork-
Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol
shoponBiomedicalLanguageProcessing,pages140–
Hwang,JaehyeonKim,Hong-inLee,andMoontae
149.
Lee.2023. Qasa: advancedquestionansweringon
scientificarticles. InProceedingsofthe40thInterna- Hung Phan, Anurag Acharya, Sarthak Chaturvedi,
tionalConferenceonMachineLearning,ICML‘23. Shivam Sharma, Mike Parker, Dan Nally, Ali Jan-
JMLR.org. nesari,KarlPazdernik,MahanteshHalappanavar,Sai
Munikoti,etal.2023. Ragvs.longcontext: Examin-
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
ingfrontierlargelanguagemodelsforenvironmental
Petroni,VladimirKarpukhin,NamanGoyal,Hein-
review document comprehension. arXiv preprint
richKüttler, MikeLewis, WentauYih, TimRock-
arXiv:2407.07321.
täschel, Sebastian Riedel, and Douwe Kiela. 2021.
Retrieval-augmented generation for knowledge- PranavRajpurkar,JianZhang,KonstantinLopyrev,and
intensivenlptasks. Preprint,arXiv:2005.11401. Percy Liang. 2016. Squad: 100,000+ questions
formachinecomprehensionoftext. arXivpreprint
PaulPuLiang,ChiyuWu,Louis-PhilippeMorency,and
arXiv:1606.05250.
Ruslan Salakhutdinov. 2021. Towards understand-
ingandmitigatingsocialbiasesinlanguagemodels. ParthaPratimRay.2023. Benchmarking,ethicalalign-
InInternationalConferenceonMachineLearning, ment,andevaluationframeworkforconversational
pages6565–6576.PMLR. ai: Advancingresponsibledevelopmentofchatgpt.
BenchCouncil Transactions on Benchmarks, Stan-
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong,
dardsandEvaluations,3(3):100136.
Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu,
TongXu,EnhongChen,YiLuo,PengCheng,Haiy- BrianRaymond.2023. UNSTRUCTURED.IO. https:
ing Deng, Zhonghao Wang, and Zijia Lu. 2024. //unstructured.io/.
CRUD-RAG:AComprehensiveChineseBenchmark
forRetrieval-AugmentedGenerationofLargeLan- MatthewRichardson,ChristopherJCBurges,andErin
guageModels. Preprint,arXiv:2401.17043. Renshaw.2013. Mctest: Achallengedatasetforthe
open-domain machine comprehension of text. In
Sai Munikoti, Anurag Acharya, Sridevi Wagle, Proceedings of the 2013 conference on empirical
and Sameera Horawalavithana. 2024a. Atlantic: methodsinnaturallanguageprocessing,pages193–
Structure-awareretrieval-augmentedlanguagemodel 203.
for interdisciplinary science. In Workshop on AI
toAccelerateScienceandEngineering,TheThirty- Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-
EighthAnnualAAAIConferenceonArtificialIntelli- draBhagavatula,NicholasLourie,HannahRashkin,
gence,volume3. BrendanRoof,NoahASmith,andYejinChoi.2019.
Atomic: An atlas of machine commonsense for if-
Sai Munikoti, Anurag Acharya, Sridevi Wagle, and then reasoning. In Proceedings of the AAAI con-
Sameera Horawalavithana. 2024b. Evaluating the ferenceonartificialintelligence,volume33,pages
effectivenessofretrieval-augmentedlargelanguage 3027–3035.
modelsinscientificdocumentreasoning. InProceed-
ings of the 4th Workshop on Scholarly Document AmanpreetSingh,MikeD’Arcy,ArmanCohan,Doug
Processing@ACL2024.AssociationforComputa- Downey, and Sergey Feldman. 2022. Scirepeval:
tionalLinguistics. Amulti-formatbenchmarkforscientificdocument
representations. arXivpreprintarXiv:2211.13308.
Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.
StereoSet: Measuringstereotypicalbiasinpretrained Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
languagemodels. InProceedingsofthe59thAnnual AbuAwalMdShoeb,AbubakarAbid,AdamFisch,
Meeting of the Association for Computational Lin- Adam R Brown, Adam Santoro, Aditya Gupta,
guisticsandthe11thInternationalJointConference Adrià Garriga-Alonso, et al. 2022. Beyond the
onNaturalLanguageProcessing(Volume1: Long imitation game: Quantifying and extrapolating the
Papers),pages5356–5371,Online.Associationfor capabilities of language models. arXiv preprint
ComputationalLinguistics. arXiv:2206.04615.
AnusriPampari,PreethiRaghavan,JenniferLiang,and Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jian Peng. 2018. emrqa: A large corpus for ques- JonathanBerant.2018. Commonsenseqa:Aquestion
tionansweringonelectronicmedicalrecords. arXiv answeringchallengetargetingcommonsenseknowl-
preprintarXiv:1809.00732. edge. arXivpreprintarXiv:1811.00937.AdamTrischler,TongWang,XingdiYuan,JustinHarris,
Alessandro Sordoni, Philip Bachman, and Kaheer
Suleman.2016. Newsqa: Amachinecomprehension
dataset. arXivpreprintarXiv:1611.09830.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: Amulti-taskbenchmarkandanalysisplatform
fornaturallanguageunderstanding. arXivpreprint
arXiv:1804.07461.
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu,
JieyuZhang,SatyenSubramaniam,ArjunRLoomba,
Shichang Zhang, Yizhou Sun, and Wei Wang.
2023. Scibench: Evaluatingcollege-levelscientific
problem-solvingabilitiesoflargelanguagemodels.
arXivpreprintarXiv:2307.10635.
Johannes Welbl, Amelia Glaese, Jonathan Uesato,
Sumanth Dathathri, John Mellor, Lisa Anne Hen-
dricks, Kirsty Anderson, Pushmeet Kohli, Ben
Coppin, and Po-Sen Huang. 2021. Challenges
in detoxifying language models. arXiv preprint
arXiv:2109.07445.
JohannesWelbl,NelsonFLiu,andMattGardner.2017.
Crowdsourcing multiple choice science questions.
arXivpreprintarXiv:1707.06209.
Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and
Aidong Zhang. 2024. Benchmarking retrieval-
augmented generation for medicine. Preprint,
arXiv:2402.13178.
LiangZhang,KatherineJijo,SpurthiSetty,EdenChung,
Fatima Javid, Natan Vidra, and Tommy Clifford.
2024. Enhancinglargelanguagemodelperformance
to answer questions and extract information more
accurately. Preprint,arXiv:2402.01722.