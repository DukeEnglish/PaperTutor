[
    {
        "title": "GRAB: A Challenging GRaph Analysis Benchmark for Large Multimodal Models",
        "authors": "Jonathan RobertsKai HanSamuel Albanie",
        "links": "http://arxiv.org/abs/2408.11817v1",
        "entry_id": "http://arxiv.org/abs/2408.11817v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11817v1",
        "summary": "Large multimodal models (LMMs) have exhibited proficiencies across many\nvisual tasks. Although numerous well-known benchmarks exist to evaluate model\nperformance, they increasingly have insufficient headroom. As such, there is a\npressing need for a new generation of benchmarks challenging enough for the\nnext generation of LMMs. One area that LMMs show potential is graph analysis,\nspecifically, the tasks an analyst might typically perform when interpreting\nfigures such as estimating the mean, intercepts or correlations of functions\nand data series. In this work, we introduce GRAB, a graph analysis benchmark,\nfit for current and future frontier LMMs. Our benchmark is entirely synthetic,\nensuring high-quality, noise-free questions. GRAB is comprised of 2170\nquestions, covering four tasks and 23 graph properties. We evaluate 20 LMMs on\nGRAB, finding it to be a challenging benchmark, with the highest performing\nmodel attaining a score of just 21.7%. Finally, we conduct various ablations to\ninvestigate where the models succeed and struggle. We release GRAB to encourage\nprogress in this important, growing domain.",
        "updated": "2024-08-21 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11817v1"
    },
    {
        "title": "SynPlay: Importing Real-world Diversity for a Synthetic Human Dataset",
        "authors": "Jinsub YimHyungtae LeeSungmin EumYi-Ting ShenYan ZhangHeesung KwonShuvra S. Bhattacharyya",
        "links": "http://arxiv.org/abs/2408.11814v1",
        "entry_id": "http://arxiv.org/abs/2408.11814v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11814v1",
        "summary": "We introduce Synthetic Playground (SynPlay), a new synthetic human dataset\nthat aims to bring out the diversity of human appearance in the real world. We\nfocus on two factors to achieve a level of diversity that has not yet been seen\nin previous works: i) realistic human motions and poses and ii) multiple camera\nviewpoints towards human instances. We first use a game engine and its\nlibrary-provided elementary motions to create games where virtual players can\ntake less-constrained and natural movements while following the game rules\n(i.e., rule-guided motion design as opposed to detail-guided design). We then\naugment the elementary motions with real human motions captured with a motion\ncapture device. To render various human appearances in the games from multiple\nviewpoints, we use seven virtual cameras encompassing the ground and aerial\nviews, capturing abundant aerial-vs-ground and dynamic-vs-static attributes of\nthe scene. Through extensive and carefully-designed experiments, we show that\nusing SynPlay in model training leads to enhanced accuracy over existing\nsynthetic datasets for human detection and segmentation. The benefit of SynPlay\nbecomes even greater for tasks in the data-scarce regime, such as few-shot and\ncross-domain learning tasks. These results clearly demonstrate that SynPlay can\nbe used as an essential dataset with rich attributes of complex human\nappearances and poses suitable for model pretraining. SynPlay dataset\ncomprising over 73k images and 6.5M human instances, is available for download\nat https://synplaydataset.github.io/.",
        "updated": "2024-08-21 17:58:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11814v1"
    },
    {
        "title": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs",
        "authors": "Yuanyang YinYaqi ZhaoYajie ZhangKe LinJiahao WangXin TaoPengfei WanDi ZhangBaoqun YinWentao Zhang",
        "links": "http://arxiv.org/abs/2408.11813v1",
        "entry_id": "http://arxiv.org/abs/2408.11813v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11813v1",
        "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated\nremarkable perceptual and reasoning abilities, typically comprising a Vision\nEncoder, an Adapter, and a Large Language Model (LLM). The adapter serves as\nthe critical bridge between the visual and language components. However,\ntraining adapters with image-level supervision often results in significant\nmisalignment, undermining the LLMs' capabilities and limiting the potential of\nMultimodal LLMs. To address this, we introduce Supervised Embedding Alignment\n(SEA), a token-level alignment method that leverages vision-language\npre-trained models, such as CLIP, to align visual tokens with the LLM's\nembedding space through contrastive learning. This approach ensures a more\ncoherent integration of visual and language representations, enhancing the\nperformance and interpretability of multimodal LLMs while preserving their\ninherent capabilities. Extensive experiments show that SEA effectively improves\nMLLMs, particularly for smaller models, without adding extra data or inference\ncomputation. SEA also lays the groundwork for developing more general and\nadaptable solutions to enhance multimodal systems.",
        "updated": "2024-08-21 17:58:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11813v1"
    },
    {
        "title": "EmbodiedSAM: Online Segment Any 3D Thing in Real Time",
        "authors": "Xiuwei XuHuangxing ChenLinqing ZhaoZiwei WangJie ZhouJiwen Lu",
        "links": "http://arxiv.org/abs/2408.11811v1",
        "entry_id": "http://arxiv.org/abs/2408.11811v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11811v1",
        "summary": "Embodied tasks require the agent to fully understand 3D scenes simultaneously\nwith its exploration, so an online, real-time, fine-grained and\nhighly-generalized 3D perception model is desperately needed. Since\nhigh-quality 3D data is limited, directly training such a model in 3D is almost\ninfeasible. Meanwhile, vision foundation models (VFM) has revolutionized the\nfield of 2D computer vision with superior performance, which makes the use of\nVFM to assist embodied 3D perception a promising direction. However, most\nexisting VFM-assisted 3D perception methods are either offline or too slow that\ncannot be applied in practical embodied tasks. In this paper, we aim to\nleverage Segment Anything Model (SAM) for real-time 3D instance segmentation in\nan online setting. This is a challenging problem since future frames are not\navailable in the input streaming RGB-D video, and an instance may be observed\nin several frames so object matching between frames is required. To address\nthese challenges, we first propose a geometric-aware query lifting module to\nrepresent the 2D masks generated by SAM by 3D-aware queries, which is then\niteratively refined by a dual-level query decoder. In this way, the 2D masks\nare transferred to fine-grained shapes on 3D point clouds. Benefit from the\nquery representation for 3D masks, we can compute the similarity matrix between\nthe 3D masks from different views by efficient matrix operation, which enables\nreal-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan\nshow our method achieves leading performance even compared with offline\nmethods. Our method also demonstrates great generalization ability in several\nzero-shot dataset transferring experiments and show great potential in\nopen-vocabulary and data-efficient setting. Code and demo are available at\nhttps://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for\ntraining and evaluation.",
        "updated": "2024-08-21 17:57:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11811v1"
    },
    {
        "title": "Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models",
        "authors": "Chun-Yen ShihLi-Xuan PengJia-Wei LiaoErnie ChuCheng-Fu ChouJun-Cheng Chen",
        "links": "http://arxiv.org/abs/2408.11810v1",
        "entry_id": "http://arxiv.org/abs/2408.11810v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11810v1",
        "summary": "Diffusion Models have emerged as powerful generative models for high-quality\nimage synthesis, with many subsequent image editing techniques based on them.\nHowever, the ease of text-based image editing introduces significant risks,\nsuch as malicious editing for scams or intellectual property infringement.\nPrevious works have attempted to safeguard images from diffusion-based editing\nby adding imperceptible perturbations. These methods are costly and\nspecifically target prevalent Latent Diffusion Models (LDMs), while\nPixel-domain Diffusion Models (PDMs) remain largely unexplored and robust\nagainst such attacks. Our work addresses this gap by proposing a novel\nattacking framework with a feature representation attack loss that exploits\nvulnerabilities in denoising UNets and a latent optimization strategy to\nenhance the naturalness of protected images. Extensive experiments demonstrate\nthe effectiveness of our approach in attacking dominant PDM-based editing\nmethods (e.g., SDEdit) while maintaining reasonable protection fidelity and\nrobustness against common defense methods. Additionally, our framework is\nextensible to LDMs, achieving comparable performance to existing approaches.",
        "updated": "2024-08-21 17:56:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11810v1"
    }
]