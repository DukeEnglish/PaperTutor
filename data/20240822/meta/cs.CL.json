[
    {
        "title": "Great Memory, Shallow Reasoning: Limits of $k$NN-LMs",
        "authors": "Shangyi GengWenting ZhaoAlexander M Rush",
        "links": "http://arxiv.org/abs/2408.11815v1",
        "entry_id": "http://arxiv.org/abs/2408.11815v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11815v1",
        "summary": "$K$-nearest neighbor language models ($k$NN-LMs), which integrate retrieval\nwith next-word prediction, have demonstrated strong performance in language\nmodeling as well as downstream NLP benchmarks. These results have led\nresearchers to argue that models trained on poor quality or outdated data could\nperform well by employing a $k$NN extension that has access to a higher-quality\ndatastore. In this work, we ask whether this improved ability to recall\ninformation really translates into downstream abilities. We extensively\nevaluate $k$NN-LMs on a diverse set of tasks, ranging from sentiment\nclassification and commonsense reasoning to multi-hop reasoning. Results show\nthat $k$NN-LMs excel at memory-intensive tasks, where utilizing the patterns in\nthe input is sufficient for determining the output, but struggle with reasoning\ntasks that require integrating multiple pieces of information to derive new\nknowledge. We further demonstrate through oracle experiments and qualitative\nanalysis that even with perfect retrieval, $k$NN-LMs still fail to determine\nthe correct answers, placing an upper bound on their reasoning performance.\nCode and datastores are released at https://github.com/GSYfate/knnlm-limits/.",
        "updated": "2024-08-21 17:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11815v1"
    },
    {
        "title": "PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting and Permitting domain",
        "authors": "Rounak MeyurHung PhanSridevi WagleJan StrubeMahantesh HalappanavarSameera HorawalavithanaAnurag AcharyaSai Munikoti",
        "links": "http://arxiv.org/abs/2408.11800v1",
        "entry_id": "http://arxiv.org/abs/2408.11800v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11800v1",
        "summary": "In the rapidly evolving landscape of Natural Language Processing (NLP) and\ntext generation, the emergence of Retrieval Augmented Generation (RAG) presents\na promising avenue for improving the quality and reliability of generated text\nby leveraging information retrieved from user specified database. Benchmarking\nis essential to evaluate and compare the performance of the different RAG\nconfigurations in terms of retriever and generator, providing insights into\ntheir effectiveness, scalability, and suitability for the specific domain and\napplications. In this paper, we present a comprehensive framework to generate a\ndomain relevant RAG benchmark. Our framework is based on automatic\nquestion-answer generation with Human (domain experts)-AI Large Language Model\n(LLM) teaming. As a case study, we demonstrate the framework by introducing\nPermitQA, a first-of-its-kind benchmark on the wind siting and permitting\ndomain which comprises of multiple scientific documents/reports related to\nenvironmental impact of wind energy projects. Our framework systematically\nevaluates RAG performance using diverse metrics and multiple question types\nwith varying complexity level. We also demonstrate the performance of different\nmodels on our benchmark.",
        "updated": "2024-08-21 17:43:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11800v1"
    },
    {
        "title": "Practical token pruning for foundation models in few-shot conversational virtual assistant systems",
        "authors": "Haode QiCheng QianJian NiPratyush SinghReza FazeliGengyu WangZhongzheng ShuEric WayneJuergen Bross",
        "links": "http://arxiv.org/abs/2408.11799v1",
        "entry_id": "http://arxiv.org/abs/2408.11799v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11799v1",
        "summary": "In an enterprise Virtual Assistant (VA) system, intent classification is the\ncrucial component that determines how a user input is handled based on what the\nuser wants. The VA system is expected to be a cost-efficient SaaS service with\nlow training and inference time while achieving high accuracy even with a small\nnumber of training samples. We pretrain a transformer-based sentence embedding\nmodel with a contrastive learning objective and leverage the embedding of the\nmodel as features when training intent classification models. Our approach\nachieves the state-of-the-art results for few-shot scenarios and performs\nbetter than other commercial solutions on popular intent classification\nbenchmarks. However, generating features via a transformer-based model\nincreases the inference time, especially for longer user inputs, due to the\nquadratic runtime of the transformer's attention mechanism. On top of model\ndistillation, we introduce a practical multi-task adaptation approach that\nconfigures dynamic token pruning without the need for task-specific training\nfor intent classification. We demonstrate that this approach improves the\ninference speed of popular sentence transformer models without affecting model\nperformance.",
        "updated": "2024-08-21 17:42:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11799v1"
    },
    {
        "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
        "authors": "Sharath Turuvekere SreenivasSaurav MuralidharanRaviraj JoshiMarcin ChochowskiMostofa PatwaryMohammad ShoeybiBryan CatanzaroJan KautzPavlo Molchanov",
        "links": "http://arxiv.org/abs/2408.11796v1",
        "entry_id": "http://arxiv.org/abs/2408.11796v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11796v1",
        "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
        "updated": "2024-08-21 17:38:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11796v1"
    },
    {
        "title": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework",
        "authors": "Zhifei XieDaniel TangDingwei TanJacques KleinTegawend F. BissyandSaad Ezzini",
        "links": "http://arxiv.org/abs/2408.11788v1",
        "entry_id": "http://arxiv.org/abs/2408.11788v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11788v1",
        "summary": "Current video generation models excel at creating short, realistic clips, but\nstruggle with longer, multi-scene videos. We introduce \\texttt{DreamFactory},\nan LLM-based framework that tackles this challenge. \\texttt{DreamFactory}\nleverages multi-agent collaboration principles and a Key Frames Iteration\nDesign Method to ensure consistency and style across long videos. It utilizes\nChain of Thought (COT) to address uncertainties inherent in large language\nmodels. \\texttt{DreamFactory} generates long, stylistically coherent, and\ncomplex videos. Evaluating these long-form videos presents a challenge. We\npropose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene\nStyle Consistency Score. To further research in this area, we contribute the\nMulti-Scene Videos Dataset containing over 150 human-rated videos.",
        "updated": "2024-08-21 17:21:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11788v1"
    }
]