[
    {
        "title": "Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction",
        "authors": "Anthony GX-ChenKenneth MarinoRob Fergus",
        "links": "http://arxiv.org/abs/2408.11816v1",
        "entry_id": "http://arxiv.org/abs/2408.11816v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11816v1",
        "summary": "In the face of difficult exploration problems in reinforcement learning, we\nstudy whether giving an agent an object-centric mapping (describing a set of\nitems and their attributes) allow for more efficient learning. We found this\nproblem is best solved hierarchically by modelling items at a higher level of\nstate abstraction to pixels, and attribute change at a higher level of temporal\nabstraction to primitive actions. This abstraction simplifies the transition\ndynamic by making specific future states easier to predict. We make use of this\nto propose a fully model-based algorithm that learns a discriminative world\nmodel, plans to explore efficiently with only a count-based intrinsic reward,\nand can subsequently plan to reach any discovered (abstract) states.\n  We demonstrate the model's ability to (i) efficiently solve single tasks,\n(ii) transfer zero-shot and few-shot across item types and environments, and\n(iii) plan across long horizons. Across a suite of 2D crafting and MiniHack\nenvironments, we empirically show our model significantly out-performs\nstate-of-the-art low-level methods (without abstraction), as well as performant\nmodel-free and model-based methods using the same abstraction. Finally, we show\nhow to reinforce learn low level object-perturbing policies, as well as\nsupervise learn the object mapping itself.",
        "updated": "2024-08-21 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11816v1"
    },
    {
        "title": "Great Memory, Shallow Reasoning: Limits of $k$NN-LMs",
        "authors": "Shangyi GengWenting ZhaoAlexander M Rush",
        "links": "http://arxiv.org/abs/2408.11815v1",
        "entry_id": "http://arxiv.org/abs/2408.11815v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11815v1",
        "summary": "$K$-nearest neighbor language models ($k$NN-LMs), which integrate retrieval\nwith next-word prediction, have demonstrated strong performance in language\nmodeling as well as downstream NLP benchmarks. These results have led\nresearchers to argue that models trained on poor quality or outdated data could\nperform well by employing a $k$NN extension that has access to a higher-quality\ndatastore. In this work, we ask whether this improved ability to recall\ninformation really translates into downstream abilities. We extensively\nevaluate $k$NN-LMs on a diverse set of tasks, ranging from sentiment\nclassification and commonsense reasoning to multi-hop reasoning. Results show\nthat $k$NN-LMs excel at memory-intensive tasks, where utilizing the patterns in\nthe input is sufficient for determining the output, but struggle with reasoning\ntasks that require integrating multiple pieces of information to derive new\nknowledge. We further demonstrate through oracle experiments and qualitative\nanalysis that even with perfect retrieval, $k$NN-LMs still fail to determine\nthe correct answers, placing an upper bound on their reasoning performance.\nCode and datastores are released at https://github.com/GSYfate/knnlm-limits/.",
        "updated": "2024-08-21 17:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11815v1"
    },
    {
        "title": "Approaching Deep Learning through the Spectral Dynamics of Weights",
        "authors": "David YunisKumar Kshitij PatelSamuel WheelerPedro SavareseGal VardiKaren LivescuMichael MaireMatthew R. Walter",
        "links": "http://arxiv.org/abs/2408.11804v1",
        "entry_id": "http://arxiv.org/abs/2408.11804v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11804v1",
        "summary": "We propose an empirical approach centered on the spectral dynamics of weights\n-- the behavior of singular values and vectors during optimization -- to unify\nand clarify several phenomena in deep learning. We identify a consistent bias\nin optimization across various experiments, from small-scale ``grokking'' to\nlarge-scale tasks like image classification with ConvNets, image generation\nwith UNets, speech recognition with LSTMs, and language modeling with\nTransformers. We also demonstrate that weight decay enhances this bias beyond\nits role as a norm regularizer, even in practical systems. Moreover, we show\nthat these spectral dynamics distinguish memorizing networks from generalizing\nones, offering a novel perspective on this longstanding conundrum.\nAdditionally, we leverage spectral dynamics to explore the emergence of\nwell-performing sparse subnetworks (lottery tickets) and the structure of the\nloss surface through linear mode connectivity. Our findings suggest that\nspectral dynamics provide a coherent framework to better understand the\nbehavior of neural networks across diverse settings.",
        "updated": "2024-08-21 17:48:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11804v1"
    },
    {
        "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
        "authors": "Sharath Turuvekere SreenivasSaurav MuralidharanRaviraj JoshiMarcin ChochowskiMostofa PatwaryMohammad ShoeybiBryan CatanzaroJan KautzPavlo Molchanov",
        "links": "http://arxiv.org/abs/2408.11796v1",
        "entry_id": "http://arxiv.org/abs/2408.11796v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11796v1",
        "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
        "updated": "2024-08-21 17:38:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11796v1"
    },
    {
        "title": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design",
        "authors": "Nathaniel H. ParkTiffany J. CallahanJames L. HedrickTim ErdmannSara Capponi",
        "links": "http://arxiv.org/abs/2408.11793v1",
        "entry_id": "http://arxiv.org/abs/2408.11793v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11793v1",
        "summary": "Molecular property prediction and generative design via deep learning models\nhas been the subject of intense research given its potential to accelerate\ndevelopment of new, high-performance materials. More recently, these workflows\nhave been significantly augmented with the advent of large language models\n(LLMs) and systems of LLM-driven agents capable of utilizing pre-trained models\nto make predictions in the context of more complex research tasks. While\neffective, there is still room for substantial improvement within the agentic\nsystems on the retrieval of salient information for material design tasks.\nMoreover, alternative uses of predictive deep learning models, such as\nleveraging their latent representations to facilitate cross-modal retrieval\naugmented generation within agentic systems to enable task-specific materials\ndesign, has remained unexplored. Herein, we demonstrate that large, pre-trained\nchemistry foundation models can serve as a basis for enabling semantic\nchemistry information retrieval for both small-molecules, complex polymeric\nmaterials, and reactions. Additionally, we show the use of chemistry foundation\nmodels in conjunction with image models such as OpenCLIP facilitate\nunprecedented queries and information retrieval across multiple\ncharacterization data domains. Finally, we demonstrate the integration of these\nsystems within multi-agent systems to facilitate structure and\ntopological-based natural language queries and information retrieval for\ncomplex research tasks.",
        "updated": "2024-08-21 17:25:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11793v1"
    }
]