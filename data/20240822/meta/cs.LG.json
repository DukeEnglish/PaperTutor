[
    {
        "title": "Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction",
        "authors": "Anthony GX-ChenKenneth MarinoRob Fergus",
        "links": "http://arxiv.org/abs/2408.11816v1",
        "entry_id": "http://arxiv.org/abs/2408.11816v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11816v1",
        "summary": "In the face of difficult exploration problems in reinforcement learning, we\nstudy whether giving an agent an object-centric mapping (describing a set of\nitems and their attributes) allow for more efficient learning. We found this\nproblem is best solved hierarchically by modelling items at a higher level of\nstate abstraction to pixels, and attribute change at a higher level of temporal\nabstraction to primitive actions. This abstraction simplifies the transition\ndynamic by making specific future states easier to predict. We make use of this\nto propose a fully model-based algorithm that learns a discriminative world\nmodel, plans to explore efficiently with only a count-based intrinsic reward,\nand can subsequently plan to reach any discovered (abstract) states.\n  We demonstrate the model's ability to (i) efficiently solve single tasks,\n(ii) transfer zero-shot and few-shot across item types and environments, and\n(iii) plan across long horizons. Across a suite of 2D crafting and MiniHack\nenvironments, we empirically show our model significantly out-performs\nstate-of-the-art low-level methods (without abstraction), as well as performant\nmodel-free and model-based methods using the same abstraction. Finally, we show\nhow to reinforce learn low level object-perturbing policies, as well as\nsupervise learn the object mapping itself.",
        "updated": "2024-08-21 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11816v1"
    },
    {
        "title": "Scaling Cross-Embodied Learning: One Policy for Manipulation, Navigation, Locomotion and Aviation",
        "authors": "Ria DoshiHomer WalkeOier MeesSudeep DasariSergey Levine",
        "links": "http://arxiv.org/abs/2408.11812v1",
        "entry_id": "http://arxiv.org/abs/2408.11812v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11812v1",
        "summary": "Modern machine learning systems rely on large datasets to attain broad\ngeneralization, and this often poses a challenge in robot learning, where each\nrobotic platform and task might have only a small dataset. By training a single\npolicy across many different kinds of robots, a robot learning method can\nleverage much broader and more diverse datasets, which in turn can lead to\nbetter generalization and robustness. However, training a single policy on\nmulti-robot data is challenging because robots can have widely varying sensors,\nactuators, and control frequencies. We propose CrossFormer, a scalable and\nflexible transformer-based policy that can consume data from any embodiment. We\ntrain CrossFormer on the largest and most diverse dataset to date, 900K\ntrajectories across 20 different robot embodiments. We demonstrate that the\nsame network weights can control vastly different robots, including single and\ndual arm manipulation systems, wheeled robots, quadcopters, and quadrupeds.\nUnlike prior work, our model does not require manual alignment of the\nobservation or action spaces. Extensive experiments in the real world show that\nour method matches the performance of specialist policies tailored for each\nembodiment, while also significantly outperforming the prior state of the art\nin cross-embodiment learning.",
        "updated": "2024-08-21 17:57:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11812v1"
    },
    {
        "title": "ACE: A Cross-Platform Visual-Exoskeletons System for Low-Cost Dexterous Teleoperation",
        "authors": "Shiqi YangMinghuan LiuYuzhe QinRunyu DingJialong LiXuxin ChengRuihan YangSha YiXiaolong Wang",
        "links": "http://arxiv.org/abs/2408.11805v1",
        "entry_id": "http://arxiv.org/abs/2408.11805v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11805v1",
        "summary": "Learning from demonstrations has shown to be an effective approach to robotic\nmanipulation, especially with the recently collected large-scale robot data\nwith teleoperation systems. Building an efficient teleoperation system across\ndiverse robot platforms has become more crucial than ever. However, there is a\nnotable lack of cost-effective and user-friendly teleoperation systems for\ndifferent end-effectors, e.g., anthropomorphic robot hands and grippers, that\ncan operate across multiple platforms. To address this issue, we develop ACE, a\ncross-platform visual-exoskeleton system for low-cost dexterous teleoperation.\nOur system utilizes a hand-facing camera to capture 3D hand poses and an\nexoskeleton mounted on a portable base, enabling accurate real-time capture of\nboth finger and wrist poses. Compared to previous systems, which often require\nhardware customization according to different robots, our single system can\ngeneralize to humanoid hands, arm-hands, arm-gripper, and quadruped-gripper\nsystems with high-precision teleoperation. This enables imitation learning for\ncomplex manipulation tasks on diverse platforms.",
        "updated": "2024-08-21 17:48:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11805v1"
    },
    {
        "title": "Approaching Deep Learning through the Spectral Dynamics of Weights",
        "authors": "David YunisKumar Kshitij PatelSamuel WheelerPedro SavareseGal VardiKaren LivescuMichael MaireMatthew R. Walter",
        "links": "http://arxiv.org/abs/2408.11804v1",
        "entry_id": "http://arxiv.org/abs/2408.11804v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11804v1",
        "summary": "We propose an empirical approach centered on the spectral dynamics of weights\n-- the behavior of singular values and vectors during optimization -- to unify\nand clarify several phenomena in deep learning. We identify a consistent bias\nin optimization across various experiments, from small-scale ``grokking'' to\nlarge-scale tasks like image classification with ConvNets, image generation\nwith UNets, speech recognition with LSTMs, and language modeling with\nTransformers. We also demonstrate that weight decay enhances this bias beyond\nits role as a norm regularizer, even in practical systems. Moreover, we show\nthat these spectral dynamics distinguish memorizing networks from generalizing\nones, offering a novel perspective on this longstanding conundrum.\nAdditionally, we leverage spectral dynamics to explore the emergence of\nwell-performing sparse subnetworks (lottery tickets) and the structure of the\nloss surface through linear mode connectivity. Our findings suggest that\nspectral dynamics provide a coherent framework to better understand the\nbehavior of neural networks across diverse settings.",
        "updated": "2024-08-21 17:48:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11804v1"
    },
    {
        "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
        "authors": "Sharath Turuvekere SreenivasSaurav MuralidharanRaviraj JoshiMarcin ChochowskiMostofa PatwaryMohammad ShoeybiBryan CatanzaroJan KautzPavlo Molchanov",
        "links": "http://arxiv.org/abs/2408.11796v1",
        "entry_id": "http://arxiv.org/abs/2408.11796v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11796v1",
        "summary": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.",
        "updated": "2024-08-21 17:38:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11796v1"
    }
]