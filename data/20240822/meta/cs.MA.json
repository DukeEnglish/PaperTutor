[
    {
        "title": "VIRIS: Simulating indoor airborne transmission combining architectural design and people movement",
        "authors": "Yidan XueWassim JabiThomas E. WoolleyKaterina Kaouri",
        "links": "http://arxiv.org/abs/2408.11772v1",
        "entry_id": "http://arxiv.org/abs/2408.11772v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11772v1",
        "summary": "A Viral Infection Risk Indoor Simulator (VIRIS) has been developed to quickly\nassess and compare mitigations for airborne disease spread. This agent-based\nsimulator combines people movement in an indoor space, viral transmission\nmodelling and detailed architectural design, and it is powered by topologicpy,\nan open-source Python library. VIRIS generates very fast predictions of the\nviral concentration and the spatiotemporal infection risk for individuals as\nthey move through a given space. The simulator is validated with data from a\ncourtroom superspreader event. A sensitivity study for unknown parameter values\nis also performed. We compare several non-pharmaceutical interventions (NPIs)\nissued in UK government guidance, for two indoor settings: a care home and a\nsupermarket. Additionally, we have developed the user-friendly VIRIS web app\nthat allows quick exploration of diverse scenarios of interest and\nvisualisation, allowing policymakers, architects and space managers to easily\ndesign or assess infection risk in an indoor space.",
        "updated": "2024-08-21 16:54:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11772v1"
    },
    {
        "title": "Bayesian Optimization Framework for Efficient Fleet Design in Autonomous Multi-Robot Exploration",
        "authors": "David Molina ConchaJiping LiHaoran YinKyeonghyeon ParkHyun-Rok LeeTaesik LeeDhruv SirohiChi-Guhn Lee",
        "links": "http://arxiv.org/abs/2408.11751v1",
        "entry_id": "http://arxiv.org/abs/2408.11751v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11751v1",
        "summary": "This study addresses the challenge of fleet design optimization in the\ncontext of heterogeneous multi-robot fleets, aiming to obtain feasible designs\nthat balance performance and costs. In the domain of autonomous multi-robot\nexploration, reinforcement learning agents play a central role, offering\nadaptability to complex terrains and facilitating collaboration among robots.\nHowever, modifying the fleet composition results in changes in the learned\nbehavior, and training multi-robot systems using multi-agent reinforcement\nlearning is expensive. Therefore, an exhaustive evaluation of each potential\nfleet design is infeasible. To tackle these hurdles, we introduce Bayesian\nOptimization for Fleet Design (BOFD), a framework leveraging multi-objective\nBayesian Optimization to explore fleets on the Pareto front of performance and\ncost while accounting for uncertainty in the design space. Moreover, we\nestablish a sub-linear bound for cumulative regret, supporting BOFD's\nrobustness and efficacy. Extensive benchmark experiments in synthetic and\nsimulated environments demonstrate the superiority of our framework over\nstate-of-the-art methods, achieving efficient fleet designs with minimal fleet\nevaluations.",
        "updated": "2024-08-21 16:22:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11751v1"
    },
    {
        "title": "Networked Communication for Mean-Field Games with Function Approximation and Empirical Mean-Field Estimation",
        "authors": "Patrick BenjaminAlessandro Abate",
        "links": "http://arxiv.org/abs/2408.11607v1",
        "entry_id": "http://arxiv.org/abs/2408.11607v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11607v1",
        "summary": "Recent works have provided algorithms by which decentralised agents, which\nmay be connected via a communication network, can learn equilibria in\nMean-Field Games from a single, non-episodic run of the empirical system.\nHowever, these algorithms are given for tabular settings: this computationally\nlimits the size of players' observation space, meaning that the algorithms are\nnot able to handle anything but small state spaces, nor to generalise beyond\npolicies depending on the ego player's state to so-called\n'population-dependent' policies. We address this limitation by introducing\nfunction approximation to the existing setting, drawing on the Munchausen\nOnline Mirror Descent method that has previously been employed only in\nfinite-horizon, episodic, centralised settings. While this permits us to\ninclude the population's mean-field distribution in the observation for each\nplayer's policy, it is arguably unrealistic to assume that decentralised agents\nwould have access to this global information: we therefore additionally provide\nnew algorithms that allow agents to estimate the global empirical distribution\nbased on a local neighbourhood, and to improve this estimate via communication\nover a given network. Our experiments showcase how the communication network\nallows decentralised agents to estimate the mean-field distribution for\npopulation-dependent policies, and that exchanging policy information helps\nnetworked agents to outperform both independent and even centralised agents in\nfunction-approximation settings, by an even greater margin than in tabular\nsettings.",
        "updated": "2024-08-21 13:32:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11607v1"
    },
    {
        "title": "Subgoal-based Hierarchical Reinforcement Learning for Multi-Agent Collaboration",
        "authors": "Cheng XuChangtian ZhangYuchen ShiRan WangShihong DuanYadong WanXiaotong Zhang",
        "links": "http://arxiv.org/abs/2408.11416v1",
        "entry_id": "http://arxiv.org/abs/2408.11416v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11416v1",
        "summary": "Recent advancements in reinforcement learning have made significant impacts\nacross various domains, yet they often struggle in complex multi-agent\nenvironments due to issues like algorithm instability, low sampling efficiency,\nand the challenges of exploration and dimensionality explosion. Hierarchical\nreinforcement learning (HRL) offers a structured approach to decompose complex\ntasks into simpler sub-tasks, which is promising for multi-agent settings. This\npaper advances the field by introducing a hierarchical architecture that\nautonomously generates effective subgoals without explicit constraints,\nenhancing both flexibility and stability in training. We propose a dynamic goal\ngeneration strategy that adapts based on environmental changes. This method\nsignificantly improves the adaptability and sample efficiency of the learning\nprocess. Furthermore, we address the critical issue of credit assignment in\nmulti-agent systems by synergizing our hierarchical architecture with a\nmodified QMIX network, thus improving overall strategy coordination and\nefficiency. Comparative experiments with mainstream reinforcement learning\nalgorithms demonstrate the superior convergence speed and performance of our\napproach in both single-agent and multi-agent environments, confirming its\neffectiveness and flexibility in complex scenarios. Our code is open-sourced\nat: \\url{https://github.com/SICC-Group/GMAH}.",
        "updated": "2024-08-21 08:22:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11416v1"
    },
    {
        "title": "Deep Reinforcement Learning for Decentralized Multi-Robot Control: A DQN Approach to Robustness and Information Integration",
        "authors": "Bin WuC Steve Suh",
        "links": "http://arxiv.org/abs/2408.11339v1",
        "entry_id": "http://arxiv.org/abs/2408.11339v1",
        "pdf_url": "http://arxiv.org/pdf/2408.11339v1",
        "summary": "The superiority of Multi-Robot Systems (MRS) in various complex environments\nis unquestionable. However, in complex situations such as search and rescue,\nenvironmental monitoring, and automated production, robots are often required\nto work collaboratively without a central control unit. This necessitates an\nefficient and robust decentralized control mechanism to process local\ninformation and guide the robots' behavior. In this work, we propose a new\ndecentralized controller design method that utilizes the Deep Q-Network (DQN)\nalgorithm from deep reinforcement learning, aimed at improving the integration\nof local information and robustness of multi-robot systems. The designed\ncontroller allows each robot to make decisions independently based on its local\nobservations while enhancing the overall system's collaborative efficiency and\nadaptability to dynamic environments through a shared learning mechanism.\nThrough testing in simulated environments, we have demonstrated the\neffectiveness of this controller in improving task execution efficiency,\nstrengthening system fault tolerance, and enhancing adaptability to the\nenvironment. Furthermore, we explored the impact of DQN parameter tuning on\nsystem performance, providing insights for further optimization of the\ncontroller design. Our research not only showcases the potential application of\nthe DQN algorithm in the decentralized control of multi-robot systems but also\noffers a new perspective on how to enhance the overall performance and\nrobustness of the system through the integration of local information.",
        "updated": "2024-08-21 04:49:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.11339v1"
    }
]